Yo-Sub Han 
György Vaszil (Eds.)
LNCS 13439
24th IFIP WG 1.02 International Conference, DCFS 2022 
Debrecen, Hungary, August 29–31, 2022 
Proceedings
Descriptional Complexity 
of Formal Systems

Lecture Notes in Computer Science
13439
Founding Editors
Gerhard Goos
Karlsruhe Institute of Technology, Karlsruhe, Germany
Juris Hartmanis
Cornell University, Ithaca, NY, USA
Editorial Board Members
Elisa Bertino
Purdue University, West Lafayette, IN, USA
Wen Gao
Peking University, Beijing, China
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Moti Yung
Columbia University, New York, NY, USA

More information about this series at https://link.springer.com/bookseries/558

Yo-Sub Han
• György Vaszil (Eds.)
Descriptional Complexity
of Formal Systems
24th IFIP WG 1.02 International Conference, DCFS 2022
Debrecen, Hungary, August 29–31, 2022
Proceedings
123

Editors
Yo-Sub Han
Yonsei University
Seoul, Korea (Republic of)
György Vaszil
University of Debrecen
Debrecen, Hungary
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-031-13256-8
ISBN 978-3-031-13257-5
(eBook)
https://doi.org/10.1007/978-3-031-13257-5
© IFIP International Federation for Information Processing 2022
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, expressed or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This volume contains the papers presented at the 24th International Conference on
Descriptional Complexity of Formal Systems (DCFS 2022) which was held at the
University of Debrecen, Hungary, during August 29–31, 2022. It was jointly organized
by the Working Group 1.02 on Descriptional Complexity of the International Feder-
ation for Information Processing (IFIP) and by the Department of Computer Science at
the Faculty of Informatics of the University of Debrecen.
The DCFS conference series is an international venue for the dissemination of new
results related to all aspects of descriptional complexity including, but not limited to,
the following:
– Automata, grammars, languages, and other formal systems; various modes of
operations and complexity measures
– Succinctness of description of objects, state-explosion-like phenomena
– Circuit complexity of Boolean functions and related measures
– Size complexity of formal systems
– Structural complexity of formal systems
– Trade-offs between computational models and modes of operation
– Applications of formal systems (e.g., in software and hardware testing, in dialogue
systems, in systems modeling or in modeling natural languages) and their com-
plexity constraints
– Cooperating formal systems
– Size or structural complexity of formal systems for modeling natural languages
– Complexity aspects related to the combinatorics of words
– Descriptional complexity in resource-bounded or structure-bounded environments
– Structural complexity as related to descriptional complexity
– Frontiers between decidability and undecidability
– Universality and reversibility
– Nature-motivated (bio-inspired) architectures and unconventional models of
computing
– Blum static (Kolmogorov/Chaitin) complexity, algorithmic information
DCFS became an IFIP working conference in 2016, continuing the former Work-
shop on Descriptional Complexity of Formal Systems, which was a merger in 2002 of
two other workshops: Formal Descriptions and Software Reliability (FDSR) and
Descriptional Complexity of Automata, Grammars and Related Structures (DCAGRS).
DCAGRS was previously held in Magdeburg (1999), London (2000), and Vienna
(2001). FDSR was previously held in Paderborn (1998), Boca Raton (1999), and San
Jose (2000). Since 2002, DCFS has been successively held in London, Ontario, Canada
(2002), Budapest, Hungary (2003), London, Ontario, Canada (2004), Como, Italy
(2005), Las Cruces, New Mexico, USA (2006), Nový Smokovec, High Tatras,
Slovakia (2007), Charlottetown, Prince Edward Island, Canada (2008), Magdeburg,

Germany (2009), Saskatoon, Canada (2010), Giessen, Germany (2011), Braga,
Portugal (2012), London, Ontario, Canada (2013), Turku, Finland (2014), Waterloo,
Ontario, Canada (2015), Bucharest, Romania (2016), Milan, Italy (2017), Halifax,
Nova Scotia, Canada (2018), and Košice, Slovakia (2019). The next DCFS conferences
were planned to be held in Vienna, Austria (2020), and in Seoul, South Korea (2021),
but both of these events were canceled as in-person meetings due to the COVID-19
pandemic. The accepted papers appeared only in the conference proceedings.
This year 17 papers were submitted by authors from 14 different countries. The
number of submissions was less than usual, probably due to the current problems in the
world and to the desirable and aspired return to an in-person conference. On the other
hand, these submissions were of extraordinary quality. Therefore, after the review of
each paper by three referees, the Program Committee were able to accept 14 papers out
of the 17 submissions.
The program also included four invited talks by
– Mikołaj Bojańczyk, University of Warsaw, Poland,
– Stefano Crespi Reghizzi, Polytechnic University of Milan, Italy,
– Szabolcs Iván, University of Szeged, Hungary,
– Galina Jirásková, Slovak Academy of Sciences, Košice, Slovakia.
We thank all invited speakers, contributing authors, Program Committee members,
and external referees for their valuable contributions towards the realization of DCFS
2022.
We are also grateful to the editorial staff at Springer for their guidance and help
during the process of publishing this volume, and for supporting the event through
publication in the LNCS series.
Partial ﬁnancial support for the conference was provided by the Department of
Computer Science and by the Faculty of Informatics of the University of Debrecen.
Finally, we would like to thank the members of the organizing committee who
worked hard to make this edition successful and all participants who, either in-person
or virtually, contributed to the success of the conference.
We are looking forward to DCFS 2023 in Potsdam, Germany.
June 2022
Yo-Sub Han
György Vaszil
vi
Preface

Organization
Steering Committee
Cezar Câmpeanu
University of Prince Edward Island, Canada
Erzsébet Csuhaj-Varjú
Eötvös Loránd University, Hungary
Stavros Konstantinidis
Saint Mary’s University, Canada
Martin Kutrib (Chair)
Justus Liebig University Giessen, Germany
Giovanni Pighizzini
University of Milan, Italy
Rogério Reis
University of Porto, Portugal
Kai Salomaa
Queen’s University, Canada
Program Committee
Henning Bordihn
University of Potsdam, Germany
Johanna Björklund
University of Umeå, Sweden
Cezar Câmpeanu
University of Prince Edward Island, Canada
Erzsébet Csuhaj-Varjú
Eötvös Loránd University, Hungary
Szilárd Zsolt Fazekas
Akita University, Japan
Pawel Gawrychowski
University of Wrocław, Poland
Dora Giammarresi
Tor Vergata University of Rome, Italy
Yo-Sub Han (Co-chair)
Yonsei University, South Korea
Géza Horváth
University of Debrecen, Hungary
Galina Jirásková
Slovak Academy of Sciences, Košice, Slovakia
Stavros Konstantinidis
Saint Mary’s University, Canada
Martin Kutrib
Justus Liebig University Giessen, Germany
Ian McQuillan
University of Saskatchewan, Canada
Alexander Okhotin
St. Petersburg State University, Russia
Andrei Pǎun
University of Bucharest, Romania
Giovanni Pighizzini
University of Milan, Italy
Narad Rampersad
University of Winnipeg, Canada
Rogério Reis
University of Porto, Portugal
Michel Rigo
University of Liège, Belgium
Kai Salomaa
Queen’s University, Canada
György Vaszil (Co-chair)
University of Debrecen, Hungary
Matthias Wendlandt
Justus Liebig University Giessen, Germany
Lynette van Zijl
Stellenbosch University, South Africa

Additional Reviewers
Sabine Broda
Jürgen Dassow
Jozef Jirásek
Andreas Malcher
Nelma Moreira
Timothy Ng
Luca Prigioniero
Marek Szykuła
Bianca Truthe
Organizing Committee
Bence Hegedűs
University of Debrecen, Hungary
Géza Horváth
University of Debrecen, Hungary
Arnold Pintér
University of Debrecen, Hungary
György Vaszil
University of Debrecen, Hungary
viii
Organization

Abstracts of Invited Talks

Polyregular Functions
Mikołaj Bojańczyk
Institute of Informatics, University of Warsaw, Poland
bojan@mimuw.edu.pl
Transducers are like automata, but instead of accepting/rejecting they produce an
output, such as a string or a tree. This talk is about a class of string-to-string functions,
called the polyregular functions, which can be seen as a candidate for the notion of
regular string-to-string transducers of polynomial growth. The class has many equiv-
alent characterisations, including monadic second-order logic, two-way automata, an
imperative programming language with for loops, and functional programming
languages.

On Scattered Context-free Order Types
(Extended Abstract)
Szabolcs Iván1
Department of Informatics, University of Szeged, Hungary
szabivan@inf.u-szeged.hu
1
Introduction
When the alphabet R of a language LR is linearly ordered, the language itself can be
seen as a linearly ordered set, by the lexicographic ordering \ in which xay\xbz if
a\b and x\xy if y 2 R þ . As an example, with R ¼ fa; bg and a\b, the order types
of the languages a, a þ b and ba are x, x þ x and x2, respectively, with x
denoting the order type of the natural numbers. (For the last one, consider the chain
e\a\aa\. . .\b\ba\baa\. . .\bb\. . .)
Clearly, we can encode any such R by a constant-length homomorphism into
fa; bg preserving the order type of the language (e.g. for R ¼ fa; b; c; dg we can use
faa; ab; ba; bbg as the image of the letters) so generally it sufﬁces to consider the
binary alphabet when we are interested only in the order types. An order type is called
regular (context-free, resp.) if it is the order type of some regular (context-free, resp.)
language. Since the set R of all R-words is countable as well, the order type of any
language is countable; on the other hand, since every countable order type can be
embedded into the order type g of the rationals and L ¼ faa; bbgab has the order type
g (since it is a dense ordering without least and greatest elements), every countable
order type arises as the order type of some language.
An operational characterization of the regular order types was given in [11]. It was
shown in [2] that an ordinal is regular if and only if it is less than xx.
The central topic of the presentation, the study of context-free order types was
initiated in [1]. From the model checking aspect of interactive programs, studying
scattered order types might have its actual usage: an order type is scattered if it does not
have a dense subordering. Hausdorff assigned a (countable) ordinal to the (countable)
scattered orderings (see e.g. [13]), called its rank. In our results, we use a slightly
modiﬁed deﬁnition of the original rank as follows: ﬁnite order types have rank 0 and if
an order type is a ﬁnite sum of f-sums of scattered order types each having a rank less
then a, then its order type is at most a. Formally we can deﬁne for each ordinal a a class
Ha of (scattered, countable) order types as H0 consisting of the ﬁnite order types and
Ha being the smallest class containing each order type of the form
P
j2f1;...;ng
P
i2Z
oj;i with
1 Support of the ITM NKFIA TKP2021 grant is acknowledged.

each oj;i being a member of some Hb with b\a. Then the rank of a (scattered
countable) order type o is the least ordinal a with o 2 Ha. Due to Hausdorff’s theorem,
every scattered order type has a rank. As examples, x, f, xk and xx have ranks 1, 1, k
and x respectively, for the latter one we can write e.g. xx ¼ 1 þ x þ x2 þ x3 þ . . .
which is an x-sum of order types having a ﬁnite rank.
2
Selected Results
It is known [3] that an ordinal is regular if and only if it is less than xx and it is
context-free if and only if it is less than xxx. Also, the rank of any scattered regular
(context-free, resp.) order type is less than x (xx, resp.) [7, 11]. The other reason why
it is interesting to study scattered context-free orderings is that it is decidable whether a
context-free grammar G generates a scattered language [5] while it is undecidable
whether it generates a dense one [6]. For the general case, it is even undecidable
whether the order type of a context-free language is g [6]. However, for scattered
context-free order types we do have some positive results: it is known [10] that the
order type of a well-ordered language generated by a preﬁx grammar (i.e. in which each
nonterminal generates a preﬁx-free language) is computable, thus the isomorphism
problem of context-free ordinals is decidable if the ordinals in question are given as the
lexicograpic ordering of preﬁx grammars. Also, the isomorphism problem of regular
orderings is decidable as well [4, 14]. It is unknown whether the isomorphism problem
of scattered context-free orderings is decidable – a partial result in this direction is that
if the rank of such an ordering is at most one (that is, the order type is a ﬁnite sum of the
terms x, x and 1), then the order type is effectively computable from a context-free
grammar generating the language [8, 9]. Moreover, it is also decidable whether a
context-free grammar generates a scattered language of rank at most one. It is a very
plausible scenario though that the isomorphism problem of scattered context-free
orderings is undecidable in general – the rank 1 is quite low compared to the upper
bound xx of the rank of these orderings, and there is no known structural character-
ization of scattered context-free orderings. Clearly, among the well-orderings, exactly
the ordinals smaller than xxx are context-free but for scattered orderings the main
obstacle is the lack of a ﬁnite “normal form” – as every x-indexed sum of the terms x
and x is scattered of rank two, there are already uncountably many scattered
orderings of rank two and thus only a really small fraction of them can possibly be
context-free. So it makes sense to study language classes lying strictly between the
regular and the context-free languages. One candidate can be that of the deterministic
context-free languages: for these it is known that their order types are exactly the
(general) context-free order types [7].
Another candidate for the next step is the class of the one-counter languages: these
are the ones that can be recognized by a pushdown automaton having only one stack
symbol. In [12], a family of well-ordered languages Lnfa; b; cg was given for each
integer n  0 so that the order type of Ln is xxn (thus its rank is x  n) and Kuske
formulated two conjectures: i) the order type of well-ordered one-counter languages is
On Scattered Context-free Order Types
xiii

strictly less than xx2 and more generally, ii) the rank of scattered one-counter lan-
guages is strictly less than x2. Of course the second conjecture implies the ﬁrst.
In the main part of the presentation we aim to prove this second conjecture.
References
1. Bloom, S.L., Ésik, Z.: Regular and algebraic words and ordinals. In: Mossakowski, T.,
Montanari, U., Haveraaen, M. (eds.) Algebra and Coalgebra in Computer Science, vol. 4624,
pp. 1–15. Springer, Berlin, Heidelberg (2007). 10.1007/978-3-540-73859-6_1
2. Bloom, S.L., Choffrut, C.: Long words: the theory of concatenation and omega-power.
Theor. Comput. Sci. 259(1), 533–548 (2001)
3. Bloom, S.L., Ésik, Z.: Algebraic ordinals. Fundam. Inform. 99(4), 383–407 (2010)
4. Bloom, S.L., Ésik, Z.: The equational theory of regular words. Inform. Comput. 197(1), 55–
89 (2005)
5. Ésik, Z.: Scattered context-free linear orderings. In: Mauri, G., Leporati, A. (eds.) Devel-
opments in Language Theory, vol. 6795, pp. 216–227. Springer, Berlin, Heidelberg (2011).
10.1007/978-3-642-22321-1_19
6. Ésik, Z.: An undecidable property of context-free linear orders. Inform. Process. Lett. 111
(3), 107–109 (2011)
7. Ésik, Z., Iván, S.: Hausdorff rank of scattered context-free linear orders. In: Fernández-Baca,
D. (eds.) LATIN 2012: Theoretical Informatics, vol. 7256, pp. 291–302. Springer, Berlin,
Heidelberg (2012). 10.1007/978-3-642-29344-3_25
8. Gelle, K., Iván, S.: On the order type of scattered context-free orderings. In: The Tenth
International Symposium on Games, Automata, Logics, and Formal Veriﬁcation, 2–3
September 2019, pp. 169–182 (2019)
9. Gelle, K., Iván, S.: The order type of scattered context-free orderings of rank one is com-
putable. In: Alexander, C., et al. (eds.) SOFSEM 2020: Theory and Practice of Computer
Science - 46th International Conference on Current Trends in Theory and Practice of
Informatics, SOFSEM 2020, Limassol, Cyprus, 20–24 January 2020, Proceedings of Lecture
Notes in Computer Science, vol. 12011, pp. 273–284. Springer, Cham (2020). 10.1007/978-
3-030-38919-2_23
10. Gelle, K., Iván, S.: The ordinal generated by an ordinal grammar is computable. Theor.
Comput. Sci. 793, 1–13 (2019)
11. Heilbrunner, S.: An algorithm for the solution of ﬁxed-point equations for inﬁnite words.
RAIRO – Theor. Inform. Appl. 14(2), 131–141 (1980)
12. Kuske, D.: Logical aspects of the lexicographic order on 1-counter languages. In: Chatterjee,
K., Sgall, J. (eds.) Mathematical Foundations of Computer Science 2013 - 38th International
Symposium, MFCS 2013, Klosterneuburg, Austria, 26–30 August 2013. Proceedings, vol.
8087 of Lecture Notes in Computer Science, vol. 8087, pp. 619–630. Springer, Berlin,
Heidelberg (2013). 10.1007/978-3-642-40313-2_55
13. Rosenstein, J.G.: Linear orderings. Pure Appl. Math. (1982)
14. Thomas, W.: On frontiers of regular trees. ITA 20(4), 371–381 (1986)
xiv
S. Iván

Operations on Unambiguous Finite Automata
(Extended Abstract)
Galina Jirásková1
Mathematical Institute, Slovak Academy of Sciences, Grešákova 6, 040 01,
Košice, Slovakia
jiraskov@saske.sk
Abstract. We investigate the complexity of basic regular operations on lan-
guages represented by unambiguous ﬁnite automata. We get tight upper bounds
for intersection (mn), left and right quotients (2m  1), positive closure
3
4  2n  1


, star
3
4  2n


, shufﬂe (2mn  1), and concatenation
3
4  2m þ n  1


.
To describe witnesses, we use a binary alphabet for intersection and left and
right quotients, a ternary alphabet for positive closure and star, a ﬁve-letter
alphabet for shufﬂe, and a seven-letter alphabet for concatenation. We also
discuss some partial results for complementation (between 2logloglogn and
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 1
p
 2n=2) and union (between mn þ m þ n and m þ n 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m þ 1
p
 2m=2 where
m  n).
1
Introduction
A nondeterministic ﬁnite automaton (with multiple initial states, NFA) is unambiguous
(UFA) if it admits at most one accepting computation on every input string. Ambiguity
in ﬁnite automata was ﬁrst considered by Schmidt [15] in his unpublished thesis, where
he developed a lower bound method for the size of unambiguous automata based on the
rank of certain matrices. He also obtained a lower bound of 2Xð ﬃﬃn
p Þ on the conversion of
unambiguous ﬁnite automata into deterministic ﬁnite automata (DFAs).
Leung [10] improved the UFA-to-DFA trade-off to the tight upper bound 2n. He
described, for every n, a binary n-state UFA with a unique initial state whose equiv-
alent DFA requires 2n states. A similar binary example with multiple initial states was
given by Leiss [8], and a ternary one was presented already by Lupanov [11]; notice
that the reverse of Lupanov’s witness for NFA-to-DFA conversion is deterministic.
Using an elaborated Schmidt’s lower bound method, Leung [11] described, for every n,
an n-state NFA, in fact, a DFA with multiple initial states, whose equivalent UFA
requires 2n  1 states.
Stearns and Hunt [17] showed that it can be tested in polynomial time whether or
not a given nondeterministic ﬁnite automaton is unambiguous. They also provided
polynomial-time algorithms for the equivalence and containment problems for unam-
biguous ﬁnite automata.
1 Research supported by VEGA grant 2/0132/19.

Hromkovič et al. [4] further elaborated a lower bound method for UFAs. Using
communication complexity they showed that so-called exact cover of all 1’s with
monochromatic sub-matrices in a communication matrix of a language provides a
lower bound on the size of any UFA for this language, and they simpliﬁed some proofs
presented in [15, 17].
Okhotin [13] examined unambiguous automata over a one-letter alphabet. He
proved that the UFA-to-DFA trade-off in the unary case is given by a function in
eHð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
nðlnnÞ2
3p
Þ, while the NFA-to-UFA trade-off is e
ﬃﬃﬃﬃﬃﬃ
nlnn
p
ð1 þ oð1ÞÞ. He also obtained the
tight upper bound ðn  1Þ2 þ 1 for star, an upper bound mn, tight if m; n are relatively
prime, for concatenation, and a lower bound n2e for complementation of unary
unambiguous automata.
Here we discuss the results on the complexity of basic regular operations on lan-
guages represented by unambiguous ﬁnite automata over an arbitrary alphabet obtained
by Jirásek, Jirásková, and Šebej [6]. To get upper bounds, we provide a construction of
a UFA recognizing the language resulting from an operation. In the case of intersection,
the corresponding product automaton is unambiguous. In all the remaining cases, we
ﬁrst describe a nondeterministic automaton for the resulting language, and then count
the number of its reachable non-empty sets. Such a number provides an upper bound on
the size of an equivalent partial deterministic, so unambiguous, subset automaton.
To get lower bounds, we ﬁrst restate the lower bound method from [10, 15]. To any
NFA N, we assign a matrix MN whose rows are indexed by sets that are reachable in N
and columns by sets that are co-reachable in N, and whose entry ðS; TÞ includes 0 if S
and T are disjoint and it includes 1 otherwise. The rank of such a matrix provides a
lower bound on the number of states in any unambiguous automaton recognizing the
language LðNÞ. Then, using the known fact that the rank of the matrix is 2n  1 if its
rows and columns are indexed by all the non-empty subsets of a set of size n and its
entries are as described above, we get an observation that the number of reachable sets
in any NFA provides a lower bound on the size of any equivalent UFA if all the
non-empty sets are co-reachable in the given NFA.
We use this observation to get lower bounds for quotients, positive closure, shufﬂe,
and concatenation. We describe witness languages in such a way that in an NFA for the
resulting language, all the non-empty sets are co-reachable, and the number of
reachable sets is as large as possible. In the case of our intersection witnesses, the
matrix corresponding to the resulting product automaton is an identity matrix of size
mn, while in the case of star, we must inspect carefully the rank of the corresponding
matrix.
An upper bound on the complexity of complementation of a language represented
by a UFA is given by the number of reachable set in a given UFA, as well as by the
number of its co-reachable sets. We show that the minimum of these two numbers is at
most 20:79n þ logn. This upper bound can be further decreased to
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 1
p
 2n=2 as shown
by Indzhev and Kiefer [5]. A superpolynomial lower bound on the complexity of
complementation on unambiguous automata has been recently obtained by Raskin [14].
xvi
G. Jirásková

2
Preliminaries
We assume that the reader is familiar with basic notions in formal languages and
automata theory. For details and all the unexplained notions, the reader may refer to [3,
16].
A nondeterministic ﬁnite automaton (NFA) is a 5-tuple N ¼ ðQ; R; D; I; FÞ, where
Q is a ﬁnite nonempty set of states, R is a ﬁnite nonempty set of input symbols called
the input alphabet, DQ  R  Q is the transition relation, IQ is the set of initial
states, and FQ is the set of ﬁnal states. Each element ðp; a; qÞ of D is called a
transition of N. A computation of N on an input string a1a2    an is a sequence of
transitions ðq0; a1; q1Þðq1; a2; q2Þ    ðqn1; an; qnÞ 2 D. The computation is accepting
if q0 2 I and qn 2 F; in such a case we say that the string a1a2    an is accepted by N.
The
language
accepted
by
the
NFA
N
is
the
set
of
strings
LðNÞ ¼ fw 2 Rj w is accepted by Ng.
An NFA N ¼ ðQ; R; D; I; FÞ is unambiguous (UFA) if it has at most one accepting
computation on every input string, and it is (partial) deterministic (DFA) if j Ij ¼ 1
and for each state p in Q and each symbol a in R, there is at most one state q in Q such
that ðp; a; qÞ is a transition of N. It follows immediately from the deﬁnition that every
(partial) deterministic automaton is unambiguous.
The transition relation D may be viewed as a function  : Q  R ! 2Q, and it can be
extended to the domain 2Q  R in the natural way. We denote this extended function
by  as well. Then LðNÞ ¼ fw 2 Rj I  w \ F 6¼ ;g.
Every NFA N ¼ ðQ; R; ; I; FÞ can be converted to an equivalent deterministic
automaton DðNÞ ¼ ð2Q; R; ; I; fS 2 2Qj S \ F 6¼ ;gÞ, called the subset automaton of
N [16]. Removing the empty set from the subset automaton results in an equivalent
partial
deterministic,
so
unambiguous,
automaton.
This
gives
the
following
observation.
Proposition 1. Every language accepted by an n-state NFA is recognized by a UFA of
at most 2n  1 states.
□
A subset S of the state set Q of an NFA N ¼ ðQ; R; ; I; FÞ is reachable if S ¼ I  w
for some string w, and it is co-reachable if it is reachable in the reverse of N obtained
from N be reversing all its transitions and by swapping the roles of its initial and ﬁnal
states. Using these notions we get the following characterization of unambiguous
automata.
Proposition 2. A nondeterministic ﬁnite automaton is unambiguous if and only if
j S \ Tj  1 for each reachable set S and each co-reachable set T.
□
If the reverse of an NFA is deterministic, then each co-reachable set in N is of size
one,which gives the next observation.
Proposition 3. An nondeterministic ﬁnite automaton is unambiguous if its reverse is
(partial) deterministic.
□
Now we restate the lower bound method from [10, 15].
Operations on Unambiguous Finite Automata
xvii

Proposition 4 (Lower bound method for UFAs). Let N be an NFA. Let M be the
matrix with rows (columns) indexed by reachable (co-reachable) sets of N, in which
the entry ðS; TÞ includes 0 if S and T are disjoint, and 1 otherwise. Then every UFA
recognizing LðNÞ has at least rankðMÞ states.
Proof. Let A be a minimal n-state unambiguous automaton recognizing LðNÞ. Consider
a matrix M'A whose rows are indexed by the states of A, and columns are indexed by
strings generating the co-reachable sets in N. The entry ðq; wÞ of M'A is 1 if wR is
accepted by A from the state q, and it is 0 otherwise. Since A is unambiguous, for every
column in M'A there is at most one row that contains a 1. It follows that the row of MN
indexed by a set S is a sum of the rows of M'A corresponding to the states in S. Thus
every row of MN
is a linear combination of rows in M'A, and therefore
rankðMNÞ  rankðM'AÞ  n.
□
Let Mn be a matrix with rows and columns indexed by all the non-empty subsets of
a set of size n, and such that the entry ðS; TÞ is 0 if S and T are disjoint, and it is 1
otherwise. Then rankðMnÞ ¼ 2n  1 [9, Lemma 3]. This gives the following corollary.
Proposition 5. If every non-empty set is co-reachable in a nondeterministic ﬁnite
automaton, then the number of its reachable sets provides a lower bound on the
number of states in any equivalent unambiguous automaton.
□
3
Results
Let us start with the trade-offs between deterministic, nondeterministic, and unam-
biguous ﬁnite automata. Every unambiguous automaton of n states can be simulated by
a DFA of at most 2n states obtained by the subset construction. To get tightness,
consider an NFA from from Fig. 1. Since its reverse is deterministic, this NFA is
unambiguous. As shown by Leung [10, Theorem 1], every equivalent DFA has at least
2n states.
Every NFA of n states can be simulated by a partial deterministic, so unambiguous,
subset automaton of at most 2n−1 states. To get tightness of this upper bound, consider
the binary NFA from Fig. 2, a witness for complementation on NFAs from [7, The-
orem 5]. Every non-empty set is reachable in this NFA, and since the reverse of this
1
2
3
4
· · ·
n−1
n
a
a
a
b
b
b
b
b
b
b
a
a
a
Fig. 1. A binary UFA-to-DFA witness meeting the upper bound 2n [10].
xviii
G. Jirásková

NFA is, in fact, the same NFA, every non-empty set is co-reachable as well. Hence
every equivalent UFA has at least 2n−1 states. Moreover, every equivalent DFA has at
least 2n states, since the empty set is reachable in the NFA from Fig. 2. The trade-offs
between these three models of automata are shown in Fig. 3.
Now we continue with operational complexity on languages represented by
unambiguous ﬁnite automata. Table 1 shows the known results on the complexity of
basic regular operations on languages represented by deterministic and nondetermin-
1
2
3
· · ·
n−1
n
a, b
a, b
a, b
a, b
a, b
b
b
b
b
b
b
b
b
Fig. 2. A binary NFA-to-UFA witness meeting the upper bound 2n−1.
DFA
UFA
NFA
2n
2n
2n −1
Fig. 3. The trade-offs between deterministic, nondeterministic, and unambiguous ﬁnite automata.
Table 1. The complexity of regular operations on languages represented by deterministic and
nondeterministic ﬁnite automata [2, 7, 12, 18].
Operation
DFA
R
j j
NFA
R
j j
Reversal
2n
2
n
2
Intersection
mn
2
mn
2
Left quotient
2m  1
2
m þ 1
2
Right quotient
m
1
m
1
Shufﬂe
?
mn
2
Concatenation
m  2n  2n1
2
m þ n
2
Positive closure
3
4  2n  1
2
n
1
Star
3
4  2n
2
n þ 1
1
Complementation
n
1
2n
2
Union
mn
2
m þ n
2
Operations on Unambiguous Finite Automata
xix

istic ﬁnite automata, while Table 2 summarizes the corresponding results for unam-
biguous automata from Jirásek Jr., Jirásková, Šebej [6]. Both tables also display the
size of alphabet used to describe witness languages. Let us discuss the results for UFAs
in more detail.
Reversal. Since the reverse of an unambiguous automaton is unambiguous, the upper
bound is n for the reversal operation. This upper bound is met by a one-string unary
language an1 recognized by an n-state partial deterministic, so unambiguous,
automaton. Its reversal is the same language which cannot be accepted by any non-
deterministic automaton with less than n states.
Intersection. Notice that the product automaton for intersection of two unambiguous
automata is unambiguous. This gives an upper bound mn for the intersection operation.
The binary languages fw 2
a; b
f
g j jwja ¼ m  1g and fw 2
a; b
f
g j jwjb ¼ n 
1g meet this upper bound since in the corresponding product automaton each singleton
set is reachable and co-reachable, and therefore the corresponding matrix is the identity
matrix of size mn.
Left and Right Quotient. The left (right) quotient of a given language is recognized by
a nondeterministic automaton obtained from an automaton for the given language by
changing the set of initial (ﬁnal) states. Applying the subset construction to the
resulting automaton and omitting the empty set results in an incomplete deterministic,
so also unambiguous, automaton for the language resulting from the quotient operation.
This gives the upper bound 2m  1 in both cases. To get witness for left quotient,
consider the partial deterministic, so unambiguous, automaton from Fig. 4 and its left
quotient by the language a recognized by a one-state unambiguous automaton. In the
corresponding nondeterministic automaton for the left quotient, each non-empty set is
reachable and co-reachable; notice that a shifts every subset cyclically by one, and b
eliminates the state m. A similar idea works for the right quotient of the language
recognized by the automaton from Fig. 4 by the empty string. Let us recall that the
Table 2. The complexity of regular operations on languages represented by unambiguous ﬁnite
automata [6].
Operation
UFA
R
j j
Reversal
n
1
Intersection
mn
2
Left quotient
2m  1
2
Right quotient
2m  1
2
Shufﬂe
2mn  1
5
Concatenation
3
4  2m þ n  1
7
Positive closure
3
4  2n  1
3
Star
3
4  2n
3
Complementation
 20:8n
−
Union (m  n)
mn þ m þ n    m þ n  20:8m
4
xx
G. Jirásková

upper bound on the complexity of right quotient on DFAs and NFA is just n since
changing the set of ﬁnal states in any DFA or NFA results in a DFA or NFA,
respectively. However, changing the set of ﬁnal states in an unambiguous automaton
may not be unambiguous.
Shufﬂe. The shufﬂe of two languages represented by UFAs of m and n states is
recognized by an mn-state NFA. This gives an upper bound 2mn  1 for the shufﬂe
operation on unambiguous automata. To describe witnesses, we use a ﬁve-letter
alphabet and consider the languages recognized by partial deterministic, so unam-
biguous, automata shown in Fig. 5; cf.[1]. In the corresponding shufﬂe automaton, each
non-empty set is reachable and co-reachable.
Concatenation. An automaton for the concatenation of two languages can be con-
structed from the corresponding unambiguous automata by adding the e-transition from
every ﬁnal state of the ﬁrst automaton to the initial state of the second automaton. In the
resulting automaton, at least 2m þ n2 set of states are unreachable – those including a
ﬁxed ﬁnal state of the ﬁrst automaton and not including the initial state of the second
automaton. After excluding the empty set, we get an upper bound 3
4  2m þ n  1 for the
concatenation operation. For tightness, we consider the languages recognized by
unambiguous automata shown in Fig. 6 deﬁned over the seven-letter alphabet
1
2
· · ·
m−1
m
a, c
a, c
a, c
a, c
a
d
d, e
d, e
d, e
1
2
· · ·
n−1
n
b, d
b, d
b, d
b, d
b
c
c, e
c, e
c, e
Fig. 5. Quinary witnesses for shufﬂe meeting the upper bound 2mn −1.
1
2
· · ·
m−1
m
a
a
a
a
a
b
b
b
Fig. 4. A binary witness for left quotient (by a*) meeting the upper bound 2m −1.
Operations on Unambiguous Finite Automata
xxi

a; b; c; d; a; b; c
f
g; notice that the reverse of the ﬁrst automaton as well as the second
automaton are deterministic. In the corresponding automaton for concatenation of these
two languages, each non-empty set is co-reachable, while 3
4  2m þ n  1 non-empty sets
are reachable.
Positive Closure. To get an automaton for the positive closure of a regular language
represented by an unambiguous automaton, we only need to add the e-transition from
every ﬁnal state of this automaton to its initial state. In the resulting automaton, each set
of states that contains a ﬁxed ﬁnal state and does not contain the initial state is
unreachable which, after excluding the empty set, gives the upper bound 3
4  2n  1. For
tightness, we consider the binary witness DFA for star from [18], and we add a loop on
a new symbol c in each state, except for the state n  1 to get a ternary partial
deterministic, so unambiguous, automaton shown in Fig. 7. The third symbol guar-
antees the co-reachability of every non-empty subset in the corresponding NFA for
positive closure, while by strings over
a; b
f
g we get the reachability of 3
4  2n  1
non-empty sets.
Star. In the case of the star operation, we need to add a new initial (and ﬁnal) state in
the construction from the previous paragraph which increases the upper bound by one.
1
2
· · ·
m−1
m
a, c
a, b, c
a, b, c
a, b, c
c
α, β, γ, a, b, d
α, β, γ, d
α, β, γ, d
α, β, γ
1
2
· · ·
n−1
n
α
α, β
α, β
α, β
α, β
a, b, c, d, β
a, b, c, d, γ
a, b, c, d, γ
a, b, c, d, γ
Fig. 6. Septenary witnesses for concatenation meeting the bound 3
4  2m þ n  1.
1
2
· · ·
n−2
n−1
n
a
a, b
a, b
a, b
a, b
a, b
b, c
c
c
c
Fig. 7. A ternary witness for positive closure meeting the upper bound 3
4  2n  1.
xxii
G. Jirásková

The witness is the same as for the positive closure, but now we have to inspect carefully
the binary matrix corresponding to the automaton for its star since now we cannot have
the case when all non-empty sets are co-reachable.
Complementation. The complementation operation looks to be really challenging on
unambiguous automata. A lower bound of X n2e
ð
Þ has been obtained by Okhotin [13],
while a superpolynomial lower bound has been recently provided by Raskin [14].
Although we are not able to improve these lower bounds, we can decrease the trivial
upper bound 2n to 20:79n þ logn. The idea of the proof is to observe that given an n-state
unambiguous automaton, the complement of its language is recognized by an unam-
biguous automaton of min R
j
j; Cj j
f
g states, where R and C are the families of reachable
and co-reachable sets in a given UFA, respectively. If the maximum of sizes of
reachable sets is k, then
R
j
j 
n
1


þ
n
2


þ    þ
n
k


Cj j  k þ 1
ð
Þ  2nk
since every co-reachable set may have just one state from a ﬁxed reachable set of
size k. If k  n=2, then Cj j is small enough. Otherwise, R
j
j is upper bounded by an
increasing function r k
ð Þ ¼ n  en
k
 k and Cj j is upper bounded by a decreasing function
function c k
ð Þ ¼ n  2nk, and we show that min r k
ð Þ; c k
ð Þ
f
g is at most 20:79n þ logn.
Recently, Indzhev and Kiefer [5] decreased this upper bound to
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 1
p
 2n=2 by
showing that the size of a UFA for the complemented language recognized by an n-
state unambiguous automaton is upper bounded by the minimum of the number of
cliques and co-cliques (independent sets) of a graph with n vertices, and then by
showing that in every such graph the product of the number of its cliques with the
number of its cocliques is bounded by n þ 1
ð
Þ  2n.
Union. First, notice that the standard NFA for union is unambiguous if two languages
represented by unambiguous automata are disjoint. Without loss of generality, we may
assume that m  n. Since K [ L ¼ K [ L \ Kc
ð
Þ, and the languages K and L \ Kc are
disjoint, we get an upper bound m þ n  20:79m þ logm for union on unambiguous auto-
mata. Taking into account the result from [5], this upper bound can be decreased to
m þ n 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m þ 1
p
 2m=2. To get a lower bound of mn þ m þ n, we consider the quaternary
partial deterministic, so unambiguous, automata with all states ﬁnal such that in the ﬁrst
automaton, the symbol a performs a cyclic permutation, while b maps each state,
except for the initial one, to itself, and c; d perform the identity. In the second
automaton, the symbols a and b perform the identity, while c and d play the same role
as a and b in the ﬁrst automaton. Then, in the NFA for their union, all the non-empty
sets are co-reachable, while mn þ m þ n sets of size one and two are reachable.
Operations on Unambiguous Finite Automata
xxiii

4
Open Problems
In this section we state some problems that remain open in the research of the com-
plexity of regular operations on languages represented by unambiguous automata. The
problem of ﬁnding the exact complexity of complementation seems to be the most
challenging.
Open Problem 1. What is the exact complexity of complementation for unambiguous
automata?
Even some better lower or upper bounds for the complementation operation would
be of interest; recall that the known lower bound is 2logloglogn [14], while the best known
upper bound is
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 1
p
 2n=2 [5]. We used the results for complementation to get an
upper bound for union. Nevertheless, the gap between lower and upper bound is large
in the case of union.
Open Problem 2. What is the complexity of union for unambiguous automata?
Our strategy for ﬁnding a witness for positive closure was to take the binary witness
for the star operation on DFAs, and then deﬁne the transition on one more symbol to
guarantee the co-reachability of all non-empty subsets in an NFA for positive closure.
Perhaps, a new, completely different, witness could be described over a binary
alphabet. A similar question arises in the case of the star operation.
Open Problem 3. What is the complexity of positive closure or star for unambiguous
automata in the binary case?
In the case of shufﬂe and concatenation, our witnesses are deﬁned over a ﬁve-letter
and seven-letter alphabet, respectively. Our aim was to have proofs as simple as
possible in [6], and we did not consider the possibility of decreasing the size of input
alphabet.
Open Problem 4. Can unambiguous witnesses for shufﬂe or concatenation be
described over a smaller alphabet?
The research on the complexity of operations for unambiguous automata [6] was
really interesting, funny, and exciting for all three of us, and we believe that trying to
solve the open problems stated above could be interesting, funny, and exciting as well.
Acknowledgment
I would like to thank my son Jozef and my student Juraj for our Mondays’ seminars
while working on the topic. I really miss them a lot.
References
1. Câmpeanu, C., Salomaa, K., Yu, S.: Tight lower bound for the state complexity of shufﬂe of
regular languages. J. Autom. Lang. Comb. 7(3), 303–310 (2002)
2. Holzer, M., Kutrib, M.: Nondeterministic descriptional complexity of regular languages. Int.
J. Found. Comput. Sci. 14(6), 1087–1102 (2003). 10.1142/S0129054103002199
xxiv
G. Jirásková

3. Hopcroft, J.E., Ullman, J.D.: Introduction to Automata Theory, Languages and Computa-
tion. Addison-Wesley (1979)
4. Hromkovič, J., Seibert, S., Karhumäki, J., Klauck, H., Schnitger, G.: Communication
complexity method for measuring nondeterminism in ﬁnite automata. Inf. Comput. 172(2),
202–217 (2002). 10.1006/inco.2001.3069
5. Indzhev, E., Kiefer, S.: On complementing unambiguous automata and graphs with many
cliques and Cocliques. Inf. Process. Lett. 177 (2022). 10.1016/j.ipl.2022.106270. To apper
6. Jirásek Jr, J., Jirásková, G., Šebej, J.: Operations on unambiguous ﬁnite automata. Int.
J. Found. Comput. Sci. 29(5), 861–876 (2018). 10.1142/S012905411842008X
7. Jirásková, G.: State complexity of some operations on binary regular languages. Theor.
Comput. Sci. 330(2), 287–298 (2005). 10.1016/j.tcs.2004.04.011
8. Leiss, E.L.: Succint representation of regular languages by Boolean automata. Theor.
Comput. Sci. 13, 323–330 (1981). 10.1016/S0304-3975(81)80005-9
9. Leung, H.: Separating exponentially ambiguous ﬁnite automata from polynomially
ambiguous ﬁnite automata. SIAM J. Comput. 27(4), 1073–1082 (1998). 10.1137/
S0097539793252092
10. Leung, H.: Descriptional complexity of NFA of different ambiguity. Int. J. Found. Comput.
Sci. 16(5), 975–984 (2005). 10.1142/S0129054105003418
11. Lupanov, O.B.: Uber den vergleich zweier typen endlicher quellen. Probleme der Kybernetik
6, 328–335 (1966)
12. Maslov, A.N.: Estimates of the number of states of ﬁnite automata. Soviet Math. Doklady
11, 1373–1375 (1970)
13. Okhotin, A.: Unambiguous ﬁnite automata over a unary alphabet. Inf. Comput. 212, 15–36
(2012), 10.1016/j.ic.2012.01.003
14. Raskin, M.A.: A superpolynomial lower bound for the size of non-deterministic complement
of an unambiguous automaton. In: Chatzigiannakis, I., Kaklamanis, C., Marx, D., Sannella,
D. (eds.) ICALP 2018. LIPICS, vol. 107, pp. 138:1–138:11. Schloss Dagstuhl -
Leibniz-Zentrum für Informatik (2018). 10.4230/LIPIcs.ICALP.2018.138
15. Schmidt, E.M.: Succinctness of description of context-free, regular, and ﬁnite languages. Ph.
D. thesis. Cornell University (1978)
16. Sipser, M.: Introduction to the theory of computation. Cengage Learn. (2013)
17. Stearns, R.E., Hunt, H.B.: On the equivalence and containment problems for unambiguous
regular expressions, regular grammars and ﬁnite automata. SIAM J. Comput. 14(3), 598–611
(1985). 10.1137/0214044
18. Yu, S., Zhuang, Q., Salomaa, K.: The state complexities of some basic operations on regular
languages. Theor. Comput. Sci. 125(2), 315–328 (1994). 10.1016/0304-3975(92)00011-F
Operations on Unambiguous Finite Automata
xxv

Contents
The Alphabetic Complexity in Homomorphic Definitions of Word,
Tree and Picture Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Stefano Crespi Reghizzi
Ranking Binary Unlabelled Necklaces in Polynomial Time . . . . . . . . . . . . .
15
Duncan Adamson
On the Power of Recursive Word-Functions Without Concatenation . . . . . . .
30
Jérôme Durand-Lose
Clusters of Repetition Roots Forming Prefix Chains . . . . . . . . . . . . . . . . . .
43
Szilárd Zsolt Fazekas and Robert Mercaş
Nearly k-Universal Words - Investigating a Part of Simon’s Congruence . . . .
57
Pamela Fleischmann, Lukas Haschke, Annika Huch, Annika Mayrock,
and Dirk Nowotka
State Complexity of Binary Coded Regular Languages . . . . . . . . . . . . . . . .
72
Viliam Geffert, Dominika Pališínová, and Alexander Szabari
Reset Complexity and Completely Reachable Automata with Simple
Idempotents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
Stefan Hoffmann
On the Descriptional Complexity of the Direct Product of
Finite Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
Markus Holzer and Christian Rauch
Operations on Subregular Languages and Nondeterministic State
Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
Michal Hospodár, Peter Mlynárčik, and Viktor Olejár
On Simon’s Congruence Closure of a String . . . . . . . . . . . . . . . . . . . . . . .
127
Sungmin Kim, Yo-Sub Han, Sang-Ki Ko, and Kai Salomaa
Approximate NFA Universality Motivated by Information Theory. . . . . . . . .
142
Stavros Konstantinidis, Mitja Mastnak, Nelma Moreira,
and Rogério Reis
Lazy Regular Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
Orna Kupferman and Asaf Petruschka

State Complexity of Finite Partial Languages . . . . . . . . . . . . . . . . . . . . . . .
170
Martin Kutrib and Matthias Wendlandt
Yet Another Canonical Nondeterministic Automaton . . . . . . . . . . . . . . . . . .
184
Hendrik Maarand and Hellis Tamm
Union-Complexities of Kleene Plus Operation . . . . . . . . . . . . . . . . . . . . . .
197
Benedek Nagy
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
xxviii
Contents

The Alphabetic Complexity in Homomorphic
Deﬁnitions of Word, Tree and Picture Languages
Stefano Crespi Reghizzi(B)
DEIB and CNR - IEIIT, Politecnico di Milano, Milan, Italy
stefano.crespireghizzi@polimi.it
Abstract. The Medvedev’s Theorem (MT) characterizes a regular language as the
projection of a local, i.e., a strictly-locally-testable language of order k = 2 (2-slt),
over an alphabet larger than the terminal one by a factor depending on the state
complexityoftheﬁniteautomaton(FA).MTwaslatergeneralizedtootherlanguage
domains that instead of words contain trees or rectangular pictures, namely the
regular tree languages and the tiling-system recognizable (TS-rec) languages. For
trees and pictures the notion of local testability based on the occurrence of digram
(2-factors) in a word, is changed into a suitable neighborhood of size 2, resp. a
tree of height one, or a two-by-two tile, to be generically called 2-grams- A more
recent MT extension goes in the direction of enlarging the neighborhood using as
generators the languages, characterized by the k-grams, k > 2, and called k-slt;
of course the k-gram types are different for words, trees and pictures. For all three
domains a remarkably similar Extended Medvedev’s Theorem (EMT) holds: a
word/tree/picture language R over a terminal alphabet Σ is regular/regular/TS-rec
if there exists a k-slt language L over an alphabet Λ of size double of Σ, and a
letter-to-letter homomorphism from Σ to Λ such that R is the image of L; the
value of k is in O(lg(n)), n being the state set size of an automaton recognizing the
word/tree language R (the tiling system alphabet size for pictures). The alphabetic
ratio |Λ|/|Σ| of EMT is thus two, against the value n of MT. For some languages
the value two of the ratio is the minimal possible. We present a new simpliﬁed
proof of EMT for words and hints to the similar proofs for trees and for pictures.
The central idea, say for words, is to sample a run of the FA so that the states
traversed every k transition steps are evidenced; then each state is encoded by
means of a comma-free binary code-word of length k; such an encoding is known
to be a 2k-slt language. For trees the same idea requires to lay the code-word bits
over the root-to-leaf tree paths. For pictures 2D comma-free codes are needed. The
possibility of further generalizations of EMT is raised at the end.
1
Introduction
Of the several approaches available to deﬁne formal language families, the homomor-
phic characterization of interest here essentially says that a language belongs to a certain
family, if, and only if, it is the homomorphic image of a language belonging to a sim-
pler sub-family. Since it would be difﬁcult to be more precise without ﬁxing a language
family, we start with a classic case: Medvedev’s theorem [15,18] (MT) for regular word
languages. The theorem says that a language R over a terminal alphabet Σ is regular if,
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 1–14, 2022.
https://doi.org/10.1007/978-3-031-13257-5_1

2
S. Crespi Reghizzi
and only if, there exists a local language L over another alphabet Λ and a letter-to-letter
homomorphism ϑ : Σ∗→Λ∗such that R = ϑ(L). Therefore each word x ∈R is the
image ϑ(y) of a word y in Λ, also called a pre-image of x.
We recall that a local language is a strictly locally testable [14] (slt) language of
testability order k = 2, in short a 2-slt language; the k-slt families form a strict hier-
archy with respect to inclusion. The notion of local testability is the pivot of all the
developments reported here, also for tree languages and for picture languages. For an
slt language the validity of a word can be decided by a series of local tests: by moving
on the word a sliding window of width k and collecting the window contents, i.e., the
k-factors present in the word, and then by checking that they are included in a given set
called a test set, where also the preﬁxes and sufﬁxes are speciﬁed as such.
For the other two families, the k-factors, become respectively certain ﬁnite pieces
of trees and of pictures, that we may call k-grams (k-tiles for pictures).
The local alphabet in the proof [18] of MT is Λ = Σ × Q, where Q is the state
set of a ﬁnite-state automaton (FA) recognizing language R. We prefer to say that the
alphabetic ratio |Λ|
|Σ| of MT is |Q|, to evidence how the local alphabet size depends on
the state complexity of the language.
It is known that homomorphic characterizations `a la Medvedev exist also for lan-
guages comprising entities more complex than words, when a suitable deﬁnition of
locality and strict local testability is possible. This is the case of the regular tree lan-
guages (e.g. in [3,9]) and of the picture languages deﬁned by the tiling systems [10,11]
(TS-rec), for brevity simply called tree languages and picture languages.
The question addressed in a series of studies concerns the reduction of the alphabetic
ratio that can be achieved in a Medvedev’s theorem if the local language is replaced by
a k-slt language, where k ≥2. The series started with a result [5] for regular word
languages, further developed in [6], then similar results where obtained for the regular
tree languages [7], and lastly for the picture languages deﬁned by the tiling systems [4].
In all such cases a similar Extended Medvedev’s Theorem (EMT) with alphabetic ratio
2 was proved.
The EMT instance for words says that a word language is regular if, and only if,
it is the image of a k-slt language of alphabetic ratio 2. Moreover the value of k is in
O(lg(|Q|)) where Q is the state set of a ﬁnite-state automaton (FA) recognizing the
language. The value two is the minimal possible for some regular languages, no matter
how large a k is taken. Such an EMT is a new property that carries over from regular
word languages to tree languages and to picture languages.
Since it would be too long to cover the word, tree and picture cases at the same
level of detail, we concentrate on the basic case of words for which we provide a new
simpler proof. The other two cases cannot be discussed here and their speciﬁc aspects
are intuitively presented in a later section.
EMT for Words. It may be surprising at ﬁrst that the EMT property holds for three very
different language domains, but a deep justiﬁcation comes from the common approach
used in all the three proofs. We explain it, referring to the regular word languages over
an alphabet Σ. The chief aspects present in the proof are:
(1) The focus in the MT proof [18] on the recognizing runs performed by an FA, rep-
resented as state-labeled paths over the local alphabet Λ = Σ × Q. The projection
on Σ are the recognized words.

The Alphabetic Complexity
3
(2) The encoding of the states on the run by means of comma-free code-words [2,12].
Actually not all the states are encoded but only those that occur k −1 transition
steps apart from each other. This means that the states are taken with a ﬁxed sam-
pling rate k ≥2. The value of k determines the code-word length, hence the code
dictionary numerosity that must sufﬁce to encode the states.
(3) The self-synchronization property of comma-free codes permits to decode such an
encoded run using a 2k-slt machine, which is the same as saying that the successful
encoded runs belong to an slt language of order 2k. The known results on the
numerosity of binary comma-free codes of length k permit to prove that k is in
O(lg(|Q|)).
We raise a technical point that has to do with the words (and also with trees and
pictures) which are (i) smaller than k or (ii) of a size that is not multiple of k. Case (i)
is easily dismissed as in the classic theory of k-slt word languages, by stipulating that
any words shorter than k belonging to the language are simply united to the language
deﬁned by the k-slt test. On the other hand, in case (ii) a language may comprise an
inﬁnite number of words that cannot be encoded (as in item (2) above) with a uniform
comma-free code X ⊂{0, 1}k since X just contains code-words of ﬁxed-length k. The
solution we adopt here transforms the language into a padded version by appending to
every word not in (Σk)+ up to k−1 new symbols ($) so that all words have the required
length for encoding. EMT is proved initially for the padded language, obtaining the k-
slt test set. The test set is then pruned from the $’s and adjusted to construct the ﬁnal
test set.
Paper Organization. Section 2 contains the basic deﬁnitions for word languages and
for comma-free codes, and some preliminary properties. Section 3 contains a result on
the minimal alphabetic ratio, the proof of EMT for the regular word languages, and a
complete example. Section 4 brieﬂy describes the cases of tree languages and picture
languages by comparing them with the case of words. The Conclusion Sect. 5 indicates
a possible generalization.
2
Basic Deﬁnitions and Properties
2.1
Regular and Strictly Locally Testable Word Languages
For brevity, we omit the classical deﬁnitions for language and automata theory and just
list our notations. The empty word is denoted ε. The Greek upper-case letters Γ, Δ, Θ, Λ
and Σ denote ﬁnite terminal alphabets.
The i-th letter of a word x is x(i), 1 ≤i ≤|x|, i.e., x = x(1)x(2) . . . x(|x|). The
character # is not present in the alphabets, and is used as word delimiter to mark the
start and end of a word, but it is not counted as true input symbol. A homomorphism
ξ : Λ∗→Σ∗is letter-to-letter if for every b ∈Λ, ξ(b) is in Σ; we only use letter-to-
letter homomorphisms.
A ﬁnite automaton (FA) A is deﬁned by a 5-tuple (Σ, Q, →, I, F) where Q is the
set of states, →is the state-transition relation (or graph) →⊆Q × Σ × Q; I and F are
resp. the nonempty subsets of Q comprising the initial and ﬁnal states. If (q, a, q′) ∈→,

4
S. Crespi Reghizzi
we write q
a
−→q′. The transitive closure of →is deﬁned as usual, e.g., we also write
q
x
−→q′ with x ∈Σ+ with obvious meaning. If q ∈I and q′ ∈F, then the word x is
recognized by A.
Strictly Locally Testable Languages. There are different yet asymptotically equivalent
deﬁnitions of the family of strictly locally testable (slt) languages [14]; the following
deﬁnition is based on delimited words. Notice also that in the deﬁnition and throughout
the paper we disregard for simplicity a ﬁnite number of short words that may be present
in the language. The next notation is useful: given an alphabet Λ and for all k ≥2, let
Λk
# = #Λk−1 ∪Λk ∪Λk−1#. Thus the set Λk
# includes all the words of length k over
Λ and all the words of length k −1 bordered on the left or on the right by #. For all
words x, |x| ≥k, let Fk(x) ⊆Λk
# be the set of factors of length k (k-factors) present
in #x#. The deﬁnition of Fk is extended to languages as usual.
Deﬁnition 1 (Strict local testability). A language L ⊆Γ ∗is k-strictly locally testable
(k-slt), if there exist a set Mk ⊆Γ k
# such that, for every word x ∈Γ ∗, x is in L if, and
only if, Fk(x) ⊆Mk. Then, we write L = L(Mk), and we call Mk the test set of L.
A language is slt if it is k-slt for some value k, which is called the testability order. A
forbidden factor of Mk is a word in Γ k
# −Mk.
The order k = 2 yields the family of local languages. The k-slt languages form an
inﬁnite hierarchy under inclusion, ordered by k.
2.2
Comma-Free Codes
A ﬁnite set or dictionary X ⊂Δ+ is a code [2] if every word in Δ+ has at most one
factorization (i.e., decoding) in words of X, also known as code-words. We assume that
Δ is the binary alphabet {0, 1}. We use a code X to represent a ﬁnite alphabet Γ by
means of a one-to-one homomorphism, denoted by  X : Γ ∗→Δ∗, called encoding,
such that αX ∈X for every α ∈Γ.
The family of codes we use is named comma-free [2,12] because the code-words
in a text are not separated by a reserved character (the “comma”). Let k ≥1. A code
dictionary X ⊂Δk is comma-free [12], if, intuitively, no code-word overlaps the con-
catenation of two code-words, more precisely:
X2 ∩Δ+XΔ+ = ∅i.e., ∀t, u, v, w ∈Δ∗if tu, uv, vw ∈X then

u = w = ε∨
t = v = ε
.
(1)
An example of comma-free code dictionary is X = {0010, 0011, 1110}.
Numerosity of Comma-Free Code Dictionaries
Proposition 1. For every m ≥2, there exist k ≥2 and a comma-free code X ⊂Δk
such that |X| ≥m, with k ∈O(lg m).
Proof. Assume without loss of generality that m is a power of 2, i.e., m = 2h for
some h ≥1. Let k be any prime number between 2h and 4h, which always exists

The Alphabetic Complexity
5
by Bertrand-Chebyshev theorem; hence, k is in O(lg m). From [8] (as a special case
of Theorem 2 pag. 267) it is known that for every prime integer k > 1 there is a
comma-free code of length k with 2k−2
k
code-words, whence the following inequality:
2k−2
k
≥22h−2
2h
≥22h−1
2h . Then, this value is at least as large as m if 22h−1
2h
≥2h, i.e., if
2h−1 ≥2h which is true for every h ≥2.
⊓⊔
Comma-Free Codes and slt Languages. To prepare for later proofs, we state a funda-
mental relation between comma-free codes and slt languages. Given an alphabet Λ and
a comma-free dictionary X that encodes the symbols of Λ, let L ⊆Λ∗be a local lan-
guage. Then the encoding of L, i.e., the language LX ⊆(Δk)∗is slt. Such result is
known and derives from early studies on local parsability [13,17].
Theorem 1 (preservation of slt by encoding). Let L ⊆Γ + be the 2-slt language
L(M2) deﬁned by a test set M2 ⊆Γ 2
#. The encoding of L by means of a comma-free
code X of length k, i.e., the language LX ⊆(Δk)∗, is a 2k-slt language.
In the special case when L = Γ Γ + the encoding XX+ is the (2k)-slt language deﬁned
by the set F2k(XX+) of the factors of length 2k of #X X+#.
3
The Extended Medvedev’s Theorem for Words
Theorem 2 (Medvedev’s Theor. for words (see e.g. [18])). A language R ⊆Σ∗is
regular if, and only if, there exist a local language L ⊆Λ∗and a letter-to-letter homo-
morphism ϑ : Λ∗→Σ∗such that R = ϑ(L). If R is recognized by an FA with state set
Q the alphabet is Λ = Σ × Q.
Thus, each element of Λ can be written as
a→q, intuitively meaning that from some
state an a labeled arc goes to state q. We call alphabetic ratio the quotient |Λ|
|Σ|. Thus the
alphabetic ratio of the MT statement is |Q|. A natural question is whether, by relaxing
the condition that language L is local and permitting it to be k-slt with k > 2, the
alphabetic ratio of such an extended Medvedev’s statement may be reduced and how
much. First, we prove with a simple witness that in general, no matter how large k, the
alphabetic ratio cannot be smaller than two.
Theorem 3 (minimality of alphabetic ratio two [5]). For every alphabet Σ, there
exists a regular language R ⊆Σ+ such that for every k ≥2, R is not the homomorphic
image of a k-slt language L ⊆Λ, with |Λ| = (2 · |Σ| −1).
Proof. Let R = 
a∈Σ (aa)∗. By contradiction, assume that there exist k ≥2 and a
local alphabet Λ of cardinality 2˙|Σ| −1, a mapping π : Λ →Σ and a k-slt language
L ⊆Λ+ such that π(L) = R. Since |Λ| = 2 · |Σ| −1, there exists at least one element
of Σ, say, a, such that there is only one symbol b ∈Λ with π(b) = a. Since a2k ∈R,
there exists x ∈L such that π(x) = a2k. By deﬁnition of π and of Λ, x = b2k. Consider
the word xb = b2k+1. Clearly, π(xb) = a2k+1, which is not in R, since all words in R
have even length. Hence, xb ̸∈L. But the k-factors of #x# coincide with the k-factors
of #xb# therefore xb is in L, a contradiction.
⊓⊔

6
S. Crespi Reghizzi
In other words, some regular languages cannot be generated as images of an slt lan-
guage, if the alphabetic ratio is too small. The remaining question whether an alphabetic
ratio of two is sufﬁcient was positively settled in [5].1 The proof presented here is sim-
pler than the original one and is based on the use of comma-free codes (as already in [6]
where local functions are used instead of homomrphisms), combined with a convenient
padding technique, already in [4] for picture languages.
Sampled Runs. Referring to Theorem 2, given k ≥2, reorganize each computation as
follows. Starting in the initial state, group together every k consecutive steps, until the
computation ends in a ﬁnal state or h < k residual steps are left before the end; in the
second case, group together all the h steps. Such a representation is called a run with
sampling rate k schematized as:
y1
→qi1
y2
→qi2 . . .
yn−1
→qin−1
z→qin, yi ∈Σk, z ∈Σh, (1 ≤h ≤k), q ∈Q, qin ∈F.
Now, interpret each symbol such as
y2
→qi2 in the run as an element of a ﬁnite set called
sampling alphabet, Λk = Σk × Q ∪Σh × F (1 ≤h ≤k) where F ⊆Q are the
ﬁnal states. Thus, a sampled run is a word over Λk. To illustrate, consider for the FA in
Example 1 the sampled run recognizing a5b:
aaaa
→q0·
ab
→q2
From Theorem 2 the following is obvious: (i) the projection on alphabet Σ of a
sampled run is exactly the word recognized by the corresponding run of FA A; and (ii)
the language of sampled runs is local.
Padding to a Multiple of the Sampling Rate. To prove EMT we will encode the states
visible in the sampled run, using binary comma-free code-words of length k. In Exam-
ple 1 see at item 3. the encoding of the states by code-words of length 4. Thus the
concatenation of the code-words q1 q0 is a an 8 bit string, against an input word
aaaaab of length just 6, too short to assign one bit per input character when encoding
the states visible in the run. Since it would be complicated to use code-words of variable
length, we prefer to stretch the last symbol of a sampled run, in the example
ab
→q2. We
append to it as many symbols $ (assumed not in Σ) as needed to obtain a length equal
to the sampling rate. We call padded such modiﬁed sampled runs and from now on we
only deal with them. In our case the padded sampled run is
aaaa
→q0
ab$$
→q2.
Deﬁnition 2 (sampled runs). The sampling alphabet (with padding) is Λk$ ⊆Σk ×
Q ∪

Σh · $k−h
× F where 1 ≤h < k. A sampled run is
y1
→qi1
y2
→qi2 . . .
yn−1
→qin−1
z→qin where
⎧
⎨
⎩
q0
y1
→qi1,
yi ∈Σk, z ∈Σh · $k−h,
all q ∈Q and qin ∈F.
(2)
The following proposition is immediate.
1 A similar construction in [19] (proof of Theorem 5.2) is used to logically characterize regular
languages; it may provide an alternative proof that the alphabetic ratio of EMT is two, though
with the k ∈O(|Q|) bound.

The Alphabetic Complexity
7
Proposition 2 (language of sampled runs). Let R ⊆Σ∗be the language recognized
by an FA A. Let k ≥2 be the sampling rate and Λk$ the sampling alphabet with
padding. A word x is in R if, and only if, A has a sampled run y ∈Λk$
+ such that the
projection of y on Σ is equal to x. The language of sampled runs L ⊆(Λk$)+ is local.
Merged Comma-Free Code-Words. It is convenient to introduce a binary operator that
merges two words of identical length into one of the same length on the Cartesian
product of the alphabets. Deﬁne the operator ⊗: Δ+ × Σ+ →(Δ × Σ)+ for any two
words y ∈Σ+, u ∈Δ+, with |y| = |u|, as: y ⊗u = (y(1), u(1), ) . . . (y(|y|, u(|y|))).
E.g., if y = aabb and u = 0101 then y ⊗u = (a, 0)(a, 1)(b, 0)(b, 1). The operator can
be extended in the usual way to a pair of languages. We also need the projections, resp.
denoted by [ ]Σ and [ ]Δ onto the alphabets Σ and Δ i.e., [u ⊗y]Σ = y, [u ⊗y]Δ = u.
Proposition 3 (merged comma-free code). If X ⊂Δk is a comma-free code of length
k, then every subset Z ⊆Σk ⊗X is also a comma-free code of length k.
Proof. We prove that Z satisﬁes the deﬁnition of comma-free code in Sect. 2.2, Eq. (1).
Let t, u, v, w ∈(Σ×Δ)∗be such that tu, uv, vw ∈Z. By deﬁnition of Z, [tu]Δ, [uv]Δ,
[vw]Δ ∈X, with [u]Δ = [w]Δ = ε or [t]Δ = [v]Δ = ε since X is a comma-free code;
by deﬁnition of ⊗, it must be that also u = w = ε or t = v = ε.
⊓⊔
The EMT for words (Theorem 8 of [5]) is now straightforward to prove.
Theorem 4 (Extended Medvedev’s theorem for words). For any regular language
R ⊆Σ∗, there exist an slt language L ⊆Λ∗, where Λ is an alphabet of size |Λ| = 2|Σ|,
and a letter-to letter homomorphism ϑ : Λ∗→Σ∗, such that R = ϑ(L).
If R is recognized by an FA with |Q| states, the language L is k-slt with k in O(lg(|Q|)).
Proof. Let R = L(A) where A = (Σ, Q, →, q0, F). Let k ∈O(lg |Q|) be a value
such that by Proposition 1 there is a comma-free dictionary X with |X| = |Q|. With
reference to Deﬁnition 2, let L ⊆Λ+
k$ be the 2-slt language of the sampled (padded)
runs of A. Deﬁne the comma-free code Z = Λk$ ⊗X, and apply this encoding to
L, obtaining the language LZ. Notice that it exclusively contains (padded) words of
length multiple of k. The language LZ is 2k-slt by Theorem 1. Denote with M2k the
test set such that LZ = L(M2k).
The next transformation of M2k eliminates or modiﬁes the 2k-factors containing one
or more $’s in order to clean LZ from the padding symbols.
(1) Remove from set M2k any 2k-factor that contains as substring ($, β)($, γ) or
($, β)#, β, γ ∈{0, 1}.
(2) At last, replace any occurrence of ($, β) with # (dropping the bit β).
Let M ′
2k be the resulting set. Clearly, L(M ′
2k) ⊆L(M2k). Since it is obvious that
[L(M ′
2k)]Σ ⊆R, it remains to prove [L(M ′
2k)]Σ ⊇R.
By contradiction, assume that a sampled run α of A is such that αZ is not in
L(M ′
2k).
Let x be the projection of α on Σ. If |x| is a multiple of k, all the 2k-factors
of αZ are free from $ symbols, therefore they are all preserved in M ′
2k since they

8
S. Crespi Reghizzi
are untouched by the steps 1. and 2. above. If |x| is not a multiple of k, α termi-
nates with a symbol of Λk$ having the form
u($)k−h
→
q with q ∈F and u ∈Σh,
1 ≤h < k. For brevity we discuss the case α = . . .
y→q′
c($)k−1
→
q where
y = a1a2 . . . ak ∈Σk and c ∈Σ. The encoded run αZ has therefore the form
. . . a1β1 a2β2 . . . akβk cγ1 $γ2 . . . $γk−1 where each Greek letter stands for a bit. The
2k-factor a2β3 . . . akβk cγ1 $γ2 occurs in αZ, it is in M2k and after step (2) above
becomes a2β3 . . . akβk cγ1 # ∈M ′
2k. Since all dollar-less 2k-factors are untouched,
αZ ∈L(M ′
2k).
⊓⊔
Example 1. The example illustrates the constructions used in the proof of EMT applied
to a case simple enough to ﬁt in the paper.
1. Finite automaton:
q0
q1
q2
A = →
→
a
a
b
R = a(aa)∗b
2. Sampled padded runs with sampling rate k = 4 (see Deﬁnition 2)
– Alphabet Λ4$ =
	aaaa
→q0,
aaab
→q2,
ab$$
→q2

and two sampled runs:
word
sampled run
a5b
aaaa
→q0·
ab$$
→q2
a11b
aaaa
→q0·
aaaa
→q0·
ab$$
→q2
(3)
– The language L of sampled runs is local (Proposition 2) and deﬁned by the test
set:
M2 =
⎧
⎪
⎨
⎪
⎩
#
aaaa
→q0, #
aaab
→q2, #
ab$$
→q2,
aaaa
→q0
aaaa
→q0,
aaaa
→q0
aaab
→q2,
aaaa
→q0
ab$$
→q2,
aaab
→q2 #,
ab$$
→q2 #
⎫
⎪
⎬
⎪
⎭
3. Comma-free dictionary X and encoding of FA states:
q0X q1X q2X
0001 0011 0111
4. The merged comma-free dictionary Z = Λ4
4$ ⊗X of Proposition 3 is:
aaaa ⊗q0X
aaab ⊗q2X
ab$$ ⊗q2X
a0 · a0 · a0 · 1a a0 · a1 · 1a · b1 a0 · b1 · $1 · $1
5. Apply the encoding . . .Z : (Λ4$)+ →Z+ to the sampled runs, obtaining the
language LZ, which is 8-slt by Theorem 1. It is deﬁned by a test set M8 that
contains the 8-factors occurring in the runs at line (3):
# a0 a0 a0 a1 a0 a0 a0 a1 a0 a1 a1 b1 #
# a0 a0 a0 a1 a0 b1 $1 $1 #
6. To obtain the ﬁnal test set M ′
8, transform the set M8 as follows:
– The 8-factors a0 a0 a0 a1 a0 b1 $1 $1 and a0 a0 a1 a0 b1 $1 $1 # contain
two $ and are canceled.
– The8-factora1 a0 a0 a0 a1 a0 b1 $1isreplacedbya1 a0 a0 a0 a1 a0 b1 #.
The test set is M ′
8 =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
# a0 a0 a0 a1 a0 a0 a0, a0 a0 a0 a1 a0 a0 a0 a1,
a0 a0 a1 a0 a0 a0 a1 a0, a0 a1 a0 a0 a0 a1 a0 a1,
a1 a0 a0 a0 a1 a0 a1 a1, a0 a0 a0 a1 a0 a1 a1 b1,
a0 a0 a1 a0 a1 a1 b1 #,
# a0 a0 a0 a1 a0 b1 #
⎫
⎪
⎪
⎬
⎪
⎪
⎭
.

The Alphabetic Complexity
9
7. The projection of language L(M ′
8) on Σ is R −{ab, aaab}.
Notice that this language admits a more economical ad hoc EMT statement with alpha-
betic ratio 3
2, represented by the projection on {a, b} of the 2-slt language a(a′ a)∗b.
4
The Extended Medvedev’s Theorem for Trees and Pictures
For comparability sake, the same presentation scheme is used in both cases: (i) the def-
inition of the language family, (ii) the notion of k-gram, (iii) the Medvedev’s theorem,
(iv) the extended Medvedev’s theorem.
4.1
Tree Languages
The ranked tree languages are recognized by nondeterministic root-to-leaves tree
automata [3,9] (TA), assumed to be familiar to the reader. Given a tree with inter-
nal nodes labeled over the ranked alphabet Σ, and leaves labeled over Σ0 ⊂Σ, the
machine starts in an initial state in the root, then it computes in one step the states of
the sibling nodes. Then recursively it does the same for each sibling subtree, until the
computation reaches a leaf. The state must be a ﬁnal one in all leaves for the tree to
be recognized. Thus the effect of the computation run is to label each node with a state
from the state set Q.
The analogy with the runs of an FA on words is manifest; the difference is that
an FA run traverses a linear graph whereas a TA run traverses a tree graph along all
the root-to-leaf paths. The result is a state-labeled tree, isomorphic to the original one,
where nodes are labeled over the alphabet Σ × Q.
The notion of k-gram, k ≥2, requires some preliminary concepts. A tree domain
D is a ﬁnite, non empty, preﬁx-closed subset of N∗
>0 satisfying the following condition:
if xi ∈D for x ∈N∗
>0 and i ∈N>0, then xj ∈D for all j with 1 ≤j ≤i. A tree
t over a ﬁnite alphabet Σ is a mapping t : domt →Σ, where domt is a tree domain.
A node of t is an element x of domt. The root of t is the node ε. The successors of a
node x are all the nodes of the form xi, with xi ∈domt, i ∈N>0. The yield or frontier
of a tree is the word of leaf labels read from left to right. A path is a sequence of nodes
x1 . . . xm, such that xi+1 is a successor of xi. The label of a path x1 . . . xm is the word
t(x1) · · · t(xm), i.e., the concatenation of the labels of nodes x1, . . . , xm.
Given a node x ∈domt, the portion of the tree rooted at x is denoted as t|x and
called the subtree of t at node x, i.e., t|x is the subset of nodes of domt having the
form xy, y ∈N∗
>0. A subtree t|x is not formally a tree but it can be made into a tree
t′ by removing the preﬁx x from every node of t|x, by positing t′(y) = t|x(xy) for all
y ∈N∗
>0; in this case, the subtree is said to be normalized.
Tree Languages Deﬁned by Local Tests
Deﬁnition 3 (k-gram). Let k ≥2, let t ∈TΔ and x a node of t. The k-gram2 of t
at node x ∈domt is the subset of nodes of the normalized subtree t|x at downwards
2 The meaning of digram is a sequence of 2 letters or symbols; or also of patterns such as the
colors in a ﬂag. For non-textual languages the term k-gram is preferable to k-factor. Other
terms are in use, e.g., our deﬁnition of k-gram corresponds to the (k −1)-type of [16].

10
S. Crespi Reghizzi
distance less than k from x. When x = ε (i.e., the root of t) the k-gram is called a root
k-gram. The set of k-grams of t is denoted by ⟪t⟫h.
Note: the yield of a k-gram, unlike the one of a tree, may include symbols in Σ −Σ0.
Deﬁnition 4 (strictly locally testable tree language). Let k ≥2. A language T ⊆TΣ
is k-strictly locally testable (k-slt) if there exist two sets Γk and Θk of k-grams, called
the test sets, with Θk ⊆Γk, such that the membership of a tree in T can be decided
just by considering its k-grams, namely, for all t ∈TΣ: t ∈T if, and only if, ⟪t⟫k ⊆
Γk
and the root k-gram of t is in Θk. Then we write T = T(Γk, Θk).The value k is
called the order of T. A language is strictly locally testable (slt) if it is k-slt for some
k ≥2; if k = 2 it is also called local.
Two examples of local language are: the state-labeled trees, denoted by 
T(M), of a
language T(M) ⊆TΣ, and the syntax trees of a context-free grammar.
Medvedev’s Theorem and Its Extension The next well-known proposition (e.g., in [9],
Sect. 2.8) is a Medvedev-like characterization of tree languages.
Theorem 5 (MT for trees). A tree language T ⊆TΣ is regular if, and only if, there
exists a ranked alphabet Λ, a local (i.e. 2-slt) tree language T ′ ⊆TΛ and a projection
η : Λ →Σ such that T = η(T ′). Moreover, if a tree automaton recognizing T has the
state set Q, then the alphabetic ratio is |Λ|
|Σ| ≤|Q|.
The proof of the theorem is based on the observation that the set of all state-labeled trees
of a TA M is a local tree language, since a transition of M operates on the neighborhood
of a node consisting of its children.
The EMT (Theorem 4) for words and the minimality of the alphabetic ratio 2 (The-
orem 3) have been recently proved for tree languages [7]; the mimality result simply
comes from the fact that a word is also a linear tree. therefore the same witness holds in
both cases.
Theorem 6 (EMT for trees (theorem 1 of [7])). For every ranked alphabet Σ, there
exist a ranked alphabet Λ, with alphabetic ratio |Λ|
|Σ| ≤2, and a projection η : TΛ →
TΣ, such that for every regular tree language T ⊆TΣ there exist k ≥2 and a 2k-slt
tree language T ⊆TΛ such that T = η( T).
To explain the proof of EMT for trees, we look at a state-labeled tree, and we consider
each root-to-leaf path. On each path we encode, with binary comma-free code-words
of length k, the states that are located at nodes at distance 0, k, 2k, . . . from the root.
Notice that, for any internal node at a distance multiple of k from the root, the same
code-word is placed on all the downward paths originating from it. Paths too short to
contain a whole code-word, will contain just a preﬁx. Having placed the binary code-
words on the path, we cancel all state labels that where present in the state-labeled tree.
The result is a tree isomorphic to the state-labeled one, aptly called an encoding tree.
As said, its node labels are over the alphabet Σ × {0, 1}. Analogously with Theorem 1
about the preservation of the slt property by encoding, it is possible to prove that the
language of encoded trees is 2k-slt, and its projection on Σ coincides with language T.

The Alphabetic Complexity
11
4.2
Picture Languages
The case of TS-rec pictures sets itself apart from the cases of words and trees since the
primary deﬁnition of the tiling-system recognizable picture languages is not based on
an automaton (or on a regular expression) but on Medvedev’s theorem.
Assuming some familiarity with the subject, we list a few essential deﬁnitions
(see [10,11]). A picture p is a rectangular array with |p|row rows and |p|col columns,
each cell containing a symbol (or pixel) from an alphabet Σ. The set of all pictures of
size m, n is Σm,n and the set of all pictures is Σ++. A picture contained in another
one is a subpicture. A (square) picture in Σk,k, k ≥2, is called k-tile and simply tile
if k = 2: k-tiles play the role of k-factors for words and k-grams for trees. The bor-
dered version p of p is the picture of size (|p|row + 2, |p|col+2 that surrounds p with a
rectangular frame containing the reserved symbol #.
Deﬁnition 5 (strictly locally testable picture languages). A picture language, L ⊆
(Σ ∪{#})++ is k-strictly-locally testable (k-slt) if there is a set Tk ⊆(Σ ∪{#})k,k
of k-tiles, called the test set such that p ∈L if, and only if, the k-tiles occurring in p as
subpictures are included in Tk. Then we write L = L(Tk). A pictures language is slt if
it is k-slt for some k.
The k-slt, k ≥2, family is an inﬁnite strict hierarchy, for every non-unary alphabet.
As said, the deﬁnition of TS-rec is an MT statement.
Theorem 7 (MT for pictures). A picture language R ⊆Σ++ is tiling-system recog-
nizable (TS-rec) if it is the projection of a 2-slt (i.e., local) language L ⊆Λ++, i.e.,
deﬁned by a test set T2 ⊆(Σ ∪{#})2,2. In formula, R = π(L(T2)) where π : Λ →Σ.
The quadruple (Σ, Λ, T2, π) is called a tiling system.
Tradeoff Between Alphabet Cardinality and Tile Size. The deﬁnition of tiling system
has been extended towards the k-tiling system. It uses a set Tk ⊆Γ k,k of k-tiles, k ≥2
instead of 2-tiles. The alphabetic ratio of a k-tiling system is the quotient |Γ |
|Σ|.
Theorem 8. Given a k-slt language L ⊆Σ++ deﬁned as L = L(Tk) where Tk ⊆
(Σ ∪{#})k,k, there exists an alphabet Γ, a local language L′ ⊆Γ ++ and a projection
π : Γ →Σ s.t. L = π(L′). Hence the family of k-TS recognizable languages coincides
with TS-rec.
The proof in [10] has the alphabet size |Γ| = |Σ| · |Tk|.
Extended Medvedev’s Theorem. The EMT (Theorem 4) for words and the minimality
of the alphabetic ratio 2 (Theorem 3) have been recently proved for TS-rec pictures [4];
the minimality simply follows from the fact that a word is also a one-row picture.
Theorem 9 (EMT for pictures). For any R ⊆Σ++ in TS-rec there exist k ≥2 and a
2k-tiling system with alphabetic ratio 2, recognizing R. Moreover, if n is the size of the
local alphabet of a tiling system recognizing R then the value of k is O(lg(n)).
The proof follows the one for words in Sect. 3 with some important differences.

12
S. Crespi Reghizzi
1. If the number of rows or columns is not multiple of k, the picture has to be padded
(as we did for words in Sect. 3) on the east and south sides with $ symbols, so that
the padded picture can be tessellated with k-tiles, that ﬁt in a mesh of a k × k grid.
2. A comma-free picture (i.e., 2D) code is, intuitively, a set of k-tiles (code-pictures)
such that, for any picture tessellated with such tiles, none of the k × k subpictures at
positions misaligned with respect to the grid, can be a code-picture. The slt proper-
ties of such 2D codes and for code-words are similar.
The property of code-words that XX+ is 2k-slt (special case of Theorem 1)
becomes: let X ⊆Λk,k be a comma-free code, then the language X++ is 2k-slt.
Instead of Theorem 1, the statement is: let T ⊆Γ 2,2 be a set of tiles deﬁning the
local language L(T) and let X ⊆Δk,k be a comma-free 2D code with numerosity
|X| = |Γ|. The encoding L(T)X is a 2k-slt language.
Although a general formula for the numerosity of 2D comma-free codes is lacking,
in [1] a useful lower bound for a family of codes that cannot overlap is given; the
non-overlapping condition is stronger than the comma-free one.
3. The major difference is that for pictures we cannot rely on an automaton analogous
to FA for words and TA for trees.
Therefore the technique in Sect. 3 of sampled
computations labeled with the states traversed has
to be replaced by another approach. The frame
f(p) of a picture p ∈Γ k,k is the square ring com-
posed by the four sides (np, ep, sp, wp) each one
being a word of length k; each corner is shared by
two words. A bordered picture of size (2k, 2k) tes-
sellated with four k-tiles each one with its frame,
is shown.
# . . .
. . . # # . . .
. . . #
#
a nx
ny b #
. . .
. . .
wx
u ex wy v
ey
. . .
. . .
#
sx
sy
#
# nz
nt #
. . .
. . .
wz
w ez wt z
et
. . .
. . .
#
c
sz
st d #
# . . .
. . . # # . . .
. . . #
4. A comma-free code-picture encodes each frame, i.e., a quadruple (np, ep, sp, wp);
code-pictures are schematized by u, v, w, z in the ﬁgure. The frame contains 4k sym-
bols of Γ and can be encoded by a code-picture in X, which contains k2 bits, since
for k large enough, the numerosity of the family of non-overlapping 2D codes [1]
sufﬁces to encode all possible frames.
5. In each pixel the original terminal symbol from Σ is accompanied by a bit of the
code-picture, so that the alphabetic ratio is |Σ|·|{0,1}|
|Σ|
= 2.
6. The language of encoded pictures is 2k-slt, hence language R is 2k-TS recognizable
with alphabetic ratio 2. The proof is more combinatorial than for words.
5
Conclusion
For words, trees and pictures we have evidenced the similarity between the statements
and proofs of the Extended Medvedev’s Theorem. At closer reﬂection, we may attribute
such similarity to the fact that in all cases a recognizing computation sweeps over a
discrete structure, a directed graph whose nodes are labeled with terminal symbols and
states. The graph is respectively a total order, a tree order, and an acyclic graph with
square meshes. The computation never returns to an already visited node. The sampling
technique with sampling rate k clusters the computation into k-grams. Such k-grams

The Alphabetic Complexity
13
are then taken as symbols of a new alphabet and the correct adjacencies between k-
grams are speciﬁed by a 2-slt language. Then the encoding of each k-gram symbol by
means of a binary comma-free code transforms the 2-slt language into a 2k-slt language
over the doubled alphabet Σ × {0, 1}.
An open question is whether the EMT property holds for other language domains
beyond the three considered, as for instance the directed-acyclic-graph automata that
from time to time have been proposed in the literature.
Acknowledgment. The present simpler proof of EMT for words has been worked out jointly
with P. San Pietro and incorporates an original idea of A. Restivo about padding.
References
1. Anselmo, M., Giammarresi, D., Madonia, M.: Non-expandable non-overlapping sets of pic-
tures. Theor. Comput. Sci. 657, 127–136 (2017)
2. Berstel, J., Perrin, D., Reutenauer, C.: Codes and Automata. Encyclopedia of Mathematics
and its Applications, vol. 129. CUP (2009)
3. Comon, H., et al.: Tree automata techniques and applications (2007). http://www.grappa.
univ-lille3.fr/tata
4. Crespi Reghizzi, S., Restivo, A., San Pietro, P.: Reducing local alphabet size in recognizable
picture languages. In: Moreira, N., Reis, R. (eds.) DLT 2021. LNCS, vol. 12811, pp. 103–
116. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-81508-0 9
5. Crespi-Reghizzi, S., San Pietro, P.: From regular to strictly locally testable languages. Int. J.
Found. Comput. Sci. 23(8), 1711–1728 (2012)
6. Crespi Reghizzi, S., San Pietro, P.: Regular languages as local functions with small alpha-
bets. In: ´Ciri´c, M., Droste, M., Pin, J.´E. (eds.) CAI 2019. LNCS, vol. 11545, pp. 124–137.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-21363-3 11
7. Crespi Reghizzi, S., San Pietro, P.: Homomorphic characterization of tree languages based
on comma-free encoding. In: Leporati, A., Mart´ın-Vide, C., Shapira, D., Zandron, C. (eds.)
LATA 2021. LNCS, vol. 12638, pp. 241–254. Springer, Cham (2021). https://doi.org/10.
1007/978-3-030-68195-1 19
8. Eastman, W.L.: On the construction of comma-free codes. IEEE Trans. Inf. Theory 11(2),
263–267 (1965)
9. Gecseg, F., Steinby, M.: Tree Automata. arXiv (2015)
10. Giammarresi, D., Restivo, A.: Recognizable picture languages. Int. J. Pattern Recogn. Artif.
Intell. 6(2–3), 241–256 (1992)
11. Giammarresi, D., Restivo, A.: Two-dimensional languages. In: Rozenberg, G., Salomaa, A.
(eds.) Handbook of Formal Languages, pp. 215–267. Springer, Heidelberg (1997). https://
doi.org/10.1007/978-3-642-59126-6 4
12. Golomb, S.W., Gordon, B., Welch, L.: Comma-free codes. Can. J. Math. 10, 202–209 (1958)
13. Hashiguchi, K., Honda, N.: Properties of code events and homomorphisms over regular
events. J. Comput. Syst. Sci. 12(3), 352–367 (1976)
14. McNaughton, R., Papert, S.: Counter-Free Automata. MIT Press, Cambridge (1971)
15. Medvedev, Y.T.: On the class of events representable in a ﬁnite automaton. In: Moore, E.F.
(ed.) Sequential Machines - Selected Papers, pp. 215–227. Addison-Wesley (1964). Origi-
nally Published in Russian in Avtomaty, pp. 385–401 (1956)
16. Place, T., Segouﬁn, L.: A decidable characterization of locally testable tree languages. Log.
Methods Comput. Sci. 7(4) (2011)

14
S. Crespi Reghizzi
17. Restivo, A.: On a question of McNaughton and Papert. Inf. Control 25(1), 93–101 (1974)
18. Eilenberg, S.: Automata, Languages, and Machines, vol. A. Academic Press (1974)
19. Thomas, W.: Classifying regular events in symbolic logic. J. Comput. Syst. Sci. 25(3), 360–
376 (1982)

Ranking Binary Unlabelled Necklaces
in Polynomial Time
Duncan Adamson(B)
Reykjavik University, Reykjavik, Iceland
duncana@ru.is
Abstract. Unlabelled Necklaces are an equivalence class of cyclic words
under both the rotation (cyclic shift) and the relabelling operations. The
relabelling of a word is a bijective mapping from the alphabet to itself.
The main result of the paper is the ﬁrst polynomial-time algorithm for
ranking unlabelled necklaces of a binary alphabet. The time-complexity
of the algorithm is O(n6 log2 n), where n is the length of the considered
necklaces. The key part of the algorithm is to compute the rank of any
word with respect to the set of unlabelled necklaces by ﬁnding three other
ranks: the rank over all necklaces, the rank over symmetric unlabelled
necklaces, and the rank over necklaces with an enclosing labelling. The
last two concepts are introduced in this paper.
1
Introduction
For classes of words under lexicographic (or dictionary) order, a unique integer
can be assigned to every word corresponding to the number of words smaller
than it. Such an integer is called the rank of a word. The ranking problem asks
to compute the rank of a given word. Ranking has been studied for various
objects including partitions [13], permutations [9,10], combinations [12], etc.
The ranking problem is straightforward for the set of all words over a ﬁnite
alphabet (assuming the standard lexicographic order), however this ceases to be
the case once additional symmetry is introduced. One such example is combina-
torial necklaces [6]. A necklace, also known as a cyclic word, is an equivalence
class of all words under the cyclic rotation operation, also known as a cyclic
shift. Necklaces are classical combinatorial objects and they remain an object of
study in other contexts such as total search problems [4] or circular splicing sys-
tems [3]. The ﬁrst class of cyclic words to be ranked were Lyndon words - ﬁxed
length aperiodic cyclic words - by Kociumaka et al. [7] who provided an O(n3)
time algorithm, where n is the length of the word. An algorithm for ranking
necklaces - ﬁxed length cyclic words - was given by Kopparty et al. [8], without
tight bounds on the complexity. A quadratic algorithm for ranking necklaces was
provided by Sawada et al. [11]. More recently algorithms have been presented
for ranking multidimensional necklaces [1] and bracelets [2].
Our Results. This paper presents the ﬁrst polynomial time algorithm for rank-
ing binary unlabelled necklaces. Informally, binary unlabelled necklaces can be
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 15–29, 2022.
https://doi.org/10.1007/978-3-031-13257-5_2

16
D. Adamson
though of as necklaces over a binary alphabet with the additional symmetry over
the relabelling operation, a bijection from the set of symbols to itself. Considered
in terms of binary values, the words 0001 and 1110 are equivalent under the rela-
belling operation, however 1010 and 1100 are not. We provide an O(n6 log2 n)
time algorithm for ranking an unlabelled binary necklace within the set of unla-
belled binary necklaces of length n.
2
Preliminaries
Let Σ be a ﬁnite alphabet. For the remainder of this work we assume Σ to be
{0, 1} where 0 < 1. We denote by Σ∗the set of all words over Σ and by Σn
the set of all words of length n. The notation ¯w is used to clearly denote that
the variable ¯w is a word. The length of a word ¯w ∈Σ∗is denoted by | ¯w|. We
use ¯wi, for any i ∈{1, . . . , | ¯w|} to denote the ith symbol of ¯w. Given two words
¯w, ¯u ∈Σ∗, the concatenation operation is denoted by ¯w : ¯u, returning the word
of length | ¯w| + |¯u| where ( ¯w : ¯u)i equals either ¯wi, if i ≤| ¯w| or ¯ui−| ¯
w| if i > | ¯w|.
The tth power of a word ¯w, denoted by ¯wt, equals ¯w repeated t times.
Let [n] be the ordered sequence of integers from 1 to n inclusive and let
[i, j] be the ordered sequence of integers from i to j inclusive. Given two words
¯u, ¯v ∈Σ∗, ¯u = ¯v if and only if |¯u| = |¯v| and ¯ui = ¯vi for every i ∈[|¯u|]. A
word ¯u is lexicographically smaller than ¯v if there exists an i ∈[|¯u|] such that
¯u1¯u2 . . . ¯ui−1 = ¯v1¯v2 . . . ¯vi−1 and ¯ui < ¯vi. Given two words ¯v, ¯w ∈Σ∗where
|¯v| ̸= | ¯w|, ¯v is smaller than ¯w if ¯v| ¯
w| < ¯w|¯v| or ¯v| ¯
w| = ¯w|¯v| and |¯v| < | ¯w|. For a
given set of words S, the rank of ¯v with respect to S is the number of words in
S that are smaller than ¯v.
The subword of a cyclic word ¯w ∈Σn denoted ¯w[i,j] is the word ¯u of length
n + j −i + 1 mod n such that ¯ua = ¯wi+a mod n, i.e. the word such that the ath
symbol of ¯u corresponds to the symbol at position i + a mod n of ¯w. The value
of the tth symbol of ¯w[i,j] is the value of the symbol at position i + t −1 of ¯w.
By this deﬁnition, given ¯u = ¯w[i,j], the value of ¯ut is the i + t −1th symbol of ¯w
and the length of ¯u is |¯u| = j −i + 1. The notation ¯u ⊑¯w denotes that ¯u is a
subword of ¯w. Further, ¯u ⊑i ¯w denotes that ¯u is a subword of ¯w of length i.
The rotation of a word ¯w ∈Σn by r ∈[0, n −1] returns the word ¯w[r+1,n] :
¯w[1,r], and is denoted by ⟨¯w⟩r, i.e. ⟨¯w1 ¯w2 . . . ¯wn⟩r = ¯wr+1 . . . ¯wn ¯w1 . . . ¯wr. Under
the rotation operation, the word ¯u is equivalent to the word ¯v if ¯v = ⟨¯u⟩r for
some r. A word ¯w is periodic if there is a subword ¯u ⊑¯w and integer t ≥2
such that ¯ut = ¯w. Equivalently, word ¯w is periodic if there exists some rotation
0 < r < | ¯w| where ¯w = ⟨¯w⟩r. A word is aperiodic if it is not periodic. The period
of a word ¯w is the aperiodic word ¯u such that ¯w = ¯ut.
A necklace is an equivalence class of words under the rotation operation. The
notation ˜w is used to denote that the variable ˜w is a necklace. Given a necklace
˜w, the canonical representation of ˜w is the lexicographically smallest element of
the set of words in the equivalence class ˜w. The canonical representation of ˜w is
denoted by ⟨˜w⟩, and the rth shift of the canonical representation is denoted by
⟨˜w⟩r. Given a word ¯w, ⟨¯w⟩denotes the canonical representation of the necklace

Ranking Binary Unlabelled Necklaces in Polynomial Time
17
containing ¯w, i.e. the canonical representation of the necklace ˜u where ¯w ∈˜u.
The set of necklaces of length n over an alphabet of size q is denoted by N n
q .
Let ¯w ∈N n
q denote that the word ¯w is the canonical representation of some
necklace ˜w ∈N n
q . An aperiodic necklace, known as a Lyndon word, is a necklace
representing the equivalence class of some aperiodic word. Note that if a word is
aperiodic, then every rotation of the word is also aperiodic. The set of Lyndon
words of length n over an alphabet of size q is denoted by Ln
q .
As both necklaces and Lyndon words are classical objects, there are many
fundamental results regarding each objects. The ﬁrst results for these objects
were equations determining the number of necklaces or Lyndon words of a
given length. The number of (1D) necklaces is given by the equation |N n
q | =
1
n

d|n
φ
 n
d

qd where φ(n) is Euler’s totient function [6]. Similarly the number of
Lyndon words is given with the equation |Ln
q | = 
d|n
μ
 n
d

|N d
q |, where μ(x) is the
M¨obius function [6]. The rank of a word ¯w in the set of necklaces N n
q is the
number of necklaces with a canonical representation smaller than ¯w.
2.1
Unlabelled Necklaces
An unlabelled necklace is a generalisation of the set of necklaces. At a high level,
two words ¯v, ¯u ∈Σn belong to the same unlabelled necklace class ˜w if there
exists some labelling function ψ(x) : Σ 	→Σ and rotation r ∈[n] such that
(⟨¯v⟩r)i = ψ(¯ui) for every i ∈[n]. More formally, let ψ(x) be a bijection from
Σ into Σ, i.e. a function taking as input some symbol in Σ and returning a
symbol in Σ such that {ψ(x)|∀x ∈Σ} = Σ. For notation ψ( ¯w) is used to denote
the word constructed by applying ψ(x) to every symbol in ¯w in order, formally
ψ( ¯w) = ψ( ¯w1)ψ( ¯w2) . . . ψ( ¯wn). Similarly, the notation ψ( ˜w) is used to denote the
necklace class constructed by applying ψ( ¯w) to every word ¯w ∈˜w. Further, let
Ψ(Σ) be the set of all such functions. The unlabelled necklace ˜w with a canonical
representation ¯w contains every word ¯v ∈Σn where ψ(⟨¯v⟩r) = ¯w for some
ψ(x) ∈Ψ(Σ) and r ∈[n]. As in the labelled case, the canonical representation
of an unlabelled necklace ˜w, denoted ⟨˜w⟩, is the lexicographically smallest word
in the equivalence class. The set of unlabelled q-arry necklaces of length n is
denoted ˆ
N n
q , and the set of q-arry Lyndon words of length n ˆLn
q .
In this paper we study binary unlabelled necklaces, in other words unlabelled
necklaces restricted to a binary alphabet. In this case Σ = {0, 1} and Ψ(Σ)
contains the identity function I(x), where I(x) = x, and the swapping function
S(x) where S(x) =

0
x = 1
1
x = 0. Gilbert and Riordan [5] provide the following
equations for computing the sizes of ˆ
N n
2 and ˆLn
2:

18
D. Adamson
| ˆ
N n
2 | =

odd d|n
φ(d)2n/d
| ˆLn
2| =

odd d|n
μ(d)2n/d
In this paper we introduce two subclasses of unlabelled necklaces, the class of
symmetric unlabelled necklaces and the class of enclosing unlabelled necklaces for
some given word ¯w. Observe that a binary unlabelled necklace ˜w may correspond
to either one or two (labelled) necklaces. Informally, a symmetric unlabelled
necklaces is such an unlabelled necklaces that corresponds to only a single neck-
lace. An enclosing unlabelled necklace relative to a word ¯w is a non-symteric
unlabelled necklace corresponding to a pair of necklaces ˜v and ˜u such that
˜v < ¯w < ˜u. Any Lyndon word that is a symmetric unlabelled necklace is a sym-
metric unlabelled Lyndon word, and any unlabelled Lyndon word that encloses
a word ¯w is an enclosing unlabelled Lyndon word of ¯w.
Deﬁnition 1 (Symmetric Necklaces). A binary necklace ˜w is symmetric if
and only if ˜w = S( ˜w).
Deﬁnition 2 (Enclosing Unlabelled Necklaces). An unlabelled necklace ˜u
encloses a word ¯w if ⟨˜u⟩< ¯w < ⟨S(˜u)⟩. An unlabelled necklace ˜u is an enclosing
unlabelled necklace of ¯w if ˜u encloses ¯w.
2.2
Bounding Subwords
One important tool that is used in the ranking of unlabelled necklaces are bound-
ing subwords, introduced in [2]. Informally, bounding subwords of length l ≤n
provide a means to partition Σl into n + 2 sets based on the subwords of some
¯w ∈Σn of length l. Given two subwords ¯v, ¯u ⊑l ¯w such that ¯v < ¯u the set
S(¯v, ¯u) contains all words in Σl that are between the value of ¯v and ¯u, formally
S(¯v, ¯u) = {¯x ∈Σl|¯v ≤¯x < ¯u}. In this paper we are only interested in sets
between pairs ¯v, ¯u ⊑l ¯w where there exists no ¯s ⊑l ¯w such that ¯v < ¯s < ¯u. As
such, we deﬁne a subword of ¯w as bounding some word ¯v if it is the lexicograph-
ically largest subword of ¯w that is smaller than ¯v.
Deﬁnition 3 (Bounding Subwords). Let ¯w, ¯v ∈Σ∗where | ¯w| ≤|¯v|. The
word ¯w is bounded (resp. strictly bounded) by ¯s ⊑| ¯
w| ¯v if ¯s ≤¯w (resp. ¯s < ¯w)
and there is no ¯u ⊑| ¯
w| ¯v such that ¯s < ¯u ≤¯w.
Proposition 1 ([2]). Let ¯v ∈Σn. The array WX[¯s ⊑¯v, x ∈Σ], such that
WX[¯s, x] strictly bounds ¯w : x for every ¯w strictly bounded by ¯s, can be computed
in O(k · n3 · log(n)) time where |Σ| = k.
For the remainder of this paper, we can assume that the array WX has been
precomputed for every ¯s ⊑¯v, x ∈Σ. Note that in our case k = 2, therefore the
process of computing WX requires only O(n3 · log(n)) time.

Ranking Binary Unlabelled Necklaces in Polynomial Time
19
3
Ranking
In this section we present our ranking algorithm. For the remainder of this
section, we assume that we are ranking the word ¯w that is the canonical repre-
sentation of the binary unlabelled necklace ˜w. We ﬁrst provide an overview of
the main idea behind our ranking algorithm.
Theorem 1. Let RankAN( ¯w, m) be the rank of the word ¯w ∈Σn within the
set of non-symmetric unlabelled necklaces of length n that do not enclose ¯w,
let RankSN( ¯w, m) be the rank of ¯w within the set of symmetric necklaces of
length m and let RankEN( ¯w, m) be the rank of ¯w within the set of necklaces
of length m that enclose ¯w. The rank of any necklace ˜w represented by the
word ¯w within the set of binary unlabelled necklaces of length m is given by
RankAN( ¯w, m) + RankSN( ¯w, m) + RankEN( ¯w, m). Further the rank can be
found in O(n6 log2 n) time for any m ≤n.
Proof. Observe that every unlabelled necklace must be one of the above classes.
Therefore the rank of ¯w within the set of all binary unlabelled necklaces of length
m is given by RankAN( ¯w, m) + RankSN( ¯w, m) + RankEN( ¯w, m). Lemma 1
shows that the rank of ¯w within the set of non-symmetric unlabelled necklaces
of length m that do not enclose ¯w can be found in O(n6 log2(n)) time. Theorem
2 shows that the rank of ¯w within the set of symmetric necklaces can be found
in O(n6 log2 n) time. Theorem 3 shows that the rank of ¯w within the set of
necklaces enclosing ¯w can be found in O(n6 log n) time.
Lemma 1. Let RankAN( ¯w, m) be the rank of
¯w within the set of non-
symmetric unlabelled necklaces of length m that do not enclose ¯w, and let
RankN( ¯w, m) be the rank of ¯w within the set of all necklaces of length m. Then
RankAN( ¯w, m) = (RankN( ¯w, m)−RankSN( ¯w, m)−RankEN( ¯w, m))/2. Fur-
ther, this rank can be found in O(n6 log2 n) time for any m ≤n.
Proof. Note that any asymmetric unlabelled necklace appears exactly twice
in the set of necklaces smaller than ¯w. Further, any enclosing or symmetric
necklace appears exactly once in the same set. Therefore RankAN( ¯w, m) =
RankN( ¯
w,m)−RankSN( ¯
w,m)−RankEN( ¯w,m)
2
. As the value of RankN( ¯w, m) can be
found in O(n2) time using the algorithm due to Sawada and Williams [11], the
value of RankSN( ¯w, m) found in O(n6 log2 n) time from Theorem 2, and the
of RankEN( ¯w, m) found in O(n6 log n) time from Theorem 3, the total time
complexity is O(n6 log2 n).
4
Symmetric Necklaces
In this section we show how to rank a word ¯w within the set of symmetric neck-
laces of length m. Before presenting our computational tools, we ﬁrst introduce
the key theoretical results that form the basis for our ranking approach. The key
observation is that any symmetric necklace ˜v must have a period of length 2 · r
where r is the smallest rotation such that ⟨˜v⟩r = S(⟨˜v⟩). This is formally proven
in Proposition 2, and restated in Observation 1 in terms of Lyndon words.

20
D. Adamson
Proposition 2. A necklace ˜w represented by the word ¯w ∈Σn is symmetric if
and only if there exists some r ∈[n] s.t. ¯wi = S( ¯wi+r mod n) for every i ∈[n].
Further, the period of ¯w equals 2 · r where r ∈[n] is the smallest rotation such
that ⟨¯w⟩r = S( ¯w).
Proof. As ˜w is symmetric, S( ¯w) must belong to the necklace class ˜w. There-
fore, there must be some rotation r such that ⟨¯w⟩r = S( ¯w). We now claim
that r ≤
n
2 . Assume for the sake of contradiction that r >
n
2 . Then ¯wi =
S( ¯wi+r mod n) = ¯wi+2r mod n = . . . = ¯wi+2·k·r mod n = S( ¯wi+(2·k+1)r mod n). As
r >
n
2 this sequence must imply that either ¯wi = S( ¯wi), an obvious contra-
diction, or that there exists some smaller value p = GCD(n, r) ≤n
2 such that
¯wi = S( ¯wi+p mod n). Further, ¯w must have a period of at most 2 · r.
Assume now that r is the smallest rotation such that ⟨¯w⟩r = S( ¯w) and
for the sake of contradiction further assume that the period of ¯w is p < r.
Then, as ¯wi = ¯wi+p mod n for every i ∈[n], ¯wi+r mod n = ¯wi+r−p mod n, hence
¯wi = S( ¯wi+r−p mod n), contradicting the initial assumption. The period can not
be equal to the value of r as by deﬁnition ¯wi = S( ¯wi+r mod n). Assume now
that the period p of ¯w is between r and 2 · r. As ¯wi = ¯wi+c·p+2k·r mod n for
every c, k ∈N and i ∈[n]. Further both r and p must be less than n
2 . Therefore
¯wi = ¯wi+((n/p)−1)p+2·r mod n =
¯
i + 2 · r −p mod n and hence ¯w is periodic in
2 · r −p. As p > r, 2 · r −p < r, however as no such period can exist, this leads
to a contradiction. Therefore, 2 · r is the smallest period of ¯w.
Lemma 2. Let RA( ¯w, m, S, r) contain the set of words belonging to an sym-
metric necklace smaller than ¯w such that ¯vi = S(¯vi+r mod m) for every ¯v ∈
RA( ¯w, m, S, r). Further let RB( ¯w, m, S, r) ⊆RA( ¯w, m, S, r) contain the set of
words belonging to an symmetric Lyndon word smaller than ¯w such that r is the
smallest value for which ¯vi = S(¯vi+r mod m) for every ¯v ∈RB( ¯w, m, S, r). The
size of RB( ¯w, m, S, r) is given by:
|RB( ¯w, m, S, r)| =

p|r
μ
m
p

|RA( ¯w, m, S, p)|
Proof. Observe that every word in RA( ¯w, m, S, r) must have a unique period
which is a factor of 2 · r. Therefore, the size of RA( ¯w, m, S, r) can be expressed
as 
d|r
|RB( ¯w, m, S, r)|. Applying the M¨obius inversion formula to this equation
gives |RB( ¯w, m, S, r)| = 
p|r
μ

m
p

|RA( ¯w, m, S, p)|.
Observation 1. Observe that any symmetric Lyndon word ˜v must have length
2 · r, where r is the smallest rotation such that ⟨¯v⟩r = S(⟨¯v⟩).
Lemma 3. Let RankSL( ¯w, 2 · r) be the rank of ¯w within the set of sym-
metric Lyndon words of length 2 · r. The value of RankSL( ¯w, r) is given by
|RB( ¯
w,2·r,S,r)|
2·r
.

Ranking Binary Unlabelled Necklaces in Polynomial Time
21
Proof. Observe that any symmetric Lyndon word has exactly 2 · r unique trans-
lations. Further, as any word in RB( ¯w, 2·r, S, r) must correspond to an aperiodic
word, following Observation 1, the size of RB( ¯w, 2 · r, S, r) can be used to ﬁnd
RankSL( ¯w, 2 ˙r) by dividing the cardinality of RB( ¯w, 2 · r, S, r) by 2 · r.
Lemma 4. Let RankSN( ¯w, m, r) be the rank of ¯w within the set of symmet-
ric necklaces of length m such that for each such necklace ˜v, r is the smallest
rotation such that ⟨˜v⟩r = S(⟨˜v⟩). The value of RankSN( ¯w, m, r) is given by

d|2r
RankSL( ¯w, d).
Proof. Following the same arguments as in Lemma 2, observe that every neck-
lace counted by RankSN( ¯w, m, r) must have a period that is a factor of 2 · r.
Therefore, the value of RankSN( ¯w, m, r) is given by 
d|2r
RankSL( ¯w, d).
Lemma 5. Let RankSN( ¯w, m) be the rank of ¯w within the set of symmetric
necklaces of length m and let RankSN( ¯w, m, r) be the rank of ¯w within the set
of symmetric necklaces of length m such that for each such necklace ˜v, r is the
smallest rotation such that ⟨˜v⟩r = S(⟨˜v⟩). The value of RankSN( ¯w, m) is given
by

r|(m/2)
RankSN( ¯w, m, r).
Proof. Observe that every necklace counted by RankSN( ¯w, m) must have a
unique translation that is the minimal translation under which it is symmetric.
Further this translation must be a factor of
m
2 . Therefore RankSN( ¯w, m) =

r|(m/2)
RankSN( ¯w, m, r).
Following
Lemmas
2,
3,
4,
and
5
the
main
challenge
in
computing
RankSN( ¯w, m) is computing the size of RA( ¯w, m, S, r). In order to do so,
RA( ¯w, m, S, r) is partitioned into two sets, α( ¯w, r, j) and β( ¯w, r, j) where j ∈[r].
Let ¯v be some arbitrary word in the set RA( ¯w, m, S, r). The set α( ¯w, r, j) con-
tains the word ¯v if j is the smallest rotation under which ⟨¯v⟩j ≤¯w. The set
β( ¯w, r, j) contains ¯v if j is the smallest rotation under which ⟨¯v⟩j ≤¯w and
⟨¯v⟩t > ¯w for every t ∈[r + 1, 2 · r]. Note that by this deﬁnition, β( ¯w, r, j) ⊆
α( ¯w, r, j).
Observation 2. Given any word ¯v ∈RA( ¯w, m, S, r) such that ¯v /∈α( ¯w, r, j)
for any j ∈[r], there exists some j′ ∈[r] for which ⟨¯v⟩r ∈β( ¯w, r, j′).
Proof. As ¯v ∈RA( ¯w, m, S, r), there must be some rotation t such that ⟨¯v⟩t < ¯w.
As ¯v /∈α( ¯w, r, j), t must be greater than r. Therefore, ⟨¯v⟩r must belong to
β( ¯w, r, t −r) conﬁrming the observation.
Observation 3. For any ¯v ∈β( ¯w, r, j), ⟨¯v⟩r /∈α( ¯w, r, j′) for any j′ ∈[r].
Proof. As ¯v ∈β( ¯w, r, j), for any rotation t > r, ⟨¯v⟩t > ¯w. Therefore ⟨¯v⟩t /∈
α( ¯w, r, j′) for any j′ ∈[r].

22
D. Adamson
Combining Observations 2 and 3, the size of RA( ¯w, m, S, r) can be given in terms
of the sets α( ¯w, r, j) and β( ¯w, r, j) as 
j∈[r]
|α( ¯w, r, j)|+|β( ¯w, r, j)|. The remainder
of this section is laid out as follows. We ﬁrst provide a high level overview of
how to compute the size of α( ¯w, r, j). Then we provide a high level overview
on computing the size of β( ¯w, r, j). Finally, we state Theorem 2, summarising
the main contribution of this section and showing that RankSN( ¯w, m) can be
computed in at most O(n6 log2 n) time.
Computing the size of α( ¯w, r, j). We begin with a formal deﬁnition of α( ¯w, r, j).
Let α( ¯w, r, j) ⊆RA( ¯w, m, S, r) be the subset of words in RA( ¯w, m, S, r)
such that for every word ¯v ∈α( ¯w, r, j), j is the smallest rotation for which
⟨¯v⟩j ≤¯w. Note that if j is the smallest rotation such that ⟨¯v⟩j ≤¯w, the
ﬁrst j symbols of ¯v must be such that for every j′ ∈[j −1], ¯v[j′,2r] > ¯w. Let
A( ¯w, p, ¯B, i, j, r) ⊆α( ¯w, r, j) be the set of words of length 2 · r such that every
word ¯v ∈A( ¯w, p, ¯B, i, j, r):
1. ⟨¯v⟩s > ¯w for every s ∈[j −1].
2. ⟨¯v⟩j < ¯w.
3. ¯v[1,r] = S(¯v[r+1,2·r]).
4. The subword ¯v[r+1,r+i] is strictly bound by ¯B ⊑i ¯w.
5. The subword ¯v[i−p,i] = ¯w[1,p].
Rather than computing the size of A( ¯w, p, ¯B, i, j, r) directly, we are instead
interested in the number of unique suﬃxes of length r −i of the words
in A( ¯w, p, ¯B, i, j, r). Note that as every word in A( ¯w, p, ¯B, i, j, r) belongs to
a symmetric necklace, the number of possible suﬃxes on length r −i of
words in A( ¯w, p, ¯B, i, j, r) equals the number of unique subwords of words in
A( ¯w, p, ¯B, i, j, r) between position i + 1 and r. Let SA( ¯w, p, ¯B, i, j, r) be a func-
tion returning the number of unique suﬃxes of length r −i of the words within
A( ¯w, p, ¯B, i, j, r). The value of SA( ¯w, p, ¯B, i, j, r) is computed in a dynamic man-
ner relaying on a key structural proposition regarding A(A( ¯w, p, ¯B, i, j, r)).
Proposition 3. Given ¯v ∈A( ¯w, p, ¯B, i, j, r), such that ¯v[i−s,i+1] ≥¯w[1,s] for
every s ∈[i], ¯v also belongs to A( ¯w, p′, WX[ ¯B, ¯vi+1], i + 1, j, r) where p′ = p + 1
if ¯vi+1 = ¯wp+1 and 0 otherwise.
Proof. By deﬁnition, if ¯v ∈A( ¯w, p, ¯B, i, j, r) then there must exists some p′ ∈
[i + 1], and ¯B ⊑i ¯w such that ¯v ∈A( ¯w, p′, ¯B′, i, j, r). From Proposition 1, the
value of ¯B′ = WX[ ¯B, S(¯vi+1)]. Further ¯vi+1 ≥¯wp+1 as otherwise ¯v[i−p,i+1] <
¯w[1,p+1], contradicting the original assumption. If ¯vi+1 = ¯wp+1 then p′ = p + 1
by deﬁnition. Otherwise p′ = 0 as ¯v[i−s,i+1] > ¯w[1,s+1].
Corollary 1. Let ¯v, ¯u ∈A( ¯w, p, ¯B, i, j, r) be a pair of words and let ¯v′ = ¯u[1,i] :
¯v[i+1,r] : S(¯v[1,i] : ¯u[i+1,r]). Then ¯v′ ∈A( ¯w, p′, WX[ ¯B, ¯vi+1], i+1, j, r) if and only
if ¯v ∈A( ¯w, p′, WX[ ¯B, ¯vi+1], i + 1, j, r).

Ranking Binary Unlabelled Necklaces in Polynomial Time
23
Proposition 3 and Corollary 1 provide the basis for computing the value of
SA( ¯w, p, ¯B, i, j, r). This is done by considering 4 cases based on the value of i
relative to the values of j and r which we will sketch bellow. The key observation
behind this partition is that the value of the symbol at position i+1 is restricted
diﬀerently depending on the values of i, j, r and p.
If i < j, then ¯vi+1 must be greater than or equal to ¯wp+1 for every
v ∈A( ¯w, p, ¯B, i, j, r), to avoid a contradiction caused by there being a rotation
smaller than j for which ¯v is smaller than ¯w. This gives two cases. If ¯wp = 1 then
the only possible value of ¯vi+1 is 1 and therefore the value of SA( ¯w, p, ¯B, i, j, r) is
equal to the value SA( ¯w, p + 1, WX[ ¯B, 0], i + 1, j, r). Alternatively, if ¯wp+1 = 0,
then ¯vi+1 can be either 0 or 1. The number of suﬃxes of length r −i of words
in A( ¯w, p, ¯B, i, j, r) where the symbol at position i + 1 is 0 equals the value of
SA( ¯w, p+1, WX[ ¯B, 1], i+1, j, r). The number of suﬃxes of length r−i of words
in A( ¯w, p, ¯B, i, j, r) where the symbol at position i + 1 is 1 equals the value of
SA( ¯w, 0, WX[ ¯B, 0], i + 1, j, r). Therefore, if i < j and ¯wp+1 = 0, the value of
SA( ¯w, p, ¯B, i, j, r) is SA( ¯w, p + 1, WX[ ¯B, 1], i + 1, j, r) + SA( ¯w, 0, WX[ ¯B, 0], i +
1, j, r).
If i = j then the value of ¯vi+1 depends on the value of p for every ¯v ∈
A( ¯w, p, ¯B, i, j, r). In order for j to be the smallest rotation for which ¯v is smaller
than ¯w, the value of p must be 0, as otherwise the rotation by j −p would be a
smaller rotation for which ¯v is smaller than ¯w. Hence, if p > 0, A( ¯w, p, ¯B, i, j, r) =
∅and by extension SA( ¯w, p, ¯B, i, j, r) = 0. If p = 0 and i = j, then the value of
¯vi+1 must be 0, as otherwise the rotation by r leads to a word that is greater
than ¯w. Therefore, when p = 0 and i = r, the value of SA( ¯w, p, ¯B, i, j, r) is
exactly equal to the value SA( ¯w, 1, WX[ ¯B, 1], i + 1, j, r) of length r −i −1.
If j < i < r and p < i −j, then the rotation of ¯v ∈A( ¯w, p, ¯B, i, j, r)
by j leads to a word smaller than ¯w regardless of the value of ¯vi+1, and hence
SA( ¯w, p, ¯B, i, j, r) = 2r−i, corresponding to the set of all possible words of length
i −r over the binary alphabet. If j < i < r and p = i −j, then the symbol
at position i + 1 must be less than or equal to ¯wp+1, therefore the value of
SA( ¯w, p, ¯B, i, j, r) of length r −i is determined by the value of ¯wp+1. If ¯wp+1 =
0 then the value of ¯vi+1 must be 0 to avoid a contradiction, and hence the
value of SA( ¯w, p, ¯B, i, j, r) equals the value of SA( ¯w, p + 1, WX[ ¯B, 1], i + 1, j, r).
Otherwise, if ¯wp+1 = 1 then the value of ¯vi+1 can be either 0 or 1. Any word in
A( ¯w, p, ¯B, i, j, r) where the symbol at position i + 1 is 0 will be smaller than ¯w
after being rotated by j regardless of the value of the symbols at position i+2 to
r. Therefore, the number of suﬃxes of length r −i of words in A( ¯w, p, ¯B, i, j, r)
where the symbol at position i+ 1 is 0 is 2r−i−2. Further, the number of suﬃxes
of length r −i of words in A( ¯w, p, ¯B, i, j, r) where the symbol at position i + 1
is 1 is equal to the value of SA( ¯w, p + 1, WX[ ¯B, 0], i + 1, j, r).
Finally, if i = r then the number of unique zero length suﬃxes of words in
A( ¯w, p, ¯B, i, j, r) is determined by the value of p and ¯B. If p < i −j, then for
every ¯v ∈A( ¯w, p, ¯B, i, j, r), the rotation of ¯v by j is less than ¯w regardless of
the value of ¯B. Therefore the number of possible suﬃxes of length 0 of words
in A( ¯w, p, ¯B, i, j, r) is 1, representing the empty word. On the other hand, if

24
D. Adamson
p = i −j, then the number of possible suﬃxes of length 0 can be determined
by the value of ¯B. Note that the rotation of any word in A( ¯w, p, ¯B, i, j, r) by
j is less than ¯w if and only if ¯w[1,p] : ¯B < ¯w[1,p+r]. Therefore the value of
SA( ¯w, p, ¯B, i, j, r) is either 1, if ¯w[1,p] : ¯B < ¯w[1,p+r], or 0 otherwise.
Lemma 6. The value of SA( ¯w, p, ¯B, i, j, r) can be computed in O(n3) time.
Proof (Sketch). Following the outline given above, the value of SA( ¯w, p, ¯B, i, j, r)
is computed in a dynamic manner, starting with i = r as a base case,
and progressing in descending value of i. For each value of i, the value of
SA( ¯w, p, ¯B, i, j, r) is computed for every ¯B ⊑i ¯w, and p ∈[1, i] if i ≤j, or
p = i −j if i > j. For i = r, the value of SA( ¯w, i −j, ¯B, i, j, r) can be computed
in O(n) time for every ¯B ⊑i ¯w. For i < r the value of SA( ¯w, p, ¯B, i, j, r) can be
computed in O(1) time provided the value of SA( ¯w, p′, ¯B′, i + 1, j, r) has been
precomputed for every p′ ∈{p + 1, 0} and ¯B′ ⊑i+1 ¯w. As there are only n values
of ¯B ⊑r ¯w to consider in the base case, and at most O(n3) total possible value
of i, p ∈[r], ¯B ⊑i ¯w, the total complexity of this process is O(n3).
Lemma 7. The size of α( ¯w, j, r) can be computed in O(n4) time.
Proof. From Lemma 6, the value of SA( ¯w, p, ¯B, i, j, r) can be computed in O(n3)
time for any value of i, p ∈[n] and ¯B ⊑i ¯w. Note that SA( ¯w, 0, ∅, 0, j, r) allows
us to count the number of words ¯v ∈α( ¯w, j, r) where ¯v[r+1,r+i] ̸⊑¯w for every
i ∈[r], or equivalently, where S(¯v[1,i]) ̸⊑¯w. To compute the remaining words,
let ¯u ⊑i−1 ¯w and let x ∈{0, 1} be a symbol such that ¯u : x ̸⊑¯w. Further let
¯B ⊑i ¯w be the subword of ¯w strictly bounding ¯u : x and let p be the length
of the longest suﬃx of S(¯u : x) that is a preﬁx of ¯w, i.e. the largest value such
that S(¯u : x)[i−p:i] = ¯w[1,p]. Observe that S(¯u : x)[1,p] is the preﬁx of some word
¯v ∈α( ¯w, j, r) if and only if one of the following holds:
– if i < r then (¯u : x)[i−s,i] > ¯w[1,s] for every s ∈[p + 1, i].
– if i = r then p = 0.
– if i > r then p = i −r.
As each condition can be checked in at most O(n) time, and there are at most
O(n2) subwords of ¯w, it is possible to check for every such subword if it is a
preﬁx of some word in A( ¯w, p, ¯B, i, j, r) in O(n3) time. Following Corollary 1,
the number of suﬃxes of each word in A( ¯w, p, ¯B, i, j, r) is equal to the value of
SA( ¯w, p, ¯B, i, j, r). By precomputing SA( ¯w, p, ¯B, i, j, r), the number of words in
α( ¯w, j, r) with ¯u : x as a preﬁx can be computed in O(1) time. Therefore the
total complexity of computing the size of α( ¯w, j, r) is O(n3).
Computing the Size of β( ¯w, r, j). We start by subdividing β( ¯w, r, j) into the
subsets B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r). Let B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) ⊆β( ¯w, r, j) be
the subset of β( ¯w, r, j) containing every word ¯v ∈β( ¯w, r, j) where ¯v satisﬁes:
1. ¯v[1,r] = S(¯v[r+1,2·r]).
2. The ﬁrst i symbols of ¯v are strictly bound by
¯
Bf ⊑i ¯w ( ¯
Bf standing for
bounding the front).

Ranking Binary Unlabelled Necklaces in Polynomial Time
25
3. The subword ¯v[r+1,r+i] is strictly bound by
¯
Bb ⊑i
¯w ( ¯
Bb standing for
bounding the back).
4. The subword ¯v[i−pf ,i] = ¯w[pf ] (pf standing for the front preﬁx).
5. the subword ¯v[r+i−pb,r+i] = ¯w[pb] (pb standing for the back preﬁx).
Proposition 4. Given ¯v ∈B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r), where ¯v[i−s,i+1] ≥¯w[1,s]
for every s
∈
[i], ¯v also belongs to B( ¯w, p′
f, p′
b, XW[ ¯
Bf, ¯vi+1], XW[ ¯
Bb,
S(¯vi+1)], i, j, r) where p′
f = pf +1 if ¯vi+1 = wpf +1 or 0 otherwise, and p′
b = pb+1
if S(¯vi+1) = ¯wpb+1, and 0 otherwise.
Proof. Following the same arguments as Proposition 3, observe that ¯v[1,i+1] is
bound by XW[ ¯
Bf, ¯vi+1] and S(¯v[1,i+1]) is bound by XW[ ¯
Bb, S(¯vi+1)]. Similarly,
the value of p′
f is pf + 1 if and only if ¯vi+1 = ¯wpf +1, and must be 0 otherwise.
Further the value of p′
b is pb + 1 if and only if S(¯vi+1) = ¯wpb+1, and 0 otherwise.
Corollary 2. Let ¯v, ¯u ∈B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) be a pair of words and let
¯v′ = ¯u[1,i] : ¯v[i+1,r] : S(¯v[1,i] : ¯u[i+1,r]). Then ¯v′ ∈B( ¯w, p′
f, p′
b, ¯
Bf
′, ¯
Bb
′, i, j, r) if
and only if ¯v ∈B( ¯w, p′
f, p′
b, ¯
Bf
′, ¯
Bb
′, i, j, r).
Proposition
4
and
Corollary
2
are
used
in
an
analogous
manner
the
Proposition 3. As before, the goal is not to directly compute the size of
B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r), but rather to compute the number of suﬃxes of length
r−i of the words therein. To that end, let SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) be the num-
ber of unique suﬃxes of length r −i of words in B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r). Note
that the number of suﬃxes of length r −i of words in B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r)
equals the number of unique subwords between positions i+1 and r of the words
B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r). Additionally, note that following Corollary 2, the size
of B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) can be computed by taking the product of the num-
ber of unique preﬁxes of words in B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r), and the number of
unique suﬃxes of words in B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r). The process of computing
the number of such suﬃxes is divided into four cases based on the values of i, j
and r.
When i < j, for every word ¯v ∈B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r), vi+1 must be
greater than or equal to ¯wpf +1 to avoid there being a rotation less than j for
which ¯v is less than ¯w. Further, the value of the relabelling of ¯vi+1 must be
greater than or equal to ¯wpb+1 to avoid any rotation in [r + 1, 2 · r] being less
than ¯w. Therefore, the symbol at position i + 1 can be 0 if and only if ¯wpf +1 =
0, and can be 1 if and only if ¯wpb+1 = 0. The number of suﬃxes of length
r −i of words in B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) where the symbol at position i + 1
is 0 is equal to the value of SB( ¯w, p′
f, p′
b, XW[ ¯
Bf, 0], XW[ ¯
Bb, 1], i, j, r), and the
number of suﬃxes where the symbol at position i + 1 is 1 is equal to the value
of SB( ¯w, p′
f, p′
b, XW[ ¯
Bf, 1], XW[ ¯
Bb, 0], i, j, r).
When i = j, then the value of SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) depends pri-
marily on the value of pf. If pf
> 0, then as ⟨v⟩j <
¯w for every v ∈
B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r), ⟨v⟩j−pf < ¯w, contradicting the assumption that j
is the smallest rotation for which ¯v is smaller than ¯w. Hence, if pf > 0, then the

26
D. Adamson
set B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) must be empty and by extension have no suﬃxes
of length r−i. If pf = 0 then as ¯w1 = 0, the symbol ¯vi+1 must be 0 for every ¯v ∈
B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r). Therefore, the value of SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) is
exactly equal to the value of SB( ¯w, 1, p′
b, , XW[ ¯
Bf, 0], WX[ ¯
Bb, 1], i + 1, j, r) of
length r −i −1.
To count the number of suﬃxes of length r −i when i > j, an auxiliary,
technical set Y( ¯w, i, pb, ¯Bf) is introduced. Informally, Y( ¯w, i, pb, ¯Bf) contains
the set of words of length i such that every pair of words ¯u ∈Y( ¯w, r −i, pb, ¯Bf)
and ¯v ∈B( ¯w, pf, pb, ¯
Bf, ¯
Bb, r−i, j, r), every suﬃx of the word S(¯v[1,r−i] : u) : ¯
Bf
of length at least r is greater than the preﬁx of ¯w of the same length. In other
words, Y( ¯w, i, pb, ¯Bf) contains the set of words that can be appended to preﬁxes
of B( ¯w, pf, pb, ¯
Bf, ¯
Bb, r−i, j, r) while maintaining the condition that any rotation
by more than r results in a word strictly greater than ¯w. Treating the method of
counting Y( ¯w, i, pb, ¯Bf) as a black box, the number of suﬃxes of length i −r in
B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) when pf < i−j is exactly the size of Y( ¯w, i, pb, ¯Bf). If
pf = i −j, then the observe that every word ¯v ∈B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) must
satisfy the conditions that ¯vi+1 ≤¯wpf +1 and S(¯vi+1) ≥¯wpb+1. If ¯wpf +1 = 1 and
¯wpb+1 = 1 then ¯vi+1 must be 0, giving a total of |Y( ¯w, r−i−1, pb+1, WX[ ¯Bf, 0])|
suﬃxes of length r −i. If ¯wpf +1 = 1 and ¯wpb+1 = 0 then there are |Y( ¯w, r −
i −1, 0, WX[ ¯Bf, 0])| suﬃxes of length r −i where the ﬁrst symbol is 0, and
the number of r −i length suﬃxes where the ﬁrst symbol equals 1 is equal to
the value of SB( ¯w, pf + 1, pb + 1, WX[ ¯
Bf, 1], WX[ ¯
Bb, 0], i + 1, j, r). If ¯wpf +1 =
0 then ¯vi+1 must be 0, and hence the value of SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) is
SB( ¯w, pf + 1, p′
b, WX[ ¯
Bf, 0], WX[ ¯
Bb, 1], i + 1, j, r).
When i = r, the number of zero length suﬃxes of B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r)
is either 0, if ¯w[1,pf ] : ¯
Bf ≥¯w[1,pf +r], or 1 otherwise.
Lemma 8. The size of β( ¯w, j, r) can be computed in O(n5) time.
Proof (Sketch). The size of β( ¯w, j, r) is computed in an analogous manner to
the size of α( ¯w, j, t) as shown in Lemma 7. This is done by computing the size
value of SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) using the layout given above.
The value of SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) can be computed in O(n) time if
i = r, and O(1) time if i < r and the size of SB( ¯w, p′
f, p′
b, ¯
Bf
′, ¯
Bb
′, i+1, j, r) has
been precomputed for every p′
f ∈{0, pf + 1}, pb ∈{0, pb + 1} and ¯
Bb
′, ¯
Bf
′ ⊑i ¯w.
As there are at most O(n4) possible values of pf, pb ∈[r] and ¯
Bb, ¯
Bf ⊑r ¯w, the
value of SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, r, j, r) can be computed for every pf, pb ∈[r] and
¯
Bb, ¯
Bf ⊑r ¯w in O(n5) time. Similarly, as there are at most O(n5) possible values
of i ∈[r], pf, pb ∈[i] and ¯
Bb, ¯
Bf ⊑i ¯w, the value of SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r)
can be computed in O(n5) time for every value of i ∈[r], pf, pb ∈[i] and
¯
Bb, ¯
Bf ⊑i ¯w.
Note that the set B( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) does not include the words in
β( ¯w, r, j) with a preﬁx that is a subword of ¯w. The number of such words can
be computed in a brute force manner by ﬁnding the length of the longest preﬁx
that is a subword of ¯w, and determining the number of possible suﬃxes. The
number of such suﬃxes are counted in using SB( ¯w, pf, pb, ¯
Bf, ¯
Bb, i, j, r) in a

Ranking Binary Unlabelled Necklaces in Polynomial Time
27
manner analogous to the way SA( ¯w, p, i, j, r) is used in Lemma 7, to count the
number of words in α( ¯w, j, r) with a preﬁx that is a subword of ¯w.
Theorem 2. The value of RankSN( ¯w, m) can be computed in O(n6 log2 n) time
for any m ≤n.
Proof. Following Lemmas 2, 3, and 4, the value of RankSN( ¯w, m, r) is:
RankSN( ¯w, m, r) =

d|r
⎛
⎝1
2 · r

p|d
μ
d
p

|RA( ¯w, m, S, p)|
⎞
⎠
From Observations 3 and 2 , the size of RA( ¯w, m, S, r) equals

j∈[m]
|α( ¯w, r, j)|+
|β( ¯w, r, j)|. Following Lemma 7, the size of α( ¯w, r, j) can be computed in O(n3)
time. Following Lemma 8, the size of β( ¯w, r, j) can be computed in O(n5) time.
As there are at most O(n) values of j, the total time complexity for determining
the size of RA( ¯w, m, S, r) is O(n6). As there are at most O(log n) possible divi-
sors of r, the size of RA( ¯w, m, S, p) needs to be evaluated at most O(log n) times,
giving a total time complexity of O(n6 log n). The value of RankSN( ¯w, m, r) can
then be computed in at most O(log2 n) time once the size of RA( ¯w, m, S, p) has
been precomputed for every factor p of r. Finally, following Lemma 5, the value
of RankSN( ¯w, m) can be computed from the value of RankSN( ¯w, m, r) for at
most O(log n) values of r. Therefore the total time complexity of computing
RankSN( ¯w, m) is O(n6 log2 n).
5
Enclosing Necklaces
This section shows how to rank a word ¯w within the set of binary unlabelled
necklaces enclosing ¯w. Note that the rank of ¯w within this set is equivalent to
the number of binary unlabelled necklaces enclosing ¯w. As with the ranking
approach to symmetric necklaces, we start with the key theoretical results that
inform our approach.
Lemma 9. Let RankEN( ¯w, m) be the rank of ¯w within the set of necklaces of
length m that enclose ¯w and let RankEL( ¯w, m) be the rank of ¯w within the set of
Lyndon words of length m that enclose ¯w. RankEN( ¯w, m) = 
d|m
RankEL( ¯w, d).
Proof. Observe that every necklace counted by RankEN( ¯w, m) must have a
unique period that is a factor of m, hence RankEN( ¯w, m) = 
d|m
RankEL( ¯w, d).
Lemma 10. Let EL( ¯w, m) be the set of words of length m belonging to a Lyndon
word that encloses ¯w. RankEL( ¯w, m) = |EL( ¯
w,m))
m
.
Proof. Following the same arguments as in Lemma 3, every aperiodic necklace
counted by RankEL( ¯w, m) must have exactly m words in EL( ¯w, m) representing
it. Therefore RankEL( ¯w, m) = |EL( ¯
w,m))
m
.

28
D. Adamson
Lemma 11. Let EL( ¯w, m) be the set of words of length m belonging to a Lyndon
word that encloses ¯w and let EN( ¯w, m) be the set of words of length m belonging
to a necklace that encloses ¯w. The size of EL( ¯w, m) equals 
d|m
μ
 m
d

|EN( ¯w, d)|.
Proof. Following the same arguments as in Lemma 9, the size of EN( ¯w, m)
can be expressed in terms of the size of EL( ¯w, d) for every factor d of m as
|EN( ¯w, m)| = 
d|m
|EL( ¯w, d)|. Applying the M¨obius inversion formula to this
equation gives |EL( ¯w, m)| = 
d|m
μ
 m
d

|EN( ¯w, d)|.
As in the Symmetric case, we partition the set of necklaces into a series of
subsets for ease of computation. Let γ( ¯w, m, r) denote the set of words belonging
to a necklace which encloses ¯w such that r is the smallest rotation for which
¯v ∈γ( ¯w, m, r) is smaller than ¯w, i.e. the smallest value where ⟨¯v⟩r < ¯w. We
further introduce the set C( ¯w, i, r, ¯
Bf, ¯
Bb, pf, pb) ⊆γ( ¯w, m, r) as the set of words
where every ¯v ∈C( ¯w, i, r, ¯
Bf, ¯
Bb, pf, pb) satisﬁes the following conditions:
1. ⟨¯v⟩s > ¯w for every s ∈[r −1].
2. ⟨S(¯v)⟩s > ¯w for every s ∈[m].
3. ⟨¯v⟩r < ¯w.
4. ¯v[1,i] is bound by ¯
Bf ⊑i ¯w.
5. S(¯v[1,i]) is bound by ¯
Bb ⊑i ¯w.
6. pf is the length of the longest suﬃx of ¯v[1,i] that is a preﬁx of ¯w, i.e. the
largest value such that ¯v[i−pf ,i] = ¯w[1,pf ].
7. pb is the length of the longest suﬃx of S(¯v[1,i]) that is a preﬁx ¯w, i.e. the
largest value such that S(¯v[i−pb,i]) = ¯w[1,pb].
Note that Conditions 1, 2, and 3 are the necessary conditions for ¯v to be in
γ( ¯w, m, r). As before, we break our dynamic programming based approach into
several sub cases based on the value of i relative to r. As in the symmetric case,
we relay upon a technical proposition.
Proposition 5. Given ¯v ∈C( ¯w, i, r, ¯
Bf, ¯
Bb, pf, pb), ¯v also belongs to C( ¯w, i +
1, r, WX[ ¯
Bf, ¯vi+1], WX[ ¯
Bb, ¯vi+1], p′
f, p′
b)
Corollary 3. Given a pair of words ¯v, ¯u ∈C( ¯w, i, r, ¯
Bf, ¯
Bb, pf, pb) let ¯v′ =
¯v[1,i] : ¯u[i+1,m]. Then ¯v′ ∈C( ¯w, i+1, r, ¯
Bf
′, ¯
Bb
′, p′
f, p′
b) if and only if ¯v ∈C( ¯w, i+
1, r, ¯
Bf
′, ¯
Bb
′, p′
f, p′
b).
Theorem 3. Let RankEN( ¯w, m) be the rank of ¯w within the set of necklaces of
length m which enclose ¯w ∈Σn. The value of RankEN( ¯w, n) can be computed
in O(n6 log n) time for any m ≤n.
Proof (Sketch). The high level idea is to compute the size of C( ¯w, i, r, ¯Bf,
¯Bb, pf, pb) in a dynamic manner analogous to the computation of the size of
A( ¯w, p, ¯B, i, j, r). Starting with i = m as the base case and progressing in
descending value of i, the size of C( ¯w, i, r, ¯Bf, ¯Bb, pf, pb) is computed for every

Ranking Binary Unlabelled Necklaces in Polynomial Time
29
¯Bf, ¯Bb ⊑i ¯w, pf, pb ∈[i]. By showing that the size of C( ¯w, i, r, ¯Bf, ¯Bb, pf, pb) can
be computed in O(1) time for any i < m, and O(n) time when i = m, the size
of C( ¯w, i, r, ¯Bf, ¯Bb, pf, pb) for every i, j ∈[m], ¯Bf, ¯Bb ⊑i ¯w, and pf, pb ∈[i] is
computed in O(n6) time. The additional complexity is due to number of lengths
that need to be computed following Lemmas 9, 10 and 11.
References
1. Adamson, D., Deligkas, A., Gusev, V.V., Potapov, I.: Combinatorial algorithms
for multidimensional necklaces. arXiv preprint https://arxiv.org/abs/2108.01990
(2021)
2. Adamson, D., Deligkas, A., Gusev, V.V., Potapov, I.: Ranking bracelets in poly-
nomial time. In: 32nd Annual Symposium on Combinatorial Pattern Matching
(CPM 2021). Leibniz International Proceedings in Informatics (LIPIcs), vol. 191,
pp. 4:1–4:17 (2021)
3. De Felice, C., Zaccagnino, R., Zizza, R.: Unavoidable sets and circular splicing lan-
guages. Theor. Comput. Sci. 658, 148–158 (2017). Formal languages and automata:
models, methods and application. In: Honour of the 70th birthday of Antonio
Restivo
4. Filos-Ratsikas, A., Goldberg, P.W.: The complexity of splitting necklaces and
bisecting ham sandwiches. In: Charikar, M., Cohen, E. (eds.) Proceedings of the
51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019,
Phoenix, AZ, USA, 23–26 June 2019, pp. 638–649. ACM (2019)
5. Gilbert, E.N., Riordan, J.: Symmetry types of periodic sequences. Ill. J. Math.
5(4), 657–665 (1961)
6. Graham, R.L., Knuth, D.E., Patashnik, O.: Concrete Mathematics: A Foundation
for Computer Science. Addison-Wesley (1994)
7. Kociumaka, T., Radoszewski, J., Rytter, W.: Computing k-th Lyndon word
and decoding lexicographically minimal de Bruijn sequence. In: Kulikov, A.S.,
Kuznetsov, S.O., Pevzner, P. (eds.) CPM 2014. LNCS, vol. 8486, pp. 202–211.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-07566-2 21
8. Kopparty, S., Kumar, M., Saks, M.: Eﬃcient indexing of necklaces and irreducible
polynomials over ﬁnite ﬁelds. Theory Comput. 12(1), 1–27 (2016)
9. Mareˇs, M., Straka, M.: Linear-time ranking of permutations. In: Arge, L., Hoﬀ-
mann, M., Welzl, E. (eds.) ESA 2007. LNCS, vol. 4698, pp. 187–193. Springer,
Heidelberg (2007). https://doi.org/10.1007/978-3-540-75520-3 18
10. Myrvold, W., Ruskey, F.: Ranking and unranking permutations in linear time. Inf.
Process. Lett. 79(6), 281–284 (2001)
11. Sawada, J., Williams, A.: Practical algorithms to rank necklaces, Lyndon words,
and de Bruijn sequences. J. Discret. Algorithms 43, 95–110 (2017)
12. Shimizu, T., Fukunaga, T., Nagamochi, H.: Unranking of small combinations from
large sets. J. Discret. Algorithms 29, 8–20 (2014)
13. Williamson, S.G.: Ranking algorithms for lists of partitions. SIAM J. Comput.
5(4), 602–617 (1976)

On the Power of Recursive
Word-Functions Without Concatenation
Jérôme Durand-Lose(B)
Univ. Orléans, INSA Centre Val de Loire, LIFO, EA 4022, 45067 Orléans, France
jerome.durand-lose@univ-orleans.fr
Abstract. Primitive recursion can be deﬁned on words instead of natu-
ral numbers. Up to usual encoding, primitive recursive functions coincide.
Working with words allows to address words directly and not through
some integer encoding (of exponential size). Considering alphabets with
at least two symbols allows to relate simply and naturally to complexity
theory. Indeed, the polynomial-time complexity class (as well as NP and
exponential time) corresponds to delayed and dynamical evaluation with
a polynomial bound on the size of the trace of the computation as a
direct acyclic graph.
Primitive recursion in the absence of concatenation (or successor for
numbers) is investigated. Since only suﬃxes of an input can be out-
put, computation is very limited; e.g. pairing and unary encoding are
impossible. Yet non-trivial relations and languages can be decided. Some
algebraic (anbn, palindromes) and non-algebraic (anbncn) languages
are decidable. It is also possible to check arithmetical constrains like
anbmcP (n,m) with P polynomial with positive coeﬃcients in two (or
more) variables. Every regular language is decidable if recursion can be
deﬁned on multiple functions at once.
Keywords: Primitive recursion · Recursion on words · String
recursion · Word-Functions
1
Introduction
Primitive recursion and general recursion (or μ-recursion) are well-known and
addressed in every textbook on computability. They are based on Peano’s
axiomatisation of natural numbers and form a neat deﬁnition of computable
functions over numbers. They have been studied for a century and are the topic
of innumerable articles. Nowadays, computability is not anymore considered to
be just about numbers but to be about any kind of information that can be rep-
resented and manipulated through textual/symbolic representations. In recent
decades, the term recursive has been shifting to be replaced by computable [10] to
reﬂect the preeminence of the computer age and to stress on operational models
rather than conceptual deﬁnitions.
The present paper advocates an alternative deﬁnition of primitive recursion
grounded on sequences of symbols, i.e. words, instead of numbers. Although the
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 30–42, 2022.
https://doi.org/10.1007/978-3-031-13257-5_3

On the Power of Recursive Word-Functions Without Concatenation
31
deﬁnition is natural with more than one successor, it has not been much studied.
Or rather it has been proved that all the main properties coincide, so that there
is little interest in a less reﬁned design.
We feel otherwise for at least two epistemological reasons. The ﬁrst one is
that many articles addressing recursion on words ﬁrst provide an encoding of
words as numbers then work on numbers. It certainly proves that words can be
represented as numbers and worked upon this way (at the cost of complexity).
Shouldn’t it be the other way round? Numbers are represented by words and
all our basic arithmetical algorithms (e.g. multiplication) are taught for decimal
representation and implemented with binary-based representations. The second
reason is meeting colleagues not keen on proving by induction, and instead, they
introduce some numerical measure (e.g. depth of a formula) and then make a
(numerical) recursion. When dealing with words, we should use induction to
manipulate them (and numbers and recursion when we need counting)1.
There are also more practical reasons for word recursion: connections to com-
plexity theory with a natural measure of evaluations and to language theory. The
ﬁrst point is to note that the time complexity naturally corresponds to the size
of trace of the evaluations when nothing is reevaluated (dynamic programming)
and evaluated only if needed (delayed evaluation).
The connection with language theory is developed in the paper by consid-
ering recursion without concatenation/successor (preventing encoding between
numbers and words as well as pairing). It already exhibits the ability to decide
some non-algebraic languages and do some arithmetic checking. In the rest of the
introduction, we present a brief state of the art on recursion on words, then, the
complexity connection, some results on language decision without concatenation,
and ﬁnally, the outline of the paper.
In the literature, the topic is referred to as recursion on string, recursion
on word, recursive word/string-functions or recursion on representation. The
last denomination often means a representation of natural numbers by words
enumerated in shortlex/military order (length then alphabetically) leading to a
non-trivial successor word-function. The literature is rather ancient (for com-
puter science) with a peak in the 1960’s. Most of the literature deal with hier-
archies and, like almost everything in the ﬁeld in those days, is number-centric.
We concentrate on overviews and more recent and accessible papers (and in
English).
The transcription by B. Kapron of notes on a course of S. Cook in Berkeley
in 1967 [5] contains the m-adic notation of numbers (digits does not include 0)
and relations on weak classes (including polynomial time functions, FP, from
[4]). This paper does not exactly use word-functions: it has primitives {n →
10n+i}0≤i≤9 emulating concatenation on words together with number ordering.
In [6], the authors provide a survey on counterparts on words of classical
results for primitive number recursion (Ackermann function, limited recursion,
Grzegorczyk and loop hierarchies). They prove that everything coincides.
1 We restrain from coining the primitive induction term to avoid any misunderstanding
with close ﬁelds of research.

32
J. Durand-Lose
There exists research on inﬁnite alphabet (not the case here) like [11]. Up to
some encoding with numbers, it corresponds to computation over ﬁnite sequences
of numbers encoded by numbers.
Variations on base functions and operators exist in mentioned papers (e.g.
limited recursion for Grzegorczyk hierarchy) as well as others. A restriction to
unitary word-functions is considered in [1,3,9]. To mention a more recent work,
the nowhere deﬁned function is added to primitive recursive word-functions in
[7].
As expected, as soon as a class is powerful enough to provide functions to
encode and decode from one setting to another, the hierarchies correspond with
the number setting. This is a motivation to investigate restrained classes. As far
as we know, recursion without concatenation was not investigated.
Comparing to primitive recursion on numbers, the successor function is
replaced by left concatenation for every symbol and the recursion operator has
to consider every possible ﬁrst symbol. Various examples of word-functions are
provided, some have no counterpart in the number setting like reverting a word
or testing whether a word is a palindrome. An encoding of tuples of words on any
alphabet as a word in a 2-symbol alphabet is provided; thus multiple recursion
is not adding any power to primitive recursion and the number of symbols does
not change the hierarchies and complexity classes when there are at least two
symbols.
The numbers in Peanos’s axiomatisation are identiﬁed with words of a 1-
symbol alphabet, and so are the functions. Proving that primitive recursive
functions on integers coincide is quite straightforward with the following encod-
ing. Let Σ = {a1, a2, · · · , ar} be the alphabet, the r-adic encoding function
of words into natural numbers: ⟨ε⟩= 0 and ak · w, ⟨ak · w⟩= k + r · ⟨w⟩. This
encoding is onto and corresponds to the ranking number (starting from 0) of the
reverse of w in the shortlex order2. For example ⟨ai+1 · w⟩= ⟨ai · w⟩+ 1 and
⟨ajai+1 · w⟩= ⟨ajai · w⟩+ r.
The natural evaluation scheme of primitive recursion functions is not very
eﬃcient (especially for numbers in unary notations), so a diﬀerent scheme is used
to show the proximity with the Turing machine model. The delayed dynamic eval-
uation scheme of word-functions is when the functions are called by name (not
value) and only the needed expressions are evaluated (delayed) and all the evalu-
ation results are stored so that nothing is re-evaluated (dynamic programming).
An evaluation is represented by a direct acyclic graph (DAG) whose nodes are
calls to function evaluations. Each node is labelled with the call: expressions
of the function and of the arguments and its value (if computed). The DAG-
complexity of an evaluation of a function is the number of nodes in the DAG.
The size of the input is deﬁned by the sum of the lengths of the input words.
Given the expression of the initial function, the out-degrees of the nodes are
bounded by present arities; the number of edges is thus linearly bounded in
the number of nodes. Nodes are atomic operations, the length of any value is
2 The usual deﬁnition is on the reversed word, but we deﬁne it in coherence with the
restriction to left concatenation.

On the Power of Recursive Word-Functions Without Concatenation
33
bounded by the input size and the DAG depth. The whole description of the
labelled DAG is bounded by a polynomial in the size of its complexity.
The class of polynomial-time functions (from classical complexity theory)
corresponds to the class of word-functions such that the DAG-complexity of any
evaluation is bounded by a polynomial in the input size. One way, given the
expression of the function, it is possible to generate an algorithm that, for any
input, builds the DAG and outputs the result in polynomial time. The other
way, consider a Turing machine implantation of any polynomial-time algorithm
together with a polynomial that bounds its execution time. It is possible to
evaluate the entry size and then the polynomial, to get the result in unary and
then to do a recursion on the TM simulation. The DAG-complexity is linear
in the polynomial value that bounds the iteration time of the Turing machine.
Although we are using unary representation, it is still polynomial in the size of
the input.
This proof can be adapted to NP (with polynomial-size certiﬁcates) and to
exponential time. Please note that there also exists syntactic characterisation of
FP in the number setting [2].
The paper focuses on primitive recursion without concatenation. Recursion
can be used to chop oﬀinitial symbols and only suﬃx of the input can be output
preventing the existence of any pairing or encoding function. As functions, they
look rather bland; but, as language deciders (as pre-images of the empty word)
they prove quite rich. Some algebraic (anbn, palindromes) and non-algebraic
(anbncn) languages are decidable. It is also possible to check arithmetical con-
strains like anbmcP (n,m) with P polynomial with positive coeﬃcients in two (or
more) variables. As a side results, this provides non-trivial examples of unary
languages.
Multiple recursion allows to deﬁne various functions in one recursion. Usually,
this operator is synthesised from single recursion using some pairing function,
but no such function is available without concatenation. If multiple recursion is
available, any regular language can be decided. Basically, each function corre-
sponds to a state of a ﬁnite deterministic automaton.
A rough companion python3 library was developed to manipulate primitive
recursive word-functions and check our constructions. It is available at
https://www.univ-orleans.fr/lifo/Members/Jerome.Durand-Lose/Recherche/Co
mpanion/2022_DCFS.tgz.
Section 2 collects all the deﬁnitions while Sect. 3 provides the expression of
various usual functions. Section 4 investigates the concatenation-less primitive
recursion functions as language deciders. Section 5 shows that adding multiple
recursion to the concatenation-less primitive recursion functions allows to decide
all the recursive languages. Concluding remarks and perspectives are gathered
in Sect. 6.
2
Deﬁnitions
An alphabet, Σ, is a non-empty ﬁnite set: {a1, a2, · · · , ar} where r is its size.
Unless otherwise speciﬁed, its size is least 2; The set of words are deﬁned by

34
J. Durand-Lose
the free monoid Σ∗. Let · denote the concatenation operator and ε denote the
empty word. Teletype fonts are used to denote symbols from Σ and math fonts
to denote words. To ease notation, the concatenation symbol is often omitted,
e.g. aaa stands for a · a · a.
For any number k, a k-ary (word-)function is a function from (Σ∗)k to Σ∗.
The projection of the ith component of a tuple of size n (1 ≤i ≤n) is denoted
πi
n. The identity function is denoted id (=π1
1). The constant ε function is denoted
ε (formally there is one of arity 1, others are generated with compositions and
projections). For any symbol a, the 1-ary left concatenation function associated
with a, is deﬁned by: a · (w) = a · w = aw. The notation ⃗x denotes a vector of
arguments. Sans serif fonts are used to denote functions (in lower case) and
operators (capitalised).
Numbers correspond to a 1-symbol alphabet (0 corresponds to ε). The suc-
cessor of n is denoted S(n) (the only available left concatenation).
Composition Operator. Let j, k be positive numbers. Let g be a k-ary function
and (hi)1≤i≤k be j-ary functions. The j-ary function f = Comp(g, (hi)1≤i≤k) is
uniquely deﬁned by:
f(⃗x) = g (h1(⃗x), · · · , hk(⃗x))
where ⃗x represents j arguments.
(Single) Recursion Operator on Σ. Let k be a positive number. Let g be a k-ary
function and, for each a of Σ, ha be a k+2-ary function. The k+1-ary function
f = Rec(g, (ha)a∈Σ) is uniquely deﬁned by:
f(ε, ⃗y) = g(⃗y)
and
∀a ∈Σ, f(a · w, ⃗y) = ha(w, f(w, ⃗y), ⃗y)
where ⃗y represents k arguments and w is any word in Σ∗.
To increase readability, vertical displays of function vectors are often used
for composition and recursion.
The set of primitive recursive functions is the smallest set of functions con-
taining the empty-word function, left concatenation for every symbol, all the
projections, and closed for the composition and the recursion operators.
From functions, relations are deﬁned as the pre-image of the ε. A unary
relation represents a subset of Σ∗, i.e., a language.
3
First Constructions
In the spirit of the next section, concatenations are avoided as much as possible.
Expressions are provided for an alphabet of size 3 (or 2 when the expression is
large). The generalisation to larger alphabets is straightforward.
A test is a function that returns ε if and only if the condition is satisﬁed. It
is a membership test for languages and relations.

On the Power of Recursive Word-Functions Without Concatenation
35
3.1
Word Manipulations
By composition, it is possible to get any function concatenating a ﬁxed word on
the left, e.g. a1a2a3· = Comp(a1·, (Comp(a2·, (a3·)))). By composing with constant
empty-word function, it is possible to get any constant function, e.g. 
a1a2a3 =
Comp(a1·, (Comp(a2·, (Comp(a3·, (ε)))))).
Basic operations on words are straightforward. The 2-ary concatenation oper-
ator can be generated from composition and recursion:
· = Rec

id, (Comp

a1·, (π2
3)

, Comp

a2·, (π2
3)

, Comp

a3·, (π2
3)

)

.
Right concatenation functions can be generated as in:
·a1 = Rec

a1, (Comp

a1·, (π2
2)

, Comp

a2·, (π2
2)

, Comp

a3·, (π2
2)

)

.
It is possible to manipulate a word as a stack/list with functions to extract
the ﬁrst symbol and the rest of a word:
head = Rec(ε, ( a1, a2, a3))
and
tail = Rec(ε, (π1
2, π1
2, π1
2))
Please note that for head, the ﬁrst symbol is consumed by the recursion so that
it has to be generated again using a concatenation. This phenomenon makes
more involving if not prevent the expression of functions without concatenation.
In the following, we avoid constant functions (to avoid concatenation), so that
needed constants have to be provided as arguments.
The following functions act depending on the presence of a1 at the beginning
of the ﬁrst argument. The ﬁrst function returns the rest of the ﬁrst argument if
present, the second argument otherwise. The second function returns its argu-
ment with leading a1 removed (if any).
suppresselse
a1 = Rec

id,

π1
3, π3
3, π3
3

,
suppressa1? = Comp(suppresselse
a1 , (id,id)).
The usual test for equality over numbers does not yield a test for equality
but a test to decide whether one word is the reverse of the other. This is because
computation in the recursion is done after the recursive call. This is invisible
with numbers since in unary notation all words are palindrome.
testreverse = Comp
⎛
⎜
⎜
⎝Rec
⎛
⎜
⎜
⎝π2
1

Comp
	
Rec
	
id

π1
3
π3
3

 
π2
4
π4
4

Comp
	
Rec
	
id

π3
3
π1
3

 
π2
4
π4
4

⎞
⎟
⎟
⎠

π1
2
π2
2
π1
2
⎞
⎟
⎟
⎠.
This can be used to test if a word is a palindrome:
testpalindrome = Comp
	
testreverse

id
id

.

36
J. Durand-Lose
It is possible to reverse a word and then test for equality:
reverse = Rec
⎛
⎜
⎜
⎝ε

Comp
	
Rec
	
a1

Comp

a1 ·
π2
2

Comp

a2 ·
π2
2


 π2
2

Comp
	
Rec
	
a2

Comp

a1 ·
π2
2

Comp

a2 ·
π2
2


 π2
2

⎞
⎟
⎟
⎠,
testequality = Comp
	
testreverse

π1
2
Comp

reverse
π2
2


.
3.2
Logical Functions
Each of these if functions works like a ternary operator with a condition/test on
the ﬁrst argument returning the second argument if the test succeeds, otherwise
the third argument. A test succeeds if it evaluates to the empty word. The
most basic function just tests whether the ﬁrst argument is the empty word
(ifε(ε, y, z) = y, and ∀x ̸= ε, ifε(x, y, z) = z ). It is deﬁned by:
ifε = Rec

π1
2, (π4
4, π4
4)

.
Conjunction and disjunction operators are deﬁned as 2-ary functions:
andε = Comp

ifε, (π1
2, π2
2, π1
2)

and
orε = Comp

ifε, (π1
2, ε, π2
2)

.
If a non-ε constant is provided, the negation function can be deﬁned by
Comp

ifε, (π1
2, π2
2, ε)

. This function has arity 2 (for the constant).
The following functions use the conditions: to start with a1, to belong to the
regular language a∗
1, and to the language a+
1 :
ifa1Σ∗= Rec

π2
2, (π3
4, π4
4, π4
4)

,
ifa∗
1 = Rec

π1
2, (π2
4, π4
4, π4
4)

, and
ifa+
1 = Rec

π2
2, (Comp

ifa∗
1, (π1
4, π3
4, π4
4)

, π4
4, π4
4)

.
3.3
Encoding and Pairing
Any word on any ﬁnite alphabet can be encoded on 2-symbol alphabet by:
ε →a1a1, and
ai1 · ai2 · · · · · aik →a1ai1
2 a1ai2
2 a1 · · · aik
2 a1.
This function is primitive recursive like its decoding function as constructed
below. The special value for ε has to be taken into account both in coding and
decoding. The encoding is constructed by concatenating all a1ai
2 to a ﬁnal a1.

On the Power of Recursive Word-Functions Without Concatenation
37
encode = Comp
⎛
⎜
⎜
⎜
⎜
⎝
ifε

id

a1a2
Rec
⎛
⎝a1

Comp

a1a2 ·
π2
2

Comp

a1a2
2 ·
π2
2

Comp

a1a3
2 ·
π2
2

⎞
⎠
⎞
⎟
⎟
⎟
⎟
⎠
.
For decoding, a new a|Σ| to be rotated is concatenated on the left for each
a1 but the ﬁrst. For each a2 the function rotﬁrst rotates the ﬁrst symbol of its
argument.
ε →ε
ak · w →ak mod r+1 · w
and rotﬁrst = Rec
⎛
⎝ε

Comp

a2 ·
π1
2

Comp

a3 ·
π1
2

Comp

a1 ·
π1
2

⎞
⎠.
decode = Comp
⎛
⎜
⎜
⎜
⎜
⎝
ifε

Comp (tail |tail)
ε
Comp
⎛
⎝Rec
⎛
⎝ε

Comp

a3 ·
π2
2

Comp

rotﬁrst
π2
2

ε
⎞
⎠

tail
⎞
⎠
⎞
⎟
⎟
⎟
⎟
⎠
.
The special value for ε allows a simple pairing by concatenation.
pair = Comp
	
·

Comp

encode
π1
2

Comp

encode
π2
2


.
To recover the ﬁrst and second values of the pair, the middle a1a1 should be
found while potential leading or ending a1a1 encoding ε are treated correctly.
To recover the ﬁrst value, the ﬁrst a1 is discarded and a1a1 is searched for,
preserving only what is crossed.
pairﬁrst = Comp
⎛
⎜
⎜
⎜
⎜
⎝
decode

Rec
⎛
⎜
⎜
⎜
⎜
⎝
ε

Comp
⎛
⎝ifa1Σ∗

π1
2
a1
Comp

a1 ·
π2
2

⎞
⎠
Comp

a2 ·
π2
2

Comp

a3 ·
π2
2

⎞
⎟
⎟
⎟
⎟
⎠
⎞
⎟
⎟
⎟
⎟
⎠
.
To recover the second value, the ﬁrst a1 is discarded and a1a1 is searched
for, discarding what is crossed.
pairsecond = Comp
⎛
⎜
⎜
⎜
⎜
⎝
Rec
⎛
⎜
⎜
⎜
⎜
⎝
ε

Comp
⎛
⎝ifa1Σ∗

π1
2
Comp

pairﬁrst
π1
2

π2
2
⎞
⎠
π2
2
π2
2
⎞
⎟
⎟
⎟
⎟
⎠

suppressa1?
⎞
⎟
⎟
⎟
⎟
⎠
.
This paring scheme extends straightforwardly to encode any tuple.

38
J. Durand-Lose
4
Primitive Recursion Without Concatenation
Let Σ-CL-PRec be the smallest set of functions containing the empty-word func-
tion, all the projections, and closed by the composition and the primitive recur-
sion operators on Σ∗. A direct induction shows that:
Lemma 1. The output of any word-function in Σ-CL-PRec must be a suﬃx of
a word in the input.
In particular, if the input is composed only of ε, then the output is ε. This
limits the computing power and even constrains language recognition: unless a
non-ε constant is provided, ε is accepted. This means that if ε is not in the
language, a non-empty constant has to be provided in the input.
Since logical operators do not use concatenation, the set of decidable lan-
guages/relations is closed under union, intersection and complement (with a
constant).
4.1
Some Algebraic Languages Decided in Σ-CL-PRec
Palindromes. Test for palindrome p. 6 does not use concatenation. This lan-
guage is algebraic, non-ambiguous but not deterministic (it cannot be recognised
by deterministic push-down automata, DPDA: it has to guess when the middle
of the w is read).
Language an
1 an
2 . Function testan
1 an
2 ﬁrst considers the case of input ε (accepted).
Otherwise, the input is not ε and is stored as a fail value. The ﬁrst symbol has
to be a1 (otherwise fail) and then for each discarded a1, a function that removes
one a2 (or fail) is used on the output.
Technical detail: testfail
an
1 an+1
2
consumes the ﬁrst a2 to know when an
2 starts;
to keep balance testan
1 an
2 consumes the ﬁrst a1 before handling the rest of the
word to testfail
an
1 an+1
2
. The label fail in the name means that a fail value has to be
provided as second argument. It diﬀers from the meaning of else since the fail
value might not be used to indicate failure.
testfail
an
1 an+1
2
= Rec
⎛
⎜
⎜
⎝id

Comp
	
suppresselse
a2

π2
3
π3
3

π1
3
π3
3
⎞
⎟
⎟
⎠,
testan
1 an
2 = Comp
⎛
⎜
⎜
⎝Rec
⎛
⎜
⎜
⎝ε

Comp(testfail
an
1 an+1
2

π1
3
π3
3

π3
3
π3
3
⎞
⎟
⎟
⎠

id
id
⎞
⎟
⎟
⎠.
If the word is not in an
1an
2, then either the fail value is used or a an
1an
2 preﬁx
is removed leaving a non-ε word.
This language is deterministic algebraic (can be recognised by DPDA).

On the Power of Recursive Word-Functions Without Concatenation
39
Language an
1 an
2 am
1 ∪an
1 am
2 am
1 . On a word from an
1an
2am
1 , testan
1 an
2 should return
am
1 . So that the end of the test is carried out by removing remaining a1. Removing
leading a∗
1 is done with suppressa∗
1. To avoid consuming one extra symbol (the
ﬁrst a̸=1), one suppressa1? is stacked for each a1 and then the composition is used
on a copy of the input.
suppressa∗
1 = Comp
⎛
⎝Rec
⎛
⎝ε

Comp

suppressa1?
π2
3

π3
3
π3
3
⎞
⎠

id
id
⎞
⎠,
testan
1 an
2 am
1 = Comp

suppressa∗
1
testan
1 an
2

.
The language an
1am
2 am
1 is decided by removing all leading a1 and then using
previous test (swapping a1 and a2): testan
1 am
2 am
1 = Comp

testan
1 an
2
 suppressa∗
1

.
Since union of decidable languages is decidable, the algebraic language
an
1an
2am
1 ∪an
1am
2 am
1 is decidable. This language is ambiguous.
4.2
Some Non-algebraic Languages Decided in Σ-CL-PRec
Languages an
1 an
2 an
1 . Since intersection of decidable languages is decidable, the
language an
1an
2an
1 = an
1an
2am
1 ∩an
1am
2 am
1 is decidable. This language is not alge-
braic. Similarly, it is possible to prove that the languages an
1an
2an
1 · · · an
1 are all
decidable.
Languages an
1 aP (n)
2
with P Polynomial with Positive Coeﬃcients. The
idea is to deal with functions that discard (or fail) the right amount of a2 accord-
ing to the number of a1 for each monomial. So that the result is empty only if
the sum matches.
For each monomial, a ternary function is deﬁned. The ﬁrst argument starts
with an
1a̸=1 to provide the value for n. The second argument is the one to remove
the a2 from. The third argument is returned if removing is not possible.
For constant monomial 3, the function is
removeelse
a3
2 = Comp
⎛
⎝suppresselse
a2

Comp
	
suppresselse
a2

suppresselse
a2
π2
2

π2
2
⎞
⎠
removea3
2 = Comp
	
removeelse
a3
2

π2
3
π3
3

.
For the monomial 3x, this is done x times:
remove3ax
2 = Comp
⎛
⎜
⎜
⎝Rec
⎛
⎜
⎜
⎝π2
3

Comp
	
removeelse
a3
2

π2
5
π5
5

π4
5
π4
5
⎞
⎟
⎟
⎠

π1
3
π1
3
π2
3
π3
3
⎞
⎟
⎟
⎠.
For the monomial 3x2, it is done x times again. The function remove3ax2
2
is:

40
J. Durand-Lose
Comp
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Rec
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
π2
3

Comp
⎛
⎜
⎜
⎝Rec
⎛
⎜
⎜
⎝π2
3

Comp

removeelse
a3
2

π5
2
π5
5

π4
5
π4
5
⎞
⎟
⎟
⎠

π3
5
π3
5
π2
5
π5
5
⎞
⎟
⎟
⎠
π4
5
π4
5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠

π1
3
π1
3
π2
3
π3
3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Even though the deﬁnition looks involving, these are just nested for loops.
It is possible to design concatenation-less functions that yield each maxi-
mal suﬃxes of a+
1 a+
2 a+
3 · · · a+
m of the form a+
k · · · a+
m. Hence, all the languages
a+
1 a+
2 · · · an
i · · · aP (n)
j
· · · a+
m (for given i ̸= j and P) are all decidable.
Using the same tools, it is also possible to test such languages as an
1am
2 aP (n,m)
2
with P polynomial in 2 variables with positive coeﬃcients. More than two vari-
ables is similarly possible.
5
Regular Languages are Decidable in Σ-CL-PRec
with Multiple Recursion
The multiple recursion operator is usually synthesised with the use of a pairing
function, i.e. a one-to-one function from Σ∗× Σ∗to Σ∗. Yet, no such function
is available without concatenation since any pairing function would have to map
{(ai
1, aj
1)}0≤i,j<2 to four distinct values, but the only possible outputs are in
{ε, a1} (the suﬃxes). (Adding constants would no work for {(ai
1, aj
1)}0≤i,j<k for
every k.)
Lemma 2. There is no pairing function in Σ-CL-PRec.
Multiple Recursion Operator on Σ. Let m and k be any positive numbers. Let
(gi)1≤i≤m be k-ary functions and, for each a of Σ, (ha,i)1≤i≤m be (k+m+1)-ary
functions. The (k+1)-ary functions
(fi)1≤i≤m = Recm
(gi)1≤i≤m, (ha,i)a∈Σ,1≤i≤m

are uniquely deﬁned by ∀i, 1 ≤i ≤m:
fi(ε, ⃗y) = gi(⃗y)
and
∀a ∈Σ,
fi(a · w, ⃗y) = ha,i(w, f1(w, ⃗y), · · · , fm(w, ⃗y), ⃗y)
where ⃗y represents k arguments.
The set Σ-CL-PRec∗is deﬁned like Σ-CL-PRec, but with the addition of
the closure by the recursion operators of every arity. Lemma 1 extends to
Σ-CL-PRec∗: the output has to be a suﬃx of an input.
Regular Languages are Decidable in Σ-CL-PRec∗. Let L be a regular lan-
guage. It is decided by some deterministic ﬁnite automaton (Q, δ, q0, A) where

On the Power of Recursive Word-Functions Without Concatenation
41
Q is ﬁnite set of state, δ is the transition table, q0 is the initial state, and A is
the set of accepting states. We suppose that ε ∈L (otherwise add a constant to
the input and complement).
Let the 2-ary functions (fq)q∈Q be deﬁned by multiple recursion from pro-
jections by:
∀∀q ∈A,
fq(ε, w1) = ε(w1) = ε
∀q ∈Q\A,
fq(ε, w1) = π1
1(w1) = w1
∀q ∈Q,
∀a ∈Σ,
fq(a · w, w1) = πi
|Q|+2

w, (fs(w, w1))s∈Q, w1

= fr(w, w1)
where δ(q, a) = r and i suitably chosen
The transition table is encoded in the recursion. The following function decides
L.
testL = Comp

fq0, (π1
1, π1
1)

6
Conclusion
Word-recursion is a rich context allowing to address words directly and to
relate to complexity theory. Although forbidding concatenation seems limiting, it
allows to decide non trivial languages. It is open whether all algebraic languages
are decidable, and if not, which of them are not and why. More generally, a con-
dition for a function to be (un)computable without concatenation that would
rule out functions (e.g., equality) and languages is to be found.
Without concatenation it is still possible to check constrains expressed with a
polynomial with positive coeﬃcients. Although we advocate recursion on words,
the range of integer languages decidable is also wide; e.g. by testing all possible
splitting in two terms, the language {n + n2|n ∈N} can be decided.
We conjecture that even though this class is restricted, there should be some
undecidable properties. For example, emptiness of accepted language might be
undecidable (using diophantine equations [8]).
Any function deﬁned without concatenation, f, satisﬁes |f(x1, · · · , xk)| ≤
max(|x1|, · · · , |xk|), so that this class is included in the level E0 of the Grze-
gorczyk hierarchy (see [6] for deﬁnitions). Relatively to the relations/languages
theses classes deﬁned, we lack an example to show that the inclusion is strict.
We conjecture that the height of recursion in the function deﬁnition provides a
proper hierarchy inside the class.
Some of provided constructions rely on duplicating the input. We are won-
dering whether forbidding duplication leads to a non-trivial class. Otherwise,
how can it be characterised?
We would like to close this article by addressing minimisation. The few oper-
ators for words in the literature are usually number representation based (related
to the shortlex order) in settings where the successor is not a base function but a

42
J. Durand-Lose
non-trivial word-function. We want to avoid the inﬂuence of numbers and refuse
to impose a non-trivial order on words. In the number setting, one can consider
the successor function to be just a function to provide from the current one the
next value to test. We propose to take that point of view: that the minimisation
operator requires another word-function to generate from the current one the
next word to try (starting from the empty word), without any constraint on this
function (does not have to onto, one-to-one or total, as long as it is in the class).
Although it seems more complex, it corresponds to the update of variables in
while loops.
References
1. Asser, G.: Primitive recursive word-functions of one variable. In: Börger, E. (ed.)
Computation Theory and Logic. LNCS, vol. 270, pp. 14–19. Springer, Heidelberg
(1987). https://doi.org/10.1007/3-540-18170-9_150
2. Bellantoni, S.J., Cook, S.A.: A new recursion-theoretic characterization of the
polytime functions. Comput. Complex. 2, 97–110 (1992). https://doi.org/10.1007/
BF01201998
3. Calude, C., Sântean, L.: On a theorem of günter asser. Math. Log. Q. 36(2), 143–
147 (1990)
4. Cobham, A.: The intrinsic computational diﬃculty of functions. In: Bar-Hillel, Y.
(ed.) Studies in Logic and the Foundations of Mathematics. In: Proceedings of the
1964 International Congress, North-Holland, pp. 24–30 (1965)
5. Cook, S.A., Kapron, B.M.: A survey of classes of primitive recursive functions.
Electron. Colloquium Comput. Complex. 1 (2017). https://eccc.weizmann.ac.il/
report/2017/001
6. von Henke, F.W., Rose, G., Indermark, K., Weihrauch, K.: On primitive recur-
sive wordfunctions. Computing 15(3), 217–234 (1975). https://doi.org/10.1007/
BF02242369
7. Khachatryan, M.H.: On generalized primitive recursive string functions. Math.
Probl. Comput. Sci. 43, 42–46 (2015)
8. Matiyasevich, Y.: Hilbert’s tenth problem and paradigms of computation. In:
Cooper, S.B., Löwe, B., Torenvliet, L. (eds.) CiE 2005. LNCS, vol. 3526, pp. 310–
321. Springer, Heidelberg (2005). https://doi.org/10.1007/11494645_39
9. Santean, L.: A hierarchy of unary primitive recursive string-functions. In: Das-
sow, J., Kelemen, J. (eds.) IMYCS 1990. LNCS, vol. 464, pp. 225–233. Springer,
Heidelberg (1990). https://doi.org/10.1007/3-540-53414-8_45
10. Soare, R.I.: Computability and incomputability. In: Cooper, S.B., Löwe, B., Sorbi,
A. (eds.) CiE 2007. LNCS, vol. 4497, pp. 705–715. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-73001-9_75
11. Vučkovi, V.: Recursive word-functions over inﬁnite alphabets. Math. Log. Q. 13(2),
123–138 (1970)

Clusters of Repetition Roots Forming
Preﬁx Chains
Szil´ard Zsolt Fazekas1(B) and Robert Merca¸s2
1 Graduate School of Engineering Science, Akita University, Akita, Japan
szilard.fazekas@ie.akita-u.ac.jp
2 Department of Computer Science, Loughborough University, Loughborough, UK
R.G.Mercas@lboro.ac.uk
Abstract. We investigate lower bounds on the size of clusters (sets of
starting positions of occurrences) of common preﬁxes shared by repeti-
tion roots. Such lower bounds in terms of the constituent roots in the sets
provide upper bounds on the number of distinct repetitions. In the case
of distinct square roots which are totally ordered by the preﬁx relation
it has been shown that there must be more occurrences of the common
preﬁx than the number of roots. Here we develop the theory further by
presenting the tools to extend the bounds to exponents higher than 2 and
we show that they are optimal in the sense that any sequence of cluster
sizes satisfying the lower bounds can be realized. We also take the next
step towards the bounds on arbitrary (only partially preﬁx-ordered) sets
of roots by proving a lower bound on unbordered preﬁxes shared by two
overlapping preﬁx chains of roots.
1
Introduction
Repetitions in words are one of the most studied topic in word combinatorics [17],
partly due to their various applications in string matching [5], molecular biol-
ogy [11], or text compression [19]. The most basic repetition is xx, where x is a
non-empty string. Such strings are also called, due to the form xx = x2, squares.
A string is said to be square-free or repetition-free if it contains no squares.
Combinatorics on words arguably started with the work of Thue [21,22] who
showed that there exist square-free strings over ternary alphabets and cube-free
ones over two letters. Over two letters, trivially every string of length at least 4
contains a square and it has also been shown that any suﬃciently long binary
string must contain at least three distinct squares [9].
A string of length n can have Θ(n2) squares (just take a unary sequence).
If the root x of each square xx must be primitive (not a repetition), one can
still have at most Θ(n log n) squares [5]. When the roots of the squares must be
distinct, then the maximal number becomes linear in the length of the string.
Fraenkel and Simpson proved [10] that the maximum number of distinct squares
S. Z. Fazekas—This Work Was Supported By JSPS KAKENHI Grant Number
JP19K11815.
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 43–56, 2022.
https://doi.org/10.1007/978-3-031-13257-5_4

44
S. Z. Fazekas and R. Merca¸s
in a string is not more than twice the length of the string and they conjectured
that the bound can be signiﬁcantly improved:
Conjecture 1. The number of distinct squares in a length n word is less than n.
They also constructed lower bounds which asymptotically match the conjec-
tured upper bound except for a sublinear term. We will use another simple lower
bound construction by Jonoska, Manea and Seki [15] as our starting point for
discussing optimality later on.
There have been several developments in the last 25 years on the topic. Some
alternative and simple proofs of the 2n upper bound were found [12,13], after
which the bound was improved to 2n−Θ(log n) [14]. Deza, Franek and Thierry [6]
proved the best (peer-reviewed) bound as of now, 11n/6, by a deep investigation
of left aligned last occurrences of distinct squares. There was a claim of further
improvement to 3n/2 very recently [20], but it has not appeared in peer-reviewed
publication to the best of our knowledge.
Regarding exponents larger than 2 it was shown [3] that for ﬁxed integers
ℓ> 2, there can be no more than
n
ℓ−2 powers of exponent ℓin a word of length
n. For cubes, that is, ℓ= 3 the bound was improved to 4n/5 [4]. The study of
repetitions of higher ﬁxed exponents was inspired by the importance of counting
runs, i.e., repetitions whose exponent is at least 2 and which cannot be extended
in either direction without increasing the period. The bound on this number was
conjectured to be less than the word’s length [16] (not much after Fraenkel and
Simpson’s square conjecture was published) and recently proved to be so by a
very elegant and simple argument [1].
There were other developments relevant to the question even though they
did not necessarily improve upper bounds. By using square density increasing
mappings it was shown that binary strings can achieve maximum density if the
conjectured upper bound holds [18]. In the case of partial words (strings with
holes) tight upper bounds have been proved depending on the number of holes [2].
Another recent paper [8] proposed a framework to integrate existing results and
facilitate new ones in the analysis of distinguished positions of squares.
Our Contribution. Finally, the basis of our current work proposed another
angle of attack using clusters of repetition roots [7]. The techniques used there
extract global properties of occurrences of repetitions in a word from local ones
and we continue that line of investigation. We group the repetitions by the par-
tial order imposed on their roots by the preﬁx ordering. All repetitions whose
roots share a common preﬁx are in one group and our aim is to show that there
are ‘many’ occurrences of this common preﬁx forced by the occurrences of the
repetitions. We are working toward proving the conjecture on the lower bound
on the number of those preﬁxes which would imply Fraenkel and Simpson’s. We
will introduce notation and the line of attack in the next section. In Sect. 3 we
generalize the lower bound technique used recently for preﬁx chains of squares,
to the case of higher exponents. More speciﬁcally, we show that if two ℓ-powers
are aligned at the end of their second or further root occurrence, then the shorter

Clusters of Repetition Roots Forming Preﬁx Chains
45
root must be non-primitive. In our previous work this was used to assign unique
positions to primitively rooted squares, followed by a diﬀerent assignment proce-
dure for non-primitively rooted ones, so it forms the basis of lower bound results
for preﬁx chains of repetition roots. Afterwards we discuss the optimality of the
bounds obtained for squares. As opposed to the other bounds mentioned in the
introduction, ours are tight in the sense that for each sequence of cluster sizes sat-
isfying the bounds we can ﬁnd a word and repetition roots in it which have those
exact cluster sizes. We present a simple construction to achieve those bounds.
We also show that a counting argument of similar ﬂavor can be applied to runs
whose suﬃxes of length equal to the run’s period form a preﬁx chain. In Sect. 4
we develop the technique further by designating special occurrences of a shared
unbordered preﬁx of roots in two overlapping preﬁx chains. The main result in
that section is a counterpart of the theorem in Sect. 3: alignment of repetitions
at their suitably deﬁned anchor means that the shorter one is non-primitive. The
challenge is that the anchor has to be deﬁned diﬀerently in the case of root sets
which are not linearly ordered by the preﬁx relation. We present a solution in
the case when such a set is the union of two preﬁx chains with minimal elements
that are unbordered.
2
Preliminaries
A word or string is a concatenation of letters from a ﬁnite alphabet Σ. The empty
word ε is the word of length 0. For a word w = xyz, we call x a preﬁx (denoted
by x ≤p w, or x <p w if x ̸= w) and z a suﬃx of w, while each of x, y, z are
called factors of w. The word y is an inside factor of w if neither x nor z are
empty. A factor is proper if it is non-empty and not equal to w. If x = z, then
x is also a border of w. If two words u and v are not comparable by the preﬁx
relation, we write u <>p v. The longest common preﬁx of two words u = xau′
and v = xbv′ is lcp(u, v) = x if either au′ or bv′ is empty or otherwise a ̸= b.
We call p a period of w if the letters repeat every p positions apart in w. The
minimal period is given by the smallest such p. By |w|x we denote the number
of times x occurs as a factor of w (including overlaps).
A repetition represents consecutive concatenations of the same word. An
ℓ-power (ℓ-repetition) represents ℓsuch repetitions of the same factor. If a word
is not a repetition, then it is called primitive. Moreover, if w = uℓis an ℓ-
repetition we say that u is a root of w, and call u the primitive root of w when
u is primitive.
For a word u and a preﬁx u′ of u, all words uℓu′ with integer exponent ℓ≥0
have period |u|. A word can have multiple periods, e.g., ababa has periods 2
and 4, since ababa = (ab)2a = (abab)1a. While repetitions are deﬁned in terms
of integer powers, rational powers are also possible. Namely, u = tk for some
rational k, if |u| = k|t| and |t| is a period of u. For instance, the word abcabca is
a fractional power of abc since abcabca = (abc)
7
3 . A run is given by the positions
in the word that contain a maximal repetitive factor with period at most half
as long as the length of the factor (a repetition is maximal, if taking a previous

46
S. Z. Fazekas and R. Merca¸s
or following position changes the period). In other words, a run is a factor that
has an exponent at least 2, and which cannot be extended to either left or right.
Finally, by tω we denote the inﬁnite word consisting in consecutive repetitions
of t.
We also recall the following well-known results about primitivity of words
and multiple periods.
Lemma 1. [17] A word w is primitive if and only if it occurs only twice in ww.
Theorem 1 (Fine and Wilf). [17] If a word w has periods p and q and |w| ≥
p + q −gcd(p, q), then gcd(p, q) is also a period of w.
2.1
Clusters of Repetition Roots
In this subsection we introduce clusters of repetition roots and explain the con-
jecture which is the ﬁnal goal of our study.
When wanting to count all distinct ℓ-powers for a ﬁxed ℓ, we denote by
clustw(u), for each factor uℓof w, the set that contains the starting position of
all suﬃxes having u as a preﬁx. We will call this set the cluster of u. Clearly, if
an ℓ-repetition uℓis a factor of a word, then the cluster of u is of size at least ℓ.
As every word, and therefore every suﬃx starting with v also has u as preﬁx
when u <p v, the next observation is straightforward.
Observation 1. For any two factors u and v of a word w, we have u ≤p v ⇔
clustw(v) ⊆clustw(u) ⇔clustw(u) ∩clustw(v) ̸= ∅and |u| ≤|v|.
In this paper we attempt to get closer to the following conjecture, which, if
true, would give a general upper bound for integer exponent distinct repetitions:
Conjecture 2. [7] For any word w, any positive integer ℓ> 1, and any set of
words S = {u1, u2, . . . , un} such that, for all i ∈{1, . . . , n}, uℓ
i is a factor of w
and u1 ≤p ui, we have |S| <
1
ℓ−1|w|u1.
In the paper proposing the conjecture, it was proved for the case where ℓ= 2
and u1 ≤p · · · ≤p un, that is, S is a set of roots of distinct squares, totally ordered
by the preﬁx relation. Such a collection of square roots is called a (preﬁx) chain
and with that, the result can be stated as
Theorem 2. [7] For a word w and a preﬁx chain S = {u1, u2, . . . , un} of square
roots of w, with ui ≤p ui+1 for all i ∈{1, . . . , n −1}, we have |S| < |w|u1.
In the next section we generalize the results necessary to prove Conjecture 2
for preﬁx chains of roots in the case of repetitions of arbitrary exponents. Due
to the page limit we do not present the reassignment procedure, which allocates
distinct positions to the non-primitively rooted repetitions. Compared to the
results in [6,10,14,15], the bound in Theorem 2 is diﬀerent because it is in a
sense optimal, as we will argue at the end of Sect. 3. Furthermore, while the
bounds on distinct repetitions would be direct corollaries of Conjecture 2, the
converse does not hold.

Clusters of Repetition Roots Forming Preﬁx Chains
47
3
Single Chains
In this section we show that the non-primitivity conditions on the roots of col-
liding powers used to prove Conjecture 2 in the special case of single chains of
square roots, are valid for arbitrary exponent K. These are conceptually simple
proofs following the argument of their counterparts for squares (Lemma 5 and
Corollary 2 in [7]). Afterwards we discuss the optimality of the bound w.r.t. the
existence of words w, u1, . . . , un for every possibility of cluster sizes satisfying
the bound. Finally we show that preﬁx chains of square roots at the end of runs
can help ﬁnd alternative techniques for counting maximal repetitions, too.
For a preﬁx x ≤p u and natural number ℓ∈{2, . . . , K}, we say that the
(ℓ, x)-representative ((ℓ, x)-rep) of uK is the longest preﬁx of uℓwhich ends in x.
Note that this x-rep is of length at least (ℓ−1)|u| + |x|. Formally, the (ℓ, x)-rep
of uK is uℓ−1u′x ≤p uK such that for all y we have that uℓ−1yx ≤p uℓimplies
|y| ≤|u′|.
Let w be a word which contains uK as a factor. For the leftmost occurrence
in w of the (ℓ, x)-rep uℓ−1u′x of the K-power uK, let us be its starting position
and um = us + (ℓ−1)|u|.
We deﬁne the (ℓ, x)-anchor of uK in w as the starting position of the right-
most occurrence of x in the ﬁrst occurrence of the (ℓ, x)-rep of the power uK in
w. This (ℓ, x)-anchor is denoted by Ψw(uℓ, x). If the (ℓ, x)-rep of uK is uℓ−1u′x,
then Ψw(uℓ, x) = us + (ℓ−1)|u| + |u′|.
For example, in the word w below
a b a a b c a b a a
b
a a
b
c
a
b
a a
b
a a
b
a
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
we have the cube (3-power) u = (aba)3 starting at position 16. The (2,aaa)-rep of
u3 is abaabaaa = u2, ﬁrst occurring at 7, so Ψw(u2, a) = 7 + |abaab| = 12. The
(2,ab
ab
ab)-rep of u3 is abaab
ab
ab, ﬁrst occurring at 1, therefore Ψw(u2, ab) = 1+|aba| = 4.
The (3,ab
ab
ab)-rep of u3 is (aba)2ab
ab
ab whose only occurrence is at 16, meaning that
Ψw(u3, ab) = 16 + |(aba)2| = 22.
While the (ℓ, x) anchors are not exactly at the right edge of the repetitions
uℓ, as we will see, when two repetitions are aligned by their anchors it has a
similar consequence as if they were aligned at their right edge: the shorter one
is non-primitive. We show that this is true for all pairs of coinciding anchors.
Lemma 2. Let w be an arbitrary word with two K-powers uK, vK such that
u <p v, and let x be a common preﬁx of u and v. If there are ℓ, ℓ′ ∈{2, . . . , K}
such that Ψw(uℓ, x) = Ψw(vℓ′, x), then u = tk for some primitive word t with
|t| < |x| and k ≥2. Moreover, tu′x ≤p v, where u′x is the longest preﬁx of u
bordered by x.
Proof. Assume Ψw(uℓ, x) = Ψw(vℓ′, x). We distinguish three cases based on
the relative positions of us, um and vm, and will derive contradictions in all of
them, except in the last case, when u is non-primitive with its root shorter than

48
S. Z. Fazekas and R. Merca¸s
Fig. 1. The cases analyzed in Lemma 2.
x. Note that vm ≤um always holds, since u ≤p v implies Ψw(uℓ, x) −um ≤
Ψw(vℓ′, x) −vm. In what follows, let the (ℓ, x)-rep of uK be uℓ−1u′x.
(1) vm ≤us, see Fig. 1(1). In this case the (ℓ, x)-rep of uK is a factor of v,
therefore it also occurs at us −|v|, a contradiction.
(2) vm = um, see Fig. 1(2). This means that u is a suﬃx of v and since |v| > |u|,
we have v = yu, for some non-empty word y. From Ψw(uℓ, x) = Ψw(vℓ′, x), we
get that the x-rep of vK is vℓ′−1u′x. However, yu′x ≤p v which means that the
rightmost x occurrence in v is at least |yu′| positions from its start, so
Ψw(vℓ′, x) ≥vs + (ℓ′ −1)|v| + |yu′| > vs + (ℓ′ −1)|v| + |u′| = Ψw(vℓ′, x),
a contradiction.
(3) us < vm < um. Let the (ℓ′, x)-rep of vK be vℓ′−1zu′x, where z is the non-
empty word starting at vm and ending at um −1. Both zu′x and u are preﬁxes
of v, so they are preﬁxes of each other. If zu′x ≤p u, then
Ψw(uℓ, x) ≥us + (ℓ−1)|u| + |zu′| > us + (ℓ−1)|u| + |u′| = Ψw(uℓ, x),
which is a contradiction. The only remaining possibility is if u ≤p zu′x. Then,
there is an occurrence of u at vm and by Lemma 1 this means that u is not
primitive. (see Fig. 1(3.1)).
Now let u = tk, with t primitive and k ≥2. If |uu′| ≥|v| then a conjugate
of v is a preﬁx of uu′, because uu′ is a factor of v2. From here, v has period |t|
and the fact that tk is its preﬁx and t is its suﬃx means that v = tm for some
m > k. This, in turn, means that uℓand hence the (ℓ, x)-rep of uK occurs at
position vs, so the occurrence at us is not the leftmost, another contradiction.
We are left with the case |uu′| < |v|. We have an occurrence of x at um −|u|.
If that x ﬁnishes before position vm, that is, vm −|x| ≥um −|u|, then there
should be an occurrence of x located |v| positions further to the right in vK.
That would give Ψw(vℓ′, x) ≥um −|u| + |v| > um −|u| + |uu′| = Ψw(uℓ, x),
contradicting Ψw(vℓ′, x) = Ψw(uℓ, x). Hence, we get that vm −(um −|u|) < |x|,
which means |t| < |x|. (see Fig. 1(3.2)).

Clusters of Repetition Roots Forming Preﬁx Chains
49
As x is a preﬁx of u = tk, it has the form x = trt′ for some r < k and t′ ≤p t.
The longest preﬁx of u = tk bordered by x is tk−1t′ = u′x. As um > vm, we get
that ttk−1t′ = tu′x ≤p v.
⊓⊔
Corollary 1. Let uK
1 , . . . , uK
n and vK
1 , . . . , vK
n be powers in some word w with
their roots all from the same chain and let x be a common preﬁx of those roots,
such that for all i ∈{1, . . . , n} there are ℓi, ℓ′
i ∈{2, . . . , K} with Ψw(uℓi
i , x) =
Ψw(vℓ′
i
i , x). Then, there exists some primitive word t shorter than x, such that
ui = tki with ki ≥2, for all i ∈{1, . . . , n}.
Proof. From Lemma 2, whenever the (ℓi, x)-anchor of uK
i and the (ℓ′
i, x)-anchor
of vK
i
coincide, there is some primitive ti with |ti| < |x| such that ui = tki
i
with
ki ≥2 and tix is a preﬁx of vi. Given that the roots of these powers form a preﬁx
chain, we get that the words tix also form a preﬁx chain, that is, for all i, j ∈
{1, . . . , n} either tix ≤p tjx or tjx ≤p tix. Furthermore, since x is a common
preﬁx of all the powers, we have x ≤p tix, so x has period |ti|, and therefore,
trivially, so does tix. For any pair ti, tj, with |ti| ≤|tj|, we know that tix ≤p tjx,
so tix also has period |tj|. Since |tix| > |ti| + |tj| > |ti| + |tj| −gcd(|ti|, |tj|), we
can apply Theorem 1 and get that ti and tj have a common primitive root t. We
already know that ti and tj are primitive, so ti = tj = t.
⊓⊔
Not surprisingly, the same anchor assignment procedure does not produce
the desired conclusion if we apply it to powers whose roots are not linearly
ordered by the preﬁx relation. The reason is that what we exploit in the proofs
above is that aligning the right edge of powers whose roots are preﬁxes of each
other results in (at least the shorter one of) them being non-primitive. However,
right-aligning powers which merely share some preﬁx does not provide the same
strict conclusion.
An alternative way of anchoring which might work for powers in two preﬁx
chains with an overlapping part is to assign the symmetric diﬀerence of the
chains by their longest common preﬁx, and anchoring the intersection by the
shortest root as before. Further on we show a scheme which works in a special
case when the shortest root is unbordered. Before moving on to that, however,
we ﬁrst discuss the sharpness of the bounds implied by our conjecture.
3.1
Optimality
Consider a chain of square roots u1 <p · · · <p un as before. From Theorem 2 we
already know that |clust(ui)| ≥n −i + 2, for all i ∈{1, . . . , n}, and trivially,
|clust(ui−1)| ≥|clust(ui)|, but it is natural to ask whether the bounds are
optimal, that is, whether all possible combinations of cluster sizes satisfying
those conditions can actually be realized in some string w. Using the lower bound
construction in [15], we can easily illustrate the extremal cases. We are only
interested in the situations where |clustw(u1)| = n + 1 because we can trivially
add further occurrences of all roots at the end of w to accommodate the other
cases. When |clustw(ui)| = n −i + 2, that is, the topmost cluster has size 2 and

50
S. Z. Fazekas and R. Merca¸s
then each subsequent cluster is one larger than the previous, take ui = abi−1 and
the word w = u1u2 · · · unun. The case |clustw(u1)| = |clustw(un)| = n + 1 is
realized by the roots ui = an−1bai−1 and again a word of the form u1u2 · · · unun.
From this starting point we can develop an algorithm to realize any combi-
nation of cluster sizes. The idea is to start from the case when all clusters are
equal and then reduce the relevant clusters by adding further as to the end of
their roots. We start out with ui = an−1bai−1 as before and the word in which
we realize the clusters will be the concatenation of all the ui. At this point all
clusters are equal to n + 1 and we set ri = i −1 for all i ∈{1, . . . , n}. We will
reﬁne iteratively the values ri and in the end will set ui = an−1bari. To remove
the occurrence immediately preceding the k-th b from the clusters of each root
ui with i ≥j we add rk + n −rj many as to each such ri. After updating the ri
in question, we keep repeating the removal as many times as necessary. To see
whether the construction is correct, note that increasing ri does not aﬀect the
cluster of any of the uj with j < i. By adding rk + n −rj to rj we get that the
unary a-tail of uj is of length rk + n which is more than the distance between
the k-th and (k + 1)-th b in the word, removing all occurrences of uj (and hence
all longer roots, as well) starting before the k-th b.
For example, let the clusters of u1, . . . , u6 be of length 7, 7, 5, 5, 3, 3, respec-
tively. This means n = 6, so initially we set u1 = a5b, u2 = a5ba, u3 = a5ba2,
u4 = a5ba3, u5 = a5ba4 and u6 = a5ba5 and ri = i −1. First we need to remove
the ﬁrst occurrence of the clusters of u3, . . . , u6, so we get k = 1 and j = 3. This
means adding rk + n −rj = 0 + 6 −2 = 4 to each ri with i ≥3. Now the ri
values are 0, 1, 6, 7, 8, 9. Next we need to remove the occurrences preceding the
second b from the same cluster so k = 2 and j = 3, and hence we need to add
rk + n −rj = 1 + 6 −6 = 1 to each of those r values, resulting in 0, 1, 7, 8, 9, 10.
Removing the next two occurrences, k = 3 and 4, respectively, from the clusters
of u5 and u6 is by ﬁrst adding 7 + 6 −9 = 4 to them and then 8 + 6 −13 = 1,
respectively. The end result is 0, 1, 7, 8, 14, 15, so the clusters are realized by the
occurrences of u1 = a5b, u2 = a5ba, u3 = a5ba7, u4 = a5ba8, u5 = a5ba14 and
u6 = a5ba15 in the word u1 · · · u6u6.
This construction is not optimal in the sense that in most cases there exist
much shorter words w and u1, . . . , un which have a chain of clusters satisfying the
same conditions. We expect that investigating the shortest words which realize a
combination of cluster sizes could lead to improvements in both lower and upper
bounds on distinct repetitions.
3.2
Single Chains of Run Ending Squares
A related direction for expanding the theory of clusters is to ﬁnd a proof of
the upper bound on runs in terms of clusters. We present a brief argument for
a simple bound for runs whose “ending squares” form a preﬁx chain. We can-
not readily apply the technique used for distinct squares, because here multiple
occurrences of a repetition have to be taken into account.
Consider a run (a1 · · · an)
k
n in a word w, where ai ∈Σ, k ≥2n, and a1 · · · an
is primitive. Let this run begin at some position i in w. The run ending square is

Clusters of Repetition Roots Forming Preﬁx Chains
51
the square starting at position i+k −2n and ending at i+k −1. For example, if
w = aababaa and we consider the run (ab)
5
2 starting at i = 2 in w, then the run
ending square is baba, which starts at i + k −2n = 3 and ends at i + k −1 = 6.
Each run has a run ending square, so an upper bound on their number is
implicitly an upper bound on the number of runs. The crucial property of run
ending squares uu is that the letter following uu in the word is diﬀerent from
the ﬁrst letter of u. Consider roots of run ending squares u <p v ∈Σ∗, with a
being their ﬁrst letter. Although uu may occur followed by a, but in those cases
it is not the suﬃx of a run with period |u|. An occurrence of uu in w is a run
ending square if it is followed by some b ̸= a or if it is a suﬃx of w. Let the
run ending occurrences of u2 start at positions i1 < · · · < ik. This means that
{i1, i1+|u|, . . . , ik, ik+|u|} ⊆clust(u). However, for all j ∈{1, . . . , k−1} we have
w[ij+|u|] ̸= w[ij+2·|u|], and w[ik+|u|] ̸= w[ik+2·|u|] or ik+2·|u| = |w|+1. From
here, for each j ∈{1, . . . , k}, at least one of the two positions ij and ij +|u| is not
in clust(v), so |clust(u)|−|clust(v)| ≥k. Applying this argument to consecutive
roots in a preﬁx chain u1 <p · · · <p un, we get that clust(ui) is larger than the
number of all runs with run ending square u2
j, j ≥i. However, similarly to the
case of distinct powers, this argument does not extend easily to overlapping
chains of run ending squares, so one either has to deﬁne roots diﬀerently for a
run or ﬁgure out how to treat the case of run ending squares u2, v2, w2 where u
is a common preﬁx of v and w, but the latter two are incomparable.
4
Two Overlapping Chains
Using the anchor positions seen before one can prove the hypothesis for single
chains in the general case. As a ﬁrst extension of the bounds to multiple chains,
we will prove a special case when two overlapping chains share an unbordered
preﬁx, in terms of whose occurrences we can upper-bound the number of distinct
roots in the two chains combined. Here we will use a type of argument relying
on the fact that the preﬁxes in question are unbordered. First we look at some
simple bounds for single chains which, although already obsolete because of
Theorem 2, serve as simple demonstrations of the beneﬁts aﬀorded by considering
unbordered preﬁxes.
We will need the following simple lemma establishing restrictions on the
relative positions of the rightmost occurrences of two squares whose roots have
the same cluster.
Lemma 3. [7] Let u2 ̸= v2 be two squares in some word w with u ≤p v and
clustw(u) = clustw(v). If their corresponding rightmost occurrences start at
positions us and vs, respectively, then |us −vs| ≥|u|.
We call S a grounded chain if the shortest u which is the root of a square
occurring in w and is a common preﬁx of all elements of S, is also in S. For some
ui, uj ∈S, we call u2
i covered by uj if ui <p uj ≤p u2
i or u2
i ≤p uj. The shortest
square root of a grounded chain S is denoted by ssr(S) and represents the
shortest element in S. Note that ssr(S) is not bordered. If it were, say ssr(S) =

52
S. Z. Fazekas and R. Merca¸s
pqp, for some p ̸= ε, then ssr(S)2 = pqppqp, contains p2, so p ∈S, which
contradicts ssr(S) being the shortest element in S. Finally, for two diﬀerent
square roots x and u with x <p u denote diﬀ(x, u) = |clust(x)| −|clust(u)|.
Lemma 4. For a grounded chain S, let m be the number of covered squares with
roots in S and let x = ssr(S). Then, |un|x ≥m.
Proof. If a square u2
i is covered by some uj, then x occurs at position |ui| in uj
and in all uk, with k > j, because uj ≤p uk. In fact, x occurs at position |ui|
even in uℓ, for i < ℓ< j, as ui ≤p uℓ≤p uj.
⊓⊔
Lemma 5. For a grounded chain of square roots S = {u1, . . . , un}, with x =
ssr(S), we have diﬀ(x, ui) + |ui|x < diﬀ(x, ui+1) + |ui+1|x.
Proof. Since ui ≤p ui+1, we have |ui|x ≤|ui+1|x, while clust(ui+1) ⊆clust(ui)
gives diﬀ(x, ui) ≤diﬀ(x, ui+1), so both terms in the sum are non-decreasing.
Moreover, at least one of them increases in each step: if ui+1 covers u2
i , then
|ui|x < |ui+1|x, while if it does not, then |clust(ui+1)| < |clust(ui)|).
⊓⊔
Corollary 2. For a grounded chain of square roots S = {u1, . . . , un} with x =
ssr(S) we have n < 3|clust(x)|
2
−1.
Proof. Since u1 = x we have diﬀ(x, x) = 0 and |u1|x = 1. By Lemma 5, for
each i ∈{1, . . . , n}, the sum diﬀ(ui, x) + |ui|x is strictly increasing, so n <
diﬀ(x, un) + |un|x. Since |clust(un)| ≥2, we have diﬀ(un, x) = |clust(x)| −
|clust(un)| ≤|clust(x)| −2. Also, since u2
n occurs, the size of clust(x) is at
least 2|un|x, that is, |un|x ≤clust(x)
2
. Adding the two gives us the statement. ⊓⊔
By the above we have that, for a chain S, the number of clusters is bounded
by 3n/2, where n = |clust(ssr(S))|. Using Lemma 3 we can further reﬁne this.
Proposition 1. For a grounded chain of square roots S = {u1, . . . , un} with
x = ssr(S) we have n < 4|clust(x)|/3.
Proof. Let us look at the topmost level where two clusters are equal, that is,
suppose clust(u) = clust(v) for u <p v and for all y with v <p y there exists
no z with clust(y) = clust(z).
Since clust(u) = clust(v), by Lemma 3 we have that |clust(u)| ≥3 and there
are at least three non-overlapping occurrences of u. From here, if x = ssr(S), we
get that |u|x ≤|clust(x)|
3
, but the consecutive clusters above v are never equal,
hence the number of clusters is at most |clust(x)| plus the number of times when
two consecutive cluster are equal. The latter is at most |u|x, hence we get that
the number of clusters is at most 4·|clust(x)|
3
.
⊓⊔
As the main focus of this section we present an adaptation of the technique
we used for the upper bound on single chains, for showing that the combined
size of two overlapping preﬁx chains of roots cannot be larger than the number
of occurrences of their common preﬁx, when that preﬁx is unbordered. The latter

Clusters of Repetition Roots Forming Preﬁx Chains
53
qualiﬁcation is an important one, even though we believe that this is a promising
direction towards the full solution of the conjecture. The requirement that the
preﬁx is unbordered not only means that we cannot deduce our conjecture for
arbitrary base clusters, but also that we cannot generalize the result to multiple
overlaps between multiple chains in a straightforward manner. This, in turn,
means that a piece of the puzzle is still missing for the proof of Conjecture 2.
For easy referencing we will denote by diﬀerent letters the roots which are
in the shared part of the two chains and the diﬀering parts of the chains,
respectively. Let X = {x1 <p · · · <p xk} be the common part. The chains
U = {u1 <p · · · <p um} and V = {v1 <p · · · <p vn} are the diﬀering parts, so
we have u1 <>p v1, and of course, as the xi are the common part, xk <p u1
and xk <p v1. Since the result in this section does not yield a full proof of the
conjecture yet anyway, we will only treat the case of squares instead of general
K-powers, to simplify the exposition.
First we show a slightly stronger version of Lemma 1, where we do not
necessarily need the whole word to occur three times in its square to imply
its non-primitivity.
Lemma 6. Let t1, . . . , tn with n ≥2 be arbitrary words and let x be any unbor-
dered word such that |ti|x = 0, for all i ∈{1, . . . , n}. Let Pi denote the product
xt1xt2 · · · xti. If
|P 2
n|Pn−1x > 2
then Pn is non-primitive.
Proof. Since x is unbordered and is not contained in ti, we can reformulate
the statement into an equivalent one over the alphabet containing the letters ti,
i ∈{1, . . . , n} as follows: |(t1 · · · tn)2|t1···tn−1 > 2 implies that the word t1 · · · tn of
length n is non-primitive. Since Pi−1x occurs at least 3 times in P 2
i , we get that
t1 · · · tn−1 occurs at least three times in (t1 · · · tn)2. Let the second occurrence
of t1 · · · tn−1 start at the ith letter (with i > 1) of the square (t1 · · · tn)2. This
means that t1 · · · tn has period i −1 and therefore r = t1 · · · ti−1t1 · · · tn−1 also
has period i −1. At the same time r is the preﬁx of the square (t1 · · · tn)2, so it
also has period n, moreover, its length is n −1 + (i −1) = n + (i −1) −1. From
here by the Fine and Wilf theorem r has period gcd(i −1, n) and we get that
t1 · · · tn is not primitive which implies the statement.
⊓⊔
To describe the assignment of positions to squares we need some deﬁnitions
ﬁrst. For a given x, the ℓ-level x-preﬁx ((ℓ, x)-preﬁx) of a word z is a word z′
such that
– z′ <p z, and
– |z′|x = ℓ.
Further, the ℓ-level x-representative ((ℓ, x)-rep) of a square z2 is the longest
preﬁx of z2 bordered by the (ℓ, x)-preﬁx of z2. The assignment will diﬀer for ui
and vj depending on the number of occurrences of x in them. Let us partition
the roots in U based on whether they have more x’s than lcp(u1, v1) or not, so

54
S. Z. Fazekas and R. Merca¸s
U = U= ∪U> with U= = {u ∈U | |u|x = |lcp(u1, v1)|x} and U> = U \ U=.
We partition V similarly into V= and V>. Now we are ready to describe the
assignment of anchors as follows:
– For all x2
i and also for all u2
i , v2
j with ui ∈U= and vj ∈V=, we set the x-rep
as in the single chain case, i.e., the longest preﬁx of the square ending in x
and start of the last x in the leftmost x-rep as the anchor.
– For the other ui and vj, we set the start of the last x in their leftmost occurring
(ℓ, x)-rep as the anchor, where ℓ= |lcp(u1, v1)|x + 1.
Lemma 7. Let u, v ∈X ∪U ∪V be two distinct square roots. If the anchors of
u2 and v2 coincide then the shorter between u and v is non-primitive.
Proof. We have to check what happens when squares collide for each pairing of
X, U=, U>, V= and V>. These potentially 25 pairings reduce to 15 as the order
does not matter, and can be treated in 7 groups, as we will see below. Like before,
the starting position of the x-rep of an arbitrary square z2 will be denoted by
zs and we set zm = zs + |z|.
1. u ∈U> and v ∈V>: impossible, because in the x-rep of u2 at the ℓth x
before the anchor we have the (ℓ, x)-preﬁx starting, whereas in the x-rep of
v2 we would have the (ℓ, x)-preﬁx of v at the same position, but the two are
incomparable by the preﬁx relation as they are longer than lcp(u1, v1).
2. u ∈U> and v ∈U> (analogous to pairing (V>, V>)): possible; the (ℓ, x)-preﬁx
of u and v are the same, say y. In this case we can apply Lemma 2 as the y-
anchors of u and v coincide, giving the non-primitivity of the shorter between
the two with a primitive root of length less than u1.
3. u ∈U> and v ∈V= (analogous to the pairing (V>, U=)): possible; in this
case we have us < vs. If um ≤vs, then the x-rep of v2 occurs earlier, a
contradiction. If us < vs < um, then we can apply Lemma 6 and we get that
v is non-primitive.
4. u ∈U> and v ∈U= (analogous to the pairings (V>, V=), (U>, X), (V>, X)):
possible; here the anchor of u is deﬁned as the last x occurrence in a copy
of its (ℓ, x)-preﬁx u′x, whereas the anchor of v is the last occurrence of x in
its x-rep vv′x. As u contains more occurrences of x than v does, which has
exactly as many as the lcp of u1 and v1, we get that v′x <p u′x. Now we can
apply Lemma 6 and conclude that v is non-primitive.
5. u ∈U= and v ∈V=: impossible because the fact that |u|x = |v|x implies
um = vm which, in turn, also means us = vs. However, at us we have an
occurrence of u1 and at vs an occurrence of v1, which are incomparable.
6. u ∈U= and v ∈U= (analogous to (V=, V=)): impossible, by an argument
similar to the previous point. Since u and v have the same number of x
occurrences, we get um = vm and then us = vs which implies u = v.
7. u ∈U= and v ∈X (analogous to pairings (V=, X), (X, X)): possible; this is
again a case where Lemma 2 applies as the anchors are all x-anchors, giving
non-primitivity of v with root shorter than x.

Clusters of Repetition Roots Forming Preﬁx Chains
55
All 15 cases have been listed above and all are either impossible or result in
the non-primitivity of the shorter root.
⊓⊔
We obtained that the collision of anchors results in non-primitive shorter root.
A reallocation of the non-primitively rooted squares is likely possible following
the logic used for squares ([7] proof of Theorem 2). However, it is probably more
technically involved in this overlapping case, and since we do not know how
to generalize Lemma 7 to more chains with complex overlapping structure, it
seems of limited use at the moment and we decided not to pursue it here due to
the space restrictions. However, some manner of separately anchoring the chains
based on their lcp with neighboring incomparable chains seems a promising way
towards a ﬁnal solution, so we expect that the analysis above will prove useful.
References
1. Bannai, H., I, T., Inenaga, S., Nakashima, Y., Takeda, M., Tsuruta, K.: The “runs”
theorem. SIAM J. Comput. 46(5), 1501–1514 (2017)
2. Blanchet-Sadri, F., Merca¸s, R., Scott, G.: Counting distinct squares in partial
words. Acta Cybern. 19(2), 465–477 (2009)
3. Crochemore, M., Fazekas, S., Iliopoulos, C., Jayasekera, I.: Number of occurrences
of powers in strings. Int. J. Found. Comput. Sci. 21(4), 535–547 (2010)
4. Crochemore, M., Iliopoulos, C., Kubica, M., Radoszewski, J., Rytter, W., Wale´n,
T.: The maximal number of cubic runs in a word. J. Comput. System Sci. 78(6),
1828–1836 (2012)
5. Crochemore, M., Rytter, W.: Squares, cubes, and time-space eﬃcient string search-
ing. Algorithmica 13(5), 405–425 (1995)
6. Deza, A., Franek, F., Thierry, A.: How many double squares can a string contain?
Disc. Appl. Math. 180, 52–69 (2015)
7. Fazekas, S.Z., Merca¸s, R.: Clusters of repetition roots: single chains. In: Bureˇs,
T., et al. (eds.) SOFSEM 2021. LNCS, vol. 12607, pp. 400–409. Springer, Cham
(2021). https://doi.org/10.1007/978-3-030-67731-2 29
8. Fazekas, S.Z., Seki, S.: Square network on a word. Theor. Comput. Sci. 894, 121–
134 (2021)
9. Fraenkel, A., Simpson, J.: How many squares must a binary sequence contain?
Electron. J. Comb. 2, R2 (1995)
10. Fraenkel, A., Simpson, J.: How many squares can a string contain? J. Comb. Theory
Ser. A 82(1), 112–120 (1998)
11. Gusﬁeld, D.: Algorithms on Strings, Trees, and Sequences - Computer Science and
Computational Biology. Cambridge University Press, Cambridge (1997)
12. Hickerson, D.: Less than 2n distinct squares in a word of length n (2003). Com-
municated by Dan Gusﬁeld
13. Ilie, L.: A simple proof that a word of length n has at most 2n distinct squares. J.
Comb. Theory Ser. A 112(1), 163–164 (2005)
14. Ilie, L.: A note on the number of squares in a word. Theoret. Comput. Sci. 380(3),
373–376 (2007)
15. Jonoska, N., Manea, F., Seki, S.: A stronger square conjecture on binary words. In:
Geﬀert, V., Preneel, B., Rovan, B., ˇStuller, J., Tjoa, A.M. (eds.) SOFSEM 2014.
LNCS, vol. 8327, pp. 339–350. Springer, Cham (2014). https://doi.org/10.1007/
978-3-319-04298-5 30

56
S. Z. Fazekas and R. Merca¸s
16. Kolpakov, R., Kucherov, G.: Finding maximal repetitions in a word in linear time.
In: Proceedings of 40th FOCS, pp. 596–604. IEEE Computer Society Press (1999)
17. Lothaire, M.: Combinatorics on Words. Cambridge University Press, Cambridge
(1997)
18. Manea, F., Seki, S.: Square-density increasing mappings. In: Manea, F., Nowotka,
D. (eds.) WORDS 2015. LNCS, vol. 9304, pp. 160–169. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-23660-5 14
19. Storer, J.A.: Data Compression: Methods and Theory. Computer Science Press,
Inc. (1988)
20. Thierry, A.: A proof that a word of length n has less than 1.5n distinct squares
(2020). https://arxiv.org/abs/2001.02996
21. Thue, A.: ¨Uber unendliche Zeichenreihen. Kra. Vidensk. Selsk. Skrifter. I Mat.
Nat. Kl. 7 (1906)
22. Thue, A.: ¨Uber die gegenseitige Lage gleicher Teile gewisser Zeichenreihen. Kra.
Vidensk. Selsk. Skrifter. I Mat. Nat. Kl. 1 (1912)

Nearly k-Universal Words - Investigating
a Part of Simon’s Congruence
Pamela Fleischmann(B), Lukas Haschke, Annika Huch, Annika Mayrock,
and Dirk Nowotka
Kiel University, Kiel, Germany
{fpa,lha,dn}@informatik.uni-kiel.de,
{stu216885,stu217133}@mail.uni-kiel.de
Abstract. Determining the index of Simon’s congruence is a long out-
standing open problem. Two words u and v are called Simon congruent
if they have the same set of scattered factors (also known as subwords or
subsequences), which are parts of the word in the correct order but not
necessarily consecutive, e.g., oath is a scattered factor of logarithm but
tail is not. Following the idea of scattered factor k-universality (also
known as k-richness), we investigate nearly k-universality, i.e., words
where exactly one scattered factor of length k is absent. We present a
full characterisation as well as the index of the congruence in this spe-
cial case and the shortlex normal form for each such class. Moreover,
we extend the deﬁnition to m-nearly k-universality (exactly m scattered
factors of length k are absent), show some results for m > 1, and give a
full combinatorial characterisation of m-nearly k-universal words which
are additionally (k −1)-universal.
1
Introduction
Given a word w, a scattered factor (also known as (scattered) substring or sub-
word) of w is a word, that is obtained by deleting letters from w while pre-
serving the order, i.e., formally u of length n ∈N0 is a scattered factor of
w (denoted by u ∈ScatFact(w)) if w = v1u[1]v2u[2]...vnu[n]vn+1 for existing
(possibly empty) words v1, ..., vn+1. For instance, flow, poor, wow are scattered
factors of powerflower but rope, loop are not scattered factors since the let-
ters do not occur in the correct order in w. Therefore, scattered factors are a
complexity measure for words (strings) in terms of existing or absent parts of
information. Hence, scattered factors are not only of a theoretical interest, but a
practical, too. When examining discrete data, e.g., protein sequences or incom-
plete or faulty transmissions of signals [7,11,23], scattered factors can be used as
a representation [6,28]. Moreover, scattered factors can be found in some famous
algorithmic problems like searching for longest (increasing) subsequences [1,3,4],
shortest common supersequences [22], string-to-string correction problems [27],
most unusual time series subsequence [18], fast subsequence matching in time-
series databases [8]. Furthermore, there exist neural machine translations, which
use rare words with subword units [25] or byte-level subwords [28].
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 57–71, 2022.
https://doi.org/10.1007/978-3-031-13257-5_5

58
P. Fleischmann et al.
In 1972, Simon deﬁned the famous congruence relation regarding scattered
factors in the context of piecewise testable events [26], today known as Simon’s
congruence: two words x and y are called congruent w.r.t. k ∈N (x ∼k y), iﬀx
and y have the same set of scattered factors of length up to k, i.e., ScatFactℓ(x) =
ScatFactℓ(y) for all ℓ≤k, with the index denoting the length of the considered
scattered factors. Thus, we have aba ∼2 aabaa and aba ̸∼2 abab since bb is a
scattered factor of abab but not of aba. A profound introduction into scattered
factors and Simon’s congruence can be found in [21, Section 6] by Sakarovich and
Simon. Although ∼k is well studied from diﬀerent perspectives with deep insights
(cf. [9,21,26]), determining its index, i.e., determining |Σ∗/ ∼k | for a given
alphabet Σ and k ∈N, is still an open problem. Remarkable results regarding
a normal form for congruence classes w.r.t. ∼k can also be found in [20,24].
In [14–16] the notion of k-richness was introduced in the context of piecewise
testable languages: w is k-rich if ScatFactk(w) = Σk . This notion coincides with
the notion of k-universality introduced in [2] (therein also the relation of both is
explained). The language of k-universal words was intensively investigated and
characterised in [2,5,10]. One of the main insights of k-universal words is that a
word w is k-universal iﬀw’s arch factorisation [13] has k arches. Very recently,
Gawrychowski et al. presented an algorithm to test Simon’s congruence in time
linear in the sum of the words’ length [12]. This very important result is based on
a new data structure called Simon tree. Pursuing the idea of k-universality, where
the main focus is on the cardinality of a word’s scattered factors set, one can
deﬁne the sets Mi,k = {L ⊆Σ∗| ∃w ∈Σ∗: ScatFactk(w) = L, |L| = i} for all
1 ≤i ≤|Σ|k which contain all languages of cardinality i occurring as a scattered
factor set of some word w w.r.t. a length k. Thus, each such L ∈Mi,k represents
a congruence class of ∼k. A special subclass of M|Σ|k−m,k has recently been
studied from an algorithmic point of view in [19]. There the authors investigated
shortest absent scattered factors (SAS) of words, i.e., for a given (k−1)-universal
word w they determined the set of words with length k that are not scattered
factors of w. If this set has cardinality m, we obtain a subset of M|Σ|k−m,k. This
subset may be proper witnessed by the word aabbb which is 13-nearly 4-universal
but not 3-universal.
Our Contribution. In this work, we investigate and characterise the set
M|Σ|k−1,k and give some insights into Mi,k for some other i < |Σ|k −1. We
call a word m-nearly k-universal if | ScatFactk(w)| = |Σ|k −m, i.e., k-universal
words are 0-nearly k-universal in the new notion. We compute the shortlex nor-
mal form for each congruence class of 1-nearly k-universal words and present an
algorithm which decides in linear time whether a word is 1-nearly k-universal
that in contrast to [12] does not need additional data structures.
Structure of the Work. In Sect. 2 we give the basic deﬁnitions and notations.
In Sect. 3 we present the results on 1-nearly k-universal words including the
characterisation and the congruence classes w.r.t. ∼k. The results for m > 1
including a combinatorial characterisation of m-nearly k-universal words which
are also (k −1)-universal are presented in Sect. 4.

Nearly K-Universal Words - Investigating a Part of Simon’s Congruence
59
2
Preliminaries
Let N be the set of all natural numbers, N0 = N ∪{0}, [n] = {1, . . . , n}, and
[n]0 := [n] ∪{0}. An alphabet Σ is a non empty ﬁnite set whose elements are
called letters. Set σ = |Σ|. A word is a ﬁnite sequence of letters from Σ. Let
Σ∗be the set of all ﬁnite words over Σ with concatenation and the empty word
ε as neutral element. Set Σ+ := Σ∗\ {ε}. Let w ∈Σ∗. For all n ∈N0 deﬁne
inductively, w0 = ε and wn = wwn−1. The length of w is the number of w’s
letters; thus |ε| = 0. For all k ∈N0 set Σk := {w ∈Σ∗| |w| = k} and denote
w’s ith letter by w[i] and set w[i..j] = w[i] · · · w[j] if i < j, and ε if i > j for all
i, j ∈[|w|]. Set alph(w) = {a ∈Σ | ∃i ∈[|w|] : w[i] = a} as w’s alphabet and
for each a ∈Σ set |w|a = |{i ∈[|w|] | w[i] = a}|. The word u ∈Σ∗is called
a factor of w if there exist x, y ∈Σ∗such that w = xuy. In the case x = ε,
we call u a preﬁx of w and suﬃx if y = ε. Let Fact(w), Pref(w) and Suﬀ(w),
respectively, be the corresponding sets and let Prefi(w) denote the preﬁx of
length i of w for all i ∈[|w|]0. Deﬁne the reverse of w by wR = w[|w|] · · · w[1]
and if w = xk1
1 xk2
2 · · · xkℓ
ℓ∈Σ∗with ki, ℓ∈N, i ∈[ℓ], the condensed form (print)
of w is deﬁned by cond(w) = x1 · · · xℓunder the assumption that xj ̸= xj+1
for j ∈[ℓ−1]. Let <Σ be a total order on Σ and denote the lexicographical
order on Σ∗by <. Deﬁne wΣ as the word in Σσ with wΣ[i] <Σ wΣ[i + 1] and
alph(w) = Σ. A mapping f : Σ∗→Σ∗is called a morphism if f(uv) = f(u)f(v)
holds for all u, v ∈Σ∗. Thus, a morphic mapping is already completely deﬁned,
if the images for all letters in Σ are given. If f is additionally bijective, f is
called a morphic permutation. For further deﬁnitions see [21]. After these basic
notations, we introduce the scattered factors.
Deﬁnition 1. Let w ∈Σ∗and n ∈N0. A word u ∈Σn is called a scattered
factor of w (u ∈ScatFact(w)) if there exist v1, . . . , vn+1 ∈Σ∗such that w =
v1u[1]v2u[2] · · · vnu[n] vn+1. Set ScatFactk(w) = {u ∈ScatFact(w)| |u| = k}.
The words cau, cafe, life and ufo are all scattered factors of cauliflower
but neither flour nor row. Tightly related to the notion of scattered factors is
the famous Simon congruence.
Deﬁnition 2. Two words w, v ∈Σ∗are Simon congruent w.r.t. k ∈N0 (w ∼k
v) if ScatFactℓ(w) = ScatFactℓ(v) for all ℓ≤k. Given w ∈Σ∗, the word
u ∈Σ∗is called shortlex normal form of w w.r.t. ∼k if u ∼k w and u is the
lexicographically smallest word among the shortest words in w’s congruence class
[w]∼k.
Since ScatFactk(w) ⊆Σk holds for all k ∈N0, determining the index of
Simon’s congruence can be split into the parametrised problem on determining
how many scattered factor sets - or equivalently how many diﬀerent words - exist
with | ScatFactk(w)| = σk −m for all m ∈N0 (cf. [2,5,10] for m = 0).
Deﬁnition 3. A
word
w
∈
Σ∗
is
called
m-nearly
k-universal
if
| ScatFactk(w)| = σk −m. If m = 0, we call w k-universal. Denote by ι(w) the

60
P. Fleischmann et al.
universality index, i.e., the largest k ∈N0 such that w is k-universal. We call 1-
universal words, UnivΣ,k, universal and 1-nearly k-universal words, NUnivΣ,k,
nearly k-universal.
Remark 4. By deﬁnition, all k-universal words are congruent modulo k and a
k-universal word w ∈Σ∗is also k′-universal for all k′ ≤k.
In this work, we mainly investigate nearly k-universal words, thus words,
where in comparison to Σk, exactly one word of length k is absent from the
scattered factor set. In the unary alphabet ε is the only word which has σk−1 = 0
scattered factors; therefore we only consider at least binary alphabets. Moreover,
we assume Σ = alph(w) for a given w, if not stated otherwise.
Remark 5. Notice that w ∈NUnivΣ,k does not imply w ∈NUnivΣ,k−1: we have
aba ∈NUnivΣ,2 but by ι(aba) = 1, aba is not nearly 1-universal but 1-universal.
One of the main tools for the investigation of nearly k-universal words is the
arch factorisation which was introduced by Hebrard [13]. In this factorisation a
word is factorised into universal factors and a rest.
Deﬁnition 6. For a word w ∈Σ∗the arch factorisation is given by w =
ar1(w) · · · ark(w) r(w) for k ∈N0 with
(a) ι(ari(w)) = 1 for all i ∈[k],
(b) ari(w)[|ari(w)|] /∈alph(ari(w)[1 · · · |ari(w)| −1]) for all i ∈[k], and
(c) alph(r(w)) ⊊Σ.
The words ari(w) are called arches of w and r(w) is the rest of w. Deﬁne the
modus m(w) = ar1(w)[|ar1(w)|] · · · ark(w)[|ark(w)|]. The inner of the ith arch of
w is deﬁned as the preﬁx of ari(w) such that ari(w) = ini(w) m(w)[i] holds.
To visualise the arch factorisation in explicit examples we use parenthesis.
For example we write (aab) · (bba) · a to mark the two arches, namely aab and
bba and the rest, a, which is denoted without parenthesis.
Remark 7. The modus m(w) consists of all unique last letters of the arches and
is therefore uniquely deﬁned.
Based on the arch factorisation we deﬁne perfect universal words, which are
words without a rest.
Deﬁnition 8. We call a word w ∈Σ∗perfect k-universal if ι(w) = k and
r(w) = ε. The set of all these words with alph(w) = Σ is denoted by PUnivΣ,k.
For the algorithmic results in Sect. 3 and 4 we use the standard computational
model RAM with logarithmic word-size (see, e.g., [17]), i.e., we follow a standard
assumption from stringology, if w is the input word for our algorithms, we assume
Σ = alph(w) = {1, 2, . . . , σ}.

Nearly K-Universal Words - Investigating a Part of Simon’s Congruence
61
3
Nearly k-Universal Words
In this section we characterise NUnivΣ,k: we show that there exist exactly σk
diﬀerent classes w.r.t. ∼k, i.e., for each word v ∈Σk there exists a word w ∈
Σ∗such that ScatFactk(w) = Σk\{v}. The ﬁrst lemma proves that nearly k-
universal words are (k −1)-universal and that all letters of Σ but one have to
occur in the rest.
Lemma 9. If w ∈NUnivΣ,k then ι(w) = k −1 and | alph(r(w))| = σ −1.
Remark 10. Lemma 9 implies that the length of a nearly k-universal word is at
least kσ −1 since we have k −1 arches and a rest of length σ −1. Moreover, for
each nearly k-universal word w, there exists a unique letter aw with alph(r(w)) =
Σ\{aw}. This implies ScatFactk(w) = Σk\{m(w)aw} for w ∈NUnivΣ,k.
The conditions of Lemma 9 do not suﬃce for a characterisation of NUnivΣ,k.
Consider the word w = (acb)·ba with ι(w) = 1 and alph(r(w)) = {a, b}. We have
| ScatFact2(w)| = |Σ2\{cc, bc}| and thus w ̸∈NUnivΣ,2. The following, na¨ıve,
but intuitive characterisation uses Remark 10: all words of length k ending in
aw, but m(w)aw, have to appear within the word (all others appear necessarily).
Proposition 11. A word w is nearly k-universal iﬀι(w) = k −1, alph(r(w)) =
Σ\{aw}, and for all v ∈Σk with v[1..k −1] ̸= m(w) and v[k] = aw there exists
i ∈[k −1] with v[i]v[i + 1] ∈ScatFact2(ari(w)).
We have w = (accb) · (bac) · ab ∈NUnivΣ,3 as ac, cc ∈ScatFact2(ar1(w))
and ac, bc ∈ScatFact2(ar2(w)). This characterisation is not very helpful since
checking whether a word is nearly k-universal means to check all σk−1 options
for v. The following characterisation does not only provide an eﬃcient way to
check whether w ∈NUnivΣ,k but also builds the basis for an eﬃcient algorithm
regarding ∼k. In beforehand, we prove that cutting oﬀℓarches at the beginning
of a nearly k-universal word, leads to a nearly (k −ℓ)-universal word.
Lemma 12. Let ℓ≤k −1. If w ∈NUnivΣ,k with w = ar1(w) · · · ark−1(w) r(w),
then arℓ+1(w)...ark−1(w) r(w) ∈NUnivΣ,k−ℓ.
Remark 13. Notice that Lemma 12 is not applicable for arches in the middle:
(ab) · (aab) · b ∈NUnivΣ,3 but (ab) · b ̸∈NUnivΣ,2.
The following theorem captures our main combinatorial result: a suitable
characterisation for nearly k-universal words serving as basis for the algorithms.
Here, ScatFactk(wR) = {uR| u ∈ScatFactk(w)} plays an important role.
Theorem 14. For w ∈Σ∗the following statements are equivalent
1. w ∈NUnivΣ,k,
2. ι(w) = k −1, | alph(r(w))| = σ −1 = | alph(r(wR))|, and
(a) if k is even then there exist u1, v2 ∈PUnivΣ, k
2 , u2, v1 ∈PUnivΣ, k
2 −1
and xi ∈Σ+ with | alph(xi)| = σ −1 with w = uixivR
i for i ∈[2].

62
P. Fleischmann et al.
(b) if k is odd then there exist u, v ∈PUnivΣ, k−1
2 , and x ∈Σ+ with
| alph(x)| = σ −1 with w = uxvR.
3. ι(w) = k −1, | alph(r(w))| = σ −1 = | alph(r(wR))|, and for all ˆk, ˜k ∈N with
ˆk + ˜k + 1 = k there exist u ∈PUnivΣ,ˆk, v ∈PUnivΣ,˜k, and x ∈Σ+ with
| alph(x)| = σ −1 such that w = uxvR.
Proof. The implication 3. to 2. is immediate. Now, we prove 2. implies 1. We
have to show that w is nearly k-universal under the three constraints. We know
m(w)aw ̸∈ScatFactk(w) and let y ∈Σk\{m(w)aw}. If y[k] ̸= aw, we have imme-
diately y ∈ScatFactk(w) by the second condition. Thus, assume y[k] = aw.
Case 1: k is even
Choose u1, u2, v1, v2, x1, x2 according to condition a. Since u1 and v2 are per-
fect k
2-universal and u2 and v1 are perfect k
2 −1-universal, we have y

1.. k
2

∈
ScatFact(u1), y
 k
2 + 2..k

∈ScatFact(vR
1 ), y

1.. k
2 −1

∈ScatFact(u2), and
y
 k
2 + 1..k

∈ScatFact(vR
2 ). Thus, if y[ k
2 + 1] ∈alph(x1) or y[ k
2] ∈alph(x2),
we have y ∈ScatFactk(w). Assume y[ k
2 + 1] ̸∈alph(x1) and y[ k
2] ̸∈alph(x2).
Since we have also proven the claim if two consecutive letters of y are in
one arch of u1, u2, v1, or v2, we may assume that y[1.. k
2 −1] = m(u2) and
y = [ k
2 +2..k] = m(v1)R. By | alph(x1)| = σ−1, we have y[ k
2 +1] = m(v2)[ k
2], and
analogously by | alph(x2)| = σ−1, we have y[ k
2] = m(u1)[ k
2]. Choose i1, i2 ∈[|w|]
with w[i1] = m(v2)[ k
2] and w[i2] = m(u1)[ k
2]. If i1 ≥i2, w would have at least
ι(u1) + ι(vR
2 ) = k arches - a contradiction (Fig. 1). Thus we have i1 < i2. This
implies that y[ k
2 + 1] has be chosen before y[ k
2] in w. This implies ι(w) < k −1
- a contradiction.
Case 2: k is odd
Choose u, v, x according to condition b. Since u and v are perfect
k−1
2 -
universal, we have y

1.. k−1
2

∈ScatFact(u) and y
 k−1
2
+ 2..k

∈ScatFact(v). If
y[ k−1
2
+ 1] ∈alph(x), the claim is proven. Thus, assume y[ k−1
2
+ 1] ̸∈alph(x).
Again we can also assume y[1.. k−1
2 ] = m(u) and y[ k−1
2
+ 2..k] = m(v)R. Since
| alph(x)| = σ −1, we have y[ k−1
2
+ 1] = m(w)[ k−1
2
+ 1] which occurs after
m(v)[ k−1
2 ] in w. Again we obtain ι(w) < k−1 - a contradiction. Finally, we prove
that 1. implies 3. Consider ﬁrstly w be nearly k-universal. Then the ﬁrst two
claims follow immediately by Lemma 9 and the fact that wR is nearly k-universal.
By ι(w) = k −1, we have w, wR ∈UnivΣ,k′ for all k′ ≤k −1. Let ˆk, ˜k ∈N<k
with ˆk + ˜k + 1 = k. Thus, there exist u ∈PUnivΣ,ˆk and v ∈PUnivΣ,˜k with
Fig. 1. The factorisation of w for even k where y’s letters occur as the modus.

Nearly K-Universal Words - Investigating a Part of Simon’s Congruence
63
u ∈Pref(w) and v ∈Pref(wR). Choose x ∈Σ∗with w = uxvR. By Lemma 12,
we get xvR ∈NUnivΣ,k−ˆk. Thus, vxR ∈NUnivΣ,k−ˆk. Applying Lemma 12
again, we obtain xR ∈NUnivΣ,1. By Lemma 9 we get | alph(x)| = σ −1. This
concludes the proof.
⊓⊔
We have w = (aab)·(ba)·(ab)·a ̸∈NUnivΣ,4 since the factorisation (aab)(ba)·
a · ((ab))R meets the requirements but the factorisation (aab) · ε · ((ab)(aab))R
does not, witnessing that both are needed (cf. Theorem 14).
Corollary 15. We have wwR ∈NUnivΣ,2k−1 iﬀw ∈NUnivΣ,k as well as
wawR ∈NUnivΣ,2k−1 with a ∈Σ iﬀw ∈NUnivΣ,k and a ∈alph(r(w)).
With Theorem 14 we are able to solve the following two problems (for a
given k) eﬃciently: decide whether v ∈NUnivΣ,k and ﬁnd for a given u ∈Σk,
w ∈NUnivΣ,k such that u ̸∈ScatFactk(w). The latter one leads immediately to
the index of Simon’s congruence restricted to nearly k-universal words. Notice
that for the ﬁrst problem, a linear time algorithm is implicitly given in [12]: if
w is a word of length n, the Simon tree can be constructed in time O(n) and in
time O(k) the lexicographically smallest SAS can be determined; if there is only
one SAS, we have w ∈NUnivΣ,k. The following algorithm only checks whether
a word is nearly k-universal but does not need any additional data structures.
Proposition 16. Given w ∈Σ∗, k ∈N, we decide whether w ∈NUnivΣ,k in
time O(|w|) and compute if so the absent scattered factor.
Data: Given w ∈Σ∗with arch factorisation and k ∈N.
Result: True, if w ∈NUnivΣ,1,k. False, otherwise.
if ι(w) ̸= k −1 || | alph(r(w))| ̸= σ −1 || | alph(r(wR))| ̸= σ −1 then
return false;
else
if k mod 2 == 0 then
wv1 := (ar k
2 (wR) · · · ark−1(wR) r(wR))R ;
/* The index denotes
the deleted archs of w’s factorisation */
wv2 := (ar k
2 +1(wR) · · · ark−1(wR) r(wR))R;
return | alph(r(wv1))| == σ −1
&&
| alph(r(wv2))| == σ −1;
else
wv := (ar k−1
2 (wR) · · · ark−1(wR) r(wR))R;
return | alph(r(wv))| == σ −1;
end
end
Algorithm 1: Testing nearly k-universality
Remark 17. With Theorem 14 nearly k-universal words can be constructed: if
k is odd choose u, v ∈PUnivΣ, k−1
2
and x with | alph(x)| = σ −1, then uxvR
is nearly k-universal. If k is even, choose u, v ∈PUnivΣ, k−1
2
and x1, x2 with
| alph(x1)| = | alph(x2)| = σ −1. Then, ux2yx1v ∈NUnivΣ,k iﬀy[|y|] ̸∈alph(x2)
and y[1] ̸∈alph(x1).

64
P. Fleischmann et al.
Now, we present an algorithm for the second problem. Please recall that
Σa = Σ\{a} and wΣa is the word containing all letters of Σa w.r.t. a predeﬁned
order <Σ on Σ. These words can be preprocessed in time O(σ) for all a ∈
Σ. The intuitive candidate for w ∈NUnivΣ,k with u ̸∈ScatFactk(w) is w =
w2
Σu[1]u[1] · · · w2
Σu[k−1]u[k−1]wΣu[k] but this is not the shortlex normal form: with
u = aaccb the intuitive way leads to (bcbca) · (bcbca) · (ababc) · (ababc) · ac
while the shortlex normal form is given by (bca) · (bcca) · (abc) · (abbc) · ac.
Theorem 18. Given u ∈Σk for k ∈N, one can compute w ∈Σ∗with
ScatFactk(w) = Σk\{u} in time O(k). More precisely, there exists an algorithm
needing k steps computing w ∈NUnivΣ,k in shortlex normal form (Fig. 2).
Data: Given u ∈Σk with Σ = {a1, . . . aσ}.
Result: nearly k-universal word w ∈Σ∗with ScatFactk(w) = Σk \ {u}
w := ε;
wΣ = a1 · · · aσ;
for i = 1 to k −1 do
if u[i] ̸= u[i + 1] then
w := w · wΣu[i] · u[i + 1] · u[i];
else
w := w · wΣu[i] · u[i];
end
end
w := w · wΣu[k];
return w;
Algorithm 2: Computing w ∈NUnivΣ,1,k for u ∈Σk absent.
Fig. 2. Illustrating the algorithm of Theorem 18 for u = abccab.
Let u = abccab and • represent placeholder. Since u[1..5] is m(w), we get
(•a)·(•b)·(•c)·(•c)·(•a). By alph(r(w)) = Σ\{b}, we get (•a)·(•b)·(•c)·(•c)·
(•a)·ac. Including the arches of wR we obtain (•ba)·(•cb)·(•c)·(•ac)·(•ba)·ac.
Now, the • are replaced by the missing letters from each arch of w. Thus, we
ﬁnaly get (bcba) · (accb) · (abc) · (abac) · (bcba) · ac. Notice that we do not need
to preprocess all wΣa for all a ∈Σ but only for all a ∈alph(m(w)).
Remark 19. Notice that the length of the resulting nearly k-universal word w
depends on the given absent scattered factor u ∈Σk. If σ ≥2 and cond(u) =
u1 · · · ur for an r ∈N, we have |w| = kσ + r −2. Thus, if u is unary, we have
|w| = kσ −1. For k > 1 and σ > 2, the shortlex normal form is strictly shorter
than the intuitive candidate by σk −σ −2k + 2 letters.

Nearly K-Universal Words - Investigating a Part of Simon’s Congruence
65
Deﬁnition 20. Given u ∈Σ∗and an order <Σ on Σ, let wu ∈NUnivΣ,k
denote the shortlex normal form w.r.t. ∼k with u ̸∈ScatFactk(wu).
Corollary 21. Given k ∈N, we have |NUnivΣ,k/ ∼k | = σk, i.e., restricting
Simon’s congruence to nearly k-universal words leads to σk diﬀerent congruence
classes.
By Corollary 21 we know how many congruence classes in NUnivΣ,k w.r.t.
∼k exist. Now we show when w ∼k wu holds for w ∈Σ∗, i.e., we characterise
[wu]∼k. Therefore, we need some further insights into nearly k-universal words.
Lemma 22. Given w ∈NUnivΣ,k, we have ar1(w) · · · ari−1(w)α ari+1(w) · · ·
ark−1(w)β ∈NUnivΣ,k for all i ∈[k −1], if α[|α|] = m(w)[i], alph(α[1..|α| −
1]) = alph(ini(w)), ini(w) ∈ScatFact(α[1..|α| −1]), | r(w)| ≤|β|, and alph(β) =
alph(r(w)).
Deﬁnition 23. Let P(w) be the set of all words obtainable from w by Lemma 22.
Remark 24. Lemma 22 implies that for all n ∈N and u ∈Σk with n ≥|wu|
there exists w ∈NUnivΣ,k ∩Σn, i.e., there are inﬁnitely many words in |[wu]∼k|.
We are now able to give a characterisation of the congruence classes of ∼k
in NUnivΣ,k. Since we know that for each u ∈Σk there exists one congru-
ence class, we ﬁx u ∈Σk. We know so far that for each w obtained by the
application of Lemma 22, we have w ∈[wu]∼k. Notice that Lemma 22 cannot
be generalised to an equivalence, since deleting letters from arches may violate
the nearly k-universality: considering w = (aab) · b and deleting one a in the
ﬁrst arch, indeed does not change the modus, but it deletes aa and thus we
have | ScatFact2(abb)| < 3. Recall that the output wu of the algorithm in The-
orem 18 is w.r.t. a given order <Σ on Σ, in particular Prefσ−1(ari(wu)), for
all i ∈[k −1], is the lexicographically smallest word containing all letters of Σ
but m(w)[i]. Analogously, r(wu) is the lexicographically smallest word contain-
ing all letters but u[k]. If we change this order, we obtain other words of the
same length, which are all by Theorem 18 of minimal length. Moreover, if we
choose diﬀerent orders for each arch and for the rest, we still obtain a nearly
k-universal word since the crucial point of Theorem 18 still holds. Thus, each
such word can be obtained from wu by applying some morphic permutation of
Σ on Prefσ−1(ari(wu)) and r(wu) for all i ∈[k −1].
Deﬁnition 25. Let πj, for j ∈[σ!], be the diﬀerent morphic permutations on
Σ and set pi = Prefσ−1(ari(wu)) for all i ∈[k −1]. Choose s1, . . . , sk−1 ∈Σ∗
with wu = p1s1 · · · pk−1sk−1 r(wu). Deﬁne the basis of [wu]∼k by
Bu = {w ∈Σ∗| ∃i1, . . . , ik ∈[σ!] : w = πi1(p1)s1 · · · πik−1(pk−1)sk−1πik(r(wu))}.
Remark 26. For u ∈Σk, we have |Bu| = ((σ −1)!)k−1(σ −1)!.
Based on this Bu and Lemma 22, we can characterise [wu]∼k.

66
P. Fleischmann et al.
Theorem 27. Given u ∈Σk, we have [wu]∼k = {w ∈Σ∗| ∃v ∈Bu : w ∈P(v)}.
Proof. If w ∈P(v) for some v ∈Bu, we have w ∈[wu]∼k. Assume w ∈[wu]∼k.
Thus w ∈NUnivΣ,k and m(w)aw = u. Now, we examine w’s ith arch for a ﬁxed
i ∈[k −1]. We know alph(ini(w)) = Σ\{u[i]}. Let u[i] ̸= u[i + 1]. Suppose that
|ini(w)|u[i+1] = 1. Theorem 14 with ˆk = i−1 and ˜k = k −i implies that the (k −
i)th-arch from vR ends in this occurrence of u[i + 1], i.e., u[i], u[i + 1] ̸∈alph(x).
Since this is a contradiction to w ∈NUnivΣ,k we not only have |ini(w)|u[i+1] ≥2
but also Theorem 14 leads to ari(w) = αiu[i + 1]βiu[i] with alph(αi) = Σ\{u[i]}
and βi ∈(Σ\{u[i]})∗. Thus, there exists vi ∈ScatFactσ−1(αi) with alph(v) =
Σ\{u[i]} and a permutation π on Σ which morphically applied yields π(vi) =
Prefσ−1(ari(wu)). Since alph(r(w)) = Σ\{u[k]} we get similarly an r which is a
permuation of r(wu). This leads to v = v1u[1] · · · v[k −1]u[k −1]r ∈Bu. Adding
all letters of αi, βi and r(w) not in vi and r, resp., implies w ∈P(v).
⊓⊔
Let u = abbc and a < b < c. By Theorem 18 we get wu = (bcba) · (acb) ·
(accb) · ab and w ∈Bu iﬀw = w1w2w3w4 with w1 ∈{bcba, cbba}, w2 ∈
{acb, cab}, w3 ∈{accb, cacb}, w4 ∈{ab, ba}. Thus, we have 16 basis elements
for u. Each of these words can be enriched by additional letters in the inner of
an arch and the rest w.r.t. Lemma 22 to obtain all elements equivalent to wu.
We ﬁnish this section with a third characterisation of nearly k-universal words
that relies on Theorem 14 and Lemma 12 and illustrates the relation of w and
wR in NUnivΣ,k.
Proposition 28. We have w ∈NUnivΣ,k iﬀι(w) = k−1, | alph(r(w))| = σ−1,
and ar2(wR) ... ark−1(wR) r(wR) ∈NUnivΣ,k−1.
Notice that only the deletion of a reversed arch from the beginning leads to an
equivalence. Deleting the ﬁrst arch of w does not suﬃce for a characterisation as
witnessed by w = bcaabcab: indeed, we have ι(abcab) = 1, alph(r(w)) = Σ\{c},
and abcab ∈NUnivΣ,2 but we get (bca) · (abc) · ab ̸∈NUnivΣ,3.
In this section, we presented in Theorem 14 a characterisation for nearly
k-universal words. This combinatorial insight led to two linear time algorithms
that check whether a word is nearly k-universal and compute the shortlex normal
form for a given absent scattered factor. Both algorithms do not require further
data structures. Based on this shortlex normal form we presented the congruence
classes w.r.t. ∼k and characterised the classes fully (cf.Theorem 27).
4
m-Nearly k-Universal Words
For a full characterisation of Simon’s congruence with this approach, all sets
Mi,k for a ﬁxed k and i ∈[σk] have to be investigated. In this section, we give
some ﬁrst insights into Mi,k for i < σk −1, in particular, we consider m-nearly
k-universal words, where m is not necessarily 1, i.e., we are interested in w ∈Σ∗
with | ScatFactk(w)| = σk −m. Therefore, we extend our deﬁnitions.

Nearly K-Universal Words - Investigating a Part of Simon’s Congruence
67
Deﬁnition 29. A word w ∈Σ∗is called m-nearly k-universal, if we have
| ScatFactk(w)| = σk −m. Let NUnivΣ,m,k denote the set of all m-nearly k-
universal words.
Implicitly, a subset of these words was investigated in [19]. There, the
authors determine all shortest absent scattered factors, i.e., if ι(w) = k −1
and | ScatFactk(w)| = σk −m, we have that w ∈NUnivΣ,m,k. In contrast to
nearly k-universal words, for m > 1, ι(w) = k −1 does not necessarily hold
as witnessed by ababca ∈NUnivΣ,14,3 with ι(ababca) = 1 ̸= 2. Moreover, we
have, for instance, w = (accab), w′ = (abc) · a ∈NUnivΣ,3,2 with r(w) = ε and
| alph(r(w′))| < σ −1. The results from Sect. 3 do not hold in general for m > 1:
w = (abc) · (bca) · bb ∈NUnivΣ,7,3 but (bca) · bb ∈NUnivΣ,3,2 ̸= NUnivΣ,7,2
(cf. Lemma 12).
Thus, a thorough characterisation of NUnivΣ,m,k is still open. Unfortunately,
we cannot give such a characterisation but we present some ﬁrst insights for
m ∈{2, σk −2, σk −1, σk} as well as a full characterisation of the subclass
established in [19] including the congruence classes of ∼k in this case. If w ∈Σk,
we have immediately w ∈NUnivΣ,σk−1,k. Therefore, we have |w| ≥k + 1 for all
w ∈NUnivΣ,m,k with m < σk −1.
Remark 30. Similar to NUnivΣ,0,k = UnivΣ,k, the set NUnivΣ,σk,k provides
exactly one equivalence class for ∼k, since exactly the words strictly shorter
than k do not have any scattered factor of length k.
Proposition 31. For each k ∈N, we have |NUnivΣ,σk−1,k/ ∼k | = σk.
If a word has exactly two scattered factors, the following lemma shows that
we are immediately in the binary alphabet. This leads to the index proven in
the following proposition.
Lemma 32. If w ∈NUnivΣ,σk−2,k then | alph(w)| = 2 = |cond(w)|.
Proposition 33. For each k ∈N, we have |NUnivΣ,σk−2,k/ ∼k | = 2
σ
2

k.
Proposition 33 shows that the formula determining the index of ∼k gets more
complicated the farther m is from 0 or σk, resp. Now we show a similar result to
Lemma 9 for NUnivΣ,2,k backing the observation that the conditions on w get
more complicated. Notice that, w ∈NUnivΣ,σ−1,k implies ι(w) = k −1.
Proposition 34. Let w ∈NUnivΣ,2,k with σ > 2. Then ι(w) = k −1 and
either | alph(r(w))| = | alph(r(wR))| = σ −1, or | alph(r(u))| = σ −1 and
| alph(r(uR))| = σ −2 for all u ∈{w, wR}.
The remainder of this section presents results for arbitrary but ﬁxed m. First,
we give an estimation for the number of arches of an m-nearly k-universal word.
In Sect. 3 we saw that for nearly k-universal words ι(w) = k −1 holds. This
does not hold for m > 1, witnessed by (aab) · aa, (ab) · (bba) ∈NUnivΣ,4,3 and
ι((aab) · aa) = 1 ̸= 2 = ι((ab) · (bba)). Moreover, there is not just one ﬁxed ι(w)
which can be uniquely determined by m and k. A ﬁrst estimation is presented
in the following remark.

68
P. Fleischmann et al.
Remark 35. If w ∈NUnivΣ,m,k and σi−1 ≤m ≤σi −1, then ι(w) ≥k −i. Sup-
pose ι(w) < k −i. Then there exists v ∈Σk−i with v /∈ScatFactk−i(w). Thus,
for all v′ ∈Σi we have vv′ /∈ScatFactk(w) which are σi distinct words. Estimat-
ing the number of absent scattered factors results in σk −m = | ScatFactk(w)|
≤σk −σi ≤σk −(σi −1). Therefore, m > σi −1 follows - a contradiction. By
induction we get that for m < σi−1 there exists j < i such that m ≤σj −1.
Thus, ι(w) ≥k −j which is a worst lower bound since j < i and ι(w) < k for
all w /∈UnivΣ,k.
We ﬁnish this section by characterising NUnivΣ,m,k ∩UnivΣ,k−1. Recently,
in [19] Kosche et al. presented algorithmic results to enumerate absent scat-
tered factors of (k −1)-universal words: given a word w with ι(w) = k −1,
they inductively construct a tree structure (SAS-tree) that allows to eﬃciently
query this (possibly) exponentially large set. The idea behind this construction
is that for any u ̸∈ScatFactk(w) and i < k the letter u[i] must occur in ari(w)
and u[i + 1] must not, but can only be found in the following arch and only
as a localised modus (otherwise u would not be absent since there are exactly
k −1 −i succesive arches to choose the remaining letters from). For a combi-
natorial presentation of these conditions, we introduce a factorisation of these
words that will be of use for the remainder of this section. Let from now on
w ∈UnivΣ,k−1. By | alph(r(wR))| < σ, we have r(w)R ∈Pref(ar1(wR)) and
m(wR)[1] ∈alph(ark−1(w)). Thus, choose αk−1, βk−1 ∈Σ∗with ark−1(w) =
αk−1βk−1 and ar1(wR) = (βk−1 r(w))R. With alph(βk−i) ⊆Σ, inductively there
exist αi, βi ∈Σ∗such that ari(w) = αiβi and ark−i(wR) = (βiαi+1)R with
αk = r(w) and α1 = r(wR)R, for all i ∈[k −1] (Fig. 3).
Fig. 3. α-β factorisation of w.
Proposition 36. Let u ∈Σk. Then u ̸∈ScatFactk(w) iﬀu[1] ∈alph(β1) \
alph(α1), u[i] ∈alph(βi), u[i]u[i+1] ̸∈ScatFact2(βiαi+1) for all i ∈[k −1]\{1},
and u[k] ̸∈alph(r(w)).
Given any u ∈Σk, Proposition 36 allows to test whether u ∈ScatFactk(w).
Due to the inductive nature of the conditions it cannot be used straightforwardly
to compare words w, w′ w.r.t. ∼k. Thus, we introduce functions fw mapping
positions in w to their appropriate arches, gw,l for ﬁnding leftmost occurrences
of letters within these arches l, and sets Mw,i to capture the letters that can
succeed w[i] according to Proposition 36, and M ′
w,i to index them in w. Lastly,
hw is used to recursively count how many possible u ̸∈ScatFactk(w) can exists.

Nearly K-Universal Words - Investigating a Part of Simon’s Congruence
69
Deﬁnition 37. Let fw : [|w|] →[k] such that fw(i) = ℓiﬀw’s ith letter belongs
to arℓ(w), for i ∈[|w| −| r(w)|], and fw(i) = k otherwise. Moreover, deﬁne
gw,ℓ: Σ →[|w|] by gw,ℓ(a) = min{i | w[i] = a ∧fw(i) = ℓ} for all ℓ∈[k−1]. Set
Mw,1 = alph(β1)\alph(α1) and Mw,j = (alph(βi+1)\alph(βi[j′ +1..|βi|]αi+1))∩
alph(βi[1..j′]) where fw(j) = i, j′ = j −(i
l=1 |arl(w)| + |αi|) and alph(βk) =
Σ \ alph(r(w)), as well as M ′
w,1 = gw,1(Mw,1) and M ′
w,j = gw,f(j)+1(Mw,j) for
all 2 ≤j < max{m |fw(m) < k}. Let hw(i) = 
j∈M ′
w,i hw(j) for all i ∈{ℓ|
fw(ℓ) < k −1} and hw(i) = |Σ \ alph(r(w))| otherwise.
Remark 38. Notice that by the deﬁnition of m(w) and Proposition 36, we have
m(w)[i + 1] ∈Mw,gw,fw(i)(m(w)[i]) for all i ∈[k −2].
Proposition 39. If w ∈NUnivΣ,m,k ∩UnivΣ,k−1 then m = hw(1).
Proof. Choose a sequence of numbers Iu ∈Nk−1 such that Iu[1] ∈M ′
w,1 and
Iu[i + 1] ∈M ′
w,Iu[i] for all i ∈[k −1]. Then for the word u ∈Σk such that
u[i] = w[Iu[i]] for i ∈[k −1] and u[k] ∈Σ \ alph(r(w)) we have
– u[1] ∈alph(β1) \ alph(α1),
– u[i] ∈βi, u[i]u[i + 1] ̸∈ScatFact2(βiαi+1) for all i ∈[k −2]\{1}, and
– u[k] ̸∈alph(r(w)).
Thus, by Proposition 36, we have u ̸∈ScatFactk(w). Then, calculating hw(1)
recursively equals the number of possibilities to choose such sequences Iu and
extend them with any letter aw ̸∈r(w). Each such sequence is associated to
a diﬀerent absent scattered factor u, i.e., h(1) equals exactly the number m of
length k absent scattered factors in w.
⊓⊔
The following lemma shows that u ∈Σk is absent in w, w′ iﬀthe sets of
possible candidates for positions in βi coincide for w and w′ respectively.
Lemma 40. Let w, w′ ∈UnivΣ,k−1 with alph(r(w′)) ⊆alph(r(w)) and u ∈Σk
with u ̸∈ScatFactk(w). Let I ∈[|w|]k−1 with I[1] ∈M ′
w,1 and I[i + 1] ∈M ′
w,I[i]
such that u[1..k −1] = w[I[1]] · · · w[I[k −1]]. Then u ̸∈ScatFactk(w′) iﬀthere
exist I′ ∈[|w|]k−1 with I′[1] ∈M ′
w′,1 and I′[i + 1] ∈M ′
w′,I′[i] with u[1..k −1] =
w′[I′[1]] · · · w[I′[k −1]] and u[i] ∈Mw,I[i] ∩Mw′,I′[i] for all i ∈[k −1].
For w, w′ ∈UnivΣ,k−1 and u ∈Σk, let C(u, w, w′) be the predicate of the
iﬀ-conditions for u ̸∈ScatFactk(w).
Theorem 41. For all w, w′ ∈NUnivΣ,m,k ∩UnivΣ,k−1, we have w ∼k w′ iﬀ
C(u, w, w′) and C(u, w′, w) for all u ∈Σk.
Notice that w ∼k w′ is equivalent to Mw,j = Mw′,j′ for all j, j′ according to
appropriate sequences I and I′.
In this section we showed for some m how NUnivΣ,m,k looks like and deter-
mined m for w ∈NUnivΣ,m,k ∩UnivΣ,k−1 as well as [w]∼k.

70
P. Fleischmann et al.
5
Conclusion
In this work, we pursued the approach to partition Σ∗w.r.t. the number of
absent scattered factors of a given length k. This leads to the notion of m-
nearly k-universal words, which are words where exactly m scattered factors of
length k are absent. We have chosen this perspective to investigate the index of
Simon’s congruence ∼k and indeed we were able to fully characterise 1-nearly k-
universal words and give the index as well as a characterisation of ∼k restricted
to this subclass. Moreover, we gave some insights for m > 1, especially for
m ∈{2, σk −1, σk −2, σk} (notice that m = 0 is fully investigated in [2]).
Additionally in Sect. 4, we followed the idea from [19] from a combinatorial
point of view, showing for instance that letters have the same dist-value in [19]
iﬀthey are in the same arch of wR. By this approach we showed that m can be
determined recursively for w with ι(w) = k −1 by investigating the overlaps of
the arches from w and wR. Moreover, we proved when two words w1, w2 with
ι(w1) = ι(w2) = k −1 fulﬁl w1 ∼k w2.
Unfortunately, we were not able to give a full characterisation of NUnivΣ,m,k
for arbitrary m. A ﬁrst step could be to determine ι(w) for w ∈NUnivΣ,m,k.
We conjecture that choosing i ∈[σk] such that σi ≤m ≤σi+1 −1 leads to
k −⌊m
σi ⌋−1 ≤ι(w) ≤k −⌊m
σi ⌋. A subpartition of NUnivΣ,m,k depending on
ι(w) (as introduced in [19] and used in Sect. 4) could prove useful.
References
1. Baik, J., Deift, P., Johansson, K.: On the distribution of the length of the longest
increasing subsequence of random permutations. J. Am. Math. Soc. 12(4), 1119–
1178 (1999)
2. Barker, L., Fleischmann, P., Harwardt, K., Manea, F., Nowotka, D.: Scattered
factor-universality of words. In: Jonoska, N., Savchuk, D. (eds.) DLT 2020. LNCS,
vol. 12086, pp. 14–28. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-
48516-0 2
3. Bergroth, L., Hakonen, H., Raita, T.: A survey of longest common subsequence
algorithms. In: SPIRE, pp. 39–48. IEEE (2000)
4. Blumer, A., Blumer, J., Haussler, D., Ehrenfeucht, A., Chen, M.T., Seiferas, J.:
The smallest automation recognizing the subwords of a text. Theor. Comp. Sci.
40, 31–55 (1985)
5. Day, J., Fleischmann, P., Kosche, M., Koß, T., Manea, F., Siemer, S.: The edit
distance to k-subsequence universality. In: STACS, vol. 187, pp. 25:1–25:19 (2021)
6. Do, D., Le, T., Le, N.: Using deep neural networks and biological subwords to
detect protein s-sulfenylation sites. Brief. Bioinform. 22(3) (2021)
7. Dress, A., Erd˝os, P.: Reconstructing words from subwords in linear time. Ann.
Combinatorics 8(4), 457–462 (2005)
8. Faloutsos, C., Ranganathan, M., Manolopoulos, Y.: Fast subsequence matching in
time-series databases. ACM Sigmod Rec. 23(2), 419–429 (1994)
9. Fleischer, L., Kuﬂeitner, M.: Testing Simon’s congruence. In: Proceedings of MFCS
2018, LIPIcs, vol. 117, pp. 62:1–62:13 (2018)
10. Fleischmann, P., Germann, S., Nowotka, D.: Scattered factor universality-the
power of the remainder. preprint arXiv:2104.09063 (published at RuFiDim) (2021)

Nearly K-Universal Words - Investigating a Part of Simon’s Congruence
71
11. Fleischmann, P., Lejeune, M., Manea, F., Nowotka, D., Rigo, M.: Reconstructing
words from right-bounded-block words. Int. J. Found. Comput. 32, 1–22 (2021)
12. Gawrychowski, P., Kosche, M., Koß, T., Manea, F., Siemer, S.: Eﬃciently testing
Simon’s congruence. In: STACS, LIPIcs, vol. 187, pp. 34:1–34:18 (2021)
13. Hebrard, J.J.: An algorithm for distinguishing eﬃciently bit-strings by their sub-
sequences. Theor. Comput. Sci. 82(1), 35–49 (1991)
14. Karandikar, P., Kuﬂeitner, M., Schnoebelen, P.: On the index of Simon’s congru-
ence for piecewise testability. Inf. Process. Lett. 115(4), 515–519 (2015)
15. Karandikar, P., Schnoebelen, P.: The height of piecewise-testable languages with
applications in logical complexity. In: Proceedings of CSL, LIPIcs, vol. 62, pp.
37:1–37:22 (2016)
16. Karandikar, P., Schnoebelen, P.: The height of piecewise-testable languages and
the complexity of the logic of subwords. LICS 15(2) (2019)
17. K¨arkk¨ainen, J., Sanders, P., Burkhardt, S.: Linear work suﬃx array construction.
J. ACM 53(6), 918–936 (2006)
18. Keogh, E., Lin, J., Lee, S.H., Van Herle, H.: Finding the most unusual time series
subsequence: algorithms and applications. KAIS 11(1), 1–27 (2007)
19. Kosche, M., Koß, T., Manea, F., Siemer, S.: Absent subsequences in words. In:
Bell, P.C., Totzke, P., Potapov, I. (eds.) RP 2021. LNCS, vol. 13035, pp. 115–131.
Springer, Cham (2021). https://doi.org/10.1007/978-3-030-89716-1 8
20. K´atai-Urb´an, K., Pach, P., Pluh´ar, G., Pongr´acz, A., Szab´o, C.: On the word
problem for syntactic monoids of piecewise testable languages. Semigroup Forum
84(2), 323–332 (2012)
21. Lothaire, M.: Combinatorics on Words. Cambridge Mathematical Library, Cam-
bridge University Press, Cambridge (1997)
22. Maier, D.: The complexity of some problems on subsequences and supersequences.
J. ACM (JACM) 25(2), 322–336 (1978)
23. Maˇnuch, J.: Characterization of a word by its subwords. In: DLT, pp. 210–219.
World Scientiﬁc (2000)
24. Pach, P.: Normal forms under Simon’s congruence. Semigroup Forum 97(2), 251–
267 (2018)
25. Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words with
subword units. preprint arXiv:1508.07909 (2015)
26. Simon, I.: Piecewise testable events. In: Brakhage, H. (ed.) GI-Fachtagung 1975.
LNCS, vol. 33, pp. 214–222. Springer, Heidelberg (1975). https://doi.org/10.1007/
3-540-07407-4 23
27. Wagner, R., Fischer, M.: The string-to-string correction problem. JACM 21(1),
168–173 (1974)
28. Wang, C., Cho, K., Gu, J.: Neural machine translation with byte-level subwords.
In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34, pp.
9154–9160 (2020)

State Complexity of Binary Coded
Regular Languages
Viliam Geﬀert(B), Dominika Paliˇs´ınov´a, and Alexander Szabari
Department of Computer Science, P. J. ˇSaf´arik University,
Jesenn´a 5, 04154 Koˇsice, Slovakia
{viliam.geffert,alexander.szabari}@upjs.sk,
dominika.palisinova@student.upjs.sk
Abstract. For the given non-unary input alphabet Σ, a maximal pre-
ﬁx code h mapping strings over Σ to binary strings, and an optimal
deterministic ﬁnite automaton (DFA) A with n states recognizing a lan-
guage L over Σ, we consider the problem of how many states we need
for an automaton A′ that decides membership in h(L), the binary coded
version of L. Namely, A′ accepts binary inputs belonging to h(L) and
rejects binary inputs belonging to h(LC), where LC is the complement
of L. The outcome on inputs that are not valid binary codes for any string
in Σ∗can be arbitrary: A′ may accept, reject, or halt in a “don’t care”
state. We show that any optimal deterministic don’t care ﬁnite automa-
ton (dcDFA) A′ solving this promise problem uses at most (∥Σ∥−1)·n
states but at least n states. We also show that, for each non-unary input
alphabet Σ, there exists a maximal binary preﬁx code h such that, for
each n ≥2 and for each N in range from n to (∥Σ∥−1)·n, there exists a
language L over Σ such that the optimal DFA recognizing L uses exactly
n states and any optimal dcDFA for solving the above promise problem
uses exactly N states. Thus, we have the complete state hierarchy for
deciding membership in the binary coded version of L, with no magic
numbers in between the lower and upper bounds.
Keywords: state complexity · ﬁnite automata · don’t care automata ·
preﬁx codes · promise problems
1
Introduction
One of the earliest results in automata theory is the subset construction [16]:
every n-state nondeterministic ﬁnite automaton (NFA) can be replaced by an
equivalent deterministic ﬁnite automaton (DFA) using at most 2n states. This
raised later the question of whether it is possible, for a given number n, to ﬁnd
some N ∈{n, . . . , 2n} such that there is no optimal DFA with exactly N states,
equivalent to some optimal NFA with exactly n states [10]; such numbers were
named “magic”. The problem was solved in [11], showing that there are no magic
numbers for ternary languages, contrary to the unary languages [5]. Since then,
Supported by the Slovak grant contract VEGA 1/0177/21.
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 72–84, 2022.
https://doi.org/10.1007/978-3-031-13257-5_6

Binary Coded Regular Languages
73
the magic numbers were studied for language operations, e.g., in [9], it was shown
that, for the intersection of two languages, given by two DFAs with n and m
states, we have no magic numbers in {1, . . . , n·m}. Such state hierarchies were
studied for other operations as well [4,7,9,11].
From a diﬀerent starting point, we are going to land in yet another complete
state hierarchy with no magic numbers. Our initial motivation was the fact
that most present-day computers store data in a binary coded form. This raises
the following natural question: given a standard DFA A with n states for a
regular language L over an input alphabet Σ, how many states we need to
recognize h(L), the binary coded version of L? Clearly, the answer depends also
on h : Σ∗→{0, 1}∗, the binary code in use. In most cases, we can work with the
assumption that the code is a homomorphism, such that h(α1) = h(α2) implies
α1 = α2, so that each encoded string can be unambiguously decoded back. Since
it is well known that regular languages are closed under any homomorphism
(not necessarily a code—see e.g. [8, Sect. 4.2.3]), the situation seems clear at
ﬁrst glance:1 construct an optimal DFA for h(L).
However, if the automaton for h(L) receives only inputs that are valid binary
images of strings in Σ∗, the outcome on inputs that are not valid images can be
quite arbitrary, which allows us to save some states. This brings us to a modiﬁed
problem: given a code h : Σ∗→{0, 1}∗and a standard DFA A with n states for
L ⊆Σ∗, how many states we need for an automaton A′ that accepts each
β ∈h(L) and rejects each β ∈h(LC)? Here LC denotes the complement of L.
This approach is not completely new: in general, we are given a pair of disjoint
languages ⟨L⊕, L⊖⟩over the same alphabet Σ, called a promise problem, and we
decide whether w ∈L⊕or w ∈L⊖by the use of a don’t care deterministic ﬁnite
automaton (dcDFA) which, besides accepting and rejecting states, may also use
neutral or “don’t care” states, otherwise it behaves like a standard DFA (see
e.g. [6,13]). In our settings, L⊕= h(L) and L⊖= h(LC), where h : Σ∗→{0, 1}∗is
a code. We shall concentrate on the most common binary codes used in practice,
that allow decoding by one-way deterministic ﬁnite-state transducers in real
time and minimize 
a∈Σ |h(a)|, the sum of lengths of codewords. Such codes
are called maximal preﬁx codes in literature [1,3]. (See Deﬁnition 1.)
This paper shows that, for each maximal preﬁx code h : Σ∗→{0, 1}∗and
each optimal DFA A with n states recognizing some L over the alphabet Σ,
the binary promise problem ⟨h(L), h(LC)⟩can be solved by a dcDFA A′ using
at most (∥Σ∥−1)·n states, but at least n states.2 We also show that, for each
non-unary input alphabet Σ, there exists a maximal binary preﬁx code h such
that, for each n ≥2 and each N ∈{n, . . . , (∥Σ∥−1)·n}, there exists a language
L ⊆Σ∗such that the optimal DFA recognizing L uses exactly n states and any
optimal dcDFA for solving ⟨h(L), h(LC)⟩uses exactly N states.
1 State complexity of homomorphisms depends on the length of the images of symbols
and is somewhat diﬃcult to deﬁne in the general case. Perhaps the only existing
related result is the state complexity of projections (that is, homomorphisms map-
ping each symbol either to itself or to ε), which was determined to be 3/4·2n −1
in [12].
2 Throughout the paper, ∥X∥denotes the cardinality of the set X.

74
V. Geﬀert et al.
h
˜h
a5
a2
a0
a0
a1
a2
a3
a4
a1
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
Fig. 1. Examples of homomorphisms establishing some binary preﬁx codes. Each homo-
morphism is displayed as a tree in which each leaf represents some ai, a letter of the
original input alphabet; the edges are labeled so that the path from the root to ai
gives the corresponding string h(ai). Internal nodes of the tree are related to preﬁxes
of strings in {h(a) : a ∈Σ}. The code h (left), deﬁned by h(a0) = 1, h(a1) = 01,
h(a2) = 001, h(a3) = 0001, h(a4) = 00001, and h(a5) = 00000, is maximal, while the
code ˜h (right), with ˜h(a0) = 1, ˜h(a1) = 00011, and ˜h(a2) = 00010, is not—it can be
extended, e.g., by deﬁning ˜h(a3) = 01.
2
Preliminaries
Here we shall ﬁx some basic deﬁnitions, notation, and preliminary properties.
For more details, we refer the reader to [6,8], or any other standard textbooks.
Deﬁnition 1. A homomorphism between strings over two alphabets is a map-
ping h : Σ∗
1→Σ∗
2 preserving concatenation, i.e., h(α1·α2) = h(α1)·h(α2), for each
α1, α2 ∈Σ∗
1. The image of a language L ⊆Σ∗
1 is h(L) = {h(α) : α ∈L} ⊆Σ∗
2.
If h(α1) = h(α2) implies that α1 = α2, then h is called a code.
h is a preﬁx code, if no string in h(Σ1) = {h(a) : a ∈Σ1} is a proper preﬁx
of another one. The code h is maximal, if there is no other code h′ : Σ′
1
∗→Σ∗
2
(for some Σ′
1 ⊇Σ1) such that h′(Σ′
1) is a proper superset of h(Σ1).
Each homomorphism h is completely determined by the strings in h(Σ1), since
h(a1· · ·aℓ) = h(a1)· · ·h(aℓ), for each a1, . . . , aℓ∈Σ1. In addition, if h is a code,
each β ∈h(Σ∗
1) has a unique factorization into β = β1· · ·βℓ, where ℓ≥0 and
β1, . . . , βℓ∈h(Σ1). For examples of codes, see Fig. 1.
Preﬁx codes allow easy decoding by a one-way deterministic ﬁnite-state
machine such that, for the given β ∈h(Σ∗
1), it computes the factorization of β
into h(a1)· · ·h(aℓ) and prints a1· · ·aℓon the output in real time [1, Prop. 5.1.6].
Maximal codes minimize 
a∈Σ |h(a)| and do not have “gaps” in images: each
β ∈Σ∗
2 can be extended to an image of some α ∈Σ∗
1, that is, to β·β′ = h(α),
for some β′ ∈Σ∗
2 and some α ∈Σ∗
1, not excluding β′ = ε.
Since we shall deal with binary codes only, we are going to simplify notation
and write Σ instead of Σ1 and ﬁx Σ2 = {0, 1}.
Deﬁnition 2. A don’t care deterministic ﬁnite automaton (dcDFA) is a 6-tuple
A = ⟨Q, Σ, qI, f, F ⊕, F ⊖⟩, in which Q is a ﬁnite set of states; Σ is a ﬁnite input
alphabet; qI ∈Q is the initial state; f : Q × Σ →Q is a transition function;
F ⊕⊆Q is the set of accepting states; and F ⊖⊆Q the set of rejecting states,
F ⊕∩F ⊖= Ø. The remaining states are called neutral or “don’t care” states.

Binary Coded Regular Languages
75
A (standard) deterministic ﬁnite automaton (DFA) is a 5-tuple A
=
⟨Q, Σ, qI, f, F⟩, with F ⊆Q denoting the set of accepting states and Q\F the set
of rejecting states; the remaining components have the same meaning as above.
The transition function f can be extended to f ∗: Q × Σ∗→Q in a natural
way, taking by deﬁnition f ∗(q, ε) = q and f ∗(q, αa) = f(f ∗(q, α), a), for each
q ∈Q, α ∈Σ∗, and a ∈Σ. To simplify notation, f ∗(q, α) = q′ shall sometimes
be displayed in a more compact form q
α
−→q′.
A promise problem (see e.g. [6]) is a pair of disjoint languages ⟨L⊕, L⊖⟩over
the same alphabet Σ. The promise problem is solved by a dcDFA A, if A accepts
each w ∈L⊕(that is, f ∗(qI, w) ∈F ⊕) and rejects each w ∈L⊖(f ∗(qI, w) ∈F ⊖).
We do not have to worry about the outcome on inputs belonging neither to L⊕
nor to L⊖: on such inputs, A may accept, reject, or halt in a neutral state.
A is optimal for ⟨L⊕, L⊖⟩, if it solves ⟨L⊕, L⊖⟩and there is no dcDFA A′
that solves ⟨L⊕, L⊖⟩with fewer states than does A.
If L⊕∪L⊖= Σ∗, then A has no neutral reachable states and can be viewed
as a standard DFA; we have the standard language recognition and say that
L⊕is recognized by A. The language L⊕is then usually denoted by L(A) and its
complement L⊖by L(A)C. In this case, the concept of optimal dcDFA coincide
with the concept of minimal DFA for L⊕.
Note that if a promise problem ⟨L⊕, L⊖⟩can be solved by a dcDFA A with
n states, it can also be solved by a standard dcDFA A′ with n states, none of
which is neutral: any neutral state could be set as accepting or rejecting, without
aﬀecting ⟨L⊕, L⊖⟩. This leaves us some degree of freedom, leading to diﬀerent
machines. Namely, if A uses k neutral reachable states, we obtain 2k diﬀerent
automata solving the same promise problem, all of them of size at most n.
These automata do not agree in acceptance/rejection on inputs not belonging to
L⊕∪L⊖. Thus, the given dcDFA can also be viewed as a more concise template
representing these 2k DFAs.
This is related to the following separation problem: given DFAs A⊕and A⊖
for two disjoint languages L⊕and L⊖, ﬁnd a DFA A′ with minimal number
of states, such that L⊕⊆L(A′) and L⊖⊆L(A′)C. In general, this problem
is NP-complete [13, Thm. 9]. This was shown by a simple application of NP-
completeness for a slightly diﬀerent computational model (in which some tran-
sitions f(q, a) may be undeﬁned), presented in [14,15].
The next theorem will play the same role for don’t care automata as the
fooling set technique [2] for standard automata:
Theorem 1. Let L⊕, L⊖be two disjoint languages over the same alphabet Σ.
Suppose there exist m-tuple X = ⟨xe⟩m
e=1 and
m
2

-tuple Y = ⟨ye,˜e⟩m
e,˜e=1, e<˜e
consisting of strings in Σ∗such that, for each e, ˜e ∈{1, . . . , m}, e < ˜e,
(I) both xe·ye,˜e and x˜e·ye,˜e are in L⊕∪L⊖,
(II) xe·ye,˜e ∈L⊕if and only if x˜e·ye,˜e ∈L⊖.
Then any dcDFA solving the promise problem ⟨L⊕, L⊖⟩uses at least m states.

76
V. Geﬀert et al.
Proof. Let A = ⟨Q, Σ, qI, f, F ⊕, F ⊖⟩, satisfying F ⊕∩F ⊖= Ø, be an arbitrary
dcDFA for solving ⟨L⊕, L⊖⟩. Suppose that A uses less than m states.
Now, for the given m-tuple X = ⟨x1, x2, . . . , xm⟩, let q1, q2, . . . , qm denote
the respective states reached by A on these inputs, that is, qI
xe
−→qe, for each
e ∈{1, . . . , m}. But then, using a pigeonhole argument, some state must be
repeated, i.e., we have qe = q˜e, for some 1 ≤e < ˜e ≤m. This implies that the
computations on inputs xe·ye,˜e and x˜e·ye,˜e must end in the same state, denoted
here by r. That is, we have the following computations: qI
xe
−→qe
ye,˜e
−−−−→r and
qI
x˜e
−→q˜e = qe
ye,˜e
−−−−→r. There are now two possibilities:
First, let xe·ye,˜e ∈L⊕. Then, using (II), we also have that x˜e·ye,˜e ∈L⊖. This
implies that the computation on xe·ye,˜e ends in r ∈F ⊕and, at the same time,
the computation on x˜e·ye,˜e in r ∈F ⊖. But this is a contradiction: F ⊕∩F ⊖= Ø.
Second, let xe·ye,˜e /∈L⊕. Then, using (II), we have x˜e·ye,˜e /∈L⊖. Now, by (I),
we see that xe·ye,˜e ∈L⊖and x˜e·ye,˜e ∈L⊕. This leads to the same kind of
contradiction as above, swapping the roles of xe·ye,˜e and x˜e·ye,˜e.
⊓⊔
Note that, apart from providing the lower bound on the number of states,
the statement of the above theorem does not deal with states in any dcDFA
solving ⟨L⊕, L⊖⟩but, rather, with strings in Σ∗. However, if there does exist
a dcDFA A with m states for ⟨L⊕, L⊖⟩, that is, if the lower bound provided
by Theorem 1 matches the upper bound, then one can establish a one-to-one
correspondence between states in A and strings in X = ⟨x1, x2, . . . , xm⟩, with
Y = ⟨ye,˜e⟩m
e,˜e=1, e<˜e giving the pairwise distinguishability of states in A. The
standard fooling set technique (for DFAs) uses ye,˜e1 = ye,˜e2 = . . . , with the
condition (I) satisﬁed automatically.
3
Upper and Lower Bounds
We are now going to show that (∥Σ∥−1)·n states are suﬃcient but n states
necessary for a dcDFA that decides whether the given binary input is in h(L) or
in h(LC), that is, for a dcDFA solving ⟨h(L), h(LC)⟩, the binary promise-problem
version of L. Here h : Σ∗→{0, 1}∗is a maximal preﬁx code and A is an optimal
DFA with n states, recognizing a language L over a non-unary alphabet Σ.
For the given code h, let us begin by ﬁxing some additional notation for the
images of letters and proper preﬁxes:
H = {h(a) : a ∈Σ},
P = {π : π·β ∈H, for some β ∈{0, 1}+}.
(1)
Recall that h is a maximal preﬁx code. Thus, P includes the empty string ε, but
no string from H. Next, if π ∈P, then π·0, π·1 ∈P ∪H (see also Fig. 1). The
next two theorems provide the upper and lower bounds.
Theorem 2. Let h : Σ∗→{0, 1}∗be a maximal binary preﬁx code and let L be a
language over the non-unary alphabet Σ. Then, if L can be recognized by a DFA
A = ⟨Q, Σ, qI, f, F⟩with n states, the binary promise problem ⟨h(L), h(LC)⟩can
be solved by a dcDFA A′ with at most n′ ≤(∥Σ∥−1)·n states.

Binary Coded Regular Languages
77
Proof (a sketch). The idea of the construction is to remember q ∈Q, the current
state of A at the moment when A has read a1· · ·aℓ∈Σ∗, and π ∈P, the preﬁx
of a code for the next input symbol aℓ+1, not completed yet. This leads to
Q′ = Q×P, with q′
I = ⟨qI, ε⟩, F ⊕= F×{ε}, and F ⊖= (Q\F)×{ε}. Transitions
in A′ are deﬁned as follows, for each q ∈Q, π ∈P, and b ∈{0, 1}:
(I) f ′(⟨q, π⟩, b) = ⟨q, πb⟩, provided that π·b ∈P,
(II) f ′(⟨q, π⟩, b) = ⟨f(q, a), ε⟩, provided that, for some a ∈Σ, π·b = h(a) ∈H.
⊓⊔
The above construction can be updated so that it works for preﬁx codes that
are not maximal. Then π ∈P does not imply that π·b is in P ∪H. In such cases,
we can deﬁne f ′(⟨q, π⟩, b) = q′
E, where q′
E is an additional trap state, in which
we scan the rest of the input—the input can no longer be extended to a string
in h(Σ∗
1) = h(H∗). However, for such codes, ∥P∥is not bounded by ∥Σ∥−1.
Theorem 3. Let h : Σ∗→{0, 1}∗be a binary preﬁx code (not necessarily maxi-
mal) and let L be a language over the alphabet Σ (not necessarily non-unary).
Then, if the binary promise problem ⟨h(L), h(LC)⟩can be solved by a dcDFA
A′ = ⟨Q′, {0, 1}, q′
I, f ′, F ⊕, F ⊖⟩with n′ states, the language L can be recognized
by a standard DFA A = ⟨Q, Σ, qI, f, F⟩with at most n ≤n′ states.
Proof. If, for some q, q′ ∈Q′ and a ∈Σ, the original A′ has a path beginning
in q, ending in q′, and reading from the input the string h(a) ∈H (introduced
by (1)), we shall add the transition q
a
−→q′ to A. Recall that A′ is deterministic
and h(a) = b1· · ·bm is unique for each a ∈Σ. But then q′ = f ′∗(q, h(a)) is also
unique for each q ∈Q′ and each a ∈Σ, and hence A will be deterministic.
In addition, we can restrict the set of ﬁnite control states in A to states that
can be reached from q′
I by reading some β ∈h(Σ∗) = H∗, that is, to
R = {f ′∗(q′
I, β) : β ∈H∗}.
(2)
Thus, Q = R, qI = q′
I, and F = R ∩F ⊕. Clearly, R ⊆F ⊕∪F ⊖, since no A′
solving ⟨h(L), h(LC)⟩halts in a neutral state on any h(α) ∈h(Σ∗). Finally, let
f(q, a) = f ′∗(q, h(a)), for each q ∈R and each a ∈Σ.
It is easy to see, for each q, q′ ∈R and each a ∈Σ, that A has a transition
from q to q′ reading the symbol a ∈Σ if and only if A′ has a path connecting
the same states and reading the string h(a) ∈H. By a straightforward induction
on the length of α ∈Σ∗, we have q
α
−→q′ in A if and only if q
h(α)
−−−−→q′ in A′.
Thus, if α ∈L, then h(α) ∈h(L) must be accepted by dcDFA A′, which
gives q′
I
h(α)
−−−−→q′ for some q′ ∈F ⊕. Moreover, since h(α) ∈H∗, q′ must also
belong to R. But then, for A, we have qI = q′
I
α
−→q′ with q′ ∈R ∩F ⊕= F, and
hence α is accepted by A. Similarly, if α /∈L, then A′ has a path q′
I
h(α)
−−−−→q′
ending in q′ ∈R ∩F ⊖. For A, this gives qI = q′
I
α
−→q′ ending in q′ ∈R\F ⊕=
R\(R ∩F ⊕) = Q\F, and hence α is rejected by A.
Summing up, A is a standard DFA recognizing L, with n ≤n′ states.
⊓⊔

78
V. Geﬀert et al.
By combining Theorems 2 and 3, we get:
Theorem 4. Let h : Σ∗→{0, 1}∗be a maximal binary preﬁx code and let L be a
language over the non-unary alphabet Σ. Then, if the optimal DFA A recogniz-
ing L uses n states, any optimal dcDFA A′ solving the binary promise problem
⟨h(L), h(LC)⟩uses at least n states and at most (∥Σ∥−1)·n states.
Proof. The upper bound is a direct consequence of Theorem 2: this theorem
does not necessarily produce a dcDFA that is optimal, however, it does guarantee
(∥Σ∥−1)·n states for A′. The lower bound follows from Theorem 3: suppose that
⟨h(L), h(LC)⟩can be solved by A′ with n′ < n states. But then, by this theorem,
we could obtain a standard DFA recognizing L with fewer states than n, which
is a contradiction, since A is optimal.
⊓⊔
4
The Hierarchy
We are now ready to establish the complete state hierarchy and provide a witness
language for each N between n and (∥Σ∥−1)·n. First, for the given
Σ = {a0, a1, . . . , ad}, where d = ∥Σ∥−1 ≥2,
deﬁne a maximal binary preﬁx code hΣ : Σ∗→{0, 1}∗as follows:
hΣ(aj) = 0j1, for j ∈{0, . . . , d −1},
hΣ(ad) = 0d.
(3)
Second, for the given Σ and any given
n ≥2,
g ∈{0, . . . , n},
k ∈{0, . . . , d −2},
(4)
deﬁne a DFA AΣ,n,g,k = ⟨Q, Σ, qI, f, F⟩with the state set Q = {0, . . . , n −1},
qI = 0, F = {0}, and the following transitions:
f(i, a) = (i + 1) mod n,
if i < g and a ∈Σ,
f(g, a) = (g + 1) mod n,
if g ≤n −2 and a ∈{a0, . . . , ak} ∪{ad},
= (g + 2) mod n,
if g ≤n −2 and a ∈{ak+1, . . . , ad−1},
f(i, a) = (i + 1) mod n,
if i > g and a ∈Σ\{ad},
= i,
if i > g and a = ad.
(5)
There are two special cases. The ﬁrst one is g = n −1, with no states i ∈Q
satisfying i > g. Moreover, this case diﬀers in one value, namely, in f(g, ad):
f(g, a) = (g + 1) mod n = 0, if g = n −1 and a ∈{a0, . . . , ak},
= (g + 2) mod n = 1, if g = n −1 and a ∈{ak+1, . . . , ad}.
(6)
The second special case is g = n, with no states i ∈Q satisfying i > g or i = g.
Thus, the condition i < g is satisﬁed automatically for each i ∈Q, which reduces
(5)–(6) above to f(i, a) = (i + 1) mod n for each i ∈Q and each a ∈Σ.

Binary Coded Regular Languages
79
Fig. 2. Examples of a DFA AΣ,n,g,k (bottom) and the corresponding dcDFA A′
Σ,n,g,k
(top), if Σ = {a0, . . . , a5}, d = 5, n = 7, g = 3, and k = 2. Accepting states are tagged
by “+”, rejecting states by “−”, and neutral don’t care states by no sign. To simplify
notation for A′
Σ,n,g,k, the ordered pairs “⟨i, j⟩” are displayed here in the form “ij”.
Examples of hΣ, AΣ,n,g,k, and subsequent A′
Σ,n,g,k
are displayed in
Fig. 1 (left), Fig. 2 (bottom), and Fig. 2 (top), respectively, for d = 5, n = 7,
g = 3, and k = 2. The special case of g = n −1 is illustrated by Fig. 3.
Finally, for the given Σ, n, g, k satisfying (4), consider a dcDFA A′
Σ,n,g,k =
⟨Q′, {0, 1}, q′
I, f ′, F ⊕, F ⊖⟩, not constructed for AΣ,n,g,k by the use of Theorem 2,
but deﬁned as follows. First, let Q′ = Q0 ∪. . . ∪Qn−1, where
Qi = {⟨i, 0⟩, . . . , ⟨i, d −1⟩}, for i ∈{0, . . . , g −1},
Qg = {⟨g, 0⟩, . . . , ⟨g, k⟩},
Qi = {⟨i, 0⟩},
for i ∈{g + 1, . . . , n −1}.
(7)
There are no sections Qi with i > g, if g = n −1, and no section Qg, if g = n,
in accordance with the two special cases for AΣ,n,g,k.
Now, let q′
I = ⟨0, 0⟩, F ⊕= {⟨0, 0⟩}, and F ⊖= {⟨1, 0⟩, ⟨2, 0⟩, . . . , ⟨n −1, 0⟩}.
Transitions are deﬁned as follows:
f ′(⟨i, j⟩, 1) = ⟨(i + 1) mod n, 0⟩,
for each ⟨i, j⟩∈Q′,
f ′(⟨i, j⟩, 0) = ⟨i, j + 1⟩,
if i < g and j < d −1,
= ⟨(i + 1) mod n, 0⟩,
if i < g and j = d −1,
f ′(⟨g, j⟩, 0) = ⟨g, j + 1⟩,
if j < k,
= ⟨(g + 1) mod n, 0⟩
= ⟨g + 1, 0⟩, if g ≤n −2 and j = k,
= ⟨(g + 1) mod n, j + 1⟩= ⟨0, k + 1⟩, if g = n −1 and j = k,
f ′(⟨i, 0⟩, 0) = ⟨i, 0⟩,
if i > g.
(8)
Note that also here the case of g = n −1 is diﬀerent. (See also Fig. 3).
Lemma 1. Let hΣ, AΣ,n,g,k, and A′
Σ,n,g,k be the binary code, DFA, and
dcDFA deﬁned above. Then A′
Σ,n,g,k
solves the binary promise problem
⟨hΣ(L(AΣ,n,g,k)), hΣ(L(AΣ,n,g,k)C)⟩.

80
V. Geﬀert et al.
Fig. 3. Examples of AΣ,n,g,k and A′
Σ,n,g,k for the special case of g = n −1, namely,
for Σ = {a0, . . . , a5}, d = 5, n = 7, g = n −1 = 6, and k = 2 (graph rotated, so that
the state g = n −1 is not displayed at the right end).
Proof. First, it is not too hard to see that if AΣ,n,g,k has a transition i
a
−→i′,
then A′
Σ,n,g,k has the corresponding computation path ⟨i, 0⟩
hΣ(a)
−−−−→⟨i′, 0⟩. This
can be shown by consulting (3) and by comparing all transitions presented by
(5), (6), and (8), for each i ∈Q and each a ∈Σ:
– f(i, a) = (i + 1) mod n, if i < g and a ∈Σ (which covers the case of g = n):
• ⟨i, 0⟩
0j
−→⟨i, j⟩
1
−→⟨(i+1) mod n, 0⟩, for hΣ(a) = 0j1, j ∈{0, . . . , d−1},
• ⟨i, 0⟩
0d−1
−−−−→⟨i, d −1⟩
0
−→⟨(i + 1) mod n, 0⟩, for hΣ(a) = 0d.
– f(g, a) = (g + 1) mod n, if g ≤n −2 and a ∈{a0, . . . , ak} ∪{ad}:
• ⟨g, 0⟩
0j
−→⟨g, j⟩
1
−→⟨(g + 1) mod n, 0⟩, for hΣ(a) = 0j1, j ∈{0, . . . , k},
• ⟨g, 0⟩
0k
−→⟨g, k⟩
0
−→⟨g + 1, 0⟩0d−k−1
−−−−→⟨g + 1, 0⟩= ⟨(g + 1) mod n, 0⟩,
for hΣ(a) = 0d.
– f(g, a) = (g + 2) mod n, if g ≤n −2 and a ∈{ak+1, . . . , ad−1}:
• ⟨g, 0⟩
0k
−→⟨g, k⟩
0
−→⟨g + 1, 0⟩0j−k−1
−−−−→⟨g + 1, 0⟩
1
−→⟨(g + 2) mod n, 0⟩,
for hΣ(a) = 0j1, j ∈{k + 1, . . . , d −1}.
– f(i, a) = (i + 1) mod n, if i > g and a ∈Σ\{ad}:
• ⟨i, 0⟩
0j
−→⟨i, 0⟩
1
−→⟨(i+1) mod n, 0⟩, for hΣ(a) = 0j1, j ∈{0, . . . , d−1}.
– f(i, a) = i, if i > g and a = ad:
• ⟨i, 0⟩
0d
−→⟨i, 0⟩, for hΣ(a) = 0d.
The same can be seen for diﬀerent transitions in the case of g = n −1:
– f(g, a) = (g + 1) mod n = 0, if g = n −1 and a ∈{a0, . . . , ak}:
• ⟨g, 0⟩
0j
−→⟨g, j⟩
1
−→⟨(g + 1) mod n, 0⟩, for hΣ(a) = 0j1, j ∈{0, . . . , k}.
– f(g, a) = (g + 2) mod n = 1, if g = n −1 and a ∈{ak+1, . . . , ad}:
• ⟨g, 0⟩
0k
−→⟨g, k⟩
0
−→⟨0, k+1⟩0j−k−1
−−−−→⟨0, j⟩
1
−→⟨1, 0⟩= ⟨(g+2) mod n, 0⟩,
for hΣ(a) = 0j1, j ∈{k + 1, . . . , d −1},
• ⟨g, 0⟩
0k
−→⟨g, k⟩
0
−→⟨0, k + 1⟩0d−k−2
−−−−→⟨0, d −1⟩
0
−→⟨1, 0⟩=
⟨(g + 2) mod n, 0⟩, for hΣ(a) = 0d.

Binary Coded Regular Languages
81
Now, by induction on the length of α = ai1· · ·aiℓ∈Σ∗, we easily obtain
that the computation path i
α
−→i′ in AΣ,n,g,k implies the existence of the corre-
sponding path ⟨i, 0⟩
hΣ(α)
−−−−→⟨i′, 0⟩for A′
Σ,n,g,k, for each i, i′ ∈Q and each α ∈Σ.
Thus, if AΣ,n,g,k has a path qI = 0
α
−→0 ∈F, then A′
Σ,n,g,k has the cor-
responding path q′
I = ⟨0, 0⟩
hΣ(α)
−−−−→⟨0, 0⟩∈F ⊕, and hence hΣ(α) is accepted
by A′
Σ,n,g,k, if α ∈L(AΣ,n,g,k). On the other hand, if this path in AΣ,n,g,k halts
in some i′ ̸= 0, that is, in some i′ ∈Q\F, the corresponding path in A′
Σ,n,g,k will
halt in ⟨i′, 0⟩∈F ⊖, and hence hΣ(α) is rejected by A′
Σ,n,g,k, if α ∈L(AΣ,n,g,k)C.
Therefore, A′
Σ,n,g,k is a valid dcDFA for solving the binary promise problem
⟨hΣ(L(AΣ,n,g,k)), hΣ(L(AΣ,n,g,k)C)⟩.
⊓⊔
A′
Σ,n,g,k uses fewer states than dcDFA obtained by the use of Theorem 2,
but it may accept/reject some binary inputs that are not images of any α ∈Σ∗.
That is, it does not necessarily halt in a neutral state on such inputs.
Lemma 2. Let AΣ,n,g,k be the DFA deﬁned above. Then AΣ,n,g,k is optimal and
uses exactly n states.
Proof. Using (5) and (6), we see that f(i, a0) = (i + 1) mod n, for each i ∈Q =
{0, . . . , n−1}, not excluding the special cases of g = n−1 or g = n. Since qI = 0
and F = {0}, ae
0 ∈L(AΣ,n,g,k) if and only if e is an integer multiple of n.
This implies that AΣ,n,g,k cannot be replaced by an equivalent DFA
using fewer states: such DFA would accept an
0
by a computation path
r0
a0
−→r1
a0
−→r2
a0
−→· · ·
a0
−→rn along which some state would be repeated,
which gives a valid accepting computation path for some ae
0 with 0 < e < n, a
contradiction.
⊓⊔
Lemma 3. Let hΣ, AΣ,n,g,k, and A′
Σ,n,g,k be the binary code, DFA, and dcDFA
deﬁned above. Then A′
Σ,n,g,k is optimal for solving the binary promise problem
⟨hΣ(L(AΣ,n,g,k)), hΣ(L(AΣ,n,g,k)C)⟩and uses exactly m = (∥Σ∥−1)·g + (k +
1)+(n−g −1) states, if g ≤n−1, but exactly m = (∥Σ∥−1)·n states, if g = n.
Proof. By Lemma 1, A′
Σ,n,g,k solves the given promise problem and, by (7), it
uses m = d·g +(k +1)+(n−g −1) states, if g ≤n−1. For g = n, all sections Qi
are of equal size d in (7), which gives m = d·n. Since d = ∥Σ∥−1, the upper
bound for m = ∥Q′∥follows.
We only have to show that this bound cannot be reduced. Let ⟨i, j⟩, ⟨˜ı, ˜j⟩
represent two arbitrary—but diﬀerent—states in Q′, that is, either i < ˜ı, or
i = ˜ı but j < ˜j. For i = ˜ı we have two subcases, depending on whether i < g or
i = g. (There are no pairs of diﬀerent states with i = ˜ı > g; this condition implies
j = ˜j = 0, by (7). We do not consider i > ˜ı, or i = ˜ı with j > ˜j either—the roles
of ⟨i, j⟩, ⟨˜ı, ˜j⟩can be swapped). Let us now deﬁne the following binary strings:
x⟨i,j⟩= 1i·0j,
for each ⟨i, j⟩∈Q′,
y⟨i,j⟩⟨˜ı,˜j⟩= 1·1n−i−1,
if 0 ≤i < ˜ı ≤n −1,
y⟨i,j⟩⟨i,˜j⟩= 0d−˜j1·1n−i−1,
if 0 ≤i < g and 0 ≤j < ˜j ≤d −1,
y⟨g,j⟩⟨g,˜j⟩= 0k+1−˜j1·1n−g−1, if g ≤n −1 and 0 ≤j < ˜j ≤k.
(9)

82
V. Geﬀert et al.
It can be seen from (8) that each state ⟨i, j⟩∈Q′ is reached by reading x⟨i,j⟩
from the input: q′
I = ⟨0, 0⟩
1i
−→⟨i mod n, 0⟩= ⟨i, 0⟩
0j
−→⟨i, j⟩.
If g = n −1, we get y⟨g,j⟩⟨g,˜j⟩= 0k+1−˜j1, if g = n, we do not deﬁne y⟨g,j⟩⟨g,˜j⟩.
We are now going to show that (i) both x⟨i,j⟩·y⟨i,j⟩⟨˜ı,˜j⟩and x⟨˜ı,˜j⟩·y⟨i,j⟩⟨˜ı,˜j⟩are
valid binary images of some strings in Σ∗, i.e., both of them are in hΣ(Σ∗), and
that (ii) the computation of A′
Σ,n,g,k on x⟨i,j⟩·y⟨i,j⟩⟨˜ı,˜j⟩starts in q′
I = ⟨0, 0⟩and
ends in ⟨0, 0⟩∈F ⊕, while the computation on x⟨˜ı,˜j⟩·y⟨i,j⟩⟨˜ı,˜j⟩starts in q′
I = ⟨0, 0⟩
and ends in some ⟨i′, 0⟩∈F ⊖, with i′ ̸= 0. Taking into account (i), this gives that
x⟨i,j⟩·y⟨i,j⟩⟨˜ı,˜j⟩∈hΣ(L(AΣ,n,g,k)) and x⟨˜ı,˜j⟩·y⟨i,j⟩⟨˜ı,˜j⟩∈hΣ(L(AΣ,n,g,k)C). These
statements can be shown by analyzing all cases, using (9), (3), and (8) (see also
Figs. 2 and 3):
– If 0 ≤i < ˜ı ≤n −1, and hence 0 < ˜ı −i ≤n −1, with j, ˜j ∈{0, . . . , d −1}:
• x⟨i,j⟩·y⟨i,j⟩⟨˜ı,˜j⟩= 1i·0j·1·1n−i−1 = hΣ(ai
0·aj·an−i−1
0
),
⟨0, 0⟩
1i0j
−→⟨i, j⟩
1
−→⟨(i + 1) mod n, 0⟩1n−i−1
−−−−→⟨0, 0⟩,
• x⟨˜ı,˜j⟩·y⟨i,j⟩⟨˜ı,˜j⟩= 1˜ı·0˜j·1·1n−i−1 = hΣ(a˜ı
0·a˜j·an−i−1
0
),
⟨0, 0⟩
1˜ı0˜j
−→⟨˜ı, ˜j⟩
1
−→⟨(˜ı + 1) mod n, 0⟩1n−i−1
−−−−→⟨˜ı −i, 0⟩̸= ⟨0, 0⟩.
– If 0 ≤i < g and 0 ≤j < ˜j ≤d −1, and hence 1 ≤j + d −˜j ≤d −1:
• x⟨i,j⟩·y⟨i,j⟩⟨i,˜j⟩= 1i·0j·0d−˜j1·1n−i−1 = hΣ(ai
0·aj+d−˜j·an−i−1
0
),
⟨0, 0⟩
1i0j
−→⟨i, j⟩
0d−˜j
−−−−→⟨i, j + d −˜j⟩
1
−→⟨(i + 1) mod n, 0⟩1n−i−1
−−−−→⟨0, 0⟩,
• x⟨i,˜j⟩·y⟨i,j⟩⟨i,˜j⟩= 1i·0˜j·0d−˜j1·1n−i−1 = hΣ(ai
0·ad·an−i
0
),
⟨0, 0⟩
1i0˜j
−→⟨i, ˜j⟩0d−1−˜j
−−−−→⟨i, d −1⟩
0
−→⟨(i + 1) mod n, 0⟩
1n−i
−−−−→⟨1, 0⟩.
– If g ≤n −1 and 0 ≤j < ˜j ≤k, and hence 1 ≤j + k + 1 −˜j ≤k:
• x⟨g,j⟩·y⟨g,j⟩⟨g,˜j⟩= 1g·0j·0k+1−˜j1·1n−g−1 = hΣ(ag
0·aj+k+1−˜j·an−g−1
0
),
⟨0, 0⟩1g0j
−→⟨g, j⟩0k+1−˜j
−−−−→⟨g, j+k+1−˜j⟩
1
−→⟨(g+1) mod n, 0⟩1n−g−1
−−−−→⟨0, 0⟩,
• x⟨g,˜j⟩·y⟨g,j⟩⟨g,˜j⟩= 1g·0˜j·0k+1−˜j1·1n−g−1 = hΣ(ag
0·ak+1·an−g−1
0
),
⟨0, 0⟩1g0˜j
−→⟨g, ˜j⟩
0k−˜j
−−−−→⟨g, k⟩, with two diﬀerent ways to continue:
if g ≤n −2, then ⟨g, k⟩
0
−→⟨g + 1, 0⟩
1n−g
−−−−→⟨(n + 1) mod n, 0⟩= ⟨1, 0⟩,
if g = n −1, then ⟨g, k⟩
0
−→⟨0, k + 1⟩
1n−g
−−−−→⟨(n −g) mod n, 0⟩= ⟨1, 0⟩.
Summing up, we have constructed m-tuple X = ⟨x⟨i,j⟩⟩⟨i,j⟩∈Q′ and
m
2

-tuple
Y = ⟨y⟨i,j⟩⟨˜ı,˜j⟩⟩⟨i,j⟩,⟨˜ı,˜j⟩∈Q′, ⟨i,j⟩̸=⟨˜ı,˜j⟩, where m = ∥Q′∥, consisting of binary strings
such that, for each pair ⟨i, j⟩̸= ⟨˜ı, ˜j⟩,
– both x⟨i,j⟩·y⟨i,j⟩⟨˜ı,˜j⟩and x⟨˜ı,˜j⟩·y⟨i,j⟩⟨˜ı,˜j⟩are in
hΣ(Σ∗) = hΣ(L(AΣ,n,g,k)) ∪hΣ(L(AΣ,n,g,k)C),
– x⟨i,j⟩·y⟨i,j⟩⟨˜ı,˜j⟩∈hΣ(L(AΣ,n,g,k)) and x⟨˜ı,˜j⟩·y⟨i,j⟩⟨˜ı,˜j⟩∈hΣ(L(AΣ,n,g,k)C).
But then, by Theorem 1, any dcDFA solving the binary promise problem
⟨hΣ(L(AΣ,n,g,k)), hΣ(L(AΣ,n,g,k)C)⟩must use at least m = ∥Q′∥states, which
gives that A′
Σ,n,g,k is optimal.
⊓⊔
Theorem 5. For each non-unary input alphabet Σ, there exists a maximal
binary preﬁx code h : Σ∗→{0, 1}∗such that, for each n ≥2 and each value
N ∈{n, . . . , (∥Σ∥−1)·n}, there exists a language L ⊆Σ∗such that the opti-
mal DFA recognizing L uses exactly n states and any optimal dcDFA for solving
⟨h(L), h(LC)⟩, the binary promise-problem version of L, uses exactly N states.

Binary Coded Regular Languages
83
Proof. Let us handle the pathological case of Σ = {a0, a1} ﬁrst. There are only
two maximal preﬁx codes in this case, both of them are bijections from {a0, a1}
to {0, 1}, and none of them can change the state complexity of any language.
This corresponds to the fact that here N ∈{n, . . . , (∥Σ∥−1)·n} = {n}.
Now, for the given Σ, with ∥Σ∥≥3, let us ﬁx h = hΣ, as introduced by (3).
Next, the witness language L depends on Σ and on the given values n and N:
First, if N ≤(∥Σ∥−1)·n −1 = d·n −1, we can take L = L(AΣ,n,g,k), using
the following parameters:
g = ⌊(N −n)/(d −1)⌋, k = (N −n) mod (d −1).
Clearly, g ≤⌊(d·n −1 −n)/(d −1)⌋= ⌊n −
1
d−1⌋= n −1 and k ≤d −2. By
Lemma 2, AΣ,n,g,k is the optimal DFA for recognizing L, using exactly n states.
Similarly, by Lemma 3, A′
Σ,n,g,k is optimal dcDFA for solving ⟨hΣ(L), hΣ(LC)⟩
and uses exactly m = d·g + (k + 1) + (n −g −1) states. This gives m =
d·g + (k + 1) + (n −g −1) = (d −1)·g + k + n = (d−1)·⌊(N −n)/(d −1)⌋+
(N −n) mod (d −1) + n = (N −n) + n = N states, using the fact that
a·⌊b/a⌋+ b mod a = b.
Second, if N = (∥Σ∥−1)·n, take L = L(AΣ,n,g,k) with g = n and k = 0.
(Here AΣ,n,g,k does not actually depend on k.) Again, by Lemma 2, AΣ,n,g,k is
optimal and uses exactly n states and, by Lemma 3, A′
Σ,n,g,k is optimal for
solving ⟨hΣ(L), hΣ(LC)⟩, this time with exactly m = (∥Σ∥−1)·n = N states. ⊓⊔
5
Concluding Remarks
By a more careful analysis of the construction in Theorem 3, we see that it does
not increase the number of accepting or rejecting states. As a direct consequence,
if the optimal DFA A recognizing L uses n⊕accepting and n⊖rejecting states
(neither of these values can be reduced, since the optimal A is unique), then any
optimal dcDFA A′ solving the binary promise problem ⟨h(L), h(LC)⟩must use
at least n⊕accepting and at least n⊖rejecting states. But all states in A′ that
cannot be reached from q′
I by reading some β ∈h(Σ∗) can be made neutral (see
also (2) in the proof of Theorem 3). This will only change acceptance/rejection
to “don’t care” answers on some inputs not belonging to h(Σ∗).
This allows to establish some kind of pseudo-isomorphism between A and A′.
(Proof omitted here due to space constraints, to appear in a journal version.)
Namely, there exists a bijective function t : Q→F ⊕∪F ⊖that maps qI to q′
I,
F to F ⊕, and Q\F to F ⊖, preserving the machine’s transitions, i.e., t(f(q, a)) =
f ′∗(t(q), h(a)), for each q ∈Q and a ∈Σ. However, such pseudo-isomorphism
does not exclude, for some transition q
a
−→q′ in A, passing through some state
t(q′′) ∈F ⊕∪F ⊖along the corresponding path t(q)
h(a)
−−−−→t(q′) in A′—even more
than once in the meantime.3
3 This phenomenon can be seen in Fig. 2, where we have “3”
a4
−→“5” for AΣ,n,g,k.
Since h(a4) = “00001”, this corresponds to t(“3”) = “30”
00001
−−−−→“50” = t(“5”) for
A′
Σ,n,g,k, passing twice through t(“4”) = “40”.

84
V. Geﬀert et al.
There are more open questions in the related area than the known answers.
As an example, we do not know the cost of binary coded intersection; the same
holds for other basic operations with regular languages. It can be expected that
answers may depend also on the code h in use, and we expect some anomalies
for preﬁx codes that are not maximal.
References
1. Berstel, J., Perrin, D., Reutenauer, C.: Codes and Automata. Cambridge University
Press, Cambridge (2010)
2. Birget, J.-C.: Intersection and union of regular languages and state complexity.
Inform. Process. Lett. 43, 185–190 (1992)
3. Bruy`ere, V.: Maximal codes with bounded deciphering delay. Theoret. Comput.
Sci. 84, 53–76 (1991)
4. ˇCevorov´a, K.: Kleene star on unary regular languages. In: Jurgensen, H., Reis,
R. (eds.) DCFS 2013. LNCS, vol. 8031, pp. 277–288. Springer, Heidelberg (2013).
https://doi.org/10.1007/978-3-642-39310-5 26
5. Geﬀert, V.: Magic numbers in the state hierarchy of ﬁnite automata. Inform. Com-
put. 205, 1652–1670 (2007)
6. Goldreich, O.: On promise problems: a survey. In: Goldreich, O., Rosenberg, A.L.,
Selman, A.L. (eds.) Theoretical Computer Science. LNCS, vol. 3895, pp. 254–290.
Springer, Heidelberg (2006). https://doi.org/10.1007/11685654 12
7. Holzer, M., Rauch, C.: The range of state complexities of languages resulting from
the cascade product—the unary case (extended abstract). In: Maneth, S. (ed.)
CIAA 2021. LNCS, vol. 12803, pp. 90–101. Springer, Cham (2021). https://doi.
org/10.1007/978-3-030-79121-6 8
8. Hopcroft, J., Motwani, R., Ullman, J.: Introduction to Automata Theory, Lan-
guages, and Computation. Addison-Wesley, Boston (2001)
9. Hricko, M., Jir´askov´a, G., Szabari, A.: Union and intersection of regular languages
and descriptional complexity. In: Proceedings of Descriptional Complexity of For-
mal Systems, pp. 170–181. IFIP & University, Milano (2005)
10. Iwama, K., Kambayashi, Y., Takaki, K.: Tight bounds on the number of states of
DFA’s that are equivalent to n-state NFA’s. Theoret. Comput. Sci. 237, 485–494
(2000)
11. Jir´askov´a, G.: Magic numbers and ternary alphabet. Internat. J. Found. Comput.
Sci. 22, 331–344 (2011)
12. Jir´askov´a, G., Masopust, T.: On a structural property in the state complexity of
projected regular languages. Theoret. Comput. Sci. 449, 93–105 (2012)
13. Moreira, N., Pighizzini, G., Reis, R.: Optimal state reductions of automata with
partially speciﬁed behaviors. Theoret. Comput. Sci. 658, 235–245 (2017)
14. Paull, M.C., Unger, S.H.: Minimizing the number of states in incompletely speciﬁed
sequential switching functions. IRE Trans. Electron. Comput. 3, 356–367 (1959)
15. Pﬂeeger, C.P.: State reduction in incompletely speciﬁed ﬁnite-state machines. IEEE
Trans. Comput. C–22, 1099–1102 (1973)
16. Rabin, M., Scott, D.: Finite automata and their decision problems. IBM J. Res.
Develop. 3, 114–125 (1959)

Reset Complexity and Completely
Reachable Automata with Simple
Idempotents
Stefan Hoﬀmann(B)
Informatikwissenschaften, FB IV, Universit¨at Trier, Trier, Germany
hoffmanns@informatik.uni-trier.de
Abstract. Every regular ideal language is the set of synchronizing words
of some automaton. The reset complexity of a regular ideal language is
the size of such an automaton with the minimal number of states. The
state complexity is the size of a minimal automaton recognizing a regular
language in the usual sense. There exist regular ideal languages whose
state complexity is exponentially larger than its reset complexity. We
call an automaton sync-maximal, if the reset complexity of the ideal
language induced by its set of synchronizing words equals the number of
states of the automaton and the gap between the reset complexity and
the state complexity of this language is maximal possible. An automaton
is completely reachable, if we can map the whole state set to any non-
empty subset of states (for synchronizing automata, it is only required
that the whole state set can be mapped to a singleton set). We ﬁrst
state a general structural result for sync-maximal automata. This shows
that sync-maximal automata are closely related to completely reachable
automata. We then investigate automata with simple idempotents and
show that for these automata complete reachability and sync-maximality
are equivalent. Lastly, we ﬁnd that for automata with simple idempotents
over a binary alphabet, subset reachability problems that are PSPACE-
complete in general are solvable in polynomial time.
Keywords: ﬁnite automata · synchronization · set of synchronizing
words · automata with simple idempotents · completely reachable
automata · sync-maximal automata
1
Introduction
Let Σ be a ﬁnite set of symbols and Σ˚ be the free monoid with neutral element
ε (called the empty word). Languages are subsets of Σ˚. A language I Ď Σ˚ is
an ideal language if x, y P Σ˚ and u P I imply xuy P I. A (semi-)automaton is a
triple A “ (Q, Σ, δ) where Q is ﬁnite set of states and δ : Q ˆ Σ Ñ Q a (totally
deﬁned) transition function. Here, we simplify our notation by not mentioning δ
explicitly, i.e., we write q.a when applying the transition function δ to the state
q P Q and letter a P Σ and we write an automaton as a 2-tuple A “ (Q, Σ).
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 85–99, 2022.
https://doi.org/10.1007/978-3-031-13257-5_7

86
S. Hoﬀmann
The transition function is extended to a function Q ˆ Σ˚ Ñ Q (denoted
by the dot notation as well) by setting q.ε “ q and q.ua “ (q.u).a for u P Σ˚,
a P Σ and q P Q. Furthermore, we extend it to subsets S Ď Q by setting
S.u “ {q.u | q P S} for u P Σ˚.
A language L Ď Σ˚ is a regular language if there exists an automaton A “
(Q, Σ) and q0 P Q and F Ď Q such that L “ {u P Σ˚ : q0.u P F}. We say
the automaton A accepts (or recognizes) the language L. For a regular language
L Ď Σ˚ we denote by sc(L) “ min{|Q| : A “ (Q, Σ) accepts L} the state
complexity of L.
Set Syn(A) “ {u P Σ˚ | |Q.u| “ 1}, the set of synchronizing words of A
(which is an ideal language). If Syn(A) ‰ H, the automaton is called synchro-
nizing. For every n-state automaton A we have sc(Syn(A)) ď 2n ´ n [15]. We
call A sync-maximal if sc(Syn(A)) equals 2n ´ n. If I Ď Σ˚ is a regular ideal
language, then it is precisely the set of synchronizing words of the automaton
with the least number of states accepting it [9,15]. The reset complexity of I is
rc(I) “ min{|Q| : A “ (Q, Σ) with I “ Syn(A)},
i.e., the least number of states such that I is realized as the set of synchronizing
word of an automaton. We have rc(I) ď sc(I). There are known families of ideal
languages In associated to synchronizing automata such that sc(In) “ 2n ´ n
but rc(In) “ n [15]. For example, the set of synchronizing words of the ˇCern´y
family of automata Cn “ (Qn, {a, b}) deﬁned by ˇCern´y [6] for n ą 1 by
i.a “

i
if i ă n,
1
if i “ n;
i.b “

i ` 1
if i ă n,
1
if i “ n.
The automaton Cn is shown in Fig. 1.
Hence, describing regular ideal languages as the set of synchronizing words
can be exponentially more succinct than the usual notion of acceptance by
automata.
For L Ď Σ˚, let ||L|| “ min{|u| | u P L} be the length of a shortest word
in L. In combinatorial automata theory (and also in related applications, see [19]
for a survey in the context of model-based testing), the question on the length
of shortest synchronizing words arises. The ˇCern´y conjecture states that for an
n-state automaton A we have || Syn(A)|| ď (n´1)2. For the n-state automata Cn
from Fig. 1 we have || Syn(Cn)|| “ (n´1)2. The best general upper bound proven
so far is cubic in the number of states [21]. For more information and further
references, see the recent survey [22]. Investigating the length of shortest words
in regular ideal languages yields a natural approach to the ˇCern´y conjecture,
more precisely it is equivalent to the statement that rc(I) ě

||I|| ` 1 for every
regular ideal language I Ď Σ˚.
In fact, the connection between regular ideal languages and synchronizing
automata is even deeper. It can be shown that every regular ideal language
equals the set of synchronizing word of a strongly connected automaton [16].

Reset Complexity and Automata with Simple Idempotents
87
Automata with simple idempotents have been introduced in [18] and it has
been shown that a shortest synchronizing word has at most quadratic length for
these automata.
Overview and Contribution. In Sect. 2 we introduce automata with simple idem-
potents and further notation that was not already introduced in the introduction.
Then we make the conditions for sync-maximality more precise. Example 1 gives
an automaton that is completely reachable, but not sync-maximal.
Section 3 discusses completely reachable automata.
In Sect. 4 we determine the structure of sync-maximal automata. Our results
show that both notions are closely connected, as every sync-maximal automaton
contains a completely reachable subautomaton. In Example 2 we give automata
that are sync-maximal but not completely reachable.
Then in Sect. 5 we investigate automata with simple idempotents. Hence, this
chapter is concerned with the reset complexity of ideal languages induced by
automata with simple idempotents. However, we do not mention ideal languages
anymore, but rather express our result directly with automata (ideal languages
were introduced in the introduction to give a broader context of our results). We
show that an automaton with simple idempotents that is completely reachable is
sync-maximal. Note that Example 2 and Example 3 give automata with simple
idempotents that are sync-maximal, but not completely reachable. However, for
strongly connected automata with simple idempotents, it follows that complete
reachability and sync-maximality are equivalent.
Fig. 1. The automaton Cn
2
Some More Notation and Preliminary Results
Let f : X Ñ Y be a function. Here, function application is written on the
right, i.e., xf or (x)f denotes the function f applied to x. The same applies
to the extension to subsets, i.e., if S Ď X, then (S)f and Sf denote the set
{xf | x P S}. In this respect, if g : Y Ñ Z is another function, the function

88
S. Hoﬀmann
composition fg is the function x(fg) “ (xf)g, i.e. f is applied ﬁrst. This “right
action notation” deviates from the more usual “left action notation” f(x) used
in formal language theory. We chose this notation as, in our opinion, it makes
certain algebraic manipulations in Sect. 5 easier to read, as it conforms better
with the way function composition is deﬁned and read from left to right. This
notation1 is actually quite common in more algebraic approaches, for example,
in [1].
For n ě 0, we set [0] “ H and [n] “ {1, . . . , n} if n ą 0. If f : [n] Ñ [n] is a
permutation, i.e., a bijective mapping, then f ´1 : [n] Ñ [n] denotes the inverse
mapping with xff ´1 “ xf ´1f “ x for all x P [n].
For algebraic notions as semigroup, monoid, generating set etc., we refer
to the literature, e.g., the textbook [14]. By Tn we denote the transformation
monoid of all mappings on a ﬁnite set of cardinality n.
Sets with precisely k elements are called k-sets, and 1-sets are also called
singleton sets.
For f : X Ñ Y with X, Y ﬁnite, the defect is |X\{xf | x P X}|.
A mapping f : X Ñ X on a ﬁnite set X is a simple idempotent (mapping)
if it has defect one and for each x P X we have xff “ xf. Note that a simple
idempotent mapping is completely speciﬁed by the two points x P X and xf
with xf ‰ x. In Sect. 5 we need the following “cancellation property” of simple
idempotent mappings.
Lemma 1. Let f : [n] Ñ [n] be a simple idempotent mapping and A, B Ď [n]
with |A| “ |B|. If Af “ Bf and there exist x P A, y P B such that xf ‰ x and
yf ‰ y, then A “ B and x “ y.
Remark 1. Let f : {1, 2} Ñ {1, 2} with (1)f “ (2)f “ 2. Then {1}f “ {1, 2}f
and {1}f “ {2}f. The former equation shows that the assumption |A| “ |B|
is necessary in Lemma 1, the latter equation shows the assumption that the
element that is moved by f is contained in both sets is necessary.
Let A “ (Q, Σ) be an automaton. The defect of a letter a P Σ is the defect
of the induced function q →q.a for q P Q. A simple idempotent letter is a letter
that induces a simple idempotent mapping on the states, and a permutational
letter is a letter that induces a permutation on the state set. The automaton A
is an automaton with simple idempotents if every letter is either a permutational
letter or a simple idempotent letter. A subset S Ď Q deﬁnes (or induces) a
subautomaton if s.a P S for each s P S and a P Σ. In this case, the set S
together with the transition function restricted to S in the arguments and the
image gives a totally deﬁned function, i.e., we can regard it as an automaton on
its own.
The transformation monoid of the automaton A is the monoid generated by
the mappings q →q.a for q P Q induced by each letter a P Σ on the state set.
1 Note that we actually mix both notations, as we write certain operators (which are
never composed here), for example Syn(A), in the other convention. But this is also
done in the literature, for example [1], and should pose no problem.

Reset Complexity and Automata with Simple Idempotents
89
A state t P Q is reachable from another state s P Q if there exists u P Σ˚ such
s.u “ t. A strongly connected component is a maximal subset of states such that
for every two states in this subset are reachable from each other. The automaton
A is strongly connected if Q is a strongly connected component.
An automaton A is minimal [13] for {u P Σ˚ | q0.u P F} with q0 P Q and
F Ď Q if and only if every state is reachable from q0 and every pair of distinct
state q, q′ P Q is distinguishable, which means that there exists u P Σ˚ such that
precisely one of the two states q.u and q′.u is ﬁnal, i.e., the following holds true:
q.u P F if and only if q′.u R F.
The power automaton is PA “ ({S | H ‰ S Ď Q}, Σ) where the transi-
tion function of PA is the transition function of A extended to subsets. The
automaton A is completely reachable, if every non-empty subset is reachable
from the state Q in the power automaton of A. Setting F “ {{q} | q P Q},
as Syn(A) “ {u P Σ˚ | Q.u P F}, the power automaton accepts Syn(A). The
states in F can be merged into a single state to get another automaton accepting
Syn(A). Hence, sc(Syn(A)) ď 2n ´ n.
Translating the condition of minimality of an automaton to the speciﬁc lan-
guage Syn(A) and the power automaton, we ﬁnd that the sync-maximality of A
is equivalent to the following two conditions:
1. Every non-empty subset with at least two states is reachable in PA and at
least one singleton subset is reachable in PA.
2. For any two non-empty and distinct subsets S, T Ď Q with min{|S|, |T|} ě 2
there exists a word u P Σ˚ such that precisely one of the two subsets T.u and
S.u is a singleton subsets, i.e., both subset are distinguishable (in PA).
In [10, Lemma 3.1] (and a little more general in [12, Theorem 7]) it was
shown that distinguishability of states in the power automaton with respect to
Syn(A) can be simpliﬁed by only considering 2-sets.
Proposition 2. Let A “ (Q, Σ). Then all states in the power automaton PA
are distinguishable if and only if all 2-sets are distinguishable in PA.
Example 1. The automaton (see Fig. 2) with the state set Q “ {1, 2, 3}, input
letters a[1], a[2], a[3], a[1,2] and transition function given by
i.a[1] “

2
if i “ 1, 2,
3
if i “ 3;
i.a[2] “

1
if i “ 1, 2,
3
if i “ 3;
i.a[3] “

1
if i “ 1, 2,
2
if i “ 3;
i.a[1,2] “ 3 for all i “ 1, 2, 3.
is taken from [3, Example 2] as an example of a completely reachable automaton.
However, it is not sync-maximal as the two 2-sets {1, 3} and {2, 3} are not
distinguishable.
Next, we introduce two decision problems that have been investigated
in [2,3,17]. Subsets as in the latter problem were called totally extensible in [2],

90
S. Hoﬀmann
and in [17] the problem was called the global inclusion problem for non-initial
automata.
Deﬁnition 3. Reachable Subset
Input: A “ (Q, Σ) and H ‰ S Ď Q.
Question: Exists w P Σ˚ with Q.w “ S?
Deﬁnition 4. Sync-Into-Subset
Input: A “ (Q, Σ) and S Ď Q.
Question: Exists w P Σ˚ with Q.w Ď S?
Fig. 2. Left: A completely reachable automaton (taken from [3, Example 2]) that is
not sync-maximal. Right: A minimal automaton for the set of synchronizing words.
Lastly, we mention the following easy facts [10] that will be used without
special mentioning.
Lemma 5. A strongly connected sync-maximal automaton is completely reach-
able. A completely reachable automaton is strongly connected.
3
Completely Reachable Automata
The notion of a completely reachable automaton was introduced in [3], based on
a suﬃcient condition for the reachability of all subsets in circular automata [7].
This suﬃcient condition was generalized in this very ﬁrst work [3], and later
extended to a characterization with more general constructions in [4]. An
extended version of [3,4] (and with further results) is under submission and a
preliminary version available on arXiv [5]. Complete reachability has been used
in [10] to characterize primitive permutation groups.
Given a ﬁnite automaton and two subsets of states, it is complete for deter-
ministic polynomial space to check if there exists a word mapping one subset
onto the other, see [3,17]. This implies that complete reachability can be checked
in non-deterministic polynomial space by checking if a given automaton is not
completely reachable in the following way: non-deterministically guess a non-
empty subset and check if it is reachable from the whole state set, if not, then
the automaton is not completely reachable. By Savitch’s theorem [20] this prob-
lem is solvable in deterministic polynomial space as well, which implies that

Reset Complexity and Automata with Simple Idempotents
91
complete reachability is decidable in deterministic polynomial space. However,
the precise computational complexity of deciding complete reachability is an
open problem [3–5].
With Proposition 2, which yields a polynomial time procedure to check dis-
tinguishability of all non-empty subset [10, Corollary 3.2], we get the next result.
Proposition 6. Deciding if a given automaton is sync-maximal can be done in
polynomial space.
In [8] a completely reachable automaton with letters of defect one was given
for which the suﬃcient condition from [3] is not fulﬁlled, which was meant to
be a counter-example to a conjecture from [3]. However, with another result
from [8, Theorem 20] it can be deduced that complete reachability is decidable
in polynomial time for automata with simple idempotents.
Theorem 7. For automata A “ (Q, Σ) with simple idempotents it is decidable
in polynomial time if they are completely reachable.
Proof (sketch). In [3] a suﬃcient (graph-theoretical) condition for complete
reachability was stated, and in [8] it was shown that this suﬃcient condition
can be checked in polynomial time. Furthermore [8, Theorem 20] states that the
following implies the mentioned suﬃcient condition: For every proper non-empty
S Ď Q there exists w P Σ˚ with S “ Q.w and w1, w2 P Σ˚ with w “ w1w2 such
that |Q.w|`1 “ |Q.w1| and w2 has defect one. Now, it can be shown that this is
fulﬁlled for completely reachable automata with simple idempotents. Combining
these facts yields the claim.
⊓⊔
4
The Structure of Sync-Maximal Automata
Here, we determine the structure of sync-maximal automata. We show that they
are either completely reachable or consist of precisely two strongly connected
components, where one contains only a single “dangling state” and the other
component forms a completely reachable subautomaton.
Theorem 8. Let A “ (Σ, Q) be an n-state semi-automaton with n ě 3. If A is
sync-maximal, i.e., the smallest recognizing automaton for Syn(A) has 2n ´ n
states, then either A is completely reachable or all of the the following statements
hold true:
1. |Σ| ě 3,
2. we have two strongly connected components {q}, q P Q, and S “ Q\{q},
3. there exists a P Σ with2 q.a P S and such that q′.a ‰ q.a for at least one state
q′ P S,
4. there exists b P Σ having defect one and c P Σ\{b} with q.b “ q.c “ q,
5. if |Σ| “ 3 and n ě 4, then the letter b cyclically permutes S.
2 Observe that as {q} and S are strongly connected components, the condition q.a P S
implies that the state q is not reachable from any state in S and so Q.u ‰ {q} for
all u P Σ˚.

92
S. Hoﬀmann
Proof. As Syn(A) ‰ H, there must exist a state s P Q and a synchronizing word
w P Σ˚ such that Q.w “ {s}. Let S Ď Q be the strongly connected component
containing s. As q.w “ s for every q P Q, this strongly connected component
is uniquely determined for any choice of a state s such that there exists a word
w P Σ˚ with Q.w “ {s}. Furthermore, it has the property that, once entered, we
cannot leave S, i.e., S.u Ď S for all u P Σ˚. However, this implies S∩Q.u ‰ H for
every u P Σ˚. Hence, no non-empty subset of Q\S is reachable. As by assumption
every non-empty subset with at least two elements is reachable, we ﬁnd |S| “ |Q|
or |S| “ |Q| ´ 1.
In the ﬁrst case, A is strongly connected. This implies that if at least one
singleton subset is reachable, then all singleton subsets are reachable and so A
is completely reachable.
In the second case, we can write Q “ S Y {q} with q R S. Note that in this
case A is not completely reachable, as {q} is not reachable. As Q.w P S, there
exists at least one letter mapping q into S.
Let s, t P S be two arbitrary distinct states. Consider the states {q, s} and
{q, t} in the power automaton. They must be distinguishable, i.e., there must
exist a word u P Σ˚ mapping precisely one, say {q, s}, to a singleton set but not
the other. Then S.u Ď S and we can write u “ u′au′′ with u′, u′′ P Σ˚ and a P Σ
such that q.ua P S and q.u “ q and so q.a P S. We must have |{q, t}.ua| “ 2,
which implies t.a ‰ q.a.
Now, suppose n ě 3. Then we must have at least two distinct letters b, c P Σ
such that q.b “ q.c “ q. To see this, consider a non-empty subset T Ď S with
|T| “ n ´ 2. The subset T Y {q} must be reachable. This is only possible if there
exists a letter b P Σ with q.b “ q (recall q R S.u for each u P Σ˚) having defect
one, for if every letter ﬁxing q permutes the states or has defect at least two,
then no subset of the form T Y {q} with |T| “ n ´ 2 is reachable.
Next, consider a subset T ′ Y {q} with |T ′| “ n ´ 2 such that T ′ Y {q} ‰ Q.b.
We cannot use the letter a to reach the subset T ′ Y {q} as q R Q.a. Let i ě 2.
Then |Q.bi| ď n ´ 1 and |Q.bi| “ n ´ 1 implies, as Q.bi Ď Q.b, that Q.bb “ Q.b.
So, there must exist a third letter c P Σ\{a, b} with q.c “ q to reach the subset
T ′ Y {q}.
Lastly, suppose Σ “ {a, b, c}. Then a is the only letter such that q.a P S.
Furthermore, for each q′ P S the subset {q} Y S\{q′} must be reachable. If Q.ub
contains q and has size n ´ 1, then, as Q.ub Ď Q.b, we must have Q.ub “ Q.b. If
|Q.c| ď n and n ě 3, then Q.c must be a subset of size n ´ 1 to reach another
subset not equal to Q.b of size n´1. However, as shown before for the letter b, if
Q.cc has size n´1, then Q.cc “ Q.c. So, if n ě 4 there exists a subset of size n´1
that is not reachable and hence in this case we must have |Q.c| “ n. Putting all
the arguments together, every subset of size n´1 containing q must be reachable
by a word of the form bci for some i ě 0. Let q′ P S such that Q.b “ Q\{q′}
and choose q′′ P S. Then there exists i ě 0 such that Q.bci “ Q\{q′′}. As c
permutes the states, this implies q′.c “ q′′. However, a single permutation that
maps all states in S onto each other is only possible if this permutation induces
a single cycle on these states and we conclude that c cyclically permutes the
states in S.
⊓⊔

Reset Complexity and Automata with Simple Idempotents
93
In case a sync-maximal automaton is not completely reachable, then we can
show that one strongly connected component forms a completely reachable sub-
automaton.
Theorem 9. Let A “ (Q, Σ) be a sync-maximal automaton that is not com-
pletely reachable. Suppose q P Q is the “dangling state” that exists according
to Theorem 8. Then the states in Q\{q} form a strongly connected, completely
reachable and sync-maximal subautomaton.
We can use sync-maximal and completely reachable automata to construct
sync-maximal automata that are not completely reachable by adding a dangling
state, as done in the next example.
0
1
n
2
n 1
3
a, b
b
b
b
a, c
a, c
a, c
a, c
a, b
c
c
. . .
Fig. 3. The automaton An from Example 2
Example 2. We derive from the ˇCern´y family a new family of automata by
adjoining an additional state and a new letter (see Fig. 3) that give sync-maximal,
but not completely reachable (as they are not strongly connected) automata. Let
An “ ({0, 1, . . . , n}, {a, b, c}) with
i.a “

i
if i ă n,
1
if i “ n;
i.b “
⎧
⎪
⎨
⎪
⎩
0
if i “ 0,
i ` 1
if 0 ă i ă n,
1
if i “ n.
i.c “

1
if i “ 0,
i
if i ‰ 0.
Observe that the automaton induced on the states {1, . . . , n} and by the letters
{a, b} is precisely the ˇCerny automaton Cn. The ˇCerny automata are completely
reachable and sync-maximal (which is implied by results from [11], but also by
Theorem 12 of the present work). Hence, all non-empty subsets of {1, . . . , n}
are distinguishable. If S, T Ď {0, 1, . . . , n} are non-empty and distinct, and at
least one, say S, contains the state 0, we can distinguish them the following way:
(1) If 0 R T, then choosing any word from {a, b}˚ that maps T to a singleton
distinguishes S and T, as it maps S to a 2-set. (2) If 0 P T, then write S “ {0, s}

94
S. Hoﬀmann
and T “ {0, t}. By assumption s ‰ t. There exists m ą 0 such that s.bm “ 1.
Then t.bm ‰ 1. Hence S.bmc “ {1} and T.bma “ {1, t.bm} is a 2-set. So, S and
T are distinguishable.
5
Automata with Simple Idempotents
A strongly connected sync-maximal automaton is also completely reachable [10,
Lemma 3.3]. Here, we show that if an automaton with simple idempotents is
completely reachable, then it is sync-maximal. Hence, for strongly connected
automata with simple idempotents, complete reachability and sync-maximality
are equivalent.
Let f : [n] Ñ [n] be a mapping and g : [n] Ñ [n] be a permutation, then the
conjugate of f by g, written f g, is the mapping given by f g “ g´1fg. Note that
if f is a simple idempotent mapping with a ‰ b and af “ b, then f g is a simple
idempotent mapping ag to bg.
Crucial for our result are the following two Lemmata 10 and 11 formulated
in the language of transformation semigroups. The ﬁrst lemma says that for
reachability of subsets, it is suﬃcient to consider products of simple idempotents.
Lemma 10. Let T ď Tn be a transformation semigroup generated by permu-
tations and simple idempotents and S Ď [n]. If there exists t P T such that
S “ ([n])t, then there exists a product t′ P T of conjugates of the generators that
are simple idempotents by permutations in T such that S “ ([n])t′. In particular,
t′ is a product of simple idempotents from T.
Proof. Write t “ g1f1g2f2g3 · · · fn´1gnfngn`1 where the fi are the simple idem-
potents from the generating set and the gi are permutations generated by the
permutations in the generating set. Then (this relation was already observed
in [1])
t “ g1f1g2f2g3 · · · fn´1gngn`1f gn`1
n
“ g1f1g2f2g3 · · · gn´1gngn`1f gngn`1
n´1
f gn`1
n
“ . . . “ g1g2 · · · gn`1f g2g3···gngn`1
1
f g3···gngn`1
2
· · · f gngn`1
n´1
f gn`1
n
.
Now, as g1g2 · · · gn`1 is a permutation, we have ([n])(g1g2 · · · gn`1) “ [n]. Hence,
if we set t′ “ f g2g3···gngn`1
1
f g3···gngn`1
2
· · · f gngn`1
n´1
f gn`1
n
we have S “ ([n])t′. Each
conjugate f g
i where g is a permutation is simple idempotent. Observe that the
number of simple idempotent in the resulting product of t′ is the same as the
number of simple idempotents used in t.
⊓⊔
Next, we give a suﬃcient condition for the distinguishability of 2-sets.
Lemma 11. Let T ď Tn be a transformation semigroup generated by permu-
tations and simple idempotents and containing a constant map. Then for every
two distinct 2-sets there exists an element in T mapping precisely one 2-set to a
singleton but not the other.

Reset Complexity and Automata with Simple Idempotents
95
Proof. Let {a, b}, {c, d} Ď [n] be distinct 2-sets. By assumption and Lemma 10,
there exists a product of simple idempotents f P T such that |{a, b}f| “ 1 or
|{c, d}f| “ 1. Choose the element f that is expressible as a shortest possible
product of simple idempotents, i.e. f “ f1 · · · fm with m is minimal, the fi are
simple idempotent mappings and |{a, b}f| “ 1 or |{c, d}f| “ 1.
Assume |({a, b})f| “ |({c, d})f| “ 1.
The function fm has defect one. Hence the two elements mapped to a single
element are unique. By the choice of f as a minimal product and as fm is applied
at the end, we can conclude that |{a, b}(f1 · · · fm´1)| “ |{c, d}(f1 · · · fm´1)| “ 2
and {a, b}(f1 · · · fm´1) “ {c, d}(f1 · · · fm´1).
Let i P {1, . . . , m ´ 1}. Set hi “ f1 · · · fi and let h0 denote the identity
transformation. Assume {a, b}hi “ {a, b}hi`1, then |{a, b}(hifi`2 · · · fm)| “ 1
and f can be written as a product of m ´ 1 simple idempotents, contradicting
the minimal length of the product. Similarly, we must have {c, d}hi ‰ {c, d}hi`1.
Hence, for every i P {0, 1, . . . , m ´ 1} we have
{a, b}hi ‰ {a, b}hi`1 and {c, d}hi ‰ {c, d}hi`1.
(1)
Now, for i P {1, . . . , m ´ 1}, suppose {a, b}hi “ {c, d}hi. Set A “ {a, b}hi´1 and
B “ {c, d}hi´1. By Eq. (1), we have A ‰ Af and B ‰ Bf. So there exist x P A,
y P B such that xfi ‰ x and yfi ‰ y. By Lemma 1 we can conclude A “ B. So,
inductively, as {a, b}hm´1 “ {c, d}hm´1, we ﬁnd {a, b} “ {c, d}. However, this
contradicts our assumption that both 2-sets are distinct and so we cannot have
that both are mapped to a singleton.
⊓⊔
If we consider the transformation monoid of a given automaton with simple
idempotents, Proposition 2 and Lemma 11 directly give the main result of this
section.
Theorem 12. Let A “ (Q, Σ) be an automaton with simple idempotents. Then
if A is completely reachable, then it is sync-maximal.
As every strongly connected sync-maximal automaton is completely reach-
able, we get the next corollary.
Corollary 13. Let A “ (Q, Σ) be a strongly connected automaton with simple
idempotents. Then A is completely reachable if and only if it is sync-maximal.
By Theorem 8, every sync-maximal automaton over a binary alphabet is
completely reachable, which implies that the automaton is strongly connected.
This yields the next corollary.
Corollary 14. Let A be an automaton with simple idempotents over a binary
alphabet. Then A is completely reachable if and only if it is sync-maximal.
The equivalence between complete reachability and sync-maximality holds
only for strongly connected automata with simple idempotents and for automata
over a binary alphabet. Example 2 gives automata with simple idempotents over
a ternary alphabet that are sync-maximal but not completely reachable. Next,
we give a diﬀerent example.

96
S. Hoﬀmann
Example 3. Here, we give further examples an automata with simple idempo-
tents that are sync-maximal but not completely reachable. Let the automaton
A “ ({0, 1, . . . , n}, {a, b, c, d}) be such that
i.a “

1
if i “ 0,
i
if i ą 0;
i.b “

1
if i “ 2,
i
if i ‰ 1;
i.c “
⎧
⎪
⎨
⎪
⎩
1
if i “ 2,
2
if i “ 1,
i
if i R {1, 2};
i.d “
⎧
⎪
⎨
⎪
⎩
i ` 1
if i R {0, n},
0
if i “ 0,
1
if i “ n.
Then A is an automaton with simple idempotents. As A is not strongly con-
nected, it is not completely reachable. However, it is sync-maximal. This follows
as the letters d and c generate the full symmetric group on {1, 2, . . . , n}. Let
{q1, q2}, {p1, p2} be two distinct 2-sets. If {q1, q2}, {p1, p2} Ď {1, 2, . . . , n}, then
there exists a word u P {c, d}˚ such that {q1, q2}.u “ {1, 2} and {p1, p2}.u ‰
{1, 2} and the word ub maps {1, 2} to {2} and {p1, p2} to another 2-set. Other-
wise, at least one subset contains 0, say, without loss of generality, 0 P {q1, q2}
and q1 “ 0. Let u P {c, d}˚ be a word such that q2.u “ 1. Then {q1, q2}.ua “ {1}
as {q1, q2}.u “ {0, 1}. Furthermore, {p1, p2}.ua is a 2-set. If 0 R {p1, p2}, this
is clear as ua permutes the states {1, . . . , n}. If 0 P {p1, p2}, then q2 R {p1, p2},
which implies {0, 1} “ {p1, p2}.u ‰ {q1, q2}.u and hence |{q1, q2}.u| “ 2.
Example 4. Here, we give an inﬁnite family of synchronizing automata with
simple idempotents over a binary alphabet that are neither sync-maximal nor
completely reachable. Let A “ ({0, 1, . . . , n}, {a, b}) with
i.a “

n
if i “ n ´ 1,
i
if i ‰ n ´ 1;
i.b “

n
if i “ n,
i ` 1 mod n
if i P {0, . . . , n ´ 1};
The word a(ba)n´2 synchronizes A. However, A is not completely reachable as
it is not strongly connected, which implies, by Theorem 8, as it is over a binary
alphabet, that it is not sync-maximal.
Lastly, we state that the problems Sync-Into-Subset and Reachable
Subset are solvable in polynomial time for automata with simple idempotents
over a binary alphabet. This is based on the following lemma about the structure
of the reachable subsets in automata with simple idempotents.
Lemma 15. Let Σ “ {a, b} and A “ (Q, Σ) be an automaton with Q “
{0, 1, . . . , n ´ 1} and q.b “ (q ` 1) mod n and a P Σ be a simple idempotent
letter with a state q P Q such that δ(q, a) ‰ q. Let d ą 0 be the greatest common
divisor of n and the number3 0 ă r ď n with q.a “ q.br. Then for S Ď Q we
have Q.u “ S for some u P Σ˚ if and only if S “ A0 Y . . . Y Ad´1 for non-empty
subsets Ai Ď Q such that s P Ai implies s ” i (mod d).
3 The case r “ n is a borderline case as it essentially implies that a acts as the identity.
However, the statement entails it with S “ Q being the only reachable subset.

Reset Complexity and Automata with Simple Idempotents
97
Proposition 16. For automata with simple idempotents over a binary alpha-
bet, the problems Sync-Into-Subset and Reachable Subset are solvable in
polynomial time.
6
Conclusion
We have introduced the sync-maximal automata and determined their structure.
They are closely connected to completely reachable automata in the sense that
they are either completely reachable or contain a completely reachable subau-
tomaton. Furthermore, in a sync-maximal automaton all subsets with at least
two states are reachable.
A natural question is how sync-maximality relates to the length of shortest
synchronizing words. Intuitively, it means the set of synchronizing words is a
“complicated” set, and one might expect that this might yield lower bounds
on shortest possible paths in the power automaton. However, we can clearly
construct automata with very short synchronizing words that are sync-maximal
by adding to an existing automaton that is sync-maximal a single letter that
maps everything to a single state, as the property of sync-maximality is retained
when adding letters. But such a construction feels rather artiﬁcial, and a natural
question is then what happens if we do not have arbitrary many letters at hand
or the letters have to fulﬁll a certain property (like being idempotent or only
having a certain defect). What can we say about lower bounds for shortest
synchronizing words for automata over a binary alphabet, or only having the
least number of letters of a certain type yielding a sync-maximal automaton
on the given state set? For an upper bound, note that for completely reachable
automata over a binary alphabet, the ˇCern´y conjecture has been conﬁrmed in [5].
As sync-maximal automata over a binary alphabet are completely reachable by
Theorem 8, sync-maximal automata over a binary alphabet also fulﬁll ˇCern´y’s
conjecture.
Furthermore, we have shown that a completely reachable automaton with
simple idempotents must be sync-maximal. Hence, for strongly connected
automata over simple idempotents being completely reachable is equivalent to
sync-maximality. It is known that as soon as the transformation monoid of an
automaton contains a primitive permutation group, it is both completely reach-
able and sync-maximal [10]. But what properties on the letters do we need to
retain this equivalence (or simply that complete reachability already implies
sync-maximality) that are more general than being either a permutation or
a simple idempotent? In our method of proof, we used the idempotency (i.e.,
q.aa “ q.a for each state q P Q) and the fact that the letters have defect one.
But what when letters of defect more than one are involved? What about letters
that instead of being idempotent fulﬁll the property Q.aa “ Q.a, i.e., the image
Q.a is permuted by a?

98
S. Hoﬀmann
Acknowledgement. I thank the anonymous reviewers for careful reading, spotting
typos and unclear formulations, and pointers to the literature. In particular, I thank
one reviewer for spotting an error in the original formulation and proof of Theorem 8,
which has been ﬁxed, and another reviewer for giving a very easy argument related to
checking complete reachability.
References
1. Ara´ujo, J., Bentz, W., Cameron, P.J.: Groups synchronizing a transformation of
non-uniform kernel. Theor. Comput. Sci. 498, 1–9 (2013). https://doi.org/10.1016/
j.tcs.2013.06.016
2. Berlinkov, M.V., Ferens, R., Szykula, M.: Preimage problems for deterministic
ﬁnite automata. J. Comput. Syst. Sci. 115, 214–234 (2021). https://doi.org/10.
1016/j.jcss.2020.08.002
3. Bondar, E.A., Volkov, M.V.: Completely reachable automata. In: Cˆampeanu, C.,
Manea, F., Shallit, J. (eds.) DCFS 2016. LNCS, vol. 9777, pp. 1–17. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-41114-9 1
4. Bondar, E.A., Volkov, M.V.: A characterization of completely reachable automata.
In: Hoshi, M., Seki, S. (eds.) DLT 2018. LNCS, vol. 11088, pp. 145–155. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-98654-8 12
5. Bondar, E.A., Casas, D., Volkov, M.V.: Completely reachable automata: an inter-
play between automata, graphs, and trees. CoRR abs/2201.05075 (2022). https://
arxiv.org/abs/2201.05075
6. ˇCern´y, J.: Pozn´amka k. homog´ennym experimentom s konecn´ymi automatmi. Mat.
fyz. ˇcas SAV 14, 208–215 (1964)
7. Don, H.: The ˇCern´y conjecture and 1-contracting automata. Electron. J. Comb.
23(3), P3.12 (2016)
8. Gonze, F., Jungers, R.M.: Hardly reachable subsets and completely reachable
automata with 1-deﬁcient words. J. Autom. Lang. Comb. 24(2–4), 321–342 (2019).
https://doi.org/10.25596/jalc-2019-321
9. Gusev, V.V., Maslennikova, M.I., Pribavkina, E.V.: Finitely generated ideal lan-
guages and synchronizing automata. In: Karhum¨aki, J., Lepist¨o, A., Zamboni, L.
(eds.) WORDS 2013. LNCS, vol. 8079, pp. 143–153. Springer, Heidelberg (2013).
https://doi.org/10.1007/978-3-642-40579-2 16
10. Hoﬀmann, S.: Completely reachable automata, primitive groups and the state com-
plexity of the set of synchronizing words. In: Leporati, A., Mart´ın-Vide, C., Shapira,
D., Zandron, C. (eds.) LATA 2021. LNCS, vol. 12638, pp. 305–317. Springer, Cham
(2021). https://doi.org/10.1007/978-3-030-68195-1 24
11. Hoﬀmann, S.: State complexity of the set of synchronizing words for circular
automata and automata over binary alphabets. In: Leporati, A., Mart´ın-Vide,
C., Shapira, D., Zandron, C. (eds.) LATA 2021. LNCS, vol. 12638, pp. 318–330.
Springer, Cham (2021). https://doi.org/10.1007/978-3-030-68195-1 25
12. Hoﬀmann, S.: Sync-maximal permutation groups equal primitive permutation
groups. In: Han, Y., Ko, S. (eds.) DCFS 2021. LNCS, vol. 13037, pp. 38–50.
Springer, Cham (2021). https://doi.org/10.1007/978-3-030-93489-7 4
13. Hopcroft, J.E., Ullman, J.D.: Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley Publishing Company, Boston (1979)
14. Howie, J.M.: Fundamentals of Semigroup Theory. Oxford University Press, Oxford
(1996)

Reset Complexity and Automata with Simple Idempotents
99
15. Maslennikova, M.I.: Reset complexity of ideal languages over a binary alphabet.
Int. J. Found. Comput. Sci. 30(6–7), 1177–1196 (2019). https://doi.org/10.1142/
S0129054119400343
16. Reis, R., Rodaro, E.: Ideal regular languages and strongly connected synchronizing
automata. Theor. Comput. Sci. 653, 97–107 (2016). https://doi.org/10.1016/j.tcs.
2016.09.026
17. Rystsov, I.K.: Polynomial complete problems in automata theory. Inf. Process.
Lett. 16(3), 147–151 (1983). https://doi.org/10.1016/0020-0190(83)90067-4
18. Rystsov, I.K.: Estimation of the length of reset words for automata with simple
idempotents. Cybern. Syst. Anal. 36(3), 339–344 (2000). https://doi.org/10.1007/
BF02732984
19. Sandberg, S.: 1 homing and synchronizing sequences. In: Broy, M., Jonsson, B.,
Katoen, J.-P., Leucker, M., Pretschner, A. (eds.) Model-Based Testing of Reactive
Systems. LNCS, vol. 3472, pp. 5–33. Springer, Heidelberg (2005). https://doi.org/
10.1007/11498490 2
20. Savitch, W.J.: Relationships between nondeterministic and deterministic tape com-
plexities. J. Comput. Syst. Sci. 4(2), 177–192 (1970). https://doi.org/10.1016/
S0022-0000(70)80006-X
21. Shitov, Y.: An improvement to a recent upper bound for synchronizing words of
ﬁnite automata. J. Autom. Lang. Combin. 24(2–4), 367–373 (2019). https://doi.
org/10.25596/jalc-2019-367
22. Volkov, M.V., Kari, J.: ˇCern´y’s conjecture and the road colouring problem. In:
´Eric Pin, J. (ed.) Handbook of Automata Theory, vol. I, pp. 525–565. European
Mathematical Society Publishing House (2021)

On the Descriptional Complexity
of the Direct Product of Finite Automata
Markus Holzer(B) and Christian Rauch
Institut f¨ur Informatik, Universit¨at Giessen, Arndtstr. 2, 35392 Giessen, Germany
{holzer,christian.rauch}@informatik.uni-giessen.de
Abstract. In [4] the descriptional complexity of certain automata prod-
ucts of two ﬁnite state devices, for reset, permutation, permutation-reset,
and ﬁnite automata was investigated. Although an almost complete pic-
ture emerged for the magic number problem, there were several open
problems related to the direct product, also called cross product, of ﬁnite
automata, in particular for permutation and permutation-reset devices.
We solve these left open problems and show (i) that for two permutation-
reset automata of n- and m-states the whole range [1, nm] of state com-
plexities is obtainable for the direct product, if the automata have at
least a quaternary input alphabet, while (ii) for binary input alphabet
this is not the case, and (iii) for the direct product of a permutation
and a permutation-reset automaton the number α = 2 is always magic
if n and m fulﬁll some property, i.e., cannot be obtained by the direct
product of any automata of this kind. Moreover, our results can be seen
as a generalization of previous results in [7] for the intersection operation
on automata.
1
Introduction
The direct or cross product of automata is well known from the intersection
and union construction from automata theory. It is only a special case of more
complex automata operations, which were recently studied from a descriptional
complexity perspective in [4]. In general, a product of automata is obtained by
series (cascading), parallel, and/or feedback composition of automata. In the
direct product there is no communication between the component automata,
while for instance, in the cascade product that is yet another well known prod-
uct of automata, the second automaton receives along with the input letter also
the state of the ﬁrst automaton. For the hierarchy of automata products of
increasing feedback dependencies the magic number problem was almost com-
pletely classiﬁed for all meaningful product types of two automata on the classes
of reset (RFA), permutation (PFA), permutation-reset (PRFA), and determin-
istic ﬁnite state automata in general (DFA)—see Table 1 for the results on the
direct product. Let us explain how to interpret the “yes” and “no” entries within
the table: a “no” means that there are no magic numbers, i.e., the whole range
[1, nm] of state complexities can be reached by m- and n-state automata of the
appropriate type not including reset automata if the input alphabet is at least
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 100–111, 2022.
https://doi.org/10.1007/978-3-031-13257-5_8

On the Descriptional Complexity of the Direct Product of Finite Automata
101
Table 1. The magic number problem for the direct product of diﬀerent types of
automata. A “no” entry indicates that there are no magic numbers and the whole
induced interval of state complexities can be reached, while a “yes” entry gives rise
to at least one state complexity that cannot be reached, i.e., a magic number. If not
speciﬁed elsewhere all automata have an input alphabet of size at least two.
Direct product
RFA PFA
PRFA
DFA
RFA
no
no
no
no
PFA
yes
yes
no
PRFA
no, if |Σ| ≥4
no
yes, if |Σ| = 2
DFA
no
binary. For instance, the “no” entry for the direct product of DFAs is due to [7]
and all other “no” entries are from [4]. On the other hand, a “yes” entry indicates
that at least one magic number α exists under the same condition on the input
alphabet as mentioned above, i.e., it cannot be reached by a direct product of
appropriate automata of m- and n-states, respectively. The “yes” entry in the
PFA-PFA cell is due to [4].
The gray shaded entries in Table 1 are the results that are presented here.
Previously in [4] magic numbers for these cases were announced, which where
found by exhaustive computer programs for small values of m, n, and α. To be
more precise,
– α = 2 is magic, for n = m = 3 and alphabets of size at most three for the
direct product of a PFA and PRFA, and
– α = 8 is magic, for n = m = 3 and at most binary alphabets1 for the direct
product of two PRFAs.
A complete understanding of the magic number problem for both cases is missing
in [4]. We partially close this gap and show the following results: (i) α = 2 is
magic for m and n both odd and at least three for binary input alphabets in case
of the direct product of a PFA and a PRFA. For larger alphabets the value α
remains magic, but we can only prove it for ﬁxed n = 3 and odd m at least three.
(ii) For the direct product of two PRFAs we ﬁrst show that no magic numbers
exist if the input alphabet is at least four. Whether this result is optimal w.r.t.
the input alphabet size is left open, but we can narrow the search for the answer
to a small interval of numbers for the outcome of the direct product for two given
permutation-reset input automata. In passing we show that the above mentioned
result for α = 8 is best possible w.r.t. the input alphabet size, because with three
letters this number is obtainable for n = m = 3—see Example 1. In the light
of [7] and the previously obtained results of the authors on automata products
the existence of magic numbers is expected, because if several restrictions are
1 In [4] there is a misprint on the alphabet size, which was said to be at most three.

102
M. Holzer and C. Rauch
being imposed on automata, then, sooner or later, some values of the state
complexity become unreachable. However these results solve the main open issues
from [4] and thus complete the overall picture of automata products on ﬁnite
automata. Nevertheless, certain ﬁne grain details on the question whether a
particular value α is magic or not for the direct product of the automata under
consideration are still open and await solution.
The paper is organized as follows: next we introduce the necessary notations
on automata and the direct product. Then we start our investigation and ﬁrst
give an overview on the previously obtained results on the direct product of
automata w.r.t. the magic number problem. Then we prove our new results and
ﬁnally we conclude with an open problem and topics for further investigations.
2
Preliminaries
We recall some deﬁnitions on ﬁnite automata as contained in [3]. A deterministic
ﬁnite automaton (DFA) is a quintuple A = (Q, Σ, ·, q0, F), where Q is the ﬁnite
set of states, Σ is the ﬁnite set of input symbols, q0 ∈Q is the initial state, F ⊆Q
is the set of accepting states, and the transition function · maps Q×Σ to Q. The
language accepted by the DFA A is deﬁned as L(A) = { w ∈Σ∗| q0 · w ∈F },
where the transition function is recursively extended to a mapping Q × Σ∗→Q
in the usual way. Obviously, every letter a ∈Σ induces a mapping from the state
set Q to Q by q →q ·a, for every q ∈Q. A DFA is unary if the input alphabet Σ
is a singleton set, that is, Σ = {a}, for some input symbol a. Moreover, a DFA is
said to be a permutation-reset automaton (PRFA) if every input letter induces
either a permutation or a constant mapping on the state set. If every letter of the
automaton induces only permutations on the state set, then we simply speak of
a permutation automaton (PFA). Finally, a DFA is said to be a reset automaton
(RFA) if every letter induces either the identity or a constant mapping on the
state set. The class of reset, permutation, permutation-reset, and deterministic
automata in general are referred to as RFA, PFA, PRFA, and FA, respectively.
It is obvious that the inclusions XFA ⊆PRFA ⊆FA, where X ∈{P, R}, hold.
Moreover, it is not hard to see that the classes RFA and PFA are incomparable.
The direct product of two DFAs, also known as the cross product, A =
(QA, Σ, ·A , q0,A, FA) and B = (QB, QA × Σ, ·B , q0,B, FB), denoted by A × B,
is deﬁned as the automaton2
A × B = (QA × QB, Σ, · , (q0,A, q0,B), FA × FB),
where the transition function is given by
(q, p) · a = (q ·A a, p ·B a),
for q ∈QA, p ∈QB, and a ∈Σ. Observe, that the transitions of A and B depend
only on Σ. We say that A is the ﬁrst automaton and B the second automaton
2 In [4] the direct product was referred to as ν0-product and with ◦ν0 notated. This
naming originates from the hierarchy of automata products studied in automata
networks, see, e.g., [2].

On the Descriptional Complexity of the Direct Product of Finite Automata
103
in the product. Observe, that although the statements to come on the direct
product explicitly refer to ﬁrst and second automaton of a certain type, these
types can be obviously commuted, since in the direct product the order of the
operand automata is not relevant to the product automaton (up to isomorphism).
For the choice of the ﬁnal set of states of the direct product automaton we follow
the lines of [1] and the forerunner papers [4–6]. One observes, that the device
A × B accepts the intersection of the language accepted by A and B.
We give a small example.
Example 1. Consider the PRFA A = ({q0, q1, q2}, {a, b, c, d}, ·A , q0, {q0, q2}),
where
q0 ·A a = q0,
q0 ·A b = q0,
q0 ·A c = q2,
q0 ·A d = q0,
q1 ·A a = q1,
q1 ·A b = q1,
q1 ·A c = q2,
q1 ·A d = q2,
q2 ·A a = q2,
q2 ·A b = q2,
q2 ·A c = q2,
q2 ·A d = q1.
Then let
B = ({p0, p1, p2}, {a, b, c, d}, ·B , p0, {p0, p2}),
be the PRFA, where
p0 ·B a = p2,
p0 ·B b = p0,
p0 ·B c = p1,
p0 ·B d = p0,
p1 ·B a = p2,
p1 ·B b = p1,
p1 ·B c = p0,
p1 ·B d = p1,
p2 ·B a = p2,
p2 ·B b = p2,
p2 ·B c = p2,
p2 ·B d = p2.
The automata A and B are depicted in Fig. 1 on the top and lower right, respec-
tively. It is easy to see that both automata are minimal.
By construction the ν0-product of A and B is given by
A × B = ({q0, q1, q2} × {p0, p1, p2}, {a, b, c, d}, · , (q0, p0), {q0, q2} × {p0, p2}),
where the transitions of the initially reachable states
(q0, p0), (q0, p2), (q1, p0), (q1, p1), (q1, p2), (q2, p0), (q2, p1), (q2, p2),
can be deduced from Fig. 1, too, on the lower left. By inspection no initially
reachable states in A × B are equivalent and (q0, p1) is not reachable. Hence,
the minimal DFA accepting L(A × B) has α = 8 states. One may have noticed
that the letter b induces the identity mapping on all involved automata. The
transitions of the letters a, b, c, and d are chosen such that in A × B the letter a
maps every state onto the last row, the letter b induces a cycle in the ﬁrst column
of speciﬁc length (here one), the letters b and c map the states in the last column
without (qn−1, pm−1) transitively onto each other and the letter d forms row-wise
cycles of a speciﬁc length (here two) beginning in the last column.
⊓⊔

104
M. Holzer and C. Rauch
q0
q1
q2
(q0, p0)
(q1, p0)
(q2, p0)
(q2, p1)
(q2, p2)
(q0, p1)
(q1, p1)
(q0, p2)
(q1, p2)
p0
p1
p2
c
c
d
a
a
c
a
a
a
a
a
c
c
c
c
c
c
d
d
d
Fig. 1. The example automata A and B both with input alphabet {a, b, c, d} on the
top and lower right, respectively. For a better representability not all transitions of
the automata are shown. In particular, this is the case for the automaton A × B,
where only the transitions of the initially reachable states are shown. Additionally no
self-loops are shown. For instance, letter a acts as the identity on the state set of A.
The direct-product A × B is depicted on the lower left.
When considering the descriptional complexity of the product of two
automata, we limit ourselves to the case where the involved automata are non-
trivial, i.e., they have more than one state. Thus, in the following we only con-
sider non-trivial automata. It is easy to see that n · m states are suﬃcient for
any product of an n-state and m-state automaton.
3
Results
First let us recall what is known from the literature for the magic number prob-
lem of the direct product, which is the following question: which numbers of
states of the minimal DFA for the direct product of two minimal automata of
state size n and m are reachable? Whenever a number is not obtainable, it is
called “magic”. Obviously the answer to this question depends on the types of
the involved input automata. The following results are known:
1. In [7] it was shown that if the input automata are arbitrary deterministic
ﬁnite automata the whole range [1, nm] can be reached (DFA-DFA case),
and
2. all combinations of RFAs, PFAs, PRFAs, and DFAs were considered in [4],
where the following results were shown:

On the Descriptional Complexity of the Direct Product of Finite Automata
105
(a) Whenever a RFA is involved in the direct product (RFA-RFA, RFA-
PFA, RFA-PRFA, and RFA-DFA case) no magic numbers exist and the
whole interval can be reached. Note that minimal RFAs have state size
at most two.
(b) For the PFA-PFA case the answer to the magic number problem is “yes”,
because magic numbers were already identiﬁed for the more complex
cascade product of permutation automata.
(c) For the cases PFA-PRFA and PRFA-PRFA magic numbers were iden-
tiﬁed only by exhaustive computer programs for small cases of m, n,
and α. In particular, for the direct product of a PFA and a PRFA the
value α = 2 is magic for n = m = 3 and alphabets of size at most three.
(d) Finally, no magic numbers exist for the PFA-DFA case and thus for the
more general PRFA-DFA case.
Thus, only the PFA-PRFA and PRFA-PRFA lack a complete theoretical under-
standing, since in this case only computer determined evidence for magic num-
bers were given. In the forthcoming we close this gap in the aﬃrmative of the
magic number problem. We start our investigation with the PFA-PRFA case.
As mentioned above α = 2 was identiﬁed magic for n = m = 3 and alphabets
of size at most three by a computer program.3 Already in [4] it was conjectured
that α = 2 is magic whenever m and n are odd and at least three. The next
lemma shows that this is actually the case for binary alphabets.
Lemma 2. Let n, m ≥3 be both odd. Then there exists no minimal binary n-
state PRFA A and no minimal binary m-state PFA B such that the minimal
DFA for the language L(A × B) has 2 states.
Proof. We prove the statement by contradiction. Therefore assume to the con-
trary that there is a minimal n-state PRFA A and a minimal m-state PFA B
such that the minimal DFA for the language L(A × B) has two states.
First we prove that A is neither a RFA nor a PFA. In case A is a RFA, i.e.,
all input letters are resets, we obtain a contradiction on the minimality of A,
because every minimal RFA has at most two states [6], but A is a minimal device
with at least 3 states. Hence, not all letters of the input alphabet of A are resets.
Next assume that A is a PFA. In [6] it was shown that for every α in [2, nm] that
is coprime to n, there does not exist a minimal n-state PFA A and a minimal m-
state PFA B such that the minimal DFA accepting the language of the cascade
product of A and B has α states. Since the direct product is a special case of the
cascade product this result also holds if the direct product ν0 is considered. Thus,
one letter, say a, of the input alphabet of A induces a reset on the state set QA
of A and the other letter, say b, induces a permutation on QA. For convenience
let QB refer to the state set of B. Thus, the input alphabet of A and also B,
since we consider the direct product, is equal to Σ = {a, b}.
3 Surprisingly the computer program also reveals that every other number in the
range [1, nm] = [1, 9] is reachable.

106
M. Holzer and C. Rauch
Next we deﬁne the state sets
QA,1 :={q0 · w | w ∈Σ∗for w inducing a permutation on QA},
and
QA,2 :={q0 · w | w ∈aΣ∗},
for q0 being the initial state of A. Clearly this results in the properties
QA,1 · a = {q0 · a} ⊆QA,2,
QA,2 · a = {q0 · a} ⊆QA,2,
QA,1 · b = QA,1,
QA,2 · b = QA,2,
and
QA = QA,1 ∪QA,2,
where the union is not necessarily disjoint. Observe, that QA,2 contains at least
one state, since a is the reset letter. Moreover, note that for every word w ∈Σ∗
which induces a permutation there is a word w−1 ∈Σ∗which induces the inverse
permutation on the state set of A. Therefore either QA,1 is a subset of QA,2 or
the two sets are disjoint. Since n is at least equal to three the ﬁrst case can
only appear for |QA,2| ≥3. Nevertheless the argumentation to come for the
case |QA,2| ≥3 does not require QA,1 and QA,2 to be disjoint. We want to
mention that in all cases b permutes the states of QA,2 transitively because A
is a binary device. Now we are ready to consider the following cases for QA,2,
where we will conclude a contradiction in each case:
1. Case |QA,2| = 1. Let QA,2 = {q}, for some state q in QA. We ﬁrst assume
that q is an accepting state. So the set {q} × QB induces a PFA which is
isomorphic to B up to the initial state. Since B is minimal the states in {q}×
QB cannot contain any equivalent states which contradicts α = 2. Thus q has
to be non-accepting which implies that all states in {q} × QB are equivalent.
This implies the existence of an accepting state in the set QA,1 × QB which
is initially reachable. Since there is at least one reachable state in A × B for
each state q′ in QA,1, which has q′ as its ﬁrst component the assumption
that only one state of QA,1 × QB is reachable implies that QA,1 consists of
one state. Indeed this gives us that |QA,1 ∪QA,2| = 1 + 1 = 2, which is
a contradiction to 2 < n = |QA|. Therefore there are at least two states
of QA,1 × QB reachable. But on the other hand the states in QA,1 have
to contain an accepting and a non-accepting state. Therefore an accepting
state and a non-accepting state in QA,1 × QB is reachable. Since there is a
word w ∈Σ∗which maps the non-accepting state in QA,1 × QB onto an
accepting state the reachable states in QA,1 × QB cannot contain a state
equivalent to the reachable states of {q} × QB. Therefore the minimal DFA
for the language A × B has at least three states which is a contradiction to α
equal to two.
2. Case |QA,2| = 2. Clearly QA,2 contains one accepting and one non-accepting
state, because otherwise the above described closure properties of QA,2 con-
tradicts the minimality of A.

On the Descriptional Complexity of the Direct Product of Finite Automata
107
We claim that each state in QA,2 × QB is reachable in A × B. Since B is a
PFA for each pair of states p and p′ there is a word w in Σ∗a which maps p
onto p′. Let q′ be the image of the reset induced by a in A. Therefore every
state (q, p) is mapped onto (q′, p′) for q being a state of A. Clearly this implies
that all states in {q′}×QB are reachable in A×B. Since every letter induces
a permutation on QB and since for every state q in QA,2 there is a word which
maps q′ onto q the claim follows.
The b-cycles of the state set QA,2 × QB can be interpreted as unary cyclic
PFAs P0, P1, . . . , Pk, for some k ≥0. Recall that a cyclic automaton con-
sists of one cycle. Observe, that there is an accepting state in one of the
PFAs P0, P1, . . . , Pk, say this is Pi. Additionally there must also be a non-
accepting state in Pi which has the non-accepting state of QA,2 as its ﬁrst
component.
In [6] it was shown that for every (non-)minimal PFA there exists a number x
such that every of its states is equivalent to x states. Therefore this also holds
for each of the PFAs P0, P1, . . . , Pk. Since all accepting states of a PFA Pi and
all non-accepting states are equivalent this implies that the number of accept-
ing and non-accepting states has to be equal in Pi. On the other hand all of
the non-accepting states of all the PFAs are equivalent which implies that all
PFAs must contain an accepting state and a non-accepting state. This holds
because if Pi contains only non-accepting states and Pj contains an accept-
ing state there is a word w ∈b∗which maps a non-accepting state of Pi onto
a non-accepting state Pi and a non-accepting state of Pj onto an accepting
state of Pj. In conclusion this means that for each of the PFAs P0, P1, . . . , Pk
the number of accepting and non-accepting states has to be equal. Because
the union of their state sets is equal to QA,2 ×QB we observe that QA,2 ×QB
contains |QA,2 × QB|/2 accepting states.
This is a contradiction to the fact that only the half of the states in QA,2,
i.e., only one, is accepting and QB contains at least one non-accepting state4
which implies that the number of accepting states in QA,2 × QB is strictly
less than |QA,2 ×QB|/2. We want to mention that this causes a contradiction
in all cases since there cannot be a single PFA P0, for k = 0, with two states
because |QA,2 × QB| = 2 · |QB| is at least equal to six.
3. Case |QA,2| ≥3. We use the notation as in the previous case, in particular
the b-cycle PFAs P0, P1, . . . , Pk, and argue along similar lines up to the con-
tradiction in the last paragraph. Recall that each of the PFAs P0, P1, . . . , Pk
contains an accepting and a non-accepting state.
Since all accepting (non-accepting, respectively) states are equivalent it is
easy to understand that this is only possible if the ﬁnality of the states in
each cycle alternates. The ﬁrst components appear in the states of Pi in the
same ordering as in QA,2. The ordering of QA,2 may occur multiple times
in Pi but this will not matter for our reasoning. Indeed this implies that
without loss of generality every state which is on an even position in QA,2 is
accepting. There have to be also accepting states on odd positions in QA,2
4 This is due to the fact that B is minimal and |QB| is at least three.

108
M. Holzer and C. Rauch
or |QA,2| because otherwise all accepting and all non-accepting states in QA,2
would be equivalent which would contradict the minimality of A. In both
cases there are consecutive states in QA,2 which are accepting.
We show that the ﬁnality of the states in each b-cycle of B alternates, too. It
is already known that every state in QA,2 × QB is reachable. If p and p′ are
non-accepting states of B such that p · b = p′ we obtain that for q · b = q′ the
states (q, p) and (q′, p′) are also non-accepting and that (q, p) · b = (q′, p′).
Indeed this would contradict the fact that the ﬁnality of the states in each Pi
alternates. If p and p′ are accepting states of B such that p · b = p′ we obtain
that for q ·b = q′ the states (q, p) and (q′, p′) are also accepting if q and q′ are
accepting5 and that (q, p) · b = (q′, p′). Again this would contradict the fact
that the ﬁnality of the states in each Pi alternates.
Additionally we observe that each b-cycle of B has at least two states because
otherwise there would be a PFA Pi which is either isomorphic to the PFA
induced by QA,2 up to the initial state or which is a cycle of length |QA,2|
of non-accepting states. The ﬁrst case contradicts the fact that all accepting
states are equivalent in Pi and we proved already that the latter case is ruled
out.
It is not hard to see that the ordering of QA,2 is the same ordering as for the
ﬁrst components of the states in each of the PFAs P0, P1, . . . , Pk. Recall that
there is an accepting state q in QA,2 which is followed by an accepting state.
Since we have shown that every state in QA,2 × QB is initially reachable we
know that there is a reachable state (q, p) that is accepting. Since the ﬁnality
of the states in all b-cycles of B alternates we obtain that the cycle of (q, p)
contains two consecutive non-accepting states which is a contradiction to the
fact that the ﬁnality of the states in each of the PFAs P0, P1, . . . , Pk alter-
nates.
⊓⊔
By a careful inspection of the statement of the previous lemma we show that
it can be improved to alphabets of arbitrary size restricting one automaton to
three states. We have to leave open whether a more general improvement is
possible.
Theorem 3. Let n = 3 and m be odd with m ≥3. Then there does not exist a
minimal n-state PRFA A and no minimal m-state PFA B such that the minimal
DFA for the language L(A × B) has 2 states.
Proof. We prove this statement by showing that for n = 3 the reasoning of the
proof of Lemma 2 is also valid for arbitrary alphabet size greater or equal to
two. Therefore we use the same notation as in the previous proof which was
mainly guided by the size of the state set QA,2, a subset of QA, the state set of
the PRFA A.
By inspecting of the case |QA,2| = 1 of the previous proof we obtain that it
only requires the input alphabet to contain the letters a and b which implies
5 As mentioned before the existence of these states is guaranteed by the minimality
of A.

On the Descriptional Complexity of the Direct Product of Finite Automata
109
that there can be arbitrary many other letters in the input alphabet. The
cases |QA,2| = 2 and |QA,2| ≥3 rely on the fact that there is letter b which
induces a permutation and acts transitively on the set QA,2, e.g., it forms a
cycle on QA,2. We prove now that the argument is also true for all alphabets
with at least two elements if n = 3. To this end we consider two cases depending
on the size of QA,2:
1. Case |QA,2| = 2. The proof that there is a letter that permutes the states
of QA,2 non-trivially is shown by contradiction. Assume to the contrary that
all letters which induce a permutation act on QA,2 trivially. Since n = 3
and |QA,2| = 2 we know that |QA,1| = 1 and QA,1 = QA\QA,2. Due to
the deﬁnition of QA,1 we know that every permutation ﬁxes the sole state
in QA,1. This implies that every permutation induces the identity on QA =
QA,1 ∪QA,2. So A is a RFA which is a contradiction to the fact that A is
minimal and has three states.
2. Case |QA,2| ≥3. We distinguish three subcases with respect of the size
of QA,1; note that in fact |QA,2| = 3, since n = 3:
(a) Subcase |QA,1| = 1. We observe that the arguments used in the case
|QA,2| = 2 of the proof of Lemma 2 imply for the case |QA,2| = 3 under
consideration that all states of A×B are initially reachable because n = 3.
One ﬁnds that there are three states of A×B which have the single state
of QA,1 as their ﬁrst component. These may contain zero, one, or two
accepting states depending on the ﬁnality of the sole state in QA,1 and the
number of accepting states of B. These three states are either transitively
mapped onto each other which makes them inequivalent if at least one
of them is accepting or one of these states, say q, is only mapped onto
itself by permutations. We will show the contradiction for the second case
because if all three states are transitively permuted and non-accepting
they are equivalent while they are inequivalent to every non-accepting
state that is mapped onto an accepting state by a permutation. Indeed
this causes a contradiction in a similar fashion like it will for the case
that q is only mapped onto itself. So q is not mapped onto either an
accepting or a non-accepting state. Since A is not an RFA there must be a
permutation c which acts non-trivially on the state set of A. Furthermore,
letter c has to permute two states of diﬀerent ﬁnalities to preserve the
minimality of A. Thus, one of the cycles induced by c in A × B contains
a non-accepting and an accepting state while q is a ﬁxpoint of c. These
three states cannot be equivalent because there is a word in c∗which
maps them onto states of diﬀerent ﬁnality. This implies that the minimal
DFA accepting L(A×B) has at least three states which is a contradiction.
(b) Subcase |QA,1| = 2. It is not hard to see that the arguments in the
previous subcase can also be used for |QA,1| = 2, if we exchange q in the
reasoning above by the state ˜q in QA\QA,1 and by observing that ˜q is
also mapped onto itself by every permutation.
(c) Subcase |QA,1| ≥3—By a similar reasoning as for the size of QA,2
together with n = 3 we are actually in the case |QA,1| = 3. Since |QA,1| =

110
M. Holzer and C. Rauch
3 either there is a permutation that permutes QA,1 = QA,2 transitively or
there are at least two non-trivial unequal permutations on that set. Due
to the fact that they are non-trivial each of them must permute at least
two elements while each of them permutes less than |QA,2| = 3 elements.
Obviously they have order two, e.g., they are transpositions. Addition-
ally they have one element in common since they permute QA,2 which
has only three elements. So the composition of the two transposition has
order three and therefore permutes QA,2 transitively.
Therefore all possible cases lead to a contradiction.
⊓⊔
Next we consider the PRFA-PRFA case. Here also at least one magic number
was announced in [4] with the help of a computer program. This number is
α = 8 = nm −1, for n = m = 3 and alphabet of size at most two. In fact, if
the alphabet size is large enough, we show that no magic number in the PRFA-
PRFA case exists. Due to the lack of space the proof of the following statement
has to be omitted.
Theorem 4. Let n, m ≥2. Then for every α with 1 ≤α ≤nm, there exists a
quaternary minimal n-state PRFA A and a quaternary minimal PRFA B such
that the minimal DFA for the language L(A × B) has α states.
Now the question arises whether the above theorem is best possible w.r.t. the
input alphabet size. For alphabet size two α = 8 is magic as mentioned above for
n = m = 3. Unfortunately, this is not true anymore if we consider alphabets of
size at least three, which is shown next for the more general case of α = nm −1
for large enough m and n.
Lemma 5. Let n, m ≥3. Then for α = nm −1, there exists a ternary minimal
n-state PRFA A and a ternary minimal PRFA B such that the minimal DFA for
the language L(A×B) has α states. This results holds true even if one automaton
is a PFA.
Proof. Deﬁne the PRFA
A = ({q0, q1, . . . , qn−1}, {a, b, c}, ·A , q0, {q0, qn−1})
with
qn−1 ·A a = qn−2,
qn−2 ·A a = qn−1,
qi ·A b = q1,
for 0 ≤i ≤n −1
qi ·A c = qi+1,
for 1 ≤i ≤n −3
qn−2 ·A c = q1,
where all not explicitly mentioned transitions are self-loops. Moreover, let
B = ({p0, p1, . . . , pm−1}, {a, b, c}, ·B , p0, {p0, pm−1}),

On the Descriptional Complexity of the Direct Product of Finite Automata
111
be the PRFA, where
pm−2 ·A b = pm−1,
pm−1 ·A b = pm−2,
pi ·A c = pi+1 mod (m−1),
for 0 ≤i ≤m −2
where all not explicitly mentioned transitions are self-loops. The minimality of
both automata are immediate. Observe, that B is even a permutation automa-
ton. The argumentation that the minimal automata that accepts the language
L(A × B) requires exactly α = nm −1 states is left to the interested reader. ⊓⊔
The previous lemma does not answer the question whether Theorem 4 is
best possible for the stated alphabet size. A careful inspection of the proof
of Theorem 4 together with the previous lemma and results in [4] reveal that
optimality is given if there is a number in the interval
[max{n + 2m −1, m + 2n −1}, nm −2]
that is magic for the PRFA-PRFA case for ternary alphabet size. Hopefully
further research will give an answer to this question.
References
1. Ae, T.: Direct or cascade product of pushdown automata. J. Comput. Syst. Sci.
14(2), 257–263 (1977)
2. D¨om¨osi, P., Nehaniv, C.L.: Algebraic Theory of Automata Networks: An Introduc-
tion. SIAM (2005)
3. Harrison, M.A.: Introduction to Formal Language Theory. Addison-Wesley, Boston
(1978)
4. Holzer, M., Rauch, C.: More on the descriptional complexity of products of ﬁnite
automata. In: Han, Y.S., Ko, S.K. (eds.) DCFS 2021. LNCS, vol. 13037, pp. 76–87.
Springer, Cham (2021). https://doi.org/10.1007/978-3-030-93489-7 7
5. Holzer, M., Rauch, C.: The range of state complexities of languages resulting from
the cascade product—the general case (extended abstract). In: Moreira, N., Reis, R.
(eds.) DLT 2021. LNCS, vol. 12811, pp. 229–241. Springer, Cham (2021). https://
doi.org/10.1007/978-3-030-81508-0 19
6. Holzer, M., Rauch, C.: The range of state complexities of languages resulting from
the cascade product—the unary case (extended abstract). In: Maneth, S. (ed.) CIAA
2021. LNCS, vol. 12803, pp. 90–101. Springer, Cham (2021). https://doi.org/10.
1007/978-3-030-79121-6 8
7. Hricko, M., Jir´askov´a, G., Szabari, A.: Union and intersection of regular lan-
guages and descriptional complexity. In: Mereghetti, C., Palano, B., Pighizzini, G.,
Wotschke, D. (eds.) Proceedings of the 7th Workshop on Descriptional Complexity
of Formal Systems, pp. 170–181. Universita degli Studi di Milano, Como (2005)

Operations on Subregular Languages
and Nondeterministic State Complexity
Michal Hospod´ar1(B)
, Peter Mlyn´arˇcik1,2, and Viktor Olej´ar1,3
1 Mathematical Institute, Slovak Academy of Sciences, Koˇsice, Slovakia
hosmich@gmail.com, olejar@saske.sk
2 Faculty of Humanities and Natural Sciences, University of Preˇsov, Preˇsov, Slovakia
3 Department of Computer Science, P. J. ˇSaf´arik University, Koˇsice, Slovakia
Abstract. We study the nondeterministic state complexity of basic reg-
ular operations on subregular language families. In particular, we focus
on the classes of combinational, ﬁnitely generated left ideal, group, star,
comet, two-sided comet, ordered, and power-separating languages, and
consider the operations of intersection, union, concatenation, power,
Kleene star, reversal, and complementation. We get the exact complex-
ity in all cases, except for complementation of group languages where we
only have an exponential lower bound. The complexity of all operations
on combinational languages is given by a constant function, except for
the k-th power where it is k+1. For all considered operations, the known
upper bounds for left ideals are met by ﬁnitely generated left ideal lan-
guages. The nondeterministic state complexity of the k-th power, star,
and reversal on star languages is n. In all the remaining cases, the non-
deterministic state complexity of all considered operations is the same as
in the regular case, although sometimes we need to use a larger alphabet
to describe the corresponding witnesses.
1
Introduction
The ﬁelds of study at the intersection of mathematics and computer science,
known as formal language theory and automata theory, contain a rich history
of publications interesting both from a practical and theoretical point of view.
One of the primary investigated language classes in the ﬁeld, regular languages,
have a number of combinatorial, algebraic, and computational properties still
prominently investigated today. The topic of interest in this publication are the
notions of nondeterministic ﬁnite automata (NFA) accepting some subregular
languages and operational state complexity.
The deﬁnition of NFAs originates from the seminal paper by Rabin and
Scott [15]. A conversion procedure to deterministic ﬁnite automata (DFA) called
“subset construction” was provided as well, showing that an NFA with n states
can be simulated by a DFA with at most 2n states. This model is connected to
the measure of nondeterministic state complexity of a given language L, which
Research supported by VEGA grant 2/0132/19 and grant APVV-15-0091.
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 112–126, 2022.
https://doi.org/10.1007/978-3-031-13257-5_9

Operations on Subregular Languages
113
represents the number of states of the smallest NFA accepting L. The nonde-
terministic state complexity of a given regular operation is the nondeterministic
state complexity of the language resulting from this operation, considered as a
function of the sizes of NFAs for operands. A more rigorous investigation of this
measure comes from Holzer and Kutrib [8] for the Boolean operations, concate-
nation, iteration, and reversal.
By restricting the operands to certain subclasses of regular languages, it
turns out that the resulting nondeterministic state complexities of these oper-
ations might diﬀer from the general case to various degrees. This observation
motivated several publications focusing on speciﬁc subregular language classes.
Han et al. [5,6] considered the complexities of some of the mentioned basic oper-
ations for preﬁx-free and suﬃx-free languages. Additional results were provided
for star-free languages by Holzer et al. [9], for union-free languages by Jir´askov´a
and Masopust [14], and recently Hospod´ar et al. [12] examined various subclasses
of convex languages.
In this paper, we continue with such investigations focusing on the operations
of intersection, union, concatenation, power, Kleene star, reversal, and comple-
mentation. We consider the language classes mainly from [2], more speciﬁcally
combinational languages, ﬁnitely generated left ideals, group languages, stars,
comets, two-sided comets, ordered languages, and power-separating languages.
We get the exact complexity for each pair of operation and class except for
complementation on group languages where we obtain only a lower bound. For
combinational languages, the complexity does not depend on the size of input
NFAs. In most other cases, the complexity is the same as for regular languages,
except for ﬁnitely generated left ideals where the complexity of all operations is
the same as for general left ideals [12]. To get lower bounds, instead of commonly
used fooling sets for regular languages, we rather use fooling sets for MNFAs
consisting of pairs of a reachable and a co-reachable set for each state. Then, we
only test the emptiness of intersection of ﬁnite sets instead of deciding whether
or not a string is in a language.
2
Preliminaries
We assume that the reader is familiar with the standard notation and deﬁni-
tions in formal language and automata theory. For details and a more thorough
introduction, refer to [10].
We denote the set of positive integers by N. Let Σ be a non-empty alphabet
of symbols. Then Σ∗denotes the set of all strings over Σ, including the empty
string ε. A language over Σ is any subset of Σ∗.
The reversal of a string w over Σ denoted wR is deﬁned as εR = ε if w = ε,
and wR = anan−1 · · · a2a1 if w = a1a2 · · · an−1an with ai ∈Σ. The reversal
of a language L is the language LR = {wR | w ∈L}. The complement of a
language L over Σ is the language Lc = Σ∗\L. The intersection of languages K
and L is the language K ∩L = {w | w ∈K and w ∈L}, while the union of K
and L is K ∪L = {w | w ∈K or w ∈L}. The concatenation of languages K

114
M. Hospod´ar et al.
and L is the language KL = {uv | u ∈K and v ∈L}. For a given positive
integer k, the k-th power of a language L is the language Lk = LLk−1 with
L0 = {ε}. The positive closure of a given language L is L+ = 
k≥1 Lk, while the
Kleene star of L is deﬁned as L∗= 
k≥0 Lk and it is equal to {ε} ∪L+. We use
the notation of regular expressions over Σ in a standard way with ∅(empty set),
ε, and each σ ∈Σ being regular expressions; furthermore if r and s are regular
expressions, then rs (concatenation), r+s (union), and r∗(star) are also regular
expressions. For a regular expression r, the expression rk denotes the k-th power
of the language of r, and r≤k denotes the expression r0 + r1 + · · · rk.
A nondeterministic ﬁnite automaton with multiple initial states (MNFA) is a
quintuple M = (Q, Σ, ·, I, F) where Q is a ﬁnite non-empty set of states, Σ is a
ﬁnite set of input symbols (i.e., input alphabet), I ⊆Q is the set of initial states,
F ⊆Q is the set of ﬁnal (accepting) states, and ·: Q × Σ →2Q is the transition
function which can be naturally extended to the domain 2Q ×Σ∗. The language
accepted by the MNFA M is L(M) = {w ∈Σ∗| I · w ∩F ̸= ∅}. If R and S are
two sets of states of M, then R
σ−→S denotes that R · σ = S.
An MNFA whose set of initial states is a singleton is called a nondeterministic
ﬁnite automaton (NFA). An NFA is a (complete) deterministic ﬁnite automaton
(DFA) if |q ·σ| = 1 for each q ∈Q and each σ ∈Σ; in such a case, · is a mapping
from Q × Σ to Q. A non-ﬁnal state q with transitions (q, σ, q) for each σ in Σ is
called a dead state.
A given language L is called regular if and only if there exists an MNFA M for
which L = L(M). Two MNFAs A and B are equivalent if they accept the same
language. Every MNFA M = (Q, Σ, ·, I, F) can be converted into an equivalent
complete DFA D(M) = (2Q, Σ, ·, I, {S ∈2Q | S ∩F ̸= ∅}), by the subset
construction [10] where · is the extension of the transition function of M to the
domain 2Q × Σ. The DFA D(M) is referred to as the subset automaton.
The reverse of an MNFA M, denoted M R, is the MNFA obtained from M
by reversing all transitions and by swapping the roles of initial and ﬁnal states.
A subset S of states of M for which exists a string w such that I ·w = S is called
reachable in M. If a set is reachable in M R, we say it is co-reachable in M.
Sometimes we allow an MNFA to have ε-transitions, and then a set S is
reached from a set R on a symbol σ if S = E({q · σ | q ∈R}) where E(P) is the
set of states reachable from a state in the set P via ε-transitions.
The nondeterministic state complexity of a regular language L, denoted by
nsc(L), is the smallest number of states in any NFA for L. The nondeterministic
state complexity of a unary regular operation ◦is a mapping from N to N deﬁned
as
n 	→max{nsc(L◦) | L is accepted by an n-state NFA}.
The nondeterministic state complexity of a binary regular operation ◦is a map-
ping from N2 to N deﬁned as
(m, n) 	→max{nsc(K ◦L) | K, L accepted by m-state and n -state NFAs, resp.}.
In order to obtain lower bounds on the nondeterministic state complexity
of regular languages, the so-called fooling set method is usually used. A set of

Operations on Subregular Languages
115
pairs of strings {(xi, yi) | 1 ≤i ≤n} is called a fooling set for some given
language L if (1) xiyi ∈L for each i, and (2) if i ̸= j, then xiyj /∈L or xjyi /∈L.
It is well known that if F is a fooling set for the given regular language L,
then nsc(L) ≥|F| [1, Lemma, p. 188].
To describe fooling sets for languages can be tedious and checking whether
or not a string xiyj is in L may be hard. To avoid such diﬃculties, we use the
technique of fooling sets for MNFAs where to each state of a given MNFA M,
we assign a pair of subsets of the state set of M.
Deﬁnition 1. Let M = (Q, Σ, ·, I, F) be an MNFA. A set {(Rq, Cq) | q ∈Q},
where Rq and Cq are subsets of Q, is a fooling set for the MNFA M if for all
states p, q,
(1) Rq is reachable and Cq is co-reachable in M,
(2) q ∈Rq ∩Cq,
(3) if p ̸= q, then Rp ∩Cq = ∅or Rq ∩Cp = ∅.
Notice that by the deﬁnition above, a fooling set for L(M) exists if and only
if a fooling set for M of the same size exists; if each set Rq is reached by xq and
each set Cq is co-reached by yq, then {(xq, yR
q ) | q ∈Q} is a fooling set for L(M),
and vice versa. Therefore, we immediately get the following observation.
Lemma 2 ([11, Lemma 4], [12, Lemma 4]).
Let M = (Q, Σ, ·, I, F) be an
MNFA such that at least one of the following conditions holds:
(a) there exists a fooling set {(Rq, Cq) | q ∈Q},
(b) each singleton subset of Q is reachable and co-reachable in M.
Then nsc(L(M)) ≥|Q|.
⊓⊔
To describe a fooling set for the complement of a language may be cumber-
some, cf. [13, Theorem 5]. The condition in the following lemma guarantees the
existence of such a fooling set.
Lemma 3 ([12, Proposition 6]). Let L be a language accepted by an NFA in
which k subsets of the state set are reachable and each of their complements is
co-reachable. Then nsc(Lc) ≥k.
⊓⊔
So if we prove that each subset of states of an NFA A is both reachable and
co-reachable, then nsc(L(A)c) = 2n. Notice that the reachability of all subsets in
the NFA M from [13, Theorem 5] can be easily shown, and since M is isomorphic
to its reverse, so can be the co-reachability. This immediately gives a lower
bound 2n for the complement of L(M).
The union of two NFAs of m and n states is accepted by an (m + n)-state
MNFA. To get an NFA for union, one more state may be needed. However, we
cannot construct a fooling set for union of size m + n + 1 since we already have
an MNFA of size m + n. A similar observation works for reversal as well. In [14,
Lemma 4], a modiﬁed fooling set method has been described. Now we present it
using the reachable and co-reachable sets instead of strings.

116
M. Hospod´ar et al.
Lemma 4 (ST -Lemma, cf. [11, Lemma 8]). Let Q be a set of states. Let S
and T be disjoint sets of pairs of subsets of Q and let U and V be two subsets
of Q such that S ∪T , S ∪{(I, U)}, and T ∪{(I, V )} are fooling sets for the
MNFA M = (Q, Σ, ·, I, F). Then nsc(L(M)) ≥|S| + |T | + 1.
⊓⊔
Next, we introduce the language classes considered in this paper. These lan-
guages were already examined to some extent in [2], with the exception of group
languages which were investigated in [7]. A language L is
• combinational (class abbreviation CB): if L = Σ∗H for H ⊆Σ;
• ﬁnitely generated left ideal (FGLID): if L = Σ∗H for some ﬁnite language H
(in [2] called noninitial deﬁnite);
• left ideal (LID): if L = Σ∗L (in [2] called ultimate deﬁnite);
• group language (GRP): if it is accepted by a permutation DFA (equivalently,
if the minimal DFA for L is a permutation one);
• star (STAR): if L = G∗for a regular language G [3] (equivalently, L = L∗);
• comet (COM): if L=G∗H for some regular languages G, H with G/∈{∅, {ε}};
• two-sided comet (2COM): if L=EG∗H for regular E, G, H with G/∈{∅, {ε}};
• ordered (ORD): if it is accepted by a (possibly non-minimal) DFA with
ordered states such that p ⪯q implies p · σ ⪯q · σ for each symbol σ [17];
• star-free (SFREE): if L is constructable from ﬁnite languages, concatenation,
union, and complementation (equivalently, if L has an aperiodic DFA) [16];
• power-separating (PSEP): if for every x in Σ∗there exists an integer m such
that 
i≥m{xi} ⊆L or 
i≥m{xi} ⊆Lc [18].
We have CB ⊊FGLID ⊊LID, STAR ⊊COM ⊊2COM, and ORD ⊊STFR ⊊
PSEP [2]; the only star language that is not a comet is {ε}.
3
Results
In this section, we gradually present our obtained results by lemmas individually
focusing on the considered operations and language classes. They are grouped
together based on their proof structure, with summarizing tables included in
the Conclusions section. We proceed with the proposition considering all oper-
ations on the class of combinational languages. This class is special since every
combinational language has nondeterministic state complexity at most two.
Proposition 5. Let m, n ≥2. Let K and L be combinational languages over Σ
accepted by m-state and n-state NFAs. Then
(1) nsc(K), nsc(L) ≤2,
(2) nsc(K ∩L), nsc(K ∪L), nsc(LR) ≤2, and this bound is tight if |Σ| ≥1,
(3) nsc(L∗), nsc(Lc) ≤2, and this bound is tight if |Σ| ≥2,
(4) nsc(KL) ≤3 and nsc(Lk) ≤k + 1, and these bounds are tight if |Σ| ≥1.
The next two lemmas consider intersection and union on ﬁnitely generated
left ideal languages.

Operations on Subregular Languages
117
Lemma 6. Let m, n ≥3 and Σ = {a1, a2, . . . , am−1, b1, b2, . . . , bn−1, c, d, e}.
Let A, B, C, and D be the NFAs from Fig. 1. Then L(A), L(B), L(C), and L(D)
are ﬁnitely generated left ideal languages such that nsc(L(A) ∩L(B)) = mn
and nsc(L(C) ∪L(D)) = m + n −1.
Proof. First we consider intersection. We can get an NFA accepting a ﬁnite
generator of L(A) from A by removing loops in the initial state, adding ﬁnal
states m + 1, m + 2, . . . , 2m connected through transitions m + 1
bj
−→m + 2
bj
−→
· · ·
bj
−→2m for each j, and adding transitions (q, σ, m + 1) for each transition
(q, σ, m) in A. A similar construction can be done for B. Hence L(A) and L(B)
are ﬁnitely generated left ideals. Consider the product automaton A × B for
L(A) ∩L(B). For each (i, j) in {1, 2, . . . , m} × {1, 2, . . . , n}, deﬁne the following
sets:
[i, j]
= {1, 2, . . . , i} × {1, 2, . . . , j} if 1 ≤i ≤m −1 and 1 ≤j ≤n −1;
i, n = {1, 2, . . . , i} × {1, n} if 1 ≤i ≤m −1;
m, j = {1, m} × {1, 2, . . . , j} if 1 ≤j ≤n −1;
m, n = {1, m} × {1, 2, . . . , n}.
To each state (i, j) in A × B, we assign a pair of sets Ri,j and Ci,j as follows:
(Ri,j, Ci,j) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
([i, j], {(i, j)}),
if 1 ≤i ≤m−1 and 1 ≤j ≤n−1;
(i, n, {(i, n−1), (i, n)}),
if 1 ≤i ≤m−1 and j = n;
(m, j, {(m−1, j), (m, j)}),
if i = m and 1 ≤j ≤n−1;
(m, n, {(m, n)}),
if i = m and j = n.
It can be shown that the set {(Ri,j, Ci,j) | 1 ≤i ≤m and 1 ≤j ≤n} is a
fooling set for A × B of size mn. Hence nsc(K ∩L) = mn by Lemma 2(a).
Now we consider union. We have L(C) = (a + b + c + d)∗a(a + b)m−2a∗=
(a + b + c + d)∗a(a + b)m−2a≤m−1, so L(C) is a ﬁnitely generated left ideal. By
a similar argument, the language L(D) is a ﬁnitely generated left ideal as well.
Construct the NFA N for L(C)∪L(D) from automata C and D by omitting the
state 0 and by adding the transition (1, c, m+1). For each state of N, deﬁne the
following pairs of sets:
(Ri, Ci) =

({1, i}, {i}),
if 1 ≤i ≤m + n −2and i ̸= m;
({1, i}, {i −1, i}),
if i ∈{m, m + n −1}.
Then {(Ri, Ci) | 1 ≤i ≤m+n−1} is a fooling set for the NFA N of size m+n−1.
Hence nsc(L(C) ∪L(D)) = m + n −1 by Lemma 2(a).
⊓⊔
The next lemma considers intersection, union, and concatenation on the class
of group languages; notice that we use the same witnesses for all three operations.
Lemma 7. Let m, n ≥2. Let A and B be the NFAs from Fig. 2. Then L(A)
and L(B) are group languages, and nsc(L(A)∩L(B)) = mn, nsc(L(A)∪L(B)) =
m + n + 1, and nsc(L(A)L(B)) = m + n.

118
M. Hospod´ar et al.
Fig. 1. Finitely generated left ideal witnesses for intersection and for union.
Proof. In the product automaton for L(A)∩L(B), each singleton set is reachable
and co-reachable, so nsc(L(A) ∩L(B)) = mn by Lemma 2(b).
In the case of union, we may assume that m ≤n. Construct the MNFA M
for L(A) ∪L(B) in the standard way. In M, each set {pi, qj} is reachable and
co-reachable. For each state q of M, we deﬁne the pair of sets (Rq, Cq) as follows:
(Rpi, Cpi) = ({pi, q(i−1) mod m}, {pi, q(i−2) mod m}),
(Rqj, Cqj) = ({qj, p(j+2) mod m}, {qj, p(j+1) mod m}).
Then S = {(Rpi, Cpi) | i = 0, 1, . . . , m−1}, T = {(Rqj, Cqj) | j = 0, 1, . . . , n−1},
I = {p0, q0}, U = {q0, p1}, and V
= {p0, qm−2} satisfy the conditions of
Lemma 4, so nsc(L(A) ∪L(B)) = m + n + 1.
To get an NFA N for L(A)L(B) from NFAs A and B, add the transi-
tion (pm−1, ε, q0), and make the state pm−1 non-ﬁnal and the state q0 non-initial.
Then the set {({pi}, {pi, qn−1}) | 0 ≤i ≤m−2}∪{({pm−1, q0}, {pm−1, qn−1})}∪
{({p0, q0}, {pm−1, q0})} ∪{({p0, qj}, {qj}) | 1 ≤j ≤n −1} is a fooling set for N
of size m + n, so nsc(L(A)L(B)) by Lemma 2(a).
⊓⊔
Fig. 2. Binary group witnesses for intersection, union, and concatenation.
To get star witnesses, construct the NFAs A′ and B′ from A and B in Fig. 2
by making the initial state ﬁnal and all other states non-ﬁnal. Then L(A′)
and L(B′) are star languages. Moreover, all the sets from the proof above

Operations on Subregular Languages
119
are still reachable and co-reachable, so we have nsc(L(A′) ∩L(B′)) = mn
and nsc(L(A′) ∪L(B′)) = m + n + 1. The concatenation of star languages (am)∗
and (bn)∗has nondeterministic state complexity m + n, as shown in [8, The-
orem 7]. The next lemma considers intersection, union, and concatenation on
ordered languages.
Lemma 8. Let m, n ≥2, K = (b∗a)m−1b∗, and L = (a∗b)n−1a∗. Then K and L
are ordered languages accepted by m-state and n-state NFAs, nsc(K ∩L) = mn,
nsc(K ∪L) = m + n + 1, and nsc(KL) = m + n.
Proof. The languages K and L are accepted by DFAs A and B from Fig. 3. In
the product automaton A × B for K ∩L, each singleton set is reachable and
co-reachable. This gives the tight lower bound mn by Lemma 2(b).
Let M be the MNFA containing all states and transitions of A and B.
Then L(M) = K ∪L. In the MNFA M, the initial set is {p1, q1}, and each
singleton set is both reachable and co-reachable by a string in a∗b∗or in b∗a∗.
Let S = {({pi}, {pi}) | 1 ≤i ≤m}, T = {({qj}, {qj}) | 1 ≤j ≤n}. I = {p1, q1},
U = {q1}, and V = {p1}. Then the sets S ∪T , S ∪{(I, U)}, and T ∪{(I, V )}
are fooling sets for A ∪B. Hence nsc(K ∪L) = m + n + 1 by Lemma 4.
Construct the NFA N from M by adding the transition (pm, ε, q1), mak-
ing the state q1 non-initial, and making the state pm non-ﬁnal. Then L(N) =
KL, and the set {({pi}, {pi}) | 1 ≤i ≤m −1} ∪{({pm, q1}, {pm})} ∪
{({q1}, {pm, q1})} ∪{({qj}, {qj}) | 2 ≤j ≤n} is a fooling set for N. Hence
nsc(KL) = m + n.
⊓⊔
Fig. 3. Binary ordered witnesses for intersection, union, and concatenation.
In what follows, we consider the unary operations of the k-th power, star,
reversal, and complementation. We start with the class of star languages.
Lemma 9. Let L be a star language. Then Lk = L∗= L and nsc(LR) = nsc(L).
Proof. We have Lk ⊆L∗= L by deﬁnition. To show that L ⊆Lk, let w ∈L.
Since L = L∗, we have ε ∈L. Set u1 = w and u2 = u3 = · · · = uk = ε.
Then w = u1u2 · · · uk with ui ∈L, so w ∈Lk. Thus Lk = L∗= L.
For reversal, notice that each star language is accepted by an NFA A with a
single ﬁnal state which is the initial state. Then LR is accepted by the NFA AR
which has the same number of states and the same initial and ﬁnal state. Hence
nsc(LR) ≤nsc(L). Since (LR)R = L, we have nsc(LR) = nsc(L).
⊓⊔

120
M. Hospod´ar et al.
The language (an−1b)∗an−1 is a comet and ordered language and it meets
the upper bound kn on the complexity of the k-th power if n ≥2 [4, Theorem 3].
Now we consider the k-th power on the class of group languages and we show
that the complexity in this class is kn as well.
Lemma 10. Let k ≥2 and n ≥3. Let A be the binary NFA from Fig. 4.
Then L(A) is a group language and nsc(L(A)k) = kn.
Proof. Construct the NFA N for L(A)k from k copies of A in the standard way;
assume that the copies are numbered from 1 to k and the state j in the i-th copy
is denoted (i, j). For each state (i, j) with j ̸= n −1, set
Ri,j = {(i, j)} ∪{(i −ℓ, (j −ℓ) mod (n −1)) | 1 ≤ℓ≤i −1},
that is, in Ri,j we have states (i, j), (i −1, j −1), (i −2, j −2), . . . where the
second component is modulo n −1. Next, set
Ri,n−1 ={(p, q) | q ≤n −3 and (p, q) ∈Ri,n−2}∪
{(p, n −1) | (p, n −2) ∈Ri,n−2} ∪{(i + 1, 0)},
that is, to get Ri,n−1, we move each state of Ri,n−2 in column n −2 to the
corresponding state in column n −1, and we add the state (i + 1, 0), so we have
Ri,n−2
b−→Ri,n−1. Moreover Ri,n−1
b−→Ri,n−2 ∪{(i + 1, 0)} = Ri+1,0.
Denote by i, j the set {(i, j), (i + 2, j), . . . , (i + 2p, j)} where i + 2p is either
k −1 or k, that is, the set containing the state j in copies i, i + 2, . . . , i + 2p. Set
Ci,j =
⎧
⎪
⎨
⎪
⎩
i, j ∪i + 1, n −1,
if 1 ≤j ≤n −2 or (i, j) = (1, 0);
i, n −1 ∪i + 1, n −2,
if j = n −1;
i, 0 ∪i −1, n −1,
if j = 0 and i ̸= 1.
The set {(Ri,j, Ci,j) | 1 ≤i ≤k and 0 ≤j ≤n −1} is a fooling set for N of
size kn. Hence nsc(L(A)k) = kn by Lemma 2(a).
⊓⊔
It was shown in [8, Theorem 9] that for the language L = (an)∗an−1, we
have nsc(L∗) = n + 1. Since L is a group and comet language, L+ is ordered,
and (L+)∗= L∗, the nondeterministic state complexity of star in the classes of
group, comet, and ordered languages is n + 1. The next lemma shows that the
complexity of star on ﬁnitely generated left ideal languages is n + 1 as well.
Lemma 11. Let n ≥4. Let K = (a + b)∗an−2(a + b)a∗. Then K is a ﬁnitely
generated left ideal language accepted by an n-state NFA, and nsc(K∗) = n + 1.
Fig. 4. A binary group witness for power meeting the upper bound kn.

Operations on Subregular Languages
121
Proof. Since we have (a + b)∗an−2(a + b)a∗= (a + b)∗an−2(a + b)a≤n−1, the
language K is a ﬁnitely generated left ideal accepted by the NFA A from Fig. 5.
Construct the MNFA A∗for L(A)∗by adding a new initial and ﬁnal state 0 and
the transitions (n −1, a, 1), (n −1, b, 1), (n, a, 1). To each state i of A∗, we assign
the following pair of sets:
(Ri, Ci) =
⎧
⎪
⎨
⎪
⎩
({0, 1}, {0, n}),
if i = 0;
({1, 2, . . . , i}, {i}),
if 1 ≤i ≤n −1;
({1, n}, {n −1, n}),
if i = n.
Then {(Ri, Ci) | 0 ≤i ≤n} is a fooling set for A∗. Hence nsc(L(A)∗) = n + 1. ⊓⊔
Fig. 5. A binary ﬁnitely generated left ideal witness for star meeting the bound n+1.
The following two lemmas consider the reversal on classes of ﬁnitely generated
left ideal and group languages.
Lemma 12. Let n ≥4. Consider the NFAs A and B from Fig. 6 and the lan-
guage L = (an−1b)∗a≤n−1. Then L(A) is a ﬁnitely generated left ideal language,
L(B) is a group language, L is a comet and ordered language accepted by an
n-state NFA, and we have nsc(L(A)R) = nsc(L(B)R) = nsc(LR) = n + 1.
Proof. In the lemma statement, we consider three witness languages. We present
the proofs gradually, starting with L(A) = (a+b+c)∗
c+(cc+an−3(a+b))a∗	
=
(a + b + c)∗
c + (cc + an−3(a + b))a≤n−2	
, so L(A) is a ﬁnitely generated left
ideal. For each state i of AR, deﬁne the sets
Ri =

{i},
if 1 ≤i ≤n −1;
{n −1, n},
if i = n,
Ci =

{1, 3, 4, . . . , i},
if 3 ≤i ≤n −1,
{1} ∪{i},
if i ∈{1, 2, n}.
Next, let S = {(Ri, Ci) | 1 ≤i ≤2}, T = {(Ri, Ci) | 3 ≤i ≤n}, I = {2, n},
U = {1, n}, and V = {1, 2}. Then S ∪T , S ∪{(I, U)}, and T ∪{(I, V )} satisfy
the conditions of Lemma 4, so nsc(L(A)R) = n + 1.
Now let us consider the witness for reversal on group languages, i.e., NFA B,
which is actually a DFA. Since the symbols a and b perform permutations on the
state set of B, the language L(B) is a group language. Consider the following
pairs of subsets of states of B:
(Ri, Ci) =

({i, i + 1}, {i}),
if 1 ≤i ≤n −1;
({n, 2}, {n}),
if i = n,

122
M. Hospod´ar et al.
and set S = {(Ri, Ci) | i = 1, 2, . . . , n−1}, T = {(Rn, Cn)}, I = {n, 1}, U = {n},
and V = {1}. Then S ∪T , S ∪{(I, U)}, and T ∪{(I, V )} satisfy the conditions
of Lemma 4, so nsc(L(B)R) = n + 1.
Finally, consider the comet language L = (an−1b)∗a≤n−1. It is accepted by
the NFA C shown in Fig. 6. To get an ordered DFA for L from C, add the dead
states 0 and n+1 and the transitions (n, a, n+1) and (i, b, 0) for i = 1, 2, . . . , n−1.
Hence L is ordered. In C, each singleton set {i} is reachable by ai−1, and in CR,
the initial set is I = {1, 2, . . . , n} and each singleton set {i} is reachable by
ban−i. Set S = {({1}, {1})}, T = {({i}, {i}) | 2 ≤i ≤n}, U = {2}, and V = {1}.
Then S ∪T , S ∪{(I, U)}, and T ∪{(I, V )} are fooling sets for CR. It follows
that nsc(LR) = n + 1 by Lemma 4.
⊓⊔
Fig. 6. A ﬁnitely generated left ideal, group, and ordered witnesses for reversal.
In the last two lemmas, we consider complementation. The upper bound on
left ideals is known to be 2n−1 [12, Theorem 37(1)] and we provide a ﬁnitely gen-
erated witness for this bound. For stars and ordered languages, the complexity
is 2n, and for group language, we have a hyperpolynomial lower bound
 n−1
⌊n/2⌋
	
.
Lemma 13. Let n ≥3. Let A, B, C be the NFAs from Fig. 7; ai..j denotes
the transitions on ai, ai+1, . . . , aj. Then L(A) is a ﬁnitely generated left ideal,
L(B) is a star language, L(C) is ordered, and we have nsc(L(A)c) = 2n−1
and nsc(L(B)c) = nsc(L(C)c) = 2n.
Proof. We provide a proof only for the ordered witness L(C). In the subset
automaton D(C), let us assign the value pS = 2i1 + 2i2 + · · · + 2ik to a set S =
{i1, i2, . . . , ik} with n−1 ≥i1 > i2 > · · · > ik ≥0. It follows from the transitions
deﬁned in the NFA C that in D(C), we have pS
a−→⌊pS/2⌋, pS
b−→0 if pS ∈{0, 1}
and pS
b−→2n−1 + ⌊pS/2⌋otherwise, and pS
cj
−→pS if j /∈S or 0 ∈S, and pS
cj
−→
pS + 1 otherwise. Since all these transformations preserve the order of states
in D(C) given by their corresponding values, the language L(C) is ordered.

Operations on Subregular Languages
123
Fig. 7. Finitely generated left ideal, star, and ordered witnesses for complementation.
In C, the empty set and each singleton set is reachable by a string in a∗. Each
set {n −1} ∪S of size k is reached from the set {i + 1 | i ∈S} of size k −1
by b, and each set S with n −1 /∈S of size k is reached from a set of size k
containing n −1 by a string in a∗. This proves the reachability of all subsets
in C by induction. To get co-reachability, we use the symbols cj. The empty set
and each singleton set is co-reachable by a string in a∗. Each set S with 0 ∈S
and max S = j is co-reached from S \ {j} by cj. Each set S with 0 /∈S is co-
reached from a set of the same size containing 0 by a string in a∗. It follows that
all sets are co-reachable. Hence by Lemma 3, we have nsc(L(C)c) = 2n.
⊓⊔
Lemma 14. Let n ≥4. Let M be the binary MNFA from Fig. 8 with k = ⌊n/2⌋.
Then L(M) is a group language with nsc(L(M)) ≤n and nsc(L(M)c) =
n−1
k
	
.
Proof. Since in M, the symbols a and b form a generator of all permutations on
states from 1 to n −1, each subset of {1, 2, . . . , n −1} of size k is reachable and
each subset of {1, 2, . . . , n −1} of size n −1 −k is co-reachable in M. In total
we have
n−1
k
	
=
 n−1
⌊n/2⌋
	
reachable sets and their co-reachable complements.
This gives the lower bound for nsc(L(M)c) by Lemma 3. To get an equivalent
n-state NFA A from M, add the initial state 0, make all other states non-
initial, and add transitions (0, b, i) and (0, a, i + 1) for each i = 1, 2, . . . , k. Then
nsc(L(A)c) = nsc(L(M)c) ≥
 n−1
⌊n/2⌋
	
.
⊓⊔
Fig. 8. A binary group language meeting the lower bound
 n−1
⌊n/2⌋

for complementation.

124
M. Hospod´ar et al.
4
Conclusions
Our results are summarized in the following two theorems and tables. Recall
CB ⊊FGLID ⊊LID, STAR ⊊COM ⊊2COM, ORD ⊊STFR ⊊PSEP [2].
Theorem 15. The nondeterministic state complexity of intersection, union,
and concatenation on some subclasses of regular languages is given by Table 1.
Proof. The results for combinational languages follow from Proposition 5.
For the remaining classes, ﬁrst we handle intersection. The upper bounds
are the same as for regular languages. Finitely generated left ideal witnesses are
described in Lemma 6. Group witnesses are given in Lemma 7; notice that if we
modify them such that only the initial state is ﬁnal, they are star (so also comet
and two-sided comet) witness languages. Ordered (so also power-separating)
witnesses are provided in Lemma 8.
Now consider union. The ﬁnitely generated left ideals from Lemma 6 meet
the upper bound for left ideals. In the remaining classes, the upper bounds are
the same as in the regular case. Group witnesses that can be modiﬁed to star (so
also comet and two-sided comet) languages are described in Lemma 7. Ordered
(so also power-separating) witnesses are given by Lemma 8.
Finally we discuss concatenation. The result for ﬁnitely generated left ideals
follows from [12, Theorem 16] where it is shown that the upper bound for left
ideals is m + n −1; the unary witnesses a∗am−1 and a∗an−1 described in this
theorem are ﬁnitely generated left ideals. In all the remaining cases, we have the
regular upper bounds. The group witnesses are given in Lemma 7. A proof using
the same fooling set as for the group languages works also for the star (so also
comet and two-sided comet) witnesses (am)∗and (bn)∗, cf. [8, Theorem 7]. The
ordered (so also power-separating) witnesses are deﬁned in Lemma 8.
⊓⊔
Table 1. Results for binary operations; ⋄means the witness from above can be used.
K ∩L
|Σ|
K ∪L
|Σ|
KL
|Σ|
CB
2
1
2
1
3
1
FGLID
mn
m + n + 1
m + n −1
4
m + n −1
1
LID [12]
mn
2
m + n −1
2
⋄
GRP
mn
2
m + n + 1
2
m + n
2
STAR
mn
2
m + n + 1
2
m + n
2
COM
⋄
⋄
⋄
2COM
⋄
⋄
⋄
ORD
mn
2
m + n + 1
2
m + n
2
STFR [9]
⋄
⋄
⋄
PSEP
⋄
⋄
⋄
REG [8]
mn
2
m + n + 1
2
m + n
2

Operations on Subregular Languages
125
Theorem 16. The nondeterministic state complexity of power, star, reversal,
and complementation on some subclasses of regular languages is given by Table 2.
Proof. The results for combinational languages follow from Proposition 5. Now
let us examine the remaining classes.
First we consider the k-th power. The upper bound k(n−1)+1 for left ideals
is met by the unary ﬁnitely generated left ideal a∗an−1 [11, Theorem 12(c)]. The
tight upper bound for star languages is n by Lemma 9. The general upper bound
kn is met by the group language described in Lemma 10. The ordered language
(an−1b)∗an−1 is also a comet (and two-sided comet) language and it meets the
upper bound kn as shown in [4, Theorem 3].
The complexity of the star and reversal operations on star languages is n
by Lemma 9. For the other classes, the upper bound n + 1 for star is met by
binary ﬁnitely generated left ideal language from Lemma 11 which is also a comet
and two-sided comet language, by unary group and comet language (an)∗an−1,
and by unary ordered (so also power-separating) language (an−1 + an)∗an−1,
as shown in [8, Theorem 9]. This upper bound for reversal is met by a ternary
ﬁnitely generated left ideal language, a binary group language, and the binary
comet and ordered language (an−1b)∗a≤n−1, as shown in Lemma 12.
The upper bound 2n−1 for complement on left ideals from [12, Theorem 37(1)]
is met by the ﬁnitely generated left ideal over an alphabet of size n −1 from
Lemma 13. The regular upper bound 2n is met by a binary star (so also comet
and two-sided comet) language and by ordered language over an alphabet of
size n + 1, as shown in Lemma 13, and by the binary star-free (so also power-
separating) language from [9, Theorem 5]. Finally, a binary group language meet-
ing the lower bound
 n−1
⌊n/2⌋
	
for complementation is described in Lemma 14.
⊓⊔
Table 2. Results for unary operations;⋄means the witness from above can be used.
Lk
|Σ|
L∗
|Σ|
LR
|Σ|
Lc
|Σ|
CB
k + 1
1
2
2
2
1
2
2
FGLID
k(n −1) + 1
1
n + 1
2
n + 1
3
2n−1
n −1
LID [12]
⋄
⋄
n + 1
2
2n−1
2
GRP
kn
2
n + 1
1
n + 1
2
≥
 n−1
⌊n/2⌋

2
STAR
n
1
n
1
n
1
2n
2
COM
kn
2
n + 1
1
n + 1
2
⋄
2COM
⋄
⋄
⋄
⋄
ORD
kn
2
n + 1
1
n + 1
2
2n
n + 1
STFR [9]
⋄
⋄
⋄
2n
2
PSEP
⋄
⋄
⋄
⋄
REG
kn [4]
2
n + 1 [8]
1
n + 1 [13]
2
2n [13]
2

126
M. Hospod´ar et al.
References
1. Birget, J.: Intersection and union of regular languages and state complex-
ity. Inf. Process. Lett. 43(4), 185–190 (1992). https://doi.org/10.1016/0020-
0190(92)90198-5
2. Bordihn, H., Holzer, M., Kutrib, M.: Determination of ﬁnite automata accepting
subregular languages. Theor. Comput. Sci. 410(35), 3209–3222 (2009). https://
doi.org/10.1016/j.tcs.2009.05.019
3. Brzozowski, J.A.: Roots of star events. J. ACM 14(3), 466–477 (1967). https://
doi.org/10.1145/321406.321409
4. Domaratzki, M., Okhotin, A.: State complexity of power. Theor. Comput. Sci.
410(24–25), 2377–2392 (2009). https://doi.org/10.1016/j.tcs.2009.02.025
5. Han, Y.S., Salomaa, K.: Nondeterministic state complexity for suﬃx-free regular
languages. Electron. Proc. Theor. Comput. Sci. 31, 189–196 (2010). https://doi.
org/10.4204/eptcs.31.21
6. Han, Y.S., Salomaa, K., Wood, D.: Nondeterministic state complexity of basic
operations for preﬁx-free regular languages. Fundam. Inform. 90, 93–106 (2009).
https://doi.org/10.3233/FI-2009-0008
7. Hoﬀmann, S.: State complexity bounds for the commutative closure of group lan-
guages. In: Jir´askov´a, G., Pighizzini, G. (eds.) DCFS 2020. LNCS, vol. 12442, pp.
64–77. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-62536-8 6
8. Holzer, M., Kutrib, M.: Nondeterministic descriptional complexity of regular lan-
guages. Int. J. Found. Comput. Sci. 14(6), 1087–1102 (2003). https://doi.org/10.
1142/S0129054103002199
9. Holzer, M., Kutrib, M., Meckel, K.: Nondeterministic state complexity of star-free
languages. Theor. Comput. Sci. 450, 68–80 (2012). https://doi.org/10.1016/j.tcs.
2012.04.028
10. Hopcroft, J.E., Ullman, J.D.: Introduction to Automata Theory, Languages and
Computation. Addison-Wesley, Boston (1979)
11. Hospod´ar, M.: Power, positive closure, and quotients on convex languages. Theor.
Comput. Sci. 870, 53–74 (2021). https://doi.org/10.1016/j.tcs.2021.02.002
12. Hospod´ar, M., Jir´askov´a, G., Mlyn´arˇcik, P.: Nondeterministic complexity in sub-
classes of convex languages. Theor. Comput. Sci. 787, 89–110 (2019). https://doi.
org/10.1016/j.tcs.2018.12.027
13. Jir´askov´a, G.: State complexity of some operations on binary regular languages.
Theor. Comput. Sci. 330(2), 287–298 (2005). https://doi.org/10.1016/j.tcs.2004.
04.011
14. Jir´askov´a, G., Masopust, T.: Complexity in union-free regular languages. Int.
J. Found. Comput. Sci. 22(07), 1639–1653 (2011). https://doi.org/10.1142/
S0129054111008933
15. Rabin, M.O., Scott, D.: Finite automata and their decision problems. IBM J. Res.
Dev. 3(2), 114–125 (1959). https://doi.org/10.1147/rd.32.0114
16. Sch¨utzenberger, M.P.: On ﬁnite monoids having only trivial subgroups. Inf. Control
8(2), 190–194 (1965). https://doi.org/10.1016/S0019-9958(65)90108-7
17. Shyr, H.J., Thierrin, G.: Ordered automata and associated languages. Tamkang J.
Math. 5, 9–20 (1974)
18. Shyr, H.J., Thierrin, G.: Power-separating regular languages. Math. Syst. Theory
8(1), 90–95 (1974). https://doi.org/10.1007/BF01761710

On Simon’s Congruence Closure
of a String
Sungmin Kim1, Yo-Sub Han1(B), Sang-Ki Ko2(B), and Kai Salomaa3
1 Department of Computer Science, Yonsei University, 50 Yonsei-Ro,
Seodaemun-Gu, Seoul 03722, Republic of Korea
{rena rio,emmous}@yonsei.ac.kr
2 Department of Computer Science and Engineering, Kangwon National University,
1 Kangwondaehak-gil, Chuncheon-si, Gangwon-do 24341, Republic of Korea
sangkiko@kangwon.ac.kr
3 School of Computing, Queen’s University, Kingston, ON K7L 3N6, Canada
ksalomaa@cs.queensu.ca
Abstract. Two strings are Simon’s ∼k-congruent if they have the same
set of subsequences of length at most k. We study the Simon’s congruence
closure of a string, which is regular by deﬁnition. Given a string w over
an alphabet Σ, we present an eﬃcient DFA construction that accepts all
∼k-congruent strings with respect to w. We also present lower bounds for
the state complexity of the Simon’s congruence closure. Finally, we design
a polynomial-time algorithm that answers the following open problem:
“given a string w over a ﬁxed-sized alphabet, an integer k and a (regular
or context-free) language L, decide whether there exists a string v ∈L
such that w ∼k v.” The problem is NP-complete for a variable-sized
alphabet.
Keywords: Simon’s congruence · State complexity · Finite
automata · Shortlex normal forms
1
Introduction
Simon’s congruence ∼k is a relation on strings such that two strings are ∼k-
congruent if they have the same set Sk of subsequences of length at most k [12].
For example, strings w1 = ababac and w2 = aabbac are ∼2-congruent since
S2(w1) = S2(w2). However, they are not ∼3-congruent because bab ∈S3(w1) but
bab /∈S3(w2).
Given two strings w1 and w2, the MaxSimK problem is to compute the
largest k for which w1 ∼k w2. After H´ebrard [7] presented a linear-time algo-
rithm for MaxSimK over a binary alphabet, it was a long standing open problem
to ﬁnd a generalized optimal algorithm. Garel [5] presented an algorithm based
on ﬁnite automata running in O(|Σ||w2|) for a special case when w2 = w1a for a
symbol a. Fleischer and Kuﬂeitner [3] and Barker et al. [1] suggested using binary
search on k by repetitively testing for w1 ∼k w2 using a linear decision algo-
rithm. Recently, Gawrychowski et al. [6] proposed the ﬁrst linear-time optimal
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 127–141, 2022.
https://doi.org/10.1007/978-3-031-13257-5_10

128
S. Kim et al.
algorithm over general alphabets using novel data structures called Simon-Trees,
and settled the MaxSimK problem for two strings. Then, as future work, they
proposed the following problems between a string w and a regular or context-free
language L.
– LangSimK: Decide whether there exists a string v ∈L such that w ∼k v.
– MaxLangSimK: Find the maximum k for which there exists a string v ∈L
such that w ∼k v.
The motivation for our research is to ﬁnd an eﬃcient solution for LangSimK
and MaxLangSimK. We tackle the problem by constructing a deterministic
ﬁnite-state automaton (DFA) that recognizes a set Closurek(w) of strings that
are ∼k-congruent with w. For example, Closure2(aba) = L(a+ba+). While it is
obvious that Closurek(w) is regular, the number of possible subsequences |Sk(w)|
is exponential in |Σ|. Indeed, Karandikar et al. [9] proved that the number of
equivalence classes |{Closurek(w) | w ∈Σ∗}| is in 2Θ(k|Σ|−1) log k. This leads us
to study an eﬃcient automaton construction for the language Closurek(w) for a
given string w and an integer k.
The Simon’s congruence closure is closely related to the permutation opera-
tion (the commutative closure). The permutation set of a string w (or a language
L) is deﬁned to be the set of all strings w′ that have the same Parikh vectors
with w (or any string in L). For instance, Closure1(w) = perm(L) holds if
L = {u | u ∈Σ∗w1Σ∗w2 · · · Σ∗w|w|Σ∗, w = w1w2 · · · w|w|, and Σ = ∪|w|
i=1{wi}}.
Cho et al. [2] studied the state complexity of the permutation operation on ﬁnite
languages as the permutation operation is not regularity-preserving. Recently,
Hoﬀman [8] extended the result to the permutation operation on alphabetical pat-
tern constraints (APC) that are deﬁned as union of the form Σ∗
0a1Σ∗
1 · · · anΣ∗
n
with Σ0, . . . , Σn ⊆Σ. Another topic related to Simon’s congruence is the k-
binomial equivalence relation ≡k [4,10] that takes the number of occurrences of
each subsequence into account when computing the equivalence classes. Frey-
denberger et al. [4] presented two polynomial algorithms for testing k-binomial
equivalence between two strings. In case of k = 2, Lejeune et al. [10] proposed
an algorithm to enumerate all words in the k-binomial closure of string w.
We present a DFA construction that is polynomial in |w| and k. The
main idea of the construction is to reverse a normalization algorithm, which
computes the lexicographically smallest string among the shortest strings in
Closurek(w) [3]. Then, using the resulting DFA Aw for Closurek(w), we eﬃ-
ciently solve LangSimK by intersecting Aw and a given regular or context-free
language L and checking its emptiness; the intersection is exactly the set of
all strings from L that are ∼k-congruent with w. Finally, we eﬃciently solve
MaxLangSimK by repeating the procedure log |w| times.
2
Preliminaries
Basic Deﬁnitions and Notation. Let Σ be a ﬁnite alphabet and Σ∗be the set
of all strings over Σ. A language is a subset of Σ∗. The cardinality of a ﬁnite

On Simon’s Congruence Closure of a String
129
set S is denoted by |S|. Given a string w, we use |w| to denote the length of
w. A nondeterministic ﬁnite-state automaton (NFA) is a machine deﬁned by a
tuple A = (Q, Σ, δ, Q0, F), where Σ is a ﬁnite alphabet, Q is a ﬁnite set of states,
Q0 ⊆Q is a set of initial states, F ⊆Q is a set of ﬁnal states and δ is a transition
function from Q × Σ into 2Q. The automaton A is deterministic (a DFA) if Q0
is a singleton set and δ is a partial function Q×Σ →Q. The size |A| of a DFA A
is the number of states. We use L(A) to denote the language recognized by A.
The state complexity sc(L) of a regular language L is the size of the minimal
DFA recognizing L [15]. For more background in automata theory, the reader
may refer to the textbook [14].
Positions, Rankers and Coordinates. For a string w of size n, a true position of
w refers to an integer i ∈[0, n −1] that maps to a 0-indexed character w[i] in w.
Similarly, an in-between position of w is an integer j ∈[0, n] that maps to a space
at the beginning, between two consecutive characters or at the end of w. The
notation for a substring w[i : j] uses two in-between positions i and j to represent
the sequence of characters between i and j. For example, a string w = abc
has 3 true positions such that w[0] = a, w[1] = b and w[2] = c, and has 4
in-between positions {0, 1, 2, 3}. Then, we have w[1 : 3] = bc because the in-
between position 1 is the space between a and b and the in-between position 3
is the space after c. A ranker is a function that indicates the next occurrence
of a character in a given string [11,13]. An X-ranker Xσ is a binary operator
that takes an in-between position i and a string w as arguments and returns
an in-between position j > i such that w[j −1] = σ and w[i : j −1] does not
contain σ. For instance, when i = 3 and w = abcaacba, we have 3Xbw = 7.
Symmetrically, a Y-ranker Yσ is a binary operator that takes an in-between
position i and a string w as arguments and returns an in-between position j < i
such that w[j] = σ and w[j + 1 : i] does not contain σ. The default position
argument for X- and Y-rankers is 0 and n, respectively. We call a sequence
of rankers a ranker chain. We process a ranker chain from left to right. For
example, for w = aabbaaddc and a ranker chain r = XbXaXdXd, we have
r(w) = 0XbXaXdXdw = 3XaXdXdw = 5XdXdw = 7Xdw = 8. The length of a
ranker chain is the number of operations. Fleischer and Kuﬂeitner [3] deﬁned the
X-coordinate X(w, i) of a true position i to be the length of the shortest X-ranker
chain rX that satisﬁes 0rXw = i + 1. Similarly, they deﬁned the Y-coordinate
Y(w, i) of a true position i is the length of the shortest Y-ranker chain rY such
that nrY w = i. The pair of X- and Y-coordinates forms an attribute of a true
position i. For w = aabbaaddc and a true position 4, the shortest X-ranker chain
returning 5 is XbXa and the shortest Y-ranker chain returning 4 is YaYa. The
attribute of position 4 is (2, 2).
Simon’s Congruence. A subsequence u of a string w is a string that satisﬁes
u = w[i1]w[i2] · · · w[ik] for positions 0 ≤i1 < i2 < · · · < ik ≤|w| −1. Let
Sk(w) denote the set of all subsequences u of w such that |u| ≤k. We say
that two strings w1 and w2 are ∼k-congruent if Sk(w1) = Sk(w2). The con-
gruence relation ∼k is called Simon’s Congruence. Now, for a string w and an

130
S. Kim et al.
integer k, we can deﬁne the Simon’s congruence closure, Closurek(w), of w to
be a set of strings that are ∼k-congruent with w; namely, Closurek(w) = {x ∈
Σ∗| x ∼k w}. We deﬁne the shortlex normal form string ShortLexk(w) to be
the lexicographically smallest string among the shortest strings in Closurek(w).
Fleischer and Kuﬂeitner [3] presented an O(|Σ||w|)-time algorithm to obtain
ShortLexk(w), and Barker et al. [1] designed an improved O(|w|)-time algo-
rithm. The core idea of both algorithms is to repeatedly remove positions that
satisfy X(w, i) + Y(w, i) > k + 1. When no more eliminations are possible, the
algorithm radix-sorts consecutive positions that have the same attribute such
that the sum of X- and Y-coordinates equal k + 1. The result is the lexicograph-
ically smallest string among the shortest strings in Closurek(w). We call this
procedure the shortlex normalization algorithm. The following is a key property
for the correctness of the algorithm.
Proposition 1 ([3]). Let w = uσv with |u| = i and σ ∈Σ. If X(w, i)+Y(w, i) >
k + 1, then uσv ∼k uv.
If k is clear in the context, we use the shorthand notation ws to indicate
ShortLexk(w). We use wp to denote an arbitrary permutation of ws that is ∼k-
congruent with ws. Thus, the shortlex normalization algorithm can be considered
as a process that ﬁrst converts w into wp by deleting unnecessary symbols in w,
then transforms wp into ws by relocating symbols in wp to obtain the lexico-
graphically smallest string.
3
Main Contributions
Our contributions are three-fold as follows. First, we show that given a string w,
the state complexity of a DFA recognizing Closurek(w) is bounded from above
by a linear function of the length |ws| of the shortlex normal form string of
w. We establish the bound by presenting an eﬃcient DFA construction. Recall
that |ws| ≤|w| and we can obtain ws from w in O(|w|) time [1]. Second, we
consider the lower bound for the state complexity, and demonstrate that the
lower bound is linear in |ws| for a ﬁxed-size alphabet but is exponential for a
variable-sized alphabet. Lastly, we provide polynomial-time algorithms for the
two open problems LangSimK and MaxLangSimK in Gawrychowski et al. [6]
using our DFA construction for a ﬁxed-sized alphabet. We also prove that, for a
variable-sized alphabet, the problem is NP-complete.
3.1
Vectors and Blocks
Our ﬁrst ingredient is vectors. Intuitively, vectors represent potential X- and
Y-coordinates if a character is inserted in the middle of a given string. This is
because we consider inserting characters between existing characters as long as
it does not change the set of subsequences of length up to k. We give formal
deﬁnitions of potential X- or Y-coordinates, and vectors.

On Simon’s Congruence Closure of a String
131
Deﬁnition 1. For a string w, an in-between position i and a character σ ∈Σ,
let z = w[0 : i]σw[i : n] be a string obtained by inserting σ at position i. We deﬁne
the potential attribute of i to be ( ˆX(w, i, σ), ˆY (w, i, σ)), where
ˆX(w, i, σ) =
X(z, i) and ˆY (w, i, σ) = Y(z, i) are the potential X- and Y-coordinate, respec-
tively.
With the potential attributes, we can deﬁne Σ-indexed arrays of potential
attributes for a speciﬁc string w and in-between position i:
Deﬁnition 2. Given a string w over Σ and an in-between position i, we deﬁne
the X-vector −→
X(w, i) and the Y-vector −→
Y (w, i) to be the array of potential X-
and Y-coordinates deﬁned as follows:
−→
X(w, i)[σ] = ˆX(w, i, σ) and −→
Y (w, i)[σ] = ˆY (w, i, σ).
Additionally, k-bound X- and Y-vectors −→
X k(w, i) and −→
Y k(w, i) are X- and Y-
vectors with each element capped at k + 1:
−→
X k(w, i)[σ] = min(k + 1, −→
X(w, i)[σ]),
−→
Y k(w, i)[σ] = min(k + 1, −→
Y (w, i)[σ]).
Example 1. Consider a string w = abcababc. For an in-between position i = 5
and a character σ = a, we have z = w[0 : i]σw[i : n] = abcabaabc. Since XcXbXa
is one of the shortest X-rankers that reach the in-between position i + 1 on z
and YaYa is one of the shortest Y-rankers that reach the in-between position i
on z, we have that −→
X(w, 5)[a] = ˆX(w, 5, a) = 3 and −→
Y (w, 5)[a] = ˆY (w, 5, a) = 2.
The k-bound X- and Y-vectors −→
X k and −→
Y k serve as states in our DFA construc-
tion. At each input, the character inserting mechanism checks whether the sum
of potential X- and Y-coordinates exceeds k + 1 for that input. This reverses
the deletion process of the shortlex normalization algorithm. In other words, X-
and Y-vectors help restore w from wp with the character insertion. Algorithm 1
shows the relationship between bound vectors.
Algorithm 1. Iter(x ∈{1, 2, . . . , k + 1}|Σ|, σ ∈Σ)
1: x[σ] ←min(x[σ] + 1, k + 1)
2: for σ′ ∈Σ do
3:
x[σ′] ←min(x[σ], x[σ′])
4: end for
5: return x
The function Iter(x, σ) simulates an iteration of the X-coordinate computa-
tion function of the shortlex normalization algorithm, with the exception that
each vector cell is capped at k + 1 and no X-coordinate is remembered. Thus,
Iter(−→
X k(w, i), w[i]) = −→
X k(w, i + 1) and Iter(−→
Y k(w, i + 1), w[i]) = −→
Y k(w, i)

132
S. Kim et al.
for i ∈[0, n −1]. Also, each call of Iter() uses the min operation exactly |Σ| + 1
times and, thus, the function takes Θ(|Σ|) time.
Our second ingredient is saturated blocks. We use saturated blocks to indicate
maximal substrings in the shortest string wp such that any permutation within
a saturated block does not result in the change in the set of subsequences.
Deﬁnition 3. We deﬁne a block B of a string wp to be a maximal range of
true positions such that for positions i, j ∈B, we have X(wp, i) = X(wp, j) and
Y(wp, i) = Y(wp, j). A block B is saturated if X(wp, i) + Y(wp, i) = k + 1 for
positions i ∈B.
Checking for saturated blocks and computing Y-vectors for all permutations
within each block of ShortLexk(w) reverses the sorting process of the shortlex
normalization algorithm. In other words, the saturated blocks help identify all
permutations wp that map to ws when the characters in each saturated block
are relocated in lexicographical order. Note that a saturated block remains a
saturated block when permuted. From the deﬁnition of a saturated block, we
observe the following property.
Observation 1. Each symbol in Σ occurs at most once in a saturated block.
Next, we use saturated blocks to establish conditions that yield the same
vectors. The following statement shows that there can be multiple permutations
that yield the same vector.
Lemma 1. Given a string wp and an integer k, let wp = uv1v2y such that
positions for v1v2 form a saturated block. Let strings x1 and x2 be permuta-
tions of v1 and v2, respectively. Then, −→
X(uv1v2y, |uv1|) = −→
X(ux1x2y, |ux1|) and
−→
Y (uv1v2y, |uv1|) = −→
Y (ux1x2y, |ux1|).
Proof. For any σ ∈Σ, let zv = uv1σv2y and zx = ux1σx2y. Also, let RXσ be one
of the shortest X-ranker chains that satisﬁes RXσzv = |uv1| + 1. We show that
there exists an X-ranker chain of length equal to RXσ and yields |ux1| + 1 on
string zx. Showing the above proves ˆX(uv1v2y, |uv1|, σ) = ˆX(ux1x2y, |ux1|, σ)
as the permutation relation between v1 and x1 is symmetric. First, if Rzv ∈
[0, |u|], then σ does not occur in string u[Rzv : |u|]v1. Thus, σ does not occur
in string u[Rzx : |u|]x1 as well. It follows that RXσ is an X-ranker chain that
yields |ux1| + 1 on string zx. Otherwise, the X-ranker chain R yields Rzv ∈
[|u| + 1, |u| + |v1|]. If σ does not occur in v1, then σ does not occur in x1. Thus,
RXσ yields |ux1| + 1 on string zx. If σ does occur in v1, then recall that all
positions in v1 have the same attributes by the deﬁnition of a saturated block.
Hence, we may assume that R = R1Xσ for an X-ranker chain R1 since the length
of RXσ stays minimal. We have R1zx = R1zv ∈[0, |u|] by Observation 1, while
there exists a single σ in zv[R1zv : |uv1|]. Note that σ also occurs only once in
zx[R1zx : |ux1|]. Thus, RXσ is indeed an X-ranker chain that yields |ux1| + 1 on
string zx. We have shown that ˆX(uv1v2y, |uv1|, σ) = ˆX(ux1x2y, |ux1|, σ). Since
the equation holds for all σ ∈Σ, we have −→
X(uv1v2y, |uv1|) = −→
X(ux1x2y, |ux1|).
It follows from symmetry that −→
Y (uv1v2y, |uv1|) = −→
Y (ux1x2y, |ux1|).

On Simon’s Congruence Closure of a String
133
⊓⊔
Now we deﬁne the set of all Y-vectors that can be produced by all possible
permutations wp of ws.
Deﬁnition 4. The set Y(ws) for a shortlex form string ws is the set of Y-
vectors −→
Y (wp, i) for all in-between positions i ∈[0, |ws|] and all permutations wp
of ws such that ws ∼k wp. Namely,
Y(ws) = {−→
Y (wp, i) | |wp| = |ws|, wp ∼k ws, i = 0, 1, . . . , |ws|}.
By Lemma 1, for positions 0 ≤i ≤|ws|, multiple permutations of ws map
to a single element of Y(ws). From a topological perspective, all Y-vectors in
Y(ws) form a directed acyclic graph when connected using the Iter() function.
Only one Y-vector [1, 1, . . . , 1] has an in-degree of 0. Moreover, only Y-vectors
for a saturated block may have an out-degree greater than 1. Finally, Lemma 1
proves that the graph should converge into a single Y-vector at the end of the
saturated block. From these results, we obtain the following bound on |Y(ws)|.
Lemma 2. |Y(ws)| is at most 2|Σ||ws|
|Σ|
+ 1.
Proof. For a string ws in shortlex normal form, let ai be the number of true
positions in ws that is a member of a block of length i, for 1 ≤i ≤|Σ|. We can
rewrite
2|Σ| × |ws|
|Σ| = 2|Σ|
|Σ|
i=1 ai
|Σ|
=
|Σ|

i=1
2|Σ|
|Σ| ai.
On the other hand, we have
|Y(ws)| −1 ≤
|Σ|

i=1
((2i −1) × ai
i ) ≤
|Σ|

i=1
2i
i ai,
where the ﬁrst inequality comes from the assumption that all blocks are saturated
blocks. Finally, by subtracting the two results, we obtain
|Σ|

i=1
(2|Σ|
|Σ| −2i
i )ai ≥0
since 2|Σ|
|Σ| ≥2i
i and ai ≥0 for all i ≥1. Thus, we have |Y(ws)|−1 ≤2|Σ||ws|
|Σ|
. On
the other hand, there exists a shortlex form string ws with |Y(ws)| ∈Ω( 2|Σ||ws|
|Σ|
).
Consider string w = (σ1σ2 . . . σ|Σ|)k for Σ = {σ1, σ2, . . . , σ|Σ|}. For each true
position ik + j ∈[0, |w| −1] with j ∈[0, |Σ| −1], we have X(w, ik + j) = i and
Y(w, ik+j) = (k+1)−i, thus every position satisﬁes X(w, ik+j)+Y(w, ik+j) =
k +1. Hence w = ws. Note that we have |ws|
|Σ| saturated blocks of length |Σ| and,
thus, we have (2|Σ|−1)|ws|
|Σ|
+ 1 unique Y-vectors for ws. Therefore, the bound is
tight and |Y(ws)| ∈Θ

2|Σ||ws|
|Σ|

.
⊓⊔

134
S. Kim et al.
Similarly, we can compute Y(ws) in time linear in |ws| as follows.
Lemma 3. Given a positive integer k and a string ws in shortlex normal form
with respect to k, we can compute Y(ws) in O(2|Σ||ws|) time.
Proof. For every Y-vector in Y(ws), each cell does not contain a value greater
than k + 1 because ws is in shortlex normal form. Thus, all Y-vectors in Y(ws)
can be treated as a k-bound Y-vector. We prove by induction on the in-between
positions of ws.
Basis: position i = |ws|. We have −→
Y k(ws, i) = −→
Y k(wp, i) = [1, 1, . . . , 1] ∈
{1, 2, . . . , k + 1}|Σ| for all wp of ws. Note that the starting Y-vector is unique.
Induction: position i < |ws| and we have computed all the values for positions
greater than i for all wp of ws. We can use the Iter() function to obtain the
Y-vectors for position i from the Y-vectors for position i + 1. Recall that the
Iter() function takes O(|Σ|) time. If the Y-vector for position i is singular, we
simply use −→
Y k(ws, i) = Iter(−→
Y k(ws, i + 1), w[i]). Otherwise, i must belong to a
saturated block. Based on Lemma 1, we can use dynamic programming for each
combination of characters to fetch a Y-vector for which corresponding character
set is one character shorter. Thus, each Y-vector can be computed in Θ(|Σ|) time
with the Iter() function and there are Θ

2|Σ||ws|
|Σ|

unique Y-vectors by Lemma 2.
Thus, the overall time to compute all Y-vectors is bound by Θ(2|Σ||ws|).
⊓⊔
Finally, we bound the length of ShortLexk(w) in terms of k and |Σ| as follows.
Lemma 4. |ShortLexk(w)| ≤
k+|Σ|
|Σ|

−1.
Proof. Given an alphabet Σ = {σ1, σ2, . . . , σ|Σ|} and an integer k, let i and j be
integers for i ∈[1, |Σ|] and j ∈[0, k]. We deﬁne a string M(i, j) by the following
recurrence relation:
– if i = 1, then M(i, j) = (σ1)j.
– if i > 1, then M(i, 0) = λ and M(i, j) = M(i −1, j)σiM(i, j −1).
Thus,
– |M(1, j)| = j
– |M(i, 0)| = 0
– |M(i, j)| = |M(i −1, j)| + |M(i, j −1)| + 1
Consider the string w = M(|Σ|, k). Starting with X-vector [1, 1, . . . , 1], we have
(k+|Σ|)...(k+1)
|Σ|!
−1 calls of Iter() until we reach X-vector [k + 1, k + 1, . . . , k + 1]
at the end of string w. At this point, the sum of the X- and Y-coordinates of
the next input will exceed k + 1 because the X-coordinate alone equals k + 1.
Moreover, the string exploits the fact that we can push the maximum values in
an X-vector up to k + 1 before they are capped by another lower value. This
extensive search of possible X-vectors ensures there are no shortlex form strings
under the relation ∼k which are longer than w. Solving the recurrence relation,

On Simon’s Congruence Closure of a String
135
we have |M(|Σ|, k)| =
k+|Σ|
|Σ|

−1. Thus, |ShortLexk(w)| ∈O
k+|Σ|
|Σ|

. Note
that the bound becomes O

(k+|Σ|)...(k+1)
|Σ|!

= O

k|Σ|
when |Σ| is constant. ⊓⊔
We use the bounds for |Y(ws)| and |ShortLexk(w)| to obtain the bounds for
the DFA state complexity and design polynomial-time algorithms for LangSimK
and MaxLangSimK.
3.2
DFA Construction
From the shortlex normalization algorithm, we know that a true position of a
string x ∈Closurek(w) is either an eliminated position or a permuted posi-
tion. For permuted positions, the DFA state’s Y-vector component has an out-
transition to the next Y-vector if the ﬁrst character of the unread part of wp
matches the input. For eliminated positions, on the other hand, the state’s Y-
vector component has an out-transition to the same Y-vector if the sum of
the potential X- and Y-coordinates exceeds k + 1. The X-vector component
of each state is always updated following the Iter() function. This helps us to
keep track of the current k-bound X- and Y-vector and check if any insertions
are allowed. Given a string ws in shortlex normal form, we construct a DFA
A = (Q, Σ, δ, s, F), where
– Q = K|Σ|×Y(ws) where K = {1, 2, . . . , k+1}. For notational convenience, we
regard state q as a two-dimensional array for which q[0] represents a k-bound
X-vector and q[1] represents a Y-vector in Y(ws).
– δ consists of the following transitions:
1. Consuming transitions. For all ∼k-congruent permutations wp of ws
and all true positions i = 0, 1, . . . , |ws| −1, any state q = [x, −→
Y (wp, i)]
has a transition δ(q, wp[i]) = [Iter(x, wp[i]), −→
Y (wp, i + 1)]. Note that the
transitions are deterministic, since −→
Y (wp1, i) = −→
Y (wp2, i) and wp1[i] =
wp2[i] implies −→
Y (wp1, i + 1) = −→
Y (wp2, i + 1). These transitions are called
“consuming transitions” because they “consume” characters in wp.
2. Inserting transitions. For all states q = [x, −→
Y (wp, i)] and all characters
σ ∈Σ that satisfy the following conditions:
• q[0][σ] + q[1][σ] > k + 1 and
• δ(q, σ) is not deﬁned as a consuming transition,
we have δ(q, σ) = [Iter(x, σ), −→
Y (wp, i)]. These transitions are called
“inserting transitions” because they “insert” characters into wp to make
some string z such that wp ∼k z.
3. Sink transitions. Any other undeﬁned transitions go to the implied sink
state.
– s = [−→
X k(ws, 0), −→
Y (ws, 0)].
– F = {q ∈Q | q[1] = −→
Y (ws, |ws|)}.
Example 2. For our running example w = abcababc and k = 3, the X- and
Y-coordinates are written above each character as follows:

136
S. Kim et al.
1 3 1 3 1 2 2 2 2 2 3 1 3 1 2 1
a
b
c
a
b
a
b
c
Note that w is already in shortlex normal form. Observe that the true posi-
tions can be partitioned into ﬁve blocks B0 = {0, 1}, B1 = {2}, B2 = {3, 4}, B3 =
{5, 6}, and B4 = {7}. Blocks B0, B2 and B3 are saturated blocks since the sum
of X- and Y-coordinates equal 4. We compute Y(ws) using the algorithm in the
proof of Lemma 3 and obtain the following result.
Y(ws) =
a
b
c
⎡
⎣
0
4
1
3
1′
4
2
3
3
3
4
2
4′
3
5
2
6
1
6′
2
7
1
8
1
4 4 3 3 3 3 2 2 2 1 1 1
3 3 3 3 2 2 2 2 2 2 2 1
⎤
⎦
The columns represent Y-vectors. The labels on the top row are the in-
between positions, and labels with a prime indicate the Y-vectors obtained from
the ∼k-congruent permutation of w. This example has a single alternative Y-
vector for each saturated block because every saturated block has at most two
elements.
Given a string w, let Aw be the resulting DFA of the proposed construction in
Sect. 3.2 for Closurek(w). The construction guarantees that X-vector transitions
are always simulated by the Iter() function. From the Y-vector perspective, on
the other hand, the DFA AX in Fig. 1 is a cross-section of Aw representing the Y-
vectors and transitions among them. The DFA AX is a cross-section in the sense
that Aw consists of (k + 1)|Σ| copied layers of AX’s states, with each layer rep-
resenting a k-bound X-vector. Moreover, transitions in Aw are deﬁned between
states in diﬀerent layers instead of states within the same cross-section DFA
layer. A state in Aw has a consuming out-transition whenever the corresponding
state in AX has an out-transition to a diﬀerent state in AX (blue transitions in
Fig. 1). On the other hand, a state in Aw has an inserting out-transition only
when the corresponding state in AX has a self-looping transition (red transitions
in Fig. 1) and the sum of potential coordinates for an input symbol exceeds k+1.
Note that inserting transitions for a character c in Fig. 1 do not exist because
|ShortLexk(w)|c < k. Each state has subscripts that match each column label.
Fig. 1. A cross-section of the DFA Aw for Closurek(w) when w = abcababc.
We can use the cross-section DFA AX along with a k-bound X-vector starting
at [1, 1, . . . , 1] to simulate computations in Aw. For each input character σ, the

On Simon’s Congruence Closure of a String
137
k-bound X-vector −→
X k is updated with the Iter() function. Before this update,
however, we ﬁrst check whether the current state in AX has a valid transition
reading σ. If the transition in AX is deﬁned between two diﬀerent states, then
we move on to the next state in AX while we consume a character from some wp
using a consuming transition in Aw. If the transition is self-looping in AX and
the k-bound X-vector and Y-vector’s sum for σ exceeds k+1, then we stay in the
same state in AX while we use an inserting transition in Aw. If both conditions
fail, then Aw rejects the string.
We next prove the correctness of the DFA construction.
Lemma 5. The constructed DFA A recognizes Closurek(w).
Proof. We prove the statement by showing the equality between L(A) and
Closurek(w).
First, we show that if x ∈L(A), then x ∈Closurek(w). If x ∈L(A), then
there exists a computation (s, x) ⊢A · · · ⊢A (f, λ), where f ∈F. Note that no
sink transition can be used in this computation since the sink state is not a
ﬁnal state. For a conﬁguration in this computation, let i and c be the number
of inserting and consuming transitions used until arriving at that conﬁguration,
respectively. Since each single-step computation computes Iter() for q[0] and the
automaton is a DFA, the (i + c)th state q satisﬁes q[0] = −→
X k(x, i + c). Moreover,
for all consuming transitions used, let wp be the permutation of ShortLexk(w)
used in generating those transitions. By construction, q satisﬁes q[1] = −→
Y (wp, c).
Performing induction on the reverse order of computation steps, we prove that
the property x ∼k x[0 : i + c]wp[c : |wp|] remains invariant.
Basis: Start from the last conﬁguration (f, λ). Then q = f, i + c = |x| and
c = |wp|. Thus, x[0 : i + c]wp[c : |wp|] = x ∼k x since ∼k is reﬂexive.
Induction: Let (q1, σz) be the current conﬁguration and (q1, σz) ⊢A (q2, z) be
a single-step computation in the given computation sequence. If the transition
used is a consuming transition, then q1[1] = −→
Y k(wp, c) and q2[1] = −→
Y k(wp, c+1).
Moreover, by construction, we have that wp[c] = x[i + c] = σ. So, we have
x[0 : i + c]wp[c : |wp|] = x[0 : i + c + 1]wp[c + 1 : |wp|] ∼k x by the induction
hypothesis. Otherwise, if the transition used is an inserting transition, we have
that X(x[0 : i+c+1]wp[c : |wp|], i+c)+Y(x[0 : i+c+1]wp[c : |wp|], i+c) > k+1.
By Proposition 1, we have x[0 : i + c]wp[c : |wp|] ∼k x[0 : i + c + 1]wp[c : |wp|],
and by the induction hypothesis, x[0 : i + c]wp[c : |wp|] ∼k x.
End condition: Eventually the current conﬁguration must become (s, x), with
i = j = 0. Then, the invariant x[0 : i+c]wp[c : |wp|] ∼k x implies wp ∼k x. There-
fore, it is guaranteed that a string accepted by A should exist in Closurek(w).
Next, we prove the other direction; if x ∈Closurek(w), then x ∈L(A). From a
string x ∈Closurek(w), by applying the repeated deletion process of the shortlex
normalization algorithm, we can obtain a shortest string xp ∼k x. Without loss
of generality, assume that we erase the last occurrences of consecutive same
symbols. For example, if we can erase two a’s in string bbaaaabb, we erase true

138
S. Kim et al.
positions 4 and 5 rather than erasing true positions 2 and 3. We show that x
is accepted by A by induction on the computation steps and proving that the
property q = [−→
X k(x, i + c), −→
Y (xp, c)] remains invariant, where i is the number of
inserting transitions used and c is the number of consuming transitions used.
Basis: Start from the initial conﬁguration (s, x). Since no transitions were used
until now, i = c = 0. By construction, we have s = [−→
X k(x, 0), −→
Y (xp, 0)].
Induction: Let (q, x[i + c : |x|]) be the current conﬁguration. By the induction
hypothesis, we have q = [−→
X k(x, i + c), −→
Y (xp, c)]. Assume that true position i + c
of x was not erased in obtaining xp. By deﬁnition, there exists a consuming
transition that reads x[i + c] = xp[c] and goes to state q′ = [−→
X k(x, i + c +
1), −→
Y (xp, c + 1)] because xp is a permutation of ws. Thus, the target property
remains invariant. Otherwise, true position i + c of x was erased in xp. Since
x[0 : i + c]xp[c : |xp|] ∼k x[0 : i + c + 1]xp[c : |xp|], we have that −→
X k(x[0 :
i + c]xp[c : |xp|], i + c)[x[i + c]] + −→
Y (x[0 : i + c]xp[c : |xp|], i + c)[x[i + c]] > k + 1
by Proposition 1. Since the consuming transition case was handled previously
and we assumed that elimination of consecutive characters happen as late as
possible, there must exist an inserting transition that reads x[i + c] and goes
to state q′ = [−→
X k(x, i + c + 1), −→
Y (xp, c)]. Thus, the target property remains
invariant.
End condition: The computation will use exactly |x| transitions since there
are no λ-transitions in a DFA. Also, the computation will use exactly |xp|
consuming transitions since the induction implies that consuming transitions
will be used if and only if a true position has not been erased in xp. Finally,
the induction shows that sink transitions are not used. Thus, after reading
the last input character, the computation will halt at conﬁguration (f, λ)
where f = [−→
X k(x, |x|), −→
Y (xp, |xp|)] ∈F. Thus, DFA A accepts all strings in
Closurek(w). This concludes the proof that L(A) = Closurek(w).
⊓⊔
3.3
State Complexity for Closurek(w)
We investigate the state complexity sc(Closurek(w)) of a DFA recognizing the
Simon’s congruence closure of a string w. First, we present an upper bound of
the state complexity.
Theorem 1. Given a string w over Σ and an integer k, the minimal DFA for
Closurek(w) needs at most
O
(2k + 2)|Σ| ×
k+|Σ|
|Σ|

|Σ|

states.

On Simon’s Congruence Closure of a String
139
Proof. From the DFA construction in Sect. 3.2, we know that the resulting DFA
has exactly (k+1)|Σ||Y(ws)| states. Since |Y(ws)| is at most 2|Σ||ShortLexk(w)|
|Σ|
+1
(from Lemma 2) and |ShortLexk(w)| is no more than
k+|Σ|
|Σ|

−1 (from Lemma 4),
we have
(k + 1)|Σ||Y(ws)| ∈O
(2k + 2)|Σ|k+|Σ|
|Σ|

|Σ|

.
⊓⊔
Next, we present two lower bounds for sc(Closurek(w)). We obtain the ﬁrst
lower bound from the fact that the DFA should be able to recognize all permu-
tations of a saturated block of length at most |Σ|, and the second lower bound
from the distinguishability of vectors in Y(ws).
Lemma 6. Given a string w over Σ, the minimal DFA recognizing Closurek(w)
needs at least (k + 1)|Σ| states in the worst-case.
Proof. By the deﬁnition of Closurek(w), the following equation always holds for
a string w = (σ1σ2 . . . σ|Σ|)k of length k|Σ| over Σ = {σ1, σ2, . . . , σ|Σ|}:
Closurek(w) = perm(σk
1σk
2 . . . σk
|Σ|),
where perm(z) = {u ∈Σ∗| |u|σ = |z|σ∀σ ∈Σ}. It is already known that
we need at least (k + 1)|Σ| states to recognize Closurek(w) from the recent
result by Hoﬀmann [8] on the state complexity of the permutation operation on
alphabetical pattern constraints.
⊓⊔
Lemma 7. Given a string w over Σ, the minimal DFA recognizing Closurek(w)
needs at least |Y(ws)| states. Moreover, |Y(ws)| is at most 2|Σ|
|Σ| ×
k+|Σ|
|Σ|

.
Proof. Let L = Closurek(w) and wp1, wp2 be two diﬀerent permutations of
ShortLexk(w). For two positions i and j, we have −→
Y (wp1, i) ̸= −→
Y (wp2, j) if
and only if wp1[: i] is not a permutation of wp2[: j]. Without loss of generality,
we can decompose wp1 = xz1 and wp2 = yz2 such that |z1| ≤|z2| and x is not a
permutation of y. Then, xz1 ∈Closurek(w) but yz1 /∈Closurek(w) because:
– if |z1| = |z2|, then wp1 and wp2 are cut in the middle of a saturated block B.
Since x is not a permutation of y, there must exist characters that are included
in portions of y and z1 that correspond to B. Thus, yz1 is not a permutation
of wp1. Since |yz1| = |wp1| and yz1 is not a permutation of ShortLexk(w), we
have yz1 /∈Closurek(w).
– if |z1| < |z2|, then |yz1| < |ShortLexk(w)|. By the deﬁnition of shortlex normal
form, there does not exist a shorter string than ShortLexk(w) in Closurek(w).
Hence yz1 /∈Closurek(w).
This implies −→
Y (xz1, |x|) ̸= −→
Y (yz2, |y|) ⇒x ̸≡L y, where ≡L is the Nerode
equivalence. Thus, the number of states in the minimal DFA recognizing L is
bounded from below by Ω(|Y(ws)|). Using Lemmas 2 and 4, we can bound the
cardinality of |Y(ws)| by O

2|Σ|
|Σ| ×
k+|Σ|
|Σ|

.
⊓⊔

140
S. Kim et al.
Note that the bound in Lemma 6 is independent of the length of w whereas
the bound in Lemma 7 depends on the length of the shortlex form of w. The
bound is tight if k = 1. From these two lemmas, we establish the following lower
bound.
Theorem 2. Given a string w over Σ and an integer k, the minimal DFA for
Closurek(w) needs at least
max

(k + 1)|Σ| or 2|Σ||ShortLexk(w)|
|Σ|

states.
We are now ready to tackle LangSimK and MaxLangSimK.
Theorem 3. Given an integer k, a string w and a context-free language L over
Σ, we can solve LangSimK and MaxLangSimK in polynomial time if Σ is
a ﬁxed-sized alphabet. For a variable-sized alphabet, LangSimK is NP-complete
when L is either regular or context-free.
Proof (sketch). Our algorithm is to construct a DFA ACk(w) for Closurek(w)
and check the emptiness of the intersection between ACk(w) and L. Recall that
ACk(w) has at most O

(2k+2)|Σ||ws|
|Σ|

states. Therefore, for a ﬁxed-sized alpha-
bet, we solve the problems in polynomial time. We solve LangSimK by com-
puting the maximum k by the binary search between 1 and |w|.
For a variable-sized alphabet, we can reduce the Hamiltonian path problem
to LangSimK when L is regular. We can also show that the problem still lies
in NP when L is context-free.
⊓⊔
4
Conclusions
We have presented an eﬃcient DFA construction that accepts Closurek(w).
Moreover, we have presented upper and lower bounds of the state complex-
ity of the Simon’s congruence closure of a string. Finally, based on the ﬁnding
that the problem LangSimK is NP-complete, we have presented eﬃcient algo-
rithms for solving LangSimK and MaxLangSimK under the assumption that
|Σ| is constant. For future works, we plan to investigate more eﬃcient automata
construction for Closurek(w) as well as tighten the bounds for sc(Closurek(w)).
We suspect that ﬁnding tighter bounds require extensive research on the dis-
tribution and properties of shortlex normal form strings with respect to the
congruence relation ∼k. Moreover, another interesting follow-up research topic
is extending our construction to eﬃciently recognize the Simon’s congruence clo-
sure Closurek(L) of a language L or the general k-binomial congruence closure
of a string.
Acknowledgments. We wish to thank the referees for letting us know related ref-
erences and providing valuable suggestions that improve the presentation of the
paper. This research was supported by the NRF grant funded by MIST (NRF-
2020R1A4A3079947).

On Simon’s Congruence Closure of a String
141
References
1. Barker, L., Fleischmann, P., Harwardt, K., Manea, F., Nowotka, D.: Scattered
factor-universality of words. In: Jonoska, N., Savchuk, D. (eds.) DLT 2020. LNCS,
vol. 12086, pp. 14–28. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-
48516-0 2
2. Cho, D., Goˇc, D., Han, Y., Ko, S., Palioudakis, A., Salomaa, K.: State complexity
of permutation on ﬁnite languages over a binary alphabet. Theor. Comput. Sci.
682, 67–78 (2017)
3. Fleischer, L., Kuﬂeitner, M.: Testing Simon’s congruence. In: 43rd International
Symposium on Mathematical Foundations of Computer Science, pp. 62:1–62:13
(2018)
4. Freydenberger, D.D., Gawrychowski, P., Karhum¨aki, J., Manea, F., Rytter, W.:
Testing k-binomial equivalence. Multidisciplinary Creativity: homage to Gheorghe
Paun on his 65th birthday, pp. 239–248 (2015)
5. Garel, E.: Minimal separators of two words. In: Apostolico, A., Crochemore, M.,
Galil, Z., Manber, U. (eds.) CPM 1993. LNCS, vol. 684, pp. 35–53. Springer, Hei-
delberg (1993). https://doi.org/10.1007/BFb0029795
6. Gawrychowski, P., Kosche, M., Koß, T., Manea, F., Siemer, S.: Eﬃciently testing
simon’s congruence. In: 38th International Symposium on Theoretical Aspects of
Computer Science, vol. 187, pp. 34:1–34:18 (2021)
7. H´ebrard, J.: An algorithm for distinguishing eﬃciently bit-strings by their subse-
quences. Theor. Comput. Sci. 82(1), 35–49 (1991)
8. Hoﬀmann, S.: State complexity of permutation and related decision problems on
alphabetical pattern constraints. In: Maneth, S. (ed.) CIAA 2021. LNCS, vol.
12803, pp. 115–126. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-
79121-6 10
9. Karandikar, P., Kuﬂeitner, M., Schnoebelen, P.: On the index of simon’s congruence
for piecewise testability. Inf. Process. Lett. 115(4), 515–519 (2015)
10. Lejeune, M., Rigo, M., Rosenfeld, M.: The binomial equivalence classes of ﬁnite
words. Int. J. Algebra Comput. 30(07), 1375–1397 (2020)
11. Schwentick, T., Th´erien, D., Vollmer, H.: Partially-ordered two-way automata: a
new characterization of DA. In: Kuich, W., Rozenberg, G., Salomaa, A. (eds.) DLT
2001. LNCS, vol. 2295, pp. 239–250. Springer, Heidelberg (2002). https://doi.org/
10.1007/3-540-46011-X 20
12. Simon, I.: Piecewise testable events. In: Brakhage, H. (ed.) GI-Fachtagung 1975.
LNCS, vol. 33, pp. 214–222. Springer, Heidelberg (1975). https://doi.org/10.1007/
3-540-07407-4 23
13. Weis, P., Immerman, N.: Structure theorem and strict alternation hierarchy for
FO2 on words. Logical Meth. Comput. Sci. 5(3), 1–23 (2009)
14. Wood, D.: Theory of Computation. Harper & Row, New York (1987)
15. Yu, S., Zhuang, Q., Salomaa, K.: The state complexities of some basic operations
on regular languages. Theor. Comput. Sci. 125(2), 315–328 (1994)

Approximate NFA Universality
Motivated by Information Theory
Stavros Konstantinidis1(B), Mitja Mastnak1, Nelma Moreira2,
and Rog´erio Reis2
1 Saint Mary’s University, Halifax, NS, Canada
s.konstantinidis@smu.ca, mmastnak@cs.smu.ca
2 CMUP and DCC, Faculdade de Ciˆencias da Universidade do Porto,
Rua do Campo Alegre, 4169-007 Porto, Portugal
{nelma.moreira,rogerio.reis}@fc.up.pt
Abstract. In coding and information theory, it is desirable to construct
maximal codes that can be either variable length codes or error con-
trol codes of ﬁxed length. However deciding code maximality boils down
to deciding whether a given NFA is universal, and this is a hard prob-
lem (including the case of whether the NFA accepts all words of a ﬁxed
length). On the other hand, it is acceptable to know whether a code is
‘approximately’ maximal, which then boils down to whether a given NFA
is ‘approximately’ universal. Here we introduce the notion of a (1 −ε)-
universal automaton and present polynomial randomized approximation
algorithms to test NFA universality and related hard automata prob-
lems, for certain natural probability distributions on the set of words.
We also conclude that the randomization aspect is necessary, as approxi-
mate universality remains hard for any ﬁxed polynomially computable ε.
1
Introduction
It is well-known that NFA universality is a PSPACE-hard problem and that
block NFA universality (whether an NFA of some ﬁxed length words accepts all
the words of that length) is a coNP-hard problem. Here we consider polynomial
approximation algorithms for these NFA problems by considering the concept
of an approximate universal NFA, or block NFA, where for instance 95% of all
words are accepted by the NFA. In general, for some tolerance ε ∈(0, 1), we
assume that we are happy to know that an NFA is at least (1 −ε) universal.
While approximate universality is still hard, it allows us to consider polynomial
randomized algorithms that return an incorrect answer with small probability.
Inspired from [14, pg 72], we view estimating the universality index of an NFA
as the problem of estimating the parameter of some population.
Our motivation for deﬁning the concept of approximate universality comes
from the problem of generating codes (whether variable length codes, or ﬁxed
Research supported by NSERC, Canada (Discovery Grants of S.K. and of M.M.) and
by CMUP through FCT project UIDB/00144/2021.
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 142–154, 2022.
https://doi.org/10.1007/978-3-031-13257-5_11

Approximate NFA Universality
143
length error control codes) that are maximal, where on the one hand the question
of deciding maximality is hard, but on the other hand it is acceptable to generate
codes that are maximal within a tolerance ε, [4,12]. For inﬁnite languages, we
deﬁne approximate universality relative to some probability distribution on the
set of words. This idea is consistent with our interpretation of languages in the
context of coding and information theory where words are in fact abstractions
of physical network signals or magnetic polarities, [10,13], and the amount of
energy they require should not be exponential. Our work falls under the general
framework of problems about parameter estimation or approximate counting
[1,6,14], however, we are not aware of the application of this framework in hard
NFA problems, especially in the case where the NFA accepts an inﬁnite language.
Main Results and Structure of the Paper. The next section contains basic
notions including concepts of probability distributions on the nonnegative inte-
gers, in particular the three distributions: uniform, Lambert and Dirichlet. The
Dirichlet distribution is a good substitute for the ‘ﬁctitious’ uniform distribution
on the nonnegative integers [7]. Section 3 discusses what a polynomial random-
ized approximation (PRAX) algorithm should be for the case of a hard decision
problem on NFAs. The necessity for PRAX-like algorithms for NFA universality
is discussed. Section 4 is about probability distributions on words over some
alphabet As = {0, 1, . . . , s} such that the length sets of these distributions follow
the above three distributions on the nonnegatives. Section 5 considers whether
an NFA a is universal relative to a maximum language M (i.e., whether L(a) = M),
and takes the approach that M is the domain of a probability distribution W on
the set of words, in which case the universality index W(a) of a is the proba-
bility that a word selected from the distribution W belongs to L(a). Then, a is
p%-universal relative to W, if W(a) ≥p%. The section closes with two simple
random processes about estimating the universality index of NFAs. Section 6
gives PRAX algorithms for three hard NFA problems. Section 7 deﬁnes what
a tractable length distribution (on the nonnegatives) is and gives a PRAX algo-
rithm for whether a given NFA is universal relative to any ﬁxed, but arbitrary,
tractable word distribution (including the word distributions that are based on
the Lambert and Dirichlet length distributions). The last section contains a few
concluding remarks.
2
Basic Notation and Background Information
We use the notation N for the set of positive integers, N0 for the nonnegative
integers, and N>x for the positive integers greater than x, where x is any real
number. We assume the reader to be familiar with basics of formal languages
and ﬁnite automata [9,15]. Our arbitrary alphabet will be As = {0, 1, . . . , s −1}
for some positive integer s. Then, we use the following notation
ε = empty word,
|w| = length of word w
Aℓ
s = all words of length ℓ,
A≤ℓ
s
= all words of length at most ℓ
DFA = all DFAs (deterministic ﬁnite automata)

144
S. Konstantinidis et al.
NFA = all NFAs (nondeterministic ﬁnite automata)
ADFA = all acyclic DFAs (accepting ﬁnite languages)
BNFA = all block NFAs = NFAs accepting languages of a ﬁxed word length.
BNFA[s] = all block NFAs over the alphabet As.
|a| = the size of the NFA a = number of states plus the number of transitions.
L(a) = the language accepted by the NFA, or DFA, a.
Next we list some decision problems about automata that are known to be hard,
or easily shown to be hard.
UNIV NFA = {a ∈NFA : L(a) = A∗
s}: Deciding whether a given NFA is universal
is a PSPACE-complete problem, [9].
UNIV BNFA = {b ∈BNFA : L(b) = Aℓ
s, where ℓis the word length of b}: Decid-
ing whether a given block NFA of some word length ℓaccepts all words of
length ℓis a coNP-complete problem, [12].
UNIV MAXLEN NFA = {(a, ℓ) : a ∈NFA, ℓis unary in N, L(A≤ℓ
s ) ⊆L(a)}:
Deciding whether L(A≤ℓ
s ) ⊆L(a), for given a ∈NFA and unary ℓ∈N0,
is coNP-complete, [5].
ADFA SUBSET NFA = {(a, b) : a ∈NFA, b ∈ADFA, L(b) ⊆L(a)}: Deciding
whether L(b) ⊆L(a), for given a ∈NFA and b ∈ADFA is PSPACE-complete.
EMPTY DFA = {(a1, . . . , an) : n ∈N, ai ∈DFA, ∩n
i=1L(ai) = ∅}: Deciding
whether the intersection of given DFAs is empty is PSPACE-complete, [5].
Note that the problem remains hard even if we know that the languages of the
given DFAs belong to low levels of the dot-depth or the Straubing-Th´erien
hierarchies [2].
Probability Distributions. Let X be a countable set. A probability distribution
on X is a function D : X →[0, 1] such that

x∈X
D(x) = 1.
The domain of D, denoted by dom D, is the subset {x ∈X | D(x) > 0} of X.
If X = {x1, . . . , xℓ}, for some ℓ∈N, then we write D =

D(x1), . . . , D(xℓ)

.
Following [8], for any subset S of X, we deﬁne the quantity
D(S) =

x∈S
D(x)
and refer to it as the probability that a randomly selected element from D is in S.
The notation x
$
←−D, borrowed from cryptography, means that x is randomly
selected from D.
The author of [8] considers three families of probability distributions on N0
that are meaningful in information and/or number theory. These distributions
are called uniform, Lambert and Dirichlet, and are deﬁned, respectively, as follows,
where d ∈N0, M ∈N, z ∈(0, 1) and t ∈(1, +∞) are related parameters.

Approximate NFA Universality
145
Uniform: UM(n) = 1/M for n < M, and UM(n) = 0 otherwise.
Lambert: L1/z,d(n) = (1 −z)zn−d for n ≥d, and L1/z,d(n) = 0 otherwise.
Dirichlet: Dt,d(n) = (1/ζ(t))(n+1−d)−t for n ≥d, where ζ is the Riemann zeta
function, and Dt,d(n) = 0 otherwise.
In fact [8] considers distributions on N, but here we use N0 instead as we intend
to apply these distributions to modelling lengths of words, including possibly
the empty word ε whose length is 0. We also note that [8] considers L1/z,d and
Dt,d only for the case where the displacement d = 1. We also note that in [7] the
same author considers the Dirichlet distribution to be the basis where “many
heuristic probability arguments based on the ﬁctitious uniform distribution on
the positive integers become rigorous statements”.
We shall call any probability distribution N on N0 a length distribution.
3
Randomized Approximation of [0, 1]-Value Problems
We consider problems for which every instance1 x has a value v(x) ∈[0, 1] and we
are interested in those instances x for which v(x) = 1. Our main set of instances
is the set of NFAs (or subsets of that) and the main value function v is the
universality index of NFAs, which is deﬁned in Sect. 5. However for the purposes
of this section, our main set of instances is BNFA[2] = all block NFAs over the
alphabet {0, 1}, and the [0, 1]-valued function v is such that v(a) = |L(a)|/2n,
where n is the word length of the block NFA a. In general, for a ﬁxed but
arbitrary [0, 1]-valued function v, we deﬁne the language (problem)
Lv = {x : v(x) = 1}.
Deciding whether a given instance x is in Lv might be hard, but we assume that
we are happy if we know whether v(x) ≥1 −ε, for some appropriate tolerance
ε ∈(0, 1). So we deﬁne the following approximation language for Lv:
Lv,ε = {x : v(x) ≥1 −ε}.
One can verify that Lv = 
ε∈(0,1) Lv,ε; hence Lv can be approximated as close
as desired via the languages Lv,ε. Unfortunately deciding Lv,ε can be harder
than deciding Lv:
Theorem 1. The following problem about block NFAs is coNP-hard, for any
(ﬁxed) δ ∈(0, 1) that is computable within polynomial time2.
Bδ = {a ∈BNFA[2] : |L(a)|
2n
≥δ, where n = word length of a}.
1 Following the presentation style of [6, pg 193], we refrain from cluttering the notation
with the use of a variable for the set of instances.
2 A real x ∈(0, 1) is computable if there is an algorithm that takes as input an integer
n > 0 and computes the n-th bit of x. It is polynomially computable if the algorithm
works in time O(nk), for some ﬁxed k ∈N0, when the input n is given in unary.

146
S. Konstantinidis et al.
Another idea then is to show that Lv is in the class coRP, that is, there is a
polynomial randomized algorithm A(x) such that (i) if x ∈Lv then A(x) =
True (with probability 1), and (ii) if x /∈Lv then A(x) = False with probability3
at least 3/4. However, as Lv can be hard, it is unlikely that it is in coRP.
The next idea is to devise an approximating algorithm for Lv via Lv,ε. As stated
in [6, pg 417], “The answer to [what constitutes a “good” approximation] seems
intimately related to the speciﬁc computational task at hand... the importance
of certain approximation problems is much more subjective...[which] seems to
stand in the way of attempts at providing a comprehensive theory of natural
approximation problems”. It seems that the following approximation method is
meaningful. Although our domain of interest involves NFAs, the below deﬁnition
is given for any set of instances and refers to a ﬁxed but arbitrary [0, 1]-valued
function v on these instances.
Deﬁnition 1. Let v be [0, 1]-valued function. A polynomial approximation (PAX)
algorithm for Lv is an algorithm A(x, ε) such that
– if x ∈Lv then A(x, ε) = True;
– if x /∈Lv,ε then A(x, ε) = False;
– A(x, ε) works within polynomial time w.r.t. 1/ε and the size of x.
Explanation. In the above deﬁnition, if A(x, ε) returns False then x /∈Lv,
that is, v(x) < 1 . If A(x, ε) returns True then x ∈Lv,ε, that is, v(x) ≥1 −ε.
Thus, whenever the algorithm returns the answer False, this answer is correct
and exact; when the algorithm returns True, the answer is correct within the
tolerance ε.
It turns out that, in general, there are problems for which no approximation
algorithm can do better than the exact algorithms:
Proposition 1. There is no polynomial approximation algorithm for the prob-
lem UNIV BNFA, unless P=coNP.
Remark 1. Theorem 1 implies that, unless P=coNP, block NFA universality over
the binary alphabet cannot be approximated by some sequence

Bδn

, with
lim δn = 0 and each δn being polynomially computable. Based on this observation
and on Proposition 1, we conclude that, in general, it is necessary to add a
randomized aspect to our approximation methods. We also note that there are in
fact cases where a PAX algorithm for a hard problem exists.
The next deﬁnition is inspired from the “approximate” algorithmic solution
of [12] for the task of generating an error-detecting code of N codewords, for
given N, if possible, or an error-detecting code of less than N codewords which
is “close to” maximal.
3 Many authors specify this probability to be at least 2/3, but they state that any
value ≥1/2 works [1,6].

Approximate NFA Universality
147
Deﬁnition 2. Let v be [0, 1]-valued function. A polynomial randomized approx-
imation (PRAX) algorithm for Lv is a randomized algorithm A(x, ε) such that
– if x ∈Lv then A(x, ε) = True;
– if x /∈Lv,ε then P[A(x, ε) = False] ≥3/4;
– A(x, ε) works within polynomial time w.r.t. 1/ε and the size of x.
Explanation. In the above deﬁnition, if A(x, ε) returns False then x /∈Lv. If
A(x, ε) returns True then probably x ∈Lv,ε, in the sense that x /∈Lv,ε would
imply P[A(x, ε) = False] ≥3/4. Thus, whenever the algorithm returns the
answer False, this answer is correct (x /∈Lv); when the algorithm returns True,
the answer is correct within the tolerance ε (x ∈Lv,ε) with probability ≥3/4. The
algorithm returns the wrong answer exactly when it returns True and x /∈Lv,ε,
but this happens with probability < 1/4.
Use of a PRAX Algorithm. The algorithm can be used as follows to
determine the approximate membership of a given x in Lv with a probabil-
ity that can be as high as desired: Run A(x, ε) k times, for some desired
k, or until the output is False. If the output is True for all k times then
P[A(x, ε) = True for k times
| x /∈Lv,ε] < 1/4k, that is, the probability of
incorrect answer is < 1/4k.
4
Word Distributions
A word distribution W is a probability distribution on A∗
s, that is, W : A∗
s →[0, 1]
such that 
w∈A∗s W(w) = 1. If a is an NFA then we use the convention that
W(a) means W(L(a)). The domain of W is dom W = {w ∈A∗
s | W(w) > 0}.
Example 1. For a ﬁnite language F, we write UF to denote the uniform word
distribution on F, that is, UF (w) = 1/|F| for w ∈F, and UF (w) = 0 for w /∈F.
Some important examples of uniform word distributions are:
– UAℓ
s, where ℓis any word length. Then, UAℓ
s(w) = 1/sℓ.
– UA≤ℓ
s , where ℓis any word length. Then, UA≤ℓ
s (w) = 1/t, where t = ℓ
i=0 si.
– UL(a), where a is an acyclic NFA. We also simply write Ua for UL(a).
Deﬁnition 3. Let N be a length distribution. Then ⟨N⟩is the word distribution
such that ⟨N⟩(w) = N(|w|)s−|w|. Any such word distribution is called a length-
based distribution.
Remark 2. For any length distribution N, we have that ⟨N⟩(An
s ) = N(n) and
⟨N⟩(A>n
s ) = N

N>n
, where n ∈N0.
Example 2. Using the Lambert length distribution Ls,d(n) = (1 −1/s)(1/s)n−d,
we deﬁne the Lambert, or geometric, word distribution ⟨Ls,d⟩on A∗
s such that
⟨Ls,d⟩(w) = 0 if |w| < d and, for |w| ≥d, ⟨Ls,d⟩(w) = (1−1/s)(1/s)2|w|−d. Then,
for all n, d ∈N0 with n ≥d, we have
⟨Ls,d⟩(An
s ) = (1 −1/s)(1/s)n−d, ⟨Ls,d⟩(A>n
s ) = (1/s)n+1−d.

148
S. Konstantinidis et al.
In particular, for the alphabet A2 = {0, 1}, we have that ⟨L2,1⟩(A2) = 1/2,
⟨L2,1⟩(A2
2) = 1/22, etc.
Example 3. Using the Dirichlet length distribution Dt,d(n) = (1/ζ(t))(n + 1 −
d)−t, we deﬁne the Dirichlet word distribution ⟨Dt,d⟩on A∗
s such that ⟨Dt,d⟩(w) =
0 if |w| < d and, for |w| ≥d, ⟨Dt,d⟩(w) = (1/ζ(t))(|w| + 1 −d)−ts−|w|. Then, for
all n, d ∈N0 with n ≥d, we have ⟨Dt,d⟩(An
s ) = (1/ζ(t))(n + 1 −d)−t and
⟨Dt,d⟩(A>n
s ) = 1 −

1/ζ(t)
 n+1−d

i=1
i−t.
In particular, for t = 3, d = 1 and alphabet A2 = {0, 1}, we have that
⟨D2,1⟩(An
2) = (1/ζ(3))n−3.
Selecting a Word from a Distribution. We are interested in word distri-
butions W for which there is an eﬃcient (randomized) algorithm that returns
a randomly selected element from W. We shall assume available (randomized)
algorithms as follows.
– tossCoin(p): returns 0 or 1, with probability p or 1−p, respectively, where p ∈
[0, 1], and the algorithm works in constant time for most practical purposes—
this is a reasonable assumption according to [1, pg 134].
– selectUnif(s, ℓ): returns a uniformly selected word from Aℓ
s, and the algorithm
works in time O(ℓ).
Remark 3. As in [1, pg 126], we assume that basic arithmetic operations are
performed in constant time. Even if we account for a parameter q for arithmetic
precision, the arithmetic operations would require a polynomial factor in q.
The next lemma seems to be folklore, but we include it here for the sake of
clarity and self-containment.
Lemma 1. There is a polynomial randomized algorithm selectFin(D), where D
is a ﬁnite distribution

D(x1), . . . , D(xn)

on some set {x1, . . . , xn}, that returns
a randomly selected xi with probability D(xi). The algorithm is linear time under
the assumption of constant cost of tossCoin and arithmetic operations.
Augmented Word Distributions. Selecting a word from a distribution W
with inﬁnite domain dom W could return a very long word, which can be
intractable. For this reason we would like to deﬁne distributions on A∗
s ∪{⊥},
where ‘⊥’ is a symbol outside of As, which could select the outcome ‘⊥’ (no word).
These could be versions of word distributions in which there is a bound on the
length of words they can select. Let W be a word distribution and let M ∈N0. We
deﬁne the augmented distribution W M on A∗
s ∪{⊥} such that W M(w) = W(w),
if |w| ≤M and W M(⊥) = W(A>M
s
). The probability that W M selects a word
longer than M is zero. We have that dom W M = (dom W ∩A≤M
s
) ∪{⊥}. More-
over, the following facts about W M and any language L are immediate
W(L ∩A≤M
s
) = W M(L ∩A≤M
s
),
W(A>M
s
) = W M(⊥).
(1)

Approximate NFA Universality
149
5
Universality Index of NFAs
Here we intend to deﬁne mathematically the informal concept of an “approxi-
mately universal NFA” with respect to a certain ﬁxed language M. Our motivation
comes from coding theory where the codes of interest are subsets of M, and it is
desirable that a code is a maximal subset of M. Two typical cases are (i) M = A∗
s,
when variable-length codes are considered, such as preﬁx or suﬃx codes; and
(ii) M = An
s for some n ∈N, when error control codes are considered. Testing
whether a regular code C is a maximal subset of M is a hard problem and, in
fact, this problem normally reduces to whether a certain NFA that depends on
C accepts M—see e.g., [4,12]. In practice, however, it could be acceptable that a
code is “close” to being maximal, or an NFA is “close” to being universal.
Our approach here assumes that the maximum language M is equal to dom W,
where W is the word distribution of interest. Let a be an NFA, and let p ∈[0, 1].
– We say that a is universal relative to W, if L(a) = dom W.
– We say that a is p-universal relative to W, if W(a) ≥p. We call the quantity
W(a) the universality index of a (relative to W).
Example 4. Let b be a block NFA. If |L(b)|/sℓ≥p, where ℓis the word length
of b, then b is p-universal relative to the uniform distribution on Aℓ
s and the
quantity |L(b)|/sℓis the universality index of b.
Remark 4. The universality index W(a) represents the probability that a ran-
domly selected word from W is accepted by a. When W(a) is close to 1 then a
is close to being universal, that is, L(a) is close to dom W. The concept of a p-
universal NFA formalizes the loose concept of an approximately universal NFA.
Thus, for example, we can talk about a 98%-universal block NFA with respect
to the uniform distribution on Aℓ
s, where ℓis the word length of the NFA.
Remark 5. The method of [11] embeds a given t-code4 K into a maximal one
by successive applications of an operator μt on K which yields supersets Ki
of K until these converge to a maximal t-code. The operation μt on each Ki
(represented as an NFA) can be time-expensive and one can simply stop at a step
where the current superset Ki is close to maximal, or according to the concepts
of this paper, when the NFA for

t(Ki) ∪t−1(Ki) ∪Ki

is close to universal.
Consider the case where b is a block NFA of length ℓand W is the uniform
word distribution on Aℓ
s. In this work, we view estimating the universality index
of b as a parameter estimation problem for ﬁnite populations [14, pg 72]: let p
be an unknown population parameter (ratio of elements having some attribute
over the cardinality of the population). Select n elements from the population
(here, n words from Aℓ
s) and compute c, the number of these elements having the
attribute of interest (here, words that are in L(b)). Then, c/n is an estimate for
4 Depending on t, which is a transducer, one can have preﬁx codes, suﬃx codes, inﬁx
codes, error control codes.

150
S. Konstantinidis et al.
the population parameter p (here, the estimate is for W(b)) in the sense that the
expected value of the random variable c/n is equal to p and P[ |c/n −p| > ε] <
e−nε2/2 + e−nε2/3. Here we extend the idea of parameter estimation to various
distributions on languages. Moreover, we use the simpler Chebysev inequality
for bounding the error probability, as it gives in practice a smaller bound than
the one in the above inequality. Let X be a random variable and let a > 0. The
Chebyshev inequality is P[ |X −E(X)| ≥a] ≤σ2/a2, where σ2 is the variance
of X. When X is the binomial random variable with parameters n = ‘number
of trials’ and p = ‘probability of success in one trial’, then E(X) = np and
σ2 = np(1 −p). For p ∈[0, 1], the maximum value of p(1 −p) is 1/4; therefore,
the above inequality becomes as follows:
P[ |X −E(X)| ≥a] ≤n/(4a2).
(2)
Fig. 1. These random processes refer to a particular word distribution W. The left one
returns an estimate of the universality index W(a) of the given NFA a. For the right
one, when M is chosen such that W M(⊥) is small enough, then the returned quantity
cnt/n can be an acceptable estimate of W(a).
Lemma 2. Let a be an NFA, let W be a word distribution, and let p, g ∈[0, 1]
with p > g. Consider the random process UnivIndexW (a, n) in Fig. 1, and let
Cnt be the random variable for the value of cnt when the algorithm returns. If
W(a) < g then P[Cnt/n ≥p] ≤
1
4n(p−g)2 .
In Sect. 6 we give a polynomial randomized approximation algorithm (PRAX)
for testing universality of block NFAs, which is based on the left random process
in Fig. 1. That process, however, cannot lead to a PRAX for the universality of
NFAs accepting inﬁnite languages, as the selection w
$
←−W could produce a
word of exponential length. The right process in Fig. 1 makes sure that a selected
word cannot be longer than a desired M ∈N0—in Sect. 7 we investigate how
this can lead to a PRAX for the universality of any NFA relative to tractable
word distributions.
Lemma 3. Let a be an NFA, let W be a word distribution, let M ∈N0,
and let p, g ∈[0, 1] such that p > g + W(A>M
s
). Consider the random process

Approximate NFA Universality
151
UnivIndexMaxLenW (a, n, M) in Fig. 1, and let Cnt be the random variable whose
value is equal to the value of cnt when the algorithm returns. If W(a) < g then
P[Cnt/n ≥p] ≤1
x +
1
4n

p −g −xW(A>M
s
)
2 ,
for all x ∈

1,
p −g
W(A>M
s
)

.
6
Randomized Approximation of NFA Problems Relative
to Uniform Distributions
In this section we consider polynomial randomized approximation algorithms for
the problems ADFA SUBSET NFA, UNIV BNFA, UNIV MAXLEN NFA. As dis-
cussed below, the latter two problems are essentially special cases of the prob-
lem ADFA SUBSET NFA, but they can also be answered using a couple of more
standard tools leading to more eﬃcient algorithms.
Theorem 2. Algorithm ADFASubsetNFA(a, b, ε) in Fig. 2 is a polynomial ran-
domized approximation algorithm for ADFA SUBSET NFA.
Fig. 2. On the left is the PRAX for ADFA SUBSET NFA: whether L(b) ⊆L(a) for given
NFA a and given acyclic DFA b. This is equivalent to whether L(b) ⊆L(a) ∩L(b). The
function selectUnif(b) returns a uniformly selected word from L(b). The version on the
right is logically equivalent; it mimics the left process in Fig. 1 and is intended to give
a more clear exposition of correctness.
The next corollary follows from the above theorem; however, using a more
self-contained choice of tools we can get more eﬃcient algorithms with estimates
of their time complexity.
UnivBlockNFA(a, ε)
ℓ:= the word length of L(a);
n := ⌈1/ε2⌉;
repeat n times:
w := selectUnif(s, ℓ);
if (w /∈L(a)) return False;
return True;
UnivMaxLenNFA(a, ℓ, ε)
t := 1 + s + · · · + sℓ;
N := (1/t, s/t, . . . , sℓ/t);
n := ⌈1/ε2⌉;
repeat n times:
k := selectFin(N);
w := selectUnif(s, k);
if (w /∈L(a)) return False;
return True;

152
S. Konstantinidis et al.
Corollary 1. UnivBlockNFA(a, ε) is a PRAX algorithm for block NFA uni-
versality and works in time O

ℓ|a|(1/ε)2
, where ℓis the word length of a.
UnivMaxLenNFA(a, ℓ, ε) is a PRAX algorithm for UNIV MAXLEN NFA. In fact
the algorithm works in time O

ℓ|a|(1/ε)2
under the assumption of constant
cost of tossCoin and of arithmetic operations.
Use of Algorithm UnivBlockNFA(a, ε). Suppose that we want to test whether
a block NFA a of some word length ℓis universal relative to the uniform dis-
tribution on Aℓ
s, and that we allow a 2% approximation tolerance, that is, we
consider it acceptable to say that a is universal when it is in fact 98%-universal.
Then we run the algorithm using ε = 0.02. If a is universal, then the algorithm
correctly returns True. If a is not 98%-universal, then the probability that the
algorithm returns True is at most 1/4. Note that for this choice of arguments,
the loop would iterate at most 2500 times.
7
Randomized Approximation of NFA Universality
Here we present an analogue to the uniform distribution algorithms for the
case where NFAs accept inﬁnite languages and universality is relative to some
word distribution ⟨T⟩. The PRAX algorithm of this section is based on the
right process in Fig. 1 and requires that ⟨T⟩be tractable, which loosely speaking
means that words longer than a certain length M = M(ε) have low probability
and can be ignored when one wants to approximate the universality index of the
given NFA within a given tolerance ε—recall, this approach is consistent with
our interpretation of languages in the context of coding and information theory.
Deﬁnition 4. A length distribution T is called tractable, if the following condi-
tions hold true.
1. For all ε ∈(0, 1), there is M ∈N0 such that T(N>M) ≤ε, M is of poly-
nomially bounded magnitude w.r.t. 1/ε, that is, M = O

(1/ε)k
for some
k ∈N0, and there is an algorithm maxLenT (ε) that returns such an M and
works within polynomial time w.r.t. 1/ε.
2. There is an algorithm probT (m), where m ∈N0, that returns the value T(m)
and works within polynomial time w.r.t m.
Theorem 3. Let T be a tractable length distribution. UnivNFAT (a, ε) in Fig. 3
is a PRAX algorithm for NFA universality relative to ⟨T⟩.
Theorem 3 can be applied to the Lambert and Dirichlet Distributions.
Corollary 2. There is a polynomial randomized approximation algorithm for
NFA universality relative to the Lambert distribution. In fact the algorithm works
in time O

|a|(1/ε)2 log(1/ε)

under the assumption of constant cost of tossCoin
and of arithmetic operations.
Corollary 3. There is a polynomial randomized approximation algorithm for
NFA universality relative to the Dirichlet distribution. In fact the algorithm
works in time O

|a|(1/ε)2 t−1
1/ε2 
under the assumption of constant cost of
tossCoin and of arithmetic operations.

Approximate NFA Universality
153
Fig. 3. The PRAX algorithm for NFA universality with respect to a certain tractable
word distribution ⟨T⟩—see Theorem 3. The algorithm selects repeatedly either a word
w of length ≤M from ⟨T⟩or the outcome ‘⊥’. The ﬁnite probability distribution D
refers to the outcomes {0, 1, . . . , M, ⊥}; that is, a length ℓ≤M or ‘⊥’. Statement
w
$
←−W M of the right process in Fig. 1 corresponds, for W = ⟨T⟩, to the ﬁrst two
statements of the ‘repeat’ loop.
8
Concluding Remarks
The concept of approximate maximality of a block code introduced in [12] leads
naturally to the concept of approximately universal NFAs. These concepts are
meaningful in coding theory where the languages of interest are ﬁnite or even
regular and can be represented by automata, [3,13,16].
Our approach can be used to deﬁne approximate versions of other hard prob-
lems such as EMPTY DFA and the problem of whether two given NFAs accept
the same language. Algorithm UnivNFA can be used to decide approximate uni-
versality of a context-free language L(a), where now a is a context-free grammar.
Of course context-free grammar universality is undecidable! However, extending
our approach to grammars is outside our motivation from coding and informa-
tion theory and we cannot tell whether it could lead to any fruitful results.
It can be shown that every coNP language L is a [0, 1]-value language Lv;
hence, it can be approximated by languages Lv,ε. As this generalization is outside
the scope of the present paper, we leave it as a topic for future research.
References
1. Arora, S., Barak, B.: Computational Complexity - A Modern Approach. Cambridge
University Press, New York (2009)
2. Arrighi, E., et al.: On the complexity of intersection non-emptiness for star-free
language classes. CoRR, abs/2110.01279 (2021)
3. Berstel, J., Perrin, D., Reutenauer, C.: Codes and Automata. Cambridge University
Press, Cambridge (2009)

154
S. Konstantinidis et al.
4. Dudzinski, K., Konstantinidis, S.: Formal descriptions of code properties: decidabil-
ity, complexity, implementation. Int. J. Found. Comput. Sci. 23(1), 67–85 (2012)
5. Fernau, H., Krebs, A.: Problems on ﬁnite automata and the exponential time
hypothesis. Algorithms 10(1), 24 (2017)
6. Goldreich, O.: Computational complexity - A Conceptual Perspective. Cambridge
University Press, Cambridge (2008)
7. Golomb, S.W.: A class of probability distributions on the integers. J. Number
Theory 2, 189–192 (1970)
8. Golomb, S.W.: Probability, information theory, and prime number theory. Discret.
Math. 106(107), 219–229 (1992)
9. Hopcroft, J.E., Motwani, R., Ullman, J.D.: Introduction to Automata Theory,
Languages, and Computation, 2nd edn. Addison-Wesley-Longman (2001)
10. J¨urgensen, H.: Complexity, information, energy. Int. J. Found. Comput. Sci. 19(4),
781–793 (2008)
11. Konstantinidis, S., Mastnak, M.: Embedding rationally independent languages into
maximal ones. J. Automata Lang. Comb. 21(4), 311–338 (2016)
12. Konstantinidis, S., Moreira, N., Reis, R.: Randomized generation of error control
codes with automata and transducers. RAIRO - Theo. Inform. Appl. 52, 169–184
(2018)
13. Marcus, B.H., Siegel, P., Roth, R.: Constrained systems and coding for recording
channels. In: Handbook of Coding Theory, pp. 1635–1764. Elsevier (1998). http://
www.math.ubc.ca/∼marcus/Handbook/
14. Mitzenmacher, M., Upfal, E.: Probability and Computing: Randomization and
Probabilistic Techniques in Algorithms and Data Analysis, 2nd edn. Cambridge
University Press, Cambridge (2017)
15. Rozenberg, G., Salomaa, A. (eds.): Handbook of Formal Languages. Springer, Hei-
delberg (1997). https://doi.org/10.1007/978-3-642-59136-5
16. Vardy, A.: Trellis structure of codes. In: Handbook of Coding Theory, pp. 1989–
2117. Elsevier, Amsterdam (1998)

Lazy Regular Sensing
Orna Kupferman1(B) and Asaf Petruschka2
1 School of Engineering and Computer Science, The Hebrew University,
Jerusalem, Israel
orna@cs.huji.ac.il
2 Department of Mathematics and Computer Science,
The Weizmann Institute of Science, Rehovot, Israel
asaf.petruschka@weizmann.ac.il
Abstract. A complexity measure for regular languages based on the
sensing required to recognize them was recently introduced by Almagor,
Kuperberg, and Kupferman. Intuitively, the sensing cost quantiﬁes the
detail in which a random input word has to be read in order to decide
its membership in the language, when the input letters composing the
word are truth assignments to a ﬁnite set of signals. We introduce the
notion of lazy sensing, where the signals are not sensed simultaneously.
Rather, the signals are ordered, and a signal is sensed only if the values
of the signals sensed so far have not determined the successor state. We
study four classes of lazy sensing, induced by distinguishing between
the cases where the order of the signals is static or dynamic (that is,
ﬁxed in advance or depends on the values of the signals sensed so far),
and the cases where the order is global or local (that is, the same for
all states of the automaton, or not). We examine the diﬀerent classes
of lazy sensing and the saving they enable, with respect to each other
and with respect to (non-lazy) sensing. We also examine the trade oﬀs
between sensing cost and size. Our results show that the good properties
of sensing are preserved in the lazy setting. In particular, saving sensing
does not conﬂict with saving size: in all four classes, the lazy-sensing
cost of a regular language can be attained in the minimal automaton
recognizing the language.
1
Introduction
The classical complexity measure for regular languages is the size of a mini-
mal deterministic automaton that recognizes the language. In [1], the authors
introduced a new complexity measure, namely the sensing cost of the language.
Intuitively, the sensing cost of a language measures the detail with which a ran-
dom input word needs to be read in order to decide membership in the language.
The study is motivated by the use of ﬁnite-state automata in reasoning about
on-going behaviors of reactive systems. In particular, when monitoring a com-
putation, we seek a monitor that minimizes the activation of sensors used in the
Work partially supported by the Israel Science Foundation, ISF grant agreement no
2357/19.
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 155–169, 2022.
https://doi.org/10.1007/978-3-031-13257-5_12

156
O. Kupferman and A. Petruschka
monitoring process, and when synthesizing a system, we prefer I/O-transducers
that satisfy a given speciﬁcation while minimizing the activation of sensors (of
input signals) [1]. Sensing has been studied in several other computer-science
contexts. In theoretical computer science, in methodologies such as PCP and
property testing, we are allowed to sample or query only part of the input [5].
In more practical applications, mathematical tools in signal processing are used
to reconstruct information based on compressed sensing [3], and in the context
of data streaming, one cannot store in memory the entire input, and therefore
has to approximate its properties according to partial “sketches” [8].
The automata used in formal methods are over alphabets of the form 2P ,
for a ﬁnite set P of signals. Consider a deterministic automaton (DFA) A over
an alphabet 2P . For a state q of A, we say that a signal p ∈P is sensed in
q if at least one transition taken from q depends on the truth value of p. The
sensing cost of q is the number of signals it senses, and the sensing cost of a
run is the average sensing cost of states visited along the run. The deﬁnition is
extended to DFAs by deﬁning the sensing cost of A as the limit of the expected
sensing of runs over words of increasing length, assuming a uniform distribution
of the letters in 2P , thus each signal p ∈P holds in each moment in time with
probability 1
2. It is easy to extend the setting to a non-uniform distribution on
the letters, given by a Markov chain. The sensing cost of a language L ⊆(2P )∗
is then the inﬁmum of the sensing costs of DFAs for L.
In this work, we reﬁne the notion of regular sensing from [1], which we call
naive sensing, to a new notion called lazy sensing. Intuitively, in naive sensing,
the signals in P are sensed simultaneously. Consequently, if a signal p is deﬁned
to be sensed in a state q, then a sensor for p must indeed be activated whenever
a run of the DFA is in state q and needs to determine the successor state.
In lazy sensing, the signals are not sensed simultaneously. Instead, they can
be activated “on demand”, one after the other, and we may reach a decision
about the successor state before they are all sensed. This is demonstrated in the
following simple example.
Example 1. Let P = {a, b}, and consider a state q0 with three successor states
q1, q2, and q3, and transitions as shown on the right. According to the deﬁnition
in [1], both a and b are sensed in q0. Indeed, in naive sensing, when the signals
are sensed simultaneously, both a and b must be sensed in order to determine
the successor state.
In lazy sensing, we can start by sensing only the
signal a. If a is True, then we know that the suc-
cessor state is q1, and there is no need to sense b.
Accordingly, if we assume that a has probability 1
2
to be True, the number of sensors we are expected
to activate in state q0 is only 1 1
2, rather than 2.
⊓⊔
q0
q2
q3
q1
{a}, {a, b}
{b}
{ }
The underlying idea of lazy sensing is simple and is similar to short-circuit
evaluation in programming languages. There, the second argument of a Boolean
operator is executed or evaluated only if the ﬁrst argument does not suﬃce
to determine the value of the expression [7]. Our study examines such a lazy
evaluation in the context of DFAs.

Lazy Regular Sensing
157
In order to perform lazy sensing, each state of the DFA should be equipped
with a data structure that directs it which signal to sense next. We examine
four diﬀerent classes of lazy sensing, induced by the following two parameters:
(1) Is ordering of signals sensed dynamic or static: in the dynamic classes, the
order may depend on the truth value of signals sensed earlier. That is, the data
structure supports policies like “if a is True, then next sense b, and if a is False,
then next sense c”. In the static classes, the data structure is a linear order on
the signals – the order in which they are going to be sensed, regardless of the
result. (2) Is the sensing policy local or global: in the local classes, each state
may have its own data structure. In the global ones, the same data structure is
used for all the states. Note that both parameters are irrelevant in short-circuit
evaluation in programming languages. Indeed, lazy evaluation concerns Boolean
expressions, each evaluated independently, and the control ﬂow is induced by
the structure of the expression.
The diﬀerence between the dynamic and static classes can be viewed as fol-
lows. Consider a DFA with state space Q, and consider a state q ∈Q. The data
structure maintaining the transitions from q is a sensing tree: a decision tree
in which each vertex is labeled by a signal in p ∈P and has two successors,
corresponding to the two truth values that p may have. Each path in the tree
corresponds to a set of assignments to the signals in P – assignments that are
consistent with the truth values that the path assigns to signals that appear
in it. Accordingly, if we label the leaves of the tree by states in Q, then each
sensing tree maintains a function f : 2P →Q. In the static classes, all paths in
the sensing tree follow the same ﬁxed order of the signals in P. Thus, the sensing
tree is related to a multiple-valued decision diagram [2,6]. On the other hand,
in the dynamic classes, the order of the signals in each path of the sensing tree
may be diﬀerent.
For all the four classes, the lazy sensing cost of a state q ∈Q is the expected
number of signals sensed when a transition from q is taken and sensing is per-
formed according to the sensing tree. Then, the lazy sensing cost of a DFA is the
limit of expected sensing of runs overs words of increasing length, with the best
possible choice of the allowed data structure. For example, in the static-global
class, this best possible choice is a single vector of the signals in P, maintaining
a linear order that is used by all states of the DFA. Finally, the sensing cost of
a regular language L is the inﬁmum of sensing costs of a DFA for L.
We examine the diﬀerent classes of lazy sensing and the saving they enable,
with respect to each other and with respect to naive sensing. We also examine
the trade oﬀs between sensing cost and size. Our results show that the good
properties of naive sensing are preserved in lazy sensing. In particular, the lazy
sensing cost of a DFA can be calculated by using the stationary distribution of
its induced Markov chain. Also, saving sensing does not conﬂict with saving size:
in all four classes, the lazy sensing cost of a regular language can be attained in
the minimal automaton recognizing the language.
Due to the lack of space, some proofs are omitted and can be found in the
full version, in the authors’ URLs.

158
O. Kupferman and A. Petruschka
2
Deﬁning Lazy Sensing
2.1
Deterministic Finite Automata
A deterministic automaton on ﬁnite words (DFA, for short) is A = ⟨Σ, Q, q0,
δ, α⟩, where Σ is a ﬁnite alphabet, Q is a ﬁnite set of states, q0 ∈Q is an
initial state, δ : Q × Σ →Q is a transition function, and α ⊆Q is a set of
accepting states. A run of A on a word w = σ1 · σ2 · · · σm ∈Σ∗is the sequence
of states q0, q1, . . . , qm such that qi+1 = δ(qi, σi+1) for all i ≥0. The run is
accepting if qm ∈α. A word w ∈Σ∗is accepted by A if the run of A on w is
accepting. For i ≥0, we use w[1, i] to denote the preﬁx σ1 · σ2 · · · σi of w, and
use δ(w[1, i]) to denote the state qi that A visits after reading the preﬁx w[1, i].
Note that w[1, 0] = ϵ. The language of A, denoted L(A), is the set of words that
A accepts. For a state q ∈Q, we use Aq to denote A with initial state q.
2.2
Potentially Sensed Signals
We study languages over an alphabet Σ = 2P , for a ﬁnite set P of signals. A letter
σ ∈Σ corresponds to a truth assignment to the signals. When we deﬁne lan-
guages over Σ, we use predicates on P in order to denote sets of letters. For exam-
ple, if P = {a, b, c}, then the expression (True)∗·a·b·(True)∗describes all words
over 2P that contain a subword σa · σb with σa ∈{{a}, {a, b}, {a, c}, {a, b, c}}
and σb ∈{{b}, {a, b}, {b, c}, {a, b, c}}.
Consider a DFA A = ⟨2P , Q, q0, δ, α⟩. For a state q ∈Q and a signal p ∈P,
we say that p is potentially sensed in q if there exists a set S ⊆P such that
δ(q, S\{p}) ̸= δ(q, S ∪{p}). Intuitively, a signal is potentially sensed in q if
knowing its value may aﬀect the destination of at least one transition from q.
We use psensed(q) to denote the set of signals potentially sensed in q.
Recall the situation in Example 1. For S = ∅, we have δ(q0, S ∪{a}) = q1
and δ(q0, S\{a}) = q3, so a is potentially sensed in q0. Also, δ(q0, S ∪{b}) = q2
and δ(q0, S\{b}) = q3, so b is also potentially sensed in q0.
In the naive sensing setting, studied in [1], sensing of input signals happens
simultaneously; that is, we sense together all of the signals whose truth value
might aﬀect the decision to which state to proceed. Accordingly, the notions
of a sensed signal in [1] and our deﬁnition above of a potentially sensed signal
coincide. In the following sections, we formalize the notion of lazy sensing, where
sensing need not be simultaneous.
2.3
Sensing Trees
The main feature of lazy sensing is a data structure termed sensing tree, which
directs the order in which signals are sensed. A sensing tree is a labeled tree
T = ⟨V, E, τ⟩, where V is a set of vertices, E ⊆V × {True, False} × V is a
set of directed labeled edges, and τ : V →P ∪Q is a labelling function. Each
vertex v ∈V is either internal, in which case it has exactly two children, vleft
and vright, with ⟨v, False, vleft⟩and ⟨v, True, vright⟩, or is a leaf, in which case

Lazy Regular Sensing
159
a
b
b
c
c
c
c
q1
q2
q3
q3
q2
q1
q4
q1
f
t
f
t
f
t
f
t
f
t
f
t
f
t
Fig. 1. The sensing tree T.
a
b
b
c
q3
c
c
q1
q2
q2
q1
q4
q1
f
t
f
t
f
t
f
t
f
t
f
t
Fig. 2. Reducing the tree T.
it has no children. Let Int(T) and Leaves(T) denote the sets of internal vertices
and leaves of T, respectively. We assume that T has a single root – a vertex with
no incoming edges.
The labelling function τ labels internal vertices by signals in P and labels
leaves by states in Q. The function τ is such that for each signal p ∈P and leaf
ℓ∈V , the single path from the root to ℓincludes at most one vertex labeled
p. Accordingly, each subset S ∈2P corresponds to a single leaf, namely the leaf
reached by following the path that corresponds to the assignment S. Formally,
reading an input S ∈2P , we start from the root of the tree, and then in each step,
we sense the signal p that labels the current vertex. If it is True (i.e., p ∈S), we
proceed to the right child. If it is False (i.e., p /∈S), we proceed to the left child.
By the requirement on τ, we encounter each signal at most once. In particular,
as some signals may not appear in the traversed path, the above process may
reach a leaf before all signals have been sensed. Let fT : 2P →Leaves(T) map
each S ∈2P to the leaf that corresponds to S. We sometimes refer to a sensing
tree also as a function T : 2P →Q, where for every S ∈2P , we have that
T(S) = τ(fT (S)), thus each assignment is mapped to the label of the leaf that
corresponds to S. Also, for S ∈2P , we use sensed(T, S) for the set of signals
sensed in the process of ﬁnding T(S). Note that |sensed(T, S)| is the length of
the path from the root to fT (S).
Example 2. Let P = {a, b, c} and Q = {q1, q2, q3, q4}. The tree T appearing
in Fig. 1 represents the function f with f(∅) = f({a, c}) = f({a, b, c}) = q1,
f({c}) = f({a}) = q2, f({b}) = f({b, c}) = q3, and f({a, b}) = q4.
⊓⊔
We assume that sensing trees are reduced: they do not include redundant
tests, namely internal vertices whose two children root identical subtrees. A
sensing tree may be reduced in polynomial time by repeatedly replacing an
internal vertex with two identical children by one of its children. It is not hard
to see that the order in which such replacements are applied is not important.1
1 Note that the above deﬁnition of a reduced tree is syntactic, in the sense it examines
whether subtrees are identical. An alternative semantic deﬁnition removes a vertex
if its two children root subtrees that represent the same function. Since the order of
the signals along diﬀerent paths may be diﬀerent, two subtrees that represent the
same function need not be identical, even if both are reduced. Thus, the semantic
deﬁnition may result in smaller sensing trees. However, reducing trees according to

160
O. Kupferman and A. Petruschka
Example 3. Consider the sensing tree T from Fig. 1. The vertex reached with the
assignment a = False and b = True has two identical successors. By reducing
T, we obtain the sensing tree T ′ in Fig. 2.
⊓⊔
A layout is a sensing tree L = ⟨V, E, τ⟩in which τ is not deﬁned for the leaves.
Accordingly, a layout cannot be reduced, and all its paths include vertices that
label all signals in P. A sensing tree T follows a layout L if T is obtained from L
by reducing the sensing tree obtained by extending τ to the leaves. Intuitively, L
directs the required sensing in T, but some tests that exist in L can be skipped
in T.
2.4
The Sensing Cost of a Sensing Tree
Consider a sensing tree T = ⟨V, E, τ⟩. The sensing cost of T is the expected
number of signals that are sensed when evaluating an assignment S ∈2P . Recall
that we assume that each signal is valid with probability 1
2. Thus, the probability
of each assignment is
1
2|P | . Accordingly, the sensing cost of T, denoted scost(T),
is scost(T) =
1
2|P |

S∈2P |sensed(T, S)|.
An equivalent deﬁnition of scost(T) is based on a discounted sum of the
vertices in T. For v ∈V , let depth(v) denote the length of the path from the
root to v. Thus, the depth of the root is 0, the depth of its children is 1, and so on.
Since the probability to reach the internal vertex v when reading an assignment
S ∈2P that is chosen uniformly at random, is 2−depth(v), we have the following.
Lemma 1. ForeverysensingtreeT,wehavethatscost(T) = 
v∈Int(T ) 2−depth(v).
Example 4. The sensing cost of the tree T ′ from Fig. 2 is 1
8 · (3 + 3 + 2 + 2 + 3 +
3 + 3 + 3) = 2 3
4. Using discounted sum, we get 1 + 2 · 1
2 + 3 · 1
4 = 2 3
4.
⊓⊔
2.5
Static vs. Dynamic Sensing Trees
The sensing tree T ′ from Fig. 2 is such that the labelling function τ follows the
same order of the signals in P in all its branches. Indeed, all branches ﬁrst sense
a, then b, and then c, possibly skipping some of the signals (speciﬁcally, skipping
c after reading a = False and b = True). This corresponds to situations where
the order of signals sensed is decided in advance and is static. In contrast, the
order of signals sensed may be dynamic and depends on the valuation of signals
sensed earlier.
Example 5. Consider the function f represented by the sensing tree T ′ from
Fig. 2. The two sensing trees appearing in Fig. 3 represent f too. Both are
reduced. The tree on the left is static, and it follows the order c < b < a. It
is reduced, and still its sensing cost is 3, as all tree signals are read in all assign-
ments. The tree on the right is dynamic: When a = False, the next signal to
sense is b. When a = True, the next signal to sense is c. Its sensing cost is 2 1
2,
which is in fact the minimal sensing cost required for evaluating f.
⊓⊔

Lazy Regular Sensing
161
c
b
b
a
a
a
a
q1
q2
q3
q4
q2
q1
q3
q1
f
t
f
t
f
t
f
t
f
t
f
t
f
t
a
b
c
c
q3
b
q1
q1
q2
q2
q4
f
t
f
t
f
t
f
t
f
t
Fig. 3. A static (left) and a dynamic (right) sensing tree for f.
Note that, like a sensing tree, a layout may be static or dynamic. In particular,
a static layout corresponds to a permutation on P. Indeed, such a layout is a
sensing tree in which the vertices along all paths from the root to a leaf are
labeled by all signals in P, with all paths follow the same ordering.
2.6
The Sensing Cost of a DFA and a Regular Language
Consider a DFA A = ⟨2P , Q, q0, δ, α⟩. Essentially, the sensing cost of A is the
expected number of signals that A needs to sense in each transition when it
runs on a random long word. Deﬁning the sensing cost of A, we ﬁrst have to
deﬁne the expected number of signals that A needs to sense in each state q ∈Q.
In [1], this is the number of potentially sensed signals in q. Deﬁning the lazy
sensing cost of A, we allow the states to maintain sensing trees that represent
the transition function. Indeed, the function δ : Q × 2P →Q induces, for each
state q ∈Q, a function δq : 2P →Q, where for every assignment S ∈2P , we
have that δq(S) = δ(q, S). We distinguish between four classess, induced by the
following two parameters.
– Static vs. Dynamic. That is, whether the sensing trees for δq are static or
dynamic.
– Global vs. Local. That is, whether the sensing trees of the diﬀerent states
follow the same layout.
We denote the four classes by SG, SL, DG, and DL.
Let T be the set of all sensing trees (over P and Q, which we omit from the
notation). A legal choice of sensing trees for the DFA A is a function γ : Q →T ,
such that for every state q ∈Q, the sensing tree γ(q) represents the function
δq, and the following hold. Note that we can view γ as a mapping of states to
layouts, which are then reduced to sensing trees. In particular, note that there is
a unique way to reduce a layout to a sensing tree for a given function f : 2P →Q.
– In the LD class, there are no restrictions on γ.
– In the LS class, the image of γ contains only static sensing trees.
– In the GD class, all the sensing trees in the image of γ follow the same layout.
the semantic deﬁnition is more complex. All our results apply also to the semantic
deﬁnition.

162
O. Kupferman and A. Petruschka
– In the GS class, all the sensing trees in the image of γ follow the same layout,
which is static.
Consider a DFA A. Let γ be a choice of sensing trees for A. For a word
w = w1 · · · wm ∈(2P )∗, the sensing cost of
w by A with respect γ is
scostA,γ(w) = 1
m
m−1

i=0
|sensed(γ(δ(w[1, i]), wi+1))|.
That is, scostA,γ(w) is the average number of signals that a state in the run of A
on w senses when it reads w using the sensing trees chosen by γ. Note that the
deﬁnition does not take into account the last state in the run, namely δ(w[1, m]),
as indeed no letter is read in it.
The sensing cost of
A with respect to
γ is then deﬁned as the expected
sensing cost of words of length tending to inﬁnity, when the letters in 2P are
uniformly distributed. Formally,
scost(A, γ) = lim
m→∞|2P |−m

w∈(2P )m
scostA,γ(w).
That is, scost(A, γ) is the expected sensing cost of words of length tending to
inﬁnity, when the letters in 2P are uniformly distributed.
Now, the sensing cost of
A is the sensing cost of A using an opti-
mal legal choice γ
: Q →T
of sensing trees. Formally, for every class
ζ
∈{LD, LS, GD, GS}, we deﬁne ζscost(A) as min{ζscost(A, γ) : γ
∈
QT is legal in ζ}.
Finally, the sensing cost of a regular language L ⊆(2P )∗is the inﬁ-
mum of the sensing costs of DFAs that recognize L. That is, for every class
ζ ∈{LD, LS, GD, GS}, we have that ζscost(L) = inf{ζscost(A) : L(A) = L}.
We use inﬁmum in the deﬁnition since the number of DFAs recognizing L is
unbounded. In fact, a-priori, there is no guarantee that ζscost(L) is attained by
a DFA.
3
Probability-Based Deﬁnition of Lazy-Sensing Cost
The deﬁnition of sensing cost of a DFA in Sect. 2.6 is not eﬀective, in the sense
it does not suggest a way to calculate the sensing cost of a DFA. In this section
we describe an alternative deﬁnition, which does suggest such a way. Essentially,
while the deﬁnition in Sect. 2.6 refers to the sensing cost of words of increas-
ing length, our deﬁnition here refers to the sensing costs of states visited by
random walks on the DFA. We ﬁrst need some deﬁnitions and notations about
probability.
A Markov chain M = ⟨S, P⟩consists of a ﬁnite state space S and a stochas-
tic transition matrix P : S × S →[0, 1]. That is, for all s ∈S, we have

s′∈S P(s, s′) = 1.

Lazy Regular Sensing
163
Consider a directed graph G = ⟨V, E⟩. A strongly connected component
(SCC) of G is a maximal (with respect to containment) set C ⊆V such that for
all x, y ∈C, there is a path from x to y. An SCC (or state) is ergodic if no other
SCC is reachable from it, and is transient otherwise.
An automaton A = ⟨Σ, Q, q0, δ, α⟩induces a directed graph GA = ⟨Q, E⟩
in which ⟨q, q′⟩∈E iﬀthere is a letter σ such that q′ = δ(q, σ). When we talk
about the SCCs of A, we refer to those of GA. Recall that we assume that the
letters in Σ are uniformly distributed, thus A also corresponds to a Markov
chain MA in which the probability of a transition from state q to state q′ is
pq,q′ =
1
|Σ||{σ ∈Σ : δ(q, σ) = q′}|. Let C be the set of A’s SCC, and Ce ⊆C be
the set of its ergodic SCC’s.
Consider an ergodic SCC C ∈Ce. Let PC be the matrix describing the
probability of transitions in C. Thus, the rows and columns of PC are associated
with states, and the value in coordinate q, q′ is pq,q′. By [4], there is a unique
probability vector πC ∈[0, 1]C such that πCPC = πC. This vector describes the
stationary distribution of C: for all q ∈C it holds that πC(q) = limm→∞
EC
m(q)
m
,
where EC
m(q) is the average number of occurrences of q in a run of MA of length
m that starts anywhere in C [4]. Thus, intuitively, πC(q) is the probability that
a long run that starts in C ends in q. In order to extend the distribution to
the entire Markov chain of A, we have to take into account the probability of
reaching each of the ergodic components. The SCC-reachability distribution of
A is the function ρ : Ce →[0, 1] that maps each ergodic SCC C of A to the
probability that MA eventually reaches C, starting from the initial state. The
limiting distribution π : Q →[0, 1] is now deﬁned by π(q) = 0, if q is transient,
and π(1) = πC(q) · ρ(C), if q is in some C ∈Ce. By [4], the limiting distributions
can be computed in polynomial time by solving a system of linear equations.
Intuitively, the limiting distribution of state q describes the probability of
a run on a random and long input word to end in q. Formally, we have the
following lemma.
Lemma 2 [1]. Let Em(q) be the expected number of occurrences of a state q in
a run of length m of MA that starts in q0. Then, π(q) = limm→∞
Em(q)
m
.
The alternative deﬁnition is based on the following lemma, see proof in the
full version.
Lemma 3. Let A = ⟨2P , Q, q0, δ, α⟩be a DFA, and let γ be a choice of sensing
trees for A. Then,
scost(A, γ) = lim
m→∞|2P |−m

w∈(2P )m
1
m
m−1

i=0
scost(γ(δ(w[1, i])).
Lemma 3 enables us to follow the exact same considerations in [1], thus
computing the lazy-sensing cost of a DFA by examining its induced Markov
chain. Formally, we have the following.

164
O. Kupferman and A. Petruschka
Theorem 1. Let A be a DFA with alphabet 2P , state space Q, and limiting
distribution π : Q →[0, 1]. Then, for every choice γ of sensing trees for A, we
have that scost(A, γ) = 
q∈Q π(q) · scost(γ(q)).
4
Lazy-Sensing Cost vs. Size
In this section we examine the trade-oﬀbetween the size of a DFA and its sensing
cost in the four lazy classes of sensing. It is shown in [1] that in the naive setting
of sensing, namely when all signals are read simultaneously, minimizing the size
of a DFA goes hand in hand with minimizing its sensing cost. Thus, minimal
naive sensing is attained in a minimal-size DFA. In this section, we show that
this good news is carried over to lazy sensing.
Consider a language L ⊆Σ∗. For two ﬁnite words u1 and u2, we say that u1
and u2 are right L-indistinguishable, denoted u1 ∼L u2, if for every z ∈Σ∗, we
have that u1·z ∈L iﬀu2·z ∈L. Thus, ∼L is the Myhill-Nerode right congruence
used for minimizing automata. For u ∈Σ∗, let [u] denote the equivalence class
of u in ∼L and let ⟨L⟩denote the set of all equivalence classes. Each class
[u] ∈⟨L⟩is associated with the residual language u−1L = {w : uw ∈L}. When
L is regular, the set ⟨L⟩is ﬁnite, and induces the residual automaton of L, deﬁned
by RL = ⟨Σ, ⟨L⟩, δL, [ϵ], α⟩, with δL([u], a) = [u · a], for all [u] ∈⟨L⟩and a ∈Σ.
Also, α contains all classes [u] with u ∈L. The DFA RL is well deﬁned and is
the unique minimal DFA for L.
Lemma 4. Consider a regular language L ⊆Σ∗. For every DFA A with L(A) =
L and lazy-sensing class ζ ∈{LD, LS, GD, GS}, it holds that ζscost(RL) ≤
ζscost(A).
Proof: Let A = ⟨2P , Q, q0, δ, α⟩be a DFA such that L(A) = L. Consider a
reachable state q ∈Q. Let u ∈(2P )∗be a word such that A reaches the q
after reading u, thus q = δ∗(q0, u). Recall that RL reaches the state [u] after
reading u. We claim that for every layout T of a sensing tree over P, we have
that scost(T[u]) ≤scost(Tq), where T[u] is the sensing tree obtained from T by
reducing it according to the transitions of RL from [u], and Tq is the sensing
tree obtained from T by reducing it according to the transitions of A from q.
By
Lemma
1,
for
every
sensing
tree
T,
we
have
scost(T)
=

v∈Int(T ) 2−depth(v). Accordingly, it suﬃces to prove that for all letters σ, σ′ ∈
2P if δ(q, σ) = δ(σ′), then δL([u], σ) = δL([u], σ′). Indeed, this would guarantee
that every vertex that is deleted from the layout T when it is reduced to Tq is
also deleted when T is reduced to T[u]. In the full version, we prove this claim.
⊓⊔
Since L(RL) = L, then for every class ζ ∈{LD, LS, GD, GS}, we have that
ζscost(L) ≤ζscost(RL). Thus, together with Lemma 4, we can conclude with
the following.
Theorem 2. For every regular language L ⊆(2P )∗and lazy-sensing class ζ ∈
{LD, LS, GD, GS}, we have that ζscost(L) = ζscost(RL).

Lazy Regular Sensing
165
5
Comparing the Diﬀerent Sensing Classes
In this section we examine the saving of sensing that the lazy classes enable.
We start by comparing the lazy classes with the setting in [1], where all signals
are sensed simultaneously, and continue to examine the relations among the four
lazy classes.
5.1
Lazy vs. Naive Sensing
Recall that in the setting of [1], which we refer to as naive sensing, the sensing
cost of a DFA A is deﬁned as follows (we use the preﬁx N for naive).
Nscost(A) = lim
m→∞|2P |−m

w∈(2P )m
1
m
m−1

i=0
|psensed(δ(w[1, i]))|,
as all the potentially sensed signals must in fact be sensed. The sensing cost of
a regular language in the naive sensing setting is then deﬁned as Nscost(L) =
inf{Nscost(A) : L(A) = L}. We ﬁrst show that, as expected, the sensing cost in
all lazy classes is never higher than the naive one.
Theorem 3. For every DFA A over an alphabet 2P and for every lazy-sensing
class ζ ∈{LD, LS, GD, GS}, we have that ζscost(A) ≤Nscost(A).
Proof: We prove the theorem for the static classes LS and GS. Since every
choice function γ that is legal in these classes is legal also in the corresponding
dynamic class, the result for LD and GD follows. Let A = ⟨2P , Q, q0, δ, α⟩, and
let γ be a choice of sensing trees for the DFA A that is legal with respect to
ζ ∈{LS, GS}. We claim that for every state q ∈Q, if p /∈psensed(q), then there
is no internal vertex with the label p in γ(q). Since this holds for all choices
function γ, in particular these that attain ζscost(A), the theorem follows.
In order to prove the claim, consider a state q and let γ(q) = ⟨V, E, τ⟩.
Consider an internal vertex v ∈V such that τ(v) = p. If p /∈psensed(q), then for
every S ∈2P , we have that δ(q, S\{p}) = δ(q, S ∪{p}). Therefore, regardless of
ζ, the subtrees of ⟨V, E, τ⟩with roots vleft and vright calculate the same function.
Since ζ is static, this implies that vleft and vright root identical subtrees, and so
we can reduce ⟨V, E, τ⟩by redirecting the edge that enters v to vleft.
⊓⊔
Corollary 1. For every regular language L ⊆(2P )∗and lazy-sensing class ζ ∈
{LD, LS, GD, GS}, we have that ζscost(L) ≤Nscost(L).
We now show that, on the one hand, there are cases where lazy sensing is not
helpful (Theorem 4), and, on the other hand, there are cases where the saving
that lazy sensing enables is unbounded (Theorem 5).
Theorem 4. For every ﬁnite set P of signals, there is a regular language L ⊆
(2P )∗such that for every lazy-sensing class ζ ∈{LD, LS, GD, GS}, we have that
ζscost(L) = Nscost(L).

166
O. Kupferman and A. Petruschka
Proof: Let L = {w1 · · · wm ∈(2P )∗: m ≥1 and |wm| is even} be the language
of all words in (2P )∗that end with a letter that consists of an even number of
signals. A DFA A for L must sense all the signals in P in all states. Indeed, the
DFA A has to identify, in all states, whether the current input letter consists of
an even number of signals. Thus, for every state of q of A and choice function
γ, we have that scost(γ(q)) = |P|. By Theorem 1 and the deﬁnition of the naive
sensing cost of a DFA, we conclude that ζscost(A) = Nscost(A) = |P|. Since
the above holds for every DFA A recognizing L, the result follows.
⊓⊔
Theorem 5. For every n
≥
1, there is a regular language Ln
over
2{p1,...,pn} such that Nscost(Ln) = n, yet for every lazy-sensing class ζ ∈
{LD, LS, GD, GS}, we have that ζscost(Ln) < 2.
Proof: Let Pn = {p1, ..., pn} be a set of n signals, let σ = Pn, and let Ln be
the language of all words with an even number of occurrences of the letter σ. A
minimal DFA An that recognizes Ln consists of two states, keeping track of the
parity of occurrences of σ, see Fig. 4 below.
An
q0
q1
p1 ∧p2 ∧· · · ∧pn
p1 ∧p2 ∧· · · ∧pn
(¬p1) ∨(¬p2) ∨· · · ∨(¬pn)
(¬p1) ∨(¬p2) ∨· · · ∨(¬pn)
Fig. 4. Lazy sensing is better than naive sensing.
It is easy to see that psensed(q0) = psensed(q1) = Pn. Indeed, for every
pi ∈Pn we have δ(q0, Pn∪{pi}) = q1 ̸= q0 = δ(q0, Pn\{pi}) and δ(q1, Pn∪{pi}) =
q0 ̸= q1 = δ(q1, Pn\{pi}). Since q0 and q1 are the only states of An, it follows that
Nscost(An) = n. Also, as the naive sensing cost of a regular language is attained
in the minimal DFA recognizing the language [1], it follows that Nscost(Ln) = n.
We now consider the lazy sensing cost of Ln. By Theorem 2, here too we can
consider the DFA An. It is easy to see that a sensing tree T of minimal sensing
cost for each of the states qi, with i ∈{0, 1}, consists of n internal vertices,
one in each height from 0 to n −1, labeled by all of
the signals in Pn in some arbitrary order.
For each such internal vertex, its left child is a leaf
labeled qi. If the vertex is in height diﬀerent from n−1,
its right child is another internal vertex. If it is in
height n −1, its right child is a leaf labeled with q1−i.
The ﬁgure on the right shows such a minimal sensing
tree for the state q0.
It is not hard to see that the suggested sensing tree is legal in all classes ζ.
Indeed, the same layout is used for q0 and q1, and the tree follows an order on

Lazy Regular Sensing
167
P that is independent of the values read. By Lemma 1, we have that scost(T) =

v∈Int(T ) 2−depth(v) = n−1
i=0 2−i = 2 −
1
2n−1 . Thus, by Theorems 2 and 1, for
all classes ζ ∈{LD, LS, GD, GS}, we have that ζscost(Ln) = scost(An) =
2 −
1
2n−1 < 2.
⊓⊔
5.2
Comparison of the Diﬀerent Lazy-Sensing Classes
In this section, we compare the sensing costs in the diﬀerent lazy sensing classes.
First, since every choice function that is legal in the global classes is legal in the
local ones, and every choice function that is legal in the static classes is legal in
the dynamic ones, we immediately have the following.
Theorem 6. For every regular language L ⊆(2P )∗, the following holds
(i)
LDscost(L) ≤GDscost(L),
(ii)
LSscost(L) ≤GSscost(L),
(iii)
LDscost(L) ≤LSscost(L),
(iv)
GDscost(L) ≤GSscost(L).
Theorem 4 implies that there are languages for which the sensing costs in the
four classes coincide. In the following, we describe cases where the inequalities
in Theorem 6 are strict. In addition, we show that the local-static class and the
global-dynamic class are incomparable: there is a language L with LSscost(L) <
GDscost(L) and also a language L with LSscost(L) > GDscost(L).
We start with the advantage of the local classes over the global ones:
Lemma 5. There exists a regular language L ⊆(2P )∗such that LDscost(L) =
LSscost(L) < GDscost(L) = GSscost(L).
Proof: Let P = {a, b} and consider the DFA A with alphabet 2P shown in
Fig. 5.
q0
q1
q2
¬a ∧b
a
¬a ∧¬b
¬a ∧¬b
a
b
a ∧¬b
¬a
Fig. 5. Local lazy sensing is better than global one.
It can be easily veriﬁed that A is a minimal DFA, for example using
the standard DFA minimization algorithm. In the full version, we prove that
LDscost(A) = LSscost(A) < GDscost(A) = GSscost(A), which, by Theorem 2,
implies that L(A) satisﬁes the conditions in the lemma.
⊓⊔

168
O. Kupferman and A. Petruschka
We continue with the advantage of the dynamic classes over the static ones:
Lemma 6. There exists a regular language L ⊆(2P )∗such that LDscost(L) =
GDscost(L) < LSscost(L) = GSscost(L).
Proof: Let P = {a, b, c} and consider the DFA A with alphabet 2P shown in
Fig. 6.
Fig. 6. Dynamic lazy sensing is better than static one.
Again, it can be veriﬁed that A is minimal, thus it is left to prove that
LDscost(A) = GDscost(A) < LSscost(A) = GSscost(A), which we do in the full
version.
⊓⊔
6
Directions for Future Research
We introduced lazy sensing for deterministic ﬁnite automata. We studied the
basic problems about the setting, namely a study of four natural classes of
lazy sensing, their comparison with naive sensing, and the trade-oﬀbetween
minimizing the sensing cost of a DFA and minimizing its size. We left open
several interesting problems, which we discuss below.
Computing Lazy Sensing Cost: In [1], it is shown that the naive sensing
cost of a DFA can be calculated in polynomial time using standard Markov
chain algorithms. Accordingly, the naive sensing cost of a regular language can
also be calculated in polynomial time using the classical minimization algorithm
for DFA. In order to compute the sensing cost in lazy-sensing classes, one also
needs to ﬁnd the optimal sensing trees for a given DFA.
The involved questions now depend on the lazy-sensing class. For the SL class,
the problem is strongly related to the problem of ﬁnding an optimal ordering for
the variables in a BDD, and the complexity depends on the way the transition
function of the DFA is given. In the GS class, there is the extra requirement
that the same order is used in the transition functions of all states. Then, in

Lazy Regular Sensing
169
the dynamic classes, the layouts we may use need not follow an ordering for the
variables, and techniques from the theory of BDDs are less relevant.
Random Lazy Sensing: While dynamic and local lazy sensing may save more
than static and global lazy sensing, they require the maintenance of more com-
plex data structures. In addition to studying lazy sensing classes with some
bounded level of dynamics or locality, it is interesting to examine a stochastic
approach, where the signal to be sensed next is chosen randomly. It is not hard
to see that our results in Sects. 4 and 5.1 apply also to the random lazy setting,
when we examine the expected sensing cost of a DFA, with expectation now
referring to both the input words and the order in which signals are sampled.
References
1. Almagor, S., Kuperberg, D., Kupferman, O.: Regular sensing. In: Proceedings of
34th Conference on Foundations of Software Technology and Theoretical Computer
Science. LIPIcs, vol. 29, pp. 161–173. Schloss Dagstuhl - Leibniz-Zentrum fuer Infor-
matik, Germany (2014)
2. Burch, J.R., Clarke, E.M., McMillan, K.L., Dill, D.L., Hwang, L.J.: Symbolic model
checking: 1020 states and beyond. Inf. Comput. 98(2), 142–170 (1992)
3. Donoho, D.L.: Compressed sensing. IEEE Trans. Inform. Theory 52, 1289–1306
(2006)
4. Grinstead, C., Laurie Snell, J.: 11: Markov chains. In: Introduction to Probability.
American Mathematical Society (1997)
5. Kindler, G.: Property testing, PCP, and Juntas. Ph.D. thesis, Tel Aviv University
University (2002)
6. Miller, D.M., Drechsler, R.: On the construction of multiple-valued decision dia-
grams. In: 32nd IEEE International Symposium on Multiple-Valued Logic, pp. 245–
253. IEEE Computer Society (2002)
7. Minker, J., Minker, R.G.: Optimization of Boolean expressions-historical develop-
ments. IEEE Ann. Hist. Comput. 2(3), 227–238 (1980)
8. Muthukrishnan, S.: Theory of data stream computing: where to go. In: Proceedings
of 30th Symposium on Principles of Database Systems, pp. 317–319 (2011)

State Complexity of Finite Partial
Languages
Martin Kutrib(B)
and Matthias Wendlandt
Institut f¨ur Informatik, Universit¨at Giessen, Arndtstr. 2, 35392 Giessen, Germany
{kutrib,matthias.wendlandt}@informatik.uni-giessen.de
Abstract. Partial word ﬁnite automata are deterministic ﬁnite automa-
ta that may have state transitions on a special symbol ⋄which represents
an unknown symbol or a hole in the word. Together with a subset of
the input alphabet that gives the symbols which may be substituted
for the ⋄, a partial word ﬁnite automaton (⋄-DFA) represents a regular
language. However, this substitution implies a certain form of limited
nondeterminism in the computations when the ⋄-transitions are replaced
by ordinary transitions. In this paper we consider the state complexity
of partial word ﬁnite automata accepting ﬁnite languages. We study the
state complexity of the NFA to ⋄-DFA conversion for ﬁnite languages
as well as the state complexity of the ⋄-DFA to DFA conversion for
ﬁnite languages. Then we consider the operational state complexity with
respect to complementation, union, reversal, and concatenation of ﬁnite
languages. It turns out that the upper and lower bounds for all these
operations are exponential. Moreover, we establish a state complexity
hierarchy on the number of productive ⋄-transitions that may appear
in ⋄-DFAs accepting ﬁnite languages. The levels of the hierarchy are
separated by quadratic state costs.
Keywords: Partial words · ﬁnite languages · deterministic ﬁnite
automata · minimal automata · determinization · operational state
complexity · hierarchies on the number of unknown symbol transitions
1
Introduction
Partial words are strings where certain positions are not speciﬁed. These posi-
tions are often called holes or don’t cares and printed by a diamond symbol ⋄.
Apart from theoretical reasons, the basic motivation for studying this mechanism
comes from the study of biological operations in connection with DNA strands.
The ﬁrst time the idea of words with don’t cares has been investigated goes
back to [12], where they were considered in connection with string matching.
The term partial word was ﬁrstly deﬁned in [2].
Partial words were mainly investigated in connection with combinatorics on
words. A survey can be found in [4]. An interesting motivation in theory for
this model is that ordinary languages can be compressed by the usage of holes.
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 170–183, 2022.
https://doi.org/10.1007/978-3-031-13257-5_13

State Complexity of Finite Partial Languages
171
Consider for example the language L over the ternary alphabet Σ = {a, b, c},
L = {aaa, aba, aca}. It can be compressed by using a hole into L′ = {a ⋄a}.
Simply by replacing the diamond by a, b, or c the original language L can be
achieved. In 2012, partial words were studied in connection with families of
formal languages [11]. In particular, a regular language is represented by the
image of a partial language under a substitution that only replaces the hole
symbols. In connection with DFAs it turned out that the usage of holes can be
somehow seen as a limited nondeterminism, since it allows to deﬁne DFAs with
outgoing edges that are labeled with ordinary symbols and additionally with a
diamond. If some of the ordinary symbols may be substituted for the hole symbol
as well, the corresponding state allows a nondeterministic choice with respect to
the target language (see for example [1,11,16]).
The applications of deﬁning language families by partial words via partial
word ﬁnite automata have also been investigated from a complexity point of
view. Concerning the descriptional complexity, in [1] it has been shown that the
state complexity for a DFA that simulates a partial word DFA is exponential in
general. Moreover also the state complexity of the simulation of an NFA by a
partial word DFA may become exponential. In [19] further basic constructions,
the operational state complexity for Boolean operations, and a hierarchy depen-
dent on the number of ⋄-transitions in the state graph are addressed. Concerning
the computational complexity, diﬀerent problems as, for example, minimization
have been studied for partial word automata [5,16].
The main aim of this paper is to extend the investigations on the descriptional
complexity of partial word automata to ﬁnite languages.
Finite languages are an important subclass of regular languages. They are
accepted by incomplete acyclic DFAs. Acyclic automata are widely used in appli-
cations, for example as eﬃcient data structure for dictionary representation. The
time needed to access an entry is linear in its word length. Another example is
the manipulation of Boolean functions, where minimal acyclic automata are used
to obtain fast algorithms for problems as satisﬁability test, equivalence test, and
function composition [6]. For ﬁnite languages, linear time minimization algo-
rithms are known [21] while the minimization of general DFAs takes O(n log n)
time. Another important construction is the determinization of nondeterminis-
tic ﬁnite automata. The powerset construction requires a tight bound of 2n for
general regular languages. For ﬁnite languages a smaller tight bound has been
obtained in [22]. It is of order O(ℓ
n
1+log2 ℓ) and depends on the alphabet size ℓ.
Further results for DFAs accepting ﬁnite languages include the maximal number
of states of the minimal DFA accepting a subset of Σk or Σ≤k [8,10]. A diﬀerent
approach to represent ﬁnite languages by ﬁnite automata has been introduced
in [9]. The concept is known as cover automata, where the idea is that a DFA
may accept also words not belonging to the ﬁnite language as long as these words
are longer than the longest word in the ﬁnite language. Cover automata have
attracted great interest and have since been intensively investigated.
The paper is organized as follows. In the next section we present the underly-
ing deﬁnitions and preliminary remarks. Section 3 deals with the state trade-oﬀs
for converting the representation of a ﬁnite language from an NFA to a partial

172
M. Kutrib and M. Wendlandt
word ﬁnite automaton (⋄-DFA) as well as with the state trade-oﬀs for convert-
ing the representation from a ⋄-DFA to a DFA. For both conversions an upper
bound of order O(ℓ
n
1+log2 ℓ) follows from the state costs of the determinization.
We prove a lower bound of order O(ℓ
n
2 log2 ℓ) for the ⋄-DFA to DFA conversion.
For the NFA to ⋄-DFA conversion we can show the lower bound of O(ℓ
n
1+log2 ℓ).
However, it is not tight since we start with a (2n + 1)-state NFA and use an
alphabet of size 2ℓ. Anyway, the state costs for both conversions are exponen-
tial. Section 4 considers the operational state complexity for complementation,
union, reversal, and concatenation. It turns out that upper and lower bounds
are exponential. In the last section we consider the impact of the number of
productive ⋄-transitions in a partial word ﬁnite automaton, where a transition is
called productive, if it does not lead to the rejecting sink state. For ⋄-DFAs rep-
resenting general regular languages, it is known that even the reduction of one
of these transitions may lead to an exponential state explosion [19]. In contrast
to these results, here it comes out that the state costs for removing one pro-
ductive ⋄-transition is quadratic. Therefore, removing any constant number of
⋄-transitions from a ⋄-DFA accepting a ﬁnite language causes only a polynomial
state blow-up.
2
Preliminaries
We denote the non-negative integers {0, 1, 2, . . . } by N. Let Σ∗denote the set of
all words over the ﬁnite alphabet Σ. and Σ≤k denote its restriction to words of
length at most k, for any k ≥0. A subset L ⊆Σ∗is said to be a formal language
over Σ. We write L for the complement of L with respect to Σ, that is for Σ∗\L.
The empty word is denoted by λ and the reversal of a word w by wR. For the
length of w we write |w|. We use ⊆for inclusions and ⊂for strict inclusions.
Setting Σ⋄= Σ ∪{⋄}, where ⋄/∈Σ represents undeﬁned positions or holes,
a partial word over Σ is a sequence of symbols from Σ⋄. Denoting the set of all
partial words over Σ by Σ∗
⋄, a partial language over Σ is a subset of Σ∗
⋄. Partial
languages can be transformed to (ordinary) languages by using ⋄-substitutions
over Σ. A ⋄-substitution σ: Σ∗
⋄→2Σ∗satisﬁes σ(a) = {a}, for all a ∈Σ, σ(⋄) ⊆
Σ, and σ(uv) = σ(u)σ(v), for u, v ∈Σ∗
⋄. As a result, σ is fully deﬁned by σ(⋄),
for example, if σ(⋄) = {a, b} and L = {⋄b, ⋄c} then σ(L) = {ab, bb, ac, bc}.
So, applying σ to a partial language L ⊆Σ∗
⋄results in a (ordinary) language
σ(L) ⊆Σ∗.
A nondeterministic ﬁnite automaton (NFA) is a system M = ⟨Q, Σ, δ, q0, F⟩,
where Q is the ﬁnite set of internal states, Σ is the ﬁnite set of input symbols,
q0 ∈Q is the initial state, F ⊆Q is the set of accepting states, and δ: Q×Σ →2Q
is the transition function. In the forthcoming, we sometimes refer to δ as a
subset of Q × Σ × Q. A ﬁnite automaton M is deterministic (DFA) if and
only if |δ(q, a)| = 1, for all q ∈Q and a ∈Σ. In this case, we simply write
δ(q, a) = q′ for δ(q, a) = {q′} assuming that the transition function is a total
mapping δ: Q × Σ →Q. Note that here any DFA is complete, that is, the
transition function is total, whereas it may be a partial function for NFAs in
the sense that the transition function of nondeterministic machines may map

State Complexity of Finite Partial Languages
173
to the empty set. A ﬁnite automaton is said to be minimal if there is no ﬁnite
automaton of the same type with fewer states, accepting the same language. Note
that a rejecting sink state is counted for DFAs, since they are always complete,
whereas it is not counted for NFAs, since their transition function may map to
the empty set.
Generally speaking, a language L can be represented by a partial language L′
together with a ⋄-substitution σ such that σ(L′) = L. In particular, for regular
languages, from the descriptional complexity point of view it is an interesting
question to what extent there are regular languages L′ such that the minimal
DFA accepting L′ has less states than the minimal DFA accepting L? In order to
distinguish between ﬁnite automata accepting (ordinary) languages from those
accepting partial languages, we refer to the latter as partial word deterministic
ﬁnite automata (⋄-DFA). Thus, ⋄-DFAs treat the hole symbol ⋄as an ordinary
input letter.
Given some ⋄-DFA M with ⋄-substitution σ, we construct its canonical DFA
as follows. First, M is modiﬁed to the NFA ˆ
M by resolving the ⋄-substitution.
That is, any ⋄-transition is replaced by transitions on the symbols in σ(⋄).
Then, ˆ
M is determinized and the outcome is minimized. This construction is
presented as Algorithm 1 in [1].
The intermediate NFA in the construction exhibits the limited nondetermin-
ism provided by ⋄-DFAs. In fact, for each state of the NFA, there are at most
two outgoing transitions for each input symbol.
The number of states of the (complete) minimal DFA accepting a regular
language L is denoted by minDFA(L). Similarly, minNFA(L) denotes the minimal
number of states necessary for some NFA to accept L. For partial languages, we
write min⋄-DFA(L) to denote the minimal number of states of a ⋄-DFA accepting
a language L′ such that there exists a ⋄-substitution σ with σ(L′) = L.
In connection with lower bounds on the number of states necessary for an
automaton to accept a given language, the problem arises to prove the min-
imality of a given automaton. While a couple of techniques exist to prove the
minimality of DFAs, only a few techniques exist for NFAs. The situation is much
worse for ⋄-DFAs. Clearly, a ⋄-DFA can be seen as a DFA over the alphabet Σ⋄.
But, in general, the minimization of a ⋄-DFA M changes the language that it
represents, that is, σ(L(M)). The problem to ﬁnd a minimal ⋄-DFA (together
with a ⋄-substitution) for a given regular language has been studied in detail
in [5], where algorithms are given for the construction of minimal partial lan-
guages, associated with some ⋄-substitution, as well as approximation algorithms
for the construction of minimal ⋄-DFAs. However, for particular languages that
witness certain lower bounds, their minimality has to be proved almost from
scratch.
For our purposes the so-called (extended) fooling set technique (see, for exam-
ple, [3,14,18]) is useful. It is a technique to prove lower bounds on the number
of states necessary for an NFA to accept a given language.
Theorem 1. Let L ⊆Σ∗be a regular language and suppose there exists a set
of pairs P = { (xi, yi) | 1 ≤i ≤n } such that (1) xiyi ∈L, for 1 ≤i ≤n,
and (2) i ̸= j implies xiyj ̸∈L or xjyi ̸∈L, for 1 ≤i, j ≤n. Then any

174
M. Kutrib and M. Wendlandt
nondeterministic ﬁnite automaton accepting L has at least n states. Here P is
called an (extended) fooling set for L.
3
State Trade-Oﬀs Between Nondeterminism, Partial
Words, and Determinism
Ranges of possible state trade-oﬀs between NFAs and ⋄-DFAs as well as between
⋄-DFAs and DFAs have been studied in [1] for several types of regular languages,
some of them are ﬁnite. Here we turn to the worst case scenarios for both trade-
oﬀs for general ﬁnite languages over ﬁxed alphabets. It turned out that the size of
the alphabet plays a crucial role for the state complexity of ﬁnite languages. For
example, in [20] it is shown that for each n-state NFA accepting a ﬁnite language
over a binary alphabet, there exists an equivalent DFA which has O(2
n
2 ) states,
and that this bound is tight in the order of magnitude. However, for larger
alphabet sizes this upper bound is no longer true. The general case of an arbitrary
ℓ-letter alphabet, ℓ≥2, has been solved in [22]. It is shown that for any n-state
NFA accepting a ﬁnite language over an ℓ-letter alphabet there is an equivalent
DFA with O(ℓ
n
1+log2 ℓ) states. Moreover, this bound is tight.
3.1
State Complexity of the NFA to ⋄-DFA Conversion
An upper bound for the state trade-oﬀbetween NFAs and ⋄-DFAs accepting
ﬁnite languages is given by the mentioned determinization.
Corollary 2. Let n ≥1 and ℓ≥2 be integers, Σ be an ℓ-letter alphabet, and L ⊆
Σ∗be an arbitrary ﬁnite language accepted by an n-state NFA. Then O(ℓ
n
1+log2 ℓ)
states are suﬃcient for a ⋄-DFA that accepts L.
In order to obtain a lower bound we consider an idea of [11] and show a
generalized result. In particular, we can translate the lower bounds of the state
trade-oﬀs for the determinization of NFAs to lower bounds of the state trade-
oﬀs for the NFA to ⋄-DFA conversion. In this way, we derive lower bounds
dependent on the sizes of languages that can be plugged in. So, in particular,
for our purposes we can use ﬁnite languages.
Lemma 3. Let n ≥1 and ℓ≥2 be integers, Σ be an ℓ-letter alphabet, and M
be a minimal n-state NFA accepting a language over Σ that does not contain a
word of length one. There exists a regular language L over a 2ℓ-letter alphabet
such that there is an NFA accepting L with at most 2n + 1 states and any
⋄-DFA accepting L has at least min⋄-DFA(L(M)) + minDFA(L(M)) −3 states.
Moreover, L is ﬁnite if and only if L(M) is ﬁnite.
Lemma 3 can be applied to obtain lower bounds for inﬁnite languages as
well. In [11] it has been shown that there exists a regular language such that any
minimal NFA accepting it has at most 2n + 1 states and every ⋄-DFA requires
at least 2n −2n−2 states for its representation. Before we continue with ﬁnite
languages, we improve this lower bound in the next proposition.

State Complexity of Finite Partial Languages
175
Proposition 4. Let n ≥3 be an integer. There exists a (2n + 1)-state NFA M
such that any ⋄-DFA representing L(M) has at least 2n states.
Proof. In order to apply Lemma 3 we take some minimal n-state NFA M that
does not accept any word of length one and that causes the maximal state blow-
up for determinization. That is, any equivalent DFA has at least 2n states.
Now the application of Lemma 3 yields that there is a regular language L
such that there is an NFA accepting L with at most 2n+1 states and any ⋄-DFA
accepting L has at least 2n states.
⊓⊔
A lower bound for the NFA to ⋄-DFA state trade-oﬀfor ﬁnite languages is
shown in the next theorem. The proof uses the witness languages for the lower
bound of the NFA to DFA state trade-oﬀs for ﬁnite languages derived in [22].
Theorem 5. Let ℓ≥2 and n ≥2⌈log2 ℓ⌉+ 2 be integers, and Σ be an ℓ-letter
alphabet. There exists a (2n+1)-state NFA accepting a ﬁnite language such that
any ⋄-DFA representing the same language has at least Ω(ℓ
n
1+log2 ℓ) states.
Proof. First, we deﬁne the witness languages for the assertion. To this end, we
take the witness languages from [22]. Let Σ = {a1, a2, . . . , aℓ} be an ℓ-letter
alphabet. Then we set t = ⌈log2 ℓ⌉and m = ⌊n
t+1⌋. Each letter in Σ is encoded
by the t-digit binary sequence that is the binary expansion of its index minus one.
For each 1 ≤i ≤t, we deﬁne the set Si = { aj ∈Σ | the ith digit of j −1 is 1 }.
Then, language L′
n is deﬁned as
{ w = vtxtvt−1xt−1 · · · v1x1v0 |
vj ∈Σm−1, xj ∈Sj, 1 ≤j ≤t, and |w| = n −1 }.
Now, let Ln be the set of all suﬃxes of length at least m of all words in L′
n. In
order to obtain an n-state NFA accepting Ln we start with an n-state DFA that
accepts L′
n. Basically, the DFA consists of a chain of n-states that allows to check
the length n −1 of the words and to check that the symbols at the positions
m, 2m, . . . , t · m are from the sets St, St−1, . . . , S1, respectively. Then, an NFA is
obtained from the DFA by ﬁrst adding λ-transitions from the initial state to all
except the last m states. However, it is well known that the λ-transitions can be
removed without increasing the number of states and, thus, the NFA has n states.
These sophisticated languages have been constructed in [22], where it is shown
that any DFA accepting Ln has at least (ℓ⌊
n
1+log2 ℓ⌋+1 −1)/ℓ−1) ∈Ω(ℓ
n
1+log2 ℓ)
states.
Next, we turn to plug the languages Ln in Lemma 3. Clearly, we have m ≥2,
for all n ≥2⌈log2 ℓ⌉+ 2, and, thus, the language Ln does not contain any word
of length one. Moreover, since the maximal length of the words in Ln is n −1,
any NFA accepting Ln has at least n states. So, the preconditions of Lemma 3
are met and we derive that there is a regular language L such that there is an
NFA accepting L with at most 2n + 1 states and any ⋄-DFA accepting L has at
least Ω(ℓ
n
1+log2 ℓ) states.
⊓⊔

176
M. Kutrib and M. Wendlandt
It is worth noticing that the lower bound of Theorem 5 is not tight for ﬁnite
languages. The reason is that the alphabet size does matter for ﬁnite languages
and that the ﬁnite language L which witnesses the lower bound is over a 2ℓ-letter
alphabet. So, a converted lower bound is Ω(( ℓ
2)
n
log2 ℓ).
3.2
State Complexity of the ⋄-DFA to DFA Conversion
Here we turn to the question of how many states can we save when we allow
wildcard symbols in the sense of ⋄-substitutions for DFAs. To this end, we con-
sider the state trade-oﬀbetween ⋄-DFAs and DFAs. For inﬁnite languages it is
known that 2n −1 is a tight bound for this trade-oﬀ[1].
The situation for ﬁnite languages is diﬀerent. On the one hand, the maxi-
mal state blow-up for determinization is diﬀerent. On the other hand, to our
knowledge no NFA being limited nondeterministic in a suitable form is known
that witnesses the maximal state blow-up for the determinization in the ﬁnite
language case. However, we clearly have the upper bound of the general deter-
minization also as upper bound for the ⋄-DFA to DFA conversion.
Corollary 6. Let n ≥1 and ℓ≥2 be integers, Σ be an ℓ-letter alphabet, and
L ⊆Σ∗be an arbitrary ﬁnite language accepted by an n-state ⋄-DFA. Then
O(ℓ
n
1+log2 ℓ) states are suﬃcient for a DFA that accepts L.
For a lower bound we proceed as follows.
Theorem 7. Let n ≥3 and ℓ≥2 be integers. There exists an n-state ⋄-DFA
representing a ﬁnite language over an ℓ-letter alphabet such that any DFA accept-
ing this language has at least Ω(ℓ
n
2·log2 ℓ) states.
Proof. Let Σ = {a1, a2, . . . , aℓ} be an ℓ-letter alphabet. We set k = ⌊n−1
2 ⌋
and deﬁne the ﬁnite language Lk as { w | w = ua1v, u ∈Σ≤k−1, v ∈Σk−1 }.
The ⋄-DFA depicted in Fig. 1 accepts Lk with 2k + 1 states and ⋄-substitution
σ(⋄) = Σ.
Fig. 1. A (2k + 1)-state ⋄-DFA with ⋄-substitution σ(⋄) = Σ such that any DFA
accepting the language represented has Ω(ℓ
n
2·log2 ℓ) states. The rejecting sink state is
not depicted.
It has been proven that any DFA that accepts Lk has at least 2k+1 −1
states [22]. Transforming 2 = ℓlogℓ2 = ℓ
1
log2 ℓ, for n = 2k + 2 > 2k + 1 we obtain
the lower bound ℓ
k+1
log2 ℓ−1 ∈Ω(ℓ
n
2·log2 ℓ).
⊓⊔

State Complexity of Finite Partial Languages
177
4
Operational State Complexity
Let ◦be a ﬁxed operation on languages that preserves regularity. Then the
◦-language operation problem for ⋄-DFAs is deﬁned as follows:
– Given an n-state ⋄-DFA M1 with ⋄-substitution σ1 and an m-state ⋄-DFA M2
with ⋄-substitution σ2,
– how many states are suﬃcient and necessary in the worst case (in terms of n
and m) for a ⋄-DFA M3 with some ⋄-substitution σ3 such that σ3(L(M3)) =
σ1(L(M1)) ◦σ2(L(M2))?
Obviously, this problem generalizes to unary language operations like, for exam-
ple, complementation or reversal.
An upper bound for the operation of complementation follows immediately
from the ⋄-DFA to DFA conversion. Given M1 and σ1, it is suﬃcient to construct
the canonical DFA M ′ that accepts σ1(L(M1)). Now interchanging accepting
and non-accepting states of M ′ gives a DFA that accepts the complement of
σ1(L(M1)). Since, for all regular languages L, min ⋄-DFA(L) ≤minDFA(L) [11],
we have the following proposition.
Proposition 8. Let n ≥1 and ℓ≥2 be integers, and M1 be an n-state ⋄-DFA
with ⋄-substitution σ1 accepting a ﬁnite language over an ℓ-letter alphabet. Then
O(ℓ
n
1+log2 ℓ) states are suﬃcient for a ⋄-DFA M2 with some ⋄-substitution σ2
such that σ2(L(M2)) is the complement of σ1(L(M1)).
The lower bound for the complementation is as follows.
Theorem 9. Let n ≥3 and ℓ≥2 be integers. There exists an (n + 1)-state
⋄-DFA M1 with ⋄-substitution σ1 accepting a ﬁnite language over an ℓ-letter
alphabet, such that any ⋄-DFA M2 with any ⋄-substitution σ2, where σ2(L(M2))
is the complement of σ1(L(M1)), has at least Ω(ℓ
n
3·log2 ℓ) states.
Proof. Let Σ = {a1, a2, . . . , aℓ} be an ℓ-letter alphabet. We set k = ⌈n
3 ⌉and
deﬁne the ﬁnite language Lk as
{ w | w = u1a1vxu2, u1, u2 ∈Σ≤k−1, v ∈Σk−1, x ∈Σ \ {a1} }.
The ⋄-DFA depicted in Fig. 2 accepts Lk with 3k + 1 states and ⋄-substitution
σ(⋄) = Σ.
In order to show that even any minimal NFA accepting the complement Lk
has at least 2k states, we apply Theorem 1 by providing a fooling set P as follows.
Let h: Σ∗→{0, 1}∗be the homomorphism h(a1) = a1 and h(x) = a2, for
x ∈Σ \ {a1}. Then we set P = { (v, v) | v ∈h(Σk) }. To verify the fooling
set property of P for Lk, ﬁrst we note that vv belongs to Lk, for every v ∈
{a1, a2}k. Next, let (v1, v1) and (v2, v2) be two diﬀerent pairs in P. Since v1
and v2 are diﬀerent, there exists a position 1 ≤p ≤k at which v1 has symbol a1
and v2 has symbol a2, or vice versa. Therefore, either v1v2 or v2v1 is of the form
y1y2 · · · yp−1a1yp+1 · · · ykz1z2 · · · zp−1a2zp+1 · · · zk and, thus, belongs to Lk.

178
M. Kutrib and M. Wendlandt
So, P is the desired fooling set. It includes 2k pairs which induces that any
minimal NFA accepting Lk has at least 2k states. Since, in general, minNFA(L) ≤
min⋄-DFA(L) [11], we derive that any ⋄-DFA that accepts Lk has at least 2k states
as well.
Transforming 2 = ℓlogℓ2 = ℓ
1
log2 ℓ, for n = 3k > 3k −1 > 3k −2, we obtain
the lower bound ℓ
k
log2 ℓ∈Ω(ℓ
n
3·log2 ℓ).
⊓⊔
Concerning the two remaining Boolean operations, it is clear that, in gen-
eral, neither the union nor the intersection of partial languages gives a partial
language whose substitution is the union or intersection of the substitutions of
the given partial languages. So a simple cross-product construction does not
help. In the following, we consider the union. The idea for the union of general
regular languages from [19] yields the currently best upper bound. It applies
also for ﬁnite languages. The construction is to take a ⋄-DFA for one of the
given partial languages and the canonical DFA for the other one, and build their
cross-product automaton which is a ⋄-DFA. The proof of the following theorem
is almost literally the same as in the case of general regular languages.
Theorem 10. Let m ≥n ≥1 and ℓ≥2 be integers, M1 be an m-state ⋄-DFA
with ⋄-substitution σ1, accepting a ﬁnite language and M2 be an n-state ⋄-DFA
with ⋄-substitution σ2 accepting a ﬁnite language over an ℓ-letter alphabet. Then
O(m · ℓ
n
1+log2 ℓ) states are suﬃcient for a ⋄-DFA M3 with some ⋄-substitution σ3
such that σ3(L(M3)) = σ1(L(M1)) ∪σ2(L(M2)).
A lower bound for the union can be obtained along the lines of the proof of
Lemma 3.
Proposition 11. Let m ≥n ≥3 and ℓ≥2 be integers. There exist an m-
state ⋄-DFA M1 with ⋄-substitution σ1 accepting a ﬁnite language over an ℓ-
letter alphabet and an n-state ⋄-DFA M2 with ⋄-substitution σ2 accepting a ﬁnite
language over a disjoint ℓ-letter alphabet such that any ⋄-DFA M3 with any ⋄-
substitution σ3 where σ3(L(M3)) = σ1(L(M1)) ∪σ2(L(M2)) has at least m +
Ω(ℓ
n
2·log2 ℓ) −3 states.
Fig. 2. A (3k + 1)-state ⋄-DFA with ⋄-substitution σ(⋄) = Σ that witnesses the lower
bound for the complementation. The rejecting sink state is not depicted.

State Complexity of Finite Partial Languages
179
Next, we turn to the operation reversal. Since the reversal operation com-
mutes with the ⋄-substitution an upper bound follows from the upper bound for
the reversal of ﬁnite languages.
Proposition 12. Let n ≥3 and ℓ≥2 be integers, and M1 be an n-state ⋄-DFA
with ⋄-substitution σ1 accepting a ﬁnite language over an ℓ-letter alphabet. Then
O(ℓ
n
1+log2 ℓ) states are suﬃcient for a ⋄-DFA M2 with some ⋄-substitution σ2
such that σ2(L(M2)) is the reversal of σ1(L(M1)).
The lower bound for the reversal is once more derived with the help of the
witness languages for the lower bound of the NFA to DFA state trade-oﬀs for
ﬁnite languages shown in [22].
Theorem 13. Let n ≥2 and ℓ≥2 be integers. There exists a 2n-state
⋄-DFA M1 with ⋄-substitution σ1 accepting a ﬁnite language over a 2ℓ-letter
alphabet, such that any ⋄-DFA M2 with any ⋄-substitution σ2, where σ2(L(M2))
is the reversal of σ1(L(M1)), has at least Ω(ℓ
n
1+log2 ℓ) states.
Proof. In order to deﬁne the witness languages for the assertion we start with the
witness language Ln for the lower bound of the NFA to DFA state trade-oﬀs for
ﬁnite languages from [22], that has been deﬁned in the proof of Theorem 5. This
language is derived as set of all suﬃxes of a certain minimal length of all words
from language L′
n, where L′
n is accepted by some incomplete DFA consisting of
a chain of n-states with one accepting state. Therefore, the reversal of L′
n and,
thus, the reversal of Ln is accepted by some incomplete n-state DFA.
Now, we are going to use two copies N1 = ⟨Q1, Σ1, δ1, q0,1, F1⟩and N2 =
⟨Q2, Σ2, δ2, q0,2, F2⟩of this DFA with disjoint state sets and disjoint sets of input
symbols. These two copies are assembled to an incomplete DFA N = ⟨(Q1∪Q2∪
{q0})\{q0,1, q0,2}, Σ1 ∪Σ2, δ, q0, F1 ∪F2⟩by merging the initial states. So, for all
but the initial state, the transition function δ contains all transitions of δ1 and
δ2. For q0 we set δ(q0, a) = δ1(q0,1, a), for a ∈Σ1, and δ(q0, a′) = δ2(q0,2, a′),
for a′ ∈Σ2. Finally, we add the input symbol ⋄and a rejecting sink state and
direct all transitions that are undeﬁned so far to it. In particular, these are all
⋄-transitions. The result of this construction is a (complete) 2n-state ⋄-DFA M1
accepting the language ˆLR
n ∪˜LR
n , where ˆLR
n and ˜LR
n are essentially LR
n but over
disjoint alphabets.
Now we turn to some ⋄-DFA M2 with ⋄-substitution σ2 such that σ2(L(M2))
is the reversal of σ1(L(M1)) = L(M1). As in the proof of Lemma 3 we can
conclude that either σ2(⋄) ⊆Σ1 or σ2(⋄) ⊆Σ2 (note that L(M1) does not
contain any word of length one).
Both cases can be treated in the same way. So, let us assume that σ2(⋄) ⊆Σ1.
Then, for any word w from L(M2) ∩Σ∗
2 = (ˆLR
n )R = ˆLn we have σ−1
2 (w) =
{w} and, thus, w is accepted by M2. So, we can delete all transitions on ⋄
and on symbols from Σ1 from the transition function of M2 and obtain a DFA
accepting ˆLn. By the result of [22] we conclude that the DFA and, thus, M2 have
at least Ω(ℓ
n
1+log2 ℓ) states.
⊓⊔

180
M. Kutrib and M. Wendlandt
As a ﬁnal operation we consider the concatenation. Here we have the following
more or less straightforward upper bound.
Proposition 14. Let m, n ≥1 and ℓ≥2 be integers, M1 be an m-state
⋄-DFA with ⋄-substitution σ1 accepting a ﬁnite language and M2 be an n-state
⋄-DFA with ⋄-substitution σ2 accepting a ﬁnite language, both over an ℓ-letter
alphabet. Then O(ℓ
m+n−3
1+log2 ℓ) states are suﬃcient for a ⋄-DFA M3 with some ⋄-
substitution σ3 such that σ3(L(M3)) = σ1(L(M1))σ2(L(M2)).
Proof. A ﬁrst step is to extend the given ⋄-DFAs to NFAs by replacing any ⋄-
transition by corresponding transitions on the symbols that may be substituted
for ⋄. Additionally, the rejecting sink states are removed. In this way, we obtain
an (m −1)-state NFA accepting σ1(L(M1)) and an (n −1)-state NFA accepting
σ2(L(M2)). In [17] the upper bound m + n −1 for the nondeterministic con-
catenation of ﬁnite languages has been shown. By applying this construction we
obtain an (m + n −3)-state NFA accepting σ1(L(M1))σ2(L(M2)). Finally, this
NFA is determinized. The outcome is a DFA with at most O(ℓ
m+n−3
1+log2 ℓ) states,
which is clearly also an upper bound for any ⋄-DFA.
⊓⊔
For the lower bound of the concatenation we have the following result.
Theorem 15. Let m, n ≥3 and ℓ≥2 be integers. There exist an m-
state ⋄-DFA M1 with ⋄-substitution σ1 accepting a ﬁnite language over an ℓ-
letter alphabet and an n-state ⋄-DFA M2 with ⋄-substitution σ2 accepting a
ﬁnite language over a disjoint ℓ-letter alphabet such that any ⋄-DFA M3 with
any ⋄-substitution σ3 where σ3(L(M3)) = σ1(L(M1))σ2(L(M2)) has at least
Ω(ℓ
min{m,n}
2·log2 ℓ) states.
5
Hierarchy of ⋄-Transitions
Here we turn to consider the number of productive ⋄-transitions in a ⋄-DFA,
where a transition is called productive, if it does not lead to the rejecting sink
state. Corollary 6 and Theorem 7 provide upper and lower bounds for the ⋄-DFA
to DFA conversion. So, the state costs for removing all ⋄-transitions follow. But
this raises the question for the state costs when only some of the productive
⋄-transitions are removed. Here, we consider the following (k1, k2)-⋄-transition
problem:
– Let k1 > k2 ≥0 be two integers.
– Given an n-state ⋄-DFA M1 with ⋄-substitution σ1 having at most k1 pro-
ductive ⋄-transitions and accepting a ﬁnite language,
– how many states are suﬃcient and necessary in the worst case (in terms of n)
for a ⋄-DFA M2 with some ⋄-substitution σ2 having at most k2 productive
⋄-transitions such that σ2(L(M2)) = σ1(L(M1))?

State Complexity of Finite Partial Languages
181
For general regular languages exponential lower bounds are known [19]. In
particular, the lower bound for the (k1, k1−1)-⋄-transition problem turned out to
be exponential in the order of magnitude. Moreover, for every further productive
⋄-transition that is removed, an exponential number of states is additionally
necessary in the worst case.
Again, the situation is diﬀerent for ﬁnite languages. Here we will show that
the state costs for removing one productive ⋄-transition is quadratic. Therefore,
the (k1, k2)-⋄-transition problem for ﬁnite languages causes only a polynomial
state blow-up. We continue with the upper bound.
Proposition 16. Let n ≥3, ℓ≥2, and k ≥1 be integers, and M1 be an n-state
⋄-DFA with ⋄-substitution σ1 having k productive ⋄-transitions that accepts a
ﬁnite language over an ℓ-letter alphabet. Then O(n2) states are suﬃcient for a
⋄-DFA M2 with ⋄-substitution σ2 having at most k −1 productive ⋄-transitions,
such that σ2(L(M2)) = σ1(L(M1)).
Proof. Let {p0, p1, . . . , pn−1} be the state set of M1, where p0 is the initial state.
We safely may assume that M1 is minimal. Since M1 accepts a ﬁnite language
its state graph does not contain any cycle. Therefore, we can order the states
such that if pj is reachable from pi then i < j. So, we may assume that pn−1 is
the rejecting sink state. Now we determine the state with a maximal index less
or equal to n −2 that has an outgoing ⋄-transition. Say that it is state pi.
We construct M2 from M1 as follows, where δ1 and δ2 denote their transition
functions. We start with M1 and replace the ⋄-transition from state pi by the
transitions δ2(pi, x) = δ1(pi, ⋄), for all x ∈σ1(⋄). The results is an NFA. Finally,
the NFA is determinized to obtain the ⋄-DFA M2 (where σ2 = σ1). Clearly, by
the construction M2 has at most k−1 productive ⋄-transitions and σ2(L(M2)) =
σ1(L(M1)).
In order to determine the number of states of M2 we consider the state pi.
Since the state set was ordered and state pi was the one with the largest index
that has an outgoing ⋄-transition (except for the sink state pn−1), in particular,
all states reachable from pi (except for the sink state pn−1) do not have an
outgoing ⋄-transition. Moreover, the determinization of the constructed NFA
by the powerset construction reveals only single NFA states or pairs of NFA
states as states of the resulting ⋄-DFA M2. We conclude that M2 has at most
n +
n
2

∈O(n2) states.
⊓⊔
The next theorem provides a lower bound that is tight in the order of mag-
nitude. Ingredients of the construction are DFAs that have been constructed
in [15]. These DFAs witness a lower bound of Ω(min{m, n}2) for the determin-
istic state complexity of union of ﬁnite languages.
Theorem 17. Let k ≥1 be a constant integer. Then, for inﬁnitely many n ≥1,
there exists an n-state ⋄-DFA M1 with ⋄-substitution σ1 having k productive ⋄-
transitions, that accepts a ﬁnite language over the alphabet {⋄, a, b, #, $}, such
that any ⋄-DFA M2 with ⋄-substitution σ2 having at most k −1 productive ⋄-
transitions and σ2(L(M2)) = σ1(L(M1)) has at least Ω(n2) states.

182
M. Kutrib and M. Wendlandt
6
Conclusion
In this paper, we have studied some aspects of state complexity of partial
word ﬁnite automata accepting ﬁnite languages. In particular, we considered
the ‘determinization’. Since the ⋄-substitutions imply a certain form of limited
nondeterminism we looked at the two problems to convert an NFA to a ⋄-DFA
as well as to convert a ⋄-DFA to a DFA. For both conversions the upper bound
is of order O(ℓ
n
1+log2 ℓ). It turned out that a lower bound for the ⋄-DFA to DFA
conversion is of order O(ℓ
n
2 log2 ℓ). For the NFA to ⋄-DFA conversion we could
show the lower bound of O(ℓ
n
1+log2 ℓ).
However, the lower bound for the NFA to ⋄-DFA conversion is not tight
for ﬁnite languages. The reason is that the alphabet size does matter for ﬁnite
languages and that the ﬁnite language which witnesses the lower bound is over
a 2ℓ-letter alphabet. So, a converted lower bound is Ω(( ℓ
2)
n
log2 ℓ). Moreover, the
lower bound for the ⋄-DFA to DFA conversion, that is O(ℓ
n
2 log2 ℓ), has been
presented in a form that incorporates the alphabet size. The reason is that
the alphabet size matters for ﬁnite languages and we wish to have a better
comparison with other bounds. In fact, we have O(ℓ
n
2 log2 ℓ) = O(2n/2). This
presentation has been chosen at several places. Anyway, the state costs for both
conversions are exponential.
Then we considered the operational state complexity with respect to comple-
mentation, union, reversal, and concatenation of ﬁnite languages. It turns out
that the upper and lower bounds for all these operations are exponential. How-
ever, upper and lower bounds do not match. It would be interesting to know
whether the upper and/or lower bounds can be improved. Moreover, there are
several operations for which the operational state complexity is still untouched,
and it would clearly be of interest to determine upper and lower bounds for these
operations as well.
Finally, we considered the impact of the number of productive ⋄-transitions
in a partial word ﬁnite automaton. It came out that the state costs for removing
one productive ⋄-transition is quadratic. This bound is tight in the order of
magnitude. So, removing any constant number of ⋄-transitions from a ⋄-DFA
accepting a ﬁnite language causes only a polynomial state blow-up.
References
1. Balkanski, E., Blanchet-Sadri, F., Kilgore, M., Wyatt, B.J.: On the state complex-
ity of partial word DFAs. Theory Comput. Sci. 578, 2–12 (2015)
2. Berstel, J., Boasson, L.: Partial words and a theorem of Fine and Wilf. Theory
Comput. Sci. 218, 135–141 (1999)
3. Birget, J.C.: Intersection and union of regular languages and state complexity.
Inform. Process. Lett. 43, 185–190 (1992)
4. Blanchet-Sadri, F.: Algorithmic Combinatorics on Partial Words. CRC Press, Dis-
crete mathematics and its applications (2008)
5. Blanchet-Sadri, F., Goldner, K., Shackleton, A.: Minimal partial languages and
automata. RAIRO Inform. Th´eor. 51, 99–119 (2017)

State Complexity of Finite Partial Languages
183
6. Bryant, R.E.: Graph-based algorithms for Boolean function manipulation. IEEE
Trans. Comput. 35, 677–691 (1986)
7. Cˆampeanu, C., Culik, K., Salomaa, K., Yu, S.: State complexity of basic operations
on ﬁnite languages. In: Boldt, O., J¨urgensen, H. (eds.) WIA 1999. LNCS, vol. 2214,
pp. 60–70. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-45526-4 6
8. Cˆampeanu, C., Ho, W.H.: The maximum state complexity for ﬁnite languages. J.
Autom. Lang. Comb. 9, 189–202 (2004)
9. Cˆampeanu, C., Santean, N., Yu, S.: Minimal cover-automata for ﬁnite languages.
Theory Comput. Sci. 267, 3–16 (2001)
10. Champarnaud, J., Pin, J.: A maxmin problem on ﬁnite automata. Discrete Appl.
Math. 23, 91–96 (1989)
11. Dassow, J., Manea, F., Merca¸s, R.: Regular languages of partial words. Inf. Sci.
268, 290–304 (2014)
12. Fischer, M.J., Paterson, M.S.: String-matching and other products. In: Complexity
of Computation. SIAM-AMS Proceedings, vol. 7, pp. 113–125. AMS (1974)
13. Gao, Y., Moreira, N., Reis, R., Yu, S.: A survey on operational state complexity.
J. Autom. Lang. Comb. 21, 251–310 (2016)
14. Glaister, I., Shallit, J.: A lower bound technique for the size of nondeterministic
ﬁnite automata. Inform. Process. Lett. 59, 75–77 (1996)
15. Han, Y.S., Salomaa, K.: State complexity of union and intersection of ﬁnite lan-
guages. Int. J. Found. Comput. Sci. 19, 581–595 (2008)
16. Holzer, M., Jakobi, S., Wendlandt, M.: On the computational complexity of partial
word automata problems. Fund. Inform. 148, 267–289 (2016)
17. Holzer, M., Kutrib, M.: Nondeterministic descriptional complexity of regular lan-
guages. Int. J. Found. Comput. Sci. 14, 1087–1102 (2003)
18. Holzer, M., Kutrib, M.: Nondeterministic ﬁnite automata-recent results on the
descriptional and computational complexity. Int. J. Found. Comput. Sci. 20, 563–
580 (2009)
19. Kutrib, M., Wendlandt, M.: State complexity of partial word ﬁnite automata. In:
Han, Y.S., Ko, S.K. (eds) DCFS 2021. LNCS, vol. 13037. pp. 113–124. Springer,
Cham (2021). https://doi.org/10.1007/978-3-030-93489-7 10
20. Mandl, R.: Precise bounds associated with the subset construction on various
classes of nondeterministic ﬁnite automata. In: Princeton Conference on Infor-
mation Sciences and Systems (CISS 1973), pp. 263–267 (1973)
21. Revuz, D.: Minimisation of acyclic deterministic automata in linear time. Theory
Comput. Sci. 92, 181–189 (1992)
22. Salomaa, K., Yu, S.: NFA to DFA transformation for ﬁnite languages over arbitrary
alphabets. J. Autom. Lang. Comb. 2, 177–186 (1997)

Yet Another Canonical Nondeterministic
Automaton
Hendrik Maarand(B) and Hellis Tamm
Department of Software Science, Tallinn University of Technology, Tallinn, Estonia
{hendrik,hellis}@cs.ioc.ee
Abstract. Several canonical forms of ﬁnite automata have been intro-
duced over the decades. In particular, if one considers the minimal
deterministic ﬁnite automaton (DFA), the canonical residual ﬁnite state
automaton (RFSA), and the ´atomaton of a language, then the ´atomaton
can be seen as the dual automaton of the minimal DFA, but no such
dual has been presented for the canonical RFSA so far. We ﬁll this gap
by introducing a new canonical automaton that we call the maximized
prime ´atomaton, and study its properties. We also describe how these
four automata can be extracted from suitable observation tables used in
the automata learning context.
Keywords: Canonical automaton · regular language · atoms of
regular languages · automata learning
1
Introduction
It is well known that every regular language has a unique minimal deterministic
ﬁnite automaton (DFA) accepting the language. However, this nice property
does not hold for the class of nondeterministic ﬁnite automata (NFAs), because
a language may have several non-isomorphic NFAs with a minimum number
of states. Nevertheless, several canonical forms of NFAs have been introduced
over the decades: the universal automaton [9], the canonical residual ﬁnite state
automaton (canonical RFSA) [6] (also known as jiromaton [10]), the ´atomaton
[5], and the maximized ´atomaton [12] (same as distromaton [10]). We note that
none of these NFAs are necessarily minimal NFAs.
While the states of the minimal DFA of a language L correspond to the (left)
quotients of L, the canonical RFSA of L may have less states, since it is based
on the prime quotients [6] of L, that is, non-empty quotients that are not unions
of other quotients. The states of the ´atomaton of L correspond to the atoms [5]
of L, which are non-empty intersections of complemented and uncomplemented
quotients. Also, the notion of a prime atom was deﬁned in [14], however, no
automaton based on prime atoms has been presented so far.
This work was supported by the Estonian Research Council grant PRG1210.
H. Maarand was also supported by the ERDF funded Estonian CoE project EXCITE
(project 2014-2020.4.01.15-0018).
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 184–196, 2022.
https://doi.org/10.1007/978-3-031-13257-5_14

Yet Another Canonical Nondeterministic Automaton
185
We ﬁll this gap by introducing a new canonical NFA that we call the maxi-
mized prime ´atomaton, because it is a subautomaton of the maximized ´atomaton
and its states correspond to the prime atoms of a language. While the ´atomaton
of L is isomorphic to the reverse NFA of the minimal DFA of LR [5], we show that
the maximized prime ´atomaton of L is the reverse of the canonical RFSA of LR.
An informal description of the relationship between these automata is presented
in the picture below. By applying saturation and reduction operations [6] to the
minimal DFA, the canonical RFSA is obtained. By applying corresponding dual
operations to the ´atomaton, we get the maximized prime ´atomaton.
minimal DFA
sat - red


rev
 ´atomaton
dual sat - dual red

canonical RFSA 
rev
 maximized prime ´atomaton
Another way to construct a canonical RFSA is by using a modiﬁed subset
construction operation C [6,12]. We deﬁne a dual operation of C and show how
to use this operation to obtain the maximized prime ´atomaton.
We also describe how the four automata in the above picture can be extracted
from suitable observation tables used in the automata learning context [1]. If an
observation table is closed and consistent both for rows and columns (Deﬁni-
tion 7), then its proper part forms the quotient-atom matrix [8,13] of the lan-
guage. We believe that it can be helpful to think of these automata in terms of
such matrices where the row and column indices are the right and left congruence
classes of the language, respectively.
2
Automata, Quotients, and Atoms of Regular Languages
A nondeterministic ﬁnite automaton (NFA) is a quintuple N = (Q, Σ, δ, I, F),
where Q is a ﬁnite, non-empty set of states, Σ is a ﬁnite non-empty alphabet,
δ : Q × Σ →2Q is the transition function, I ⊆Q is the set of initial states, and
F ⊆Q is the set of ﬁnal states. We extend the transition function to functions
δ′ : Q × Σ∗→2Q and δ′′ : 2Q × Σ∗→2Q, using δ for all these functions. The
left language of a state q of N is LI,q(N) = {w ∈Σ∗| q ∈δ(I, w)}, and the
right language of q is Lq,F (N) = {w ∈Σ∗| δ(q, w) ∩F ̸= ∅}. A state q of N is
reachable if LI,q(N) ̸= ∅, and it is empty if Lq,F (N) = ∅. The language accepted
by an NFA N is L(N) = {w ∈Σ∗| δ(I, w)∩F ̸= ∅}. Two NFAs are equivalent if
they accept the same language. An NFA is minimal if it has a minimum number
of states among all equivalent NFAs. The reverse of an NFA N = (Q, Σ, δ, I, F)
is the NFA N R = (Q, Σ, δR, F, I), where q ∈δR(p, a) if and only if p ∈δ(q, a)
for p, q ∈Q and a ∈Σ.
A deterministic ﬁnite automaton (DFA) is a quintuple D = (Q, Σ, δ, q0, F),
where Q, Σ, and F are as in an NFA, δ : Q × Σ →Q is the transition function,
and q0 is the initial state. The left quotient, or simply quotient, of a language
L by a word w ∈Σ∗is the language w−1L = {x ∈Σ∗| wx ∈L}. It is well
known that the left quotients of L are the right languages of the states of the

186
H. Maarand and H. Tamm
minimal DFA of L. Any NFA N can be determinized by the well-known subset
construction, yielding a DFA N D that has only reachable states.
Let L be a non-empty regular language with quotients K0, . . . , Kn−1. An
atom of L is any non-empty language of the form 
K0 ∩· · · ∩
Kn−1, where 
Ki
is either Ki or Ki, and Ki is the complement of Ki with respect to Σ∗[5]. An
atom is initial if it has L (rather than L) as a term; it is ﬁnal if it contains ε.
There is exactly one ﬁnal atom, the atom 
K0 ∩· · · ∩
Kn−1, where 
Ki = Ki
if ε ∈Ki, and 
Ki = Ki otherwise. If K0 ∩· · · ∩Kn−1 is an atom, then it is
called the negative atom, all the other atoms are positive. Thus atoms of L are
pairwise disjoint languages uniquely determined by L; they deﬁne a partition
of Σ∗. Every quotient Ki (including L) is a (possibly empty) union of atoms.
An NFA N is atomic if the right languages of its states are unions of atoms of
L(N).
It is well known that quotients of L are in a one-one correspondence with
the equivalence classes of the Nerode right congruence ≡L of L [11] deﬁned as
follows: for x, y ∈Σ∗, x ≡L y if for every v ∈Σ∗, xv ∈L if and only if yv ∈L.
Atoms of L are the classes of the left congruence L≡of L: for x, y ∈Σ∗, x L≡y
if for every u ∈Σ∗, ux ∈L if and only if uy ∈L [7].
Let A = {A0, A1, . . . , Am−1} be the set of atoms of L, let AI be the set of
initial atoms, and let Am−1 be the ﬁnal atom.
The ´atomaton of L is the NFA A = (SA, Σ, α, IA, {sm−1}) where SA =
{s0, s1, . . . , sm−1}, IA = {si ∈SA | Ai ∈AI}, and sj ∈α(si, a) if and only if
Aj ⊆a−1Ai, for all i, j ∈{0, . . . , m −1} and a ∈Σ. It was shown in [5] that the
atoms of L are the right languages of the states of the ´atomaton, and that the
reverse NFA of the ´atomaton is the minimal DFA of the reverse language LR.
The next theorem is a slightly modiﬁed version of the result by Brzozowski [4]:
Theorem 1. If an NFA N has no empty states and N R is deterministic, then
N D is minimal.
By Theorem 1, for any NFA N, the DFA N RDRD is the minimal DFA equiv-
alent to N. This result is known as Brzozowski’s double-reversal method for
DFA minimization. In [5], a generalization of Theorem 1 was presented, provid-
ing a characterization of the class of NFAs for which applying determinization
procedure produces a minimal DFA:
Theorem 2. For any NFA N, the DFA N D is minimal if and only if N R is
atomic.
3
Residual Finite State Automata
Residual ﬁnite state automata (RFSAs) were introduced by Denis, Lemay, and
Terlutte in [6]. In this section, we state some basic properties of RFSAs. However,
we note here that we usually prefer to use the term “quotient” over “residual”.
An NFA N = (Q, Σ, δ, I, F) is a residual ﬁnite state automaton (RFSA) if
for every state q ∈Q, Lq,F (N) is a quotient of L(N). Clearly, any DFA having
only reachable states, is an RFSA.

Yet Another Canonical Nondeterministic Automaton
187
Let L be a regular language over Σ. A non-empty quotient of L is prime if it
is not a union of other quotients. Let K′ = {K0, . . . , Kn′−1} be the set of prime
quotients of L.
The canonical RFSA of L is the NFA R = (QK′, Σ, δ, IK′, FK′), where QK′ =
{q0, . . . , qn′−1}, IK′ = {qi ∈QK′ | Ki ⊆L}, FK′ = {qi ∈QK′ | ε ∈Ki}, and
δ(qi, a) = {qj ∈QK′ | Kj ⊆a−1Ki} for every qi ∈QK′ and a ∈Σ.
The canonical RFSA is a state-minimal RFSA with a maximal number of
transitions. One way to build a canonical RFSA is to use the saturation and
reduction operations deﬁned in the following.
Let N = (Q, Σ, δ, I, F) be an NFA. The saturation operation S, applied
to N, produces the NFA N S = (Q, Σ, δS, IS, F), where δS(q, a) = {q′ ∈Q |
aLq′,F (N) ⊆Lq,F (N)} for all q ∈Q and a ∈Σ, and IS = {q ∈Q | Lq,F (N) ⊆
L(N)}. An NFA N is saturated if N S = N. Saturation may add transitions and
initial states to an NFA, without changing its language. If N is an RFSA, then
N S is an RFSA.
For any state q of N, let R(q) be the set {q′ ∈Q\{q} | Lq′,F (N) ⊆Lq,F (N)}.
A state q is erasable if Lq,F (N) = 
q′∈R(q) Lq′,F (N). If q is erasable, a reduction
operator φ is deﬁned as follows: φ(N, q) = (Q′, Σ, δ′, I′, F ′) where Q′ = Q\{q},
I′ = I if q /∈I, and I′ = (I\{q})∪R(q) otherwise, F ′ = F ∩Q′, δ′(q′, a) = δ(q′, a)
if q /∈δ(q′, a), and δ′(q′, a) = (δ(q′, a)\{q}) ∪R(q) otherwise, for every q′ ∈Q′
and every a ∈Σ. If q is not erasable, let φ(N, q) = N.
If N is saturated and if q is an erasable state of N, then φ(N, q) is obtained
by deleting q and its associated transitions from N. An NFA N is reduced if
there is no erasable state in N. Applying φ to N does not change its language.
If N is an RFSA, then φ(N, q) is an RFSA. The following proposition is from [6]:
Proposition 1. If an NFA N is a reduced saturated RFSA of L, then N is the
canonical RFSA for L.
The canonical RFSA can be obtained from a DFA having only reachable
states, by using saturation and reduction operations.
Next we will discuss another method to compute the canonical RFSA, sug-
gested in [6]. In Sect. 2, we recalled the result that for any NFA N, the DFA
N RDRD is the minimal DFA equivalent to N. In [6], a similar double-reversal
method is proposed to obtain a canonical RFSA from a given NFA, using a
modiﬁed subset construction operation C to be applied to an NFA as follows:
Deﬁnition 1. Let N = (Q, Σ, δ, I, F) be an NFA. Let QD be the set of states
of the determinized version N D of N. A state s ∈QD is coverable if there
is a set Qs ⊆QD\{s} such that s = 
s′∈Qs s′. The NFA N C is deﬁned as
(QC, Σ, δC, IC, FC), where QC = {s ∈QD | s is not coverable }, IC = {s ∈QC |
s ⊆I}, FC = {s ∈QC | s ∩F ̸= ∅}, and δC(s, a) = {s′ ∈QC | s′ ⊆δ(s, a)} for
any s ∈QC and a ∈Σ.
Applying the operation C to any NFA N produces an RFSA N C. Denis et
al. [6] have the following result:

188
H. Maarand and H. Tamm
Theorem 3. If an NFA N has no empty states and N R is an RFSA, then N C
is the canonical RFSA.
By Theorem 3, for any NFA N, the RFSA N RCRC is the canonical RFSA
equivalent to N. Hence, the operation C has a similar role for RFSAs as deter-
minization has for DFAs.
In Sect. 2, we recalled Theorem 2 from [5], a generalization of Theorem 1,
characterizing the class of NFAs to which applying the determinization procedure
produces a minimal DFA. Theorem 3 was generalized in [12] in a similar way:
Theorem 4. For any NFA N of L, N C is a canonical RFSA if and only if the
left languages of N are unions of left languages of the canonical RFSA of L.
4
Maximized ´Atomaton
Let L be a non-empty regular language, K = {K0, . . . , Kn−1} be the set of
quotients, and A = {A0, . . . , Am−1} be the set of atoms of L, with the set of
initial atoms AI ⊆A, and the ﬁnal atom Am−1.
In [12], the notions of a maximized atom and the maximized ´atomaton of a
regular language L were introduced. For every atom Ai of L, the corresponding
maximized atom Mi is the union of all the atoms which occur in every quotient
containing Ai:
Deﬁnition 2. The maximized atom Mi of an atom Ai is the union of atoms
Mi = {Ah | Ah ⊆
Ai⊆Kk Kk}.
Clearly, since atoms are pairwise disjoint, and every quotient is a union of
atoms, Mi = 
Ai⊆Kk Kk. In [12], the following properties of maximized atoms
were shown:
Proposition 2. Let Ai and Aj be some atoms of L. The following properties
hold:
1. Ai ⊆Mi.
2. If Ai ̸= Aj, then Mi ̸= Mj.
3. Ai ⊆Mj if and only if Mi ⊆Mj.
4. Aj ⊆a−1Mi if and only if Mj ⊆a−1Mi.
Let M = {M0, . . . , Mm−1} be the set of the maximized atoms of L. The
maximized ´atomaton was deﬁned in [12] as follows:
Deﬁnition 3. The maximized ´atomaton of L is the NFA deﬁned as M =
(QM, Σ, μ, IM, FM), where QM = {q0, q1, . . . , qm−1}, IM = {qi ∈QM | Ai ∈
AI}, FM = {qi ∈QM | Am−1 ⊆Mi}, and qj ∈μ(qi, a) if and only if
Mj ⊆a−1Mi, for all i, j ∈{0, . . . , m −1} and a ∈Σ.
It was shown in [12] that the maximized ´atomaton M of L is isomorphic to
the reverse NFA of the saturated version of the minimal DFA of LR.
Using results from [13] and Proposition 2, we can see that the right language
of any state of the maximized ´atomaton is the corresponding maximized atom:
Proposition 3. For every state qi ∈QM of the maximized ´atomaton M =
(QM, Σ, μ, IM, FM) of L, the equality Lqi,FM (M) = Mi holds.

Yet Another Canonical Nondeterministic Automaton
189
5
Maximized Prime ´Atomaton
We recall that a non-empty quotient is prime if it is not a union of other quo-
tients.
The notion of a prime atom was deﬁned in [14] as follows: any positive atom
Ai = 
j∈Si Kj ∩
j∈Si Kj, where Si ⊆{0, . . . , n−1} and Si = {0, . . . , n−1}\Si,
is prime if the set {Kj | j ∈Si} of uncomplemented quotients in the intersection
of Ai is not a union of such sets of quotients corresponding to other atoms.
By results in [5], it is known that the reverse of the ´atomaton A of L is the
minimal DFA of LR. Since the right language of any state of A is some atom
of L, and the right language of any state of AR is some quotient of LR, there
is a natural one-one-correspondence between the set of atoms of L and the set
of quotients of LR, based on the state set of A (and AR). Also, there is a one-
one correspondence between the set of prime atoms of L and the set of prime
quotients of LR:
Proposition 4. The right language of any state of the ´atomaton A of L is a
prime atom of L if and only if the right language of the same state of AR is a
prime quotient of LR.
Now, let A′ ⊆A be the set of prime atoms of L, and let M ′ ⊆M be the
corresponding set of maximized prime atoms. We deﬁne the maximized prime
´atomaton of L as follows:
Deﬁnition 4. The maximized prime ´atomaton of L is the NFA deﬁned by M′ =
(QM ′, Σ, μ, IM ′, FM ′), where QM ′ = {qi | Mi is prime}, IM ′ = QM ′ ∩IM,
FM ′ = QM ′ ∩FM, and qj ∈μ(qi, a) if and only if Mj ⊆a−1Mi, for qi, qj ∈QM ′
and a ∈Σ.
In [12], it was shown that the maximized ´atomaton M of L is isomorphic
to ESR, where E is the minimal DFA of LR. That is, MR is isomorphic to ES.
Now, the canonical RFSA of LR is the reduced version of ES, where those states
of ES corresponding to non-prime quotients of LR, have been removed. Since by
Proposition 4, the states of E corresponding to prime quotients of LR are exactly
those states of ER corresponding to prime atoms of L, the canonical RFSA of LR
is isomorphic to the subautomaton of MR, where the states corresponding to
non-prime atoms, together with their in- and out-transitions, have been removed.
We have the following result:
Proposition 5. The maximized prime ´atomaton M′ of L is isomorphic to the
reverse NFA of the canonical RFSA of LR.
There is a one-one correspondence between the set M ′ of maximized prime
atoms and the state set QM ′ of the maximized prime ´atomaton of L. However,

190
H. Maarand and H. Tamm
the right language of a state qi of M′ is not necessarily equal to the corresponding
maximized prime atom Mi. By a result in [12], for the left language Li of a state
qi of the canonical RFSA of LR, the inclusions AR
i ⊆Li ⊆M R
i
hold, where Ai
and Mi are respectively the corresponding atom and the maximized atom of L.
Since the right language of any state of the maximized prime ´atomaton of L is
the reverse of the left language of the corresponding state of the canonical RFSA
of LR, we can state the following:
Proposition 6. For any state qi of the maximized prime ´atomaton M′ of L,
the inclusions Ai ⊆Lqi,FM′(M′) ⊆Mi hold.
By Proposition 5, we are able to obtain the maximized prime ´atomaton of L
by ﬁnding the canonical RFSA of LR, and then reversing it. Since by Theorem 3,
for any NFA N, the RFSA N RCRC is the canonical RFSA equivalent to N, it
is clear that N CRCR is the maximized prime ´atomaton of L.
We deﬁne an operation coC to be applied to an NFA as follows:
Deﬁnition 5. Let N = (Q, Σ, δ, I, F) be an NFA. Let QcoD be the set of
states of the determinized version N RD of N R. A state s ∈QcoD is cov-
erable if there is a set Qs ⊆QcoD\{s} such that s = 
s′∈Qs s′. The NFA
N coC = (QcoC, Σ, δcoC, IcoC, FcoC) is deﬁned as follows: QcoC = {s ∈QcoD | s
is not coverable}, IcoC = {s ∈QcoC | s ∩I ̸= ∅}, FcoC = {s ∈QcoC | s ⊆F},
and for any s, s′ ∈QC and a ∈Σ, s′ ∈δcoC(s, a) if and only if for every q ∈s
there is some q′ ∈s′ such that q′ ∈δ(q, a).
Clearly, N coC is isomorphic to N RCR. Hence, given any NFA N of L, the
maximized prime ´atomaton of L can be obtained by applying ﬁrst the operation
C to N, yielding N C, and then applying coC to N C, resulting in the automaton
N C(coC). Also, the NFA N (coC)C is the canonical RFSA of L. The following
theorem holds:
Theorem 5. For any NFA N of L, the NFA N coC is the maximized prime
´atomaton of L if and only if the right language of every state of N is a union of
right languages of the maximized prime ´atomaton of L.
Example 1. We consider a modiﬁcation of an example from [6], and deﬁne a
family of NFAs Bn = (Q, Σ, δ, I, F), n ⩾1, where Q = {q0, . . . , qn−1}, Σ =
{a, b}, I = {qi | 0 ⩽i < n/2}, F = {q0}, and δ(qi, a) = {q(i+1) mod n} for
i = 0, . . . , n −1, and δ(q0, b) = {q0, q1}, δ(q1, b) = {qn−1}, and δ(qi, b) = {qi−1}
for 1 < i < n. The NFA B4 is shown in Fig. 1 and its reverse BR
4 is in Fig. 2.
We claim that the NFA BR
n is a canonical RFSA of L(Bn)R. Indeed, BR
n is an
RFSA, because the right languages of BR
n are quotients of L(BR
n ): Lq0,F (BR
n ) =
ε−1L(BR
n ) and Lqi,F (BR
n ) = (an−i)−1L(BR
n ), for i = 1, . . . , n −1. Denoting Ki =
(a(n−i) mod n)−1L(BR
n ) and noticing that a(i−⌈n/2⌉+1) mod n, . . . , ai mod n ∈Ki,
and a(i+1) mod n, . . . , a(i+⌊n/2⌋) mod n /∈Ki, for i = 0, . . . , n −1, it is easy to see
that Ki’s are pairwise incomparable. Therefore, BRC
n
is isomorphic to BR
n , and
it is clear that BR
n is a canonical RFSA of L(Bn)R.

Yet Another Canonical Nondeterministic Automaton
191
Hence, by Proposition 5, Bn is the maximized prime ´atomaton of L(Bn).
Also, by Theorem 3, BC
n is the canonical RFSA of L(Bn). The automaton BC
n
has

n
⌈n/2⌉

states, because any candidate state of BC
n with more than ⌈n/2⌉
elements can be covered by those with exactly ⌈n/2⌉elements. Thus, for n ⩾4,
Bn is smaller than the canonical RFSA for L(Bn), and the diﬀerence between
the sizes of these two NFAs grows with n. Moreover, Bn is a minimal NFA
for L(Bn), as can be seen by the fooling set method [2] using the fooling set
{(ε, an), (a, an−1), . . . , (an−1, a)} of size n.
Fig. 1. The automaton B4.
Fig. 2. The automaton BR
4 .
6
Observation Tables
We now turn to observation tables known from the L∗learning algorithm [1] and
how to read out various canonical automata from suitable observation tables.
These tables can be seen as submatrices of the quotient-atom matrix [13] of
a language, which is used, for example, in ﬁnding a minimal NFA of the lan-
guage [8,13].
The L∗algorithm works by performing membership (whether a word belongs
to the unknown language) and equivalence (whether a hypothesis is equivalent
to the unknown language) queries. Informally, an observation table is used in
the L∗algorithm to collect the observations that have been made so far and also
to organize the observations in such a manner that it can be determined which
observations need to be performed next. The membership queries are always
performed for words composed from a preﬁx s and a suﬃx e. If the result of the
membership query for the word se is positive, then the entry in the table at row
s and column e is set to 1, otherwise it is set to 0.
Deﬁnition 6. An observation table is a triple T = (S, E, T) where S ⊆Σ∗
is a preﬁx-closed set of words, E ⊆Σ∗is a suﬃx-closed set of words and T :
Σ∗→2 is a ﬁnite function. The proper part of the table consists of S rows and

192
H. Maarand and H. Tamm
E columns. The row extensions of the table consist of the rows S · Σ\S. The
column extensions of the table consist of the columns Σ · E\E. The entry in the
table at row s and column e is T(se).
A row of T = (S, E, T) is an E-indexed vector consisting of the corresponding
entries of the table. That is, for s ∈S and e ∈E, row(s)(e) = T(se). A column
of T is an S-indexed vector. That is, for e ∈E and s ∈S, col(e)(s) = T(se).
Note that row(sa)(e) = row(s)(ae) = col(ae)(s) = col(e)(sa).
Deﬁnition 7. An observation table T = (S, E, T) is called
– row-closed when, for every s ∈(S · Σ)\S, there exists s′ ∈S such that
row(s) = row(s′);
– column-closed when, for every e ∈(Σ · E)\E, there exists e′ ∈E such that
col(e) = col(e′);
– row-consistent when, for every s, s′ ∈S, if row(s) = row(s′), then, for every
a ∈Σ, row(sa) = row(s′a).
– column-consistent when, for every e, e′ ∈E, if col(e) = col(e′), then, for
every a ∈Σ, col(ae) = col(ae′).
Note that what are called closed and consistent in [1] are respectively called
row-closed and row-consistent in our setting.
We also use row(S) to denote the set {row(s) | s ∈S} and col(E) for
{col(e) | e ∈E}. Two indices s1 and s2 are equivalent when row(s1) = row(s2).
This partitions S and we write [s] for the equivalence class of s as well as its rep-
resentative. Similarly, we have an equivalence relation on E and we write [e] for
the equivalence class of e and its representative. We can use the lexicographically
minimal element as the representative.
6.1
Row Automaton
Let T = (S, E, T) be a row-closed and row-consistent observation table. Deﬁne
a function suc : row(S) × Σ →row(S) as suc(r, a) = row([r]a). The co-domain
is row(S) as for any r ∈row(S), we have [r] ∈S and by being row-closed, there
is an s ∈S such that row([r]a) = row(s). Since the table is consistent, this
function respects the equivalence classes.
Deﬁnition 8. The row automaton of T , denoted by Arow(T ), is the automaton
(Q, Σ, δ, q0, F) where Q = row(S), δ(q, a) = suc(q, a), q0 = row(ε) and F =
{q ∈Q | q(ε) = 1}. The transition function δ extends to words by δ(q, ε) = q
and δ(q, ua) = δ(δ(q, u), a). The language of the automaton is L(Arow(T )) =
{u ∈Σ∗| δ(q0, u) ∈F}.
Proposition 7. If T is row-closed and row-consistent, then Arow(T ) is the min-
imal DFA accepting L(Arow(T )).
Since Arow(T ) is minimal, the left language of a state row(s) is a right
congruence class of L(Arow(T )) and we denote it by [s]row. Furthermore, this
congruence class contains the equivalence class [s] of S. This is the automaton
constructed by the L∗algorithm.

Yet Another Canonical Nondeterministic Automaton
193
6.2
Column Automaton
Let T = (S, E, T) be a column-closed and column-consistent observation table.
Deﬁne a function pre : Σ × col(E) →col(E) as pre(a, c) = col(a[c]). The co-
domain is col(E) as for any c ∈col(E), we have [c] ∈E and by being column-
closed, there is an e ∈E such that col(a[c]) = col(e). Since the table is consistent,
this function respects the equivalence classes.
Deﬁnition 9. The column automaton of T , denoted by Acol(T ), is the automa-
ton (Q, Σ, δ, I, f) where Q = col(E), δ(q, a) = {q′ ∈Q | q = pre(a, q′)},
I = {q ∈Q | q(ε) = 1} and f = col(ε). The transition function extends to
sets of states and words in the usual way: δ(K, a) = {δ(k, a) | k ∈K} and
δ(K, ε) = K and δ(K, ua) = δ(δ(K, u), a). The language of the automaton is
L(Acol(T )) = {u ∈Σ∗| f ∈δ(I, u)}.
Proposition 8. If T is column-closed and column-consistent, then Acol(T ) is
the ´atomaton of L(Acol(T )).
Since Acol(T ) is the ´atomaton, the right language of a state col(e) is an
atom and thus a left congruence class of L(Acol(T )) which we denote by [e]col.
Furthermore, this congruence class contains the equivalence class [e] of E. This
automaton can be learned by a column-oriented variant of L∗. Recall that the
reverse of the ´atomaton is the minimal DFA of the reverse language.
6.3
Rows and Columns
Let T = (S, E, T) be an observation table that is closed and consistent both for
rows and columns. We have Arow(T ) and Acol(T ) associated with T .
Proposition 9. For any u, v ∈Σ∗, we have uv ∈L(Arow(T )) if and only if
row([u]row)([v]col) = 1.
We thus see that the right language of the state of the row automaton corre-
sponding to u (that is row([u]row)) consists of those words v for which the entry
at row [u]row and column [v]col is 1.
Proposition 10. For any u, v ∈Σ∗, we have uv ∈L(Acol(T )) if and only if
col([v]col)([u]row) = 1.
Similarly, we see that the left language of the state of the column automaton
corresponding to v (that is col([v]col)) consists of those words u for which the
entry at column [v]col and row [u]row is 1. Since row(s)(e) = col(e)(s), we can
state the following:
Proposition 11. For any observation table T that is closed and consistent both
for rows and columns, the equality L(Arow(T )) = L(Acol(T )) holds.

194
H. Maarand and H. Tamm
6.4
Primes
Rows and columns are vectors of Booleans. We partially order such vectors by
extending the order 0 ⩽1 to vectors as the product order. For any s, s′ ∈S, we
say row(s) ⩽row(s′) when, for every e ∈E, row(s)(e) ⩽row(s′)(e). The join
of two rows is given pointwise: (row(s) ∨row(s′))(e) = row(s)(e) ∨row(s′)(e).
Column vectors are treated similarly.
We say that a vector v is covered by {v1, . . . , vn} when v = v1 ∨. . . ∨vn. We
say that a vector v is prime wrt. a set of vectors V = {v1, . . . , vn} if v is not zero
and no subset V ′ ⊆V covers v. The set of prime vectors of a set V , denoted
by primes(V ), consists of those v ∈V that are prime wrt. V \v. Every v ∈V is
covered by the vectors below it in primes(V ). The primes are also referred to as
the join-irreducible elements [10].
6.5
Prime Row Automaton
From the prime rows of an observation table we can construct an NFA that
accepts the same language as the row automaton.
Deﬁnition 10. Let T
= (S, E, T) be closed and consistent for rows and
columns. The prime row automaton of T , denoted by Arow ′(T ), is the automaton
given by (Q, Σ, Δ, I, F) where Q = primes(row(S)), I = {q ∈Q | q ⩽row(ε)},
F = {q ∈Q | q(ε) = 1}, Δ(q, a) = {q′ ∈Q | q′ ⩽δ(q, a)} and δ is the transition
function of Arow(T ).
Recall that the right language of a state row(s) in the Arow(T ) consists of
those left congruence classes (atoms) for which the corresponding entry in the
vector row(s) is 1. Thus a prime row corresponds to a state whose right language
is prime, i.e., it is not a union of right languages of other states. Furthermore,
the right language of a state row(s) in Arow ′(T ) is the same as in Arow(T ).
Proposition 12. If T is closed and consistent for rows and columns, then
Arow ′(T ) is the canonical RFSA of L(Arow(T )).
The canonical RFSA can be learned with the NL∗algorithm [3] which, how-
ever, has diﬀerent conditions on consistency and closedness of the table than the
construction given here.
6.6
Prime Column Automaton
From the prime columns of an observation table we can construct an NFA that
accepts the same language as the column automaton.
Deﬁnition 11. Let T
= (S, E, T) be closed and consistent for rows and
columns. The prime column automaton of T , denoted by Acol′(T ), is the
automaton given by (Q, Σ, Δ, I, F) where Q = primes(col(E)), I = {q ∈Q |
q(ε) = 1}, F = {q ∈Q | q ⩽col(ε)}, Δ(q, a) = {q′ | ∃q′′. q′ ∈δ(q′′, a) ∧q ⩽q′′}
and δ is the transition function of Acol(T ).

Yet Another Canonical Nondeterministic Automaton
195
Recall that the left language of a state col(e) in the column automaton con-
sists of those right congruence classes for which the corresponding entry in the
vector col(e) is 1. Thus, a prime column corresponds to a state whose left lan-
guage is prime, i.e., it is not a union of left languages of other states. Furthermore,
the left language of a state col(e) in Acol′(T ) is the same as in Acol(T ).
Proposition 13. If T is closed and consistent for rows and columns, then
Acol′(T ) is the maximized prime ´atomaton of L(Acol(T )).
The maximized prime ´atomaton can be learned with a column-oriented vari-
ant of NL∗, but, again, the conditions on consistency and closedness of the table
would be diﬀerent than the construction given here.
6.7
Learning NFAs
An observation table that is closed and consistent for rows and columns can be
obtained from a table that is closed and consistent only for rows or only for
columns. For example, when L∗terminates, then we have a minimal DFA and
an observation table that is row-closed and -consistent. We can then use the
learned automaton to ﬁll in the missing parts of the table to make it closed and
consistent also for columns. From such a table we can construct the ´atomaton
and also calculate the prime elements to construct the canonical RFSA and the
maximized prime ´atomaton.
7
Conclusions
We introduced a new canonical NFA for regular languages, the maximized prime
´atomaton, and studied its properties. Being the dual automaton of the canonical
RFSA, the maximized prime ´atomaton can be considered as a candidate for a
small NFA representation of a language.
We described how four canonical automata – the minimal DFA, the canonical
RFSA, the ´atomaton, and the maximized prime ´atomaton – can be obtained
from suitable observation tables used in automata learning algorithms. We also
believe that interpreting these observation tables in terms of quotients and atoms
of a language can provide new insights on automata learning problems.
References
1. Angluin, D.: Learning regular sets from queries and counterexamples. Inf. Comput.
75(2), 87–106 (1987). https://doi.org/10.1016/0890-5401(87)90052-6
2. Birget, J.: Intersection and union of regular languages and state complex-
ity. Inf. Process. Lett. 43(4), 185–190 (1992). https://doi.org/10.1016/0020-
0190(92)90198-5
3. Bollig, B., Habermehl, P., Kern, C., Leucker, M.: Angluin-style learning of NFA.
In: Boutilier, C. (ed.) IJCAI 2009, Proceedings of the 21st International Joint
Conference on Artiﬁcial Intelligence, Pasadena, California, USA, 11–17 July 2009,
pp. 1004–1009 (2009). http://ijcai.org/Proceedings/09/Papers/170.pdf

196
H. Maarand and H. Tamm
4. Brzozowski, J.A.: Canonical regular expressions and minimal state graphs for deﬁ-
nite events. In: Proceedings of Symposium on Mathematical Theory of Automata.
MRI Symposia Series, vol. 12, pp. 529–561. Polytechnic Press, Polytechnic Institute
of Brooklyn, N.Y. (1963)
5. Brzozowski, J.A., Tamm, H.: Theory of ´atomata. Theor. Comput. Sci. 539, 13–27
(2014)
6. Denis, F., Lemay, A., Terlutte, A.: Residual ﬁnite state automata. Fund. Inform.
51, 339–368 (2002)
7. Iv´an, S.: Complexity of atoms, combinatorially. Inf. Process. Lett. 116(5), 356–360
(2016)
8. Kameda, T., Weiner, P.: On the state minimization of nondeterministic ﬁnite
automata. IEEE Trans. Comput. 19(7), 617–627 (1970)
9. Lombardy, S., Sakarovitch, J.: The universal automaton. In: Flum, J., Gr¨adel,
E., Wilke, T. (eds.) Logic and Automata: History and Perspectives [in Honor of
Wolfgang Thomas]. Texts in Logic and Games, vol. 2, pp. 457–504. Amsterdam
University Press (2008)
10. Myers, R.S.R., Ad´amek, J., Milius, S., Urbat, H.: Coalgebraic constructions of
canonical nondeterministic automata. Theor. Comput. Sci. 604, 81–101 (2015)
11. Nerode, A.: Linear automaton transformations. Proc. Amer. Math. Soc. 9, 541–544
(1958)
12. Tamm, H.: Generalization of the double-reversal method of ﬁnding a canonical
residual ﬁnite state automaton. In: Shallit, J., Okhotin, A. (eds.) DCFS 2015.
LNCS, vol. 9118, pp. 268–279. Springer, Cham (2015). https://doi.org/10.1007/
978-3-319-19225-3 23
13. Tamm, H.: New interpretation and generalization of the Kameda-Weiner method.
In: 43rd International Colloquium on Automata, Languages, and Programming
(ICALP 2016). Leibniz International Proceedings in Informatics (LIPIcs), vol.
55, pp. 116:1–116:12. Schloss Dagstuhl-Leibniz-Zentrum f¨ur Informatik, Dagstuhl
(2016)
14. Tamm, H., van der Merwe, B.: Lower bound methods for the size of nondetermin-
istic ﬁnite automata revisited. In: Drewes, F., Mart´ın-Vide, C., Truthe, B. (eds.)
LATA 2017. LNCS, vol. 10168, pp. 261–272. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-53733-7 19

Union-Complexities of Kleene Plus
Operation
Benedek Nagy(B)
Department of Mathematics, Faculty of Arts and Sciences,
Eastern Mediterranean University, Famagusta, North Cyprus, Mersin-10, Turkey
nbenedek.inf@gmail.com
Abstract. Union-free expressions are used in union normal form to
decompose any regular language to a ﬁnite union of union-free languages.
Based on the automata characterisation of the union-free languages, by
restricting the 1CFPAs not to have transitions by the empty word, or to
be deterministic, the n-union-free and the deterministic union-free lan-
guages are deﬁned. Union-complexity as a measure of descriptional com-
plexity of regular languages was introduced recently. By the minimum
number of union-free/n-union-free/deterministic union-free languages
needed to get a regular language as their union, its union-complexity/n-
union-complexity/d-union-complexity is deﬁned. It is already known that
union-complexity and n-union-complexity are ﬁnite for every regular
language, however there are regular languages with inﬁnite d-union-
complexity. Operational union-complexity, that is, to predict the union-
complexity of a language obtained by a language operation from lan-
guages with known union-complexity is an important and interesting
question belonging to the ﬁeld of descriptional complexity of formal sys-
tems. In the present paper, the Kleene plus, the positive Kleene closure
operator is studied. As the Kleene star and plus operations have very
diﬀerent eﬀects on the union-free languages, it is an interesting prob-
lem to investigate how the union-complexities may change under this
operation. In particular, we show that the union-complexity of a regular
language is not growing when this operation is being applied on it. On
the other hand, the n-union-complexity of the Kleene plus of an n-union-
free language remains 1, but the n-union-complexity of the Kleene plus
of other regular languages may grow. Further, the deterministic union-
complexity may jump to an inﬁnite value even if the original language
had a relatively small deterministic union-complexity, e.g., 4.
Keywords: union-complexity · union-free languages · regular
expressions · Kleene closure
1
Introduction
Various classes of subregular languages are important from various points of
view, see, e.g., [5,9]. The union-free languages are deﬁned by regular expres-
sions without the union, they are the star-dot regular languages [2]. Automata
c
⃝IFIP International Federation for Information Processing 2022
Published by Springer Nature Switzerland AG 2022
Y.-S. Han and G. Vaszil (Eds.): DCFS 2022, LNCS 13439, pp. 197–211, 2022.
https://doi.org/10.1007/978-3-031-13257-5_15

198
B. Nagy
theoretical characterisation [11] allowed to deﬁne the deterministic counter-
part: the deterministic union-free languages [3,7,8] and by the nondetermin-
istic λ-transition-free automata, the n-union-free languages. The classes of the
union-free, n-union-free and d-union-free languages form a proper hierarchy [14]
and they were studied in [2,4,7,11], [14] and [3,8], respectively. Based on pos-
sible decomposition of regular languages to ﬁnite unions of those languages,
the union-complexity, n-union-complexity and d-union-complexity are deﬁned
[1,10,12,13,15] (note that this latter could be inﬁnite according to [8] even if
the language is regular). The operational union-complexity is studied in details
under various operations in [15], except, e.g. Kleene plus. On the other hand, the
class of union-free languages is closed under concatenation, Kleene plus and also
under Kleene star. Moreover, for any regular language, its Kleene star is union-
free, but a similar statement does not hold for Kleene plus. Further, this class is
not closed under union and this gives the possibility to deﬁne the union normal
form and union-complexity of regular languages. Since Kleene plus and Kleene
star behave in diﬀerent ways from our point of view, it is worth to study Kleene
plus and we concentrate on this issue in this paper. Closure, or indeed, more
precisely, anti-closure properties of n-union-free and d-union-free languages were
studied in [8,14], respectively. The non trivial closure properties of the classes
of n-union-free and d-union-free languages also give the challenge to analyse
the analogous union-complexity measures under various operations. Here as we
already mentioned, the Kleene plus is in our focus.
While another usual measure of descriptional complexity of regular languages
is connected to the minimal number of states of the accepting ﬁnite automata,
the union-complexity is closely connected to the union normal form and thus to
the regular expressions describing the language [10,12,13].
2
Preliminaries
In this section, ﬁrst we recall the deﬁnition of the union-free languages and the
corresponding class of ﬁnite automata. We assume that the reader is familiar with
the basic concepts of formal languages and automata, thus for each unexplained
concepts she/he is referred to any standard textbook on the topic, e.g., to [6]
or to the Handbook chapter [17]. Here we show only speciﬁc notions closely
related to the topic of this paper. The empty word is denoted by λ; Σ is a
ﬁnite alphabet, while ∪, ·, ∗, + denote the usual operations on languages, i.e., the
union, the concatenation, the Kleene star and the Kleene plus. Now we recall
some (formal) concepts, deﬁnitions and notions from earlier mentioned studies.
A regular expression is a union-free expression if only the operators con-
catenation and Kleene star are used in its description. A regular language is a
union-free language if there is a union-free expression that deﬁnes it.
We note here that in the literature sometimes a wider class of languages are
called union-free, those which have a description by operations concatenation,
Kleene star and complement [9], somewhat similarly as the description of star-
free languages goes by concatenation, union and complement [17].

Union-Complexities of Kleene Plus Operation
199
Now we brieﬂy recall the concept of ﬁnite automata and ﬁx some notations.
A 5-tuple A = (Q, S, Σ, δ, F) is a non-deterministic ﬁnite automaton, with
the ﬁnite set of states Q. Further, S ∈Q is the initial state, Σ is the (input)
alphabet and F ⊂Q is the set of ﬁnal (or accepting) states. The function
δ : Q × (Σ ∪{λ}) →2Q is the transition function.
A path Q0a1Q1a2Q2 . . . an−1Qn−1anQn where Qi+1 ∈δ(Qi, ai+1) for every
0 ≤i < n (with n > 0) is called a cycle if Q0 = Qn. A path without any repeated
state is called a cycle-free path.
A path is called an accepting path, if it ends in a ﬁnal state. Fur-
ther, it is an accepting path of a word w if it is written as (S
=
Q0)a1Q1a2Q2...an−1Qn−1anQn with Qn ∈F and w = a1a2...an (ai ∈Σ ∪{λ}),
i.e., it is an accepting path starting at the initial state. A word is accepted by
the ﬁnite automata if it has an accepting path.
Deﬁnition 1 (1CFPA, n-1CFPA, d-1CFPA). A nondeterministic ﬁnite
automaton A is a 1 cycle-free path automaton, a 1CFPA, for short, if there
is a unique cycle-free accepting path from each of its states. Moreover, if the
automaton A does not have any λ-transitions, then it is an n-1CFPA, and if A
is deterministic, then it is a d-1CFPA.
In this paper, we use only automata with the following property: for each
state Qi of the automaton there is a word such that it has an accepting path that
contains Qi. Consequently, there is no useless or sink state and the automaton
may not be fully determined, i.e., it may happen that for a state Qi and an input
letter a the transition function assigns the empty set.
As a consequence of the deﬁnition above, a 1CFPA has exactly one ﬁnal state.
From now on F will refer not only to the set of ﬁnal states, but to its unique
element, as well, in case of a 1CFPA. One of the main results of [11] states that
the family of languages which are described by union-free expressions and the
family of languages recognized by 1CFPAs are exactly the same. Based on this
relation, two further classes of union-free languages are deﬁned as follows:
Deﬁnition 2 (d-union-free and n-union-free languages). A language is
deterministic union-free if there is a deterministic 1CFPA which accepts it [8,13].
The short form d-union-free will also be used for these languages. Further, the n-
union-free languages are exactly those union-free languages that can be accepted
by n-1CFPA [14].
Observe that by deﬁnition, in a 1CFPA, a transition between two distinct
states cannot be part of the unique cycle-free path from any state to the ﬁnal
state, if there is a parallel transition between the same two states. The issue
with parallel transitions can be resolved by a construction duplicating some
parts of the automaton. Based on this, every x-union-free language (x ∈{λ,
n, d}) is accepted by an x-1CFPA such that for any two distinct states P, R
there is at most one letter such that there is a transition from P to R with that
letter.Therefore, in various constructions and proofs, w.l.o.g., we may assume
that there is no transition with two diﬀerent letters between two distinct states
of the automaton.

200
B. Nagy
Since in a 1CFPA, from every state R, there is exactly one transition that
goes to the direction of F (without cycle), the word which transfers the state R
to F in a cycle-free path is unique for each state. We recall that the backbone of
the automaton is the cycle-free path from the initial state (S) to the ﬁnal state
(F). The word accepted by the backbone is called the backbone word.
The following facts are known about union-free languages [11]:
– An x-union-free (x ∈{λ, n, d}) language is either inﬁnite or contains at most
one word.
– The shortest word of a union-free language L is unique and it is the backbone
word. In a union-free language each word contains the backbone word (maybe
in a scattered way).
The usual expression tree concept can be used to represent regular expres-
sions. To each leaf of the tree a letter of the alphabet or the empty word is
assigned. To each other vertex the sign of a regular operation is assigned such
that a vertex with assigned ∪or · has exactly two children in tree, while a vertex
with assigned ∗or + has exactly one child. Let L be a union-free language. Note
that λ ∈L if and only if the backbone word is the empty word. This implies that
every letter is under a Kleene star in the tree of the regular expression. Under
these circumstances the language can be accepted by a 1CFPA with backbone
word λ. If L is n-union-free and λ ∈L, then S = F in the corresponding
n-1CFPA. Since every 1CFPA (and thus d-1CFPA) has exactly one accepting
state, languages which cannot be accepted by deterministic ﬁnite automata with
only one ﬁnal state are not d-union-free languages.
It is known (see, [8,11]) that the family of union-free languages is closed
under the operations concatenation, Kleene star and Kleene plus. The family
of n-union-free languages is not closed under union and concatenation, but it
is closed under Kleene plus [14]. Furthermore, the class of unary n-union-free
languages is closed under concatenation, Kleene star, Kleene plus. On the other
hand, we have only anti-closure properties for the d-union-free languages: e.g.,
their class is not closed under union, concatenation, Kleene star, [8] and Kleene
plus [14].
In fact, all d-union-free languages are n-union-free languages and all n-union-
free languages are union-free. The language a∗b∗is union-free. However, it is not
n-union-free [14]. The language a(b∪ba)∗is n-union-free, but not d-union-free [8].
Thus, there is a proper hierarchy among the three mentioned union-free classes.
By the decomposition result mentioned in [2,10,16], the union-complexity of
regular languages is deﬁned in [10,12]. As one of the main results of [14] states,
every regular language is a union of ﬁnitely many n-union-free languages. Based
on these analogies, we present the deﬁnition in a general way (based also on [15]).
However, it should be noted that while every regular language can be expressed
as a union of a ﬁnite number of union-free and also as a union of a ﬁnite number
of n-union-free languages, similar statement does not hold in general for the d-
union-free languages (as proven in [8]), therefore, the d-union-complexity may be
inﬁnite although the studied language is regular. The following deﬁnition gives

Union-Complexities of Kleene Plus Operation
201
back the original deﬁnition by the choice x = λ; gives the n-union-complexity
with x = n; moreover it also gives the d-union-complexity with x = d.
Deﬁnition 3 (Union-complexity,
n-union-complexity
and
d-union-
complexity). Let x ∈{λ, n, d}. The form L =
k
i=1
Li is a minimal x-decom-
position of the language L if each Li is an x-union-free language and there is no
m < k such that L =
m
i=1
Ki, where each Ki is x-union-free. Then, k is called
the x-union-complexity of the language L. However, in the case that L cannot be
written in the form
k
i=1
Li with any natural number k for deterministic union-free
languages Li (1 ≤i ≤k), then L has an inﬁnite deterministic union-complexity.
The class of union-free languages is an interesting class including several
languages since for each regular language L, the language L∗is union-free reg-
ular. We can summarise some others of the simplest known results about the
union-complexities (see, e.g., [14]):
– The x-union-complexity of an x-union-free languages is at most 1 (x ∈{λ,
n, d}); it is 0 for the empty language and 1 for every nonempty x-union-free
language.
– For every ﬁnite language, its x-union-complexity is exactly the cardinality of
the language.
– A language is regular if and only if its union-complexity is ﬁnite.
– A language is regular if and only if its n-union-complexity is ﬁnite.
As we already mentioned, the d-union-complexity could be inﬁnite, as, e.g.,
one of the main results of [8] states:
Proposition 1. The language deﬁned by the regular expression ((a ∪b)(a ∪b))∗
cannot be expressed as a union of a ﬁnitely many deterministic union-free lan-
guages.
In [1] it has been proven that the union-complexity of regular languages
is computable. However, the method is very complex and cannot be used in
practical applications. Some bounds may be computed much faster, e.g., an x-
decomposition (may also be called x-union normal form) of a regular language
deﬁnes an upper bound for its x-union-complexity.
Before continuing with further more technical concepts, we are already at
the stage that all the necessary concepts are shown to understand an example
that could highlight the non-trivial nature of the problem we investigate here.
Example 1. Let us consider the language L deﬁned by (a∗b ∪dc∗). On the one
hand, the language has 2 shortest words, b and d, and thus it is not union-
free. On the other hand, Fig. 1 shows the two d-1CFPAs that accept a∗b and
dc∗, respectively proving that L has union-complexity, n-union-complexity and
d-union-complexity 2.

202
B. Nagy
Fig. 1. Two deterministic 1-cycle-free-path-automata: the accepted languages, a∗b
(left) and dc∗(right) are d-union-free.
Let us consider now L+, i.e., (a∗b∪dc∗)+. As it has again 2 shortest words, b
and d, this is neither union-free. On the one hand, one can easily check that L+ is
the union of the two languages accepted by 1CFPAs of Fig. 2, as the 1CFPA on
the left accepts exactly those words of L+ which start with a∗b and the 1CFPA
on the right accepts exactly those which start with d. On the other hand, both of
the 1CFPAs use λ-transitions, i.e., they are not n-1CFPAs and not d-1CFPAs.
Fig. 2. Two nondeterministic 1-cycle-free-path-automata with λ-transitions such that
the union of their accepted languages is exactly (a∗b ∪dc∗)+.
Furthermore, we show that the n-union-complexity of L+ is greater than 2.
The proof is by contradiction, thus assume that there are 2 n-1CFPAs such that
the union of their accepted languages is exactly L+. As there are two shortest
words in L+, the backbone of one of the n-1CFPAs, let us say, A, must be SAbFA
and the backbone of the other, let us say, B, must be SBdFB. The word db is also
in L+, thus one of the 1CFPAs A or B must accept it. The n-1CFPA accepts
db, must use the above described backbone transition, and another transition to
process the other letter, thus this other transition must be a self-loop transition.
Thus, in the former case, there is a cycle in A as SAdSA, while in the latter
case there is a cycle in B as FBbFB. Now, on the one hand, ab ∈L+, and this
must be accepted by A which implies the cycle SAaSA in A. However, if A has
also the cycle with letter d on its initial state (as we assumed in the ﬁrst case),
then A would also accept the word adb which is clearly not in L+. Thus the ﬁrst

Union-Complexities of Kleene Plus Operation
203
case cannot hold, db cannot be accepted by A. Now, on the other hand, we have
that dc ∈L+, and this word must be accepted by B. However, it implies the
cycle FBcFB in B. But, now, B would also accept dbc ̸∈L+, which provides the
contradiction and the proof that L+ has a larger n-union-complexity than 2.
Fig. 3. Three n-1CPAs such that the union of their accepted languages is exactly
(a∗b ∪dc∗)+.
Now, to prove that L+ has n-union-complexity 3, consider the union of the
languages accepted by n-1CFPAs shown in Fig. 3. In fact the automaton on the
left accepts the words of L+ that start with a∗b, the n-1CFPA in the middle
accepts those which start with d, but not with dc+, while the n-1CFPA on the
right is accepting the words that start with a word of dc+.
Observing that the 1CFPAs we used in the previous descriptions are highly
not deterministic, i.e., in many of them there are more than one transition from
some states by the same letter, the d-union-complexity of the language L+ could
be even much higher than 3. We may conjecture it here, without any other
explanations, that it is inﬁnite.
Now, we also give new concepts, the tail (and tail-cycles) of the 1CFPAs and
another technical concept, the branching (states and transitions).
Deﬁnition 4 (branching, head, tail). A state P ̸= F of a 1CFPA is called a
branching state if there are at least two diﬀerent transitions from this state, i.e.,

204
B. Nagy
there is a transition which does not follow the only cycle-free accepting path from
P, these transitions are called branching transitions. The ﬁnal state F is called
a branching state if there is a transition from it, moreover, all of the transitions
from F are branching transitions (as none of them is part of the empty path F,
i.e., the shortest cycle-free path from F to F).
If the initial state is a branching state, then the cycle(s) in one of the following
forms are called head-cycles:
– either it contains exactly one transition step of the form SaS with a letter
a ∈Σ; or
– it starts with a branching transition step SaR and continues with the cycle-
free accepting path from R till S is reached again.
A cycle starting from the ﬁnal state F is called a tail-cycle of the 1CFPA if
it is in one of the following forms:
– either it contains exactly one transition step of the form FaF with a letter
a ∈Σ; or
– it starts with a transition step FaR (R ̸= F) and continues with the cycle-free
accepting path from R.
If a 1CFPA does not have any tail-cycles, we say that it is tail-cycle-free or
tailless (or without a tail).
The following facts are due to the structure of 1CFPAs.
– Any self-loop transition is a branching transition.
– If there is a branching transition from a state P on the backbone to another
state R in the backbone, then R has a longer cycle-free accepting path than
P has.
– If there is a branching transition from a state P on the backbone and its
transition goes to the state R that is not on the backbone, then the cycle-free
accepting path from R reaches the backbone before or on P, i.e., maybe some
of the last steps of the cycle starting from P with the branching transition is
already on the backbone to reach P again.
– If there is a branching transition from a state P ̸= F in a tail-cycle to another
state R, then R has a longer cycle-free accepting path than P has, moreover,
this cycle-free path arrives back to the tail-cycle before or in P (i.e., on one
of the states that the cycle already touched after F by reaching P).
– A tail-cycle may reach the backbone in any of its states P and then, it must
follow the backbone till reaching F. (In a special case, it may contain only F
from the backbone.)
– A branching transition from a state P going to R always implies that all
accepting paths from R will reach P again.
– The number of tail-cycles of a 1CFPA is the number of the (branching) tran-
sitions from F.

Union-Complexities of Kleene Plus Operation
205
Now we also deﬁne another new concept, the set of substates:
Deﬁnition 5. A state R ̸= P is a substate of P if there is a branching transition
from P to R. Moreover, the states which are not in the shortest (i.e., cycle-
free) path from S to P, but are in the cycle-free accepting path from R are also
substates of P, the set of these states is denoted as sub(PR). Further, the set
of states of the cycle starting with the branching transition from P to R and
then following the cycle-free accepting path till P is reached again is denoted by
sub◦(PR).
We have the following:
– The states on the backbone are not substates of any state of the 1CFPA.
– Each state of an 1CFPA is either on the backbone or it is a substate of another
state.
– A tail-cycle contains F, some of the substates of F (if there are more tail-
cycles) and maybe some other states of the backbone.
The language ∅is a very special language, its union-complexity is 0, as well
as its n-union-complexity and d-union-complexity are also 0, since we need 0
languages to unite them to obtain it. On the other hand, the Kleene plus of ∅is
itself, that is, ∅+ = ∅. There is not so much about to say this special languages,
and thus, in the rest of the paper we may assume that the language we consider
is not the empty one.
We recall some of the main results of [15], the operational union-complexity
of the three regular operations, union, concatenation and Kleene star.
Proposition 2. Let L1 and L2 be two regular languages with union-complexities
n and m, respectively. Then the union of them, i.e., L = L1 ∪L2 could have the
union-complexity at most n + m. Moreover, this bound is tight, i.e., for any two
positive integers n, m, there are languages L1 and L2 with union-complexities n
and m, such that their union has union-complexity exactly n + m.
Proposition 3. Let L1 and L2 be two regular languages with union-complexities
n and m, respectively. Then the concatenation of them, i.e., L = L1 · L2, could
have the union-complexity at most n · m, and this bound is tight.
Proposition 4. Let L be a regular language with union-complexity n. Then the
language L∗has the union-complexity exactly 1 independently of the value of n.
Now, we are ready to present our main results concerning the union-com-
plexity of languages created by Kleene plus operation.
3
Operational Union-Complexity of the Kleene Plus
Operation
First, we present the case of the general union-complexity, and then in subsec-
tions we show the cases of the n-union-complexity and the d-union-complexity.
One of our main result, complementing the results shown in [15] is as follows.

206
B. Nagy
Theorem 1. Let L be a regular language with union-complexity n. Then the
language L+, the Kleene plus of L has a union-complexity of at most n.
Proof. Obviously L+ = L·L∗. From Propositions 3 and 4 and one may establish
the upper bound as n · 1 = n.
To prove that this bound is tight, let us start with a language over an n-
ary alphabet Σn. Let Ln = Σn = {a1, . . . , an}. Then, L+
n = Σ+
n has clearly n
shortest words, i.e., the letters of the alphabet as words showing that its union-
complexity cannot be less than n.
However, this construction uses a larger and larger alphabet with growing
value of n. After this initial result on the tightness, let us consider the binary
alphabet Σ2 = {0, 1}. Let us encode n diﬀerent letters with a binary block
code. More precisely, let L2 = {10k, 10k−11, . . . , 1wi, . . . , 1wn−1}, where wi is
the binary representation of number i with k digits. (The value of k should be
at least ⌈log2(n)⌉to make this possible.)
Clearly, L2 is a ﬁnite language with n words, thus its union-complexity is n.
Now, the union-complexity of L+
2 can be estimated from below by the number
of its shortest words which is n and gives the proof of the tightness already for
the case of a binary alphabet.
⊓⊔
The unary alphabet plays some special importance and it is also interesting
since already some of the closure properties of the union-free languages works
in a diﬀerent manner for this special case, consider, e.g., the closure under con-
catenation [14].
Although over the unary alphabet, the properties of the Kleene star and
the Kleene plus are usually very similar, we intend to show that they work in a
diﬀerent way when the union-complexity is studied. It is well-known, and we have
already mentioned, that L∗of any regular language has the union-complexity 1.
Now we show that this is not the case with L+ even if the regular language is
over a unary alphabet.
Theorem 2. The regular language L described by a4(a9)∗∪a7(a5)∗has the
union-complexity 2. Further its Kleene plus, L+ has also union-complexity 2.
Proof. As 4 and 9 are co-primes, the language L1 deﬁned by (a4(a9)∗)+ is co-
ﬁnite, i.e., there are only ﬁnitely many natural numbers ℓsuch that aℓis not
in the language L1, and thus, the diﬀerence of a∗and L1 is a ﬁnite language.
A similar statement is true for (a7(a5)∗)+. In the former, the following positive
lengths are missing: 1, 2, 3, 5, 6, 7, 9, 10, 11, 14, 15, 18, 19, 23, 27. As usual over
the unary alphabet, the words can be identiﬁed by their lengths. By a theorem
of Frobenius (actually, Sylvester has published in the 1880’s its solution for the
case we need), the longest word that cannot be given in the form 4k1 + 9k2
by nonnegative integer values of k1 and k2 is 4 × 9 −(4 + 9) = 36 −13 = 23,
however, we have the condition k1 ≥1 which shifts this limit a little bit. In fact,
(a4(a9)∗)+ has all the words of length 4k1 with k1 ≥1, all the words of length
4k1 + 9, 4k1 + 18 and 4k1 + 27. These for sets of integers contain all the integers
ℓ> 27.

Union-Complexities of Kleene Plus Operation
207
For the following lengths of the above list, a word with length exists in the
language L+:
7 : 7,
11 : 4 + 7,
14 : 7 + 7,
15 : 4 + 4 + 7,
18 : 4 + 7 + 7,
19 : 7 + 7 + 5,
23 : 4 + 7 + 7 + 5,
27 : 4 + 4 + 7 + 7 + 5.
The co-ﬁnite language L+ does not have the lengths 1, 2, 3, 5, 6, 9 and 10,
i.e., L+ = {a4, a7, a8} ∪{ak | k > 10}. Now, we show that L+ is not union-free.
If it would be, then the backbone word must be aaaa, as it is the shortest word
of L+. Further the 1CFPA must also accept a7 meaning that there is a cycle
accessible from the backbone by pumping 3 as into the word. However, then by
doing this pumping cycle again, the word a10 would be obtained and accepted.
This is contradicting to the fact that a10 is not in L+. Thus, this language is
not union-free, it has a union-complexity of at least 2. By applying Theorem 1,
since L has union-complexity 2, it cannot be more than 2. Therefore, it has been
proven that L+ has union-complexity 2.
⊓⊔
The precise investigation of the unary case is left for the future:
Open Problem 1. Whether the result stated in Theorem 1 is also tight in the
case of the unary alphabet for larger union-complexities, is left open.
3.1
On n-Union-Complexity
In this subsection, the n-union-complexity is studied. As we already mentioned,
for each regular language, its n-union-complexity is a ﬁnite number. On the one
hand, the closure of the class of the n-union-free languages under Kleene plus
([14]) gives the immediate corollary:
Corollary 1. Let L be a language with n-union-complexity 1. Then, the n-
union-complexity of the language L+ is also 1.
On the other hand, as we have seen in Example 1, the union-complexity
and the n-union-complexity may behave in a diﬀerent manner. Moreover, the
constructions in the proofs of Propositions 3 and 4 were based on language
operations (like regular expressions) [15], which can be translated to automata
only with intensive usage of λ-transitions: remember that the class of n-union-
free languages is not closed under concatenation. Thus, we may need to ﬁnd new
constructions to estimate the n-union-complexity.
Theorem 3. Let a regular language L be given with an n-union-complexity k.
Further, let L1, . . . , Lk be some n-union-free languages such that L =
k
i=1
Li. Let
Ai be an n-1CFPA accepting Li for each 1 ≤i ≤k. Let the number of tail-cycles
of Ai be ti. Then, the n-union-complexity of L+ is at most m = k +
k
i=1
ti.

208
B. Nagy
Proof. The proof is a construction to show that L+ is the union of m n-union-
free languages. Based on the condition given in the theorem, let A1, . . . , Ak, be
k n-1CFPAs that accept the languages L1, . . . , Lk such that L =
k
i=1
Li.
Let the structure of each Ai be given with its backbone states and the sets
of the substates including the substates of the tail-cycles (if any). See Fig. 4, top
left.
Now, let us have ti + 1 copies of each Ai, a copy Bi that is similar to
the original, but does not include any of the tail-cycles of Ai (see Fig. 4, top
right); and a copy Cℓ
i for each tail-cycle which are expanded variants of Ai
as it is explained below (ℓis the numbering of the tail-cycles of the automaton,
1 ≤ℓ≤ti). Let the backbone of Cℓ
i be Sℓ
i aℓ
i . . . F ℓ
i bℓ
iR
ℓ
i . . . F
ℓ
i where Sℓ
i aℓ
i . . . F ℓ
i is
a copy of the backbone of Ai and F ℓ
i bℓ
i . . . F
ℓ
i is a copy of the ℓ-th tail-cycle, that is
starting with the copy of the branching transition F ℓ
i bℓ
iRℓ
i. Thus, the backbone
of Cℓ
i contains a copy of each backbone state of Ai and also an (additional)
overlined copy of the states of sub◦(F ℓ
i Rℓ
i). Let now all the cycles of Ai be
added by adding the substates of each not overlined state, and their substates
iteratively by their transitions including all tail-cycles from the state F ℓ
i . The
substates and the cycles of each overlined state should also be added, but the
copy F
ℓ
i of the ﬁnal state (that actually is the ﬁnal state of Cℓ
i, but in this way,
it will not be part of any cycles in Cℓ
i. (See the second line of Fig. 4.)
So far, by our constructions, each 1CFPA Bi accepts exactly those words of
the language Li that are accepted without using any transitions of the ﬁnal state
(i.e., without using any of the tail-cycles); and each 1CFPA Cℓ
i accepts exactly
those words of Li which are accepted in such a way that from the ﬁnal state the
last used branching transition deﬁnes the tail-cycle ℓin Ai. Now let us take copies
of each of these automata, from each one more as the number of its head-cycles
(i.e., the number of branching transitions of the initial state), let these copies be
Bj
i and Cℓ,j
i , where j is a nonnegative integer not more than hi, the number of
head-cycles of Ai. Let all B0
i and Cℓ,0
i
be identical to Bi and Cℓ
i, respectively,
but without any head-cycles (the transitions and the states of the head-cycles
and their substates are simply removed, see the third line of Fig. 4). Further, let
Bj
i and Cℓ,j
i
with j > 0 be deﬁned as follows. (In the next part we use ℓ= 0 to
index the states of Bj
i, while ℓ> 0 for the states of Cℓ,j
i .) Let the backbone of
these automata be ˜Sℓ,j
i bℓ,j
i,1 . . . Sℓ,j
i aℓ
i,1 . . . F ℓ,j
i
where the part ˜Sℓ,j
i bℓ,j
i,1 . . . Sℓ,j
i
is a
copy of the j-th head-cycle that starts with branching transition from Sℓ,j
i
in Ai
and in this way it is becoming not a cycle, but part of the backbone in the new
1CFPA from ˜Sℓ,j
i
with letter bℓ,j
i,1; and the rest of the backbone, Sℓ,j
i aℓ
i,1 . . . F ℓ,j
i
,
is the original backbone of Bi or Cℓ
i. Further, all states of the 1CFPA Bi or
Cℓ
i are kept, respectively, with their transitions. Also all the substates of the
states of the j-th head-cycle (but the initial state) are copied and reached from
the states of the ﬁrst part ˜Sℓ,j
i bℓ,j
i,1 . . . Sℓ,j
i
of the backbone, with their transitions
(between pairs of copied states). Moreover, all head-cycles and their substates

Union-Complexities of Kleene Plus Operation
209
Fig. 4. Some parts of the construction in the proof of Theorem 3.
with their transitions are also copied with a branching transition from Sℓ,j
i . (See
the bottom of Fig. 4.)
In this way, each 1CFPA B0
i accepts all words of Li that can be accepted by a
path neither containing any branching transition from the initial state of Ai, nor
from its ﬁnal state. Further, each 1CFPA Bj
i accepts exactly those words of Li
that are accepted by a path starting by the j-th branching transition (identifying
the j-th head-cycle) from the initial state of Ai and do not use any branching
transitions from the ﬁnal state of Ai. Also, each Cℓ,0
i
accepts those words of Li
that are accepted by using none of the head-cycles of Ai, but the ℓ-th tail-cycle
was used in the end of the word (the last used branching transition from the
ﬁnal state of Ai used the branching transition deﬁning the tail-cycle ℓ). Finally,
each Cℓ,j
i
accepts exactly those words of Li that have an accepting path in Ai

210
B. Nagy
that starts with the j-th head-cycle that is deﬁned by a branching transition at
the initial state of Ai and the last branching transition at the ﬁnal state used
in the accepting path deﬁning the tail-cycle ℓ.
We continue the construction by modifying every Bi and Cℓ
i in a very similar
way. Thus, let us consider one, let us say X of the 1CFPAs Bi or Cℓ
i, let its
initial state be denoted by S and its ﬁnal state be denoted by F. It is clear by
the construction, that none of those 1CFPAs has tail-cycles, i.e., all of them are
tailless. Now, consider all Bj
i (1 ≤i ≤k, 0 ≤j ≤hi) and Cℓ,j
i
(1 ≤i ≤k,
1 ≤ℓ≤ti, and 0 ≤j ≤hi). For each of these 1CFPAs, let us say Y, we
expand the 1CFPA X as follows. For the ﬁnal state F, let us put a branching
transition for each Y copying all states and transitions of Y into X such that
all the transitions from the initial state of Y are coming from F in X, moreover
all the transitions reaching the ﬁnal state of Y are going to F instead in X.
Clearly, each of the constructed automata accepts only words of the language
L+. Moreover, any word of L+ is accepted by at least one of the previously
constructed automata, actually, depending on the structure of the ﬁrst word
that is used to compose the given word from the words of the languages Li.
⊓⊔
Based on the fact that over a unary alphabet any union-free language is also
an n-union-free language [14], we can restate and reformulate Theorem 2.
Corollary 2. There is a regular language such that its Kleene plus has n-union-
complexity greater than 1. For example, considering L = {ak | k = 4+9n or k =
7 + 5n for all n ≥0}, both the n-union-complexities of L and L+ are 2.
3.2
On Deterministic Union-Complexity
Now we give our result about deterministic union-complexity.
Theorem 4. There is a regular language L with ﬁnite d-union-complexity such
that L+ has inﬁnite d-union-complexity.
Proof. Consider the language L = {aa, ab, ba, bb}, clearly its d-union-complexity
is a 4. Now, let us consider L+ = (aa ∪ab ∪ba ∪bb)+, which actually contains
all nonempty words over Σ = {a, b} with even lengths. Moreover, every word
of Σ∗is a preﬁx of some words of L+, thus based on an analogous proof of
Proposition 1, L+ cannot be written as a ﬁnite union of deterministic union-free
languages [8].
Open Problem 2. Is there any language L with smaller d-union-complexity
than 4 such that its Kleene plus, L+ has already inﬁnite d-union-complexity?
May, e.g., the language of Example 1 have this property?
Since the class of deterministic union-free languages is not closed under any
of the usual language operations (union, complement, concatenation, Kleene star
etc., see [8,13]), it seems to be a non-trivial task, to ﬁnd operational d-union-
complexity of languages.

Union-Complexities of Kleene Plus Operation
211
References
1. Afonin, S., Golomazov, D.: Minimal union-free decompositions of regular lan-
guages. In: Dediu, A.H., Ionescu, A.M., Mart´ın-Vide, C. (eds.) LATA 2009. LNCS,
vol. 5457, pp. 83–92. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-00982-2 7
2. Brzozwski, J.A.: Regular expression techniques for sequential circuits. Ph.D Dis-
sertation, Department of Electrical Engineering, Princeton University, Princeton,
NJ, June 1962
3. Brzozowski, J.A., Davies, S.: Most complex deterministic union-free regular lan-
guages. In: Konstantinidis, S., Pighizzini, G. (eds.) DCFS 2018. LNCS, vol. 10952,
pp. 37–48. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94631-3 4
4. Crvenkovi´c, S., Dolinka, I., ´Esik, Z.: On equations for union-free regular languages.
Inform. Comput. 164(1), 152–172 (2001)
5. Holzer, M., Kutrib, M.: Structure and complexity of some subregular language
families. In: The Role of Theory in Computer Science, pp. 59–82 (2017)
6. Hopcroft, J.E., Ullman, J.D.: Introduction to Automata Theory, Languages and
Computation. Addison-Wesley Publishing Company, Reading MA (1979)
7. Jir´askov´a, G., Masopust, T.: Complexity in union-free regular languages. Int. J.
Found. Comput. Sci. 22, 1639–1653 (2011)
8. Jir´askov´a, G., Nagy, B.: On union-free and deterministic union-free languages. In:
Baeten, J.C.M., Ball, T., de Boer, F.S. (eds.) TCS 2012. LNCS, vol. 7604, pp.
179–192. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-33475-
7 13
9. Kutrib, M., Wendlandt, M.: Expressive capacity of subregular expressions. RAIRO
ITA: Theory Inf. Appl. 52(2–3–4), 201–218 (2018)
10. Nagy, B.: A normal form for regular expressions. In: Calude, C.S., Calude, E., Din-
nen M.J. (eds.) Supplemental Papers for DLT 2004 (8th International Conference
Developments in Language Theory), CDMTCS Report 252, Auckland, pp. 51–60
(2004)
11. Nagy, B.: Union-free regular languages and 1-cycle-free-path-automata. Publ.
Math. Debrecen 68, 183–197 (2006)
12. Nagy, B.: On union-complexity of regular languages, CINTI. In: 11th IEEE Interna-
tional Symposium on Computational Intelligence and Informatics 2010, pp. 177–
182 (2010)
13. Nagy, B.: Union-freeness, deterministic union-freeness and union-complexity. In:
Hospod´ar, M., Jir´askov´a, G., Konstantinidis, S. (eds.) DCFS 2019. LNCS, vol.
11612, pp. 46–56. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
23247-4 3
14. Nagy, B.: Union-freeness revisited – between deterministic and non-deterministic
union-free languages. Int. J. Found. Comput. Sci. 32, 551–573 (2021)
15. Nagy, B.: Operational union-complexity. Inform. Comput. 284, 104692 (2022)
16. Shallit, J.: A Second Course in Formal Languages and Automata Theory. Cam-
bridge University Press, Cambridge (2008)
17. Yu, S.: Regular languages. In: Rozenberg, G., Salomaa, A. (eds.) Handbook of
Formal Languages, pp. 41–110. Springer, Heidelberg (1997). https://doi.org/10.
1007/978-3-642-59136-5 2

Author Index
Adamson, Duncan
15
Crespi Reghizzi, Stefano
1
Durand-Lose, Jérôme
30
Fazekas, Szilárd Zsolt
43
Fleischmann, Pamela
57
Geffert, Viliam
72
Han, Yo-Sub
127
Haschke, Lukas
57
Hoffmann, Stefan
85
Holzer, Markus
100
Hospodár, Michal
112
Huch, Annika
57
Kim, Sungmin
127
Ko, Sang-Ki
127
Konstantinidis, Stavros
142
Kupferman, Orna
155
Kutrib, Martin
170
Maarand, Hendrik
184
Mastnak, Mitja
142
Mayrock, Annika
57
Merca¸s, Robert
43
Mlynárˇcik, Peter
112
Moreira, Nelma
142
Nagy, Benedek
197
Nowotka, Dirk
57
Olejár, Viktor
112
Pališínová, Dominika
72
Petruschka, Asaf
155
Rauch, Christian
100
Reis, Rogério
142
Salomaa, Kai
127
Szabari, Alexander
72
Tamm, Hellis
184
Wendlandt, Matthias
170

