
BAYESIAN INFERENCE 
IN STATISTICAL ANALYSIS 
GEORGE E. P. BOX and GEORGE C. TJAO 
Department 0/ Statistics 
University 0/ Wisconsin 
ADDISON-WESLEY PUBLISHING COMPANY 
Readi ng, Massac husetts 
Menlo Park, California· London· Amsterdam· Don Mills, Ontario· Sydney 

This book is in the 
ADDISON-WESLEY SERIES I~ BEHAVIORAL SCIENCE: 
QUANTITATIVE METHODS 
Consli/ling Edilor 
FREDERICK MOSTELLER 
Copyright © 1973 by Addison-Wesley Publishing Company. Inc. Philippines copyright 1973 by 
Addison-Wesley Publishing Company. Inc. 
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or 
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording, or 
otherwise, without the prior written permission of the publisher. Printed in the United States 
of America. Published simultaneously in Canada. Libl'ary of Congress Catalog Card No. 
78-172804. 
ISBN 0-201-00622-7 
DEfGHIJKLM-HA-89876543210 

To BARBARA, HELEN, and HARRY 

_ 
__ 
_ 
_ 
_ 
_ 
_ 
_ 
_ 
_ 
_ 
_ 
~__ 
__ 
_ 
w 
-

PREFACE 
The object of this book is to explore the use and relevance of Bayes' theorem to 
problems such. as arise in scientific investigation in which inferences must be made 
concerning parameter values about which little is known a priori. 
In Chapter 1 we discuss some important general aspects of the Bayesian 
approach, including: the role of Bayesian inference in scientific investigation, 
the choice of prior distributions (and, in particular, of noninformative prior 
distributions), the problem of nuisance parameters, and the role and relevance 
of sufficient statistics. 
]n Chapter 2, as a preliminary to what follows , a number of standard problems 
concerned with the comparison of location and scale parameters are discussed. 
Bayesian methods, for the most part well known, are derived there which closely 
parallel the inferential techniques of sampling theory associated with {-tests, 
F-tests, Bartlett's test, the analysis of variance, and with regression analysis. 
These techniques have long proved of value to the practicing statistician and it 
stands to the credit of sampling theory that it has produced them. It is also 
encouraging to know that parallel procedures may, with at least equal facility, 
be derived using Bayes' theorem. 
~ow, practical employment of such 
techniques has uncovered further inferential problems, and attempts to solve 
these, using sampling theory, have had only partial success. 
One of the main 
objectives of this book, pursued from Chapter 3 onwards, is to study some of these 
problems from a Bayesian viewpoint. In this we have in mind that the value of 
Bayesian analysis may perhaps be judged by considering to what extent it 
supplies insight and sensible solutions for what are known to be awkward 
problems. 
The following are examples of the further problems considered: 
I. How can inferences be made in small samples about parameters for which 
no parsimonious set of sufficient statistics exists? 
2. To what extent are inferences about means and variances sensitive to 
departures from assumptions such as error Normality, and how can such 
sensitivity be red uced ? 
3. How should inferences be made about variance components? 
4. How and in what circumstances should mean squares be pooled in the analysis 
of variance? 
, 
vii 

viii 
Preface 
5. How can information be pooled from several sources when its precIsion IS 
not exactly known, but can be estimated, as, for example, in the "recovery 
of interblock information" in the analysis of incomplete block designs? 
6. How should data be transformed to produce parsimonious parametrization 
of the model as well as to increase sensitivity of the analysis? 
The main body of the text is an investigation of these and similar questions 
with appropriate analysis of the mathematical results illustrated with numerical 
examples. We believe that this (I) provides evidence of the value of the Bayesian 
approach , (2) offers useful methods for dealing with the important problems 
specifically considered and (3) equips the reader with techniques which he can 
apply in the solution of new problems. 
There is a continuing commentary throughout concerning the relation of the 
Bayes results to corresponding sampling theory results. 
We make no apology 
for this arrangement. In any scientific discussion alternative views ought to be 
given proper consideration and appropriate comparisons made. 
Furthermore, 
many readers will already be familiar with sampling theory results and perhaps 
with the resulting problems which have motivated our study. 
This book is principally a bringing together of research conducted over the 
years at Wisconsin and elsewhere in cooperation with other colleagues, in 
particular David Cox, Norman Draper, David Lund, Wai-Yuan Tan, and 
Arnold Zellner. 
A list of the consequent source references employed in each 
chapter is given at the end of this volume. 
An elementary knowledge of probability theory and of standard sampling 
theory analysis is assumed, and from a mathematical viewpoint, a knowledge 
of calculus and of matrix algebra. 
The material forms the basis of a two-
semester graduate course in Bayesian inference; we have successfuJly used 
earlier drafts for this purpose. Except for perhaps Chapters 8 and 9, much of the 
material can be taught in an advanced undergraduate course. 
We are particularly indebted to Fred Mosteller and James Dickey, who 
patiently read our manuscript and made many valuable suggestions for its 
improvement, and to Mukhtar Ali, Irwin Guttman, Bob Miller, and Steve 
Stigler for helpful comments. We also wish to record our thanks to Biyi Afonja, 
Yu-Chi Chang, William Cleveland, Larry Haugh, Hiro Kanemasu, David Pack, 
and John MacGregor for help in checking the final manuscript, to Mary Esser 
for her patience and care in typing it, and to Greta Ljung and Johannes Ledolter 
for careful proofreading. 
The work has involved a great deal of research which has been supported 
by the Air Force Office of Scientific Research under Grants AF-AFOSR-llS8-66, 
AF-49(638) 1608 and AF-AFOSR 69-1803, the Office of Naval Research 
under Contract ONR-N-OOOI4-67-A-0128-0017, the Army Office of Ordnance 
Research 
under Contract DA-ARO-D-3J-124-C917, 
the National Science 
Foundation under Grant GS-2602, and the British Science Research Council. 

Preface 
ix 
The manuscript was begun while the authors were visitors at the Graduate 
School of Business, Harvard University, and we gratefully acknowledge support 
from the ford foundation while we were at that institution. 
We must also 
express our gratitude for the hospitality extended to us by the University of Essex 
in England when the book was nearing completion. 
We are grateful to Professor E. S. Pearson and the Biometrika Trustees, to the 
editors of Journal oj the Americall Statistical Association and Journal oj the Royal 
Statistical Society Series E, and to our coauthors David Cox, Norman Draper, 
David Lund, Wai-Yuan Tan, and Arnold Zellner for permission to reprint con-
densed and adapted forms of various tables and figures from a,rticles listed in the 
principal source references and general references. 
We are also grateful to 
Professor O. L. Davies and to G . Wilkinson of the Imperial Chemical Industries, 
Ltd., for permission to reproduce adapted forms of Tables 4.2 and 6.3 in 
Statistical Methods in Research and Production, 3rd edition revised, edited by 
O. L. Davies. 
We acknowledge especial indebtedness for support throughout the whole 
project by the Wisconsin Alumni Research Foundation, and particularly for their 
making available through the University Research Committee the resources of the 
Wisconsin Computer Center. 
/vl adison, Wisconsin 
August 1972 
G.E.P.B. 
G.c.T. 

" 
~ 
-
- -- ---- ----- - -----
- - - - - -
-
-
-
- -
- -
-
-

Chapter 1 
1.1 
1.2 
1.3 
1.4 
1.5 
1.6 
1.7 
Chapter 2 
2.1 
CONTENTS 
Nature of Bayesian Inference 
Introduction and summary 
l.l.l 
-::-he role of statistical methods in scientific investigation 
1.1.2 
Statistical inference as one part of statistical analysis . 
1.1.3 
Th.:question of adequacy of assumptions . 
1.1.4 
An iterative process of model building in statistical analysis 
1. 1.5 
The role of Bayesian analysis 
Nature of Bayesian inference 
1.2.1 
Bayes'theorem 
1.2.2 
Application of Cayes' theorem with probability interpreted 
frequencies 
1.2.3 
1.2.4 
1.2.5 
Application of Bayes' theorem with subjective probabilities 
Bayesian decision problems . 
Application of Bayesian analysis to scienti fic inference 
Noninformative prior distributions. 
1.3.1 
The Normal mean 8((J2 known) 
1.3.2 
The 
ormal standard deviation (J ce known) 
1.3.3 
bact data translated likelihoods and noninformative priors 
1.3.4 
Approximate data translated likelihood. 
1.3.5 
Jeffreys' rule, information measure, and non informative priors 
1.3.6 
Noninformative priors for multiple parameters 
1.3.7 
Noninformative prior distributions: A summary 
Sufficient statistics 
1.4.1 
Relevance of suffic ient statistics in Bayesian inference . 
1.4.2 
An example using the Cauchy distribution 
Constrai nts on parameters 
Nuisance parameters. 
1.6.1 Application to robustness studies 
1.6.2 
Caution in integrating out nuisance paramete rs 
Systems of inference . 
1.7.1 
Fiducial inference and likelihood inference. 
as 
Appendix Al.I 
Combination of a Normal prior and a Normal 
likelihood . 
Standard Normal Theory Inference Problems 
Tntroduction 
2. 1.1 
The Normal distribution. 
2.1.2 
Common Normal-theory problems 
2.1.3 
Distributional assumptions 
xi 
1 
4 
5 
6 
7 
9 
10 
10 
12 
14 
19 
20 
25 
26 
29 
32 
34 
41 
46 
58 
60 
63 
64 
67 
70 
70 
71 
72 
73 
74 
76 
77 
79 
80 

xii 
Contents 
2.2 
Inferences concerning a single mean from observations assuming com-
mon known variance . 
82 
2.2.1 
An example 
82 
2.2.2 
Bayesian intervals 
84 
2.2.3 
Parallel results from sampling theory 
85 
2.3 
Inferences concerning the spread o[ a Normal distribution from observa-
tions having common known mean. 
. 
. 
. 
. 
. 
. 
86 
2.3.1 
The inverted /, inverted I., and t he log I. d iSlri but ions . 
87 
2.3.2 
Inferences about the spread of a Normal distribution 
89 
2.3.3 
An example 
90 
2.3.4 
Relationship to sampling theory results 
92 
2.4 
Inferences when both mean and standard deviation are unknown 
92 
2.4.1 
An~ampk 
~ 
2.4.2 
Component distributions of pce, 
(J i y) 
. 
95 
2.4.3 
Posterior intervals for e 
97 
2.4.4 
Geometric interpretation of the derivation of p(O I y) 
98 
2.4.5 
Informative prior distribution of (J 
. 
99 
2.4.6 
Effect of changing the metric of (J [or locally uniform prior 
101 
2.4.7 
Elimination of the nuisance parameter (J in Bayesian and 
sampling theories. 
102 
2.5 
Inferences concerning the difference between two means. 
103 
2.S.1 Distribution of O2 -
OJ when aT = 0'1 
. 
103 
2.5.2 
Distribution of82 -
8 1 when crT and cr~ are not assumed equal 
104 
2.5.3 
Approximations to the Behrens-Fisher distribution 
107 
2.5.4 
An example 
107 
2.6 
Inferences concerning a variance ratio 
109 
2.6.1 
H.P.D . intervals 
110 
2.6.2 
An example 
111 
2.7 
Analysis of the linear model 
. 
. 
113 
2.7.1 Variance 0'2 assumed known . 
115 
2.7.2 
variancecr 2 unknown 
116 
2.7.3 
An example 
118 
2.8 
A general discussion of highest posterior density regions 
122 
2.8.1 
Some properties of the H.P.D. region 
123 
2.8.2 
Graphical representation 
124 
2.8.3 
Is 8 0 inside or outside a given H .P. D. region? 
J 25 
2.9 H.P.D. Regions for the linear model : a Bayesian justification of 
analysis of variance 
125 
2.9.1 The weighing example 
126 
2.10 Comparison of parameters . 
127 
2.11 
Comparison of the means of k >...;ormal populations 
127 
2.11.1 Choice of linear contrasts 
128 
2.11.2 Choice oflinear contrasts to compare location parameters 
131 
2.12 
Comparison of the spread o[ k distributions . 
132 
2.12.1 Comparison of the spread of k Normal populations 
132 
2.12.2 Asymptotic distribution of M 
134 
2.12.3 Bayesian parallel to Bartlett's test 
136 
2.12.4 An example 
136 

Contents 
xiii 
2.13 
Summarized calculations of various posterior distributions 
Appendix A2.1 
Some useful integrals 
Appendix A2.2 
Stirling's series 
Chapter 3 
Bayesian Assessment of Assumptions 
1. Effect of Non-Normality on Inferences about a Population Mean with 
Generalizations 
138 
144 
146 
3.1 
Introduction 
149 
3.1.1 
Measures of distributional shape, describing certain types of non-
Normality. 
149 
3.1.2 
Situations where Normality would not be expected 
15J 
3.2 
Criterion robustness and inference robustness illustrated using Darwin's 
data 
152 
3.2.1 
A wider choice of the parent distribution 
156 
3.2.2 
Derivation of the posterior distribution of B for a specific sym-
3.2.3 
3.2.4 
3.2.5 
3.2.6 
3.2.7 
3.2.8 
metric parent . 
Properties of the posterior distribution of B for a fixed [3 
. 
Posterior distribution of Band [3 when [3 is regarded as a random 
variable 
Marginal distribution of [3 
Marginal distribution ofB 
Information concerning the nature of the parent distribution 
coming from the sample . 
Relationship to the general Bayesian framework for robustness 
studies . 
3.3 
Approximations to the posterior distribution pCB I [3, y) 
3.3.1 
Motivation for the approximation 
3.3.2 Quadratic approximation to M(A) 
3.3.3 
Approximation of pCB I y) 
3.4 Generalization to the linear model 
3.4.1 
An illustrative example 
160 
162 
164 
166 
167 
169 
169 
170 
170 
172 
174 
176 
178 
3.5 
Further extension to nonlinear models 
186 
3.5.1 
An illustrative example 
187 
3.6 Summary and discussion 
193 
3.7 
A summary of formulas for posterior distributions 
196 
Appendix A3.1 
Some properties of the posterior distribution pCB I [3, y) 
199 
Appendix A3.2 
A property of locally uniform distributions 
Chapter 4 
Bayesian Assessment of Assumptions 
2. Comparison of Variances 
201 
4.1 
Introduction 
203 
4.2 
Comparison of two variances 
203 
4.2.1 
Posterior distribution of V = (J~ /(Jr for fixed values of (AI' B2 , [3). 
204 
4.2.2 
Relationship between the posterior distribution p(V I jJ, e, y) and 
sampling theory procedures 
206 
4.2.3 
Inference robustness on Bayesian theory and sampling theory 
208 

xiv 
Contents 
4.2.4 
4.2.5 
4.2.6 
4.2.7 
4.2.8 
4.2.9 
4.2.10 
Posterior distribution of a~ /aI when f3 is regarded as a random 
variable 
Inferences about V with /3 eliminated 
Posterior distribution of V when 8 1 and 82 are regarded as 
random variables . 
Computational 
procedures 
for 
the posterior distribution 
p(V I /3, y) . 
Posterior distribution of V for fixed f3 with 81 and 82 eliminated 
Marginal distribution of /3 
Marginal distribution of V 
209 
210 
211 
213 
215 
217 
218 
4.3 
Comparison of the variances of k distributions 
219 
4.3.1 
Comparison of k variances for fixed values of (9, /3) 
219 
4.3.2 
Posterior distribution of f3 and ¢ 
224 
4.3.3 
The situation when 0" .. . , Ok are not known 
228 
4.4 
Inference robustness and criterion robustness. 
232 
4.4.1 
The analyst example . 
233 
4.4.2 
Derivation of the criteria. 
235 
4.5 
A summary of formulae for various prior and posterior distributions. 
236 
Appendix A4.1 
Limiting distributions for the variance ratio V when /3 
approaches -1 
241 
Chapter 5 Random Effect Models 
5.1 
Introduction 
5.1.1 
The analysis of variance tabk 
5.1.2 Twoexamples. 
5. 1.3 
Difficulties in the sampling theory approach 
5.2 Bayesian analysis of hierarchical classifications with two variance 
244 
245 
246 
248 
components 
249 
5.2.1 
The likelihood function 
250 
5.2.2 Prior and posterior distribution of (8, at, a~) 
251 
5.2.3 
Posterior distribution of a~/aI and its relationship to sampling 
theory results . 
253 
5.2.4 
Joint posterior distribution of a I and a~ 
255 
5.2.5 
Distribution of aT' . . . . . . . . . 
258 
5.2.6 
A scaled X - 2 approximation to the distribution of aT 
261 
5.2.7 
A Bayesian solution to the pool ing dilemma 
264 
5.2.8 
Posterior distribution ofa~ . 
266 
5.2.9 
Computation ofp(a~ I y) 
268 
5.2.10 A summary of approximations to the posterior distributions of 
(aT,a~) 
273 
5.2.11 Derivation of the posterior distribution of variance components 
using constraints Jirectly . .. 
.... .. 
. 273 
5.2.12 Posterior distribution of aL and a scaled X- 2 approximation 
274 
5.2.13 Use of features of posterior distribution to supply sampling 
theory estimates 
276 
5.3 
/\ three-component hierarchical design model 
5.3.1 
The likelihood function 
5.3.2 
Jo int posterior distribution of (uT, a~, aD 
5.3.3 
Posterior distribution of the: ratio aT 2WT 
276 
277 
278 
280 

Contents 
xv 
5.3.4 
Relative contribution of variance components 
283 
5.3.5 
Posterior distribution of O"T 
286 
5.3.6 
Posterior distribution of O"~ 
290 
5.3.7 
Posterior distribution of 0"5 
290 
5.3.8 
A summary of the approximations to the posterior distributions 
of(O"T, O"~, aD 
291 
5.4 
Generalization to q-component hierarchical design model 
292 
Chapter 6 
6.1 
Appendix AS.l 
The postenor density of aT for the two component 
model expressed as a X - 2 series 
. 
294 
Appendix AS.2 
Some useful integral identities 
295 
Appendix AS.3 
Some properties of the posterior distribution of a~ for 
the two-component model 
296 
Appendix AS.4 
An asymptotic expansion of the distribution of a~ for 
the two-component model 
298 
Appendix AS.S 
A criticism of the prior distribution of (aT, O" ~) 
303 
Appendix AS.6 "Bayes" estimators 
304 
Appendix AS.7 
Mean squared error of the posterior means of 0"2p 
315 
Analysis of Cross Classification Designs 
Introduction 
6. 1.1 
A car-driver experiment : Three different classifications 
6.1.2 Two-way random effect model 
6.1 .3 
Two-way mixed model 
6.1.4 
Special cases of mixed models 
6. J.5 
Two-way fixed effect model 
317 
31 7 
320 
323 
325 
328 
6.2 
Cross classification random effect model 
329 
6.2. I 
The likelihood function 
. 
329 
6.2.2 
Posterior distribution of (O"~, 6,2, 0"1, 0"2) 
332 
6.2.3 
Distribution of (6~, al, 0";) 
333 
6.2.4 
Distribution of (O"J, at) 
334 
6.2.5 
An illustrative example 
335 
6.2.6 
A simple approximation to (he di st ribution of (O"J, 6J) . 
339 
6.3 
The additive mixed model 
341 
6.3.1 
The likelihood function 
341 
6.3.2 
Posterior distributions of (IJ, Q>, O"~, O"Ee) ignoring the con-
straint C 
344 
6.3.3 
Posterior distribution of a3 and O"E 
. 
345 
0.3.4 
Inferences about Jlxed effects fo r the mixed model 
346 
6.3.5 
Comparison of two means 
346 
6.3.6 
Approximations to the distribution peT I y) . 
351 
6.3.7 
Relationship to some sampling theory results and the problem of 
pooling 
353 
6.3.8 
Comparison of I means 
. 
357 
6.3.9 
Approximating the posterior distribution of V = m($)/me 
360 
6.3.10 Summarized calculations 
36 I 
6.4 
The interaction model 
362 
6.4.1 
The likelihood function 
362 
6.4.2 
Posterior distribution of ce, $, O"~, 0"]" 0";, c) . 
364 

xvi 
Contents 
6.4.3 
Comparison of I means . 
6.4.4 Approximating the distributionp(<!> [ y) 
6.4.5 
An example 
Chapter 7 
Inference about Means with Information from more than One Source: 
One-Way Classification and Block Designs 
365 
365 
367 
7.1 
Introduction 
369 
7.2 
Inferences about means for the ont:-way random effect model 
370 
7.2.1 
Various models used in the analysis of one-way classification 
370 
7.2.2 
Cse of the random effect model in the study of means . 
371 
7.2.3 Posterior distribution of 0 via intermediate distributions . 
372 
7.2.4 Random effect and fixed effect models for inferences about means 
377 
7.2.5 
Random effect prior versus fixed effect prior 
379 
7.2.6 
Effect of different prior assumptions on the posterior distribution 
oro 
383 
7.2.7 
Relationship to sampling theory results 
388 
7.2.8 
Conclusions 
390 
7.3 
Inferences about means for models with two random components 
391 
7.3.1 
A general linear model with two random components . 
393 
7.3.2 The likelihood function 
393 
7.3.3 
Sampling theory estimation problems associated with the model 
396 
7.3.4 
Prior and posterior distributions of (0";, O"t., 0) 
396 
7.4 
Analysis of balanced incomplete block designs 
397 
7.4. I 
Properties of the BIBD model 
397 
7.4.2 
A comparison of the one-way classification, RCBD and BIBD 
models. 
400 
7.4.3 
Posteriordistributionof!jl= (¢I""'¢J_I)'foraBlBD 
403 
7.4.4 
An illustrative example 
405 
7.4.5 
Posterior distribution of (J~. IO"~ . 
407 
7.4.6 Further properties of, and approximations to, p(!jl I y) 
409 
7.4.7 
Summarized calculations for approximating the posterior distri-
butions of!jl 
415 
7.4.8 
Recovery of interblock information in sampling theory 
417 
Appendix A7.1 
Some useful results in combining quadratic forms. 
418 
Chapter 8 Some Aspects of Multivariate Analysis 
8.1 
Introduction 
421 
8.1.1 
A general univariate model 
421 
8.2 
A general multivariate Normal model 
423 
8.2.1 
The likelihood function 
424 
8.2.2 
Prior distribution of (8, 1:.) 
425 
8.2.3 
Posterior distribution of (0, 1:.) 
427 
8.2.4 The Wishart distribution 
427 
8.2.5 
Posterior distribution of 0 
428 
8.2.6 
Estimation of common parameters in a nonlinear multivariate 
fuo~1 . 
4~ 
8.3 
Linear multivariate models 
435 
8.3.1 
The lise of linear theory approximations when the expectation is 
nonlinear in the parameters . 
436 
8.3.2 Special cases of the general linear multivariate model . 
437 

Contents 
xvii 
8.4 
Inferences about e for the case of a common derivative matrix X. 
438 
8.4.1 
Distribution ofe . 
439 
8.4.2 Posterior distribution of the means"from a m-dimensional Nor-
mal distribution . 
440 
8.4.3 
Some properties of the posterior matric-variate t distribution of 
e 
441 
8.4.4 H.P.D regions ofe 
448 
8.4.5 
Distribution of Vee) 
448 
8'.4.6 
An approximation to the distribution of V for general m . 
451 
8.4.7 Inferences about a general parameter point of a block sub-
matrix ofe 
452 
8.4.8 
An illustrative example . 
453 
8.5 
Some aspects of the distribution of L for the case of a common 
derivative matrix X . 
459 
8.5.1 
loint distribution of (9, I:) . 
460 
8.5.2 Some properties of the distribution of I: 
461 
8.5.3 
An example . 
464 
8.5.4 Distribution of the correlation coefficient PI2 . 
465 
8.6 A summary of formulae and calculations for making inferences about 
(9, I:) . 
470 
Appendix AS.l 
The lacobians of some matrix transformations. 
473 
Appendix AS.2 
The determinant of the information matrix of I:- J 
475 
Appendix AS.3 The normalizing constant of the t mk[a, (X'X) - J , A, v 1 
distribution 
475 
Appendix A8.4 The Kronecker product of two matrices 
477 
Chapter 9 Estimation of Common Regression Coefficients 
9.1 
Introduction: practical importance of combining information from 
different sources 
478 
9.2 The case of m-independent responses 
480 
9.2. I 
The weighted mean problem 
481 
9.2.2 Some properties of pee I y) . 
483 
9.2.3 
Compatibility of the means. 
485 
9.2.4 The fiducial distributions ore derived by Yates and by Fisher 
487 
9.3 
Some properties of the distribution of the common parameters for two 
independent responses . 
489 
9.3.1 
An alternative form for pee I y) . 
489 
9.3.2 Further simplifications for the distribution of9 
491 
9.3.3 
Approximations to the distribution ofe 
494 
9.3.4 An econometric example 
495 
9.4 
Inferences about common parameters for the general linear model with 
a common derivative matrix 
500 
9.5 
General linear model with common parameters. 
501 
9.5.1 
The case m = 2 . 
502 
9.5.2 Approximations to the distribution ofe for m = 2 
505 
9.5.3 An example . 
507 

xviii 
C()nten ~s 
9.6 
Chapter 10 
10.1 
10.2 
10.3 
10.4 
10.5 
10.6 
10.7 
A summary of various posterior distributions for common parameters 
511 
Appendix A9.1 
Asymptot ic expansion of the posterior distribution of 
e for two independent responses 
515 
Appendix A9.2 
Mixed cumulants of the two quadratic forms Q 1 
and Q2 
520 
Transformation of Data 
Introduction . 
522 
10.1.1 A factorial experiment on grinding 
523 
Analysis of the biological and the texti le data 
525 
10.2.1 Some biological data 
525 
10.2.2 The textile data . 
527 
Estimation of the transformation 
530 
10.3. J Prior and posterior di stributions of )_ 
531 
10.3.2 The simple power transformation 
533 
10.3.3 The biological example . 
534 
10.3.4 The texti Ie example . 
536 
10.3.5 Two parameter transformation 
538 
Analysis of the effects 0 after transformation 
539 
Further analysis of the biological data 
542 
10.5.1 Successive constraints 
545 
10.5.2 Summary of analysis 
548 
Further analysis of the textile data 
548 
A summary of formulae for various prior and posterior distributions 
550 
Tables 
References 
Author Index . 
Subject Index 
554 
571 
583 
585 

CHAPTER I 
NATURE OF BAYESIAN INFERENCE 
1.1 INTRODUCTION AND SUMMARY 
Opinion as to the value of Bayes' theorem as a basis for statistical inference 
has swung between acceptance and rejection since its publication in 1763. 
During periods when it was thought that alternative arguments supplied a 
satisfactory foundation for statistical inference Bayesian results were viewed, 
sometimes condescendingly, as an interesting but mistaken attempt to solve 
an important problem. When subsequently it was found that initially unsuspected 
difficulties accompanied the alternatives, interest was rekindled . 
Bayes' mode 
of reasoning, finally buried on so many occasions, has recently risen again with 
astonishing vigor. 
In addition to the present growing awareness of possible deficiencies in the 
alternatives, three further factors account for the revival. 
First, the work of a 
number of authors, notably Fisher, Jeffreys, Barnard, Ramsey, De Finetti, 
Savage, Lindley, Anscombe and Stein, has, although not always directed to that 
end, helped to clarify and overcome some of the philosophical and practical 
difficulties. 
Second, while other inferential theories had yielded nice solutions in cases 
where rather special assumptions such as \JormaJity and independence of errors 
,./ could be made, in other cases, and particularly where no sufficient statistics 
existed, the solutions were often unsati sfactory and messy. Although it is true 
that these special assumptions covered a number of situations of scientific 
interest, it would be idle to pretend that the set of statistical problems whose 
solution has been or will be needed b) the scientific investigator coincides with 
the set of problems thus amenable to convenient treatment. 
Data gathering 
is frequently expensive compared with data analysis. 
It is sensible then that 
hard-won data be inspected from many different viewpoints. 
In the selection 
of viewpoints, Bayesian methods allow greater emphasis to be given to 
scientific interest and less to mathematical convenience. 
Third, the nice solutions based on the ralher special assumptions have been 
popular for another reason- they were easy to compute. This consideration has 
much less force now that the desk calculator is no longer the most powerful 
instrument for executing statistical analysis. 
Suppose, using a desk calculator, 
it takes five hours to perform a data analysis appropriate to the assumption that 
errors are 1\ormal and independent, then the five hundred hours it might take 

2 
Nature of Bayesian Inference 
1.1 
to explore less restrictive assumptions could be prohibitive. By contrast, the u~'e 
of an electronic computer can so reduce the time base that, with gen~ral programs" 
available, the wider analysis can be almost as immediate and (!conomic as the 
more restricted one. 
. 
Scientific investigation uses statistical methods in an i eration in which 
I 
controlled data gathering and data analysis alternate. 
q>ata analysis is a 
subiteration in which inference from a tentatively entertaine~d model alternates 
with criticism of the conditional inference by inspection \ of residuals and 
other means. Statistical inference is th us only one of the res~onsi bili ties of the I 
statistician. 
It is however an important one. 
Bayesian infere)l~e alone seep1~ 
to offer the possibility of sufficient flexibility to allow reactio~" t.<Lsc·entific 
complexity free from impediment from purely technical limitation. 
A prior distribution, which is supposed to represent what is known about 
unknown parameters before the data is available, plays an important role in 
Bayesian analysis. Such a distribution can be used to represent prior knowledge 
or relative ignorance. In problems of scientific inference we would usually, were 
it possible, like the data "to speak for themselves." 
Consequently, it is usually 
appropriate to conduct the analysis as if a state of relative ignorance existed 
a priori. 
In this book, therefore, extensive use is made of "noninformative" 
prior distributions and very little of informative priors. The aim is to obtain an 
inference which would be appropriate for an unprejudiced observer. The under-
standable uneasiness felt by some statisticians about the use of prior distributions 
is often associated with the fear that the prior may dominate and distort 
"what the data are trying to say." We hope to show by the examples in this book 
that, by careful choice of model structure and appropriate noninformative priors, 
Bayesian analysis can produce the reverse of what is feared. It can permit the data 
to comment on dubious aspects of a model in a manner not otherwise possible. 
The usefulness of a theory is customarily assessed by tentatively adopting it, 
and then considering whether its consequences agree with common sense, and 
whether they provide insight where common sense fails. 
It was in this spirit 
that some years ago the authors with others began research in applicati,ons of the 
Bayesian theory of inference. A series of problems were selected in the sol ution 
of which difficulties or inconsistencies had been encountered with other 
approaches. Because Bayesian analysis of these problems has see,med consistently 
helpful and interesting, we believe it is now appropriate to bring this and other 
related work together, and to consider its wider aspects. 
The objective of this book, therefore, is to explore Bayesian inference in 
statistical analysis. The book consists of ten chapters. Chapter 1 discusses the 
role of statistical inference in scientific investigation. 
In the light of that 
discussion the nature of Bayesian inference, including the choice of noninformative 
prior distributions, is considered. The chapter ends with an account of the role 
and relevance of sufficient statistics, and discusses the problem of nuisance 
parameters. 
--~---------

v 
1.1 
Introduction and Summary 
3 
In Chapter 2 a number of standard Normal theory inference problems 
concerning location and scale parameters are considered. 
Bayes' solutions are 
given which closely parallel sampling theory techniques+ associated with 
I-tests, F-tests, the analysis of variance and regression analysis. 
While these 
procedures have long proved valuable to practising statisticians, efforts to extend 
them in important directions using non-Bayesian theories have met serious 
difficulties. An advantage of the Bayes approach is that it can be used to explore 
the consequences of'any type of probability model, without restriction to those 
having special mathematical forms. Thus, in Chapter 3 the problem of making 
inferences about location parameters is considered for a wider class of parent 
probability models of which the Normal distribution is a member. 
In this 
framework, we show how it is possible to assess to what extent inferences about 
location parameters are sensitive to departures from Normality. 
Further, it is 
shown how we can use the evidence from the data to make inferences about the 
form of the parent distributions of the observations. The analysis is extended 
.~ 
in Chapter 4 to the problem of comparing variances. 
., 
Chapters 5 and 6 discuss various random effect and mixed models associated 
with hierarchical and cross classification designs. 
With sampling theory, one 
experiences a number of difficulties in estimating means and variance components 
in these models. 
Notably one encounters problems of negative variance 
estimates, of eliminating nuisance parameters, of constructing confidence 
intervals, and of pooling variance estimates. 
Analysis, from a Bayesian 
standpoint, is much more tractable, and in particular provides an interesting and 
sens)ble solution to the pooling dilemma . 
Chapter 7 deals with two further important problems in the analysis of 
variance. The first concerns the estimation of means in the one-way classification. 
When it is sensible to regard such means as themselves a sample from a 
population, the appropriate Bayesian analysis shows that there are then /11'0 
sources of information about the means and appropriately combines them. The 
chapter ends with a discllssion of the recovery of interblock information in the 
balanced incomplete block design model. 
This is again a problem in which 
two sOllrces of information need to be appropriately combined and for which the 
sampling theory solution is 
unsatisfactory. 
In Chapters 8 and 9 a general treatment of linear and nonlinear Normal 
multivariate models is given. 
While Bayesian results associated with standard 
linear models are discussed , particular attention is given to the problem of 
estimating common location parameters from several equations. 
The latter 
problem is of considerable practical importance, but is difficult to tackle bysampling 
theory methods, and has not previously received much attention. 
t We shall assume in this book that the reader has some familiarity with standard 
ideas of the sampling theory approach explained for example in Mood and Graybill 
(J 963) and Hogg and Craig (J 970). 

4 
Nature of Bayesian Inference 
I.l 
''-, 
Finally, in Chapter 10, we consider the important problem of data 
transformation fro m a 
Bayesian viewpoint. 
The problem is to\ select a 
transformation which, so far as possible, achieves N ormality, homoge,niety of 
variance, and simplicity of the expectation function in the transformed Jvariate. 
A bald statement of a mathematical expression , however correct, fri quently 
fails to produce understanding. Many Bayesian results are of particuly interest 
because they seem to provide a kind of higher intuition. Mathematical results 
which at first seemed puzzling have later been seen to 
'ro~ 
maturer kind 
of common sense. 
For this 
reason , throughout this 
book, individual 
mathematical formul ae are carefully analyzed and illustrated with examples 
and diagrams. 
Also, appropriate approximations are developed when they 
provide deeper understanding of a situation, or where they simp!Jfy calculation. 
For the convenience of the reader a number of short summaries of formulas 
and calculations are given in appropriate places. 
)<1.1.1 The Role of Statistical Methods in Scientific Investigation 
Statistical methods are tools of scientific investigation. 
Scientinc investigation 
is a controlled learning process in \\ hich various aspects of a problem are 
illuminated as the study proceeds. It can be thought of as a major iteration within 
which secondary iterations occur. 
The major iteration is that in which a 
tentati ve conjecture suggests a n experiment, appropriate analysis of the data 
so generated leads to a modi ned conjecture, and this in turn leads to a new 
experiment, and so on. An idealization of this process is seen in Fig. 1.1 . 1, involving 
an alternation between conjecture and experiment carried out via experimental 
design and data ana/ysis.t 
As indicated by the Lig-zag line at the bottom of the 
ngure, most investigations involve not one but a number of alternations of this 
kind. 
An efficient investigation is one where convergence to the objective oc~urs 
as quickly and unambiguously as possible. 
A basic determinant of efficiency, 
which we must suppose is outside the control of the statistician, is the 
originality, imagination, and subject matter knowledge of the investigator. Apart 
from this vital determining factor, however, efficiency is decided by the 
appropriateness and force of the methods of design and analysis employed. 
In moving from conjecture to experimental data, (D), experiments must be 
designed which make best use of the experimenter's current state of knowledge 
and which best illuminate his conjecture. 
In moving from data to modined 
conjecture, (A), data must be analyzed so as to accurately present information in 
a manner which is readily understood by the experimenter. 
t The words design and experiment are broadly interpreted here to refer to any data 
gathering process. 
In an economic study, a conjecture might lead the investigator to 
study the functional relationship between money supply and interest rate. The difficult 
decision as to what types of money supply and interest rate data to use, here constitutes 
the design. In social studies a particular sample survey might be the experiment. 

J.l 
Conjec[ure 
Ex pe riment 
D~A 
Co njecture / 
Design 
Analysis 
Introduction and Summary 
Experiment 
(Data) 
5 
Fig. 1.1.1 Tterative process of scientific investigation (the alternation between conjecture 
and experiment), 
A full treatise on the use of statistical methods in scientific investigation 
therefore would necessarily include consideration of statistical design as well 
as statistical analysis. The aims of this book are, however, much more limited. 
We shall not discuss experimental design , and will be concerned only with one 
~spect of statistical analysis, namely, statistical inference, 
1.1.2 Statistical Inference as one Part of Statistical Analysis 
For illustration, suppose we were studying the useful life of batteries produced 
by a particular machine, It might be appropriate to assume tentatively that the 
observed lives 
of batleries coming from 
the 
machine were distributed 
independently and I\ormally about some mean f:) with variance () 2 . The proba-
bility distribution of a projected sample of 11 observations y' = (Y I' .'" .1'11) 
would then be 
p(YI8,0'2) X 0' "exp r __ 
1_2 f (y, - wl, 
l 2() 
t = 1 
-
ct) < Yt < w , ( I. I. I ) 
Given the value of the parameters () and () 2, this expression permits the 
calculation of the probability density p(y : 0,0'2) associated with any hypothetical 
data set y before any data is taken. For statistical analysis this is, in most cases, 
the converse of what is needed. The analyst already has the data but he does not 
know 0 and ()2 
He can, however, use p(y I e, ()2) indirectly to make inferences 
about the values of () and (f 2, given the 11 data values, 
Two of the methods by which this may be attempted employ 
'\I a. 
Sampling Theor , ., 
b. 
Bayes' T,Veorem, 
" 
We now giv,e a brief descripti6p of each of these approaches using tbe l\ormal 
probability m<7del (1.1.1) for illustration , 
Sampling Thehry Approach 
In this approach inferences are made by directing attention to a reference set of 
hypothetical data vectors Y l' Y2, .. , Yj, , .. which could have been generated 
by the probability model p(y eo , (f ~) of (l.l, 1), where 80 and 
(f~ are the 

6 
"'Iature of Bayesian Inference 
1.1 
hypothetical true values of 8 and (52 
Estimators B(y) and 0-2 (y), which are 
functions of the data vector y, are selected, 
By imagining values B(y) and 
0-2(y) to be calculated for each hypothetical data vector yj' reference sets are 
generated for B(y) and 0-2(y). Inferences are then made by comparing the values 
of B(y) and 0-2(y) actually observed with their "sampling distributions" generated 
by the reference sets, 
The functions B(y) and 0-2(y) are usually chosen so that the sampling distribu-
tions of the extima tors B(y) and 0-2(y) are, in some sense, concentrated as closely 
as possible about the true values 80 and (50' To provide some idea of how far away 
from the true values the calculated quantities B(y) and 0-2(y) might be, 
confidence intervals are calculated. 
For example, the 1 -
CI. confidence interval 
for 8 would be of the form 
til (y) < 8 < 82 (y), 
where al(y) and a2(y) would be functions of y, chosen so that in repeated 
sampling the computed confidence intervals included the value 80 , a proportion 
1 -
CI. of the time. 
Bayesian Approach 
In a Bayesian approach, a different line is taken, As part of the model a prior 
distribution peG, (52) is introduced. This is supposed to express a state of knowledge 
or ignorance about 8 and (52 before the data are obtained. 
Given the prior 
distribution, the probability model p(y 18, (52) and the data y, it is now possible 
to calculate the probability distribution peG, (521 y) of 8 and (52, given the data y, 
This is called the posterior distribution of 8 and (52. From this distribution inferences 
about the parameters are made. 
1.1.3 The Question of Adequacy of Assumptions 
Consider the battery-life example and suppose n = 20 observations are available. 
Then, whichever method of inference is used, conditional on the assumptions 
we can summarize all the information in the 20 data values in terms of inferences 
about just 111'0 parameters, 8 and (52. 
The inferences are, in particular, conditional on the adequacy of the probability 
model in (1.1.1). It is not difficult, however, to imagine situations in which this 
model, and therefore the associated inferences, could be inadequate. It might 
happen, for example, that during the period of observation, a quality characteristic 
x of a chemical additive, used in making the batteries, could vary and could cause, 
via an approximate linear relationship, a corresponding change in the mean life 
time of the batteries. In this case, a more appropriate model might' be 
-
co < y, < co. 
(1.1.2) 

1.1 
Introduction and Summary 
7 
Alternatively, it might be suspected that the first battery of a production rlln was 
always faulty, in which case a more adequate model could be 
p(y . (]~, (]2, 8 I, 8) cc 0- ~ I (] -19 ex p [- 2:~ (y I -
8 J )2 - 2: 2 '~2 (y, -
8) 21 ' 
-
(jJ < y, < oc . 
( 1.1.3) 
Again it could happen that successive observations were not distributed indepen-
dently but followed some time series, or it might be that their distribution was 
highly non-Normal. The reader will have no difficulty in inventing many other 
situations that might arise and the probability models that could describe them. 
Clearly the inferences which can be made wiiJ depend upon which model is 
selected. Whence it is seen that a basic dilemma exists in all statistical analysis. 
Such analysis implies the summarizing of information contained in a body of data 
via a probability model containing a minimum of parameters. We need such a 
summary to see clearly, and so to make progress, but if the model were inappropri-
ate the summary could distort and exclude relevant information. 
1.1.4 An Iterative Process of Model Building in Statistical Analysis 
Because we can never be sure that a postulated model is entirely appropriate, 
we must proceed in such a manner that inadequacies can be taken account of 
and their implications considered as we go along. To do this we must regard 
sl<1tistical analysis, which is a step in the major iteration of Fig. 1.1. J, as itself 
an iteration. To be on firm ground we must do more than merely postulate a 
model; we must build and test a tentative model at each stage of the investigation. 
Entertained 
model 
Inference 
- ---. Conditional 
analysis 
Criticism 
Fig. 1.1.2 Statistical analysis of data as an iterative process of model building. 
Only when the analyst and the investigator are satisfied that no important fact 
has been overlooked and that the model is adequate to the purpose, should it be 
lIsed to further the major iteration. The iterative model building processt taking 
place \\'ithin a statistical analysis is depicted in Fig. 1.1.2. 
The process usually begins by the postulating of a model worthy to be 
tentatively entertained. The data analyst will have arrived at this tentative model 
t A fuller discllssion is found, for example, in Box Clnd Jenkins (1970), where in the 
context of time series analysis the steps in this iteration are discussed in terms of model 
identification, model fitting, and mod 
diagnostic checking. 

8 
:--:ature of [3ayesian Inference 
1.1 
in cooperation with the scientific investigator. They will choose it 'So that, in the 
light of the then available knowledge, it best takes account of relevant phenomena 
in the simplest way possible. 
it will usually contain unknown parameters. 
Given the data the analyst can now make statistical inferences about the 
parameters conditional on the correctness of this first tentative model. 
These 
inferences form part of the conditional analysis. 
If the model is correct, they 
provide all there is to know about the problem under study, given the data. 
Up to now the analyst has proceeded as if he believed the model absolutely. 
He now changes his role from tentative sponsor to tentative critic and broadens his 
analysis with cumputations throwing light on the question: " Is the model ade-
yuate?" Residual quantities are calculated which, while they would contain no 
information if the tentative model were true, could suggest appropriate 
modifications if il were false . Resulting speculation as to the appropriateness 
of the initially postulated model and possible need for modification, again 
conducted in cooperation with the investigator, may be called model criticiSI11.t 
For example, suppose the l\ormal probability model of (1.1.1) was initially 
postulated, and a sample of 20 successive observations were taken from a 
production run. These would provide the data from which, conditional on the 
model, inferences could be made about e and 0'2. 
An effective way of criticizing the adequacy of the assumed model (1.1.1) 
employs what is called "an analysis of residuals." 
Suppose for the moment that 
(J and 0'2 were known; then if the model (1 .l.l) were adequate, the quantities 
U 1 = (Yl - 8)'0', ... , U, = (y, - 8)/0', ... would be a random sample from a 
Normal distribution with zero mean and unit variance. Such a sequence would by 
itself be informationless, and is sometimes referred to as white noise. 
Thus, a 
check on model adequacy would be provided by inspection of the quantities 
y,-O=u~a, 1= 1,2, .. . 
Any suggestion that these quantitIes were nonrandom, or 
that they were related to some other known variable, could provide a hint that the 
entertained model (l.l.l) should be modified . 
In practice, e would be unknown but we could proceed by substituting the 
sample mean y. The resulting quantities r, = y, - y, t = 1,2, ... , would for this 
example be the residuals. If, for example, they seemed to be correlated with the 
amount of additive x" this would suggest that a model like (1.1.2) might be more 
appropriate. This new model might then b~ entertained, and the iterative process 
of Fig. 1.1.2 repeated. 
Useful devices for model criticism have been proposed, in particular by 
Anscombe (1961), Anscombe and Tukey (1963), and Daniel (1959). Many of [hese 
involve plotting residuals in various ways. However, these techniques are not part 
of statistical inference as we choose to consider it, but of model criticism which is 
an essential adjunct to inference in the adaptive process of data analysis depicted 
in Fig. 1.1.2. 
This apt term is due to Cuthbert Daniel. 

J.i 
Introduction and Summary. 
9 
1.1.5 The Role of Bayesian Analysis 
The applications of Bayes' theorem which we discuss, therefore, are examples of 
statistical inference. While inference is only a part of statistical analysis, which is 
in turn only a part of design and analysis, used in the investigatory iteration, 
nevertheless it is an important part. 
Among different systems of statistical inference, that derived from Bayes' 
theorem will, we believe, be seen to have properties which make it particularly 
appropriate to its role in scientific investigation. [n particular: 
l. Precise assumption introduced on the left in Fig. 1.1.2 leads, via a leak proof 
route, to consequent inference on the right. 
2. It follows that, given the model, Bayesian analysis automatically makes use 
of all the information from the data. 
3. [t further follows that inferences that are unacceptable must come from 
inappropriate assumption and not from inadequacies of the inferential 
system. Thus all parts of the model, including the prior distribution, are 
exposed to appropriate criticism. 
4. Because this system of inference may be readily applied to any probability 
model, much less attention need be given to the mathmetical convenience of 
the models considered and more to their scientific merit. 
5. Awkward problems encountereu in sampling theory, concerning choice of 
estimators and of confidence intervals, do not arise. 
6. Bayesian inference provides a satisfactory way of explicitly introducing and 
keeping track of assumptions about prior knowledge or ignorance. It should 
be recognized that some prior knowledge is employed in all inferential systems. 
For example, a sampling theory analysis . using (1..1.1) is made, as is a 
Bayesian analysis, as if it were believed a priori that the probability distribution 
of the data was exactly 1\0rmal, and that each observation had exactly the 
same variance, and was distributed exactly independently of every other 
observation. But after a study of residuals had suggested model inadequacy, 
it might be desirable to reanalyse the data in relation to a less restrictive 
model into which the initial model was embeded. 
[f non-Normality was 
suspected, for example, it might be sensible to postulate that the sample came 
from a wider class of parent distributions of which the Normal was a member. 
The consequential analysis could be difficult via sampling theory but is readily 
accomplished in a Bayesian framework (see Chapters 3 and 4). 
Such an 
analysis allows evidence/i'om the dala to be taken into account about the form 
of the parent distribution besides making it possible to assess to what extent 
the prior assumption of exact :--.Iormality is justified. 
The above introductory survey suggests that Bayes' theorem provides a system 
of statistical inference suited to iterative model building, which is in turn 
f 

• 
]0 
"Iat'ure of Bayesian Inference 
1.2 
an essential part of scientific investigation. On the other hand, we have pointed 
out that statistical inference (Bayesian or otherwise) is only a part of statistical 
method . It is, we believe, equally unhelpful for enthusiasb to seem to claim that 
Bayesian analysis can do everything. as it is for its detractors to seem to assert 
that it can do nothing. 
1.2 "'ATURE OF BA YESJA"i INFERENCE 
/ \ 1.2.1 Bayes' Theorem 
\ Suppose that y' = (Y1 , ... , y ,,) is a vector of n observations whose probability 
distribution pCy I a) depends on tbe values of k parameters e' = (8 1, ... , Ok)' 
Suppose also that a itself has a probility distribution p(9). Then, 
p(y i a)p(9) = p(y, 9) = p(9 I y)p(y). 
Given the observed data y, the conditional distribution of 0 is 
(9 I ) = p(y I 9)p(9) . 
p 
y 
pCy) 
Also, we can write 
I r pCy I 9)p(e) de 
p(y) = E p(y I 9) = c" 1 = . 
\ L p(y : 9)p(e) 
9 continuous 
9 discrete 
(1.2.1) 
( 1.2.2) 
(1.2.3) 
wbere the sum or the integral is taken over tbe admissible range of 9, and where 
E[f(a)] is the mathematical expectati on of f(9) with respect to the distribution 
p(9). 
Thus we may write (1.2.2) alternatively as 
p(a I y) = cp(y I 9)p(e). 
( 1.2.4) 
The statement of (1.2.2), or its equivalent (1.2.4), is usually referred to as Bayes' 
theorem . In this expression, p(9), which tells us what is known about 9 without 
knowledge of the data, is caJled the prior distribution of 9, or the distribution of 
a a priori. Correspondingly, pee I y), which tells us what is known about a given 
knowledge of the data, is called the posterior distribution of e given y, or the dis-
tribution of 9 a posteriori. The quantity c is merely a "normalizing" constant 
necessary to ensure that the posterior distribution pee i y) integrates or sums to one. 
I n what follows we sometimes refer to the prior distri bution and the 
posterior distribution simply as the "prior" and the "posterior", respectively. 
Bayes' Theorem and the Likelihood Function 
I\ow given the data y,p(y I a) in (1.2.4) may be regarded as a function not of y 
but of a. When so regarded , following Fisher (1922), it is called the likelihood 
function of a for given y and can be written t(e I y). We can thus write Bayes' 
formula as 
p(9 I y) = t(a I y)p(9). 
(1.2.5) 

"Iature of Bayesian Inference 
11 
Tn other \'.urds, then , Bayes' theorem tells us that the probability distribution for 
9 pClslerior to the data y is proportional to the product of the distribution for 9 
priC'r ;0 the data and the likelihood for 9 given y. That is, 
posterior distribution oclikelihood x prior distribution. 
The likelihood function 1(9 [y) plays a very important role in Bayes' formula. 
II is the function through which the data y modifies prior knowledge of 9; it can 
:herefore be regarded as representing the information about 9 coming from the 
data. 
The likelihood function is defined up to a multiplicative constant, that is, 
multiplication by a constant leaves the likelihood unchanged. This is in accord 
with the role it plays in Bayes' formula, since multiplying the likelihood functi on 
by an arbitrary constant will have no effect on the posterior distribution of 9. 
The constant will cancel upon normalizing the product on the right hand side of 
(1.2.5). 
It is only the relative value of the likelihood which is of importance. 
The Standardized Likelihood 
. 
When the integral J 1(9 [ y) d9, taken over the admissible range of 9, IS finite, 
then occasionally it will be convenient to refer to the quantity 
( 1.2.6) 
We shall call this the standardized likelihood, that is, the likelihood scaled so that 
the area, volume, or hypervolume under the curve, surface, or hypersurface, is one. 
Sequel7lial Nature of Bayes' Theorem 
The theorem in (1.2.5) is appealing because it provides a mathematical formulation 
of how previous knowledge may be comhined with new knowledge. lndeed , the 
theorem allows us to continually update information about a set of parameters 
9 as more observations are taken. 
Thus, suppose we have an initial sample of observations Yl' then Bayes' 
formula gives 
(1.2.7) 
Now, suppose we have a second sample of observations Y2 
distributed 
independently of the first sample, then 
p(9 [ Y2, Yl) oc p(9)/(9 [Yl)/(9[ )'2) 
( 1.2.8) 
. The expression (1.2.8) is precisely of the same form as (1.2.7) except that 
p(9 [ Y I), the posterior distribution for 9 gi ven Y I, plays the role of the prior 
distribution for the second sample. Obviously this process can be repeated any 

12 
I\ature of Bayesian Inference 
1.2 
number of times. 
In particular, if we have 11 independent observations, the 
posterior distribution can, if desired, be recalculated after each new observation, 
so that at the mth stage the likelihood associated with the mth observation is 
combined with the posterior distribution of e after m -
I observations to give 
the new posterior distribution 
m = 2, ... ,11 
(1.2.9) 
where 
Thus, Bayes' theorem describes, in a fundamental way, the process of learning 
from experience, and shows how knowledge about the stale of nature represented 
by 0 is continually modified as new data becomes available. 
1.2.2 Application of Bayes' Theorem with Probability Interpreted as Frequencies 
Mathematically, Bayes' formula is merely a statement of conditional probability, 
and as such its validity is not in question. 
What has been questioned is its 
applicability to general problems of scientific inference. The difficulties concern 
a. the meaning of probability, and 
b. the choice of, and necessity for, the prior distribution. 
Specific examples can be found of applications of Bayes' theorem where the 
probabilities involved may be directly interpreted in terms of frequencies and may 
therefore be said to be objective, and where the prior probabilities can be 
supposed exactly known. The validity of applications of this sort has not been in 
serious dispute. 
An example of this situation is described by Fisher (1959, 
p.19). In this example, there are mice of two colors, black and brown. The black 
mice are of two genetic kinds. homozygotes (BB) and heterozygotes (Bb), and 
the brown mice are of one kind (bb). It is known from established genetic theory 
that the probabilities associated with offspring from various matings are as 
follows: 
Table 1.2.1 
Probabilities for genetic character of mice offspring 
Mice 
BB mated with bb 
Bb mated with bb 
Bb mated with Bb 
BB (black) 
Bb (black) 
bb (brown) 
o 
o 
o 
~. 
.1. 
4 
Suppose we have a "test" mouse which is black and has been produced by a 
mating between two (Bb) mice. Using the information in the last line of the table, 
-
r.1._/:T . 
~ __________ 
. 
• 

~ature of i3ayesian fnference 
it is seen that, in this case, the prior probabilities of the test mouse being 
homozygous (BB) and heterozygous (Bb) are precisely known, and are i and} 
respectively. Given this prior information, Fisher supposed that the test mouse 
was now mated with a brown mouse and produced (by way of data) seven black 
offspring. One can then calculate, as risher did, the probabilities, posterior to the 
data. of the test mouse being homozygous (BB) and heterozygous (Bb) uSing 
Bayes' theorem. 
Specifically, if we use e to denote the test mouse being (BB) or (Bb), 
(BB) 
(Bb) 
then the priur knowledge is represented by the distribution 
pce = 0) = Pr(BB) = '1. 
pee = I) = Pr(Bb) =~. 
Further, letting y denote the offspring, we have the likelihood 
I(e = 0 1 y = 7 black) ex Pr(7 black 1 BB) = I, 
I(e = I 1 y = 7 black) x. Pr(7 black 1 Bb) = (}) 7 
It follows from (1.2.5) that 
p(O = 01 y = 7 black) x. ! , 
pee = J Iy = 7 black) (X (~ )(1)7 
Upon normalizing the posterior probabilities are then 
pee = 01 y = 7 black) = Pr(BB 17 black) = 
pee = I 1 y = 7 black) = I -
Pr(SB 1 7 black) = ti. 
which represent the posterior knowledge of the test mouse being (BB) or (Bb). 
We see that, given the genetic characteristics of the offspring, the mating results 
of 7 black offspring changes our knowledge considerably about the test mouse 
being (BB) or (Bb), from a prior probability ratio of 2:1 in favor of (Bb) to a 
posterior ratio of 64: I against it. 
As an illustration of the sequential nature of Bayes' theorem , suppose the 7 
black offspring are viewed as a sequence of seven independent observations; 
then, if we let y' = (Y1' "', Y7), the likelihood can be written 
where 
I(e 1 y = 7 black) = I(e [ .F! = black) ... Ice 1 Y~ = black) 
I(e [ Ym = black) C( {~ 
e = 0 
e=1 
m = I, ... ,7. 
Applying (1.2.9), the changes in the probabilities of the test mouse being (BB) 
or (Bb) after the mth observation, m = I, ... ,7, are given in Table 1.2.2. 

-
\="" 
14 
Nature of Bayesian Inference 
1.2 
Table 1.2.2 
Probabilities for the test mouse being homozygous and heterozygous 
Mice 
Probabilities 
e = 0 (BB) e = I (Bb) 
Initial 
I 
2 
3" 
:l 
1st black 
I 
I 
2 
'2 
2nd black 
2 
I 
3" 
3" 
3rd black 
4 
1 
5 
5 
4th black 
-~ 
.L 
9 
y 
5th black 
1 6 
I 
T"f 
f"1 
6th black 
3 2 
1 
J :l 
TI 
7th black · 
64 
-L 
TI 
6 5 
This shows the increasing certainty of the test mouse being (BB) as more and more 
,black offspring are observed. 
----- ~ Other applications of this sort are to be found in the theory of design of 
s ampling inspection schemes. See, for example, Barnard (1954). In these examples, 
all the probabilities, both prior and posterior, are objective in the sense that they 
may be given a direct limiting frequency interpretation and are, in principle, 
subject to experimental confirmation. 
In most scientific applications, however, exactly known objective prior dis-
tributions are rarely available. 
1.2.3 Application of Bayes' Theorem with Subjective Probabilities 
Following Ramsay (1931), De Finetti (1937), and Savage (1954, 1961a, b, 1962), 
we shall in this book regard probability as a mathematical expression of our 
degree of belief with respect to a certain proposition. In this context the concept 
of verification of probabilities by repeated experimental trials is regarded merely 
as a means of calibrating a subjective attitude. Thus, to say that one feels the 
probability is one half that Miss A and Mr. B will get married means that we 
have the same belief in the proposition" Mr. B will marry Miss A" as we would 
in the proposition "a toss of a fair coin will produce a head." We do not need to 
imagine an infinite series of situations in half of which A and B are wedded, and in 
half of which they are not. 
The actual elucidation of what is believed by a particular person can be 
attempted in terms of betting odds. If, for example, the value of a continuous 
parameter e is in question, we may, in suitable circumstances, infer an 
experimenter's prior distribution by asking at what value eo he would be 
prepared to bet at particular odds that e > eo. Given that a subjective probability 
distribution of this kind represents a priori what a person believes, then the 
posterior distribution obtained by combining this prior with the likelihood function 
shows how the prior beliefs are modified by information coming from the data. 
. ---- - - -----
. 

1.2 
Nature of Bayesian Inference 
15 
F.stimation of a Physical Constant 
"'0 consolidate ideas, we consider the example illustrated in Fig. 1.2.1. Suppose 
two physicists, A and B, are concerned \\oith obtaining more accurate estimates 
of some physical constant e, previously known only approximately. 
Suppose 
physicist A, being very familiar with this area of study, can make a moderately 
good guess of what the answer will be, and that his prior opinion about 0 can-be 
approximately represented by a 
ormal distribution centered at 900, with a 
standard deviation of 20. Thus 
1 
l' 
1 (e-900)2J 
P (8) = - -- exp --
-
- -
. 
A 
2IT 20 
. 
2 
20 
(1.2.10a) 
According to A, a priori 0 ~ N(900, 20 2) where the notation means that 0 is 
distributed Normally with "mean 900 and variance 20 2 " 
This would imply, in 
particular, that to A the chance that the value of 8 could differ from 900 by more 
than 40 was only about one in twenty. By contrast, we suppose that B has had little 
previous experience in this area ,and his rather vague prior beliefs are represented 
by the Normal distribution 
I 
[I (e - 800 )2] 
PB(8) = ,-,,2IT HO exp -2 
80 
. 
(1.2. lOb) 
Thus, according to B, 8 ~ N(800, 80 2). 
He centers his prior at 800 and is 
considerably less certain abouL 8 than A is. To B, a value anywhere between 700 
and 900 would certainly be plausible. 
The curves in Fig. 1.2.1 (a) labeJled 
PA(8) and Pn(8) show these prior distributions for A and B. 
Suppose now that an unbiased method of experimental measurement is 
availa ble and that an obsenation y made by this method, to a sufficient 
approximation, follows a Normal distribution with mean 8 and standard 
deviation 40, that is Y ~ N(e, 402 ) ./ If now a single observation Y is made, the 
standardized likelihood function is represented by a Normal curvet centered 
at y with standard deviation 40. Then we can apply Bayes' theorem to show how 
each man's opinion regarding 8 is modified by the 'information coming from that 
piece of data. 
If a priori e ~ N(8o, 0'6), and the standardized likelihood function is 
represented by a Normal curve centered at y with standard deviation 0', then it is 
t We refer to the function 
I(x)'= (27[0'2) - 1/2 exp r _ 
~ (x ~ Jl ) 2] 
as the Normal function, and the corresponding curve as the )Jorma] curve. When the 
Normal function is empl oyed to represent a probability distribution, it becomes the 
Nurmal distribution. The standardized likelihood function in this example is a Normal 
function, but it is not a probability distribution. 

16 
Nature of Bayesian Inference 
1.2 
0.01 
==-__ 
L-__ 
-'--..,..£._--'-_'O";::= __ 0-' 
700 
900 
1000 
(a) Prior distributions for A and B 
800 
900 
1000 
(b) Standardized likelihood for I observation y = 850 
0.0 1 
p)O ly) 
~----~ 
__ ~ 
__ -L __ L-__ ~~L-__ ~ 
_____ O~ 
700 
800 
900 
1000 
(e) Posterior distributions for A and B after I observatio n 
0.10 
~----~-------L __ -L __ ~ 
____ ~L-_____ O~ 
700 
800 
900 
1000 
(d) Standardized likelihood for 100 observations with y = 870 
0.10 
L-----~ ______ -L __ ~L-1L 
______ L_ _____ e~ 
700 
800 
900 
1000 
(e) Posterior distributions for A and B after 100 observations 
Fig. 1.2.1 Prior and posterior distributions for physicists A and B. 
-
. 
. 
-----------
- -- . 
. -
\ 
. 

1.2 
Nature of Bayesian Inference 
17 
shown in Appendix A 1.1 that the posterior distribution of e given y, pee I y), is 
the Normal distribution N(e, (11) where 
with 
\\ '0 = -1 
(10 
and 
1 
-=2 = 1\'0 + II'I 
(1 
(1.2.11) 
The posterior mean e is a weighted average of the prior mean eo and the 
observation y, the weights being proportional to \\"0 and \1 ' [ which are, respectively, 
the reciprocal of the variance of the prior distribution of e and that of the 
observation. This is an appealing result, since the reciprocal of the variance is a 
measure of information which determines the weight to be attached to a given 
value, The variance of the posterior distribution is the reciprocal of the sum of the 
two measures of information \1'0 and 11'1' reflecting the fact that the two sources 
of information are pooled together. 
Suppose the result of the single observation is y = 850; then the likelihood 
function is shown in Fig, 1.2.1 (b). 
Physicist A's posterior opinion now is 
represented by the Normal distribution PAce I y) with mean 890 and standard 
deviation 17.9, while that for 8 is represented by the Normal distribution P8(e I y) 
with mean 840 and standard deviation 35.78. These posterior distributions are 
shown in Fig. 1.2.1 (c). The complete inferential process is sketched in Table 1.2.3. 
Table 1.2.3 
Prior and posterior distributions or e for physicists A and B. 
Prior distribution 
Likelihood from data 
Posterior distribution 
A 
A 
e ~ .V(890, 17.9 2 ) 
---. r-----------~~ 
B 
/ 
I N(850, 40') I~ 
B 
r--e-~-N-(-80-0-
, -80-2-) ---,"1 
0-1 
e-~-N-(-84-0-, 3-5-.7-0-
2 )--'1 
We see that after this single observation the ideas of A and B about e, as 
represented by the posterior distributions, are much closer than before, although 
they still differ considerably. We see that A, relati vely speaking, did not learn 
much from the experiment, while B learned a great deal. The reason. of course, 
is that to A , the uncertainty in the measlJrement, as reflected by CI = 40, was larger 
than the uncertainty in his prior ((10 = 20). On the other hand, the uncertainty 
in tbe measurement was considerably smaller tban that in 8's prior (CIo = 80). 

18 
Nature of Bayesian Inferel.~e 
1.2 
For A, the prior has a stronger influence on the posterior distribution than has the 
likelihood, while for B the likelihood has a stronger influence than the prior. 
Suppose 99 further independent measurements are made and the sample 
mean y = Tb 
0 L y; of the entire 100 observations is 870. 
In general, the 
likelihood function of 8 given n independent observations from the Normal 
population N(8, cr 2), is 
1(8 I y) x C/2~ a r 
exp [ -
2~2 L (y ; - 8)2] . 
(I.2.12) 
Also since 
L (y; - 8)2 = L (y; _ y)2 + n(8 _ y)2, 
(1.2.13) 
and, given the data, L (y; - y)2 is a fixed constant, the likelihood is 
[ 
J ( 8 - y)2] 
/(81 y) rx exp 
- 2 
(1/ n 
' 
(1.2.14) 
which is a Normal function centred about y with standard deviation cr/ .. '/n. 
{n the present example, therefore, the likelihood is ,the Normal function centered 
at y = 870 with standard deviation cr/ 'n = i g = 4 shown in Fig. 1.2.1 (d). We 
can thus apply the result in (1.2.11) as if y were a single observation with variance 
(12/n, that is, with weight 11/(1 2 . 
The posterior distribution of 8 obtained by 
combining the likelihood function (1.2.14) with a Normal prior N(8o, cr6) is 
the Normal distribution N(B", iY~), where 
with 
and 
--=2 = Wo + w", 
(1" 
n 
H'n = 2
' 
(1 
(1.2.15) 
Thus the posterior distributions of A and Bare N(871.2,3 .92) 
and 
N(869.8,3 .995 2), respectively. 
These two distributions, shown in Fig. 1.2.le), 
are, for all practical purposes, the same, and are closely approximated by the 
Normal distribution N(870,42), which is the standardized form of the likelihood 
function in (1.2.14). Thus, after 100 observations, A and B would be in almost 
complete agreement. 
This is because the information coming from the data 
almost completely overrides prior differences. 
Influence of the Prior Distribution all the Posterior Distribution 
In the above example, we were concerned with the value of a location parameter 
e, namely, the mean of a Normal distribution. In general, we shall say that a 
parameter Yi is a location parameter if addition of a constant c to all the 
observations changes Yi to Yi + c. 

1.2 
Nature of Bayesian Inference 
19 
In this example, the contribution of the prior in helping to determine the 
posterior distribution of the location parameter e was seen to depend on its 
sharpness or flatness in relation to the sharpness or flatness of the likelihood with 
which it was to be combined (see again Fig. 1.2. J). After a single observation, the 
likelihood was not sharply peaked relative to either of the prior distributions 
PAce) or Pn(e). 
These priors were therefore influential in deciding the posterior 
distribution. Because of this, the two different priors, when combined with the 
same likelihood, produced different posterior distributions. On the other hand, 
after 100 observations, both the priors PACe) and Pace) were rather Rat compared \,ith 
the likelihood function I(a I y) = I(e I ji). 
These priors were therefore not very 
influential in deciding the corresponding posterior distributions of the location 
parameter a. 
We can say that, after 100 observations, the priors were 
dominated by the likelihood. 
1.2.4 Bayesian Decision Problems 
The problems which we treat in this book are nearly all concerned with the situation 
common in scientific inference where the prior distribution is dominated by the likelihood. 
However, we must at least mention the important topic of Bayesian decision analysis 
[Schlaifer (1959), Raiffa and Schlaifer (1961), and DeGroot (1970)J, where it is often not 
true that the prior is dominated by the likelihood. In Bayesian decision analysis, it is 
supposed that a choice has to be made from a set of available actions (a I' ... , ar) , where the 
payoff or utility of a given action depends on a state of nature, say e, which is unknown. 
The decision maker's knowledge of e is represented by a posterior distribution which 
combines prior knowledge of e with the information provided by an experiment, and he 
is then sLipposed to choose that action which maximizes the expected payoff over the 
posterior distribution. An important application of such analysis is to business decision 
problems, sLich as whether or not to introduce a new industrial product. J n such problems, 
a subjective prior distribution based, for example, on the opinion of an executive concern-
ing the potential size e of a market may be influential in determining the posterior 
distribution. 
The fact that in such situations different decisions can result from different choices 
of prior distribution has worried some statisticians. 
We feel, however, that making 
explicit the dependence of the decision on the choice of what is believed to be true is an 
advantage of Bayesian analysis rather than the reverse. Suppose four different executives, 
a fter careful consideration, produce four different prior distributions for the size of a 
potential market and separate analyses are made for each. Then either (I) the decision 
(e.g. whether to market the product) will be the same in spite of differences in the priors, 
or I~) the decision will be different. In either case the Bayesian decision analysis will be 
valua ble. ln the first case, the ultimate arbiter would be reassured that such differences in 
opini(' n did not logically lead to differences on what the appropriate action should be. 
In the second case, it would be clear to him that 017 present evidel/ce a real conflict existed. 
He would , in this case, either have to take the responsibility of ignoring the judgement of 
one or more of his executives, or of arraRging that further data be obtained to resolve the 

20 
Nature of Bayesian Inference 
1.2 
conflict. Far from nullifying the value of Bayesian analysis, the fact that such analysis 
shows to what extent different decisions mayor may not be appropriate when different 
prior opinions are held, seems to enhance it. For problems of this kind any procedure 
which took no account of such opinion would seem necessarily ill conceived. 
1.2.5 Application of Bayesian Analysis to Scientific Inference 
Important as the topic is, in this book, our concern will not be with statistical 
decision problems but with statistical inference problems such as occur in 
scientific investigation . By statistical inference we mean inference about the state 
of nature made in terms of probability, and a statistical inference problem is 
regarded as solved as soon as we can make an appropriate probability statement 
about the state of nature in question. Usually the state of nature is described by 
the value of one or more parameters. Such a parameter e could, for example, 
be the velocity of light or the thermal conductivity of a certain alloy, Thus, a 
solution to the inference problem is supplied by a posterior distribution pee I y) 
which shows what can be inferred about the parameters S from the data y given a 
relevant prior state of knowledge represented by peS). 
Dominance of the Likelihood in the .Vormal Theory Example 
Let us return again to the example of Section 1,2.3 concerning the estimation of 
the location parameter e of a Normal distribution. In general, if the prior distribu-
tion is Normal N(Bo, 0'6) and n independent observations with average ji are taken 
from the distribution N(B, 0'2), then from (1.2.15) the posterior distribution of e is 
B ~ N(e,,, a,~ ), 
with 
and 
where ll'O = 0'0 2 is the weight associated with the prior distribution and Wn = 
n/(J2 is the weight associated with the likelihood , In this expression, if 11'0 is small 
compared with )V,p then approximately the posterior distIi bution is numerically 
equal to the standardized likelihood, and is 
(1.2.16) 
Strictly speaking, this result is attained only when the prior vanance 0'6 
becomes infinite so that 1Vo is zero. 
Such a limiting prior distribution would , 
however, by itself make little theoretical or practical sense. 
For, when 
O'~ -> co, in the limit the prior density becomes uniform over the entire line from 
-
co to 00, and is therefore not a proper density function, Furthermore, it repre-
sents a situation where all values of 0 from -
co to 00 are equally acceptable 
a priori. 
But it is difficult, if not impossible, to imagine a practical situation 
where sufficiently extreme values could not be virtually ruled out. The practical 
situation is represented 1101 by the limiting case where 1\'0 = 0, but by the case 

1.2 
Nature of Bayesian Inference 
21 
where 11'0 is small compared with 11'", that is, where the prior is locally flat so that the 
likelihood dominates the prior. 
It is, therefore, important to note that the use of the limiting posterior in 
(1.2. 16) corresponding to \1'0 = 0 to supply a numerical approximation to the 
practical situation 'is not the same thing as assuming H'o is actually zero. Limiting 
cases of this kind are frequently used in this book, but it must be remembered 
this is for the purpose of supplying a numerical approximation and for this 
purpose only. 
"Proper" and "Improper" Prior Dis(ribulions 
A basic property of a probability density functionJ(x) is that it integrates or sums over its 
admissible range to I, that is, 
S Jex) dX} = 1 
'i.J(x) 
{
ex continuous), 
(x discrete). 
Now, if J(x) is uniform over the entire line from -
00 to 00, 
J(x) = /{, 
-
00 < x < 00 , K> 0, 
then it is not a proper density since the integral 
(1.2.17) 
does not exist no matter how small K is. Density functions of this kind are sometimes called 
improper distributions. As another example, the function 
J(x) = I(X- I , 
0< x < 00, 
K > 0 
(1.2.18) 
is also improper. In this book, density functions of the types in (1.2.17) and (1.2.18) are 
frequently employed to represent the local behavior of the prior distribution in the region 
where the likelihood is appreciable, but not over its entire admissible range. By supposing 
that to a sufficient approximation the prior follows the form (1.2.17) or (1.2.18) only over 
the range of appreciable likelihood and that it suitably tails to zero outside that range we 
ensure that the priors actually used are proper. Thus, by employing the dist ributions in a 
way that makes practical sense we are relieved of a theoretical difficulty. 
The Role of (he Dominant Likelihood in (he Analysis of Scientific Experiments 
It is often appropriate to analyze data from scientific investigations on the 
assumption that the likelihood dominates the prior. Two reasons for this are : 
I. A scientific investigation is not usually undertaken unless information supplied 
by the investigation is likely to be considerably more precise than information 
already available. 
For instance, suppose a physical constant '8 had been 
estimated at 0.85 ± 0 .05 ; then usually there would be no justification for making 

22 
"Iature of Bayesian Inference 
1.2 
a new determination whose accuracy was ± 0,25, t but there might be considerable 
justification for making one whose accuracy was ± 0.01. 
In brief, a scientific 
investigation is not usually undertaken unless it is likely to increase knowledge by 
a substantial amount. 
Therefore, as is illustrated in Figs. 1.2.2 and 1.2.3, 
analysis with priors which are dominated by the likelihood often realistically 
represents the true inferential situation. Situations of this kind have been referred 
to by Savage (1962) and Edwards, Lindman, and Savage (1963) as those where the 
principle of "precise measurement" or "stable estimation" applies. 
Prior distribution 
e-
Fig. 1.2.2 Dominant likelihood (often appropriate to the analysis of scientific data). 
u-
Fig. 1.2.3 Dominant prior (rarely appropriate to the analysis of scientific data). 
2. Even when a scientist holds strong prior beliefs about the value of a parameter 
D, nevertheless, in reporting his results it would usually be appropriate and most 
convincing to his colleagues if he analyzed the data against a reference prior which 
is dominated by the likelihood. He could then say that, irrespective of what he 
or anyone else believed to begin with, the posterior distribution represented what 
someone who a priori knew very little about D should believe in the light of the 
data, t 
t Special circumstances could, of course, occur when the new determination was justifieil; 
for example, if it were suspected that the original method of determination might be 
subject to a major bias. 
t As a separate issue his colleagues might also like to know what his prior opinion was 
and how this would affect the conclusions, 
, 
. 
-
._---
. 
---

1.2 
Nature of Bayesian Inference 
23 
In judgi ng the data in relation to a "neutral" reference prior, the scientist 
employs Ilha t may be called the "jury principle." Cases are tried in a law court 
before a ju ry which is carefully screened so that it has no possible connection 
with the principals and the events of the case. The intention is clearly to ensure 
that information gleaned from "da ta" or testimony may be assumed to dominate 
prior ideas that members of the jury may have c:oncerning the possible guilt of the 
defendant. 
The Ref erence Prior 
I n the above we have used t he word reference prior. I n general we mean by this a 
prior which it is convenient to use as a standard. In principle, a reference prior 
might or might not be dominated by the likelihood , but in this book reference 
priors which are dominated by the likelihood are often employed. 
Dominant Likelihood and Loca//y r..r ni(orm Priors 
The argument so far has been illustrated by the single example concerning the 
location parameter 0 of a No rmal distribution with a Normal prior. In particular, 
we have used this example to illustrate the important situation where the 
likelihood dominates the prior. We now consider the dominant likelihood idea 
more generally. 
In general, a prior wh ich is domInated by the likelihood is one which does 
not change very much o ver the region in which the likelihood is appreciable and 
does not assume large values outside that range (see Fig. 1.2.2). We shall refer to a 
prior distributi on which has these properties as a loca//y lIm/orm prior. For such a 
prior distribution we can approximate the result from Bayes' formula by 
substituting a constant for the prior distribution so that 
a _ 
1(0 I y)p(O) 
p( I Y) - J 1(0 I y) p(O) dO 
I( 0 I y) 
- J /(01 y) dO' 
(1.2.19) 
Thus, for a locally uniform prior, the posterior distribution is approximately 
numerically equal to the standardized ljkelihood as we have previously found in 
(1.2. I 6) for the very special case of a 1\iormal prior dominated by a Normal 
likelihood . 
Difficulties Associated \\ilh Loea//;: L'niform Priors 
Historically, the choice of a prior to characterize a situation where "nothing (or, 
more realistically, little) is known a priori" has long been, and still is, a matter 
of dispute. 
Bayes tentatively suggested th:.lt where such knowledge was lacking 
concerning the nature of the prior distribution, it might be regarded as uniform . 
This suggestion is usually referred to as Bayes' postulate. He seemed , however, 
to have been himself so doubtful as to the validity of this postulate that he did not 
publish it, and his work was presented (Bayes, 1763) to the Royal Society 
posthumously by his friend Richard Price. This was accompanied by Price's own 

24 
Nature of Bayesian Inference 
1.2 
commentary which might not have reflected Bayes' final view. 
Fisher (1959) 
pointed out that although Bayes considered this postulate in his essay, in his 
actual mathematics he avoided its use as open to dispute and showed by example 
how the prior distribution could be determined by an auxilliary experiment. 
The postulate was accepted without question by later writers such as Laplace, but 
its reckless application led unfortunately to the falling into disrepute of the 
theorem itself. 
We now examine some objections which have been made to Bayes' postulate, 
and then discuss ways which have been proposed to overcome these objections 
and extend the concept. In refutation of Bayes' postulate, it has been argued that, 
if the distribution of a continuous parameter 8 were taken locally uniform , then 
the distribution of log 8,8- 1, or some other transformation of 8 (which might 
provide equally sensi ble bases for parametrizing the problem) would not be 
locally uniform . Thus, application of Bayes' postulate to different transformations 
of 8 would lead to posterior distributions from the same data which were 
inconsistent. 
This argument is of course correct, but the arbitrariness of the choice of 
parametrization does not by itself mean that we should not employ Bayes' 
postulate in practice. Arbitrariness exists to some extent in the specification of any 
statistical model. The only realistic expectation from a statistical analysis is that 
the conclusions will provide a good enough approximation to the truth. In applied 
(as opposed to pure) mathematics, arbitrariness is inadmissible only in so far as 
it produces results outside accepta ble limits of approximation. In particular: 
a) 
If, as would often be the case, the range of uncertainty for 8 was not large 
compared with its mean value, then over this ral1ge, transformations such as 
the logarithmic and the reciprocal would be nearly linear, in which case 
approximate uniformity for 8 would imply approximate uniformity for the 
transformed 8. 
b) Although the argument (a) would fail for an extreme transformation such as 
8 10, it is equally true that a rational experimenter would not agree to employ 
a uniform distribution after such a transformation. Thus, suppose that an 
investigator was concerned with measuring the specific gravity 8 of a sample 
of ore; he expected that 8 would be about 5 and felt happy with the idea that 
the probability that 8 lay between 4 and 5 was about the same as the 
probability that 8 lay between 5 and 6. A uniform distribution on 8 10 would 
imply that the probability that it lay between 5 and 6 was almost six times as 
great as the probability that it lay between 4 and 5. Once he understood the 
implication of taking a constant prior distribution for this extreme transfor-
mation, he would be unwilling to accept it. 
c) 
For large or even moderate-sized samples, fairly drastic modification of the 
prior distribution may only lead to minor modification of the posterior 
................ 
MB~ ...... 
--~~ 
____ ·~_------

1.3 
Noninformative Prior Distributions 
25 
densi ty. Thus, for independent observations Y(, .. . "~ n' the posterior distribution 
can be written 
11 
pee I Y(, .. . , Yn) oc p(O) n P(Yi 18). 
(1.2.20) 
;=1 
and, for sufficiently large 11, the 11 terms introduced by the likelihood will 
tend to overwhelm the single term contributed by the prior [see Savage, 
(1954)]. 
An illuminating iJlustration of the robustness of inference, under 
sensible modification of the prior, is provided by the study of Mosteller and 
Wallace (1964) on disputed authorship. 
The above arguments indicate only that arbitrariness in the choice of the 
transformation in terms of which the prior is supposed locally uniform is often 
not catastrophic and that effects on the posterior distribution are likely to be of 
order 11- 1 and not of order I in relation to the data. For instance, we shall discuss 
in Chapter 2 a Bayesian derivation of Student's t distribution, and in so doing 
we must choose a prior distribution for the dispersion of the supposed Normal 
distribution of the observations. In various contexts, the dispersion of a Normal 
distribution can with some justification be measured in terms of (J2, (J, log (J, 
(J-I, or (J-2 Depending on which of these metrics are regarded as locally uniform, 
at distribution is obtained having 11 -
3, n - 2, 11 -
I, n, or n + I degrees of 
freedom, respectively. What we have in this case is an uncertainty in the degrees 
of freedom (which in turn implies an uncertainty in the variance of the posterior 
distribution) of order n- 1. This degree of arbitrariness would not matter very much 
for large samples but it. would have an appreciable effect for small samples. We 
are thus led to ask whether there is some way of eliminating, or at least reducing 
it so that the situation where "little is known a priori" can be more closely and 
meaningfully approximated. 
1.3 ~ONINFORMATIVE PRIOR DISTRIBUTIONS 
In this section we present an argument for choosing a particular metric in terms 
of which a locally uniform prior can be regarded as noninformative about the 
parameters. It is important to bear in mind that one can never be in a state of 
complete ignorance ; further, the statement "knowing little a priori" can only 
have meaning relative to the information provided by an experiment. 
For 
instance, in Fig. 1.2.1 , physicist A's prior knowledge is substantial compared 
with the information from a single observation but it is noninformative relative 
to that from a hundred observations. Now, a prior distribution is supposed to 
represent knowledge about parameters before the outcome of a projected 
experiment is known. Thus, the main issue is how to select a prior which provides 
little information relative to what is expected to be provided by the intended 
experiment. We consider flfst the case of a single parameter. 

26 
Nature of Bayesian Inference 
1.3 
1.3.1 The Normal Mean e (0"2 Known) 
Suppose y' = (YI' .. . ,Yn) is a random sample from a Normal distribution 
N(e, 0"2), where 0" is a supposed known. Then, from (1 .2.14), the likelihood function 
of e is 
I(e I 0", y) oc. exp [ - 2;2 (e - .)1)2] 
(1.3.1) 
where, as before, y is the average of the observations. The standardized likelihood 
function of e is graphically represented by a Normal curve located by y, with 
standard deviation O"/y'n. Figure 1.3.1 (a) shows a set of standardized likelihood 
curves which could result from an experiment in which n = 10 and 0" = l. 
Three different situations are illustrated with data giving averages of y = 6, 
Y = 9, and y = 12. Now it could happen that the quantity of immediate scientific 
interest was not e itself but the reciprocal K = e- I . In that case the likelihood is 
(1.3.2) 
and the standardized likelihood curves would have the appearance shown in Fig. 
1.3.1 (b). 
In our previous discussion of the Normal mean, the prior was taken to be 
locally uniform in e, which implies of course that it is not uniform in K . We now 
consider whether this choice can be justified, and whether the principle can be 
extended to a wider context. 
Data Translated Likelihood and Non-informative Prior 
Our problem is to express the idea that little is known a priori relative to what the 
data has to tell us about a parameter e. What the data has to tell us about e is 
expressed by the likelihood function, and in the case of the Normal mean with 
nand 0"2 known, the data enter the likelihood only via the sample average y. 
Figure 1.3.1 (a) illustrates how, when the likelihood is expressed in terms of e, 
the sample average y affects only the location of the likelihood curve. Different 
sets of data translate the likelihood curve on the e axis but leave it otherwise un-
changed. On the other hand, Fig. 1.3.1 (b) illustrates how, when the likelihood is 
expressed in terms of K = e- I , both the location and the spread of the likelihood 
curve are changed when the data (and hence y) are changed. 
Now, in general, suppose it is possible to express the unknown parameter e 
in terms of a metric 4>(e), so that the corresponding likelihood is data translated. 
This means that the likelihood curve for ¢ (e) is completely determined a priori 
except for its location which depends on the data yet to be observed. Then to say 
that we know little a priori relative to what the data is going to tell us, may be ex-
pressed by saying that we are almost equally willing to accept one value of ¢(e) 
as another. This state of indifference may be expressed by taking ¢(e) to be locally 
, 
. 
-
"-- --
---

1.3 
A noninformative 
prior for IJ 
_\.-
y=6 
y=9 
"Ioninforrnative Prior Distributions 
27 
y = 12 
Solid curves show 
standardized 
likelihood curves 
in terms of IJ 
-------------£----~--~~~--~~--~~--~~----~----~--~~----~--IJ~ 
The correspond ing 
non informative prior 
for K 
0.20 
y=6 
(a) The normal mean IJ 
Solid curves 
show corresponding 
sta nd ard ized Ii k el i hood 
curves in terms of K 
0.\5 
y=9 
(b) Reciprocal of the normal mean K = IJ 
I 
y = 12 
0,10 
Fig. 1.3.1 Noninformative prior distributions and standardized likelihood curves: (a) for 
the Normal mean e, and (b) for K = 6 - I . 
uniform, and the resulting prior distribution is called Iloninformative for 4>(6) 
with respect to the data. 
In the particular case of the Normal mean, the likelihood of 6 is a Normal 
curve completely known a priori except for location which is determined by y. 
That is, the likelihood is data translated in the original metric e. Therefore, in 
this case, 4>(8) = e and a noninformative prior is locally uniform in e itself. 
That is, locally 
pee I 0') cc C. 
( 1.3.3) 
This noninformative pnor distribution is shown In Fig. 1.3.1 (a) by the dotted 
line, Since 
p(K I 0') == pee I 0') I:~ I = pce I 0')62 
CC K- 2, 
(1.3.4) 
the corresponding noninformative prior for K is not uniform but is locally 
proportional to e2, that is, to K - 2 In general, if the noninformative prior is locally 

28 
Nature of Bayesian Inference 
uniform in 4>(8), then the corresponding noninformative prior for 8 is locally 
proportional to Id4>/d81, assuming the transformation is one to one. 
It is to be noted that we regard this argument only as indicating in what metric 
(transformation) the local behaviour of the prior should be uniform. 
Figure 
1.3.2 illustrates what might be the situation over a wider range of the parameter. 
Here p(8 I 0) is a proper distribution which is merely flat over the region of interest. 
Similarly, p(K I a) is a proper distribution obtained by transformation which 
is proportional to K- 2 over the region of interest. This point is important, because 
it would be inappropriate mathematically and meaningless practically to suppose, 
for example, that pC8 I a) was uniform over an infinite range, or that p(K I a) was 
proportional to K- 2 over an infinite range. We do not assume this nor do we 
need to. 
y=6 
y=9 
y=12 
___ d----A--A---A--,--~~::~~:,~,b~O,, __ 
6 
0.20 
10 
(a) The normal mean e 
0.15 
(b) Reciprocal of the 
normal mean I< = e I 
0.10 
14 
Y= 12 
The corresponding proper 
prior distribution for I< 
--L 
....... 
0.05 
Fig. 1:3.2 Noninformative prior distributions and standardized likelihood curves: (a) for 
the Normal mean e, and (b) for K = e- 1 seen over a wider range of parameter values. 
Posterior Distribution of the Normal Mean 8 
On multiplying the likelihood in (1.3.1) by the locally uniform noninformative 
prior in (1.3.3), and introducing the appropria,te normalizing constant, we have 
p(8 I a, y) == (21tn(J2) -1/2 exp [ - 2:2 (8 -
ji)2] , 
-00 < 8 < 00. 
(1.3.5) 
That is, when it is desired to assume little prior knowledge about 8 relative to that 
which would be supplied from the data, and given a sample of n observations 

1.3 
Noninformative Prior Distributions 
29 
from a Normal distribution with known variance (J2, then a posteriori e is 
approximately Normally distributed with mean y and variance (J2 'n, 
As an example, Fig. 1,3.3 shows the posterior distribution calculated from (1 .3.5) 
when a sample of 16 observations has been taken whose average value is y = 10, it being 
known that (J = 8. The figure shows e distributed about y = 10 with standard deviation 
(J/,/n = 2, 
It is perhaps appropriate to emphasize the meaning which attaches to this 
distribution, To someone who, before the data was collected, was indifferent to the 
choice of e in the relevant range , the posterior distri bution represents what, given the data, 
his attitude should now be, He could, for example, state that the probability that e was 
less than 8 was 15.9%, this being the size of the shaded area shown in the figure, Relative 
to the same state of prior indifference he could, moreover, employ the same posterior 
distribution of Fig, 1,3.3 to obtain, by transformation, the posterior distribution for any 
function K(e) which was of interest. For example he could state that the probability that 
K was greater than 1,'8 was 15.9%, Other probabilities are readily obtained by using a table 
of the Normal probability integral, such as Table I at the end of the book. 
p(a y) 
0.4 
, 
0.2 
o 
2 
4 
6 
8 
10 
12 
14 
16 
18 
Fig, 1.3.3 Posterior distribution of the Normal mean 0 (noninformative prior), when 
51 = 10, (J = 8, n = 16. 
1.3.2 The Normal Standard Deyiation (J (IJ known) 
As a second example, consider the choice of a noninformati ve prior distribution 
for (J, the standard deviation of a Normal distri bution for which the mean e is 
supposed known. In this case, the likelihood is 
( ns2) 
/«(J I e, y) cc (J-n exp 
-
2(J2 
' 
(1.3.6) 

--
30 
Nature of Bayesian Inference 
1.3 
where 
For illustration , suppose there are n = 10 observations, then Fig. 1.3.4(a) shows 
the standardized likelihood curves for (J with s = 5, s = 10, and s:= 20. Clearly, 
in the original metric IJ, the likelihood curves are not data translated . According 
to the principle stated in the preceding section therefore a noninformative prior 
should 1101 be taken to be locally uniform in (J . 
35 
40 
a ) Normal standard deviatio n a 
5 = 5 
s = 10 
5 = 20 
---~---IO'O-
1.0 
2.0 
3.0 
b) Lo g of Normal st and ard deviation, log a 
Fig. 1.3.4 Noninformative prior distributions and standardized likelihood curves: (a) for 
the Normal standard deviation (J, and (b) for log (J (broken curves are noninformative 
priors and solid curves are the standard likelihoods). 
Figure 1.3.4(b) shows, however, that the corresponding likeJihood curves 
in terms of log (J are exactly data translated. 
To see this mathematically, note 
that multiplication by the constant s" leaves the likelihood unchanged. Therefore 
we can express the likelihood of log (J as 
!(Iog (J I e, y) IX exp { - n(log (J -
log s) -
; exp [ - 2(log IJ -
log s)] } . 
(1.3.7) 

1.3 
i'."oninformative Prior Distributions 
31 
Thus, in this logarilhmic metric the data acting through s serve only to relocate 
the likelihood . 
A n()ninformative prior should therefore be locally uniform in 
log a. 
When expressed in the metric a, the noninformative prior is thus locally 
proportio nal to a 
J 
I 
d log 0" I 
p(O" i 8) -X. 
da I = a 
1. 
(1.3.8) 
If we use this prior distribution , then the posterior disLribution of 0" is 
p(a I e, y) cC a - (II + 1) ex p ( -
~;: ) . 
(1.3.9) 
It will be seen in Section 2.3, where the implication of this distribution is discussed 
in greater detail, that the normalizing constanl required to make the distribution 
integrate to unity is 
k= 
2(11/ 2) l.r (n/2) 
(1.3.10) 
Thus, given a sampJe y of 
11 
observations from a 
),jormal distribution 
N(O, ( 2), with e known and little prior information about 0" relative to that 
supplied by the data, the posterior distribution of a is approximately 
(ns2)"/2 
( 
ns2) 
( 
1 0 ). 
- (li T I) 
0 
P a 
,Y = 2 ( 11/ 2) -1 1(n/2) a 
exp 
-
2a 2
' 
a > .\ 
(1.3 .11) 
and the corresponding posterior distribution of any function of a may be found 
by an appropriate transformation of (1.3.11). 
Figure 1.3.5 illustrates the situation where the sample standard deviation calculated 
from n = 10 observations is 
The distribution shows what, given the assumptions and the data, can be said about a. 
Tail area probabilities are readily found using the fact that (1.3.11) implies that 
ns2 ja 2 has the "chi-square" (l) distribution with 11 degrees of freedom, 
J 
p(X2) = 
(X 2)(n/2 ) - I exp ( 
I X 2) 
1(n/2)2"/2 
' 
-2· , 
, 
(1.3.12) 
For instance, suppose we wish to find the probability that a is greater than ao = 1.5. 
We have 
ns2 
10 
a6 -
J .52 = 4.4, 

32 
Nature of Bayesian Inference 
pro I yl 
2 
o 
1.3 
3 
Fig. 1.3.5 Posterior distribution of the ?--Iormal standard deviation (J (noninformative 
prior), when s = I and fl = 10. 
so that, 'With X~ referring to a chi-square variate with v degrees of freedom, the required 
probability corresponding to the shaded area in the diagram can be obtained from a table 
of X2 integral and is found to be 
Pr{x~ 0 < 4.4} = 7.5%. 
1.3.3 Exact Data Translated Likelihoods and Noninformatiye Priors 
We can summarize the above discussion of the choice of prior for a single 
parameter as follows. 
If (pee) is a one-to-one transformation of e, we shall say that a prior 
distribution of e which is locally proportional to Id¢ldel is noninformative for 
the parameter 8 if, in terms of ¢; the likelihood curve is data translated, that is, 
the data only serve to change the location of the likelihood I(¢ I y). Mathematically, 
a data translated likelihood must be expressible in the form 
Ice I y) = g [1>(8) - I(y)], 
(1.3.13) 
where g(x) is a known function independent of the data y and fCy) is a function 
of y. 
The examples we have so far considered are both special cases of the above 
principle. For the Normal mean, 1>Ce) = e,j(y) == y, and for the Normal standard 
deviation, ¢«(J) = log (J, iCy) = log s. 
r n particular, we see that any likelihood of the form 
[ 
s(y) ] 
1(0' I y) oc I -;;-
(1.3.14) 
--
---- -
, 
. 

1.3 
Noninformative Prior Distributions 
33 
can be bought into the form 
I((J I y) = g[log (5 -
log s(y)] 
(1.3.15) 
so that it is data translated in terms of the logarithmic transformation 
e/>((J) = log (5. 
The choice of a prior which is locally uniform in the metric e/> for which the likelihood 
is data translated, can be viewed in another way. Let 
I(e/> I y) = g[e/> - fey)] , 
(1.3.16) 
and assume that the function g is continuous and has a unique maximum g. Let (1. be an 
arbitrary positive constant such that 0 < a < g. Then, for any given (1. , there exist two 
constants c i and c2 (c i < c2), independent of y such that g[e/> - fey)] is greater than a 
for <p in the interval 
fey) + CI < 1; <fey) + C2 · 
(1.3 17) 
This interval may be called the a highest likelihood interval. Now suppose the trans-
formation from e/> to J. is monotone. Then the corresponding (1. highest likelihood interval 
for ), is 
( 1.3.18) 
We see that, in terms of 1;, the length of the interval in (1.3.17) is (e2 -
c l ) independent 
of the data y, while for the metric ;, the corresponding length 
, 
will in general depend upon y (except when the transformation is linear). For example, in 
t he case of the Normal mean, e/>(e) = e, f(y) = y, and for n = 10, (J = I, 
g(x) = exp ( - 2:2 X2 ) = exp ( - 5x2) 
so that g = 1. Suppose we take a = 0.05; then 
C 1 = -
0.77, 
C2 = 0.77. 
For the three cases y = 6, Y = 9, and y = 12 considered earlier, the corresponding 0.05 
highest likelihood intervals for e are 
6 ± 0.77 
(5.23, 6.77), 
9 ± 0.77 
(8.23, 9.77), 
12 ± 0.77 
(11.23,12.77), 
( 1.3.19) 
havingthesamelength cz -
C 1 = 1.54. However,intermsofthemetric A = - /( = -1/8, 
which is a monotone increasing function of e, the 0.05 highest likelihood interval is 
- (ji -
0.77)-1 < i. < - (ji + 0.77)-1, 

, 
34 
Nature of Bayesian Inference 
so that for the three values of y considered we have 
ji 
Interval 
Length 
6 
(-0.191, -0.1 48) 
0.043 
9 
(-0.122, -0.102) 
0.020 
12 
( - 0.089, - 0.078) 
0.011 
1.3 
( 1.3.20) 
If we say we have little a priori knowledge about a parameter e relative to the infor-
mation expected to be supplied by the data, then we should be equally willing to accept 
the information from one experimental outcome as that from another. 
Since the 
information from the data is contained in the likelihood, this is saying that, over a relevant 
region of B, we would have no a priori preference for one likelihood curve over anolh r. 
This state of local indifference can then be represented by assigning approximately eqllal 
probabilities to all a-highest likelihood intervals. 
Now, in terms of <P for which the 
likelihood is data translated, the intervals all have the same length, so that the prior 
density must be locally uniform. 
In the above example we would assign equal prior probabilities to the three intervals 
in (J .3. 19), and the corresponding one in (1.3.20). It then follows that the noninformat ive 
prior distribution is locally uniform in B but is locally proportional to 1 dBjd;, 1 = ), - 2 
in terms of A. 
1.3.4 Approximate Data Translated Likelihood 
As might be expected, a transformation which alJows the likelihood to be 
expressed exactly in the form (1.3.13) is not generally available. However, for 
moderate sized samples, because of the insensitivity of the posterior distribution 
to minor changes in the prior, all that it would seem necessary to require is a 
transformation <p(B) in terms of which the likelihood is approximately data 
translated. That is to say, the likelihood for <p is nearly independent of the data y 
except for its location. 
The Binomial Mean 11 
To illustrate the possibilities we consider the case of n independent trials, in each 
of which the probability of success is 11. The probability of y successes in n trials 
is given by the binomial distribution 
n! 
P (y 111) = '( 
)' rr
Y (1 - 11)" - Y, 
y. n - y . 
y = 0, ... ,n, 
(1.3.21) 
so that the likelihood is 
(1.3.22) 
Suppose for illustration there are n = 24 trials. Then Fig.l.3.6(a) shows the stan-
dardized likelihood for y = 3, Y = 12, and y = 21 successes. 
Figure 1.3.6(b) 
is the corresponding diagram obtained by plotting in the transformed metric 
<p(11) = sin -1 "In. 
(1.3.23) 

1.3 
"Ioninformative Prior Distributions 
35 
0.01 
0.05 0.10 
0.20 
0.30 
OAO 
0. 60 
0.70 
0.80 
0.90 0.95 
0.99 
\ 
"\ "" '" 
\ 
\ 
I 
/ 
/ 
/ / 
I 
a) The binomial mean 7r 
b) The transformed mean 1> ; sin -I ,;; 
Fig. 1.3.6 Noninformative prior distributions and standardized likelihood curves: (a) for 
the binomial mean rr, and (b) for the transformed mean ¢ = sin-1,,'ir (broken curves are 
the non informative priors and solid curves are standardized likelihoods). 
Although in terms of ¢ the likelihood curves are not exactly identical in shape 
and spread, they are nearly so. In this metric the likelihood curve is very nearly 
data translated and a locally uniform prior distribution is nearly noninformative. 
This in turn implies that the corresponding nearly noninformative prior for rr 
is proportional to 
I 
d¢ I 
' 
p(rr) oc Tn = [rr( I - rr)r ' · 
(1.3.24) 
If we employ this approximately noninformative prior, indicated in Fig. 
1.3.6(a) and (b) by the dotted Jines, then as was noted by Fisher (1922), 
O<rr<1. 
( 1.3.25) 
After substitution of the appropriate normalizing constant, we find that the 

, 
36 
Nature of Bayesian Inference 
1.3 
corresponding posterior distribution for n is the beta distribution 
pen I y) = ___ 
r_(_n_+_l) ___ ny- i' (1 - n),,-y-t 
r(y + -t) r(n -
y + t) 
O<n<l. 
( 1.3.26) 
0.6 
0.7 
0.8 
0.9 
1.0 
11-+ 
Fig. 1.3.7 Posterior distribution of the binomial mean n (noninformative prior) for 2 I 
successes out of 24 trials. 
Figure 1.3.7 shows the posterior distribution of n given that 21 out of 24 binomial trials 
(a proportion of 0.875) are successes. 
For illustration, tail area probabilities can be 
obtained by consulting the incomplete beta function tables. 
The shaded area shown in the diagram is the probability that the parameter n lies 
between 0.7 and 0.9, and this is given by 
J
O.9 
[(25) 
----- 7[20. 5 (I - n/· 5 dn = 66.2% . 
o 7 [(21.5)r(3.5) 
We note in passing that, for this example where we have a moderately sized sample 
of n = 24 observations, the posterior density is not very sensitive to the precise choice of a 
prior. For instance, while for 21 successes the noninformative prior (1.3.24) yielded a 
posterior density proportional to n 20 .5 (I -
n)2. S, for a uniform prior in the original metric 
n the posterior density would have been proportional to n 21 (l -
n)3. The use of the non-
informative prior for n, rather than the uniform prior, is in general merely equivalent to 
reducing the number of successes and the number of "not successes" by 0.5. 
Derivation of Transformations Yielding Approximate Data Translated Likelihoods 
We now consider methods for obtaining parameter transformations in terms of 
which the likelihood is approximately data translated as in the binomial case. 
Again, let y' = (Yl' ... , )In) be a random sample from a distribution p(y 18). 
When the distribution obeys certain regularity conditions, Johnson (1967, 1970), 

1.3 
"Ioninformalive Prior Distributions 
37 
then for sufficiently large n, the likelihood function of e is approximately N ormal, 
and remains approximately Normal under mild one-to-one transformations of 
e. In such a case, the logarithm of the likelihood is approximately quadratic, so 
that 
n 
L(e I y) = log /(8 I y) = log f1 p(Yu I e) 
u=l 
(1.3.27) 
where () is the maximum likelibood estimate of e. In general, the quantity 
IS a positive function of y. 
For the moment we shall discuss the situation In 
which it can be expressed as a function of e only, and write 
Now the logarithm of a Normal function p(x) is of the form 
logp(x) = const - t (x -
~)2/(J2 
(1.3.28) 
( 1.3.29) 
and, given the location parameter p, is completely determined by its standard 
deviation (J. Comparison of (1.3.27) and (1.3.29) shows that the standard deviation 
of the likelihood curve is approximately equal to n-~ r
{ (8). 
Now suppose 
epee) is a one-lo-one transformation ; then, 
_ ( I 0
2 L) ( 1 a
2 L) ( de ) 2 
_ 
( de ) 2 
J(ep) = 
-
-
-.., -
= 
-
-
-
-
= J(8) -
. 
n O¢2 
ii> 
n ae2 $ 
dep 
iJ 
d¢ 
~ 
(1.3.30) 
J t follows that if ep(e) is chosen such that 
I :: 10 (£ r 
1/ 2 (e), 
(1.3.31) 
then J($) will be a constant independent of $, and the likelihood will be 
approximately data translated in terms of ep. Thus, the metric for which a locally 
uniform prior is approximately noninformative can be obtained from the 
relationship 
or 
(1 .3.32) 

38 
Nature of Bayesian Inference 
1.3 
This, In turn, implies that the corresponding noninformative prior for e is 
(1.3.33) 
As an example, consider again the binomial mean n. The log likelihood is 
L(n I y) = log I(n I y) = const + y log n + (n - y) log (1 -
n). 
(1.3.34) 
Thus 
aL 
an 
y 
n -
y 
-----
n 
1 -
n 
n-y 
(I -
n)2 
(1.3.35) 
For y * 0 and y * n, by setting aL/an = 0, one obtains the maximum likelihood 
estimates as ft = Yin, so that 
J(ft) = (_ ~ _e
2L) = (~+ _1_) = _I_ 
n an 2 
>l 
ft 
1 - it 
ftC l -
ft) 
( 1.3.36) 
which is a function of ft only, whence the noninformative prior for n is 
proportional to 
J I /2(n) cc n- 1/2 (I _ n)-12, 
( l.3.37a) 
which is the prior used in (1.3.24). Also, the transformation 
(1.3.37b) 
is precisely the metric employed in plotting the nearly data translated likelihood 
curves in Fig. 1.3.6. 
We recognize the sin -l-Jrr transformation as the weJl-
known asymptotic variance stabilizing transformation for the binomial, originaJly 
proposed by Fisher. [See, for example, Bartlett (1937) and Anscom be (1948a)]. 
In the above we have specifically supposed that the quantity 
( 1 a
2 L) 
- -;; a8 2 
iJ 
is a function of e only. 
It can be shown that this will be true whenever the 
observations yare drawn from a distribution p(y I 8) of the form 
p(y I 8) = h(y)w(8) exp [c(8)u(y)], 
(1.3.38) 
where the range of y does not depend upon 8. For the cases of the Normal mean 
e with a 2 known, the Normal standard deviation a with 8 known and the 
binomial mean n, the distribu tions are of this form. In fact, this is the form for 
which a single sufficient statistic for 8 exists, a concept which will be discussed 
Jater in Section 104. 
T 
t 
• 
, 
• 
-
I'".~ 
_____ 
'_
. 
.. 
l,,----

1.3 
Noninformative Prior Distributions 
The Poisson Mean A. 
As a further example, consider the Poisson distribution with mean A., 
)/ 
p(y 1...\) = -
exp (-),), 
y! 
y = 0, .. . ,00, 
39 
(1.3.39) 
which is of the form in (\.3.38). Suppose y' = (YI' ... ,Yn) is a set of n independent 
frequencies each distributed as (1.3.39). Then, given y, the likelihood is 
/P_I y) ex Any exp ( - nA), 
I 
ji = -L Yu' 
(1.3.40) 
n 
Thus, 
L(A 1 y) = const + nji log A. -
nA. 
(1.3.41) 
1.0 
2.0 
3.0 
4.0 
b) The lransrormed mean 1> = ;\1/2 
Fig. 1.3.8 Standardized likelihood curves: (a) for thy Poisson mean )., and (b) for the 
transformed mean ¢ = ).1/2. 

40 
Nature of Bayesian Inference 
1.3 
and 
oL 
ny 
82 = J: - n, 
For y "# 0, the maximum likelihood estimate of 2 obtained from oLla2 = 0 
is X = Y so that 
. 
( . I ;;2 L) 
I 
J(J..) = - -
-
=-::0-. 
n oJ.. 2 
J. 
J.. 
According to (1.3.33), a noninformative prior for A is 
p(J..) cc Jl'2(2) cc r 1/2, 
(1.3.42) 
( 1.3.43) 
and <P = 2 I ' 2 is the metric for which the approximate noninformative prtor IS 
locally uniform. 
The effectiveness of the transformation in achieving data 
translated curves is illustrated in Fig. 1.3.8(a) and (b), with n = 1 and y = y = 1, 
Y = 4, and y = 9. 
Using the noninformative prior (1.3.43), the posterior distribution of 2 is 
p(AI y) = d"Y- + exp (-n),), 
A> 0, 
where, on integration, the Normalizing constant is found to be 
c = n-(lIy+ t ) [f(nji + ·m- I . 
(1.3.44 ) 
Equivalently, we have that n2 is distributed as 'ix 2 with 2ny + 1 degrees of freedom. 
Figure 1.3.9 shows the posterior distribution of )., given that n = 1 and a frequency of 
y = 2 has been observed, where little is known about }, a priori. The shaded area 
p(A,y) 
0.3 
0.2 
0. 1 
4 
6 
8 
Fig. 1.3.9 Posterior distribution of the Poisson mean 2 (noninformative prior) for an 
observed frequency y = 2. 

1.3 
'1oninformative Prior Distributions 
41 
corresponds to the probability that J. < 2 which is 
PrUx~ < 2} = Pr{x; < 4} = 45.1% . 
1.3.5 Jeffreys' Rule, Information Measure, and Noninformative Priors 
In general, the distribution p(y I e) need not belong to the family defined by 
(1.3.38), and the quantity 
in (1.3.27) is a function of all the data y. 
The argument leading to the 
approximate noninformative prior in (1.3.33) can then be modified as follows. 
It is to be noted that, for given 8, 
(1.3.45) 
is the average of n identical functions of (YI' ... , y,,), respectively. Now suppose 
80 is the true value of 8 so that yare drawn from the distribution p(y I ( 0), 
It 
then follows that, for large n, the average converges in probability to the 
expectation of the function, that is, to 
E [-
02 IOgP (YI8)] = -J 8
1
Iogp(yI8) 
(
18)d'= (88) 
;w2 
~1l2 
P Y 
0 
Y 
a
, 0' 
y lOo 
uo 
Ou 
assuming that the expectation exists. Also, for large n, the maximum likelihood 
estimate 8 converges in probability to 80 , 
Thus, we can write, approximately, 
(1.3.46) 
where J(e) = aCe, 8) is the function 
8 
[
2210gP(Y 10) J 
[ C logp(y 18) ] 2 
J( ) = -
E 
= E 
ylO 
08 2 
yl8 
a8 
(1.3.47) 
Consequently, if we use J(8), which depends on 8 only, to approximate 
in (1.3.27), then, arguing exactly as before, we nnd that the metric 1>(e) for which 
a locally uniform prior is approximately noninformative is such that 
or 
( 1.3.48) 

42 
Nature .of Bayesian Inference 
1.3 
Equivalently, the noninformative prior for 8 should be chosen so that, locally, 
( 1.3.49) 
It is readily confirmed that, when the distribution p(y 10) is of the form 
(1.3.38), J(e) == . .f(e). 
Thus, the prior in (1.3.33), when applicable, is in fact 
identical to the prior of (1.3.49) and the latter fo rm can be used generally. 
For illustration, consider again the binomial mean T.. 
The likelihood is 
equivalent to the likelihood from a sample of 11 independent point binomials 
identically distributed as 
p(x In) = n-' (I 
)l-X 
-
n 
, 
x = 0, I. 
(1 .3.50) 
Thus 
82 Jogp 
x 
I-x 
on2 
7[2 
(J -
n)2 . 
(1.3.51) 
Since E (x) = n, it follows that 
(1.3.52) 
whence, according to (1.3.49), the noninformative prior of n is locally proportional 
to n - I/ 2(1 -
n)-1/2 as obtained earlier in (1.3.37a). Also, we see from (1.3.36) 
and (1.3.52) that J(ft) and J(ft) are identical. 
Now, the quantity .1(8) obtained in (1.3.47) above is Fisher's measure of 
information about 8 in a single observation y. More generally, Fisher's measure 
(1922, 1925) of information about 8 in a sample y' = (YI' ""YIt) is defined as 
( 1.3.53) 
where, as before, L is the log likelihood and the expectation is taken with respect 
to the distribution p(y I fJ). When y is a random sample, .11/(8) = nJ(8). Thus, 
(1.3.49) can be expressed by the following rule. 
Jeffreys' rule: The prior distribution for a single parameter 8 is approximately 
noninformative if it is taken proportional to the square root of Fisher's information 
measure. 
This rule for the choice of a noninformative prior distribution was first given 
by Sir Harold Jeffreys (1961), who justified it on the grounds of its invariance 
under parameter transformations. 
For, suppose 
cjJ = cf>(e) is a one-to-one 
transformation of e; then it is readily seen that 
(
de )2 
J(¢) = J(8) 
d¢ 
. 
(1.3.54) 

1.3 
Noninformative Prior Distributions 
43 
Now, if some principle of choice led to pee) as a noninformative prior for e, the 
same principle should lead to 
p(¢) = pee) I :; I 
(1,3.55) 
as a noninformative prior for ¢' The principle of choice In (1.3.49) precisely 
satisfies this requirement: if we use it, then the prior of ¢ is 
(1.3.56) 
The Locatiol1 Scale Family 
For illustration we show how results in (\.3 .3) and (1.3,8) concerning the 
parameters (8,0') for the Normal distribution may, as an approximation, be 
extended to cover the general location-scale family of distributions 
( 
y - 8) 
p(y Ie, 0') cc 0'- 117 -0'-
. 
( 1.3.57) 
where the range of y does not involve (8,0') and h satisfies certain regularity 
conditions. 
Suppose we have a sample of 11 independent observations y' = 
(y J, "" Yn) from this distri bution. 
a) 
8 unknoH'l1, 0' kI1O\l'J1. The likelihood is 
Now 
[; log pCy I 8,0') 
Z8 
Thus, from (1.3.47) 
n (y _ e) 
1(8 I 0', y) cc Dl h _1'-0'-
, 
h'(x) 
0' 
h(x)' 
where 
=(~) 
x 
. 
(J 
~(e) = E [ illogp(y I 8,a) '12= ~ E [ h'(X) ]2, 
yl 8 
a8 
0' 
x 
hex) 
(1.3.58) 
( 1.3,59) 
(1.3 ,60) 
where the expectation on the extreme right is taken over the distribution 
p(x) = hex). 
Since this expectation does not involve 8 and 0'2 
IS known, 
~(8) = constant. So we are led to take 8 locally uniform a priori, 
p(8 I 0') <X ~12(e) = constant, 
(1.3,61) 
and the corresponding posterior distribution of () is approximately 
8 
TI
n 
(Yu - e) 
p( I 0', y) cc 1' = 1 h -a- , 
- ro < 8 < 00. 
( 1.3.62) 

44 
Nature of Bayesian Inference 
b) 
(Y unknown, e known. Here the likelihood is 
n 
(Y _ e) 
l(u I (), y) IX u- n .IIl h _"_u-
. 
Since 
o logp(y I e, u) 
ou 
-- 1+ -
-
1 [ 
xh'(x) J 
(Y 
h(x) 
, 
it follows that 
I 
[ 
xh' (x) J 
2 
1 
.1(0") = 2£ 1 + -,-)-
IX 2' 
0" 
x 
1(X 
0" 
Consequently, Jeffreys' rule leads to taking the prior 
I 
p(O" I e) IX -
or 
p (log U) IX const. 
U 
The corresponding posterior distribution of 0" is then 
0" > O. 
Caution in the Application of Jeffreys' Rule 
1.3 
( 1.3.63) 
(1 .3.64) 
( 1.3.65) 
( 1.3.66) 
Jeffreys' rule given by (1.3.49), like most rules, should not be mechanically 
applied. 
It is obviously inapplica ble for example when fee) does not exist. 
Furthermore, as Jeffreys pointed out, the rule has to be modified in certain cir-
cumstances which we will discuss later in Section 1.3.6. 
We believe that it is 
best, to treat as basic the idea of seeking a transformation for which the 
likelihood is approximately data translated , to treat each problem individually, 
and to regard equation (1.3.49) as a means whereby in appropriate circumstances 
this transformation can be determined. 
Dependence of the Noninformative Prior Distrihution on the Probability Model 
In the development followed above the form of the noninformative prior distribution 
depends upon the probability model of the observations. It might be argued that this 
dependence is objectionable because the representation of rolal ignorance about a 
parameter ought to be the same, whatever the nature of a projected experiment. On the 
contrary, the position adopted above is that we seek to represent not total ignorance but 
an amount of prior information which is small relalive to what the particular projected 
experiment can be expected to provide. The form of the prior musl then depend on the 
expected likelihood. 
As a specific example, suppose for example n is the proportion of successes in a 
Bernoulli population. Now IT can be estimated (I) by counting the number of successes 

1.3 
!'Ioninformative Prior Distributions 
45 
y in a fixed /lumber of trials II and using the fact that y has the binomial distribution in 
(1.3.21), or (2) by counting the number of trials z until a fixed number oj sliccesses r is 
obtained and supposing that z has the Pascal distribution 
(
z - I) 
r-I 
nr(l _ rr)z-r. 
z = r, r + I, 
(1.3.67a) 
These two experiments lead, on the argument given above. to slightly different non-
informative priors. 
Specifically, for the binomial case, the information measure is, 
from (1.3.52), 
whence 
a 
I 
I 
~2L ) 
E (--~ -
= nrr' (I -
rr)- , 
0IT 2 
and the corresponding noninformative prior is locally uniform in ¢ = sin - I", n as in 
(1.3.37a, b), On the other hand , it is easily shown that the information measure for the 
Pascal experiment is 
whence 
and the corresponding noninformative prior is locally uniform in 
I - .}1 -
IT 
¢ = log -----,== 
I + 
1 -
rr ' 
(U.67b) 
(1 .3.67c) 
Now, these two kinds of experiments would lead to exactly the same likelihood when 
II = z and y = r; and when this is so it has been argued that inference about IT from both 
experiments ought to be identical. The use of the above two noninformative priors 
however will not yield this result. For illustration let us suppose there were 24 trials with 
21 successes. If to arrive at this result sampling had been continued till the number of 
Irials was 24 the posterior distribution obtained with the appropriate noninformative 
prior would have been 
However if sampling had been continued till the number of sllccesses was .21 then the 
posterior distribution obtained with the appropriate noninformative prior in (1.3.67c) 
would have been 

46 
:--Iature of Bayesian Inference 
1.3 
This says that when we sample till the number of successes reaches a certain value some 
downward adjustment of probability is needed relative to sampling with Axed 11. We find 
this result much less surprising than the claim that they ought to agree. 
Tn general we feel that it is sensible to choose a noninformative prior which expresses 
ignorance re/alive to information which can be suppJied by a particular experiment. 
If the experiment is changed, then the expression of relative ignorance can be expected to 
change correspondingly. 
1.3.6 :"ioninformative Priors for Multiple Parameters 
We now extend previous ideas to include multiparameter models. We begin by 
considering the Normal linear model with (J assumed known. 
The Parameters 8 in a Normal Linear Model, (J Assumed Known 
Suppose y' = (YI' ... ,y,,) is a set of Normally and independently distributed 
random variables having common variance (52, and the expected value of y" is a 
linear function of k parameters 0' = (8 1, ... , Ok) such that 
u=1,2, ... ,n, 
(J .3.68) 
where the x's are known constants. 
This Normal linear model is of basic importance. 
In particular, for 
suitable choice of the x's, it provides the structure for general Analysis of 
Variance and for Regression (Least Squares) Analysis. 
Special cases include 
models already considered. For example, the model (1.1.1) for a Normal sample 
is obtained by setting k = 1,8 1 = O,andxul = 1 (u = 1,2, ... ,n). 
In general, if X is the n x k matrix {XUj } of known constants, then the n 
equations may be written concisely as 
E(y) = XO. 
If (5 is known, then the likelihood is 
1(0 I (J, y) ex exp [- 2: 2 (y - X9)' (y -
X9)] . 
( 1.3.69) 
We shall suppose that the rank of the matrix X lS k. 
The quadratic form in 
(1.3.69) may then be written 
(y -
X9)' (y -
X9) = (y -
y)' (y -
y) + (9 - a)'X'X(9 - 8), 
(1.3.70) 
where 
is the vector of least squares estimate of 9, and y = X() is the vector of fitted 
values so that (y -
Y)' (y - y) is a function of data not involving 9. 
The 

1.3 
"Ioninformative Prior Distributions 
47 
10 
-
8 
6 
20 
2 J 
22 
23 
6 1 ., 
Fig. 1.3.10 Normal linear model: con lours of likelihood function for different sets of 
data. 
likelihood can therefore be expressed as 
[ 
1 
, 
, ] 
/(9\ a, y) cc exp 
- 2a 2 (9 - 9)'X' X(9 - 9) , 
(1.3.71) 
which is in the form of a multivariate Normal functiont centered at 9 and having 
covariance matrix a 2(X'X)-I. The likelihood contours in the parameter space 
of 9 are thus ellipses (k = 2), ellipsoids (k = 3), or hyperellipsoids (k > 3) 
defined by 
(9 - 6),X'X(9 - 8) = const. 
(1.3.72) 
Figure 1.3, I 0 Illustrates the case k = 2, where Ii keli hood contours are shown for 
different sets of data yielding different values of 81 and 82 , The likelihood is data 
translated. Specifically, from (1.3.71) it is seen that, as soon as an experimental 
t We refer to the function 
lVI-Ill 
I(x) = 
exp [_ .1 (x -
~)' V- I (x 
~)], 
(2nf/2 
2 
where x' = (x I' , .. , x p) and !1' = (111' .. " I1p) are p x 1 vectors, and V is a p x p positive 
definite symmetric matrix, as the multivariate Normal function, When x are random 
variables, the function becomes the multivariate Normal distribution with mean vector 
~ and covariance matrix V, 

48 
Nature of Bayesian J nference 
1.3 
design has been decided and hence X is known, all features of the likelihood except 
its location are known prior to the data being taken. By contrast, location of the 
likelihood is determined through 6 solely by the data. 
The idea that little is 
known a priori relative to what the data will tell us is therefore expressed by a 
prior distribution such that, locally, 
p(9 I 0) cc c. 
(1.3.73) 
On multiplying the likelihood in (1.3.71) by a nearly constant prior, and 
introducing the appropriate normalizing constant so that the posterior distri-
bution integrates to one, we have approximately 
IX'XII/2 l 
1 
-" 
] 
p(9Ia,y) = 
2k/2exp --2 (9-9) XX(9-0) , 
(2M ) 
2a 
-
00 < e; < 00, 
I = I, ... , k, 
(1.3.74) 
a multivariate Normal distribution which we denote by Nk [6, (J2(X' X)-I]. 
This distribution will be discussed in detail in Section 2.7. 
For certain common situations discussed more fully in Chapter 7 this 
formulation of the multivariate prior distribution may be inappropriate. 
For 
example, it may be known that the e's themselves are a sample from some 
distribution. When such information is available and particularly when k is large, 
the locally uniform prior of (1.3.73) may supply an inadequate approximation. 
Multiparameter Data Translated Likelihoods 
In general, suppose the distribution of the data y involves k parameters 
8' = (0 1, "" ek). 
A data translated likelihood must be of the form 
1(0 I y) = 9 [<1> -
fey)], 
(1.3.75) 
where g(x) is a known function independent of y, <1>' = (<PI' , .. , ¢k) is a 
one-to-one transformation of 9, and the elements fl(Y)' ,.
"
j~(y) of the k xl 
vector fey) are k functions of y. Extending the single parameter case, a noninforma-
tive prior is supposed locally uniform in <1>. 
The corresponding noninformative 
prior in e is then 
pee) :x I J I , 
(1.3.76) 
where 
o¢ I 
a¢1 
ael 
aek 
IJI = !a;¢I""'¢k)! 
o(e1> .. " Ok) '.' ! O<Pk 
a<Pk 
ael 
aek 
+ 

1.3 
Noninformative Prior Distributions 
49 
is the absolute value of the Jacobian of the transformation. 
For the Normal 
linear model (1.3.68), 9 = «I> so that a noninformative prior is locally uniform 
in 9 as given earlier in (1 .3.73). 
Multiparameter Problems Involving Location and Scale Parameters 
Special care must be exercised in choosing noninformative priors for location 
and scale parameters simultaneously. 
As mentioned earlier, we regard a 
parameter 11 as a location parameter of a distribution p(y) if addition of a 
constant c to y changes YJ to YJ + c. Thus, the Normal mean and , more generally, 
the parameter 8 in the family of distributions (1 .3.57) are location parameters. 
The elements of 9 in the Normal linear model (1.3.68) are also location 
parameters in a general sense, since it can be shown that they are location 
parameters of the distribution of the least squares estimates 9. 
On the other 
hand, a scale parameter A of a distribution p(y) is such that multiplication of y 
by a constant c changes A to lei A. Examples of scale parameters are the Normal 
standard deviation and, more generally, the parameter (J in (1 .3.57) and in the 
linear model (1.3.68). 
Normal Mean e and Standard Deviation (J 
We first consider the choice of prior in relation to a sample from a Normal 
distribution N(8, (J2), where 0 and (J are both unknown. The likelihood of (8, (J) 
IS 
1(8, (J I y) oc (J-n exp [ - 2: 2 I. (Yu - 8)2 J . 
Now 
I. (Yu -
8)2 = I. (Yu - y) 2 + I1(Y -
8)2 
= (11 -
l)s2 + n(y - 8)2, 
where S2 "" I. (y" - y)2 /(n -
I). Thus, 
1(8, (J i y) oc (J-n exp r _ 11(8 - y)2 _ (n -
J )S2J 
2(J 2 
-
2(J2 
. 
(1.3.77a) 
(1.3.77b) 
Also, since multiplication by the constant s" leaves the likelihood unchanged, 
1(8, (J I y) oc ((JS )n exp [_ nCO - y)2 
(~ ) _ (n -
I)S2] 
2S2 
(J2 
2(J2
' 
(1 .3.77c) 
which can be written , 
{ 
n (8-Y)2 
} 
I(G, (J I y) oc exp 
- 2 -s-
exp [ - 2(1og (J -
log s)] 
x exp{ -I1(10g(J-10gs)-C;1) eXP [-2(log(J-logS)J} (J.3.77d) 

so 
Nature of Bayesian Inference 
1.3 
and is therefore of the form 
(O-y 
) 
1(8,aly)ccF -s-, loga-logs G(loga-logs). 
(l .3.77e) 
To aid understanding of this expression, Fig. 1.3.1 J shows likelihood contours 
for 8 and log a given by four different samples of n = 10 observations. 
For 
fixed s a change in y relocates the likelihood surface on the 0 axis. For fixed y 
a magnification in s appropriately relocates the likelihood surface on the log a 
axis, while at the same time its spread along the 0 axis is correspondingly 
magnified. This magnification reflects the greater uncertainty in the likelihood 
about 0 which occurs when a larger a is implied by an increase in s. 
It would 
usually be the case, however, that prior opinions about 8 bear little relationship 
to those about a, so that such magnifications would be irrelevant to the choice 
of transformations of 8. Thus, we are led to seek a transformation which, apart 
from this inherent magnification along the 8 axis, is such that the data serves only 
to relocate the likelihood function . In this case the appropriate transformation 
is clearly obtained in terms of 8 and log a. A noninformative prior is therefore 
taken to be one for which approximately log a and 8 are locally uniform, 
p(O, log a) cc c. 
(1.3.78) 
y = 34 
Y = 40 
2-
s = 4 
D 
", 
.Q 
0 
@ 
@ 
s = I 
:n 
34 
38 
40 
42 
Fig. 1.3.11 The Normal mean () and standard deviation a: contours of likelihood function 
of (8, log a) for different sets of data. 

J.3 
'1oninformative Prior Distributions 
51 
Equivalently 
pee, a) IX a- 1. 
(1.3.79) 
Employing the prior in (1.3.79) with the likelihood (1.3.77b), we found that 
the posterior distribution of (0, a) is 
(8 
I) ' 
- (n + 1) 
[ 
nCO - y)2 
(n -
I)S2] 
_ 00 < G < 00, 
a :;::. 0, 
p ,a y if. a 
exp -
2a2 
-
2a 2 
' 
(1.3.80) 
which will be discussed in detail in Section 2.4. 
Normal Linear Model, a Unknown 
We now turn to the case of the general Normal linear model in (1.3.68), and 
suppose that the standard deviation a as well as the parameters e are unknown. 
The likelihood can be written 
( 
S)" 
[ 
(n - k)S2 
S2 (e - 0)' x'xce - e)] 
lee, a I y) if. 
-;; 
exp 
-
2a2 - -
2a2 
S2 
' (1.3.81) 
where 
2 
1 
S 
= (n _ k) (y -
y)' (y -
y). 
By comparing (1.3.81) with (1.3.77b .. e), it is evident that in terms ofe and log a, 
for any fixed s, a change in 0 merely relocates the likelihood. On the other hand, 
for fixed e, the volume enclosed in a given contollr is proportional to s'. Arguing 
as in the case of (8, a), we seek a transformation which, apart from this inherent 
magnification in the space of e, is such that the data serves only to relocate the 
likelihood surface. Clearly, the appropriate transformation is obtained in terms 
of e and log a. 
A noninformative prior in this case is then one for which 
approximately loga and (8" ... ,0.) are locally uniform. 
Specifically, it is 
assumed that, locally, 
pen, log a) if. C 
or 
p(9, a) IX l f a. 
( 1.3.82) 
The corresponding posterior distribution is then 
( 
.' 
-(n+ 1) 
[ 
(n - k)S2 
(9 - o)'x'X(e - 0) ] 
p e, a I y) if (J 
exp 
-
20'2 
- - --2-0'~2---
-
00 < 0 i < if. , 
i = I, ... , k; 
a > 0, 
( 1.3.83) 
which will be further discussed in Section 2.7. 
Prior Independence Between Parameters 
(n some examples, certain parameters or sets of parameters may be judged a 
priori 
:,. 
·~;stributed independently of cert"in other parameters or sets of 

52 
Nature of Bayesian Inference 
1.3 
parameters. 
When this is so, the choice of prior distribution is sometimes 
simplified because the independent sets of parameters may be separately 
co nsidered. 
In particular, it is usually appropriate to take location parameters to be 
distributed independently of scale parameters. 
This is because any prior idea 
one might have about the location e of a distribution would usually not be 
much influenced by one's idea about the value of its scale parameter (J. Thus 
pee I 0") ='= pee). 
Consider the case of the f"ormal distribution. 
We have 
seen that for the case where (J is known, a noninformative prior for 0 is 
obtained by taking pee I (J) locally uniform . With the additional independence 
assumption this implies that p(IJ) should be uniform. A similar argument leads 
to our taking p((J) a:. l/ (J. Thus 
p(e, 0") ='= p(O)p(O") X I/(J 
( 1.3.84) 
as in (1.3.79). 
In certain circumstances, such an assumption of independence between 
e and (J could appear inappropriate. If, for example, we know that grains of sand, 
with mean weight one milligram, were to be weighed, we should expect that (J 
would be less than a milligram, and so it has been argued that if e is small, then 
(J is likely to be small also, while if e is large, (J is likely to be large. 
To this we reply, (I) that dependence of this kind is most often associated with 
a natural constraint on the data (for example, that all observations must be non-
negative); (2) that this kind of dependence is usually removed when a more 
appro'priate metric is adopted in terms of which the data are not so constrained; 
(3) that usually when there is no such constraint one does not expect this 
dependence. We illustrate with the following examples. 
An example of prior dependence removed by appropriate trans/ormation. 
Suppose we knew that the mean income () of a certain community was $5000 
and the standard deviation was $1000. Then given another community where the 
mean income was known to be $50,000, we might guess that the standard 
deviation was closer to $10,000 than to $1000. In other words, prior beliefs 
about 0 and 0" would be dependent. However, this guess is clearly based on the 
supposition that, in examples of this kind, it is the coefficient of variation 
(Jle and not (J itself which is more likely to be approximately constant over 
different values of e. 
But if this supposition is correct, then log income is the 
appropriate quantity to consider. 
For, if we denote income by y and suppose 
that q; and ), are the mean and standard deviation of log y, then 
q; = E (logy) ='= log e, 
A = v'var (logy) ='= (JIB. 
Measured in the logarithmic metric, the standard deviation A can realistically 
be assumed independent of the mean q; a priori. We notice that, in this example, 
if y is an observed income, 0 < y < co but -
co < logy < co. 

1.3 
Noninformative Prior Distributions 
53 
An example where measurements may be negative. Except in examples like the 
above, where the measurement scale has a natural constraint such as a truncation 
at zero, values of B which are small in magnitude need not be associated with small 
values of (J. 
For example, suppose we were checking the declination of a 
compass from magnetic north, using an instrument which could detect a 
declination from -.l80
C to 180. For a properly constructed compass, we would 
expect the declination B to be close to zero but this would not ordinarily affect 
our ideas about (J. 
In this book, we shall usually assume that location and scale parameters 
are approximately independent a priori. In particular, arguing as before, for the 
parameters (e, (J) in the linear Normal model, we suppose that pee, (J) == 
p(e)p((J) so that pee) == pee I (J) and p((J) == p((J I e). 
It then follows that 
pee, (J) ex (J-l as given in (1.3.82). 
Extension of Jeffreys' Rule to Multiparameter Models 
In the multiparameter examples discussed above, transformations were avaiJable 
which had the property that, apart from the inherent magnifying effect of the 
scale factor-in the location parameter space, the likelihood was data translated. 
Although transformations of this kind are available for many of the applications 
discussed in later chapters, they are not available in general. In some cases, then, 
to obtain noninformative prior distributions, we must rely on a somewhat less 
satisfactory argument leading to the multiparameter version of Jeffreys' rule. 
If the distribution of y, depending on k parameters e, obeys certain regularity 
conditions, then, for sufficiently large samples, the likelihood function for e and for 
mild transformations of e approaches a multivariate Normal distribution. The 
log likelihood is thus approximately quadratic, 
_ 
n 
_ 
_ 
L(e I y) = log I(e I y) == L(e I y) - 2 ce - e)'Dil(e - e), 
(1.3.85) 
where e IS the vector of maximum likelihood estimates of e and - nDiJ is the 
k x k matrix of second derivatives evaluated at e, that is, 
{
I 
Z2 L } 
De = 
-
-;; oBjoBj 
B' 
i,j = 1, ... , Ie 
In general, De will depend upon alJ the data y. But for large n, it can be closely 
approximated by 
( 1.3.86) 
which is a function of e only. Specifically, JPnCe) is the matrix function 
(1.3.87) 

54 
Nature of Bayesian Inference 
:.3 
where the expectation is taken with respect to the data distribution p(y I 0). 
In 
other words, ..?",,(9) is the information matrix associated with the sample }. 
:-.Jow, ideally one would seek a transformation <I>(e) such that ..?"n(<J,) would 
be a constant matrix independent of <J, so tha t the likelihood would be 
approximately data translated. Becaus(; this is not possible in general , we may 
seek a transformation <I> which ensures that the content of the approximate likeli-
hood region of <1>, 
(<I> - 4>)"§n (<I» (¢ -
<1» < const. , 
( 1.3.88) 
remains constant for different <J,. 
Since the square root of the determinant, 
"§II(<J,)II/l, measures the volume of the likelihood region , the above requirement 
is equivalent to asking for a transformation for which the I·§" (<i»I is independent 
of <1>. 
To find such a transformation , note that 
..?",,( <1» = '\5,,(9)A', 
(1.3.89) 
where A is the k x k matri x of partial derivatives, 
Thus, 
(1.3 .90) 
whence the above requirement wiJJ be satisfied if <l> is such that 
(1.3.9\) 
and an approximate noninformative prior is one which is locally uniform in <1>. 
The corresponding noninformative in 9 is then 
that is, 
(J .3.92) 
From the above we obtain the following rule: 
Jeffreys' rule for multiparameter problems: The prior distribution for a set of 
parameters is taken to be proportional to the square root of the determinant of 
the information matrix . 
As in the case of a single parameter, Jeffreys derived this general rule by 
requiring invariance under parameter transformation. He himself pointed out that 
this multiparameter rule must be applied with cautio n, especially where scale and 
location parameters occur simultaneously. We first consider an application where 
no difficulty occurs. 
\, 
~ 
. 
' . 
. 
, 
. . 

1.3 
~oninformative Prior Distributions 
ss 
M ultinomiai Distribution 
Consider the deriva tion of an appropriate prior for the parameters of the 
multinomial distribution. Sunpose the result of a trial must be to produce one of 
m different outcomes, the probabilities for which are 71: 1,71: 2 , • . . , 7[",. 
Thus, the 
trial might consist of the random drawing with replacement of a ball from a bag. 
The probabilities eQuid then refer to the proportions n' = (7T I' ... , 7T",) of balls 
of m different colors where IT", = I - L;n [17T i , Suppose n independent trials were 
made, resulting in a sample of y' = (y I' ... , Ym) balls of the various types, where 
Ym = n -
L;'~-/ Yi' Then 
so that 
n! 
p(yln) = 
( 
')-
(7T~')(7Ti2) .. , (7T~:") 
(y I !) (Yl ') ... y",. 
m 
L = log !(re I y) = L Yj Jog ni' 
j - I 
On differentiating, we obtain 
and 
YIII n; , 
i, j = I, . ., m -
t. 
Taking expectations over the distribution p(y I Tt), we have 
11 
n 
- E(bj ) 
= -
+ - - , 
7Ij 
Tt m 
n 
- E(b;) =-, 
1[m 
After some algebraic reduction we find that 
Thus, Jeffreys' rule says that we should [;ike for a noninformative prior 
(1.3.93a) 
(1.3.93b) 
(1 ,3.94 ) 
(1.3.95) 
( 1.3.96) 
(1.3.97) 
For the case where little is known a priori about the probabilities, this leads to 
the posterior density 
(1,3.98) 
which is proportional to the likelihood for n. with each cell frequency reduced 
by one half. 
In the particular case k = 2, we obtain the binomial result 
(1.3.26) considered earlier. 

56 
:-.Jature of Bayesian Inference 
Fig. 1.3.12 Likelihood regions of different shape having the same size. 
Some Comments on the Application oj the .'v!ultiparameter Jeffreys' Rule 
The mUltipara meter version of Jeffreys' rule leading to pCS) cc IJ,,(S)I : corres-
ponds with a less stringent and less convincing transformation requirement on 
the likelihood than data translation. Specifically, if approximate Normality of the 
likelihood function is assumed, the rule seeks a transformation which ensures, 
irrespective of the data, that corresponding likelihood regions for <p are of the same 
size. As illustrated in Fig. 1.3.12 for the case of Ie = 2 parameters, regions can of 
course be of the same size and yet be very different. 
A further difficulty associated with the blanket application of Jeffreys' rule 
arises when parameters of different kinds are considered simultaneously. 
We 
have already seen, for example, that when the mean fJ and standard deviation (Y 
of a Normal distribution are being considered simultaneollsly- -·see Fig. 1.3.11 , 
it is not usualJy appropriate to seek a transformation which produces likelihood 
regions of the same size. To further appreciate the difficulty, we discuss again the 
choice of prior for the location and scale parameters of the Normal distribution 
N(e, (Y2). 
Location and Scale Parameters 
Jeffreys argued that in cases where fJ and (Yare known to be independent 
a priori, priors for the two parameters should be considered separately, leading to 
pee,O') == p(fJ)p (0') CC 0'-1 as in (J .3.84). 
Now, the information matrix of (e, a-) is 
[
(J-2 0] 
[J(e) 
5" (e, (J) = n 
= n 
o 2(Y- 2 
0 
(1.3.99) 
[f the pnor independence of (e, O') were ignored, then application of the 
generalized rule would lead to a prior locally proportional to 
IJn(O, (J)1 1. 2 CC (y-2, 
(1.3.100) 

1.3 
~oninformative Prior Distributions 
57 
which differs from the prior in (1.3.84) by an additional factor (J-I. Some light 
is thrown on this factor if we consider the problem from a transformation 
point of view. 
Suppose 1((8) and ¢(a) are, respectively, one-to-one transformations of 8 
and (J. Then the information matrix of (I(, ¢) is 
o 1-
[5(J() 
_ 21 da 12 
- n 
0 
2a 
d¢ 
(1.3.1 OJ) 
and by setting¢(a) = log a, the lower right hand element in the matrix, representing 
information about ¢((J), is made independent of a so that 
[ 
-2 1d812 
5"[K(8), log a] = n 
a 
odK I 
(1.3 .102) 
This reflects the fact that whatever transformation /(8) is made on 8, information 
about /( will be inversely proportional to a- 2_ data having a smaJi a lead to a 
more accurate determination of the location parameter 8 or of any transformation 
K(O) of it. On the other hand, the metric for which the information is independent 
. of 8 is clearly e itself. 
The situation can be related to the likelihood regions illustrated in Fig. 1.3. I I. First, 
the zero off-diagonal elements of the matrix (1.3.102) reflects the fact that the "axes" of 
the likelihood contours lie parallel to the B and log a axes. Second, independence of 
5 (log a) and 5(8) with respect to 8 corresponds to the fact that, for a fixed s, the 
likelihood is merely relocated when ji is changed. Third, constancy of 5(1og a) has led to 
a relocation of the likelihood along the log a axis when s is changed. Finally, the same 
change in s magnifies the spread of the likelihood along the e axis because ..f (8) C( a · 2. 
Thus, when the assumption of prior independence of 8 and a is incorporated, 
the fact that the information of 8, or any function of it, is proportional to a- 2 
is clearly irrelevant to the choice of a noninformative prior for B. The additional 
factor a- 1 in (1.3.100) arises only from a misapplication which ignores pnor 
independence. 
A similar situation occurs for the linear Normal model in (1 .3.68). 
The 
informa".ion matrix for 9 and log is a 
5" (9, log a) = [ 
0] 
where 
211 ' 
(1.3.\03) 
Thus, 
1..f,,(9, log a)1 1, 2 ex a .. 
Arguing exactly as above, the factor a- k is irrelevant and the appropriate prior 
distribution is p(9, log a) ex c or p(S, a) ex a-I as in (1.3.82). 

S8 
Nature of Bayesian Inference 
1.3 
Finally, for the general location-scale family of (1.3.57), 
the information matrix of (8, log 0") is 
b 
-2 
[ 
10" 
~1I(8, log 0") = n 
0 
(1.3.104) 
where b1 and b2 are two positive constants independent of (8,0"). If 8 and 0" 
are considered independent a priori, then Jeffreys' rule applied separately yields 
p(8, log 0") oc C 
or 
1 
p(8, 0") oc -, 
0" 
whence the corresponding posterior distribution of (8,0") is 
(1.3.105) 
p(8, 0" I y) oc 0"-(11+1) r1 h (YII - 8), 
-
00 < 8 < 00, 
0" > O. 
(1.3.106) 
u= 1 
(j 
Necessity for Individual Consideration of Mu/tiparameter Prior Distributions 
Choice of noninformative prior distributions in multiparameter problems requires 
careful consideration in each particular instance. 
Although devices such as 
Jeffreys' rule can be suggestive, it is necessary to investigate transformation 
implications for each example in the light of any appropriate knowledge of prior 
independence. 
1.3.7 Noninformative Prio .. Distributions: A Summary 
In the previous sections, methods have been developed for selecting prior 
distributions to represent the situation where little is known a priori in relation 
to the information from the data. 
The concept of a "data translated 
likelihood" leads to a class of what we call "noninformative" priors. 
In the 
case of a single parameter, the resulting prior distributions correspond exactly 
to those proposed by Jeffreys on grounds of invariance. 
A noninformative prior does not necessarily represent the investigator's 
prior state of mind about the parameters in question. 
It ought, however, to 
represent an "unprejudiced" state of mind. In this book, noninformative priors 
are frequently employed as a point of reference against which to judge the kind 
of unprejudiced inference that can be drawn from the data. 
The phrase "knowing little" can only have meaning relative to a specific 
experiment. 
The form of a noninformative prior thus depends upon the 
experiment to be performed, and for two different experiments, each of which 
can throw light on the same parameter, the choice of "noninformative" prior Gan 
be different. 

Table 1.3.1 
A summary of noninformative prior and corresponding posterior distributions 
Parameter(s) 
Binomial mean n 
Multinomial means ~I' ""n m 
Poisson mean A 
Normal mean 0 (0' known) 
Normal standard deviation 0' (0 known) 
Normal 0 and 0' 
Normal linear model 9 (0' known) 
Normal linear model 9 and 0' 
Location-scale family 0 (0' known) 
Location-scale family 0' (0 known) 
Location-scale family 0 and 0' 
Noninformative Prior 
n- t (I -
n)- -!-
- + 
- + 
n l - ",nm -
, 
'. 
/, 
2 
c 
0" 
I 
0'- I 
c 
0' - I 
c 
0'-1 
0' 
Posterior 
nY -;' (I - n)"-y-t 
n~l-l .. n::;~ I - ·~ 
}:Y- 'i exp (-n),) 
[ 
n(B _ jl)2] 
exp 
-
20'2 
_(11+1) 
[I(Yu- O)2] 
0' 
exp 
-
2 
20' 
_ (/1+1) 
[nCO-ji)2 
I( y" -ji)Z] 
0' 
exp 
-
20'2 
-
20'2 
l 
(S - ih'X'X(S - 9)] 
exp 
-
20'2 
v 
exp 
-
--.,;..-:..-....:.-"""'::"'~-.:.....:..:..:.::.::.......::.: 
~ -- (II+I) 
r (y - y)' (y - y) + (9 - 9)' X 'X(9 - 9)] 
20' 2 
ri h(~) 
1, = 1 
a 
O'-(n+ I) (I h( Y" - B) 
1/ = I 
0' 
0'-("; I) rI h(Y" - 0) 
u=1 
0' 
(., 
:z 
o 
'" :i' 
0' 3 
2? <' 
<> 
~ 
o· 
.., 
o 
",' 
:; 
0' 
S. 
o· 
'" 
en 
Ut 
;a 

60 
"Iature of Bayesian Inference 
1.4 
When more than one parameter is involved, the problem of choosing 
noninformative priors can be complex. Each problem has to be considered on 
its merits. Careful consideration must be given to transformation implications 
and to knowledge of prior independence. 
Considerable literature exists concerning the choice of prior distribution 
to characterize a state of "ignorance". Jeffreys himself has discussed a number 
of criteria for choosing prior distributions, invariance being the most important 
among them. 
Other contributions in this area include those of Huzurbazar 
(1955), Perks (1947), Welch and Peers (/963), Novick and Hall (1965), Novick 
(1969), Hartigan (1964, 1965) and Jaynes (1968). 
Table 1.3.1 summarizes the results obtained in Sections 1.3.1 - 6 of the 
noninformative priors, and the corresponding posteriors for the parameters of the 
various models considered. 
The distributions are given in unnormalized form 
and we suppose that n observations are available. 
1.4 SUFFICIENT STATISTICS 
When discussing the example in which the mean e of a Normal distribution 
was supposed unknown but a was known, we found in (t.3. I) that the only function 
of the observations appearing in the likelihood, apart from the sample size 11, 
was the sample average ji. Thus, for example, if a was known to be equal to 
unity, the likelihood would be 
I(e I a, y) = exp [ -
; (ji -
e)2 J . 
Since the data enter Bayes' formula only through the likelihood, it follows 
that all other aspects of the data, with the exception of ji, are irrelevant in 
deciding the posterior distribution of e and hence in making inferences about e. 
In these circumstances, following Fisher (1922, 1925), ji is said to be sufficient 
for e, and is called a sufficient statistic for e. 
Similarly when, for a Normal 
sample, e is assumed known but a is unknown, the likelihood corresponds to 
(1.3.6) and the posterior distribution employs the data only through nand 
S2 = L(Y - O)2jn. 
In this case, S2 is said to be sufficient for a (or for a 2). 
Further, when both e and a are unknown. the joint likelihood function for (e, a), 
given a random sample from the Normal distribution N(e, a 2 ), IS 
[ 
1 
II 
] 
lee, a I y) = a-II exp 
- --2 L (Yu _ e)2 , 
2a 
u~ 1 
(1.4.1) 
which, as in (1.3 .77a), by setting S2 = L (YII -
y)2/(n -
I) can be written 
(J .4.2) 

1.4 
Sufficient Statistics 
6) 
Apart from n, the only functions of the observations involved are ji and S2, 
which are said to be joinlly sufficient for 0 and (J. Jf n is given, y and S2 can be 
constructed from knowledge of Iy" and I (y" - jl)2 or from I y" and I y,; , so 
that anyone of these pairs of quantities are jointly sufficient for 0 and (J. 
Of course, it would also be true that the three quantities Iyu , I(yu _ jl)2, 
I y,; were sufficient for 0 and (J. 
However, since 
I(yu - y/ = IY,7 - n- 1 (Iy,i 
there is an obvious redundancy. The notion is therefore used of a "minimally 
sufficient set" of statistics, which contains the smallest number of independent 
functions of the data needed to write down the likelihood. 
In this example, 
assuming n given, a minimally sufficient set contains two functions which could 
be chosen in any way which allows the likelihood to be written down. 
In 
particular, they could be anyone of the three choices (y, S2), (I y", I(y" - y/), 
or (I y u, Iy,~). 
Since n is also needed to write down the likelihood, this 
quantity is sometimes treated as a statistic and added to the sufficient set. 
We see that for 
~ormal samples sufficient statistics are available which 
conveniently " match" the parameters. 
For example, y and S2 are sample 
quantities which one would expect to supply information about 8 and (J. 
Convenient parsimonious sets of sufficient statistics do exist for some other 
distributions. In general, however, if we have k parameters in the likelihood, the 
minimal sufficient set of q ~ k functions of the data which appear in the 
likelihood function will not be such that q = k . Consider, for example, the dis-
tribution 
p(y 10, (J) ex (J- J exp [- ( y : 0 fJ, - 00 < y < (x: . 
( 1.4.3) 
The likelihood function based upon 11 independent observations is 
[ 
n 
(Y - 8)4J 
/(0, (J I y) :x (J-It exp - L -"--
II =-:: I 
(J 
(J .4.4) 
= (J -It exp [;4 ( - S4 -~ 40S J -
60 2 S 
2 + 40 J S 
1 -
04 ) ] 
( 1.4.5) 
where 
Sp = 2:::: 
1 y;'. 
In 
this case, 
then, a minimal 
sufficient set of 
statistics for the k = 2 parameters 0 and 
(J consists of the q = 4 sums 
5 1,52,53 , and 54' Note, however, that if the power in the exponent in (1.4.3) 
had not been an integer (suppose, for example, it had been 4.1 instead of 4.0) 
then no tinite expansion of the form of (IA.5) would have been possible, and the 
q = n observations themselves would have been a minimally sufficient set of 
statistics. 
In general, we may define sufficient statistics as follows. 
Definition (1.4.1) 
Let y' = (YI' ... , YIt) 
be a vector of observations whose 
distribution depends upon the k parameters a' = (8 1, ... , Ok)' Let l' = (tl, .. " tq) 

~ature of Bayesian Inference 
1.4 
be q functions of y. Then the set of statistics t is said to be jointly sufficient for 9 
if the likelihood function I(e I y) can be expressed in the form 
1(9Iy)et:.g(e l t), 
(J .4.6) 
and provided the ranges of O. if dependent on the observations, can also be 
expressed as functions of t. 
Thus, considering distributions which have been used as examples in this 
chapter, sufficient statistics exist for the parameters (e, CT) in the Normal 
distribution, for (e, CT) of the Normal linear model in (1.3.68), for the Poisson 
mean X in (1.3.39), for the binomial mean n in (1.3.21), for the multinomial means 
(nl ' ... , nm ) in (J .3.93a), and for (e, a') in the "fourth power" distribution in . 
(1.4.3). r n the case of a si ngle parameter e, if y is a random sample from the 
distribution p(y I 8) and the range of y does not depend on 8, then it was shown 
by Pitman (1936) that a single sufficient statistic exists for 0 if and only if 
p(y I e) is a member of the exponential family previously referred to in (J .3.38). 
When the ranges of the observations yare independent of 0, a useful property 
of sufficient statistics is given by the following lemma. 
Lemma 1.4.1 
Let 
t be jointly sufficient for e, having joint distribution 
pet I 9) Then, 
1(91 y) et:. II (91 t) 
where 
11(91 t) cc pet I e). 
(1.4. 7) 
In other words, the likelihood function obtained from the distribution of t is the 
same as that obtained from the distribution of y. 
Proofs of the lemma can be found in standard text books such as 
Kendall and Stuart (1961) and Wilks (1962). Two examples follow. 
I. Normal mean, variance known. 
We have seen in (1.3.1) that if y' = 
(Yl' ... ,y,,) is a random sample from N(e, CT 2), where CT 2 is assumed known, then 
the likelihood function is 
Ice I CT, y) cc exp [ - 2:2 (8 - 51?]' 
( 1.4.8) 
Now, the sample mean 51 is distributed as 
00 < 51 < 00 , 
(1.4.9) 
so that, given 51, 
(1.4.10) 
which is the same as in (1.4.8). 

1.4 
Sufficient Statistics 
63 
2. Normal distribution, both mean and variance unknown. I n this case, the likelihood 
function /(0, (J ! y) based upon the n independent observations y is that given in 
(l.4.2). Now, it is well known that 
a) y is distri buted as N(e, (J2 jn), 
b) (n -
1)52 = L"ey" - ji)2 is distributed as (J2 X,7_l' and 
c) y and 52 are statistically independent. 
Thus, 
p(jl, .1'2 I 0, (J) = p(y I e, (J2) p(5 2 I (J2), 
where p(y 10, (J2) is that in (1.4.9) and 
[ 
(n -
I)S2] 
x exp 
-
2 
' 
2(J 
It follows that, given (y,S2), 
I(e, (J I y, .1'2) rx (J-II exp [ -
2~2 [(n - 1)52 + nee _ ji)2]} , 
which is identical with (1.4.2). 
1.4.1 Relevance of Sufficient Statistics in Bayesian Inference 
(1.4.1 I) 
(1.4.12) 
(1.4.13) 
Sufficient statistics playa vital role in sampling theory. For, if inferences about 
fixed parameters are to be made using the distributional properties of statistics 
which are functions of the data, then, to avoid inefficiency due to leakage of infor-
mation, it is essential that a small minimally sufficient set of statistics be available 
containing all the information about the parameters. 
By happy mathematical accident such sets of sufficient statistics do exist 
for a number of important distributions and, in particular, for the Normal 
distribution. 
However, serious difficulties can accompany the exploration of 
less restricted models which may be motivated by scientific interest, but for 
which no convenient set of sufficient statistics happens to be available. 
Because Bayesian analysis is concerned with the distribution of parameters, 
given known (fixed) data, it does not suffer from this artificial constraint. 
It 
does not matter whether or not the distribution of interest happens to have the 
special form which yields sufficient statistics. 
For example, the likelihood, and 
hence the posterior density, can be calculated with almost the same ease from 
(1.4.4), which expresses the likelihood in terms of the data, as it can from 
(1.4.5), which expresses it in terms of the sufficient statistics alone. Furthermore, 
very little more effort would be needed to compute (1.4.4) if the power in the 
exponent were 4.1 or some other noninteger value. 

64 
Nature of Bayesian Inference 
1.4 
1.4.2 An Example Using the Cauchy Distribution 
To illustrate these points further, we consider the problem of making inferences 
about the location parameter e of the Cauchy distribution 
-
00 < y < 00, 
(1.4.14) 
where from the form of the distribution it is apparent that no summarizing 
statistics exist and that the observations themselves are a minimum sufficient 
set. 
This fact does not embarrass the Bayesian approach . 
The Cauchy 
distribution is a special case of the location scale family (1.3.57) and , using the 
argument leading to (1.3.61), a noninformative prior is locally uniform in 
e, whence the posterior density function is immediate. 
To provide numerical 
illustration, a sample of n = 5 observations (11A, 7.3, 9.8, 13.7, 10.6) were 
randomly drawn from the Cauchy distribution shown in Fig. 1.4. 1. 
Thus, 
assuming little were known about e a priori. the posterior distribution is 
approximately 
p(e I y) == cH(e), 
-
00 < e < 00, 
(1.4.15) 
where 
the factor 105 is merely a convenient multiplier, and c is the normalizing constant. 
The posterior distribution obtained by evaluating this expression for a suitable 
series of values of 8 is shown in Fig. 1A.2. Thus, in spite of the fact that we do not 
have a sufficient statistic for e, the posterior distribution, from which inferences 
can be made, is easily determined. 
Calculation oj p(8 I Y) 
To obtain the density explicitly, we need to determine the normalizing factor c. That is, 
we have to evaluate the integral 
f
+ ' 
c-
l = 
-00 H(e)d8. 
(1.4.l6) 
pr y 0 ) 
0.3 
0.2 
0.1 
6 
14 
16 
Fig. 1.4.1 Density curve of a Cauchy distribution (dots show 5 observations drawn from 
the distribution). 

1.4 
Sufficient Statistics 
65 
p((J , y) 
o.s 
0.4 
0.3 
0.2 
0.1 
6 
8 
10 
12 
14 
16 
Fig. 1.4.2 Posterior distribution for the location parameter 0 of a Cauchy distribution 
(noninformative prior), given the data shown in Fig. 1.4.1 . 
. 
Also, to obtain the probability that Ii is less than some value 00' we need to evaluate the 
ratio 
( 1.4.17) 
Frequently the integrals which occur in applications of Bayes' formula will not possess 
convenient solutions in closed form, or solutions which have been tabulated. However, 
this is of little practical importance . . 
In the first place, the best way to convey to the experimenter what the data tell 
him about 8 is to show him a picture of the posterior distribution. For this purpose, the 
scale of the vertical axis is superfluous and it is sufficient to plot H(8). In the second place, 
just as the question of the existence of convenient sufficient statistics ought to be irrelevant, 
so should the question of whether or not an integral happens to be one which can be 
expressed in terms of tabled functions . As the Bayesian approach sets us free from the 
yoke of sufficiency, so numerical integration and the availability of computers set us free 
from the need to worry about the "integrability in closed form" of the function. 
For univariate distributions, even summing ordinates or drawing the distribution on 
graph paper and counting squares could approximate the integral with sufficient accuracy 
for most practical purposes. Since small differences in probability cannot be appreciated 
by the human mind , there seems little point in being excessively precise about uncertainty. 
For illustration, a specimen calculation using Simpson's rule is shown in Table 1.4.1. 
Suppose for the Cauchy example that for some reason p(O I y) itself is needed and not merely 
H(O), and that the probability Pr{8 < 11.5} is required. The first column in the table 
shows 0 at intervals of 0.5 over the range of interest. The second column shows the 
corresponding value of H(8) to the nearest whole number. The third column shows the 

66 
Nature of Bayesian Inference 
1.4 
approximate integral given by Simpson's rule. Thus, for example, 
J
8. S 
0.5 
H(O)dO:, -[(I xO) + (4 x I) + (2x2) + (4x 5) + (1 x II)J = 6.5. (104.18) 
-ro 
3 
Proceeding in this way we find that 
f
+ ro 
c- I = 
- ro H(O) de :, 535.0 
(1.4.19) 
so that c = 0.001869, whence p(8/ y) is calculated and entered in the fourth column. The 
values for the cumulative probability 
f
oO 
fOO 
_oop(B / y) de = c 
- oc Hee) dO 
( 1.4.20) 
are given in 
the fifth column. 
In particu lar (see Fig. 
104.2), we find 
that 
Table 1.4.1 
Calculation of the posterior density function and cumulative distribution function 
for the location parameter B of a Cauchy distribution 
() or 00 
H(O) r
O 
_ if, H(B) dO 
p(G/y) = cH(O) fO 
fOO 
-.<J p(B/y) dO = c 
-
00 H(e) dO 
6.5 
0 
0 
0.000 
0'000 
7.0 
0.002 
7.5 
2 
1.0 
0.004 
0.002 
8.0 
5 
0.009 
8.5 
II 
6.5 
0.021 
0.012 
9.0 
28 
0.052 
9.5 
83 
40.8 
0.155 
0.076 
10.0 
196 
0.336 
10.5 
291 
233.8 
0.544 
0.437 
11.0 
250 
00467 
11.5 
129 
470.5 
0.241 
0.879 
12.0 
47 
0.088 
12.5 
17 
526.2 
0.032 
0.983 
13.0 
7 
0.013 
13.5 
2 
534.0 
0.004 
0.998 
14.0 
I 
0.002 
14.5 
0 
535.0 
0.000 
1.000 
+00 
-I J H(B) d() = 535.0, 
c = 0.001869. 
c 
= 
-
00 

1.5 
Constraints on Parameters 
67 
Pr{O < 11.5} = 0.879. Values of the cumulative probability for intermediate values can 
be found by interpolation in column five. Greater accuracy is obtainable by using a finer 
interval in O. 
1.5 CONSTRAINTS ON PARAMETERS 
Examples occur later in this book where, as part of the model, certain constraints 
must be imposed on the values which the parameters B can take. Such problems 
can usually be dealt with by choosing the prior distribution so as to include 
the constraint. 
Alternatively, it is sometimes more convenient to solve a 
fictitious unconstrained problem, and then modify the solution to take account 
of the constraint. 
Tn general, let D be the unconstrained parameter space of 0 and let C be a 
constraint, such that 
(1.5.1) 
where Dc is a subspace of D. Let 9s be a subset of 0 (which could be 0 itself), 
and Rs c Dc be a region in the parameter space of as. 
Then, by definition of conditional probability, the posterior probability that 
Os E Rs, given the constraint C, is 
Pr{C lOsE Rs, y} 
Pr{OsERsl c,y} = Pr{OsERsly}-----
-
Pr{Cly} 
( 1.5.2) 
It follows that the posterior distribution of 9s given the constraint C can be 
written 
Pr{CI9s,Y} 
pCBs I C, y) = pees I y) 
Pr{C I y } 
( 1.5.3) 
It sometimes happens that C takes the form 
C: r(e) = d 
( 1.5.4) 
where f is a vector of q functions of 0, and d a vector of q constants. In this case, 
p(Cles, y) 
pCBs I C, y) = pCBs I y) 
p(C I y) 
(1.5.5) 
Note that Pr(C I y) and peC I y): are constants independent of es. 
Thus, the 
posterior distribution of Bs, given the constraint C, is equal to the posterior 
distribution for as which would have been obtained if no such constraint were 
applied, multiplied by a modifying factor. This modifying factor is proportional 
to the conditional probability, or the conditional density, of the constraint C 
given es. 

68 
Nature of Bayesian Inference 
1.5 
As an example, suppose we wished to make inferences about the percentage conversion 
of a certain chem.ical obtained in a particular experiment. Suppose the percentage con-
version was determined by a biological assay method which was unbiased but was subject 
to fairly large approximately Normally distributed errors having known standard 
deviation IJ = 4. Suppose finally that the results of four analytical determinations were 
Yl = 93, Y2 = IOJ , Y3 = JOO and Y4 = 98, yielding a sample average of ji = 98. We need 
to consider what inferences could be made about (J, bearing in mind that values of e 
greater than 100 are impossible. 
If it were reasonable to suppose that in the relevant neighborhood the prior of 0 is 
locally uniform for e < JOO, then the problem could be solved by the straightforward 
application of Bayes' theorem. We have 
pCB I y) oc I(B I y)p(B) 
where in the relevant region 
Thus 
wh~re 
{ 
== const B < 100, 
p(O) 
= a 
0> 100. 
,/n 
[n 
2] 
p(O I y) = c ~ 
exp 
-
~2 ce - ji) 
, 
" 2rr IJ 
21J 
c- I = ;v 
exp 
--2(O-ji)2 dO, 
n J 
I 00 
[11 
] 
v 2n (J 
- 0'0 
2(J 
(1.5.6) 
(1.5.7) 
0<100, 
(1.5.8) 
1/ = 4, 
ji = 98. 
As illus1rated in Fig. 1.5.1, the posterior distribution of (J is proportional to the uncon-
strained likelihood for (J < 100 and is zero for B > 100. The posterior distribution for e is 
thus a Nor'mal distribution N( ji, (J2 jn), with ji = 98 and IJ/ n = 2, truncated from above 
at e = JOO. The normalizing constant in (1.5.8) is c = 1.189. 
It is chosen so that its 
reciprocal c- I = 0.8413 is the area under unit Normal curve truncated at one standard 
deviation above the mean. 
To illustrate that results of this kind can be obtained equally well by an application 
of (1.5.3), consider first the (fictitious) unconstrained problem in which (J is not limited in 
any way and is supposed to have a locally uniform distribution over the whole region in 
which the likelihood is appreciable. With this setup the unconstrained posterior distribu-
tion of B would be an untruncated Normal distribution having a standard deviation of 2 
and centered at ji = 98. 
is 
For this problem, the constraint is C: 0 < JOO so that the modifying factor of (1.5.3) 
Pr{CIB, y} 
Pr{ C I y} 
Pr{e < 100 Ie, y} 
Pr{O < 100 I y} 
(1.5.3) 
. 
• . 
• 
&. 

1.5 
Constraints on Parameters 
69 
Now the numerator of this expression is one if () < 1 ~O, and zero otherwise. Furthermore, 
the denominator is the unconstrained posterior probability that 8 < 100 given the data. 
This is the probability that an unrestricted Normally distributed random variable with 
mean 98 and standard deviation 2 would not exceed 100 and is precisely the same as c- I 
in (1.5.8). The effect, then, of the modifying factor is to multiply the Normal density by 
1.189 if 8 < 100, and by zero if 8 > 100, leading to the same result as before. 
1(0 y) 
0. 2 
0.1 /' 
~~~--~------~------~------~------~----~-e > 
94 
96 
98 
100 
102 
a) Unconslra'ned slilndardized likelihood 
p(O i y) 
0.2 
0.1 / 
~~---L------~--__ ~L-____ -L ______ e . 
y 
94 
96 
98 
100 
h) Conslra ined po slerior dislriblilio n o r e 
Fig. 1.5.1 Posterior distribution of the Normal mean 8 subject to the constraint 8 < 100. 
The device of first solving a fictitious unconstrained problem and then 
applying constraints to the solution is particularly useful for variance component 
problems discussed in Chapters 5 and 6. 
In deriving solutions and in 
understanding them, it is helpful to begin by solving the unconstrained problem, 
which allows negative componeDts of variance, and then constraining the 
solution. 

70 
;\Iature of Bayesian inference 
' .. 6 
1.6 NUISANCE PARAMETERS 
Frequently the distribution of observations y depends not only upon a set 
of r parameters 0 1 = (8 1, . •. , Or) of interest, but also on a set of, say t - r 
further nuisance parameters 9 2 = (8r • l ' . .. , OJ 
Thus we may wish to make 
inferences about the mean 8 = 01 of a Normal population with unknown variance 
CJ 2 = 82 , 
Here the parameter of interest, or inference parameter, is the mean 
8 = 01, wh ile the nuisance. or incidental, parameter is CJ2 = 8 2 , Again, we may 
wish to make inferences about a single parameter 0i in the Normal theory linear 
model of (1.3.68) involving k + I parameters 8 1, O2 , .•. , 8k and CJ 2 . 
In this 
case, 0[, ... , 8 i -
I ' 8 i ' j, ... , 8k and CJ 2 all occur as nuisance parameters. 
In the 
above examples, where sufficient statistics exist for all the parameters, no 
particular difficulty is encountered with the sampling theory approach . 
But 
when this is not so, difficulties arise in dealing with nuisance parameters by 
non-Bayesian methods. Furthermore, even when sufficient statistics are available, 
examples can occur in sampling theory where there is difficulty in eliminating 
nuisance parameters. One such example is the Behrens-Fisher problem which will 
be discussed in Section 2.5. 
In the Bayesian approach, " overall" inferences about 8 1 are completely 
determined by the posterior distribution of 8 1, obtained by "integrating out" 
the nuisance parameter 8 2 from the joint posterior distribution of 8 1 and 8 2 , 
Thus, 
(1.6. J) 
where R2 denotes the appropriate region of 9 2 , 
Now we can write the joint posterior distribution as the product of the 
conditional (posterior) distribution of 8 1 given O2 and the marginal (posterior) 
distribution of 9 2 , 
( 1.6.2) 
The posterior distribution of 8 j can be written 
(\.6.3) 
in which the marginal (posteri or) distribution p(8 2 I y) of the nuisance parameters 
acts as a weight function multiplying the conditional distribution p(8 1 192, y) of 
the parameters of interest. 
1.6.1 Application to Robustness Studies 
It is often helpful in understanding a problem and the nature of the conclusions 
which can safely be drawn, to consider not only p(O, I y), but also the 
components of the integrand on the right-hand side of (1.6.3). 
One is thus Jed 
to consider the conditional distrihutions of 8, for particular values of the 

1.6 
Nuisance Parameters 
71 
nuisance parameters 9z in relation to the distribution of the postulated values 
of the nuisance parameters. 
]n particular, in judging the robustness of the inference relative to 
characteristics such as non-Normality and lack of independence between errors, 
the nuisance parameters 9 2 can be measures of departure from Normality and 
independence. The distribution of the parameters of interest 9), conditional on 
some specific choice O2 = 9zo , will indicate the nature of the inference which we 
could draw if the corresponding set of assumptions (for example, the assumptions 
of Normality with uncorrelated errors) are made, while the marginal posterior 
density p(9z = 9zo I y) reflects the plausibility of such assumptions being correct. 
The marginal distribution p(9 1 I y). obtained by integrating out 9z, indicates the 
overall inference which can be made when proper weight is given to the various 
possible assumptions in the light of the data and their initial plausibility. 
Examples using such an approach will be considered in detail in Chapters 
3 and 4. 
1.6.2 Caution in Integrating Out :'IIuisance Parameters 
As has been emphasized by Barnardt, cation should be exercised in integrating Olll 
nuisance parameters. In particular, if the conditional distribution p(9 1 I 9z, y) in (1.6.3) 
changes drastically as 9z is changed, we would wish to be made aware of this. It is true 
that whatever the situation, p(9 1 I y) will theoretically yield the distribution of 9 1, How-
ever, in cases where p(9 1 I 9z, y) changes rapidly as 9 2 is changed, great reliance is being 
placed on the precise applicability of the weight function p(9 2 I y), and it would be wise to 
supplement p(9 1 I y) with auxiliary information. 
Thus, if we wished to make inferences about 9 1 alone by integrating out 9 2 and it was 
found that p(9 1 I 9z, y) was very sensitive to changes in 9 2 , it would be important to 
examine carefully the distribution p(92 I y), which summarizes the information about 9 2 
in the light of the data and prior knowledge. Two situations might occur: 
I. If p(9z l y) were sharp, with most of its probability mass concentrated over a small 
region about its mode 92 , then integrating out 9 2 would be nearly equivalent to 
assigning the modal value to 9 2 in the conditional distributionp(9 1 192 , y), so that 
( 1.6.4) 
Thus, even though inferences on 9 1 would have been sensitive to 9 2 over a wider 
range, the posterior distribution p(92 I y) would contain so much information about 
9 2 as essentially to rule out values of 9 2 not close to 92 , 
2. On the other hand, if p(92 I y) were rather flat, indicating there was little information 
about 9 2 from prior knowledge and from the sample, this sensitivity would warn .that 
t Personal communication. 

72 
Nature of Bayesian Inference 
1.7 
we should if possible obtain more information about 9 2 so that the inferences about 9 [ 
could be sharpened. If this were not possible, then as well as reporting the marginal 
distribution p(9J I y) obtained by integration, it would be wi;:e to add inforrration 
showing how p(9] 192, y) changed over the range in which p(92 I y) was appreciable. 
1.7 SYSTEMS OF INFERENCE 
It is not our intention here to study exhaustively and to compare the various 
systems of statistical inference which have from time to time been proposed . 
We assume familarity with the sampling theory approach to statistical inference, 
and, in particular, with significance tests, confidence intervals, and the Neyman-
Pearson theory of hypothesis testing. 
The main differences between sampling 
theory inference and Bay'esian inference are outlined below. 
In sampling theory we are concerned with making inferences about unknown 
parameters in terms of the sampling distributions of statistics, which are 
functions of the observations. 
1. The probabilities we calculate refer to the frequency with which different 
values of statistics (arising from sets of data other than those which have 
actually happened) could occur for some fixed but unknol\·n values of the 
parameters. The theory does not employ a prior distribution for the parameters, 
and the relevance of the probabilities generated in such manner to inferences 
about the parameters has been questioned (see, for example, Jeffreys, 1961). 
Furthermore, once we become involved in discussing the probabilities of sets of 
data which have not actually occurred, we have to decide which "reference set" 
of groups of data which have not actually occurred we are going to contemplate, 
and this can lead to further difficulties (see, for example, Barnard 1947). 
2. 
If we accept the relevance of sampling theory inference, then for finite samples 
we can claim to know all that the data have to tell us about the parameters 
only if the problem happens to be one for which all aspects of the data which 
provide information about the parameter values are expressible in terms of a 
convenient set of sufficient statistics. 
3. 
Usually when making inferences about a set of parameters of primary 
interest, we must also take account of nuisance parameters necessary to the 
specification of the problem . Except where suitable sufficient statistics exist, it 
is difficult to do this with sampling theory. 
4. 
Using sampling theory it is difficult to take account of constraints which 
occur in the specification of the parameter space. 
By contrast, in Bayesian analysis, inferences are based on probabilities 
associated with different values of parameters which could have given rise to the 
fixed set of data which has actually occurred. In calculating such probabilities 

1.7 
Systems of Inference 
73 
we must make assumptions about prior distributions, but we are not dependent 
upon the existence of sufficient statistics, and no difficulty occurs in taking account 
of parameter constrai nts. 
1.7.1 Fiducial Inference and Likelihood Inference 
Apart from the sampling and the Bayesian approaches, two other modes of 
inference, proposed originally by I-isher (1922, 1930, 1959), have also attracted 
the attention of statisticians. These are fiducial inference and likelihood inference. 
Both are in the spirit of Bayesian theory rather than sampling theory, in that 
they consider inferences that can be made about variable parameters given a 
set of data regarded as fixed. 
Indeed , fiducial inference has been described by 
Savage (196Ib) as "an attempt to make the Bayesian omelette without breaking 
the Bayesian eggs." 
It has been further developed by Fraser (1968), and 
Fraser and Haq (1969), using what they refer to as structural probability. 
Although this approach does not employ prior probability, Fisher made it 
clear that fiducial inference was intended to cover the situation where nothing 
was known about the parameters a priori, and the solutions which are accessible 
to this method closely parallel those obtained from Bayes theorem with non-
informative prior distributions. 
For example, one early application of fiducial 
inference was to the so-called Behrens- Fisher problem of comparing the means 
of two Normally distributed populations with unknown variances not assumed 
to be equal. The fiducial distribution of the difference between the population 
means is identical with the posterior distribution of the same quantity, first obtained 
by Jeffreys (1961), when noninformative prior distributions are taken for the 
means and variances. 
By contrast, Welch's sampling theory solution (1938 , 
1947), does not parallel this result. We discuss the Bayesian solution in Section 
2.5 in more detail. 
While Fisher's employment of maximum likelihood estimates was followed up 
with enthusiasm, by sampling theorists, few workers took account of his suggestion 
for considering not only the maximum but the whole likelihood function. Notable 
exceptions are to be found in Barnard (1949), Barnard, Jenkins and Winsten 
(1962) and Birnbaum (1962). 
Barnard has frequently stated his opinion that 
inferences ought to be drawn by studying the likelihood function. 
As we have 
seen earlier in Section 1.3, if the likelihood function can be plotted in an 
appropriate metric, it is identical with the posterior distribution using a non-
informative prior. 
However, the likelihood approach also suffers from 
fund amental difficulties. It is meaningless, for example, to integrate the likelihood 
in an attempt to obtain "marginal likelihoods." Yet if the whole likelihood 
function supplies information about the parameters jointly, one feels it should 
be able to say something about them individually. Thus, while fiducial inference 
and likelihood inference each can lead to an analysis similar to a Bayesian analysis 
with a noninformative prior, each is frustrated in its own particular way from 
possessing generality. 

74 
Nature of Bayesian Inference 
A 1.1 
APPENDIX AI.1 
COMBINA nON OF A NORMAL PRIOR AND A NORMAL LIKELIHOOD 
Suppose a priori a parameter 8 is distributed as 
p(O) = __ 
1_ 
exp [_ ~( 8 -
00 )2] 
2rro-o 
2 
0-0 
' 
-
0:., < 0 < 00, 
(AI.I.I) 
and the likelihood function of 8 is proportional to a Normal function 
[ 
1 (0 - 1;')2] 
1(01 y) ex exp 
- 2 
.~ . 
(A I. 1.2) 
where x is some function of the observations y. Then the posterior distribution 
of 0 given the data y is 
) 0 
= 
p(O)/(O I y) 
t< Iy) 
J:",p(O)/«(;I [y)dO 
/(0 1 Y) 
-
00 < 0 < 00, 
(AU.3) 
where 
f 
I [( 0 - 00 ) 1 (X - R) 2] ) 
lU) 1 y) = exp l - 2 
0-0 
+ 
-0--,-
J . 
(AI.IA) 
Using the identity 
A(z -
a)2 + B( z -
b)l = (A + B) (z -
e)2 + 
AB 
(a -
h)2
1(AI.1.5) 
A + B 
with 
J 
c = -
-- (Aa + Bbl. 
A + B 
we may write 
( 0 - f) 0 ) 2 
(X - 0) 2 
2 
2' 
2 
---;;-
+ 
-~ = (0-0' + o-~ ) (A -
I)) + d, 
where 
and d is a constant independent of (J. Thus. 
(A 1.1.6) 

AJ.t 
Appendix 
7S 
so that 
f '" 
( 
d )f'Y, 
-
. 
1(0 I y) dO = exp -
-
exp [ -
}(0-02 + o-~ 2) (0 -
fJ)2] dO 
-00 
2 
-
00 
(A 1.1.7) 
It follows that 
-
ex; < 0 < 00, 
(A 1.1.8) 
which is the Normal distribution 
NflJ, (0-0 2 + 0-~2)-1]. 

CHAPTER 2 
2.1 INTRODt:CTION 
STANDARD NORMAL THEORY 
INFERENCE PROBLEMS 
Variation in experimental observations may be associated with known or unknown 
caLises. Suppose observations are made many times under conditions in which 
suspected causes of variation are held constant. Then we can usefully think of the 
unknown causes generating a hypothetical distribution of observations which are 
said to differ because of "experimental error. " Furthermore, we can sometimes 
regard a particular observation y as a random drawing from this hypothetical 
underlying distribution p(y). Given a sample of observations, statistical inferences 
can then be made about the distribution from which the observations are supposed 
to have been drawn. 
Scientific investigation often focuses attention upon certain aspects of distri-
butions which are of special importance. In particular, attention may be focussed 
on the location and spread of distributions. 
Measures of Location 
The investigator often desires to know to what extent some treatment he has applied 
has caused a shift in the loea/ion of a distribution. He might, for example, want 
to discover by how much the yield of a particular product in a chemical re'action 
is changed by modifying the method of preparation. Suppose he makes n runs 
using a standard method A, and a further II runs using a modified method B, and 
he gets a scatter of yield values for method A and a second scatter for method B. 
He is concerned not with this particular set of 211 values per se, but rather with 
what they might allow him to infer about the overall superiority (or otherwise) of 
the modification. 
Thus, he might wish to draw inferences about the relative 
locations of the two underlying distributions from which the observations are 
presumed to be samples. 
Many different measures of location might be considered. Examples are the 
mean 
J.1 = E(y) = I~(/P(Y)dY, 
and the median m defined by 
f'oo P(Y) dy = f'p(y) dy. 
76 
(2.1.1) 
(2.1 .2) 
---~~------------------~ 

2.1 
Introduction 
77 
[n general, we shall say that a parameter f) is a measure of location if addition 
of a constant c to all the observations (changing y to y + c) changes f) to fJ i- c. 
Measures of Spread. Scale Parameters 
Another feature of distribution which is of special interest is its spread. Thus, a 
chemist who is developing a new analytical method may wish to compare the spreads 
of the distributions of results obtained by two different techniques. 
There are 
many different parameters that can be used to measure the spread of a distribution. 
for example, the variance (J2, the standard deviation (J, the precision constant 
(J' 2, and the mean devia tion ElF -
t;(Y)I. 
in this book, a parameter 8 (8 > 0) 
qualifies as a spread parameter if a linear recoding of the observations in which 
Y is changed to (a + by) changes the parameter 8 to IWO. This implies that the 
logarithm, log 8, is changed by a Axed constant. Further, if y and 0 are two spread 
parameters of a distribution then they are related by y = c8'.t In terms of the 
logarithm, log y and log 0 are linear functions of each other. 
A particular class of spread parameters are scale parameters. A spread para-
meter is a scale parameter if a linear recoding of the data changes e to Ibi8. Thus, 
a scale parameter has the same units of measurement as the observations. 
In 
particular (J , (J2, and log (J are all spread parameters, but among this group only 
(J is a scale parameter. 
2.1.1 The Normal Distribution 
Since location and spread are characteristics of a distribution which are of particu-
lar importance, and since statistical models should include the smallest number of 
parameters needed to realistically descri be the phenomenon under study, it is 
natural to seek, for representational purposes, some convenient two-parameter 
family of probability distributions with one parameter measuring location and 
one measuring spread. 
A distribution of this kind, which lends itself to ready 
manipulation and simple analysis, and has been widely used to represent the 
distributions of observational data, is the Normal or Gaussian distribution 
[ 
(y -
8)2 J 
p(y) = (2IT(J2)-.1 exp -
2(J2 
' 
-oo < y<oo 
(2.1.3) 
which we denote as :v(D, (J2) . 
For this distribution, the mean D, which is also the mode and the median, is 
a location parameter. The standard deviation (J is a spread parameter and, since it 
has the same units as y, is also a scale parameter. 
The Role of the Normal Distribution 
While the assumption of Normality can lead to greatly simplified analysis it would 
be cynical and unrealistic to argue that this was the only reason for its use. When 
t To see this, let y = lCO). Then for some (q, q'), IW'y = leD IW), that is, Iblq'/(D) = 
Ice IW); thus, we obtain the functional equation 'JeD) =/(12D), whose general solution 
is[CO) = eOa. 

78 
Standard Normal Theory Inference Problems 
2.\ 
extensive sets of data have been examined it has often been found that they were 
roughly ~ormally distributed either in the original metric or in some simple trans-
formation such as the logarithm. Furthermore, under "central limit conditions," 
theory suggests that data distributions should "cluster" about the 1\ormal and not 
about some other distribution. These central limit conditions arise when the errors 
in the observations behave like linear aggregates of independent component 
errors which are of comparable importance. 
Feor example, suppose that, from an experiment on a small scale reactor, an engineer 
obtained a certain number of grams of chemical product a sample of which was sent for 
chemical analysis. From the results, he might calculate a quantity y which measured the 
"yield" of a particular chemical entity of interest. This calculated yield, y, might typically 
be a function of z], the weight of crude material; of Z2, the weight of sample taken; of 
2J, 24, z5, various analytical results ; and of z6, Z7, z8' Z9, . .. , zP' experimental conditions 
such as temperature, pressure, concentration and flow rate. 
Thus, the final calculated yield might be wrilLen as 
(2. J.4) 
wherep would be moderately large, and the variables ZI' ... ,z" would all be subject to 
error. To study the effect of the errors, suppose Zi = 
~ i + ci (i = I, .. . , p), where the 
ci is a random variable distributed with zero mean and variance (J;. If the Cj'S could 
be assumed to be small compared to the corresponding (j's-we could, perhaps, 
expect that no z would be measured with more than say a 20 per cent error-
then, after 
expand ing / about the point ~ = 
(~]' ... , (p ) 
(2.1.5) 
where 
and 
W = O/! 
I 
OZ j
~' 
If the 8;'S were independent, the variance of y would now be approximately 
p 
Var (y) == I (wpi, 
(2. J .6) 
i"l 
so tha t the contribution of any particular source of error would be large or small depending 
upon whether (Wj(Jj)2 was large or small. Lsually some of the contributions would be 
negligible compared with the others but a number would be comparable. With suitable 
regularity conditions, it is possible to show that the distribution of y will approach the 
Normal as the number of these comparable contributions increases, and we shall call such 
a tendency a central limit effect and the conditions in which it operates central limit 
conditions. 
The above argument does not, of course, imply that all distributions met in practice 
can safely be assumed Normal. 
It does suggest that, in situations where there are a 
numher of sources of variation none of which dominates the others, distributions which are 

2.1 
lntroduction 
79 
roughly Normal will occur. We discuss in more detail questions of non-Normality in 
Chapter 3. 
2.1.2 Common Normal-theory Problems 
Statisticians have used Normal distributions to represent the variation of obser-
vational data in many different applications, in particular, they have considered 
the comparison of the means of two or more such distributions, both when the 
variances of the distributions are known and when they are unknown, and when 
these variances can, and cannot, be assumed equal. They have also considered 
problems of comparing variances of two or more Normal distributions. As an 
extension of the problem of comparing means, they have investigated inferences 
which can be made about the .parameters of the general linear model 0.3,68) , 
The associated analysis is often referred to as regression analysis or least squares 
analysis. 
Sampling Theory Approaches-
The Confidence Distribution 
The most common approach to these problems has used sampling theory, In this 
theory an appropriate criterion, which is a function of the observations, is first 
selected, For example, in the problem of comparing the means ()2 and OJ of two 
Normal distributions with known variances O"~ and O"~, the observed difference 
Y2 -.VI of the sample means is the criterion considered, 
lnferences about the 
possible values that flz -
01 might take, in the light of the data, are made by 
considering the distribution of Y2 -.VJ in repeated sampling from the same 
populations. 
Sampling theory inferences are usually made in terms of significance tests and 
corrfidence intervals. Although we assume that the reader is familiar with these 
ideas, it is useful to recall certain features of confidence intervals. With samples of 
111 and n2 observations from distributions A'(OI, O"t) and N(02' O"~), the criterion 
Yz - YJ is distributed as ,'V(()2 - 01,0'2), where 0' = (O'~ /J71 + O'~ /n2)J / 2 . The I -
CI. 
confidence interval for 82 -
01 may take the form (12 - 11) ± O'U,/2, where U,,/2 
is the unit Normal deviate which cuts off an upper tail area of rxj2 of the N(O, I) 
distribution. The meaning to be associated with the confidence interval is that, 
in repealed sampling from the same popUlations, the calculated intervals will 
cover the true location difference O2 -
8 1 , which is supposed fixed, a proportion 
I -
rx of the time. 
1 n practice, a variety of intervals could be calculated for 
different values of I-C/.. A device for simultaneously showing all such intervals 
is the confidence distribution. 
Suppose that having computed Y2 - YI from a particular set of data, we drew 
a \iormal distribution centered at 12 - YI with standard deviation 0". Then this 
diagram would immediately allow the intervals for every value of I -
a to be 
obtained. Thus, if in Fig. 2.1.1 any interval were marked off, then, the total area 
under the distribution curve enclosed within two verticals drawn at the ends of 
the interval , would supply the value of I -
CI.. It should be carefully noted that the 

80 
Standard Normal Theory Inference Probl.ems 
2.1 
Fig. 2.1.1 Confidence distributIOn for the difference of two Normal means 82 -
81, 
confidence distribution is simply a convenient device for associating the value of 
J -
Ci. with an interval. The contldence distribution is nol a probability distribution 
of 82 -
OJ; it will turn out in some common cases, however, that it is numerically 
identical to the posterior distribution obtained with a noninformative prior distri-
bution. 
Bayesian Approach 
Procedures for comparing Normal means and Normal variances are often given 
special names. Some of these are Student's " t" test, the Behrens- Fisher test, the 
analysis of variance, Normal theory least squares, and Bartlett's test for the 
comparison of variances. In this chapter, we discuss a number of these Normal 
theory problems from a Bayesian viewpoint. In most cases, the analysis wiJl be 
given on the basis of noninformative reference priors. The results will therefore 
be appropriate to situations in which either (a) little is known a priori about the 
parameters, or (b) it is desired to know what the inference would be if little were 
known a priori. 
Most of the results presented In this chapter have been given earlier by 
Jeffreys (J 961), Savage (1961 a), Lindley (1965) and others. This review, however, 
serves to bring the results together here as a starting point for the consideration in 
later chaplers of a wider class of problems. 
2.1.3 Distributional Assumptions 
Normal theory problems are those in which the data are treated as a sample of 11 
observations (YJ' ... ,y,,), wherey", l1 = J, ... , 11, is ~ormally distributed with mean 
/1" and variance a~. That is, 
() 
(2 
2)-1 /2 
[ 
1 (Ju - II" )2] 
P Y" = 
71f7" 
exp - "2 
a" 
, 
- cc < Y" < 00. 
(2.1.7) 
As before, we shall say YII is distributed as N(I1", a~) . The corresponding notation 
j\'q(TJ, E) is used to denote the q dimensional multivariate Normal distribution 
p(y) = [(2n)"jElr 1/2 exp l- i (y -
TJ)' E' 1 (y - 1)J, 
(2.1.8) 

2.1 
Introduction 
81 
where 
y' = (YI ' ,,, ,Yq) (-
CC <),,, < (0), 
and the quantities 
I = I, ""q, 
j = I, "" q, 
are respectively the q x 1 vector of means and the q x q covariance matrix, 
We make the important assumption throughout this chapter that the n obser-
vations y' = (YI' ""y") of the sample are independently distributed, Thus, the 
n-variate vector y is distributed as N"(rt, ~) with 
and 
(2,1.9) 
That observations are independently distributed is perhaps the most sensitive 
of the assumptions made in the theory that follows, Violation of this assumption 
can cause dramatic differences ;n the inferences which may be legitimately drawn 
from a set of observations, Box (1954), Zellner and Tiao (1964), 
In those 
situations where planned experiments can be independently conducted the inde-
pendence assumption will often be plausible; however, for data, such as economic 
data, over whose generation the investigator has no control, the independence 
assumption is frequently quite untenable and, if legitimate inferences are to be 
made, models which can take direct account of data dependence must be employed; 
see, for example, Box and Jenkins (1970), 
In most of the problems discussed in this chapter, it is also assumed that 
each observation has the same variance, so th'at ~ = 10'2 where 1 is an identity 
matrix of appropriate size, 
This assumption, which amounts to saying that 
observations have equal weight, is often a pla usible approximation ; however, 
situations do occur when it is not. For example, in some investigations it is the 
percentage error rather than the absolute error that one would expect to have 
constant variance, 
In such a case logarithms of the observations wuuld have 
constant variance, and an analysis in terms of the logarithms might be more 
appropriate, We take up this general problem of data transforma tion to achieve 
closer correspondence with assumptions in Chapter 10, 
When we refer to a set of random variables which are Normally and inde-
pendently distributed with the same variance, we shall sometimes say that they are 
spherically Normal. This terminology is used because the density contours of the 
distribution N,,(l], 10'2) in the space of the random variables are hyperspheres, It 
is convenient to use 1" to denote an 11 x J vector of ones. If, for example, n spheric-
aJly Normal observations have the same mean so that /]1 = /] 2 = ". = /]" = 0, 
then l] = 01,,, and the distribution is N"(Ol,,, 10'2). 

82 
Standard Normal Theory Inference Problems 
2.2 
2.2 INFERENCES CONCERNING A SINGLE MEAN FROM OBSERVATIONS 
ASSt:MING COMMON K"IOW~ VARIANCE 
Suppose that the n observations y' = (YI ' · .. ,Yn) can be regarded as a random 
sample from a Normal population N(8, 0- 2 ) with 0-2 known . Thus, it is assumed 
that the n observations are spherically Normally distributed as 
N II(81 n , 10-2). 
We consider what inferences can be made about the unknown mean 8. 
Given the assumptions, the sample mean, y = (J /n) L Yi, is a sufficient statistic 
for 8, and the sampling distribution of y is N(e, 0-2,11). As in (J .4.10), the likelihood 
function is therefore 
/(8 I y) <X p(y I 8, 0- 2) 
(2.2.1) 
<X exp [ - 2: 2 (8 _ y)2] , 
so that the posterior distribution for e is 
p(8 I y) <X p(8)/(81 y) 
x p(8)p(y I 8, ( 2 ) , 
where p(8) is the prior distribution. 
( 
(2.2.2) 
As in (1.3.3), a locally uniform distribution in 8 itself provides the appropriate 
noninformative reference prior, and inferences can be made by treating p(8) as 
constant in (2.2.2). Thus, approximately, 
p(8 I y) <X exp [ - 2:2 (8 _ y)2] . 
(2.2.3) 
The normalizing constant k which makes the right-hand side of (2.2.3) integrate to 
f"" 
[n 
] 
(2na2)1/2 
k-
1 = 
_ ro exp -2a2(8-y)2 d8= -n-
, 
one is 
so that finaJly 
( (J2)-1 f2 
[11 
] 
p(81 y) = 2n -;; 
exp 
-
2(J2 (8 -
y)l 
, 
-
00 < 8 < 00 . 
(2.2.4 ) 
We may summarize the above into the following : 
Theorem 2.2.1 let the sample quantity y be distributed as N(8, (J2 jn) in which 8 
is unknown and (1 2 is known. 1f the prior distribution of 8 is locally uniform, then 
given y, the posterior distribution of e is approximated by N(y, a 2/n). 
For illustrations consider the following example 
2.2.1 An Example 
Table 2.2.1 gives the observed breaking strength (in grams) for 20 samples of yarn 
taken randomly from spinning machines in a certain production area. Past ex-

2.2 
Inferences Concerning a Single Mean 
83 
perience indicates that observations of this kind are Normally distributed about 
a mean value e with standard deviation 
(J = (20)1/2 = 4.472 (grams). 
What 
conclusion can be drawn about 8? 
Table 2.2.1 
Observed breaking strength of 20 samples of yarn (in grams) 
46 
58 
40 
47 
47 
53 
43 
48 
50 
55 
49 
50 
52 
56 
49 
54 
5J 
50 
52 
50 
Y = 50 
On the basis of prior local indifference and Normality, and given the infor-
mation thatji = 50 and (J2 = 20, e will have the approximate posterior distribution 
N(50, I). 
The posterior density function of e is shown in Fig. 2.2.1. This distribution 
implies that for an investigator who knew relatively little about e a priori the 
probability a posteriori that, for example, e is greater than 52 is given to a close 
approximation by the tail area to the right of e = 52 grams. 
p(O i Y) 
46 
47 
48 
49 
50 
51 
S2 
53 
Fig. 2.2.1 Breaking strength data : posterior distribution oftl with (J assumed known and 
equal to 4.472. 

84 
Standard ~ormal Theory Inference Problems 
2.1. 
To obtain such probabilities we use a table of the probability integral of the 
Normal curve, such as Table I (at the end of the book). In this table u is used to 
indicate a unit .Yarma! deviate having the distribution :Y(O, I). The table gives 
values of 1Ia such that 
Pr {u > ua } = Pr {11 < -u"} = ex. 
In the present example, the quantity 
~ (O - y)/a is distributed as u so that we 
can evaluate Pr {e > Do} by finding an 'Y. in Table 1 such that Ua = .jn(8 0 -
y)/a 
In particular, if eo = 52, Ua = (52 - 50)/1 = 2, and C( ='= 2'3 %. 
As a further example, suppose the investigator requires the probability that 
48 < 0 < 51, corresponding to the shaded area in Fig. 2.2.1. The required prob-
ability is 
Pr {48 < D < 51 = 1 - Pr {8 > 51 } -
Fr {e < 48} 
{
51 -
50 } 
{ 
4 8 -
50 } 
1 - Pr 
11 > 
1 
- Pr 
U < -
- ,- -
I -
Pr {u > I} - Pr {11 > 2}. 
Using the table, this probability is approximately 82%. 
2.2.2 Bayesian Intervals 
All the information coming from the data on the basis of prior local indifference 
and the Normal assumption is contained in the posterior distribution sketched in 
~ig . 2.2.'. To convey to the experimenter what, on this basis, he is entitled to 
believe about the mean breaking strength e, it would be best to show him this 
sketch. With slightly increased convenience to the statistician and lessened conveni-
ence to the experimenter, the same information could be conveyed by telling him 
that the posterior distribution of D is .V(50, I) and by supplying him with a table 
of Normal probabilities. 
A.nother way to summarize partially the information 
contained in the poslerior distribution is to quote one or more intervals which 
contain stated amounts of probability+ Sometimes the problem itself will dictate 
certain limits which are of special interest. In the breaking strength example, 48 
and 51 grams might have been of interest because they were specified limits 
between which the true mean strength should lie. The 82 percent probability will 
then be the chance that the mean breaking strength lies within specification. 
A rather different situation occurs when there arc no limits of special interest 
but an interval is needed to show a range within which "most of the distribution 
lies." It seems sensible that such an interval should have the property that the 
density for every point inside the interval is greater than that for every point 
t Posterior intervals based on noninformative priors are called "credible intervals" by 
Fdwards, Lindman, and Savage (1963) , and "Bayesian confidence intervals" by Lindley 
(1965). 

2.2 
inferences Concerning a Single Mean 
8S 
outside the intaval. A second desirabk property is that for a given probability 
content the interval should be as short as possible. A moment's reAection will 
reveal that these two requirements are eqUivalent. 
For a posterior distribution 
with two tails like the Normal, such an interval is obtained by arranging its ex-
tremes to have equal density. 
Since for this type of interval \::very point included has higher probability 
density than every point excluded, we shall call it a highest posterior densitr 
interval, or an H.P. D. interval for short. 
Intervals derived from sampling theory are customarily associated with 90, 
95, or 99 percent of the total probability, with 95 percent the most popular choice. 
When several intervals are given, it is wise to choose them with their probability 
content differing enough so as to provide a fairly detailed picture of the posterior 
distribution . 
For example, 50, 75, 90, 95, and 99 percent Intervals might be 
quoted together with the mode. 
More fundamentally, it is desirable to quote 
intervals adequate to outline important features of the posterior distribution. 
Thus, while a single 95 percent interval might give sufficient information about 
the nature of a Normal posterior distribution, several intervals might be needed 
to indicate less familiar distributions. For example, several intervals in the upper 
range might be needed to show the nature of a distribution which had a very long 
tail. As the number of intervals qUClted increases, so we come closer to specifically 
defining the entire posterior distribution itself, and as we have said, presentation 
of the entire posterior distrihution is aJways desirable if this can be conveniently 
accomplished. For the breaking strength data, the statement that the mean 0 is 
Normally distributed about )' = 50 grams, with a 95 percent H.P.D. interval 
eXl\.:nding roughly from 48 to 52 grams, wuuld be sufficient for persons having 
knowledge of the Normal distribution. 
We shall return to the discussion of H.P.D. intervals and regions in Section 2.8 
where a more formal treatment is given. 
2.2.3 Parallel Results from Sampling Theory 
In making inferences about the Norma l mean 0 with (J assLimed known, Bayesian 
results, on the basis of a noninformative reference prior, parallel sampling theory 
resu lts, in the sense that the posterior distribution of e is numerically identical to 
the sampling theory confidence distribution. This is because the confidence inter-
vals are based upon the sufficient statistic S', and in both theories the quantity 
u = "n(O - n/(] 
is distributed as N(O, I). Thus in Fig. 2.2.1 , the interval (48 , 51) which contains 
82 percent of th.: posterior probability, is also an 02 percent confidence interval 
for the mean breaking load O. Also, for this problem, the (I -
(X) H.P.D. interval 
is numerically equivalent to the (I -:x) shortest confidence interval. 
Thus, 
for the breaking strength example, the interval (48,52) which is the 95 percent 
H.P.D. interval is also the 95 percent shortest confidence interval for O. 

86 
Standard Normal Theory Inference Problems 
2.3 
While Bayesian and confidence intervals for e are numerically identical, it 
IS important to remember that their interpretations are quite different under the 
two theories. [n the Bayesian formulation , e is a random variable and a (I - :l) 
Interval is one which on the basis of a given y, computed from the observed data, 
includes values of 0 whose probability mass is a proportion (1 -
x) of the total 
posterior probability. In sampling theory. however, e is an unknown but fixed 
constant and a (1 -
'X) confidence interval is a realization of a random interval, 
for example, y ~ «(JI·lil )u'12' which in repeated sampling from the same population 
will cover ea proportion 1 -
Y. of the time. 
2.3 INFERENCES CONCERNING THE SPREAD OF A l\IORMAL DISTRIBL'TIOl" 
FROM OBSERVATIONS HAVIl"G COMMOl\o KNOWN MEAi'\ 
Suppose that a random sample y' = (JI" 
· , JII) IS drawn from a Normal population 
N(e, a 2l, in which () is known, and we wish to make inferences about the spread of 
this distribution. 
The likelihood function of (52 is 
/«(52 I y) oc p(y I (52) X (5-- 11 exp [ -
2~2 ICy" - 0)2] . 
(2.31) 
The posterior distribution of (52 is then 
p((52 i Y) oc p«(52)/«(52 I y) 
ex: p((52)(5- 11 exp ( -
~~: ) , 
(2.3.2) 
where 52 = (l 'n) I (YII -
0)2 is the sample variance and p((52) is the prior distri-
bution. 
Alternatively, since l(a 2 1 y) involves only the sample quantity 52 which is 
sufficient for (J2, we can employ Lemma 1.4.1 (on page 62) to derive (2.3.2) from 
the distri bu tion of 52 . 
I t will be recalled that if y' = (y I, .. . , )'11) are spherically 
Normal with zero means and common variance (52, then the distribution of 52 is 
(a 2in) X,~ , where / 
is a chi-square variable with 1/ degrees of freedom, That is, 
p(52 ; (52) = [1( ~) r I ( ; f2 (J - II(S2/".' 2l- 1 exp ( - ;::), 
52 > O. 
(2.3.3) 
Since for given 52 the quantity [f(n/2)r 1 (nI2)",2 (5 2)<',/ 21'1 is a fixed constant, 
the posterior distribution of (52, given 52, is 
p((52/ 52) u: p(a2)p(52I (5 2) :c p(a1)(5 - " exp ( -
;:: ) 
(2.3.4) 
whil:h is identical with (2.3.2). 
Following our discussion in Section 1.3.2, we adopt as our noninformative 
reference prior a distribution locally uniform in log (5. 
This implies that 
(2.3,5) 

2.3 
Inferences Concerning the Spread of a "Iormal Distribution 
87 
Using (2.3.5) and applying the integral formula (A21.2) in Appendix A2.1 to find 
the appropriale normalizing constant, we obtain 
(2.3.6) 
where 
[ (11)]-1(1152)//,2 
k=1-
--. 
2 
2 
From (2.3.6) the posterior distribution of the standard deviation a is then 
a > 0, 
(2.3.7) 
where 
, _ [\ (11)] -\ ( 1152 )///2 
k -T -
--
2 
2 
. 2 
as we have seen earlier in (1.3.11). In addition, we can also deduce from (2.3.6) 
the posterior distribution of log a 2 = 210g a as 
~ exp (logl1 + log5 2 -
10ga l )] , 
-
00 < log a2 < ex. 
2.3.1 The Inverted I!, Inverted /, and the Log / Distributions 
(2.3.8) 
Before showing how this theory can be used , we consider some properties of the 
distributions ora 2 , a, and loga l . We first consider some sampling results. Using 
the assumptions and notation of the previous section , the sampling distribution of 
the quantity 
'/ ()' _ 0)2 
115 2 
!~ = I - "-2-
-
-
// .. \ 
a 
-7-
IS 
/ > O. 
(2.3.9) 
which IS the / 
distribution with 11 degrees of freedom given earlier in (1 .3.12). 

88 
Standard Normal Theory lnference Problems 
2.3 
Further, the sampling distribution of /." = ~n s!a (the positive square root of 
!.~) is 
p(!.,,) = [r( ; )2("m-1 r 
I/,_ I exp (-l/), 
/. > O. 
(2.3.10) 
This is known as the X distribution with 17 degrees of freed om. Finally, the sampling 
distribution of 
log !.I~ = (log 11 + logs2 - log ( 2) 
IS 
p (log XI~) = [r( ; )2"/2] - t (/),,2 exp (- 1/) 
-co < log/ < 00 . 
(2.3.11 ) 
In the corresponding Bayesian analysis, it is the reciprocals X,~ 2 and X,~ I which 
naturally appear. The "inverted" / and the "Inverted" X distributions having 17 
degrees of freedom are derived from (2.3.9) and (2.3.10) by making the trans-
formations 
..... 
to yield 
• 2 
1 
X" 
= -2 ' 
!." 
-
t 
1 
/." =-
Xn 
p(XI~2) = [r( ;)2"f2r
1 
U- Z) -[(,,/ 21 + ll exp ( -
2X~2)' 
and 
p(/.,~ I) = r r( ; )2(,,/21- 1 r \x- I) - ('" 
II exp ( -
2C!.~ I)l)' 
x- Z > 0, 
(2.3.12) 
[t > O. 
(2.3.13) 
Now comparing the posterior distributions in (2.3.6) and (2.3.7) with (2.3.12) and 
(2.3.13), we see that a posteriori, the quantities a2jl1s Z and (Ji n s are distributed 
respectively as x,~ 2 and x,~ t 
Further, from (2.3.8) and (2.3.11) the posterior 
distribution of (logl7 + logsZ - log( 2) is that of log X,7. 
In dealing with the 
posterior distributions of such a quantity as (Jz/I7Sz , it must be remembered that 
a2 is the random variable and S2 is a fixed quantity computed from the observed 
data. 
To summarize: 
Theorem 2.3.1 
Let the sample quantity S2 be distributed as «(J21V)X~. If the prior 
distribution of log a is locally uniform, then, given S2, (J2 is distributed a posteriori 

-
-
-~~- -
. -
~ 
2.3 
Inferences Concerning the Spread of a Normal Distribution 
89 
as (vs 2)xv- 2, (J is distributed as 
(,/ ~ s);;:; I, and finally log (J2 is distributed as 
log vs 2 -
log X~ (or log vs 2 + log X,~ 2). 
Tn what follows we shall always refer to the constant vas the number of degrees 
offreedom of the distribution. 
2.3.2 Inferences About the Spread of a :"1ormal Distribution 
The X- 2 , X-I, and log/ distributions provide posterior distributions for (J2, (J , 
and log (J2 To partially summarize the information contained in these distributions 
H.P.D. intervals may be quoted. For example, if we are specifically interested in 
making inferences about the variance (J2, then the end points ofa (I -
Ct) H.P.D. 
interval in (J2 are given by two values ((J6, (J~), such that 
a) p((J~ 1 y) = p((J~ 1 y), 
While the probability cuntained in a given interval of (J2 will, of course, be the same 
as that in the corresponding intervals of (J and log (J2, it is not true that limits of 
an H.P.D. interval in (J2 will correspond to the limits of H.P.D. intervals in (J and 
log (J2 
This is easily seen by noting that the posterior distributions p((J2 1 y), 
p((J 1 y), and p(log 0'21 y) in (2.3.6) (2.3.8) are not proportional to one another. 
Consequently, the pair of values ((J6, (J~) which satisfy 
will not make 
[n other words, H.P.D. intervals are not invariant under transformation of the 
parameter unless the transformation is linear. This raises the question of para-
meterization in quoting H.P. D. intervals. 
Standardized H.P.D. Intervals 
Inferential statements about the spread of a Normal distribution could be made 
in terms of the variance (J2, the standard deviation (J , or the precision constant (J " 2 
The H.P.D. intervals associated with a given probability would be slightly different 
depending on which of these metrics was used, although each interval would exactly 
include the stated probability. Tn this book when a noninformative reference prior 
is assumed for any parameter, we shall for definiteness present standardized H.P. D. 
intervals. These will be H.P.D. intervals calculated in that metric in which the 
noninformative prior is locally uniform . Thus, for the spread of the Normal 
distribution, we shall quote intervals which are H.P.D. intervals for log(J. Since 

90 
Standard Normal Theory Inference Problems 
2.3 
H.P.D. intervals are equivalent under linear transformation of log cr, we may 
employ intervals for any member of the class of transformations 
c + q log 0' 
(2.3.14) 
where c and q are two arbitrary constants. In particular, we may work with 
ns 2 
log X~ = Jog -
2-
= log ns 2 -
2log cr. 
cr 
(2.3.15) 
Limits of H.P.D. intervals for log X~ may be obtained from Table II (at the 
end of this book). To avoid calculation of logarithms, the limits are given in 
terms of X; = ns 2 /cr2 
Specifically, for various combinations of (Ct, n), Table II 
provides the lower limit ten, ex) and the upper limit 72(11, a) for which 
a) 
(2.3.16) 
that is, 
and 
b) 
Pr{t < '/ < t} = 1 -
CI.. 
(2.3.17) 
The use of the table is illustrated by the following example. 
2.3.3 An Example 
Practical situations where e is known but (t2 is unknown are extremely rare, but 
for illustration we consider again the breaking strength data in Table (2.2.1). 
Suppose it were knOl l 11 that 0 = 50 and it was desired to consider the inferential 
situation concerning a 2 in the light of the noninformative reference prior distri-
bution p(log ( 2 ) IX constant. Then, log a 2 would be distributed a posteriori as 
log ns 2 -
log X,;. For this exam pIe, 
11 = 20, 
log 11 = log 20 = 2.9957, 
log S2 = 2.8565, 
Jog ns2 = 5.8522. 
348 
20 
17.4, 

2.3 
Inferences Concerning the Spread of a Normal Distribution 
91 
The distribution of log (J2 = 5.8522 -
log X ~o is shown in Fig. 2.3.1. [t is almost 
symmetrical about log s2. Also shown in the same figure is the appropriate 95 
percent H.P.D. interval. The abscissas of the distribution in the figure are labelled 
in terms of (J2 , but plotted on a logarithmic scale to emphasize that it is for this 
scaling that the ordinates at the extremes of the H.P.D. interval are equal. The 
limits of the 95 percent interval are obtained from Table 11 as follows: 
Corresponding to CI. = 0.05 and n = 20, we find 
t = 9.9579, 
x2 = 35.227. 
Thus, the lower and upper limits are 
and 
t 
;;:, 
N 
C 
00 
.£ c: 
1.4 
1.2 
1.0 
08 
0.6 
0.4 
0.2 
0.0 
2 
ns 2 
348 
(J o = - - = -- = 9.8788 
t 
35.227 
2 
ns 2 
348 
(J, = -
2- = --- = 34.9471. 
~ 
9.9579 
0 2 --> 
Fig. 2.3.1 Breaking strength data: posterior distribution of log (J2 (scaled in (J2). 

92 
Standard l\ormal Theory Inference Problems 
2.4 
2.3.4 H.elationship to Sampling Theor)' Results 
As was the case for the location parameter, parallel results in Bayesian and sampling 
theories exist for making inferences about the spread of the Normal distribution. 
The posterior distributions of (f2, (J, and log (J in (2.3.6)-(2.3.8) are numerically 
identical to the corresponding confidence distributions. This is because the con-
fidence intervals are based upon the sufficient statistic 52, and in both theories 
the quantity n52l(J2 is distributed as X~. Shortest confidence intervals, like H.P.D. 
intervals, are not invariant under nonlinear transformation. 
For example, the 
(l - a) shortest interval for (J does not correspond to the (I -
::I.) shortest confi-
dence interval for log (J and one faces the same problem of deciding on the choice 
of parameterization. 
2.4 INFERENCES WHEN BOTH MEAN AND STANDARD DEVIATION 
ARE UNKNOWN 
Suppose a random sample of 11 independent observations is drawn from a Normal 
population N(e, (J2) but both 0 and (J are unknown. As we have seen in Section 
L.4, the sample mean y and the sample variance 52 = V I L (YII - y/, v = n -
I, 
are jointly sufficient for (e, (J), and are distributed independently as NCe, (J2/n) and 
«(J~ f v)x~ respectively. From (1.4.l3) the likelihood function is 
(2.4.1 ) 
where p(y Ie, (J2) and p(52 : (J2) are given in (1.4.9) and (1.4.12), respectively. Thus, 
given the data y, the joint posterior distribution of (e, (J) is 
(2.4.2) 
where pCB, (J) is the prior distribution. 
Following the discussion in Section 1.3.6. kading to (1.3.84), we assume that 
a priori e and (J are approximately independent, so that 
pee, (J) == pCB) p«(J), 
(2.4.3) 
and we adopt as our noninformative reference priors for e and (J 
pCB) c<. c 
(2.4.4) 
p(log (J) ex c 
or 
(2.4.5) 

2.4 
Inferences When Both Mean and Standard Deviation are Unknown 
93 
From (2.4.1), (2.4.4) and (2.4.5), the correspo~ding posterior distribution is 
-
J) < a < 00 , 
cr > 0, 
(2.4.6) 
where v = n -
j and k IS the appropriate Normalizing constant. Using the in-
tegral formulae (A2.1.4) and (A2.1.9) in Appendix A2.1, we find 
_ 1---;; [ ( V )1-) (VS2) ,,'2 
k -
\/~ 1r 2 J 
2 
. 
(2.4.7) 
Contours of pee, (J I y) 
On the basis of locally uniform prior distributions for ° 
and log cr, inferences about 
(0, cr) for a particular sample y should be based upon the posterior distribution in 
(2.4.6). Since prO, cr I y) is a function of two variables, a plot of the entire distri-
bution would involve constructing a three-dimensional figure ; however, a sufficient 
understanding of the nature of the distribution is obtained by plotting probability 
density contours in the (8, cr) plane. Each contour is a curve in the (0, cr) plane, 
prO, cr I y) = c, 
(2.4.8) 
where c> 0 is a suitable constant. 
On taking logarithms in (2.4.8), a density 
contour is defined by 
I 
-
(n T I)logcr - -2 rl'S2 -'-- n(8 -
ji)2] = d, 
2cr 
(2.4. 9) 
where d is a function of c. By differentiating the left-hand side of (2.4.9\ it is easily 
shown that the mode of p(O, cr I y) is 
(2.4. j 0) 
Thus, -
'XJ < d < do where 
do = -
(n -r I)logo- -
2~2 [vs 2 + n(e -
ji)2] 
= -
(n -
I) (logO- + 1). 
(2.4.11 ) 
Three or four suitably chosen contours would usually allow the investigator to 
have a good apprc:ciation of the main features of the distribution. 

94 
Standard ,,<ormal Theory Inference Problems 
2.4 
Probability Con/el1f of a Contour 
For a given contour, it will certainly be useful to know the posterior probability 
content of its interior region . The required probability is the double integral 
where the region 
I 
R: -
(11 + I) logO' - -2 [vs2 + 11(8 - ji)2] < d, 
20' 
(2.4.12) 
which, in principle, can be evaluated by numerical methods. As an approximation, 
we may use the fact that, for Jarge samples the joint distribution tends to Normality, 
[see JeJ1reys (1961)] Therefore, 
p(e, aIY) 
2 
-
210g 
"'-' 'I 
(e
A 
A I ,) 
1. 2' 
p ,a,} 
(2.4.13) 
where the symbol" ",-," means "approximately distributed as". 
It follows that 
the contour defined by 
logp(e, a I y) = log pee, fr I y) -il 2 (2, ,1.), 
(2.4.14) 
where /(2, Ci) is the upper JOO:x per cent point of a / 
distribution with 2 degrees 
of freedom, encloses a region whose probability content is approximately (I - a). 
That is, it contains approximately a proportion (I -
:x) of the posterior proba bility. 
Equivalently, the contour is given by 
I 
-
(11 + I) log 0' - -2 [vs1 + n(8 - ji/] = do - 1/ (2, a). 
20' 
(2.4.15) 
I n practice, a good idea of the distri bution can be obtai ned by plotting the approxi-
mate 50, 75, and 95 percent contours together with the mode. 
2.4.1 An Example 
Using the breaking strength data of Table 2.2.1, suppose now that both 0' and e 
are unknown. The necessary sample quantities are y = 50, vs 2 = L (y" _ ji)2 = 
348 , v = 19 so that 52 = 18.32 and 5 = 4.29, whence the joint posterior distribution 
for a and e, on the basis of the noninformative reference prior distributions (2.4.4) 
and (2.4.5), is 
pet), a I y) = ka- 21 exp [ -
2~2 [348 + 20(8 - 50?]} . 

2.4 
Inrerences When Both Mean and Standard Deviation are Unknown 
95 
From (2.4.10), the mode is 
e = y = 50, 
(J
' = (3
2
4
1
8) 1.'2 = 4.07, 
so that, accordi ng to (2.4.11), 
do = -
21 (log4.07 + 0.5) = -
39.98. 
The contour which contains approximately a proportion (I - a) of the posterior 
probability is therefore given by 
where 
1 
(-21) log (J -
-2 [348 + 20(0 -
50)2] = d, 
2(J 
d = -
39.98 -
0.5/(2, a). 
Values of x2(n, a), for various values of 11 and !Y., are given in Table [II (at the end 
of this book). For the 50, 75, and 96 percent contours, we find 
/(2,0.50) = 1.39, 
/(2,0.25) = 2.77, 
and 
/(2,0.05) = 5.99, 
whence the corresponding values for -d are (40.68, 4J.37, 42.98), respectively. 
These three contours, together with the mode, are shown in Fig. 2.4. J. 
2.4.2 Component Distributions of p(G, (J ! y) 
The joint posterior distribution of (G, (J) in (2.4.6) can be written as the product 
p(G, (J I y) = p(O I (J, y)p((J I y), 
(2.4.16) 
where the first factor is the conditional posll!rior distribution of G, given (J, and the 
second factor is the marginal posterior distribution of (J. 
The Conditional Distrihution of 0, Given (J 
Treating (J as a known constant in (2.4.6), the conditional posterior distribution 
of G, given (J, is 
p(G! (J, y) = J,/~ exp [- --; (G - y/] , 
2n: (J 
2(J 
-:x < G < 00. 
(2.4.17) 
That is, given (J, G is distributed a posteriori as :V(ji, (J2 ;11). 

..... 
~,........----------- -
-
-
-
96 
Standard Normal Theory Inference Problems 
t a 
4 
48 
e .... 
I 
I 
I 
I 
I 
I 
y 
50 
52 
Fig. 2.4.] Breaking strength data : contours of p(e, (J I y). 
The MarKina! Distribution of (J 
The marginal posterior distribution of (J is 
p((J I y) = pee, (J I y) . 
pee I cr, y) 
From (2.4.6) and (2.4.17), we fin d that 
with 
' _[1 (V)]-I(vs2)V/2 
k -
ir -
-. 
2 
2 
(J > 0, 
2.4 
(2.4.18) 
(2.4.19) 
Thus, (J is distributed a posteriori as ,,'-;; SX; I. This distribution is of exactly the 
same form as that in (2.3.7). The only difference is that whereas with 8 known 
the distribution of (J/(.)l7s), with /152 = I( y" -
8)2, had n degrees of freedom; 
with e unknown the distribution of (J/(.,/ vs), with vs2 = I(y" -
y)2, has v = 17 -
I 

2.4 
Inferences When Both Mean and Standard Deviation are Unknown 
97 
degrees of freedom . Inferences about (J, with 8 unknown, can thus be made follow-
ing the analysis in Section 2.3. 
The,Marginal Poslerior Dislribulion 0/0 
The marginal posterior distribution of 8 can be obtained by integrating out (J 
from the joint posterior distribution of e and (J, 
p(G I y) = L' p(8, (J I y)d(J 
(2.4.20) 
= LN> pC8 I (J, y)p«(J I y)d(J. 
Making use of (A2.1.4) in Appendix A2.1 , we find from (2.4.6) that 
o 
= 
(s/'/;;)-[ [ 
11(8 -
ji)2 ] -·; (d I) 
p( I y) 
Be 1)
-
J + 
2 
' 
-tV, 'rvv 
vs 
-
<X < () < 00, 
(2.4.21) 
where B(p,q) is the complete beta function B(p,q) = f(p)r(q) /f(p + q). 
Setting 
we then have 
8-ji 
1=--._-
S/V 11 
( O-y j ) 
1 
( 
t
2 )- H""' 11 
pl=--_-y = 
1+-, 
s /~n 
B(1v, 'r),/v 
v 
-
00 < I < 00, 
(2.4.22) 
which will be recognized as Student's I distribution with (v = n -
I) degrees of 
freedom. It should be remembered that 0 is the random variable in this expression 
and (y,S2) are known sample quantities. As was first shown by Gossett (190H) 
(2.4.22) provides the sampling distribution of (8 -
ji)/ (S/.Jn), in which 0 is 
regarded as fixed and ji and s are random variables. 
The di stribution of 0 in 
(2.4.21) will be denoted by l(y, S2 ,'I1 , v). 
From the above discussions, we have the following general results. 
Theorem 2.4.1 Let the sample l\uantities ji and S2 be independently distributed as 
NC8, (J2 / n) and «(J2 J V)x~, respectively. Suppose a priori that 8 and log (J are approxi-
mately independent and locally uniform. Then, given (ji, S2), (a) (J is distributed 
as -.,/VSX; [, (b) conditional on (J, e is distributed as N(y, (J2:n), and (c) uncondition-
ally, 8 is distributed as I(y, S2 /11, 1'). 
2A.3 Posterior Intervals for 8 
The posterior distribution of 8 in (2.4.21) is a symmetric distribution centered 
at ji with scaling factor s/'/;;' To obtain H.P.D. intervals for 8 we use the fact that 
the teO, 1, v) distribution of I = (0 -
ji) / (s J..j ~) has been tabulated. 

98 
Standard Normal Theory Inference Problems 
2.4 
If we denote '( 0 2)(V) as the value for which 
Pr {I > ' (.!2)(V)} = Cl;2, 
then from symmetry of the t distribution , Pr {I < -1(2f2)(V)} = (1./2 so that 
Pr {It I < '(af2)(v)} = I -
a. It follows that the limits of the (I -
a) H.P.D. inter-
vals of tev, S2 / 11 , v) are given by 
(2.4.23) 
Table IV, at the end of this book, gives values of '(o/2)(V) for various values of (I. 
and v. The use of the table is illustrated by the following example. 
An Example 
Consider again the yarn breaking strength data of Table 2.2.1. Suppose little is 
known about 0 and (J a priori, and we wish to find a 95 percent H.P. D. interval 
for 0 on this basis, then , y = 50, sf.Jii = 0.96, v = 19, and from Table lV, 
/002 5( 19) = 2.093, so that the 95 per cent limits are 50 ± 0.96(2.093) or (47.99, 
52.01 ). 
2.4.4 Geometric Interpretation of the Deriyation of pCG I y) 
The implications of the integration in expression (2.4.20) are illustrated in Fig. 
2.4.2. The conditional distribution p(GI(J, y) of 8 for any fixed (J is a Normal 
distribution N(y, (J2,11). 
The joint distribution pee, O'ly) can be regarded as an 
aggregation of such conditional Normal distributions weighted by the marginal 
distribution p(O'ly). 
When (J is unknown , p(Oly) is obtained by averaging these 
conditional Normal distributions p(OI(J, y) using as a weight function the marginal 
distribution of a . The resulting f distribution for 0 has the characteristics we would 
expect of a distribution obtained this way. In particular, for large v, the distri-
bution of 0 will be very nearly Normal because the conditional Normal distri-
butions p(OI(J, y) will be averaged only over a narrow "weight" distributionp(O'ly), 
with (J close to s. But, for small v, the distribution of e will be leptokurtic with 
much heavier tails because the conditional Normal distributions p(OI(J, y) will 
be averaged over a more disperse distribution p(O'ly), in which values of a wLdely 
different from s are given considerable weight. 
Figure 2.4.2 is drawn using the breaking strength data for which y = 50, 
s = 4.29 and 11 = 20. For each givel1 (J, 8 has a Normal distribution with mean 
50 and standard deviation al 20. Also (J is marginally distributed as 
T9(4.29)x~91. 
Finally, e has the t distribution with 19 degrees of freedom, centered at the value 
y = 50 and scaled by the quantity s/,/ 20 == 0.96. 
The three conditional distri-
butions of e which are shown are for the arbitrarily chosen values (J = 5.6, 4.47 
and 3.2. 

-
-~ -------
-
2.4 
t a 
6 
5 
4 
3 
Inferences When Both \lean and Standard Deviation are Unknown 
I 
I 
LL<:' 
p(lIia = 4.47. y) 
p(Ola=3.2,y) 
48 
50 
52 
Fig. 2.4.2 Breaking strength data: component distributions of pee, (J I y). 
99 
2.4.5 Informative Prior Distribution of (J 
We discuss in this section how prior knowledge or ignorance about (J affects inferences 
about B. When a priori e is locally uniform and independent of (J, (2.4.2) and (2.4.3) imply 
that the joint distribution of (e, (J) can be expressed in the general form 
where 
and 
p(8, (J I y) = p(BI (J, y) p«(J I y) 
p(BI (J, y) oc p(y , e, (J2) , 
p«J I y) oc p«(J) p(S2 I (J2). 
(2.4.24) 
(2.4.25) 
(2.4.26) 

100 
Standard Normal Theory Inference Problems 
2.4 
Thus the conditional posterior distributi0n of 0 is 
-
CO < () < CO . 
(2.4.27) 
Also , thc marginal posterior distribution of e, obtained by integrating out a from 
(2.4.24), is 
pee i y) = for pee I a, y) p(Cl I y) da. 
(2.4.28) 
This integration makes it possible to see hov. various states of knowledge about a are 
taken account of in the Bayesian analysis. First, consider the situation where Cl is supposed 
exactly known . .ror definiteness suppose, as we did earlier, that Cl was known to be equal 
to 4.47. This would correspond to the suppositi on that p«(J) was a spike with all its 
probability concentrated at the point a = 4.47 and with zero density elsewhere. The 
posterior distribution p(a I y) is proportional [Q pea) p(s2 I ( 2) and so would also have all 
its probability concentrated at Cl = 4.47. From (2.4.28), the posterior distribution of e 
would then degenerate to the conditional Normal distribution with (J = 4.47, with all 
other conditional Normal distributions having zero weight. 
This may be contrasted with the situation already considered where very little is 
known about a a priori, so that essentially all the information about (J has to come from 
the sample itself. This information is represented by the marginal posterior distribution 
of (J in (2.4.19) and supplies appropriate weights for the conditional Normal distributions. 
Integration over these weights gives the marginal { distribution for e as has already been 
illustrated in Fig. 2.4.2. 
We have disclIssed two extreme cases : in the first a great deal was known about (J 
and in the second little was known about (J. Sometimes (J i~ not precisely known but there 
may be preliminary information which it is desired to incorporate. Such knowledge may 
be expressed by employing an appropriate informative prior distribution for Cl in (2.4.26). 
For example, suppose a preliminary sample is available from which an estimate Sl 
having VI degrees of freedom is obtained. Then, on the assumption that initially log (I is 
locally uniform, the posterior distri bution of (J/ 
; ISI' given {he preliminary sample, is that 
of X:, J . This posterior distribution now becomes the appropriate prior distribution for the 
new sample and we have 
(J > 0, 
(2.4.29) 
with 

2.4 
Inferences When Both Mean and Standard Deviation are Unknown 
101 
Combining this distribution with p(s21 (J2) in (2.4.26) and applying the integral formula 
(A2.1.4) in Appendix A2.I , we obtain the posterior distribution of (J, 
( 
, '2 ) 
p«(J I y) =k'(J - (V'+ " exp 
-
~s2 ' 
(J > 0, 
(2.430) 
, 
(J 
where 
= [1T ( ~') ] t c'~'2r ' 2, 
k' 
\' = \' + v, 
and 
' ,2 
v s 
= v,s~ + vs 2 . 
That is, (J distributed as ,N s'I.:: '. 
Substituting (2.4.30) into (2.428), we get 
< 0 < 00, 
(2.4.31 ) 
so that I = (0 - y)/(s'/vin) has the distribution 
-OC<I<CX:, 
(2.4.32) 
which is a I distribution with v' = v + \', degrees of freedom. That is, E! is distributed 
a paSleriari as I(Y , 5,2/ 11, v'). 
The effect on the posterior distribution of using (2.4.29) 
instead of (2.4.5) to repn:sent p«(J) is to change the degrees of freedom from v to 
v' = v + v, and the scaling factor from slJ~ to s'!.J;, with V'S'2 = vJsi + vs2 and 
v' = v, + v. 
Often, even though some previous informatiori is available about (J, its relevance to 
the particular experiment might be questionabJe and one might prefer to consider, either 
as an alternative or in addition, what inferences were possible from the new sample alone 
in the light of a noninformative reference prior for (J, 
2.4.6 Effect of Changing the Metric of (J for Locally l :niform Prior 
It was argued in Section 1.3 that when we wished to analyze data as if little were known 
about (J a priori, we should take log(J as locally uniform. By further adopting a locally 
uniform and independent prior distributiun for 0, the information supplied by spherically 
Normally distributed observations y' = (y, • ... , YII ) then results in a I(Y, s2,'n, 1/ -
I) 
posterior distribution for 0, 10 study the effect of changing the metric of (J, suppose it is 
assumed either that log (J or that some power of (J is locally uniform. Equivalently 
if (Jq assumed locally uniform, 
(2.4.33 ) 
if log a assumed loca lly uniform (q = 0). 

102 
Standard Normal Theory Inference Problems 
2.4 
From (2.4.26) the posterior distribution of 0' is then 
(2.4.34) 
0' > O. 
The appropriate normalizing constant can be obtained by using (A2.1.4) in Appendix 
A2.1. Substituting (2.4.34) into (2.4.28), we obtain 
(! I 
,, ·_q/v 11 
f 
(s 
i ; - )-1 
[ 
liCe _ \i)2] - -l:(v - q + 1 i 
p( 
y) = 
[ 
I] 
I + 
2 
' 
8·i(I' -
q),'2 .JI' - q 
(I' -
q) S,_q 
-00 < 8 < 00, 
(2.4.35) 
where I' = n -
I, and 
S~. _q = L (Yi -
y)2 (v -
q). 
Thus, if (Jq were assumed locally uniform a priori, the posterior distribution of e would 
be 1(jI, S;_q/l1, v -
q). 
This derivation serves to emphasize the comparative insensitivity of the posterior 
distribution of 0 to the choice of prior on 0', at least for moderate sized samples. A change 
of q, even by one unit, can produce a major change in the prior distribution of (J. Yet such 
a change has only a minor eflect on pee I y) provided 17 is nOI too small. 
2.4.7 Elimination of the Nuisance Parameter 0' in Bayesian and Sampling Theories 
We have seen that with appropriate assumptions and interpretations, the quantity 
y-O 
sf.Jn 
(2.4. 36) 
has the 1(0, 1, 11 -
J) distribution, both from a sampling theory viewpoint and from 
a Bayesian viewpoint. The corresponding confidence distribution and the posterior 
distribution for e are therefore numerically identical. While both the Bayesian 
and the sampling results are independent of the unknown nuisance parameter (J, 
it is important to distinguish the manner in which (J is eliminated under the two 
theories. In the Bayesian framework 0' is eliminated by integration. The condition-
al posterior distributions of e for various values of (J are averaged over a weight 
function which is the marginal posterior distribution of 0'. This method of elimi-
nating (J has the advantage that it ma) be used for the elimination of any set of 
nuisance parameters. 
However, using sampling theory, the confidence distri-
bution can be obtained only because it so happens the sampling distribution of 
the "pivotal" quantity n(y - 8)/s is independent of 0'. Unhappily, this sampling 
theory procedure cannot be generally applied because quantities which are func-
tions of the observations and the parameters of interest, but whose sampling distri-
butions do not involve the nuisance parameters, do not usuaJly exist. 

2.5 
Inferences Concerning the Difference Between Two Means 
103 
2.5 INFERENCES CONCERNING THE DIFFERENCE BETWEEi\ TWO MEANS 
An investigator is often interested in the difference between mean va lues. Suppose 
for instance it was decided to investigate a possible mean difference in the breaking 
strength of yarn made by standard and modified spinning machines, and that 
data were available from a random sample of 111 standard machines and from a 
corresponding sample of 112 modified machines. 
To produce a mathematical formulation of the situation it may be supposed 
that the 111 observations Cyt l' ... , YIII') are as ifdrawn independently from a Normal 
population NC01' O'~) and the 112 observations (hi, "',Y211,) are as if drawn inde-
pendently from anotiler Normal population .'V(8 2 , O'~) with all four parameters 
fI" 
82 , O'~ , and O'~ un known. Consider the problem of obtai ning the posterior 
distribution of,' = C02 - 0,). 
2.5.1 Distribution of O2 -
8 1 when 0', = 0'2 
We first discuss the situation where the variances O'~ and O'~ , although unknown, 
can be assumed equal, so that O'T = 
O'~ = 0'2 The quantities ()II' Y2, 52), where 
and 
are then jointly sufficient for (fl l , O2 , (J2), and are independently distributed as 
N(OI ,0'2/11 ,), N(02, 0'2/112), and (0'2 / V)X~ , respectively. 
Assuming that a priori 
01, O2 and log 0' are approximately independent and locally uniformly distributed, 
it can be readily verifled by using Theorems 2.2.1 (on page 82) and 2.3.1 (on page 88) 
that the joint posterior distribution of 8 1, 82 and (J2 is 
(2.5.1 ) 
where 0'2 has the (1'5 2)Xv 2 distribution and, for given 0'2, the parameters 0 1 and 82 
are independent, having Normal distributions 'Y(YI ' 0'2(11,) and N(Y2,0'2/n2), 
respectively. Thus, the joint distribution of y = 82 -
81 and 0'2 IS 
(2.5.2) 
where, for given 0'2, I' has the Normal distribution N[Jiz -
)II ' 0'2(I /n, + IjnJ]. 
Applying formula (A2.1.2) in Appendix A2.1 to integrate out 0'2, we deduce the 
posterior distribution of y as 
- co < YI -
Y2 < co. 
(2.5.3) 

Standard Normal Theory Inference Problems 
2.S 
Thus Y is distributed as t[h - .v1,s2(i /Il] + 1/112),11[ -:- 1/ 2 -
2J, or, equivalently, 
y -
(Si2- .v [) 
1 = S(I !171 + 1/llz) 112 
(2.5.4) 
has the teO, I, n I + 1/ 2 -
2) distribution. Again, this exactly parallels the corres-
ponding sampling theory result, and a confidence distribution for y exactly matches 
the posterior distri bution. 
As before, inferences about the difference,' can best be made by studying the 
complete posterior distribution in (2.5.3), but H.P.D. intervals having any desired 
probability content may be constructed using Table III (at the end of this book), 
and will partially represent the information contained in the distribution. 
2.5.2 Distribution of O2 -
0 [ when ai and a~ are not Assumed Equat 
1n some experimental situations, the assumption that ai = 
(J~ is inappropriate. 
For instance, in the textile example quoted above, the two kinds of spinning 
machines might differ substantially not only in their means but in their variances 
as well. Suppose we assume that the two sets of observations are random sa mples 
independently drawn from the Normal populations'V(OI ' (Jf) and N(Oz , a~). 
Then the quantities (YI, s~) are jointly sufficient for (0 1, a~) having independent 
sampling distributions N(OI ' a~ , I)I) and e(J~ · v l)/.~ " respectively. Similarly, CY2,si) 
are jointly sufficient for (0 2 , (J~) ha vi ng independent sampling distributions 
Ne02, ai,i7 2 ) and (ai/r2)X~" 
Assuming for our noninformative reference prior that 8[, O2 , log a 1 and log (Jz 
are approximately independent and locally uniform, it can be verified by using 
Theorem 2.4.! (on page 97) that the joint posterior distribution of 8 1 and O2 is 
(2.5.5) 
that is, a posteriori 0 1 ami O2 are independent, and distributed as I(YI, s~lnl' 1'1) 
and t(jl2, S~/1l2 ' v2), respectively. 
The posterior distribution of '/ = 8z - 0 1 can be obtained, for example, by 
making the transformation 
(2.5.6) 
and integrating out YI from the joint distribution of (YI) y). 

2.5 
Inferences Concerning the Difference Between Two Means 
105 
The distribution of y = 82 -
8 1 cannot be expressed in terms of simple 
functions but may be computed by numerical integration to any desired accuracy. 
For the purpose of tabulation, following Fisher (1935), it is convenient to define 
the quantity 
and write 
1 = 
y -
(Y2 - YI) 
(S~ / 111 + s~ /n2)1/2 
(2.5.7) 
(2.5.8) 
where II and 12 are independently distributed as 1(0, I, Vj) and 1(0,1,1'2)' respect-
ively. 
The posterior cistribution of (J2 -
8 1 can then be determined from the 
distribution of 1. The density function of 1 may now be obtained from the joint 
distribution of 11 and 12 by performing the integration 
p(1 I VI' 1'2 ,1» = 
JJ 
p( /j )p(/2) dl l d1 2, 
, 2 COS ¢ -
II si ncb.:: r 
-
00 < 1 < 00, 
(2.5.9) 
where 
( 
12)- t {U'+ I) 
p(I;) = -----=-
1 + --.:.. 
B(-L 1-vJJ v; 
V, 
i = 1,2. 
This integration process can be viewed geometrically in Fig. 2.5.1. In the figure, 
the joint distribution of II and 12, which is the product p(/])p(/2)' is illustrated by 
the three contours. Also shown is the desired distribution of 1. For a given value 
of" say ,.= '0, the equation 
(2.5.10) 
determines a line in the (tl, 12) plane. By varying the value of, in its permissible 
range, we get a set of parallel lines as illustrated in the figure . The density function 
of, at "0 , pC, = '011'1,1'2 , 1», is obtained simply by "aggregating" all the joint 
densities of (1],/ 2 ) on the line (2.5.10). Repeating this process for all values of 
'. we obtain the entire distribution of, as shown. 
To perform the integration in (2.5.9), we may make the transformation 
{T = 12 cos cp -
II sin cp, 
\z=1 2 sinc/>+ I]COSc/>. 
(2.5.11 ) 

106 
I 
I 
I 
Standard Normal Theory Inference Problems 
/ 
/ 
I 
/ 
", .... 
", 
"- ...... ...... --
/ ~ 
Contours of the joint 
/ 
distribution of (tI · 11 ) 
/ " 
/'" 
--
2.5 
The 
distribution 
PIT , VI' V1 • q,) 
Fig.2.S.1 Geometric interpretation of the integration process in producing the distribution 
of r . 
Since the transforma tion has unit Jaco bian , it follows that 
J
"" [ 
(z cos-¢-rsin¢)2]- +(YI+I ) 
per I v( , V2 , ¢) = k 
1 + -------
• . 00 
VI 
-
00 < r < ::I) , 
(2.5.12) 
where 
k- 1 = B G·vt,t)BUV2,t)(V t I'2)I f 2. 
Note that the distribution of r depends only on three parameters (1'1,1'2 , ¢). 
A result which is identical to (2.5.12) was justified by Fisher (1935, 1939) 
using fiducial theory. 
J-isher refers to earlier work by Behrens (1929), and the 
distribution (2.5. 12) has come to be called the Behrens-Fisher distribution . A 
table of percentage points of the distribution was ta bulated by Sukhatme (1938). 
The Behrens-Fisher distribution is a symmetric distribution and is like the 1-
distri bution in its general appearance. Since Bayesi an H. P. D . intervals are numeric-
ally (but not, of course, logically) identicaJ with Fisher's fiducial intervals, they 
ca n be obtained using Sukhatme's table. The Bayesian derivation was first given 
by Jeffreys (1961). 

2.5 
Inferences Concerning the Difference Between Two Means 
107 
2.5.3 Approximations to the Behrens- Fisher Distribution 
As we emphasized, where possible it is desirable to show the whole posterior 
distribution rather than particular intervals. 
Although the distribution is not 
exactly expressible in terms of tabled functions, a number of attempts have been 
made to approximate it in terms of tabled functions. We employ an approximation 
proposed by Patil (1964), who fits a scaled t distribution to the distribution of r 
by equating the second and the fourth moments. It is shown by Pati! that r is 
approximately distributed as 1(0, a 2 , b), where 
( 
b - 2) 
a
2 = --b- II' 
(2.5.13) 
and, as before, 
and 
To this degree of approximation, the difference of the mean values y = (e 2 - el ) 
is distributed a posteriori as I[h - .VI' a 2(s Unl + s~ /n2) ' b]. Having calculated a 
and b, tables of the density of Student's I distribution may be used to obtain 
approximate ordinates from which the posterior distribution can be sketched, 
and the probability integral of t may be used to obtain Bayesian intervals for 
probability levels not tabulated by Sukhatme. 
2.5.4 An Example 
Suppose, in the spinning modification expe1iment the following results are 
obtained: 
.VI = 50, 
.V2 = 55, 
s~ = 12, 
s~ = 40 . 
Using the approximation in (2.5.13), we find 
2 
H 
50 
cos c/> -
-
-
-
u + ±..Q. -
59 ' 
20 
I 2 
2 
2 
9 
si n 
A. = I -
cos 
A.. = -
'I' 
'I' 
59' 
.... 
VI = n J -
I = 19, 
II, 

108 
Standard ;\Iormal Theory 1nference Problems 
2.5 
so that 
C91)G~) + C~)(;9) = 1.2063, 
12 = ('91r(+)(~~r + C~rC~)(;9r = 0.1552, 
from which 
• 
b = 4 + f) = 13.376, 
2 
a
2 = ( 
b - 2) 
-b-
, II = 1.026. 
Th us, on the basis of a 10cally uniform prior for (Ii I ' li 2 , log (J I' log (J 2), the posterior 
distribution of the difference in means li2 -
iiI of standard and modified spinning 
machines would be closely approximated by a 1(5,4,035, 13,38) distribution. That 
is, by a scaled I-distribution having b == 13,4 degrees of freedom centered at 
Y2 - Yt = 5, with scale factor 
This approximate distribution is shown by the broken curve in Fig, 2.5.2, 
Also shown by the solid curve, in the same figure, is the exact distribution of 
-
-
Exact 
0.14 8 
--- Approximal e 
0.186 
0.124 
0.062 
Yl - Y1=S 
Fig. 2.5.2 Comparison of spinning machines: posterior distribution of y = 82 -
8,. 

2.6 
Inferences Concerning A Variance Ratio 
109 
y = (8 2 -
0\). Table 2.5.1 gives a specimen of the exact and approximate density 
of ~'. The agreement is exceedingly close. Limits of the approximate 95 percent 
H.P.D. interval are given by 
From Table IV (at the end of this book), 10025( 13.38) is nearly 2.15 so that the 
extreme points are (0.673, 9.327). This interva l is also shown in Fig. 2.5.2. 
No results paralleling the Bayesian (Behrens- Fisher) solution are available 
from sampling theory. Welch (1938 , 1947) has considered the problem from the 
sampling viewpoint but the solution he produced does not correspond with the 
Bayesian solution . 
Table 2.5.1 
Exact and approximate posterior density of y 
y 
Exact density 
Approximate dc:nsity 
5.0 
0.19411 
0.19490 
5.2 
0.19310 
0.19387 
5.4 
0.19010 
0.19080 
5.6 
0.18523 
0.18581 
5.8 
0.17863 
0.17907 
6.0 
0.17054 
0.17081 
6.2 
0.16120 
0.16130 
6.4 
0.15089 
0.150H3 
6.6 
0.13990 
0.13970 
6.8 
0.12851 
0.12820 
7.0 
0.11700 
0.1 .1660 
7.2 
0.10561 
0.10515 
7.4 
0.09453 
0.09405 
7.6 
0.08395 
0.08347 
7.8 
0.07398 
0.07354 
8.0 
0.06474 
0.06434 
8.4 
0.04859 
0.04832 
8.8 
0.03562 
0.03547 
9.2 
0.02558 
0.02554 
~.6 
0.01805 
0.01809 
10.0 
0.01257 
0.01264 
2.6 INFERENCES CONCERMNG A VARIANCE RA no 
Consider again the example just discussed in Section 2.5, where the investigator 
was interested in studyine the effect of a spinning modification on the breaking 

110 
Standard l\IormaJ Theory Inference Problems 
2.6 
strength of yarn. Previously we supposed that his interest centered on a possible 
difference in the means. However, variability of the strength of yarn is also of 
great practical importance, and we now consider how inferences might be made 
about a possible change in the variability of yarn bdween the standard machines 
and the modified machines. 
We suppose, as before, that III independent observations Y'l = (YII, ··· ,Yln) 
are as if drawn from a Normal population N(G I , (J~) , and /1 2 observations y~ = (hi' 
... , J'2n) are as ifdrawn from :V(G 2 , a~). We first derive the posterior distribution of 
the variance ratio a~ / ai . 
Again assuming for our noninformative reference prior 
that 01, 01 , log ai ' and log a2 are approximately independent and locally uniform, 
by using Theorem 2.4, I (on page 97) we find that a posteriori a N vIs~ and ai /v2si 
are independent and distributed as x,~ 2 and x;/, respectively. Consequently, the 
posterior distribution of the ratio 
a ~/s~ 
ai /a~ 
SiiSi 
F = ~
/ 2 = -2-'-2- = ~/ 
2 
a l Sl 
S 2/ S t 
at a2 
(2.6.1 ) 
has the usual F distribution with (VI' I'z) degrees of freedom, that is, 
F > O. 
(2.6.2) 
Again , with appropriate interpretation this parallels the sampling theory result. 
In repeated samples from Normal populations the quantity (s fisn / (ai l a~), in 
which s~ ;si is the random variable and the ratio a~ /a~ is an unknown fixed constant, 
is also distributed as an F variable with (VI' vz) degrees of freedom . It follows 
that the posterior distribution of a~ / aL on the basis of a locally uniform prior for 
(0 1, G2 , log ai' log ( 2) , is numerically equivalent to the corresponding confidence 
distribution . 
2.6.1 H.P.D. Intervals 
In comparing the spread of two Normal distributions, there are many measures 
that could be used besides a~ / ai, for example, the reciprocal ratio ai/ai, the ratio 
of standard deviations a z/a I, and log a z -
log (J I' 
Since log a I and log a 2 are 
assumed locally uniform a priori, the standardized H.P.D. intervals will be the 
H.P.D. intervals of log a2 -
log at. Noting that H.P.D. intervals are equivalent 
under linear transformation of log a 2 -
log a I, 
(2,6.3) 
where c and q are two arbitrary constants, we may consider simply the logarithm 
of F, 
log F = (log a~ -
log af) -
(log si - log si). 
(2,6.4) 

2.6 
Inferences Concerning a Variance Ratio 
III 
The distribution of log F is 
-
00 < log F < 0::,. 
(2.6.5) 
which is the form first derived by Fisher (1924). 
In terms of F, the moue of the distribution in (2.6.5) is F = 1 which means that 
the mode of distribution of log a~ 'a~ is 
(2.6.6) 
Limits of H.P.D. intervals of log F are given in Table Y (at the end of this 
book) for a combination of values (a , v" v2 ) . 
To avoid computing logarithms 
the limits are given in terms of F. Th us, the values [,(1'" 1'2, a) and F( v j, v2 , C() 
in the table are such that 
a) 
and 
b) 
Pr {F < D + Pr {F > F} = rJ.. 
(2.6.7) 
further, the table only gives F. and F for VI ;:: 1'2' for V2 > v" the corresponding 
limits may be obtained by using the relationships 
and 
The use of the table is illustrated by the following example. 
@AnExamPle 
I Consider again the spinning modification example for which 112 = 12, si = 40, 
11, = 20 and s~ = 12. Figure 2.6.1 shows the posterior distribution of log ((J~ aD. 
The abcissas are plotted in terms of the variance ratio (J ~. at. To obtain a 95 
per cent H.P.D. interval we have from Table Y, 
F( 19, 11,0.05).= 0.352 
and 
]:'(19, 11 , 0.05) = 3.150, 
so that the lower and upper limits (Va, V,) are respectively 
( si ) 
Vo = --;r [,(19, 11 , 0.05) = (3.33)(0.352) = 1.1722 

Jl2 
Standard Normal Theory Inference Problems 
2.6 
0.6 
>-
N-
-2. 
0.4 
NN 
<:> 
Ol} 
E 
"-
0.2 
Fig.2.6.1 Comparison of spinning machines: posterior distribution of log (a~/ai) 
(scaled in a~ /aD. 
and 
( si )-
Vj = 
~ 
F( 19, 11 , 0.05) = (3 .33)(3.150) = 10.4895, 
which are also shown in the figure. 
A reasonably accurate impression of the distribution p[log(aia~) I yJ is gained 
by stating the mode and a suitable set of H.P.D. intervals. For the present example, 
the mode of log (ai ,af) is at aU a~ = si /s~ = 3.33 and a suitable set of intervals is It 
given in the foJIowing table.X 
Table 2.6.1 
Probability (J -
ex) that aVaf lies within limits (Vo, Vj) 
---- .--------------------------
(1 -
cx) 
Vo 
VI 
0.75 
1.8015 
6.3836 
0.90 
1.3853 
8.6047 
0.95 
1.1722 
10.4895 
0.99 
0.8458 
15.7842 

2.7 
Analysis of the Linear Model 
113 
2.7 ANALYSIS OF THE LINEAR MODEL 
We consider in this section the linear model 
y = xe + t, 
(2.7.1 ) 
where y is a 11 x I vector of observations, X a 11 x k matrix of known constants, 
e <l k x I vector of unknown regression coefficients, and tan x I vector of 
random variables having zero means (often referred to as errors). Explicitly, the 
model is 
l" 
. I 
XII···Xlk 
01 
tl 
YII 
X IIJ " 'X Jlk 
+ 
£" 
(2.72) 
)"/1 
XII!" 'x Hk 
Ok 
t /l 
and, in particular, for the uth observation 
(2.7.3) 
In this model, y may be called the dependent variable, the response, or the all/put 
variable, while the x's are referred to as the independent variables or input variables. 
The relationship between the dependent and independent variables, regarded as 
vectors, is brought out by writing the model in the form 
(2.74) 
where 
i = 1, .. ,k, 
are the columns of the matrix X which is called the matrix a/independent variables 
or the derivative matrix. 
The model has very wide applicability. 
1. It is frequently employed in regression problems where (xIII' ... , Xllk) may be 
the levels, or functions of levels, of quantities such as temperature and pressure, 
which have been set or observed in a series of experiments. 
2. The model is equally applicable where some, or all, of the Xi ' S are vectors of 
"indicator" variables. An indicator XII;' taking only the values one or zero , 
may be used to signify the presence, or absence, of a contribution 0; to the 
ubservation Y,r 
As a simple example, the model 
Yu = 0 + ell' 
can be written equally well as • 
u = 1, ... , n 
(2.7.5a) 
u = I, .. . , n 

,114 
Standard Normal Theory Inference Problems 
2.7 
• 
or as 
(2,7,5b) 
where XI is a 11 x I column of ones, This model is now a special case of (2,7.4), 
If we assume that £"'s are independently distributed as N(O, ( 2 ) , the model (2,7,5b) 
is identical to the one discussed in Sections 2,2 and 2.4. 
As a second example, suppose the response is expected to be a quadratic 
function of a variable ¢, so that the model takes the form 
Then writing 
X/I l 
I, 
11 = I, " ,,11, 
we have 
y = x j 8 1 + x28z + X383 + E , 
Again this model is a special case of (2,7.4), 
(2,7,6a) 
(2,7,6b) 
As a final example, in the one-way classification analysis of variance models, 
we are typically concerned with the comparison of k treatments for which the mean 
responses are 81, ' '', 8k , 
Suppose then we have 11 = 2:7= 111; observations, the 
first /1 1 of these being made with treatment one, the second /12 with treatment two, 
etc, The appropriate model written in the form of (2,7,2) would be 
)!~'-' , 
Yn,+ I 
YII 
.... 
1 
+ 
(2,7,7) 
Thus, models associated with polynomial regression, multiple regression, analysis 
of variance, randomized block designs, incomplete block designs, and factorial 
designs are all subsumed in the general linear model. 
In this section, we assume that E has the multivariate spherically Normal 
distribution NII(O, (
21) and that the rank of the matrix X is k, so that 
p(y I e, ( 2) = (-!-)"a- II exp [- ~ 
(y -
xe)'(y -
X9)] 
.J2rr, 
2a 
~ 
a-" exp - -2 [vs 2 + (9 -
O)'X'X(O -
9)] 
( I)" 
{I 
_ 
_ } 
.J2rr 
2a 
(2.7,8) 

2.7 
Analysis of the Linear Model 
115 
where 
e = (X'X)-l X'y, 
v = 11 -
k, 
52 = (I l v)(y -
y)'(y -
y) 
and 
y = XO. 
It then follows (hat: 
a) e is a vector of statistics jointly sufficient for S if a 2 is known; 
b) e and S2 are jointly sufficient for (S, ( 2 ); 
c) e has the multivariate Normal distribution NkCS, (T2(X'X) I], and 
d) vs 2 is distributed independently of 0 as (T2X~. 
We shall distinguish between two situations: the first in which the variance a2 is 
supposed known , and the second where a 2 is supposed unknown. 
2.7.1 Variance a 2 Assumed Known 
If a 2 is known, then from Lemma 1.4.1 (on page 62) the posterior distribution of 
Sis 
p(S I y) x: p(S)p(e I S, (T2), 
(2.7.9) 
where p(S) is the prior distribution and pee IS, (T2) is the density of a multivariate 
Normal distribution Nk[S, (T2(X'X) -IJ As in (1.3.73), if Ie is not large, we may 
employ p(S) IX canst. as a noninformative reference prior for S. Then, the poskrior 
distribution for 8 is approximately 
[ 
I 
. 
. ] 
p(S I y) u: exp -
2(T2 (S - s)' X' X(S -
S) 
. 
(2.7.10) 
Since 
f [
I
. 
. J 
(T' ( 27d 
exp --2(S-S)'X'X(S-S) d8=-X'X' !l2 ' 
R 
2a 
I 
" 
where R is the region 
-
0'.) < (); < 00 , i = I, ... , k, we have 
IX'XI
U 2 [I. 
. J 
p(S I y) ,,; 
k exp 
-
-
2 (S - 8)'X'X(8 - S) , 
(.j2rr. a) 
2(T 
-
fJ < 0i < u:: , 
i = I, ... ,Ie 
(2.7. I ') 
All relevant inferences about e can now be made from the knowledge that 
the posterior distribution of S is 
the multivariate Normal distribution 
.\1 kCe, a 2(X'X) - 1 J 
• 

1I6 
Standard Normal Theory Inference Problems 
2.7 
• 
Some Properties of the Distribution of 9 
The density function p(9 j y) is a monotonically decreasing function of the positive 
definite quadratic form 
Q(9) = (9 - B)'X'X(9 -
B), 
(2.7.12) 
so that 
Q(9) = c, 
c > 0 
(2.7.13) 
defines a contour of the distribution in the k-dimensional space of 9. For k = 2, 
(2.713) is the equation of an ellipse; for k = 3, it is that of an ellipsoid, and for 
k > 3, that of a hyper-ellipsoid. 
Frum properties of the multivariate Normal 
distribution, Wilks (1962), the quadratic form Q(9) is distributed, a posteriori, as 
(J2Xl. Therefure the inequality 
Q(9) ~ /ek, ell 
(2.7.14) 
defines an ellipsoidal region in the parameter space of8, and (I -
el) is the posterior 
probability contained in this region. The interpretation of such a region is discussed 
la ter. 
Denoting 
r 
,,
-
r 
k - r 
X'X = [X:.)(I·X;Xz]r 
X;XIX~X2 k - r 
(2.7.15) 
where I ~ r < k, it follows from properties of the multivariate Normal distribution 
that: 
a) The marginal posterior distribution of a subset of 6, say e1, is the multivariate 
1\Jormal distribution Nr(B 1 , 0'2C 11 l and, in particular, the marginal distribution 
of OJ is N (8 j , (J2 Cjj), where Cjj is the ith diagonal element of (X' Xl - I. 
b) The conditional posterior distribution or81 . given e1, is the multivariate Normal 
distribution ,Vk - r[B 21 , 0'2(X;Xll 'J, where 
(2.7.16) 
2.7.2 Variance 0'2 l,;nknown 
In many cases the variance 0'2 will not be known, and information about 0'2 
coming from the sample is used. rrom Lemma 1.4.1 (on page 62) the posterior 
distribution of (e, 0'2) is then 
(2.7. 17) 

2.7 
Analysis of the Linear :vTodel 
117 
From Section (1.3.6) we obtain a noninformative prior with 9 and log (J approxi-
mately independent and locally uniform , so that 
(2.7.18) 
Substituting (2.7.1 8) into (2.7. 17), and following an argument similar to that used 
in Section 2.4 the joint posterior distribution of (e, (J2) may be factored such that 
(2.7.19) 
where the marginal posterior distribution of (J2 is 
VS2 X ,~ 2, I' = 
11 -
k, and the 
conditional posterior distribution of e, given (J 2 , is the multivariate Normal 
distribution given in (2.7.11). Applying formula (A2.1.2) in Appendix A2.1 to 
integrate out (J 2 from (2.7.19), the (marginal) posterior distribution of 9 is 
1[-t(1' + k)] IX'Xll iZS- k 
p(9 I y) = 
[rc})]kr(-tv)(vld 
[ 
(9 - 9)'X'X(e - 8)] -1-(Y ';'k ) 
I + 
2 
vs 
i = ], ... , k, 
(2.7.20) 
which is the multivariate 1 distribution discovered independently by Cornish (1954) 
and by Dunnet and Sobel (1954). We shall denote the distribution in (2.7.20) as 
Ik[9,s2(X 'Xr-1 , v]. 
Some Properties oj the Dislribulion of 9 
The multivariate t distribution has the following properties : 
I. 
The density function in (2.7.20) is a monotonically decreasing function of 
the quadratic formQ (9) = (e - 8)'X'X(9 - 8), so that, like the multivariate 
)\iormal posterior distribution of 0 discussed in Section 2.7.1, contours of pee I y) 
are also ellipsoidal in the parameter space of 9. 
2. 
The quantity Q(9)/ksz is distributed a posteriori as F with (k, v) degrees of 
freedom. The ellipsoidal contour of pee i y) defined by 
Q(9) 
--2- = F(k, v, a:), 
ks 
(2.7.21) 
where F(k , 1' , a) is the upper 100a percentage point of an F distribution with (k, v) 
degrees of freedom , wil] thus delineate a region containing a proportion 
(1 -
a) of the posterior probability. Table VI (at the end of this book) gives values 
of F(v 2 , VI _ a) for various values of (1'2' VL ' IX) with V2 < VI' 
3. 
The marginal distribution of a r-dimensional subset, 9 t , has the multivariate 
t distribution Ir(9 1, S 2C II , v) , that is, 
)(9 I ) = 
2 
,
I I 
[, 
I 
I 
I I 
L 
I 
1[.1 (v + r)] IC- I II / 2S-r [ 
(9 - 8 )'C- I (9 - 9 )] -.! ( y ., rl 
I 
I Y 
[r(t )]T(1v)( 
~y 
T 
vs2 
' 
-
00 < 8i < 00 , 
i = I, ... , r, 
(2.7 .22) 
• 

118 
Standard Normal Theory Inference Jfroblems 
2.7 
where the notation is that employed in (2.7.15). In particular 8j has the distribution 
t(8 j , S2C jj , v); that is, 
8j -
{)j 
I = -----=-
S.}Cii 
(2.7.23) 
has the Student's t distribution with v = 11 -
k degrees of freedom . 
4. 
The conditional posterior distribution of a (k - r) dimensional subset of 
parameters e2 , given ai, is also a multivariate I distribution. Specifically, a2 has 
the distribution 
(2.7.24) 
where 
and 
sL 
= (v + rr I[VS 2 -i- (e) -
(1)'C~II(el - ( 1)]. 
5. 
Suppose cj> = (cPI , .. . , cP,.,)" m ~ k, is a set of parameters such that 
q, = De, 
(2.7.25) 
where D is an m x k matrix of rank 111. Then a posteriori q, is distributed as 
(2.7.26) 
All the above results, of course, parallel sampling theory results. In particular, 
the posterior distribution (2.7.20) is identical to the confidence distribution of e, 
and the elli psoidal posterior region enclosed by the contour given in (2.7.21) is 
numerically equivalent to a (I -
Ci) confidence region for a. 
2.7.3 An Example 
A micro method for weighing extremely Jight objects is believed to have an error 
which is approximately Normal. It is also believed that over the relevant range of 
weights the error has expectation zero and constant variance. The following data 
(Table 2.7.1) are available from observations made in 18 weighings of two speci-
Table 2.7.1 
Observed weights of light objects in micrograms 
A Only 
B Only 
A and B Together 
109(8) 
114( 1) 
J 29(5) 
217(2) 
233(1) 
85(4) 
121 ( 6) 
98(11) 
20P ) 
221(9) 
J40(12) 
J 34(14) 
243( 10) 
22] ( 13 1 
12i 15 ) 
133( 16 ) 
229( 17) 
125(18) 
,·1, 
-
--
-
-. 
~. 
. '  
-,' 
~ 
• 
: .  
. 
WIIIIIl 
I ' "  
I 
& 
, 
~,'" 
• 
I 
I 
• 
' 

2.7 
Analysis of the Linear Model 
119 
mens, A and B. 
Two weighings were available for specimen A alone, nine for 
specimen B alone, and seven for specimens A and B together. 
The numbers 
in round brackets associated with each weight refer to the order in which 
the weighings were made. In the present analysis this is assumed to be irrelevant. 
As explained in Section 1.1 , in a full statistical treatment the analysis about to 
be described would be regarded as tentative and would be followed by criticism 
which would certainly include plotting residuals against time order and checks 
on additivity and other assumptions. This could in turn lead to modification of 
the model form. We will not follow through with this process here but will use 
the data to illustrate only the initial inferentia l analysis. 
We postulate the standard linear model 
y = xe + E. 
The 18 x 2 matrix X and the vector of observations yare shown below. The ele-
ments of the first column of X take values one or zero, depending upon whether 
specimen A was, or was not, present. Similarly, the elements of the second column 
indicate the presence, or absence, of specimen B. 
109 
I 
0 
85 
1 0 
1I4 
0 
129 
0 
121 
0 
98 
0 
140 
0 
134 
0 
122 
0 
Y= 
133 
X= 0 
125 
0 
217 
233 
203 
221 
243 
221 
229 
For the analysis, we require 
X'X = [~ 
(X'Xf
l =[ 
0.1684 
-0.0737 
- 0.0737 
0.0947 
X 'y = [
1761] 
2683 ' 
e = [~J] = [ 98.895J 
82 
124.421' 
• 
] , 

• 
120 
Standard Normal Theory Inference Problems 
2.7 
j 8 
'\ ),2 = 510501 
L 
I 
') 
9'X'y = 507,976, 
j= 1 
18 
18 
I (Yi - Jli)2 = I y ~ - 9'X'y = 2,525, 
i = I 
j= 1 
5 2 = 157.8, 
s = 12.56. 
Q(9) = (9 - e)' X' X(S - 9) 
= 9(Vj - 98.9)2 + 14(VJ -
98.9)(V2 -
124.4) + 16(02 -
124.4)2. 
Given the assumptions and the data, all that can be said about the vector of 
parameters 9' = (OJ , ( 2 ), in the light of the noninformative reference prior 
p(9, a 2 ) oc a- 2 , :s contained in the joint posterior distribution of (OJ , 02) , which 
from (2.7.20) is the bivariate 1 distribution 
{[ 
98.9] 
[0.1684 
'2 124.4' 157.8 
-0.0737 
-0.0737] 
} 
0.0947 ' 16 . 
Elliptical contours of constant density which incJude 75, 90, and 95 percent of 
the probability are shown in Fig. 2.7.1. 
From (2.7.21) the equations of these 
contours are readily computed by setting Q(9)'2s2 equal in turn to 1.51,2.67 and 
3.63 which are, respectively, the upper 25, 10 and 5 percent points of the F distri-
bution with 2 and 16 degrees of freedom . 
The marginal probability distributions for 8 j and 82 are also shown. From 
(2.7.23) the distribution for VI is the scaled t distribution 
1(98.9,157.8 x 0.1684,16), 
and for Vz is the scaled 1 distribution 
1(124.4,157.8 x 0.0947, 16). 
On each of these distributions, pairs of ordinates of equal heights have been drawn 
to mark off "contour points" which include respectively. 75, 90 and 95 percent of 
the marginal probability. 
The problem of making inferences about 9 can be summarized as follows : 
Given the assumptions, all that can be known about 8 = (0 1, 02) is contained 
in the joint posterior bivariate t distribution. MathematicaUy speaking, therefore, 
the problem is solved as soon as this distribution is written down . To assist in 
the mental appreciation of the joint posterior distribution however, various 
relevant features may be described. These features are all derived from the original 
joint distribution and of course add no new mathematical information, but 

2.7 
Analysis of the Linear 'VIodeJ 
12J 
135 
130 
125 
120 
Fig. 2.7.1 The weighing example: contours of joint posterior distribution and marginal 
distributions of (JI and 02. 
consideration of these features can lead to a better mental comprehension of the 
implications of the posterior distribution, and so to better inferences. Features of 
particu lar im porta nee are (a) the H. P. D. regions, and (b) the ma rginal distributions. 
H.P.D. Regionsjor et and O2 
As we have mentioned, the elliptical contours of the joint posterior distribution 
shown in Fig. 2.7. 1 include, respectively, 75, 90 and 95 percent of the probability. 
This uses the fact that a posteriori the quantity Q(e)/2s 2 is distributed as F with 
(2,16) degrees of freedom. Further, since from (2.7;20) the joint posterior distri-
bution of peel' 82 i)') is a monotonically decreasing function of Q(e). 2S2, every 
point inside a given contour has a higher posterior density than every point out-
side. The regions bounded by the three elliptical contours are thus the 75, 90, and 
95 percent highest posterior density (H.P. D.) regions in (8" ( 2), respectively. 
The pairs of vertical lines shown in the marginal distributions similarly 
provide the limits of the 75 , 90, and 95 percent H.P.D. intervals for the marginal 
d istri butions. 
• 

• 
122 
Standard Normal Theory Inference Problems 
2.8 
M arginai Distributions-- -J oint Inferences versus M arginai Inferences 
If we wish to make probability statements aboL~t 8 1 or 82 individually, this can 
be done by using the marginal posterior distribution p(BI I y) or p(8 2 I y). 
It is 
important, of course, not to make the elementary mistake of supposing that 
joint probability statements can generally be made using the marginal distributions. 
For example, it is easily seen from Fig. 2.7.1 that values of 81 and 82 can readily 
be found which are each individually plausible, although they are not jointly so. 
Consider, for example, the point A = (92.5, 120) which is well inside the 
90 percent H.P.D. intervals of the marginal distributions, but is excluded by the 
90 percent H.P.D. region of the joint distribution. The common sense of the situ-
ation is easy to see. When we are considering the marginal distribution of, say 8 1, 
we are asking what values can 81 take irrespective of 82 - It is implicit here that 82 
is allowed to vary over all its admissable values with the appropriate probability 
density. But when we are considering the joint distribution, we are interested in 
the plausibility of the pair (8 1, ( 2), Now it is certainly true in the example that 
for some values of 82 , the value 81 = 92.5 is a very plausible one. It does not 
happen to be very plausible, however, if 82 = J 20. 
The distinction between 
marginal intervals and joint regions becomes more important as the correlation of 
81 and 82 becomes greater. The reader will have no difficulty in imagining a case 
where diagonal elliptical contours occur which are more attenuated. The marginal 
intervals would then be even more misleading if they were incorrectly used as 
indicating plausible combinations of (8 1, ( 2), 
2.8 A GENERAL DISCUSSION OF HIGHEST POSTERIOR DENSITY REGIONS 
The above example serves to illu ~ trate the importance of considering jointly the 
distribution of parameters, and of partially summarizing information in the 
joint posterior distribution by means of multidimensional regions of hIghest 
posterior density. We consider now the general properites and usefulness of such 
regions. 
Suppose in a particular investigation we have computed an appropriate 
posterior distribution p(SI y), where e is a k-dimensional vector of parameters of 
interest and y is a n-dimensional vector of observations_ Then, from the Bayesian 
point of view, all inferential problems concerning 0 may be answered in terms of 
p(Oly). 
In practice, inference involves a communication with the mind, and 
usually it is difficult to comprehend a function in k-dimensions. However, there 
are often specific individual features of p(S I y) of interest which can be appreciated 
by one, two, or three dimensional thought. For exa mple, marginal distributions 
may be of interest. Or we may inspect conditional distributions of a small subset 
of the parameters for specific values of the other parameters. With high-speed 
electronic computers available, print-outs of two dimensional sections of such 
distributions can be readily obtained . The value of stKh appraisals of the esti-
mation situation is very great, as has been repeatedly pointed out by Barnard. 

2.8 
A General Discussion of Highest Posterior Density Regions 
123 
In searching for ways to summarize the information in the posterior distri-
bution p(91 y), it is to be noted that although the region over which the posterior 
density is nonzero may extend over infinite ranges in the parameter space, 
nevertheless over a substantial part of the parameter space the density may be 
negligible. 
Jt may, therefore, be possible to delineate a comparatively small 
region which contains most of the probability, or more generally, to delineate a 
number of regions which contain various stated proportions of the total proba-
bility. 
Obviously there are an infinite number of ways in which such regions 
can be chosen. In some cases, a specific region (or regions) of interest may be 
dictated by the nature of the problem. 
When it is not, we must decide what 
properties we would like the region to have. As with H.P.D. intervals, either of 
the following principles of choice seems intuitively sensi ble: 
I. The region should be such that the probability density of every point inside 
it is at least as large as that of any point outside it. 
2. The region should be such that for a given probability content, it occupies 
the smallest possible volume in the parameter space. 
It is easy to show that if either principle is adopted the other follows as a natural 
consequence. We will in general call the region so produced a region of highest 
posterior density, or an H.P. D. region. The first principle will be employed to 
give a formal definition. 
Definition. 
Let p(91 y) be a posterior density function. A region R in the parameter 
space of 9 is called an H.P.D. region of content (I -
a) if 
a) Pr{9ER I y} = I -
0:, 
b) for 9 1 E Rand 92 ~ R, 
pC9 1 I y) ~ p(9 2 I y). 
(2.8.1) 
2.8.1 Some Properties of the H.P.D. Region 
I. It follows from the definition that for a given probability content (I -
Ct.), 
the H.P.D. region has the smallest possible volume in the parameter space of 9. 
2. ff we make the assumption that p(9 1 y) is nonuniform over every region in the 
space of e, then the H.P.D. region of content (I -
a) is unique. Further, if e1 
and 92 are two points such that p(el I y) = p(9 2 I y), then these two points are 
simultaneously included in (or excluded by) a (I -
e<) H.P.D. region. The con-
verse is also tne. That is, if p(el I y) #- p(9 2 I y), then there exists a (I -
e<) H.P.D. 
region which includes one point but not the other. 
Effect of Transformation : Standardized regioJ/s. Let <I> = f(9) define a one-to-one 
transformation of the parameters from 9 to <1>. Any region of content (I -
e<) in 
the space of 9 transforms into a region of the same content in the space of <\>, but 
it is clear from the definition that an H.P.D. region for 9 will not, in general, trans-
form into an H.P.D. region for <1>, unless the transformation is linear. 
As in 

124 
Standard "'Iormal Theory Inference Problems 
2.8 
the univariate case, when a noninformative prior is used, which is equivalent to 
assuming that some transformed set of parameters <I> = f(9) are locally uniformly 
distributed, then standardized H.P.D. regions calculated in terms of <I> are 
available. 
Smallest confidence regions of sampling theory are similarly not invariant under 
general transformation. On the implied assumption that such lack of invariance is bad, 
it has been suggested, for example, that the region should be based on the likelihood itself. 
That is, the boundary of the region should follow a likelihood contour. In particular. 
Hildreth (1963) has proposed that a 100(1 -
u.) percent region R be based on 
(2.8.2) 
where n is the parameter space of 9, with the property for 9 1 EO Rand 9 2 rf: R, 
(2.8.3) 
It will be observed that although the inequality (2.8.3) is preserved under general 
transformation, the equality (2.8.2) will not be preserved. A posterior region based upon 
the likelihood, which is, in a sense, invariant under general transformation, can be obtained 
as follows. For a fixed prior distribution Po(9), choose a region R such that 
a) 
(2.8.4) 
b) for OJ EO Rand 9 2 r/; R, 
(2.8.5) 
Both (2.8.4) and (2.8.5) are invariant under general transformations. This region, which 
tries to make the best of both worlds is, however, a omewhat artificial construction. If we 
believe in the appropriateness of the prior distribution Po(9), then we should surely not 
adopt a region for wh ich the posterior density for points outside can be greater 
than that for points inside. The authors feel that in general nonlinear transformation 
ollght to change the relative credibility of any two parameter points and that invariance 
under nonlinear transformation is therefore not to be expected. Insistence on invariance 
for problems which ought not to be invariant serves only to guarantee inappropriate 
solutions. 
2.8.2 Graphical Representation 
Clearly when there are only two parameters a diagram, showing the point of 
maximum posterior density and, say, a 95 per cent H.P.D. region , would advise 
the investigator of a great deal of what the data had to tell him. 
A more 
informative plot would be one showing simultaneously the boundaries of, say, 
the 50, 75, 90, 95, and 99 percent H.P.D. n:gions. In such a case we would be 
back to the plot of posterior density contours labeled according to their interior 
-
---
. 
I"'i,: ' 
~. 
. , 
.. 
• r 
.. 
• 
_ 
. 
. 
.. 
; , 
. 
. -. 
. 
:.. 
. 
_.' 

j 
• 
" 
• 
. 
. 
. 
'. . 
. 
,. 
. 
_ 
_.,r 
e;'_ 
--
.. ~ 
2.9 
H.P.D. Regions for the Linear Model 
125 
probability content. This graphical approach could be extended to three or four 
parameters by exhibiting a " grid" of two-dimensional (8, , 02 ) diagrams for various 
combinations of 0) and 04' 
Plotters can produce such diagrams automatically 
from digital computer output. Such plotting, which should be part of the normal 
stock in trade of the modern practicing statistician , is invaluable for appreciating 
pecularities+ in an estimation situation and is, therefore, of particular imporlance 
in exploring new problems. 
2.8.3 Is 0 0 Inside or Outside a Given H.P.D. Region? 
While it is true that all the information about k parameters 0 is contained in the 
posterior distribution p(O i y), because of our three-dimensional human limilations 
it may not be easy to comprehend what a distribution of higher dimensionality 
implies. Inspection of appropriate conditional and marginal distributions, or at 
least of H.P.D. regions, will greatly assist understanding. We now discuss one 
further device which , when used in conjunction with those mentioned above, can 
further illuminate the situation. 
This is a way of answering the question whether or not a particular parameter 
point of interest 0o lies inside or outside a H.P.D. region of content (I -
Ct.). 
From properties of H. P. D. regions, we see that if R. is an H. P. D. region of content 
(I -
:I), then the event OE R~ is equivalent to the event that p(O I y) > c, where c 
is a suitably chosen positive constant. It follows that the parameter point 00 is 
covered by the H.P.D. region of content (I -
C/) if and only if 
Pr (p(O I y) > p(Oo I y) I y} ~ I -
C/. 
(2.8.6) 
In this expression , the density function p(O I y) is treated as a random variable. 
Thus, once the posterior distribution of the quantity p(O I y) or some monotonic 
function of it can be determined, this question can be answered. In what follows 
we consider the specific nature of the region p(O I y) > c for a number of examples 
of interest. 
2.9 H.P.D. REGIONS FOR THE LINEAR MODEL: 
A BAYESIAN JUSTIFICATION OF ANALYSIS OF VARIANCE 
As a first example, we return to the linear model discussed in Section 2.7. We 
have seen in (2.7.20) that in relation to a noninformative prior in 0 and (J the 
posterior distribution of 0 is in the form of a multivariate t distribution. Further, 
in (2.7.21) the quantity Q(0){ks 2 = (0 - 9)'X'X(0 -
9)/ks2 is distributed a 
posteriori as F with (k, v) degrees of freedom . Suppose we are now interested in 
the question: Is the parameter point 0o = (0 1°' ... , OkO)' included in the H.P.D. 
region of content (I -
.'l.)? According to the above argument, we then need to 
calculate the probability of the event p(O I y) > p(Oo I y). 
t Visual display devices, which fur example aUow the investigator to see the changing 
two dimensional contours as he moves through a higher dimensional parameter space, 
can also be very helpful. 

126 
Standard Normal Theory Inference Problems 
Now p(91 y) is a monotonic decreasing function of the quantity 
(9 - 8YX'X(9 - 8)/ks 2 . 
2.9 
The particular point 90 is then included in the H.P.D. region of content (1 -
0:) if 
and only if 
(90 -
8YX' X(9 0 -
8) < ks 2F(k, v, Ct). 
(2.9.1 ) 
Equivalently, the quantity 
f 
(90 -
8YX'X(90 - 8) I 
Pr \F1k,V) < 
2 
J ' 
ks 
(2.9.2) 
where F(k ,v) is an F variable with (k, v) degrees of freedom , gives the content of 
the H.P.D. region which just includes the point 90 , 
The results in (2.9.1) and (2.9.2) supply a Bayesian justification for the 
AnaJysis of Variance. The (I -
ex) H.P.D. region is numerically identical with 
the (I -
ex) smallest confidence region, and the inequality in (2.9.1) is also 
appropriate to decide if a given point 90 lies inside or outside the corresponding 
confidence region. 
Further, the complementary probability of (2.9.2) gives the 
significance level associated with the null hypothesis 9 = 90 against the alternative 
o =I 00 , Generalization to the corresponding multivariate linear model is discussed 
in Chapter 8. 
2.9.1 The Weighing Example 
The weighing example of Section 2.7 may be used for illustration. Suppose that 
particular interest was associated with the parameter values 810 = 820 = 120. 
Such interest could arise because of some theory that both specimens should 
weigh 120 micrograms. A question of interest would then be whether the parameter 
point (120, 120) was included or excluded from , say, the 9S percent H.P.D. region. 
Inspection of Fig. 2.7.1 immediately shows that this point is excluded. But the 
question can be answered without recourse to the diagram by considering whether 
Q(90)/2s2 is greater or less than F(k, II, C(). We find 
9 (120 - 98.9)2 + 14 (120 - 98.9) (J 20 -
120.4) + 16 (120 -
124.4)2 
2 x IS7.8 
30lS 
= -- = 9.S6 
31S.6 
' 
which is greater than F(2, 16, O.OS) = 3.63 so that the point (120, 120) is excluded. 
Again , we emphasize that in the above calculation what we are interested in is the 
plausibility that 01 and O2 are simultaneously equal to 120. It is obvious that the 
plausibility of O2 alone being equal to 120 would be a completely different question . 
. .,-
.. -
~ 
. 
. '. 
"
. 
"." -
.' 
..' 
• 
. 
• 
" f '  
. 
~ 
. 

, 
".~' 
. 
. 
"'-
.~ 
. '  
, 
_. 
~ 
-
2.11 
Comparison of the Means of k Normal Populations 
127 
This question would be answered by considering not the joint posterior distri-
bution of (0 1, O2 ) , but the marginal posterior distribution of O2 , 
For purpose of convenient calculation and checking, it is useful to set out 
the quantities needed in the form of an analysis of variance table as shown in 
Table 2.9.1. Wh ile such a table is customarily interpreted in terms of sampling 
theory, it has an equally useful function in a Bayesian framework . In particular, 
the question of whether 80 is, or is not, included in a given H.P.D. region is deter-
mined by computing the ratio of the mean squares in the last column of the table .. 
and referring the ratio to the F(k , v, C() values in Table VI (at the end of this book). 
Table 2.9.1 
Analysis of variance table to determine whether the point (0 10 = 120, (J2 0 = 120) 
is included within a given H.P.D. region 
Sources 
Sum of squares 
Degrees of 
Mean 
Mean square 
freedom 
square 
ratio 
Parameter 
discrepancy 
(80 - a)' X'XC8o - 9) = 3,015 
2 
1,507.51 
157.81 
9.56 
Residual 
(y - y)' (y - y) = 2,525 
16 
Total 
(y -
X8o)' (y -
X8o) = 5,540 
18 
2.10 COMPARISOi\. OF PARAMETERS 
In the previous sections we have discussed problems of deciding whether a 
particular parameter point 80 is, or is not, included in the (I -
C() H.P.D. region. 
Frequently. the investigator is concerned with the comparative values of para-
meters rather than with their absolute values. 
Suppose we have k parameters 
e = (0 1, ... , Ok)" 
We may define (k -
I) nonredundant comparisons as (k -
I) 
independent functions 
i = \, ... , (k -
I), 
(2.10. I) 
which are all equal to zero if and only if OJ = .. . = Ok' There is clearly a very 
wide range of choices for such functions. Some thought must therefore be given as 
to how such comparisons should be parametrized . This question is now considered 
for two important problems in statistics: (a) the comparison of the means of k 
Normal distributions, and (b) the comparison of the variances of k Normal 
distri butions. 
2,}1 COMPARISON OF THE MEA"lS OF k NORMAL POPCLATIONS 
Returning to the linear model discussed in Section 2.7, consider the special case 
where the elements of the 11 x I vector of observations yare independent 

. 
~ . 
128 
Standard ","ormal Theory Inference Problems 
2.ll 
samples y'I=(YI1"",)'lll) , ... ,y'k=(Ykl,"·,Ykll,)' of size 
11 1, ... ,nk (L.I1, = 11) 
from k Normal populations with means (° 1, ... , Ok), respectively, and common 
variance (J2. The posterior distribution of e in (2.7.20) reduces to 
p(el y) cr. [1 + L7= llIi~:~ - y;/]-HHkl, 
-
00 < 8i < 0::" 
i = I, .. ,k, 
with 
(2.11.1) 
I 
.vi = -LYij, 
Ili 
v = 11 - k, 
and 
,
1 
2 
S" = -- L L (Yij - yJ . 
11 -
k i 
j 
. This distribution allows us to decide whether a parlicu/ar set of values of the 
means eo = (8 10 , .. . , 0kO)' is included in the k-dimensional (l -
::x) H.P.D. region. 
In practice, we are most often concerned with such questions as "How different 
are the effects of treatments 1 and 2'1" Such questions must be answered in terms 
of comparisons among the 8i's. 
Statements about the relative rather than the absolute values of the 8i's may be 
made in terms of k -1 parameters <P1,<P2,,,,,<Pk-I' measuring independent 
contrasts between the 8;'5. In particular, the possibility that aU the O;'s are equal 
(to some unknown 8) corresponds to the possibility that all the <p's are zero. For 
example, suppose k = 3. Then we might consider the contrasts 
<PI = 01 - O2, 
(2.11.2) 
<P2 = 81 -
03' 
The two equalities (<PI = 0, <P2 = 0) together imply that 
81 = 82 = 03 = 8, 
irrespective of the actual value of the common parameter O. The possibility that 
8 1 = O2 = 03 can therefore be examined by studying the joint distribution of 
(<PI' <P2)' and checking whether the point (<PI = 0, <P2 = 0) lies inside or outside a 
particular H.P. D. region . 
2.11.] Choice of Linear Contras.ts 
We shall say that a set of k -
1 parameters Ij> = (<p I, ... , <Pk _ I)' measure contrasts 
among the parameters if there are linear functions 
k 
<Pj = I ajl8i = aje, 
j= I, ... ,k -
I, 
(2.11.3) 
; =- 1 
such that (<PI = 0, ... , <Pk- I = 0) necessarily implies (81 = ... = 8k = 8) for 
some unknown O. 
For this property to hold, it is sufficient and necessary that 
a) the vectors ai' .. . , ak 
I are linearly independent, and 
b) a;1k = 0, that is, D laji = 0, j = 1, ... , k -
1. 
.. ;. .,. "
. 
. . 
. 
" 
, 
. 
'. 
. 
. 
. ,. 
, 
. 
... 

, 
I 
• 
. '  
• 
~ 
." 
I 
~ 
, 
.. 
' . 
~ ~ 
" 
.. 
. 
-_...... 
~ 
- -
2.11 
Comparison of the Means of k Normal Populations 
129 
To see this, write 
A{) = Ij>, 
(2. I J.4) 
where A is the (k -
I)xk matrix A = [al ' ... ,ak - I]' . 
For Ij> = 0, we have 
A{) = 0, 
(2.11.5) 
which defines a system of (k -
1) equations in k unknowns. To show sufficiency, suppose 
the conditions (a) and (b) hold, then since rank (A) = k -
1 and Alk = 0, it foJlo~s that 
all the solutions of the equations in (2.11.5) must be of the form () = elk' that is, 
01 = ... = Ok = O. Now to show necessity, suppose () = 8lk. · This means that elk 
satisfies the equations (2.11.5) so that ail. = 0, j = I, ... , k. 
Further, the vectors aj 
must be linearly independent. 
For if not, then there exists a choice of () other than 
() = elk which also satisfies (2.11.5). But this contradicts the supposition that () = 01 k . 
There are obviously an infinity of ways in which a set of k -
I contrasts among the 
k parameters () might be chosen. For example, instead of using the contrasts 1>1 and 1>2 of 
(2. 11.2) we could use 
(2. 11.6) 
These new contrasts can be expressed in terms of the old contrasts by 
so that «(/>7, (vn is a nonsingular linear transformation of (1)1,1>2)' 
In general, the vectors aj (j = 1,2, ... , k -
I) span a (k -
I)-dimensional space, 
and there always exists a nonsingu1ar linear transformation 
Ij>* = TIj>, 
(2.11.8) 
expressing one parameterization in terms of another. 
Since the H.P.D. region is invariant under linear transformation, the: question of 
whether or not a particular point in the space of the contrasts is, or is not , included in 
a H.P.D. region will be unaffected by the manner in which the COntrasts are parametrized . 
This is illustrated in Fig. 2.11.1, in terms of the parameterizations expressed by the 
equations in (2. 1 J .2) and (2.11.7). The figure shows a contrast space with rectangular 
coordinates for 1>1 and 1>2 and the corresponding oblique coordinates for 1>~ and 1>; 
defined by (2. 1 J .7). Also shown is an elliptical H.P.D . region . The question of whether 
any point P in the contrast space is, or is not, included in this region is, of course, inde-
pendent of which coordinate system is used to define P. 
In particular, the point 0 is the 
origin for every system of contrasts, so that Ij> = 0 implies 0 1= ... = Ok, however we choose 
the nonsingular matrix T in Ij>* = TIj>. 

r----------
-
130 
Standard Normal Theory Inference Problems 
2.ll 
Fig. 2.11.1 Invariance of H.P.D. region under linear transformation from (cPl'cPZ) to 
(cPi, cPi)· 
I t is convenient here to consider the particular set of contrasts 
¢i = 8i -
lJ (i = I, "', k -
I) 
where 
(2.11.9) 
From (2.7.26), we find that the posterior distri bution of <\> = (cP I, .. , cPk _ 1) is 
{ 
Q(<\» } - Hv+(k-I)) 
pC<\> I y) IX. 
I + -Z-
, 
vs 
-ct:.<¢i <XJ, i=I, ... ,k-l, 
(2.11.10) 
where 
k 
Q(<\» = I ni [¢i -
(5\ - .Y)]2, 
i = 1 
k 
and 
I l7icPi = o. 
i= 1 
Thus, a particular point <\>0 is included in the (l - a) H.P.D. region if and only if 
Q(<\>o) 
---""'"
2 < F(/C -
1, v, a). 
(k -
I)s 
(2.11.11) 
~ '. 
. 
.. 
. 
. 
. 
. 
. 
.' 
. 
. 
. . 
1[1 

2.11 
Comparison of the Means of k Normal Populations 
131 
In particular, the point eVo = 0 corresponding to BJ = ... = (h is included In the 
(I -
C() region if and only if Q(O)/(k -
J)S2 < F(k -
I, v, C(), that is, 
L nj(Yj _ y)2 
---'--'---------:,=-- < F (k -
I, v, C(), 
(k -
J )s-
(2.11.12) 
which parallels the well known significance test. 
Again, the calculations are 
conveniently set out in the familiar analysis of variance of Table 2. J 1.1. 
Table 2.11.1 
Analysis of variance table to determine whether the point eV = 0 (13 1 = ... = 13k ) 
is included in a given H.P.D. region 
Sources 
Sum of squares 
D.F. 
Mean square 
Mean square ratio. 
Inequality of means Q(O) = LI1;(jij _ ji)2 
k-I 
Q(O)/(k - I) 
Residual 
L L (Yij _ ji;)2 
11 -
k 
S2 
Q(O) 
(k -
1)s2 
2.11.2 Choice of Linear Contrasts to Compare Location Parameters 
We have tacitly assumed that it was appropriate to compare the Bj's in terms of linear 
contrasts. There are other ways in which the parameters 9 might have been compared. 
For example, we might have considered ratios such as 13218 1 and 8318J • Special problems 
occasionally occur where sllch ratios are of interest and, ~hen this happens, their 
distributions may of course be obtained directly from the joint distribution of 9. Linear 
contrasts are appropriate in the commonly considered situation where interest centers on 
the relative location of k distributions which are otherwise similar. To see this, notice that 
addition of any constant c to each observation should leave measures of relative location 
unchanged. Now such an addition will change 8j to 8j + c, and if we are to have k -
I 
independent comparison functions II' ... .fk- I' such that 
j = 1,2, .. . , k -
I, 
then the fj's must be functions of the linear contrasts 
k 
¢j = L ajj8j 
j= 1 
where 
k 
L ajj = 0, 
j = 1.2, ... , k -
1. 
i= 1 
(2.11.13) 
We show this for the case k = 2. A linear contrast is then proportional to the differ-
ence 82 -
81, Write 
where 
(2.11.14) 
Now, 
(2.11.15) 

132 
Standnrd '\lor mal Theory Inference Problems 
2.12 
implies that 
(2. I 1.16) 
Since c is arbitrary, it follows that g(¢>, 0\) is independent of 0 1 , that is, 1(0\,02 ) is a 
function of O2 -
0 1 only. 
The simplest functions of linear contrasts, are the contrasts themselves, and 
consequently the problem of comparing lucations has been expressed in these terms. 
2.12 COMPARISON OF THE SPREAD OF k DISTRIB nONS 
Consider now comparison of the spread of k distributions, which may differ 
only in their location and scale parameters but are otherwise the same. It is reason-
able to require that comparisons of k scale parameters (0 1 , ••• , Ok) should be un-
affected by any linear recoding of the data. This would ensure, for example, that 
the measures comparing scale parameters would be the same whether the 
observations were in feet or inches. "'Jowa scale parameter 0 is such that a linear 
recoding of the data which changes an observation y to a + by transforms D to 
IbiD. If we are to have k -
I independent comparison functions gj such that 
j= I, ···,k -
I, 
(2.12.1) 
then the g/s must be functions of linear contrasts ¢j among the logarithms of 
the O's, 
k 
k 
¢j = I aji log Oi' 
I aji = 0, 
j=J, .. . , k-1. 
(2.12.2) 
i = 1 
; : 1 
The simplest such functions are the ¢/s themselves. For ~ormal distributions, 
where the standard deviations aI' (J2, --., (Jk are scale. parameters, suitable com-
parison functions are 
k 
I 
k. 
(Pj = I ajilogai = - I ajilog a'!. 
i'- l 
q i · 1 
(2.12.3) 
The second equality on the right shows that the comparison function will be of 
_ the same form for any af, and in particular, for the variance (J;' 
2.12.1 Comparison of the Spread of k Normal Populations 
If we suppose that k samples Chl'·"'Yln)'''',(Ykl'''',YkIlJ, of 11 1 , ... ,l1k 
independent observations, are drawn from Normal 
populations N(Bi' (J;) 
(i = I, ... , k), where both the means 0i and the variances (J; are unknown, and that 
a priori 8i and log (Ji are all approximately independent and locally uniform, it 
then follows from Theorem 2.4.1. (on page 97) that the joint posterior distri-
-
----
. 
. . 
-
. 
~ ". 
. 
. 
. 
. 
. 
. 
. 

2.12 
Comparison of the Spread of k Distributions 
133 
bution of a~ , ... , af is the prod UC! , 
p(af, .. . ,a; ly) = (I Wj(af) - C'H' ;) +IJexp(- VjS~)\, 
j c 
I 
2a i 
ai2 > 0, 
i = I, ... ,k, 
where 
Vj = Il i -
I, 
2 
I 
_ 2 
Si = -
L (Yij -
)'i) , 
V; 
(2.12.4) 
To compare the sp,cad of the k Normal populations, we may choose for our 
comparison functions any (k -
I) linearly independent contrasts in log ai• 
For 
this development we set 
1;j = log a; -
log a; = 2(log aj - log ak), 
1 = I, .. . , (k -
J). 
(2.12.5) 
It is straightforward to veri fy from (2.12.4) that the posterior distribution of 
(j>' = (1;" .. . , 1;k - ') is 
rev. 2) 
(..10.1) 
TI "':2 .. · T ~"!'. -I,) / 2(I+TI
+ .. ·+T.k_ I) - "/l, 
P'I' Y = D7. , r(vJ 2) 
• 
-':I.: < 1;; < x, 
1 = I, .. . , k -
J, 
(2.12.6) 
where 
1=1 , .. . , k-1 
and 
l'=v,+"·+vk. 
Upon differentiation , it is readily shown that this distribution has a unique mode 
at 
1 = I , .. . , k -
I., 
(2.12.7) 
so tha~ T; can be alternatively written as 
V · 
• 
Tj = ~exp [-(1;; - 1;;l]. 
(2.12.8) 
10k 
A particular point <1>0 is included in the H.P.D. region of content (I -
:x) if and 
only if 
Pr {p(<I>; y) > p(<I>oIY)ly } < I -
'1 . 
(2. 12.9) 

134 
Standard Normal Theory Inference Problems 
2.12 
Now, the density function p(<p ~ y) is a monotonic decreasing function of 
tv! = -210g W, 
(2.12.10) 
where 
(
Vk)-V/2[ 
k-I Vi 
~ 
]-V. 2k-1 
• 
W = -
1 +I - exp [ - (<Pi - <Pi)J 
11 exp [ - i VJ<Pi - <PJ]' 
v 
1=1 Vk 
1=1 
(2.12.11) 
I n terms of a} and s;, 
(2.12.12) 
which is the ratio of a weighted geometric mean over a weighted arithmetic mean 
of the quantities s; 'af, with weights proportional to I'i' From (2.12.10), the event 
p(<\> I y) ;:, p(<\>o I y) is therefore equivalent to the event :vt < - 210g Wo, where Wo 
is obtained by substituting ePiO for <Pi in (2.12.11). [n particular, we may be inter-
ested in the point <Po = 0 which corresponds to the situation crt = ... = ar 
From (2.12.12), on setting §2 = (1 / 1')2::7.,1 ViS; , we see that 
k 
- 2 log Wo = - I 
I'i (log Sj2 -
log §2). 
(2.12.13) 
i= 1 
The expression on the right is identical with Bartlett's criterion for testing 
homogeniety of variances developed from sam pli ng theuy [see Bartlett (1937)]. 
2.12.2 Asymptotic Distribution of M 
We now discuss some asymptotic results from. which the distribution of tv! can 
be closely approximated. Making lise of the Dirichlet integral CA2.I.S) in Appendix 
A2.1, we obtain the moment generating function of tv! as 
£(etM ) = 
[(1'/2) 
r1 (1') 1')'"., f[(vsl2)(1 -
21)J_ . 
(2.12.14) 
['[(v/2)(1 - 2/)J F 
1 
f(l'si2) 
Taking logarithms and employing Stirling's series for the logarithm of the Gamma 
function [see (A2.2.11) in Appendix A2.2] , we find the cumulant generating 
function of M to be 
k -
1 
"" 
/(M(I) = a - --log (I -
2/) + I er:r(1 -
21) - (2r-l), 
2 
r= 1 
(2.12.15) 
where 
~ 
a = 
I 
Ct. r 
r::=. 1 

-. 
-
-
. 
~-. 
. 
2.12 
Comparison of the Spread of k Distributions 
135 
is a constant independent of 1, 
_~~2r 
_
_ 22<-1 
'" 
- ( 2r-l) 
- (2 r - 1) 
B 
[ k 
] 
'lo 
= 
L 
v· 
-
v 
r 
2r(2r -
I) 
i ; I 
I 
and B 2r are the Bernoulli numbers. 
Now the quantity Ct.r is of order maxi v-(2r-1 >. Thus, as the v/s become large, 
'lor tends to zero and in the limit 
k-I 
}i~ r"M(I) = - - 2- log (I -
21), 
(2. I 2. I 6) 
i :;; I ... .. k 
which is the cumulant generating function ofax~_ J variable. Consequently, as 
Vi -> 00 (i = I, ... , k), M is asymptotically distributed as d-I' so that 
lim Pr{p(<I>ly»p(<I>oly) ly}= lim Pr{M < -210gWo} 
\'j -oo 
"i- ex:. 
= Pr{x1-J < -210g Wo}, 
(2.12.17) 
For large samples, then, the point <1>0 is included in the (I -
Ci) H.P.D. region if 
- 2 log Wo < X2(k - I, Ci) . 
For moderate sample sizes, a modification, due to 
Bartlett (1937), can be used to approximate the distribution of M. 
Bartlett's Approximation 
From (2.12.15) the rth cumulant of M is, to order maXi vi- 1 
"r(M) = 2'"1 (r -
I)! (k -
1)(1 + Ar), 
(2.12. J 8) 
where 
A = 
1 
( i Vi- 1 - v-l) . 
3(k -
1) ;'1 
If we take 
M..;... (I + A)xi-J 
(2.12.19) 
then 
1(1 (M) = (k. -
1)( I + A), 
"r(M) = 2r- l (r -
I)' (k -
1)(1 + A)', 
= 2
r 
- 1 (r -
I)! (k -
I) [1 + fA + ( ; ) A 2 + ... ] . 
(2.12.20) 
Thus, to order maXi V;-I , the cumulanlS in (2.12. I 8) and (2. I 2.20) are identical. 
It follows that, to this degree of approximation, 
J 
-210g Wo } 
Pr{p(<I> ly»p(<I>oly)ly } =Pr\X:-I < 
I+A 
. 
(2.12.21) 
. 
. 
. 
_.-.: ....... 

136 
Standard "Iormal Theory Inference Problems 
2.12 
This diA'ers from (2.12.17) by the factor (I + ;I) which can be thought of as the 
first order adjustment. The distribution may be approximated to greater accuracy 
lsee for example Hartley (1940), Brookner and Wald (1941), and Box (1949)]. We 
shall not, however, discuss these methods here because Bartlett's approximation, 
which is easy to apply, appears adequate even for rather small sample sizes. 
2. n.3 Bayesian Parallel to Bartlett's Test 
We have seen that the sample quantity employed in deciding whether the point 
<P = 0, that is, «(J~ = ". = (J;) , lies inside or outside a H.P.D. region is the same 
as Bartlett's criterion for testing homogeneity of variances developed in sampling 
theory. The distribution theory is also parallel, and the procedure for deciding 
whether or not <Po = 0 is included in the (1 -
:x) H.P.D. region for 11>, is identical 
with Bartlett's test at level !Y.. This correspondence arises because the sampling 
distri bution of sf I (J? , wi th s} a random varia ble and a} fixed, is identical to the 
posterior distribution of s;. (J;2, with Sj2 fixed and (J; the random variable, under 
the assumption that OJ and Jog (Jj are uniform a priori. 
A serious difficulty in the practical use of Bartlett's criterion in the context 
of sampling theory is its extreme sensitivity to non-~ormality [Box (1953a, b)]. 
For samples from non-Normal populations with kurtosis i'2 = 
1\4 / 1\~, for instance, 
Mo is asymptotically distributed as (I + 'h /2);.t _ I and not as;d _ ,. In this chapter 
Bayesian procedures are derived under the same assumptions as are customary 
in the sampling approach. Later in Chapter 4 we turn to the problem of comparing 
the spread of distributions when 0Jormality cannot be assumed. 
2.12.4 An Example 
As an example of the Normal theory procedure, SLI~pose an investigation conducted 
to compare the spread of three distributions yields 
s~ = 52.785, 
s~ = 34.457, 
S5 = 66.030, 
From which 
.52 = 51. 091 , 
and 
v = 90. 
L.:nder the assumptions of Normality and noninformative priors, the joint distri-
bution of the two contrasts 
and 
cPz = log (J~ -
log (J~ 
is, from (2.12.6), 
p(cPl , <Pl l y) = 
[;(~~;J3 Ti sn 5 (1 + T, + T2)-45 , 

, 
. 
-
. 
' 
, 
. -_. '-".~ 
- ...... ~ 
_.-
2.12 
where 
Comparison of the Spread of k Distributions 
2 
TI = 
V1 S~ e- 1>, = 0.79941 e - 4>" 
v3 S 3 
137 
Using (2.12.7), the mode of the distribution is 
¢I = log s; -
log s5 = -0.2239, 
• 
2 
2 
cP2 = log S2 -
log S3 = 
- 0,6504. 
Figure 2,12.1 shows the mode and contours of the approximate 75, 90, and 95 
per cent H.P, D. regions, 
Making use of Bartlett's approximation , the contours 
were drawn so that 
tv! = -2log W = (1 + A)/(2, o:) 
, for CI. = (0.25, 0.10, 0.05), respectively, For this example, we have from (2,12, 10) 
and (2,12,12) 
tv! = -9010g3 + 9010g{1 -r exp[ -(cPl -l- 0.2239)J + exp [-(</)2 + 0,6504)J} 
+ 30[(cPl + 0,2239) + (cP2 + 0,6504)], 
A = (-t)(~o -
9'0 ) = 0,0148, 
and from Table III (at the end of this book), x2(2, 0,25) = 2,77, x2 (2, 0,10) = 4,61, 
and X2(2, 0.05) = 5,99, 
The contours are nearly elliptical as might be expected 
because, for samples of this size, cPl and cP2 will be roughly jointly Normally 
distributed, 
Also shown in the figure is the point cj> = 0 corresponding to a~ = ai = O'~ 
For this example, <j> = 0 is included in the 90 per cent region but excluded from 
the 75 per cent region. Of course, if we were merely interested in deciding whether 
<j> = 0 lies inside a particular H,P,D, region, we could employ (2.12.13) directly 
and obtain 
3 
- 210g Wo = - L Vi (log s; -
log 52) = 3,1434, 
i= I 
so that 
-210gWo 
3,J434 
1 + A 
= 1.0148 = 3,098, 
which falls between /(2,0,25) and /(2,0,10). Thus, we woulJ know straight 
away that <!> = 0 is included in the 90 per cent regio n but excluded from the 75 
per cent region without recourse to the diagram, 
By consulting a table of l probabilities, we can also determine the approximate 
content of the H.P.D, region which just covers a particular parameter point 

---
. 
138 
Standard ;\Jormal Theory Inference Problems 
2.13 
0.35 
-0.65 
-1.65 
~ 
__________ ~ 
______ ~ 
____ ~ 
____ 
L-~ 
____ ~ 
______ ~ 
__ ¢I~ 
- 1.5 
-/.O 
-O.S 
0 
0.5 
1.0 
Fig.2.12.1 Contours of posterior distribution of (<PI' <P2). 
<1> = <1>0· For instance, for the point <1> = 0, using the Bartlett approximation , we 
obtain 
Pr{p(<1> I y) > peO I y) I y} = Pr{/.~ < 3.098} = 0.79. 
2.13 SL'MMARIZED CALCULATIONS OF V ARIOL'S 
POSTERIOR D1STRIBL'T10NS 
Table 2.13.1 provides a summary of the ~ormaJ theory posterior distributions, 
based on noninformative priors, for making inferences about various location 
and scale parameters. The examples in most cases are those discussed in this 
chapter. 
Table 2.13.1 
Summarized calculations of various Normal theory [losterior distributions 
I. Normal meall 0, variance 0' 2 known 
From (22.4), 
J 
Y = -Iy. 
/1 
) 

2.13 
Summarized Calculations of Various Posterior Distributions 
Table 2.13.1 Continued 
The (I -
a) H.P.D. interval of e is 
_
. 0 
a 
y -
lIa/ 2 -= < 8 < ji + ua/ 2 ~, 
n 
n 
139 
where u./2 can be obtained from Table I (at the end of this book). Thus, for 
(ji = 50, a
2 = 20, 11 = 20), then 8 "" N(50,J). 
For a = 0.05, uO.02S = 1.96 == 2 
and the 95% H.P.D. interval is (48 < () < 52). 
2. Normal variance a Z, 8 known. 
From (2.3.6), 
2 
J 
2 
S 
= -
L (Yj -
8) . 
n 
The standardized (I -
a) H .P.D. interval is obtained in terms of the metric loga, 
and the corresponding interval in a 2 is 
ns2 
I1S2 
--- < 0 2 < -z--
, 
22(n, a) 
~ (n, IX) 
where ~2(n, a) and i(n, a) are given in Table II (at the end of this book). From this 
suppose (n = 20, 
I1Sz = 348, 
S2 = 17.4), then a 2 ~ 348X2~' 
For 
IX = 0.05, 
(~2 = 9.96, X2 = 35.23) and the 95% H.P.D. interval is (9.88 < a 2 < 34.94). 
3. Normal variance 0 2,8 unknown. 
From (2.4.19), 
OZ ~ (VS2)X;2, 
v = /I -
I , 
and 
As in the case 8 is known, the standardized (I -
a) H.P.D. interval is obtained in 
terms of log 0, and the corresponding interval in a Z is 
Vs 2 
2 
Vs 2 
-Z--<o <~2--' 
X (v,a) 
'! (v,a) 
4. Normal mean 8, 0 2 unknown. 
From (2.4.21), 
The (I -
Gi) H.P.D. interval of 8 is 
s 
S 
ji -
'a/ Z(v)-;-- < (j < ji + /a/2 (V)" 
vII 
v n 

---
140 
Standard "'ormal Theory Inference Problems 
2.13 
Table 2.13.1 Conlil/lled 
where la/2(V) is given in Table IV (at the end of this book). 
Thus, for 
(y = 50, 
sl.J" = 0.96, 
v = 19), 
then 
8 ~ 1(50,0.92,19). 
For a = 0.05, 
IO.02S(19) = 2.09 so that the 95% H.P.D. interval is (47.99 < 8 < 52.01). 
5. Differel/ce ollwo Normal means y = 82 - 8 I' when the variances ()i and ()~ are 
unknown but equal. 
From (2.5.3), 
2 
- I '<" (y 
- )2 
51 = VI 
L-
Ij -
Y1 
' 
and 
The (I -
ex) H.P.D. interval of',' is 
I 1 
1 ) 1/ 2 
Y2 - YI -
1./ 2 (11 1 + n2 -
2) s 1- -I- -
< 'i 
\ 111 
11 2 
For example, Y2 - 5'1 = 5, 
n) = 112 = 10, 52 = 4; then, 'I ~ 1[5, }, 18]. 
For 
a = 0.05,10 .025 ( 18) = 2.10 and the 95% H.P.D. interval is (3.12 < Y < 6.88). 
6. Difference oltwo Normal means'}' = 82 - e 
I ' when ()~ and ()~ are unequal and unknown 
From (2.5.13), 
where 
b = 4 17 
+ 12' 
(
b - 2) 
.-b- II' 
( _V_2 _) cos 2 cp + (_V_I_) sin2 cp, 
v1 - 2 
VI - 2 
and 
The (I -
a) H.P.D. interval is, approximately, 

......
. 
2.13 
Summarized Calculations of Various Posterior Distributions 
141 
Table 2.13.1 Continued 
Thus, for Y2 -
5'1 = 5, "I = 20, "2 = 12, s~ = 12, and s~ = 40, we find 
/1 = 1.2063. 
12 = 0.1552, 
h = 13.376, 
and 
(12 = 1.026, 
so that 
y "'- /(5,4.035, 1338). 
For a = 0.05,/0025 (13.38) == 2.15 and the 95~:, H.P.D. intt.:rval is approximately 
(0.67 < Y < 9.33). 
7. The ralio oj IWO 'l/ol'm(11 pariallees O'~ 'O'L whell 01 alld O2 are UllkIlOWIl. 
From (2.6. I), 
The standardized (I -
Ci) H. P. D. interval is expressed in terms of log 0' 2 -
log 0' I' 
and the corresponding interval in O'~/O'~ is 
where F(v" 
V2 ,Ci) and F( I'I' \' 2,Ci) are given in Tab le V (at the end of this book). 
Thus, for ~~=12, s~=40, "I =20, "2 =
12, then 
0'~
'0' ~~(3.33) F (19,11)' 
For 
r.t = 0.05, F(19, II, 0.05) = 0.352, F(19, 
I I, 0.05) = 3.15, and the 
95 ~~ H.P.D. 
interval- is (1.172 < O'i/O'~ < 10.490). 
8. The regression coefficients 8 o/Ihe Normal lillear model, willl 0'2 unknown. 
First, from (2.7.1) and (2.7.8) comput (; X' X, C = (X'X)-I 
{) = (X'Xr IX'y, 
where 
y = X6 
and 
v = II -
k. 
From (2.7.20), 
The (I -
ex) H. P. D. region of e is given by 
(8 -
I'h'( X ' X)(8 - 6) < F(k, \',a) , 
--~--

142 
- - ---
-
Standard Normal Theory Inference Problems 
2.13 
Table 2.13.1 Conlinlled 
where F(k, I',:X) is igven in Table VI (at the end of this book). For inferences about a 
subset of e, let 
c = [~~: 
~~:]. 
Then from (2.7.22), 
For inferences concerning the ith individual element of e, i = 1, .. . , k, 
where Cjj is the ith diagonal element of C. The ( I -::I.) H .P.D. interval of OJ is 
Thus, for k = 2, 11 = 18, 
c = [ 
0.168 
- 0.074 ] 
- 0.074 
0.095' 
9 = [ 98.9] 
124.4 
' 
and for v = 16 and 52 = 157.8, 
e =
[ O I] ~1 ( [98.9] 1578[ 
0.168 
-0.074] 
'6) 
O2 
21 
124.4
' 
. 
-0.074 
0.095' 1 I' 
For <J. = 0.05, F(2, 16, 0.05) = 3.63 and the 95 ~u H. P. D. region ofe is given by 
Q({) 
9(0 1 -
98.9)2 + 14(0 1 -
9S.9)(Oz -
124.4) + 16(8z -
124.4)2 
-2 = 
-
< 3.63. 
2s 
2 x 157.8 
As an example, for the point eo = (120, 120), 
Q(eo) 
3015 
--z = -
-
= 9.56 > 3.63, 
2s 
315.6 
and therefore eo is excluded from the 957~ region. For individual inferences 
01 ~ 1(98.9, 157.8xO.168, 16), 
O2 ~ 1(124.4, 157.8 x O.095, 16), 
and for 
fY. = 0.05, 'O.02S( 16) = 2. 12 so that the individual 95% intervals are 
(88.25 < 01 < 109.55) and (116.41 <02 < 132.39). 

--
--~ 
2.13 
Summarized ' :alculations of Various Posterior Distributions 
43 
Table 2.13.1 COIl/inlled 
9. Comparisoll nf k Norll/al II/eans ai' ... ,0" wi/Ii comlllOIl IIl1knolVll unrinllce 0 2. 
From (2.11.10), the (I -:;() H.P.f). region of the (k -
I) linearly independent 
contrasts 
¢i=Oi-O, i=I,,,,k-l, 
I ' 
lJ = - Ln.D 
n 
J J' 
is given by 
2..:k n[¢ -
(ji -
\,)J2 
I 
I 
,. 
< F(k _ I, \', a), 
(k -
I) S2 
where 
I 
Y = -
Lil y 
11 
) J' 
S2 = 1,- 1 L 2..: (Yij - Y/, 
i 
j 
and 
v = II -k. 
In particular the point <PI = 
(1 -
el) region if and only if 
= ¢. _ I = 0 (i.e., 01 = ... = Ok) is included in the 
Lk nj(Y; _ y)2 
- ---"'"z-
< F(k -
I , V,:;(). 
(k -
I) s 
For example, suppose k = 3, "I = 10, 1/ 2 = 10, 113 = 12, J;I ,-" 8, Yz = 10,.h = 7, 
and S2 = 9; then j i = 8.25 and L 17;(Y; -
y)2 = 50. For CJ. = 0.05, F(2, 29, 0.05) = 
3.33 and i ~ == 2.8 so that the point <PI = <P2 = 0 (i.e., 01 = O2 = 03 ) is included in 
the 95 ~~ H .P.D. region. 
10. C ol11pariSOIl 0/ k Normal L'Oriallces 0 i, . . ,01, lVi/h /llIk nOW/1 mealls 0 I' .. . , Ok' 
From (2.12.11) to (2.12.21 ), for the (k -
I ) contrasts 
¢i = log 0-; - log 0';, 
i = I, ... ,1< -
I, 
the H.P.D. region of content approximately (I -
el) is given by 
where 
-2logW 
~--- < /(k -
I, el), 
I 
A 
-210gW = VIOg( ~v
'k) + I'log [I + ki l 
~ exp[-(¢; - ¢i)]] 
i 
1 Vk 
\'i = Jli -I, 
¢i = logs; -
log sf, 
k-I 
T L vM; - ¢;l, 
i " r 
v = LVi' 
i=1 

144 
Standard ~ormal Theory Inference Problems 
A2.1 
Table 2.13.1 Continued 
and /(k -
I, a) is given in Table Iff (at the end of this book). [n particular, for 
1>, = 0 (i = I , ... , k -
I) (i.e., O'~ = ... = O't), 
k 
- 2 log W = - L v,(log s? - log 52), 
i= t 
where 
.S2 = V-I L ViS?, 
i= I 
Thus, for k = 3, VI = "2 = " ) = 30, s~ = 52.79, si = 34.46, s; = 66.03, we have 
v = 90, 52 = 51.09, A = 0.015, ~I = -0.22, (j;2 = -0.65, a nd 
-210gW 
l+A 
(-90 log 3 + 90 [I + exp [- (<PI + 0.22)J + exp [- (<Pz + 0.65)JJ 
+ 30 [(<PI + 0.22) + (<P2 + 0.65)J} ( 1.015. 
For tJ. = 0.05, /(2,0.05) = 5.99 so that the 95% H.P.D. region is given by 
_ 210g W i( I + A) < 5.99. [n particular, for 1>1 = <P2 = 0 (a~ = O'i = O'~) 
-21ogW 
I+A 
- I.,o: 1 vi( lOgs; -log52) 
3.143 
= -
-
= 3.098, 
I+A 
1.015 
so that the point <P t = 1>2 = 0 is included in the 95% region. 
- ---------------
- ---- --------
APPENDIX A2.1 
Some Useful Integrals 
We here give several integral formulae which are useful in the derivation of a 
number of distributions discussed in this book. 
The Gamma, Inverted Gamma, and Related Integrals 
For a > 0 , p > 0 , 
CA2.1.1) 
CA2.1.2) 
(A2.1.3) 

A2.1 
Appendix 
145 
and 
(A2.IA) 
More generally, for a > 0, P > 0, and (J. > 0, 
I" p- ! '- ax ' I 
I 
- P11r( P ) 
x 
e 
ex = -a 
-
, 
o 
ex 
ex 
(A2. I.S) 
and 
I
'" 
- (p + l) - a·'-'d 
I -P1ar(P) 
X 
e 
x = -a 
-. 
o 
'Y. 
ex 
(A2.1.6) 
The Dirichlet Integral 
For Ps > 0, S = I , .... 11 + I, and 11 = 1, 2, .. the integral 
XP1 - 1 
p"-!(I 
r 
)P", ,-Idx: 
d 
= 
s=1 
Ps 
CA2.1.7) 
J 
I1
"
.~ I r( ) 
R I 
.. . XII 
-
- I -
.. . -
X" 
- I .. . x" 
ro=~ ; : PsJ ' 
where R: (xs > 0, L;= I Xs < I), is known as the Dirichlet integral. Alternatively. 
it can be expressed in the inverted form 
The Normal Integrals 
For - 00 < '1 < 00 , C > 0, 
f
cc 
[ 
I (X - '1) 2] 
_ ",exp 
- 2' - c-
dx = J 2n c. 
(A2.1.9) 
Let TJ be a 17 x 
vector of constants and C a 17 x 11 positive dennite symmetric 
matrix. Then 
f
ro 
f O'-
.. . 
exp[-}ex -
TJ),C-I(x -lJ)Jdx l .. . dx" = (,/ 2rr)"ICI! /2, 
-
w 
-
00 
CA2.!.IO) 
where x = (XI' ... , XII)" 
The t Integrals 
For II > 0, 
f_
OO
. oo [I + (X -c '1)2//v]-tI
L 
1) 
rmrc{v)
, 
dx= r[-Hv + I)J 'V vc, 
CA2.1.11) 

.. 
-
J46 
Standard ~ormal Theory Inference Problems 
A2.2 
where '1 and c are defined in (A2.1.9). For v > 0, 
J
'" 
J'" [ 
(X-ll)'C-1(X-TJ)]- ;(v+lI) 
... 
I + 
dX 1 ... dXn 
-
::0 
-
Cf 
V 
= [fCl)]'T(}v) (v v)"ICl 12 
f[1Cv + I1)J 
' 
(A2.1.12) 
where 11, C , and x are defined in (A2.1.10). 
APPENDIX A2.2 
Stirling's Series 
To approximate the Gamma function, we often employ an asymptotic series 
discovered by Stirling, and later investigated by Bayes [see Milne-Thomson (1960)J 
The expansion involves an interesting class of polynomials discovered by D. 
Bernoulli. 
Bernoulli Polynomials 
The Bernoulli polynomial of degree r, BrV), is generated by the following 
function, 
te
X
' 
'" t' 
-,- = I -Br(x), 
e -
I 
r=O r! 
Upon equating coefficients, we find 
BoCx) = I 
B 1 (x) = (x -
}) 
BJ(x) = x(x -
l)(x -
t) 
(A2.2.1) 
(A2.2.2) 
The Bernoulli numbers Br are generated by setting x = 0 in (A2.2.1), 
1 
_ 
\. tr 
-,-- -
L., -
B" 
e -
1 
r=O r! 
CA2.2.3) 
In particular BI 
1. 
Adding 1/2 to both sides of (A2.2.3), we have 
t 
(e' + I) 
I 
~ I r 
---=-+ \ - -
B 
2 (e' -
I) 
2 
/:"0 r! 
r' 
(A2.2.4) 

A2.2 
Appendix 
147 
Since the left of (A2.2.4) is an even function of f, it follows that 
p = 1,2, ... . 
(A2.2.S) 
It can also be shown that 
(-l)P I B2p > 0 
(that is, B 2 " alternates in sign) 
(A2.2.6) 
and IB 2,,1 tends to infinity as P --+ 00 . The first few Bernoulli numbers are 
Bo = I, 
BI = 
Stirling's Series 
I 
-"30 ' 
Bs = 0, 
(A2.2.7) 
Stirling's series provides an asymptotic expansion of the logarithm of the Gamma 
function rep + h) which is asymptotic in p for bounded h. We have 
log rep + h) = 
~ log (211) + fJ logp - p + BI (h) logp 
" 
B 
(h) 
- L (-I)' 
,.,.1 
,+R,,(p), 
, 
I 
r(r + I)p 
(A2.2.8) 
where B,(h) is the Bernoulli polynomial of degree r given in (A2.2.1). The remainder 
term R,,(p) is such that 
IR ( )1 _ _ c,,_ 
"p - Ipl"', l' 
CA2.2.9) 
where C" is some positive constant independent of p. The series in (A2.2.8) is an 
asymptotic series in the sense that 
lim IR"(p)p" l = 0 
(11 fixed) 
Ipl- :c 
and 
(A2.2.lO) 
lim IR"(p)p"; = ex:. 
(p fixed) . 
Thus, even though the series in (A2.2.8) diverges for fixed p , it can be used to 
approximate log rep + /7) when p is large. Setting h = 0, we obtain 
log rep) :;= -~ log (211) + (p - 1) logp - p 
" 
B 
+ I 
2, 
2 
1 + R;,(p), 
, = J 2r(2r _ I)p , 
(A2.2.11) 
where use is made of the fact that B2, + I = O. It can be shown that the remainder 
term R;, (p) satisfies the inequality 
R'()I":::: 
I. 
IB
2 (" + I ' \ 
(A2212) 
, "p '" 2(n + 1)(2n -l- I) 
p2"+J 
. 
. . 

. --
148 
Standard Normal Theory Inference Problems 
A2.2 
This, in conjunction with the fact that B 2(n+ 1) alternates in sign, impJies that, for 
positive p , the value of log f(p) always lies between the sum of n terms and 
the sum of 11 + I terms of the series, and that the absolute error of the series is 
less than the first term neglected. Even though (A2.2.11) is an asymptotic series, 
surprisingly close approximations can be obtained with it, even for small values 
of p. Taking the exponential of the log series, we obtain 
571 
) 
248320p4 '" 
, 
(A2.2.13) 
which is known as Stirling's series for the Gamma function . 

CHAPTER 
3 
BA YESIAN ASSESSMENT OF ASSUM PTIONS 
I. EFFECT OF NON-NORMALITY ON 
INFERENCES ABOUT A POPULATION MEAN 
WITH GENERALIZATIONS 
3.1 INTRODGCTlON 
In Chapter 2 a number of conventional problems in statistical inference were 
considered. 
Although interpretation was different, in most cases the Bayesian 
results closely paralleled those from sampling theory.t 
Whichever mode of 
inferential reasoning was adopted, certain assumptions were necessary in deriving 
these results. Speciflcally, it was assumed that observations could be regarded as 
normally and independently distributed . Although at flrst sight this assumption 
seems restrictive, abundant evidence has accumulated over the years that the 
basic results are of great usefulness in the actual conduct of scientific investigation. 
Nevertheless, exploration in new directions is desirable but has tended to be 
confined by the technical limitations of sampling theory. I n particular, develop-
ment along this route becomes unwieldy unless a set of minimal sufficient statistics 
happens to exist for the parameters considered. 
The Bayesian approach is not restricted in this way, and a principal object of 
our book is to explore some of the ways in which this Aexibility may be put to use. :!: 
I n particular, the consequences of relaxi ng conventional assumptions may be studied. 
In the present chapter the assumption of Normality is relaxed. 
Specifically 
inferential problems about means and regression coefficients are considered for a 
broader class of distributions which includes the Normal as a special case. 
3.1.1 Measures of Distributional Shape, Describing Certain Types of non-Normality 
Following Fisher, let K j be the jth cumulant of the distribution under study. 
Then /(1 = J1 is the mean, and 1(2 = a 2 is the variance. 
Since for the Normal 
distribution the K j are zero for all j > 2, each scale-free quantity 
j = 3,4, ... , 
(3. 1.1 ) 
measures some aspect of non-Normality. The measures 1'1 and 1'2, flrst considered 
by Karl Pearson, are of special interest. 
T The Behrens- Fisher problem was one instance in which the Bayesian result had no 
-ampiIng parallel. 
~ Mosteller & Wallace ([964) provide other interesting illustrations of the use to which 
this freedom can be put. 
149 

150 
Bayesian Assessment of Assumptions 1 
3.1 
Skewness 
The standardized third cumulant, 
"., 
E(v -
/-1)3 
Y =--= ----
I 
I(~ .2 
a J
' 
(31.2) 
provides a measure of skewness of the distribution. Thus in Fig. 3.1.1. (I IS zero 
for the symmetric distribution seen at the center of the diagram ; it is negative 
for the distribution to the left. which is said to be negatively skewed; it is 
positive for the distribution to the right, which is said to be positively skewed. 
"II <0 
Negutivdy skl.:w 
"II cO 
SYlllllll..:trit: 
"II :· 0 
Posilivcly ,k~w 
Fig. 3.1.1 SymmetriC and skewed distributions. 
K urlosis 
The standardiLed fourth cumulant, 
"'4 
E(r -
II)" 
Y2 = -
= 
.j 
-
3, 
/(~ 
a 
measures a characteristic of the distribution called kurtosis. 
(3.1.3) 
For the Normal distribution Y2 = 0. If}'2 > 0, the distribution is said to be 
/eptokurtic. 
If Y2 < 0, it is said to be p/Qlykurlic. 
The Normal distribution is 
contrasted with a symmetric leptokurtic and a symmetric platykurtic distribution 
in Fig 3.1.2. Typically a leptokurtic distribution has less pronounced "shoulders" 
and heavier tails than the Normal. For instance, Student's t distribution is mark-
edly leptokurtic when the number of degrees of freedom is small. On the other 
hand, a platykurtic distribution typically has squarer shoulders and lighter tails. 
As an example, the rectangular distribution is highly platykurtic. 
. 1'2 <0 
Plary kllrti c 
1'2 = 0 
Normal 
1'2 >0 
LL'plokurti( 
Fig. 3.1.2 The l'ol'lnal distribution contrasted with a symmetric platykurtic distribution 
and a symmelric leptokurtic distribution. 

3.1 
Introduction 
151 
Of course. distributions can exhibit kurtosis and skewness at the same lime. 
Thus the -/ distribution is kptokurtic and positively skewed (especially when 
the number of degrees of freedom is small). 
3.1.2 Situations where "Iormality would not be Expected 
As we have said in Section 2.1, one can expect the distributions of the observa-
tions to be approximately Normal when the experimental conditions are such as 
to produce a central limit tendency, that is to say, when the errors arise from a 
variety of independent sources. none of which are dominant. 
Nevertheless we would expect that certain kinds of measurements would not 
be Normally distributed. An example is yarn breaking strength. If the yarn is 
thought of as being made up of a number of links (like a chain), with the break 
occurring at the weakest link, and if the distribution of the strength of an individual 
link was Normal, the breaking strength will be distributed like the distribution of 
the smallest observation from a i'I 0 fill a I sample. This extreme value distribution 
is skewed and highly leptokurtic. 
However, this does not mean that the Normal assumption is unrealistic for all 
experiments where the data are breaking strengths. 
Consider again the experiment 
described in Section 2.5 in which two different types of spinning machines were compared 
The non-Normal error contributed by breaking strength measurement might be a minor 
contribulor to the overall error; for Ihis would usually be dominated by machine 
differences and sampling errors. The many components associated with these dominant 
sources would be likely to produce a central limit tendency and approximate Normality 
might be expected in this example, even though the data analyzed were breaking strengths. 
Platykurtic distributions for individual errors can also occur. For example, 
suppose that in successive runs of a chemical apparatus, temperature varied 
about a set point. The experimenter might accept minor variations, but when a 
larger deviation occurred the run might be abandoned and a new run substituted. 
While the resulting "truncation" could lead to a platykurtic distribution, again, 
when other sources of error are present, this component could be swamped by 
the effect of the other errors. 
The investigator will rarely be In the position where he can be certain of the 
precise form of the overall error distribution. Rather, his opinion may be described 
by a distribution of distributions centered about some central distribution. His 
state of mind will depend on the experimental setup and situations will some-
times occur where he expects a dominant non-Normal source of variation to 
determine the overall error distribution . More frequently, the setup will be one 
where he expects a finite number of sources each to have an important role. 
While in these circumstances the central limit theorem, which describes an asymp-
totic property, does not allow him to assume exact ~ormality, it does provide a 
basis for thinking of his distribution of distributions as cOllcentrated about the 

152 
Bayesian Assessment of Assumptions 1 
3.2 
~ortnal. Furthermore, he is sometime concerned only with differences between 
observations drawn from distributions supposed similar except in location. 
It 
may then be assumed that the parent distribution of these differences is a member 
of a class of symmetric distributions clustered about the Normal. 
In this chapter we explore the use of a class of symmetric distributions with 
kurtosis measured by a non-Normality parameter f3 which takes the value zero 
for the "Jormal distribution. The state of uncertainty about the parent distribu-
tion can then be expressed by giVing to f3 an informative unimodal prior probability 
distribution centered at zero. The sharpness of this prior distribution can be varied 
so that the effect of varying degrees of uncertainty about "Jormality can be repre-
sented. In particular, when the rrior distribution becomes a (j function at r~ ~. 0, 
exact >Jormality is assumed. This is an illustration of the manner in which the 
Bayesian approach can be used to assess the effect of uncertainty in assumptions. 
3.2 CRITERION ROBlSfi'ESS AND INFERE~CE ROBLST1\;ESS 
ILLLSTRATED USING DARWIVS DATA 
On sampling theory, once the data-generating model is assumed, criteria appro-
priate to that assumption can be derived for inferential purposes. For example, 
suppose the observations)'I' .. ,),,, are regarded as a random sample from a Normal 
population'V(O, (J2) and it is desired to make inferences about rhe ml!an O. Then, 
if (J is unknown. the usual criterion is the I statistic 
jI -O 
s/Jn . 
Apart from the sample size 11, thi~ criterion involves the data only via the sample 
mean y and the sample standard deviation s = [(/1 -
I)" 1 L (y -
ji)2} , which 
are jointly sufficient for (0, (J). Inferences about 0 are then based on the sampling 
distribution of I, assuming a Normal parent distribution . 
It is customary to 
justify the use of such a Normal theory criterion in the practical circumstance in 
which Normality cannot be guaranteed, by arguing that the distribution of this 
t ailerion is but little affected by moderatl! non-Normality in the parent distri-
bution- that is, it is robust under non-l\ormality. We shall refer to this type of 
insensitivity as criterion robustness under non-I\ ormality. 
This argument, however, does not take into account the fact that if the parent 
distribution really differed from the "Jormal, the appropriate criterion would no 
longer be the Normal theory statistic. For Instance, suppose it was known that 
the parent distribution was rectangular (uniform); then the same sampling theory 
arguments previously leading to the t criterion would show that inferences were 
best made using the criterion 

3.2 
Criterion Robustness and Inference Robustness 
153 
which now involves the data only via Y(>I) and Y( I), the largest and the smallest 
observations in the sample. 
Thus, on the assumption that the sample comes 
from a rectangular distribution, inferences about 8 ought to be based not on the 
distribution of t but on the distribution of W. 
The example which follows shows that although the distribution of the Nor-
mal theory t criterion is not changed very much by assuming the parent to have 
some distribution other than the Normal, the inference to be drawn from a particular 
sample can be markedly different when we employ a criterion appropriate to this 
other distribution. To distinguish it from criterion robustness, the property of 
insensitivity of inferences to departures from assumptions we shall call inference 
robustness. 
Darwin's Data 
Consider the analysis of Darwin's data on the difference in heights of self- anq 
cross-fertilized plants quoted by Fisher (1960, p. 37). The data consists of measure-
ments on J 5 pairs of plants. 
Each pair contained a self-fertilized and a cross-
fertilized plant grown in the same pot and from the same seed. Following Fisher, 
we shall treat as our observations the 15 differences Yi (i = I, ... ,15), which are 
set out in Table 3.2.1 and plotted below the horizontal axis in Fig. 3.2.1. 
Table 3.2.1 
Darwin's data: differences (in eighths of an inch) of heights of 15 pairs of 
self- and cross-fertilized plants 
49 
23 
24 
-67 
28 
75 
8 
41 
60 
16 
14 
-48 
6 
56 
29 
On sampling theory, given that the differences are a random sample from a 
Normal parent population N(O, (12), one should interpret these data using the 
paired t test. 
In particular, for a significance test appropriate to the hypothesis 
that e = 0 against the alternative 8 > 0, the associated significance level is 2.485 %. 
The curve on the right in Fig. 3.2.1 is an appropriately scaled t distribution with 
14 degrees of freedom centered about y = 20.933, with scale factor sf, n = 9.746, 
where S2 = L(Yi - yf(n - I) = 1,424.6. 
For definiteness, we shall call this 
distribution the reference distribution for O. 
This reference distribution may be 
variously interpreted. It was regarded by Fisher as the fiducial distribution of O. 
It is also a "confidence distribution" simultaneously allowing every confidence 
interval to be associqted with its appropriate confidence coefficient. 
Finally, if 
as in Section 2.4, we make the assumption that 0 and log (J are approximately 
independent and locally uniform a priori, it is the posterior distribution of 8. 
Now suppose that instead of assuming Normality for the parent distribution, 
we supposed it to be uniform over some unknown range 0 -
j <1 to 0 + J j <1. 

154 
Bayesian Assessment of Assumptions I 
0.06 
0.04 
o.O} 
Rec! angular 
parent 
Mid· 
point 
! 
3.2 
Mean 
Fig.3.2.1 Distributions of (J for Darwin's data: Norma l and rectangular parent 
distributions (dots under the horizontal axis are the 15 differences in height recorded by 
Darwin). 
Such a supposition would, of course, be quite ridiculous in the present example. 
First, we know that many contributing errors arising from genetic differences, 
soil differences, and so forth , tend to produce a central limit effect, so that we 
can expect with good reason that the heights themselves and, even more, their 
differences will be closely Normally distributed. 
Second, the evidence from the 
sample itself does not support the uniform assumption. 
However, for illustration, let us make the assumption of a rectangular instead 
of a Normal parent and let us consider the effect of this extreme degree of non-
Normality on the distribution of the (statistic. This can be approximately cal-
culated using, for example, the work of Geary (1936), Gayen (1949, 1950), or of 
Box and Andersen (1955). Following these latter authors, it can be shown that, 
when the parent is non-Normal , the null distribution of (2 is approximated by 

3.2 
Criterion Robustness and Inference Robustness 
ISS 
an F distribution with band 8(n -
1) degrees of freedom, where 
8 = I + £(b -
3) . 
n 
with 
(n + 2) L Y~ 
b=-----· 
(2.y;/ 
(3.2. J) 
£(b -
3) == Y2 -
11-
1(2Y4 -
3y~ + IIY2) + 11 
2(3Y6 -
16Y4/2 + J5y~ 
+ 38Y4 -
3y~ + 86yz), 
r = 3,4, ... , 
and the Kr'S are the cumulants of the parent distribution of the differences. 
In 
our present example, b is found to be 0.913. Thus, /2 is approximately distributed 
as F with 0.913 and 12.78 (instead of 1 and 14) degrees offreedom. In particular, 
the significance level associated with the hypothesis that e = 0 against the alter-
native e > 0 is now 2.388 % as compared with the previous value of 2.485 %. 
The test of the hYP0thesis that the true difference is zero, using the t criterion, is 
thus very little affected by this major departure from Normality. Furthermore, 
confidence intervals and hence the co nfidence distribution, based on the / statistic, 
would be almost unchanged. 
However, if we reall y kne\V that the parent distribution was rectangular, then 
the largest observation )'(1/) and the smallest observation Y(I) would be jointly 
sufficient for (8, 0") ; and, as mentioned earlier, we would, on classical sampling 
theory, be led to consider not the / criterion but the function 
m - 0 
W=-- -
17/(11 -
I) ' 
(3.2.2a) 
where m = ~(YII/) + YI I» is the midpoint, and 17 = HY(I/) -
Y(1» is the half range. 
On the assumption that the parent is rectangular, the variate W is distributed as 
I ( 
IWI ) - ., 
P(W)=2
1+;-=-( 
, 
- oo< W<oo, 
(3.2.2b) 
[Neyman and Pearson (1928); Carlton (l946)J. Thus, the va riate I W I has the F 
distribution with 2 and 2(n -
I) degrees of freedom. The distribution of W in 
(3.2.2b) may be called a "d ouble F distribution" with [2, 2(n - I)J degrees of 
freedom, since it consists of two such F distributions standing "back to back." 

156 
Bayesian Assessment of Assumptions 1 
3.2 
The curve on the left in Fig. 3.2.1 is an appropriately scaled double F distri-
bution with (2,28) degrees of freedom centered at the sample midpoint, m = 4.0 
with scale factor h/(n - I) = 5.07, where the sample half range is h = 71. Just 
as the right-hand curve in the figure exemplifies the inferential situation with the 
Normal assumption , so the curve on the left correspondingly exemplifies the 
situation with the rectangular assumption. As before, it can be interpreted either 
as a fiducial distribution , as a confidence distribution , or, on parallel assumptions 
to those previously used , as a posterior distribution. It is seen to be very different 
from the one appropriate to the Normal assumption. In particular, the sampling 
theory significance level associated with the hypothesis that () = 0 against the 
alternative () > 0 is now not 2.485 %, but 23.2 I 5 %. Thus, whichever form of 
derivation we favor, we see thaL if we assume a rectangular parent distribution , 
the inferences to be drawn are very different from those appropriate for a Normal 
parent. This is so even though the I criterion itself is very little affected by even 
this large departure from Normality. 
One reason for the large difference of the distributions in Fig. 3.2.1 is that 
one curve is Lentered at the sample mean y = 20.9, and the other at the sample 
midpoint m = 4.0. The mean and the midpoint for Darwin's data are very different 
mainly because of two rather large negative differences, and for this data we have 
an example in which the criterion is robust but the inference is not. 
As we have explained, it is not seriously suggested that the rectangular distri-
bution is a reasonable choice for the parent. 
We wish only to emphasize that 
uncertainty in our knowledge of the parent distribution transmits itself rather 
forcefully into an uncertainty about the inferences we can make concerning e, 
and the difficulty which this presents in our interpretation of the data is not· 
avoided by knowledge of robustness under non-Normality of the criterion. 
The difficulty can be resol ved by making provision for an appropriate state of 
uncertainty about the parent distribution in the formulation. Possible knowledge 
about the parent distribution is of two kinds, that coming from the sample itself 
and a priori knowledge which may come from familiarity with the physical setup. 
Both of these can be taken account of in an appropriate Bayesian formulation . 
3.2.1 A Wider Choice of the Parent Distribution 
Lf, in the analysis of Darwin's data, we supposed thal the parent distributions of 
self-and cross-fertilized plants were identical except for location, then the distri-
bution of the differences would certainly be symmetric. We will assume therefore 
that the parent distribution of differences is a member of a class of symmetric 
distributions which includes the t\ormal, together with other distri butions on the 
one hand more leptokurtic, and on the other hand more platykurtic. 
Now the standardized Normal distribution may be written 
p(x) = k exp ( - t Ixl
q
) 
with 
q = 2. 

3.2 
Criterion Robustness and Inference Robustness 
157 
By allowing q to take values other than two we obtain what may be called the 
class of exponential power distributions. 
These distributions were considered 
by Diananda (1949), Box (J953b), and Turner (1960) ; with q = 2/(1 + (3) they 
can be written in the general form 
( 
I \y - eI 2/(1+ P») 
p( y le, cp,fJ)=kcp-l exp 
-
2 .-cp- · 
' 
- ro < y < ro, 
(3.2.3) 
where 
and 
cp > 0, 
- ro < D < ro, 
- I < fJ ~ I. 
In (3.2.3), () is a location parameter and cp is a scale parameter. + I t can be readily 
shown that 
fe y ) = e, 
V ( ) = a2 = il+p)f l[}(1 + fJ)] }cp2. 
ar y 
\. rue I + (3)] 
(3.2.4) 
We may alternatively express (3.2.3) as 
[ 
I
y - IJj2/(J +P)] 
p(y I e, a, fJ) = w(fJ)a- 1 exp 
- c«(3) -a-
, 
- ro < y < ro, 
(3.2.5) 
where 
c 
_ {nW + f3)] } 1.'(1 + Pl 
(fJ) -
ITW + P)] 
and 
{r[1( J + P)] } 1'2 
w(P) = (I + P){1[-}(1 + P)J}J,2' 
a > 0, 
- ro < e < ro, 
- J < .B ~ I. 
The parameters e and a are then the mean and the standard deviation of the 
population, respectively. The parameter P can be regarded as a measure of kurtosis 
indicating the extent of the " non-Normality" of the parent population. In par-
ticular, when fJ = 0, the distribution is Normal. When.B = I , the distribution is 
the double exponential 
I 
( 
Iy -Ol) 
p(y I e, a, P = 1) = v'L a exp 
- ,/1. . -a-
, 
-
00 < y < ro. 
(3.2.6a) 
'1' In our earlier work (1962) the form (3.2.3) was employed, but there the symbol a was 
used for the general scale parameter now called cp. 

158 
Bayesian Assessment of Assumptions i 
3.2 
Finally, when [3 tends to - I, it can be shown thal the distribution tends to the 
rectangular distribution, 
I 
lim p(y : {3, e, 0') = --- , 
P~-I 
2,,30' 
o - y'3 (J < Y < e + .)3 (J. 
(3.2.6b) 
figure 3.2.2 shows the exponential power distribution for various values of [3. 
The distributions shown have common mean and standard deviation. 
We see 
that for {3 > 0 the distributions are leptokurtic and for fJ < 0 they are platykurtic. 
To further illustrate the effect of fJ on the shape of the distribution, Tabk 3.2.2 gives 
the upper 100:x percent points in units of (J for various choices of {3 with () assumed zero. 
Except for fJ equals 0 and 1, and the limiting case {3 ~ - I, the percentage points in the 
table were calculated by numerical integration on a computer [Tiao and Lund (1970)]. 
In (3.2.5) we have employed the non-Normality measure {3 which makes the double 
exponential and the rectangular distribution equally discrepant from the Normal. 
However, we might have used, for example, the familiar kurtosis measure Y2 = 
J(4/J(~ 
for the class of exponential power distributions. It is readily shown that 
f[1( I + {3)] f["W + /3)] 
--------,--- -
3. 
{feW + {3)J}2 
(3.2.7) 
Table 3.2.3 gives the value of Y2 for various values of p. 
In terms of Y2, the double 
exponential distribution would appear as 3 and the rectangular distribut ion as - 1.2. 
However, whether {3, Y2' or any similar measure of non-Normality is adopted, the 
analysis which follows will be much the same. 
Table 3.2.2 
Upper 100:x percent points of the exponential power distribution for various values of fJ 
in units of standard deviation 0' 
(8 = 0) 
P 
rJ. = 0.25 
0.10 
0.05 
0.Q25 
0.01 
0.005 
0.001 
-1.00 
0.87 
1.39 
1.56 
1.65 
1.70 
1.71 
1.73 
-0.75 
0.84 
1.36 
1.57 
1.71 
1.84 
1.91 
2.05 
-0.50 
0.80 
1.35 
1.61 
1.81 
2.03 
2.16 
2.41 
-0.25 
0.73 
1.31 
1.63 
1.89 
2.18 
2.37 
2.75 
0.00 
0.67 
1.28 
1.64 
1.96 
2.33 
2.58 
3.09 
0.25 
0.62 
1.25 
1.65 
2.02 
2.46 
2.77 
3.43 
0.50 
0.58 
1.22 
1.65 
2.06 
2.58 
2.94 
3.76 
0.75 
0.53 
1.18 
1.64 
2.09 
2.68 
3.10 
4.08 
1.00 
0.49 
1.14 
1.63 
2.12 
2.77 
3.28 
4.39 

3.2 
Criterion Robustness and Inference Robustness 
159 
{J ~
O . 75 
I \ 
{J ~ -0.50 
{J = - 0.25 
{J = LOU 
(J=O(Normal) 
(Double exponen tial) 
Fig. 3.2.2 F-xponential power distributions with common standard deviation for various 
values of {3. 

160 
Bayesian Assessment of Assumptions I 
3.2 
Table 3.2.3 
Relationship between (fJ, Y2) 
fJ 
-1.0 
-0.75 
-0.50 
-0.25 
0.0 
0.25 
0.50 
0.75 
1.00 
Yz 
-1.2 
-1.07 
-0.S1 
-0.45 
0.0 
0.55 
1.22 
2.03 
3.00 
3.2.2 Derivation of the Posterior Distribution of 0 for a Specific Symmetric Parent 
Given. a sample of n independent observations from a member of the class of 
distributions in (3.2.5), the likelihood function is 
l· 
II ly- eIZI( I+ P)] 
I(e, (J, fJly) x [w (fJ)] II (J-lIe xp -C(fJ)JI -'(J- . 
. 
(3.2.8) 
Consider now the posterior distribution of e assuming that the parameter fJ is 
known. For a fixed fJ, the likelihood function is 
[ 
1
1' - e 
12 / ( 1 +P)] 
I(e, (J I fJ, y) OC (J-II exp -c(fJ) ~ -'-(J-
. 
(3.2.9) 
In addition, the noninformative reference prior for e and (J is such that locally 
pee, (J I fJ) OC 0'-1. 
(3.2.10) 
[See the discussion in Section 1.3 concerning the l.ocation-scaJe family (J .3.57), 
of which (3.2.5) is a special case, and the discussion leading to (1.3. \05).J 
The joint posterior distribution of (e, 0') is then 
r 
1 
y - e 12/(1 +Pl] 
pee, a I [3, y) OC a-(1I+1 l exp -c([3) ~
. ~ 
, 
-
00 < 0 < 00, a > O. 
(3.2.11) 
Employing (A2.1.6) in Appendix A2.1 to integrate out (J , the posterior distri-
bution of e is then 
- oc < e < 00, 
(3.2.12) 
where 
M(e) = I, IYi _ e1 2/(l Cp) 
i 
and [J(fJ)] - 1 is the appropriate normalizing constant. 
Thus, for any fixed fJ, 
pee I [3, y) is simply proportional to a power of .'v1(e),n, the absolute moment of 
order 2/(1 + fJ) of the observations about e. The constant [J(,B)r 1 is such that 
(3.2.13) 

3.2 
Criterion Robustness and Inference Robustness 
161 
and is merely a normalizing factor which ensures that the total area under the 
distribution is unity. This integral cannot usually be expressed as a simple function 
of the observations ; it can, of course, always be computed numerically, and with 
the availability of electronic computers, this presents no particular difficulty. lf 
all that is required is to draw the posterior distribution so that it can be presented 
to the investigator, it is seldom necessary to bother with the value of the normali-
zing constant at all. He appreciates that a probability distribution must have unit 
area and for most inferential purposes the ordinates need not even be marked. 
Using (3.2.12), posterior distributions computed from Darwin's data for various 
values of [3 are shown in Fig. 3.2.3. 
Mid-
p (O i {l, y ) 
. 
0.06 
(3= -0.9 
I 
0.04 
I 
0.02 
o 
~\ 
1\ 
1\ 
I \ 
, \-.-!l= 09 
, \ 
, 
I 
, 
I 
I 
1 
I 
I 
!l= 0.2 
Fig. 3.2.3 Posterior distribution of e for various choices of [3: Darwin's data. 

162 
Bayesian Assessment of Assumptions 1 
3.2 
3.2.3 Properties of the Posterior Distribution of 8 for a Fixed [3 
Since p(8 I [3, y) is a monotonic function of M(8), we find (see Appendix A3.1): 
1. p(8 I y, f3) is continuous, and, for -I < [3 < I, is differentiable and unimodal, 
although not necessarily symmetric; the mode being attained in the interval 
[y( I), Y(n)l 
The modal value is in fact the maximum likelihood estimate of 8. 
However, it should be noted that we are not concerned with the distribution of 
this maximum likelihood estimate; rather, we are considering the distribution of 
8 given the data. 
2. When [3 = 0, M (8) = L(Y; - 8)2 = (n -
1 )S2 + n(y - 8)2, and making the 
necessary substitutions we obtain, for the posterior distribution of 8, 
( 8-Y\ 
) 
p t=~ [3=O,y =p(tn - 1), 
S, 'V n 
(3.2.14) 
where p(t" _ 1) is the density of t (0, 1, n - I) distribution. 
3. When [3 approaches - I, 
lim [M(8)]<P+1)'2 = (h + 1m - 81), 
p~ -I 
where m and h are as in (3.2.2a). Making the necessary substitutions, we find 
1 ( 
h )-1 ~ 
IWI1-n 
lim p(8 I [3, y) = -
--
I + --
, 
fl~-I 
2 n-l 
. 
n-I 
-00 < 8 < 00, 
(3.2.15) 
where 
8-m 
w=--,-------
h,(n -
I) 
Thus, 
. 
( 
18 - ml I 
) 
pl~~ 1 P F = hl(n _ I) [3, y = p[F 2,2(" -1)J, 
F> 0, 
(3.2.16) 
where p[F 2,2(11 _ 1)J is the density of an F variable with [2, 2(n -
I)] degrees of 
freedom. For Darwin's data, (3.2.15) is the reference distribution for 8, shown by 
the curve on the left in Fig. 3.2.1, but now derived as a I imiting posterior distri-
bution. 
Thus, we see that, when the parent is Normal ([3 = 0), our expression (3.2. I 2) 
yields the I distribution as expected, and when the parent approaches the rectan-
gular ([3 --> -
1), again as expected, (3.2.12) tends to the double F distribution 
with 2 and 2(n -
1) degrees of freedom. In each case, the posterior distribution 
can be expressed in terms of simple funL:tions of the observations which are 
minimal sufficient statistics for 8 and (J. 

3.2 
Criterion Robustness and Inference Robustness 
163 
4. When fJ approaches 1, the distri but ion in (3.2.12) is not expressi ble in terms 
of simple functions of the observations. However, in the limit the mode of the 
posterior distribution is the median of the observations if n is odd, and is some 
unique value between the values of the middle two observations if n is even. When 
fJ = I and n is even, the density is, in fact, constant for values of e between the 
middle two observations. 
5. In certain other cases, it is possible to express the posterior distribution of 
e in terms of a fixed number of functions of the observations. 
For instance, 
letting 
fJ = (2 - q)/q, 
then for 
q = 2,4, .. . , 
we have 
pee, a I fJ, y) x a-(n+ l)exp [ -c(fJ)a-
q Jo (- I)' (~) ersq-rl ' 
-
CXl < e < 00, 
a > 0, 
(3.2.17) 
and 
-
00 < e < 00 , 
(3.2.18) 
where 
It is readily seen that the set of q functions, SI' S2' ... , Sq, of the observations 
are jointly sufficient for e and a. 
In general, however, the posterior distribution cannot be expressed in terms 
of a few simple functions of the observations. If we wish to think in terms of 
sufficiency and information as defined by Fisher (1922, 1925), our posterior 
distribution always, of course, employs a complete set of sufficient statistics, 
namely, the observations themselves. Consequently, no matter what is the value 
of fJ, no information is lost. 
From the family of distributions for various values of /3, sbown in Fig. 3.2.3, 
we see that very different inferences would be drawn concerning 8, depending 
upon which value of fJ was assumed . The chief reason for this wide discrepancy 
is the fact that in Darwin's data, the center of the posterior distribution changes 
markedly as fJ is changed. In particular, for this sample, the median , mean, and 
tbe midpoint are 24.0, 20.9, 4.0 respectively ; and these are the modes of the 
posterior distributions for the double exponential, Normal , and rectangular 
parent, respectively. 

164 
Bayesian Assessment of Assumptions 1 
3.2 
3.2.4 Posterior Distribution of 8 and fJ when fJ is Regarded as a Random Variable 
Because of the wide differences which occur in the posterior distribution of 8 
depending on which parent distribution we employ, it might be thought that 
there would be considerable uncertainty about what could be inferred from this 
set of data. It turns out that this is not the case when appropriate evidence con-
cerning the value of 13 is put to use. There are two possible sources of information 
about the value of fJ, one from the data itself and the other from knowledge 
a priori. Both types of evidence can be injected into our analysis by allowing fJ 
itself to be a random variable associated with a prior distribution p(fJ). 
J oint Distribution of 8 and fJ 
Let us assume tentatively that, a priori, fJ is distributed independently of the mean 
8 and the standard deviation a so that 
p(8, a, fJ) = p(fJ)p(e, a). 
As before, we adopt the noninformative reference prior for (8, 0) 
pC8, a) ex a- J. 
Then, the joint posterior distribution of (8, (1, 13) is 
(3.2.19) 
(3.2.20) 
pee, (1, fJl y) ex (1- J p(fJ)/(e, a, fJl y), 
-oo < e<oo, 
a > O, 
-1<fJ ~
l, 
(3.2.21) 
where I(e, a, fJl y) is the likelihood function given in (3.2.8). 
After eliminating 
the standard deviation a, we obtain the joint posterior distribution of (e, fJ) as 
pee, 131 y) ex p(fJ)[M(8)r tn(l +il) rei + 'In(1 + fJ)J{r[l + } (1 + fJ)J} -n, 
-
00 < e < 00 , 
-I < fJ ~ I. 
(3.2.22) 
Assumptions about locally uniform priors and particularly about independence of 
the component parameters have to be made with caution. Although it seems reasonable 
that e should be assumed to be independent of a and fJ a priori, we might have second 
thoughts about the independence of the scale parameter a and the non-Normality 
parameter fJ. The definition of a scale parameter is always arbitrary to the extent of a 
multiplicative constant, and in particular, if we write the distribution in the form (3.2.3), 
using ¢ as the scale parameter, then 
¢ = /(fJ)a, 
where /(fJ) is some function of fJ. It might be supposed, therefore, that if fJ and a were 
independent then (J and ¢ could not be. It would then follow that the marginal posterior 
distribution p(8, fJl y) would be different if it was assumed [as in our earlier work (I 962)J 
that log ¢ was locally uniform and independent of fJ instead of assuming, as we have done 
-------

3.2 
Criterion Robustness and Inference Robustness 
165 
here, that log u was locally uniform and independent of fJ. In fact, the results obtained are 
approximately the same whatever functionf(m is used. To see this, Jet LIS suppose that 
p(log u, fJ) = p(log u) pep), 
(3.2.23) 
with p(log u) ex; c. Since log ¢ = 10gf(fJ) + log u , it follows that, for given 13, locally, 
p(log ¢) x c. 
(3.2.24) 
Further, we can see from Appendix A3.2 that if[ogu is locally uniform and independent of 
{J, then log ¢ and fJ will be approximately independe;,t. 
A Reference Distribution for fJ 
It will turn out from our subsequent discussion that a useful reference prior 
distribution for fJ is a uniform distri burion over the range ( - I, I). If we denote 
the posterior distribution of e and 13 based on this choice by puce, fJl y), then 
Pllce, fJ I y) ex; [M (e)r ~ n ( 1+ P) r[ I + }n( I + fJ)] {fC I + ~ (I + fJ)]}-1/ 
(3.2.25) 
and 
pee, PI y) ex; puce, f31 y)p(f3), 
- co < e < cx.;, 
-
1 < 13 ~ 1, 
(3.2.26) 
where pCP) is any appropriate prior distr~bution of 13. 
The joint distribution PII(e, fJl y) actually obtained from the Darwin data is 
shown in Fig. 3.2.4. The sections of the joint distribution for various fixed values 
Fig. 3.2.4 The joint distribution pl/(e, fJ i y): Darwin's data. 

166 
Bayesian Assessment of Assumptions 1 
3.2 
of [3 are, apart from a weighting factor, the conditional distributions p(e I [3, y) 
already sketched in Fig. 3.2.3. The weighting factor is of course p,,([31 y) and it is 
evident that this factor attaches little credibility to platykurtic parenthood. 
3.2.5 Marginal Distribution of [3 
If we integrate (3.2.25) over e we obtain the weighting factor mentioned above 
which is the posterior marginal distribution of [3 for a uniform reference prior 
in [3, 
Pu([31 y) cc rei +In(I + [3)]{r[1 + HI + [3)]}-n1([3), 
-I < [3:( 1, 
(3.2.27) 
where 1([3) is given in (3.2.13). This distribution , shown by the solid curve In 
Fig. 3.2.5 for a = I, aJlows appropriate inferences about [3 to be drawn in the light 
of the data for a uniform prior over the range -
J < [3 :( I. 
Now, in practice, 
there will be few instances where this prior would actually coincide with the 
attitude of the investigator, nevertheless, its use as a reference provides a valuable 
1.0 
0.5 
1.0 
0.5 
' 
a =\ 
p,,(!l.y) 
p(ill 
-----~--- --------
1'--___ __ 
--'-_____ 
---' !l ~ 
\ 
a=J 
/ 
p({3 l yl-r-
P({3)-/ 
I 
/ 
I 
/ 
I 
o 
",-
) 
\ .0 
0.5 
._ \ 
o 
a = \ 0 
\ .5 
' 
\.0 
0.5 
.... 
M ... /=-____ 
--'-____ 
'--""'" {3 -"+ 
__ 
--"''''----__ 
--'-___ 
~_---.J {3 ,. 
- \ 
0 
\ 
- \ 
0 
\ 
Fig.3.2.5 Prior and posterior distributions of f3 for various choices of the parameter 
"a": Darwin's data. 

3.2 
Criterion Robustness and Inference Robuslness 
167 
'intermediate step to more realistic choices. r or, since 
p(f3 l y) rx Pu(f3 i y)p(fJ), 
(3.2.28) 
we can produce the posterior distribution of fJ for any prior p(f3) from p"Cf3 l y) 
merely by multiplication and renormalizing. 
Choice of a Prior Distribution for fJ 
ln problems like the analysis of Darwin's data, usually some central limit effect 
would be expected. While this does not warrant an outright assumption of Nor-
mality which can be represented in the present context by making p(f3) a b function 
at fJ = 0, it does mean that the investigator would want to associate high prior 
probability with distributions in the neighbourhood of the Normal. 
He could 
represent this attitude by choosing a prior distribution for fJ having a single mode 
at fJ = 0. A convenient distribution for this purpose is a symmetric beta distri-
buti0n having mean zero and extending from -
I to + I with one adjustable 
parameter which we call "a". Specifically, we assume thatt 
-1<f3~l, 
(3.2.29) 
where 
W= f(2a)[l(a)r 2 2-(2 a -I), 
a ~
l. 
When a = }, the distribution is uniform. With a> I, it is a symmetric distribution 
having its mode at the normal theory value 13 = 0, and it becomes more and more 
concentrated about 13 = ° 
as "a" is increased . When "a" tends to infinity, p(f3) 
approaches a delta fl)nction , representing an assumption of exact Normality. 
The dotted curves in Fig. 3.2.5 show this distribution for a = 1,3,6, and 10. 
The corresponding posterior distributions p(f3l y) are shown by the solid curves in 
Fig. 3.2.5. 
The Figure shows how increasing prior certainty about Normality 
tends to override the information from the sample. When "a" tends to infinity, 
pCf3l y) will approach a b function at fJ = 0 and Normality will be assumed no 
matter what the information from the sample. 
3.2.6 Marginal Distribution of e 
The posterior distribution of e is obtained by integrating out fJ from the joint 
distribution of 8 and p, yielding 
-
CJJ < e < 00. 
(3.2.30) 
t Letting x = (I + 13)/2 and p = q = a, we have the usual beta distribution 
( ) = f(p + q) P-I(I_ )q-J 
P x 
f(p)f(q) x 
x, 
O<x<J. 

168 
Bayesian Assessment of Assumptions 1 
3.2 
Also, we may write 
pee, {31 y) = pee I {3, y)p({3 I y). 
(3.2.31) 
The posterior distri bution of e, 
pee I y) = f /(e I fJ, y)p({31 y) dfJ, 
-
00 < e < 00 , 
(3.2.32) 
can thus be thought of as a weighted average of the "I-like" distributions pee I (J, y) 
of Fig. 3.2.3, with a weight function p({31 y) given by a solid curve of Fig. 3.2.5. 
Since p(fJ I y) ex. p,,(fJ I y) p(fJ) we have also 
pee I y) ex. f J pee I (J, Y)Pu(fJl y)p({3) d(J, 
-
00 < e < 00. 
(3.2.33) 
Thus, we are averaging the f-like distributions with a weight function which 
depends partly on p({3), representing prior information, and partly on Pu({31 y), t 
representing information which is independent of the prior. 
Finally we can write 
pee I y) ex. J 
~ 1 pu(e, {3 I y)p(fJ) d(J 
(3.2.34) 
so that pce I y) is the marginal distribution obtained when Pl/ce, fJl y) shown in 
the three-dimensional diagram of Fig. 3.2.4 is averaged over the weight function 
p«(J). 
The marginal distributions pce I y) for a = I, 3, 6 and 00 are drawn in Fig. 
3.2.6. They show to what extent inferences about e depend on the strength of 
central limit effect assumed. The curve for a -> 00 is a I distribution corresponding 
to an outright assumption of parent Normality. In view of the very large differ-
ences exhibited by the conditional distributions pee I {3, y), it is remarkable how 
little the marginal distribution pee I y) is affected by choices of "a" covering a 
range representing exact Normality (a -> (0) to "no central limit effect" (a = I). 
The reason for this is that widely discrepant conditional distributions generated 
by parents which approach the uniform ({3 -> -
1) are almost ruled out by in-
formation coming from the sample itself. 
The curve for a = 10 is not shown in Fig. 3.2.6 because it is almost identical 
to the t distribution obtained when a -> 00. 
This is interesting because, as 
seen from Fig. 3.2.5, the central limit effect implied even by a = 10 is not an 
overwhelmingly strong one. For instance, with this distribution the probability 
a priori that -0.33 < {3 < 0.33 (that is, q = 2/(1 + {3) is between 1.5 and 3) is 
only 87 percent. 
t We are employing noninformative prior distributions for e and a; a change in the 
prior distribution pee, a) will of course change Pu({3 I y). 

3.2 
Criterion Robustness and Inference Robustness 
]69 
p(1I1 y) 
0.04 
0.02 
Fig. 3.2.6 Posterior distribution of e for various choices of the parameter "a": Darwin's 
data. 
3.2.7 Information Concerning the Nature of the Parent Distribution Coming from 
the Sample 
In the past, the Normality (or otherwise) of a sample has often been assessed by 
inspecting the empirical distribution, by applying goodness-of-fit tests such as 
the l test and the Kolmogoroff-Smirnoff test (1941), and by checking measures 
of skewness and kurtosis. In cases such as the present one, where it is reasona ble 
to assume symmetry, the calculation of PIl([31 y) would provide another way of 
expressing sample information about the nature of the parent distribution. How-
ever, with this approach more can be done than merely "test" the assumption of 
Normality and then, in the absence of a "significant" result, assume it. The infor-
mation about [3 coming from the sample is appropriately used in making inferences 
about e. In particular, for Darwin's data it plays an important role in virtually 
eliminating the influence of unlikely parent distributions having extreme nega-
tive kurtosis. 
3.2.8 Relationship to the General Bayesian Framework for Robustness Studies 
It will now be seen how Darwin's example illustrates the general approach introduced in 
Section 1.6. After integrating out (J, we have the joint distribution of the mean e and the 
non-Normality parameter [3, which correspond to 9J and 92 respectively, in the general 
discussion. While it is true that we can obtain the marginal posterior distribution of e 
immediately by integrating out [3 from pCB, [31 y), it is informative to write the integral in 
the form 
(3.2.35) 

\ 
170 
Bayesian Assessment of Assumptions] 
3.3 
and to say that p({31 y) serves as a weight function acting on the various conditional 
posterior distributions pee I (3. y). A study of the conditional distribution p({) I /3. y) for 
various (3, together with the weight function p({31 y) as was done in Sections 3.2.2 through 
3.2.5, corresponds precisely to a study of the component distributions p(9. 192, y) and 
p(92 I y) in (1.6.3) of the general approach. 
Indeed. the attractiveness of this approach is further increased by the fact that if we 
let p(9) and p(92 ) be the prior distributions for 9) and 92 assumed independent, and let 
1(9.,92 I y) represent the joint likelihood, we may then write 0.6.3) in the form 
where 
p(9) I y) = c. tp(6. 192 , y)p.,(62 I y)p(92)d92 , 
pu(e2 1 y) = c2 J p(9) )/(9.,92 I y) d9. 
(3.2.36) 
(3.2.37) 
is the likelihood integrated over a weight function p(9.) and c. and C2 are normalizing 
constants. The marginal distribution p(e 2 I y), which is proportional to the product, 
(3.2.38) 
is separated into the prior distribution of 9 2 and a part which is independent of this prior 
distribution. The function Pu(9 2 I y) can be regarded as a posterior distribution on the 
basis of a uniform reference prior for 9 2 , It provides a basic reference posterior density 
which can be converted into a posterior distribution with an appropriate prior p(az) by 
multiplication. It is informative to consider the effect of varying p(9z) to see how sensitive 
is the final result to changes in prior assumptions. and also to study p.,(92 I y) itself. 
Thus, for the present example, the weight function p({31 y) in (3.2.35) can be written 
p({3 I y) oc p({3)Pu({J I y), 
(3.2.39) 
with p({J) the prior distribution of (3, corresponding to p(92) in the general framework. 
Also 
Pu({31 y) oc J Pu({J, ely) de 
(3.2.40) 
corresponds to p,,(az I y) in the general formulation and can be thought of as the likelihood 
integrated over a weight function uniform in e and log (5. 
3.3 APPROXIMATIONS TO THE POSTERIOR DlSTRIBL'TlON pee I {3, y)t 
In this section, we consider again the posterior distribution of e for given {J, p({) I (3, y) 
in (3.2.12), and discuss a method which can be used to approximate it. 
3.3.1 Motivation for the Approximation 
It turns out that over a wide range of values of {3 (say, from {3 = -0.75 to {J = 0.75) the 
distribution pee I (3, y) can be satisfactorily approximated by a 
t distribution. 
t Much of the material in this section al'Jd Section 3.4 are taken from D. R. Lund's 
Ph.D. thesis (1967). 
-----

3.3 
Approximations to the Posterior Distribution p(e I p, y) 
171 
Specifically, we shall demonstrate that 
pee I {3, y) oc [M(B) + dee - tJ) 2r £n(l+p), 
-
00 < e < 00, 
(3.3.1) 
where 
n 
M(e) = I Iyu _eI 2/(J '-PJ, 
u=l 
d is an appropriately chosen constant, and 8 is the mode of pee I {3, y). The symbol oc 
means "approximately proportional to." To this degree of approximation, 0 is distributed 
as the t distribution t{8, .'v1(8)/d[n(\ -I- {3) - 1],11(1 + {J) -I}. 
This type of approximation has to be justified in somewhat different ways, depending 
on whether {3 is negative or positive. In particular, for positive {3 the approximating process 
itself can be somewhat tedious. 
We shall, therefore, make clear why this kind of 
approximation is important. 
Cenainly,for many purposes of inference all that we really need is to be able to com-
pute the posterior density function of e and present its graph to the investigator. This 
computation is best made using the exact form 
pee I {3, y) cc [M(e)rt,,(l + Il), 
-
00 < e < 00, 
(3.3.2) 
as given in (3.2.12). 
The approximation (3.3.1) has two uses: 
a) In some instances we may wish to calculate probability integrals. Although we can do 
this directly by integrating the density function (3.3.2) numerically, it is an advantage 
to be able to make use of the already tabled t integrals. 
b) In problems discussed in Chapter 4, we shall often need to be able to integrate out 8 
from some posterior distribution involving other parameters (for example, the 
variance (}2). 
Specifically, we shall be involved with evaluating expressions of the form 
Using (3.3.1), we have 
J":oo [M(8)] -C, dA 
J": 00 [M(8)] c, dO' 
J":oo [M(O)r
C 1 de . J":oo [M(B) + d(8 - 8)2r
C1 de 
J":oo [M(&)] c2 d8 = J ~ oo [M(8) + d(8 - 8)2r C2 dO 
(3.3.3) 
(3.3.4) 
"iote that, for this approximation, actual calculation of the value of d is not necessary 
since it cancels out on integration. So far as this more important application is concerned, 
what follows is mainly intended to show that an approximation of the form of (3.3.1) can 
be quite accurate and that, therefore, the approximate integration (3.3.4) can be used. 

172 
Bayesian Assessment of Assumptions 1 
3.3 
3.3.2 Quadratic Approximation to M(8) 
Since p(8 1 {3, y) is a monotonic decreasing function of M(8), we shall first conduct the 
discussion in terms of M(8). Now, M(8) is a convex function of 8 and, for -1 < {3 < 1, 
possesses a unique minimum 8 [which is, of course, the mode of p(8 1 {3, y)]. Further, 
for {3 = 0 
(3.3.5) 
Now if for {3 :f. 0 we can use an approximation of the form 
(3.3.6) 
where d is some constant to be determined, then the corresponding approximate 
distribution of 8 can be expressed in terms of the familiar Student's t form. In what 
follows, we first discuss the problem of determining the mode and then the question of 
finding the constant d. 
Determination of the Mode 8 
Two methods may be employed to obtain 8. The first of these makes use of the fact that 
M'(8) is monotonically increasing in 8 so that M/(a) = 0 has only one solution, 8. Thus, 
M'(8) ~ 0 implies that 8 ~ 8. By calculating M ~ (8) at a point 8, we, therefore, know in 
which direction to proceed toward e. 
We then increase or decrease the value of 8, 
whichever is required, in steps until M'(8) changes sign. Repeating this process, but using 
steps which are halved after each sign change, we can obtain {} to any desired degree of 
accuracy. 
The second method employs Newton's iteration procedure for extracting roots of an 
equation. Starting with an initial guess value 80' we can write 
(3.3.7) 
provided the second derivative Mil (8) exists. Setting the right-hand side of (3.3.7) to zero, 
we obtain a new estimate, 8 l of {}, such that 
(3.3.8) 
Repeating the procedure with 8l in the role of 80' we obtain a second new estimate B2 , 
and so on. We continue the process until 18i+l -
8;\/18il < 0.001, say, where 8i+l is 
the value obtained in the (i + l)th iteration. Now 
and 
-2 
n 
M'(8) = -- L Iyu - 81- 2{J/(l+{J) (Yu -
8) 
l+{3u=l 
2(1 -
{3) 
n 
M"(8) = 
"I - 81- 2{J/(1+ (J) 
(1 + {3)2 uf'l Yu 
, 
(3.3.9) 
(3.3.10) 

3.3 
Approximations to the Posterior Distribution p(e ! P. y) 
173 
so that 
B," I = B _ M'(e j ) 
, 
M"(e j ) 
= (-2{3\
, 
(1 +(3)L:=lly"-B jl- 2P/iJ+Ply,, 
\1 - {3! B, + I -
(3 
L~"lly, - ejl 
2P1(I+P) 
. 
(3.3.11 ) 
When the iteration process is terminated, we have, by setting e = Bj , I = Bj , 
II 
e = L allY'"~ 
(3.3.12) 
u=1 
where 
f nterpretGtiol1 oj Result s 
It is interesting to note that (3.3.12) says that modal value (') is in th~ form of a weighted 
average of the observations, When (3 ---> 0, all of the weights a" tend to I 'n as they should, 
yielding (') = Y in the limit. As fJ decreases below zero, observations corresponding to the 
residuals y" -
[) with large absolute values tend to receive more weight. 
This is an 
intuitively pleasing result because for /3 < 0, the parent distributions have tails shorter 
than those of the '\iormal, and one would expect that extreme observations would thus 
provide more information about the location of the distribution, We have already seen 
that as /3 ---> -1 in the limit, e = (Y(I ) ...L Y(III) 2 (the sample midpoint), so that all of the 
weight is divided equally between the largest and smallest observations, while all of the 
intermediate observations get zero weights. Indeed, the weights in (3 ,3.12) tend to this 
limit. The iteration procedure will, however, converge very slowly for {3 near -1 because 
the factor (I + (3)/(I -
{J) in (3.3,11) is small and Bj + I is determ ined primarily by Bj , 
As (3 increases above zero, more weight is given to observations corresponding to the 
residuals Y" - e of smaller absolute value. This is again intuitively appealing because the 
parent then has longer tails than those of the Normal, and one would now expect that the 
"extreme" observations would provide relatively little information about the location of 
the parent distribution. 
When /3 < 0, 8j ~ J in (3,3.11) is a weighted average of two quantities Bj and aj, ""here 
(3,3.13) 
with weights equal to -2/3f(1 -
j3) and (I + j3li(l - /3), respectively, The quantity 8j is 
the previous estimate of e, and the quantity {}j is itself a weighted average of the 
observations, Clearly 8H I lies between 8j and aj . 
However, when /3 > 0, 8j 7 J ' instead of lying between 8 j and ai' becomes a weighted 
difference of these two quantities and thus could be distinct from either one, Indeed, as 

174 
Bayesian Assessment of Assumptions 1 
3.3 
13 -> I, 8i + I becomes a large multiple of 0i - ai . The iteration procedure (3.3.11) may, 
therefore, be very erratic and unpredictable for 13 > 0, especially for 13 near I. In practice, 
it may proceed toward the solution, but accelerate too fast and overshoot. It may tben 
reverse direction and begin to diverge. 
Thus, the second method should be used for 13 < 0 only and the first method used 
when 13 > O. In practice, the second method often converges for 0 < 13 ~ 0.25, and in our 
experience, the final estimate in such cases has always been the same from botb methods. 
The first method, however, seems more reliable. 
3.3.3 Approximation of p(a I y) 
For 13 ~ 0, the second derivative M " (8) exists for all a and we can employ Taylor's theorem 
to write 
. 
~ 
M"(O) 
• 2 
'vI(8) = .\1(8) + -- (8 -
a) . 
2 
(3.3.14) 
This expression is, of course, exact for 13 = O. 
To this degree of approximation, the 
posterior distribution p(8 I 13, y) is 
(3.3.15) 
That is, for 13 < 0, a is approximately distributed as 
t{e,2M(e)/M" (e)[n(1 +/3) - IJ, n(l + 13) - I}. 
We have found in practice that (3.3.J5) gives very close ap'proximation in the range 
-0.5 ~ . 13 ~ O. To illustrate, Fig. 3.3.1 shows, for Darwin's data, the exact distribution 
(solid curve) and the corresponding approximation (broken curve) from (3.3.15) for 
13 = -0.25, -0.50, and -0.75, respectively. Tn the first two instances, the agreement is 
very close. The approximation becomes somewhat less satisfactory for 13 = - 0.75. This 
is to be expected, since the limiting distribution of a as f3 -> - 1 is not of the t form 
[see (3.2.15)]. 
p (81 fJ. y) 
OO(,~ 
0 04~ 
;; = '0.25 
,A 
o.ot 
- 20 
0 
20 
40 
-- F.XJ(( 
p(IJ I {J . y ) 
0.06 
0.04 
0.02 
I e ~ 
60 
- 20 
0 
---- Approx imate 
pre I ~ . y) 
0.06 
/3 = -0.75 
Fig. 3.3.1 Comparison of exact and arproximate distributions of 8 for several negative 
values of f3: Darwin's data. 

3.3 
Approximations to the Posterior Distribution p(B I /J, y) 
175 
For {J > 0, M" (e) does not exist if 8 = Yu , (ll = j, ... , /1), so that the approximation 
in (3.3.14) may break down. In practice, even in cases where e is not equal to any of the 
observations, good approximations are only rarely obtained through the use of this 
method. There is, however, reason to believe that the form (3.3.1) with d appropriately 
chosen will produce satisfactOry results for fJ > O. In our earlier work (1964a), this form 
was employed to obtain very close approximation to the moments of the variance ratio 
O"VO"i in the problem of comparing the variances of two exponential power distributions 
(this is discussed in Chapter 4). We now describe a method for determining d which does 
not depend on the existence of '\1 "c{h 
This method employs the idea of least squares. For a set of suitably chosen values of 
e, say, el , e2 , .. ,em , we determine d so as to minimize the quantity 
m 
S = I, {l'v/(8) -
[M(8) + d(Bj - 8)2]}2. 
j=l 
Solving as/ad = 0, the solution is 
d = L'jr~ l [MCBj ) -
M(8)](ej - {N 
I,J= I (ej -
8)4 
(3.3.16) 
(3.3.17) 
All that need be determined now is where to take the m points Bj . If m is large, say forty 
or more, the exact location of the points should not be critical so long as they are spread 
out over the range for which the density of e is appreciable. In particular, if fI1 is odd and 
if the B/s are uniformly spaced at intervals of length c, with the center point at e = 8, 
then (3.3.17) can be simplified to 
d = 240 {[Lj'=1 M(B)(Bj -
8)2J -
M(8)c
2Cm- 1)111(111 + 1)/12} 
c4 (m -
l)m(1Il + 1)(3m 2 -
7) 
. 
To this degree of approximation, the posterior distribution of e is 
-- Exaci 
----Approx imale 
::6t
il
. y) 
0.04 
0.02 
i3 = 0.25 
:~:lt
'3' y) 
0.04 
0.0:' 
0.06 
i3 = 0.50 
0.04 
0.02 
'--"""'-_--' __ 
'---"'--...J e -, 
o 
20 
40 
60 
o 
:'0 
40 
(3.3.18) 
(3.3.19) 
(J = 075 
Fig.3.3.2 Comparison of exact and approximate distributions of e for several positive 
values of {J: Darwin's data. 

176 
Bayesian Assessment of Assumptions 1 
3.4 
that is, a 
t{e, M(t9)jd[n(1 + j3) -1],n(1 + f3) -I} 
distribution. 
For illustration, Fig. 3.3.2 shows, for f3 = 0.25, 0.50, and 0.75, a comparison of the 
actual distribution (solid curve) and the approximation (broken curve) using the present 
method for Darwin's data. In calculating the approximating form, the value of d was 
determined by using a set of 101 equally spaced points spread over four Normal theory 
standard deviations [:E(Yi - y)2 in(n - I)] 1. 2 centered at e = e. 
The agreement is 
remarkably close for the cases f3 = 0.25 and f3 = 0.50. The result for the case f3 = 0.75 
is less satisfactory. In general, one would not expect the distribution of e to be well 
approximated by that of a I form when fJ is near unity. Indeed, when f3 = I and 11 an even 
integer, the density will be constant between the middle two observations. Nevertheless, 
for Darwin's data, the approximation for fJ = 0.75 still seems to be close enough for 
practical purposes. 
The process described above is admittedly somewhat tedious. However, as we have 
pointed out, the object of t his section is not primarily to provide means of calculating d. 
Rather, it is to show thatan approximation of the form M(e) == M(8) + dee - e)2 for 
some d can be used, and so to provide an easy means of integrating out e from functions 
containing ,VI(e). In our subsequent applications d will cancel and need not be computed. 
3.4 GENERALIZA nON TO THE LINEAR MODEL 
The analysis in Section 3.2 can be readily extended to the general linear model 
y = X6 + E, 
(3.4.1) 
where y is a n x 1 vector of observations, X a n x k full rank matrix of fixed ele-
ments, 0 a k x 1 vector of un known regression coefficients, and E a n x 1 vector 
of random errors. When this model was considered in Section 2.7, it was assumed 
that E had the multivariate spherically :'\Iormal distribution l\'n(O, Icr2). We now 
relax the assumption of Normality and suppose that the elements of E are indepen-
dent, each having the exponential power distribution 
[ 
I
e \'2/(I+Pl] 
pee I (J, [3) = W({3)(J-1 exp -c([3).-:; 
, 
-
<X) < e < 00, 
(3.4.2) 
where w(f3) and c(f3) are given in (3.2.5). Our objective will be to study the effect 
of departures from Normality of this kind on inferences about the regression 
coefficients 9. 
The likelihood function of (9, (J, [3) is 
[ 
n 
1 y. -
x' 91 2/
(1 +PJ] 
1(9, 0', f31 y) ex:. [w(f3)]n O'-n exp 
- c(f3) i~l 
I 
cr (,) 
, 
(3.4.3) 

3.4 
Generalization to the Linear Model 
177 
where x;;) is the ith row of X. Following arguments similar to those leading to 
(3.2.19) and (3.2.20), suppose a priori that 
p(G, <J, fJ) = p(fJ)p(G, a) 
(3.4.4) 
with 
p(G, <J) oc (J- I 
and p(fJ) temporarily unspecified. Combining (3.4.3) and (3.4.4) and integrating 
out <J, we obtain the joint posterior distribution of (9, fJ), which can be written as 
the product 
p(9, fJl y) = pee I fJ, y)p(f31 y). 
(3.4.5) 
The conditional distribution pee I {J, y) 
In (3.4.5), the conditional posterior distribution pee I {J, y) is 
-
00 < 8j < 00 , 
j = I, ... , k , 
(3.4.6) 
where 
" 
M(e) = I Iy; -
x;;)eI 2/(1 +P), 
i = 1 
(3.4.7) 
)(fJ) = L 
[M(e)r i"(1 +P ) de, 
and R: ( -
00 < 8j < co, j = I, ... , k). Jf 9 consists of a single element and X is a 
column of ones, the distribution in (3.4.6) reduces to that in (3.2.12) and much 
of the analysis which follows parallels that for the single mean. 
I n general, by studying the distri bution pee I fJ, y) as a function of {3, we can 
determine how sensitive inferences about e are, to departures from Normality 
of the type postulated. In particular, when f3 = 0 (exact Normality), 
M(e) = (y - xe)' (y -
X9), 
(3.4.8) 
so that the distribution in (3.4.6) reduces to the tk[e, S2 (X'X) - I, vJ distribution 
obtained in (2.7.20). For other values of fJ, it does not seem possible to express 
the distribution in terms of simple functions of the observations except when 
f3 --t -
1 and X assumes special forms [see Lund (J 967)J, However, when the 
number of parameters in G is small, say k = 2 or 3. contours of M(G) can be plotted. 
By investigating the changes in the location and shape of the contours for different 
\alucs of fJ, one obtains a good appreciation of the effect of changes in fJ on in-
ferences about e. An iJlustrative example will be given later in this section. 
For - 0.5 < {J < 0.5, Lund (J 967) has demonstrated that the distribution 
(3.4.6) can be satisfactorily approximated by a multivariate I distribution. 

178 
Bayesian Assessment of Assumptions 1 
3.4 
The Marginal DiSlrihutions 
The posterior distribution p([3 : y) in (3.4.5) can be written 
p([3 I y) cc puC[3 : y) p(f3), 
-1<f3~1 , 
(3.4.9) 
where 
pi[31 y) cc r[1 + ·}n(l + (3)] {r[1 + t(1 + (3)] }-n 1([3) 
(3.4.10) 
is the posterior distribution of [3 for a uniform reference pnor In the range 
-I < [3 ~ I. From (3.4.5), the posterior distribution of e is 
pee I y) = f 
1 pee I fJ, y)p(fJ I y) dfJ 
As in the case of a single parameter e, we are here averaging the conditional 
distributions pee I fJ, y) for various fJ by a weight function p(fJ i y) which is the 
product of p.,(fJ I y) and p(fJ)· 
3.4.1 An Illustrative Example 
The data of Table 3.4.1 refers to an experiment relating the rate K of a chemical 
reaction to the absolute temperature T at which the experiment was conducted. 
The dependence of K on T is expected to be represented by the Arrhenius law 
E I 
100 K = Jog A - --
b 
. 
R T ' 
(3.4.12) 
where R is the known gas constant ; A and E are constants to be estimated. The 
twenty experimental runs were performed in random order, and the reasonable 
assumption was made that over the ranges of temperature employed log K had 
constant variance. We, therefore, consider the 'simple ·linear model 
i = 1, 2, ... ,20, 
(3.4.13) 
where 
Yi = log K i. 
e _ 
E 
2 -
50,000R 
and 
The "Arrhenius plot" of Y against x is shown in Fig. 3.4.1. Tn terms of the linear 
model in (3.4.1), the (20 x 2) matrix X has two columns, the first is a column of 

3.4 
y = log K , , 
-3.3 
-3.5 
-3.9 
-4.1 
-4.3 
-4.5 
-4.7 
-4.9 
, 
\. , 
., 
{J=-0.9 
\.-,( 
\.~ 
.\. , 
\. . 
\. 
{J = - 09 
Y = - 3.996 - 0 2 I 8x 
(3 =00 
y =-4.013 - 0.20Jx 
(J = 0.9 
Y = -4.010 
0.20Jx 
-J 
833 
-2 
820 
:\ 
-I 
806 
~ 
~ 
Generalization to the Linear Model 
~ 
~ 
• 
o 
794 
• 
I 
781 
• 
2 
769 
(3 = 0 
{J=09 
• 
• 
3 x 
758 T 
179 
Fig. 3.4.] Plot of reaction rate data with fitted straight lines for f3 = - 0.9, 0.0, 0.9. 
ones and the elements of the second are the values of x as defined abov~. The 
two columns are, of course, orthogonal. From (3.4.6), the posterior distribution 
of (8,,8 2 ) for fixed f3 is 
-co < 8, < co, 
-co < 82 < co, 
(3.4.14) 
where 
20 
M(a) = L [y; - 8, - x j82 [2 /('+P). 
i= 1 

180 
Bayesian Assessment of Assumptions 1 
3.4 
Table 3.4.1 
Reaction rate data 
T 
x 
y = log K 
833 
-3 
-3.50, - 3.40, - 3.36, - 3.50 
820 
-2 -3.51, -3.62, -3.53 
806 
-1 
-3.86, -3.70 
794 
0 
-3.99, -4.18 
781 
-4.02, -4.23 
769 
2 
-4.44, -4.62, -4.34 
758 
3 
-4.74, -4.49, -4.61, -4.62 
Table 3.4.2 gives the mode of the distribution, 6 = eel' (2), for f3 = -0.9(0.3)0.9. 
The fitted straight lines which result from using 6 associated with f3 = -0.9,0.0, 
and 0.9 are shown in Fig. 3.4.l. 
Figure 3.4.2 shows, for each of the seven values of f3 considered, three con-
tours A, B, and C of the corresponding posterior distribution of e8 1, ( 2) together 
with the mode eel' (2)' The density levels of the three contours are: 
A: pe8 1, 82 1 f3, y) = 
0.5 pCBl> ezl f3, y), 
B: 
p(8 1 , 82 I fJ, y) = 0.25 p(e l , e2 I f3, y), 
C: p(8 1, 82 [f3, y) = 0.05 p(B I , e2 1 f3, y). 
Table 3.4.2 
Values of the mode of e as a function of f3: reaction rate data 
fJ 
-0.9 
-0.6 
-0.3 
0.0 
0.3 
0.6 
0.9 
81 
-3.996 
-4.017 
-4.016 
-4.013 
-4.011 
-4.010 
-4.010 
e2 
-0.218 
-0.205 
-0.204 
-0.203 
-0.203 
-0.203 
-0.203 
Using the bivariate Normal approximation, the contours would very roughly be 
the boundaries of the 50, 75, and 95 percent H.P.D. regions, respectively. 
The contours, together with the modal values in Table 3.4.2, show how 
inferences about (8 1, ( 2) are affected by changes in f3 in the range considered. 
For this example, both the location and the shape of the contours change appreci-
ably as f3 decreases below -0.3. 
In contrast, the effect of f3 is relatively small 
in the range from - 0.3 to 0.9. Since the two columns in X are orthogonal, the 
parameters 8 1 and 82 are a posteriori uncorrelated when -f3 = O. 
Figure 3.4.2 
shows that this "orthogonality" property is gradually lost as f3 moves away from 
zero. 
In particular, the parameters 8 1 and 8 2 become increasingly negatively 
correlated and the dispersion tends to increase as fJ decreases from -0.3, namely, 
as the parent distribution becomes more and more platykurtic. 

82 
0.17 
- 0.19 
--0.21 
-0.23 
82 
0.1 7 
- 0.19 
·0.21 
. '0.23 
i) Il= - 0.9 
ii) Il= - 0.6 
iii) Il = -0.3 
iv) (3 = 0.0 
-4.05 
-4.00 
'3.95 
-'--__ 1 
I 
1 
8
1 
-4.05 
- 4.00 
--3.95 
-4.05 
- 4.00 
- 3.95 
-4.05 
- 4.00 
- 3.95 
v) Il = 0.3 
vi) (3 = 0.6 
vii) {J = 0.9 
- 4.05 
- 4.00 
L 
[ 
I 
8 
- 4.05 
-4.00 
-3.95 
I 
-3.95 
- 4.05 
-4.00 
- 3.95 
Fig. 3.4.2(i)- (vii) Contours of the posterior distribution of (8 1.82 ) for various values of f3: reaction rate data 
(the density levels are such that p (8\.82 I y)/p(8 1• 82 I y) = (0.5. 0.25. 0.05) for (A, B , C), respectively), 
.... 
~ 
C'l 
II> " 
II> 
~ .r 
2:. 
(5-
" 
IS 
:r 
II> 
C " 
~ 
~ 
~ 
&. 
~ 
-
IX> -

182 
Bayesian Assessment of Assumptions 1 
3.4 
The posterior distribution of either parameter, conditional on /3, can be 
obtained from (3.4.14) by integrating out the other, 
-w<8j <w, )=1,2,1=1,2, and )=/-1. 
(3.4.15) 
Figures 3.4.3 and 3.4.4 show respectively the marginal distributions of (JI and of 
(J2 for the seven values of /3 considered. As expected, both the location and the 
spread of the marginal distributions are quite sensitive to changes in f3 in the 
range -0.9 ~ /3 ~ -0.3. Also, thc sensitivity seems to be somewhat greater for 
(J2 than for (JI' 
16 
12 
8 
4 
(3 = - 0.9 
(3=0 3 
Fig. 3.4.3 Posterior distribution of (J J for various choices of /3: reaction rate data. 
Thus, so far as inferences about 8 J and 82 are concerned, the assumption of 
Normality would not, for the present example, lead us much astray if the true 
parent were leptokurtic. On the other hand, such an assumption could lead to 
rather erroneous conclusions if the true parent distribution were close to the 
rectangular. 
Proceeding as before, overall inferences about (JI and (J2 can be made by 
averaging the posterior distributions in (3.4.14) and (3.4.15) over the posterior 

·~.----
3.4 
Generalization to the Linear Model 
183 
40 
30 
20 
\0 
L---~~~~~--~----L---~---L--~--~~~~~--- 82~ 
-0.230 
- 0.2\ 0 
- 0. \90 
- 0\70 
Fig. 3.4.4 Posterior distribution of 82 for various choices of [3: reaction rate data. 
distribution of {J. From (3.4.11), the joint distribution of (B 1, B2) is 
(3.4.16) 
and from (3.4.15) the marginal distributions are 
. p(ej I y) cc L 
1 p(Bj I (J, y) PuC[3 1 y) p([3) d[3, 
-
00 < Bj < 00, 
j = I, 2, 
(3.4.17) 
where p.({J I y) is the posterior distribution of [3 on the basis of a uniform reference 
prior for [3, and p([3) is the appropriate prior distribution. 
The distribution 
Pu(f31 y) cc r[J + 10(1 + [3)]{f[1 + HI + [3)]}-20 1([3), 
- I < [3 ~ I, (3.4.18) 
where 
1([3) = f'<:oL: [I IYi - Bl -
B2X;\ 2/0 +P)] -10(1 +P) dBl de 2 
is shown in Fig. 3.4.5 (a = 1). We see that the mode is near [3 = 0, indicating that 
the Normal distribution is a plausible parent. However, the evidence is not very 
strong. There is a probability of about 17 % that [3 ~ 0.6 and a probability of 
about 7 % that f3 :( -0.6. Thus, parents which are quite different from the Nor-
mal cannot be ruled out from the evidence provided by pl/([31 y) alone. 

184 
Bayesian Assessment of Assumptions 1 
3.4 
a=1 
a = 3 
1.0 
1.0 
0.5 
0.5 
~ 
____ L-____ 
~ 
____ 
~ 
_____ ~~ 
·0.5 
o 
0.5 
~~ 
__ L-____ L-____ L-__ 
~~~ 
. 0.5 
o 
0.5 
a=6 
p(~iy) 
1.0 
1.0 
p( m 
0.5 
0.5 
L-~~L-____ ~ 
____ 
L-~ 
__ 
~
~ 
'0.5 
0 
0.5 
L-__ 
~L-____ L-____ ~ 
____ 
~ ~ 
'0.5 
0 
0.5 
Fig.3.4.5 Prior and posterior distributions of fJ for various choices of the parameter 
"a": reaction rate data. 
In experiments of this kind a tendency toward Normality is to be expected. 
We represent this by supposing as before that 
-1<fJ;(l, 
(3:4.19) 
where the parameter "a" can be adjusted to represent a greater or lesser central 
limit effect. The result of varying "a" is shown in Fig. 3:4.5, where p(fJ) and the 
corresponding posterior distribution of f3 
p(fJ I y) ex: p,,(f31 y)p(f3), 
-1<[J;(1. 
(3:4.20) 
are plotted for a = I, 3, 6, and 9, respectively. For a ;:: 6, the distribution practi-
cally reproduces the prior. 
For any specific value of "a" , overall inference about (8\,82) can now be 
made by averaging the conditional distributions p(8j I f3, y) over the appropriate 
p(f31 y). We have computed the marginal distributions of8\ and 82 in (3:4.17) for 
a = I, 3, 6, and 9, and specimens of the densities are shown in Tables 3:4.3, and 
3:4.4, respectively. Also shown in the same tables are densities of the conditional 
distributions p(8j I fJ = 0, y) which correspond to a --+ 00, that is, to the assump-
tion of exact Normality. These distributions are very little affected by the choice 

3.4 
Generalization to the Linear Model 
185 
Table 3.4.3 
Posterior distribution of 01 for various choices of "0" : reaction rate data 
p(G I I Y) 
01 
a = 1 
a = 3 
a = 6 
a = 9 
a -'> oc 
-4.090 
0.19 
0.19 
0.19 
0.19 
0.19 
-4.078 
0.55 
0.56 
0.57 
0.57 
0.56 
-4.066 
1.51 
1.55 
1.56 
1.56 
· 1.54 
-4.054 
3.70 
3.78 
3.79 
3.79 
3.76 
-4.042 
7.66 
7.81 
7.82 
7.81 
7.73 
-4.030 
12.88 
13.02 
13.00 
12.97 
12.82 
-4.018 
16.72 
16.71 
16.65 
16.61 
16.49 
-4.012 
17.10 
17.01 
16.95 
16.94 
16.88 
-4.006 
16.30 
16.13 
16.10 
16.10 
16.12 
- 3.994 
12.12 
11 .87 
11.85 
11.87 
11.98 
-3.982 
6.80 
6.74 
6.78 
6.81 
6.95 
-3.970 
3.10 
3.12 
3.16 
3.18 
3.27 
- 3.958 
1.23 
1.25 
1.27 
1.28 
1.31 
- 3.946 
0.44 
0.45 
0.46 
0.46 
0.47 
- 3.934 
0.15 
0. 15 
0.15 
0.15 
0.16 
Table 3.4.4 
Posterior distribution of O2 for various choices of "a": reaction rate data 
p(G 2 I Y) 
O2 
a = I 
a = 3 
a.= 6 
a = 9 
a -'> 00 
-0.2420 
0.18 
0.16 
0.16 
0.16 
0.15 
-0.2364 
0.57 
0.53 
0.51 
0.50 
0.49 
-0.2308 
1.74 
1.60 
1.56 
1.54 
1.51 
-0.2252 
4.77 
4.43 
4.34 
4.31 
4.25 
-0.2196 
11.03 
10.62 
10.50 
10.46 
10.36 
-0.2140 
21.21 
21.02 
20.92 
20.87 
20.72 
-0.2084 
32.61 
32.61 
32.54 
32.49 
32.29 
-0.2028 
37.68 
37.83 
37.85 
37.84 
37.75 
-01972 
31.57 
32.11 
32.33 
32.42 
32.64 
-0.1916 
20.0J 
20.53 
20.75 
20.85 
21.15 
-0. 1860 
10.25 
10AI 
10.48 
10.51 
10.66 
-0.1804 
4.45 
4.40 
4.38 
4.38 
4.40 
-0.1748 
1.72 
1.63 
1.60 
1.59 
1.57 
-0.1692 
0.61 
0.55 
0.53 
0.52 
0.51 
-0.163G 
0.20 
0.J8 
0.17 
0.16 
0.16 

186 
Bayesian Assessment of Assumpt ions [ 
3.5 
of "a" . Even for the extreme case a = I where p(fJ y) = p,,(fi I y), the respective 
marginal distributions of OJ are quite close to the limiting p(Oj I f3 = 0, y) distri-
butions. 
for this data, then, where weak evidence from the sample would support a 
supposition of approximate Normality, analysis with wider assumptions confirms 
the Normal theory analysis. 
3.5 FCRTHER EXTE~SJOi\ TO ~ONLINEAR 'YrODELS 
The object of much experimentation is to study the relati onship between a 
response or output variable l' subject to error and input variables ¢I' ~2 ' .. . , Sf! 
the levels of which are suitably chosen by the experimenter. Suppose the model 
can be written 
Y =I)(s I0) + 8, 
(3.5.1 ) 
wheres' = ( ~ I'" , ¢p), F,is theerrortermwithzeroexpectationand8' = (OI, .. ·,Ok) 
is the vector of k unknown parameters to be l:stimated. A model is linear in the 
parameters 0 if 
where x I' ... , X k are functions of ~ I' ... , ~ P ' Otherwise, the model is called" non-
linear" (in the parameters 0). 
Linear models were considered in the last section. 
We now show ho\\> the approach is extended to Include nonlinear models. 
In 11 experiments, let y; be the ith observation, S; = ( ~ ;I
' .. . , ~if!) the setting 
of the p input variables, and e; the corresponding ~rr o r term. We then have 
Y; = I)(S; I 0) + c," 
i = I, .. . , 11 , 
(3.5.2) 
where E(B;) = ° or equivalently E( y ;) = I)(S; I El). We shall suppose that the errors 
(1' 1' .. . , ell) are independent, each having the exponential power distribution in 
(3.4.2). Thus, irrespecti ve of whether the model (3.5.2) is linear or nonlinear in 
the parameters 0, the likelihood functions of (0, a, (3) is 
[ 
" le' 12/(1 "Pl] 
1(0. a, fJ I y) ex [w(fJ)]" a" exp - c(fJ) ;LI I ~
. 
' 
(3.5.3) 
where 
e; = y; -
E( y;) = y ; -
I)(S; I 0), 
and w({3) and c(li) are given in (3.2.5). 
If we now suppose that 0 and log a are 
approximately independent and locally uniform a priori, then for given (3 the joint 
posterior distribution of 0 and a is 
[ 
/I I e'12!( 
I +/)lJ 
p(O, a I (3, y) ex a - (" + I l exp - c(fJ) ;~I I 
~ 
, 
(J > 0, 
-
00 < OJ < 00 , 
j = I, .. , k. 
(3.5.4) 

3.5 
Further Extension to ~onlil1ear Models 
187 
On integrating out (J we obtain the joint distribution of 0 in the simple form 
-
00 < OJ < 00, 
j = I, ... , k, 
(3.5.5) 
where 
" 
M(O) = I IYi -
I)(~i I OW /( I +fJI 
i ··..: I 
and J({J) is the normalizing integral 
with 
J(fJ) = r [M(O)r""( I + 01 dO, 
JR 
R: ( -
if:; < () j < if:;, j = I, ... , k). 
Also if fJ is regarded as a random variable independent of (0, (J) a priori, the 
posterior distribution of fJ ('or a given prior pUJ) will be 
p([3 I y) if p,,([3 I y)p((i), 
-
1 <[3~
I
, 
where 
f[ I + }n(1 + IJ)] 
p,,([3 I y) Cf.... {f[ I + -}( I + 1m}" J(fJ) 
(3.5.6) 
is the posterior distribution of 13 on the basis of a uniform reference prior over 
the range (- I, I). 
Finally, when fJ is not known the marginal posterior distri-
bution of 0 is 
p(O I y) = r 
I p(O I [3, y)p(fJ I y) dl3 
if J 
II p(O I [3, y)p,,([31 y)p(fi) dli, 
-
00 < OJ < 00, j "" I, .. , k. 
(3.5.7) 
It will be noted that in the above we have not needed to assume either that 
a) the model was linear in 0; or that 
b) the error distribution was Normal. 
3.5.1 An Illustrative Example 
The following simple example, which serves to show the generality of the approach , 
concerns an experiment in a continuous chemical reactor. 
The object was to 
estimate the rate constant K of a particular chemical reaction. The quantity)' 
is the observed concentration of a reactant A in the outlet from the reactor, 
expressed as a fraction of its concentration at the inlet. The inlet concentration 
was held fixed throughout the investigation but the Aow rate was varied ['rom one 

I81! 
Bayesian Assessment of Assumptions I 
3.5 
experimental run to another. 
If the reaction is first order with respect to A , it 
is possible to show that the outlet concentration IS given by IJ = 1/( 1+ 00, where 
() is a known multiple of rhe rate constant K and ~ is the ratio of the inlet concen-
tration to the flow rate. Our problem then is to make inferences about 0, given 
data in the form of 11 pairs of values of y and ~, and given the model 
I 
E(y·) = 1)(t. 1 0) = --
, 
"1 + U~i' 
i = I, ... ,11. 
In eight runs the following results were obtained: 
y 
50 
0.78 
75 
0.69 
Table 3.5.1 
Chemical reactor data 
100 
0.54 
125 
0.57 
150 
0.54 
200 
0.37 
250 
0.40 
300 
0.32 
(3.5.8) 
We shall calculate, to a sufficient approximation, posterior densi ties of 0, p(OI/J, y), 
for Ii -> -I and /j = -0.5, 0.0,0.5, 1.0. The corresponding five parent distribu-
tions are, then, the uniform, the fourth-power distribution, the Normal distribution , 
the 4,3 power distribution , and the double exponential distribution. For clarity 
we shall set out the calculations in some detail. 
Table 3.5.2 shows values of the residuals ri = h -
I'/( ~i 10) for a suitable 
range of values of 0 at intervals or 0.025. From this table we calculate for each 
value of fJ the quantities: 
(where r L is the deviate largest in absolute magnitude). 
These five quantities provide unstandardlzed ordinates of the conditional 
posterior distributions of 0, p(O i fJ, y). for Ii -> - 1.0 and Ii = -0.5,0.0,0.5, 1.0. 
For any given value of Ii, the quantity j(fJ) = (sum of these unsta ndardized ordi-
nates x interval in 0) provides, for our present purpose,t an approximation 
sufficiently close to the normalizing integral J(fJ). On dividing the unstandardized 
ordinates through by j(/i) we obtain the approximate ordinates of the posterior 
distribution of 0 for the five selected values of fJ. 
These calculations are shown 
in Table 3.5.3 (on page 192). 
Using these values the posterior distributions of () are plotted in Fig. 3.5.1. 
For thi s example, the distributions differ quite considerably from one another, 
both in their location and shape. Inferences about 0 thus depend rather heavily 
upon the assumed form of the parent population. 
It is of interest to consider the modal values of 0 for various values of (3, that 
is, the values OW) giving maximum posterior density. 
t A considerable improvement can be obtained, of course, by using a simple numerical 
integration procedure. 

3.5 
Further Extension to Nonlinear Models 
189 
Table 3.5.2 
Values of residuals rj = Yi -
I'/(~j I e) for various values of e computed from the 
chemical reactor datat 
~ e 0.500 
0.525 
0.550 
0.575 
0.600 
0.625 
0.650 
0.675 
50 
-20 
-13 
-4 
4 
11 
19 
26 
23 
75 
-37 
-27 
-17 
-8 
0 
9 
18 
26 
100 
-127 
-116 
-105 
-95 
-85 
-76 
-66 
-57 
125 
-45 
-35 
-23 
-12 
-1 
8 
18 
28 
150 
-31 
-20 
-8 
4 
14 
24 
34 
43 
200 
-130 
-118 
-106 
-96 
-85 
-74 
-65 
-56 
250 
-44 
-33 
-21 
-10 
0 
10 
19 
28 
300 
-80 
-68 
-57 
-47 
-37 
-28 
-19 
-11 
~ e 0.700 
0:725 
0.750 
0.775 
0.800 
0.825 
0.850 
50 
39 
47 
53 
60 
66 
72 
78 
75 
34 
43 
50 
58 
65 
72 
78 
100 
-48 ' 
-40 
-31 
-23 
-16 
-9 
-2 
125 
37 
45 
54 
62 
70 
78 
84 
150 
52 
61 
69 
78 
85 
92 
100 
200 
-47 
-38 
-30 
-22 
-15 
-8 
0 
250 
36 
44 
52 
60 
67 
74 
80 
300 
-3 
6 
12 
19 
26 
32 
37 
t The entries in the table are values of r, x 100, where r, = y, - (I + 8C;,)- '. 
Reading from the graphs we find approximately: 
Parent 
Rectangular 
Fourth 
Normal 
4 power 
Double 
distribution 
power 
exponential 
Value of fJ 
-1 
- } 
0 
! 
Value of (J(fJ) 
0.70 
0.69 
0.665 
0.63 
0.595 
Thus, the value 
8(-1)=0.70 
(rectangular parent) mInImIzes the maxImum absolute 
deviations from the fitted curve, 
(J(O) = 0.665 
(Normal parent) is the least squares value and minimizes the 
sum of the squared deviations from the fitted curve, and 
e(l) = 0.595 
(double exponential parent) minimizes the sum of the 
absolute deviations from the fitted curve. 
The original data and fitted functions for these three values of e are shown in 
Fig. 3.5.2. 

190 
Bayesian Assessment of Assumptions 1 
p(B (3, y) 
20 
10 
0.50 
0.60 
3.5 
070 
0.80 
Fig.3.5.1 Posterior distribution of e for various values of p: chemical reactor data. 
Information About p 
Approximate unstandardized ordinates for PII(P I y) can be obtained by replacing 
J(P) in (3.5.6) with .1(P), so that 
(I 
• r[l + ±n(1 + P)] J( 
PII(pl y) oc {reI + W +p)]}" 
[3), 
which are shown in the last row of Table 3.5.3 and plotted in Fig. 3.5.3. From 
this sample of only 8 observations, as is to be expected, there is very little evidence 
about the form of the parent distribution. In particular, the highest ordinate of 
PII([31 y) is only about three times the lowest. To see how slight is this apparent 
preference for negative values of [3 (for platykurtic parent distributions), we can 
temporarily consider PII([3 1 y) as a genuine posterior distribution (that is, con-
sider it as if there were a uniform prior for [3). 
Then the appropriate function 
puCPI y), obtained in Fig. 3.5.3 by joining the ordinates with a dotted curve, would 
indicate a probability of about 1/3 that P was in fact positive (the parent was 
leptokurtic ). 
To obtain the marginal distribution of 0, one must integrate p(B I P, y) with 
p(PI y) as the weight function. Now the weight function is given by pCP I y) oc 
puC[31 y)p([3) and, for this example, it is the prior p([3) which dominates rather 
than Pu([31 y). Moreover, pee I [3, y) changes drastically for different p. Here then 
--.

3.5 
y 
0.60 
OAO 
{3 = \ (Double exponential parent) 
Least mean devi a! ion 
8 = 0.595 
o 
Further Extension to Nonlinear Models 
191 
o ; 0 (Norma) pHren!) 
Least squares 
e = 0.665 
0.60 
OAO 
o
·~ 
o 
0.20 '------'-_---'-_.1----'-_
-'-_
L-__ 
_ 
0.20 '----'-_ ___'__L---'-_---'-_-'--__ 
_ 
100 
200 
y 
0.60 
DAD 
300 
100 
{3 = -\ (Uniform parent ) 
Leas t m ax imum d eviate 
e = 0.70 
O. 20 '--~----'---'---'----'--~---
\00 
200 
300 
200 
300 
Fig. 3.5.2 Fitted functions using modal values of e: chemical reactor data. 
we are put on notice by the Bayes analysis that, for this example, the conclusions 
depend critically on what is assumed a priori about fJ and hence about the nature 
of the parent distribution. The data analyst is made aware that he is in a situation 
where prior assumptions about fJ (about Normality) must be given very careful 
attention, the data being too few to supply much information on their own 
account. If extensive records exist of similar experiments, he will be led to ex-
amine these to determine whether or not they suggest approximately Normal 
error structure. He will also carefully consider whether the hypothesis of many 
error sources, none of which dominate, is sensible for this experimental set-up. 
If he decides that it is sensible to use for pCfJ) a distribution centered at fJ = 0 
as in Fig. 3.5.4 . Then for this data p(fJ I y) will be much the same as pCf3). A verag-
ing pce I [3, y) with this weight function will be nearly the same as averaging 
with the prior pC[3). Thus the averaging process would lead approximately to the 
marginal distribution of e, pce I y) == pce I [3 = 0, y), and to Normal theory as 
an approximation. 
The above kind of argument could also be used if his investigation led the 
data analyst to some central distribution other than the Normal. For example, 

192 
Bayesian Assessment of Assumptions 1 
3.5 
Table 3.5.3 
Approximate ordinates of posterior distribution of e for various values of {J: 
chemical reactor data 
(J 
-1 
-1 
0 
-t 
Value of 8 
rZ 8;J(-I) (I.r 4)-2;J(-t) (I.r 2)-4;J(0) ( I.lrI4/3)-6;J(-t) (I.lrl) - 8/J(I) 
0.500 
0.0 
0.0 
0.0 
0.0 
0.0 
0.525 
0.0 
0.0 
'0.1 
0.1 
0.1 
0.550 
0.1 
0.1 
0.3 
0.6 
0.7 
0.575 
0.1 
0.2 
0.9 
2.3 
3.7 
0.600 
0.4 
0.6 
2.4 
6.7 
14.3 
0.625 
1.1 
1.8 
5.4 
9.0 
8.7 
0.650 
2.7 
4.8 
8.9 
8.2 
5.1 
0.675 
8.8 
9.8 
8.9 
5.8 
3.1 
0.700 
18.3 
12.3 
6.7 
3.6 
2.1 
0.725 
5.1 
6.2 
3.4 
1.8 
1.0 
0.750 
1.9 
2.5 
1.7 
1.0 
0.5 
0.775 
0.8 
0.9 
0.7 
0.5 
0.3 
0.800 
0.4 
0.4 
0.4 
0.2 
0.2 
0.825 
0.2 
0.2 
0.2 
0.1 
0.1 
0.850 
0.1 
0.1 
0.1 
0.1 
0.0 
J(/3) 
102.0 
13.52 
0.6045 
0.02319 
0.000804 
rei + tn(l + (J)] 
4.390 
63.07 
1,414.4 
40,320 
1 
{reJ + 'W + {J)J}" 
Approximate 
Pu({J I y) 
102.0 
59.4 
38.1 
32.8 
32.4 
(unstandard-
ized) 
------
~ 
______ ~ 
____ ~L_ 
__ ~ 
__ L_ ____ ~~~ 
-1.0 
-O.S 
0 
O.S 
1.0 
Fig. 3.5.3 Approximate ordinates of Pu({J I y): chemical reactor data. 

3.6 
-, 
Summary and Discussion 
\ 
\ 
\ 
\ 
\~P((3) 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
L-__ -L __ -L ________ L-______ -L~~ 
__ ~ (3
~ 
-1.0 
-0.5 
o 
0.5 
1.0 
193 
Fig.3.5.4 Combination of the function p.lf3 I y) with a moderately informative prior 
p(f3): chemical reactor data. 
we have said that breaking-strength test measurements often have a markedly 
leptokurtic distribution, so that in an experiment to determine the mean breaking 
strength from a sample of test pieces in which almost the only source of experimen-
tal error was the test itself, the analyst might employ a weight function p(f3) 
centered not on zero but on say 13 = 0.8. The resulting distribution pee I y) could 
then be considerably different from that obtained if the Normal were regarded 
as the most likely parent. 
It is an advantage of the Bayesian analysis that when, as in this case, there 
is little information from the data itself concerning the value of some nuisance 
parameter such as 13, then we are put on notice by evidence like that set out in Fig. 
3.5.1 that assumptions about fJ ought not to be made lightly. 
If we really do 
have some ex.ternal evidence about the parent distribution then we should use it; 
but if we do not, then the uncertainty about e must be correspondingly increased. 
This small sample situation may be contrasted with that which applies when there 
is a larger number of observations. 
In' this case the sample evidence about 13 
is much stronger and there is correspondingly less need for strong prior evidence. 
3.6 SUMMARY AND DISCUSSION 
It seems pertinent at this point to summarize and discuss the results obtained in 
this chapter. The models employed in the analysis of the three sets of data in 
this chapter are all special cases of 
y = '1(~ I 8) + e 
(3.6.1) 

194 
Bayesian Assessment of Assumptions] 
3.6 
given earlier in (3.5.1), with y the observed output, 1; a set of inputs and 9 a set 
of parameters. Specifically, the models and their special features are as follows: 
Darwin's data 
y = 8 + e 
Parameter 8 is the popula-
(Section 3.2) 
tion mean. 
Reaction rate data 
(Section 3.4) 
Chemical reactor data 
(Section 3.5) 
Parameters 8 1 and 82 appear 
linearly in regression equa-
tions. 
Parameter fi appears non-
linearly. 
The analysis of such sets of data would often be made on the assumption 
that the error e was Normally distributed. 
Bayesian analysis makes it possible 
to study the effect of wider distributional assumptions, and to illustrate this, we 
have supposed that the parent error distribution was a member of the symmetric 
family 
[ 
i
e \2/(I+P)] 
p( e) = (j)(fJ) (J - 1 ex p 
- c(fJ) -;; f 
(3.6.2) 
in (3.4.2), which includes the Normal as a special case. 
By changing fJ we are 
able to study the effect of changing the kurtosis of the parent distribution in a 
particular way. 
1. For the general class of models represented by (3.6.2), and on reasonable 
assumptions which include the idea that there is no appreciable prior knowledge 
concerning 8 or (J , the conditional posterior distribution p(8 I fJ, y) has the remark-
ably simple general fo rm 
(3.6.3) 
2. In all the examples studied , p(O I fJ, y) changes quite markedly as fJ is changed. 
Consequently, the inferences which would be made about 9 could differ mater-
ially depending on the value of fJ. 
We have contrasted this lack of inference 
robustness with the criterion robustness enjoyed by the model under certain 
circumstances [see, for example, Box and Watson (1962)]. 
3. lnferences about the value of fJ itself can be made by studying the distribution 
p(fJl y) oc pl/(fJl y)p(fJ) 
(3.6.4) 
where pl/(fJl y) is the distribution of fJ in relation to a uniform reference prior. 
4. For small samples p,,(fJ I y) will be rather flat. This indicates that, as might 
be expected, little information about the nature of a parent distribution is con-
tained in a small sample of observations. 

3.6 
Summary and Discussion 
195 
5. 
The marginal posterior distribution 
(3.6.5) 
summarizes what overall inferences might be made about 9. In this expression 
p(fJl y) ex:. PII(fJl y) p(fJ) acts as a weight function. 
6. Some caution is needed in interpreting the integration in (3.6 .5) if we need 
to rely heavily upon the form of p(fJ). However, in many examples where a central 
limit tendency is expected , the precise form of p(fJ) will not be of much importance. 
In the Darwin example, even though Pu(fJl y) is far from sharp, it is sufficient to 
ensure that extreme conditional distributions p(91 fJ, y) enter with little weight. 
Thus, even though the conditional distributions p(91 /1, y) differ markedly over 
the entire range of fJ, the inferences to be drawn about 9 do not change very much 
for reasonable choices of pCfJ). The same effect can be seen for the reaction rate 
data in Section 3.4. 
7. An important advantage of the Bayesian approach is that it makes a deeper 
analysis of inference problems possible. It does this by showing 
a) how sensitive the conditional distribution p(91 (J, y) is to changes in the 
nuisance parameter (J, 
b) what the data can tell us about the nuisance parameter (J. 
Various situations could occur: 
a) p(91 (J, y) might be very insensitive to fJ, in which case it would be apparent 
that exact knowledge of the nuisance parameter fJ was not needed. 
b) p(91 fJ, y) might be sensitive to changes in fJ, in which case the integration 
of this conditional distribution with weight function p(fJl y) u:. PII(fJl y)p(fJ) 
would need to be studied further. 
c) If PII(fJl y) was sharp (as it would be if the number of observations was large), 
then inferences would rest primarily on the information coming from the 
sample, as represented by PII(fJl y), and very little on the choices of p(fJ). 
d) If, on the other hand, p,JfJl y) was (ather flat, as is particularly true in the 
third example of the chemical reactor data in Section 3.5, then any integration 
we make would in this case depend critically on the choice of p(fJ), In this 
case then, where the inference \Vas dependent to a large degree on prior 
assumptions, this fact would be clearly pointed out to us. 
Suppose for in-
stance, that in the third example we made an outright assumption of Nor-
mality (corresponding to choosing p(fJ) to be a () function at (J = 0), then 
the Bayesian analysis would make it perfectly clear that, for this particular 
example, the conclusions rested heavily on that choice, and that we ought to 
look to its justification. 

196 
Bayesian Assessment of Assumptiuns 1 
3.7 
In general, in order to draw conclusions on topics of importance to him, the 
experimenter needs information about primary parameters (for example, 9) in 
the context of a model structure about which he is uncertain. Such uncertainties 
can sometimes be represented as uncertainties about nuisance parameters. What-
ever system of inference the investigator embraces, possible information about 
structure can either appear as prior assumptions or as information from the sample 
itself. In some instances reliance on prior assumptions could be greatly lessened 
by making proper use of appropriate information in the sample as can be done 
with Bayesian analysis. 
The investigator is often tempted to make whatever 
assumptions he thinks he needs to allow him to make inferences about primary 
parameters. Unless he has some means of keeping track of the consequences of 
such assumptions, he runs the risk of being misled. 
Bayesian analysis helps to bring out into the open how much we need to assume 
and about what. Assumptions may be 
a) necessary or unnecessary, 
b) well founded or ill founded, 
and inference procedures may be 
c) sensitive or insensitive to assumptions. 
Bayesian analysis supplies information in (a) and (c) and can draw our attention 
to the necessity for making up our minds about (b). 
3.7 A SUMMARY OF FORMULAS FOR POSTERIOR DISTRillUTIONS 
Table 3.7.1 provides a short summary of the formulas for various prior and posterior 
distributions discussed in this chapter. 
Table 3.7.1 
A summary of prior and posterior distributions 
I. Suppose the data y' = (Yt> ... , Yn) are drawn independently from the exponential 
power distribution in (3.2.5), 
[ 
I 
y - e \2/(1 + Pl] 
p(yle,(J,{3) = W({3)(J-l exp 
-c({3) -(J-
, 
-00 < y < 00, 
where 
c 
_{r[1(1+{3)]}I /(l+Pl 
((3) -
rH(l + (3)] 
and 
{r[1(1 + (3)]}1{2 
w({3) = (l + (3) {r[w + (3)J}3/2 . 

3.7 
A Summary of Formulas for Posterior Distributions 
Table 3.7 . I Continued 
2. Suppose further that the prior distribution is, from (3.2.20) and (3.2.29). 
pee, a, (J) = pce, a)p({3), 
with 
p(O, a) cc a- I 
and 
3. Given {3, the conditional posterior distribution of e is in (3.2. I 2), 
-
Cf..., < e < co, 
where 
II 
M(O) = L IYi -
0 2/ ( 1 + CJ 
and 
i = I 
In particular, for f3 = 0 .. 
8-y 
51,.'11 
~ ',,- I, 
where 
J 
j: = - IYi 
and 
2 
I 
2 
.I' 
= -- ICYi -9)· 
11-1 
II 
Also, for f3 -+ -
I 
10 -
/III 
- --' ~F 
h/(n _ 1) 
2: 2 (n- I) ' 
and 
4. The marginal distribution of {3 is in (3.2.28), 
pC{3 ; y) cc p({3)p,,({31 y) , 
-\ <{3~1, 
where 
5. The marginal distribut ion of e is from (3.2.32), 
"1 
cc J _ 
IP(81 {3, y) p"C{31 y) p({3) d{3, 
- oc < e < CO . 
197 

198 
Bayesian Assessment of Assumptions I 
3.7 
Table 3.7.1 (OI/Ii/llled 
6. The condi tional distribution p(O ! (3, y), for (3 not close to - I, can be approximated by 
the I distribution 
I{e, M(e)/d[II(1 + (3) -
IJ , 1/(1 -I- (3) -
I}, 
where & is the mode and d is a positive constant. Procedurse for computing e and d 
are given in Section 3.3. 
• 
7. For the linear model y = Xe + E in (3.4.1), where X is an 
II x k matrix, 
e ' = (0), ... , 8k ), a nd E' = (E J' . .. , E,,), the formulae are of the same form as those 
given in (2) through (6) above. The only substitutions needed are 
a) replace 8 bye, 
b) replace M(tJ) by 
M(e) = L /Yi -
X;ile/2/(1 cPI , 
; :-:::: 1 
where X;il is the ith row of X, and 
c) redefine J«(3) as 
where R: ( -
X < OJ < 00, j = I, .. , k). 
8. For the no nlinear model in (3.5.2) 
Yi = 11(~i / a) + ei' i = I, .. , 11, 
where~; = (~iJ' ... , ~ip) · The substitutions required for the formulae in (2) through 
(6) are 
a) replace 8 by a, 
b) replace M(8) by 
" 
M(a) = L /Yi -
11(~,
/ a) /2/( J "\- Pl, 
; :--- 1 
c) redefine 
where R: (-00 < OJ < OC, j = 1, ... ,k). 

A3.1 
Appendix 
199 
APPENDIX A3.1 
SOME PROPERTIES OF THE POSTERIOR DISTRIBUTION p(() I (J, y) 
In Section 3.2.3 we have asserted certain properties of the posterior distribution 
pee i (J, y) in the permissible range of {J. 
These properties follow from work on 
the sample median by Jackson (1921). For the class of parent distribu,tions givt:n 
in (3.2.3), the maxim um likelihood estimate of 0 for fixed {J, which is also the mode 
of p(O I (J, y), has been considered for certain specific choices of fJ by Turner 
(1960). In our notation , consider the function 
n 
M(e) = I iYi -
OI W " P), 
CA3.1. J) 
i :.... I 
For convenience, let us denote q = 2/( J + (J), so that 
" 
M(O) = I IYi - el
q
, 
q>1. 
(A3. J .2) 
;= 1 
I. We first show that 
a) N/(e) is convex, continuous, and for q > I, has continuous first derivative; 
b) for q> J, M(B) has a unique minimum which is attained in the interval 
[y( 1)' Y(n)l 
To see (a), consider 
i = I , 2, .. . , n. 
(A3.1.3) 
Clearly gjCB) IS convex and continuous everywhere. 
Now, suppose that q> I. 
Then for e < Yi, 
for fJ > Yi' 
g; (8) = q(O - y;)q 
I 
and as 8 approaches Yi from both directions 
lim g; (0) = lim g; (8) = 0, 
(A3.1.5) 
or Yi 
G.Yi 
which implies that g; (Yi) = O. 
Since q - I > 0, g;(O) exists and is continuous 
everywhere. Assertion (a) follows since M(8) is the sum of all gi(e). 
let us now consider M'(e). We see that for q > I, 
" 
M '(8) = q I IY(i) - olq Oi, 
(A3.1.6a) 
i ~= t 

200 
Bayesian Assessment of Assumptions 1 
where 
for 
for 
8 :::; Y(i ) 
0> Y(i) 
and Y(i)' (i = I, .. . , /1) are the ordered observations. Thus, for 8 < Y(i)' 
II 
M'(8) = -q L (Yi -
8)q- I < 0 
j :::. 1 
and for 8 > Y(II)' 
II 
M'(8) = q I (8 - yJq- I > o. 
i= I 
A3.1 
(A3.1 .6b) 
(A3.1.6c) 
Thus, by properties of continuous function, there exists at least one 00' 
Y(I):::; 00
:::; Y(II)' such that M'(80 ) = O. 
Further, it is easy to see from (A3.1 .6a) thal M'(8) is monotonically increasing 
in 8, so that M'(8) can vanish once (and only once) and that the extreme value of 
M(8) must be a minimum. This demonstrates assertion (b). 
2. 
It has been shown by Jackson that when q approaches I, in the limit the value 
of 0 which minimizes M(O) is the median of the Yi'S, if 11 is odd; and, if 11 is even, 
is some unique value between the middle two of the Yi'S. 
3. 
When q = I, it is readily seen that M(O) is minimized at the median of the y's, 
for 11 odd, and is constant between the middle two observations for n even. 
4. 
We now show that, when q is arbitrarily large 
lim [M(8)JI /q == (h + 1111 - 01), 
(A3.1.7) 
where 
Proof Consider a finite sequence of monotone increasing positive numbers {a,,} 
and a number S, such that 
where 
(
" 
) l / Q 
S = L aq 
i= 1 
= f" (:!.!...)1Jl/
Q 
Gn I 
) 
j :-: l 
a'r 
a· 
~:::;J 
a" 
for all i. 
(A3.1.8) 

A3.2 
Hence 
( 
S ) 
[n (a. )q] l.·q 
-=2::--'-
, 
an 
;= 1 
all 
so that 
log (~) = ~JOg [t (~)q] . 
all 
q 
. -1 
all 
When q --+ co, 
[ " (a. )qJ 
~~~ . log 
i~l 
a~ 
= logr, 
where I ,,;; r ,,;; n. But this implies that 
lim Jog (~) = 0, 
q- 'f 
an 
from this 
lim S = all' 
('- oc 
Thus, for any given value of 0, when q is arbitrarily large, 
lim [M(O)J l / q = max IYi - 01 
= max [ 10 -
Y( 1)1, 10 - Y(II)I] 
(
11 + (m -
(J) 
= 
h 
h + (8 -
m) 
for 
8 < m, 
for a = m, 
for a > 111. 
Hence, lim [M(a)] I 'q = (h + 1m - Ill) and the assertion follows. 
q- ce 
APPENDIX A3.2 
A PROPERTY OF LOCALLY U"lIFORM DISTRIBUTIONS 
Appendix 
201 
(A3.1.9) 
(A3.1. 10) 
(A3 .1.11) 
(A3.1.12) 
(A3.1.13) 
Let YI and Yl be two independently distributed random variables. 
Suppose Yl 
has a distri bution such that the density is approximately uniform , 
over a range of YI which is wide relative to the range in which the density of Y2 
is appreciable. Then the variable X = YJ + Y2 has a locally uniform distribution 
and is distributed approximately independently of Yl. 

202 
Bayesian Assessment of Assumptions 1 
Proof The conditional density of X given Y2 is 
I( X I Y2) = I(y! = X - Y2 I Y2) 
= pCy ! = X -
Y2) 
== c. 
A3.2 
I t foll ows that X is approximately independent of Y2 and its marginal density 
is 
reX) == c. 

--_., 
CHAPTER 4 
BA YESIAN ASSESSMENT OF ASSUMPTIONS 
2. COMPARISON OF VARIANCES 
4.1 INTRODUCTION 
In the preceding chapter, we analyzed the problem of making inferences about 
location parameters when the assumption of Normality was relaxed . We now 
extend the analysis to the problem of comparing variances. 
As was shown by Geary (1947), Gayen (1950), Box (1953a), and others, the 
usual Normal sampling theory tests to compare variances are not criterion-robust 
under non-Normality. 
That is to say, the sampling distributions of Normal 
theory criteria such as the variance ratio and Bartlett's statistic to compare several 
variances can be seriously affected when the parent distribution is moderately 
non-Normal. 
The difficulty arises because of "confounding" of the effect of 
inequality of variances and the effect of kurtosis. 
In particular, critical regions 
for tests on variances and tests of kurtosis overlap substantially, so that it is 
usually difficult to know whether an observed discrepancy results from one or 
the other, or a mixture of both [see Box, (1953b)]. 
Within the Bayesian framework we are not limited by the necessity to consider 
"test criteria." However, as we have seen, the question of the sensitivity of the 
inference does arise. 
Therefore, following our earlier work (1964a), we now 
consider the comparison of variances in a wider Bayesian framework in which 
the parent distributions are permitted to exhibit kurtosis. 
We again employ as 
parent distributions the class of exponential power distributions in (3.2.5), which 
are characterized by three parameters (fJ, 0,0'). 
The densities for samples from k such populations will thus depend in general 
upon 3k unknown parameters, 
C/31'(}I , 0'1), (/32,82, 0'2) , ... , (Pk,8k , O'k)' 
It will often be reasonable to suppose that the parameters fJI' ... , Pk are 
essentially the same, 'and we shall make this assumption. Tn what follows, the 
case of two populations will be considered first, and the analysis is then extended 
to k ~ 2 populations. 
4.2 COMPARISOJ'; OF TWO VARIANCES 
For simplicity, the analysis is first carried through on the assumption that the 
location parameters 8 1 and 8 2 of the two populations to be compared, are known. 
203 

204 
Bayesian Assessment of Assumptions 2 
4.2 
Later this assumption is relaxed. Proceeding as before, the inference robustness 
of the variance ratio CTVCTT may be studied for any particular sample y by computing 
the conditional posterior distribution p(CT ~ ': CTi I fJ, y) for various choices of f3. The 
marginal posterior distribution, which can be written as the product p(f31 y) ex 
p(fJ)PII(f31 y), indicates the plausiblity of the various choices. In particular, a 
prior distribution p(f3), concentrated about f3 = 0 to a greater or lesser degree, 
can represent different degrees of central limit tendency, and marginal posterior 
distributions p( dl CTi I y) appropriate to these different choices of p(f3) may be 
computed. 
To relax the assumption that the location parameters 81 and O2 are known 
involves two further integrations, and proves to be laborious even on a fast 
electronic computer. However, we shall show that a close approximation to the 
integral is obtained by replacing the unknown means 8 1 and 81 by their modal 
values in the integrand, and changing the "degrees of freedom" by one unit. An 
example is worked out in detail. 
4.2.1 Posterior Distribution of CT~ /CTi for Fixed Values of (8 1, 81 , {3) 
The likelihood function of (0"1 ' CT 2 , 81, 82 , f3) given the two samples 
and 
IS 
where 
and c(f3) and w(fJ) are given in (3.2.5) as 
_ I r[H I + f3)] } 1 '( I+{J ) 
c(f3) -
\ r[-HI -1-- f3)] 
, 
{rew + f3)]} 1/ 2 
w({3) = (1 +f3) {IlHI +{3)J }),2 
(4.2.1) 
We assume, as before, that the means (8 1, 82) and the logarithms of the 
standard deviations (CT I, CT2) are approximately independent and locally uniform 
a priori, so that 
or 
1 
p(cr;) ex -, 
CTi 
(4.2.2) 
i = 1, 2. 
(4.2.3) 

4.2 
Comparison of Two Variances 
205 
Further, we suppose that these location and scale parameters are distributed 
independently of the non-Normality parameter (J. 1t follows that for given values 
of 0 = (8 1, ( 2) the joint posterior distribution of ert, Cl2, and (J can then be written 
(4.2.4a) 
where p(fJ) is the prior distribution of (J and I( (J j , (J 2, (J I e, y) is the likelihood 
function in (4.2.1) when (8),8 2) are regarded as fixed. Now, we may write the 
joint distribution as the product 
p(Cl I , er 2 , (J I e, y) = p«(J 10, y)p(er), (J21 {J, e, yJ 
(4.2.4b) 
For any given value of {J, the conditional posterior distribution of CIt and 
er 2 is then 
2 
p(erl ' er21 {J, a, y) = TI p«Jj I (J,8j, Yi), 
(4.2.5) 
i= 1 
where 
( 
I fJ 8) 
- (IIi " I 1 
[
'ZC«(J)nisj«(J, 8;)] 
p (Ji 
, j, Yj oc er j 
exp 
-
er;/() + P) 
, 
(Jj > O. 
This has the form of the product of two independent inverted gamma distributions. 
By making the transformation V = d/eri and W = er 1, and integrating out W , 
we find that the posterior distribution of er~ / (Ji is 
V > 0, 
(4.2.6) 
where 
w«(J,O) = t_I_) [[-l2(n) + n2)(l + (J)J [n j S1«(J,8 t)];III( I+P) 
\ 1 + fJ 
TIj= 1 [[1nj(l + {J)] 
n2S2«(J, ( 2) 
It is convenient to consider the quantity [s) ({J, 8 I )iS2 «(J, ( 2)J V ) /( 1+ Pl, where 
it is to be remembered that V = d 'erf is a random variable and sl «(J,8 j )/S2«(J,82) 
is a constant calculated from the observations. We then have that 
[F=S)C{J, 8)V)/(l+Pl l(J e] [ 
] 
p 
S2({J, (
2
) 
, 
, y 
= P FII 1(I -'- P).n l (l +P) , 
(4.2.7) 
where p[Fn,(l + P ),1I1( 1 ~ PJJ is the density of an F variable with n 1 (I + (J) and n2 (I + (J) 
degrees of freedom. In particular, when (J = 0, the quantity 
V Lj~ I (Ytj - 8)2/nl 
Lj~ 1 (Y2j - ( 2)2 1n2 
(4.2.8) 
is distributed as F with n) and n2 degrees of freedom. Further, when the value of 

206 
Bayesian Assessment of Assumptions 2 
4.2 
{J tends to -
J so that the parent distributions tend to the rectangular form, the 
quantity 
(4.2.9) 
where (h j , h2) and (ml' n12) are the half-ranges and mid-points of the first and 
second samples, respectively, has the limiting distribution (see Appendix A4.1) 
( 
nln2 
u ,n\-I 
lim p(u I (J, e, y) = 
2(11\ + 11 2) 
p- -I 
I1 j l1 2 
u- ;"2-1 
for 
1 < U < 00. 
2(111 + 112) 
for 
0 < U < 1, 
(4.2.10) 
Thus, for given [3 not close to - I, probability levels of V can be obtained 
from the F-table. In particular, the probability a posteriori that the variance 
ratio V is less than unity is simply 
(4.2.11) 
4.2.2 Relationship Between the Posterior Distribution p( V I (J, 0, y) and Sampling 
Theory Procedures 
In this simplified situation where 81, 82, and [3 are assumed known, a parallel 
result can be obtained from sampling theory. It is readily shown that the two 
power sums 11 1S I ([3,8 1) and n 2s2([3, 82 ) in (4.2.1), when regarded as functions of 
the random variables YI and Y2' are sufficient statistics for (0"1' 0"2) and have their 
joint moment generating function 
_ 
2 [_ 
O";/(1+p)]-~n'(I+p) 
.\11 z(t I' t 2) - TI 
I 
2fi 
, 
1=1 
c([3) 
(4.2.12) 
where 
Thus, letting 
we obtain 
2 
Mz.(l j , ( 2 ) = n (1 -
2r,)--] n'( l+ PJ 
(4.2.13) 
i= 1 
which is the product of the moment-generating functions of two independently 
distributed X2 distributions, with /1J(1 + (3) and 11 2(1 + [3) degrees of freedom, 
respectively. Thus, on the hypothesis that O"~ . O"T = 1, the criterion S2 ([3,8 J/s 1 ([3,82) 

4.2 
Comparison of Two Variances 
207 
is distributed as F with n 2(l + [3) and n,(l + [3) degrees of freedom and, in fact, 
provides a uniformly most powerful similar test for this hypothesis against the 
alternative that (j~/(j7 > I. 
The significance level associated with the observed 
5 2 ([3, 8t)/Sj ((3, ( 2) is 
(4.2.14) 
and is numerically equal to the probability for V < I given in (4.2.11). The level 
of significance associated with the observed ratio 52«(3,0 j)15 j «(3, ( 2) which can be 
derived from sampling theory is, therefore, precisely the probability a posteriori 
that the variance ratio (j~, (jT is less than unity when log (jj and log (J2 are supposed 
locally uniform a priori. 
Table 4.2.1 
Results from analyses of identical samples 
[y = (percent of carbon - 4.50) x 100] 
Analyst A, 
Analyst Az 
Sample No. 
y, 
Sample No. 
yz 
1 
-8 
-10 
2 
-3 
2 
16 
3 
20 
3 
-8 
4 
22 
4 
9 
5 
3 
5 
5 
6 
5 
6 
-5 
7 
10 
7 
5 
8 
14 
8 
-11 
9 
-21 
9 
25 
10 
2 
10 
22 
II 
7 
II 
16 
12 
8 
12 
3 
13 
16 
13 
40 
Mean 
5.77 
14 
0 
15 
-5 
16 
16 
17 
30 
18 
-14 
19 
25 
20 
-28 
Mean 
6.55 

208 
Bayesian Assessment of Assumptions 2 
4.2 
4.2.3 Inference Robustness on Bayesian Theory and Sampling Theory 
The above problem of comparing two variances with the means (e 1, e2 ) known, 
is an unusual one because we have the extraordinary situation where sufficient 
statistics [n IS 1 (,8, e 
1)' n2s2 (,8, e2)J for O"~ and O"i exist for each value of the nuisance 
parameter {3. 
Inference robustness under changes of {3 can in this special case, 
therefore, be studied using sampling theory and a direct parallel can be drawn 
with the Bayesian analysis. We now illustrate this phenomenon with an example. 
0. 10 
6: 
'E'~ 0.08 
::i~ 
~ci: 
-: = 0.06 
u :; V 
u 
:::. 
~~ 0.04 
oll "'-
Vi 
0.02 
(a) 
\.0 
0.8 
>. 
q) 
""-
0.6 
",,-' 
0.4 
0.2 
- 1.0 
0 
(b) 
Fig. 4.2.1 Inference robustness: the analyst data. 
a) The sampling theory significance level for various choices of {3. This is numerically 
equal to the posterior probability that V is less than unity. 
b) Posterior distribution of {3 for given e1 and e2 when the prior distribution of f3 is 
taken as uniform (a = I). 

4.2 
Comparison of Two Variances 
209 
The data in Table 4.2.1 were taken from Davies (1949). 
These data were 
collected to compare the accuracy of an experienced analyst A l and an in-
experienced analyst A2 in their assay of carbon in a mixed powder. We shall let 
(J~ and (J ~ be the variances of Al and A 2 , respectively. The population means 
O) and 82 are, of course, unknown but for illustration we temporarily assume 
them known and equal to the sample means 5.77 and 6.55, respectively. in the 
original Normal sampling theory analysis, a test of the null hypothesis (Jt (Ji = I 
against the alternative (J~(Ji > I, yielded a significance level of 7.6%. 
. 
[n Fig. 4.2.1 (a), for each value of 13, a significance level et.(f3) based on the 
uniformly most powerful similar criterion has been calculated. The figure shows 
how, for the wider class of parent distributions, the significance level changes 
from <,bout 2.1 % for 13 close to - I (rectangular parent) to 9.9 % when 13 is close 
to I (double exponential parent). In the sampling theory framework the Normal 
theory variance ratio lacks criterion robustness to kurtosis, but the inference 
robustness now considered is of a different character in which the criterion itself is 
changed appropriately as the parent distribution is changed. 
As mentioned earlier, the significance level a(f3) has a Bayesian interpretation. 
It is the posterior probability, given 13, that the variance ratio V is less than unity. 
4.2.4 Posterior Distribution of (Ji, (J; when fJ is Regarded as a Random Variable 
Using the Bayesian approach, we can write the joint distribution of V = (J~ / (J~ 
and 13 as the product 
p(V, 1319, y) = p(V i 13,9, y)p(fJ I9, y). 
(4.2.15) 
The posterior distribution of V is then 
p( V Ie, y) = L'"J} p(V 113, 9, y)p(fJ I9, y) dfJ, 
v> o. 
(4.2.16) 
As before, the marginal distri bution of f3 which acts as the weight function can be 
written 
p(f3 l e, y) c£ p(f3)Pu(f3l e, y), 
-1
<fJ~
1. 
(4.2.17) 
From (4.2.4a), we obtain 
2 
p,,(f3 ! e, y) C£ (f[ I + HI + f3)]} - (n l +112) TI f[ I + }n j( I + 13)] [n jSj(f3, eJr : nl(1 + P) 
j =-; 1 
(4.2. 18) 
which is the posterior distribution of fJ for a uniform reference prior. 
It should be remembered that this reference prior is not necessarily one which 
represents one's genuine prior opinion; it does, however, provide a useful "way-
stage" allowing us to compute PII([J I e, y) which, when multiplied by any pnor 
p(f3) of genuine interest, yields the appropriate posterior distribution. 
The 

210 
Bayesian Assessment of Assumptions 2 
4.2 
function p/f3l e, y) can also be thought of as the likelihood integrated over 
the weight function «() I ()2) -I, which corresponds to the assumption of a uniform 
prior for log ()I and log ()2' As was seen earlier, this choice is appropriate to the 
common situation where there is little prior information about these scale para-
meters relative to that which will be obtained from the data. It is important to 
notice, however, that Pu(f3l e, y) is dependent on the choice of priors for () I and ()2' 
For the analyst example, the function PIlCf3l e, y) is shown in Fig. 4.2.1 (b). 
4.2.5 Inferences About V with 13 Eliminated 
Now although sampling theory provides, in this exceptional instance, a basis for 
determining inference robustness in the sense that we can interpret Fig. 4.2.1 (a) 
as showing the change in significance level that occurs for different values of 13, 
it does not provide us with any satisfactory basis for using sample information 
about 13. On the other hand, on Bayesian theory the overall probability 
Pr{V <119, y}= f:l1pr{V <llf3,e,y }p(f3I9, y)df3 
(4.2.19) 
can be obtained by integrating the ordinates Pr {V < 1 I 13,9, y} of Fig. 4.2.1 (a) 
with the weight function p(f3l e, y) (X p,,(f3l 9, y)p(f3). Now p/f3l e, y) is given for 
the example in Fig. 4.2.1 (b), so that to obtain Pr {V < I I 9, y} we have first to 
multiply the distribution in Fig. 4.2.1 (b) by an appropriate prior pep), and then use 
the normalized product as a weight function in the integration of the ordinates of 
Fig. 4. 2.1 (a). 
Certainly, the experimental situation here is one where we might expect a 
central limit effect, for it is likely that a large number of different components of 
error, associated with the various manipulations of the analysts, will make con-
tributions of comparable importance to the overall error. 
Thus, as before in 
(3.2.29), we introduce the flexible prior distribution for 13 having a mode at 13 = 0, 
p(f3) = w(l- f32y-l, 
-1 < 13 ~ I , 
where 
(4.2.20) 
a ~
l. 
The dotted curves of Fig. 4.2.2 show p(f3) for various choices of "a." Multiplication 
of p(f3) by Pu(f3l e, y) then produces p(f3l 9, y) shown by the solid curves in 
Fig. 4.2.2. 
With p(f3) uniform (a = I) we find Pr { V < 11 e, y} = 7.91 %, which happens 
to agree fairly closely with the value 7.59 % obtained using Normal theory. This 
is not surprising because the distribution Pu(P Ie, y) is, for this data, roughly 
centered about the value zero. The averaging of (4.2.19) can, therefore, be expected 
to yield a result close to that for Normal theory. Any injection of a sharper prior 
obtained by setting a > I and representing a prior expectation of central limit 
tendency could be expected to bring the probability even closer to that for Normal 
theory. 

4.2 
Comparison of Two Variances 
211 
To faciJitate comparison with sampling theory, we have so far calculated only 
the posterior probability that V < I , which parallels directly the significance level. 
In most problems the whole posterior density function would be of interest. 
This fuller analysis will be illustrated below when the unrealistic assumption 
that 8 1 and ()2 are known a priori is relaxed. 
1.5 
a= 1 
1.5 
p,,(ili 6, y) 
1.0 
1.0 
p(/l) 
0.5 i __ _ 
0.5 
L---~~----~------~-----/l .... 
-0.5 
0 
0.5 
L--L-L~----~------~~~_il"" 
-0.5 
0.5 
a = 3 
1. 5 
p(/l16, y) 
1.5 
1.0 
1.0 
p(/l) 
/ 
0.5 .\// 
0.5 
/ 
/ 
/ 
,/ 
13 .... 
/l .... 
-os 
0 
0.5 
-0.5 
0 
0.5 
Fig. 4.2.2 Prior and posterior distri bu[ions of f3 for various choices of the parameter 
"a": the analyst data. 
4.2.6 Posterior Distribution of V when () 1 and ()z are Regarded as Random Variables 
To concentrate attention on important issues, we supposed initially that the 
means e = (()I, ()2) were known. Within the Bayesian formulation no difficulties 
of principle are associated with the elimination of such nuisance parameters. 

212 
Bayesian Assessment of Assumptions 2 
4.2 
WiLh the prior assumptions of (4.2.2), (4.2.3) and (4.2.20), the joint posterior 
distribution of (e, O"j, 0"2' (3) is 
- 00 < Bj < 00, 
a j > 0, 
i = 1, 2, 
and 
- I < [3 :::::; 1, 
(4.2.21) 
where l(a j , a 2,8 j ,82, [3 1 y) is given in (4.2.1). 
Upon making the transformation from (e, ai' 0"2' (3) to (e, V, ai ' (3) and 
integrating out (e, a j), the posterior distribution of [3 and V can be written as 
p( V, 13 1 y) = p(V 1 [3, y)p([3 1 y). 
(4.2.22) 
The conditional posterior distribution of V for a fixed value of [3 is 
V> 0, 
(4.2.23) 
where p(V 1 [3, e, y) is the distribution given in (4.2.6) and p(e ! [3, y), the joint 
distribution of 8 1 and 82 , is 
2 
p(O 1 [3, y) = TI p(8i 1 [3, Yi) 
i ~
l 
(4.2.24) 
2 
[n jsj(j3,8ar+n,(I + P) 
TI S(I) [ 
([38)]- l n,( 1+Pl d8 
- 00 < 81 < CO, 
-00 < 82 < 00, 
;=l 
- 00 
njSj 
, 
i 
i 
with Sj([3, 8J (i = 1,2) given in (4.2.1). That is, 
where 
When the parents are Normal ([3 = 0), the quantity 
v ~ (Ylj - YI//(l1 j 
-
I) 
~ (Ylj - Y2)2/(112 -
1) 
v> 0 
(4.2.25) 
(4.2.26) 
is distributed as F with (nl - I) and (n2 -
I) degrees of freedom. Also, when the 
parents tend to the rectangular form ([3 -+ - 1), the quantity u* = V(h j ,'h2)2, 

4.2 
Comparison of Two Variances 
213 
where, as before, hi and /7 2 are the half ranges of the two samples YI and Y2, 
has the limiting distribution (see Appendix A4.l) 
lim 
p(u" I {J, y) 
{I~ -I 
{
kU~(II'-I)-1 [(111 + 11 2 ) -
(111 + 112 -
2)1I~: 2] 
ku;' \ (112-1)-1 [(111 + 11 2 ) -
(111 + 112 -
2)u;, 1'2J 
for 
0 < u* < I, 
(4.2.27) 
for 
I < u" < 00. 
with 
k = _ 111172_ 
(111 -
1)(112 -
I) 
2(111 + 11 2) (111 + 112 -
1)(111 + 112 -
2) . 
For other choice of parenb, it does not appear possible to express the posterior 
distribution of V in terms of simple functions. Methods are now derived which 
yield a close approximation to this distribution. 
4.2.7 Computational Procedures for the Posterior Distribution p( V I [3, y) 
Numerical evaluation of (4.2.25) involves, among other things, computing a 
double integral for each value of V. This is laborious even on a fast electronic 
compulcr. However, a general expression for the rth moment of V for (r < 112 '2) is 
readily obtained as 
J
'7' 
[/1 S ([3 0 )J - ? ('IJ + 2rj( 1 + P) de 
f( vr I f3 
) = k({J) - 'L 
I 
1 
, 
I 
I 
,Y 
\'00 
[ 
([30)J 
cI-II,(I +P )d(J 
. - xc nisI' I 
I 
(4.2.28) 
with 
k([3) = fU(I1\ + 2r)(1 + {3)]fU(112 -
2r)(1 + f3)J IlJ fUnj(1 + {3)]. 
Computation of the moments thus only involves the comparatively simple 
evaluation of one-dimensional integrals. It would now be possible to proceed by 
evaluating these moments and fitting appropriate forms of distributions suggested 
by the exact results obtainable when fJ = 0 and fJ --> -
I. 
However, a simpler 
and more intuitively satisfying procedure is as follows. 
Employing the approximating form (3.3.6) we may write 
i = I, 2 
(4.2.29) 
where OJ is the value which minimizes I1 js;(fJ, OJ) and is, therefore, also the mode of 
the posterior distribution of (Jj in (4.2.24). 
Substituting (4.2.29) into (4.2.28), 
the integrals in the numerator and the denominator can then be explicitly evaluated. 

---
214 
Bayesian Assessment of Assumptions 2 
Specifically, for the integrals in the numerator, we obtain 
f: 
}n l s IC{3, °1)r\'(lIl ,2r)(1 , p) dBI f 
~oo [n 2s 2(f3, 02)r'(II,-2r)(1 · Pl de 2 
= [n1s I U~, 81 )r t [(lIl + 2r)(1 +P) -' 1] d~ 1/ 2 B{t, 1-[(17 1 + 2r)(I + f3) - IJ } 
x [n 2S2(f3, 8z)r -J- [(n,- 2r)( I'P) - I] d2" 1/ 2 Brt, H(n 2 -
2r)( 1 + f3) - J]}, 
and for those in the denominator, we obtain 
inl f: 
,> [l1iS;(f3, BJr :",( I + Pl dBi = lJl [l1iSi(f3, 8;)r H II,( 1+ P) - I] 
4.2 
(4.2.30) 
x di-I/lB{L 1[ni(1 + f3)-IJ}, 
(4.2.31) 
where B(p,q) = r(p)r(q)T(p + q). 
By taking the ratio of (4.2.30) and (4.2.31), we see that the quantities d l and 
dz are cancelled. The rth moment, (r < +112 - -HI + f3) - 1), of V is, approximately, 
[{-Hen] + 2r)(1 + f3)- J]}r{}[(n 2 -
21")(1 + {3)- IJ} 
£(vr\f3,y)= 
1l~= lr {t[ni(I+{3)-I] } 
(4.2.32) 
That is, the moments of 
( 4.2.33) 
are approximately the same as those of an F variable with II 1 (I + {3) -
I and 
n2(1 + f3) - I degrees of freedom. 
Table 4.2.2 
Comparison of exact and approximate moments of the variance ratio V for the 
analyst data with various choices of {3 
f3 
-0.8 -0.6 -0.4 -0.2 
0.2 
0.4 
0.6 
0.8 
/1; 
{ exact 
2.16 
2.03 
2.07 
2.20 
2.55 
2.72 
2.89 
3.05 
aprrox. 
2.11 
2.02 
2.06 
2.20 
2.55 
2.72 
2.89 
3.04 
Jl~ 
{ exact 
5.12 
4.71 
5. I 3 
6.10 
9.00 
10.79 
12.77 
14.97 
approx. 
4.99 
4.69 
5.1 I 
6.08 
9.01 
10.79 
12.74 
14.84 
f13 
{ exact 
13.31 
12.50 
15.23 
21.24 
44.02 
62.14 
86.19 
117.63 
approx. 
13. 14 
12.49 
15.18 
2.\. I 7 
44.07 
62.14 
85.69 
115.96 
f1~ 
{ exact 
38.04 
38.16 
54.53 
93.54 
300.60 
525.11 
896.08 
1497.80 
approx. 
39.00 38.44 
54.51 
93.36 
301.04 
524.67 
888.07 
1465.83 
-
. 
-

4.2 
Comparison of Two Variances 
215 
Table 4.2.2 shows, for the analyst example, the exact and approximate first 
four moments of V for various value's of [3. The exact moments were obtained by 
direct evaluation of (4.2.28), using Simpson's rule, and the approximate moments 
were computed using (4.2.32). 
The moments approximation discussed above suggests that the quantity in 
(4.2.33) is approximately distributed as an F variable with 11 1(1 + [3) -
1 and 
11 2(1 + [3) -
I degrees of freedom. This means that in (4.2.23) the process of 
averaging p(V 1[3, e, y) over the weight function pee 1[3, y) may be approximated 
by simply replacing the unknown (8 1, 02) in p(V 1[3, e, y) by their modal values 
and reducing the degrees of freedom by one unit. That is 
p(V I [3, y) 
where 
w([3,e) = f_l_) f~Hnl + 112)(1 + [3) -I] [nISI([3'~I)] :[II I(I+P) - I). 
\l + f3 
TIi= 1 f{}[l1i(l + [3) -IJ } 
/12 S2([3,82) 
This approximating distribution can be justified directly by using the expansion 
(4.2.29) to represent the quantities 11 151([3,8 1) and /1252([3, O2) in (4.2.25) and 
integrating directly. We have followed through the moment approach here because 
the exact moments can be expressed in rather simple form and, as we pointed 
out, could be used, if desired, to fit appropriate distributions. 
It should be noted that in obtaining the approximating moments (4.2.32) 
and the distribution (4.2.34) through the use of the expansion (4.2.29), it is only 
necessary to. determine the modes iJ l and 82 , The more difficult problem of finding 
suitable values for d l and d 2 (see Section 3.3) does not arise here because of the 
cancellation in the integration process. 
Although calculating the modal values 
(8 1,82) still req uires the use of numerical methods such as those discussed in 
Section 3.3, this is a procedure of great simplicity compared with the exact 
evaluation of multiple integrals for each V in (4.2.25). 
Table 4.2.3 shows exact and approximate densities of V for several values of 
f3 using the analyst data. The exact densities are obtained by direct evaluation 
of (4.2.25) using Simpson's rule, anu the approximate densities are calculated 
using (4.2.34). Some discrepancies occur when [3 = -0.8 (that is, as [3 approaches 
- I) but for the remainder of the (3 range the agreement is very close. 
4.2.8 Posterior Distribution of V for Fixed /3 with 111 and 82 Eliminated 
For the analyst data, the graphs of p( V i /3, y) are shown in Fig. 4.2.3 for various 
choices of {3, where, this time, the simplifying assumption that the location para-
meters 8 I and O2 are known and equal to the sample means was not made. 
In 
computing the posterior distributions of V for [3 = -0.6 and [3 = 0.99, 8 1 and 

216 
Bayesian Assessment of Assumptions 2 
4.2 
O2 were eliminated by actual integration and also by the approximate procedure 
mentioned above. In each of these two cases, the curves obtained by the two 
methods are practically indistinguishable. For the cases f3 = 0 and f3 = - I, the 
exact distributions were calculated directly using (4.2.26) and (4.2.27), respectively. 
Table 4.2.3 
Specimen probability densities of the variance ratio V for various values of {J: 
the analyst data 
V 
{J= -0.8 
f3= -0.6 
(3= -0.2 
exact 
approx. 
exact 
approx. 
exact 
approx. 
0.5 
0.005 
0.020 
0.017 
0.021 
0.048 
0.049 
1.0 
0.099 
0.159 
0.229 
0.245 
0.299 
0.301 
1.2 
0.209 
0.264 
0.378 
0.388 
0.389 
0.390 
1.4 
0.353 
0.388 
0.507 
0.510 
0.444 
0.444 
1.6 
0.503 
0.510 
0.583 
0.580 
0.461 
0.46J 
1.8 
0.619 
0.597 
0.594 
0.588 
0.449 
0.448 
2.2 
0.628 
0.579 
0.474 
0.468 
0.372 
0.371 
2.6 
0.412 
0.380 
0.301 
0.297 
0.276 
0.275 
3.0 
0.206 
0.194 
0.168 
0. 167 
0.192 
0.192 
4.0 
0.026 
0.028 
0.032 
0.032 
0.070 
0.069 
V 
{J = 0.2 
{J = 0.6 
[3 = 0.8 
exact 
approx . 
exact 
approx. 
exact 
approx. 
0.5 
0.070 
0.069 
0.090 
0.089 
O.JOO 
O.JOO 
1.0 
0.274 
0.274 
0.258 
0.257 
0.253 
0.253 
1.2 
0.329 
0.329 
0.294 
0.293 
0282 
0.283 
1.4 
0.359 
0.359 
0.311 
0.31 J 
0.294 
0.295 
1.6 
0.367 
0.368 
0.313 
0.313 
0.295 
0.296 
1.8 
0.359 
0.360 
0.305 
0.306 
0.287 
0. 287 
2.2 
0.3J4 
0.315 
0.272 
0.273 
0.256 
0.257 
2.6 
0.255 
0.256 
0.23\ 
0.23J 
0.219 
0.219 
3.0 
0.200 
0.200 
0.190 
0.190 
O. J 83 
0.183 
4.0 
0.100 
0.100 
0.110 
0.110 
O.IJ \ 
0.111 
We see from these graphs how the posterior distribution of the variance 
ratio V changes as the value of f3 is changed. When the parents are rectangular 
({J -> -I), the distribution is sharply concentrated around its modal value. As 
f3 is made larger so that the parent distribution passes through the Normal to 
become Jeptokurtic, the spread of the distribution of V increases, the modal value 
becomes smaller, and the distribution becomes more and more positively skewed. 
It is evident in this example that inferences concerning the variance ratio depend 
heavily upon the form of the parent distributions. 

-
-
4.2 
Comparison of Two Variances 
p ( V[ Il , y) 
0.7 
0.6 
O.S 
0.4 
0.3 
0.2 
0.1 
o 
/I , \ , ' 
I 
' , ' 
I 
' 
I 
' 
I 
' 
, ' 
, ' 
I 
I 
1l ~-0 .6 _____ /·\' 
I..-- Il ~ - [.0. 
--,
.' 
I 
i~· 
\ 
. 
1\ 
, 
I 
I ' 
I 
. 
I \ 
, 
I 
I' 
I 
. I' , 
I 
I 
" 
' 
. 
I 
. 
I 
\ 
, 
\ 
\ 
\ 
\ 
\ \ 
. 
I 
1/ , ...... , 
' 
II 
I 
" 
. 
\ 
,,\ \ 
/',' 
(l~099 
\. \ 
\ \ 
~ 
. \ 
......... 
\\ 
......... 
\' 
......... 
. , -----
~~ 
....,;:,""-. 
~~~-L----~--____ ~ 
____ 
-L~~ 
__ ~ 
____ 
~V~ 
2 
J 
4 
6 
217 
Fig. 4.2.3 Posterior distribution of V for various choices of fJ: the analyst data. 
4.2.9 Marginal Distribution of 13 
The margihal posterior distribution of f3 , p(f31 y), with (9, (fl' (f2) eliminated may, 
as before, be written as the product 
p(f31 y) oc PII(f31 y)p(f3), 
- I < B ~ I, 
(4.2.35) 
where 
(13 I) TIl ~ I l[ J +inJ I + 13)] TI2 f" [ (13 ()] - !-II,(I +P) d() 
Pu 
y oc {r[J + .HI + f3)] }", +II' i = 1 
-co nisi 
, 
i 
i 
is the posterior distribution of f3 relative to a uniform prior for - 1 < f3 ~ I. 
For the analyst example, the distribution PIICf3l y) shown in Fig. 4.2.4 is very 

218 
Bayesian Assessment of Assumptions 2 
4.2 
little different from Pu(/J 19, y) shown in Fig. 4.2.1 (b). As before, the mode of the 
distribution is close to fJ = o. 
'---'_.......l._...J..._....I...._...l...-_.l...---'_.......l._...J...._....I....il.... Box a Tiao 
1.0 
4.2-4 
Fig. 4.2.4 Posterior distribution of fJ for a uniform reference prior : the analyst data. 
4.2.10 Marginal Distribution of V 
The main object of the analyst investigation was to make inferences about the 
variance ratio V . This can be done by eliminating fJ from the posterior distribution 
of V by integrating the conditional distributionp(V I fJ, y) with the weight function 
p(PI y), 
J
+l 
p(VIY)= _IP(VIP,y)p(Ply) dP, 
V> O. 
(4.2.36) 
Fig. 4.2.5 shows the distribution of V for a uniform reference prior in fJ, that is, 
with the weight function puCfJl y) in (4.2.35). Also shown by the dotted curve is 
the posterior distribution on the assumption of exact Normality. Any prior on 
P which is more peaked at P = 0 can be expected to produce even greater agreement 
with the Normal theory posterior. For this particular set of data, therefore, the 
Normal theory inference evidently approximates very closely that which would be 
appropriate for a wide choice of "a" in (4.2.20). This agreement is to be expected 
because, in this particular instance, the sample evidence expressed through 
puCPI y) (see Fig. 4.2.4) indicates the Normal to be about the most probable 
parent distribution. However, such agreement might not be found if the sample 
evidence favored some other form of parent. 

4.3 
p(V/y) 
0.4 
0.3 
0.2 
01 
Comparison of the Variances of k Distributions 
--- Distribution with uniform 
reference prior on (3 
------
Distribution under 
assumption of NORMALITY 
~~---L 
______ L-____ -L ______ 
~ 
____ -L ____ 
~V~ 
o 
234 
6 
219 
Fig. 4.2.5 Posterior distribution of V for a uniform reference prior in f3 : the analyst data. 
4.3 COMPARISON OF THE VARIANCES OF k DISTRIBUTIONS 
We now consider the problem of comparing the variances of k exponential power 
distributions when k can be greater than two. As mentioned earlier, we shall 
assume that the k populations have the same non-Normality parameter f3, but 
possibly different values of 8 and a. When independent samples of size n l , .
. , nk 
are drawn from the k populations, the likelihood function is 
where 
II k 
_II, 
[}C(f3)l1 js/f3, 8j)] 
/(<r, e,f3Jy) = [w(f3)] jIJ a j 
exp 
-
a7 /(I+P) 
, 
Yj = (Yj ! , ... ,Yjll), 
k 
11 = I 
I1 j , 
j= 1 
and c(f3) and w(f3) are given in (4.2.1). 
4.3.1 Comparison of k Variances for Fixed Values of (e, f3) 
(4.3.1 ) 
At first, we suppose that f3 and the location parameter e in each population are 
known. With 
or 
I 
p( aj) ex -- , 
aj 
i = I, .. . , k 
(4.3.2) 

220 
Bayesian Assessment of Assumptions 2 
4.3 
and for given (9, /3), the posterior distribution of (J is 
k 
p( (J I 9, /3, y) = TI pC (Ji I fJi' /3, y;), 
(Ji>O, i=l, ... ,k, 
(4.3.3) 
;=1 
where 
p( (Ji I fJi' /3, Yi) CC (Ji-(Il,:· I) exp [ - lcC/3)nisi(/3, fJ;)/ (J;/(1 +PJ]. 
The joint distribution for «(JI> ... , (Jk) is thus a product of k independent inverted 
gamma distributions. 
In a certain academic sense, the obtaining of this joint 
distribution solves the inference problem. 
However, there is still the practical 
difficulty of appreciating what this k dimensional distribution has to tell us. 
One question of interest is whether a particular parameter point is, or is not, 
included in an H.P.D. region of given probability mass. For instance, we may be 
interested in considering the plausibility of equality of variances. Following our 
earlier argument in Section 2.12, we consider the joint distribution of the Ck - 1) 
contrasts <\>' = CrPI, ... , rPk-l) in the logarithms of the standard deviations (J, 
i=I, ... ,k-l. 
C 4.3.4) 
It is straightforward to verify that the posterior distribution of <\> is 
9 
_ 
["Un(l + /3)] 
pC<\>I ,/3,y) - (l + /3yk-1JTI~=1 ["[-}n;(l + /3)J 
-00 < rPl < 00, ... , -00 < rPk-l < 00, 
(4.3.5) 
where 
Vi = l1iSiC/3, fJ;) exp [ -rPd(l + /3)J, 
I1kSkC/3A) 
i = 1, ... , (k -
1). 
Except for the factor (1 + f3) - I, this distribution is in precisely the same form as 
the distribution in (2.12.6), with Vi . l1iC 1 + /3), and s;C/3, fJ;) replacing T;, Vi' and s;, 
respectively. 
From the developments in Section 2.12 it follows, in particular, 
that the distribution C4.3.5) has a unique mode at 
i=I, ... ,k-l. 
(4.3.6) 
Further, in deciding if a particular point <\>0 is included in the H.P.D. region of 
content (1 - a), we may make use of the fact that p(<\> I 9, /3, y) is a monotonic 
decreasing function of M, where 
M = -210g W, 
(4.3.7) 
with 
n"l:Il( I + P) 
[k - I 
J . 
W = 
TI V ~ Il,(1+P) (1 + V + ... + V 
)-i ll(l+ P) 
rI~ = lnt";(l+P) ;=1 
I 
1 
k-l 
( 4.3.8) 

4.3 
Comparison of the Variances of k Distributions 
221 
The cumulant generating function of M , is, for (3 i= -I, 
k -1
' 
.JO 
1-.,11(/) = a - --log (l -
21) + L (,(r(I - 2t) -(2r-l), 
2 
r~ 1 
(4.3.9) 
with 
CI. 
= 
B2r 
(_2_)2r - 1 [ t n.-(2r-l) _ n- (2r - I)] 
r 
2r(2r -
1) 
1 + [3 
i~ 1 
/ 
and "a" is a constant. Bartlett's method of approximation discussed in Section 
2.12 can be extended here so that 
Pr {p(<I> 19, (3, y) > p(<I>o I e, (3, y) I y} = Pr {M < -210g Wo} 
. 
{2 
- 2 log Wo} 
= Pr Xk - 1 < 
I + A 
' 
(4.3.10) 
where 
A = 3(k _ 1~(l + [3) Ltl I1
j
-
1 
- n-
11 
and Wo is obtained by inserting a particular parameter point of interest <1>0 in 
(4.3.8). 
In particular, if <1>0 = O. so that 0'1 = 0'2 = ... = O'k> then -2 log Wo 
reduces to 
k 
- 210g Wo = - I I1j( I + [3)[log Sj(fJ, Bi) -
log 5([3, 9)J, 
(4.3.11) 
i= 1 
with 
In this special case where the 8'5 are assumed known, exactly parallel results 
can be obtained within the sampling theory framework. In fact, the likelihood 
ratio criterion for testing the null hypothesis (Ho: 0'1 = 0'2 = ... = ak ) against 
the alternative HI that they are not all equal, is 
_ k [S;([3, Bj)] -
):II,(l +P) 
X(fJ) - 1] 
-([3 9) 
. 
/- 1 
S 
, 
(4.3.12) 
Thus - 210g X(m is given by (4.3.11) and on the null hypothesis, the cumulant 
generating function of the sampling distribution of - 210g ),([3) is precisely that 
given by the right-hand side of (4.3.9). It follows that the complement of the 
probability (4.3.10) is numerically equivalent to the significance level associated 
with the observed likelihood ratio statistic },([3). 
An Example 
For illustration, consider the three samples of 30 observations set out in Table 4.3 .1. 
These observations were, in fact, generated from exponential power distributions 

222 
Bayesian Assessment of Assumptions 2 
4.3 
with common mean fi = 0 and fJ = - 0.5. 
Their standard deviations a j were 
chosen to be 0.69, 0.59, 0.76, respectively. The analysis will be made assuming 
the means to be known. 
Table 4.3.1 
Data for three samples generated from the symmetric exponential power distribution 
with common fJ = -0.5 and fi = 0 
Sample 
Observation 
One 
Two 
Three 
number 
(a = 0.69) 
(a = 0.59) 
(a = 0.76) 
0.23 
-0.84 
0.90 
2 
0.24 
0.61 
-0.26 
3 
-0.26 
0.20 
-0.03 
4 
-1.13 
-0.83 
0.04 
5 
-0.72 
0.46 
0.73 
6 
-0.52 
-0.38 
-0.06 
7 
0.65 
0.13 
-0.91 
8 
-1.16 
0.33 
-1.38 
9 
0.25 
-0.20 
1.24 
10 
-0.83 
-0.45 
-0.09 
11 
0.90 
1.16 
-0.40 
12 
-0.28 
-0.55 
-1.32 
13 
0.19 
0.42 
-0.57 
14 
-0.43 
-0.73 
-1.34 
15 
-1.12 
0.37 
0.97 
16 
-0.93 
-0.37 
0.68 
17 
-0.58 
-0.50 
0.70 
18 
0.62 
-0.50 
-0.52 
19 
0.61 
-0.59 
0.34 
20 
0.42 
-0.77 
-1.00 
21 
-1.34 
-0.99 
-1.28 
22 
-0.l3 
0.06 
0.21 
23 
-0.92 
0.46 
1.42 
24 
-1.24 
0.03 
0.31 
25 
-1.02 
-1.01 
-0.31 
26 
-0.40 
0.68 
-0.14 
27 
-0.70 
-0.28 
0.99 
28 
0.10 
-0.62 
1.14 
29 
0.38 
-0.70 
0.21 
30 
-0.73 
-0.32 
-0.76 
-~--'----
-

4.3 
Comparison of the Variances of k Distributions 
223 
<P2 
<P 2 
0.5 
13 = -0.5 
0.5 
13=0 
0 
0 
0 
-O S 
-0.5 
~ 
-1 .0 
95% 
-1.0 
90% 
-1.5 
75,1. 
-1.5 
75 <;1 
<PI 
<PI 
-1.0 - 0.5 
0 
0.5 
1.0 
-1.0 -0.5 
0 
0.5 
1.0 
<P2 
0.5 
13= 05 
0 
-0. 5 
-1.0 
-1.5 
<PI 
-1.0 -0.5 
0 
OS 
1.0 
Fig. 4.3.1 Contours of the posterior distribution of (cp l' <P2) for various values of {3: the 
generated example. 
The first object is to study how inferences about the comparative values of (Jj 
would be affected by changes in the val ue of {3. For fixed {3, the posterior distribution 
of the two contrasts 
and 
is that given in (4.3.5) with k = 3. Figure 4.3.1 shows for {3 = -0.5, 0.0, and 0.5 
the mode and three contours of the posterior distribution for cP I and CP2' 
These 
contours, which correspond approximately to the 75, 90, and 95 % H .P.D. regions, 
were drawn such that 
M = -2log W = (1 + A)/(2, IX), 
(4.3.14) 

22<1 
Bayesian Assessment of Assumptions 2 
4.3 
for ex = 0.25, 0.10. 0.05, respectively. For this example, 
M = -90(1 + ,8) log 3 + 90(1 +,8) Jog (1 + exp{ - (I + ,8)-1[4>1 - ¢ l C,8)] } 
+ exp{ -(I + ,8)-1[4>2 -
¢2 (,8)] }) 
+ 30{[4>1 - ¢ [(,8)] + [4>2 - ¢2{P)]} 
(4.3.15) 
and 1 + A = 1 + (1 + ,8) - [(0.0148). 
Inspection of Fig. 4.3.1 shows that in all cases ¢[ and 4>2 are positively 
correlated. This might be expected, for although 0'7, O'~, and 0'; are independent 
a posteriori, ¢1 = log O'i -
log ()' ~ and 4>2 = log O'~ -
log O'~ contain log O'~ in 
common. Also, for this particular set of data, the mode of the posterior distribution 
is not much affected by the choice of ,8, and the roughly elliptical shape of the 
contours is similar. The distributions differ markedly, however, in their spread ; 
the dispersion becoming much larger as ,8 is increased . Inferences about the 
relative values of the variances are for this reason very sensitive to the choice of ,8. 
The possibility that the variances are all equal so that Ij> = 0, is often of special 
interest to the investigator. Figure 4.3.1 shows that the parameter point Ij> = 0 
is just excluded by the approximate 95 % H .P .D . region for ,8 = -0.5, but lies 
well inside the 90 % region for ,8 = 0, and the 75 % region for,8 = 0.5. 
To 
present a more complete picture, we may calculate, as a function of ,8, the 
probabil ity associated with the region 
p(<jll 0, ,8, y) < p(<\> = 0 I e, ,8, y). 
(4.3.16) 
For this example, we have from (4.3.10) and (4.3.11), with k = 3, 
{
-log Wo 
} 
Pr {p(1j> 10, /3, y) < p(<I> = 0 I e,,8, y) I y} == exp I + (1 + ,8)-1(0.0148) ,(4.3.17) 
with 
3 
log Wo = 15(1 + ,8) I [log Sj(,8, 8j ) 
-
log 5(,8, e)]. 
j= 1 
Figure 4.3.2 shows this probability for various values of ,8 ranging from -0.9 to 
1.0. It is less than I % for ,8 = -0.9, monoto nically increasing to 21 % for ,8 = 0, 
a nd to almost 58 % for ,8 = I. As mentioned earlier, for a fixed value of,8, (4.3.17) 
is numerically identical to the significance level associated with the likelihood 
ratio criterion for testing the null hypothesis that the variances are equal. Inferences 
about the equality of the variances are thus very sensitive to changes in /3, 
irrespective of whether we adopt the sampling or the Bayesian theory. 
4.3.2 Posterior Distribution of ,8 and <I> 
As before, the non-Normality para meter ,8 ca n, in the Bayesian framework , be 
included in the formulation as a variable parameter. Assuming that ,8 and the 
standard deviations 0' are a priori independent, we may write 
p(/3, a) = p(,8)p( a), 
(4.3.18) 

4.3 
Comparison of the Variances of k Distributions 
225 
where pCP) is the prior of P and p(cr) is the distribution in (4.3.2). 
Combining 
(4.3.18) with the likelihood function in (4.3.1) and integrating out cr. for fixed 6 
the posterior distribution of P is obtained as 
p(f31 fl, y) rx p(f3)pJP I fl, y), 
-1 <P~l. 
(4.3.19) 
In this expression 
puCP 16, y) rx [r (I + 1 + P)] -nfI r [1 + !2 (I + P)] [nisi(p, 8i)] -~. " ; ( 1 +fJ) 
2 
r =[ 
2 
(4.3.20) 
is the posterior distribution of P corresponding to a uniform reference prior. 
0.6 
0.4 
0.2 
1.0 -0.8 
- 06
0.4 
-0.2 
o 
0.2 
0.4 
0.6 
0.8 
Fig. 4.3.2 The posterior probability Pr { p(<j> I e, P, y) < p(<j> = 0 I e, P, y) I y} as a 
function of P for the generated example; the probability is numerically equivalent to the 
sampling theory significance level. 
Figure 4.3.3 for a = I shows the distribution piP I fl, y) for the data of Tabk 4.3. 1. 
Its close concentration about the mode P = -0.7, shows it to be quite informative 
about P and this is to be expected because of the large number of observations 
(n = 90). Even after multiplication by a prior distribution p(P) which indicated a 
rather strong belief in a central limit tendency, inferences about P would still not 
be much different from that based upon pf/C{31 fl, y). In each of the four sets of 
graphs in Fig. 4.3.3, the dotted curve is the assumed prior and the solid curve 
the corresponding posterior distribution. 
The prior distributions were taken 
from (4.2.20) with a = I, 3, 6, 10, respectively. 
Thus with "a" as high as 10, 
the information from pf/(fJ I fl, y) still dominates the "prior" information. 
This 
figure may be contrasted with Fig. 3.2.5 for which only 15 observations C differences) 
were available and the information from Pu({3 I y) had much less weight. 

2.S 
2.S 
p(lli /) , y) 
2.0 
/ 
2.0 
/' 
1.5 
I.S 
I \ 
~ 
I 
\ p((l) 
/ 
"-
I 
\ 
I 
\ p(jJ) 
\ 
I 
\ 
\ 
1.0 
I 
\ 
1.0 
\ 
\ 
\ 
\ 
\ 
\ 
O.S 
\ 
0.5 
\ 
\ 
\ 
, 
\ 
\ 
, 
"-
\ 
"-
{3~ 
il~ 
·O.S 
0 
O.S 
-O.S 
0 
O.S 
(e) 
(d) 
Fig. 4.3.3 Prior and posterior distribution of f3 for various choices of the parameter 
"a": the generated example. 

4.3 
Comparison of the Variances of k Distributions 
227 
Posterior Distribution of <I> and its Approximation 
In the Bayesian framework, uncertainty in the inferences about <I> due to changes 
in {3 can be removed by considering the marginal posterior distribution of <1>. 
From the distribution of <1>, given {3, in (4.3.5) and that of {3 in (4.3.19), we may 
write 
p(<I> 19, y) = f 1 p(<I> 19, {3, y)p(fJ 19, y) dfJ, 
-00 <<p;<oo, i=l, ... ,k-1. 
(4.3.21) 
Although both the distribution p(fJ 19, y) and the conditional distributiGn 
p(<I> 19, P, y) involve only simple functions of the observations, it does not seem 
possible to express the marginal distribution in a simple closed form. For k ~ 3, 
complete evaluation of the distribution would be very burdensome even on a 
fast computer. 
However, in situations in which the distribution of P is sharp 
and nearly symmetrical, we can write, approximately, 
p(<I> 19, y) == p(<I> 19, jJ, y) 
(4.3.22) 
where jJ is the mode of p(PI 9, y). 
For instance, the posterior distribution 
Pu(PI 9, y) based upon a reference uniform prior in f3 has its mode close to f3 = - 0.7 
for the present example. 
Using the approximation in (4.3.22), the marginal 
posterior distribution is thus nearly p(<I> 19, fJ = -0.7, y). 
Contours of this 
approximate distribution are shown in Fig. 4.3.4 . 
o 
-{l.S 
- 1.0 
- 1.5 
• 0 
~75 
.. ~: 
U
90o/, 
9S'li 
Fig. 4.3.4 Contours of the approximate posterior distribution of (<PI' <P2)' P(<PI' <P2 I 9, y) 
== P(ePl' <P2 19, fJ = -0.7, y): the generated example. 
Accuracy 0/ the Approximation 
To check the accuracy of the modal approximation, the exact marginal distributions 
may be compared with the approximate marginals implied by (4.3.22). 
From 

228 
Bayesian Assessment of Assumptions 2 
4.3 
(4.3.5), it is straightforward to verify that, given [3, the distribution of ¢i is 
(-1,.10 [3 
) = 
rU(ni + nk)(1 + [3)] 
u ~ n;(l +P) (I + U)- i-(n;+nk )(l +P) 
p '1', 
, 
, Y 
(1 + [3)fUni(l + (:?)]f[}nk(l + [3)] 
, 
, 
-0') < <Pi < 00, 
(4.3.23) 
where 
Vi = nis;(f3, 8J exp [ - (l + [3) - 1¢;], 
nksk([3,8k) 
Thus, the marginal distribution of ¢i is 
i=I , ... ,k-1. 
p(¢; 1 0, Y) = f I p(<p; I o. {3, y)p(f31 e, y) df3 
(4.3.24) 
which, by adopting the same argument leading to (4.3.22), is approximately 
p(¢; 1 0, y) == p(<p; 1 e ,~, y). 
(4.3.25) 
For example, the solid curve in Fig. 4.3.5 shows the exact distribution of ¢t for 
the generated data based upon PII({31 0, y), and the dotted curve in the same 
figure is the corresponding approximate distribution. 
The agreement between 
the two is fairly close. 
4.3.3 The Situation When el , . . . , ek Are Not Known 
In the above, we have assumed that the location parameters el , . .. , ek are known. 
The more common situation is when they are not known. With the assumption 
that 0, 0", and f3 are a priori approximately independent, and that 
k 
p(S) = TI pee;) 
where 
i = 1 
p(1)t :11, y) 
2.0 
1.5 
1.0 
0.5 
Exact 
----- Approximate 
-=~~----~---J----~----~--~----==~1>I~ 
·0.80.6 
- 0.4 
0 
0.2 
0.4 
Fig.4.3.5 Posterior distribution of ¢l: the generated example. 
(4.3.26) 
Box a Tioo 
4.3-5 

4.3 
Comparison of the Variances of k Distributions 
229 
the posterior distribution of e, for a given p, is 
k 
pee I p, y) = TI pee; ! {3, y;), 
-00 < 0; < Xl, 
i = I, ... ,k, 
(4.3.27) 
j ;:;; t 
where 
Consequently, for a given {3, the posterior distribution of the (k -
J) contrasts 
4> defined in (4.3.4) becomes, 
< 4>; <00, i=I, .. ,k-1 
(4.3.2R) 
where the first factor in the integrand is given by (4.3.5). 
[n the special case 
where the populations are Normal, it is readily verified that the integral can be 
evaluated exactly yielding, 
(4.3.29) 
where 
V; = n; -
I, 
v = n -
k , 
and 
"II, ( 
- )2 
T -
L 
Yij -
Y; 
-1>, 
i -
~ I/.: 
2 e 
) 
L 
(Ykj -
Yk) 
which is, of course, the same distribution obtained earlier in (2.12.6). 
In the more general situation when the parent populations are not necessarily 
0Jormal, it does not seem possible to express the integral exactly in terms of simple 
functions. However, when dealing with the ratio of two variances, it was demon-
strated that the effect of integrating over the posterior distribution of (8 1, ()2) was 
essentially to replace the 8's by their corresponding modal values and to reduce 
the "degrees of freedom," n;(1 + ,8), (i = 1, 2), by one unit. We can extend this 
approximation to the more general case where again it is exact if {3 = O. 
As in 
(3.3.6) we write 
i = I, ... , k, 
(4.3.30) 
where 8i is the value of 0; minimizing I1;S;(fJ, O;) and, therefore, is also the mode 
of the posterior distribution p(G;: {3, )'J in (4.3.27). 
Substituting (4.3.30) into 

230 
Bayesian Assessment of Assumptions 2 
4.3 
(4.3.28) and integrating out e, the posterior distribution pC<\> 1/3, y) is then 
approximately 
-00 < <PI < ce, ... , -00 < <Pk-I < CO, 
(4.3.31) 
where 
In j = f1j(l + /3) - I, 
In = n(l + /3) - k, 
and 
i=I, ... ,k-\' 
As ill (4.3.24) for the case of the comparison of two variances, in obtaining 
the approximate distribution (4.3.31) through the use of (4.3.30), it is only necessary 
to determine the modal values e;, the quantities d; being cancelled in the integration 
process. The distribution (4.3.31) is of exactly the same form as that in (4.3.5). 
Consequently, to this degree of approximation, the cumulant generating function of 
I'v/* = -210g W* 
(4.3.32) 
where 
is given by 
k-I 
(I" 
KM.(t) = a - --log (I -
2/) + I C(r(l -
2t)-(2r-l l, 
2 
r = 1 
(4.3.33) 
where 
_ 
B2r 
22r-1 [ ~ 
-(2r-l) _ 
-(2r-))] 
ar -
L. In; 
m 
. 
2,.(2r - I) 
; ~ I 
Hence the distribution of M* can, as before, be approximated using Bartlett's 
method. 
To decide whether a parameter point <Po is included in the H.P.D. region of 
content (I - a), we calculate 
" 
* 
(, 2 
- 2 Jog W6} 
Pr{M > -210g Wo } = Pr lXk-1 < 
, 
. 
[+ A* 
(4.3.34) 
where Wo* is obtained by substituting <Po into (4.3.32) and 
A" = 
I m ·- J -
m- I 
[ 
[ 
k 
J 
3(k -
1) 
; = J 
I 
. 

4.3 
Comparison of the Variances of k Distributions 
231 
In particular, for <Po = 0, - 2 log W; reduces to 
- 2 log Wo* = - ± 
J1J i '[log ~iS;([J, 8;) - log s([J, 6)] , 
i= t 
In; 
(4.3,35) 
with 
_ 
I 
~ 
_ 
s([J, 8) = -
L l1iSi([J, B;), 
m i=J 
in the case fJ = 0, we obtain precisely the !\ormal theory results already discussed . 
using Bartlett's method of approximation the somewhat remarkable result is 
obtained, therefore, that for any known value of [J (not close to -I), the decision 
as to whether the point <Po lies inside or ou tside the (I -
ex) H. P. D. region is made 
by referring - 210g Wo* to a scaled '/ distribution. 
In the case <Po = 0 which 
corresponds to O'J = '" = O'k> the quantity -210g Wo* is in exactly the same form 
as Bartlett's modified form of the likelihood ratio statistic for [J = 0, except that 
ni is replaced by n;(1 ..L [J) a nd the sample variances s; by the quantities 
nisiCfJ, 8;) 
ni(J + [J) -
I 
L jYij -
8..1 2/( 1+11) 
ni( I + fJ) -
J 
We are thus able to obtain , for each [J, the approximate probability content 
of a n H.P,D. region which would just excl ude the point <p = O. For a given set of 
data, we can, therefore, study how inferences about equality of the standard 
deviations 0' may be affected by the departure from Norm ality in the parent 
populations, If fJ is assumed eq ual to zero and/or 8 are assumed known, sampling 
res ults can be obtained which exactly ma tch the Bayesian results. 
But in the 
more general situation when fJ -=I 0 and the 8 unknown, no corresponding sampling 
result is available. 
Marginal Posterior Distributions of [J and <P 
Combining the likelihood function (4,3.1) with the priors of (0', [J, 8) in (4.3.2), 
(4.2.20), and (4,3.26), and integrating out 8 and 0', we obtain the posterior 
distri burion of [J 
p([J I y) cc p(fJ)p"CfJ i y), 
-I <
fJ ~ I , 
(4.3.36) 
where 
[ 
( 
I + [J)] -
II 
k 
r 
n 
] J'" 
PII(fJ ly) cc r 1+-'-2-
ilJ1rl+-t(I+fJ)
_ [nisi([J, Bi)r i";( J '11) dOi 
is the posterior distribution of p, corresponding to a uniform reference prior 
distribution. 
Now, we can write the margi nal distribution of <p as 
p(<p I y) = f J p(<p I [J, y)p([J I y) d[J, 
( 4.3,37) 

232 
Bayesian Assessment of Assumptions 2 
4.4 
where the first factor of the integrand is given by (4.3 .28). 
In obtaining this 
distribution, the unknown location parameters e and the non-Normality parameter 
fJ are eliminated by integration. The distribution thus provides the final overall 
inferences about the linear contrasts of the logarithms of the variances of the k 
populations. 
In practice, numerical integration of f3 and e, and particularly e, would be 
exceedingly burdensome. 
However, the factor p(<\! I f3, y) can be approximated 
by (4.3.31) for values of f3 not close to -I, and when p(fJ : y) is concentrated 
about some modal value fJ we can employ the further approximation 
pC<\! I y) :, p(<\! I jJ, y). 
( 4.3.38) 
Thus in appropriate circumstances. p(<\! ! y) may be approximated by the right-
hand side of (4.3.31) with jJ substituted for the unknown fJ. Although evaluation 
of jJ for (4.3.36) still requires numerical integration of one-dimensional integrals 
as well as numerical determination of a maximum, these are simple processes 
compared with evaluation of the exact distribution. 
4.4 INFERENCE ROBUSTNESS AND CRITERIOi'\ ROBUSTNESS 
Using the notation of Section 1.6 suppose that e I are a set of parameters of interest 
and O2 a set of nuisance parameters measuring discrepancies from "ideal" 
conditions 82 0 , Then: 
a) robustness of inferences about 8 1 to departures from the ideal conditions, 
may be studied by considering how the conditional posterior distribution 
p(8 1 182, y) changes, as the elements of O2 are moved av.ay from the values 920 , 
b) at the same time, the marginal posterior distribution p(ezl y) measures t~e 
plausibility of various choices for 82, and integration of peel I e2. Y) with p(8 2 I Y) 
as weight function yields p(el i y), which, in suitable circumstances, shows what 
overall inferences can be made a bout 9 I' 
The ideas have becn iJiustrated , in this chapter and the previous one, with 
C2 = fJ measuring a departure from Normality and with the elements of 01 being 
in turn means, regression coefficients and variances. Since in any given instance 
we never know for certain how elaborate assumptions need to be, the general 
possibility of embedding a tentative parsimonious subset of assumptions in a 
more prodigal set and studying the sensitivity of the conditional inference can 
provide a highly informative technique of preliminary data analysis. 
Such studies of inference robustness are, as has been said, of a different 
character from the customary criterion robustness studies of sampling theory. 
However, inference robustness can be interpreted in terms of sampling theory 
although usually the limitations of that theory make it difficult to apply. 
In the sampling theory study of criterion robustness, an "optimal" criterion 
rewey) is selected which is appropriate for making inferences about the parameters 

4.4 
Inference Robustness and Criterion Robustness 
233 
9 t for some fixed 9 2 = 920 correspond ing to the "ideal" assumptions. The change 
in the sampling distribution of C!)lO(Y) is then stud ied as the parameters 92 are 
changed from 920 , 
By contrast, to study inference robustness in the sampling framework, we 
need to study the sampling distribution not of Ce'Q(Y) but of Co,(Y) as 9 2 are 
changed. 
It is an extraordinary fact that for a sample drawn from an exponential power 
distribution of which the mean is assumed know n, sufficient statistics exist for the 
variance over the en/ire range of the kurtosis parameter (3. In this unusual circum-
stance therefore, it is possible with sampling theory to study not only criterion 
robustness but also inference robustness. We find in this exceptional case, where 
study of inference robustness is accessible to sampling theory, that it parallels 
exactly the Bayesian result. 
4.4.1 The Analyst Example 
As a specific illustration, we may again consider the variance comparison dal;! 
of Table 4. 2 . .1 consisting of J 3 independent observations made by an Analyst 
A 1 and 20 made by an Analyst A 2. Although popUlation means were unknown , 
for the purpose of this demonstration we shall assume them known and equal to 
the sample means. 
On the Normal theory test of equality of variances, the 
significance level, that is, the probabi lity of exceeding the observed variance ratio 
by chance, is 7.6 %. 
Table 4.4.1 
Changes in percentage significance level induced by departures of the population f3 
from the f30 defining the criterion Cpo(Y): the analyst data 
f3 1 
Parent population 
fJo 
-0.6 
-0.4 
-0.2 
0.0 
0.2 
0.4 
0.6 
-0.6 
6.0 
11.0 
-0.4 
3.5 
6.5 
9.0 
14.0 
c: 
-0.2 
3.0 
4.8 
7.0 
9.8 
11.5 
17.0 
22.0 
0 
'C 
0.0 
2.8 
4.5 
6.0 
7.6 
9.5 
12.0 
14.5 
~ 
'--
0.2 
2.5 
4.0 
5.0 
6.0 
8.0 
10.0 
12.0 
U 
0.4 
2.4 
3.5 
4.8 
6.0 
8.0 
8.6 
9.5 
0.6 
2.4 
4.0 
4.8 
5.0 
6.5 
7.5 
9.2 
Table 4.4.1 shows the percentage significance levels calculated on standard 
sampling theory fo r this example under a number of different circumstances. 
The details of how these calculations w<.:re made are given later. In calculating 
the table, we have assumed the two parent distributions to be members of the 
class of exponential power distributions defined in (3.2.5) having common value 
of (J and known means. Then , for any fixed value of {3, say {3 = f30, the uniformly 

234 
Bayesian Assessment of Assumptions 2 
4.4 
most powerful si mi lar test of the hypothesis H 0: a~ / a~ = 1 against the alternative 
HI: ai ai > 1 is provided by the ratio 
s (Il 8) 
"'12 Iy . - 8 :2/(1 +Pol 1n 
C ( ) _ 
2 PO, 
2 
_ 
L, 
2j 
21 
2 
Po Y -
sICfJo,8 1) - I"'lyu - 8112/(l+Po)/nl' 
(4.4.1) 
in which the numerator and the denominator are sufficient statistIcs for the 
variances O"~ and O"i, respectively. In particular, when fJo = 0 the criterion CoCy) 
is the usual F statistic. 
Consider for example the row in the table for flo = O. 
The entries show how the percentage significance levels for the F criterion change 
as the fJ value of the parent popUlation changes. 
For instance, 7.6 % is the F 
criterion significance level for a Normal parent ({3 = 0) and the value 9.5% 
immediately to the right of this shows the significance level for a somewhat more 
leptokurtic parent distribution (fJ = 0.2) but with the same "formal theory F 
criterion Co(y). Similarly, if we take the values corresponding to the next row 
for (30 = 0.2, we have the corresponding probabilities for the criterion 
n [""' 1 
v . - 8 1
2/1
.
2
] 
C () _ ~ L, 
. Ij 
2 
0.2 Y -
"'" I' - 8 ,2/1. 2 
' 
nz 
L.. 
}Ij 
II 
(4.4.2) 
which will provide a uniformly most powerful similar test for a parent with 
{3 = 0.2. In particular, the significance level for this parent population is 8.0%. 
Now, however, consider the change from 7.6 % to 8.0 % in the diagonal. This 
gives a measure of inference robustness. 
Specifically, it shows how much the 
significance level changes when the parent distribution and the appropriate 
criterion are changed together. 
Thus, while the familiar criterion robustness is measured by the changes 
occurring horizontally across the table, inference robustness is measured by changes 
occurring in the diagonal elements which are printed in bold type. It is noticeable 
that the changes which occur horizontally are, for this data, considerably greater 
than those which occur diagonally. J n fact, whereas the probability of the error 
of the first kind for the Normal theory criterion is changed by a factor of 5 (from 
2.8% to 14.5 %), in changing from a platykurtic distribution with {3 = -0.6 to a 
leptokurtic distribution with {3 = 0.6, it is changed only by a factor of 1.5 (from 
6.0% to 9.2 %) when appropriate modification is made in the criterion. While 
one cannot on this evidence draw any general conclusions, it is true that for these 
particular data, the inferential probabilities about the ratio of variances are much 
less affected by changes in fl than are the probabilities associated with a particular 
criterion. 
The above discussion has been conducted so far entirely in terms of sampling 
theory. 
It will be recaJJed from Section 4.2.3 that the diagonal elements of the 
table are precisely the a posteriori probabilities that the variance ratio O"~ 'ai is 
less than unity for the corresponding values of {30 in the parents, The inference 
robustness study under sampling theory and the Bayesian robustness study thus 
give precisely parallel results, 

4.4 
I nference Robustness and Criterion Robustness 
235 
4.4.2 Derivation of the Criteria 
For any specific value of fi, say f3 = f3o, the uniformly most powerful similar 
criterion Cpo(y) in (4.4.1) follows an F distribution with n2(1 + fio) and nl(1 + fio) 
degrees of freedom when the hypothesis H 0: a~1 ai = 1 is true. Equivalently, the 
statistic 
W 
_ 
L"' IYlj - 0112/( 1 +Po) 
(f3o) - L"' IYli - 0112/0 +Po) + L:n' IY2j _ 8212/0 +Po) 
(4.4.3) 
has a beta distribution with parameters }n I (I + fio) and }n2 (1 + fio). 
Using this result, the exact probabilities in the diagonal of Table 4.4. I, namely 
Pr {FIII(I +po),n,(1 +Po ) > Cpo(y)}, 
can be readily calculated. 
For the off-diagonal elements, we need to find the 
distribution of the criterion Cpo(Y) when the parent f3 takes some value other than 
f3o. This can be approximated using permutation theory. 
For the exponential power distribution (3.2.5), it is readily shown by employing 
(A2.1.5) in Appendix A2.1 that the variate 
X = I Y ~ 8 1
2/
(I+P O) 
(4.4.4) 
has as its rth moment (about the origin) 
= r{(l + (3)[-} + rl(l + f3o)]} [c(fm-'(I ' p)/(I+flo) 
)1, 
fCte l + (3)] 
r = I, 2,3,.... 
(4.4.5) 
Now write 
Xj = 
-~ 
j = 0, 1, ... , n l , 
(I
Yl . - 0 
'1 2/(1 +Po) 
I
e 1 2 / (1 +floJ 
Y2(j -no) -
2 
. 
+ I 
}=n l 
,11 1 
. 
(12 
+ 2, ... ,11 1 + nz. 
Then, on the hypothesis H 0: o-taf = I and following the method in Box and 
Andersen (1955), the permutation moments of 
"~" 
X· 
W (f3 ) = L, = I 
J 
° "" 
X. ' 
LJ= 1 
J 
( 4.4.6) 
can be written 
n l 
£[ W (f3o)] = -, 
11 
p 
2n 1n2 
[ 
1 
n 
] 
. 
V[W(f3o)] = 
2( 
2) 
1 + -2 --I (b 2 -
3) ; (4.4.7) 
p 
nn+ 
/1-

236 
Bayesian Assessment of Assumptions 2 
4.5 
where 
By taking the expectation over all samples of the permutation moments, we 
obtain the ordinary moments of W (f3o) as 
E[W(f3o)] = ~, 
(4.4.8) 
n 
Expanding the denominator of b2 around the mean of X and taking expectations, 
we find that, to order 11- 2 
E(b2 - 3) = /12 _ 3 + 11- 1 [fl2 _ 2{1J + 3/1~ 1 
flf 
{IT 
{Ii 
/1i J 
(4.4.9) 
where {I .. are given in (4.4.5). 
For a specific value of (3, it can be shown that the statistic W({3o) is approxi-
mately distributed as a beta variable with parameters [in I (l + f3o)b, 1n2(1 + f3o)b], 
where 
(4.4.10) 
represents the modification due to departure of f3 from f3o . Equivalently, Cpo(Y) 
is approximately distributed as an F variable with tl 2(1 + f3o)f> and 11 1(! + f3o)b 
degrees of freedom from which the off-diagonal probabilities in Table 4.4.1 can 
be approximately determined. The result is, of course, exact when f3 = o. 
4.5 A St:~MARY OF FORMULAE FOR VARIOUS PRIOR AND 
POSTERIOR DISTRIBLTIONS 
For convenience of the reader, Table 4.5.1 gives a summary of the formulae for 
various prior and posterior distributions concerning inferences about comparison 
of variances. 
Table 4.5.1 
A summary of prior and posterior distributions 
I. Suppose k samples y; = (YI1' ... , Yln), .. , y~ = (Ykl' ... , Yknk) are drawn from 
possibly different members of the exponential power distribution with common f3 
[ 
\
y -
81 2/(I+Pl] 
p(yI8,a,{3)=w(f3)a- 1 exp 
-c(f3) -a-
, 
-
CD < Y < 00, 

4.5 
where 
A Summary of Formulae for Various Prior and Posterior Distributions 
Table 4 .5.1 COlllinm'd 
c 
_ (f[W + ,8)J} ] /( 1" PI 
(,8) - l r[w + ,8)J 
' 
{ f'[~( 1 + ,8)W
2 
(1)(,8) = (I + ,8){r[W + ,8)JP' Z' 
2. Let 0 = (8 1 , ... ,8k ) and (J = (u I , ... ,uk ) 
The prior from (43.2), (4.3.18), and (4.3.26) is 
pee, cr,,B) = p(e)p(a)p(,B) 
with 
k 
pee) 0 
c, 
pea) Ct;. I r O';-l, 
i= I 
. The Case k = 2 
237 
3. Conditional on (e,,8), the posterior distribution of V = u~/a~, p( V: {1, e, y), is in 
(4.2.7), 
where 
Thus 
f 
SI({J,O)} 
PdV < 11f3,O,y} = Pr \F",WP ).Hl()+P) < 52([3,8
2
) 
. 
[n particular, for [3 = 0 
L(Ylj - 81)2/ 11 ] 
V 
~F 
L(Y2j - 8Z)2/112 
",. "1' 
For ,8 --> -1 , the distribution of 
is, from (4.2.10), 
2(n] + 11 2) 
p(l! I ,8 --> -
I, e, y) = 
( 
I1t"2 
U + III -
1 
n lll z 
U -+" 2-
l 
2(11] + liZ) 
0< II < I 
1 < II < A), 
where (hI' hz) and (111 1,1112) are the half-ranges and the mid-points, respectively. 

238 
Bayesian .\ssessment of Assumptions 2 
4.5 
Table 4.5. I lontinlled 
4. Conditional on e, the posterior distribution of [3 is, in (4.2.17), 
p([31 e, y) X p([3)PII({31 e, y), 
-1<{3:(I, 
with 
2 
X n rei + ·i llj(i +1i )][llj sj (Ii,B)r ~·", (I +P ) 
;=l 
5. The posterior distribution of V given e is, from (4.2. J 6), 
p(V Ie, y) = f 
~ p(V I Ii, e, y)p(1i I e, y) dli, 
V> 0. 
6. The posterior distribution of e, given (3, is, in (4.2.24), 
9 
= 
2 
[l1js;C{3,Bj)] -'~ >I,(I+P) 
p( 1[3, y) 
1] Je< [ . (Ii B)r t ",(J +P) dO ' 
1-1 
-
0:) n,s, } I 
, 
- 00 < Bj < Xl. 
7. The posterior distribution of v, given (3, with e eliminated is, in (4.2.23) and (4.2.25), 
p(V I Ii, y) = r: f 
_: p(V I (3, 9, y)p(e I Ii, y) dBI de2 · 
In particular, for {3 = 0 
L:(Ylj -
9 J )2(
111 -
[) 
V ~( 
_ 
)1( 
_ [) -
F",-I.>I,- J' 
..., hj 
Y2 
n2 
and when Ii --+ -
[ the distribution of u* = V(h l /h 2)2 is, from (4.2.27), 
p(u* I {3 --+ - 1, y) = { 
kut(II , -I ) -I[(111 + n 2) -
(111 + 112 -
2)II ~ J , 0 < u* < I 
ku;,'(11,-I)-I[(111 + 11 2) -
(I/J + 112 -
2)u; -\:J, I < u* < 00 
with 
111112 
k=----
2(111 + 112) (II J + 11 2 -
[)(11, + 112 -
2) 
For Ii not close to - I, from (4.2.34), we have the approximation 
v1 /(l+p)II,SI(.B'~I)/ [I1I(J +/3) -I] "'-' F 
1l2S2({J,B2)/[1l2(1 + til - 1] 
II,(I+P)-I.II,(I+P)-I' 
where (e l , e2) is the mode of pee I {3, y). 

4.5 
A Summary of Formulae for Various Prior and Posterior Distributions 
239 
Table 4.5.1 Continued 
8. With 0 eliminated, the posterior distribution of fJ is, in (4.2.35), 
p({3 1 y) oc p({3)PII({3 1 y), 
-I <
{3 ~
1 
with 
PII(PI y) oc {f'[1 + W + {J)]} - (II , +" 2) 
9. With e and {3 eliminated, the final posterior distribution of V is, in (4.2.36), 
p(V I y) = r 
I p(V I {3, y)p({31 y) d{3. 
T he General Case k > 2 
10. Conditional on (e, /J), then from (4.3.5) the posterior distribution of the (k -
I) 
contrasts 
i = I, ... ,k -
1 
is 
I" -1 
] 
( 
" -
I 
) - -t,,( I + fJ) 
p(<\> I e, {3, y) = C LVI V -l,, ;( l -t fJ) 
I + i~l Vi 
, 
where 
and 
-00 <<Pi < 00, 
i=J, ... ,k-J 
k 
n = 2: IIj , 
i=l 
c = r [~ (I + /3)] (I + {3) - (k - J) {,f1 r [ ~ (I + {J)] r 
1 
Thus, from (4.3 .7) to (4.3.1 J), the approximate (l -
a ) H .P.D . region of <\> is 
- 210gW < -/ (k -
I, a), 
l+A 
where 
A= 
J 
(Inj-1-n-I), 
3(k -
1)(1 + {3) 
j=l 

240 
Bayesian Assessment of Assumptions 2 
4.5 
Table 4.5.1 Contil/ued 
and /(k -
I,a) is given in Table III (at the l:nd of this book). In particular, for 
<pj = 0 (i = I, ... ,k -
I) (that is, (J~ = ... = (J~) 
k 
- 2 log W = - L 11;( I + j3)[Iog Sj(fJ, ej) -
log s(fJ, e;)J, 
i = ( 
with 
1 I. Conditional on e, the posterior distribution of fJ is, in (4.3.19) and (4.3.20), 
p(fJ I e, y) cc p(fJ)Pu(fJ I e, y), 
-
I <(3~I, 
with 
12. W ith /3 eliminated, the posterior distribution of <1>' = (c/>I' ... ,<Pk-j), given e, is 
from (4.3.22), approximately 
p(<I> I e, y) == p(<I> I e, 13, y), 
where /3 is the mode of p(/31 e, y). 
13. Conditional on (3, the posterior distribution of e is, in (4.3.27), 
- cc < 8; < co, 
i = 1, ... , k. 
14. Conditional on /3, with e eliminated and for {J not close to -I, the posterIor 
distributi on of <I> is approximately, from (4.3.31 ), 
-00 <c/>;<co, i=l, ... ,k-I, 
where 
k 
III = I In j , 
In; = I1 j (1 + (J) -
I, 
;=1 

A4.1 
Appendix 
241 
Table 4.5. 1 Contillued 
and (e\, ... , ek ) is the mode of p(BI [3, y). 
Thus, from (4.3.32) to (4.3.35) the 
approximate (I -
('1) H.P.D. region of t\> is given by 
- 210g W* 
---- < ,/(k -
I, (X), 
I + A* 
where 
- 2 log w * = JI 111; log ~ -
~~: III; log Yi + mlog (I + ~~ 'I;) 
and 
1 
. k 
) 
A*= 
(, 
I 
-\ 
3(k _ I) 
;~I 111; 
-
m 
. 
In particular, for the point t\> = 0 «T~ = ... = (T~) 
~ 
[ 
I1;S;([3, e;) 
, I 
- 2 log W * = - L 
111; 
log 
-
Jog 5([3, fI) 
i=1 
111i 
with 
15. With t:I eliminated, the posterior distribution of f3 is, in (4.3.36), 
p(f31 y) <X p({J) PII(f31 y), 
-
I < f3 ~ I, 
where 
k 
Pu(/JI y) <X {r[1 + -}(I + (3)J } -" TI rei + ~ l1i (I + f3)] 
i ;: 1 
16. Finally, with fI and f3 eliminated, the posterior distribution of t\> is approximately 
pet\> I y) == pet\> 1/3, y), 
where /3 is the mode of p(f31 y). 
APPE"DlX A4.1 
LIMITING DISTRIBUTIONS FOR THE VARIANCE RATIO V WHEN 
f3 APPROACHES -1 
We here sketch the derivation of the limiting distributions given in (4.2.10) and 
(4.2.27) for the variance ratio V when f3 approaches -1. These results follow 
readily by making use of (A3.1.7) in Appendix A3.1. 

242 
Bayesian Assessment of Assumptions 2 
A4.1 
Specifically, for the distribution of u in (4.2.10), we first make the transfor-
mation in (4.2.9), 
(A4.1.1) 
so that 
p(u I [3, e, y) = q([3)[r([3, 8 1,82)]'" U i " l - 1 [I + u I/( 1+ Pl r([3, 8 1,82)2/( 1+ Plr }(lIl +n,)(1 + Jl), 
0 <: u < 00 , 
where 
h2 + 1m2 - e21 
[n 2s2([3, 82)]t(l +P) 
and 
As [3 -t -
I , 
Jim q([3) = 
nJn2 
= q, 
P--I 
2(n, + n2) 
say, and by using (A3.1.7), 
lim r([3,8 1,82) =1. 
P--I 
Further, following the argument in (A3.l.8) through (A3. l.12), we have 
lim [1 + ul /(1+P) r([3,8J,82)2/(1+p)r (l+p) = r 
for 
I < U < 00, 
for 
P- - I 
• 
I 
O<u ~l. 
1 t follows that 
{qu }n'-J 
for 0 < u ~ I, 
lim p(u I [3, e, y) = 
_,_ 
for 
p_ - 1 
qu 
.,n2 
I 
1< u < 00, 
as given in (4.2.1 0). 
(A4.1.2) 
(A4.1.3) 
(A4.1.4) 
(A4.1.5) 
(A4.1.6) 
To derive the distribution of u* in (4.2.27), we first note that from the result 
in (3.2.15) as [3 -t -I, the joint distribution of (8 1,8 2) in (4.2.24) tends to 
2 
lim pee I [3, y) = TI~(ll i - l )h(II;-l) (hi + 1m; -
8;1) -11 1, 
p- -J 
;; I 
-
00 < e; < 00, 
i = I, 2. 
(A4.1.7) 
It is readily verified from (A4.1.7) that the quantity 
(A4.1.8) 

A4.1 
Appendix 
243 
is distributed a posteriori as 
where 
Noting that 
f
CZ,H III-l)-l 
lim p(z I (3, y) = 
/1 - -1 
Icz-1<II,-I)-1 
(n1 -
1)(n2 -
I) 
for 
O<z~l, 
for 
I < z'< 00, 
c = -=--.:'-----'--=---, 
2(nl + 112 -
2)' . 
u* = V (~) 2 = uz. 
172 
(A4.1.9) 
(A4.I.IO) 
We obtain from (A4. I .6) the conditional posterior distribution of 1I* given z as 
for 
0 < u* < z, 
for z < 1I* < 00 . 
It follows from (A4.1.9) and (A4.1.11) that for 0 < 1I* ~ I, 
1 
lim p(u* I (3, y) = qc lU;+II'- 1 f"' Z1<II, + 11,-1)-1 dz + U~"' -1 J z- '.-1 dz 
p- -I O u .  
= ku~(nJ -1 )- t [(17 1 + 112) -
(111 + 172 -
2)U~ / 2 ], 
and for I < u* < ce, 
2qc 
with k = ---
-
- as given in (4.2.27). 
111 + 11 2 -
I 
(A4.1.1 I) 
(A4.1.l2) 

CHAPTER 5 
RANDOM EFFECT MODELS 
5.1 INTRODUCTION 
In previous chapters problems about means, variances, and regression coefficients 
were discussed. We now consider another im portant class of practical problems 
concerned with variance componel1ls. 
To illustrate how variance component problems arise, consider a chemical 
process producing batches of material which are sampled and then analysed 
for some characteristic such as product yield . When the total variance a 2 of the 
observed yield is large, the effici~nt operation of the process is hampered, control 
becomes difficult, and process improvement studies are hindered. So that etlort 
can be effectively directed to reducing (5 2, it is necessary to discover the relative 
importance of the variolls sources from which variation may spring. 
It would 
for example be fruitless to devote effort to improving the analytical method if 
in fact an inadequate technique for sampling the material were the main cause of 
variation. With this in mind a special type of experiment may be conducted using 
what is called a hierarchical design. 
I = 4 batches 
j 
"'" J 'Jmpies 
per batch 
K = 2 analy s ~s 
per '<lmple 
Fig.S . .I.1 Illustration of a hierarchical classification. 
figure 5.1.1 illustrates a hierarchical design for four batches, three samples 
per batch and two analyses per sample. More generally suppose that I batches of 
product are randomly taken, each batch is sampled J times and K analyses are 
performed on each sample. Then on the assumption that batches, samples, and 
analyses vary independently, and addit'ively contribute errors e j , eij, and I:'jjk, we 
have the mathematical model 
i=I"I; j =
I ... . ,J; k=I , ... ,K 
(5.1.1) 
""here Yjjk are the observations, 0 J. common location parameter, E'j' e jj and ejjk 
244 

5.1 
Introduction 
245 
are independently distributed random variables with zero means and variances 
Var (eijk ) = crt, Var (eij) = 
cr~ and Var (ei) = 
()~. Thus the total variance of the 
(ijk)th observation Yijk becomes 
()~ + ()i + (); and the quantities 
(()~, ()~, ()~) 
are called the variance components. 
Obviously, more than three sources of variation may be involved and many 
different applications of this kind of study could be quoted. 
The random variables ej , ejk are sometimes called random effects and 
the model in (5.l.!) is known as a random effect model. This is in contrast 
to the model in (2.7.7) for the comparison of Normal means considered in Section 
2.11. In the sampling theory framework, these means are usually regarded as 
fixed constants and the model is therefore commonly called a fixed effect model. 
. The relationship between these two types of models will be considered in more 
detail later in Chapters 6 and 7. 
5.1.1 The Analysis of Variance Table 
We have already seen that in the comparison of Normal means, certain calculations 
are conveniently set out in the form of an analysis of variance table. It so happens 
that such a table is also of value in analyzing variance components. An appro-
priate analysis of variance table for the present random effect model is shown in 
Table 5. J.l. 
Table 5.1.1 
Analysis of variance of hierarchical classification with three variance components 
Mean 
Sampling expectation 
Source 
Sum of squares 
Degrees of 
square 
of mean square 
(S.S.) 
freedom (d.f.) 
(M.S.) 
(E. M.S.) 
Due to 
batches 
53 = J K L. (Yi .. - Y )2 
V3 = (I -
1) 
1113 = 5 3/V3 
()~23 =()T + K()~ + J K()~ 
Due to 
samples 
5 2 = K L. L (Yij. - Yi.l 
V2 = 1(1 -
I) 
1/12 = 5 Z/v2 
()~ 2 = ()T + K()~ 
Due to 
analyses 
51 = L L. L. (Yijk - Yij/ 
VI =JJ(K -
J) 
1111 =51,'v l 
()2 
I 
Total 
L L L. (Yijk - y j 
JJK -
I 
In Table 5.1.1 (and hereafter) we adopt the notation by which a dot rep/acing 
a subscript indicates al1 average over that subscript. Thus, 
l 
Yi j. = K L Yijk, 
Yi. 
I L y·· 
J 
I) .' 
and 
I 
Y = -
L.y· 
.. 
1 
I. .• 
From the sampling theory point of view, by pooling the sample variances 

246 
Random Effect Models 
5.1 
within the individual samples, we obtain an estimate of cr~, the analytical variance. 
Then by pooling the sample variances of the sample means within each batch, 
we obtain an estimate of cr~ + K cr~ where cri measures the variation due to 
sampling. 
Finally, from the variation of the batch means, we can obtain an 
estimate of cr~ + Kcr~ + JKcr~ where cr~ is the batch variance. It is then customary 
to obtain estimates of the variance components (cri, cr~, crD by solving the equations 
in the expectations. Thus, from Table 5.1.1 we have the relationships 
(5.1.2a) 
so that the usual estimates of the variance components are 
and 
5.1.2 Two Examples 
We will begin by illustrating the simple case where there are only two components 
of variance. The model is then 
j=I, ... ,J; k=I, ... ,K, 
(5.1.3) 
which is a special case of (5.1.1) when I = 1. 
The first data set we consider is taken from Davies (1967, p. 105). The object 
of the experiment was to learn to what extent batch to batch variation in a certain 
rilw material was responsible for variation in the final product yield. Five samples 
from each of six randomly chosen batches of raw material were taken and a single 
laboratory determination of product yield was made for each of the resulting 30 
samples. The data are set out in Table 5.1.2 with an analysis of variance in Table 
5.1.3. In this example j = I, ... , 6 rcfers to the batches and for each j, k = I, ... , 5 
refers to the samples. The "within batches" componenl cr: will be referred to as 
the "analyses" component although it includes sampling errors as well as chemical 
Table 5.1.2 
Dyestuff data (Yield of dyestuff in grams of standard color) 
Batch 
Individual 
observations 
y ) . 
1545 
1440 
1440 
1520 
1580 
1505 
2 
3 
1540 
1595 
1555 
1550 
1490 
1605 
1560 
1510 
1495 
1560 
1528 
1564 
y .. = 1527.5 
4 
5 
1445 
1595 
1440 
1630 
1595 
1515 
1465 
1635 
1545 
1625 
1498 
1600 
6 
1520 
1455 
1450 
1480 
1445 
1470 

5.1 
Introduction 
247 
analysis errors. The variance component ()~ associated with "batches" represents 
the variation associated wi th raw material quality changes when sampling and 
analytical errors are fully allowed for. 
The data are characterized by the fact that the between-batches mean square 
is large compared with the within-batches mean square, strongly indicating that the 
component ()~ associated with batches is nonzero. 
Source 
Between batches 
Within batches 
(analyses) 
Total 
a-i = 2,451.25, 
Table 5.1.3 
Analysis of variance for the dyestuff example 
S.S. 
d.f. 
M.S. 
S2 = 56,357.5 
v2 = 5 
1n2 = 1 J ,271.50 
SI = 58,830.0 
VI = 24 
Inl = 2,451.25 
115,187.5 
29 
a-~ = (11,271.50 -
2,451.25)/5 = 1,764.05, 
E.M.S. 
(Ji + 5()~ 
(J2 
1 
The second data set we consider illustrates the case where the between-batches 
mean square is less than the within-batches mean square. These data had to be 
constructed for although examples of this sort undoubtedly occur in practice they 
seem to be rarely published . The model in (5.1.3) was used to generate six groups 
of five observations each. 
The errors ejk were dra wn from a table of random 
Normal deviates with () 1 = 4, the ej were drawn from the same table but with 
(J2 = 2, and the parameter e was set to be equal to five. For convenience we shall 
refer. to the components (()i, 
()~) in this second example associated with 
" analyses" and " batches" as we did in the first. The data are set out in Table 5.1.4 
and the corresponding analysis of variance, in Table 5.1. 5. 
Table 5.1.4 
Data generated from a table of random normal deviates for the two-component model 
(with e = 5, (Jl = 4, ()2 = 2) 
Batch 
Individual 
observations 
7.298 
3.846 
2.434 
9.566 
7.990 
6.2268 
2 
5.220 
6.556 
0.608 
11.788 
-0.892 
4.6560 
Y .. 
3 
0.110 
10.386 
J 3.434 
5.510 
8.166 
7.5212 
= 5.6656 
4 
5 
6 
2.212 
0.282 
1.722 
4.852 
9.014 
4.782 
7.092 
4.458 
8.106 
9.288 
9.446 
0.758 
4.980 
7.198 
3.758 
5.6848 
6.0796 
3.8252 

248 
Random Effect Morlcls 
Source 
Between batches 
Within batches 
(ana[yses) 
Total 
ai = 14.9459, 
Table 5.1.5 
Analysis of variance for the generated example 
S.S. 
d.f. 
M.S. 
52 = 41.68[6 
vl = 
5 
1/72 = 
8.3363 
51 = 358.7014 
VI = 24 
1/71 = 14.9459 
400.3830 
29 
a~ = (8.3363 -
14.9459)/5 = - 1.3219, 
5.1.3 Difficulties in the Sampling Theory Approach 
5.1 
E.M.S. 
()~ + 5d 
()l I 
The sampling thcory approach to the problem of variance components encounters 
a number of snags. 
These have bothered statisticians for many years, as is 
evidenced by the great variety of attempts which have been made to resolve the 
problems.t The difficulties can conveniently be discussed in terms of the examples 
quoted above. 
Negative Estimate of (J"~ 
The most commonly used estimate of (J"; is obtained from the difference of the 
two mean squares (m l , m l , ) and we have referred to this estimate as 8-~. When 
the between batch mean square in2 is smaller than the within batch mean square 
m I as it is in the second example above, (J" ~ will be negative. Since (J"~ must be 
nonnegative, this is generally regarded as objectionable. 
Difficulties with Confidence Intervals 
Even with the assumption that e j and ejk are Normally and independently distri-
buted, the sampling distribution of 8-i is complicated and depends on the unknown 
variance ratio (J"~/(J"T. The problem of obtaining a confidence interval for (J"~ is 
complex, and no generally accepted procedure has been found . Also , in estimating 
the variance ratio (J"~ i (J"f the commonly used confidence intervals, which are based 
upon the sampling distribution of the mean square ratio ml /m l , may include 
negative values. 
Intuitively, one would feel that the part of the interval associated with negative 
values of ()~ ought to be discounted in some way, but attempts to do this within 
the sampling theory framework are difficult to justify. 
t See for example, Daniels (1939), Crump (1946), Fisenhart ( 1947), Henderson (1953), 
Moriguti (1954), Tukey (1956), Graybill and Wortham (1956), Bulmer (1957), Searle 
(1958), Herbach (1959), Gates and Shine (1962), Thompson (1962,1963), Gower (1962), 
Williams (1962), Bush and Anderson (J 963), Wang (1967), Zacks (1967). For a review of 
recent work in this area, see Tan (1964) and Ali (1969). 

S.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
249 
Pooling of Estimates 
When ml and m I are not very different, it has sometimes been argued that a 
pooled estimate (vlm l + V1ln2)/(V j + v2) of O'i should be used. 
But how does 
one decide when to pool and when not to pool? And how is the sampling distri-
bution of the estimate of O'i affected by the fact that pooling will only be practiced 
when m l and Inl are sufficiently close? Attempts have been made to get around 
these difficulties and to study the consequences of various proposed procedures, 
but the situation remains unsatisfactory. 
Departures from Assumptions 
Additional complications arise when we consider the effect of departures from the 
assumptions of Normality and independence. 
It is shown in Scheffe (I 959) 
that non-Normality in e j and lack of independence in e jk will have serious effects 
on the distri butions of the criteria which one uses to make inferences about (O'i, 
O'~) . This further confuses an already chaotic situation. 
In summary, then, traditional sampling theory methods have led to worrisome 
difficulties in the variance component problem to which no generally accepted 
set of solutions have been obtained. Our aim in this chapter is to reexamine 
these problems from a Bayesian standpoint, making the standard Normality 
and independence assumptions. 
Within the Bayesian framework, no difficulty 
in principle occurs in relaxing the Normality and independence assumptions. 
Indeed, Tiao and Tan (1966) and Hill (1967) have studied variance component 
problems while relaxing the independence assumption for the e jk , and more recently 
Tiao and Ali (1971 a) have considered the effect of departure from Normality of 
the ej. 
5.2 BAYESIAN ANALYSIS OF HIERARCHICAL CLASSIFICATIONS 
WITH TWO VARIANCE COMPONENTS 
As a preliminary to the Bayesian analysis. a summary of notation and assumptions 
for the two-component random effect model is given below. 
With J groups (batches) of K ohservations (analyses) we employ the model 
j = I, ... , J; 
k = 1, ... , K , 
(5.2.1) 
where the e's are supposed Normally and independently distributed with 
Var (eJ = O'L 
and 
(5.2.2) 
Thus, Var (Yjk) = O'i + (J~ , and the traditional unbiased estimators for the 
variance components O'~ and O'~ are respectively 
and 
(5.2.3) 

250 
Random Effect Models 
5.2 
with m 2 and m l the "between" and "within" mean squares defined in Table 
5.2.1. It is to be noted that an important quantity occurring in the analysis is 
af2 = af + Ka~ . 
Table 5.2.1 
Analysis of variance of hierarchical classifications with two components 
Source 
Between batches 
Within batches 
Total 
S.S. 
5.2.1 The Likelihood Function 
d.f. 
V2=(J-l) 
vI=J(K-l) 
JK-' j 
M.S. 
m2=S2/V2 
m. =Sl /Vl 
E.M.S. 
To derive the likelihood function for random effect models, we first recall certain 
useful results summarized in the following theorem . 
Theorem 5.2.1 Let XI, ... , Xn be n independent observations from a Normal 
distribution N(O,0"2). Let x be the sample mean and (Xi - x) (i = I, ... , n), be the 
residuals. Then 
I. x is distributed independently of the Xi -
X as N(O, 0"2/n) , 
2. L (XI -
X)2 ~ (J2 X~-I' and 
3. In so far as the Xi -
X arc concerned, L (Xi -
X)2 is a sufficient statistic 
for 172• 
Turning to the model in (5.2.1), one convenient way to obtain the likelihood 
function is to work with the group means Yj. and the residuals Yjk -
Yj. ' Clearly, 
in terms of (e, ej , ejk) 
and 
(5.2.4) 
Jt follows from the Normality and independence assumptions of (ej , ejk) and 
the results in Theorem 5.2.1 that 
a) the Yj are independent, each having a Normal distribution N(B, O"i 2IK), 
b) h are distributed independently of Yjk - Yj ' 
c) the quantity L k(Yjk - Yj)2 is distributed as 
(J~XtK-I) so that the sum 
L L (Yjk - Yj.)2 = SI = vlm l is distributed as a~X; I ' and finally 
d) in so far as the Y jk - Yj. are concerned, vim I is a sufficient statistic for ai. 

5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance, Components 
251 
Thus, the likelihood function is 
(5.2.5) 
where it is to be noted that O"T2 > O"T. 
Alternatively, in terms of (8, O"T, O"~) and 
noting that 
(5.2.6) 
we have 
(5.2.7) 
5.2.2 Prior and Posterior Distribution of (8, O"i, O"~) 
From (5.2.5) the likelihood function can be regarded as having arisen from J 
independent observations from a population N(8,O"i2/K) and J(K -
I) further 
independent observations from a population N(O,O"i). 
Treating the location 
parameters 8 separately from the variances (O"T, O"T 2), we therefore take, as a 
noninformative reference prior, a distribution with (8, log 0";, log O"i2) locally 
uniform and locally independent. The fact that O"T 2 = O"T + KO"i must be positive 
leads to the restriction O"i2 > O"i in the parametel space of (O"T, O"T2)' 
Thus the non informative prior is defined by 
(5.2.8) 
with 
p(8) cc c 
and 
Alternatively, in terms of (8, O"T, O"~), the prior distribution is 
(5.2.9) 
This choice of the prior distribution has been criticized by Stone and Springer 
(1965). A discussion of their criticism is given in Appendix A5.5. An important 
iss ue raised by K [otz, Mi lton and Zacks (1969) and Portnoy (1971) in connection 
with sampling theory point estimators of (O"i, O"~) based on this prior will be con-
sidered in detail in Appendix A5.6. 

252 
Random Effect Models 
5.2 
The noninformative reference prior distribution for at and a~ may also be obtained 
directly by applying Jeffreys' rule discussed in Section 1.3. It can be readily shown that 
the information matrix for (aT, a~) is 
(S.2.10a) 
Thus, the determinant is 
(S.2.lOb) 
By combining the prior in (5.2.9) with the likelihood in (5.2.7), the posterior 
di£tribution of (e, aT. aD is obtained as 
p(e,a~,aily)cx: (a~)-(i.'+I)(a~ + KaD- }('2+1)-J 
{ 
1 [Vlm~ 
V2m2 
JK(y .. -ef ]} 
exp 
-
-
-- + 
2 
2 + 
2 
2 
' 
2 _ af 
0' 1 + Ka2 
0'1 + Ka2 
-
<::I) < e < 00, 
a~ > 0, 
a~ > 0. 
(5.2.11 ) 
To obtain the posterior distribution of the variance components (a~ , aD, (5.2.11) 
is integrated over e yielding 
p(ai,ai I y) = w(a~)-(-t·, + I) (a~ + Kai)- -t('2+ l)exp [ - + 
CJ~I + aTV:~ai)] 
a~ > 0, 
a~ > 0, 
(5.2. 12) 
where w is the appropriate normalizing constant which, as it will transpire in the· 
next section, is given by 
W= 
K(Vlml) ~ 'J (v 2m2yl-'2 2- -}(·,h 2 ) 
1(t vl)1(1v2)Pr{F,2", < m 2/m l } 
(5.2. 13) 
From the definition of the X- 2 distribution , we may write the joint distribution of 
(a~, aD as 
p«(J~ , ai I y) = 
Pr {F. 2 • • , < 1112} 
1111 
a~ > 0_ 
a~ > 0, 
(5.2. J 4) 

5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
253 
where p(X: 2 = x) is the density of a n X- 1 variable with v degrees of freed o m 
evaluated at x. 
5.2.3 Posterior Distribution of d /O'T and its Relationship to Sampling Theory 
Results 
Before considering the distributions of O'i and (d separately we discuss the 
distribution of their ratio. 
Problems can arise where this is the only quantity 
of interest. For example, in deciding how to allocate a fix ed total effort to the 
sampling and analyzing of a chemical product, one is concerned only wi th O'i/O'i 
and not with O'f and O'~ individually. 
For mathematical convenience, we work with O'fl/O'i = I + K(O'~ / O'i) rather 
than O'~/O'i itself. 
In the joint distribution of (O'T, O'D in (5.2.12), we make the 
transformation 
v = O'i, 
and integrate over V to obtain 
peW I y) = wK- 1 W- c(V2+ 1 ) 1
00 
v- l (v,+v2 + 2)exp [- 2~ (vlm l + V1 n12/W)] dV. 
(5.2.15) 
For fixed W the integral in (5.2.15) is in the form of an inverted gamma function 
and can be evaluated explicitly using (A2.1.2) in Appendix A2.1. 
Upon 
normalizing, the expression for w, already given in (5.2.13), is obtained. The 
distribution of W can then be written as 
W > 1 
(5.2.16) 
where p(F., ... , = x) is the density of an F variable with (VI' v2) degrees of freedom 
evaluated at x. Probability statements about the variance ratio O'~ /O'T can thus 
be made using an F table. In particular, for 0 < '11 < 112. the probability that 
O' ~/ O'i falls in the interval ('II' '12) is 
(5.2.17) 
Note that the distribution of W is defined over the range W > 1. 
Therefore, 
H.P.D . or other Bayesian posterior intervals will not extend over values of W 
less than one, that is, they will not cover negative values of O'~/O'i . 

254 
Random Effect Models 
5.2 
Now from (5.2.16) we can write for the distribution of (m2/m I) W - 1 
p (!!2 W- I I Y) = P (F,.,.v, = ~ w-1)! prf\F,.,.v, < !!2lf ' 
w > l. 
111) 
m l 
m l 
(5.2.18) 
This result merits careful study. If O'T 2 and 0'7 were unconstrained by the inequality 
0'~ 2 > Ci~, then using an asterisk to denote unconstrained probability densities, 
we should have (from the results in Section 2.6), 
p* (!!2 W- I I Y) = p* (I-'v 2.v, = ~ w- t ). 
In] 
I 
m l 
W > 0, 
(5.2.19) 
where from the Bayesian viewpoint W is a random variable and 1112 /1111 is a ratio 
of fixed sample quantities. This probability distribution also has the interpretation 
that given the fixed ratio W = O'L/O'~, the sampling distribution of (m 2m l )W- 1 
follows the Fy"v, distribution. With this interpretation (5 .2. 19) gives the confi-
dence distribution of W from sampling theory. 
In contrast to the distributions in (5.2.16) and (5.2.18), the confidence 
distribution in (5.2.19) is defined over the entire range W > O. 
Confidence 
intervals for W based on this distribution could thus cover values of W in the 
interval 0 < W < l,thatis, _K - t < 0'~/0'7 < O. SinceO'~ /0'7 must be nonnegative. 
this result is certainly objectionable. 
In comparing (5.2.18) with (5.2.19), we see that the posterior distribution In 
(5.2.18) contains an additional factor 
From (5.2.19), 
Pr ftFv, .y, < ml} = Pr* {W > I} = Pr* {O'iz > 0'7}. 
I11 t 
(5.2.20) 
The Bayesian result can then be written 
(
m2 W-l I ) = P* [F y 2." , = (m2!m l ) W- 1] 
p 
m) 
. Y 
Pr* { W > 1} 
, 
W > l. 
(5.2.21) 
In other words, the constrained distribution appropriate to the variance com-
ponent problem is the unconstrained F distribution truncated to the left at W = I 
and normalized by dividing through by the total area of the admissible part. The 
truncation automatically rules out values of W less than one which are inadmissible 
on a priori grounds. 
If we write the factor Pr {Fv" v, < m 2 m l } as 1 - a, then, on sampling theory, 
a may be interpreted as the significance level of a test of the null hypothesis that 
0'72/aT = 1 against the alternative that ai 2/0'7 > I. However, while the interpre-

5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
255 
tation of '1. as a significance level is clear enough, it seems difficult to produce any 
sampling theory justification for its use in the intuitively sensible manner required 
by the Bayesian analysis. 
5.2.4 Joint Posterior Distribution of ITi and O'~ 
Our a posteriori knowledge about the parameters O'T and O'~ appropriate to the 
reference prior considered is, of course, contained in their joint distribution 
given by (5.2.14). From discussion in the preceding section, we can also regard 
this distribution as the constrained distribution of the variance components 
(O'i, IT~), the constraint being given by 
IT~ 2 > IT~. 
If the constraint were not 
present, the posterior distribution of (IT~, lTD would be 
P*(ITT, IT~ I y) = (1',111,)-' p {x:/ = ~} 
K(v 2m2) -1 p {X:,2 = ITT + KITi} , 
1',111, 
v2m 2 
ITi > 0, 
IT~ > - -
ITT. 
K 
(5.2.22) 
Figures 5.2.1 and 5.2.2 show contours of the joint posterior distributions for the 
two examples introduced earlier in Section 5.1.2. Dotted lines indicate those parts 
of the distributions which have been truncated. The examples respectively illu-
strate situations in whi'ch e~ has a positive and a negative value. Knowledge of 
the joint distribution of O'T and O'~ clearly allows a much deeper appreciation of 
the situation than is possible by mere point estimates. 
Modal Values of the 10illt Posterior Distribulion 
On equating partial derivatives to zero, it is easily shown that the mode of the 
unconstrained distribution (5.2.22) is at 
IT~ o = 
(5.2.23) 
The mode is thus to the right of the line IT~ = 0 provided that 
1112 
VI (1'2 + 2) 
-> 
. 
1111 
V2 (VI + 2) 
(5.2.24) 
In this case, the modes of the posterior distribution in (5.2.14) and of the unconstrained 
distribution are at the same point. For the dyestuff example, i112/in, = 4.6 which is 
much greater that 1'1 (1'2 -'- 2)/[v 2(v( + 2)J = 1.3 so that from (5.2.23) the mode is at 
(O'To = 2,262.7,0'~o = 1,277.7), as shown by the point P in Fig. 5.2.l. 
When the inequality (5.2.24) is reversed, it can be readily shown that the mode of 
the constrained distribution is on the line O'~ = 0 and is al 
(5.2.25) 

I 
I 
I 
I 
I 
I 
I , 
I 
\ 
o 
~ 
001 
0.8 
1.6 
2.4 
3.2 
40 
4.8 
5.6 
a~ x 10 3 
(The contours are labeled by the level of the density of (of x 10· 3, O~ x 10' J) 
calculated from (5.2.14). Dotted contour indicates the inadmissible portion of 
the un constrained distribution.) 
6.4 
Fig. 5.2.1 Contours of the joint distribution of the variance components (CiT, Ci~): the dyestuff example. 
tv 
U1 
0.. 
~ 
'" 
;:I 
Co 
0 3 
[%l 
;;r< 
~ 
:: 
0 c. 
II> 
(;;' 
7.2 
8.0 
Ul 
N 

5.2 
4.4 
3.6 
2.8 
2.0 
12 
", ...... 
' ... , , 
, ... " ..... ,," 
0.4 I 
b" I 
- 1.0 
-0.8 
-0.6 
0.4 
-0.2 
o 
0.2 
0.4 
0.6 
08 
1.0 
.2 
a~ x 10 I 
(The contours are labeled by the level of the densi ty of (0; , o~) 
calculated from (5.2 . 14). Dotted contours indicate the inadmissible portion of 
the unconstrained distribution.) 
1.4 
1.6 
Fig. 5.2.2 Contours of the joint distribution of the variance components ((J~, (J~): the generated example, 
u, 
2.0 
Ul 
tv 
t;I:I ., 
..., 
to '" ,;' 
:::l 
t ., 
Q" 
'" 
;n' 
o 
...., 
l: 
~ . ., 
... ,., :r 
;' 
:::. 
n 
~ 
'" '" ;; 
,., 
~ 
0' 
:::l 
'" 
~. 
;; 
..., 
~ 
o 
<: ., 
... ,;' 
" 
,., 
to 
n 
o 
3 
"0 
o 
:::l 
to 
:::l r;r 
N 
Ul 
-.J 

258 
Random Effect Models 
5.2 
For the generated example m2/m, = 0.56 which is less than 1'1(1'2 + 2)/[1'2(1' , + 2)J 
= 1.3 and consequently the mode is at (er~ 0 = 12.133, er~o = 0) as illustrated by the 
point P in Fig. 5.2.2. ff we ignored the constraint er~ > 0, then from (5.2.23) the mode 
would be at (eria = 13.796, er~o = - 1.568) as shown by the point p ' in the same figure. 
A negative value of &~ always leads to a distribution of (er~, er~) with its mode 
constrained on the line er~ = O. 
This is because &~ = 0 corresponds to (m2/m,) < 1 
which, with VI > v2 , implies that (m 2/m,) < vl(Vz + 2)/[V 2 (V 1 + 2)]. 
The mode reflects only a single aspect of the distribution. Inferences can best be 
made by plotting density 
contour~, a task readily accomplished with electronic 
computation. 
5.2.5 Distribution of eri 
The posterior distribution of er~ is obtained by integrating out er~ from the joint 
distribution (S.2.14) to give 
er~ > 0, 
(S.2.26) 
where 
J(eri) = Pr{X~2 < v1m2/ern . 
Pr {F" ,\" < m2/m,} 
We sball later in Section S.2.11 discuss an alternative derivation by the method of 
constrained distributions discussed in Section 1.5. 
for the moment we notice 
that if, as before, we denote an unconstrained distribution by an asterisk, then 
from (S.2.22) 
and 
PI' {F'2'" < m2} = Pr* {er~ > 0 I y} = Pr* {C I y}, 
m, 
(S.2.27a) 
(S.2.27b) 
where C is the constraint eri2 > er~ (or equivalently er~ > 0). We can thus write 
(S.2.26) as 
(S.2.2/<» 
where 
2 
Pr* {C I eri, y} 
J(er,) = 
Pr* {C I y} 
. 
Consider the first factor (v,mJ)-lp(X:/ = eri/v1m J ) on the right of (5.2.26). 
In this expression, eri is a random variable and 111, a fixed sample quantity, but 
since the sampling distribution of Vtln, for any fixed value of er~ is erix~" the same 

--
-~-
5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
259 
expression would also supply the usual confidence distribution for (Ji within the 
sampling theory framework. In this instance, therefore, the confidence distribution 
does not correspond to the posterior distri bution because the latter includes an 
additional modifying factor f(IJi) . 
Because this second factor has no counterpart in sampling theory, its presence 
in the Bayesian result is of special interest. It expresses the additional information 
about (Ji which comes from 111 2 , 
That some information of this kind exists is 
obvious on commonsense grounds. For instance, in the dyestuff example, 1112 = 
11 ,271 .5 is an estimate of (J~ + SIJ~. This tells us H'i/houl any reference to 1111 that 
values of (J~ say four times as large as 11 ,271 .5 are unlikely. 
This intuitive 
argument is given a precise expression in the Bayesian analysis through the 
modifying factor.r ((J~). The numerator of f ((J7) given by (S.2.27a) is a function 
of (Ji depending only on m 2 . It is Pr* {C I (Ji, y}, the probability of the constraint 
(J7 2 > IJ; being true for each specific value of (Ji. 
The denominator, given by 
(S.2.27b), is Pr* {C I y}, the probability of the same constraint being true over all 
values of (Ji . It is independent of (Ji and is merely a normalizing constant. Figures 
5.2.3 and 5.2.4 show the effect of the modifying factor for the two sets of data we 
have previously discussed. Shown in each case are: 
pta; Iy) x 103 
0.50 
0. 25 
2.0 
4.0 
Int = 2,451.25 
ml = J 1,271 .50 
6.0 
2. 0 
1.0 
Fig. 5.2.3 Decomposition of the posterior distribution of IJ~ : the dyestuff example. 
-

260 
Random Effect Models 
5.2 
a) the unconstrained distribution p*«(J~ I y) ; this is also the confidence distri-
bution of (JT which would be obtained from sampling theory, 
b) the modifying factor f«(Jf) , and 
c) the product of (a) and (b) which is the posterior distribution of (J~. 
The roles played by the two factors in determining the distribution of (Jf 
depend critically on the relative size of In 1 and m2 . In the first example, 1n2 is 
large compared with 111 1, and we see that over the range in which p * «(J~ I y) is 
appreciable,f«(Jf) is relatively flat. Multiplication then produces a p«(J~ I y) which 
is close to P*«(JT I y) and the modifying factor has little effect on the distribution. 
For the second example, however, beca use m 2 is actually less than In J , the factor 
f«(Jf) falls off quite sharply over the r?nge in which p*(CJi I y) i, appreciable and 
multiplication produces a p(aT i y) which is considerably modified. 
p(a ; l y) x IO 
1.0 -, , 
\ 
\ 
\ 
0. 5 
\ 
\ 
1.0 
2.0 
nJ, = 14.9 5 
1n 2 = 8.34 
3.0 
40 . 
2.0 
Fig. 5.2.4 Decomposition of the posterior distribution of IJ'f: the generated example. 
The decomposition illustrated in Figs. 5.2.3 and 5.2.4 of the posterior distri-
bution of CJi into its two basic components is of interest for another reason. 
Examples are occasiona Ily met where m2 is significantly smaller than mI ' r n such 
cases it has been suggested, for example by Anscombe (l948b) and NeIder (J 954), 
in the sampling theory framework, that the model itself should be regarded as 
suspect. From the Bayesian point of view we are led to the same conclusion. 
For, when 1n 2 is very small compared with 111 1, the factor f(CJf) will fall sharply and 
the information about CJT coming from the two components will be contradictory. 

5.2 
Bayesian Analysis of Hierarchical Classifications nith Two Variance Components 
261 
Effects of this kind can in particular be produced by serial correlation between 
the errors and, as has been pointed ou t earl ier, Bayesian analysis which takes ac-
count of this correlation has been carried out. 
Sampling Theory interpretation of the Variolls Factors 
Once more, while it does not seem possible to justify a similar analysis on sampling 
theory, the meaning which can be attached to the various factors in (5.2.26), from the 
sampling theory point of view, is interesting. From (5.2.26) we have 
(5.2.28) 
Sampling theory inferences about a~ arc usually made using the fact that 
(5.2.29) 
where vlm l is the random variable and ar is the fixed but unknown parameter. The 
second factor of (5.2.28) may be written 
Pr {X~, < v2m2/af} 
Pr {Fv2 •vl < m2!md 
1 - a(af) 
I-a 
(5.2.30) 
In this expression, the quantity a(a7), which is a jimetion of af, is the significance level 
associated with the test in which the mean square m2 is employed to test the 
hypothesis that a~ 2 = aT against the alternative a~ 2 > a~ for each specified value of a~. 
As mentioned earlier in Section 5.2.3, the quantity a is the significance level associated 
with the over-all test, based on the mean square ratio m2 /ml' of the hypothesis that 
a72 = a~ against the alternative that aT 2 > aT where a7 is not specified. 
Direct Calculation of p(a7 I y) 
The distribution of ai in (5.2.26) is equal to the product of two factors, the first 
being the density of an X-
2 variable with VI degrees of freedom and the second, 
the ratio of a / 
probability integral and a F integral. The density function is 
readily calculated therefore using tables or charts of the X2 density and integral, 
and of the F integral. The distribution can also be expressed as a weighted series 
of X2 densities. Details are given in Appendix (AS.I). 
5.2.6 A Scaled X - 2 Approxim.ation to the Distribution of a7 
Although the density function of aT can be calculated directly from (5.2.26), for 
many practical purposes and in particular to study the problem of "pooling of 
variance estimates," it is useful to use an approximation involving only a single 
x2 variable. 

262 
Random Effect Models 
Writing 
vlm l 
z =
-
2-
0") 
5.2 
(5.2.31) 
and applying the identity (AS.2.1) in Appendix AS.2, we nnd directly from (5.2.12) 
that the rth moment of z is 
Further, the moment generating function is 
{ 
m2!ml }' 
Pr Fv"v, < ~ 
Mz(/)=(1-2t) -
~ V
' 
, 
{ 
in2 \ 
Pr F V2,v, < -1 
/11 1 
(5.2.32) 
If I < l 
(5.2.33) 
Consider the two extremes case when nil / 111 I is very large and when it is close 
to zero. In the first case, .'Vfz(t) tends in the limit to (I - 2ff i'" so that the distri-
bution of z tends to the X2 distribution with VI degrees of freedom. When /112/ml 
tends to zero, it is easy to verify by applying L'Hospital's rule that 
(5.2.34) 
so that the distribution of z again tends to the X2 distribution but with V2 additional 
degrees of freedom. 
The preceding discussion suggests that the distribution of z could be well 
approximated by a scaled X2 variable. 
By equating the first two moments of z 
to those ofax~ , we find 
VI i x Ctv 2,i;v 1 + I) 
2 
ixCtv2, 'h) 
(5.2.35) 
where 
v2m 2 
x=-----
vlm l + V2111 2 
and ix(p,q) is the incomplete beta integral. 
To this degree of approximation, 
then 

5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
263 
where, as before, the symbol 
"
,..c," means "approximately distributed as". 
In terms of O'~ we thus have 
(5.2.37) 
It can be verified that 0 < a < I and b > Vj' To illustrate this approximation, 
we show below in Table 5.2.2 the exact and approximate densities for the two sets 
of data introduced earlier. 
Using Pearson 's Tables of the Incomplete Befa-
Function (1934) we find that for the dyestuff example that a = 0.9912 and 
b = 24.26 and for the generated example a = 0.9426 and b = 28.67. The agree-
ment between the exact and the approximate values is exceedingly close for both 
examples. 
Table 5.2.2 
Comparison of the exact density p(O'i I y) with approximate density obtained 
v1m 1/O'i ,..c, ax; 
from 
Dyestuff example 
Generated example 
Approximate 
Approximate 
O'f x 10- 3 
Exact density 
density 
O'i x 10- 1 
Exact density 
density 
x 103 
x 103 
xl0 
xlO 
1.00 
0.0018 
0.0017 
0.60 
0.0064 
0.0062 
1.25 
0.0350 
0.0345 
0.80 
0.2099 
0.2097 
1.50 
0.1652 
0.1647 
1.00 
0.7959 
0.7975 
1.75 
0.3666 
0.3674 
1.20 
1.1605 
1.1610 
2.00 
0.5282 
0.5300 
1.40 
1.0530 
1.0518 
2.25 
0.5854 
0.5870 
1.60 
0.7432 
0.7418 
2.50 
0.5499 
0.5504 
1.80 
0.4573 
0.4566 
2.75 
0.4639 
0.4633 
. 2.00 
0.2611 
0.2611 
3.00 
0.3646 
0.3633 
2.20 
0.1435 
0.1437 
3.25 
0.2733 
0.2718 
2.40 
0.0775 
0.0778 
3.50 
0.1986 
0.1973 
2.60 
0.0416 
0.0419 
3.75 
0.1413 
0.1403 
2.80 
0.0225 
0.0227 
4.00 
0.0992 
0.0986 
3.00 
0.0122 
0.0124 
4.25 
0.0691 
0.0688 
3.20 
0.0067 
0.0068 
4.50 
0.0479 
0.0479 
3.40 
0.0038 
0.0038 
4.75 
0.0332 
0.0333 
3.60 
0.0021 
0.0022 
5.00 
0.0230 
0.0232 
3.80 
0.0012 
0.0013 
5.25 
0.0160 
0.0162 
4.00 
0.0007 
0.0007 
5.50 
0.0111 
0.0114 

264 
Random Effect Models 
5.2 
It will be recalled from (5.2.26) that if the additional information coming 
from m2 were ignored, then for each set of data we would have 
In fact we find 
Dyestuff example 
Generated example 
As expected, the modification which occurs in the dyestuff exampJe is very slight, 
whereas that for the generated example is considerably greater. 
5.2.7 A Bayesian Solution to the Pooling Dilemma 
On sampling theory, when m l and in2 are of about the same size they are often 
pooled. The idea is that if it can be assumed thaI a~ is zero, the appropriate estimate 
of ai is (vlm l + V2m2)'(VJ + v2 ) :lnd not 111 1, 
Thus: 
a) if /112/ml is large, the estimate of O"~ is taken to be /11 1 , distributed as 
O"fXv:/v J, 
b) if mz/ml is close to one, the estimate of (Ji is taken to be (vlm l + v2m 2)! 
(VI + vz), distributed as (JTX;, +v,!(v J + V2)' 
Another way of saying this would be, th~t the estimate is taken to be 
vJmJ + },V2m 2 
VI + WV2 
with the weights) and W such that 
a) ) = W = 0 
if mztmJ is large 
b) ) = W = I 
if m2 fml is close to one 
Now, from the Bayesian viewpoint the distribution of (Ji may be approximated , 
as in (5.2.36), by (vlm1)·'(Ji '" ax;. This approximation may equally weJl be 
written in terms of the pooled estimate as 
X
2 
", VI +WV l 
(5.2.38a) 
VI + WV 2 
or in terms of sums of squares as 
(5.2.38b) 

5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
265 
where 
(5.2.38c) 
W= 
Since 0 < a < J and b > VI' the weights A and ware both positive. 
For illustration, consider first the dyestuff example. Here the mean square ratio 
(m2/ml = 4.6) is quite large and the corresponding weights (), = 0.009, w = 0.052) 
are therefore small. 
The contribution of m 2 for this example may thus be 
ignored for practical purposes, and the posterior distribution and the sampling 
theory confidence distribution essentially coincide. For the generated example, 
however, the mean square ratio (m2/m I = 0.56) is small and the corresponding 
weights (A = 0.524, w = 0.934) are no longer negligible. In th is case then, the 
contribution of m2 to the posterior distribution of O'i is considerable. These two 
posterior distributions have of course already been shown in Figs. 5.2.3 and 
5.2.4 and their derivation discussed . The present excursion merely provides an 
alternative but interesting means of viewing them. 
Using (5.2.35) with (5.2.38c), A and w may be plotted as functions of m2!m J 
for any J and K. Figure 5.2.5 shows such graphs for the case J = 6 and K = 5 
corresponding to the examples we have considered. As might be expected, for 
large values of m 2.'m l , }. and w approach zero but, as m 2/m l becomes smaller, 
larger values of ), and w calling for increasing degrees of pooling occur. 
1.0 
0.8 
0.6 
0.4 
0.2 
L __ L __ 
--L __ --1._==:::::r=""""-.:C=====_ ~} ~ 
1.0 
3.0 
5.0 
tn , 
Fig. 5.2.5 Values of (A., w) as functions of m2/mj (J = 6, K = 5). 

266 
Random Effect Models 
5.2 
It will be noticed that w is larger than ), for all values of m2/mj . To see the reason 
for this, consider the case where m l and m2 are equal so that m2/mj = 1. 
While m l 
provides an estimate of a~ alone, m2 is an estimate of aL = (f~ + KO"i and d must be 
positive or zero. Thus with m2!m j = I the evidence coming from 1112 and 1/1 1 together 
implies a smaller value for O"~ than would have been suggested by m) alone. This is 
consonant with the fact that, for m2/mj = I, OJ is larger than }" and corresponding 
compensation would be expected, and is found, for other values of m2/m j • 
Th us, in the Bayesian context there is no dilemma "to pool or not to pool." 
The distribution of O"i ahrays depends on n72 as well as on m), as obviously it 
should. No "decision" needs to be made, instead there is a steady pooling tran-
sition which depends on the evidence suppJied by the sample. 
5.2.8 Posterior Distribution of a~ 
We now turn to the problem of making inferences about a~ which in the context 
of sampling theory is accompanied by many difficulties. The posterior distribution 
of a~ is obtained by integrating (5.2.14) over ar to yield 
( 
2 
) _ K(vlm))-I (v 1m 2)-)IW (, -2 _ of + Ka~) (. -2 _ 
O"f ) d 2 
p 0"21 y -
P {F 
} 
P Xv, -
P Xv, 
- --
ai ' 
r 
'"2. \", < m2/ m l 
0 
V2111 2 
vlm l . 
O"~ > O. 
(5.2.39) 
For the noninformative reference prior distribution (5.2.9), the distribution 
p(a~ 1 y) summarizes all our a posteriori knowledge about a~ . It is defined over 
the range (0, co) and thus no problem of "negative variance estimate" will ever 
arise. The exact posterior distributions of O"~ for the dyestuff example and for the 
generated example are given by the solid curves in Figs. 5.2.6 and 5.2.7 
respec.~ively. 
Some properties of the distribution of O"~ are discussed in Appendix AS.3., , In 
particular, it is shown that 
a) if 
m2 
v2 +2 
VI 
->----
m) 
V2 
VI + 2' 
then the distribution is monotonically increasing in the interval 0 < O"~ < CI' and 
monotonically decreasing in the interval O"~ > c2' where 
(5.2.40) 

5.2 
-~ 
--
-
-~~-
_.-
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
0.3 
'0 
>. 
0.2 
NN 
-3 
"-
0.1 
I.~ 
I 
~ 
~ 
I 
Exact 
----- Approximate 
Fig. 5.2.6 Posterior distribution of (Ji: the dyestuff example. 
--- Exact 
--- -
Approxil11Jte 
4.0 
0 
>-
~I 
"-
2.0 
1.0 
2.0 
ol x )0- 1 
Fig. 5.2.7 Posterior distribution of (Ji: the generated example. 
267 

268 
Random Effect Models 
5.2 
so that the mode must lie in the interval c l < (J2 < C2, and 
b) if 
nl2 
(V2 + 2) ( 
Vj 
) 
m l < -v-
2 -
VI + v2 + 4 ' 
then the distribution is monotonically decreasing in the entire range (Ji > 0 and the 
mode is at the origin. 
For the dyestuff example, m2/mJ == 4.60, which is greater than 
( _VJ ) (~) 
== 1.29 
vJ + 2 
v2 
so that the mode must lie in the interval 
C l < (J2 < C2 where cl = 684.08 and 
C2 = 1,253.67. 
Inspection of Fig. 5.2.6 shows that the mode is approximately at 
(Ji = I , I 00. 
For the generat.:d example, 1112/111 I = 0.558 which is less than 
(~) 
( 
VI 
) = 1.02. 
v2 
VI + v2 + 4 
Thus, as shown in Fig. 5.2.7, the distribution is J shaped, having its mode at the origin 
and monotonically decreasing in the entire range (Ji > o. 
5.2.9 Computation of p((J~ I y) 
It does not appear possible to express the distribution of (J~ in (5.2.39) in terms 
of simple functions. The density for each value of (J~ can, however, be obtained 
to any desired accuracy by numerical calculation of the one-dimensional integral. 
This was the method used to obtain the exact distributions of Figs. 5.2.6 and 
5.2.7. Approximations which require less labour are now described. 
A Simple X2 Approximation 
It is seen from the posterior distribution of (J~ and (J~ in (5.2.14) that if (J~ were 
known, say (JT = (JTo, then the conditional distribution of CT~ would be in the form 
of a truncated "inverted" X2 distribution. That is, 
(J~ > 0, 
(5.2.41) 
so that the random variable 
(5.2.42) 
is distributed as X- 2 with 
V2 degrees of freedom truncated from (JiO/(V2 m2). 
Although the marginal posterior distribution p«(J~ I y) cannot be expressed exactly 
~-
-
~ ~-
-
--
-

5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
269 
in terms of such a simple form, nevertheless, the expression of the integral in 
(5.2.39) does suggest that, for large vu2, we can write 
fo
a p (X:,2 = (Ji + K(J~)p (x~/ =~) d(~) 
Y2ml 
vim i 
ylm l 
(5.2.43) 
where the substitution z = Yl m 1: (2CJi) is made and t = Yr/2 is the value of z which 
maximizes the factor zi
V1 ,2) e- c appearing in the integrand. Making use of (5.2.43) 
and upon renormalizing, we thus arrive at a simple truncated inverted / 
approximating form for the posterior distribution of (JL 
u~ > O. 
(5.2.44) 
or equivalently, 
( 
-2 
In! + Kui) 
( 
K 21) 
P X
V
2 = 
P Inl ~ 
Y == 
Y2 ln 2 
V2m
2 
Pr f X;,2 > ~} 
l 
V2ln 2 
m[ + K(J~ 
1111 
- --->--. 
V2m2 
V2m2 
(5.2.45) 
In words, we have that (mr -+- KuD/(v 2m 2 ) is approximately distributed as X- 2 
with V2 degrees of freedom truncated from below at i11! / (V 2lnl)' Or equivalently, 
v1m2/(m r + Ku~) is distributed approximately as X2 with V2 degrees of freedom 
truncated from above at V 2ln 2 :m!. Posterior probabilities for u~ can thus be 
approximately determined from an ordinary X2 table. In particular, for I'} > 0 
{ 
v2m2 
2 
V2 m 2 } 
Pr 
< Xv, <--
Pr {O < u~ < I'} I y} == 
m! + KI'} 
-
m] 
Pr {X;'2 < V~~2 } 
(5.2.46) 
The posterior densities for d for each of the two examples obtained from this 
approximation are shown by the dotted curves in Figs. 5.2,6 and 5.2.7. It is seen 
that this simple approximation gives satisfactory results even for the rather small 
sample sizes considered. 

270 
Random Effect Models 
5.2 
Table 5.2.3 
Comparison of exact and approximate posterior density of O"~t 
a) Dyesluff Example 
p(d I y) x 103 
(1) 
(2) 
(3) 
(4) 
O"~ x 10- 3 
1° 
1-1 
{- 2 
Exact 
0.0 
0.0070 
0.0246 
0.0332 
0.0290 
0.2 
0.0591 
0.0865 
0.085J 
0.0841 
0.4 
0.1518 
0.1694 
0.1665 
0.1667 
0.6 
0.2386 
0.2455 
0.2433 
0.2440 
0.8 
0.2948 
0.2952 
0.2938 
0.2946 
1.0 
0.3200 
0.3169 
0.3161 
0.3170 
1.1 
0.3233 
0.3193 
0.3187 
0.3196 
1.2 
0.3221 
0.3175 
0.3J 70 
0.3179 
1.3 
0.3173 
0.3123 
0.3120 
0.3128 
1.4 
0.3099 
0.3047 
0.3045 
0.3053 
1.5 
0.3006 
0.2953 
0.2952 
0.2959 
1.6 
0.2899 
0.2847 
0.2846 
0.2853 
1.7 
0.2784 
0.2733 
0.2732 
0.2740 
1.8 
0.2664 
0.26J5 
0.2615 
0.2621 
1.9 
0.2543 
0.2495 
0.2495 
0.2502 
2.0 
0.2422 
0.2376 
0.2376 
0.2383 
2.2 
0.2187 
0.2145 
0.2146 
0.2151 
2.4 
0.1967 
0.1930 
0.1930 
0.1935 
2.6 
0.1766 
0.1732 
0.1733 
0.1737 
2.8 
0.1584 
0.1554 
0.1555 
0.1559 
3.0 
0.142J 
0.1395 
0.1395 
0.1399 
3.5 
0.1089 
0.1070 
0.1070 
0.1073 
4.0 
0.0843 
0.0829 
0.0829 
0.0831 
4.5 
0.0661 
0.0650 
0.0650 
0.0652 
5.0 
0.0524 
0.0516 
0.0516 
0.0518 
5.5 
0.0421 
0.0415 
0.0415 
0.0416 
6.0 
0.0342 
0.0337 
0.0337 
0.0338 
6.5 
0.0281 
0.0277 
0.0277 
0.0278 
7.0 
0.0232 
0.0230 
0.0230 
0.0230 
7.5 
0.0194 
0.0192 
0.0192 
0.0193 
8.0 
0.0164 
0.0162 
0.0162 
0.0162 

S.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
271 
Table 5.2.3 (continued) 
b) Generated Example 
p((J~ I y) x 10 
(1) 
(2) 
(3) 
(4) 
(J~ X lO-1 
to 
t -I 
t- 2 
Exact 
0.00 
5.3566 
5.5170 
5.4635 
5.4795 
0.05 
3.8069 
3.8943 
3.8829 
3.8897 
0.10 
2.7671 
2.7997 
2.8003 
2.8039 
0.15 
2.0563 
2.058 1 
2.0623 
2.0644 
0.20 
j .5595 
1.5463 
1.5514 
1.5527 
0.25 
1.2046 
1.1853 
1.1902 
1.1911 
0.30 
0.9460 
0.9251 
0.9294 
0.9301 
0.35 
0.7538 
0.7336 
0.7373 
0.7379 
0.40 
0.6087 
0.590J 
0.5932 
0.5937 
0.45 
0.4974 
0.4807 
0.4833 
0.4837 
0.50 
0.4107 
0.3961 
0.3982 
0.3986 
0.60 
0.2881 
0.2769 
0.2784 
0.2787 
0.70 
0.2086 
0.2002 
0.2012 
0.2014 
0.80 
0.1552 
0.1488 
0.1495 
0.1497 
0.90 
0.1181 
0.1132 
0.1137 
0.1138 
1.00 
0.0916 
0.0878 
0.0882 
0.0883 
1.20 
0.0579 
0.0556 
0.0558 
0.0559 
1.40 
0.0386 
0.037J 
0.0372 
0.0373 
1.60 
0.0268 
0.0258 
0.0259 
0.0259 
l.80 
0.0193 
0.0186 
0.0187 
0.0187 
2.00 
0.0143 
0.0138 
O.OJ 38 
0.0138 
t Accuracy of various approximations [see expression (/\5.4.7) in Appendix A5.4 for the formulas). 
(I) Asymptotic expansion with the leading term. 
(2) Asymptotic expansion with terms to order I " l 
(3) AsymptOtic expansion with terms to order I 
2 
(4) Exact evaluation by numerical integration. 
An Asymptotic Expansion 
T he argument which led to (5.2.44) can be further exploited to yield an asymptotic 
expansion of p«(J~ I y) in powers of I - I = (vl /2) - I following the method discussed for 
example in Jeffreys and Swirlee (1956) and De Bruijn (1961). Details of the derivation 
of the asymptotic series are given in Appendix A5.4. The series is such that the simple 
approximation in (5.2.44) is the leading term. 
Table 5.2.3 shows, for the two 
examples, the degree of improvement that can be obtained by including terms up to 
order , - I and up to order ,-2, respectively. 

272 
Random Effect Models 
5.2 
T he Distribution of u~ When the .\1 ean Square Ratio is Close to Zero 
Finally, we remark here that in the extreme situation when /1121111 approaches 
zero the posterior distribution of Kui l1112 becomes diffuse, a result noted by Hill 
(1965). 
This behavior 
i ~ reflected by the approximating form (5.2.44) from 
which it will be seen that as 1112/ml tends to zero, the distribution of Ku~ /m2 
approaches a horizontal line. In the limit, then , all values of Ku~ ,'m2 become 
equally probable and no information about u~ is gained . It has been noted in 
Section 5.2.5 that if m2/m is, close to zero, this will throw doubt on the adequacy 
of the model. In a Bayesian analysis the contradiction between model and data 
is evidenced by the conflict between p*(u~ I y) and fCui). 
The distribution of 
K(J~ .t 111 2 thus becomes diffuse precisely as overwhelming evidence becomes 
available that the basis on which this distribution is computed is unsound. 
If 
assumptions are persisted in, in spi te of evidence that they are false, bizarre 
conclusions can be expected. 
Table 5.2.4 
Summarized calculations for approximating the posterior distributions of (uT, u~ ) 
(illustrated using dyestuff data) 
1. 
Use standard analysis of variance (Table 5.2.1) to compute 
1111 = 2,451.25, 
1112 = 11 ,271.50, 
K = 5. 
2. 
Use (5.2.35) to compute 
x = 0.4893, 
a = 0.9912, 
b = 24.26. 
3. Then, using (5.2.37) the posterior distribution of the "within" component ai is given 
by 
and for this example 
()'~ rV 59,352.30X2L6' 
4. Also from (5.2.45) posterior distribution of the "between" component (J~ is given by 
truncated from below at 
For this example, 
Inl + Kai 
- 2 
---- rV XV2 
V21112 
0.043 + (0.89 x 1O-4a~) rV XS2 
truncated from below at 0.043. 

--
-
~--- -
-
-- ---~~ 
5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
273 
5.2.10 A Summary of Approximations to the Posterior Distribution of (aT , aD 
Using the dyestuff data for illustration Table 5.2.4. above provides a summary of 
the calculations needed to approximate the posterior distributions of aT and a~ . 
5.2.11 Derivation of the Posterior Distribution of Variance Components Using 
Constraints Directly 
It is of some interest to deduce directly tbe distributions of the variance components 
by employing the result (1.5 .3) on constrained distributions. We bave seen tbat 
the data can be thought of as consisting of J independent observations from a 
population N(e, aTz/K) and J(K -
1) further independent observations from a 
population N(e, aT). 
Suppose that, as before, we assume that the joint prior 
distribution pee, log ai, log ai 2) is locally uniform but now we temporarily omit 
the constrai nt C: ai 2 > a~ . Then (TT and aT z would be independent a posteriori. 
Also, if, as before, p*(x) denotes the unconstrained density of x, then 
ai> 0, 
(5.2.47) 
(5.2.48) 
W > 0, 
(5.2.49) 
where W = ai2/ai. It follows that 
(5.2.50) 
(5.2.5\) 
and 
(5.2.52) 
Using (1.5.3) the joint posterior distribution of (aT, aiz) given the constraint 
Cis 
aiz > at > o. 
(5.2.53) 

274 
Random Effect Models 
5.2 
Noting that given (ai, aiz) such that ai2 > a7, Pr"'{C I a~, aiz, y} = 1, we have 
( 2 2 I ) _ (vjml)-I p [X;,2 = ai/(vjm j)] (v2m2)-1 p [X:/ = aT2t(v2 m2)] 
p aj, al2 Y -
, 
Pr {Fy"v, < m2/ml} 
ai2 > a~ > 0, 
(5.2.54) 
which is, of course, equivalent to the posterior distribution of (ai, aD in (5.2.14) 
if we remember that ai2 = d + Ka~. The distribution of the ratio W given C 
is, by a similar argument, 
p"'(W I y) Pr'" {C I W, y} 
p(Wly)= 
Pr"'{Cly} 
(m l !m 2)p(FY"
Y2 = (mdm2) W) 
Pr {F."y, < m2/ml} 
W> I, 
(5.2.55) 
where Pi'" (C I W, y) = !. This is the same distribution obtained earlier in (5.2.16). 
Further, the distribution of ai subject to the constraint C is 
2 
) _ p"'(ai I y) Pr'" {C I ai, y} 
p(ally -
Pr*{Cly} 
-1 (-2 af) 
{2 
v2m
2 } 
(vjm l ) 
P 
XVI 
= --
Pr Xv, < -2-
Vjmj 
0'1 
= -------------------------
Pr {F. 2,vl < :~} 
aT > 0, 
(5.2.56) 
which is identical to expression (5.2.26) . 
Finally, by the same argument, the 
distribution of ai 2 given C is 
(a2 I ) _ p*(ai21 y) Pr* {C I ai2' y} 
p 
12 Y -
Pr* {C I y} 
aT 2 > 0. 
(5.2.57) 
Certain properties of this distribution are discussed in the next section. 
5.2.12 Posterior Distribution of ai2 and a Scaled X- 2 Approximation 
Earlier we saw that not only mj but also m 2 contained information about the 
component ai. In a similar way we find that both mj and m 2 contain information 
about ai 2' 
While this parameter is not usually of direct interest for the two-
component model, we need to study its distribution and methods of approximating 

5.2 
Bayesian Analysis of Hierarchical Classifications with Two Variance Components 
275 
it for more complicated random effect models. From (5.2.57) it is seen that the 
posterior distribution of a~2 is proportional to the product of two factors. The 
first factor is a X- l density function while the second factor is the right tail area 
ofaXl distribution . This is in contrast to the posterior distribution of aT in (5.2.56), 
where the probability integral is a left tail probability of a / 
variable. Using the 
identity (A5.2.1) in Appendix A5.2, the rth moment of z 1 = v2m z/uT 2 is 
r(V2+r)pr{Fvl+zrV' < 
Vz 
ml} 
2 
'v + 2r m 
E(z~) = 2r . .  
2 
1 
r (;1) Pr {Fv"v, < ::} 
(5.2.58) 
and the moment generating function is 
Mzj(t) = (I - 2/)- }v, Pr {F V2 ,v, < (ml/m 1)(1 - 2t)}, 
It I < -}. 
(5.2.59) 
Pr {FV1,v, < m2/m 1 } 
When 111 2/m 1 approaches infinity, Mz,(t) tends to (I - 2t)-),'2 so that the distri-
bution of Z 1 tends to the X2 distribution with V2 degrees of freedom. This suggests 
that, for moderately large values of m2/ml, the distribution of Zl = V2ml/d1 
might well be approximated by that of a scaled / 
variable, say ex;. 
By equating 
the first two moments, we find 
e = -
+ 1 
-
-
-
- ----,--
( 
Vz 
) IxCtVl + 2, -tv l) 
V2 IxC-}V1 + 1, i-vI) 
2 
IxCi-v2 + l,-!Vl) 
2 
IxC-tv1, ! v1) 
(5.2.60) 
d = ~ IxC1-vz + 1, -tv 1) 
c 
IxCfv2,1Vl) 
, 
where we recall from (5.2.35) that x = V2ml'(v1ml + vlml). Thus, to this degree 
of approximation, 
or 
(5.2.61) 
and values of c and d can be readily determined from tables of the incomplete 
beta function. 
J n practice, agreement between the exact distri bution and the approximation 
is found to be close provided m 2/ 111 1 is not too small. Ta}:>le 5.2.5 gives specimens 
of the exact and the approximate densities for the dyestuff and the generated 
examples. The agreement for the dyestuff example is excellent. For the generated 
example, for which m2/ml = 0.56, the approximation is rather poor. 
The 
reason for this can be seen by studying the limiting behavior of the moment gener-
ating function of Z 1 in (5.2.59) when m1/m 1 -> O. Applying L'Hospital's rule, we 
find 
lim 
.'v1z,(t) = I, 
"l2/»IL-0 

276 
Random Effect Models 
5.3 
which is far from the moment generating function ofaX2 distribution. Thus, we 
would not expect the approximation (5.2.61) to work for small values of m2Im! . 
Table 5.2.5 
Comparison of the exact density p(aTzl Y) with approximate density obtained from 
v2m2/(JTz rV cXJ 
Dyestuff example 
Generated example 
ai 2/1000 
Exact 
Approximate 
(Ji2 /1O 
Exact 
Approximate 
3.0 
0.0041 
0.0047 
0.8 
0.0172 
0.0396 
5.0 
0.0399 
0.0385 
1.0 
0.1233 
0.1435 
7.0 
0.0626 
0.0619 
1.2 
0.3074 
0.2774 
9.0 
0.0640 
0.0639 
1.4 
0.4543 
0.3852 
11.0 
0.0557 
0.0564 
1.6 
0.5101 
0.4427 
13.0 
0.0461 
0.0468 
1.8 
0.4936 
0.4541 
15.0 
0.0372 
0.0379 
2.0 
0.4409 
0.4336 
17.0 
0.0300 
0.0304 
2.2 
0.3776 
0.3955 
\9.0 
0.0242 
0.0245 
2.4 
0.3170 
0.3501 
21.0 
0.0196 
0.0198 
3.0 
0.1836 
0.2217 
25.0 
0.0132 
0.0133 
3.5 
0.1198 
0.1456 
30.0 
0.0084 
0.0084 
4.0 
0.0812 
0.0955 
35.0 
0.0054 
0.0053 
4.5 
0.0571 
0.0633 
5.2.13 Use of Features of Posterior Distribution to Supply Sampling Theory 
Estimates 
In recent years sampling theorists have used certain features of Bayesian posterior 
distribution to produce "point estimators" which are then judged on their sam pIing 
properties. We briefly discuss this approach in Appendix A5.6. 
5.3 A THREE-COMPONENT HIERARCHICAL DESIGN MODEL 
The results obtained in the preceding sections may be generalized in various ways. 
In the remainder of this chapter an extension from the two-component to the 
three-component hierarchical design model is studied. 
In Chapters 6 and 7, 
certain two-way cross classification random effect models and mixed models are 
considered. 
The following development of the three-component model is broadly similar 
to that for two components. However, the reader's attention is particularly directed 
to the study in Section 5.3.4 of the relative contribution of variance components. 
Here new features arise and study of an example leads to somewhat surprising and 
disturbing conclusions which underline the danger of using point estimates. 

5.3 
A Three-component Hierarchical Design Model 
277 
For convenient reference we repeat here the specification of the three-compo-
nent model already given at the beginning of this chapter. 
It is supposed that 
1=1, ... , 1; )=1, ... , 1; k=l, .. . ,K, 
(5.3.1) 
where Yijk are the observations, e is a common location parameter, ei, eij and eijk 
are three different kinds of random effects. We further assume that the random 
effects (ei, eij, eijk) are all independent and that 
ei ~ N(O, O"D, 
and 
(5.3.2) 
It follows in particular that Var (Yijk) = O"f + O"i + O"~ so that the parameters 
(ai, O"i, O"~) are the variance components. 
As previously mentioned, a hierarchical design of this type might be used in 
an industrial experiment to investigate the variation in the quality of a product 
from a batch chemical process. One might randomly select 1 batches of material, 
take 1 samples from each batch, and perform K repeated analyses on each sample. 
A particular result Yijk would then be su bject to three sources of variation, that 
due to batches ei, that due to samples eij and that due to analyses eijk. The purpose 
of an investigation of the kind could be (a) to make inferences about the relative 
contribution of each source of variation to the total variance or (b) to make infer-
ences about the variance components individually. 
5.3.1 The Likelihood Function 
To obtain the likelihood function, it is convenient to work with the transformed 
variables Yi .. , Yij. - Yi., and Yijk - Yij. rather than the observations Yijk themselves. 
By repeated application of Theorem 5.2.1 (on page 250) the quantities 
Yi. . = e + ei + ei. + ei.. , 
Yij. - Yi. . = (eij -
ei ·) + (eij. - eiJ, 
are distributed independently and 
where 
Yi.. ~ N(O, O"i23/1K), 
KII(Yij . - hi ~ d2X;(J-l )' 
I I I (Yijk - Yij)2 ~ O"i X ;J(K-l)' 
and 
as defined in Table 5.1.1. 
(5.3.3) 
(5.3.4) 
(5.3.5) 

278 
Random Effect Models 
5.3 
Further, insofar as Yijk - Yij. and Yij. - Yi.. are concerned, ~ ~ ~ (Yijk - YuY 
and K ~ ~ (Yij. -
Yi. Yare sufficient for a~ and a~ 2, respectively. 
Thus, the 
likelihood function is 
(5.3.6a) 
where (m l , n12, m3) and (VI' V2, 1'3) are the mean squares and degrees of freedom 
given in Table 5.1.1, that is, 
VI = JJ(K -
I), 
V2 = J(J -
I), 
1'3 = (l -
I), 
52 
K L L (Yij. - YiJ
2 
m 2 =- = 
(5.3.6b) 
v2 
V2 
and 
53 
JKL(h.-YY 
m3 = -
= 
1') 
v3 
The likelihood function in (5.3.6a) can be regarded as having arisen from J inde-
pendent observations from N(O, ai23jJK), together with J(] -
I) independent 
observations from N(O, aT2/K) and a further JJ(K -
1) independent observations 
from N(O, ai). 
From (5.3.5) the parameters (ai, a;2, ai2)) are subject to the 
constraint 
(5.3 .7) 
On standard sampling theory, the unbiased estimators for (a~ , ai, a~) are 
~2 
a2 = 
5.3.2 Joint Posterior Distribution of (ai, aL aD 
(5.3 .8) 
As before, the posterior distribution of the parameters is obtained relative to 
the noninformative prior distribution p(e, log a;, logai2 ' loga;23) rx constant. 
If there were no constraint, then, a posteriori (o-i, 0-12, o-f 23) would be independently 
distributed as (v lm j )x;,2, (V2m2)X:22, and (v3m3)x~2, respectively. Using (1.5.3), 

5.3 
A Three-component Hierarchical Design Model 
279 
with the constraint C: o-T < o-L < o-T 23, the joint posterior distribution is 
p(o-f, o-i2, o-i23 I Y) = p*(o-i, o-T2, o-in l C, y) 
*( 2 
2 
2 
1 )Pr*{C
1 o-i,o-i2,o-T23'Y} 
-p 0-
0-
a 
v 
-
1>] 2, 
J 23 
J 
Pr* {C 1 y} 
_ 
*( 21 
) 
*( 2 1 ,) *( 2 
1 ,) Pr* {C 1 o-i, o-i2' ai23' y} 
-
p a I Y P 
0- 12 
) P a 12 3 ) 
Pr* {C 1 y} 
, 
(5.3.9) 
where, as before, an asterisk indicates an unconstrained distribution. Since the 
factor Pr* {Clai,ai2,o-i23,Y} takes the value unity when the parameters satisfy 
the constraint and zero otherwise, it serves to restrict the distribution to that region 
in the parameter space where the constraint is satisfied . Thus, 
p*(ai 1 Y) P*«(JT 2 1 y) p*(ai23 1 Y) 
Pr* {C 1 y} 
P {
X~I 
vim] 
X~2 
112m 2 } 
r -2->--'-2 >--
Xl" 
V2m 2 
XV} 
V3m3 
ai23 > ai2 > ai> 0, 
(5.3.10) 
where in the denominator (X~I' X~" X;') are independently distributed X2 variables 
with (VI' v2 , v3) degrees of freedom respectively. 
From (5.3.10), the joint posterior distribution of the variance components 
(ai, o-~, an is 
(5.3.11) 
where 

--
280 
Random Effect Models 
5.3 
It can be verified that 
(5.3.!:?) 
where lAp, q) and B(p, q) are the usual incomplete and complete beta functions, 
respectively. 
We now discuss various aspects of the variance component distribution. 
5.3.3 Posterior Distribution of the Ratio CTi 2/CTT 
In deriving the distributions in (5.3.10) and (5.3.11), use has been made of the fact 
that the variances (CTi, CT~2' CTi23) are subject to the constraint C: CTi23 > CTT2 > CTT. 
To illustrate the role played by this constraint, we now obtain the posterior distri-
bution of the ratio CTi 2/CTi and show its connection with sampling theory results. 
For the two-component model the posterior distribution of the ratio CTi 2/CTT, 
which we there called W, was discussed in Section 5.2.3. In the persent case the 
posterior distribution of CTT 2/()T can be obtained from (5.3.10) by making a 
transformation from (CTf, CTT 2' CTi23) to (CTi2ICTT, CTT, CJT23) and integrating out CTi 
and CJT23. Alternatively, since 
it follows that 
where 
Pr* {C I CJT 2/CT;, y} 
Pr* {C I y} 
(5.3.13) 
(5.3.14) 
The first factor (m 1
' m2)p[Fv,.v, = (CTT2 /CTT)(m dm2)] is the unconstrained distri-
bution of CTT 2/CTT and the factor H (CTT 2/CTT) represents the effect of the constraint 

5.3 
A Three-component Hierarchical Design Model 
281 
C. Now, we can regard the C as the intersection of the two constraints C 1: crf 2 > cri 
and C2 : crf32 > af2' Thus, we can write 
Pr~ {C I ~, y} 
Pr*{Cly} 
Pr* {c I I ~ , y} 
Pr* {C I I y} 
pr*{c 2 j CI'~ ' Y} 
Pr* {C2 1 C I , y} 
(5.3.15) 
from which it can be verified that 
(5.116) 
with 
and 
P {
X~' 
vlm l 
X~2 
V2m 2 } 
r -2>--'-2>--
XV, 
V2n1 2 
XV) 
V3m3 
As in the two-component model, the effect of the constraint C I: crf < crf2 
is 
to truncate the unconstrained distribution from below at the point 
a~ 2 /cr~ = I and H I «T~2 /crD is precisely the normalizing constant induced by the 
truncation . 
The distribution in (5.3.16), however, contains a further factor 
H 2'1 (crf 2/cri) which is a monotonic decreasing function of the ratio cri2/cri. Thus, 
the " additional" effect of the second constraint C2: cr~ 2 < crT23 is to pull the 
posterior distribution of ai 2/crT towards the left of the distribution for which C2 
is ignored and to reduce the spread. In other words, for Yf > 1 
Pr {I < cr;; < Yf I Y} ~ Pr {:: < F 
V"v, < Yf ::} I 
Pr {F V2 • V , < ::}. 
(5.3.17) 
The extent of the influence of HI and H 2 ' 1 depends of course upon the two 
mean squares ratios m2/ml and n13/m2' In general, when m2/ml is large, the effect 
of HI will be small and for given m 2/ml' a large n1) / n12 will produce a small effect 
from H 2V I ' On the other hand, when the values of m2/m l and n1) / n12 are moderate, 
the combined effect can be appreciable. 
For illustration, consider the set of data given in Table 5.3.1. 
The obser-
vations were generated from a table of random Normal deviates using the model 
(5.3.1) with (8 = 0, crT = 1.0, a~ = 4.0, crj = 2.25, 1= 10, J = K = 2). 
The 
relevant sample quantities are summarized in Table 5.3.2. 

282 
Rando'll Effect Models 
5.3 
Table 5.3.1 
Data generated from a table of random normal deviates for the three-component model 
so that 
e = 0, 
a~ = 1.0, 
a~ = 4.0, 
a~ = 2.25, 
I = 10, 
J = K = 2 
a72 = 9.0, 
a~23 = 18, ai/(a f + ai + a~) = 13.8%, 
a~/(ai + a~ + a~) = 55.2% 
a~/(a~ + a~ + a~) = 31.0%. 
j=1 
j=2 
1 
k = 1 
k = 2 
k = 1 
k=2 
I 
2.004 
2.713 
0.603 
0.252 
2 
4.342 
4.229 
3.344 
3.057 
3 
0.869 
-2.621 
- 3.896 
-3.696 
4 
3.531 
4.185 
1.722 
0.380 
5 
2.579 
4.271 
-2.101 
0.651 
6 
- 1.404 
-1.003 
-0.775 
- 2.202 
7 
- 1.676 
-0.208 
-9.139 
- 8.653 
8 
1.670 
2.426 
1.834 
1.200 
9 
2.141 
3.527 
0.462 
0.665 
10 
-1.229 
-0.596 
4.471 
1.606 
Table 5.3.2 
Analysis of variance for the generated data 
s.s. 
S3 = 240.28 
S2 = 122.43 
SI = 20.87 
B-i = 1.04 
B-~ + B-~ +a~ = 10.26 
1.04 
-- = 10.1 percent 
10.26 
d.f. 
V3 = 
9 
v2 = 10 
VI = 20 
a~ = 5.62 
5.62 
-- = 54.7 percent 
10.26 
M.S. 
m3 = 26.70 
m2 = 12.29 
m l = 
1.04 
a~ = 3.60 
3.60 
-- = 35.1 percent 
10.26 
The solid curve in Fig. 5.3.1 shows for this example the posterior distribution 
of a~ 2/ai when C I and C2 are both included. The broken curve in the same figure 
is the unconstrained posterior distribution. For this example, m2/ml is large and 

5.3 
A Three-component Hierarchical Design 'V1odel 
283 
so that the effect of C 1 is negligible and the broken curve is essentially also the 
posterior distribution of (J~ 2/(Ji with the constraint C I' The effect of C 2 is, however, 
more appreciable. The moderate size of in3lin2 implies that (Ji 2/(J~ may be slightly 
smaller than would otherwise be expected. 
--- Exact distribution 
----- Unconstrained distribution 
0.06 
0.02 
IS 
Fig.s.3.1 Posterior distribution of (JT2/(Ji: the generated data. 
We can now relate the above result to the sampling theory solution to this 
problem. 
It is well known that the sample quantity (m l /m 2 ) is distributed as 
«(Ji,(J~2)F"'.V2' Thus, the unconstrained posterior distribution 
(m t!m 2 ) p[F V[,"2 = «(Ji 2 /(J~)(m l /m2)] 
is also the confidence distribution of (Ji2 (Ti. 
Inference procedures based upon 
this confidence distribution are not satisfactory. In the first place the distribution 
extends from the origin to infinity so that the lower confidence limit for «(Ti 2/(Ji) 
= I + K«(J~ /(Ji) can be smaller than unity and the corresponding limit for (J~/(Ji 
less than zero. In addition, the sampling procedure fails to take into account the 
information coming from the distribution of the mean square 111 3 , which, in the 
Bayesian approach, is included through the constraint (Til3 > (Jil' 
5.3.4 Relative Contribution of Variance Components 
One feature of the posterior distribution of «(Jf, (J~, (JD, which is often of interest 
to the investigator, is the relative contributions of the variance components to the 

284 
Random Effect Models 
5.3 
total variance of Yijk ' Inferences can be drawn by considering the joint distribution 
of any two of the three ratios 
Since r) = I -
r 2 -
'3 ' we make the transformation in (5.3. I I) from (u~ , u~, uD 
to (u~ , '2, (3) and integrate out u1 to obtain 
p(rl , '2, '31 y) = N[X IX 2(J -
1) + X1¢2 + X2¢ lJ(K -
I)J3 X\v'; 2)-2 Xi
V1
/ 2)-2 
x (I + Xl + X2)-(VI+V 1+VJ )!2, 
(5.3.19) 
where 
This distribution is analytically complicated. However, for a given set of data, 
contours of the density function can be plotted. 
Using the example in Table 
5.3.1, three contours of the posterior distribution of (,) , r2 ' (3) are shown in Fig. 
5.3.2. To allow for the constraint '"I + (2 + r] = 1, the contours were drawn as 
a tri-coordinate diagram. Tbe mode of the distribution is at approximately the 
point P = (rio = 0.075, r20 = 0.425, r30 = 0.500). The 50%, 70% and 90% in the 
figure were drawn such that 
50%: IOgp('lO, ' 20' r 30 1 y) -
logp(rl' (2 ' (31 y) == 0.69 = -h~(2, 0.5), 
70% : logp(rlo, r 20, '301 y) -
logp(rl ' ' 2, r31 y) == 1.20 = 1/(2, 0.3), 
90% : logp(rlo, r20 , r30 1 y) -
logp(r" r2 , (31 y) == 2.30 = ~/(2 , 0.1), 
and that the density of every point included exceeds that of every point excluded. 
That is, they are the boundaries of H.P. D. regions with a~pro ximate probability 
contents 50, 70, and 90 percent respectively. 
Figure 5.3.2 provides us with a very illuminating picture of the inferential 
situation for this example. It is seen that on the one hand rather precise inferences 
are possible about 'I (the percentage contribution of uf). 
In particular, nearly 
90% of the probability mass is contained in the interval 0.05 < r, < 0.25. 
On 
the other hand, however, the experiment has provided remarkably little information 

) 
5.3 
A Three-component Hierarchical Design Model 
285 
/ 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
-Batches 
Fig. 5.3.2 Contours of the posterior distribution of (rl' r2, (3) : the generated data. 
on the relative importance of 6~ and 6 ~ in accounting for the remaining variation 
Thus values of the ratio 1"2/r3 = 6~ / 6~ ranging from t to 9 are included in the 
90 percent H.P. D. region. 
The diagram clearly shows that while we are on 
reasonably firm ground in making statements about the relative contributions 
of 6i on the one hand and (6~ + 6D on the other to the total variance, we can 
say very little about the relative contributions of (J~ and (Jj to their total contri-
bution «J~ + (3). There would be need for additional data before any useful 
conclusions could be drawn on the relative contribution of these two components. 
The result is surprising and disturbing because the data comes from what we 
would have considered a reasonably well designed experiment. It underlines the 
danger we run into if we follow the common practice of considering only point 

286 
Random Effect Models 
5.3 
estimates of variance components. 'Within the sampling theory framework such 
estimates of the relative contributions of (O"~, O"~, 0"5) may be obtained by taking 
the ratios of the unbiased estimators (o-i, o-L 0-;), 
The resulting ratios correspond to the point Pion the diagram. For large samples, 
the point PI would tend to the maximum likelihood estimate and the asymptotic 
confidence regions obtained from \iormal theory would tend to the H.P.D . regions 
in the Bayesian framework. The small sample properties of the estimates (f I, 
1'2 , 1'3) are, however, far from clear. 
5.3.5 Posterior Distribution of O"~ 
We now begin to study the marginal posterior distributions of the variance 
components (O"~, O"~ , O"~) from which inferences about the individual components 
can be drawn. 
We shall develop an approximation method by which all the 
distributions involved here can be reduced to the X- 2 forms discussed earlier 
for the two-component model. 
This method not only gives an approximati 
solution to the present three-component hierarchical model, but also can be applied,. 
to the general q-component hierarchical models where q is any positive integer. 
For the posterior distribution of O"f, we have from (5.3 .10) or (5.3.11) 
O"i > 0, 
(5.3.21) 
where 
The distribution of O"i is thus proportional to the product of two factors, the 
first being a x- 2 density with VI degrees of freedom and the other a double integral 
of'; variables. The first factor (vlml)-l p(X:, 2 = ai/vlm l) is the unconstrained 
distribution of af and the integral on the right of (5.5.21) is induced by the 
constraint C. Because this integral is a monotonic decreasing function of crL the 
effect of the constraint is, as is to be expected, to pull the distribution towards 
the left of the unconstrained x- 2 distribution. Exact evaluation of the distribution 
is tedious even on an electronic computer. But to approximate the distribution, 
one may employ the scaled X2 approach by equating the first two moments of 

5.3 
A Three-component Hierarchical Design Model 
287 
Vjl11l,'O"~ to that of an ad variable. Using the identity (A5.2.2) in Appendix A5.2 
we find that the rth moment of (vlml / O"~) is 
{ 
X~ , 
vjm J 
X:', 
V21'll2} 
Pr -
> -- -->--
X~2 
v2m2 ' X:3 
v)m) 
(5.3.22) 
where (X;', + 2n X~2' X;') in the numeralor are independent X2 variables with 
appropriate degrees of freedom. From (5.3.12), evaluation of (a, b) would thus 
involve calculating bivariate Dirichlet integrals. Although this approach simplifies 
the problem somewhat, existing methods for approximating such integrals, see 
for example, Tiao and Guttman (1965), still seem to be too complicated for 
routine practical use. 
A Two-stage Scaled X- 2 Approximation 
We now describe a two-stage scaled x- 2 approximation method. First, consider 
the joint distribution of (O"~, 0"~2)' Integrating out 0";23 in (5.3.10), we get 
p(O";, O"f 2 I y) 
(5.3.23) 
where 
Pr {F"J.Vl < :~} 
and ~ is defined in (5.3.21). The quantity G(O"i2) is in precisely the same form as 
the posterior distribution of O"f in (5.2.26) for the two-component model. 
Re-
gard i ng the function G( O"f z) as if it were the distri bu tion of 0"; 2, we can then employ 
the scaled x- 2 method developed in Section 5.2.6 to approximate it by the distri-
bution of v~m;X;i 
2 where 
, 
v2 Ix,(-}V3, 1V2 + 1) 
V 2 =-
a J 
Jx,(-}V3, -}V2) 
(5.3.24) 

288-
Random Effect Models 
5.3 
and 
V3m 3 
XI =-----
V2m2 + V3m 3 
To this degree of approximation, the distribution in (5.3.23) can be written 
p(O'i, O'i2i Y) == ~'(Vlml)-l p(X:,2 = ~)(v;m;)-l p(X:12 = 
~i2,), 
Vjml 
v2m 2 
O'i2 > O'i > O. 
(5.3.25) 
The effect of integration is thus essentially to change the mean square m2 to m; 
and the degrees of freedom V2 to v~ . Noting that 0'72 = ai + Ka~, it follows that 
CK(v1mJ- 1 p X:/ = _1- (v;m;)-l P X;',2 = ----''-------' 
( 
a2 ) 
( 
at +, K, a~) 
vim! 
V2m2 
p(at,a~IY) == ------ --------{-- ----v;-m-;-}--------------
Pr F y ' 
VI < ----
,. 
vlm l 
af > 0, ai > 0, 
,(5.3.26) 
where 
If we ignore tbe constant ~", the distribution in (5.3.26) is of exactly the same 
form as the posterior distribution in (5.2.14). We can therefore apply the results 
for the two-component model to deduce the marginal posterior distributions 
of af and d for our three-component model. In particular, for the error variance 
cd, we may employ a second scaled X- 2 approximation so that O'i is approximately 
distributed as V'lm'IX:;2 where 
and 
VI Ix,(-tv;, -}VI + 1) 
a2 
Ix/iv;, tv l ) 
, 
(5.3.27) 
Thus, making use of an incomplete beta function table, the quantities (ai' v;, m;; 
a2, V'I, m;) can be conveniently calculated from which the posterior distribution 
of a~ is approximately determined. 
For the generated example, we find (a l = 
0.865, v; = 12.35, m; = 1l.51; Q2 = 1.0, V'I = 20.0, m'l = 1.04) so that a posteriori 
the variance at is approximately distributed as (20.87)X - 2 with 20 degrees of 
freedom. Since for this example (VI = 20, m l = 1.04), the effect of the constraint 
c: a~23 > 0'~2 > a7 is thus negligible and inference about a~ can be based upon 

5.3 
A Three-component Hierarchical Design Model 
289 
the unconstrained distribution (vlml)-l p(x:-/ = af!vlm l ). This is to be expected 
because for this example the mean square 1n2 is much larger than the mean square 
Inl' 
Pooling of Variance Estimates lor the Three-component Model 
The two-stage scaled X- 2 approximation leading to (5.3.27) can alternatively be 
put in the form 
where 
a 2 
I 
(5.3.28) 
2) 
, . 
V2 
This expression can 'be thought of as the Bayesian solution to the "pooling" of 
the three mean squares (ln l , m2 , m 3) in estimating the component (Ji. 
For 
illustration, suppose we have an example for which I = 10, J = K = 2 and the 
three squares (m l , m 2 , m 3 ) are equal. We find Al = 0.48, )'2 = 0.157, WI = 0.754 
and W2 = 0.48. Thus, 
so that 
ml 
2 
-2 (0.82) N '1.3206/32.06, 
(JI 
m l 
VI + 0.48 V2 + 0.157 V3 _ ~ 8 
-(J2 
0 
0 8 
-
2 O. 2 
I 
V I + .754 v 2 + .4 v 3 
(J I 
that is, 
ai 
-2 
-
N (0.82)(32.06)X32.06' 
Inl 
In particular, the mean and variance of (Ji/m l are 
and 
By contrast, the evidence from In I alone would have given 
that is, 
so that 
and 
Thus, the additional evidence about (Ji coming from 1n2 and m3 indicates that ai 
is considerably smaller than would have been expected using In I alone. In addition, 
the variance of the distribution of ai/m l is seen to be only about half of what it 
would have been had only the evidence from Inl been used. 

290 
Random Effect Models 
5.3 
5.3.6 Posterior Distribution of cd 
Using the approximation leading to (S.2.44), it follows from the joint distribution 
of (O'L O'D in (S.3.26) that the posterior distribution of O'~ is approximately 
( 21 ) . K(v~m~)-l P[X;,2 = (m! + KO'~) / (v~m~)] 
p 0'2 Y = 
P { 2 
I 
I 
} 
r Xv, < v 1m 2i m 1 
O'~ > O. 
(S.3.29) 
To this degree of approximation, then, the quantity (mJ + KO'~),I'~m~ has the 
X- 2 distribution with v~ degrees of freedom truncated from below at ml / (v~m~). 
For the example in Table S.3.1, the quantity 0.007 + 0.014 O'~ thus behaves like an 
X- 2 variable with 12.3S degrees of freedom truncated from below at 0.007. 
Posterior intervals about 
O'~ can then be calculated from a table of / 
___ 
probabilities. 
5.3.7 Posterior Distribution of O'~ 
From the joint posterior distribution of (O'L O'~, O'j) in (S.3.11), we may integrate 
out (0';, O'~) to get the posterior distribution of O'~, 
O'~ > O. 
(S .3.30) 
The density function is a double integral which does not seem expressible in terms 
of simple forms. To obtain an approximation, we shall first consider the joint 
posterior distribution of (£Ti 2, O'i 23) and then deduce the result by making use of 
the fact that £Til3 = £Ti2 + JKO'~. From (S.3.10), we obtain 
P(O'i2' O'i23 1 Y) 
= ((v 3m 3r J p(x:,z = 0'1~3 )(V2m 1}-1 p(x:/ = 
O'i2) Pr (Xv: > v1m21 ) 
V3m 3 
V2m 2 
0' 12 
(S.3.31) 
where 
(''' = (Pr {FV2 • V1 < m 2/m 1 }, 
and (is given in (S.3.21). The function G 1(O'i2) is in exactly the same form as the 
distribution of £Til in (S.2.S7) for the two-component model. Provided m2/ml is 

5.3 
A Three-component Hierarchical Design Model 
291 
not too small, we can make use of the approximation method in (S.2.60) to get 
where 
and 
"2 i x ,(-1:V2 + 1,11'1) 
a J 
i x ,(-}V2,1vI) 
V2m 2 
XJ= -----
vlm l + V2 m 2 
It follows that the posterior distribution of a; is approximately 
( 
2 
). JK(vJmJ)-l (v~m~)-IJ'X) (, -2 _ aiz + JK(5) 
paJly = P {F 
/ "} 
p Xv, -
r 
'J,y; < mJ m2 
0 
vJmJ 
(S.3.32) 
(S.3.33) 
(S.3.34) 
The reader will note that the distribution in (S.3.34) is in precisely the same form 
as the posterior distribution of a~ in (S.2.39) for the two-component model. 
Provided v~' is moderately large, we can thus employ (S.2.44) to express the distri-
bution of a~ approximately in the form of a truncated x- 2 distribution 
j
(' 21 ) -"- JK(vJmJ)-l p[X;,2 = (m~ + JKa~)!(vJm3)J 
p a 3 y -
P {2 
"/()} 
, 
r X"J > m2, V3 m J 
a5 > O. 
(S.3.3S) 
To this degree of approximation, the quantity (m~ + J K(5)/(v3m3) is distributed 
as an X- 2 variable with VJ degrees of freedom truncated from below at m~/(v3m3)' 
from which Bayesian intervals for a~ can be determined. For the set of data in 
Table S.3. J, we find (a3 = J.O, v~ = 10.0, m~ = 12.29) so that the quantity 0.051 
+ 0.017 a~ is approximately distributed as X- 2 with 9 degrees of freedom trun-
cated from below at the point O.OSI. 
5.3.8 A Summary of the Approximations to the Posterior Distributions of (ai, ai, a~) 
For convenience in calculation, Table S,3.3 provides a short summary of the 
quantities needed for approximating the individual posterior distributions of the 

292 
Random Effecl Models 
S.4 
variance components CaL aL aD. The numerical values shown are those for the 
set of data introduced in Table 5.3.1. 
Table 5.3.3 
Summarized Calculations for Approximating the Posterior Distributions of (a~ , a ~ ,a~) 
(illustrated using the data in Table 5.3.1) 
1. Use Table 5. I.l, or expression (5.3.6b), to compute 
m) = 1.04, 
m 2 = 12.29, 
m3 = 26.70, 
VI = 20, 
K = 2, 
V2 = 10, 
JK = 4, 
2. Use (5.3.24) to determine 
XI = 
0.662, 
v; = 12. 35, 
3. Use (5.3.27) to determine 
x 2 = 0.872, 
V'1 = 20.0 
4. Use (5.3.33) to determine 
X3 = 0.854, 
v~ = 10.0, 
5. Then, for inference about af 
G I = 0.865, 
m; = 11.51. 
G2 = 1.0, 
m') = 1.04. 
G3= 1.0, 
m~ = 12.29. 
2 
I 
I 
-
2 
a 1 N vjm1 x v ', 
af N (20.8)X202 • 
6. For inference about a ~ 
111)+Ka ~. 
- 2 
I 
I 
1"'0..1 X \l~ 
v2m2 
0.007 +O.014a~ N 
X ~22 35 
7. For inference about a~ 
truncated from below at m! /(v ; m; ), 
truncated from below at 0,007. 
m;'+JKa ~ 
- 2 
N XV) 
truncated from below at m;'/(v3n73), 
V3 m 3 
0.051 +O . 017a~ N X9 2 
truncated from below at 0.05!. 
5.4 GENERALIZATION TO q-COMPONENT HIERARCHICAL DESIGN MODEL 
The preceding analysis of the three-component model and in particular the 
approximation methods for the posterior distributions of the individual variance 

5.4 
Generalization to q-Component Hierarchical Design Model 
293 
components can be readily extended to the general q-component hierarchical 
model 
yi •.. . i, = e + ei• + e iqiq _, + ... + e i q .. . it + ... + e iq .. . i " 
iq = I, ... , I q; 
... , 
i1 = 1, ... , I I, 
(5.4.1) 
where Yiq ... i, are the observations, e ;'s a common location parameter, and eiq' ... , 
eiq ... i , are q different kinds of random effects. 
Assuming that these effects are 
Normally and independently distributed with zero means and 
variances 
(0';, u;_ 
1 , ... , u
t
2 , ... , ui) and fol/owing the argument in Sections 5.2 and 5.3, it is 
readiJy shown that the joint posterior distribution of the variance components is 
where 
( 
2 
2 I ) - (Iq - 1 jq - 2 
1 
) (2 
2 
2 
2 
I) 
PO'I ' ... , 0' q Y -
) 
2 
... 
q - I P 0' 1> U 12, ... , 0' J ... 1> .. . , 0' I ... q y, 
O'T > 0, ... , u; > 0, 
(5.4.2) 
O'T2 = (ui + 110'~)' ... ,uL.t = (O'L.(t-l) + II .. ·1'_IO'r
2), ... , 
O'i..,q = (O'L.(q-l) + II .. ·1q_) 0';), 
p(O'L ... , uLI> ... , O'i..,q I y) 
[p {
X~' 
vIm) 
X;t-I 
vr-Imr - 1 
X2 
vq-1mq - 1 }J-
1 
x 
r 2' > --, ... , --2 -
> 
, ... , v;-, > -'---"--'--'-
Xv, 
V2 m 2 
XVt 
vrm, 
Xv. 
Vqmq 
0< O'T < O'i2 < ... < uL.r < ... < O'T... q , 
(5.4.3) 
and the m/s and the v/s are the corresponding mean squares and the degrees of 
freedom . The distributions in (5.4.3) and (5.4.2) parallel exactly those in (5.3.10) 
_ 
. ~nd (5.3.11). 
, In principle, the marginal posterior distribution of a particular variance com-
ponent, say, O'r2, can be obtained from (5.4.2) simply by integrating out O'i, ... , O'r2_1' 
0'/+ 1, ... ,0';. In practice, however, this involves calculation of a (q -
I) dimensional 
integral for each value of u,2 and would be difficult even on a fast computer. 
A 
simple approximation to the distribution of O'r2 can be obtained by first considering 
the joint distribution p(UL.(/-l)' O'L., I y). 
The latter distribution is obtained 
by integrating (5.4.3) over (O'7...(r+ 1)' ... , O'L.q) and (0';, ... , O'L.(,-2»' It is clear 
that the set (O'i...(r + 1)' ... , O'i .. ) can be eliminated in reverse order by repeated 
applications of the ax~ approximation in (5.2.35). The effect of each integration 
is merely to change the values of the mean s4uare m and the degrees of freedom v 
of the succeeding variable. Similarly, the set (O'i, ... , d ... 
(1 _ 2» can be approximately 

· 294 
Random Effect Models 
.\5.1 
integrated out upon repeated applications of the ex; method in (S.2.60). Thus, 
the distribution of (0"7...(1-1), 0"r..1) takes the approximate form 
( V' 
m' )p(v-: 2 = CJT",(I-ll)(V'I11')- l ( -:2 = 0"i..1) 
,- 1 
I - I 
I V, _ I 
' 
, 
I 
I 
P Xv, 
" 
p(CJ7.. .(I-ll' O"~ .. 11 y) == 
{V
I
- 1
11'1
1
- 1 
'} 
vlm
l 
Pr F · · <~ 
Y t • Vt -
I 
I 
/11.1_1 
CJi..., > ai .. (1-1) > O. 
(S.4.4) 
Noting that CJ~ ... I = aT ... (I-I) + II .. ·1 1 _ 1a,2, the corresponding posterior distri-
bution of (CJi...(I-I)' a,l) is then of exactly the same form as the posterior distri-
bution of (O'~, CJ~) in (S.2.14) for the two-component model. 
The marginal 
distribution of 0'12 can then be approximated using the truncated X- 2 form (S .2.44) 
or the associated asym ptotic formulas . Finally, in the case I = 2; so that VI = 1"), 
1111 = m'l and the posterior distribution of the variance O'T can be approximated 
by that of an inverted X: distribution as in the two-component model. 
APPENDIX AS.1 
The Posterior Density of CJ~ for the Two-Component Model Expressed as a X - 2 Series 
We now express the distribution of a; in (S .2.26) as a X- 2 series. 
Using the 
Mittag-Leffler formula [see Milne- Thomson (1960), p. 331J , we can write 
2 
2 
[( 1'21112)] 
~ (V2m2/2IJTt·,,2) .. r 
Pr { XV2 < V2Jn 2/IJd = exp 
-
-2 
2 
2: 
r(~ + + I) . 
CJ I 
r = O 
' Vz 
r 
(AS.!.I) 
Thus, the distribution of CJi/(vl ml + V2m2) can be alternatively expressed 
as a weighted series of the densities of X- 2 variables, 
(AS.l.2) 
with the weights Wr given by 
W = 
r 
[ V2m2 /(VII11I)J(v 2/ 2) +r [J + V2111 2! (VII11I)r(I, +>-2)+ r 
r[{(Vl + v2) + rJ 
Pr {F V2• V1 < 111 2/ 1111} 
rctv2 + ,. + i)r(}v l )' 
We note that when 1'2 is even, the right ·hand side of (A5.!.l) can, of course, 
be expressed as a finite Poisson series so that 
( 21) 
(vlml)'l p[X:,2 = IJi/(vlm l)] 
P IJ1 Y = 
Pr {F V2• V, < Jn2!l11l} 
{ 
'r'~=
2 -o I [V 2 n1 2/(,.2!ai)J'} . 
x 
1 -
exp [ -
V1in2/(2IJi)] 
L.. 
(A5.1.3) 

A5.2 
Appendix 
295 
In this case, posterior probabil ities of ui can be expressed as a fInite series of X2 
probabilities, that is, 
2 
d 
} 
Pr { V 1nJ 1ld2 < X;, < V l l11 l id1 } 
P r {d 1 < u 1 < 
2 I y = _--'--"---'-c---=--_~_--',----:'-'--"-'. 
Pr {Fy2 ,v I < 111)/ I11 I } 
(AS. 1.4) 
where 
r(1v1 + r)[V 2ln 2!(v t m t )J,[1 + V2 iJ12!(I'II11 I)r(vl / 2)+r 
cPr = 
r! r(vl /2) Pr {F V2 • VI < 111 2!ml } 
, 
so that probability integrals of CJi can be evaluated exactly from an ordinary X2 
table. 
APPENDIX AS.2 
Some Useful Integral Identities 
The derivation of the posterior distribution in (5 .2.12) leads to the following 
useful identity. For PI> 0, P2 > 0, a1 > 0, a2 > ° and c > 0 
L .c fo""x-(PI+1) (x + cy)- CP2-'- ll exp [- (~ + x :2CY)] dxdy 
where lJp, q) is the usual incomplete beta function. 
Similarly, the results in (5.3.10) for the three-component model implies the 
identity: for Pi> 0, Gi > 0, i = 1, 2, 3 and C I > 0, C2 > O. 
fo
oo fo'" fo'" Xl - ( PI + 1) (XI + CI X2)- ( P2+ 1l (XI + CIX2 + C2X3)-(P3+ I) 
= ( . )-1 (113 f( .) :-PI)p {xipi > ~ X ~P2 
G2 } 
C 1C2 
Pia l 
r 
2 
' 
2 
> 
, 
i=l 
X2P2 
a2 
X2P3 
a3 
where X~PI' X ~ P 2 and X~P3 are independent / 
variables with 2p I ' 
degrees of freedom, respectively. 
(A5.2.2) 

296 
Random Effect Models 
AS.3 
We record here another integral identity the proof of which can be found in 
Tiao and Guttman (1965). For a > 0, p > 0 and n a positive integer 
f
a 
"-I(n-I) . 
. 
(l + X)-(P+II) X,, - l dx = B(I7,p) - I 
. 
aJ(I +a)-(P +JI B(p+), n-), 
o 
j=O 
} 
(A5.2.3) 
where B(p, q) is the usual complete beta function. 
, 
APPENDIX AS.3 
Some Properties of the Posterior Distribution of.lJ~ for the Two-component Model 
We now discuss some properties of the posterior distribution of lJ~ in (5.2.39). 
Moments 
By repeated application of the identity (A5.2.1) in Appendix A5.2, it can be verified 
from (5.2.12) that for VI > V2 > 2r, the rth moment of lJi is 
p; = (V2m2 )r t (~) (_ V21112)i Ix[(v2/2) -
17 + i, (vJm - iJ 
2K 
i=O 
I 
vJm l 
Ix(v 2/2, vl /2) 
V2m 2 
x=-----
Vl 11l 1 + V2 ln 2 
(AS.3.1) 
Making use of an incomplete beta function table, the first few moments of lJ~ , 
when they exist, can be conveniently calculated and will, of course, provide the 
investigator with valuable information about the shape of the distribution . 
Mode 
To obtain the mode of the distribution of O"L it is convenient to make the trans-
formation 
2KlJ~ 
u=--. 
(AS.3.2) 
V2 ln 2 
so that from (5 .2.39) 
p(u I y) = M 1'" [(¢Z)-I + ur CV2 + J ) 
x exp {- [(¢Z)-l + uri }zh-lexp(-Z)dZ 
(AS.3.3) 
where M is a positive constant, ¢ = V2 11l 2 /(V I I1l I ) and the substitution z = 
v)l1ltl(2lJD is made. 
-~-
. 
-

AS.3 
Appendix 
297 
Upon differentiating (A5.3.3) with respect to u, we find 
ap(~~ Y) = M{ [1- (~ + I)U If' 
g(u, z)dz -
(~2 + 1 )¢-I L''' g(U,Z)Z-1 dZ}, 
(A5.3.4) 
where 
g(u, z) = [(¢Z) -I + ur( '~ Y 2 +3) z-l v1 - 1 exp ( -
{z + [(¢Z)-I + uri }). 
Applying integration by parts 
S x dv = xv - J v dx 
(A5.3.5) 
and setting 
dv = zl(v, +\'2) + 1, X = [¢- I + lIZr
(i Y2+3) exp( - {z + [(¢Z)-I+Ur1 }) , (A5.3.6) 
the second integral on the right-hand side of (A5.3.4) can be written 
roo g(U,Z)Z -1 dz = ( 
2 
) 1
00 
g(u, z)[1 + N(u, z)] dz, 
(A5.3.7) 
Jo 
VI + V2 + 4 
0 
where 
N (u, z) = (~ + 3 ) (¢ - I + uz) - I U + r I (¢ - 1 + liZ) - 2 
Substituting the right-hand side of (A5.3.7) into (A5.3.4), we have 
op(uIY) 
[ 
(V2
)( 
2¢-1 )]1"" 
M- I 
= 1- -+1 
u -
g(u,z)dz 
au 
2 
V2 + V I + 4 
0 
_ (~ ,+ 1)( 
2¢ -1 
)Joo g(u,z)N(u,z)dz. 
2 
VI + V2 + 4 
0 
Since N(u, z) ?:: 0, it follows that 
where 
Also since, 
therefore, 
Op(u I Y) < 0 
au 
if 
u > u* , 
N(u, z) ?:: (;2 + 3) ¢u + ¢, 
Op(l( I y) > 0 
au 
if 
(A5.3.8) 
(A5.3.9) 
(A5.3.10) 
(A5.3.l!) 

298 
Random Effect Models 
AS.4 
where 
1'2+ 6 
) - Il(~ + J)'-I _ 2(¢'1 + J)] 
VI + 
1'2 + 
4 
2 
VI + 
V2 + 4 
and 
Hence, 
a) if u* > 0, then p(u I y) is monotonically Increasing in the interval ° < u <'u* 
and monotonically decreasing in the interva l II > u'" so that the mode must· 
lie in the interval 11* < 11 < u*; and 
b) if 1I* < 0, then p(u i y) is monotonically decreasing in the entire range u > 0 
and the mode is at the origin. 
In terms of (J ~ and the mean squares 1171 and 111 2 , we may conclude that : 
a) If 
( _VI )(~), 
\1 1 + 2 
\'2 
then p«(J~ I y) is monotonically increasing in the interval ° < (J ~ < ( I and 
monotonically decreasing in the interval (J ~ > C2 so that the mode lies in the 
interval CI < (J ~ < C2 , where 
-1 ( 
VI + V 2 + 4 ) l 1' 2 
K 
-
--111 2 
1'1 + 21'2 + 10 
1'2 -ir 2 
and 
(A5.3.12) 
b) If 
nl 2 
(V2
+ 2)( 
VI 
) 
m 1 < -\-'2-
VI + 1' 2 + 4 
' 
then p«(J~ I y) is monotonically decreasing in the entire range (Ji > ° 
and the 
mode is at the origin. 
APPENDIX AS.4 
An Asymptotic Expansion of the Distribution of (J' ~ for the Two-component \'lodel 
We here derive an asymptotic expansion of the distribution of (J' ~ in (5.2.39) in 
powers of t - \ = (v 1/2)-I .t 
To simplify writing, we shall again work with the 
t In Tiao and Tan (1965), the distribution of 
(J' ~ was expanded in powers of 
[(1'2 /2) _1)-1 with slightly different results. 

distribution of u= 2K(J~ /(v2ml) rather than (J~ itself. Upon making the substitution 
Z = 
1'11111 '(2a~), the distribution of u is 
where 
and 
VI 
t= -
2 
(AS.4.I) 
(AS.4.2) 
For fixed u, the function h(u, z) in the integral (AS.4.I) is clearly analytic in 
0 < z < 00. Usi ng Taylor's theorem, we can expand h(u, z) around t. By reversing 
the order of integration, we obtain 
where 
h(O)(u,t) = [r(v2f2)r l (r l + ur ;(v,+2)exp[- (r l + U) -l ], 
h(l )(u, t) = -t-lh(O)(u, t)RI(}.,u), 
h(2 )(U, t) = t - 2 h(O)(u, /)[R 2 ()., u) + 2R I (A, u)], 
h(3 )(u,t) = -t- 3 h(O)(U,I)[R 3(A, u) + 6R 2()" u) + 6R I(A,U)], 
(A5.4.3) 
h(4)(U, t) = 1- 4 h(°>Cu, t)[R4(A, u) + 12R 3 (}" u) + 36R 2 (}" u) + 24R I (J" u)], 

300 
Random Effect Models 
AS.4 
+ (~2 + 1) (~ + 2) (~2 + 3 ) (I; + 4) W4] , 
and 
It is to be noted that for fixed i = cpl, IP )(u, I) is of order' -1, h(2)(u, I) is of order 
1- 2 and in general h(I")(u, r) is of order ,-T. 
From the asymptotic relationship 
between the gamma distribution and the normal distribution one can easily verify 
that the integral 
-
(z-t)'z,-Ie-zdz 
1 foo 
f(/) 0 
(AS.4.4) 
is a polynomial in I of degree [}(I' -
I)J, where [qJ is the smallest nonnegative 
integer greater than or equal to q. Thus, the expression in (AS.4.3) can be written 
as a power series of 1 ' 1. Also, to obtain the coefficient of ,'1 ) we need only to 
evaluate the first three terms of the series in (AS.4.3) and to obtain that of 1- 2 , 
the first five terms of the series and so on. 
Now the quantity Pr {F V2 ,vl < III 2 il11 I } in (AS.4.1) can be written 
= eel) J: Z'~ '2-1 exp (-Z)[ (I -, -7-r" ex p{ ~: - ;/32 + .. } 1 
dz, 
(A 5.4.5) 
where 
Applying Stirling's series (see Appendix A2.2) to eel) and expanding the factor 
in the square bracket of the integrand in powers of I .1, we find that for Axed )" 
(AS.4.6) 

A_ 
-~ -
. -
A5.4 
Appendix 
301 
where 
+ C;l -+ 
)l3 - ic~] , 
Gi~V1) is the cumulative distribution of a gamma variable with parameter 
v2/2 evaluated at J., and gJ}v 1) is the corresponding density, From (AS.4.3) and 
(AS.4.6), we find that Cor fixed i. the distribution of u can be written 
U-' J + u)- O-"2 t l) 
p(u l y) == 
exp[-U- 1 + !I) - I] 
G ).OV 1 ) l(-}vz) 
(AS.4.7) 
with 
AIU) 
B1(u, },)=}Rz()"u)+R1U, u)--
1 
' 
G) (-2-V 2) 
B1(U, l) = -~-R4(Jc,U) + -~R3(J"U) + ~ R2(J"U) + R1(X, u) 
where R 1 (l, u), ''' ' R4 (A , u)-are given in (AS.4.3), and Al V,) and A 2U) are given in 
(A5.4,6). The distribution of II is thus expressed as a series in powers of I-I The 
leading term is in the form of a truncated "inverted" gamma distribution and is 
equivalent to the distribution in (S.2.44). The coefficients Bl (u, ),), B2 (u, X), etc, 
are polynomials in (). 
1 + U)-l, 
It is straightforward to verify that when one 
integrates the distribution (AS.4,7) over II all terms except the leading one vanish 
so that (A5.4.7) defines a proper density function. By making the transformation 
/ = 21V, - J + u) one can, of course, alternatively express the distribution in 
(AS.4,7)'as a series of / densities, 
[f one is interested in individual posterior probabilities of u, a corresponding 
asymptotic expansion of the probability integrals in powers of t -1 can be readily 
obtained. From (AS.4,7), it can be verified that for fixed )" 
P { 
}
' 
I 
1 
-3 
r u> Uo I y 
= PO~;IO) + -PJ(yo) + -ZP 2(yo) + OCt 
), 
t 
t 
(AS.4.8) 

302 
where 
and 
Random Effect Models 
{( "/o) 
S (YO)2[ 2 
(V2 
)] 
X 
J: Yo + 2 --; 
)'0 -
Yo 2 + I '" 
9 "oC-tv 2) 
G ;(-}v 2 ) 
A5,4 
7(1'0)3[3 
2(V2 
) 
+ 6 l 
/0 - 21'0 2 + 2 
( V2 
)(V2 
)] 
I (YO)4[ 4 
3("2 
) 
+ Yo 2 + 1 2 + 2 
+ 8 J: 
Yo - 31'0 2 + 3 
Posterior probabilities of u and therefore ()~ can thus be conveniently calcu-
lated using (AS.4.8) in conjunction with a standard incomplete gamma function 
table or a X2 table. 
While the asymptotic expansions of p(u I y) and the related probability integrals 
as obtained above are justified for large t, tlleir usefulness depends, of course, 
upon how close they approximate the exact distribution and probabilities for 
moderate values of t . We have already seen in Figs. S.2.6 and S.2.7 that the pos-
terior distribution of ()~ obtained just by employing the leading term of (AS.4. 7) 
is in close agreement with the exact distribution for both the dyestuff and the gen-
erated example. Tables S.2.3(a) and S.2.3(b) illustrate how further improvement 
can be made by employing additional terms in the expansion (AS.4.7). The first 
columns of the two tables give, respectively, a specimen of the densities of ()~ for 
the two examples calculated from (AS.4.7) withjust the leading term. In the second 
and third columns of the same tables are shown densities of ()~ wbich now include 
terms to order t -I and t - 2, respectively, and in the fourth column the 
corresponding exact densities obtained from (S.2.39) by numerical integration are 
listed. The values of I for these two examples are not at all large. Further, the 
dyestuff example has a moderate value of i1l2/1I1 1, while the generated example has 
a rather smal! 1112/m I' Thus, the results for the two examples demonstrate that 
even with moderate values of f , the expansion formula in (AS.4.7) is useful both 
for small and for moderate values of m2!111 1. 

A5.5 
Appendix 
303 
The main practical advantage of the expressions (A5.4.7) and (A5.4.8) is, 
of course, their computational simplicity. The leading terms of order t -1 in (A5.4.7) 
and (A5.4.8) are simple functions of X2 densities and probabilities which can be 
easily evaluated on a desk calculator in conjunction with a standard X2 table or 
an incomplete gamma function table. If terms of order t - 2 or higher are included, 
the use of an electronic computer is then desirable. >.levertheless, the expansion 
gives an efficient computational method compared with the direct numerical 
integration of the exact expressions in (5.2.39) and its related probability integrals. 
APPENDIX AS.S 
A Criticism of the Prior Distribution of (ai, ai) 
We now discuss an interesting theoretical point raised by Stone and Springer 
(1965) concerning the prior distribution of (ai, aT 2) in (5.2.8) 
(A5.5.1) 
They argue that this distribution can be regarded as the limiting distribution of 
the following sequence of distributions 
(A5.5.2) 
as e --+ O. 
Making the transformation from (aT, aT 2) to 
and 
(A5.5.3) 
they claim that the marginal prior distribution of p is 
o < p < 1, 
E; > O. 
(A5.5.4) 
Letting 3 > 0, the prior probability that 0 < p < 3 is 
Pr (0 < p < 3) = 3' 
(A5.5.5) 
which approaches 1 as e --+ O. This means that the probability limit 
p lim p = O. 
(A5.5.6) 
Now, consider the distribution of the mean squares ratio Jn J /Jn 2 • 
Since con-
ditional on p 
it follows from (A5.5.6) that, a priori 
(A5.5.8) 

304 
Random Effect Models 
AS.6 
Thus, the prior distribution in (A5.5.1) implies that a priori, one would expect 
m J 'm 2 to be very close to zero, and would therefore be surprised if a somewhat 
large value of ml / nJ2 were observed. 
Consider now the posterior distri bution of ai in (5.2.26). 
If- nJ, 1m2 were 
small, (that is, m 2/m l were very large), then 
lim I(a7) = 1. 
ntl/m 2 ·· 0 
The implication is, then, since a prior one would expect i11[ . 1112 to be small in any 
event, it should be therefore unnecessary to consider this factor at all in the pos-
terior inference. And if m, il71 2 turned out to be somewhat large, it would be in 
conflict with the implied prior belief. 
While the above argument sounds interesting, given the way Stone and 
Springer approach the problem , it has hard ly any practical ' s rgni.fi ~anse so far 
as the posterior inference is concerned. Consider the two situations e -> 0 and 
e = 0.5, and let !3 = 0.2. Then, a priori, 
lim Pr (0 < p < 0.2) = I 
and 
Pr (0 < p < 0.21 e = 0.5) = y'0.2 = 0.447. 
,-0 
Thus, a large difference exists between the prior for which e = 0.5 and the prior 
in (A5.5.1), and in the case e = 0.5, Stone and Springer's objection would not 
arISe. 
However, the use of e = 0.5 will only serve to change, in thc posterior 
distribution of (ai, aD in (5.2.14), VJ to vi = VI -
0.5 and i11[ to mi = mJ(v[ 'vi). 
The reader can verify that for either the dyestuff or the generated example or any 
other example in which VI is of moderate size, the posterior inferences about 
(ai, aD would be scarcely affected. 
APPE~DIX A5.6 
"Bayes" Estimators 
In this book we treat sampling theory inference and Bayesian inference as quite 
distinct concepts. 
In sampling theory, inferences about parameters, supposed 
fixed, are made by considering the sampling properties of specifically selected 
functions of the observations called esLimators. In Bayesian inference, the para-
meters are considered as random variables and inferences are made by considering 
their distributions condi tional on the fixed data. In recent years, some workers 
have used a dual approach which , in a sense, goes part of the way along the 
Bayes route and then switches to sampling theory. 
SpecificaiJy, the Bayesian 
posterior distribution is obtained, but not used to make inferences directly. 
Instead, it is employed to suggest appropriate functions of the data for use as 
estimators. Inferences are then made by considering the sampling properties of 
these "Bayes" estimators. Because this dual approach is now quite widespread-
see, for example, Mood and Graybill (1963)- , we feel that we should say something 
about it here. 

------
A5.6 
Appendix 
305 
The present authors believe that while estimators undoubtedly can serve some 
useful purposes (for example, in the calculation of residuals in the criticism phase 
of statistical analysis), we do not believe their sampling properties have much 
relevance to scientific inference. 
Description of Distributions (Including Posterior Distributions) 
We have argued in this book that tlie whole of the appropriate posterior distri-
bution provides the means of making all relevant inferences about a parameter 
or a set of parameters. We have further argued that, by using a noninformative 
prior, a posterior distribution is obtained appropriate to the situation where the 
information supplied by the data is large compared with that available a priori. 
If for some reason it was demanded that we characterized a continuous posterior 
distribution without using its mathematical form and without using diagrams, 
we could try to do this in terms of a few descriptive measures. The principles we 
would then have to employ and the difficulties that would then confront us would 
be the same as those we would meet in so describing any other continuous distri-
bution or indeed in representing any infinite set by a finite set. 
10 
~-= __ ~ 
______ ~-L 
__ ~~ 
____ ~~ 
__ ~ 
~~ 
10 
35 
20 
40 
(a) 
Posterior 
mean 
= 45.8 
(b) 
60 
Fig. AS.6.1 Two hypothetical posterior distributions 
~ .... 
80 

306 
Random Effect Models 
A5.6 
For example, the description of the posterior distribution of Fig. AS.6.1 (a) is 
simple. It is a Normal distribution with mean 22 and standard deviation 3.7 
Thus, with knowledge of the ~ormal form , two quantities suffice to provide an 
exact description. 
Description of the posterior distribution in Fig. A5.6.1 (b) would be more diffi-
cult. We might describe it thus: "It is a bimodal distribution with most of its 
mass between 10 and 80. The left hand mode at 30 is about half the height of 
the right hand mode at 57. ;\ local minimum occurs at 43 and is of about one tenth 
the height of the right hand mode" . 
Alternatively, we could quote, say, 10 
suitably spaced densities to give the main features of the distribution. However 
we proceeded we would need to quote five or more differentD\ll"!!gers to provide 
even an approximate idea of what we were talking about. 
ow there is nothing 
unrealistic about this hypothetical posterior distribution . 
It would imply that 
values of the parameter around 57 and around 30 were plausible but that inter-
mediate values around 43 were much less so . Practical examples of this sort are 
not uncommon and one such case will be discussed later in Section 8.2. 
Posterior distributions met in practice are usually neither so simple as the Nor-
mal nor so complex as the bimodal form , just considered. 
But what is clear is 
that the number and kinds of measures needed to approximately descrihe the 
posterior distribution must depend on the type and complexity of that distribution. 
Questions about how to describe a posterior distribution cannot be (and do not 
need to be) decided until after that distribution is computed. Furthermore, they 
are decided simply on the basis of what would best convey the right picture to 
the investigator's mind . 
Point Estimators 
Over a very long period, attempts of one sort or another have been made 
to obtain by some prior recipe a single number calculated from the data which in 
some way best represents a parameter under study. Historically there has been 
considerable ambivalence about objectives. Classical writers would for example 
sometimes state the problem as that of finding the most "probable" value and 
sometimes that of finding a best "average" or "mean" value. 
A Measure of Goodness of Estimators 
Modern sampling theorists refer to the single representative quantity calculated 
from a sample y as a point estimator and tbey argue as follows. Suppose we are 
interested in a particular parameter cp. Then any function J>(y) of the data which 
provides some idea of the value of cp may be called an estimator. Usually a very 
large number of possible estimators can be devised. For example, the variance 
of a distribution might be estimated by the sample variance, the sample range, 
the sample mean absolute deviation and so on. Therefore a criterion of goodness 
is needed . using it the various estimators can be compared and the "best" selected. 
It is argued that goodness of an estimator ought to be measured by the average 

A5.6 
Appendix 
307 
closeness of its values over all possible samples to the true value of ¢. 
The 
criterion of average closeness most frequently used nowadays (but of course an 
arbitrary one) is the mean squared error (M.S.E.). Thus among a class of possible 
candidate estimators ¢ J (Y), ... , ¢;(y), .. , ¢k(Y)' the goodness of a particular one, 
say, ¢j(Y) would be measured by the size of 
M.S.E.(¢J = E[¢j(Y) - (pJ2 = f/¢), 
y 
where the expectation is taken over the sampling distribution of y. The estimator 
¢i(Y) would be " best" in the class considered if 
j = l , ... ,k; ji=i 
for all values of ¢, and would be called a minimum mean squared error (M.M.S.E.) 
estimator. 
At first sight, the argument seems plausible, but the M.S.E. criterion is an 
arbitrary one and is easily shown to be unreliable. A simple example is the esti-
mation of the reciprocal of the mean of the Normal distribution N(e, I). From a 
random sample Y, there is surely something to be said for the maximum likelihood 
estimator y- J which is, after all , sufficient for e- J, but it so happens that the 
M.S.E. of r t is infinite. 
The real weakness of the argument lies in that it requires a universal measure 
of goodness. 
Even granted that some measure of goodness may be desirable, 
it ought to depend on the probabilistic structure of the estimator as well as what 
the estimator is used for. 
But inherent in the theory is the supposition that, a 
criterion is to be arbitrarily chosen a priori and then applied universally to all 
estima tors. 
How to Obtain the Estimators 
If we were to accept the above argument, we would have a means of ranking a 
set of estimators once they \lwe presented to us. But how can we cope with the 
embarrassingly large variety of functions of t.he data which might have relevance 
as estimators? Is there some way in which we can simplify the problem so as to 
pick out, if nofene, then at least a limited number of good estimators which may 
then be compared? 
One early attempt to make the problem manageable was to arbitrarily limit 
the estimators to be (a) linear functions of Y which were (b) unbiased, and then 
to find the one which had minimum variance. 
These requirements had the 
advantage that the "best" estimators could often be obtained from their definition 
by a simple minimization process. 
For example, in a random sample of size n 
from a population with finite mean ¢ and finite variance, the sample mean y is 
the "best" estimator of ¢ satisfying these requirements. 
However, after it was realized that biased estimators often had smaller mean 
squared errors than unbiased ones, the requirement of unbiasedness was usually 

308 
Random Effect Models 
A5.6 
dropped and the criterion of minimum M.S.E. substituted for that of minimum 
variance. 
As an example, for the exponential distribution fey) = cJ> -I e- Yi¢ , 
(y> 0, cJ> > 0). The linear function ny/(n + I), although biased, has smaller M.S.E. 
than that of the sample mean y in estimating cJ>. 
When it was further shown 
that non-linear estimators could be readily produced (from Bayesian sources) 
having smaller M.S.E. than the "best" linear ones for all values of ¢ , linear 
estimators could no longer be wholeheartedly supported.t 
Possible ways of obtai ning non-linear estimators which sti/l might minimize mean 
squared error were therefore examined. Perhaps then the logical final step will be 
to abandon the arbitrary criterion of minimum M.S .E. With this step taken, it 
would seem best to give up the idea of point estimation altogether and use instead 
the posterior distribution for inferential purposes. 
One method of finding estimators which are not necessarily linear in the 
observations is Fisher's method of maximum likelihood. Here the estimator is 
the mode of the likelihood, and , under certain conditions, asymptotically at least, 
the estimators have Normal distribution and smallest M.S. E. 
However, some 
investigators have-Celt that the likelihood method itself was too limited for their 
purposes. A few ye*rs ago therefore, the Bayesians were surprised by unexpected 
bed fellows anxious/to examine their posterior distributions for interesting features. 
Whereas it was the mode of the likelihood function which had received almost 
exclusive attention, it now seemed to be the mean of the posterior distribution 
which became the main center of interest. Nevertheless the posterior mode and 
the median have also come in for some examination and each has been recom-
mended or condemned on one context or another. 
A Decision-theoretic Approach 
Justification for this way of choosing estimators is sometimes attempted on 
formal decision-theoretic grounds. In this argument, let $ be the decision-maker's 
"best" guess of the unknown cJ> relative to a loss function L($, cJ». 
Then the 
so-called "Bayes estimator" is the guessed value which minimizes the posterior 
expected loss 
E L($, cJ» 
= f "" L($, ¢)p(cJ> I y) dcJ>. 
¢ Iy 
- "" 
]n particular, if the loss function is the squa red error 
L1($, cJ» 
= ($ - cJ»2 
(AS.6.la) 
then, provided it exists, the Bayes estimator is the posterior mean ,! but by suitably 
t An example of this kind will be discussed later in Section 7.2. 
t When authors write of the "Bayes estimator" without further qualification they usually 
refer to the posterior mean. The naming of "Bayes" estimator seems particularly unfair 
to the Rev. Thomas Bayes who already has had much to answer for and should be cleared 
of all responsibility for fathering this particular foundling. 

A5.6 
Appendix 
309 
changing the loss function the posterior mode or median can also be produced. 
Thus, see Pratt, Raiffa and Schlaifer (1965), for 
for 
(A5.6.1 b) 
L,(q,,¢) ~ (: 
1<$ -
4>1 > e 
1<$ - ¢I < e 
where e > 0 is an arbitrarily small constant, the Bayes estimator is a mode of the 
posterior distribution. Also, for 
the Bayes estimator is the posterior median. Indeed, if the loss function were 
then by appropriately choosing a 1 and a2 , any fractile of the posterior distri-
bution could be an estimator for 4>. 
In a proper Bayesian decision analysis, the loss function is supposed to re-
present a realistic economic penalty associated with the available actions. This 
justification, in our view, only serves to emphasize the weakness of the point 
estimation position. (a) In the context in which the estimator was to be employed, 
if the consequences associated with inaccurate estimation were really what the 
loss function said they were, then the resulting estimator would be best for 
minimizing this loss and this would be so whether its M.S.E. in repeated sampling . 
was large or small. (b) It seems very dubious that a general loss function could be 
chosen in advance to meet all situations. In particular, it is easy to imagine practical 
situations where the squared error loss (A5.6.1a) would be inappropriate. 
In 
summary it seems to us that this Bayesian decision argument has little to do with 
point estimation for inferential purposes. 
Are Point Estimates Useful for Inference? 
The first question a Bayesian should ask is whether or not point estimates provide 
a useful tool for making inferences. 
Certainly, if he were presented with the 
Normal distribution of Fig. AS.6.1 (a) and told he must pick a single number to 
represent ¢ he would have little hesitation in picking 22 which is both the mean 
and the mode of the distribution. Even here he would regret that he had not been 
allowed to say anything about the uncertainty associated with the parameter 4>. 
But what single number should he pick to represent 4> in Fig. AS.6.1 (b)? If he chose 
the higher mode at 57, he would certainly feel unhappy about not mentioning 
the lower mode at 30. He would surely not choose the "Bayes" estimator--the 
posterior mean at 4S.8 which is close to a local minimum density. 
While it easy to demonstrate examples for which there can be no satisfactory 
point estimate, yet the idea is very strong among people in general and some 

310 
Random Effect Models 
A5.6 
statisticians in particular that there is a need for such a quantity. To lhe idea 
that people like to have a single number we answer that usually they shouldn' t 
get it. Most people know they live in a statistical world and common parlance is 
full of words implying uncertainty. A in the case of w~ather forecasts, statements 
about uncertain quantities ought to be made in terms which reRect that uncertainty 
as nearly as possible. t 
Should inferences be Based 011 Sampling Distrihutioll of Poillt Estimators') 
Having computed the estimators (rom the posterior distribution, the sampling 
theorist often uses its sa/JIpling distribution in an attempt tll make inferences about 
the unknown parameter. 
The bilJlodal example perhaps makes it easy to see 
why we are c.iubious about the logic of this practice. 
first we feel that having 
gone to the trouble of computing the posterior distribution , it may as well be put 
to use to make appropriate inferences about the paramelers. Second we cannot 
see what relevance the sampling distribution of, say, the mean of the posterior 
distribution would have in making inferences about cp. 
Why :vfean Squared Error and Squared Error Loss ? 
We have mentioned that the M.S.E. criterion is arbitrary (as are alternative 
measures of average closeness). We have also said that in the decision theoretic 
framework the quadratic loss function leading to the posterior mean is 
arbitrary (as are alternative loss functions yielding different features of the 
posterior distribution). The question remains as to why many sampling theorists 
seem to cling rather tenaciously to the mean squared error criterion and the quad-
ratic loss function . The reasons seem to be that (a) for their theory to work, they 
must cling to something, arbitrary though it be and (b) given this, it is best to cling 
to something that works weU for the Normal distribution. 
The latent belief in universal near-1\Jormality of estimators is detectable in the 
general public's idea that the "most probable" and the "average" are almost 
synonymous, implying a belief, if not in Normality, at least that "mean equals 
mode". Many statisticians ramiliar with ideas of asymptotic I\ormality of maxi-
mum likelihood and other estimators have a similar tendency. 
This has led to a curious reaction by some workers when the application of 
these arbitrary principles led to displeasing results. The Bayesian approach or 
the Bayesian prior have been blamed and 11'0t the arbitrary features, curious mecha-
nism and dubious relevance of the point estimation method itself. 
t The public is ill served when they are supplied with single numbers published in news-
papers and elsewhere with no measure of their llncertainty. For example, most figures in a 
financial statement, balanced down to the last penny, are in fact merely estimates. When 
asked whether it would be a good idea to publish error limits for those figures, an 
accounting professor replied "This is commonly done internally in a company, but it 
would be too much of a shock for the general publ ie!" 

A5.6 
Appendix 
311 
Poinl Estimators for Variance Components 
For illustration we consider some work on the variance component problem 
tackled from the standpoint of point estimation. Two papers are discussed, one 
by Klotz, Milton and Zacks (1969) and the other by Portnoy (1971) concerning 
point estimators of the component o-~ for the two-component model. 
M.S.E. of Variolls Estimators of o-~ 
These authors computed the M.S.E. of a variety of estimators for (o-i, o-~). 
Specifically, for the component o-~, they have considered for various values of 
J and K the following estimators: 
a) the usual unbiased estimator a-~ 
b) the maximum likelihood estimator 
c) the niean of the posterior distribution (S.2.39)-see (AS.3.1) in Appendix 
AS.3 
d) the component for o-~ of the mode of the joint distribution (S.2.14) 
e) the component for o-~ of the mode of the corresponding joint distribution 
of (o-~2, o-~), 
among some others. 
Their calculation shows that over all values of the ratio 
o-~/(o-~ + o-~) 
i) the M.S.E. of the estimators (b), (d) and (e) are comparable and are much 
smaller than that of (a), 
ii) by far the worst estimator is the posterior mean (c). For example, when J = S 
and K = 2, the M.S.E. of (c) is at least 8 times as large as those of (b), (d) 
and (e). 
The fact that the posterior mean has very large M.S. E. seems especially disturbing 
to these authors, since such a choice is motivated by the squared error loss 
(AS.6.1a). Thus, they concluded that 
"The numerical results on the M.S.E. of the formal Bayes estimators strongly 
indicate that the practice of using posterior means may yield very inefficient esti-
mators. The inefficiency appears due to a large bias caused by posteriors with 
heavy tails resulting Fom the quasi prior distributions. 
Other characteristics of 
the posterior distri bu tions such as the mode or median can give more efficien t 
point estimators". (1. Al1u;r. Statist. Assoc., 64, p. 1401). 
The italics in the above are ours and they draw attention to a point we cannot 
understand. 
The authors' conclusions and associated implications may be 
summarized in part as follows. 

312 
Random Effect ylodels 
AS.6 
Table AS.6.1 
Comparison of estimators of a~ 
, 
Implied 
Feature of 
Estimators 
Prior 
loss function 
posterior distribution 
judged 
Criterion 
[see (A5.6.la,b)] 
Noninformative 
L, 
mean 
bad 
M.S.E. 
(5.2.9) 
Noninformat ive 
L 2 
mode of d 
good 
M.S.E. 
(5.2.9) 
w.r.t. (a L a ~ ) 
Noninformative 
L 2 
mode of a ~ 
good 
M.S.E. 
(5.2.9) 
W.Lt. (a!2 , a ~ ) 
But since the same prior distribution is assumed in every case yielding the same 
posterior, how is the prior to blame for the alleged inefficiency of one of the 
estimators? Or why on this reasoning should it not be praised for the efficiency 
of the other two? 
We are neither encouraged by the fact that based on the reference prior in 
(5 .2.9), some characteristics of the posterior distribution in (5.2.14) such as the 
mode has desirable sampling properties, nor are we dismayed by the larger 
M.S.E. of the corresponding posterior mean of a~. One simply should not confuse 
point estimation with inference. 
Inferences about a parameter are provided not by some arbitrarily chosen 
characteristic of the posterior distribution , but by the en/ire distribution itself. 
One of the first lessons we learn in Statistics is that the mean is but one possible 
measure of the location of a distribution. While it can be an informative measure 
in certain cases such as the Normal, it can be of negligible value for others. 
Other Examples of the InejficielJc:y of the Posterior Mean 
For the problem of the Normal mean with known variance a 2 , the posterior 
distribution of e based on the uniform reference prior is N(r, a2,'n). As mentioned 
earlier, this distribution allowed us to make inferences not only about e but also 
about any transformation of (J such as eO or (J-' . While the posterior mean of e, 
y, is a useful and important characteristic of the Normal posterior, the posterior 
mean of eO, e·H l a
2
/ " , tells us much less about the log Normal posterior, and the 
posterior mean of A-I , since it fails to exist, tell us nOfhil1R about the distribution 
of (F I! Thus, so far as inference is concerned, it is irrelevant whether some 
arbitrarily chosen characteristic of the posterior has or does not have some allegedly 
desirable sampling property. In particular, it is irrelevant that y is the M.M.S.E. 

--'- -
-
AS.6 
Appendix 
313 
estimator for e, or that for eO, as can be readily shown, the posterior mean ey+ ~ a2 /JI 
has larger M.S.E. than that of the posterior mode ey - a'/JI (which, for that matter, 
has larger M.S. E than that of the estimator eY-1. 50 ' /JI) . Furthermore, the non-
informative prior should not be to blame for causing the heavy right tail of the 
log Normal posterior for eO, nor should the same prior be praised for producing 
the symmetric Normal posterior for e. It would certainly seem illogical to "judge" 
the appropriateness of the posterior and therefore of the prior on such grounds. 
Adjusting Prior to Produce " Efficient" Posterior Mean 
In view of all this, the analysis given in Portnoy's paper we find even more 
perplexing. He proposes the scale invariant loss function 
-
2 
($_0"~)2 
L(c/>, 0"2) = 4(O"i + KO"D2 
(A5.6.2) 
and proceeds to obtain Bayes estimators of O"~ with respect to the class of priors 
p(O"i, O"D if.. (O"iy-(I!+1) (O"i + KO"Db. 
(A5.6.3) 
For given (a, b), the estimator is the ratio of the posterior expectations 
-
{O"i} I 
2 
2 -2 
c/>(a.b) = E 
2 
Z 2 I E (0"1 + KO"z) 
. 
ailY 
(0"1 + KO"z) 
a;IY 
Using the "admissibility" proofs si milar to James and Stein (1961), he concludes 
that the best estimator is the one corresponding to (a = -
I, b = -
I) in (A5.6.3) 
which is then precisely our l1oninformative prior. 
His computation shows that the 
M.S.E. of $(_1._1) is much smaller than the corresponding posterior mean. 
This result is certainly not surprising if we consider the similar but algebraicalJy 
much simpler problem of the )\;ormal variance 0"2 with the mean e known . As we 
have discussed in Section 2.3, the posterior distribution of 0' 2 based on the non-
informative prior p(a2) f 
0" 
2 is ns2XI~Z where nsz = LCy; -
8)2. 
It is readily 
seen that the Bayes estimator with respect to the scale invariant loss function 
is then 
which , as is well known, is the minimum M.S.E. estimator of (52 among all function 
of S2 having the form cs 2 
In particular, the M.S. E. of $ is smaller than that of 
the postr'rior mean ns 2 (1/ - 2). 
What is extraordinary is the conclusions Portnoy draws from his study. 8sing 
our notation, he says that 
"For the case of the present estimator, $( - 1. _I) ' we actually find that the pnor 
distribution (for a = - I, b = -I) corresponds exactly to the Jeffreys' prior, 

314 
Random Effect Models 
AS.6 
0-;2(o-f + Ko-D-l. However, $(-1.-1) is not the posterior mean, but the posterior 
expected value of o-t (o-i + KCTD 2 over the posterior expected value of I/(o-i + 
Ko- ~ ) 2 , the denominator coming from the normalizing factor in the loss function. 
This is the same as taking the posterior mean for the prior 0-;2(0-~ + Ko-~) - ~. The 
reasonable size of the mean squared error of $( -1. _) shows that this latter pos-
terior distribution is centred at least as weJl and has su~stanti a lly smaller variance 
(on the average) than the posterior for the Jeffreys' prior. Thus, if one wishes to 
make inferences based on a posterior distribution, one can seriously recommend 
using the prior 0-;2(0-~ + Ko-~) - 3 instead of the Jeffreys' prior. This serves to 
justify, in my opinion , the use of squared error as loss: by using squared error and 
by taking an appropriate limit of what might be called Bayes invariant priors, one 
is assured of finding a posterior distribution which, on the average, is reasonably 
centered and has reasonably small variance. As this example shows, the Jeffreys' 
prior can lead to posterior distri butions with mean and variance far too large." 
(Ann. Math . Statist . 42, p.1394). 
This seems to be suggesting that in order to obtain a good estimator with 
respect to the scale invariant loss function in (A5.6.2), Jeffreys' prior is needed , 
but the use of such a prior is, at the same time, undesirable in making inferences 
because it leads to a posterior mean having large mean squared error. To conclude 
that this study serves to justify "the use of squared error loss" is indeed difficult 
to follow. 
To see where this sort of argument leads, consider again the problem of the 
Normal variance 0- 2 with known mean . Let the prior distribution be of the form 
Suppose we wish to estimate some power of 0- 2 , say 0-2P. Now for different values 
of r:t., a family of posterior means for 0-2p can be generated . . If we were to insist on 
a squared error loss function and hence on the posterior mean as the "best" point 
estimator of 0-2 p, then it is shown in Appendix 5.7 that the estimator which mini-
mizes the M.S.E. is the one for which the prior would be such that 
CJ. = 2p + I, 
that is, 
(A5.6.4) 
provided 
n > -4p. 
The implications would then be: (i) if we were interested in (12 (p = 1), the prior 
should be proportional to (0"2)-3 which is similar to the prior suggested by Portnoy 
for estimating o- ~ ; (ii) on the other hand, if p = 0 correspo~ding to log CT, then we 
would use the noninformative prior (1 . . 2; but (iii) if we should be interested in 
0-- 100 (p = -50), then we ought to employ a prior which is proportional to 
0- 198 ! 

AS.7 
Appendix 
315 
To put the matter in another way, the posterior distribution of (52 corres-
ponding to (AS.6.4) would then be (for II > - 4p) 
(AS.6.S) 
with n' = n + 4p and S'2 = ns2,'n', which would be the same as that resulting 
from Jeffreys' noninformative prior but with 11' = 17 + 4p observations. Noting 
that n's'2,'(52 '" X,7" so that (n's'2)p /(52p", {X,7.)P this would mean for example 
that if we wish to estimate ((52)10 from a sample of, say, 11 = 8 observations, we 
would have to base our inferences on a posterior distribution which would corres-
pond to a confidence distribution appropriate for 48 observations. On the other 
hand, if we were interested in estimating «(52)-10 from a sample of 11 = 48 obser-
vations, the posterior we would use would be numerically equivalent to a conn-
dence distribution relative to only 8 observations. 
Conclusions 
The principle that the prior, once decided, should be consistent under trans-
formation of the parameter seems to us sensible. By contrast, the suggestion that 
to allow for arbitrariness in the methods of obtaining point estimators, the 
principle be abandoned in favour of freely varying the prior wherever a different 
function is to be estimated seems less so. 
APPENDIX AS.7 
Mean Squared Error of the Posterior Means of (J2p 
In this appendix, we prove the result stated in (AS.6.4). That is, suppose we have 
a random sample of n observations from the Normal population N(e, (52) where 
. (52 is known, and supposed that the prior distribution of (52 takes the form 
(AS.7.l) 
then among the class of estimators of (J2p which are the posterior means of (J2 p, 
the estimator which minimizes the M.S. E. is obtained by setting the prior to be 
such that 
rx = 2p + I 
(AS.7.2) 
provided n > - 4p. 
Corresponding to the prior (AS.7.1), the posterior distribution of (52 is 
(ns2)+v 
p«(J2 I y) = 
«(52) - (tv + I) e - (" S'/ 2,,') 
r (-iv)21:
V 
, 
' 
(AS.7.3) 

316 
Random Effect Models 
where 
v = 11 + 2(~ -
1) > O. 
Thus, using (A2.1.2) in Appendix A2.1, the posterior mean of a2p is, 
E«(J2 p I y) = c(ns2)p, 
where 
_ r(-!-v - p) (J.)P 
C -
r(JV) 
2, 
AS.7 
(AS.7.4) 
provided '~v -
p > O. 
Now, the sampling distribution of ns2 is (J2X;' . It follows 
that the M.S.E. of C(I1S 2y is, for (-1/1 + 2p) > 0 
M.S.E. = E [c(ns2)p -
(J2 P]2 = a2p I + c 2 
I 
-
2c -=-C--:--7"'-
[ 
1(J/1 + 2p)22
p 
rc-}n + P)2 P] 
y 
fUn) 
rUn)
' 
(AS.7.S) 
Letting 
(
M.S.E. 
) 
g(v) = 7P - I f(-}n) 
we have 
[ 
rc~v - P)1 2 
r(-!-v - p) 
g(v) = 
r(-}v) 
fen + 2p) - 2 
r(-}v) 
f(-~11 + p). 
(AS.7.6) 
Differentiating with respect to v and setting the derivative to zero, we obtain 
1(-!-n + 2p) {fn·v - p) f'(·}v - p) -
[f(-}v - p)]2 f'(~ v) ; 1(-}v)} 
- r('ill + p)[f(1v) f'(}v - p) - f(-1v - p) f '(}v)] = 0, 
where f'(x) is the digamma function. The solution is 
v = n + 4p 
or 
~ = 2p + 1. 
(AS .7.7) 
By differentiating g'(v) it can be verified that this solution minimizes the M.S.E. 

CHAPTER 6 
ANAL YSIS OF CROSS CLASSIFICATION DESIGNS 
6.1 INTRODUCTION 
In the previous chapter we have discussed random effect models for hierarchical 
designs. In some situations it is appropriate to run cross classification designs and 
it may be appropriate to assume either 
a) that all factors have random effects, 
b) that all factors have fixed effects, or 
c) that some factors have random effects and some have fixed effects. 
We begin with a general discussion of the situation when there are just two factors 
and for illustration we suppose the factors to be car drivers and cars. 
6.1. t A Car-Driver Experiment: Three Different Classifications 
Consider an experiment in which the response of interest was the performance, as 
measured by mileage per gallon of gasoline, achieved in driving a ncw Volkswagen 
sedan car. Suppose that 
a) eight drivers and six cars were used in the investigation, 
b) each driver drove each car twice, and 
c) the resulting 96 trials were run in random order. 
The above description of the investigation is, by itself, inadequate to determine an 
appropriate analysis for the results. To decide this a number of other things must 
be known. Most important are: 
a) the objectives of the investigation, 
b) the design of the experiment to achieve these objectives, in this example the 
method of selection of drivers and cars. 
Objectives: The Particular or the General ? 
ln one situation the objective may be to obtain information about the mean per-
formance of each of the particular six cars and eight drivers included in the experi-
ment. Comparisons between such means are sometimes called "fixed effects" and 
the analysis is conducted in terms of a fixed effect model. 
317 

318 
Analysis of <':ross Classification Designs 
6.1 
In another situation the individual cars and 'or drivers in the experiment may 
be of no interest in themselves. The objective may be to use them only to make infer-
ences about the populations of cars and/or drivers from which they are presumed to 
be randomly drawn. In particular, the variances (and possibly certain covariances) 
of these populations will be of special interest. In this situation our analysis is 
cond ucted in terms of a random effect model. 
In still another situation the objective may be to learn about the mean perfor~ 
mance of each car as it performs in the hands of a population of drivers. In this 
case the analysis is conducted in terms of a mixed (random and fixed effect) model. 
With the two jactor cross classification then, three situations can be distinguish-
ed which are associated with specific models as follows: 
a) both factors random-two-way random effect model, 
b) one factor random, one factor fixed- two-way mixed model, and 
c) both factors fixed-
twa-way fixed effect model. 
Design Considerations 
So far we have talked only of objectives but it is important that the experiment 
be conducted so that it is possible to achieve these objectives. 
In the random 
effect model, interest centers on the characteristics of populations, and in particular, 
on their variance components. 
To estimate such characteristics the particular 
cars or drivers included in the experiment must be random selections from the 
relevant populations of cars and drivers. For example, the two-way random effect 
setup would be appropriate if the six VW sedan cars and the eight drivers were 
randomly drawn from relevant and definable populations. The sampled population 
of cars might be all new VW sedans available for sale in the State of Wisconsin 
in a particular week, The sampled population of drivers might be all students at 
the University of Wisconsin-Madison prepared to take part in the experimenl for 
a payment of $20. 
It will often happen, that the population one would really like to make in-
ferences about is different from the population that can be conveniently sampled. 
For instance, we may really be interested in the whole population of drivers in the 
United States, but may settle for the student drivers to produce a feasible experi-
ment. In such a case it must be remembered that the statistical conclusions apply 
only to the limited population sampled. Any extrapolation to a wider population 
is based solely on an opinion that, for example, "student drivers are similar to 
other drivers." 
Questions concerning the car-driver populations which this kind of experiment 
_ might answer are: 
I. How large a variance in gas mileage might be experienced by different drivers 
operating new VW sedan cars. 

6.1 
Introduction 
319 
2. What total proportion of this total variance was associated with: (a) variation 
in cars, (b) variation in drivers, (c) interaction between cars and drivers, and (d) 
unassignable variation. 
By contrast, the fixed effect setup implies that our interest centers on the mean 
performance of each of the six particular cars or the eight particular drivers which 
mayor may not be realistically considered as relevant random samples from specific 
popUlations. 
The present discussion uses the familiar sampling-theory terms, " random " and 
"fixed" effects. As we shall see later, in the Bayesian framework, where of course 
all effects (parameters) are random variables, appropriate choice of model forms 
with suitable prior distributions achieves the different inferential objectives. 
Classification of the Data in a Two- way Table 
Suppose in general that the test cond ucted with I cars denoted by (I, ... , i, .. . , 1) 
a nd J drivers denoted by (J, ... , j, ... , J) and every driver operates every car K 
times (K ?o J). Let Yij k be the kth (k = I, .. , K) observation for the jth dri ver on 
the ith car. The data can be set out in an J x J arrangement with K observations 
per cell as in Table 6. J .1. In the example considered above there are I = 6 cars, 
J = 8 drivers, and K = 2 replicate runs. 
Table 6.1.1 
Arrangement of observations in a two-way cross classification design 
Drivers 
j 
J 
Yijl 
Cars 
Y/jk 
YiJf. 
I 
I 
J 

320 
Analysis of Cross Classification Designs 
6.1 
Whichever setup is being considered we can always write 
(6.1.1) 
J n this equation e is the overall mean gas mileage, 6ij is the mean increase or decrease 
in gas mileage achieved when the )th driver is operating the ith car, and ejjk is the 
experimental error which is responsible for differences between replicate runs 
performed by the same driver in the same car. It will be supposed throughout that 
these experimental errors are distributed independently of each other, and of 
the bjj , with zero mean and variance cr; . 
6.1.2 Two-way Random Effect Model 
We now consider the formulation of an appropriate model when both factors are 
random. 
Classification of the Population in a Two-way Table 
We can picture the whole population of possibJe combinations of cars and drivers 
as given by the cells in the f x f table of Table 6.1.2 having aJl possible columns 
Cars 
Column 
(driver) means 
Table 6.1.2 
Two-way population of increments {)ij 
Drivers 
Row 
(car) 
means 
-
-
- ------ - -
- --- - ----- -
-
1'---
------- ------ -- --I--- ~ 
--- - -------- -----
- -- - --If----
---1----1- -- - -- - -
-
- -- --- -
---
------ - -- ------ --
-
- -- - -
·-
If---
-
- ----- -
-
- 1--
-
-1---- - -- - -
-
---11--
- -1 
---- - ---
-
-
-
-
-
- ---11---
- 1 
r~ 
====11==1==== =11===\ 
c· 
.J 
o 
.~.-j,!, 
-
• 
. '  
~ 
-
-

6.1 
Introduction 
321 
(drivers) indexed by j = 1,2, ... , cf and aJi possible rows (cars) indexed by i = 
1,2, ... , f 
Then 0,/ wilJ be the mean increment associated with the l'th car oper-
ated by the Jth driver. That is, the amount by which the average gas mileage 
achieved by this car-driver combination exceeds or falJs short of the overalJ popu-
lation mean O. 
It is now convenient to define the row means and column means of the iJi/ in 
this population table as the (random) effects for cars and drivers respectively and 
the "nonadditive" increment as the (random) interaction effect. 
Thus, 
r i = 0i. = random effect for ith row (car), 
c) = O.i = random effect for jth column (driver), 
t i.j = 0 iJ - 0 i. -
o.J = random interaction effect (car x driver), 
where, as before, we use the notation that a dot replacing a subscript indicates 
the average over that subscript. The identity 
I. I. 0:/ = c1 LO} +..f L0
2
j + LL (0,) - 0,. - oy 
can then be written 
If we now define the variance components as 
2 
Lr; 
(J =--
r 
..f 
row (car) component, 
2 
L c~ 
(J =--
c 
c1 
column (driver) component, 
2 
I. tL 
(J =--
t 
..fc1 
interaction (car-driver) component. 
Then letting 
we obtain 
( 6.1.2) 
(6.1.3) 
( 6.1.4) 
On this formulation, then, we can substitute oij = rj + cj + ti) in (6.1.1). 
The 
model for the actual test data in the 6 x 8 replicated array of Table 6.1. I is thus 
Yjjk = e + rj + cj + Ii) + ejjk' 
We now consider how the rj , cj , and lij in the equation are to be interpreted. 

322 
Analysis of Cross Classification Designs 
6.1 
The random effect model assumes that the 1=6 rows (cars) were selected 
randomly from a population of f 
rows (cars) and the J = 8 columns (drivers) 
were similarly selected from a population of j' columns (drivers). We show in 
the next section that, assuming the populations to be large, this random sampling 
ensures that the rio cj, and tij may all be treated as uncorre/ated random variables. 
Also the errors eijk are by assumption uncorrelated with the Oij' 
Thus finally the two-way random effect model may be written 
i=I" .. , I ; j= I, ... , J; 
k = I, ... ,K, 
(6.1 .5) 
where ri, cj , lij ' eijk are all uncorrelated random variables with zero means and 
E(rl)= CJ;, 
E(c;)=CJ~, 
E(t;;)=CJ,2, 
E(etjk)=CJ,~, 
(6. 1.6) 
so that 
( 6.1.7) 
In the traditional sampling-theory analysis of the problem, this setup wouJd be 
appropriate for making inferences about the behavior of an infinite population of 
drivers and cars, but not about the parricular drivers an d cars tested. The problem 
might arise for example in a general production study by a large manufacturer of 
motor cars. The conclusions drawn could indicate the degree of variability of 
gas mileage over the relevant populations of drivers and cars and the relative 
contribution of cars, drivers, and interaction to that overall variability. 
J ustificatiol1 of the TIro-way Random Effect M ode/ 
To show that the r i, Cj' and t ij in (6.1.5) are uncorrelated random variables with the 
properties just mentioned we regard the particular small sets of 1 rows (cars) and J 
columns (drivers), actually under study, as random samples (without replacement) from 
the f rows and j' columns of Table 6.1.2. The quantities Oij in (6.1. J) are then random 
variables. It can be readily shown that 
1 
£(0 .. 0 . ,) = (52 + 
«(52 -
(52) 
'J 'if 
r 
(j' _ I) 
r 
, 
j #- j , 
(6.1.8) 
and 
f 
j' 
_ ___ __ (5 2 -
CJ;, 
(..I - 1)(j' -
1) 
C 
(f - 1)(/- J) 
i#- i,j#-j; i,t'= 1, .. ,,/; j,j= I, ... ,J. 

6.1 
Introduction 
323 
On the assumption that the populations are large, we have 
and 
(6.1.9) 
If IJ is a J x J identity matrix and Ij is a J x 1 vector of ones, then the /J x /J covariance 
matrix of the quantities (jij is given by 
: 1(22)+11,2 : 
j 
(Ie + (I, 
J 
j(Ir 
1 J 
(6.1.10) 
The form of this matrix confirms the appropriateness of the formulation set out in 
(6.1.5) through (6.1.7). 
6.1.3 Two-way Mixed ~ode1 
The same car-driver data could arise in quite different circumstances. Suppose 
a car rent~1 agency has a particular set of I = 6 cars, not necessarily all the same 
model or make, and it is desired to estimate their individual mean performances 
when operated by a relevant population of drivers. To do this a test is conducted 
with J = 8 drivers randomly chosen from the population of interest. The under-
lying setup is represented by the elements in Table 6.1.3, which can be thought 
of as sub-population of Table 6.1.2 in which are selected the I particular rows 
(cars) of interest. 
Table 6.1.3 
A sub-population of 1 particular rows of the population of increments (jij 
DRIVERS 
1 
f 
I~ 
(j] i 
CARS 
(jj,j 
rj 
I 
(jTI 
r[ 

324 
Analysis of Cross Classification Designs 
6.1 
The behavior of the increments of the J cars with, say, the j th driver, is thus 
represented by an i-variate observation ()~ = (b l / , ... , bJ) 
from a large popula-
tion of", drivers with 
i, t' = I, ... , I , 
(6.1.11) 
where 
1 
2 
aii = '" L«(>;', - r;) , 
I 
ai, = '" L(bil - r;)(b,) - rJ. 
Further, it can be verified that when ", --> Cf) the increments associated with two 
different drivers are uncorrelated, that is, 
Cov (b i j , bi ) 
= 0, 
The model in (6.1.1) can then be written 
j *" j. 
j=I, ... ,J; k=I , .. ,K, 
with 
8j = 8 + ri, 
we have finally 
(6.1.12) 
(6.1.13) 
(6.1.14) 
The principal objective here is to make inferences .about the mean vector 9 which 
determines the mean performances of the cars. 
However the meaning to be 
attached to the individual elements of the covariance matrix VJ of the b's is also 
worth considering. 
The ith diagonal element aii of this matrix represents the 
variation in performance when the ith car is operaled by the whole population 
of drivers. 
The covariance ai' may be written ai' = Pi,(aija,J 1/ 2 where P;; 
measures the correlation between performances achieved on cars i and t' by the 
population of drivers. If the two cars are similar, the driver who does well on one 
will usually do well on the other so that Pi' would be positive. For two cars with 
widely different characteristics, Pi. may be a quite small positive number or even 
negative. 
For example, if car i was easily manipulated by right-handed people 
but not by left-handed people while car ~. was easily manipulated by left-handed 
people but not by right-handed people, then Pi, could be negative. We shall see 

6.1 
Introduction 
325 
in the next section that by use of this correlation structure we can allow for " inter-
actions" of a much more sophisticated kind than is possible with what has been 
the customary assumptions about " interaction variance." Indeed the customary 
assumptions correspond to a very restricted special case of the general model 
given here. 
6.1.4 Special Cases of Mixed Models 
The mixed model set out in (6.1.12) involves I unknown means ai > }/(J -- I) -I- I 
unknown elements in the covariance matrix VI and an unknown (J; , as parameters. 
Situations occur where si mpler models are appropriate. Two special cases may 
be of interest. 
1. 
Additive Model 
What sometimes is called the additive mixed model is obtained if we assume that 
(6.1.15) 
I n terms of the entries in Table 6.1.3, this is saying that 
bi.j - ri = b 'l - ri 
so that 
(Jii = ai ; = a;, 
all 
(i, t) . 
1 n this case, the model (6.1.1) may be written in the form 
(6.1.16) 
where cj a'1d eijk are uncorrelated random variables with zero means and E(,}) = 
a;, E(etjk) = a; so that E(Yijk) = l3i and Var (Yijk) = a~ + a; . If for the moment 
we ignore the experimental error eijk the model can be represented as in Fig. 6.1.1 
for 1 = 2, J = 3. 
For the driver-car example, C I is the incremental performance for the j th 
driver. It measures his ability to get a little more or little less gas mileage. The 
Perfo rmance of three 
Performance of the ,ame three 
drivers on car I 
drivers on car 2 
(a) 
(h) 
Fig. 6.1.1 Graphical illustration of an additive model. 

326 
Analysis of Cross Classification Designs 
6.] 
additive model will be appropriate if, apart from experimental error, this incre-
mental performance cj associated with the j th driver remains the same for alii 
cars which he drives (that is, c; = bij -
r, = b'i - r,). 
In choosing a random 
sample of dri vers we will choose a random sample of incremental performance so 
that the C " C2, .. . , CJ can be taken as a random sam pIe from a d istri bution (for 
example, the :-\ormal shown in Fig. 6.1.1) with zero mean and fixed variance 0";. 
This structure is sometimes adopted in modelling data generated by Fisher's 
randomized block designs. From this point of view we would here be testing I 
treatments (cars) in J randomly chosen blocks (drivers). 
2. 
Interaction Mode/ 
Another special case arises when the covariance matrix V J takes the form 
(6.1.17) 
where I pi < 1. 
In terms of the entries in Table 6.1.3, this says that, the I elements 
(b Ii' . . , b I;) are equi-correlated and have common variance, that is, 
1 
2 
2 
-
~(b . - r) = rI, 
J 
'/ 
I 
If'' , 
1 
2 
-
~«(j .. - r)(8 . - r) = rI, p 
/-
'/ 
' 
' J 
' 
0/
) 
i,/=I, ... , I; 
if=t·. 
( 6.1.18) 
If we further assume that p > 0, the covariance matrix in (6.1.17) can be written 
(6.1.19) 
where 0'; = ¢2 P and 0",2 = ¢2 (I -
p). The model in (6.1.1) can be equivalently 
expressed as 
(6.1.20) 
where c j , tij' and eijk are uncorrelated variables with zero means and 
and 
so that 
and 
This model is sometimes known as the mixed model wilh interaction. 
When 
0',2 = 0, it reduces to the additive mixed model in (6. I .16). 
The implications of this interacti on moc.iel are illustrated in Fig. 6.1.2. Ignoring 
the measurement error ei jk the two dots in Fig. 6.1.2(a) show the contribution 
8 1 + C I + 1,1 and 01 + C2 + t12 for drivers 1 and 2 using car 1. The quantities 
0, and O2 are fixed constants, C I and C 2 are drawn from a Normal distribution 

6.1 
I" 
Performance of Iwo 
drivers on- car I 
(al 
introduction 
Pe rform;lnce of Ih e sarn e Iwo 
drivers on car 2 
(b) 
Fig. 6.1.2 Graphical illustration of an interaction model. 
327 
having variance!J; and til and 11 2 are random drawings from a Normal distribution 
having variance !J,2 . Figure 6.1.2(b) shows the situation for the same two drivers 
operatinga second car. Note that although a shift in mean forB I to0 2 has occured, 
CI and C2 are assumed to remain the same as before but a different sample of I'S 
is added. 
This interaction model is less restrictive than the additive mixed model because 
it allows the possibility that an increment associated with a given driver will be 
different on different cars. 
However, because the additional increments Ii) are 
a : :··~.!' -' ,.:. b'~ independent and to have the same variance for each driver and each 
car. it is sli!. roo restrictive to represent many real situations. 
For example, consider two particular cars, say car I and car 2, which were 
in luded II: the experiment. According to the model, the Increments associated 
with ti .t J dhvers would be as follows: 
Driver j 
Driver j 
Driver J 
('ar j 
C, + I" 
cj + 11) 
CJ + III 
('ar 2 
cJ + 12J 
c) -+- ' 2 ) 
CJ + t 2J 
'\low 5U vose that the panicular cars I and 2 were very much alike in some impor-
tant f-:.' ~' but differed from the remaining cars. 
Thus, these particular cars 
migl-.l ,',2. c. : , . leg room for the driver than the remaining I -
2 cars which were 
tested. 
Then we would expect that the incremental performance c) of a short 
legged driver j would be enhanced by a factor II) almost identical to 12 ) while 
the incremental performance of a long legged driver i would be reduced almost 
eqmllly and the negative contributions II ) and 12/ would also be almost equal. 
Therefore, for these almost identical cars we would expect tl) to be the same as 
12 ) for all j. 
I n other words, we would expect II and 12 to be highly correlated 
within drivers and not uncorreJated as required by the model. The assumption 

328 
Analysis of Cross Classification Designs 
6.1 
of common variance for lij might also be unrealistic because, for example, the 
differences between drivers might be emphasized or de-emphasized by particular 
cars. 
6.1.5 Two-way Fixed Effect Model 
In some situations, no question would arise of attempting to deduce general 
properties of either the population of drivers or the population of cars. Interest 
might center on the performance of a particular set of J cars with a particular 
group of J drivers. In this case, setting 
where 
1 
b 
= -Ib 
I . 
J 
I} ' 
I 
b 
= - Io· 
.} 
I 
I}' 
the model (6.1.1) becomes 
1 
b .. = -
I Ib 
I J 
I} ' 
[)=8+b .. , 
(6.1.21) 
(6.1.22) 
This is the two-way classification fi xed effect model appropriate in this context 
for estimating the parameters Ct.i, {3j , }'ij' As we have mentioned earlier, the term 
fixed effect is essentially a sampling theory con~ept because in this framework . the 
effects Ct. i, {3j and Yij are regarded as fixed but unknown constants. 
From the 
Bayesian viewpoint, all parameters are random variables and the appropriate 
inference procedure for these parameters depends upon the nature of the prior 
distribution used to represent the behavior of the factors corresponding to rows 
and columns. 
Problems could arise where a noninformative situation could be 
approximated by allowing these to take locally uniform and independent priors. 
We should then be dealing with a special case of the linear model discussed in 
Section 2.7. On the other hand and in particular for the driver-car example it 
might be more realistic to assume that the ex's, {3's, and y's were themselves random 
samples from distributions with zero means and variances u;, u;, u;, respectively. 
The problem of making inferences about the parameters (Xi' {3j , Yij would then 
parallel the approach given later in Section 7.2 for the estimation of means from 
a one-way classification random effect model. 
In the above we have classified a number of different problems which can be 
pertinent in the analysis of cross classification designs. In what follows we shall 
not attempt a comprehensive treatment. 
Rather, a few familiar situations will 
be studied to see what new features come to light with the Bayesian approach. 
We begin by considering the two-:-vay random effect model in (6.1.5) and then 
devote the remainder of the chapter to the analysis of the additive mixed model 
in (6.1.16) and the mixed model with interaction in (6.1.20). 

6.2 
Cross Classification Random Effect Model 
329 
6.2 CROSS CLASSIFICATION RANDOM EFFECT MODEL 
In this section the problem is considered of making inferences about the variance 
components in the two-way model 
i=I , ... , I ; }=I, ...• J ; k=I, ... , K, (6.2.1) 
where Yijk are the observations, 0 is a location parameter, ri , ej , and lij are three 
different kinds of random effects and eijk are the residual errors. In the context of 
our previous discussion the random effects ri, cj ' and Ii) could be associated respec-
tively with cars, drivers, and car· driver interaction. 
The model would come 
naturally if the samples under study were randomly drawn from large popula-
tions of cars and drivers about which we wished to make inferences. It was shown 
that the ri , cj , lij' and eijk could then be taken to be uncorrelated with zero means 
and variances a;, a:, a
t
2 , and a;, respectively so that 
v ( 
) 
2 
'
2 
2 
2 
ar Yijk = a.. + (J c + at + a e' 
(6.2.2) 
The form of the model (6.2.1) is closely related to the three-component 
hierarchical design model discussed in Section 5.3. 
Indeed, expression (6.2.1) 
reduces to (5.3.1) with appropriate changes in notation if either a; or (J: is known 
to be zero. 
The relevant sample quantities are conveniently arranged in the analysis of 
variance shown in Table 6.2.1. 
Using the table, we can write: 
A2 
m, - m t 
IT 
= ---
T 
JK 
where these sampling theory point estimates of the components a;, a~, at
2 , and (J; 
are ~btained by solving the equations in expectations of mean squares as in the 
case of hierarchical design models. 
The difficulties encountered are similar to 
those discussed earlier. 
6.2.1 The Likelihood Function 
To obtain the likelihood function , it will be useful to recall first a number of 
results related to the analysis of two-way classifications which are summarized In 
the following theorem. 
Theorem 6.2.1 
Let xij (i = I, ... ,I,) = I, ... ,J) be IJ independent observations 
from a Normal distribution NCO, (J2) arranged in a two-way table of I rows and J 
columns. Let x., Xi., Xj' and xij -
Xi. -
Xj + X . be respectively the grand mean, 
row means, column means, and residuals. Then 
I. I I X;j = lJx~ + J I(x/. -
X Y + I I (Xj -
X .)2 
+ I I (xlj -
x/. -
Xj + x.y, 

Source 
G rand mean 
Main .effect "r" 
Ma in effect " e" 
Interaction r x C 
Residual 
Total 
Table 6.2.1 
Analysis of variance of two-way random effect models 
S.S. 
IJK(y . - W 
S, = J K 'L (Yi _ Y . )2 
S = [K(y . _ Y 
)2 
C 
.J.. 
.. 
S,= K'L'L(Yij. -Yi - Y. j + Y . / 
Se= 'L'L'L(Yijk- YijY 
LLL(Yijk _0)2 
d.f. 
V,= (I-I) 
vc= (J-l) 
v, = (I-I)(J-l) 
ve=IJ(K-J) 
UK 
M.S. 
UK(y .. _0)2 
In, = Sri v, 
rIIc=Sc/vc 
In, =Sr/v, 
I11c=S) Ve 
E.M.S. 
CJ; + KCJ,2 + [KCJ; + J KCJ; 
CJ;" = CJ; + KCJ,2 + J KCJ; 
u;,c = CJ; + KU,2 + [Ku; 
u;, =CJ; + KU,2 
CJ2 
e 
'--' 
'--' 
o 
)-
:0 ., 
q 
'" 
in' 
o .., 
(") 
.., 
o 
'" 
'" 
(") 
0;-
'" '" :;; 
~ 
o· 
:0 
b 
'" '" 
a"C' 
:0 
'" 
a-
N 

6.2 
Cross Classification Random Effect Model 
331 
2. The sets of quant;'ies x ., {x,. - .x.}, {Xj - x.} , and {Xij -
Xi -- '1 j -!- x .. } 
are distributed independently of one another, 
3. J L (Xi. -
X Y ~ a 2X~1 I» T L (Xj -
X. Y ~ a2xtJ _ J) and 
L L (xij -
Xi. -
Xj -:. x .. f ~ a2X}I _ I)(J _ I)' 
4. So far as each of the sets {Xi. -
X J, {Xj -
x J, and {xi) -
Xi. -
x. j + x J 
is concerned, the corresponding sum of squares given in (3) is a sufficient 
statistic for a 2 . 
In the theorem, (I) is an algebraic identity, (2) can be proved by showing that the 
four sets of variables are uncorrelated with one another, (3) follows from Cochran's 
theorem (1934) on distribution of quadratic forms, and (4) can be seen from 
inspection of the likelihood function. 
For the model (6.2. 1), we shall make the usual assumption that (ri' cj' Ii), eijk ) 
are Normally distributed. Since the effects are uncorrelated, the assumption of 
Normality implies that they are also independent. 
To obtain the likelihood 
function , it is convenient to work with the set of quantities Y ... , {Yi .. - Y .. }, 
{Y.j . - Y .. J, {Yi j. - y, .. - Y.j. + Y .. }, and {Yijk - YijJ. We have 
Y. =()+r ic +1 +e., 
Yi.. - Y 
= (ri - r) + (Ii. -
t J + (e, 
-
e . J, 
Y j 
-
Y . = (C j -
c) + (t.j -
t J + (e j . -
e J, 
(6.2.3a) 
Yij - Yi . - Y.j. + Y ... = (Iij -
'i. - tj + tJ + (eij . -
ei .. -
e j . + e .J, 
It follows from repeated use of Theorem 5.2.1 (on page 250) and Theorem 6.2.1 
(on page 329) that the grand mean Y . . and the four mean squares (111" l11e, 111" me) 
in Table 6.2.1 are jointly sufficient for (0, a;, a;" a;,,, a;,J and are independently 
distributed as 
(6.2.3b) 
The likelihood function is, therefore, 
{ 1!·lJK(0-y .. J2 
, "eme 
",m,
. v,m, . Vel11el} 
X exp -
-
2 
2 
2 T -2- + -2- T -2-
T -2 -
. 
2 aelr + ae'e - a., 
a c 
a., 
aerr 
ae,e 
(6.2.4) 
The likelihood can alternatively be thought of as arising from one observation 
from a Normal distribution with mean 0 and variance (a;" + a;,e - a;,)j(1JK), 

332 
Analysis of Cross Classification Designs 
6.2 
together with (ve> v" v" vc) further independent observations drawn from Normal 
distri butions with zero means and variances (u;, U,2, u;'r> u;,J, respectively. The 
model supplies the additional information that the parameters (u;, u;" u;'r' u;,c) 
are subject to the constraint 
(6.2.5) 
6.2.2 Posterior Distribution of (u;, U,2, u;, u~) 
Adopting the argument in Section 1.3 and treating the location parameter () 
separately from the variances (u;, r;;" u;". u;,c)' we employ the noninformative 
reference prior distribution 
(6.2.6) 
subject to the constraint C. Upon integrating out 0, the joint posterior distribu-
tion of (u;, u;" u;", u;,c) is 
(6.2.7) 
where 
f 1. 2 
V 111 
1.
2 
v m 
1.
2 
v m \ 
-
1 
p.* (C I) 
p 
Ve 
e 
e 
e"r 
, 
r 
r 
eVe 
C 
C 
W 
= 
I 
Y = 
r \ -2 > --, -2 <... --, -2 < --J' 
XV, 
V,111, 
XV, 
vim, 
Xv, 
v,m, 
and (X~c' l" X;" x;J are independently distributed i variables with (ve, Vn v" vJ 
degrees of freedom, respectively. 
It follows that the joint distribution of the 
variance components (u;, U,2, u;, r;;) is 

6.2 
Cross Classification Random Effect \'Iodel 
333 
As might be expected, this distribution is similar to the distribution in (5.3.11) 
for the three-component model. The scaled X- 2 approximation techniques de-
veloped in Sections 5.2.6 and 5.2.12 can now be employed to obtain marginal 
. distribution of certain subsets of the components. 
6.2.3 Distribution of (cr;, cr,2, cr;) 
To iJlustrate, suppose we wish to make inferences jointly about (cr;, cr,2, cr;). The 
corresponding posterior distribution can, of course, be obtained by integrating 
(6.2.8) over cr~. Alternatively, we may use (6.2.7) and first obtain the distribution 
of (cr; , cr;" cr;,r) by integrating out cr;,c yielding 
( 6.2.9) 
where 
and 
Wi = w Pr {F v v < me} . 
c . 
t 
m, 
We see that the function G(cr;,) is in precisely the same form as the posterior 
distribution of the variance cr ~ for the one-way model in (5.2.26). It follows by 
using the technique developed in Section 5.2.6 that G( cr:,) is approximately 
where 
. 
( 
cr
l 
) 
2 
I 
I 
-
t 
> -
2 
el 
G(cre,) == (v,m,) 
p X\: 
= -,-, 
' 
, 
'V,m, 
J 
vIm, 
ml =--, ' 
atv, 
(6.2.10) 
and, as before, Ix(p, q) is the incomplete beta function. To this degree of approxi-
mation, the joint distribution of (cr; , cr,l , cr;) is 
2 2 2. 
_I (-2 
cr; ) (v;m;) - I (-2 _ cr; + Kcr;) 
p(cre , cr" 
crr I y) = WI (Veme) 
P Xv. 
= --
-K 
P X\: 
-
' 
I' 
Verne 
1 
l 
vrmt 
X (vrmr) -I (-2 = cr; + Kcr,2 + JKcr;) 
JK 
p Xv, 
' 
v,m, 
cr; > 0, Cl,2 > 0, 
(J; > 0, 
(6.2. 11) 

334 
Analysis of Cross Classification Designs 
6.2 
where 
2 
_ 1 
P {XV. 
v.me 
WI 
= 
r -2- > -,-, , 
Xv; 
v, m, 
which is exactly the same form as the distribution in (5.3.11) and can thus be 
analyzed as before. 
It is clear that an analogous argument can be used to derive the joint distribu-
tion of (a;, a,2, a;) if desired. 
6.2.4 Distribution of (a;, a;) 
The derivation given above allows us to make inferences about the individual 
components (a;, a,2, a;, a;) and joint inferences about the sets (a;, (J,2, a;) and 
(a;, (J,2, a;). In cross classification designs, we are often interested mainly in the 
"main effect" variances «(J;, (J;). The corresponding joint posterior distribution 
is, of course, given by integrating (6.2.8) over a; and (J,2, 
(6.2.12) 
It does not seem possible to express the exact distribution in terms of simple 
functions and direct calculation of the distribution would require numerical 
evaluation of a double integral for every pair of values «(J;, (J;). 
We therefore 
introduce an approximation method which reduces (6.2. t 2) to a one-dimensional 
integral. First, we obtain the distribution of (a;" (J;'r' (J;,c) from (6.2.7), 
(6.2.13) 
where 
and 
" _ 
{ 
m'l 
W 
-
w Pr F v,.v. < meJ . 
The function H«(J;,) is in the same form as the distribution of O'L in (5.2.57) for 
the two-component model. It follows from the method in (5.2.60) that 
{ 
a2 
I 
2 
. 
'f 
II -1 
.. - 2 _ 
ct 
\ 
H(ue,) = (v, m,) 
p 
X,." - -"- ,, J 
t 
v, mt 
(6.2.14) 

-
-
. 
. 
_. 
_- _ 
_ "'~L -~-..., 
, 
-
6.2 
Cross Classification Random Effect Model 
335 
where 
" 
V, J X,(}V, + I, -lYe) 
v = -
-=------'--
I 
a2 
I.",(1V" Jve) 
II 
vim, 
m =--
, 
" ' 
a2v, 
and 
Hence, 
where 
so that 
u; > 0, 
u~ > O. 
(6.2.15) 
Calculation of the approximating distribution in (6.2.15) thus involves computing 
only one dimensional integrals, a task which is considerably simpler than exact 
evaluation of (6.2.12). 
6.2.5 An Illustrative Example 
To illustrate the analysis presented in this section, Table 6.2.2 gives the results 
for a randomized experiment. The data consists of 162 observations representing 
mileages per gallon of gasoline for 9 drivers on 9 cars with each driver making 
two runs with each car (l = J = 9, K = 2). 
We shall analyze the example by 
supposing that the underlying Normality and independence assumptions for 
the model (6.2.1) are appropriate and shall adopt the noninformative reference 
prior in (6.2.6) for making inferences. 

Table 6.2.2 
Gas mileage for 9 drivers driving 9 cars (duplicate runs) 
Drivers 
<..> 
c.. 
Cars 
2 
3 
4 
5 
6 
7 
8 
9 
'" 
32.431 
26.111 
29.719 
31.915 
34.582 
28.712 
31.518 
26.513 
29.573 
31.709 
26.941 
29.218 
32.183 
36.490 
27.091 
30.448 
28.440 
29.464 
> 
2 
26.356 
22.652 
25.966 
27.856 
31.090 
25.956 
23.375 
25.329 
28.648 
::J 
I" 
26.225 
22.139 
26.835 
26.241 
31.320 
23.653 
25.298 
24.098 
27.136 
.;; 
'" 
3 
30.243 
25.218 
27.682 
28.912 
33.821 
29.394 
28.713 
26.005 
31.174 
[;;. 
31.785 
27.189 
27.521 
28.059 
34.462 
27.859 
29.302 
27.020 
28.003 
0 .... 
4 
29.830 
25.192 
25.962 
28.717 
34.619 
27.663 
27.511 
26.145 
26.834 
rJ 
... 
29.859 
25.081 
28.715 
29.783 
34.653 
25.516 
30.906 
23.299 
29.549 
0 
5 
33.464 
24.631 
29.567 
27.140 
34.553 
27.746 
31.371 
27.290 
33.239 
~ 
rJ 
30.307 
25.930 
28.368 
30.818 
35.337 
26.210 
21.495 
24.689 
32.319 
., 
6 
28.313 
21.809 
28.030 
28.447 
31.432 
26.551 
28.073 
24.575 
28.026 
'" '" 
27.998 
23.144 
28.234 
27.670 
32.203 
24.538 
28.148 
23.999 
28.820 
5i 
7 
28.294 
22.236 
26.467 
25.716 
31.916 
25.028 
25.238 
21.607 
27.687 
~ 
o· 
27.363 
22.245 
27.115 
25.059 
31.541 
26.296 
25.083 
21.900 
29.357 
::J 
8 
29.864 
24.542 
27.103 
25.051 
32.282 
25.096 
27.655 
25.038 
26.414 
0 
27.363 
23.816 
24.817 
25.293 
31.295 
25.909 
26.207 
22.951 
26.256 
~ 
o'C' 
9 
27.438 
21.472 
25.108 
28.419 
31.241 
27.020 
26.676 
22.795 
27.638 
'" '" 
27.486 
23.130 
27.589 
25.941 
32.459 
24.445 
25.738 
23.299 
27.385 
The averages for each driver on each car as well as the driver means, car means, and grand mean are given in Table 6.2.3. The breakdown of the 
sum of squares and other relevant quantities for the analysis are summarized in Table 6.2.4 in the form of an analysis of variance table. 
T3.ble 6.2.3 
Average mileage for 9 drivers on 9 cars 
Drivers 
Row 
Cars 
1 
2 
3 
4 
5 
6 
7 
8 
9 
means 
1 
32.0700 
26.5260 
29.4685 
32.0490 
35.5360 
27.9015 
30.9830 
27.4765 
29.5185 
30.1699 
2 
26.2905 
22.3955 
26.4005 
27.0485 
31.2050 
24.8045 
24.3365 
24.7135 
27.8920 
26. J 207 
3 
31.0140 
26.2035 
27.6015 
28.4855 
34.14J5 
28.6265 
29.0075 
26.5125 
29.5885 
29.0201 
4 
29.8445 
25.1365 
27.331>5 
29.2500 
34.6360 
26.5895 
29.2085 
24.7220 
28.1915 
28.3241 
5 
31.8855 
25.2805 
28.9675 
28.9790 
34.9450 
26.9780 
30.4330 
25.9895 
32.7790 
29.5819 
6 
28.1555 
22.4765 
28.1320 
28.0585 
31.8175 
25.5445 
28.1105 
24.2870 
28.4230 
27.2228 
7 
27.8285 
22.2405 
26.7910 
25.3875 
31 .7285 
25.6620 
25.1605 
21.7535 
28.5220 
26.1193 
8 
28.6135 
24.1790 
25.9600 
25.1720 
31.7885 
25.5025 
26.9310 
23.9945 
26.3350 
26.4973 
9 
27.4620 
22.3010 
26.3485 
27.1800 
31.8500 
25.7325 
26.2070 
23.0470-
27.5115 
26.4044 
c;-, 
Column 
29.2404 
24.0821 
27.4453 
27.9567 
33.0720 
26.3713 
27.8197 
24.7218 
28.7512 
27.7178 
... 
means 
(Grand mean) 
-

6.2 
Source 
Grand mean 
Main effect 
cars 
Main effect 
drivers 
Interaction 
Residuals 
8"; = 1.1759, 
Cross Classification Random Effect Model 
337 
Table 6.2.4 
Analysis of variance of 9 drivers on 9 cars 
S.S. 
d.f. 
M.S. 
E.M.S. 
162(8 - 27.7178)2 
362.0985 
Vr = 
8 
mr = 45.2623 
6;{y = 6; + 26,2 + 186; 
1,011.6550 
v = 
c 
8 me = 126.4569 
6;,c = 6; + 2o} + 186; 
109.6123 
v,=64 
m = 
, 
1.7127 
6;, = 6; + 26,2 
95.2460 
ve = 81 
me= 
1.1759 
6 2 
e 
8",2 = 0.2684, 
8"; = 6.9302, 
8"; =2.4194 
In examples of this kind, our main interests usually center on the "main effect" 
variances (6;, 6;) and the interaction variance 6,2 However it seems clear from 
Table 6.2.4 [and can be readily confirmed by a fuller Bayesian analysis using, 
for example, (6.2.11)J that, for this example, 6,2 is small compared with 6; and 
6; . 
If we wish to make inferences about the "main effect" variances (6;,6;) we 
may use the approximation (6.2.14) to eliminate 6; yielding 
109.6123 
0 3 
X2 = 204.8583 = 
.5 51, 
I x2(34,40.5) 
Ix2(33, 40.5) 
a2 = 33 x 
-
32 x 
Ix ,(33,40.5) 
Ix2(32 ,40.5) , 
= 33(0.98161) -
32(0.98461) = 0.88561, 
v;' = O.8:~61 (0.98461) = 71.1544, 
109.6123 
m" = 
= 1.7395 
t 
(0.8856 1)(71.1544) 
, 
and 
v;'m;' = 123.7704. 

338 
Analysis of Cross Classification Designs 
6.2 
From (6.2.15), the approximate posterior distribution of (a;, a;) is then 
-2 
aet 
d2 
(
2) 
x P X71.1544 = 123.7704 
aet · 
Figure 6.2.1 shows four contours of the distribution calculated by numerical 
integration together with the mode (a;o' (J;o) which is at approximately (I.92, 5.52). 
20.0 
-
10.0 
o 
2.0 
4.0 
6.0 
a; (Cars) 
Fig.6.2.1 Contours of posterior distribution of «(J~, (J~): the car-driver data. 
The levels of the contours are respectively 50, 25, 10 and 5 per cent of the density 
at the mode. Using the asymptotic Normal approximation and since the X2 dis-
tribution with two degrees of freedom is an exponential, these contours are very 
roughly the boundaries of the 50, 75, 90 and 95 per cent H. P.D. regions. It will 
be seen that for this example both the main effect components are substantial and 
that they appear to be approximately independently distributed. 

6.2 
Cross Classification Random Effect Model 
339 
6.2.6 A Simple Approximation to the Distribution of (a;, a;) 
When , as in this example, the modified degrees of freedom v;' is large, the last 
factor in the integrand of (6.2.15) will be sharply concentrated about 111;'. We 
may adopt an argument similar to the one leading to the modal approximation 
in (5.2.43) to write 
To this degree of approximation, (J; and a; are independent, each distributed 
like a truncated inverted '/ variable. More specifically, 
m ;' + JKa; 
,.v X:;. 2 
truncated from below at 
m;' 
v,m, 
v,m, 
and 
(6.2.17) 
m;' +IK(J; 
,.v X-:c2 
truncated from below at 
m;' 
Veme 
Verne 
20.0 
10.0 
o 
2.0 
4.0 
6.0 
0; (Cars) 
Fig. 6.2.2 Contours of the approximate posterior distribution of ((J~ . (J~) : the car-driver 
data. 

340 
Analysis of Cross Classification Designs 
6.2 
Returning to our example, the posterior distribution of the variances (0';, a~) 
using the present approximation is 
2 2 
(-2 
1.7395 + 180';) (-2 
1.7395 + 18a~) . 
p(a" ac I y) ex: P X8 
= 
362.0985 
P X8 
= 
1,011.6550 
Figure 6.2.2 gives four contours corresponding to the 50, 75, 90 and 95 per cent 
H.P.D. regions, together with the mode (8';, a~). 
They are very close tc{ the 
contours in Fig. 6.2.1. In fact, by overlaying the two figures one finds that they 
almost completely coincide, showing that for this example (6.2.16) is an excellent 
approximation to (6.2.15). 
pro; y) 
0.40 
Cars 
0 30 
020 
0.10 
0; -? 
0 
10 
15 
Fig. 6.2.3 Approximate posterior 
distribution of a;: the car-driver 
data. 
pro: y) 
0.40 
0.30 
0.20 
0.10 
0 
Drivers 
L.-L-_L-__ L-__ 
L--==-t=~_..Lo2 -, 
25 c 
10 
15 
20 
Fig. 6.2.4 Approximate posterior distribution 
of a~: the car-driver data. 
Using (6.2.17), individual inferences about a; can be made by referring the 
quantity (1.7395 + 18a;)/362.0985 to an inverted X2 with 8 degrees of freedom 
truncated from below at 0.0048 or equivalently 362.0985/(1.7395 + 180';) to a 
X2 with 8 degrees of freedom truncated from above at 208.33. ·Similarly, inferences 
about a~ can be made by referring (1.7395 + 18a~).1,101.655 to xi 2 truncated 
from below at 0.0016 or 1,101.655/(1.7395 + 18a~) to X~ truncated from above at 
633.3. 
In both cases, the effect of truncation is negligible. 
The approximate 
posterior distributions of a; and a~ are shown respectively in Figs 6.2.3 and 6.2.4. 

6.3 
The Additi\'c .:Vlixed Model 
341 
6.3 THE ADDITIVE MIXED MODEL 
We consider in this section the model 
i=I, ... ,I; J=I, ... ,J 
(6.3.1) 
which is the additive mixed model in (6.l.16) with k = 1. 
The random effects 
cj and the errors eij are assumed to independently follow Normal distributions 
with zero means and variances 0'; and 0'; , respectively. In terms of the previous 
example we would have I particular cars of interest tested on a random sample 
of J drivers. The assumption of additivity implies that the expected increment of 
performance cj associated with a particular driver J is the same whichever of the 
I cars he drives. Though restrictive this assumption is adequate in some contexts. 
Suppose it was appropriate in the case of the car rental agency interested in the 
mean performances 8i of I = 6 cars tested by a random sample of drivers. Then 
the variance component 0',2 would measure the variation ascribable to the differences 
in drivers, and 0'; would measure the experimental error variation. 
6.3.1 The Likelihood Function 
For the likelihood function, it is convenient to transform the I J observations 
into {Yi.}, {Y.j - y,} and {Yij -
Y; -
Y.j + y}. We have 
Y.j - Y .. = cj -
c + e. j 
- e., 
(6.3.2) 
Yij -
Yi. - Y.j + Y .. = eij - ei . -
e j + e 
Using Theorem 5.2. 1 (on page 250) and Theorem 6.2.1 (on page 329), we may 
conclude that: 
l. the three sets of variables {Yi,}, {Y.j - y,}, and {Yij -
Y; -
Y j + y} are 
independent one of another; 
2. so far as the set {Y .j - Y J is concerned, the sum of squares I I (Yj -
Y r 
is a sufficient statistic for CO'; + 10';) and is distributed as (0'; + 100;)X~J-l ); 
3. so far as the set {Yij - Yi. - Y.j + Y. J is concerned, the sum of squares 
I I (Yij - Yi - Y.j + Y. Y is a sufficient statistic for 0'; and is distri buted 
as h;X(I-I)(J-l) ; and 
4. the vector of variables y: = (YI., ... , Y1 J is distributed as the I-variate Normal 
N 1(8, V) where 
and 
(6.3.3) 
Noting that 

342 
Analysis of Cross Classification Designs 
6.3 
the likelihood function is 
1(0.0";. 0"; I y) ex:. (0";)-[JU- I )f2] CO"; + [0";)-J/2 
{ I [1 I (y . - y)2 
I I (y .. -
y. -
y . + Y )2 
J} 
xexp -_
. 
2') 
~. + 
') 
"2 
.) 
. . 
+Q(O) 
, 
2 
0". + /O"c 
O"e 
• 
(6.3.4) 
where 
Q(9) = (y - 9)'V- I (y. - 9). 
In the analysis of additive mixed models, interest is usually centered on the com-
parative values of the location parameters 8j rather than 8j themselves. We may, 
therefore, work with 
i = I, .. . ,1; 
Since 
v-I=J -2[1_ 
a~ 
11'] 
O"e 
2 + 1 2 
1 I 
ae 
O"c 
= JO";2(I-+IJ1;.) + j/-I(O"; +i(JD- I l l l;, 
we may express the quadratic form Q(O) as 
(6.3.5) 
(6.3.6) 
The various sums of squares appearing in the likelihood function can be 
conveniently arranged in Table 6.3.1 in the form of an analysis of variance. 
In Table 6.3.1 , «>' = (<PI, ",,<P1-1) and note that <p] = -
(<PI + ... + <PI-I)' 
We can express the likelihood function in (6.3.4) as 
(6.3.8) 
where 
and 
It is clear from the above expression that I[ (e. (J;e I y) can be regarded as the 
likelihood function of a sample consisting of one observation y. drawn from a 
Normal distribution N(B. O";e/lJ) and Vc independent observations from a Normal 
population N(O, O"~e) ' 
Similarly 12(<(>, (J; I y) can be taken as that of a sample of 

Table 6.3.1 
Analysis of variance for the additive mixed model 
Source 
S.S. 
d.r. 
Grand mean 
IJ({j-y/ 
Fixed effect 
S(Q»=J~[¢i-(Yi _yJJ2 
" <1> =(1-1) 
(cars) 
Random effect 
S =l~ ( y ._y )2 
Ve=(J-l) 
(drivers) 
C 
.J 
. . 
Error 
S =~~( y. _ y _y+y )2 
e 
I ) 
I . 
.J 
. ' 
v. =(I-I)(1-I) 
M.S. 
IJ(rJ-yj 
m(Q» = S(Q»/v<I> 
me = Se/ve 
Ille = Sp/Ve 
E.M .S. 
(J;e=(J; + 1(J; 
(J; 
(J ~C' 
(J 2 e 
"" 
~ 
>-l 
:r .. 
>-
0. 
~ 
:; . .. 
~ 
,. .. 
0. 
~ 
o 
Co 
~ 
~ 
W 

344 
Analysis of Cross Classification Oesigns 
6.3 
one observation from a (I - 1) dimensional Normal distribution with means 
(cPl. ··"cPT - I) and covariance matrix (O";, J)[I(/_l) - (l/l)lu -J)l;JiI)J and of Vc 
further observations from a Normal population N(O, a;). Since a;. = a; + I a;, 
the parameters (o';e, a;) are thus subject to the constraint 
( 6.3.9) 
As before, it is convenient to proceed first ignoring the constraint C. 
The 
posterior distribution of the various parameters appropriate to the model (6.3.1) 
are then conveniently derived from the unconstrained distributions using the 
result (1.5.3). 
6.3.2 Posterior Distributions of (e, q" a;, O";e) Ignoring the Constraint C 
With the noninformative reference prior distribution+ 
the unconstrained posterior distribution of (e, q" a;, o~e) is 
p*(8,q" a;, O"~e I y) cc (0";) - Hy.,-t v .. )-I (O"~e)- -}(Yc - I)-l 
(6.3.10) 
-
00 < e < oc, 
-00 < cPi < 00, .i = I, ... , (1- J), 0"; > 0, 0";. > 0, 
(6.3.11) 
from which for the unconstrained situation we deduce the following solutions 
which parallel known sampling results, 
Given 0";, the conditional posterior distri bution of q, is the (J - I) 
dimensional multivariate Normal 
N(I - 1)( cf>, 0"; E) 
and 
with 
:j,' = (YI. - Y .. , ·"'YU-l) . - yJ 
In particular, given 0";, the conditional distribution of the differ-
ence b = ei - e 
i = cPi -
cP i between two particular means of 
interest is Normal ,V(Yi . - Yi,' 20";/J). 
The marginal posterior distribution of q, is the (I -
1) dimen-
sional multivariate t distri bution t(l _ 1)( cf>, me E,. ve)· 
(6.3.1 La) 
(6.3.l2b) 
(6.3.13a) 
When the number of means I is large, the assumption that the 8's are locally uniform 
can be an inadequate approximation, and a more appropriate analysis may follow the 
lines given later in Section 7.2. 

6.3 
The Addith'e Mixed Model 
345 
Hence the difference b is distributed as ((Yi. -
y;., 2Iilo/J, Ve so 
that the quantity r = [6 -
(Yi - y;)] (2meiJ)1 ' 2 is distributed as 
teO, I, vel. 
(6.3.13b) 
Also, the quantity V = m(<I»/m" is distributed as F with v,p and 
Ve degrees of freedom. 
The quantity (J; lvom,. has the X,:.2 distribution. 
Given <1>, the conditional posterior distribution of (J; is such that 
(J; /[veme + v¢m(<I»] has the X- 2 distribution with v" + v,p degrees 
of freedom. 
It follows in particular that given b, (J; [m,,(v e + r2)] is distri-
buted as X,~~ I' 
The quantity (J;,) (v cl17c ) has the X- 2 distribution with Ve degrees 
of freedom. 
The ratio mc(J; /(me(J;e) has the F distribution with Vc and I',. 
degrees of freedom. 
Given <1>, the conditional distribution of the ratio (J;/(J;" is such 
tha t the qua n ti ty 
u; 
n1c 
(J ;0 [v oln e + v ,p~m-(--'-<I>-) ]-/-( v-e-+-v-",) 
is distributed as F with Ve and v,, + v,p degrees of freedom. 
In particular, if just c5 is given, the quanti.t)' 
(J; 
l11e 
(J:e me(v" + r 2)/(ve + I) 
follows the F distribution with Vc and Ve + I degrees of freedom. 
6.3.3 Posterior Distribution of u; and u; 
(6.3.14) 
( 6.3.15) 
(6,3,16a) 
(6.3.16b) 
(6.3.17) 
(6.3.18) 
(6.3.19a) 
(6.3.19b) 
The unconstrained posterior distributions given above are, of course, not them-
selves the appropriate distributions for the model (6.3. 1), but provide a useful 
stepping stone from which they may be reached. When interest centers on the 
variance components ((J;, (J;), we may first employ the result (1.5.3) to obtain 
from (6.3.15) and (6.3.17) the distribution of ((J;, (J;J, 
p((J;, (J:e I y) = 
(v.I11,,) -1 P (Xv,2 =~) (Veme) 1 P (X:c 2 = (J;e) 
v (!m(J 
Verne 
Pr* {C I (J;, (J; .. y} 
x 
Pr*{ ely} 
(6.3.20a) 

346 
Analysis of Cross Classification Designs 
6.3 
Using (6.3 .18), the overall probability of the constraint C: 0'; < O';e is 
(6.3.20b) 
In addition. given 0'; and 0';., Pr*{C 10';,0':., y} is clearly unity for 0'; < a:. and 
zero otherwise. Thus, the posterior distribution of (0';,0';) is 
0'; > 0, 0'; > O. 
(6.3.21) 
The reader will note that if we setl = K , VI = \Ie. 1'2 = \Ie> nil = me' and 111 2 = me, 
the distribution in (6.3.21) is precisely the same as that obtained in (5.2.14). Infer-
ences about (0';, 0';) can thus be made using the results obtained earlier. 
6.3.4 Inferences About Fixed Effects for the Mixed Model 
In many problems, the items of principal interest are the comparisons <Pi = (Ji -
D. 
For instance, for the rental car agency the aim of an analysis would most likely 
be to determine the relative performance of the cars. 
We would then have an 
example of the analysis of a randomized block design using a mixed model with 
0'; a variance component associated with blocks (drivers), and 0'; a component 
due to error, and with the <Pi representing comparisons of the treatments (cars). 
6.3.5 Comparison of Two 'Vleans 
Consider the problem of comparing two particular means, say those for the ith 
and the dh treatment. To make inferences about 6 = 0i - 0; it is convenient to , 
work with the quantity 
1(
2111 ) 1/ 2 
, = [D -
(Y;, - Yi)] 
--; 
(6.3.22) 
From the unconstrained distribution of, in (6.3.13b), we again employ the result · 
(1.5.3) to obtain the posterior distribution of, as 
Pr*{CI " y} 
peT I y) = p(tv .. = ,) Pr*{C I y} , 
- (f)< , <x, 
(6.3.23) 
where Pr*{ ely} is given in (6.3.20b). From (6.3. I 91;J), the conditional posterior 
probability of C, given T, is 
{ 
} 
{ 
111 c( \I e --i- 1) ) 
Pr* CIT, Y = Pr F vc• v. '; I < ---2 
J 
m,(ve + ,) 
(6.3.24) 

6.3 
The Additive Mixed Model 
347 
The posterior distribution of -r is, therefore, 
p(-r I y) = p(ty. = -r) g (r21 ::) , 
-w < -r < w, 
(6.3.25) 
a Student's t distribution with Ve degrees of freedom modified by a factor 
g(-r21 mclme) which is the ratio of two F probability integrals 
f 
me(V,, +I)( 
r2)-I} 
Pr \ F" , . .. I < -
--
1 + -
c. c 
In(' 
Ve 
Ve 
(6,3.26) 
We now discuss some properties of this modified t distribution. 
Since both 
p(t., = r) and g(r21 m J m e) are functions of r2, the distribution is symmetric about 
the origin. 1 t follows that all odd moments when they exist are zero. Using the 
identity (A5.2, I) in Appendix A5,2, the 2rth (r < v,/2) moment ofT is 
2 
B[(t ve) -
r, r] Pr{F •• - 2r < (me/me) (v.-2r)lv,} 
£( -r r I y) = 
v~ 
I 
_-'-----'-,:-' ,,-e --"-'-_---'----=-----'-'-_c _
_ 
---'-'-
Bh , r) 
Pr {F.c•v• < me/me} 
(6,3.27) 
where, as before, B(p, q) is the complete beta function. The function g(r21 me/me) 
is monotonically decreasing in -r2. 
This implies that the distribution per I y) is 
uniformly more concentrated about the origin than p(i,," = -r). 
That is, for an 
arbitrary constant d > 0, 
Pi {Irl < d I y} ~ Pr {II,., I < d}. 
(6.3.2B) 
The above result is indeed a sensible one. For, the distribution p(-r I y) is obtained 
from the distribution p(t., =.r) by imposing the constraint C : u; < a;,. We see 
in (6.3.12b) that a;, if known, is proportional to the conditional variance of o. 
Thus, it is not at all surprising that the posterior distribution of r with the con-
straint C is more concentrated about the origin than the distribution of -r without 
the constraint. We have here in fact a further example of Bayesian pooling which 
is discussed in more detail in Section 6.3.7. 
An Example 
In the particular case J = 2, the randomized block arrangement results in pairs 
of observations. On standard sampling theory, the comparison of the two means 
is usually ca,ried out by considering the differences Zj = Y2j - YIj(J = I, .. , J) 
between the pairs and making use of the fact that the sampling distribution of 
t = [0 -
(Y2 . - YI J] 
(2m,iJ)1/2 
(6.3.29) 

348 
Analysis of Cross Classification Designs 
6.3 
Table 6.3.2 
Measurement of percentage of ammonia by two anal:/'sts 
Days (blocks) 
2 
3 
4 
5 
6 
7 
8 
Analyst I 
37 
35 
43 
34 
36 
48 
33 
33 
(% Ammonia- 12) x 100 
Analyst 2 
37 
38 
36 
47 
48 
57 
28 
42 
where 2 = )-1 LZj , is the teO, I, veJ distribution. In this analysis, however, any 
"inter-block information" provided by me about (J; is not taken account of. 
We consider this "paired (" problem in some detail using for illustration an 
example quoted in Davies (1967, p. 57). The data shown in Table 6.3.2 are deter-
minations made by two analysts of the percentage of ammonia in a plant gas 
made on 'eight different days. The primary purpose of the experiment was to make 
inferences about a possible systematic mean difference D, that is, a bias between 
results from the two analysts. The example illustrates the use of the additive 
mixed model (6.3 .1) with J = 2, ) = 8. It should be borne in mind that we assume 
that (i) day to day variations of the true ammonia percentage follow a Normal 
distribution having variance (J;, (ii) the eight particular daily samples considered 
may be regarded as a random sample from this distribution , (iii) there is no 
-
interaction between analysts and samples, and (iv) the Normally distributed analy-
I 
tical error has the same variance (J; for both analysts. The sample quantities 
needed in the analysis are given in Table 6.3.3 in the form of an analysis of 
variance. 
Trible 6.3.3 
Analysis of variance for t he analyst data 
Source 
Fixed effect 
(bias between analysts) 
5.5. 
S(c'l)= ~ J(D - 2)2 
.=4(D -4.25/ 
Random block effect (days) 
Se= 553.0 
Sr=206.75 
Error 
Average for analyst 1 
Average for analyst 2 
me/me =2.67 
d.f. 
ve=7 
YI . = 37.38 
12 . =41.63 
M.S. 
4(c'l -4.25)2 
l11e=79.0 
111.=29.54 

6.3 
The Additive Mixed Model 
349 
The solid curve in Fig. 6.3. I is the posterior distribution of the bias b = 82 -8 1 
calculated from expression (6.3.25), corresponding to a noninformative reference 
prior. This distribution is seen to be more concentrated about its mean, Z = )'2 . -
)'1. = 4.25, than the unconstrained posterior distribution of b shown by the dotted 
curve in the same figure. This latter curve (which is also the confidence distribu-
tion of D) is a scaled t distribution centered at the same mean with 7 degrees of 
freedom . In particular, an H.P, D. interval for D is shown with content 91.2 %. 
This would have the somewhat smaller content 90.0% if the unconstrained distri-
bution were used. The broken curve shown in the figure is the posterior distri-
bution of D obtained by taking (J: = 0 in (6.3.1), the implication of which will be 
discussed later in this section, 
0.10 
0.05 
91.2\\ H.P.D. interval 
0.0 
5.0 
100 
posterior distribution of 0, (Bayesian "partial pooling") 
- - - - - - - -
unconstrained posterior distribution of 0, and confidence distribution given by 
" paired I ." (No pooling) 
-
-
-
-
posterior distribution of 0 obtained by laking (J~ , 0. and confidence distribution 
given'by " unpaired t," (Complete pooling) 
Note that Bayesian " partial pooling" yields the sharp~st distribution, 
Fig. 6.3.1 
Posterior distribution of the analytical bias D. 
Effect o/me/me on the Distriburion ofb 
In the example considered, the posterior distribution of the bias D coming from the 
Bayesian analysis is not very different from the unconstrained distribution which 
parallels the sampling results. This is because the ratio mJ me = 2.67 is fairly 
large so that the influence of the factor g(r2 I me/me) is relatively mild . However, 
when me 'me is close to or less than unity, the effect of g(r2 I me/me) will be much 
greater. 
For illustration, suppose that in the analyst example the pair (48,57) is 
excluded. 
One then obtains the analysis of variance presented in Table 6.3.4. 

350 
Analysis of Cross Classification Designs 
6.3 
0.10 
0.05 
L-______ -L ______ -L ______ W-______ 
~ 
______ 
~ 
___ 
o~ 
-2.0 
1.0 
4.0 
7.0 
10.0 
posterior distribution of b. (Bayesian "partial pooling") 
- - - . - - - -
unconstrained posterior distribution of cl, and confidence distribution given by 
"paired I." (No pooling) 
-
-
-
-
posterior distribution of 0 obtained by taking 0"; = 0, and confidence distribution 
given by "unpaired I." (Complete pooling) 
Note that Bayesian "partial pooling" yields the sharpest distribution. 
Fig. 6.3.2 Posterior distribution of the analytical bias b for data excluding the pair (48,57). 
Here the ratio me/me = 0.86 is less than unity and, as shown in Fig. 6.3.2, the 
posterior distribution of b is markedly different from the unconstrained distribu-
tion. It will be seen later in Section 6.3.7 that g(r2 I mcfme), in fact, reflects the 
effect of pooling the variance estimates me and m e in the estimation of 0";. 
Table 6.3.4 
Analysis of variance for the analysts data excluding the pair (48, 57) 
Source 
Fixed effect 
(bias between analysts) 
Random block effect (days) 
Error 
S.S. 
S(b) = P(b - 2)2 
= 3.5(b - 3.57)2 
Se=166.71 
S. = 193.86 
)'\ . = 35.86, 
me/me = 
0.86 
d.f. 
M.S. 
3.5(b-3.57)2 
ve=6 
me=27.79 
ve=6 
me=32.31 
Y2. =39.43 
For given T, the modifying factor g(T2
1 mel me) is a function of mcfme. It is 
instructive to consider the behavior of per I y) in ' the extreme cases mclme --> 00 and 
me/me"" O. 
When me/me--> oo,g(T21 me/me) approaches unity for any fixed T, so that 

6.3 
The Additive Mixed Model 
351 
the distribution per I y) tends in the limit to the unconstrained distribution p(lv = r). 
On the other hand, when me/me -> 0, applying L'Hospital's rule, one finds that 
(6.3.30) 
Hence in thelim it the distribution of the random variable [1/(/- 1 )J1/2 r approaches the 
/(0,1, ve + ve) distribution. 
Further, it can be verified that the mixed derivative 
IS 
where 
if log g(r2 I me/me) 
~iO(mclme) 
(6.3.31) 
and N(me,'me) is a non-negative function af me. l11e' Clearly H(O) = 0 and upon 
differentiating we find that H'(a) > O. Thus 
2
2 10g g(r21111c1me) 
~ O 
Or 2 o(me/me) 
(6.3.32) 
for all values of me/me and r2. This implies that the posterior distribution of r2 has the 
"monotone likelihood ratio" property .. see e.g. Lehman (1959). 
It follows that for 
d> 0, the probability Pr{lrl < d I y} is a monotonically decreasing function of 
me/me' This, together with (6.3.28) and (6.3.30), shows that, in obvious notation 
(6.3.33) 
which provides an upper and a lower bound for the probability Pr{lrl < d I y}. 
6.3.6 Approximations to the Distribution pCr I Y) 
In this section, we discuss certain methods which can be used to evaluate the 
posterior distribution of r. 

352 
Analysis of Cross Classification Designs 
6.3 
When ve = (J -
1) is a positive even integer, we can use the identity (A5.2.3) in . 
Appendix A5.2 to expand the numerator of g(T2 I me/me) into 
where 
x=-----
Substituting (6.3.34) into (6.3.25), we obtain 
where 
It follows that for d > 0, 
~-Vc- 1 
L W/YjP(tv. ,C 2j = )'/r), 
j=O 
and 
tvc- 1 
-
00 < T < CO, 
L Wj Pr {Jfvc + 2) < )'jd} 
j = O 
(6.3.34) 
(6.3.35) 
(6.3.36) 
which can be used to calculate probabilities to any desired degree of accuracy using a 
standard f table in conjunction with an incomrlete beta function table. When "e is large,-
evaluation of the probability of T from (6.3.36) would be rather laborious and for odd 
values of "e' this formula is not applicable. When appropriate it can, however, be used 
to check the usefulness of simpler approximations. 
A Scaled t Approximation 
We now show that a scaled I distribution can provide a simple and overall satis-
factory approximation to the posterior distribution of T. This result is to be expec-
ted because we have seen that in the two extreme cases when me/me -> 00 and 
m)me ...... 0, T follows a scaled t distribution exactly. We can write (6.3.25) as 
p(rIY)= E p(TJa;,y) 
O" ~ IY 
- co < T < co. 
(6.3.37) 

6.3 
The Additive Mixed Model 
353 
From (6.3.12b), the unconstrained conditional distribution of T, given (J';. is Normal 
N(O , (J';.m,). Once (J'; is given, the constraint C: (J'; < (J'~e has no effect on the 
distribution of r. 
It follows that the Arst factor in the integrand, per I (J'; , y), is 
the Normal distribution N(O, u;/me)' Now from (6.3.21) the posterior distribution 
p( u; I y) in the integrand is 
Pr {x~c < vem)u;} 
Pr{F vc•v• < me/me} 
(6.3.38) 
This distribution is of exactly the same form as the distribution of ui in (5. 2.26) 
for the two-component random model effect. 
Thus, employing the method 
developed in Section 5.2.6, the distribution of u; can be closely approximated by 
that of a scaled X- 2 variable. Using the resulting approximation , we obtain 
where 
That is, the quantity 
b -
(Yi. - Yi) 
(2m:/J)l /2 
(6.3.39) 
and 
X= 
(6.3.40) 
is approximately distributed as 1(0, I, v ~ ) ,. or equivalently, b is approximately 
distributed as I(Yi. - y ,., 2m ~/J, v~). 
For the complete analysts data, we have 
(v~ = 8.57, m~ = 27.55). On the other hand, excluding the pair (48,57) one would 
get (v ~ = 10.48, m~ = 23.80). Tables 6. 3.5a and 6.3.5b give, respectively, for these 
two cases specimens of the posterior densities of (j obtained from exact evaluation 
of (6.3.25) and from the above approximation. The agreement is very close. 
6.3.7 Relationship to Some Sampling Theory Results and the Problem of Pooling 
The standard sampling theory "paired /" analysis depends on the fact that the 
sampling distribution of r = [b -
(Yi - y,)] /(2m,.J)l /1 is the 1(0, I, "e) distri-
bution. The resulting confidence distribution of b is numerically equivalent to 
the unconstrained posterior distribution of (j. 
Now E(me) = (J'; and E(mJ = 
2(J'; + u;. 
In obtaining this confidence distribution, therefore, one may feel 
intuitively that some information about the variance component (J'; is lost by not 
taking me into a~count because its sampling distribution also involves u;. What 
has sometimes been done is to use the ratio me/me to test the hypothesis (J'; = 0 

354 
Analysis of Cross Classification Designs 
TallIe 6.3.5 Comparison of exact and approximate density of b 
(a) Data of Table 6.3.2 
b 
4.25 
4.45 
4.65 
4.85 
5.05 
5.25 
5.45 
5.65 
5.85 
6.05 
6.25 
6.65 
7.05 
7.45 
7.85 
8.25 
8.65 
9.05 
9.45 
9.85 
10.25 
Exaci 
0.147830 
0.147353 
0.145932 
0.143600 
0.140411 
0.136437 
0.134182 
0.126489 
0.120718 
0.114559 
0.108120 
0.094818 
0.081560 
0.068940 
0.057370 
0.047087 
0.038185 
0.030645 
0.024376 
0.019244 
0.015097 
Approximate 
0.147659 
0.147181 
0.145759 
0.143425 
0.140232 
0.136253 
0.133995 
0.1 26292 
0.120512 
0.114343 
0.107894 
0.094571 
0.081299 
0.068675 
0.057115 
0.046858 
0.037995 
0.030505 
0.024292 
0.019216 
0.015122 
(b) Data of Table 6.3.2 excluding the pair (48,57) 
o 
Exact 
Approximate 
3.57 
0.149453 
0.149380 
3.77 
0.148973 
0.148900 
3.97 
0.147543 
0.147470 
4.17 
0.145197 
0.145122 
4.37 
0.141985 
0.141908 
4.57 
0.137977 
0.137898 
4.77 
0.133258 
0.133177 
4.97 
0.127925 
0.127839 
5.17 
0.122079 
0.121990 
5.37 
0.115830 
0.115736 
5.57 
5.97 
6.37 
6.77 
7.17 
7.57 
7.97 
8.37 
8.77 
9. 17 
9.57 
9.97 
0.109286 
0.095729 
0.082174 
0.069237 
0.057359 
0.046800 
0.037672 
0.029966 
0.023591 
0.018407 
0.014256 
0.01097J 
0.109187 
0.095621 
0.082060 
0.069124 
0.057253 
0.046711 
0.037604 
0.029924 
0.023576 
0.018419 
0.014290 
0.011024 
6.3 

6.3 
The Additive Mixed Model 
355 
and , in the absence of a significant result. to run an " unpaired /" analysis. That 
is, to employ the quantity (vern, + Verne) (Ve + vJ as a pooled estimator of 0"; 
with (ve + ve) degrees of freedom. In this case, inferences about (j are made by 
referring the quantity 
b -
(Yi. -y,) 
(6.3.4t) 
to the teO, t, Vc + ve) distribution. From the Bayesian point of view, this paralJels 
the case when 0"; is known to be equal to zero. To see this, suppose we now take 
the prior distribution of (8, 0";) to be, locally, 
(6.3.42) 
Then, on combining this with the likelihood in (6.3.4) conditional on 0"; = 0, 
it can be verified that a posteriori the quantity 'I follows the same teO, I, Ve + ve) 
distri bu tion. 
Note that 
D -
(Y2. - YI) 
T = {L(Zj -
Z)2j[J(J -
J)]}I /2 
and 
{L L (Yu -
Yi)2 /[J(J -
1)]}1 /2 
(6.3.43) 
where, as before in (6.3.29), Zj = .hj - Ylj and z = (1,J)Lz j . 
The problem is 
thus the familiar one of deciding whether an unpaired t test or a paired / test 
should be adopted in analyzing paired data wh en it is felt that there might or 
might not be variation from pair to pair. 
For illustration, consider again the complete set of analyst data. On sampling 
theory, testing the hypothesis that 0"; = ° against the alternative 0"; > ° (or 
equivalently testing O";eiO"; = I against O";eJO"; > I) involves calculating 
The result is thus not quite significant at the 10% level. The confidence distribu-
tion of b shown by the dotted curve in Fig. 6.3.1 which is obtained from , 
(paired t) is, however, appreciably different from that given by the broken curve 
in the same figure when 'I (unpaired t) is used. As mentioned earlier, from the 
Bayesian point of view, these two curves correspond, respectively, to the posterior 
distribution of i5 when the constraint c: 0"; < O";e is ignored and that of D when 
0"; is assumed zero. Both of these curves are different from the solid curve which 
is the appropriate posterior distribution of D. In obtaining the latter distribution 

356 
Analysis of Cross Classification Designs 
6.3 
we do not make the outright assumption that (J~ = 0, noi' do we ignore the con-
straint CJ; < a;" given by the model. 
The difficulty in the sampling theory approach in deciding whether to use, 
or TI is another example of the "pooling" dilemma discussed in Section 5.2.7. 
The posterior distribution p(, I y) can be analyzed in this light. The discrepancy 
between peT I y) with the constraint (' and the unconstrained distribution p(I, .• = T) 
can in fact be regarded as a direct consequence of "Bayesian pooling" of me 
and m ,. in obtaining the former distribution. 
As seen in (6.3.37), the posterior 
distribution of T can be written 
pCT I y) = f: pCr 10';, y)p(O'; I y)dCJ;, 
-00 < , < 00, 
a; > 0. 
Thus, the departure of the posterior distribution of, from the 1 (0, I, v,.) distribu-
tion depends entirely upon the departure of the distribution of CJ; fr.om that of 
(v"mJx,~2. 
For the complete analyst data, (ve = 7, me = 29.54) and (v: = 8.57, m~ = 
27.55). Thus, the posterior distribution of T (or equivalently of 6) was not much 
different from the unconstrained distribution of T. In terms of the pooling discus-
sion in Section 5.2.7, we can write 
(6.3.44) 
where from (5.2.38), 
and 
For this example, ), = 0.05 and w = 0.22 so that 
Verne + AVclne = 206.75 + 0.05 X 553.0 = 206.75 + 29.8 = 236.55 
Ve + WVc = 7 + 0.22 X 7 = 7 + 1.6 = 8.6. 
Thus, approximately 
206.8 + 29.8 
2 
2 
,z, X(7 + 1.6)' 
a e 
The combined effect of the addition of 29.8 to the sum of squares and of 1.6 to 
the degrees of freedom is small and the distribution of CJ; is nearly the same as 

6.3 
for the unconstrained situation which would give 
206.8 
2 
-
-2- ~ x~ · 
0'1 
The Addilh'e Mixed "Iodel 
357 
The much larger effect when the pair (48,57) is excluded-
Fig. 6.3.2 ·arises 
from the much greater degree of pooling which occurs here. We have Vc = 6, 
me = 32.21, v~ = 10.48, m ~ = 23.80 so that 
), = 0.33, 
w = 0.751 . 
Thus, 
193.9 + 0.33 x 166.7 
2 
"-X6+0.7)X6 
or 
193.9 + 55.7 
1 
2 
""- X6~4.5 
O'e 
as l:ompared with the unconstrained distribution 
193.9 
2 
--2- -
X6' 
O'e 
In this example, therefore, me contributes significantly to the posterior distribu-
tion of 0';. On sampling: theory, because of the small size of the mean squares 
ratio l77ell11" = 0.86, one might be tempted to pool me anJ me' Then 
so that 
193.9+ 166.7 
2 
. 
2 
-
X6-6' 
0'. 
defines the confidence distribution of 0'; for this set of data. The corresponding 
confidence distribution of 0 which results from this complete pooling and which, 
from the Bayesian viewpoi nt, is the posterior distribution of 0 on the assumption 
that O'~ = 0, has been shown in Fig. 6.3.2. 
As in the analysis of the complete 
data (Fig. 6.3.1), the posterior distribution of 0 corresponding to a Bayesian 'partial 
pooling' is sharper than \I'ilh "no pooling" or "complete pooling". 
6.3.8 Comparison of I Means 
When [ (1 ~ 2) means are to be compared, it is convenient to consider the I - 1 
linearly independent contrasts ¢i = 0i - 6, i = I, ... , (1- I). The sample quanti-
ties needed in the analysis were conveniently summarized in Table 6,3.1 , the 
analysis of variance table for the present model. As we have already indicated, 
the model can be appropriate in the analysis of randomized block data in those 
cases where it is sensible tv represent block contributions as random effects. 

358 
Analysis of Cross Classification Designs 
6.3 
The posterior distribution of <1>' = (¢>I'" 
¢>,_ I) may be obtained from 
(6.3.13a), (6.3.18), and (6.3.19a) by applying the restilt (1.5.3) concerning con-
strained distributions. We thus obtain 
p(<I> I y) = P(tU-I ) = q, -
4> I me l:, Ve) g(<I», 
-OC!<¢>;<00, i=l, ... , f-l , 
(6.3.45) 
where the factor P(t(/_I) = <I> -
4> I me 1:, ve) is the density of a 1(/_1)(4>,111,1:, v.) 
distribution which is the unconstrained distribution of <1>, and the modifying 
factor g(<I» is 
Pr*{C 1<1>, y} 
Pr {F,"e.ve+V~ < mj l11.(<I»} 
g(<I» = Pr*{C I y} = 
P {f-
/} 
r 
Ye,V. < l11e 111. 
(6.3.46) . 
with 
l11e(<I» = [v el11e + S(<I»J/(v e + v1»' 
Two "F" ratios occur in the modifying factor g(<I», m e/ me = (Scll'c)/(Se/vc) is 
the usual ratio of random effect and error mean squares whereas mc!l11e(q,) is the 
ratio of the random effect mean square to a modified "error" mean square 
me(q,) = [Sc + S(<I»] 'eve + 1'1» 
in which the treatment and error mean squares 
are pooled. Both the unconstrained distribution and the factor g(<I» are functions 
of Seq,) only, so that the posterior distribution p(<I> I y) has the same ellipsoidal 
contours as the unconstrained multivariate distribution. However, since g(<I» is 
monotonically decreasing in S(<I» , the probability contained within any given 
ellipsoidal contour defined by Seq,) = d is greater for p(<I> I y) than for the un-
constrained distribution. 
To study this effect more formally we notice that since p(<I> I y) is a function 
of Seq,) only, the probability content of any given contour may be obtained by 
considering the distribution of S(<I» or more conveniently of 
v = m(<I» 
= 5(<I»/v1> 
me 
Se/ve 
(6.3.47) 
From (6.3.14), (6.3.18), and (6.3.19a) we have 
p(V I Y) = p (Fv<J"ve = V) 9 (v I
l11e) , 
l1Ie 
V> O. 
(6.3.48) 
The first factor is the ordinary F density with ("1>' ve) degrees of freedom which 
would define the posterior distribution of V if there were no constraint. 
The 
modifying factor g(V 1111,.'111,) is the same as g(<I» in (6.3.46) but now considered 
as a function of V. 
It is, therefore, monotonically decreasing in V. 
It is readily shown that V is stochastically smaller than an FY</>,Y e variable. 
That is, l"nr Vo > 0 
Pr {V < Voly};:: Pr{FY,Pe < Vo}· 
(6.3.49) 

6.3 
The Additive Mixed Model 
359 
To see this, since p(V I y) is a probability den sity, we have 
J"-
J.r 
( 
'I' 111 
) 
( I tn ) 
1= 
p(Vly)dV= 
p(Fv~ .v, = V)g V ~ dV= Eg V _C 
o 
0 
me 
111,. 
(6.3.50) 
where the expectation E on the extreme right is taken over the unconst rained 
Fv ~ . v. distribution. 
Now g (V I II1ci l11e) is monotonically decreasing in V so that there 
exists a value V' such that 
g(VlmC»1 
lI1e 
for 
for 
Consider the difference 
levo) = Pr{V < Vo I y} -
Pr {Fv~ ,v " < Vol· 
Clearly 1(0) = I( ex:. ) = O. Upon differentiating, we have 
V > V'. 
(6.3,51) 
(6.3.52) 
(6,3,53) 
Expressions (6,3.51) and (6.3.53) together imply that levo) can never be negative, and 
(6.3.49) follows at once, 
As in the case of comparing two means, when mc'me tends to infinity, 
g(V I m)tI1e) approaches unity for all V and the posterior distribution p(V I y) tends 
to the unconstrained F(v¢.vr) distribution. It can be verified that the mixed derivative 
(6.3.54) 
for aU V and lIIe/me' so thaI for Vo > 0, the probability Pr{V < Vo I y} is monotonically 
decreasi ng in l71c1me' Further as l1Ic1me -> 0, we obtain 
(6.3.55) 
Thus, corresponding to (6.3.33), 
(6.3.56) 
which provides an upper and a lower bound for PI' {V < Vo I y}, 
The relationship between these n;sults and those of sampling theory is similar to 
that discussed earlier for the comparison of two means. In particular, the ellipsoid 
defined by m( <I> )/rne = F( v tP' v C' a) which encloses the smallest (I -
CI) confidence 
region will also be the (I -
ct) H.P.D, region for the unconstrained distribution 

360 
Analysis of Cross Classification Designs 
6.3 
of <1>. The same ellipsoid will define an H .P. D. region for the posteriord istri bu tion 
of <1>, but it is clear from (6.3.49) that the probability content will be greater than 
(1 -
ex). 
6.3.9 Approximating the Posterior Distribution of V = m(<I» /me 
Using expression (6.3.48), it is computationally simple to calculate the exact 
probability density of V, the probability integrals involved being obtained from 
an F table or a table of incomplete beta functions. 
Alternatively, as in the case of comparing two means, the distribution of V can be 
expressed as a finite series when Vc is even. Applying the identity (AS.2.3) in Appendix 
A5.2 to expand the numerator of g(V 1117,/l11e) = g(<I» 
in (6.3.46) and afrer some 
simplification, we get 
V > 0, 
(6.3.57) 
where Wj and Yj are given in (6.3.35). It foJlows that 
(6.3.58) 
which can be calculated using an F table or an incomplete beta function table. This 
expression is, however, not applicable for odd values of \'e and becomes rather 
complicated, computationally, for large Vc 
A Scaled F Approximation 
Adopting an argument similar to that leading to the scaled t approximation for 
the individual comparison , we write 
v > O. 
(6.3.59) 
hom the unconstrained joint distribution of <I> in (6.3.12a), it is clear that 
given a;, the quantity V = m(<I» /l11e has the (a;l11e)x.~~ / v-t> distribution. further, 
once a~ is given the constraint C does not affect the distribution of V. Thus the 
first factor in the integrand of (6.3.59) is, in fact, a (a;, m,.)x ~", /v", d istri bu tion. 
The second factor is, of course, the same posterior distribution uf a; given in 
(6.3.38). 
Employing the scaled [
2 approximation implied by (6.3.39) to the 
distribution of 0';, we obtain 
(6,3,60) 
The probability integral of V can thus be approximately determined by using an 
F table or an incomplete beta function table. 

.s.3 
The Additive Mixed Model 
361 
6.3.10 Summarized Calculations 
For the complete set of analyst data introduced in Table 6.3.2, Table 6.3.6 
provides a summary of the approximating posterior distributions of the various 
parameters of interest for the additive mixed model. 
Table 6.3.6 
Summarized calculations for the variolls arproximate posterior distributions for the 
additive mixed model (applied to the analyst data of Table 6,3,2) 
I, Use Tables 6,3, I and 6.3,3 to obtain 
YI , = 37,38, 
h,=41.63, 
m,, =79,O, 
lIIe = 29.54, 
v",= 1 
1= 2, 
J=8 
2. Use (6,3.39) to calculate 
x = 0,728, 
v~ = 8,57, 
y
= 39,51 
m(<j» = 8[(4), + 2.13)2 + (4)2 - 2.13)2J 
(I = 0.876 
m~=27.55 
3, Then, for making inferences about 0'; 
a; '" v :,nl~x ~_2 
4, For making inferences about (J; 
0,053 + 0,0036 (J~ "'- X 72 
5. For comparison of two means 
truncated franl below at 
Jne 
verne 
truncated frolll below at 0,053, 
6 = U, - U, 
b-(Yj,-Y,) 
, 
- -----;-' /"'2' ",,/(0, I, vel 
(2m~ / J) 
6-4,25 
-
-
-
",,/(0, 1,8.6) 
2,62 
6. For general comparison of I means (in this case 1=2) 

362 
Analysis of Cross Classification Designs 
6.4 
6.4 THE INTERACTION MODEL 
The method of analysis in the previous section can be easily extended to the 
so-called mixed model with interaction in (6.1.20), 
i=I , .. ,I; j=I, ... ,J ; k=l , ... , K 
(6.4.1) 
where Yijk are the observations, 8i are location parameters, cj are random effects, 
lij are random "interaction" effects, and eijk are random errors. The I J K observa-
tions can be arranged in a two way table with I rows, J columns, and K observa-
tions per cell as in Table 6.1.1. 
Following our previous discussion, we assume 
that cj , Ii), and eijk are all independent and that 
cj ~ N(O, cr;), 
Ii) ~ N(O, cr/), 
and 
eijk ~ N(O, cr;). 
(6.4.2) 
6.4.1 The Likelihood Function 
To derive the likelihood function, it is convenient to define 
and write 
(6.4.3) 
( 6.4.4) 
From Theorem 5.2.1 (on page 250), the deviations )'ijk - Yij. are distri buted 
independently to the cell means Yij. and also Se = LLL (Yijk - YijY has the 
a;x2 distribution with lJ(K - 1) degrees of freedom. Further, the model in (6.4.4) 
for the cell means Yij. is of exactly the same form as the additive model discussed 
in the preceding section with Gij having Normal distribution N(O, cr,2 + K-1cr;). 
I t follows that the likelihood function is 
( 6.4.5) 
The quantities appearing in the likelihood functi on can be conveniently arranged 
in analysis of variance form as in Table 6.4.1. 
In Table 6.4.1, tJ = (Ill) LOi, 4>' = (¢I, ... , ¢/-t), ¢i = 8i - e, and 
/- 1 
¢T = 
- I ¢i' 
i = , 
From the definitions of (a;, a;" a;,c), we see that these parameters are sub-
ject to the constraint 
(6.4.6) 

Table 6.4.1 
Analysis of variance for the interaction model 
Source 
S.S. 
d.r. 
M.S. 
Grand mean 
IJK(V - y . )2 
IJK(U-y .. )2 
Fixed effects 
S(<I» = J K I [<f.>; - (Yi .. - Y )J2 
v4>=I-1 
m(<I» = S(<I» /v.p 
Random effects 
Sc=IKI(Y.j. -Y. )2 
ve=J-1 
me = Sel l'e 
Interact ion 
S, = KI I(Yij. - Yi. . - Y.j. + Y .. / 
v, = (1- 1)(1 -I) 
m, = Sri v, 
Error 
S.> =I I I(Yijk - Yiji 
ve =IJ(K-I) 
me = Selve 
F.M.S. 
U;,e = u; + KCJ,2 + I KCJ~ 
CJ;, =CJ; + KCJ,2 
2 
(J etC' 
a;, 
CJ2 e 
0-
j,. 
...; 
:r 
II> -
'" ;; 
;;l 
!l 
o· 
'" 
::s: 
o 
Co 
!!. 
W 
0-
W 

364 
An:llysis of Cross Classification Designs 
6.4 
6.4.2 Posterior Distribution of (B, 9, a;, a;" a;,c) 
Adopting an argument similar to that given in the previous section, we employ 
the noninformative reference prior distribution 
(6.4.7) 
with the constraint C, from which it follows that the posterior distribution can be 
written 
- co < e < CD; 
-
CD < <Pi < co, i = 1, ... , I - 1; 
a;,c > a;, > a; > O. 
( 6.4.8) 
In (6.4.8), (i) the conditional distribution of B given a;,c> p(1l I a;/C, y), is Normal 
N (y ... , a;,c/IJ K); (ii) the conditional distri bution of 9 given a;" p( 9 I a;" y), is 
the (I -
I) dimensional multivariate Normal 
(6.4.9) 
with 
and (iii) the marginal distribution of (a;, a;" a;,J is 
(6.4.10) 
where (X;.' X;" x;J are independent X
2 variables with (v e , v" vJ degrees of free-
dom respectively. Expression (6.4.10) is of precisely the same form as the distri-
bution in (5.3.10) for the three component hierarchical design model. Inferences 
about the variances (a; , a,2, a~) can thus be made using the corresponding results 
obtained earlier. 

6.4 
The Interaction Model 
365. 
6.4.3 Comparison of I Means 
Integrating out (e, u;, u;" U,7,c) from (6.4.8), the posterior distribution of the 
(l -
J) contrasts q, is 
p(q, I y) = P(tU-I) = q, - if> I m, L, v,) g(q,), 
- w < ¢i < CO, 
i = I, .. , 1 -
I. 
(6.4.1 I) 
whose first factor is the densi ty of the multivariate I-distribution '(!_ I ) (if>, m,L, v,) 
which would be the distribution of q, if the constraint C in (6.4.6) were ignored. 
The modifying factOr g(q,) in (6.4. 1 I), which represents the effect of the con-
straint C in (6.4.6), is 
g(q,) = 
(6.4.12) 
{
"/2 
V m 
'12 
v n1 } 
P 
t..>,Je 
e 
(! 
'~\'r 
t 
I 
r -
> -
- -> --
,,2 
V m ' ,,2 
V m 
AVt 
t 
I 
t .. y(" 
c 
c 
where X;" 
v~ is an X2 variable with v, + v</> degrees of freedom independent of X;, 
and X~" 
Since g(q,) is a function of S(q,l only, it follows that the center of the 
distribution in (6.4.11) remains at if>, and the density contours of <!> are ellipsoidal. 
I t is easy to see that if> is the vector of the posterior means of q, and is also the 
unique mode of the distribution. These properties are very similar to the distri-
bution of q, in (6.3.45) for the additive mixed model. However, unlike the latter 
distribution, g(q,) is no longer monotonically decreasing in S(q,) so that 9 does 
not necessarily make the distribution of q, more concentrated about if>. The reason 
is that the quantity u;, appearing in the covariance matrix of q, for the conditional 
distribution p(q, I u;" y) in (6.4.9) can be regarded as subject to two constraints, 
u,:, < u;,c and 0";, > u,:. While the former constraint tends to reduce the spread of 
lhe distribution of q" the latter tends to increase it. The combined effect of these 
two constraints depends upon the two mean square ratios me. m, and me/m,. . n 
general the smaller the ratios mell11, and me'm" the larger will be the reduction 
in the spread of the distribution of q,. 
6.4.4 Approximating the Distribution p( q, : y) 
For a given set of data, the specific effect of the constraint C can be approximately 
determined by the procedure we now develop. The posterior distribution of q, 
can be written as the integral 
-
~o < cPi < CO , 
;=1 , ... ,1-1. 
(6.4.13) 

366 
Analysis of Cross Classification Designs 
6.4 
In the integrand, the weight function p(~;, I y) is 
p(~~, I y) = J U~' J ': p(~;, ~;" ~;,c I y) d~;,( d~; , 
o 
Gel 
(6.4.14) 
Adopting the scaled / 
approximation techniques developed in Sections 5,2.6 
and 5.2.12, to first integrate out IJ;,c and then eliminate ~;, we find 
(6.4.15) 
where 
* 
V, I x,(1l'c, tv, + I) 
I' =-
r 
a2 
[x,(i'vo ~ v,) 
, 
V, 1<>(-11'(> lv, + I) 
2 
1",(-1-1'0 11',) 
and 
To this degree of approximation, 
p(<\> I y) ='= P(t(l-l) = <\> -
<f> I m; E, v'), 
- co < 1>i < CO, 
i = I, .",1 -
I, 
(6.4,16) 
i.e., a l(l-l)(<f>, m; E, 1';) distribution. Thus, the approximate effect of the modi-
fying factor g(<\» is the change in the scale factor from m//2 to (m;)l'2 and the 
"degrees of freedom" from v, to v;. 
[n particular, if one is interested in comparing two means, say 0i and 0" then 
the approximate posterior distribution of the difference in means !J = 0, - 0i is 
I(Y" - Yi." (2/JK)m;, 1';) and H.P.D. intervals for 2> can be conveniently cal-
culated, Foran overall comparison of all I means 9' = (Ol, " " fi/), H.P.D, regions 
for the (l - I) linearly independent contrasts <\> can be approximately determined 
by making use of the fact that the quantity 
v = me<\»~ = 5(<\», v.p 
m; 
111; 
(6.4,17) 
is approximately distributed as F with (v.p, v;) degrees of freedom. 

6.4 
The Interaction Model 
367 
6.4.5' An Example 
Consider again the car-driver data introduced in Table 6.2.2, but now suppose 
that we are interested in comparing the performance of the 9 particular cars. 
The relevant sample quantities are: 
me = 126.4569 
v</> = 8 
Car 
<Pi = Yi . - Y 
1 
2.4521 
2 
- 1.5971 
3 
1.3023 
4 
0.6063 
5 
1.8641 
I11r = 1.7127 
1', = 64 
mr =J.1759 
Ve = 81 
J = 9 
K = 2 
Car ¢i =Yi . -Y .. 
6 
- 0.4950 
7 
-1.5985 
8 
-1.2205 
9 
-\.3134 
On the basis of the non-informative reference prior distribution in (6.4.7) the pos-
terior distribution of the I - I = 8 independent contrasts Q> = (¢l' ... , ¢s)' is 
that given in (6.4.11). To approximate this distribution, we find by using (6.4.15) 
x = 1,0 11.655 = 0.902 
2 
1,121.267 
' 
1 9 02(4,34) = I g02 {4, 33) = 1.902(4 ,32) == 10 
so that 
a 2 == 1.0. 
Thus, 
109.6123 
Xl = 204.858 
= 0.5351 
1", (34 ,40.5) 
1",(33,40.5) 
a l = 33 Y 
-
32 x ---'----
I x ,(33, 40.5) 
1,,(32, 40.5) 
= 33(0.9816) - 32(0.9846) = 0.8856 
v' = 64(0.9846) = 71.15 
, 
0.8856 
and 
109.6123 
m; = 
= 1.74. 
0.8856x71.15 

368 
Analysis of Cross Classification Designs 
6.4 
It follows that q, is approximately distributed as ,8 (<\>, 1.74E. 71.15) where from 
(6.4.9), 
In particular, suppose we wish to compare the means of cars I and 2. Then 
(; = 0) - O2 = ¢I - ¢2 
is approx imately distri buted as 1 (4.05, O. j 933, 71.15). Lsing the Normal approxi-
mation, limits of the 95% H.P.D. interval are (3,)9,4.91). 
For overall comparison of the means of all nine cars, the (I -
0:) H.P.D. 
region of q, is given by 
where 
m(q,) 
-- ~ F(8, 71.15, :;) 
1.74 
18 ~ 
~ 
2 
m(q,) = -
L., (¢j - ¢J . 
8 j .1 
In particular, one may wish to decide whether the parameter point <P = 0 (i.e., 
01 = ... = (9) lies inside the 95 % H.P.D. region. We find 
m( <p = 0) 
45.26 
-'--::-~ = -- = 26.01 
and 
F(8, 71. 15, 0.05) == 2.14 
1.74 
1.74 
so that the point <p = 0 is excluded. 
Finally, we note that if the constraint (6.4.6) were ignored, then the posterior 
distribution of <p for the present example would be ,8 (<\>, J.71 1:, 64), compared 
with the approximating ,8 (<\>, 1.741:.71.15). The effect of the constraint is seen to 
be very slight. 
In the above we have assumed that the nine means OJ are locally uniform 
a priori. In some situations, especially when the nine cars were of the same make 
and year, they might be more realistically regarded as a random sample from a 
large population of cars. 
In these circumstances, a more appropriate analysis 
would follow the procedure developed in the foJiowing chapter. 

CHAPTER 7 
INFERENCE ABOUT MEANS WITH INFORMATION 
FROM MORE THAN ONE SOURCE: ONE-WAY 
CLASSIFICATION AND BLOCK DESIGNS 
1.1 !:'I\TRODUCTIO]'\ 
.V1 U1ch scientific work has as its object the detection and measuremeni of possible 
cha nges in some observable response associated with qualitative change in the 
experimental conditions. To make it possible to assess such influences, experi-
ments are frequently arranged so that the data can be classified in some convenient 
manner. Thus, in our earlier discussion, we have considered data coming from 
one-way classification arrangements, cross classifications, and hierarchical designs. 
\\ith sampling theory, when the object was to compare means, the analysis 
was ordinarily conducted using a "fixed effect" model , but when variance com-
ponents were of interest the analysis employed a "random effect" model. Mixed 
models were employed when some factors were thought of as contributing fixed 
and others random effects. As we have seen in Chapters 5 and 6, a number of 
difficulties were associated with analysis of these models in the sampling theory 
approach. 
By considering two other important problems in this general area we hope to 
show further, in the present chapkr, how the Bayes approach illuminates and 
resolves difficulties. The two selected problems concern the comparison of means 
i) for the one-way classification design, 
ii) for balanced incomplete block designs. 
For problem (i) the sampling theory Jifficulties are evidenced, for example, by 
the results of James and Stein (1961) which show the usual averages to be inad-
missable estimators of the group means. 
For problem (ii) in sampling theory 
terms, one difficulty is that of appropriately "recovering inter-block information." 
An inadequacy of the framework outlined above is that it does not provide 
for the common situation in which we wish to compare means which are neverthe-
less "random effects." Similarly block effects are often best treated as random. 
These inadequacies are easily remedied in the Bayesian approach, and when this 
is used, it be~omes clear that the probJem~ encountered with sampling theory 
concern once more the difficulty, on that theory, of appropriate poo/inK of informa-
tion from more than one source. 
369 

370 
Inference about Means with Information from more than one Source 
7.2 
7.2 INFERENCES ABOUT MEANS FOR THE ONE-WAY RANDOM EFFECT 
'MODEL 
Suppose we have data Yjk arranged in a one-way classification with J groups and 
K observations per group. For example, consider again the dyestuff data [taken 
from Davies (1967, p. 105)J for K = 5 laboratory determinations made on samples 
from each of J = 6 batches. These data were previously considered in Sections 
5.1 and 5.2. The observations and the batch averages are shown in Table 7.2.1. 
Table 7.2.1 
Yield of dyestuff in grams of standard color 
Batch 
2 
3 
4 
5 
6 
145 
140 
J95 
45 
J95 
120 
Individual 
40 
155 
150 
49 
230 
55 
observations 
40 
90 
205 
195 
115 
50 
(yield-I 400) 
120 
160 
110 
65 
235 
80 
180 
95 
160 
145 
225 
45 
Averages 
105 
128 
164 
98 
200 
70 
7.2.1 Various Models Used in the Analysis of One-way Classification 
In Section 2.11 we have considered the analysis of data arranged 10 this way in 
relation to a "fixed effect" model 
j=1 , 2, .. . ,J : k=I , 2, ... , K, 
(7.2.1 ) 
with the errors distributed independently and )lormally such that ejk ~ N(O, ()"~). 
In that formulation , the 8j were supposed to have locally uniform reference priors. 
This would seem appropriate if the means 8j were expected to bear no strong 
relationship one to another. 
Certainly, problems occasionally do occur where 
this is so. Thus, in comparing the laboratory yields for several different methods 
of making a particular chemical product, we could have a situation approximated 
by the supposition that anyone of the methods could give yields anywhere within a 
wide range, independently of the others. Given this supposition, we have seen that 
a posteriori the 8j would have a multivariate I distribution, and consideration as 
to whether a particular parameter point e was or was not included in a particular 
H.P.D. region could be decided by the use of the F distribution. These results all 
have exact parallels in the standard sampling theory analysis. 
It is aprarent, however, that locally uniform priors for the 8j would be totally 
inappropriate for the dyestuff data. A model more likely to fit these circumstances 
would be one where the batch means 8j were regarded as independent drawings 

7.2 
Inferences about Means for the One-way Random Effect Model 
371 
from a distribution. These data have already been studied in connection with such 
a model in (5.2.1). SpecificalJy, writing OJ = 0 + ej , it was assumed that 
ej ~ N(O,O"D, 
that is, 
OJ ~ N(O, O"D, 
(7.2.2) 
but was supposed that we wished to learn about o-~, the variance of the distribution 
of the OJ. This "random effect" model could, however, equally well be appropriate 
when interest centered on the individual batch means 0 1, ... , °6 , rather than on 
the variance o-~. 
As a further example, if the performance of six Volkswagen cars bought in 
the same year was tested on five different days, the main objective could be to 
compare the mean performance of the particular six cars being tested, even though 
the cars could reasonably be regarded as random drawings from a population of 
Volkswagens. As Lindley has pointed out in his discussion of the work of Stein 
(1962), it is common for a random effect model to be appropriate, and yet to wish 
to make inferences about means. 
In what follows then, the random effect model of Section 5.2 is used but the 
object now is to make inferences about the individual batch means OJ. We later 
consider in some detail the contrast between this random effect analysis of the 
means and the corresponding fixed effect analysis, and also its relation to certain 
results in sampling theory. 
7.2.2 Use of the Random Effect Model in the Study of Means 
Assuming then the random effect model defined j n (7.2.1) and (7.2.2), the usual 
associated analysis of variance is given below in Table 7.2.2. 
Table 7.2.2 
Analysis of variance ror one-way classification random effect model 
Sampling 
expectation 
Source 
5.5. 
d.f. 
M.S. 
or M.S. 
Between groups 
S2=K'i.(Yj .-Y i 
v2 = J-) 
m2 = S2/v2 
O"i2 =O"T + KO"~ 
(batches) 
Within groups 
SI ='i.'i.(Yjk-Yj/ "I=J(K-l) 
m l =SJ VJ 
0"2 I 
As before, we have adopted the notation that a subscript replaced by a dot 
means the average over that subscript. 
Prior and Posterior Distributions of (e, 0, o-i, O"~) 
Let e be a vector whose elements are the group means °1, °2 , ... , OJ. 
This 
vector must be distinguished from the scalar 0 = E(O),j = J, 2, ... , J. 
The 
joint distribution of the JK observations y and the unknown parameters 

372 
Inference about Means with lnformation from more than one Source 
7.2 
(e, crT , crL 0) may be wri tten 
pry. e, cri, cri, 0) = pCO, cri, cr~) p(G I cr~ , 0) pry ; e, a~) 
(7.2.3) 
where from (7.2.1) 
2 
2 -1K'2 
f 
1 
2} 
pry I 0, cr 1) cc (cr l ) 
exp \ - - 2 [v]m] + K L (OJ - h) ] , 
2cr 1 
-
00 <Yjk< Xl. 
j=l, ... ,J, 
k=l, ... ,K, 
(7.2.4) 
from (7.2.2) 
pee I cr~ , 8) oc (cr~)-1 1 1 exp [ - ~ 
L (OJ - 0)2] , 
2cr2 
-00 < OJ < 00, j= I, ... ,J 
(7.2.5) 
and pro, cri, cr~) is the prior distribution of (0, cri, cri). 
For a noninformative reference prior of (0, cri, i:T ~ ), note that by combining 
(7.2.4) and (7.2.5) and integrating out 0, the joint distribution of y given (0, crT, cr~) 
is precisely proportional to lhe likelihood function in (5.2.7). Thus, given y, the 
likelihood function of (0, cri, cr~) is 
(7.2.6) 
Thus, following the previous approach to variance component models, inferences 
are considered against the background of the noninformative reference prior 
(7.2.7) 
in which log cri, log ai 2 , and 0 are supposed locally uniform, and the fact that 
cr?2 = cri + Kcr~ implies the constraint crT2 > cr~. 
Given the sample y, the joint posterior distribution of (0, cri, crL 0) is then 
prO, aT, a~, 8 I y) cc cr; 2cr;} pCO I crL 8) p(~/ I e, crT) cc a;} (ai}-(t .IK + 1) (a~) - tJ 
] } , 
cr~ >
O, 
a~>O, 
-00 <()j< OO, 
j=I, ... ,J. 
(7.2.8) 
7.2.3 Posterior Distribution of e yia Intermediate Distributions 
Before obtaining the posterior distribution of e, various intermediate distributions 
of (G, cr;, a~, 0) are first derived . This approach facilitates subsequent comparison 
of random and fixed effect models. 

7.2 
Inferences about Means for the One-way Random Effect Model 
373 
Conditional Posterior Distribution 0/ e Given (U, (J~, (J~) 
The joint distribution in (7.2.8) can be written as the produt:t 
pee, (J~, (JL 0 I y) = p(S i (JL (JL 0, y) p«(J~, (JL 0 I y). 
(72.9) 
Using (A. I. 1.5) in Appendix AI.I, we can write 
K 
2 
1
2
K 
-1 
K 
2 
~(8j - Yj) + 0"; (Uj -
0) 
= 
O"~(I _ z) (OJ -
OJ) + 0"~2 (D - )'j) . 
(7.2.10) 
where 
OJ = (I -
Z)Yj + zU 
(7.2.11 ) 
and 
Note t ha t z is the reci procal of W = 
O"~ 2 /(J~ , the quantity considered in (5.2 .15). 
It follows that conditional on (U, O"~, O"~) the OJ are independently distributed 
a posteriori as 
(7.2.12) 
Posterior Distribution 0/ (0, O"~, O"~) 
Also, the posterior distribution of (0, (J~, O"~) is 
0"7 > 0, (J~ > 0, 
- 00 < 0 < Xl , 
(7.2.13) 
which is equivalent to the distribution in (5.2.11). This implies that in particular 
which is a truncated X- 2 distribLltion . 
Posterior Distribution of (9, z) 
Using (7.2.10) 
(J~ > O. 
(7.2.14) 
-21 [ vlm l + K ~ (OJ - .. !:>.i + __ 
L_(_O,,-j ..".-_°1_
2_1 
I l 
(J~ 
. 
2 
2 
2 
= -2 vlm l + 2
1 (0 -
0) 
0" I 
(J 2 
20" I 
(J 2 
+ KS(9, Z)1 ' 
(7.2.15) 

374 
Inference about V1eans with Information from more than one Source 
7.2 
where 
and 
&=r1I.fJj
. 
Making the transformation from (a, O'~, O'~, fJ) 
to (€I, O'~, z, 8) and integrating 
out 0 and O'T, in (7.2.8), we obtain 
. 
p(a, z I y) ex:. Z·}v,-1 (J - z)- .~v, [vIm, + KS(a, z)r':(VI+V2+J), 
O<z<l, 
-00 <OJ<oo, 
j=I , ... ,J. 
(7.2. 16) 
Now the quantity S(a, z) may be written 
z 
s(a, z) = (a - Y.na - y) + -I -
8'(1 - rl11')a 
- z 
(7.2.17) 
where Y: = (Yt.> ... , Yl,) and 1 is a J x I vector of ones. By making use of the 
identity (A7.1.l) in Appendix A7.1, we find 
KS(8, z) = ZV 2ln 2 + (I - z)-' Q(a, z), 
(7.2.18) 
where 
Q(a, z) = K[a - e(z)]'(1 - zr '11')[a - e(z)J, 
e'(z) = [G,(z) , ... ,GAz)J, Gj(z) = h -
z(Yj. - yJ, j = \, ... , J. 
Thus, 
p(a, z I y) ex:. ztv, -, (I - z) - iv, [v 1 m 1 + zv 2m 2 + (I - z) -1 Q(a, z)r i(VI + ,., +1), 
O<z<I, 
-co<8j <co, 
j=I, ... ,J. 
Posterior Distribution oj 8 Conditional on z 
From (7.2.19) it follows at once that, conditional 0 11 z, 
a) the distribution of a is the J-dimensional t distribution 
where 
and 
t)[8(z), s2(z)K -1 (I - zr IllY 1, V, + v2J, 
8'(z) = eel (z), ... , 8Az)J, 
8/z) = h - z(Yj - yJ = (I -
z)Yj. + zY., 
(I - zr 1 l1')-' = I + r 1(_2_)11" 
I -
z 
(7.2.\9) 
(7.2.20) 

7.2 
Inferences about Means for the One-way Random Effect Model 
375 
i.e .. 
-Cf) < OJ < OC, 
j = I, ... ,J, 
b) the OJ have means 8/z). j = I .... J, and they have common variance and 
covariance 
(v, + v2 ) 
[ 
z] 2 
Var(8j lz)= 
1+ 
5 (z). 
K(V,+V2-2) 
J(I-z) 
(7.2.21 ) 
c) the marginal distribution of OJ is a I distribution having v, + V2 degrees 
of freedom; specincally, the quantity 
OJ -
O/z) 
, 
Var(Ojlz) 
[(
V + V2 - 2) 
]' /2 
v, + V 2 
is distributed as /(0. I. VI + 1'2), 
(7.2.22) 
d) more generally, the marginal distribution of a linear function of the means 
'1 = 1'9, where I' = (I, ... , /)), is such that the quantity 
with 
fj(z) = l' 9(z) 
is distri buted as 
and 
'1 -
fj(z) 
~(I) 
(7.2.23) 
e) in particular, the marginal distribution of a particular difference OJ -
OJ 
is a 1 distribution having V, +. V2 degrees of freedom. namely, the quantity 
(OJ - 0) -
(I -
z)(Yj. - Yj) 
[2K 
IS 2(Z)JI12 
has the /(0, I, v, + v2 ) distribution. 
(7.2.24) 
f) the joint distribution of the (J -
I) linearly independent contrasts <I> = 
(4), •...• 4» - ,)' where 4>j = OJ - n, j = I, ... ,J -
I, is given by the J -
1 

376 
Inference about Means with Information from more than one Source 
dimensional t distribution having VI + \/2 degrees of freedom defined by 
p(<!> I z y) oc II I + K Lj x I [<p j -
(I -
Z)(Yj - yJ]2} -{ (VI +V,+ 1-1 ), 
(VI + \/2)S2(Z) 
7.2 
-OO«Pj<XJ, 
j=I, ... ,J-J , 
where <P1 = -
(cPl + ... T <PJ- I)' 
(7.2,25) 
Posterior Distribution oj z 
Now from (5,2.16) and remembering that z = W -1, the marginal distribution 
of z = (J7 /(J~ 2 is given by 
(m2 /ml)p[Fv2.VI = (m 2/I11 I )z] 
p(z I y) = 
, 
Pr {FV2:V I < /11 2/11I 1} 
0< z < I, 
(7.2.26) 
where, as before, p(Fv"vl = c) is the density of an F variable with (v2, 1'1) degrees 
of freedom evaluated at c. The rth moment (r < -tv 1) of z is, therefore, 
, 
8(-iv2 + r,~vl - r) 
IxC~V2 + r, tVI -
r) (1')17'1 1 )r 
p = E(z)' = 
--
r 
8(11'2, ~VI) 
IxC-t1'2' }V I) 
V2m2' 
(7.2.27) 
I' 2/112 
__ 
::"-CC-_, and 8(p,q) and IxCp,q) are, respectively, the complete 
vim! + V2m 2 
where x = 
and the incomplete beta functions. 
Posterior Distribution 0/ ij 
The unconditional distribution of the means a may now be obtained by integrating 
the conditional distribution of a for given z in (7.2.20) with p(z I y) as weight 
function. Thus 
pee I y) = I p(a I z, y) p(z I y)dz, 
(7.2.28) 
In particular, the unconditional distribution of any linear function 17 = fa 
of the elements of a can be similarly obtained by integrating the conditional distri-
bution of 17 given z in (7.2.23) over the distribution p (z I y). Also, to obtain the 
moments of OJ, we can take the expected values over z of the conditional moments. 
In particular, we And 
E(Oj I y) = OJ = E[(Jj(z)] = YJ. ~ /1'1 (Yj - Y J 
= (I -
/1~)Yj + P'I y, 
Var (OJ I y) = E Var (8 j I z) + E[8 j (z) - 8J2 
z 
z 
J(v!ml + p'! \12m2) -
(J -
1)(p'l vlm l + Il~v2m2) 
JK(vl + v2 -
2) 
(7.2.29) 
+ Cr'j. - Y./P2' 
(7 .2.30) 

7.2 
Inferences about "leans for the One-way Random Effect Model 
377 
and 
Cov (8j , 8j [ y)-= E Cov (8 j , 8j [ z) + E[8 j(z) - 8;][8/z) -
8j ] 
z 
z 
where 
7.2.4 Random Effect and Fixed Effect Models for Inferences about Means 
Inferences about the means Bj resulting from the random effect formulation are 
different from those for the fixed effect formulation discussed earlier in Section 
2.11. Specifically, in contrast to the results in (7.2.28) through (7.2.31), we recall 
from (2.11 . 1) that for the fixed effect model, a posteriori 
where y: = (Yl., ... , YJ)' 
In particular, the marginal distribution of a linear function IJ = I'e is 
IJ ~ t(l'Y" m1K-1l'l, Vj) 
Also, the means, variances and covariances of the ej are 
(7.2.32) 
(7,2.33 ) 
£(8 j [ y) = YJ., 
and 
Cov (8 j , Gj [y) = 0, 
(7,2.34) 
An Illustrative Example 
For illustration, consider the dyestuff data quoted in Table 7.2.1, 
!n this 
example ej is the mean yield for the jth batch of dyestuff and 
J = 6, K = 5, 
VI = 24, 
v2 = 5, 
mj = 2,451.25, 
1112 = 1 i,271.50 
Y .. = 127.5, 
J-I.; = 0.233, 
112 = 0.026. 
Figure 7,2,1 contrasts the posterior distributions of the 8j from random and 
fixed effect models. The greater clustering of the distributions about y. which 
occurs with the random effect model is clearly seen. Table 7.2.3 contrasts the means 
and standard deviations of the distributions obtained from these two models. 
To facilitate comparison , we have arranged and numbered the groups in order of 
magnitude of the sample means Yj., Inspection of the table shows the clustering 
of the posterior means and also the ~light reduction in standard deviation of the 
distributions with the random effect models. 
Figure 7.2.2 shows the distribution of 86 -
G1 for the random model effect 
together with the corresponding distribution appropriate to the fixed effect model. 
In this extreme case, a very large difference is seen between the two distributions, 

378 
Inference about Means with Information from more than one Source 
0.015 
0.005 
0.015 
0.005 
o Samp Ie mean 
• Posterior mean 
L..<=.....-=---"=-----'L-L-=--A.J ___ ~1L____=::""_a..;::"""'=_.&:I:=__'____==__'__~6 i -+ 
Random effect model 
.....,.=--'--.=.-"'-_---'-.=:.Ll....L_--D="'----_'_-=>-=~___"::=::<'___'_____= __ e i -+ 
50 
90 
130 
170 
210 
Fixed effect model 
Fig. 7.2.1 Posterior distributions of ej for the dyestuff data. 
Table 7.2.3 
7.2 
Means and standard deviations Gf 6j for random effect and fixed effect models: the 
dyestuff data 
Group 
Posterior means E(ej I y) 
Standard deviations [Var (ej I y)]1/2 
(ordered by 
magniiude 
Random effect 
Fixed effect 
Random effect 
Fixed effect 
of mean) 
Yj . -O.233(Yj. - Y.) 
y . 
J. 
. [420.4+0.026(Yj. - y./]1,2 
(534.8)1/2 
83 
70 
22.5 
23.1 
2 
105 
98 
21.1 
23.1 
3 
110 
105 
20.8 
23.1 
4 
128 
128 
20.5 
23.1 
5 
156 
164 
21.3 
23.1 
6 
183 
200 
23.6 
23.1 
Although for the fixed effect model the e 
j are uncorrelated, this is not so for 
the random effect model. The rather slight correlations that occur in the present 
example are shown in Table 7.2.4. 

7.2 
0.010 
0.005 
Inferences about Means for the One-way Random Effect Model 
;.-- ..... 
/ 
" 
/ 
' , 
\ 
-- -
Fixed effect model 
-- Random effect model 
" 
\ 
40 
70 
100 
130 
\ 
\ 
\ 
\ 
" 
\ 
160 
\ 
\ 
" , , 
" '-. 
Fig. 7.2.2 Posterior distributions of 86 -
81 for the dyestuff data. 
Table 7.2.4 
Correlation matrix of 8j for the random effect model: the dyestuff data 
Pij = COY C8 j , 8j I y)/[Var (8 j I y) Var (OJ I y)J1 /2 
Group 
2 
3 
4 
5 
6 
0.14 
0.12 
0.05 
-0.07 
-0.16 
2 
0.09 
0.05 
-0.01 
-0.07 
3 
0.05 
+0.00 
-0.04 
4 
0.05 
0.05 
5 
0.18 
6 
I 
7.2.5 Random Elfect Prior versus Fixed Elfect Prior 
379 
In the Bayes framework there can, properly speaking, be no "fixed" effects 
since the parameters 8j are regarded as having probability distributions in any 
case. The terminology "fixed effect" and "random effect" are sampling theory 
concepts which we have retained to make certain analogies clear. The distinctions 
arise from the different prior distributions appropriate in different circumstances. 
Specifically, corresponding to the "fixed effect" model , the appropriate non-
informative prior is one where' p(9) is taken to be locally uniform so that 
p(9, u~) = p(9) p(ui) 
(7.2.35) 
with 
peS) ex:: c 
and 
This will be called the "fixed effect" prior. 

31ll) 
Inference about Means with Information from more than one Source 
7.2 
On the other hand, corresponding to the "random effect" model, the appropri-
ate prior is 
pee, (Ji, (J~, B) = pCB, (Ji, (JD p(O I (J~, B), 
(7.2.36) 
where 
pCB, (Ji, (In ex (J 12 (J 122 
is our non informative prior for (B, (Ji, (JD and pee I (J~ . 8) is supposed Normal as 
in (7.2.5). This we call the "random effect" prior. 
In either case, the posterior distribution of C is obtained by combining the 
appropriate prior with the same likelihood 
lee, (Ji I y) ex p(y I e, (J~), 
(7.2.37) 
where p(y I e, (Ji) is given in (7.2.4). Thus, the posterior distributions of e are 
different because the priors are. 
For purposes of comparison, both can be thought of in terms of a general 
model in which the prior for e, (Ji, (J~ and 8 is written in the form 
where 
and 
p(8) ex c 
pee, (J~ I 8, (Ji) = p«(Ji I (J7) p(O I (J~, 8) 
IX. p«(J~ I (Ji) (J-;J exp [- ~ 
L (8j -
8)2]. 
2(J2 
(7.2.38) 
(7.2.39) 
(7.2.40) 
In both cases, the 8j may be supposed to be a random sample from a Normal distri-
bution N(G. d). The crucial question concerns the choice of p«(J~ I (JD. 
The assumption of a locally uniform prior for the Bj in the fixed effect formu-
lation amounts to postulating that they are distributed about some unknown 8 
with a large variance do. We can accomodate this assumption within the general 
framework by making p«(J~ I (JD in (7.2.40) a delta function at (J~ = 
a~o. On the 
other hand, by employing a noninformative prior p«(J~ I (JT) ex (J122 , we are led to 
the random effect analysis where, by allowing a~ to become a random variable, 
we let the data y determine what can be said about the spread of the 8j . 
Uniform Prior and M ultiparameter Problem of High Dimension 
We have seen that in the fixed effect formulation, the noninformative prior for e is 
supposedly locally uniform. It is natural to ask "Specifically what prior distribution 
for the 8j is implied by the random effect prior of (7.2.36)." For simplicity, suppose 
(JT is known. 
We have 
(7.2.41) 

7.2 
Inferences about Means for the One-way Random Effect Model 
381 
where from (7.2.36) and (7.2.40) 
a I 2 
2 -) ' 2 
I 
I ,,' 
21 
p( 
0'2,0) u: (0'2) 
exp 
-
2 
L (b j -
OJ 
. 
20' 2 
' 
(7.2.42) 
and 
(7.2.43) 
Thu,. for large J, 
(7.2.44) 
and. since the second factor dominates the first, 
(7.2.45) 
,This distribution is very close to the prior for a suggested by Stein (1962) and by Anscombe 
(1963). Specifically, they proposed that an appropriate prior for high dimensional a is 
the multivariate I distribution 
(7.2.46) 
\\'here \' is an arbitrarily small positive constant. 
The prior of a in (7.2.45) has it s 
probability mass spread over a very wide region in the space of e. 
In particular, it 
can be shown that the variance of any linear function of a is infinite. 
To see the distinction between (7.2.45) and the locally uniform prior p(a) (f~ c, 
onsider the quantity 
(7.2.47) 
\'. lieh is, in a sense, an "estimate" of O"~. In particular, consider what prior distribution 
r ;,-~ is implied by the random effect prior for a ill (7.2.45). h rst, conditional on O"~, 
"e obtain from (7.2.42) that 
r
2 
) 
( -2 I 
;z 0) 
.. .1 - J 
2 
( 
0" 2 
P 0' 2 0' 2' 
ex 0" 2 
0" 2 
ex p 
-
--2 . 
20"2 
(7.2.48) 
B) integrating over the distribution of O"~ in (7.2.43), we then find that, for large J, 
(7.2.49) 
\\ hich is a decreasing function of O'~ and, as might be expected, is of the same form as 
t . .: noninformative prior for O"i. 
By contrast, consider what the prior of O'~ would be if the OJ were strictly uniformly 
l1. tributed over some region R in the parameter space ofe. Now O'~ = i. defines a hyper-
" h rical surface with radius proportional to ;J centered at 81 where 1 is a Jx J 
':,llr of ones. Suppose this surface lies within R. Then the probability that 0'2 lies 
'.\ [thi n i.! and J.'. + 6, where (J 
is a small positive constant, is proportional to 
,iIi.! )) - I. Thus, a strictly uniform prior for 0 implies that 
p(O'i I (J) ex (O'~)-±:) - 1 
(7.2.50) 

382 
Inference about Means with Information from more than one Source 
7.2 
which, for J > 2, is an increasing function of a~ and asserts that a~ is large with high 
probability. This is not surprising if we remember that a Aat prior for e can be produced 
by allowing a~ to become large in the prior distribution (7.2.42) 
Thus, we obtain 
from (7.2.48) 
as in (7.2.50) so that , for the " fixed effects" prior, a~ is large because a~ is tacitly assumed 
to be large. 
One is thus Jed to contrast the two results 
random effect (approximate result) 
p(a~ I 8) x (ai + Ka~)-1 
strictly uniform prior 
(or prior with a~ --) co) 
p(a~ I 8) cc (a~)~J-1 
and we see that these two expressions become more and more discordant as the 
dimension J increases. As was pointed out by Anscombe, these results should lead one 
to approach with some caution the choice of noninformative prior distributions when 
the number of parameters is large. 
One might wonder how can we justify two different priors for e both of which are 
supposed to be noninformative? 
The starting point for deriving a noninformative 
prior in Section 1.3 was that we wished to express the state of knowing little about 
the parameters in a given model relative to what the data would have to tell us. This 
implies. as we have seen already, that differel1l priors will express this state when we 
contemplate different models. For this example the "fixed effect" prior expresses the idea 
directly that we know very little about anyone of the parameters 8j . By contrast, the 
" random effect" model says that the 8j are random drawings from a Normal distribution 
,Y«), (J"~) and the prior expresses the fact that little is known about the mean 
~nd 
variance of rhal dislribulion. The implication here is that the means do cluster together 
and correctly implies a different prior for the OJ. 
Alternative Assumptions About the Prior 
To discuss the choice of prior further, It IS useful to reiterate the general role 
of assumptions in statistical methods. In practice, we know that all assumptions 
are false. For example, there never was an exactly straight line nor an exact Normal 
distribution . Logically, then , in selecting assumptions we should not ask (a) "Is 
this assumption true?" nor only (b) "Does this assumption approximate the truth 
concerning the experimental setup?" Rather we have to ask (c) "Does the use of 
this assumption lead to an approximately correct result?" This is so because the 
answer to (a) must always be "No" and so the question is irrelevant while the 
answer to (b) can be "Yes" when the answer to (c) is "No" and vice versa . The 
nature of the experimental setup should provide some guide, though not necessarily 
a decisive one, in what ought to be assumed. Therefore, in considering the choice 
of a prior distribution for a group of J means we shall consider both questions 
(b) and (c) above. 

7.2 
Inferences about Means for the One-way Random Effect Model 
383 
Priors for Different Experimental Set-ups 
For the one-way classification model being considered, in rare instances it 
might be realistic to analyse the data against a noninformative reference prior 
which supposed that the treatment means ej were, approximately, independently 
and uniformly distributed over a wide range. Strictly speaking, this prior is only 
truly representative of a situation where the experimenter 
i) knows little abo ut any of the treatments, 
ii) has no reason to believe any of the treatment will produce similar results. 
[n practice, however, (ii) will rarely represent his state of mind. Usually some of 
the treatments would be modifications of others and could be expected a priori to 
behave similarly. [n particular, (ii) is clearly not appropriate in the example we 
have considered where the " treatments" are "batches." There, the alternative 
supposition is much closer to reality, whereby the ej are independent drawings 
from a Normal distribution N(O, O"D with e and O"~ only vaguely known and hence 
approximately represented by noninformative priors. 
Obviously the two alternative possibilities considered in this chapter are by no 
means exhaustive. When a large number of treatments are under consideration , 
the experimenter may for example be able to divide them into subgroups within 
which he expects similarities. Alternatively even if the "treatments" were different 
batches, he might not believe that the ej were independent but rather that they 
formed a time series represented perhaps by an autoregressive process.t 1n this 
case the prior would be defined in terms of the parameters of this process. 
The Bayes approach has the advantage that any possibility of interest can be 
modelled and the appropriately chosen prior will invite the data to comment on 
relevant uncertain aspects (for instance on the variance of the population of the 
8j in the random effect model, or on the value of the autoregressive parameter 
if the 8j are represented as a time series). 
7.2.6 Effect of Different Prior Assumptions on the Posterior Distribution of (} 
We have said that with data occurring in the form of a one-way classification, 
there are many prior assumptions that in different circumstances could make sense. 
The two priors for (} examined here are the "fixed effect" uniform prior and the 
"random effect" prior. 
As illustrated by the dyestuff example, the random effect prior results in a 
greater clustering of the posterior means about the grand average y .. as well as 
an overall increase in the precision of the posterior distributions of the {)j. 
A Simpler Situation 
A better intuitive understanding of how this happens is obtained by con-
sidering the simpler situation in which O"L 
a~ and e are all supposed knOll"n. 
t See, for example, Tiao and Ali (197Jb). 

384 
Inference about Means with Infurmation from more than one Source 
7.2 
Then, for the random effect model we would have two sources of information about 
a particular batch mean OJ. 
First, we would know from (7.2.5) that the Bj was 
drawn from a Normal distri bution with mean B and variance u~. Second, we w0uld 
known from (7.2.4) that the sample mean Yj. was distributed Normally about Bj 
with variance uUK. Combining these facts we see from (7.2.12) that a posleriori 
the Bj would be )JormalJy distributed with mean 
where 
-
2 
2 
Bj = E(Bj I 0'1 , 0'12 ' e'Yj) = (l - Z)Yj. + zO, 
(7.2.5\) 
2' 2 
z=a L u 12 · 
Thus, the posterior mean would be a linear interpolation between the sample 
mean Yj and the prior mean 0. Also, the variance would be 
2 
2 
2 
0'1 
Var(Ojlal,a12,B,Yj)=K(1 - z).\ 
(7.2.52) 
Further, the Bj would be distributed independently of one another. 
Note that 
Z- l = u~ 2 / U~ = 1 + K(a~/a~) measures the relative variation of the group means 
OJ compared with the within group variation of the data. 
On the other hand, for the fixed effect model with a~ assumed known, we 
would only have the second source of information from (7.2.4) so that a posteriori 
the means Bj would be l'<ormally independently distributed with 
£(OJ I aL Yj) = Yi 
and 
Var (OJ I a~, Yj) = ailK, 
j = 
\ , ... , J. 
(7.2.53) 
Thus, the effect of the information from the random effect prior is to "pull" the 
posterior mean of OJ towards the prior mean e and to decrease the variance by 
a factor (I -
z) = 
\ -
afiO"f2' 
When a~, a~ and e are not known , the posterior mean of OJ for the random 
effect model becomes that in (7.2.29), that is, 
£(e j I Y) = 8j = (I -
)1'1) h + )1 'IY .. , 
where 
)1'1 = E(z I y), 
which is an interpolation between the sample mean J'j. and the grand average 
y. Compared with (7.2.51), we are thus replacing 0 by Y .. and z by its posterior 
expectation. 
Silualion When J is Large 
Now, if the number of groups J is large so that V J and \'2 are both large, then 
in the integrand of (7.2.28) the conditional multivariate I distribution pee I z, y) 
given in (7.2.20) approaches a J-dimensional multivariate ~ormal distribution. 
Also, the distribution p(z I y) in (7.2.26) will become sharply concentrated about 

-, 1 
Inferences about Means for the One-way Random Effect Model 
385 
r he reciprocal of the mean square ratio 
A 
m l 
1 
Z=-=-
m2 
F 
(7.2.54) 
--0) Ided F > I. 
In this case, using the approximations /1'1 == I/F, /1~ == (I /F)2 and /12 == 0 in 
-_.29) through (7.2.31) and noting that the integration process 10 (7.2.28) is 
--enlially equivalent to replacing the unknown z in pee I z, y) by z, the OJ are 
~i iributed approximately Normally and independently with 
EI'-
' ) ==(l-~)Y . +~Y 
) -
F) ' 
F" 
and 
m 1 ( 
1 \ 
Var (OJ I y) == K 
I - F) . (7.2.55) 
B~ contrast, for the fixed prior, it is clear from (7.3.32) that, for large J, the 
(Ire approximately Normally and independently distributed such that 
and 
(7.2.56) 
Thus. by using the random effect prior (7.2.36) 
a) the posterior means cluster more closely about Y .. , 
bl the variance of the distribution of OJ tends to be reduced by a factor which, 
for large J, is approximately I -
I;F. 
It follows that when F is large so that I /F is small, the clustering and increase in 
precision is negligible and the random effect prior yields what is essentially the 
fix~d effect solution. On the other hand, when F is not large but takes a value such 
as 2 or 3, considera.ble modifications in the posterior means and variances occur 
leading to modifications in the inferences about 9. 
The dyestuff data of Section 7.2.4 represent an intermediate situation. As 
we have seen, for this data, inferences about individual elements ofe' = (0 1 , ... ,06 ) 
are not very different for the random and the fixed effect formulations. More 
generally, if F = m2im l is fairly large (say F > 10), the random effect analysis will 
not differ very markedly from the fixed effect analysis. 
~ow F = m1 /m l is a sample measure of 
2 
2 
() I 2 
.(J 2 
-- = 1 + K --
ai 
ai 
indicating how large a~ is compared with a ~ i K. In the random effect analysis, 
then , we are as it were using the data (as reflected by the spread of the sample 
means Yj in relation to the within group variances milK) to comment on the 
spread of the means and hence tell us to what extent the 10caJJy uniform prior 
assumption on OJ is justified. 

386 
Inference about Means with Information from more than one Source 
7.2 
Size of H.D.P. regions 
From (7.2.55) and (7.2.56), we see that for any individual mean Bj , when J is large, 
length of (1 -
a) H.D.P. interval for random effect model = -( 1 _ 
FI ) 1/ 2. 
length of (l -
.:t) H.P.D. interval for fixed effect model 
On the other hand, for the Bj considered jointly 
volume of (I - a) H.P.D. region for random effect model 
volume of (1 -
Ct.) H.P.D. region for fixed effect model 
I 
(7.2.57) 
(7.2.58) 
For large J, (1 -
I / F)J/2 could be small even when F was large and (I _ I/ F)I,2 
was close to unity, so that it might seem at first that, for large J, a great increase in 
precision is to be expected from the use of internal evidence about (J~. To set this 
result in proper perspective, however, it should be remembered that the situation 
is exactly as if the standard deviation (J I of the errors ejk in (7.2.1) had been decreased 
by a factor (I -
I/F/,2. 
This would also result in the volume of the H.P.D. region 
heing reduced by the factor (I -
I/F)Ji2. 
Overall Analysis of Contrasts 
In the sampling theory analysis of the one-way classification, a useful preliminary 
to more detailed study is the overall test of the hypothesis of equality of the group 
means 9' = (B I , .. . , BJ ). This is accomplished by referring the ratio of mean squares 
m2/m l to a table of the F distribution, In Section 2,J I we have discussed the 
Bayesian analogue of this procedure. Given Ij>' = (cPt, .. . ,cPJ-l)' a set of] -
1 
linearly independent contrasts among the J parameters e, it was shown that the 
point Ij> = 0 was not included in the (I -
Ct.) H.P.D. region for Ij> if 
(7.2.59) 
or, for Vt large, if 
where, as before, F(] -
1, VI' a) is the upper a percentage point of a F distribution 
with [(1 -
I), VI] degrees of freedom and x2(1 -
I, a) is that ofaX2 distribution 
with (J -
1) degrees of freedom. This analysis was based on the assumption of a 
fixed effect prior. 
Consider now the corresponding result for the random effect prior of (7.2.36) 
and for the moment let us take cPt = Bt - e, ... , cPJ-I = BJ - I - e. 
Using the result (7.2,25), conditional on z, we see that the quantity 
(7.2,60) 

7.2 
Inferences about Means for the One-way Random Effect Model 
387 
where 
¢j = (I -
Z)(Yj. - yJ 
and 
is distributed as F with [(1 -
I), (VI + v2 )] degrees of freedom. Thus, when J 
is large, Z converges in probability to IfF = ml /m 2 provided F> 1 so that 
and approximately 
K L~= I [ifJj -
(1 -
IIF)(Yj. - yJ]2 
(J -
l)ml(l -
IIF) 
(7.2.61) 
(7.2.62) 
is distributed as F with [(1 -
I), (VI + v2)] degrees of freedom. Thus, for the 
point.p = 0 not to be included in the 1 -
Ci H.P.D. region, we require approximately 
that 
(7.2.63) 
that is, 
or to a further approximation, 
/11 2 
/.(1 -
1,::x) 
--1> 
. 
Inl 
J -
1 
Thus, for large J, the effect on inferences about the value .p = 0 of employing 
the random effect prior is approximately represente.d by subtracting one from the 
mean ratio before referring to the appropriate tables. 
The situation is illustrated in Fig. 7.2.3. An H.P.D. region for two orthogonal 
contrastst .p* = (ifJi,ifJi)' is shown for the fixed effect model centered about ~*. 
The region has radius r. 
For the random effect model, the corresponding H.P.D. region is centered 
about(l -
JiF)~*whi1etheradiusoftheregionisreduced by a factor (1 -
I;F)1/2. 
In the situation illustrated, the point .p = 0 is just outside the fixed effect H.P.D. 
region but is inside the corresponding random effect region. 
t The set of linearly independent contrasts ifJI = 81 - 8 .... ,ifJJ-I = 8J - I -
ti can 
always be transformed into a set of (J -
1) orthogonal linear contrasts having spherical 
H.P.D. regions. 

388 
Inference about Means with Information from more than one Source 
//--R~~~;-" 
" 
effect 
\ 
" 
" 
hxed-
\ effect 
/ 
\; 
I 
\ 
\ , 
I 
I 
I 
I 
7.2 
Fig.7.2.3 H.P.D. regions for orthogonal contrasts 4>i and ¢*2 with "fixed-effect" and 
"random-effect'· priors. 
7.2.7 Relationship to Sampling Theory Results 
We have seen earlier in Section 2.11 that, for the fixed-effect model , the standard 
sampling theory results parallel those obtained from a Bayesian analysis with a 
non informative prior which is locally uniform in O. By contrast, in this chapter 
a random effect Bayesian analysis of the means 8j has been explored and has led 
to intuitively sensible results. It is interesting, therefore, that in this case also, 
parallel sampling theory results exist. 
The "parallel" sampling theory comes out of the work of James and Stein 
(1961) who considered the problem of estimating a set of means 8 1"" ,8j of the one = 
way model (7.2.1) so as to minimize mean squared error. In general, let 9' = (e" 
... , ej ) be a set of point estimators for 0' = (8 1,,,
, 8J ) and let J - 1 L eej - 8y 
be the appropriate squared error loss function. Then, as discussed in Appendix 
A5.6, the mean squared error of e is the expectation 
(7.2.64) 
where E is taken with respect to the distribution of the data y. 
For simplicity let us suppose that O'T is known. Then the analysis proceeds 
as follows. If considerations are limited to e which are linear functions of the data, 
then it is well known that the minimum mean squared error point estimators are 
the sample means y: = (Y[, ... , YJ} Specifically, 
2 
-J 
2 
0'1 
M.S.E.(yJ = J 
E L (Yj. - 8J = K ' 
(7.2.65) 
However, these authors relaxed the unnecessarily restrictive assumption of 
linearity and considered instead for J ~ 2 estimators ej which are non-linear 

7.2 
Inferences about Means for the One-way Random Effect Model 
functions of the data sLich that 
that is, 
where 
( 
I ' 
I 
OJ' = 
1 - -)' ), + -
() 
, 
F' 
J. 
F" 
F' = 
K L (Yi - W 
(J -
2)O'f 
j = 1, ... , J. 
and () is any arbitrary constant. They then showed that 
• 
I 
• 
2 
O'~ ( 
I) 
M.S.E.(9) = J 
£ L(D j -
OJ) = K 
1- P' ' 
where 
and I F " is such that 
I 
O~-<I.. 
..... F" 
I 
lim -
=0. 
ii~ . oc F" 
389 
(7.2.66) 
(7.2.67) 
It was thus demonstrated that the non-linea r estimators (7.2.66) have the remark-
able sa mpling property that, whatever the true value of O'~, M.S.r.(9) can never 
exceed M.S.E. (yJ Note that O'~ is a measure of the spread of the means aj . 
Thus, the M.S.E. of 9 will approach the M.S.E. of y. when the means are spread 
out. 
For comparison, we see that with the sim plifying assumption that O'~ a nd 0 
are known, the posterior distribution or e is obtained by integrating the con-
ditional ;'\Iormal distribution for 0 in (7.2.12) over the distribution of O'~ in (7.2.14). 
Now for large J , the distribution of O'~ is sharply concentrated about its mode 
a-~ , where 
2 + K. 2 
K L (8 - Yi)2 I 
0'1 
0'2 = 
, 
(7.2.68) 
J + 2 
provided o-~ > O. It follows that, for large J, the OJ are approximately Normally 
distributed independent of one another with posterior means 8i such that 
8 
[ 
(J + 2)0'~ I 
0 
. 
j 
- e = 
I -
K L (Yi - W 
(Yi -
) 
that is, 
a = (I - -~))J ' + _1-0 
J 
F* 
J . 
F* 
where 
F* _ K L (y i. - W 
(J + 2)a~ 
(7.2.69) 

390 
Inference about Means with Information from more than one Source 
7.2 
and with common variance 
~(1 __ 1 ). 
K 
F* 
(7.2.70) 
Thus, the estimators in (7.2.66) which have remarkable sampling properties 
are of precisely the same form as the posterior means in (7.2.69) resulting from the 
Bayesian random effect analysis, the only difference is that J - 2 replaces J + 2 
. 
(J + 2) 
. 
(that IS, F' = (J _ 2) P). Indeed , Stein (1962) proposed that the appropriate 
non-linear functions to be used as estimators should be obtained from the posterior 
distribution with "suitably chosen" priors. 
For reasons explained in Appendix 
A5.6, we would not employ this approach but would feel that, having come so 
far towards the Bayesian position, one more step might as well be taken and the 
posterior distribution itself, rather than some arbitrary feature of it, be made the 
basis for inference. 
7.2.8 Conclusions 
1) Given an appropriate metric, a state of relative ignorance about a parameter 
can frequently be represented to an adequate approximation by supposing that, 
over the region where the likelihood is appreciable, the prior is constant, However, 
careful thought must be given to selecting the appropriate form of the model, and 
to deciding which of its characteristics it is desired to express ignorance about in 
this way. For example, suppose there are a number of parameters 8j which (i) are 
expected to be random drawings from a distribution or (ii) are expected to be 
successive values if a time series. Then it would be the parameters of (i) the distri-
bution or (ii) the time series, about which we would wish to express ignorance. 
It would not be appropriate in these cases to so express ignorance about the ()j 
directly, for this would not take account of available information (i) that the ()j 
clustered in a distribution or (ii) that the 8j were members of a time series. 
2) In this chapter we have discussed in some detail a situation of the first 
kind where the assumption of a uniform prior distribution applied directly to the 
8j might not be appropriate. Instead it was supposed a priori that the 8j were 
randomly drawn from a Normal distribution with unknown mean 8 and unknown 
variance (J'~. 
By employing noninformative priors for these parameters, the 
data themselves are induced to provide evidence about the location and dispersion 
of the 8j . 
These ideas are of more general application and are akin to those 
employed by Mosteller and Wallace (1964) and provide a link with the empirical 
Bayes procedures of Ro bbins (1955, 1964). 
3) Analysis in terms of a random effect prior is, of course, not necessarily 
appropriate (although, since clustering of means is so often to be expected, it is an 
important point of departure). Applications have been mentioned in which the 
means arranged in appropriate sequence might be expected to form a time series. 
In another situation two or more distinct clusters might be expected. By suitably 

7.3 
Inferences about Means for "'lodels with Two Random Components 
391 
choosing the structure of the model and introducing suitable parameters associated 
with noninformative priors, the data are invited to comment on relevant aspects of 
the problem. 
4) An important practical question is "How much does a correct choice of 
prior matter?" or equivalently "How sensitive (or robust) is the posterior distri-
bution of the e 
j to change in the prior d istri bution?" The Bayes analysis ma kes it 
clear that the problem is one of combining, in a relevant way, information coming 
from the data in terms of comparisons between groups with information from 
within groups. Now suppose there are J groups of K observations. The different 
priors reflect different relations be/lt"een the J groups. Thus if J is small compared 
with K one can expect that inferences will be robust with respect to choice of 
prior. However, when J is large compared with K, between group information wilJ 
become of relative importance, and the precise choice of prior will be critical. 
5) In general, Bayes' theorem says that 
p(8 I y) a.: p(8) /(8 I y) 
where y represents a set of observations and 8 a set of unknown parameters. Thus, 
given the data y, inferences about 8 depend upon what we assume about the prior 
distribution p(8) as well as what is assumed about the data generating model 
leading to the likelihood function /(8 I y). 
In Chapters 3 and 4, we studied the 
robustness of inferences about 8 by varying /(8 I y) to determine to what extent 
such commonly made assumption as Normality can be justified. We can equally 
well, as has been done here, study the effect on the posterior distribution of varying 
the prior distribution p(8). 
Examples of the latter type of inference robustness 
studies have in fact been given earlier in Section 1.2.3 and in Section 2.4.5. 
7.3 INFERENCES ABOLT MEANS FOR MODELS WITH TWO RANDOM 
COMPONE'JTS 
I n Section 6.3 we considered the analysis of cross classification designs and in 
particular the analysis of a two-way layout using a model in which the row means 
were regarded as fixed effects and the column means as random effects. 
It is 
often appropriate to use such a two-way model in the analysis of randomized 
block designs where the blocks are associated with the randoO) column effects 
and the treatments with the fixed row effects. Such a model contains two random 
components: one associated with the "within block error" and one with the 
"between block error." 
In Section 6.3 the error variance component and the between columns variance 
were denoted by (J; and (J; respectively. In what follows where we associate the 
random column component with blocks we shall use (J; and (J~ respectively for 
the error variance component and the between blocks variance component. 
I n practice, th·;' blocks correspond to portions of experimental material which 
are expected to be more homogeneous than the whole aggregate of material. 

392 
.:nference about Means with Information from more than one Source 
7.3 
Thus in an animal experiment the blocks might be groups of animals from the same 
litter. The randomized block design ensures that comparisons between the treat-
ments can be made II'ithin blocks (for example, between litter mates within a litter) 
and so are not subject to the larger block to block errors. 
When the available block size is smaller than the number of treatments, 
incomplete block designs may be employed- see, for example, Cocbran and Cox 
(1950), Kempthorne (1952). Thus six pairs of identical twins might be used to 
compare four different methods of reading instruction in the balanced arrangement 
shown in Table7.3.l. 
Table 7.3.1 
A balanced incomplete block design 
BLOCKS (Twin Pair) 
2 
3 
4 
5 
6 
TREATMENTS 
A 
x 
x 
x 
(method of 
B 
x 
x 
x 
reading 
C 
x 
x 
x 
instruction) 
D 
x 
x 
x 
The design is such that treatments A and 8 are randomly assigned to the first 
pair of twins, A and C 10 the second, and so on. In general, in a balanced incom-
plete block design (B I B D), I treatments are examined in J blocks of equal size K. 
Each treatment appears r times and occurs}. times ill a block Il'ith every other treat-
ment. In the reading instruction example I = 4, J = 6, K = 2, r = 3, and}, = 1. 
Designs of this kind supply two different kinds of information about the 
treatment contrasts. 
The first uses within (intra) block comparisons and the 
second uses between (inter) block comparisons. 
Thus one within-block 
comparison is the difference in the scores of the first pair of twins, which yields 
information about the difference between treatments A and B. The difference in 
scores of the second pair of twins similarly yields information about the difference 
between A and C, and so on. All comparisons of this sort are affected only by 
the within block variance a~. 
Analysis of incomplete block designs has sometimes been conducted as if 
tl1ese wuthin block comparisons were the onl)' source of information about the 
treatment contrasts. However, as was pointed out by Yates (1940), a second source 
of information abollt the treatment contrasts is supplied by comparisons betll'een 
the block averages. Thus the average score for the ilrst pair of twins, supplies an 
estimate of the average of the effects of treatments A and 8; the average score 
for the second block supplies an estimate of the average of the effects of treatments 
A and C. and so on. Comparison of these block average scores which have an error 
variance <J; +~<J; thus supplies further information abollt the treatment contrasts. 

7.3 
Inferences about Means for Models with Two Random Components 
393 
On sampling theory, estimation of the treatment contrasts using either (I) 
within block information only or, (ii) between block information only, is readily 
achieved by a standard application of least squares. It is, however, far from clear 
how these estimates ought to be combined and resulting inferences drawn. 
7.3.1 A General Linear Model with Two Random Components 
There is no particular reason to limit the initial discussion to a particular kind of 
design and we now consider a general linear model appropriate to any design 
having J blocks with fixed block siLe K. Specifically, we consider the linear model 
j = I, ... , J. 
(7.3.1 ) 
In this model Yj = (Ylj, · · , YK) is a K x I vector (or block) of observations, 
and Aj is a K x I matrix of fixed elements. The quantity a = (0], ... ,0,)' is a 
I x I vector of regression coefficients, bj is the jth block (random) effect having 
mean zero and variance a;, I is a K x I vector of ones, and ej = (e lj, ... , eKj)' 
is a K x I vector of random errors. 
[n this model the vector Aja is a "fixed" 
component of the vector of observations Yj and the vectors bjl and ej are two 
distinct random components. 
For an incomplete block design, a is the I x I vector of treatment means and 
i\ consists of 1 's and 0'5, each row of which contains a single 1 indicating which 
treatments are included in thejth block. Thus fo r the reading instruction design 
o 0] 
o ° ' 
[
0 0 
I 0] 
A6 = ° 0 ° I . 
7.3.2 The Likelihood Function 
We assume that bj and the elements of ej,j = 1 .... , J, are all independent and that 
(7.3.2) 
where 0 is a null vector and I is an identity matrix, both of size K. To derive the 
likelihood function of the parameters (9, a; , a;), it is convenient to work with the 
block averages and the within block residuals 
" 
K 
K - l }, 
K-I I 
)' , = 
v' = 
)' k ' 
,J 
• J 
J 
and 
k . I 
where R = K- 1 iT . 
The model in (7.3.1) can thus be alternatively expressed as 
Y.j = K - I~'Aja + bj + e j 
(7.3.3) 
(7.3.4) 

394 
where 
Inference about \1eans with Information from more than One Source 
K 
e j = K- I I'ej = K- I I ekj 
k = I 
7.3 
It follows from the Normality and independence assumptions of hj and ej that 
a) the tw~o sets of random variables {Y,j}, {(I -
R)Yj} are independent of each 
other, 
b) Yj are independently distri buted as Normal N (K - II' A }l, 0"; + K - J 0";), and 
c) (I -
R)Yj 
are independently Normally distributed 
with 
mean 
vector 
(1 -
R) Aje and a singular covariance matrix 0";(1 -
R), 
Thus, the likelihood function can be expressed as the product 
I(e, 0";, 0"; I y) = Ib(9, O";e I Y,j' j = I, , .. , J)/e(9, 0"; I (I -
R)Yj' j = 1, .. " J), (7,3.5) 
where the first factor is 
with 
and 
Ib(9, O";e I Y,j' j = I, .. " J) ex p(Y,j' j = I, .. " J I e, 0";.) 
J 
Sb(e) = K I (Y ,j - K- 1l'Aj e)2 , 
j=1 
(7.3 ,6) 
(7,3,7) 
(7.3.8) 
For the second factor in (7.3,5), we first observe that (1 - R)I = 0 and the matrix 
(I - R) is idempotent of rank K -
I, Thus there exists a K x K orthogonal 
matrix P, where 
K-I 
P=[P*IK-;l] 
such that 
, 
[I K - I 
. 
P (I - R)P = 
." 0' .. . 
~ j. 
(7.3.9) 
Let 
(7.3 ,10) 
Then the (K -
I) x I vector x j is 1\ ormaJJy distributed as N K _ 1(0, Ik -1 a;). 
Noting that 
xjXj = (Yj - Ajenl - R)(Yj -
Aje), 
the second factor of the likelihood (7,3.5) is found to be 
Ie (e, a; I (I - R)Yj' j = I, .. " J) ex p(x j , j = I, .. " J I a;) 
( 2)-'J(K-I) 
[ 
1 Se(e)j 
ex 
O"e' 
exp - ---,- , 
2 
(J; 
(7.3.11) 
(7.3.12) 

7.3 
Inferences about :Yleans for :Vlodels with Two Random Components 
395 
where 
J 
Se(9) = L (Yj -
Aj9)'(1 - R)(Yj -
Aj9). 
(7.3.13) 
j = J 
Now, we can write 
Sb(9) = Sb + Qb(9), 
(7.3.14) 
where 
Qb(9) = (9 - ey (JJ AjRAj ) (9 - e) 
and e satisfies the normal equations 
(JI AjRAj)e = JI A i lY .j 
(7.3.15) 
Similarly, we can write 
Se(9) = Se + Qe(9), 
(7.3.16) 
where 
QeC9} = (9 - 8y lJI Aj(1 -
R)Aj 1 
(9 - 8) 
Se = Se(8) , 
and 8 satisfies the normal equations 
[JJ Aj(1 -
R)Aj J e = Jl Aj(1 - R)Yj . 
(7.3.17) 
It is convenient to arrange the quantities Sh' Qb(9), Se, and QeC9) and to 
indicate sampling distributional properties in the familiar form of an analysis 
of variance as in Table 7.3.2. 
Table 7.3.2 
Analysis of variance of the linear model with two random components 
Sampling distribution 
of S.S. 
Sources 
S.S. 
d.f. 
(all independent) 
Between 
Qb(e) 
C/b 
2 
2 
0' beXq" 
(inter) 
blocks 
Sb 
J-qb 
0'2 X2 
be J - qb 
Within 
Q.(9) 
qe 
O';X;c 
(intra) 
blocks 
Se 
J(K - 1) -qe 
a;X7(K-I )-qr 
In Table 7.3.2, qb and qe are, respectively, the rank of the matrices 
J 
J 
L AjRAj 
and 
L Aj(1 - R)Aj . 
j = 1 
j= J 

396 
Inference about Means with Information from more than one Source 
7.3 
7.3.3 Sampling Theory Estimation Problems Associated with the l\'1odel 
/ 
In the sampling theory framework, the usual difficulties are associated with making 
inferences about the variance components 0"; and 0";. 
The unbiased estimator 
of 0"; which has often been used 
Se 
(7.3.18) 
K[J(K -
1) - q.] 
is intuitively unsatisfactory because it can take negative values. In addition, the 
sampling distribution of 6~ involves the nuisance parameter O"i,'0",2, and con-
sequently leads to difficulties in constructing confidence intervals for a;. Finally, 
while inkrences about a; are often based upon Sc alone, yet if (5~ / 0"; is not large, 
Sb can also contribute appreciable informatiun about (5;. Attempts to deal with 
this problem by "pooling variances" are accompanied by familiar problems 
previously discussed in Chapters 5 and 6. 
Usually the parameters of principal interest are the elements of the vector 
o or linear functions of them . hom Table 7.3.2 we see that if the inter-block 
information was ignored , confidence regions for 0 could be constructed by referring 
the quantity 
Se/[J(K -
I) - qe] 
(7.3.19) 
to an F distribution with [q,. J(K -
I) - qeJ degrees of freedom. Similarly, if 
the intra-block information was ignored, a different set of confidence regions for 
9 could be obtained by referring the quantity 
Qb(9)iQo 
Sbl(J -
qb) 
to an F distribution with (qb, J -
qo) degrees of freedom. 
(7.3.20) 
In attempts to combine intra- and inter-block information the major problem 
for sampling theory is the difficulty of eliminating the nuisance parameter (5;. (5;, 
see, for example Yates (1940), Scheffe (1959, pp. 170-178), Graybill and Weeks 
(1959), and Seshadri (1966). 
We now consider the problems from a Bayesian 
viewpoint and discuss in detail the analysis of balanced incomplete block designs. 
7.3.4 Prior and Posterior Distributions of «(5;, o";c, 9) 
From (7.3.5), (7.3.6) and (7.3.12) we see that information contained in the likeli-
hood function can be regarded as coming from J independent observations Yj 
from Normal populations with variance (5t.IK , and J(K -
J) independent obser-
vations drawn from Normal populations with variance (5:. The means of these 
observations are linear functions of the elements of the vector 9. 

7.4 
Analysis of Balanced Incomplete Block Designs 
397 
As In Chapters 5 and 6, we carry through the analysis llsing the noninformative 
reference prior distribution 
(7.3.21) 
where, since (J;e = (J;' ~ K(J;, the constraint (J~r > (J; is implied. Combining (7.3.5) 
with (7.3.21) we obtain the posterior distribution as 
p((J;,(J;"O I y) ex ((J~e) - ( ! J+ \) ((J;) - (}J(K 
I ). \) 
{
I [ Sb + QbCO) 
Se + Qe(O) J \ 
x ex p 
~ -
2 
+ 
2 
I ' 
2 
(Jbe 
(J, 
(JL>(J;>o, 
~ XJ<8i<OO, i=I, ... ,J. 
(7.3.22) 
We shall not proceed further with the general model but consider the especially 
important case of balanced incomplete block designs. 
7.4 ANALYSIS OF BALA'\JCED INCOMPLETE BLOCK DESIGNS 
As we have noted earlier, in a balanced incomplete block design (BIBD), J 
treatments are examined in J blocks of equal size K. 
Each treatment appears 
r times and occurs ), times in a block with every other treatment. For any such 
design, we must have 
J ~ 1, 
JK = Ir, 
),(1 -
I) = r(K -
I). 
(7.4.J) 
In terms of the general model (7.3.1), Aj is a K x I matrix of I's and O's, each 
row of which contains a single 1 in the column corresponding to the treatment 
applied. Clearly Aj consists of orthogonal rows. The I x J matrix 
(7.4.2) 
is known as the incidence matrix or the design matrix whose elements consist of 
l's and O's indicating the presence and absence of the I treatments in the J blocks. 
For the instruction design in Table 7.3. I, the incidence matrix is 
Block 
2 3 4 
5 J=6 
Treatment 
L 
~ L 
I 
1 o 0 
0 1 
2 
I 
0 0 
I 
0 
3 
0 
0 
I 
0 
1=4 
0 0 
0 
7.4.1 Properties of the BIBD Vlodel 
The BIBD model has the following properties: 
/ 
J I AjAj = rI, 
(7.4.3) 
j
' ~ I 

398 
Inference about Means with Information from more than one Source 
7.4 
(7.4.4) 
(7.4.5) 
;f;. 
, 
J../ 
I' 
L A/I - R)Aj = -
(I, - r 1,1,) 
j=l 
K 
(7.4.6) 
J I Aj l d 'j = rB, 
(7.4.7) 
j= I 
J I Aj(I -
R)Yj = reT -
B), 
(7.4.8) 
j~J 
I~T = lIB = iy 
(7.4.9) 
where I, is an I x 1 identity matrix , I, is an i x 1 vector of ones, B' = (BJ' ... , B,), 
Bi is the average of Y.j for blocks) in which the ith treatment appears, T' = (T1' ... , 
T,) is the vector of treatment averages, and Y .. is the grand average. 
BreakdolVl1 oj Sb(S) 
Using (7.4.4), (7.4.5), (7.4.7), and (7.4.9), the quantity Qb(S) in (7.3.14) can be 
written 
QbCS) = ( r ~ A )J! (8i - ea
2 + ~ [J! (8; - 8Jr, 
(7.4.10) 
where 
~ 
I 
8 = -- (rK B -
My ) 
, 
(r -
},) 
, 
.. , 
r > I,. 
(7.4.11 ) 
In a BIBD, it is the comparative values of the 0; which are of primary interest. 
fn this connection it is convenient to work with the contrasts 
<P; = 8; - e, 
i = I, ,. ,I, 
(7.4.12) 
I 
I 
where 
t1=/ - IIO; 
so that 
I ¢i = 0, 
and to write 
i= J 
i= L 
(7.4.13) 
with 
~ 
rK 
¢ = --(B - Y ) 
I 
r _ }1. 
f 
" 
, 
r > j,. 
(7.4.14) 
-
-~ 
, 
,

7.4 
Anal~' sis of Balanced Incomplete Block Designs 
399 
Since Sb(a) = S6 + Qb(a) is true for all a, by setting 0 = 0 (which implies that 
<Pi = 0, i = I, ... ,1), we obtain from (7.3.8) and (7.4.13) that 
J 
2 
2 (r - )') I 
-2 
S6 = K I Yj - rly - -- I 4>i 
j = 1 
K 
;= 1 
=KI (Yj - Y ) - -- I(p;· 
J 
2 
(' r -
), ) 
I 
-2 
j = 1 
K 
i = 1 
(7.4. I 5) 
Breakdoll'l1 of Se(6) 
From (7.4.6) the quantity Qe(9) ill (7.3.16) can be written 
}.Jr~ 
'2 
l
~ 
" J2} 
Qeca) = -
L., (0; -
e;) -
[ . 1 
,.~
_
, (e; -
0;) 
K k, 
. 
(7.4.16) 
),1 
~ 
, 
2 
= K 
i~l (4); -
<p;) , 
where 4>; is defined in (7.4.12), 
1 
(fi; = 8; - 1- 1 L 81 
(7.4. 17) 
hI 
and e = (8 1, .",01)' satisfies the normal equations (7.3.17). 
It follows from 
(7.4.6), (7.4.8), and (7.4.9) that 
, 
rK 
6=-(T-B) 
(7.4.18) 
n 
is a particu lar solution of (7.3.17) for the BI BD and that 
, 
, 
rK 
4>; = 0; = li 
(~ - B;), 
(7.4,19) 
i = I" 
/ . 
Consequently, we may write Se(O) in (7.3.16) as 
. 
n ~
, 
2 
SeeS) = Se + K ;~l (4); -
<p;) 
(7.4.20) 
with 
J 
K 
2 
J 
2 
).j 
1 
'2 
Se = L L hj -
K L )'J -
-
L <P; 
j " lk = 1 
j~1 
K ;-1 
(7.4.21) 
The expressions for (fii' (fi;, Sb' and Se given above are, of course, well known . 
We have sketched the development in order to relate the present results for the 
BIBD to those for the general linear model set out in Section 7.3. 

400 
Inference about Means with Information from more than one Source 
7.4 
The Analysis of Variance 
The breakdowns of SbCfI) and SeCfI) are summarized In Ta ble 7.4.1 Il1 the usua I 
analysis of variance form. 
Sources 
Between 
(inter) 
blocks 
Within 
(intra) 
blocks 
Table 7.4.1 
Analysis of variance of a BIBD model 
S.S. 
d.f. 
c5(J -1) 
(J-I)-o(l-I) 
/-1 
J(K-l)-(l-l) 
In Table 7.4.1, 0=1 if r>/. and 0=0 if r=; .. 
Sampling distribution 
of S.S. 
(all independent) 
7.4.2 A Comparison of the One-way Classification, RCBD and BIBD Models 
It is instructive to relate these results to those for the additive mixed model 
(Section 6.3) and those for the one-way classification (Sections 2.1 J and 7.2). 
If, as 
we shall suppose, blocks may be represented as a "random effect, " then the additive 
mixed model is appropriate for the analysis of the randomized block design. We shall 
call the latter the randomized comp/ele block desigl1 (ReB D) to distinguish it from 
the BIBD. 
For a BlBD, the model in (7.3.1) may be written 
j = I, . . , J; 
k = 1, ... , K, 
(7.4.22) 
where Ykj is the kth observation in the jth block, bj the block effect and ekj the within 
block residual error. The expectation E(Yk) is 
(7.4.23) 
if the ith treatment, i = I, .. . , I, is applied to Ykj ' Alternatively, we may write the model 
as 
i=I, ... , I; 
m=J , ... ,r 
(7.4.24) 
where Y(im) is the !11th observation receiving the ith treatment and £Um ) is the error 
term such that 
(7.4.25) 

7.4 
Analysis of Balanced Incomplete Block Designs 
401 
if Y(im) happens to be in the kth position of thejth block . In other words, (7.4.22) and 
(7.4.24) show that we can classify the J K = rl observations in two different ways, one 
according to the blocks and the other according to the treatments. 
If there is no block effect, bj == 0, then the model in (7.4.24) is the one-way classi-
fication model discussed earlier with the appropriate change in notation. On the other 
hand, if the size of a block equals to the number of treatments, K = I, and every 
treatment appears in every block, so that J = r = }., then the model in (7.4.22) is 
i= 1, ... ,1; 
j= t, .. . ,} 
(7.4.26) 
which is the ReBD model given in (6.3.1) upon substituting hj for Cj' 
Now, the sample averages needed to compute 4>i, 4); and the sum of squares in 
Table (7.4.1) are 
a) Y. = Y( .. ) 
b) Ti 
= Y(i.) 
c) Yj 
d) Bi 
grand average 
treatment averages 
block averages 
average of block 
averages 
for 
blocks 
In 
which 
the 
ith 
treatment appears. 
Note that when bj == 0 or K = I,Bi == Y .. , otherwise rl LiBi = Y .. ' 
Table 7.4.2a compares the point estimates of the contrasts ¢i = Di - 8, i = I . .. . , I, 
for the one-way classification, RCBD and BIBD models. 
While the estimate Ti -
Y .. is the same for both the one-way classification and the 
RCBD, it is regarded as an "intra-block" estimate for the latter because 
T - Y 
= ~ I (Y' - Y .) 
1 
.. 
} 
j 
I} 
.} 
(7.4.27) 
that is, because it is the average of J within block contrasts among the observations. 
For the B[8D, there are two sources of information about ¢i' the "intra-block" estimate 
4)i and the "inter-block" estimate 4>i' Let 
Ii) 
Y. m 
11/ = I, ... , I' 
(7.4.28) 
denote the r block averages in which the ith treatment appears. Then, the "intra-
block" estimate 4); of ¢i is proportional to 
I " 
( 
T, -
Bi = -
L (Y(i",) -
Y:~) 
r 
m 
(7.4.29) 
which is made up of r within block contrasts among the observations. On the other 
hand, the inter-block estimate ;Pi is proportional to 
B i -
Y .. 

402 
Inference about Means with Information from more than one Source 
7.4 
representing a contrast among the block averages. 
The cOnstants of proportionality 
( '-1 and (I _/)-1 are such that 
J.I 
f 
. = rK' 
1-1= ( rK-}.J)=(~), 
rK 
rK 
o < I~ I. 
(7.4.30) 
The quant ity 1 is sometimes called the "efficiency fact or" of a BIBD. When K = / 
and r = },J= I and the BIBD reduces to the RCBD. In this case 
(7.4.31) 
so that, as far as sampling theory is concerned, there is only one source of information 
about (p;. 
Table 7.4.2 
Comparison of one-way classification, RCBD and BIBD models 
a) Point estimates of the contrast ¢i = OJ - &: 
One-way classification 
RCBD 
BIBD 
I 
[ nter-block 
--
¢= -
-
(B·- Y ) 
I 
I -I 
I 
.. 
Tj-Y. 
I 
Intra-block 
T i - Y. 
<Pi = 7 (Ti -
Bi) 
b) Decomposition of the sum of squares L L (Y(im) - ei: 
Source 
One-way classification 
RCBD 
BIBD 
Grand mean 
r/C& - y/ 
rl(D- Y.l 
1'/(8 - yj 
inter-
(t - I)r"i(¢i - ¢i 
-
block 
Treatment 
rI{¢i- (Ti -y.)]2 
intra-
rI.[¢i- (Ti -y)]2 
IrI.(¢i - <Pi 
block 
inter-
KI.(Yj-Y )2 
KI.(Yj - Y.Y 
block 
-
(I - f)rI.¢2 
Residuals 
I. I. (YUm) - Ti 
intra-
I. I. (Ykj _ y )2 
I. I. (Ykj - Y.Y 
block 
-rI.(Ti-y./ 
-frI.<p; 

7.4 
Analysis of Balanced Incomplete Block Designs 
Table 7.4.2b gives a corresponding comparison of the decomposition of the sum 
of squares for the three models. The reader will have no difficulty in associat ing the 
entries in the table with the corresponding ones in Tab.le 2.11.1 for the one-way 
classification, in Table 6.3.1 for the RCBD and in Table 7.4.1 for the BIBD. 
7.4.3 Posterior Distribution of G> = (<p I , ... , </> I _ I)' for a BIBD 
The quantities of principal interest are contrasts between the treatment parameters. 
If in (7.3.22) we make the transformation from 0, .... , 01 to[}and the 1 -
1 linearly 
'independent contrasts </>1 = 01 - O. i = I, .... 1 -
I, then upon substituting for 
Qb(9) and Qe(9) into (7.3.22) from Table 7.4.1 and integrating out 8, we obtain 
- cc< </>;<oc , 
i = I, ... , 1 -
I; 
o <(J; < (J~c < 00, 
(7.4.32) 
where and in what follows it is to be remembered that </>1 = 
-
(</>, + ... + </>1 - ')' 
In (7.4.32), use is made of the definition of the efficiency factor f = 21(rK) in 
(7.4.30). Applying the identity (AS.2.1) in Appendix AS.2 to integrate out (J; and 
(JL. we have 
I
' 
I 
J-HJ - ') 
X _ Sb + (I - /)r JI (</>; -
r1>Y 
r~ J(K - I)] 
1"(<1» l 2' 
2 
' 
-
J) < </>; < ::I), 
I = I, .. . , 1 -
I , 
(7.4.33) 
where 
and, as before, 1.(p,q) is the incomplete beta function. The posterior distribution 
is the product of three factors. The first of these is in the form of a (I -
I) 
dimensional multivariate I distribution centered at <i> = (4)\ ... .. 4)1 - 1)" with co-
variance matrix proportional to (1 1 _ 1 -
I - I I, 11;_,). 
For r > }" the second 
factor again takes the form of an (I -
1 )-dimensional multivariate I distribution 
centered at <i> = (r1>" .... r1>1 
I)', with covariance matrix also proportional 
[0 
(II - J -
/ - 1 11 _ 11; _ 1)' 
The third factor is an incomplete beta integral whose 
upper limit depends on G>. We note that the distribution p(G> I y) is a multivariate 
generalization of a form of distribution obtained in Tiao and Tan (J966, Eqn. 
7.2). 

404 
Inference about "leans with Information from more than one Source 
7.4 
Relationship to Previous Resulls 
The first factor in (7.4.33) is equivalent to the confidence distribution of IjJ in the 
sampling theory framework if only the intra-block information is employed. 
It 
would also be the appropriate posterior distribution of <l> if only the portion of the 
likelihood corresponding to "within-block" was used in conjunction with a non-
informative uniform prior in log a,,, (a. > 0). 
For r> }" the second factor is 
equivalent to the confidence distribution of IjJ if only the inter-block information is 
employed. [t would also be the appropriate posterior distribution of IjJ if only the 
portion of the likelihood corresponding to "between-blocks" was used in con-
junction with a noninformative uniform prior in log abe' (abe> 0). 
The product of the first two factors provides a blending of intra- and inter-
block information to form a posterior distriblltion for 4J which would be 
appropriate if the constraint (J;e> 13; could be ignored. 
It is clear that the 
distribution of IjJ based upon the first and second factors would be centered between 
the centers of the two separate multivariate I distributions.t 
The third factor arises as a direct consequence of the constraint 13;. > 13;, the 
effect of which is to pull the distribution of IjJ towards the multivariate f distribution 
given by the first factor. 
To see this, consider the density constrained un oily straight line in the space 
of the variables 4J. Any straight line can be written in parametric form 
4J = (I -
11)1jJ1 + p1jJ2, 
where <l>1 and 1jJ2 are any two points. 
Substitution of IjJ into (7.4.33) yields the form 
p(pi y) cr:. [a~ + deep -
Ce)2] .. ;-J(/\ - I) [ob + db(~1 -
Cb)2] -
~ iJ - I) 
[
J- I J(K- I)] 
x i"!iJ) -2- ' 
2 
' 
- co < t~ < 00, 
(7.4.34) 
where 
Now ou(p)/op ~ 0 in the interval between Co and cb as c,. ~ Cb. 
Remembering that 
1"!iJ)Ll(J- I),·P(K- I)J is monotonically increasing in u(jI), the effect of the this 
function is to give more weight to the first factor in determining the center of the 
complete distribution p(p I y). Since the line we have chosen is arbitrary, it follows that the 
function 1,,(!') U(J -
I), }J( K -
I)] gives more weight to the first factor of p(1jJ I y) than 
to the second. 
Finally, in the special case r = Jc, namely, a randomized complete block 
design model, the distribution in (7.4.33) degenerates to that given' in (6.3.45) 
t Other results of the same kind will be considered further in Chapters 8 and 9 when 
combining the information about common regression coefficients from several sources, 
in cases where no variance restrictions exist. 

7.4 
Analysis of Balanced Incomplete Block Designs 
405 
with appropriate changes in notation. In this case, the second factor is a constant 
and the third factor, which is now monotonically decreasing in L!= 1 (</>j - ({>Y, 
tends to make the distribution more concentrated about .j) than it would be if 
only the first factor alone were considered. 
7.4.4 An Illustrative Example 
To illustrate the theory we consider an example involving I = 3 treatments, 
J = 15 blocks of size K = 2, each treatment replicated r = 10 times; thus A = 5. 
Table 7.4.3 shows the data with some peripheral calculations. For convenience 
in presentation, we have associated the blocks (random effects) with rows 
and the treatment (fixed effects) with columns. The data was generated from a 
table of random Normal deviates using a BIBD model with 8 1 = 5, 82 = 8) = 2, 
(J"; = 2.25, and (J"; = 1. 
For this example the distribution in (7.4.33) is, of 
Table 7.4.3 
Data from a BIBD (1= 3,J= 15, K= 2, r= 10, A= 5,/=0.75) 
Treatment 
2 
3 
y . 
. j 
I 
10.05 
4.92 
7.485 
2 
10.10 
5.49 
7.795 
3 
5.52 
2.97 
4.245 
4 
9.90 
3.72 
6.810 
5 
9.99 
5.68 
7.835 
6 
5.93 
0.24 
3.085 
7 
6.76 
3.80 
5.280 
Block 
8 
7.94 
2.90 
5.420 
9 
11.44 
2.10 
6.770 
10 
9.74 
3.94 
6.840 
11 
0.79 
4.83 
2.810 
12 
6.09 
4.41 
5.250 
13 
5.78 
6.47 
6.125 
14 
3.22 
5.30 
4.260 
IS 
5.78 
4.18 
4.980 
T; 
8.737 
4.444 
3.817 
y .. = 5.666 
Bj 
6.1565 
5.7595 
5.082 
Se = 29.1841 (13 d.f.) 
({>j 
3.4406 
- 1.754 
-1.6866 
Sb = 49.04448 (12 d.f.) 
;Pj 
1.962 
0.374 
-2.336 

406 
Inference about Means with Information from more than one Source 
7.4 
course, a bivariate one. Note that 
3 '\ 
-
2 
2 
2 
L (¢i - ¢) = 2[(¢I -
3.4406) + (¢2 + 1.754) + (¢l -
3.4406)(¢2 + 1.754)J 
i= 1 
3 '\ 
-
Z 
1 
2 
L (¢i - ¢J = 2[(¢1 -
1.962) 
-j- (¢2 - 0.374) 
--I- (¢I -
1.962)(¢2 - 0.374)J 
i ~ 1 
Figure 7.4.1 shows a number of contours related to the distribution (7.4.33). 
l. The contour (I) centered at PI = (3.44, -1.75) is the 95 per cent H.P.D. region 
derived from the first factor alone. It was calculated usi ng the fact that, if the first 
factor alone were the posterior distribution, then the quadratic form 
jr'L[=I (¢i - <f;Y/(J - I) 
Se/[J(K -
I) -
(J -
I)J 
would be distributed as an F with (I -
I) and J(K -
I) -
(J -
I) degrees of 
freedom. 
2. The contour (2) centered at Pz = (1.96,0.37) is the 95 per cent H .P.D. region 
derived from the second factor alone. It was calculated by referring the quantity 
(l - f)r'Lf=l (¢i - JJY/U - 1) 
Sb f(J - 1) 
to an F distribution with (J -
I) and (J - J) degrees of freedom. 
3. The broken lines (3) are contours of the third factor, with the contour values 
shown. 
4. The contour (4) centered at P = (3.26, -1.50) defines the approximate 
95 per cent H.P.D. region derived from the complete distribution (7.4.33). It was 
calculated from the formula 
logp (¢r, ¢i I y) -
log!, (¢l' ¢zl y) = 5.99 = X
2 (2, 0.05) 
where (¢i, ¢i) are the coordinates of P, the center and maximum point. This is 
equivalent to 
p(¢J' ¢21 y) = 0.05p(¢j, ¢i I y). 
We can see from Fig. 7.4.1 that, as expected, the overall distribution contour 
curve (4), is located between the contours (I) and (2) from the first and second 
factors respectively. In fact the center P is practically collinear with the other 
centers P J and Pz. Contours (1) and (2) are elliptical and have parallel axes, since 
the covariance matrices of the two corresponding bivariate t distributions are 
proportional. Contour (2) is much larger than contour (I) essentially because, 
for this example, 
Sb/[(J - 1)(1 - j)J ~ Se/{f[J(K -
I) -
(J -
I)]} 

7.4 
Analysis of Balanced Incomplete Block Designs 
407 
implying that the first factor will have a dominating role in determining the overall 
distribution. The lines (3) show that the third factor is increasing in a South-east 
direction confirming that this factor tends to pull the final distribution of 4> 
towards the bivariate I distribution given by the first factor. However, for this 
example, the effect is quite small, as is seen by the slight changes in the value of 
the function over the region in which the dominating first factor is appreciable, 
The shape of contour (4) is very nearly elliptical which suggests (as turns out to 
be the case) that the distribution might be approximated by a bivariate t distri-
bution. 
(2) 
o 
2 
o 
2 
4 
In the figure 
J. 95 per cent contour of 41 based on first factor of (7.4.33) 
2. 95 per cent contour of cP based on second factor of (7.4.33) 
3. Contours of Q for various values of 1u(4J) H(J-
I), ~·J(K - 1») as marked 
4. Approximate 95 per cent contour of the entire distribution of cP in (7.4.33) 
Fig.7.4.1 Contours for the components of the posterior distribution of (cPl' cPz) for the 
BIBD data. 
7.4.5 Posterior Distribution of at./(J; 
Tn this section, we digress from our discussion of the problem of making inferences 
about the contrasts <p to obtain the distribution of the variance ratio 
(7.4.35) 
which plays an important role In approximating the distribution of <p. 

408 
Inference about 'Vieans with Information from more than one Source 
7.4 
In the joint distribution peer;, er;e, <I> I y) In (7.4.32) we may use the identity 
(A7.1.1) in Appendix A7.1 to write 
I 
I 
erb'/ (I - f)r L C¢i - 4>Y + er;2/r L C¢i - CfiY 
i= 1 
i= 1 
{( I-I) I 
_ 
(1- /)-I} 
= er;:-2 
1+ -w- r JI [¢i -
¢JIV)]2 + 
IV + -1-
S" 
(7.4.36) 
where 
I 
S, = (1-f),. L (4)i - CfiY 
(7.4.37) 
i= 1 
and 
-
( l-/)-I[ - (1-/)_J 
(p;(w) = 1+ -w-
I¢i + -19- ¢i . 
(7.4.38) 
Making the transformation from (er~e> 0';) to (It', er;) and integrating out 0';, the 
posterior distribution of (11', <1» is 
pC w, <I> I y) if. W - [-}(J -
J ) + I] 
{ 
( 
1 -/)-1 
( 
I -I) 
J 
}-[t(JK-I)] 
x 
Se + W-ISb + 
w + f 
S, + 1+ -w-
r i~1 [¢i -
qJi(W)]2 
, 
-
00 < ¢i < 00, 
i = 1, ... ,1 -
1, 
w> J. 
(7.4.39) 
Upon eliminating <I> by integration and recalling from Table 7.4.1 that b = 1 if 
,. > }, and b = 0 if r = 2, the posterior distribution of IV is 
p(w I y) if. HI (w)H 2(W), 
IV> 1, 
(7.4.40) 
with 
( 
S 
)-H/(r-l)-6(1-1») 
HJCw) if. W~[J(K-I)-(l-l»)-1 1 + S: w 
and 
( 
w 
)t(l-I) ( 
So) -·,6(/-1) 
H2(W) X 
1+-1-1' 
IV + (I -nIl 
Sb 
x 
1+ 
- ' 
l+_e w 
[ 
( 
W 
) (S ) ( S )-IJ-+/(r-l) 
w + (1 - I)!f 
Sb 
Sb 
. 
This distribution is constrained in the interval (l, OJ) and is proportional to the 
product of two factors HI (IV) and H 2(11'). The constraint w > 1 arises from the 
inequality O'~e > 0';. If there were no constraint, the factor HI (w) would be propor-
tional to an F distribution with J(K -
I) -
(1 -
I) and (J -
I) - b(I -
I) 
degrees of freedom and would represent a quantity proportional to the confidence 
distribution of w based on Se and Sb' 
It would also be proportional to the 
posterior distribution of w based upon the portion of the likelihood relating to 

7.4 
Analysis of Balanced Incomplete Block Designs 
409 
Sb and Se alone in conjunction with a uniform reference prior In log IJbe and 
log IJ e' 
For r > A, the second factor H 2 (IV) occurs because the quanti ties <I> and <ii 
are estimating the same parameters.p. 
Thus, both the constraint II' > 1 and 
the factor H leW) can be thought of as resulting from the additional information 
supplied by the BIBD model. 
To illustrate, consider again the data of Table 7.4.3. Curve (1) in Fig. 7.4.2 
gives the posterior distri bution of w in (7.4.40). Curve (2) in the same figure re-
presents the confidence distribution of II' based upon HI (w) . Finally, the curve 
(3) would be the appropriate distribution of w if the constraint II' > I were ignored . 
For this example, then, the effect of both the constraint and the second factor 
appears to be appreciable. 
In particular, if our inferences about w were based 
upon HI (II') alone, then the usual 95 per cent interval, Bayesian or otherwise, 
would have its lower limit less than unity which is a priori unacceptable. F inally, 
comparing Fig. 7.4.1 and Fig. 7.4.2 we see that while the effect of the constraint 
w > 1 is important insofar as inferences about ware concerned, it has much less 
effect on inferences about the contrasts <1>. 
7.4.6 Further Properties of, and Approximations to, p(.p I y) 
We now return to our discussion of the posterior distribution p(<I> I y) in (7.4.33). 
In the analysis of data from a balanced incomplete block design, we are often 
p(WI Y) 
0.750 
0.625 
0.500 
In the figure 
1. Posterior distribution of W in (7.4.40) 
2. Posterior distribution of w based on H,(w) alone, also a confidence distribution of w 
3, Posterior distribution of IV ignoring the constraint IV > 1, 
Fig. 7.4.2 Posterior distributions of w for the BIBD data, 

410 
Inference about Means with Information from more than one Source 
7.4 
concerned with the problem of making inferences about one or more linear con-
trasts of the treatment means OJ. 
Since the ¢j are themselves linear contrasts, 
a contrast in the 01 can be expressed as a contrast of ¢l' .,., ¢/. We are thus led to 
the problem of finding the marginal distributions of linear contrasts of ¢I' .'" ¢/. 
From the joint distribution (7.4.33) it is clear that the conditional distribution 
of a subset of ¢l ' "., ¢I_I' given the remainder, is of exactly the same form as 
the original distribution. 
However, the marginal distribution of a subset is 
not To obtain the marginal distributions, it is helpful to write the joint distribu-
tion of 41 in the alternative form , 
p(q,ly) = f
X) p(q" w,y)p(wly)d\\', 
i = 1, .'" 1 -
1, 
(7.4.41) 
where p(wly) is given in (7.4.40). From the joint distribution pew, q,ly) in (7.4.39) 
it is readily seen that, given II', the conditional distributionp(q, I \-I', y) is the (J -
J)-
dimensional multivariate I distribution 
(7.4.42) 
where 
and 
( 1-1) 
( 1-/)-1 
I(r -
l)rl w + f 
S2(W) = Sb + wSe + w w + f 
Si, 
(7.4.43) 
Note that from (7.4.38), the conditional posterior mean of 4>;, given w, 
is a weighted average of the intra- and inter-block estimates ($j, $;) with weights 
proportional to I and (l - I)/w respectively. 
From (7.4.41) and properties of the multivariate t distribution, the marginal 
distribution of any set of p linearly independent contrasts l] = HI = L( 41' : ¢/)', 
where L is a p x I matrix of rank p ,:; (I -
1) such that LII = 0, is 
pel] I y) = f" 
pel] I w, y)p(w I y) dw, 
-
00 < l]j < 00, 
i = 1, .. "p, 
(7.4.44) 
where pel] I w, y) is the Ip[iJ(W), s2 (w)LL', I(r -
I)J distribution with 
ft(w) = L[(ji'(lV) . cPl(I1·)]'. 
In particular, the posterior distribution of a single contrast 1] = e'l = (41' • ¢/)l is 
p(YJ I y) = lO 
p(1] I IV, y) p(l'.' I y) dw, 
-oc < n<oo, 
(7.4.45) 

7.4 
where, for given w, the quantity 
Analysis of Balanced Incomplete Block Designs 
IJ - iJ(w) 
s(w)(l'l): 
follows the teO, I, f(r -
I)J distribution. 
411 
(7.4.46) 
Although the conditional distribution of TJ given w is of the multivariate t 
form, it does not seem possible to express the unconditional distribution in terms 
of simple functions when p < (I -
1). However, for given data, the unconditional 
distribution can be obtained by a one dimensional numerical integration. 
The solid curves (I) in Figs. 7.4.3 and 7.4.4 show, respectively, the marginal 
distributions P(4)1 I y) and P(4)2 I y) for the data in Table 7.4.3. The distribution 
of 4>1 is derived from (7.4.45) by setting l' = (j, - 1. - t) so that IJ = 4>1' 
The 
distribution of 4>2 is obtained similarly. These distributions are nearly symmetrical, 
centered about 4>1 = 3.25 and 4>2 = -1 .50, respectively. 
Also shown in the 
same figures by the broken curves (2) centered at $1 = 3.44 and $2 = -1.75, 
are respectively the distributions of ¢ I and 4>2 based upon the first factor 
(intra-block) of (7.4.33) alone. 
These curves (2) are numerically equivalent 
to the corresponding confidence distributions of 4>1 and 4>2 in the sampling theory 
framework if inter-block information is ignored. By comparing the curves labeled 
2 
3 
4 
In the figure 
I. Posterior distribution of IPI obtained from (7.4.45) 
2. Posterior distribution of IPI based on the first factor of (7.4.33), also a confidence distribution 
of IP I 
3. Approximate distribution of IPI. from (7.4.52) 
Fig. 7.4.3 Posterior distributions of <PI for the BIBD data. 
0602 

412 
Inference about Means with Information from more than one Source 
In the figure 
P( ~l
l y) 
ISO 
1.00 
0.50 
o ~~~----~----~----~----~~~~---~2~ 
-3 
- 2 
-I 
0 
I. Posterior distribution of ¢J, obtained from (7.4.45) 
7.4 
2. Posterior distribution of ¢J2 based on the first factor of (7.4. 33), also a confidence distribution 
of <P2 
3. Approximate distribution of ¢J2 , from (7.4.52) 
Fig. 7.4.4 Posterior distributions of rP2 for the BIBD data. 
(l) and (2) in each figure, one sees how the inter-block information modifies the 
marginal inferences of the contrasts, and for the present example the modification 
is appreciable. The dotted curves (3) are approximations discussed below. 
Approximations 
In obtaining the marginal distributions of cPJ and cP2 shown in Figs. 7.4.3-4, 
numerical integration on a computer was necessary in both cases. 
However, 
both distributions are almost symmetrical and resemble Student's t distributions. 
In fact, close approximation to p(</> I y) can be obtained by use of the multivariate 
t distribution. In general, we can write 
p(<» I y) = E p(<» I lV, y), 
- 00 <cPi< 00, 
i = 1, ... , / -
1, 
(7.4.47) 
g(w) 
where p(<» I w, y) is given in (7.4.42) and the expectation is taken over the posterior 
distribution of g(w) which is some monotonic function ·of lV. If the conditional 
distribution p(<» I lV, y) is changing gently in the region of g(w) where its density 
is appreciable, then 
p(</>ly)=p(</>liV,y), 
-00 < cPi<OO, 
i = 1, .. . , /- 1 
(7.4.48) 

7.4 
Analysis of Balanced Incomplete Block Designs 
413 
where g(w) = Eg(H'). 
From the distribution of \r in (7.4.40), we see that 
evaluation of the expectation Eg(l\) would necessitate numerical integration 
irrespective of the choice of g. Alternatively, we can write 
p(<!> I y) == p(<!> I ~V, y) + h E[g(lI') - g(w)] 
(7.4.49) 
where g(lv) is the mode of g(\!) and h is the first derivative of p(<!> I I\" y) with respect 
to g(II'), evaluated at II!. If g(ll) is chosen so that its distribution is symmetrical, 
then 
p(<!> I y) == p(<!> I w, y), 
-00 <<P;<:O, 
i= 1, ... ,/-1. 
(7.4.50) 
To this degree of approximation, the posterior distribution of <!> is the multivariate 
t distribution 
1 (I - I) [ (j) ( IV), S 
2 
( w) ( 1/ _ I - + 
1/ - 11; _ 1 ) , / (r - 1)] 
where (j)(w) and S2(IV) are obtained simply by substituting IV into (7.4.42) and 
(7.4.43). To decide whether a particular parameter point <!> = <!>o lies inside or 
outside the (I -
a) H.P.D. region, we may thus compare the quantity 
L{= I [<p; -
cP;(i\·)r 
(I -
l)s2(~v) 
(7.4.51) 
with the JOOct upper percentage point of an F variable with (J -
I) and l(r -
J) 
degrees offreedom. Making use of the properties of the multivariate I distribution, 
any set of p :( (I -
J) linearly independent contrasts will have an approximate 
p-dimensional multivariate I d istri bution. I n particular, the quantity 
YJ -
i](IV) 
S(IV)(l'l) l '2 
is approximately distributed as 1[0, I, I(r -
I)]. 
(7.4.52) 
Now for the data of Table 7.4.3 the distribution of II', as whown in Fig. 7.4.2, 
is clearly not symmetrical. An F-Iike distribution with a long tail to the right 
(such as the one shown) can however be transformed into a more symmetrical 
distribution by a log transformation g(II') = log I\'. 
l\oting that p(log l\' I y) = 
Ivp(II'1 y), it can be readily verified from (7.4.40) that for r > ), the mode of log II' 
is a root of the cubic equation, 
where 
J(K - I) (1 -1)2 ~ 
do = 
-
(J -
I) 
I 
Se ' 
(7.4.53) 

414 
Inference about :Yleans with Information from more than one Source 
I = (J - I) (I - f )2 _ _ 1_ (I - f)[2J(K 
( I 
\J - 1 
/ 
I -
1 
f 
Sb 
I) -
(I -
l)J-
Se 
7.4 
and 
+ C = 
~) C 
~ f) ~~ 
dz = (2J ; ~ ~) C 
~ f) - [lj ~ 11) -
I H 
Sh S: S,) . 
For the example, (7.4.53) becomes 
II' } -
!.S091 0 I 11'2 -
0.850443 II' -
0.200062 = 0 
and the appropriate root is \{' = 1.99 to two decimal places. Using this value in 
(7.4.52) we find that, to this degree of approximation, 
CPI -
3.23 
-
---
0.42 
and 
¢2+ 1.45 
0.42 
are distributed marginally as teO, 1,27). The curves marked (3) in Figs. 7.4.3 and 
7.4.4 represent the approximating distributions. 
In both cases, the agreement 
between the exact distribution obtained by numerical integration [curve (l)] 
and the approximating distributions seems to be sufficiently close for any practical 
inferential purpose. 
To give further illustration of this approximation, Table 7.4.4 shows specimens 
of the exact and the approximate densities of ¢ 1 = e I -lJ using the data in Federer 
(1955, pp. 419·422). In this example, J = 15, 1= 10, K = 4,r = 6, ), = 2 
S .. = 17.878935, 
Sb = 1.379463, 
S, = 26.996222 
$1 = 3.3450, 
4>1 = 1.90476 
and the cubic equation in (7.4.53) becomes 
w3 -
3.809690 w 2 + 0.032859 IV -
0.009920 = 0 
of which the appropriate root is w = 3.80. The agreement between the exact and the 
approximate distributions is again dose. 
While we have illustrated the use of the log modal approximation only in the 
one dimensional case, i.e., a single contrast, the close agreement in the examples 
shown as well as the near elliptical shape of the contour of p(<<I> I y) shown in 
Fig. 7.4.1 suggest that such approximations will also be useful in higher 
dimensions. 

7.4 
Analysis of Balanced Incomplete Block Designs 
415 
Table 7.4.4 
Specimens of the exact and approximate densities of 0, : Federer's example 
Exact 
Approximate 
2.37 
0.02027 
0.01580 
2.47 
0.04505 
0.03770 
2.57 
0.09397 
0.08332 
2.67 
0.18237 
0.16919 
2.77 
0.32640 
0.31316 
2.87 
0.53407 
0.52445 
2.97 
0.79236 
0.78959 
3.07 
1.05818 
1.06303 
3.17 
1.26442 
1.27463 
3.27 
1.34595t 
l.35779t 
3.37 
1.27319 
J .28383 
3.47 
1.06983 
1.07832 
3.57 
0.79989 
0.80649 
3.67 
0.53412 
0.53922 
3.77 
0.32028 
0.32400 
3.87 
0.17367 
0.17608 
3.97 
0.08586 
0.08718 
4.07 
0.03904 
0.03964 
4.17 
0.01648 
0.01668 
i' Density at the common mode. 
7.4.7 Summarized Calculations for Approximating the Posterior Distributions of <I> 
Table 7.4.5 provides a summary of the calculations needed for approximating 
the posterior distributions of the contrasts 0" ... ,01' 
The numerical values 
shown are those associated with the data in Table 7.4.3. 
Table 7.4.5 
Summarized calculations for approximating the posterior distribution of <I> 
Data of Table 7.4.3. 
1. 1= 15, 
K=2, 
1=3, 
r= 10, 
}=5 
2. y.. 
Grand average 
Average of observations corresponding to treatment i, i = 1, ... , 1 
Average of block averages Y.j for the r blocks in which the ith treatment 
appears, i= I, ...• J 
Y. =5.67 
Tl =8.74, 
Bl =6.16. 
T 2 =4.44, 
B2 =5.67. 
T3 = 3.82 
B3 =5.08 

416 
Inference about Means with Information from more than one Source 
3. <1>1 = 3.44, 
4)1 = 1.96, 
4. S, =29.18 
Sb=49 .04 
S, = 17.77 
5. ~v = 1.99 
<1>2 = -1.75, 
<1>2 = 
0.37 
Table 7.4.5 Conlinued 
<1>3 = -1.69 
4)3 = - 2.34 
8. Marginal posterior distribution of ¢j 
Use 
Use 
Use 
Use 
Use 
¢i -(j)jUv) 
. [0 1 I( 
J)] 
[S2(IV)(1-1)/ I]I (2 ~f, 
r-
, 
i= I, ... , f. 
¢j -(j)j(IIJ) ..v 1(0 1 27) 
0.42 
'" 
i= 1, 2, 3. 
9. Comparison of two treatment means. 
"y/=8j-8" =¢j-¢,' 
11-[(j)j(lv)-(j),(lv)] N 1[0 J f(r-l)J 
[2s2(1:v)JI 12 
' , 
e.g. i= 1, 
i =2 
6-4.68 
--
N 1(0, 1,27) 
0.71 
10. Overall comparison of I treatment means. 
7.4 
(7.4.17-18) 
(7.4.14) 
(7.4.21) 
(7.4.15) 
(7.4.37) 
(7.4.53) 
(7.4.38) 
(7.4.43) 

7.4 
Analysis of Balanced Incomplete Block Designs 
417 
7.4.8 Recovery of Interblock Information in Sampling Theory 
We now consider to what extent the results in the preceding sections may be 
related to those of sampling theory. If the variance ratio \t' = I + (Ka; :a;) were 
known, then , on sampling theory, the quantity <p;(1I') in (7.4.38) would be the 
natural estimator for 4>; and is, in fact, the minimum variance unbiased estimator- -
see Graybill and Weeks (1959). 
In the case of a single contrast I) = /'9, the sampling distribution of the quantity 
in (7.4.46) would be teO, I, I(r -
I)]. Thus, for given II, the conditional posterior 
distribution p(llllI', y) in (7.4.45) is numerically equivalent to the confidence 
distribution of I). 
In practice 11' is of course 110{ known. Then in the sampling theory approach 
two problems arise ; one concerns the choice of estimator for II and the other, 
the distribution of the resulting estimator for 4>; given that choice. Concerning 
the first problem, from Table 7.4.1 and using (7.4.36), it can be verified that for 
r> A, the quantities (Se, Sb' Sf) are independent and that 
2 ( 
I - I) 2 
Sf ~ a, 
11' + f 
X/-I' 
(7.4.54) 
Clearly, IV can be estimated in many different ways. In particular, the estimators 
of II' proposed by Yates and by Graybill and Weeks are based upon separate 
unbiased estimators of a; and a;, which are themselves linear combinations of 
(Se, Sb, Sf)' 
The appropriateness of these estimators seems questionable since 
it is the ratio wand not the individual components, on which inferences about 4>; 
depend. Further, granted that separate un biased estimators of a; and a; are of 
interest, presumably one should use those with minimum variance. However, it is 
readily verified that among linear functions of (Se, Sb' Sf)' the UMV estimators 
of (a;, a; ) depend upon the unknown IV. Another approach would be to estimate 
w by the method of maximum likelihood . By substituting for Sb(9) in (7.3.6) and 
SeeS) in (7.3.12) from (7.4.13), (7.4.15) and (7.4.20), (7.4.21), respectively, and by 
making use of (7.4.36), it can be verified that, for IV > I, 
l(w I y) = max I(a; , 8, «1>, 11'1 y) 
a;, O, cj>Iw 
[ 
Sb 
( 
I-r)-I J-tlK 
CC 14'-1/2 
Se + -;- + 
H' + J 
Sf 
(7.4.55) 
It follows that the maximum likelihood estimator of 11 ' is the root of a cubic 
equation unless the maximum of 1(\\' 1 y) occurs at II' = 1. Whether an estimator 
of 11' is obtained by using separate unbiased estimators of a; and (J; or by maxi-
mum likelihood, the resulting distributional properties of the corresponding 
estimator of 4>; are unknown . 

418 
Inference about Means with Information from more than one Source 
7.4 
In the Bayesian approach, as we have seen, an estimator IV of 1\' may be used 
to obtain approximations to the distributions of the ¢i' 
However, no point 
estimate of II' is actually necessary since, given the data, the actual distribution 
of the ¢i can always be obtained through a single integration as in (7.4.41). The 
integration in (7.4.41) averages the conditional distribution r(<I> I II', y) over a 
weight function p(1\' I y) which reflects the information about I\, in the light of the 
data y and the prior assumption, 
We have discussed in detail the problem of combining inter- and intra-block 
information about linear contrasts of treatment means 0; only for the BIBD model. 
The methods can, however, be readily extended to the general iinear model of 
(7.3.1),t Cn the above the prior distribution of (Ji was supposed locally uniform . It 
is often more appropriate to employ the random-effect prior in Section 7,2 to the 
present problem. For details of this analysis, see Afonja (1970), 
APPENDIX A7.1 
Some Useful Results in Combining Quadratic Forms 
We here give two useful lemmas for combining quadratic forms. 
Lemma 1. Let x, a and b be k x 1 vectors, and A and B be k x k symmetric 
matrices such that the inverse (A + B) - I exists. Then, 
(x -
a)' A(x -
a) + (x -
b)'B(x -
b) = (x - cnA + fi)(x -
c) 
where 
Proof: 
where 
Now, 
+ (a - by A(A + B) - I B(a -
b) 
c = (A + fir I(Aa + Bb). 
(x -
a)'Aex -
a) + (x -
b)'fiex -
b) 
= x'(A + B)x - 2x'(Aa + Bb) -1- a'Aa + b'Bb 
= x'(A + B)x -
2x' (A + fi)e + e' CA + B)e + d 
= (x - cnA + B)(x -
e) + d 
d = a'Aa + b'Bb -
c'(A + fi)e 
e'(A + fi)e = (Aa + Bb)'(A + ll)-l(Aa + llb) 
= [A(a -
b) + (A + B)b]'(A + B) - I[(A + B)a -
B(a -
b)] 
= -(a -
b)'A(A + B)-IB(a -
b) + a'Aa + h'llb 
t See Tiao and Draper (1968). 
(A7,J.I) 
(A7.1.2) 
(A7.1.3) 
(A7.1.4) 

A7.1 
Appendix 
419 
Substituting (A 7.1.4) into (A 7.1.3), the lemma follows at once. Note that if both 
A and B have inverses, then 
(A7.1.5) 
it sometimes happens that we need to combine two quadratic forms for which 
the matrix (A + B) has no inverse. In this case, Lemma 1 may be modified as 
foJlows: 
Lemma 2. Let x, a and b be k x I vectors and A and B be two k x k positive 
semidefinite symmetric matrices. Suppose the rank of the matrix A + B is q( < k). 
Then, subject to the constraints Gx = 0, 
(x - a)'A(x - a) + (x - b)'B(x - b) = (x - c*)'(A + B + M)(x - c*) 
+ (a -
b)'A(A + B + M)-IB(a -
b) 
(A7.1.6) 
where G is any (k - q) x k matrix of rank k - q such that the rows ofG are linearly 
independent of the rows of A + B, M = G'G and 
c* = (A + B + Mf 1 (Aa + Bb). 
Proof We shalJ first prove that 
M(A + B + M) . I A .= M(A -;- B + M) - In = O. 
(A 7.1.7) 
Since (A + B) is of rank q, G is of rank k - q and the rows of G are linearly 
independent of the rows of (A + B), there exists a (k -
q) x k matrix U of rank 
(k -
q) such that UG' is non-singular and 
U(A + B) = O. 
Now, 
UtA + B + M)(A + B + M) 1 = U. 
From (A 7.1.8) 
LM(A + B + M) - I = L. 
Postmultiplying both sides by (A + B), we get 
UM(A + B + M) - l(A + B) = O. 
Since M = G'G and UG' is non-singular, (A 7.1.9) implies that 
G(A + B + M) - 1 (A + B) = 0 
so that 
M(A + B + M) - I (A + B) = o. 
(A7.1 .8) 
(A7.1.9) 
(A 7. 1. lOa) 
Postmultiplying both sides of (A7.I.IOa) by (A + B + M) - IM, we obtain 
M(A + B + M) - 1 A(A + B + M) 1 M 
+ M(A + B + M)IB(A + B + M) - IM = 0 
(A7.1.10b) 

420 
Inference about '\1eans with Information from more than one Source 
7.4 
Since A and B are assumed positive semidefinite, it follows that both terms on the 
left of (A 7.1.1 Ob) are positive semidefinite matrices. Thus the equality implies that 
both must be null matrices. Writing A = C'C and B = D 'D where C and Dare 
k x k matrices we must then have 
M(A + B + M)-IC' = M(A + B + )i)-lD' = 0 
and the assertion (A 7.1.7) follows. 
We now prove (A 7.1.6). Using the fact that \'Ix = 0, we have 
(x - a)'A(x - a) + (x - b)'B(x - b) = x'(A + B + M)x - 2x'(A + B + M)e* 
+e*'(A + B + M)e* + d l 
= (x - e*)'(A + B + M)(x - e*) + d l 
(A7.1.1I) 
where 
d l ::; a'Aa + b'Bb - c*'(A + B + M)c*. 
(A7.I.l2) 
Now, 
e* '(A + B + M)c* = (Aa + Bb)'(A + B + ;VI)-I(Aa + Bb) 
= [A(a - b) + (A + B)b]'(A + B + M)-l [(A + B)a - B(a - b)] 
-(a - b)'A(A + B + M)-IB(a - b) 
+ b'(A + B) (A + B + )i)-irA + B)a 
+ (a - b)'A(A + B + M)-l (A + B)a 
- b'(A + B)(A + B + M)-IB(a - b) 
(A7.1.13) 
Using (A7.l.lOa), the second term on the extreme right of (A7.1.13) becomes 
b'(A + B)(A + B + M) - I(A + B)a = b'(A + B)a. 
Applying (A 7.1 .7), the third and fourth terms are, respectively, 
(a - b)'A(A + B + M) - I(A + B)a = (a - b)'Aa, 
-b'(A + B)(A + B + M) - IB(a - b) = -(a -
b)'Bb. 
Substituting (A7.1.14-16) into (A7.1.13), we get 
(A 7.1.14) 
(A7.l.lS) 
(A7.1.16) 
e*'(A + B + :vI)e* = -(a - b)'A(A + B + M)-lB(a -
b) + a'Aa + a'Bb 
so that 
d l ::; (a - b)'A(A + B + M)-IB(a - b) 
and the lemma is proved. 

CHAPTER 8 
SOME ASPECTS OF MUL TIVARJATE ANALYSIS 
8.1 I'\jTRODLCTION 
In all the problems we have so far considered, observations are made of a single 
unidimensional response or output y . The inference problems that resu lt are calJed 
univariate problems. 
In this and the next chapter, we shaJ I consider problems 
which arise when the output is multidimensional. 
Thus, in the study of a 
chemical process, at each experimental setting one might observe yield YI ' 
density Y2, and color YJ of the product. 
Similarly, in a study of consumer 
behavior, for each household one might record spending on food y" 
spending 
on durables Y2, and spending on travel and entertainment h . We would then say 
that a three-dimensional output or response is observed. 
Inference problem~ 
which arise in the analysis of such data are called multivariate. 
In this chapter, we shall begin by reviewing some univariate problems In a 
general setting which can be easily extended to the multivariate case. 
8.1.1 A General Uniyariate Model 
It is often desired to make inferences about parameters 8" ... , Ok contained in 
the relationship between a single observed outpUI variable or response Y subject 
to error and p input in variables c; I' ... , ~p whose values are assumed exactly known. 
It should be understood that the inputs could include qualitative as well as 
quantitative variables. For example, ~i might take values of 0 or 1 depending on 
whether some particular quality was absent or present in which case ~i is called 
an indicator variable or less appropriately a dummy variable. 
The Design .'VI atrix 
Suppose, in an investigation, 11 experimen tal "runs" are made, and the uth run 
consists of making an observation ),,, at some fixed set of input conditions 
1;;, = ( ~ III' ~ " 2 > ... , ~"p). The 11 xp design matrix 
s', 
~
, , ~ 12 
~JP 
/;= 
/;;, 
~1I1 
~1I2 
~"P 
(8.1 .1 ) 
1;;, 
~n 1 
~"2 
~IIP 
421 

422 
Some Aspects of 'Iultivariate Analysis 
8.1 
lists the jJ input conditions to be used in each of the n projected runs and the 
Ifth row of 1; is the vector ~:,. The phraseology "experimental run", "experimental 
design" is most natural in a situation in which a scientific experiment is being 
conducted and in which the levels of the inputs are at our choice. 
In some 
applications, however, and particula rl y in economic studies, it is often impossible 
to choose the experimental conditions. We have only historical data generated 
for us in circumstances beyond our control and often in a manner we would not 
choose. 
It is convenient here to extend the terminologies "experimental run" 
and "experimental design" to include experiments designed by nature, but we 
must, of cou rse, bear in mind the limitations of such historical data. 
To obtain a mathematical model for our set-up we need to link the 11 
observations y ' = (YI' ... , y,,) with the inputs~. This we do by defining two 
functions called respectively the expeClatiol1 junction and the error junction. 
The Expectation Function 
The expected value E(y,.) of the output from the uth run is assumed to be a known 
function 1]" of the p fixed inputs 1;" employed during that run, involving k unknown 
parameters 0' = (0 1 " 
., Ok), 
(8.1.2) 
The vector valued function 
TJ = TJ (1;, 6).11' = (I] I' .
. , 1]" • ... , 1],,) , is called 
I he 
expectation junction. 
The Error Function 
The expectation function links E(y,,) to 1;" and O. 
We now have to link Yu to 
E(yJ = 1],.. 
This is done by means of an error distribution junction in 
E' = (6 1, ... , 10,,). The n experimental errors E = Y -
TJ which occur in making the 
runs are assumed to be random variables having zero means but in general not 
necessarily independently or Normally distributed. 
We denote the density 
function of the n errors by p(r. In) where n is a set of error distribution 
parameters whose values are in general unknown. 
Finally, then, the output in the form of the n observations y and the input 
in the form of the n sets of conditions ~ are linked together by a mathematical 
l110del containing the error function and the expectation function as follows 
TJ =' TJ( ~ .e ) 
plY - 'l l ") 
1; -
--+ TJ 
y. 
(8.1.3) 
This model involves a function 
iCy, e,lt, 1;) 
(8.1A) 
of the ' 'y;ervations y, the parameters {) of the expectation function, the parameters 
It of th e error distribution and the design 1;. 

S.2 
A General 1\iultil'ariate Normal Model 
423 
Data Genttraliol1 .'VI ode/ 
J(we knew 9 and It and the design~. we could use the function (8.1A) to calculate 
the probabililY density associated with any particular set of data i 
This data 
generation model (which might, for example, be directly useful for simulation 
and Monte-Carlo studies) is the function j(y, 9, 1t,~) with 0, 1t and c; held fixed 
and we denote it by 
p(y I G ,. 1t,~) = I(y , 0, n, ~) , 
(8.1.5) 
which emphasizes that the density is a function of y alone for fixed D, 1t, and ~. 
The Likelihood Function and the Posierior Distribution 
in ordinary statistical practice. we are not directly interested in probabilities 
associated with various sets of data. given jixed values of the parameters e and 1t. 
On the contrary, we are concerned with the probabilities associated with various 
sets of parameter values, given a fixed set of data which is known to have occurred. 
After an experiment has been performed, J' is known and fixed (as is ~) bUI 
e and It are unknown . The likelihood function has the same form as (8.IA), 
but in it y and ~ are fi xed and e and 1t are not to be regarded as variables. Thus, the 
likelihood may be written 
ICe, It I y, 1;) = f( ~', 6, 1t, ~). 
(8.1.6) 
In what follows we usually omit specific note of dependence on 1; and write 
l(e,1t I y, 1;) as 1(0, 1t ' y). 
In lhe Bayesian framework , inferences about e and 1t can be made by suitable 
study of the posterior distribution p(O, 1t ' y) of e and 1t obtained by combining the 
likelihood with the appropriate prior distribution pee, 1t), 
p(e, 11: I y) if /(9,1t I y) pee, 1t). 
(8.1.7) 
An example in which the expectation function is nonlinear and the error 
distribution is non-l\ormal was given in Section 3.5. 
In this chapter, we shall 
from now on assume Normality but will extend our general model to cover 
multivariate problems. 
8.2 A GENERAL MULTIVARIATE '\'ORMAL MODEL 
Suppose now that a number of output responses are measured in each 
experimental run. Thus, in a chemical experiment, at each setting of the process 
conditions ¢I = temperature and ¢2 = concentration, observations might be 
made on the output responses YI = yield of product A , Y2 = yield of product B, 
and Y3 = yield of product C. In general, then, from each experimental run the 

424 
Some Aspects of Multivariate Analysis 
8.2 
m-variate observation 
would be available. 
There would now be m expectation functions 
where 
(8.2.1) 
where ~tlj would contain pj elements (~lllj, . . . , ~llSj, . '" 
~tlP;;) and OJ would contain 
k j elements (e lj, ... , egj , ". , &k;j) . 
The expectation functions y/"j might be linear 
or non I inear both in the parameters OJ and the inputs /;"j' Also, depending on the 
problem, some or all of the pj elements of ~llj might be the same as those of /;llj 
and some or all of the elements of OJ might be the same as those of OJ. That is 
to say, a given output would involve certain inputs and certain parameters which 
might or might not be shared by other outputs. 
8.2.1 The Likelihood Function 
Let us now consider the problem of making inferences about the OJ for a set 
of 11 m-variate observations. We assume that the error vector 
u = 1, ... , 11, 
(8.2.2) 
is, for given 0 and I , distributed as the m-variate Normal M",(O, I), and that the 
runs are made in such a way that it can be assumed that from run to run the 
observations are independent. 
Th us, in terms of the general framework of 
(8.1.4), ~ = 1t are the parameters of an error distribution which is multivariate 
Normal. We first derive some very general results which apply to any model of 
this type, and then consider in detail the various important special cases that emerge 
if the expectation functions are supposed linear in the paramett:rs OJ. 
The joint distribution of the n vectors of errors E = (E(I)' ... , E(u)' ... , E(n»' 
IS 
II 
peE I ~, 0) = n P(E(ll ) I ~ , 0) 
11=( 
( 
1 
1/ 
) 
= (2n:),.,II/2IEI- n, 2 exp 
- -
"E' E-IE 
2 
1..., 
( II) 
(II) 
u :-: 1 
-
ro <ellj <
ro, 
i=l,,, .,m, u=l, ... ,n, 
(8.2.3) 

8.2 
A General Multivariate Normal Model 
425 
where L = {a,J is the m x m covariance matrix, L_I = {aij} its inverse and 
e refers to the complete set of all the (k 1 + ... + kin) parameters el' ... ,e",. 
Denoting See) to be the 111 x m symmetric matrix 
with 
n 
n 
S;j(e;, e) = I 
Cui Cllj = I [Ylli -
l'/i(~lIi' e;)] [Yll j -
I'/j(~Uj' e)], 
;,)=1, ... ,111, 
u= I 
,, =-= 1 
(8.2.4 ) 
then the exponent in (8.2.3) can be expressed as 
n 
no 
m 
I 
1:;11) 1:- 11:(11) = trS(e)1:- 1 = I I 
aijSij(ei, e) 
(8.2.5) 
u= I 
;= 1 j= 1 
where tr A means the trace of the matrix A. 
Given the observations, the 
likelihood function can thus be written 
l(e,1: 1 y) cc p(1: 1 1:, e) 
oc 11:1-
11
/ 2 exp [ - 1 tr 1:- ls(e)] . 
(8.2.6) 
To clarify the notation , we emphasize that y refers to the 11 x 111 matrix of 
observations 
YII 
Y,i ... 
f 'm 
Y; I) 
Y = 
Yul 
J'ui 
Yllm 
= [)I I , ... , y j , ... , Y uJ 
y;,,) 
YII I 
Jini 
.rnlfl 
Y;n) 
where Yi = (Y li , "")'11;)' is the vector of 11 observations correspond ing to the ith 
response and Y(II) = (YIII' .. . , YII"')' is the vector of m observations of the utn 
experimental run. Similarly, I: refers to the 11 x m matrix of errors 
6 1 I 
6 1 i 
elm 
1:; I) 
E = 
Cu i 
ell; 
e,m, 
= [E I , . . " E;, . . " Em] = 
1:;11) 
6nl 
e'l; 
enm 
, 
1:(11) 
8.2.2 Prior Distribution of (e, 1:) 
For the prior distri bution of the parameters (e,1:), we shall first of all assume 
that e and 1: are approximately independent so that 
p(e,1:) == pee) p(1:). 
(8.2.7) 

426 
Some Aspects of '\1ultivariate Analysis 
8.2 
We shall further suppose that the parameterization in terms of e is so chosen 
such that it is appropriate to take e as locally uniform,t 
pee) oc constant. 
(8.2.8) 
For the prior distribution of the 'lln(m + 1) distinct elements of r., application 
of the argument in Section 1.3 for the multiparameter situation leads .to the 
non informative reference prior 
(S.2.9) 
Now, 
(8.2.10) 
'.'!here 
(S.2.11) 
is the Jacobian of the transformation from the elements aij of r. to the elements 
aU of r.- l • It is shown in Appendix AS.2 that 
I·.?'(r.-I)j oc I j)~~l I 
(8.2.12) 
and that 
I
~I' = jDm+'. 
ar.- I 
' 
Thus, 
p(r.) oc jr.l-t (m + I l. 
In this special case In = I, (S.2.14) reduces to 
I 
p(a) I) oc --
all 
(8.2.13) 
(8.2.14) 
(S.2. 1 5) 
which coincides with the usual assumption concerning a noninformative prior 
distribution for a single variance. Another special case of interest is when the 
errors (e,d' " ., cum) are uncorrelated, that is, (Jij = ° if i =1= j. 
In this case, the 
same argument leads to 
m 
p(r.1 (Jij = 0, i =1= i) = P«(Jll, .'" (J,,",,) oc n 
(S.2.16) 
i = 1 
t As we have mentioned earlier, when the parameter space is of high dimension, the use 
of the locally uniform prior may be inappropriate and more careful considerations should 
be given to the structure of the model in selecting a noninformative prior. 

8.2 
A General Multivariate Normal Model 
427 
8.2.3 Posterior Distribution of (8, E) 
Using (8.2.6), (8.2.8), and (8 .2.14), the joint posterior distribution of (8, E) is 
p(8, Ely) if. IEI- ' (II -i- nr + I) exp [ - l tr E - I see)], 
-
00 < e < 00, 
E > 0, 
(8.2.17) 
where the notation -
00 < a < 00 means that each element of the set of 
parameters a can vary from -
00 to OC!, and the notation E > 0 means that the 
·lm(m + I) elements uij are such that the random matrix E is positive definite. 
It is sometimes convenient to work with the elements of E- ' = {uij } rather 
than the elements of E. Since 
p(e'E-lly)=p(e'EIY)lo~ll' 
it follows from (8.2.13) that the posterior distribution of (8, E - ' ) is 
pea, E - I I y) if. IE- 11!-(II-III - I) exp [-
.~ tr E- 1 Sea)], 
-
00 < a < 00, 
E- ' > o. 
8.2.4 The Wishart Distribution 
(8.2.18) 
(8.2.19) 
We now introduce a distribution which is basic in Normal theory multivariate 
problems. 
Let Z be a m x m positive definite symmetric random matrix which 
consists of t m(m + I) distinct random variables zij (i, j = I, .. . , m; i ?- j). 
Let 
q> 0, and B be a m x m positive definite symmetric matrix of fixed constants. 
The distribution of zij' 
p(Z) if. IZli q- 1 exp (- t tr ZB), 
Z > 0 
(8 .2.20) 
obtained by Wishart (1928), is a multivariate generalization of the X2 distrihution. 
It can be shown that 
f 
IZI ~q-I exp (-1 tr ZB) dZ = IBI -l (q , no - I) 2t m(q +m-l) r", (q -t- n~ -
I ) 
z > o 
(8.2.21) 
where fp(b) is the generalized gamma function, Siegel (\935) 
p-l 
h> --. (8.2.22) 
2 
We shall denote the distribution (8.2.20) by w'n (B- t , q) and say that Z is 
distributed as Wishart with q degrees of freedom and parameter matrix B I. 
For a discussion of the properties of the Wishart distribution, see for example 
Anderson (195H). 
Note carefully that the parameterization used in (8.2.20) is 
different from the one used in Anderson in one respect. 
In his notation, the 

428 
Some Aspects of Multh'ariate Analysis 
8.2 
distribution in (8.2.20) is denoted as W (B- 1 , v) where v = q + m -
1 is said 
to be the degrees of freedom. 
As an application of the Wishart distribution, we see in (8.2.19) that, given 
9, ~ - t is distributed as Wm[S -1 (9), 11 - m + I J provided 11 ~ m . 
8.2.5 Posterior Distribution of 9 
Using the identity (8.2.21), we immediately obtain from (8.2.19) the marginal 
posterior distribution of 9 as 
p(91 y) ex IS(9)I-n/2, 
-
00 < 9 < 00, 
(8.2.23) 
provided 11 ~ m. 
This extremely simple result is remarkable because of its generality. It will 
be noted that to reach it we have not had to assume either : 
a) that any of the input variables ~II; were or were not common to more than 
one output, or 
b) that the parameters 9; were or were not common to more than One output, or 
c) that the expectation functions were linear or were nonlinear in the parameters. 
This generality may be contrasted with the specification needed to obtain 
"nice" sampling theory results. 
For example, a common formulation assumes 
that the SII; are common, that the 9; are 110t. and that the expectation functions 
are all linear in the parameters. 
In the special case in which there is only one output response y, (8 .2.23) 
reduces to 
p(91 y) ex [S(9)rnl, 
-
00 < 9 < 00, 
(8.2.24) 
with S(9) = L:7. 
I [YII -
IJ(S", 9)J2. 
As we have seen, this result can be regarded 
as supplying a Bayesian justification of least squares, since the modal values of 9 
(those associated with maximum posterior density) are those which minimize S. 
The general result (8.2.23) supplies then, among other things, an appropriate 
Bayesian multivariate generalization of least squares. 
The "most probable" 
values of 9 being simply those which minimize the determinant IS(9)1. 
Finally, in the special case (J;j = 0, i =1=), combining (8.2.16) with (8.2.6) 
and integrating out (Jll' .. . , (JIMIn yields 
m 
p(91 y) ex fl [S;;(9;)r n '2, 
-
00 < 9 < 00. 
(8.2.25) 
;= 1 
8.2.6 Estimation of Common Parameters in a Nonlinear Multivariate Model 
We now illustrate the general applicability of the result (8.2.23) by considering 
an example in which: 

8.2 
A General Multivariate Normal \'lodel 
a) certain of the ()'s are common to more than one output, and 
h) the expectation functions are nonlinear in the parameters. 
429 
-------------------=-;-;,=-.,.,...-
----- -----
-----
-----
=-':-=-=-=-11\ :-=-':-':-=-
Time~~ 
B 
fil\ 
• 
I 
• 
c 
Fig. 8.2.1 Diagrammatic representation of a system A ~ B ~ C. 
Suppose we have the consecutive system indicated in Fig. 8.2.i, which shows 
water running from a tank A via a tap opened an amount cPl into a tank B which 
then runs into a tank C via a tap opened an amount cPz' 
If /}(, /}2 and /}3 are the proportions of A, B, and C present at time ¢, with 
initial conditions (/}1 
1,112 = 0, /}3 = 0), the system can be described by the 
differential equations 
d/}2 
df = cPl/}1 -
cPZ/}2, 
(8.2.26) 
d/}3 
df = cP2/}2' 

430 
Some Aspects of Multivariate Analysis 
8.2 
Systems of this kind have many applications in engineering and in the 
physical and biological sciences. In particular, the equation (8.2.26) could represent 
a consecutive first-order chemical reaction in which a substance A decomposed 
to form B, which in turn decomposed to form C. The responses 11 I' 112, 113 would 
then be the mole fractions of A, B, and C present at time ( and the quantities 
¢ 1 and ¢z would then be rate constants associated with the first and second 
decompositions and would normally have to be estimated from data. 
If we denote by Yl' Y2, and Y3 the observed values of 111' '12, and 113' then, 
on integration of (8.2.26), we have the expectation functions 
E(YI) = 111 = e-tJ>,~, 
E(Yz) = 112 = 
(e-tJ>I~ -
e-tJ>2~) ¢1 /(¢z -
cPI)' 
E(Y3) = 11J = I + (- 4>2 e-tJ>, ¢ + ¢I e-tJ>2{)/(4)z -
4>1)' 
and it is to be noted that for all (, 
111 + 112 + 113 == 1. 
(8.227a) 
(8.2.27b) 
(8.2.27c) 
(8.2.27d) 
Observations on Yl could yield information only on ¢l, but observations on 
Yz and Y3 could each provide information on both 4>1 and 4>2' If measurements 
of more than one of the quantities (YI' Yz, YJ) were available, we should certainly 
expect to be able to estimate the parameters more precisely. 
The Bayesian 
approach allows us to pool the information from (YI, h, Y3) and makes it easy 
Table 8.2.1 
Observations on the yield of three substances in a chemical reaction 
Yield of 
Yield of 
Yield of 
A 
B 
C 
Time=(11 
Ylu 
YZ u 
Y3u 
J. 
0.959 
0.025 
0.028 
2 
t 
0.914 
0.061 
0.000 
0.855 
0.152 
0.068 
0.785 
0.197 
0.096 
2 
0.628 
0.130 
0.090 
2 
0.617 
0.249 
0.118 
4 
0.480 
0.184 
0.374 
4 
0.423 
0.298 
0.358 
8t 
0.166 
0.147 
0.651 
8t 
0.205 
0.050 
0.684 
16t 
0.034 
0.000 
0.899 
J6t 
0.054 
0.047 
0.991 
+ These four runs are omitted in the second analysis. 

8.2 
A General Multivariate Normal Model 
431 
to appreciate the contributio n from each of the three responses. In this example 
~ is the only input variable and is the elapsed time since the start of the reaction. 
We denote by Y;I/) = (yu l' Yu2, Y1/3) a set of m = 3 observations made on 
IJ 1u' '12,,, '1311 at time (I/' A typical set of such observations is shown in Table 8.2. I. 
In some cases observations may not be available on aJl three of the outputs. 
Thus only the concentration Yz of the product B might be observable, or Yz 
and Y3 might be known, but there might be no independently measured 
observation Yl of the concentration of A.t 
We suppose that the observations of Table 8.2.1 may be treated as having 
arisen from 12 independent experimental runs, as might be appropriate if the runs 
were carried out in random order in sealed tuhes, each reaction being terminated 
at the appropriate time by sudden cooling. 
Furthermore, we suppose that 
(Yl, Y2, YJ) are functionally independent so that the 3 x 3 matrix ~ may be assumed 
to be positive definite and contains three variances and three covariances, all 
unknown. It is perhaps most natural for the experimenter to think in terms of the 
logarithms Ifl = log cPl and 1f2 = log cP2 of the rate constants and to regard these 
as locally uniformally distributed a priori.t 
We sha ll , therefore, choose as our 
reference priors for elf l , 1f2) and ~ the distributions in (8.2.8) and (8.2.14), 
respectively. 
t When the chemist has difficulty in determining one of the products he sometimes makes 
use of relations like (8.2.27d) to "obtain it by calculation." Thus he might "obtain" Yl 
from the relation Yl = I -
Y2 -
Y3' 
For the resulting data set, the 3 x 3 covariance 
matrix ~ will of course not be positive definite, and the analysis in terms of three-
dimensional responses will be inappropriate. [n particular, the determinant of the sums 
of squares and products which appears in (8.2.23) will be zero whatever the values of the 
parameters. The difficulty is of course overcome very simply. The quantity Yl is not an 
observation and the dala has two dimensions, not three. The analysis should be carried 
through with Y2 and Y3 which have actually been measured. For a fuller treatment of 
problems of this kind arising because of data dependence or near dependence, see Box, 
Erjavec, Hunter and MacGregor (1972). 
t Suppose that (a) the expectation functions were linear in 8 1(<\» 
and 1f2 (</» 
where 
<\> = (cPl' cP2), (b) little was known a priori about either parameter compared with the 
information supplied by the data, and (c) any prior information about one parameter would 
supply essentially none about the other. 
Then, arguing as in Section 1.3, a noninformative reference prior to e should be locally 
uniform. 
Conditions (b) and (c) are likely to be applicable to this problem at least as 
approximations, but condition (a) is not, because the expectation functions are non-linear 
in cPl and cP2 and no general linearizing transformation exists. However, [see for example 
Beale (1960), and Guttman and Meeter (1965)J the expectation functions are more "nearly 
linear" in Ifl = log cPl and Bl = log cP2' Thus, the assumption that 81 and If2 are locally 
uniform provides a better approximation to a noninformative prior for the rate constants. 
For reasons we have discussed earlier, the assumption is not critical and, if for example we 
assume cPl and cP2 themselves to be locally uniform, the posterior distribution is not altered 
appreciably. 

432 
Some Aspects of Multivariate Analysis 
8.2 
Expression (8.2.23) makes it possible to compute the posterior density for the 
parameters assuming observations are available on some or all of the products 
A, B, and C. Thus, we may consider the posterior distribution of 0 = (8 1,82), 
a) if only yields Y2 of product B are available 
pee I Y2) ex [522 (e)r"'2, 
-
00 < e < 00, 
(8.2.28a) 
b) jf only yields Y3 of product C are available, 
pee I Y3) ex [533(9)rnI2, 
-
00 < 9 < 00, 
(8.2.28b) 
c) if only yields Yz and Y3 of Band C are available 
I
S22(9) 
523(0) l- nl 2 
p(O I Y2, Y3) ex S23(9) 
5
33 (9) : 
' 
-
00 < e < 00, 
(8.2.28c) 
and 
d) if yields YI' Y2 and Y3 of the prodllcts A, Band C are all available 
pee I y):x. IS(e)l- nI2, 
-
00 < 9 < 00, 
(8.2.28d) 
where See) = {Sjj(9)}, i, j = 1,2,3. 
---t-~A---fr---+--l--+---+-el = log 4>1 
-2 
Fig.8.2.2 99.75 % H.P.D. regions for 81 and 82 for the chemical reaction data. 

8.2 
A General :vJultivariate Normal Model 
433 
Since there are only two .parameters 8 1 and 82 , the posterior distributions 
can be represented by contour diagrams which may be superimposed to show the 
contributions made by the various output responses. Single contours are shown 
in Fig. 8.2.2 of the posterior distributions of 8 1 and 82 for (a) Y2 alone, (b) Y3 
alone, (c) Y2 and Y3 jointly, and (d) y" Y2' and Y3 jointly. The contours actually 
shown are those which should correspond to an H.P.D. region containing 
approximately 99.75% of the probability mass calculated from 
10gp(O I,) - 10gp(O I,) = -t/(2,ex), 
ex = 0.0025 
where pee I,) refers to the appropriate distributions in (8.2.28a-d) and 0, the 
conesponding modal values of O. 
In this example, it is apparent, particularly 
for Y3' that the posterior distributions are non-Normal. Nevertheless, the above 
very crude approximation will suffice for the purpose of the present discussion. 
In studying Figure 8.2.2, we first consider the moon-shaped contour obtained 
from observations Y3 on the end product C alone. In any sequential reaction 
A -> B -> C -> '" etc., we should expect that observation of only the end product 
(C in this case) could provide little or no information about the individual para-
meters but only about some aggregate of these rate constants. 
A diagonally 
attenuated ridge-like surface is therefore to be expected. 
However, it should 
be further noted that since in this specific instance 173 is symmetric in 8, and 82 
[see expression (8.2.27c)], the posterior surface is completely symmetric about 
the line 8, = 82 , 
In particular, if (81 , ( 2) is a point of maximum density the 
point (e 2 , ( 1) will also give the same maximum density. In .general the surface 
will be bimodal and have two peaks of equal height symmetrically situated 
about the equi-angular line. 
Marginal distributions will thus display precisely 
the kind of behaviour shown in Fig. AS.6.l. 
Figure 8.2.2 shows, how, for this data, the inevitable ambiguity arising 
when only observations Y3 on product C are utilized, is resolved as soon as the 
additional information supplied by values Y2 on the intermediatc product B is 
considered. As can be expected, the nature of the evidence that the intermediate 
product Y2 contributes, is preferentially concerned with the difference of the 
parameters. 
This is evidenced by the tendency of the region to be obliquely 
oriented approximately at right angles to that for Y3' By combining information 
from the two sources we obtain a much smaller region contained within the 
intersection of the individual regions. Finally, information from y, which casts 
further light on the value of 8 1, causes the region to be further reduced. 
Data of this kind sometimes occur in which available observations trace 
only part of the reaction. To demonstrate the effect of this kind of inadequacy 
in the experimental design, the analysis is repeated omitting the last four 
observations in Table 8.2.1. As shown in Fig. 8.2.3, over the ranges studied, the 
contours for Y2 alone and Y3 alone do not now close. r-..evertheless, quite precise 
estimation is possible using Y2 and Y3 together and the addition of y, improves 
the estimation further. 

434 
Some Aspects of Multivariate Analysis 
8.2 
Yl alone 
--'t--Y'x--+I----+--t-I----+---I----+-- 8 1 = log <p 1 
Fig. 8.2.3 99.75 % H.P,D. regions for 8 1 and 82 • excluding the last four observations. 
Precautions in the Estimation of Common Parameters 
Even in cases where only a single response is being considered, caution is 
needed in the fitting of functions. As explained in Section 1.1.4, fitting should be 
regarded as merely one element in the iterative model building process. 
The 
appropriate attitude is that when the model is initially fitted it is tentatively 
entertained rather than assumed. 
Careful checks on residuals are appJied in a 
process of model criticism to see whether there is reason to doubt its 
applicability to the situation under consideration. 
The importance of such precaution is even greater when several responses 
are considered, In multivariak problems, not only should each response model 
be checked individually but they must also be checked for overall consistency, 
The investigator should in practice not revert immediately to a joint analysis 
of responses. He should: 

8.3 
Linear Multivariate Models 
435 
I) check the individual fit of each response, 
2) compare posterior distributions to appraise the consistency of the infor-
mation from the various responses (an aspect discussed in more detail in 
Chapter 9). 
Only in those cases where he is satisfied with the individual fit and with the 
consistency sha ll he revert to the joint analysis. 
8.3 LINEAR MULTIVARIATE MODELS 
In discussing the general m-variate ~-';ormal model above, we have not needed to 
assume anything specific about the form of the m expectation functions 11. In 
particular, they need not be linear in the parameters+ nor does it matter whether 
or not some parameters appear in more than one of the expectation function s. 
Many interesting and informative special cases arise if we suppose the expectation 
functions to be linear in the O's. 
Moreover, as will be seen, the linear results 
can sometimes supply adequate local approximations for models non-linear in 
the parameters. From now on then we assume that 
i = I, ... , m, 
U = I, ... , n, 
(8.3.1 ) 
where 
and 
with 
independent of all the O's. 
The n x k j matrix X j whose uth row is X;'Ii) will be called the derivative matrix 
for the ith response. 
Our linear m-variate model may now be written as 
Yl = Xl OJ + El 
(8.3.2) 
Certain characteristics of this mode of writing the model should be noted. 
In particular, it is clear that while the elements of X (ui) will be functions of the 
t Although, so that a uniform density can represent an approximately noninformative 
prior and also to assist local linear approximation, parameter transformations in terms of 
which the expectation function is more nearly linear, will often be employed. 

436 
Some Aspects of Multivariate Analysis 
8.3 
elements of the vector input variables ~u;, they will in general not be proportional 
to the elements of ~,,; themselves. Thus if 
then 
log ~ul; 
~u2; 
and 
~ 1l1i ~u3i 
8.3.1 The Use of Linear Theory Approximations when the Expectation is Nonlinear 
in the Parameters 
The specific form of posterior distributions which we shall obtain for the linear 
case will often provide reasonably close approximations even when the 
expectation functions '1(,,) is nonlinear in 9. This is because we need only that 
the expectation functions are approximately linear in the region of the parameter 
space covered by most of the posterior distribution, say within the 95% H.P.D. 
reg·ion.t 
For moderate n, this can happen with functions that are highly 
nonlinear in the parameters when considered over their whole range. Then, in the 
region where the posterior probability mass is concentrated (say the 95% 
H.P.D. region), we may expand the expectation function around the mode e; 
k, 
E(y,,;) = YJ,,; == YJ;(~u;' eJ + L X"g; (Bg; - 8g;), 
(8.3.3) 
g=1 
where 
which is, approximately, in the form of a linear model. 
Thus, the posterior 
distributions found from linear theory can, in many cases, provide close 
approximations to the true distributions. 
For example, in the univariate case 
(m = I) with a single parameter B, the posteriror distribution in (8.2.24) would be 
approximately 
(8.3.4) 
where 
v = n -
I, 
2 
1 
1\ 
2 
S = -
L[Yu -
YJ(~II' 0)] 
V 
and 
Xu = aYJ(~Il' B)I ' 
aB 
O=U 
so that the quantity 
(8.3.5) 
s 
would be approximately distributed as teO, I, v). 
t A possibility that can be checked a posteriori for any specific case. 

8.3 
Linear Multivariate \-Iodels 
437 
When, as in the case in which multivariate or univariate least squares is appropriate, 
a convenient method of calculating the 8's is available for the linear case but not for 
the corresponding nonlinear situation, the linearization may be used iteratively to find 
the 8's for the nonlinear situation. 
For example, in the univariate model containing a single parameter 8, with a 
first guess 80' we can write approximately 
where 
and 
x = 
II 
8"'(~1I' 8) 
88 
(8.3.6) 
Applying ordinary least squares to the model, we obtain an estimate of the correction 
(J - 80 and hence hopefully an improved "guess" 81 from 
e 
0 
LZ"o x" 
1 = ° +---. 
LX;' 
(8.3.7) 
This is the well-known Newton-Gauss method of iteration for nonlinear least squares, 
Box (1957, 1960), Hartley (1961), Marquardt (J 963), and under favorable conditions 
the successive iterants will converge to e. 
8.3.2 Special Cases of the General Linear \1ultivariate Model 
In general, the joint distribution of 9 and 1: is given by (8.2.17) and the 
marginal distribution of e is that in (8.2.23) quite independently of whether 
f/i(I;II;, e;) is linear in ei or not. For practical purposes, however, it is of interest 
to consider a number of special cases. 
For orientation we reconsider for a moment the linear univariate situation 
discussed earlier in Section 2.7, 
y = XO + e, 
(8.3.8) 
where y is a n x 1 vector of observations. X a 11 x k matrix of fixed elements, e 
a k x 1 vector of parameters and tan x J vector of errors. In this case, 
p(9, (J2 I y) IX. «(J2)-c·n + 1) exp [- ~~~) ], 
and 
(J2 > 0, -
00 < e < 00, 
(8.3.9) 
pee I y) IX. [s(e)r
n
' 2 , 
-
00 < e < 00, 
(8.3.10) 
The determinant IS(9)1 in (8.2.23) becomes the single sum of squares 
See) = (y -
X e)' (y -
X e). 
(8.3.1 J) 
I n this linear case, we may write 
See) = (n - k) S2 + (9 - 6)' x' x(e - 6), 
(8.3.12) 

438 
Some Aspects of Multivariate Analysis 
where 
(11 -
k)5 2 = (y - y)' (y - y) = (y - xey (y -
Xe) 
and, assuming X is of rank k , 
fI = (X'X)-l X'y 
so that, writing v = 11 -
k, 
[ 
(9 - e)'x'X(O -
e) l- "~ ('" 
k) 
p(9 I y) cc 
1 + _. 
2 
' 
V5 
-
00 < 0 < 00. 
8.4 
(8,3.13) 
The posterior distribution of 0 is thus the k dimensi onal Ik[e, 5 2(X'Xr l, v] 
distribution. Further, integrating out {) fr()m (8.3,9) yields the distribution of u 2 , 
( 
\'5
2
) 
p(u 2
1 y) if:. (u 2 ) -0,'''' 1) exp -
2u 2 
' 
(8.3.14) 
so that u 21(v5 2) has the X:2 distribution, All the above results have, of course, 
been already obtained earlier in Section 2.7, 
it is clear that the general linear model (8.3.2) which can be regarded as the 
multivariate generalization of (8,3,8) need not be particularized in any way. The 
matrices XI' "" Xm mayor may 1101 have elements in common; furthermore, 
the vectors of parameters 8 I ' "" Om mayor may 1101 have elements in common. 
Using sampling theory, Zellner (1962, 1963) attempted to study the situation 
in wh ich the Xj were not assu med to be identical. The main difficulty with his 
approach was that the minimum variance estimator for 0 involves the unknown ~, 
and the estimators proposed are "optimal" only in the asymptotic sense, 
Cases of special interest which are associated with practical problems of 
importance and which relate to known results include: 
a) wilen the derivative matrices XI = 
= X ", = X are common but the 
parameters 8 1 , "., 9m are not, 
b) when 9 1 
". = 0111 but the matrices XI ' "" Xm are not, and 
c) when 9 1 = ... = Om al1d XI = ." = XIII' 
In the remaining part of this chapter, we shall discuss case (a), The problem 
of estimating common parameters which includes (b) and (c) will be treated in 
the next chapter. 
8.4 I~FERE~CES ABOUT 9 FOR THE CASE OF A COMMO]'; DERIVATIVE 
"'lATRIX X 
The model for which Xl = ", = XIII = X (so that kl =, = k m = k) and 
9 1 i= ." i= Om has received most attention in the sampling theory framework 
·see, for example, Anderson (J 958). 
From the Bayesian point of view, the 
problem has been studied by Savage (1961 a), Geisser and Cornfield (1963), 
Geisser (1965a), Ando and Kaufman () 965), and others, 
In general, the 

8.4 
fnferences about e for the Case of a Common Derivative Matri>: X 
multivariate model in (8.3.2) can now be written 
y=XO+r. 
[yJ=[X] [OJ+[EJ 
I7xm Ilxk kxm nxm 
439 
(8.4.1 ) 
where the notation beneath the matrices indicates that Y IS an n x 111 matrix of 
m-variate observations, 0 is a k x 111 matrix of parameters and f: an 
11 x m 
matrix of errors. 
The model would be appropriate for example if say a 2P factorial experiment 
had been conducted on a chemical process and the output )' 1 = product yield, 
Yz = product purity, Y3 = product density had been measured. 
The elements 
of each column of the common matrix X would then be an appropriate sequence 
of + l's and - I's corresponding to the experimental conditions and the "effect" 
parameters 0i would be different for each output. In econometrics, the model 
(8.4.1) is frequently encountered in the analysis of the reduced form of 
simultaneous equation systems. 
We note that the k x m matrix of parameters 
0= 
(8.4.2) 
can be written in the two alternative forms 
0= [0 1 , ... ,Oi, ... ,O",J 
(8.4.3) 
where 0i is the ith column vector and O;g) is the gth row vector of O. 
For 
simplicity, we shall assume throughout the chapter that the rank of X is k . 
8.4.1 Distribution of 0 
Consider the elements of the m x m matrix S(O) = {Sij(Oi, O)} of (8.2.4). When 
XI = ... = Xm = X, we can write 
Sij(Oi,O) = (Yi -
X OJ' (Yj -
X OJ) 
= (Yi -
X 9Y (Yj -
X 9) + (Oi - ey X'X(Oj - 0) 
(8.4.4) 
where ei=(X'X)-lX'Yi 
is the least squares estimates of Oi,i=l , ... ,m. 

440 
Some Aspects of Multivariate Analysis 
8.4 
Consequently, 
S(9) = A + (9 - 6)' X'X(S - 0), 
(8.4.5) 
where 6 is the k x m matrix of least squares estimates 
(8.4.6) 
and A is the m x m matrix 
with 
i,j = I, ... ,111, 
(8.4.7) 
that is, A is proportional to the sample covariance matrix. 
For simplicity, we 
shall assume tbat A is positive definite. From the general result in (8.2.23), the 
poslerior distribution of 9 is then 
p(SI y) cc IA + (S - 6)' X'X(S - OW"/2, 
-
00 < 9 < 00. 
(8.4.8) 
As mentioned earlier, when there is a single output (m = I), (8.4.8) is in the form 
of a k-dimensional multivariate f distribution. The distribution in (8.4.8) is a 
matrie-variate generalization of the t distribution. 
It was first obtained by 
Kahirsagar (J960). A comprehensive discussion of its properties has been given 
by Dickey (1967b). 
8.4.2 Posterior Distribution of the Vleans from a m-dimensional Normal 
Distribution 
In the case Ie = I where each 9; consists of a single element and X is a 11 x J 
vector of ones, expression (8.4.8) is the joint posterior distribution of the 
m means when sampling from an m-dimensional multivariate Normal distribution 
N",(S,1:.). 
In this case 
II 
X' X = n 
and 
a 
"( 
- ) ( 
- ) 
ij = 
L 
Ylli -
Y; 
Yuj -
Yj , 
(8.4.9) 
u= 1 
where 
I 
II 
Yi = n· I Ylli· 
u~l 

8.4 
Inferences about 9 for the Case of a Common Derivative Matrix X 
441 
The posterior distribution of 0 can be written 
p(OI y) oc IA + n(O -
9)' (0 - 9W n/2 
oc II + n A -! (0 -
9)' (0 -
9)1-,,/2, 
-
00 < 0 < 00. 
(8.4.10) 
We now make use of the fundamental identity 
Ilk - P QI = II, -
Q PI, 
(8.4.11) 
where Ik and I, are, respectively, k x k and a I x I identity matrices, P is a k x I 
matrix and Q is a I x k matrix. 
Noting that (0 - 9) is a 1 x m vector, we 
immediately obtain 
p(OI y) oc [I + n(O - 9) A - I (0 - 9)'] -11/ 2, 
-
00 < tJ i < 00 , 
i = 1, ... , m, 
(8.4.12) 
which is a rn-dimensional tm [9', n -I (n -
m) -1 A, n -
mJ distribution, a result 
first published by Geisser and Cornfield (1963). 
Thus, by comparing (8.4.12) 
with (8.3.13), we see that both when m = 1 and when k = I, the distribution In 
(8.4.8) can be put in the multivariate t form. 
8.4.3 Some Properties of the Posterior Matric-variate t Distribution of 0 
When neither rn nor k is equal to one, it is not possible to express the 
distribution of 0 as a multivariate t distribution. 
As we have mentioned, the 
distribution in (8.4.8) can be thought of as a matric-variate extension of the 
t distribution. We now discuss some properties of this distribution. 
TII'o equivalent Representations of the Distribution of 0 
It is shown in Appendix A~U that, for v > 0, 
J" 
II", + A-I (0 -
9)' X'X(O - 9)I- t (v+k + m-1) dO 
-00<0 < 00 
where 
v = n -
(k + rn) + 1, 
r [_l( v + In -
J)J 
c(rn, k, v) = [r(})]mk _
'_" _2 _____ 
_ 
l,JHv + k + m -
I)J 
(8.4.13) 
(8.4.14) 
and 1 pCb) is the generalized Gamma function defined in (8.2.22). Thus, 
p(OI y) = (c(rn, k, v)r I IX'Xlm,21 A - Ilk/211m + A - I (0 - 9)' x' X(O _ 9)1 -"H, rH m- I), 
-
00 < 0 < 00. 
(8.4.15) 

442 
Some Aspects of Multivariate Analysis 
8.4 
and we shall say that the k x m matrix of parameters 6 is distributed as 
tkm [9, (X'X)-I, A, v]. 
Note that by applying the identity (8.4.11), we can write 
11m + A -1(0 - 9)' X'X(8 -
9)1 = Ilk + (X' X) (8 -
9)A -1 (0 -
9)'1 
(8.4.16) 
so that in terms of the m x k matrix e' the roles of m and k on the one hand 
and of the matrices (X'X)-I and A on the other are simultaneously interchanged. 
Thus, we may conclude that if 
6 ~ tkm[9, (X'X)-I, A, v], 
then 
(8.4.17) 
It follows from these two equivalent representations that 
c(k, 111 , v) == c(m, k, v), 
1k [-HI' + k -
I)J 
= ~m [ H I' + m -
I)] 
1k [HI' + k + m -
1)] -
1m [1(v + k + m -
I)J' 
that is, 
(8.4.18) 
Marginal and Conditional Distributions of Subsets of Co/umns of 8 
We now show that the marginal and conditional distributions of subsets of the 
m columns of 8 are also matric-variate t distributions. Let m = m l + mz and 
partition the matrices 6, 9, and A into 
Then: 
a) conditional on 61*, the subset 6H is distributed as 
where 
H- 1 = (X'X)-l + (8 1* -
91*)A~/ (6 1* - 91*)', 
6z* = 9a + (6 1* - 91 *)A~II A 12 , 
A zz' l = A2l -
A ZI A~II Au · 
b) 61* is distributed as 
81* ~ tkm , [91*' (X'X)-I , All , vl 
To prove these results, we can write 
(6 -
9)A- I(6 -
9)' = (61* -
91*)A~lt(61* - 9 1*)' 
+ (6H 
- 62 *)A2't 1 (6H 
- 62*)', 
(8.4.19) 
(8.4.20) 
(8.4.21 ) 
(8.4.22) 

8.4 
Inferences about e for the Case of a Common Derivative Matrix X 
The determinant on the right-hand side of (8.4.16) can now be written 
Ilk + (X'X) (9 - fhA -I (9 -
8)' 1 = Ilk + (X'X)(91 * - 81 *)A ~/ (91 * - 8 1 ~J 'I 
X Ilk + H(62* - 9H )A;2
1 1 (62* - 92*)'1. 
443 
(8.4.23) 
Substituting (8.4.23) into (8.4. J 5), we see that, given 9 1 *, the conditional distribution 
of6h is 
p(92* 161 *. y) oc Ilk + H(9h 
- 9H )A;21'1 (9h 
- 92*)'1 -i(v+k + m-I) 
oc II + A -I (9 
- 9 )'H(9 
- 9 
)1- ·~[(v+ml)+k+m2 - ll 
'"1 
22'1 
1* 
2* 
2* 
1* 
(8.4.24) 
From (8.4.13), the normalizing constant is 
[c(m2,k , v + ml)rIIHlm,'2IAi}1Ik 2. 
(8.4.25) 
Thus. given 9 1*, the k x m 2 matrix of parameters 9 2* is distributed as fkm1 (OH' H - I, 
Al2 . I .V + m l ). For the marginal distribution of 6 1 *. since 
use of (8.4.23) through (8.4.25) yields 
p(9 1* I y) oc IHI-m 2l2 Ilk + (X'X) (91* -
81*)A~11 (91* _ 81*)'I -}(v+k+m-l) 
II + A -I (9 
9-
)'X'X(9 
9-
)I-:(v+k+m,-I) 
OC 
m, 
II 
1* -
1* 
1 * -
1* 
' 
-
00 < 9u < 00 . 
That is, the k x Inl matrix of parameters 91* is distributed as 
fkm , [81*, (X'X)-I, All' v]. 
MarRinal Distribution of a Particular Column of 9 
(8.4.26) 
In particular, by setting 
1111 = I in (8.4.21), the marginal distribution of 
9 1 * = 9 1 is the k-dimensional multivariate I distribution 
-
00 < 6 1 < 00, 
(8.4.27) 
where all = All is now a scalar, that is, 6 1 ~ Ik [8 1 , V-I a l I (X'X)-l, v]. 
By mere relabeling we may conclude that the marginal distribution of the 
ith column of 6 in (8.4.2) is 
-
00 < 9j < 00. 
(8.4.28) 
It will be noted that this distribution is identical to that obtained in 
(8.3.13) when only a single output was considered, except that in (8.3.13) 

444 
Some Aspects of Multivariate Analysis 
8.4 
v = n - k, but in (8.4.28) v = 11 -
k -
(m -
I). 
In a certain sense, the 
reduction of the degrees of freedom by 111 -
I is not surprising. In adopting the 
multivariate framework, 
m(m - I )/2 additional parameters 
(Jij 
(i #- j) are 
introduced. A part of the information from the sample is therefore utilized to 
estimate these parameters and (/11 -
I) of them «(Jil, ... , (Ji(i-l), (Ji(i+ I)' ... , (Jim) 
are connected with y;. We may say that 'one degree of freedom is lost' for each of 
the (m -
I) additional parameters. 
On the other hand, it is somewhat puzzling that if we ignored the 
multivariate structure of the problem and treated y; as the output of a univariate 
response, then on the basis of the noninformative prior p(8;, (J;;) cc (Jii j. we 
would obtain a posterior multivariate t distribution for 8i with (m -
I) additional 
degrees of freedom. This would seem to imply that. by ignoring the information 
from 
the other 
(m -
I) 
responses Yj, ···,Yi-j,y;+\, ... ,Ym, 
more precise 
inference about 8; could be made than when all the m responses were considered 
jointly. This phenomenon is related to the "paradox" pointed out by Dempster 
(1963) and the criticisms of the prior in (8.2.14) by Stone (1964). 
The above implication is admittedly perplexing, and further research is needed 
to clarify the situation. We feel, however, that the multivariate results presented 
in this chapter are of considerable interest and certainly provide a sensible basis 
for inference in the common practical situation when (n -
k) is large relative to m. 
Distribution oj 8 Expressed as a Product oj Multivariate t Distributions 
We note that for the partition in (8.4.19), if we set mj = m -
J and in2 = I, 
then from (8.4.20) the conditional distribution of 9Z* = 9"" 
given 91* = 
[81> ... ,9",_,], is 
(8.4.29) 
where 
a, ... 12 ... (m-l) = A 22 · 1· 
From the marginal distribution of 8 1* = [9 1, .. , 8m -
1J in (8.4.21) if we 
partition 9 1* into [8s* :8m - 1] where 8s* = [9 1,· ··,9",-2J, it is clear that the 
conditional distribution of 8m -
I , given 8s*, is again a k-dimensional multivariate 
t distribution. It follows by repeating the process m -
I times that, if we express 
p(8 I y) as the product 
(8.4.30) 
then each factor on the right-hand side is a k-dimensional multivariate 
t distribution. 
Marginal and Conditional Distributions oj ROll'S oj 9 
Results very similar to those given above for column decomposition of 8 can 

8.4 
Inferences about e for the Case of a Common Derivative Matrix X 
445 
now be obtained for the rows of 8. Consider the partitions 
k, 
k2 
k ! 
k2 
8' = [8( 1)* : 8(2)*J m' 
8' = [8( 1)* ~ 9(2)*J /II 
(8.4.31 ) 
(X'X) - l = e = [~:~
: ~::.]:> 
where it is to be remembered that 6; 1)* are the first k I rows and 9;2)* the remaining 
k2 rows of 8. Since the m x k matrix 9' is distributed as Imk[9', A, (X'X)-I, v] , 
it can be readily shown that 
a) given 6( I )*, 
(8.4.32) 
where 
e 22' 1 = e 22 -
e 21 e~ll e 12, 
G(2)* = 8(2)* + (9(1)* -
8(1)*) e~11 elZ' 
and 
b) Marginally, 
(8.4.33) 
or equivalently, 
9(1)* -
[kim [9;1)*' ell' A, v]. 
c) The gth row of e, 6(0)' is distributed as 
, 
-
I 
9(g) -
1m [6(g ), V 
Cgg A , v J 
(8.4.34) 
where Cgg is the (gg )th element of C. 
d) The distribution of 6 can alternatively be expressed as the product 
(8.4.35) 
where, parallel to (8.4.30), each factor on the right-hand side is an tn-dimensional 
multivariate I distribution. 
Comparing expression (8.4.34) with the result in (8.4.12), we see that, as was 
the case with a column vector of 9, the two distributions are of the same form 
except for the difference in the "degrees of freedom" They now differ by (k -
I), 
simply because an additional (Ie -
1) "input variables" are included in the model. 
Marginal Dislribution of a Block of Elements of 9 
Finally consider now the partitions 
ni, 
. ] k I, 
k, 
. ] k I • 
k , 
(8.4.36) 

446 
SOllie Aspects of Multivariate Analysis 
8.4 
It follows from (8.4.21) and (8.4.33) that the k I x 1111 matrix of parameters 0 1 , 
is distributed as 
(8.4.37) 
The marginal distributions of 9 12 • 92 " 
022, and indeed that of any block of 
elements of 0 can be similarly obtained, and are left to the reaJer. 
In the above we have shown that the marginal and the conditional 
distributions of certain subsets of 0 are of the matric-variate 'form. This is, 
however, not true in general. For example, one can show that neither the marginal 
distribution of (9", ( 22 ) nor the conditional distribution of (9'2, 92 ,) given 
(9 ll • ( 22) is a matric-variate t distribution. The problem of obtaining explicit 
expressions for the marginal and the conditional distributions in general is quite 
complex, and certain special cases have recently been considered by Dreze 
and Morales (1970) and Tiao. Tan, and Chang (1970). 
Means and Covariance Matrix orO 
From (8.4.28), the matrix of means of the posterior distribution of 9 is 
£(9) = 9 
(8.4.38) 
and the covariance matrix of 9j is 
a· 
Cov (9J = --" 2- (X' Xr-1, 
v-
i = I, ... , m. 
(8.4.39) 
For the covariance matrix of 9j and 9i , with no loss In generality we consider 
the case i = 1 and j = 2. Now 
£(9 1 - 9,) (9 2 -
( 2)' = £ (9 1 -
( 1) £ (9 2 -
( 2)" 
0 , 
O,la, 
If we set 111, = 2 in (8.4.21) and perform a column decomposition of the k x 2 
matrix 9,* = [9,,9 2], it is then clear from (8.4.20) that 
£ (92 -
(2) = 
a~11 a'2 (9, - 9,) 
O2 \9, 
so that, as might be expected, 
= __ 1_ [all a12] ® (X'X)-l 
v-2 a'2 a22 
(8.4.40) 
(8.4.41) 

8.4 
Inferences about (} for the Case of a Common Derivative .Vlatrix X 
447 
where Q9 denotes the Kronecker product- see Appendix A8.4. 
In general, if 
we write the elements of 0 and e as 
0 ' = (0'[, _._ ,0;,,), 
0 ' = (8'1' ... ,6;,,), 
where 0 and 0 are kin x 1 vectors, rhen 
~ 
~ 
I 
Cov (0) = £(0 - 0) (0 - 0)' = -- A Q9 (X'X)-l. 
V -
2 
Bya similar argument, if we write 
then 
0~ = (0; I I' ... , 0rk»), 
I 
Cov (0*) = -- (X'X) - l Q9 A. 
v - 2 
Linear Transformation of 0 
(8.4.42a) 
(8.4.42b) 
(8.4.43a) 
(8.4.43 b) 
Let P be a kJ x k(kl ~ k) matrix of rank kl and Q be a I11xm l (m J ~ 111) 
matrix of rank 111 1 , Suppose <!> is the k [ x m 1 matrix of random variables obtained 
from the linear transformation 
<!> = POQ. 
(8.4.44) 
Then <!> is distributed as Iklln l [P 6 Q, P(X'X) - Jp', Q' AQ, v]. 
The proof is left 
as an exercise for the reader. 
Asymptotic Dislribution of 0 
When v tends to infinity, the distribution of 0 approaches a km dimensional 
multivariate Normal distribution, 
x exp [- i td:- l (8 - 6)' X'X (0 ~ 6)J, 
-
00 < 8 < 00, 
(8.4.45) 
where 
t = v lA, 
and we shall say that, asymptotically, 0 ~ Nmk [8, t Q9 (X'X)-l]. 
To see this, in (8.4.15) let 
Q = v A -I CO - 6)' X'xeo - 8). 
= t- [ e8 - 8)' X'X(9 - e). 
Then, we may write 
It! 
11m + v-1QI = TI (I + v-J},J 
(8.4.46) 
i= 1 

448 
Some Aspects of Multivariate Analysis 
where (AI' ... ,Am) are the latent roots of Q. Thus, as v -> 00 
= exp ( - t tr Q). 
Since 
tr Q = (0 - 0)' t- 1 ® (X'X)(0 - 0) 
where 0 and 0 are the km x I vectors defined in (8.4.42a), and noting that 
the desired result follows at once. 
It follows that 
or, alternatively, 
E (0) = 0, 
E (0*) = 0*, 
Cov (0) = t® (X'X)-l 
8.4 
(8.4.47) 
(8.4.48a) 
(8.4.48b) 
where (0,0) and (0*,0*) are defined in (8.4.42a) and (8.4.43a), respectively. 
8.4.4 H.P.D. Regions of e 
Expressions (8.4.28) and (8.4.34) allow us to make inferences about a specific 
column or row of e. Using properties of the multivariate t distribution , H.P.D. 
regions of the elements of a row or a column can be easily determined . 
We now discuss a procedure for the complete set of parameters e, which 
makes it possible to decide whether a general point e = eo is or is not included 
in an H.P.D. region of approximate content (l -
ry). 
It is seen in expression (8.4.15) that the posterior distribution of e is a 
monotonic increasing function of the quantity Vee), where 
vee) = IA + (e _I~I, X'X(9 _ 9)1 = 111/, A + -1 (e - 9)' X'X (9 - 8)1-
1 
(8.4.49) 
Consequently, the parameter point e = eo lies inside the (I -
ry) H.P.D. region 
if and only if 
Pr {vee) > v(eo) I y} ~ (I -
ry), 
(8.4.50) 
8.4.5 Distribution of Vee) 
To obtain the posterior distribution of Vee) so that (8.4.50) can be calculated, 
we first derive the moments of Vee). Applying the integral identity (8.4.13) the 

8.4 
Inferences about e for the Case of a Common Derivative Matrix X 
hth moment of V(9) is found to be 
E[V(W I y] = c(m, k, v + 2h) 
c(m,k,v) 
fI r[J(v -
1 + s) + h] r[t(v -
I + k + s)] 
s=l r[Hv -
1 + s)] r[-Hv -
I + k + s) + h] 
449 
(8.4.51) 
From (8.4.46) and (8.4.49) it follows that V = V(9) is a random variable 
defined on the interval (0, I) so that the distribution of V is uniquely determined 
by its moments. Further, expression (8.4.51) shows that distribution of V is a 
function of (m, k, v). 
Adopting the symbol U (m,k,v) to mean a random variable 
whose probability distribution is that implied by the moments in (8.4.51), we 
have the following general result. 
Theorem 8.4.1 Let 9 be a k x m matrix of constants, X' X and A be , respectively, 
a k x k and a m x m positive definite symmetric matrix of constants, and 
v > O. If the k x m matrix of random variables 
then 
U(9) ~ U(m,k , v) 
where 
U(9) = 11m + A -1 (9 - 9)' X'X (9 - 8)1- 1 
As noted by Geisser (1965a), expression (8.4.51) correspond exactly to that 
for the 11th sampling moment of V(90 ) in the sampling theory framework when 
9 are regarded as fixed and y random variables. Thus, the Bayesian probability 
Pr{U(9) > V(9 0) I y} is numerically equivalent to the significance level associated 
with the null hypothesis 9 = 90 against the alternative 9 =1= 90 , 
Some Distributional Properties 0/ V (m.k,v)' 
Following the development, for example, in Anderson (1958), we now discuss some 
properties of the distribution of U(m ,k,v)' 
It will be noted that the notation V(m,k,v) 
here is slightly different from the one used in Anderson. Specifically, in his notation, 
v is replaced by v + m -
I. 
a) Since from (8.4.18) c(k, m, v) = c(k, m, v), the hth moment in (8.4.51) can be 
alternatively expressed as 
I 
k r[!(v -
J + t) + h] r[}(v -
1 + m + t)] 
£[U(9)'1 y] = TI 
. 
1=1 r[-}(v -
1 + t)]r[-i(v -
1 + m + t) + h] 
(8.4.52) 
By comparing (8.4.51) with (8.4.52), we see that the roles played by m and k can 
be interchanged. That is, the distribution of U(m.k,v) is identical to that of V(k,m,v)' In 
other words, the distribution of V = V(9) arising from a multivariate model with m 
output variables and k regression coefficients for each output is identical to that from a 

450 
Some Aspects of :YIultivariate Analysis 
8.4 
multivariate model with k output variables and m regression coefficients for each output. 
With no loss in generality, we shall proceed with the m-output model, i.e., the 
V(m,k,v) distribution. 
b) Now (8.4.51) can be written 
E(V"ly)=11B 
+h, -
B 
,-, 
m (V-l+S 
k)/ (V-l+S k) 
s= 1 
2 
2 
2 
2 
(8.4.53) 
where B(P, q) is the complete beta function. The right-hand side is the hth moment of 
the product of m independent variables Xl'"'' xm having beta distributions with 
parameters 
It follows that V is distributed as the product X I ... x m . 
c) Suppose m is even. Then we can write (8.4.53) as 
[
V + 2(1 -
1) 
k 1 (V - 1 + 21 
k ) 
B 
+ h,~ B 
+ h,-
m/ 2 
2 
2 
2 
2 
E( Vh I y) = 11 
. 
1=1 
B[V+2(/-l) !....JB(V-I+21!"") 
2 
' 2 
2' 2 
(8.4.54) 
Using the duplication formula 
(8.4.55) 
so that 
B(p + l,q) B(p, q) = 22q B(2p, 2q) B(q, q), 
(8.4.56) 
we obtain 
m/2 B [v + 2(1 -
I + h) kJ 
E(V"I y) = 11 
' 
1=1 
B[v + 2(1 -
1), kJ 
(8.4.57) 
where ZI' .. . , z",/2 are m/2 independent random variables having beta distributions with 
parameters [v + 2(1 -
1), k], t = 1, .. . , m12, respectively. Thus, V is distributed as the 
d 
2 
2 
pro uct zl .. . zml2' 
The Case m = 1. 
When m = 1 so that v = n -
k, it follows from (8.4.53) that, V has the beta 
distribution with parameters [en - k)j2, kj2] so that the quantity (n -
k) (I -
V)/(k V) 
is distributed as F with (k, n -
k) degrees of freedom. This result is of course to be 

8.4 
Inferences about 9 for the Case of a Common Derivative Matrix X 
451 
expected, since for m = 1, we have IAI = (n -
k)s2, where 
S2 = (n -
k) -I (y -
y)' (y - y), 
so that 
( 
I - U) (n -k ) = (9 - 6)' X'X(9 - 6) 
U 
k 
ks 2 
(8.4.58) 
which, from (2.7.21), has the F distribution with (k, n -
k) degrees of freedom. 
The Case m = 2. 
When m = 2, v = n -
k -
1 and from (8.4.57) Ul/ 2 is distributed as a beta variable 
with parameters (n -
k -
I, k). Thus, the quantity 
(I - U
J
/
2
) (n - k - 1) 
(n - k - 1 ) 
U 1/ 2 
k 
= (112 + A- 1 (9 - 9),X'X(e - 9W/2) - I) 
k 
(8.4.59) 
has the F distribution with [2k, 2(n -
k -
I)] degrees of freedom. 
8.4.6 An Approximation to the Distribution of U for General m 
For m ~ 3, the exact distribution of U is complicated, see e.g. Schatzoff (1966) 
and Pillai and Gupta (1969). We now give an approximation method following 
Bartlett (1938) and Box (1949). In expression (8.4.51), we make the substitutions 
x = tv, 
M = -
rp v log L", 
as = i (s + k -
I) 
t = - h/(rp v), 
and 
bs = }(s -
1), 
where rp is some arbitrary positive number, so that 
£(U h I y) = £(etM I y) 
= fI rex + as) 
['[rpx(J - 2t) + x(I -
rp) + bsJ 
s = 1 rex + bs) f'[rpx(l - 2t) + x(l -
rp) + as] 
(8.4.60) 
(8.4.61) 
In terms of the random variable M, (8.4.6\) is then its moment-generating 
function. 
Taking logarithms and employing Stirling's series (see Appendix 
A2.2), we obtain the cumulant generating function of M as 
(8.4.62) 
where 
(- IY 
()J, = -r-'-( ,-+-1 )--:(-rpx--")-' 
m I 
{B,+J[x(l -
rp) + bsJ - B,+I[X(l -
rp) + aJ} 
s= 1 

452 
Some Aspects of Multivariate Analysis 
8.4 
and BrCz) is the Bernoulli polynomial of degree r and order one. 
The 
asymptotic expansion in (8.4.62) is valid provided cp is so chosen that x(1 -
cp) 
is bounded. In this case, Wr is of order O[(cpx)-r] in magnitude. 
The series in (8.4.62) is of the same type as the one obtains in (2.12.15) 
for the comparison of the spread of k Normal populations. In particular, the 
distribution of M = -
cpv log V can be expressed as a weighted series of '/ densities, 
the leading term having mk degrees of freedom. 
The Choice of cp 
It follows that, if we take the leading term alone, then to order O[(cpx) -I] = 
O[(cp1v)-I] the quantity 
M = -
cpvlog V 
(8.4.63) 
is distributed as X;'k> provided 11'(1 -
cp) is bounded. 
In particular, if we 
take cp = 1, we then have that M = -
v log V is distributed approximately 
as X;k. 
For moderate values of v, the accuracy of the X2 approximation can be 
improved by suitably choosing cp so that WI = O. This is because when WI = 0, 
the quantity M will be distributed as X;k to order O[(cp-l V)-2]. 
Using the fact 
that 
B2(z) = Z2 -
Z + .~ 
it is straight forward to verify that for WI = 0, we require 
1 
cp = I + -
(m + k - 3). 
21' 
(8.4.64) 
(8.4.65) 
This choice of cf; gives very close approximations in practice. An example 
with v = 9, m = k = 2 will be given later in Section 8.4.8 to compare the 
approximation with the exact distribution. 
It follows from the above discussion that to order O[(cp _~V)-2J , 
with 
Pr {V(9) > V(9o) [y} == Pr {X;k < - cpv log V(9 0 ) } 
I 
cp = 1 + -
(m + k -
3), 
2v 
log V(90 ) = 
-
log [1m + A -I (eo - 8)' X'X (eo - 8)[ 
(8.4.66) 
which can be employed to decide whether the parameter point e = 90 lies 
approximately inside or outside the (\ -
IX) H.P.D. region . 
8.4.7 Inferences about a General Parameter Point of a Block Submatrix of e 
In the above, we have discussed inference procedures for (a) a specific column 
of e, (b) a specific row of 9, and (c) a parameter point eo for the complete set of e. 

8.4 
Inferences about {I for the Case of a Common Derivative Matrix X 
453 
In some problems, we may be interested in making inferences about the 
parameters belonging to a certain block submatrix of 9. 
Without loss of 
generality we consider only the problem for the k 1 X m 1 matrix 9 11 defined 
in (8.4.36). From (8.4.37) and Theorem 8.4.1 (on page 449), it follows that the 
quantity 
(8.4.67) 
is distributed as U(ml,kl, v)' 
This distribution would then allow us to decide 
whether a particular value of 911 lay inside or outside a desired H.P.D. region . 
In particular, for m l > 2 and kl > 2, we may then make use of the approximation 
(8.4.68) 
where 
so that the parameter point 9 11 ,0 lies inside the (J -
a) H.P.D. region if and 
only if 
8.4.8 An Illustrative Example 
An experiment was conducted to study the effect of temperature on the yield 
of the product Yl and the by-product Y2 of a chemical process. Twelve runs were 
made at different temperature settings ranging from 161.3°P to 195.7°F. 
The 
data are given in Table 8.4.1. 
The average temperature employed isT = 177.86. 
We suppose a model 
to be entertained whereby, over the range of temperature explored, the 
relationships between product yield and temperature and by-product yield and 
temperature were nearly linear so that to an adequate approximation 
u = I, .. . , 12, 
(8.4.70) 
where Xu = (T., - T)iIOO, the divisor 100 being introduced for convenience in 
calculation. The parameters ell and e l2 will thus determine the locations of the 
yield-temperature lines at the average temperature 'f while e 2 l and e2 2 will 
represent the slopes of these lines. 
The experimental runs were set up 
independently and we should therefore expect experimental errors to be indepen-
dent from run to run. 
However, in any particular run, we should expect the 
error in Yl to be correlated with that in Y2 since slight aberations in reaction 

454 
Some Aspects of Multivariate Analysis 
8.4 
Table 8.4.1 
Yield of product and by-product of a chemical process 
Temp.oF 
Product Yl 
By-product Y2 
161.3 
63.7 
20.3 
164.0 
59.5 
24.2 
165.7 
67.9 
18.0 
170.1 
68.8 
20.5 
173.9 
66.1 
20.1 
176.2 
70.4 
17.5 
177.6 
70.0 
18.2 
181.7 
73.7 
15.4 
185.6 
74.1 
17.8 
189.0 
79.6 
13.3 
193.5 
77.1 
16.7 
195.7 
82.8 
14.8 
conditions or in analytical procedures could simultaneously affect observations 
of both product and by-product yields. Finally, then, the tentative model was 
YIII = 811x II I + 821 X ll2 + ell I 
Yu2 = 81 2X III + 822x u2 + eu2, 
(8.4.71) 
where X II 2 = x," and x u l = 1 is a dummy variable introduced to "carry" the 
parameters 81l. and B12 • 
It was supposed that (eUI ' e1l2 ) followed the bivariate 
Normal distribution N 2(0,1:). 
Given this setup, we apply the results arrived at earlier in this section to make 
inferences about the regression coefficients 
6= 
(8.4.72) 
against the background of a noninformative reference prior distribution for 
8 and 1:. 
The relevant sample quantities are summarized below : 
A = [ 61.4084 
- 38.4823 
11 = 12 
m=k=2 
c = (XX) - I = [0.0~33 
- 384823J 
36.7369 
A -I = [0.0474 
0.0496 
6 .8~50J 
0.0496l 
0.Q792J 
(8.4.73) 

8.4 
and 
The fitted lines 
Inferences about e for the Case of a Common Derivative Matrix X 
(j = [7l.1417 
54.4355 
18.0666] 
, 
, 
[9(1)] 
-20.0933 = [0 1 , ez] = 
9(2) . 
91 = ell + e2 1(T- T) x 10- 2 
92 = e12 + e22(T -
T) X 10- 2 
455 
together with the data are shown in Fig. 8.4.1. As explained earlier, in a real data 
analysis, we should pause at this point to critically examine the conditional infer-
ence by study of residuals. We shall here proceed with further analysis supposing 
that such checks have proved satisfactory. 
80 
x 
?O 
70 
X 
• 
10 
• 
Product Y I 
60 
• 
X By-produ ct Y 2 
T .... 
160 
170 
180 
190 
200 
Fig. 8.4.1 Scatter diagram of the product Yt and the by-product Y2 together with the best 
fitting lines. 
Inferences about el = (8 1 1, 821 ), 
When interest centers primarily on the parameters at for the product Yl' we 
have from (8.4.28) that 
peel I y) cc [61.4084 + (e t - 9t ), x'xce t - 9t)r ll / 2 
(8.4.74) 
that is, a bivariate t 2[9 1, 6.825(X'X) - I , 9J distribution. 
Since the matrix X'X 
is diagonal, 8 11 and 821 are uncorrelated (but of course not independent). 

456 
Some Aspects of Multivariate Analysis 
8.4 
Figure 8.4.2a shows contours of the 50, 75 and 95 per cent H.P.D. regions 
together with the mode 61, from which overall conclusions about fll may be drawn. 
74 
72 
70 
68 
L-~------~------~------~------~------~--e21~ 
30 
40 
50 
60 
70 
80 
(a) 
Fig. 8.4.2a Contours of the posterior distribution of fll' the parameters of the product 
straight line. 
20 
18 
16 
L-~----------~------~------~------~-----a22~ 
- 40 
- 30 
- 20 
- 10 
0 
(b) 
Fig.8.4.2b Contours of the posterior distribution offl2, the parameters of the by-product 
straight line. 
Inferences about fl2 = (812 , ( 22)' 
Similarly, from (8.4.28), the posterior distribution of fl2 for the by-product Y2 is 
P(fl2 I y) ex:. [36.7369 + (fl2 -
( 2)' X'X(fl2 - ( 2)r 11/2, 
(8.4.75) 
--

8.4 
Inferences about e for the Case of a Common Derivative Matrix X 
457 
which is a t 2 [6z,4.08(X'X)-1,9] distribution. 
Again, the parameters 8 12 and 
8z2 are uncorrelated. The 50, 75 and 95 per cent H.P.D. contours together with 
the mode ez for this distribution are shown in Fig. 8.4.2b. The contours have 
exactly the same shape and orientation as those in Fig. 8.4.2a because the same 
X' X matrix is employed; the spread for 92 is however smaller than that for 9 I 
since the sample variance from Y2 is less than that from YI' 
Inferences about 9(2) = (821 , ( 22)' 
In problems of the kind considered, interest often centers on 9(2) = (8 21 , ( 22) 
which measure respectively the slopes of the yield/temperature lines for the product 
and by-product. From (8.4.34), the posterior distribution of 9(2) IS 
p(9(2) I y) ex: [6.875 + (9(z) - e(2)' A -I C9( 2) -
6(2))] -11 / 2, 
(8.4.76) 
that is, a t Z[9(2)' 0.764A, 9J distribution. 
Figure 8.4.3 shows tbe 50, 75 and 
95 per cent contours together with the mode 9(2)' Also shown in the same figure 
are the marginal distributions t(e 21 , 46.90, 9) and t(e22 , 28.05, 9) and the 
-10 
- 20 
-30 
-40 
~--------------~~~--~------~----~------~--~---821-
30 
40 
50 
60 
70 
Fig. 8.4.3 Contours of the posterior distribution of 9(2) and the associated marginal 
distributions for the product and by-product data. 

458 
Some Aspects of Vlultiva riate Analysis 
8.4 
corresponding 95 per cent H.P.D. intervals for 821 and 822' 
Figure R.4.3 
summarizes, then, the information about the slopes (821,822) coming from the 
data on the basis of a noninformative reference prior. 
The parameters are 
negatively correlated; the correlation between C8 21 , 822) is in fact the sample 
correlation between the errors (eul , eu2 ) 
(8.4.77) 
It is clear from the figure that care must be taken to distinguish between 
individual and joint inferences about (821 ,8 22), 
It could be exceedingly 
misleading to make inferences from the individual H .P.D. intervals about the 
parameters jointly (see the discussion in Section 2.7). 
J oint Inferences about 9 
To make overall inferences about the parameters [9 1, 92J, we need to calculate 
the distribution of the quantity U(9) defined in (8.4.49). For instance, suppose 
we wish to decide whether or not the parameter point 
171 
-30J 
lies inside the 95 per cent H.P.D. region for 9. 
We have 
so that 
(9 - 8) 'X'X(9 _ 9) = [ 31.8761 
o 
0 
-0.6108 
-0.6108] 
27.9272 
;A\ 
U(Oo) = IA + (90 - 9) 'X'X(90 -
9)1 
775.0668 
= 4503.8878 = 0.1720, 
(8.4.78) 
(8.4.79) 
(8.4.80) 
Since m = 2, we may use the result in (8.4.59) to calculate the exact probability 
that U(9) exceeds U(9 0 ). We obtain 
Pr {C(9) > U(9o) 1 y} = 1 -IJo.l72o (9,2) = 1 - 0.0022 = 0.9978. 
(8.4.81) 
From (8.4.50), we conclude that the point Co lies outside the 95 per cent H.P.D 
region of O. 
Note that while the point 90 = (910,9 20) is excluded from the 
95 per cent region, Figs. 8.4.2a,b show that both the points 910 and 920 are 
included in the corresponding marginal 95 per cent H.P.D. regions. This serves 
to illustrate once more the distinction between joint inferences and marginal 
inferences. 
Approximating Distribution of U = U (0) 
It is 
informative to compare the exact distribution of U(O) with the 

8.5 
Some Aspects of the Distribution of :E 
459 
approximation in (8.4.03) using the present example (v = 9, m = k = 2). From 
(8.4.59), the exact distri bution of V is found to be 
1 
p(V) = 
V 3 .5 (l -
Vl/2) 
2B~,~ 
, 
O<V<1. 
Using the approximation given in (8.4.63) to (8.4.65), we find 
and 
A, = II 
'f' 
1 8 ' 
cjJv = 9.5, 
p(M) == tM exp ( - -tM), 
M = - 9.510g V 
0< M < 00. 
This implies that the distribution of V is approximately 
p(V) == (22.5625) (- log V) U 3 .75 , 
O<V<l. 
(8.4.82) 
(8.4.83) 
(8.4.84) 
Table 8.4.2 gives a specimen of the exact and the approximate densities of U 
calculated from (8.4.82) and (8.4.84). Although the sample size is only 10, the 
agreement is very close. 
Table 8.4.2 
Comparison of the exact and the approximate distributions of U for n = 12 and m = k = 2 
p(U) 
U 
Exact 
Approximate 
0.05 
0.00098 
0.00089 
0.10 
0.00973 
0.00924 
0.20 
0.08900 
0.08688 
0.30 
0.30098 
0.29731 
0.40 
0.66947 
0.66548 
0.50 
1.16498 
1.16239 
0.60 
1.69708 
1.69718 
0.70 
2.10935 
2.11244 
0.80 
2.17561 
2.18045 
0.90 
1.59706 
1.60130 
0.95 
0.95220 
0.95478 
8.5 SOME ASPECTS OF THE DISTRIBUTION OF 1: FOR THE CASE OF A 
COMMON DERIVATIVE MATRIX X 
We discuss in this section certain results pertaining to the posterior distribution 
of the elements of the covariance matrix 1: = {() ij}.t 
t For the important probJem of making inferences about the latent roots and vectors of1: 
which is not discussed in this book, see Geisser (1 965a) and Tiao and Fienberg (1969). 

460 
Some Aspects of Multil'ariate Analysis 
8.5 
8.5.1 Joint Distribution of (9, 1:) 
When the X/s are common, the joint posterior distribution of (9, 1:) in (8.2.17) 
can be written 
p(9, 1:\ y)oc \1:\-t(V+k+2m)exp {- itr1:- 1 [A + (9 -
9)'X'X(9 -
6)J}, 
-
00 < 9 < 00 , 
1: > 0, 
(8.5.1) 
where v = n -
(k + m) + I and use is made of (8.4.4) and (8.4.5). 
The 
individual and joint inferences about (8 2 ] , ( 22 ). 
It could be exceedingly 
distribution can be written as the product p(9, 1: \ y) = p(9 \ 1:, y) p(1: \ y). 
Conditional Distribution of 9 given 1: 
Given 1:, we have that 
p(a \ 1:, y) oc exp [ - t tr 1:- 1 (9 -
9)'X' X(9 -
9)J, 
-
00 < a < 00, 
(8.5.2) 
which by comparison with (8.4.45), is the km-dimensional Normal distribution 
(8.5.3) 
Marginal Distribution of 1: 
Thus, the marginal posterior distribution of 1: is 
1: > 0, 
(8.5.4) 
From the Jacobian in (8.2.13), the distribution of 1:- 1 is 
(8.5.5) 
which, by comparing with (8.2.20), is recognized as the Wishart distribution 
Wm(A -1, v) provided v > O. The distribution of 1: in (8.5.4) may thus be called 
an m-dimensional "inverted" Wishart distribution with I' degrees of freedom , 
and be denoted by w.,~ leA, v). 
From (8.2.21) the normalizing constant for the 
distri bu tions of 1: and 1: - 1 is 
IAIHv+m-l) 2--!-m(v+m-1) [r",C + ~ - 1)] -1. 
(8.5.6) 
Note that when m = 1 and v = n - k the distribution in (8.5.4) reduces to 
an inverted X2 distribution, 
P((Jll\Y)OC(J~;~. (n-k+2)exp( -
;~:1)' 
(Jl] > 0, 
(8.5.7) 
which is the posterior distribution of (52 = (J 1] with data from a univariate 
multiple regression problem (see Section 2.7) . . 
From the results in (8.5.1), (8.5.3), (8.5.4) and (8.4.15), we have the 
following useful theorem. 

8.5 
Some Aspects of the Distribution of 1: 
461 
Theorem 8.5.1 Let e be a Ie x In matrix of constants, X'X and A be, respectively, 
a k x k and a In x m positive definite symmetric matrix of constants, and v > O. 
If the joint distribution of the elements of the k x m matrix e and the m x m 
positive definite symmetric matrix 1: is 
pee, 1: I y) ex:. 11: 1 - ~ (v +k+ 2m) exp{ -
-1 tr1:- 1[A + (0 - 0)' X' X(O - e)] }, 
then, 
(a) given 1:, e~Nm k [e,1: ® (X'X) - lJ , 
(b) 
1: ~ W,~I(A ,v) and 
(c) 
e ~ t"'k[e, (X'X) - I, A, v]. 
8.5.2 Some Properties of the Distribution of 1: 
Consider the partition 
(8. 5.8) 
We now proceed to obtain the posterior distribution of (1: 11 ,n, T), where 
and 
(8.5.9) 
It is to be remembered that 1: 11 is the covariance matrix of the m 1 errors 
(eu1 , .. . , ellm ,), nand T are, respectively, the covariance matrix and the 
m 1 x m 2 matrix of "regression coefficients" for the conditional distribution of 
the remaining m2 errors (eU("'j + 1)' ... , ell"') given' (elll , •.. , eumJ Denoting 
A = [1~: \ 1::'] ::::, 
and 
it can be readily shown that 
a) 1:11 is distributed independently of (T,n), 
b) 
1:ll~ W,~/(All 'V), 
c) 
d) 
-
-I 
T ~ tlnlm2 (T, All ' A22 . I , V + In l ) , 
U(T) = 11m2 + AZ/l (T - TrAil (T - 1')1- 1 
(8.5. 10) 
(8.5.11) 
(8.5. 12) 
(8.5.13) 
(8.5.14) 

462 
Some Aspects of Multivariate Analysis 
8.5 
The above results may be verified as follows, 
Since 1: IS positive definite, we can 
express the determinant and the inverse of 1: as 
11:1 = 11:111 lUI 
1:-
1 = 
[l:~( ~ J + M, 
(8.5. is) 
where 
M = l ~~~II~,
~L~ - Il 
Expression (8.5.15) may be verified by showing 1: -I E = 1. Thus, the distribution in 
(8,5.4) can be written 
E > O. 
(8.5,16) 
For fixed 1:11 , it is readily seen by making use of (A8.1.l) in Appendix A8,] that the 
Jacobian of the transformation from (1:12,1:22 ) to (T, Q) is 
J = I 0(1: 12 , 1:d I = '1: 
1m2. 
: o(T,U) 
, 
I II 
(8.5.17) 
Noting that M does not depend on 1:11 , it follows from (8.5.16) and (8.5.17) that 1:11 
is independent of (T,U) so that 
P(1:II , T,Uly) =p(T,Qly)p(1:IIIY) 
where 
p(T,Q I y) :;c IQI - (·, v+m) exp (-'~ tr MA), 
and 
U > 0, 
-
00 < T < 00 
Thus, El1 is distributed as the inverted Wishart given in (8.5.11). 
(8.5.18) 
(8.5,19) 
(8.5.20) 
From (8.5,10) and (8.5.15), we may express the exponent of the distribution in 
(8.5.19) as 
IrMA = tr(TQ-IT'A II -
TU- IA2 1 -Q-- I TA I2 +Q- 1A 22) 
= trQ-I [AnI + (T - l')'All (T - Tn 
(8,5.21) 
In obtaining the extreme right of (8.5.21), repeated use is made of the fact that 
tr AB = trBA. 
Thus, 
p(T,n I y) ex IUI- }(v' +III , 
2 m2) exp {- i tr n- I [Ani + (T - T)'AII (T - i')]), 
-
00 < T < 00, 
U > 0, 
where v' = II + /111' which is in exactly the same form as the joint distribution of 
(1:, 0) in (8.5.1), The results in (8.5. J 2) and (8,5.13) follow by appl ication of Theorem 
8.5.1 (on p. 461). Further, by using Theorem 8.4,[ (on p. 449), we obtain (8.5.14), 

8.5 
Some Aspects of the Distribution of E 
463 
Distribution 0/ (J 11 
By setting m 1 = 1 in (8.5.11), the distribution of (J11 is 
P((Jll I y) oc (Jl't~vc'l) exp ( -
;~:1 ), 
(8.5.22) 
a x- 2 distribution with v = 11 -
k -
(rn -
1) degrees of freedom. 
Comparing 
with the univariate case in (8.5.7), we see that the two distributions are identical 
except, as expected, that the degrees of freedom differ by (m -
1). The difference 
is of minor importance when m is small relative to n -
k but can be appreciable 
otherwise-see the discussion about the posterior distribution of ei in (8.4.28). 
It is clear that the distribution of (Jii' the ith diagonal element of :E, is given 
by simply replacing the subscripts (1,1) in (8.5.22) by (i, i). 
The Two Regression Matrices :E;11:E12 and :E221 :E21 
The m 1 x m2 matrix of "regression coefficients" T = :E;/:E12 measures the 
dependence of the conditional expectation of the errors (81/(111,+1)' ... ,81/"') on 
(e Il1 , ... , 811mJ 
From the posterior distribution of T in (8.5.13) the plausibility 
of different values of the measure T may be compared in terms of e.g. the 
H.P.D. regions. 
In deciding whether a parameter point T = To lies inside or 
outside a desired H.P.D. region, from (8.5.14) and (8.4.66) one may then 
calculate 
Pr {VeT) > VeTo) I y} == Pr {XI~""'2 < -
cPt (v + mJ log VeTo)}, 
(8.5.23) 
where 
m 1 + Jn2 -
3 
cPt = I + 
. 
2(v+m 1) 
In particular, if To = 0 which corresponds to :E12 = 0, that IS, 
(8u1 , ... , 8um ,) 
are independent of (81/(111,+1)' ... , 8um), then 
Consider now the m 2 x m 1 matrix of "regression coefficients" 
Z = :E221 :E21 
(8.5.24) 
(8.5.25) 
It is clear from the development leading to (8.5.13) and (8.5.14) that by 
interchanging the roles of m 1 and m2' we have 
(8.5.26) 
where 
and 
(8.5.27) 

464 
Some A;pects of Multivariate Analysis 
:;,5 
T hus, in deciding whether the parameter point LO is included in a desired 
H.P.D. region, we calculate 
where 
m l -+- 1112 -
3 
cp"= 1 +-----
2(v + m 2 ) 
In particular, if Zo = 0 which again corresponds to 1:12 = 0, then 
U(Zo = 0) = 
IAI 
IAllllAd 
(8.5.28) 
(8.5.29) 
Although U(Zo = 0) = ["(To = 0) 
the probabilities on the right-hand 
sides of (8.5.23) and (8.5.28) will be different whenever Inl =I- m2 ' This is not a 
surprising result. For, it can be seen from (8.5.18) that the distribution of T is 
in fact proportional to the conditional distribution of 1:12 , 
given 
1;11' 
Inferences about the parameter point 1:12 = 0 in terms of the probabililY 
Pr {X.;' ,1II2 < - cp' (v + ml) log U(To = O)} can thus be interpreted as made 
conditionally for fixed 1: 11 , That is to say that we are comparing the plausibility 
of 1: 12 = 0 with other values of 1: 12 in relation to a fixed 1: I I' On the other hand, 
in terms of Pr {X.;""" < - cpu (v + 1112) log U(Zo = O)}, inferences about 1:12 = 0 
can be regarded as conditional on fixed 1:22 , Thus, one would certainly not expect 
that, in general, the two types of conditional inferences about 1:12 = 0 will be 
identical. 
8.5.3 An Example 
for illustration, consider again the product and by-product data in Table 8.4.1. 
The relevant sample quantities are given in (8.4.73). 
When interest centers on the variance 0" 11 of the error 6uI corresponding 
to the product Yl' we have irom (8.5.22) 
(8.5.30) 
Using Table II (at the end of this book), limits of the 95 per cent H.P.D. interval 
of 10gO"ll in terms of O"ll are (3.02, 20.79). 
Similarly, the posterior distribution of 0"2l is such that 
(8.5.31) 
and the limits of the corresponding 95 per cent H .P. D. interval are (1.81, 12.44). 
From (8.5.13), and since 
1111 = /11 2 = 1, the posterior distribution of 
T= 0"1- 110"12 is the univariate t(l,st, VI) distribution where 
T= a~/aI 2 = -
0.627, 

8.5 
Some Aspects of the Distribution of 1: 
465 
and 
Thus, 
T + 0.627 
(8.5.32) 
--~-
~ t lO 
0.143 
so that from Table IV at the end of this book, limits of the 95 per cent H.P. D. 
interval are (- 0.95, - 0.31). 
In particular, the parameter point 0"~/0"12 = 0 
(corresponds to 0"12 = 0) is excluded from the 95 per cent interval. 
Finally, from (8.5.26) Z = 0"2210"]2 is distributed as t(Z,s~, v1 ) where 
and 
Thus, 
all - ai2/a22 
V2 x a22 
Z + 1.05 
0.240 
~ tlO' 
= 0.0574. 
(8.5.33) 
Limits of the 95 per cent H.P.D. interval are (- 1.58, - 0.52) and the point 
0" 22' 0" 12 = 0 is again excluded. 
Further, from (8.5.14), (8.5.27), (8.5.32) and 
(8.5.33) 
Pr {U(T) > [;(0) I y} = Pr {U(Z) > U(O) I y} = Pr {I/IOI > 4.37} 
(8.5.34) 
so that inferences about 0"12 = 0 in terms of either T or Z are identical. This 
is of course to be expected since, for this example, m 1 = m2 = l. 
8.5.4 Distribution of the Correlation Coefficient P 12 
The two regression matrices ~~/~1 2 and ~22'~21 are measures of the dependence 
(or association) between 
the 
two 
set 
of responses 
(YIII ' ... , Yum.) 
and 
(Yu(m l + 1)' ... , Yum)· 
When interest centers at the association between two 
specific responses Yui and Yllj, the most natural measure of association is the 
correlation coefficient P ij = O"i)(O"iiO"jY ; 2. 
Without loss of generality, we now 
consider how inferences may be made about P 12' 
By setting m 1 = 2 in the distribution of ~II in (8.5.11), we can follow the 
development in Jeffreys (1961 , p. 174) to obtain the posterior distribution of the 
correlation coefficient Pl2 as 
- 1< P< 1, 
(8.5.35) 

466 
Some Aspects of Multivariate Analysis 
8.5 
where P = P12, 
r=rI2=( 
)1 / 2 
all a22 
is the sample correlation coefficient, and the normalizing constant is 
It is noted that this distribution depends only upon the sample correlation 
coefficient r. 
To see this, for Inl = 2, the posterior distribution of the elements (0'11,0'22,0'12) of 
l:J1 in (8.5.11) is 
P(0'Il,0'22,0'I21 Y) cc [0'110'22 (I -
p2)r (~ \ + 2) 
where from (8.5.6), the normalizing constant is, 
(a11a22 - ai2)(v+1)/2! {2(v+I) nl/2lJ r[}(v + 2 - i)J} 
We now make the transformation, due to Fisher (1915), 
The Jacobian is 
so that the distribution of (x, OJ, p) is 
(8.5.36) 
(8.5.37) 
(8.5.38) 
P(X,OJ,PIY) CC(1 _p2)'!,(V+4)OJ- 1 X- (V+2) exp [ -
(OJ + OJI -2pr)], 
2(1 -
p2)x 
OJ > 0, 
x > 0, 
-
1 < P < 1. 
(8.5.39) 
Upon integrating out x, 
( 
1 
)-(V"I) 
p(OJ,p l y) cc (1_p2) ~(v-2)OJ-I OJ + -; - 2pr 
OJ> 0, 
-
1 < P < I, 
(8.5.40) 
from which we obtain the distribution of P given in (8.5.35). 

8.5 
Some Aspects of the Distribution of E 
467 
The Special Case When t' = 0 
When r = 0, the distribution in (8.5.35) reduces to 
- 1 < p < 1, 
(8.5.41) 
which is symmetric at p = 0, and is identical in form to the sampling 
distribution of r on the null hypothesis that p = O. 
In this case, if we make 
the transformation 
then the distribution of t is 
so that the quantity t is distributed as teO, 1, v). 
The General Case When r #- 0 
(8.5.42) 
-
Cf) < t < co, 
In general, the density function (8.5.35) cannot be expressed in terms of simple 
functions of r. 
With the availability of a computer, it can always be evaluated 
by numerical integration, however. 
To illustrate, consider again the bivariate 
product and by-product data introduced in Table 8.4.1. 
Figure 8.5.1 shows 
the posterior distribution of p calculated from (8.5.35). 
For this example 
v = 9. and r = - 0.81. The distribution is skewed to the right and concentrated 
rather sharply about its mode at p == - 0.87; it practically rules out values of 
p exceeding - 0.3. 
pep I y) 
4.0 
3.0 
2.0 
1.0 
L-__ 
--L ___ 
L-__ ---1...-==='"--__ 
---L. ___ 
...l...- P -, 
- 1.0 
- 0.8 
- 0.6 
- OA 
- 0." 
0.0 
0.2 
Fig. 8.5.1 Posterior distributions of p for the product and by-product data. 

468 
Some Aspects of Multivariate Analysis 
8.S 
Series Expansions of pep I Y) 
The distribution in (8.5.35) can be put in various different forms. In particular, 
it can be expressed as 
-l<p<l , 
(8.5.43) 
where 
a.. 
I ( I + pr)' I 
(2s -
1)2 
Sv(p,r)=I+I-
TI 
I 
1=1f! 
8
,
S= I(V+S+i) 
is a hypergeometric series, and the normalizing constant is 
To see this, the integral in (8.5.35) can be written 
J
OO 
[ 
I 
]- (V +I ) 
fro 
[ 
1 
J-(V+1) 
OW-I w+o;-2 pr 
dw=2 
IW-I w + w -2pr 
dw,(8.5.44) 
On the right-hand side of (8.5.44), we may make the substitution, again due to Fisher, 
Noting that 
we have 
so that 
1 - PI' 
U = 1 -
-
-
- "----
Hw + (l/w)] - pr 
Hw + ( 1Iw)] -
1 
Hw + (l lw)] -
PI" 
~ = w- I (l -
u) (2u)12 [1 -HI + pr)u]I /2 (l -
pr)-1,2 
ow 
(8 5.45) 
(8.5.46) 
(l -
p2):c(v-Z) Jl (l -
u)" 
pep I y) oc.1. 
--1/2 [1 - }(l + pr)ur l /2 du, 
(1 - pr)"' , 
0 (2u) 
-1<p<J. 
(8.5.47) 
Expanding the last term in the integrand in powers of u and integrating term by term, 
each term being a complete beta function, we obtain the result in (8.5.43). 
~---~-
II 

8.S 
Some Aspects of the Distribution of 1: 
469 
We remark here that an alternative series representation of the distribution of p 
can be obtained as follows . In the integral of (8.5.35), since 
(8.5.48) 
by completing the square in the second factor on the right-hand side of (8.5.48), we can 
write the distribution in (8.5.35) as 
(1 - p2XHv - 2) I'" 
v [ 
(w - p/'/] -(Y + I) 
p(ply)a:: 
22(y + l) 
W 
1+ 
22 
dw 
(I-pr) 
1 
I-pr 
(8.5.49) 
Upon repeated integration by parts, the above integral can be expressed as a finite 
series involving powers of [(I -
pI') / (I + pr)]1/2 and Student's I integrals. 
The 
density function of p can thus be calculated from a table of I distribution. 
This 
process becomes very tedious when v is moderately large so its practical usefulness 
is limited. 
The series Sy(p, r) in (8.5.43) has its leading term equal to one, followed by 
terms of order v- I, 1= 1,2, .... 
When v is moderately large, we may simply 
take the first term so that approximately 
(I _ pZ)"HV-Z) 
p(ply)=c 
(I-prr++ ' 
-
I < p < 1, 
(8.5.50) 
where c is the normalizing constanL 
I
I (1 -
2)-Hv - 2) 
-1 -
p. d 
c 
-
+ c 
p. 
-1 (J - prr ' 
Although evaluation of c would still require the use of numerical methods, it is 
much simpler to calculate the distri bution of p using (8.5.50) than to evaluate 
the integral in (8.5.35) for every value of p. 
Table 8.5.1 compares the exact 
distribution with the approximation using the data in Table 8.4.1. 
In spite of 
the fact that v is only 9, the agreement is very close. 
It is easily seen that the density function (8.5.50) is greatest when p is near r. 
However, except when r = 0, the distribution is asymmetrical. 
The asymmetry can be reduced by making the transformation, 
l+p 
( = tanh- I p = ·llog---, 
J-p 
(8.5.51) 
due to Fisher (1921). Following his argument, it is found that ( is approximately 
Normal 
N[tanh-Ir-
51' 
),(v+ I)-II. 
2(v + I 

470 
Some Aspects of Multivariate Analysis 
8.6 
Setting m = 2, k = I so that v = n -
2, the distribution in (8.5.43) is 
identical to that given by Jeffreys for the case of sampling from a bivariate 
Normal population. 
Finally, we note that while we have obtained above the 
distribution of the specific correlation coefficient P = P12, it is clear that the 
distribution of any correlation Pij ' i i= j, is given simply by setting r = rij = 
aij/(aii ajY· 2 in (8.5.35) and its associated expressions. 
Table 8.5.1 
Comparison of the exact and the approximate distributions of P for II = 9 and r = -0.81 
p(p I y) 
P 
Exact 
Approximate 
-0.98 
0.2286 
0.2283 
-0.96 
1.2164 
1.2150 
-0.94 
2.4867 
2.4844 
-0.92 
3.5159 
3.5134 
-0.90 
4.1200 
4.1180 
-0.88 
4.3281 
4.3271 
-0.86 
4.2448 
4.2448 
-0.84 
3.9782 
3.9790 
-0.82 
3.6141 
3.6157 
-0.80 
3.2127 
3.2149 
-0.70 
1.5182 
1.5210 
-0.60 
0.6584 
0.6603 
-0.50 
0.2853 
0.2865 
-0.40 
0.1260 
0.1267 
-0.30 
0.0569 
0.0572 
-0.20 
0.0261 
0.0263 
8.6 A SUMMARY OF FORMULAE AND CALCL'LATIONS FOR \I1AKI"JG 
INFERENCES ABOUT (8,~) 
Using the product, by-product data in Table 8.4.1 for illustration. Table 8.6.1 below 
provides a short summary of the formulae and calculations required for making 
inferences about the elements of (8, ~) for the linear model with common derivative 
matrix defined in (8.4.1). Specifically, the model is 
y = xe + £ 
(8.6.1 ) 
where y = [y I, ... , Yrn] is a n x m matrix of observations, X is a n x k matrix of fixed 
elements with rank k, 8 = [81 , ... , 8",J is a k x 111 matrix of parameters and 
£ = [E(I)' ... , £(1/)]' is a n x 111 matrix of errors. It is assumed that feu), u = 1, .... n 
are independently distributed as NI/ICO, ~) . 

8.6 
A Summary of Formulae and Calculations for Making Inferences about (9, I:) 
471 
Table 8.6.1 
Summarized calculations for the linear model y = X6 + £ 
1. From (8.4.1), (8.4.4), (8.4.7) and (8.4.13), obtain 
m=2, 
k=2, 
n= 12, 
v=n-(m+k)+1 =9 
[
12 
0] 
X'X -- ° 0.15 ' 
[
0.08 
0] 
C = (X'X)-l = ° 6.88 
, 
j 
[71.14 
18.07] . 
6 = (X'X)- X'y = 
54.44 
- 20.09 
A={aij} i,j=I, .. . ,117 
aij = (Yi - xey (y j - X9), 
and 
A = [ 
61.41 
- 38.48] . 
-38.48 
36.74 
2. Inferences about a specific column or row of 6: 
Writing 
6 = [6j, ".'6m]=[~:j) l' 
6(k) 
then from (8.4.28) and (8.4.34), 
i=l, ... ,117, 
and 
g=l, ... ,k. 
Thus, for i= 1 and g =2 
61 ~ '2 
, 6.83 x 
, 9 
{ ( 71.14) 
[0.08 
0] 
} 
54.44 
° 6.88 
{ (
54.44) 
[ 
61.41 
- 38.48] 
} 
6(2) ~ 12 
20.09' 0.76 x 
_ 38.48 
36.74
' 9 . 
3. H.P.D. regions of 6: To decide if a general parameter point 60 lies inside or 
outside the (l -a) H.P.D. region, from (8.4.63) and (8.4.66), use the approximation 
- ¢v log U N 
X~k' 
where 
U= U(6) = IAIIA+(6-8)' X'X(6-9)1- 1 
and 
so that 60 lies inside the 1 -C( region if 
- ¢v log U(6o) < X2(l17k, C(). 
m+k-3 
¢=1 + -2-v-' 

472 
Some Aspects of Multivariate ~ nalysis 
Table 8.6.1 Continued 
eor the example, 1; = 19/18. Thus, if 
[
70 
60 = 
65 
171 
-30 ' 
then 
and the point lies inside the 1 -a region if 
-9.5JogO.17 = 16.7 < X2 (4,::x). 
4. I-I.P.D. regions for a block submatrix of B : Let 
nil 
'"2 
m, 
nl2 
6= [ 
611 t:·J :: 
9= [ 
(j II 
9 12 
621 
021 : 822 
Inr 
"'2 
k, 
k, 
A= [ 
All 
A12 J 
m, 
C = l' 
C II 
CJ2 
A21 
A22 
ml 
C21 
C22 
8.5 
l 
h, 
k2 
J k, 
h2 
To decide, for example, if the parameter point 611.0 lies inside or outside the 
(l-u.) H.P.D. region for °1 l' use the approximation in (8.4.68), 
1;lvlog U(ell),.:., X,;"k" 
where 
U(6JJ ) = IAIIll All +(611-911)'C;/(6ll-011)1-1 
and 1;J =1 +(1 /2v)(ml +k J -3), so that 611 ,0 lies inside the (1-a) region if 
-¢IV log U(6 J1 ,0) < X2 (in l k l ,u.), 
Thusifm l =k1 =1, 811,0=70, then 1;1=17/ 18 and 
'[ 
(70-71.14)2J 
U (8 11 0 =70) = 61.41 / 
61.41 + 
= 0.79, 
, 
0.08 
so that 811 .0 lies inside the (l-u.) region if 
- 8.510g 0.79 = 2.0 < X2(l, u.). 
Note that since in1 = k I = 1, exact results could be obtained and the above is for 
illustration only. 
5. Inferences about the diagonal elements of 1: : 
From (8.5.22) 
(Jii ~ aiix;2, 
Thus, for i= 1, 
U II ~61.41X92 
i=l, ... ,m. 
6. H.P.D. regions for the "regression matrix" T: Let 

AS.l 
Appendix 
473 
Table S.6.1 Continued 
To decide jf a parameter point To lies inside or outside the (1 -IX) H.P.D. 
region, from (8.5.23), use the approximation 
where 
and 
-¢'(v+m,)log Ucr),.v X;,m, 
m, +1172-3 
¢' = 1 + ----
2(v+m 1) 
U(T) = IA22-11IA221 +(1'-'1')' Al1 (T-i,)I-' 
so that To lies inside the region if 
-¢'(v+m,)log U(To) < x2(m,1172'IX). 
Thus, for m, =m2 = 1, ¢'(v +111,)=9.5, :"22., =022.1 = 12.63 and f=0.63. 
If To = 0, then 
U(To =0) = 12.63'[12.63 +(0.63)2 x 61.4IJ =0.34 
and the point To =0 will lie inside the (i-a) region if 
-9.5 JogO.34 = 10.3 < x2(1,a). 
Note that (i) L'(O)= IAI{IA II IIA 2 2ir " and (ii) since I11 J =1112=1, exact results 
are, of course, available. 
7. Inferences about the correlation coefficient Pij: Use the approximating distri-
bution in (8.5.50), 
-I <p<l, 
where 
i,j=l, ... ,n!. 
Thus, 
(1_ p2)3.5 
pep 1 y) eX:: 
-I <p < I. 
(1 +0.81p)9.5 
' 
The normalizing constant of the distribution can be obtained by numerical 
integration when desired. 
APPENDIX AS.l 
The Jacobians of Some Matrix Transformations 
We here give the lacobians of some matrix transformations useful 
in 
multivariate problems. In what follows the signs of the lacobians are ignored. 
a) Let X be a k x m matrix of km distinct random variables. 
Let A and B 
be, respectively, a k x Ie and m x m matrices of fixed elements. If 
Z3 = AXB, 

474 
Some Aspects of Multivariate Analysis 
AS;1 
then the Jacobians are, respectively, 
I :~21 = IBI\ 
I 
OZ3 I 
m 
k 
eX 
= IAI IBI· 
(A8.1.1) 
b) Let X be a m x m symmetric matrix consisting of~m(m + I) distinct 
elements and let C be a m x m matrix of fixed elements. If 
y=cxc, 
then 
(A8.1.2) 
For proofs, see Deemer and Olkin (1951), based on results given by 
P. L. Hsu, also Anderson (1958, p. 162). 
c) Let X be a m x m nonsingular matrix of random variables and Y = X -1. 
Then, 
8Y 
oz 
2X 
-y-y 
OZ 
' 
where z is any function of the elements of X. 
Proof: Since X Y = I, it follows that 
Hence 
o 
(ax) 
oY 
-
(X Y) = -
Y + X -
= o. 
oz 
oz 
oz 
oY 
oz 
The lacobians of tw.o special cases of X are of particular interest. 
a) If X has m 2 distinct random varia bles, then from (A8.1.1) 
(A8.1.3) 
(A8.IA) 
b) If X is symmetric, and consists oflm(m + I) distinct random variables, 
then from (A8.1.2) 
l
aY 1= IYl
m + 1 
ax 
. 
(A8.1.5) 

AS.2 
Appendix 
475 
APPENDIX AS.2 
The Determinant of the Information :\1atrix of ~ - 1 
We now obtain the determinant of the information matrix of ~
' I for the 
m-dimensional Normal distribution N m(/1, ~) . 
The density is 
p(y I /l, ~) = (2n) -m/ 2 I~ -11 12 exp [ - t tr...:J (y -
/l) (y - /1)'] , 
-
00 < y < 00 , 
(A8.2.1) 
where Y = (Yl , ···,Ym)', /l = (J.l.I, .. . , J.l./tI)', ~ = {()iJ and ~-I = {()iJ}, i,j = 1, ... , m. 
We assume that ~ (and ~-I) consists of ~'m(m + 1) distinct elements. Taking 
logarithms of the density function , we have 
m 
Jog p = - -
log (2rr) + i· log I~ - II - 1 tr ~ -I (y - /l) (y - /l)'. 
2 
. 
(A8.2.2) 
Differentiating log p with respect to ()ij, i, j = J, ... , m, i ~ j, 
(AS.2.3) 
Since dl~-ll/d()iJ = (iij where (iij is the cofactor of ()ij, it follows that the first 
term on the right-hand side of (A8.2.3) is simply }()jj. Thus, the second derivatives 
are 
a2 10gp 
d()ij d()k I 
(
i,j = I, ... ,m; 
k,!=i , .. . ,m; 
i ~ j) 
k ~ I . 
(AS.2.4) 
Consequently, the determinant of the information matrix is proportional to 
_ I 
I 
{ 8
2 
Jog P } I I 8~ I 
IJ(~ )1 = 
-
E 
O()ij (; ()kl 
IX 
O~-l . 
From (A8.l.S), we have that 
1 ~1=
' ~lm ' l. 
8~-1 
I 
APPENDIX AS.3 
The Normalizing Constant of the I km[9, (X'X)' 1, A, v] Distribution 
Let 6 be a k x m matrix of variables. We now show that 
(A8.2.S) 
(A8.2.6) 
J 
11m + A -l (6 - 9)' X'X(9 - 9W ~ (v+k+m-l)d9 
(A8.3.1) 
-00 < 9 < <<> 

476 
Some Aspects of :\1uIiivariate Analysis 
A8.3 
where v > 0, 8 is a k x m matrix, X'X and A -1 are, respectively, a k x k and a 
m x m positive definite symmetric matrix, 
c(m, k, v) = [rcm mk 
r",[-t(v + m -
I)J 
rmU-(v + Ie + m -
I)J 
and r pCb) is the generalized Gamma function defined in (S.2.22). 
Since X'X and A are assumed definite, there exist a k x k and a m x m 
nonsingular matrices G and H such that 
X'X = G'G 
and 
A-I = HH'. 
(AS.3.2) 
Let T be a k x m matrix such that 
T = G(8 - 8)H. 
(AS.3.3) 
Using the identity (S.4.l1) we can write 
II", + A -1(8 - 9) ' X'X(8 -
9)1 = 11m + H'(S - 9) ' G'G(S - 8)HI = 11m + T'TI 
= Ilk + TT'I· 
From (A8.! l) the Jacobian of the transformation (A8.3 .3) is 
I ~: 1= IGlm IHlk = IX' Xl mI2IA -1IkI2. 
Thus, the integral on the left-hand side of (A8.3.!) is 
IX' XI- mI2IA -1 1- k/ 2 Qm, 
where 
Q = J 
II + TT'I- +(v+k+m-l) dT 
In 
k 
. 
-00 < T < GO 
(AS.3.4) 
(AS.3.S) 
(AS.3.6) 
(A8.3.7) 
LetT= [t 1, .. . , tIl1J = [Tl,tmJ where tj is a k x l vector, i= 1, .. . ,m. Then, 
Ilk + TT'l = Ilk + TITl + tmt;,,1 
= Ilk + TjT'tl [1 + (,,(lk + TjT't)-ltm]. 
(AS.3.8) 
It follows that 
Q =J 
II +TT'
I - ~'(I'+k+m-l)qdT 
m 
k 
11m l' 
-00 <T J < 00 
(AS.3.9) 
where 
q = r 
[1 _L l' (I + T T')-lt J- -:(v+m-l+k)dt 
nr 
I 
m 
k 
11m 
m' 
.. -oo < tnt<co 
From (A2.1.l2) in Appendix A2.! , 
_ r _t 
k 
r[-}(v + til -
1)J 
qm -
[ (2)J r[-Hv + k + m _ I)J Ilk + TlT'11 1/2. 

AB.4 
Appendix 
477 
Thus, 
_ r J_ k 
r[-l(v + m -
1)J 
Qm -
[ Cz)J ru(v + k + m _ I)J Qm-I' 
where 
Q 
= f 
II + T T 1--:,(v+m-2+k) dT 
m- 1 
k 
1 
1 
1-
-r.:i:> <T j <co 
The result in (AS.3.]) follows by repeating the process m -
1 times. 
APPENDIX A8.4 
The Kronecker Product of Two Matrices 
We summarize in this appendix some properties of the Kronecker product of 
two matrices. 
Definition: If A is a m x m matrix and B is a n x n matrix, then the Kronecker 
product of A and B in that order is the (mn) x (Inn) matrix 
Properties: 
i) (A (8) BY = A' (8) B' 
ii) If A and B are symmetric, then A(8)B is symmetric. 
iii) When A and B are non-singular, then 
(A (8) B)-l = A - I (8)B- J 
iv) tr A (8) B = tr A tr B 
vi) If C is a m x m matrix and D is a 11 x n matrix, then 
(A + C) (8) B = A (8) B + C (8) B 
and 
(A (8) B) (C (8) D) = (AC) (8) (BD). 

CHAPTER 9 
ESTIMATION OF COMMON REGRESSION 
COEFFICIENTS 
9.1 INTRODUCTION: PRACTICAL IMPORTANCE OF COMBINING 
INFORMATION FROM DIFFERENT SOURCES 
Problems occur, notably in engineering, economics, and business, where Jt IS 
difficult to obtain sufficiently reliable estimates of critical parameters. One way 
to improve estimates is to seek more data of the same kind . But in engineering 
this may mean running further costly experiments while in economics and business 
further data from the same source may not be available. Another way in which 
estimates can sometimes be improved is by combining information from different 
kinds of data. Thus, in Section 8.2.6 we considered a chemical system producing 
three different products where the yield of each product depended on one or both 
of two chemical rate parameters. 
I t was demonstrated that, by appropriately 
utilizing observations on all three products, much more precise inferences could 
be drawn than would have been possible with only one. 
As a further illustration, in a study of the effect of interest rates on the demand 
for durables, an economist might wish to appropriately combine demand data 
from In different communities. Let Yj = ( Y lb ·.· ,y"Y be an JJ x 1 vector, represent-
ing demand over 17 specific periods, for the ith community. Suppose that 
Yj = E(y;) + Sj, 
(9.1.1) 
i = 1, ... , n1, 
where e; is a parameter for the ith community, measuring the effect on demand 
of a change in the interest rate, !;; is the corresponding n x 1 vector of interest 
rates, and Sj = (eli' ... , enY is the associated n x I vector of errors. 
To complete the model we must make specific assumptions about (i) the 
relationship among (e l , . . . , 8m) , (ii) the nature of the expectation functions 
T)j(!;j, 8j) and (iii) the form of the probability distribution of Sj . We discuss these 
assumptions below: 
(i) If there is reason to believe that the parameters (8 l> ... , 8m) for the In different 
communities resemble one another, then, irrespective of whether or not there are 
correlations among the In vectors of errors, (s" ... , sm), information about 8j 
from the ith response vector Yi ought to contribute to knowledge about ej . A for-
mulation which takes account of one kind of resemblence, supposes that 8" ... , 8m 
478 

9.1 
Introduction 
479 
are m independent observations from some population distributed as f(8). 
In 
particular, the random effect model discussed in Section 7.2 was of this kind. Jf 
the variance off CO) was expected to be small , the m vectors of expectation functions 
TJi might then be regarded as containing a common parameter OJ = ... = em = ee 
and data from all m communities could be used to make inferences about 8e . 
We have already considered a multivariate setup of this kind in Section 8.2.6. It 
was there possible to show that, even in the general situation where the expectation 
functions were nonlinear in the parameters, exact results could be obtained 
yielding the posterior distribution of the common parameters in a form which 
was mathematically simple. 
ii) Mathematical simplicity does not invariably lead to tractibJe computation 
however, and we now consider the problem with the added simplification that the 
expectation functions are linear in the parameter ee. 
As usual, consideration of 
the more manageable linear case has more than one motivation. On the one hand, 
practical problems do occur where the appropriate expectation functions are 
indeed linear in the parameters. On the other hand, for nonlinear problems, we 
rarely need to concern ourselves with nonlinearity over the whole permissible 
region of variation of 8e> but only in some smaller sub-region of the parameter 
space-for example, over a (l -
Ct.) H.P.D. region for some small ':I. . A function 
which is "globally" highly nonlinear in 8 can be, in this sense, "locally" nearly 
linear. In sllch a case, as is indicated in Section 8.3. I, linear distribution theory 
can provide useful approximations. 
In the Bayesian formulation, we are con-
cerned only with approximate linearity in relation to the one sample of 
observations which has in fact occurred and not with some hypothetical "repeated 
samples from the same population" which have not. It follows that we can, for 
any given data sample, check the adequacy of the assumption of local linearity 
by direct numerical computation as part of the statistical analysis. 
iii) We shall suppose that the 11 vectors [(II) = (elll , ... , E"",) , U = 1, .. . , n, are a 
sample of independent observations from the m-dimensional Normal population 
N",C9, E), where 1: = {O";J. 
In general, the relationships connecting demand with interest rate will contain, 
not one, but k common parameters ge. Thus, with the above assumptions we are 
led to a linear, Normal, common parameter model, which can be written 
i = I , ... ,m 
(9.1.2) 
with Xi an n x k matrix appropriate to the ith community and 9c a k x 1 vector 
of parameters common to all communities. 
This model (9.1.2) will be recognized as a special case of the general linear 
model set out in C8.3.2) 
i = I, ... ,m 
(9.1.3) 

480 
Estimation of Common Regression Coefficients 
9.1 
appropriate to the circumstance ~hat the regression coefficients e I, ... , 9m associated 
with the m vectors of responses are common, so that 
(9.1.4) 
In what follows we shall first discuss the situation, where the m response 
vectors YI, "., Ym are supposed independent of one another so that 
~ is 
diagonal. The posterior distribution of 9c is then a product of m multivariate 
t distributions. 
A useful and simple approximation procedure is developed 
for making inferences about subsets of 9c for the case of two responses. 
The 
more general situation is next considered where the m responses are correlated. 
Except when the derivative matrices X I , ... , Xm are identical, the resulting posterior 
distribution of 9c is rather complicated analytically. 
A useful approximation 
procedure will, however, be developed for m = 2. 
As we have pointed out in Section 8.2.6, care must be exercised in the pooling 
of information to estimate common parameters.. A natural question that arises 
in the investigator's mind will be whether the evidence from the data are compatible 
with the supposition that 9 1 = ". = em = 9c . In the following section , this question 
is considered for the important special case of estimating the assumed common 
mean of two Normal populations which have possibly unequal variances- a 
problem which has an interesting history. 
9.2 THE CASE OF m INDEPENDENT RESPONSES 
Suppose there are m independent responses related to the same parameters 
ec = (e l , " ., ek )' by possibly different linear models. 
Suppose further that the 
number of observations for each response is not necessarily the same. Then the 
likelihood function is 
(9.2.1) 
where Yi is an ni x 1 vector of observations and Xi a nj Y. k matrix of fixed 
elements. Relative to the noninformative reference prior 
m 
p(ec , all' . '" (Jrnm) cc naill, 
i= 1 
the posterior distribution is 
(9.2.2) 

9.2 
The Case of m Independent Responses 
481 
where 
and 
Integrating out 0" II ' .. . , O"mm, the posterior distribution of the common parameters 
ge is then proportional to the product of m multivariate t distributions 
m 
p(9c I y) c.c TI [au + (ge -
6;)'X;X;(9;-6Jr :rn" 
-
00 < ge < 00 . 
(9.2.3) 
i= 1 
Properties of this distribution have been considered by Dickey (1967a, 1968) and 
others. 
9.2.1 The Weighted Mean Problem 
The simplest problem of this kind has come to be called the problem of the 
weighted mean, Yates (1939). Suppose that m = 2, k = I and (XI = 111 " X2 = In ,) 
are columns of ones. Typically we have two sets of Normally distributed obser-
vations Yl = (YII, .. ·,Yn,IY and Y2 = (YI2, .·· , Yn ,2Y independent of one another 
and having different variances O"~ = 0" 11 and O"~ = 0"22 but both assumed to have 
the same mean e. 
The posterior distribution of e is then proportional to the 
product of two t distributions 
-
00 < e < 00, 
(9.2.4 ) 
where 
V; = n; -
1, 
i = 1, 2, 
a distribution given by Jeffreys (1961). Simultaneously Fisher (1961 a, b) obtained 
an identical distribution using a fiducial argument. Except for the normalizing 
constant, the density (9.2.4) can be easily calculated from a table of the density 
function of Student's t---e.g. Bracken and Schleifer (J 964). 
An Example 
The following results were obtained In estimating a physical constant by two 
independent methods 
n 
Method 1 
107.9 
12.1 
11 
Method 2 
109.5 
39.4 
13 

f 
482 
Estimation of Common Regression Coefficients 
9.2 
0.4 
0.3 
0.2 
0. 1 
~-----L----~----~------~----~-----L---------e~ 
106 
108 
110 
Fig.9.2.1 Individual posterior distributions of e: estimation of a common physical 
constant. 
0.4 
106 
pCB I y) 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
108 \ 
110 
\ 
\ 
\ 
\ /\ 
(alae) log p(O I y) 
\ 
\ 
\ 
\ 
\ , 
\ 
\ 
\ '. 
Fig. 9.2.2 Combined posterior distribution of e: estimation of a common physical 
constant. 

9.2 
The Case of m Independent Responses 
483 
If el and e2 are the means for the two methods, then crucial to what follows 
is the assumption that el = e2 = e. We shall entertain this assumption for the 
moment and later discuss it further in Section 9.2.3. With this assumption, then, 
the two I distributions shown in Fig. 9.2.1 centered at 107.9 and 109.5 with scale 
factors SI/Jn; = 1.03 and s2/Jn2 = 1.74 are the individual posterior distributions 
pee I y 1) and pee I yz) obtained respectively from the results of methods A and B. 
The unnormalized posterior distribution of e which combines information 
from both samples, is obtained by mUltiplying the I densities and is shown by the 
"I-like" distribution in Fig. 9.2.2. 
As might be expected, the combined distri-
bution is "located" between the individual t distributions. For this example, pee I y J 
has a narrower spread than pee I Y2) and consequently exerts a stronger influence 
on the final posterior distribution. 
9.2.2 Some Properties of p(e I y) 
Although the distribution pee I y) in (9.2.4) is the product of two distributions 
which are symmetric, the distribution itself will not be symmetric unless either 
S\ = Yz or nl = n2 and sf = s~. 
We see for example in Fig. 9.2.2 that the 
distribution is slightly skewed to the right. 
Upon differentiating the logarithm of pee I y) with respect to e, it is easy to 
see that for YI < .v2 
a 
ae logp(e I y) ~ 0 
e <.vI 
for 
(9.2.5) 
where 
a 
nice - Y'i) 
"elogp(e I Y) = -
2 
(e 
)2 
U 
VIS I + nl 
-.vI 
(9.2.6) 
so that the distribution is increasing in (- 00, .vI) and decreasing in (.Y2, (0) and 
consequently the mode(s) of the distribution will lie between (.Y I' Y2)' 
Bimodality 
When .vI and .vz are wide enough apart, the distribution becomes bimodal. 
By 
setting (0/8e) logp(e I y) = 0, it can be verified that when n l = n2 = n (so that 
VI = V2 = v) a sufficient and necessary condition for bimodality is 
(9.2.7) 
where 
To consider the implications of this condition, suppose for the moment that the sample 

484 
Estimation of Common Regression Coefficients 
9.2 
variances are nearly equal, so that si == s~ =" S2 and the second term on the left can be 
ignored, then (9.2.7) implies that the distribution will be bimodal only if 
(9.2.8) 
i.e., if 
where 
s.d. = ;nJv ~ 2 
is the standard deviation of the individual posterior distributions pee I )'1) and 
pee I Y2)' For example, when v = J I this means that 5'1 and 5'2 must be at least 6 standard 
deviations apart before the distribution becomes bimodal. 
The practical implication of this analysis is 
now considered. 
A question 
mentioned already and to be discussed in more detail in Section 9.2.3 is the 
appropriateness of the assumption of compatibility of the means. 
That is, the 
assumption that, to a sufficient approximation, e 1 = e2 = e. One might at first think 
that bimodality in the posterior distribution of e or its absence could provide a guide 
as to whether the compatibility assumption was reasonable. Expression (9.2.8) makes 
it clear that while bimodality would normally imply incompatibility, lack of bimodality 
certainly need not imply the converse. 
To see why this is so, we have only to remember that for sufficiently large v the 
component I distributions will approach the Normal form. 
Thus, for large sample 
sizes the right-hand side of (9.2.4) approaches the product of two Normal densities. 
But the product of two Normal densities, whatever their separation, is proportional to 
another Normal density, which is of course unimodal. It is not surprising then that, 
relative to their spread, the separation of the two component posterior density 
functions necessary to induce bimodality increases without limit as v is increased. 
Normal Approximation to pee I Y) 
When the two sample means Yl and Y2 are not widely discrepant, the combined 
posterior distribution (9.2.4) will be unimodal and located between the two 
individual t distributions. Although the unnormalized density function of e can 
be easily determined, it does not seem possible to express the posterior probability 
integrals in a simple form. Thus, for example, exact evaluation of the probability 
content of a specific H.P.D. interval would require the use of numerical methods. 
However, we see that when both VI and V2 are large the distribution in (9.2.4) is 
approximately 
p(GI y) :X exp [- 1 /1'(e - 8)2J, 
(9.2.9) 
where 

9.2 
The Case of m Inde[l~ndent Responses 
485 
As shown by Fisher (1961 b), the above expression can be made the leading term of 
an asymptotic expansion of the distributioJl in (9.2.4) in powers of v; 1 and v2 J. 
The "correction" terms in the asymptotic series are polynomials in 8 . .in Appendix 
A9.1 the asymptotic procedure is given for the more general situation when m = 2 
but Oc consists of more than one element. An alternative and simpler approximation 
to the distribution of ec based upon multivariate l distribution will be developed 
in Section 9.3. 
Returning to the distribution shown in Fig. 9.2.2 for the physical constant 
example, we see that the distribution is only slightly asymmetrical. 
Using the 
Normal approximation in (9.2.9), we have 
- (11 
13)-1[11 
13 
J 
e = -
+ -9-
-- 107.9 + -
109.5 = 108.34, 
12.1 
3.4 
12.1 
39.4 
which nearly coincides with the mode of the distribution in the figure. 
This 
itself, of course, does not imply that the distribution can be closely approximated 
by a Normal distribution without "correction" terms. 
A convenient device 
advocated by Barnardt for checking Normality is to graph the first derivative 
of the logarithm of the posterior density function against the parameter value. 
A unique characteristic of the Normal distribution is that this plot produces a 
straight line. A plot of the derivative in (9.2.6) in the range where the density is 
appreciable is shown by the broken line in Fig. 9.2.2. It is nearly linear, so that 
the posterior distributionp(8 I y) may be treated approximately as N(l08.34, 0.807). 
9.2.3 Compatibility of the Means 
Before information from m sets of measurements obtained by m different methods 
are combined, one should first consider whether the methods are measuring the 
same thing. In particular, it is helpful to compare individual posterior distributions, 
as was done for the numerical example in Fig. 9.2.1. More formally, questions 
of compatibility may be studied by considering the distribution of contrasts among 
the means derived from the joint " unconstrained" distribution of 81, • .. , 8m , 
when these parameters are not regarded as common. We illustrate with the case 
m = 2 and, as before, use an asterisk * to denote unconstrained distributions, 
the constraint being that 81 = 82 , As we have seen in Section 2.5, relative to the 
noninformative reference prior distribution 
p*(8 j , 82, cr1, cr2) ex (cr1cr2)- 1, 
the joint posterior distribution of 8 1 and 82 is 
[ 
nl(8! -2Y!? J- '~ (Vl+l) [1 + n2(8 2 -2Y2)2 J- }(V2+ 1), 
p*(8 1 , 82 I Y) ex 
1 + ----,;---
I'I S I 
V2 S 2 
- 00<81 <00, -00<82 <00. 
t Personal communication. 
(9.2.10) 
(9.2.11) 

486 
Estimation of Common Regression Coefficients 
9.2 
Figure (9.2.3) shows the contours of (8, , 82) as well as various other aspects of 
the distribution. 
2 
L-------------------~-----------------------e, ~ 
:)", 
Fig. 9.2.3 Various aspects of the unconstrained posterior distribution of (8,.82)' 
To obtain the distribution of the difference in means we write 
and 
(9.2.12) 
where 
and 
Iv l + w2 =1. 
Then the joint distribution of 0 and'8 w is 
p*(o, 8 w I y) 
oc [I + !21(8 w -
H'2~ -
y,)2 J - 1(", + I ) [I + n2(8 w + WI: - 512)2 ] - ~(V2+ I), 
VIS, 
V2S2 
-
00 < 0 < 00 , 
-
00 < 8w < 00. 
(9.2.13) 
The distribution can be written as the product 
p*(b, 8w I y) = p*(o I y)p*(8 w I b, y). 
(9.2.14) 
Whatever the choice of the weights W I and 1\12, the density p*(b I y) will be pro-
portional to the Behrens-Fisher distribution in (2.5.12) appropriate for making 

9.2 
The Case of m Independent Responses 
487 
inferences about the difference b = 8 2 -
8[ . 
The distribution p(8 I y) of the 
common mean (9.2.4) is p*(8 w I b = 0, y), the conditional distribution of 8w 
given that it is known that b = 8 1 -
82 = O. 
To shed light on the question of whether a supposition that D was close to 
zero was, or was not, tenable in the light of the data one could inspect the distri-
bution p*(b I y), the appropriately scaled Behrens-Fisher distribution for the 
difference in means. In a case like that illustrated in Fig. 9.2.4 the relevance of 
the distribution p(8 I y) in (9.2.4) would be thrown in doubt. Such evidence might 
suggest that, instead of attempting to pool incompatible information, the investi-
gator ought to direct his efforts to explain possible bias in the methods. 
Fig. 9.2.4 Posterior distribution of the difference b in relation to the value b = O. 
9.2.4 The Fiducial Distributions of 8 Derived by Yates and by Fisher 
Yates (1939), in his discussion of the problem of the weighted mean, considered the 
fiducial distribution of 
in the special case where 
(9.2.15) 
and found that 
had a Behrens- Fisher distribution. 
However, Yates' fiducial distribution is not the same as the fiducial distribution 
subsequently derived by Fisher. This, at first sight perplexing, situation is clarified by 
the Bayes analysis. 
We have already discussed Fisher's result corresponding to (9.2.4). We now consider 
Yates' result from this same viewpoint. 
The marginal posterior distribution of 

488 
Estimation of Common Regression Coelf:cients 
9.3 
8", = 
\1' J OJ + W z 8 2 is obtained in general by integrating out [) in (9.2.14), 
p*(8", I y) = f 
~J:.YJP'i'(8w I C), y)p*(b I y) db. 
(9.2.16) 
To see that this yields a density function' numerically identical to Yates' fiducial 
distribution, in (9.2.11) write 
(92. 17) 
and 
I (
ST 
2 
'[ = (a w -
jiw) 
~ 
W I 
(9.2.18) 
Z = t 2 sin ¢ + t J cos ¢ 
where 
Then. whatever weights WI and W 2 = 1 -
wI are employed, the distribution of , is 
pC, I VI .V 2.¢) 
CCf~c.o[ l + ('Sin¢~JzcoS¢)Z] -1 (VI + J)[1 + (ZSin¢:2TcoS¢)2]_ ~ (V2+I)dZ' 
-
r:f) < r < oc. 
(9.2.19) 
which is a Behrens-Fisher distribution . Yates result is obtained by using the particular 
weighting of (9.2.15). 
The basic difference between the two results p*(8", I b = 0, y) and p*(8w I y) (see 
Fig. 9.2.3) is now readily seen. Fisher's result for the distribution of the common mean, 
which corresponds with Jeffreys' Bayesian solution, is the conditional distribution of 
aw (or8\ or82), given that 8\ = 82 , 
That is to say, given that b = 8\ -
82 = O. 
On 
the other hand, Yates' result corresponds not to any conditional distribution but to a 
marginal distribution, that of 8w = wI 8J + w282' 
The relevance of Yates' result, is clearly open to question. For, if it can be assumed 
that OJ = 82 and hence that [) = 8\ -
82 = 0, one should employ the conditional 
distribution (9.2.4). Conversely, if 8\ and 82 cannot be assumed equal (implying that 
one or both contain unknown systematic error), there seems to be no basis for the 
weighting 
W\ 
IlJ/sT 
-
= --,-2 
w2 
n2 /s2 
which is based only on sampling variation. 
There will be cases where it cannot be plausibly asserted that 8\ = 8 2 exactly, but 
nevertheless we may feel fairly sure that the difference b = 8 J - 82 is small. Tn (his case 

9.3 
The Case of In Independent Responses 
489 
the Fisher-Jeffreys solution may be used as an approximation. 
Specifically, if an 
informative prior distribution p(Ci) is introduced which might for example be a Normal 
distribution centered at Ci = 0 with standard devialion ~b' then, on the same assumptions 
as before, t he posterior distribution of e", will be 
p*(e,.. 1 y) ex f:", p*(ew 1 Ci, y) p* (0 1 y) pCb) dCi. 
(9.2.20) 
Now (i) if we make ~o very large corresponding to the assertion that little is known about 
0, we obtain the marginal distribution in (9.2.16), which however will depend on the 
particular weights wI and 1V2 adopted; (ii) on the other hand, if we make ~J small so that 
pCb) approaches a delta function at b = 0, then the distribution of Ow tends to the 
Fisher ·Jeffreys solution (9.2.4) whatever the values 0/ the weights w J and W2. 
9.3 SOME PROPERTIES OF THE DISTRIBUTION OF THE COMMON 
PARA'VIETERS FOR TWO INDEPENDENT RESPONSES 
Until further notice, we shall use the symbol 8 to denote the k x I vector of 
regression coefficients common to all m responses, so that 
9' = 
8~ = (e J , •.• , 8k). 
When there are two independednt responses, from (9.2.3) the posterior distri-
bution of 8 is proportional to the product of two multivariate t distributions with 
different degrees of freedom, 
2 l 
(8 - 6)' XX(8 - 8)] - t (v, + k) 
pe81 y) x I1 
I + 
" 
2' 
, 
, 
i= 1 
ViSi 
-
00 < 8 < 00, 
(9.3.1 ) 
where 
and 8i satisfies the normal equations (X;X,)8i = X;yj. To obtain the marginal dis-
tribution of a su bset of 8, say 8; = (8 I' .. . , e I)' directly it would be necessary to 
evaluate, for each value of 81, a Ie -
/ dimensional integral. The difficulty may be 
avoided by writing the distribution in a different form which, in turn, leads to a 
simple approximation . 
9.3.1 An Alternative Form for p(8 1 y) 
Writing (Ji = (J1l and (J i = (J22 in (9.2.2) with In = 2 the posterior distribution 
of «(J~ , (J~ , 8) is 
( 2 281) 
(2)-<1:", +1)( 2) - 0112 + 1) 
{_~rV jS~ +Rl(8) +V2S~ +R2(O)]} 
p (J j . (J2, 
Y ex 
CJ j 
(J2 
exp 
2 
2
' 
2 . 
(Jj 
CJ 2 
~~ > 0, 
(Ji > 0, 
-
00 < 8 < 00 , 
(9.3.2) 

490 
Estimation of Common Regression Coefficients 
9.3 
where 
i = 1,2. 
If we integrate out (aT, an directly from (9.3.2), we obtain the form (9.3.1). 
Alternatively, we may make the transformation from 
(a~, a~, e) to (ai, lV, e) 
where 
(9.3.3) 
and integrate out aT to obtain 
pee, wi y) a: w ~ "2-1 {v,si + R, (e) + W[V2S~ + R2 (e)J} -'(Ill +n2), 
o < \1' < 00, 
-
00 < e < 00. 
(9.3.4) 
This distribution can be written as the product 
pee, Hi I y) = pee I \1', y) p(w I y). 
(9.3.5) 
Assuming that at least one of the matrices Xl and X 2 is of full rank, the identity 
(A7.1.1) in Appendix A7.1 may be used to combine the two quadratic forms 
RJO) and wR 2(0) in (9.3.4) into 
where 
and 
R(e I w) = [0 -
O(I\;)]'(X',Xl + WX~X2)[e -
O(\v)J, 
Sc(w) = w(e, - ( 2)'x',x, (X ',X, + WX~X 2)-1 X ~X 2 (el - eJ, 
It follows that the conditional distribution of () given U' is 
(9.3.6) 
-
00 < () < 00, (9.3.7) 
where 
S2(W) = 
[v,si + \VV2S~ + Sc(w)J, 
n 1 + n 2 -
k 
which is the k-dimensional multivariate t distribution 
tk[O(\\'), S2(W)(X',X, + \\X~X2) - I, n l +11 2 - k]. 
To obtain the marginal distribution p(\\' I y), we substitute (9.3.6) into (9.3.4) 
and integrate out () yielding 
0< H' < 00, 
(9.3.8) 
where d is the appropriate normalizing constant such that 
d '" 
= fo"' IX"X 1 + wX~X21 - 1/2H'}"2 - 1 [S2(H')J - H1I!+1I 1 -k)dH'. 

9.3 
Some Properties of the Distribution of the Common Parameters 
491 
An alternative form for the posterior distribution of e is therefore 
pee I y) = 50'" p(O I IV, y) pew I y) dw, · 
- 00 < 0 < 00, 
(9.3.9) 
where p(al LV, y) and p(H I y) are given in (9.3.7) and (9.3.8), respectively. 
By comraring (9.3.9) with (9.3.1), it can be verified that the normalizing constant for the 
expression (9.3.1) is 
(9.3.10) 
where d- 1 is the integral given in (9.3.8). 
Using properties of the multivariate t distribution, and partitioning 
p 
C(w) = (X'IXI + IVX~X2)-1 = l~;y\::~'c;~(;:~;t 
p = k -
I, 
(9.3.11) 
the marginal distribution of the subset a; = (8 1, ... , 8,), (I :( k), is 
p(a, I y) = I" p(a, I w, y) p(H'1 y) dw, 
-00 < a, < 00, 
(9.3.12) 
when,: p(ell LV, y) is the t,[S,(w), S2(\\')CI/(\v), 11[ + 112 - kJ distribution. A similar 
expression can be obtained for the remaining elements 
a~ = (8,+ I, . .. , Bk)' 
Although it does not seem possible to express (9.3.12) in closed form, its 
evaluation only requires numerical integration of one-dimensional integrals 
instead of the (k - I)-dimensional integration implied by (9.3.1)·· ·a very significant 
simplification. 
9.3.2 Further Simplifications for the Distribution of a 
In evaluating the posterior distribution of a and its associated marginals for 
subsets of a, if we were to employ directly the expressions in (9.3.9) and (9.3.12), 
it would be necessary to calculate the determinant and the inverse of the matrix 
(X'IX [ + LVX;X 2 ) for every w. We now show how this problem can be made more 
manageable. 

492 
Estimation of Common Regression Coefficients 
The special case where X~Xl and X~Xz are proportional 
Consider first the special case where 
The expressions in (9.3.6) then become 
R(O I w) = (I + aw)[O -
8(w)]' X'tXI [0 -
S(w)] 
-
j 
~ 
~ 
9(w) = --- (0 1 + awOz), 
1+ aw 
so that (a) the conditional distribution of 0 given IV, p(O I w, y), is 
-
00 < 0 < 00, 
where 
9.3 
(9.3.13) 
(9.3.14) 
(9.3.15) 
Z 
2 
Z 
aw 
-
~ 
~-
(n l + nz -
k)s (w) = VIS I + wV2SZ + --- (0 1 -
02)' X'IXI (0, - Oz), 
J + ow 
(b) the conditional distribution for the subset OI,p(O,1 w, y), is 
p(O,1 w, y) 
cc {(n l + 112 -
k)S2(w) + (I + aw)[O, -
S,(w)] 'D;[ 1[01 -
S,(W)]} - +(It I+n 2-HI), 
-
00 < 01 < 00, 
(9.3.16) 
where 
I 
p 
(X',X t )-' = [~~;'~;pp];) 
and (c) the marginal distribution of w,p(w I y), reduces to 
p(wl y) = d IX'tXjl - I,2 (J + aw)-k/Z wzn2 - ' [sz(w)]- t (n l+n2-k), 
0 < w < oc. 
(9.3.17) 
Employing (9.3.15) -(9.3.17), the posterior distribution of 0 in (9.3.9) and its associated 
marginal distributions for subsets of 0 in (9.3.12) can now be conveniently evaluated on 
a computer with little numerical complication. 
The posterior distribution of 0 thus obtained is of very much the same form as the 
posterior distribution of Q> in (7.4.33) and (7.4.41) for the BIBD model. In both cases 
we are combining information about regression coefficients from two sources where the 
variance ratio is not assumed known. The main distinction lies in the fact that for the 
BIBD model 'we have the added constraint w = (Ji;e/(J; > 1 on the variance ratio while 
no such restrict ion exists here. 

9.3 
~ome !'roperties of Ihe Distribution of the Common Parameters 
493 
The General Case II'hen X'tXt and X;X2 are not Proportional 
In obtaining (9.3.6). we have assumed that at least one of the two matrices, X;X\ 
and X; X2' is positive defin ite. To be specific, suppose that X~X! is positive definite. 
Then there exists a non-singular k x k matrix P such that 
and 
(9.3.18) 
where A = {}'jJ is a k x k diagonal matrix with non-negative diagonal elements. 
It can then be shown that, in the integral of (9,3,9) and (9,3.12), 
a) the conditional distribution p(O I IV, y) becomes 
p(911V, y) 
x:. {(n\ + n2 -
k)S2(W) + [0 - !l(w)],(P')-l(I + \I'A)P-l[O -
O(II')] }-
~ (" I
: "2), 
-
00 < 9 < 00, 
(9.3.19) 
with 
and 
k 
\.vA.. 
• 
A 
(nl + n2 -
Ic)S2(w) = VIS~ + \'\o' V2S~ + j~l 1 + ::A
jj (cPlj - cP2)2 , 
b) the conditional distri bution p(91 III', y) of the subset 9/ is 
P(O, I IV, y) 
oc {(nl + n 2 -
k)S2(W) + [01 -
OI(I \')J' CI~I( \V)[OI - 01 (11')]}- t<nl ':' ''2- k'l I) , 
-
00 < 91 < 00, 
(9.3 ,20) 
where 
and 
and (c) the marginal distribution pew I y) reduces to 
p(H' 1 y) = dIX;X 11-1'2 [h (I + W}'j)] - 1/2 1V~1I2-1 [S 2(IV)]-~' ('''+''2-k), 
J=! 
0 < IV < 00, 
(9.3,21) 
To obtain these results, we may write 
and 
(9.3.22) 
so that 
(9.3,23) 

494 
Estimation of Common Regression Coefficients 
It follows that for the expressions of sew) and Se(w) in (9.3.6) we now have 
Sew) = P(I + wA)-l (p-le j + WAP- l( 2) 
with 
and 
= P(I + wA)-l (<1>1 + wA<I>2) 
i = 1,2 
Se(W) = wce l - ( 2) '(p')-l p-l P(I + wA)-lp' (p,)-l A p- l (e1 - ( 2) 
= WC<l>l -<1>2)'(1 + wA)-lA(<I>l -<1>2) 
Further, the determinant IX~Xl + wX~X21 is 
IX'tXj + wX~X21 = ICP,)-lp- l + w(P,)-lAP- 11 = IP'I- 1Ip- J III + wAI 
k 
= IX'1 XII I1 (l + w}'i) 
j=l 
9.3 
(9.3.24) 
(9.3.25) 
(9.3.26) 
Substituting (9.3.24)-
(9.3.26) into (9.3.9) and (9.3.12), we obtain the desired results. 
In computing the posterior distributions in (9.3.9) and (9.3.12), standard 
numerical methods can be used to obtain first the matrices P and A. 
The 
distribution of a and its marginals of subsets of the elements of a can then be 
conveniently calculated using (9.3.19)-(9.3.21) with the appropriate normalizing 
constants inserted. All that is required is numerical evaluation of one-dimensional 
integrals involving simple functions. 
With the availability of a computer, this 
presents nO problem. 
9.3.3 Approximations to the Distribution of a 
We have shown in (7.4.50) that the distribution of <!> for the BIBD model can be 
closely approximated by setting 
p(<!> I y) == p(<!> I w, y) 
where w is the mode of the posterior distrihution of log ;I'. A similar argument 
can be applied to the distribution of a in (9.3.9). We can write 
p(al y) = 
E p(al IV, y) == p(al w, y) 
(9.3.27) 
log w 
where p(a I w, y) is obtained by inserting w for II' in (9.3.19) and w is the mode of 
the density 
p(log W I y) rx W" 2,2 rjbl (1 + WA j ) r 
1/2 [s2(\I')rl(1I1 
n2 -kl, 
- oc < log H' < CO, 
(9.3.28) 

Some Properties of the Distribution of the Common Parameters 
495 
which is derived from (9.3.21) by inserting the Jacobian [(dl dw) log wr J = w. 
Upon taking logarithms of (9.3.28) and differentiating, it can be verified that }v 
is the appropriate root of the equation 
hew) = 0, 
k 
hew) = i1zW- 1 - L }'j/l + II'AjJ-l 
j=1 
which maximizes (9.3.n). 
(9.3.29) 
To obtain the root w, it seems most convenient to employ the standard 
Newton-Raphson iteration procedure. That is, with an initial guessed value H'O' 
expand 
(9,3.30) 
where 
k 
[ 
A· · 12 
h'(w) = -
i1 2W- Z + L 
}} 
)=1 
1 + WAll 
+ (I1J + 112 - k)[s2(w)rZ [vzs~ + )tl }')j(J + WA,,)-2(¢J) - ¢zY r 
Thus, we find a new guessed value WI such that 
(9.3.31) 
h(wo) 
WI = Wo ---
h' (wo) 
which can now be used as the next guessed value, and the process repeaied until 
convergence occurs. A convenient choice tor the initial value is Wo = sf/si, 
To this degree of approximation, then, 9 is distributed as the Ie dimensional 
multivariate I distribution Ik[O(~V), S2(W)(X'lXl + wX;XZ)-l , 111 + 112 - kJ from 
which corresponding approximations to marginal distributions of subsets of 8 
can also be obtained. The accuracy of this approximation will be illustrated by 
an example in the next section. 
An alternative approximation employing an asymptotic expansion of the distri-
bution in (9.3.1) in powers of (V~I, Vii) is given in Appendix A.9.1. 
9.3.4 An Econometric Example 
For illustration, we analyse a simple econometric investment model with annual 
time series data, 1935-54, taken from Boot and De Witt (1960), relating to two 

496 
Estimation of Common Regression Coefficients 
9.3 
large corportaions, General Electric and Westinghouse. 
In this model, price 
deflated gross investment is assumed to be a linear function of expected profit-
ability and beginning of year real capital stock. 
Following Grunfe1d (1958), 
the value of outstanding shares at the beginning of the year is taken as a measure 
of a firm's expected profitability. The two investment relations are 
YIII = BOI + B\xull + B2x u 21 + 6,,1' 
Yu2 = B02 + B\XuI 2 + B 2x II 22 + 6112 , 
where u denotes the value of a variable in year u, U = 1,2, ... ,20, and 
General 
Variable 
Electric 
Westinghouse 
Annual real gross investment 
YI/I 
YI/2 
Value of shares at beginning of year 
Xull 
xuJ2 
Real capital stock at beginning of year 
xu2l 
xu22 
Error term 
eul 
eu2 
(9.3.32) 
The parameters Bl and 82 in (9.3.32) are taken to be the same for the two firms ; 
however, BOl and B02 are assumed to be different to allow for certain possible 
differences in the investment behavior of the two firms. Further, eul and el/2 are 
assumed to be independently and Normally distributed for all u as N(O, a~) and 
N(O, aD, respectively. 
We may write the model in (9.3.32) in the form 
(9.3.33) 
where 
'Yi = [B;i] , 
9= [ :: ] , 
Xiii 
X l 2i 
y; = (Yli' .. ·,Yui' .. ·,Yni) 
Xl/!i 
Xl/ 2i 
t; = (eli' .. " Bui , ... ) en;), 
i= 1,2 
and 
n = 20. 
xnli 
Xn2 i 
Thus, the two vectors of responses (y I, Y 2) are independently Normally distributed , 
and contain some (but not all) common parameters 9. On the assumption that 
(Bo l , 802,9) and log a 1 and log a 2 are locally uniform and independent a priori, 
the posterior distribution of the parameters is 
P(BOl' B02 , 9, ai, a~ I Y) cc TI (af)-(-}n+l)exp {- ~[VSi2 + (Yi - yYT;Ti(Yi - Yi)J}, 
i= 1 
2ai 
af > 0, 
-
00 < BOi < 00, 
-
00 < 9 < 00, 
(9.3.34) 

9.3 
Some Properties of the Distribution of the Common Parameters 
where 
v=n-3 
and 
VS? = (Yi - T,.9;)'(Yi - TiYi) 
Integrating out (801 and 802), we obtain 
497 
pea, aT, O"~ I y) ceIl (at) -[:(n-1) + 1] exp { - -2
12 [vst + (0 - 6J' X;X;(a - 9i)J} , 
!~ 1 
ai 
where 
at > 0, 
-
00 < a < 00, 
(9.3.35) 
1 
n 
Xji = - L X uji , 
n u~ 1 
j= 1,2. 
This distribution is of exactly the same form as that in (9.3.2) with ni = n -
1, 
Vi = v and k = 2. 
Eliminating aT and O"~ by integration, the distribution of 8 
then takes the form of the product of two bivariate t distributions as shown in 
(9.3.1) or alternatively can be expressed in the integral form (9.3.9). Numerical 
values for sample quantities needed in our subsequent analysis are given below. 
General Electric 
, 
6 [3.254 
XjX j = 10 
0.233 
0.233] 
1.193 
91 = (~11) = (0.02655) 
821 
0.15170 
sf = 0.777 X 103 
Vj = 17 
n 1 = 19 
Westinghouse 
, 
6 [0.940 
X1X2 = 10 
0.195 
0.195J 
0.074 
OJ = (~12) = (0.05289) 
-
822 
0.09241 
s~ = 0.104 X 103 
v2 = 17 
112 = 19 
A plot of the contours of the distribution of a is shown in Fig. 9.3.1. The three 
contours shown by the solid curves are drawn such that 
p(a I y) = c pee I Y) 
for 
c = 0.50,0.25,0.05 
where e == (0.037,0.145) is the mode of the distribution. In other words, they are 
boundaries of the H.P. D. regions containing approximately 50, 75 and 95 per cent 
of the probability mass, respectively. Also shown in the same figure are the 95 

498 
Estimation of Common Regression Coefficients 
9.3 
per cent contours (labelled by G.E. and W.H.) of the two factors of the distri-
bution of e together with the corresponding centers (9 1, ( 2), 
As expected, the 
distribution of e is located "between" the two individual bivariate t distributions, 
and the spread of the combined distribution is smaller than either of its components. 
Further, the influence of the first factor-· G .E.-is seen to be greater in determining 
the overall distribution because 
The solid contours in Fig. 9.3.1 are nearly elliptical suggesting that the 
distribution of e might well be approximated by the bivariate t distribution in 
(9.3.27). Since comparison of exact and approximate contours is rather difficult 
to make, the exact and approximate marginal distributions of 81 and 82 will be 
compared. 
0.221 
0. 12 
0.02 
L---~----~------~-----L----~------~O ~ 
0.09 
I 
0.01 
0.05 
Fig. 9.3.1 Contours of the posterior distribution of e and its component factors: the 
investment data. 
The solid curves in Fig. 9.3.2a and b show, respectively, the distributions of 8 1 
and 82 calculated from (9.3.12). 
The two factors in the integrand were 

9.3 
Some Properties of the Distribution of the Common Parameters 
499 
---- Exact 
---- Approximate 
32 
16 
16 
8 
~--~~--~----~~---L-e ~ 
0.09 
I 
o 
0.03 
0.06 
~--~--~~----~---e2~ 
o 
0.12 
0.24 
(a) 
(b) 
Fig. 9.3.2 Comparison of the exact and the approximate posterior distributions of 81 and 
82 : the investment data. 
determined from the expressions in (9.3.20) and (9.3.21). The matrices P and A 
in (9.3.18) and the vectors <l>i in (9.3.19) were found to be 
[
-0.19814 
P = 10- 3 
0.89469 
0.52196] 
0.22306 
<l> = p-lij = 103 [0.14331] 
I 
1 
0.10527 ' 
A = [°.0
0
2703 
0] 
0.30513 
, 
-I' 
3 [0.07128] 
1J>2 = P 
92 = 10 
. 
0.12839 
The approximating distributions shown by the broken curves in Figs. 9.3.2 and 
9.3.3 were determined using the result in (9.3.27). The root ~V of the equation 
hew) = ° in (9.3.29) was calculated by employing the iterative process in (9.3.30) 
and (9.3.31). 
Starting with the initial value Wo = si!si == 7.47, we found after 
two 
iterations, 
W == 7.349. 
Thus, 
9 
is 
approximately 
distributed 
as 
t2[G(~V), s2(~v)P(I + IvA)-lp', 36J where 
G(W) = [0.03726] 
0.14460 
[ 
0.8898 
and 
s2(w)PCI + WA)-Ip' = 10- 4 
-0.8535 
-0.8535] 
5.2060 .. 

500 
Estimation of Common Regression Coefficients 
It follows that to this degree of approximation, the quantities 
8 1 -
0.03726 
0.00943 
and 
82 -
0.1446 
-----
0.0228 
9.4 
are individually distributed as teO, 1,36) from which the broken curves were 
drawn. For this example, the agreement between the exact distribution obtained 
from numerical integration. and the approximating distributions are sufficiently 
close for most practical purposes. This, together with the near elliptical shape 
of the contours in Fig. 9.3. J, suggests that the approximation in (9.3 .27) will 
be useful for higher dimensional distributions. 
9.4 INFERENCES ABOUT COMMO~ PARAMETERS FOR THE GENERAL 
LINEAR MODEL WITH A COMMON DERIVATIVE MATRIX 
Returning to the linear multivariate model in (9.1.2), we now consider the case 
where not only the parameters but also the derivative matrices are assumed 
common , i.e., el = ... = em = ec and XI = .. = Xm = X. The elements of the 
multivariate observation vectors £(11) = (e,d' ... , ellm )', U = I, ... ,11 are, however, 
not assumed independent. 
For example, suppose observations were made by m = 3 different observers 
of a temperature which was linearly increasing with time. An appropriate model 
for the observations Yul' Y1l2, Yu3 made at time tu might be 
YIII = 8 1 + 82l ll + e lll 
YII2 = 81 + 82 f l/ + 8// 2 
YIl3 = 81 + 02 fll + (;1/3 
(9.4.1) 
If observations were made at 11 distinct times, we should have a model of the 
common parameter, common derivative form with 
and 
where X is the 11 x 2 matrix whose tlth row is (1, /..), 11 = 1, ... ,11. 
In general, the model may be written in the form of equation (8.4.1) with the 
k x m matrix e of (8.4.2) having common parameters within every row so that 
(9.4.2) 
From (8.4.8), it is readily shown that the posterior distribution of ec is 
-
00 < Oc < 00, 
(9.4.3) 
where 
and 
v = (X'X)-I + G[A- I - ~A-II I' A-I]O' 
d 
m 
111 
, 

9.5 
General Linear Model with Common Paramefers 
SOl 
which is the k-dimensional multivariate I distribution 
tk [8e , (n ~ k)d V, n - k ] . 
To show this, from (8.4.8) the posterior distributi on of Oc is 
-
00 < Oc < 00, 
(9.4.4) 
where it is to be remembered that e is a k x m matrix. Now, using (8.4.11) 
1 A + (Oel:" - e)'X'X(8): .. -
e)1 = 1,\/ 1 X'XII(X'X)-I + (8cl:/I -
O)A -I (8 e l~, - en 
(9.4.5) 
We can write 
where 
(8ct: .. - O)A -1 (8el:/I - 0)' = d(8e - 8c) (8e - lU' + 6BO', 
I 
B = A- I - -A- It l ' A-I 
d 
m m 
. 
(9.4.6) 
Making use of (9.4. 6) and (8.4.11), we see that the third factor on tbe right-hand side 
of (9.4.5) is proportional to 
(9.4.7) 
where 
v = (X'X)-J + eBO', 
and the desired result follows. 
The special case for which k = 1 and X a 11 x 1 column of ones was discussed 
by Geisser (l965b). The result for the situation when we have common parameters 
within rows of a block submatrix of 8 is similar to (9.4.3) and is left to the reader. 
9.5 GENERAL LJl\iEAR MODEL WITH COMMON PARA\1ETERS 
We now discuss certain aspects of the problem of making inferences about the 
common parameters 8c in the general situation when the derivative matrices Xi 
are not common and the responses are correlated. For simplicity in notation, 
we shall again suppress ' the subscript c and denote 0 = 8c . 
By setting 
8 1 = . . . = 8m = 8 in (8.2.23), the posterior distribution of the common parameters 
8 is then 
where 
S(8) = {Sij (O)} 
} 
Sij(O~ : (Yi - Xi8)'(Yj - X j 8) 
8 -
(8 1, .. . , Ok) 
-
00 < 0 < 00, 
(9.5.1) 
1=1, .. . , 111, j=I , ... , I11. 

502 
Estimation of Common Regression Coefficients 
9.5 
Since Xi # Xj' we cannot in general express Si/8) in (9. 5. 1) in the form given in 
(8.4.4), and thus it is no longer possible to reduce the distribution of 8 to the much 
simpler form in (9.4.3). When the vector 8 consists of only one or two elements, 
whether or not the model is linear in the parameters the distribution can be 
plotted and the contributions from the individual vector of responses Yl assessed , 
as was done for example in Section 8.2.6. 
As always, however, as the number of parameters increases, the situation 
becomes more and more complicated. Further, to obtain the marginal distribution 
of a subset, say el = (8 1, . .. , 81)', or8, it is necessary to evaluate a k -
I dimensional 
integral for every value of 81, 
In what follows we shall discuss the important 
special case In = 2 for the linear model and develop an approximation procedure 
for the distribution of 8. 
9.5.1 The Case m = 2 
When m = 2, the distribution in (9.5.1) can be written 
[ 
S2 (9)] -
~ n 
pC8Iy)cx.[S •• c9)r l-
II 
S22(9)-S::(9) 
, 
-00<8<00 . 
(9 .5.2) 
The first factor [S II (8)J -;11 is clea rly in the form of the multivariate t distribution 
t[e 1, siCX~XI)-l, vJ where, assuming XI is of full rank, 
el = (X'IX.)-IX'IYI , 
si = (YI -
XJ81)'(Yl - X1(1)"V 
(9 .5.3) 
and v = n - k. This factor can be thought of as representing the contribution 
from the first response vector Y I' 
The nature of the second factor, 
[S 22(8) -
Si2(9)/SI I (8)r -t
n
, which provides the " extra information" from Y2, 
is not easily recognizable. We now proceed to develop an alternative representation 
of the distribution of 8. 
It will be recalled that if (Y. , 12) are jointly Normal N2(~ ' I:), the distribution 
can be written 
(9.5.4) 
where 
P(YI I Jil' O"tt) = 
exp [_ (YI -
JiI) 2 ] 
v~ 
20" 11
' 
-00 < YI < 00 , 
-00 < 12 < 00, 

9.5 
General Linear Model with Common Parameters 
503 
For m = 2 and with common parameters e, the likelihood function of e and 
(all> ani' f3) is 
Ice, 0"11 , 0"22'1, f31 y) oc O"~/" O";/'; exp {- _1_ (Yl - xle)'(YI - Xle) 
20" 11 
- _1_ [Y2 - x1e -
f3(YI - xle)],[yz - x 2e -
f3(YI - Xle)]} . 
20" 22'1 
(9.5.5) 
From the noninformative reference prior distribution of e and ~ in (8.2.8) and 
(8.2.14), we make the transformation from (0"1l , a 22,0"12) to (a ll ,0"22 '1 ,f3) to 
obtain the prior 
pee, 0"11 ' 0"22'1, f3) oc 0"~/ / 2 0";23~2. 
(9.5.6) 
Consequently, the posterior distribution of (e,O"II'O"22-I,f3) is 
pee 0" 
0" 
f3ly)ocO"-[HII-1)+IJ~-r !(II+1)+11exp[_SII(e)_ S22'I(e,f3)] 
, II' 22'1' 
11 
V22 '1 
2 
' 
0"11 
20"22'1 
0"11 > 0,0"22'1> 0, 
- co < f3 < 00, 
-00 < e < 00, 
(9.5.7) 
where 
S I I (e) = (y 1 - XI e)'(y I - X I e) 
5 221 (e, fJ) = [y; - x 2e - f3(Yl - X,e)]'[Y2 - x 2e - f3(YI - x 1e)]. 
If we were to integrate out (0"11,0"22'1' f3) from (9.5.7), we would of course obtain 
the form given in (9.5.2). Alternatively, we may make the transformation from 
(0"11,0"22'1) to (0"11 ' w) where 
(9.5.8) 
to obtain 
p(9,lV,f3 IY)oc IV~(1I+1)-1 [SII(e) + w522 . 1(e, fJ)T", 
o < IV < co, 
- co < f3 < 00, 
-
00 < 9 < 00. 
(9.5.9) 
This distribution can be written as 
pee, 111, f3 I y) = pee I IV, fJ, y)p( w, fJ I y). 
(9.5.10) 
Now, we may write 
SII (9) + wSnl (e, fJ) = (211 - k)S2(w, f3) + R(S I IV, {3), 
(9.5.11) 
where 
R (9 I HI, (3) = [e - flew, {3)]'[X'IX I + WX;'I C/3)X2'1 ({3)J [e - 9(w, f3)J , 
(211 -
k)S2 (IV, fJ) = [y I - X19(11I, f3)]'[y 1 - Xl 9(w, f3)J 
+ w[y 2'1 (f3) - X21 (fJ)9( w, f3)]'[h I ({3) - X2'1 ({3)fl( lV, {3)J, 

504 
Estimation of Common Regression Coefficients 
and 
e(lV, [3) = [X'IXI + WX;I([3)X 21 ([3)r l [X'I)'I + IIX;'I([3)Y21(fJ)] 
Making use of (9.5.11), we see that 
a) the conditional distribution of e, given (11', f3), is 
p(O I H', f3, Y) ex [(211 - k)S2(\1', /3) + R(e i ll', (3)] -II , 
-oo < e < oo, 
which is the 
distribution. 
b) partitioning 
and 
[X'IXl + J1IX; I (tJ)X2l (/3)J I = [~!)::::~~~~;(~;: ,%t , 
the conditional distribution of the subset et> given (\\', /3), is 
p(e/I It', [3, Y) 
9.5 
(9.5. I 2) 
(9.5 .13) 
ex {(2n -
Ic)S2( 11', /3) + [el -
el(lI, /3)]'CI~ I (11', ,8)[el -
el(IV. /3)]} - t I2n - H I) , 
-
00 < el < 00 , 
a II[e/(I\!, /3), S2(W, j3)CI/(H', f3), 211 - kJ distribution, and 
c) the posterior distribution of (lV, (3) is 
(9 .5.14 ) 
pew, /31 Y) ex IX'IXI + \Vx; I(j3)X21 (/3)I - l/2 11'1-111 + 1)-1 [S2(1\', (3)r 1-(211 - k), 
o < H' < CO, 
- co < /3 < CIJ. 
(9.5.15) 
Thus, we may write 
pee 1 y) = fo'" f~ ,.pce 1 IV, /3, y)p(IV, [31 y) df3 dw, 
-00 < e < 00, 
(9.5.16) 
which is an alternative representation of the distribution in (9.5.2). The useful-
ness of this representation lies chieAy in the associated form for a subset el , 
p(ell y) = r">fCf. p(ell H', [3, y) pew, /31 y) dj3 dll', 
j 0 
- 00 
-oo < e/<oo. 
(9.5.17) 
If exact evaluation of the marginal distribution of el is desired , it will only be nece ~ ­
sary to calculate the double integral in (9.5.17), whatever the value of k , instead 

9.5 
General Linear Model with Common Parameters 
505 
of a Ie -
I dimensional integral implied by the form in (9.5.2). This is particularly 
useful whenever k -
I is greater than two. I n the situation where the responses 
were uncorrelated, (':1.3.12) could be simplified according to (9.3.20) and (9.3.21). 
It does not seem possible in general here to find a matrix P to similarly 
diagonalizc [XI X I + H'X~'I (,6)X2 . tC,6)J for all values of (,6, It), and consequently 
it is necessary to evaluate its determinant and inverse as functions of ,6 and II'. 
9.5.2 Approximations to the Distribution of 9 for 111 = 2 
From (9.5.16), one might attempt to approximate the distribution of 9 by writing 
p(O I y) = 
E 
p(9 I w,,6, y) == p(91 IV, E, y) 
[(w,P) 
(9.5.18) 
where ((0, E) is the mode of the distribution of some functions f(w,,6) of (w, ,6). However, 
this modal value (II', E) is not easy to determine since the distribution p(w,,61 y) in 
(9.5.15) is a rather complicated function of (w,,6). 
We now develop an alternative 
approach. 
The distribution in (9.5.9) may be written as 
p(O, ,6, wi y) = p(O,,6 I w, y)p( wi y) 
(9.5.19) 
where the first factor is 
p(9,,61 w, y) ex:. [Sll (9) + wS22 . 1 (9,,6)J -II, 
-
ex:. <,6 < 00, 
-
00 < 9 < 00. 
(9.5.20) 
For a given value of w, let (a w , Ew) be the mode of the conditional distribution (9.5.20). 
Employing Taylor's theorem, we may expand Sil (9) + wS22 . 1 (9,,6) into 
where 
and 
SII (9) + wS221 (9,,6) == (2n -
k -
l)s2(w) + Q(9,,61 w) 
I 
~, 
~ 
(B - aw ) 
Q(9,,61 w) = (9 - 0",,6 -
,6w)H(w) ,6 -
Ew 
H(w) = [~;l~~:~. 
HII(w) = X~XI + wX~'I(Ew)X2'I(E,.,,) 
h22(w) = W(YI -
X law)' (Yl -
x1a,.) 
h12 (w) = w{X~.I(Ew)(YI - x1aW ) + X'l [Y21(Ew) -
X 2 . 1 (E,,,)a,vJ} 
(9.5.21 ) 

506 
Estimation of Common Regression Coellicients 
9.5 
To this degree of approximation, the conditional distribution of (e, fi) given w is in the 
form of a (k + I )-dimensional I distribution. 
Integrating out fi from (9.5.20) using 
the approximating form (9.5.21), we have that 
(9.5.22) 
where 
Q(e I w) = (0 - fU' G(w) (e - awl 
which is the Ik[lt,s2(w)G
I (w),2n -
k -
1J 
distribution. 
Further, substituting 
(9.5.21) into (9.5.9) and integrating out (0, (3), the posterior distribution of wis. 
approximately, 
o < w < 00. 
(9.5.23) 
We may now adopt an argument similar to that in (9.3.27) to approximate the 
distribution of e as 
pee I y) = E pee I w, y) == pee I ~V, y), 
(9.5.24) 
lo)! W 
where p(S I w, y) is obtained by inserting w in (9.5.22) and w is the mode of the densit) 
-
00 < log w < 00. 
(9.5.25) 
Calculalion of Caw, (Jw, w). 
In obtaining the approximating form (9.5.22), the basic problem is to find, for a given 
value of w, the conditional mode 8w , p", of (9.5.20). 
From (9.5.7), we see that the 
quantity SII (e) + WS22'1 (e, (3) is quadratic in either [3 or e given the other. Thus, con-
ditional on e, S'I ce) + wS22-1 (e, {3) is minimized when 
(9.5.26) 
and, conditional on {3, it is minimized for 
(9.5.27) 
as given in (9.5.11) Thus, given an initial choice of e, say e = eo, expression (9.5.26) 
can be employed to obtain f30 = PCSo) which in turn can be used to calculate a(w, {30) 
from (9.5.27) and so on. 
A convenient choice for So is the center of SII(e), namely 
eo = (X'IXI)-IX\YI' 
This iterative 
procedure 
will 
converge under 
favorable 
circumstances. 
Once the conditional mode C8"" fJw) is determined for a given value of IV, the 
corresponding density of log IV in (9.5.25) for that value can be readily obtained. The 

9.5 
General Linear Model with Common Parameters 
507 
mode w may thus be found from (9.5.25) by employing standard numerical search 
procedures on a computer. A convenient preliminary estimate of ~V is given by 
(9.5.28) 
where si is given in (9.5.3), and 
2 
s~ 2 
52
2
J -
S 
-
-
2 
? 
J 
(9.5.29) 
with 
and 
v = n·- k. 
The accuracy of the approximation will be illustrated by an example in the next section. 
9.5.3 An Example 
To illustrate the theory developed abovc, we consider the following example. 
An experiment was cond ucted to assess the effect of change in pressure from 
30 psi to 50 psi on a chemical process yield, known to be subject to large errors. 
Twelve batches of raw materials were randomly selected and, from each batch , 
a pair of samples were taken by two chemists A and B for separate experimentation. 
Each chemist ran half of his experiments at high pressure and half at low pressure 
and they arranged their pressure settings such that each level of pressure run by 
chemist A was paired with each level of pressure run by chemist B an equal number 
of times. The data are given in Table 9.5.1. 
Table 9.5.1 
Pressure effect independently assessed by two chemists 
Chemist A 
Chemist B 
Barch no. 
Pressure 
Yield Yll/ 
Pressure 
Yield Y21/ 
50 
79.5 
30 
73.2 
2 
50 
76.4 
50 
85.0 
3 
50 
77.2 
30 
60.3 
4 
50 
76.0 
50 
72.9 
5 
50 
78.6 
30 
66.5 
6 
50 
82.9 
50 
80.0 
7 
30 
61.4 
30 
63.6 
8 
30 
67.4 
50 
78.3 
9 
30 
63.5 
30 
73.3 
10 
30 
69.1 
50 
81.3 
II 
30 
61.2 
30 
54.2 
J 2 
30 
69.7 
50 
76.3 

508 
Estimation of Common Regression Coefficients 
9.5 
It is assumed that In the region of the experimentation, the following model is 
appropria te 
where 
Yul = eOI + eX U ) + eul 
Y,,2 = e02 + eXu2 + eu2 
Pressure - 40 
(9.5.30) 
so that the Xui assume only two values (I, -I). The preSSUle effect e is assumed 
common for both chemists but the parameters eo I and 80 2 are taken to be different 
to aJlow for possible systematic differences in mean yield. 
The error terms 
(eub 8112) are assumed correlated because each pair of observations (YuI,Yu2) is 
made from the same batch of material. In terms of the linear model in (8.3.2) we 
have 
i = 1,2, 
(9.5.31) 
where 
(eOi ) 
6i = 
8 
' 
Xi = [1 : x;], 
11 = 12 
and 1 is a 12 x 1 vector of ones. 
We are thus in a situation in which some of the parameters 0i and part of the 
derivative matrix Xi are common between the two responses. 
Assuming that (e U )' 8u2 ) has the bivariate Normal distribution N 2 (O, I:) and 
on the basis of the reference prior distribution in (9.5.6), the posterior distribution 
of (801 ,802,8, all' a22'I, (3) is 
p( 80 I' 802 , 8, all' a 22-) , f3 I y) ex g I (eo I, e, a) I) g 2 (802 , e, a 22· I, fJ), 
-00 < eOI < 00, 
-00 < e02 < 00, 
-00 < e < 00, all> 0, 
a 22 ' 1 > 0, 
-
00 < fJ < 00. 
(9.5.32) 
where 
g (e 
e ~ )~~-[(ll t2)+llexp[ __ 1_(), 
X9)'(y 
X9)] 
I 
01, 
, v II "'-
v II 
I -
I I 
I -
I I 
. 
2a II 
x exp {- _1_[Y2 -
X292 -
fJ(YI -
X l 9 1)J'[Y2 -
X 292 -
fJ(YI -
X I9 1)]} 
2a22'1 
Since main interest centers on the pressure effect e, the parameters ceol .802) 
need first to be eliminated. Writing 
(9.5.33) 
where 
1 
n 
Yi = Yi - y;l, Yi = -
L Yui' 
n u= 1 
i = J, 2, 

9.:; 
General Linear Model with Common Parameters 
509 
1
and noting that l'Yi = 1'Xi = 0, we have that 
(Yt - Xt{lt)'(Yl - X1{l1) = (Yt -
X tB)'(Yl -
xIB) + 12(Gol -
Yl)2 
(9.5.34) 
and 
[Y2 - X 2{l2 -
/3(Yl - X1{l1)]'[Y2 - X 2{J2 -
/3(Yl - X t{lI)J 
= [Y2 - x 2B -
/3(Yl - xI 8)]'[Y2 - x28 -
/3(Yl - x1G)J 
+ 12[802 -
Y2 -
/3(801 - Yl)J 2. 
Substituting (9.5.34) into (9.5.32), (BOl' ( 02) can be easily integrated out, yielding 
p(8, () 1 I, () 221' /3 I y) ex. gi (B, () 11) gi(8, () 22-1, /3), 
-co < e < co, 
-co < /3 < co, 
()11 > 0, ()22.l > 0, 
(9.5.35) 
where 
and 
which is of the form in (9.5.7) with n = II, and the results in Sections 9.5.1-2 
are now applicable. 
Figure 9.5.1 shows the posterior distribution of B calculated from (9.5.2). 
The normalizing constant was computed by numerical integration. 
Also 
shown in the same figure are the normalized curves of the two component factors 
0.50 
0.25 
Second factor 
[ 
5 22 (8)] 
5 12 (8) - s~ 
pce, y) 
'\ , 
" 
, 
\ 
\ 
\ - /, 
)....... 
J 
" 
/ 
I 
"-
/ 
/ 
" 
\~First factor [SII (8)1" nl2 
\ 
/ 
I 
" 
/ 
~ 
........... 
/ 
/ 
" 
/ 
I 
/ 
/ 
" 
/ 
-" 
/ 
\ , 
\ 
'~--
' ........ ~-
---_/ 
L-------~~ ________ ~ 
________ ~ 
__ ~'~'~-~_L ___ e • 
4 
6 
8 
10 
Fig.9.S.1 Posterior distribution of B and its component factors: the pressure data. 

510 
Estimation of Common Regression Coefficients 
9.5 
of the distribution. The first factor, which is a univariate I distribution centered 
at e = 6.53, represents information about e coming from the results of chemist 
A, and the second factor, having mode at e = 5.49 and a long tail toward the right, 
represents the "extra information" provided by the results of chemist B. 
As 
expected, the overall distribution is located between the two component factors. 
It is a I-like distribution centered at 8 = 6.22 and skewed slightly to the right. 
Although for this example it will be scarcely necessary to approximate the 
distribution of e, for illustrative purposes we have obtained the approximate distribution 
from (9.5.24). Starting with the preliminary estimate 
Wo = sf/sL = 0.312 
we found by trial and error that ~V = 0.377. The corresponding 13,,, and ew are 
!Jw = 0.6948, 
8w =6.189 
from which we obtained 
pee I y) eX: [227.9735 + 16.4171(8 - 6.l89)2r2I2, 
That is, the quantity 
e - 6.189 
0.833 
-
co < e < co. 
is approximately distributed as teO, 1,20). Figure 9.5.2 compares the accuracy of the 
approximation for this example. The agreement between the exact and the approximate 
distributions is very close. 
p(81 y) 
0.50 
0.25 
4 
Exac t. 
Approx imate 
6 
8 
Fig. 9.5.2 Comparison of the exact and the approximate posterior distributions of 8: 
the pressure data. 

9.6 
A Summary of Various Posterior Distribution for Common Parameters 9 
9.6 A SV\IIMARY OF VARIOUS POSTERIOR DISTRIBVTIONS FOR 
COMMON PARAMETERS 9 
511 
In Table 9.6.1, we provide a summary of the various posterior distributions for 
making inferences about the common parameters 9' = C8 1, • •. , 8k ) discussed in 
the preceding sections. 
Table 9.6.1 
A summary of various posterior distributions for the common parameters 9 
Independent Responses: 
1. The model is 
i=l, ... , m, 
where Yj is a nj x 1 vector of observations 9 is a k x I vector of parameters, 
Xi is a nj x k matrix of fixed elements, and Ej is a nj x 1 vector of errors distributed 
as Normal N n,(O,O";;I). The Ej'S are assumed to be independent of one another. 
2. The distribution of 9 is, from (9.2.3) 
m 
p(91 y) oc TI [ajj + (9 -9j ) 'X;X;(9 -9;)J -tn" 
-
00 < S < 00, 
i=1 
where 
3. For m = 2, X~ X I positive definite, an alternative expression for p(9 I y) is in 
(9.3.12) 
p(9 I y) = fa""p(S' w, y)p(w I y) dw 
where from (9.3.8), 
pew I y) oc IX~ Xl + wX; X21- 1/ 2 wtn ,- 1 [S2(W)r Hn, +n , -k), 
0< W < 00, 
and, from (9.3.7), the conditional distribution p(9 I w, y) is the multivariate 
distribution 
with 
2 
1 
2 
2 
S (w) = 
[v,s\ +wV2S 2 +Sc(w)J, 
n\ +n2-k 
Sc(w) = w(9 1 -(2)' X'\X \ (X~X, + wX;X2)-1 X;X 2(9 1 -(2), 
vj=ni-k, 
i= J, 2. 

512 
Estimation of Common Regression Coefficients 
9.6 
Table 9.6.1 COl1lil1ued 
4. In particular, partitioning 
p 
e = rt-t 
[~:lt~:~ ' "~:Pp~.:~.] ~ 
the marginal distribution of St is 
p(SII y) = J: p(SII IV, y) pew I y) dw, 
where p(SII IV, y) is the multivariate I distribution 
'lcel(w), S2(W)Cl/(W), "1 + 172 - kJ. 
5. From (9.3.18) to (9.2.21) computation of p(O I y) and p(SII y) can be simpliAed 
by Arst Anding a k x k non-singular matrix P and a k x k diagona l matrix 
A = {Jcjj} , j=I, ... ,k, such that 
and 
so that 
i= 1, 2 
k 
IX',X, +wX;X21 = IX'IX, I TI (1 +w}jj) 
j =1 
and 
where 
6. A simple approximation to pee I y) is given by (9.3.27) 
p(S I y) = p(el w, y), 
i.e. 
o '"'- ,k[8Uv), s2(1:v)(X', X I + \"X ;X~) - I, 111 + 172 - kJ 
where from (9.3.28) and (9.3.29), IV is the root of the equation 
maximizing wp(w I y). 
An iterative procedure for Anding w is described in 
(9.3.30) and (9.3.3 1). 

9.6 
A Summary of Various Posterior Distributions for Common Parameters e 
513 
Table 9.6.1 Continued 
Correlated Responses with Common DerivaIive Matrix X: 
7. The model is 
i= J, ... , 11'l, 
where Yj is a 11 x 1 vector of observations, ge a k x I vector of parameters, X a 
11 x k matrix of fixed elements with rank k and I>j = (e I j, . ", ell)' a n x 1 vector 
of errors. It is assumed that 1>(11) = (eIl 1' " .,ell",)', u=l, ", ,111, are independently 
distributed as N",(O, l:). 
8. Let 
0= [01' ""O",J, 
i = l,,, .,m, 
i,j=l, ",, 111, 
and 1", be a 111 x I vector of ones. Then, from (9.4.3) the posterior distribution 
of ee is the multivariate t distribution 
where 
fj = ~OA 1 
c 
d 
1"" 
and 
General Linear Model : 
9. The model is 
i= 1, ... , In, 
where Yj is a 11 x 1 vector of observations, Xj is a n X k matrix of fixed elements, 
e a k x I vector of parameters and I>j a n X L vector of errors. It is assumed that 
1>(11) = (elll , "" G"m)', 
II = 1, .. ,,111, are independently distributed asiY",(O, l:), 
10. The posterior distribution of 9 is, from (9.5.1), 
-00 < 9<00, 
where 
S(9) = {Sij(9)} 
i, j = 1, "" 111 

514 
Estimation of Common Regression Coellicients 
9.6 
Table 9.6.1 Continued 
11. For m = 2, an alternative expression of the posterior distribution of 6 is given 
in (9.5.16) 
p(6 I y) = f "- f"'" p(6 I w, /3, y)p(w, /31 y) d/3 dw, 
o 
- co 
-00 < 6 < 00, 
where, from (9.5.15), 
pew, 131 y) cc IX'tXI + wX; .! (/3)X 2.! (/3)1 - 1/2 wt (II+ll - ![s2(w)r 1-( 211-kl, 
o < H' < CO, 
-
00 < /3 < 00 
and, from (9.5.12), the conditional posterior distribution p(6 I w, fj, y) is the 
multivariate / distribution 
with 
9(w,(j) = [X'tXl +wX2 _I (fj)X2.1«(j)rt[X'IYI + IVX2'1 «(j)Y21 (/3)] 
X2.! (fj) = X l -fjX!, 
h! (/3)=Y2 -fjYl 
and 
(2n-k)sl(W,fj> = [YI -XI9(w,/3>J[Yt -XI9(w,(j)] 
+ w[y 2. t (fJ) - X21 (fj)9( W, fJ)]'[Yll «(j) - Xl. 1 «(j)a( IV, (j)} 
12. By partitioning 
6 = [t·t 
and 
the marginal posterior distribution of 61 is in (9. 5.17), 
-
00 < 61 < 00, 
where p(6 / 11V, fj, y) is the multivariate t distribution 
13. A procedure for approximating the distribution p(6 I y) is developed in Section 
9.5.2. 

A9.1 
Appendix 
515 
APPENDIX A9.1 
Asymptotic Expansion of the Posterior Distribution of 9 for Two Independent 
Responses 
We now develop an asymptotic expansion for the distribution of 9 in (9.3 .1), 
2 
[ 
(9 - 9YXX(9 - 9)] - t ( \' ; +k) 
p(9 I y) oc TI 
1 + 
" 
2 ' 
I 
, 
i= I 
ViS i 
-
00 < 9 < 00. 
(A9.1.l) 
The results can be used to approximate the distribution as well as the associated 
marginal distributions of subsets of 9. 
The procedure is a generalization of 
Fisher's result (1961 b) for the case of the weighted mean discussed in Section 
9.2.1. 
For simplicity in notation, we write 
i = 1,2, 
where 
Expression (A9.1.1) then becomes 
p(91 y) = c-! g(Q! , Q2), 
-00 < 9<00, 
where 
and 
c = I 
_«> <0<<<> g(QI' Q2)d9 
The expression (J + Q I/v!r-Uv, H ) can be written 
(A9.1.2) 
(A9.1.3) 
( 
QI )-t(V' .,- k) 
[VI + 
k (+ QVII)] . 
1 + ~ 
= exp (-iQI) exp }QI - --2-log I 
Expanding the second factor on the right in powers of vjl, we obtain 
(I + 
QVII )- :(YI " k) 
" 
= exp (- }QI) L PiVji, 
i =O 
(A9.1.4) 
where 
Po = J, 
PI = -t(Qf - 2kQ!), 
P2 = 9~6{3Qi - 4(3k + 4)Q~ + 12k(k + 2)QT], etc. 
Similarly, we have that 
( I + ~)- H YP k) = exp(- ~Q2) t qiV;i , 
V2 
i=O 
(A9.I.S) 

516 
Estimation of Common Regression Coefficients 
A9.1 
where 
qo = I, 
ql = {'( Q ~ - 2kQ2), 
q2 = 
9 lo[3Qi - 4(3k + 4)Q~ + 12k(k + 2)Qn etc. 
Substituting (A9.1A) and (A9.1.5) into (A9.1.3) and after a little reduction , we 
can express the posterior distribution as 
pee I y) = 
11,-1 h(9), 
-
00 < 0 < 00. 
(A9.1.6) 
where 
IMII/Z 
W 
'Y) 
.• 
11(9) = (2n:f.k exp(-}Q)JoJo PiqjV~11'2J, 
Q = (9 - 8)' M(e - 9), 
and 
1\' = roo <0<00 11(9) d9 
(A9.1.7) 
The integral II' in (A9.l.7) can be evaluated term by term . 
From (A9.1A) 
and (A9.1 .S), we see that each term is a bivariate polynomial in the mixed moments 
of the quadratic forms QI and Q2 where the va riables e havea multivariate Normal 
distribution N k (8, M- l ). For this problem , it appears much simpler to obtain 
the mixed moments indirectly by first finding the mixed cumulants. 
It is 
straightforward to verify that the joint cumulant generating function of QI and 
Q2 is 
J 
1;\111/2 
K(I"t2) = log 
-
)J k exp(t1QI + 12Q2 - l Q)d9 
-00 < 0 < 00 (2][ ' 
-
}log lI -
2M- l (t I M I + 12 '\12)1 + tlll~Mllll + l211; M 2112 
+ 2(tIM 1lll + t 2M z112),(M - 2tlMI -
2t 2M 2)-1(II M llll + {2M2112), 
(A9.1.8) 
where 
and 
Upon differentiating (A9.1.8) and after some algebraic reduction , we find (see 
Appendix A9.2) 
K IO = trM- I M 1 + 11; M 1TJ 1, 
/(01 = tr M- 1M 2 + 11 2M 2112, 
J(,s = 2,+S-I(r + s -
2)1[(r + S -
1)trM- 1G" 
+ (rTJ 1 + STJ2)' G"(rll 1 + STJ2) -:- I'TJ'1 G"TJ 1 -
STJ;Gr"TJ2], 
where 
(A9.1.9) 
r + S ~ 2, 

A9.1 
Appendix 
517 
Employing the bivariate moment- cumulant inversion formulae as given by Cook 
(1951), the integral l\' in (A9.1.7) can be written as 
I; 
v . 
'" "b -i-j 
H' = 
L.. 
L.. 
i j V I V 2 , 
where 
boo = 1, 
blo = :1:(/(20 + /(~o -
2/o(J o), 
bOI = {(/(02 + I(~I -
2kiCol), 
i=D j=O 
(A9.LlO) 
b ll = -to [/(22 + 1(20/(02 + 21(L -I- 4/(j[ICOI /(10 + /(~0f(~1 + 2/(21/( 0 1 + 2/(12/(1 0 
+ 1(2 0/(61 + /(02/(;0 -
2k(K12 + /(21 + /(02/(10 + /(20/(01 + 2/(11/(10 
+ 2/(11/(01 + /(10 /(61 + /(ol/( i o) + 4k2(/(11 -
I(OI/(IO)J, 
b 20 = eft [3(/(40 + 3/(~0 + 4/(30 /( 10 + 6K1 0 K i o + I(io) 
- 4(3k + 4) ("30 + 3/(20 /(10 + Kio) + 12k(k + 2) (/(20 + Ido)J, 
b02 = <h,-[3(/(04 + 3/(62 + 4K031\01 + 61(02 /(61 + 1(61) - 4(3k + 4) 
- 4(3k + 4) (/(0 3 + 3/(0 2/(01 + /(~ I) + 12k(k + 2) (1(02 + 1(61)J, etc. 
Substituting the results in (A9. I.IO) into (A9.1.6), we obtain the following 
asymptotic expression for the posterior distribution of e, 
(9 I ) 
IMI
I
,2 
[I (0 
")' M(9 
1\)J I~ ~ d 
- i 
- j 
P 
Y2 
= (2nr !< exp 
-
'2 
-
\J 
I 
-
IJ 
i ~ Oj~O ijVj v2 
, 
-
00 < 9 < 00, 
(A9.1.11) 
where 
doD = I, 
and 9 and M are given in (A9.1.7). 
Expressions for additional terms d12 , d21, d22 , etc., can similarly be found 
if desired. 
The posterior distribution is thus expressed in the form of a multivariate 
Norma l distribution multiplied by a power series in v~ I and v; I. 
When both 
VI and V2 tend to infinity, all terms of the power series except the leading one vanish 
so that, in the limit, the posterior distribution is multivariate Normal N k (9, M - I ) . 
For finite values of VI and V2 , the terms in the power series can be regarded as 
"corrections" in a l\,jormal approximation to the distribution in (A9.l.l). hom 

518 
Estimation of Common Regression Coefficients 
A9.1 
(A9.1.4), (A9.I.S) and (A9.1.9), we see that numerical evaluation of the 
coefficients in the power series involves merely matrix inversions and mUltipli-
cations, operations which are easily performed on an electronic computer. 
We note that when the posterior distribution is a univariate distribution as 
in (9.2.4), the results in (A9.!.1 I) are in exact agreement with those obtained by 
Fisher (1961 b). I n Fisher's derivation, each term of the integral \v in (A9.1. 7) was 
expressed in terms of the moments of a univariate Normal distribution. It can 
therefore be evaluated directly without making use of the mixed-cumulant formulae 
given in (A9.1.9) which seem more convenient for tbe multivariate case con-
sidered here. 
For the univariate case, posterior probabilities can be calculated using the 
formulae given in Fisher's paper. When k > I, the corresponding formulae for 
the evaluation of joint probabilities become exceedingly cumbersome and are 
not given here. 
The M argina/ Posterior Distrihution 
When interest centers on a subset of the elements of a, say a; = (8[, .. . ,8,), an 
asymptotic expression for the corresponding marginal posterior distribution 
can be obtained by integrating out the remaining elements, a; = (81+ 1, ... , 8k ) 
from the joint distribution in (A9.I.I I). We have that 
[:v1[1/2f 
00 
00 
.. 
p(a,ly) = -( 
'. k 
exp(-}Q) I I dijV;'viJdar' 
2n) -
-00 <9,< "" 
i=O j=O 
(A9.1.l2) 
Partitioning 8 into 8' = (8; : 8;.) and the matrices M and :\1-1 into 
we can write the marginal posterior distribution as 
IV,~ 11' /2 
p(a, I y) = 
.1., exp [ - HO, -
e,)'vl~ 1 (0, - 8,)]/(0,), 
(2n) ' 
-00<0,<00, 
(A9.1.13) 
where 
with 
From the expression for dij given in (A9.1.11), we see that each term in the 
integral f(O,) is a bivariate polynomial in the quadratic form QI and Q2 where 
0, is now considered fixed and Or has a multivariate Normal distribution 
~ ... ,-
: 
. #-. 
If· 
~'" 
.;..... 
. 
. 
,~' '.". 
. 

• 
••• _''':' ., 
.... ~:-. 
f 
.. 
-.\." 
-........ 
~ 
----------
~ 
-
-
A9.1 
Appendix 
519 
"'r(9,., M':;: I). Adopting the same procedure as that described In the preceding 
;ection and by setting 
'1 2 = [CH .• ~lrll , 
Cr/ : Crr. r 
we obtain the mixed cumulants of Q I and Q2 
WIO = trMr~1 Brr + 1'1 BrrYI + (91 -
( 11),E;i 1(al - 911) 
W0 1 = tr Mr~ 1 Crr + y~ Crryz + (91 -
(21)'F/~ I (el - 6zl) 
Wrs = 2r +' - 1(r + S - 2)! [(r + S -
l)trM,~1 H" 
where 
+ (rYI + SYz)' HYS(ry] + SYz) - rY'1 HrsYI -
sy~ Hrsyz]' 
Hrs = Mrr (M,~ I Brr)' (M':;: I CrY 
YI = (Sr - 61r) + (Br~1 Brl -
M,:;:l Mr/)(al - 611) 
Yz = (Sr - 92J + (Cr~l Crt -
Mr~l M"/)(al -
eZI)' 
(A9.l.14) 
(A9.1.l 5) 
r + S ~ 2, 
Using the results in (A9.l.l5), we can express the marginal posterior distribution 
of al as 
(A9.1.16) 
with 
boo = I, 
and the quantities gi) are functions of the mixed cumulants wi) with the functional 
relationships exactly the same as those between bi) and /(ij shown in (A9.I.I0). 
It is seen that the leading term in the expansion is a multivariate Normal 
distribution NI(S/' VII)' 
When 91 consists of only one variable, the quantities 

520 
;':stimation of Comlllon Regression Coefficients 
'\9.2 
bij in (A9,1.16) are simply polynomials in that variable. 
Lmploying the well-
known expression for the moments of a Normal variable, one can easily derive 
an asymptotic expression for the moments. 
In addition, probability integrals 
can also be approximated using methods given in Fisher's previously cited paper 
. ( 1961 b). 
APPE"IDIX A9.2 
Mixed Cumulants of the Two Quadratic Forms Ql and Q2 
From the joint cumulant generating function of the quadratic forms Q\ and Q2 
given in (A9.1.8), we now derive the expressions for the mixed cumulants shown 
in (A9.1.9). In our development, we shall make use of the following lemma. 
Lemma Let PI be a 11 x 11 positive definite symmetric matrix and P2 be a 11 x n 
nonnegative definite symmetric matrix. Then, for sufficiently small i, we have 
X; 
ir 
log II - iP 1P 21 = 
- I - tr (P 1P 2Y· 
r = 1 r 
(A9.2.1) 
Employing the above lemma and for sufficiently small values of 11 and 12, we 
can expand the first term on the right of (A9.1.8) into 
<YO 
2r -
l 
--i 10giI - 2M- l (tl M I + 12MJI = I --tr(tI M - 1M I -i- 12M- 1M 2)', 
r=1 
r 
In (A9.1.8), the quadratic form 1111'IMl111 can be w'ritten 
(A9.2.2) 
I [ll'[M[ll1 = tlTJ'IM1(M -
21[M[ -
212M 2) - I(M -
211Ml -
21 2M 2)TJl 
Similarly, 
= 11TJ'[M l(M -
2/[M[ -
2/ 2M 2) - IMTJI -
2t711'tMl(M -
211Ml 
-2t 2M 2)-IM l11[ -
21\1211'[M I (M -
2tlMI -
212MJ- IM 2TJ1' 
(A9.2.3) 
1211~M2TJ2 = 1211~M2(M -
21[Ml -
212M2) - IMli12 -
2dTJ~M2(M -
211Ml 
(A9.2.4) 
Thus, the expression in (A9.1.8) becomes 
ro 2,'-1 
1((1[,/ 2 ) = I --tr(t IM- 1M I + 11lVr lM 2Y + I 1TJ'I M l(I -
2/ 1M - 1M[ 
r= 1 
r 
-2t2M-IM2)-1111 -I- 1211;M2(I -
21 1M - 1M I -
2t2M-1M2)-1 112 
-
21[12(tl1 - TJ2)'M[CI -
2t ll\,r - 1M\ -
2t 2M-- 1M 2) - 1 M- IM 2(111 -112). 
(A9.2.5) 
/ 

A9.1 
Appendix 
52] 
Since 1\1 =:\1 1 -+- M 2 , it is easy to see that the matrix M I M '
I l\1 2 is symmetric. 
By virtue of this property, we have 
(tIM - IM I + '2M-1M2)' = t (r) I; ,;- i(M - 1M I)i(M -- 1M 2),-i 
(A9.2.6) 
i=O 
I 
and , for sufficiently small values of 'I and '2' 
(I -
211 M-IM I - 2'2M -- lM 2)-1 = I f 2i + j'~d(i ~ J)(M-IM1Y(M IV( 2)j 
I ~ O )=0 
I 
(A9.2.7) 
Substituting (A9.2.67) into (A9.2.5) and after a little rearrangement, we find 
[(1 1,12) = I + I 1"-1,; [~tr(M - IMlr + l1'IM(M - 1MtY'1l] 
r = I 
I 
OC 
"': 
_ (r + S -
2)! 
. 
+ I I 
2r !-'-1 1~1 2 
1 I 
[(r + S -
l)trM- IG" 
r=l.\ ~
l 
r.S . 
where 
G" = M(M - I M I ),(M- 1VI 2Y. 
Upon differentiating (A9.2.8), we obtain 
ICrO = 2,-1 (r -
I) ![tr (M- I M I)' + r'1'1 M(M - I M I)'l1IJ, 
1C0s = 2-,-I(S -
1) I[tr(M - 1M 2Y +Sl1~M(M-1M2Y11 2J, 
r, s ~ 1 
which can then be combined into the expressions given in (A9.] .9). 
(A9.2.8) 
(A9.2.9) 
(A9.2.IO) 
(A9.2.ll) 
· 
. 
'. 
---.-

--
" 
.... 
. 
. 
CHAPTER 10 
TRANSFORMATION OF DATA 
10.1 INTRODUCTION 
In this chapter we discuss the problem of data transformation in relation to the 
linear model 
y = Eey) + E, 
E(y)=xe, 
(10.1.1) 
where y is an 11 x I vector of observations. X a 11 x Ie matrix of fixed elements, 
8 a Ie x I vector of regression coefficients, and E a 11 x I vector of errors. 
In Section 2.7, we discussed in detail the linear model (10.1.1) under ~the 
assumption that E is distributed as NII(O, (}21). This Normal theory linear model 
is of great practical importance and has been used in a wide variety of 
applications. These include, for example, regression analysis and the analysis 
of designed experiments such as Ie-way classification designs, factorials, randomized 
blocks, latin squares, incomplete blocks, and response surface designs. Normal 
theory analysis, whether from a Bayesian or sampling point of view, is attractive 
because of its simplicity and transparency. The possibility of data transformation 
greatly widens its realm of adequate validity. 
In using the Normal linear model , we make assumptions not only about 
(i) the adequacy of the expectation function E(y) = X 8 but also about the 
suitability of the spherical Normal error function to represent the probability 
distribution of E. Specifically, we assume (ii) constancy of error variance from one 
observation to another (iii), Normality of the distributions of the observations 
and (iv) independence of these distributions.t 
The last assumption (iv) is 
perhaps the most important of all. Its violation leads to dramatic consequences. 
Its relaxation leads to the consideration of a whole new range of important time 
series and dynamic models, which, however, are not treated here. 
Nevertheless, at least for data generated by designed experiments, the 
independence assumption is one whose applicability can to some extent be assured 
by the physical conduct of the experiment, and we shall here suppose it to be 
t We are, of course, also assuming that there are no "aberrant" observations. Tn practice, 
.due to the possible anomalies in the experimental setup, some of the observations might 
have been generated, not from the postulated model, but from an alternative model which 
has a large bias or a much larger variance. For a Bayesian analysis of this problem, see 
Box and Tiao (1968b). 
522 

· 
-
-~---
10.1 
Introduction 
523 
valid. 
Assumptions (i), (ii), and (iii) are usuaIJy not under the experimenter's 
control, but it happens rather frequently that, although these assumptions are 
not true for the observations in the original metric, they are reasonably well 
satisfied when the observations are suitably transformed. 
We now consider 
why this is so. 
10.1.1 A Factorial Experiment on Grinding 
Consider an experiment on the grinding of particles which are approximately 
spherical. Suppose that observations on the final size of the particles were taken 
after standard material had been fed to three different grinding machines for four 
different grinding periods, using a 3 x 4 factorial design, and that the primary 
purpose of the experiment was to determine how different machines and 
different grinding periods affected the final size of the particles. 
Now suppose it happened to be true that if "size" 
were measured by 
particle radius y, then, to an adequate approximation,t the Normal linear 
model (10.1.1) could represent the data and the effects of machines and of 
periods of grinding would behave additively. Specifically, suppose that, if "par-
ticle size" was measured in terms of radius, then, to a sufficient approximation, 
a) there would be no interaction between machines and grinding periods, 
b) the error variances would be constant, and 
c) the error distribution would be Normal. 
Now it might be inconvenient to measure particle size in terms of the 
radius y and it might not occur to the investigator to do so. Instead, he might 
lJ1easure the area of the circular section of the particle seen under the microscope 
(proportional to /) or he might weigh the particle and so effectively work with 
the volume (proportional to y3). In either case he would probably report his 
results in terms of what was actually measured. 
But if, in terms of the y's, the effects were additive and the errors 
spherically Normal, this would certainly not be true, for example, of the effects 
and errors in terms of the y2,S. Specifically, suppose that 
i = I, 2, 3; j = I, ... , 4, 
00.1.2) 
where E(Yij) = 8 i + c!>j is the average response for the ith machine run and the 
jth period of time, and etj is distributed as N(O, (J2). Then, for the particle size 
measured in terms of area, we should have the model 
( 10.1.3) 
where 
(10.1.4) 
t Since the particular radius could not be negative, the Normal assumption could not be 
true exactly. 

524 
Transformation or Data 
10.1 
and 
(10.1.5) 
with 
(10.1.6) 
We note that for the response Yi~ 
a) a nonadditive (interaction) term ei CPj now appears in the expectation 
function E(yf) in (lO.IA), 
b) the error eij in (10.1.5) has a variance which is not constant but depends 
directly on E(Yi) = ei + CPj, and 
c) since Bij has a Normal distribution the distribution of eij must certainly be 
non-Normal. 
. 
Now in spite of this it is nevertheless true that, if the investigator made an 
appropriate transformation (in this case the square root transformation) of his 
"particle area" data, the simple Normal linear theory analysis would apply. 
In general. there is often surprisingly little to be said in favour of the 
original metric in which data happen LO be taken. For example, temperature can 
be measured in terms of ' C when it is related to the expansioD of a thread of 
mercury in a thermometer. However it could equally well be measured in terms 
of molecular velocity V IX (T + 273)1 /2 which would be of more immediate 
relevance in some contexts. 
Bearing these arguments in mind, it is perhaps not surprising that numerous 
examples occur where the assumptions underlying the ~ ormal theory linear 
model , although not true for observations in the original metric, provide an 
adequate representation after suitable transformation. 
In some instances 
appropriate transformations have been arrived at from 
theoretical con-
siderations alone, in some cases by analysis of the data alone, and in some 
instances by a mixture of both. 
We shall here consider estimation of 
transformations from the data. 
It is not of course suggested that interaction, heterogeneity of variance and 
non-Normality can ahmys be eliminated by suitable transformation of the data. 
In fact, cases of interaction, inhomogeneity of variance and non-Normality can 
each be divided into two classes: transformable and non-transformable. 
In 
the first cJass the phenomena of interaction , of variance inequality or of non-
Normality are anomalies arising only because of unsuitable choice of metric. 
In the second class the phenomenon is of more fundamental character which 
transformation cannot eliminate. 
Finally it will not necessarily be true that the same transformation of the 
data will simultaneously eliminate all discrepancies from assumption . 
Cases 
can arise where, for example, approximate additivity can be achieved by a particular 
transformation but not simultaneously with variance homogeneity. 

10.2 
Analysis of the Biological and the Textile Data 
525 
In summary, we should like [0 employ the model (10.1.1) with 
a) the expectation function E(y) = X e having the simplest possible form, 
b) the individual errors 8 ;, i = I , ... , n, having the same variance (J2, and 
c) the individual errors t ; independently and Normally distributed. 
If a measurement of y does nOL possess these properties, a suita ble nonlinear 
transformation of y may improve the situation. 
I t is often the case that the 
logarithm, the reciprocal , the square root, or some other transformations of y 
will make possible an analysis which strains assumptions less and in terms of which 
a simpler representation is possible. 
10.2 At'jAJ.YSIS OF THE BIOLOGICAL AND THE TEXTILE DATA 
We now introduce two sets of data and consider in some detail the question of 
choice of appropriate models, following joint work with D .R. Cox . 
10.2.1 Some Biological Data 
Table to.2.1 shows the survival times of n = 48 animals exposed to three differelll 
poisons and subject to four different treatments. The experiment was set out in 
a 3 x 4 factorial design with a fourfold replication (four animals per group). We 
shall analyze the data using the linear expectation function E(y) = XO and we now 
discuss precisely what this involves and the justification for doing so. 
Table 10.2.1 
Survival time (in \O-hr units) of animals in a 3 x 4 factorial experiment 
Treatment 
Poison 
A 
B 
C 
D 
0.31 
0.82 
0.43 
0.45 
0.45 
LlO 
OA5 
0.71 
0.46 
0.88 
0.63 
0.66 
0.43 
0.72 
0.76 
0.62 
II 
0.36 
0.92 
0.44 
0.56 
0.29 
0.61 
0.35 
1.02 
OAO 
0.49 
0.31 
0.71 
0.23 
1.24 
0.40 
0.38 
III 
0.22 
0.30 
0.23 
0.30 
0.21 
0.37 
0.25 
0.36 
0.18 
0.38 
0.24 
0.3\ 
0.23 
0.29 
0.22 
0.33 

526 
Transformation of Data 
10.2 
The use of the linear expectation function fey) = xa for this biological 
data and for most other examples is an admitted approximation to the truth. 
In most cases there undo ubtedly exists some true underlying functional 
relationship 
fey) = f(~ , <1», 
(10.2.1) 
which is probably nonlinear in the parameters 4> and which describes in 
mechanistic terms how fey) is affected by some set of basic input variables ~ , 
involving some set of parameters <1>. 
We have tacitly decided to replace it with 
the empirical linear model (10.1 .1). 
This may be either because, in the presenl 
state of the art, this " true" relationship is unknown and would be too difficult 
to find out, or because the known mechanism is too complicated to use. 
For this biological example, it might be possible to write dillerential equations 
which represented the absorption of poison into the blood stream of the animals 
a nd to represent mechanistically by other equations the effect of the treatments. 
If this could be done, one might run appropriate experiments to test the model 
and to obtain estimates of the basic constants it contained. In the absence of such 
fundamental knowledge, we would have to set our sights lower. We could, for 
example, simply set out to estimate the "effects" of the treatments in terms of the 
increase in survival time they produced. This could be done in terms of a purely 
empirical linear model. 
Specifically, we might postulate that, associated with the tth treatment 
(/ = 1,2, 3,4) and the pth poison (p = 1, 2,3), there was a mean survival time 
BIp" 
If we were principally interested in differences associated with the various 
poison-treatment combinations, we could write the model in the form 
fe y;) = BOXOi + (Bll -
BO)Xlli + (B 12 -
BO)XI2i + ... + (e43 -
e O)X43i + e, 
i = I, ... ,48, 
( 10.2.2) 
where XO i = I and the indicator variable X/pi is 1 if the ith animal received the 
pth poison and the Ith treatment, and zero otherwise. One way of dealing with the 
problem that the resulting 48 x 13 derivative matrix would not be of full rank, 
would be to omit, say, the final term (e 43 -
eo) X 4 3i-
. The resulting empirical 
model which would be of the form of (10.1.1) would contain 12 functionally 
independent parameters. 
In some circumstances it would be reasonable to expect that a simpler 
empirical model containing fewer parameters could be found. 
In particular, it 
might be true, to an adequate approximation, that the effects of poisons and 
treatments were additive. 
Specifically, if et. -
e o was the mean change in 
survival time produced by the tth treatment and e. p - eo the mean change 
produced by the pth poison, then the model could be written 
£(Yi) = eOXOi + (e l . -
fJo)uJj + ... + (e4. -
( 0)U4i + (e' l 
-
eo) \Vii 
(10.2.3) 

10.2 
Analysis of the Biological and the Textile Data 
527 
where Uri is I or 0 depending on whether the ith animal had or did not have the 
tth treatment and Hlpi is 1 or 0 depending on whether the ith animal had the pth 
poison. 
Again we could omit, say, (84-' -
flo) U4-i and (fI'3 -
80) \\' 3i to retain 
an indicator matrix X of full rank. 
We shall call the general model in (10.2.2) the interaction model and the more 
specializeci model in (10.2.3) the additive model. It is often convenient to write 
the more general interaction model in the form of the additivl: model with 
additional terms specifically carrying parameter combinations which measure 
interaction. For the present example, we could write (10.2.2) alternatively as 
3 
2 
E(yJ = eo X Oi + L (8r -
flo) Uri + I (8. p -
flo) W pi 
1=1 
p=l 
3 
2 
+ Z L (flrp -
8t . -
flp + 80) xtpi 
(\0.2.4) 
1= 1 p= 1 
By omitting the last summation containing Lhe six independent interaction 
parameters, we would have the nonsingular form of the additive model of (10.2.3). 
Whereas the interaction model contains twelve functionally independent 
parameters, the additive model contains only six. In the interest of simplicity 
we naturally would wish to use the additive model if this were adequate. While 
the additive model might not be appropriate in the original metric, it might 
become so if a suitable data transformation were employed. 
Representation in terms of the smallest possible number of parameters we 
call "parsimonious parametrization." Clearly if by, say, a power transformation 
from y to /" we could validate a simple additive modeJ, we would have served 
the interest of parsimony. That is, by including one extra parameter A we would 
have made it possible to eliminate six interaction parameters. 
We now consider 
ormalily and constancy of variance. inspection of the 
sampleyariances in the 12 cells of Table 10.2. \ shows that they differ very markedly 
and that they tend Lo increase as the cell mean increases. It might be thal a 
transformation could make it more plausible that population variances were 
equal. Again, while one would not expect on this limited amount of data that 
there would be a great deal of information about its Normality or otherwise, it 
might very well be that some transformation of the data could improve the 
Normality of the distributions of rhe errors. 
10.2.2 The Textile Data 
As a further example. we consider the data of Table 10.2.2. These data came 
originally from an unpublished report to the Technical Committee, International 
Wool Textile Organization, in which Drs. A. Barella and f... Sust described some 
experiments on the behaviour of worsted yarn under cycles of repeated loading. 

528 
Transformation of Data 
10.2 
Table 10.2.2 gives the numbers of cycles to failure, y, obtained in a 33 experiment 
in which the factors were 
~l: length of test specimen (250, 300, 350 mm), 
¢2: amplitude of loading cycle (8, 9, 10 mm), 
~3: load (40, 45, 50 g). 
Table 10.2.2 
Cycles to failure of worsted yarn in a 33 factorial experiment 
Factor levels 
Cycles to failure 
Xl 
X2 
x) 
y 
-1 
-1 
-1 
674 
-1 
-I 
0 
370 
-1 
- 1 
+ 1 
292 
-J 
0 
-I 
338 
-1 
0 
0 
266 
-J 
0 
+1 
210 
-I 
+ 1 
-1 
170 
-1 
+1 
0 
118 
-J 
+1 
+ 1 
90 
0 
-I 
-I 
J ,414 
0 
-I 
0 
1,198 
0 
-1 
+1 
634 
0 
0 
-1 
],022 
0 
0 
0 
620 
0 
0 
+] 
438 
0 
+1 
- I 
442 
0 
+1 
0 
332 
0 
+1 
+ 1 
220 
+1 
-J 
-J 
3,636 
+1 
-I 
0 
3,184 
+1 
-I 
+1 
2,000 
+1 
0 
-1 
1,568 
+1 
0 
0 
],070 
+1 
0 
+1 
566 
+1 
+1 
-1 
],140 
+1 
+1 
0 
884 
+1 
+1 
+J 
360 
x, = (,;, - 300)/50, X2 = ~2 -
9 and X3 = (';3 -- 45)/5. 

10.2 
Analysis of the Biological and the Textile Data 
529 
Although the form E(y) = f(~, 4» 
was unknown, one might hope to 
locally represent this function by expanding it about the average levels 1;0 in a 
Taylor series. We would then have 
3 
3 
3 
E(y) = fo + I Ir«(, -
(,0) + I I Ir/f" -
(,to) «(j -
(,jO) 
t= 1 
,= 1 j= 1 
+ higher-order terms, 
(IO.2.S) 
where 
. 
of I 
./,=-
, 
O(,t 
~=~o 
If it was assumed, for example, that third- and higher-order terms could be 
neglected, then we could employ for the expectation function the second-degree 
polynomial in (,1. (,2. (3 which could be written in the form 
3 
3 
3 
E(y) = eoXo + I e,Xt + I I eljX,j 
(10.2.6) 
t = I 
t=l j=t 
where 
Xo = I, 
C2 = I , 
and 
e, = C'Ir' 
The expectation of the ith observation Yi would then be linear in the 
parameters (eo, el, O2, e3, 8 11 ,8 12 , eD, en· e23 , e33) and depend on the values 
of (XI' X2, X3) as set out in Table 10.2.2. 
It seems highly probable that the full second-degree polynomial model 
containing ten terms could represent the data fairly well. 
However, inspection 
of the data shows that the response appears to be monotonic in Xl, X 2 and X3' 
It is possible, therefore, that after some suitable data transformation a simpler 
model linear in 
1; = «(,1' 
(,2, 
~3 ) omitting the six second-degree terms 
(ell' e22 , 833 , e12 , e13, e23) might be suitable, and improvement in variance 
homogeneity and in 1 Tormality of the error distribution might be achieved also. 
In summary, the task of the data analyst is to make explicit the model 
which underlies a given body of data. J n attempting to relate data to any kind 
of model the analyst runs two kinds of risks. Obviously, he may force the data 
to a model which is inadequate. Alternatively, he may so encumber the model 
with unnecessary parameters that thl.: corresponding analysis is too complex to 
be worthy of the name. A complex analysis is justifiable when the phenomenon 
itself causes the complexity. 
Often complexity is introduced unnecessarily. 

530 
Transformation of Data 
10.2 
For example, we shall show that the textile data are better represented by a 
transformed model linear in the experimental variables ~ and containing only six 
parameters COo, 01, O2 , e), (1, A,) than it is by a model in which an untransformed 
y is represented as a quadratic function of 1, containing eleven parameters 
(0 0 ,0 1, O2 , 03 , ell, 022 , 033 , 8 J2 , 013 , ( 2 ), d· In this si tuation we can say that the 
complexity has not arisen from the physical situation but rather from failure 
to work with the appropriate metric. 
Again, in the biological example, it will 
be shown that the use of a simple transformation avoids the necessity for 
interaction parameters and also for postulating different variances in the groups. 
The analyst's problem is how to allow diversity without producing chaos. 
The principle of parsimony, aptly christened by Tukey (1961), says that 
parameters should be introduced sparingly and in such a way that the maximum 
amount of resolution is achieved for each parameter introduced. Parsimonious 
parametrization is frequently made possible by suitable data transformation. 
10.3 ESTIMATION OF THE TRANSFORMATION 
We shall use the notation y<A) to define a parametric family of nonlinear 
transformations in which A. may be a scalar or a vector containing, elements 
A\, ... , At' We suppose that yCA) is a monotonic function of y over the admissible 
range. 
Two transformations of particular usefulness are y<") = / 
and 
y<~) = (y + ,1,2)"'. 
We use these transformations in the forms 
r-' 
(A '# 0), 
y().)= 
109: 
y>O 
(A = 0), 
(10.3.1) 
r
y + l,)" - 1 
(}'l =I 0), 
y<J.) = 
,1, \ 
y> -
,1,2' 
log (y + A2) 
(,1,1 = 0), 
(10.3.2) 
because they are then continuous at A = 0 and AJ = 0, respectively. This class 
of transformations includes as special cases the logarithmic transformation, the 
reciprocal transformation , and the square root transformation. 
N ow suppose that a suitable family of transformations y<J.) has been selected 
and assume that, for some chosen 1, to a sufficient approximation, 
y(~) = X a + E, 
(10.3.3) 
where yell = (y\l), ... , y:,l»', l = (YI' ... 'YII)' is a n x 1 vector of observations, X 
is a 11 x k matri x of fixed elements having ra nk k the first column of which consists 
of 11 ones, e is a k x I vector of parameters, a nd E is a 17 x I vector of errors having 
spherical Normal distribution NII(O, (J2l). 

10.3 
Estimation of the Transformation 
531 
10.3.1 Prior and Posterior Distributions of 1 
The probability density function of the original untransformed observations y is 
1 
[(yt'.) -
X9), (y(~l - xa)] 
p(y I 1, 9, 0'2) = (2n):" a" exp 
-
20'2 
J(l; y), 
-
00 < y(~l < 00, 
(10.3.4) 
where the Jacobian J(l; y) is 
n 
Idi~ll 
J(l; y) = n -d' . 
,=J 
Yi 
(10.3.5) 
,I 
Writing 
(10.3.6) 
with 
the joint posterior distribution is 
[ 
Sl.. + (a - 91.)' X'X(a - 0,)] 
pee, log 0', j, I y) :x. O'-n exp 
-
20'2 
J(l; y)p(e, log 0', 1), 
-
IX; < log 0' < X, 
-
00 < C < 00 , 
-
00 < 1 < 00, 
where pea, log 0',1) is the prior distribution. 
Choice of the Prior Distribution pea, log 0', 1) 
(10.3.7) 
We now consider what to choose for the pnor pea, log 0', 1). 
Writing 
pen, log 0', 1) = p(l) pee, log 0' I 1), 
(10.3.8) 
we shall suppose that, for any specific j. , p(O, log 0' I j,) is locally uniform. On 
the other hand, p(O, log a 11) must clearly be dependent on 1 for a change in 
1 will magnify or diminish all the data and will hence change the value of this 
locally uniform density. We conclude then that 
pee, log 0' I 1) 
0:. 
g(l). 
(10.3.9) 
To decide the form of the function g(l), we employ the following argument. 
Let us assume that over the range of the data we can write approximately 
i = 1, 2, ... ,n 
(10.3.10) 
where 11 is some represenlative value of the gradient (d)Plidy). 
Taking expectations, we have 
E(y~~» == a). + I). l::(yJ. 
(10.3.11) 
Since 
e = (X'X)-l X' E [y(Al] , 
(10.3.12) 
· --

532 
Transformation of Data 
10.3 
it follows that 
(10.3.13) 
where <l> = (X' X) -1 X' E(y) is a set of linear functions of the expectations of the 
observations y. 
Also, let uy be the standard deviation of Yi-
Then from 
(10.3.10), 
(10.3.14) 
so that 
log U == log 1/ .. 1 + log uy. 
(l0.3.15) 
If, as it seems reasonable, we take <l> and log uy to be locally uniform and inde-
pendent a priori, then (10.3.13) and (10.3.15) imply that 
pee, log u I A) IX. II .. I- k • 
(10.3.16) 
The value we shall use for 1/ .. 1 is the geometric mean of the absolute values of the 
derivatives dy<")/dy at the actual data points, that is, 
1/ .. 1 = n _i_ 
= J(A; y)l /n = J{/JI. 
[ 
n 1 dy<") I] l in 
, = 1 dYi 
(10.3.17) 
Thus, 
pea, log u I A) IX. g(A) IX. J;:k/JI 
(10.3.18) 
and finally 
pee, log (J, I,) IX. J;: ki ll pCA). 
(10.3.19) 
Posterior Distribution of A 
With this prior distribution, the joint posterior distribution for 9, log (J, and 
A is 
[ 
S .. + (9 - 9,.)' X' X(S - 9 .. )] 
pee, log u, A I y) IX. (J-n exp 
-
2(J2 
J ~n-k) / n p(A.), 
-
CIJ < log (J < 00, 
-
00 < 9 < 00, 
-
00 < A. < 00, 
(10.3.20)/ 
where it should be noted that J). involves the observations but not the 
parameters Sand (J. 
Integrating out S and log (J, we obtain 
p(A. I y) IX. Pu(A. I y) p('A-), 
-
00 < A. < 00, 
(10.3.21) 
where 
pl/(A.I y) IX. (s).. fJi /n)-+(II-k) 
is the posterior distribution for a uniform reference prior for 'A-. 
Alternatively, 
if we work with the "normalized" data 
(10.3.22) 
then, writing ~( ). ) = X(X'X)-IX'Z()') where z().) = ( z ~1.. ) , ... , z ~).»', we have 
PII(/,I y)IX. [S(A., z)r:·(n-k), 
-
00 < A. < 00, 
(10.3.23) 
with 

. 
. 
10.3 
Estimation of the Transformation 
533 
In practice, in choosing a transformation that may hopefully result in 
parsimonious parametrization, constancy of variance, and Normality, we can 
compute the residual sum of squares S(A., z) for a suitable range of values of 'A, 
and hence, [S('A,z)]-Hn-k). 
Normalization of 
[S('A,z)r~(n-k), will then 
produce puCll y). If desired, Pu(ll y) may then be combined with any prior weight 
function p('A.). 
It should be noted that by regarding J). as a constant, (l0.3.22) implies that the 
logarithm of the standard deviation of z()') can be written 
1 
log O"(z(l.» == - -log J).. + log 0-. 
n 
Using the approximation (10.3.15) with 1/)..1 = J~ /n, we have 
" 
It follows that 
I 
logO" == -logl .. + logo-yo 
n 
(10.3.24) 
(10.3.25) 
(10.3 .26) 
so that, to the degree of approximation (10.3.15), the standard deviation of z()") is 
the same for any l. 
10.3.2 The Simple Power Transformation 
In practice, the power transformation in (10.3.1) is of particular interest. 
In 
the z form we have A. = 1, 
{
(/ - 1)/(lj/-l), 
zP.) = 
y logy, 
(1 oF 0) 
(A = 0) 
where y = (f17= 1 yyin is the geometric mean of the data. 
(10.3.27) 
In obtaining the posterior distribution of A, the chief labor is in calculating 
the residual sum of squares S(2, z) for a range of values of 1. 
Computer 
programs are now rather generally available for analyzing linear models and in 
particular in producing, via an analysis of variance table, the residual sum 
of squares. 
Writing w = y/y, we have for the power transformations 
(1 oF 0) 
(J, = 0) 
(10.3.28) 
where C;, is a constant depending on A. Now the size of the constant c). will have 
no effect on any element in the analysis of variance table except the "correction 
for the mean." Consequently, when the simple power transformations are being 
. 
. 
'.:.----

534 
Transformation of Data 
10.3 
considered, S(I" z) may be obtained simply by computing IVi = Yi/'y, II = 1,2, 
... , nand pe:forming an analysis of variance on the variates 
[ y ), -1 W)' , 
ty log w, 
(note that y J... - 1 wi. = Y when J~ = 1). 
(J, =I 0) 
(). = 0) 
(10.3.29) 
, 
So far as the calculation of puCJ...1 .1) is concerned, we can work equally well 
with the variates J... -1 w.\ and log w, since the fixed multiplication constant y will 
have no effect on PuCAI y), 
which is in any case normalized so that 
SPu(J...1 y)dJ... = I. 
10.3.3 The Biological Example 
For the biological data in Table 10.2.1 we would like, if possible, to find a trans-
formation for which: 
a) the "six parameter" additive model rather than the "twelve parameter" inter-
action model is applicable. 
b) the cell variances are equal and 
c) the errors are Normal. 
These properties seem unlikely to apply for the original data. In particular 
simple plotting demonstrates a strong tendency for the within-cell variances 1.0 
increase as the within-cell means increase. 
Furthermore, the analysis of 
variance in Table 10.3.1 for the untransformed data suggests the possibility of 
interaction. 
Interpreting this table from the Bayes point of view discussed in 
Section 2.7, we can say that in the space of the six interaction parameters the 
point (0,0,0,0,0,0) barely lies within the 90 per cent H.P. D. region. 
(The 
appropriate mean square ratio is 41.7/22.2 = 1.88 while the 10 per cent point f6r 
F with 6 and 36 degrees of freedom is 1.95.) 
Poisons 
Treatments 
PxT 
'within groups 
Table 10.3.1 
Analyses of variance of the biological data 
Mean squares x 1000 
Degrees 
Degrees 
of 
Untransformed 
of 
freedom 
freedom 
2 
516.3 
2 
3 
307.1 
3 
6 
41.7 
6 
36 
22.2 
36(35) 
Reciprocal 
transformation 
(z form) 
568.7 
221.9 
8.5 
7.8(8.0) 

.. 0.3 
Estimation of the TransforAl31 ion 
535 
Since it is hoped that after transformation the additive model will be adequate. 
the appropriate residual sum of squares to use in calculating Pu(}' I y) is that 
obtained after Rtting the additive model and is based on 42 degrees of freedom. 
We denote it by S42(...1., 2). 
Table 10.3.2 
Values of S42(J., ,,) and of Pu(}' I y) over a range of A where the density is appreciable: 
the biological data 
}, 
S42(}c, z) 
}, 
Pu(}'1 y) 
1.0 
1.0509 
0.0 
0.01 
0.5 
0.6345 
-0.1 
0.02 
0.0 
0.4239 
-0.2 
0.08 
-0.2 
0.3752 
-0.3 
0.26 
-0.4 
0.3431 
-0.4 
0.49 
-0.6 
0.3258 
-0.5 
0.94 
) 
-0.8 
0.3225 
-0.6 
1.46 
-1.0 
0.3331 
-0.7 
1.82 
-1.2 
0.3586 
-0.8 
1.82 
-1.4 
0.4007 
-0.9 
1.42 
-1.6 
0.4625 
-i.O 
0.92 
-2.0 
0.6639 
-1.1 
0.47 
-2.5 
1.1331 
-1.2 
0.19 
-3.0 
2.0489 
-1.3 
0.07 
-1.5 
0.01 
Table 10.3.2 shows values of S42(A, z), and of the resulting Pu(J.1 y), 
over a range of }, in which the density is appreciable. 
Upon normalizing 
[S42 (A, z)r 21 by numerical integration, we find 
-
00 < A < 00. 
(l0.3.30) 
The posterior distribution PI/A. I y) shown in Fig. 10.3.1 is approximately Normal 
with mean -
0.75 and star>dard deviation 0.22. The 95 per cent H .P.D. interval 
extends from about -
1.18 to about - 0.32. 
We notice in particular that 
A. = 1 (no transformation), }, = 0 (log transformation), are untenable on this 
data. On the other hand, the reciprocal y-l, which has a natural appeal for the 
analysis of mortality time data because it can be interpreted as · the "rate of 
dying", possesses most of the advantagcs obtainable from transformation. 
A 
plot of within-cell 
sample variances against 
within-cell 
means for 
the 
transformed data now shows no dependence. 
Furthermore there is now no 
suspicion of Jack of additivity. 
The analysis of variance table for the untransformed data and for the 
reciprocal transformation (in the z 
fo~m) is shown in TabJe 10.3.1. 
The 

-
. 
. 
-
'. 
. 
. 
'. ., 
. 
• 
., 
6 
,'/ -
536 
Transformation of Data 
10.3 
-) 
0 
Fig. 10.3.1 The posterior distribution p,.(AI y) for the biological data. (Arrows show 
approximate 95 per cent H.P.D. interval) 
Normal assumptions are, of course, much more appropriate for the transformed 
data and this results in a much greater sensitivity of the analysis. The within-groups 
mean square is reduced to about a third of its previous value relative to the poison 
and treatment mean squares which remain about the same size. This implies, for 
example, that the spread of the individual marginal posterior distributions of the 
effects will be reduced by a factor of about ) 3 when transformed back to the 
original metric. 
10.3.4 The Textile Example 
Consider now the textile data of Table 10.2.2. 
In the original analysis, a 
second-degree polynomial in x I, X2 and X3 was employed to represent E(y). 
However, since inspection of the data suggests that y is monotonic in Xl, X2 
and x 3 , there is a possibility that a first-degree polynomial might provide 
adequate representation for E(l), with A suitably chosen. 
We therefore work with the transformed variate (10.3.27), hoping that after 
such transformation : 
a) the expected value of the transformed response can be adequately represented 
by a model linear in the x's, 
b) the error variance will be constant, and 
c) the observations will be Normally distributed. 
The appropriate residual sum of squares to be considered is that after 
fitting a linear function to the x's. The sum of squares has (27 - 4) = 23 degrees 

to.3 
Estimation of the Transformation 
537 
Table 10.3.3 
Values of SH(J" z) and of Pu(}"1 y) over a range of A. where the density is appreciable: 
the textile data 
}, 
S23(}" z) 
A. 
p,.(J, I y) 
1.00 
5.48 JO 
0.20 
0.02 
0.80 
2.9978 
0.15 
0.09 
0.60 
1.5968 
0.10 
0.42 
0.40 
0.8178 
0.05 
1.58 
0.20 
0.41] 5 
0.00 
4.18 
0.00 
0.2519 
-0.05 
5.64 
-0.20 
0.2920 
-0.10 
4.66 
-0.40 
0.5378 
-0.15 
2.36 
-0.60 
J. 1 035 
~0.20 
0.77 
-0.80 
2.1396 
-0.25 
0.19 
-1.00 
3.9955 
-0.30 
0.04 
-0.35 
0.01 
of freedom. 
Table 10.3.3 shows the value of S23(A., z) and of the resulting 
PuCA.1 y) over a range of }, in which the density is appreciable. 
Upon 
normalizing [S23 (A., z)r 11.5 by numerical integration, we find 
-
00 < A < 00. 
(10.3.31) 
The distribution PIP I y) is plotted in fig. 10.3.2. It has its mean at -
0.06 
and the 95 per cent H.P.D. region extends from -
0.20 to 0.08, determining the 
appropriate transformation very closely. 
The log transformation U = 0) has 
considerable practical advantages for this example and is strongly supported 
by the data. The analysis of variance for the untransformed data and for the 
logarithmically transformed data taken in the z form is shown in Table 10.3.4. 
Linear 
Quadratic 
Residual 
Table 10.3.4 
Analyses of variance of the textile data 
Mean squares x 1000 
Degrees 
Degrees 
Logarithmic 
of 
Untransformed 
of 
transformation 
freedom 
freedom 
(z form) 
3 
4,9J6.2 
3 
2,374.4 
6 
704.1 
6 
8.1 
17 
73.9 
17(16) 
11.9(12.6) 
-
'. I 
. 
'. 
"::-
. 
--~ 

538 
Transformation of Data 
10.4 
[t is seen that transformation eliminates the need for second-order terms in the 
equation, while the use of the more appropriate model greatly increases the sensi-
tivity of the analysis. 
puC)" 1 y) 
4 
2 
-
95';" 
L-_
_
_ 
"-'---'--'-"'-___ 
~).. -> Box Be Tieo 
--J 
o 
10.3-2 
Fig. 10.3.2 The posterior distribution P,P I y) for the textile data. (Arrows show approxi-
mate 95 per cent H.P.D. interval) 
10.3.5 Two Parameter Transformation 
To illustrate the estimation of a ~wo-parameter transformation , we apply to the 
textile data the transformation (10.3.2), which, writing gm for "geometric mean", 
can be written in the z form (10.3.22) as 
(10.3.32) . 
where 
Figure 10.3.3 shows the contours of the joint posterior distribution 
PuP. I , ,{2 I y) obtained from a 7 x II grid of values of 
)Ogp),{l, ,{2 1 y) = const -
(11.5) log SUI' ,{2, z). 
(10.3.33) 

. 
. 
'": .. 
10.4 
0.200 
0.100 
o 
0.2 
o 
Analysis of the Effects e After Transformation 
-0.2 
-OA 
- 0.6 
A\ .... 
-0.8 
539 
Fig. 10.3.3 Contours of the posterior distribution PI/C)'l' A2 I y) for the transformation 
(y + ,.1.2»).': the textile data. (Contours labeled enclose approximate H.P.D. regions) 
As before, if the k-dimensional posterior distribution p(A I y) were multivariate 
Normal, then a (I -
Ct.) H.P.D. region for parameters A would be defined by 
logp(J" I y) - 10gp(A I y) < ~ . X2(k, Ct.), 
(10 .3.34) 
Although, for this example, the contours show that the Normal approximation is 
likely to be a poor one, we have nevertheless as a rough guide labeled H.P.D. 
regions with per cent probabilities using (JO.3.34). It is evident that, for this 
example, there is likely to be no particular advantage in including a nonzero 
value for A2' 
10.4 ANALYSIS OF THE EFFECTS 0 AFTER TRANSFORMATION 
So far we have considered only the marginal posterior distribution peA I y). 
Although there will be some circumstances in which inferences about A will be 
the major objective, in many cases the chief interest of the analysis would be in 
making inferences about 9. 
The status of A will then be that of a (very 
valuable) vector of nuisance parameters. 
As usual, we can eliminate A by integrating pee I A, y) with weight function 
peA I y) in accordance with 
p(9 I y) = Sp(9, A I y)dA = Jp(S i A, y) peA I y)dA. 
(lOA.I) 
Again in the spirit of Section 1.6, we should be cautious in practical problems 
of relying solely on this integration, for if p(SI A, y) were changing drastically 
over the range in which peA I y) was appreciable, it would be important to know 
about it. 
Thus, in principle, we should always make a study of p(O I A, y) for 
a number of values of A which are of interest. 
-- -_. 

"\. 
f, 
-
, 
~' 
. 
:,-
~~.'., 
...... 
. ... ·r·-
540 
Transformation of Data 
10.4 
From the distribution in (10.3.20) with p()..) assumed uniform, the joint 
distribution p(9, ).1 y) in z(l·) form is 
p(9,).1 y) oc J;:k/n [S()', z) + Q(9, ).)rn/2, 
-
00 < 9 < 00, 
-
00 < ). < 00, 
where 
Q(9, ).) = J;:2 /" (9 - 0..)' X'X(9 - 9i.)' 
Thus, the conditional distribution p(91)., y) is 
[ 
Q(9, ).)] -Hv + k) 
p(91 )., y) oc 
1 + s; ().) \' 
' 
-
00 < 9 < 00, 
where 
y = n - k, 
ys;().) = S()', z), 
which is the k-dimensional multivariate t distribution 
lk [Oi. , s;()')J~ /n (X'X)-l, v], 
and the marginal distribution of 9 is 
-
00 < 9 < 00. 
(10.4.2) 
(I0.4,3) 
(10.4.4) 
From (10.3.23), the mode i of the marginal distribution of ). is the value 
minimizing S()', z). We may employ Taylor's theorem to write 
S()', z) + Q(9,).) == S(5.., z) + (). -
5..) 'G()' -
5..) + Q(9, i)+ g(9, ).), 
(10.4.5) 
where 2G is the matrix of the second derivatives of SeA, z) with respect to ). 
evaluated at 5.. and 
(10.4.6) 
For moderate n, the influence of g(9, ).) will be small and can be ignored . 
Expression (10.4.4) can thus be approximated to yield 
p(91 y) d:. [S()', z) + Q(9, ).)] - Hv- ,+k) E(1;:k/n), 
(10.4.7) 
i. 
where the expectation is taken over the distribution 
[ 
(). - i)' G()' - i)] - in 
Pl()..) oc 
1 + sCi, z) + Q(9, i) 
, 
-
00 < ). < 00. 
(10.4.8) 
Now for moderate n, 
E (1;:k/n) == Jt/" 
i. 
(10.4,9) 

- .'"" .~ 
. 
. 
, 
. 
':, 
.-
-
--------.....-
10.4 
Analysis of the Effects e After Transformation 
541 
which does not involve 9, so that finally we can express the marginal 
distribution of 6 approximately as 
pee j y) eX [I + 
Q(e, }.) 
] - i( v - r+k ) 
S;-r ()..) (v - r) 
, 
-
00 < 9 < 00, 
(10.4.10) 
where 
which is the k-dimensional multivariate t distribution 
tk COl:, s;_ r ()..) Ji'" (X'X) - \ v - r]. 
Thus, approximately the marginal posterior distribution of e is obtained by 
analysing z<):) as if).. were a known fixed parameter and reducing the residual 
degrees of freedom by r, the number of transformation parameters. 
Having obtained 5., we can thus approximately justify performing a standard 
analysis with the transformed variate z<D as if the transformation had been given 
in advance. The only modification will be the reduction of the residual degrees 
of freedom . 
In the two examples, we have suggested that detailed analysis for 9 might be 
carried through not for 50 (which was - 0.75 for the biological sample and 
- 0.06 for the textile sample), but we might use the reciprocal transformation 
U = -
I) in the first case and the logarithmic transformation U = 0) in the 
second. Two arguments can be used to justify this. 
a) In the neighborhood of X,p(9 j )" y) changes only slowly in ;, so that there 
is little practical difference between using pee j X, y) which, after appropriate 
modification in the degrees of freedom, approximates (J 0.4.1) and in using 
pee j j' D, y), where ..1.0 is -
I and 0, respectively, for the two examples. 
b) For the biological example, the reciprocal transformation, which says that the 
rate of dying of the animals is additive so far as the effects of poisons and 
treatments are concerned, is one which makes some biological sense. 
Also, in many fields of technology, relationships often occLir of the form 
y = const. ~~l (g" ... , ~ ek e, suggesting a logarithmic transformation of both the 
dependent and the independent variables. 
Now, in the textile example, the 
independent variables (1' ~ 2 ' ... , ~k have been changed over such narrow ranges 
relative to their means that there the log transformations would be approximately 
linear and could be omitted. On this argument, a logarithmic transformation of 
the response y alone is a natural one to employ for the textile example. 
Thus, the values A = -
I for the first example and ), = 0 for the second 
were choices in which there was in fact reasonably stronger prior belief. :--Jow 
a reasonably strong prior distribution centered at -
I, for the first example, 
and 0 for the second would cause the posterior distribution p(lj y) to be even more 

, 
. 
. 
.~ 
• 
. 
-l .. -
'. 
-
10' ,,-' 
• 
542 
Transformation of Data 
!O.5 
sharply peaked and when substituted in (10.4.1) would justify approximately 
an analysis in terms of reciprocal and logarithmic transformations respectively. 
Finally, then, in the actual analysis of these examples, an appropriale 
transformation }'o close to ;: was made and a standard analysis performed with the 
transformed variate y;'u, but with t~e number of residual degrees of freedom 
reduced by one. 
From a Bayesian viewpoint, this analysis rests on the 
approximate use of the multivariate ( distribution p(S I )0' y) with reduced 
degrees of freedom to represent p(e ! y). 
Bayesian justifi.cation of the analysis of variance table and calculation of the 
posterior distributions of particular subsets, differences, and other linear functions 
of the elements of 9 only make use of particular aspects of this basic multivariate 
t distribution. 
The bracketed items in Tables 10.3.1 and 10.3.4 show the modifications to the 
degrees of freedom and mean squares which would be appropriate in the 
analysis of variance table on the above argument. 
10.5 FURTHER Al':ALYSIS OF THE BIOLOGICAL DATA 
It is an inherent assumption in the foregoing analysis that a transformation 
exists which simultaneously achieves simplicity of the model, homogeneity of 
variance, and ~ormality . 
This is clearly a much less restricted assumption than 
the more usual one of supposing that these requirements are all met lI'ithoUI 
transformation. However, it is of interest to perform further analysis which in 
some way separates the issues of 
a) simplicity of the linear model, 
b) homogeneity of variance, 
c) Normality 
and which makes it possiblt: to see to what extent these may be achieved with the 
same transformation. 
Consider again the biological data. We have in our previous analysis tacitly 
supposed that there existed a transformed variable )'eA) in (10.3.1) for which 
Simultaneously 
a) y~.l) had a Normal distribution N{t:[ y~i)], a} ], 
b) (J; = (J2 , and 
c) ELJ/)] was adequately represented by addiTive row and column parameters, 
i = I, .. . ,n. 
Further light is shed on the situation by supposing first that ), was 
chosen to satisfy (a) only, then considering how the situation is changed by adding 
requirement (b), and fi.nally considering the effect of adding requirement (c). 
. 
. 

-
, . 
. 
. 
. --
-
10.5 
Further Analysis of the liQlogical Dllt:'\ 
543 
If we merely suppose that a rransformation yU ) of the form (10.3.1) exists which 
simultaneously induces Normality in a11 cells but not necessarily constancy of 
cell variances additivity, then , for each cell. we have 
• . (!.) = f) 1 + E · 
J) 
j 
J' 
j = 1, ... , k, 
(10.5.1) 
where for the jth cell corresponding to the Ith treatment and pih pOlson, 
Ej is spherical ~\lormal /,'n; (0, (J7 I) (for this example, k = 12,l1j = 4). Following 
the argument in Seclion 10.3.1 , we have the prior distribution 
p(B, log u, },) cc r;k,,, peA), 
(10.5.2) 
where n 
J i. = II I 
dy~ i.) [ . . 
i = ! 
dYi 
Writing p"CA I N) to indicate the posterior distribution under Normality (N) 
only and with p(},) assumed locally uniform, it can be readily be shown that 
k 
p"V, I N) cc n [Sv; ()" z)r : vi, 
-
00 < A < co, 
(10.5.3) 
j= ! 
where SVj(J., z) is the sum of squares of deviations from the cell mean in 
terms of z(,i) for the jth cell having v j = 
17 j -
1 degrees of freedom (v j = 3 for the 
present example). The ordinates of p" (A I N ) are shown in the fourth column 
of Table 10.5.1 and the distribution is plotted in Fig. 10.5.1. 
.- 3 
- 2 
- I 
o 
2 
3 
4 
Fig. 10.5.1 Posterior distributions of ), for different models: the biological data. 

-
544 
Transformation of Data 
10.5 
It is well known that small samples are not able to tell us much about the 
Normality or otherwise of a distribution so that it is scarcely surprising that 
Pu (A I N) is found to cover an extremely wide range of A. 
Table to.S.1 
Ordinates of posterior distribution of A for different models: the biological data 
A 
PIIUI A,H,N) 
p,/), I H, N) 
PuO< IN) 
1.0 
0.335 
0.5 
0.006 
0.398 
0.0 
0.006 
0.021 
0.342 
-0.1 
0.023 
0.055 
0.324 
-0.2 
0.076 
0.127 
0.304 
-0.3 
0.257 
0.261 
0.283 
-0.4 
0.492 
0.471 
0.261 
-0.5 
0.942 
0.754 
0.240 
-0.6 
1.462 
1.059 
0.218 
-0.7 
1.823 
1.320 
0.196 
-0.8 
1.823 
1.430 
0.173 
-0.9 
1.419 
1.360 
0.153 
-l.0 
0.923 
1.136 
0.134 
-/,1 
0.468 
0.850 
0.116 
-1.2 
0.194 
0.558 
0.099 
-1.3 
0.067 
0.329 
0.083 
-1.4 
0.019 
0.170 
0.069 
-1.5 
0.005 
0.078 
0.058 
- 1.6 
0.00] 
0.032 
0.050 
-1.7 
0.009 
We now consider the effect of assuming that a transformation yU) exists 
in terms of which not only Normality but also constant variance is obtained (but 
not necessarily additivity). 
Using the model in (10.5.1) but now with 
O'~ = O'i = ... = (J~, the within-cells sum of square 
S,V Cl, z) = L}: 1 SVj (J., z) 
is pooled from the k = 12 groups of four an imals and has II ", = L~~ 1 Vj = 36 
degrees of freedom. The resulting posterior distribution of A, 
PII CA I H, N) <X. [Sw (J., z)] -
j-,w, 
(10.5.4) 
shown in Fig. 10.5. I, is now very much sharper. 
Finally, if we assume that additivity can also be achieved by transformation, 
we have the probability distribution PII V I A, H, 1\') already given in Fig. 10.3.1 
and also shown in Fig. 10.5.1 which is even sharper than Pu(J. I H, N). 
. 
. 
. 
. 
. . 
. 

10.5 
Further Analysis of the Biological Data 
545 
If we denote by SI (A, z) the sum of squares for interaction which has 
VI = 6 degrees of freedom, then the residual sum of squares SeA, z) originally 
considered is, in our new notation S (J" z) = S'" CA, z) + SI CA, z) and 
-
co < A < 00 . 
(10.5.5) 
Since in each case we obtain the posterior distribution of A conditional on 
the truth of a model containing given restrictions, we need some practical 
answer to the question of whether a constraint is justified or not. 
10.5.1 Successive Constraints 
Let C denote a particular constraint. Then, as we have seen earlier in C 1.5. 5), we 
can write 
(A 1 C) = (A) x p( C 1 },) 
p 
p 
p(C) 
, 
where p(C) = E [p(C 1 A)J is a constant independent of L 
). 
(10.5.6 ) 
We apply this to the present example with Pu CA 1 N) the " unconstrained " 
density for which it is only assumed that Normality can be achieved for some }" 
and with Pu(J, 1 H , N) the " constrained" density for which it is assumed that 
homogeneity of variance can also be achieved . 
With the dependence on 
Normality (N) understood, we can write 
p" (A 1 H) = Pu U) p" (H 1 A) oc Pu CA) Pu (H 1 A). 
Pu (H) 
(10.5.7) 
Thus, the distribution p"V, 1 H) is decomposed into two factors both of which 
are functions of A: the unconstrained "prior" distribution PII (A) and the 
"likelihood" factor Pu (H 1 A) representing the effect of the constraint H. Now, 
we can obtain Pu (H 1 A) from the relationship 
Pu (H 1 A) oc P"p~\IA~) oc [WH CA, Z)J 1/2, 
where from (10.5.3) and (10.5.4), 
In other words, 
WH (A, z) = Ii [SvP" z)/VjJ:~ 
j= 1 [Sw CA, z)/v",J 
Pu (,.1·1 O'i = ... = aD oc Pu CA) PII (d = .. . = 0'11 A) 
where 
(10.5.8) 
(10.5.9) 
(10.5.10) 
(10.5.11) 
- -... 
--
.. 

546 
Transformation of Data 
M 
30 
20 
-
10 
o 
ITX. z) 
2 
4 
F 
5 
4 
-- 3 
Fig. 10.5.2 Values of M(A, z) and F(l, z) as functions of A: the biological data. 
10.5 
Now recalling our previous discussion of the comparison of variances In 
Section 2.12, if we consider the k -
1 linear contrasts in log ai, <Pi = log a, - log O'k, 
the second factor on the right of (10.5.10) is proportional to the probability 
density for cPo=O conditional on the choice ofa particular value for A. Light may 
be shed, therefore, on the question of whether for a particular value of A the 
constraint H is acceptable or not by considering whether the point <1>0 = 0 falls 
in or outside an H.P.D. region which includes some reasonably large proportion 
1 -
IX of the posterior distribu tion. As we have already seen in (2.12.13) and 
(2.12.21), this can be done by referring 
M(A, z) = -
210g WH (J., z) 
to 
{
I l 
k, 1 
1], } 2 
1+ 
_ 
.L-- -
l.. (k-I,IX). 
3(k 
I) 
} ~ lVj 
Vw 
Thus, in the biological problem, for a particular ;., the point <1>0 = 0 would lie 
outside a 95 per cent H.P.D. region if M(l, z) > 21.8. The values for M(A, z) 
for various values of A. are plotted in Fig. 10.5.2. 
From this we see that the 
constraint O'i = O'i = .. . = O'~ is, in fact, compatible with values of A in a range 
which includes, for example, the reciprocal transformation. 
We can now proceed in a similar way to determine the appropriateness of the 
further constraint of additivity. With dependence on Normality (N) understood, 
we can write 
Pu(A I H,2) 
Pu(J,IA,H)=PII(AIH) 
( 
) 
cc Pu(..l. I H)PII(AIH, A). 
Pu AI H 
(l0.5.12) 

10.5 
Further Analysis of the I3iological Data 
547 
The "likelihood" factor Pu (A I H, A) can be obtained from 
p" (J, I A, H) 
. 
12 
p,,(AIH,A)CC p"(},IH) 
X [WAV,z)] , , 
(10.5,13) 
where, from (10.5.4) and (10.5 .5), 
W A Z = 
[Sw (A, z)r"' 
A ( ,) 
[Sw ()" z) + SI (J" Z)]"whl . 
(10.5.14) 
Writing 81 to denote the 1 interaction parameters, we thus have 
p" (A I 8[ = 0, O'i = ... = O'~) cc p" (A I (J~ = . .. = lJ~ )Pu (91 = 0 I O'f = .. , = a~, A) 
(10.5.15) 
where the second factor on the right 
p" (8[ = 0 IlJ~ = .. . = 
lJ~, A) cc [WA (}.., z)]l il 
(10.5.16) 
is the density of 9f at 91 = 0, conditional on the choice of ), and given that the cell 
variances are constant. 
From (10.4.3), it is readily seen that 8[ is distributed a posteriori as 
I[ [9[ (A), s~ (A) Jr" C, vw ] where 91 (J,) is the I x I sub-vector of OJ. corresponding 
to 9[, v",s~ (A) = S", (A, z) and C is proportional to the covariance matrix of 
81, The interaction sum of square S[ (A, z) is in fact 
(10.5.17) 
and, with the dependence on A and z understood, expression (10.5.16) can be 
written 
(10.5.18) 
The question of whether for a particular value of A the constraint A is 
acceptable or not may be resolved by considering whether the parameter point 
9[ = 0 falls in or outside an appropriate H.P.D. region . Since Sf follows a multi-
variate I distribution, the question is answered by referring the quantity 
F(A,Z) = 
(J-2 flJo~C-lef)(VIS\:) 
S,.JVw 
(10.5.19) 
Sd vr 
=--
to an F table with V1 and Vw degrees of freedom . For the present example, F has 
6 and 36 degrees of freedom. Thus, the point 01 = 0 will lie outside a 95 per cent 
H.P.D. region if F(}" z) > 2.36. The plot of F P" z) as a function of A given in 
Fig. 10.5.2 shows that the further constraint is acceptable over a fairly wide 
range which includes the interesting region close to the reciprocal transformation. 

548 
Transformation of Data 
10.6 
10.5.2 Summary of Analysis 
Finally, then, the overall posterior distribution PII U I A, H, N) shown in Fig. 
10.3.1 can be decomposed into three functions, 
Pu (). I A, H, N) cc Pu (A I N)plI (H I N, A)PII (A I H, N, A). 
(l0.S .20) 
The first factor is the posterior distribution of )" 
supposing only that the 
transformation yU) exists which induces Normality simultaneously in all the cells. 
Its product with a second factor, measuring the probability density associated 
with variance homogeneity for each value of A, is proportional to PII(A I H, N), 
the posterior distribution of }" supposing a transformation yU) exists which induces 
both Normality and homogeneity of variance. Finally, the product with a third 
factor, measuring the probability density associated with zero interaction for each 
value of A, is proportional to PI/(A I A, H, N), the posterior distribution supposing 
that a transformation i l ) exists which induces Normality, homogeneity of variance, 
and additivity. The plausibility of the constraints of homogeneity of variance and 
of additivity can be assessed for every contemplated }. by considering whether the 
constraint defines a parameter point inside or outside a suitable H.P.D. region. 
For the present data the constraints appear justified. 
However, situations 
commonly occur where, for example, nonadditivity is not reduceable by a given 
family of transformations. One would be warned of this possibility if the F plot 
of Fig. 10.5.2 indicated that for no value of A was the point Sf = (} included in, 
say, a 95 per cent H .P.D . region . Similarly, situations are common in which 
variance homogeneity cannot be induced by transformation . 
This possibility 
would be assessed by the .\1 plot of Fig. 10.5.2. 
10.6 FURTHER ANALYSIS OF THE TEXTILE DATA 
A similar decomposition is possible for the textile data analysis. In our original 
analysis, it was assumed that a transformation yO) was possible which induced 
linearity in the response surface as Ire/! as spherical ;-';ormality. It could be true, 
of course, that no such transformation was possible, in which case we could fall 
back on the possibility of inducing spherical Normality with the original 
quadratic model. 
The introduction of additivity which we have previously 
discussed is a special case of the induction of parsimony in the parametrization 
of the linear model. In general we have 
(10.6.1) 
in which O2 are the parameters which hopefully may not be needed. 
In the particular example of the textile data, the elements of 92 are the 
coefficients uf the second-degree terms. Wc have 
(10.6.2) 

10.6 
Further Analysis of the Textile Data 
549 
6 
4 
3 
-I 
o 
Fig. 10.6.1 Posterior distributions of}. for different models: the textile data. 
The "constrained" and "unconstrained" probability density of 
.A. 
for the 
textile data are shown in Fig. 10.6.1. 
Writing (SI> S2, SR' VI, v2 , vR ) for the 
sums of squares and degrees of freedom associated with linear terms, quadratic 
terms, and residuals, we have 
S"£< '2 
(S 
S )-<v" ':' '',)/2 = S-vR/2 
R 
R + 
2 
R 
(SR +-S-2-)<:-\·-"-:-+-,,,-;-),"""2 . 
(10.6.3) 
We find by the previous argument that the second factor on the right is 
proportional to the ordinate at 9 2 = 0 of the multivariate t distribution for 92 , 
The appropriateness of the constraint 9 2 = 0 for any value of ), may thus be 
judged by considering whether O2 = 0 is included in an appropriate (I - a) 
H.P. D. region, which in turn is equivalent to referring 
to the 
C( significance point of the F-distribution with V2 and 
VR degrees of 
freedom. A plot of F(.A., z) in Fig. 10.6.2 shows that, in this case, for values of ), 
in the region around }, = 0, there is no reason to question the applicability of the 
constraint that the surface in the transformed response could be planar. 

550 
Transformation of Data 
F 
" \ 
10 
8 
6 --
4 
\ 
\ 
\ 
\ 
\ \ 
/~ 
\ 
/ 
\ 
I 
\ 
I 
\ 
" 
\ 
I 
\ 
I 
\ 
" 
\ 
/F(A . Z) 
\ 
I 
\ 
I 
\ 
I 
\ 
I 
\ 
I 
\ 
I 
\ 
J---F= 270 
\ 
I 
\ 
I 
\ 
I 
\ 
I 
\, .. / 
~----------~----------~
I 
A~ 
- · 1 
0 
I 
Fig. 10.6.2 Values of FO" z) as a function of J.: the textile data. 
10.7 
10.7 A SC\1MARY OF FOR\1ljLAE FOR VARIOUS PRIOR AND POSTERIOR 
DISTRIBUTIONS 
In this chapter the frequently made assumption that the Normal linear model is 
adequate in the original metric is relaxed , and instead it is assumed only that there 
exists some transformatio/1 /A) of the observation y for which the linear model is 
appropriate. Specifically, the model is given in (10.3.3) as 
yeA) = XO + t . 
where y(l.) = (i/'>, ... , J~l.»)', y = (y" ... , Yn)' is a n x I vector of observations, 
X is a n x k matrix of fixed elements, e is a k x I vector of parameters and t is a 
/1 x I vector of errors distributed as 
,,(0, cr 21). 
Table 10.7.1 provides a short 
ummary of the formulae for the prior and posterior distributions of (9, cr 2,A.). 
Table 10.7.1 
A summary of various prior and posterior distributions 
1. The prior is , from (1 0.3.8) to (10.3.19), 
p(9, log cr, \) = p(J,)p(O, log cr I I.), 
where pO-) is some prior for '- and 

10.7 
A Summary of Formulae for Various Prior and Post(!rior Distributions 
55l 
Table 10.7.1 COnJinued 
2. <onditional on I., the posterior distribution of (e, Jog (T) is, from (10.3.20), 
" 1/ 
[Sl.+ (e-o,yx'X(O-Ol.)] 
p et}, log (T I )., y) ex. (T 
exp 
-
2 
' 
2(T 
-
G0 < 0 < 00, 
(T > 0, 
where 
and 
3. Working with the normalized data 
the posterior distribution of J... with pO.) a uniform reference prior is in (10.3.23), 
-
00 < ). < 00, 
where 
and 
4. Tn particular, if J... = ), and for the simple power transformation in (10.3.1), 
{ 
(/ - 1)' 1. 
(J, # 0) 
y<).)= 
y>O, 
log y 
(). =0) 
then, from (10.3.27) 
{
(/'-1)/U/- 1) 
z().) = 
y logy 
where 
For the two-parameter transformation in (10.3.2), 
(1.1 #0) 
U1 =0) 
then , from (10.3.32) 
{
[(y +}'2)'1 -1]f{J.I [gm (y+ J.2)])·,-I} 
z(l.) = 
gm (y+ J 2) log (Y+}'2) 
where 

552 
Transformation of Data 
10.7 
Table 10.7.1 Continued 
5. The posterior distribution of 8 with p(1.) a uniform reference prior is in (10.4.4), 
p(8 I y) oc J 
-00 < ~ < 00 J;k'''[S(A, z) + Q(8, A)r"/2 d)" 
-
00 < 9 < 00, 
where 
This distribution can be approximated by (10.4.10), 
. [ 
Q(8, ),) J 
- .} (v-,+k) 
p(9 1 y) ex 
I + 2 
, 
' 
S,,_,(A)(V-1) 
-00 <9 < 00, 
that is, a k-dimensional multivariate { distribution 
{k[el:, s~_,(5,)Jf/II(X'X)-I, V-T], 
where v=n-k, T is the number of elements in A, ). is the value minimizing 
SeA, z), and (v -,,)s; _,(i) = S(}." z). 

REFERENCES 
PRINCIPAL SOURCE REFERENCES 
The following papers formed (he original basis for portions of the chapters indicated, and 
are not specifically referenced in the text. 
Chapter 2 
Box, G. E. P., and Tiao, G. C. (1965), " Multiparameter Problems from a Bayesian View-
point," Ann. Math. Statist. 36, 1468 
Chapter 3 
Box, G. E. P., and Tiao, G. C. (1962), "A further Look at Robustness via Bayes's 
Theorem," Biometrika 49, 419 
Chapter 4 
Box, G. E. P., and Tiao, G. C. (1964a), "A Bayesian Approach to the Importance of 
Assumptions Applied to the Comparison of Variances," Biometrika 51, 153 
Box, G. E. P., and Tiao, G. C. (1964b), "A Note on Criterion Robustness and Inference 
Robustness," Biometrika 51, 169 
Chapter 5 
Tiao, G. c., and Tan, W. Y. (1965), "Bayesian Analysis of Random-Effect Models in the 
Analysis of Variance. 1. Posterior Distribution of Variance Components," Biometrika 
52, 37 
Tiao, G. c., and Box, G. E. P. (1967), "Bayesian Analysis of a Three-Component 
Hierarchical Design Model," Biometrika 54, 109 
Chapter 6 
Tiao, G. C. (1966), "Bayesian Comparison of Means of a Mixed Model with Application 
to Regression Analysis," Biometrika 53, 11 
Chapter 7 
Box, G . E. P., and Tiao, G. C. (1968a), "Bayesian Estimation of Means for the Random-
Effect Model," 1. Amer. Stalisl. Assoc. 63, 174 
Tiao, G. c., and Draper, ]\i. R. (1968), "Bayesian Analysis of Linear Models with Two 
Random Components, with Special Reference to the Balanced Incomplete Block Design," 
Biometrika 55, 101 
Chapter 8 
Tiao, G. c., and Zellner, A. (1964a), "On the Bayesian Estimation of Multivariate 
Regression," f. Roy. Statisi. Soc. , Series B 26, 277 
571 

572 
References 
Box, G. E. P., and Draper, N . R. (1965), "The Bayesian Estimation of Common 
Parameters from Several Responses," Biometrika 52, 355 
Chapler 9 
Tiao, G. C , and Zellner, A. (l964b), "Bayes's Theorem and the Use of Prior Knowledge 
in Regression Analysis," Biometrika 51, 219 
Chapter 10 
Box, G. E. P., and Cox, D. R. (1964), "An Analysis of Transformations," 1. Roy. Slatist. 
Soc., Series B 26, 211 
GENERAL REFERENCES 
Afonja, B. (1970), "Some Bayesian Considerations of the Analysis and Choice of a Class 
of Designs," Ph.D. thesis, the University of Wisconsin, Madison 
Ali, M. M. (1969), "Some Aspects of the One-Way Random-Effects Model and the Linear 
Regression Model with Two Random Components," Ph.D. thesis, the University of 
Wisconsin, Madison 
Anderson, T. W. (1958), An Introduction to Multlvariale Slalislical Analysis, New York: 
Wiley 
Ando, A. and Kaufman, G. M. (1965), "Bayesian Analysis of the Independent Multinorma I 
Process--Neither Mean nor Precision Known," 1. Amer. Slatisl. Assoc. 60, 347 
Anscombe, F. J. (1948a), "The Transformation of Poisson, Binomial and Negative-
Binomial Data," Biomelrika 35, 246 
Anscombe, F. J. (l948b), "Contributions to the Discussion on D. G. Champernowne's 
Sampling Theory Applied to Autoregressive Sequences," 1. Roy. Slalist. Soc., Series B 
10, 239 
Anscombe, F. J. (1961), "Examination of Residuals," Proceedings a/4th Berkeley Symp. 
Malh. Slalist. Proc. 1, I 
Anscombe, F. J. (1963), "Bayesian Inference Concerning Many Parameters with Reference 
to Supersaturated Designs," Bullelin In!. Sial. Insl. 40·42, 721 
Anscombe, F. J. and Tukey, J. W. (1963), "The Examination and Analysis of Residuals," 
Technomelrics 5, 141 
Barnard, G. A. (1947), " Significance Tests for 2 x 2 Tables," Biometrika 34, 123 
Barnard, G. A. (1949), " Statistical Inference," J. Roy. Slalist. Soc., Series B 11, 115 
Barnard, G. A. (1954), "Sampling Inspection and Statistical Decisions," J. Roy. Stalist. 
Soc., Series B 16, 151 
Barnard, G. A., Jenkins, G. M., and Winsten, C B. (1962), "Likelihood Inference and 
Time Series," 1. Roy. Slalisl. Soc. , Series A 125, 321 
Bartlett, M. S. (1936), "The Square Root Transformation in Analysis of Variance," 
Suppl. 1. Roy. Stalisl. Soc., 8, 27, 85 
Bartlett, M. S. (1937), "Properties of Sufficiency and Statistical Test," Proc. Roy. Soc., 
Series A 160, 268 
Bartlett, M. S. (1938), "Further Aspects of the Theory of Multiple Regression," Proc. 
Camb. Phil. Soc. 34, 33. 
Bayes, T. R. (1763), "An Essay Towards Solving a Problem in the Doctrine of Chances," 
Phil. Trans. Roy. Soc. London 53, 370 (reprinted in Biomelrika (1958), 45, 293) 
Beale, E. M. L. (1960), "Confidence Regions in Nonlinear Estimation," 1. Roy. Statisi. 
Soc., Series B 22, 41 

----
-- - -
--
--
References 
573 
Behrens, W. V. (1929), «Ein Beitrag zur Fehlerberechnung bei Weniger Beobachtungen," 
Landw. Jb. 68, 807 
Birnbaum, A. (1962), "On the Foundation of Statistical Inference," J. Amer. Statist. 
Assoc. 57, 269 
Boot, J . C. G., and De Witt, G. M. (1960), "Investment Demand: An Empirical Contri-
bution to the Aggregation Problem," Intern. Econ. Review 1, 3 
Box, G . E. P. (1949), "A General Distribution Theory for a class of Likelihood 
Criteria," Biometrika 36, 317 
Box, G. E. P. (1953a), "Non-normality and Tests on Variances," Biometrika 40,318 
Box, G . E. P. (l953b), "A Note on Regions for Test of Kurtosis," Biometrika 40,465 
Box, G. E. P. (1954), "Some Theorems on Quadratic Forms Applied in the Study of 
Analysis of Variance Problems: II. Effects of Inequality of Variance and of Correlation 
Between Errors in the Two-Way Classification," Ann. ,\1alh. Statisl. 25, 484 
Box, G. E. P. (1957), "Use of Statistical Methods in the Elucidation of Basic 
Mechanisms," Bull. Int. Sial. Ins/. 36, 215 
Box, G. E. P. (1960), "Fitting Empirical Data," AI/n. New York Academy 0/ Sciences 86, 
792 
Box, G. E. P., and Andersen, S. L. (1955), "Permutation Theory in the Derivation of 
Robust Criteria and the Study of Departures from Assumptions," J . Roy. Statist. Soc., 
Series B 17, 1 
Box, G. E. P., and Jenkins, G. M. (1970), Time Series Analysis, Forecasting and Control, 
San Francisco : HOlden- Day 
Box, G. E. P., Erjavec, J., Hunter, W. G . and MacGregor, J. F. (1972), "Some Problems 
Associated with the Analysis of Multiresponse Data" (to appear in Technometrics) 
Box, G. E. P., and Tiao, G. C. (l968b) " A Bayesian Approach to Some Outlier 
Problems," Biometrika 55, 119 
Box, G. E . P., and Watson, G. S. (1962), "Robustness to Non-Normality of Regression 
Tests," Biometrika 49, 93 
Bracken, J., and Schleifer, A. (1964), Tables /or Normal Sampling wilh Unknown Variance, 
Cambridge, Harvard University Press 
Brookner, R. J., and Wald, A. (1941), "On the Distribution of Wilks' Statistic for Testing 
the Independence of Several Groups of Variates," Ann. Math. Statist. 12, 137 
Bulmer, M. G. (1957), "Approximate Confidence Limits for Components of Variance," 
Biometrika 44, 159 
Bush, N. , and Anderson, R. L. (1963), "A Comparison of Three Different Procedures for 
Estimating Variance Components," Technometrics 5, 421 
Carlton, G . A. (1946), "Estimating the Parameters of a Rectangular Distribution," Ann. 
Math . Statist. 17, 355 
Cochran, W. G . (1934), " The Distribution of Quadratic Forms in a Normal System, with 
Applications to the Analysis of Covariance," Proc. Camb. Phil. Soc. 30, 178 
Cochran, W. G., and Cox, G. M. (1950), Experimental Designs, second edition, New 
York: Wiley 
Cook, M. B. (1951), "Bivariate K-Statistics and Cumulants of their Joint Sampling 
Distribution, " Biometrika 38, 179 
- -

574 
References 
Cornish, E. A. (1954), "The Multivariate I-Distribution Associated with a Set of Normal 
Sample Deviates," Aust. J. Phys. 7, 531 
Crump, S. L. (1946), "Estimation of Variance Components in the Analysis of Variance," 
Biometrics 2, 7 
Daniel, C. (1959), "ese of Half Normal Plots in Interpreting Factorial Experiments," 
Technometrics I, 31 I 
Daniels, H. E. (1939), "The Estimation of Components of Variance," J. Roy. Statist. Soc. , 
Supplement 6, 186 
Davies, O. L. (editor), (1949), Statistical .'vfelhods in Research and Production, second 
edition, London: Oliver and Boyd 
Davies, O. L. (editor), (1967), Statistical Methods in Research and Production, third edit ion, 
London: Oliver and Boyd 
De Bruijn, N. G. (196]), Asymptotic Methods in Analysis, Amsterdam: North-Holland 
Deemer, W. L., and Olkin, r. (1951), 'The lacobians of Certain Matrix Transformations 
Useful in Multivariate AnalYSiS, Based on Lectures by P. L. Hsu," Biometrika 38,345 
De Finetti, B. (J 937), "La Prevision : Ses Lois Logiques, ses Sources Subjectives," 
Ann. Inst. H. Poincare 7, 1. English translation in Studies in Subjective Probability , 
H. E. Kyburg, Jr. and H. G. Smokier (editors), 1964, New York: Wiley 
De Groot, M. H. ([970), Optimal Statistical Decisiolls, New York: McGraw-Hili 
Dempster, A. P. (1963), "On a Paradox Concerning Inference about a Covariance 
Matrix," Ann .. \1alh. Statist. 34, 1414 
Diananda, P. H. (1949), "Note on Some Properties of Maximum Likelihood Estimates," 
Proc. Camb. Phil. Soc. 45, 536 
Dickey, 1. M. (l967a), "Expansions of I Densities and Related Complete lntegrals," 
Ann. Math. Slatist. 38, 503 
Dickey, J. M. (I 967b), "Matric-Variate Generalizations of the Multivariate I Distribu-
tion and the Inverted Multivariate t Distribution," Ann. Math. Statist. 38, 511 
Dickey, 1. 
M. (1968), "Three Multidimensional-Integral Identities with Bayesian 
Applications," Anll. Math. Statist. 39, 1615 
Dreze, J. H., and Morales, J. A. (1970), "Bayesian Full Information Analysis of the 
Simultaneous Equation Model," Cenler for Operations Research and Econometrics, 
Discussion Paper No. 7031, Universite Catholique de Louvain 
Dunnett, C. W., and Sobel, M. (1954), "A Bivariate Generalization of Student 's 1-
Distribution With Tables for Certain Special Cases," Biomelrika 41, J 53 
Edwards, W., Lindman, H., and Savage, L. J. (1963), "Bayesian Statistical Inference for 
Psychological Research," Psychological Rev. 70, 193 
Eisenhart, C. (1947), "The Assumptions Underlying the Analysis of Variance," Biometrics 
3, 1 
Federer, W. T. (1955), Experimenlal Design, New York: Macmillan 
~isher, R. A. (1915), " Frequency Distribution of the Value of the Correlation Coefficient 
in Samples from an IndeAnitely Large Population," Biometrika 10, 507 
Fisher, R. A. (1921), " On the 'Probable Error' of a Coefficient of Correlation Deduced 
from a Small Sample," Me/ron 1, '3 
Fisher, R. A. (1922), "On the Mathematical Foundations of Theoretical Statistics," 
Phil. Trans. Roy. Soc. , Series A 222, 309 
-----~ - - --
~ 

References 
5"/5 
Fisher, R. A. (1924), "On a Distribution Yielding the Error Function of Several Well-
Known Statistics," Proc. II/t. Math. Congress, Toronto, 805 
Fisher, R. A. (1925), "Theory of Statistical Estimation," Proc. Comb. Phil. Soc. 22, 700 
Fisher, R. A. (1930), "Inverse Probability," Proc. Comb. Phil. Soc. 26, 528 
Fisher, R. A. (1935), "The Fiducial Argument in Statistical Inference," Ann. Ellgen. 6, 391 
Fisher, R. A. (1939), "The Comparison of Samples with Possibly Unequal Variances," 
Ann. Eugen. 9, 174 
Fisher, R. A. (1959), Statistical .'vIelhods and Scientific Inference, second edition, London: 
Oliver and Boyd 
Fisher, R. A. (1960), The DeSign of Ex peri men IS, seventh edition, New York: Hafn'er 
Fisher, R. A. (J 961a), "Sampling the Reference Set," Sankhy6, Series A 23, 3 
Fisher, R . A. (196Ib), "Weighted Mcan of Two Samples With Unknown Variance 
Ratio," Sankhy6, Series A 23, 103 
Fraser, D . A. S. (1968), The Structure of Inference, New York : Wiley 
Fraser, D . A. S., and Haq, M. S. (1969), "Structural Probability and Prediction for the 
Multivariate Model," J. Roy. Statist. Soc., Series B 31,317 
G ates, C. E., and Shine, C. J. (1962), "The Analysis of Variance of the S-Stage Hierarchical 
Classification," Biometrics 18, 529 
Gayen, A. K. (1949), "The Distribution of 'Student's' I in Random Sample of any Size 
Drawn from Non-Normal Cniverse," Biometrika 36, 353 
Gayen, A. K. (1950), "The Distribution of the Variance Ratio in Random Samples of any 
Size Drawn from a Non-Normal Universe," Biometrika 37, 236 
Geary, R . C. (1936), "The Distribution of 'Student's' Ratio for Non-Normal Samples," 
J. Roy. Slalist. Soc., Supplement 3, 178 
Geary, R. C. (1947), "Testing for Normality," Biometrika 34,209 
Geisser, S. (I 965a), "Bayesian Estimation in Multivariate Analysis," Ann. Math . Statist. 
30, 150 
Geisser, S. (1965b), " A Bayes Approach for Combining Correlated Estimates," J. ArneI'. 
Statisl. Assoc. 60, 602 
Geisser, S., and Cornfield, J. (1963), "Posterior Distributions for Multivariate Normal 
Parameters," J. Roy. Slalist. Soc., Series B 25, 368 
Gossett, W. S. ["Student"] (1908), "The Probable Error of a Mean," Biometrika 6, 1 
Gower, J. C. (1962), " Variance Component Estimation for Unbalanced Hierarchical 
Classification," Biometrics 18, 537 
Graybill, F. A., and Weeks, D. L. (1959), "Combining Inter-Block and Intra-Block 
Information in Balanced Incomplete Blocks," Ann. Math. Statist. 30, 799 
Graybill, F. A., and Wortham, A. W. (1956), "A Note on Uniformly Best Unbiased 
Estimators for Variance Components," J. Amer. Statist. Assoc. 51, 266 
Grunfeld, Y. (J 958), "The Determinants of Corporate Investment," Ph.D. thesis, 
University of Chicago 
Guttman, 1., and Meeter, D. A. (1965), "On Beale's Measure of Nonlinearity," Techno-
mefrics 7, 623 
Hartigan, J. A. (1964), "Invariant Prior Distributions," Ann. Math. Stalist. 35,836 

576 
References 
Hartigan, J. A. (1965), " The Asymptotic Unbiased Prior Distribution," Ann. Math. 
Statist. 36, 1137 
Hartley, H . O. (1940), "Testing the Homogeneity of a Set of Variances," Biometrika 31, 
249 
Hartley, H. O. (1961), "The Modified Gauss-Newton Method for the Fitting of Nonlinear 
Regression Functions by Least Squares," Technometrics 3, 269 
Henderson, C. R. (1953), "Estimation of Variance and Covariance Components," 
Biometrics 9, 226 
Herbach, L. H. (1959), "Properties of Model II-type Analysis of Variance Tests," 
Ann. Math. Statist. 30, 939 
Hildreth, C. (1963), " Bayesian Statisticians and Remote Clients," Econometrika 32,422 
Hill, B. M. (1965), "Inference About Variance Components in the One-Way Model," 
J. Amer. Statist. Assoc. 60, 806 
Hill, B. M . (1967), "Correlated Errors in the Random Model," J. Amer. Statist. Assoc. 
62, 1387 
Hogg, R. V., and Craig, A. T. (1970), Introduction to Mathematical Statistics, second 
edition, New York : Macmillan 
Huzurbazar, V. S. (1955), "Exact Forms of Some Invariants for Distributions Admitting 
Sufficient Statistics," Biometrika 43, 533 
Jackson, D . (1921), " Note on the Median of a Set of Num'bers," BIIII. Amer. Math. Soc. 
27, 160 
James, W., and Stein, C M. (1961), "Estimatio)l with Quadratic Loss Function," PI'OC 
4th Berkeley Symp. Math. Statist. Proc. 1, 361 
Jaynes, E. T. (1968), " Prior Probabilities," IEEE Trans. Systems Science and Cybernetics 
SSC-4,227 
Jeffreys, H . (1961), Theory of Probability, third edition, Oxford : Clarendon Press 
Jeffreys, H ., and Swirlee, B. (1956), Methods of Mathematical Physics, Cambridge: 
Cambridge University Press 
Johnson, R . A. (1967), "An Asymptotic Expansion for Posterior Distributions," 
Ann. Math . Statist. 38, 1899 
Johnson, R . A. (1970), "Asymptotic Expansions Associated with Posterior Distributions," 
Ann. Math. Statist. 41, 851 
Kahirsagar, A. M. (1960), "Some Extensions of the Multivariate t-Distribution and the 
Multivariate Generalization of the Distribution of the Regression Coefficients," 
Proc. Camb. Phil. Soc. 57, 80 
Kempthorne, O. (1952), The Design and Analysis of Experiments, New York: Wiley 
Kendall, M. G., and Stua'rt, A. (1961), The Advanced Theory of Statistics, Volume 2, 
New York: Hafner 
Klotz, J. H., Milton, R. C, and Zacks, S. (1969), "Mean Square Efficiency of Estimators of 
Variance Component," J. Amer. Statist. Assoc. 64, 1383 
Kolmogoroff, A. (1941), "Confidence Limits for an C nknown Distribution Function," 
Ann. Math. Statist. 12, 461 
Lehman, E. L. (1959), Testing Statistical Hypotheses. New York: Wiley 
Lindley, D. V. (1965), Introduclionto ProbabililY and Stalislicsjrom a Bayesian Viewpoinl, 
Part 2, Inference, Cambridge: Cambridge University Press 

References 
577 
Lindley, D. V. (1971), "Bayesian Statistics, a Review," Regional Conference Series in 
Applied Malhematics, S.f.A.M. 
Lund, D. R. (1967), "Parameter Estimation in a Class of Power Distributions," Ph.D. 
thesis, the University of Wisconsin, Madison 
Marquardt, D. W. (1963), "An AlgorittuTI For Least Squares Estimation of Nonlinear 
Parameters," J. Soc. Ind. Appl. Math. 11,431 
Milne-Thomson, L. M. (1960), The Calculus of Finite Differences, New York: Macmillan 
Mood, A. M., and Graybill, F. A. (1963), Introduction 10 the Theory of Statistics, New 
York: McGraw- Hili 
Moriguti, S. (1954), "Confidence Limits for a Variance Component," Rep. Stat. Appl. 
Res. Juse. 3, 29 
Mosteller, F., and Wallace, D. L. (1964), Inference and Disputed Authorship:1he Federalist, 
Reading, Mass.: Addison-Wesley 
,. 
Neider, J. A. (1954), "The Interpretation of Negative Component of Variance," 
Biometrika 41, 544 
Neyman, 1., and Pearson, E. S. (1928), "On the Use and Interpretation of Certain Test 
Criteria for Purposes of Statistical. Inference, Part I," Biometrika 20A, 175 
Novick, M. R. (1969), "Multiparameter Bayesian Indifference Procedures," J. Roy. 
Statist. Soc. Series B 31, 29 
Novick, M. R., and Hall, W. J. (1965), "A Bayesian Indifference Procedure," J. Amer. 
Statist. Assoc. 60, 1104 
Patil, V. H. (1964), "The Behrens-Fisher Problem and its Bayesian Solution," J. Indian 
Statist. Assoc. 2, 21 
Patnaik, P. B. (1949), "The Non-Central /- and F-Distributions and Their Applications," 
Biometrika 36, 202 
Pearson, K. (1934), Tables of the Incomplete Beta-Functioll, Cambridge: Cambridge 
University Press 
Perks, F. J. A. (1947), "Some Observations on Inverse Probability, Including a New 
Indifference Rule," J. Illst. Actuaries 73, 285 
Pillai, K . C. S., and Gupta, A. K. (J 969), "On the Exact Distribution of Wilks' Criterion," 
Biometrika 56, 109 
Pitman, E. J. G. (1936), "Sufficient Statistics and Intrinsic Accuracy," Proc. Camb. Phil. 
Soc. 32, 567 
Portnoy, S. (J 971), "Formal Bayes Estimation with Application to a Random Effect 
Model," Ann. Math. Statist. 42, 1379 
Pratt, J. W., Raiffa, H., and Schlaifer, R. (1965), Introduction to Statistical Decision 
Theory. New York: McGraw-Hill 
Raiffa, H ., and Schlaifer, R . (1961), Applied Statistical Decision Theory, Cambridge: 
Harvard University Press 
Ramsey, F. P. (1931), The FOllndation of Mathematics alld Other Logical Essays, London: 
Routledge and Kegan Paul 
Robbins, H. (1955), "A n Empirical Bayes Approach to Statistics," Proc. 3rd Berkeley 
Symp. Math. Statist. Proc. 157 
Robbins, H. (1964), "The Empirical Bayes Approach to Statistical Problems," Ann. Math. 
Statist. 35, J 

578 
References 
Savage, L. 1. (1954), The Foundation 0/ Statistics, New York: Wiley 
Savage, L. J. (1961a), "The Subject Basis of Statistical Practice," unpublished manuscript, 
the University of Michigan 
Savage, L. J. (1961 b), "The Foundation of Statistics Reconsidered ," Proc. 4th Berkeley 
Symp. 1, 575 
Savage, L. J., et al. (1962), The Foundation of Statistical Inference, London: Methuen 
Schatzoff, M. (1966), "Exact Distribution of Wilk's Likelihood Rat io Criterion," 
Biometrika 53, 347 
Scheffe, H. (1959), The Analysis 0/ Variance, New York: Wiley 
Schlaifer, R. (1959), Probability and Statistics /01' Business Decision, ;-..lew York: McGraw· 
Hill 
Searle, S. R . (1958), "Sampling Variances of Estimates of Components of Variance," 
Ann. Math . Statist. 29, J 67 
Seshadri, V. (1966), " Comparison of Combined Estimators in Balanced Incomplete 
Blocks," Ann. Math . Statist. 37, 1832 
Siegel, C. L. (1935), "Ueber die Analytische Theorie der Quadratischen Formen," Ann. 
Math. 36, 527 
Stein, C. M. (1962), " Confidence Sets for the Mean of a Multivariate Normal Distribu-
tion," 1. Roy. Statist. Soc., Series B 24. 265 
Stone, M. (1964), "Comments on a Posterior Distribution of Geisser and Cornfield," 
J. Roy. Statist. Soc., Series B 26, 274 
Stone, M., and Springer, B. G. F. (1965), "A Paradox Involving Quasi Prior 
Distributions," Biometrika 52, 623 
Sukhatme, P. V. (1938), "On Fisher and Behrens' Test of Significance for the Difference 
in Means of Two Normal Samples," Sankhyil 4, 39 
Tan, W. Y. (1964), "Bayesian Analysis of Random Effect Models," Ph.D. thesis, the 
University of Wisconsin, Madison 
Thompson, W. A., Jr. (1962), "The Problem of Negative Estimates of Variance 
Components," Ann. Math. Statist. 33, 273 
Thompson, W. A., Jr. (1963), "Non-Negative Estimates of Variance Components," 
Technometrics 5, 44 [ 
Tiao, G. c., and Ali, M. M. (l971a), "Effect of Non-Normality on Inferences About 
Variance Components," Technometrics 13, 635 
Tiao, G. c., and Ali, IvI. M . (197Ib), "Analysis of Correlated Random Effect : Linear 
Model with Two Random Components," Biometrika 58, 37 
Tiao, G . c., and Fienberg, S. (1969), "Bayesian Estimation of Latent Roots and Vectors 
with Special Reference to the Bivariate Normal Distribution," Biometrika 56, 97 
Tiao, G. c., and Guttman, 1. (1965), "The Inverted Dirichlet Distribution with 
Applications," 1. Amer. Statist. Assoc. 60, 793 
Tiao, G. c., and Lund, D. R. (1970), " The ese of OLUMV Estimators in Inference 
Robustness Studies of the Location Parameter of a Class of Symmetric Distributions," 
J. Amer. Statist. Assoc. 65, 370 
-j'iao, G. C, and Tan, W. Y. (1966), "Bayesian Analysis of Random-Effect Models in the 
Analysis of Variance. II. Effect of Autocorrelated Errors," Biometrika 53, 477 

References 
579 
Tiao, G. c., Tan, W. Y., and Chang, Y C. (1970), "A Bayesian Approach to Multivariate 
Regression Subject to Linear Constraints," paper presented to the Second World Congress 
of Ihe Economelric Sociely, Cambridge, England 
Tukey, J. W. (1956), "Variances of Variance Components: I. Balanced Designs," 
Ann. Math. Sialisi. 27, 722 
Tukey, J. W. (1961), "Discussion Emphasizing the Connection Between Analysis of 
Variance and Spectrum Analysis," Teehnomelrics 3, 191 
Turner, M. C. (1960), "0[1 Heuristic Estimation Methods," Biomelrics 16, 299 
Wang, Y. Y. (1967), "A Comparison of Several Variance Component Estimators," 
Biometrika 54, 30 I 
Welch, B. L. (1938), "The Significance of the Difference Between Two Means When the 
Population Variances are Unequal," Biomelrika 29, 350 
Welch, B. L. (1947), "The Generalization of 'Student's' Problem when Several Different 
Population Variances are Involved," Biometrika 34, 28 
Welch, B. L., and Peers, H. W. (1963), "On Formulae fOJ Confidence Points Based on 
fntegrals of Weighted Likelihood," J. Roy. Sialisi. Soc. Series, B 25, 318 
Wilks, S. S. (1962), .Vfalhematical Sialisties, New York: Wiley 
Williams, J. S. (1962), "A Confidence Interval for Variance Components," Biometrika 
49,278 
Wishart, J. (1928), "The Generalized Product Moment Distribution in Samples from a 
Normal. Multivariate Population," Biomelrika 20A, 32 
Yates, F. (1939), "An Apparent Inconsistency Arising from Tests of Significance Based on 
Fiducial Distribution of Unknown Parameters," Proc. Comb. Phil. Soc. 35, 579 
Yates, F. (1940), "The Recovery ofInter-Block Information in Balanced Incomplete Block 
Designs," Ann. Eugel1. 10, 317 
Zacks, S. (1967), "More Efficient Estimators of Variance Components," Technical Report 
No. 4, Department of Statistics, Kansas State University, Manhattan, Kansas 
Zellner, A. (1962), "An Efficient Method of Estimating Seemingly Unrelated Regression 
and Tests for Aggregation Bias," 1. Amer. Sialisl. Assoc. 57, 348 
Zellner, A. (1963), "Estimators for Seemingly Unrelated Regression Equations: Some 
Finite Sample Results," J. Amer. Sialisi. Assoc. 58, 977 
Zellner, A., and Tiao, G. C. (1964), "Bayesian Analysis of the Regression Model with 
Autocorrelated Errors," 1. Amer. Sialist. Assoc. 59, 763 

.. 

INDEXES 

· 
-
~ -

Afonja, B., 418, 572 
Ali, M. M ., 248, 249, 383, 572, 578 
Andersen, S. L., 154,235, 573 
Anderson, R. L., 248, 573 
Anderson, T. W., 427, 438,449,474, 572 
Ando, A" 438,572 
Anscombe, F . J., 1, 8,38,260,381,382,572 
Barnard, G . A., 1, 14,71,72, 73, 122,485, 
572 
Bartlett, M. S., 38, 134, 135, 203, 221, 230, 
451,572 
Bayes, T. R., 23, 24, 308, 572 
Beale,E. M.L.,43J,572 
. 
Behrens, W. Y., 106,573 
Birnbaum, A., 73, 573 
Boot, J. C. G., 495,573 
Box, G . E. P., 7, 8J, J 36, 154, 157, J 94,203, 
235,431,437,45J,522,571 , 572,573 
Bracken, J., 481,573 
Brookner, R. J., 136, 573 
Bulmer, M. G., 248, 573 
Bush, N" 248, 573 
Carlton, G. A., 155,573 
Chang, Y. c., 446,579 
Cochran, W. G., 331, 392, 573 
Cook, M. B., 517, 573 
Cornfield, J. 438, 441,575 
Cornish, E. A" 117,574 
Cox, D. R., 525, 572 
Cox, G. M., 392, 573 
Craig, A. T., 3, 576 
Crump, S. L., 248, 574 
Daniel, C., 8, 574 
Daniels, H. E., 248, 574 
Davies, O. L., 209, 246, 348, 370, 574 
DeBruijn, N. G ., 27J, 574 
Deemer, W . L., 474, 575 
DeFinetti, B., I, 14,574 
AUTHOR INDEX 
DeGroot, M. H., 19,574 
Dempster, A. P., 444, 574 
DeWitt, G. M., 495, 573 
Diananda, P. H., 157, 574 
Dickey, J. M., 440,481,574 
Draper, N. R., 4J8, 571, 572 
Dreze, J. H., 446, 574 
Dunnett, C. W., 117,574 
East, D. A., 555 
Edwards, W., 22, 84, 574 
Eisenhart, c., 248, 574 
Erjavec, J., 431,573 
Federer, W., T., 414, 574 
-ienberg, S., 459,578 
Fisher, R. A" 1, 12, 13,24, 35, 38,42,60, 73, 
105, 106, Ill, 149, 153, 163, 308,326,466, 
468,469,481,485, 487,488,489,515,518, 
520,574, 575 
Fraser, D. A. S., 73,575 
Gates, C. E., 248, 575 
Gayen, A. K., 154,203,575 
Geary, R. c., 154,203,575 
Geisser, S., 438, 441,449, 459,501, 575 
Gossett, W. S., "Student", 97, 575 
Gower, J. c., 248,575 
Graybill, F. A., 3, 248, 305, 396, 417, 575, 
583 
577, 
Grunfeld, Y., 496,575 
Gupta, A. K., 451 , 577 
Guttman, 1., 287, 296, 431,575,578 
Hall, W. J., 60, 577 
Hamilton, P. A., 555 
Haq, M. S., 73, 575 
Hartigan, J. A., 60, 575 
Hartley, H. 0., 136,437,556,570,576 
Henderson, C. R., 248, 576 
Herbach, L. H" 248, 576 

584 
Author Index 
Hildreth, c., 124,576 
Hill, B. M., 249, 272, 576 
Hogg, R. V., 3, 576 
Hsu, P. L., 474 
Hunter, W. G.,431, 573 
Huzurbazar, V. S., 576 
Jackson, D., 199, 200, 576 
James, W., 313, 369, 388, 576 
Jaynes, E. T., 60, 576 
Jeffreys, H., 1,41,42,44,54,56,58,60,72, 
80,94,106,271,481,488,489,576 
Jenkins, G. M., 7, 73, 81, 572, 573 
Johnson, R. A., 36, 576 
Kahirsagar, A. M., 440,576 
Kaufman, G. M., 438,572 
Kempthorne, 0., 392, 576 
Kendall, M. G., 62, 576 
Klotz, J. H., 251, 311, 576 
Kolmogoroff, A" 169, 576 
Lehman, E. L., 351,576 
Lindley, D. V., 1,80,84,371,555,576,577 
Lindman, H., 22, 84, 574 
Lochner, R. H., 565 
Lund, D. R., 158,170, ]77,577,578 
MacGregor, J., 431, 573 
Marquardt, D. W., 437,577 
Meeter, D. A., 431,575 
Merrington, M., 557 
Milne-Thomson, L. M., 146,294,577 
Milton, R. c., 251, 311, 576 
Mood,/\.. M., 3, 305, 577 
Morales, J. 1\..,446,574 
Moriguti, S., 248, 577 
Mosteller, F., 25,149,390,577 
Neider, J. A., 260, 577 
Neyman, J., 72,155,577 
Novick, M. R., 60, 577 
Oikin, 1.,474,574 
Patil, V. H., 107, 577 
Patnaik, P. B., 577 
Pearson, E. S., 72, 155,556,570,577 
Pearson, K., 149, 263,577 
Peers, H. W., 60, 579 
Perks, F. J. A., 60, 577 
Pillai, K. C. S., 451, 577 
Pitman, E. J. G., 577 
Portnoy, S., 251, 311, 313, 314, 577 
Pratt,J. W., 309, 577 
Raiffa, H., 19,309,577 
Ramsey, F. P., 1, 14, 577 
Robbins, H., 390, 577 
Savage, L. J., I, 14,22,25, 73, 80, 438,574, 
578 
Schatzoff, M., 451, 578 
Scheffe,H., 249, 396, 578 
Schlaifer, R., 19,309,577,578 
Schleifer, A., 481,573 
Searle, S. R., 248, 578 
Seshadri, V., 396, 578 
Shine, C. J., 248, 575 
Siegel, C. L., 578 
Smirnoff, 169 
Sobel, M., 117, 574 
Springer, B. G . F., 251, 303, 304 
Stein, C. M., 1, 313, 369, 371, 381, 388, 390, 
576, 578 
Stone, M., 25] , 303,304,444,578 
Stuart, A., 62, 576 
Sukhatme, P. V., 106, 107, 578 
Swirlee, B., 271,576 
Tan, W. Y.,248,249,298,403,446,571,578, 
579 
Thompson, W. A., Jr., 248, 578 
Tiao, G. C , 8], 157, 158,249,287,296,298, 
383,403,418, 446,459,522,565,571,572, 
573,578,579 
Tukey,J. W.,8,248,530,572,579 
Turner, M. C, 157, 199, 579 
Wald, A., 136, 573 
Wallace, D. L., 25,149,390,577 
Wang, Y. Y., 248,579 
Watson, G. S., 194, 573 
Weeks, D. L., 396,417,575 
Welch, B. L., 60, 73,109,579 
Wilks, S. S., 62,116,579 
Williams, J. S., 248, 579 
Winsten, C. B., 73, 572 
Wishart, 1., 427,579 
Wortham, A. W., 248, 575 
Yates, F., 392, 396,417,481,487,488, 579 
Zacks,S.,248,25I, 311,576, 579 
Zellner, A., 81, 438, 571, 572, 579 

Analysis of variance tables 
additive mixed model, 343 
balanced incomplete blocks, 400 
comparison Normal means, 131 
interaction modeJ, 363 
linear model, two random components, 
395 
NormalJinear model, 127 
three component hierarchical, 245 
two component random effect, 250, 371 
two-way random effect, 330 
Balanced incomplete block design (BIBD) 
392,397 
efficiency factor, 402 
recovery of inter-block information, 417 
Bartlett's test, variance homogeneity 
(see Comparison of variances) 
Bayes' theorem 
applied to 
frequencies, 12 
scientific inference, 20 
subjective probabilities, 14 
definition, 10 
sequential nature, 11 
Bayesian inference, appropriateness, 9 
Behrens-Fisher distribution 106 
approximations, 107 
Bernoulli polynomials, 146 
Beta distribution, 36 
Binomial distribution 34 
parameter inference, 36 
Cuachy distribution, 64 
Central limit conditions, 78 
Chi "X" distribution, 88 
inverted, 88 
Chi-square "X 2 " distribution, 87 
inverted, 88 
logX 2 88 
585 
SUBJECT INDEX 
Common parameters, inference 
independent responses, 480 
two responses, 489, 515 
linear bivariate model, 502 
linear multivariate, 501 
common derivative matrix, 500 
nonlinear multivariate, 428 
Comparison of Normal means 
additive mixed model, 357 
scaled Fapproximation, 360 
balanced incomplete blocks, 403, 409 
fixed effect model (k-populations), 130 
interaction model, 365 
scaled t approximation, 366 
random effect model, 376, 386 
two means 
additive mixed model, 346, 352 
random effect model, 375 
variances equal, 103 
variances unequal, 104 
Comparison of variances 
k-samples 
Bartlett's approximation, 135, 221, 
230 
non-Normal parents, 220, 227, 229, 
231 
Normal parents, 133 
2-samples 
non-Normal (exponential power) 
parents, 205,209, 212,214,218 
Normal parents, 110 
Confidence distribution, 79 
Confidence interval, 6 
Contrasts, linear, 128 
comparison of 
location, 131 
spread, 132 
Correlation coefficient (Normal), 
inference, 465 

586 
Subject Index 
Covariance matrix (Normal), inference, 460, 
461 
latent roots and vectors, 459 
CrosscJassification design, 319 
Decision theory, 19 
Derivative matrix, 113 
Design matrix, 421 
Digamma function, 316 
Double exponential distribution, 157 
Double F distribution, 155 
Estimators 
Bayes, 304 
mean-squared error, 307 
James-Stein problem, 388 
variance components, 311, 313 
Expectation function, 422 
multivariate linear, 435 
multivariate non-linear, 424 
Exponential power distribution, 157 
non-Normality (kurtosis) measure, [57 
upper per cent points, 158 
F distribution, 11 0 
Fiducial inference, 73 
Fixed effect models 
cross classification, 318, 328 
one-way clossification, 370 
Gamma function, generalized, 427 
Hierarchical design, 244 
H.P.D. (highest posterior density) region 
general, 122 
interval,85 
interval, standardized, 89 
Indicator variable, 113 
Information matrix 
definition, 53 
Normal covariance matrix, 475 
variance components, 252 
Information measur<-, single parameter, 42 
Integral identities 
Dirichlet, 145 
gamma, 144 
Normal, multivariate, 145 
t, matric-variate, 475 
multivariate, 145 
variance component distributions, 295 
Jacobians, matrix transformations, 473 
Jeffreys'rule 
single parameter, 42 
multiparameter,54 
Kronecker product, 477 
Kurtosis, measure, 150 
Kurtosis parameter (exponential power 
distribution), inference 
prior distribution, 167 
I-sample, 166, 167, 183, 187 
k-sample, unequal variances, 209, 217, 
225,231 
Likelihood, data translated, 32 
approximate, 36,41 
multiparameter,48 
Likelihood function 
definition, 10 
role in Bayes' theorem, 10 
standardized likelihood, II 
Likelihood inference, 73 
Likelihood ratio criterion, comparison of 
k non-Normal variances, 221 
Linear model 
non-Normal,176 
Normal, 46, 113 
multivariate, common derivative mat-
rix, 438 
multivariate, general, 435 
two random component, 393 
Location parameter, definition, 18 
Location-scale family distributions, 43 
parameter inference, 43, 44, 58 
Loss function, 308, 309, 313 
Matrix of independent variables, 113 
Mean, definition, 76 
Median, definition, 76 
Mixed models 
general,324 
additive, 325, 341 
interaction, 326, 362 
Monotone likelihood ratio, 351 
Multinomial distribution, 55 
parameter inference, 55 
Nonlinear model 
non-Normal, 186 
Normal,422 
linear approximation, 436 
multivariate, 423 

Non-Normal means (exponential power 
parents), inference 
I-sample, 160, 162, 167, 171 
k-samples, unequal variances, 212,229 
Normal distribution, 80 
matric-variate,447 
multivariate, 80, 115, 116 
Normal means, inference 
fixed effect model (k-populations), 128, 
377 
multivariate model, 440 
random effect model , 372,376 
random vs. fixed effect prior in high 
dimension, 379, 383 
single mean, 28, 51,82,93,95,97 
informative prior, 99 
Normal prior, 18, 74 
Normal standard deviation, inference, 31, 
87,89,93,96 
informative prior, 99 
metric for locally uniform prior, 101 
Normal variance, inference (see Normal 
standard deviation) 
Nuisance parameter, 70 
applied to robustness studies, 70, J 69 
Observation, aberrant, 522 
Pascal distribution, 45 
parameter inference, 45 
"Paired-t" problem, 348 
Poisson distribution, 39 
parameter inference, 40 
Pooling variance estimates, 249, 246, 289, 
353 
Prior distribution 
definition, 10 
improper, 21 
locally uniform, 23 
non informative, definition, 32 
dependence on model, 44 
location and scale parameters, 56 
multiparameter, general, 54 
summary of examples, 59 
reference, 23 
Posterior distribution 
definition, 10 
parameters under constraints, 67 
Probability, subjective, 14 
Subject Index 
58" 
Quadratic forms 
combination of two, 418 
mixed cumulants, 520 
Random effect models 
hierarch ical design, 244,249,277,293 
cross classification design, 318, 322, 329 
one-way c1assitkation,371 
Randomized (complete) block designs (see 
mixed model additive), 326 
Regression coefficients, inference 
linear model, non-Normal, 177, J 78 
linear model, Normal, 48, 51, 115,117, 
125,437 
joint vs. marginal, 122 
multivariate, common derivative 
matrix, 440, 441, 448, 452 
two random components, 396 
nonlinear modeJ, non-Normal, J87 
nonlinear model, Normal 
multivariate, general, 427, 428 
independent responses, 428 
transformation of data, 531,539 
Robustness 
cri terion, 152 
inference, 153 
variance ratio, 208 
inference vs. criterion 
mean, 153 
variances, 232 
Sampling theory inference, 5, 72 
Scale parameter, definition, 49 
Scientific investigation, iterative process, 4 
Skewness, measure, 150 
Spread parameter, definition, 77 
Stable estimation, 22 
Statistical analysis, in scientific investigation, 
5 
Statistical inference, general, 5 
Stirling's series, 147 
Student's t distribution (see t distribution) 
Sufficient statistics 
additive mixed model, 341 
definition, 61 
exponential power distribution, variance, 
206 
hierarchical random effect models, 250, 
278 

588 
Subject Index 
Normal distribution, 60 
Normal linear model, 115 
relevance in Bayesian inference, 63 
two-way random effect model, 331 
uniform (rectangular) distribution, 155 
f distribution, 97 
matric-variate, 441 
multivariate, 117 
product of multivariate, 481,489 
asymptotic expansion, 515 
product of 2 univariate, 481 
t 2 criterion, non-Normal parent, 154 
Transformation of data 
general model, 530 
inference, 531,539 
logarithmic (textile data), 527, 536,548 
reciprocal (biological data), 525,534, 542 
square root (grinding experiment), 523 
U distribution, 449 
approximations, 451 
Uniformly most power similar test 
two non-Normal variances, 207 
Variance components, definition, 245 
Variance components, inference 
additive mixed model, 346 
balanced incomplete blocks model, 407 
interaction model, 364 
q-component hierarchical model, 293 
2-component hierarchical model 
asymptotic expansion, 271, 298 
individual components, 255, 258, 
266,272, 294, 296 
noninformative prior, 251, 303 
ratio, 253 
sampling theory difficulties, 248 
3-component hierarchical model 
individual components, 279, 286, 290 
ratio, 280 
relative contribution, 283 
scaled X- 2 approximation, 261,274,287, 
291 , 335,353,366 
two-way random effect model, 332 
Weighted mean problem, 481 
Wishart distribution, 427 
inverted, 460 


