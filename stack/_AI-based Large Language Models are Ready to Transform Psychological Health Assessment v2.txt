 
AI-based Large Language Models are Ready to  
Transform Psychological Health Assessment 
 
Oscar N.E. Kjell*1,2, Katarina Kjell1, H. Andrew Schwartz*1,2 
 
*Contributed equally to this perspective 
1Lund University, Psychology Department. 
2Stony Brook University, Computer Science Department 
 
 
Summary abstract 
Artificial intelligence-based (AI-based) language analysis has been undergoing a purported 
“paradigm shift” initiated by new machine learning models, large language models. These models, 
such as GPT3 or BERT, have led to unprecedented accuracies over most computerized language 
processing tasks such as web search, automatic machine translation, and question answering, while 
their chat-based forms like ChatGPT have captured the interest of over a million users. The success 
of the large language model is mostly attributed to its capability to numerically represent words 
in their context, long a weakness of popular language assessment techniques that use “bag-of-
words” or word count approaches. While many potential applications, from therapy to health 
information education, are beginning to be studied on the heels of chatGPT’s success, here we 
suggest these models are already ready to improve how we do psychological assessment. We 
present the case for AI's paradigm shift to large language models to be the missing piece for a 
parallel transformation in mental health assessment from the strong reliance on rating scale 
responses. 
 
Keywords: Transformers; Artificial Intelligence; Psychology; Assessment. 
Open data, code and material: https://osf.io/7aenc/?view_only=5e9eafcd2afe4b92a8ae74d371f88314  
Funding: Oscar Kjell received funding from the Swedish Research Council (2019-06305), and Andrew Schwartz 
from DARPA Young Faculty Award (W911NF-20-1-0206) and the NSF/NIH (R01 MH125702-01). 
Conflict of Interest: Oscar Kjell and Katarina Kjell have co-founded and hold shares in a start-up using computational 
language assessments to diagnose mental health problems.  
 
 
 

 
Recently artificial intelligence-based (AI-based) text analysis has undergone a “paradigm shift” 
fundamentally changing how systems are developed in the field (1). Just a few years ago, natural 
language systems were primarily purpose-built –statistically optimized for a particular task such 
that, for example, systems for answering natural language questions (i.e., question answering) used 
a different model than that for sentiment analysis (scoring the positivity or negativity of a text) or 
paraphrasing (producing alternative phrases for a small section of text). Now, nearly all AI 
language systems are built on a large language model base or “foundational” model. The state-of-
the-art system for sentiment analysis, question answering, paraphrasing, and dozens of other 
language tasks are based on the same underlying statistical deep learning model, which only needs 
to be “fine-tuned’’ or adapted to perform particular tasks. In fact, this technology now touches the 
daily life of nearly everyone with a smartphone, as it has quickly become the basis for modern 
Web search (2), digital assistants’ language (Alexa, Siri, etc.), machine translation, and keypad 
autocompletion. 
 
The technology enabling this supposed paradigm shift is the transformer-based Large Language 
Model (1,3). Large language models owe their success in large part to their ability to statistically 
model words in the context they are used by using a particular deep learning layer, the transformer 
(3,4). Bringing such context to psychological text analysis, large language models can more 
precisely quantify the specific meaning of language and yield a truer understanding of the person 
behind the words. 
  
The link between language and psychological phenomena has long been known (5–7), and while 
the use of AI in psychology is not yet widespread, it has been used successfully to gain insights 
into, e.g., who we are (8–11), how we feel (12–14), our behaviors (15–17), and other topics (18–
21). However, quantitative assessment of the primary way humans communicate (language) has 
yet to reach widespread adoption (5)1. 
 
 
 
1 Although note that Latent Semantic Analysis, a bag-of-word approach representing words with numeric 
representations, was developed by psychology researchers in the 1990s.  

 
Even without transformers, using AI-based language analysis of probed language (i.e., open-ended 
responses to survey questions), it is possible to derive a quantified score of a psychological 
construct with moderately high convergence (r=.72) with rating scales (22). Large language 
models push the accuracy to a theoretical upper limit of predicting rating scales, upwards of r=.85 
(23). Hence, based on early empirical successes of using large language models for mental health 
assessments (e.g., (23–26)), our viewpoint suggests that this technique needs not only transform 
the field of AI, but that it is the missing piece for a corresponding transformation in psychological 
mental health assessment (Figure 1). We propose that the technique has the potential to modernize 
assessment methods from the reliance on closed-ended rating scale responses to more accurate, 
fine-grained, and ecologically valid assessments of individuals’ state-of-mind. By fully leveraging 
individuals’ personal descriptions of their mental state in their own words, the technique has the 
potential to –not only improve current assessments incrementally– but transform the very nature 
of how individuals’ state-of-minds are both measured and described, and ultimately increasing our 
understanding of mental health. 
 
We support this position by 1) reviewing 
the intrinsic advantages of natural 
language 
in 
communicating 
mental 
health, and showing how language has 
favorable measurement characteristics, 2) 
describing how word context matters in 
mental health, and reviewing how the 
unique contributions of large language 
models may realize the measurement 
precision of language, and then (3) 
provide evidence indicating how these 
advantages of large language models can make the long-held goal of grounding psychological 
mental health assessment in natural language a reality. 

 
Intrinsic Advantages of Natural Language 
A new patient is in the midst of their intake assessments, however, instead of being presented with 
a slew of questionnaires, they are presented with a prompt for a natural conversation, and starts 
talking with AssessmentTransformer (AT), an AI-based social dialog agent. 
  
 
AT starts:  
How are you feeling? 
 
Patient:  
Eh. A bit down. Just having a rest day. 
AT:  
 
Please elaborate, how are you feeling a bit down? 
  Patient:  
I’m tired and not motivated to do anything today. Work isn’t improving 
despite raising my concerns. I feel trapped, but I’m putting on a brave face 
for my family.  
 
 
An engaging interaction follows, where the patient expresses their psychological traits and states 
in their natural medium of communication. AT has a decent grasp of the psychological meaning 
of the words being expressed because it understands them in their context –that they have concerns 
with work, that feel trapped refers to things not improving, and that putting on a brave face does 
not mean they are literally putting on a mask. After the exchange, AT produces robust quantitative 
scores for the patients’ mental health (e.g., level of depression, anxiety, stress). In addition, 
qualitative explanations and descriptions of such assessments provide context to the quantitative 
scores in the patient’s own words (e.g., the level of depression relates to feeling down, tired, not 
motivated, and trapped at work).  
Could standard clinical assessments look like this in five years? 
 
 
Accurate mental health assessments are central to ensuring great care. It is a prerequisite for 
precision in healthcare: to provide the right treatment for the right person at the right time (i.e., 
precision mental health; (27)). In addition, accurate assessment is the core for improving care: It 
is the foundation to systematically develop and secure the quality of care. Intrinsically, natural 
language as a response format has ecological validity –it has long been considered a “window” 

 
into psychological states and is our natural way of communicating inner experiences and states-
of-mind (e.g., (7)).  
 
Quantitatively assessing language in an accurate manner has previously been difficult. Likert's 
(28) popular closed-ended rating scale format side-steps this by attempting to capture a one-
dimensional latent variable of individuals’ attitudes (which has been generalized to assess 
psychiatric disorders, and experiences more broadly). However, besides this being an unnatural 
way for people to communicate their states-of-mind, the scale reduces possible responses to a 
relatively narrow, fixed range and a constrained resolution –there is only a finite number of 
possible responses and scores. Advances from Classical Test Theory and Item Response Theory, 
with sufficient development cycles, have introduced methods to better select items and aggregate 
closed-ended responses into latent variables thought to better represent true states and traits 
(29,30). However, the closed-ended nature necessitated by these methods does not allow 
respondents any flexibility in expressing a state-of-mind that deviates from the posed items –
complex or unusual views are lost. They are also still limited to the inherent information loss of 
the item-response format. Compared to open-ended natural language, the rating scale method is 
overly reductionist.   
 
Information-rich language  
The idea of capturing more information can be formalized via information theory (31), whereby a 
mathematical concept of self-information can be measured as the amount of diversity that can be 
represented in a dataset. Self-information is a key measure in machine learning as it shows the 
amount of information that algorithms have at their disposal to learn. The greater the self-
information, in general, the greater the expected ability to predict variables from a given dataset 
(32). For example, a yes/no item that is answered 50% yes and 50% no in a dataset will have more 
information than one that is answered 90% yes (because the latter does not distinguish patients as 
well). Data from more item responses yield more information only if answered in a way that cannot 
be mapped from the other items. 
 

 
To examine the difference in self-information of natural language versus rating scale responses, 
we asked 100 participants2: How are you feeling?3, leaving an open response box. This was 
followed by the closed-ended rating scale, the Positive and Negative Affect Schedule (PANAS; 
(33)), comprising 20 affect-related items rated from 1=“very slightly or not at all” to 
5=“extremely”.  
 
Applying a self-information measure (the Diversity index) to both response formats (Figure 2A) 
demonstrates 4.8 times more self-information from the natural language responses (Diversity 
Index=367) as compared with the rating scale responses (Diversity Index=77). Thus, the natural 
language response is telling us more, yielding greater ability to distinguish responses, than the full 
PANAS item-response scale. As such, language comprises many favorable measurement 
characteristics including high range, resolution, dimensionality (Figure 2B–C) and openness. 
 
The range of language enables us to describe the extremes (absolutely loves and hates), while its 
resolution yields nuanced differences between (cherishes, loves, adores, likes). The multi-
dimensionality of language affords efficient and detailed communication of complex states of mind 
(love, excitement, joy, awe), which are not constrained to one dimension. The openness of natural 
language also enables creatively and fittingly construct personal ways of describing our state-of-
mind (e.g., choosing adoration or despise rather than love or hate; or communicating multi-faceted 
descriptions for a situation that researchers or clinicians may not have anticipated: it was rough 
but it’s over now). 
 
Of course, that natural language responses comprise more information than rating scales, does not 
necessarily mean they yield better measurement of any particular construct –the information may 
not be relevant to the psychological construct. Next, we focus on how large language models also 
deliver on psychological construct-relevant information. 
 
2 Participants were recruited online from Prolific. Age: M = 41.3, SD = 14.6, range = 18 – 88 years; Gender:  
females = 58, males = 42; Nationality = U.K. 
3 The question was coupled with instructions to: “Imagine that it is a dear friend you haven’t seen for a few 
weeks that asks you and that you want to answer as precisely and clearly as possible.”  

 
 
Context matters 
An important aspect of language is that words take on different meanings depending on context. 
Understanding words in their context computationally (word sense disambiguation) has been 
considered “an AI-complete problem, that is, a task whose solution is at least as hard as the most 
difficult problems in artificial intelligence” (34). The ability to contextualize word sense is very 
important for capturing different psychological dimensions. For example, consider the bolded 
words in the responses from Person A and B: 
 
Person A 
Person B 
How are you? 
I feel fine –even great!   
My life is a great mess! I’m having a 
very hard time being happy. 
What is going 
on? 
Earlier, I played the game 
Yahtzee with my partner. I  
could not get that die to roll a 1! 
Now I’m lying on my bed for a 
rest. 
My business partner was lying to me. He 
was trying to game the system and 
played me. I think I am going to die –he 
left and now I have to pay the rest of his 
fine.  
 

 
The meaning of play is different in the examples, from amusing recreational activity (Person A) 
to being taken advantage of (Person B). One would not be hard-pressed to be convinced that each 
has a different psychological meaning –the affective valence ranges from likely positive (Person 
A) to likely negative (Person B). In fact, according to the popular dictionary WordNet (35), play 
has at least 52 senses. When analyzing words outside of context, they are strikingly ambiguous, 
which is especially true for frequently used words that tend to have considerably more senses (36).4 
 
The different word senses brought out from the contexts have important connotations for the 
meaning, and thus also for psychological insights. The word order within a context is also 
important; for example, consider the difference in meaning between ”the patient loves the therapy 
session with the therapist” versus “the therapist loves the patient in the therapy session”; as 
ChatGPT notes: “In the first, … the focus is on the patient's feelings and their positive experience 
of the therapy session”, while “In the second statement, … the focus is on the therapist's feelings 
and their positive connection with the patient” with the additional context “it is not appropriate for 
a therapist to have romantic or sexual feelings for their patient”. While the goal of integrating 
context into language for psychological analysis has been sought previously (7,11,37), it was not 
possible to achieve effectively for every mention of a word prior to the large language model. 
The development of contextual word representations 
The need for word embeddings is rooted in the need to turn language data (i.e., lists of letters) into 
a quantitative form that captures word meaning with which statistical techniques can directly be 
applied. The fundamental approach to word representations is to map each word to a list of 
numbers (i.e., word vectors). The idea to capture the meaning of words numerically took off in the 
1950s with the convergence of ideas from psychology, linguistics, and computer science (Figure 
3A; (38)). Until recently, most methods utilized a bag-of-words approach whereby the order of the 
words in context is not taken into account (an obvious oversimplification that is nevertheless often 
useful). This method can be contrasted with ordered context from positional embeddings, which 
enable encoding implicit syntactic structure. Large language models are the product of a long-term 
 
4 Exactly how to partition words into discrete senses is hotly debated in linguistics, but fundamentally, the 
context of a word is essential to its meaning. For more on the debate see (34). Modern AI has cleverly 
side-stepped the sense-partitioning debate by representing meaning in a latent semantic space – similar to 
factors derived from factor analysis as we discuss in the next section. 

 
goal within AI to go beyond “bag-of-words” (e.g., see the predecessors to BERT called ELMo, 
(39)). 
 
The success of large language models to go beyond bag-of-words is attributed to deep neural 
architectures/algorithms, advances in specialized hardware, large language datasets, and their 
algorithms enabling large training sizes5. The technique enables capturing non-linear relationships 
of how words relate and interact with each other. An important part of the training process of 
transformers involves predicting a missing (masked) word within a given sequence of words. To 
succeed with the prediction, the model needs to learn syntax and associations among words; so the 
model learns from how the words are used in the training dataset. In addition, the transformer 
algorithm relies largely on attention, a mechanism that weights the effect of context words on a 
target word in a given sequence. Hence, attention enables representing the relationship between 
words in a sequence, which can capture long-term information, dependencies, and interactions of 
words in a text (Figure 3B-C). 
 
 
 
5 A recurrent neural network (RNN) might be able to get the same performance if it could parallelize its 
training routine in the same way transformers do. 

 
 
 
Large language models 
The first widely adopted large language model is called BERT (short for Bidirectional Encoder 
Representations from Transformers; (3)). Released for open use by Google in 2018, BERT has 
been followed by a family of large language models, including RoBERTa (40), GPT3 (41), and 
XLNet (42). 
 

 
Large language models brought about unprecedented accuracy increases across a wide range of 
standardized Natural Language Processing tasks (NLP; AI's subfield on language analysis), even 
surpassing the non-expert human baseline. Language models are typically evaluated and compared 
on a variety of different tasks, where two of the most common collections of standardized tests 
and benchmarks include: The General Language Understanding Evaluation (GLUE; (43)) and the 
SuperGLUE (44). The GLUE suite comprises nine and the SuperGLUE eight, carefully selected, 
standardized language understanding tasks6. The tests include diverse tasks such as sentiment 
prediction, paraphrasing, similarity, grammar control, word sense disambiguation, causal 
reasoning, commonsense reasoning, reading comprehension, natural language inference, and 
question answering. With the diverse set of tasks, GLUE and SuperGLUE favor language models 
that demonstrate ”general-purpose language understanding”(44). 
 
At this moment (September 2022), there are 20 different models that surpass the performances of 
humans. Table 1 presents examples of standardized NLP tasks from GLUE along with person-
level language tasks (such as assessing depression and suicide risk as presented later), describing 
top-performing approaches and their performance. All top-performing approaches include large 
language models, for both standard NLP tasks and person-level tasks.
 
6 In these tasks, the test answers are not available to the public, so researchers submit their models’ answers 
for 
testing, 
and 
the 
results 
are 
then 
presented 
publicly 
on 
leaderboards 
(https://gluebenchmark.com/leaderboard) 

 
 
Table 1. Examples of Standard NLP and Person-Level Language Tasks: Top Performing Systems and their Performance 
Standard Document-Level NLP tasks (GLUE) 
Person-Level Psychological Tasks 
Task 
Top performing approachA 
Performance 
Task 
Top performing approach 
Performance 
Sentiment (SST-2) 
Is a review of a movie 
positive or negative? 
Transformer-architecture for 
large-scale knowledge 
enhanced pre-training 
 
(EARNIE1) 
Accuracy = .978 
 
 
GLUE Human 
baselineB = .978 
Assessing depression 
using Twitter data (from a 
shared task (CLPsych 2015 
(45). 
Transformers (MentalRoBERTa4, 
see also 5)  
 
RoBERTA fine tuned on mental 
health related Reddit data.     
F1 = .697 
Paraphrase (MRPC) 
Is sentence B a 
paraphrase of sentence 
A? 
Transformers 
decoding-enhanced BERT with 
disentangled attention 
 
(DeBERTa / Turing NLR v42) 
F1 = .940 
Accuracy = .920 
 
GLUE Human 
baselineB = .863/.808 
Assessing suicide risk 
using Reddit data from the 
SuicideWatch online forum 
(Suicide forums only), and 
users’ other Reddit posts 
(Suicide + all forums)  
Suicide forums only:  
Transformers (Multifeature Fusion 
Attention Network)6 
Suicide + all forums: Transformers 
with multi-level dual-context 
language and BERT7 
Suicide forums only:  
F1  = .514 
 
Suicide + all 
forums:  
F1  = .457 
Similarity (STS-B) 
How similar are the two 
sentences A and B? 
Transformers-architecture with 
efficient denoising pretraining 
 
(METRO / Turing NLR v53) 
Pearson  r = .937 
 
GLUE Human 
baselineB = .927  
Assessing personality  
from social media (Facebook) 
language 
Transformers (word- and message-
level attention in combination with 
past approaches)9 
Disattenuated 
Pearson r = .54–.66 
Acceptability (CoLA) 
Is a sentence 
grammatical or 
ungrammatical? 
Transformer-architecture for 
large-scale knowledge 
enhanced pre-training 
(EARNIE1) 
Mathew’s 
Correlation  = .738 
GLUE Human 
baselineB = .664 
Assessing well-being (harmony 
in life)  
from probed language 
Transformers (BERT)8 
Pearson r = .85 
 
Dissattenuated 
Pearson r = 1.00 
Notes. SST-2 = The Stanford Sentiment Treebank; MRPC = The Microsoft Research Paraphrase Corpus; STS-B = The Semantic Textual Similarity Benchmark (STS-B); CoLA 
= The Corpus of Linguistic Acceptability. 
A Top performing systems are selected from the GLUE leaderboard (https://gluebenchmark.com/leaderboard), where the system needs to be in top 50 overall and be described 
with a URL and accompanied with a manuscript describing the system.  
B GLUE Human baseline = “a conservative estimate of human performance”,  where the participants/annotators were non-experts recruited through crowdsourcing (46). 
1 ERNIE (https://github.com/PaddlePaddle/ERNIE (47);  2 DeBERTa (https://github.com/microsoft/DeBERTa (48); 3 METRO / Turing NLR v5 
(https://arxiv.org/abs/2204.06644, (49); 4 MentalRoBERTa (50); 5   (51) shows that RoBERTA performs more accurately than non-transformers; 6  (52); 7 (24); 8 (23); 9 (53). 

 
 
Leveraging big data information for small samples 
The typical focus in NLP is to model language itself using huge amounts of data and to employ 
these language models to solve language tasks (e.g., GLUE-tasks). However, human-level AI 
models the individual behind the language (54,55). Modeling a person behind a text may include 
assessing their depression or suicide risk (54). Importantly, to apply transformers in clinical 
sciences, a huge amount of participant-generated data is not required. First, small language 
samples can be submitted to a pre-trained model, based on a large language model, trained for 
producing a psychological score. Existing large language models can be downloaded and applied 
using both Python (e.g., see DLATK; (56)) and R (e.g., see text; (57)). Second, developing 
predictive models with relatively small sample sizes can be achieved through dimensionality 
reduction of word embeddings (54); this has, for example, been done when predicting 
demographics (age, gender), personality (extraversion, openness), and mental health (suicide risk), 
with as low as 50 participants (and results approaching large-scale model accuracies with as few 
as 500  participants). 
 
Further, large language models can handle multiple languages, such as multilingual BERT 
(mBERT), which was trained on the top 104 languages on Wikipedia, from English and Mandarin 
Chinese to Aragonese and Tagalog. For mental health researchers and practitioners, this not only 
opens up the possibility to use the techniques in many different languages but also the potential 
for new types of cross-cultural research; for example, mBERT has been employed to study 
misinformation about COVID-19 and health on social media in English, Arabic, and Bulgarian 
(58). 
Psychological Insights through Contextualized Language 
Contextualized language, we argue, is the natural way of expressing and understanding complex 
psychological phenomena. Language plays a central role in processing and structuring emotions 
and thoughts introspectively and in communicating them to others. Language helps us sort 
memories to remember the past and plan the future. We cooperate, learn and teach through 
language. Language can also be destructive and violent: A tool in arguments, manipulation, and 

 
deceit. These are all central parts of human life –so ignoring the context and structure of language 
data misses vital information and reduces ecological validity.  
Ecological contextualized language   
Large language models have been instrumental in recent AI-models predicting clinically relevant 
outcomes from individuals’ naturally occurring text. This is demonstrated in the recently shared 
tasks of predicting suicide risks from Proceedings of Computational Linguistics and Clinical 
Psychology (CLPsych). The computer science research community has a tradition of arranging 
shared tasks, where researchers work on common tasks with the same datasets to develop 
competing methods to identify mental health disorders and related issues: The winning model(s) 
of a shared task is typically considered state-of-the-art. Prior transformers, deep learning 
techniques were not able to benefit language-based predictions of mental health tasks (59). In 
contrast, the two top-performing models included contextual word embeddings in the CLPsych 
shared task in 2019 (24–26). The shared task included 15 research teams and focused on the 
extremely difficult task of predicting individual suicide risk from (de-identified) Reddit data, 
where large language model-based techniques were able to reduce error by 12.7–56.6% over strong 
baselines. This is further evidence of a shift where the latest in AI techniques (i.e., deep learning), 
in general, started giving a win in NLP for psychology. 
Probed contextualized language  
It might not come as a surprise, but none of the participants in our study communicated their 
response to the question: How are you feeling?, with a numeric response (e.g., I’m a 7 on a scale 
from 1 to 10). Neither did anyone only use descriptive words meant to be interpreted without any 
context (e.g., a list of words: happy, excited, balanced). All respondents used natural language to 
describe their state-of-mind, and in fact only 13 of the 100 participants used at least one of the 
words from the items of the PANAS rating scale commonly used to measure feelings. The main 
focus of self-report assessments is to measure/quantify the degree of a psychological construct. 
The typical rating scales of self-report measures comprising frequency labels (e.g. 0=Not at all to 
4=Nearly every day; (60)) or agreement labels (e.g. Strongly disagree to Strongly agree), can be 
replaced by probes for language. Kjell et al. (22) found that probed language-based assessments 

 
of well-being and mental health can produce scores correlating with closed-ended rating scales 
upwards of Pearson r=.72.  
 
Using large language models, language-based assessments have been shown to approach the 
theoretical upper limits in convergence with standard psychological assessments of well-being –
the measures' own reliability. Kjell et al., (23) achieved a Pearson r=.85 for language-based 
assessment with corresponding rating scale scores for the harmony in life scale. This correlation 
is stronger than the scales’ own inter-item-correlation average (r=.76), test-retest reliability 
(r=.71–.77), and it is in line with the item-total average correlation (r=.84). Using transformers 
particularly improved the prediction from text responses rather than descriptive word responses:  
AI language analyses are now at the point where a quantitative score can be derived from natural 
language responses without sacrificing accuracy as measured by rating scales. The large language 
models behind this advance can become a widespread alternative in digital mental health, an 
avenue for modernizing the self-report of mental health assessments and ultimately improving our 
understanding of psychiatric conditions and human experiences more broadly. 
Beyond rating scales: True scores   
Text-responses contain valuable information that large language models can extract into scores 
converging with validated rating scales (23). However, rating scales themselves are only an 
observable “proxy” and not a perfect true score (Figure 1).  Most psychometric theories, such as 
classical test theory (e.g., (61)) or item-response theory (62), view self-report responses as an 
approximation of the true latent variable that is sought. Therefore, the validity of language-based 
assessments and rating scales should go beyond evaluating their convergence. This has so far only 
been studied in a few studies; one such study compared the two methods’ ability to accurately 
categorize external stimuli of pictures depicting facial expressions including sad, happy, and 
contemptuous (22). It was found that language-based assessment (based on the bag-of-words 
approach) more accurately categorizes facial expressions as compared with rating scales. Further, 
a study focusing on theoretically relevant behaviors –cooperation– to harmony in life, showed that 
language-based assessments significantly correlated (Pearson’s r=.18; and r=.35 in individuals 
categorized as prosocials) with cooperative behaviors, whereas the corresponding rating scale (63) 
did not (16). 
 

 
Beyond rating scales: Descriptions more than a score  
Language-based assessments also have the ability to be self-descriptive, moving beyond mere 
scores as the output. For example, statistically significant descriptive words and key phrases can 
be visualized based on their underlying meaning along relevant dimensions such as low versus 
high scores of personality traits (11), depression (64), or in relation to behaviors such as 
cooperation (16). Figure 4A shows AI-generated summaries of the ten most negative and positive 
answers to How are you feeling? from our study7. Figure 4B further demonstrates the power of 
large language models to understand contextualized language and produce psychologically 
nuanced content. 
 
 
7 The NLP analyses were done in R using the text-package. First, individuals’ text responses to how they 
were feeling were transformed into word embeddings using the transformer, roberta-large. Then, the 
affective valence of each response was estimated using an AI valence estimator trained on an open affect 
dataset (65), with our estimator having a cross-validated accuracy with raters of Pearson r =.74. Lastly, the 
responses were divided into those that were positive and negative and then summarized using the large 
transformer language model t5-large. See the open code for details. 

 
 
Biases, Risks, and Ethical Considerations 
The weaknesses and strengths of transformers come with ethical considerations and 
responsibilities. It is important to distinguish that we are advocating for large language models for 
assessment as a class of techniques rather than advocating that all instances of such models should 
be trusted for assessment. As a class of techniques, large language model assessments have 
demonstrated validity and reliability on par or better than rating scales, but any particular instance 
of such an assessment should go through a thorough evaluation for validity (including bias) and 
reliability before use in clinical practice just as any rating scale should. These advances in 

 
measurement validity still come with psychological measurement ethics considerations:  validation 
for specific populations and use contexts, mitigation of biases in models, consideration of who 
receives the scores (clinicians versus participants), and protocols to keep participant identities 
secure. There is a growing interest in ethical principles and frameworks for developing and 
deploying AI (66,67), and no reason that psychometric standards cannot be met.  
 
Leidner and Plachouras (68) specifically discuss ethical challenges to NLP, where they emphasize 
that ethical values should be incorporated in both the development and the application of NLP; 
this, for example, includes considering biases in large-scale language models (69,70), privacy in 
regard to the open-ended language format and increased predictive power. Thus, it is important to 
undertake extra privacy-preservation steps, including extra data access restrictions and thorough 
checks for identity if seeking to share data (see (71) for a thorough discussion). It is out of the 
scope of this perspective to describe these resources at length.  
Conclusions 
We suggest that the more precise language scores provided by large language models can 
transform how mental health is assessed by enabling patients and study participants to respond in 
their own words whilst resulting in large improvements in accuracy, and expanded scope of 
insights. Many studies already collect open-ended responses for qualitative review and such 
techniques can be used to complement traditional rating scales while being established. Further, 
there are many resources assisting in enabling the application of these methods to mainstream 
mental health research (Python library DLATK; (56); R-package text;(57)). We believe that AI's 
paradigm shift to transformers is the missing piece for a parallel transformation in psychology 
from the nearly ubiquitous reliance on rating scale responses to more accurate, fine-grained, and 
ecologically valid assessment by fully leveraging participants’ own words. 
 
 

 
 
References 
1. Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, et al. On the opportunities and 
risks of foundation models. ArXiv Prepr ArXiv210807258. 2021; 
2. Nayak P. Understanding searches better than ever before [Internet]. Google. 2019 [cited 2022 Dec 3]. 
Available from: https://blog.google/products/search/search-language-understanding-bert/ 
3. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers 
for Language Understanding. In: Proceedings of the 2019 Conference of the North American Chapter 
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long 
and Short Papers) [Internet]. Minneapolis, Minnesota: Association for Computational Linguistics; 
2019. p. 4171–86. Available from: https://www.aclweb.org/anthology/N19-1423 
4. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. 
In: Advances in neural information processing systems. 2017. p. 5998–6008. 
5. Boyd RL, Schwartz HA. Natural language analysis and the psychology of verbal behavior: The past, 
present, and future states of the field. J Lang Soc Psychol. 2021;40(1):21–41. 
6. Pennebaker JW, Mehl MR, Niederhoffer KG. Psychological aspects of natural language use: Our 
words, our selves. Annu Rev Psychol. 2003;54(1):547. 
7. Tausczik YR, Pennebaker JW. The psychological meaning of words: LIWC and computerized text 
analysis methods. J Lang Soc Psychol. 2010;29(1):24–54. 
8. Argamon S, Koppel M, Pennebaker JW, Schler J. Mining the blogosphere: Age, gender and the 
varieties of self-expression. First Monday. 2007; 
9. Berger J, Packard G. Using natural language processing to understand people and culture. Am Psychol. 
2021; 
10. Kwantes PJ, Derbentseva N, Lam Q, Vartanian O, Marmurek HHC. Assessing the Big Five personality 
traits with latent semantic analysis. Personal Individ Differ. 2016 Nov 1;102:229–33. 
11. Schwartz HA, Eichstaedt JC, Kern ML, Dziurzynski L, Ramones SM, Agrawal M, et al. Personality, 
gender, and age in the language of social media: The open-vocabulary approach. PloS One. 
2013;8(9):e73791. 
12. De Bruyne L, Atanasova P, Augenstein I. Joint emotion label space modeling for affect lexica. Comput 
Speech Lang. 2022;71:101257. 
13. Eichstaedt JC, Smith RJ, Merchant RM, Ungar LH, Crutchley P, Preoţiuc-Pietro D, et al. Facebook 
language predicts depression in medical records. Proc Natl Acad Sci. 2018;115(44):11203–8. 
14. Sun J, Schwartz HA, Son Y, Kern ML, Vazire S. The language of well-being: Tracking fluctuations in 
emotion experience through everyday speech. J Pers Soc Psychol. 2020;118(2):364. 
15. Curtis B, Giorgi S, Buffone AEK, Ungar LH, Ashford RD, Hemmons J, et al. Can Twitter be used to 
predict county excessive alcohol consumption rates? PLOS ONE. 2018;13(4):e0194290. 
16. Kjell O, Daukantaitė D, Sikström S. Computational Language Assessments of Harmony in Life—Not 
Satisfaction With Life or Rating Scales—Correlate With Cooperative Behaviors. Front Psychol. 
2021;12. 
17. Macavaney S, Mittu A, Coppersmith G, Leintz J, Resnik P. Community-level research on suicidality 
prediction in a secure environment: Overview of the CLPsych 2021 shared task. In: Proceedings of the 
Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access. 2021. 
p. 70–80. 
18. Eichstaedt JC, Kern ML, Yaden DB, Schwartz HA, Giorgi S, Park G, et al. Closed-and Open-
Vocabulary Approaches to Text Analysis: A Review, Quantitative Comparison, and 
Recommendations. Psychol Methods. 2020; 
19. Iliev R, Dehghani M, Sagi E. Automated text analysis in psychology: Methods, applications, and future 
developments. Lang Cogn. 2015;7(2):265–90. 
20. Jackson J, Watts J, List JM, Puryear C, drabble  ryan, Lindquist K. From Text to Thought: How 
Analyzing Language Can Advance Psychological Science. Perspect Psychol Sci. 2021 Mar 3; 

 
21. Schwartz HA, Ungar LH. Data-driven content analysis of social media: a systematic overview of 
automated methods. Ann Am Acad Pol Soc Sci. 2015;659(1):78–94. 
22. Kjell ON, Kjell K, Garcia D, Sikström S. Semantic measures: Using natural language processing to 
measure, differentiate, and describe psychological constructs. Psychol Methods. 2019;24(1):92. 
23. Kjell ONE, Sikström S, Kjell K, Schwartz HA. Natural language analyzed with AI-based transformers 
predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. 
Sci Rep. 2022 Mar 10;12(1):3918. 
24. Matero M, Idnani A, Son Y, Giorgi S, Vu H, Zamani M, et al. Suicide risk assessment with multi-level 
dual-context language and bert. In: Proceedings of the sixth workshop on computational linguistics and 
clinical psychology. 2019. p. 39–44. 
25. Mohammadi E, Amini H, Kosseim L. CLaC at CLPsych 2019: Fusion of neural features and predicted 
class probabilities for suicide risk assessment based on online posts. In: Proceedings of the Sixth 
Workshop on Computational Linguistics and Clinical Psychology. 2019. p. 34–8. 
26. Zirikly A, Resnik P, Uzuner O, Hollingshead K. CLPsych 2019 shared task: Predicting the degree of 
suicide risk in Reddit posts. In: Proceedings of the sixth workshop on computational linguistics and 
clinical psychology. 2019. p. 24–33. 
27. Delgadillo J, Lutz W. A development pathway towards precision mental health care. JAMA Psychiatry. 
2020;77(9):889–90. 
28. Likert R. A technique for the measurement of attitudes. Arch Psychol. 1932;22 140:55. 
29. Lord FM. Applications of item response theory to practical testing problems. Routledge; 2012. 
30. Thomas ML. The value of item response theory in clinical assessment: a review. Assessment. 
2011;18(3):291–307. 
31. Shannon CE. A mathematical theory of communication. Bell Syst Tech J. 1948;27(3):379–423. 
32. MacKay DJC. Information Theory, Inference, and Learning Algorithms. Camb Univ Press. :640. 
33. Watson D, Clark LA, Tellegen A. Development and validation of brief measures of positive and 
negative affect: The PANAS scales. J Pers Soc Psychol. 1988;54(6):1063–70. 
34. Navigli R. Word sense disambiguation: A survey. ACM Comput Surv CSUR. 2009;41(2):1–69. 
35. Miller GA. WordNet: a lexical database for English. Commun ACM. 1995;38(11):39–41. 
36. Resnik P. Using information content to evaluate semantic similarity in a taxonomy. ArXiv Prepr Cmp-
Lg9511007. 1995; 
37. Landauer TK. Latent semantic analysis: A theory of the psychology of language and mind. Discourse 
Process. 1999;27(3):303–10. 
38. Jurafsky D, Martin JH. Speech and Language Processing: An Introduction to Natural Language 
Processing, Computational Linguistics, and Speech Recognition [Internet]. 2020. Available from: 
https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf 
39. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, et al. Deep Contextualized Word 
Representations. In: Proceedings of the 2018 Conference of the North American Chapter of the 
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) 
[Internet]. New Orleans, Louisiana: Association for Computational Linguistics; 2018 [cited 2022 Jan 
18]. p. 2227–37. Available from: https://aclanthology.org/N18-1202 
40. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. Roberta: A robustly optimized bert pretraining 
approach. ArXiv Prepr ArXiv190711692. 2019; 
41. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language models are few-shot 
learners. ArXiv Prepr ArXiv200514165. 2020; 
42. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. Xlnet: Generalized autoregressive 
pretraining for language understanding. In: Advances in neural information processing systems. 2019. 
p. 5754–64. 
43. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman SR. GLUE: A multi-task benchmark and 
analysis platform for natural language understanding. ArXiv Prepr ArXiv180407461. 2018; 
44. Wang A, Pruksachatkun Y, Nangia N, Singh A, Michael J, Hill F, et al. Superglue: A stickier 
benchmark for general-purpose language understanding systems. Adv Neural Inf Process Syst. 

 
2019;32. 
45. Coppersmith G, Dredze M, Harman C, Hollingshead K, Mitchell M. CLPsych 2015 shared task: 
Depression and PTSD on Twitter. In: Proceedings of the 2nd workshop on computational linguistics 
and clinical psychology: from linguistic signal to clinical reality. 2015. p. 31–9. 
46. Nangia N, Bowman SR. Human vs. muppet: A conservative estimate of human performance on the 
GLUE benchmark. ArXiv Prepr ArXiv190510425. 2019; 
47. Sun Y, Wang S, Feng S, Ding S, Pang C, Shang J, et al. ERNIE 3.0: Large-scale Knowledge Enhanced 
Pre-training for Language Understanding and Generation [Internet]. arXiv; 2021 [cited 2022 Sep 30]. 
Available from: http://arxiv.org/abs/2107.02137 
48. He P, Liu X, Gao J, Chen W. DeBERTa: Decoding-enhanced BERT with Disentangled Attention 
[Internet]. arXiv; 2021 [cited 2022 Sep 30]. Available from: http://arxiv.org/abs/2006.03654 
49. Bajaj P, Xiong C, Ke G, Liu X, He D, Tiwary S, et al. METRO: Efficient Denoising Pretraining of 
Large Scale Autoencoding Language Models with Model Generated Signals [Internet]. arXiv; 2022 
[cited 2022 Sep 30]. Available from: http://arxiv.org/abs/2204.06644 
50. Ji S, Zhang T, Ansari L, Fu J, Tiwari P, Cambria E. Mentalbert: Publicly available pretrained language 
models for mental healthcare. ArXiv Prepr ArXiv211015621. 2021; 
51. Matero M, Hung A, Schwartz HA. Understanding RoBERTa’s Mood: The Role of Contextual-
Embeddings as User-Representations for Depression Prediction. ArXiv Prepr ArXiv211213795. 2021; 
52. Li J, Zhang S, Zhang Y, Lin H, Wang J. Multifeature fusion attention network for suicide risk 
assessment based on social media: algorithm development and validation. JMIR Med Inform. 
2021;9(7):e28227. 
53. Lynn V, Balasubramanian N, Schwartz HA. Hierarchical modeling for user personality prediction: The 
role of message-level attention. In: Proceedings of the 58th Annual Meeting of the Association for 
Computational Linguistics. 2020. p. 5306–16. 
54. Ganesan AV, Matero M, Ravula AR, Vu H, Schwartz HA. Empirical evaluation of pre-trained 
transformers for human-level nlp: The role of sample size and dimensionality. In: Proceedings of the 
conference Association for Computational Linguistics North American Chapter Meeting. NIH Public 
Access; 2021. p. 4515. 
55. Soni N, Matero M, Balasubramanian N, Schwartz HA. Human Language Modeling. ArXiv Prepr 
ArXiv220505128. 2022; 
56. Schwartz HA, Giorgi S, Sap M, Crutchley P, Ungar L, Eichstaedt J. Dlatk: Differential language 
analysis toolkit. In 2017. p. 55–60. 
57. Kjell O, Schwartz HA, Giorgi S. Text: an R-package for analyzing and visualizing human language 
using natural language processing and deep learning. 2021; 
58. Panda S, Levitan SI. Detecting multilingual COVID-19 misinformation on social media via 
contextualized embeddings. In: Proceedings of the Fourth Workshop on NLP for Internet Freedom: 
Censorship, Disinformation, and Propaganda. 2021. p. 125–9. 
59. Lynn V, Goodman A, Niederhoffer K, Loveys K, Resnik P, Schwartz HA. CLPsych 2018 shared task: 
Predicting current and future psychological health from childhood essays. In: Proceedings of the Fifth 
Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic. 2018. p. 
37–46. 
60. Kroenke K, Spitzer RL. The PHQ-9: a new depression diagnostic and severity measure. Psychiatr Ann. 
2002;32(9):1–7. 
61. Novick MR. The axioms and principal results of classical test theory. J Math Psychol. 1966;3(1):1–18. 
62. Reise SP, Waller NG. Item response theory and clinical measurement. Annu Rev Clin Psychol. 
2009;5(1):27–48. 
63. Kjell ONE, Daukantaitė D, Hefferon K, Sikström S. The Harmony in Life Scale Complements the 
Satisfaction with Life Scale: Expanding the Conceptualization of the Cognitive Component of 
Subjective Well-Being. Soc Indic Res. 2016 Mar;126(2):893–919. 
64. Kjell K, Johnsson P, Sikström S. Freely Generated Word Responses Analyzed With Artificial 
Intelligence Predict Self-Reported Symptoms of Depression, Anxiety, and Worry. Front Psychol. 2021 

 
Jun 4;12:602581. 
65. Preoţiuc-Pietro D, Schwartz HA, Park G, Eichstaedt J, Kern M, Ungar L, et al. Modelling valence and 
arousal in facebook posts. In: Proceedings of the 7th workshop on computational approaches to 
subjectivity, sentiment and social media analysis. 2016. p. 9–15. 
66. Jobin A, Ienca M, Vayena E. The global landscape of AI ethics guidelines. Nat Mach Intell. 
2019;1(9):389–99. 
67. Peters D, Vold K, Robinson D, Calvo RA. Responsible AI—two frameworks for ethical design 
practice. IEEE Trans Technol Soc. 2020;1(1):34–47. 
68. Leidner JL, Plachouras V. Ethical by design: Ethics best practices for natural language processing. In: 
Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. 2017. p. 30–40. 
69. Kurita K, Vyas N, Pareek A, Black AW, Tsvetkov Y. Measuring Bias in Contextualized Word 
Representations. In: Proceedings of the First Workshop on Gender Bias in Natural Language 
Processing. 2019. p. 166–72. 
70. Shah DS, Schwartz HA, Hovy D. Predictive Biases in Natural Language Processing Models: A 
Conceptual Framework and Overview. In: Proceedings of the 58th Annual Meeting of the Association 
for Computational Linguistics [Internet]. Online: Association for Computational Linguistics; 2020 
[cited 2021 Dec 1]. p. 5248–64. Available from: https://aclanthology.org/2020.acl-main.468 
71. Lison P, Pilán I, Sanchez D, Batet M, Øvrelid L. Anonymisation Models for Text Data: State of the 
art, Challenges and Future Directions. In: Proceedings of the 59th Annual Meeting of the Association 
for Computational Linguistics and the 11th International Joint Conference on Natural Language 
Processing (Volume 1: Long Papers) [Internet]. Online: Association for Computational Linguistics; 
2021 [cited 2021 Dec 1]. p. 4188–203. Available from: https://aclanthology.org/2021.acl-long.323 
72. Markov AA. Essai d’une Recherche Statistique Sur le Texte du Roman ‘Eugène Oneguine’. Bull Acad 
Imper Sci St Petersburg. 1913;7. 
73. Osgood CE. The nature and measurement of meaning. Psychol Bull. 1952 May;49(3):197–237. 
74. Switzer P. Vector images in document retrieval. Stat Assoc Methods Mech Doc. 1964;163–71. 
75. Jelinek F, Bahl L, Mercer R. Design of a linguistic statistical decoder for the recognition of continuous 
speech. IEEE Trans Inf Theory. 1975;21(3):250–6. 
76. Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R. Indexing by latent semantic 
analysis. J Am Soc Inf Sci. 1990;41(6):391. 
77. Brown PF, Della Pietra VJ, Desouza PV, Lai JC, Mercer RL. Class-based n-gram models of natural 
language. Comput Linguist. 1992;18(4):467–80. 
78. Blei DM, Ng AY, Jordan MI. Latent dirichlet allocation. J Mach Learn Res. 2003;3(Jan):993–1022. 
79. Bengio Y, Ducharme R, Vincent P, Jauvin C. A neural probabilistic language model. J Mach Learn 
Res. 2003;3(Feb):1137–55. 
80. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks 
with multitask learning. In: Proceedings of the 25th international conference on Machine learning. 
2008. p. 160–7. 
81. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases 
and their compositionality. In 2013. p. 3111–9. 

