Scalable Generative Models for Multi-label Learning with Missing Labels
Vikas Jain 1 * Nirbhay Modhe 1 * Piyush Rai 1
Abstract
We present a scalable, generative framework for
multi-label learning with missing labels.
Our
framework consists of a latent factor model for
the binary label matrix, which is coupled with
an exposure model to account for label missing-
ness (i.e., whether a zero in the label matrix is
indeed a zero or denotes a missing observation).
The underlying latent factor model also assumes
that the low-dimensional embeddings of each la-
bel vector are directly conditioned on the respec-
tive feature vector of that example. Our gener-
ative framework admits a simple inference pro-
cedure, such that the parameter estimation re-
duces to a sequence of simple weighted least-
square regression problems, each of which can be
solved easily, efﬁciently, and in parallel. More-
over, inference can also be performed in an on-
line fashion using mini-batches of training ex-
amples, which makes our framework scalable for
large data sets, even when using moderate com-
putational resources. We report both quantita-
tive and qualitative results for our framework on
several benchmark data sets, comparing it with a
number of state-of-the-art methods.
1. Introduction
Multi-label learning (Gibaja & Ventura, 2015; 2014) is the
problem of assigning to an object a subset of labels from a
potentially very large label vocabulary (Prabhu & Varma,
2014; Jain et al., 2016; Babbar & Sch¨olkopf, 2017). In
contrast to binary or multi-class classiﬁcation, in multi-
label learning, each example is associated with a binary
label vector (potentially very large), denoting the pres-
ence/absence (relevance/irrelevance) of each label. Multi-
label learning has applications in several domains such as
computer vision (Wang et al., 2016), computational adver-
*Equal contribution
1Department of Computer Science and
Enginerring, IIT Kanpur, Kanpur 208016, UP, India.
Corre-
spondence to: Vikas Jain <vikasj@iitk.ac.in>, Nirbhay Modhe
<nirbhaym@iitk.ac.in>, Piyush Rai <piyush@cse.iitk.ac.in>
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
tising and recommender systems (Prabhu & Varma, 2014;
Jain et al., 2016), etc.
Several state-of-the-art methods for multi-label learning are
based on certain structural assumptions on the binary la-
bel matrix. Some of the key structural assumptions that
have been used in prior work include low-rank assump-
tion (Yu et al., 2014), locally low-rank assumption (Bha-
tia et al., 2015), and low-rank plus sparse assumption (Xu
et al., 2016), and clusters/topics of labels assumption (Ciss´e
et al., 2016; Rai et al., 2015). Models based on these as-
sumptions are broadly dubbed as embedding based meth-
ods for multi-label learning and offer two key advantages:
(1) The relatedness/correlation among labels can be easily
modeled/captured, and (2) the label vector for each exam-
ple can be represented as a low-dimensional embedding,
which faciliates developing computationally scalable mod-
els for multi-label learning. A more detailed discussion of
prior work is provided in the Related Work section.
Despite the considerable recent interest and progress on
the problem of multi-label learning (Yu et al., 2014; Bhatia
et al., 2015; Wang et al., 2016; Ciss´e et al., 2016), a number
of important issues still remain. One of such issues, espe-
cially for the embdding based methods, is the ambiguity
regarding the zeros vs unobserved (missing) entries in the
binary label vector of each example. Since, in practice, the
true value (0/1) for only a small subset of all the labels can
be obtained, the zeros in the label vector do not necessarily
represent negative labels. A typical heuristic employed by
multi-label learning algorithms is to simply treat all such
the zeros in the label vector as are true negatives (Yu et al.,
2014). Another heuristic is to assign different weights to
the zeros and ones in the binary label matrix (Yu et al.,
2017), which is inspired by matrix factorization based col-
laborative ﬁltering models that learn from implicit (binary)
feedback data (Hu et al., 2008). However, a more princi-
pled strategy to address this issue is highly desirable.
Another important desideratum is scalability, especially in
the case of extreme multi-label learning problems (Prabhu
& Varma, 2014; Jain et al., 2016; Babbar & Sch¨olkopf,
2017), which are characterized by a massive number of la-
bels, features, and examples. Although a number of recent
multi-label learning models have been proposed that can
scale to large-scale problems, these models usually require

Scalable Generative Models for Multi-label Learning with Missing Labels
large computational resources to truly scale to massive data
sets (Babbar & Sch¨olkopf, 2017; Bhatia et al., 2015; Jain
et al., 2016). Moreover, most of the scalable multi-label
learning algorithms only operate in batch setting and are
usually not designed to work (Prabhu & Varma, 2014; Bha-
tia et al., 2015; Jain et al., 2016) in online settings with
continuous stream of training examples.
In this paper, we present a scalable, generative framework
for multi-label learning, that not only bring to bear the
modeling ﬂexibility of probabilistic, generative models for
the multi-label learning problem (Kapoor et al., 2012; Rai
et al., 2015), but is also designed to handle the above-
mentioned challenges in a principled way. Our framework
is based on a latent factor model for the binary label matrix,
and has the following distinguishing aspects: (1) It natu-
rally handles the issue of missing vs negative labels via a
principled generative model with a exposure model (Liang
et al., 2016) for the label matrix; (2) It is accompanied by
a simple and scalable inference procedure (both via Gibbs
sampling and via fast point estimation); and (3) Inference
can also be easily performed in an online fashion, enabling
us to apply it on large-scale problems, even when using
moderate computational resources.
2. The Model
In the multi-label learning problem, we assume that we are
given N training examples {(x1, y1), . . . , (xN, yN)} with
xn ∈RD and yn ∈{0, 1}L, n = 1, . . . , N. We will de-
note X = {x1, . . . , xN} ∈RN×D to be the feature matrix
and Y = {y1, . . . , yN} ∈{0, 1}N×L to be the label ma-
trix. Given training data {X, Y}, the goal in multi-label
learning is to learn a model that can predict the label vector
y∗∈{0, 1}L for a new test input x∗∈RD.
Note that an entry ynℓ= 0 in the label matrix Y may not
necessarily mean a negative label but could simply mean
that this label is missing (and its true value could be 0 or 1).
As we shall show, our generative model can infer the miss-
ingness of a label ynℓ= 0 by associating another binary
latent variable ξnℓ(called exposure variable). These expo-
sure variables will be incorporated in a latent factor model
(Sec. 2.1) for the label matrix Y and are jointly learned
along with the rest of the model parameters.
2.1. An Exposure-based Latent Factor Model for the
Binary Label Matrix
We model the binary label matrix Y using a latent fac-
tor model. Speciﬁcally, we assume that each training ex-
ample n = 1, . . . , N is associated with a latent factor
un ∈RK and each label ℓ= 1, . . . , L is associated
with a latent factor vℓ∈RK. We further condition un
on the feature vector xn ∈RD of example n by as-
suming that the prior distribution of un is conditioned on
xn, as p(un|xn) = N(un|Wxn, λ−1
u IK). Here, W =
[w1, . . . , wK]⊤∈RK×D which denotes the matrix of re-
gression weights that map the feature vector xn to the mean
of the Gaussian prior on un. We further assume a zero-
mean Gaussian prior p(vℓ) = N(vℓ|0, λ−1
v IK) on label
latent factors vℓ, ℓ= 1, . . . , L. Note that, although we do
not consider it here, our model can also be easily extended
to incorporate label features (if available) by conditioning
Gaussian prior on vℓon those label features, in the same
manner we condition the prior on un on input features.
Figure 1. Our generative model in plate notation. Note: Hyperpa-
rameters not shown for brevity.
The complete generative story for each label ynℓof the bi-
nary label matrix Y is given by
un|xn
∼
N(un|Wxn, λ−1
u IK)
(1)
vℓ
∼
N(vℓ|0, λ−1
v IK)
(2)
ξnℓ
∼
Bernoulli(µnℓ)
(3)
ynℓ
∼
(
Bernoulli
 ynℓ|σ(u⊤
n vℓ)

,
if ξnℓ= 1
δ0,
if ξnℓ= 0
(4)
where σ(z) = 1/(1 + exp(−z)) denotes the logistic func-
tion. Note that we have associated a binary exposure la-
tent variable ξnℓwith each label ynℓsuch that ξnℓ= 0
implies that ynℓis 0 because it is missing (not exposed),
and ξnℓ= 1 implies that ynℓis exposed (and could be 0
or 1 depending on the outcome of the Bernoulli draw). In
Eq. 4, δ0 denotes a point-mass at zero, which means that, if
ξnℓ= 0, then ynℓis zero with probability 1. Otherwise, we
draw the observed label ynℓfrom a Bernoulli distribution
as ynℓ∼Bernoulli(ynℓ|σ(u⊤
n vℓ)). Note that, effectively,
each ynℓis being modeled using a mixture of two distri-
butions - a Bernoulli with probability given by the sigmoid
σ(u⊤
n vℓ)) and a point-mass at 0.
ynℓ∼ξnℓBern
 ynℓ|σ(u⊤
n vℓ)

+(1 −ξnℓ)I[ynℓ= 0] (5)
Note that the latent variable ξnℓdecides which of the two
distributions from this mixture generates ynℓ.
Figure 1
shows our model in the plate notation. Also note that if
ynℓ= 1 then ξnℓ= 1 with probability 1 and therefore ξnℓ
only needs to be inferred for entries for which ynℓ= 0.

Scalable Generative Models for Multi-label Learning with Missing Labels
The generative model speciﬁed in Eq (1)-(4) has two addi-
tional parameters: W = [w1, . . . , wK]⊤∈RK×D which
denotes the matrix of regression weights that map each
input feature vector xn to the corresponding latent factor
un ∈RK, and a probability parameter µnℓ∈(0, 1) which
denotes the probability of the label ynℓbeing exposed (but
note that ynℓcan be 0 or 1, depending on the outcome of
Bernoulli(ynℓ|σ(u⊤
n vℓ))). We refer to µnℓas the exposure
probability of label ℓfor example n.
We assume each regression weight vector wk to have a
Gaussian prior, i.e., wk ∼N(wk|0, λ−1
w ID). Note that
the spherical covariance of this prior can also be replaced
by a more ﬂexible diagonal covariance, which will give the
model ability to perform feature selection.
For the exposure probability µnℓ, we consider two types of
priors. In the ﬁrst case, we simply assume µnℓ= µℓ, ∀n,
which means that the probability that a label ℓis observed
is the same for all the examples (i.e., the label exposure for
the label ℓis global, not example speciﬁc). In this case,
we assume a Beta prior on µℓ, i.e., µℓ∼Beta(α1, α2).
In the second case, we assume access to some contexual
information (often available in applications such as recom-
mender systems) that we may have for each example-label
pair (n, ℓ), in form of some given covariates φnℓ∈RM.
Given these covariates, we model the label exposure prob-
ability as µnℓ= σ(β⊤φnℓ), where β ∈RM is a vector of
regression coefﬁcients. We assume a Gaussian prior on β,
i.e., β ∼N(β|0, λ−1
β IM)
3. Inference
Although the generative model speciﬁed in Eq. 1-4 is not
readily conjugate because the logistic-Bernoulli likelihood
is not conjugate to the Gaussian prior on the latent fac-
tors, we can leverage data-augmentation techniques (Pol-
son et al., 2013) to make the model locally conjugate. This
enables us to develop a simple Gibbs sampling algorithm
for doing inference in our model. The conjugacy also al-
lows us to design an online expectation maximization (EM)
algorithm (Capp´e & Moulines, 2009), which enables us to
apply our model on large-scale problems.
We handle the non-conjugate logistic-Bernoulli likelihood
using the P´olya-gamma augmentation technique (Polson
et al., 2013), which is based on the following identity
(exp(ψ)a
(1 + exp(ψ))b = 2−b exp (κψ)
Z ∞
0
exp
 −ωψ2/2

p(ω)dω
where κ
=
a −b/2 and p(ω)
=
PG(b, 0) denotes
the P´olya-gamma distribution (Polson et al., 2013). This
identity allows us to write any likelihood of the form
(exp(ψ)a
(1+exp(ψ))b (e.g., Bernoulli, binomial, negative-binomial)
as a Gaussian distribution, when conditioned on a PG ran-
dom variable ω|ψ ∼PG(b, ψ). Speciﬁcally, using PG aug-
mentation, we can write the logistic-Bernoulli likelihood
from Eq. 4 as a Gaussian when conditioned on ωnℓ∼
PG(1, u⊤
n vℓ). In particular, ψnℓ= u⊤
n vℓ, conditioned on
ωnℓ, becomes a Gaussian
p(ψnℓ|ωnℓ) ∝exp

κnℓψnℓ−1
2ωnℓψ2
nℓ

(6)
where κnℓ= ynℓ−0.5. This likelihood with the Gaussian
priors on the latent factors un and vℓresults in Gaussian
posteriors on un and vℓ. When doing EM, this also leads to
subproblems that are like least square regression problems.
3.1. Gibbs Sampling
Using the PG augmentation, we can derive the posterior
distributions of all the latent variables in our model, and
perform Gibbs sampling for doing inference in our model.
Due to conjugacy, the inference updates are straightforward
to derive as are summarized below.
Sampling ξnℓ: Note that if ynℓ= 1 then ξnℓ= 1 with
probability one, and therefore need not be inferred. For
ynℓ= 0, we sample ξnℓfrom the posterior
p(ξnℓ= 1|.)
∝
µnℓσ(−u⊤
n vℓ)
(7)
p(ξnℓ= 0|.)
∝
(1 −µnℓ) × 1
(8)
Sampling µnℓ: For the case when µnℓ= µℓ, ∀n, with
Beta(α1, α2) prior on each µℓ, the posterior will be
p(µnℓ|.) = Beta(α1 +
N
X
n=1
ξnℓ, α2 + N −
N
X
n=1
ξnℓ)
(9)
Note that, if we parameterize each µnℓas µnℓ= σ(β⊤φnℓ)
where φnℓis the interaction feature vector for the example-
label pair, and the regresssion weight β is assumed to have
a Gaussian prior, the model is not conjugate. However,
using the PG augmentation allows us to easily derive a
closed-form Gaussian posterior for β.
Sampling un: Given the PG variables Ωn,: = {ωnℓ}L
ℓ=1
and the other latent variables, the posterior of un will be
un ∼N(un|µun, Σun) where the covariance is given by
Σun = (PL
ℓ=1 ξnℓωnℓvℓv⊤
ℓ+ λuIK)−1 and the mean is
given by µun = Σun(PL
ℓ=1 ξnℓκnℓvℓ+ λuWxn). Note
that if a label ℓis inferred as not exposed for example n,
i.e., ξnℓ= 0, it does not contribute to the update of un.
Sampling vℓ: Given Ω:,ℓ= {ωnℓ}N
n=1 and the other latent
variables, the posterior vℓwill be vℓ∼N(vℓ|µvℓ, Σvℓ)
where covariance Σvℓ= (PN
n=1 ξnℓωnℓunu⊤
n +λvIK)−1
and the mean µvℓ= Σvℓ(PN
n=1 ξnℓκnℓun). Note that
if an example n is inferred as not exposed to label ℓ, i.e.,
ξnℓ= 0, it does not contribute to the update of vℓ.
Sampling W:
Each row {wk}K
k=1 of the regression
weights matrix W = [w1, . . . , wK]⊤∈RK×D will have
a Gaussian posterior given by wk ∼N(wk|µwk, Σwk)
where covariance Σwk = (X⊤X + λwID)−1, the mean
µwk = Σwk(X⊤U), and U = [u1, . . . , uN] ∈RK×N.

Scalable Generative Models for Multi-label Learning with Missing Labels
3.2. Scalable Inference via EM and Online EM
Although the Gibbs sampler (Sec. 3.1) is easy to derive and
implement in practice, sampling tends to be slow in prac-
tice and convergence may be slow. We therefore present
an online expectation maximization algorithm (Capp´e &
Moulines, 2009) for doing efﬁcient inference in our model.
We ﬁrst show the batch EM updates for our model param-
eters and then describe the online EM algorithm which can
process the training data in small mini-batches of examples,
and results in faster convergence in practice.
3.2.1. THE EM ALGORITHM
The EM algorithm for our model alternates between com-
puting the expectations of the local latent variables, namely
the P´olya-gamma variables {ωnℓ} and the binary exposure
latent variables {ξnℓ} in the E step, and then using these ex-
pectations to estimate the other model parameters un, vℓ,
W, and exposure probabilities {µnℓ} in the M step.
The E Step: The E step involves computing the expecta-
tions of the latent variables {ωnℓ} and {ξnℓ}, given the cur-
rent values of the other model parameters un, vℓ, W, and
µnℓestimated in the previous M step. The E step update
equations are given below:
• Expectations of P´olya-gamma variables {ωnℓ}, ∀n, ℓ
are known to be available in closed form (Scott & Sun,
2013), and are given by
ηnℓ= E[ωnℓ|ψnℓ] =
1
2ψnℓ
tanh
ψnℓ
2

(10)
where ψnℓ= u⊤
n vℓis computed using the estimates
of un and vm from the previous M step.
• Expectations of each of the binary exposure variables
ξnℓ, ∀n, ℓ, are given by
pnℓ= E[ξnℓ|ψnℓ] =
µnℓσ(−ψnℓ)
µnℓσ(−ψnℓ) + (1 −µnℓ)
(11)
The M Step: Given the expectations of the latent vari-
ables computed in the E step, the M step maximizes the
following expected complete data log-likelihood plus log-
prior terms, which we denote as Q(U, V, W, µ), where
U = {un}N
n=1, V = {vℓ}L
ℓ=1, W, and µ = {µnℓ}, ∀n, ℓ
Q(U, V, W, µ) = −1
2
X
n,ℓ
pnℓ
(κnℓ−ηnℓu⊤
n vℓ)2
ηnℓ
+
X
n,ℓ
log Bernoulli(pnℓ|µnℓ) −λu
N
X
n=1
||un −Wxn||2
−λv
L
X
ℓ=1
||vℓ||2 −λw||W||2 +
X
n,ℓ
log Beta(µnℓ|α1, α2)
(12)
Note that the ﬁrst term in the objective function given in
Eq. 12 is due to the logistic likelihood transformed into a
Gaussian (using PG augmentation). This term is akin to
a weighted least squares objective where each label being
associated with a weight pnℓ= E[ξnℓ|ψnℓ]. Intuitively, in
the ﬁrst term, the contribution of each label ynℓto the log-
likelihood gets modulated based on its expected exposure.
Maximizing Q(U, V, W, µ) w.r.t. each of the model pa-
rameters U, V, W, µ, ﬁxing the rest, yields closed-form
updates for each of these. The updates are as follows:
• Estimating each of the latent factors {un}N
n=1 is a
weighted ridge-regression problem with solution
un = Σun
 L
X
ℓ=1
pnℓκnℓvℓ+ λuWxn
!
(13)
where Σun = (PL
ℓ=1 pnℓηnℓvℓv⊤
ℓ+ λuIK)−1. Note
that the updates for {un}N
n=1 are all independent of
each other and are easily parallelizable.
• Estimating each of the label latent factors {vℓ}L
ℓ=1 is
a weighted ridge-regression problem with solution
vℓ= Σvℓ
 N
X
n=1
pnℓκnℓun
!
(14)
where Σvℓ
=
PN
n=1 pnℓηnℓunu⊤
n + λvIK
−1
.
Again, note that the updates for {vn}L
ℓ=1 are all in-
dependent of each other and are easily parallelizable.
• Estimating the regression weight matrix W is equiva-
lent to solving a vector-valued linear regression prob-
lem un ≈Wxn, ∀n, with the following updates
W⊤= (X⊤X + λwID)−1(X⊤U)
(15)
Note that solving Eq. (15) exactly requires inverting a
D × D matrix which will be expensive for large D.
However, the EM algorithm does not require solving
for W exactly in each M step. We therefore solve
for W efﬁcient using gradient based methods, such
as conjugate-gradient (CG) method (Bertsekas, 1999),
which allows us to also leverage the sparsity in the
feature matrix X. Typically, a small number of CG
iterations are sufﬁcient in practice.
• Given pnℓfrom the E step, the updates for µnℓfor the
case when µnℓ= µℓ, ∀n, is simply the MAP solution
µℓ= α1 + PN
n=1 pnℓ−1
α1 + α2 + N −2
(16)
For the other case when each µnℓin modeled as µnℓ=
σ(β⊤φnℓ) with a Gaussian prior on β, estimating β
reduces to solving a regression problem with the train-
ing data being {φnℓ, pnℓ}, ∀n, ℓ, where φnℓis the
given feature vector for the input-label pair n, ℓand
pnℓis estimated in the E step. Ignoring the prior term
(equivalent to ℓ2 regularizer on β), we can estimate β
iteratively using gradient-descent updates
β = β −
τ
NL
X
n,ℓ
(σ(β⊤φnℓ) −pnℓ)φnℓ
(17)
where τ denotes the learning rate.

Scalable Generative Models for Multi-label Learning with Missing Labels
3.2.2. ONLINE EM
The EM algorithm described in Section 3.2.1 is more efﬁ-
cient than the Gibbs sampler described in Section 3.1. It is
also highly parallelizable since the updates for {u}N
n=1 and
{vℓ}L
ℓ=1 can be easily parallelized, and solve for W efﬁ-
ciently using CG updates. However, it is a batch procedure
and requires going over the entire training data in every iter-
ation. For large-scale multi-label learning problems, which
are characterized by large N, D, and L, the batch setting
may not be feasible in practice, especially when having ac-
cess to moderate computational resources and storage.
We therefore present an efﬁcient online version of the EM
algorithm for our model which allows it to scale up to
massive-sized data sets even on machines with moderate
hardware. As we show in our experiments, this enables us
to apply our model to be run efﬁciently on massive data
sets (e.g., one of the data sets we experiment with has more
than 600k examples with about 50k features per example)
even on a standard laptop with very moderate hardware.
The online EM algorithm works by maintaining sufﬁcient
statistics of all the model parameters and updates these
sufﬁcient statistics with every mini-batch of data.
For
each mini-batch of training examples, the E step computes
the relevant expectations associated with these observa-
tions and then uses the expectations to update the sufﬁcient
statistics of the parameters to be estimated in the M step.
For example, noting that the sufﬁcient statistics for updat-
ing the label latent factors vℓ= A−1b are given by A =
PN
n=1 pnℓηnℓunu⊤
n +λvIK and b = PN
n=1 pnℓκnℓun, we
can update A and b using a small mini-batch containing Nb
examples as {(xn, yn)}Nb
n=1 as follows
A(t+1) = (1 −γt)A(t) + γtA(new)
(18)
b(t+1) = (1 −γt)b(t) + γtb(new)
(19)
where A(new)
=
(PNb
n=1 pnℓηnℓunu⊤
n + λvIK), and
b(new) = PNb
n=1 pnℓκnℓun are computing using only the
current mini-batch. The sufﬁcient statistics of the other
model parameters can also be updated in the same man-
ner. Here γt is a decaying learning rate (or a forgetting
factor), which also acts as a trade-off between the contri-
bution from the old sufﬁcient statistics computed thus far
and the sufﬁcient statistics contribution from the new mini-
batch of data. We set γt = (a0 + t)−ϵ with a0 = 1 and ϵ to
be close to 0.5 (Capp´e & Moulines, 2009).
3.2.3. PREDICTION
Given a new test input x∗, we ﬁrst predict its latent factor
u∗∈RK as Wx∗and then predict each entry of its label
vector y∗as E[y∗ℓ|u∗, vℓ] = σ(u⊤
∗vℓ). If we are only
interested in the top few labels, fast search methods such as
maximum inner product search (Fraccaro et al., 2016) can
be used to reduce the computational cost at test time.
4. Related Work
A prominent line of work on multi-label learning has been
based on models that learn a low-dimensional embedding
of the label vectors (Chen & Lin, 2012; Yu et al., 2014; Rai
et al., 2015; Bhatia et al., 2015). Note that this amounts to
assuming that the label matrix is low-rank.
Since many real-world data sets have a large number of
rare labels, sometimes the low-rank assumption may not
be appropriate. To address this issue, (Bhatia et al., 2015)
proposed a method which assumes the label matrix to be
locally low-rank. One way to impose this assumption is to
learn embeddings that only try to preserve distances in a
small neighborhood of each example. Another approach to
handle the rare labels is to assume that the label matrix is a
sum of a low-rank and a sparse matrix (Xu et al., 2016).
Note that our latent factor model is equivalent to imposing
a low-rank assumption on the label matrix, and is there-
fore similar in spirit to the label-embedding approaches.
However, unlike the existing label-embedding based ap-
proaches, our generative framework has a principled mech-
anism to handle/infer the unobserved labels.
Moreover,
none of the existing label-embedding methods can work
in online fashion, and scaling up these methods to large-
scale problems requires large computational resources. In
addition, our model readily allows incorporating the label
features (if available) by a simple modiﬁcation to the prior
on the label latent factors.
Apart from the label-embedding based multi-label learn-
ing methods, tree-based methods for multi-label learn-
ing (Agrawal et al., 2013; Prabhu & Varma, 2014; Jain
et al., 2016) are also popular due to being fast at test time,
especially when the number of labels is large. However,
these models usually have high training costs and cannot
be trained easily in an online fashion, unlike our model.
On the other hand, for faster predictions at test time, our
framework model can easily be adapting by replacing the
Gaussian prior on the label latent factors vℓby a von Mises-
Fisher prior (Fraccaro et al., 2016), which naturally fa-
cilitates using maximum inner-product search techniques,
without the requirement of any post-processing.
Among other models to address the missing labels prob-
lem in multi-label learning, recently, (Kanehira & Harada,
2016) proposed a ranking based framework for learning
from positive and unlabeled data in the context of multi-
label learning.
Although this is similar in spirit to our
model in terms of not treating the unobserved labels as ze-
ros, the approach in (Kanehira & Harada, 2016) is funda-
mentally different than ours. Moreover, their setting is not
amenable to online learning, nor does it leverage the low-
rank structure of label matrices with a huge number of la-
bels. Other approaches that try to handle missing labels in-

Scalable Generative Models for Multi-label Learning with Missing Labels
clude (Bucak et al., 2011) which uses group LASSO adap-
tation of a multi-label ranking objective, and (Kong et al.,
2014), which learns a model using a positive and unlabeled
(PU) stochastic gradient descent procedure. However, it
works in batch setting, uses stacking to leverage label cor-
relations, and does not scale to large number of labels.
One-class matrix factorization (OCMF) is also an ap-
proach (Yu et al., 2017) to solve the missing labels prob-
lem by assigning different (but ﬁxed) weights to the ones
and zeros. In contrast to this method, our generative frame-
work can learn the weight for each label by modeling these
weights as latent variables. In another recent work, (Liang
et al., 2016) proposed an exposure model for recommender
system problems posed as matrix factorization of implicit
feedback data. Their approach of modeling the exposure
similar in spirit to our framework.
Some of the early works on generative models for multi-
label learning problems include models speciﬁcally de-
signed for image annotation problems (Barnard et al., 2003;
Feng et al., 2004). Other recent attempts on doing multi-
label learning in more general problem settings include
models such as Bayesian compressive sensing (Kapoor
et al., 2012) and multi-label learning using Bayesian non-
negative matrix factorization (Rai et al., 2015). However,
these models do not have a mechanism to distinguish be-
tween unobserved and negative labels, have complicated
inference, and do not scale to large-scale problems.
Our generative framework is also amenable for various in-
teresting extensions. For example, it can be be extended
to a mixture of latent factor models, which can handle the
situation when the label matrix is not low-rank but a mix-
ture of several low-rank matrices. Note that such an exten-
sion would be a fully generative counter-part of the model
in (Bhatia et al., 2015) which learns a locally low-rank
model but has to rely on an ad-hoc clustering step before-
hand, which is known to be unstable in practice (Bhatia
et al., 2015). Another nice aspect of our framework is that
is naturally allows active learning (Kapoor et al., 2012; Va-
sisht et al., 2014) where we can selectively ask for most
informative labels for an unannotated example. Moreover,
our framework is ﬂexible and inference in our model can
be performed in a fully Bayesian manner (e.g., MCMC or
variational inference) as well as fast point estimation meth-
ods such as (online) EM, that we used in this work.
To summarize, our generative framework offers a ﬂex-
ible way to model the label generation mechanism for
real-world multi-label data sets, which most of the exist-
ing models currently lack. We can model label missing-
ness/observability rigorously under our framework and in-
fer the model parameters easily using a simple inference
procedure. Moreover, the simplicity of the inference proce-
dure makes it easy to design scalable inference algorithms,
such as online EM for our model, which enables updating
the model whenever fresh training data is available. This is
in contrast to some of the other state-of-the-art multi-label
learning methods, which although scalable (Bhatia et al.,
2015; Prabhu & Varma, 2014; Jain et al., 2016), are not
suitable to be applied in such online settings.
5. Experiments
We evaluate our framework on a number of benchmark
data sets and compare it with several state-of-the-art meth-
ods. Our baselines include both label-embedding methods
as well as tree-based methods. The statistics of data sets
we use in our experiments are summarized in Table 1.
Dataset
N
Ntest
D
L
Bibtex
4880
2515
1836
159
Mediamill
30993
12914
120
101
Eurlex-4K
15539
3809
5000
3993
Movielens
4000
2040
29
3952
RCV
623847
155962
47236
2456
Wikipedia
14146
6616
101938
30938
Table 1. Dataset used for the experiments with their properties.
D: number of features, L: number of labels, N: number of train-
ing examples, Ntest: number of test examples
We report both quantitative results (in terms of label predic-
tion accuracies) as well as some qualitative results, namely
looking at the relationship of empirical label frequencies
and label exposure. Note that the label frequency for a
given label denotes how many examples had this label as
1, while label exposure µℓ∈(0, 1) in general refers to
how popular the label ℓis.
In our experiments, we compare with the following state-
of-the-art baselines.
• LEML: This is a low-rank embedding based multi-
label learning model (Yu et al., 2014).
LEML as-
sumes the label matrix Y to be modeled as Y ≈UV
where U = XW. LEML considers various types of
loss functions such as squared loss, logistic loss, hinge
loss, etc. Interestingly, note that LEML with logistic
loss can be seen as a special non-probabilistic case of
our model when also considering λu →∞, and the
label exposure model turned off.
• BCS: Bayesian Compressive Sensing (BCS) is a gen-
erative model (Kapoor et al., 2012) for the label vec-
tor. It assumes a compressive sensing model for the
label vectors and is essentially a low-rank model.
• FastXML: This is a fast tree-based multi-label learn-
ing model which uses an ensemble of trees (Prabhu &
Varma, 2014).
• PfasterXML: This is an extension of FastXML and
uses propensity-weighted scores to improve perfor-
mance on rare labels (Jain et al., 2016).

Scalable Generative Models for Multi-label Learning with Missing Labels
• PD-Sparse: This model takes a different approach
as compared to label-embedding methods and uses a
margin-maximizing loss for the multi-label learning
problem (Yen et al., 2016).
For the baselines, the reported results are either obtained
using publicly available implementations (with the recom-
mended hyperparameter settings), or the publicly known
best results. We refer to our model as GenEML (for Gen-
erative Exposure-based model for Multi-label Learning)
Hyperparameter Settings: For our model, we set the hy-
perparameters λu and λv to 0.001, which works well on
all the data sets we experimented with. We select the other
two hyperparameters λw and K (number of latent factors)
using cross-validation. On small-/medium-scale data, both
EM and online EM perform comparably and we only report
the results using online EM. On large data sets, we only use
online EM. On the small and medium-scale data, we how-
ever also show a separate experiment comparing EM and
online EM for our model in terms of convergence speed
versus accuracy. For the conjugate gradient (CG) method
used by the M step of our inference algorithm, we run 5 it-
erations, which was found to be sufﬁcient. For online EM,
for each data set, we use mini-batch sizes of 1024 and 4096
and report the one which gives better results.
5.1. Quantitative Results
5.1.1. BENEFIT OF EXPOSURE MODEL
In our ﬁrst experiment, we assess the beneﬁt of using the
exposure model. For this, we apply our model with and
without exposure on a synthetic data set.
For this ex-
periment, we generate a synthetic data set with N=500,
D=100, and L=20 and use varying degrees of exposure
probabilities µℓ∈{0.01, 0.05, 0.1, 0.3, 0.5, 0.9} for the
different labels ℓ= 1, . . . , 20. We also create a test set
with 500 test examples.
The results are shown in Table 2.
As the results show,
our model with exposure turned on outperforms the model
when the exposure is turned off. This clearly demonstrate
the beneﬁt of the exposure model when a signiﬁcant frac-
tion of labels are missing (i.e., not exposed). Our model
also outperforms LEML which does not have a mechanism
to model label exposure.
GenEML
GenEML w/o
Exposure
LEML
P@1
87.8
79.2
78.8
P@2
75.2
68.9
69.1
P@3
65.2
59.0
58.9
Table 2. Precision@k values on synthetic data obtained by our
model with and without using the label exposure
5.1.2. PREDICTION ACCURACIES
In our next set of experiments, in Table 3 we compare
our model (with exposure on) with the other baselines,
in terms of Precision@1, Precision@3, and Precision@5
scores. As Table 3 shows, our model outperforms the other
baselines in most of the cases, except for the RCV and
Wikipedia data, on which our model is outperformed by
LEML and/or PfasterXML. Note, however, that these state-
of-the-art baselines use batch inference methods whereas
we only ran our model in the online setting on a moderate 4
core processor with 8GB RAM. Moreover, our results may
further improve with a more careful hyperparameter tuning
(including selection of minibatch size). The point of the
large-scale data experiment was to mainly show that the our
model can be feasibly run on such large-scale data sets, on
standard machines with moderate computational resources.
Most of the other existing models for multi-label learning
are infeasible to run under such restrictive settings.
5.1.3. BATCH EM VS ONLINE EM
The online version of our EM algorithm is scalable and
faster than its batch counterpart. Fig 2 shows that online
EM converges faster and to a precision score which is very
similar to the batch EM on Bibtex and Mediamill datasets.
Furthermore, online inference is also more effecient,
storage-wise, due the need of maintaining just the sufﬁcient
statistics as in Eq 19 for the updates of each latent factor
un and vℓ. For very large datasets, the size of the the suf-
ﬁcient statistics (a D × D covariance matrix) for updating
the regression weight matrix W might not be feasible to
store and update. Therefore, we use cheap, ﬁrst-order gra-
dient based updates for ﬁnding an approximate solution to
the update equation of W in each iteration of the EM algo-
rithm (note that we need not solve for W exactly; the EM
algorithm just requires a few steps of updates for W in the
M step). This further reduces the memory requirement of
our model, while also speeding up inference due to faster
computation of gradients as compared to CG updates.
Figure 2. Convergence time comparison of the batch and online
EM algorithm for inference in our model
5.2. Qualitative Results
Finally, we do some qualitative analyses of our model’s be-
havior.
We investigate whether the global frequency of
a label necessarily correlates to its exposure probability.

Scalable Generative Models for Multi-label Learning with Missing Labels
Dataset
BCS
WSABIE
FastXML
PfasterXML
PD-Sparse
LEML
GenEML
Bibtex
P@1
60.24
54.78
63.42
63.46
61.29
62.54
64.09
P@3
34.87
32.29
39.23
39.22
35.82
38.41
40.14
P@5
24.48
23.98
28.86
29.14
25.74
28.21
29.47
Mediamill
P@1
-
81.29
84.22
83.98
81.86
84.01
87.15
P@3
-
64.74
67.33
67.37
62.52
67.20
69.98
P@5
-
49.83
53.04
53.02
45.11
52.80
55.21
Eurlex-4K
P@1
-
68.55
71.36
75.45
76.43
63.40
77.75
P@3
-
55.11
59.90
62.70
60.37
50.35
63.98
P@5
-
45.12
50.39
52.51
49.72
41.28
53.24
Movielens
P@1
-
-
-
-
-
54.22
55.78
P@3
-
-
-
-
-
50.44
50.62
P@5
-
-
-
-
-
48.63
48.86
RCV-2K
P@1
-
-
-
-
-
89.09
87.21
P@3
-
-
-
-
-
70.96
69.22
P@5
-
-
-
-
-
50.73
49.53
Wiki10-31K
P@1
-
-
83.03
83.57
-
73.47
76.38
P@3
-
-
67.47
68.61
-
62.43
44.49
P@5
-
-
57.76
59.10
-
54.35
32.06
Table 3. Performance Comparison using Prec@k of the model with other baselines. The - denotes that either these results were not
available or the method was infeasible to run on that data set. On the large-scale data sets (RCV and Wiki), our model was run using
online EM based inference.
While it may be the case for some data sets where high la-
bel frequency implies a high inferred label exposure prob-
ability (e.g., see Fig. 3 for Bibtex and Mediamill data), it
need not be the case with other data sets. For example,
for Movielens data, each user-movie (example-label) pair
has an some context information (user and movie features)
available for it. As we show in Fig 4, the inferred expo-
sure probability (which depends on the context features) of
the same movie (label) indeed turns out to be different for
different users (examples).
Fig. 4 shows the plot of inferred exposure probabilities µnl
for two users (one female, one male) plotted against the
label frequencies (movie popularities).
Figure 3. Inferred label exposure probabilities for Bibtex and Me-
diamill data sets
As Fig. 4 shows, our model infers that, a popular movie
(shown in red dot in Fig 4) has a high exposure probabil-
ity for the left user (Female, 25, Healthcare/Doctor) while
it has a low exposure probability for the right user (Male,
35, artist). This example illustrates that a high label fre-
quency does not necessarily imply a high exposure proba-
bility, which can be context (user in this case) dependent.
6. Conclusion
We presented a ﬂexible and scalable generative framework
for multi-label learning. Our framework is based on a latent
Figure 4. Inferred user-speciﬁc label exposure probabilties for
two users on Movielens data set: Female, 25, Healthcare/Doctor
(left) and Male, 35, artist (right), with label frequency for each
label.
The red circle shows the most frequent movie (Hair-
spray(1988) Comedy, Drama) and its exposure probability for
both the users.
factor model for the label matrix and does not assume that
the zeros in the label matrix are necessarily negative labels.
We use a set of label exposure latent variables to model this,
and infer these exposure probabilities from data. Incorpo-
rating these latent variables leads to improve multi-label
classiﬁcation accuracies, and also enables doing interesting
qualitative analyses. Our model admits a simple inference
procedure which can be implemeted using Gibbs sampling
or EM. We further develop a highly scalable online EM
algorithm for performing inference in our model, which al-
lows our model to be applied on large-scale data sets, even
on standard machines with moderate hardware. The gener-
ative framework makes it easy to extend our model in many
interesting ways. For example, it can be extended to a mix-
ture of latent factor models, which will allow handling the
cases where a single low-rank model does not adequately
capture the structure of the label matrix.
Acknowledgements: PR acknowledges support from Extreme
Classiﬁcation research grant from Microsoft Research India,
DST-SERB Early Career Research Award, Dr. Deep Singh and
Daljeet Kaur Fellowship, and Research-I Foundation, IIT Kanpur.

Scalable Generative Models for Multi-label Learning with Missing Labels
References
Agrawal, Rahul, Gupta, Archit, Prabhu, Yashoteja, and Varma,
Manik. Multi-label learning with millions of labels: Recom-
mending advertiser bid phrases for web pages. In WWW, 2013.
Babbar, R. and Sch¨olkopf, B. DiSMEC- distributed sparse ma-
chines for extreme multi-label classiﬁcation. In WSDM, 2017.
Barnard,
Kobus,
Duygulu,
Pinar,
Forsyth,
David,
Freitas,
Nando de, Blei, David M, and Jordan, Michael I. Matching
words and pictures. JMLR, 2003.
Bertsekas, Dimitri P. Nonlinear programming. Athena scientiﬁc
Belmont, 1999.
Bhatia, Kush, Jain, Himanshu, Kar, Purushottam, Varma, Manik,
and Jain, Prateek. Sparse local embeddings for extreme multi-
label classiﬁcation. In NIPS, 2015.
Bucak, Serhat Selcuk, Jin, Rong, and Jain, Anil K. Multi-label
learning with incomplete class assignments. In CVPR, 2011.
Capp´e, Olivier and Moulines, Eric.
On-line expectation–
maximization algorithm for latent data models. Journal of the
Royal Statistical Society: Series B (Statistical Methodology),
2009.
Chen, Yao-Nan and Lin, Hsuan-Tien. Feature-aware label space
dimension reduction for multi-label classiﬁcation.
In NIPS,
2012.
Ciss´e, Moustapha, Al-Shedivat, COM Maruan, and Bengio,
Samy. Adios: Architectures deep in output space. In ICML,
2016.
Feng, SL, Manmatha, Raghavan, and Lavrenko, Victor. Multiple
bernoulli relevance models for image and video annotation. In
CVPR, 2004.
Fraccaro, Marco, Paquet, Ulrich, and Winther, Ole. Indexable
probabilistic matrix factorization for maximum inner product
search. In AAAI, 2016.
Gibaja, Eva and Ventura, Sebasti´an. Multilabel learning: A re-
view of the state of the art and ongoing research. Wiley Inter-
disciplinary Reviews: Data Mining and Knowledge Discovery,
2014.
Gibaja, Eva and Ventura, Sebasti´an.
A tutorial on multilabel
learning. ACM Comput. Surv., 2015.
Hu, Yifan, Koren, Yehuda, and Volinsky, Chris. Collaborative
ﬁltering for implicit feedback datasets. In ICDM, 2008.
Jain, Himanshu, Prabhu, Yashoteja, and Varma, Manik. Extreme
multi-label loss functions for recommendation, tagging, rank-
ing & other missing label applications. In KDD, 2016.
Kanehira, Atsushi and Harada, Tatsuya. Multi-label ranking from
positive and unlabeled data. In CVPR, 2016.
Kapoor, Ashish, Viswanathan, Raajay, and Jain, Prateek. Mul-
tilabel classiﬁcation using bayesian compressed sensing.
In
NIPS, 2012.
Kong, Xiangnan, Wu, Zhaoming, Li, Li-Jia, Zhang, Ruofei, Yu,
Philip S, Wu, Hang, and Fan, Wei.
Large-scale multi-label
learning with incomplete label assignments. In SDM, 2014.
Liang, Dawen, Charlin, Laurent, McInerney, James, and Blei,
David M.
Modeling user exposure in recommendation.
In
WWW, 2016.
Polson, Nicholas G, Scott, James G, and Windle, Jesse. Bayesian
inference for logistic models using p´olya–gamma latent vari-
ables.
Journal of the American Statistical Association, 108
(504):1339–1349, 2013.
Prabhu, Yashoteja and Varma, Manik. FastXML: a fast, accurate
and stable tree-classiﬁer for extreme multi-label learning. In
KDD, 2014.
Rai,
Piyush,
Hu,
Changwei,
Henao,
Ricardo,
and Carin,
Lawrence. Large-scale bayesian multi-label learning via topic-
based label embeddings. In NIPS, 2015.
Scott, James G and Sun, Liang. Expectation-maximization for
logistic regression. arXiv preprint arXiv:1306.0040, 2013.
Vasisht, Deepak, Damianou, Andreas, Varma, Manik, and
Kapoor, Ashish. Active learning for sparse bayesian multilabel
classiﬁcation. In KDD, 2014.
Wang, Jiang, Yang, Yi, Mao, Junhua, Huang, Zhiheng, Huang,
Chang, and Xu, Wei. CNN-RNN: A uniﬁed framework for
multi-label image classiﬁcation. In CVPR, 2016.
Xu, Chang, Tao, Dacheng, and Xu, Chao. Robust extreme multi-
label learning. In KDD, 2016.
Yen, Ian EH, Huang, Xiangru, Zhong, Kai, Ravikumar, Pradeep,
and Dhillon, Inderjit S. PD-sparse: A primal and dual sparse
approach to extreme multiclass and multilabel classiﬁcation. In
ICML, 2016.
Yu, Hsiang-Fu, Jain, Prateek, Kar, Purushottam, and Dhillon, In-
derjit S. Large-scale multi-label learning with missing labels.
In ICML, 2014.
Yu, Hsiang-Fu, Huang, Hsin-Yuan, Dhillon, Inderjit S, and Lin,
Chih-Jen. A uniﬁed algorithm for one-class structured matrix
factorization with side information. In AAAI, 2017.

