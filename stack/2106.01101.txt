Learning a Single Neuron with Bias Using Gradient Descent
Gal Vardi* , Gilad Yehudai* , and Ohad Shamir
Weizmann Institute of Science
{gal.vardi,gilad.yehudai,ohad.shamir}@weizmann.ac.il
Abstract
We theoretically study the fundamental problem of learning a single neuron with a bias term (x 7â†’
Ïƒ(âŸ¨w, xâŸ©+ b)) in the realizable setting with the ReLU activation, using gradient descent. Perhaps sur-
prisingly, we show that this is a signiï¬cantly different and more challenging problem than the bias-less
case (which was the focus of previous works on single neurons), both in terms of the optimization ge-
ometry as well as the ability of gradient methods to succeed in some scenarios. We provide a detailed
study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and
providing positive convergence guarantees under different sets of assumptions. To prove our results,
we develop some tools which may be of independent interest, and improve previous results on learning
single neurons.
1
Introduction
Learning a single ReLU neuron with gradient descent is a fundamental primitive in the theory of deep
learning, and has been extensively studied in recent years. Indeed, in order to understand the success of
gradient descent on complicated neural networks, it seems reasonable to expect a satisfying analysis of
convergence on a single neuron. Although many previous works studied the problem of learning a single
neuron with gradient descent, none of them considered this problem with an explicit bias term.
In this work, we study the common setting of learning a single neuron with respect to the squared loss,
using gradient descent. We focus on the realizable setting, where the inputs are drawn from a distribution D
on Rd+1, and are labeled by a single target neuron of the form x 7â†’Ïƒ(âŸ¨v, xâŸ©), where Ïƒ : R â†’R is some
non-linear activation function. To capture the bias term, we assume that the distribution D is such that its
ï¬rst d components are drawn from some distribution ËœD on Rd, and the last component is a constant 1. Thus,
the input x can be decomposed as (Ëœx, 1) with Ëœx âˆ¼ËœD, the vector v can be decomposed as (Ëœv, bv), where
Ëœv âˆˆRd and bv âˆˆR, and the target neuron computes a function of the form x 7â†’Ïƒ(âŸ¨Ëœv, ËœxâŸ©+ bv). Similarly,
we can deï¬ne the learned neuron as x 7â†’Ïƒ(âŸ¨Ëœw, ËœxâŸ©+ bw), where w = ( Ëœw, bw). Overall, we can write the
objective function we wish to optimize as follows:
F(w) :=
E
xâˆ¼D
1
2

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)
2
(1)
=
E
Ëœxâˆ¼ËœD
1
2

Ïƒ( ËœwâŠ¤Ëœx + bw) âˆ’Ïƒ(ËœvâŠ¤Ëœx + bv)
2
.
(2)
*equal contribution
1
arXiv:2106.01101v1  [cs.LG]  2 Jun 2021

Throughout the paper we consider the commonly used ReLU activation function: Ïƒ(x) = max{0, x}.
Although the problem of learning a single neuron is well studied (e.g. [16, 21, 5, 4, 9, 17, 12, 13]), none
of the previous works considered the problem with an additional bias term. Moreover, previous works on
learning a single neuron with gradient methods have certain assumptions on the input distribution D, which
do not apply when dealing with a bias term (for example, a certain â€spreadâ€ in all directions, which does not
apply when D is supported on {1} in the last coordinate).
Since neural networks with bias terms are the common practice, it is natural to ask how adding a bias
term affects the optimization landscape and the convergence of gradient descent. Although one might con-
jecture that this is just a small modiï¬cation to the problem, we in fact show that the effect of adding a bias
term is very signiï¬cant, both in terms of the optimization landscape and in terms of which gradient descent
strategies can or cannot work. Our main contributions are as follows:
â€¢ We start in Section 3 with some negative results, which demonstrate how adding a bias term makes the
problem more difï¬cult. In particular, we show that with a bias term, gradient descent or gradient ï¬‚ow1
can sometimes fail with probability close to half over the initialization, even when the input distribution
is uniform over a ball. In contrast, [21] show that without a bias term, for the same input distribution,
gradient ï¬‚ow converges to the global minimum with probability 1.
â€¢ In Section 4 we give a full characterization of the critical points of the loss function. We show that adding
a bias term changes the optimization landscape signiï¬cantly: In previous works (cf. [21]) it has been
shown that under mild assumptions on the input distribution, the only critical points are w = v (i.e., the
global minimum) and w = 0. We prove that when we have a bias term, the set of critical points has a
positive measure, and that there is a cone of local minima where the loss function is ï¬‚at.
â€¢ In Sections 5 and 6 we show that gradient descent converges to the global minimum at a linear rate, under
some assumptions on the input distribution and on the initialization. We give two positive convergence
results, where each result is under different assumptions, and thus the results complement each other.
We also use different techniques for proving each of the results: The analysis in Section 6 follows from
some geometric arguments and extends the technique from [21, 5]. The analysis in Section 5 introduces a
novel technique, not used in previous works on learning a single neuron, and has a more algebraic nature.
Moreover, that analysis implies that under mild assumptions, gradient descent with random initialization
converges to the global minimum with probability 1 âˆ’eâ„¦(d).
â€¢ The best known result for learning a single neuron without bias using gradient descent for an input distri-
bution that is not spherically symmetric, establishes convergence to the global minimum with probability
close to 1
2 over the random initialization [21, 5]. With our novel proof technique presented in Section 5
this result can be improved to probability at least 1 âˆ’eâ„¦(d) (see Remark 5.7).
Related work
Although there are no previous works on learning a single neuron with an explicit bias term, there are many
works that consider the problem of a single neuron under different settings and assumptions.
Several papers showed that the problem of learning a single neuron can be solved under minimal assump-
tions using algorithms which are not gradient-based (such as gradient descent or SGD). These algorithms
1I.e., gradient descent with inï¬nitesimal step size.
2

include the Isotron proposed by [8] and the GLMtron proposed by [7]. The GLMtron algorithm is also ana-
lyzed in [3]. These algorithms allow learning a single neuron with bias. We note that these are non-standard
algorithms, whereas we focus on standard gradient based methods.
In [12] the authors study the empirical risk of the single neuron problem. However, their analysis does
not include the ReLU activation, or adding a bias term. A related analysis is also given in [13], where the
ReLU activation is not considered.
Several papers showed convergence guarantees for the single neuron problem with ReLU activation
under certain distributional assumptions, although none of these assumptions allows for a bias term. Notably,
[18, 16, 9, 1] showed convergence guarantees for gradient methods when the inputs have a standard Gaussian
distribution, without a bias term. [4] showed that under a certain subspace eigenvalue assumption a single
neuron can be learned with SGD, although this assumption does not allow adding a bias term. [21, 5] use an
assumption about the input distribution being sufï¬ciently â€spreadâ€ in all directions, which does not allow for
a bias term (since that requires an input distribution supported on {1} in the last coordinate). [21] showed a
convergence result under the realizable setting, while [5] considered the agnostic and noisy settings. In [17]
convergence guarantees are given for the absolute value activation, and a speciï¬c distribution which does
not allow a bias term.
Less directly related, [19] studied the problem of implicit regularization in the single neuron setting.
In [20, 10] it is shown that approximating a single neuron using random features (or kernel methods) is
not tractable in high dimensions. We note that these results explicitly require that the single neuron which
is being approximated will have a bias term. Thus, our work complements these works by showing that
the problem of learning a single neuron with bias is also learnable using gradient descent (under certain
assumptions). Agnostically learning a single neuron with non gradient-based algorithms was studied in
[3, 6].
2
Preliminaries
Notations.
We use bold-faced letters to denote vectors, e.g., x = (x1, . . . , xd). For u âˆˆRd we denote
by âˆ¥uâˆ¥the Euclidean norm. We denote Â¯u =
u
âˆ¥uâˆ¥, namely, the unit vector in the direction of u. For
1 â‰¤i â‰¤j â‰¤d we denote ui:j = (ui, . . . , uj) âˆˆRjâˆ’i+1. We denote by 1(Â·) the indicator function, for
example 1(t â‰¥5) equals 1 if t â‰¥5 and 0 otherwise. We denote by U([âˆ’r, r]) the uniform distribution over
the interval [âˆ’r, r] in R, and by N(0, Î£) the multivariate normal distribution with mean 0 and covariance
matrix Î£. Given two vectors w, v we let Î¸(w, v) = arccos

âŸ¨w,vâŸ©
âˆ¥wâˆ¥âˆ¥vâˆ¥

= arccos(âŸ¨Â¯w, Â¯vâŸ©) âˆˆ[0, Ï€]. For a
vector u âˆˆRd+1 we often denote by Ëœu âˆˆRd the ï¬rst d components of u, and denote by bu âˆˆR its last
component.
Gradient methods.
In this paper we focus on the following two standard gradient methods for optimizing
our objective F(w) from Eq. (2):
â€¢ Gradient descent: We initialize at some w0 âˆˆRd+1, and set a ï¬xed learning rate Î· > 0. At each iteration
t â‰¥0 we have: wt+1 = wt âˆ’Î·âˆ‡F(wt).
â€¢ Gradient Flow: We initialize at some w(0) âˆˆRd+1, and for every time t â‰¥0, we set w(t) to be the
solution of the differential equation Ë™w = âˆ’âˆ‡F(w(t)). This can be thought of as a continuous form of
gradient descent, where the learning rate is inï¬nitesimally small.
3

The gradient of the objective in Eq. (1) is:
âˆ‡F(w) = E
xâˆ¼D
h
Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

Â· Ïƒâ€²(wâŠ¤x)x
i
.
(3)
Since Ïƒ is the ReLU function, it is differentiable everywhere except for 0. Practical implementations of
gradient methods deï¬ne Ïƒâ€²(0) to be some constant in [0, 1]. Following this convention, the gradient used by
these methods still correspond to Eq. (3). We note that the exact value of Ïƒâ€²(0) has no effect on our results.
3
Negative results
In this section we demonstrate that adding bias to the problem of learning a single neuron with gradient
descent can make the problem signiï¬cantly harder.
First, on an intuitive level, previous results (e.g., [21, 5, 16, 4, 17]) considered assumptions on the input
distribution, which require enough â€spreadâ€ in all directions (for example, a strictly positive density in some
neighborhood around the origin). Adding a bias term, even if the ï¬rst d coordinates of the distribution satisfy
a â€spreadâ€ assumption, will give rise to a direction without â€spreadâ€, since in this direction the distribution
is concentrated on 1, hence the previous results do not apply.
Next, we show two negative results where the input distribution is uniform on a ball around the origin.
We note that due to Theorem 6.4 from [21], we know that gradient ï¬‚ow on a single neuron without bias will
converge to the global minimum with probability 1 over the random initialization. The only case where it
will fail to converge is when w0 is initialized in the exact direction âˆ’v, which happens with probability 0
with standard random initializations.
3.1
Initialization in a ï¬‚at region
If we initialize the bias term in the same manner as the other coordinates, then we can show that gradient
descent will fail with probability close to half, even if the input distribution is uniform over a (certain)
origin-centered ball:
Theorem 3.1. Suppose we initialize each coordinate of w0 (including the bias) according to U([âˆ’1, 1]).
Let Ïµ > 0 and let ËœD be the uniform distribution supported on a ball around the origin in Rd of radius Ïµ.
Then, w.p > 1/2 âˆ’Ïµ
âˆš
d, gradient descent on the objective in Eq. (2) satisï¬es wt = w0 for all t (namely, it
gets stuck at its initial point w0).
Note that by Theorem 6.4 in [21], if there is no bias term in the objective, then gradient descent will
converge to the global minimum w.p 1 using this random initialization scheme and this input distribution.
The intuition for the proof is that with constant probability over the initialization, bw is small enough so that
Ïƒ( ËœwâŠ¤Ëœx+bw) = 0 almost surely. If this happens, then the gradient will be 0 and gradient descent will never
move. The full proof can be found in Appendix B.1.
3.2
Targets with negative bias
Theorem 3.1 shows a difference between learning with and without the bias term. A main caveat of this
example is the requirement that the bias is initialized in the same manner as the other parameters. Standard
deep learning libraries (e.g. Pytorch [14]) often initialize the bias term to zero by default, while using
random initialization schemes for the other parameters.
4

Alas, we now show that even if we initialize the bias term to be exactly zero, and the input distribution is
uniform over an arbitrary origin-centered ball, we might fail to converge to the global minimum for certain
target neurons:
Theorem 3.2. Let ËœD be the uniform distribution on B = {Ëœx âˆˆRd : âˆ¥Ëœxâˆ¥â‰¤r} for some r > 0. Let
v âˆˆRd+1 such that Ëœv = (1, 0, . . . , 0)âŠ¤and bv = âˆ’
 r âˆ’
r
2d2

. Let w0 âˆˆRd+1 such that bw0 = 0 and Ëœw0
is drawn from the uniform distribution on a sphere of radius Ï > 0. Then, with probability at least 1
2 âˆ’od(1)
over the choice of w0, gradient ï¬‚ow does not converge to the global minimum.
We prove the theorem in Appendix B.2. The intuition behind the proof is the following: The target
neuron has a large negative bias, so that only a small (but positive) measure of input points are labelled
as non-zero. By randomly initializing Ëœw, with probability close to 1
2 there are no inputs that both v and
w label positively. Since the gradient is affected only by inputs that w labels positively, then during the
optimization process the gradient will be independent of the direction of v, and w will not converge to the
global minimum.
Remark 3.3. Theorem 3.2 shows that gradient ï¬‚ow is not guaranteed to converge to a global minimum
when bv is negative, instead it converges to a local minimum with a loss of F(0). However, the loss F(0) is
determined by the input distribution. Take v =
 1, 0, . . . , 0, âˆ’
 r âˆ’
r
2d2
âŠ¤considered in the theorem. On
one hand, for a uniform distribution on a ball of radius r as in the theorem we have:
F(0) = 1
2 Â· E
x

Ïƒ(vâŠ¤x)
2
= 1
2 Â· E
x

1(vâŠ¤x â‰¥0)

vâŠ¤x
2
= 1
2 Â· E
x

1

x1 â‰¥r âˆ’
r
2d2
 
x1 âˆ’

r âˆ’
r
2d2
2
â‰¤1
2 Â· r2
4d4 Â· Pr
x

x1 â‰¥r

1 âˆ’
1
2d2

â‰¤r2eâˆ’â„¦(d) .
Thus, for any reasonable r, a local minimum with loss F(0) is almost as good as the global minimum. On
the other hand, take a distribution ËœD with a support bounded in a ball of radius r, such that half of its mass
is uniformly distributed in A :=
Ëœx âˆˆRd : x1 > r âˆ’
r
4d2
	
, and the other half is uniformly distributed in
B \ A. In this case, it is not hard to see that the same proof as in Theorem 3.2 works, and gradient ï¬‚ow will
converge to a local minimum with loss F(0) = â„¦
  r
d2

, which is arbitrarily large if r is large enough.
Although in the example given in Theorem 3.2 the objective at w = 0 is almost as good as the objective
at w = v, we emphasize that w.p almost 1
2 gradient ï¬‚ow cannot reach the global minimum even asymp-
totically. On the other hand, in the bias-less case by Theorem 6.4 in [21] gradient ï¬‚ow on the same input
distribution will reach the global minimum w.p 1. Also note that the scale of the initialization of w0 has no
effect on the result.
4
Characterization of the critical points
In the previous section we have shown two examples where gradient methods on the problem of a single
neuron with bias will either get stuck in a ï¬‚at region, or converge to a local minimum. In this section we
delve deeper into the examples presented in the previous section, and give a full characterization of the
critical points of the objective. We will use the following assumption on the input distribution:
5

Assumption 4.1. The distribution ËœD on Rd has a density function p(Ëœx), and there are Î², c > 0, such that ËœD
is supported on {Ëœx : âˆ¥Ëœxâˆ¥â‰¤c}, and for every Ëœx in the support we have p(Ëœx) â‰¥Î².
The assumption essentially states that the distribution over the ï¬rst d coordinates (without the bias term)
has enough â€spreadâ€ in all directions, and covers standard distributions such as uniform over a ball of radius
c. Other similar assumptions are made in previous works (e.g. [21, 5]). We note that in [21] it is shown
that without any assumption on the distribution, it is impossible to ensure convergence, hence we must have
some kind of assumption for this problem to be learnable with gradient methods. Under this assumption we
can characterize the critical points of the objective.
Theorem 4.2. Consider the objective in Eq. (2) with v Ì¸= 0, and assume that the distribution ËœD on the ï¬rst
d coordinates satisï¬es Assumption 4.1. Then w Ì¸= 0 is a critical point of F (i.e., is a root of Eq. (3)) if and
only if it satisï¬es one of the following:
â€¢ w = v, in which case w is a global minimum.
â€¢ w = ( Ëœw, bw) where Ëœw = 0 and bw < 0.
â€¢ Ëœw Ì¸= 0 and âˆ’bw
âˆ¥Ëœwâˆ¥â‰¥c.
In the latter two cases, F(w) = F(0). Hence, if 0 is not a global minimum, then w is not a global minimum.
We note that F(0) = 1
2 Ex[Ïƒ(vâŠ¤x)2], so 0 is a global minimum only if the target neuron returns 0 with
probability 1.
Remark 4.3 (The case w = 0). We intentionally avoided characterizing the point w = 0, since the
objective is not differentiable there (this is the only point of non-differentiability), and the gradient there is
determined by the value of the ReLU activation at 0. For Ïƒâ€²(0) = 0 the gradient at w = 0 is zero, and this
is a non-differentiable saddle point. For Ïƒâ€²(0) = 1 (or any other positive value), the gradient at w = 0 is
non-zero, and it will point at a direction which depends on the distribution. We note that in [16] the authors
deï¬ne Ïƒâ€²(0) = 1, and use a symmetric distribution, in which case the gradient at w = 0 points exactly at
the direction of the target v. This is a crucial part of their convergence analysis.
We emphasize that with a bias term, there is a non-zero measure manifold of critical points (correspond-
ing to the third bullet in the theorem). On the other hand, without a bias term the only critical point besides
the global minimum (under mild assumptions on the input distribution) is at the origin w = 0 (cf. [21]).
The full proof is in Appendix C.
The assumption on the support of ËœD is made for simplicity. It can be relaxed to having a distribution with
exponentially bounded tail, e.g. standard Gaussian. In this case, some of the critical points will instead have
a non-zero gradient which is exponentially small. We emphasize that when running optimization algorithms
on ï¬nite-precision machines, which are used in practice, these â€almostâ€ critical points behave essentially
like critical points since the gradient is extremely small.
Revisiting the negative examples from Section 3, the ï¬rst example (Theorem 3.1) shows that if we do
not initialize the bias of w to zero, then there is a positive probability to initialize at a critical point which
is not the global minimum. The second example (Theorem 3.2) shows that even if we initialize the bias
of w to be zero, there is still a positive probability to converge to a critical point which is not the global
minimum. Hence, in order to guarantee convergence we need to have more assumptions on either the input
distribution, the target v or the initialization. In the next section, we show that adding such assumptions are
indeed sufï¬cient to get positive convergence guarantees.
6

5
Convergence for initialization with loss slightly better than trivial
In this section, we show that under some assumptions on the input distribution, if gradient descent is initial-
ized such that F(w0) < F(0) then it is guaranteed to converge to the global minimum. In Subsection 5.2,
we study under what conditions this is likely to occur with standard random initialization.
5.1
Convergence if F(w0) < F(0)
To state our results, we need the following assumption:
Assumption 5.1.
1. The distribution D is supported on {x âˆˆRd+1 : âˆ¥xâˆ¥â‰¤c} for some c â‰¥1.
2. The distribution ËœD over the ï¬rst d coordinates is bounded in all directions: there is câ€² > 0 such that for
every Ëœu with âˆ¥Ëœuâˆ¥= 1 and every a âˆˆR and b â‰¥0, we have PrËœxâˆ¼ËœD
ËœuâŠ¤Ëœx âˆˆ[a, a + b]

â‰¤b Â· câ€².
3. We assume w.l.o.g. that âˆ¥vâˆ¥= 1 and câ€² â‰¥1.
Assumption (3) helps simplifying some expressions in our convergence result, and is not necessary.
Assumption (2) requires that the distribution is not too concentrated in a short interval. For example, if ËœD is
spherically symmetric then the marginal density of the ï¬rst (or any other) coordinate is bounded by câ€². Note
that we do not assume that ËœD is spherically symmetric.
Theorem 5.2. Under Assumption 5.1 we have the following. Let Î´ > 0 and let w0 âˆˆRd+1 such that
F(w0) â‰¤F(0) âˆ’Î´. Let Î³ =
Î´3
3Â·122(âˆ¥w0âˆ¥+2)3c8câ€²2 . Assume that gradient descent runs starting from w0 with
step size Î· â‰¤Î³
c4 . Then, for every t we have
âˆ¥wt âˆ’vâˆ¥2 â‰¤âˆ¥w0 âˆ’vâˆ¥2 (1 âˆ’Î³Î·)t .
The formal proof appears in Appendix D, but we provide the main ideas below. First, note that
âˆ¥wt+1 âˆ’vâˆ¥2 = âˆ¥wt âˆ’Î·âˆ‡F(wt) âˆ’vâˆ¥2
= âˆ¥wt âˆ’vâˆ¥2 âˆ’2Î·âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©+ Î·2âˆ¥âˆ‡F(wt)âˆ¥2 .
Hence, in order to show that âˆ¥wt+1 âˆ’vâˆ¥2 â‰¤âˆ¥wt âˆ’vâˆ¥2 (1 âˆ’Î³Î·) we need to obtain an upper bound for
âˆ¥âˆ‡F(wt)âˆ¥and a lower bound for âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©. Achieving the lower bound for âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©is
the challenging part, and we show that in order to establish such a bound it sufï¬ces to obtain a lower bound
for Prx

wâŠ¤
t x â‰¥0, vâŠ¤x â‰¥0

. We prove that if F(wt) â‰¤F(0) âˆ’Î´ then Prx

wâŠ¤
t x â‰¥0, vâŠ¤x â‰¥0

â‰¥
Î´
c2âˆ¥wtâˆ¥. Hence, if F(wt) remains at most F(0) âˆ’Î´ for every t, then a lower bound for âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©
can be achieved, which completes the proof. However, it is not obvious that F(wt) remains at most F(0)âˆ’Î´
throughout the training process. When running gradient descent on a smooth loss function we can choose
a sufï¬ciently small step size such that the loss decreases in each step, but here the function F(w) is highly
non-smooth around w = 0. That is, the Lipschitz constant of âˆ‡F(w) is unbounded. We show that if
F(wt) â‰¤F(0) âˆ’Î´ then wt is sufï¬ciently far from 0, and hence the smoothness of F around wt can be
bounded, which allows us to choose a small step size that ensures that F(wt+1) â‰¤F(wt) â‰¤F(0) âˆ’Î´.
Hence, it follows that F(wt) remains at most F(0) âˆ’Î´ for every t.
As an aside, recall that in Section 4 we showed that other than w = v all critical points of F(w) are in
a ï¬‚at region where F(w) = F(0). Hence, the fact that F(wt) remains at most F(0) âˆ’Î´ for every t implies
7

that wt does not reach the region of bad critical points, which explains the asymptotic convergence to the
global minimum.
We also note that although we assume that the distribution has a bounded support, this assumption is
mainly made for simplicity, and can be relaxed to have sub-Gaussian distributions with bounded moments.
These distributions include, e.g. Gaussian distributions.
5.2
Convergence for Random Initialization
In Theorem 5.2 we showed that if F(w0) < F(0) then gradient descent converges to the global minimum.
We now show that under mild assumptions on the input distribution, a random initialization of w0 near zero
satisï¬es this requirement. We will need the following assumption, also used in [21, 5]:
Assumption 5.3. There are Î±, Î² > 0 s.t the distribution ËœD satisï¬es the following: For any vector Ëœw Ì¸= Ëœv,
let ËœD Ëœw,Ëœv denote the marginal distribution of ËœD on the subspace spanned by Ëœw, Ëœv (as a distribution over R2).
Then any such distribution has a density function p Ëœw,Ëœv(Ë†x) over R2 such that inf Ë†x:âˆ¥Ë†xâˆ¥â‰¤Î± p Ëœw,Ëœv(Ë†x) â‰¥Î².
The main technical tool for proving convergence under random initialization is the following:
Theorem 5.4. Assume that the input distribution D is supported on {x âˆˆRd+1 : âˆ¥xâˆ¥â‰¤c} for some
c â‰¥1, and Assumption 5.3 holds. Let v âˆˆRd+1 such that âˆ¥vâˆ¥= 1 and âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î± Â·
sin( Ï€
8 )
4
. Let
M =
Î±4Î² sin3( Ï€
8 )
256c
. Let w âˆˆRd+1 such that bw = 0, Î¸( Ëœw, Ëœv) â‰¤3Ï€
4 and âˆ¥wâˆ¥< 2M
c2 . Then, F(w) â‰¤
F(0) + âˆ¥wâˆ¥2 Â· c2
2 âˆ’âˆ¥wâˆ¥Â· M < F(0).
We prove the theorem in Appendix F. The main idea is that since
F(w) = E
x
1
2

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)
2
= F(0) + 1
2 E
x

Ïƒ(wâŠ¤x)
2
âˆ’E
x
h
Ïƒ(wâŠ¤x)Ïƒ(vâŠ¤x)
i
â‰¤F(0) + âˆ¥wâˆ¥2 Â· c2
2 âˆ’âˆ¥wâˆ¥Â· E
x
h
Ïƒ( Â¯wâŠ¤x)Ïƒ(vâŠ¤x)
i
,
then it sufï¬ces to obtain a lower bound for Ex

Ïƒ( Â¯wâŠ¤x)Ïƒ(vâŠ¤x)

. In the proof we show that such a bound
can be achieved if the conditions of the theorem hold.
Suppose that w0 is such that Ëœw0 is drawn from a spherically symmetric distribution and bw0 = 0. By
standard concentration of measure arguments, it holds w.p. at least 1 âˆ’eâ„¦(d) that Î¸( Ëœw0, Ëœv) â‰¤3Ï€
4 (where
the notation â„¦(d) hides only numerical constants, namely, it does not depend on other parameters of the
problem). Therefore, if Ëœw0 is drawn from the uniform distribution on a sphere of radius Ï < 2M
c2 , then
the theorem implies that w.h.p. we have F(w0) < F(0). For such initialization Theorem 5.2 implies
that gradient descent converges to the global minimum. For example, for Ï =
M
c2 we have w.h.p. that
F(w0) â‰¤F(0) + Ï2c2
2
âˆ’ÏM = F(0) âˆ’M2
2c2 , and thus Theorem 5.2 applies with Î´ = M2
2c2 . Thus, we have
the following corollary:
Corollary 5.5. Under Assumption 5.1 and Assumption 5.3 we have the following. Let M =
Î±4Î² sin3( Ï€
8 )
256c
, let
Ï = M
c2 , let Î´ = M2
2c2 , and let Î³ =
Î´3
3Â·122(Ï+2)3c8câ€²2 . Suppose that âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î± Â·
sin( Ï€
8 )
4
, and w0 is such that
bw0 = 0 and Ëœw0 is drawn from the uniform distribution on a sphere of radius Ï. Consider gradient descent
8

with step size Î· â‰¤Î³
c4 . Then, with probability at least 1 âˆ’eâ„¦(d) over the choice of w0 we have for every t:
âˆ¥wt âˆ’vâˆ¥2 â‰¤âˆ¥w0 âˆ’vâˆ¥2 (1 âˆ’Î³Î·)t .
We note that a similar result holds also if Ëœw0 is drawn from a normal distribution N(0, Ï2
d I).
Remark 5.6 (The assumption on bv). The assumption âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î± Â·
sin( Ï€
8 )
4
implies that the bias term bv may
be either positive or negative, but in case it is negative then it cannot be too large. This assumption is indeed
crucial for the proof, but for â€well-behavedâ€ distributions, if this assumption is not satisï¬ed (for a large
enough Î±), then the loss at F(0) is already good enough. For example, for a standard Gaussian distribution
and for every Ïµ > 0, we can choose Î± large enough such that for any bias term (positive or negative) we
either: (1) converge to the global minimum with a loss of zero, or; (2) converge to a local minimum with a
loss of F(0), which is smaller then Ïµ. Moreover, we can show that by choosing Î± appropriately, and using
the example in Theorem 3.2, if âˆ’bv
âˆ¥Ëœvâˆ¥â‰¥2Î± then gradient ï¬‚ow will converge to a non-global minimum with
loss of F(0). This means that our bound on Î± is tight up to a constant factor. For a further discussion on
the assumption on bv, and how to choose Î± see Appendix G.
Previous papers have shown separation between random features (or kernel) methods and neural net-
works in terms of their approximation power (see [20, 10], and the discussion in [11]). These works show
that under a standard Gaussian distribution, random features cannot even approximate a single ReLU neu-
ron, unless the number of features is exponential in the input dimension. That analysis crucially relies on the
single neuron having a non-zero bias term. In this work we complete the picture by showing that gradient
descent can indeed ï¬nd a near-optimal neuron with non-zero bias. Thus, we see there is indeed essentially
a separation between what can be learned using random features and using gradient descent over neural
networks.
Remark 5.7 (Learning a neuron without bias). [21] studied the problem of learning a single ReLU neuron
without bias using gradient descent on a single neuron without bias. For input distributions that are not
spherically symmetric they showed that gradient descent with random initialization near zero converges to
the global minimum w.p. at least 1
2 âˆ’od(1). Their result is also under Assumption 5.3. An immediate
corollary from the discussion above is that if we learn a single neuron without bias using gradient descent
with random initialization on a single neuron with bias, then the algorithm converges to the global minimum
w.p. at least 1âˆ’eâ„¦(d). Moreover, our proof technique can be easily adapted to the setting of learning a single
neuron without bias using gradient descent on a single neuron without bias, namely, the setting studied in
[21]. It can be shown that in this setting gradient descent converges w.h.p to the global minimum. Thus, our
technique allows us to improve the result of [21] from probability 1
2 âˆ’od(1) to probability 1 âˆ’eâ„¦(d).
6
Convergence for spread and symmetric distributions
In this section we show that under a certain set of assumptions, different from the assumptions in Section 5,
it is possible to show linear convergence of gradient descent to the global minimum. The assumptions we
make for this theorem are as follows:
Assumption 6.1.
1. The target vector v satisï¬es that bv â‰¥0 and âˆ¥Ëœvâˆ¥= 1.
2. The distribution ËœD over the ï¬rst d coordinates is spherically symmetric.
9

3. Assumption 5.3 holds, and denoting by Ï„ := EËœxâˆ¼Ëœ
D[|Ëœx1Ëœx2|]
EËœxâˆ¼Ëœ
D[Ëœx2
1] , then Î± â‰¥2.5
âˆš
2 Â· max
n
1,
1
âˆšÏ„
o
where Î± is
from Assumption 5.3.
4. Denote by c := EËœxâˆ¼ËœD

âˆ¥Ëœxâˆ¥4
, then c < âˆ.
Under these assumptions, we prove the following theorem:
Theorem 6.2. Assume we initialize w0 such that âˆ¥w0 âˆ’vâˆ¥2 < 1, bw0 â‰¥0 and that Assumption 6.1
holds. Then, there is a universal constant C, such that using gradient descent on F(w) with step size
Î· < C Â·
Î²
cÎ±2 min{1, Ï„} yields that for every t we have âˆ¥wt âˆ’vâˆ¥2 â‰¤(1 âˆ’Î·Î»)tâˆ¥w0 âˆ’vâˆ¥2 , for Î» = C Â·
Î²
cÎ±2 .
This result has several advantages and disadvantages compared to those of the previous section. The
main disadvantage is that the assumptions are generally more stringent: We focus only on positive target
biases (bv â‰¥0) and spherically symmetric distributions ËœD. Also we require a certain technical assumption
on the the distribution, as speciï¬ed by Ï„, which are satisï¬ed for standard spherically symmetric distributions,
but is a bit non-trivial2. Finally, the assumption on the initialization (âˆ¥w0 âˆ’vâˆ¥2 < 1 and bw0 â‰¥0) is much
more restrictive (although see Remark 6.3 below). In contrast, the initialization assumption in the previous
section holds with probability close to 1 with random initialization. On the positive side, the convergence
rate does not depend on the initialization, i.e., here by initializing with any w0 such that âˆ¥w0 âˆ’vâˆ¥2 < 1
and bw0 â‰¥0, we get a convergence rate that only depends on the input distribution. On the other hand, in
Theorem 5.2, the convergence rate depends on the parameter Î´ which depends on the initialization. Also,
the distribution is not necessarily bounded â€“ we only require its fourth moment to be bounded.
Remark 6.3 (Random initialization). For bv = 0 the initialization assumption (âˆ¥w0 âˆ’vâˆ¥2 < 1) is satisï¬ed
with probability close to 1/2 with standard initializations, see Lemma 5.1 from [21]). For bv > 0, a similar
argument applies if bw is initialized close enough to bv.
The proof of the theorem is quite different from the proofs in Section 5, and is more geometrical in
nature, extending previously used techniques from [21, 5]. It contains two major parts: The ï¬rst part is an
extension of the methods from [21] to the case of adding a bias term. Speciï¬cally, we show a lower bound
on âŸ¨âˆ‡F(w), w âˆ’vâŸ©, which depends on both the angle between Ëœw and Ëœv, and the bias terms bw and bv (see
Theorem A.2). This result implies that for suitable values of w, gradient descent will decrease the distance
from v. The second part of the proof is showing that throughout the optimization process, w will stay in an
area where we can apply the result above. Speciï¬cally, the intricate part is showing that the term âˆ’bw
âˆ¥Ëœwâˆ¥does
not get too large. Note that due to Theorem 4.2, we know that keeping this term small means that w stays
away from the cone of bad critical points which are not the global minimum. The full proof can be found in
Appendix E.
7
Discussion
In this work we studied the problem of learning a single neuron with a bias term using gradient descent.
We showed several negative results, indicating that adding a bias term makes the problem more difï¬cult
than without a bias term. Next, we gave a characterization of the critical points of the problem under
some assumptions on the input distribution, showing that there is a manifold of critical points which are
not the global minimum. We proved two convergence results using different techniques and under different
2For example, for standard Gaussian distribution, we have that Ï„ =
2
Ï€ â‰ˆ0.63, hence we can take Î± = 4.5, and Î² = O(1).
Since the distribution ËœD is symmetric, we present the assumption w.l.o.g with respect to the ï¬rst 2 coordinates.
10

assumptions. Finally, we showed that under mild assumptions on the input distribution, reaching the global
minimum can be achieved by standard random initialization.
We emphasize that previous works studying the problem of a single neuron either considered non-
standard algorithms (e.g. Isotron), or required assumptions on the input distribution which do not allow a
bias term. Hence, this is the ï¬rst work we are aware of which gives positive and negative results on the
problem of learning a single neuron with a bias term using gradient methods.
In this work we focused on the gradient descent algorithm. We believe that our results can also be ex-
tended to the commonly used SGD algorithm, using similar techniques to [21, 15], and leave it for future
work. Another interesting future direction is analyzing other previously studied settings, but with the ad-
dition of a bias term. These settings can include convolutional networks, two layers neural networks, and
agnostic learning of a single neuron.
References
[1] A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,
2017.
[2] S. Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980, 2014.
[3] I. Diakonikolas, S. Goel, S. Karmalkar, A. R. Klivans, and M. Soltanolkotabi. Approximation schemes
for relu regression. In Conference on Learning Theory, pages 1452â€“1485. PMLR, 2020.
[4] S. S. Du, J. D. Lee, and Y. Tian.
When is a convolutional ï¬lter easy to learn?
arXiv preprint
arXiv:1709.06129, 2017.
[5] S. Frei, Y. Cao, and Q. Gu. Agnostic learning of a single neuron with gradient descent. arXiv preprint
arXiv:2005.14426, 2020.
[6] S. Goel, S. Karmalkar, and A. Klivans. Time/accuracy tradeoffs for learning a relu with respect to
gaussian marginals. arXiv preprint arXiv:1911.01462, 2019.
[7] S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efï¬cient learning of generalized linear and single
index models with isotonic regression. In Advances in Neural Information Processing Systems, pages
927â€“935, 2011.
[8] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT.
Citeseer, 2009.
[9] S. M. M. Kalan, M. Soltanolkotabi, and A. S. Avestimehr. Fitting relus via sgd and quantized sgd. In
2019 IEEE International Symposium on Information Theory (ISIT), pages 2469â€“2473. IEEE, 2019.
[10] P. Kamath, O. Montasser, and N. Srebro.
Approximate is good enough: Probabilistic variants of
dimensional and margin complexity. In Conference on Learning Theory, pages 2236â€“2262. PMLR,
2020.
[11] E. Malach, P. Kamath, E. Abbe, and N. Srebro. Quantifying the beneï¬t of using differentiable learning
over tangent kernels. arXiv preprint arXiv:2103.01210, 2021.
11

[12] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for non-convex losses. arXiv
preprint arXiv:1607.06534, 2016.
[13] S. Oymak and M. Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the
shortest path? arXiv preprint arXiv:1812.10004, 2018.
[14] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint
arXiv:1912.01703, 2019.
[15] O. Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In International
Conference on Machine Learning, pages 144â€“152, 2015.
[16] M. Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information Processing
Systems, pages 2007â€“2017, 2017.
[17] Y. S. Tan and R. Vershynin. Online stochastic gradient descent with arbitrary initialization solves
non-smooth, non-convex phase retrieval. arXiv preprint arXiv:1910.12837, 2019.
[18] Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications
in convergence and critical point analysis. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 3404â€“3413. JMLR. org, 2017.
[19] G. Vardi and O. Shamir. Implicit regularization in relu networks with the square loss. arXiv preprint
arXiv:2012.05156, 2020.
[20] G. Yehudai and O. Shamir. On the power and limitations of random features for understanding neural
networks. In Advances in Neural Information Processing Systems, 2019.
[21] G. Yehudai and O. Shamir.
Learning a single neuron with gradient methods.
arXiv preprint
arXiv:2001.05205, 2020.
Appendices
A
Auxiliary Results
In this appendix we extend several key results from [21] for the case of adding a bias term. Speciï¬cally, we
extend Theorem 4.2 from [21] which shows that under mild assumptions on the distribution, the gradient of
the loss points in a good direction which depends on the angle between the learned vector w and the target
v. We also bound the volume of a certain set in R2, which can be seen as an extension of Lemma B.1 from
[21].
Lemma A.1. Let P = {y âˆˆR2 : wâŠ¤y > b, vâŠ¤y > b, âˆ¥yâˆ¥â‰¤Î±} for b âˆˆR and w, v âˆˆR2 with
âˆ¥wâˆ¥, âˆ¥vâˆ¥= 1 and Î¸(w, v) â‰¤Ï€ âˆ’Î´ for Î´ âˆˆ[0, Ï€]. If b < Î± sin
  Î´
2

then Vol(P) â‰¥(Î± sin( Î´
2)âˆ’b)
2
4 sin( Î´
2)
.
12

Proof. The volume of P is smallest when the angle is exactly Ï€ âˆ’Î´, thus we can lower bound the volume
by assuming that Î¸(w, v) = Ï€ âˆ’Î´. Next, we can rotate to coordinates to consider without loss of generality
the volume of the set
P â€² =

(y1, y2) âˆˆR2 : Î¸((y1, y2 âˆ’bâ€²), e2) â‰¤Î´/2, âˆ¥(y1, y2)âˆ¥â‰¤Î±
	
,
where bâ€² =
b
sin(Î´/2) and e2 = (0, 1). Let P â€²â€² = {(x, y) âˆˆR2 : x2 + (y âˆ’bâ€²)2 â‰¤(Î± âˆ’bâ€²)2} be the
disc of radius Î± âˆ’bâ€² around the point (0, bâ€²). It is enough to bound the volume of P â€² âˆ©P â€²â€². We deï¬ne the
rectangular sets:
P1 =
(Î± âˆ’bâ€²)
2
sin
Î´
4

, (Î± âˆ’bâ€²) sin
Î´
4

Ã—

bâ€² + (Î± âˆ’bâ€²)
2
cos
Î´
4

, bâ€² + (Î± âˆ’bâ€²) cos
Î´
4

P2 =

âˆ’(Î± âˆ’bâ€²) sin
Î´
4

, âˆ’(Î± âˆ’bâ€²)
2
sin
Î´
4

Ã—

bâ€² + (Î± âˆ’bâ€²)
2
cos
Î´
4

, bâ€² + (Î± âˆ’bâ€²) cos
Î´
4

See Figure 1 for an illustration. We have that P1, P2 âŠ†P â€² âˆ©P â€²â€². We will show it for P1, the same
argument also works for P2. First, P1 âŠ†P â€²â€² is immediate by the deï¬nition of the two sets. For P â€², the
straight line in the boundary of P â€² is deï¬ned by y2 = bâ€² + y1 Â·
cos( Î´
2)
sin( Î´
2) . It can be seen that each vertex of the
rectangle P1, is above this line. Moreover, the norm of each vertex of P1 is at most Î±. Hence all the vertices
are inside P â€², which means that P1 âŠ†P â€². In total we get:
Vol(P) â‰¥Vol(P â€² âˆ©P â€²â€²) â‰¥Vol(P1 âˆªP2)
= (Î± âˆ’bâ€²)2
2
sin
Î´
4

cos
Î´
4

=
 Î± sin
  Î´
2

âˆ’b
2
4 sin
  Î´
2

Theorem A.2. Let w, v âˆˆRd+1 , denote by Ëœw, Ëœv their ï¬rst d coordinates and by bw, bv their last coordinate.
Assume that Î¸( Ëœw, Ëœv) â‰¤Ï€ âˆ’Î´ for some Î´ âˆˆ[0, Ï€), and that the distribution D is such that its ï¬rst d
coordinates satisfy Assumption 4.1 (1) from [21], and that its last coordinate is a constant 1. Denote
bâ€² = max{âˆ’bw/âˆ¥Ëœwâˆ¥, âˆ’bv/âˆ¥Ëœvâˆ¥, 0} Â·
1
sin( Î´
2), and assume that bâ€² < Î±, then:
âŸ¨âˆ‡F(w), w âˆ’vâŸ©â‰¥(Î± âˆ’bâ€²)4 sin
  Î´
4
3 Î²
84
Â· min

1, 1
Î±2

âˆ¥w âˆ’vâˆ¥2
Proof. Let Ëœx be the ï¬rst d coordinates of x. We have that:
âŸ¨âˆ‡F(w), w âˆ’vâŸ©= Exâˆ¼D
h
Ïƒâ€²(wâŠ¤x)(Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x))(wâŠ¤x âˆ’vâŠ¤x)
i
â‰¥Exâˆ¼D
h
1(wâŠ¤x > 0, vâŠ¤x > 0)(wâŠ¤x âˆ’vâŠ¤x)2i
= âˆ¥w âˆ’vâˆ¥2 Â· Exâˆ¼D
h
1( ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx > âˆ’bv)((w âˆ’v)âŠ¤x)2i
â‰¥âˆ¥w âˆ’vâˆ¥2 Â·
inf
uâˆˆspan{w,v},âˆ¥uâˆ¥=1 Exâˆ¼D
h
1( ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx > âˆ’bv)(uâŠ¤x)2i
13

Figure 1: An illustration of the set P â€² (in red), the circle P â€²â€² (in blue) and the two rectangles P1, P2 (in
black), for the case of Î´ = Ï€/2, Î± = 1 and b = 0.3. For b = 0, P â€² would be a pie slice, and the blue circle
P â€²â€² will coincide with the red circle.
Let b = max{âˆ’bw/âˆ¥Ëœwâˆ¥, âˆ’bv/âˆ¥Ëœvâˆ¥, 0}, then we can bound the above equation by:
âˆ¥w âˆ’vâˆ¥2 Â·
inf
uâˆˆspan{w,v},âˆ¥uâˆ¥=1 Exâˆ¼D
h
1( Ëœw
âŠ¤Ëœx > b, Ëœv
âŠ¤Ëœx > b)(uâŠ¤x)2i
â‰¥âˆ¥w âˆ’vâˆ¥2 Â·
inf
uâˆˆspan{w,v},âˆ¥uâˆ¥=1 EËœxâˆ¼ËœD
h
1( Ëœw
âŠ¤Ëœx > b, Ëœv
âŠ¤Ëœx > b, âˆ¥Ëœxâˆ¥â‰¤Î±)(ËœuâŠ¤Ëœx + bu)2i
(4)
Here bu is the bias term of u, Ëœu are the ï¬rst d coordinates of u and ËœD is the marginal distribution of x
on its ï¬rst d coordinates. Note that since the last coordinate represents the bias term, then the distribution
on the last coordinate of x is a constant 1. The condition that âˆ¥uâˆ¥= 1 (equivalently âˆ¥uâˆ¥2 = 1) translates to
âˆ¥Ëœuâˆ¥2 + b2
u = 1.
Our goal is to bound the term inside the inï¬mum. Note that the expression inside the distribution depends
just on inner products of Ëœx with Ëœw or Ëœv, hence we can consider the marginal distribution D Ëœw,Ëœv of Ëœx on the
2-dimensional subspace spanned by Ëœw and Ëœv (with density function p Ëœw,Ëœv). Let Ë†w and Ë†v be the projections
of Ëœw and Ëœv on that subspace. Let P = {y âˆˆR2 : Ë†w
âŠ¤y > b, Ë†v
âŠ¤y > b, âˆ¥yâˆ¥â‰¤Î±}, then we can bound
Eq. (4) with:
14

âˆ¥w âˆ’vâˆ¥2 Â·
inf
uâˆˆR2,buâˆˆR:âˆ¥uâˆ¥2+b2u=1 Eyâˆ¼D Ëœ
w,Ëœv
h
1(y âˆˆP) Â· (uâŠ¤y + bu)2i
= âˆ¥w âˆ’vâˆ¥2 Â·
inf
uâˆˆR2,buâˆˆR:âˆ¥uâˆ¥2+b2u=1
Z
yâˆˆR2 1(y âˆˆP) Â· (uâŠ¤y + bu)2p Ëœw,Ëœv(y)dy
â‰¥Î²âˆ¥w âˆ’vâˆ¥2 Â·
inf
uâˆˆR2,buâˆˆR:âˆ¥uâˆ¥2+b2u=1
Z
yâˆˆP
(uâŠ¤y + bu)2dy
Combining with Proposition A.3 ï¬nishes the proof
Proposition A.3. Let P = {y âˆˆR2 : Ë†w
âŠ¤y > b, Ë†v
âŠ¤y > b, âˆ¥yâˆ¥â‰¤Î±} for b âˆˆR and Ë†w, Ë†v âˆˆR2 with
Î¸( Ë†w, Ë†v) â‰¤Ï€ âˆ’Î´ for Î´ âˆˆ[0, Ï€]. Then
inf
uâˆˆR2,buâˆˆR:âˆ¥uâˆ¥2+b2u=1
Z
yâˆˆP
(uâŠ¤y + bu)2dy â‰¥(Î± âˆ’bâ€²)4 sin
  Î´
4
3
84
Â· min

1, 1
Î±2

for bâ€² =
b
sin( Î´
2).
Proof. As in the proof of Lemma A.1, we consider the rectangular sets:
P1 =
(Î± âˆ’bâ€²)
2
sin
Î´
4

, (Î± âˆ’bâ€²) sin
Î´
4

Ã—

bâ€² + (Î± âˆ’bâ€²)
2
cos
Î´
4

, bâ€² + (Î± âˆ’bâ€²) cos
Î´
4

P2 =

âˆ’(Î± âˆ’bâ€²) sin
Î´
4

, âˆ’(Î± âˆ’bâ€²)
2
sin
Î´
4

Ã—

bâ€² + (Î± âˆ’bâ€²)
2
cos
Î´
4

, bâ€² + (Î± âˆ’bâ€²) cos
Î´
4

with bâ€² =
b
sin(Î´/2). Since we have P1 âˆªP2 âŠ†P, and the function inside the integral is positive, we can lower
bound the target integral by integrating only over P1 âˆªP2. Now we have:
inf
uâˆˆR2,buâˆˆR:âˆ¥uâˆ¥2+b2u=1
Z
yâˆˆP
(uâŠ¤y + bu)2dy
â‰¥
inf
u1,u2,buâˆˆR:u2
1+u2
2+b2u=1
Z
yâˆˆP1âˆªP2
(u1y1 + u2y2 + bu)2dy
=
inf
u1,u2,buâˆˆR:u2
1+u2
2+b2u=1
Z
yâˆˆP1âˆªP2
(u1y1)2dy +
Z
yâˆˆP1âˆªP2
(u2y2 + bu)2dy +
Z
yâˆˆP1âˆªP2
2u1y1(u2y2 + bu)dy
=
inf
u1,u2,buâˆˆR:u2
1+u2
2+b2u=1
Z
yâˆˆP1âˆªP2
(u1y1)2dy +
Z
yâˆˆP1âˆªP2
(u2y2 + bu)2dy
where in the last equality we used that P1 âˆªP2 are symmetric around the y2 axis, i.e. (y1, y2) âˆˆP1 âˆªP2 iff
(âˆ’y1, y2) âˆˆP1 âˆªP2. By the condition that u2
1 + u2
2 + b2
u = 1 we know that either u2
1 â‰¥1
2 or u2
2 + b2
u â‰¥1
2.
Using that both integrals above are positive, we can lower bound:
inf
u1,u2,buâˆˆR:u2
1+u2
2+b2u=1
Z
yâˆˆP1âˆªP2
(u1y1)2dy +
Z
yâˆˆP1âˆªP2
(u2y2 + bu)2dy
â‰¥min
(
1
2
Z
yâˆˆP1âˆªP2
y2
1dy,
inf
u2,u3âˆˆR:u2
2+u2
3= 1
2
Z
yâˆˆP1âˆªP2
(u2y2 + u3)2dy
)
.
15

We will now lower bound both terms in the above equation. For the ï¬rst term, note that for every
y âˆˆP1 âˆªP2 we have that |y1| â‰¥(Î±âˆ’bâ€²)
2
sin
  Î´
4

. Hence we have:
1
2
Z
yâˆˆP1âˆªP2
y2
1dy â‰¥
â‰¥1
2
Z
yâˆˆP1âˆªP2
(Î± âˆ’bâ€²)2
4
sin
Î´
4
2
dy
=(Î± âˆ’bâ€²)2
8
sin
Î´
4
2
Â· (Î± âˆ’bâ€²)2
2
sin
Î´
4

cos
Î´
4

â‰¥(Î± âˆ’bâ€²)4
16
âˆš
2
sin
Î´
4
3
(5)
where in the last inequality we used that Î´ âˆˆ[0, Ï€], hence Î´/4 âˆˆ[0, Ï€/4].
For the second term we have:
inf
u2,u3âˆˆR:u2
2+u2
3= 1
2
Z
yâˆˆP1âˆªP2
(u2y2 + u3)2dy
=
inf
uâˆˆ
h
âˆ’1
âˆš
2 , 1
âˆš
2
i
Z
yâˆˆP1âˆªP2
 
uy2 +
r
1
2 âˆ’u2
!2
dy
=(Î± âˆ’bâ€²) sin
Î´
4

inf
uâˆˆ
h
âˆ’1
âˆš
2 , 1
âˆš
2
i
Z
y2âˆˆC
 
uy2 +
r
1
2 âˆ’u2
!2
dy2 .
(6)
The last equality is given by changing the order of integration into integral over y2 and then over y1, denoting
the interval C =
h
bâ€² + (Î±âˆ’bâ€²)
2
cos
  Î´
4

, bâ€² + (Î± âˆ’bâ€²) cos
  Î´
4
i
, and noting that the term inside the integral
does not depend on y1.
Fix some u âˆˆ
h
âˆ’1
âˆš
2,
1
âˆš
2
i
. If u = 0, then we can bound Eq. (6) by (Î±âˆ’bâ€²)2
4
sin
  Î´
4

cos
  Î´
4

. Assume
u Ì¸= 0, we split into cases and bound the term inside the integral:
Case I:

q
1
2 âˆ’u2
u
 â‰¥bâ€² + 3
4 Â· (Î± âˆ’bâ€²) cos
  Î´
4

. In this case, solving the inequality for u we have
|u| â‰¤
r
1
2+2(bâ€²+ 3
4 (Î±âˆ’bâ€²) cos( Î´
4))
2 . Hence, we can also bound:
r
1
2 âˆ’u2 â‰¥
s
1
2 âˆ’
1
2 + 2
 bâ€² + 3
4(Î± âˆ’bâ€²) cos
  Î´
4
2 =
v
u
u
t
 bâ€² + 3
4(Î± âˆ’bâ€²) cos
  Î´
4
2
2 + 2
 bâ€² + 3
4(Î± âˆ’bâ€²) cos
  Î´
4
2
16

In particular, for every y2 âˆˆ
h
bâ€² + (Î±âˆ’bâ€²)
2
cos
  Î´
4

, bâ€² + 5(Î±âˆ’bâ€²)
8
cos
  Î´
4
i
we get that:
uy2 +
r
1
2 âˆ’u2

â‰¥

v
u
u
t
 bâ€² + 3
4(Î± âˆ’bâ€²) cos
  Î´
4
2
2 + 2
 bâ€² + 3
4(Î± âˆ’bâ€²) cos
  Î´
4
2 âˆ’
v
u
u
t
 bâ€² + 5
8(Î± âˆ’bâ€²) cos
  Î´
4
2
2 + 2
 bâ€² + 3
4(Î± âˆ’bâ€²) cos
  Î´
4
2

â‰¥
(Î± âˆ’bâ€²) cos
  Î´
4

8
q
2 + 2
 bâ€² + (Î± âˆ’bâ€²) cos
  Î´
4
2
Case II:

q
1
2 âˆ’u2
u
 < bâ€² + 3
4 Â· (Î± âˆ’bâ€²) cos
  Î´
4

. Using the same reasoning as above, we get for every
y2 âˆˆ
h
bâ€² + 7(Î±âˆ’bâ€²)
8
cos
  Î´
4

, bâ€² + (Î± âˆ’bâ€²) cos
  Î´
4
i
that:
uy2 +
r
1
2 âˆ’u2
 â‰¥
(Î± âˆ’bâ€²) cos
  Î´
4

8
q
2 + 2
 bâ€² + (Î± âˆ’bâ€²) cos
  Î´
4
2
Combining the above cases with Eq. (6) we get that:
inf
u2,u3âˆˆR:u2
2+u2
3= 1
2
Z
yâˆˆP1âˆªP2
(u2y2 + u3)2dy
â‰¥(Î± âˆ’bâ€²) sin
Î´
4
 Z
y2âˆˆC
(Î± âˆ’bâ€²)2 cos
  Î´
4
2
82(2 + 2
 bâ€² + (Î± âˆ’bâ€²) cos
  Î´
4
2)
dy2
â‰¥
(Î± âˆ’bâ€²)4 cos
  Î´
4
3 sin
  Î´
4

2 Â· 82  2 + 2
 bâ€² + (Î± âˆ’bâ€²) cos
  Î´
4
2
â‰¥
(Î± âˆ’bâ€²)4 sin
  Î´
4
3
2 Â· 82âˆš
2
 2 + 2
 bâ€² + (Î± âˆ’bâ€²) cos
  Î´
4
2
â‰¥(Î± âˆ’bâ€²)4 sin
  Î´
4
3
84
Â· min

1, 1
Î±2

(7)
where in the second inequality we used that for Î´ âˆˆ[0, Ï€] we have sin
  Î´
4

â‰¤cos
  Î´
4

, and in the last
inequality we used that bâ€² â‰¤Î±, and (Î±âˆ’bâ€²) cos
  Î´
4

â‰¤Î±. Combining Eq. (5) with Eq. (7) ï¬nishes the proof.
B
Proofs from Section 3
B.1
Proof of Theorem 3.1
Let Ïµ > 0, for the input distribution, we consider the uniform distribution on the ball of radius Ïµ. Let bw be
the last coordinate of w, and denote by Ëœw, Ëœx the ï¬rst d coordinates of w and x. Using the assumption on
17

the initialization of w0 and on the boundness of the distribution ËœD we have:
|âŸ¨Ëœw0, ËœxâŸ©| â‰¤âˆ¥Ëœw0âˆ¥âˆ¥Ëœxâˆ¥â‰¤Ïµ
âˆš
d.
Since bw0 is also initialized with U([âˆ’1, 1]), w.p > 1/2 âˆ’Ïµ
âˆš
d we have that bw0 < âˆ’Ïµ
âˆš
d. If this event
happens, since the activation is ReLU we get that Ïƒâ€²(âŸ¨w0, xâŸ©) = 1(âŸ¨Ëœw0, ËœxâŸ©+ bw0 > 0) = 0 for every Ëœx in
the support of the distribution. Using Eq. (3) we get that âˆ‡F(w0) = 0, hence gradient ï¬‚ow will get stuck
at its initial value.
B.2
Proof of Theorem 3.2
Lemma B.1. Let w âˆˆRd+1 such that bw = 0, Ëœw1 < âˆ’4
âˆš
d, and âˆ¥Ëœw2:dâˆ¥â‰¤2
âˆš
d. Then,
Pr
xâˆ¼D
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
= 0 .
Proof. If vâŠ¤x â‰¥0 then x1 â‰¥r âˆ’
r
2d2 and hence x2
1 â‰¥r2 âˆ’r2
d2 . Since we also have âˆ¥Ëœxâˆ¥â‰¤r then
âˆ¥Ëœx2:dâˆ¥2 = âˆ¥Ëœxâˆ¥2 âˆ’x2
1 â‰¤r2 âˆ’

r2 âˆ’r2
d2

= r2
d2 .
Hence,
Pr
xâˆ¼D
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
â‰¤Pr
xâˆ¼D
h
wâŠ¤x â‰¥0, x1 â‰¥r âˆ’
r
2d2 , âˆ¥Ëœx2:dâˆ¥â‰¤r
d
i
.
Since bw = 0, âˆ¥Ëœw2:dâˆ¥â‰¤2
âˆš
d and Ëœw1 < âˆ’4
âˆš
d, then for every Ëœx âˆˆB such that x1 â‰¥r âˆ’
r
2d2 â‰¥r
2 and
âˆ¥Ëœx2:dâˆ¥â‰¤r
d we have
wâŠ¤x = ËœwâŠ¤Ëœx = Ëœw1Ëœx1 + âŸ¨Ëœw2:d, Ëœx2:dâŸ©< âˆ’4
âˆš
d
Â· r
2 + 2
âˆš
d Â· r
d = 0 .
Therefore, Prxâˆ¼D

wâŠ¤x â‰¥0, vâŠ¤x â‰¥0

= 0.
Lemma B.2. With probability 1
2 âˆ’od(1) over the choice of w0, we have
Pr
xâˆ¼D
h
wâŠ¤
0 x â‰¥0, vâŠ¤x â‰¥0
i
= 0 .
Proof. Let w âˆˆRd+1 such that bw = 0 and Ëœw âˆ¼N(0, Id). Since Ëœw1 has a standard normal distribution,
then we have Ëœw1 < âˆ’4
âˆš
d with probability 1
2 âˆ’od(1). Moreover, note that âˆ¥Ëœw2:dâˆ¥2 has a chi-square distri-
bution and the probability of âˆ¥Ëœw2:dâˆ¥2 â‰¤4d is 1 âˆ’od(1). Hence, by Lemma B.1, with probability 1
2 âˆ’od(1)
over the choice of w, we have
Pr
xâˆ¼D
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
= 0 .
Therefore,
Pr
xâˆ¼D

Ï wâŠ¤
âˆ¥wâˆ¥x â‰¥0, vâŠ¤x â‰¥0

= Pr
xâˆ¼D
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
= 0 .
Since Ï wâŠ¤
âˆ¥wâˆ¥has the distribution of w0, the lemma follows.
18

Lemma B.3. Assume that w0 satisï¬es Prxâˆ¼D

wâŠ¤
0 x â‰¥0, vâŠ¤x â‰¥0

= 0. Let Î³ > 0 and let w âˆˆRd+1
such that Ëœw = Î³ Ëœw0, and bw â‰¤0. Then, Prxâˆ¼D

wâŠ¤x â‰¥0, vâŠ¤x â‰¥0

= 0. Moreover, we have
â€¢ If âˆ’bw
âˆ¥Ëœwâˆ¥< r, then d Ëœw
dt = âˆ’s Ëœw for some s > 0, and dbw
dt < 0.
â€¢ If âˆ’bw
âˆ¥Ëœwâˆ¥â‰¥r, then d Ëœw
dt = 0 and dbw
dt = 0.
Proof. For every x we have: If wâŠ¤x = Î³ ËœwâŠ¤
0 Ëœx+bw â‰¥0 then Î³ ËœwâŠ¤
0 Ëœx â‰¥0, and therefore wâŠ¤
0 x = ËœwâŠ¤
0 Ëœx â‰¥0.
Thus
Pr
xâˆ¼D
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
â‰¤Pr
xâˆ¼D
h
wâŠ¤
0 x â‰¥0, vâŠ¤x â‰¥0
i
= 0 .
(8)
We have
âˆ’d Ëœw
dt = âˆ‡ËœwF(w) = E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

Ïƒâ€²(wâŠ¤x)Ëœx
= E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

1(wâŠ¤x â‰¥0)Ëœx
= E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

1(wâŠ¤x â‰¥0, vâŠ¤x < 0)Ëœx
+ E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

1(wâŠ¤x â‰¥0, vâŠ¤x â‰¥0)Ëœx
(Eq. (8))
=
E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

1(wâŠ¤x â‰¥0, vâŠ¤x < 0)Ëœx
= E
x

Ïƒ(wâŠ¤x)

Ëœx
= E
Ëœx 1( ËœwâŠ¤Ëœx > âˆ’bw)( ËœwâŠ¤Ëœx + bw)Ëœx .
If âˆ’bw
âˆ¥Ëœwâˆ¥â‰¥r then for every Ëœx âˆˆB we have ËœwâŠ¤Ëœx â‰¤âˆ¥Ëœwâˆ¥r â‰¤âˆ’bw and hence d Ëœw
dt = 0. Note that if
âˆ’bw
âˆ¥Ëœwâˆ¥< r, i.e., âˆ¥Ëœwâˆ¥r > âˆ’bw, then PrËœx
 ËœwâŠ¤Ëœx > âˆ’bw

> 0. Since ËœD is spherically symmetric, then we
obtain d Ëœw
dt = âˆ’s Ëœw for some s > 0.
Next, we have
âˆ’dbw
dt = âˆ‡bwF(w) = E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

Ïƒâ€²(wâŠ¤x) Â· 1
= E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

1(wâŠ¤x â‰¥0)
(Eq. (8))
=
E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

1(wâŠ¤x â‰¥0, vâŠ¤x < 0)
= E
x

Ïƒ(wâŠ¤x)

= E
Ëœx 1( ËœwâŠ¤Ëœx > âˆ’bw)( ËœwâŠ¤Ëœx + bw) .
If âˆ’bw
âˆ¥Ëœwâˆ¥â‰¥r then for every Ëœx âˆˆB we have ËœwâŠ¤Ëœx â‰¤âˆ¥Ëœwâˆ¥r â‰¤âˆ’bw and hence dbw
dt = 0. Otherwise, we have
dbw
dt < 0.
Proof of Theorem 3.2. By Lemma B.2 w0 satisï¬es Prxâˆ¼D

wâŠ¤
0 x â‰¥0, vâŠ¤x â‰¥0

= 0 w.p. at least 1
2 âˆ’
od(1). Then, by Lemma B.3 we have for every t > 0 that Ëœwt = Î³t Ëœw0 for some Î³t > 0, bwt < 0, and
19

âˆ’bwt
âˆ¥Ëœwtâˆ¥â‰¤r. Moreover, we have Prxâˆ¼D

wâŠ¤
t x â‰¥0, vâŠ¤x â‰¥0

= 0. Hence, for every t we have
F(wt) = 1
2 Â· E
x

Ïƒ(wâŠ¤
t x) âˆ’Ïƒ(vâŠ¤x)
2
= 1
2 Â· E
x

Ïƒ(wâŠ¤
t x)
2
+ 1
2 Â· E
x

Ïƒ(vâŠ¤x)
2
âˆ’E
x

Ïƒ(wâŠ¤
t x)Ïƒ(vâŠ¤x)

= 1
2 Â· E
x

Ïƒ(wâŠ¤
t x)
2
+ 1
2 Â· E
x

Ïƒ(vâŠ¤x)
2
â‰¥1
2 Â· E
x

Ïƒ(vâŠ¤x)
2
= F(0) .
Thus, gradient ï¬‚ow does not converge to the global minimum F(v) = 0 < F(0).
C
Proofs from Section 4
Proof of Theorem 4.2. The gradient of the objective is:
âˆ‡F(w) = E
xâˆ¼D
h
Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

Â· Ïƒâ€²(wâŠ¤x)x
i
.
We can rewrite it using that Ïƒ is the ReLU activation, and separating the bias terms:
âˆ‡F(w) = E
Ëœxâˆ¼ËœD
h
Ïƒ( ËœwâŠ¤Ëœx + bw) âˆ’Ïƒ(ËœvâŠ¤Ëœx + bv)

Â· 1( ËœwâŠ¤Ëœx + bw > 0)x
i
.
First, notice that if Ëœw = 0 and bw < 0 then 1( ËœwâŠ¤Ëœx + bw > 0) = 0 for all Ëœx, hence âˆ‡F(w) = 0. Second,
using Cauchy-Schwartz we have that |âŸ¨Ëœw, ËœxâŸ©| â‰¤c Â· âˆ¥Ëœwâˆ¥. Hence, for w with Ëœw Ì¸= 0 and âˆ’bw
âˆ¥Ëœwâˆ¥â‰¥c we have
that 1( ËœwâŠ¤Ëœx + bw > 0) = 0 for all Ëœx in the support of the distribution, hence âˆ‡F(w) = 0. Lastly, it is
clear that for w = v we have that âˆ‡F(w) = 0. This shows that the points described in the statement of the
proposition are indeed critical points. Next we will show that these are the only critical points.
Let w âˆˆRd+1 which is not a critical point deï¬ned above - i.e. either Ëœw = 0 and bw > 0, or Ëœw Ì¸= 0 and
âˆ’bw
âˆ¥Ëœwâˆ¥< c. Then we have:
âŸ¨âˆ‡F(w), w âˆ’vâŸ©= Exâˆ¼D
h
Ïƒâ€²(wâŠ¤x)(Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x))(wâŠ¤x âˆ’vâŠ¤x)
i
= Exâˆ¼D
h
1(wâŠ¤x > 0, vâŠ¤x > 0)(Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x))(wâŠ¤x âˆ’vâŠ¤x)
i
+
+ Exâˆ¼D
h
1(wâŠ¤x > 0, vâŠ¤x â‰¤0)Ïƒ(wâŠ¤x)(wâŠ¤x âˆ’vâŠ¤x)
i
â‰¥Exâˆ¼D
h
1(wâŠ¤x > 0, vâŠ¤x > 0)(wâŠ¤x âˆ’vâŠ¤x)2i
+
+ Exâˆ¼D
h
1(wâŠ¤x > 0, vâŠ¤x â‰¤0)(wâŠ¤x)2i
.
= Exâˆ¼D
h
1( ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx > âˆ’bv)(wâŠ¤x âˆ’vâŠ¤x)2i
+
+ Exâˆ¼D
h
1( ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx â‰¤âˆ’bv)(wâŠ¤x)2i
.
(9)
Denote:
A1 := {Ëœx âˆˆRd : ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx > âˆ’bv, âˆ¥Ëœxâˆ¥< c}
A2 := {Ëœx âˆˆRd : ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx â‰¤âˆ’bv, âˆ¥Ëœxâˆ¥< c}
20

Since w is not a critical point as deï¬ned above, we know that the set {Ëœx âˆˆRd : ËœwâŠ¤Ëœx > âˆ’bw, âˆ¥Ëœxâˆ¥< c} has
a positive measure, hence either A1 or A2 have a positive measure. Assume w.l.o.g that A1 have a positive
measure, the other case is similar. Since both terms inside the expectations of Eq. (9) are positive, we can
lower bound it with:
Exâˆ¼D
h
1( ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx > âˆ’bv)(wâŠ¤x âˆ’vâŠ¤x)2i
= âˆ¥w âˆ’vâˆ¥2Exâˆ¼D
h
1(Ëœx âˆˆA1)((w âˆ’v)âŠ¤x)2i
(10)
Denote u := w âˆ’v, and note that w Ì¸= v, hence âˆ¥uâˆ¥= 1. Denote by p(Ëœx) the pdf of ËœD, then we can
rewrite Eq. (10) as:
âˆ¥w âˆ’vâˆ¥2 Â·
Z
ËœxâˆˆRd 1(Ëœx âˆˆA1) Â· (ËœuâŠ¤Ëœx + bu)2p(Ëœx)dËœx
= âˆ¥w âˆ’vâˆ¥2 Â·
Z
ËœxâˆˆA1
(ËœuâŠ¤Ëœx + bu)2p(Ëœx)dËœx
(11)
Since the set A1 has a positive measure, and the set {Ëœx : ËœuâŠ¤Ëœx + bu = 0} is of zero measure, there is a
point Ëœx0 such that ËœuâŠ¤Ëœx + bu Ì¸= 0. By continuity, there is a small enough neighborhood A of Ëœx0, such that
ËœuâŠ¤Ëœx + bu Ì¸= 0 for every Ëœx âˆˆA. Using Assumption 4.1 we can lower bound Eq. (11) by:
âˆ¥w âˆ’vâˆ¥2 Â· Î²
Z
ËœxâˆˆA
(ËœuâŠ¤Ëœx + bu)2dËœx
where this integral is positive. This shows that âŸ¨âˆ‡F(w), wâˆ’vâŸ©> 0, which shows that âˆ‡F(w) Ì¸= 0, hence
w is not a critical point.
D
Proofs from Section 5
The following lemmas are required in order to prove Theorem 5.2. First, we show that if F(w) â‰¤F(0) âˆ’Î´
then we can lower bound âˆ¥wâˆ¥and Prx

wâŠ¤x â‰¥0, vâŠ¤x â‰¥0

.
Lemma D.1. Let Î´ > 0 and let w âˆˆRd+1 such that F(w) â‰¤F(0) âˆ’Î´. Then
âˆ¥wâˆ¥â‰¥Î´
c2 ,
and
Pr
x
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
â‰¥
Î´
c2âˆ¥wâˆ¥.
Proof. We have
F(0) âˆ’Î´ â‰¥F(w) = 1
2 E
x(Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x))2
= 1
2 E
x(Ïƒ(wâŠ¤x))2 + 1
2 E
x(Ïƒ(vâŠ¤x))2 âˆ’E
x(Ïƒ(wâŠ¤x)Ïƒ(vâŠ¤x))
â‰¥F(0) âˆ’E
x(Ïƒ(wâŠ¤x)Ïƒ(vâŠ¤x)) .
21

Hence
Î´ â‰¤E
x Ïƒ(wâŠ¤x)Ïƒ(vâŠ¤x) = E
x 1(wâŠ¤x â‰¥0, vâŠ¤x â‰¥0) Â· wâŠ¤x Â· vâŠ¤x
â‰¤âˆ¥wâˆ¥c2 Â· Pr
x
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
.
Thus,
âˆ¥wâˆ¥â‰¥
Î´
c2 Â· Prx [wâŠ¤x â‰¥0, vâŠ¤x â‰¥0] â‰¥Î´
c2 ,
and
Pr
x
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
â‰¥
Î´
c2âˆ¥wâˆ¥.
Using the above lemma, we now show that if F(w) â‰¤F(0) âˆ’Î´ then âˆ¥w âˆ’vâˆ¥decreases.
Lemma D.2. Let Î´ > 0 and let B > 1. Let w âˆˆRd+1 such that F(w) â‰¤F(0) âˆ’Î´ and âˆ¥w âˆ’vâˆ¥â‰¤B âˆ’1.
Let Î³ =
Î´3
3Â·122B3c8câ€²2 and let 0 < Î· â‰¤Î³
c4 . Let wâ€² = w âˆ’Î·âˆ‡F(w). Then,
âˆ¥wâ€² âˆ’vâˆ¥2 â‰¤âˆ¥w âˆ’vâˆ¥2 Â· (1 âˆ’Î³Î·) â‰¤(B âˆ’1)2 .
Proof. We have
âˆ¥wâ€² âˆ’vâˆ¥2 = âˆ¥w âˆ’Î·âˆ‡F(w) âˆ’vâˆ¥2
= âˆ¥w âˆ’vâˆ¥2 âˆ’2Î·âŸ¨âˆ‡F(w), w âˆ’vâŸ©+ Î·2âˆ¥âˆ‡F(w)âˆ¥2 .
(12)
We ï¬rst bound âˆ¥âˆ‡F(w)âˆ¥2. By Jensenâ€™s inequality and since Ïƒ is 1-Lipschitz, we have:
âˆ¥âˆ‡F(w)âˆ¥2 â‰¤E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)
2
Ïƒâ€²(wâŠ¤x)âˆ¥xâˆ¥2

â‰¤c2 E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)
2
â‰¤c2 E
x

wâŠ¤x âˆ’vâŠ¤x
2
= c2 E
x

(w âˆ’v)âŠ¤x
2
â‰¤c4âˆ¥w âˆ’vâˆ¥2 .
(13)
Next, we bound âŸ¨âˆ‡F(w), w âˆ’vâŸ©. Let u = w âˆ’v. We have
âŸ¨âˆ‡F(w), w âˆ’vâŸ©= E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

Ïƒâ€²(wâŠ¤x)(wâŠ¤x âˆ’vâŠ¤x)
= E
x

wâŠ¤x âˆ’vâŠ¤x
2
1(wâŠ¤x â‰¥0, vâŠ¤x â‰¥0)+
E
x wâŠ¤x Â· (wâŠ¤x âˆ’vâŠ¤x)1(wâŠ¤x â‰¥0, vâŠ¤x < 0)
â‰¥âˆ¥w âˆ’vâˆ¥2 Â· E
x 1(wâŠ¤x â‰¥0, vâŠ¤x â‰¥0)(uâŠ¤x)2 .
22

Let Î¾ =
Î´
12Bc3câ€² . The above is at least
âˆ¥w âˆ’vâˆ¥2 Â· Î¾2 Â· Pr
x
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0, (uâŠ¤x)2 â‰¥Î¾2i
= âˆ¥w âˆ’vâˆ¥2 Â· Î¾2 Â·

Pr
x
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0
i
âˆ’Pr
x
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0, (uâŠ¤x)2 < Î¾2i
.
By Lemma D.1, and since âˆ¥wâˆ¥â‰¤âˆ¥w âˆ’vâˆ¥+ âˆ¥vâˆ¥â‰¤B âˆ’1 + 1 = B, the above is at least
âˆ¥w âˆ’vâˆ¥2 Â· Î¾2 Â·

Î´
c2âˆ¥wâˆ¥âˆ’Pr
x
h
wâŠ¤x â‰¥0, vâŠ¤x â‰¥0, |uâŠ¤x| < Î¾
i
â‰¥âˆ¥w âˆ’vâˆ¥2 Â· Î¾2 Â·
 Î´
c2B âˆ’Pr
x
h
|uâŠ¤x| â‰¤Î¾
i
= âˆ¥w âˆ’vâˆ¥2 Â· Î¾2 Â·
 Î´
c2B âˆ’Pr
x
h
|ËœuâŠ¤Ëœx + bu| â‰¤Î¾
i
.
(14)
We now bound Prx

|ËœuâŠ¤Ëœx + bu| â‰¤Î¾

. We denote a = âˆ¥Ëœuâˆ¥. If a â‰¤
1
4c, then since âˆ¥uâˆ¥= 1 we have
bu â‰¥
q
1 âˆ’
1
16c2 â‰¥
q
1 âˆ’1
16 =
âˆš
15
4 . Hence, for every x with âˆ¥xâˆ¥â‰¤c we have
|ËœuâŠ¤Ëœx + bu| â‰¥|bu| âˆ’|ËœuâŠ¤Ëœx| â‰¥
âˆš
15
4
âˆ’ac â‰¥
âˆš
15
4
âˆ’1
4 > 1
2 .
Note that
Î¾ =
Î´
12Bc3câ€² â‰¤
F(0)
12Bc3câ€² =
1
12Bc3câ€² Â· 1
2 E
x(Ïƒ(vâŠ¤x))2 â‰¤
1
12Bc3câ€² Â· 1
2c2 =
1
24Bccâ€² â‰¤1
24 ,
where the last inequality is since B, c, câ€² â‰¥1. Therefore, |ËœuâŠ¤Ëœx + bu| > Î¾. Thus,
Pr
x
h
|ËœuâŠ¤Ëœx + bu| â‰¤Î¾
i
= 0 .
Assume now that a â‰¥1
4c. We have
Pr
x
h
|ËœuâŠ¤Ëœx + bu| â‰¤Î¾
i
= Pr
x
h
ËœuâŠ¤Ëœx âˆˆ[âˆ’Î¾ âˆ’bu, Î¾ âˆ’bu]
i
= Pr
x

Â¯ËœuâŠ¤Ëœx âˆˆ[âˆ’Î¾
a âˆ’bu
a , Î¾
a âˆ’bu
a ]

â‰¤câ€² Â· 2 Â· Î¾
a
â‰¤8ccâ€²Î¾ .
Combining the above with Eq. (14), we obtain
âŸ¨âˆ‡F(w), w âˆ’vâŸ©â‰¥âˆ¥w âˆ’vâˆ¥2 Â· Î¾2 Â·
 Î´
c2B âˆ’8ccâ€²Î¾

= âˆ¥w âˆ’vâˆ¥2
Î´2
122B2c6câ€²2 Â·
 Î´
c2B âˆ’8ccâ€² Â·
Î´
12Bc3câ€²

= âˆ¥w âˆ’vâˆ¥2
Î´2
122B2c6câ€²2 Â·

Î´
3c2B

= âˆ¥w âˆ’vâˆ¥2
Î´3
3 Â· 122B3c8câ€²2 .
(15)
23

Combining Eq. (12), (13) and (15), and using Î³ =
Î´3
3Â·122B3c8câ€²2 , we have
âˆ¥wâ€² âˆ’vâˆ¥2 â‰¤âˆ¥w âˆ’vâˆ¥2 âˆ’2Î·âˆ¥w âˆ’vâˆ¥2 Â· Î³ + Î·2c4âˆ¥w âˆ’vâˆ¥2
= âˆ¥w âˆ’vâˆ¥2 Â·
 1 âˆ’2Î·Î³ + Î·2c4
.
Since Î· â‰¤Î³
c4 , we obtain
âˆ¥wâ€² âˆ’vâˆ¥2 â‰¤âˆ¥w âˆ’vâˆ¥2 Â·

1 âˆ’2Î·Î³ + Î·c4 Â· Î³
c4

= âˆ¥w âˆ’vâˆ¥2 Â· (1 âˆ’Î³Î·) â‰¤âˆ¥w âˆ’vâˆ¥2 â‰¤(B âˆ’1)2 .
Next, we show that F(w) remains smaller than F(0) âˆ’Î´ during the training. In the following two
lemmas we obtain a bound for the smoothness of F in the relevant region, and in the two lemmas that follow
we use this bound to show that F(w) indeed remains small.
Lemma D.3. Let w âˆˆRd+1 such that F(w) â‰¤F(0). Then, âˆ¥âˆ‡F(w)âˆ¥â‰¤c
p
2F(0).
Proof. By Jensenâ€™s inequality, we have
âˆ¥âˆ‡F(w)âˆ¥2 â‰¤E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)
2
Ïƒâ€²(wâŠ¤x)âˆ¥xâˆ¥2
â‰¤c2 E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)
2
â‰¤c22F(w) â‰¤2c2F(0) .
Lemma D.4. Let M, B > 0 and let w, wâ€² âˆˆRd+1 be such that for every s âˆˆ[0, 1] we have M â‰¤
âˆ¥w + s(wâ€² âˆ’w)âˆ¥â‰¤B. Then,
âˆ¥âˆ‡F(w) âˆ’âˆ‡F(wâ€²)âˆ¥â‰¤âˆ¥w âˆ’wâ€²âˆ¥Â· c2

1 + 8(B + 1)câ€²c2
M

.
Proof. We assume w.l.o.g. that âˆ¥w âˆ’wâ€²âˆ¥â‰¤M
2c . Indeed, let 0 = s0 < . . . < sk = 1 for some integer k, let
wi = w + si(wâ€² âˆ’w), and assume that âˆ¥wi âˆ’wi+1âˆ¥â‰¤M
2c for every i. If the claim holds for every pair
wi, wi+1, then we have
âˆ¥âˆ‡F(w) âˆ’âˆ‡F(wâ€²)âˆ¥= âˆ¥
kâˆ’1
X
i=0
âˆ‡F(wi) âˆ’âˆ‡F(wi+1)âˆ¥
â‰¤
kâˆ’1
X
i=0
âˆ¥âˆ‡F(wi) âˆ’âˆ‡F(wi+1)âˆ¥
â‰¤
kâˆ’1
X
i=0
âˆ¥wi âˆ’wi+1âˆ¥Â· c2

1 + 8(B + 1)câ€²c2
M

= c2

1 + 8(B + 1)câ€²c2
M

âˆ¥w âˆ’wâ€²âˆ¥.
24

We have
âˆ¥âˆ‡F(w) âˆ’âˆ‡F(wâ€²)âˆ¥
= âˆ¥E
x(Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x))Ïƒâ€²(wâŠ¤x)x âˆ’(Ïƒ(wâ€²âŠ¤x) âˆ’Ïƒ(vâŠ¤x))Ïƒâ€²(wâ€²âŠ¤x)xâˆ¥
â‰¤âˆ¥E
x 1(wâŠ¤x â‰¥0, wâ€²âŠ¤x â‰¥0)

wâŠ¤x âˆ’Ïƒ(vâŠ¤x) âˆ’wâ€²âŠ¤x + Ïƒ(vâŠ¤x)

xâˆ¥+
âˆ¥E
x 1(wâŠ¤x â‰¥0, wâ€²âŠ¤x < 0)

wâŠ¤x âˆ’Ïƒ(vâŠ¤x)

xâˆ¥+
âˆ¥E
x 1(wâŠ¤x < 0, wâ€²âŠ¤x â‰¥0)

wâ€²âŠ¤x âˆ’Ïƒ(vâŠ¤x)

xâˆ¥.
By Jensenâ€™s inequality and Cauchy-Shwartz, the above is at most
E
x 1(wâŠ¤x â‰¥0, wâ€²âŠ¤x â‰¥0)âˆ¥w âˆ’wâ€²âˆ¥Â· âˆ¥xâˆ¥Â· âˆ¥xâˆ¥+
E
x 1(wâŠ¤x â‰¥0, wâ€²âŠ¤x < 0) (âˆ¥wâˆ¥Â· âˆ¥xâˆ¥+ âˆ¥vâˆ¥Â· âˆ¥xâˆ¥) Â· âˆ¥xâˆ¥+
E
x 1(wâŠ¤x < 0, wâ€²âŠ¤x â‰¥0)
 âˆ¥wâ€²âˆ¥Â· âˆ¥xâˆ¥+ âˆ¥vâˆ¥Â· âˆ¥xâˆ¥

Â· âˆ¥xâˆ¥.
By our assumption we have âˆ¥xâˆ¥â‰¤c and âˆ¥wâˆ¥, âˆ¥wâ€²âˆ¥â‰¤B. Hence, the above is at most
âˆ¥w âˆ’wâ€²âˆ¥c2 + Pr
x
h
wâŠ¤x â‰¥0, wâ€²âŠ¤x < 0
i
Â· c2 Â· (B + 1)
+ Pr
x
h
wâŠ¤x < 0, wâ€²âŠ¤x â‰¥0
i
Â· c2 Â· (B + 1) .
(16)
Now, we bound Prx

wâŠ¤x â‰¥0, wâ€²âŠ¤x < 0

. If wâŠ¤x â‰¥0 and wâ€²âŠ¤x < 0 then
wâŠ¤x = wâ€²âŠ¤x + (w âˆ’wâ€²)âŠ¤x < 0 + âˆ¥w âˆ’wâ€²âˆ¥Â· âˆ¥xâˆ¥â‰¤c Â· âˆ¥w âˆ’wâ€²âˆ¥.
Hence, we only need to bound
Pr
x
h
wâŠ¤x âˆˆ[0, c Â· âˆ¥w âˆ’wâ€²âˆ¥]
i
= Pr
x
h
ËœwâŠ¤Ëœx + bw âˆˆ[0, c Â· âˆ¥w âˆ’wâ€²âˆ¥]
i
.
We denote a = âˆ¥Ëœwâˆ¥. If a â‰¤M
4c , then since âˆ¥wâˆ¥â‰¥M we have |bw| â‰¥
q
M2 âˆ’
  M
4c
2 = M
p
1 âˆ’1/(16c2).
Hence for every x we have
| ËœwâŠ¤Ëœx + bw| â‰¥|bw| âˆ’| ËœwâŠ¤Ëœx| â‰¥|bw| âˆ’ac â‰¥M
p
1 âˆ’1/(16c2) âˆ’M
4 â‰¥M
p
1 âˆ’1/16 âˆ’M
4
= M Â·
âˆš
15 âˆ’1
4
> M/2 â‰¥câˆ¥w âˆ’wâ€²âˆ¥.
Thus, Prx
 ËœwâŠ¤Ëœx + bw âˆˆ[0, c Â· âˆ¥w âˆ’wâ€²âˆ¥]

= 0.
Assume now that a â‰¥M
4c . Hence, c
a â‰¤4c2
M . Therefore, we have
Pr
x
h
ËœwâŠ¤Ëœx + bw âˆˆ[0, c Â· âˆ¥w âˆ’wâ€²âˆ¥]
i
= Pr
x

Â¯ËœwâŠ¤Ëœx âˆˆ[âˆ’bw
a , âˆ’bw
a + c
a Â· âˆ¥w âˆ’wâ€²âˆ¥]

â‰¤Pr
x

Â¯ËœwâŠ¤Ëœx âˆˆ[âˆ’bw
a , âˆ’bw
a + 4c2
M Â· âˆ¥w âˆ’wâ€²âˆ¥]

â‰¤câ€² Â· 4c2
M Â· âˆ¥w âˆ’wâ€²âˆ¥.
25

Hence, Prx

wâŠ¤x â‰¥0, wâ€²âŠ¤x < 0

â‰¤câ€² Â· 4c2
M Â· âˆ¥w âˆ’wâ€²âˆ¥. By similar arguments, this inequality holds
also for Prx

wâŠ¤x < 0, wâ€²âŠ¤x â‰¥0

. Plugging it into Eq. (16), we have
âˆ¥âˆ‡F(w) âˆ’âˆ‡F(wâ€²)âˆ¥â‰¤âˆ¥w âˆ’wâ€²âˆ¥

c2 + 2 Â· c2 Â· (B + 1) Â· câ€² Â· 4c2
M

= âˆ¥w âˆ’wâ€²âˆ¥Â· c2

1 + 8(B + 1)câ€²c2
M

.
Lemma D.5. Let f : Rd â†’R and let L > 0. Let x, y âˆˆRd be such that for every s âˆˆ[0, 1] we have
âˆ¥âˆ‡f(x + s(y âˆ’x)) âˆ’âˆ‡f(x)âˆ¥â‰¤Lsâˆ¥y âˆ’xâˆ¥. Then,
f(y) âˆ’f(x) â‰¤âˆ‡f(x)âŠ¤(y âˆ’x) + L
2 âˆ¥y âˆ’xâˆ¥2 .
Proof. The proof follows a standard technique (cf. [2]). We represent f(y) âˆ’f(x) as an integral, apply
Cauchy-Schwarz and then use the L-smoothness.
f(y) âˆ’f(x) âˆ’âˆ‡f(x)âŠ¤(y âˆ’x) =
Z 1
0
âˆ‡f(x + s(y âˆ’x))âŠ¤(y âˆ’x)ds âˆ’âˆ‡f(x)âŠ¤(y âˆ’x)
â‰¤
Z 1
0
âˆ¥âˆ‡f(x + s(y âˆ’x)) âˆ’âˆ‡f(x)âˆ¥Â· âˆ¥y âˆ’xâˆ¥ds
â‰¤
Z 1
0
Lsâˆ¥y âˆ’xâˆ¥2ds
= L
2 âˆ¥y âˆ’xâˆ¥2 .
Hence, we have
f(y) âˆ’f(x) â‰¤âˆ‡f(x)âŠ¤(y âˆ’x) + L
2 âˆ¥y âˆ’xâˆ¥2 .
Lemma D.6. Let B, Î´ > 0 and let L = c2 
1 + 16(B+1)câ€²c4
Î´

. Let w âˆˆRd+1 such that F(w) â‰¤F(0) âˆ’Î´
and let wâ€² = w âˆ’Î· Â· âˆ‡F(w), where Î· â‰¤min

Î´
2c3âˆš
2F(0), 1
L

= min

Î´
2c3âˆš
2F(0),
Î´
Î´c2+16(B+1)câ€²c6

.
Assume that âˆ¥wâˆ¥, âˆ¥wâ€²âˆ¥â‰¤B. Then, we have F(wâ€²) âˆ’F(w) â‰¤âˆ’Î·
 1 âˆ’L
2 Î·

âˆ¥âˆ‡F(w)âˆ¥2, and F(wâ€²) â‰¤
F(w) â‰¤F(0) âˆ’Î´.
Proof. Let M =
Î´
2c2 . By Lemmas D.1 and D.3, we have âˆ¥wâˆ¥â‰¥Î´
c2 and âˆ¥âˆ‡F(w)âˆ¥â‰¤c
p
2F(0). Hence for
every Î» âˆˆ[0, 1] we have
âˆ¥w âˆ’Î»Î·âˆ‡F(w)âˆ¥â‰¥Î´
c2 âˆ’Î· Â· c
p
2F(0) â‰¥Î´
c2 âˆ’
Î´
2c3p
2F(0)
Â· c
p
2F(0) =
Î´
2c2 = M .
Since âˆ¥wâˆ¥, âˆ¥wâ€²âˆ¥â‰¤B, we also have âˆ¥w âˆ’Î»Î·âˆ‡F(w)âˆ¥â‰¤B. By Lemma D.4, we have for every Î» âˆˆ[0, 1]
that
âˆ¥âˆ‡F(w) âˆ’âˆ‡F(w âˆ’Î»Î·âˆ‡F(w))âˆ¥â‰¤Î»Î·âˆ¥âˆ‡F(w)âˆ¥Â· c2

1 + 8(B + 1)câ€²c2
M

.
26

We have L = c2 
1 + 16(B+1)câ€²c4
Î´

= c2 
1 + 8(B+1)câ€²c2
M

. By Lemma D.5 we have
F(w âˆ’Î·âˆ‡F(w)) âˆ’F(w) â‰¤âˆ’Î·âˆ¥âˆ‡F(w)âˆ¥2 + L
2 Î·2âˆ¥âˆ‡F(w)âˆ¥2 .
Since Î· â‰¤1
L, we also have F(w âˆ’Î·âˆ‡F(w)) â‰¤F(w) â‰¤F(0) âˆ’Î´.
We are now ready to prove the theorem:
Proof of Theorem 5.2. Let B = âˆ¥w0âˆ¥+ 2. Assume that Î· â‰¤min

Î´
2c3âˆš
2F(0),
Î´
Î´c2+16(B+1)câ€²c6 , Î³
c4

. We
have âˆ¥w0 âˆ’vâˆ¥â‰¤âˆ¥w0âˆ¥+ âˆ¥vâˆ¥= âˆ¥w0âˆ¥+ 1 â‰¤B âˆ’1. By Lemmas D.2 and D.6, for every t we have
âˆ¥wt âˆ’vâˆ¥â‰¤B âˆ’1 (thus, âˆ¥wtâˆ¥â‰¤B) and F(wt) â‰¤F(0)âˆ’Î´. Moreover, by Lemma D.2, we have for every
t that âˆ¥wt+1 âˆ’vâˆ¥2 â‰¤âˆ¥wt âˆ’vâˆ¥2 Â· (1 âˆ’Î³Î·). Therefore, âˆ¥wt âˆ’vâˆ¥2 â‰¤âˆ¥w0 âˆ’vâˆ¥2 (1 âˆ’Î³Î·)t.
It remains to show that
min
(
Î´
2c3p
2F(0)
,
Î´
Î´c2 + 16(B + 1)câ€²c6 , Î³
c4
)
= Î³
c4 .
Note that we have Î´ â‰¤F(0) = 1
2 Ex(Ïƒ(vâŠ¤x))2 â‰¤1
2 Â· c2. Thus
Î³
c4 =
Î´3
3 Â· 122B3c12câ€²2 â‰¤
Î´
3 Â· 122B3c12câ€²2 Â· c4
4 =
Î´
123B3c8câ€²2 .
We have
Î´
2c3p
2F(0)
â‰¥
Î´
2c4 â‰¥Î³
c4 ,
where the last inequality is since B, c, câ€² â‰¥1. Finally,
Î´
Î´c2 + 16(B + 1)câ€²c6 â‰¥
Î´
c4
2 + 16(B + 1)câ€²c6 â‰¥
Î´
17(B + 1)câ€²c6 â‰¥
Î´
34Bcâ€²c6 â‰¥Î³
c4 .
E
Proofs from Section 6
Before proving Theorem 6.2, we ï¬rst proof two auxiliary propositions which bounds certain areas for which
the vector w cannot reach during the optimization process. The ï¬rst proposition shows that if the norm of
Ëœw is small, and its bias is close to zero, then the bias must get larger. The second proposition shows that if
the norm of Ëœw is small, and the bias is negative, then the norm of Ëœw must get larger.
Proposition E.1. Assume that âˆ¥Ëœw âˆ’Ëœvâˆ¥2 â‰¤1, and that Assumption 6.1 holds. If âˆ¥Ëœwâˆ¥â‰¤0.4 and bw âˆˆ
h
0, Î±3Î²
640
i
then (âˆ‡F(w))d+1 â‰¤âˆ’Î±3Î²
640 .
27

Proof. The d + 1 coordinate of the distribution D is a constant 1. We denote by ËœD the ï¬rst d coordinates of
the distribution D. Hence, we can write:
(âˆ‡F(w))d+1 = Exâˆ¼D
h
(Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x))1(wâŠ¤x > 0)
i
= EËœxâˆ¼ËœD
h
(Ïƒ( ËœwâŠ¤Ëœx + bw) âˆ’Ïƒ(ËœvâŠ¤Ëœx + bv))1( ËœwâŠ¤Ëœx > âˆ’bw)
i
= EËœxâˆ¼ËœD
h
( ËœwâŠ¤Ëœx + bw) Â· 1( ËœwâŠ¤Ëœx > âˆ’bw)
i
âˆ’
âˆ’EËœxâˆ¼ËœD
h
(ËœvâŠ¤Ëœx + bv) Â· 1( ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx > âˆ’bv)
i
(17)
We will bound each term in Eq. (17) separately. Using the assumption that ËœD is spherically symmetric,
we can assume w.l.o.g that Ëœw = âˆ¥Ëœwâˆ¥e1, the ï¬rst unit vector. Hence we have that :
EËœxâˆ¼ËœD
h
( ËœwâŠ¤Ëœx + bw) Â· 1( ËœwâŠ¤Ëœx > âˆ’bw)
i
=EËœxâˆ¼ËœD

(âˆ¥Ëœwâˆ¥x1 + bw) Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

=âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

x11

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

+ bwEËœxâˆ¼ËœD

1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

(a)
â‰¤0.4EËœxâˆ¼ËœD

x11

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

+ bw
(b)
â‰¤0.4EËœxâˆ¼ËœD [x11(x1 > 0)] + bw .
(18)
Here, (a) is since âˆ¥Ëœwâˆ¥â‰¤0.4, and EËœxâˆ¼ËœD
h
1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥
i
â‰¤1, (b) is since bw â‰¥0, hence
EËœxâˆ¼ËœD

x11

0 > x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

â‰¤0 .
For the second term of Eq. (17), we assumed that âˆ¥Ëœwâˆ’Ëœvâˆ¥2 â‰¤1, which shows that Î¸( Ëœw, Ëœv) â‰¤Ï€
2 , and the
term is largest when this angle is largest. Hence, to lower bound this term we can assume that Î¸( Ëœw, Ëœv) = Ï€
2 ,
and since the distribution is spherically symmetric we can also assume w.l.o.g that Ëœv = e2, the second unit
vector. Now we can bound:
EËœxâˆ¼ËœD
h
(ËœvâŠ¤Ëœx + bv) Â· 1( ËœwâŠ¤Ëœx > âˆ’bw, ËœvâŠ¤Ëœx > âˆ’bv)
i
â‰¥EËœxâˆ¼ËœD

(x2 + bv) Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥, x2 > âˆ’bv

â‰¥1
2EËœxâˆ¼ËœD [(x2 + bv) Â· 1 (x2 > âˆ’bv)]
â‰¥1
2EËœxâˆ¼ËœD [x2 Â· 1 (x2 > 0)] + 1
2EËœxâˆ¼ËœD [(x2 + bv) Â· 1 (0 > x2 > âˆ’bv)]
â‰¥1
2EËœxâˆ¼ËœD [x2 Â· 1 (x2 > 0)] = 1
2EËœxâˆ¼ËœD [x1 Â· 1 (x1 > 0)] ,
(19)
where we used the assumption bv â‰¥0 and the symmetry of the distribution. Combining Eq. (18), Eq. (19)
with Eq. (17) we get:
28

(âˆ‡F(w))d+1 â‰¤bw âˆ’0.1EËœxâˆ¼ËœD [x1 Â· 1(x1 > 0)] .
Let Ë†D be the marginal distribution of ËœD on the plane spanned by e1 and e2, and denote by Ë†x the projection
of Ëœx on this plane. By Assumption 6.1(3) we have that the pdf of this distribution is at least Î² in a ball or
radius Î± around the origin. This way we can bound:
EËœxâˆ¼ËœD [x1 Â· 1(x1 > 0)] = EË†xâˆ¼Ë†D [x1 Â· 1(x1 > 0)]
â‰¥Î±Î²
2 P(Î±/2 < âˆ¥Ë†xâˆ¥< Î±, x1 > Î±/2)
â‰¥Î±Î²
2 P(x1 âˆˆ[Î±/2, 3Î±/4], x2 âˆˆ[âˆ’Î±/4, Î±/4]) = Î±3Î²
32 .
Combining the above, and using the assumption on bw we get that:
(âˆ‡F(w))d+1 â‰¤bw âˆ’Î±3Î²
320 â‰¤âˆ’Î±3Î²
640
Proposition E.2. Assume that âˆ¥Ëœw âˆ’Ëœvâˆ¥< 1, and Assumption 6.1 holds. Denote by Ï„ =
EËœxâˆ¼Ëœ
D[|x1x2|]
EËœxâˆ¼Ëœ
D[x2
1]
where ËœD is the projection of the distribution D on its ï¬rst d coordinates. If âˆ¥Ëœwâˆ¥â‰¤Ï„
2 and bw â‰¤0 then
âŸ¨âˆ‡F(w)1:d, ËœwâŸ©â‰¤0.
Proof. Denote by ËœD the projection of the distribution D on its ï¬rst d coordinates, we have that:
âŸ¨âˆ‡F(w)1:d, ËœwâŸ©=Exâˆ¼D
h
Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)

1(wâŠ¤x > 0) ËœwâŠ¤Ëœx
i
=EËœxâˆ¼ËœD
h
Ïƒ( ËœwâŠ¤Ëœx + bw) âˆ’Ïƒ(ËœvâŠ¤Ëœx + bv)

1( ËœwâŠ¤Ëœx > âˆ’bw) ËœwâŠ¤Ëœx
i
â‰¤EËœxâˆ¼ËœD
h
ËœwâŠ¤Ëœx + bw âˆ’Ïƒ(ËœvâŠ¤Ëœx)

Â· 1( ËœwâŠ¤Ëœx > âˆ’bw) ËœwâŠ¤Ëœx
i
.
(20)
The inequality above is since bv â‰¥0. Recall that our goal is to prove that the above term is negative,
hence we will divide it by âˆ¥Ëœwâˆ¥. Also, since the distribution ËœD is symmetric we can assume w.l.o.g that
Ëœw = âˆ¥Ëœwâˆ¥e1. Hence, it is enough to prove that the following term is non-positive:
âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

âˆ¥Ëœwâˆ¥x1 + bw âˆ’Ïƒ(ËœvâŠ¤Ëœx)

Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

x1

=âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD
 âˆ¥Ëœwâˆ¥x2
1 + bwx1

Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

âˆ’âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

x1ËœvâŠ¤Ëœx Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥, ËœvâŠ¤Ëœx > âˆ’bv

â‰¤âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD
 âˆ¥Ëœwâˆ¥x2
1 + bwx1

Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

âˆ’âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

x1ËœvâŠ¤Ëœx Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥, ËœvâŠ¤Ëœx > 0

.
(21)
We will ï¬rst bound the second term above. Since the term only depend on inner products between Ëœw, Ëœv
with Ëœx, we can consider the marginal distribution Ë†D, of ËœD on the plane spanned by Ëœw and Ëœv. Since ËœD is
symmetric we can assume w.l.o.g that Ë†D is spanned by the ï¬rst two coordinates x1 and x2. Let Ë†Ëœv be the
29

projection of Ëœv on this plane, then we can write Ë†Ëœv = (v1, v2) where v2
1 + v2
2 = 1. Note that since the
distribution Ë†D is symmetric, we have that E[x2
1] = E[x2
2]. By Cauchy-Schwarz we have:
|cov Ë†D(x1, x2)| â‰¤
q
var Ë†D(x1) Â· var Ë†D(x2) = var Ë†D(x1)
Again, by symmetry of Ë†D we have that E[x1] = E[x2]. Opening up the above terms we get that E[x1 Â· x2] â‰¤
E[x2
1]. Also, we assumed that âˆ¥Ëœw âˆ’Ëœvâˆ¥< 1, then Î¸(Ëœv, Ëœw) â‰¤Ï€
2 which means that v1 â‰¥0. Hence, the second
term of Eq. (21) is smallest when Ëœv = e2. In total, we can bound Eq. (21) by:
âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD
 âˆ¥Ëœwâˆ¥x2
1 + bwx1

Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

âˆ’âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

x1x2 Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥, x2 > 0

â‰¤âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

âˆ¥Ëœwâˆ¥x2
1 + bwx1 âˆ’1
2x1|x2|

Â· 1

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

=âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

âˆ¥Ëœwâˆ¥x1 + bw âˆ’1
2|x2|

Â· x11

x1 > âˆ’bw
âˆ¥Ëœwâˆ¥

(22)
By our assumption, bw â‰¤0. Both terms inside the expectation in Eq. (22) are largest when bw = 0.
Hence, we can bound Eq. (22) by:
âˆ¥Ëœwâˆ¥EËœxâˆ¼ËœD

âˆ¥Ëœwâˆ¥x1 âˆ’1
2|x2|

Â· x11 (x1 > 0)

=âˆ¥Ëœwâˆ¥2
2
EËœxâˆ¼ËœD

x2
1

âˆ’âˆ¥Ëœwâˆ¥
4 EËœxâˆ¼ËœD [|x1x2|]
â‰¤âˆ¥Ëœwâˆ¥2
2
EËœxâˆ¼ËœD

x2
1

âˆ’âˆ¥Ëœwâˆ¥Ï„
4
EËœxâˆ¼ËœD

x2
1

= c1
âˆ¥Ëœwâˆ¥2
2
âˆ’âˆ¥Ëœwâˆ¥Ï„
4

.
(23)
In particular, for âˆ¥Ëœwâˆ¥â‰¤Ï„
2, Eq. (23) non-positive.
We are now ready to prove the main theorem:
Proof of Theorem 6.2. Denote bt = max{0, âˆ’bwt
âˆ¥Ëœwtâˆ¥}. We will show by induction on the iterations of gradi-
ent descent that throughout the optimization process bt < 2.4 Â· max
n
1,
1
âˆšÏ„
o
and Î¸( Ëœwt, Ëœv) â‰¤Ï€
2 for every
t â‰¥0.
By the assumption on the initialization we have that âˆ¥Ëœw0 âˆ’Ëœvâˆ¥2 â‰¤âˆ¥w0 âˆ’vâˆ¥2 < 1, and also âˆ¥Ëœvâˆ¥= 1,
hence Î¸( Ëœw0, Ëœv) â‰¤Ï€
2 . We also have that bw0 â‰¥0, hence b0 = 0 this proves the case of t = 0. Assume this
is true for t. We will bound the norm of the gradient of the objective using Jensenâ€™s inequality:
âˆ¥âˆ‡F(w)âˆ¥2 â‰¤Exâˆ¼D
h
(Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x))21(wâŠ¤x > 0)xâŠ¤x
i
â‰¤Exâˆ¼D
h
(wâŠ¤x âˆ’vâŠ¤x)2xâŠ¤x
i
â‰¤âˆ¥w âˆ’vâˆ¥2Exâˆ¼D

âˆ¥xâˆ¥4
= âˆ¥w âˆ’vâˆ¥2c .
(24)
For the (t + 1)-th iteration of gradient descent we have that:
âˆ¥wt+1 âˆ’vâˆ¥2 =âˆ¥wt âˆ’Î·âˆ‡F(wt) âˆ’vâˆ¥2
=âˆ¥wt âˆ’vâˆ¥2 âˆ’2Î·âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©+ Î·2âˆ¥âˆ‡F(wt)âˆ¥2
â‰¤âˆ¥wt âˆ’vâˆ¥2 âˆ’2Î·âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©+ Î·2câˆ¥wt âˆ’vâˆ¥2 .
(25)
30

By Theorem A.2, and the induction assumption on Î¸( Ëœwt, Ëœv) we get that there is a universal constant c0,
such that âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©â‰¥c0Î²(Î±âˆ’
âˆš
2bt)
Î±2
âˆ¥wt âˆ’vâˆ¥2. Using the induction assumption that bt < 2.4 Â·
max
n
1,
1
âˆšÏ„
o
and Assumption 6.1(3) we can bound (Î± âˆ’
âˆš
2bt) â‰¥0.1. In total we get that âŸ¨âˆ‡F(wt), wt âˆ’
vâŸ©â‰¥
c0Î²
10Î±2 âˆ¥wt âˆ’vâˆ¥2. By taking Î· â‰¤
c0Î²
10cÎ±2 and combining with Eq. (25) we have that:
âˆ¥wt+1 âˆ’vâˆ¥2 < âˆ¥wt âˆ’vâˆ¥2 .
In particular, âˆ¥Ëœwt+1 âˆ’Ëœvâˆ¥2 â‰¤âˆ¥wt+1 âˆ’vâˆ¥2 < âˆ¥wt âˆ’vâˆ¥2 < 1, which shows that Î¸( Ëœwt+1, Ëœv) â‰¤Ï€
2 , and
concludes the ï¬rst part of the induction.
The bound for bt is more intricate, for an illustration see Figure 2. Let tâ€² be the ï¬rst iteration for which
âˆ¥Ëœwtâ€²âˆ¥â‰¥0.4. First assume that t â‰¤tâ€², we will show that in this case bt = 0. Assume otherwise, and let t0
be the ï¬rst iteration for which bt0 > 0, this means that bwt0 < 0 and bwt0âˆ’1 â‰¥0. We have that:
bwt0 = bwt0âˆ’1 âˆ’Î·âˆ‡F(wt0)d+1 .
If bwt0âˆ’1 â‰¤
Î±3Î²
640 , then by Proposition E.1 the last coordinate of the gradient is negative, hence bwt0 >
bwt0âˆ’1 â‰¥0. Otherwise, assume that bwt0âˆ’1 > Î±3Î²
640 . By Eq. (24): |âˆ‡F(wt0)d+1| â‰¤âˆ¥âˆ‡F(w)âˆ¥â‰¤âˆšc.
Hence, by taking Î· <
Î²
640âˆšc â‰¤
Î±3Î²
640âˆšc, we get that bwt0 â‰¥0, which is a contradiction (note that by
Assumption 6.1(3), we have Î± â‰¥1). We proved that if t â‰¤tâ€² then bwt â‰¥0, which means that bt = 0.
Assume now that t > tâ€². We will need the following calculation: Assume that âˆ¥Ëœwtâˆ¥= Î´, Then
âˆ¥Ëœwt âˆ’Ëœvâˆ¥2 â‰¥(1 âˆ’Î´)2, and the minimum is achieved at Ëœw = Î´Ëœv. Since we have:
âˆ¥Ëœwt âˆ’Ëœvâˆ¥2 + (bwt âˆ’bv)2 = âˆ¥wt âˆ’vâˆ¥2 â‰¤1 ,
we get that (bwtâˆ’bv)2 â‰¤1âˆ’(1âˆ’Î´)2 â‰¤2Î´. If we further assume that bwt â‰¤0, then b2
wt â‰¤(bwtâˆ’bv)2 â‰¤2Î´.
Combining all the above, we get that if âˆ¥Ëœwtâˆ¥= Î´ then:
bt = max

0, âˆ’bwt
âˆ¥Ëœwtâˆ¥

â‰¤
r
2
Î´ .
(26)
To show the bound on bt we split into cases, depending on the norm of Ëœwt:
Case I: 2Ï„
5 < âˆ¥Ëœwtâˆ¥â‰¤Ï„
2 and bwt â‰¤0. In this case we have:
âˆ¥Ëœwt+1âˆ¥2 = âˆ¥Ëœwt âˆ’Î·âˆ‡F(wt)1:dâˆ¥2
= âˆ¥Ëœwtâˆ¥2 âˆ’2Î·âŸ¨Ëœwt, âˆ‡F(wt)1:dâŸ©+ Î·2âˆ¥âˆ‡F(wt)1:dâˆ¥2
â‰¥âˆ¥Ëœwtâˆ¥2 âˆ’2Î·âŸ¨Ëœwt, âˆ‡F(wt)1:dâŸ©.
We can use Proposition E.2 to get that âŸ¨Ëœwt, âˆ‡F(wt)1:dâŸ©â‰¤0, hence âˆ¥Ëœwt+1âˆ¥2 â‰¥âˆ¥Ëœwtâˆ¥2. By Eq. (26) we
get that bt+1 â‰¤
q
5
Ï„ â‰¤2.4
âˆšÏ„ .
Case II: âˆ¥Ëœwtâˆ¥â‰¥min

0.4, Ï„
2
	
. In this case, by choosing a step size Î· <
1
40c min{1, Ï„}we can bound
âˆ¥Ëœwt+1âˆ¥â‰¥âˆ¥Ëœwtâˆ¥2 âˆ’2Î·âŸ¨Ëœwt, âˆ‡F(wt)1:dâŸ©
â‰¥âˆ¥Ëœwtâˆ¥2 âˆ’2Î·âˆ¥Ëœwtâˆ¥âˆ¥âˆ‡F(wt)1:dâˆ¥
â‰¥âˆ¥Ëœwtâˆ¥2 âˆ’2Î·âˆ¥Ëœwtâˆ¥âˆ¥âˆ‡F(wt)âˆ¥
â‰¥âˆ¥Ëœwtâˆ¥2 âˆ’2Î· Â· 2c â‰¥min

0.39, 2Ï„
5

.
31

Figure 2: A 2-d illustration of the optimization landscape. The x axis represents âˆ¥Ëœwâˆ¥, and the y-axis
represents bw. In the ï¬gure, for simplicity, we assume that bv = 0, and Ï„ = 0.1 which means that 2Ï„
5 = 0.4.
The red circle represents the area with âˆ¥w âˆ’vâˆ¥â‰¤1, throughout the optimization process wt stays in this
circle. The black region represents the area where bt = âˆ’bw
âˆ¥Ëœwâˆ¥can be potentially large, our goal is to show
that wt stays out of this region. Case I shows that wt cannot cross the blue region. Case II shows that if wt
is to the right of the black region, then bt is upper bounded. Case III shows that wt cannot cross the orange
region (sub-cases (a) and (b)), and cannot cross from the green region directly to the black region (sub-case
(c)).
Again, by Eq. (26) we get that bt+1 â‰¤max
nâˆš
5.2, 2.4
âˆšÏ„
o
â‰¤2.4Â·max
n
1,
1
âˆšÏ„
o
. This concludes the induction.
Case III: âˆ¥Ëœwtâˆ¥â‰¤min

0.4, 2Ï„
5
	
. We split into sub-cases depending on the previous iteration: (a) If
bwtâˆ’1 â‰¤0, then by Case I the norm of Ëœw cannot get below 2Ï„
5 , hence this sub-case is not possible; (b) If
bwtâˆ’1 â‰¥0 and âˆ¥Ëœwtâˆ’1âˆ¥â‰¤min

0.4, 2Ï„
5
	
, then by the same reasoning in the case of t < tâ€², bwt cannot get
smaller than zero. Hence, we must have that bwt+1 â‰¥0; (c) If bwtâˆ’1 â‰¥0 and âˆ¥Ëœwtâˆ’1âˆ¥â‰¥min

0.4, 2Ï„
5
	
then the bound depend on whether âˆ¥Ëœwtâˆ’1âˆ¥is larger than 0.4 or not. If âˆ¥
Ëœ
wtâˆ’1âˆ¥â‰¤0.4, then using the
same reasoning as the case of tâ€² < t twice (both for the t âˆ’1 and t iterations) we get that bt+1 â‰¥0. If
âˆ¥Ëœwtâˆ’1âˆ¥> 0.4 and bwt â‰¥0, then again this is the same case as in the case of tâ€² < t (since âˆ¥Ëœwtâˆ¥â‰¤0.4. The
last case is when âˆ¥Ëœwtâˆ’1âˆ¥> 0.4 and bwt < 0, here using the same calculation as in Case II, we have that
âˆ¥Ëœwtâˆ¥â‰¥0.39. Since âˆ¥Ëœwtâˆ¥â‰¤min

0.4, 2Ï„
5
	
, using Proposition E.2, the norm of Ëœwt can only grow, hence
by the same reasoning as in Case I we can also bound bt+1 < 2.4 max
n
1,
1
âˆšÏ„
o
.
Until now we have proven that throughout the entire optimization process we have that Î¸( Ëœwt, Ëœv) â‰¤Ï€
2
32

and bt â‰¤2.4 Â· max
n
1,
1
âˆšÏ„
o
. Let Î´ = Ï€ âˆ’Î¸( Ëœwt, Ëœv), we now use Theorem A.2 and Eq. (24) to get that:
âˆ¥wt+1 âˆ’vâˆ¥2 =âˆ¥wt âˆ’Î·âˆ‡F(wt) âˆ’vâˆ¥2
=âˆ¥wt âˆ’vâˆ¥2 âˆ’2Î·âŸ¨âˆ‡F(wt), wt âˆ’vâŸ©+ Î·2âˆ¥âˆ‡F(wt)âˆ¥2
â‰¤âˆ¥wt âˆ’vâˆ¥2 âˆ’2Î·

Î± âˆ’
bt
sin( Î´
2)
4
Î²
84Î±2
sin
Î´
4
3
âˆ¥wt âˆ’vâˆ¥2 + Î·2câˆ¥wt âˆ’vâˆ¥2
â‰¤âˆ¥wt âˆ’vâˆ¥2 âˆ’Î·
 Î± âˆ’
âˆš
2bt
4 Î²
84Î±2
sin
Î´
4
3
âˆ¥wt âˆ’vâˆ¥2 + Î·2câˆ¥wt âˆ’vâˆ¥2
â‰¤âˆ¥wt âˆ’vâˆ¥2 âˆ’Î· ËœCÎ²
Î±2 âˆ¥wt âˆ’vâˆ¥2 + Î·2câˆ¥wt âˆ’vâˆ¥2
(27)
where ËœC is some universal constant, and we used the bounds from the induction above that Î´ âˆˆ
Ï€
2 , Ï€

,
bt â‰¤2.4 Â· max
n
1,
1
âˆšÏ„
o
, and by the assumption that Î± â‰¥2.5
âˆš
2 max
n
1,
1
âˆšÏ„
o
. By choosing Î· â‰¤
ËœCÎ²
2cÎ±2 , and
setting Î» =
ËœCÎ²
2cÎ±2 we get that:
âˆ¥wt âˆ’vâˆ¥2 âˆ’Î· ËœCÎ² min

1, 1
Î±2

âˆ¥wt âˆ’vâˆ¥2 + Î·2câˆ¥wt âˆ’vâˆ¥2
â‰¤(1 âˆ’Î»Î·)âˆ¥wt âˆ’vâˆ¥2 â‰¤Â· Â· Â· â‰¤(1 âˆ’Î·Î»)tâˆ¥w0 âˆ’vâˆ¥2 ,
which ï¬nished the proof.
F
Proofs from Subsection 5.2
F.1
Proof of Theorem 5.4
We have
F(w) = 1
2 E
x

Ïƒ(wâŠ¤x) âˆ’Ïƒ(vâŠ¤x)
2
= F(0) + 1
2 E
x

Ïƒ(wâŠ¤x)
2
âˆ’E
x Ïƒ(wâŠ¤x)Ïƒ(vâŠ¤x)
â‰¤F(0) + âˆ¥wâˆ¥2c2
2
âˆ’âˆ¥wâˆ¥E
x Ïƒ( Â¯wâŠ¤x)Ïƒ(vâŠ¤x) .
(28)
Let Î¾ =
Î±
4âˆšc sin
  Ï€
8

. We have
E
x Ïƒ( Â¯wâŠ¤x)Ïƒ(vâŠ¤x) â‰¥Î¾2 Â· Pr
x
h
Ïƒ( Â¯wâŠ¤x)Ïƒ(vâŠ¤x) â‰¥Î¾2i
â‰¥Î¾2 Â· Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc

.
(29)
In the following two lemmas we bound Prx
h
Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc
i
.
33

Lemma F.1. If bv â‰¥0 then
Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc

â‰¥Î²
 Î± sin
  Ï€
8

âˆ’2âˆšcÎ¾
2
4 sin
  Ï€
8

.
Proof. If âˆ¥Ëœvâˆ¥â‰¥1
4c, then we have
Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc

â‰¥Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, ËœvâŠ¤Ëœx â‰¥
Î¾
2âˆšc

= Pr
x

ËœÂ¯wâŠ¤Ëœx â‰¥2âˆšcÎ¾, Â¯ËœvâŠ¤Ëœx â‰¥
Î¾
2âˆšcâˆ¥Ëœvâˆ¥

â‰¥Pr
x
h
ËœÂ¯wâŠ¤Ëœx â‰¥2âˆšcÎ¾, Â¯ËœvâŠ¤Ëœx â‰¥2âˆšcÎ¾
i
â‰¥Î²
 Î± sin
  Ï€
8

âˆ’2âˆšcÎ¾
2
4 sin
  Ï€
8

,
where the last inequality is due to Lemma A.1, since Î¸( Ëœw, Ëœv) â‰¤3Ï€
4 .
If âˆ¥Ëœvâˆ¥â‰¤1
4c, then
bv â‰¥
r
1 âˆ’
1
16c2 â‰¥
r
1 âˆ’1
16 =
âˆš
15
4
> 3
4 ,
and hence
vâŠ¤x = ËœvâŠ¤Ëœx + bv > âˆ’1
4c Â· c + 3
4 = 1
2 â‰¥Î¾ â‰¥
Î¾
2âˆšc .
Therefore,
Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc

= Pr
x
h
Â¯wâŠ¤x â‰¥2âˆšcÎ¾
i
= Pr
x
h
ËœÂ¯wâŠ¤Ëœx â‰¥2âˆšcÎ¾
i
.
For Ëœu âˆˆRd such that âˆ¥Ëœuâˆ¥= 1 and Î¸( Ëœw, Ëœu) = 3Ï€
4 , Lemma A.1 implies that the above is at least
Pr
x
h
ËœÂ¯wâŠ¤Ëœx â‰¥2âˆšcÎ¾, ËœuâŠ¤Ëœx â‰¥2âˆšcÎ¾
i
â‰¥Î²
 Î± sin
  Ï€
8

âˆ’2âˆšcÎ¾
2
4 sin
  Ï€
8

.
Lemma F.2. If bv < 0 and âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î± Â·
sin( Ï€
8 )
4
, then
Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc

â‰¥Î²
 Î± sin
  Ï€
8

âˆ’2âˆšcÎ¾
2
4 sin
  Ï€
8

.
Proof.
Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc

= Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, ËœvâŠ¤Ëœx â‰¥
Î¾
2âˆšc âˆ’bv

= Pr
x

ËœÂ¯wâŠ¤Ëœx â‰¥2âˆšcÎ¾, Â¯ËœvâŠ¤Ëœx â‰¥
Î¾
2âˆšcâˆ¥Ëœvâˆ¥âˆ’bv
âˆ¥Ëœvâˆ¥

.
(30)
34

Moreover, we have
 
Î± Â· sin
  Ï€
8

4
!2
â‰¥
 bv
âˆ¥Ëœvâˆ¥
2
= 1 âˆ’âˆ¥Ëœvâˆ¥2
âˆ¥Ëœvâˆ¥2
=
1
âˆ¥Ëœvâˆ¥2 âˆ’1 ,
and hence
âˆ¥Ëœvâˆ¥2 â‰¥
16
Î±2 sin2   Ï€
8

+ 16 â‰¥
16
 Î± sin
  Ï€
8

+ 4
2 â‰¥
16
(c Â· 1 + 4c)2 ,
where in the last inequality we used c â‰¥Î± and c â‰¥1. Thus,
âˆ¥Ëœvâˆ¥â‰¥4
5c â‰¥1
2c .
Combining the above with Eq. (30), and using âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î± Â·
sin( Ï€
8 )
4
, we have
Pr
x

Â¯wâŠ¤x â‰¥2âˆšcÎ¾, vâŠ¤x â‰¥
Î¾
2âˆšc

â‰¥Pr
x
"
ËœÂ¯wâŠ¤Ëœx â‰¥2âˆšcÎ¾, Â¯ËœvâŠ¤Ëœx â‰¥âˆšcÎ¾ + Î± Â· sin
  Ï€
8

4
#
= Pr
x
h
ËœÂ¯wâŠ¤Ëœx â‰¥2âˆšcÎ¾, Â¯ËœvâŠ¤Ëœx â‰¥2âˆšcÎ¾
i
â‰¥Î²
 Î± sin
  Ï€
8

âˆ’2âˆšcÎ¾
2
4 sin
  Ï€
8

,
where the last inequality is due to Lemma A.1, since Î¸( Ëœw, Ëœv) â‰¤3Ï€
4 .
Combining Eq. (29) with Lemmas F.1 and F.2, we have
E
x Ïƒ( Â¯wâŠ¤x)Ïƒ(vâŠ¤x) â‰¥Î¾2 Â· Î²
 Î± sin
  Ï€
8

âˆ’2âˆšcÎ¾
2
4 sin
  Ï€
8

= Î±2 sin2   Ï€
8

16c
Â· Î²
  Î±
2 sin
  Ï€
8
2
4 sin
  Ï€
8

= Î±4Î² sin3   Ï€
8

256c
= M .
Plugging the above into Eq. (28) we have
F(w) â‰¤F(0) + âˆ¥wâˆ¥2c2
2
âˆ’âˆ¥wâˆ¥Â· M .
The above expression is smaller than F(0) if âˆ¥wâˆ¥< 2M
c2 .
G
Discussion on the Assumption on bv
In Corollary 5.5 we had an assumption that âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î± Â·
sin( Ï€
8 )
4
. This implies that either the bias term bv is
positive, or it is negative but not too large. Here we discuss why this assumption is crucial for the proof of
the theorem, and what can we still say when this assumption does not hold.
35

In Theorem 3.2 we showed an example with bv < 0 where gradient descent with random initialization
does not converge w.h.p. to a global minimum even asymptotically3. In the example from Theorem 3.2
we have âˆ’bv
âˆ¥Ëœvâˆ¥= r
 1 âˆ’
1
2d2

, and the input distribution is uniform over a ball of radius r. In this case, we
must choose Î± from Assumption 5.3 to be smaller than r (otherwise Î² = 0) and hence âˆ’bv
âˆ¥Ëœvâˆ¥> Î±
 1 âˆ’
1
2d2

.
Therefore it does not satisfy the assumption âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î±Â·
sin( Ï€
8 )
4
(already for d > 1). If we choose, e.g., Î± = r
2,
then the example from Theorem 3.2 satisï¬es âˆ’bv
âˆ¥Ëœvâˆ¥= Î±
 2 âˆ’1
d2

â‰¤2Î±. It implies that our assumption on
âˆ’bv
âˆ¥Ëœvâˆ¥is tight up to a constant factor, and is also crucial for the proof, since already for âˆ’bv
âˆ¥Ëœvâˆ¥= 2Î± we have
an example of convergence to a non-global minimum.
On the other hand, if âˆ’bv
âˆ¥Ëœvâˆ¥> Î± Â·
sin( Ï€
8 )
4
(i.e. the assumption does not hold) we can calculate the loss at
zero:
F(0) = 1
2 Â· E
x

Ïƒ(vâŠ¤x)
2
= 1
2 Â· E
x

1(ËœvâŠ¤Ëœx + bv â‰¥0)

ËœvâŠ¤Ëœx + bv
2
= 1
2 Â· E
x
"
1

Â¯ËœvâŠ¤Ëœx â‰¥âˆ’bv
âˆ¥Ëœvâˆ¥

âˆ¥Ëœvâˆ¥2

Â¯ËœvâŠ¤Ëœx + bv
âˆ¥Ëœvâˆ¥
2#
â‰¤âˆ¥Ëœvâˆ¥2
2
Â· E
x
"
1
 
Â¯ËœvâŠ¤Ëœx â‰¥Î± Â· sin
  Ï€
8

4
! 
Â¯ËœvâŠ¤Ëœx
2
#
.
Let Ïµ > 0 be a small constant. Suppose that the distribution ËœD is spherically symmetric, and that Î± is large,
such that the above expectation is smaller than Ïµ. For such Î±, we either have âˆ’bv
âˆ¥Ëœvâˆ¥â‰¤Î± Â·
sin( Ï€
8 )
4
, in which
case gradient descent converges w.h.p. to the global minimum, or âˆ’bv
âˆ¥Ëœvâˆ¥> Î± Â·
sin( Ï€
8 )
4
, in which case the
loss at w = 0 is already almost as good as the global minimum. For standard Gaussian distribution, we can
choose Î± to be a large enough constant that depends only on Ïµ (independent of the input dimension), hence
Î² will also be independent of d. This means that for standard Gaussian distribution, for every constant Ïµ > 0
we can ensure either convergence to a global minimum, or the loss at 0 is already Ïµ-optimal.
Note that in Remark 3.3 we have shown another distribution which is non-symmetric and depends on
the target v, such that the loss F(0) is highly sub-optimal, but gradient ï¬‚ow converges to such a point with
probability close to 1
2.
3In Theorem 3.2 we have âˆ¥vâˆ¥Ì¸= 1, but it still holds if we normalize v, namely, replace v with
v
âˆ¥vâˆ¥.
36

