Lynx: An Open Source Hallucination Evaluation Model
Selvan Sunitha Ravi1, Bartosz Mielczarek1, Anand Kannappan1, Douwe Kiela2,3, Rebecca Qian1
1 Patronus AI 2 Contextual AI 3 Stanford University
Abstract
Retrieval Augmented Generation (RAG) tech-
niques aim to mitigate hallucinations in Large
Language Models (LLMs). However, LLMs
can still produce information that is unsup-
ported or contradictory to the retrieved con-
texts. We introduce LYNX, a SOTA halluci-
nation detection LLM that is capable of ad-
vanced reasoning on challenging real-world
hallucination scenarios.
To evaluate LYNX,
we present HaluBench, a comprehensive hallu-
cination evaluation benchmark, consisting of
15k samples sourced from various real-world
domains. Our experiment results show that
LYNX outperforms GPT-4o, Claude-3-Sonnet
and closed and open-source LLM-as-a-judge
models on HaluBench.
We release LYNX,
HaluBench and our evaluation code for pub-
lic access.
1
Introduction
Large Language Models (LLMs) learn in-depth
knowledge from their pre-training data (Petroni
et al., 2019) making them useful for knowledge-
intensive downstream tasks such as Question An-
swering. However, their knowledge cannot be eas-
ily expanded and they often struggle with “hallu-
cinations” (Roller et al., 2020; Dziri et al., 2022;
Cao et al., 2022). This has led to the adaptation
of Retrieval Augmented Generation (RAG) (Lewis
et al., 2020) systems, which enables flexibility and
extensibility of LLMs to internal data stores. How-
ever, these systems are still prone to generating text
that is inconsistent with the provided knowledge
source (Mallen et al., 2022).
In an ideal RAG system, LLMs exhibit “faith-
fulness” by producing outputs that are grounded
in the retrieved contexts. Detecting whether gen-
erated answers are faithful to the provided context
is therefore critical to the success of RAG systems
in production. RAGAS (Es et al., 2023) use LLMs
to generate statements from a question-answer pair
and compute a faithfulness score based on how
many statements are supported by the given con-
text. Other methods involve using LLM-as-a-Judge
(Zheng et al., 2023) or fine-tuning lightweight LLM
judges (Saad-Falcon et al., 2024) to evaluate hallu-
cinations.
While LLMs as judges have shown promise in
automated evaluation on certain tasks (Zheng et al.,
2024; Zhu et al., 2023; Kim et al., 2023), hallu-
cination detection presents a complex challenge
as it requires language models to have ability to
perform nuanced reasoning and disambiguation.
Figure 1 illustrates such an example where vari-
ous language models evaluate whether a Context-
Question-Answer triplet contains hallucinations.
GPT-4o and Claude-3-Sonnet both fail to identify
that the answer, though it makes a correct statement,
is not properly contextualized by the document and
question.
Additionally, closed source LLMs as judges lack
transparency and accessibility. Open-source LLMs
still exhibit a significant gap in performance com-
pared to closed source alternatives (Li et al., 2023).
The gap in baseline performance between closed
and open-source models increases when applied to
specialized domains such as finance and medicine
(Islam et al., 2023; Wu et al., 2023a).
To address these issues, we propose LYNX (70B)
that outperforms GPT-4o and closed source LLMs
in hallucination detection tasks, while being fully
reproducible and open source. LYNX (8B) pro-
duces high quality evaluations at a fraction of the
size and cost of closed source LLMs. LYNX is the
first open source hallucination detection model that
outperforms GPT-4o and closed source LLMs-as-
Judge.
To train LYNX, we finetune Llama-3-70B-
Instruct on data from multiple domains, focusing
on hard-to-detect hallucinations. We source exam-
ples from existing Question Answer (QA) datasets
such as CovidQA (Möller et al., 2020), PubmedQA
arXiv:2407.08488v1  [cs.AI]  11 Jul 2024

(Jin et al., 2019), DROP (Dua et al., 2019) and
FinanceBench (Islam et al., 2023), introducing per-
turbations to generate hallucinated answers that
appear plausible but are not faithful to the context.
Evaluating the performance of different LLMs as
judges in real-world hallucination detection tasks
is difficult due to the lack of a comprehensive and
diverse benchmark. Halueval (Li et al., 2023) and
RAGTruth (Wu et al., 2023b) provide a large col-
lection of generated and human-annotated halluci-
nated samples but cover limited domains. To evalu-
ate hallucination detection systems, we construct
HaluBench, a large scale hallucination evaluation
benchmark that consists of 15k hallucinated as well
as faithful responses to questions across multiple
real-world domains.
Our contributions are as follows:
• We present HaluBench, a hallucination eval-
uation benchmark of 15k samples that con-
sists of Context-Question-Answer triplets an-
notated for whether the examples contain
hallucinations. Compared to prior datasets,
HaluBench is the first open-source benchmark
containing hallucination tasks sourced from
real-world domains that include finance and
medicine.
• We train LYNX, the first open-source LLM ca-
pable of high quality, reference-free hallucina-
tion detection in RAG settings. We show that
LYNX outperforms GPT-4o, Claude-3-Sonnet
and other closed and open-source models on
HaluBench.
• We propose a novel method to generate hard-
to-detect hallucination examples from Ques-
tion Answering tasks by applying semantic
perturbations to LLM responses. We find that
our perturbed examples are challenging for
LLM judges, as nuanced differences in seman-
tic meaning can lead to different reasoning
outcomes.
• We conduct experiments to benchmark
LYNX against closed and open-source LLMs
and RAG evaluation metrics. We release all
models, datasets and experiment results for
public access.
LYNX1and HaluBench2 are available on Hug-
gingFace. We are also releasing the training data,
1HuggingFace Model: https://huggingface.co/PatronusAI/Llama-
3-Lynx-70B-Instruct
2HaluBench: https://huggingface.co/datasets/PatronusAI/HaluBench
code and model generations on Github3. A visu-
alization of HaluBench is publicly available on
Nomic Atlas 4
2
Related Work
While LLMs have shown remarkable performance
in knowledge-intensive tasks such as Question An-
swering, one of the major drawbacks is genera-
tion of inaccurate or false information (Azaria and
Mitchell, 2023; Ji et al., 2023). Several techniques
have been proposed to evaluate hallucinations
in LLMs (Guerreiro et al., 2022; Lin and Chen,
2023; Manakul et al., 2023) including Retrieval-
Augmented Generation (RAG) (Lewis et al., 2020;
Shuster et al., 2021a; Yu, 2022; Biswas et al., 2022).
By leveraging retrieval, RAG helps LLMs acquire
domain-specific knowledge and ground their out-
puts in factual information (Shuster et al., 2021b).
However, RAG systems can still suffer from hal-
lucinations (Wu et al., 2023b; Li et al., 2023; Es
et al., 2023; Saad-Falcon et al., 2024). Automatic
evaluation is crucial for quickly deploying these
systems in new environments where creating a tra-
ditional benchmark dataset from the ground up is
challenging.
To evaluate RAG systems, LLMs have been uti-
lized to compute metrics such as answer correct-
ness, groundedness and the relevance of retrieved
contexts (Zhao et al., 2019; Yuan et al., 2021; Liu
et al., 2023; Es et al., 2023; Saad-Falcon et al.,
2024). RAGAS (Es et al., 2023) relies on a set of
heuristic, hand-written prompts, while using LLMs
and embedding-based similarity scores. Similarly,
the EXAM (Sander and Dietz, 2021) metric eval-
uates retrieval-augmented generation (RAG) sys-
tems by estimating how many exam questions a
simulated QA system can correctly answer based
on the generated responses. ARES constructs LLM
judges using few-shot demonstrations and a boot-
strapped training dataset (Saad-Falcon et al., 2024).
We find that heuristics-based metrics such as RA-
GAS perform poorly on hallucination tasks com-
pared to LLM-as-judge evaluators (Table 3). While
ARES offers more flexibility than prior metrics, the
construction of training datasets on the fly intro-
duces significant overhead, making the approach
less suitable for production settings.
A related area of research is Natural Language
3Github repo: https://github.com/patronus-ai/Lynx-hallucination-
detection
4Nomic
Atlas:
https://atlas.nomic.ai/data/patronus-
ai/halubench/map

Figure 1: LLM-as-a-judge responses of GPT-4o, Claude-3-Sonnet and LYNX (70B) for a Question Answering
example from HaluEval.
Inference (NLI) (Chen et al., 2018), where the task
of categorizing whether statements entail or con-
tradict one another is similar to detecting LLM
outputs that are inconsistent with provided con-
texts. Recent work has drawn parallels between the
task of NLI and hallucination detection (Honovich
et al., 2022). While most existing models are fine-
tuned solely to output an evaluation score, LYNX is
trained using both reasoning chains and evaluation
scores similar to NLI tasks, thereby improving the
interpretability of the evaluation score.
Another line of research assesses the factuality
of LLM responses. Several datasets have been con-
structed for fact extraction and verification, includ-
ing FEVER (Aly et al., 2021) and AIS (Rashkin
et al., 2022), which assesses whether outputs are
attributable to identifiable sources. Prior work on
automated factuality evaluation includes metrics
and model based approaches (Ganesan, 2018; Kry´s-
ci´nski et al., 2019; Sellam et al., 2020). More re-
cently, Wei et al. (2024) proposed augmenting eval-
uator LLMs with Google Search for scoring long
form factuality. Though factuality is often mea-
sured in search-based settings, TruthfulQA (Lin
et al., 2022) measures how models respond to com-
mon falsehoods and misconceptions . While factu-
ality is important for building trust in AI systems,
our work focuses on the problem of hallucination
detection as it applies to RAG settings.
To evaluate the performance of models on hallu-
cination detection, Wu et al. (2023b) introduced
RAGTruth, a dataset of 18k responses consist-
ing of generations from a variety of LLMs. Sim-
ilarly Li et al. (2023) introduced the HaluEval
dataset, which contained synthetic responses gen-
erated from LLMs that were prompted to generate
hallucinatory outputs. However, these datasets do
not include domain specific tasks, which can be sig-
nificantly more complex and more similar to real-
world scenarios that users encounter in industry
applications. While CRAG (Yang et al., 2024) con-
sists of several industry domains, the task includes
external APIs for end-to-end system testing as op-
posed to focusing on hallucinations in provided
contexts. In our construction of HaluBench we use
existing datasets as well as synthetic data pertur-
bations to construct a comprehensive hallucination
evaluation benchmark that includes specialized do-
main specific QA tasks from finance and medicine.
3
Methodology
In a RAG pipeline, we first (1) Retrieve the rele-
vant context(s) given a query, then (2) Generate an
answer to the query given the retrieved context(s)
with an LLM. “Hallucinations” (Jian et al., 2022;
Ji et al., 2023) occur when the generated answers
are not faithful to the context (intrinsic hallucina-
tions) or don’t align with factual reality (extrinsic
hallucinations). In this paper, we focus solely on in-
trinsic hallucination evaluation since in real-world
settings, user-provided documents may contain in-
formation that conflicts with external knowledge
sources. The purpose of LYNX is to provide a
reference-free metric for automated RAG evalua-

tion, thus we consider factuality assessments out of
scope for this work.
In the following sections, we describe the pro-
cess for training LYNX, a SOTA hallucination de-
tection LLM. We begin with the definition of hallu-
cination (Section 3.1), followed by the construction
process of our training and evaluation data (Sec-
tion 3.2). Finally, we present experimental results
on hallucination tasks sourced from real-world do-
mains.
3.1
Hallucination Evaluation
For a given question x, we say that the LLM is hal-
lucinating if the answer P(x) is not supported by
the context C(x) when contextualized by the ques-
tion. In practice, LLM generated answers are often
inconsistent with the retrieved context (Li et al.,
2023). In our definition of hallucination, we do not
assess the relevance of the retrieved context C(x)
to the query x. If the answer, P(x) is consistent
with the irrelevant context, C(x) we will consider
the answer to be faithful to the context. Similarly,
if the answer, P(x) to a question, x is incorrect
but it states information consistent with the context,
C(x) it will be evaluated as faithful.
3.2
HaluBenchConstruction
We sourced examples from several existing QA
datasets to build the hallucination evaluation bench-
mark. We constructed tuples of (question, context,
answer, label), where label is a binary score that de-
notes whether the answer contains a hallucination.
HaluBench consists of the following tasks:
• FinanceBench (Islam et al., 2023):
Fi-
nanceBench consists of 10k questions, con-
texts and answers over financial documents,
containing both tables and bullet point lists.
FinanceBench was designed to be similar to
real-world financial question and answering
from financial analysts. We randomly sam-
pled 1k samples, of which 500 contain hallu-
cinations.
• DROP (Dua et al., 2019): DROP is an En-
glish reading comprehension benchmark that
assesses reasoning ability over the content of
paragraphs. The dataset was crowdsourced
and adversarially created. We randomly sam-
pled 1k samples, of which 500 contain hallu-
cinations.
• COVID-QA (Möller et al., 2020): COVID-
QA consists of 2k question-answer pairs an-
notated by volunteer biomedical experts on
scientific articles related to COVID-19. We
randomly sampled 1k samples, of which 500
contain hallucinations.
• PubMedQA (Jin et al., 2019): PubMedQA is
a biomedical question answering (QA) dataset
collected from PubMed abstracts. The task
consists of answering research questions with
yes/no/maybe responses. It also contains a
long answer that provides evidence from the
context for the response.
• HaluEval (Li et al., 2023): HaluEval is a
hallucination evaluation dataset consisting of
general user queries with ChatGPT responses
and task-specific examples from three tasks,
i.e., question answering, knowledge-grounded
dialogue, and text summarization. We used
the qa_samples subset, which contains 10k
questions with knowledge from Wikipedia
and question text and ground-truth answers
collected from HotpotQA.
• RAGTruth (Wu et al., 2023b): RAGTruth is
a corpus containing word-level hallucination
annotations on LLM generated text. We used
the test split that comprised of 900 samples, of
which 160 examples contain hallucinations.
Construction of Hallucination Examples
Four
of the QA datasets we sourced from (DROP, Fi-
nanceBench, COVID-QA, PubMedQA) do not
contain answers that are not faithful to the con-
text. To construct unfaithful answers, we used the
Context-Question-Answer to generate semantically
perturbed versions of gold answers to questions.
We define a semantic answer perturbation as a re-
sponse that is minimally different to the gold an-
swer, but contains an inconsistency with the con-
text that results in a hallucination. We use GPT-4o
to construct these perturbations. The prompt and
generation settings are in Appendix A.
Let {q, c, x, y} denote the question, context, an-
swer and label of a given example in dataset D,
where y ∈{0, 1}. For our perturbation generator
fp, let ˜x ∼fp(q, c, x) be the semantically altered
perturbation output. Our perturbed dataset is thus
D′ = {(q, c, ˜x, 1 −y)|(q, c, x, y) ∈D}
(1)

Dataset
Example
HaluEval
Context: 750 Seventh Avenue is a 615 ft (187m) tall Class-A office skyscraper in New York City. 101
Park Avenue is a 629 ft tall skyscraper in New York City, New York.
Question: 750 7th Avenue and 101 Park Avenue, are located in which city?
Answer: 750 7th Avenue and 101 Park Avenue are located in Albany, New York.
DROP
Context: Hoping to rebound from the road loss to the Chargers, the Rams went home for Week 9, as
they fought the Kansas City Chiefs in a ¨Show Me State Showdown¨. The Chiefs struck first as RB Larry
Johnson got a 1-yard TD run for the only score of the period. In the second quarter, things got worse
for the Rams as QB Damon Huard completed a 3-yard TD pass to TE Tony Gonzalez, while kicker
Lawrence Tynes nailed a 42-yard field goal. St. Louis got on the board with RB Steven Jackson getting a
2-yard TD run, yet Huard and Gonzalez hooked up with each other again on a 25-yard TD strike. Rams
kicker Jeff Wilkins made a 41-yard field goal to end the half. In the third quarter, QB Marc Bulger
completed a 2-yard TD pass to WR Kevin Curtis for the only score of the period, yet the only score of
the fourth quarter came from Huard completing an 11-yard TD pass to TE Kris Wilson. With the loss,
the Rams fell to 4-4.
Question:Which team scored the longest field goal kick of the game?
Answer: Rams
CovidQA
Context: .......An important part of CDC’s role during a public health emergency is to develop a test
for the pathogen and equip state and local public health labs with testing capacity. CDC developed an
rRT-PCR test to diagnose COVID-19. As of the evening of March 17, 89 state and local public health
labs in 50 states......
Question: What kind of test can diagnose COVID-19?
Answer: rRT-PCR test
FinanceBench
Context: Consolidated Statement of Income PepsiCo, Inc. and Subsidiaries Fiscal years ended December
29, 2018, December 30, 2017 and December 31, 2016 (in millions except per share amounts) 2018 2017
2016 Net Revenue $ 64,661......
Question: What is the FY2018 fixed asset turnover ratio for PepsiCo? Fixed asset turnover ratio is
defined as: FY2018 revenue / (average PP&E between FY2017 and FY2018). Round your answer to
two decimal places.
Answer: 3.7%
PubmedQA
Context: .......The study cohort consisted of 1,797 subjects (1,091 whites and 706 blacks; age = 21-48
years) enrolled in the Bogalusa Heart Study since childhood. BP variability was depicted as s.d. of 4-8
serial measurements in childhood.......
Question: Is adult hypertension associated with blood pressure variability in childhood in blacks and
whites : the bogalusa heart study?
Answer: No. Increases in BP variations as well as levels in early life are not predictive of adult
hypertension, which suggests that childhood BP variability does not have a significant impact on the
natural history of essential hypertension.
Table 1: Examples of hallucinations from HaluBench. If the answer is not supported by the context, it is regarded
as a hallucination.
To construct HaluBench, we randomly sampled
500 examples from each of the four datasets. We
then additionally sampled 500 examples and con-
structed a perturbed set containing hallucinations
by applying the perturbation generator fp. The
final task consisted of a balance of positive and
negative labels. See Table 2 for a breakdown of
tasks in HaluBench. We present some examples
from HaluBench in Table 1. The hallucinated an-
swers from DROP and FinanceBench demonstrate
answer perturbations. We adopted the same pertur-
bation approach to construct training and validation
datasets to finetune models for faithfulness evalua-
tion.
Human Annotation
To verify that LLM gener-
ated samples in HaluBench are of high quality and
that our perturber fp did actually induce hallucina-
tions, we selected a random subset of 50 examples
each from DROP, FinanceBench, CovidQA and
PubMedQA for human annotation. Expert annota-
tors manually checked the original and perturbed
answers as well as reasoning provided for each
example. Annotators found the data to be of rela-
tively high quality (see Table 2), with high human
agreement of 0.94 across 200 samples.
Dataset
Score
DROP
0.92
FinanceBench
0.90
CovidQA
0.96
PubmedQA
0.96
Table 2: Agreement with human annotator for a subset
of HaluBench. We use n=50 samples for each of the
above datasets.

3.3
Model Training
Training Dataset Construction
The training
dataset for LYNX consists of 2400 samples, along
with 800 samples for validation. The dataset con-
sists of demonstrations sourced from RAGTruth,
DROP, CovidQA and PubMedQA. For each sub-
task, we sampled 600 examples from the train split
of the source dataset, of which 300 were perturbed
to construct hallucinated answers.
Chain of Thought (CoT) has been shown to im-
prove zero-shot performance of LLMs (Wei et al.,
2022). To distill the evaluation reasoning capabili-
ties of GPT-4o to our finetuned open-source model,
we used GPT-4o to generate reasoning for the label
of each example in our training set. We provided
this as as part of the assistant response, along with
the label in the instruction tuning process. The
prompt to generate reasoning traces is present in
Appendix A.
Self-Instruct Tuning
We trained two models
with supervised fine-tuning using the Llama-3-70B-
Instruct and Llama-3-8B-Instruct checkpoints on a
dataset of 2400 (question, answer, context, label)
examples. Examples are formatted for instruction-
tuning (Wei et al., 2021) in a chat-based format,
where the evaluation task is provided in the instruc-
tion to the assistant, and the gold answer is the
assistant response. The model is tasked to output
JSON in the following format:
{
"REASONING": <reasoning provided as
bullet points>,
"SCORE": <final score of PASS or FAIL>
}
We trained the model for 3 epochs with a learn-
ing rate of 5.0e-7 and batch size of 256. For super-
vised finetuning on 70B models, we trained on 32
Nvidia H100 GPUs. We used several performance
optimizations including FSDP and flash attention.
Our full training setup for both LYNX (8B) and
LYNX (70B) is described in detail in Appendix B.
4
Results
4.1
Evaluation Results
We evaluated LYNX on HaluBench to assess its per-
formance on hallucination detection in real world
settings.
To put the results in context, we compared our
proposed solution (shown as LYNX in Table 2) with
several baseline methods. We prompted the model
to assess whether the response was faithful to the
context, and provided the question, answer and con-
text. We instructed the model to produce a binary
score, where "FAIL" indicates that it was halluci-
nated and "PASS" indicates that the response was
faithful. We also instructed the model to produce
reasoning for the score. We used the same zero-
shot prompt for all models and tasks, to ensure a
fair comparison and generalization of our approach
to new domains. We additionally show results for
RAGAS by setting a faithfulness threshold of 50%,
where any score less than 50% is treated as a hal-
lucination. Results are shown in Table 3. The
evaluation prompt is available in Appendix A.
Out of all closed source and open-source models
evaluated, LYNX (70B) reports the highest accuracy
on all evaluation tasks. LYNX (70B) outperformed
GPT-4o by almost one percent accuracy on average
across all tasks. For domain specific tasks, this
difference is even more pronounced; LYNX (70B)
is 8.3% more accurate than GPT-4o at identifying
inaccurate responses in medical answers in Pub-
MedQA. LYNX (8B) and LYNX (70B) both show
an increase in accuracy on all tasks compared to
the baseline Llama 3 models, with the finetuned
70B model resulting in a 7.8% increase in aver-
age accuracy. When compared to closed source
LLMs, LYNX outperforms GPT-3.5-Turbo by an
even wider margin, with an average increase of
27.6% across all tasks. LYNX (70B) is the best
performing model overall, with 87.4% accuracy
on HaluBench. GPT-3.5-Turbo showed the lowest
accuracy out of all models evaluated, with 58.7%
accuracy averaged across all tasks.
5
Conclusion
As RAG systems continue to rise in popularity,
automated reference-free evaluation of RAG sys-
tems is critical for the safe deployment of RAG
systems at scale. We propose an evaluation model
that assesses faithfulness of model responses in
reference-free settings, which has important impli-
cations in business contexts ranging from detecting
erroneous responses in financial Q&A to prevent-
ing misinformation in medical AI assistants. Our
results show that LYNX outperforms industry and
academic alternatives on HaluBench.
We have introduced HaluBench, a compre-
hensive hallucination evaluation benchmark that
contains annotations of the faithfulness of tex-
tual responses across several real-world domains.

Model
HaluEval
RAGTruth
FinanceBench
DROP
CovidQA
PubMedQA
Overall
GPT-4o
87.9%
84.3%
85.3%
84.3%
95.0%
82.1%
86.5%
GPT-4-Turbo
86.0%
85.0%
82.2%
84.8%
90.6%
83.5%
85.0%
GPT-3.5-Turbo
62.2%
50.7%
60.9%
57.2%
56.7%
62.8%
58.7%
Claude-3-Sonnet
84.5%
79.1%
69.7%
84.3%
95.0%
82.9%
78.8%
Claude-3-Haiku
68.9%
78.9%
58.4%
84.3%
95.0%
82.9%
69.0%
RAGAS Faithfulness
70.6%
75.8%
59.5%
59.6%
75.0%
67.7%
66.9%
Mistral-Instruct-7B
78.3%
77.7%
56.3%
56.3%
71.7%
77.9%
69.4%
Llama-3-Instruct-8B
83.1%
80.0%
55.0%
58.2%
75.2%
70.7%
70.4%
Llama-3-Instruct-70B
87.0%
83.8%
72.7%
69.4%
85.0%
82.6%
80.1%
LYNX (8B)
85.7%
80.0%
72.5%
77.8%
96.3%
85.2%
82.9%
LYNX (70B)
88.4%
80.2%
81.4%
86.4%
97.5%
90.4%
87.4%
Table 3: Accuracy of different LLMs on HaluBench. Note that DROP, CovidQA and PubMedQA contain
semantically perturbed samples in addition to the original samples. The best performance among open-source
models is denoted by underline and the best overall performace is denoted by bold.
HaluBench is unique for containing balanced dis-
tributions of positive and negative examples, and
for a high percentage of examples grounded in real-
world contexts. HaluBench consists of challenging
hallucination detection examples, and shows high
agreement with human annotations.
Lastly, we are open sourcing our evaluation
datasets and model outputs, along with human an-
notations. The LYNX model is lightweight and easy
to use, and can provide developers of RAG systems
with useful insights, especially in the absence of
ground truth annotations.
6
Limitations and Future Work
Failures outside of LLM Generation
In real
world deployments of RAG systems, there are often
failures outside of RAG systems that can result in
inaccuracies in LLM outputs. A common failure
in RAG systems outside of LLM generation is in
the retrieval component, where the retriever does
not return relevant contexts to the query. This can
result in downstream hallucinations, as the context
provided to the generation model does not contain
sufficient information to address the input.
Other sources of failures in RAG systems un-
related to the LLM include pre-processing and
post-processing steps, database queries and incon-
sistencies in data sources. In particular, source
documents that contain conflicting information, or
misinformation, present a challenge for failure de-
tection due to its ambiguity. The resolution of
conflicting information in fact checking tasks is a
continued area of research. We leave an in depth ex-
ploration of improving retrieval modules to future
work.
Multilingual Coverage
The bulk of datasets
used in HaluBench is in English, which presents
a limitation in real-world applications that include
multilingual inputs and contexts. We hope to en-
hance coverage and diversity in HaluBench and
training data by incorporating non-English and low
resource languages in future extensions.
Summarization
and
Other
NLP
Tasks
HaluBench is focused on Question Answering
tasks due to the prevalence of chat-based interfaces
used by knowledge workers in industry RAG
applications. An area for future work is extending
LYNX to additional NLP domains, including
abstractive summarization tasks where LLM
produced summaries may contain inconsistent
information with the source document.
Truthfulness and World Knowledge
LYNX fo-
cuses on the problem of hallucination detection.
The assessment of truthfulness and factuality is also
an important factor of consideration, and requires
the incorporation of external knowledge sources as
world knowledge.
Natural Language Inference
An interesting
area for future work involves applying LYNX to
NLI tasks. Since the problem of hallucination de-
tection is closely related to NLI, a strong hallucina-
tion detection model is likely capable of perform-
ing reasoning in other NLP domains. We leave
research on the relationship between evaluation
tasks and other NLP tasks to future work.
7
Acknowledgements
We would like to thank Brandon Cui, Connor Jen-
nings from Databricks Mosaic AI, Candace Ross
from Meta AI and Andriy Mulyar from Nomic AI
for their feedback and contributions to the paper.
We additionally thank the Mosaic Databricks AI
team for their support in the development of LYNX.
We also thank Nino Scherrer for his contributions
to the research process. Finally, we want to thank

our partners, Nvidia and Nomic AI for their support
of LYNX and HaluBench.
References
Rami Aly, Christos Christodoulopoulos, Oana Co-
carascu, Zhijiang Guo, Arpit Mittal, Michael
Schlichtkrull, James Thorne, and Andreas Vlachos,
editors. 2021. Proceedings of the Fourth Workshop
on Fact Extraction and VERification (FEVER). As-
sociation for Computational Linguistics, Dominican
Republic.
Amos Azaria and Tom Mitchell. 2023. The internal
state of an llm knows when it’s lying. arXiv preprint
arXiv:2304.13734.
Biplob Biswas, Renhao Cui, and Rajiv Ramnath. 2022.
Retrieval based response letter generation for a cus-
tomer care setting. In Proceedings of the 2022 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: Industry Track, pages 168–175,
Hybrid: Seattle, Washington + Online. Association
for Computational Linguistics.
Meng Cao, Yue Dong, and Jackie Cheung. 2022. Hal-
lucinated but factual! inspecting the factuality of
hallucinations in abstractive summarization. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 3340–3354, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana
Inkpen, and Si Wei. 2018. Neural natural language
inference models enhanced with external knowledge.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2406–2417, Melbourne, Aus-
tralia. Association for Computational Linguistics.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
Drop:
A reading comprehension benchmark re-
quiring discrete reasoning over paragraphs. arXiv
preprint arXiv:1903.00161.
Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and
Siva Reddy. 2022. On the origin of hallucinations
in conversational models: Is it the datasets or the
models? In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 5271–5285, Seattle, United States.
Association for Computational Linguistics.
Shahul Es, Jithin James, Luis Espinosa-Anke, and
Steven Schockaert. 2023. Ragas: Automated evalua-
tion of retrieval augmented generation.
Kavita Ganesan. 2018. Rouge 2.0: Updated and im-
proved measures for evaluation of summarization
tasks.
Nuno M Guerreiro, Elena Voita, and André FT Martins.
2022. Looking for a needle in a haystack: A com-
prehensive study of hallucinations in neural machine
translation. arXiv preprint arXiv:2208.05309.
Or Honovich, Roee Aharoni, Jonathan Herzig, Ha-
gai Taitelbaum, Doron Kukliansy, Vered Cohen,
Thomas Scialom, Idan Szpektor, Avinatan Has-
sidim, and Yossi Matias. 2022. True: Re-evaluating
factual consistency evaluation.
arXiv preprint
arXiv:2204.04991.
Pranab Islam, Anand Kannappan, Douwe Kiela, Re-
becca Qian, Nino Scherrer, and Bertie Vidgen. 2023.
Financebench: A new benchmark for financial ques-
tion answering. arXiv preprint arXiv:2311.11944.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of halluci-
nation in natural language generation. ACM Comput-
ing Surveys, 55(12):1–38.
Yiren Jian, Chongyang Gao, and Soroush Vosoughi.
2022. Embedding hallucination for few-shot lan-
guage fine-tuning. In Proceedings of the 2022 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 5522–5530, Seattle,
United States. Association for Computational Lin-
guistics.
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W
Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset
for biomedical research question answering. arXiv
preprint arXiv:1909.06146.
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,
Shayne Longpre,
Hwaran Lee,
Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, et al.
2023. Prometheus: Inducing fine-grained evaluation
capability in language models. In The Twelfth Inter-
national Conference on Learning Representations.
Wojciech Kry´sci´nski, Bryan McCann, Caiming Xiong,
and Richard Socher. 2019. Evaluating the factual
consistency of abstractive text summarization.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems, 33:9459–9474.
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun
Nie, and Ji-Rong Wen. 2023. Halueval: A large-
scale hallucination evaluation benchmark for large
language models. arXiv preprint arXiv:2305.11747.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
TruthfulQA: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3214–3252, Dublin,
Ireland. Association for Computational Linguistics.

Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval:
Unified multi-dimensional automatic evaluation for
open-domain conversations with large language mod-
els. arXiv preprint arXiv:2305.13711.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023. G-eval:
Nlg evaluation using gpt-4 with better human align-
ment.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2022.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. arXiv preprint arXiv:2212.10511.
Potsawee Manakul, Adian Liusie, and Mark JF Gales.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language
models. arXiv preprint arXiv:2303.08896.
Timo Möller, Anthony Reina, Raghavan Jayakumar, and
Malte Pietsch. 2020. Covid-qa: A question answer-
ing dataset for covid-19. In Proceedings of the 1st
Workshop on NLP for COVID-19 at ACL 2020.
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An-
ton Bakhtin, Yuxiang Wu, Alexander H Miller, and
Sebastian Riedel. 2019. Language models as knowl-
edge bases? arXiv preprint arXiv:1909.01066.
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,
Lora Aroyo, Michael Collins, Dipanjan Das, Slav
Petrov, Gaurav Singh Tomar, Iulia Turc, and David
Reitter. 2022. Measuring attribution in natural lan-
guage generation models.
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Kurt Shuster, Eric M Smith, et al. 2020. Recipes
for building an open-domain chatbot. arXiv preprint
arXiv:2004.13637.
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and
Matei Zaharia. 2024. Ares: An automated evalua-
tion framework for retrieval-augmented generation
systems.
David P Sander and Laura Dietz. 2021. Exam: How
to evaluate retrieve-and-generate systems for users
who do not (yet) know what they want. In DESIRES,
pages 136–146.
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh.
2020. Bleurt: Learning robust metrics for text gener-
ation.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021a. Retrieval augmentation
reduces hallucination in conversation. In Findings
of the Association for Computational Linguistics:
EMNLP 2021, pages 3784–3803, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Kurt Shuster, Jack Urbanek, Emily Dinan, Arthur Szlam,
and Jason Weston. 2021b.
Dialogue in the wild:
Learning from a deployed role-playing game with
humans and bots. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021,
pages 611–624, Online. Association for Computa-
tional Linguistics.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in neural
information processing systems, 35:24824–24837.
Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu,
Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng,
Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le.
2024. Long-form factuality in large language mod-
els.
Sean Wu, Michael Koo, Lesley Blum, Andy Black, Liyo
Kao, Fabien Scalzo, and Ira Kurtz. 2023a. A com-
parative study of open-source large language models,
gpt-4 and claude 2: Multiple-choice test taking in
nephrology.
Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum,
Cheng Niu, Randy Zhong, Juntong Song, and Tong
Zhang. 2023b. Ragtruth: A hallucination corpus for
developing trustworthy retrieval-augmented language
models. arXiv preprint arXiv:2401.00396.
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,
Xiangsen Chen, Sajal Choudhary, Rongze Daniel
Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong,
Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan,
Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang,
Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah,
Rakesh Wanga, Anuj Kumar, Wen tau Yih, and
Xin Luna Dong. 2024. Crag – comprehensive rag
benchmark.
Wenhao Yu. 2022.
Retrieval-augmented generation
across heterogeneous knowledge. In Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies: Student Research
Workshop, pages 52–58, Hybrid: Seattle, Washington
+ Online. Association for Computational Linguistics.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text gener-
ation. Advances in Neural Information Processing
Systems, 34:27263–27277.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-
tian M Meyer, and Steffen Eger. 2019. Moverscore:
Text generation evaluating with contextualized em-
beddings and earth mover distance. arXiv preprint
arXiv:1909.02622.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems, 36.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena.
Lianghui Zhu, Xinggang Wang, and Xinlong Wang.
2023.
Judgelm:
Fine-tuned large language
models are scalable judges.
arXiv preprint
arXiv:2310.17631.

A
Prompts
A.1
Data Generation
For generating the perturbed answers, we use
the following prompt with GPT-4o with temper-
ature=0.
QUESTION:
{question}
GOLD_ANSWER:
{gold_answer}
EVIDENCE_TEXT:
{evidence_text}
How can we change the GOLD_ANSWER subtly
such that it would be wrong? The perturbed
answer
should still give the impression
of a
valid answer, but inspection of
the
EVIDENCE_TEXT would reveal that the
perturbed answer is factually wrong.
Output the new answer and change made
in JSON format with the key 'new_answer'
and 'change_made'.
To generate the reasoning chains, we use the fol-
lowing prompts with GPT-4o with temperature=0.
I. For perturbed samples:
Below is the System Prompt:
You are given a QUESTION, CONTEXT,
CHANGE_MADE, GOLD_ANSWER and ANSWER.
Explain why the ANSWER is not faithful
to the CONTEXT, given the QUESTION.
CHANGE_MADE specifies the change
made to the GOLD_ANSWER which made
the ANSWER not faithful. Do not refer
explicitly to the words 'CHANGE_MADE'
or 'GOLD_ANSWER' in your reasoning.
Generate your reasoning in JSON format:
{"REASONING": "<your reasoning steps as
bullet points>"}
Below is the User Prompt:
<QUESTION>
{question}
</QUESTION>
<CONTEXT>
{context}
</CONTEXT>
<CHANGE_MADE>
{change_made}
</CHANGE_MADE>
<GOLD_ANSWER>
{answer}
</GOLD_ANSWER>
<ANSWER>
{new_answer}
</ANSWER>
II. For original samples:
System Prompt:
You are given a QUESTION, CONTEXT, ANSWER.
Explain the similarities between the CONTEXT
and the ANSWER. Reason about why the
ANSWER is faithful to the CONTEXT given
the QUESTION. Generate your reasoning in
JSON format:
{"REASONING": "<your reasoning steps as
bullet points>"}
User Prompt:
<QUESTION>
{question}
</QUESTION>
<CONTEXT>
{context}
</CONTEXT>
<ANSWER>
{answer}
</ANSWER>
A.2
Evaluation
We use the following prompt for instruction fine-
tuning as well as for evaluation of models:
User Prompt:
Given the following QUESTION, DOCUMENT
and ANSWER you must analyze the provided
answer and determine whether it is
faithful to the contents of the DOCUMENT.

The ANSWER must not offer new information
beyond the context provided in the DOCUMENT.
The ANSWER also must not contradict
information provided in the DOCUMENT.
Output your final verdict by strictly
following this format: "PASS" if the
answer is faithful to the DOCUMENT
and "FAIL" if the answer is not
faithful to the DOCUMENT. Show your
reasoning.
--
QUESTION (THIS DOES NOT COUNT
AS BACKGROUND INFORMATION):
{question}
--
DOCUMENT:
{context}
--
ANSWER:
{answer}
--
Your output should be in JSON FORMAT with
the keys "REASONING" and "SCORE":
{{"REASONING": <your reasoning as
bullet points>, "SCORE": <your final score>}}
B
Training and Evaluation Details
B.1
Setup
For LYNX , we do mixed precision training with
flash attention. We use a cosine scheduler with
warmup. warmup steps is set to 100. We use li-
onw optimizer with β1 = 0.9 and β2 = 0.95 and
norm gradient clipping with threshold=1.0. FSDP
is used with FULL_SHARD strategy and activa-
tion_checkpointing enabled.
For evaluating the 70B models, we use vLLM
on 8 H100s with tensor_parallel = 8. For eval-
uating the 8B variants, we use model and data
sharding with accelerate. We use HuggingFace
pipelines for the generations, with greedy decoding
and max_new_tokens = 600.
B.2
Llama-2-13B-Chat Evaluation
We observe that the Llama-2-13B-Chat model does
not output JSON data or adhere to the response
structure requested in the prompt. However, af-
ter finetuning the model, we are able to parse re-
sponses to extract REASONING and SCORE. The
results are present in Table 5.
B.3
Training with extended datasets
As LYNX (70B) performed worse than Llama-3-
Instruct-70B on the RAGTruth test split, we ex-
tended the training data to include 2k samples from
RAGTruth. We finetuned Llama-3-Instruct-70B
on this extended dataset. The results are reported
in Table 4. While the performance increases on
the RAGTruth split, we see a slight decrease in
performance on the other splits. The overall perfor-
mance gain with the extended RAGTruth dataset is
∼0.4%.

Model
HaluEval
RAGTruth
FinanceBench
DROP
CovidQA
PubMedQA
Overall
Llama-3-Instruct-70B
87.0%
83.8%
72.7%
69.4%
85.0%
82.6%
80.1%
Llama-3-Instruct-70B (RAGTruth+)
88.8%
85.8%
81.2%
85.3%
96.9%
88.8%
87.8%
LYNX (70B)
88.4%
80.2%
81.4%
86.4%
97.5%
90.4%
87.4%
Table 4: Accuracy of Llama-3-Instruct-70B model when finetuned on additional RAGTruth samples (denoted by
RAGTruth+).
Model
HaluEval
RAGTruth
FinanceBench
DROP
CovidQA
PubMedQA
Overall
Llama-2-Chat-13B
3.1%
5.1%
4.4%
2.6%
2.0%
1.1%
3.3%
Llama-2-Chat-13B (Finetuned)
79.3%
77.6%
62.9%
76.4%
88.8%
81.8%
77.8%
Table 5: Performance of Llama-2-Chat-13B model on HaluBench. Finetuning improves parsability of the responses.

