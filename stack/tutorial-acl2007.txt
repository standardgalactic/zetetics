Structured Bayesian Nonparametric Models
with Variational Inference
ACL Tutorial
Prague, Czech Republic
June 24, 2007
Percy Liang and Dan Klein
Probabilistic modeling of NLP
• Document clustering
• Topic modeling
• Language modeling
• Part-of-speech induction
• Parsing and grammar induction
• Word segmentation
• Word alignment
• Document summarization
• Coreference resolution
• etc.
Recent interest in Bayesian nonparametric methods
2
Probabilistic modeling is a core technique for many NLP tasks such as the ones listed. In recent years, there
has been increased interest in applying the beneﬁts of Bayesian inference and nonparametric models to these
problems.

A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
-364
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.
A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
2
-204
-204
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.

A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
2
-204
4
-82
-82
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.
A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
2
-204
4
-82
12
12
21
21
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.

A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
2
-204
4
-82
12
21
20
20
86
86
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.
A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
2
-204
4
-82
12
21
20
86
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.

A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
-368
-368
2
-204
4
-82
12
21
20
86
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.
A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
-368
2
-204
-206
-206
4
-82
12
21
20
86
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.

A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
-368
2
-204
-206
4
-82
-128
-128
12
21
20
86
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.
A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
-368
2
-204
-206
4
-82
-128
12
12
21
-147
-147
20
86
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.

A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
-368
2
-204
-206
4
-82
-128
12
21
-147
20
20
86
-173
-173
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.
A motivating example
How many clusters?
K
training
log-likelihood
test
log-likelihood
1
-364
-368
2
-204
-206
4
-82
-128
12
21
-147
20
86
-173
3
In this example, four mixtures of Gaussians were generated and EM was used to learn a clustering. The example
shows the fundamental problem of using maximum likelihood as a criterion for selecting the complexity of a
model. As the complexity (number of clusters) increases, the training likelihood strictly improves. However, the
test likelihood improves initially but then degrades after a certain point.

Model selection: traditional solutions
Deﬁne an sequence of models of increasing complexity
Θ1
Θ2
Θ3
Θ4
...
• Cross-validation: select a model based on
likelihood on heldout set
• Bayesian model selection: use marginal likelihood or
minimum description length
Discrete optimization: combinatorial search over
models
4
The traditional approach to select the model of the right complexity is to search through a discrete space of
models, using a complexity penalty as a criteria for guiding the search. This might be diﬃcult if we need to
integrate out model parameters (e.g., in computing marginal likelihood).
A nonparametric Bayesian approach
Deﬁne one model with an inﬁnite number of clusters but
penalize the use of more clusters.
Θ
This penalty is accomplished with the Dirichlet process.
Continuous optimization: model complexity governed by
magnitude of parameters
Advantages:
• Works on structured models
• Allows EM-like tools
5
The nonparametric Bayesian approach does not choose between diﬀerent models but instead deﬁnes one model,
thus blurring the distinction between model and parameters. The advantage of this approach is that we will be
able to use familiar tools similar to the EM algorithm for parameter estimation.

True or false?
1. Being Bayesian is just about having priors.
Being Bayesian is about managing uncertainty.
2. Bayesian methods are slow.
Bayesian methods can be just as fast as EM.
3. Nonparametric means no parameters.
Nonparametric means the number of
eﬀective parameters grows adaptively with
the data.
4. Variational inference is complicated and foreign.
Variational inference is a natural extension of EM.
6
There are many myths about Bayesian methods being slow and diﬃcult to use. We will show that with suitable
approximations, we can get the beneﬁts of being Bayesian without paying an enormous cost.
Tutorial outline
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
7

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part I
8
Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part I / Distributions and Bayesian principles
9

Two paradigms
Traditional (frequentist) approach
data x
maximum likelihood
θ∗
θ
Bayesian approach
data x
Bayesian inference
q(θ)
θ
An example:
coin ﬂips (data):
x = H T H H
probability of H (parameter): θ = 3
4? 4
6?
We need a notion of distributions over parameters...
Part I / Distributions and Bayesian principles
10
The traditional frequentist approach is to estimate a single best parameter θ∗given the data x, using, for
example, maximum likelihood. However, in reality, the data is noisy so actually we are uncertain about our
parameter estimate θ∗. Therefore, we should maybe return several θs in order to reﬂect our uncertainty about
the best θ. This is a bottom-up motivation for the Bayesian approach.
Multinomial distribution
Most parameters in NLP are distributions p(z) over discrete objects
z ∈{1, . . . , K}.
Possible p(z)s are the points φs on the simplex. For K = 3:
φ =


0
1
0


1
2
3
φ =


0.2
0.5
0.3


1
2
3
0
1
0
1
φ1
φ2
p(z)
⇒
p(z | φ)
⇒
φz
A random draw z from a multinomial distribution is written:
z ∼Multinomial(φ).
If we draw some number of independent samples from
Multinomial(φ), the probability of observing cz counts of observation
z is proportional to:
φ1
c1 · · · φK
cK
Part I / Distributions and Bayesian principles
11
A multinomial distribution is a distribution over K possible outcomes {1, . . . , K}. A parameter setting for a
multinomial distribution is a point on the simplex: φ = (φ1, . . . , φK), φz ≥0, PK
z=1 φz = 1. We have turned
multinomial distributions into ﬁrst-class objects, so we can refer to φ instead of p(z). The form of the probability
of drawing (c1, . . . , cK) will play an important role in Dirichlet-multinomial conjugacy.

Distributions over multinomial parameters
A Dirichlet distribution is a distribution over multinomial
parameters φ in the simplex.
Like a Gaussian, there’s a notion of mean and variance.
Diﬀerent means:
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
Diﬀerent variances:
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
0
1
0
1
φ1
φ2
Part I / Distributions and Bayesian principles
12
All distributions we encounter in practice have a mean and variance. For a Gaussian N(µ, σ2), they are explicitly
represented in the parameters. For the Dirichlet distribution, the mapping between parameters and moments is
not as simple.
Dirichlet distribution
A Dirichlet is speciﬁed by concentration parameters:
α = (α1, . . . , αK), αz ≥0
Mean:

α1
P
z αz, . . . ,
αn
P
z αz

Variance: larger αs →smaller variance
A Dirichlet draw φ is written φ ∼Dirichlet(α),
which means p(φ | α) ∝φ1
α1−1 · · · φK
αK−1
The Dirichlet distribution assigns probability mass to multinomials
that are likely to yield pseudocounts (α1 −1, . . . , αK −1).
Mode:

α1−1
P
z(αz−1), . . . ,
αn−1
P
z(αz−1)

Part I / Distributions and Bayesian principles
13
The full expression for the density of a Dirichlet is p(φ | α) = Γ(PK
z=1 αz)
QK
z=1 Γ(αz)
QK
z=1 φz
αz.
Note that unlike
the Gaussian, the mean and mode of the Dirichlet are distinct. This leads to a small discrepancy between
concentration parameters and pseudocounts: concentration parameters α correspond to pseudocounts α −1.

Draws from Dirichlet distributions
Dirichlet(.5,.5,.5)
0
1
0
1
φ1
φ2
Dirichlet(1,1,1)
0
1
0
1
φ1
φ2
Dirichlet(5,10,8)
0
1
0
1
φ1
φ2
Part I / Distributions and Bayesian principles
14
A Dirichlet(1, 1, 1) is a uniform distribution over multinomial parameters.
As the concentration parameters
increase, the uncertainty over parameters decreases. Going in the other direction, concentration parameters
near zero encourage sparsity, placing probability mass in the corners of the simplex. This sparsity property is
the key to the Dirichlet process.
The Bayesian paradigm
data x
Bayesian inference
q(θ)
What is q(θ)?
q(θ)
def
= p(θ | x)
| {z }
posterior
=
1
p(x) p(θ)
|{z}
prior
p(x | θ)
| {z }
likelihood
Part I / Distributions and Bayesian principles
15
The Bayesian paradigm gives us a way to derive the optimal distribution over the parameters q(θ) given data.
We start with a prior p(θ), multiply the likelihood of the data we observe p(x | θ), and renormalize to get the
posterior p(θ | x). The computation of this posterior is generally the main focus of Bayesian inference.

Posterior updating for Dirichlet distributions
Model:
φ ∼Dirichlet3(0.5, 0.5, 0.5
|
{z
}
α
)
x ∼Multinomial7
3(φ)
Prior: p(φ)
∝φA
0.5−1 φB
0.5−1 φC
0.5−1
0
1
0
1
φA
φB
Likelihood: p(x | φ) = φA
2
φB
4
φC
1
A
B
B
C
A
B
B
Posterior: p(φ | x) ∝φA
2.5−1 φB
4.5−1 φC
1.5−1
0
1
0
1
φA
φB
Result: p(φ | x) = Dirichlet(2.5, 4.5, 1.5)
Part I / Distributions and Bayesian principles
16
The simplest example of computing the posterior is when there are no hidden variables. Assume we have 3
outcomes (A, B, C). Notation: the subscript 3 on the Dirichlet means that the dimensionality is 3 and the
superscript on the multinomial means that x is a sequence of 7 observations. Since the Dirichlet and multinomial
distributions are conjugate, the posterior can be computed in closed form and has the same form as the prior.
A two-component mixture model
φ1 ∼Dirichlet(1, 1)
A
B
φ2 ∼Dirichlet(1, 1)
A
B
zi ∼Multinomial(1
2, 1
2)
2
xi ∼Multinomial5(φzi)
A
A
B
B
A
Observed data:
x1 = A
B
B
B
B
x2 = A
B
B
B
B
x3 = B
A
A
A
A
Unknown parameters:
θ = (φ1, φ2)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
φ1
φ2
p(θ | x)
• True posterior p(θ | x) has symmetries
• φ1 explains x1, x2 and φ2 explains x3
in upper mode (or vice-versa in lower mode)
• The component explaining x3
has higher uncertainty
Part I / Distributions and Bayesian principles
17
For the Dirichlet example, we could compute the posterior analytically. This is not always the case, in particular,
when there are hidden variables. In the two-component mixture model example, the posterior is shown with
the hidden variables z marginalized out. Convention: letters denote outcomes of the data and numbers denote
components.

Using the posterior for prediction
Setup:
Assume a joint probabilistic model p(θ)p(x, y | θ)
over input x, output y, parameters θ.
Training examples: (x1, y1), . . . , (xn, yn)
Test input: xnew
Traditional: ynew∗= argmaxynew p(ynew | xnew, θ)
Bayes-optimal prediction:
ynew∗= argmaxynew p(ynew | xnew, {(xi, yi)})
Explicitly write out the integrated parameters:
p(ynew | xnew, {(xi, yi)}) =
R
p(ynew | xnew, θ) p(θ | {(xi, yi)})
|
{z
}
posterior
dθ
We can plug in an approximate posterior:
p(ynew | xnew, {(xi, yi)}) =
R
p(ynew | xnew, θ)qapprox(θ)dθ
Part I / Distributions and Bayesian principles
18
We have mentioned the desire to compute the posterior over parameters p(θ | x).
Now we motivate this
computation with a prediction task. Note that this decision-theoretic framework supports arbitrary loss functions,
but we have specialized to the 0-1 loss (which corresponds to maximizing probability) to ease notation.
Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part I / Variational Bayesian inference
19

Variational Bayesian inference
Goal of Bayesian inference: compute posterior p(θ, z | x)
Variational inference is a framework for approximating the:
true posterior with the best from a set of distributions Q:
q∗= argminq∈Q KL(q(θ, z)||p(θ, z | x))
Q
p(θ, z | x)
q∗(θ, z)
Part I / Variational Bayesian inference
20
The variational inference framework gives a principled way of ﬁnding an approximate distribution which is as
close (as measured by KL) to the posterior. This will allow us to tackle posterior computations for models such
as mixtures.
Types of variational approximations
q∗= argmin
q∈Q
KL(q(θ, z)||p(θ, z | x))
What types of Q can we consider?
q(θ, z)
=
q(θ)
q(z)
Hard EM
EM
mean-ﬁeld
Part I / Variational Bayesian inference
21
The quality of the approximation depends on Q: the bigger the Q, the better. Two of the familiar classical
algorithms (hard EM and EM) are based on a particular kind of Q. We will show that mean-ﬁeld is a natural
extension, by expanding Q but staying within the same framework. Of course, there are even larger Qs, but we
risk losing tractability in those cases.

Heuristic derivation of mean-ﬁeld
Algorithm template: iterate between the E-step and M-step.
Let us ﬁrst heuristically derive the mean-ﬁeld algorithm.
E-step
M-step
Find best z
Find best θ
z∗= argmaxz p(z | θ∗, x)
θ∗= argmaxθ p(θ | z∗, x)
Find best distrib. over z
Take distrib. into account
q(z)∗∝p(z | θ, x)
θ∗= argmaxθ
P
z q(z)p(θ | z, x)
θ∗= argmaxθ
Q
z p(θ | z, x)q(z)
Take distrib. into account
Find best distrib. over θ
q(z)∗∝Q
θ p(z | θ, x)q(θ)
q(θ)∗∝Q
z p(θ | z, x)q(z)
Inﬁnite product over θ has closed form if q(θ) is a Dirichlet.
Part I / Variational Bayesian inference
22
Before we formally derive algorithms based on the variational framework, let us heuristically consider what the
update for mean-ﬁeld might look like. Note that when we taking the approximating distribution into account,
we use a “geometric” expectation rather than a simple average. This follows from using KL-divergence and
makes computation tractable. The new challenge introduced by the mean-ﬁeld E-step is the inﬁnite product,
but this can be handled by exploiting certain properties of q.
Formal derivation of mean-ﬁeld
q∗= argmin
q∈Q
KL(q(θ, z)||p(θ, z | x))
Steps:
1. Formulate as an optimization problem (variational
principle)
2. Relax the optimization problem (e.g., mean-ﬁeld)
3. Solve using coordinate-wise descent
Part I / Variational Bayesian inference
23
Now that we know what the ﬁnal algorithm will look like, let us derive the algorithms directly as consequences
of the KL objective.

Kullback-Leibler divergence
KL measures how diﬀerent two distributions p and q are.
Deﬁnition:
KL(q||p)
def
= Eq log q(θ)
p(θ)
An important property:
KL(q||p) ≥0
KL(q||p) = 0 if and only if q = p
KL is asymmetric:
Assuming KL(q||p) < ∞,
p(θ) = 0 ⇒q(θ) = 0
[q“ ⊂”p]
Part I / Variational Bayesian inference
24
The optimization problem is in terms of KL-divergence. Before solving the optimization problem, we review
some of the properties of KL.
Minimizing with respect to KL
Since KL = 0 exactly when two arguments are equal, we have:
p = argminq∈Qall KL(q||p)
and
p = argminq∈Qall KL(p||q)
where Qall is all possible distributions.
Asymmetry revealed when we replace Qall with Q ⊊Qall.
Let p =
9.8
19.6
0.1
0.3
p
and Q = all possible Gaussians
q1 = argmin
q∈Q
KL(q||p) (mode-ﬁnding):
9.8
19.6
0.2
0.4
p
q1
q2 = argmin
q∈Q
KL(p||q) (mean-ﬁnding):
9.8
19.6
0.1
0.3
p
q2
Part I / Variational Bayesian inference
25
In ﬁtting a model, the approximating distribution is often the second argument (with the ﬁrst being the empirical
distribution). These distributions are over observed data, where one wants to assign mass to more than just
the observed data.
For variational inference, the distributions are over parameters, and the approximating
distribution appears in the ﬁrst argument.
This yields a tractable optimization problem.
Also, it allows us
to consider degenerate q with non-degenerate p (as in EM). The symmetric modes of a mixture model are
redundant, so it makes sense to capture only one of them anyway.

Step 1: formulate as optimization problem
Variational principle: write the quantity we wish to compute
as the solution of an optimization problem:
q∗def
= argmin
q∈Qall
KL(q(θ) || p(θ | x)),
where Qall is set of all distributions over parameters.
Solution: q∗(θ) = p(θ | x), which achieves KL = 0.
This is not very useful yet because q∗is just as hard to
compute...
Normalization of p not needed:
argminq KL(q(θ)||p(θ | x)) = argminq KL(q(θ)||p(θ, x))
Part I / Variational Bayesian inference
26
The variational principle is a general technique which originated out of physics, and can be applied to many
other problems besides inference in graphical models. See [Wainwright, Jordan, 2003] for a thorough treatment
of variational methods for graphical models. Note that the full posterior is p(θ, z | x), but we integrate out z
here so that we can visualize the posterior over parameters p(θ | x) = P
z p(θ, z | x) on the next slide.
Step 2: relax the optimization problem
q∗def
= argmin
q∈Qall
KL(q(θ) || p(θ | x))
Qall
p
Qall = all distributions
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
φ1
φ2
p(θ | x)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
φ1
φ2
Optimal q∗(θ)
Part I / Variational Bayesian inference
27
The advantage of using optimization is that it leads naturally to approximations: either by changing the domain
(which we do) or the objective function (which would lead to algorithms like belief propagation). Here, we show
the optimal approximating distributions for various Qs on the simple two-component mixture example.

Step 2: relax the optimization problem
q∗def
= argmin
q∈Qdeg
KL(q(θ) || p(θ | x))
Qall
p
Qdeg
q∗
Qdeg =
n
q : q(θ) = δθ
∗(θ)
o
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
φ1
φ2
p(θ | x)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
φ1
φ2
Degenerate q∗(θ)
Part I / Variational Bayesian inference
27
The advantage of using optimization is that it leads naturally to approximations: either by changing the domain
(which we do) or the objective function (which would lead to algorithms like belief propagation). Here, we show
the optimal approximating distributions for various Qs on the simple two-component mixture example.
Step 2: relax the optimization problem
q∗def
= argmin
q∈Qmf
KL(q(θ) || p(θ | x))
Qall
p
Qdeg
Qmf
q∗
Qmf =
n
q : q(θ) =
n
Y
i=1
qi(θi)
o
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
φ1
φ2
p(θ | x)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
φ1
φ2
Mean-ﬁeld q∗(θ)
Part I / Variational Bayesian inference
27
The advantage of using optimization is that it leads naturally to approximations: either by changing the domain
(which we do) or the objective function (which would lead to algorithms like belief propagation). Here, we show
the optimal approximating distributions for various Qs on the simple two-component mixture example.

Step 3: coordinate-wise descent
Goal: minimize KL(q(θ)||p(θ | x)) subject to q ∈Q
Assume: q(θ) = Q
i qi(θi)
Algorithm: for each i, optimize qi(θi) holding all other
coordinates q−i(θ−i) ﬁxed.
If qis degenerate (qi(θi) = δθ∗
i (θi)):
θ∗
i = argmaxθi p(θi | θ∗
−i, x)
If qis non-degenerate:
qi(θi) ∝exp{Eq−i log p(θi | θ−i, x)}
Part I / Variational Bayesian inference
28
Mean-ﬁeld only speciﬁes the optimization problem based on the variational framework.
We could use any
number of generic optimization algorithms to solve the mean-ﬁeld objective. However, a particularly simple and
intuitive method is to use coordinate-wise descent. To simplify notation, let θ both the parameters and the
hidden variables.
Mean-ﬁeld recipe
qi(θi) ∝exp{Eq−i log p(θi | θ−i, x)}
∝exp{Eq−i log p(θ, x)}
Recipe for updating qi:
1. Write down all the unobserved variables in the model
(including parameters and latent variables)
2. For each variable θi:
a. Write down only the factors of the joint distribution
that contain θi
b. Set qi ∝exp{Eq−i log(those factors)}
Part I / Variational Bayesian inference
29
We give a generic recipe for deriving a mean-ﬁeld algorithm for any probabilistic model. The remaining work
is to actually compute the exp E log, which is not obviously doable at all.
It turns out that it works out
when we have conjugate and our distributions are in the exponential family. Later we show this is true for the
Dirichlet-multinomial pair.

Non-Bayesian variational inference
· · ·
· · ·
• The E-step (computing p(z | x)) is sometimes
intractable when there are long-range dependencies
• Variational EM: approximate E-step using
argminq KL(q(z)||p(z | x))
• Variational EM ﬁts into the general variational
framework: one true posterior, various approximating
families
Part I / Variational Bayesian inference
30
Often the term variational is associated with another setting, where we wish to do inference in a loopy graph,
either as part of an E-step for a directed model with hidden variables or a gradient computation for an undirected
model. These cases can be interpreted as instances of the variational framework.
Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part I / Mean-ﬁeld for mixture models
31

Bayesian ﬁnite mixture model
β
zi
xi
i
n
φz
z
K
Parameters: θ = (β, φ) = (β1, . . . , βK, φ1, . . . , φK)
Hidden variables: z = (z1, . . . , zn)
Observed data: x = (x1, . . . , xn)
p(θ, z, x) = p(β)
K
Y
z=1
p(φz)
n
Y
i=1
p(zi | β)p(xi | φzi)
β ∼DirichletK(α′, . . . , α′)
For each component z ∈{1, . . . , K}:
φz ∼G0
For each data point i ∈{1, . . . , n}:
zi ∼Multinomial(β)
xi ∼F(φzi)
Prior over component parameters:
G0 (e.g., DirichletV (α′, . . . , α′))
Data model:
F (e.g., Multinomial)
Part I / Mean-ﬁeld for mixture models
32
There are several ways to represent a probabilistic model.
The graphical model allows one to visualize the
dependencies between all the random variables in the model, which corresponds to a particular factoring of
the joint probability distribution, where each factor is for example, p(zi | β). The procedural notation actually
speciﬁes the form of the distributions via a sequence of draws (e.g., zi ∼Multinomial(β)). Note that we are
representing the data model abstractly as (G0, F).
Bayesian ﬁnite mixture model
Running example: document clustering
β ∼DirichletK(α′, . . . , α′) [draw component probabilities]
K = 2 clusters
β:
1
2
For each component (cluster) z ∈{1, . . . , K}:
φz ∼DirichletV (α′, . . . , α′) [draw component parameter]
V = 3 word types
φ1:
A
B
C
φ2:
A
B
C
For each data point (document) i ∈{1, . . . , n}:
zi ∼MultinomialK(β) [choose component]
xi ∼Multinomialm
V (φzi) [generate data]
n = 5 documents
m = 4 words/doc
z1:
2
x1: C C A C
z2:
2
x2: C C A C
z3:
1
x3: A A B B
z4:
2
x4: C A C C
z5:
1
x5: A C B B
Part I / Mean-ﬁeld for mixture models
33
We will use the Bayesian ﬁnite mixture model as an example of applying the mean-ﬁeld algorithm. Later, it will
be extended to the DP mixture model.

Hard EM for the ﬁnite mixture model
Model: p(θ, z, x) = p(β)
K
Y
z=1
p(φz)
n
Y
i=1
p(zi | β)p(xi | φzi)
Approximation: q(θ, z) = q(β)
|{z}
δβ∗(β)
QK
z=1 q(φz)
| {z }
δφ∗z(φz)
Qn
i=1 q(zi)
|{z}
δz∗
i (zi)
E-step:
For each data point i ∈{1, . . . , n}:
zi∗= argmaxzi p(zi | β∗, x) = argmaxzi p(zi | β∗)p(xi | φzi)
M-step:
β∗= argmaxβ p(β | z∗, x) = argmaxβ p(β) Qn
i=1 p(z∗
i | β)
For each component z ∈{1, . . . , K}:
φ∗
z = argmaxφ p(φ) Qn
i=1 p(xi | φ)1[z∗
i =z]
Part I / Mean-ﬁeld for mixture models
34
We can derive hard EM for the Bayesian ﬁnite model using the recipe given at the end of Part I. Since
all qs are degenerate, optimizing the distribution is the same as optimizing a point.
For example, z∗
i =
argmaxz exp Eq−zi log p(z | β)p(xi | φz). Since q is degenerate, the exp and log cancel, leaving us with the
classical hard E-step.
Mean-ﬁeld for the ﬁnite mixture model
Model: p(θ, z, x) = p(β)
K
Y
z=1
p(φz)
n
Y
i=1
p(zi | β)p(xi | φzi)
M-step: optimize q(φz) and q(β)
q(β) ∝
Y
z
p(β | z, x)q(z)
∝p(β)
n
Y
i=1
K
Y
zi=1
p(zi | β)q(zi) = p(β)
K
Y
zi=1
βzi
Pn
i=1 q(zi)
Deﬁne expected counts of component z: Cz = Pn
i=1 qzi(z)
Recall the prior: p(β) = Dirichlet(β; α) ∝Qk
z=1 βz
α−1
Prior: QK
z=1 βz
α−1
“Likelihood”: QK
z=1 βz
Cz
“Posterior”: QK
z=1 βz
α−1+Cz
Conclusion: q(β) = Dirichlet(β; α + C)
Part I / Mean-ﬁeld for mixture models
35
Now we show the mean-ﬁeld algorithm. The M-step for the mean-ﬁeld algorithm involves just updating the
Dirichlet distribution. Rather than normalizing, the M-step keeps the full counts obtained during the E-step.
This extra degree of freedom gives mean-ﬁeld the ability to deal with uncertainty.
For simplicity, we only
consider the update for the component probabilities β, not the component parameters φ.

Mean-ﬁeld for the ﬁnite mixture model
Model: p(θ, z, x) = p(β)
K
Y
z=1
p(φz)
n
Y
i=1
p(zi | β)p(xi | φzi)
E-step: optimize q(zi)
q(zi) ∝
Y
θ
p(zi | θ, x)q(θ)
∝
Y
β
p(zi | β)q(β)
|
{z
}
W (zi)
Y
φzi
p(xi | φzi)q(φzi)
|
{z
}
W (xi|zi)
W(zi)W(xi | zi) is like p(zi)p(xi | zi), but multinomial weights W
takes into account uncertainty in θ.
Part I / Mean-ﬁeld for mixture models
36
The E-step requires a bit more work, as it involves the inﬁnite product over θ, or equivalently computing
exp E log.
Because the approximation is fully-factorized the optimal update breaks down into independent
integrals, each over a separate parameter. We call these quantities multinomial weights.
Mean-ﬁeld: computing multinomial weights
W(z) = Q
β p(z | β)q(β) = exp{Eq(β) log p(z | β)}
What’s q(β)?
E-step: expected counts: Cz = Pn
i=1 qzi(z)
M-step: q(β) = Dirichlet(β; α + C
| {z }
α′
)
Mean-ﬁeld multinomial weight:
W(z) =
exp(Ψ(α′
z))
exp(Ψ(PK
z′=1 α′z))
Compare with EM (q(β) = δβ∗(β) is degenerate):
W(z) = β∗
z =
α′
z−1
PK
z′=1(α′
z′−1)
Ψ(·) is the digamma function and is easy to compute.
Part I / Mean-ﬁeld for mixture models
37
The multinomial weights are like sub-probabilities (they always sum to less than 1 by Jensen’s inequality). The
more uncertainty there is, the smaller the weights. The computation of the multinomial weights bridges the
M-step and the E-step: they are computed based on the posterior distributions computed in the M-step and
used in the E-step. The digamma function Ψ(·) = ∂Γ(x)
∂x
is an easy function whose code can be copied out of
Numerical Recipes.

Mean-ﬁeld overview
E-step: q(zi) ∝W(zi)W(xi | zi), W(z) =
exp Ψ(αz+Cz)
exp Ψ(PK
z′=1 αz′+Cz′)
M-step: q(β) = Dirichlet(β; α + C), Cz = Pn
i=1 qzi(z)
EM
Mean-ﬁeld
E-step
q(zi)
C
θ∗
M-step
E-step
q(zi)
C
W
q(θ)
M-step
Part I / Mean-ﬁeld for mixture models
38
This slide shows the data ﬂow for the EM and mean-ﬁeld algorithms. While the output of each step is technically
a distribution over either parameters or hidden variables, the other step only depends on an aggregated value
(C for the M-step, W for the E-step).
Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part II
39

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part II / Emulating DP-like qualities with ﬁnite mixtures
40
Interpreting mean-ﬁeld through exp(Ψ(·))
EM with α = 1 (maximum likelihood):
W(z) =
Cz
PK
z′=1 Cz′
Mean-ﬁeld with α = 1:
W(z) =
exp(Ψ(1+Cz))
exp(Ψ(1+ PK
z′=1 Cz′))
(≊adding 0.5)
Mean-ﬁeld with α = 0:
W(z) =
exp(Ψ(Cz))
exp(Ψ( PK
z′=1 Cz′))
(≊subtracting 0.5)
0.4
0.8
1.2
1.6
2.0
x
0.4
0.8
1.2
1.6
2.0
x
exp(Ψ(·))
exp(Ψ(x)) ≊x −0.5
(for x > 1)
Part II / Emulating DP-like qualities with ﬁnite mixtures
41
We now embark on the development of the DP. Earlier, we said that the DP prior penalizes extra clusters. It
turns out that this penalty is based on uncertainty in parameter estimates. The more clusters there are, the
more fragmented the counts are, leading to greater uncertainty. We start by giving concrete intuition about
how just the mean-ﬁeld algorithm on ﬁnite mixtures can achieve this. The exp(Ψ(·)) plot captures the essence
of the DP from the point of view of mean-ﬁeld for multinomial data.

The rich get richer
What happens when α = 0?
E-step: q(zi) ∝W(zi)W(xi | zi)
Cz = Pn
i=1 q(zi)
M-step: W(z) =
exp(Ψ(Cz))
exp(Ψ(PK
z′=1 Cz′)) ≊
Cz−0.5
(PK
z′=1 Cz′)−0.5
Eﬀect of mean-ﬁeld with α = 0 on component probabilities β?
Thought experiment: ignore W(xi | zi).
10
20
30
iteration
2
4
6
8
10
counts Cz
z = 1
z = 2
z = 3
z = 4
When subtract 0.5,
small counts are hurt
more than large ones
(like a regressive tax)
The algorithm achieves sparsity by choosing one component.
In general, data term ﬁghts the sparsity prior.
Part II / Emulating DP-like qualities with ﬁnite mixtures
42
By ignoring the data-dependent factor W(xi | zi), we can focus on the tendencies of the mean-ﬁeld algorithm.
As the diagram shows, W(zi) encourages very few clusters. On the other hand, W(xi | zi) takes into account
the data, for which we might need many clusters to explain the data. A balance is obtained by combining both
factors.
The rich get even richer
E-step: q(zi) ∝W(zi)W(xi | zi)
Czj = Pn
i=1 qzi(z)xij
M-step: W(j | z) =
exp(Ψ(Czj))
exp(Ψ(PV
j′=1 Czj′)) ≊
Czj−0.5
(PV
j′=1 Czj′)−0.5
Thought experiment: ignore W(zi), focus on W(xi | zi).
Suppose C1A = 20, C1B = 20, C2A = 0.5, C2B = 0.2
W(A | 1)
W(A | 2)
EM:
20
20+20 = 0.5
0.5
0.5+0.2 = 0.71
Mean-ﬁeld:
eΨ(20+1)
eΨ(20+20+1) = 0.494
eΨ(0.5+1)
eΨ(0.5+0.2+1) = 0.468
For observation A: EM prefers component 2
mean-ﬁeld prefers 1
Key property: multinomial weights are not normalized,
allowing global tradeoﬀs between components.
Part II / Emulating DP-like qualities with ﬁnite mixtures
43
Now we focus on the data-dependent factor W(xi | zi). The example shows that even in the absence of W(zi),
there is a tendency to prefer few larger clusters over many small ones. To guard against this type of overﬁtting,
add-ϵ smoothing is often used. However, note that add-ϵ smoothing in this case will only mitigate the diﬀerence
but will never strictly prefer component 1 over 2.

Example: IBM model 1 for word alignment
we deemed it inadvisable to attend the meeting and so informed COJO .
nous ne avons pas cru bon de assister `a la r´eunion et en avons inform´e le COJO en cons´equence .
p(x, z; φ) =
n
Y
i=1
p(zi)p(xi | ezi; φ)
E-step: qzi(j) ∝p(xi | ej)
Ce,x =
P
i,j:ej=e,xi=x
qzi(j)
M-step: W(x | e) =
Ce,x
P
x′ Ce,x′
exp(Ψ(Ce,x))
exp(Ψ(P
x′ Ce,x′))
Garbage collectors problem: rare source words have large
probabilities for target words in the translation.
Quick experiment: EM: 20.3 AER, mean-ﬁeld: 19.0 AER
Part II / Emulating DP-like qualities with ﬁnite mixtures
44
This example shows a concrete case where being sensitive to uncertainty in parameters better using mean-ﬁeld
can improve performance with a very localized change to the original EM code. [Moore, 2004] also took note
of this problem with rare words and used add-ϵ smoothing.
An approximate Dirichlet process (DP) mixture model
Take ﬁnite mixture model, deﬁne p(β) = Dirichlet(α0
K , . . . , α0
K )
Theorem:
As K →∞, ﬁnite mixture model →DP mixture model.
As α0
K →0, mean-ﬁeld enters rich-gets-richer regime.
How to Bayesianify/DPify your EM algorithm
In the M-step of EM code where counts are normalized,
replace
Cz
P
z′ Cz′ with
exp Ψ(Cz)
exp Ψ(P
z′ Cz′).
Part II / Emulating DP-like qualities with ﬁnite mixtures
45
At this point, we’ve basically done. We have a model that acts empirically like a DP mixture model (we’ll deﬁne
this next) and a simple concrete algorithm for doing approximate inference.

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part II / DP mixture model
46
The limit of ﬁnite mixtures?
Theoretical goal:
Deﬁne a nonparametric (inﬁnite) mixture model.
First attempt:
Look at β ∼Dirichlet(α0
K , . . . , α0
K ) with α0 = 1:
K = 2 K = 3
K = 4
K = 5
K = 6
K = 7
K = 8
K→∞
−→?
Problem: for each component z, Eβz = 1
K →0.
The issue is that the Dirichlet is symmetric.
We need an asymmetric approach, where the large
components are ﬁrst on average.
Part II / DP mixture model
47
A ﬁrst attempt at deﬁning an inﬁnite mixture model is to take the limit of ﬁnite mixture models.
This
doesn’t work because Dirichlet distributions are symmetric, but the limiting index set {1, 2, . . . } is intrinsically
asymmetric.

Size-biased permutation
Solution: take a size-biased permutation of the components:
• Generate β′ ∼Dirichlet(α0
K , . . . , α0
K ):
• S ←∅
• For z = 1, . . . , K:
– Choose j ∼Multinomial(β′) conditioned on j ̸∈S
– βz ←β′
j
– S ←S ∪{j}
β′
1
β′
2
β′
3 β′
4
β′
5
β′
6
β′
7
β′
8
β1
β2
β3
β4
β5
β6
β7
β8
Stick-breaking characterization of distribution of β:
Deﬁne vz =
βz
PK
z′=z βz′ as the fraction of the tail-end of the stick
Fact: vz ∼Beta(1 + α0
K , α0 −zα0
K ).
Part II / DP mixture model
48
A way to get an asymmetric set of component probabilities is to generate from a symmetric Dirichlet and
permute the probabilities. The size-biased permutation will tend to put the large ones ﬁrst on average. Note:
the distribution on sorted component probabilities when K = ∞is the Poisson-Dirichlet distribution.
The stick-breaking (GEM) distribution
Now we can take the limit:
vz ∼Beta(1 + α0
K , α0 −zα0
K )
K →∞
vz ∼Beta(1, α0)
1:1 relationship between stick-breaking proportions v = (v1, v2, . . . )
and stick-breaking probabilities β = (β1, β2, . . . )
v1
1 −v1
1
v2
1 −v2
(1 −v1)
v3
1 −v3
(1 −v1)(1 −v2)
...
vz =
βz
βz + βz+1 + · · ·
βz = (1 −v1) · · · (1 −vz−1)vz
Write β ∼GEM(α0) to denote the stick-breaking distribution.
Part II / DP mixture model
49
Size-biased permutation motivates the asymmetric GEM distribution, but it can be deﬁned directly.
The
stick-breaking probabilities decrease exponentially in expectation, but of course sometimes a large stick can
follow a small one. It can be shown that the stick-breaking probabilities sum to 1 with probability 1.

Examples of the GEM distribution
vz ∼Beta(1, α0)
As α0 increases, sticks decay slower ⇒more clusters
GEM(0.3)
0
1
0
1
φ1
φ2
GEM(1)
0
1
0
1
φ1
φ2
GEM(3)
0
1
0
1
φ1
φ2
Part II / DP mixture model
50
This slide shows draws from the GEM distribution for three values of α0. No matter what value α0 takes, the
stick lengths decrease in expectation.
A cautionary tale about point estimates
Question: what is the most likely value of β ∼GEM(1.2)?
p(v) = QK
z=1 Beta(vz; 1, 1.2)
For each z, best vz = 0, so best βz = 0.
0.2
0.4
0.6
0.8
1.0
x
0.6
0.7
0.8
0.9
1.0
Beta(x; 1, 1.2)
But in typical draws, components decay...
A contradiction? No!
• Problem: mode not representative of entire distribution
• Solution: need inference algorithms that work with
entire distribution
Part II / DP mixture model
51
There is another complication, which is that the densities between the two parameterizations are related through
a non-identity Jacobian: p(β) = p(v) · Jacobian. Therefore, argmaxv p(v) does not necessarily correspond to
argmaxβ p(β). The most likely point depends on which parameterization you pick. Full Bayesian inference
integrates out these parameters, so parameterization is not an issue.

DP mixture model
Finite DP mixture model
β ∼DirichletK(α, . . . , α) GEM(α)
For each component z ∈{1, . . . , K} {1, 2, . . . }:
φz ∼G0
For each data point i ∈{1, . . . , n}:
zi ∼Multinomial(β)
xi ∼F(φzi)
Mean-ﬁeld inference [Blei, Jordan, 2005]:
Approximation in terms of stick-breaking proportions v:
q(β) = Q∞
z=1 q(vz)
How to deal with an inﬁnite number of parameters?
Choose a truncation level K and force q(vK) = 1.
Now q(vz) and q(φz) for z > K are irrelevant.
Part II / DP mixture model
52
Finally, we formally deﬁne the DP mixture model. In practice, there is not a noticeable diﬀerence between using
a ﬁnite Dirichlet prior with small concentration parameters and using a truncated stick-breaking prior. After
all, both converge to the DP. A more empirically important knob is the choice of inference algorithm. It is
important to note that using a stick-breaking truncation of K is not the same as just using a K-component
mixture model with concentration parameters that do not scale with 1/K. In the former, as K increases, the
approximation to the same DP becomes strictly better, whereas in the latter, the models become more complex.
Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part II / Other views of the DP
53

Three views of the Dirichlet process
Stochastic process
[Ferguson, 1973]
A1
A2
A3
A4
Chinese restaurant process
[Pitman, 2002]
. . .
Stick-breaking process
[Sethuraman, 1994]
Part II / Other views of the DP
54
We have focused on the deﬁnition of the Dirichlet process via the stick-breaking deﬁnition.
Later, we will
continue to use it to deﬁne structured models. But for completeness, we present some other deﬁnitions of the
DP, which are useful for theoretical understanding and developing new algorithms.
Deﬁnition of the Dirichlet process
DP mixture model
β ∼GEM(α)
For each component z ∈{1, 2, . . . }:
φz ∼G0
} G ∼DP(α, G0)
For each data point i ∈{1, . . . , n}:
zi ∼Multinomial(β)
}ψi ∼G
xi ∼F(φzi)
}xi ∼F(ψi)
G = P∞
z=1 βzδφz fully speciﬁes the
parameters.
Write G ∼DP(α, G0) to mean G has
a Dirichlet process prior.
0
1
0
1
φA
φB
0
1
0
1
φA
φB
G0
G
Part II / Other views of the DP
55
We present one deﬁnition of the Dirichlet process based on the stick-breaking construction, building from
the DP mixture model.
The Dirichlet process is a distribution on distributions G, where G is constructed
by combining stick-breaking probabilities and i.i.d. draws from G0.
This allows us to encapsulate both the
component probabilities and parameters into one object G. It turns out this prior over G can be deﬁned in
other ways as we shall see.

Stochastic process deﬁnition
G ∼DP(α, G0)
⇔
Finite partition property:
(G(A1), . . . , G(Am)) ∼Dirichlet(αG0(A1), . . . , αG0(Am))
for all partitions (A1, . . . , Am) of Ω,
where Ωis the set of all possible parameters.
A1
A2
A3
A4
Ω
Example: Ω=
0
1
0
1
φA
φB
(the set of multinomial parameters)
When Ωis ﬁnite, Dirichlet process = Dirichlet distribution.
Signiﬁcance: theoretical properties, compact notation, deﬁning HDPs
Part II / Other views of the DP
56
The stochastic process deﬁnition is an alternative way to deﬁne G. Unlike the stick-breaking construction, the
stochastic process deﬁnition is more declarative, expressing the distribution of G in terms of the distribution
of parts of G. This is typically the way a stochastic process is deﬁned. Note that when Ωis a ﬁnite set, the
Dirichlet process is equivalent to an ordinary Dirichlet distribution. Viewed in this light, the Dirichlet process is
just a generalization of the Dirichlet distribution.
Chinese restaurant process
What is the distribution of ψi | ψ1, . . . , ψi−1 marginalizing out G?
Metaphor: customer = data point, dish = parameter,
table = cluster
A Chinese restaurant has an inﬁnite number of tables.
Customer i:
• joins the table of a previous customer j and shares the dish, or
• starts a new table and randomly picks a new dish from G0
. . .
1
2
3
4
5
probability:
α0
0+α0
α0
1+α0
1
2+α0
α0
3+α0
2
4+α0
In symbols:
ψi | ψ1, . . . , ψi−1 ∼
1
α+i−1
 Pi−1
j=1 δψj + αG0

Rich-gets-richer property: tables with more customers get more.
Signiﬁcance: leads to eﬃcient sampling algorithms.
Part II / Other views of the DP
57
The Chinese restaurant process (CRP) is yet a third way to view the Dirichlet process. This time, we are not
interested in G itself, but rather draws from G with G integrated out. Formally, the CRP is a distribution
over partitions (clusterings) of the data points. The P´olya urn refers to distribution over dishes. An important
property about the CRP is that despite its sequential deﬁnition, the dishes (and tables) are actually exchangeable,
meaning p(ψ1, . . . , ψn) = p(ψπ(1), . . . , ψπ(n)) for any permutation π. This can be directly seen as a consequence
of de Finetti’s theorem.

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part III
58
Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part III / Structured models
59

Building DP-based structured models
DP
single mixture model
HDP
several mixture models
sharing same components
HDP-HMM
each state has a
mixture model over next states
HDP-PCFG
each state has a
mixture model over pairs of states
Part III / Structured models
60
We will use the DP mixture model as a building block in creating more complex models. First, we develop the
HDP mixture model, which allows many mixture models to share the same inventory of components. Based on
this, we then create structured models such as HDP-HMMs and HDP-PCFGs.
Hierarchical DP mixture models
Suppose we have a collection of J mixture models.
Goal: share common inventory of mixture components
β
π1
z1i
x1i
i
n
...
φz
z
∞
...
πJ
zJi
xJi
i
n
• Component parameters {φz} are shared globally
• Each mixture model has individual component
probabilities πj tied via β
Part III / Structured models
[Teh, et al., 2006]
61
The top-level probabilities β determine the overall popularity of components, to a ﬁrst-order approximation,
which components are active.
Each πj is based on β, but is tailored for group j.
An application is topic
modeling, where each group is a document, each component is a topic (distribution over words), and each data
point is a word. In this light, the HDP is an nonparametric extension of LDA. Note: the HDP mixture model
can also be deﬁned using the stochastic process deﬁnition or the Chinese restaurant franchise, the hierarchical
analogue of the CRP.

Hierarchical DP mixture models
β
πj
zji
xji
i
n
j
J
φz
z
∞
β ∼GEM(α)
For each component z ∈{1, 2, . . . }:
φz ∼G0
For each group j ∈{1, . . . , J}:
πj ∼DP(α′, β)
For each data point i ∈{1, . . . , n}:
zji ∼Multinomial(πj)
xji ∼F(φzji)
β determines the rough component probabilities
πj ∼DP(α′, β) makes πj close to β
Part III / Structured models
62
We tie the πjs together using a Dirichlet process, which is best understood in terms of the stochastic process
deﬁnition, where the distributions involved are over the positive integers. Note: there is another deﬁnition of
the HDP where for each group j, we draw a separate set of stick-breaking probabilities from GEM(α′), which
then can be used to induce the corresponding distribution on πj. The disadvantage of this approach is that it
introduces extra variables and indirect pointers which complicates variational inference.
Sharing component probabilities
Draw top-level component probabilities β ∼GEM(α):
α = 2
Draw group-level component probabilities π ∼DP(α′, β):
α′ = 1
α′ = 5
α′ = 20
α′ = 100
As α′ increases, the more π resembles β.
Part III / Structured models
63
There are two concentration parameters, α and α′. The parameter α controls how many components there are
overall and α′ controls how similar the prior distributions over components across groups.

Mean-ﬁeld inference for the HDP
β
πj
zji
xji
i
n
j
J
φz
z
∞
β ∼GEM(α)
For each component z ∈{1, 2, . . . }:
φz ∼G0
For each group j ∈{1, . . . , J}:
πj ∼DP(α′, β)
For each data point i ∈{1, . . . , n}:
zji ∼Multinomial(πj)
xji ∼F(φzji)
Mean-ﬁeld approximation: q(β) Q∞
z=1 q(φz) QJ
j=1 q(πj) Qn
i=1 q(zi)
• As in variational DP, truncate β at level K
• πj ∼DP(α′, β) reduces to ﬁnite K-dimensional
Dirichlet πj ∼Dirichlet(α′, β), so we can use ﬁnite
variational techniques for updating q(π).
• Let q(β) = δβ∗(β) for tractability, optimize with gradient descent
Part III / Structured models
64
The only added challenge in doing mean-ﬁeld inference in the HDP is how to optimize the top-level components.
Because the GEM prior is not conjugate with the DP draws from β, it’s convenient to let q(β) be degenerate
and optimize it using standard gradient methods.
HDP hidden Markov models
β
πz
φz
z
∞
z1
z2
z3
· · ·
x1
x2
x3
· · ·
β ∼GEM(α)
For each state z ∈{1, 2, . . . }:
φz ∼G0
πz ∼DP(α′, β)
For each time step i ∈{1, . . . , n}:
zi+1 ∼Multinomial(πzi)
xi ∼F(φzi)
Each state z is a component and has the following:
• πz: transition parameters
• φz: emission parameters
Key:
β ∼GEM(α) speciﬁes which states will be used (global)
πz ∼DP(α′, β) speciﬁes distribution over next states (per state)
Part III / Structured models
[Beal, et al., 2002; Teh, et al., 2006]
65
We can think of an HMM as a mixture model, where each component corresponds to a state of the HMM.
Given a state, we need to emit an observation and advance to the next state. The component parameters must
specify the distributions associated with these stochastic choices. The transition parameters must specify a
distribution over states, which is naturally represented with a draw from a DP. The HDP framework allows us
to tie all the transition DPs.

Mean-ﬁeld inference for the HDP-HMM
β
πz
φz
z
∞
z1
z2
z3
· · ·
x1
x2
x3
· · ·
β ∼GEM(α)
For each state z ∈{1, 2, . . . }:
φz ∼G0
πz ∼DP(α′, β)
For each time step i ∈{1, . . . , n}:
zi+1 ∼Multinomial(πzi)
xi ∼F(φzi)
Structured mean-ﬁeld approximation: q(β)
 Q∞
z=1 q(φz)q(πz)

q(z)
EM:
E-step: run forward-backward using p(z′ | z, π) = πzz′
M-step: normalize transition counts: πzz′ ∝C(z →z′)
Mean-ﬁeld:
E-step: run forward-backward using multinomial weights W(z, z′)
M-step: compute multinomial weights given transition counts:
W(z, z′) = exp Ψ(α′βz′ + C(z →z′))
exp Ψ(α′ + C(z →∗))
Top-level: optimize q(β) = δβ
∗(β) using gradient descent
Part III / Structured models
66
Mean-ﬁeld inference is similar to EM for classical HMMs.
It is straightforward to check that the forward-
backward dynamic program for the E-step still remains tractable when using a non-degenerate q(φ). Here, K
is the truncation applied to β.
HDP probabilistic context-free grammars
β ∼GEM(α)
For each symbol z ∈{1, 2, . . . }:
φT
z ∼Dirichlet
φE
z ∼G0
φB
z ∼DP(α′, ββT)
For each node i in the parse tree:
ti ∼Multinomial(φT
zi) [rule type]
If ti = Binary-Production:
(zL(i), zR(i)) ∼Multinomial(φB
zi) [children symbols]
If ti = Emission:
xi ∼Multinomial(φE
zi) [terminal symbol]
β
φB
z
φT
z
φE
z
z
∞
z1
z2
x2
z3
x3
HDP-HMM
HDP-PCFG
At each i... transition and emit
produce or emit
DP is over...
1 next state
2 children symbols
Variational inference: modiﬁed inside-outside
Part III / Structured models
[Liang, et al., 2007]
67
The nonparametric Bayesian machinery carries over from HMMs to PCFGs again by replacing normalization
with an application of exp(Ψ(·)). The addition of DPs centered on the cross-product of the top-level distribution
does complicate the optimization of β.

Prediction revisited
Train on x; now get test xnew, want to ﬁnd best znew.
p(znew | xnew) = Ep(θ|x)p(znew | xnew, θ)
≊Eq(θ)p(znew | xnew, θ)
Contrast with training (has log):
q(znew) ∝exp Eq(θ) log p(znew | xnew, θ)
To ﬁnd argmaxznew Eq(θ)p(znew | xnew, θ), consider all znew:
intractable when znew is a parse tree because cannot use
dynamic programming (unlike training)
Approximations:
• Use mode of q(θ)
• Maximize expected log-likelihood as in training
• Reranking: get an n-best list using proxy; choose best one
Part III / Structured models
68
We have spent most of our time approximating posteriors. Now let’s see how we can use them for structured
prediction. When we have point estimates of parameters, the standard prediction rule is the same as running a
hard E-step during training. However, with non-degenerate parameters, the log during training decouples parts
of the structure, allowing dynamic programming. At test time, dynamic programming can no longer be used in
the absence of the log.
Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part III / Survey of related methods
69

Top-down and bottom-up
Model
DP mixture models, HMMs, PCFGs
Inference
algorithm
EM, mean-ﬁeld, sampling
Data
words, sentences, parse trees
priors
smoothing, discounting
Part III / Survey of related methods
70
The top-down (Bayesian) approach: deﬁne a model (such as a DP mixture model) and then approximate
the posterior using an inference algorithm. The bottom-up approach: design an algorithm directly based on
properties of the data. Sometimes the two approaches coincide: for example, smoothing and discounting can
have interpretations as Bayesian priors. Many NLP methods are reasonable and eﬀective but lack a top-level
interpretation, which could provide additional insight.
Why doesn’t my variational DP work?
Modeling issues
• Setting concentration parameters is still an art.
• Is the DP even the right prior to use?
Inference issues
• Mean-ﬁeld is an approximation of the true posterior.
• The coordinate-wise descent algorithm for optimizing
the mean-ﬁeld objective is susceptible to local minima
problems, just as is EM.
Part III / Survey of related methods
71
We have focused on using Dirichlet processes with variational inference for clustering, sequence modeling, etc.
However, there are two places something could go wrong: having bad modeling assumptions or having a local
optima problem with inference.

Other inference methods
We focused on variational inference using the stick-breaking
construction.
Algorithm:
• Variational (mean-ﬁeld, expectation propagation, etc.):
deterministic, fast, converge to local optima
• Sampling (Gibbs, split-merge, etc.): converges to true
posterior (but don’t know when), could be stuck in
local optima for a long time
Representation:
• Stick-breaking construction: simple, concrete
• Chinese restaurant process: works well for simple
mixture models
Part III / Survey of related methods
72
If inference is a problem, one could consider other inference methods. There are two major axes along which
we could classify an inference algorithm: variational/sampling or CRP/stick-breaking, leading to four diﬀerent
algorithms. Each has its advantages, but the problem of posterior inference is fundamentally a challenging one.
The Bayesian elephant
Even just the elephant posterior is intractable.
Mean-ﬁeld guy: “feels like smoothing/discounting”
Sampling guy: “feels like stochastic hill-climbing”
A common property: rich gets richer
Part III / Survey of related methods
73
Viewing a model through one particular inference algorithm leads to a particular way of thinking about the
problem.
It is important to remember that it is not the complete story.
Of course, we are talking about
one model, so there are common properties (e.g., rich gets richer) which are shared by the diﬀerent inference
algorithms.

Other nonparametric priors
• Pitman-Yor process: component probabilities less
sparse compared to the DP; yields power-law
distributions useful for language modeling. [Pitman,
Yor, 1997; Teh, 2006]
• Beta process / Indian buﬀet process: allows data
points to belong to more than one cluster; inﬁnite
number of latent features, each data point generated
using a ﬁnite subset. [Griﬃths, Ghahramani, 2007;
Thibaux, Jordan, 2007]
• etc.
Many of these priors also have stochastic process, Chinese
restaurant, stick-breaking counterparts.
Part III / Survey of related methods
74
If inference is not the problem, perhaps the model is not suitable for the task at hand. For example, Pitman-Yor
priors give power law distributions more suitable for modeling the distribution of words than Dirichlet priors.
Other types of problems
Clustering (unsupervised)
non-Bayesian
Bayesian
parametric
k-means, EM
Bayesian mixture models
nonparametric
agglomerative clustering
Dirichlet processes
Classiﬁcation (supervised)
non-Bayesian
Bayesian
parametric
logistic regression, SVMs
Bayesian logistic regression
nonparametric
nearest neighbors, kernel methods
Gaussian processes
Part III / Survey of related methods
75
We’ve focused primarily on models built from mixtures, which are useful for inducing hidden structure in data.
We have extended classical mixture models along two axes: being more Bayesian and being more nonparametric.
Moving along these two axes is also applicable in other methods, e.g. classiﬁcation, regression, etc.

Roadmap
• Part I
– Distributions and Bayesian principles
– Variational Bayesian inference
– Mean-ﬁeld for mixture models
• Part II
– Emulating DP-like qualities with ﬁnite mixtures
– DP mixture model
– Other views of the DP
• Part III
– Structured models
– Survey of related methods
– Survey of applications
Part III / Survey of applications
76
Bayesian trends in NLP
90
91
92
93
94
95
96
97
98
99
00
01
02
03
04
05
06
year
12
24
36
48
60
number of hits
“Bayesian”
90
91
92
93
94
95
96
97
98
99
00
01
02
03
04
05
06
year
12
24
36
48
60
number of hits
“Dirichlet”
90
91
92
93
94
95
96
97
98
99
00
01
02
03
04
05
06
year
12
24
36
48
60
number of hits
“prior”
90
91
92
93
94
95
96
97
98
99
00
01
02
03
04
05
06
year
12
24
36
48
60
number of hits
“posterior”
Part III / Survey of applications
77
There has been a signiﬁcant increase in the number of papers at ACL about Bayesian modeling, as measured
by the number of Google hits on ACL papers.

Applications
• Topic modeling
– ﬁnite Bayesian model; variational [Blei, et al., 2003]
– HDP-based model; sampling [Teh, et al., 2006]
• Language modeling
– Pitman-Yor ⇒power-law; sampling [Goldwater, et al., 2005]
– Kneser-Ney ⇔Pitman-Yor; sampling [Teh, 2006]
• POS induction using a ﬁnite Bayesian HMM
– Collapsed sampling [Goldwater, Griﬃths, 2007]
– Variational [Johnson, 2007]
• Parsing using nonparametric grammars
– Collapsed sampling [Johnson, et al., 2006]
– Collapsed sampling [Finkel, et al., 2007]
– Variational stick-breaking representation [Liang, et al., 2007]
• Coreference resolution
– Supervised clustering; collapsed sampling [Daume, Marcu, 2005]
– HDP-based model; sampling [Haghighi, Klein, 2007]
Part III / Survey of applications
78
This is a list of some recent work that applies Bayesian and/or nonparametric methods to NLP problems.
Conclusions
• Bayesian methods are about keeping uncertainty in
parameters
• Variational inference: hard EM →EM →mean-ﬁeld
– Represent a distribution over parameters
• The nonparametric Dirichlet process prior penalizes use
of extra clusters
– Inference algorithms have rich-gets-richer property
• Structured models can be built from nonparametric
Bayesian parts
79
This ends our tutorial on Structured Bayesian Nonparametric Models with Variational Inference.

Resources
• References
• Derivations
• Glossary and notation
Resources / References
80
References (DP theory)
1. T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals
of Statistics, 1973. Deﬁnes the Dirichlet process using stochastic processes.
2. D. Blackwell and J. B. MacQueen. Ferguson distributions via P´olya urn
schemes. Annals of Statistics, 1973. Connection between DP and the P´olya
urn.
3. J. Pitman. Combinatorial stochastic processes. UC Berkeley, 2002. A
comprehensive set of tutorial notes.
4. C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian
nonparametric problems. Annals of Statistics, 1974.
5. M. Beal and Z. Ghahramani and C. Rasmussen. The inﬁnite hidden Markov
model. NIPS, 2002. Introduces the (inﬁnite) HDP-HMMs.
6. Y. W. Teh and M. I. Jordan and M. Beal and D. Blei. Hierarchical Dirichlet
processes. JASA, 2006. Sets up the HDP, a mechanism of tying diﬀerent DPs
together (gives another treatment of HDP-HMMs).
Resources / References
81

References (variational inference)
1. D. Blei and A. Ng and M. I. Jordan. Latent Dirichlet allocation. JMLR, 2003.
Uses variational inference for LDA.
2. D. Blei and M. I. Jordan. Variational inference for Dirichlet process mixtures.
Bayesian Analysis, 2005. First mean-ﬁeld algorithm for DP mixtures
(stick-breaking representation).
3. K. Kurihara and M. Welling and N. Vlassis. Accelerated variational Dirichlet
mixture models. NIPS, 2007. Use KD-trees to speed up inference for DP
mixtures (stick-breaking representation).
4. Y. W. Teh and D. Newman and M. Welling. A collapsed variational Bayesian
inference algorithm for Latent Dirichlet Allocation. NIPS, 2007. Variational
inference in the collapsed CRP representation for LDA.
5. K. Kurihara and M. Welling and Y. W. Teh. Collapsed variational Dirichlet
process mixture models. IJCAI, 2007. Compares several variational inference
algorithms.
Resources / References
82
References (Bayesian applications)
1. S. Goldwater and T. Griﬃths. A fully Bayesian approach to unsupervised
part-of-speech tagging. ACL, 2007.
2. M. Johnson. Why doesn’t EM ﬁnd good HMM POS-taggers?.
EMNLP/CoNLL, 2007.
3. K. Kurihara and T. Sato. An application of the variational Bayesian approach
to probabilistic context-free grammars. International Joint Conference on
Natural Language Processing Workshop Beyond Shallow Analyses, 2004.
4. K. Kurihara and T. Sato. Variational Bayesian grammar induction for natural
language. International Colloquium on Grammatical Inference, 2006.
5. M. Johnson and T. Griﬃths and S. Goldwater. Adaptor grammars: a
framework for specifying compositional nonparametric Bayesian models. NIPS,
2006.
6. H. Daume and D. Marcu. Bayesian query-focused summarization.
COLING/ACL, 2006.
Resources / References
83

References (nonparametric applications)
1. M. Johnson and T. Griﬃths and S. Goldwater. Adaptor grammars: a
framework for specifying compositional nonparametric Bayesian models. NIPS,
2006.
2. J. R. Finkel and T. Grenager and C. Manning. The inﬁnite tree. ACL, 2007.
3. P. Liang and S. Petrov and M. I. Jordan and D. Klein. The inﬁnite PCFG using
hierarchical Dirichlet processes. EMNLP/CoNLL, 2007.
4. S. Goldwater and T. Griﬃths and M. Johnson. Interpolating between types and
tokens by estimating power-law generators. NIPS, 2005.
5. Y. W. Teh. A hierarchical Bayesian language model based on Pitman-Yor
processes. COLING/ACL, 2006.
6. H. Daume and D. Marcu. A Bayesian model for supervised clustering with the
Dirichlet process prior. JMLR, 2005.
7. S. Goldwater and T. Griﬃths and M. Johnson. Contextual dependencies in
unsupervised word segmentation. COLING/ACL, 2006.
8. A. Haghighi and D. Klein. Unsupervised coreference resolution in a
nonparametric Bayesian model. ACL, 2007.
Resources / References
84
Resources
• References
• Derivations
• Glossary and notation
Resources / Derivations
85

KL-divergence and normalization
KL(q||p)
def
= Eq log q(θ)
p(θ)
Usually, we only know p up to normalization:
Example: p = p(θ | x), ˜p = p(θ, x), Z = p(x)
p = ˜p
Z
(unnormalized distribution: tractable)
(normalization constant: intractable)
No problem, due to the following identity:
KL(q||p) = KL(q||˜p) + log Z
• argminq KL(q||p) = argminq KL(q||˜p)
• Since KL ≥0, we get a lower bound on log Z as a bonus:
log Z
| {z }
intractable
≥−KL(q||˜p)
|
{z
}
tractable
Resources / Derivations
86
Note that the optimization problem we wish to solve is to minimize KL(q(θ, z)||p(θ, z | x), but unfortunately
this quantity is in terms of the posterior we are trying to compute! Fortunately, to optimize this objective, it
suﬃces to know the posterior up to a normalization constant, as we demonstrate here.
KL-divergence and normalization
KL(q||p)
(1)
= Eq[log q(θ)
p(θ)]
(2)
= Eq[log q(θ) −log p(θ)]
(3)
= Eq[log q(θ)] −Eq[log p(θ)]
(4)
= −H(q) −Eq[log p(θ)]
(5)
= −H(q) −Eq[log ˜p(θ)
Z ]
(6)
= −H(q) −Eq[log ˜p(θ)]
|
{z
}
free energy
+log Z
(7)
= KL(q||˜p) + log Z
(8)
Resources / Derivations
87
We relate KL-divergence with a normalized second argument to the case where the second argument is not
normalized.

Entropy
Entropy measures the amount of uncertainty in a distribution.
Deﬁnition: H(q)
def
= −Eq log q(θ) = −
R
q(θ) log q(θ)dθ
Property: H(q) ≥0
Suppose q is fully-factorized (as in mean-ﬁeld): q(θ) = Qn
i=1 qi(θi)
Then the entropy decomposes:
H(q)
(1)
= −Eq[log Qn
i=1 qi(θi)]
(2)
= Pn
i=1 −Eq[log qi(θi)]
(3)
= Pn
i=1 −Eqi[log qi(θi)]
(4)
= Pn
i=1 H(qi)
(5)
Resources / Derivations
88
Entropy is a measure of uncertainty. An important property about the entropy of distributions that factorize is
that the entropy also decomposes.
Formal derivation of mean-ﬁeld updates
Goal: optimize each qi holding q−i ﬁxed.
Strategy: manipulate KL(q||p), throwing away terms that do not
depend on qi.
KL(q||p)
(1)
= −H(q) −Eq[log p(θ)]
(2)
=
 X
j
−H(qj)

−Eq[log p(θ)]
(3)
= −H(qi) −Eq[log p(θ)] + C
(4)
= −H(qi) −Eqi[Eq−i log p(θ)] + C
(5)
= −H(qi) −Eqi[log(exp Eq−i log p(θ))] + C
(6)
= KL(qi||exp Eq−i log p(θ)) + C
(7)
Recall that KL is minimized when the two arguments are equal.
Conclusion: argminqi KL(q||p) ∝exp{Eq−i log p(θ)}.
Resources / Derivations
89
We derive the formal generic update for optimizing one coordinate in the mean-ﬁeld approximating distribution,
holding all else ﬁxed.

Derivation of exp Ψ
Goal: show W(z)
def
= exp{EDirichlet(φ;α) log φz} =
exp{Ψ(αz)}
exp{Ψ(P
z′ αz′}
Write the Dirichlet distribution in exponential family form:
p(φ | α) = exp
n X
z
αzlog φz −
hX
z
log Γ(αz) −log Γ
 X
z
αz
io
Suﬃcient statistics: log φz
Log-partition function: P
z log Γ(αz) −log Γ
 P
z αz

Fact: mean of suﬃcient statistics = derivative of log-partition function
Deﬁnition: the digamma function is Ψ(x) = ∂log Γ(x)
∂x
E log φz = ∂A(α)
∂αz
= Ψ(αz) −
X
z′
Ψ(αz′)
Exponentiating both sides:
exp{E log φz} =
exp{Ψ(αz)}
exp{Ψ(P
z′ αz′}
Resources / Derivations
90
Resources
• References
• Derivations
• Glossary and notation
Resources / Glossary and notation
91

Glossary
Bayesian inference: A methodology whereby a prior over
parameters is combined with the likelihood of observed data to
produce a posterior using the laws of probability.
Chinese restaurant process: The distribution of the clustering
induced by draws from the Dirichlet process (marginalizing out
component probabilities and parameters).
Conjugacy: Two distributions are conjugate (e.g., Dirichlet and
multinomial).
Dirichlet distribution: A distribution over (parameters of) ﬁnite
probability distributions.
Dirichlet process: A distribution over arbitrary distributions
(generalizes the Dirichlet distribution).
Resources / Glossary and notation
92
Glossary
Likelihood: The probability of observing the data.
Marginalization: Integrating or summing out unobserved quantities
in a model.
Marginal likelihood: The probability of the observed data,
marginalizing out unknown parameters. This quantity is lower
bounded in variational inference.
Markov Chain Monte Carlo: A randomized approximate inference
algorithm with nice asymptotic properties.
Mean-ﬁeld: A fully-factorized approximation for variational
inference.
Resources / Glossary and notation
93

Glossary
Nonparametric: Refers to models where the number of eﬀective
parameters can grow with the amount of data. The Dirichlet
process is an example of a nonparametric prior.
Posterior: The distribution over unknown quantities in a model
(parameters) conditioned on the observed data.
Prior: A distribution over unknown quantities in the model
(parameters) before observing data.
Stick-breaking representation: A constructive deﬁnition of the
Dirichlet process prior.
Variational Bayes: Variational inference for computing the
posterior in Bayesian models.
Resources / Glossary and notation
94
Notation
θ
All parameters (β, φ)
z
All hidden variables
βz Probability of component z
vz Stick-breaking proportion of component z
φz Parameters of component z
zi Component that point i is assigned to
xi Data point i
α0 Concentration parameter of the Dirichlet process prior
G0 Base distribution parameter of the Dirichlet process prior
G A draw from the Dirichlet process
ψi Component parameters used to generate point i
Resources / Glossary and notation
95

