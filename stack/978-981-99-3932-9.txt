Lecture Notes in Networks and Systems 728
Siba K. Udgata
Srinivas Sethi
Xiao-Zhi Gao   Editors
Intelligent 
Systems
Proceedings of 3rd International 
Conference on Machine Learning, IoT 
and Big Data (ICMIB 2023)

Lecture Notes in Networks and Systems 
Volume 728 
Series Editor 
Janusz Kacprzyk 
, Systems Research Institute, Polish Academy of Sciences, 
Warsaw, Poland 
Advisory Editors 
Fernando Gomide, Department of Computer Engineering and Automation—DCA, 
School of Electrical and Computer Engineering—FEEC, University of Campinas— 
UNICAMP, São Paulo, Brazil 
Okyay Kaynak, Department of Electrical and Electronic Engineering, 
Bogazici University, Istanbul, Türkiye 
Derong Liu, Department of Electrical and Computer Engineering, University 
of Illinois at Chicago, Chicago, USA 
Institute of Automation, Chinese Academy of Sciences, Beijing, China 
Witold Pedrycz, Department of Electrical and Computer Engineering, University of 
Alberta, Alberta, Canada 
Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland 
Marios M. Polycarpou, Department of Electrical and Computer Engineering, 
KIOS Research Center for Intelligent Systems and Networks, University of Cyprus, 
Nicosia, Cyprus 
Imre J. Rudas, Óbuda University, Budapest, Hungary 
Jun Wang, Department of Computer Science, City University of Hong Kong, 
Kowloon, Hong Kong

The series “Lecture Notes in Networks and Systems” publishes the latest 
developments in Networks and Systems—quickly, informally and with high quality. 
Original research reported in proceedings and post-proceedings represents the core 
of LNNS. 
Volumes published in LNNS embrace all aspects and subﬁelds of, as well as new 
challenges in, Networks and Systems. 
The series contains proceedings and edited volumes in systems and networks, 
spanning the areas of Cyber-Physical Systems, Autonomous Systems, Sensor 
Networks, Control Systems, Energy Systems, Automotive Systems, Biological 
Systems, Vehicular Networking and Connected Vehicles, Aerospace Systems, 
Automation, Manufacturing, Smart Grids, Nonlinear Systems, Power Systems, 
Robotics, Social Systems, Economic Systems and other. Of particular value to both 
the contributors and the readership are the short publication timeframe and 
the world-wide distribution and exposure which enable both a wide and rapid 
dissemination of research output. 
The series covers the theory, applications, and perspectives on the state of the art 
and future developments relevant to systems and networks, decision making, control, 
complex processes and related areas, as embedded in the ﬁelds of interdisciplinary 
and applied sciences, engineering, computer science, physics, economics, social, and 
life sciences, as well as the paradigms and methodologies behind them. 
Indexed by SCOPUS, INSPEC, WTI Frankfurt eG, zbMATH, SCImago. 
All books published in the series are submitted for consideration in Web of Science. 
For proposals from Asia please contact Aninda Bose (aninda.bose@springer.com).

Siba K. Udgata · Srinivas Sethi · Xiao-Zhi Gao 
Editors 
Intelligent Systems 
Proceedings of 3rd International Conference 
on Machine Learning, IoT and Big Data 
(ICMIB 2023)

Editors 
Siba K. Udgata 
School of Computer and Information 
Sciences 
University of Hyderabad 
Hyderabad, Telangana, India 
Xiao-Zhi Gao 
Department of Computer Science 
University of Eastern Finland 
Kuopio, Finland 
Srinivas Sethi 
Department of Computer Science 
and Engineering 
Indira Gandhi Institute of Technology 
Dhenkanal, Odisha, India 
ISSN 2367-3370
ISSN 2367-3389 (electronic) 
Lecture Notes in Networks and Systems 
ISBN 978-981-99-3931-2
ISBN 978-981-99-3932-9 (eBook) 
https://doi.org/10.1007/978-981-99-3932-9 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Singapore Pte Ltd. 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. 
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, 
Singapore

Preface 
This Springer LNNS volume contains the papers presented at the 3rd International 
Conference on Machine Learning, Internet of Things and Big Data (ICMIB-2023) 
held during March 10 to 12, 2023, organized by Indira Gandhi Institute of Technology 
(IGIT), Sarang, Odisha, India. A lot of challenges at us and no words of appreciation 
are enough for the organizing committee who could still pull it off successfully. 
The conference draws some excellent technical keynote talks and papers. Two 
tutorial talks by Prof. Deepak Tosh (Senior Member, IEEE), University of Texas at 
El Paso, and Prof. Sanjay Kumar Panda, National Institute of Technology, Warangal, 
and an Innovative Project Showcase are planned on March 10, 2023. The over-
whelming response for the tutorial talks is worth mentioning. Apart from the tutorial 
sessions, seven keynote talks by Prof. Gheorghita (George) Ghinea, Professor in 
Mulsemedia Computing, Department of Computer Science, Brunel University, UK, 
Dr. Padmanabh Kumar, Senior Researcher, EBTIC (a research laboratory of British 
Telecom), Prof. Amit Mishra (University of Cape Town, South Africa), Prof.(Dr.) 
Saroj K. Meher, Systems Science and Informatics Unit, Indian Statistical Insti-
tute, Bangalore, Dr. Haesik Kim, Head/Coordinator of 5G-HEART, VTT Technical 
Research Centre, Finland, Prof. Xiao-Zhi Gao, University of Eastern Finland, and 
Dr Bijaya Kumar Sahu, Regional Manager and Head, NRDC-Intellectual Property 
Facilitation Centre and Technology Innovation Centre are delivered. We are grateful 
to all the speakers for accepting our invitation and sparing their time to deliver the 
talks. 
We received 209 full paper submissions, and we accepted only 53 papers with 
an acceptance rate of 25%, which is considered very good in any standard. The 
contributing authors are from different parts of the globe that includes UAE, Nepal, 
Sweden, UK, Turkey, Norway, Bangladesh, Czech Republic and India. The confer-
ence also received papers from distinguished authors from the length and breadth 
of the country including 12 states and many premier institutes. All the papers are 
reviewed by at least three independent reviewers and in some cases by as many as 
ﬁve reviewers. All the papers are also checked for plagiarism and similarity score. 
It was really a tough job for us to select the best papers out of so many good papers 
for presentation in the conference. We had to do this unpleasant task, keeping the
v

vi
Preface
Springer guidelines and approval conditions in view. We take this opportunity to thank 
all the authors for their excellent work and contributions and also the reviewers who 
have done an excellent job. 
On behalf of the technical committee, we are indebted to Prof. L. M. Patnaik, 
General Chair of the Conference, for his timely and valuable advice. We cannot 
imagine the conference without his active support at all the crossroads of decision-
making process. The management of the host institute, particularly Director Prof. 
Satyabrata Mohanta, Prof. (Mrs.) Sasmita Mishra, HOD, CSEA, Program Co-Chair 
and Convenor Prof. Srinivas Sethi, and Organizing Chair Prof. S. N. Mishra have 
extended all possible support for the smooth conduct of the conference. Our sincere 
thanks to all of them. 
We would also like to place on record our thanks to all the keynote speakers, 
tutorial speakers, reviewers, session chairs, authors, technical program committee 
members, various chairs to handle ﬁnance, accommodation, and publicity, and above 
all to several volunteers. 
Our sincere thanks to all press, print, and electronic media for their excellent 
coverage of this conference. 
We are also thankful to Springer Nature publication house for agreeing to publish 
the accepted papers in their Lecture Notes in Networks and Systems (LNNS) series. 
Please take care of yourself, your loved ones, and stay safe. 
March 2023
Siba Kumar Udgata 
Srinivas Sethi 
Xiao-Zhi Gao

Organization 
Reviewers List 
Abhinav Tomar 
Alok Tripathy 
Anisha Kumari 
Anitha A. 
Ashima Rout 
Atish Nanda 
Bibhudatta Sahoo 
Bibudhendu Pati 
Bichitra Mandal 
Bunil Balabantaray 
Chinmayee Rout 
Debabrata Dansana 
Debasis Mohapatra 
Deepak Singh 
Devesh Bandil 
Sudhansu Patra 
Jui Pattnaik 
Gopal Behera 
J. Chandrkanta Badajena 
Jitendra Kumar 
Jitendra Rout 
Kaibalya Panda 
Kali Rath 
kallam suresh 
Kalyan Kumar Jena 
Kauser Ahmed 
Kshira Sahoo 
Lakhmi Das
vii

viii
Organization
Lalatendu Muduli 
Lalitha K. 
Manmath Bhuyan 
Manoj Das 
Manoj Kumar Patra 
Niranjan Panigrahi 
Nitin Bommi 
Om Prakash Jena 
Prahallad Sahu 
Prajna Nanda 
Prasant Dhal 
Pratap Sekhar 
Preeti Chandrakar 
Pritam Raul 
Swarup Roy 
Punyaban Patel 
Purna Sethi 
Pushkar Kishore 
Rabindra Behera 
Rajendra Nayak 
Rajiv Senapati 
Rakesh Chandra Balabantaray 
Ramesh Sahoo 
Rohit Kumar Bondugula 
S. Gopal Krishna Patro 
Sambit Mishra 
Sampa Sahoo 
Sandhya Sahoo 
Sangharatna Godboley 
Sangita Pal 
Sanjaya Panda 
Sanjib Nayak 
Santanu Dash 
Saroj Panigrahy 
Sasmita Acharya 
Siba Udgata 
Sohan Pande 
Sonali Jena 
Sourav Bhoi 
Srichandan Sobhanayak 
Srinivas Sethi 
Subasish Mohapatra 
Subhasish Pani 
Suman Paul 
Sumit Kar

Organization
ix
Sunita Dhalbisoi 
Suraj Sharma 
Surya Das 
Suvendra Jayasingh 
Swarupananda Bissoyi 
Tirimula Rao Benala 
Umakanta Samantsinghar 
Umesh Sahu 
V. Ramanjaneyulu Yannam 
Committee Members 
Patron 
Satyabrata Mohanto (Director) IGIT Sarang 
General Chair 
Lalit Mohan Patnaik National Institute of Advanced Studies and Indian Institute of 
Science, Bangalore 
Program Chair 
Siba K. Udgata University of Hyderabad, India 
Program Co-chairs 
Srinivas Sethi 
IGIT Sarang 
Xiao-Zhi Gao University of Eastern Finland, Finland 
Organizing Chairs 
S. N. Mishra
IGIT Sarang 
Sasmita Mishra IGIT Sarang

x
Organization
Convenors 
Srinivas Sethi
IGIT Sarang 
Sanjaya Kumar Patra IGIT Sarang 
Publicity Chairs 
Sourav Roy
Sikim University 
B. P. Panigrahi 
IGIT Sarang 
Ashima Rout
IGIT Sarang 
S. K. Tripathy
IGIT Sarang 
Subhrashu Das GCE, Keunjhar 
Hospitality Chairs 
Biswanath Sethi
IGIT Sarang 
Anshuman Padhy
IGIT Sarang 
P. R. Dhal
IGIT Sarang 
Sujit Kumar Pradhan IGIT Sarang 
Finance Chairs 
Ashima Rout
IGIT Sarang 
Sanjaya Kumar Patra IGIT Sarang 
Logistic Chairs 
Dillip Kumar Swain IGIT Sarang 
Medimi Srinivas
IGIT Sarang 
Priyabrat Sahoo
IGIT Sarang 
Niroj Kumar Pani
IGIT Sarang 
Rabi Nayan Sethi
IGIT Sarang 
Anand Gupta
IGIT Sarang

Organization
xi
Accommodation Chairs 
Rabindra Kumar Behera IGIT Sarang 
Monoj K. Choudhury
IGIT Sarang 
Manoj Kumar Muni
IGIT Sarang 
K. D. Sa
IGIT Sarang 
Supriya Sahoo
IGIT Sarang 
Organizing Committee 
Jogendra Mahi
IGIT Sarang 
Sandeep Sahoo
IGIT Sarang 
Kashinath Barik
IGIT Sarang 
Rabinarayan Murmu
IGIT Sarang 
July Randhari
IGIT Sarang 
Ritesh Patel
IGIT Sarang 
S. R. Pradhan
IGIT Sarang 
Gaurab Ghose
IGIT Sarang 
Deepak Suna
IGIT Sarang 
Ashok Pradhan
IGIT Sarang 
Himanshu Dash
IGIT Sarang 
Sangita Pal
IGIT Sarang 
Sangram Nayak
IGIT Sarang 
Sushant Kumar Sahoo
IGIT Sarang 
Anupama Sahoo
IGIT Sarang 
Subhendu Bhusan Rout IGIT Sarang 
Suvendu Kumar Jena
IGIT Sarang 
Ramesh Kumr Sahoo
IGIT Sarang 
Binaya Kumar Patra
IGIT Sarang 
Supriya Lenka
IGIT Sarang 
Bapuji Rao
IGIT Sarang 
Technical Program Committee 
Manas Ranjan Patra
Berhampur University 
Siba Kumar Udgata
University of Hyderabad 
O. B. V. Ramanaiah
OBV, JNTU Hyderabad 
G. Suvarna Kumar
MVGRCE, Vijayanagaram 
G. Sandhya
MVGRCE, Vijayanagaram 
R. Hemalatha
University College of Engineering, Osmania University

xii
Organization
Amit Kumar Mishra
University of Cape Town, South Africa 
R. Thangarajan
Kongu Engineering College, Tamil Nadu 
Birendra Biswal
Gayatri 
Vidya 
Parishad 
College 
of 
Engineering, 
Vishakhapatnam 
Somanath Tripathy
IIT Patna 
A. K. Turuk
NIT Rourkela 
B. D. Sahoo
NIT Rourkela 
D. P. Mohapatra
NIT Rourkela 
P. M. Khilar
NIT Rourkela 
P. G. Sapna
CIT, Coimbatore 
R. K. Dash
NIC, Mizoram 
B. K. Tripathy
VIT, Vellore 
Moumita Patra
IIT, Guahati 
S. N. Das
GIET University, Gunupur, Odisha 
Ram Kumar Dhurkari
IIM, Sirmaur, Himachal Pradesh 
Chitta Ranja Hota
BITS Pilani, Hyderabad 
A. Kavitha
JNTU, Hyderabad 
Subasish Mohapatra
CET Bhubaneswar 
Sanjaya Kumar Panda
NIT, Warangal 
S. Mini
NIT, Goa 
S. Nagender Kumar
University of Hyderabad 
Lalit Garg
University of Malta, Malta 
Lalitha Krishna
Kongu Engineering College, Tamil Nadu 
C. Poongodi
Kongu Engineering College, Tamil Nadu 
Sumanth Yenduri
Kennesaw University, USA 
Shaik Shakeel Ahamad
Majmaah University, Saudi Arabia 
K. Srujan Raju
CMR Technical Campus, Hyderabad 
R. Hemalatha
University College of Engineering, Osmania University, 
Hyderabad 
P. Sakthivel
Anna University 
Pavan Kumar Mishra
NIT, Raipur 
Tapan Kumar Gandhi
Department of Electrical Engineering, IIT, Delhi 
Annappa B.
NIT Surathkal 
Prafulla Kumar Behera
Utkal University, Bhubaneswar, Odisa 
Nekuri Naveen
School 
of 
Computer 
and 
Information 
Sciences, 
University of Hyderabad 
Ch. Venkaiah
School 
of 
Computer 
and 
Information 
Sciences, 
University of Hyderabad 
Rajendra Lal
School 
of 
Computer 
and 
Information 
Sciences, 
University of Hyderabad 
Dillip Singh Sisodia
Department of Computer Science and Engineering, NIT, 
Raipur 
Pradeep Singh
Department of Computer Science and Engineering, NIT, 
Raipur 
Jay Bagga
Ball State University, USA

Organization
xiii
Sumagna Patnaik
JB Institute of Engineering and Technology, Hyderabad 
Ajit K. Sahoo
University of Hyderabad 
Atluri Rahul
Neurolus Systems, Hyderabad 
Samrat L. Sabat
Center for Advanced Studies in Electronic Science and 
Technology (CASEST), University of Hyderabad 
Nihar Satapathy
Sambalpur University 
Susil Kumar Mohanty
Department of Computer Science and Engineering, IIT, 
Patna 
Kagita Venkat
NIT, Warangal 
Sanjay Kuanar
GIET University, Gunupur, Orissa 
Bhabendra Biswal
College of Engineering, Bhubaneswar 
Padmalaya Nayak
GR Institute of Engineering and Technology, Hyderabad 
Bhibudendu Pati
R.D Womens University, Bhubaneswar 
Chabi Rani Panigrahi
R.D Womens University, Bhubaneswar 
Rajesh Verma
Infosys Ltd, Hyderabad 
Arun Avinash Chauhan
School 
of 
Computer 
and 
Information 
Sciences, 
University of Hyderabad 
Khusbu Pahwa
Delhi Technological University, New Delhi 
Soumen Roy
DRDL, Hyderabad 
Satyajit Acharya
Tech Mahindra, Hyderabad 
Subhrakanta Panda
BITS Pilani, Hyderabad 
Vineet P. Nair
School 
of 
Computer 
and 
Information 
Sciences, 
University of Hyderabad 
Subash Yadav
Department of Computer Science, Central University of 
Jharkhand, Ranchi 
Layak Ali
Central University Karnataka, Gulbarga 
Deepak Kumar
NIT, Meghalaya 
Bunil Balabantaray
NIT, Meghalaya 
Sumanta pyne
NIT Rourkela 
Asis Tripathy
VIT, Vellore 
Mousumi Saha
NIT, Durgapur 
Abhijit Sharma
NIT, Durgapur 
Mayukh Sarkar
MNNIT, Allahabad 
Oishila Bandyopadhyay
IIIT, Kalayani 
Subrat Kumar Mohanty
IIIT, Bhubaneswar 
Ramesh Chandra Mishra IIIT Manipur 
Hirak Maity
Kolaghat Engineering College 
Sandeep Kumar Panda
ICFAI, Hyderabad 
Prasanta Kumar
Swain NOU, Odisha 
Ashim Rout
IGIT Sarang 
Srinivas Sethi
IGIT Sarang 
S. N. Mishra
IGIT Sarang 
Urmila Bhanja
IGIT Sarang 
D. J. Mishra
IGIT Sarang 
S. Mishra
IGIT Sarang

xiv
Organization
Sangita Pal
IGIT Sarang 
Sanjaya Patra
IGIT Sarang 
Biswanath Sethi
IGIT Sarang 
Niroj Pani
IGIT Sarang 
Dillip Kumar Swain
IGIT Sarang 
Pranati Dash
IGIT Sarang 
B. P. Panigrahy
IGIT Sarang 
Rabindra Behera
IGIT Sarang 
L. N. Tripathy
CET Bhubaneswar 
B. B. Choudhary
IGIT Sarang 
Dhiren Behera
IGIT Sarang 
R. N. Sethi
IGIT Sarang 
Anand Gupta
IGIT Sarang 
Ayaskanta Swain
NIT Rourkela 
Gayadhr Panda
NIT, Meghalaya 
S. K. Tripathy
IGIT Sarang 
B. B. Panda
IGIT Sarang 
Md. N. Khan
IGIT Sarang 
Anukul Padhi
IGIT Sarang 
Debakanta Tripathy
IGIT Sarang 
S. K. Maity
IGIT Sarang 
Devi Acharya
VIT, Vellore 
Sanjaya Kumar
PRSU, Raipur, India 
V. Patle
PRSU, Raipur, India 
Saurov Bhoi
PMEC Berhampur 
Kalyan Kumar Jena
PMEC Berhampur 
Alok Ranjan Prusty
Skill Development, Delhi 
Babita Majhi
GGU, Chatisgarh 
Subhrashu Das
GCE, Keunjhar 
Puspalata Pujahari
GGU, Chatisgarh 
Tirimula Rao
JNTU Kakinada 
Kshirsagar Sahoo
Umea University, Sweden 
Maheswar Behera
IGIT Sarang 
Niranjan Panigrahy
PMEC Berhampur 
Trilochon Rout
PMEC Berhampur 
P. K. Panigrahy
GIET, India 
Mihir Kumar Sutar
UCE Burla 
Sukanta Besoi
CVRCE, Bhubaneswar 
Gopal Behera
GCEK, Bhawanipatna 
Rajendra Prasad Nayak
GCEK, Bhawanipatna 
Kaliprasan Sethi
GCEK, Bhawanipatna 
Jitendra Kumar Rout
NIT, Raipur

Contents 
Effect of the Longitudinal Strain of PM Fiber on the Signal Group 
Velocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 
Karel Slavicek, David Grenar, Jiri Vavra, Martin Kyselak, Jan Radil, 
and Jakub Frolka 
Machine Learning Algorithms Aided Disease Diagnosis 
and Prediction of Grape Leaf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11 
Priyanka Kaushik 
Optimized Fuzzy PI Regulator for Frequency Regulation 
of Distributed Power System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23 
Smrutiranjan Nayak, Subhransu Sekhar Dash, Sanjeeb Kumar Kar, 
Ananta Kumar Sahoo, and Ashwin Kumar Sahoo 
Detecting Depression Using Quality-of-Life Attributes 
with Machine Learning Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29 
J. Premalatha, S. Aswin, D. JaiHari, and K. Karamchand Subash 
Patient Satisfaction Through Interpretable Machine Learning 
Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39 
S. Anandamurugan, P. Jayaprakash, S. Mounika, and R. Narendranath 
Predicting the Thyroid Disease Using Machine Learning 
Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49 
Lalitha Krishnasamy, M. Aparnaa, G. Deepa Prabha, and T. Kavya 
An Automatic Trafﬁc Sign Recognition and Classiﬁcation Model 
Using Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59 
Rajalaxmi Padhy, Alisha Samal, Sanjit Kumar Dash, and Jibitesh Mishra 
An Artiﬁcial Intelligence Enabled Model to Minimize Corona 
Virus Variant Infection Spreading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73 
Dipti Dash, Isham Panigrahi, and Prasant Kumar Pattnaik
xv

xvi
Contents
SoundMind: A Machine Learning and Web-Based Application 
for Depression Detection and Cure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87 
Madhusha Shete, Chaitaya Sardey, and Siddharth Bhorge 
Japanese Encephalitis Symptom Prediction Using Machine 
Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99 
Piyush Ranjan, Sushruta Mishra, Tridiv Swain, and Kshira Sagar Sahoo 
Smart Skin-Proto: A Mobile Skin Disorders Recognizer Model . . . . . . . .
113 
Sushruta Mishra, Shubham Suman, Aritra Nandi, 
Smaraki Bhaktisudha, and Kshira Sagar Sahoo 
Machine Learning Approach Using Artiﬁcial Neural Networks 
to Detect Malicious Nodes in IoT Networks . . . . . . . . . . . . . . . . . . . . . . . . . .
123 
Kazi Kutubuddin Sayyad Liyakat 
Real Time Air-Writing and Recognition of Tamil Alphabets Using 
Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135 
S. Preethi, T. Meeradevi, K. Mohammed Kaif, S. Hema, and M. Monikraj 
A Fuzzy Logic Based Trust Evaluation Model for IoT . . . . . . . . . . . . . . . . .
147 
Rabindra Patel and Sasmita Acharya 
Supervised Learning Approaches on the Prediction of Diabetic 
Disease in Healthcare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157 
Riyam Patel, Borra Sivaiah, Punyaban Patel, and Bibhudatta Sahoo 
Solar Powered Smart Home Automation and Smart Health 
Monitoring with IoT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169 
Atif Afroz, Sephali Shradha Khamari, and Ranjan Kumar Behera 
Seasonal-Wise Occupational Accident Analysis Using Deep 
Learning Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183 
N. Nandhini and A. Anitha 
MLFP: Machine Learning Approaches for Flood Prediction 
in Odisha State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195 
Subasish Mohapatra, Kunaram Tudu, Amlan Sahoo, 
Subhadarshini Mohanty, and Chandan Marandi 
Vision-Based Cyclist Travel Lane and Helmet Detection . . . . . . . . . . . . . . .
207 
Jyoti Madake, Shripad Bhatlawande, and Madhusha Shete 
Design and Experimental Analysis of Spur Gear–A Multi-objective 
Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221 
S. Panda and Jawaz Alam 
Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using 
Various Feature Extraction Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231 
Sareeta Mohanty and Manas Ranjan Senapati

Contents
xvii
Computer Vision and Image Segmentation: LBW Automation 
Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249 
Jeebanjyoti Nayak, Jyotsnarani Jena, Hrushikesh Pradhan, 
Jyotiprakash Das, and Surendra N. Bhagat 
A Mixed Collaborative Recommender System Using Singular 
Value Decomposition and Item Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259 
Gopal Behera, Ramesh Kumar Mohapatra, and Ashok Kumar Bhoi 
Hybrid Clustering-Based Fast Support Vector Machine Model 
for Heart Disease Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269 
Chaitanya Datta Maddukuri and Rajiv Senapati 
Forecasting and Analysing Time Series Data Using Deep Learning . . . . .
279 
Snigdha Sen, V. T. Rajashekar, and N. Dharshan 
Intelligent Blockchain: Use of Blockchain and Machine Learning 
Algorithm for Smart Contract and Smart Bidding . . . . . . . . . . . . . . . . . . . .
293 
Jyotiranjan Rout, Susmita Pani, Sibashis Mishra, Bhagyashree Panda, 
Satya Swaroop Kar, and Sanjay Paramanik 
Weed Detection in Cotton Production Systems Using Novel 
YOLOv7-X Object Detector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303 
G. V. S. Narayana, Sanjay K. Kuanar, and Punyaban Patel 
Smart Healthcare System Management Using IoT and Machine 
Learning Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315 
P. Sudam Sekhar, Gunamani Jena, Shubhashish Jena, 
and Subhashree Jena 
Automatic Code Clone Detection Technique Using SDG . . . . . . . . . . . . . . .
327 
Akash Bhattacharyya, Jagannath Singh, and Tushar Ranjan Sahoo 
Simulated Design of an Autonomous Multi-terrain Modular 
Agri-bot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
339 
Safwan Ahmad, Shamim Forhad, Mahmudul Hasan Shuvo, 
Sadman Saifee, Md Shahadat Hossen, Kazi Naimul Islam Nabeen, 
and Mahbubul Haq Bhuiyan 
Customer Segmentation Analysis Using Clustering Algorithms . . . . . . . .
353 
Biyyapu Sri Vardhan Reddy, C. A. Rishikeshan, 
VishnuVardhan Dagumati, Ashwani Prasad, and Bhavya Singh 
SP: Shell-Based Perturbation Approach to Localize Principal 
Eigen Vector of a Network Adjacency Matrix . . . . . . . . . . . . . . . . . . . . . . . .
369 
Baishnobi Dash and Debasis Mohapatra 
Development of a Robust Dataset for Printed Tamil Character 
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381 
M. Arun, S. Arivazhagan, and R. Ahila Priyadharshini

xviii
Contents
An Efﬁcient CNN-based Method for Classiﬁcation of Red Meat 
Based on its Freshness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393 
Abhishek Bajpai, Harshvardhan Rai, and Naveen Tiwari 
Multi-class Pathogenic Microbes Classiﬁcation by Stochastic 
Gradient Descent and Discriminative Fine-Tuning on Different 
CNN Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407 
Nirajan Jha, Dibakar Raj Pant, Jukka Heikkonen, and Rajeev Kanth 
Early Prediction of Thoracic Diseases Using Rough Set Theory 
and Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
421 
Radhanath Hota, Sachikanta Dash, Sujogya Mishra, P. K. Pattnaik, 
and Sipali Pradhan 
Predicting Liver Disease from MRI with Machine Learning-Based 
Feature Extraction and Classiﬁcation Algorithms
. . . . . . . . . . . . . . . . . . . .
435 
Snehal V. Laddha, Manish Yadav, Dhaval Dube, Mahansa Dhone, 
Madhav Sharma, and Rohini S. Ochawar 
An Improved Genetic Algorithm Based on Chi-Square Crossover 
for Text Categorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
445 
Gyananjaya Tripathy and Aakanksha Sharaff 
Tuna Optimization Algorithm-Based Data Placement 
and Scheduling in Edge Computing Environments . . . . . . . . . . . . . . . . . . . .
457 
P. Jayalakshmi and S. S. Subashka Ramesh 
Frequency Control of Single Area Hybrid Power System with DG . . . . . .
471 
Ashutosh Biswal, Prakash Dwivedi, and Sourav Bose 
Prediction of Heart Disease and Heart Failure Using Ensemble 
Machine Learning Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
481 
Abdullah Al Maruf, Aditi Golder, Abdullah Al Numan, 
Md. Mahmudul Haque, and Zeyar Aung 
Veriﬁable Secret Image Sharing with Cheater Identiﬁcation . . . . . . . . . . .
493 
Franco Debashis Ekka, Sourabh Debnath, Jitendra Kumar, 
and Ramesh Kumar Mohapatra 
An ECC-Based Lightweight CPABE Scheme with Attribute 
Revocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
505 
Avinash Chandel, Sourabh Debnath, Jitendra Kumar, 
and Ramesh Kumar Mohapatra 
Prediction of Schizophrenia in Patients Using Fuzzy AHP 
and TOPSIS Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
517 
R. Anoop, Impana Anand, Mohammed Rehan, R. Yashvanth, 
Ashwini Kodipalli, Trupthi Rao, and Shoaib Kamal

Contents
xix
Sports Activity Recognition - Shot Put, Discus, Hammer 
and Javelin Throw . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529 
Swati Shilaskar, Gayatri Aurangabadkar, Chinmayee Awale, 
and Sakshi Awale 
User Acceptance of Contact Tracing Apps: A Study During 
the Covid-19 Pandemic
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
541 
Inger Elisabeth Mathisen, Kanika Devi Mohan, Tor-Morten Grønli, 
Tacha Serif, and Gheorghita Ghinea 
Digital Watermark Techniques and Its Embedded and Extraction 
Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
551 
Satya Narayan Das and Mrutyunjaya Panda 
Galvanic Skin Response-Based Mental Stress Identiﬁcation Using 
Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
561 
Padmini Sethi, Ramesh K. Sahoo, Ashima Rout, and M. Mufti 
A Federated Learning Based Connected Vehicular Framework 
for Smart Health Care . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
571 
Biswa Ranjan Senapati, Sipra Swain, Rakesh Ranjan Swain, 
and Pabitra Mohan Khilar 
ELECTRE I-based Zone Head Selection in WSN-Enabled Internet 
of Things . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
581 
Sengathir Janakiraman, M. Deva Priya, A. Christy Jeba Malar, 
and Suma Sira Jacob 
Fabrication of Metal Oxide Based Thick Film pH Sensor and Its 
Application for Sweat pH Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
595 
Vandana Pagar, Shweta Jagtap, Arvind Shaligram, and Pravin Bhadane 
Reliable Data Delivery in Wireless Sensor Networks with Multiple 
Sinks and Optimal Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
607 
Vasavi Junapudi and Siba K. Udgata

Effect of the Longitudinal Strain of PM 
Fiber on the Signal Group Velocity 
Karel Slavicek 
, David Grenar 
, Jiri Vavra  , Martin Kyselak 
, 
Jan Radil 
, and Jakub Frolka 
Abstract The polarization of the light can be used as the core principle of ﬁber optic 
sensors. One of the physical quantities which can be detected or measured this way 
is the longitudinal tension of the ﬁber. A set of measurements leading to approval of 
the suitability of polarization for this purpose was performed. This paper analyzes 
the dependency of differential group delay of the signal in slow and fast axes of the 
birefringent optical ﬁber on the longitudinal tension. 
Keywords Birefringent ﬁber · Polarization · Differential group delay · Sensors 
1 
Introduction and Motivation 
Our research group has been studying polarization properties of the ﬁber optic for a 
long time [ 1– 4]. The main aim is to utilize polarization for sensing purposes. The key 
sensing element of our sensors is a birefringent ﬁber - usually a Panda one. The basic 
K. Slavicek 
Masaryk University, Brno, Czech Republic 
e-mail: karel@ics.muni.cz 
D. Grenar (
) · J. Frolka 
Department of Telecommunications, Faculty of Electrical Engineering and Communications, 
Brno University of Technology, Brno, Czech Republic 
e-mail: xgrena04@vutbr.cz 
B
J. Frolka 
e-mail: frolka@vutbr.cz 
J. Vavra · M. Kyselak 
University of Defence, University of Defence, Czech republic, Czech Republic 
e-mail: jiri.vavra@unob.cz 
M. Kyselak 
e-mail: martin.kyselak@unob.cz 
J. Radil 
Department of Computer Science, Faculty of Science, University of South Bohemia in Ceske 
Budejovice, Ceske Budejovice, Czech Republic 
e-mail: jradil@prf.jcu.cz 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_1 
1

2
K. Slavicek et al.
Linearly 
Polarized 
Light 
Detector 
slow 
axis 
Sensitive 
segment 
π/4 
Fig. 1 The basic structure of polarization based sensors. 
principle of the sensor is denoted in Fig. 1. The sensing system consists of a source 
of linearly polarized light which is inserted into birefringent (commonly Panda type) 
ﬁber used as the sensing element. The input signal should be evenly distributed into 
slow and fast axes of the Panda ﬁber. The measured physical quantity causes a change 
in the polarization state or a change in the differential group delay. The change can 
be detected by a polarimeter (in the case of lab experiments) or by a dedicated optical 
detector (in the case of a production sensor system). 
The main advantage of polarization-based sensors is the speed of the detector. 
This type of sensor is very suitable for the detection of rapid changes in the measured 
physical quantity. Frequently it is used just to detect that a change has occurred and 
provides binary output only. 
In the case of tension measurement, the longitudinal tension has an impact on 
the differential group delay [ 5]. In this paper, we study the inﬂuence of longitudinal 
tension on the differential group delay. Our goal is to check the ability of polarization-
based sensors to provide not only the binary output but provide information about 
the intensity of the tension power. 
2 
Physical Principles 
The light beam in the optical ﬁber propagates in two orthogonal polarization planes. 
Commonly, we denote as zz axis the direction of wave propagation, the horizontal 
plane as the xx or fast axis direction and the vertical plane as the yy or slow axis, see 
Fig. 2. 
Optical signal propagation velocity differs in both polarization planes. A light 
pulse is propagated throughout the ﬁber line. At the input, the pulse is projected into 
both the slow and fast axis of the ﬁber. Signal in both axes is propagated throughout 
the ﬁber with different velocities. This causes different times of the arrival of pulse 
projection into the slow and into the fast axis at the end of measuring ﬁber. See Fig. 3. 
The situation denoted in Fig. 3 is a bit simpliﬁed and is valid for the polarization 
maintaining ﬁber only. In the case of a legacy single-mode ﬁber (G.652) we have a 
set of segments with different orientations of the slow and fast axes The reason for

Effect of the Longitudinal Strain of PM Fiber ...
3
Optical Fiber 
Slow Axis 
Fast Axis 
Electrical Field Vector 
Fig. 2 Model of the light polarization inside ﬁber optic. 
DGD 
VSlow 
VFast 
VSlow 
VFast 
PANDA PM 
FIBER 
Fig. 3 Signal propagation throughout a polarization maintaining ﬁber. 
this situation is differences in the intensity and orientation of external stress causing 
ﬁber microbending, differences in thermal stress, etc. 
Moreover, the ﬁnal differential group delay (DGD) at the far end of the ﬁber 
varies according to variations of the external inﬂuences alongside the ﬁber. These 
differences in the signal group velocity can cause some distortion of the optical pulses 
which is very meaningful in telecommunications. As the DGD causes some variation, 
in this case, we use an average value called PMD - Polarization Mode Dispersion -
instead. There is a close relation between DGD and PMD so the measuring equipment 
designed for PMD measurement in telecommunication networks can be used for 
DGD measurement on polarization maintaining ﬁber as well [ 6]. 
The PMD, its inﬂuence on telecommunications networks, and methods of its 
measurement and analysis are discussed in several research papers and whitepaper 
prepared by measurement equipment manufacturers [ 7– 9].

4
K. Slavicek et al.
3 
DGD Measurement 
There are several methods for DGD and PMD measurement documented in the 
ITU-T Recommendation G650.2 [ 10]. The most widely used one in contemporary 
telecommunication networks is the GINTY - Generalized Interferometric Method. 
GINTY measurement setup consists of a polarized light source, ﬁber under test 
(FUT), the input analyzer at the output of the FUT, the interferometer, polarization 
beam splitter, and two photodetectors as shown in Fig. 4. The light velocity in the 
ﬁber line depends among other parameters on the wavelength. This is the reason for 
a broadband light source. The polarizer in the polarimeter source serves to set several 
different states of polarization of the light source entering the FUT. The analyzer in 
the polarimeter detector serves to bring two orthogonal states of polarization of the 
light to the interferometer. The polarization beamsplitter is used to split the output 
light into two orthogonal polarization planes. 
The optical power on two orthogonal polarization planes is then presented on 
outputs upper P Subscript xPx and upper P Subscript yPy. These outputs are subsequently used to calculate the DGD and 
PMD coefﬁcients. In the case of birefringent ﬁber, the DGD can be read directly 
from the interferogram. 
4 
Experimental Results 
The aim of our experimental measurement was to study the dependency of DGD 
in polarization maintaining ﬁber on the longitudinal tensile force. In the ﬁrst step, 
we employed a proof tester commonly used to ensure good mechanical properties 
of ﬁber lines after manufacturing like writing FBG (Fibre Brag Grating), splicing, 
or similar. In our measurement setup, the light source EXFO FTB-5800A and the 
analyzer EXFO FTB-5700 were used. 
BROUDBAND LIGHT 
SOURCE 
POLARMETER 
RECIVER 
POLARIZER
ANALYZER 
FUT 
POLARIMETER SOURCE 
PBS 
INTEFEROMETER 
PX
PY 
Fig. 4 The GINTY PMD measurement setup.

Effect of the Longitudinal Strain of PM Fiber ...
5
4.1 
Measurement on Proof Tester 
The proof tester used was SFO Proof Tester from NWG company. The proof tester 
can apply a tensile force from 0 N up to 70 N to the ﬁber under test (FUT) with 
step 0.1 N. It uses a 45mm mandrel to hold the FUT. The FUT is wounded on both 
mandrels and ﬁxed by small magnetic holders. 
We have set up the measurement as depicted in Fig. 5. After the ﬁrst measurement 
without applying tensile force, which was used as a kind of calibration, we applied 
a tensile force from 1 N up to 25 N with step 1 N. 
As expected, the DGD increases with increasing tensile force. The DGD depen-
dence on tensile force is linear as can be easily read from the graph in Fig. 6. The  
interferograms for tensile force from 1 N up to 7 N look almost like they were copied 
from a textbook on PMD measurement, see Fig. 7. Since force about 8 N additional 
peaks occurred in the interferogram, see Fig. 8. Even if the distance of the main 
peaks from the center corresponds to our expectations, we intended to explain this 
behavior. 
Our assumption was, that the additional peaks were caused by crosswise stress. 
This stress is caused by the mechanical stress of the ﬁber on the proof tester’s man-
drels. To verify this assumption, we performed two sets of additional measurements 
documented in the following subsections. 
GINTY SIGNAL 
SOURCE 
GINTY SIGNAL 
RECEIVER 
PM FUT 
SMF
SMF 
REEL HOLDER 
Fig. 5 Measurement setup with the proof tester. 
2.85 
2.9 
2.95 
3 
3.05 
3.1 
0
5
10
15
20
25 
DGD [ps] 
Tensile Force [N] 
Fig. 6 The DGD dependence on tensile force.

6
K. Slavicek et al.
Fig. 7 Interferogram of applied tensile force 1 N. 
Fig. 8 Interferogram of applied tensile force 8 N. 
4.2 
Straining Fiber on Connectors 
To get rid of the crosswise strain, we decided to stress the ﬁber by pulling it by the 
connectors. In this case, we have used the fact, that the tested ﬁber was equipped with 
FC/PC connectors. We have constructed a special holder holding the FC adapter. One 
of these holders was afﬁxed on top of the rack in our lab. The second one was simply 
hanging on the ﬁber under test, and a proper weight was connected to this holder. 
The measurement setup is denoted in Fig. 9.

Effect of the Longitudinal Strain of PM Fiber ...
7
Fig. 9 Measurement setup using connectors as a ﬁber holder. 
In this measurement setup, we were not able to prove our assumptions. The 
mechanical strength of the ﬁber is not sufﬁcient to perform this experiment. With the 
tensile force above 5 N, the ﬁber was snapped. For this reason, we have prepared the 
next measurement setup. 
4.3 
Usage of Fiber Clamps 
The aim of the next setup is to avoid variation of the crosswise stress on the ﬁber 
caused by the longitudinal tension applied to the ﬁber. The usage of connectors, 
which avoids the crosswise stress proved to be not usable. Another option is to apply 
constant crosswise stress in all cases, which means in the whole range of applied 
longitudinal tension. 
The constant crosswise stress can be achieved by the usage of clamps holding the 
ﬁber under test. The situation is denoted in Fig. 10. In this setup, additional peaks 
Fig. 10 Measurement setup using clamps to hold the FUT.

8
K. Slavicek et al.
Fig. 11 Interferogram of applied tensile force 2 N. 
occurred in all applied tensile forces. In this setup, we were not able to perform the 
measurement in the whole range of applied tensile force as in the case of the proof 
tester. The reason is the mechanical properties of used clamps. 
The ﬁber under test was in 250muµm primary jacket. Common clamps are designed 
for regular ﬁber cables with a diameter 2 mm and above. The holding of these clamps 
is ﬁrm enough to apply longitudinal tension of up to 6 N. The clamps make almost 
constant crosswise strain on the measured ﬁber, independent of the applied longitu-
dinal tension. This crosswise strain causes peaks in the interferogram very similar 
to the case of proof tester usage. In this case, the “additional” peaks occur for all 
applied values of longitudinal tension force and are almost constant, see Fig. 11. 
5 
Funding 
This work was supported by the grant project of the Ministry of Interior of the Czech 
Republic, No VK01030060. 
6 
Conclusion 
The measurements performed have approved, that the polarization is usable for the 
measurement of longitudinal tension. In the case of a ﬁxed length of birefringent 
ﬁber (what’s the case of the expected sensor system), the dependence of the DGD 
on longitudinal tension force is linear. Even though the uneven crosswise tension 
force applied causes changes in the interferogram on PMD measurement, it doesn’t

Effect of the Longitudinal Strain of PM Fiber ...
9
inﬂuence the overall DGD. That means polarization proved sufﬁcient robustness for 
longitudinal tension measurement. The next step in the practical utilization of this 
principle is to transform any physical quantity of interest into longitudinal tension 
of the measuring ﬁber. 
References 
1. Kyselák M, Slavicek K, Grenar D, Bohrn M, Vavra J (2022) Fiber optic polarization temperature 
sensor for biomedical and military security systems. In: Smart biomedical and physiological 
sensor technology XIX, vol 12123, pp 121230A. https://doi.org/10.1117/12.2618911 
2. Kyselák M, Vyležich Z, Vávra J, Grenar D, Slavíˇcek K (2021) The long ﬁber optic paths to 
power the thermal ﬁeld disturbance sensor. In: Optical components and materials XVIII, vol 
11682, p 116821A. https://doi.org/10.1117/12.2575832 
3. Kyselak M, Maschke J, Panasci M, Slavicek K, Dostal O, Grenar D, Cucka M, Filka M (2020) 
Birefringence inﬂuence on polarization changes and frequency on optical ﬁber. In: Electro-
optical remote sensing XIV, vol 11538, p 115380F. https://doi.org/10.1117/12.2573696 
4. Kyselak M, Dvorak F, Maschke J, Vlcek C (2018) Optical birefringence ﬁber temperature 
sensors in the visible spectrum of light. Adv Electr Electron Eng 15:885–889. https://doi.org/ 
10.15598/aeee.v15i5.2419 
5. Grenar D, Cucka M, Filka M, Slavicek K, Vavra J, Kyselak M (2022) Optical sensor based 
on birefringent ﬁber type PANDA used for tensile detection. In: 2022 IEEE international 
conference on internet of things and intelligence systems (IoTaIS), pp 57–63 
6. https://www.c3comunicaciones.es/Documentacion/ﬁberguide2_bk_fop_tm_ae.pdf 
7. Gordon J, Kogelnik H (2000) PMD fundamentals: polarization mode dispersion in optical 
ﬁbers. Proc Natl Acad Sci USA 97:4541–4550 
8. https://www.corning.com/media/worldwide/coc/documents/Fiber/white-paper/WP5051-12_ 
12.pdf 
9. Jurdana I, Pilinsky S, Batagelj B (2006) PMD Measurements in Telecom Networks 
10. Recommendation ITU-T G650.2. https://www.itu.int/rec/T-REC-G.650.2-201508-I/en

Machine Learning Algorithms Aided 
Disease Diagnosis and Prediction 
of Grape Leaf 
Priyanka Kaushik 
Abstract The range of diseases that can affect grape leaves has made it vital to 
analyze them. High-end data analytics and predictive analysis are required for a 
number of diseases, including black rot esca black measles, blight isariopsis, and 
others, in order to predict disease occurrence. For the prediction of leaf diseases, 
convolution neural networks combined with data augmentation have increased the 
degree of veriﬁcation. For illness predictive analytics, a proper confusion matrix 
for support vector machines driven by CNN was created. Along with k-mean clus-
tering, fuzzy logic with accurate feature extraction, and color moment deﬁnition, 
we also compared our results with these techniques. The ﬁndings indicate a higher 
effectiveness of up to 95% in correctly predicting grapes leaf disease. 
Keywords Leaf diseases · DI · Pests · Classiﬁcation · Detection · Forecasting ·
SVM · Convolution neural network · k mean clustering · Fuzzy logic 
1 
Introduction 
In India, China, and several nations in the southeast paciﬁc region, grape output 
reached a total of 20.083 million tons in 2022, making it one of the most well-known 
signiﬁcant, and prosperous fruit sectors. On the other side, diseases that harm grape 
leaves have stunted the grape industry’s expansion and caused signiﬁcant monetary 
losses. This has led to a great lot of effort being put forth by orchard workers and 
experts in the prevention and treatment of diseases and pests to identify and diagnose 
illnesses that harm grape leaves. 
To begin with, in order for CNN models to be successfully trained, a sizable 
amount of input data is required. However, because each illness that damages the 
grape leaf occurs at a distinct period, there is only a limited window of opportunity
P. Kaushik envelope symbol
Computer Science Engineering Department (AIT CSE (AIML)), Chandigarh University, Gharuan, 
Punjab, India 
e-mail: Kaushik.priyanka17@gmail.com; priyanka.e13618@cumail.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_2 
11

12
P. Kaushik
for taking pictures of sickness. Since there aren’t enough images of grape leaves with 
damage, the model can’t be taught from them. 
Second, transfer learning-trained models have difﬁculty achieving adequate levels 
of performance because of the challenging nature of the ﬁne-grained image catego-
rization for diseases of the grape leaves. Making the most accurate diagnosis of 
diseases that harm grape leaves for the CNN structure is therefore a difﬁcult task. 
The main innovation of this study is to detect disease on grape leaves using a 
reform CNN algorithm [20]. The other signiﬁcant advancements are enumerated as 
follows: 
The compiling of a data set on the disease of the grape leaf lays a crucial foun-
dation for the generalization of the model. As a prediction tool, it guides people 
in recognizing grape diseases and will be grateful for its average accuracy rate of 
98.57%. 
In order to increase the model’s robustness, images of ill grape leaves are ﬁrst 
gathered with both intricate and homogeneous backgrounds. Additionally, the raw 
images of the diseased grape leaves are run via a data augmentation technology and 
then used to build a suitable amount of training photos in order to avoid the issue of 
the model being too accurate. 
2 
Literature Review 
The prevention of disease spread and maintenance of the grape industry’s health 
depends on prompt diagnosis and accurate treatment. This study by Bin Liu et al. [1] 
offers a distinct recognition method for vine leaf diseases. The analysis results show 
that the suggested approach can reliably diagnose grape leaf diseases. 
According to Xiaoyue Xie, et al. [2], the absence of a real-time diagnosis tool for 
diseases that damage grape leaves prevents grape plants from developing normally. 
It is suggested grape leaf disease detector by deep learning CNN. 
Zhaohua Huang, et al. [3] speak for Pets and grapevine illnesses can result in 
signiﬁcant ﬁnancial losses for farmers and grape output if they are not identiﬁed and 
treated quickly. Using a developed grape leaf dataset, four updated deep-learning 
models for diagnosing and categorizing grape leaf diseases are created. 
According to Miaomiao Ji, et al. [4], creatingan automated detection tool for 
diseases of grape leaves is essential. It’s possible to extract complementary discrim-
inative features using the proposed United Model. Yun Peng and others [5] for grape 
yield and quality, grape leaf disease must be quickly and accurately identiﬁed. This 
article suggests a way for recognizing vine leaf disorders based on combined deep 
neural network characteristics from convolutions (CNN) and AVM (SVM). 
Real-time disease identiﬁcation at the press would be possible because of the 
employment of segmentation and classiﬁcation algorithms on low-powered devices, 
according to Lucas Mohimont et al. [6]. Using a hierarchical technique, the backdrop 
can be removed and photographs with grape diseases can be found. A pertained 
MobileNet-V2 model had an F1 score of 94% for classifying illnesses.

Machine Learning Algorithms Aided Disease Diagnosis and Prediction …
13
According to Carlos S. Pereira et al. [7], shots of grapevine leaves and bunches 
taken during harvest seasons have a low picture volume, a high degree of resem-
blance among grape species, signs oﬂeaf senescence, and signiﬁcant changes. In 
two different geographical locations and throughout two different harvest seasons, 
natural vineyard images were gathered. 
Mathilde Chen and others [8] Regarding the environment and public health, mini-
mizing the actual treatments is a major problem. Finding vineyards might be one 
approach. The algorithms under consideration use the date the disease began as well 
as/or monthly average temperatures and precipitation as input variables. 
According to Tanmay A. Wagh et al. [9], the disease on grape plants frequently 
starts on the leaf before spreading to the stem, root, and fruit. The entire plant perishes 
when the spreads to the fruit. In our proposed system, a type of neural network called 
a CNN is referred to as a “deep learning model.” The accuracy is 98.23% when 
comparing bacterial spots to powdery mildew. 
According to S.M. Jaisakthi et al., Crop diseases are largely a natural occurrence 
due to variables like climate change and environmental alterations. An autonomous 
system for diagnosing ailments in grape plants was created using image processing 
and machine learning techniques. They were able to achieve a 93% testing accuracy 
using SVM. 
Ujjwal Singh et al. [11] Regular and proactive monitoring, the use of disease 
detection techniques, and the reduction of aesthetic and ﬁnancial losses brought 
on by plant diseases are all necessary for the production of grapes. An automated 
computer vision technique for identifying Black Measles disease has been built using 
grape leaf image samples. 
Y. Nagaraju et al. [12] Fruits like grapes and apples are the most lucrative but also 
the most prone to disease. Here, the outcome layer of the prior source is deleted, and 
a new output layer is attached to it in order to ﬁne-tune the VGG-16 Network. With a 
97.87% accuracy rate, the deep- convolutional neural network performs remarkably 
well in the diagnosis of apple and grape leaf diseases. 
Khaing Zin Thet, among others [13] Grape plants become infected from the leaves 
to the stem, fruit, and root. The VGG16network, one of CNN Architecture’s networks, 
is tuned using this system to detect illnesses on grape leaves. Instead of the two 
completely connected layers of VGG16, the system used a GAP layer before the 
ﬁnal classiﬁcation. 
Sandy Lauguico and others [14]. It is essential to identify various disorders in 
leaves in order to boost crop output. The comparison has been done to predict which 
of the 3 pre-trained networks—AlexNet, GoogleLeNet, and ResNet-18—performed 
best. 
The accuracy of the other two models, GoogLeNet and ResNet-18, was only 
92.29% and 89.49%, respectively. Among others, Stefania Barburiceanu [15] extrac-
tors for new rotation, illumination, and observation scale invariant color texture 
categorization features. Comparing the suggested feature extractors to traditional 
grayscale LBP-based techniques reveals a considerable improvement in terms of 
accuracy, correctness, and recall. Data is categorized using the ﬁndings of the 
experimental section of the study that used Support Vector Machines (SVMs).

14
P. Kaushik
[16] Changjian Zhou et al. early sickness onset identiﬁcation is essential for prac-
tical usage since similar treatments might be used when a plant disease is still in its 
early stages. A ﬁne-grained- GAN based on ﬁve cutting-edge deep learning models, 
most notably ResNet-50, was suggested as a way to further strengthen the general-
ization categorization models’ power. This strategy greatly increased identiﬁcation 
accuracy. 
Aravind K R and others [18] One of the most important fruit crops that suffer 
from illness is the grape. The classiﬁcation accuracy of the model was 97.62%. The 
Support uses feature data acquired from different layers of the same network. When 
MSVM was combined with AlexNet’s Rectiﬁed Linear Unit (ReLu) layer, the highest 
classiﬁcation accuracy of 99.23% was attained. Early disease detection is essential 
to preventing crop damage, according to Suviksha Poojari, et al. [19] because these 
diseases cause signiﬁcant agricultural damage and ﬁnancial loss. Choosing a cate-
gorizing system is never easy when there are so many available, especially since the 
output consistency varies depending on the input data. 
Arie Moh, et al. [20] An accurate identiﬁcation of the plant disease is necessary 
in order to implement suitable management measures as a tool for the diagnosis to 
identify and group grape leaf diseases, a CNN network was employed [17]. 
According to the literature, the feature selection procedure needs to be revised. 
CNN and support vector machines are two deep learning techniques that could be 
used (SVM). For precise disease prediction, a comparison between CNN and support 
vector machines should be conducted. 
3 
Diagnosis of Grape Leaf Diseases 
Major vine shoots diseases are discussed in this study The following major vine 
shoots or leaf diseases were chosen for the prediction: 
1. Spanish measles: Figure 1 is a true photograph of a grape leaf infected with 
Esca Black Measles disease. The surface spots of the fruit are referenced by the 
name “measles”. Over the course of the season, the dots could mix over the skin’s 
surface, rendering the berries black. Between the time of fruit set and a few days 
before harvest, spotting may emerge. During fruit set, berries that are harmed do 
not develop and instead wilt and dry off. Later in the season, fruit that has been 
damaged will also have a bad ﬂavor. Figure 1 illustrates the “tiger stripe” pattern 
that characterizes leaf symptoms when infections are strong from year to year 
[10].
2. Black rot - Leaf symptoms ﬁrst appear as small, round, reddish-brown spots: 
The condition is depicted in real life in Fig. 2. Black rot, a deadly disease of both 
cultivated and wild grapes, is brought on by the fungus Guignardiabidwellii. The 
worst effects of this condition occur during warm, wet months. The fruit leaves, 
fruit stems, tendrils, shoots, leaf stems, and all other green parts of the vine are 
all attacked [9].

Machine Learning Algorithms Aided Disease Diagnosis and Prediction …
15
Fig. 1 Grape leaf and Grapes infected by Spanish Measles disease
Fig. 2 Grape leaf and 
Grapes infected by Black rot 
disease 
3. Leaf blight Isariopsis Leaf Spot: This is identiﬁed with distributed purple 
brownish marks on the leaf surface. Figure 3 depicts a real-world example of 
this condition. Pseudocercospora vitis fungi cause infected leaves to turn yellow 
with brown spots [23]. 
Fig. 3 Grape leaf infected by Leaf blight Isariopsis Leaf Spot disease

16
P. Kaushik
4 
Data Analysis and Augmentation 
Several processing digital picture technologies are used to give data expansion all 
types of image intensity interference are used to mimic how the weather affects 
photography. The pixels’ red, green, and blue values are randomly increased or 
decreased to alter the brightness of each image [22]. Assume that V represents the 
changed value, the brightness transformation factor, and that represents the original 
RGB value [20]. The way involved in transforming RGB values are: 
up per V equal s up
per V Baseline 0 plus left parenthesis 1 plus d right parenthesis
The image’s contrast value is modiﬁed by raising the bigger RGB values and 
reducing the smaller RGB values based on the brightness’s median value. The RGB 
values are transformed in the following way: 
up per V equ al s i plus  left parenthesis upper V Baseline 0 minus i right parenthesis left parenthesis 1 plus d right parenthesis
To alter the sharpness value, the Laplacian template is applied to the picture. 
Assume that a pixel in an image RGB picture [20] is represented by 
a s c left parenthesis x comma y right parenthesis equals left bracket upper R left parenthesis x comma y right parenthesis comma upper G left parenthesis x comma y right parenthesis comma upper B left parenthesis x comma y right parenthesis right bracket
The formula is as follows: 
nabla 2 left  bracket c left parenthesis x c
omma y right parenthesis right bracket equals left bracket nabla 2 upper R left parenthesis x comma y right parenthesis nabla 2 upper G left parenthesis x comma y right parenthesis nabla 2 upper B left parenthesis x comma y right parenthesis right bracket
Images are rotated by moving each pixel around the center at the same degree. 
Assume that P(x,y) is a random point in the picture and that after clockwise rotation 
by 90°, its new coordinate is (x,h-y). The two points’ computed coordinates are 
written as [20]. 
Sta rtLa yout  1s t Ro w 1st Colu mn left brace x  equ als 2nd  Col umn
 r c osine alpha y e qual s r sin e le f
t brace upper X equals r cosine left parenthesis alpha minus theta right parenthesis equals x cosine theta plus y r sine theta upper Y 2nd Row 1st Column equals 2nd Column r sine left parenthesis alpha minus theta right parenthesis equals minus x cosine theta plus y r cosine theta EndLayout
Cascade Dense Inception Module 
Varieties of grape leaves differ substantially in the prevalence of the disease spots. 
The ultimate recognition accuracy greatly depends on the model’s capacity to extract 
features at various scales. It is common practice to evaluate the temporal complexity 
of the CNN model using ﬂoating-point operations. One-way to represent a single 
convolutional layer’s temporal complexity is as follows: 
upper T i m e s ti lde le ft parenthesis upper M Baseline 2 asterisk upper K Baseline 2 asterisk upper C i n asterisk upper C o u n t right parenthesis

Machine Learning Algorithms Aided Disease Diagnosis and Prediction …
17
The temporal complexity of the Inception structure may be represented as the 
total of the operation times of all the convolutional layers: 
upper T
 i
 m e  s ti lde l eft parenthesis sigma sum mat
i
on upper M i Baseline 2 asterisk upper P i asterisk upper Q i asterisk upper C left parenthesis i comma i n right parenthesis asterisk upper C left parenthesis i comma o u t right parenthesis upper D i equals 1 right parenthesis
where D = Inception structure convolutional layers, Pi denotes the convolution 
kernel’s length, and Qi denotes the convolution kernel’s width and Qi != Pi when 
asymmetric. 
The model’s recognition accuracy is greatly harmed by this loss of features. The 
dense connection technique was presented in Dense Net as a way to improve the 
movement of data between layers even further. As shown in Equation, the l layer 
collects feature maps from all previous layers: 
x equals left  p a renthesis left bracket x o comma x Baseline 1 comma ellipsis x lamda minus 1 right bracket right parenthesis
where [x_0,x_1,…,x_(λ-1)] denotes the concatenation of the preceding layers’ maps 
[21]. 
Adaptive Connectivity Strategy 
The classiﬁcation of grape leaf diseases is done by CNN Technique. The training 
results are signiﬁcantly inﬂuenced by the optimization technique employed. 
Adaptive moment estimation (Adam) was used as the model’s optimization 
method as opposed to the more common stochastic gradient descent (SGD). The data 
from the previous iteration is used to compute the new weights, and the following is 
how the weight optimization process is described: 
g t  equal s nabla  left pa rent hesis  theta t minus 1 right parenthesis m t equals beta Baseline 1 period m t minus 1 plus left parenthesis 1 minus beta Baseline 1 right parenthesis period g t
v t  equals beta  Base line 2 perio d v t mi nus 1 plus left parenthesis 1 minus beta Baseline 2 right parenthesis period g t Baseline 2 m t equals m t divided by left parenthesis 1 minus beta Baseline 1 t right parenthesis
v Mo difyingA bove t Wi th ca ret equals v t div ided by left parenthesis 1 minus beta Baseline 2 t right parenthesis theta t equals theta t minus 1 minus alpha period m ModifyingAbove t With caret divided by left parenthesis square root v ModifyingAbove t With caret plus epsilon right parenthesis
where a represents the learning rate, β1 and β2 the following are the formulas for 
calculating the recognized performance of each indicator [24]. 
upper P r e c  i  s i o
 n equals upper T upper P upper T upper P plus upper F upper P
upper R  e c a  l l  
equals upper T upper P upper T upper P plus upper F upper N
StartLay out 1st Row 1st Column upper F Base line 1 up
per S c  o r e  equa l s  2n d
 Column 2 times upper P r e c i s i o n times upper R e c a l l upper P r e c i s i o n times upper R e c a l l 2nd Row 1st Column equals 2nd Column 2 times upper T upper P Baseline 2 times upper T upper P plus upper F upper N plus upper F upper P EndLayout

18
P. Kaushik
5 
Simulation Result 
Using training and testing datasets for grape leaf disease, the accuracy of grape 
leaf disease prediction for the chosen diseases has been examined. Initially, grape 
leaf disease accuracy was demonstrated using CNN technology; however, after 
monitoring the results, SVM technology has emerged as a potentially useful tool. 
Convolutional Neural Network 
The confusion matrix is a matrix that represents this accuracy. The accuracy score 
for CNN is 0.375. The results show that deep learning algorithms not only take a lot 
of time but also make bad predictions (Fig. 4). 
Support Vector Machine (SVM) 
With a classiﬁcation accuracy of 0.5 for the 4b transcript variables and four fewer 
factors than logistic regression. The linear SVM model appears to be the most 
promising for usage in clinical settings, as this result shows a desirable sensitivity 
and speciﬁcity (Figs. 5 and 6). 
Fig. 4 a Accuracy Graph of 
Convolutional Neural 
Network (CNN) b Loss 
Graph of Convolutional 
NeuralNetwork (CNN) 
Fig. 5 a Accuracy Graph of SVM (CNN) b Loss Graph of SVM (SVM)

Machine Learning Algorithms Aided Disease Diagnosis and Prediction …
19
Fig. 6 a Confusion Matrix for CNN b Confusion Matrix for SVM 
6 
Conclusion 
Grape leaf disease analysis has grown in importance as a part of the industry because 
of how different plants may detect illness. Effective illness prediction requires sophis-
ticated data analytics and predictive analysis. This category includes naming a few, 
illnesses like black rot, esca black measles, and blight isariopsis. There is now a 
greater level of veriﬁcation for the prediction of leaf diseases thanks to the deploy-
ment of a convolutional neural network and data augmentation. For accurate illness 
prediction analytics, a suitable confusion matrix for support vector machines driven 
by CNN was developed. Additionally, we contrasted our ﬁndings with those obtained 
using the k-mean clustering method, fuzzy logic with precise feature extraction, and 
color moment deﬁnition. The execution time increases along with the network’s 
complexity. The main obstacle in accurately predicting grape leaf disease is to 
increase classiﬁcation accuracy and execution speed. 
References 
1. Liu B, Ding Z, Tian L, He D, Li S, Wang H (2020) Grape leaf disease identiﬁcation using 
improved deep convolutional neural networks. Front Plant Sci 11:1082. https://doi.org/10. 
3389/fpls.2020.01082. ISSN1664-462X 
2. Xie X, Ma Y, Liu B, He J, Li S, Wang H (2020) A deep-learning-based real-time detector 
for grape leaf diseases using improved convolutional neural networks. Front Plant Sci 11:751. 
https://doi.org/10.3389/fpls.2020.00751. ISSN 1664-462X 
3. Huang Z, Qin A, Lu J, Menon A, Gao J (2020) Grape leaf disease detection and classiﬁcation 
using machine learning, pp 870–877. https://doi.org/10.1109/iThings-GreenCom-CPSCom-
SmartData-Cybermatics50389.2020.00150 
4. Ji M, Zhang L, Wu Q (2020) Automatic grape leaf diseases identiﬁcation via UnitedModel based 
on multiple convolutional neural networks. Inf Process Agric 7(3):418–426. ISSN 2214-3173 
5. Peng Y, Zhao S, Liu J (2021) Fused-deep-features based grape leaf disease diagnosis. Agronomy 
11:2234. https://doi.org/10.3390/agronomy11112234

20
P. Kaushik
6. Mohimont L, Alin F, Gaveau N, Steffenel LA (2022) Lite CNN models for real-time post-
harvest grape disease detection. In: Workshop on edge AI for smart agriculture (EAISA 2022), 
Biarritz, France. ffhal-03647740f 
7. Pereira CS, Morais R, Reis MJCS (2019) Deep learning techniques for grape plantspecies 
identiﬁcation in natural images. Sensors 19:4850. https://doi.org/10.3390/s19224850 
8. Chen M, Brun F, Raynal M, Makowski D (2020) Forecasting severe grape downymildew attacks 
using machine learning. PLoS ONE 15(3):e0230254 
9. Liu B, Ding Z, Tian L, He D, Li S, Wang H (2020). Grape leaf disease identiﬁcation using 
improved deep convolutional neural networks. Front Plant Sci 11:1082. https://doi.org/10.3389/ 
fpls.2020.0108 
10. Wagh TA, Samant RM, Gujarathi SV, Gaikwad SB (2019) Grapes leaf disease detection using 
convolutional neural network. Int J Comput Appl 178(20):7–11. https://doi.org/10.5120/ijca20 
19918982 
11. Jaisakthi SM, Mirunalini P, Thenmozhi D (2019) Grape leaf disease identiﬁcation using 
machine learning techniques. In IEEE 2019 international conference on computational intelli-
gence in data science (ICCIDS), Chennai, India, pp 1–6. https://doi.org/10.1109/ICCIDS.2019. 
8862084 
12. Singh U, Srivastava A, Chauhan D, Singh A (2020) Computer vision technique for detection 
of grape esca (black measles) disease from grape leaf samples. In: IEEE 2020 international 
conference on contemporary computing and applications (IC3A)-Lucknow, India, pp 110–115. 
https://doi.org/10.1109/IC3A48958.2020.233281 
13. Nagaraju Y, Swetha S, Stalin S (2020) Apple and grape leaf diseases classiﬁcation using transfer 
learning via ﬁne-tuned classiﬁer. In: 2020 IEEE international conference on machine learning 
and applied network technologies (ICMLANT). https://doi.org/10.1109/icmlant50963.2020. 
9355991 
14. Thet KZ, Htwe KK, Thein MM (2020) Grape leaf diseases classiﬁcation using convolu-
tional neural network. In: 2020 international conference on advanced information technologies 
(ICAIT). https://doi.org/10.1109/icait51105.2020.9261801 
15. Lauguico S, Concepcion R, Tobias RR, Bandala A, Vicerra RR, Dadios E (2020) Grape leaf 
multi- disease detection with conﬁdence value using transfer learning integrated to regions 
with convolutional neural networks. In: 2020 IEEE region 10 conference (TENCON). https:// 
doi.org/10.1109/tencon50793.2020.9293866 
16. Barburiceanu S, Terebes R, Meza S (2020) Grape leaf disease classiﬁcation using LBP-derived 
texture operators and colour. In: IEEE 2020 IEEE international conference on automation, 
quality and testing, robotics (AQTR) - Cluj- Napoca, Romania, pp 1–6. https://doi.org/10.1109/ 
AQTR49680.2020.9130019. Zhou C, Zhang Z, Zhou S, Xing J, Wu Q, Song J (2021) Grape 
leaf spot identiﬁcation under limited samples by ﬁne grained-GAN. IEEE Access 9:100480– 
100489. https://doi.org/10.1109/access.2021.3097050 
17. Ali A, Ali S, Husnain M, Saad Missen MM, Samad A, Khan M (2022) Detection of deﬁciency 
of nutrients in grape leaves using deep network. Math Probl Eng 2022, Article ID 3114525, 12 
p 
18. Aravind KR, Raja P, Aniirudh R, Mukesh KV, Ashiwin R, Vikas G (2019) Grape Crop Disease 
Classiﬁcation Using Transfer Learning Approach. https://www.researchgate.net/publication/ 
331634971 
19. Poojari S, Sahare D, Pachpute B, Patil M (2020) Identiﬁcation and solutions for grape leaf 
disease using convolutional neural network (CNN). In: 2nd international conference on commu-
nication & information processing (ICCIP). https://ssrn.com/abstract=3648108. http://dx.doi. 
org/https://doi.org/10.2139/ssrn.3648108 
20. Liu B, Ding Z, Tian L, He D, Li S, Wang H (2020) Grape leaf disease identiﬁcation using 
improved deep convolutional neural networks. Front Plant Sci 11:1082 
21. Rathore R (2022) A study on application of stochastic queuing models for control of congestion 
and crowding. IJGASR 1(1):1–6 
22. Kaushik P (2022) Role and application of artiﬁcial intelligence in business analytics: a critical 
evaluation. Int J Glob Acad Sci Res 1(3):1–11. https://doi.org/10.55938/ijgasr.v1i3.15

Machine Learning Algorithms Aided Disease Diagnosis and Prediction …
21
23. Vijayaganth V, Krishnamoorthi M (2022) Soft computing-based ensemble learning model for 
multi-disease classiﬁcation of plant leaves. https://doi.org/10.1080/10106049.2022.2112300 
24. Hasan M, Riana D, Swasono S, Priyatna A, Pudjiarti E, Prahartiwi L (2020) Identiﬁcation of 
grape leaf diseases using convolutional neural network. J Phys: Conf Ser 2020(1641):012007. 
https://doi.org/10.1088/1742-6596/1641/1/012007

Optimized Fuzzy PI Regulator 
for Frequency Regulation of Distributed 
Power System 
Smrutiranjan Nayak, Subhransu Sekhar Dash, Sanjeeb Kumar Kar, 
Ananta Kumar Sahoo, and Ashwin Kumar Sahoo 
Abstract In this article improved fuzzy PI regulator is stated for frequency regula-
tion of Automatic Control of distributed power systems. Originally, a two-region 
nonwarm framework is utilized. The advantage of the stated fuzzy PI regulator 
is shown with the help of contrasting the outputs. All real structure shows non-
straight nature, subsequently, traditional regulators are not generally ready to give 
great and precise outcomes. So fuzzy-logic controller can be utilized to get more exact 
outcomes. The primacy of the stated hybrid particle swarm optimization & pattern 
search (hPSO-PS) approach adjusted fuzzy-PI selector over PS changed fuzzy PI 
selector, PSO changed fuzzy PI selector, hBFOA-PS changed PI selector, Differen-
tial Evolution (DE) changed PI selector and Bacteria Foraging optimization algorithm 
(BFOA) adjusted PI selector is demonstrated. It is seen that the Fuzzy PI regulator 
is more effective for controlling frequency relative to the PI regulator. 
Keywords Nonlinearity · Distributed power system · PI regulator · Integral time 
absolute error · Area control error
S. Nayak envelope symbol · S. K. Kar 
Department of Electrical Engineering, Siksha ‘O’ Anusandhan (Deemed to Be University), 
Bhubaneswar, Odisha, India 
e-mail: smrutikiit40@gmail.com 
S. K. Kar 
e-mail: sanjeebkar@soa.ac.in 
S. S. Dash 
Department of Electrical Engineering, GCE, Keonjhar, Odisha, India 
e-mail: subhransudash_fee@gcekjr.ac.in 
A. K. Sahoo · A. K. Sahoo 
Department of Electrical Engineering, CV Raman Global University, Bhubaneswar, Odisha, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_3 
23

24
S. Nayak et al.
1 
Introducion 
In present systems, the goal of AGC is to keep on the balance in the middle of 
creation & peak loads, therefore reducing the recurrence variation. It additionally 
controls the tie-line-power trade over different control regions consequently guaran-
teeing solid activity of the appropriated power framework [1, 2]. Generation control 
recognizes the organization’s repeat and tie-line ﬂows and replaces the put situa-
tion of the generator’s interior space to stay the time usual of Area-Control-Error 
at the very least worth. As the ACE is changed under zero by the AGC, the two 
recurrence and tie-line power blunders will become zero [3, 4]. In target work 
utilizing Integral-Time-Absolute-Error (ITAE), damping proportion of prevailing 
Eigen-values & settling-time is suggested where the PI regulator boundaries are 
redesigned utilizing Differential-Evolution (DE) calculation & outcomes stood out 
from BFOA and PSO improved ITAE based PI regulator to appear its beneﬁt [5–11]. 
In Artiﬁcial-Bee-Colony calculation is registered to update the PI and PID regulators 
for inter-connected warm nuclear energy framework and shows that better powerful 
execution is accomplished by ABC adjusted regulators differentiated to PSO adjusted 
regulators [12, 13]. Fuzzy-Logic regulator reﬁnes the shut circle execution of PI regu-
lator and can deal with trade in working point or framework boundary by internet 
refreshing the regulator boundaries. Various control loops are operating to carry on 
the system frequency at its set point. Most supply–demand balancing is attained by 
controlling the outputs of dispatchable generating units. Fuzzy PI means the union 
of fuzzy and PI controller. To lessen rise time Proportional gain is used & to continue 
error as little as possible integral gains are used. It is seen that the Fuzzy PI regulator 
is more productive in controlling the frequency relative to the PI regulator. 
2 
System Model and Controller Structure 
Two-locale power systems as given in Fig. 1. The structure of the regulator has 
appeared in Fig. 2. The Area-Control-Errors is given by: 
ACE S ub sc ript  1 Bas
eline equals upper B Subscript 1 Baseline normal upper Delta upper F Subscript 1 Baseline plus normal upper Delta upper P Subscript Tie
ACE S ub sc ript  2 Bas
eline equals upper B Subscript 2 Baseline normal upper Delta upper F Subscript 2 Baseline minus normal upper Delta upper P Subscript Tie
B1 &B2 are frequency bias parameters,ΔF1 &ΔF2 are small changes in frequency 
and ΔPtie is variance in power in the tie-line. 
Three-sided participation capacities are utilized with ﬁve fuzzy phonetic factors 
like negative large, negative little, zero, positive little, and positive enormous for both 
the information sources and the yield. Mamdani fuzzy interface motor is chosen for 
this work [14, 15].

Optimized Fuzzy PI Regulator for Frequency Regulation of Distributed …
25
Fig. 1 Two area’s power-system 
Fig. 2 Structure Fuzzy-Logic selector 
2.1 
Objective Function 
ITAE minimizes the settling time & also reduces the peak overshoot. The main 
outcome is given by 
up pe r J e q
u al
s 
ITAE eq ua ls int eg ral Subscript 
0 Superscript t Baseline left parenthesis StartAbsoluteValue normal upper Delta upper F 1 EndAbsoluteValue plus StartAbsoluteValue normal upper Delta upper F 2 EndAbsoluteValue plus StartAbsoluteValue normal upper Delta upper P Subscript t i e Baseline EndAbsoluteValue right parenthesis period t period dt
So ΔF = small change in frequency, normal upper Delta upper P Subscript Tie = variance in power in tie-line, and t 
= simulation in time.

26
S. Nayak et al.
3 
Overview of Fusion PSO and PS Calculation 
Every one of these calculations is portrayed underneath. The PSO technique is an 
individual from a wide class of a multitude of knowledge strategies for taking care of 
advancement issues. In PSO every molecule endeavor to work on itself by copying 
qualities from its fruitful companions. The area relating to the best wellness is p-
best & the general best out of the multitude of molecules in the populace is called 
g-best.
. All specialists step by step draw near to the worldwide ideal utilizing the various 
headings of p-best and g-best.
. The strategy is registered to the constant issue. Be that as it may, the strategy is 
applied to the separate issue utilizing networks for X–Y position and its speed.
. There is no irregularity in looking through methods regardless of whether constant 
and discrete state factors are used. 
Pattern-Search 
The P-S enhancement procedure is a subsidiary-free transformative calculation to 
tackle an assortment of advancement issues such lie the extent of the quality improve-
ment techniques. It is adaptable and even administrators to improve and adjust 
the neighborhood search. The Pattern-Search estimation processes a succession of 
focuses that might move toward the ideal point. The calculation begins with a bunch 
of focuses called networks, around the underlying focuses. The underlying focuses 
are given by the PSO strategy. The cross-section is made by putting on the current 
highlight a scalar numerous of a bunch of vectors called an example. 
4 
Results and Discussions 
Fuzzy-PI regulators are thought about for each area. The target work is determined in 
the m-document and utilized in the enhancement calculation. With the PI regulator 
structure, a more modest ITAE esteem is gotten with hPSO-PS upgraded fuzzy PI 
regulator contrasted with PSO advanced fuzzy PI regulator and PS enhanced fuzzy 
PI regulator. Recurrence changes and tie-line power change results are displayed in 
Fig. 3 to Fig. 4. Therefore, better framework execution as far as the least settling time 
in recurrence & tie-line-power variations is accomplished for the expressed hPSO-PS 
streamlined fuzzy-PI regulator contrasted with different methodologies.

Optimized Fuzzy PI Regulator for Frequency Regulation of Distributed …
27
Fig. 3 Frequency change of region-1 for 0.1 SL rise in region-1 
Fig. 4 Tie-line power change for 0.1 SL rise in region-1 
5 
Conclusion 
In this article, the fuzzy-PI regulator is stated for programmed age control of many-
region frameworks. At ﬁrst, a two-region nuclear energy framework is thought 
of & the information scaling components and gains of the fuzzy-PI regulator are 
all the while advance utilized. The stated half-and-half procedure exploits the world-
wide investigation potential of PSO and the nearby misuse capacity of PS. The 
primacy of the stated hPSO-PS approach adjusted fuzzy-PI selector over PS ﬁne-
tuned fuzzy PI selector, PSO ﬁne-tuned fuzzy PI selector, hBFOA-PS ﬁne-tuned PI 
selector, DE ﬁne-tuned PI selector, and BFOA ﬁne-tuned PI selector are demon-
strated. Most developments are attained for the stated approach contrasted to some 
recently reported approaches. The expressed methodology is additionally reached out 
to a two-region four-unit aqueous force framework with/without HVDC connects. 
Further, an affectability examination is done to show the power of the regulator. It 
is expressed that hPSO-PS represents proposed approaches and may turn into an 
extremely encouraging calculation for taking care of more mind-boggling designing 
advancement issues in future exploration.

28
S. Nayak et al.
References 
1. Parmar KPS, Majhi S, Kothari DP (2012) Load frequency control of a realistic power system 
with multi-source power generation. Int J Electr Power Energy Syst 42:426–433 
2. Nayak S, Kar SK, Dash SS, Das MC, Swain SC (2022) PIDA regulator for frequency limitation 
of conventional power systems, intelligent systems. Lecture notes in networks and systems, 
vol 431. Springer, Singapore, pp 11–17 
3. Chandrakala KRMV, Balamurugan S, Sankar Narayanan K (2013) Variable structure fuzzy 
gain scheduling-based load frequency controller for the multi-source multi-area hydrothermal 
system. Int J Electr Power Energy Syst 53:375–381 
4. Nayak SR, Kar SK, Dash SS (2021) Performance comparison of the hSGA-PS procedure with 
PIDA regulator in AGC of the power system. In: ODICON-2021, pp 1–4 
5. Ali ES, Abd-Elazim SM (2011) Bacteria foraging optimization algorithm-based load frequency 
controller for an interconnected power system. Int J Electr Power Energy Syst 33:633–638 
6. Nayak S, Kar SK, Dash SS (2021) A Hybrid search group algorithm & pattern search optimized 
PIDA controller for AGC of the interconnected power system. In: ICICA, pp 309–322 
7. Patel NC, Debnath MK, Sahu BK, Dash SS, Bayindir R (2018) Multi-staged PID controller 
tuned by an invasive weed optimization algorithm for LFC issues. In: 7th international 
conference on renewable energy research and applications 
8. Nayak S, Kar SK, Dash SS (2022) Change detection ﬁlter technique of HVDC transmission 
link fed by a wind farm. In: 4th international conference on energy, power, and environment 
(ICEPE), pp 1–6 
9. Parida SM, Rout PK, Kar SK (2020) Fuzzy multi-objective approach-based small signal 
stability analysis and optimal control of a PMSG-based wind turbine. Int J Comput Aided 
Eng Technol 12(4):513–534 
10. Nayak S, Dash SS, Kar SK (2021) Frequency regulation of hybrid distributed power 
systems integrated with renewable sources by optimized type-2 fuzzy PID controller. In: 9th 
international conference on smart grid, smart-grid, pp 259–263 
11. Padhi JR, Debnath MK, Pal S, Kar SK (2019) AGC investigation in wind-thermal-hydro-
diesel power system with 1 plus fractional order integral plus derivative controller. Int J Recent 
Technol Eng 8(1):281–286 
12. Nayak S, Kar SK, Dash SS, Vishnuram P, Thanikanti SB, Nastasi B (2022) Enhanced salp 
swarm algorithm for multimodal optimization and fuzzy based grid frequency controller design. 
Energies 15(9):3210 
13. Nayak S, Kar SK, Dash SS, Das MC (2022) Synchronization and its use in communication 
network with frequency control, intelligent systems. Lecture notes in networks and systems, 
vol 431. Springer, Singapore, pp 19–29 
14. Sahoo BP, Panda S (2018) Improved grey wolf optimization technique for fuzzy aided PID 
controller design for power system frequency control. Sustain Energy Grids Netw 278–299 
15. Khamari D, Sahu RK, Panda S (2019) Application of search group algorithm for automatic 
generation control of interconnected power system. In: Computational intelligence in data 
mining. Springer, Singapore, pp 557–568

Detecting Depression Using 
Quality-of-Life Attributes with Machine 
Learning Techniques 
J. Premalatha, S. Aswin, D. JaiHari, and K. Karamchand Subash 
Abstract Worldwide, depression affects millions of individuals even without their 
knowledge and is a crippling afﬂiction. Primary care physicians frequently discover 
that they must treat mental health problems like depression despite having little 
or no formal training in how to do so. There is proof that an integrated strategy, 
where doctors regularly screen patients for mental health issues and collaborate with 
psychologists and other mental health specialists to treat patients, results in lower 
costs and improved patient outcomes. In order to handle and study the heterogeneous 
data and understand the correlation between aspects of quality of life and depression, 
this paper uses machine learning techniques. Machine learning is used to predict 
people who might have depression based on data that is found in CDC National 
Health and Examination Survey (NHAES) website. These forecasts could be used 
to more quickly and easily connect patients with qualiﬁed mental health specialists. 
Keywords Depression · Afﬂiction · Quality of life · CDC NHAES 
1 
Introduction 
No matter the situation of a nation, healthcare is one of the biggest issues that is 
faced by almost every nation. Smart and effective healthcare systems are consid-
ered as the most important thing in enhancing the quality of life globally. Finding 
a patient’s mental health issues remains difﬁcult for healthcare organizations and
J. Premalatha · S. Aswin · D. JaiHari · K. Karamchand Subash envelope symbol
Department of IT, Kongu Engineering College, Erode, India 
e-mail: Karamchandsubashk.19it@kongu.edu 
J. Premalatha 
e-mail: jprem@kongu.ac.in 
S. Aswin 
e-mail: Aswins.19it@kongu.edu 
D. JaiHari 
e-mail: Jaiharid.19it@kongu.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_4 
29

30
J. Premalatha et al.
doctors, especially with younger patients. The ability of machine learning and deep 
learning to identify people’s psychiatric problems and understand the effects of those 
disorders on lifestyle has recently been demonstrated. The phrase “quality of life” 
highlights various aspects of a person’s existence, including their emotional, phys-
ical, and psychological health. These characteristics explain how an individual expe-
riences life, and various researchers and health professionals are taking them into 
consideration. Machine learning has the capacity to search for Quality-of-Life (QoL) 
variables in a wider method in order to identify the relationship between depression 
and QoL aspects. One of the most dynamic subﬁelds of artiﬁcial intelligence is 
machine learning. Its learning approach is allegedly employed in various intelligent 
environments, including self-driving cars, healthcare speech recognition services, 
and it also offers recommendations based on google searches. This paper mainly 
focuses in healthcare perspective. Below mentioned are the important contributions 
of this paper: 
• For the purpose of integrating the heterogeneous data, a data consolidation 
technique is developed. 
• Along with more established statistical methods, data pertaining to quality of life 
is examined through machine learning techniques. 
2 
Related Work 
M Masood Habib et al. (2022) [1] describes an experimental study to investigate 
the relationship between depression and variables linked to quality of life. Several 
machine learning methods were used to conduct the investigation. The consolidation 
strategy provided a foundation for the development and veriﬁcation of the research 
idea. 
L. Castelli et al. (2020) [2] assessed the results indicating a common agreement 
between the MADRS and the HADS (K-test: 0.44), however selecting a cut-off of 
11 for the HADS resulted in a much higher underestimating of depressive patients. 
There were 151 participants in the research with mixed cancer pathology. 
M. Milic et al. (2020) [3] compared the effects of smoking on students’ health in 
two different university contexts. When sociodemographic, behavioral, and health 
characteristics were taken into account, smoking was linked to lower Mental 
Composite Scores (MCS) and Physical Composite Scores (PCS). 
Ibrahim Aljarah et al. (2019) [4] suggested a hybrid strategy built on the 
Grasshopper optimization method (GOA). A recent algorithm called GOA was moti-
vated by the biological behaviour of swarms of grasshoppers. The proposed strategy 
aims to improve the SVM model’s parameters. 
I.C. Passos et al. (2019) [5] looked into the research on use of big data techniques 
for diagnosing and treating mental illness. These includes discussion of different 
types of mental illnesses such as anxiety, depression, and personality problems. The 
impact of users’ mental health on behaviours like drug abuse and suicidal thoughts 
is highlighted (Fig. 1).

Detecting Depression Using Quality-of-Life Attributes with Machine …
31
Fig. 1 Proposed methodology 
3 
Proposed Methodology 
Different quality-of-life datasets was collected from National Health and Nutri-
tion Examination Survey (https://www.cdc.gov/nchs/nhanes/index.htm) such as drug 
usage, occupation, alcohol usage and sleep disorders. During this study, the datasets 
collected through past 7 years under each quality-of-life attribute were separately 
assessed to identify the factors that work effectively with fresh new, untested data. In 
this paper, the methods that were used to develop the models for predicting depression 
in a technical manner is explained. 
3.1 
Data Pre-processing 
Data Cleaning – In data analytics, data cleaning is crucial because it eliminates 
unused and noisy data. Datasets are checked for null values and ambiguous values 
are replaced with null values. This is carried out for all the quality-of-life attributes 
datasets.

32
J. Premalatha et al.
3.2 
K-means Clustering 
The clustering issues in machine learning or data science are resolved using the 
unsupervised learning algorithm K-Means Clustering. The data will be clustered 
using K-Means and added as a feature for modelling. It is hoped that by using this 
method, the model will gain some accuracy without the need for additional data 
collection. 
3.3 
Classiﬁcation 
Three different classiﬁcation algorithms are used in this paper: 
Random Forest - The subsection’s discussion of the decision tree’s overﬁtting issue 
shows how random forests can ﬁx this issue. It is an ensemble made up of numerous 
independent decision trees working together. The core tenet of random forest is 
straightforward but effective. The class with the most votes determines the forecast 
for our model. 
Support Vector Machine - The SVM approach determines the best decision 
boundary or solution line for categorising n-features. As a result, we can group 
the points in the new dataset appropriately. The best boundary is referred to as a 
hyperplane. 
Logistic Regression - The likelihood of the target attribute is ascertained using the 
supervised approach technique of LR. Ordinal, interval, or ratio-level independent 
variables are either true or false, 0 or 1, etc. as a result of this regression. The 
distinction between a dependent variable and additional independent variables is 
described and explained using LR. 
4 
Results and Discussion 
4.1 
Parameters for Evaluation 
Three prediction models were used in this experiment to predict depression. Here, 
Random Forest, logistic regression, and SVM are employed as classiﬁcation algo-
rithms. The parameters that were considered are Precision, Recall, F1-Score and 
Accuracy. 
Precision - Precision is the ratio of appropriately assessed samples tested to all 
favorably classiﬁed samples.

Detecting Depression Using Quality-of-Life Attributes with Machine …
33
Precision equals T P sl
ash TP plus FP
TP: True Positive - Observation rightly detected as positive. 
FP: False Positive - Observation wrongly detected as positive. 
Recall - The percentage of Positive samples that were accurately labelled as Positive 
relative to all Positive samples is how the recall is calculated. The recall increases as 
more positive samples are found. 
Recall equals T P sl
ash TP plus FN
FN: False Negative - Observation wrongly detected as negative. 
F1-Score – Mean of precision and recall. 
upp er F 1 m inus  Score equ al s 2 asteris k left par en thesis Rec
all asterisk Precision right parenthesis divided by left parenthesis Recall plus Precision right parenthesis
Accuracy - The accuracy performance metric, which is just the ratio of rightly 
predicted observations to total observations, is the simplest to comprehend. If our 
model is correct, then it must be the best, right? Accuracy is a great indication, but 
only if the dataset’s false positive and false negative rate values are about comparable. 
As a result, while evaluating the performance of the model, additional things must 
be taken into account. 
Accuracy eq ual s TP plu s TN slash  TP 
plus FP plus FN plus TN
TN: True Negative - Observation rightly detected as negative. 
4.2 
Performance of Random Forest 
See Figs. 2 and 3.
4.3 
Performance of Support Vector Machine 
See Figs. 4 and 5.
4.4 
Performance of Logistic Regression 
See Figs. 6 and 7.

34
J. Premalatha et al.
Fig. 2 Confusion 
matrix – Random Forest 
Fig. 3 ROC 
Curve – Random Forest
Fig. 4 Confusion 
matrix – SVM

Detecting Depression Using Quality-of-Life Attributes with Machine …
35
Fig. 5 ROC Curve – SVM
Fig. 6 Confusion 
matrix – LR 
Fig. 7 ROC Curve – LR

36
J. Premalatha et al.
Table 1 Comparing three algorithms 
Algorithm
Accuracy (%)
Precision (%)
Recall (%)
F1-Score (%) 
SVM
75
97
76
85 
RF
89
94
94
94 
LR
75
97
97
85 
The Table 1 shows the accuracy, precision, recall and F1-Score of various classiﬁer 
models. It indicates that the maximum accuracy was achieved by the Random Forest 
classiﬁcation model. 
5 
Conclusion 
It is not unexpected that predicting depression is a challenging subject to model 
because it is a complicated, multifaceted problem. It could be possible to ﬁnd a tactic 
that would be ideal for this assignment by looking at additional model kinds. The 
apparent possibility in pursuing this work would be to conduct additional feature 
evaluation to eliminate those that are not useful and perhaps experiment to see if new 
features might be introduced that would turn out to be beneﬁcial. It’s obviously easier 
said than done to strike the correct balance between the acceptable level of inaccuracy 
and the amount of data required for an accurate model. An alternate scoring measure, 
such as maximising simply for recollection or weighting recall much more highly in 
a customised F-statistic scoring item, could possibly be used because the depressed 
class was so challenging to categorise. We observed that Logistics Regression and 
Support vector machine gave almost same results but based on accuracy, the random 
forest algorithm can be given higher than others. However, for a higher number of 
features in the feature set, Random Forest and decision tree are better options. Finally, 
we conclude that for detecting depression Random Forest Algorithm is more accurate 
than the support vector machine and logistic regression. 
References 
1. Habib M, Wang Z, Qiu S, Zhao H, Murthy AS (2022) Machine learning based healthcare system 
for ınvestigating the association between depression and quality of life. J Biomed Health Inform 
26(5):1–12 
2. Qiu S et al (2022) Multi-sensor information fusion based on machine learn ing for real appli-
cations in human activity recognition: state-of-the-art and research challenges. Inf Fusion 
80:241–265 
3. Daniel SC, Azuero A, Gutierrez OM, Heaton K (2021) Examining the relationship between 
nutrition, quality of life, and depression in hemodialysis patients. Qual Life Res 30(3):759–768

Detecting Depression Using Quality-of-Life Attributes with Machine …
37
4. Hazarika A, Abraham A, Kandar D, Maji AK (2021) An im proved lenet-deep neural network 
model for Alzheimer’s disease clas siﬁcation using brain magnetic resonance images. IEEE 
Access 9:161194–161207 
5. Niu S, Liu M, Liu Y, Wang J, Song H (2021) Distant domain transfer learning for medical 
imaging. IEEE J Biomed Health Informat 25(10):3784–3793 
6. Riemann D, Krone LB, Wulff K, Nissen C (2020) Sleep, insomnia, and depression. 
Neuropsychopharmacology 45(1):74–89 
7. Darimont T, Karavasiloglou N, Hysaj O, Richard A, Rohrmann S (2020) Body weight and 
self-perception are associated with depression: Results from the national health and nutrition 
examination survey (NHANES) 2005–2016. J Affect Disorders 274:929–934 
8. Cahuas A, He Z, Zhang Z, Chen W (2020) Relationship of physical activity and sleep with 
depression in college students. J Amer College Health 68(5):557–564 
9. Castelli L, Torta R, Mussa A, Caldera P, Binaschi L (2020) Fast screening of depres-sion in 
cancer patients: the effectiveness of the HADS. Eur J Cancer Care 20(4):528–533 
10. Milic M et al (2020) Tobacco smoking and health-related quality of life among university 
students: Mediating effect of depression. PLoS ONE 15(1):1–18 
11. Kim SY et al (2020) Gender and age differences in the association between work stress and inci-
dent depressive symptoms among Korean employees: a cohort study. Int Arch Occup Environ 
Health 93(4):457–467 
12. Kandola A, Ashdown-Franks G, Hendrikse J, Sabiston CM, Stubbs B (2019) Physical activity 
and depression: towards un derstanding the antidepressant mechanisms of physical activity. 
Neurosci Biobehavioral Rev 107:525–539 
13. Francis HM, Stevenson RJ, Chambers JR, Gupta D, Newey B, Lim CK (2019) A brief diet 
intervention can reduce symptoms of depression in young adults – a randomised controlled 
trial. PLoS ONE 14(10):1–17 
14. Dong Y, Dragut EC, Meng W (2019) Normalization of duplicate records from multiple sources. 
IEEE Trans Knowl Data Eng 31(4):769–782 
15. Passos IC, Ballester P, Pinto JV, Mwangi B, Kapczinski F (2019) Big data and machine learning 
meet the health sciences. In: Personalized psychiatry, vol 81. Springer, Cham, pp 1–13 
16. Shatte ABR, Hutchinson DM, Teague SJ (2019) Machine learning in mental health: a scoping 
review of methods and applications. Psychol Med 49(9):1426–1448 
17. Aljarah I, Al-Zoubi AM, Faris H, Hassonah MA, Mirjalili S, Saadeh H (2018) Simultaneous 
feature selection and support vector machine optimization using the grasshopper optimization 
algorithm. Cogn Comput 10(3):478–495 
18. Wolohan JT, Hiraga M, Mukherjee A, Sayyed ZA (2018) Detecting linguistic traces of depres-
sion in topic-restricted text: attending to self-stigmatized depression with NLP. In: Workshop, 
pp 11–21 
19. Chen S, Conwell Y, Cerulli C, Xue J, Chiu HFK (2018) Primary care physicians’ perceived 
barriers on the management of depression in China primary care settings. Asian J Psychiatry 
36:54–59 
20. González-Blanch C, Hernández-de-Hita F, Muñoz-Navarro R, Ruíz-Rodríguez P, Medrano 
LA, Cano-Vindel A (2018) The association between different domains of quality of life and 
symptoms in primary care patients with emotional disorders. Sci Rep 8(1):11180 
21. Dwyer DB, Falkai P, Koutsouleris N (2018) Machine learning approaches for clinical 
psychology and psychiatry. Annu Rev Clin Psychol 14(1):91–118 
22. Ledesma S, Ibarra-Manzano MA, Cabal-Yepez E, Almanza-Ojeda DL, Avina-Cervantes JG 
(2018) Analysis of data sets with learning conﬂicts for machine learning. IEEE Access 6:45062– 
45070 
23. Srividya M, Mohanavalli S, Bhalaji N (2018) Behavioral modeling for mental health using 
machine learning algorithms. J Med Syst 42:1–12 
24. Yazdavar AH et al (2017) Semi-supervised approach to monitoring clinical depressive symp-
toms in social media. In: Proceedings of IEEE/ACM international conference on advances in 
social networks analysis and mining, pp 1191–1198

Patient Satisfaction Through 
Interpretable Machine Learning 
Approach 
S. Anandamurugan, P. Jayaprakash, S. Mounika, and R. Narendranath 
Abstract In Patient satisfaction, the most important factor in assessing the quality is 
patient happiness. The happiness key factor impacts the health policy decisions. An 
individual’s speciﬁc health requirements, individualised treatment, and desired health 
results are of the utmost importance in the period of patient-centered care. Across 
the past decade, treatment delivery, management, and reimbursement practices have 
all been impacted by patient satisfaction as a clear insight and quality management 
of patient experiences. Using machine learning algorithms, the most relevant factors 
for patient satisfaction are founds. 
Keywords Synthetic minority oversampling technique · Random forest ·
XGBoost · K-nearest neighbour · Support vector machine · Patient satisfaction 
1 
Introduction 
In healthcare industry one important factor in assessing the quality of healthcare is 
patient happiness. The key factors like cleanliness, doctor’s experience, exact diag-
nosis, modern equipments, etc. inﬂuences health policy decisions. An individual’s 
speciﬁc health requirements, individualized treatment, and desired health results are 
of the utmost importance in the period of patient-centered care. The Centers for 
Medicare & Medicaid Services introduced the Hospital Value-Based Purchasing 
programme (CMS). Patient happiness is one of these criteria, and poor performance
S. Anandamurugan · P. Jayaprakash · S. Mounika envelope symbol · R. Narendranath 
Kongu Engineering College, Erode, India 
e-mail: mounikas.19it@kongu.edu 
S. Anandamurugan 
e-mail: samurugan@kongu.ac.in 
P. Jayaprakash 
e-mail: jayaprakashp.19it@kongu.edu 
R. Narendranath 
e-mail: narendranathr.19it@kongu.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_5 
39

40
S. Anandamurugan et al.
on it can put hospitals at greater ﬁnancial risk. To reduce the ﬁnancial risks faced by 
the hospitals they need to focus on improving the factors which affects the patient 
satisfaction. Choosing the most relevant factors for patient satisfaction is the most 
confusing one. In past years, many methods were used to know the most important 
factors but most of them failed. To ﬁnd most relevant factors machine learning algo-
rithms are used here. The results were interpreted and best factors were taken into 
account. 
2 
Related Work 
Eunbi kim (2022) and colleagues created predicted models for hospitalisation based 
on machine learning, including as XGBoost. Emergency department (ED) over-
crowding has long been an issue for the safety and satisfaction of patients worldwide. 
Overcrowding is frequently brought on by delays in ED patients’ boarding times as 
they wait for inpatient beds. Patients’ hospitalizations can be expected in EDs early 
enough to allow for the preparation of inpatient beds and a shorter waiting period. 
Jing Yu et al. [1] designed a method to boost medical effectiveness and patient 
happiness, suggested the use of a multi-patient treatment modality (MTM). The 
patients were categorized based on common disease symptoms. Thereafter selected 
the best patient matching method from the groups. The development of a math-
ematical model of the DPCMP involved the application of numerous ant colony 
optimization approaches. techniques were improved. methods were created, in order 
to address the aforementioned issue. 
The use of applied regression models to measure patient satisfaction as well as 
correlation approaches is one of the crucial considerations made by G. Sabarmathi 
et al. [4]. When developing the higher quality of health care application models. It 
helps in coming to a decision by taking into account the workplace and administrative 
traits relevant to patient satisfaction. 
Gavin Tsang [3] suggested a methodology for chronic, incurable diseases like 
dementia. A machine learning technique called ensemble deep neural networks 
(ECNN) uses entropy regularisation to provide high hospitalisation prediction accu-
racy for dementia patients while also making it possible to understandably analyse 
the model architecture using heuristics that can spot speciﬁc features that matter 
within a broad feature domain space. 
Berk Ustun et al. (2016) developed a novel technique called as Supersparse Linear 
Integer Model for developing data-driven scoring systems (SLIM). Due to the direct 
control, it has over these parameters, SLIM can build acceptable models without 
parameter adjustment while accommodating a variety of accuracy and sparsity-
related operational constraints. We establish restrictions on constraints on the SLIM 
scoring systems’ testing and training accuracy as well as a cutting-edge data reduc-
tion technique which can increase scalability by getting rid of some of the training 
data in advance.

Patient Satisfaction Through Interpretable Machine Learning Approach
41
The three essential elements of explainable machine learning were assessed by 
Ribana Roscher et al. (2020) in light of applications in the natural sciences. such 
as transparency, interpretability, and explainability. Regarding these fundamental 
concepts, they included an overview of contemporary scientiﬁc studies that use 
machine learning as well as examples of how explainable machine learning is 
combined with application-speciﬁc domain knowledge. 
Based on the positive assessments of the nurses’ work performance in the hospi-
tals and the comfort of the patients, Salman Alsaqri [2] claimed that this study 
had demonstrated a very high level of nursing care quality. Results of the relation-
ship between patient satisfaction and ward size, past health, and marital status were 
shown to be statistically signiﬁcant. Signiﬁcant correlations between the means of 
admission, past admission, and staff nurses’ work performance were also discovered. 
Youness Frichi et al. identiﬁed the scope of hospital logistics includes a number of 
issues that affect patient satisfaction, including as waiting times, hospitality services, 
and healthcare workers’ satisfaction with their jobs, as well as the availability of 
resources and effective planning. The creation of an association matrix that estab-
lishes the dependent relationship between satisfaction criteria in healthcare facili-
ties and hospital logistics activities sheds further light on the relationship between 
satisfaction and hospital logistics. 
Athar Mohd et al. suggested that patients’ satisfaction should be regularly assessed 
because it is a useful indicator of the quality of healthcare. The objective was to 
evaluate and compare the degree of patient satisfaction in a hospital’s outpatient 
department. The outpatient department (OPD) is the hospital’s ﬁrst point of contact 
with patients and acts as a storefront for all community-based healthcare services. 
3 
Proposed System 
The proposed methodology presented here will help the hospitals to know what are 
the places they need to improve to gain patient’s satisfaction using machine learning. 
Choice of data is crucial for choosing meaningful documents for analysis and to 
acquire and generate useful knowledge. The dataset was downloaded from the kaggle 
website to study about the patient satisfaction. The acquired dataset was cleared of any 
potential irregular data and pre-processed with several methods followed by feature 
selection and model building. The classiﬁers employed here were SVM, Random 
Forest, XG boost and KNN. The performances of all the classiﬁers were assessed 
based on the accuracy scores, confusion matrices, and classiﬁcation report. The work 
ﬂow diagram is shown in the following Fig. 1

42
S. Anandamurugan et al.
Fig. 1 Proposed System 
3.1 
Dataset Description 
The dataset was downloaded from kaggle data repository. The link for the dataset is 
https://www.kaggle.com/datasets/vdimitrievska/patient-satisfaction-dataset. In this  
dataset, patients answered to the questions asked in an online survey which was 
conducted by an organization. The data is aggregated online for a period of 3 months 
using Likert scale from 1–5 scores. This dataset is in CSV format. There were 453 
records. More speciﬁcally there are 17 attributes namely
. Satisfaction in overall
. Time wating
. Check up appointment
. Admin procedures
. Time of appointment
. Hygiene and cleaning
. Quality/experience of Doctor
. Communication with Doctor
. Specialists available
. Exact diagnosis
. Friendly health care workers
. Modern equipment
. Lab services
. Waiting Rooms

Patient Satisfaction Through Interpretable Machine Learning Approach
43
. Availability of Drugs
. Parking, Playing rooms & Caffes
. Hospital rooms quality 
3.2 
Data Pre-processing 
If the data in dataset is in wrong format or having missing values leads to fault in 
classiﬁcation. The format of the data in machine learning projects must be correct in 
order to get better results from the applied model. For instance, null values are not 
accepted as input for the Random Forest method., hence null values must be handled 
from the original raw dataset in order to run the Random Forest model. Some speciﬁc 
Machine Learning models require data in a speciﬁc format. 
Rows or columns with all cells having null values can be removed. The loss of 
data can be dealt using this strategy. 
Snake case is a writing style in which the ﬁrst letter of each word is written in 
lowercase and each whitespace is substituted by the underscore (_) character. It is 
a naming convention that is frequently used in computers, such as for ﬁlenames, 
variables, and subroutine names so that we can access easily. 
3.3 
Balancing Dataset with SMOTE 
The synthetic minority oversampling technique or SMOTE, is one of the most popular 
methods of oversampling to handle the imbalance problem. It attempts to balance 
the distribution of classes by randomly replicating examples of minority classes. 
SMOTE synthesises previously existing minority instances to produce new minority 
instances. Since the dependent variable was in unbalanced state we used SMOTE 
here to oversample it in a balanced way. The process involved in SMOTE:
. It ﬁrst identiﬁes the minority class vector.
. Then it decides the k (nearest neighbours to be chosen) value. It uses k-NN to ﬁnd 
the nearest neighbours.
. Then compute the line between the chosen data points based on k and create the 
new point in the link.
. The previous step is repeated for all minority data points till the dataset is balanced. 
3.4 
Feature Selection 
Feature selection is one of the major part in machine learning. Since the input variable 
and output variable both of them were categorical values here we used Chi-square. 
Using the Chi-square approach, categorical features in a dataset were assessed. The

44
S. Anandamurugan et al.
Chi-square between each feature and the dependent variable was calculated and the 
features with the highest Chi-square scores were then selected. This test’s objective 
is to determine whether a disparity between observed and anticipated data was due 
to chance or a correlation between the variables. Formula for Chi-square is χ^2 =
Σ(O-E)^2/E. Here in this formula the square difference between the observed value 
O and expected value E and then divided by expected value E to obtain the Chi-square 
score. 
4
Models
 
XGBoost Classiﬁer 
XGBoost (Extreme Gradient Boosting) algorithm designed for supervised learning 
tasks. XGBoost is an ensemble learning technique, which means it makes predictions 
by combining the output of numerous base learners (models). It comes under the type 
boosting. XGBoost employs DTs as their base learners, just like Random Forests. 
Then, a robust and precise model is created by combining these various classiﬁers/ 
predictors. It has numerous parameters so it can be altered to see which gets better 
accuracy. 
SVM Classiﬁer 
SVM is a popular and effective machine learning technique. The SVM approach 
appears to determine the best decision boundary or solution line for categorising n-
features. The best choice boundary is referred to as a hyperplane. The support vector 
machine, which chooses the high data points, creates the hyperplane. But generally 
SVM doesn’t support multiclass classiﬁcation. So to do that we used OVR (One Vs 
Rest). Here 3 classes which means 1 class vs other class the hyperplane is drawn. 
Random Forest Classiﬁer 
It is an ensemble classiﬁer. It comes under the type Bagging. The class with the 
most votes determines the forecast for our model. In a conventional decision tree, 
just one decision tree was produced. Several decision trees were produced during the 
random forest process. Two criteria were used to classify data frames: Variables and 
Observation. When a large number of decsion trees are produced and used, a forest 
was established. In this Random Forest we used 100 Decision trees and some other 
parameters as input to predict the outcome. 
KNN Classiﬁer 
The K-nearest Neighburs (KNN) technique is a kind of supervised machine learning 
algorithm that may be applied to challenges involving classiﬁcation and regression 
predictive modelling. The next two characteristics would suitably characterise KNN: 
Lazy learning algorithm - KNN uses all of the data for training while classifying, 
making it a lazy learning algorithm since it lacks a dedicated training phase. 
Non-parametric learning algorithm - A non-parametric learning algorithm, KNN 
makes no assumptions regarding the underlying data.

Patient Satisfaction Through Interpretable Machine Learning Approach
45
5 
Results and Discussion 
The dataset contains 17 variables and among them 5 signiﬁcant features that were 
helpful in evaluating the system were chosen. These features represent the expected 
characteristics resulting in patient satisfaction. If all the features were taken into 
account, the creator receives a less efﬁcient system. Attribute selection was carried 
out to improve efﬁciency. In this case, 5 characteristics were chosen in order to 
evaluate the model that provides greater accuracy. Some dataset features had virtually 
equal correlations, so they were eliminated. The efﬁciency signiﬁcantly declines if 
all the features in the dataset were considered. 
Before using classiﬁer models, SMOTE was used here because the dataset was 
imbalanced. This oversampling was done because of very low number of records 
present. If the dataset was heavily imbalanced then it affects the model by reducing 
accuracy. We used four machine learning models here and interpreted their perfor-
mance and results. XGBoost and SVM more or less gave the similar performance 
score. Since K-Nearest Neighbors is based on grouping the individual data points, 
it didn’t give that much expected result. Probably because of the low number of 
records, Random Forest also didn’t give expected result. After using machine learning 
approach for training and testing, we discovered that the XGBoost algorithm gave 
higher accuracy when compared to other algorithms. The confusion matrix for each 
algorithm was used for the conclusion. In this case, the number count for TP, TN, FP, 
and FN was provided and value was obtained using the accuracy equation. The accu-
racy of each of the four machine learning techniques is evaluated, from which one 
prediction model is created. Therefore, the objective was to employ a variety of eval-
uation metrics, such as the confusion matrix, accuracy, precision, recall, and f1-score, 
which effectively predicted patient satisfaction. The extreme gradient boosting clas-
siﬁer has the highest accuracy of 89.66% when compared to the other three and the 
confusion matrix for each algorithm was shown below (Figs. 2, 3, 4, 5 and Table 1). 
Fig. 2 Confusion 
Matrix – Random Forest

46
S. Anandamurugan et al.
Fig. 3 Confusion 
Matrix – XGBoost 
Fig. 4 Confusion 
Matrix – SVM 
Fig. 5 Confusion 
Matrix – KNN

Patient Satisfaction Through Interpretable Machine Learning Approach
47
Table 1 Comparing four classiﬁers 
Classiﬁers
Accuracy (%)
Precision (%)
Recall (%)
F1-Score (%) 
XGBoost
89.66
90
90
90 
SVM
88.97
89
89
89 
Random Forest
85.52
86
86
85 
KNN
84.83
86
85
84 
6 
Conclusion and Future Works 
Using supervised machine learning techniques, they were concentrated on improving 
predictive models to achieve better accuracy in predicting and also to ﬁnd successful 
attributes. It’s obviously easier said than done to strike the correct balance between 
the acceptable level of inaccuracy and the amount of data required for an accurate 
model. In SVM, the accuracy was more or less equals to the XGBoost classiﬁer. 
In Random Forest and KNN the accuracy, precision, recall and f1-score were all 
less compared to SVM and XGBoost. So XGBoost had the amazing capacity to raise 
categorization and forecasting precision. Thus, XGBoost model gave higher accuracy 
than the other three algorithms compared here with the accuracy of 89.66%. 
For future works, to improve accuracy various modern models can be used. Here 5 
attributes were selected based on chi-square test. So various feature selection can also 
be used to select even more attributes which may tend to increase the prediction even 
more accurately for patient satisfaction. But sometimes lesser attributes have more 
chance of gaining accuracy. It depends on the dataset. Furthermore, Randomized 
Search can be used to improve the performance of the classiﬁcation models.

48
S. Anandamurugan et al.
References 
1. Yu J, Xing L, Tan X, Ren T, Li Z (2019) Doctor-patient combined matching problem and its 
solving algorithms. IEEE Access 7:177723–177733 
2. Alsaqri S (2016) Patient satisfaction with quality of nursing care at governmental hospitals, 
Ha’il City, Saudi Arabia. J Biol Agric Healthc 6(10):128–142 
3. Tsang G, Zhou SM, Xie X (2020) Modeling large sparse data for feature selection: hospital 
admission predictions of the dementia patients using primary care electronic health records. 
IEEE J Transl Eng Health Med 9:1–13 
4. Sabarmathi G., Chinnaiyan R (2019) Reliable machine learning approach to predict patient satis-
faction for optimal decision making and quality health care. In: Proceedings of the fourth inter-
national conference on communication and electronics systems (ICCES 2019). IEEE Xplore. 
ISBN 978-1-7281-1261-9

Predicting the Thyroid Disease Using 
Machine Learning Techniques 
Lalitha Krishnasamy, M. Aparnaa, G. Deepa Prabha, and T. Kavya 
Abstract An endocrine gland that is allocated in the front of the neck is called 
the thyroid, which produces thyroid hormones as its main job. Thyroid hormone 
may be produced insufﬁciently or excessively as a result of its potential malfunc-
tion. There are various thyroid types including Hyperthyroidism, Hypothyroidism, 
Thyroid Cancer Thyroiditis, swelling of the thyroid. A goiter is an enlarged thyroid 
gland. When your thyroid gland produces more thyroid hormones than your body 
requires, you have hyperthyroidism. When the thyroid gland in our body doesn’t 
provide enough thyroid hormones, then our body has hypothyroidism; when you 
have euthyroid sick, your thyroid function tests during critical illness taken in an 
inpatient or intensive care setting show alterations. Hypothyroid, hyperthyroid, and 
euthyroid conditions are expected from these thyroid conditions. The Three similarly 
used machine learning algorithms are: Support Vector Machine (SVM), Logistic 
Regression, and Random Forest methods, were evaluated from among the various 
machine learning techniques to forecast and evaluate their performance in terms of 
accuracy. Random forest can perform both regression and classiﬁcation tasks. 
Logistic Regression is used to calculate or predict the probability of a binary 
(yes/no) event occurring. SVM classiﬁers offers great accuracy and work well 
with high dimensional space. A thyroid data set from Kaggle is used for this. This 
study has demonstrated the use of SVM, logistic regression, and random forest as 
classiﬁcation tools, as well as the understanding of how to forecast thyroid disease.
L. Krishnasamy 
Department of Computer Science and Engineering, School of Engineering and Technology, 
Christ (Deemed to be) University, Kengeri Campus, Bengaluru 560074, India 
e-mail: vrklalitha24@gmail.com 
M. Aparnaa envelope symbol · G. Deepa Prabha · T. Kavya 
Department of Information Technology, Kongu Engineering College, Erode, Tamil Nadu, India 
e-mail: aparnaam.19it@kongu.edu 
G. Deepa Prabha 
e-mail: deepaprabhag.19it@kongu.edu 
T. Kavya 
e-mail: kavyat.19it@kongu.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_6 
49

50
L. Krishnasamy et al.
1 
Introduction 
The healthcare sector uses computational biology advancements. It made it possible 
to gather patient data that had been saved to forecast medical diseases. Various intel-
ligent prediction algorithms are available to diagnose a disease in its early stages. 
The medical information system contains a variety of data types, but no advanced 
algorithms exist that can quickly analyze disorders. Machine learning algorithms 
signiﬁcantly increase the size of a prediction model over time by dealing with chal-
lenging and complex tasks. Any illness prediction model needs to have characteristics 
that can be quickly and accurately utilized to classify healthy patients using data from 
a range of datasets.. Otherwise, misclassiﬁcation can force a healthy patient to receive 
needless care. As a result, it’s important to anticipate any diseases that might develop 
in addition to thyroid issues. 
The thyroid is a neck-based endocrine gland. It develops in the more constrained 
region called the neck in the human body, beneath the Apple of Adams, and helps the 
thyroid gland produce thyroid hormones that affect how quickly proteins are synthe-
sized and how quickly metabolism occurs. The body’s metabolism is controlled by 
thyroid hormones in a variety of ways, including by monitoring the rate at which the 
heart beats and the rate at which calories are expended. 
In order to determine if a patient has hypothyroidism, hyperthyroidism, or euthy-
roid disease, this study suggests a method based on machine learning techniques that 
takes advantage of thyroid hormone measurements as well as other clinical informa-
tion about the patient. A simple classiﬁcation of hyperthyroidism, hypothyroidism, 
and euthyroid is made possible with the aid of the three proposed algorithms. Finally, 
measure the evaluation accuracy increases with time built model training set. On the 
basis of the patient’s historical and present data, the author of the study [5] utilized 
it to forecast the treatment trend of thyroid illness. Gradient Boosting Classiﬁer, 
Decision Tree, and Nave Bayes are the algorithms that were utilized to predict the 
course of treatment. According to the patient’s tumor kind and stage at the time of 
diagnosis, the study in [8] describes the treatment for thyroid cancer in patients. 
The main objective of this paper is to identify various types of thyroid diseases in 
patients using machine learning techniques like SVM, Random Forest and Logistic 
regression. 
2 
Related Work 
Multivariable Logistic Regression was suggested by Machen et al. [1] as  a way  
to predict the cure which is biochemical for patients affected with medullary 
thyroid cancer. The most current data demonstrates that after the compartment-
oriented surgery done for node-positive Medullary Thyroid Cancer (MTC), the 
number of node metastases inﬂuences the biochemical cure. the challenge of elimi-
nating cancers that have been surgically removed yet are still present in a scarred area

Predicting the Thyroid Disease Using Machine Learning Techniques
51
that has been surgically removed yet are still present in a scarred area. following an 
initial operation may be reﬂected in the drawback of repeat surgery as a predictor of 
failure to obtain the biochemical cure. In comparison to patients who did not achieve 
a biochemical cure, patients who underwent surgery had a centroid of 2 versus 16 
metastases during the initial procedure and 4 versus 12 metastases at the subsequent 
surgery. 
In research [2] published by Gyanendra Chau bey and colleagues, the accuracy 
of three machine learning (ML) algorithms—logistic regression, decision trees, 
and KNN—was evaluated in the prediction of thyroid illness. This study primarily 
illustrates intuition on how to forecast thyroid problems. In this technique, the thyroid 
dataset is used to predict the disease. The author, however, makes no mention of the 
patient having a particular thyroid condition, such as hyperthyroidism or hypothy-
roidism. In contrast to this paper, we present a dataset on the thyroid conditions that 
inﬂuence patients. 
In paper [3], Ritesh Jha and colleagues used the spark platform to carry out 
all experiments in a distributed setting. The thyroid dataset is employed, and ML 
methods like PCA, Decision Tree, and KNN assist in delivering the results. 
Utilizing several approaches to dimension reduction, the features discovered by these 
techniques were then fed into the classiﬁers. Two classiﬁers employ the methods for 
dimension reduction and data enrichment. This document presents the experiments’ 
comprehensive results. The accuracy of the feature reduction and data augmentation 
strategies used in this paper is 98.7% and 99.95%, respectively. 
It is suggested in [4] by Lerina Aversanoa et al. that the prognosis of the course 
of treatment for the patient with hypothyroidism, as well as the early diagnosis 
of a potential malfunction, plays a key role. This information can be very helpful 
to physicians who are treating patients and additionally to anticipate therapy for 
thyroid disease. Gradient Boosting Classiﬁer, Decision Tree, and Naive Bayes are 
the machine learning (ML) methods that are utilized in this work to predict. In this 
work, a real-time dataset from patients at a hospital in Naples is employed. The 
primary phase entails preparing the data that will be used in the classiﬁer in relation 
to the application of the SMOTE method. 
Early predictions of the spread of thyroid disease in women were made by 
Dhyan Chandra Yadav et al. [5]. With early sickness discovery, the dangerous condi-
tion of thyroid cancer can be prevented. The ﬁndings of these classiﬁers can also be 
obtained using decision trees, random forests, and classiﬁcation and regression trees 
(CART), and this paper enhanced the results using the bagging ensemble technique. 
The stored datasets are mined for hidden patterns using the decision tree. 
In a paper [6], Eun Joo Lee et al. explain that the type of tumor and its stage at 
the time of diagnosis determine the course of treatment for thyroid cancer patients. 
Many thyroid tumors are still tiny, stable, and asymptomatic. Patients with thyroid 
cancer who have a total thyroidectomy had higher survival rates and lower recurrence 
rates [7, 8]. The process of treating patients involves the use of a logistic regression 
method [9]. This research largely focuses on treating thyroid cancer patients based 
on stage, which increases mortality.

52
L. Krishnasamy et al.
Fig. 1 Proposed System 
3 
Proposed System 
This article’s recommended technique identiﬁes the patient’s thyroid conditions, 
including hypothyroidism, hyperthyroidism, and euthyroidism. Logistic Regression, 
Random Forest, and (SVM) were the classiﬁers utilized. Before being chosen for 
usage in the model and its features, the collected dataset is cleaned of any potential 
irregular data and pre-processed using a variety of techniques. The effectiveness of the 
classiﬁers is assessed using confusion matrices and accuracy scores. Selecting mean-
ingful documents for analysis is essential if you want to employ different machine-
learning approaches to learn and generate usable knowledge. Here, the downloaded 
Kaggle dataset is used to make predictions on hypothyroidism, hyperthyroidism, and 
euthyroidism. The work ﬂow is shown in the following Fig. 1. 
3.1 
Dataset Description 
a. Age
b. Sex 
c. Thyroxine
d. Query on thyroxine 
e. On antithyroid medication
f. Sick 
g. Pregnant
h. Thyroid Surgery
(continued)

Predicting the Thyroid Disease Using Machine Learning Techniques
53
(continued)
i. 1131 Treatment
j. Query Hypothyroid 
k. Query Hyperthyriod
l. Lithium 
m. Goitre
n. Tumor 
o. Hypopituitary
p. Psyco 
q. TSH measured
r. TSH 
s. T3 measured
t. T3 
u. TT4 measured
v. TT4 
w. T4U measures
x. T4U 
y. T3
z. FTI 
aa. TBG Measured
ab. TBG 
ac. Referral source
ad. category 
3.2 
Data Pre-processing 
The processes that are used to modify or encrypt data such that a machine can 
swiftly and easily decode it are referred to as data preparation. In this paper three 
types of preprocessing are used, they are label Binarize, Min-MaxScaler, and Label 
Encoder. Binarization divides the data into two groups and assigns one of two values 
to each group. All data characteristics are scaled by Min-MaxScaler in the range [0, 
1]. In order for machines to read labels, they must be transformed into a numeric 
representation. 
3.3 
Models 
SVM Classiﬁer 
SVM is a highly used and run-in efﬁcient ML method or algorithm. A model that can 
be used for regression and classiﬁcations. The SVM method appears to determine 
the best decision boundary or solution line for categorizing n-features. As a result, 
we may place the new datasets points in the appropriate groupings. The optimal 
choice boundary is referred to as a hyperplane. The hyperplane is created using the 
support vector machine, which selects the high data points. The decision boundary 
or hyperplane is used to classify three separate groups: Both for non-linear and linear 
data, the SVM model is used. The major preparation information is converted into a 
greater measurement via a nonlinear mapping. 
Random Forest Classiﬁer 
It is an ensemble classiﬁer. It comes under the type Bagging. The class with the most 
votes determines the forecast for our model. In a conventional decision tree, just one

54
L. Krishnasamy et al.
DT is produced. Several DTs are produced during the random forest process. Two 
criteria are used to classify data frames: Variables and Observation. When a large 
number of DT are produced and used, a forest is established. In this Random Forest 
we used 100 Decision trees as input to predict the outcome. 
Logistic Regression Classiﬁer 
The most popular machine learning algorithm in the supervised learning area is 
logistic regression. This method makes use of a group of independent factors to 
forecast the category-dependent variable. By using logistic regression, a dependent 
categorical variable’s output is predicted. Therefore, the outcome should be discrete 
or of categorical value. Rather than delivering an exact value between 0 and 1, it 
delivers probabilistic values that are in the range of 0 and 1. It can be either Yes or 
No, 0 or 1, true or false, etc. Logistic regression and linear regression have many char-
acteristics but are used Differently. Classiﬁcation issues are resolved using logistic 
regression, whereas regression-related issues are resolved using linear regression. 
4 
Results and Discussion 
Classiﬁers have been used to build the intended framework. The accuracy scores 
obtained from the confusion matrix are used to evaluate the efﬁciency of the 
classiﬁers. The accuracy of the created classiﬁers can be predicted by, 
Accuracy e
q u als  S tartFraction upper T upper P plus upper T upper N Over upper T upper P plus upper F upper P plus upper T upper N plus upper F upper N EndFraction
A c cur a c y e q u als  S tartFraction upper T upper P plus upper T upper N Over upper T upper P plus upper F upper P plus upper T upper N plus upper F upper N EndFraction
TP: True Positive—Observation correctly predicted that the device must store 
energy 
TN: True Negative—Observation correctly predicted that the device must not 
store energy 
FP: False Positive—Observation incorrectly predicted that the device must store 
energy 
FN: False Negative—Observation incorrectly predicted that the device must not 
store energy 
Support Vector Machine model produced the following confusion matrix using 
test data of 806 instances with the target variable being the class values hypothy-
roid, hyperthyroid and euthyroid. The confusion matrix evidently depicts that 618 
instances have been correctly classiﬁed while 188 instances were not and that this 
classiﬁer model’s accuracy is 78.28% (Figs. 2 and 3).
The logistic regression model produced the following confusion matrix using 
test data of 806 instances with the target variable being the class values hypothy-
roid, hyperthyroid and euthyroid. The confusion matrix evidently depicts that 686

Predicting the Thyroid Disease Using Machine Learning Techniques
55
Fig. 2 Confusion 
matrix – SVM 
Fig. 3 Confusion 
matrix – Logistic Regression
instances have been correctly classiﬁed while 120 instances were not and that this 
classiﬁer model’s accuracy is 85.24% (Fig. 4).
Random Forest model produced the following confusion matrix using Test data of 
806 instances with the target variable being the class values hypothyroid, hyperthy-
roid and euthyroid. The confusion matrix evidently depicts that 680 instances have 
been correctly classiﬁed while 126 instances were not and that this classiﬁer model’s 
accuracy is 84.37% and all the three algorithm’s accuracy level comparison is given 
in Table 1.

56
L. Krishnasamy et al.
Fig. 4 Confusion 
matrix – Random Forest
Table 1 Comparing three 
algorithms
Classiﬁers
Accuracy (%) 
SVM
78.28% 
Logistic Regresson
85.24% 
Random Forest
84.37% 
5 
Conclusion and Future Works 
The healthcare sector generates enormous amounts of data that are used in daily life. 
This study used machine learning approaches to analyze the raw data and come to a 
novel conclusion on hypothyroid, hyperthyroid, and euthyroid conditions. However, 
in this work, we suggest a technique to identify individuals affected by all hypothy-
roid, hyperthyroid, and euthyroid conditions. Various papers have proposed various 
solutions to identify thyroid-affected people or any one speciﬁc thyroid. In the sphere 
of medicine, this prediction is tough and signiﬁcant. This study makes use of a thyroid 
dataset that was collected from Kaggle. There are 806 instances and 31 attributes 
accessible in the dataset. The Support Vector Machine (SVM), Logistic Regression, 
and Random Forest are combined in the suggested approach. These algorithms help 
to identify which type of thyroid disease is affected by people. This also includes the 
count of male and female affected people, pregnant people who are all affected by 
the thyroid, and the graphical representation of these counts is represented to easily 
identify them. Finally, the accurate results obtained by SVM, Logistic Regression, 
and Random Forest are 78.28%, 85.24%, and 84.37% respectively. From this project, 
we can able to know the number of male and female people affected and also predict 
the age group of people who must be affected by which type of thyroid disease.

Predicting the Thyroid Disease Using Machine Learning Techniques
57
By comparing the three algorithms, Logistic Regression provides the most accurate 
results in identifying disease. 
References 
1. Machens A, Lorenz K, Dralle H (2020) Prediction of biochemical cure in patients with medullary 
thyroid cancer. Br J Surg 107:695–704 
2. Chaubey G, Bisen D, Arjaria S, Yadav V (2020) Thyroid disease prediction using machine 
learning approaches. Natl Acad Sci Lett 44:233–238 
3. Jha R, Bhattacharjee V, Mustaﬁ A (2021) Increasing the prediction accuracy for thyroid disease: 
a step towards better health for society. Wirel Pers Commun 122:1921–1938 
4. Aversano L, Bernardi ML, Cimitile M, Iammarino M, Macchia PE, Nettore IC, Verdone C (2021) 
Thyroid disease treatment prediction with machine learning approaches. Procedia Comput Sci 
192:1031–1040 
5. Yadav DC, Pal S (2020) Prediction of thyroid disease using decision tree ensemble method. 
Hum-Intell Syst Integr 2:89–95 
6. Nguyen QT, Lee EJ, Huang MG, Park YI, Khullar A, Plodkowski RA (2015) Diagnosis and 
treatment of patients with thyroid cancer. Am Health Drug Beneﬁts 8(1):30 
7. Elliott Range DD, Dov D, Kovalsky SZ, Henao R, Carin L, Cohen J (2020) Application of a 
machine learning algorithm to predict malignancy in thyroid cytopathology. Cancer Cytopathol 
128(4):287–295 
8. Yadav DC, Pal S (2020) Discovery of hidden pattern in thyroid disease by machine learning 
algorithms. Indian J Public Health Res Dev 11(1):61–66 
9. Selwal A, Raoof I (2020) A multi-layer perceptron based intelligent thyroid disease prediction 
system. Indones J Electr Eng Comput Sci 17(1):524–533

An Automatic Trafﬁc Sign Recognition 
and Classiﬁcation Model Using Neural 
Networks 
Rajalaxmi Padhy, Alisha Samal, Sanjit Kumar Dash, and Jibitesh Mishra 
Abstract The signiﬁcance of trafﬁc symbol recognition technologies, which have 
played a key role in street security, has been the subject of much interest to researchers. 
To accomplish their assessment, specialists employed Artiﬁcial Intelligence, deep 
learning, and image processing tools. Convolutional Neural Networks (CNN) are 
deep learning-based designs that have sparked a new and ongoing research into trafﬁc 
symbol classiﬁcations and recognition frameworks. The objective of this paper is to 
establish a CNN model that is suitable for insertion purposes and has a high level 
of order exactness. For the series of street symbols, we used an upgraded LeNet-
5 model. The German Trafﬁc Sign Recognition Benchmark (GTSRB) information 
base will function as the framework for our model architecture, which outperformed 
existing models. GTSRB will have 99.84 percent accuracy. We decided to use a 
camera to verify the proposed model for an implanted application because of its 
softness and reduced number of boundaries (0.38 million) based on the improved 
LeNet-5 structure. The outcomes are advantageous, demonstrating the effectiveness 
of the discussed strategy. 
Keywords Convolutional Neural Network · Deep Learning · Image 
Preprocessing · Feature Extraction · Normalization · Gray Scaling · Local 
Histogram Equalization · Data Augmentation 
1 
Introduction 
In India, approximately 400 road accidents occur every day, according to government 
ﬁgures. Road symbols aid in the prevention of trafﬁc accidents by assuring the safety 
of both drivers and pedestrians. The high frequency of trafﬁc deaths has resulted in 
numerous personal injuries and property damages [1]. Furthermore, trafﬁc signals 
ensure that road users follow speciﬁed regulations, reducing the chances of trafﬁc 
offences. The usage of trafﬁc symbols also helps with route guidance. All road users,
R. Padhy · A. Samal · S. K. Dash envelope symbol · J. Mishra 
Odisha University of Technology and Research, Bhubaneswar, India 
e-mail: sanjitkumar303@gmail.com 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_7 
59

60
R. Padhy et al.
including automobiles and pedestrians, should give priority to road symbols. For a 
variety of reasons, such as focus issues, tiredness, and sleep deprivation, we overlook 
trafﬁc signs. The detection and categorization of trafﬁc symbols are the two sub-tasks 
of trafﬁc sign recognition systems [2]. 
The objective is to keep a strategic distance from accidents using both manual 
and automated approaches, with all actions being executed in accordance with the 
identiﬁed trafﬁc symbols. We’re interested in using CNN for trafﬁc symbol identiﬁ-
cation because of the success of deep learning-based classiﬁcation and recognition 
approaches in several ﬁelds. As a solution, a convolutional neural network is used 
in the suggested system. This paper is mainly inspired by the work of various scien-
tists, which is summarized in Sect. 2. The German Trafﬁc Sign datasets, which are 
mainly illustrated in Sect. 3 of this paper, besides this, Sect. 3 also focuses on the 
system design, data preprocessing part of the project and the model architecture 
and basic components of the CNN architecture such as convolutional layers, sub-
sampling layers, dropouts etc. It also explains the training and testing aspects of the 
project. Finally, Sect. 4 describes the future scope and the conclusion of the project, 
mentioning the learning curve and the project accuracy. 
2 
Literature Review 
Wu et al. [3] developed their methodology using a combination of color transforma-
tion and a ﬁxed layer CNN that can help to reduce the number of locations that the 
classiﬁer must deal with, thereby speeding up the detection process. Zhu et al. [4] 
suggested an innovative and effective method for detecting and recognizing trafﬁc 
signs which uses a fully convolutional network, which signiﬁcantly decreases the 
search area for trafﬁc signs while maintaining detection rates. Lee et al. [5] intro-
duced a system that is used to predict the location and precise boundaries of trafﬁc 
signs at the same time. Supraja and Kumar [6] proposed that the primary purpose for 
mechanizing trafﬁc-sign stock management is to address a large number of trafﬁc sign 
categories. The location and acknowledgment are displayed using a strategy based 
on the Squeeze net CNN. Narejo et al. [7] implemented a system for recognizing 
and interpreting warning trafﬁc signs where color info and the geometric property 
of the road signs are used to classify the recognized trafﬁc signs. Alexander et al. 
[8] considered an implementation of the method for trafﬁc sign classiﬁcation which 
was combined with earlier work of preprocessing and localization procedures. Cao 
et al. [9] presented an enhanced trafﬁc symbol recognition algorithm for automated 
vehicles to address issues such as the poor real-time performance of trafﬁc sign recog-
nition approaches. Zuo et al. [10] represents the highest level in object recognition, 
as it eliminates the need to manually extract picture attributes and can automatically 
segment images to generate candidate region recommendations. Radzak et al. [11] 
introduced a system which aims to identify the regions of interest (ROI) using several 
techniques and methodologies, such as binarization, region of interest, and pixel clas-
siﬁcation. Prasad et al. [12] approached a system where the photos of the road scene

An Automatic Trafﬁc Sign Recognition and Classiﬁcation Model Using …
61
were ﬁrst converted to grayscale images. Second; they used the maximally stable 
external regions approach to extract the region of interest. Finally, they employed 
convolution neural networks using simpliﬁed Gabor feature maps. 
Luo et al. [13] suggested video sequences, containing both symbol-based and text-
based indicators, taken by an automobile camera which includes areas of interest 
(ROIs), reﬁning and categorization of ROIs, and post-processing. Akshata. [14] 
covered the three stages of the real-time trafﬁc sign recognition and classiﬁcation 
system, which include image segmentation, trafﬁc sign detection, and classiﬁcation 
based on the input image. Mammeri et al. [15] demonstrated how to construct a TSDR 
system by going over some of the strategies that can be employed at each stage of the 
process. These strategies were organized into distinct groups for each level. Anuraag 
Velamati [16] developed a model that is stored as a h5 ﬁle. They were also successful 
in designing the GUI and using TensorFlow, CNN, and OpenCV. Shukla et al. [17] 
proposed a software technology that would aid in the detection and identiﬁcation 
of trafﬁc signs. For that, A deep learning-based trafﬁc sign recognition algorithm is 
proposed. Akhil Sharma [18] pointed out that after using data augmentation provided 
by the deep learning package, it was possible to discriminate between distinct types 
of trafﬁc signs. Zhang et al. [2] developed a multi-scale attention technique that uses 
dot-product and softmax to create weighted multi-scale features, which are then ﬁne-
tuned to highlight trafﬁc sign characteristics and increase trafﬁc sign identiﬁcation 
accuracy. Santos et al. [33] implemented a system with voice alert using python and 
four preprocessing. Yadav et al. proposed an approach for detecting and recognizing 
trafﬁc signs from a video sequence that takes into account all of the challenges asso-
ciated with object recognition in outdoor contexts [20]. Saha et al. [21] devised  a  
system that needs the fewest learnable parameters and the least amount of training 
time. M. M. Sruti [22] discussed the design and implementation of a Trafﬁc Sign 
Recognition (TSR) system for Bangladeshi trafﬁc signs utilizing CNNs as both a 
feature extractor and a classiﬁer. Vennelakanti et al. [23] presented a detection and 
recognition system separated into two parts. Wali et al. [24] divided their TSR system 
into three primary steps: detection, tracking, and classiﬁcation. 
Zang et al. [25] solved the problem of unmanned autonomous vehicle trafﬁc sign 
recognition by using the Faster R-CNN algorithm that is utilized to recognize trafﬁc 
signs in each frame of the input image sequences. Veliˇckovi´c et al.  [26] trained 
and assessed CNN numerous times, and the accuracy averaged approximately 80%. 
They discovered that the CNN had trouble discriminating between signs with similar 
shapes. Zhao et al. [27] discussed a faster R-CNN target detection algorithm, which 
is based on the migration learning concept, extracts image features using pre-trained 
neural network models and is suitable for training a target detection model system 
with a small amount of data. Jain et al. [28] suggest a novel technique for the TSR 
system which focuses on the concept of domain transfer learning for each layer of the 
pre-trained CNN model (VGG-16). López and Guzman [29] proposed a model that 
uses CNN and visual processing to improve autonomous trafﬁc sign identiﬁcation. 
Zhang et al. [2] proposed a cascaded R-CNN. Except for the initial layer, each 
layer of the cascaded network fuses the output bounding box of the preceding layer 
for joint training. Shaﬁei et al. [30] proposed different deep learning models that

62
R. Padhy et al.
were examined in terms of classiﬁcation accuracy as well as prediction speed. They 
have suggested implementation with VGG-19. Cao et al. [31] improved a Sparse 
R-CNN, a neural network model that is being inspired by Transformer. K. Prakash 
[32] presented a technique for classiﬁcation, where the author employed CNN and 
DCNN. VGG16, VGG19, and AlexNet are used to implement the model. Haque 
et al. [33] suggested a simple CNN model for trafﬁc sign recognition that does not 
require the utilization of a GPU. The authors presented a deep thin architecture with 
three modules: input processing, learning, and prediction. 
Ellahyani et al. proposed a rapid approach of selecting the observed candidates for 
random forests classiﬁer with a mix of HOG and LSS features [34]. Jacopo Credi [35] 
used Conv.NET, which is the ﬁrst attempt to create a lightweight toolbox for deep 
learning inside the.NET framework. The Conv.NET class library was built from the 
ground up for this project. It utilizes Open CLTM, an open platform for heterogeneous 
parallel computing. Hatolkar et al. proposed an improved technique for identifying 
road trafﬁc indicators. Short, low-quality videos are captured by a camera mounted 
on a car [36]. The fuzzy classiﬁcation module is an optimizer that improves CNN’s 
results. Carl Ekman [37] worked on the Mobile Net V2, a CNN architecture that is 
speciﬁcally designed to be computationally efﬁcient. For complete feature extraction, 
the authors used CNN instead of the Hough transform, or local binary patterns [38]. 
Bangquan and Xiong [39] developed two new efﬁcient TSC networks, ENet (efﬁ-
cient network) and EmdNet. Matoš et al. [40] proposed the Hough Circle feature for 
trafﬁc detection. It locates circles within images using the Hough transformation and 
has used the SVM classiﬁer for training and testing. Song et al. [41] proposed a CNN 
model for small trafﬁc sign detection. For the evaluation, the Tsinghua-Tencent data 
set was employed as a raw data set. Chauhan et al. [42] developed a highly successful 
approach for TSR that uses CLAHE (Contrast Limited Adaptive Histogram Equal-
ization), SVM, KNN classiﬁer, and artiﬁcial neural network. Robertson [43] used the  
existing state-of- the-art YOLO (You Only Look Once) framework. The key advan-
tages of YOLO are that it is incredibly fast, produces fewer background mistakes than 
other approaches. Bichkar et al. [44] suggested the categorization of trafﬁc signs using 
CNN with multiple ﬁlters. For detection of trafﬁc signs in the environment and clas-
sifying the image, the detection model used YOLO and BLOB analysis. Rachapudi 
et al. [45] suggested a model where the complexity of the photos has decreased after 
they applied gray scaling to the dataset images and the accuracy has improved. They 
continued to use normalizing approaches, and the accuracy grew by a tiny amount. 
Wan et al. [46] discussed a CNN model named TS-Yolo for achieving accuracy in 
trafﬁc identiﬁcation during extreme weather situations. 
3 
System Model 
Trafﬁc-sign recognition system evaluates photos captured by a car’s camera in real 
time to recognize symbols. It provides assistance to the driver by delivering warnings. 
The recognition module acknowledges the symbol region found in the image/video by

An Automatic Trafﬁc Sign Recognition and Classiﬁcation Model Using …
63
Fig. 1 Block Diagram of System Model 
the detection module. During the detection step, the symbol areas with the maximum 
possibility are selected and transmitted to the recognition system for classiﬁcation. 
So, with respect to the above context, we design the model with 6 stages as shown 
in the Fig. 1. 
3.1 
Image Preprocessing 
The goal of preprocessing is to increase the image quality so that we can better analyse 
it. To acquire the best possible outcome, we applied several image preprocessing 
methods such as shufﬂing, gray-scaling the image, normalization, and local histogram 
equalization. 
1. Grayscaling: Grayscale images fall in between binary and color images. It’s 
the only part that doesn’t get cut off. According to a survey, using grayscale 
images rather than colored images enhances ConvNet accuracy. Therefore, we 
have converted the training images to grayscale using OpenCV and the. cvtColor 
function. 
2. Local Histogram Equalization: This method mainly spreads out the most 
common intensity values in an image, resulting in low-contrast images being 
enhanced or adjusted. To apply local histogram equalization to the training 
photos, we are using a function, i.e., equalizeHist. 
3. Normalization: It basically adjusts the range of pixel intensity values. Therefore, 
we applied normalization to the data in the range of (0,1), which was accom-
plished by setting the rescale argument using the line of code (img = (img/255)). 
The mean of the resulting dataset was not precisely zero, but it was lowered 
from around 81.65 to around 0.0039. Thus, it beneﬁts during training because

64
R. Padhy et al.
it eliminates the chance of having an outspread distribution in the data, which 
makes it more difﬁcult to train with a single learning rate. 
4. Image Data Augmentation: Image data augmentation used for enhancing the 
training dataset in order to improve the model’s execution and generalisation 
capability. For that, we made various changes to the dataset by changing the 
width_shift_range to 0.1, height_shift_range to 0.1, zoom_range to 0.2, shear_ 
range to 0.1, and rotation range to 10 with the help of the Image Data Generator, 
and then ﬁnally we called the data-generator to augment the images in real time 
with a particular layout. As a result, data augmentation aids in reducing the gap 
between training and validation loss and accuracy, hence reducing overﬁtting. 
The steps involved in image pre-processing stage are as follows: 
1. Import the required libraries i.e., numpy, pandas, random, matplot. 
2. Initialize the images with coordinates (32,32,3) i.e., width, height, RGB value. 
3. Load the image from a speciﬁed path. 
4. Use the. cvtcolor method for converting the colored image into a grey-scaled 
image. 
5. Apply simple contrasting to the grayscale image by equalisation method. 
6. Apply image pixel normalization to the image to range it between 0 and 1. 
7. Call the Data Generator to augment images in real time. 
8. Resize the image by changing the axes value where 20 and 5 represent the width 
and height. 
9. Apply data augmentation. 
10. Save the image for further classiﬁcation. 
3.2 
Deep Learning Model 
CNN is a deep learning model that works in a near way similar to traditional neural 
networks. The inputs are received by the neural network, which then conducts a 
dot operation on the input before applying a nonlinear function (ReLU Activation). 
ConvNets and CNNs work in a similar way: they take an image as input and apply 
weights and biases to various characteristics of that picture. Preprocessing is often 
not necessary in ConvNets because it has the potential to learn features of an image. 
ConvNets minimizes the image to make it easier to process while preserving the 
image’s features. Each image in CNN is processed through a set of convolutional 
layers, which include ﬁlters, pooling, and fully connected layers. The output layer 
uses a softmax function to classify the objects with probabilities between 0 and 1 as 
shown in Fig. 2.
From the standpoint of work, the images input into CNN are grayscale images 
with pixel values recorded in a data frame. The ﬁrst step is to get the photos out of 
the data frame and separate the data from the labels. Splitting the data into train and 
validation sets, as well as their labels, is the second stage. After that, the image’s 
pixel values are adjusted before being fed to CNN. When normalized photos are sent 
into CNN, they are processed through the various layers as shown in Fig. 3.

An Automatic Trafﬁc Sign Recognition and Classiﬁcation Model Using …
65
Fig. 2 General CNN Architecture
Fig. 3 CNN Model Architecture 
3.3 
Feature Extraction 
Convolution layer consists of different sets of learnable kernels. Each kernel is small 
but extends throughout the image. The ﬁlter is slid across the width and height of 
the image to compute the dot product between the ﬁlter values and image input. 
Convolution is to extract features such as edges. The result we achieve after this 
operation is either the convolved feature is reduced in dimensionality or the convolved 
feature remains the same. In this case, the latter has been used which is termed as the 
Same Padding. Relu-activation was used to introduce non-linearity in our convolution 
layer. Then the pooling layer reduces the computational power required for data 
processing by reducing the size of the convolved feature. It also helps in extracting

66
R. Padhy et al.
the necessary features which are uniform to rotational and positional changes. We 
used max-pooling as it acts as a better noise reducer as well as dimensionality reducer. 
3.4 
Classiﬁcation 
A fully connected neural network usually needs a huge number of layers and neurons 
in the network for classifying images, which enhances the model parameters and 
causes over-ﬁtting. As all neurons are interconnected to each other, the input image 
may lose its pixel correlation properties. CNN has emerged as an approach to these 
problems which uses kernel ﬁlters to extract the key features from an input image 
and then inject them into a fully connected network to classify the class. In proposed 
system model, we’ve chosen the LeNet-5 convolutional neural network, which was 
originally trained to recognize handwritten digits. It has six layers: four levels of 
convolution and reﬁnement functions constructed with 3 × 3 kernel ﬁlters, and a 2 × 
2 max pooling ﬁlter to reduce the 32 × 32 input image to 60 5 × 5 maps. The feature 
images provide the most crucial characteristics that describe a speciﬁc object. 
3.5 
Training and Testing of Data 
To train the model, we started with the normalized dataset, which had a validation 
accuracy of 92.7 percent. Although this accuracy was satisfactory, it was insufﬁcient 
for the development of a robust classiﬁer. Running the same model with gray-scaled 
and normalized inputs as stated in the LeNet lab using the default architecture (batch 
size: 128; epochs: 10; rate: 0.001; mu: 0; sigma: 0.1). The accuracy of validation was 
approximately 94.4 percent, which was a signiﬁcant improvement. However, as we 
started working with the real model, we began tweaking the rate, epoch, and batch 
size in the hopes of improving the model. We discovered that when the model was 
either turned too much or too little, it only improved a little (and in some cases, it acted 
signiﬁcantly worse). Despite the fact that this paper on using LeNet Architecture for 
trafﬁc signs was written a long time ago, it was still reliable enough to analyse trafﬁc 
symbols with 99 percent accuracy. 
As a result, we decided to scrap the traditional architecture in favour of the archi-
tecture described in the article. The optimizer, Adam Optimizer, which was previ-
ously implemented in the LeNet Lab, and the mu and sigma values, which are 0 and 
0.1 respectively, were two items that we did not change throughout implementation. 
The remaining hyper parameters were tweaked across several tests by setting the 
batch size to 2000, epochs value to 10, learning rate to 0.0009 and dropout keep 
probability to 0.5 which was used to achieve highest accuracy as shown in table 1.

An Automatic Trafﬁc Sign Recognition and Classiﬁcation Model Using …
67
Table 1 The different models obtained after the modiﬁcation of the default models 
Models
Inputs to 1st 
Convolutional 
layer 
Inputs to 2nd 
Convolutional 
layer 
Neurons present in 
fully connected 
layer 
epochs
Batch 
size 
Accuracy 
Model 1 
28 × 28x60
24 × 24x60
480
10
2000
99.84 
Model 2 
28 × 28x60
24 × 24x60
480
10
600
99.2 
Model 3 
28 × 28x60
24 × 24x60
480
10
400
98.6 
4 
Data Set Analysis 
The dataset used is German Trafﬁc Sign Benchmark (GTSRB) which is 
obtained from Kaggle (Source: “https://bitbucket.org/jadslim/german-trafﬁc-sig 
ns.git”) whose samples are shown in Fig. 4. For the simplicity of work, we have 
also referred to Bitbucket where the extracted images of this dataset are already 
present. There are 43 classes in this picture collection (many unique trafﬁc symbol 
images). There are about 34,799 photos in the training set, 12,630 images in the test 
set, and 4410 images in the validation set. Thus the size of training set(i.e. basically 
used for training the network) becomes 34,799, the size of the validation set(i.e. 
used for the supervision of network performance while training) becomes 4410, The 
size of test set(i.e. basically used for evaluation of ﬁnal network) becomes 12,630. 
Moreover, the pattern of an image of a trafﬁc sign so far (32, 32, 3) (3 because of 
R,G,B{Red, Green, Blue} Channels). 
The majority of the images have a distinct appearance, primarily in terms of 
contrast and brightness. If we want to train models with great precision, this might not 
be the best option. To gain substantial accuracy, we can use some form of histogram 
equalisation (increases contrast for lower contrast image) (In ML terms this will be
Fig. 4 Sample datasets depicting various classes 

68
R. Padhy et al.
Fig. 5 Distribution of Training Dataset 
good for better feature extraction). The proper distribution of data is being predicted 
through a graph illustrated in Fig. 5. 
5 
Results and Discussions 
The learning curve showing performance comparison of the above model are shown 
in Fig. 6 and Fig. 7 considering batch size 1000. We’ll now utilize the testing set to 
assess the model’s accuracy when applied to unknown samples. We were successful 
in attaining a Test accuracy of 96.8 percent, which is an outstanding result. 
For checking more efﬁciency, we even tried changing the batch size to 2000 and 
try to plot the graphs for learning curves as shown in Fig. 8 and Fig. 9. It is observed 
that on changing the value of the batch size we are getting very slight differences in 
the point values and thus able to achieve 97.6 percent testing accuracy which was 
closer to previous achievement.
Fig. 6 Training vs 
Validation Accuracy with 
batch size 1000

An Automatic Trafﬁc Sign Recognition and Classiﬁcation Model Using …
69
Fig. 7 Training vs 
Validation Loss with batch 
size 1000 
Fig. 8 Training vs 
Validation Accuracy with 
batch size 2000 
Fig. 9 Training vs 
Validation Loss with batch 
size 2000
To demonstrate the new architecture’s efﬁciency, we try to forecast images that the 
trained model architecture has never seen before. For that we imported pictures from 
the internet and tried to test it with the proposed model. And when we tried processing 
this image, we get the resultant output as Predicted Category as 28 and Predicted sign 
as Children crossing which was very accurate as shown in the above Fig. 10. There 
are several applications for automatic trafﬁc sign categorization, However, when

70
R. Padhy et al.
Fig. 10 Flow diagram of Trafﬁc Sign Classiﬁcation
compared to previous research, the system’s accuracy was exceptional and among 
the best. The framework has been developed and changed after being inﬂuenced 
by the well-known LeNet-5. The advancements enable us to achieve 99.84 percent 
accuracy while using less training parameters in comparison to the depth of the model. 
We can test the model using a webcam-enabled embedded application because of its 
lightweight. The categorization is likewise extremely accurate in this example. 
6 
Conclusion 
In this paper, we introduce a more efﬁcient model having the highest accuracy in 
classifying trafﬁc symbols using various preprocessing techniques such as grey-
scaling, local histogram equaliser, normalization, data augmentation, and convolu-
tional neural networks. The proposed methodology uses the LeNet architecture for 
classiﬁcation, which is made up of 7 layers. The use of four preprocessing techniques 
helps in removing the noise present in the data and thus gives us more accuracy while 
classifying it. While the model provided in this research takes us one bit closer to 
the ideal Advanced Driver Assistance System or perhaps a self-driving vehicle, there 
is still much to be improved. This paper uses the color and geometry of a sign to 
determine its identity. If there is a reﬂection on the sign that alters its color, then it 
might create an issue. This application can also have a text-to-speech module. The 
driver here has to read the text that is printed on the classiﬁed sign in the existing 
application, however, further comfort is provided by using a speech module. With 
more datasets and from multiple countries, overall performance could be increased 
and modiﬁed.

An Automatic Trafﬁc Sign Recognition and Classiﬁcation Model Using …
71
References 
1. Li W, Li D, Zeng S (2019) Trafﬁc sign recognition with a small convolutional neural network. 
IOP Conf Ser Mater Sci Eng 688(4):044034. https://doi.org/10.1088/1757-899X/688/4/044034 
2. Zhang J, Xie Z, Sun J, Zou X, Wang J (2020) A cascaded R-CNN with multiscale attention 
and imbalanced samples for trafﬁc sign detection. IEEE Access 8:29742–29754 
3. Wu Y, Liu Y, Li J, Liu H, Hu X (2013). Trafﬁc sign detection based on convolutional neural 
networks. In: The 2013 International Joint Conference on Neural Networks (IJCNN). IEEE, 
pp. 1–7 
4. Zhu Y, Zhang C, Zhou D, Wang X, Bai X, Liu W (2016) Trafﬁc sign detection and recognition 
using fully convolutional network guided proposals. Neurocomputing 214:758–766 
5. Lee HS, Kim K (2018) Simultaneous trafﬁc sign detection and boundary estimation using 
convolutional neural network. IEEE Trans Intell Transp Syst 19(5):1652–1663 
6. Supraja S, Kumar P (2021) An intelligent trafﬁc signal detection system using deep learning. 
SSRG Int J VLSI Signal Process 8(1):5–9 
7. Narejo S, Talpur S, Memon M, Rahoo A (2020) An automated system for trafﬁc sign recognition 
using convolutional neural network. 3C Tecnología_Glosas de innovación aplicadas a la pyme 
9:119–135. https://doi.org/10.17993/3ctecno.2020.specialissue6.119-135 
8. Shustanov A, Yakimov P (2017) CNN design for real-time trafﬁc sign recognition. Proc Eng 
201:718–725 
9. Cao J, Song C, Peng S, Xiao F, Song S (2019) Improved trafﬁc sign detection and recognition 
algorithm for intelligent vehicles. Sensors 19(18):4021 
10. Zuo Z, Yu K, Zhou Q, Wang X, Li T (2017) Trafﬁc signs detection based on faster R-CNN. 
In: 2017 IEEE 37th International Conference on Distributed Computing Systems Workshops 
(ICDCSW). IEEE, pp. 286–288 
11. Radzak MY, Alias MF, Arof S, Ahmad MR, Muniandy I (2015) Study on trafﬁc sign recognition. 
Int J Res Stud Comput Sci Eng (IJRSCSE) 2(6):33–39 
12. Prasad S, Desai S, Kumar S, Adarsha MD (2021) Trafﬁc sign detection using CNN. Int J Adv 
Res Comput Commun Eng 10(6):2021 
13. Luo H, Yang Y, Tong B, Wu F, Fan B (2017) Trafﬁc sign recognition using a multi-task 
convolutional neural network. IEEE Trans Intell Transp Syst 19(4):1100–1111 
14. Akshata VS, Panda S (2019) Trafﬁc sign recognition and classiﬁcation using convolutional 
neural networks. J Emerg Technol Innov Res 6(2):132–147 
15. Mammeri A, Boukerche A, Almulla M (2013) Design of trafﬁc sign detection, recognition, 
and transmission systems for smart vehicles. IEEE Wirel Commun 20(6):36–43 
16. Velamati A (2021) Trafﬁc sign classiﬁcation using convolutional neural networks and computer 
vision. Turk J Comput Math Educ (TURCOMAT) 12(3):4244–4250 
17. Shukla SK, Dubey S, Pandey AK, Mishra V, Awasthi M, Bhardwaj V (2021) Image caption 
generator using neural networks. Int J Sci Res Comput Sci Eng Inf Technol. 1–7 https://doi. 
org/10.32628/CSEIT21736 
18. Sharma A (2019). Trafﬁc Sign Recognition & Detection using Transfer learning. Doctoral 
dissertation, Dublin, National College of Ireland 
19. Santos A, Abu PAR, Oppus C, Reyes RS (2020) Real-Time Trafﬁc Sign Detection and 
Recognition System for Assistive Driving 
20. Yadav S, Patwa A, Rane S, Narvekar C (2019) Indian trafﬁc signboard recognition and driver 
alert system using machine learning. Int J Appl Sci Smart Technol 1(1):1–10 
21. Saha S, Kamran SA, Sabbir AS (2018) Total recall: understanding trafﬁc signs using deep 
convolutional neural network. In: 2018 21st International Conference of Computer and 
Information Technology (ICCIT). IEEE, pp. 1–6 
22. Surti MM. Real time trafﬁc sign detection and classiﬁcation system on Jetson TX1 Embedded 
Development Board 
23. Vennelakanti A, et al (2019) Trafﬁc sign detection and recognition using a CNN ensemble. In: 
2019 IEEE International Conference on Consumer Electronics (ICCE). IEEE, pp. 1–4

72
R. Padhy et al.
24. Wali SB et al (2019) Vision-based trafﬁc sign detection and recognition systems: current trends 
and challenges. Sensors 19(9):2093 
25. Zang D, Wei Z, Bao M, Cheng J, Zhang D, Tang K, Li X (2018) Deep learning–based trafﬁc 
sign recognition for unmanned autonomous vehicles. Proc Inst Mech Eng Part I J Syst Control 
Eng 232(5):497–505 
26. Veliˇckovi´c N, Stojkovi´cZ,Dimi´c G, Vasiljevi´c J, Nagamalai D (2018). Trafﬁc sign classiﬁcation 
using convolutional neural network. In: Computer Science & Information Technology, pp. 177– 
186 
27. Zhao W, Wang Z, Yang H (2018) Trafﬁc sign detection based on faster R-CNN in scene graph. 
In: 2018 International Conference on Mechanical, Electrical, Electronic Engineering & Science 
(MEEES 2018). Atlantis Press, pp. 35–42 
28. Jain A, Mishra A, Shukla A, Tiwari R (2019) A novel genetically optimized convolutional 
neural network for trafﬁc sign recognition: a new benchmark on Belgium and Chinese trafﬁc 
sign datasets. Neural Process Lett 50(3):3019–3043 
29. De Arriba López V, Cobos-Guzman S (2021). Development of a deep learning model for 
recognising trafﬁc signs focused on difﬁcult cases. J Amb Intell Human Comput. 1–13 
30. Shaﬁei SF, Rauf HR, Singh Y. Trafﬁc Sign Detection Using Deep Learning By Group 36. 
31. Cao J, Zhang J, Jin X (2021) A trafﬁc-sign detection algorithm based on improved sparse r-cnn. 
IEEE Access 9:122774–122788 
32. Prakash K (2021) A deep convolutional neural network for trafﬁc sign classiﬁcation and 
detection with voice recognition. Turk J Comput Math Educ (TURCOMAT) 12(6):5522–5529 
33. Haque WA, Areﬁn S, Shihavuddin ASM, Hasan MA (2021) DeepThin: a novel lightweight 
CNN architecture for trafﬁc sign recognition without GPU requirements. Expert Syst Appl 
168:114481 
34. Ellahyani A, Ansari ME, Jaafari IE, Charﬁ S (2016) Trafﬁc sign detection and recognition 
using features combination and random forests. Int J Adv Comput Sci Appl 7(1):686–693 
35. Credi J (2016) Trafﬁc sign classiﬁcation with deep convolutional neural networks. Master’s 
thesis 
36. Hatolkar Y, Agarwal P, Patil S (2018) A survey on road trafﬁc sign recognition system using 
convolution neural network. Int J Current Eng Technol 8(1):104–108 
37. Ekman C (2019) Trafﬁc Sign Classiﬁcation Using Computationally Efﬁcient Convolutional 
Neural Networks. 
38. Tabernik D, Skoˇcaj D (2019) Deep learning for large-scale trafﬁc-sign detection and 
recognition. IEEE Trans Intell Transp Syst 21(4):1427–1440 
39. Bangquan X, Xiong WX (2019) Real-time embedded trafﬁc sign recognition using efﬁcient 
convolutional neural network. IEEE Access 7:53330–53346 
40. Matoš I, Krpi´c Z, Romi´c K (2019) The speed limit road signs recognition using Hough trans-
formation and multi-class Svm. In: 2019 International Conference on Systems, Signals and 
Image Processing (IWSSIP). IEEE, pp. 89–94 
41. Song S, Que Z, Hou J, Du S, Song Y (2019) An efﬁcient convolutional neural network for 
small trafﬁc sign detection. J Syst Architect 97:269–277 
42. Chauhan A, Rastogi A, Gaur A, Singh A (2020) Trafﬁc sign detection using deep learning. Int 
J Eng Appl Sci Technol. 5:355–358 
43. Robertson CE (2020) Deep Learning-Based Speed Sign Detection and Recognition. Doctoral 
dissertation, University of Cincinnati 
44. Bichkar M, Bobhate S, Chaudhari S (2021) Trafﬁc sign classiﬁcation and detection of Indian 
trafﬁc signs using deep learning. Int J Sci Res Comput Sci Eng Inf Technol. 215–219. https:// 
doi.org/10.32628/CSEIT217325 
45. Rachapudi V, Ram MS, Dileep P, Roy TD (2021) Trafﬁc sign classiﬁcation using convolutional 
neural network. Turk. J. Physiother. Rehabil. 32:2 
46. Wan H, Gao L, Su M, You Q, Qu H, Sun Q (2021) A novel neural network model for trafﬁc 
sign detection and recognition under extreme conditions. J Sens 2021:1–16. https://doi.org/10. 
1155/2021/9984787

An Artiﬁcial Intelligence Enabled Model 
to Minimize Corona Virus Variant 
Infection Spreading 
Dipti Dash, Isham Panigrahi, and Prasant Kumar Pattnaik 
Abstract Many nations including India are being very badly affected by the second 
wave of the COVID-19 infections. The critical situation prevails in some states and 
cities of India. The mortality rate varies state to state depending on the health care 
facilities, immunological response of the individuals & comorbidities and vaccination 
status of that particular state. The multiclass prediction model is developed based on 
the status of data available from the different states of India considering their level of 
population density, intensity economic activities, education level, vaccination status 
and timing of lockdown or shut down. Based on this prediction model we can develop 
an application to motivate the internet of health things (IoHT), which can monitor 
the state and help in governing. This paper uses a multi class prediction model using 
Deep Neural Network (DNN) and validates the data set up to the year 2022, with 
accuracy level 98%. In this architecture, we have used 4 hidden layers between input 
and output layer. We have collected data from JHU CSSE Covid-19 and also follow 
our own algorithm to create our own dataset. We have taken 80% of data for training 
purposes and 20% of the dataset as validation purposes. 
Keywords COVID · Deep Neural Network · Multi class Prediction Model ·
Vaccination and IoHT
D. Dash envelope symbol · I. Panigrahi · P. K. Pattnaik 
Kalinga Institute of Industrial Technology (Deemed to Be University), Bhubaneswar, India 
e-mail: dipti.dashfcs@kiit.ac.in 
I. Panigrahi 
e-mail: ipanigrahifme@kiit.ac.in 
P. K. Pattnaik 
e-mail: patnaikprasantfcs@kiit.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_8 
73

74
D. Dash et al.
1 
Introduction 
People in different countries on the earth are now ﬁghting against a common enemy. 
These are the different variants of Corona Viruses. Many human beings have been 
severely infected by these viruses and many have also died. This pandemic also leads 
to big losses to business houses, service sectors and industries across the world. It is 
greatly affecting the economic development goals of all the nations. The livelihood 
of many are at risk [1]. This pandemic is considered to be the worst challenge faced 
by mankind in recent times. 
Now, many nations like India are being very badly affected by the second wave 
of the COVID-19 infections in the year 2021 and the most dangerous and critical 
situation prevails in some states and cities of India [2]. The mortality rate varies 
state to state depending on the health care facilities, immunological response of the 
individuals, comorbidity [3] and vaccination status of that particular area. 
During the ﬁrst wave in the year 2020 of the pandemic, we had no vaccine available 
to prevent or counter these viruses. But, at present, we have a number of vaccines 
at our disposal. The vaccinations are considered to be the most effective means 
to prevent or counter its spread across the world. Vaccines reduce the severity of 
infections, need for hospitalization and controls the overall mortality rate [4, 5]. 
The vaccination drives have not been able to achieve their desired momentum 
till the onset of the second wave. This was because of vaccine hesitancy among 
some sections of the population. This paper tries to ﬁnd out the positive impact of 
vaccination on reducing severity of infection, hospitalization and mortality rate. At 
the same time this paper tries to ﬁnd out the different possible causes of vaccine 
hesitancy in India. The present vaccination status of different states and cities of 
India are studied. The slow vaccination rate is strongly related to active infection 
cases. The results are found out from the available data, at present from different 
continents, countries, and states of India, using statistical tools [6] and mathematical 
prediction models [7]. 
In India, two vaccines were approved in the year 2021 and administered to the 
public [8]. In the ﬁrst phase of the vaccination drive all the front line health workers 
were eligible to take the jab, it is seen that the infection and mortality rate of doctors 
and nurses are much less in the second wave as compared to the ﬁrst wave. But, still 
some of them are critical about the efﬁcacy of these vaccines and some of them are 
apprehensive about their probable side effects in the long term. In the second phase 
the elderly citizens were being vaccinated, this category of people is considered to be 
the most susceptible to COVID virus infections but the pace of vaccinations is very 
slow and not up to the mark yet that time probably leads to adverse implications. 
This paper tries to analyse the available vaccination data of different states of India 
and compares these with similar studies of vaccination data and results of other 
countries. We have collected data from JHU CSSE Covid-19 and also follow our 
own algorithm to create our own dataset. The prediction model is validated with the 
existing data and the data from other countries; those are already gone through the 
second wave of infections in the year 2021. Here we have used the DNN architecture

An Artiﬁcial Intelligence Enabled Model to Minimize Corona Virus …
75
model to validate the dataset because it can solve more real complex problems like 
classiﬁcation. It concluded that level of vaccination plays a very important role in 
reducing the number and severity of infections. Beside the vaccination level, the 
timing of the shutdown also played a very important role in reducing the cases of 
infections in different states of India. 
1.1 
Brief History of Corona 
Many nations, including India, are affected by COVID-19 infections. Even if COVID-
19 infection was ﬁrst diagnosed in Wuhan, China in late 2019, and the World Health 
Organization (WHO) declared the eruption a global public health emergency in Jan 
2020. It is not a new infection. It has evolved from the human coronavirus disease, ﬁrst 
detected in 2002 in Asia SAR-CoV. It leads to severe breathing problems [23]. Coro-
naviruses range drastically in hazard issues [21]. It can cause colds to have important 
symptoms like fever, sore throat, etc., [22]. It can also lead to pneumonia and bron-
chitis. In 2003, severe breathing problems had started globally, and WHO revealed a 
new virus SARS-CoV-1. Around 800 people died and more than 8000 people from 
different places around the globe had been infected [24, 25]. New version of Coron-
avirus was recognized in Sept 2012 and ofﬁcially given the name MERS-CoV [26, 
27]. The Ministry of the health of France 2013 conﬁrmed that it was a spreading 
infection from person to person. In 2015, a plague of MERS-CoV happened in the 
Republic of Korea causing the most important eruption of MERS-CoV out of the 
door in the middle east [28, 29]. Till December 2019, by using tests in the labora-
tory around 2500 cases of MERS-CoV have been conﬁrmed and out of this number 
851 have been lethal, with a mortality rate of about 34.5% [30]. 2020 onwards, the 
whole world has been badly affected by the COVID-19, caused by SARS-CoV-2, 
which is another strain of coronavirus. Provisionally this infection was called novel 
coronavirus (2019-nCov). It was ﬁrst detected in Wuhan city, China and then the 
whole world came under the grip of this infection. As of 12 July 2022, there were 
a minimum 6,354,564 [31] conﬁrmed deaths and extra than 55,341,787 [31] cases 
in this pandemic. Now a research concern is whether this disease came from bats 
or any other animal. It is a contagious disease, human to human transmission and 
spreading rate is also high. As this type of pandemic may come in future, we should 
always be aware of it and try to reduce the spreading rate by analyzing properly. We 
have to live with corona taking certain precautions and restrictions. 
1.2 
Pandemic Challenges 
This pandemic affected India and all other countries. It has created mental health 
issues of children, adults and old aged people. The pandemic has equally impacted 
poor and rich countries. Initially the mortality rate was very high in India because

76
D. Dash et al.
of high population density and lower medical resources, the condition became more 
critical. In the pandemic period, people in India were facing problems with less 
supplies of important medicines, lack of proper information, ﬁnancial loss, stigma 
and infection fear. 
India is one of the densest countries in the world. Here it is not unusual for more 
than one family, families with multiple generations to live together. So the biggest 
challenge for all physical distancing mandates. Some people can access advanced 
solid care. But in general here in India the health infrastructure is not adequate for 
example they do not have enough beds and equipment such as ventilators. Especially 
in rural areas hospital capacity is a biggest concern. The high rates of comorbidity 
conditions that are contributing to COVID-19 spikes. India is a low income country, 
so it is very difﬁcult for all to follow a better developed hygiene standard. People were 
unaware of how long people to be quarantined. An infected person has a potential to 
infect hundreds, as some of the COVID virus strains are highly contagious. The social 
media misinformation on COVID. Nowadays with growing social media intake by 
the public leads to many wrong procedures and practices. The conduction of mass 
gathering emerges as a super spreader event with high rate increase of cases. The 
potential super spreader events were religious, personal and political gatherings. 
From the available data, It has been observed through statistical analysis, there is a 
direct inﬂuence of these gatherings on the increase in overall number of COVID-19 
cases in some cities and localities. 
2 
Background 
We have considered ﬁve important parameters for all the Indian states and three levels 
based on the stages of infection and their present trend and probable situation in near 
future. The deep learning technique is used to predict the caseload for different levels 
of vaccination and timing of lockdown for different combinations of data sets and 
further validate it from the actual data available from different states of India. The 
results of ﬁnding are very closely agreeing with the actual infections reported. The 
data of countries like America, U.K and Israeli with better vaccination level and its 
effects on the caseloads are considered in giving extra weight to vaccination and 
lockdown in reducing infection level. This gives a better understanding of beneﬁcial 
effects of vaccination in curbing the COVID virus induced pandemic in other coun-
tries [4, 5, 10]. It concludes that universal vaccination is the better alternative instead 
of implementing strict lockdowns. The lockdowns lead to big economic loss to the 
country and great hardship to the general public. Now the most pressing issue is to 
address vaccine hesitancy among some sections of the public. This methodology and 
the same mathematical model may also be used to predict the caseload in similar 
situations, in other countries. This will help governments in swift planning of their 
responses and better management of limited resources available with them.

An Artiﬁcial Intelligence Enabled Model to Minimize Corona Virus …
77
We consider ﬁve important factors in the following order. First, the population 
density, which increases probability of more proximity, between the peoples [8, 11]. 
Second, the business activities which required a high level of mobility and more 
close contact between peoples [9]. Third, the education levels of individuals also 
play a very important role in compliance of COVID appropriate behavior. Fourth, 
the vaccination level plays the most important role [4, 13–15]. Finally, the ﬁfth 
important point is timing of lockdown or shutdown implementations [8, 9]. 
All these above ﬁve parameters have signiﬁcant correlations and bearing on 
conﬁrmed caseloads. In turn, these ﬁve factors also show the major effects in deter-
mining the caseloads or conformed cases count, which were being reported from 
different states of India, in the months of February, March, April and May 2021.This 
is the important period of second waves in India. 
In Fig. 1 is clearly showing, the new cases are rising at an alarming rate in Asia. 
The signiﬁcant contributions to new cases are coming from India only because of 
its population density, new mutants of COVID viruses and low vaccination status. 
The new variants are found to be more infectious then the original COVID-19 strain. 
The total conﬁrmed cases are 21,491,598, the recovered cases are17612351 and the 
death cases are 234,083 as per the data available till ﬁrst week of May 2021. 
It can be clearly seen in the Fig. 2 death cases are rising very fast. The high 
mortality rate in the year 2021 is due to the high caseloads. Hospitals have been over 
stretched and over loaded with limited medical facilities. The health workers are 
overburdened too. The big cities like Mumbai, Delhi and Bengaluru are also reeling 
under severe resource constraints in this second wave in India.
In the plot shown in Fig. 3, we can clearly see the top ﬁve states of India in terms of 
conﬁrmed cases of infections in million, incidentally these states are also the states 
having high population densities. Some have a high level of industrial activities 
or business activities too, which demands very close contacts in shop ﬂoors and 
ofﬁces. Some have also delayed lockdown or shutdown implementations because of 
elections or political rallies, religious congregations and economic considerations. 
Besides, some of the states have low or medium vaccination levels till now. The 
Table 1 clearly shows how the infection levels are being directly affected by these
Fig. 1 Continent wise new cases in second wave of COVID 

78
D. Dash et al.
Fig. 2 Plot of cumulative 
death cases in the year 2021 
in India
ﬁve features in ten Indian states during the peak time period of the second waves in 
the year 2021. In the Fig. 4 the latest treads are shown and it can be seen the cases 
are rising slowly again. 
The total score for ﬁnding out the infection levels of different states are calculated 
using the weights assigned to ﬁve features in the Table 2. For example taking the case 
of Maharashtra, population density was high there so weight of population density 
over there is 3. Accordingly we can ﬁnd the weight of the other 4 features. The target 
predicted output infection levels as per the summation of all the ﬁve features and 
their level’s weight scores are categorized as per their range, low (5–9), medium 
(10–14) and high (15–19) respectively. We have elaborated the algorithm below. In 
total 243 possible data sets are considered for DNN modelling.
Fig. 3 Top ﬁve states of India in terms of conﬁrmed cases during 2nd wave in the year 2021

An Artiﬁcial Intelligence Enabled Model to Minimize Corona Virus …
79
Table 1 Five factors affecting COVID caseloads in ten states 
Name of 
State or City 
COVID 
Cases 
Population 
Density 
Industry 
or 
Business 
activities 
or 
Mobility 
Education 
status or 
Appropriate 
Behaviour 
Vaccination 
Status 
Lockdown 
/Shutdown 
enforced 
time /Period 
Maharashtra
High
High
High
Medium
Medium
Delayed 
Kerala
High
High
High
High
Medium
Delayed 
West Bengal
Medium
High
Medium
High
Medium
Delayed 
Karnataka
High
High
High
High
Medium
Timely 
Delhi
High
High
High
Medium
Low
Timely 
Bihar
Medium
High
Low
Low
Low
Promptly 
Odisha
Medium
Medium
Medium
Medium
Medium
Timely 
U Pradesh
High
High
Low
Low
Low
Delayed 
H Pradesh
Low
Low
Low
Medium
High
Promptly 
Gujrat
Medium
High
High
High
High
Timely 
0 
100 
200 
300 
400 
500 
600 
700 
800 
900 
1000 
Jan2022
Feb2022
Mar2022
Apr2022
May2022
June2022
July2022 
Trend of death cases in India 
Fig. 4 death cases in the year 2022 in India
Table 2 Weights assigned to 
ﬁve features for three 
different levels 
Features
Weights Assigned 
Low
Medium
High 
Population Density
1
2
3 
Education Level
1
2
3 
Business Activity
1
2
3 
Vaccination Status
5
3
1 
Lockdown Timing
5
3
1

80
D. Dash et al.
Algorithm 
/* Output: Infection Level(IL) */ 
/*Input: 
Population Density( PD)[Low=1,Medium=2, High=3], 
Education Level( EL)[Low=1,Medium=2, High=3], 
Business Activity( BA)[Low=1,Medium=2, High=3], 
Vaccination Status (VS)[Low=5,Medium=3, High=1], 
Lockdown Timing (LT)[Low=5,Medium=3, High=1] */ 
Input PD, EL, BA, VS, LT 
IL=(PD+EL+BA+VS+LT) 
if (IL>=5 and <=9) 
then Print “Infection Level is Low” 
else if (IL>=10 and IL<=14) 
then Print “Infection Level is Medium” 
else Print “Infection Level is High” 
3 
Prediction Using Deep Neural Network Model 
In this paper, we have used deep neural networks [16, 17, 18]. This data science tech-
nique is being successfully used in many application areas like robotics, bio infor-
matics and engineering. The advantage of deep neural networks is, it does not only 
depend upon algorithms but also analyse and predict based on some past experiences. 
We can see in most of the deep learning supervised problems, there is a prediction 
of an output Y using a set of inputs X. The set of inputs X are the independent features 
and Y are called dependent variables or labels. In this architecture, three layers are 
there input layer, hidden layer and output layer. Each layer has a number of neurons. 
Mathematically we can express, 
u p per 
Y e quals upper T left parenthesis upper S right parenthesis
up per  T i s the activ ation fun ctio n a
n
d u p er S e
quals sigma summation w Subscript i Baseline times x Subscript i Baseline plus b
where wi are weights and b is bias. 
The data sets are prepared after analyzing the data of some states of India, we 
concluded that there are clear correlations between conform cases of COVID infec-
tions with the population density, education level of people, level of business activi-
ties, vaccination status and lockdown implementation timings of those states. Further, 
each of these ﬁve parameters were assigned some weights. Then, these parameters 
are given three levels. like low, medium and high. In the data set, these ﬁve features

An Artiﬁcial Intelligence Enabled Model to Minimize Corona Virus …
81
Fig. 5 The DNN Model used for prediction of COVID infection levels in different situations 
are the inputs to our multi classiﬁcation model for predicting the target multilevel 
output as low, medium and high COVID infection levels. 
In this paper as shown in the Fig. 5, the DNN architecture used has 4 hidden layers 
between the input and output layers [19, 20]. We have tried the model taking different 
hidden layers and neurons in each layer and ﬁnally got the best result having 4 hidden 
layers. These hidden layers have 12,15,8 and 10 neurons or nodes respectively. The 
input layer has 5 nodes and the output layer has only one with multiple classes. We 
have taken 80% of data for training purposes and 20% of the dataset as validation 
purposes. We have used an open source API called KERAS written in python, running 
on the top of the machine learning platform TensorFlow have executed on google 
COLAB network well suited for data analysis and machine level problems. 
Here, we have used Adam optimization algorithm for training the model as it 
updates network weights iteratively based on training data. In each hidden layer 
RELU activation function is used. It is a very famous activation function in the deep 
learning domain and used in most cases these days. In the output layer, SoftMax 
functions have been used as it is a multi class classiﬁcation problem. When there 
are multiple label classes, compute the cross-entropy metric between the labels and 
prediction. 
4 
Result and Performance Analysis 
To evaluate the performance of the developed model, a confusion matrix is used. 
There are four elements in the confusion matrix for binary classiﬁcation problems. 
These are true positive (TP), true negatives (TN), false positive (FP) and false negative 
(FN), Fig. 6 describes the confusion matrix.

82
D. Dash et al.
Fig. 6 The multiclass 
confusion matrix 
Each entry in the confusion matrix denotes the number of predictions made by the 
model where it classiﬁed the classes correctly or incorrectly. In our dataset, we have 
three class labels namely low, medium and high. Here, unlike binary classiﬁcation 
there are no positive or negative classes. As we have three classes, our confusion 
matrix is a 3 × 3 matrix with the left side showing the actual class and at the top 
showing the predicted class. Each element j of the matrix would be the number of 
items with actual class j that were classiﬁed as being in class i. 
In the Table 3 classiﬁcation report, 20% of the data is being taken as a test data 
set, after validation from the confusion matrix, it yields accuracy 98%. The accuracy 
rate is found to be good here. The target is not 100% accuracy but high precision 
with higher recall value. The F1 score of each class to have a single measure for the 
whole model. 
From the plot of accuracy in Fig. 7 and the plot of loss in Fig. 8, it can be seen 
that the model has comparable on both train and validation dataset. We can also see 
that the model has not over ﬁtted the training dataset.
The DNN results also validated the extra importance given to vaccination and 
lockdowns in controlling spread of COVID infections in the populations. The vacci-
nation is deﬁnitely a better option than lock downs for any governments in the long 
run to protect their economies. 
The Fig. 9(a) shows the vaccination status of India as compared to US and UK till 
ﬁrst week of May 2021, it is much lesser in percentage of the total population then 
the other two counties.
Table 3 Classiﬁcation Report 
Levels
Precision
Recall
Fscore
Support 
Low
0.92
1
0.96
12 
Medium
1.00
1
1
9 
High
1.00
0.98
0.98
28 

An Artiﬁcial Intelligence Enabled Model to Minimize Corona Virus …
83
Fig. 7 DNN Accuracy plot 
Fig. 8 DNN loss plot

84
D. Dash et al.
(a)   
(b)                                  
40% 
47% 
13% 
Atleast one Dose 
US 
UK 
INDIA 
52.30% 
47.70% 
0.12% 
Male Vaccinated 
Female Vaccinated 
TransGender 
Fig. 9 a The percentage of population vaccinated in India, US and UK b The percentage of Male 
and Female population vaccinated in India 
The incomplete vaccination in elderly population, health workers and front line 
workers is because of vaccine hesitancy prevailing among them. This vaccine hesi-
tancy is because of misinformation in social media and fear of side effects among 
these groups. It is found that the vaccine hesitancy is more prevalent in the case 
of rural population and women population as shown in Fig. 9(b), because fear of 
infertility may affect them after vaccination. Governments should take more steps 
to increase the awareness among these groups to achieve their vaccination targets to 
successfully win over COVID pandemic. 
5 
Conclusion 
It concluded that level of vaccination plays a very important role in reducing the 
numbers and severity of infections. Beside the vaccination level, the timing of the 
lockdown has also played a very important role in reducing the cases of infections in 
different states of India. It concludes that universal vaccination is the better alternative 
instead of implementing strict lock downs. The lock downs lead to big economic loss 
to the country and great hardship to the general public. Now the most pressing issue 
is to address vaccine hesitancy among some sections of the public. We have tried a 
new multi input multi output prediction model for spreading of covid like diseases. 
This methodology may also be used to predict the caseload in similar situations, in 
other countries. This will help governments in swift planning of their responses and 
better management of limited resources available with them.

An Artiﬁcial Intelligence Enabled Model to Minimize Corona Virus …
85
References 
1. kavitha G (2020) A review on Tamil Nadu struggling with covid-19 pandemic. J Eng Sci 
11(6):6. ISSN no: 0377–9254 
2. Ranjan R, Sharma A, Verma MK (2021) Characterization of the second wave of covid-19 in 
India, medrxiv, 21255665 
3. Paces J, Strizova Z, Smrz D, Cerny J (2020) Covid-19 and the immune system. Phys. Res. 
69:379–388 
4. Aran D (2021) Estimating real-world covid-19 vaccine effectiveness in Israel using aggregated 
counts medrxiv 21251139 
5. Ke R, Romero-Severson E, Sanche S, Hengartner N (2021) Estimating the reproductive 
number of sars-cov-2 in the United States and eight European countries and implications 
for vaccinations. J Theor Biol 517: 110621 
6. ghaffarzadegan N, Rahmandad H (2020) Simulation-based estimation of the early spread of 
covid-19 in Iran Syst Dyn Rev 36(1):101–129 
7. Binti Hamzah F, Lau C, Nazri H, Ligot D, Lee G, Tan Cl, et al (2020) Coronatracker: worldwide 
covid-19 outbreak data analysis and prediction. Bull World Health Organ 1:1–32 (2020) 
8. Sanchez-Caballero S, Selles MA, Peydro MA, Perez-Bernabeu E (2020) An efﬁcient covid-19 
prediction model validated with the cases of china, Italy and Spain: total or partial lockdowns? 
J Clin Med 9:1547. https://doi.org/10.3390/jcm9051547 
9. Marschner IC (2020) Back-projection of covid-19 diagnosis counts to assess infection incidence 
and control measures: Analysis of Australian data. Epidemiol Infect 148(e97):1–8 
10. Mason T (2021) Effects of bnt162b2 MRNA vaccine on covid-19 infection and hospitalisation 
among older people: matched case control study for England, medrxiv 26(28):21255461 
11. Hamidi S, Sabouri S, Ewing R (2020) Does density aggravate the covid-19 pandemic. J Am 
Plan Assoc 86(4):495–509. https://doi.org/10.1080/01944363.2020.1777891 
12. Carteni A, Di Francesco L, Martino M (2020) How mobility habits inﬂuenced the spread of 
the covid-19 pandemic: results from the Italian case study. Sci Total Environ 741:140489 
13. Shayak B, Sharma MM, Mishra AK (2021) Impact of immediate and preferential relaxation 
of social and travel restrictions for vaccinated people on the spreading dynamics of covid-19: 
a model-based analysis. Medrxiv https://doi.org/10.1101/2021.01.19.21250100 
14. Klimek P, Hanela R (2020) A network-based explanation of why most covid-19 infection 
curves are linear. PNAS 117(37):22684–22689 
15. Chodick G, Tene L (2021) The effectiveness of the ﬁrst dose of bnt162b2 vaccine in 
reducing sars-cov-2 infection 13–24 days after immunization: real-world evidence. Medrxiv 
29:21250612 
16. Farooq J, Abid Bazaz M (2021) A deep learning algorithm for modeling and forecasting of 
covid-19 in ﬁve worst affected states of India. Alexandria Eng J 60(1):587–596 
17. Alazab M (2020) Covid-19 prediction and detection using deep learning. Int J Comput Inf Syst 
Ind Manag Appl. 2:168–181 
18. Devaraj J (2021) Forecasting of covid-19 cases using deep learning models: Is it reliable and 
practically signiﬁcant? Results Phys 21:103817 
19. Shorten C, Khoshgoftaar TM, Furht B (2021) Deep learning applications for covid-19. J Big 
Data 8:18 
20. Zoabi Y, Deri-Rozov S, Shomron N (2021) Machine learning-based prediction of covid-19 
diagnosis based on symptoms. NPJ Digital Med 4:3 
21. Fehr AR, Perlman F (2015) Coronaviruses: an overview of their replication and pathogenesis. 
Methods Molecular Biol 1282:1–23. https://doi.org/10.1007/978-1-4939-2438-7_1. ISBN 978-
1-4939-2438-7. PMC 4369385. PMID 25720466 
22. Liu P, Shi l, Zhang W, He J, Liu C, Zhao C, et al (2017) Prevalence and genetic diversity 
analysis of human coronaviruses among cross-border children. Virology J14(1):230. https:// 
doi.org/10.1186/s12985-017-0896-0. PMC 5700739. PMID 29166910 
23. Forgie S, Marrie TJ (2009) Healthcare-associated atypical pneumonia. Seminars Respir Critical 
Care Med. 30(1):67–85. https://doi.org/10.1055/s-0028-1119811. PMID 19199189

86
D. Dash et al.
24. Pasley J (2020) How SARS terriﬁed the world in 2003, infecting more than 8,000 people and 
killing 774. Bus. Insider. Accessed 08 Nov 2020 
25. Li
F,
Li
W,
Farzan
M,
Harrison
SC
(2005)
Structure
of 
SARS
coronavirus
spike
receptor-binding
domain
complexed
with 
receptor. Science 309(5742):186468. Bibcode:2005sci...309.1864l. https://doi.org/10.1126/ 
science.1116480. Pmid 16166518. S2cid 12438123 
26. Doucleff M, Scientists go deep on genes of SARS-like viruses. Associated press. Archived from 
the original on 27 Sep 2012. Accessed 27 Sep 2012 
27. Falco M, New SARS-like virus poses medical mystery. CNN health. Archived from the original 
on 01 Nov 2013. Accessed 16 Mar 2013 
28. Sarkar N, Mandal BK, Paul S New SARS-like virus found in middle east. Al-jazeera. 24 Sep 
2012. Archived from the original on 09 Mar 2013. Accessed 16 Mar 2013 
29. Sang-hun C, Mers virus’s path: one man, many South Korean hospitals. The New York 
Times. Archived from the original on 15 Jul 2017. Accessed 01 Mar 2017 
30. Middle East respiratory syndrome coronavirus (MERS-COV). Who? Archived from the 
original on 18 Oct 2019. Accessed 10 Dec 2019 
31. Covid-19 dashboard by the center for systems science and engineering (CSSE) at Johns Hopkins 
university (JHU). Arcgis. Johns Hopkins University. Accessed 12 Jul 2022

SoundMind: A Machine Learning 
and Web-Based Application 
for Depression Detection and Cure 
Madhusha Shete, Chaitaya Sardey, and Siddharth Bhorge 
Abstract This paper presents a machine learning and web-based application for the 
detection of depression. The system mainly serves two components: two machine-
learning-based models to detect depression and a web-based application. The ﬁrst 
machine learning model is implemented to classify the positive and negative text 
entered by the user/patient. The negative text states the use of words indicating 
depression, which can be termed as one factor in deciding a patient’s mental health. 
The model is built using libraries such as Natural Language Toolkit (NLTK), and 
WordCloud. The second model predicts the presence of depression based on multiple 
health-related features such as the patient’s data related to various other disorders 
he/she is having, age, weight, BMI, blood-related features such as levels of calcium, 
CO2, phosphorus, iron, etc., and work-life related parameters. The prediction is 
carried out based on the classiﬁcation result implemented using Logistic Regression. 
The model predicts the results with 91.85% test accuracy, 93% precision, 99% recall, 
and 96% f1 score. The above-mentioned models are deployed on the web application. 
The web application not only helps in predicting mental health but also suggests the 
proper treatment to cure the condition. 
Keywords Mental health · Depression detection · Machine learning · Natural 
language processing (NLP) · Logistic regression
M. Shete envelope symbol · C. Sardey · S. Bhorge 
Department of E&TC Engineering, VIT Pune, Pune 411037, India 
e-mail: madhusha.shete19@vit.edu 
C. Sardey 
e-mail: chaitanya.sardey19@vit.edu 
S. Bhorge 
e-mail: siddharth.bhorge@vit.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_9 
87

88
M. Shete et al.
1 
Introduction 
More than 2640 lakh individuals worldwide suffer from depression [1], making it a 
widespread disorder. Depression is fundamentally a mental health condition having 
main symptoms such as a continuously downbeat mood or a diminished involvement 
in various activities. Up to 85% of individuals from low- and middle-economy coun-
tries do not obtain treatment for their disorder [2], even though established, efﬁcient 
treatments exist for such mental diseases. Lack of ﬁnances is one of the reasons 
behind not taking proper treatment for mental health-related problems. The lack of 
awareness about the severity of mental health issues and misconceptions leading to 
social stigma is also the main barriers to the complete cure of the individual’s mental 
health. However, inaccurate assessment is arguably one of the biggest obstacles [3]. 
Depression is not diagnosed in patients because the patient does not approach the 
doctor. 
Early diagnosis of depressive symptoms, evaluation, and therapy can signiﬁcantly 
improve the chance that symptoms and the underlying illness can be controlled, along 
with helping to reduce any side effects on one’s health, well-being, and quality of 
life in terms of personal, professional, and social circles. However, it is challenging 
and time-consuming to identify depression symptoms. The most prevalent practices 
currently used are in-hospital consultations and question-based surveys conducted 
by doctors or organizations. 
The alternative to clinical consultations and physical interviews with doctors 
regarding mental health can be replaced by online consultations and personalized 
treatments. The ﬁelds such as Artiﬁcial Intelligence (AI), ML, and the web have 
made it possible to diagnose diseases and seek proper treatment without going to the 
hospital. The machine learning-based web application uses patients’ medical data, 
current emotional conditions including thought processes, and traditional question-
naire survey-based methods to detect depression in the patient. The web application 
also includes various depression treatment methods ensuring ﬁrst aid to mentally 
unﬁt people. 
2 
Literature Survey 
The use of natural language processing on Twitter feeds to undertake emotion analysis 
with an emphasis on depression was suggested in [4]. The data pre-processing module 
was used to create datasets by systematically churning the data through tokenization, 
stemming, and stop word removal. The POS tagger then determined which parts of 
the text should be used. In the training phase, the text classiﬁer was trained on the 
pre-processed text data from Twitter. The SVM classiﬁer was fed with a vector 
produced through the processed tweet and the label. The advantage of this model 
was that pre-processing of text was done efﬁciently by using tokenization, stemming, 
stop word removal, and POS tagger and the disadvantage of this model was that the

SoundMind: A Machine Learning and Web-Based Application …
89
methodology did not yield results as per expectations. NB classiﬁer was accurate up 
to 83%. CNN architecture implemented in [5] for depression analysis improved and 
optimized word embedding. Various stages involved were pre-processing, creating a 
vector, and then feeding it into the classiﬁer. The main advantage of this model was 
the use of a neural network for word embedding made it more efﬁcient to learn feature 
representation tasks, but the time required for this model was more than the models 
that used NLP for pre-processing. An unsupervised approach using Clustering was 
proposed in [6]. The data containing 8000 tweets were gathered using keywords, 
hashtags, and manual gathering. Data Pre-processing was carried out using NLTK 
and Genism libraries. Modiﬁed c means and Extended K means algorithms were 
implemented. The pros of this model were that the approach detects the risk in users 
at an early stage. The classiﬁcation was accurate in this model. The model could 
detect common slang and short forms. The model was only for the English language. 
The use of mixed language limited the performance. 
The sadness and suicidal tendencies of persons affected by the Covid ’19 s lock-
down were targeted [7]. In the proposed paper, they used a signature-based senti-
ment analysis approach built in python. It contains 3 modules. 1) Data scraping from 
Twitter – In this, they have used the python module tweeps to access Twitter API. 
2) Keywords and Hashtags used - #covid19 etc. 3000 public tweets are extracted. 
3) Filtering and pre-processing in this Removal of special characters, usernames, 
web links Duplicates records. Emoticon recognition. Sentiment analysis helped in 
understanding - A) affected tweets Entity Recognition. B) Emoji in tweet C) Iden-
tiﬁcation of emotion for this tool called Vader lexicon is used present in the NLTK 
module of python & classify the tweet. The rate constraint in scraping data from 
social networking websites is a disadvantage of this strategy. Text message perfor-
mance might be hampered by the usage of abbreviations and acronyms. For depres-
sion identiﬁcation, the methodology in [8] employs deep learning and anaphora 
resolution. The database was obtained by utilizing the Twitter API to gather public 
tweets. Pretrained GloVe word vectors were used to choose a set of users who were 
depressed. The GloVe is a strategy for unsupervised machine learning and natural 
language processing that encodes a word as a collection of word vectors. The predic-
tion model’s structure illustrates how supervised neural networks in an ML model 
classify users and generate a shifting pattern of generated text on psychological 
health or other topics. ML social network 1. Tweet encoder architecture 2. A user 
encoder 3. Tweet classiﬁcation 4. Word focus on a tweet 5. Pay attention to tweets 
6. Categorization of users’ Keras library with TensorFlow backend for training. But 
the time required for this model was a little more than the existing models. 
A machine learning method employed in [9] studied the mental well-being of 
medical personnel under difﬁcult circumstances. The study discussed the develop-
ment of a new methodology for predicting mental wellness. A dataset from the 
"Mental Health Status of Medical Workers during COVID-19" survey was used 
to create the algorithm. To predict mental analysis logistic regression, binary Bat 
algorithm, and hybrid enhanced dragonﬂy method were used. The proposed new 
algorithm BPNNIGCBA was the model’s main advantage. BPNN enhanced mental 
health predictive performance. MIV evaluated the contribution of feature variables on

90
M. Shete et al.
mental health. GCBA increased the performance of BPNN by reducing the compu-
tational burden. To speed things up, IGCBA created a nonlinear equation. However, 
the architecture of the BPNN has not been improved, and the parameters in the BP 
network have not been optimized, making further optimization difﬁcult for a speciﬁc 
number of neural networks. Training and testing methods used in [10] were a combi-
nation of the Sentiment140 dataset and the Depressive Tweet Processing dataset. The 
train and test model datasets had been converted to format (10,345, 140) tensors. 
Google’s pre-trained word2vec model was used. A deep learning model: LTSM was 
also implemented. A CNN-based architecture was built, and a layer consisting of an 
input layer and 9 hidden layers were passed through one exit layer. Hidden layers 
consisted of a maximum pooling layer, a high-density layer, and a dropout layer. The 
model was trained with a learning rate of 0.001, Adam Optimizer, and 10 epochs. The 
split ratio was 7: 3. The accuracy of this proposed LTS MCNN-based architecture 
was 99.42%, which was more accurate than other models. 
The survey [11] proposed a dataset for mental health analysis and depression 
detection. The dataset was created by evaluating the answers to an online mental 
health questionnaire. The survey was created with Google Forms and sent online for 
volunteer participation. A questionnaire of 395 participants were collected for the 
study. This web-based the questionnaire consisted of questions that characterized 18 
prominent symptomatic markers for each disease. Each symptom was used as a func-
tion to train a classiﬁer that can predict the type of disorder. The binary Classiﬁcation 
model was created using a Naïve Bayes classiﬁer. This model gave an accuracy of 
92.15%. This model can help determine underlying illness. The mentioned charac-
teristics may not be adequate to identify whether the individual has a psychological 
condition because they only produce anxiety disorder and mood disorder, which 
can sometimes lead to misdiagnosis. After all, the person may be suffering from 
another sickness. 
3 
Methodology 
A system for the recognition as well as cure of depression is discussed in this paper. 
The paper discusses two machine learning models and web applications. The ﬁrst 
machine learning model is implemented to classify the positive and negative text 
entered by the user/patient. The second model predicts the presence of depression 
based on multiple health-related features such as the patient’s data related to various 
other disorders he/she is having, age, weight, BMI, blood-related features such as 
levels of calcium, CO2, phosphorus, iron, etc., and work-life related parameters. 
The two machine learning models are deployed on a web application. The web 
application also has traditional approaches to detect mental health disorders along 
with the machine learning approach. The machine learning models and the web 
application are discussed in detail in the further paper.

SoundMind: A Machine Learning and Web-Based Application …
91
3.1 
Model-1: Positive and Negative Text Classiﬁcation 
The model implements the classiﬁcation of positive and negative texts based on the 
individual words used in the sentence/input. The data used for the training of the 
model contains sample text labeled either positive or negative. 
Figure 1 showcases the data used for the training of this model. The data labeled 
as 0 indicated the positive class. The data labeled as 1 indicates the negative class. 
This data is fed to the model for further classiﬁcation. 
The NLTK library is used in this model. It is a collection of software tools in 
statistical language processing. One of the most potent NLP libraries, it includes 
tools that allow computers to comprehend human language and respond appropriately 
when it is used. Along with NLTK WordCloud library is also used in this model. 
The visual depiction of the data or information is a word cloud. By making the most 
frequently utilized words look bigger or bolder in comparison to the other words 
around them, it demonstrates the popularity of certain words or phrases. The library 
helped in visualizing the most commonly used words in the negative sentences. This 
indeed helped in the classiﬁcation of positive and negative sentences/ inputs. 
Figure 2 indicates the positive words given by WordCloud. Figure 3 indicates the 
negative words given by WordCloud. The words appearing bigger are having a larger 
frequency in the sentences and the comparatively smaller size words are having fewer 
frequencies.
The data comprising of these positive and negative words are further classiﬁed for 
the detection of positive and negative sentences or statements given by the patients. 
The Naive Bayes algorithm is implemented for the classiﬁcation purpose considering 
the effectiveness and quick speed. The model can correctly classify the positive and 
negative statements with 96% precision, and 95% recall.
Fig. 1 Sample data for text 
classiﬁcation 

92
M. Shete et al.
Fig. 2 WordCloud output: 
Positive words 
Fig. 3 WordCloud output: 
Negative words
3.2 
Model-2: Depression Detection Based on Patient Reports 
The model implements the predictive analysis of the presence of depression based 
on multiple health-related features such as the patient’s data related to various other 
disorders he/she is having, age, weight, BMI, blood-related features such as levels of 
calcium, CO2, phosphorus, iron, etc., and work-life related parameters. The model

SoundMind: A Machine Learning and Web-Based Application …
93
uses the medical records of patients to predict their depression state. The dataset used 
in this model has been taken from the "Centers for Disease Control and Prevention 
National Health and Nutrition Examination Survey”. 
The dataset was pre-processed. Firstly, the missing value treatment was imple-
mented which included imputation using either mean or mode, and scaling of data to 
get a better result. The dataset had 500 plus attributes. The more relevant and strongly 
correlated attributes were kept and rest of them were dropped to reduce overﬁtting. 
Figure 4 illustrates the block diagram of the depression detection model. The 
model takes the input data having various features such as race, citizenship, marital 
status, pregnancy or not, birthplace, veteran, ﬁrst cancer type, second cancer type, 
third cancer type, arthritis type, full-time work, work type, out of work, vigorous 
recreation, moderate recreation, lifetime alcohol consumption. The data frame 
containing all the above features is pre-processed. The one-hot encoding method 
is applied to data for the encoding of non-numerical data. Further, the encoded data 
is scaled to avoid the large variation in the numbered values as it may lead to a 
decrease in accuracy. 
The pre-processed data is passed to the clustering algorithm. K-Means clus-
tering algorithm is implemented for the clustering purpose. It is an unsupervised 
learning-based algorithm for addressing clustering-related problems. It follows a 
centroid-based strategy. The clusters formed using K-Means have unique centroids. 
The number of clusters is ﬁnalized according to the silhouette score. The clustering 
algorithm was carried out for 6 clusters. 
The clustered data is passed further to the classiﬁer for classiﬁcation purposes. 
The logistic regression algorithm is implemented to achieve bi-class classiﬁcation as 
it is more prominent in binary classiﬁcations. The computations of sigmoid functions 
that help in binary classiﬁcation are implemented in the logistic regression classiﬁer. 
The model can yield 91.85% test accuracy, 93% precision, 99% recall, and 96% f1 
score.
Fig. 4 Block Diagram for 
Patients’ Data-Based 
Depression Detection 

94
M. Shete et al.
3.3 
Web Application 
The two machine learning models mentioned above are deployed on the web-based 
application for user assistance. Along with these machine learning algorithms the web 
applications also contains traditional survey and question–answer-based approach 
for the detection of depression. The web application also suggests treatments and 
depression-curing remedies such as meditation, positive thinking, communication, 
vitamin supplements, music therapy, and many more. 
The web application is developed using front-end and back-end technologies that 
involve HTML5, CSS3, Bootstrap, JavaScript, JQuery, and PHP. 
4 
Result and Discussion 
The machine learning and web-based application for the detection and treatment 
of mental health conditions such as depression are based on two machine learning 
models, a traditional question-based doctoral survey, and multiple remedies as well 
as medications to detect and cure depression. 
The NLTK and WordCloud-based positive and negative text classiﬁcation model 
successfully detects the depressive text and classiﬁes it as negative indicating the 
presence of depression. If the classiﬁcation result turns out to be positive, it indicates 
that there are no depression symptoms in the patient. The below-mentioned Table 1 
indicates the precision, recall, and F1-score of the text classiﬁcation model. 
The second model is the depression detection model based on the patient’s reports. 
The model takes inputs that are the major indications of an individual’s health and 
predicts the presence of depression. The Logistic regression classiﬁcation algorithm 
is implemented for detection purposes. Table 2 indicates the measures of the accuracy 
of the model. 
Table 1 Precision, recall and F1 score text classiﬁcation model 
Sr. No
Classiﬁer
Precision
Recall
F1 
Score 
1
Logistic 
Regression 
0.96
0.99
0.97 
2
Naive 
Bayes 
0.96
0.95
0.95 
Table 2 Precision, recall and F1 score of Patients’ Data-Based Depression Detection model 
Sr. No
Classiﬁer
Precision
Recall
F1 
Score 
1
Logistic 
Regression 
0.93
0.99
0.96

SoundMind: A Machine Learning and Web-Based Application …
95
Fig. 5 Confusion matrix for 
Patients’ Data-Based 
Depression Detection model 
Table 3 Comparison of proposed system with existing systems 
Sr. No
System Type
Approach
Accuracy 
1
[2] Machine Learning
SVM, RF, DT, KNN, NB
Max 90% 
2
[4] Machine Learning
Multinomial NB, SVM
83%, 79% 
3
[5] Deep Learning
CNN, BILSTM
81.81%, 80.51% 
4
[8] Deep Leaning
Usr2Vec Model
84.38% 
5
[Proposed System] Machine 
Learning 
Logistic Regression (LR), 
Naïve Bayes(NB) 
Model 1 – 
LR 96%, 
NB – 96% 
Model 2 – 
LR 91.85% 
Figure 5 represents the confusion matrix of the medical reports and the patient’s 
data-based depression detection model. The value 6526 represents the true posi-
tive detections. The value 92 represents the false positive detection. The value 499 
presents the false negative detections and the value 135 represents the true negative 
detection. 
The comparison of the proposed system and the existing systems for depression 
detection is illustrated in Table 3. The model for depression detection was imple-
mented on an Intel Core i5-7200U CPU with a 2.50 GHz processor and windows 10 
operating system. 
5 
Conclusion and Future Scope 
This paper presented a machine learning and web-based application for the detection 
of depression. The machine learning models illustrated in the paper are successfully 
deployed as an application for the detection of depression. The model based on the 
medical reports and health-related data was able to yield 91.85% test accuracy, 93% 
precision, 99% recall, and 96% f1 score. The model for the classiﬁcation of positive

96
M. Shete et al.
and negative statements was able to correctly classify the positive and negative state-
ments with 96% precision, and 95% recall. The model-based results and the web 
application have shown a notable effect in the detection and treatment of depres-
sion. The system’s performance can be further increased by tuning the parameters 
in depression detection based on the patient’s data model. The model can also be 
implemented using deep learning and modern machine learning algorithms. 
References 
1. Verde L et al (2021) A lightweight machine learning approach to detect depression from 
speech analysis. In: 33rd IEEE international conference on tools with artiﬁcial intelligence, pp 
330–335. Washington, DC, USA 
2. Victor DB, Raza DM (2022) Depression detection from Facebook using machine learning 
techniques. In: 6th IEEE international conference on trends in electronics and informatics. pp 
1447–1454. Tirunelveli, India 
3. Narayanrao PV, Lalitha P, Kumari S (2020) Analysis of machine learning algorithms for 
predicting depression. In: international conference on computer science, engineering and 
applications, pp 1–4. Gunupur, India 
4. Deshpande M, Rao V (2021) Depression detection using emotion artiﬁcial intelligence. In 
IEEE international conference on intelligent sustainable systems, pp 1–6. Palladam, India 
5. Orabi AH, Buddhitha P, Orabi MH, Inkpen D (2018) Deep learning for depression detection 
of Twitter user. In: Proceedings of the ﬁfth workshop on computational linguistics and clinical 
psychology, pp 88–97. New Orleans, LA 
6. Joshi D, Patwardhan M (2020) An analysis of mental health of social media users using 
unsupervised approach. In: Elsevier Ltd (2020) 
7. Sharma S, Sharma S (2020) Analyzing the depression and suicidal tendencies of people affected 
by COVID-19’s lockdown using sentiment analysis on social networking websites. J Stat Manag 
Syst 24(1):115–133. https://doi.org/10.1080/09720510.2020.1833453 
8. Wongkoblap A, Vadillo MA (2021) Deep learning with anaphora resolution for the detection 
of tweeters with depression: algorithm development and validation study. JMIR Ment Health 
8:e19824 
9. Wang X, Li H, Sun C (2021) Prediction of mental health in medical workers during COVID-19 
based on machine learning. Front Public Health 9:697850 
10. Ghosh T, Al Banna MH (2021) A hybrid deep learning model to predict the impact of COVID-19 
on mental health from social media big data 
11. Gupta D, Castillo O, Kumar A (2021) Machine learning for psychological disorder prediction 
in Indians during COVID-19 nationwide lockdown. Intell Decis Technol 15(1):161–172 
12. Keya MS, Han A (2022) A performance analysis of depression ratio using machine learning 
approaches. In: IEEE second international conference on artiﬁcial intelligence and smart 
energy, pp 215–219 
13. Bagga N, Vashistha P, Yadav P (2021) Predicting depression from social networking data using 
machine learning techniques. In: 3rd IEEE international conference on advances in computing, 
communication control and networking, pp 849–854 
14. Ahmed A et al (2020) A machine learning approach to detect depression and anxiety 
using supervised learning. In: IEEE asia-paciﬁc conference on computer science and data 
engineering, pp 1–6 
15. Yadav S, Kaim T, Gupta S, Bharti U, Priyadarshi P (2020) Predicting depression from routine 
survey data using machine learning. In: 2nd IEEE international conference on advances in 
computing, communication control and networking, pp 163–168

SoundMind: A Machine Learning and Web-Based Application …
97
16. Solieman H, Pustozerov EA (201) The detection of depression using multimodal models based 
on text and voice quality features. In: IEEE Conference of Russian young researchers in 
electrical and electronic engineering, pp 1843–1848 
17. Uddin MZ, Dysthe KK, Følstad A et al (2022) Deep learning for prediction of depressive 
symptoms in a large textual dataset. Neural Comput Appl 34(1):721–744 
18. Babu NV, Kanaga EGM (2021) Sentiment Analysis in social media data for depression detection 
using artiﬁcial intelligence: a review. In Springer, Cham 
19. Zogan H, Razzak I, Wang X et al (2022) Explainable depression detection with multi-aspect 
features using a hybrid deep learning model on social media. In: Springer, Cham, pp 281–304 
20. Kour H, Gupta MK (2022) A hybrid deep learning approach for depression prediction from user 
tweets using feature-rich CNN and bi-directional LSTM. Multimedia Tools Appl 81:23649– 
23685 
21. Wang X, Zhang C, Ji Y, Sun, L, Wu L, Bao Z (2023) A depression detection model based 
on sentiment analysis in micro-blog social network. In: Springer - Trends and Applications in 
Knowledge Discovery and Data Mining, vol 7867. pp 201–213 
22. Richter T, Fishbain B, Markus A et al (2020) Using machine learning-based analysis for 
behavioral differentiation between anxiety and depression

Japanese Encephalitis Symptom 
Prediction Using Machine Learning 
Algorithm 
Piyush Ranjan, Sushruta Mishra, Tridiv Swain, and Kshira Sagar Sahoo 
Abstract In India Japanese Encephalitis (JEV) has been a major public health 
problem. In endemic districts of country each year there is a large-scale outbreak 
occurring of JEV. Research says that Japanese Encephalitis is a ﬂavivirus related 
to West Nile Virus, Yellow Fever and Dengue and it is escalated by mosquitoes. 
Japanese Encephalitis is although rare, but the fatality rate is around 30%. Till now 
there is no cure for JEV, the entire treatment is focused for supporting the patient 
to overcome disease and relieving severe clinical sign. Maximum number of JEV 
cases in India are of infants and the fatality rate is around 30% which is a great 
matter of concern. Here Force of Infection denotes the rate at which sensitive indi-
viduals acquire an infectious disease. In India, states which report major outbreak of 
Japanese Encephalitis are Uttar pradesh, Andhra Pradesh, West Bengal, Karnataka, 
Assam, Tamil Nadu, Bihar, Goa and Manipur. The impacting factors include Climate, 
Rice Distribution, Livestock Distribution, Population Density, Speciﬁc Age Group 
Density, Urban/Rural Category and Elevation. Impacting Factors may change with 
the location. Here we have used Machine learning algorithms like Ridge Regression, 
Lasso Regression, ElasticNet Regression and Multi-layer Perceptron for the predic-
tion of Force of Infection of Japanese Encephalitis Virus. ElasticNet Regression 
Algorithm is also used for extracting the signiﬁcant attribute from the JEV Dataset. 
The proposed model generated an optimum performance in context to the error rate 
and accuracy of prediction. 
Keywords Japanese Encephalitis · Encephalitis · JEV FOI · Machine Learning ·
Prediction
P. Ranjan envelope symbol · S. Mishra · T. Swain 
Kalinga Institute of Industrial Technology, Deemed to Be University, Bhubaneswar, India 
e-mail: piyushdsranjan@gmail.com 
S. Mishra 
e-mail: sushruta.mishrafcs@kiit.ac.in 
K. S. Sahoo 
Department of Computing Science, Umea University, Umea, Sweden 
e-mail: kshirasagar.s@srmap.edu.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_10 
99

100
P. Ranjan et al.
1 
Introduction 
In India Japanese Encephalitis has been a major public health problem. In endemic 
districts of country each year there is a large scale outbreak occurring of JEV. 
Research says that Japanese Encephalitis is a ﬂavivirus related to West Nile Virus, 
Yellow Fever and Dengue and it is escalated by mosquitoes. Vomiting and gastroin-
testinal pain in children may be the initial leading symptoms. But in case of serious 
infection symptoms can be very High Fever, Neck Stiffness, Headache, Coma, Disori-
entation, Spastic Paralysis, Seizures and Stroke of Death [1]. The Survivors of the 
JE infection may suffer from Permanent Behavioral, Intellectual or Neurological 
Sequelae such as inability to speak or recurrent seizures and paralysis. Japanese 
Encephalitis is although rare, but the fatality rate is around 30%. There is a risk of 
infection to more than 3 billion people of 24 countries in South-East Asia and Western 
Paciﬁc Regions. Till now there is no cure for JEV, the entire treatment is focused 
for supporting the patient to overcome disease and relieving severe clinical sign. For 
prevention of JE effective and safe vaccines are available [2]. Vaccination of JE can 
be integrated to the national immunization scheme in all 24 country of South-East 
Asia and Western Paciﬁc Region. Maximum number of JEV cases in India are of 
infants and the fatality rate is around 30% which is a great matter of concern. Here 
Force of Infection denotes the rate at which sensitive individuals acquire an infec-
tious disease. In India states which report major outbreak of Japanese Encephalitis 
are Uttar Pradesh, Andhra Pradesh, West Bengal, Karnataka, Assam, Tamil Nadu, 
Bihar, Goa and Manipur[3]. There are different factors which are going to impact 
the FOI of JEV. The Factors include Climate, Rice Distribution, Livestock Distri-
bution, Population Density, Speciﬁc Age Group Density, Urban/Rural Category and 
Elevation. Impacting Factors may change with the location. 
Climate 
The climate provides many information about a location such as precipitation or 
wind speed, temperature. 
Rice Distribution 
This Variable describes about physical areas from which Yield & Production and 
Harvest area can be calculated. A mosquito called Culex Tritaeniorhynchus which 
is responsible for carrying JE, It breeds in Rice Fields so physical area of Rice 
cultivation is taken into account. 
Livestock Distribution 
It tells us about the distribution of Pigs as it plays a very critical role in the enzootic 
cycle of Japanese Encephalitis, so the distribution of Pigs needs to be considered. 
Urban / Rural Category 
It is generally seen that in rural areas the transmission of Japanese Encephalitis are 
higher with respect to urban areas. In rural areas, more availability of livestock are 
seen, here specially pigs are considered.

Japanese Encephalitis Symptom Prediction Using Machine Learning …
101
Elevation 
In the transmission of Japanese Encephalitis Elevation also plays a role. It is found 
that there exist a Negative Co-relation between presence of JE vectors and Elevation. 
Research says that almost no cases of Japanese Encephalitis were found above 1200 m 
elevation [4]. 
The main contributions of the work are as follows:
. The study deals with assessment of Japanese Encephalitis Virus using machine 
learning approach.
. WEKA software is used to preprocess the dataset and regression methods are 
applied to extract the signiﬁcant attributes.
. Multi-layer perceptron algorithm is subsequently used to predict the force of 
infection of Japanese Encephalitis Virus in patients.
. Performance evaluation of the proposed model suggests that the error rate is 
minimized while making it more reliable. 
The paper is organized as follows. Section 1 introduces the emergency of diag-
nosing Japanese Encephalitis and its symptoms. The use of machine learning in 
assessing its risks is also discussed. Section 2 highlights some vital background 
study and existing works done on the domain. Section 3 presnets a novel Japanese 
Encephalitis prediction framework using both regression and multilayer perceptron 
methods. Section 4 discusses the implementation results obtained for evaluation of 
the proposed model. Finally, Sect. 5 concludes the analysis and provides a brief 
understanding of the work. 
2 
Related Work 
In this paper the researchers have used a mathematical model with age-stratiﬁed data 
to overcome the lack of JE patient local burden data and data of the impact of past 
vaccination. Here the researchers estimate in 2015 JE cases which is around 100,308 
and the number of deaths are 25,125 and that between year 2000 and 2015 total of 
30,774 JE cases are avoided due to vaccination. Here the results of research work 
spotlight areas which can have most gain from starting the vaccination programs, two 
stages of analysis is used here in the research work, In ﬁrst stage a systematic review 
is conducted to gather data which is age-stratiﬁed and to gather the vaccination data. 
Then a model is ﬁtted to this obtained data to estimate force of infection or trans-
mission intensity. In the second stage the researchers have extrapolated the force of 
infection for all the areas which are endemic from their prior estimates. Researchers 
have used vaccination data and processed population in all endemic areas. Then they 
have used a model to generate Burden Cases (Quantities) in two scenarios, ﬁrst is with 
Vaccination of JE and other is without JE Vaccination. Researchers have estimated 
a very high FOI in Indonesia and India, Hannah E. Clapham et al. [5]. In this review 
paper the researchers have found that climate change can play a very critical role in 
transmission of diseases. Diseases like cholera, dengue giardiasis and salmonellosis

102
P. Ranjan et al.
grow in high temperature. It is also found that heavy rainfall may increase the spread 
of disease through animals. It is also found that for identiﬁcation of dengue outbreak 
the Gaussian Process Regression framework model perform better with respect to 
other ML Algorithms like Decision Tree, Multiple Regression, Supervised Learning 
algorithms. Sum of Squared Error, R-Squared, Q-Squared,Mean Square Error, Root 
Mean Square Error are found to be low with respect to ML Algorithm. Constraint 
with Gaussian Process Regression is that it can only be applicable for big data sets, 
K. Sathesh Kumar et al. [6]. In this paper the researchers have done the study on the 
Indian perspective of Japanese Encephalitis Virus - JEV. In India there have been 
total 103,338 reported cases of JEV till 2007 and the number of death because of 
JEV is 33,729 and approximately around 59,754,200 people in India live in regions 
which are JE endemic regions and the cases reported per year is around 1500 to 4000. 
All the above ﬁgure of cases are based on reported JEV cases. There can be a number 
of JEV cases which are not reported. In India states like Assam, Bihar, Goa, Delhi, 
Karnataka, Uttarpradesh and Hariyana shows maximum number of JEV cases, while 
the number of JEV cases in states like Manipur, Punjab, Nagaland and Uttrakhand are 
negligible, Tapan N. Dhole et al. [7]. Here the researchers have done Bibliometrics 
Analysis for the Global Research Work on JEV from 1934 to 2020. A research has 
been continuously taken place in countries like Japan, India, China and South Korea, 
and it shows that the JEV is an epidemic in Asian Countries. During the biblio-
metric analysis the researchers have found a total of 3023 publication from 1934 
to 2020. Maximum publications were found from India which is 704 publications, 
Japan a total of 543 publications, China a total of 676 publications and USA a total 
of 697 publications, Kai Nie & Hunanyu Wang et al. [8]. In this paper the Authors 
have used Bayesian Network for the prediction of Japanese Encephalitis Vector in 
Andhra Pradesh Kurnool District. Here the required data was collected from six areas 
of Kurnool District which consist of 5 rural areas and 1 urban area. The areas are 
Nandanapalli, Peddathumbalam, Gudur, Nandikotkur, Cherukulapadu and the data 
which was collected is from 2001 to 2006. The dataset consist of data like agriculture, 
entomological and meterological data. JABNET carries its JE mosquito population 
prediction in three phases. In the result analysis of JABNET the prediction accuracy 
of Culex tritaniorhynchus is 73.17%, Upadhyayula Suryanarayana Murty et al. [9]. 
In this Paper the researchers have used Maxnet program to model the spread of Cx. 
Tritaeniorhynchus in Republic of Korea. Here they have used data like temperature, 
precipitation, land cover, elevation, mosquito collection data and SPOT normalized 
difference vegetation index -NDVI. The model were created for period 5 years for 
each month. Here the Maxnet model generates output maps which replicate the 
highest occurrence probability of mosquito in the month of August and September. 
The Maxnet model shows less probability of Cx tritaeniorhynchus occurrence in 
mountains covered with forest and the elevation is above 1000 m. Maximum Entropy 
Method or Maxnet takes a text ﬁle of location of known species and set of grided envi-
ronmental layers and produces the map which represent the species potential distri-
bution, Penny Masuoka et al. [10]. In this research paper the author have performed 
Systematic Literature Review -SLR on deep learning and machine learning tech-
nique to support the Arboviral disease clinical diagnosis. The deep learning and

Japanese Encephalitis Symptom Prediction Using Machine Learning …
103
machine learning techniques which is used here are Decision Tree, Random Forest, 
AdaBoost and Gradient, Support Vector Machine, Logistic Regression, Naive Bayes, 
K-Nearest Neighbour, Convolution Neural Network, Patrika Takako Endo et al. [11]. 
In this paper the author have done a case study on spatial Delimation, control and 
forecasting of JEV. There are different risk factors which are associated with the 
Japanese Encephalitis Virus transmission. The risk factor include wet land crops -
rice to dry land crops, animal husbandry- pig breeding and duck farming, presence 
of bird sanctuaries or parks in the area, rainfall, temperature and climate, Shanmu-
gavelu Sabesan et al. [12]. In this research paper the authors have used ecological 
niche modeling to estimate the Japanese Encephalitis Virus distribution in Asia. 
Here Maxnet program is used to map the regions with the favourable environment 
conditions for growth of Cx. Tritaeniorhynchus vector. Here the model inputs are 
rainfall, elevation, temperature and mosquito map which tells us about the known 
location of vector presence. To construct the ecological niche model a total of 139 
unique sited of documented Cx. Tritaeniorhynchus geographical regions were used. 
Using different combination of environmental layer, the model was run four times. 
Here the statistical results indicates that bioclimatic layer and elevation is included in 
most accurate model, Robin H. Miller et al. [13]. In this paper the authors have esti-
mated the FOI and Corresponding reproduction number for Dengue Transmission in 
Multiple countries. The result analysis shows that the estimates across and within the 
country varied widely which highlights the spatio temporally heterogeneous nature 
of the dengue transmission, Natsuko Lmai et al. [14]. 
3 
Proposed Work 
For the Force of Infection prediction of Japanese Encephalitis Virus Our data is taken 
from Github. Data is generated by Duy M. Nguyun and Quan M. Trans. Earlier 
Data was in Rds. format. We have used R-Studio software for converting the .rds 
data to .csv data. Data Preprocessing of our Japanese Encephalitis virus dataset is 
done using weka, where we have performed standardization and normalization of 
Dataset. Our Dataset consist of 27 attributes and All the attributes are of type numeric. 
There are no missing value found in Dataset. Dataset consist of 701,307 Rows. Out 
of 27 attributes 19 are bio climatic variables which are derived from rainfall and 
temperature, and others are Elevation, Pigs, Rice Distribution, Urban/Rural Category, 
Adjusted Population Count of 2015, Vector Distribution, all these attributed are Inde-
pendent Attribute and our Dependent Attribute is FOI- Force of Infection. FOI range 
is from 0 to 0.5, any value of FOI which is equal and greater to 0.25 is to be considered. 
Data is collected from endemic countries like Australia, Bhutan, Bangladesh, India, 
Burma, Brunei, China, Combodia, Japan, Indonesia, Malaysia, Laos, Nepal, North 
Korea, Papua New Guinea, Pakistan, Russia, Philippines, South Korea, Singapore, 
Srilanka, Thailand, Taiwan and Vietnam. We have used WEKA software for Data 
Preprocessing. The preprocessed data samples are subjected to different regression 
techniques to extract signiﬁcant attributes. The regression methods include Ridge,

104
P. Ranjan et al.
Fig. 1 Proposed model for Japanese Encephalitis Virus assessment 
Lasso and Elastic Net Regression. Later Multilayer Perceptron Algorithm and R-
Studio software are used for the prediction of FOI. The cleaned and feature reduced 
dataset is used with Multilayer Perceptron Algorithm for the prediction of force of 
Infection of Japanese Encephalitis. The performance is validated using metrics like 
RMSE values. Figure 1 illustrates the proposed methodology workﬂow. 
4 
Regression Methods Used in Study 
Popular regression methods like Ridge, Lasso and Elastic Net Regression are applied 
in the proposed model for the extraction of relevant attributes froom the dataset. 
Ridge Regression 
It is a machine learning algorithm which is used for model tuning. It analyses the 
data set for multicollinearity [15]. If there exist multicollinearity in dataset then then 
there will be a large difference between predicted value and actual value of target 
variable. Through Ridge Regression we can shrink the parameter and it reduces the 
complexity of the model by shrinking the coefﬁcients. Through this algorithm bias 
and variance made proportional to each other which reduces the difference between 
predicted and actual value. This method perform L2 regularization. 
Lasso Regression 
Lasso Regression is a machine learning algorithm which comes under linear regres-
sion and it uses a technique called shrinkage which leads to data points that are 
shrunk towards mean or central point. Through this algorithm we can obtain a model

Japanese Encephalitis Symptom Prediction Using Machine Learning …
105
with fewer parameter i.e. simple or sparse models. This algorithm is best suited when 
we want to automate variable selection or parameter elimination or model selection. 
This algorithm performs regularization. It adds penalty which is equal to absolute 
value of coefﬁcients magnitude. 
Elastic Net Regression 
This algorithm is combination of both Ridge and Lasso Regression. It uses both 
the penalties from ridge and lasso regression technique for regularizing the model 
[16]. This technique improves the limitation of lasso that is in lasso regression if 
there exist a highly correlated group, it tends to choose only one variable from the 
correlated group and entirely ignore the rest. But in elastic net regression technique 
there is inclusion of n-number of variable until saturation. In elastic net regression 
regularization and variable selection are performed simultaneously [17]. 
Multilayer Perceptron 
Multilayer Perceptron is a kind of feed forward neural network. There are three layers 
present in multilayer perceptron and the layers are Input Layer- which takes input 
signal which is going to be processed. Second layer is Hidden Layer- which acts 
as the computational engine for the Neural Network. And the third layer is Output 
Layer which produces output. Output layer can perform tasks like prediction and 
classiﬁcation based on the requirement [18]. The ﬂow of data in MLP is in forward 
direction i.e. from input layer to output layer and the training of neurons are based 
back propagation learning algorithm. Multilayer Perceptron Algorithm can be used 
for Recognition, Classiﬁcation, Approximation and Prediction. 
5 
Result and Analysis 
This section discusses the results obtained form implemneting the proposed model 
using a combination of regression m,ethods and multilayer perceptron. RMSE values 
of these methdos are compared for analysis. 
A. Implementation of Ridge Regression 
In R-STUDIO to implement Ridge Regression we have to use caret and glmnet 
library. The caret library consist of set of functions which are used to well order 
the predictive model generation process. Caret library have tools for visualizing, 
Preprocessing and data splitting [19]. In glmnet package the ﬁrst half i.e. glm stands 
for Generalized Linear model and can be applied to logistic regression and linear 
regression and few other models as well. The second half i.e. net is associated with 
Elastic-Net. By using glmnet package instead of using two different λ - lambda we 
can use single λ. Here  λ is used for penalty adjustment i.e. coefﬁcient shrinkage. λ 
is the strength of penalty on the coefﬁcients. Another parameter is also used called α
- Alpha, it can be any value from 0 to 1. When the value α is 0 then whole penalty of 
lasso goes to 0, and goes away and we left with just the penalty of Ridge Regression.

106
P. Ranjan et al.
Here for Ridge Regression we have divided the data into 7:3 ratio i.e. 70% training 
and 30% test set. TrainControl function which is in caret package is used to create 
custom control parameter, here 10 Fold Cross Validation is used which is repeated 
5 times. Plot () is used for building different plots like variable importance graph, 
Regularization Vs RMSE graph. 
The above Fig. 2 shows the Regularization Vs RMSE plot which highlights that 
for higher values of λ the RMSE error increases. On y-axis we have RMSE and 
on x-axis we have Regularization Parameter. For calculating RMSE, repeated cross 
validation is used. We have achieved 0.070 RMSE for λ = 0.0001000. 
Figure 3 depicts the variable Importance plot shows the importance of Independent 
Variable in predicting the FOI in our Ridge Regression Model. Here we can see that 
Pig is the most important independent variable in our Data Set followed by Bio_ 
13, Bio_02, Bio_18, Adjusted_pop_Count_2015, Bio_16, Bio_14, Bio_12, Bio_03, 
Bio_09, Elv etc. And the least useful or least signiﬁcant variables are Bio_11, Bio_ 
06 and Bio_07. 
Fig. 2 Regularization parameter(λ) Vs RMSE analysis 
Fig. 3 Variable Importance analysis

Japanese Encephalitis Symptom Prediction Using Machine Learning …
107
B. Implementation of Lasso Regression 
In R-STUDIO to implement Lasso Regression we have to use caret and glmnet 
library. The caret library consist of set of functions which are used to well order 
the predictive model generation process. Caret library have tools for visualizing, 
Preprocessing and data splitting. In glmnet package the ﬁrst half i.e. glm stands 
for Generalized Linear model and can be applied to logistic regression and linear 
regression and few other models as well. The second half i.e. net is associated with 
Elastic-Net. By using glmnet package instead of using two different λ - lambda we 
can use single λ. Here  λ is used for penalty adjustment i.e. coefﬁcient shrinkage. λ 
is the strength of penalty on the coefﬁcients. Another parameter is also used called α
- Alpha, it can be any value from 0 to 1. When the value α is 1 then whole penalty of 
Ridge goes to 0, and goes away and we left with just the penalty of Lasso Regression. 
Here for Lasso Regression we have divided the data into 7:3 ratio i.e. 70% training 
and 30% test set. TrainControl function which is in caret package is used to create 
custom control parameter, here 10 Fold Cross Validation is used which is repeated 
5 times. Plot () is used for building different plots like variable importance graph, 
Regularization Vs RMSE graph. 
Figure 4 shows the Regularization Vs RMSE plot which presents for higher values 
of λ the RMSE error increases. On y-axis we have RMSE and on x-axis we have 
Regularization Parameter. For calculating RMSE, repeated cross validation is used. 
We have achieved 0.068 RMSE for λ = 0.0001000. 
The above Fig. 5 depicts the variable Importance plot showing the importance of 
Independent Variable in predicting the FOI in our Ridge Regression Model. Here we 
can see that Pigs is the most important independent variable in our Data Set followed 
by Bio_13, Bio_10, Bio_12, Bio_05, Bio_14, Bio_17, Bio_02, Bio_03, Bio_16, Bio_ 
09, Adjusted_pop_Count_2015 etc. And the least useful or least signiﬁcant variables 
are Bio_11, Bio_06 and Bio_07, Bio_15, DG_000_014bt_dens etc.
Fig. 4 Regularization parameter(λ) Vs RMSE Graph 

108
P. Ranjan et al.
Fig. 5 Variable Importance Plot (pre-variable reduction) 
C. Implementation of ElasticNet Regression 
In R-STUDIO to implement ElasticNet Regression we have to use caret and glmnet 
library. The caret library consist of set of functions which are used to well order 
the predictive model generation process. Caret library have tools for visualizing, 
Preprocessing and data splitting. In glmnet package the ﬁrst half i.e. glm stands 
for Generalized Linear model and can be applied to logistic regression and linear 
regression and few other models as well. The second half i.e. net is associated with 
Elastic-Net. By using glmnet package instead of using two different λ - lambda we 
can use single λ. Here  λ is used for penalty adjustment i.e. coefﬁcient shrinkage. λ 
is the strength of penalty on the coefﬁcients. Another parameter is also used called α
- Alpha, it can be any value from 0 to 1. The value of α is between 0 and 1 so that we 
can have combination of the two penalties. And this mixture of two penalties lead 
to a better correlated variable shrinkage than either Ridge and Lasso does on their 
own. Here for ElasticNet Regression we have divided the data into 7:3 ratio i.e. 70% 
training and 30% test set. TrainControl ﬁnction which is in caret package is used 
to create custom control parameter, here 10 Fold Cross Validation is used which is 
repeated 5 times. Plot () is used for building different plots like variable importance 
graph, Regularization Vs RMSE graph. 
In Fig. 6, X-axis represent the mixing of α between 0 and 1. The different colour 
lines on the plot are for Regularization Parameter. Here the orange line shows 
maximum RMSE for Regularization parameter 1 and Blue line shows minimum 
RMSE for Regularization Parameter 0.0001. Here we have achieved RMSE = 0.065 
for α = 0.8888 and λ = 0.0001.
Figure 7 highlights the variable Importance plot showing the importance of Inde-
pendent Variable in predicting the FOI in our Ridge Regression Model. Here we can 
see that Pigs is the most important independent variable in our Data Set followed by 
Bio_12, Bio_10, Bio_14, Bio_13, Bio_17, Bio_05, Bio_01, Bio_16, Bio_03, Bio_

Japanese Encephalitis Symptom Prediction Using Machine Learning …
109
Fig. 6 Mixing Percentage Vs RMSE Plot
02, Bio_11, Adjusted_pop_Count_2015 etc. And the least useful or least signiﬁcant 
variables are Bio_15, Bio_06 and Bio_07, DG_000_014bt_dens. 
D. Implementation of Multilayer Perceptron Using WEKA 
To implement Multilayer Perceptron in WEKA software tool we have to use Multi-
layer Perceptron algorithm which is present inside function folder of classiﬁer [20, 
21]. Here we have used a new dataset which we have obtained after ElasticNet 
Regression. New dataset consist of total 16 attributes in which 15 are independent 
and one is dependent. Here tenfold cross validation is used for the model. The number 
of Epoch used is 500. We have used two hidden layer, the ﬁrst hidden layer consist 
of 4 nodes and the second hidden layer consist of 6 nodes and the learning rate is 0.3.
Fig. 7 Variable Importance Plot (post-variable reduction) 

110
P. Ranjan et al.
Fig. 8 Neural Network 
Here we have achieved RMSE = 0.057 which is best among all the other machine 
learning algorithm which were used by us. 
The above Fig. 8 presents the Multilayer Perceptron model. Our Multilayer 
Perceptron model consist of 4 layers. One input layer, two hidden layers and one 
output layer. The ﬁrst hidden layer consist of 4 nodes and the hidden layer consist of 
6 nodes. Here we can see the 15 independent variables in green i.e. our Input layer 
and The dependent variable is FOI represented in yellow i.e. our output layer. 
E. Rmse - Root Mean Square Error 
RMSE represent the residual error which is the difference between the actual value 
of Dependent Variable and the Predicted value of Dependent variable. It measures 
the Residual spread around the best ﬁt line. Lower RMSE stipulate intense data point 
throughout the best ﬁt line. Lower the RMSE replicate better model. 
In Table 1, we can see that the Multilayer Perceptron algorithm shows less RMSE 
than other three algorithms which we have used for the prediction of FOI. So, for our 
FOI prediction model Multilayer Perceptron perform best with only 0.057 RMSE. 
Table 1 Evaluation Table for 
RMSE analysis
S.no
Algorithm
RMSE 
1 
2 
3 
4 
Ridge Regression 
Lasso Regression 
ElasticNet Regression 
Multilayer Perceptron 
0.070 
0.068 
0.065 
0.057

Japanese Encephalitis Symptom Prediction Using Machine Learning …
111
6 
Conclusion 
Our main goal s the prediction of FOI of Japanese Encephalitis Virus. FOI predic-
tion is done by using different machine learning algorithms like Ridge Regression, 
Lasso Regression, ElasticNet Regression and Multilayer Perceptron, where Multi-
layer Perceptron shows less RMSE with respect to Ridge Regression, Lasso Regres-
sion and ElasticNet Regression. Prediction of Force of Infection can restrict the 
spread of Japanese Encephalitis Virus. Force of Infection prediction allows us to take 
preventive measures prior the spread of Japanese Encephalitis Virus. We have also 
found the important independent variable in our dataset that are going to affect the 
spread of Japanese Encephalitis Virus. There can be other factors which is speciﬁc 
to geographical location which affect the spread of Japanese Encephalitis Virus. 
New software technology and software tools can always help to predict the Force Of 
Infection of Japanese Encephalitis Virus in future. 
References 
1. Sahoo S, Das M, Mishra S, Suman S (2021) A hybrid DTNB model for heart disorders 
prediction. In: Advances in electronics, communication and computing. Springer, Singapore, 
pp. 155–163 
2. Maeki T et al (2019) Analysis of cross-reactivity between ﬂaviviruses with sera of patients 
with Japanese encephalitis showed the importance of neutralization tests for the diagnosis of 
Japanese encephalitis. J. Infect. Chemother. 25:786–790 
3. Erlanger TE, Weiss S, Keiser J, Utzinger J, Wiedenmayer K (2009) Past, present, and future 
of Japanese encephalitis. Emerg Infect Dis 15:1–7 
4. Jena L, Mishra S, Nayak S, Ranjan P, Mishra MK (2021) Variable optimization in cervical 
cancer data using particle swarm optimization. In: Mallick PK, Bhoi AK, Chae G-S, Kalita 
K (eds) Advances in Electronics, Communication and Computing ETAEERE 2020. Springer 
Nature, Singapore, pp 147–153. https://doi.org/10.1007/978-981-15-8752-8_15 
5. Quan TM, Thao TTN, Ouy NM, Nhat TM, Clapham HE (2019) Estimates of the global burden 
of Japanese Encephalitis and the impact of vaccination from 2000–2015, 25 September 2019 
6. Indhumathi K, Sathesh Kumar K (2020) A review on prediction of seasonal diseases based on 
climate change using data. In: Materials Today: Proceedings, Elsevier, 18 August 2020 
7. Tiwari S, Singh RK, Tiwari R, Dhole TN (2012) Japanese encephalitis: a review of the Indian 
perspective. Braz J Infect Dis 16(6):564–573. https://doi.org/10.1016/j.bjid.2012.10.004 
8. Xu CX (2022) A bibliometric analysis of global research on Japanese encephalitis from 1934 
to 2020. Front Cellul Infect Microbiol 12:833701. https://doi.org/10.3389/Fcimb.2022.833701 
9. Murty US, Rao MS, Arunachalam N (2009) Prediction of Japanese encephalitis vectors in 
Kurnool district of Andhra Pradesh, India by using Bayesian Network. Appl Artif Intell 
23(9):828–834. https://doi.org/10.1080/08839510903235362 
10. Masuoka P (2009) Modeling and analysis of mosquito and environment data to predict the risk 
of Japanese encephalitis. In: ASPRS 2009 Annual Conference, Baltimore, Maryland, March 
9–13, 2009 
11. Rogério S et al (2022) Machine learning and deep learning techniques to support clinical 
diagnosis of arboviral diseases: a systematic review. PLOS Negl Trop Dis 16(1):e0010061. 
https://doi.org/10.1371/journal.pntd.0010061 
12. Sabesan S, Kanuganti HKR, Perumal V (2008) Spatial Delimation forecasting and control of 
Japanese Encephalitis: India - a case study. Open Parasitol J 2:59–63

112
P. Ranjan et al.
13. Miller RH, Masuoka P, Klein TA, Kim H-C, Somer T, Grieco J (2012) Ecological niche 
modeling to estimate the distribution of Japanese encephalitis virus in Asia. PLoS Negl Trop 
Dis 6(6):e1678. https://doi.org/10.1371/journal.pntd.0001678 
14. Imai N, Dorigatti I, Cauchenmez S, Ferguson NM (2016) Estimating dengue transmis-
sion intensity from case - notiﬁcation data from multiple countries. PLOS Negl. Trop. Dis. 
10:e0004833 
15. Esser HJ et al (2019) Risk factors associated with sustained circulation of six zoonotic arbo-
vivruses: a systematic review for selection of Surveillance sites in non - endemic areas. BMC. 
12:1–7 
16. Jena L, Kamila NK, Mishra S (2014) Privacy preserving distributed data mining with evolu-
tionary computing. In: Proceedings of the International Conference on Frontiers of Intelligent 
Computing: Theory and Applications (FICTA) 2013. Springer, Cham, pp. 259–267 
17. Dutta A, Misra C, Barik RK, Mishra S (2021) Enhancing mist assisted cloud computing toward 
secure and scalable architecture for smart healthcare. In: Hura GS, Singh AK, Hoe LS (eds) 
Advances in Communication and Computational Technology: Select Proceedings of ICACCT 
2019. Springer Nature Singapore, Singapore, pp 1515–1526. https://doi.org/10.1007/978-981-
15-5341-7_116 
18. Rath M, Mishra S (2020) Security approaches in machine learning for satellite communication. 
In: Machine Learning and Data Mining in Aerospace Technology. Springer, Cham, pp. 189–204 
19. Mishra S, Jena L, Tripathy HK, Gaber T (2022) Prioritized and predictive intelligence of 
things enabled waste management model in smart and sustainable environment. PLoS ONE 
17(8):e0272383 
20. Tripathy HK, Mishra S, Suman S, Nayyar A, Sahoo KS (2022) Smart COVID-shield: an IoT 
driven reliable and automated prototype model for COVID-19 symptoms tracking. Computing 
104(6):1233–1254. https://doi.org/10.1007/s00607-021-01039-0 
21. Suman S, Mishra S, Sahoo KS, Nayyar A (2022) Vision navigator: a smart and intelligent 
obstacle recognition model for visually impaired users. Mob Inf Syst 2022:1–15. https://doi. 
org/10.1155/2022/9715891

Smart Skin-Proto: A Mobile Skin 
Disorders Recognizer Model 
Sushruta Mishra, Shubham Suman, Aritra Nandi, Smaraki Bhaktisudha, 
and Kshira Sagar Sahoo 
Abstract With the advancement and rapid development of the internet, the most 
convenient strategies for patients are mainly provided with digital healthcare systems 
that mainly includes the use of mobile health technology which is quite efﬁcient. 
Moreover, this ﬁeld is slightly shifting and also indicating interest towards the smart 
and intelligent models as there are quite a lot of beneﬁts associated with it like 
cost decrement, easy to understand and also including the personal satisfaction of 
patients. The latest application of m-health medical treatment is now still on the 
process of the investigation because still users are facing challenges in the clinical 
environment. This m-health approach can be applied to accurately determine skin 
cancer symptoms in patients. In this paper, an impact of m-healthcare on disease 
diagnosis is demonstrated. A new m-health module for skin cancer diagnosis called 
‘Smart Skin-Proto’ is developed. Then its usage in skin cancer assessment is also 
highlighted and upon implementation, the model records optimal performance which 
records an accuracy of 96.2% with 15 decision trees count. Also the overall latency 
of this application is less than other existing mobile apps. 
Keywords Internet of Things (IoT) · Machine Learning (ML) · Skin cancer ·
m-health · Personalized health · Decision tree
S. Mishra envelope symbol · A. Nandi · S. Bhaktisudha 
Kalinga Institute of Industrial Technology, Deemed to Be University, Pune, India 
e-mail: sushruta.mishrafcs@kiit.ac.in 
S. Suman 
Birla Global University, Bhubaneswar, India 
K. S. Sahoo 
Department of Computing Science, Umea University, Umea, Sweden 
e-mail: kshirasagar.s@srmap.edu.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_11 
113

114
S. Mishra et al.
1 
Introduction 
Cancer begins when there is an abnormal changing or out of control growth of the 
healthy cells lead to the formation of a mass known as tumor (cancerous). A cancerous 
tumor also leads to the spreading of other parts of the body which can be dangerous 
[1]. A benign tumor is known as a tumor that may develop yet may not rapidly spread 
over. Medical experts diagnosis leads to the conclusion that skin cancer is rapidly 
rising on a larger scale. If it is founded at its early stage then it can be treated with 
proper medical care including topical medications. Skin cancer is merely accountable 
for below 1% of death related to cancers [2]. But still in few scanarios, this can lead 
to a more serious case which needs advanced medical solutions thereby including 
surgeons and other skin specialist [3] (Fig. 1). 
Cancers are mainly divided into four types which include:
. Basal cell carcinoma: These are mainly circular shaped cells that are mainly 
available in lower tissue and is a very generic kind of skin cancer found (about 
80%). The root cause of it’s development is because of this kind of cells.
. Squamous cell carcinoma: Remaining 20% of skin cancers are mainly caused due 
to these cells mainly known as squamous cell carcinomas. They are commonly 
seen over lips, at places of visible scars and outside the mouth skin, anus, or 
woman’s vagina.
. Merkel cell cancer: Merkel cell cancer begins at hormone generating tissues 
below skin as well as in hair follicles and are highly aggressive, fast-growing and 
also rare cancer. It is commonly available near neck or head site [4].
. Melanoma: These cancer are formed mainly on the scattered cells known as 
melanocytes in the meeting region of epidermis and dermis which is a very serious 
form of cancer.
Fig. 1 Illustration of Types of skin cancer [3] 

Smart Skin-Proto: A Mobile Skin Disorders Recognizer Model
115
Usage of smart phone devices for health care facilities has transformed the total 
over view of the medical practices [5]. They provide many beneﬁts like increase 
in the access point-of-care tools which help in the betterment of medical decision-
making and cherish patients with fruitful outcomes which has also helped within 
increasing the digital or wireless health care delivery facilities. Considering numbers 
of people affected by skin cancer and the potential of lifestyle changes, both patients 
and their doctors are now looking to digital solutions as the means for broadening 
prevention efforts and detecting the disease in its early stage or curable stages. When 
it comes to skin cancer, the development of m-Health applications is largely focused 
on preventive measures. Nevertheless, m-Health technologies also work great for 
healthcare providers [10]. They mainly promote the increase of trust between a 
patient and their doctor and also ensure the efﬁciency of certain important processes, 
including provision of remote care for patients, faster response time and avoidance 
of unnecessary referrals for face-to-face consultations. In this research analysis, a 
novel m-health prototype application called ‘Smart Skin-Proto’ is developed and 
implemented. 
The main contributions of the research is highlighted below.
. Signiﬁcance of mobile healthcare in skin diseases diagnosis is highlighted in the 
study.
. A novel model ‘Smart Skin-Proto’ for skin cancer risks analysis is proposed to 
asisst medical experts.
. Further decision tree algorithm is used to clasisfy the skin cancer patients into 
‘high risk’ and ‘low risk’ categories.
. The model records an optimal performance thereby recording an accuracy of 
96.2% with 15 decision trees. Also latency in this model is only 0.7 s which is 
the least when compared with other existing apps. 
The paper is organized as follows. Section 1 presents a succinct analysis of skin 
cancer diagnsois in the present scenario and the role of mobile healthcare in assesisng 
skin cancer risks. Section 2 highlights the relevant existing models related to skin 
cancer and its disorders. Section 3 proposes a novel framework using mobile health-
care and machine learning based classiﬁcation with decision tree to detect the risk 
level in patients. Section 4 depicts the outcome of implementing the proposed model 
and verifying its accuracy of prediction. Section 5 concludes the study with a suitable 
conclusion. 
2 
Literature Review 
In many scenarios, digital applications assist doctors for assessment of skin cancer 
through the screening procedure which appears feasible. This section represents 
some important works that might be done in context with respect to m-health usage 
for skin cancer diagnosis. Various computer assisted treatment are developed to 
deal with auto enabled detection of skin cancer [6]. Recently majority of methods

116
S. Mishra et al.
utilize machine learning trained on dermoscopy images [7]. Phillips et al. [8] devel-
oped a web based interface to distinguish melanoma and non-melanoma applying 
a Support Vector Machine (SVM) validated on a data samples consisted of some 
image data of three kinds of skin lesions. Since the framework was designed with 
less data, it offered an average performance. Ly et al. [9] designed a deep neural 
network approach to operate on diverse operating systems. It was later veriﬁed with 
skin cancer dataset which performed much better than the base model in context 
to its computational effectiveness. Dai et al. [11] discussed a smart phone based 
model for melanoma detection using advanced neural network. It used HAM10000 
dataset [12], that consists of thousands of dermoscopic data categorized into seven 
diverse kind of skin cells. Pacheco and Krohling [13] proposed a skin lesion classiﬁer 
for some general skin related disorders. The classiﬁer works on the basis of a deep 
learning approach that take into account medical image based data and patient demog-
raphy features gathered through mobile devices. Next, Castro et al. [14] developed a 
model using mutation process of evolutionary methods to tackle uneven data distri-
bution issue. In addition, they implement an application to classify melanoma and 
non-melanoma skin lesions. Recent trends in mobile skin cancer treatment permits 
convenient methods for robust melanoma diagnosis at an early stage [15]. Various 
applications on skin cancer using mobile devices like DermaIA and Skin-vision are 
available which offer an ease of carrying out risk diagnsois by taking skin scans 
using mobile camera [16]. But most of these existing models are not so scalable and 
reliable. 
3 
Proposed M-health Prototype for Skin Cancer 
Smart Skin-Proto is a developed mobile based application built for self-monitoring 
or gaining the knowledge of your problems before hand and its prototype is depicted 
Fig. 2. The diagram of the developed prototype deﬁnes us a perfect example of appli-
cation which includes capturing the behavior of your infection. The main objective 
of these model is basically to take photos on ﬁrst sight of your moles and then upload 
them on the server to carry out assessment of skin cancer disorders. It can also be used 
for storing previously taken scans that help in collecting information of previous anal-
ysis. Extra attributes included to intend rise of melanoma involve ultraviolet signals, 
next examination notiﬁcation along with a module for easy determination of closest 
dermatologist. The outcome of binary risk rating is comparatively low or high. This 
app doesn’t provide any diagnosis feature. So, for high-risk cases, the users are tend 
to receives some advice from the team to visit a dermatologist as soon as possible. 
Initial steps include image pre-processing with segmenting images as done for every 
mole photo analysis. In personal capacity, users are provided with an extra feature 
for consultation with experts.
A Smart Skin-Proto high-risk alert means it needs a really quick medical attention 
by a doctor immediately for the treatment, medium means it requires instant moni-
toring to detect a rapid change in it’s size, shape or colour. This app has the ability

Smart Skin-Proto: A Mobile Skin Disorders Recognizer Model
117
Fig. 2 Demonstration of the proposed Smart Skin-Proto prototype
to identify risks which are normally related to risky moles. The algorithm mainly 
looks for patterns of the outlines and dimensional change of a mole that is likely 
to be malignant. It is then divided into categories of high, medium or low risk. The 
primary feature of the Smart Skin-Proto solution is its unique multi feature approach. 
The mathematical calculation that the method computes is the fractal dimension of 
skin lesions and surrounding skin tissues that develops a structured region to depict 
various tissues growth. The online analysis method processes potential chaotic devel-
opment of pigmented and non-pigmented skin tissues. It shows a green, yellow, or 
red risk level on basis of severity index of the skin condition. 
4 
Result and Analysis 
Smart Skin-Proto was developed through a deep study and the research achieved a 
constructive outcome for the detection of malignant or pre-malignant lesions. Our 
app was designed mainly focusing on the early assessment of skin cancer. It uses 
medically veriﬁed predictive method decision tree method to mitigate skin cancer 
risk on skin lesions [17, 18]. There are some other relevant mobile applications on 
skin cancer diagnosis in market. Some widely used applications are as follws.
. Miiskin app: It is a paid to user application using very sharp resolution scans 
taken from larger body parts. It permits user to analyse evolution of moles to 
identify variations in skin [19].

118
S. Mishra et al.
. Molemapper app: It is collaborative effort of multiple organizations to design 
this application that comes free of cost. It assists medical staffs to track sceptical 
areas in skin without regular visists to medical centers.
. MoleScope app: This simple application is associated with a module which is 
connected to smartphone. Photos scanned through the attachment and communi-
cated online to skin specialists for further diagnsois.
. SkinVision app: This paid application was designed by a group of skin doctors 
and it utilizes deep learning approach to process mole scans and determine the 
risk level in quick time [20].
. UMSkinCheck app: This application is a non-paid module by which a skin cancer 
test can be held using deﬁned suggestions and variations can be tracked over time. 
It is also equipped with available trained images for compatision and tracking 
variations. 
The proposed Smart Skin-Proto is compared with the existing m-health models 
as shown in Fig. 3. It is observed that the time complexity of Smart Skin-Proto is 
relatively less compared to other models. While it also generates a maximal accuracy 
of 96.2 than other applications, it is made free of cost to be utilized by maximum 
needy patients. 
Figure 4 demonstrates a comparative study of the accuracy rate analysis using 
decision tree with other classiﬁers. It is observed that accuracy with decision tree 
was determined to be 96.2% while it is relatively less with other existing models. 
Accuracy with RBF model is the least with 79.4%.
In another performance evaluation as depicted in Fig. 5, the accuracy rate is 
calculated in context to the number of decision trees used in the model. A total of 25
Fig. 3 Comparison of Smart Skin-Proto with other existing prototypes 

Smart Skin-Proto: A Mobile Skin Disorders Recognizer Model
119
Fig. 4 Accuracy analysis of decision tree used in Smart Skin-Proto with other classiﬁers
decision trees is taken into consideration in the prototype. It is observed that with 15 
decision trees, the model recorded an optimal accuracy of 96.2%. 
Fig. 5 Accuracy analysis of Smart Skin-Proto in context to number of decision trees used

120
S. Mishra et al.
5 
Conclusion 
Skin cancer is a widely spreading disease which needs immediate attention [21, 22]. 
Though there are some existing good models for this, they are not so accurate and 
reliable. In this research, a mobile algorithm for skin cancer assessment is developed 
and implemented. The model used decision tree algorithm for classiﬁcation into skin 
cancer types. The outcome obtained is very promising. The accuracy generated is 
96.2% while it I noted that it performs best with 15 decision trees into consideration. 
Smart Skin-Proto welcomes research on its application. Thus the proposed Smart 
Skin-Proto application tries to take forward a small step towards assessment of skin 
cancer thereby helping the medical experts in their diagnosis. 
References 
1. Siegel, R.L., Miller, K.D., Jemal, A.: Cancer statistics, 2019. CA: a Cancer Journal for 
Clinicians 69(1), 7–34 (2019) 
2. Jena, L., Mishra, S., Nayak, S., Ranjan, P., Mishra, M.K.: Variable optimization in cervical 
cancer data using particle swarm optimization. In: Advances in Electronics, Communication 
and Computing, pp. 147–153 (2021). Springer, Singapore. https://doi.org/10.1007/978-981-
15-8752-8_15 
3. Schadendorf, D., et al.: Melanoma. The Lancet 392(10151), 971–984 (2018) 
4. Tripathy HK, Mishra S, Thakkar HK, Rai D (2021) CARE: a collision-aware mobile robot 
navigation in grid environment using improved breadth ﬁrst search. Comput Electr Eng 
94:107327 
5. Sahoo, S., Das, M., Mishra, S., Suman, S.: A hybrid DTNB model for heart disorders prediction. 
In: Advances in electronics, communication and computing, pp. 155–163 (2021). Springer, 
Singapore. https://doi.org/10.1007/978-981-15-8752-8_16 
6. Zhang N, Cai Y-X, Wang Y-Y, Tian Y-T, Wang X-L, Badami B (2020) Skin cancer diagnosis 
based on optimized convolutional neural network. Artif Intell Med 102:101756 
7. Hekler A et al (2019) Superior skin cancer classiﬁcation by the combination of human and 
artiﬁcial intelligence. Eur J Cancer 120:114–121 
8. Phillips, K., Fosu, O., Jouny, I.: Mobile melanoma detection application for android smart 
phones. In: 2015 41st Annual Northeast Biomedical Engineering Conference (NEBEC), pp. 1–2 
(2015) 
9. Ly, P., Bein, D., Verma, A.: New compact deep learning model for skin cancer recognition. 
In: 9th IEEE Annual Ubiquitous Computing, Electronics Mobile Communication Conference 
(UEMCON), pp. 255–261 (2018) 
10. Berseth, M.: ISIC 2017 - Skin Lesion Analysis Towards Melanoma Detection (2017) 
11. Dai, X., Spasic, I., Meyer, B., Chapman, S., Andres, F.: Machine learning on mobile: an on-
device inference app for skin cancer detection. In: Fourth International Conference on Fog and 
Mobile Edge Computing (FMEC), pp. 301–305 (2019) 
12. Tschandl, P.: The HAM10000 Dataset, a Large Collection of Multisource Dermatoscopic 
Images of Common Pigmented Skin Lesions (2018). https://doi.org/10.7910/DVN/DBW86T 
13. Pacheco AGC, Krohling RA (2020) The impact of patient clinical information on automated 
skin cancer detection. Comput Biol Med 116:103545 
14. Castro, P.B.C., Krohling, B.A., Pacheco, A.G.C., Krohling, R.A.: An app to detect melanoma 
using deep learning: An approach to handle imbalanced data based on evolutionary algorithms. 
In: International Joint Conference on Neural Networks. IEEE, pp. 1–8 (2020)

Smart Skin-Proto: A Mobile Skin Disorders Recognizer Model
121
15. Zaidan A et al (2018) A review on smartphone skin cancer diagnosis apps in evaluation and 
benchmarking: coherent taxonomy, open issues and recommendation pathway solution. Health 
Technol. (Berl) 8(4):223–238. https://doi.org/10.1007/s12553-018-0223-9 
16. Maier T et al (2015) Accuracy of a smartphone application using fractal image analysis 
of pigmented moles compared to clinical diagnosis and histological result. J. Eur. Acad. 
Dermatology Venereol. 29(4):663–667. https://doi.org/10.1111/jdv.12648 
17. Jena, L., Kamila, N.K., Mishra, S.: Privacy preserving distributed data mining with evolu-
tionary computing. In: Proceedings of the International Conference on Frontiers of Intelligent 
Computing: Theory and Applications (FICTA) 2013, pp. 259–267 (2014). Springer, Cham. 
https://doi.org/10.1007/978-3-319-02931-3_29 
18. Mishra S, Tripathy HK, Panda AR (2018) An improved and adaptive attribute selection 
technique to optimize dengue fever prediction. Int J Eng Technol 7:480–486 
19. Mohapatra, S.K., Nayak, P., Mishra, S., Bisoy, S.K.: Green computing: a step towards eco-
friendly computing. In: Emerging Trends and Applications in Cognitive Computing, pp. 124– 
149 (2019). IGI Global. 
20. Mishra S, Jena L, Tripathy HK, Gaber T (2022) Prioritized and predictive intelligence of 
things enabled waste management model in smart and sustainable environment. PLoS ONE 
17(8):e0272383 
21. Tripathy, H.K., Mishra, S., Suman, S., Nayyar, A., Sahoo, K.S.: Smart COVID-shield: an IoT 
driven reliable and automated prototype model for COVID-19 symptoms tracking. Computing, 
pp. 1–22 (2022) 
22. Suman, S., Mishra, S., Sahoo, K.S., Nayyar, A.: Vision navigator: a smart and intelligent 
obstacle recognition model for visually impaired users. Mobile Information Systems, 2022 
(2022)

Machine Learning Approach Using 
Artiﬁcial Neural Networks to Detect 
Malicious Nodes in IoT Networks 
Kazi Kutubuddin Sayyad Liyakat 
Abstract Devices can now effortlessly and wirelessly share data with one another 
over the internet or other networked systems thanks to a relatively new technology 
called Internet of Things (IoT). Despite these advantages, IoT systems are now more 
vulnerable to hacker attacks, which could lead to unfavourable outcomes. This is 
because of the IoT ecosystem’s continual expansion. These incursions may cause 
potential ﬁnancial and physical harm. The Internet of Things is the automatically 
conﬁguring network. This network is susceptible to a variety of attacks, all of which 
can be started by rogue nodes. For instance, during a denial of service attack, a 
malicious node bombards a targeted node with a large number of packets. For the 
purpose of locating these malicious nodes in a network, a threshold-based procedure 
utilising cutting-edge machine learning techniques is launched. By checking the path 
latency and alerting on it if it exceeds a set threshold value, the suggested method 
can help identify an attacker node. The NS2 programme will be used to mimic the 
suggested method. We evaluate the suggested methodology and demonstrate that our 
system performs well in terms of a number of measures, such as throughput, latency, 
and packet loss. 
Keywords IoT · Cyber attacks · ML · ANN · Nodes
K. K. S. Liyakat envelope symbol
Department of Electrical Engineering, Brahmdevdada Mane Institute of Technology, Solapur, 
MS, India 
e-mail: drkkazi@gmail.com 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_12 
123

124
K. K. S. Liyakat
1 
Introduction 
There are many potential areas of use for IoT devices, including the smart home, 
healthcares, public safety, monitoring, and environmental safety, and their prevalence 
is only expected to grow [1]. In the IoT, different devices are linked together and 
share data with one another. In a multi-hop network, the sensory data is typically 
relayed by a number of devices before arriving at the aggregation node, where it 
can be used to inform the user’s decision-making process. Many Internet of Things 
protocols use the mesh topology now [2], including Z-Wave, Thread, Zig-Bee/IEEE 
802.15.4 [3]. Using multi-hop routing, devices with this capability can talk to nearby 
ones and even the sink. [4]. The Internet of Things is progressing rapidly, yet there 
are numerous security risks associated with IoT devices [5]. IoT devices may be 
susceptible to a wide range of assaults due to the complexity of IoT networks. For 
a passive attack is when an adversary simply listens in on communications among 
nodes via a wireless control [6]. Due to the fact that a passive attack just collects data 
with no altering, how a protocol usually works, it’s hard to ﬁnd out [7]. Active attack 
[8] (for example, Black-hole attack [9], Sybil Attack [10]): Actively introducing 
false information, discarding and manipulating data packets, and otherwise severely 
affecting on network protocols is how an attacker node violates the security plan and 
threatens the network’s availability and integrity [11]. Attacks can be classiﬁed as 
either external or internal, depending on where they originate. Attacks on a network 
can be classiﬁed as either external (originating from a node outside of the network) 
or internal (originating from nodes already inside the network and hence authorized 
to access its resources illegally) [12]. 
It is crucial to promptly identify malicious nodes due to the extensive harm they 
can do to the system and network functionality if they are not removed [13]. The 
detection and identiﬁcation of malicious activity and trafﬁc has been the subject of 
numerous studies in the academic literature. In this study, we take a formal approach 
to this problem by ﬁrst noting that the reputation of a Regression analysis is a suitable 
model for describing node, and then proceeding to deﬁne the connection between 
a network’s nodes’ & routes’ trustworthiness [14]. The node’s trustworthiness and 
maliciousness are then learned with the use of the linear regression (GD) and support 
vector machines (SVM) algorithms [15]. Both methods are capable of modifying the 
detection model based on the provided data, allowing for the identiﬁcation of nodes 
with the most reliable reputations [16]. 
There has been a rise in numbers of IoT devices, necessitating real-time manage-
ment of their operation, testing, debugging, and security by organizations [17]. 
However, this task is difﬁcult for three main reasons: First, devices are difﬁcult 
to manage because they are deployed from afar; second, the setting’s diversity 
makes it hard for people to effectively communicate with one another. Lastly, the 
work itself is challenging. Thirdly, and probably even more signiﬁcantly, is the 
problem of protecting systems and data against vulnerabilities, attacks, and other 
ﬂawsv [18]. To overcome this latter difﬁculty, it is necessary to ﬁrst identify the 
unusual activity of IoT devices before developing adaptive and creative abnormality

Machine Learning Approach Using Artiﬁcial Neural Networks to Detect …
125
Fig. 1 Machine Learning [ML] Solutions for Various Attacks in IoT [29] 
detection techniques or malicious node detections. Ofﬂine log analytics, statistical 
anomaly discovery, and rule-based detection are only some of the common device 
ﬁngerprinting methodologies that have been employed to meet these issues [19]. By 
viewing machine learning thru the lens of the traditional CIA model, they extract 
behavioral traits that can be used to make judgments in areas where privacy and 
security are still relevant [20–24]. 
In contrast to ANN-Artiﬁcial Neural Network [25] based feature extraction 
methodologies, aforementioned methods cannot offer a real-time device categoriza-
tion solution. However, it’s not quite as simple as it sounds [26]. We found no methods 
in the literature that emphasize investigating devices’ long-term behavior, which 
necessitates extracting statistical features from raw IoT trafﬁc [27] data to capture 
their behavior in a single snapshot [28]. In addition, our study is one of the ﬁrst to 
attempt to extract features for classifying dangerous nodes from the header informa-
tion of IoT trafﬁc data [29]. The picture shows a categorization of IoT attack types and 
the corresponding deep [30] and machine learning (ML/DL) [31] countermeasures 
[29, 32, 33]. (Fig. 1) 
The paper is oganized by steps as- Literature review, Proposed methodology, 
Experimentation, and conclusion. ˙In proposed method, the algorithm steps are 
discussed. 
2 
Literature Survey 
Numerous studies on the implementation of ML in ﬁelds like object/ pattern 
matching, text and image processing have recently been reported. Deep Learning 
(DL) methods had also been widely used in a great deal of previous security research 
[34]. The authors of [35] discuss how big data and IoT have developed in smart city.

126
K. K. S. Liyakat
In [36], author describes how CC has developed and how large data have contributed 
to the progress of the IoT [37, 38]. “IoT security” refers to the practice of safe-
guarding devices that are connected to networks or Internet. Although the concept of 
IoT was introduced about a decade ago, the basic terminologies of IoT is currently 
in the growth stage for quite some time [10, 39]. IoT is a network of intercon-
nected computing devices, services, and physical objects that enables new forms of 
interactivity and improves existing ones by bringing together previously disparate 
entities and facilitating their exchange of data and command over the web. Internet of 
Things was created to make our daily life and the functioning of the modern world as 
smooth as possible. IoT is now ubiquitous. Smart sensors, ﬁtness smartphone apps, 
thermostat, Solar pv, air conditioners, or even kitchen appliances are all intercon-
nected online [40, 41]. It is becoming increasingly difﬁcult to safeguard IoT data 
against threats such as hackers, unauthorized access, and harmful trafﬁc because of 
the rapid evolution of IoT technology [42–44]. As a result, various safeguards are 
being designed, tested, and integrated into IoT frameworks and systems to ensure 
the safety of sensitive data. Several machine learning techniques have been deployed 
[45, 46, 48] to strengthen protections for the Internet of Things. In addition, people 
use cloud services for more reliable, safe, affordable, and effective network connec-
tions. In [47], a technique was created to secure cloud information while the cloud 
provider (CSP) is seeking & exchanging the information by means of end-user via a 
keyword-based mechanism. Also, unlike previous models, this one doesn’t require 
constant new rounds of public key (PKG) generating to encrypt the key. The authors 
[48] presented a technique to manage encrypted outsourced data while checking its 
fairness and veriﬁcation. Veriﬁcation & Fairness Attribute-based Generally System 
was built to check if the data supplied by the server is correct or whether there 
was malicious assertion in the data. Authors strengthened the data’s protection by 
encrypting it once more using a cipher whose text-policy attributes are based on the 
policy’s veriﬁcation and fairness. Communication-lock encryption is employed so 
that regular people can’t read the unencrypted message [49]. 
The scheme reversible essential element encryption-data integrity (RABE-DI) 
was given in reference [50] to allow the server to withdraw access to clients, even 
those with valid keys. To protect a cloud service’s security when revocation is applied, 
this model was proposed. Improved results were achieved by employing algorithms 
like Revoke and Decre. A user in [51] was able to communicate simple data to many 
groups using a key mechanism by using revocable identity-based broadcasting proxy 
re-encryption (RIB-BPRE). To realize the right revocable idea, the suggested method 
makes use of a crucial mechanism. Due to the large amount of data and the wide range 
of information stored in the cloud, the requisite procedure is essential [52]. While 
Support Vector Machine (SVM) is utilized to identify assaults in [53, 54], It’s not 
easy to make sure that every nodes in a multi-hop system is only one hop distant from 
a reliable one. Unsupervised machine learning methods were used to spot outliers 
in both experiments [55]. All of them looked for strange network activity but failed 
to pinpoint the perpetrators. The use of auto-encoder neurons in Emerging Wireless 
Networks for the purpose of identifying hostile nodes is detailed in [39] suggested 
a outﬂow discovery algorithm in a water pipeline using WSN that relied on Linear

Machine Learning Approach Using Artiﬁcial Neural Networks to Detect …
127
Discriminant Analyse (FDA) and a support vector machine (SVM) classiﬁer to spot 
anomalies. In a later paper, [56, 57],. introduced a reputation measure to quantify 
every routing trail and utilized unsupervised culture to determine which node in 
a multiple nodes IoT environment were malicious [58]. However, their strategy is 
based on the faulty assumption that all nodes on a given routing line have the same 
trust value [59]. In practice, this might not be the case, which could lead to inaccurate 
conclusions. 
3 
Proposed Methodology 
Based upon Artiﬁcial Neural Network, we devised a method for identifying poten-
tially harmful components of Internet of Things systems (ANN). An assortment of 
supervised methods, including ANN classiﬁers, were implemented, and their results 
were compared. Using our dataset, we have analyzed how well the chosen algo-
rithms perform. Following is some pseudo code for Algorithm 1 that represents our 
approach. 
Phase 1: At this point in the process, we have a representation of the network’s 
source mote and its destination mote. With the source mote, RREQ messages ﬂood 
the network. The Routing Time (RRT) of Route Request (RREQ) & Route Reply 
(RREP) messages is calculated by the originating mote. The RRT is calculated by 
starting a timer at the source when the RREQ ﬂood begins and stopping it when 
all nodes have received the RREP. The source route stores RREQ & RREP round 
travel time for each node. The focus of the source is on balancing two factors to 
determine the optimal travel path. The optimal path between two points is the one 
with the smallest number of hops and the highest sequence number. The distance 
between the source and the target is calculated by the source mole as well, taking 
into consideration the total number of hops taken. 
Phase 2: This step is carried out so that the malicious critters in the network 
may be identiﬁed. Following this, the source mote intends to broadcast information 
along the chosen path connecting the two destinations. Time is measured from the 
time a data packet leaves the source and arrives at each intermediate node until 
it reaches the destination. The RREP messages’ Round Trip Time (RTT) and the 
amount of time it takes to travel between nodes are compared. Some of the network’s 
nodes, especially those with a high temporal criticality for sending data packets, 
have declared themselves to be malicious. The malicious swarm is isolated from the 
network by employing a multipath routing strategy. Therefore, if the evil ﬂies leave 
a certain path, that path will be ignored in the network design.

128
K. K. S. Liyakat
ALGORITHM . DETECTION OF MALICIOUS NODES USING ARTIFICIAL 
NEURAL NETWORK(ANN) Input: Classification training attributes are stored in 
a dataset, together with the 'learning rate' parameter L and the multi-layer neural 
classifier (ANN). 
Result: Detection of Malicious nodes using ANN Techniques 
Begin 
1. Implement WSN with limited no. of nodes 
2. Locate the network nodes that will serve as both destinations and origins. 
3. Establish a Path 
3.1. If the path exits amid source and destination 
3.1.1. Start data transmission 
Else 
3.1.2. Source flood RREQ packets in the network 
3.1.3. Source start timer for noticing RRT 
3.1.4. Source maintains list of every node with the time after receiving route reply 
message at source 
3.1.5. Source selects the finest path amid source &target on the basis of hop count 
and sequence number 
3.1.6. The path having least hop count and extreme sequence number is chosen as 
best path amid source & target 
4. Malicious node Detection 
4.1. The source starts to transmit data across the chosen path 
4.2. The source notices data PAT at every hop 
4.3. if node has higher time than RRT 
4.3.1 Node marked as malicious 
4.4. Else 
4.4.1 source transmits data across the chosen path 
4.4.2. Repeat step 4 till data transmission 
Completed 
End 
4 
Experimentation 
Here, we take a look at how our proposed ANN-based attack detection approach 
performs in terms of a few key metrics, including packet loss, delay, and throughput. 
We have used NS-2 simulator to check performance of our proposed system. Loss 
of data packets after their transmission through a network is referred to as “packet 
loss.” The delay is the extra time it takes for a packet to reach its destination. The 
throughput of a network is measured as the percentage of packets received that are

Machine Learning Approach Using Artiﬁcial Neural Networks to Detect …
129
Fig. 2 NS-2 Simulation setup with nodes 
equivalent to total number of data packets. An evaluation is performed over a 500 m 
x 500 m area network with a 1000 byte packet size and 250 packets transferred per 
session. Total bandwidth (BW) is scheduled to range between 10 and 20 megahertz. 
IoT Nodes are only allowed access to a 2 MHz, 4 MHz, or 6 MHz bandwidth [22]. 
A 2 Mbps bandwidth allocation has been made for the shared control channel. The 
data were averaged from 10 separate simulation runs to arrive at the ﬁnal numbers 
presented here. 
Creating a path from the origin to the destination is shown in Fig. 2. There are 
malicious nodes in the network, prolonging the delay. When the system delay is 
too great, an advanced ANN-based ML detection method is used to identify these 
malicious activity. 
Detecting malicious nodes with the proposed ANN based hybrid veriﬁcation 
approach results in less packet loss than with the current scheme, as shown in Fig. 3
In Fig. 4, we see the differences between the new method and the old one in terms 
of the amount of time it takes to complete a task. In comparison to the established 
method, shown in green, for detecting malicious nodes in a network, the proposed 
method has a minimal delay, as shown by the blue line in the study. It’s clear that the 
red-highlighted delay case is the most signiﬁcant among the submitted cases.
Figure 5 depicts a comparison of the proposed system and the current systems in 
terms of throughput. As per the study, the suggested technique has higher throughput 
indicated with blue line in contrast to the earlier approach, indicated by green color 
to identify the malicious nodes in the system. The red-highlighted throughput case 
is preferable to the other two.

130
K. K. S. Liyakat
Fig. 3 Packet loss Comparison
Fig. 4 Delay Comparison

Machine Learning Approach Using Artiﬁcial Neural Networks to Detect …
131
Fig. 5 Throughput Comparison 
5 
Conclusion 
Internet of Things is a system that allows data collection and transmission from 
distributed sensor nodes to a central receiving point (sink). Attacker nodes are not 
prevented from entering this network, which results in a wide variety of incursions, 
both active and passive. Distributed denial of service attacks (DDoS) are intrusions 
into networks in which attacker nodes focus on ﬂooding the target node with an 
excessive number of packets, thereby degrading network performance. The goal of 
this study is to discover malicious activity in a network by the implementation of a 
novel technique. As a means of identifying harmful nodes, the proposed technique is 
based on a machine-learning-based ANN scheme. The proposed method is compared 
to the established trust-based mechanism. The suggested system outperforms the 
current trust-based mechanisms in terms of performance, packet loss, and delay. 
Over attack and trust-based scenarios are both provided by the proposed method. 
References 
1. Javed F, Afzal MK, Sharif M, Kim B-S (2020) Internet of things (IoT) operating systems 
support, networking technologies, applications, and challenges: a comparative review. IEEE 
Commun Surv Tuts 20:2062–2100 
2. Corak, B.H., Okay, F.Y., Guzel, M., Murt, S., Ozdemir, S.: Comparative analysis of IoT 
communication protocols. In: 2020 International Symposium on Networks, Computers and 
Communications (ISNCC), pp. 1–6 (2018) 
3. Tseng F-H, Chiang H-P, Chao H-C (2018) Black hole along with other attacks in MANETs: a 
survey. J Inf Process Syst 14:56–78

132
K. K. S. Liyakat
4. Jan MA, Nanda P, Liu RP (2018) A sybil attack detection scheme for a forest wildﬁre monitoring 
application. Fut Gener Comp Syst 80:613–626 
5. Papernot, N., McDaniel, P., Sinha, A., Wellman, M.P.: Sok: Security and privacy in machine 
learning. In: 2018 IEEE European Symposium on Security and Privacy (EuroS P), pp. 399–414 
(2021) 
6. Meidan Y, Bohadana M et al (2019) N-baiot—network-based detection of iot botnet attacks 
using deep autoencoders. IEEE Pervasive Comput 17(3):12–22 
7. Mendhurwar S, Mishra R (2021) Integration of social and IoT technologies: architectural 
framework for digital transformation cyber security challenges. Enterprise Inf Syst 15(4):565– 
584 
8. Allam Z, Dhunny ZA (2019) On big data, artiﬁcial intelligence and smart cities. Cities 89:80–91 
9. Mohbey, K.K.: An efﬁcient framework for smart city using big data technologies and internet 
of things. In: Progress in Advanced Computing and Intelligent Engineering, Singapore (2019) 
10. Sharma, N., Shamkuwar, M., Singh, I.: The history, present and future with IoT. In: Internet 
of Things and Big Data Analytics for Smart Generation; Springer International Publishing: 
Cham, Switzerland, pp. 27–51 (2019). https://doi.org/10.1007/978-3-030-04203-5_3 
11. Kazi KSL (2022) Implementation of e-mail security with three layers of authentication. J 
Operating Syst Dev Trends 9(2):29–35 
12. Shahid, J., Ahmad, R., Kiani, A.K., Almuhaideb, A.M., et al.: Data Protection and Privacy of 
the Internet of Healthcare Things (IoHTs). Appl. Sci. 12, 1927 (2022) 
13. Abbasi MA, Zia MF (2017) Novel TPPO based maximum power point method for photovoltaic 
system. Adv. Electr. Comput. Eng. 17:95–100 
14. Ashraf S, Shawon MH, Khalid HM, Muyeen S (2021) Denial-of-service attack on IEC 61850-
based substation automation system: a crucial cyber threat towards smart substation pathways. 
Sensors 21:6415 
15. Khalid HM, Peng JCH (2017) Immunity toward data-injection attacks using multisensor track 
fusion-based model prediction. IEEE Trans. Smart Grid 8:697–707 
16. Kazi KSL (2022) Text analysis in heath care study using IoT. J Comput Technol Appl 13(3):11– 
18 
17. Khan, H.M.A., Inayat, U., et al.: Voice over internet protocol: vulnerabilities and assessments. 
In: Proceedings of the International Conference on Innovative Computing (ICIC), Lahore, 
Pakistan, pp. 1–6 (2021) 
18. Choi C, Choi J (2019) Ontology-based security context reasoning for power IoT-cloud security 
service. IEEE Access 7:110510–110517 
19. Ge C, Susilo W et al (2021) Secure keyword search and data sharing mechanism for cloud 
computing. IEEE Trans. Dependable Secur. Comput. 18:2787–2800 
20. Ge, C., Susilo, W., Baek, J., Liu, Z., Xia, J., Fang, L.: A veriﬁable and fair attribute-based proxy 
re-encryption scheme for data sharing in clouds. IEEE Trans. Dependable Secur. Comput. 
(2021) 
21. Ge, C., Susilo, W., Baek, J., Liu, Z., Xia, J., Fang, L.: Revocable attribute-based encryption 
with data integrity in clouds. IEEE Trans. Dep. Secur. Comput. (2021) 
22. Ge C, Liu Z, Xia J, Fang L (2021) Revocable identity-based broadcast proxy re-encryption for 
data sharing in clouds. IEEE Trans. Dependable Secur. Comput. 18:1214–1226 
23. Kaplantzis, S., Shilton, A., Mani, N., Sekercioglu, Y.A.: Detecting selective forwarding attacks 
in wireless sensor networks using support vector machines. In: IEEE ISSNIP, pp. 335–40 (2008) 
24. Liyakat, K.K.S.: Predict the severity of diabetes cases, using k-means and decision tree 
approach. J Adv Shell Programm 9(2), 2431 (2022) 
25. Akbani, R., Korkmaz, T., Raju, G.V.S.: A machine learning based reputation system for 
defending against malicious node behavior. GLOBECOM, pp. 2119–23 (2008)

Machine Learning Approach Using Artiﬁcial Neural Networks to Detect …
133
26. Nahiyan, K., Kaiser, S., Ferens, K., McLeod, R.: A multi-agent based cognitive approach to 
unsupervised feature extraction and classiﬁcation for network intrusion detection. In: ACC’17, 
pp. 25–30 (2017) 
27. Dromard J, Roudiere G, Owezarski P (2017) Online and scalable unsupervised network 
anomaly detection method. IEEE Trans Netw Serv Manag 14(1):34–47 
28. Luo, T., Nagarajan, S.G.: Distributed anomaly detection using autoencoder neural networks 
in WSN for iot. In: 2018 IEEE International Conference on Communications (ICC), pp. 1–6 
(2018) 
29. Al-Garadi MA, Mohamed A et al (2020) A survey of machine and deep learning methods for 
internet of things (IoT) security. IEEE Commun Surveys & Tutorials 22(3):1646–1685 
30. Liu, X., Abdelhakim, M., Krishnamurthy, P., Tipper, D.: Identifying malicious nodes in Multi-
hop IoT networks using diversity and unsupervised learning. In: IEEE International Conference 
on Communications, pp. 1–6 (2018) 
31. Kazi, K.: Multiple object Detection and Classiﬁcation using sparsity regularized Pruning on 
Low quality Image/ video with Kalman Filter Methodology (Literature review) (2022) 
32. AnanthaNatarajan, V.: Forecasting of wind power using LSTM recurrent neural network. J. 
Green Eng (JGE) 10(11) (2020) 
33. Kazi KS (2017) Signiﬁcance and usage of face recognition system. Scholarly J Humanity Sci 
English Language 4(20):4764–4772 
34. Dixit, A.J., et al.: Iris recognition by daugman’s method. IntJ Latest Technol Eng, Manage 
Applied Sci 4(6), 9093 (2015) 
35. Wale Anjali, D., Rokade, D., et al.: Smart agriculture system using IoT. International J 
Innovative Res Technol 5(10), 493-497 (2019) 
36. Hotkar, P.R., Kulkarni, V., et al.: Implementation of low power and area efﬁcient carry select 
adder. International J Res Eng Science and Manage 2(4), 183-184 (2019) 
37. Nikita, K., Supriya, J., et al.: Design of vehicle system using CAN protocol. International J 
Res Applied science Eng Technol 8(V), 1978–1983 (2020) 
38. Ki, K.: Lassar methodology for network intrusion detection. Scholarly Res J Humanity science 
and English Language 4(24), 6853–6861 (2017) 
39. Sayyad Liyakat, K.K.: Situation invariant face recognition using PCA and feed forward neural 
network. Proceeding of International Conference on Advances in Engineering, Science and 
Technology, pp. 260263 (2016) 
40. Sayyad Liyakat, K.K.: An approach on yarn quality detection for textile industries using image 
processing. Proceeding of International Conference on Advances in Engineering, Science and 
Technology, pp. 325–330 (2016) 
41. Nagare, M.S., et al.: Different segmentation techniques for brain tumor detection: a survey. MM-
International society for green, Sustainable Engineering and Manage 1(14), 29–35 (2014) 
42. Dixit AJ (2014) A review paper on iris recognition. Journal GSD International society for 
green, Sustainable Engineering and Manage 1(14):71–81 
43. Kazi KSL (2023) IoT-based healthcare system for home quarantine people. J Instrumentation 
Innovation Sci 8(1):1–8 
44. Nagare, M.S., et al.: An efﬁcient algorithm brain tumor detection based on segmentation and 
thresholding. J Manage Manufacturing and Services 2(17), 19–27 (2015) 
45. Dixit AJ (2015) Iris recognition by daugman’s algorithm – an efﬁcient approach. J Applied 
Res Social Sci 2(14):1–4 
46. Kazi, K.S., Shirgan, S.S.: Face recognition based on principal component analysis and feed 
forward neural network. National Conference on Emerging trends in Engineering, Technology, 
Architecture, pp. 250–253 (2010) 
47. Aavula, R., Deshmukh, A., Mane, V.A., et al.: Design and Implementation of sensor and IoT 
based remembrance system for closed one. Telematique 21(1), 27692778 (2022)

134
K. K. S. Liyakat
48. Nikita, S., et al.: Announcement system in Bus. J. Image Processing and Intelligent Remote 
Sensing, 2(6) (2022) 
49. Kamuni, M.S., et al.: Fruit quality detection using thermometer. J Image Processing and 
Intelligent Remote Sensing 2(5) (2022) 
50. Kazi, K., Liyakat, S.: A novel design of IoT based ‘love representation and remembrance’ 
system to loved one’s. Gradiva Review J 8(12), 377–383 (2022) 
51. Akansha, K., et al.: Email Security. J Image Process Intelligent Remote Sensing 2(6) (2022) 
52. Kapse, M.M., et al.: Smart grid technology. Int J Inf Technol Comput Eng 2(6) 
53. Vaijnath, S.P., Prajakta, M., et al.: Smart safty Device for Women. Int J Aquatic Sci 13(1), 556-
560 (2022) 
54. Kazi, K., Liyakat, S., et al.: Multiple object detection and classiﬁcation based on Pruning using 
YOLO. Lambart Publications, ISBN – 978–93–91265–44–1 (2022) 
55. Tadlgi, P.M., et al.: Depression detection. J Mental Health Issues and Behavior (JHMIB), 2(6), 
1–7 (2022) 
56. Maithili W et al (2022) Smart watch system. International J Inf Technol Computer Eng (IJITC) 
2(6):1–9 
57. Alsharif M, Rawat DB (2021) Study of machine learning for cloud assisted IoT security as a 
service. Sensors 21:1034 
58. Swami D et al (2022) Sending notiﬁcation to someone missing you through smart watch. 
International J Inf Technol. Computer Eng (IJITC) 2(8):19–24 
59. Kalmkar, S., et al.: 3D E-Commers using AR. International J Inf Technol Computer Eng 
(IJITC), 2(6), 18–27 (2022)

Real Time Air-Writing and Recognition 
of Tamil Alphabets Using Deep Learning 
S. Preethi, T. Meeradevi, K. Mohammed Kaif, S. Hema, and M. Monikraj 
Abstract Writing has always been a prominent way of communication. The way in 
which the letters are written has been varying with time. From the conventional pen 
and paper to touch pad and stylus, the way of writing has evolved. Air- Writing is 
another development in which the characters are written in free space without being 
limited to a speciﬁc tool. This method of writing makes the hand movement easier 
compared to the conventional methods. Therefore, the air writing and recognition 
model will be of great help for children who start learning a language. The trajectory 
of the air written characters is obtained by mapping the focal point using Optical 
ﬂow in OpenCV. The obtained trajectory is then preprocessed and given to Dense 
Net 121 which is a type of CNN model widely used for pattern matching along 
with the dataset from HP labs which contains 3000 images for 11 Tamil vowels. 
The model which is trained obtained a maximum training and validation accuracy of 
98.2% and 91.83% respectively with minimum training and validation loss of 6.35% 
and 21.04% respectively. 
Keywords Convolutional Neural Network (CNN) · Optical ﬂow · Dense net 121 ·
Open cv
S. Preethi envelope symbol · T. Meeradevi · K. M. Kaif · S. Hema · M. Monikraj 
Department of Electronics and Communication Engineering, Kongu Engineering College, Erode, 
Tamil Nadu, India 
e-mail: preethi.s.ece@kongu.edu 
T. Meeradevi 
e-mail: meeradevi.ece@kongu.edu 
K. M. Kaif 
e-mail: mohammedkaifk.19ece@kongu.edu 
S. Hema 
e-mail: hemas.19ece@kongu.edu 
M. Monikraj 
e-mail: monikrajm.19ece@kongu.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_13 
135

136
S. Preethi et al.
1 
Introduction 
When a child grows it is normal for him/her to always touch and feel things by hand. 
During the learning process children tend to struggle with holding a pen or pencil. 
Simply writing by ﬁngers will make the hand movements easier than conventional 
methods. 
Also speciﬁc learning disabilities like dyslexia, dysgraphia, autism spectrum 
disorder, hyperactivity disorder affects up to 10 percent of the population which 
has direct or indirect effect on the writing abilities of children. Dyslexia is a neuro-
logical disorder who lack on learning abilities. They mainly lack on the side that 
involves reading difﬁculties and speech sounds recognition and learning how they 
relate to letters and words. Dysgraphia is a learning disorder of written expression 
that impairs coherence and handwriting in addition to overall writing abilities. It is 
a writing condition linked with poor handwriting, orthographic coding, and ﬁnger 
sequencing. It is also a speciﬁc learning disability (SLD) and transcribing disability. 
It often comes with other speech related or reading on neurological disorder. 
These children have trouble with following the normal writing method of pen up 
pen down in the early stages of learning. The air writing method will help them learn 
how to write without having to worry about handling the pen. 
2 
Literature Review 
Md.Shahinur Alam et al. [1] have proposed a paper in which an air writing recognition 
model using a depth camera which collected the three-dimensional trajectories by 
tracking the ﬁngertip was developed. In order to recognize the air written text (CNN) 
Convolution Neural Network and Long Short-Term Memory is used. A (6DMG) 6D 
motion gesture alphanumeric character data set is used and they achieved an accuracy 
of 99.32%. 
Vishal Vaidya, T.Pravanth, D.Viji [2] proposed an application which is a helpful 
tool that may be used to show dyslexic individuals how an alphabet, or a number, 
is drawn or sounded. The study demonstrates how various technologies can be 
employed to enhance dyslexic individual’s learning growth. This application was 
created using a combination of computer vision and machine learning techniques. 
The user’s “air gestures” are tracked by the computer’s webcam and classiﬁed into 
one of 36 classes by the air writing recognition system: 26 letters and 10 digits. 
Mingyu Chen et al. [3] have published a paper where air writing is based on 
two levels which are motion image and characters. The motion characters can be 
recognized like gesture with better sophistication. To create a meaningful word in 
the case of motion word recognition, the characters must be joined, but they are 
connected in the window. In order to represent words statistically, clustered ligature 
models and individual letter models are combined. It uses a hidden Markov model 
for recognition and air-writing. The model achieved a word recognition error rate

Real Time Air-Writing and Recognition of Tamil Alphabets Using Deep …
137
of 0.8% and recognition of character error rate of 1.9%. Air-writing and a virtual 
keyboard have respective word counts of 5.43 and 8.42 per minute. 
Mingyu Chen et al. [4] have proposed a paper which dealt with identifying and 
detecting activities of continuous motion trajectory without delimitation that involve 
writing in the air. Unlike conventional pattern recognition issues, superﬂuous ﬁnger 
movement when writing a character or a word is a challenge for air writing. To solve 
the issue of unintentional ﬁnger movement, recordings are obtained of a dataset 
that includes both writing and non-writing ﬁnger motions. To track ﬁngers without 
using markers or gloves, use the LEAP from Leap Motion. It uses a window-based 
algorithm that can automatically recognize words written in the air in a continuous 
stream of motion data. A writing segment is created out of a series of writing events. 
Faisal Baig and Muhammad Fahad Khan [5] presented a paper where a real time 
video based pointing approach was suggested. There are two key components to 
it. Prior to applying English (OCR) Optical Character Recognition on the plotted 
images to identify the written characters, it tracks the colored ﬁngertip in video 
frames. OpenCV and Java were used to create an application. 
3 
Methodology 
3.1 
Dataset Description 
The dataset contains a total of 3000 images which includes 11 Tamil vowels taken 
from HP Tamil Dataset for vowels. On an average each character has 270 images 
on an average except
, which has only one image as dataset. The dataset is 
divided into 11 classes. 80 percent of the dataset is divided into training dataset and 
20 percent of the dataset is divided into testing dataset. The Table 1 shows the details 
of the dataset. 
The Fig. 1. shows the unique characters generated in the code while training the 
dataset. The Table 2 shows the count of each vowel in the dataset along with the 
unique character given to the vowel during training the dataset.
Table 1 Dataset details 
DATASET
NUMBER OF 
SUBJECTS 
SAMPLES 
PER 
SUBJECT 
TOTAL 
SAMPLES 
NUMBER OF 
TRAINING 
SAMPLES 
NUMBER OF 
TESTING 
SAMPLES 
HP Tamil 
Dataset 
11
270
3000
2400
600 

138
S. Preethi et al.
Fig. 1 Numbers representing unique characters 
Table 2 Dataset count 
UNIQUE 
CHARACTER 
EQUIVALENT 
VOWELS 
NUMBER 
OF DATA 
000
276 
001
274 
002
276 
003
272 
004
276 
005
275 
006
267 
007
275 
008
269 
009
271 
010
268 
155
1 
3.2 
Block Diagram 
The proposed method consists of three major parts: 1. Focal point mapping 2. Pre-
processing 3. Prediction as shown in the Fig. 2 The ﬁrst step is to map the trajectory 
of the character that is being written. In order to obtain the trajectory focal point 
mapping is done. The focal point is mapped by using Optical ﬂow in Open CV 
package. The Optical ﬂow plots the coordinates of the focal point in every frame 
of the live video and ﬁnally obtains the trajectory. The trajectory is normalized and 
preprocessing [7] is done. The BGR image is converted into grayscale. After that 
thresholding is done followed by dilating and ﬁnally canny. After this process the 
preprocessed image is obtained. The dataset which trained and tested is also made. 
The dataset is trained and tested using Dense Net 121 which is a CNN model. The 
Dense Net 121 is an image processing and pattern matching algorithm. This model 
has been chosen because this overcomes the vanishing gradient problem in other CNN 
architectures. Vanishing gradient is a condition in which the actual data is lost after 
being processed by multiple layers of the model. The Dense Net 121 architecture 
overcomes this issue by having the memory element. This helps in retrieving the 
actual data after each layer.

Real Time Air-Writing and Recognition of Tamil Alphabets Using Deep …
139
Fig. 2 Block diagram 
3.3 
Trajectory Mapping 
Optical ﬂow [6] is one of the methods of Open CV package. The task of estimating 
the per-pixel mobility between two successive frames in the playing video is known 
as optical ﬂow. The primary purpose of optical ﬂow is to calculate the displacement 
vector of an object brought on by camera or motion. According to a theoretical 
method, if a grayscale image exists, the matrix containing the pixel intensity and the 
function up per I left parenthesis x comma y comma t right parenthesis commawhere x, y are the pixel coordinates and t are the frame numbers, 
are both available. The precise pixel intensity at frame t is determined by the I (x, y,

140
S. Preethi et al.
Fig. 3 Image Pre-processing 
t) function. Therefore, 
up per I left parent hesis x  comma y co m ma 
t right parenthesis equals t left parenthesis x plus x comma y plus y comma t plus t plus plus t right parenthesis
The object displacement does not change the intensity of the pixels that belong 
to the exact object. Using Taylor series expansion, the equation can be rewritten as 
mentioned in the formula. 
up per I left parent h esis x  comma y  comma 
t r
u pper  I prim e  x  u
 plus upper I prime y u equals minus upper I prime t
Equation (2) is written as Eq. (3) where u eq uals d  x d ivided by d t comma v equals d y divided by d t. I’x, I’y are 
image gradients. This is a function approximation utilize only the ﬁrst- order Taylor’s 
expansion since it is crucial that we presume that sections of higher-order Taylor 
series are unimportant.The Lucas-Kanade [9] approach has been implemented using 
the function calcOpticalFlowPyrLK() [11] in this case. 
3.4 
Pre-Processing 
The obtained trajectory is converted from BGR to grayscale image removing all 
color information. The resultant grayscale image is converted into binary image by 
thresholding. The image is then dilated where pixels are added to the boundaries of 
the image. Finally, an edge detector function canny is used. The Fig. 3. shows  the  
resultant image after each process. 
3.5 
Dense – Net 121 
CNN [8] is a type of artiﬁcial neural network that uses both time series and image 
as data. Due to this CNN is used in image recognition, pattern matching and image 
classiﬁcation application. CNN consists of three layers. 1. Convolutional layer 2. 
Pooling layer 3. Fully connected layer. Every Convolutional layer is followed by

Real Time Air-Writing and Recognition of Tamil Alphabets Using Deep …
141
Fig. 4 Dense net 121 architecture 
a ﬁlter or kernel which extracts the features. The complexity of feature extrac-
tion increases after each convolutional layer. The size of the data after convolution 
increases due to zero padding. The pooling layer reduces the size of the data. Finally, 
all the information from the layers is used in the fully connected layer for image 
classiﬁcation. 
The Fig. 4. shows the architecture of Dense Net 121. The 121 says the number 
of layers in the model. The model consists of four dense blocks. The input is passed 
into a convolutional layer where a ﬁlter of size 2*2 and stride of 2 is used. This is 
then passed into pooling layer of size 3*3 with stride of 2. Each dense block has 
different number of convolutional layers. The ﬁrst dense block has 6 convolutional 
layers and the second block has 12 convolutional layers and third dense block has 24 
such layers and the last dense block has 16 convolutional layers. After dense block 
there are transition layer. There are three transition layers in total. One convolutional 
layer and an average pooling layer are present in each transition layer. Inside the 
dense block, each layer is connected to the next. On top of the existing feature maps, 
each layer adds some new features. The features are added by concatenation and in 
order to perform concatenation the feature map that are being concatenated should 
be of same size. Because of this reason down mapping is done between dense layers 
and not within dense layer. The transition layer takes care of down sampling. After 
passing through every dense block the ﬁnal output is passed to pooling layer and 
ﬁnally the fully connected layer predicts the output. 
4 
Results and Discussion 
4.1 
Testing and Training 
The trained model obtained a maximum accuracy of 98.2% and maximum valida-
tion accuracy of 91.83%. The model obtained minimum training loss of 6.35% and 
minimum validation loss of 21.04%. The graph obtained for accuracy and loss has

142
S. Preethi et al.
Fig. 5 Accuracy graph 
Fig. 6 Loss graph 
been given below in the ﬁgure. The model produces the expected results. The model 
properly maps the trajectory that is being written in the air and recognizes them with 
good efﬁciency. The below Fig. 5 shows the accuracy graph and Fig. 6 shows the 
loss graph. The graphs show that there is increase in accuracy and decrease in loss. 
4.2 
Confusion Matrix 
A confusion matrix is used to describe how well a classiﬁcation system performs. 
The output of a classiﬁcation algorithm is shown and summarized in a confusion 
matrix. On the below given Fig. 7 it clearly shows that it has plotted a graph for 11 
classes starting from ﬁrst (uyir eluthukal) character to the last character. As mentioned 
above each character is classiﬁed as class. In the below given graph value from top to 
bottom is true value and value from left to right is predicted value. As shown in the 
graph for the percentage of prediction is marked on scale of 70. For character
it 
says that 54/70 can be character
and 2/70 can be character
. For character 
it says that 63/70 can be character
, 5/70 can be character
, 1/70 can 
be character
and 1/70 can be character
. For character
it says that 
36/70 can be character
and 1/70 can be character
. For character
it 
says that 56/70 can be character
and 1/70 can be character
. For character 
it says that 56/70 can be character
. For character
it says that 48/70 
can be character
1/70 can be character
and 1/70 can be character
. For

Real Time Air-Writing and Recognition of Tamil Alphabets Using Deep …
143
Fig. 7 Confusion matrix 
character
it says that 49/70 can be character
and 4/70 can be character
. 
For character
it says that 47/70 can be character
and 4/70 can be character 
. For character
it says that 52/70 can be character
and 2/70 can be 
character
. For character
it says that 46/70 can be character
and 7/70 
can be character
. For character
it says that 44/70 can be character 
and 18/70 can be character
. 
4.3 
Output 
Four command keys are used in the model. The ‘w’ key is to start writing. The 
‘n’ key is to clear the window. The ‘q’ key is to pause writing. The ‘a’ key is to 
start prediction. The equivalent phonetics of Tamil alphabet is given as output. For 
example, for the character
the phonetic equivalent ‘a’ is the output. The Figs. 8, 
9, 10, 11, 12, 13, 14, 15, 16, 17 and 18 shows the predicted results. 
Fig. 8 Character detection 
Fig. 9 Character detection

144
S. Preethi et al.
Fig. 10 Character detection 
Fig. 11 Character detection 
Fig. 12 Character detection 
Fig. 13 Character detection 
Fig. 14 Character detection 
Fig. 15 Character detection 
Fig. 16 Character detection

Real Time Air-Writing and Recognition of Tamil Alphabets Using Deep …
145
Fig. 17 Character detection 
Fig. 18 Character detection 
5 
Conclusion 
Air writing has been achieved by using Optical Flow in Open CV which mapped 
the trajectory of the character that has been written in the free space. The resultant 
image has been gray scaled and preprocessed. The resultant image along with the 
pre trained HP dataset has been given to the Dense Net 121, a CNN model which 
gave the predicted output for the 11 Tamil vowels. The model obtained a maximum 
training and validation accuracy of 98.2% and 91.83% respectively with minimum 
training and validation loss of 6.35% and 21.04% respectively. 
References 
1. Alam MdS, Kwon K-C, Alam MdA, Abbass MY, Imtiaz SM, Kim N (2020) Trajectory-based 
air-writing recognition using deep neural network and depth sensor. Sensors 20:376 
2. Vaidya, V., Pravanth, T., Viji, D.: Air writing recognition application for dyslexic people. In: 
2022 International Mobile and Embedded Technology Conference (MECON), pp. 553–558, 
Noida, India (2022). 
3. Chen, M., AlRegib, G., Juang, B.-H.: Air-writing recognition—part ı: modeling and recognition 
of characters, words, and connecting motions. IEEE Trans. Human-Machine Systems 46(3), 
403–413 (2016) 
4. Chen, M., AlRegib, G., Juang, B.-H.: Air-writing recognition—part ıı: detection and recog-
nition of writing activity in continuous stream of motion data. IEEE Trans. Human-Machine 
Syst. 46(3), 436–444 (2016) 
5. Baig, F., Fahad Khan, M., Beg, S.: Text writing in the air. J Information Display 14(4), 137–148 
(2013) 
6. Shah STH, Xuezhi X (2021) Traditional and modern strategies for optical ﬂow: an investigation. 
SN Appl. Sci. 3:289 
7. Ranganathan G (2021) A study to ﬁnd facts behind preprocessing on deep learning algorithms. 
J Innov Image Processing (JIIP) 3(01):66–74 
8. Bhatt D, Patel C, Talsania H, Patel J, Vaghela R, Pandya S, Modi K, Ghayvat H (2021) CNN 
variants for computer vision: history, architecture, application. Challenges and Future Scope. 
Electronics. 10:2470

146
S. Preethi et al.
9. Fernando, W.S.P., Udawatta, L., Pathirana, P.: Identiﬁcation of moving obstacles with Pyra-
midal Lucas Kanade optical ﬂow and k means clustering.Third International Conference on 
Information and Automation for Sustainability, Melbourne, VIC, Australia, pp. 111–117 (2007) 
10. Mohammadi S, Maleki R (2019) Real-time Kinect-based air-writing system with a novel 
analytical classiﬁer. IJDAR 22:113–125 
11. Optical ﬂow – Open CV. https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html

A Fuzzy Logic Based Trust Evaluation 
Model for IoT 
Rabindra Patel and Sasmita Acharya 
Abstract Internet of Things (IoT) is a way of connecting the physical world to the 
internet where various devices are capable of communicating with each other. They 
all need a secure environment but the problem is implementation of any security 
approach for an IoT node is very difﬁcult as resources of IoT nodes are very limited. 
So, trust evaluation and trust assessment are very important for IoT nodes. The paper 
proposes a fuzzy logic based trust evaluation model for IoT that employs different 
trust factors like End-to-end Packet Forwarding Ratio (EPFR), Amount of Energy 
Conversion (AEC), Packet Delivery Ratio (PDR) and Security Grade (SG) in order 
to construct a Fuzzy Inference System (FIS) that can calculate the trust value of each 
IoT node. Based on the resultant trust value, the IoT nodes are classiﬁed into three 
categories: Not Trustworthy, Not Sure and Trustworthy. An IoT node which belongs 
to Trustworthy only gets the access to forward the data packets or communicate with 
other IoT nodes. The Not Trustworthy and Not Sure IoT nodes are set to sleep mode 
for power conservation. 
Keywords Fuzzy inference system · Wireless sensor network · Internet of 
Things · Packet delivery ratio · Packet forwarding ratio · Trust 
1 
Introduction 
“Internet of Things (IoT)” is a collection of multiple devices which can communicate 
with each other with the help of internet and form a well-structured network where 
each node is independent and decentralized. It is a way to connect the physical 
world to the internet. Every node has a sensing unit, an actuator and RFID readers 
which can sense the environment and then the node need to send the data for further 
processing. To do the further processing like packet forwarding, routing and network 
management etc., all the nodes need to work collaboratively with Wireless Sensor 
Network (WSN). This type of computing is called ubiquitous computing and also
R. Patel envelope symbol · S. Acharya 
Veer Surendra Sai University of Technology, Burla, India 
e-mail: rabindrapatel329@gmail.com 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_14 
147

148
R. Patel and S. Acharya
internet connection is needed to connect the entire WSN. So, the network service 
can be available at global scale. 
This type of IoT where nodes are heterogeneous devices but need ubiqui-
tous computing increases the probability of node misbehavior. The possibilities of 
challenges are as listed below: 
1. As nodes are wireless, so it is likely that any IoT node can be entrapped by an 
opponent and then that node may work as a malicious node. 
2. If some nodes get damaged, it will create problem for the entire network and 
work negatively by partitioning the network into a number of disjoint segments. 
3. As the nodes are poor in performance and conﬁguration, it is difﬁcult to 
implement the algorithms for network security and data forwarding. 
Among the above mentioned challenges many other issues can affect the overall 
performance of IoT devices. So, a proper trust evaluation model for the network is 
compulsorily required that can calculate the trust of each node in regular manner. 
The calculation and evaluation of trust makes the communication safe and healthy 
among the IoT nodes. Now each node can easily decide where the data can be secured 
further and their decision will not hamper the IoT network thus resulting in a very 
reliable and robust network. 
The paper proposes a trust evaluation model for IoT devices that is based on 
fuzzy logic to calculate Trustworthy of each node by considering four factors. They 
are “End-to-end Packet Forwarding Ratio (EPFR)”, “Amount of Energy Conversion 
(AEC)”, “Packet Delivery Ratio (PDR)” and “Security Grade (SG)”. 
The rest of the paper is organized as: the work performed by other researchers 
is outlined in Sect. 2, Sect. 3 elaborates on the proposed fuzzy logic based trust 
evaluation model for IoT. Section 4 presents and analyzes the simulation results. At 
the end, Sect. 5 presents the conclusion. 
2 
Related Work 
In paper [1] it gives emphasis on trust and reputation of the node. It tries to describe the 
relation between trust and reputation and also tries to show the cooperation amongst 
the nodes based on trust and reputation. Paper [2] presents a fuzzy inference system 
to ﬁnd out the trust result of each node in a WSN and based on the result packet 
forwarding among the nodes is carried out. Paper [3] also presents a fuzzy logic model 
for evaluation of security trust of each node on the basis of different input parameters. 
Paper [4] notices the memory constraints of IoT devices and builds up a concept of 
memory proﬁle for each IoT agent and that proﬁle utility. Paper [5] presents a fuzzy 
way to “Trust Based Access Control (FTBAC)”. Paper [6] proposes a new ﬁtness 
function for subsequent node selections on the basis of some parameters. Paper [7] 
presents a “Systematic Literature Review (SLR)” on a list of evaluation techniques to 
calculate the trust. Paper [8] ensures the safety of IoT devices on the basis of block-
chain technology. Paper [9] provides a fuzzy logic based Trust—“ABAC model” for

A Fuzzy Logic Based Trust Evaluation Model for IoT
149
accessing the control of the proposed IoT model. Paper [10] proposes two algorithms 
as “Simple Multi-Attribute Rating Technique (SMART)” and “Long Short-Term 
Memory (LSTM)” for trust management of the proposed IoT model. Paper [11] 
describes a deep survey on blockchain based trust management in IoT by giving 
emphasis on the list of evaluation criteria, issues and suggestions towards scope 
in future research. Paper [12] provides a brief description about different factors, 
deﬁnitions, characteristics, classiﬁcations, architectures, challenges and directions 
of trust and trust model to evaluate trust in an IoT environment. 
3 
Proposed Fuzzy Logic Based Trust Evaluation Model 
The paper proposes a fuzzy logic based trust evaluation model that calculates the 
trust value for each IoT node by considering different parameters related to trust. 
Trust has different deﬁnitions some of which are outlined below. 
Deﬁnition 1: “In a wireless network of IoT, a node P’s trust in another node Q is 
the subjective of node P which receives positive outcomes through the transactions 
with node Q” [1]. 
Deﬁnition 2: “Trust is the subjective probability by which an individual X, expect 
that another individual, Y, performs a given action on which its welfare depends” 
[13]. 
Deﬁnition 3: “In real life communities, Trust is the consequence of the satisfaction 
of certain desired properties” [14]. 
By considering the above deﬁnitions, the authors have taken four parameters as 
fuzzy input variable, added the respective membership functions, set a rule base, given 
proper speciﬁcation for output and these all jointly constituted a “Fuzzy Inference 
System (FIS)” to evaluate the trust of each IoT node. Here each node applied a 
neighbor monitoring process for the purpose of collecting information about the 
forwarding behavior of neighbor node. Based on these observations, the following 
factors were considered. 
• “End-to-end Packet Forwarding Ratio (EPFR)”: EPFR is a ratio that is calcu-
lated by considering the total number of packets received by the application layer 
of receiver node with respect to the total number of packets forwarded by the 
application layer of the source node. So, the value of EPFR can be evaluated as, 
u p pe r E 
up
er  P upper F upper R equals StartFraction sigma summation Underscript normal i Overscript normal k Endscripts upper R upper E upper C upper V Subscript i Baseline Over sigma summation Underscript i Overscript n Endscripts upper S upper E upper N upper D Subscript i Baseline EndFraction comma 0 less than or equals normal k less than or equals normal n
up
er  E up
er P  up er 
F u
where RECVi and SENDi denote the received data packet and send data packet 
by ith node of destination and the ith node of the source respectively. And k

150
R. Patel and S. Acharya
represents the successful receiving times, whereas n denotes the total number of 
packets forwarded. 
• “Amount of Energy Conversion (AEC)”: AEC is helpful to measure the total 
amount of energy consumed by an IoT network. Here the equation for energy 
consumption is represented as, 
uppe r
 A 
uppe r E upper C equals StartFraction sigma summation Underscript k equals 1 Overscript n Endscripts c o n s u m e Subscript k Baseline Over sigma summation Underscript k equals 1 Overscript n Endscripts s e n d Subscript k Baseline plus r e c v Subscript k Baseline plus tau EndFraction
up
er A  upper  E upper  C e
quals StartFraction sigma summation Underscript k equals 1 Overscript n Endscripts c o n s u m e Subscript k Baseline Over sigma summation Underscript k equals 1 Overscript n Endscripts s e n d Subscript k Baseline plus r e c v Subscript k Baseline plus tau EndFraction
where sendk and recvk represent the sending and receiving end energy consump-
tion by kth node while sending and receiving message. Consumek represents the 
overall energy consumption cost of the kth node and taudenotes the consumption 
of energy used for normal running and maintenance of the node. 
• “Packet Delivery Ratio (PDR)”: PDR is hampered by the packet loss and packet 
retransmission. Because of many reasons packet loss can happen. Out of those 
here we are considering only the middle node behavior that instead of forwarding 
the received data packet to the next hop node, the gained packets are knowingly 
thrown down. 
• “Security Grade (SG)”: It can be explained with reference to its application 
environment such as in battleﬁeld area, emergency situation, environmental and 
wild life monitoring etc. The value of SG = 1, when the requirement of security 
is high, SG = 0.7, when the security requirement is medium, SG = 0.4, when the 
security requirement is very low and SG = 0.2, when the security requirement is 
low. 
These four factors are used in the fuzzy based trust evaluation model as input 
parameter and according to the rule base the fuzzy model will output the trust value 
which is further categorized into three types: “Not Trustworthy (NT)”, “Not Sure 
(NS)”, and “Trustworthy (T)”. 
3.1 
Proposed Fuzzy Inference System 
A fuzzy model consists of the following four steps: 
• Fuzziﬁcation: 
Here the general input crisp values (0 and 1) are converted into respective fuzzy 
input value. For this conversion of crisp value to fuzzy value a fuzziﬁcation unit 
is used. 
• Membership Functions: 
This part is used to map the “crisp value” to suitable “linguistic variable” and 
vice-versa. It is needed in both the fuzziﬁcation and defuzziﬁcation unit. 
• Fuzzy Rule Base:

A Fuzzy Logic Based Trust Evaluation Model for IoT
151
It is a set of “IF-THEN” rules on the basis of requirement of the model. This rule 
base acts as the core of the model that helps to generate the desired output from 
the given input. 
• Defuzziﬁcation: 
It is opposite to the fuzziﬁcation process. This process again converts the fuzzy 
output result to crisp output. 
The various fuzzy input and output parameters to evaluate the trust, the linguistic 
variables, and different ranges of input and output variables are provided in tabular 
form in Table 1. 
In this proposed trust evaluation model, the input parameters are taken as “End-
to-end Packet Forwarding Ratio (EPFR)”, “Amount of Energy consumption (AEC)”, 
“Packet Delivery Ratio (PDR)” and “Security Grade (SG)”. Here, we have considered 
four “linguistic variables” for the fuzzy input parameters as “Very Low (VL)”, “ Low 
(L)”, “Medium (M)” and “High (H)”. Also, “Not Trustworthy (NT)”, “Not Sure (NS)” 
and “Trustworthy (T)”are taken as linguistic variables to evaluate the trust value as 
output. The proposed model is based on “Mamdani Fuzzy Inference System” and 
uses trapezoidal membership function for input variables and triangular membership 
function for output variable. The ﬁgures of fuzzy membership function for different 
input and output parameters are depicted in Figs. 1, 2, 3, 4 and 5.
Table 1 Input parameters and their range for the proposed Fuzzy Inference System 
Parameter
Type
Linguistic variable
Range
Membership function 
End-to-end packet 
forwarding ratio 
(EPFR) 
Input
“Very Low(VL)” 
“Low (L)” 
“Medium (M)” 
“High (H)” 
[0–0.31] 
[0.2–0.45] 
[0.40–0.71] 
[0.66–1] 
Trapezoidal 
Amount of energy 
consumption (AEC) 
Input
“Very Low (VL)” 
“Low (L)” 
“Medium (M)” 
“High (H)” 
[0–0.31] 
[0.2–0.45] 
[0.40–0.71] 
[0.66–1] 
Trapezoidal 
Packet delivery ratio 
(PDR) 
Input
“Very Low (VL)” 
“Low (L)” 
“Medium (M)” 
“High (H)” 
[0–0.31] 
[0.2–0.45] 
[0.40–0.71] 
[0.66–1] 
Trapezoidal 
Security grade (SG)
Input
“Very Low (VL)” 
“Low (L)” 
“Medium (M)” 
“High (H)” 
[0–0.31] 
[0.2–0.45] 
[0.40–0.71] 
[0.66–1] 
Trapezoidal 
Trust value (Trust)
Output
“Not Trustworthy 
(NT)” 
“Not Sure (NS)” 
“Trustworthy (T)” 
[0–0.4] 
[0.36–0.70] 
[0.66–1] 
Triangular 

152
R. Patel and S. Acharya
Fig. 1 EPFR membership function 
Fig. 2 AEC membership function 
Fig. 3 PDR membership function 
Fig. 4 SG membership function

A Fuzzy Logic Based Trust Evaluation Model for IoT
153
Fig. 5 Trust value membership function 
4 
Results of Simulation 
4.1 
Simulation Domain 
To build up the simulation of the proposed fuzzy inference model, here “MATLAB 
(Version-2014a)” and Simulink were taken. Also, three things were given emphasis 
in the proposed IoT model to get the satisfactory result. Firstly, all the static base 
station needs to be considered. Secondly, a “Threshold Trust Value (TTV)” was taken 
as 0.66 for the whole simulation model. If the result of the simulation is greater than 
TTV then the model is considered to be trustworthy. Third, some faulty nodes were 
used to evaluate the trustworthiness of the proposed fuzzy model. The simulation 
parameters are listed in Table 2. 
The proposed model was built using “Mamdani based fuzzy inference system” 
with four fuzzy inputs, a rule base of twenty six rules and one fuzzy output. Then the 
model was simulated in Simulink which is a graphical way to represent the model 
and also provides an easy interface to understand the model.
Table 2 Simulation 
parameters
Parameter
Value 
“Size of network”
300 * 300 
“Location of base station”
(40, 40)m 
“No. of IOT nodes”
120 
“Simulation time”
550 s 
“Transmission energy”
0.75 w 
“Reception energy”
0.43 w 
“Packet size”
1 kb  

154
R. Patel and S. Acharya
4.2 
Simulation Results and Analysis 
This section presents the simulation results in Simulink and their analysis for the 
proposed system. 
Here in Fig. 6, the Simulink result is 0.8222, which is falling under the range of 
“Trustworthy” (0.66–1.0). It means that the IoT node is highly trustable and data can 
be forwarded through this node. 
In Fig. 7, the Trust Value output generated for the given input parameters is 0.5. 
This result is falling under the range of “Not Sure” (0.36–0.70). So, as it is not fully 
trustable, the IoT node is not allowed to forward its data. 
The Fig. 8 shows the output result of the FIS as 0.2 which is very poor which results 
in accordance with the trust. This value comes under the range of “Not Trustworthy”
Fig. 6 Simulation result as trustworthy of proposed FIS in simulink 
Fig. 7 Simulation Result as Not Sure of Proposed FIS in Simulink 

A Fuzzy Logic Based Trust Evaluation Model for IoT
155
Fig. 8 Simulation result as not trustworthy of proposed FIS in simulink 
(0–0.4) that directly shows that this IoT node is not trustworthy and should not be 
used for data forwarding. 
The result of the Simulink depends on the input parameter used, type of member-
ship function used and the set of rules mentioned in rule base. In the proposed FIS 
where trust is the matter of concern, out of the above mentioned factors, the trust 
result may be categorized into 3 types, i.e., “Trustworthy”, “Not Sure” and “Not 
Trustworthy”. Among these three categories the proposed model will permit only 
that IoT node to forward the data packet whose FIS trust result is falling under the 
range of “Trustworthy”. The remaining IoT nodes whose results are under the range 
of “Not Sure” and “Not Trustworthy” are not allowed to participate in any type 
of data transmission. So, to conserve energy, it is recommended to switch off the 
transceivers of the “Not Sure” and “Not Trustworthy” category IoT nodes and set 
these category IoT nodes to sleep mode. 
5 
Conclusion and Future Scope 
A fuzzy logic based inference system was proposed to evaluate the trust factor of the 
nodes in an IoT environment because the implementation of any security algorithm 
is tedious with low performance IoT nodes. The proposed model calculates the value 
of resultant trust of each and every IoT node in the network and gives the permission 
to forward the data packet to those, which have result trust value that clear the barrier 
of “Threshold Trust Value (TTV)”. Also, a non–participating node is set to sleep 
mode. In this way a trustful environment of IoT is created where each IoT node 
can trust other available IoT nodes and independently communicate with each other 
without any security concern. This work may be further enhanced in future by using 
a neuro-fuzzy based estimator to improve the prediction of faulty IoT nodes so as to 
improve on the reliability of an IoT network.

156
R. Patel and S. Acharya
References 
1. Chen D, Chang G, Sun D, Li J, Jia J, Wang X (2011) TRM-IoT: a trust management model 
based on fuzzy reputation for Internet of Things. Comput Sci Inf Syst 8:1207–1228. https:// 
doi.org/10.2298/CSIS110303056C 
2. Patra M, Acharya S (2022) A fuzzy logic-based trust management scheme for wireless sensor 
network. https://doi.org/10.1007/978-981-19-1018-0_14 
3. Khalil A, Mbarek N, Togni, O (2019) Fuzzy logic based security trust evaluation for IoT 
environments. In: 2019 IEEE/ACS 16th international conference on computer systems and 
applications (AICCSA), pp 1–8. https://doi.org/10.1109/AICCSA47632.2019.9035294 
4. Bradbury M, Jhumka A, Watson T (2022) Information management for trust computation on 
resource-constrained IoT devices. Future Gener Comput Syst 135:348–363. ISSN 0167-739X. 
https://doi.org/10.1016/j.future.2022.05.004 
5. Mahalle PN, Thakre PA, Prasad NR, Prasad R (2013) A fuzzy approach to trust based access 
control in internet of things. Wirel VITAE 2013:1–5. https://doi.org/10.1109/VITAE.2013.661 
7083 
6. Seyyedabbasi A, Kiani F, Allahviranloo T, Fernandez-Gamiz, U, Noeiaghdam S (2023) Optimal 
data transmission and pathﬁnding for WSN and decentralized IoT systems using I-GWO and 
Ex-GWO algorithms. Alex Eng J 63:339–357. ISSN 1110-0168. https://doi.org/10.1016/j.aej. 
2022.08.009 
7. Mohammadi V, Rahmani AM, Darwesh A, Sahaﬁ A (2019) Trust-based recommendation 
systems in Internet of Things: a systematic literature review. Hum Centric Comput Inf Sci 9:1– 
61 
8. Singh AK, Kushwaha N (2021) Software and hardware security of IoT. In: 2021 IEEE interna-
tional IOT, electronics and mechatronics conference (IEMTRONICS), pp 1–5. https://doi.org/ 
10.1109/IEMTRONICS52119.2021.9422651 
9. Ouechtati H, Ben Azzouna, N, Ben Said, L (2020) A fuzzy logic based trust-ABAC model for 
the Internet of Things. https://doi.org/10.1007/978-3-030-15032-7_97 
10. Alghofaili Y, Rassam M (2022) A trust management model for IoT devices and services based 
on the multi-criteria decision-making approach and deep long short-term memory technique. 
Sensors 22:634. https://doi.org/10.3390/s22020634 
11. Liu Y, Wang J, Yan Z, Wan Z, Jantti R (2023) A survey on blockchain-based trust management 
for Internet of Things. IEEE Internet Things J 1. https://doi.org/10.1109/JIOT.2023.3237893 
12. Alduais N (2023) Trust evaluation model in IoT environment: a comprehensive survey. IEEE 
Access 11:11165–11182. https://doi.org/10.1109/ACCESS.2023.3240990 
13. Gambetta T (1990) Can we trust trust? In: Gambetta D (ed) Trust: making and breaking 
cooperative relations. Basil Blackwell, Oxford, pp 213–238 
14. Better business Bureau. http://www.bbb.org

Supervised Learning Approaches 
on the Prediction of Diabetic Disease 
in Healthcare 
Riyam Patel, Borra Sivaiah, Punyaban Patel, and Bibhudatta Sahoo 
Abstract There are many chronic diseases out of which Diabetes is one; that 
increases sugar level in the blood and is one of the most fatal that effect different 
organs in the human body. Diabetes can cause a variety of slow bad consequences 
if not detected and left without given medical care. The emergence of machine 
learning approaches, on the other hand, solves this crucial issue. The purpose and 
objectives of this work is to build a prototypical model that can properly forecast 
diabetes whether or not a person will suffer from it. To detect diabetes at an early 
stage, our work employs three classiﬁcation algorithms based on supervised learning: 
Random Forest, Naïve Bayes Classiﬁer and Multilayer Perceptron Network. The 
PIDD Database has been used in the experiments. The Precision, Accuracy, Recall, 
F-Measure, and ROC Area are all used to calculate the efﬁciency of the above three 
algorithms. The correctness and accuracy of a classiﬁcation system is measured by 
the number of occurrences that are correctly classiﬁed and those that are mistakenly 
classiﬁed. 
Keywords Precision · Accuracy · F-Measure · Recall · ROC area · Diabetics ·
Random Forest · Naïve Bayes · Multilayer perceptron · Supervised learning
R. Patel envelope symbol
Department of Computational Intelligence, College of Engineering and Technology, SRM 
Institute of Science and Technology (Formerly SRM University), Kattankulathur, Chennai, India 
e-mail: riyampatel2001@gmail.com 
B. Sivaiah 
Department of Computer Science and Engineering, CMR College of Engineering and Technology, 
Kandlakoya, Hyderabad, India 
P. Patel 
Department of Computer Science and Engineering, CMR Technical Campus, Kandlakoya, 
Hyderabad, India 
B. Sahoo 
Department of Computer Science and Engineering, National Institute of Technology, Rourkela, 
Odisha, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_15 
157

158
R. Patel et al.
1 
Introduction 
Now-a-days, the diabetes is a common health issues in human beings which threat-
ening the health, social and economic development in the family. Diabetes is a 
primary basis of death and shortening of life expectation. It is a lingering disease that 
occurs when enough insulin is not created in pancreas or the lack of insulin produced 
in the body. Insulin is a hormone that assists in the regulating of bloodsugar levels. 
Hyperglycemia, or high blood sugar, is caused by uncontrolled diabetes and causes 
long-term injury to different parts of the body, together with blood vessels and nerve 
cells. Also, now physicians are not able to know as to why this is happening, and have 
coined the term “x syndrome” to describe it. Historically, its treatment has primarily 
addressed the symptoms rather than the fundamental cause. On the basis of WHO 
Organization, around 5% of the world’s population has been affected by it, and the no. 
of patients is steadily rising [02]. According to the International Diabetes Federation 
(IDF), in 2017 worldwide there had been suffering diabetes around 451 million 
adults, with a predicted growth to 693 million by the year 2045 if no work efﬁciently 
and effective will be adopted [1]. Diabetics are divided into three categories: Type 2, 
Type 1, and Gestational diabetes. Type 2 diabetes is caused due to the excess body 
weight and physical inactivity and poor utilisation of insulin by the body. Type 2 
diabetes has become more common in recent years among those between the ages of 
30 and 40 [1]. Type 1 diabetes, which is insulin-dependent, is characterised by low 
insulin production and necessitates insulin injection daily basis. There is no known 
cause for Type 1 diabetes, nor is there a way to prevent it. Hyperglycemia during 
pregnancy occur when the blood glucose levels are higher than usual but not that 
much to be diagnosed as diabetes. 
Let it be discuused brieﬂy about the data mining technique related to the diabatic 
disease. Data mining technique is a stage/task in Knowledge Discovery in Databases 
(KDD)m where it extracts useful information from massive amount of data. It has 
signiﬁcant role in the healthcare business, enabling accurate disease prediction and 
deeper analysis of medical data. The authors employ a variety of data mining tech-
niques to identify brain stroke, heart disease, hypothyroidism, diabetes, and cancer 
type pf disease, among others. The major aim of this study is to use various data 
mining approaches to predict the existence of diabetes in people of all ages at an 
early stage which help people better understand about diabetes and diagnose it sooner 
thereby stopping its spread is critical. The use of data mining tool speeds up data 
processing, allowing analysts to analyse existing data to spot diabetes patterns and 
trends. Timely ﬁnding of diabetes can support to avoid the condition from progressing 
and reducing the risk of catastrophic consequences. Making the appropriate lifestyle 
adjustments at the right time can help prevent diabetes and all of the problems that 
come with it. Here, we have used PIMA Indian Diabetes dataset to predict diabetic 
complications. 
In medical data mining, classiﬁcation is one of the data analysis strategies has 
been employed. The classiﬁcation algorithm is a supervised learning method for 
categorising characteristics into different groups. Huge and massive medical data

Supervised Learning Approaches on the Prediction of Diabetic Disease ...
159
sets from patients at a hospital or health institution can be used to forecast future 
outcomes using this technique. The data mining software like WEKA tool is being 
applying with algorithms that allow users to analyse larger sets of medical data to 
uncover hidden patterns that can then be utilised to predict the condition of patients. 
The classiﬁcation technique provides insight into the patient’s details, allowing clin-
ical support to be provided through analysis. Medical data mining is useful for 
discovering hidden patterns that can be used for clinical diagnosis in any disease 
dataset [03]. Different WEKA classiﬁcation approaches have been used for various 
medical datasets; nevertheless, ﬁnding a superior classiﬁcation algorithm is prob-
lematic because it is difﬁcult to compare different classiﬁcations in different sets of 
data. 
The objective of research is to give information to a patient whether a patient 
suffering from diabetic or not. But it is limited to diabetic in healthcare and can be 
applied to multiple diseases. 
This paper has organised as the Sect. 1: Introduction, Sect. 2: Literature 
review, Sect. 3. Classiﬁcation Algorithms, Sect. 4: Performance measures, Sect. 5: 
Experimental Analysis, and ﬁnally Sect. 6: Concluded the paper. 
2 
Literature Review 
Many research papers are available related to the classiﬁcation of diabetic patients 
to non-diabetic patients. Few of them has been brieﬂy discussed. 
In [4], a diabetic illness is explored and diagnosed using data-mining tech-
niques. Human services frameworks utilise information mining with the help of a 
programmable device that can distinguish diabetes by assessing its infection intensity 
and predicting the best therapy. 
Prescient AI-based exhibits show life-sustaining chemical processes disruption 
and diabetic illness development as in [5]. This study investigates the link between 
diabetes and hazardous factors associated with the condition. The J48 and the Naive 
Bayes algorithms can be used to ﬁnd this disease. 
Various artiﬁcial intelligence and data mining strategies in diabetes research are 
discussed by Kavakiotis, I. et al. [6]. This paper examines many applications of artiﬁ-
cial intelligence (AI) and the data mining techniques in diabetes research, including 
forecasting, taking decisions, complications due to diabetes, and health insurance 
coverage. 
A photoplethysmogram-based logistic regression model has been suggested in 
[8] for diabetes categorization. The model was trained with 459 patient data points 
and tested with 128. Their method detected 552 non-diabetics with 92% accuracy. 
The proposed technique, on the other hand, is not compared to recent advanced 
techniques. 
A web based application has been created in [9] that successfully predicts diabetes 
using Tensorﬂow. For a successful diagnosis, this proposed method requires patient 
data, and techniques such as artiﬁcial neural network, support vector machine, Naive

160
R. Patel et al.
Bayes, and k-nearest neighbour are utilised to forecast the disease. The data is split 
into two sections: training and testing. Pre-processing and standardisation of data 
would improve the model’s accuracy. For increasing accuracy, the Min-Max Scaler 
normalisation model is utilised [9]. The author [10] employed genetic programming 
to assess and categorise diabetes patients based on previously stored information. 
Genetic programming has been used in conjunction with data mining to identify 
patients as non-diabetic, diabetic or pre-diabetic. The authors employed Decision 
Tree, SVM, and Nave Bayes classiﬁers to predict diabetics in [11]. 
For identifying diabetic patients, the authors [12] utilised SVM, Decision Tree, 
Random Forest, KNN, and Logistic Regression. A comparative analysis of diabetes 
classiﬁcation approaches was published where they [13] employed PIMA Indian data 
set. Support Vector Machine and Naive Bayes algorithms were utilised in the PIMA 
Indian Diabetes dataset for diabetes categorization [14]. A feature selection method 
and k-fold cross-validation were also used to improve the performance of the models. 
In the experiments, the support vector machine outperformed the naive Bayes model. 
While accuracy has been achieved, a current state-of-the-art comparison has been 
omitted. 
The authors [15] demonstrated a machine learning-based diabetic mellitus classi-
ﬁcation method which primarily used diabetes data from the University of California 
Repository and applied support vector machine. To identify diabetes, [17] investi-
gated Deep Neural Network, Logistic regression, Support Vector Classiﬁer, Naive 
Bayes, and Decision tree approaches. This work was completed in four stages: The 
best pre-processing for the classiﬁer is ﬁrst identiﬁed. The parameters are then ﬁne-
tuned. In the third phase, the accuracy of these methodologies is compared, and the 
importance of these traits is later examined. The age, Plasma glucose concentration, 
and the number of pregnancies were discovered to be more noteworthy [16, 17]. 
It has been developed a hybrid diabetes disease framework [18] for accurate 
prediction of diabetes disease where it has used K-mean, Least square support vector 
machine giving good and ﬁnd good result. The authors [19] has built a Framework 
that can estimate with maximum precision, probability of diabetes in Patients. This 
is why the authors are interested in early diabetes detection using Machine Learning 
techniques like Support Vector Machines, Decision Trees, and Naive Bayes classiﬁer. 
The authors [20] have examined several diabetes prediction models with the goal 
of solving the diabetes prediction problem by locating, analyzing, and combining 
data from the best available studies. Two-stage procedure is proposed by authors 
[21]. In the ﬁrst step, the major common risk variables for DM and CVD are inferred 
using two ML models (logistic regression and Evimp functions) implemented in a 
multivariate adaptive regression splines model, with the correlation matrix applied to 
cut down on redundancy. Second, the model is built with the help of a classiﬁcation 
and regression technique. The accuracy, sensitivity, and speciﬁcity of the models’ 
predictions were used as measures of their efﬁcacy. The authors [22] have analyses 
many machine learning classiﬁer techniques for predicting Type 2 Diabetes Mellitus 
Disease. 
The authors [23], Hepatic steatosis is best evaluated ﬁrst using ultrasound. 
Ultrasound radiofrequency (RF) signals are affected by fatty droplets in the liver

Supervised Learning Approaches on the Prediction of Diabetic Disease ...
161
parenchyma. Ultrasound parametric imaging of hepatic steatosis and ﬁbrosis is 
proposed in this article using sample entropy which a measure of irregularity in 
time-series data governed by the dimension and tolerance. 
Attenuation imaging performance and inter-observer variability were compared 
to the hepatorenal index, a liver fat measuring approach [24]. (HRI). Two observers 
examined attenuation coefﬁcients (AC) in a phantom, 20 healthy volunteers, and 27 
patients scheduled for biopsy for probable widespread liver disease. 
3 
Classiﬁcation Algorithms 
The two most critical tasks in data mining are classiﬁcation and prediction. In data 
mining, several classiﬁer algorithms are available, however this work focuses on the 
Naïve Bayes classiﬁer, Random forest classiﬁers, and Multilayer perceptron. 
(a) Naive Bayes Classiﬁer 
Nave Bayes is a supervised learning method based on Bayes’ theorem that solves 
classiﬁcation issues. It’s probabilistic, therefore it forecasts based on an object’s 
probability. It implies a class’s features are unrelated. The Naïve Bayes can be deﬁned 
as 
upper P  l eft parenthesis h divided by upper D right parenthesis equals StartFraction upper P left parenthesis upper D divided by h right parenthesis upper P left parenthesis h right parenthesis Over upper P left parenthesis upper D right parenthesis EndFraction
upper
 P left parenthesis h divided by upper D right parenthesis equals StartFraction upper P left parenthesis upper D divided by h right parenthesis upper P left parenthesis h right parenthesis Over upper P left parenthesis upper D right parenthesis EndFraction
where, upper P left parenthesis upper D divided by h right parenthesis = probability of upper D given h, upper P left parenthesis h divided by upper D right parenthesis = probability of h given upper D, 
upper P left parenthesis h right parenthesis = prior probability of hypothesis h, upper P left parenthesis upper D right parenthesis = prior probability of training data 
upper D. The output of the Naive Bayes model is presented in Table 1. 
(b) Random Forest Classiﬁer 
Decision trees are the building blocks of a random forest algorithm. It is made up 
of three parts: leaf nodes, decision nodes, and a root node, which divides a training 
dataset into branches and then separates them into various branches. This pattern 
repeats till reached to leaf node. The attributes of the PIMA diabetic data set are 
utilised to estimate and the decision tree nodes are assigned according to the outcomes 
values. The classiﬁcation based on Random forest uses an aggregate methodology to 
get the desirable outcomes. During training, multiple decision trees are trained using 
the training data.
Table 1 Output of the Naive Bayes model 
Total Number of Instances: 768 
Incorrectly Classiﬁed Instances: 182 (23.6979%) 
Correctly Classiﬁed Instances: 586 (76.3021%) 
Kappa Statistic: 0.4664 
Mean Absolute Error: 0.2841 
Relative Absolute Error: 62.5028% 
Root Relative Squared Error: 87.4349% 
Root Mean Squared Error: 0.4168 

162
R. Patel et al.
When nodes are divided, features and observation from this dataset are randomly 
selected. It is decided by a majority vote what will be the ﬁnal product. In this case, 
the ultimate output of the random forest system is the output chosen by the majority 
of the decision trees in the system. Random Forest works in two-phase. It builds the 
random forest through the ﬁrst phase by combining N number of decision trees, and 
then makes forecasts for each tree formed in the ﬁrst phase in the second phase. The 
Working process of random forest is as below: 
Working process/procedure: 
i. Randomly select M data points from the training data set. 
ii. Construct the decision tree using those selected M data points. 
iii. Decide the number P for constructing the trees. 
iv. 
Repeat Step i & ii 
v. 
Find each decision tree’s predictions for new data items and assign those new 
data points to the most-voted category. 
The output of random forest classiﬁer is shown in Table 2. 
(c) Multi-layer Perceptron 
The Weka tool has been used to create multilayer perceptron (MLP) network model 
is shown in the Fig. 1. It contains 8, 3, 2 neurons corresponding to input layer, hidden 
layer and output layer respectively for the PIMA diabetic data set. The output of the 
neural network is taken as tested negative and tested positive. Here, the learning 
parameter, momentum and no. of epoch are 0.3, 0.2 and 500 respectively.
The output of the Multilayer Perceptron network model is shown in Table 3.
Table 2 Random forest classiﬁer 
Total Number of Instances (TNI): 768 
Correctly Classiﬁed Instances (CCI): 582 
(75.7813%) 
Incorrectly Classiﬁed Instances (ICI): 186 
(24.2188%) 
Kappa Statistic (KS): 0.4479 
Mean Absolute Error (MAE): 0.3117 
Root Mean Squared Error (RMSE): 0.3966 
Relative Absolute Error (RAE): 68.5757% 
Root Relative Squared Error (RRSE): 
83.2097% 

Supervised Learning Approaches on the Prediction of Diabetic Disease ...
163
Fig. 1 Multilayer perceptron (MLP) neural network
Table 3 Output of the multilayer perceptron model 
Total Number of Instances (TNI): 768 
Correctly Classiﬁed Instances (CCI): 579 
(75.3906%) 
Incorrectly Classiﬁed Instances (ICI): 189 
(24.6094%) 
Kappa Statistic (KS): 0.4484 
Mean Absolute Error (MAE): 0.2955 
Root Mean Squared Error (RMSE): 0.4215 
Relative Absolute Error (RAE): 65.0135% 
Root Relative Squared Error (RRSE): 
88.4274% 
4 
Performance Measures 
There are different types of metrics used in this paper for prediction of diabetics that 
has brieﬂy explained below Given a vector M
odifyingAbove y With caret of upper N predictions, ModifyingAbove y With quotation dash as the mean and 
the vector y of upper N actual observed target values of diabetic data. 
(i) Mean Square Error (MSE) and Mean Absolute Error (MAE) 
In statistics, the mean squared error (MSE) of an estimator is one of several 
methods for calculating how far an estimator deviates from the true value of 
the quantity being estimated. 
uppe r M upper S upper E equals StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared
up
p
e
r M 
upp
e r S uppe
r E equals StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared
The Mean Absolute Error (MAE) can be deﬁned as:

164
R. Patel et al.
u pp e r M upper A upper E equals StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline EndAbsoluteValue
up
p
e
r M
 upp
e r A 
up
per E equals StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline EndAbsoluteValue
(ii) Root Mean Squared Error (RMSE): It is called the root of the MSE. 
upper R upper M upper S upper E equals StartRoot upper M upper S upper E EndRoot equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared EndRoot
√
uppe r R upper M upper S upper E equals StartRoot upper M upper S upper E EndRoot equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared EndRoot
|
|
|
| upper R upper M upper S upper E equals StartRoot upper M upper S upper E EndRoot equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared EndRoot
up
p
e
r R 
upp
e r M uppe
r S upper E equals StartRoot upper M upper S upper E EndRoot equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared EndRoot
(iii) Relative Absolute Error (RAE): 
The relative absolute error can be calculated as 
uppe r
 R 
upp
er A
 uppe
r E equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline EndAbsoluteValue Over sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y With quotation dash minus y Subscript i Baseline EndAbsoluteValue EndFraction
up
er R upper A upper E equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline EndAbsoluteValue Over sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y With quotation dash minus y Subscript i Baseline EndAbsoluteValue EndFractionup per R
 upper A upper E equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline EndAbsoluteValue Over sigma summation Underscript i equals 1 Overscript upper N Endscripts StartAbsoluteValue ModifyingAbove y With quotation dash minus y Subscript i Baseline EndAbsoluteValue EndFraction
(iv) Root Relative Squared Error (RRSE): The root relative absolute error can be 
calculated as 
upper  
R u
ppe
r
 R
 uppe
r A upper E equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared Over sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y With quotation dash minus y Subscript i Baseline right parenthesis squared EndFraction
up
er R  upper R upper A upper E equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared Over sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y With quotation dash minus y Subscript i Baseline right parenthesis squared EndFractionup per R up
per R upper A upper E equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y Subscript i Baseline With caret minus y Subscript i Baseline right parenthesis squared Over sigma summation Underscript i equals 1 Overscript upper N Endscripts left parenthesis ModifyingAbove y With quotation dash minus y Subscript i Baseline right parenthesis squared EndFraction
(v) Accuracy (A): It determines the algorithm’s accuracy in predicting instances. 
Mathematically, 
no r
mal upper A equals StartFraction left parenthesis upper T upper P plus upper T upper N right parenthesis Over left parenthesis normal upper T normal o normal t normal a normal l normal n normal o normal o normal f normal s normal a normal m normal p normal l normal e normal s right parenthesis EndFraction
normal upp er A equals 
StartFraction left parenthesis upper T upper P plus upper T upper N right parenthesis Over left parenthesis normal upper T normal o normal t normal a normal l normal n normal o normal o normal f normal s normal a normal m normal p normal l normal e normal s right parenthesis EndFraction
(vi) Precision (P): 
It gives the Classiﬁers correctness/accuracy is measured by Precision. 
no r
mal upper P equals StartFraction upper T upper P Over left parenthesis upper T upper P plus upper F upper P right parenthesis EndFraction
norm al upp
er P equals StartFraction upper T upper P Over left parenthesis upper T upper P plus upper F upper P right parenthesis EndFraction
(vii) Recall (R): It is used to measure the classiﬁers completeness or sensitivity. 
Mathematically it can be represented as 
no r
mal upper R equals StartFraction upper T upper P Over left parenthesis upper T upper P plus upper F upper N right parenthesis EndFraction
norm al up
er R equals StartFraction upper T upper P Over left parenthesis upper T upper P plus upper F upper N right parenthesis EndFraction
(viii) F-Measure: It is the weighted average of precision and the recall. Mathemat-
ically it can be represented as 
up per F equ als StartFraction 2 asterisk left parenthesis normal upper P asterisk normal upper R right parenthesis Over left parenthesis normal upper P plus normal upper R right parenthesis EndFraction
upp er F 
equals StartFraction 2 asterisk left parenthesis normal upper P asterisk normal upper R right parenthesis Over left parenthesis normal upper P plus normal upper R right parenthesis EndFraction

Supervised Learning Approaches on the Prediction of Diabetic Disease ...
165
5 
Experimental Analysis 
In this article, we used diabetic data set with 768 instances. It contains 500 tested 
negative samples and 268 positive samples. There are eight variables in the dataset: 
glucose tolerance, the number of pregnancies, BMI, age, blood pressure, insulin, and 
diabetes pedigree function. The training dataset in this classiﬁcation model includes 
eight attributes listed in Table 4. We implemented Random Forest Classiﬁer (RFC), 
Naïve Bayes Classiﬁer (NBC), and Multilayer Perceptron Classiﬁer (MFC) in Weka 
Tool. Using PIMA Indian dataset for the same experimental setup, we have compared 
the proposed diabetes disease classiﬁcation and prediction system against recent 
advanced approaches. The performance measure employed and the ﬁndings obtained 
for classiﬁcation and prediction were highlighted in the following Table 2. We used  
test option of tenfold cross validation in PIMA data set with 768 instances. Among 
the three algorithms, according to Tested Negative cases, the Naive Bayes algorithm 
is performing better in terms of accuracy than random forest and the multilayer 
perceptron, whereas in case of Tested Positive the Multilayer Perceptron performs 
better. The performance of the three algorithms is given in the Table 5 and plotted in 
Fig. 1 (Fig. 2). 
Table 4 Statistical analysis of the PIMA diabetic dataset 
Attribute
Description
Minimum
Maximum
Mean
Standard 
deviation 
Pregnancies
No. of pregnancies
0
17
3.845
3.37 
Plasma glucose Oral glucose 
tolerance test for 
plasma 
glucose 
concentration for 
2 h  
0
199
120.895
31.973 
Blood pressure 
Blood pressure in 
mm Hg 
0
122
69.105
19.356 
Skin thickness
Skinfold thickness 
of triceps (mm) 
0
99
20.536
15.952 
Insulin
Two hours of serum 
insulin (mu U/ml) 
0
846
79.799
115.244 
BMI
Body mass index 
((weight in kg/ 
(height in m)2) 
0
67.1
31.993
7.884 
Diabetic 
pedigree 
function 
Attribute used in 
diabetes prognosis 
0.078
2.42
0.472
0.331 
Age
Age (Years)
21
81
33.241
11.76

166
R. Patel et al.
Table 5 Accuracy, Precision, Recall, F-measure, and ROC area of Naïve Bayes Classiﬁer, 
Multilayer Perceptron, and Random Forest classiﬁer 
Name of the 
algorithm 
Accuracy
Case
Precision
Recall
F-Measure
ROC Area 
Naive Bayes 
classiﬁer 
Correctly 
classiﬁed 
instances 
76.30
Tested 
negative 
0.802
0.844
0.823
0.819 
Incorrectly 
classiﬁed 
instances 
23.69
Tested 
positive 
0.678
0.612
0.643
0.819 
Multilayer 
perceptron 
Correctly 
classiﬁed 
instances 
75.39
Tested 
negative 
0.798
0.832
0.815
0.793 
Incorrectly 
classiﬁed 
instances 
24.60
Tested 
positive 
0.660
0.608
0.633
0.793 
Random 
forest 
Correctly 
classiﬁed 
instances 
75.78
Tested 
negative 
0.801
0.836
0.818
0.820 
Incorrectly 
classiﬁed 
instances 
24.21
Tested 
positive 
0.667
0.612
0.638
0.820 
Fig. 2 Performance analysis of Naïve Bayes classiﬁer, multilayer perceptron, and Random Forest 
classiﬁer for negative cases

Supervised Learning Approaches on the Prediction of Diabetic Disease ...
167
6 
Conclusion 
As it can reliably and efﬁciently classify data, classiﬁcation is an extremely valuable 
tool for knowledge discovery from databases. As mentioned in the literature review, 
numerous studies have been performed on diabetes datasets utilising classiﬁcation 
approaches in order to assist medical professionals in identifying diabetic prob-
lems. In this paper, popular and widely used classiﬁcation algorithms such as Nave 
Bayesian, Random Forest, and Multilayer Perceptron were analysed on a dataset (the 
PIMA Indian Diabetic dataset) in order to identify the best classiﬁcation strategy for 
predicting diabetes diseases. The quality of the paper can be improved if it could 
classify type 1, type2, and type3 diabetes. 
References 
1. Lin X et al (2020) Global, regional, and national burden and trend of diabetes in 195 countries 
and territories: an analysis from 1990 to 2025. Sci Rep 10, Article no 14790 
2. Choudhary G, Narayan Singh S (2020) Prediction of heart disease using machine learning 
algorithms. In: International conference on smart technologies in computing, electrical and 
electronics (ICSTCEE), pp 197–202 
3. Sivakumar S, Venkataraman S, Bwatiramba A (2020) Classiﬁcation algorithm in predicting 
the diabetes in early stages. J Comput Sci 16(10):1417–1422 
4. Bai BM, Nalini BM, Majumdar J (2019) Analysis and detection of diabetes using data mining 
techniques. Emerg Res Comput Inf Commun Appl 
5. Perveen S, Shahbaz M, Keshavjee K, Geurgachi A (2019) Metabolic syndrome and develop-
ment of diabetic mellitus: predictive modeling based on machine learning techniques. IEEE 
Access 7:1365–1375 
6. Kavakiotis I et al (2017) Machine learning and data mining methods in diabetes research. 
Comput Struct Biotechnol J 15:104–110 
7. VijiyaKumar K et al (2019) Random forest algorithm for the prediction of diabetes. In: 
Proceeding of international conference on systems computation automation and networking 
8. Qawqzeh YK et al (2020) Classiﬁcation of diabetes using photoplethysmogram (PPG) 
waveform analysis: logistic regression modelling. BioMed Res Int 2:6 
9. Dey SK, Hossain A, Rahman Md.M (December, 2018) Implementation of a web application to 
predict diabetes disease: an approach using machine learning algorithm. In: 21st International 
conference of computer and information technology (ICCIT), pp 21–23 
10. Pradhan PMA et al (2012) A genetic programming approach for detection of diabetes. Int J 
Comput Eng Res 2(6):91–94. (ijceronline.com) 
11. Sisodia D, Sisodia DS (2018) Prediction of diabetes using classiﬁcation algorithms. In: Inter-
national conference on computational intelligence and data science (ICCIDS 2018). Procedia 
Computer Science, Science Direct, 132, pp 1578–1585 
12. Naveen Kishore G et al (2020) Prediction of diabetes using machine learning classiﬁcation 
algorithms. Int J Sci Technol Res 9(01) 
13. Choubey DK, Kumar M, Shukla V, Tripathi S, Dhandhania VK (2020) Comparative analysis 
of classiﬁcation methods with PCA and LDA for diabetes. Curr Diabetes Rev 16(8):833–850 
14. Gupta S, Verma HK, Bhardwaj D (2021) Classiﬁcation of diabetes using Naive Bayes and 
support vector machine as a technique. Oper Manag Syst Eng 365–376 
15. Pethunachiyar GA (January, 2020) Classiﬁcation of diabetes patients using kernel based support 
vector machines. In: Proceeding of the international conference on computer communication 
and informatics (ICCCI), Coimbatore, India, pp 1–4. IEEE

168
R. Patel et al.
16. Usha Ruby A, Theerthagiri P, Jeena Jacob I, Vamsidhar Y (2020) Binary cross entropy with deep 
learning technique for image classiﬁcation. Int J Adv Trends Comput Sci Eng 9(4):5393–5397 
17. WeiZhao S, Zhao X, Miao XC (2018) A comprehensive exploration to the machine learning 
techniques for diabetes identiﬁcation. In: IEEE 4th world forum on Internet of Things (WF-IoT) 
18. Srivastava AK et al (2021) Hybrid diabetes disease prediction framework based on data 
imputation and outlier detection techniques. Expert Syst e12785:1–17 
19. Shaﬁ S, Ansari GA (2021) Early prediction of diabetes disease & classiﬁcation of algo-
rithms using machine learning approach. In: International conference on smart data intelligence 
(ICSMDI 2021). SSRN 
20. Saxena R et al (2022) A comprehensive review of various diabetic prediction models: a literature 
survey. Hindawi J Healthc Eng 1–15 
21. Abdalrada AS et al (2022) Machine learning models for prediction of co-occurrence of diabetes 
and cardiovascular diseases: a retrospective cohort study. J Diabetes Metab Disord 21:251–261 
22. Shamreen Ahamed B et al (2022) Prediction of type 2 diabetes millitus disease using machine 
learning classiﬁers and techniques. Mini Rev Front Comput Sci 1–5 
23. Chan H-J et al (2021) Ultrasound sample entropy imaging: a new approach for evaluating 
hepatic steatosis and ﬁbrosis. Med Imaging Diagn Radiol IEEE J Transl Eng Health Med 9 
24. Jesper D et al (2020) Ultrasound-based attenuation ımaging for the non-ınvasive quantiﬁcation 
of liver fat a pilot study on feasibility and ınter-observer variability. Med Imaging Diagn Radiol 
8

Solar Powered Smart Home Automation 
and Smart Health Monitoring with IoT 
Atif Afroz, Sephali Shradha Khamari, and Ranjan Kumar Behera 
Abstract In this paper we present the prototype of smart home which is powered 
by solar. It has a smart MPPT, smart health care tracking system and a smart home 
automation system. The sensors are spread all across the entrance Gate, corridor, 
room and kitchen. This (IOT) design prototype has LCD transistor which keep on 
provides the information. We have also use Wi-Fi technology for online control 
and monitoring. we also have an LCD which keeps us providing the information 
regarding the data. We have also used Wi-Fi technology for the purpose of real time 
controlling and monitoring. The designed smart home utilizes the power from the 
solar panels through a maximum power point tracker and it has an LCD display which 
continuously gives us information regarding the solar input, charging efﬁciency and 
discharging rate etc. The internal infrastructure is so designed that it can work against 
an unexpected condition which may occur when the owner is not present in home 
and it also notify the owner about the problem that has occurred. All the power 
requirements of smart homes are met by a self-generated solar power. 
Keywords Internet of things (IoT) · Wi-Fi · Smart health monitoring · MPPT ·
Arduino · Smart home automation system · Blynk · NodeMCU · ESP8266 · Smart 
home 
1 
Introduction 
This paper describes the setup and operation of the smart home automation system 
powered by solar energy, as depicted in Fig. 1. With the use of a wireless sensor 
network, the proposed smart house is achieved and can be seen online. Solar energy 
serves as the main source of energy for the entire smart home. Low maintenance
A. Afroz 
Gurukula Kangri Vishwavidyalaya, Haridwar, India 
S. S. Khamari envelope symbol · R. K. Behera 
Department of Electrical Engineering, Indian Institute of Technology, Patna, Bihar, India 
e-mail: sephali_1901ee71@iitp.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_16 
169

170
A. Afroz et al.
costs are involved. In this paper, the problems of security, ﬁre, and health monitoring 
are discussed, along with any potential solutions. When the owner is not at home, the 
system can still respond in some circumstances. By keeping an eye on the problems 
that arise in daily life and making decisions to address them, it also addressed the 
concern of smart personal management. Commercially available home automation 
systems come in a wide variety of conﬁgurations, but they can be broadly divided 
into two groups: remotely controlled and locally controlled systems. Users of the 
remotely controlled home automation systems can remotely control their homes 
using a smartphone or personal computer and a Wi-Fi or Internet connection. Users 
of locally controlled home automation systems can operate their appliances using a 
built-in controller with stationary or wireless (such as GSM, Zigbee, and Bluetooth) 
communication technology. However, there are numerous issues and problems that 
must be taken into account when designing such a home automation system. 
To set up, monitor, and control home appliances effectively and easily, a home 
automation system should offer a simple, dependable, and user-friendly interface. 
To fully beneﬁt from wireless technology, an automation system should be quick 
enough to deliver dependable connectivity with an acceptable rate of data transfer 
and communication range. Last but not least, the controller for the home automation 
system needs to be very affordable so that the average person can use and beneﬁt 
from it. 
In order to address these design problems and minimise the problems with home 
automation systems, a combination for both in the neighbourhood and remotely 
controlled systems has been discussed. The proposed smart house will give a locally 
controlled home automation system with a Wi-Fi interface and low cost Arduino 
microcontroller. This enables the system to be location independent for both the 
mobile provider and the user. The created automation system can be controlled 
locally or remotely using a variety of smart phones or an IoT platform that allows 
users to manage, monitor, and control their devices and appliances over the Internet. 
This paper’s remaining sections are organised as follows: The following section 
discusses System Architectures. Additionally, Methodology is found in Sect. 3. 
Section 4 discusses the implementation of the suggested system, Sect. 5 introduces
Fig. 1 Graphical 
representation of IoT based 
home automation system 

Solar Powered Smart Home Automation and Smart Health Monitoring …
171
the analysis and results, and Sect. 6 draws a conclusion and emphasises the need for 
further research. 
2 
System Archıtecture 
2.1 
The Design Architecture 
The wireless sensor networks are placed at 4 locations in the smart home: at main 
gate, in room corridor, in kitchen and in living room respectively. Figure 2 shows the 
block diagram of wireless sensor network represented through block diagram. 
The main door is locked using a keyboard-based lock system, and the rest of the 
doors are made of HC-SR04 sensors with servo motors installed at gate opening 
mechanisms. The proposed smart home is implemented using Node MCU. A MQ-2 
sensor is installed in the kitchen to monitor for harmful gases, smoke, and LPG. 
The weight of smoke and LPG is calculated using the load sensor. DHT 22 sensors 
are placed in rooms to monitor the temperature and humidity, and PIR sensors are 
positioned in the home’s hallway to detect intruder motion. A smart health scanning 
system for the bedroom is made with the pulse rate and LM35 sensor. It ensures the 
senior population’s health and wellness. The PIR sensor’s function is to detect human 
presence in the room and control the lights and fan accordingly. These gadgets need 
Wi-Fi to connect to the network.
Fig. 2 Smart home block diagram consisting of WSN 

172
A. Afroz et al.
The Blynk application is used to receive data on smart phones and computers so 
that they can monitor and manage smart homes. 
2.2 
Descreiption of the Components Used 
a) Nodemcu- The NodeMCU (Node Microcontroller Unit) is the primary micro-
controller of this project. It has a ESP8366 module which is a use to connect 
objects and let the data transfer using Wi-Fi technology. 
b) Arduino Nano- We are using it in our MPPT as the primary microcontroller. We 
are using it in our MPPT circuit. 
c) Matrix Keyboard- matrix keypad is a mini compact input device that is use to 
accept user input and processed it by microcontroller. It is being use in our 
entrance gate lock system. 
d) Sensor- The sensor’s speciﬁcation is given in the following table. 
Sensors
Speciﬁcations 
LM35
Voltage range: 0 to 5 V 
10 mA operating current 
200 C maximum temperature 
Minimum temperature: −20 C 
DHT 22
3.3–5 V operating voltage 
Temperature range: 0–60 °c 
Temperature range: 0–60 °c 
range of humidity 10 to 98% 
Pulse Sensor
Voltage range: 3–5 V 
100 mA operating current 
Light source: super red LEDs at 660 nm 
MQ 2 Sensor
Voltage range: 3–5 V 
20 k Load resistance  
Sensing resistance for heaters: 10–60 k 
Range of concentration: 100 to 1000 ppm 
Heating requirements: 900 mW 
PIR Sensor
Voltage range: 3.3–12 V 
Range of sensitivity: 5 m 
Range of detection: 110–70° 
HC-SR04
3.3–5 V for operating voltage 
Operating frequency: 40 kHz Operating current: 15 mA 
Max. span: 4 m 
a minimum of 1 cm 
Angle measurement: 15
(continued)

Solar Powered Smart Home Automation and Smart Health Monitoring …
173
(continued)
Sensors
Speciﬁcations
LDR 
Flame Sensor 
Operating voltage:5 V 
Dark Resistance range:1–20 M ohm 
Operating voltage: 5 V 
Operating current:20 mA Spectrum range:760 –1100 nm 
Detection angle: 0–60° 
Operating temperature: −25–85 °C 
3 
Methodology 
This section talks about the conceptual framework and methodology that sensors used 
for their research. At various stages, it also includes their systematic organisation and 
coordination. In Fig., an IoT-based smart home’s proposed methodology is displayed 
Fig. 3. 
The management, auto-monitoring, and control of devices via smartphone and 
PC form the fundamental framework of this research paper. The use of solar power 
smart energy management system are also discussed in this paper. 
By comparing the amount of temperature, smoke, and ﬂame sensor, this house 
is protected from ﬁre. Installing a PIR sensor in the corridor provides security and 
protection against intruders. It also alerts the home with the help of a buzzer, a pop-up
Fig. 3 Flowchart of the 
Smart Home Automation 
architecture’s methodology 

174
A. Afroz et al.
message, and notiﬁcations sent to a PC and a smartphone. NodeMCU and Arduino 
Uno are used to control a variety of electrical equipment. NodeMCU sends signals 
to a relay board to cause these electrical equipment to turn on and off. 
In the smart home, personal management is kept up-to-date and intelligent. With 
the help of a smartphone and a computer connected to Wi-Fi, you can turn on and 
off the lights and other electrical equipment. 
4 
Implementıonal Detaıls 
4.1 
Smart Solar Energy Management – The Power 
Requirement of the Proposed Smart Home is 
Powered by using solar energy system which is on the rooftop of the house. To get 
maxium efﬁciency of solar panel we have used a MPPT. This MPPT controller is 
in-charge of: charging the battery in three different modes, protection of both the 
the solar panel battery and from overcurrent, to disable or enable the load when 
the battery is in undervoltage stage and also keep the track of the charged capacity. 
We have included 3 different stage of charging, This ﬁrst stage is called bulk mode 
of charging in this a constanct current is supply to the battery to charge it to 70%, 
and the second stage is absorption stage, in this stage of charging we don’t have 
constant current anymore but we provide a constant voltage. The voltage is kept by 
the microcontroller at a speciﬁc value and the current keeps on getting lower and 
lower while the battery is charging the last 25% and after obtaining the current value 
below a certain value, then our charging third and the last stage of charging which is 
known as ﬂoat stage, in ﬂoat stage the microcontroller reduces the voltage to a preset 
value and keeps a current ﬂow less than 5% of the battery capacity. By this will keep 
the battery fully charged indeﬁnitely and also increase the battery life (Fig. 4). 
Fig. 4 Data of MPPT on LCD display

Solar Powered Smart Home Automation and Smart Health Monitoring …
175
This MPPT also has an LCD display which continuously gives us information 
regarding the solar input, load, charging efﬁciency and discharging rate. When the 
sun is in different positions throughout the day, in various weather conditions, with 
varying and changing roof pitches, and with varying numbers of panels per string, 
MPPT is used to transfer the maximum power from the photovoltaic module. We 
cannot use solar panels at night, on cloudy or snowy days, or when it is cold outside. 
To get around these issues, we store energy in batteries. In this process, a voltage 
regulator is used to regulate the voltage. By stepping up and down the voltage in 
accordance with the needs of the home appliances, we are able to protect them from 
burnout and underperformance. Arduino,NodeMCU and all our sensors are also 
powered by solar energy. 
In order to prevent excessive energy loss and promote energy conservation, we 
installed IR sensors in each room to detect when a person enters the space. If a person 
is present in the room, the light and fan are turned ON; if not, they are turned off. 
This intelligent solar energy management system can be handled without the need for 
manual operation. The fan and lights are turned OFF and ON by using smartphone 
and PC. This is especially helpful for very elderly people and people with disabilities 
who have trouble moving and walking. 
4.2 
Sensor Networking 
a) At Entrance gate- A password lock security system has been installed at the 
primary gate of the house, so that unknown person can’t get inside the house 
without owner permission. 
b) At Room door- The distance of the individual from the gate is calculated by 
ultrasonic sensor. If the distance between the person and the door is 10 cm then 
the signal will be sent to Arduino and Arduino gives command to servo motor 
to open the door. The servo motor and Arduino is power from self-solar power 
generated power system of house. 
c) At Corridor-To protect our smart home from intruder we have install the PIR 
sensor at the corridor which detect the intruder and if any intruder is detected 
we get a pop-up message and notiﬁcation in our smartphone and PC. The buzzer 
inside the home also started ringing when a intruder is detected. 
d) In Living room- The DHT 22 sensor is meant for measuring the ttemperature 
and humidity in the smart home in real time and it can be operated and recorded 
by Smartphone with Blynk app. As the temperature increases above threshold 
value the fan will be ON and if it is below the threshold value the heating system 
get started. Under abrupt rise in temperature and it is higher than 50 °C. This 
condition will be compared with the MQ 2 sensor data for detecting the ﬁre 
present in the house. The presence of ﬁre is conﬁrmed when the smoke presence 
in the house is above 760 ppm and hence, ﬁre signal is activated. The information 
and notiﬁcation is directly sent to the occupant. The buzzer is employed and it 
will alert the persons present in smart home.

176
A. Afroz et al.
e) Smart Health care system- The health scanning is performed by using LM35 
sensor and pulse rate sensor, the value is also shown on LCD screen connected 
to Arduino and a ESP8266 is attach which transfer the data to server for online 
monitoring. If the sensor detect the value of pulse rate and body temperature 
beyond the normal range of the human body a popup message and notiﬁcation 
will be send to the smartphone and PC. These data are sent to the smartphone 
and PC. This data is saved in server to maintain the medical record of the person 
health. 
f) Kitchen- When the value of the smoke measurement by MQ 2 sensor exceeds 
the predetermined limit, a buzzer sounds and an exhaust fan is activated to clear 
the smoke and any LPG gas leak. If the smoke value is extremely high, an alert 
is sent to the home via message and notiﬁcation, and the buzzer starts to ring 
to alert anyone present in the smart home. LPG is connected to a load sensor, 
the weight of the LPG gas is checked, if it is found to be lesser than 15% then 
an alerts is send to the owner to call for the providing of a new LPG cylinder 
g) Smart water tank level- By using HC-SR04 sensor we are able to measure the 
water level in the water tank, and water level is indicated on the LCD display 
placed in living room and we can also monitor the water level by mobile and PC. 
All the data of the sensor system is also visible on a build-in LED installed in the 
house, and the data will also be monitor by smart phone or PC. 
Figure 5 depicts the smart implementation model of solar-powered smart home 
automation and smart health monitoring, while Fig. 6 depicts an experimental 
prototype version of these models.
5 
Results and Dıscussıon 
This app is created for controlling and monitoring smart homes is depicted in Fig. 6. 
The light and fan can be turned on and off from a distance using a phone or computer. 
The level boxes on the phone and computer display the room’s temperature, 
humidity, and smoke levels. The value of the heart rate and body temperature are 
also displayed in their appropriate level boxes. Additionally, pop-up messages and 
notiﬁcations are sent to mobile and PC in the event of any abnormal circumstances 
(Figs. 7, 8, 9 and 10).

Solar Powered Smart Home Automation and Smart Health Monitoring …
177
Fig. 5 System architecture of Solar Powered Smart Home Automation and Smart Health care 
system monitoring with IoT 
Fig. 6 Prototype model of the Solar Powered Smart Home Automation and Smart Health care 
system monitoring with IoT

178
A. Afroz et al.
Fig. 7 Data and reading can be get in Mobile Application
Various sensor data and readings are displayed in various boxes. The Fig. 9, which 
depicts the person’s heartbeat observation, is a graph.

Solar Powered Smart Home Automation and Smart Health Monitoring …
179
Fig. 8 Website to get the data on PC 
Fig. 9 Heart pulse on serial monitor
The serial monitor continuously updates the data of every sensors on the PC and 
mobile app, also receives the sensor data output. Figure 11 displays the DHT 22 
sensor’s reading. Mobile devices and computers can also be used for controlling and 
monitoring.

180
A. Afroz et al.
Fig. 10 Body temperature and Pulse rate on website
Fig. 11 DHT 22 sensor output displayed on a serial monitor

Solar Powered Smart Home Automation and Smart Health Monitoring …
181
6 
Conclusion 
The study discusses Wi-Fi-enabled smart health monitoring and solar-powered smart 
home automation. Through the use of Wi-Fi technology, all of the household’s elec-
trical appliances may be bent effectively and seamlessly. The sensor measures the 
indoor temperature and humidity. Smart home automation powered by solar energy 
and smart health monitoring enhance quality of life by offering comfort, intelligence, 
good security, and smart health tracking. Our power cost may be greatly decreased by 
utilising this Solar Powered Smart Home Automation and Smart Health Monitoring 
since the user can operate the electrical appliances whenever they want using a phone 
or computer. 
The development of a sustainable, clean energy system is the next step in our 
research. We might switch out some sensors for wireless ones in order to address the 
wiring problem. To connect all sensors and devices to an IoT platform, we will build 
a gateway. With the help of a relay board, our ﬁnished product will be a small box that 
can quickly and safely be connected to an existing switching board for appliances 
found in real homes, providing safer control and a self-sufﬁcient household. 
References 
1. Alkar AZ, Uhar UB (200) An internet based wireless home automation system for multifunction 
devices. IEEE Trans Consum Electron 51(4):1169–1174 
2. http//www.itu.int/en/ITU-T/gsi/iot 
3. Anandhavalli D, Mubina NS, Bharathi P (2015) ‘Smart home automation control using 
Bluetooth and GSM.’ Int J Informative Futuristic Res. 2(8):2547–2552 
4. http://smartcities.gov.in 
5. http://en.wikipedia.org/wiki/Internet_of_Things 
6. http://www.arduino.cc 
7. https://en.wikipedia.org/wiki/ESP8266 
8. https://www.nodemcu.com/index_en.html 
9. https://en.wikipedia.org/wiki/NodeMCU 
10. http://esp8266.net/ 
11. Mattoo A, Kumar S (2019) Internet of things: a progressive case study. In: Handbook of IoT 
and big 
12. Ahmed EM, Shoyama M (2010) Highly efﬁcient variable-step-size maximum power point 
tracker for PV systems. In: 2010 3rd international symposium on electrical and electronics 
engineering (ISEEE), pp 112–117. https://doi.org/10.1109/ISEEE.2010.5628532 
13. https://www.solar-electric.com/learning-center/mppt-solar-charge-controllers.html/ 
14. https://en.wikipedia.org/wiki/Maximum_power_point_tracking 
15. Gaga A, Errahimi F, Es-Sbai N (2014) Design and implementation of MPPT solar system based 
on the enhanced P&O algorithm using Labview. Int Renew Sustain Energy Conf (IRSEC) 
2014:203–208. https://doi.org/10.1109/IRSEC.2014.7059786 
16. Acharjya D, Geetha MK, Sanyal S (2017) Internet of things: novel advances and envisioned 
applications vol 25, Springer, Cham 
17. Piyare R, Tazil M (2011) Bluetooth based home automation system using cell phone. In: 2011 
IEEE 15th international symposium on consumer electronics (ISCE), pp 192–195. https://doi. 
org/10.1109/ISCE.2011.5973811.

182
A. Afroz et al.
18. Kazi R, Tiwari G (2015) IoT based Interactive Industrial Home wireless system, Energy 
management system and embedded data acquisition system to display on web page using 
GPRS, SMS & E-mail alert. Int Conf Energy Syst Appl 2015:290–295. https://doi.org/10. 
1109/ICESA.2015.7503358 
19. Zamri MA, Kamaluddin MU, Zaini N (2021) Implementation of a microcontroller-based home 
security locking system. In: 2021 11th IEEE international conference on control system, 
computing and engineering (ICCSCE), pp 216–221. https://doi.org/10.1109/ICCSCE52189. 
2021.9530966

Seasonal-Wise Occupational Accident 
Analysis Using Deep Learning Paradigms 
N. Nandhini and A. Anitha 
Abstract In recent years, occupational accidents causes a huge loss of human life 
and the development of the economy of the country. Many techniques are evolved 
for automating the safety precautions for employees in the industrial sectors such as 
mining, metals, construction, chemical, and electrical sections. However, the automa-
tion cannot be accurate as the data analysis is based on real-life data. Since the real-life 
data are imbalanced and uncertain, it is necessary to identify better tools to overcome 
these issues. Thus the proposed model utilizes SMOTE (Synthetic Minority Over-
sampling Technique) for data balancing, whereas a rough set is used for identifying 
the signiﬁcant features that help to maintain data consistency. The consistent data 
is then applied to the Deep Neural Network (DNN) for the classiﬁcation process. 
The performance of the proposed model is checked against the evaluation metrics 
and compared with the existing deep learning models to exhibit the efﬁciency of the 
proposed model. Thus the ﬁndings of the proposed model may improve the abil-
ities of safety professionals in the industrial sector to develop safety intervention 
activities. 
Keywords Synthetic minority over-sampling technique · Deep neural network ·
Occupational accidents · Rough set · Industrial sector 
1 
Introduction 
The International Labor Organization (ILO) reports that between 2016 and 2019, 
there were about 374 million non-fatal injuries and 2.78 million fatal occupational 
injuries [1]. The operation of businesses is impacted by occupational illnesses and 
accidents at work in terms of a reduction in the production process, a lack of compe-
tent labour, and a weakening of competitiveness, which lowers productivity [2]. Since
N. Nandhini · A. Anitha envelope symbol
School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, 
India 
e-mail: aanitha@vit.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_17 
183

184
N. Nandhini and A. Anitha
statistics and data on industrial accidents are valuable, it is necessary to use trust-
worthy and durable methodologies to extract information from the data to manage 
and produce prediction patterns of industrial accidents more effectively [3]. To build 
prediction models for industrial accidents, a variety of strategies, including conven-
tional statistical techniques and machine learning (ML) methods, are used. ML algo-
rithms are more popular and may outperform conventional statistical methods since 
they can analyze large amounts of raw data. These have led to the development of deep 
learning methods because of their efﬁciency [4–7]. To manage safety and accident 
predictions, the paper has proposed an effective prediction model with less compu-
tation time. Therefore, the following are speciﬁc methods this study speciﬁcally 
aims: 
• To select the important features and balance the data for forecasting the probability 
of accidents in the manufacturing industry; 
• To construct Deep Neural Network (DNN); and 
• To assess prediction accuracy using test data. 
The dataset used in this work is produced by a Brazilian company, IHM Stefanini, 
which provides statistical data on accidents in 12 manufacturing plants across three 
nations. The conceptual framework for addressing injuries used in this study consists 
of two parts. The ﬁrst component consists of initial processing using smote tech-
nique. Since data processing techniques clean up IHM Stefanini data to increase 
data quality prediction accuracy and hence they are essential. The next component is 
the processing of data using DNN. This study will support proactive safety measures 
to lower accidents in manufacturing industries and aid stakeholders in predicting the 
likelihood of accident occurrence. 
The remaining sections are organised as follows: Related work is discussed in 
Sect. 2. The background fundamentals, methodologies, and the development of the 
prediction models used in this study are discussed in Sect. 3. The experimental 
analysis using DNN is detailed in Sect. 4 of the paper. Comparative analysis using 
existing models with the proposed model is exhibited in Sect. 5, and followed by 
conclusion and future scope as Sect. 6. 
2 
Related Work 
Numerous machine learning algorithms have typically been employed to calculate 
and forecast industrial accidents. For instance, support vector machines, and decision 
tree-based ensemble models are used for predicting occupational accidents in the 
construction industry [8]. Similarly, to achieve a higher degree of accuracy and 
robustness, two well-known machine learning algorithms—support vector machine 
(SVM) and artiﬁcial neural network (ANN)—have been used. Their parameters have 
been optimized by two optimization algorithms, genetic algorithm (GA) and particle 
swarm optimization (PSO) [9]. Also, the decision tree (DT) approach was adopted 
for safety categorization and accident analysis at railway stations to forecast the

Seasonal-Wise Occupational Accident Analysis Using Deep Learning …
185
characteristics of accident victims. The prediction of accidents is only taken into 
account in the majority of current research, but the severity of the accident was 
considered with the help of six prediction algorithms such as SVM, ANN, KNN, 
Naive Bayes, classiﬁcation and regression tree analysis, and random forest [10]. 
Even though some of these algorithms are straightforward to model, they come 
with some limitations; for instance, as a result of decision trees’ large sample vari-
ation, projected classiﬁcations, and probability are not stable for new cases. Like-
wise, SVM and ANN have computational complexity and adaptability using non-
parametric theory. However, if the parameters of all these machine learning algo-
rithms are not correctly set, they do not deliver ideal outcomes like classiﬁcation 
accuracy and understandability. Hence, deep learning technology is effective at iden-
tifying intricate dataset structures, we employed it to address the issue of manu-
ally extracting important properties from the unstructured data source. Additionally, 
using high-dimensional data, deep learning approaches may dynamically create new 
task-speciﬁc properties. Thus the proposed model applied Synthetic Minority Over-
sampling Technique (SMOTE) to handle the data imbalance in the dataset. The devel-
opment of deep learning models for safety management in manufacturing industries 
was also consolidated in this study. Finally, the deep learning model’s performance 
is compared with existing models. 
3 
Basics of Deep Neural Network 
A deep neural network (DNN) converts a vector of inputs into an output vector which 
has become a very popular area of machine learning. DNNs are conceptually similar 
to artiﬁcial neural networks, but they include additional hidden layers in the middle of 
input and output to analyze complicated information more effectively [11]. A DNN 
model’s objective is to capture all potential aspects of the input rather than to learn 
the key points of information. The architecture of DNN with the training and testing 
phase is described in Fig. 1. DNNs perform well and have a lot of useful applications, 
however, they are sensitive to data imbalance. Consequently, performance slightly 
declines, and DNNs eventually make incorrect predictions with a high degree of 
conﬁdence. To address the issue of class imbalance, data resampling was used. The 
so-called “Synthetic Minority Over-sampling Technique,” or SMOTE, is a particu-
larly effective method for producing new data. Its foundation is the simple generation 
of data points on the line segment linking a randomly chosen data point and one of 
its K-nearest neighbours in order to sample data from the minority class.

186
N. Nandhini and A. Anitha
Fig. 1 The architecture of Deep Neural Network 
3.1 
Initial Data Description 
The Brazilian company IHM Stefanini generated the dataset utilized in this study, 
which offers statistical information on accidents in 3 manufacturing sectors that 
occurred in four seasons. The feature selection is done for the accidents by segregating 
them into 6 categories. There is a brief explanation for each variable in Table 1.
Data analysis is the most difﬁcult activity in a dynamic setting owing to the fast 
expansion of data over time. The dataset must be preprocessed or standardized before 
analysis [8]. Before retrieving information from massive data repositories, feature 
selection is used. The Rough set theory is a feature selection, which is particularly 
useful for classiﬁcation issues that are adopted in this study to classify important 
features such as the number of accidents that occurred in different seasons like spring, 
summer, autumn, and winter in Brazil. The result shows most of the accidents have 
occurred in autumn and summer and safety measures are to be undertaken to reduce 
these accident percentages with respect to the four seasons. 
3.2 
Initial Processing Using SMOTE Technique 
Due to the fewer data in the dataset, there was only one method to artiﬁcially balance 
it by adding new instances of the classes with a low population. When dealing with 
minor or medium-sized industrial challenges, SMOTE is the popular approach [12]. 
The dataset is preprocessed to overcome the data imbalance with the help of SMOTE 
technique. The working process of SMOTE is represented in Fig. 2.
Following are the steps to compute SMOTE oversampling:

Seasonal-Wise Occupational Accident Analysis Using Deep Learning …
187
Table 1 Variables from 439 industrial accident data items are described 
Variable
Type
No. of elements
Element names 
Year
Binary
2
2016,2017 
Countries
Categorical
3
Country_01,Country_02, 
Country_03 
Local
Categorical
12
Local_01, Local_02, Local_03, 
Local_04, Local_05, Local_06, 
Local_07, Local_08, Local_09, 
Local_10, Local_11, Local_12, 
Seasons
Categorical
4
Spring, summer, autumn, winter 
Industry Sector
Categorical
3
Metal, Mining, Others 
Accident level
Categorical
5
I, II, III, IV, V 
Potential accident level
Categorical
6
I, II, III, IV, V, VI 
Genre
Categorical
2
Female, Male 
Employee type
Categorical
3
Employee, Third party, Third 
party(Remote) 
Critical risk
Categorical
21
Category 1: Bees, Venomous 
animals 
Category 2: Blocking and 
isolation of energies, Conﬁned 
space, Chemical substances, and 
suspended loads 
Category 3: Individual 
equipment, machine protection, 
manual tools 
Category 4: Burn, cut, liquid 
metals, plates, pressed, 
pressurized 
Category 5: Electrical shock, 
electrical installation, power lock 
Category 6: Fall prevention, 
trafﬁc, vehicles and mobile 
equipment
Do the following for each pattern X in the minority class: 
– Choose one of its K’s closest neighbours C. 
– On the line segment that connects the pattern and the chosen neighbour, make a 
new pattern S at a random position as shown below as given in Eq. (1). 
r 1  equal s uppe r C 11 p lus l eft pa
renthesis d Subscript upper C Baseline minus d Subscript upper C Baseline 11 Baseline right parenthesis asterisk left parenthesis upper C minus upper C 11 right parenthesis
– where [0,1] is the range for the uniform random variable r1, d S ubscript upper C Baseline minus d Subscript upper C Baseline 11 is the 
difference between the point C and ﬁrst minority subset C11.

188
N. Nandhini and A. Anitha
Fig. 2 Work ﬂow of SMOTE algorithm
3.3 
Final Processing Using DNN 
To secure the gathering and the accuracy of data, preprocessing is of utmost impor-
tance. The 158 critical risks in the industry sector make up the dataset which is used 
in this investigation, which is retrieved from the Kaggle Repository. To reduce the 
noisy data, which has missing feature values on 12 rows in the dataset. Before being 
processed by the suggested neural network, the preprocessed data must adhere to 
the requirements. The three parts of the proposed paradigm data processing, data 
analytics, and data visualization are described in Fig. 3. The imbalance of the indus-
trial injury dataset is overcome by the model’s ﬁrst component, which uses SMOTE 
to carry out the extract, transform, and load (ETL) procedure. Oversampling creates 
ﬁctitious data for the minority class, balancing, enhancing, and expanding the dataset.
The crucial portion will be the second component, where predictive analytics 
under data analytics will be carried out. DNN was used in the creation of this compo-
nent’s predictive analytics. The gradients of the loss function in neural networks 
approach zero when more layers with speciﬁc activation functions are added, making 
the network challenging to train. As a result, the Adam optimization will improve

Seasonal-Wise Occupational Accident Analysis Using Deep Learning …
189
Fig. 3 Description of the proposed model
the learning rate and training of the batch normalisation layers. The trained model 
with the best epochs, the largest data batch, and the verbose will be stored in the 
JSON and h5 formats, respectively. To determine the accuracy of the trained model, 
the learning rate will ﬂuctuate. The structure of the proposed model is described in 
Fig. 4.
At the end of this procedure, a ﬁle with the prediction’s ﬁnal ﬁndings will 
be produced and stored. The full model process will be shown to represent the 
overall performance, deep analysis, and subprocesses of the proposed model. When 
compared to existing deep learning models, the suggested approach generates greater 
accuracy. In keeping with the ultimate purpose of this study, which is to enhance 
prediction accuracy even more, this deep learning prediction model combines the 
best prediction records gained from earlier model training.

190
N. Nandhini and A. Anitha
Fig. 4 Structure of the proposed model
4 
Experimental Analysis Using DNN 
After the model has been trained and tested, the results are generated. Training data 
is segregated from testing data, with a ratio of 70:30. The suggested model allows for 
the most effective training with a minimal size of data. The industrial injury dataset’s 
tiny batch size is ideal for the DNN to produce better predictions during testing. 
The suggested model’s ultimate results are 98.6%. This accuracy is the greatest 
among existing deep-learning models and with an F1 score of 98.06. The result from 
the rough set shows autumn has more accidents and hence the graph that describes the 
accuracy, f1-score, recall, and precision for most occurring categorical accidents in 
autumn from the balanced dataset are described in Fig. 5 which shows that category 
4 (burn, cut, liquid metals, plates, pressed and pressurised systems) is found to have 
more accuracy when compared with other categorical accidents.

Seasonal-Wise Occupational Accident Analysis Using Deep Learning …
191
5 
Comparative Analysis Using Existing Models 
with the Proposed Model 
Deep Learning algorithms are adaptable and can capture complicated nonlinear 
connections in datasets despite missing or imbalanced data. Deep learning models 
outperformed other machine learning approaches in terms of prediction accuracy. 
The prediction models utilized in this study proved to be accurate and useful for 
managing industrial injuries. As a result, the ﬁndings of this study have practical 
implications. The created predictive analytics algorithms can help health and safety 
ofﬁcials forecast and manage injuries. Avoiding injuries will have a favourable inﬂu-
ence on project objectives such as project completion on time and within budget, as 
well as reducing the number of productive hours missed due to worker sickness. The 
existing models are compared to have less accuracy than the proposed model. Also, 
there is an increase in the accuracy of DNN with SMOTE with 98.6% accuracy 
when compared to the existing model of DNN without SMOTE with 92% accu-
racy [11]. Figure 5 describes the performance metrics of categorical accidents in the 
autumn season. Figure 6 describes the accuracy of existing models compared with 
the proposed model. 
Fig. 5 Performance metrics for most occurred categorical accidents in the autumn season

192
N. Nandhini and A. Anitha
Fig. 6 Comparative analysis of existing models with respect to the accuracy 
6 
Conclusion and Future Scope 
Accidents occur often on construction sites across the world, even in industries with 
rigorous safety regulations [3]. Industrial safety is a big problem, particularly in 
the manufacturing industries, because the jobs involved are exceedingly dangerous 
and are a major cause of worker deaths and injuries. However, as demonstrated 
in this work, algorithms may be constructed to forecast the chance of an accident 
occurring in a season and rank an industrial project based on its safety hazards. 
DNN with SMOTE predictive performance was compared to the random forest with 
SMOTE [13], MLP without SMOTE [13], RBF without SMOTE [13], and DNN 
without SMOTE [11] models. Compared to other existing models, the suggested 
model achieved higher prediction accuracy. In the future, multimedia data will be used 
to build prescriptive frameworks that integrate powerful deep learning algorithms to 
foresee site workers and mobility equipment motions in order to improve construction 
site safety and safety management.

Seasonal-Wise Occupational Accident Analysis Using Deep Learning …
193
References 
1. Wadsworth E, Walters D (2019) Safety and Health at the Heart of the Future of Work: Building 
on 100 Years of Experience 
2. Sarkar S, Maiti J (2020) Machine learning in occupational accident analysis: a review using 
science mapping approach with citation network analysis. Saf Sci 131:104900 
3. Oyedele AO, Ajayi AO, Oyedele LO (2021) Machine learning predictions for lost time injuries 
in power transmission and distribution projects. Mach Learn Appl 6:100158 
4. Uddin S, Khan A, Hossain ME, Moni MA (2019) Comparing different supervised machine 
learning algorithms for disease prediction. BMC Med Inform Decis Mak 19(1):1–16 
5. Manna T, Anitha A (2023) Deep ensemble-based approach using randomized low-rank 
approximation for sustainable groundwater level prediction. Appl Sci 13(5):3210 
6. Manna T, Anitha A (2022) Forecasting air quality ındex based on stacked LSTM. In: 2022 IEEE 
7th ınternational conference on recent advances and ınnovations in engineering (ICRAIE), vol 
7, pp 326–330. IEEE 
7. Acharjya D, Anitha A (2017) A comparative study of statistical and rough computing models 
in predictive data analysis. Int J Ambient Comput Intell (IJACI) 8(2):32–51 
8. Lee JY, Yoon YG, Oh TK, Park S, Ryu SI (2020) A study on data pre-processing and accident 
prediction modelling for occupational accident analysis in the construction industry. Appl 
Sci 10(21):79496 
9. Sarkar S, Vinay S, Raj R, Maiti J, Mitra P (2019) Application of optimized machine learning 
techniques for prediction of occupational accidents. Comput Oper Res 106:210–224 
10. Sarkar S, Pramanik A, Maiti J, Reniers G (2020) Predicting and analyzing injury severity: a 
machine learning-based approach using class-imbalanced proactive and reactive data. Saf Sci 
125:104616 
11. Radaideh MI, Pigg C, Kozlowski T, Deng Y, Qu A (2020) Neural-based time series forecasting 
of loss of coolant accidents in nuclear power plants. Expert Syst Appl 160:113699 
12. Elreedy D, Atiya AF (2019) A comprehensive analysis of synthetic minority oversampling 
technique (SMOTE) for handling class imbalance. Inf Sci 505:32–64 
13. Bustillo A, Pimenov DY, Mia M, Kapłonek W (2021) Machine-learning for automatic predic-
tion of ﬂatness deviation considering the wear of the face mill teeth. J Intell Manuf 32(3):895– 
912

MLFP: Machine Learning Approaches 
for Flood Prediction in Odisha State 
Subasish Mohapatra, Kunaram Tudu, Amlan Sahoo, 
Subhadarshini Mohanty, and Chandan Marandi 
Abstract Out of all existing natural calamities, ﬂood is one of the most dangerous 
among all. It occurs when an excessive amount of water is gathered in a given space. 
It frequently occurs as a result of severe rain. Floods are one of the worst affecting 
natural phenomena which cause heavy damage to property, infrastructure, and most 
importantly human life. To prevent such disasters, various predictive models are 
used to forecast the ﬂoods that can occur in the future. It’s hard to create a predictive 
model because of its complexity. In this system, the rainfall data is fed into different 
machine-learning models. Before this process, the data is cleaned and pre-processed, 
and the dataset for training is split into a train set and a test set in an 80:20 ratio. 
Then the accuracy of each model is compared and the confusion matrix parameters 
are taken to evaluate and analyze. In the end, the best model is chosen by comparing 
the accuracy. 
Keywords Machine learning · Flood · K-Nearest neighbor · Logistic regression ·
Naïve bayes · Accuracy 
1 
Introduction 
The Flood is a natural disaster caused due to natural events like rain, storm, cyclone, 
etc. Whenever there is rainfall it ﬁlls the rivers, lakes, and other water reserves. The 
yearly data shows that the amount of rainfall is increasing and it’s due to climate 
change. Flood is predicted in several locations using some advanced technologies 
which just helps people to be prepared for upcoming disasters [1]. It is very difﬁcult 
to create a predictive model using machine learning. Machine learning provides 
computers the ability to learn without the presence of an openly programmed basis.
S. Mohapatra envelope symbol · K. Tudu · A. Sahoo · S. Mohanty · C. Marandi 
Odisha University of Technology and Research, Bhubaneswar, India 
e-mail: smohapatra@outr.ac.in 
S. Mohanty 
e-mail: sdmohantycse@outr.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_18 
195

196
S. Mohapatra et al.
It has a role in preventing many natural disasters like earthquakes, ﬂoods, and many 
more. Machine learning makes decisions using past data and these data are fed into 
the algorithms and the output is predicted [2]. The machine is then given an original 
group of instances (data) to examine the training data (set of training instances) and 
produce a proper outcome from labeled information utilizing the supervised learning 
technique. Unsupervised learning is a branch of machine learning algorithms in 
which models are supervised without the use of a training dataset. On the other hand, 
employ data to classify previously unknown patterns and insights [3]. Unsupervised 
learning methods such as clustering and association are two examples. Clustering 
is a method of grouping items that keeps similar kinds of items or data points in a 
single group while dissimilar ones are in the other one. The association rule is also an 
unsupervised learning method for determining links among different data attributes 
in a larger dataset. This could be helpful in the areas where ﬂash ﬂood occurs. This 
system takes the input as the rainfall data all over the Odisha state (from the year 
2008 to 2021) and processes it using different machine learning prototypes the top 
model is obtained with the help of the accuracy of different algorithms, which would 
help individuals prior, save lives while also conserving a great deal of climatological 
work. 
The residue of the paper is organized as follows. Section 3 summarizes all the 
literature surveys for this paper. The methodology and ﬂow diagram of the proposed 
model is demonstrated in Sect. 4. Section 5 depicts various types of algorithms and 
techniques used during the simulation work. The analysis of the various performance 
matrices is described in Sect. 6. Section 7 contains a detailed analysis of the results. 
Section 8 embeds the future scopes along with the conclusion of this paper. 
2 
Related Works 
In recent literature works, a lot of researchers have used different kinds of predictive 
models for the accurate detection and prediction of ﬂoods. Rao et al. [4] proposed 
a ﬂooding prediction prototypical for the Godavari Basin. The distributed approach 
that is modeling ﬂood calculation uses real-time hydrometeorological information as 
feedback, as well reliability of 87 percent was observed in computing peak discharge 
throughout ﬂood occasions in comparison with noticed ﬂows. 
Dankers et al. [5] have proposed the LISFLOOD model to examine the inﬂuence of 
change in climate on ﬂooding hazards in Europe. Analytical analysis and comparison 
between GEV (Generalized Extreme Value) and Gumbel had been performed, which 
unveiled a change in the return this is certainly 100-year into the H12A2 scenario, 
aided by the degree of communication among the GEV and Gumbel ﬁts discovered 
to be around 65 percent. 
Madhuri et al. [6] detected climatic scenarios in GHMC, India, researchers applied 
ﬁve machine learning (ML) algorithms: KNN, AdaBoost, LR, SVM, and XGBoost. 
Following the assessment, XGBoost came out on top, with a mean AUC score of 
0.83. Balaji S et al. [7] proposed a model where KNN, LR, Support Vector Machine,

MLFP: Machine Learning Approaches for Flood Prediction in Odisha State
197
and MLP algorithms were among the four Machine Learning approaches applied. 
Multilayer Perceptron algorithms have efﬁcient performance linked to KNN and 
SVM Algorithms. Multilayer perceptron is discovered to be the most algorithm that 
is effective by having a 97.4% accuracy. 
Costache et al. [8] proposed a model where different Machine learning classiﬁers 
are being used which included the SVM, Random Forest, ANFIS, J48 Decision Tree, 
Artiﬁcial Neural Network, Alternating Decision Tree (ADT), etc. The six machine 
learning algorithms were trained using 70% of the total ﬂood facts that were related 
to the values of ﬂood forecasters. The RF model was the most accurate (0.973), 
while the J48 model was the least accurate (0.825). Haribabu et al. [9] proposed 
a prediction model for weather forecasting where they used SVM, LR, KNN, and 
Multilayer Perceptron are four machine learning approaches. They found that LR 
has a 95.3% accuracy, SVM has a 95.85% accuracy, KNN has a 95.85% accuracy, 
and MLP has a 97.40% accuracy. MLP outperformed the other three techniques in 
terms of accuracy. 
3 
Motivation and Objective 
Flood detection and prediction play a critical role in ﬂood management in the 
impacted area. Some existing methods employ fewer data, resulting in lower detec-
tion accuracy. The main ﬂaw in existing systems is that they don’t know which 
classiﬁer is better. Some of the systems blend Twitter photos with Artiﬁcial Intelli-
gence to determine which is more time-consuming. There’s also a method that uses 
technology in each ﬂood-prone location, with sensors monitoring ﬂood factors and 
anticipating ﬂood occurrence. The major problem with these systems is that when a 
ﬂood occurs, the network in such areas shuts down, making it impossible to collect 
new information about the area [10, 11]. In such instances, recognizing the ﬂood 
using Machine Learning and, as a result, determining the optimal classiﬁer to train 
the model plays a critical role. As a result, it must be detected with extreme preci-
sion. We employ state-of-the-art Machine Learning approaches to achieve ﬂawless 
results [12, 13]. The main objective of this paper incorporates an analysis of various 
Machine learning models for achieving improved accuracy. This technique includes: 
• Collection of Real-time rainfall data from the publicly available online database. 
• Predicting the time of occurrence of rainfall in real-time and classifying and 
training the data using different predictive learning techniques. 
• To compare the results among various Machine learning classiﬁers for achieving 
the best classiﬁer. 
• To predict and analyze the output.

198
S. Mohapatra et al.
4 
Proposed Model 
Machine Learning Techniques rely heavily on the dataset and how it is processed. 
They have a straight effect on the system’s performance and efﬁciency. The proposed 
prediction and detection system of ﬂood systems consists of the following modules 
(Fig. 1). 
4.1 
Data Preparation 
The goal is to prepare and compile rainfall statistics for Odisha’s various regions. 
The dataset is created and tagged appropriately after a systematic investigation of 
the ﬂood-affected area. The dataset we’re using for our project comes primarily from 
https://rainfall.nic.in. Rainfall records for 29 districts of Odisha from 2008 to 2021 
are included in the dataset. 
4.2 
Odisha Rainfall Dataset 
The dataset consists of 407 records and 19 features. The features are Districts, Year, 
Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sept, Oct, Nov, Dec, Annual, Jan–Feb, 
Mar–May, Jun–Sept, Oct–Dec of type string and numeric values. 
4.3 
Data Preprocessing 
Because inconsistent data might lead to incorrect conclusions, this is the most crucial 
phase for detection and prediction. Importation of Libraries Numpy and Pandas,
Fig. 1 Flowchart of the proposed methodology 

MLFP: Machine Learning Approaches for Flood Prediction in Odisha State
199
where the libraries are loaded for processing the dataset, is a crucial step. Numpy is 
the most signiﬁcant Python package for systematic computing. Pandas is a high-
performance, open-source data manipulation, and analysis package. One of the 
crucial steps is to import a dataset. The prepared data is saved as a value ﬁle in 
total which is then imported into the notebook scenario via the panda’s library. There 
is a potential that some values will be missing. As a result, assumptions about the data 
will be incorrect. The approach of removing data with missing values is inefﬁcient. 
We are using null values instead of missing values. In the data cleansing process, 
there are numerous aspects to consider. Irrelevant data columns were removed, data 
was incorrectly labeled, and the same category appeared many times. Because the 
data cleansing process has an impact on the ﬁndings, it must be carefully considered. 
4.4 
Data Visualization 
Data visualization is a critical step in machine learning, which focuses on quantitative 
data descriptions and evaluations of data. It is a powerful set of tools for gaining a 
qualitative understanding of a dataset. It can help with pattern recognition, data 
corruption, outlier detection, and much more. In more visceral plots and charts, data 
visualization can also be utilized to demonstrate critical relationships. 
4.5 
Outlier Detection Process 
The range and distribution of attribute values in the input data are known to affect 
many machine-learning techniques. The presence of outliers in the data is the prime 
factor in producing erroneous results and skewing the training method of the algo-
rithm, which may result in lengthier training periods, fewer correct models, and 
eventually poorer outcomes. Similarly, Cross-Validation is a method in which we 
need to train our model on a subset of the dataset and then test it on another subset. 
Cross-validation is beneﬁcial because it gives a more correct valuation of out-of-
sample accuracy and makes better usage of data by using each remark for both 
training and testing purposes. 
The steps are being involved in cross-validation: 
• A part of a sample dataset is set aside as a reserve. 
• To train the model, use the rest of the data. 
• Using the dataset that has been set aside, test the trained model.

200
S. Mohapatra et al.
4.6 
Dataset Splitting 
The Odisha dataset is then split into a train and test dataset. The training dataset is 
about 80% and the testing dataset is about 20%. 
4.7 
Model Selection 
The most crucial aspect of the Machine Learning Model is algorithm selection. At 
this point, we choose the algorithm that performs the best on the dataset. Floods are 
predicted using various kinds of augmented machine-learning algorithms. 
5 
Algorithms and Techniques 
5.1 
K-Nearest Neighbor (KNN) 
The KNN classiﬁer is a variable and supervised machine learning classiﬁer that helps 
in solving various types of classiﬁcations or else prediction problems and about the 
combination of a single information point based on its proximity. It is used to address 
problems consisting of regression or classiﬁcation, but it is furthermost usually used 
as a classiﬁcation method as it is built based on the assumption of ﬁnding very 
similar points at a close enough distance. It’s also worth mentioning that the KNN 
approach is part of the “lazy learning” family of simulations, which means it keeps 
a training dataset instead of going over the training process. This also indicates that 
all calculations are done when a classiﬁcation or prediction is made. 
5.2 
Support Vector Machine (SVM) 
SVM algorithm is a type of assisted learning that comes under the group of classi-
ﬁcation approach. It’s a binary classiﬁcation algorithm that uses the training dataset 
to forecast a perfect hyperplane in an n-dimensional space. Data are classiﬁed using 
SVM algorithms in both a 2-dimensional plane and a multidimensional hyperplane. 
On both linear and non-linear data, it can do classiﬁcation and regression. When the 
no. of features surpasses the no. of data points, this technique is effective.

MLFP: Machine Learning Approaches for Flood Prediction in Odisha State
201
5.3 
Decision Tree (DT) 
A decision tree is a type of supervised learning method that is making itself avail-
able to solve various problems involving classiﬁcation plus regression problems, 
though, it’s most commonly utilized to solve classiﬁcation issues. A decision tree 
technique separates a training dataset into branches, which are then further divided. 
This sequence will continue until it reaches a leaf node. The leaf node can’t be divided 
any afar. It is capable of dealing with both categorical and numerical data. It has a 
low data preparation need because it is robust to outliers. 
5.4 
Random Forest (RF) 
It is one of the machine learning techniques for classifying and predicting outcomes. 
It takes the beneﬁt of ensemble learning, which is further used for the integration 
of numerous classiﬁers to solve multifaceted difﬁculties. A random forest process is 
combined of numerous decision trees. The random forest method’s word ‘forest’ is 
learned by bagging or bootstrap combination. The “Bagging” process is an ensemble 
learning process for enhancing accurateness by a group of several machine learning 
algorithms together. It’s a valuable means of dealing with the data that are missing. 
It resolves the difﬁculty that overﬁtting of decision trees. 
5.5 
Logistic Regression (LR) 
Logistic regression is a quite common statistical approach for generating a machine 
learning algorithm with a dichotomous dependent variable. To characterize the infor-
mation and the relationship between a dependent variable and the independent vari-
ables, logistic regression is applied. Here there exists only one dependent variable 
while one or more independent variables can be a possible combination. Independent 
variables can be nominal, ordinal, or interval variables. The concept of utilizing a 
logistic function inspired the term “logistic regression”. 
5.6 
Naïve Bayes (NB) 
Naïve Bayes is a classiﬁcation method purely based on Bayes’ Theorem which is 
otherwise known as the forecaster’s individuality assumption. In simple terms, a 
Naive Bayes classiﬁer is nothing but the inﬂuence or support in terms of the existing 
features in a data set. The Naive Bayes model is easy to form and works ﬁne with a 
huge dataset. As of its easiness, it is also acknowledged as outpacing uniformly along

202
S. Mohapatra et al.
with the furthermost effective classiﬁcation algorithm. The posterior probability of 
an event (A) is determined using Bayes’ theorem, as shown in Eq. 1, particularly a 
few past probabilities of event B denoted by P (A|B). 
upper P  l eft par enthesis upper A vertical bar upper B right parenthesis equals StartFraction upper P left parenthesis upper B vertical bar upper A right parenthesis asterisk upper P left parenthesis upper A right parenthesis Over upper P left parenthesis upper B right parenthesis EndFraction
upper
 P left parenthesis upper A vertical bar upper B right parenthesis equals StartFraction upper P left parenthesis upper B vertical bar upper A right parenthesis asterisk upper P left parenthesis upper A right parenthesis Over upper P left parenthesis upper B right parenthesis EndFraction
6 
Performance Metrics Analysis 
True Positive: When the model properly predicts the positive class, this is an outcome. 
When the system accurately predicts that an occurrence has occurred, the outcome 
is deemed true positive. 
True Negative: When the model properly forecasts the negative class, this is the 
result. When the algorithm properly predicts that a speciﬁc incident will not occur, 
the outcome is deemed true negative. 
False Positive: False Positive is a measure of accuracy in which the model predicts 
the positive class incorrectly. When the system is unable to correctly forecast that an 
incident has occurred, the result is classiﬁed as False Positive. 
False Negative: False Negative is an accuracy value that indicates that the model 
predicted the negative class incorrectly. When the system is unable to correctly 
anticipate that an incident has not occurred, the result is classiﬁed as False Negative. 
6.1 
Accuracy 
One of the criteria used to evaluate the classiﬁer is accuracy. It’s the proportion of 
correct predictions compared to the total no. of sample inputs. It is deﬁned as 
bold ital i
c up per A bold italic c bold italic c bold italic u bold italic r bold italic a bold italic c bold italic y equals StartFraction left parenthesis bold upper T bold upper P plus bold upper T bold upper N right parenthesis Over left parenthesis bold upper T bold upper P plus bold upper F bold upper P plus bold upper T bold upper N plus bold upper F bold upper N right parenthesis EndFraction
bold  ital ic up per A 
bold italic c bold italic c bold italic u bold italic r bold italic a bold italic c bold italic y equals StartFraction left parenthesis bold upper T bold upper P plus bold upper T bold upper N right parenthesis Over left parenthesis bold upper T bold upper P plus bold upper F bold upper P plus bold upper T bold upper N plus bold upper F bold upper N right parenthesis EndFraction
If the dataset contains a major class imbalance, then using accuracy as the main 
metric will not do well, but it performs better in the case of equal samples belonging 
to each class. 
6.2 
Precision 
Precision is nothing but the basic ratio between the accurately projected positive 
observations and the total predicted positive observations. It is formulated as:

MLFP: Machine Learning Approaches for Flood Prediction in Odisha State
203
bold italic  
upper P bold italic r bold italic e bold italic c bold italic i bold italic s bold italic i bold italic o bold italic n equals StartFraction bold upper T bold upper P Over left parenthesis bold upper T bold upper P plus bold upper F bold upper P right parenthesis EndFraction
bold  itali
c upper P bold italic r bold italic e bold italic c bold italic i bold italic s bold italic i bold italic o bold italic n equals StartFraction bold upper T bold upper P Over left parenthesis bold upper T bold upper P plus bold upper F bold upper P right parenthesis EndFraction
6.3 
Recall 
In the confusion matrix, Recall can be deﬁned as the proportion of correctly antici-
pated positive values compared to the positive actual class. The Recall value can be 
calculated by the formula given in Eq. (4). 
bold it a
lic upper R bold italic e bold italic c bold italic a bold italic l bold italic l equals StartFraction bold upper T bold upper P Over left parenthesis bold upper T bold upper P plus bold upper F bold upper N right parenthesis EndFraction
bold  itali
c upper R bold italic e bold italic c bold italic a bold italic l bold italic l equals StartFraction bold upper T bold upper P Over left parenthesis bold upper T bold upper P plus bold upper F bold upper N right parenthesis EndFraction
6.4 
F-Score 
The F1 score otherwise known as the F-Score is the average of both precision and 
recall values. It’s a statistical metric for evaluating the classiﬁer’s performance. When 
precision and recall must be balanced when class distribution is uneven, the F1 score 
can be employed. F1 score is deﬁned as 
bold ita lic upper F Baseline 1 bold italic upper S bold italic c bold italic o bold italic r bold italic e equals StartFraction 2 bold asterisk bold upper R bold e bold c bold a bold l bold l bold asterisk bold upper P bold r bold e bold c bold i bold s bold i bold o bold n Over bold upper R bold e bold c bold a bold l bold l plus bold upper P bold r bold e bold c bold i bold s bold i bold o bold n EndFraction
bold it alic upper F
 Baseline 1 bold italic upper S bold italic c bold italic o bold italic r bold italic e equals StartFraction 2 bold asterisk bold upper R bold e bold c bold a bold l bold l bold asterisk bold upper P bold r bold e bold c bold i bold s bold i bold o bold n Over bold upper R bold e bold c bold a bold l bold l plus bold upper P bold r bold e bold c bold i bold s bold i bold o bold n EndFraction
7 
Results and Discussion 
The proposed work is a way to evaluate the Odisha rainfall dataset to predict the ﬂood 
using six machine learning techniques. Figure 2 shows the average annual rainfall of 
Odisha state among which Mayurbhanj district has the highest record of 39,880.46 
and Boudh district has the lowest record of 4021.65.
Figure 3 shows the monthly rainfall of Odisha state in which July and August 
month has the highest rainfall records.
The TP, TN, FP, and FN values of the different algorithms namely KNN, RF, 
SVM, DT, NB, and LR algorithms are compared in Table 1.
The Precision, Recall, and F1 Score values of the KNN, SVM, DT, RF, LR, and 
NB algorithms are compared in Table 2.

204
S. Mohapatra et al.
Fig. 2 District-wise average 
annual rainfall of Odisha 
state
Fig. 3 Monthly rainfall of 
Odisha state
Table 1 Parameters of the confusion matrix are compared 
Algorithms
TP
TN
FP
FN 
KNN
51
27
3
1 
SVM
52
28
2
0 
DT
43
24
11
4 
DF
51
26
3
2 
LR
53
28
1
0 
NB
50
23
4
5
Table 2 Analysis of various 
machine learning algorithms
Algorithms
Precision
Recall
F1 Score 
KNN
0.900
0.964
0.931 
SVM
0.933
1.000
0.966 
DT
0.686
0.857
0.762 
RF
0.897
0.929
0.912 
LR
0.966
1.000
0.982 
NB
0.852
0.821
0.836

MLFP: Machine Learning Approaches for Flood Prediction in Odisha State
205
Table 3 Accuracy score of 
different machine learning 
algorithms 
Algorithms
Accuracy 
KNN
95.1% 
SVM
97.6% 
DT
86.6% 
RF
93.9% 
LR
98.8% 
NB
89.0% 
Fig. 4 Graphical 
representation of accuracy of 
different machine learning 
algorithms 
The accuracy of the KNN, DT, SVM, RF, NB and LR methods are compared 
in Table 3. We can deduce from the obtained data that Logistic Regression has the 
highest accuracy score among all other algorithms (Fig. 4). 
8 
Conclusion and Future Work 
Flood detection and prediction are extremely complex and challenging processes. 
But ﬂood prediction models help to predetermine ﬂood events. By developing these 
models, meteorological authorities and disaster management teams can control 
upcoming ﬂoods, and will help in the faster evacuation of the affected people to 
safer places. The proposed methodology will be helpful in evaluating the various 
performances of the model to determine the most appropriate predictive model. The 
developed model mainly focused on the Rainfall data of different districts of Odisha 
state. This study compares the accuracy score of KNN, SVM, Naïve Bayes, Decision 
Tree, LR, and RF for ﬂood detection and prediction. The results of this study indi-
cate that Logistic Regression is the most effective machine learning technique with 
an accuracy score of 98.8%. In the future, for better prediction of ﬂoods, we need 
to implement various satellite images along with more different machine learning 
algorithms.

206
S. Mohapatra et al.
References 
1. Zhou Y, Cui Z, Lin K, Sheng S, Chen H, Guo S, Xu CY (2022) Short-term ﬂood probability 
density forecasting using a conceptual hydrological model with machine learning techniques. 
J Hydrol 604:127255 
2. Munawar HS, Hammad AW, Waller ST (2022) Remote sensing methods for ﬂood prediction: 
a review. Sensors 22(3):960 
3. Ghorpade P et al (2021) Flood forecasting using machine learning: a review. In: 2021 8th 
international conference on smart computing and communications (ICSCC), pp. 32–36. IEEE 
4. Rao KHVD, Rao VV, Dadhwal VK, Behera G, Sharma JR (2011) A distributed model for 
real-time ﬂood forecasting in the Godavari basin using space inputs. Int J Disaster Risk Sci 
2(3):31–40 
5. Dankers R, Feyen L (2008) Climate change impact on ﬂood hazard in Europe: An assessment 
based on high-resolution climate simulations. J Geophys Res Atmos 113(D19) 
6. Madhuri R, Sistla S, Srinivasa Raju K (2021) Application of machine learning algorithms for 
ﬂood susceptibility assessment and risk management. J Water Clim Change 12(6):2608–2623 
7. Baalaji S, Sandhya S (2020) Flood prediction system using multilayer perceptron classiﬁer and 
neural networks. Int Res J Eng Technol 7(5):6245–6254 
8. Costache R, Arabameri A, Elkhrachy I, Ghorbanzadeh O, Pham QB (2021) Detection of areas 
prone to ﬂood risk using state-of-the-art machine learning models. Geomat Nat Haz Risk 
12(1):1488–1507 
9. Haribabu S, Gupta GS, Kumar PN, Rajendran PS (2021) Prediction of ﬂood by rainf all 
using MLP classiﬁer of neural network model. In: 2021 6th international conference on 
communication and electronics systems (ICCES), pp. 1360–1365. IEEE 
10. Motta M, de Castro Neto M, Sarmento P (2021) A mixed approach for urban ﬂood prediction 
using Machine Learning and GIS. Int J Disaster Risk Reduc 56:102154 
11. Sankaranarayanan S, Prabhakar M, Satish S, Jain P, Ramprasad A, Krishnan A (2020) Flood 
prediction based on weather parameters using deep learning. J Water Clim Change 11(4):1766– 
1783 
12. Mane P, Katti M, Nidgunde P, Surve A (2020) Early ﬂood detection and alarming system using 
machine learning techniques. Int J Res Eng Sci Manag 3(10):29–32 
13. Munawar HS, Hammad A, Ullah F, Ali TH (2019) After the ﬂood: a novel application of image 
processing and machine learning for post-ﬂood disaster management. In Proceedings of the 
2nd international conference on sustainable development in civil engineering (ICSDC 2019), 
Jamshoro, Pakistan, pp 5–7

Vision-Based Cyclist Travel Lane 
and Helmet Detection 
Jyoti Madake, Shripad Bhatlawande, and Madhusha Shete 
Abstract Cycling is an integral part of daily life for many people. This project 
presents a vision-based cyclist travel lane and helmet detection system. This system 
can serve as surveillance to detect whether the cyclist is traveling in a devoted lane and 
wearing a helmet for safety measures. The model involves the application of scale-
invariant feature extraction (SIFT) algorithm for feature description. The detection 
method is based on six machine-learning classiﬁcation algorithms. The classiﬁers 
are evaluated based on testing accuracy, and the best classiﬁer is selected for the 
ﬁnal model creation. The random forest classiﬁer provided highest training accuracy 
of 99% and testing accuracy of 85.44% for cyclist travel lane detection. The same 
classiﬁer provided the highest training accuracy of 99.53% and testing accuracy of 
87.83% for cyclists’ helmet detection. In future, this system can also serve as a small 
part of autonomous driver assistance systems by detecting the right lane. 
Keywords Cyclist travel lane detection · Helmet detection · SIFT · PCA ·
Machine learning 
1 
Introduction 
Bicycles have been a major part of transportation sources for ages. In the era of fancy 
cars and speedy motorcycles, many people still use bicycles for transportation and 
other purposes. Riding a bicycle is an agreeable as well as essential part of day-to-day 
life for individuals of any age group and capacity and is advantageous in many ways. 
Hence, cyclist travel lanes are being made to give separate space to cyclists from cars
J. Madake · S. Bhatlawande · M. Shete envelope symbol
Department of E&TC Engineering, VIT Pune, Pune 411037, India 
e-mail: madhusha.shete19@vit.edu 
J. Madake 
e-mail: jyoti.madake@vit.edu 
S. Bhatlawande 
e-mail: shripad.bhatlawande@vit.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_19 
207

208
J. Madake et al.
and motorcycles on heavy-trafﬁc roads. These lanes are also referred to as protected 
cyclist lanes. This evolution toward cyclist travel lanes started when Seattle came up 
with the Bicycle Master Plan (BMP) in 2013 [15]. This drive aspired to encourage and 
let people to ride a bicycle. Since then many countries are making cyclist travel lanes 
a part of their road structure. There are a few cities in India too who are having these 
cyclist travel lanes. These lanes not only help to control trafﬁc by parting cyclists in 
the other part of the road but also decrease the number of accidents that are caused 
due to the crashing of high-speed vehicles and comparatively low-speed cycles. But 
using cyclist travel lanes does not ensure the complete safety of cyclists. Cyclists 
must wear helmets to ensure their safety. According to government reports [17], it 
has been noted that many bicycle and bike users do not use helmets while riding. It 
is also necessary to check the presence of a helmet making it mandatory. Multiple 
such systems are being implemented by making use of disciplines such as machine 
learning algorithms. 
The proposed system promotes the presence of cyclist travel lanes, increasing the 
number of these lanes in every city of India and making people aware of it. In vision-
based cyclist travel lane and helmet detection, the system detects whether the cyclist 
is using the cyclist travel lane or normal road for cycling along with the detection 
of the helmet. The objective is to detect the presence of a cyclist travel lane with 
the help of parameters such as cyclist lane signs, cyclist travel lane structure, and 
appearance. It is mandatory to use a helmet even if the cyclist is using a protected 
lane for cycling. This project also detects the presence of helmets on cyclists’ heads. 
Two classiﬁers are implemented to achieve the mentioned objectives. One of them 
classiﬁes helmet and non-helmet inputs while the other classiﬁes cyclist lanes and 
normal roads. 
2 
Literature Survey 
This project aims to detect if a cyclist is traveling on a cyclist travel lane and wearing a 
helmet. The literature review presents an overview of the previously published works 
on the detection of cyclist travel lanes and helmets using OpenCV and other different 
algorithms. A new method for cyclist recognition in real time was presented in [1]. A 
cascaded detector was used. A histogram of gradients (HOG) features and an SVM 
classiﬁer was implemented. The training was carried out on an open-source KITTI 
dataset containing images of bikes, cyclists, and pedestrians. The SVM classiﬁer 
was implemented for training. The model was able to detect the cyclist in 0.32 s 
on video input. A UB-MPR dependent recognition proposition technique presented 
in [2] was a Fast R-CNN-based algorithm. The proposed strategy could recognize 
people on foot and cyclists simultaneously and separate them. The method proposed 
in [3] presented a helmet detection system wherein a video was captured through 
the camera and the frames were extracted using a Caffe model. Object detection 
was performed on the live video. The SVM-based model had an accuracy of 86% 
and a validation accuracy of 76%. A computer vision-based system was proposed in

Vision-Based Cyclist Travel Lane and Helmet Detection
209
[4]. The segmentation was done for moving objects followed by classiﬁcation and 
detection. In the ﬁrst step adaptive moving gaussian was applied to segment moving 
objects from the background. The techniques implemented included the selection 
of the region of interest (ROI), circular Hough transforms (CHT), HOG, and Local 
binary pattern (LBP). The model could result in 91.37% accuracy. 
A real-time automated system was proposed in [5]. The system could detect 
motorcycles with an accuracy of 99.5% with a sequential minimum optimization 
(SMO) classiﬁer. For helmet detection, experimental results showed 96.98% accu-
racy with the Logistic classiﬁer and an accuracy of 99.62% with the Custom convo-
lution neural network (CNN) classiﬁer. The study in [6] developed a safety as well 
as a security-based system to avoid outside bike riding accidents. The ultimate aim 
was to detect helmets. The data was collected from outdoor road trafﬁc in video 
format. This model proposed a multiclass kernel-based SVM classiﬁcation for deter-
mining humans with or without a helmet. Data preprocessing algorithms such as 
gaussian mixture modeling (GMM) background modeling, noise removal, thresh-
olding, and morphological ﬁltering were implemented. The classifying model could 
yield 96.67% accuracy. The proposed method in [7] presented methods such as 
aggregated channel features, deformable part model, region-based CNN, and stereo 
proposal-based fast R-CNN for cyclist detection. The dataset was collected from 
a camera capture of 22,161 images. Testing was done over three difﬁculty levels 
namely, moderate and hard wherein hard gave good accuracy for cyclists with less 
than 80% occlusion, and moderate managed for less than 40% occlusion, and needs 
it to be fully visible. 
The implementation in [8] presented a helmet detection model based on YOLOv5. 
The model was trained on 6045 images which were labeled as ’Alarm’ if the helmet 
is not there and ’Helmet’ if the helmet is there. The method trained and evaluated 
YOLOv5 (s, m, l, x) models with different depths and widths and compared them. 
The average detection speed of YOLOv5s reached 110 FPS. The method presented 
in [9] was a helmet detection model using YOLO V3. This model also presented 
the improved version of YOLO V3 full regression deep neural network architec-
ture. The improved version could detect helmets in all possible conditions with 
low image resolution. The data was collected from the internet as well as camera 
capture and trained over 1500 epochs. The helmet detection approach introduced in 
[10] provided a hybrid descriptor for feature extraction based on LBP, HOG, and 
the Hough transform descriptor. The methodology included background detection 
followed by moving object segmentation. Future extraction was carried out using 
LBP. The SVM classiﬁer was implemented. The model had a training accuracy of 
97.67% and a testing accuracy of 94.23%. Cyclist path prediction carried out in [11] 
implemented RNN and Gaussian distribution. The dataset was composed of 51 videos 
of cyclists. The decision of cyclists to turn left or move ahead was made using this 
model. The model had an average error of 33 cm while predicting the next position of 
the cyclist. Research in [12] promoted the use of local road topology. The implemen-
tation in [13] presented a stud that compared two Kalman Filter (KF) techniques for 
predicting cyclists’ trajectories at a junction. Least-squares approximation method 
was used to predict the cyclist’s future position. A multilayer perceptron ANN model

210
J. Madake et al.
was implemented. The model could give the detection results in 1 to 2.5 s on video 
data. 
The study proposed in [14] had two models for cyclist trajectory predictions. 
Motion Modelling Approach and data-driven Approach based on stacked long short-
term memory (LSTM) were implemented. The LSTM and RNN-based model could 
give detection results in 3 s on video data. Stochastic gradient descent (SGD) opti-
mization algorithm was implemented in [15] for object detection. The research 
concluded that integrating SGD into the optimization process results in improved 
detection speed. The training was done on a data set containing 15,000 images. The 
model was trained using algorithms such as libSVM, SVMLight, and SGD. The 
study in [16] showed that the number of bicyclists who use helmets while riding 
was very low. The survey was carried out to know the number of male and female 
cyclists wearing helmets. The outcomes of the study in [17] stated that cyclists had 
a lack of attention while driving. A framework for the detection of cyclists as well 
as pedestrians was proposed in [19]. A fast R-CNN algorithm was implemented. 
The model had 89% test accuracy. The LSTM-based algorithm was proposed in [20] 
for the detection of cyclist paths. The LSTM model was able to detect the required 
output in 15.6 ms. The object detection algorithm was proposed in [21]. The HOG 
feature extraction technique was followed by PCA. The detector model was trained 
using SVM. SGD algorithm was implemented for optimization. 
3 
Methodology 
A simpliﬁed method for the detection of cyclist travel lanes and helmets is proposed 
in this paper. This vision-based system is mainly divided into two different detectors. 
The ﬁrst detector detects whether the cyclist in the input source is traveling on the 
dedicated cyclist travel lane. The second detector detects whether the cyclist in the 
previous input source has a helmet on his/her head. These two models are clubbed 
in order to make a system for the detection of cyclist travel lanes and helmets. 
Figure 1 presents the block diagram of the proposed system. An image of a cyclist 
is taken as an input source to the system. The inputted image is pre-processed for 
further use using methods such as resizing and gray scaling. The pre-processed 
image is passed to the SIFT feature detector and descriptor for the feature extraction 
purpose. The extracted features are clustered using the K-Means clustering algorithm. 
The principal component analysis is carried out on the inputted data to reduce the 
dimensions to ensure superior computational complexity. The generated data is sent 
to the two different models. The ﬁrst model aims to detect the features relevant to 
the cyclist’s travel lane using a random forest classiﬁer and the other model aims to 
determine the presence of a helmet on the cyclist’s head using an SVM classiﬁer. 
The random forest and SVM classiﬁers are selected by comparing all implemented 
classiﬁers based on their test accuracy as mentioned below in the paper.

Vision-Based Cyclist Travel Lane and Helmet Detection
211
Fig. 1 Block diagram for cyclist travel lane and helmet detection system 
3.1 
Dataset and Preprocessing 
The vision-based cyclist travel and helmet detection system is trained and tested on 
a custom dataset containing 17,000 images. These images belong to four distinct 
classes: cyclist lane, non-cyclist lane, cyclist with a helmet, and cyclist without a 
helmet. The images for the dataset compilation were collected from both the internet 
and camera capture. 
Table 1 illustrates the classes and the total number of images in each class. The 
images in the cyclist lane class included the images of cyclist travel lanes in India 
as well as abroad and the signs used to indicate the cyclist travel lanes. The non-
cyclist lane class was composed of images showcasing normal road trafﬁc with 
different motorbikes, cars, and other vehicles. The cyclist with helmet class contained 
images of cyclists wearing helmets along with some helmet or hard hat images. 
These images encompassed cyclists and helmets from all possible angles. The cyclist 
without helmet class was composed of images of cyclists who were not wearing 
helmets and some bare-head images of people doing various activities on the road. 
In the real-time surveillance system, the cyclist’s travel lane and helmet scanning are 
mostly done by the cameras mounted on the poles on the side of the roads. 
All images were transformed to grayscale while pre-processing. The main inten-
tion behind the gray scaling of images was to simplify the algorithm. The gray images 
are easier for the feature description operations in comparison with colored images
Table 1 Dataset details 
Sr. No
Class
No. of images 
1
Cyclist lane
1500 
2
Non-cyclist lane
1500 
3
Cyclist with helmet
7000 
4
Cyclist without helmet
7000 
Total
17,000 

212
J. Madake et al.
Fig. 2 Sample dataset 
images 
and they require less processing time. The images were resized during pre-processing. 
Images from the cyclist lane and non-cyclist lane were resized to 200 X 200 pixels. 
Cyclists with helmets and cyclists without helmet classes were having images with 
dimensions equal to 150 X 150 pixels after pre-processing. The pre-processed images 
were fed to the feature descriptor for the extraction of features. 
Figure 2 represents the samples of the images in the proposed dataset. Images 
A, B, C, and D belong to the cyclist lane, non-cyclist lane, cyclist with helmet, and 
cyclist without helmet classes respectively. 
3.2 
Feature Extraction and Dimensionality Reduction 
The principal goal of any feature descriptor algorithm is to take an input image and 
provide the output stating the pixel coordinates i.e. location of important portions in 
the image. The proposed method uses a SIFT feature descriptor for the extraction of 
features. The SIFT feature consists of some key points in the image, each with an 
orientation and a descriptor for the area surrounding the key points. The SIFT method 
reﬁnes the location and scale of feature points or key points after determining their 
approximate location and scale. The algorithm then derives the descriptors for each 
key point by evaluating the orientation(s) for each key point. There are a couple of 
reasons for selecting SIFT for feature extraction purposes. The ﬁrst reason is that the 
size or orientation of the image does not affect SIFT features. This is advantageous 
for applications where any raw image captured from an unusual angle is fed to the 
descriptor. 
The k-Means algorithm is used for the clustering of the data that is extracted 
using SIFT. K-Means is an unsupervised learning-based algorithm used to address 
clustering problems. It follows a centroid-based strategy. The clusters formed using 
K-Means have unique centroids. This technique is used to minimize the sum of 
distances between key points and the clusters that they belonged to. K-Means is 
implemented to complete two basic tasks. In the ﬁrst task, an iterative technique

Vision-Based Cyclist Travel Lane and Helmet Detection
213
is implemented to get the best value for K center points. In the second task, each 
data point was assigned to the K-valued center that was closest to it. The data points 
that are close to speciﬁc k-center forms the cluster. The value of K was decided 
using a method named elbow. This method is used to ﬁnd the optimal number of 
clusters. Calculations for Within Cluster Sum of Squares (WCSS) are carried out in 
this method using Eq. 1. WCSS determines the total variations within a cluster. 
Start L
a
you t 1 st Row W CS S equals sigm a sum
m
a
tio n d a in Clu st er 1 dist ance  left  pa r e nt h
e
s
is Di upper C 1 right par enth esis 
2 2nd Row plus sigma summation di in Cluster 2 distance left parenthesis Di upper C 2 right parenthesis 2 plus ellipsis period period 3rd Row plus sigma summation di in Cluster n distance left parenthesis Di Cn right parenthesis 2 EndLayout
The d mentioned in above Eq. 1 is the data point. C is the centroid. The WCSS 
values are calculated based on the above-mentioned formula. The inbuilt library 
based on this formula was used in the code. The elbow method executed K-Means 
clustering on the dataset for different values of K ranging from 1 to 15. The WCSS 
value of each K value was compared to select the best K value. The value of K was 
taken as 5 for the helmet detection model and 9 for the cyclist lane detection model. 
Figure 3 represents the workﬂow of the K-Means algorithm for cyclist travel lane 
detection. The clustering algorithm was carried out for 9 clusters separately on the 
cyclist lane and non-cyclist lane images and then combined. 
Figure 4 represents the workﬂow of the K-Means algorithm for cyclist helmet 
detection. The clustering algorithm was carried out for 5 clusters separately on the 
cyclist with helmet and cyclist without helmet images and then combined.
The clustered data was further processed for standardization as building algo-
rithms with big datasets involves scaling of features, which is considered an important 
step for getting better accuracy. The data outputted by SIFT and K-Means clustering 
algorithm is scaled using a standard scalar function. The standardization technique 
converts the data to a scale ranging from -1 to 1 by removing the mean and scaling 
the unit variance. The standard score of a data point x can be calculated by Eq. 2. 
z eq ual s left
 parenthesis x minus u right parenthesis slash s
Fig. 3 K-Means block diagram for cyclist travel lane detector 

214
J. Madake et al.
Fig. 4 K-Means block diagram for cyclist’s helmet detector
The parameters u and s mentioned in the above formula are the mean and standard 
deviation of the training samples. The principal component analysis was implemented 
on the scaled data for dimensionality reduction. The dimensionality of huge datasets 
is reduced by transforming a large set of variables into a smaller set by using PCA. 
The method preserves the majority of the information in the larger set. The PCA 
involves the computation of the covariance matrix of standardized data. A covariance 
matrix depicts the covariance among each pair of data elements of a random vector in 
probability theory and statistics. The Eigenvectors and Eigenvalues of the covariance 
matrix are calculated for determining the principal components of data. The feature 
vector is a matrix that has columns containing eigenvectors of the components that 
we decide to keep. The ﬁnal dataset is achieved after PCA transformation. The ﬁnal 
dataset can be described as: 
Final dataset eq uals left  parenth es is Feature vecto r right p arenthesis upper T asterisk left parenthesis Standardized original dataset right parenthesis upper T

Vision-Based Cyclist Travel Lane and Helmet Detection
215
Algorithm 1 deﬁnes the workﬂow of the feature extraction and dimensionality 
reduction phase. This algorithm is used for extracting features of 4 different classes 
belonging to 2 models. The speciﬁcations of cluster (K) values and the number of 
PCA components are different for the 2 models. The helmet detection model has a 
K value equal to 5 and all 5 PCA components were taken into consideration while 
creating the ﬁnal dataset for the helmet detection model. The cyclist lane detection 
model has K values equal to 9 but only 6 PCA components were considered while 
creating the ﬁnal dataset for the cyclist lane detection model. 
3.3 
Classiﬁcation and Analysis 
The classiﬁcation was done using multiple algorithms for both cyclist lane detection 
and helmet detection. The algorithms such as decision tree, random forest, KNN, 
SVM, GNB, and AdaBoost were implemented on training data. 
Figure 5 showcases the ﬂowchart of the proposed methodology for training the 
dataset. The input data which is data taken from the output of PCA is divided into 
train data and tests before passing it to the classiﬁer. The training and testing images 
were taken such as there were 80% training images and 20% testing images. The 
classiﬁcation result of all the algorithms is compared based on test accuracy and the 
best accuracy giving models for both the cyclist lane detector and cyclist’s helmet 
detector are taken into consideration while making a system. The algorithm for the 
proposed system is mentioned below. 
Fig. 5 Flowchart for training of model

216
J. Madake et al.
Algorithm 2 showcases the workﬂow of a vision-based cyclist travel lane and 
helmet detection system. The input source image is given to the detector function. 
The required pre-processing and feature extraction strategy that includes gray scale 
transformation, resize operation, and SIFT-based feature description is implemented 
on the input image. The data frame of the input image is created by applying K-
Means clustering with cluster numbers equal to the cluster value used while training 
the model. After feature extraction, PCA is executed on the extracted features. The 
resultant data frame is passed to the trained models for the prediction of respective 
classes. Firstly, the cyclist’s travel lane is detected using a random forest model and 
then the helmet is detected using the SVM model. The desired output for vision-based 
cyclist travel lane and helmet detection is examined using these two above-mentioned 
predictions. 
The evaluation of implemented classiﬁers was carried out based on precision, 
recall, and F1 score. The percentage of accurately predicted positive values (TP) to 
the total number of positive values (FP + TP) is deﬁned as precision as given in 
Eq. 3. In Eq.  4, Recall is deﬁned as the proportion of accurately predicted positive 
values to the entire absolute class (FN + TP). The F1 score is computed by taking 
the weighted mean of the accuracy and recall values in Eq. 5. 
Precision Score eq uals TP  s las
h left parenthesis FP plus TP right parenthesis
Recall Score eq uals TP  s las
h left parenthesis FN plus TP right parenthesis
upp er F 1  Sco re  equals 2 asteri sk  Precis ion Score asteris k Reca ll  Score slash l
eft parenthesis Precision Score plus Recall Score slash right parenthesis

Vision-Based Cyclist Travel Lane and Helmet Detection
217
Table 2 Evaluation of cyclist travel lane detector 
Sr. No
Classiﬁer
Train accuracy
Test accuracy
Precision
Recall
F1 Score 
1
Random Forest
99%
85.44%
0.84
0.97
0.90 
2
Decision Tree
97.25%
83.22%
0.84
0.93
0.88 
3
KNN
76.08%
73%
0.76
0.88
0.82 
4 
Performance Evaluation 
The above-mentioned classiﬁers were tested on the data belonging to the 4 distinct 
classes and 2 different models. The performance evaluation of the cyclist travel lane 
detector model and cyclist’s helmet detector model were carried out separately. The 
below-mentioned two sections represent the evaluation of each classiﬁer belonging 
to the two distinct detectors. 
4.1 
Cyclist Travel Lane Detector 
The cyclist travel lane detector consists of a classiﬁcation of images based on two 
classes: cyclist lane and non-cyclist lane. The cyclist travel lane class is labeled as ‘0’ 
and the non-cyclist lane class is labeled as ‘1’. The classiﬁers such as decision tree, 
random forest, and KNN were implemented and tested on the images belonging to 
the two above-mentioned classes. The three classiﬁers were evaluated and compared 
based on their testing accuracy. 
Table 2 showcases the performance analysis of the three classiﬁers trained for the 
detection of cyclist travel lanes. The random forest could give the highest training 
accuracy, testing accuracy, precision, recall, and F1 score followed by the decision 
tree and KNN. 
4.2 
Cyclist’s Helmet Detector 
The cyclist’s helmet detector implies the classiﬁcation of images based on two 
classes: cyclist with a helmet and cyclist without a helmet. The cyclist with helmet 
class is labeled as ‘0’ and the cyclist without helmet class is labeled as ‘1’. The 
classiﬁers such as decision tree, random forest, KNN, GNB, AdaBoost, and SVM 
were implemented and tested on the images belonging to the two above-mentioned 
classes. The SVM classiﬁer was implemented using 4 different kernels. Linear, poly-
nomial, RBF, and sigmoid kernels were implemented in SVM. The six classiﬁers 
were evaluated and compared based on their testing accuracy. 
Table 3 displays the performance analysis of the six classiﬁers trained for the 
detection of cyclists’ helmets. The random forest could give the highest training and

218
J. Madake et al.
testing accuracy followed by the decision tree and SVM. The SVM classiﬁer using 
a polynomial kernel could give notable precision, recall, and F1 score followed by 
KNN and random forest. 
The comparison of the proposed system and the existing systems for cyclist lane 
and helmet detection is illustrated in table 4. The model for vision-based cyclist 
travel lane and helmet detection was implemented in Jupyter notebook IDE on an 
Intel Core i5-7200U CPU with a 2.50 GHz processor and windows 10 operating 
system. The algorithm giving the highest testing accuracy in both cyclist travel lane 
detector and helmet detector is taken into consideration for building the vision-based 
cyclist travel lane and helmet detection system. The random forest algorithm having 
a test accuracy of 85.44% for cyclist travel lane detection and 87.83% for cyclist’s 
helmet detection was used for cyclist helmet detection. 
5 
Conclusion and Future Scope 
Cyclist travel lanes are being constructed to provide a distinct place for cyclists 
from vehicles and motorcycles on congested roadways. These are also known as 
protected cyclist lanes. The main motive of this paper was to classify whether a 
cyclist is traveling in his/her lane and wearing a helmet for safety purposes. Catego-
rization of whether or not the cyclist is traveling in a speciﬁed lane is made which 
follows the decision of whether the cyclist is wearing a helmet or not. The important 
features in the dataset for travel lane and helmet detection were collected using SIFT. 
The comparison of binary classiﬁcation of both the cyclist travel lane and cyclist’s 
helmet model was carried out using machine learning algorithms such as KNN, deci-
sion tree, random forest, AdaBoost, and GMV. The random forest classiﬁer had the 
highest training accuracy of 99% and testing accuracy of 85.44% for cyclist travel 
lane detection, while for cyclist helmet detection, the random forest had the highest
Table 3 Evaluation of cyclist’s helmet detector 
Sr. No
Classiﬁer
Train accuracy
Test accuracy
Precision
Recall
F1 Score 
1
Decision Tree
99%
83.86%
0.83
0.84
0.84 
2
Random Forest
99.53%
87.83%
0.86
0.86
0.88 
3
KNN
89.23%
88.41%
0.86
0.91
0.89 
4
GNB
85.50%
84.08%
0.81
0.88
0.85 
5
AdaBoost
85.82%
84.97%
0.83
0.87
0.85 
6
SVM 
a) Linear:
85.49%
84.44%
0.81
0.89
0.85 
b ) Polynomial:
84.99%
85.33%
0.89
0.80
0.84 
c) RBF:
91.12%
88.66%
0.87
0.91
0.89 
d) Sigmoid:
59.83%
59.62%
0.59
0.60
0.60

Vision-Based Cyclist Travel Lane and Helmet Detection
219
Table 4 Comparison of proposed system with existing systems 
Ref. No
Approach
Hardware
Run time
Accuracy 
[1]
SVM
a computer with 
8 cores of 
2.7 GHz and a 
memory of 
8 GB  
0.28 s
-
[3]
CNN
Windows 10, 
NVIDIA 
GEFORCE 940 
MX 
moderate time
80% 
[20]
LSTM + RNN
GeForce GTX 
1660 GPU 
15.6 ms
91% 
Proposed system
SIFT + Random 
forest 
Intel Core 
i5-7200U CPU 
with 2.50 GHz 
processor, 
Windows 10 
Moderate time
Cyclist lane 
detection: 
85.44% 
Helmet detection: 
87.83% 
training accuracy of 99.53% and testing accuracy of 87.83%. Even though this model 
produces a good result, factors such as better image quality and a larger number of 
images can help to improve the model’s accuracy in future. Neural networks based 
algorithm can also improve the accuracy in future. This system can be further used 
in real-time surveillance and autonomous driver assistance system, for which the 
model must be trained with high-quality images.
References 
1. Tian W, Lauer M (2015) Fast cyclist detection by cascaded detector and geometric constraint. 
In: 18th IEEE international conference on intelligent transportation systems, pp 1286–1291 
2. Li X et al. (2017) A uniﬁed framework for concurrent pedestrian and cyclist detection. IEEE 
Trans Intell Transp Syst 18(2):269–281 
3. Rohith CA, Nair SA, Nair PS, Alphonsa S, John NP (2018) An efﬁcient helmet detection 
for MVD using deep learning. In: 3rd international conference on trends in electronics and 
informatics, pp 282–286 
4 e Silva, RRV, Aires KRT, Veras R d M S (2014) Helmet detection on motorcyclists using image 
descriptors and classiﬁers. In: 27th SIBGRAPI conference on graphics, patterns and images, 
pp 141–148 
5. Shine LCVJ (2020) Automated detection of helmet on motorcyclists from trafﬁc surveillance 
videos: a comparative analysis using hand-crafted features and CNN. Multim Tools Appl 
79:14179–14199 
6. Kumar N, Sukavanam N (2018) Detecting helmet of bike riders in outdoor video sequences 
for road trafﬁc accidental avoidance. In: Springer - Advances in Intelligent Systems and 
Computing, vol 941. pp 24–33. (2018) 
7. Li X et al. (2016) A new benchmark for vision-based cyclist detection. In: IEEE intelligent 
vehicles symposium, pp 1028–1033

220
J. Madake et al.
8. Zhou F, Zhao H, Nie Z (2021) Safety helmet detection based on YOLOv5. In: IEEE international 
conference on power electronics, computer applications, pp 6–11. Shenyang, China 
9. Wu F, Jin G, Gao M, HE Z Yang Y (2019) Helmet detection based on improved YOLO V3 
deep model. In: IEEE 16th international conference on networking, sensing and control, pp 
363–368 
10. Silva R, Aires K, Santos T, Abdala K, Veras R, Soares A (2013) Automatic detection of 
motorcyclists without helmet. In: IEEE XXXIX latin american computing conference, 2013, 
pp 1–7. Caracas, Venezuela (2013) 
11. Pool EAI, Kooij JFP, Gavrila DM (2019) Context-based cyclist path prediction using recurrent 
neural networks. In: IEEE intelligent vehicles symposium, pp 824–830. Paris, France (2019) 
12. Pool EA , Kooij JFP, Gavrila BM (2017) Using road topology to improve cyclist path prediction. 
In: IEEE intelligent vehicles symposium, pp 289–296. Los Angeles, CA, USA (2017) 
13. Zernetsch S, Kohnen S, Goldhammer M, Doll K, Sick B (2016) Trajectory prediction of 
cyclists using a physical model and an artiﬁcial neural network. In: IEEE intelligent vehicles 
symposium, pp 833–838. Gothenburg, Sweden (2016) 
14. Saleh K, Hossny M, Nahavandi S (2018) Cyclist trajectory prediction using bidirectional recur-
rent neural networks. In: Mitrovic T, Xue B, Li X (eds) AI 2018: advances in artiﬁcial intelli-
gence. AI 2018. LNCS, vol 11320, pp 284–295. Springer, Cham. https://doi.org/10.1007/978-
3-030-03991-2_28 
15. Wijnhoven RGJ, de With PHN (2010) Fast training of object detection using stochastic gradient 
descent. In: 20th international conference on pattern recognition, pp 424–427. Istanbul, Turkey 
(2010) 
16. Basch CH, Zagnit EA, Rajan S et al. (2014) Helmet use among cyclists in New York City. J 
Comm Health 39: 956–958 
17. Rogé J, Ndiaye D, Aillerie I, Aillerie S, Navarro J, Vienne F (2017) Mechanisms underlying 
cognitive conspicuity in the detection of cyclists by car drivers. Acc Anal Prevent 104: 88–95 
18. Char F, Serre T (2020) Analysis of pre-crash characteristics of passenger car to cyclist accidents 
for the development of advanced drivers assistance systems. Acc Anal Prevent 136:105408 
19. Li X, et al. (2017) A uniﬁed framework for concurrent pedestrian and cyclist detection. IEEE 
Trans Intell Transp Syst 18(2):269–281n 
20. Huang Z, Wang J, Pi L, Song X, Yang L (2020) LSTM based trajectory prediction model for 
cyclist utilizing multiple interactions with environment. Pattern Recog 112 (2020) 
21. Felzenszwalb PF, Girshick RB, McAllester D, Ramanan D (2010) Object Detection with 
Discriminatively Trained Part-Based Models. In: IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 32. no. 9. pp 1627–1645 (2010)

Design and Experimental Analysis 
of Spur Gear–A Multi-objective 
Approach 
S. Panda and Jawaz Alam 
Abstract Traditional design approaches involve determination of the design param-
eters that meet only one design condition at a time, which is inadequate for high 
consistency and reliability. To overcome this problem, an attempt is made to intro-
duce two new design objectives i.e., the weight/volume of the proﬁle modiﬁed spur 
gear drive and the contact or Hertzian stress at points of contacts in the multi-
objective design ﬁtness function. The main contributions of the study are multi-
objective constraint based design optimization using Particle swarm optimization 
(PSO), manufacturing of non-standard gear sets using standard tooling, experimental 
investigation to explore the possibility of protection of the optimized spur gear set 
against wear, through monitoring of oil bath temperature, gear surface temperature, 
and frictional power loss. 
Keywords Spur gears · Proﬁle shift · Altered tooth sum · Contact stress ·
Weight · scoring 
1 
Introduction 
It is quite difﬁcult to design an optimal spur gear set that has low contact stress 
over the line of action, lightweight, and has good wear resistance. Therefore, it is 
essential to enhance spur gear design by taking many performance factors, such as 
gear weight, contact stress, proﬁle modiﬁcation, vibration level, and an adequate 
EHD ﬁlm thickness into account. Extensive research studies have been carried out 
to improve the design of spur gear sets over the past two decades. 
In an experimental study, Amarnath et al. [1] have evaluated fault in a spur gear 
train using morphology based wear particle analysis, oil ﬁlm thickness analysis, and 
measurement of lubricant metal composition using energy dispersive spectrometry 
(EDS). According to the authors, these ﬁndings are related to the progression of wear 
on the tooth surfaces. Afterwards, the experimental ﬁndings on the assessments of
S. Panda envelope symbol · J. Alam 
Department of Mechanical Engineering, VSSUT, Burla, Odisha, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_20 
221

222
S. Panda and J. Alam
the gear teeth stiffness reduction in conjunction with statistical parameter analysis 
of vibration signature, have been discussed by Amarnath et al. [1]. The authors have 
used theoretical model and modal analysis, to estimate the stiffness. According to 
authors’ ﬁndings, the spur gear transmission system’s, vibration amplitude and the 
development of surface fatigue wear are directly correlated with stiffness measure-
ment. On the other hand, experimental studies on the impact of solid particle presence 
in oil and their impact on spur gear teeth wear have been conducted by Sari et al. 
[2]. The results indicate the emergence of an abrasive wear tooth surfaces with high 
sliding rates. The investigations also included the evaluation of gear tooth thickness at 
the tip, pitch, and root zone. The presence of silica particles and high sliding velocity 
resulted in a signiﬁcant diminution in tooth thickness at the root. The authors have 
also suggested considering about the investigation of vibration causes in a geared 
system as a possible area of future research. Recently, Sachidanand et al. [3, 4] have  
presented the effectiveness of proﬁle shift with altered tooth sum technique in design 
of spur gear train. In their analysis, the authors have proposed that the negatively 
altered tooth sum spur gear drive have low wear rate as compare to standard and posi-
tively altered tooth sum gear train. Further, the researchers [5] have further extended 
the study of [3, 4] and design a spur gear train on multi-objective basis considering 
weight and contact stress with ﬁlm thickness. In their study, the authors have found 
that negatively altered tooth sum gear set has minimum weight and low hertzian stress 
along line of action with minimum ﬁlm thickness at the contact interface. Hence, it 
has better performance than standard and positive altered tooth sum gear set. 
This article is the extension of the study of [5] with experimental investigations of 
the proﬁle modiﬁed spur gear set for gear wear identiﬁcation and assessment of the 
gear tooth surface. After the optimization of the gear set, experimental investigation 
has been performed to validate the theoretical results. The rest of this article is estab-
lished as follows. Section 2 discuss the objective function development. Section 3 
discussed the result of optimization analysis and Sect. 4 reports the CAD model and 
manufacturing of the optimized gear set. Section 5 presents the experimental inves-
tigation based on oil bath temperature, gear tooth surface temperature and frictional 
loss of the gear set. Finally, the study is concluded in Sect. 6. 
2 
Formulation Details 
In this paper we have proposed a multi objective function to minimization of weight 
and hertzian stress using a metaheuristic approach i.e., PSO. The objective function 
is modeled as follows: 
uppe r M i  n up pe r F Subscript Baseline left parenthesis x right parenthesis equals w 1 times StartFraction upper W left parenthesis x right parenthesis Over upper W 0 left parenthesis x right parenthesis EndFraction plus w 2 times StartFraction sigma left parenthesis x right parenthesis Over sigma 0 left parenthesis x right parenthesis EndFraction
upper M i n  u pper F Subscript Baseline left parenthesis x right parenthesis equals w 1 times StartFraction upper W left parenthesis x right parenthesis Over upper W 0 left parenthesis x right parenthesis EndFraction plus w 2 times StartFraction sigma left parenthesis x right parenthesis Over sigma 0 left parenthesis x right parenthesis EndFraction
upper 
M i n upper F Subscript Baseline left parenthesis x right parenthesis equals w 1 times StartFraction upper W left parenthesis x right parenthesis Over upper W 0 left parenthesis x right parenthesis EndFraction plus w 2 times StartFraction sigma left parenthesis x right parenthesis Over sigma 0 left parenthesis x right parenthesis EndFraction

Design and Experimental Analysis of Spur Gear–A Multi-objective …
223
where w 1 and w 2 are the weight constant such that w 1  plu s w 2 equals 1 and W0(x) is weight 
of spur gear set when it is optimized as a single objective, W(x) is weight of spur 
gear set, σ 0(x) is contact stress when it is optimized as a single objective and σ (x) is  
contact stress. Equation for weight and contact stress of the spur gear set is same as 
reported in [6]. 
2.1 
Design Variable and Constraints 
The design parameters focused in this multi-objective design optimization issue are 
same as reported in [6] i.e. module, numbers of teeth on the pinion and gear, face 
width, pinion and gear shaft diameter and hardness of gear material. In this study the 
gear ratio (i) is considered as 4:1 for the spur gear set. 
On the other hand, mainly, three types of constraints i.e., geometric, design 
and control parameter constraints are considered in this multi-objective design 
optimization problem as same as reported in [6]. 
2.2 
Particle Swarm Optimization (PSO) 
This population based design optimization approach was ﬁrst presented by Kennedy 
and Eberhart [7]. The global version of PSO was based on accelerating each particle 
toward its “p Best” and “g Best” locations by changing its velocity at each step. 
Separate random numbers are generated for acceleration toward the “p Best” and 
“g Best” locations, and these random numbers are used to weight the acceleration. 
According to Eqs. 2 and 3, the particles’ positions and velocities are updated. The 
PSO algorithm’s detailed steps are provided in [7]. 
StartLa yout 1st  Row upper V Subscript i  plus 1 Baseli
ne left bracket right bracket 
equals upper V Subscript i Baseline left bracket right bracket plus upper C 1 asterisk r a n d left parenthesis right parenthesis asterisk left parenthesis p Subscript upper B e s t Baseline left bracket right bracket minus upper C u r r e n t left bracket right bracket right parenthesis plus 2nd Row upper C Subscript 2 Baseline asterisk r a n d left parenthesis right parenthesis asterisk left parenthesis g Subscript upper B e s t Baseline left bracket right bracket minus upper C u r r e n t left bracket right bracket right parenthesis EndLayout
upper C u r  r e n t Sub s cr ipt i p
lus 1 Baseline left bracket right bracket equals upper C u r r e n t left bracket right bracket plus upper V Subscript i plus 1 Baseline left bracket right bracket
3 
Optimization Results 
In this work, the addendum shift approach is applied on the set 1 i.e., standard 
tooth sum of 90. This technique is used to minimize the weight and hertzian stress 
at different points namely A, B, B2, C, D, D2, and E over the line of action. The 
optimized design variables attained through the PSO algorithm for set 1 and set 2

224
S. Panda and J. Alam
Table 1 
Optimized dimensions (in “mm”) of spur gear sets 
Ze
b
D1
D2
R11
R12
Ra1
Ra2
Rd1
Rd2
Rb1
Rb2
Hardness 
90 (x1 = 
0,x2 = 0) 
21.32 
20
36.4 
18
72
20
74
15.5 
69.5 
16.3 
65.3 
357.49 
90 (x1 = 
1.1874,x2 = 
−1.1874) 
20.5
20
36.4 
18
72
22
76
17.5 
67.5 
16.3 
65.3 
343.99 
Table 2 
Optimum objective function 
Ze
Contact stress (MPa)
Weight (gm) 
A
B
B2
C
D
D2
E
W 
90 (x1 = 0,x2 
= 0) 
877.57 
682.40 
1147.30 
888.40 
854.40 
984.08 
345.33 
1487.20 
90(x1 = 
1.1874,x2 = − 
1.1874) 
621.65 
604.71
948.90 
906.17 
821.44 
795.23 
427.93 
1561.00 
are reported in Table 1. Both the objective function i.e., weight and contact stress are 
estimated for both the gear set, so that conclusion can be inferred from this study. 
It is inferred from Table 2 that the volume/weight of gear set set 2 is 4.96% more 
than set 1 gear set, but the contact stresses at all the point of contact are less except 
point C and E. Also it is observed that the face width of the set 2 is less, pointing 
towards better scoring resistance [8]. The above mention fact may be due to T1C >  
T1B and T1C < T1E. This condition point towards the fact that C is a single point 
contact. One of major accomplishments of this work’s is that all adopted constraints 
are satisﬁed at the best solution point. 
4 
CAD Modeling and Manufacturing of Gear Set 
The optimized geometry of spur gear drive (set 1 and set 2) were used to develop 
the optimised CAD model in Solidworks 16. Furthermore, it can be seen that no 
geometric dimension interferences occurs in the geometric modelling. The optimised 
gear sets are therefore demonstrative of their practical feasibility. 
After validation of the optimization results by using CAD model, the CAD model 
was transferred to the shop ﬂoor for manufacturing of the optimized proﬁle modiﬁed 
gear train. This step will predict the practical utility of the gear sets. So, the optimized 
non-standard gear sets of are manufactured through the gear milling process using 
standard tooling. The manufactured gear sets are shown in Fig. 2. The ﬁndings in 
this section portray the manufacturing of the optimized gear set. The next section, 
therefore, addresses the experimental ﬁndings.

Design and Experimental Analysis of Spur Gear–A Multi-objective …
225
Fig. 1 CAD model of 
optimized gear sets (a) set  
(b) set  2  
Fig. 2 Manufactured gear 
sets 
5 
Experimental Analysis 
Analytical approaches can be used to assess gear performance prior to testing. 
However, the authors are concentrated on testing the performance of the gear set. 
Previous studies have based their criteria for monitoring of gear wear in the gear set 
on different parameters like frictional power loss, efﬁciency, gear surface temper-
ature and oil bath temperature rise etc. So, in this study we have experimentally 
veriﬁed the protection of the optimized gear set against the wear by measuring the 
above said parameters for all the gear sets. In this study, the performance of proﬁle 
modiﬁed spur gear sets i.e., set 2 was investigated and compared with standard gears 
i.e., set 1. A specially designed experimental set up as shown in Fig. 3 is developed 
for experimentation. Speciﬁcation of this experimental test rig is reported in Table 3.
The gear sets (i.e., set 1, and set 2) are operated by a 1 HP D.C motor. The output 
torque of the spur gear train is delivered to belt brake spring balance system. The 
motor and input of the gearbox i.e., pinion shaft is connected via coupling. The output 
shaft is connected to a pulley that is in contact with the belt brake spring balance 
system. A speed controller, also known as a variable frequency drive (VFD), controls 
the rotation of the motor, allowing the geared system to work at various speeds. The 
gear pair has been lubricated with a multi-grade lubricant (20W40). The tests are 
carried out with a constant load of 5000 N-mm at operating speed of 800 rpm.

226
S. Panda and J. Alam
Fig. 3 Experimental set up 
Table 3 
Gear test rig speciﬁcations 
Parameters
Speciﬁcations 
Pressure angle of gear set
25° 
Module
2 mm  
Gear shaft diameter
36.4 mm 
Pinion shaft diameter
20 mm 
Material of gear set
EN24 
Spring balance capacity
1000 N 
Belt width
5 cm  
Belt thickness
2 mm  
Lubricant
20W40 
Speed of pinion shaft
800 rpm 
Thermocouple
K-type with digital temperature indicator
5.1 
Efﬁciency and Frictional Power Loss 
Heat generation is a direct reﬂection of power loss between the gear sets. As a result, 
the temperature of the gear increases in accordance with the quantity of power lost. 
The power loss depends on quite a few factors like types of gear, the mechanisms of 
the tooth contact, and the friction power at the gear teeth contact surfaces. Among 
all the losses, power loss due to sliding friction is permanent and changes w. r. t. the 
load. The rest of other losses does not dependent on tooth load and hence they are 
omitted in this analysis. So, it is necessary to examine the frictional loss of the gear 
set. In this study, the output power is estimated based on the load coming on to the 
output shaft, i.e., 5000 N-mm, 7500 N-mm, and 10000 N-mm for all the gear sets 
respectively. The input power was measured using a digital wattmeter connected to 
the VFD and the D.C. motor with an electrical circuit. The digital wattmeter reads 
the input power for all the gear set for a speciﬁed load and speed (800 rpm). The 
measured efﬁciency (ï) for all the gear sets is reported in Table 4. The results indicate 
that set 2 gears (96.36%) is more efﬁcient as compared to other standard gear sets

Design and Experimental Analysis of Spur Gear–A Multi-objective …
227
Table 4 
Efﬁciency (ï) of optimized gear sets 
Gear 
set 
Torque = 5000 N-mm Torque = 7500 N-mm Torque = 10,000 
N-mm 
ï = 
StartFraction upper P o w e r o u t p u t left parenthesis normal upper W right parenthesis Over upper P o w e r i n p u t left parenthesis normal upper W right parenthesis EndFraction
StartFraction upper P o w e r o u t p u t left parenthesis normal upper W right parenthesis Over upper P o w e r i n p u t left parenthesis normal upper W right parenthesis EndFraction
Power 
input 
(W) 
Frictional 
Power loss 
(W) 
Power 
input 
(W) 
Frictional 
Power loss 
(W) 
Power 
input 
(W) 
Frictional 
Power loss 
(W) 
set 1
109.20
4.55
163.81
6.83
218.41
9.11
95.83% 
set 2
108.60
3.95
162.91
5.93
217.21
7.91
96.36% 
(a) 
 
 
 
 
 
(b)
 
95.4 
95.6 
95.8 
96 
96.2 
96.4 
set 1
set 2 
Efficiency(ɳ) 
0 
2 
4 
6 
8 
10 
0
5000
7500
10000 
Frictional Power loss (W) 
Torque (N-mm) 
set 1 
set 2 
Fig. 4 (a) Efﬁciency of gear sets, and (b) Frictional power loss with load 
(as can be seen in Fig. 4(a)). This point towards the fact that set 2 gear has minimum 
frictional losses from the tooth surface (as can be seen in Fig. 4(b)) and these results 
are in concordance with the reported result in [4]. 
5.2 
Oil Degradation and Surface Temperature 
On the other hand, the temperature of the test gear set is recorded shortly after it 
reaches steady state in our experiment. An infrared thermometer was utilized to 
measure the temperature rise. Table 5 and Fig. 5(a) shows the temperature observed 
at different instant of time at constant speed of 800 rpm. It is observed that for the set 
2 gear the temperature rise (10.10°c) was smaller than rest of the gear sets indicating 
a less frictional power loss than the standard gear sets. It seems possible that these 
results are due to better lubrication at the tooth contact interface and this outcome 
is in line with [4]. So, the reported results are in favour of application of proﬁle 
modiﬁed gear set (set 2) as compared to set 1.
Apparently, the temperature of the lubricant (20W40) measured from the gear 
mesh location increased rapidly during the ﬁrst two hours (2 h) of the test, neverthe-
less, the oil temperature stabilized at the end of the experiment. As lubricant viscosity 
is proportional to temperature, increasing oil temperature reduces viscosity [9]. The 
increase in lubricant temperature during the operation of all gear sets is shown in

228
S. Panda and J. Alam
Table 5 Temperature (°C) variations 
Time (hrs)
Oil temperature
Surface temperature 
Set 1
Set 2
Set 1
Set 2 
0
13
11
22
23 
1
30
27
24.2
23.6 
2
33
30
30.6
29.1 
3
34
31
30.9
29.2 
4
35
32
32.1
29.6 
5
36
34
34.2
33.1 
Change in temperature (°C)
23
23
12.20
10.10 
(a)
(b) 
20 
25 
30 
35 
0
1
2
3
4
5
 
Surface Temperature (0c) 
Time (per hours) 
Set 1
Set 2 
8 
13 
18 
23 
28 
33 
38 
0
1
2
3
4
5
 
Oil Temperature (0c) 
Time (hours) 
Set 1
Set 2 
Fig. 5 Variations of (a) Gear surface temperature, and (b) Oil bath temperature
Table 5 and Fig. 5(b). What is most relevant in the reported data is that in comparison 
to the other gear sets, the rate of rise of oil temperature in set 2 gears is quite low (as 
can be seen Fig. 5(b)). This means that for set 2 gears a better oil ﬁlm thickness will 
be maintained at the contact surface during the operation and there by the gear wear 
life will be improved. 
6 
Conclusion 
In this study, the spur gear set is optimized considering weight and contact stress as 
two objective functions. The addendum modiﬁcation approach has been used on the 
standard tooth sum of 90 to achieve the required objective of the research. Afterward, 
the experimental examination has been performed to validate the theoretical result. 
From the optimization and experimental analysis, the outcomes drawn are:-
• It can be observed that set 2 gear set have high weight and low contact stress along 
line of action as compared to set 1 gear set. So, the researchers are free to use the 
gear sets as per their requirements. If the low weight of the gear set is the prime

Design and Experimental Analysis of Spur Gear–A Multi-objective …
229
objective of the design requirement then the designer will choose set 1 gear set 
but if low contact stress is the prime focus of the research then set 2 gear set must 
be chosen. 
• Manufacturing of the modiﬁed gear set can also be performed with standard 
tooling. This means the manufacturer doesn’t need any non- conventional tool for 
manufacturing of modiﬁed gear set. 
• Experimental results indicate that the modiﬁed gear set has low surface temper-
ature rise and low frictional losses in set 2 as compared to set 1. So it has high 
efﬁciency as compared to set 1 gear set. 
• Increase in oil bath temperature clearly indicate the temperature rise due to set 1 
operation is high as compared to set 2 gear set. This point towards the fact that 
set1 gear set engine oil has rapidly loses its viscosity, hence it is highly sensitive 
to the gear wear. 
• This study clearly suggest that modiﬁed gear set has better performance as 
compared to the standard gear set. So, designer may use modiﬁed gear sets, if 
weight of the gear set is not prime objective of the research. 
Acknowledgements The authors deeply acknowledge the ﬁnancial support provided by TEQIP-3 
and the computational facilities provided in CAD lab. of Dept. of Mechanical Engineering, VSSUT, 
BURLA. 
References 
1. Amarnath M, Chandramohan S, Seetharaman S (2012) Experimental investigations of surface 
wear assessment of spur gear teeth. JVC/J Vib Control 18:1009–1024. https://doi.org/10.1177/ 
1077546311399947 
2. Sari MR, Haiahem A, Flamand L (2007) Effect of lubricant contamination on gear wear. Tribol 
Lett 27:119–126. https://doi.org/10.1007/s11249-007-9215-z 
3. Sachidananda HK, Raghunandana K, Gonsalvis J (2015) Design of spur gears using proﬁle 
modiﬁcation. Tribol Trans 58:736–744. https://doi.org/10.1080/10402004.2015.1010762 
4. Sachidananda HK, Gonsalvis J, Prakash HR (2012) Experimental investigation of fatigue 
behavior of spur gear in altered tooth-sum gearing. Front Mech Eng 7:268–278. https://doi. 
org/10.1007/s11465-012-0331-6 
5. Alam J, Panda S (2021) A comprehensive study on multi-objective design optimization of spur 
gear. Mech Based Des Struct Mach. https://doi.org/10.1080/15397734.2021.1996246 
6. Alam J, Priyadarshini S, Panda S, Dash P (2021) Optimum design of proﬁle modiﬁed spur gear 
using PSO. In: Udgata SK, Sethi S, Srirama SN (Ed) Intelligent systems proceedings of ICMIB 
2020, LNNS, vol 185, pp 177–187. Springer, Singapore. https://doi.org/10.1007/978-981-33-
6081-5_16 
7. Kennedy J, Eberhart R (1995) Particle swarm optimization. In: Proceedings of IEEE international 
conference on neural networks, vol 4, 1942–1948 (1995). https://doi.org/10.1007/978-3-319-461 
73-1_2 
8. Winter H, Michaelis K (1983) Scoring load capacity of gears lubricated with Ep-Oils. AGMA 
Paper (1983) 
9. Amarnath M, Lee SK (2015) Assessment of surface contact fatigue failure in a spur geared 
system based on the tribological and vibration parameter analysis. Meas J Int Meas Confed 
76:32–44. https://doi.org/10.1016/j.measurement.2015.08.020

Chest X-Ray Image Classiﬁcation 
for COVID-19 Detection Using Various 
Feature Extraction Techniques 
Sareeta Mohanty and Manas Ranjan Senapati 
Abstract Obtaining a chest x-ray image is one of the main clinical observations for 
screening novel coronavirus. Most patients with COVID-19 viral pneumonia have 
abnormalities on a chest x-ray, such as consolidation. Computer vision-based solu-
tions are a viable option for improving COVID-19 detection accuracy. However, 
Other classiﬁcation models are presently in use in the healthcare industry. One such 
model uses radiographs to identify pneumonia cases and has attained a high enough 
level of accuracy to be applied to actual patients. This research assesses the advan-
tages of employing various feature extraction strategies in order to improve the 
classiﬁcation performance of the COVID-19 detection. The objective is to create 
a COVID-19 classiﬁer using several feature extraction techniques, such as Fractal 
Descriptor (FD), Histogram Oriented Gradient (HOG), and Local Binary Pattern 
(LBP), using the pneumonia dataset as a base. Combining these feature extraction 
methods, an accuracy of 95% was attained utilizing FD. 
Keywords Chest X-Ray · Feature extraction · Classiﬁcation · LBP · FD · HOG ·
KNN · Accuracy · Classiﬁcation 
1 Introduction 
X-ray imaging is one of the ﬁrst forms of this technology, and it has a long history of 
application in diagnosing disease. The use of the X-ray machine in the detection of 
fractures and foreign objects was already acknowledged ﬁve years after its creation, 
in 1900 [26]. X-ray exams are frequently used today to detect irregularities in the 
chest, especially those brought on by respiratory disorders. An X-ray of the patient’s 
chest will reveal their lungs, from which skilled radiologists can identify illnesses. 
The COVID-19 epidemic has had a terrible effect on both the international economy 
and the health of people everywhere. On May 24, 2020, more than 5 million people 
worldwide suffered from COVID-19. X-Ray imaging technique is very helpful in
S. Mohanty envelope symbol · M. R. Senapati 
Department of Information Technology, VSSUT, Burla, Odisha, India 
e-mail: mohanty.sareeta@gmail.com 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_21 
231

232
S. Mohanty and M. R. Senapati
Fig. 1 Sample data of healthy and unhealthy CXR images 
detecting respiratory diseases like pneumonia, COVID-19. A sample data is presented 
in Fig 1. 
In response to the COVID-19 epidemic, picture categorization has a useful use. 
Nearly all nations were afﬂicted by the corona virus disease’s spread, although Asia, 
North America, and Europe were particularly hard hit. Many of the medical research 
facilities in these nations have made some of the chest X-rays that were regularly 
used to diagnose the disease public. Since the epidemic is still ongoing as of this 
writing, more information will probably become available over time. As a result, we 
may not have as much data as deep learning algorithms, which are renowned for 
requiring big data sets, in order to achieve outstanding results. Although the data 
amount may be constrained, there are methods that can be used to still get good 
results. One such method is transfer learning, which bases a new model on a portion 
of a pre-trained model. The model that is produced can then be further trained to 
discover the speciﬁcs of the data utilized for the current task. Given the themes 
covered above, the approach used to determine if a chest X-ray image is positive or 
negative for COVID-19 is the main emphasis of this research. 
From different studies it is observed that yet almost no study concerning classiﬁ-
cation for COVID-19 detection using various feature extraction techniques. Novelty 
aspect, the present study focuses on creating a classiﬁer which classiﬁes covid 
positive or covid negative using various feature extraction techniques.

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
233
2 Feature Extraction Techniques 
2.1 
Histogram Oriented Gradient (HOG) 
One popular feature extraction technique for machine learning implementations of 
object identiﬁcation is the histogram of oriented gradients (HOG). It functions by 
deﬁning each image as a collection of local histograms, each of which represents a 
local instance of a gradient orientation. Each block of the image has a 50% overlap 
with the one before it, and each block is further subdivided into cells. One cell may 
appear in more than one block due to the block overlap. The gradients (Gx and Gy) 
in the x and y directions are computed for each pixel in each cell, thetais the angle of 
each pixel, muis the magnitude. The magnitude and phase of the gradients are then 
calculated according to the Eq. (1) and (2). 
Magnitude mu equals StartRoot upper G Subscript x Superscript 2 Baseline plus upper G Subscript upper Y Superscript 2 Baseline EndRoot
/
Magni tude 
m
u equals StartRoot upper G Subscript x Superscript 2 Baseline plus upper G Subscript upper Y Superscript 2 Baseline EndRoot
Angle theta  equals StartFraction upper G Subscript y Baseline Over upper G Subscript x Baseline EndFraction
Ang
le theta equals StartFraction upper G Subscript y Baseline Over upper G Subscript x Baseline EndFraction
Figure 2 describe HOG feature extraction visualization for normal CXR image 
and viral CXR image. 
A histogram of orientations is produced for each cell. When utilizing unsigned 
gradients, the phases are utilized to divide the vote into bins that are evenly spaced 
between 0 and 180°. When using unsigned gradients, it doesn’t matter if an edge 
transitions from dark to bright or from bright to dark. In order to do it, angles below 
0° are made 180° larger, and angles above 180° are made 180° smaller. Each angle’s 
vote is given a weight based on the gradient’s matching magnitude. After that, the 
histograms are adjusted to be comparable to the cells in the same block. The resultant
Fig. 2 HOG feature extraction visualization for CXR images 

234
S. Mohanty and M. R. Senapati
feature vector is created by concatenating the histograms for each cell into a vector 
[8, 14]. 
2.2 
Local Binary Pattern (LBP) 
A method called Local Binary Pattern (LBP) uses local changes in intensity between 
the values of the centre and surrounding pixels to extract local characteristics 
from images (neighbouring pixels). The local binary pattern’s objective is to iden-
tify various patterns by comparing the differences between its centre pixel and its 
surrounding pixels. 
After that, aggregated patterns are applied to the entire image. Thus, LBP has 
been used in a wide range of applications, including texture classiﬁcation (Ojala, 
Pietikainen, Maenpaa, 2002), medical image classiﬁcation (Malathi Meena, 2010), 
face recognition (Ahonen, Hadid, Pietik ainen, 2004), and annotation of medical 
images (Caputo, 2014). (Nanni Lumini, Brahnam, 2010) [19]. Typically, the LBP 
descriptor is used to represent grey scale images, resulting in each pixel having a 
value between (0 to 255). The shorthand for the original LBP descriptor is LBP 
P, R, where P and R stand for the circle’s radius and the number of pixels nearby, 
respectively. Therefore, the LBP with radius 1 and eight neighbours is referred to 
as having the shape of LBP 8, 1. Therefore, the local binary pattern generates in its 
most basic form. If the pixel directly adjacent to the center has a higher grey value 
than the center pixel, a value of one is provided; otherwise, a value of zero is given. 
An eight-bit code can be used to represent the eight neighbours that surround the 
center. An illustration of how to calculate the LBP descriptor is shown in Fig. 3. 
The generic LBP descriptor can be extended to take into account more neigh-
bourhood pixels with different radius values and cover wider regions, as shown in 
Fig. 4, by using bilinear interpolation. The generic LBP descriptor is not restricted 
to represent only eight pixels with R value equal to one.
Fig. 3 Local Binary Pattern Feature Extraction Phases 

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
235
2.3 
Fractal Descriptors (FD) 
Scale-space theory is used to describe the image as a series of images by using a 
Gaussian kernel with a movable scale parameter. Changes in scale can have a big 
effect on how a texture looks. The scale change of such an image intensity surface 
cannot be handled by a locally invariant texture descriptor. The goal of this study is to 
construct a texture descriptor that is robust to scale changes in the intensity surface 
of natural texture. Although the local fractal dimension alone cannot adequately 
describe true texture, it is an effective way to gauge how “rough” the surface of 
a natural image is. Picking just a few bins might not be sufﬁcient to give sufﬁcient 
discriminative power because the distribution only has a ﬁnite number of elements. A 
successful feature descriptor combines three key qualities: robustness, independence, 
and efﬁciency. By employing the fractal dimensions as weights in the histogram, 
which is an enhancement over the initial local binary pattern (LBP), the proposed 
descriptor’s discrimination power is increased. 
3 Classiﬁcation Techniques 
3.1 
K-Nearest Neighbour Algorithm (KNN) 
The supervised classiﬁcation learning algorithm k-nearest neighbour (KNN) is used 
to categories samples. This algorithm’s goal is to categorise fresh samples using 
their attributes and labelled training examples. Since the approach is memory-based, 
ﬁtting a model is not necessary. Finding the k training points that are closest to a 
query point, x0, in terms of distance (Euclidean distance), is done. The new query 
is assigned to its cluster based on the vast majority of its neighbors. Voting ties are 
resolved at random. For the nearest neighbours classiﬁer to correctly identify the 
class of an input pattern, it needs a dataset to compare against the training data refers 
to this dataset. The pth N-dimensional training vector, xtp is used to represent the 
training vector. 
Nv is a representation of the overall number of training patterns in the training 
data. The pth N-dimensional test vector, xp is used to represent the input test vector. 
Nc represents the total number of classes. The letter i stands in for the class labels. 
In order to determine the right class of the classiﬁer, the nearest neighbor classiﬁer 
compares the input test vector with the training data, which are also known as example 
vectors. mik represents the example vectors. 
m Su bscr
ipt ik Baseline equals x Subscript p
In Algorithm 1, an example of KNN pseudo code is displayed [26].

236
S. Mohanty and M. R. Senapati
4 Performance Indicators 
The confusion matrix (CM) for the binary classiﬁcation is shown in Fig. 6 to make 
it easier to grasp. The columns in the picture show the initial True and False class 
labels that were provided with the data, and the rows represent the classiﬁer’s results. 
Both the original (ground truth) and the resulting (classiﬁed) class labels are true 
and false, respectively, according to the deﬁnitions of true positive (TP) and true 
negative (TN). Contradictions are expressed in the confusion matrix (CM) as off-
diagonal False Positive (FP) and False Negative (FN) signals. Assume that N = (TP 
+ TN + FP + FN) samples in total will be tested. Higher TP and TN values hence 
result in better accuracy, but higher FP and FN values result in classiﬁer rejection. 
Numerous conclusions can be taken from the CM. In order to validate the clas-
siﬁcation results, a number of signiﬁcant metrics, including accuracy, sensitivity, 
speciﬁcity, recall, f measure, and G-Mean, are included. Accuracy is the percentage 
of true results in a population, whether they are true positive or true negative. It 
assesses the accuracy of a condition’s diagnostic test. The ratio of how many were 
accurately classiﬁed as positive to how many were truly positive is the sensitivity of 
a classiﬁer. The ratio of how many data points were accurately identiﬁed as negative 
to how many data points were truly negative is the classiﬁer’s speciﬁcity. Precision is 
how much were correctly classiﬁed as positive out of all positives. Recall and sensi-
tivity are one and the same. The usual accuracy measure is regarded to understate the 
performance of the classiﬁer compared to the F1 score. The Geometric Mean eval-
uates the balance of classiﬁcation performances on both the majority and minority 
classes (G-Mean). Low G-Mean implies poor performance in the categorization of 
the positive examples, even if the negative cases are correctly classiﬁed as such. 
accuracy equals  left p
are
sensitivity equals TP slash upper P equals 
TP slash left parenthesis TP plus FN right parenthesis equals tp normal bar rate
specificity  equals  TN slash
 upper N equals tn normal bar rate
Fig. 4 LBP Feature Extraction Different neighbourhood system 

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
237
precision equals TP  slas
h left parenthesis TP plus FP right parenthesis
recall equals sensit
ivity
upper F S ubsc ript measure B aseline equals 2 aste risk left 
parenthesis left parenthesis precision asterisk recall right parenthesis divided by left parenthesis precision plus recall right parenthesis right parenthesis
up per G m inus Me an equals  sqrt left
 parenthesis tp normal bar rate times tn normal bar rate right parenthesis
5 Dataset Description 
Data gathering is the ﬁrst and most crucial stage of every machine learning project. 
For this assignment, chest X-rays from the COVID-19, normal, pneumonia, and 
COVID-19 negative courses are needed. There isn’t currently a dataset that has 
pictures from each of those groups. As a result, a new dataset is produced by 
combining the Kaggle Chest X-ray dataset with the COVID-19 Chest X-ray dataset 
obtained by Dr. Joseph Paul Cohen of the University of Montreal [2]. The most 
typical and widely used sort of chest X-ray images are the posterior anterior chest 
images, which are available in both databases. 
Dr. Joseph Paul Cohen, Postdoctoral Fellow at the University of Montreal, recently 
published a dataset that is open to the public and includes chest X-ray and CT images 
of patients with COVID-19, Middle East respiratory syndrome (MERS), severe acute 
respiratory syndrome (SARS), and acute respiratory distress syndrome (ARDS) [24]. 
The photos utilised in this project are from the COVID-19 dataset instance that is 
current as of March 18, 2020, since additional instances are published daily. 
The machine learning community was invited to develop an algorithm that can 
detect pneumonia from chest radiographs as part of an RSNA-sponsored Kaggle 
competition in August 2018 [24]. A unique requirement of the competition and a 
more difﬁcult object identiﬁcation problem than picture classiﬁcation in this project 
is for the model to locate the lung opacities. Thousands of photos of healthy people 
and patients with pneumonia are still available in this collection, which is helpful for 
this inquiry. 
Images from those two datasets must ﬁrst be downloaded in order to create a 
dataset that can be utilised for training. The chosen images are then saved to the folder 
with the appropriate label. To be ready to be fed into the training process, images are 
loaded, preprocessed, and then converted into numpy arrays of the necessary size. 
The COVID chest X-ray dataset can be downloaded as the ﬁrst step in building 
the dataset by cloning Dr. Cohen’s GitHub repository [2]. The “images” section 
contains CT and chest X-ray images from COVID-19 patients as well as individuals 
who have MERS, SARS, and ARDS, among other diseases, as well as various chest 
views. To ﬁlter and select only COVID-19 positive chest X-ray images with the

238
S. Mohanty and M. R. Senapati
Algorithm: 1 k-NN pseudocode 
Require:     k 
N      . X is the domain, Y is the response 
a
 X ,( ,
, ) 
X* Y 
{Compute the separations between each data point and the input point.} 
for i =1    to  n do 
) 
end for 
{Find responses for the k nearest neighbours of the input point} 
for i=1 to k do 
end for 
{Determine the frequency with which each response appears among the k nearest neighbours.} 
for i= 1 to   p do 
end for 
{Find number of times each response occurs among the k nearest neighbours} 
for i = 1 to p do 
end for 
{
nses are most common, 
                                                  choose a fixed one.} 
return r {  Return the most frequent response (or, in the event of a tie, one of the most frequent esponses)}. 
Fig. 5 k-NN Algorithm 
posterior anterior (PA) view, the most popular and typical view, from the “images” 
folder, the “metadata.csv” ﬁle must be turned into a pandas data frame. All of these 
images were taken, and the dataset was subsequently built with precise tagging. Fig. 7 
displays a sample of data for both healthy and infectious pictures. 
6 Results and Discussion 
From the Fig. 8 it can be seen that the FD features resulted better accuracy in compar-
ison to state-of-art LBP and HOG features for CXR image covid classiﬁcation using 
KNN classiﬁer. From the Fig. 9 it can be seen that the FD features resulted better 
sensitivity in comparison to state-of-art LBP and HOG features for CXR image 
covid classiﬁcation using KNN classiﬁer. From the Fig. 10  it can be seen that the 
FD features resulted better speciﬁcity in comparison to state-of-art LBP and HOG 
features for CXR image covid classiﬁcation using KNN classiﬁer. From the Fig. 11 it

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
239
Fig. 6 Confusion matrix for 
binary classiﬁcation
True 
False 
True 
TP
FN 
False 
FP
TN 
Fig. 7 Sample dataset (Normal and Viral) 
can be seen that the FD features resulted better precision in comparison to state-of-art 
LBP and HOG features for CXR image covid classiﬁcation using KNN classiﬁer. 
From the Fig. 12  it can be seen that the FD features resulted better recall in 
comparison to state-of-art LBP and HOG features for CXR image. From the Fig. 
13 it can be seen that the FD features resulted better F-measure in comparison to 
state- of-art LBP and HOG features for CXR image covid classiﬁcation using KNN
Fig. 8 Accuracy of KNN classiﬁer over varying training percentage

240
S. Mohanty and M. R. Senapati
Fig. 9 Sensitivity of KNN classiﬁer over varying training percentage
classiﬁer. From the Fig. 14  it can be seen that the FD features resulted better G-
mean in comparison to state-of-art LBP and HOG features for CXR image covid 
classiﬁcation using KNN classiﬁer. 
From the Table 1, Table 2 and Table 3 it can be seen that the FD features resulted 
better accuracy in comparison to state-of-art LBP and HOG features for CXR image 
covid classiﬁcation using KNN classiﬁer.
Fig. 10 Speciﬁcity of KNN classiﬁer over varying training percentage

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
241
Fig. 11 Precision of KNN classiﬁer over varying training percentage 
Fig. 12 Recall of KNN classiﬁer over varying training percentage

242
S. Mohanty and M. R. Senapati
Fig. 13 F-measure of KNN classiﬁer over varying training percentage 
Fig. 14 G-mean of KNN classiﬁer over varying training percentage

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
243
Table 1 KNN classiﬁcation results with LBP features over varying training percentage 
10%
20%
30%
40%
50%
60%
70%
80%
90% 
Accuracy
0.929288
0.933257
0.923716
0.936002
0.934823
0.937261
0.936767
0.941086
0.947933 
Sensitivity
0.960402
0.957621
0.954871
0.962761
0.957627
0.963391
0.953115
0.964389
0.965318 
Speciﬁcity
0.809249
0.839219
0.803401
0.832714
0.846726
0.836431
0.873762
0.850746
0.880597 
Precision
0.951039
0.958313
0.949384
0.956924
0.960216
0.957854
0.966775
0.961612
0.969052 
Recall
0.960402
0.957621
0.954871
0.962761
0.957627
0.963391
0.953115
0.964389
0.965318 
F-measure
0.955698
0.957967
0.95212
0.959834
0.95892
0.960615
0.959897
0.962999
0.967181 
G-mean
0.881592
0.896468
0.875867
0.895379
0.900471
0.897669
0.912577
0.905787
0.921985

244
S. Mohanty and M. R. Senapati
Table 2 KNN classiﬁcation results with HOG features over varying training percentage 
10%
20%
30%
40%
50%
60%
70%
80%
90% 
Accuracy
0.886452
0.897303
0.902295
0.916624
0.918911
0.910482
0.90821
0.923489
0.906585 
Sensitivity
0.920377
0.934746
0.944139
0.952809
0.951464
0.94605
0.947977
0.95183
0.94027 
Speciﬁcity
0.755574
0.752788
0.740701
0.776952
0.793155
0.773234
0.75495
0.814126
0.776119 
Precision
0.935596
0.935873
0.933605
0.942821
0.946723
0.941515
0.937143
0.95183
0.942085 
Recall
0.920377
0.934746
0.944139
0.952809
0.951464
0.94605
0.947977
0.95183
0.94027 
F-measure
0.927924
0.935309
0.938843
0.947789
0.949087
0.943777
0.942529
0.95183
0.941176 
G-mean
0.833914
0.838848
0.836256
0.860399
0.868711
0.855288
0.845976
0.88029
0.854261

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
245
Table 3 KNN classiﬁcation results with FD features over varying training percentage 
10%
20%
30%
40%
50%
60%
70%
80%
90% 
Accuracy
0.944188
0.951344
0.958798
0.961866
0.956047
0.957643
0.965946
0.963382
0.955681 
Sensitivity
0.979546
0.978103
0.987254
0.988539
0.984176
0.981946
0.989171
0.983391
0.987245 
Speciﬁcity
0.807779
0.848067
0.848905
0.85891
0.847381
0.863866
0.876436
0.886171
0.833433 
Precision
0.965781
0.975572
0.976202
0.978691
0.975708
0.979635
0.982987
0.985251
0.972562 
Recall
0.979546
0.978103
0.987254
0.988539
0.984176
0.981946
0.989171
0.983391
0.987245 
F-measure
0.972614
0.976835
0.981696
0.98359
0.979923
0.980789
0.986069
0.98432
0.979847 
G-mean
0.889431
0.910715
0.915412
0.921397
0.913163
0.920974
0.931061
0.933489
0.907011

246
S. Mohanty and M. R. Senapati
7 Conclusion 
In this paper various feature extraction techniques are proposed for detection of 
COVID-19 disease using CXR images. The accuracy results obtained for LBP, HOG 
and FD are 93, 91 and 95% respectively. This strongly suggests the Fractal Descriptor 
as a better alternative for COVID-19 detection using CXR images in comparison to 
the other alternatives. 
8 Future Scope 
However, additional research can also be done to determine the signiﬁcance of deep 
learning, transfer-based learning, and various segmentation strategies. In the future, 
it might be possible to expand our current method to not only identify whether a 
patient has Covid but also to adopt a watershed-based segmentation strategy that can 
identify precise location that can then be utilised for diagnosing other diseases. 
References 
1. Bashar KMd. (2019) Improved classiﬁcation of malaria parasite stages with support vector 
machine usingcombined color and texture features. In: 2019 IEEE healthcare innovations and 
point of care technologies (HIPOCT).IEEE 
2. Cohen JP, Morrison P, Dao L (2020) COVID-19 image data collection. arXiv preprint 
arXiv:2003.11597 (2020) 
3. Dalal N, Bill T (2005) Histograms of oriented gradients for human detection. 2005 IEEE 
computersociety conference on computer vision and pattern recognition (CVPR’05), vol 1. 
IEEE 
4. Das S, Mishra S, Senapati MR (2020) New approaches in metaheuristic to classify medical 
data using artiﬁcial neural network. Arab J Sci Eng 45(4):2459–2471 
5. Das S, Patra A, Mishra S, Senapati MR (2015) A self-adaptive fuzzy-based optimised functional 
link artiﬁcial neural network model for ﬁnancial time series prediction. Int J Bus Forecast 
Market Intell 2(1):55–77 
6. Dash S, Senapati MR, Sahu PK, Chowdary PSR (2021) Illumination normalized based 
technique for retinal blood vessel segmentation. Int J Imaging Syst Technol 31(1):351–363 
7. Datal, N (2005) Histograms of oriented gradients for human detection. In: Proceedings 2005 
international conferenceon computer vision and pattern recognition, vol 2. IEEE Computer 
Society (2005) 
8. Haralick RM, Karthikeyan S, Dinstein H (1973) Textural features for image classiﬁcation. 
IEEE Trans Syst Man Cybernet 6:610–621 
9. Huang X, Chen M, Liu P (2019) Recognition of transrectal ultrasound prostate image based 
onHOG-LBP. In: 2019 IEEE 13th International conference on anti-counterfeiting, security, and 
identiﬁcation (ASID), IEEE 
10. HuangZK, Li PW, Hou LY (2009) Segmentation of textures using PCA fusion based gray-level 
co-occurrence matrix features. In: 2009 International conference on test and measurement, vol 
1. IEEE

Chest X-Ray Image Classiﬁcation for COVID-19 Detection Using …
247
11. Hasan Md J, Alom Md S, Ali Md (2021) Deep learning based detection and segmentation 
of COVID-19 & pneumonia on chest X-ray image. In: 2021 International conference on 
information and communication technology for sustainable development (ICICT4SD210–214) 
(2021) 
12. Islam MT, Aowal Md A, Minhaz AT, Ashraf K (2017) Abnormalitydetection and localization 
in chest X-rays using deep convolution neural networks, arXiv preprint arXiv:1705.09850 
13. Jafarpour S, Sedghi Z, Mehdi CA (2012) A robust brain MRI classiﬁcation with GLCM 
features.Int J Comput Appl 37(12):1–5, 40 
14. Johnson RA, Wichern DW (2014) Applied multivariate statistical analysis, vol 6, 41p. 
Pearson,London 
15. Kim SH, Lee JH, Ko B, Nam JY (2010) X-ray image classiﬁcation using random forests with 
local binary patterns. In: 2010International conference on machine learning and cybernetics, 
vol 6. IEEE 
16. Ko BC, Kim SH, Nam J-Y (2010) X-ray image classiﬁcation using random forests with local 
wavelet-based CS-local binary patterns. J Digit Imaging 24(6):1141–1151 
17. Ojala T, Pietikäinen M, Harwood D (1996) A comparative study of texture measures with 
classiﬁcation based on featured distributions. Pattern Recogn 29(1):51–59 
18. Prasad DK, Vibha L, Venugopal KR (2015) Early detection of diabetic retinopathy from digital 
retinalfundus images. In: 2015 IEEE Recent advances in intelligent computational systems 
(RAICS) IEEE 
19. Padhy R, Dash A, Dash S, Mishra J (2021) Improved Face Recognition with Fractal-Based 
Texture Analysis. Int J Comput Vis Image Proces 11:41–53 
20. Sarwinda D, Alhadi B (2018) Detection of Alzheimer’s disease using advanced local binary 
patternfrom hippocampus and whole brain of MR images. In: 2016 International joint 
conference on neural networks (IJCNN). IEEE 
21. Sarwinda D, Titin S, Alhadi B (2018) Classiﬁcation of diabetic retinopathy stages usinghis-
togram of oriented gradients and shallow learning. In: 2018 International conference on 
computer, control, informatics and its applications (IC3INA). IEEE 
22. Sharma S (2020) Drawing insights from COVID-19-infected patients using CT scan images 
and machinelearning techniques: a study on 200 patients. Environ Sci Pollut Res 27:29 (2020) 
23. Singh D, Kumar V, Kaur M (2020) Classiﬁcation of COVID- 19 patients from chest CT images 
using multi-objective differential evolution–based convolution neural networks. Eur J Clin 
Microbiol Infect Dis 39(7):1379–1389 
24. SoaresE (2020) SARS-CoV-2 CT-scan dataset: a large dataset of real patients CT scans for 
SARS-CoV- 2 identiﬁcation. MedRxiv 
25. SujithA, Aji S (2020) An optimal feature set with LBP for leaf image classiﬁcation. In: 2020 
Fourth international conference on computing methodologies and communication (ICCMC), 
IEEE 
26. Hari Krishna S, Ramkumar P, Balakrishna R, Sunitha Rani N, Parimala BS (2022) Possibil-
ities of prediction of COVID 19 using K-nearest neighbour algorithm. In: 2nd International 
conference on technological advancements in computational sciences (ICTACS) (2022) 
27. World Health Organization. Q: Could ibuprofen worsen disease for people with COVID19? 
A: Based oncurrently available information. WHO does not recommend against the use of 
ibuprofen 
28. Zhou M, Zhang X, Qu J (2019) Corona virus disease 2019 (COVID-19): a clinical update. 
Front Med 14(2):126–135

Computer Vision and Image 
Segmentation: LBW Automation 
Technique 
Jeebanjyoti Nayak, Jyotsnarani Jena, Hrushikesh Pradhan, 
Jyotiprakash Das, and Surendra N. Bhagat 
Abstract Cricket is globally a famous game in which different technologies are 
being used to help the umpire in correct decision management. The bowl delivery is 
whether a fair one or LBW which is generally a matter of concern. Sometimes the 
decision of third umpire also goes wrong. This kind of error in making the decision 
can change the dimension of the game so, it is essential to make the decision precisely. 
This paper proposes computer vision and image subtraction technique. At ﬁrst, it must 
ﬁgure out that whether a delivery is no-ball or the fair one, for that image processing 
method is used on popping crease by detecting the pixel size. If the delivery is a fair 
one then, it will track the movement of the ball by using the colour segmentation 
method, video processing on the pitch. Then by the help of ultra-edge detection 
technique it can identify whether the ball has touched the edge of the bat or not. 
Keywords Cricket · LBW · No ball · Pitch 
1 
Introduction 
In earlier days of cricket, no technologies were invented for making the decision accu-
rate regarding no-ball and LBW; later-on some technologies were stared adopting by 
the M.C.C which made the decision making strong, accurate and less time consuming. 
Still there are some errors seen in the decision making of the ﬁeld umpire as well as 
third umpire, so in this paper the aim is to eradicate the errors in making the decision 
of LBW.
J. Nayak envelope symbol · J. Jena · H. Pradhan · J. Das · S. N. Bhagat 
Deparment of Computer Science and Engineering, Balasore College of Engineering and 
Technology, Sergarh, Balasore, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_22 
249

250
J. Nayak et al.
2 
Background Study 
2.1 
Cricket - The Game 
South east England created the bat and ball sport of cricket, which is played between 
two teams of 11 players each and is overseen by two on-ﬁeld umpires. In addition to 
deciding whether a delivery is lawful, the umpire also maintains track of deliveries 
and declares when an over is ﬁnished. A team’s destiny can be altered by an umpire’s 
poor judgement. When it comes to LBW, the most difﬁcult and diplomatic judgements 
are made. Leg before wicket, or LBW [1] is an acronym that was originally used 
in the inaugural test match between England and Australia in 1876 at Melbourne 
Cricket Ground in Australia. It was modiﬁed by M.C.C. in [2] 1884. (Marylebone 
Cricket Club). At the beginning, a batsman who is unable to smash the ball with 
his bat advances his front foot 8 inches beyond the line in order to save his wicket 
(avoid being bowled). The statute of LBW was established to end this injustice. It is 
currently run by ICC (International Cricket Council). 
2.2 
Cricket Pitch 
From stump to stump, the distance in [3] cricket is 22 yards (20.12 m). The return 
crease and bowler approach area are at least 1.22 m behind the stumps on a 3.05 m-
wide pitch. There will be no ball for the ﬁelding side if the ﬁelder is on the pitch, the 
wicket keeper has moved in front of the wicket, the bowler’s front foot is outside the 
popping crease, behind the bowling crease, or outside the return crease at the time 
of the delivered delivery. The crease marks may fade or vanish after 15 to 20 overs 
of bowling, making it difﬁcult for the ground umpire and even the third umpire to 
determine if a delivery is legal or not. 
2.3 
Rules of LBW 
Simply [4] put, the following factors must be met for the LBW regulations in cricket:
. Whether the delivery was legal or not
. Whether the bowling side requested LBW from the umpire
. Where the ball contacts the pitch
. Whether it strikes the bat
. Where the ball contacts the batsman
. Whether the batsman is playing a shot or leaving the ball
. Whether the ball was likely to strike the stumps. 
RULE 1: The delivery must be lawful when bowled.

Computer Vision and Image Segmentation: LBW Automation Technique
251
RULE 2: For LBW, the ﬁelding side must make an appeal. 
RULE 3: Prohibits the ball from pitching on the wicket’s leg side. 
While a batsman is inside the batting crease, their front foot should be either 
present between the leg stump and middle stump or cross it. If the pitch misses their 
leg stump and the line of stumps, which is an illogical line established between three 
stumps, it shouldn’t be considered an out. 
In the second case, if the ball pitches in that line of stumps but more than 50% of 
it misses the leg stump or off stump, it shouldn’t be considered out. 
RULE 4: States that the bat and ball cannot make contact. 
RULE 5: States that the impact must be in line with the stumps if the batsman is 
struck by the ball while attempting to play a stroke. 
RULE 6: States that the ball must be aimed at the stumps. 
2.4 
Overstep No Ball 
If the bowler’s front foot crossed the pooping crease or their backfoot was outside 
of the return crease, the delivery might be ruled a no ball. 
Five possible outcomes are listed: overstepping the bowling line of the batting 
crease, throwing the ball before ﬁnal stepping or before touching the ground, stepping 
out of the side box of the batting crease, shifting the ﬁelders during bowling, and 
height no ball. 
3 
Related Work 
Cricket has been using a variety of technology and strategies over the past ten years. 
Rahish Tandon and Dr. Amitabha, for instance, use semantic analysis to ﬁnd events 
in broadcast footage. Another innovation is “Hawk Eye,” a computer vision system 
that uses visual tracking to provide a moving representation of the ball’s statistically 
most likely route. In addition, Hot Spot and Snicko metre technologies are employed 
to track when the ball is touched by the bat. In paper [5] author Wazir Zada Khan, 
Mohamed Y Aalsalem, and Quartul Arshad suggested a different approach in which 
two sensors would be implanted in the popping crease and one in the ballplayer’s 
front foot to identify the no ball. Yet, it is not practical to adapt every pair of bowling 
shoes. 
In paper [6] The SIFT characteristics of David Lowe and many extended tracking 
algorithms aid in the identiﬁcation of moving objects.In paper [7] AZM Ehtesham 
Chowdhury, Md Shamsur Rahim, and Md Asif Ur Rahman suggested a new detection 
method, utilising computer vision to identify the absence of a ball while using camera 
recordings.

252
J. Nayak et al.
4
Proposed Methodology 
4.1 
For No Ball Detection 
In this study, the pixel size from the video stream is analysed using an image subtrac-
tion approach to ﬁnd the no ball. Four cameras and a popping crease are positioned 
around the cricket ﬁeld’s pitch. The suggested approach will combine images from 
cameras using image fusion techniques, and the no ball detection algorithm will 
ﬁnd the absence of a ball. The programme must validate the bowled delivery before 
moving on to the next stage. The proposed algorithm is given below: 
4.2 
Algorithm 
4.2.1
Image Fusion 
The proposed algorithm used image fusion technique to get the proper image for 
processing: 
Step-1: import pywt, cv2, numpy 
Step-2: fusing the coefﬁcient 
Step-3: applying wavelet transform on every image 
Step-4: applying image fusion for each level 
Step-5: after fusing coefﬁcient, transfer back to get image 
Step-6: normalising the values in unit8 
Step-7: show image 
4.2.2
Algorthm for No Ball Detection [2, 5] 
1. templateRGB, testRGB are arrays size 1280 × 720 × 3 
2. templateFrame, testFrame are arrays of size 1280 × 720 
3. creasePoint is the X co-ordinate or column number of the marked line or crease. 
4. noBall = TRUE or FALSE 
5. thresh = getThreshold(); 
6. templateFrame = rgb2gray (templateRGB) 
7. testFrame = rgb2gray (testRGB) 
8. creasePoint = lineHoughFunc (templateFrame) + 1 
9. testFrame (1 to 720, creasePoint) = 1 
10. templateFrame (1 to 720, creasePoint) = 1 
11. changeInR1 = testFrame(1 to 720, 1 to creasePoint-1) - templateFrame(1 to 
720, 1 to creasePoint-1) 
12. changeInR2 = testFrame (1 to 720, creasePoint to 1279) - templateFrame(1 to 
720, creasePoint to 1279)

Computer Vision and Image Segmentation: LBW Automation Technique
253
13. meanR1 = mean (mean (changeInR1)) 
14. meanR2 = mean (mean (changeInR2)) 
15. if (meanR2<thresh) 
noBall = TRUE 
else if (meanR2 > thresh) 
noBall = FALSE 
else 
noBall = FALSE 
4.2.3
For Ultra-Edge Detection Technique 
Modern stumps are equipped with high-performance microphones that can pick up 
even the smallest disturbances from the batting side. It is possible to determine if the 
ball is contacting the edge of the bat by gathering and analysing the recorded data. 
Algorithm 
Code for mic (micro controller Arduino device): 
#ultra-edge detection 
Step-1: setting soundpin 
Step-2: set the threshhold value for sound sensor 
Step-3: taking input from the soundpin 
Step-4: read analog data from sound sensor 
#python code 
Step-5: import serial 
Step-6: creating object to take the input 
Step-7: for i in range (0, 6000, increment by 1) 
Step-8: taking input and decode by UTF-8 
Step-9: if there is an “edge” goto BALL TRACKING 
Step-10: else no edge detects, print “NOT OUT” 
Noise reduction technique: 
Step-11: import wavﬁle.scipy.io, noisereduce 
Step-12: load the data from soundpin 
Step-13: setting the signal for the noise ratio 
Step-14: reduce the noise 
Step-15: check if there is an “edge”, print “NOT OUT” 
Step-16: else goto BALL TRACKING 
4.2.4
Ball Tracking 
It is possible to depict ball tracking by analysing the recorded video stream and 
employing the segmentation by colour approach through pixel-to-pixel analysis.

254
J. Nayak et al.
Algorithm: 
#taking input 
Step-1: import cv2, numpy, imutils, matplotlib, os, re 
Step-2: check the input method set 1 for camera, 2 for video feed 
#Start Reading 
Step-3: while (read till end) 
Step-4: capture frame by frame 
Step-5: if score card present, remove it 
Step-6: destroy the windows 
#reading frames 
Step-7: listdown all the ﬁle names 
Step-8: reading frames and ﬁxing the threshhold before a scene change 
Step-9: taking a list 
Step-10: for i in range of length (image) 
Step-11: append i in the list 
Step-12: sort the list 
Step-13: plot the ﬁgure (ﬁgsize = (20,4)) 
Step-14: scatter the image 
Step-15: prepare the ﬁnal frame 
#applying segmentation and Gaussian blur 
Step-16: ﬁxing the threshold (threshhold = 15*10e3) 
Step-17: providing threshhold value 200 for white colour ball 
Step-18: plot the ﬁgure 
Step-19: extract the patches from an image using contours 
Step-20: preparing the dataset 
Step-21: preparing the video using the frames 
#decision 
#If the ball hit the stump by 100% clearly out 
#If it’s an on-ﬁeld umpire call, ﬁnal decision should be taken by 3rd umpire 
#If the ball misses the stumps, not out 
4.2.5
How It Works? 
When the ﬁeld umpire requests that the third umpire examine the judgement, it will 
begin to investigate the LBW procedures. 
1. Using a machine detection technology to choose the right camera and gathering 
the authorised video stream. If the delivery is legal, examination of the footage 
utilising the image subtraction technique and pixel-to-pixel analysis can identify 
the no ball.

Computer Vision and Image Segmentation: LBW Automation Technique
255
2. It is possible to determine if the ball contacts the bat by gathering data from the 
microphone implanted in the stump and using noise reduction and subtraction 
techniques. If not, it can continue if necessary. 
3. To identify the ball pitching, the video stream must be analysed. If the results 
meet the following criteria, the batsman is called out. 
Conditions 
a. If the ball pitching misses the off-stump or leg stump by more than 50% then, it 
is not out else check for the second condition. 
b. 
After pitching if the ball misses the stump by more than 50% then, it is not out. 
5 
Results 
See Fig. 1 and 2 
Fig. 1 First it will detect the bowl delivery is legal or not by image fusion and no-ball detection 
(Hoops transformation) technique

256
J. Nayak et al.
Fig. 2 This is the ﬁnal output after detecting the no-ball, checking the ultra-edge and mapping the 
ball tracking 
6 
Conclusion and Future Scope 
In order to detect no ball, ultra-edge, and LBW in a cricket match, we have critically 
examined and applied several computer visions, image subtraction techniques, and 
segmentation by colour methods in this article. 
Initially, utilising the Hough transformation algorithm and the pixel-to-pixel 
image subtraction approach in the video stream, we have identiﬁed the no ball in 
our proposed methodology. Next, for ultra-edge detection and noise reduction, we 
utilised the Python serial package; for ball tracking, we used the colour segmenta-
tion approach. The human perception mistakes and effort required to make decisions 
on the cricket ﬁeld would be reduced by our suggested strategy. To the best of our 
knowledge, the LBW detection technique is used in conjunction with a number of 
algorithms to make its deployment successful and efﬁcient. 
References 
1. Cricket, Wikipedia (2022). https://en.m.wikipedia.org/wiki/Leg_before_wicket#:~:text=The% 
20deﬁnition%20of%20leg%20before,Marylebone%20Cricket%20Club%20(MCC) 
2. M.C.C laws, Wikipedia (2016). https://www.espncricinfo.com/story/the-evolution-of-the-lbw-
1074311 
3. Cricket pitch (2019). https://www.dlgsc.wa.gov.au/sport-and-recreation/sports-dimensions-
guide/cricket 
4. Rules for LBW (2022). https://cricketershub.com/what-are-the-lbw-rules-in-cricket/

Computer Vision and Image Segmentation: LBW Automation Technique
257
5. Khan WZ, Aalsalem MY, Arshad QA (2011) The aware cricket ground. arXiv preprint arXiv: 
1109.6199 8(4):269–273 
6. Aalsalem MY, Khan WZ (2011) A general architecture for decision making during sports. Int J 
Comput Sci 9(8):54 
7. Malu MS (2015) No ball detection, vol 5, no 3, pp 111–113

A Mixed Collaborative Recommender 
System Using Singular Value 
Decomposition and Item Similarity 
Gopal Behera, Ramesh Kumar Mohapatra, and Ashok Kumar Bhoi 
Abstract Nowadays, Recommendation system plays a vital role in industries like 
e-commerce, music apps or newsgroup, retailers, etc. Broadly, recommender system 
techniques are categorized into collaborative ﬁltering and content based. In contrast, 
most recommendation models adopt collaborative ﬁltering techniques such as matrix 
factorization (MF) and cosine similarity. However, the above model only deploys 
single techniques, which leads to poor recommendations to the individuals. Therefore 
we propose a mixed collaborative ﬁltering-based recommender system (RS), a novel 
approach to improve the performance of the RS and mitigate the drawback of a 
single technique-based collaborative model. The proposed model is composed of two 
techniques such as singular value decomposition and cosine similarity techniques. 
Further, we examined our model’s performance on real-world datasets and found 
that the proposed approach signiﬁcantly outperformed the baseline models. 
Keywords Recommender system · Collaborative ﬁltering · Matrix factorization ·
Cosine similarity · SVD 
1 
Introduction 
Nowadays, recommender systems (RSs) [ 6] are extensively deployed in e-commerce 
platforms, like eBay, Amazon, and Netﬂix, to promote their sales and improve the 
individual experience. The RSs analyze users’ behavior, such as purchasing records, 
visiting, or clicking, then predict ratings or references of users’ to various products 
and recommend the highest rating item to an active user. During the last decade, RS 
G. Behera (B) · A. K. Bhoi 
Government College of Engineering Kalahandi, Bhawanipatna, Odisha, India 
e-mail: gbehera@gcekbpatna.ac.in 
A. K. Bhoi 
e-mail: akb@gcekbpatna.ac.in 
R. K. Mohapatra 
Natitional Institute of Technology Rourkela, Rourkela, Odisha, India 
e-mail: mohapatrark@nitrkl.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_23 
259

260
G. Behera et al.
has drawn much attention in various domains of research, such as data mining [ 18, 
23], information retrieval [ 10, 12, 21], and machine learning [ 5]. Collaborative ﬁl-
tering techniques such as Matrix factorization (MF) [ 4, 15] is one of the most widely 
deployed techniques in RSs. Despite its popularity, the MF still does not provide 
better recommendations; that is, the items/products with few preferences may not be 
recommended to the intended users. Scholars have tried to utilize primary data along 
with the original dataset to alleviate the above problem, such as transfer learning and 
social recommendation [ 12, 16, 25]. 
From the primary datasets, RSs could retrieve additional information on users 
or items, thus generating a better recommendation. However, these approaches are 
useful for handling sparse datasets. At the same time, it has suffered from the fol-
lowing major drawbacks as Availability of auxiliary data, Quality of auxiliary data, 
etc. Moreover, we focus on utilizing existing ones for better performance rather 
than taking primary datasets. In this article, we combine the techniques of singular 
value decomposition and cosine similarity to form a novel approach. We outline our 
contributions as follows: 
– We developed a novel approach by combining SVD and cosine similarity. Our 
approach does not require auxiliary information from the datasets, yet it still works 
well. Also, we deploy cosine similarity to raise the caliber of MF recommendations. 
– To verify the effectiveness of the proposed model, we demonstrate the experiment 
on Movielens data and compare the outcomes with the baseline methods. 
The structure of the remaining part of the paper is as follows. 
Sect. 2 illustrates an overview of the past work. In Sect. 3, we discuss the problem def-
inition of an existing model. Section 4 elaborates on the proposed solution. Section 6 
illustrates the experimental setup along with results and conclusion with the future 
scope mentioned in Sect. 7. 
2 
Literature Review 
The Matrix Factorization (MF) approximates the interaction matrix upper RR into a low-
rank user (upper UU) and item latent vectors (upper VV ), respectively. The fundamental premise 
of matrix factorization is that a customer’s choice is affected by a fewer number of 
latent features. The rating or the prediction score of an item is set on according to 
the users’ preference for an item [ 13, 14, 17]. MF is one of the most often employed 
techniques in collaborative ﬁltering because of its efﬁciency and effectiveness. In 
the year 2019 [ 22], Shah proposed a book RS using item-item CF. He deployed 
cosine metrics to compute similarity scores among books and recommend top-scored 
books to the user. Many Researchers have carried out research on fundamental MF 
to improve the effectiveness of RS. In order to enforce non-negativity in user and 
item vectors, Aghdam et al. [ 1] presented Non-negative MF (NMF), which has been 
effective in computer vision (CV) applications. By utilizing weights as an indicator

A Mixed Collaborative Recommender System ...
261
matrix, Zhang et al. [ 25] suggested Weighted NMF to signify the visibility of entries 
in R. 
Mnih et al. [ 17] introduced probabilistic MF (PMF) as well as employed a logistic 
function to restrict the anticipated range between 0 and 1 and a Gaussian distribution 
to initializeupper UU andupper VV . In [  15], Koren et al. summarized and provided a general frame-
work for MF. Also, scholars have incorporated auxiliary data to manage the sparsity 
issue of CF. In order to create virtual interactions for users’ who do not explicitly 
state their suggestions on a product, Zhang et al. [ 25] utilized review sentiment anal-
ysis. To maintain neighborhood information in user-item latent vectors, Gu et al. [ 8] 
suggested the Graph-based WNMF. Behera and Co [ 7] employed temporal features 
in collaborative ﬁltering to enhance personalized recommendations of a user. 
Wang [ 24] proposed a CF technique that combines Singular Value Decomposition 
(SVD) and Trust Factors. Also, they used cosine measures for similarity computation. 
Sandeep and Co [ 19] introduced a new technique that uses momentum-based Gradi-
ent Descent known as Accelerated SVD. Mala and co [ 20] discussed an online-based 
movie RS that suggests similar ﬁlms to users according to their interests using various 
RS techniques such as K-Nearest Neighbor (KNN), SVD, and Restricted Boltzmann 
Machines (RBM). They observed that singular value decomposition performs better 
compared to ALS and RBM. 
3 
Problem Deﬁnition 
Let upper MM users, upper NN products and a user-item interaction upper R equals upper R Subscript i comma j Superscript upper M times upper NR = RM×N
i, j
with each value 
of upper R Subscript i comma jRi, j indicating the interaction score of i Superscript t hith user to j Superscript t h jth product. To generate a 
recommendation for i Superscript t hith user, the RS model needs to estimate the missing entries of 
i Superscript t hith row of the interaction matrix (R) and suggest the items with the highest rating. 
According to MF, the upper RR of user ii to the item j j is deﬁned in Eq. 1. 
upper R equals upper U sigma summation upper VR = U
Σ
V
(1) 
whereupper UU: is the user latent vector ofupper M times upper KM × K andupper VV : is the item latent vector ofupper N times upper KN × K, 
and s u m sum: is the diagonal matrix that represents the weight of latent features. The 
cost function of Matrix Factorization (MF) is deﬁned in Eq. 2. 
upper J equals underset y Subscript u Baseline comma y Subscript v Baseline of a r g m i n sigma summation sigma summation left parenthesis upper R Subscript i j Baseline minus upper U Subscript i Baseline upper V Subscript j Superscript upper T Baseline right parenthesis squared plus StartFraction lamda Subscript u Baseline Over 2 EndFraction sigma summation StartAbsoluteValue EndAbsoluteValue upper U Subscript i Baseline StartAbsoluteValue EndAbsoluteValue squared plus StartFraction lamda Subscript v Baseline Over 2 EndFraction sigma summation StartAbsoluteValue EndAbsoluteValue upper V Subscript j Baseline StartAbsoluteValue EndAbsoluteValue squaredJ = argmin
yu,yv
Σ Σ
(Ri j −UiV T
j )2 + λu
2
Σ
||Ui||2 + λv
2
Σ
||Vj||2
(2) 
where lamda Subscript vλv and lamda Subscript uλu are regularization parameters to avoid the overﬁtting of the model. 
At the same time, we need to minimize the error by training the model.

262
G. Behera et al.
4 
Proposed Method 
In this article, we propose a mixed collaborative ﬁltering technique to improve the 
performance of the recommender system. Figure 1 illustrates the operational ﬂow of 
the proposed model. The working procedure of the proposed model is as follows. 
The model is a mix of MF and item-item CF in sequential order. That is, the model not 
only learns the user-item latent characteristics but also includes the item similarity 
component while recommending the items to the users. Therefore the model ﬁrst 
receives user-item embedding from the MF model. Then we apply cosine similarity 
between individual user latent and item vectors to predict the ﬁnal score. After sorting 
the ﬁnal score, we pick the top upper KK item to recommend to each user. Therefore the 
cost function mentioned in Eq. 2 is redeﬁned in Eq. 3. 
upper J equals underset y Subscript u Baseline comma y Subscript v Baseline of a r g m i n sigma summation sigma summation c Subscript u i Baseline left parenthesis upper R Subscript u i Baseline minus upper U Subscript u Baseline upper V Subscript i Superscript upper T Baseline right parenthesis squared plus lamda left parenthesis sigma summation StartAbsoluteValue EndAbsoluteValue upper U Subscript u Baseline StartAbsoluteValue EndAbsoluteValue squared plus sigma summation StartAbsoluteValue EndAbsoluteValue upper V Subscript i Baseline StartAbsoluteValue EndAbsoluteValue squared right parenthesisJ = argmin
yu,yv
Σ Σ
cui(Rui −UuV T
i )2 + λ(
Σ
||Uu||2 +
Σ
||Vi||2)
(3) 
where 
upper R Subscript u i Baseline equals StartLayout Enlarged left brace 1st Row 1st Column 1 2nd Column upper R greater than 0 3rd Column Blank 4th Column Blank 2nd Row 1st Column 0 2nd Column upper R equals 0 3rd Column w h e r e upper R equals r Subscript u i Baseline 4th Column Blank EndLayoutRui =
{1 R > 0
0 R = 0 where R = rui
Fig. 1 Framework of a mixed collaborative recommender system

A Mixed Collaborative Recommender System ...
263
and c Subscript u i Baseline equals 1 plus alpha r Subscript u icui = 1 + αrui, is the conﬁdence values that the user (uu) providing the value to 
item(ii). Further, we apply the ALS optimization [ 4] to minimize the cost function 
deﬁned in Eq. 3. The optimal latent features of users and items are obtained after 
taking the derivative w.r.t user factor (upper U Subscript uUu) and item factor (upper V Subscript iVi) as given below. 
upper U Subscript u Baseline equals left parenthesis upper V Superscript upper T Baseline upper V plus upper V Superscript upper T Baseline left parenthesis upper C Superscript u Baseline minus upper I right parenthesis upper V plus lamda upper I right parenthesis Superscript negative 1 Baseline upper V Superscript upper T Baseline upper C Superscript u Baseline p left parenthesis u right parenthesisUu = (V T V + V T (Cu −I)V + λI)−1V T Cu p(u)
upper V Subscript i Baseline equals left parenthesis upper U Superscript upper T Baseline upper U plus upper U Superscript upper T Baseline left parenthesis upper C Superscript i Baseline minus upper I right parenthesis upper U plus lamda upper I right parenthesis Superscript negative 1 Baseline upper U Superscript upper T Baseline upper C Superscript i Baseline p left parenthesis i right parenthesisVi = (U TU + U T (Ci −I)U + λI)−1U T Ci p(i)
Further cosine similarity is applied to determine the ﬁnal prediction score as deﬁned 
in Eq. 4. 
upper C o s left parenthesis upper U Subscript u Baseline comma upper V Subscript i Baseline right parenthesis equals StartFraction upper U Subscript u Baseline upper V Subscript i Superscript upper T Baseline Over StartAbsoluteValue EndAbsoluteValue upper U Subscript u Baseline StartAbsoluteValue EndAbsoluteValue StartAbsoluteValue EndAbsoluteValue upper V Subscript i Baseline StartAbsoluteValue EndAbsoluteValue EndFractionCos(Uu, Vi) =
UuV T
i
||Uu||||Vi||
(4) 
5 
Experiment Details and Dataset 
This section elaborates on the experimental setup and datasets. We use python lan-
guage, Ubuntu20.420.4, Intel core8 Superscript t h8th generationi Baseline 5i5 processor, and Jupyter notebook for 
the experimental purpose. The experiments are carried out by setting training and 
test datasets into the different ratios of 7:3. 
5.1 
Datasets 
Our approach needs the rating dataset only. Therefore, we use two datasets to verify 
the effectiveness of the proposed model. Table 1 shows a detailed description of the 
datasets. It is observed that the ML-1M dataset is more sparse than the ML-100K 
data, where the sparsity is measured as follows. 
upper S p a r s i t y equals StartFraction upper R Over upper V EndFractionSparsity = R
V
where upper RR: denotes total number of interactions and upper VV : be the volume of dataset i.e., 
upper V equals upper N u m b e r o f i t e m s times upper N u m b e r o f u s e r sV = Number of items × Number of users. 
Table 1 Overview of datasets 
Data
# Users
# items
No. of interaction Sparsity 
ML-100K [ 9]
943
1682
100,000
93.69% 
ML-1M [ 9]
6040
3900
10,00,000
95.75%

264
G. Behera et al.
5.2 
Compared Methods 
We brieﬂy discuss the state-of-art methods that are compared with the proposed 
system. 
– SVD [ 15]: This latent factor model uses the singular value decomposition principle 
to decompose the upper RR into low-rank approximation. 
– PMF [ 11]: This is similar to SVD. Whereas it uses probabilistic Gaussian distri-
bution with mean and variance of user and latent features. 
– NMF [ 1]: This method includes only non-negative feature values of users and 
items while ﬁnding the interactions between user-items. 
– RecSVD++ [ 2]: This technique is an extension of SVD. Which includes implicit 
features along with explicit rating data. 
6 
Performance Measures and Result Analysis 
The accuracy of an RS signiﬁes the percentages of variation from the actual score. 
RS, predict users’ interest based on RS models that are accessing the prediction 
of hidden user-item pairs. Whereas the predicted values lie between left bracket 0 comma 1 right bracket[0, 1]. A high 
value means the user is more interested in purchasing the item. In this paper, we 
considered two metrics, Root Mean Squared Error (RMSE) [ 3] and Mean Absolute 
Error (MAE), to measure the performance accuracy of the model. 
upper R upper M upper S upper E equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation left parenthesis r Subscript i Baseline minus ModifyingAbove r With caret Subscript i Baseline right parenthesis squared EndRootRMSE =
/
1
N
Σ
(ri −ˆri)2
(5) 
where r Subscript iri: is the target rating value and ModifyingAbove r With caret Subscript iˆri is the observed rating value. The lower the 
RMSE indicates the predictive model is more accurate. Similarly, MAE is deﬁned 
in Eq. 6. 
upper M upper A upper E equals StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts StartAbsoluteValue r Subscript i Baseline minus ModifyingAbove r With caret Subscript i Baseline EndAbsoluteValueM AE = 1
N
n
Σ
i=1
IIri −ˆri
II
(6) 
6.1 
Result Analysis 
The effectiveness of a mixed CF model is veriﬁed by demonstrating the experiments 
on standard datasets. Table 2 shows the outcomes of proposed and benchmark meth-
ods in terms of RMSE and MAE. The outcomes reveal that the performance of our 
model signiﬁcantly improves accuracy compared to state-of-the-art models. More-
over, we noticed that our model’s RMSE and MAE value is achieved better than 
SVD, PMF, NMF, and RecCFSVD++ in all datasets.

A Mixed Collaborative Recommender System ...
265
Table 2 RMSE and MAE comparison of various models on Movielens datasets 
Models
ML-100K
ML-1M 
RMSE
MAE
RMSE
MAE 
SVD [ 15]
0.9432
0.7447
0.8860
0.6964 
PMF [ 11]
0.9667
0.7632
0.8855
0.6974 
NMF [ 1]
0.9748
0.7660
0.9200
0.7267 
Rec-
CFSVD++ [ 2] 
0.9201
0.7219
0.8716
0.6811 
Proposed Model 
0.9159
0.7164
0.8628
0.6753 
(a) ML-1M
(b) ML-100K 
Fig. 2 Comparison of RMSE and MAE 
Also, we observed that the RMSE and MAE values reveal that, as sparsity 
increases from 93.69 to 95.75%, that is, from dataset ML-100K to ML-1M, the 
performance of NMF, PMF, SVD, and recCFSVD++ degrades. At the same time, 
our model exhibits the highest degradation as compared to the state-of-art models. 
Further, Fig. 2 presents the graphical representation of the performance compar-
ison of our model with benchmark models. It is observed that our model achieved 
better RMSE and MAE values compared to benchmark models, even in highly sparse 
data. 
7 
Conclusion and Future Scope 
In this article, we proposed a mixed collaborative ﬁltering technique to enhance 
the collaborative ﬁltering-based recommendation prediction task. The model not 
only learns the user-item latent characteristics but also includes item similarity for 
each user as an additional component to recommend top-k items to the users. We 
used two Movielens datasets for experimental purposes. The results show that the

266
G. Behera et al.
proposed model outperforms the state-of-art models. In the future, we will extend 
the experimental evaluation by considering the sentiment of each user. 
References 
1. Aghdam MH, Analoui M, Kabiri P (2017) Collaborative ﬁltering using non-negative matrix 
factorisation. J Inf Sci 43(4):567–579 
2. Anwar T, Uma V, Srivastava G (2021) Rec-CFSVD ++: implementing recommendation system 
using collaborative ﬁltering and singular value decomposition (SVD)++. Int J Inf Technol Decis 
Making 20(04):1075–1093 
3. Behera G, Nain N (2020) A comparative study of big mart sales prediction. In: Nain N, Vipparthi 
SK, Raman B (eds) CVIP 2019, vol 1147. CCIS. Springer, Singapore, pp 421–432. https://doi. 
org/10.1007/978-981-15-4015-8_37 
4. Behera G, Nain N (2021) Collaborative recommender system (crs) using optimized SGD -
ALS. In: Singh M, Tyagi V, Gupta PK, Flusser J, Ören T, Sonawane VR (eds) ICACDS 2021, 
vol 1440. CCIS. Springer, Cham, pp 627–637. https://doi.org/10.1007/978-3-030-81462-5_55 
5. Behera G, Nain N (2022) GSO-CRS: grid search optimization for collaborative recommenda-
tion system. S¯adhan¯a 47(3):1–12 
6. Behera G, Nain N (2002) Trade-off between memory and model-based collaborative ﬁltering 
recommender system. In: Proceedings of the international conference on paradigms of com-
munication, computing and data sciences. pp 137–146. Springer, Singapore. https://doi.org/ 
10.1007/978-981-19-8742-7 
7. Behera G, Nain N (2023) Collaborative ﬁltering with temporal features for movie recommen-
dation system. Procedia Comput Sci 218:1366–1373 
8. Gu Q, Zhou J, Ding C (2010) Collaborative ﬁltering: weighted nonnegative matrix factorization 
incorporating user and item graphs. In: Proceedings of the 2010 SIAM international conference 
on data mining, pp 199–210. SIAM 
9. Harper M, Konstan J (2016) Movielens data set. ACM Trans Interact Intell Syst 5(1):19 
10. He X, Zhang H, Kan MY, Chua TS (2016) Fast matrix factorization for online recommendation 
with implicit feedback. In: Proceedings of the 39th international ACM SIGIR conference on 
research and development in information retrieval, pp 549–558 
11. Huang L, Tan W, Sun Y (2019) Collaborative recommendation algorithm based on matrix 
factorization in probabilistic latent semantic analysis. Multim Tools Appl 78(7):8711–8722 
12. Konstas I, Stathopoulos V, Jose J.M (2009) On social networks and collaborative recommen-
dation. In: Proceedings of the 32nd international ACM SIGIR conference on Research and 
development in information retrieval, pp 195–202 
13. Koren Y (2008) Factorization meets the neighborhood: a multifaceted collaborative ﬁltering 
model. In: Proceedings of the 14th ACM SIGKDD international conference on Knowledge 
discovery and data mining, pp 426–434 
14. Koren Y (2009) Collaborative ﬁltering with temporal dynamics. In: Proceedings of the 15th 
ACM SIGKDD international conference on knowledge discovery and data mining, pp 447–456 
15. Koren Y, Bell R, Volinsky C (2009) Matrix factorization techniques for recommender systems. 
Computer 42(8):30–37 
16. Ma H, King I, Lyu MR (2009) Learning to recommend with social trust ensemble. In: Pro-
ceedings of the 32nd international ACM SIGIR conference on Research and development in 
information retrieval, pp 203–210 
17. Mnih A, Salakhutdinov RR (2007) Probabilistic matrix factorization. In: 20th proceedings 
conference on advances in neural information processing systems 
18. Pan R, Zhou Y, Cao B, et al (2008) One-class collaborative ﬁltering. In: 2008 Eighth IEEE 
international conference on data mining, pp 502–511. IEEE

A Mixed Collaborative Recommender System ...
267
19. Raghuwanshi SK, Pateriya RK (2021) Accelerated singular value decomposition (ASVD) 
using momentum based gradient descent optimization. J King Saud Univ Comput Inf Sci 
33(4):447–452 
20. Saraswat M, Dubey A, Naidu S, Vashisht R, Singh A (2020) Web-based movie recommender 
system. In: Hu Y-C, Tiwari S, Trivedi MC, Mishra KK (eds) Ambient Communications and 
Computer Systems, vol 1097. AISC. Springer, Singapore, pp 291–301. https://doi.org/10.1007/ 
978-981-15-1518-7_24 
21. Sarwar B, Karypis G, Konstan J, Riedl J (2001) Item-based collaborative ﬁltering recommen-
dation algorithms. In: Proceedings of the 10th international conference on World Wide Web, 
pp 285–295 
22. Shah K (2019) Book recommendation system using item based collaborative ﬁltering. Int Res 
J Eng Technol 6(5):5960–5965 
23. Shi J, Long M, Liu Q, Ding G, Wang J (2013) Twin bridge transfer learning for sparse collab-
orative ﬁltering. In: Pei J, Tseng VS, Cao L, Motoda H, Xu G (eds) PAKDD 2013, vol 7818. 
LNCS (LNAI). Springer, Heidelberg, pp 496–507. https://doi.org/10.1007/978-3-642-37453-
1_41 
24. Wang J, Han P, Miao Y, Zhang F (2019) A collaborative ﬁltering algorithm based on SVD 
and trust factor. In: 2019 international conference on computer, network, communication and 
information systems (CNCI 2019), pp 33–39. Atlantis Press (2019) 
25. Zhang W, Ding G, Chen L, Li C (2010) Augmenting Chinese online video recommendations by 
using virtual ratings predicted by review sentiment classiﬁcation. In: 2010 IEEE international 
conference on data mining workshops, pp 1143–1150. IEEE

Hybrid Clustering-Based Fast Support 
Vector Machine Model for Heart Disease 
Prediction 
Chaitanya Datta Maddukuri and Rajiv Senapati 
Abstract Over the past few decades, heart disease has seen signiﬁcant growth 
among all ages and early prediction became necessary. Data mining and machine 
learning techniques are used to solve the prediction problem utilizing new approaches 
to supervised learning. The Internet of Medical Things (IoMT) emerged from the 
combination of multiple ﬁelds and machine learning. The goal of this research is 
to develop an adaptive model for predicting cardiac disease. We provide a ranking-
based hybrid feature selection method for identifying essential characteristics. The 
model proposed in this paper employs a clustering method in conjunction with sup-
port vector machine (SVM) to save training time and eliminate classiﬁcation errors, 
hence boosting the model’s performance and increasing its efﬁciency. 
Keywords Data mining · Machine learning · Feature Selection · Clustering ·
PCA · SVM · Heart disease prediction 
1 
Introduction 
Heart disease is a widespread condition that affects people of all ages worldwide. 
Early prediction became necessary in order to identify problems at an early stage 
without making them severe. This evolves from various factors that contribute to 
prediction, such as age, cholesterol, blood pressure, and many other external factors. 
Several hybrid models are introduced by combining ML techniques with neural 
networks such as ANN and CNN to improve the performance of the model and 
achieve better prediction accuracy. The results obtained are promising, accurate, and 
C. D. Maddukuri (B) · R. Senapati 
SRM University, Amaravati, Andhra Pradesh, India 
e-mail: chaitanyadatta_m@srmap.edu.in 
R. Senapati 
e-mail: rajiv.s@srmap.edu.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_24 
269

270
C. D. Maddukuri and R. Senapati
reliable. The neural networks are considered a better tool for predicting disease. 
In this paper, we present a hybrid model, clustering-based SVM, which combines 
clustering and support vector machines. The main goal is to converge the error rate 
to negligible levels, improve the model’s performance with less training time, and 
improve the prediction accuracy. 
The key objective of the proposed approach is to overcome the drawbacks of 
standard SVM, reduce the time required for model training, improve the model’s 
classiﬁcation accuracy, and enhance its performance in terms of various criteria. 
Another challenge in developing the model is preventing overﬁtting and underﬁtting. 
Model improvement is crucial for achieving enhanced performance and accuracy, 
and it is attained by identifying and selecting an optimal subset of features that serve 
as the foundation of an improved model. This can be accomplished by employing a 
variety of selection criteria to ﬁnd the important features. 
Some of the major contributions of this work are summarized as follows. 
– In this paper, we have proposed a heart disease prediction model using clustering 
and SVM technique. 
– The hybrid prediction model proposed can reduce the training time and could 
minimize the classiﬁcation error. 
The rest of this paper is organized as follows. In Sect. 2, we provide literature review. 
In Sect. 3, we present our proposed prediction model. In Sect. 4, we discuss the 
experimental results and ﬁnally we have concluded our paper in Sect. 5. 
2 
Literature Review 
In this section we have presented a review of state-of-the-art machine learning tech-
niques used for heart disease prediction. The work reported in [ 1] presented a random 
forest based hybrid method in combination with a linear model resulting an accuracy 
of 87%. In [ 2], DBSCAN is used for outlier analysis and then XGBoost algorithm is 
used for prediction using Statlog and Cleveland datasets with an accuracy of 95.90 
and 98.40%, respectively. In [ 3] they presented a neural network-based ensemble 
model that was created by combining posterior probabilities, or the values predicted 
by previous models. They conducted their experiments on the Cleveland dataset taken 
from the UCI repository, which resulted in an accuracy of 89% for classiﬁcation. In 
[ 4], they followed rough sets based on feature selection, fuzzy c-mean clustering for 
prediction and hyper-tuned those parameters with genetic and chaos 26 ﬁreﬂy hybrid 
algorithms. In [ 5], they developed a Clinical Decision Support System (CDSS) based 
on the fuzzy Analytic Hierarchy Process (AHP) for attribute selection using weights 
and a fuzzy inference system for predicting heart disease. In [ 6], they proposed 
a hybrid model having multivariate adaptive regression splines (MARS), logistic 
regression (LR), an artiﬁcial neural network (ANN), and rough set (RS) methods for 
converging the explanatory feature set. The other feature set is used as input for the

HCFSVM Model for Heart Disease Prediction
271
ANN, and the studies are conducted on the Cleveland dataset, which comprises 76 
attributes and 303 records taken from UCI, and produces a set of various accuracy 
scores by constructing several models by combining any two of the above methods. 
In [ 7], they introduced an integrated Clinical decision support system (CDSS) based 
on the Fuzzy Analytic hierarchy process (AHP) for feature selection and an Arti-
ﬁcial neural network (ANN) for predicting heart disease and obtained an average 
accuracy of 91% for the prediction. In [ 8], they used Cluster-based decision tree 
learning (CDTL), a hybrid approach to predict heart disease, and based on entropy 
partitioning, they reduced the dimensions among features and achieved an accuracy 
of 86% by predicting them using Random Forest (RF) based on CDTL. The experi-
mental results were obtained by testing the model using Cleveland dataset from the 
UCI repository. In [ 9] a recursive model is presented using random forest with an 
improved linear model in combination with backpropagation algorithm. The exper-
imental results were obtained using Cleveland dataset from UCI repository with an 
accuracy of 96.6%. Some other techniques presented in [ 10– 17] may also be used in 
different healthcare applications. 
Several machine learning models are found in the literature for disease prediction. 
However, reducing the training time is still an important issue due to the ever-growing 
size of the dataset. Hence, in this paper, we have introduced a hybrid model for heart 
disease prediction using clustering and SVM. 
3 
Hybrid Clustering-Based Fast Support Vector Machine 
Model 
The machine learning model proposed in this work is presented in Fig. 1. The process 
starts with pre-processing the data and continues with a hybrid feature selection 
process to build the most accurate predictive model, and the iterative process will 
result in a combined ranking of features from the feature set. Phase 1 describes data 
pre-processing, Phase 2 describes feature selection, and Phase 3 describes predictive 
Data 
Preprocessing 
Outlier 
Analysis 
Hybrid 
PCA
E−M Clustering
SVM
Prediction 
of 
Heart disease 
Feature Selection 
Fig. 1 HCFSVM model for heart disease prediction

272
C. D. Maddukuri and R. Senapati
modelling with classiﬁcation using various machine learning techniques. Finally, 
model performance is discussed. 
3.1 
Dataset 
We have used heart disease dataset available in UCI repository [ 18] to test our 
proposed model. The dataset consists of 303 patients health information related 
to heart disease with two class of target variables i.e. yes and no representing the 
presence/absence of heart disease respectively. The actual dataset consists of 14 
attributes, and further data preprocessing and feature selection are applied to the 
dataset during model development. 
3.2 
Data Preparation 
Data preparation is the primary and most important stage in preparing data to suit 
the model, as it aids in the construction of a precise model and the avoidance of all 
data that may be irrelevant or missing due to human error. Following data collection, 
the heart disease data is preprocessed and organised into various records from one or 
more data sources. The master dataset contains various 303 data records, and none 
of the instances among those records have missing or null values. Variables such as 
target and class variables with multiple classes were observed during analysis. The 
target variable was used to denote the presence or absence of the disease. 
3.3 
Outlier Analysis 
Outliers are often considered as noise, and they need to be identiﬁed and removed. 
This is a data preparation step that aids in the detection of outliers and takes appro-
priate action against them. The presence of such an outlier may make the model 
inefﬁcient. In this paper, this task is carried out in the following ways. 
Univariate Outlier Analysis. A univariate outlier is a situation with an extreme data 
values falls outside the expected population for a single variable. In this approach, 
each features are analyzed independently using different statistical tools such as IQR, 
Zscore, or LOF. From this analysis, two outliers are identiﬁed from ‘thalach’ and 
‘chol’. Some other outliers are also identiﬁed using DBSCAN, and treated using 
principal component analysis. 
Density-Based Spatial Clustering of Applications with Noise (DBSCAN). A large 
dataset with multiple numeric features (also called as multivariate) are highly sus-
ceptible to outliers. In such case, it becomes very difﬁcult to handle outliers using 
statistical methods. DBSCAN is one of the technique that allows us to handle outliers

HCFSVM Model for Heart Disease Prediction
273
for the Multi-variate datasets. It considers three major parameters such as minimum 
points denoted by upper M i n upper P t sMinPts, epsilon denoted by e p seps, and the core-point denoted by 
xx for formation of clusters based on high or low density region and thereby it detects 
outliers [ 19]. 
3.4 
Feature Extraction and Selection 
A ranking-based feature selection technique is adopted in this proposed framework 
for heart disease prediction. This technique consists of Fisher score and chi squaredχ2. 
Fisher Score. It is one of the most popular supervised technique used for feature 
selection. Using this technique, we can determine the rank of a feature and that helps 
in selecting a feature based on the condition. The Fisher score for i Superscript t hith feature i.e. 
denoted byupper S Subscript iSi can be computed using the following equation in terms of mean (mu Subscript i jμi j) 
and variance (rho Subscript i jρi j). 
upper S Subscript i Baseline equals StartFraction sigma summation n Subscript j Baseline left parenthesis mu Subscript i j Baseline minus mu Subscript i Baseline right parenthesis squared Over sigma summation n Subscript j Baseline asterisk rho Subscript i j Superscript 2 Baseline EndFractionSi =
Σ n j(μi j −μi)2
Σ n j ∗ρ2
i j
(1) 
Chi-Squared Test. It is used as a univariate feature selection method. It is basically 
used to compare the observed results with expected results. The purpose is to deter-
mine the difference between upper OO i.e. the observed data and upper EE i.e. the expected data. 
In other words it represents relationship between feature and the target variable. The 
chi squaredχ2 can be computed using the following equation. 
chi Subscript c Superscript 2 Baseline equals sigma summation StartFraction left parenthesis upper O Subscript i Baseline minus upper E Subscript i Baseline right parenthesis Over upper E Subscript i Baseline EndFractionχ2
c =
Σ (Oi −Ei)
Ei
(2) 
where, upper OO, upper EE, cc denotes the observed value, expected value, and degrees of freedom 
respectively. The score of an attribute is calculated based on the chi squaredχ2 values. 
3.5 
Principal Component Analysis (PCA) 
PCA is used in the proposed framework for dimensionality reduction by reducing the 
feature set into an essential feature vector without loosing the original information 
contained in the dataset. This can help in reducing the dimensions of a feature vector 
by mapping it into another dimensional space using transformations without losing 
the originality of the data. This process includes the following two steps: 
– Data standardization transforms all the data into a particular scale using the fol-
lowing equation. 
upper Z equals StartFraction v Subscript i Baseline minus upper V Over sigma EndFractionZ = vi −V
σ
(3)

274
C. D. Maddukuri and R. Senapati
where,v Subscript ivi is the value of data point,upper VV is the mean of variable, andsigmaσ is the standard 
deviation. 
– Covariance matrix helps to clearly understand the relationship among attributes 
and ﬁnd the correlation between the attributes and the target. 
3.6 
Predictive Modeling 
Expectation-Maximization Clustering. EM clustering uses probability distribution 
to compute the cluster memberships and then to maximize the overall probability or 
likelihood of the data in the ﬁnal clusters. It is an iterative method consisting of two 
steps, these are: expectation (E) and maximization (M). Finite Gaussian mixtures 
model (GMM) is used in EM clustering for clustering purpose (Fig. 2). 
Support Vector Machine (SVM). SVM [ 20] is a machine learning technique that 
uses a hyperplane technique for classiﬁcation of the data points as a separating margin 
between the classes. The margin known as the geometric interval indicates the ability 
to classify. The margin varies accordingly depending on the support vectors; those 
are the data points that are close to the hyperplane. Kernel is a function that helps in 
mapping the input data to other forms, which are classiﬁed as nonlinear, linear, and 
polynomial. The goal of the prediction of heart disease (HD) using support vector 
machine (SVM) is to ﬁnd an optimal hyperplane out of all the hyperplanes constructed 
that helps in separating the data points into several regions. The hyperplane is said 
to be optimal if it has the largest margin and a maximum distance between the 
support vectors and the margin. Each region represents the presence or absence of 
heart disease. Figure 3 illustrates the clusters formed using EM clustering and the 
Fig. 2 A plot indicating the principal components from the overall feature set.

HCFSVM Model for Heart Disease Prediction
275
Fig. 3 A ﬁgure indicating the clusters and the hyperplane separating the regions. 
Fig. 4 Curve indicating precision (P) and recall (R) at different thresholds. 
hyperplane with their respective margins, which were identiﬁed using SVM over 
several iterations for the training data to predict heart disease. 
4 
Performance Metrics and Result Analysis 
In this section we have evaluated our model performance using the following per-
formance matrix. 
– Accuracy: Accuracy (ACC) is determined as,

276
C. D. Maddukuri and R. Senapati
Table 1 Performance metrics. 
Metric
Score 
Accuracy
0.98 
Precision
1.00 
Recall
0.97 
F1-Score
0.98 
upper A upper C upper C equals left parenthesis upper T upper P plus upper T upper N right parenthesis divided by left parenthesis upper T upper P plus upper F upper N plus upper F upper P plus upper T upper N right parenthesisACC = (T P + T N)/(T P + F N + F P + T N)
(4) 
– Precision: Precision (PRE) is determined as, 
upper P upper R upper E equals upper T upper P divided by left parenthesis upper T upper P plus upper F upper P right parenthesisP RE = T P/(T P + F P)
(5) 
– Recall: Recall is determined as, 
upper R upper E upper C equals upper T upper P divided by left parenthesis upper T upper P plus upper F upper N right parenthesisREC = T P/(T P + F N)
(6) 
– F1-Score: F1-Score (F1) is determined as, 
upper F Baseline 1 equals 2 left parenthesis upper P times upper R right parenthesis divided by left parenthesis upper P plus upper R right parenthesisF1 = 2(P × R)/(P + R)
(7) 
where P, R are Precision, Recall respectively. A P-R curve, given in Fig. 4, represents 
the trade-off between precision and recall at various thresholds. 
The results presented in Table 1, are obtained by testing the data while splitting 
it for training and testing. The overall data is divided into 70% for training and 30% 
for testing. From the result it is observed that our proposed model is giving 98% 
accuracy. 
5 
Conclusion 
In this paper, we have proposed a machine learning based heart disease prediction 
framework using EM Clustering and SVM. A hybrid feature selection technique is 
also used to extract the critical features for the development of this proposed frame-
work. From the result analysis it is observed that the proposed framework provides 
signiﬁcantly high classiﬁcation accuracy as compared to other methods. Further, the 
hybrid model for prediction may reduce the training time and could minimize the 
classiﬁcation error. The proposed framework may therefore be potentially used as a 
resource to improve heart disease screening for timely treatment plan.

HCFSVM Model for Heart Disease Prediction
277
References 
1. Mohan S, Thirumalai C, Srivastava G (2019) Effective heart disease prediction using hybrid 
machine learning techniques. IEEE Access 7:81542–81554 
2. Fitriyani NL, Syafrudin M, Alﬁan G, Rhee J (2020) HDPM: an effective heart disease prediction 
model for a clinical decision support system. IEEE Access 8:133034–133050 
3. Das R, Turkoglu I, Sengur A (2009) Effective diagnosis of heart disease through neural networks 
ensembles. Expert Syst Appl 36(4):7675–7680 
4. Long, NG, Meesad P, Unger H (2015) A highly accurate ﬁreﬂy based algorithm for heart 
disease prediction. Exp Syst Appl 42(21):8221–8231 
5. Nazari S, Fallah M, Kazemipoor H, Salehipour A (2018) A fuzzy inference-fuzzy analytic 
hierarchy process-based clinical decision support system for diagnosis of heart diseases. Exp 
Syst Appl 95:261–271 
6. Shao YE, Hou CD, Chiu H-C (2014) Hybrid intelligent modeling schemes for heart disease 
classiﬁcation. Appl Soft Comput 14:47–52 
7. Samuel OW, Asogbon GM, Sangaiah AK, Fang P, Li G (2017) An integrated decision support 
system based on ANN and fuzzy AHP for heart failure risk prediction. Exp Syst Appl 68:163– 
172 
8. Magesh G, Swarnalatha P (2021) Optimal feature selection through a cluster-based DT learning 
(CDTL) in heart disease prediction. Evol Intel 14(2):583–593 
9. Guo C, Zhang J, Liu Y, Xie Y, Han Z, Jianshe Yu (2020) Recursion enhanced random forest 
with an improved linear model (RERF-ILM) for heart disease detection on the internet of 
medical things platform. IEEE Access 8:59247–59256 
10. Chakravadhanula AS, Kolisetty J, Samudrala K, Preetham B, Senapati R (2022) Novel decen-
tralized security architecture for the centralized storage system in hadoop using blockchain 
technology. In: 2022 IEEE 7th international conference for convergence in technology (I2CT), 
pp 1–4. IEEE (2022) 
11. Sahoo A, Senapati R (2022) A parallel approach to partition-based frequent pattern mining 
algorithm. In: Udgata S.K, Sethi S, Gao XZ (eds) Intelligent systems, pp 93–102. Springer, 
Singapore (2022). https://doi.org/10.1007/978-981-19-0901-6_9 
12. Sahoo A, Senapati R (2021) A novel approach for distributed frequent pattern mining algorithm 
using load-matrix. In: 2021 International conference on intelligent technologies (CONIT), pp 
1–5. IEEE 
13. Patro PP, Senapati R (2021) Advanced binary matrix-based frequent pattern mining algorithm. 
In: Udgata SK, Sethi S, Srirama SN (eds) Intelligent Systems, vol 185. LNNS. Springer, 
Singapore, pp 305–316. https://doi.org/10.1007/978-981-33-6081-5_27 
14. Sahoo A, Senapati R (2020) A Boolean load-matrix based frequent pattern mining algorithm. 
In: 2020 International conference on artiﬁcial intelligence and signal processing (AISP), pp 
1–5. IEEE 
15. Muttineni S, Yerramneni S, Kongara BC, Venkatachalam G, Senapati R (2022) An interac-
tive interface for patient diagnosis using machine learning model. In: 2022 2nd International 
conference on emerging frontiers in electrical and electronic technologies (ICEFEET) 
16. Ganesh Prasad GSK, Ajay Chowdari A, Pritham Jona, K, Senapat R (2022) Detection of CKD 
from CT scan images using KNN algorithm and using edge detection. In 2022 2nd International 
conference on emerging frontiers in electrical and electronic technologies (ICEFEET), pp 1–4. 
IEEE 
17. Datta C, Senapati R (2022) An adoptive heart disease prediction model using machine learning 
approach. In: 2022 OITS International conference on information technology (OCIT), pp 49– 
54. IEEE

278
C. D. Maddukuri and R. Senapati
18. Dua D, Graff C (2019) UCI machine learning repository. https://archive.ics.uci.edu/: University  
of California, School of Information and Computer Science. IEEE Transactions on Pattern 
Analysis and Machine Intelligence 2019 
19. Ester M, Kriegel H-P, Sander J, Xiaowei X et al (1996) A density-based algorithm for discov-
ering clusters in large spatial databases with noise. In KDD, vol 96, pp 226–231 
20. Cortes C, Vapnik V (1995) Support-vector networks. Mach Learn 20(3):273–297

Forecasting and Analysing Time Series 
Data Using Deep Learning 
Snigdha Sen, V. T. Rajashekar, and N. Dharshan 
Abstract Rising demands in investment in cryptocurrencies are being discussed of 
late in recent times. The most established and well-known cryptocurrency is Bitcoin. 
An accurate prediction of the bitcoin price will always attract more investors. This 
paper aims to demonstrate the effectiveness and appropriateness of several deep 
learning models in time series forecasting. This experiment makes use of the Coin-
Desk Bitcoin Dataset. Our results demonstrate that the Gated Recurrent Unit (GRU) 
based model surpasses all other models in accurately predicting bitcoin prices. We 
experimented with different DL (Deep Learning) models, ranging from a simple 
model to a complicated model. Standard metrics, such as Mean Absolute Error and 
MSE, have been used to analyse each model. In order to make better decisions in the 
near future, this study will beneﬁt the ﬁnance industry. 
1 
Introduction 
Imagine you have a coffee shop and you want to predict the number of coffee cups 
you sell the next month. You will record the number of coffees sold in the present 
month which acts as the data to predict the number of coffees you may sell the next 
month. Thus, here time acts as the variable. This process of making future predictions 
using historical data is called time series processing. 
In layman’s terms, Bitcoin [1] is the name of technology, and it is the type of 
money that is totally virtual. It is the online version of cash that is it does not involve 
a physical transfer of cash. The lowercase “b” in bitcoin stands for the actual units of 
currency [2]. This bitcoin was founded in 2009 by Satoshi Nakamoto [3] a ﬁctitious 
name used by an anonymous person. Over the years many have claimed to be the 
developers of bitcoin, but their identity has not yet been veriﬁed or revealed. Bitcoin 
uses three different cryptographic methods for the data to be kept a secret. A public 
key and a private key will be created by someone who wishes to receive bitcoins. 
Those wishing to send bitcoins are given the public key. The sender will then use
S. Sen envelope symbol · V. T. Rajashekar · N. Dharshan 
Department of CSE, Global Academy of Technology, Bengaluru, Karnataka, India 
e-mail: snigdha.sen@gat.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_25 
279

280
S. Sen et al.
this public key to encrypt the bitcoins before sending them, and only the owner 
of the associated private key will be able to decrypt them [4]. To keep track of 
its transactions, Bitcoin leverages blockchain technology, a component of the third 
generation of web technologies [5]. 
Time series forecasting has gained a lot of popularity in several domains. It 
is mainly classiﬁed into two problems classiﬁcation and forecasting. Forecasting 
involves making future predictions using previous data and classiﬁcation is used to 
output a binary or categorical value (Discrete). Time series is usually a branch of 
supervised learning. It is rare for time series problems to be trained on unsupervised 
models because of so many uncertainties. The result is predicted by computing for 
various loss functions, such as the mean absolute error (MAE), root mean squared 
error (RMSE), and others. Time series problems fall under the category of regression 
problems. Time series forecasting is commonly used to predict stock prices, bitcoin 
prices, etc. and big tech companies use these to predict the chances of unemployment, 
downsizing, etc. Time series forecasting can do so much for society as well. It can 
be used in the medical ﬁeld to predict whether a person is having a certain dis-ease 
or not. 
Time Series data processing’s primary goal is to examine historical data from a 
temporal viewpoint in order to determine how the intended variables will evolve in 
the future. Sometimes not knowing the future is a beneﬁt, because a known future 
is already the past. Coming to the technical side of time series, it is found that the 
model performs well on univariate data and when more classes are added it leads 
to so many uncertainties spiking the loss functions through the roof. This can be 
handled by using different algorithms from the deep learning side of things. 
2 
Preliminaries and Background Study 
The increasing popularity of DL and ML has made them a popular choice in multiple 
domains [20–24]. Abundant work has been reported in the area of time series fore-
casting with varying model performances and approaches. For example, Jiang et al. 
[6] experimented with GRU models using 24-hour bitcoin data to predict the price 
for next hour. Another paper by Fernandez´ et al. [7] demonstrates a comparative 
study of multiple deep recurrent neural network algorithms for bitcoin prediction 
using 29 factors. Their results have a potential impact on ﬁnancial sectors. 
An innovative model that combines a deep learning model and a bagging model 
was proposed by Zhang, et al. [8] on a related subject. This deep model is applied 
to ﬁnd a complex pattern between factors that can inﬂuence bitcoin prices and the 
bagging approach is used to generate more amount of data from the training samples. 
Georgoula et al. [9] aimed to ﬁnd a correlation between the Bitcoin prices and basic 
economic variables, factors related to technology and daily Twitter feeds. As per 
their study, SVM was applied to analyse twitter feeds and it was observed that the 
Twitter sentiment ratio has a strong inﬂuence on Bitcoin prices.

Forecasting and Analysing Time Series Data Using Deep Learning
281
Patrick et al. [10] proposed an approach using bitcoin data of one hour. As per 
their study gradient boosting and recurrent neural networks are best for bitcoin price 
forecasting. They had even done comprehensive factor analysis and indicated that 
technical features ranked top among blockchain-based features and sentiment data 
features. Using data from 2012 to 2018, Sec kin, et al. [11] tested the linear regression 
and SVM methods to predict the price of bitcoin. To make a robust model they had 
used K fold cross-validation strategy as well where they had seen LR provides better 
results compared to SVM in terms of least error MAE, MSE. 
Li et al. [12] have proposed a two-step method where they applied data decom-
position initially to extract factors and then applied by directional deep learning 
model to estimate bitcoin price. They even talked about how their model’s accuracy 
compared to other cutting-edge models. A deep reinforcement learning algorithm-
based framework for automating high-frequency bitcoin based on proximal policy 
optimization was proposed by Liu, Fengrui, et al (PPO). The framework is designed 
to compare various machine learning (ML) models to anticipate a static bitcoin price, 
including support vector machines (SVM), multi-layer perceptron (MLP), long short-
term memories (LSTM), and temporal convolutional networks (TCN). Later applying 
this understanding to estimate real-time bitcoin price shows that LSTM outperforms 
other models. McNally et al. [13] applied Bayesian RNN model and LSTM model 
where LSTM reported better results in terms of satisfactory accuracy and lower 
error. For a fair comparison, they did an evaluation with ARIMA model and it is 
noticed ARIMA performs poorly on that dataset. Ramadhan, et al. [14] demonstrate 
deep neural network decomposition for feature extraction and their model shows 
around 89% accurate results. Hence we can notice there is a high demand of precise 
estimation in this area. 
3 
Dataset Description 
This dataset was obtained from CoinDesk by exporting the information in the csv 
format from their export site. This dataset includes bitcoin changes from October 1, 
2013, through May 18, 2021 [4], with minute-by-minute updates for OHLC (Open, 
High, Low, Close), indicated Currency, and Date. It has 6 columns and 2788 rows. 
Since our data has a date Component it is parsed into the index column by using the 
panda’s library. 
4 
Methodology 
In this section, we describe brieﬂy multiple deep deep learning model which will be 
used in our work. Fig 1 presents the overall workﬂow of our proposed approach.

282
S. Sen et al.
Collection of Dataset 
Data Pre-processing 
Data Visualization 
Choose A Model 
Dense Model
Conv1D Model
LSTM Model
GRU Model 
Evaluating The Model 
Improve Performance
Visualize The Results 
Fig. 1 Standard Deep Learning Workﬂow 
4.1 
Dense Neural-Network Model 
As we know data propagates through neurons in artiﬁcial neural networks, which are 
the basic form of neural networks. In general, deep models consist of a greater number 
of hidden layers. In any kind of neural network, a dense layer indicates connectivity 
of all neurons with all other neurons in the previous layer and subsequently to the 
following layer as well. Inputs are fed to the input layer, then through a series of 
hidden layers and activation functions, it propagates to the output layer to provide 
the desired output. 
4.2 
Convolutional 1D Model 
This model is mostly used in literature. The kernel advances along one dimension 
in the Conv1D Model. This is because just only one dimension of kernel sliding is 
necessary for time series data. A kernel is made up of mathematical processes that 
result in the activation map, a matrix dot product. The sample input is processed 
numerous times for this procedure using various kernels. Different features from the 
input will be extracted by each ﬁlter and its corresponding kernel. The most common 
kernel sizes are 3×3 and 5×5, these kernel sizes are initialized using initialization 
techniques like Xavier or MSRA. Each input is multiplied by the kernel matrix 
to generate an activation map, typically numerous ﬁlters are employed to produce

Forecasting and Analysing Time Series Data Using Deep Learning
283
several activation maps by layer. The way these maps are generated depends upon the 
stride and padding settings [15]. The input and output of 1D CNN are 2-Dimensional 
and are mostly used in time series data [16, 17]. For our model, we are using causal 
padding, where the output is not dependent on future inputs [9]. 
4.3 
LSTM Model 
Long-Term Memory (LTM) and Short-Term Memory (STM) are combined in LSTMs 
to provide straightforward and efﬁcient outcomes using the notion of gates. A type of 
RNN (Recurrent Neural Network) that is frequently used for sequential data predic-
tion and processing is the LSTM (Long Short-Term Memory) network. Like any 
other neural network, the LSTM has several layers that aid in pattern recognition and 
learning for enhanced performance. The objective of the LSTM model is to keep and 
store the necessary information while discarding unwanted information. The Forget 
Gate ignores and discards the information which is not useful for the network to learn 
the data and predictions. Cell state updating is being done by the input gate that in 
turn decides the importance of information where forget state output and input gate 
output are being multiplied together. The output gate also determines the network’s 
subsequent hidden state, which is where information is sent through an activation 
function. Thus, it assists the hidden state to propagate useful data and decides which 
information to carry [18] be carried out. For our experiment, we use bidirectional 
LSTM consisting of the forward pass and the backward pass. 
4.4 
GRU Model 
LSTMs with forget gates are similar to gated recurrent units (GRUs), a gating tech-
nique in RNNs [19] that has fewer parameters than LSTM because it lacks an output 
gate. Because it has fewer gates than LSTM, GRU is less complicated. A GRU cell, 
which is similar to an LSTM cell, is present here. It accepts an input X and a hidden 
state Ht-1 from the timestamp before it at each timestamp t, and it then outputs a new 
hidden state Ht, which again passes to the following timestamp. A GRU typically has 
two gates as compared to an LSTM cell’s three gates. The Reset gate is the ﬁrst gate, 
and the Update gate is the second [23]. The network’s short-term memory, which 
is a hidden state, is controlled by the reset gate (Ht). The read gate, known as the 
Update gate, is in charge of enabling the input ﬂow from the hidden layers. Two 
vector entries (0, 1) that are capable of performing a convex combination might be 
thought of as these gates.

284
S. Sen et al.
5 
Performance Evaluator Metrics 
Since we are dealing with a regression problem with time series forecasting, we are 
going to discuss multiple loss function metrics [21] for evaluating our model. They 
are: 
1) MAE: The term means mean absolute error. The difference between the target 
value and the actual value. We consider the absolute value here to avoid any 
negative results. This loss metric is very robust to outliers as well. 
norm al  upper M normal upper A normal upper E equals StartFraction 1 Over normal upper N EndFraction sigma summation Underscript normal i equals 1 Overscript normal upper N Endscripts StartAbsoluteValue normal y Subscript normal i Baseline minus ModifyingAbove normal y With caret Subscript normal i Baseline EndAbsoluteValue
n
orm
al u pper  M no
rmal upper A normal upper E equals StartFraction 1 Over normal upper N EndFraction sigma summation Underscript normal i equals 1 Overscript normal upper N Endscripts StartAbsoluteValue normal y Subscript normal i Baseline minus ModifyingAbove normal y With caret Subscript normal i Baseline EndAbsoluteValue
where N is the number of samples in the dataset, y_i is the actual value in the 
i-th sample and y^_i is the predicted value of the i-th sample 
2) MSE: This is called a mean squared error. A squared value is always positive. 
They are highly useful in addressing data outliers that may have an impact on 
the entropy values and alter the standard deviation of our time series data. 
norm al upper M normal upper S normal upper E equals StartFraction 1 Over normal n EndFraction sigma summation Underscript normal i equals 1 Overscript normal n Endscripts left parenthesis normal y Subscript normal i Baseline minus normal y overTilde Subscript normal i Baseline right parenthesis squared
no
r
m
al u
pper  M norm
al upper S normal upper E equals StartFraction 1 Over normal n EndFraction sigma summation Underscript normal i equals 1 Overscript normal n Endscripts left parenthesis normal y Subscript normal i Baseline minus normal y overTilde Subscript normal i Baseline right parenthesis squared
where n is the number of samples in the dataset, y_i is the actual value in the i-th 
sample and y^_i is the predicted value of the i-th sample. 
3) RMSE: The root mean squared error is shown in essence, it is the square root of 
the MSE value, which is calculated by summing the squares of the differences 
between the actual and the target values. Then in RMSE, the square root is 
applied. 
upper R upper M upper S upper E equals StartRoot StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus ModifyingAbove y With caret Subscript i Baseline right parenthesis squared EndRoot
[
|
|
|upper R upper M upper S upper E equals StartRoot StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus ModifyingAbove y With caret Subscript i Baseline right parenthesis squared EndRoot
up
p
e
r R 
uppe r  M upp
er S upper E equals StartRoot StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus ModifyingAbove y With caret Subscript i Baseline right parenthesis squared EndRoot
where n is the number of samples in the dataset, y_i is the actual value in the i-th 
sample and y^_i is the predicted value of the i-th sample. 
4) MAPE: It implies mean absolute percentage error. It is calculated based on the 
average value of the absolute error percentage of forecasts. It acts as a very useful 
metric for forecasting models. 
norma l upper M normal upper A normal upper P normal upper E equals StartFraction 1 Over normal n EndFraction sigma summation Underscript normal i equals 1 Overscript normal n Endscripts StartFraction StartAbsoluteValue ModifyingAbove normal y With caret Subscript normal i Baseline minus normal y Subscript normal i Baseline EndAbsoluteValue Over normal y Subscript normal i Baseline EndFraction
no
r
m
al u
pper M normal upper A normal upper P normal upper E equals StartFraction 1 Over normal n EndFraction sigma summation Underscript normal i equals 1 Overscript normal n Endscripts StartFraction StartAbsoluteValue ModifyingAbove normal y With caret Subscript normal i Baseline minus normal y Subscript normal i Baseline EndAbsoluteValue Over normal y Subscript normal i Baseline EndFraction
nor
mal upper M normal upper A normal upper P normal upper E equals StartFraction 1 Over normal n EndFraction sigma summation Underscript normal i equals 1 Overscript normal n Endscripts StartFraction StartAbsoluteValue ModifyingAbove normal y With caret Subscript normal i Baseline minus normal y Subscript normal i Baseline EndAbsoluteValue Over normal y Subscript normal i Baseline EndFraction
Where n is the number of samples in the dataset, y_i is the actual value in the 
i-th sample and y^_i is the predicted value of the ith sample.

Forecasting and Analysing Time Series Data Using Deep Learning
285
6 
Experimental Setup and Result 
The entire experiment was conducted via Google Collaboratory, which offers a plat-
form for running ML applications in the cloud. Here we deal with bitcoin prices 
ranging from mid-2013 up to late 2021 [4]. Our intention with DL models is to 
predict the prices of bitcoin a month or a year from now. The summary of all model 
results is provided in Fig 2. 
6.1 
Model 1: Dense Model 
Here, we have created a Dense model and the rest of the models using the Keras 
library using TensorFlow as the backend. For this model, we are going to use the 
TensorFlow’s Sequential API to make the model. We are using 2 dense layers with 
“rectiﬁed linear unit” as the activation and the output layer is run without activation 
with only the horizon value. Upon compilation we pass the loss function as the 
“mean absolute error” and the “Adam” optimizer evaluated on the “mae” metric. 
This is going to be the default values for our compilation. The model is ﬁt with a 100 
epochs and train set and validation set. In Fig. 3’s loss curves, we can see that the 
validation loss is steadily decreasing up to a certain point, and our model does not 
appear to be overﬁtting to the point where it may be considered worthless. The spikes 
in the curves are due to the variation in our data when we introduce 7 timesteps in 
our window to predict a single timestep of our horizon.
6.2 
Model 2: Conv 1d Model 
Here, we are using the window value as 7 and horizon value as 1 which will be the 
default value for the rest of the models. We make use of the lambda layer to increase 
the dimensionality and thus transform the input layer [21] to ﬁt our model. We pass 
the formed layer through the convolutional layer with 128 neurons and a kernel size 
of 5. We are going to be using causal padding in this model, which ﬁlls the start of 
the data with zeros which increases processing speed. This is highly recommended
MODELS
MAE
MSE
RMSE
MAPE 
DENSE MODEL
0.009138 0.000293 0.017119
2.609467 
CONV1D MODEL 0.012291 0.000462 0.021484
3.621811 
LSTM MODEL
0.011196 0.000428 0.020688
3.102905 
GRU MODEL
0.009050 0.000297 0.017228
2.606400 
Fig. 2 Loss Function Comparison for DL Models 

286
S. Sen et al.
(a) 
(b) 
Fig. 3 (a) Dense Model Predictions (b) Dense Model Loss Curves
when using a Conv1D Layer. For the activation of our input convolutional layer, we 
use the rectiﬁed linear unit (ReLU) [15], and there is no activation function used for 
the output layer. The Loss function used is Mean Absolute Error (“mae”). In the loss 
curve shown in Fig 4, we can observe the leniency of the validation loss with the 
train data loss.
6.3 
Model 3: LSTM (LONG SHORT-TERM MEMORY) 
MODEL 
RNN models with LSTMs are particularly popular, especially for issues with time 
series forecasting. These unidirectional models alone are not very fruitful thus they 
are run through a bidirectional layer thus allowing a forward (past to future) backward 
(future to past) movement of the information. A bidirectional LSTM layer with 128 
neurons and a rectiﬁed linear unit (ReLU) activation function is how we are training 
the model [16]. The Loss function used is Mean Absolute Error (“mae”). A dense 
layer with a size equal to the horizon value is the output layer. Loss curve is shown 
in Fig 5.

Forecasting and Analysing Time Series Data Using Deep Learning
287
(a) 
(b) 
Fig. 4 (a) Conv1D Model Predictions (b) Conv1D Model Loss Curves
6.4 
Model 4: GRU (Gated Recurrent Unit) Model 
The favored models for time series forecasting for smaller datasets are GRU models. 
The LSTM has three gates, while the GRU has two gates. As a result, GRU is less 
complex and better suited for our dataset than LSTM because it has fewer gates than 
the latter. A lambda layer is applied to the model, which changes the inputs into 
the desired dimensions. A GRU layer with 128 neurons with rectiﬁed linear units 
(ReLU) is how we are training the model [17]. The Loss function used is Mean 
Absolute Error (“MAE”). Loss curve is shown in Fig 6.
7 
Comprehensive Performance Evaluation 
Figure 7 provides a comparative evaluation of all models we experimented. We can 
see that the models have a similar performance When comparing their MAE and 
MSE. It is found that the RNN models have a slightly better edge compared to the

288
S. Sen et al.
(a) 
(b) 
Fig. 5 (a) LSTM Model Predictions (b) LSTM Model Loss Curves
Conv1D model and the Dense Model. This is due to their ability to remember their 
past inputs.
From Fig 8, we can see that the execution time for the LSTM takes the longest 
because of the complex architecture and the sequential ﬂow of inputs. This can be 
overcome by introducing Parallelism in the inputs by using a transformer model 
which uses multi-head self-attention to process inputs in a parallel fashion. But 
the GRU model has nearly half the execution time of the LSTM model because 
of its slightly less complex architecture. Overall, through our experimentation with 
different models, we found that the RNN models work best with time series problems 
and can perform way better with some hyperparameter tuning. We could employ some 
call-back functions to ﬁnd an ideal learning rate or use a learning rate scheduler to 
manually check for the best learning rate and use it in our model.

Forecasting and Analysing Time Series Data Using Deep Learning
289
(a) 
(b) 
Fig. 6 (a) GRU Model Predictions (b) GRU Model Loss Curves
Fig. 7 MAE And MSE Bar Chart for DL Models
MODELS
EXECUTION TIME IN SECONDS 
DENSE MODEL
19.387 
CONV1D MODEL
20.409 
LSTM MODEL
123.588 
GRU MODEL
53.985 
Fig. 8 Execution Time of DL Models

290
S. Sen et al.
8 
Conclusion 
Time series data processing and forecasting have always been remaining challenging 
difﬁcult tasks. As the investment in bitcoin is risky and price ﬂuctuation is a regular 
phenomenon, precise prediction is needed of the hour. Our paper demonstrates the 
efﬁciency of multiple DL models and shows the approaches how to tune hyper-
parameters for achieving the optimal result. It is observed that GRU based model 
can provide better accuracy than all other models. As a future extension, we will 
try to experiment Auto ML-based method to search for the best model to make it 
customized and ﬂexible. 
References 
1. Raju SM, Tarif AM (2020) Real-time prediction of BITCOIN price using machine learning 
techniques and public sentiment analysis, Malaysia. https://www.coindesk.com/price/bitcoin/. 
Accessed 18 July 2022 
2. “Bitcoins,” 14 July 2022. https://en.wikipedia.org/wiki/Bitcoin. Accessed 18 July 2022 
3. “Blockchain,” 15 July 2022. https://en.wikipedia.org/wiki/Blockchain. Accessed 18 July 2022 
4. “CoinDesk,”18 May 2021. https://www.coindesk.com/price/bitcoin/. Accessed 18 July 2022 
5. “What is Bitcoin? - In Layman’s Terms,” 20th November 2020. https://howchoo.com/bitcoin/ 
what-is-bitcoin-in-laymans-terms 
6. Bourke D (2021) GitHub 2021. https://github.com/mrdbourke/tensorﬂow-deep-learning/blob/ 
main/10_time_series_forecasting_in_tensorﬂow.ipynb. Accessed 2022 
7. Dhakal P (2017) A Naïve approach for comparing a forecast model. Int J Thesis Projects 
Dissertations (IJTPD) 5(1):1–3 
8. Hyndman RJ, Athanasopoulos G (2021) Time series patterns. In: Forecasting: principles and 
practice, 3rd edn. OTexts, Melbourne, Australlia, p Section 2.3 
9. Chauhan J, Raghuveer A (2022) Multi-variate time series forecasting on variable subsets. In: 
Problem formulation, vol 1. arXiv:2206.12626v1. [cs.LG]  
10. Rala J, Raimundo A, Postolache O, Sebastião P (2021) Neural architecture search for 1D 
CNNs—different approaches tests and measurements. Sensors 21(23):7990 
11. Nakamoto S (2008) Bitcoin: a peer-to-peer electronic cash system. Bitcoin 4. https://bitcoin. 
org/bitcoin.pdf 
12. Guo Q, Lei S, Ye Q, Fang Z (2021) MRC-LSTM: a hybrid approach of multi-scale residual 
CNN and LSTM to predict bitcoin price. LSTM, vol 1. arXiv:2105.00707v1. [q-ﬁn.TR] 
13. Saxena S. (2021) Introduction to Gated Recurrent Unit (GRU). https://www.analyticsvidhya. 
com/blog/2021/03/intr. Accessed 17 Mar 2021 
14. Srinivasamurthy RS (2018) Understanding 1D convolutional neural networks using multiclass 
time-varying signals. TigerPrints, no 2911 
15. Rajashekar VT (2022) “GitHub,” 12 July 2022. https://github.com/rajashekarvt/TimeSeriesFo 
recastingForBitcoin/blob/main/bitco%20in_data.csv. Accessed 18 July 2022 
16. Verma 
S 
(2019) 
Understanding 
1D 
and 
3D 
convolution 
neural 
network 
|Keras,” 
20 September2019. https://towardsdatascience.com/understanding-1d-and-3d-convolution-
neural-network-keras-9d8f76e29610. Accessed 2022 
17. Verma Y (2021) A Complete Guide to LSTM Architecture and its Use in Text Classiﬁcation. 
AnalyticsIndiaMag 
18. Verma Y (2021) A Complete understanding of dense layers in neural networks. AnalyticsIn-
diaMag, 19 September 2021

Forecasting and Analysing Time Series Data Using Deep Learning
291
19. Zainuddin Z, EA PA, Hasan MH (2021) Predicting machine failure using recurrent neural 
network gated recurrent unit (RNN-GRU) through time series data, vol 10, no 2, pp 870–878 
20. Mayank K, Sen S, Chakraborty P (2022). Implementation of cascade learning using apache 
spark. In: 2022 IEEE International conference on electronics, computing and communication 
technologies (CONECCT), pp 1–6. IEEE 
21. Khasnis NS, Sen S, Khasnis SS (2021). A machine learning approach for sentiment analysis 
to nurture mental health amidst COVID-19. In: Proceedings of the international conference on 
data science, machine learning and artiﬁcial intelligence, pp 284–289 
22. Sen S, Singh KP, Chakraborty P (2023) Dealing with imbalanced regression problem for large 
dataset using scalable artiﬁcial neural network. New Astron 99:101959 
23. Sen S, Chakraborty P (2022). A novel classiﬁcation-based approach for quicker prediction 
of redshift using apache spark. In: 2022 International conference on data science, agents & 
artiﬁcial intelligence (ICDSAAI), vol 1, pp 1–6. IEEE 
24. Sen S, Amrita I (2022). A transfer learning based approach for lung inﬂammation detection. 
In: Mandal JK, De D (eds) Advanced Techniques for IoT Applications. EAIT 2021. Lecture 
Notes in Networks and Systems, vol 292. Springer, Singapore. https://doi.org/10.1007/978-
981-16-4435-1_4

Intelligent Blockchain: Use of Blockchain 
and Machine Learning Algorithm 
for Smart Contract and Smart Bidding 
Jyotiranjan Rout, Susmita Pani, Sibashis Mishra, Bhagyashree Panda, 
Satya Swaroop Kar, and Sanjay Paramanik 
Abstract With the growth of population, there is an increase in demand in food, as 
well as growth in agribusiness and hence, an increase in contract farming. Contract 
farming provides the farmers with the beneﬁts such as enhanced agricultural produc-
tion and a rise in incomes of contract farmers, giving them an exposure to services 
and resources inculcated with opportunities to participate in markets. However, time 
and again contract farming is criticised for the uneven bargaining power that may 
lead to the exploitation of farmers by contractors or large agribusiness ﬁrms. The 
presence of middlemen or brokers in the supply chain further coerces the farmers to 
rely on them to sell their products. Again, there is another issue of differentiating 
among bids. So, the solution for all these problems lies in “BidBlock”, a bidding 
integrated e-commerce platform, having built over blockchain and machine learning. 
Blockchain, with smart contracts, helps in restoring the trust of the farmers with its 
decentralization and machine learning, by interpreting and simplifying bids, helps 
them gaining maximum proﬁt. Results show that most buyers prefer to have deals 
directly with the farmers and with a ease of doing business (like easy contract and 
payment). The addition of delivery services will be better too, but in this paper, it 
provides emphasis on how to simplify bidding with smart contracts, both for farmers 
and farms. Hence, introduction to BidBlock. 
Keywords Agriculture · Crop Quality · Blockchain · Artiﬁcial Intelligence ·
Machine Learning · Smart-Contract · Data Visualisation · Ease of Doing 
Business · Security · Assurance · Economy
J. Rout · S. Pani · S. Mishra envelope symbol · B. Panda · S. S. Kar · S. Paramanik 
Department of Computer Science and Engineering, Balasore College of Engineering and 
Technology, Sergarh, Balasore, Odisha, India 
e-mail: sibashis.mishra0001@gmail.com 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_26 
293

294
J. Rout et al.
1 
Introduction 
Agriculture is a part of the primary sector in economy and the prominent liveli-
hood factor in the development of India’s GDP. With the population of nearly 1.4 
billion, the demand for food is higher than other countries. The agriculture sector is 
ranked ﬁfth in production, consumption, export, and expected growth in the FY2022-
23. For more than 70% of the rural population and employs over 60% of the total 
population of India, agriculture is the primary livelihood. Hence, in current time 
we are focusing on agricultural development. To integrate the farmers with large 
agri-purchasers including exporters, agri-industries, for better price realization while 
mitigating the risks of price and market for the farmers and ensuring a smooth ﬂow 
of raw material to industries, the Union Finance Minister in the budget for 2017–18 
announced preparation of a “Model Contract Farming Act” [14]. Also, as per Farmers 
(Empowerment and Protection) Agreement on Price Assurance and Farm Services 
Act, 2020, It provides a legal framework for farmers to enter into written contracts 
with companies and produce for them. 
Based on this, the growth of contract farming is noticeable. Contract farming is 
bringing in farmers with great opportunity to get into the market and enhance their 
economy. It gives them an exposure to the usages of advanced technology, services 
and resources. Its also helping contractors and agribusiness ﬁrms for ease of doing 
business. 
Still, the farmers, being a bit novice about technology and computation, face 
exploitation by the contractors and large agribusiness ﬁrms. The middlemen and 
brokers are being the reason for the shaken trust of the farmers. Getting no other 
options, they have to rely on the brokers to sell their products.Though nowadays 
we can see technology interfering in the process, still the farmers have trust issues 
regarding the forgery and counterfeits. Another issue is related to different prices in 
the bids in different quantities. Mostly farmers are not that much educated to calculate 
the difference between different bids and the contractors and ﬁrms taking advantage 
of it, try to gain more proﬁt. In recent days, we can see various types of ads which 
don’t defy the existing laws and still deceiving. So, the farmers can’t differentiate 
the proﬁt and loss of the bids and become pray of the tricks. 
Hence, we propose BidBlock, the application built over the latest tech stacks like 
blockchain and machine learning. Bidblock can help farmers winning their trust with 
the decentralisation of blockchain and giving the beautiful analysis of the bids and 
bringing proﬁtable income to them.In this paper, we have focused on smart bidding 
and smart contract between the farmers and large ﬁrms and contractors. Even though 
there are platforms present for bidding of the farmers, they are not as reliable as this 
application is going to be, as the idea of blockchain is not introduced, neither the ML 
algorithms to simplify bids. 
In the coming sections, we will study about the methods and procedures of how 
BidBlock works and the algorithms we are going to use.

Intelligent Blockchain: Use of Blockchain and Machine Learning …
295
2 
Related Works 
Vitalik Buterin[9], outline the transition to hybrid solutions, and summarise the key 
characteristics of decentralised crypto-ledger systems. the fundamental importance 
of the blockchain while exposing the possible dangers and shortcomings of open-
source distributed ledgers, which are responsible for the shift to hybrid solutions. 
Blockchain technology can appear complicated, but by dissecting each part, it can 
be made simpler. Blockchain technology, at its most basic level, combines well-
known computer science procedures and cryptographic primitives (cryptographic 
hash functions, digital signatures, asymmetric-key cryptography)[10], along with 
principles from record keeping (such append only ledgers). Blockchain is a decen-
tralised system that eliminates the need for a middle organisation. Every node in the 
blockchain has access to and shares the details of every transaction that has ever been 
performed. Compared to centralised transactions involving a third party, the system 
is more transparent because to this feature. Additionally, since every node on the 
Blockchain is anonymous [11], it is safer for other nodes to conﬁrm transactions. 
The ﬁrst application to use Blockchain technology was Bitcoin. Ittay Eyal [12] 
examines how and why cryptocurrency blockchains have become the rage in the 
ﬁnancial technology industry in “Blockchain Technology: Transforming Libertarian 
Cryptocurrency Dreams to Finance and Banking Realities.” Blockchain transactions 
are organised into blocks, and as long as the block size restriction is followed, any 
number of transactions may be included in a block. These transactions are grouped 
together and sent throughout the network by nodes on a blockchain. Peers eventually 
synchronise to a perfect replica of the blockchain across the network. A consensus, or 
agreement among the network peers, is required for the blockchain updating process 
[13].
. The process of reaching consensus among network participants regarding the 
accurate state of the data on the system is referred to as consensus in the network.
. All nodes share the exact same data when there is consensus.
. A consensus mechanism therefore I assures that the data on the ledger is the same 
for all network nodes and (ii) prohibits bad actors from altering the data on the 
ledger. 
3 
Briefs About the Technology Used 
Blockchain is an immutable digital ledger, basically a register of accounts and trans-
actions, that are written and stored by the users. As immutable, it promises a reliable 
source of truth about the data stored in the nodes or the blocks in the chain. 
In case of farmers or agriculture, blockchain can be used to track the origin of the 
products and thus creating a reliable supply chain and built trust between producers 
and consumers. In addition, smart contracts can be used to create contracts between 
the producer and the large ﬁrms or the contractors, and can be a source of trust as

296
J. Rout et al.
once the contract saved in blockchain, it can’t be changed. As it is built having the 
idea of decentralisation, there is no need of any organisation to keep track of the data 
ﬂow. The transparency, consensus mechanism, security, all cumulatively can create 
a healthy environment for the farmers as well as for the contractors. 
Machine learning, being a ﬁeld of prediction, can help in predicting the best deal 
for the farmers taking notice of their type of crop, how much they invested, the market 
value of the production, the risk of the bid and other various attributes. Though the 
farmers are not knowledgeable about the technologies, but the graphs generated can 
simplify their life by helping decide which bid to accept. 
ML can also help in clarifying the contractor doing the Crop Condition Assess-
ment, for which no ﬁled ofﬁcer have to visit the farmer’s farm. Simply the farmer 
can take some picture of the crop or vegetable and the pre-trained ml algorithms can 
predict the quality of the crop [2, 3, 4]. 
4 
Methods Adopted InBidBlock 
BidBlock. It is the solution of the forementioned problems while keeping records of 
everydetail and giving the farmers their (nearly) perfect income [5]. The purposed one 
is a bidding integrated e-commerce application(mostly android based application, 
as large number of farmers now possess android phones). For transportation online 
courier services can be introduced to close the physical gap between the producers and 
consumers andfor authority, we are assuming the application is under the surveillance 
of the Union ministry. 
The main attributes of the proposed application are described as follows:
. Farmers: The app will need the data of the farmers including their Adhar infor-
mation, the information of farms, farming practices, cultivation process, the 
transactions with providers and processors, etc.
. Contractors: The data of providers including their identity, information of agri-
food raw materials (e.g. seeds, pesticides, and fertilizers), the transactions with 
farmers, requirements, etc. will be recorded.
. Distributors: App will keep data involving transportation details, storage 
conditions (e.g. temperature and humidity), the transactions with farmers and 
contractors, etc. 
The steps involved are: 
Step1: Both farmers and contractors register on the BidBlock app with respective 
Adhar information. 
Step2: The contractors give information/advertisements related to their bids. 
Step3: The farmer gives information about their crop, production quantity and 
capability, area of farm etc. 
Step4: The app (ML algorithms) brings out the graph of Proﬁt vs Bid (or name 
of contractor/org. of bidding). 
Step5: Farmer choose the best bid.

Intelligent Blockchain: Use of Blockchain and Machine Learning …
297
Fig. 1 Steps adopted in BidBlock app for farmers and contractors 
Step6: Crop Condition Assessment using Machine Learning for the clarity of 
Contractor. 
Step7: Smart contract between the contractor and farmer and then exchange of 
goods and money. 
The above steps, when shown in a user interface, seems very simpliﬁed to both 
of the users, and also it provides the ease of doing business. 
After doing the registration using their Adhaar informations, both parties can login 
into the platform. While the farmers give information regarding their farm, crop and 
quantity of their crop, the contractors give data about their bids and ﬁrm. These data 
is stored in the database (Fig. 1). 
Then the app shows graphs generated through the machine learning algorithms, 
which takes data of previous bids, and the data of the current bids and gives a 
prediction of proﬁt for the farmers. 
When a farmer feels comfortable with a bid, he can proceed for quality check of 
his/her crop. The machine learning based algorithms take pictures of leaves, crops 
or vegetables as data, which the farmer can provide through his mobile phone. 
Next, the data related to the quality of the crops of different farmers reach at the 
end of the contractor with their acceptance of bids. As per the quality, the contractor 
can choose the farmer can accept the bid. 
With the successful transfer of money to the farmer and digital signature of both 
parties, the smart contract is saved in a public node in Ethereum based blockchain. 
Now the deal is closed. 
The exchange of goods can be done using various services available. 
5 
The Algorithm Involved in Blockchain 
The algorithms used in blockchain depends upon the usage of the app. For this 
app we have used the algorithm based upon Ethereum, a type of Blockchain [1].A 
smart contract is deployed after conversion to byte code and written in programming

298
J. Rout et al.
languages like Solidity based on C + +  . But for demonstration purpose we are using 
pseudo code.[6] 
Voidcontract(): 
‘X’ is the number of authorized Farmers 
‘Y’ is the number of authorized Contractors 
Ethereuminscription(EI) of Farmers 
Ethereuminscription(EI) of Contractors 
Quantity, ProductId, ProductPrice, isPaid. 
1. Generated ContractCondition for Farmers (Boolean PaymentRequestMade) 
2.
Generated 
ContractCondition 
for 
Contractors 
(Boolean 
ProductRequestMade) 
3. Proceed if x ∈ X&& y ∈ Y, (i.e., authorized Farmer and Contractor) 
4. If authorized &isPaid, then 
5.
PaymentRequestMade is modified to true 
6.
ProductRequstMade is modified to ConsentToSell 
7.
ContractCondition is modified to success. 
8.
Generating alert message of successful contract for both parties. 
9. 
Scheduling for product pick up by contractor or other services. 
10.
Encrypt the data related to contract (i.e.product_id, contract_id, 
transaction_id etc.) 
11.
create a node in the blockchain 
12.
Save the node 
13. End 
14. Else 
15. ContractConditionis modified to failure& display error message 
16. End 
The above code looks like a simple code for e-commerce transaction, but at the 
end it is stored in a node in the Ethereum blockchain, which is a strong proof of the 
contract. 
The blocks in Blokchain basically are consisted of the data, the pointer to next 
block and a timestamp (the time at which the node was created). The data stored on 
the blocks are encrypted with different encryption algorithms, which entrusts both 
parties of their conﬁdentiality (Fig. 2).

Intelligent Blockchain: Use of Blockchain and Machine Learning …
299
Fig. 2 Demo of blocks added in Blockchain having data of farmers and contractors 
6 
Algorithms used in Machine Learning (Crop Quality 
Determination and Bid Visualisation) 
6.1 
Crop Quality Determination 
As we have discussed the details about the blockchain, lets have a glance at how 
machine learning is going to help in determining the crop quality in brief. For making 
the life of the farmers and the contractors easy, the pre-discovered idea of crop quality 
detection algorithms can be used, just like the below one, which was described in a 
paper by R. Poonguzhali, A.Vijayabhanu[2]. In this paper, we want to incorporate 
that idea to our app to boost its usability and to provide the contractors their trust 
winning platform.Here, in this process, mostly 5 steps are involved (related to above 
diagram). 
Step1: Image Acquisition (block 1 & 2) 
Step2: Image Pre-processing (block 3) 
Step3: Image Segmentation (block 4, 5) 
Step4: Feature Extraction (block 6) 
Step5: Classiﬁcation (block 7) 
Thus, just with their phone and taking some pictures of their crop, the farmers can 
let the contractors know the quality of their product, making the process simpler. 
First, the farmer has to take a image of the crop using his/her phone, and to upload 
the image in BidBlock. Next, the app of its own crops the image to the speciﬁc size to 
zoom on to the crops, or it can be done manually. The modiﬁed images are considered 
to be in RGB color format, so the app converts it to gray scale format. 
Then, using K means method, an unsupervised learning in AI, the unnecessary 
spots and noise in the images are removed and hence the quality of the images 
enhanced. In next step, the color and shape to be extracted (Fig. 3). 
In the above algorithm, Tensor ﬂow and artiﬁcial neural network is used for 
identiﬁcation of the health of the crop or vegetables. From the enhanced quality 
images, the app/ML algorithm, identiﬁes if the given image is up to the quality or 
not. Fuzzy classiﬁcation rules are used for image processing in neural network, which 
based upon learning from data and applied to classify pixels in grey-level images.

300
J. Rout et al.
The above algorithm, gives out the graph and data about the health of farmer’s 
crop. Now for example, if we are testing the quality of tomato, 
if (acidity < 1.7 && sweetness < 7.5) 
crop is healthy& quality satisfying 
else 
quality not satisfying. 
The above algorithm is an example of determining the quality of the product, and 
then we can show a quality check certiﬁcate to the contractor as well as the farmer 
to make both parties sure about the product. 
6.2 
Bid Visualisation 
Also, machine learning can be used to help ﬁnding bids for farmers.[7] The below 
ML generated demo graph can help the farmer predicting which bid can give him/ 
her the most proﬁt. 
This data, the algorithm takes as input are:
. MSP (minimum selling price) of the type of product farmer is selling or the 
average market price
. The total investment of the farmer in the product
. The amount in the bids 
Taking the above data in consideration, the algorithm decides the proﬁt and loss, 
as shown below: 
Function DecideBid(): 
Int totalInvest = x, MSP = y, bidAmounts = [c1, c2, c3, c4, c5] 
For each bid in bidAmounts: 
If (( MSP>totalInvest&& bid > MSP) || ( totalInvest> MSP && bid > 
totalInvest )): 
basePrice = MSP>totalInvest ?MSP :totalInvest 
Profit 
else Loss

Intelligent Blockchain: Use of Blockchain and Machine Learning …
301
Fig. 3 Steps adopted in BidBlock app for crop quality detection (Source: [2], p. 2) 
Fig. 4 Graph generated using Machine Learning 
From the above graph, the farmer can easily determine which contractor to choose 
(in this case C2). Such graphs will help them in understanding the bids and to make 
a good decision (Fig. 4). 
7 
Conclusion and Future Scope 
In current times, when data corruption, forgery, and numerous deceitful activities 
are in trend, we need something trust worthy, which Blockchain is built speciﬁcally 
for. In this paper we have tried to give a prototype of a fully functional, possible 
model of a bidding app which can simplify and enhance the quality of living of both 
farmers as well as contractors or large agri-industries [8]. Though still there are some 
shortcomings of blockchain, its still useable and can be used to build trust among 
the primary sector, as we have described. Though the methodologies focus on all the 
functionality of the app, we have not discussed about the transportation of products, 
as that can be done through different emerging courier services as well as by the 
contractors themselves. Also, the functionality of the app can be enhanced by using

302
J. Rout et al.
emerging technologies like IOT, Cloud computing etc. But so far, the app holds the 
capacity to be a smooth communicator between the producers and the consumers. 
References 
1. Prashar D, Jha N, Jha S, Lee Y, Joshi GP (2020) Joshi: Blockchain-based traceability and 
visibility for agricultural products: a decentralized way of ensuring food safety in India. 
Sustainability 12(8):3497 
2. Poonguzhali R, Vijayabhanu A (2019) Crop condition assessment using machine learning. 
IJRTE 7(6S2). ISSN: 2277-3878 
3. Deshmukh R, Deshmukh M (2019) Detection of paddy leaf diseases. In: International 
conference on advances in science and technology 
4. Sanjana Y, Sivasamy A, Sri J (2015) Plant disease detection using image processing techniques. 
In: International journal of innovative research in science engineering and technology 
5. Chiong A et al (2021) FarmBid - a bidding e-commerce platform to provide fresh produces 
from farmers to customers 
6. https://auth0.com/blog/101-smart-contracts-and-decentralized-apps-in-ethereum/. 
Accessed 
28 Feb 2023 
7. Ahila SS, Dinesh G, Kavya S, Anandkumar KM (2020) Demand based crop prediction using 
machine learning algorithm. Eur J Mol Clin Med 07(08). ISSN: 2515–8260 
8. Bakare S, Shinde SC, Hubballi R, Hebbale G, Joshi V (2021) FarmBid - a blockchain-based 
framework for agriculture subsidy disbursement. In: IOP Conference Series: Materials Science 
and Engineering, vol 1110 
9. Pilkington, M (2016) FarmBid - blockchain technology: principles and applications. Research 
handbook on digital transformations. Edward Elgar Publishing 
10. Yaga, D (2019) Blockchain technology overview. arXiv:1906.11078 
11. Yli-Huumo J (2016) Where is current research on blockchain technology?—a systematic 
review. PloS one 11(10):e0163477 
12. Treleaven P, Brown RG, Yang D (2017) Blockchain technology in ﬁnance. Computer 50(9):14– 
17 
13. Belotti M, Boži´c N, Pujolle G, Secci S (2019) A Vademecum: Blockchain technologies: when, 
which and how communications surveys and tutorials. IEEE Commun Soc 21(4):3796–3838 
14. https://pib.gov.in/PressReleseDetail.aspx?PRID=1533077. Accessed 28 Feb 2023

Weed Detection in Cotton Production 
Systems Using Novel YOLOv7-X Object 
Detector 
G. V. S. Narayana, Sanjay K. Kuanar, and Punyaban Patel 
Abstract A weed is a wild, undesired plant that grows naturally along with the 
desired crop. For their growth, they compete with the main crop for various resources 
like space, sunlight, irrigation, nutrients, etc. This leads to an enormous loss in the 
yield of the main crop and hence needs to be selectively controlled. Human inter-
vention in the process of identiﬁcation of weeds and subsequently its removal is 
extremely tedious and time-consuming too. Achieving the desired level of accuracy 
and preciseness in the manual weed identiﬁcation process is impracticable. In recent 
years, researchers have proposed computer vision-based methods for automatic weed 
identiﬁcation in precision agriculture. In this paper, we have used YOLOv7-X for 
automatically detecting weeds in cotton production systems. YOLOv7-X is a rela-
tively new addition to the You Only Look Once (YOLO) family of fast and accurate 
algorithms. The benchmark dataset used for the purpose of validating our results is 
CottonWeedid-15. This dataset is customized with annotations suitable for YOLOv7-
X by using the roboﬂow tool. The experimental study demonstrates that the YOLOv7-
X model’s mean average precision (mAP@.5) can attain 96.6 %. The average preci-
sion and average recall of the model were 0.914 and 0.953 respectively. This model 
can also be used to classify several weeds in various crops. 
Keywords YOLO · Weed detection · Artiﬁcial intelligence · Deep learning 
G. V. S. Narayana (B) · S. K. Kuanar 
GIET University, Gunupur, Odisha, India 
e-mail: gvs.narayana@giet.edu 
S. K. Kuanar 
e-mail: sanjay.kuanar@giet.edu 
P. Patel 
CMR Technical Campus,Kandlakoya(V), Medchal (M), Hyderabad, Telangana State, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_27 
303

304
G. V. S. Narayana et al.
1 
Introduction 
A weed is an unwanted plant growing wildly along with the desired crop. The impact 
of weeds on agricultural output is signiﬁcant. Weeds are the root cause of problematic 
insects and agricultural illnesses. They also carry various viruses and insects that 
destroy the desired crop plants. Weeds and crops compete for various resources like 
space, sunshine, irrigation, and nutrients. Just like insects and pests, weeds also harm 
crop cultivation and can have a greater impact on crop output reduction. 
India has the second-largest agricultural land area in the world, and agriculture 
provides a living for almost 60% of rural Indian people [ 1]. In the majority of the 
country, cotton is a Kharif crop. One of India’s most signiﬁcant ﬁber and commercial 
crops is cotton. The nation’s cotton production has increased signiﬁcantly quanti-
tatively throughout the years. India, one of the top cotton producers in the world, 
produces about 21% of the cotton used worldwide. The current production per kg 
per hectare, 445 kg/ha, is still less than the global average yield of 765 kg/ha [ 22]. 
Although the yield per hectare is still lower than the global average of 765 kg per 
hectare, signiﬁcant adjustments being made to the country’s cotton farming practices 
have the potential to bring the current productivity level closer to the global average 
in the near future [ 22]. 
The effectiveness of weed control is very crucial to enhancing cotton production 
in the country. The ﬁrst six to eight weeks after planting are crucial for controlling 
weeds because during this time weeds for nutrients and water aggressively compete 
with the crop. The percentage of loss in crop production varies based on the type of 
weeds. However, zero cultivation loss from weeds is quite uncommon. Weed detec-
tion by using an appropriate mechanism is the ﬁrst fundamental need for effective 
control. Farmers tend to use herbicides compared to manual weeding to cut down the 
expenditure. The use of continuous herbicides can cause an impact on the environ-
ment. Automation of weed detection has the potential to improve system accuracy and 
calculated density. To identify weeds using automation, there were primarily three 
methods used: Biological morphology, Spectral characteristics, and Visual texture. 
Several techniques existed for weed identiﬁcation and precise application of herbi-
cides such as weed detection from photos using machine vision, Global Positioning 
System (GPS)-guided patch application, Herbicide applicator with a variable rate 
(VRA), applying herbicide with drones (Unmanned Aerial Vehicle-UAVs), Online 
spraying system with machine vision, Robots and weeding machines using sensor-
based machine vision, weeding in a microwave, and weeders using a laser thermal 
beam. 
Few studies are available on the detection of multi-class weeds that signiﬁcantly 
affect cotton ﬁelds. A comprehensive benchmark of YOLO detectors on multi-class 
weed detection would be extremely helpful to the research community given the 
current state of YOLO detector advances. In order to discover multi-class weeds in 
the cotton production industry, this research employs a performance evaluation of 
the innovative YOLOv7, which is our motivation for this research.

Weed Detection in Cotton Production Systems ...
305
The next sections of the paper are structured as follows: Sect. 2 presents current 
techniques for weed detection, Sect. 3 describes the methodology, used dataset, and 
metrics for performance assessment, and Sect. 4 discusses the result of the YOLOv7-
X model. In the end, we presented the conclusion in Sect. 5. 
2 
Literature Review 
Currently, computer vision technology is applied to various smart agriculture activi-
ties, including the detection of plant diseases, crop productivity prediction, taxonomic 
identiﬁcation, weed detection, and irrigation and soil conservation [ 2– 4]. For future 
standardized and consistent weed management that makes use of a variety of control 
tactics, computer vision technologies are crucial [ 5]. The effectiveness of computer 
vision algorithms depends on large-scale tagged image data-set and it boosts the per-
formance of algorithms [ 6]. Good datasets for weed recognition should adequately 
capture essential weed species, environmental factors (such as temperature, illumi-
nation conditions, and soil composition), and the development of morphological or 
physiological differences. Preparing such databases is time-consuming, expensive, 
and requires domain expertise in weed detection. Recent research has been done to 
create image databases for weed control [ 7], e.g., DeepWeeds [ 8], Early crop weed 
data-set [ 9], CottonWeedID15 [ 10], the Eden Library [ 11], and Weed-AI. To the best 
of our knowledge, only CottonWeedID15 [ 10] was developed for the identiﬁcation 
of weeds particular to cotton-producing systems. This dataset only has image-level 
annotations, making it unsuitable for weed recognition tasks that need bounding box 
annotations for weed instances in the images. 
Applications for YOLO object detectors in weed detection are numerous. YOLOv3-
tiny (a condensed version) was used by Gao et al. [ 12] to identify Calystegia sepium 
in Beta vulgaris ﬁelds and achieved mean average precision (mAP) of 0.829. In 
Sharpe et al. [ 13] on Eleusine indica detection, YOLOv3-tiny attained the F1-ratings 
of 0.85 in the ﬁeld of Fragaria and 0.65 in the ﬁeld of tomato. A total mAP of 54.3% 
was calculated by Ahmad et al. [ 14] adopting YOLOv3 to identify four weed cate-
gories that are prevalent in the United States. Li et al. [ 15] used a publicly available 
dataset (Sudars et al. [ 16]) by YOLOX to examine eight object detectors for weed 
identiﬁcation and achieved the highest mAP of 79.6%. (Ge et al. [ 17]). With the 
exception of Ahmad et al., all studies only looked at a single weed class, but they 
demonstrate the effectiveness of YOLO detectors. 
Chen et al. [ 18], this paper’s research focuses on Deep Transfer Learning(DTL) 
techniques. In this study, they built a comprehensive benchmark for the weed identi-
ﬁcation task under consideration and examined 27 cutting-edge deep learning tech-
niques using transfer learning. DTL required only a minimal amount of training time 
across models to obtain excellent classiﬁcation precision of F1 scores above 95%. 
ResNet101 had the highest F1 score (99.1%), while 14 out of the 27 models had F1 
values that were higher than 98.0%. 
Thuan D [ 19], in this publication, research is provided that emphasizes the sustain-
able development of crop cultivation can be improved by using intelligent weeding

306
G. V. S. Narayana et al.
systems to carry out plant-speciﬁc procedures. These systems must be able to accu-
rately identify and classify weeds in order to prevent unintentional spraying that could 
hamper the nearest species. Among the standard challenges in computer vision is 
object detection. The object in the photograph is classiﬁed as well as localized. The 
approaches employed in decades past to solve this issue were split into two phases: 
1. Detects various portions of the image using sliding windows of multiple lengths 
and 2. Apply the classiﬁcation issue to ascertain the items’ class by using it. For 
the ﬁrst time ever, researcher Joseph Redmon and colleagues developed the YOLO 
method in 2015. An object identiﬁcation system completes all necessary steps to 
recognize an object using a single computational model (the term You Only Look 
Once). The YOLO algorithms were created on the PyTorch framework, and a month 
following the release of YOLOv4, researcher Glenn Jocher, and his Ultralytics Lim-
ited Liability Company (LLC) research division published YOLOv5 with a couple 
of modiﬁcations and enhancements. 
Olanivi et al. [ 20], the research presented in this paper focuses on the vision-based 
community that developed the generative adversarial network (GAN) in 2014, and it 
offers a variety of methods and effects that can produce highly accurate samples and 
generate accurate data representations. However, it might be challenging to ﬁnd large-
scale, unbiased, and tarmac image data–sets to support the creation of sophisticated, 
high-performance classiﬁers. 
Myloans et al. [ 11], the work in this paper is focusing on precision agriculture 
using deep learning models to boost computer vision and Artiﬁcial intelligence (AI)-
based applications. These methods, however, require large amounts of data, which 
are typically in limited supply in agriculture, in order to achieve outstanding perfor-
mances. The Eden Library, a platform for addressing the current gap in open-access 
agriculture databases that include regional and aerial images, is presented in this 
article. The offered datasets are believed to be sustained and enhanced over time 
rather than simply remaining as a static study product addressing only particular 
species, life-cycle phases, and environments, making this procedure pertinent. The 
Eden Library has information on 30 ailments, 15 distinct crops, and 9 weeds. 
3 
Proposed Methodology 
Here, we elaborate on the methodology for multi-class weed identiﬁcation by using 
the YOLOv7-X model, used dataset (CottonWeedID15), and metrics for performance 
assessment. 
3.1 
Methodology 
Figure 1 depicts our proposed methodology for identifying weeds. The input image 
annotations do not follow the format of YOLO detectors. Consequently, the input

Weed Detection in Cotton Production Systems ...
307
Fig. 1 The pipeline for YOLO feature extraction algorithms to detect weeds 
needs to be translated into the format of object detectors by using roboﬂow. After 
converting to a new format, the dataset was randomly split into the train, validation, 
and test subsets using the partition ratios of 70%-20%-10%, respectively. To meet 
the requirements of YOLO deep learning architectures, the actual pictures were 
downsized to 640times × 640 pixels for training inputs. 
3.2 
CottonWeedID15 
In this experiment, we used the CottonWeedID15 dataset, which comprises 5187 
color photos of 15 weed classes taken in cotton crops, mostly in North Carolina, in 
natural lighting conditions. The current sample for weed detection is more than ten 
times larger than the one for four weed species in terms of the number of images 
(Ahmad, 2021). Figure 2 represents the number of images in each class of the Cot-
tonWeedID15 dataset. 
3.3 
Metrics for Performance Assessment 
In computer vision, Intersection over Union (IoU) and mean average precision (mAP) 
are popular evaluation metrics used for object detection.

308
G. V. S. Narayana et al.
Fig. 2 pie chart showing the diversity of the cotton weed dataset 
3.3.1
Intersection over Union 
We calculate the overlap between the predicted bounding box and the actual bounding 
box for every bounding box. This is determined via intersection over union (IoU) 
[ 21] and represented in Eq. 1 
3.3.2
Mean Average Precision (mAP) 
The mAP is a well-known assessment measure in computer vision that is employed 
for object recognition tasks including localization and classiﬁcation. By applying 
bounding box coordinates, localization pinpoints an instance’s position, and classiﬁ-
cation identiﬁes the particular weed class that it belongs to. The ratio of true positives 
to all positive predictions is known as Precision. Equation 2 represents the Precision 
Formula. The recall is calculated as the ratio between the number of Positive samples 
correctly classiﬁed as Positive to the total number of Positive samples. Equation 3 
represents the Recall formula.

Weed Detection in Cotton Production Systems ...
309
upper I o upper U equals StartFraction upper A r e a o f upper O v e r l a p Over upper A r e a o f upper U n i o n EndFractionIoU = Areaof Overlap
Areaof Union
(1) 
upper P equals StartFraction upper T upper P Over upper T upper P plus upper F upper P EndFractionP =
T P
T P + F P
(2) 
upper R equals StartFraction upper T upper P Over upper T upper P plus upper F upper N EndFractionR =
T P
T P + F N
(3) 
where the following abbreviations stand for the number of true positive, false positive, 
and false negative predictions at the speciﬁed IoU cutoff, respectively: TP, FP, and 
FN. Next, the below ﬁnite sum is used to compute the Average Precision score, which 
represents the area covered by the curve. 
By averaging the AP results throughout all object classes, the mean Average Pre-
cision is calculated. Equation 4 represents AP and Eq. 5 represents mAP respectively. 
upper A upper P equals sigma summation Underscript n Endscripts left parenthesis upper R Subscript n Baseline minus upper R Subscript n minus 1 Baseline right parenthesis upper P Subscript nAP =
Σ
n
(Rn −Rn−1)Pn
(4) 
m upper A upper P equals StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts upper A upper P Subscript im AP = 1
N
N
Σ
i=1
APi
(5) 
4 
Results and Discussion 
Here, we elaborate on the results of the YOLOv7-X model, Analysis of the Confusion 
Matrix and F1 Curve, Precision and Recall, and PR curve. 
4.1 
Results of YOLOv7-X Model 
We used the google colab to train the YOLOv7-X model for 200 epochs. We are 
able to run 40 to 50 epochs in 8 h. To run 200 epochs, it took 5 days by resuming the 
code. After completion of all epochs, we got the results for various parameters like 
Precision, Recall, Classiﬁcation status, etc. The complete result is shown in Fig. 3. 
Table 1 shows the number of labels, Precision, Recall, mAP@.5, and mAP@.5:.95 
for each class. We got the highest precision for Ragweed, and SqurredAnoda classes 
and the lowest precision for the Prickly Sida class. We got the highest recall for 
Crabgrass, and Swinecress classes and the lowest recall for the Nutsedge class. 
Figure 4 represents qualitative weed detection by using bounding boxes.

310
G. V. S. Narayana et al.
Table 1 Model’s Accuracy for the Weed Classes. 
Class
Images
Labels
P
R
mAP@.5
mAP@.5:.95 
all
784
849
0.914
0.953
0.967
0.852 
Crabgrass
784
18
0.932
1
0.996
0.98 
Eclipta
784
67
0.899
0.94
0.928
0.787 
Goosegrass
784
39
0.861
0.949
0.968
0.886 
Morningglory
784
230
0.9
0.965
0.97
0.893 
Nutsedge
784
69
0.934
0.826
0.859
0.72 
PalmerAmaranth
784
73
0.927
0.945
0.971
0.816 
Prickly Sida
784
30
0.831
0.933
0.958
0.821 
Ragweed
784
16
1
0.981
0.996
0.838 
Sicklepod
784
46
0.959
0.978
0.991
0.916 
Spottedspurge
784
44
0.945
0.955
0.949
0.799 
SqurredAnoda
784
12
1
0.911
0.996
0.913 
Swinecress
784
13
0.84
1
0.996
0.943 
Waterhemp
784
86
0.865
0.97
0.975
0.853 
CarpetWeed
784
783
0.92
0.96
0.966
0.883 
Purslane
784
106
0.906
0.981
0.983
0.762 
Fig. 3 Results of YOLOv7-X 
4.2 
Analysis of the Confusion Matrix and F1 Score 
The Confusion Matrix displays both the actual labels and the expected labels. We have 
generated a 15*15 confusion matrix representing each class of weed in our dataset. 
As per the confusion matrix, the highest classiﬁcation rate (100%) for Crabgrass, 
Sicklepod, SqurredAnoda, and Swinecress classes and the lowest (84%) for the 
Nutsedge class. The confusion matrix’s diagonal elements indicate how closely the

Weed Detection in Cotton Production Systems ...
311
Fig. 4 Weed detection through bounding boxes 
predicted labels correspond with the actual labels. F1 Score combines the results of 
precision and recall into a single metric by considering their harmonic mean. The F1 
score varies from 0 to 1, with 0 representing low accuracy and 1 representing high 
accuracy prediction. The confusion matrix and F1 Score are illustrated in Fig. 4. 
Fig. 5 Confusion Matrix and F1 Curve

312
G. V. S. Narayana et al.
4.3 
Precision, Recall and PR Curves 
In the P curve, conﬁdence is shown along the x-axis, and precision is shown along 
the y-axis. We got the model’s precision as 1.00 at a conﬁdence of 0.951. In the R 
curve, conﬁdence is shown on the x-axis, and recall is shown on the y-axis. We got 
the model’s recall as 0.98 at a conﬁdence of 0.00. By adjusting the IoU threshold, a 
P-R slope or pattern is produced. The PR curve displays the recall on the x-axis and 
precision on the y-axis. The area under the PR curve is one square unit. According to 
the PR curve, the classes Crabgrass, Ragweed, and Square-Anoda have the highest 
Area Under Curves (AUC), whereas Eclipta has the lowest AUC. Figure 5 represents 
the P Curve and R Curve and Fig. 6 represents the PR Curve (Fig. 7). 
Fig. 6 Precision and Recall Curve 
Fig. 7 PR Curve

Weed Detection in Cotton Production Systems ...
313
5 
Conclusion 
For site-speciﬁc weed management, accurate localization of weeds by computer 
vision systems depends on weed detection and categorization. The creation of super-
vised deep learning on the weed image database and the curation of comprehensive, 
properly speciﬁc data on weeds are crucial parts of efﬁcient weed identiﬁcation. The 
dataset for weed detection that is currently most exhaustive and useful to the agricul-
ture development systems is provided in this research. It consists of 5187 images of 
15 weed classes. The mAP@0.5 ratings for the YOLO detection algorithms varied 
from 88.14% for YOLOv3-tiny to 95.22% for YOLOv4.We utilized the YOLOv7-X 
model to identify the multi-weed class in this paper. CottonWeedID15 is the bench-
mark dataset used for object detection. This YOLOv7-X model gives an accuracy of 
0.966 at map@0.50. In the future, our current work can be further extended to incor-
porate multi-weed detection with intra-similarity between weeds and inter-similarity 
between weed & crop. 
References 
1. May P, Ehrlich H-C, Steinke T (2006) ZIB structure prediction pipeline: composing a com-
plex biological workﬂow through web services. In: Nagel WE, Walter WV, Lehner W (eds) 
Euro-Par 2006, vol 4128. LNCS. Springer, Heidelberg, pp 1148–1158. https://doi.org/10.1007/ 
11823285_121 
2. Tian H, Wang T, Liu Y, Qiao X, Li Y (2020) Computer vision technology in agricultural 
automation-A review. Inf Proc Agric 7(1):1–19 
3. Mavridou E, Vrochidou E, Papakostas GA, Pachidis T, Kaburlasos VG (2019) Machine vision 
systems in precision agriculture for crop farming. J. Imaging 5(12):89 
4. Zhang S, Huang W, Wang Z (2021) Combing modiﬁed Grabcut, K-means clustering and sparse 
representation classiﬁcation for weed recognition in wheat ﬁeld. Neurocomputing 452:665–674 
5. Young SL (2018) Beyond precision weed control: a model for true integration. Weed Technol 
32(1):7–10 
6. Sun C, Shrivastava A, Singh S, Gupta A (2017) Revisiting unreasonable effectiveness of data 
in deep learning era. In: Proceedings of the IEEE international conference on computer vision, 
pp 843-852 
7. Lu Y, Young S (2020) A survey of public datasets for computer vision tasks in precision 
agriculture. Comput Electron Agric 178:105760 
8. Olsen A et al (2019) DeepWeeds: a multiclass weed species image dataset for deep learning. 
Sci Rep 9(1):1–12 
9. Espejo-Garcia B, Mylonas N, Athanasakos L, Fountas S, Vasilakoglou I (2020) Towards weeds 
identiﬁcation assistance through transfer learning. Comput Electron Agric 171:105306 
10. Chen D, Yuzhen L, Li Z, Young S (2022) Performance evaluation of deep transfer learning 
on multi-class identiﬁcation of common weed species in cotton production systems. Comput 
Electron Agric 198:107091 
11. Mylonas N, Malounas I, Mouseti S, Vali E, Espejo-Garcia B, Fountas S (2022) Eden library: 
a long-term database for storing agricultural multi-sensor datasets from UAV and proximal 
platforms. Smart Agric Technol 2:100028

314
G. V. S. Narayana et al.
12. Gao J, French AP, Pound MP, He Y, Pridmore TP, Pieters JG (2020) Deep convolutional neural 
networks for image-based Convolvulus Sepium detection in sugar beet ﬁelds. Plant Methods 
16(1):1–12 
13. Sharpe SM, Schumann AW, Boyd NS (2020) Goosegrass detection in strawberry and tomato 
using a convolutional neural network. Sci Rep 10(1):1–8 
14. Ahmad A, Saraswat D, Aggarwal V, Etienne A, Hancock B (2021) Performance of deep learning 
models for classifying and detecting common weeds in corn and soybean production systems. 
Comput Electron Agric 184:106081 
15. Li Y, Guo Z, Shuang F, Zhang M, Li X (2022) Key technologies of machine vision for weeding 
robots: a review and benchmark. Comput Electron Agric 196:106880 
16. Sudars K, Jasko J, Namatevs I, Ozola L, Badaukis N (2020) Dataset of annotated food crops 
and weed images for robotic computer vision control. Data Brief 31:105833 
17. Ge Z, Liu S, Wang F, Li Z, Sun J (2021) YOLOX: exceeding yolo series in 2021. arXiv preprint: 
arXiv:2107.08430 
18. Chen Q, Wang Y, Yang T, Zhang X, Cheng J, Sun J (2021) You only look one-level feature. 
In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 
13039-13048 
19. Thuan, D (2021) Evolution of YOLO algorithm and YOLOv5: the state-of-the-art object deten-
tion algorithm 
20. Olaniyi E, Chen D, Lu Y, Huang Y (2022) Generative adversarial networks for image augmen-
tation in agriculture: a systematic review. arXiv preprint: arXiv:2204.04707 
21. Padilla R, Netto SL, Da Silva EA (2020) A survey on performance metrics for object-detection 
algorithms. In: 2020 international conference on systems, signals and image processing (IWS-
SIP), pp 237-242. IEEE 
22. MacCarthy DS et al (2021) Potential impacts of agricultural intensiﬁcation and climate change 
on the livelihoods of farmers in Nioro, Senegal, West Africa

Smart Healthcare System Management 
Using IoT and Machine Learning 
Techniques 
P. Sudam Sekhar, Gunamani Jena, Shubhashish Jena, and Subhashree Jena 
Abstract Traditional health care system need to be smarter because of the increasing 
complicacy in health of human kind. The new technology like IoT and Artiﬁcial 
intelligence system has a big role in making it success. IoT, Cloud Computing, and 
AI have made the traditional healthcare system smarter. Improving medical care 
with IoT and AI. Combining IoT and AI provides healthcare new options. In this 
approach, AI and IoT can assist intelligent healthcare systems detect illness. This 
article uses AI and IoT convergence to discover heart disease and diabetes. The 
given model includes data collection, preprocessing, classiﬁcation, and parameter 
adjustment. Wearables and sensors make it easier to collect IoT data, which AI can 
use to diagnose illness. The suggested technique for detecting illnesses uses CSO-
CLSTM, based on Crowd Search Optimization. CSO ﬁne-tunes the CLSTM model’s 
“weights” and “bias” to enhance medical data categorization. Outliers are removed 
using the isolation Forest (iForest) approach. CSO improves CLSTM’s diagnostics. 
Healthcare data proved CSO-validity. LSTM’s CNN2D now includes a new version 
of LSTM and a CSO features selection approach. Experiments utilising Heart and 
Diabetes data reveal that extension is correct. 
Keywords Internet of Things · Convergence · Cloud computing · AI · Intelligent 
healthcare · Disease diagnostics
P. S. Sekhar envelope symbol
Department of Mathematis and Statistics, Vignan’s Foundations for Science Technology and 
Research, Guntur, India 
e-mail: sudamshekhar@gmail.com; drpss_sh@vignan.ac.in 
G. Jena 
Computer Science and Engineering, B V C Engineering College, Odalarevu, AP, India 
S. Jena 
OUTR, Bhubaneswar, India 
S. Jena 
CAS Marine Biology, Annamalai University, Cuddalore, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_28 
315

316
P. S. Sekhar et al.
1 
Introduction 
The healthcare business has employed IT to enhance diagnosis and treatment 
processes in recent years. Creating enormous volumes of digital data usually involves 
sophisticated techniques and scientiﬁc theories. Improved IT has led to improved 
healthcare apps. Intelligent health applications are easy to use, attractive, and versa-
tile. This will increase the health service’s understanding and highlight how intelli-
gent medicine will enhance treatment in the future. Modern medical care involves 
doctors, patients, clinical and research organizations. Preventing and monitoring sick-
ness, diagnosing and treating it, clinical management, health decisions, and medical 
research are all important. Mobile internet, Cloud Computing, big data, 5G systems, 
microelectronics, AI, and smart biotechnology are considered to have changed 
modern healthcare. All levels of contemporary medicine employ these strategies. 
Wearable or portable health monitoring equipment can be employed as needed Koch, 
Kwak and Rahmani [1–3]. Virtual treatment and remote home control aid them. Intel-
ligent clinical decision support systems can enhance diagnosis. Intelligent medical 
care goes beyond technology. All stages of health care are now information-based due 
to rapid expansion. Data must drive regional healthcare ecosystems, like homes and 
communities Goyal, Tuli, Mutlag [4–6]. Health care has always used data. Diabetes, 
heart disease, and arthritis require constant diagnosis, treatment, symptom moni-
toring, and lifestyle adjustments. Intelligent health care systems can automate most 
jobs with software, smart devices, health bots/telemedicine, and standard processes 
Faust, Tuli [7, 8]. This improves patient care and reduces costs. 
2 
Data Collection and Analysis 
An illness can’t be treated. Each patient’s medications and medical history must 
be considered throughout therapy. Not all medical cases are unusual. It’s happened 
before and will happen again. Smart Healthcare employs networked devices to collect 
real-time data. People think sophisticated healthcare applications are easy to use, have 
great visuals, and are multipurpose. Some changes are the development of informa-
tion technology, the extension of clinical management and the change from a system 
for treating diseases to a system for preventing diseases [9, 10]. The following adjust-
ments address fundamental necessities [11, 12]. This will increase the health service’s 
understanding and highlight how intelligent medicine will enhance treatment in the 
future.

Smart Healthcare System Management Using IoT and Machine Learning Techniques
317
2.1 
Problem Statement 
In a world when technology and innovation touch everything, letting healthcare lag 
behind seems foolish. IoT, AI, Big Data, and Cloud computing may generate complex 
global healthcare systems. This work combines AI and IoT to identify heart problems 
and diabetes. The given model includes data collection, preprocessing, classiﬁcation, 
and parameter adjustment [13, 14]. Wearables and sensors make it easier to collect 
IoT data, which AI can use to diagnose illness. 
2.2 
Work Scope 
The suggested research uses AI and IoT, gadgets with internet and cloud connec-
tivity might collect patient data, says the study’s author. Artiﬁcial intelligence would 
analyse this data to determine the patient’s health. In the suggested research, Arti-
ﬁcial Intelligence may be utilized to remove the redundancy data which are not 
needed. The data need to improve its qualities and increase prediction accuracy. A 
repeating algorithm will choose the most correct characteristic. The Crow search 
method was based on crow behaviour. Scientiﬁc contributions: SVM, KNN, J48, 
Nave Bayes. Dataset from the source divided into train and test sections utilising 
242 (80%) training records and 61 (20%) test records for Disease Diagnosis. In his 
work, the author leverages AI and IoT to improve healthcare delivery. IoT and wear-
able gadgets with cloud and internet connectivity might collect patient data, says 
the study’s author. Artiﬁcial intelligence would analyze this data to determine the 
patient’s health [15]. 
3 
Algorithms 
3.1 
SVM 
Depending on the dataset, machine learning forecast and sort data. Support Vector 
Machine classiﬁes and predicts linearly (SVM). It can handle linear and nonlinear 
issues. SVM classiﬁes data into classes using a line or hyperplane. RBF kernels 
are utilized in kernelized Machine Learning Algorithms. Support Vector Machines 
categorize objects.

318
P. S. Sekhar et al.
3.2 
Bayes 
Naive Bayes may function better than more complicated algorithms. Each class distri-
bution may be seen as a one-dimensional distribution. This lessens the dimension 
curse. Nave Bayes is a Bayes’ theorem-based probabilistic classiﬁer that requires 
feature independence. These are supervised learning algorithms. 
K-Nearest Neighbor is a basic Supervised Learning method. The K-NN algo-
rithm saves all available data and determines how to categorize fresh input based on 
similarity to preserved data. This implies K-NN can categorise fresh data rapidly. 
Population-based algorithm. Particle Swarm Optimization resembles Crowd Search 
(PSO). Crow Search Algorithm mimics crows’ intelligence. The CSA uses crow 
knowledge. 
3.3 
J48 
Using J48, we’ve grouped information. The J48 approach groups applications accu-
rately. The J48 approach analyses discrete and continuous data well. It uses more 
memory and makes categorizing medical data harder and less reliable. 
3.4 
CNN2D an Enhanced LSTM 
We introduced CSO features option to CNN2D and found that it delivered the best 
accurate results with Heart and Diabetes data [17]. 
3.5 
Framework/Architecture 
In Figs. 1 and 2, the architecture frame work of the system and design process 
have been mentioned clearly with all the steps involved in it. Further it describes the 
workﬂow and algorithms used in various steps of calculations. First of all the dataset 
is provided and removed redundancy data, normalized the data to keep as 0 and 1, 
and then split the dataset into two parts. Later stage the training algorithm discussed 
below are employed to train and test the data.
CSO-CLS: In this module, we utilized Isolation Forest to remove pick value from 
the dataset, Crowd Search to enhance features, and LSTM to train the model. The 
trained model will evaluate LSTM’s accuracy using test data. CNN2D improves on 
LSTM. We introduced a CSO features selection technique to CNN2D and tested it 
with heart and diabetes data to improve accuracy. This module will create a graph 
comparing each method’s performance [18, 19].

Smart Healthcare System Management Using IoT and Machine Learning Techniques
319
Fig. 1 Architecture/Framework 
Fig. 2 Process design

320
P. S. Sekhar et al.
4 
Methods and Outcomes 
4.1 
Information-Gathering 
Most researchers utilise https://github.com/rashida048/Datasets/blob/master/Heart. 
csv. 
It separated the 303-record dataset into train and test sections, utilising 242 (80%) 
for training and 61 for testing. 
4.2 
Measurements 
F1, Accuracy, and ROC-A Under the Curve measures model performance (ROC-
AUC). Precision and Recall determine F1-score and Accuracy. 
FPR means False Positive Rate. 
TPR; accuracy • remember 
F1-Score 
These factors impact value calculation: 
True positives (TP) are successfully identiﬁed occurrences. 
False negatives (FN) are mistakenly predicted unneeded events. 
False-positive (FP) means erroneously anticipated events. 
True negative (TN) is the amount of unnecessary occurrences. 
FPR measures machine learning’s effectiveness. This means: 
FPR=FP/(FP+TN) 
True-positive rate (TPR): Similar to recall. 
TPR=FP/(FP+TN) 
Accuracy is assessed as a ratio of accurately anticipated observations to total 
observations. 
Accuracy = (TN + TP)/(TP + FP + TN + FN) It’s the number of accurately 
anticipated positive initial observations. 
TP = remember 
Precision helps determine correct values. This implies subtracting expected 
positives from forecasted positives. Precision is TP/(TP + FP).
F1-score measures a model’s accuracy and memorability. It’s the model’s average 
performance and memory. An F-score. F1 = 2 (Precision Recall/Precision + 
Recall).

Smart Healthcare System Management Using IoT and Machine Learning Techniques
321
Fig. 3 Loaded data set 
characteristics for data 
classiﬁcation 
Fig. 4 Loaded data set 
characteristics in graphical 
form 
The ROC-AUC is a helpful classiﬁcation statistic.
Choose and upload the ‘heart dataset.csv’ ﬁle in the image above, then click 
‘Open’ to load the dataset (Fig. 3). 
In Fig. 4 the detailed characteristics of the data is explained. Here X-axis depicts 
the range from 0 (no illness) to 1 (heart disease or diabetes), while the Y-axis displays 
the frequency with which each category is represented. To read and normalise the 
dataset, choose “Read & Preprocess Dataset,” then select “Heart & Diabetes Dataset” 
to upload the dataset and obtain the aforementioned graph.
All 303 records in the dataset were read by the programme, and then split in two: 
242 (80%) were used for training, while 61 were used for testing. To train the current 
algorithm and view the results below, select “Run Existing Algorithm”. Figure 5 
describes the outcomes of the training algorithm. 
Figures 6, 7 and 8 explained the detailed results of the algorithm used. The algo-
rithm name is shown on the x-axis of the graph above, and each method’s performance 
is shown on the y-axis. Each coloured bar represents a particular statistic, such as 
precision or accuracy. The graph shows that CSO-CLSTM performs better than the 
present methodology.
This project now includes the improved LSTM Convolution 2D Neural Network 
(CNN2D). Additionally, we added the CSO feature selection method to CNN2D, 
and the research utilising data from Heart and Diabetes indicated that the extension 
algorithm provides the highest level of accuracy. The TEST ﬁle found in the “Dataset” 
folder is used by a disease prediction model that we’ve included. Use the testHeart.csv

322
P. S. Sekhar et al.
Fig. 5 CSO-CLSTM 
algorithm result
Fig. 6 Loaded data set 
characteristics for 
computation
and testDiabetes.csv ﬁles to train the Heart dataset. Here are the outcomes of each 
algorithm (Fig. 9).
On the diabetes dataset, CSO LSTM achieved 81% accuracy and CSO-CNN 
98% accuracy (see image). Click ‘Disease Diagnosis’ to input test data and get the 
outcome. In the preceding example, uploading the testDiabetes.csv ﬁle and clicking 
‘Open’ delivers the results shown below (Fig. 10).
In the image above, the TEST Data are in square brackets, and the anticipated 
output is labelled Normal or Abnormal. This results gives the idea of the data and 
predict the patient situation as normal and abnormal. This results beneﬁts the doctor 
in treat the patients with complete process to recover [20, 21].

Smart Healthcare System Management Using IoT and Machine Learning Techniques
323
Fig. 7 Accuracy of 
CSO-CLSTM analysis 
Fig. 8 Performance of CSO-CLSTM

324
P. S. Sekhar et al.
Fig. 9 Performance of CSO 
LSTM
Fig. 10 TEST results
5 
Conclusion 
This work created an AI and IoT-based model for identifying illness in smart health-
care systems. The model depicted collects, preprocesses, classiﬁes, and adjusts 
parameters. IoT technologies like wearables and sensors can help AI diagnose illness. 
The iForest approach removes patient outliers. The CSO-CLSTM model sorts data 
by illness occurrence. CSO improves CLSTM’s weights and bias settings. CSO 
improves CLSTM’s diagnostics. CSO-LSTMs were tested using healthcare data. 
The CSO-LSTM model’s greatest accuracy was 81%. This project now includes

Smart Healthcare System Management Using IoT and Machine Learning Techniques
325
CNN2D, an enhanced version of LSTM. We incorporated CSO features selection to 
CNN2D, and an experiment using Heart and Diabetes data indicated the extension 
method had 98% accuracy. 
Bibliography 
1. Mahmud R, Koch FL, Buyya R (2018) Cloud-fog interoperability in IoT-enabled healthcare 
solutions. In: Proceedings of the 19th international conference on distributed computing and 
networking (ICDCN 2018), Varanasi, India, 4–7 January, Article no 32, pp 1–10 
2. Islam SMR, Kwak D, Kabir MDH, Hossain M, Kwak KS (2015) The Internet of Things for 
health care: a comprehensive survey. IEEE Access 3:678–708 
3. Rahmani AM, Gia TN, Negash B, Anzanpour A, Azimi I, Jiang M, Liljeberg P (2018) 
Exploiting smart e-health gateways at the edge of healthcare Internet-of-Things: a fog 
computing approach. Future Gener. Comput. Syst. 78:641–658 
4. Goyal A, Kahlon P, Jain D, Soni RK, Gulati R, Chhabra ST, Aslam N, Mohan B, Anand I, 
Patel V, et al (2017) Trend in prevalence of coronary artery disease and risk factors over two 
decades in rural Punjab. Heart Asia 9:e010938 
5. Tuli S, Basumatary N, Buyya R (2019) EdgeLens: deep learning based object detection in 
integrated IoT, fog and cloud computing environments. In: Proceedings of the 4th IEEE inter-
national conference on information systems and computer networks (ISCON 2019), Mathura, 
India, 21–22 November 2019 
6. Mutlag AA, Ghani MKA, Arunkumar N, Mohammed MA, Mohd O (2019) Enabling 
technologies for fog computing in healthcare IoT systems. Future Gener Comput Syst 90:62–78 
7. Faust O, Hagiwara Y, Hong TJ, Lih OS, Acharya UR (2018) Deep learning for healthcare 
applications based on physiological signals: a review. Comput Methods Programs Biomed 
161:1–13 
8. Tuli S, Basumatary N, Gill SS, Kahani M, Arya RC, Wander GS, Buyya R (2020) HealthFog: an 
ensemble deep learning based smart healthcare system for automatic diagnosis of heart diseases 
in integrated IoT and fog computing environments. Future Gener Comput Syst 104:187–200 
9. Zhao X, Yang K, Chen Q, Peng D, Jiang H, Xu X, Shuang X (2019) Deep learning based mobile 
data ofﬂoading in mobile edge computing systems. Future Gener Comput Syst 99:346–355 
10. Lim T-S, Loh W-Y, Shih Y-S (2000) A comparison of prediction accuracy, complexity, and 
training time of thirty-three old and new classiﬁcation algorithms. Mach Learn 40:203–228 
11. Gill SS, Arya RC, Wander GS, Buyya R (2018) Fog-based smart healthcare as a big data 
and cloud service for heart patients using IoT. In: International conference on intelligent data 
communication technologies and Internet of Things, pp 1376–1383. Springer, Cham 
12. Mohammed MA, Maashi MS, Mohd O, Mohammed MA (2020) MAFC: a multi-agent fog 
computing model for the administration of healthcare important activities. Sensors (7) 
13. Muhammad G, Hossain MS, Kumar N (2021) EEG-based pathology detection for home health 
monitoring. IEEE J Sel Areas Commun 39(2):603–610. https://doi.org/10.1109/JSAC.2020. 
3020654 
14. Azimi I, et al (2017) HiCH: Hierarchical fog-assisted computing architecture for healthcare 
IoT. ACM Trans Embed Comput Syst (TECS) 16(5s):1–20 
15. Hossain MS, Muhammad G (2018) Emotion-aware connected healthcare large data for 5G. 
IEEE Internet Things J (4):2399–2406. The report’s authors are Zhu 
16. Hossain MS, Muhammad G (2020) Deep learning-based pathology diagnosis for smart-
connected healthcare. IEEE Netw (6), 120–125. The report’s authors are Zhu 
17. Uddin MZ (2019) A wearable sensor-based activity prediction system to facilitate edge 
computing in smart healthcare system. J Parallel Distrib Comput 123:46–53 
18. Kalarthi ZM (2016) A review paper on smart health care system using Internet of Things. Int 
J Res Eng Technol 5(03):8084

326
P. S. Sekhar et al.
19. Vippalapalli V, Ananthula S (2016) Internet of Things (IoT) based smart health care system. 
In: 2016 international conference on signal processing, communication, power and embedded 
system (SCOPES). IEEE 
20. Younes MB, El-Emam NN (2023) 5 information security and data management for IoT smart 
healthcare. Intell Internet Things Smart Healthc Syst 69 
21. Kaur A, Bhatia M, Ahanger TA (2023) Bibliometric analysis of smart healthcare. IEEE Syst J

Automatic Code Clone Detection 
Technique Using SDG 
Akash Bhattacharyya 
, Jagannath Singh 
, and Tushar Ranjan Sahoo 
Abstract The complexity of software is increasing day-by-day. For faster and con-
venient implementation the software developer used to write similar codes in several 
places of software and these code segments are known as code clone. During the 
maintenance, if we know the places of code clones then it will be very effective. In 
this paper, we have proposed an unique technique of code clone detection using Sys-
tem Dependence Graph (SDG). The SDG for a program is the pictorial representation 
of the control and data ﬂow in the program. The SDGs are generated automatically 
by the ASM API. We have created the SDGs for two programs and by analysing the 
SDGs we have detected the code clones. With case studies we have explained and 
demonstrated the working of our proposed technique. As an advantage of this the 
maintenance and reliability of the software will enhance. 
Keywords Code Clone Detection · Types of Code Clone · Code Clone Detection 
Process · Code Clone Detection Techniques 
1 
Introduction 
In the rising industry of information technology, the process of development of 
software are not done under idle conditions. It is a period bound development process 
where the prerequisites of the software is bound by the client and can change at any 
A. Bhattacharyya (B) · J. Singh 
School of Computer Engineering, KIIT Deemed to be University, Bhubaneswar, Odisha, India 
e-mail: 2081008@kiit.ac.in 
J. Singh 
e-mail: jagannath.singhfcs@kiit.ac.in 
T. R. Sahoo 
Department of Computer Science & Engineering, IIIT, Bhubaneswar, Odisha, India 
e-mail: tushar@iiit-bh.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_29 
327

328
A. Bhattacharyya et al.
given time. In order to meet the deadline given by the client, the developer needs 
to incorporate the given random changes and upgrades within the stipulated amount 
of time which in turn forces the developers to work overtime. Working under such 
pressurized conditions leads the developers to absentmindedly copy a certain block of 
code and paste the same in a different location with minimal to no modiﬁcation to the 
code. The process if used under controlled sections does not affect the software being 
developed. However the over utilization of such process leads to the deterioration of 
the quality of the software being developed. 
Code clone is deﬁned as code segments which are identical to one another either 
by copy paste from one location to another or by small modiﬁcations to the code. 
In the process of software development the original code is the most common. Thus 
once the code is copied of a bug is found in the original code then the bug remains 
in all copied locations. It in turn results in bug propagation which affects software 
maintenance. Code clone detection and correction are required in order to provide 
the client with a bug free code. Code clone detection process is an ongoing research 
area where the target is to ﬁnd the codes cloned and to inform the developers of such 
code clones. 
The paper has been divided up into sections to discuss about the different types 
of code clones and the code clone detection process. After the literature study we 
have proposed a new code clone detection algorithm and have also discussed about 
the working of the same in details. 
2 
Basic Concepts of Code Clones 
In this section, we have discussed some of the important topics which are crucial to 
understand our proposed technique. First we discuss about the types of code clones 
and their detection techniques present in different research papers. 
2.1 
Types of Code Clones 
Considering a broad approach to the classiﬁcation of code clones done in software 
development, there are four major types: 
Type I Clone: This form of clone is an exact copy of the original code segment 
without the inclusion of the spaces and the white spaces [ 1]. 
Type II Clone: This type of clone is found to be similar in terms of syntax or 
structure but are found to differ with respect to some of the unique identiﬁers in the 
program code. 
Type III Clone: This type of clone is an advanced form of type 2 clone where 
the fragments are sometimes modiﬁed. 
Type IV Clone: This type of code clone is completely different from the other 
three form of code clones discussed. The codes cloned in this type have a completely

Automatic Code Clone Detection Technique Using SDG
329
Table 1 Summarization of Types of Code Clone 
Type 1
Type 2
Type 3
Type 4 
Exact Clone
Renamed Clone
Near-miss Clone
Structural Clone 
Structural Clone
Parameterized Clone
Gapped Clone
Function Clone 
Function Clone
Near-miss Clone
Non-Contiguous Clone
Reordered Clone 
Function Clone
Reordered Clone
Intertwined Clone 
Structural Clone
Semantic Clone 
Function Clone 
different syntax form but the logic behind the working of the lines of codes remains 
the same [ 2]. 
Table 1 provides a summary of the four types of code clones and the different 
elements present in each one of them. 
2.2 
Code Clone Detection Process 
There are a number of different techniques which are present in order to identify code 
clones present in a program code. All these processes can be considered to be part 
of a simple set of common techniques. In Fig. 1, the ﬂow of a code clone detection 
process has been shown. This section gives an overview of the common techniques 
being used for the code clone detection processes. 
Pre-processing: During the process of program development, the programmers often 
make use of comments, white spaces and other remarks which does not contribute to 
the working functionality of the program code once it is executed. These additional 
work on the program code is done in order to provide the other programmers with 
the ability to easily understand what the program statements are required to do. 
Code Transformation: After the completion of the pre-processing step source code 
is then changed into a suitable format which can be used to apply the code clone 
detection algorithms to it effectively. Some of the various transformation activities 
Fig. 1 Overview of Code Clone Detection Process

330
A. Bhattacharyya et al.
which can be applied are source code tokenization, generating abstract syntax’s, 
parsing of source code, tree based or program dependency based graphs, and metrics 
calculation of the program. 
Clone Extraction: Once the code has been transformed into a suitable passable 
format, the code clone detection algorithms can then be applied to it. In this step the 
transformed unit is then compared to all the transformed unit of the program code in 
order to ﬁnd cloned matches. 
Formatting and Code Mapping: After the algorithm detects the code clone pair, 
it is then mapped with the original source code in order to ﬁnd the original code of 
the clone pair. Set of line numbers or directly the original source code is found and 
provided as the output of the algorithm. 
Post-processing and Aggregation: After the collection of the code clone pairs, since 
there is no automatic process of the clone pair veriﬁcation process available for such 
algorithms, the use of manual veriﬁcation is important in order to segregate the false 
positive cases found [ 3]. Once removed the output needs to be properly visualised 
for the view to understand the algorithm easily as well as know the code clone pairs 
easily. 
3 
Literature Survey 
In large software based systems, on order to identify clones of codes, there needs to 
be a detailed knowledge about the orientation and the internal working structure of 
a programming language as well as the ﬁle extensions in which the ﬁles are being 
saved in. Moreover in order to ﬁnd all the code clones present in the program it 
is important to compare each of the code components with all the available code 
components of the same program which when looked at from the point of view of 
computational power is costly as well as time consuming. It has been found that 
there are 5 techniques which are used to ﬁnd code clones from a set of programs. We 
have studied some of the research conducted on these techniques and presented our 
ﬁndings in the following section. 
3.1 
Text-Based Detection Techniques 
Ducasse et al. [ 4] had introduced the use of Duploc tool. The tool was a language 
independent dynamic pattern matching algorithm used to identify clones. However it 
was unable to identify meaningful clones from the code. Roy et al. [ 5] worked on the 
development of NICAD tool for the ﬁnding of code clone detection using text based 
algorithm on C programming language. The tool was able to provide reasonable 
results with high precision and high recall. Liu et al. [ 6] was able to propose the use

Automatic Code Clone Detection Technique Using SDG
331
of a vulnerable code clone system named VEDETECT. In this system the source 
code is used as an input and its MD5 hash value is created on each of the block of 
codes. 
3.2 
Token-Based Techniques 
Kamiya et al. [ 7] introduced the tool CCFinder. The tool is able to extract code 
clones from programming languages such as COBOL, C, Java and other large scaled 
codes with the help of sufﬁx tree matching algorithm. Kawaguchi et al. [ 8] presented 
their Shinobi tool which was implemented on a client server structure design with 
sufﬁx based array index method in order to ﬁnd token sequences. Murakami et al. [ 9] 
worked on the development of a tool with the help of sufﬁx based array algorithm 
which had high precision and high recall named FRISC tool. 
3.3 
Tree-Based Techniques 
Baxter et al. [ 10] had introduced CloneDr tool. The tool uses hashing algorithm 
and dynamic programming in order to detect Type 1 and Type 2 clones. The only 
disadvantage of this tool was that it was not able to detect near miss clones. Koschke 
et al. [ 11] made use of sufﬁx based tree method in order to compare the tokens 
created for a node created using abstract syntax tree. Nguyen et al. [ 12] presented the 
works of ClemanX tool which made use of incremental detection method in order 
to manage clones. The tool ﬁnds the clones with the help of latent clones along with 
the vector features associated with the clones. The identical code segments are then 
grouped as a clone pair. 
3.4 
Program Dependency Graph-Based Techniques 
Singh et al. [ 13] made use of program dependency graph in order to represent function 
controlling and to show data ﬂow dependency and then apply slicing algorithm to the 
program code to ﬁnd code clones sub graph by the help of PDG. Krinke et al. [ 14] 
developed a tool with the help of K-Limiting algorithm in order to ﬁnd identical sub 
graph named Duplix tool. Higo et al. [ 15] developed Scorpio which works on Java 
source code by using two way slicing to ﬁnd code clones. Liu et al. [ 16] developed 
GPLAG tool which uses PDG mining to ﬁnd code clones from source codes. Kamiya 
et al. [ 17] applied apriori algorithm to identify code clones for Python source codes. 
Tekchandani et al. [ 18] made use of grammar recovery and parse tree in order to 
identify semantic clones from source code. The technique was capable of ﬁnding 
Type 1, 2 and 3 code clones.

332
A. Bhattacharyya et al.
3.5 
Metrics-Based Techniques 
Mayrand et al. [ 19] were able to parse the source code to create an AST representation. 
They worked on collection of 21 different types of metrics for a source code program. 
Some of the compute functional metrics were grouped: layout, name, control ﬂow 
and expressions. Each and every point of comparison is used to compare for code 
clones and if two vectors are found to be similar then they are returned as code clones. 
The tool has been found to be useful in the process of removal and management of 
functional code clones which help in the proper maintenance of the software system. 
4 
System Dependence Graph (SDG) Based Code Clone 
Technique 
In this section, we explain our proposed code clone technique with working example. 
First we have to explain the meaning of SDG. 
System Dependence Graph [ 20]: In a program, there can be one or more number 
of procedures. PDG cannot be used to show the dependencies present among all the 
procedures, as it can only show the dependencies present within a single procedure. 
Hence, to represent the whole program as a dependency graph, system dependence 
graph (SDG) is used. SDG is a collection of PDGs. 
Algorithm 1. DFS (G,s) 
Require: G(V, E)where v ∈ V and  s is the  starting vertex  
Ensure: arr[v] 
1: for <all v in V> do 
2:
visited[v] = false 
3: end for 
4: S is an Empty Stack 
5: visited[s] = true 
6: Push s to S 
7: while not Empty(S) do 
8:
arr[i] = Pop(S) 
9:
if Adj[u] has at least one un-visited vertex then 
10:
Select w from Adj[u] 
11:
Push u to S 
12:
visited[w] = true 
13:
Push w to S 
14:
end if 
15:
i++ 
16: end while

Automatic Code Clone Detection Technique Using SDG
333
Algorithm 2. CodeCloneSearch(P1,P2) 
Require: SDGs of program1 and program2 i.e. P1 & P2 
Ensure: Clone pair list as (C1, C2) 
1: arrP1[] = DFS(P1,s) 
2: arrP2[] = DFS(P2,s) 
3: for <all elements in arrP1[] and arrP2[]> do 
4:
if arrP1[r1] = arrP1[r2] then //r1∈arrP1[], r2∈arrP2[] 
5:
c1=index of r1 
6:
c2=index of r2 
7:
break 
8:
end if 
9: end for 
10: C1 & C2 are lists 
11: for 
<each element in arrP1[]&arrP2[] starting from c1 and c2 
respectively> do 
12:
if arrP1[c1+1] = arrP2[c2+1] then 
13:
add node r1+1 to C1 
14:
add node r2+1 to C2 
15:
end if 
16: end for 
//Remove repetitive clones from the set of code clone pair found 
17: if two clone pairs (C1’,C2’) and (C1,C2) then 
18:
C1 belongs to C1’ & C2 belongs to C2’ 
19:
remove clone 
20: end if 
//combine clones 
21: Clone pairs (C1,C2) & (C2,C3) to be combined into (C1,C2,C3) 
22: return (C1, C2,C3) 
(a) While Loop Program
(b) For Loop Program 
Fig. 2 Program Codes for Working of Proposed Code Clone Detection Algorithm to show Detection

334
A. Bhattacharyya et al.
(a) SDG of While Loop Program
(b) SDG of For Loop Program 
Fig. 3 SDG of the Program Codes of Fig. 2 
(a) IF-ELSE Program
(b) Switch Case Program 
Fig. 4 Program Codes for Working of Proposed Code Clone Detection Algorithm to show Non-
Detection 
4.1 
Proposed Algorithm 
The proposed algorithm works with the help of two separate programs. The ﬁrst 
algorithm takes a graph as an input and gives out the DFS traversal of the graph with 
the help of an array. We mark all vertices of the graph as un-visited. In an empty S 
we start from the starting vertex s and mark the node as visited and push it into stack 
S. Iterating through the vertices of the graph we ﬁnd nodes where there is at least 
one adjacent vertex with un-visited status. We push that vertex into the stack and 
once a visited vertex is found we pop the same vertex from the stack into an array. 
The array output of the algorithm in sequence is the DFS traversal of the graph. The 
output from the algorithm is sent to the second part of the algorithm where the SDG 
algorithm is executed to give out the clone pairs of code. Iterating through the two 
DFS array of the two SDG we will get a similar single node point from two SDG 
considering only the syntactic structure of the statements. The iterating through the

Automatic Code Clone Detection Technique Using SDG
335
two array we ﬁnd the same nodes starting from r1 and r2. If they are same then we 
add it to C1 and C2. We return C1 and C2 pair and same program snippet from P1 
and P2. Remove the clones which are a part of a larger clone pairs returned. Then 
combine the clone pairs into a single large set of clones. 
4.2 
Working of Proposed Clone Detection Algorithm to Show 
Detection 
In Fig. 2a and Fig. 2b we have taken the process of binary searching technique which 
has been completed with the help of two types of loop. The two methods used are 
WHILE loop and FOR loop. The two programs displayed in the above sections has 
been compressed in order to reduce the number of lines where brackets open and 
close ({}). The ﬂowcharts in the Fig. 3 shows the System Dependency Graph (SDG) 
of the same programs. It can be seen that both the SDG of the two programs are 
similar to each other. This has been formed due to the restructuring of the programs 
for the completion of this study. 
The SDG creation is the ﬁrst step in the use of this algorithm. The SDG algorithm 
returns back a list of connecting edges to one another. The list is used in the next 
phase of the algorithm. Once the SDG has been created it is iterated through a DFS 
traversal algorithm which returns back an array of nodes traversed in DFS. The array 
is then used as a part of the code clone detection algorithm. The following Table 2 
shows all the input and output of the algorithm. 
Thus it can be seen that the algorithm is able to provide the user with the clones 
which has been detected in the two sets of codes. 
Table 2 Input and Output data for Program Set in Fig. 2 
Input
Output 
P1: (1, 2) (2, 3) (3, 4) (4, 5) (5, 6) (6, 7) (7, 
8) (7, 10) (6, 11) (11, 12) (12, 13) (13, 
14) (14, 22) (22, 6) (14, 15) (15, 16) (16, 
17) (17, 6) (16, 18) (18, 19) (19, 14) (18, 
21) (21, 14) 
Following is Depth First Traversal for P1: 0, 
1, 2, 3, 4, 5, 6, 11, 12, 13, 14, 15, 16, 18, 
21, 19, 17, 22, 7, 10, 8, 9, 20 
P2: (1, 2) (2, 3) (3, 4) (4, 5) (5, 6) (6, 7) (7, 
8) (7, 10) (6, 11) (11, 12) (12, 13) (13, 
14) (14, 22) (22, 6) (14, 15) (15, 16) (16, 
17) (17, 6) (16, 18) (18, 19) (19, 14) (18, 
21) (21, 14) 
Following is Depth First Traversal for P2: 0, 
1, 2, 3, 4, 5, 6, 11, 12, 13, 14, 15, 16, 18, 
21, 19, 17, 22, 7, 10, 8, 9, 20 
arr P1 = 0, 1, 2, 3, 4, 5, 6, 11, 12, 13, 14, 15, 
16, 18, 21, 19, 17, 22, 7, 10, 8, 9, 20 
Clone Pair = 0, 1, 2, 3, 4, 5, 6, 11, 12, 13, 14, 
15, 16, 18, 21, 19, 17, 22, 7, 10, 8, 9, 20 
arr P2 = 0, 1, 2, 3, 4, 5, 6, 11, 12, 13, 14, 15, 
16, 18, 21, 19, 17, 22, 7, 10, 8, 9, 20

336
A. Bhattacharyya et al.
Table 3 Input and Output data for Program Set in Fig. 4 
Input
Output 
P1: (1, 2), (2, 3), (3, 4), (4, 5), (4, 6), (6, 7), 
(6, 8), (8, 9), (8, 10), (10, 11), (10, 12), 
(12, 13), (12, 15) 
Following is Depth First Traversal for P1: 0, 
1, 2, 3, 4, 6, 8, 10, 12, 15, 13, 11, 9, 7, 5, 
14 
P2: (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), 
(5, 8), (8, 9), (9, 10), (8, 11), (11, 12), 
(12, 13), (11, 14), (14, 15), (15, 16), (14, 
17), (17, 18), (18, 19), (17, 20), (20, 21), 
(21, 22) 
Following is Depth First Traversal for P2: 0, 
1, 2, 3, 4, 5, 8, 11, 14, 17, 20, 21, 22, 18, 
19, 15, 16, 12, 13, 9, 10, 6, 7 
arr P1 = 0, 1, 2, 3, 4, 6, 8, 10, 12, 15, 13, 11, 
9, 7, 5, 14 
No Clone Pair Detected 
arr P2 = 0, 1, 2, 3, 4, 5, 8, 11, 14, 17, 20, 21, 
22, 18, 19, 15, 16, 12, 13, 9, 10, 6, 7 
4.3 
Working of Proposed Clone Detection Algorithm to Show 
Non-detection 
Working with the same algorithm but a different set of programs we will now see 
how the algorithm is not able to detect dissimilar codes. In this case we have selected 
the use of if-else in Fig. 4a and switch case in Fig. 4b which has been used to create a 
simple color conﬁrmation program. The SDG of the two programs has been shown 
in Fig. 5. The following Table 3 shows all the input and output of the algorithm. 
Thus it can be seen that the algorithm is unable to detect the clones in the above 
two programs as they are dissimilar to one another. 
(a) SDG of IF-ELSE Program
(b) SDG of Switch Case Program 
Fig. 5 SDG of the Program Codes of Fig. 4

Automatic Code Clone Detection Technique Using SDG
337
5 
Conclusion 
Code clone detection technique helps the developers who have the idea that their 
project might contain some code clones mostly unintentionally. However, this tech-
nique will be able to help the clients who choose the developers to complete their 
project and the developers create the whole project with the help of code collected 
from various different sources. In this work, we have proposed a new technique for 
code clone detection taking the help of SDGs. We have compared the SDGs of dif-
ferent programs and our algorithm is able to ﬁnd if any similar codes are present 
in both the programs. With two case studies we have explained the working of our 
proposed work. In future, we are working upon to make our proposed algorithm to 
detect code clone in presence of concurrency and server-client architecture in the 
program. 
References 
1. Aho A, Lam M, Sethi R, Ullman J (2007) Compilers: principles, techniques, & tools. Pearson 
Education, India 
2. Roy C, Cordy J (2009) A mutation/injection-based automatic framework for evaluating code 
clone detection tools. In: 2009 international conference on software testing, veriﬁcation, and 
validation workshops, pp 157–166 
3. Rattan D, Bhatia R, Singh M (2013) Software clone detection: a systematic review. Inf Softw 
Technol 55:1165–1199 
4. Ducasse S, Rieger M, Demeyer S (1999) A language independent approach for detecting 
duplicated code. In: Proceedings IEEE international conference on software maintenance-1999 
(ICSM 1999). Software maintenance for business change (Cat. No. 99CB36360) 
5. Roy C, Cordy J (2008) NICAD: accurate detection of near-miss intentional clones using ﬂex-
ible pretty-printing and code normalization. In: 2008 16th IEEE international conference on 
program comprehension, pp 172–181 
6. Liu Z, Wei Q, Cao Y (2017) VFDETECT: a vulnerable code clone detection system based on 
vulnerability ﬁngerprint. In: 2017 IEEE 3rd information technology and mechatronics engi-
neering conference (ITOEC), pp 548–553 
7. Kamiya T, Kusumoto S, Inoue K (2002) CCFinder: a multilinguistic token-based code clone 
detection system for large scale source code. IEEE Trans Softw Eng 28:654–670 
8. Kawaguchi S, et al (2009) SHINOBI: a tool for automatic code clone detection in the IDE. In: 
2009 16th working conference on reverse engineering, pp 313–314 
9. Murakami H, Hotta K, Higo Y, Igaki H, Kusumoto S (2012) Folding repeated instructions 
for improving token-based code clone detection. In: 2012 IEEE 12th international working 
conference on source code analysis and manipulation, pp 64–73 
10. Baxter I, Yahin A, Moura L, Sant’Anna M, Bier L (1998) Clone detection using abstract 
syntax trees. In: Proceedings of the international conference on software maintenance (Cat. 
No. 98CB36272), pp 368–377 
11. Koschke R, Falke R, Frenzel P (2006) Clone detection using abstract syntax sufﬁx trees. In: 
2006 13th working conference on reverse engineering, pp 253–262 
12. Nguyen H, Nguyen T, Pham N, Al-Kofahi J, Nguyen T (2009) Accurate and efﬁcient structural 
characteristic feature extraction for clone detection. In: International conference on fundamen-
tal approaches to software engineering, pp 440–455

338
A. Bhattacharyya et al.
13. Singh J, Mohapatra D (2018) Dynamic slicing of concurrent AspectJ programs: an explicit 
context-sensitive approach. Softw Pract Exper 48:233–260 
14. Krinke J (2001) Identifying similar code with program dependence graphs. In: Proceedings 
eighth working conference on reverse engineering, pp 301–309 
15. Higo Y, Yasushi U, Nishino M, Kusumoto S (2011) Incremental code clone detection: a PDG-
based approach. In: 2011 18th working conference on reverse engineering, pp 3–12 
16. Liu C, Chen C, Han J, Yu P (2006) GPLAG: detection of software plagiarism by program 
dependence graph analysis. In: Proceedings of the 12th ACM SIGKDD international conference 
on knowledge discovery and data mining, pp 872–881 
17. Kamiya T (2015) An execution-semantic and content-and-context-based code-clone detection 
and analysis. In: 2015 IEEE 9th international workshop on software clones (IWSC), pp 1–7 
18. Tekchandani R, Bhatia R, Singh M (2013) Semantic code clone detection using parse trees and 
grammar recovery. In: Conﬂuence 2013: the next generation information technology summit 
(4th international conference), pp 41–46 
19. Mayrand J, Leblanc C, Merlo E (1996) Experiment on the automatic detection of function 
clones in a software system using metrics. ICSM 96:244 
20. Munjal D, Singh J, Panda S, Mohapatra D (2015) Automated slicing of aspect oriented pro-
grams using bytecode analysis. In: 2015 IEEE 39th annual computer software and applications 
conference, vol 2, pp 191–199 (2015)

Simulated Design of an Autonomous 
Multi-terrain Modular Agri-bot 
Safwan Ahmad, Shamim Forhad, Mahmudul Hasan Shuvo, Sadman Saifee, 
Md Shahadat Hossen, Kazi Naimul Islam Nabeen, 
and Mahbubul Haq Bhuiyan 
Abstract Agriculture is the largest engine of economic growth in Bangladesh, 
providing food for almost half of the population. Fungal diseases in agriculture have 
a strong inﬂuence on producers’ livelihoods. To address this major worry, simulated 
design of an agriculture robot is presented in this research paper. The primary goal of 
this initiative was to improve competency in manufacturing agricultural grains, which 
will eventually lessen food shortages in Bangladesh while saving human labor, time, 
and money. All structural designs in this study were created in Autodesk Fusion 360 
and then implemented in the Webots simulator. Proteus 8.9 Professional was used to 
model electrical circuits and decision processes, while MATLAB Simulink was used 
to accomplish a portion of the simulation. The key decision and control system for 
the entire system is a microcontroller. Irrigation, seed planting, excavating, leveling, 
automated ploughing, harvesting operation, and obstacle distance assessment are 
all possible with this designed robot. The simulation data, ﬁndings, and limitations 
were all observed. Based on the results, it is feasible to conclude that this farm robot 
can effectively enhance efﬁciency when compared to human labor. As a result of 
this combined design, Using IoT infrastructure, the operator can carry out several 
farming activities quite conveniently. The study offered a comprehensive perspective 
of the prototype architecture that demonstrated how well the farm robot’s functions. 
Keywords Internet-of-things · GSM Module · Webots · Agricultural · Versatile 
Delta track · Plough · Cutter · Thresher
S. Ahmad · S. Forhad envelope symbol · M. H. Shuvo 
American International University-Bangladesh, 408/1, Kuratoli, Dhaka 1229, Bangladesh 
e-mail: mrshamimforhad@gmail.com 
S. Saifee · M. H. Bhuiyan 
Independent University Bangladesh, Plot 16 Aftab Uddin Ahmed Rd, Dhaka 1229, Bangladesh 
e-mail: mh_bhuiyan.sets@iub.edu.bd 
M. S. Hossen 
Manarat International University, Plot # CEN-16, Rd 106, Dhaka 1212, Bangladesh 
K. N. I. Nabeen 
United International University, United City, Madani Ave, Dhaka 1212, Bangladesh 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_30 
339

340
S. Ahmad et al.
1 
Introduction 
Among the oldest occupations in the entire globe is farming. With the aid of research, 
the farming industry may be expanded and made accessible to the public. 63% of 
the inhabitants was occupied by this major sector of Bangladesh’s economy, which 
contributed 19.6% towards the nation’s gross Domestic product. Producers employed 
handcrafted harvesting tools in the past. They were unable to grow sufﬁcient harvests 
as a result. It represented a squandering of resources—time, effort, cost, labor, etc. 
Previously, producers were unable to raise sufﬁcient crop production [1]. 
Precision agriculture relies heavily on two key processes, namely agriculture 
information detection and spraying. To ensure effective execution of these processes, 
it is imperative for the vehicles used to possess strong uneven terrain adaptability, 
considering the typical rough and uneven nature of crop ﬁelds. A study was conducted 
to develop an agricultural robot with this capability, which features a 3D rotating 
coupling as a crucial component. The robot underwent several types of analysis, 
including static analysis, frequency analysis, and free frequency analysis. To further 
enhance its functionality, the robot was equipped with an NDVI camera, a handheld 
ADC camera, and a canopy temperature testing system. Experiments were carried out 
in corn, cotton, and soybean ﬁelds during different growing seasons, and the results 
indicated that the data collected by the robot was of better quality and obtained more 
rapidly than that obtained through traditional manual methods [2]. 
The Agribot is an autonomous robot designed for farming purposes. Controlled by 
Arduino, it can perform various tasks like plowing, sowing, and watering. It is an asset 
to farmers, especially in developing countries, as it can replace traditional farming 
machinery. The robot operates using ultrasonic detection technology, allowing it to 
change its position within seconds. Currently, the robot is designed to work on 0.25-
acre farmland, but there is room for improvement. The Agribot is expected to greatly 
impact the farming industry by reducing costs and increasing proﬁts for farmers [3]. 
In this research work, the primary objective was to increase the production of 
harvest with the help of an automated electromechanical harvester. The signiﬁcance 
of the proposed design is various from several agricultural tasks, which will be 
consist of multiple sensors like the ultrasonic sensor, soil moisture sensor, water 
level detector etc. Capacity ranges from three to twelve hours per hectare depending 
on the kind of machinery utilized and mostly on their operating area [4]. 
2 
Methodology 
The designed robot will use image processing to examine data and will determine 
its path after identifying rows and shapes. Only if there are no obstructions will it 
move. Whenever there are no barriers, it will begin to move and do the necessary 
task. It will gauge the soil’s moisture content. If watering is necessary, this would 
initiate the engine after measuring. When a ploughing operation is necessary, it will

Simulated Design of an Autonomous Multi-terrain Modular Agri-bot
341
Fig. 1 Operation block diagram 
do one. When the crops are ready, it will chop and thresh them in accordance with 
user requirements. 
2.1 
Operation Block Diagram of the System 
The primary operating concept of our project is depicted in this block diagram. Inputs 
in this scenario include component switching devices, a webcam, and ultrasonic 
sensors. The necessary operation is chosen via input switches. Its ultrasonic sensors 
could very well identify the obstruction, and the camera is utilized to capture live 
photos. These three signals are sent straight to the micro - controller, which interprets 
the signals and visuals. The micro - controller selects its course and mode of operation 
after digesting such pictures and indications, and it then begins to avoid obstacles 
(Fig. 1). 
2.2 
Working Flowchart 
A design of the system of our project’s operational process is shown in Fig. 2 for your 
convenience. The motor must ﬁrst be started, then an operation must be selected. The 
needed component must be connected after selecting the procedure, and the switch 
must be pressed. The webcam and ultrasonic sensors will be used by the robot to 
assess pictures to avoid obstacles. The robot will begin to move when it encounters 
an impediment, but it will ﬁrst avoid it.

342
S. Ahmad et al.
Fig. 2 Working Flowchart 
3 
Structural Design 
We had to prototype many robot components using Autodesk Fusion 360 to create 
the desired robot. In Autodesk Fusion 360, the main body, adjustable delta track, 
plough, thresher, as well as 3D model of the robot were all created. 
3.1 
Main Body Design of the Robot 
The motor driver, battery, microprocessor, and other components were all intended 
to be housed inside the body. The wheel, the thresher, the cutter, the plough, the 
seeder, the water pump, and several types of sensors were some of the components 
linked to the main body. The dimension of the body is 215 mm long, 50 mm height, 
and 120 mm width. The mass of the body is 1.2 kg (Fig. 3).
3.2 
Versatile Delta Track 
Based on where the nearest obstacles are to the course and the corresponding location 
of one’s own pair of roller wheels, idler wheels, and driving wheel, a ﬂexible delta

Simulated Design of an Autonomous Multi-terrain Modular Agri-bot
343
Fig. 3 Main Body
track technique is created for supporting conversion system’s general form. The 
components of versatile delta track have Driver wheel with 52 mm radius, 3.5 mm 
thickness, and 30 mm width. It has 15 grips/spikes. The dimension of spikes is 5 mm 
height, and 10 mm width [5]. 
Spring Suspension. Spring suspension has been installed between the chassis and 
the idler wheel. Their main goal is to compensate for uneven road surfaces, which 
guarantees a very comfortable ride. Second, they must make sure that the wheels are 
always in contact with the road safely, regardless of how severe the weather is. 
Idler Wheel. Idler wheels are used to effortlessly avoid obstacles. Dimension of 
idler wheel is 20 mm radius, 10 mm width, and 2 mm thickness. 
Roller Wheel. Roller wheel is one of the most important equipment of versatile 
delta track. By using roller wheel, rubber track cling with soil. Dimension of roller 
wheel is 10.5 mm radius, 2 mm thickness, and 10 mm width. 
Also, versatile delta track has three other equipment like Versatile Driver Wheel 
Arm, Versatile Driver Wheel Chassis, and Driver Wheel Mud Guard (Fig. 4). 
Fig. 4 Versatile Delta Track

344
S. Ahmad et al.
Fig. 5 Plough 
3.3 
Plough 
The primary agricultural implement with blades for preparing soil for seeding is the 
plough. This plough is not made in the same way as a conventional plough. The 
dimension of plough is 275 mm width, 70 mm long, 75 mm height, 10 mm radius 
shaft with mixer, and 10 stick for digging (Fig. 5). 
3.4 
Cutter 
An agricultural instrument called cutter is used to harvest crops from the ground. 
The dimension of cutter is 255 mm width, 250 mm long. It has 130 mm long 6 spikes 
with 50 mm radius cutter fan (Fig. 6). 
Fig. 6 Cutter

Simulated Design of an Autonomous Multi-terrain Modular Agri-bot
345
Fig. 7 Thresher 
3.5 
Thresher 
A thresher is a harvesting instrument that threshes crops, pulls weeds, and collects 
seeds. A sprocket is attached with the shaft of the motor. Another sprocket relates 
to the thresher shaft. Motor sprocket and thresher sprocket was linked with a chain. 
A thresher has so many teethes. When the thresher rotates at high speed, there is a 
conﬂict between the thresher and crops. For that reason, seeds are separated from 
crops. After separating the seeds, those are collected in a chamber through a net. 
Crops will be throwing out for the rotating inertia of the thresher (Fig. 7). 
3.6 
3-Dimentional Prototype of the Agri-Bot 
Figure 8 shows the robot’s ﬁnished body with a thresher and cutter. 
Fig. 8. 3-D model of the Final Robot

346
S. Ahmad et al.
Fig. 9 Autonomous movement by image processing 
3.7 
Autonomous Movement of the Robot 
The robot’s route is seen in Fig. 9. The robot will use image processing to examine 
the data in this case, and after identifying the row and form, it will determine its path. 
The robot will come across an “ENTER” sign when it approaches the ﬁeld. It will 
turn left when it comes across the “LEFT” sign. Like that, it will turn to the right 
when it comes across the “RIGHT” sign. It will leave the ﬁeld by shutting down all 
activities when it comes across the “EXIT” sign [6]. 
4 
Simulation and Result 
4.1 
Proteus Implementation Results 
The plowing process as in the output of the top circuit diagram, pressing the start 
button will initiate the plowing procedure. LED in red lit up. Next, the OLED screen 
will display the message “Connect component” in a pop-up window. Acceleration 
and range will be displayed on the screen as soon as the user connects by pushing 
the conﬁrm switch. To adjust the plough during plowing operations, an ultrasonic 
sensor will be used to measure the distance between the ground and the plough. 
Figure 10 depicts the stepper motor, symbolized by a blue LED that has been turned 
on. A pumping motor has been modeled in this circuit as a green light, which will 
be shining in Fig. 10.
The irrigation system’s mechanism is depicted in Fig. 11. The moisture sensor’s 
value was measured and shown on the screen. The pump motor is started after a 
message such as “Moisture = 83.300” is shown.
The threshing process is shown in Fig. 12 in this instance. When tapping the 
conﬁrm button, the robot’s thresher will begin to revolve after a notice concerning 
the motor speed is displayed. The motor is indicated by a RED LED that is lit up 
and active. In picture 9, where BLUE LED stands in for a stepper motor, it is noted 
that such a stepper motor has been employed for the plowing action. The ultrasonic 
sensor will be used to gauge the distance between the plough and the ground when

Simulated Design of an Autonomous Multi-terrain Modular Agri-bot
347
Fig. 10 Ploughing Operation
Fig. 11 Watering Operation
plowing is necessary. A Green Light will light on when the plough has been adjusted 
based on the measurements. 
Fig. 12 Threshing Operation

348
S. Ahmad et al.
IOT Based Watering Method Simulation. The moisture content is 64%, and the 
tank level is 53%, as shown in Fig. 13. A range of 10 to 85% of moisture content was 
chosen. Water pumping will automatically begin if the water level falls below 65%. 
We need to hydrate the area for that reason. As a result, the pump motor and tank 
motor are both pumping water from the reservoir and the plant, respectively. The 
person who was automated by the GSM module receives a message once it shows 
in the virtual terminal. 
Battery Charging and Discharging 
The modeling results for photovoltaic panels are shown in Fig. 14. The switch will 
turn on when the battery begins to be charged by the solar panel, and the yellow 
LED in the illustration indicates this. Another LED will then continue to be off. 
Discharging a battery Again, the circuit will be switched off after the battery has 
fully charged thanks to the solar panel, at which point the draining cycle will begin. 
The switch has been closed at that point; the other LED is lighted. 
Fig. 13 IoT based watering method simulation 
Fig. 14 Battery Charging and Discharging

Simulated Design of an Autonomous Multi-terrain Modular Agri-bot
349
Power Consumption 
Total Power = 181 W [Found by multiplying voltage and current] 
StartL ayout 1st R ow 1st Co lumn  Tota l
 Daily Load left parenthesis 4 Hours per day right parenthesis 2nd Column equals left parenthesis StartFraction 181 Over 1000 EndFraction right parenthesis asterisk 4 2nd Row 1st Column Blank 2nd Column equals 0 period 724 kW slash upper H EndLayout
Star
t
Layo
ut  1st R ow 1st Column Total Daily Load left parenthesis 4 Hours per day right parenthesis 2nd Column equals left parenthesis StartFraction 181 Over 1000 EndFraction right parenthesis asterisk 4 2nd Row 1st Column Blank 2nd Column equals 0 period 724 kW slash upper H EndLayout
Our minimum energy delivery need is 0.724 kWh if our robot runs for 4 h every 
day. For the overall power consumption in this section, we counted all the batteries. 
We discovered that the system’s combined power output is 181 W. Total current is 
15.08 A found from the calculations. Battery’s rated conﬁguration is 12 V, 3.3 A. So, 
if we divide the total current by the battery’s rated current, we can ﬁnd the number of 
batteries. Here, we need at least 5 DC 12 V Li-Po Batteries for running the system. 
4.2 
WEBOT Simulation and Result 
For start executing our robot inside a 3-D environment for such a project, we devel-
oped it in the Webots simulator. The ﬁrst step was to export our robot into the Webots 
program and add its required components, including its hinge joint, physics, motor, 
sensors, as well as camera. As that of an object (.obj) ﬁle, we exported it. The object 
(.obj) ﬁle was then converted to a VRML97 (.wrl) ﬁle. Without VRML97, no ﬁle is 
accepted by the Webots. It was created as a VRML97 (.wrl) format and put into the 
Webots program. After implementing every component, we had to create an indi-
vidual children option. In that children option, we set our parameters in sub-option 
Physics, Hinge joint. In physics sub-option there were some by default parameters. 
By default, parameters consist of position, axis, anchor, minStop, maxStop etc. That 
parameter Webots creates by itself when the robot runs. Although our initial model 
which was made in Autodesk Fusion360 was a solid (stainless steel) body, in Webots 
we set our body metalness as 0 (zero). Webots does not give any documentary result, 
only visible result is existing. After complete giving its all functions, then we gave 
the camera code. Which was source code. That source code was modiﬁed for our 2-
wheeler robot. Then the robot run as an Obstacle Avoidance robot and automatically 
opened a window for the Camera. 
Parameter and Results. At the left side of the Webots interface, a window appeared 
called “Robot”. Here we can see different values for different positions of the robot. 
In Fig. 15, the robot was above the rectangular arena and its mass is shown in the 
window of “Robot”. We placed our designed robot in Webots simulator, but the robot 
was not able to move because of some critical mechanism of versatile delta track. 
That is why we made a demo robot. The demo robot moved successfully. Then all 
the anticipated measurements, camera output, position, rotation, and velocities were 
found.

350
S. Ahmad et al.
Fig. 15 Webots implementation results with total layout and code 
The speed at which the robot takes turns varies a different time. The Fig. 16 show 
camera capture for different velocity, position and rotation values for different time 
which clariﬁes the difference in its value. The speed at which the robot takes turns 
varies a different time. 
Future Scope. The future scope of this agricultural robot project includes improve-
ments such as an electrical arm for more efﬁcient weed removal, the use of nanotech-
nology to minimize chemical use and nutrient losses, implementation of AI tech-
nology for improved seed mapping and sowing, and the creation of a user-friendly 
Android app for controlling the robot.
Fig. 16 Movement, Position and Velocity Parameter 

Simulated Design of an Autonomous Multi-terrain Modular Agri-bot
351
5 
Conclusion 
This paper consists of a simulated model of an autonomous solar-powered agro-
bot. Firstly, the 3D models were appropriately illustrated. Secondly, the total power 
consumption of the entire system was calculated precisely. Thirdly, several simu-
lations microcontroller-based results were demonstrated, and armature voltage and 
ﬁeld current were analyzed and displayed. Battery performance and all the necessary 
ﬁgures and curves were also illustrated. Also, the solar powered battery charging and 
discharging circuit was demonstrated. Additionally examined were ﬁeld current and 
armature voltage. The Webots performance appraisal were then shown, showing all 
the location and velocity parameters. The image processing technology was included 
in our robot as well. Overall, all the calculations, data, and images were given as proof 
of concept for how this model is efﬁcient and precise from the traditional human 
operated vehicles at improving the current condition of farming. 
References 
1. Forhad S, Hossen MS, Ahsan IA, Saifee S, Nabeen KNI, Shuvo MRK (2023) An intelligent 
versatile robot with weather monitoring system for precision agriculture. In: 2023 6th Interna-
tional Conference on Information Systems and Computer Networks (ISCON), Mathura, India, 
pp 1–7. https://doi.org/10.1109/ISCON57294.2023.10112101 
2. Wang Z, Lan Y, Hoffmann C, Zhang Z, Wang Y (2013) Adjustable multiple terrain agricultural 
robot system. In: American society of agricultural and biological engineers annual international 
meeting 2013, ASABE 2013. 4. https://doi.org/10.13031/aim.20131618416 
3. Sujon MDI, Nasir R, Habib MMI, Nomaan MI, Baidya J, Islam MR (2018) Agribot: Arduino 
controlled autonomous multi-purpose farm machinery robot for small to medium scale cultiva-
tion. In: 2018 international conference on intelligent autonomous systems (ICoIAS), Singapore, 
pp 155–159. https://doi.org/10.1109/ICoIAS.2018.8494164 
4. Lv K, Mu X, Li L, Xue W, Wang Z, Xu L (2018) Design and test methods of rubber- track 
conversion system 1–27. https://doi.org/10.1177/0954407018794101 
5. Yaghoubi S, Akbarzadeh NA, Bazargani SS, Bazargani SS, Bamizan M, Asl MI (2013) 
Autonomous robots for agricultural tasks and farm assignment and future trends in agro robots. 
Int J Mech Mechatron Eng 13(3):1–6 
6. Forhad S, Zakaria Tayef K, Hasan M, Shahebul Hasan ANM, Zahurul Islam M, Riazat Kabir 
Shuvo M (2023) An autonomous agricultural robot for plant disease detection. In: Hossain MS, 
Majumder SP, Siddique N, Hossain MS (eds) The Fourth Industrial Revolution and Beyond. 
Lecture Notes in Electrical Engineering, vol 980, pp 695–708. Springer, Singapore. https://doi. 
org/10.1007/978-981-19-8032-9_50

Customer Segmentation Analysis Using 
Clustering Algorithms 
Biyyapu Sri Vardhan Reddy, C. A. Rishikeshan, VishnuVardhan Dagumati, 
Ashwani Prasad, and Bhavya Singh 
Abstract Customer segmentation has been deployed as a prudent marketing strategy 
by companies to ensure that their investments are less risky and more judicious. 
Segmenting customers helps the companies to divide the customers into groups that 
reﬂect similarity and maximize the value of each customer to the business. The main 
goal of this research is to use a machine learning clustering approach called K-means 
clustering to accomplish consumer segmentation. Besides, the research work is also 
focused on performing exploratory data analysis on the given dataset. To group the 
customers into clusters, a K-means clustering algorithm is performed on the customer 
dataset. To achieve optimization and validation of the clusters, popular heuristic, 
interpretation, and approximation methods have been included in this paper. Further, 
for analyzing and visualizing the important facets of the customer dataset and the 
operation of the K-means algorithm, the paper presents some colorful and informative 
representations. The implementation of this research work has been done in the R 
programming language. The outcome of this work includes visualizing the segments 
of the mall customers in the form of clusters based on their spending scores and annual 
incomes. Furthermore, a better customer segmentation could be achieved by taking 
product reviews and customer feedback into consideration. Nevertheless, customer 
segmentation remains a prospective topic for many researchers and companies due 
to dynamic customer behavior.
B. S. V. Reddy · C. A. Rishikeshan · A. Prasad · B. Singh 
School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, India 
e-mail: biyyapu.srivardhan2019@vitstudent.ac.in 
C. A. Rishikeshan 
e-mail: rishikeshan.ca@vit.ac.in 
A. Prasad 
e-mail: ashwani.prasad2019@vitstudent.ac.in 
B. Singh 
e-mail: bhavya.singh2019@vitstudent.ac.in 
V. Dagumati envelope symbol
Department of Computer Science and Engineering, School of Computing, Amrita Vishwa 
Vidyapeetham, Chennai, India 
e-mail: ch.en.u4cse19004@ch.students.amrita.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_31 
353

354
B. S. V. Reddy et al.
Keywords Customer segmentation · K-Means clustering · Visualization using R ·
Optimal clustering 
1 
Introduction 
In the present day and age, the signiﬁcance of dealing with the customers as the 
most prominent resource of a conglomerate is becoming greater in usefulness. 
Establishments are expeditiously providing capital for ﬁner consumer accession, 
perpetuation, and evolution. The notion of corporate brainpower has a climacteric 
role to play for the probability of establishments using methodological prowess to 
take possession of preferable understanding for capacity-building programs. Hence 
through establishing it, in this framework the idea of customer segmentation anal-
ysis accumulates much attention due to it being an all-inclusive procedure of gaining 
and preserving consumers, making use of business intellect, with the purpose of 
augmenting the customer desirability for a corporate endeavor. One of the two most 
paramount intents of customer segmentation is consumer advancement with the aid 
of awareness. This intention of customer segmentation necessitates the employment 
of systematic outlook for the sake of accurately evaluating customers’ particulars and 
exploration of the worth of consumers for greater customer insight. Keeping abreast 
of ever-changing times, establishments are altering their corporate drift prototypes 
through the deployment of systems engineering, change administration and plan-
ning out information technology responses that would be helping in gaining brand-
new consumers, alongside keeping, and maintaining the existing consumer base and 
enhancing the customer’s long-lasting merit. Because of the variety of products and 
facilities that are within an easy reach in the market and besides ﬁerce competi-
tiveness prevailing amongst diverse group of organizations, this form of customer 
relationship management has now come off as playing out quite an important role 
for the purpose of recognition and probing of a particular company’s ﬁnest clients 
as well as the acquisition of superlative marketing methodologies in order to obtain 
and preserve competitive precedence [1]. Unsupervised learning is used for a variety 
of purposes, including customer segmentation. Customer segmentation would go 
on to enable organizations in ﬁnding copious categories of clients who think and 
operate distinctively and pay heed to diverse perspectives in their expenditure and 
procurement habits. Companies can discover numerous segments of clients with the 
utilization of K-means clustering technique, allowing them to target the possible user 
base by splitting it by gender, age, interests, and other buying habits. These tech-
niques divulge inside correlative and outside miscellaneous categories. Consumers 
differ in their characteristics, needs and the primary objective of all types of clus-
tering techniques is to ﬁnd out consumer sets and classify the base into classes 
of like proﬁles for the purpose of carrying out target marketing in a much more 
well-organized manner. Data visualization would be done through differing visual 
elements in the form of plots and graphs for the provision of accessible ways to infer 
the trends, outliers, and patterns in the data [2]. K-means clustering, which is a vector

Customer Segmentation Analysis Using Clustering Algorithms
355
quantization method, would be partitioning the observations into clusters with each 
observation belonging to the cluster with the nearest mean. We would be ﬁnding 
out the optimal clusters using the following three methods- elbow, silhouette, and 
Gap Statistic. Finally, the visualization of the optimized clustering results would be 
performed with the utilization of the principal component analysis [3]. 
The paper is put in order as follows. Section 2 contains the related work i.e., the 
literature survey of the works corresponding to this paper’s topic. Section 3 explains 
the data preparation and visualization part. Section 4 elaborates the proposed method. 
Section 5 summarizes the results and discussion. Section 6 has the conclusions in it 
and ﬁnally Sect. 7 enumerates all the references. 
2 
Related Work 
Numerous works have been published in the ﬁeld of customer segmentation. Different 
approaches yield different results with their unique advantages and disadvantages. 
For instance, Kansal et al. [3] have used three distinct approaches to compartmentalize 
the customers and those clustering techniques are- K-Means, bottom-up approach/ 
hierarchical agglomerative clustering (represented by dendrogram) and mean shift 
algorithm. In this work; random, unlabeled data has been used which would hence 
explain the employing of internal clustering validation that would pick out the most 
suitable technique which could accurately cluster the input data into its contrary 
cluster. Syakur et al. [5] have used the K-Means clustering technique in combination 
with the elbow method. The objective of picking out the Elbow method is to supple-
ment the K-Means execution for the reﬁning of the extensive quantity of data utilized. 
This would occur by deciding the best number of clusters from those rendered by the 
K-Means. Ezenkwu et al. [6] have introduced the application of MATLAB program-
ming language for the purpose of carrying out the K-Means clustering method on 
the basis of data gathered from a MegaCorp clothes vocation. This research paper 
would conclude in the business orchestrating particular market schemes that would 
be appropriate for each one of its consumer sections. Kashwan et al. [7] have used  
the K-Means clustering algorithm and Statistical Package for the Social Sciences 
(SPSS) tool with the agenda of forecasting the sales of a speciﬁc market in varied 
yearly recurrent cycles. An analysis of variance was also carried out for the purpose 
of testing the stability of the clusters. The computing established set-up proved to 
be intuitive and provided outcomes to the managers for making swift and fast reso-
lutions. Aryuni et al. [8] have ﬁtted K-Means and KMedoids methods based on 
recency, frequency and monetary (RFM) analysis score in relation to an Internet 
banking system data. And the output showed that the former method has outper-
formed the latter one on the grounds of intra cluster i.e., AWC distance. Datta et al. 
[9] has used the K-Nearest Neighbor (KNN) algorithm. The K-nearest neighbors’ 
technique is a supervised Machine Learning algorithm that may be used to predict 
both classiﬁcation and regression issues. It’s a straightforward notion that uses the 
concept of viewing surrounding data to classify any set of inputs. If K = 1, only

356
B. S. V. Reddy et al.
one neighbor is cross-checked, and the input is classiﬁed based on the class category 
of that neighbor. When K = 4, four of the neighbors are examined, and the appro-
priate class is assigned. If an appropriate value of K can be roughly calculated, this 
is a proper approach. Rizki et al. [10] has used two methods for Customer Loyalty 
Segmentation on a Point-of-Sale System; they are Recency-Frequency-Monetary 
(RFM) model and K-Means algorithm. 
Besides K-Means Clustering and related approaches, other techniques for segmen-
tation were also used in several research works. For instance, Song et al. [11] consid-
ered feature selection to be an important aspect of customer segmentation. They used 
one of the advanced techniques to perform customer segmentation, called hydro-
logical cycling optimization (HCO). This method was based on a meta-heuristic 
approach. The proposed method was able to evolve a set of non-dominated solu-
tions with a smaller number of features which yielded highly accurate results. Wu 
et al. [12] integrated churn prediction with customer segmentation. Their area of 
focus lies in a telco industry, where churn management becomes crucial. They 
implemented multiple ML classiﬁcation algorithms to perform the churn prediction 
including K-means clustering. Further, to conduct the factor analysis and identify 
some key features for turnover customer segmentation, Bayesian Logistic Regres-
sion was used. Manjunath et al. [13] proposed a multi-layer hierarchical super peer 
P2P network architecture to perform the distributed clustering problem involving 
customer segmentation. Their methodology was in contrast to the centralized clus-
tering approach. The method allowed the ﬂexibility to use different datasets of varying 
sizes. José J. López et al. [14] aim at making provisions of electric efﬁcacies with a 
bulk of data on the basis of customer classiﬁcation to facilitate them to set up distinct 
sorts of excise tariffs. They have used the ensuing methods to classify the elec-
tricity clients- hierarchical clustering, modiﬁed to follow the leader and K-Means. 
Their proposition eradicates the volatility of the preliminary solution and advances 
towards the global optimum. Maree et al. [15] discovered that the clusters created 
aren’t discriminating enough for micro-segmentation. As a result, they concentrated 
on extracting temporal features with continuous values from hidden states of neural 
networking in order to forecast client spending behavior in terms of transactions. 
They created micro-segments and course segments using Long Short-Term Memory 
(LSTM) and feed-forward neural networks, respectively. 
The motivation for this research work includes the following:
1. Major emphasis on data visualization of the attributes required for customer 
segmentation. 
2. Re-deﬁning the use of K-Means Clustering algorithm with optimization so that 
the results are comparable with recent approaches for customer segmentation. If 
K-Means clustering algorithm is used all by itself, as seen in some of the related 
works, then there are highly probable chances of it producing high fallacies and 
mediocre cluster results, that is due to it being a localized maximization method. 
3. We have some rudimentary details about the customers like age, Customer 
ID, gender, yearly earnings and spending score. We ought to understand the 
customers like who the target customers are, so that the perception can be made

Customer Segmentation Analysis Using Clustering Algorithms
357
Table 1 Mall Customer Dataset 
Customer ID
Gender
Age
Annual Income (k$)
Spending Score (1–100) 
1
Male
19
15
39 
2
Male
21
15
81 
3
Female
20
16
6 
4
Female
23
16
77 
5
Female
31
17
40 
6 
… 
Female 
… 
22 
… 
17 
… 
76 
… 
as a provision to the marketing team who can eventually sketch their strategy 
correspondingly to maximize their proﬁts.
3 
Data Preparatıon and Vısualızatıon 
In this section, the dataset collected for performing customer segmentation is 
explored. Further, the visualizations of some fundamental attributes related to 
customers at a typical shopping complex is carried out using insightful and varied 
forms of graphs. 
3.1 
Data Exploration 
The dataset is collected from a typical mall. There were 200 customers in total, who 
were analyzed based on their gender, age, annual income (per 1000 $ or k $) and 
based on their spending behavior, each customer was given a spending score out of 
100. The dataset does not contain any redundant values or null values. The completed 
dataset consists of 200 rows excluding the header row and 5 columns in total. The 
ﬁrst 6 records of the dataset are shown in Table 1 for reference. 
3.2 
Visualization of Attributes 
In this section we use each attribute of our dataset for visualization purposes like 
Age, Gender, and annual income in 1000$ or k$. In this section, there will be four 
divisions (subsection) each section consists of an attribute.

358
B. S. V. Reddy et al.
Fig. 1 Bar plot to display gender comparison 
Fig. 2 Pie chart to display 
ratio of male and female 
3.2.1
Data Visualization Using Customer Gender 
A box plot and pie chart is create to to illustrate using mall_customer datset. For 
the bar graph (as show in Fig. 1), we are using red color for female and green color 
for male it will be useful for gender comparison. Then, for the pie chart (as show in 
Fig. 2), we are using red to show female percentage and sky blue for male percentage. 
Now, from bar plot (as show in Fig. 1), we come to know that the females are more 
in number than male persons. Then From the above pie-chart (as show in Fig. 2), 
we can say that the female’s percentage is 56%, and the male percentage in the mall 
customer dataset is 44%. 
3.2.2
Data Visualization Using Customer Age 
A histogram is plotted using (as show in Fig. 3) using Age attribute and a box plot 
to know which age group members are usually coming to the mall and the age of 
customers coming minimum and maximum to the mall using histogram and a box 
plot. We are using pink color for histogram and deep pink for box plot. For knowing 
the hex value of deep pink, we use color picker tool from the internet, then we get 
“#ff00ee” we use this to get a deep pink color in the box plot.
From the above two plots (as show in Fig. 3 and Fig. 4), i.e., the box plot and the 
histogram, we can deduce that the maximum customer ages are between 30 and 35, 
while the minimum and maximum client ages are 18 and 70.

Customer Segmentation Analysis Using Clustering Algorithms
359
Fig. 3 Histogram plot to 
display count of age class
Fig. 4 Boxplot for displaying detail analysis of age a 
3.2.3
Data Visualization Using Customer Annual Income (k$) 
In addition, we will construct visualizations to analyze the annual income of our 
customers. So, ﬁrst, we’ll make a histogram plot, and then we’ll move on to a density 
plot. The red color is used in the histogram (as show in Fig. 5). We are getting hex 
value in the same way above as we used in age histogram (as show in Fig. 3). Then 
we get “#ff002f”. Then for density graph (as show in Fig. 6) we are using light yellow 
color for that we get “#f2ff00”. 
From the above analysis (as show in Fig. 5 and Fig. 6), we can say that the minimum 
and maximum customers annual income is 15 and 137. Customers (people) with an 
average annual income of $70 are the most frequent in the histogram. We can also 
state that the average salary of clients is $60.56. In the preceding Density Plot, we 
can see that the annual income of the consumers follows a normal distribution.
Fig. 5 Histogram plot to display count for annual income

360
B. S. V. Reddy et al.
Fig. 6 Density Plot display detail analysis annual income
3.2.4
Data Visualization Using Customer Spending Score (1–100) 
A histogram is plotted using using the spending score attribute and a box plot to know 
the minimum and maximum spending score of customers. We are using orange color 
for boxplot (as show in Fig. 7) and blue for histogram plot. For knowing the hex 
value of orange, we use color picker from net then we get “#ffa200” for orange and 
“#0099ff” for blue which we are going to use in the histogram (as show in Fig. 8). 
Now, from the above analysis (Fig. 7 and Fig. 8), we can say that the minimum, 
maximum and average spending score is 1, 99 and 50.20. And we learned from the 
histogram plot that clients in the 40–50 age range have the highest spending score 
of all the classes, which is 40.
Fig. 7 Boxplot for detail analysis of spending score 
Fig. 8 Histogram plot to show detail analysis of spending score 

Customer Segmentation Analysis Using Clustering Algorithms
361
4 
The Proposed Method 
First, after collecting the dataset and data preparation, we would be performing data 
exploration. Then we would go on to visualize and analyze the data in R to get some 
necessary insights. Henceforth, we would be making use of the K-Means clustering 
algorithm in order to determine the optimal clusters representing different customer 
bases. Finally, the visualization of the clustering outputs would be done by making 
use of the Principal Components Analysis. The proposed methodology is presented 
in the form of a ﬂow chart as show in Fig. 9. 
4.1 
K-means Clustering Algorithm 
While we are in the phase of the utilization of the K-Means clustering algorithm, the 
pre-eminent step is to depute the quantity (number) of clusters that we are preparing 
to produce in the eventual output. The algorithm would begin by randomly selecting 
‘n’ objects from the dataset to serve as early cluster centers. These picked objects are 
the cluster means, which are also known as centroids in mathematics. The selected 
objects would then be assigned to the next closest centroid in the next stage, cluster 
assignment. The Euclidean distance, or the distance of a line segment between two
Fig. 9 Flow chart 
representing the proposed 
method 

362
B. S. V. Reddy et al.
points in Euclidean space, reveals the closest centroid. When this step is completed, 
the algorithm will compute the new mean value for each cluster in the data. The 
observations are analyzed to see if they are closer to a distinct cluster after all of this 
re-computation of the centers’ values. The items will be reassigned after the cluster 
mean has been renewed. This step would be repeated several times until the cluster 
assignments stop getting changed i.e., the clusters existing in the present iteration 
are equivalent to those which we received in the former iteration. 
The mathematical expressions used, corresponding to the above elaborated K-
Means clustering algorithm are as follows: 
upper E u c l i d e a n up per  D i s t a n c e comma d left parenthesis p comma q right parenthesis equals StartRoot left parenthesis x 2 minus x 1 right parenthesis squared plus left parenthesis y 2 minus y 1 right parenthesis squared EndRoot
/
uppe r E u c  l  i d  e  a n upper D i s t a n c e comma d left parenthesis p comma q right parenthesis equals StartRoot left parenthesis x 2 minus x 1 right parenthesis squared plus left parenthesis y 2 minus y 1 right parenthesis squared EndRoot
where p and q are the two points in the Euclidean space. 
Also, we know the formula of mean is given by: 
Mo di fyin gA bo ve x With quotation dash equals StartFraction left parenthesis x Subscript i Baseline plus x Subscript j Baseline right parenthesis Over 2 EndFraction
ModifyingAbove x With quotation dash equals StartFraction left parenthesis x Subscript i Baseline plus x Subscript j Baseline right parenthesis Over 2 EndFraction
4.2 
Cluster Optimization 
One needs to enumerate the number of clusters to be made use of beforehand itself. 
For this we would be using the optimal number of clusters. Clustering techniques’ 
major goal is to deﬁne clusters in such a way that intracluster variation is kept to a 
bare minimum. 
Minimum left parenth esis su m up per  C le f t  parenthesis upper W Subscript n Baseline right parenthesis right parenthesis having n equals 1 comma 2 comma 3 ellipsis n
where, Wn is the nth cluster and C(Wn)-intracluster variation. 
For the stated-out purpose we would be making use of the following three 
prominent methods:
. Average Silhouette method
. Gap Static method
. Elbow method 
4.2.1
Average Silhouette Method 
Utilizing this method, we would be calculating the attribute of the clustering perfor-
mance. Good clustering is indicated by a high average silhouette breadth. For different 
‘n’ values, this approach computes the average of all the silhouette observations. We 
would maximize the average silhouette over noteworthy values for k clusters. We

Customer Segmentation Analysis Using Clustering Algorithms
363
utilize the silhouette function and k-mean function for the purpose of calculating the 
average silhouette width. 
4.2.2
Gap Statistic Method 
With the usage of this method, we can contrast the aggregate intracluster disparity 
for distinct values of ‘n’ besides their contemplated values coming under the null 
reference distribution of the whole of the data. By using the Monte Carlo simulations, 
we can generate the trial dataset. Then we can compute the range between minimum 
and maximum for every variable in the generated dataset. Through this process one 
can generate values evenly starting from the lower bound up till the upper bound. 
We are using the “clusgap” function for the purpose of producing Gap Statistics 
alongside the standard error for a given result. 
4.2.3
Elbow Method 
First and foremost, we’ll compute the clustering technique for a variety of ‘n’ values. 
The former can be accomplished by varying the number of clusters inside n from one 
to 10. The aggregate intra-cluster sum of squares is then computed (s). We would be 
plotting it on the basis of the number of ‘n’ clusters which would be indicating the 
suitable number of clusters needed in our representation. The optimum number of 
clusters is designated by a bend’s location. 
5 
Results and Discussion 
As discussed in the above section there are 3 methods, we are using in this optimal 
clustering so, here in this section we are going to discuss the results of those methods 
in 3 sub divisions and another subdivision for visualizing the clusters. 
5.1 
Elbow Method 
In this section we are going to show the result for the Elbow method, so that we are 
taking the number of clusters on the x-axis and total intra-clusters sum of squares on 
the y-axis and then start the plotting. The resultant graph is shown in Fig. 10.
Now, based on Fig. 10, we may deduce that k = 6 is the correct number of clusters 
because it appears at the elbow bend in the graph above.

364
B. S. V. Reddy et al.
Fig. 10 Line graph using Elbow method
5.2 
Average Silhouette Method 
In this section we are going to show the result for the Average Silhouette method. So, 
ﬁrst we need to draw silhouette plots (as show in Fig. 11 and Fig. 12) for different 
numbers of clusters to know the maximum average. So, we start with k = 2 to k  = 
10 because we used up to 10 clusters in the above elbow graph as well and record 
the average values. 
Fig. 11 For k = 2 clusters, the average silhouette plot is shown 
Fig. 12 For k = 10 clusters, the average silhouette plot is shown

Customer Segmentation Analysis Using Clustering Algorithms
365
Fig. 13. Optimal clusters graph using Average Silhouette method 
Here, the averages we got are 0. 29, 0.38, 0.41, 0.44, 0.45, 0.44, 0.43, 0.42, 0.38 
respectively from k = 2 to  = 10. 
Now, by using them we are going to visualize the optimal clusters, so, for that on 
the x-axis we are taking the number of clusters(k) and on the y-axis we are taking 
average silhouette width. The resultant graph is illustrated in Fig. 13. 
From the above graph (as show in Fig. 13), we observe that the k = 6 is seeming 
to be the correct value for the number of clusters, because it is having the highest 
average silhouette width. 
5.3 
Gap Statistic Method 
In this section we are going to display the results for the gap static method. For 
plotting, we take the number of clusters on the x-axis and the gap static(k) on the y 
–axis. Then we get the following graph as show in Fig. 14. 
From Fig. 14, we can conclude that there are 6 optimal clusters. The trial datasets 
are genrated using by Monte Carlo simulations.
Fig. 14 Optimal clusters graph using Gap Static method 

366
B. S. V. Reddy et al.
5.4 
Visualization and Analysis of the Clustering Results 
From the above results we know that the optimal clusters are 6. So, we are going to 
plot the 6 clusters in a scatter plot using the “gg-plot” package in RStudio. So, we are 
taking annual income on x-axis and spending score on y-axis (as show in Fig. 15). 
To draw segmentations of mall customers using K-Means clustering, we are taking 
annual income on x-axis and age on y-axis to draw segmentations of mall customers 
(as show in Fig. 16). Then ﬁnally we are taking classes on x-axis and k-means on 
y-axis to show ﬁnal K-Means clusters, which is visualized in the form of a scatter 
plot as show in Fig. 17. 
From Fig. 15, we observe that:
1) Cluster 1 - Customers with a high annual income and a high annual expenditure 
make up this cluster. 
2) Cluster 2 - This cluster is characterized by a high annual income and a low annual 
outlay. 
3) Cluster 3 - Customers with a low yearly income and a low annual income spend 
are represented in this cluster.
Fig. 15. 6-optimal clusters using spending score and annual income 
Fig. 16. 6-optimal clusters using age and spending score

Customer Segmentation Analysis Using Clustering Algorithms
367
Fig. 17 Final 6-optimal clusters using k-means and classes
4) Cluster 5 - People in this cluster have a low annual income but a high annual 
spending. 
5) Customers in Clusters 6 and 4 have a medium annual salary spend as well as a 
medium annual salary income. 
Further, in Fig. 16, we see that the data is scattered randomly all over the scatter 
plot. Strong clustering is therefore difﬁcult to achieve as the points are not close 
together. We can also conclude that there are several people coming to shop at malls 
having different ages (both young and old) with varying economic status. 
From Fig. 17, we can analyze the clusters in terms of Principal Component Anal-
ysis (PCA) score. PCA score is of two types: PCA1 and PCA2. PCA1 is the linear 
combination with the largest possible explained variation, and PCA2 is the best of 
what’s left. 
1) Customers with a medium PCA1 and medium PCA2 score make up Clusters 4 
and 
2) Clients in Cluster 2 have a high PCA2 but a low yearly income spending. 
3) Cluster 3 is made up of customers with a high PCA1 and PCA2 income. 
4) Customers in Cluster 5 have a medium PCA1 score but a bad PCA2 score. 
5) Cluster 6 - This cluster represents customers with a high PCA2 and a low PCA1. 
6 
Conclusion and Future Work 
In this research work, we were successfully able to perform a customer segmentation 
on a group of 200 customers. We gave the visualization ﬂavor to our research work 
by incorporating colorful and insightful data visualization graphs. They helped us 
gaining insights into the dataset at a glimpse. Further, the principal approach used for 
the segmentation of the mall customers was the K-Means clustering. Clusters were 
visualized in the form of scatter plots and thus, we were able to group the customers 
into six groups based on certain attributes used in the dataset. To improve the clus-
tering process, we also ingeniously implemented a few optimization techniques for 
achieving better results.

368
B. S. V. Reddy et al.
For future work, we can work upon a more complex dataset consisting of thousands 
of customers. Further, the dataset to be worked upon can also include several other 
realistic attributes like customer survey data and customer staying time in malls. 
References 
1. Kashwan KR (2013) Customer segmentation using clustering and data mining techniques 
article in. Int J Comput Theory Eng. https://doi.org/10.7763/IJCTE.2013.V5.811 
2. Li Y, Chu X, Tian D, Feng J, Mu W (2021) Customer segmentation using K-Means clustering 
and the adaptive particle swarm optimization algorithm. Appl Soft Comput 113:107924. https:// 
doi.org/10.1016/J.ASOC.2021.107924 
3. Kansal T, Bahuguna S, Singh V, Choudhury T (2018) Customer segmentation using K-means 
clustering. In: Proceedings of the ınternational conference on computational techniques, elec-
tronics and mechanical systems, CTEMS 2018, pp 135–139. https://doi.org/10.1109/CTEMS. 
2018.8769171 
4. https://www.kaggle.com/datasets/shwetabh123/mall-customers 
5. Syakur MA, Khotimah BK, Rochman EMS, Satoto BD (2018) Integration K-means clus-
tering method and elbow method for ıdentiﬁcation of the best customer proﬁle cluster. In: IOP 
conference series: materials science and engineering, vol 336, no 1. https://doi.org/10.1088/ 
1757-899X/336/1/012017 
6. Ezenkwu C, Ozuomba S, Kalu C (2021) Application of K-Means algorithm for efﬁcient 
customer segmentation: a strategy for targeted customer services. Accessed 21 Nov 2021 
7. Kashwan KR, Velu CM (2013) Customer segmentation using clustering and data mining 
techniques. Int J Comput Theory Eng 856–861. https://doi.org/10.7763/IJCTE.2013.V5.811. 
8. Aryuni M, Madyatmadja ED, Miranda E (2018) Customer segmentation in XYZ bank using 
K-means and K-medoids clustering. ieeexplore.ieee.org. https://doi.org/10.1109/ICIMTech. 
2018.8528086 
9. Datta D, Agarwal R, David PE (2020) Performance enhancement of customer segmentation 
using a distributed python framework, ray. papers.ssrn.com. https://papers.ssrn.com/sol3/pap 
ers.cfm?abstract_id=3733832. Accessed 21 Nov 2021 
10. Rizki B, Ginasta NG, Tamrin MA, Rahman A (2020) Customer loyality segmentation on point 
of sale system using recency-frequency-monetary (RFM) and K-Means 5(2):130–136. https:// 
doi.org/10.15575/join.v5i2.511. join.if.uinsgd.ac.id 
11. Song X, Liu MT, Liu Q, Niu B (2021) Hydrological cycling optimization-based multiobjective 
feature-selection method for customer segmentation. Int J Intell Syst 36(5):2347–2366. https:// 
doi.org/10.1002/INT.22381 
12. Wu S, Yau WC, Ong TS, Chong SC (2021) Integrated churn prediction and customer segmenta-
tion framework for telco business. IEEE Access 9:62118–62136. https://doi.org/10.1109/ACC 
ESS.2021.3073776 
13. Kuruba Manjunath YS, Kashef RF (2021) Distributed clustering using multi-tier hierarchical 
overlay super-peer peer-to-peer network architecture for efﬁcient customer segmentation. 
Electron Commer Res Appl 47. https://doi.org/10.1016/J.ELERAP.2021.101040 
14. López JJ, Aguado JA, Martín F, Muñoz F, Rodríguez A, Ruiz JE (2011) Hopﬁeld–K-Means 
clustering algorithm: a proposal for the segmentation of electricity customers. Electric Power 
Syst Res 81(2):716–724. https://doi.org/10.1016/J.EPSR.2010.10.036 
15. Maree C, Omlin CW (2021) Clustering in recurrent neural networks for micro-segmentation 
using spending personality. Accessed 21 Nov 2021

SP: Shell-Based Perturbation Approach 
to Localize Principal Eigen Vector 
of a Network Adjacency Matrix 
Baishnobi Dash and Debasis Mohapatra 
Abstract Recently, research in the spread of information is found to be a crucial 
domain in the ﬁeld of social network analysis. Understanding information spread-
ability and controllability are the two aspects of the same study. One of the important 
network parameters, the Inverse Participation Ratio (IPR) of a network adjacency 
matrix can measure the state of information localization. Higher the value of IPR, 
the higher the state of localization. This paper proposes a new perturbation approach 
based on k-shell decomposition to meet the optimal IPR. The proposed Shell-based 
Perturbation (SP) approach is compared with one of the state-of-the-art approaches: 
Random Perturbation (RP). The result conﬁrms the superior performance of the 
proposed SP approach over the existing RP approach. 
Keywords Principal eigen vector · k-shell decomposition · Localization ·
Perturbation 
1 
Introduction 
A network represents the connection patterns among the components of a system. 
Hence, it helps in understanding the system-level behaviors through the network 
properties. Recently, network analysis [1, 2, 11, 12, 16, 17] has been adopted in 
most ﬁelds for understanding/analyzing various events taking place in large systems 
like social systems, biological systems, mechanical systems, etc. The concepts of 
network science have been useful in solving different multidisciplinary problems. 
One such problem is the modeling of the spread and controllability of information in a 
connected social network. For example, this study is helpful in controlling an already 
detected rumor in its early phase of propagation. In some recent research publications, 
it is found that the localization state of the spread of information can be achieved 
through modiﬁcation of spectral parameter called Inverse Participation Ratio (IPR).
B. Dash · D. Mohapatra envelope symbol
Department of Computer Science and Engineering, Parala Maharaja Engineering College, 
Berhampur 761003, Odisha, India 
e-mail: debasis.cse@pmec.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_32 
369

370
B. Dash and D. Mohapatra
The IPR is computed from the Principal Eigen Vector (PEV) of the network adjacency 
matrix. Higher the value of IPR, the higher the factor of localization in a network. 
In a localized network, to make the information reach almost all nodes, the rate of 
diffusion needs to be increased. However, in a general setting, the rate of diffusion is 
moderate. Therefore, the information localizes in some regions of the network. Here, 
the main challenge is to ﬁnd out the network conﬁguration that conveys maximum 
IPR with limited modiﬁcations in the network. This problem is shown to be an NP-
hard problem [7, 10]. Hence, in such a scenario polynomial time approximation 
for this problem is essential. In literature, several heuristic approaches like Random 
Perturbation (RP) [7, 10], Jaccard-guided Perturbation (JP) [8], etc. are used for this 
purpose. Towards this, a new heuristic approach based on the k-shell decomposition 
structure [4, 13] of the network is proposed in this paper. The proposed work, in each 
step of perturbation, deletes an edge between a core vertex and a non-core vertex and 
inserts a new edge between two core vertices. 
The important contributions of this work are: 
1. A Shell-based Perturbation (SP) approach is proposed that at ﬁrst, computes the 
k-shell decomposition and then applies perturbation to it to maximize the IPR 
value. 
2. An experimental setup is established to compare the proposed SP approach with 
the existing Random Perturbation (RP) approach. 
The results show that the proposed SP approach dominates RP based on IPR value 
while the number of modiﬁcations is ﬁxed to a certain percentage. The rest of the 
paper is organized as follows. Section 2 covers the related works. Basic concepts 
with problem statement is discussed in Sect. 3. Section 4 explains the methodology. 
Results and discussion are depicted in Sect. 5. And Sect. 6 concludes the paper. 
2 
Related Works 
In this information age, social network platforms are prevalent for the dissemination 
of information. The reachability of information to the netizens within a fraction of a 
second is possible due to the underneath interconnection structure. This interconnec-
tion structure of the network can be represented as a graph data structure where the 
nodes are the social entities, and the edges are the binary connections among these 
entities. The interconnection pattern may be represented by various types of graphs 
like simple unweighted, simple weighted, directed, hypergraph, etc. Maximization of 
inﬂuence in the network is a well-known problem in this area that focuses on ﬁnding 
out k targets who play the role of initial inﬂuencers and cause the information to 
cascade faster and maximum in the network [5, 6]. Several methods of inﬂuence 
maximization have been proposed that adopt various strategies of k-seed selection. 
Contrasting to this, it is also important to understand and track the direction of infor-
mation ﬂow in its early phase when the piece of information is a rumor. Following 
this, the rumor needs to be localized quickly before it passes to subsequent users.

SP: Shell-Based Perturbation Approach to Localize Principal Eigen Vector …
371
Several methods of information localization are available in the literature among 
which spectral localization tries to understand the localization property of a network 
through the PEV of the network adjacency matrix [10, 11]. 
The spectral approach of network localization is explored by Goltsev et al. [9]. 
Their contribution shows that IPR value can depict the state of localization and 
delocalization of a network. Also, it is found that the localization is centered around 
the hub vertices and weighted edges. Pradhan et al. [3, 7, 10] proposed the RP-based 
rewiring of networks to achieve PEV localization. Likewise, it is shown that in a 
localized network, the eigenvector centrality is futile to rank the nodes as in such a 
scenario, only a few of the nodes are assigned with high values whereas other nodes 
are assigned a value closer to zero. Several approximation approaches based on the 
Jaccard coefﬁcient, Betweenness centrality, etc. are also proposed to achieve PEV 
localization. 
This paper proposes a new heuristic approach to achieve a better state of PEV 
localization. It uses the concept of core-periphery structure to achieve the same. For 
this, k-shell decomposition is used to generate the core-periphery structure on top of 
which the proposed SP approach is executed. 
3 
Basic Concepts and Problem Statement 
3.1 
Basic Concepts 
In this section, we discuss the basic concepts used in this paper. These concepts are 
used subsequently to deﬁne the problem statement. 
Deﬁnition 1 (Graph G): A graph G = (V, E) is the collection of a set of vertices V 
and a set of edges E. Graph G considered here is simple, undirected, and unweighted. 
Deﬁnition 2 (Adjacency matrix of G): The adjacency matrix A of graph G is a binary 
symmetric square matrix with n rows and n columns. Aij is the element present in 
the ith row and jth column of A that is represented by Eq. (1). 
up p e r
 A Su b
s
crip t  i
 
j Ba
se
line equals
 StartLayout Enlarged left brace 1st Row 1st Column 1 comma 2nd Column i f left parenthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis element of upper E 2nd Row 1st Column 0 comma 2nd Column o t h e r w i s e EndLayout
where
l
eft p ar
enthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis
is an unordered pair of vertices vi and vj. 
Deﬁnition 3 (Principal Eigen Vector (PEV) and Inverse Participation Ratio (IPR) of  
A): As A is a nonnegative matrix it has n eigenvalues F(1), F(2), ………. , F(n) all  
are nonnegative and are taken in nonincreasing order. The largest eigenvector E(1) 
is the eigenvector corresponding to F(1). E(1) = (χ(1), χ (2), ………, χ (n))T, the  
IPR can be deﬁned as Eq. (2).

372
B. Dash and D. Mohapatra
u p per I u
ppe
r P upper R 
left parenthesis upper A right parenthesis equals sigma summation Underscript i equals 1 Overscript n Endscripts chi left parenthesis i right parenthesis Superscript 4
3.2 
Problem Statement 
The objective is to perform some minor modiﬁcations in the original graph such that 
the resultant conﬁguration generates optimal IPR. Finding out the optimal conﬁgu-
ration of network that generates optimal IPR is O(N2M) where N is the number of 
nodes and M is the number of edges, it is an NP-hard problem. Hence, we consider 
a heuristic-based approach to achieve optimality. Mathematically, this idea can be 
modeled as a constraint-based optimization problem as shown in Eq. (3). 
StartLayo u t  1st R
ow 1s
t Col
umn Bl
ank 2nd Column Ma
xi miz
e upper I upper P upper R left parenthesis upper A right parenthesis 2nd Row 1st Column Blank 2nd Column s period t period 3rd Row 1st Column Blank 2nd Column StartAbsoluteValue upper E EndAbsoluteValue equals StartAbsoluteValue upper E Superscript asterisk Baseline EndAbsoluteValue 4th Row 1st Column Blank 2nd Column StartAbsoluteValue upper E minus upper E Superscript asterisk Baseline StartAbsoluteValue plus EndAbsoluteValue upper E Superscript asterisk Baseline minus upper E EndAbsoluteValue equals t EndLayout
where the objective is to maximize the IPR value of the network adjacency matrix A, 
E is the set of edges present in the original graph G, upper E Superscript asteriskis the set of edges in optimal 
graph conﬁguration G*, and t is the number of modiﬁcations. The ﬁrst constraint 
signiﬁes the number of edges before and after all perturbations remain the same. The 
second constraint says that the number of edge deletions and additions is limited to 
a predeﬁned value t. 
4 
Methodology 
4.1 
Overall Workﬂow 
The overall working procedure is represented abstractly in Fig. 1. At ﬁrst, the original 
graph G is given as input, then the initial IPR value is computed, following this both 
RP (existing approach) and SP (proposed approach) are implemented where both 
of them adopt different strategies to ﬁnd out optimal IPR i.e., IPR*. Then both the 
optimal values are compared.

SP: Shell-Based Perturbation Approach to Localize Principal Eigen Vector …
373
Fig. 1 Overall proposal ﬂow 
4.2 
Existing Random Perturbation (RP) Approach and Scope 
of Improvement 
The existing RP approach, in each iteration, selects an edge e at random from the 
edges of the original graph G and deletes it, followed by this it selects an edge 
e1 at random from the edges do not present in G and inserts it into G. If these 
modiﬁcations enhance the IPR value, then the modiﬁcations are reﬂected otherwise 
the modiﬁcations are not considered. This step continues to the subsequent iterations 
until a predeﬁned number of modiﬁcations t is reached.

374
B. Dash and D. Mohapatra
It is well known from the information diffusion modeling that clusters play an 
important role in information localization. Based on this idea, a shell-based pertur-
bation can be used that tries to form clusters at some parts of the networks. Hence, 
by applying this strategy a better information localization state can be achieved. 
4.3 
Proposed Shell-Based Perturbation (SP) Approach 
In this section, the Shell-based Perturbation (SP) Approach is discussed in detail. 
The transformation of the given social graph (G) to a localized social graph (G*) is  
explained. Here, the proposed work adopts a k-shell decomposition-based method. 
Intially, the IPR of the given social graph (G) is calculated i.e., IIPR (Intial Inverse 
Participation Ratio). Then the perturbation takes place by deleting an edge present 
among non-core nodes followed by the insertion of an edge between non-adjacent 
core nodes. These changes are made to graph G if it enhances the IPR otherwise 
the changes are not made. This process of perturbation continues to the subsequent 
phases if the IPR increases. This procedure continues for a predeﬁned number of 
perturbations (t). 
The steps of the SP approach are executed as follows: 
1. Compute initial IPR value of the given Social graph G say IIPR 
2. Compute the k-shell decomposition of the given Social graph G = (V, E) using 
the following steps: 
i. Make a copy of graph G say G1 = G 
ii. Find the nodes of G1 with the lowest degree d and put them in the bucket d, 
remove them from G1 
iii. Find all neighbors of the nodes found in 2. (ii) and put them in bucket d if 
their degree reduces to ≤ d 
iv. 
Continue steps 2.(ii)–2.(iii) until G1 is empty 
3. From the decomposition structure of Step-2, divide the set V of the graph G into 
two groups i.e. X ⊆ V as the core group and Y = V- X as the non-core group 
4. Randomly select an edge e1 from E1 where E1 is the set of edges among the 
non-core vertices and E1 ⊆ E. Delete e1 from G 
5. Randomly select two non-adjacent core vertices a, and b. Insert an edge e2 (a, b) 
in G 
6. Calculate the IPR of modiﬁed G say the value is MIPR (Modiﬁed IPR) 
7. (a) If MIPR is less than or equal to the last IPR then undo steps 4 and 5 
(b) else continue to the next phase with modiﬁcations 
8. Continue steps 4–7 until t number of modiﬁcations is reached

SP: Shell-Based Perturbation Approach to Localize Principal Eigen Vector …
375
4.4 
Time Complexity Analysis of the SP Approach 
This section describes the time complexity of the SP approach discussed in the 
previous section. The time taken by step-1 is O(n3) [14]. As the computation of the 
IPR value depends on the computation of PEV, here power method is used for the 
computation of PEV. It takes O(n3) time [14]. Step 2 takes O(n) time [15]. Step 3 
takes O(n) time. Step 4 takes a time of O(m). Step 5 takes a time of O(n). Step 6 
takes O(n3) time. Step 7 takes a constant amount of time i.e., O(1). Here, steps 4–7 
are executed t number of times. Hence, the time complexity of steps 4–7 is t.(O(m) 
+ O(n) + O(n3) + O(1) ) ≈ O(t.n3). The overall time complexity of the SP approach 
is O(n3) + O(n) + O(n) + O(t.n3) ≈ O(t.n3). 
5 
Results and Discussion 
The experiments are executed on a system with 2.70 GHz Intel(R) Core (TM) i5-
7200U CPU @ 2.50 GHz processor and 8.0 GB of RAM with Microsoft windows 
10 operating system. The algorithms are implemented using Python 3.10.4 where 
networkx package is used for graph data analysis. 
In this experimental evaluation, some datasets are created with the help of network 
generator models, and some are inbuild in the networkx framework. In total, eight 
graph datasets are considered among which two datasets are generated by using 
Barabási–Albert (BA) model. It uses a preferential attachment mechanism for gener-
ating a scale-free network. Two datasets are generated with the help of Erdos–Renyi 
(ER) Model. This model is used for generating random networks. Two datasets are 
generated through Power-law Cluster (PC) graph model, one dataset is generated by 
using Connected Watts–Strogatz small-world (WS) graph model, and the last one is 
Zachary’s Karate Club graph dataset. The detail about the datasets is presented in 
Table 1. 
Table 1 Details of the datasets used in the experiment 
Dataset
N 
No. of vertices 
M 
No. of edges 
Description 
BA-1
20
75
Generated using BA generative model 
BA-2
15
54
Generated using BA generative model 
ER-1
13
34
Generated using ER generative model 
ER-2
18
72
Generated using ER generative model 
PC-1
20
50
Generated using PC generative model 
PC-2
16
48
Generated using PC generative model 
WS-1
15
30
Generated using WS generative model 
Zachary’s Karate Club 
34
71
Real dataset

376
B. Dash and D. Mohapatra
As both the approaches: RP and SP are based on some random choices, they 
generate different outputs over different runs. The algorithm used here are Monte-
Carlo randomized algorithms [7, 8]. Hence, we executed these two approaches for 
100 times and compare the average optimal IPR generated by them. Here, three 
different edge modiﬁcations percentages are considered i.e., 10%, 15%, and 20%. 
The Avg. Optimal IPR value i.e., IPR* is shown for both RP and SP. Figures 2, 3, 
and 4 show that the Avg. Optimal IPR reported by SP dominates the same reported 
by RP. Subsequently, Figs. 5, 6, and 7 show that among all 100 runs, SP reports the 
maximum IPR* most of the time which is the dominance of occurrences of SP on 
RP. 
Fig. 2 Optimal Avg. IPR achieved by RP and SP under 10% modiﬁcations 
Fig. 3 Optimal Avg. IPR achieved by RP and SP under 15% modiﬁcations

SP: Shell-Based Perturbation Approach to Localize Principal Eigen Vector …
377
Fig. 4 Optimal Avg. IPR achieved by RP and SP under 20% modiﬁcations 
Fig. 5 No. of dominance occurrences by RP and SP over 100 runs under 10% modiﬁcations 
Fig. 6 No. of dominance occurrences by RP and SP over 100 runs under 15% modiﬁcations

378
B. Dash and D. Mohapatra
Fig. 7 No. of dominance occurrences by RP and SP over 100 runs under 20% modiﬁcations 
6 
Conclusions and Future Scope 
This paper discusses a new approach of perturbation called Shell-based Perturbation 
(SP). The edges are selected for perturbation by considering the k-shell decompo-
sition of the nodes of the graph. The comparison of the proposed SP approach is 
made with the existing RP approach. The results show that in all datasets the Avg. 
Optimal IPR reported by SP dominates RP. Also, the dominance of occurrences is 
reported more in the case of SP. In the future, metaheuristics methods can be applied 
to achieve an improvement over the result obtained by the SP approach. 
Acknowledgements We acknowledge the OSHEC, Odisha, India for providing ﬁnancial support 
under the Odisha University Research and Innovation Incentivization Plan (OURIIP) with Grant 
Number 21SF/CS/2019. 
References 
1. Dwivedi SK, Sarkar C, Jalan S (2015) Optimization of synchronizability in multiplex networks. 
EPL (Europhys Lett) 111(1):10005 
2. Dwivedi SK, Jalan S (2014) Emergence of clustering: role of inhibition. Phys Rev E 
90(3):032803 
3. Pradhan P, Yadav A, Dwivedi SK, Jalan S (2017) Optimized evolution of networks for principal 
eigenvector localization. Phys Rev E 96(2):022312 
4. Kitsak M, Gallos LK, Havlin S, Liljeros F, Muchnik L, Stanley HE, Makse HA (2010) 
Identiﬁcation of inﬂuential spreaders in complex networks. Nat Phys 6(11):888–893 
5. Newman MEJ (2010) Networks: an introduction. Oxford University Press, New York 
6. Kempe D, Kleinberg J, Tardos É (2003) Maximizing the spread of inﬂuence through a social 
network. In: Proceedings of the ninth ACM SIGKDD international conference on Knowledge 
discovery and data mining, pp 137–146 
7. Pradhan P, Jalan S (2020) From spectra to localized networks: a reverse engineering approach. 
IEEE Trans Netw Sci Eng 7(4):3008–3017

SP: Shell-Based Perturbation Approach to Localize Principal Eigen Vector …
379
8. Mohapatra D (2021) Jaccard guided perturbation for eigenvector localization. N Gener Comput 
39(1):159–179 
9. Goltsev AV, Dorogovtsev SN, Oliveira JG, Mendes JF (2012) Localization and spreading of 
diseases in complex networks. Phys Rev Lett 109(12):128702 
10. Pradhan P, Angeliya CU, Jalan S (2020) Principal eigenvector localization and centrality in 
networks: revisited. Physica A 554:124169 
11. Forrow A, Woodhouse FG, Dunkel J (2018) Functional control of network dynamics using 
designed Laplacian spectra. Phys Rev X 8(4):041043 
12. Rings T, Bröhl T, Lehnertz K (2022) Network structure from a characterization of interactions 
in complex systems. Sci Rep 12(1):1–19 
13. Carmi S, Havlin S, Kirkpatrick S, Shavitt Y, Shir E (2007) A model of Internet topology using 
k-shell decomposition. Proc Natl Acad Sci 104(27):11150–11154 
14. Elsner L, Van den Driessche P (2001) Modifying the power method in max algebra. Linear 
Algebra Appl 332:3–13 
15. Wang Z, Zhao Y, Xi J, Du C (2016) Fast ranking inﬂuential nodes in complex networks using 
a k-shell iteration factor. Physica A 461:171–181 
16. Lehnertz K (2023) Ordinal methods for a characterization of evolving functional brain 
networks. Chaos Interdisc J Nonlinear Sci 33(2):022101 
17. Briggs JK, Kravets V, Dwulet JM, Albers DJ, Benninger RK (2022) Beta-cell metabolic activity 
rather than gap junction structure dictates subpopulations in the islet functional network. 
bioRxiv, 2022-02

Development of a Robust Dataset 
for Printed Tamil Character Recognition 
M. Arun 
, S. Arivazhagan, and R. Ahila Priyadharshini 
Abstract Despite the fact that many character datasets for several languages are 
publicly available, there are only a very few standardized datasets for Tamil char-
acters. This article presents a subset of the Mepco Tamil Character database, a 
Tamil font isolated character dataset representing the printed characters. This dataset 
includes 124 glyphs representing the 247 characters of Tamil language. This dataset 
is tested for its robustness using multiple experimentations using SVM classiﬁer 
and is compared against UJTDchar, another dataset available for Tamil language. 
Also we have veriﬁed the robustness of DIGI-Net, CNN architecture for this Tamil 
character recognition problem using the UJTDchar dataset and the Mepco Tamil 
Character dataset. We report an accuracy of 90.59% and 97.66% while using SVM 
and DIGI-Net CNN on our newly created dataset. 
Keywords Tamil · Printed · Characters · HOG · CNN · Recognition · Mepco 
1 
Introduction 
Character recognition systems have become useful in a variety of applications, 
including, identifying vehicle registration numbers from number plate images [1], 
converting printed academic records into text for storage in an electronic dataset, 
decoding ancient scripture, automatic data entry by scanning cards, checks, applica-
tion forms, and so on, making electronic images of printed documents searchable, 
and extracting information. Character recognition research is being conducted in a 
variety of languages, including English [2], Arabic [3], Chinese [4] and Tamil [5, 6], 
etc.… Intensive research has been conducted on optical character recognition (OCR), 
but the majority of these systems support Roman, Chinese, Japanese, and Arabic 
characters. Despite the fact that the Indian subcontinent has more languages, there 
are not enough studies on character recognition in Indian languages [7]. Majority of
M. Arun envelope symbol · S. Arivazhagan · R. A. Priyadharshini 
Centre for Image Processing and Pattern Recognition, Mepco Schlenk Engineering College, 
Sivakasi, Tamil Nadu, India 
e-mail: arun@mepcoeng.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_33 
381

382
M. Arun et al.
these Indian languages are based on the Abugida script, which is a segmental writing 
system in which consonant–vowel sequences are written as units; each unit is based 
on a consonant letter, and vowel notation is secondary. 
The ﬁrst generation of OCRs for Indian languages (1970s to 2000) employ intu-
itive features such as shape and water reservoir structures, as well as template 
matching for character matching [8, 9]. The second generation of OCR for Indian 
languages (from 2000 to 2012) employed different strategies than the ﬁrst. Aparna 
and Ramakrishnan [10] used geometric moments and DCT coefﬁcients in their bilin-
gual text recognition system for Tamil and English to classify sub-word symbols. The 
nearest neighbor classiﬁer uses Euclidean distance as the distance measure. Ashwin 
and Sastry [11] captured the shapes of the Kannada characters by extracting structural 
elements that describe the distribution of foreground pixels in the radial and angular 
directions for recognising Kannada script. To deal with Kannada OCR, Vijay Kumar 
and Ramakrishnan [12] used the coefﬁcients extracted from the Karhunen–Loeve 
Transform and the DCT Discrete Wavelet Transform. Sanjeev Kunte and Sudhaker 
Samuel [13] developed a Kannada OCR system to recognise basic characters (vowels 
and consonants) by extracting the features using Zernike moments and Hu’s invariant 
moments, and by using RBF neural network as the classiﬁer. Rasagna et al. devel-
oped a multi-font Telugu OCR using HOG features and an SVM classiﬁer, with 
1,453,950 Telugu character samples from 359 classes and 15 fonts used in their tests 
[14]. Tamil OCR systems used Daubechies Wavelet Transform [15], zoning and chain 
code methods [16]. Raj and Abirami used a strip tree-based hierarchical arrangement, 
the Z-ordering algorithm, and the PM-Quadtree to recognize handwritten Tamil char-
acters [17]. The same researchers have also proposed a Junction Point Elimination 
(JPE) scheme for Tamil character recognition [5]. Recently, Raj et al. have designed a 
Tamil character recognition system by dividing the character’s image into nine equal 
zones and by extracting the structural features [18]. The third generation of OCRs 
use deep learning framework for character recognition or segmentation free methods 
for the recognition process. Many research papers recently use CNNs to solve the 
problem of Indian script character recognition. Researchers have used VGGNet to 
recognize Tamil handwritten text [19], transfer learning [20] and a custom designed 
CNN architecture, DevNet [21] for recognizing the handwritten Devanagari charac-
ters. Kavitha and Srimathi have developed a CNN model from scratch for recognizing 
Tamil characters [22]. 
Even though researches are found in the literature for OCR of Indian script, 
the commercialization of OCR is not so popular for these languages because of 
the shortage of computing power earlier and due to the lack of dataset for Indian 
languages now. Speciﬁcally for the Tamil language, it is challenging to design an 
OCR system because of their complexity, cursive nature, and comparable character 
structures in its script. For instance, other than the second character’s tail shape, 
the characters “
”, “
”, and “
”, “
” are similar. Most of the research works

Development of a Robust Dataset for Printed Tamil Character Recognition
383
mentioned in the literature uses their own custom dataset or the Ofﬂine Isolated 
Handwritten Tamil Character Dataset collected by HP Labs India used in Interna-
tional Workshop on Frontiers in Handwriting Recognition. As per our knowledge for 
Tamil character recognition, only two publicly available databases are available (1) 
HPL Tamil isolated character database (http://lipitk.sourceforge.net/datasets/tamilc 
hardata.htm) and (2) UJTDchar database [23]. Other datasets used by the researchers 
are their own private database. This study focuses on printed character recognition 
of Tamil language. Instead of inventing new methods for character recognition, this 
work focuses on introducing a new curated dataset for tackling the printed character 
recognition of Tamil and testing the dataset’s robustness against the other printed 
text database UJTDchar by using the same feature extraction method and classiﬁer 
on both the datasets. Also this work focuses on applying a well-established deep 
learning framework DIGI-Net [24] for the problem of Tamil printed character recog-
nition. Moreover, the database created for this research work is made available freely 
for researchers to carry out their research works on Tamil character recognition. 
2 
Materials and Methods 
2.1 
UJTDchar Dataset 
This dataset is created for the purpose of the creation of Tamil OCR software. This 
dataset is also made available to research communities at http://www.csc.jfn.ac.lk/ 
index.php/dataset/ for creating a better OCR for Tamil. Printed Tamil characters 
scanned from antique books, periodicals, newspapers, and pamphlets as well as 
normal font characters which are bold, italicized, and bold italic characters make 
this dataset. This dataset has 124 distinct classes with 100 images each. The sample 
characters from the UJTDchar dataset are shown in Fig. 1. 
Fig. 1 Sample images from 
UJTDchar Tamil Character 
dataset

384
M. Arun et al.
Fig. 2 Sample images from 
Mepco Tamil Character 
dataset 
2.2 
Mepco Tamil Character Dataset 
This dataset is having 124 distinct classes ensuring the entire Tamil language is 
covered by these glyphs. We have created 504 samples (126 different Tamil fonts 
with four variations as normal, italic, bold, and bold italic) of the each character 
ensuring enough data available in the database suitable for deep learning models. 
This database totally has 62,496 characters in this dataset spread across 124 
classes. But these computer generated fonts can be easily recognized by any OCR 
system without any issue. So, to make this dataset robust, we have randomly resized 
the images from 10 × 10 pixels to 96 × 96 pixels as in the UJTDchar dataset. The 
sample characters from Mepco Tamil Character dataset are shown in Fig. 2. The  
database can be directly downloaded from https://www.kaggle.com/datasets/pmk 
arun/mepco-tamil-printed-characters-database 
2.3 
Feature Extraction Methods and Classiﬁcation 
Feature extraction methods extract the most important information from the original 
data and put it in a lower dimensionality. In this study, we solely experimented with 
the features of the histogram of oriented gradients (HOG) and employed the Support 
Vector Machine (SVM) as the classiﬁer for ﬁnding the robustness of the newly created 
dataset. Later, we also tested the performance of the DIGI-Net CNN architecture, 
which was initially designed for the recognition of the multi format digits, and later 
used for Tamil natural character recognition also [25]. 
HOG. In computer vision and image processing ﬁelds, HOG features are mostly 
employed for object detection [26]. HOG feature extraction technique counts the 
instances of a gradient orientation in particular regions of an image. It is similar to 
edge orientation histograms, scale-invariant feature transform descriptors, and shape 
contexts, but differs in that it is computed on a dense grid of uniformly spaced cells 
and uses overlapping local contrast normalization for more accuracy. The algorithm 
for the calculation of the HOG is mentioned below.

Development of a Robust Dataset for Printed Tamil Character Recognition
385
SVM Classiﬁer. One of the most popular supervised learning algorithms, Support 
Vector Machine (SVM), is used to handle both classiﬁcation and regression problems 
[27]. SVM algorithm’s objective is to establish the best line or decision boundary 
that can divide n-dimensional space into classes, allowing to quickly classifying 
fresh data points in the future. Kernel SVM, a variation of SVM, has more ﬂexibility 
towards non-linear data as more features can be added to ﬁt a hyper plane instead 
of a two-dimensional space. The equations for the SVM kernel functions used in 
this research work are mentioned in Table 1. In the following equations, upper X comma upper X 1 comma upper X 2
denotes the data points, W, the weight vector to be minimized, b, the linear coefﬁcient 
estimated from the training data, Γ speciﬁes how much a single training point has 
other data points around it. 
Deep Learning. Recent researches rely more on machine-learnt features than hand-
crafted features for a variety of computer vision applications such as segmentation 
[28], image restoration [29], biometric recognition [30], Ayurveda medicinal leaf
Table 1 SVM kernels used 
in Tamil character recognition
SVM Kernel
Equation of decision boundary 
Linear
f left pare nt he sis upper X right parenthesis equals upper W Superscript upper T Baseline upper X plus b
Radial Basis Function
f left parenthes is uppe r X 1 comma upper X Subscript 2 Baseline right parenthesis equals left parenthesis a plus upper X 1 Superscript upper T Baseline upper X 2 right parenthesis Superscript b
Polynomial
f left parenthes is upper X 1 comma upper X Subscript 2 Baseline right parenthesis equals e Superscript minus upper Gamma left parenthesis double vertical bar upper X 1 minus upper X 2 double vertical bar right parenthesis squared

386
M. Arun et al.
Fig. 3 DIGI-Net 
Architecture 
classiﬁcation [31] etc.… A convolutional neural network is made up of convolu-
tional layers that carry out several convolutions simultaneously, nonlinear functions 
like ReLU are applied to these convolutional layers, pooling layers summarize the 
statistics of nearby locations, and the ﬂattened output is connected to a fully connected 
neural network. 
Sparse interactions, parameter sharing, and equivariant representations are all 
attributes of CNN. In this work the DIGI-Net architecture is tested for its robustness 
and its feature extraction capabilities on classifying Tamil printed characters. Some 
minor modiﬁcations such as the input size, output size etc.… is made to the original 
architecture in order to cater the needs of these datasets. The DIGI-Net architecture 
used in this research work is shown in Fig. 3. 
3 
Results and Discussion 
The entire experimentations on UJTDchar dataset and Mepco Tamil character dataset 
are conducted with a train test ratio of 50:50 to test the robustness of the datasets 
[32]. All the character images in both the datasets have been either up or downscaled 
to 32 × 32 and 64 × 64 pixels using bicubic interpolation. Also the features are 
extracted in 4 different ways using the HOG feature descriptor. For extracting HOG 
features the block size and the bins are ﬁxed as 2 × 2 and 9 respectively. The number 
of the extracted HOG features is shown in Table 2. 
These extracted features are classiﬁed using SVM Classiﬁer with 3 different 
kernels namely RBF, polynomial and linear. The sample images representing the 
HOG features are shown in Fig. 4.
The classiﬁcation accuracy and the F1 score for the UJDTchar dataset and Mepco 
Tamil Character dataset on using the above 4 sets of features and 3 different kernels
Table 2 Number of Features 
of HOG using different cell 
size and image dimensions 
Cell Size
Image Dimensions 
64 × 64
32 × 32 
8 × 8
1736
324 
16 × 16
324
36 

Development of a Robust Dataset for Printed Tamil Character Recognition
387
Fig. 4 Sample HOG images of UJTDchar and Mepco Tamil Character datasets
of SVM are tabulated in Tables 3 and 4. From Tables 3, 4, 5 and 6, the accuracy and 
F1 score are mentioned in (%). 
Tables 3 and 4 show that, HOG features extracted on 64 × 64 image using 
8 × 8 as the cell size produce better performance on both these datasets despite being 
computationally expensive. The performance of the HOG features with 324 features 
is also noticeably better compared to 1736 features. The 324 features extracted from 
a 64  × 64 image perform better than the 324 features taken from a 32 × 32 image, 
but both yield comparative results. 
To test the robustness of the newly created database, training is done on one dataset 
and the cross testing is performed on another dataset. The same feature extractions 
and classiﬁers aforementioned are used on both the experimentations. Table 5 shows 
the performance of the Mepco Tamil character dataset on training the UJTDchar 
dataset. And Table 6 shows the performance of the UJTDchar dataset on training the 
Mepco Tamil Character dataset. 
From Table 5 and 6 we can infer that, in most of the experimentation scenarios, the 
features learnt on the Mepco Tamil character dataset is more useful in recognizing
Table 3 Performance measure on UJTDchar dataset 
SVM 
Kernel 
HOG_64_1736
HOG_64_324
HOG_32_324
HOG_32_36 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
RBF
90.40
90.45
89.44
89.49
87.48
87.48
69.34
69.37 
POLY
82.89
83.88
82.32
83.29
79.63
80.88
60.37
62.46 
LINEAR
92.53
92.55
90.85
90.88
88.56
88.56
66.16
66.21 
Table 4 Performance measure on Mepco Tamil Character dataset 
SVM 
Kernel 
HOG_64_1736
HOG_64_324
HOG_32_324
HOG_32_36 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
RBF
91.08
91.1
87.41
87.42
86.41
86.41
65.78
65.74 
POLY
88.87
89.08
84.92
85.14
83.84
84.15
63.09
63.92 
LINEAR
90.59
90.59
84.42
84.39
82.23
82.19
59.99
59.94

388
M. Arun et al.
Table 5 Performance of Mepco dataset on training UJTDchar dataset 
SVM 
Kernel 
HOG_64_1736
HOG_64_324
HOG_32_324
HOG_32_36 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
RBF
63.46
61.18
62.5
61.49
61.80
60.76
44.26
44.17 
POLY
52.2
51.46
51.00
52.76
49.5
51.22
38.53
39.93 
LINEAR
67.2
65.8
63.07
62.6
61.83
61.31
42.18
42.32 
Table 6 Performance of UJTDchar dataset on training Mepco dataset 
SVM 
Kernel 
HOG_64_1736
HOG_64_324
HOG_32_324
HOG_32_36 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
Accuracy
F1 
Score 
RBF
27.53
33.63
69.83
69.86
68.72
68.83
53.60
53.45 
POLY
59.40
59.45
69.28
69.47
68.96
69.35
52.33
52.48 
LINEAR
58.75
58.29
60.98
60.68
60.21
60.10
47.00
46.74
the characters of UJTDchar dataset when compared with the model learning the 
features of UJTDchar and tested on Mepco Tamil character dataset. Moreover it is 
to be noted that the 324 features extracted from 32 × 32 images are comparable to 
the 324 features extracted from 64 × 64 images in all the experimentations. As the 
number of images in the Mepco Tamil Character dataset is comparatively high, the 
features learnt by the model are robust enough to perform good classiﬁcation on the 
UJTDchar dataset. As, the samples in each class is high enough, this dataset is also 
suitable for the deep learning framework. 
To test the robustness of this dataset on using a deep learning framework, 
we planned to use the DIGI-Net CNN architecture. The original DIGI-Net CNN 
considers the input image to be a color image and of size 32 × 32 pixels and the 
output is of 10 classes. We experimented using the DIGI-Net CNN with grayscale 
images and input layer to be of 64 × 64 and 32 × 32. The output is modiﬁed to 124 
catering the needs of our datasets. No other modiﬁcations are done except these minor 
changes. The experimentation is conducted for 50 epochs of training with a learning 
rate of 0.001 as mentioned in the original paper. Furthermore, to check whether the 
DIGI-Net CNN can tackle the problem of printed Tamil character recognition, we 
ﬁrst compared the performance of the DIGI-Net architecture with the state of art 
methods by testing it on the UJTDchar database with a train test ratio of 70:30 and 
the performance is tabulated in Table 7.
Table 7 shows that DIGI-Net CNN is suitable for printed Tamil character recogni-
tion also despite being designed only for classifying digits. Now, to test the robustness 
of Mepco dataset on using a deep learning framework, we have maintained the same 
train and test ratio of 50:50 as the traditional approach and trained and tested both the 
databases using the DIGI-Net CNN architecture. The performance of the DIGI-Net

Development of a Robust Dataset for Printed Tamil Character Recognition
389
Table 7 Comparison of 
experimental results of 
UJTDchar database on 
state-of-art methods 
Method
Accuracy (%) 
DIGI Net 64 × 64
97.66 
DIGI Net 32 × 32
95.77 
SVM [33]
94.87 
UDT [33]
97.07
on UJTDchar and Mepco Tamil character dataset for different input image sizes are 
tabulated in Table 8. 
Table 8 shows that DIGI-Net with 64 × 64 is providing a better performance 
for Tamil character recognition, but the learnable parameters are more (162,348) in 
this case. Even with the input image size as 32 × 32, DIGI-Net gives comparable 
accuracy with 114,732 parameters. It also implies that the 5 levels of convolutional 
layers learn better feature maps for the classiﬁcation. Figure 5 shows the feature maps 
of the ﬁrst two layers of DIGI-Net architecture for both UJTDchar and Mepco dataset. 
From Fig. 5, it is inferred even though the convolutional layers learn differently, the 
core features are captured in many of the channels of the feature map. In general, 
the machine-learnt features of the ﬁrst and second convolutional layers can be seen 
as a combination of the functions of various edge detectors. From the ﬁgure, it is 
also observed that, the ﬁrst three channels of layer 1 in the ﬁrst model and the 6th 
channel of layer 1 in the second model shows no or minimal activations. The other 
layers activations largely preserve all of the information seen in the initial image. 
The deeper feature maps extracted from the convolutional layers 3, 4 and 5 has less 
activations and become less recognizable visually and more abstract. As the human 
visual system observes, it has been discovered that the unnecessary information is 
ﬁltered out and that the useful information is reﬁned in the higher level CNN layers.
Now, to test the robustness of the Mepco character dataset, cross testing is 
performed using the DIGI-Net model and the performance is shown in Table 9.
From Table 9, we can infer that the Mepco dataset features are not enough to repre-
sent the UJTDchar dataset while using deep learning framework unlike traditional 
machine learning model using SVM due to domain shift. The basic presumption of 
any model is that the statistical distribution of the training data and the unseen testing 
data is the same. Due to uncontrollable factors like dataset bias and domain shift, 
this assumption frequently fails to hold in the real world. Similarly, Mepco character 
dataset is created synthetically using computer generated fonts and UJTDchar has 
synthetically created fonts and scanned characters from old magazines, books etc.… 
So, to adopt the domain of UJTDchar, 10% of the UJTDchar dataset is retrained on
Table 8 Accuracy of 
UJTDchar and Mepco Tamil 
character dataset on using 
DIGI-Net 
Input Size
UJTDchar
Mepco 
64 × 64
94.90%
93.64% 
32 × 32
92.08%
92.32% 

390
M. Arun et al.
Fig. 5 Feature maps of the ﬁrst 2 layers of DIGI-Net CNN for sample characters from UJTDchar 
and Mepco dataset
Table 9 Accuracy of Mepco and UJTDchar Tamil character dataset on using DIGI-Net 
Input Size
Train UJTDchar Test Mepco dataset
Train Mepco dataset Test UJTDchar 
64 × 64
77.76%
62.81% 
32 × 32
71.20%
56.54%
Table 10 Accuracy of 
UJTDchar character dataset 
on using DIGI-Net after 
domain adaptation 
Input Size
Domain adopted Model 
64 × 64
80.02% 
32 × 32
80.25% 
the DIGI-Net model after learning the entire Mepco dataset for only 10 iterations. 
Now the performance of this system is veriﬁed and is tabulated in Table 10. 
From Table 10, it is evident that the swift retrain of the model on UJTD dataset 
might have effectively adapted the domain and provides better performance than 
before. The retrained model learns how to recognize characters in UJTDchar dataset 
by transferring the already learnt knowledge of Mepco Character dataset for the 
character recognition problem. 
4 
Conclusion 
In this study, we have introduced a publicly accessible dataset of isolated Tamil 
characters for printed Tamil character recognition. A total of 62,496 samples are 
created and processed and spanned across 124 classes. Multiple experiments based 
on traditional machine learning models and deep CNN models are used to test the 
Mepco dataset’s robustness, and it is compared against UJTDchar. Additionally, we

Development of a Robust Dataset for Printed Tamil Character Recognition
391
have tested the ﬂexibility of the DIGI-Net, CNN architecture for recognizing digits 
to handle the Tamil character recognition problem. The DIGI-Net CNN works well 
on both the datasets and even outperforms the state of art methods for UJTDchar 
dataset. In future, a standardized dataset for Tamil handwritten characters can be 
prepared and the same experiments can be tested on that dataset also. 
References 
1. Zaafouri A, Sayadi M, Wu W (2022) A vehicle license plate detection and recognition method 
using log gabor features and Convolutional Neural Networks. Cybern Syst 1–16. https://doi. 
org/10.1080/01969722.2022.2055400 
2. Arun M, Arivazhagan S (2022) A uniﬁed feature descriptor for generic character recognition 
based on zoning and histogram of gradients. Neural Comput Appl 34(14):12223–12234. https:// 
doi.org/10.1007/s00521-022-07110-x 
3. Altwaijry N, Al-Turaiki I (2020) Arabic handwriting recognition system using convolutional 
neural network. Neural Comput Appl 33(7):2249–2261. https://doi.org/10.1007/s00521-020-
05070-8 
4. Cao Z, Lu J, Cui S, Zhang C (2020) Zero-shot handwritten Chinese character recognition with 
hierarchical decomposition embedding. Pattern Recogn 107:107488. https://doi.org/10.1016/ 
j.patcog.2020.107488 
5. Raj MA, Abirami S (2019) Junction point elimination based Tamil handwritten character 
recognition: An experimental analysis. J Syst Sci Syst Eng 29(1):100–123. https://doi.org/10. 
1007/s11518-019-5436-6 
6. Siromoney G, Chandrasekaran R, Chandrasekaran M (1978) Computer recognition of printed 
Tamil characters. Pattern Recogn 10(4):243–247. https://doi.org/10.1016/0031-3203(78)900 
32-8 
7. Pal U, Chaudhuri BB (2004) Indian script character recognition: a survey. Pattern Recogn 
37(9):1887–1899. https://doi.org/10.1016/j.patcog.2004.02.003 
8. Sinha RMK, Mahabala HN (1979) Machine recognition of Devanagari script. IEEE Trans Syst 
Man Cybern 9(8):435–441. https://doi.org/10.1109/tsmc.1979.4310256 
9. Chaudhuri BB, Pal U (1998) A complete printed Bangla OCR system. Pattern Recogn 
31(5):531–549. https://doi.org/10.1016/s0031-3203(97)00078-2 
10. Aparna KG, Ramakrishnan AG (2002) A complete Tamil optical character recognition system. 
Lecture Notes in Computer Science, pp 53–57. https://doi.org/10.1007/3-540-45869-7_6 
11. Ashwin TV, Sastry PS (2002) A font and size-independent OCR system for printed Kannada 
documents using support Vector Machines. Sadhana 27(1):35–58. https://doi.org/10.1007/bf0 
2703311 
12. Vijay Kumar B, Ramakrishnan AG (2002) Machine recognition of printed Kannada text. 
Lecture Notes in Computer Science, pp 37–48. https://doi.org/10.1007/3-540-45869-7_4 
13. Sanjeev Kunte R, Sudhaker Samuel RD (2007) A simple and efﬁcient optical character recog-
nition system for basic symbols in printed Kannada text. Sadhana 32(5):521–533. https://doi. 
org/10.1007/s12046-007-0039-1 
14. Rasagna V, Jinesh KJ, Jawahar CV (2011) On multifont character classiﬁcation in Telugu. Inf 
Syst Indian Lang 86–91. https://doi.org/10.1007/978-3-642-19403-0_14 
15. MJose T, Wahi A (2013) Recognition of Tamil handwritten characters using daubechies wavelet 
transforms and feed-forward backpropagation network. Int J Comput Appl 64(8):26–29. https:// 
doi.org/10.5120/10656-5422 
16. Shyni SM, Antony Robert Raj M, Abirami S (2015) Ofﬂine Tamil handwritten character recog-
nition using sub line direction and bounding box techniques. Indian J Sci Technol 8(S7):110. 
https://doi.org/10.17485/ijst/2015/v8is7/67780

392
M. Arun et al.
17. Raj MA, Abirami S (2019) Structural representation-based off-line Tamil handwritten character 
recognition. Soft Comput 24(2):1447–1472. https://doi.org/10.1007/s00500-019-03978-5 
18. Antony Robert Raj M,Abirami S, Shyni SM (2023) Tamil handwritten character recognition 
system using statistical algorithmic approaches. Comput Speech Lang 78:101448. https://doi. 
org/10.1016/j.csl.2022.101448 
19. Pragathi MA, Priyadarshini K, Saveetha S, Banu AS, Mohammed Aarif KO (2019) Handwritten 
tamil character recognition using deep learning. In: 2019 international conference on vision 
towards emerging trends in communication and networking (ViTECoN). https://doi.org/10. 
1109/vitecon.2019.8899614 
20. Aneja N, Aneja S (2019) Transfer learning using CNN for handwritten devanagari character 
recognition. In: 2019 1st international conference on advances in information technology 
(ICAIT). https://doi.org/10.1109/icait47043.2019.8987286 
21. Guha R, Das N, Kundu M, Nasipuri M, Santosh KC (2020) DEVNET: an efﬁcient CNN 
Architecture for handwritten devanagari character recognition. Int J Pattern Recognit Artif 
Intell 34(12):2052009. https://doi.org/10.1142/s0218001420520096 
22. Kavitha BR, Srimathi C (2022) Benchmarking on ofﬂine handwritten Tamil character recogni-
tion using convolutional neural networks. J King Saud Univ Comput Inf Sci 34(4):1183–1190. 
https://doi.org/10.1016/j.jksuci.2019.06.004 
23. Ramanan M, Ramanan A, Charles EYA (2015) A preprocessing method for printed Tamil docu-
ments: Skew correction and textual classiﬁcation. In: 2015 IEEE seventh international confer-
ence on intelligent computing and information systems (ICICIS). https://doi.org/10.1109/int 
elcis.2015.7397266 
24. Madakannu A, Selvaraj A (2019) DIGI-Net: a deep convolutional neural network for multi-
format digit recognition. Neural Comput Appl 32(15):11373–11383. https://doi.org/10.1007/ 
s00521-019-04632-9 
25. Arun M, Arivazhagan S, Ahila Priyadharshini R (2022) A large volume natural tamil character 
dataset. Communications in Computer and Information Science, vol 1567. Springer, Cham. 
https://doi.org/10.1007/978-3-031-11346-8_37 
26. Dalal N, Triggs B (2005) Histograms of oriented gradients for human detection. In: 2005 IEEE 
computer society conference on computer vision and pattern recognition (CVPR 2005). https:// 
doi.org/10.1109/cvpr.2005.177 
27. Boser BE, Guyon IM, Vapnik VN (1992) A training algorithm for optimal margin classiﬁers. 
In: Proceedings of the ﬁfth annual workshop on computational learning theory. https://doi.org/ 
10.1145/130385.130401 
28. Roy AG, Conjeti S, Karri SP, Sheet D, Katouzian A, Wachinger C, Navab N (2017) ReLayNet: 
Retinal layer and ﬂuid segmentation of macular optical coherence tomography using fully 
convolutional networks. Biomed Opt Express 8(8):3627. https://doi.org/10.1364/boe.8.003627 
29. Lahiri A, Bairagya S, Bera S, Haldar S, Biswas PK (2021) Lightweight modules for efﬁcient 
deep learning based image restoration. IEEE Trans Circuits Syst Video Technol 31(4):1395– 
1410. https://doi.org/10.1109/tcsvt.2020.3007723 
30. Ahila Priyadharshini R, Arivazhagan S, Arun M (2020) A deep learning approach for person 
identiﬁcation using ear biometrics. Appl Intell 51(4):2161–2172. https://doi.org/10.1007/s10 
489-020-01995-8 
31. Ahila Priyadharshini R, Arivazhagan S, Arun M (2021) Ayurvedic medicinal plants identiﬁ-
cation: a comparative study on feature extraction methods. Communications in Computer and 
Information Science, pp 268–280. https://doi.org/10.1007/978-981-16-1092-9_23 
32. Joseph VR (2022) Optimal ratio for data splitting. Stat Anal Data Min ASA Data Sci J 
15(4):531–538. https://doi.org/10.1002/sam.11583 
33. Shafana MS, Ragel RG, Kumara TN (2021) An effective feature set for enhancing printed 
Tamil character recognition. J Natl Sci Found 49(2):195. https://doi.org/10.4038/jnsfsr.v49i2. 
9466

An Efﬁcient CNN-based Method 
for Classiﬁcation of Red Meat Based 
on its Freshness 
Abhishek Bajpai, Harshvardhan Rai, and Naveen Tiwari 
Abstract Red meat is one of the most popular varieties of meat in the eastern part 
of India. Consumption of prolonged accumulated or spoiled meat results in many 
fatal diseases. Traditional detection methods, such as sensory testing, physical and 
chemical testing, microbiological testing, and instrument analysis, are all complex, 
time-consuming, destructive, and uneconomical. In this study, we have designed an 
efﬁcient, and non-destructive procedure for the classiﬁcation of red meat based on 
its freshness using a novel convolutional neural network algorithm called “HarNet”. 
Our “HarNet” model gives better results and outperforms many pre-trained models 
like VGG16, VGG19, ResNet50, and InceptionV3. After testing and performing 
statistical analysis, our proposed model has achieved an accuracy of 80%. The F1-
score value for the spoiled class is 0.89 and the recall value of 0.96 is the highest 
attained by the HarNet model, followed by the fresh class with 0.78 as the F1-score 
value and the recall value of 0.82 after testing. The results given by our proposed 
model are better than many of state of the art deep learning methods. 
Keywords Red meat · Image processing · HarNet Architecture · VGG16 ·
VGG19 · ResNet50 · InceptionV3 
1 
Introduction 
Red meat is one of the most widely consumed meat varieties in India. The market 
experiences a lot of customer demand as a result of this high level of consumption. 
However, many found sellers sell red meat stored for several days or sometimes 
spoiled. The quality and freshness of the meat are two essential characteristics that 
A. Bajpai (B) · H. Rai · N. Tiwari 
Department of Computer Science and Engineering, Rajkiya Engineering College, Kannauj, Uttar 
Pradesh, India 
e-mail: abhishek@reck.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_34 
393

394
A. Bajpai et al.
consumers search for. A consumer analyses the meat’s quality merely by looking 
at it. This type of examination has drawbacks because many characteristics that 
determine a product’s quality have visual constraints or are undecipherable to the 
naked eye [ 1]. Various types of harmful bacteria, fungi, and viruses like Penicillium, 
Mucor, Cladosporium, Brochothrix thermosphacta, Carnobacterium spp., Enterobac-
teriaceae, Leuconostoc spp., Pseudomonas spp., Shewanella putrefaciens. Alternaria, 
Sporotrichium, etc., grow on stored or spoiled meat, resulting in many fatal diseases 
if consumed [ 2, 3]. Traditional detection methods, such as sensory testing, physical 
and chemical testing, microbiological testing, and instrument analysis, are complex 
and uneconomical [ 4– 6]. These methods are generally implemented in laboratories 
that are hard to understand. The available traditional methods such as sensory, bio-
chemical as well as chemical are costly, destructive, tedious, and time-consuming, 
and also need experience and expert-level knowledge to do the analysis correctly 
[ 7]. In this study, we have developed red meat classiﬁcation based on its freshness 
using a novel and self-made convolutional neural network algorithm, i.e., HarNet that 
is easily implementable. The model developed is effective, rapid, non-destructive, 
and efﬁcient in classifying and detecting red meat freshness. After training, the pro-
posed convolutional neural network model can detect and classify red meat into three 
classes, i.e., fresh, half-fresh, and spoiled. The HarNet model architecture is shown 
in Fig. 6. The developed model is giving better results in comparison with many 
pre-trained models like VGG16, VGG19, ResNet50, InceptionV3, etc. Its goals are 
to detect different types of red meat and classify the red meat into 3 classes,i.e., 
fresh, half-fresh, and spoiled, based on its freshness. In this paper, the meat used for 
preparing the dataset is brought from a local market in India. Meat images that are 
categorized as Fresh are clicked before 4 h. Meat images clicked between 6 h to 10 h 
are categorized as Half-Fresh. Similarly, images clicked after 14 h are categorized 
as Spoiled. We have used a standard ratio used in many papers for the training, val-
idation, and testing split of captured images [ 8]. There is 70% of the total collected 
images in the training dataset, 15% of the total collected images in the validation 
dataset, and 15% of the total collected images in the testing dataset. All images 
collected are cropped with the same standard height and width. By eliminating the 
darker areas of the images, image pre-processing also helps to increase the accu-
racy of the classiﬁcation of images. For the image mask that eliminates superﬂuous 
regions, thresholding and morphological transformation of the image are applied. 
After the training of the “HarNet” model, it is effectively giving the best results on 
the given image dataset which was shufﬂed and divided into three folders, i.e., train-
ing set, validation set, and testing set. The study is only concerned with classifying 
the freshness of red meat, and the samples used are only from the Indian red meat 
sector. To capture the images of red meat that will be examined, we will use a smart-
phone camera. The convolutional neural network’s HarNet architecture was used 
to categorize the red meat images according to freshness. Although the approach 
distinguishes red meat into different freshness categories according to quality, it will 
exclude the identiﬁcation of deﬁciencies and meat diseases.

An Efﬁcient CNN-based Method for Classiﬁcation of Red Meat...
395
2 
Literature Review 
Many research works are going on in the ﬁeld of artiﬁcial intelligence and deep learn-
ing. For the detection of the freshness of fruits, vegetables, ﬁsh, and meat, there was 
the use of different chemical and sensory tests. Isabel M. Perez et al. [ 9] in their work 
discussed meat freshness detection using a custom application on a smartphone. For 
monitoring the freshness of the meat, a freshness colourimetric sensor is integrated 
with the packages of meat which tracks the rising CO2 level in the package as the 
CO2 level increases due to the decaying of meat by bacteria. But, this method can 
be used only for packaged meat and not for meat sold by butcher shops directly. In 
their research paper, Ghanbari et al. [ 4] discussed ﬁsh freshness detection using a 
nonenzymatic voltammetric Xanthine biosensor. In their study work, Fang, Hairui, 
and colleagues [ 5] focused on a NiO-doped CuO composite-based sensor for H2S gas 
detection in meat freshness testing. Next, an integrated circuit detection device was 
developed to detect H2S gas at varying ambient temperatures. This research offers a 
fresh concept for determining meat freshness under cold storage circumstances. But, 
these sensor-based methods are difﬁcult and uneconomical and also need the exper-
tise to be implemented in the real world for all categories of meat sold in the market. 
Yanan Sun et al. [ 6] in their research paper talked about a double-layer indicator ﬁlm 
that uses the pH-sensitive substance anthocyanins to show if packaged meat products 
are still fresh. The ﬁrst layer was made of chitosan, and the second layer was made 
of raspberry anthocyanins. Back Propagation Artiﬁcial Neural Network was used 
to predict freshness based on the colour change of the ﬁlms. Although this paper 
involves an efﬁcient method like a back propagation ANN model for the prediction 
of meat freshness, there is no need to use a deep learning model as the changing 
different colours of the ﬁlms clearly indicate the freshness. Samira et al. [ 10] dis-
cussed an enzyme-based electrochemical biosensor using poly (L-aspartic acid) and 
multi-walled carbon nanotube(MWCNT) bio-noncomposite for xanthine detection 
in ﬁsh meat. Also, Z Liu et al. [ 11] in their research paper explained a cutting-edge 
technique for detecting adenosine triphosphate (ATP) based on S1 nuclease, FAM-
labeled ssDNA, and graphene oxide (GO) for the assessment of the freshness of the 
meat (beef). But, these types of sensors or indicators-based detection are not that 
easy, economical, or fast to be used, and also need the expertise to do the analysis 
correctly. R. Andrie Asmara et al. [ 12] used a method to identify the freshness level 
of chicken meat based on the histogram colour feature. In their research paper, there 
are three classes of chicken meat, i.e., fresh, medium, and old. The support vector 
machine (SVM) based method used on the colour histogram feature indicates the 
highest classiﬁcation accuracy of 58.33%. Ahmed M. Rady et al. [13] in their research 
paper argued on the viability of using machine learning and colour imaging to detect 
adulteration in minced meat. Colour imaging and machine learning techniques were 
found to be useful and important for freshness detection in this paper. A computer 
vision technique for predicting the freshness level of ﬁsh from its images where the 
region of interest was the eyes of ﬁsh, was used by Anamika Banwari, RC Joshi, et al. 
[ 7] in their research paper. They observed the correlation between the colour of the

396
A. Bajpai et al.
eyes and different time periods of storage. These types of change of colour centric 
and imaging can also be used for all types of meat freshness detection. Many papers 
related to meat freshness classiﬁcation discussed using neural networks like con-
volutional neural network (CNN) based models independently for non-destructive 
methodology. Chao Wang et al. [ 14] in their research paper explored a CNN-based 
computer vision system for the freshness assessment of crayﬁsh. Using a twelve 
layers CNN model, they could predict the freshness level of crayﬁsh with 83.3% 
accuracy with optimized networks. Similarly, Dongwei Liu et al. [ 15] a CNN-based 
image detection system for products made from lamb or beef slices but substituted 
with duck. Meat Texture Net (MTx-Net), a lean and incredibly effective CNN archi-
tecture, was created for this. This approach was effective enough to be used on a 
phone. MBP Garcia et al. [ 8] in their research paper discussed chicken meat fresh-
ness classiﬁcation in two classes, i.e., fresh and old, using VGG16 architecture. They 
got an overall accuracy of approximately 94% after testing. Similarly, in the paper 
of Calvin, Putra, et al. [ 16], they used a novel architecture named AyamNet is used 
to classify the boiler meat into two classes based on freshness,i.e., fresh and rotten. 
There are also many different papers on meat freshness detection in different time 
intervals of cutting meat using deep learning methods [ 17, 18]. The CNN-based 
model named HarNet discussed in this paper is also a computer vision image classi-
ﬁcation model used for the classiﬁcation and detection of red meat into three classes, 
i.e., fresh, half-fresh, and spoiled. Features essential for classiﬁcation are extracted 
using different layers like convolutional layers, pooling layers, and dropout layers 
used before the ﬂatten layer, and then dense layers are used in model architecture. 
This model has four convolutional layers, four pooling layers, and four dense layers. 
The detailed architecture of HarNet and its comparison with other pre-trained models 
are discussed in the methodology section of the paper. 
3 
Proposed Methodology 
Figure 1 depicts the paper’s conceptual structure. Images of red meat clicked using 
a smartphone camera serve as the main inputs. The two key procedures are the pre-
processing of images, which involves segmenting and removing some portions of 
the red meat images, and the training of the HarNet model, which makes use of the 
pre-processed red meat images as training and validation datasets. The classiﬁcation 
using the HarNet model, using a testing set of red meat images, determines if the 
image of red meat is fresh, half-fresh, or spoiled in the ﬁnal output. Lastly, we 
compare the testing accuracy of the HarNet model with other pre-trained models 
like VGG16, VGG19, ResNet50, and InceptionV3 using the same dataset.

An Efﬁcient CNN-based Method for Classiﬁcation of Red Meat...
397
Fig. 1 Conceptual Framework 
3.1 
Classiﬁcation Algorithm 
Figure 2 depicts the complete classiﬁcation algorithm and the complete proposed 
process using the ﬂowchart. A smartphone camera is used to capture the image of 
red meat at different timing for three classes, i.e., fresh, half-fresh, and spoiled. After 
the images are captured, they will undergo under image pre-processing process to 
remove superﬂuous portions from the images. Then, the HarNet model is used for 
the classiﬁcation of red meat freshness as shown in Fig. 2. 
Fig. 2 Classiﬁcation 
Algorithm used in this paper

398
A. Bajpai et al.
3.2 
Image Pre-processing 
Image pre-processing is a method to improve the quality of the images and highlight 
the important details present in the images. As the collected images of the dataset 
are real-time captured images using a smartphone for efﬁcient training of the mod-
els, some images are overexposed, and sometimes dark portions are visible due to 
different light conditions because of the different timing of capturing images. Image 
pre-processing aided to remove these types of problems during dataset creation. 
Image segmentation is a technique for partitioning a digital image into smaller con-
nected regions called image segments, which reduces the complexity of the image 
and makes each segment more easily processed or analyzed. In this paper, all the 
real-time pictures or images are collected using a smartphone in natural light con-
ditions. After collecting the images, the thresholding segmentation technique and 
morphological transformation of the images are used for the image mask to isolate 
the unnecessary regions of the images. The image pre-processing algorithm for the 
red meat photos taken by the smartphone camera is depicted in Fig. 3. First of all, 
the thresholding segmentation technique is used which allows for the separation of 
crucial image pixels. The basis for this is the change in colour intensity that can be 
seen in fresh, half-fresh, and spoiled red meats. The OpenCV inRange function is 
utilized in the thresholding technique. Next, the image is ﬁrst subjected to dilation 
before erosion using the morphological transformation technique known as “clos-
ing”. With this, the portion of the red meat image that takes into account the largest 
Fig. 3 Flowchart of Image 
pre-processing Algorithm

An Efﬁcient CNN-based Method for Classiﬁcation of Red Meat...
399
Fig. 4 A red meat image of 
the fresh class 
Fig. 5 A pre-processed red 
meat image of the fresh class 
value of the pixels in the neighbourhood is detected as the foreground. On the other 
hand, erosion eliminates pixels if any of the surrounding pixels have a value of 0. 
After morphological transformation, masking is performed which produces masks. 
These masks are the inverted form of the morphed images. Then, we resized the 
images to the required standard size of 224times×224. Finally, we get the pre-processed 
images and collect and use them for model training as depicted in Fig. 4 and 5. 
3.3 
Freshness Level and classes 
The meat used for preparing the dataset is brought from a local market in India. Meat 
images that are categorized as Fresh are clicked before 4 h. Meat images clicked

400
A. Bajpai et al.
between 6 h to 10 h are categorized as Half-Fresh. Similarly, images clicked after 
14 h are categorized as Spoiled [ 8, 16, 17]. Based on these criteria, the images are 
captured and collected as the dataset before implementing image pre-processing. A 
red meat image of the fresh class is shown in Fig. 4 as an example. 
3.4 
HarNet Training 
As mentioned earlier, the objective of this work is to classify and detect red meat 
freshness using a novel convolutional neural network model mentioned as “HarNet”. 
The architecture of the HarNet model is shown in Fig. 6. We have created a novel 
sequential model, i.e., HarNet using libraries like TensorFlow, sklearn. The proposed 
CNN model is a convolutional neural network that has 4 convolutional 2-D layers, 
4 max-pooling 2-D layers, 7 dropout layers, 1 ﬂattened layer, and 4 dense layers. 
All the dropout layers have a dropout of 0.25 each. One convolutional 2D layer, 
one max pooling 2-D layer, and one dropout layer are arranged in 4 pairs before 
Flatten layer. After the ﬂattened layer, 4 dense layers with a dropout layer of 0.25 
dropout are arranged with layer activation “relu function”. Then, one dense layer 
with a “softmax function” is used due to multi-classes in the dataset. The input data 
to the HarNet model is in RGB image form with the same standard image height and 
width, i.e., 224timestimes×224. The batch size during training is 13 and the model is trained 
for 60 epochs. The ‘Categorical Crossentropy’ loss function is used as a loss function 
as there are three classes and this loss function is best suited for this task practically. 
The ‘relu’ activation function is used as the activation function for all the Conv2D 
layers and the ﬁrst three dense layers after the ﬂattened layer in the model. In the last 
dense layer of the HarNet model, the ‘softmax’ activation function is used because 
there are three classes for classiﬁcation, and it is best suited in the last dense layer 
if there are more than 2 classes for classiﬁcation tasks. The ‘RMSprop’ optimizer 
Fig. 6 Architecture of ‘HarNet’ Model

An Efﬁcient CNN-based Method for Classiﬁcation of Red Meat...
401
Fig. 7 AlexNet style architecture of the ‘HarNet’ Model 
is used in the model as the adaptive learning rate algorithm during training. Before 
selecting this optimizer, the HarNet model is also trained with other optimizers like 
adam, adagrad, etc., but the model is giving the best results with this optimizer. The 
dropout value of 0.25 is selected for all the seven dropout layers used in this model 
because it is observed during training with others values that the 0.25 dropout value 
is gaining the best results. 
The HarNet model is used in training using the dataset that we have gathered. 
The three labels of the categories are fresh, half-fresh, and spoiled which contain the 
respective red meat images. A standard ratio is used to divide the dataset into three 
portions: 70% for training, 15% for validation, and 15% for testing the model [ 8]. 
The network is trained on a total of 1585 images of standard size 224times×224; 338 
images are utilized for validation; and the remaining 338 images are used for testing. 
Data augmentation is used with the ImageDataGenerator function to produce even 
more images. 
4 
Result Analysis 
During this paper, an 80 GB NVIDIA Tesla A100 server is used for model devel-
opment, training, validation, and testing. We have created and used our sequential 
model named HarNET on the collected dataset as explained in the methodology 
section. The AlexNet style architecture of the proposed CNN model is as follows in 
Fig. 7. 
The proposed model HarNET is trained using a training set and runs for 60 epochs, 
and a validation set is used to validate. Then, we noted down its training and validation 
accuracy. After training, the best saved HarNet model’s training accuracy is 91.16%, 
and the validation accuracy is 88.57% as shown in Fig. 8. After that, we tested it on 
a testing set and found overall accuracy of 80%. The F1-score value for the spoiled 
class is 0.89 and the recall value of 0.96 is the highest attained by the HarNet model, 
followed by the fresh class with 0.78 as the F1-score value and the recall value of 
0.82 after testing. 
Also, we have compared the HarNET model and its validation and testing accuracy 
with the validation and testing accuracy of other pre-trained models, i.e., VGG16, 
VGG19, ResNet50, and InceptionV3. The plot of the VGG16 model is in Fig. 9 
and the VGG19 model is in Fig. 10. Although the training accuracy of the VGG16 
AND VGG19 model for this dataset is very high, the validation accuracy and testing

402
A. Bajpai et al.
Fig. 8 Training and 
Validation Accuracy Graph 
of the ‘HarNet’ model 
Fig. 9 VGG16 Accuracy 
graph 
Fig. 10 VGG19 Accuracy 
graph 
accuracy of the HarNet model is much better. Also, the number of layers in the 
HarNet is less than while comparing both VGG models. 
Similarly, after the VGG models, we have compared the HarNet model with two 
more pre-trained models, i.e., ResNet50 and InceptionV3. The ResNet50 has 50 
layers which are very high in numbers as compared to the HarNet model. Despite 
this, the ResNet50 is giving the lowest accuracy in comparison with all other pre-
trained models used for comparison with the HarNet model. The plot of accuracy

An Efﬁcient CNN-based Method for Classiﬁcation of Red Meat...
403
Fig. 11 ResNet50 Accuracy 
graph 
Fig. 12 InceptionV3 
Accuracy graph 
for the ResNet50 pre-trained model is shown in Fig. 11. InceptionV3 is performing 
better in terms of training and validation accuracy, but lagging behind VGG19 in 
testing accuracy. For the InceptionV3 pre-trained model, the plot for the training 
and validation accuracy is shown in Fig. 12. InceptionV3 has a very complex neural 
network architecture with very high numbers of layers. 
The results of all the models in the form of accuracy metrics are shown in Table 
1 and a comparison graph of testing accuracies of all used models is presented 
in Fig. 13. Despite being a shallower neural network than all the other compared 
pre-trained models, the HarNET model is giving better results and beating other 
pre-trained deep neural network models with a high and praiseworthy margin. Also, 
for the classiﬁcation of red meat into three classes, our proposed model “HarNet” is 
better than other state-of-the-art CNN-based models. 
5 
Conclusion and Future Work 
As per Table 1, despite being a shallow neural network in comparison with other pre-
trained models used like VGG16, VGG19, ResNet50, and IneptionV3, “HarNet” is 
outperforming these pre-trained deep neural network models, for the classiﬁcation

404
A. Bajpai et al.
Fig. 13 Comparison graph 
of Testing Accuracy for 
different models 
Table 1 Comparison Table of training, validation, and testing accuracy for the different models 
used 
Model Name
Training Accuracy (%) 
(Best) 
Validation Accuracy 
(%) (Best) 
Testing Accuracy (%) 
(Best) 
VGG16
99.81
76.92
62 
VGG19
100
73.08
69 
ResNet50
76.16
66.52
60 
InceptionV3
99.81
81.07
62 
HarNet
91.16
88.57
80 
of red meat in three classes in term of freshness. HarNet model used for freshness 
detection. There may be a further scope of increasing the accuracy by hyperparameter 
tuning or taking different datasets or meat types for future aspirants in meat freshness 
detection or classiﬁcation as the proposed model is attaining overall testing accuracy 
of 80%. The F1-score value for the spoiled class is 0.89 and the recall value of 0.96 
is the highest attained by the HarNet model, followed by the fresh class with 0.78 as 
the F1-score value and the recall value of 0.82 after testing. There is further scope for 
development for the half-fresh meat class which can increase the overall accuracy of 
this classiﬁcation task. 
References 
1. Yulianti T, Yudamson A, Septama HD, Sulistiyanti SR, Setiawan FA, Telaumbanua M (2016) 
Meat quality classiﬁcation based on color intensity measurement method. In: 2016 International 
Symposium on Electronics and Smart Devices (ISESD), pp 248–252. IEEE 
2. Levine P, Rose B, Green S, Ransom G, Hill W (2001) Pathogen testing of ready-to-eat meat 
and poultry products collected at federally inspected establishments in the united states, 1990 
to 1999. J Food Prot 64(8):1188–1193 
3. Inbaraj BS, Chen B (2016) Nanomaterial-based sensors for detection of foodborne bacterial 
pathogens and toxins as well as pork adulteration in meat products. J Food Drug Anal 24(1):15–

An Efﬁcient CNN-based Method for Classiﬁcation of Red Meat...
405
28 
4. Ghanbari K, Nejabati F (2019) Construction of novel nonenzymatic xanthine biosensor based 
on reduced graphene oxide/polypyrrole/cdo nanocomposite for ﬁsh meat freshness detection. 
J Food Meas Charact 13:1411–1422 
5. Fang H, et al (2022) Ppb-level h2s gas sensor based on CuNi-MOFs derivatives for meat 
freshness detection at low temperature environment. Sens Actuators B Chem 368:132225 
6. Sun Y, Zhang M, Adhikari B, Devahastin S, Wang H (2022) Double-layer indicator ﬁlms aided 
by BP-ANN-enabled freshness detection on packaged meat products. Food Packag Shelf Life 
31:100808 
7. Banwari A, Joshi RC, Sengar N, Dutta MK (2022) Computer vision technique for freshness 
estimation from segmented eye of ﬁsh image. Eco Inform 69:101602 
8. Garcia MBP, Labuac EA, Hortinela IV CC (2022) Chicken meat freshness classiﬁcation based 
on vgg16 architecture. In: 2022 IEEE International Conference on Artiﬁcial Intelligence in 
Engineering and Technology (IICAIET), pp 1–6. IEEE 
9. Vargas-Sansalvador IMP, Erenas MM, Martínez-Olmos A, Mirza-Montoro F, Diamond D, 
Capitan-Vallvey LF (2020) Smartphone based meat freshness detection. Talanta 216:120985 
10. Yazdanparast S, Benvidi A, Abbasi S, Rezaeinasab M (2019) Enzyme-based ultrasensitive elec-
trochemical biosensor using poly (l-aspartic acid)/MWCNT bio-nanocomposite for xanthine 
detection: a meat freshness marker. Microchem J 149:104000 
11. Liu Z et al (2019) Fluorescence strategy for sensitive detection of adenosine triphosphate in 
terms of evaluating meat freshness. Food Chem 270:573–578 
12. Asmara RA, Rahutomo F, Hasanah Q, Rahmad C (2017) Chicken meat freshness identiﬁcation 
using the histogram color feature. In: 2017 International Conference on Sustainable Information 
Engineering and Technology (SIET), pp 57–61. IEEE 
13. Rady AM, Adedeji A, Watson NJ (2021) Feasibility of utilizing color imaging and machine 
learning for adulteration detection in minced meat. J Agric Food Res 6:100251 
14. Wang C et al (2022) Convolutional neural network-based portable computer vision system for 
freshness assessment of crayﬁsh (Prokaryophyllus Clarkii). J Food Sci 87(12):5330–5339 
15. Liu D, Ma Y, Yu S, Zhang C (2023) Image based beef and lamb slice authentication using 
convolutional neural networks. Meat Sci 195:108997 
16. Putra GB, Prakasa E, et al (2020) Classiﬁcation of chicken meat freshness using convolutional 
neural network algorithms. In: 2020 International Conference on Innovation and Intelligence 
for Informatics, Computing and Technologies (3ICT), pp 1–6. IEEE 
17. Lugatiman K, Fabiana C, Echavia J, Adtoon JJ (2019) Tuna meat freshness classiﬁcation 
through computer vision. In: 2019 IEEE 11th International Conference on Humanoid, Nan-
otechnology, Information Technology, Communication and Control, Environment, and Man-
agement (HNICEM), pp 1–6. IEEE 
18. You M, Liu J, Zhang J, Xv M, He D (2020) A novel chicken meat quality evaluation method 
based on color card localization and color correction. IEEE Access 8:170093–170100

Multi-class Pathogenic Microbes 
Classiﬁcation by Stochastic Gradient 
Descent and Discriminative Fine-Tuning 
on Different CNN Architectures 
Nirajan Jha, Dibakar Raj Pant, Jukka Heikkonen, and Rajeev Kanth 
Abstract The detection of microorganisms is an important task in the clinical micro-
biology ﬁeld. It is equally important during the pandemic breakout. Pathogenic 
microbes’ orientational behavior helps in distinguishing them. However, it is not an 
easy task to classify them based on that behavior only. In this research work, image 
processing and CNN methods like Resnet50, DenseNet121, Inception-ResNetv2, and 
MobileNetv2 have been implemented to classify species of 33 different pathogenic 
microbes. The pathogenic microbes have stained with Gramm’s method to distinguish 
them as gram-positive and gram-negative bacteria. Lactobacillus, Staphylococcus, 
and Enterococcus are used for their intra-general classiﬁcation. Further, Stochastic 
gradient descent and ﬁne-tuning are used to tune the learning rate. The result shows 
that 90.62 accuracies have been obtained using ResNet architecture for discriminative 
ﬁne-tuning (DFT) and 92.96 accuracies have been obtained using stochastic gradient 
descent with the warm restarts (SGDR) approach. Similarly, 91.41 accuracies have 
been obtained using DenseNet 121 architecture for DFT and an accuracy of 98.7 has 
been obtained using Stochastic gradient descent with warm restarts approach. Also, 
an accuracy of 96.88 has been obtained using MobileNet architecture for DFT and 
an accuracy of 99.2 using Stochastic gradient descent with the warm restarts method. 
Further, Inception-ResNetV2 architecture has obtained an accuracy of 99.15 using 
DFT and 99.53 for the SGDR approach.
N. Jha · D. R. Pant envelope symbol
Institute of Engineering, Tribhuvan University, Kathmandu, Nepal 
e-mail: drpant@ioe.edu.np 
N. Jha 
e-mail: 075msice013.nirajan@pcampus.edu.np 
J. Heikkonen 
University of Turku, Turku, Finland 
e-mail: jukhei@utu.ﬁ 
R. Kanth 
Savonia University of Applied Sciences, Kuopio, Finland 
e-mail: rajeev.kanth@savonia.ﬁ 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_35 
407

408
N. Jha et al.
Keywords Fine-tuning · CNN · Pathogenic microbes · SGDR · Image 
processing · Architecture 
1 
Introduction 
Several pandemics and tropical diseases spread due to microorganisms. The 
mammalian intestine consists of microorganisms and the body is more prone to 
these microbes. Bacteria have morphological complexity, and their cell structure is 
incredibly unusual as they live in various conditions. Biological diversity inﬂuences 
the identiﬁcation process. Generally, the classical technique to detect microorgan-
isms is based on primitive methods, where laboratory workers’ expertise and skill are 
involved. This fact suggests automated microorganism identiﬁcation with diagnostic 
precision and less evaluation period is required [1, 2]. 
Many researchers have contributed to the recognition and segmentation of 
pathological images by applying computer vision and machine learning [3]. 
Categorization of PET/CT scan-based lymph node images has been done using 
statistical and classical machine learning methods [4, 5], and similar works have 
been done in [6–8]. These methods could not discover the geometric features that 
could help obtain the level of accuracy using contour features. The concept of a deep 
learning model using the Inception DCNN model has been proposed by M. F. Wahid 
et al. [9], and similar works have been done in [10] using Xception architecture. M. 
G. Forero et al. [11] studied the texture of digital images that uplift recognition and 
identiﬁcation rate. W. M. Ahmed et al. [12] suggested a feature-extraction process to 
understand the effect of various scattered features present in images. T. Treebupachat-
sakul and S. Poomrittigul [13] studied the comparative study of standard-resolution 
pathogenic microbes datasets to high-resolution microbes datasets for identiﬁcation 
and multi-class classiﬁcation of bacteria and yeasts. 
Noor et al. [14] suggested extracting deep learning features to detect bacteria using 
Na¨ıve Bayes classiﬁer on 192 images of 3 bacterial species. Further, the model has 
been trained by extracting microbes’ shape features. Venkatesh et al. [15] have shown  
the classiﬁcation of 3 bacteria species using deep learning. Researchers suggested 
classifying microscopic images using a convolutional neural network [16–18]. Also, 
they have used the optimal threshold segmentation method for segmenting micro-
scopic images of bacteria based on a threshold value. F. Xing et al. [19] have proposed 
a supervised machine-learning algorithm for the classiﬁcation of microscopic images. 
In this research work, DIBaS datasets are used for a different kinds of microbes 
classiﬁcation. This work is an extended version of previous work [27]. In this 
extended version, SGDR and ﬁne-tuning are used for Resnet, DenseNet, Inception-
ResNet, and MobileNet, and different parameters of SGDR are optimized to boost 
the model’s performance.

Multi-class Pathogenic Microbes Classiﬁcation ...
409
2
Method
 
The block diagram in Fig. 1 shows the overall method. 
2.1 
Dataset 
The microorganisms species available in DIBaS are implemented in [25], which has 
been made accessible to others [24]. The dataset consists of 33 different categories 
of microbes. Each category consists of 20 different images. The dataset was accu-
mulated by Jagiellonian University (microbiology department), located in Krakow, 
Poland [26]. The dataset consists of images having resolution 2048 * 1532 pixels. 
Various types of bacteria and their number from the dataset have been collected [27]. 
Among the datasets available, this is considered the standard one and is also found 
in other literature. 
2.2 
Data Preprocessing and Scaling 
The image augmentation method is implemented to extend the number of data. The 
increased dataset has helped in good training of the model, and it has improved the 
performance of the model by preventing it from overﬁtting.
Fig. 1 System block diagram 

410
N. Jha et al.
At the training stage, the training samples Sr: (aj, bj)p j=1 in a mini-batch set at rth 
training iteration is expanded to ˜Sr: (ãj, ˜bj)p j=1 through various image augmentation 
techniques at the time of feeding it into network architectures. Image augmentation 
operations are done at the data input stage. Training data are expanded to ˜p/p times 
of the original data. Also, for other stages, each image is augmented using image 
augmentation techniques. The prediction results have been merged, resulting in a 
single concluding Eq. 1. 
y left parent
hesis 
a ri ght p are
nthesis equals left parenthesis p slash p overTilde right parenthesis sigma summation Underscript j equals 1 Overscript p slash p overTilde Endscripts y left parenthesis a overTilde j right parenthesis
where, 
y(a): ﬁnal prediction function 
y(ãj): ﬁnal prediction function for expanded batch size 
p: batch size 
˜p: expanded batch size 
a, b: image coordinates that represent width and height, respectively 
The ﬁnal prediction result is the location of the largest value in the one-dimensional 
vector y(a) and its label. 
2.3 
Network Archtectures 
Residual Net-50 Convolutional Neural Network (ResNet-50), Inception-ResNet-V2, 
DenseNet121, and MobileNetV2 have been used as fundamental architectures. The 
pre-trained weights have been used as the weights of the architectures. 
2.3.1
ResNet50 Architecture 
ResNet has alleviated the problem of training deep networks with the help of skip 
connections. Input ‘a’ is a product of layer weights in the absence of skip connectivity. 
Further, it is added to bias and operated with an activation function, i.e., y(a), resulting 
in output P(a). 
upper  P  left  par
enthesis a right parenthesis equals y left parenthesis ma plus 1 right parenthesis
or, 
upper  P  lef
t parenthesis a right parenthesis equals y left parenthesis a right parenthesis
where,

Multi-class Pathogenic Microbes Classiﬁcation ...
411
P(a): output function 
y(a): activation function 
a: input 
m: weight 
l: bias 
When skip connectivity is established, the ﬁnal result is converted as: 
upper  P  left p a
ren
2.3.2
Inception-ResNet-V2 Architecture 
The architecture consists of inception blocks along with residual or skips connections. 
Skip connections reduce the training time. There are 1 * 1 convolutional ﬁlters 
where pooling operations are performed. The results are concatenated, and multi-
level feature extraction is done. 
Multiple features at multiple levels are concatenated to obtain a sound output. 
The Inception block is designed to be analogous to the animal visual cortex. When 
inception-resnet-v2 architecture is trained with input images for a longer time, then 
small, average, and complex level features are learned by the network. Each feature 
passes layer by layer for image recognition. 
2.3.3
DenseNet121 Architecture 
DenseNet architecture is generated by expanding ResNet architecture. Each layer 
possesses connections to other layers of the architecture. The outcome of a layer is 
linked to another following layer where several operations are performed. It consists 
of convolution, pooling, normalization operations, and application of activation 
function. Further, it results to: 
up er KY
 
Subs
c
ript m Baseline equals upper K Subscript m Baseline left parenthesis y Subscript m negative 1 Baseline right parenthesis
where y0, y Subscript 1 ellipsis Baseline y Subscript m negative 1 are layers of the DenseNet121 architecture, and up
p
er K
 Subscript m Baseline left parenthesis y Subscript m negative 1 Baseline right parenthesis
is a 
composite operation function. ResNets have enhanced the feature along with skip 
connectivity, changing it into: 
up er KY
 
Subs
c
ri pt m
 Baseline equals upper K Subscript m Baseline left parenthesis y Subscript m negative 1 Baseline right parenthesis plus y Subscript m negative 1
DenseNets perform concatenation of feature maps of various layers that distin-
guish it from ResNets, and the equation is changed into: 
up er KY
 S
ubscrip t  m  Base
li
ne equals upper K Subscript m Baseline left parenthesis left bracket y Subscript 0 Baseline comma y Subscript 1 Baseline ellipsis y Subscript m negative 1 Baseline right bracket right parenthesis

412
N. Jha et al.
2.3.4
MobileNetv2 Architecture 
It is an architecture that is well-suited for mobile equipment. There is the presence 
of residual connections across various layers having an inverted structure. It consists 
of 32 ﬁlters along with 19 following residual layers. Two different blocks having 
strides of 1 and 2 have helped in decreasing the size. Both blocks possess three 
distinct layers, where the initial layer has 1 * 1 convolutions along with ReLU6. It 
consists of a few mathematical operations that have resulted in higher accuracy. 
2.4 
Formulas 
The learning rate is varied across different layers of the architecture using DFT. The 
learning rate relationship is established by Eq. 8. 
new Subscr ip t weight Baseli ne  equals exist ing Subscr
ipt weight Baseline minus learning Subscript rate Baseline asterisk gradient
The periodic renewal of SGD of a variable φ for time step s is shown by Eq. 9.
nor mal up er Phi Subs
cript s Baseline equals normal phi Subscript s negative 1 Baseline minus normal beta left parenthesis nabla normal phi right parenthesis normal upper K left parenthesis normal phi right parenthesis
where β represents the learning rate, ∇φK(φ) represents the gradient for the objective 
function. Further, variable φ is broken into (φ1 … φM) and φM possesses the model’s 
features at layer M. Also, M represents the number of layers for a classiﬁer. (β1 … 
βM) is achieved, and βm represents the learning rate. SGD is updated periodically, as 
shown in Eq. 10. 
nor
ma l phi S ubscript s Su
perscript m Baseline equals normal phi Subscript s negative 1 Baseline minus normal beta left parenthesis nabla normal phi Superscript normal m Baseline right parenthesis normal upper K left parenthesis normal phi right parenthesis
2.5 
Stochastic Gradient Descent with Warm Restarts (SDGR) 
Gradient descent usually gets stuck at the local minimum point. It should be shifted 
to the global minima which can be done using SGDR. 
A gradual decrease in the learning rate is done in each batch of image datasets. It 
is decayed with a cosine annealing for each batch. It is expressed by Eq. 11. 
nor mal bet a Subscript s  Baseline equals normal beta 
Subscript min Baseline plus left parenthesis 1 divided by 2 right parenthesis left parenthesis normal beta Subscript max Baseline minus normal beta Subscript min Baseline right parenthesis left parenthesis 1 plus cosine left parenthesis left parenthesis upper R Subscript curl Baseline divided by upper R Subscript j Baseline right parenthesis normal pi right parenthesis right parenthesis
In the equation, βmin and βmax are the corresponding minima and maximum values. 
It shows variable learning rates. Generally, βmax shows the initial learning rate that 
is set, and βmin is changed within an epoch range. βmin has helped in choosing the

Multi-class Pathogenic Microbes Classiﬁcation ...
413
value of the learning rate. Rcurl helps keep track of the number of epochs since the 
last warm restart, which is updated at every batch iteration s. 
2.6 
Model Evaluation 
2.6.1
Confusion Matrix 
A confusion matrix explains the overall performance of a classiﬁer on test data where 
actual values are well-known. Actual and predicted values are observed in the table, 
and essential information is obtained, which is required in the performance analysis 
of the model. 
(1) TP: TP (True Positive) is deﬁned as a quantity that makes an accurate prognosis 
of real-class pathogenic microbes. 
(2) FP: FP (False Positive) is deﬁned as a quantity that makes an inaccurate 
prognosis of real-class pathogenic microbes. 
(3) TN: TN (True Negative) is deﬁned as a quantity that makes an accurate prognosis 
of real non-class pathogenic microbes. 
(4) FN: FN (False Negative) is deﬁned as a quantity that makes an inaccurate 
prognosis of real non-class pathogenic microbes. 
2.6.2
Recall and Speciﬁcity 
Recall or sensitivity is derived from the quotient of true positive predictions by total 
positives 
Recall eq uals TP  s las
h left parenthesis TP plus FN right parenthesis
Similarly, speciﬁcity is derived from the quotient of true negative predictions by 
total negatives. 
Specificity  e quals T N sla
sh left parenthesis TN plus FP right parenthesis
The value of recall and speciﬁcity lies between 0 to 1. 
2.6.3
Precision and Negative Predicted Value 
It is derived from the quotient of true positive predictions by total positive predictions. 
Negative Predicted value (NPV) is obtained by dividing the true negatives by the 
summation of true negatives as well as false negatives. 
Precision eq uals TP  s las
h left parenthesis TP plus FP right parenthesis

414
N. Jha et al.
NPV eq uals TN  s las
h left parenthesis TN plus FN right parenthesis
The value of precision and negatively predicted value lie between 0 to 1. 
2.7 
Results and Analysis 
2.7.1
Results 
The different results that are generated in Google Colaboratory for pathological 
images recognition and classiﬁcation are mentioned as (Figs. 2, 3, 4, 5, 6 and 7): 
All the multi-class classiﬁcations of pathological microbes are obtained from four 
different architectures that consist of 33 different species of pathogenic microbes. 
The multi-class classiﬁcation also consists of top losses along with their associated 
probabilities. The images of top losses are the losses of the prediction for which 
the model is most conﬁdent. The actual images, along with their prediction and the 
number of losses associated with them, are observed. Similarly, the probability of 
prediction is also observed. Thus, the images are arranged in decreasing order of their 
losses, and the top 9 images with the highest losses associated with their prediction 
have been obtained. Also, the above-shown curves demonstrate good accuracy along 
with batches processed. Hence, the microorganisms present in the dataset have been 
detected signiﬁcantly. The confusion matrices obtained using ResNet, DenseNet,
Fig. 2 Pathological microbes classiﬁcation obtained for ResNet50 architecture

Multi-class Pathogenic Microbes Classiﬁcation ...
415
Fig. 3 Pathological microbes classiﬁcation obtained for DenseNet121 architecture 
Fig. 4 Pathological microbes classiﬁcation obtained for MobileNetV2 architecture

416
N. Jha et al.
Fig. 5 Pathological microbes classiﬁcation obtained for Inception-ResNet-v2 architecture 
Fig. 6 Plot of test accuracy and test loss with respect to batches for ResNet50 architecture (left) 
and DenseNet121 architecture (right)
Inception-ResNet-v2, and MobileNet have been obtained. There are substantial true 
positives and fewer false positives and false negatives. 
2.7.2
Model Performance Evaluation 
Proper evaluation of the architecture has been done by calculating the correctness 
for both of the methods along with other metrics. The accuracies and losses for 
different architectures in pathological microbes classiﬁcation are obtained as shown 
in Table 1.

Multi-class Pathogenic Microbes Classiﬁcation ...
417
Fig. 7 Plot of test accuracy and test loss with respect to batches for MobileNetV2 architecture 
(left) and Inception-ResNetV2 architecture (right)
Table 1 Parameters evaluating performance of architectures 
Architectures 
parameters 
ResNet-50
DenseNet-121
MobileNetV2
Inception-ResNetv2 
Accuracy using 
discriminative 
ﬁne-tuning 
0.9062
0.9141
0.9688
0.9915 
Loss using dis-
criminative ﬁne-
tuning 
0.0938
0.0859
0.0312
0.0085 
Accuracy using 
SGDR 
0.9296
0.987
0.992
0.9953 
Loss 
using 
SGDR 
0.0704
0.0130
0.0080
0.0047 
2.7.3
Result Validation and Discussion 
The precision or positive predicted value, speciﬁcity, recall or sensitivity, nega-
tively predicted value, and f-score for different architectures in pathological microbes 
classiﬁcation are obtained as shown in Table 2.
2.7.4
Related Works and Their Comparison 
See Table 3.

418
N. Jha et al.
Table 2 Parameters validating results of architectures 
Architectures 
parameters 
ResNet-50
DenseNet-121
MobileNetV2
Inception-ResNetv2 
Precision/Positive 
predicted value 
0.89
0.92
0.88
0.90 
Speciﬁcity
0.996
0.997
0.996
0.9967 
Recall/Sensitivity
0.8939
0.9167
0.8863
0.8964 
Negative 
predicted value 
F-Score 
0.997 
0.891 
0.9973 
0.918 
0.9964 
0.883 
0.997 
0.898
Table 3 Related works and their comparison 
Study
Methods
Classes
Accuracy (Percentage) 
Mohamed et al. [27] (2018)
BoW, SVM
10
97 
M. F. Wahid [9] (2018)
Inception DCNN
5
95 
P. Hiremath and P. Bannigidad 
[7] (2010) 
Geometrical 
Features 
3
94–96 
T. Treebupachatsakul and S. 
Poomrittigul [13] (2020) 
LeNET
4
S. aureus (56.92), 
Micrococcus spp. (95.14), 
C. albicans (100), L. 
delbrueckii (40.96) 
The Proposed
ResNet50, 
DenseNet121, 
MobileNetV2, 
Inception-
ResNet-v2 
33
Discriminative Fine-Tuning + 
SGDR (90.62, 92.96), (91.41, 
98.7), (96.88, 99.2), (99.15, 
99.53) 
3 
Conclusion 
Four different architectures have been used for the recognition of pathogenic 
microbes. The model’s performance has been boosted using two different tuning 
approaches. A signiﬁcant accuracy of 90.62 has been obtained for ResNet50 archi-
tecture implementing DFT and 92.96 on implementing the SGDR approach. Also, 
DenseNet121 architecture has obtained an accuracy of 91.41 using discriminative 
ﬁne-tuning and 98.7 using the SGDR approach. Similarly, MobileNetV2 architecture 
has obtained an accuracy of 96.88 using discriminative ﬁne-tuning and 99.2 using the 
SGDR approach. Further, Inception-ResNetV2 has obtained an accuracy of 99.15 
using discriminative ﬁne-tuning and 99.53 using the SGDR approach. 
The obtained results show that classiﬁcation accuracy has signiﬁcantly improved 
by using ﬁne-tuning and the SGDR approaches. Even with these approaches, gram-
positive and gram-negative bacteria have been clearly classiﬁed, and Inception-
ResNetV2 architecture is found to be superior to ResNet50, DenseNet121, and 
MobileNetV2 architectures.

Multi-class Pathogenic Microbes Classiﬁcation ...
419
References 
1. Lim K, Park SH, Kim J, Seonwoo H, Choung P-H, Chung JH (2013) Cell image processing 
methods for automatic cell pattern recognition and morphological analysis of mesenchymal 
stem cells-an algorithm for cell classiﬁcation and adaptive brightness correction. J Biosyst Eng 
38(1):55–63 
2. Shamir L, Delaney JD, Nikita Orlov D, Eckley M, Goldberg IG (2010) Pattern recognition 
software and techniques for biological image analysis. PLoS Comput Biol 6(11):e1000974 
3. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444 
4. Trattner S, Greenspan H, Tepper G, Abboud S (2004) Automatic identiﬁcation of bacterial 
types using statistical imaging methods. IEEE Trans Med Imaging 23(7):807–820 
5. Xu G, Cao H, Dong Y, Yue C, Zou Y (2020) Stochastic gradient descent with step cosine warm 
restarts for pathological lymph node image classiﬁcation via PET/CT images. In: 2020 IEEE 
5th international conference on signal and image processing (ICSIP), pp 490–493. IEEE 
6. Blackburn N, Hagström Å, Wikner J, Cuadros-Hansson R, Bjørnsen PK (1998) Rapid determi-
nation of bacterial abundance, biovolume, morphology, and growth by neural network-based 
image analysis. Appl Environ Microbiol 64(9):3246–3255 
7. Hiremath PS, Bannigidad P (2009) Automated gram-staining characterization of digital bacte-
rial cell images. In: Proceedings of the IEEE international conference on signal and image 
processing ICSIP, pp 209–211 
8. De Bruyne K, Slabbinck B, Waegeman W, Vauterin P, De Baets B, Vandamme P (2011) 
Bacterial species identiﬁcation from MALDI-TOF mass spectra through data analysis and 
machine learning. Syst Appl Microbiol 34(1):20–29 
9. Wahid MdF, Ahmed, T, Habib MdA (2018) Classiﬁcation of microscopic images of bacteria 
using deep convolutional neural network. In: 2018 10th international conference on electrical 
and computer engineering (ICECE), pp 217–220. IEEE 
10. Wahid MdF, Hasan MdJ, Alom MdS (2019) Deep convolutional neural network for microscopic 
bacteria image classiﬁcation. In: 2019 5th international conference on advances in electrical 
engineering (ICAEE), pp 866–869. IEEE 
11. Forero MG, Cristóbal G, Desco M (2006) Automatic identiﬁcation of mycobacterium 
tuberculosis by Gaussian mixture models. J Micros 223(2):120–132 
12. Ahmed WM, Bayraktar B, Bhunia AK, Daniel Hirleman E, Paul Robinson J, Rajwa B (2012) 
Classiﬁcation of bacterial contamination using image processing and distributed computing. 
IEEE J Biomed Health Inform 17(1):232–239 
13. Treebupachatsakul T, Poomrittigul S (2020) Microorganism image recognition based on 
deep learning application. In: 2020 international conference on electronics, information, and 
communication (ICEIC), pp 1–5. IEEE 
14. Mohamad NA, Jusoh NA, Htike ZZ, Win SL (2014) Bacteria identiﬁcation from microscopic 
morphology using Naive Bayes. Int J Comput Sci Eng Inf Technol (IJCSEIT) 4(1) 
15. Vijaykumar V (2016) Classifying bacterial species using computer vision and machine learning. 
Int J Comput Appl 151(8):23–26 
16. Hay EA, Parthasarathy R (2018) Performance of convolutional neural networks for identiﬁca-
tion of bacteria in 3D microscopy datasets. PLoS Comput Biol 14(12):e1006628 
17. Ferrari A, Lombardi S, Signoroni A (2015) Bacterial colony counting by convolutional neural 
networks. In: 2015 37th annual international conference of the IEEE engineering in medicine 
and biology society (EMBC), p 7458–7461. IEEE 
18. Zhang R, Zhao S, Jin Z, Yang N, Kang H (2010) Application of SVM in the food bacteria image 
recognition and count. In: 2010 3rd international congress on image and signal processing, vol 
4, pp 1819–1823. IEEE 
19. Xing F, Xie Y, Su H, Liu F, Yang L (2017) Deep learning in microscopy image analysis: a 
survey. IEEE Trans Neural Netw Learn Syst 29(10):4550–4568 
20. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: 
Proceedings of the IEEE conference on computer vision and pattern recognition, pp 770–778

420
N. Jha et al.
21. Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017) Densely connected convolutional 
networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 
pp 4700–4708 
22. Szegedy C, Ioffe S, Vanhoucke V, Alemi AA (2017). Inception-v4, inception-ResNet and 
the impact of residual connections on learning. In: Thirty-ﬁrst AAAI conference on artiﬁcial 
intelligence 
23. Sandler M, Howard A, Zhu M, Zhmoginov A, Chen L-C (2018) Mobilenetv2: inverted residuals 
and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern 
recognition, pp 4510–4520 
24. Misztal K, Spurek P, Brzychczy-Włoch M, Zieli´nski B, Plichta, A, Ocho´nska D (February, 
2020) Dibas dataset. http://misztal.edu.pl/software/databases/dibas/. 
25. Zieli´nski B, Plichta A, Misztal K, Spurek P, Brzychczy-Włoch M, Ocho´nska D (2017) Deep 
learning approach to bacterial colony classiﬁcation. PLoS ONE 12(9):e0184554 
26. katedra mikrobiologii (February 2022). https://km.cm-uj.krakow.pl/ 
27. http://conference.ioe.edu.np/publications/ioegc10/ioegc-10-078-10108.pdf

Early Prediction of Thoracic Diseases 
Using Rough Set Theory and Machine 
Learning 
Radhanath Hota, Sachikanta Dash, Sujogya Mishra, P. K. Pattnaik, 
and Sipali Pradhan 
Abstract An unexpected demise due to cardiac arrest is a signiﬁcant physical 
anomaly and is responsible for several deaths. Death due to unexpected cardiac 
arrest has no signiﬁcant symptoms, and cardiac arrest has no initial symptoms. In 
most cases, a person may suffer from cardiac arrest, despite having a normal electro-
cardiogram (ECG). In this work, we have used two concepts, i.e., rough set theory 
(RST) to ﬁnd the signiﬁcant symptoms of cardiac arrest and support vector machines 
(SVM) to predict cardiac arrest. Our work aims to predict unexpected cardiac arrest 
less than half an hour before its occurrence. We have validated our claim using 
statistical techniques. 
Keywords ECG · Cardiac arrest · RST · SVM · Statistical technique 
1 
Introduction 
In general, cardiac arrest occurs due to the blockage of coronary arteries, which is 
responsible for supplying blood to cardiac muscles. This sudden obstruction leads 
to damage to the heart muscle portion responsible for vital blood supply. Therefore, 
cardiac arrest is due to damage to a portion of the heart muscle. This is not the unique
R. Hota 
Department of Computer Science, College of Basic Science and Humanities, OUAT, 
Bhubaneswar, Odisha, India 
S. Dash 
Department of Computer Science and Engineering, GIET University, Gunupur, Bhubaneswar, 
Odisha, India 
e-mail: sachikanta@giet.edu 
S. Mishra · P. K. Pattnaik envelope symbol
Department of Mathematics and Humanities, Odisha University of Technology and Research, 
Bhubaneswar, Odisha, India 
e-mail: papun.pattnaik@gmail.com 
S. Pradhan 
Department of Computer Science, RBVRR Women’s College, Hyderabad, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_36 
421

422
R. Hota et al.
cause of cardiac arrest; several other reasons exist. A person may suffer from cardiac 
arrest due to ventricular ﬁbrillation. Due to ventricular ﬁbrillation, the signal inside 
the heart that regulates the heartbeat and its organization unexpectedly becomes 
completely disordered; this particular anomaly stops the heartbeat. Cardiopulmonary 
resuscitation (CPR) assists the victim’s blood ﬂow; simultaneously, a deﬁbrillator 
is used to shock the victim’s heart as quickly as feasible. The substantial shock 
enables the electrical signal to rearrange the heart, and the heart begins to beat once 
more. Unfortunately, people who suffer from cardiac arrest are not successfully 
revived because death usually occurs within a few minutes after the event unless 
professional treatment is available. Therefore, it is crucial to anticipate these cardiac 
arrests before they occur. Our work deals with the recent research gap in analyzing 
cardiac problems. Contemporary research must measure accuracy by comparing 
several machine learning algorithms (ML). In this work, we used RST, which can 
handle uncertainty signiﬁcantly. The idea of RST was developed by Pawlak [1] in  
the early ’80s. RST is a useful tool in handling imprecise and vague data; this theory 
does not require prior knowledge about the data set and can be used for testing and 
training purposes. Medical data are generally imprecise and vague, so RST is useful 
for analyzing them. 
2 
Literature Review 
Several research studies used universal medical data or prepared medical documents 
by gathering information from various sources. Das et al. [2] discussed cardiac arrest 
prediction using RST and ML concepts. Das et al. [3] used machine learning tools to 
predict heart diseases. Several approaches are also available to predict the symptoms 
of various diseases. Nayak et al. [4] predicted symptoms for malaria using the RST 
concept; several researchers have applied the soft computing concept to social issues 
in this context. Mishra et al. [5] discussed why employees of government and non-
government organizations need legal justice. Several researchers have discussed the 
application of machine learning and soft computing techniques to predict various 
meteorological phenomena like wind speed, rainfall prediction, etc.; in this context, 
Das et al. [6] discussed the prediction of wind speed using soft computing and 
machine learning tools in this direction. Das et al. [7] used several soft computing 
tools, including an artiﬁcial neural network (ANN) and RST, to predict the rainfall 
rate in Odisha with signiﬁcant accuracy. 
The modern approach of “soft computing” also had a signiﬁcant impact on the 
detection of abnormalities in the lungs. In this context, Kido [8] discussed lung abnor-
malities using a convolutional neural network (CNN), and chest performance analysis 
was discussed by Raoof [9]. Cardiac arrest is such a serious cardiac problem that it can 
take a life within a very short period. In this context, Reissig [10] discussed the impact 
of ultrasound on diagnosing pneumonia. The accuracy of ultrasound results always 
needs to be improved. In this context, Nazerian et al. [11] discussed and analyzed the 
accuracy of ultrasound results. Several researchers also work in the direction of early

Early Prediction of Thoracic Diseases Using Rough Set Theory ...
423
prediction of cardiac problems; Sheela and Vanitha [12] discussed early detection 
of sudden cardiac arrest. There are several difﬁculties in analyzing cardiac medical 
data sets in this context. Das et al. [13] discussed analyzing cardiac problem data 
sets using soft computing techniques. Several studies have been conducted to ﬁnd a 
particular disease’s symptoms by analyzing the medical data in this context. Nayak 
et al. [14] combined soft computing and machine learning tools to predict symptoms 
of cardiac arrest. 
2.1 
Background 
Medical data analysis is always a challenge for data scientists and doctors. Medical 
data is generally imprecise, lacking attributes, vague, and voluminous. The general 
medical practitioner always depends upon the patient’s medical history, which 
appears to be either similar or vague; for example, a person with typhoid and malaria 
has almost similar symptoms. In this study, we have used the concept of RST to 
handle incomplete and imprecise data. RST is a useful tool for handling incomplete 
and imprecise data; that is the reason we have used this concept in our studies. 
2.2 
Fundamental of RST 
Pawlak introduced the idea of RST in early 1980s; the fundamental RST derived 
from traditional set theory. RST always represents a table with header <R, 1, 2> 
where R is the set of records, 1 is the set of conditional attributes, and 2 is the set of 
decision attributes. The representation of RST nicely discuused by Nayak et al. [14] 
for ﬁnding reduct, core, indiscernibility in the tabular form. 
Table 1 Initial Table representing RST 
Records
A
b
C
d
2 
R
A
b
C
d
L 
R1 
M
n
 M
m
 P
 
R2
N
n
M
m
P 
R3 
M
m
 N
 m
 p

424
R. Hota et al.
2.3 
Various RST Algorithm 
Step-1 Initialize the universal set U which includes C (Conditional attributes),  
D (Decision attributes) Indiscernibility=  . 
Step-2 Exclude the decision attributes from the universal set U\D=K 
Step-3 For i=1 to  n  (n is the total number of attributes in K) 
Step-4 Compare the values of the attributes (conditional) 
x , x  V, V is the values of the conditional attribute 
If either some x or  
x matches then corresponding records are consider as 
Indiscernible  
i=1 implies single attribute, i=2 combination of two attributes so on… 
I denotes for indiscernibility then for each matching values increment the 
indiscernibility I  
and new indiscernibility set was found 
else goto step-3 
Step-5 end if 
Step-6 end for 
ii.
Algorithm for reduct 
i.
Algorithm to find Indiscernibility 
Step-1 for I , consider all indiscernibility set,  
if 
for some B  I, and elements of B are distinct the B qualify for the reduct  
 for (j=1, j<=Total number of indiscernibility), calculate the reduct set 
end 
Step-2 end if 
End 
iii.
Core= Reduct 
2.4 
Fundamental of SVM 
SVM is supervised learning technique, objective is to choose a hyperplane which 
separate two different classes. Logistic regression and SVM both used the concept of 
classiﬁcation, Logistic regression used the concept of probability for classiﬁcation 
whereas SVM used the concept of statistics for classiﬁcation. 
• Functionality of SVM 
In contrast to logistic regression, where the classiﬁer is deﬁned over all the points, 
SVM is deﬁned only in terms of the support vectors. Therefore, we don’t need to 
worry about other observations because the margin is calculated using the points 
closest to the hyperplane (support vectors). SVM thus beneﬁts from certain organic 
speedups.

Early Prediction of Thoracic Diseases Using Rough Set Theory ...
425
3 
Initial Steps for Data Analysis 
We have collected various cardiac related data form various medical sources of our 
state and prepare a dataset given in the following data: 
The data collected from various parts of our state, Puri, Cuttack, and Khordha 
are coastal areas, and Sambalpur and Kalahandi are non-coastal areas. The above 
table represents people affected by different cardiac-related diseases. Our objec-
tive is to ﬁnd the signiﬁcant cardiac problem which has signiﬁcant importance in 
affecting mass. To analyse the data better, we have used linear regression to get 
several dissimilar groups. 
3.1 
Application Linear Regression to Group the Dataset 
We apply linear regression to regroup the dataset according to the dissimilarity 
(Fig. 1). 
After regrouping the prepared dataset, we represent the given in the following 
Table. The column of the table represents the conditional attributes, and the rows
Table 2 Initial Information 
Records
Pneumonia
Asthma
Thoracic 
diseases 
Valve 
diseases 
High blood 
pressure 
Total 
Sambalpur
5,000
25,000
5,000
15,000
10,000
60,000 
Puri
2,500
2,500
35,000
10,000
10,000
60,000 
Khordha
25,000
5,000
10,000
15,000
5,000
60,000 
Cuttack
5,000
5,000
10,000
25,000
15,000
60,000 
Kalahandi
5,000
2,500
11,500
22,000
20,000
60,000 
Fig. 1 Scatter plot of the 
given data 

426
R. Hota et al.
Table 3 Table after regression 
S
Aa
Bb
cc
dd
Ee 
S1
p1
p0
p1
p0
p1 
S2
p1
p1
p1 
p0 
p0 
S3
p0
p0
p0
p1
p1 
S4
p1
p0
p1 
p0 
p1 
S5
p1
p1
p1
p0
p1 
of the table represent the conditional attributes. To keep the attributes in a table we 
rename the conditional attributes. 
<Pneumonia, Asthma, Thoracic diseases, Valve diseases, High blood pressure> as 
<aa, bb, cc, dd, bb> respectively and it’s values are signiﬁcant as p1 and insigniﬁcant 
as p0. The table derived from the original dataset prepared by us given in the following. 
3.2 
Finding Reduct and Core 
To ﬁnd the core and reduct we have used the algorithm (i) & (ii) given in the Sect. 2.3. 
Apply the algorithm data taken from Table 3. We represent the indiscernibility by 
the notation R. Apply the algorithm we get the following results i.e. 
R(aa) = {(S1, S2, S4, S5), S3}, R(bb) = {(S1, S3, S4), (S2, S5)}, R(cc) = {(S1, S2, 
S4, S5), S3}, R(dd) = {(S1, S2, S4, S5), S3}, R(ee) = {(S1, S3, S4, S5), S2}. Similarly, 
we have considered two attribute combination: 
R(aa, bb) = {(S1, S4), (S2S5), S3} to ﬁnd the indiscernibility R(bb, cc), R(aa, 
cc), R(aa, dd), R(aa, ee), R(bb, dd), R(bb, ee), R(cc, dd), R(cc, ee), R(aa, bb, cc), 
R(aa, bb, dd), R(aa, bb, ee), R(bb, cc, dd), R(bb, dd, ee), R(aa, bb, cc, dd), among all 
these indiscernibility we have the following indiscernibility produce unique values 
are i.e. IND (aa, bb, cc, dd) = {S1, S2, S3, S4, S5}, IND (aa, bb, cc, ee) = {S1, S2, 
S3, S4, S5, S6}, IND (aa, cc, dd, ee) = {S1, S2, S3, S4, S5, S6}, IND (bb, cc, dd, ee) 
= {S1, S2, S3, S4, S5, S6} as these indiscernibility produce unique records and these 
indiscernibility relation leads to reduct or candidate of data analysis. 
Core = intersectionReduct = intersection{(aa, bb, cc, dd)}intersection{(aa, bb, cc, ee) intersection{(aa, cc, dd, ee)} intersection
{(bb, cc, dd, ee)}} = cc, as cc is represents for Thoracic diseases. Now in the next 
section we have applied SVM to ﬁnd early prediction of Thoracic diseases. 
4 
Thoracic Diseases 
To analyse further, we have used the data on Thoracic diseases [15]. To analyse 
further, we have used SVM. Using SVM, our objective is to predict the immediate 
impact of cardiac arrest. We aim to detect immediate cardiac arrest half an hour

Early Prediction of Thoracic Diseases Using Rough Set Theory ...
427
before its occurrence. This study’s components include heart rate variability (HRV) 
and sudden cardiac death (SCD). We have used SVM to classify SCD and a normal 
person without any cardiac anomalies. 
4.1 
Background for Data Analysis 
The MIT-BIH database contains ECGs for several diseases. We have considered 
the normal sinus rhythm (NSR) and sudden cardiac death (SCD) databases for our 
studies. We have divided the 40 signals into two groups: 20 for NCR and 20 for 
SCD. We considered the ECG data from 30 min before cardiac arrest using the 
database. To extract the feature, we have used the 10-min window size. We have 
gathered a total of 200 signals, i.e., for every 5 patients, 10 min’ worth of signals 
were recorded. Out of 200 signals, we have classiﬁed 100 as cardiac arrest and 100 
as normal. We have used 150 signals for training and 50 signals for testing purposes. 
Intervals of time duration among two successive heartbeats are normally calculated 
in the electrocardiogram between two successive QRS complexes and denoted by 
QQ, usually called RR intervals. The duration among normal beats is named “NN 
intervals.“ We have classiﬁed the Heart rate variability (HRV) into two different 
groups, one is the time domain, and the other is the frequency domain. The time 
domain includes Standard deviation for all NN intervals, the Square root means of 
the squares of differences between adjacent for all NN, and the domain for frequency 
includes parameters of HRV such as lower frequency (LF), high frequency (HF), and 
the ratio of LF to HF. We have considered the time interval 0–30 min partitions into 
0–10 m, 10 m–20 m, 20 m–30 m, 5 m–15 m, and 15 m–25 m. The SVM approach is 
better than a neural network in several aspects. This model does not require imagining 
several neurons in the intermediate layer. SVM is applied to ﬁnd a hyperplane. 
The required hyperplane is optimum. The objective of the hyperplane is to separate 
two data sets according to feature. The minimum margin between the two sets is 
maximised to create this ideal hyperplane. As a result, the hyperplane generated 
will only depend on border training patterns known as support vectors. The support 
vector machine uses two mathematical operations to function: (1) hiding both the 
input and the result, a nonlinear mapping of an input vector into a high-dimensional 
feature space, and (2) building an ideal hyperplane to separate the features found in 
step one. Equations (5–12) are used to determine support vectors. 
4.2 
Application of SVM System 
a. We have considered an input space of dimension n0 vector x retrieve from the 
input space, 
b. 
Let Kj(x) for j varies from 1 to n1, is the transformation among the input space 
to feature space,

428
R. Hota et al.
c. Dimension of the feature space denoted by n1, 
d. {rj} denotes the linear weights connecting the output space to feature space where 
j varies from 1 to n1, 
e. Kj(x) is the input supplied weights by the feature space, 
f. We have consider b, alpha Subscript i, di as bias, Lagrange coefﬁcient, and corresponding target 
output, 
g. The hyperplane
sig
ma s umati on Un ders
cript i equals 1 Overscript upper N Endscripts alpha Subscript i Baseline d Subscript i Baseline upper L left parenthesis x comma x Subscript i Baseline right parenthesis comma
where upper L l ef t parenthesis x comma x Subscript i Baseline right parenthesis equals upper K left parenthesis x right parenthesis upper K Superscript t Baseline left parenthesis x right parenthesis, characterizes the inner product generated by the input 
vector x in the feature space and input design xi relating to the ith example. The ith 
example point to inner product kernel. 
Weight N deﬁne as 
up p
er 
N eq uals s igma s
ummation Underscript i equals 1 Overscript upper N Endscripts alpha Subscript i Baseline d Subscript i Baseline upper K left parenthesis x Subscript i Baseline right parenthesis comma
upper  K  left parenth e s i s  x right par
enthesis equals left bracket upper K Subscript 1 Baseline left parenthesis x right parenthesis upper K Subscript 2 Baseline left parenthesis x right parenthesis ellipsis ellipsis period upper K Subscript n 1 Baseline left parenthesis x right parenthesis right bracket Superscript upper T Baseline comma
K0(x) = 1 for allx, R0 represents as bias b, 
h. Introduction of L(x, xi) as the kernel chosen as radial basis function kernel (RBF) 
i. alpha Subscript i for i = 1 to M with objective function: 
upper N
 l e
f
t  p a
ren thesis beta right parenthesis equals sigma summation Underscript i equals 1 Overscript i equals upper N Endscripts normal beta Subscript i Baseline minus one half sigma summation Underscript i equals 1 Overscript i equals upper N Endscripts sigma summation Underscript j equals 1 Overscript upper N Endscripts normal beta Subscript i Baseline normal beta Subscript j Baseline d Subscript i Baseline d Subscript j Baseline upper L left parenthesis x Subscript i Baseline comma x Subscript j Baseline right parenthesis
up
p e r
 
N  l e
f
t
 p a r
enthesis 
b
eta ri
g
ht parenthesis equals sigma summation Underscript i equals 1 Overscript i equals upper N Endscripts normal beta Subscript i Baseline minus one half sigma summation Underscript i equals 1 Overscript i equals upper N Endscripts sigma summation Underscript j equals 1 Overscript upper N Endscripts normal beta Subscript i Baseline normal beta Subscript j Baseline d Subscript i Baseline d Subscript j Baseline upper L left parenthesis x Subscript i Baseline comma x Subscript j Baseline right parenthesis
subject to constraint 
s
i
g m a  
summati
on 
0 less tha n o r  equa ls  n
orm al beta Subscript i Baseline less than or equals upper C for i equals 1 to upper N
Linear weights denoted by r0 corresponding to the Lagrange’s multipliers given 
by the following equation, 
r 
S
ubscript 0
 Baseline sigma summation beta Subscript 0 i Baseline d Subscript i Baseline upper K left parenthesis x Subscript i Baseline right parenthesis
K(xi) image retrieve after dimension reduction. We have used the following 
statistical method to calculate the performances of the classiﬁers.

Early Prediction of Thoracic Diseases Using Rough Set Theory ...
429
True Positive (TP): The classiﬁer properly recognises the cardiac arrest signal 
when the subject is experiencing cardiac arrest. 
False Positive (FP): The person does not have cardiac arrest but classiﬁers wrongly 
classify cardiac arrest. 
True Negative (TN): The person don’t have cardiac arrest and classiﬁers also 
classify the person don’t have cardiac arrest. 
False Negative (FN): It is just the opposite of true positive the person don’t have 
cardiac arrest and the classiﬁer classify that the person don’t have cardiac arrest. 
Sensitivity: The percentage of accuracy by the classiﬁers. 
Sensitivity eq uals left  p are
nthesis TP right parenthesis divided by left parenthesis TN plus TP right parenthesis
Specificity  e quals lef t par
enthesis TN right parenthesis divided by left parenthesis TN plus FP right parenthesis
Positive Predictive Value eq uals left  p are
nthesis TP right parenthesis divided by left parenthesis TP plus FP right parenthesis
Positive Predictive Value (PPV) is actually representing the percentage of accuracy 
i.e. if the person having cardiac arrest and classiﬁers predict accurately predict that. 
Negative Predictive Value (NPV) is that a person doesn’t have cardiac arrest and 
classiﬁers also predict that the person doesn’t have cardiac arrest. 
Negative Predictive Value eq uals left  p are
nthesis TN right parenthesis divided by left parenthesis TN plus FN right parenthesis
Classiﬁcation Accuracy of the classiﬁers is deﬁned as ratio of accurate calculations 
to all the calculation. 
Classification  Accuracy  e qual s left par en the si s T N plu
s TP right parenthesis divided by left parenthesis TN plus TP plus FP plus FN right parenthesis
4.3 
Prediction and Analysis Using SVM Systems 
The standard signal of cardiac arrest is considered by the physiologists next. The 
signals are then divided into overlapping signals with 10 min of duration, and the 
HRV response is taken from the ECG. The signal is considered for both normal 
people who don’t have a cardiac arrest and those with a cardiac arrest. ECG signals 
are calculated for a normal person and a person suffering cardiac anomalies, with both 
parameters for the time domain and the frequency domain being retrieved. Table 1 
represents the HRV signals for the normal patient, and Table 2 represents the HRV 
signals for cardiac arrest patients. So the entire signals are divided into two groups, 
one for normal people without cardiac arrest and another group for people suffering 
from cardiac arrest, and each group consists of 100 samples. We have considered

430
R. Hota et al.
150 and 50 signals for training to achieve the training and testing. SVM’s accuracy 
level with RBF kernel provides 95% for training and 88% for testing. 
All the above Figs. 2, 3, 4, 5 and 6 (a) and (b) represent the dataset of persons 
without cardiac arrest, and with cardiac arrest respectively. After using the RBF 
kernel function of SVM, we got the result presented in Table 4 after dividing 200 
samples, 150 for training and 50 for testing. The performance measures are as follows: 
95% accuracy level in the training phase and 88% in the testing phase. The ﬁgures 
of groups a and b show no change in the results. Applying the SVM RBF kernel 
suitable for our data set provides the rest of the four ﬁgures by randomly selecting 
the samples, increasing the accuracy level. We have considered 2 groups without 
cardiac (a) with cardiac arrest as (b) experiment conducted for patient-1, patient-2, 
patient-3, patient-4, and patient-5, each patient group consists of 40 patients (Fig. 7). 
Fig. 2 Accuracy level for test data 
Fig. 3 Accuracy level for group 1 patient

Early Prediction of Thoracic Diseases Using Rough Set Theory ...
431
Fig. 4 Accuracy level for group 2 patient 
Fig. 5 Accuracy level for group 3 patient 
Fig. 6 Accuracy level for all the samples

432
R. Hota et al.
Prepared Data 
set 
Application of Linear Regres-
sion to get dissimilar records 
Application of RST to get cardiac 
arrest is the most significant attribute 
Application of SVM system on 
the Universal data set 
Result discussion 
Conclusion & future work 
Fig. 7. Data ﬂow diagram 
Table 4 Accuracy measure table 
Sl. no.
Total no. of 
signals 
TP 
FP 
TN 
FN 
Sensitivity 
Speciﬁcity 
PPV 
NPV 
Accuracy 
Training 
phase 
150
73
70
5
2
97
93
94
97
95 
Testing 
phase 
50
23
21
4
2
92
84
85
91
88.05 
5 
Conclusion and Future Work 
The entire study deals with various data sets of different diseases, and using RST, we 
have determined that cardiac arrest is the most signiﬁcant attribute. We have used a 
universal cardiac arrest data set for our study. Applying SVM to different groups, we 
have different results predicting cardiac arrest before 30 min of its actual occurrence. 
This study deals with two naive concepts. One is soft computing (RST), and the other 
is machine learning (SVM). This work can be extended to the ﬁeld of entertainment, 
and sports.

Early Prediction of Thoracic Diseases Using Rough Set Theory ...
433
References 
1. Pawlak Z (1982) Rough sets. Int J Comput Inform Sci 11:341–356 
2. Das S, Pradhan SK, Mishra S, Pradhan S, Pattnaik PK (2022) Diagnosis of cardiac problem 
using rough set theory and machine learning. Indian J Comput Sci Eng 13(4):1112–1131 
3. Das S, Pradhan SK, Mishra S, Pradhan S, Pattnaik PK (2022) A machine learning based 
approach for detection of pneumonia by analyzing chest X-ray images. In: Proceedings of 
the 2022 9th International Conference on Computing for Sustainable Global Development, 
INDIACom, pp 177–183 
4. Nayak SK, Pradhan SK, Mishra S, Pradhan S, Pattnaik PK (2021) Rough set technique to 
predict symptoms for malaria. In: Proceedings of the 2021 8th international conference on 
computing for sustainable global development, INDIACom 2021, pp 312–317 
5. Mishra S, Mohanty SP, Pradhan SK (2016) Reasons for employees need justice from legal 
bodies: a rough set approach. In: International conference on computing for sustainable global 
development (INDIACom) 
6. Das R, Mishra J, Mishra S, Pattnaik PK, Das S (2022) Mathematical modeling using rough set 
and random forest model to predict wind speed. In: Proceedings of the 2022 9th international 
conference on computing for sustainable global development, INDIACom 2022, pp 207–213 
7. Das R, Mishra J, Mishra S, Pattnaik PK (2022) Design of mathematical model for the prediction 
of rainfall. J Interdiscip Math 25(3): 587–613 
8. Kido S (2018) Detection and classiﬁcation of lung abnormalities by use of convolutional 
neural network (CNN) and regions with CNN features (R-CNN). In: Proceedings of the 2018 
international workshop on advanced image technology (IWAIT), Chiang Mai, Thailand, pp 
1–4 
9. Raoof S (2012) Interpretation of plain chest roentgenogram. Chest 141:545–558 
10. Reissig A, Gramegna A, Aliberti S (2012) The role of lung ultrasound in the diagnosis and 
follow-up of community-acquired pneumonia. Eur J Intern Med 23:391–397 
11. Nazerian P, Volpicelli G, Vanni S, Gigli C, Betti L, Bartolucci M, Zanobetti M, Ermini FR, 
Iannello C, Grifoni S (2015) Accuracy of lung ultrasound for the diagnosis of consolidations 
when compared to chest computed tomography. Am J Emerg Med 33:620–625 
12. Sheela CJ, Vanitha L (2014) Prediction of sudden cardiac death using support vector machine. 
In: 2014 International Conference on Circuit, Power and Computing Technologies [ICCPCT] 
13. Das S, Pradhan SK, Mishra S, Pradhan S, Pattnaik PK (2021) Analysis of heart diseases 
using soft computing technique. In: Proceedings - 2021 19th OITS international conference on 
information technology, OCIT 2021, 178–184 
14. Nayak SK, Pradhan SK, Mishra S, Pradhan S, Pattnaik PK (2022) Prediction of cardiac arrest 
using support vector machine and rough set. In: Proceedings of the 2022 9th international 
conference on computing for sustainable global development, INDIACom 2022, pp 164–172 
15. https://archive.physionet.org/physiobank/database/html/mitdbdir/intro.htm

Predicting Liver Disease from MRI 
with Machine Learning-Based Feature 
Extraction and Classiﬁcation Algorithms 
Snehal V. Laddha, Manish Yadav, Dhaval Dube, Mahansa Dhone, 
Madhav Sharma, and Rohini S. Ochawar 
Abstract Globally, liver disease is the leading cause of death for a huge number 
of people. Inﬂammation of the liver is caused by a number of factors. Diagnosing 
liver infection early is essential for more effective treatment. In the current scenario, 
sensors are employed to identify liver diseases. Precise classiﬁcation methods are 
necessary for the automatic diagnosis of illness samples. The cost of diagnosing this 
illness is high and complicated. The purpose of this study is to decrease the high 
cost of chronic liver disease diagnosis through prediction. This paper reviews the 
emerging techniques of data pre-processing, feature extraction, and classiﬁcation on 
liver MRI. The primary goal of the current work is to use clinical data to predict the 
presence or absence of liver disease from MRI by applying various Machine Learning 
methods. In this paper, we have performed feature extraction from liver MRI using 
the HOG method followed by the Random Forest algorithm for the classiﬁcation of 
images. With our approach, the accuracy achieved is 91.67%. 
Keywords MRI · Machine learning · Feature extraction · Classiﬁcation 
1 
Introduction 
Liver is one of the most crucial organs of the human body. It synthesizes proteins 
responsible for blood clotting and other functions. In order for our bodies to function 
properly, the liver must support almost every organ. Every year, around 2 million 
people worldwide pass away from liver disease [2]. Hence it becomes really important 
to focus our attention on liver related diseases [3]. A liver disease’s severity and type 
is determined by its symptoms. Symptoms of liver disease may not appear at an earlier 
stage, or the symptoms may be vague, such as weakness and fatigue and hence it 
becomes really difﬁcult and challenging to identify an unhealthy liver just based on 
the symptoms. An evaluation of the liver’s functional abilities is used for diagnosing
S. V. Laddha envelope symbol · M. Yadav · D. Dube · M. Dhone · M. Sharma · R. S. Ochawar 
Department of Electronics Engineering, Shri Ramdeobaba College of Engineering and 
Management, Nagpur, India 
e-mail: laddhasv2@rknec.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_37 
435

436
S. V. Laddha et al.
liver diseases [1]. For efﬁcient diagnosis, early detection and identiﬁcation of an 
unhealthy liver are crucial. 
However, the traditional methods that are used to perfectly identify or test a 
healthy liver at hospitals are relatively expensive. Hence there arises a need to ﬁnd 
an alternative & cost-efﬁcient solution for the early detection of liver disease. The 
recent advancements in the ﬁeld of machine learning and its applications in disease 
prediction is immense [7]. This motivates us to take advantage of the research work 
that has been done toward disease prediction and use it in our problem statement. 
For the purpose of predicting and diagnosing liver disease, machine learning has 
had a considerable impact on the biomedical area. Machine learning promises to 
enhance disease detection and prediction, two areas of interest in the biomedical 
profession, and they also improve the decision-making process’ objectivity [4–6]. 
Predictive analytics in medical decision-making has proven to be quite beneﬁcial. 
Machine learning algorithms can be programmed to provide insight into the types of 
treatments that will be most effective for the current patients by looking at data and 
outcomes of previous patients [1]. 
Considering the above factors, we decided to utilize machine learning algorithms 
for predictive analytics and liver disease prediction. In this paper, we have incor-
porated advanced machine learning methods for the classiﬁcation of a liver to be 
healthy or unhealthy. We utilized the MRI liver image dataset consisting of several 
healthy & unhealthy liver images and applied image preprocessing on the dataset 
followed by feature extraction and the ﬁnal step was classiﬁcation. The results that 
we got for the classiﬁcation are explained in further sections in this paper. 
2 
Related Work 
Over recent years, many researchers have used machine learning-based methods for 
the classiﬁcation of diseases in humans. 
The classiﬁcation techniques such as Naive Bayes, KNN (K-Nearest Neigh-
bors), Support Vector Machine (SVM), Random Forest (RF) and (MLP) Multi-Layer 
Perceptron were applied to the dataset for calculating the accuracy of prediction by 
various researchers [14–16]. Md. Julkar Nayeem et al. [17] performed the predic-
tion of hepatitis disease by using different data mining techniques. Their research 
showed that the random forest algorithm achieved an accuracy of 91.14% which was 
the highest. 
Pabitra Kumar Bhunia et al. [18] created a Heart Disease Prediction System 
(HDPS) to predict the amount of heart disease risk utilizing Logistic Regression, 
K Nearest Neighbor, Decision Tree, Random Forest Classiﬁer, and Support Vector 
Machine methods. The ﬁndings show that the Random Forest Classiﬁer and Support 
Vector Machine had the maximum accuracy of 90.32%, while logistic regression, 
the KNN classiﬁer, and the decision tree, respectively, achieved accuracy scores of 
87.09, 70.96, and 83.87%.

Predicting Liver Disease from MRI with Machine Learning ...
437
A. P. Pawlovsky et al. [19] In this paper a genetic algorithm for component selec-
tion has been developed to improve the accuracy of a kNN (k-Nearest Neighbor) 
method for breast cancer prognosis. The method for the UCI breast cancer data 
usually gives a 76% average accuracy, but we have found a combination of 16 
components that rises the accuracy to 79%. 
B. Poonguzharselvi et al. [20] proposed a system that identiﬁes the signiﬁcant 
features and then predicts whether or not a person has Liver Disease. They used 
genetic algorithms to identify the signiﬁcant features and then use those features to 
train different classiﬁcation models like k-Nearest, k-means, Random Forest, Support 
Vector Machines, Naïve Bayes, Logistic Regression, etc. Their research showed 
that from the various algorithms, Random forest performs the best followed with an 
accuracy of 84%. 
M. R. Haque et al. [21] this paper represents an expert scheme for the classiﬁcation 
of liver disorder using Random Forests (RFs) and Artiﬁcial Neural Networks (ANNs). 
The methods train the input features using tenfold cross validation fashion. The results 
obtained were, accuracy of 80% and 85.29% by RFs and ANNs respectively along 
with the F1 score of 75.86%. 
M. A. Kuzhippallil et al. [22] proposed a system that compares various classiﬁ-
cation models and visualization techniques used to predict liver disease with feature 
selection. Outlier detection is used to ﬁnd out the extreme deviating values and they 
are eliminated using isolation forest. The performance is measured in terms of accu-
racy, precision, recall f-measure and time complexity. The results showed that the 
accuracy of the random forest after feature selection and outlier elimination was 
found to be 88% which was better than other algorithms. 
The above papers have mainly focused on the application of classiﬁcation algo-
rithms in image-based disease prediction. However, we have devised that by utilizing 
additional pre-processing techniques on the dataset & using feature extraction 
before applying classiﬁcation—it leads to a better result in terms of increased 
accuracy [9–12] 
In this paper, we have applied the feature extraction technique HOG (Histogram 
of Oriented Gradients) before applying different classiﬁcation algorithms (k-NN, 
SVM, Decision Tree & Random Forest) on our dataset. And hence as a result, the 
accuracy that we have obtained proved to be better than the above proposed papers. 
3 
Methodology 
In this paper, we started our research with data collection which involves selecting the 
MRI liver images from the dataset which was then followed by data pre-processing. 
In our next step, we applied the HOG—Histogram of Oriented Gradients feature 
extraction method. 
In our ﬁnal step, we applied four classiﬁcation algorithms to the MRI Liver images 
to make a decision on whether the selected MRI liver images are healthy or unhealthy

438
S. V. Laddha et al.
Fig. 1 MRI liver image dataset (healthy—left and unhealthy—right) 
and noted the results. We have used one of the most popular classiﬁcation algorithms 
i.e. Random forest algorithm for classifying healthy and unhealthy liver images. 
3.1 
Data Collection 
In this experiment we have selected the MRI liver images dataset from kaggle which 
was available under the CHAOS—Combined (CT-MR) Healthy Abdominal Organ 
Segmentation global grand challenge. The MRI liver image data sets are collected 
retrospectively and randomly from the PACS of DEU Hospital. This dataset consists 
of 30 Healthy & 30 Unhealthy MRI liver images which is used for the purpose of 
training & testing of the machine learning algorithms (Fig. 1). 
3.2 
Data Pre-Processing 
In our work, we analyzed 60 liver MRI images out of which 30 were healthy & 30 were 
Unhealthy. To obtain accurate results we selected appropriate MRI images which 
were having relatively better image clarity & resolution. We performed resizing of 
the image so as to enable our analysis to be carried out uniformly and in a fast manner. 
3.3 
Feature Extraction 
The technique of turning raw data into numerical features that can be handled while 
keeping the information in the original data set is known as feature extraction [8]. 
Compared to using machine learning on the raw data directly, it produces supe-
rior outcomes. As a result of feature extraction, the classiﬁcation system is capable 
of detecting and isolating faults. Feature extraction is therefore a crucial step in 
designing fault detection and diagnosis systems based on classiﬁcation.

Predicting Liver Disease from MRI with Machine Learning ...
439
We have performed experimentation using various feature extraction methods and 
selected the HOG method as the participating method for analysis. 
HOG (Histogram of Oriented Gradients) 
A histogram of the edge direction change data in HOG serves as a representation of 
the features. 
Gradients Computation: 
This stage involves computing the horizontal gradients Gx and the vertical gradi-
ents Gy for each pixel included within a small geographic area known as a cell. The 
gradients at (x, y) may be calculated by letting I(x, y) be the intensity at pixel location 
(x, y). 
uper G Su bs cript  x  Baselin e lef t paren
thesis x comma y right parenthesis equals upper I left parenthesis x plus 1 comma y right parenthesis minus upper I left parenthesis x minus 1 comma y right parenthesis
u pper G S ub script y  B aseli ne left pa re
nthesis x comma y right parenthesis equals upper I left parenthesis x comma y plus 1 right parenthesis minus upper I left parenthesis x comma y minus 1 right parenthesis
The angle θ(x, y) and gradient magnitude M(x, y) are given by 
upper M left parenthesis x comma y right parenthesis equals StartRoot upper G Subscript x Baseline left parenthesis x comma y right parenthesis squared plus upper G Subscript y Baseline left parenthesis x comma y right parenthesis squared EndRoot
/
uper M lef t p arenthesi
s x comma y right parenthesis equals StartRoot upper G Subscript x Baseline left parenthesis x comma y right parenthesis squared plus upper G Subscript y Baseline left parenthesis x comma y right parenthesis squared EndRoot
thet a equals a r c t a n left parenthesis StartFraction upper G Subscript y Baseline left parenthesis x comma y right parenthesis Over upper G Subscript x Baseline left parenthesis x comma y right parenthesis EndFraction right parenthesis
theta equal
s a r c t a n left parenthesis StartFraction upper G Subscript y Baseline left parenthesis x comma y right parenthesis Over upper G Subscript x Baseline left parenthesis x comma y right parenthesis EndFraction right parenthesis
Orientation Binning: 
Each pixel’s gradient magnitude within a cell is divided into several orientation 
bins based on its gradient angle to create a histogram. 
Normalization and Feature Description: 
The cell histograms are standardized in this stage inside blocks of cells. A HOG 
feature descriptor is created by concatenating all of the histograms included inside 
a detection window. Figure 2 shows the visual representation of HOG method 
application on MRI T1 in-phase images.
3.4 
Classiﬁcation 
In machine learning and statistics, classiﬁcation is a supervised learning technique 
where a computer programme learns from the data that is provided to it and then 
produces new observations or classiﬁcations. On the basis of training data, the Clas-
siﬁcation algorithm is a Supervised Learning approach that is used to categorize 
fresh observations. A software that does classiﬁcation divides fresh observations into 
several classes or groups after learning from the provided dataset or observations.

440
S. V. Laddha et al.
Fig. 2 HOG analysis for MRI T1 in-phase image (input—left & output—right)
The main purpose of our study is to apply advanced & powerful classiﬁcation algo-
rithms to classify & detect an unhealthy liver image. We have considered 2 classes— 
Healthy & Unhealthy. We have implemented the various classiﬁcation algorithms 
such as k-NN, SVM, Decision tree, and Random forest methods in our experimen-
tations, and with the Random Forest (RF) classiﬁer the maximum accuracy was 
obtained. 
Random Forest Algorithm (RF) 
Supervised machine learning algorithms like random forest are frequently employed 
in classiﬁcation and regression issues. On various samples, it constructs decision 
trees and uses their average for classiﬁcation and majority vote for regression. The 
Random Forest Algorithm’s ability to handle data sets with both continuous variables, 
as in regression, and categorical variables, as in classiﬁcation, is one of its most 
crucial qualities. In terms of categorization issues, it delivers superior outcomes. The 
algorithm was implemented by Breiman [13]. The ﬁnal results are decided based on 
the majority as far as the decision tree results are concerned and the decision is made 
by either averaging or majority voting. 
3.5 
Evaluation Parameters 
For assessing the evaluation performance of the classiﬁcation algorithms, we have 
used different statistical measures such as confusion matrix, Accuracy, and F1 score 
(Fig. 3).
Confusion Matrix 
It is used in the interpretation of the model predictions systematically. It acts as the 
basic platform of representation for most of the classiﬁcation metrics (Fig. 4).
From the confusion matrix we can derive the following metrics.

Predicting Liver Disease from MRI with Machine Learning ...
441
Fig. 3 Random Forest algorithm for MRI liver image classiﬁcation
Fig. 4 Confusion matrix
Accuracy 
The prediction algorithm’s accuracy is measured as the proportion of all correctly 
predicted classes to the dataset’s actual classes. The model’s accuracy is calculated 
using Eq. (5). Any prediction model typically generates four distinct outcomes, 
including true negatives (TN), false positives (FP), true positives (TP) and false-
negatives (FN) [21]. 
upper A c  
c  u r  a  c y equals StartFraction upper T upper P plus upper T upper N Over upper T upper P plus upper T upper N plus upper F upper N plus upper F upper P EndFraction
u p per  A  c c  u  r  a  
c y equals StartFraction upper T upper P plus upper T upper N Over upper T upper P plus upper T upper N plus upper F upper N plus upper F upper P EndFraction

442
S. V. Laddha et al.
F1-Score 
Recall & Precision handle the imbalanced dataset efﬁciently. It represents the 
harmonic mean of the balanced scores for both recall and precision. F1 score is a 
weighted average of recall and precision. As we know in precision and in recall there 
is false negative and false positive so it also considers both of them. In most cases, 
the F1 score is more helpful than accuracy, particularly if your class is distributed 
unevenly. When false positives and false negatives cost about the same, accuracy 
performs best. It is preferable to include both Precision and Recall if the costs of 
false positives and false negatives are signiﬁcantly different. 
up er F B as el in e 1 upper S c o r e equals StartFraction 2 times upper P r e c i s i o n times upper R e c a l l Over upper P r e c i s i o n plus upper R e c a l l EndFraction
upper F Ba se line 1 
upper S c o r e equals StartFraction 2 times upper P r e c i s i o n times upper R e c a l l Over upper P r e c i s i o n plus upper R e c a l l EndFraction
4 
Results 
KNN, Decision Tree, Random Forest, and SVM were developed as classiﬁcation 
methods for the MRI Liver Patient Dataset. On the test set, the models that were 
created for the training set were assessed. Based on prediction accuracy, it was found 
that Random Forest had the highest accuracy (91.67%). 
The results showed that the RF algorithm performed the best, with an accuracy of 
91.67%, and F1-score of 91.67%. The SVM and k-NN algorithms showed similar 
performance, with accuracy values of 83.33% both. 
A comparison with traditional hand-crafted features and machine learning-based 
features showed that the latter outperformed the former in terms of classiﬁcation accu-
racy. This highlights the potential of using machine learning for feature extraction 
in medical imaging applications. 
The results of this study demonstrate the feasibility of using machine learning-
based feature extraction and classiﬁcation algorithms for predicting liver disease 
from MRI. The high accuracy and F1-score values suggest that the developed model 
has the potential to be used in clinical practice for supporting the diagnosis of liver 
disease (Table 1).
It is important to note that this study was conducted on a limited dataset and 
further validation on a larger and more diverse population is needed to conﬁrm the 
results and evaluate the model’s generalizability. 
5 
Conclusion 
The main goal of our research work is to develop a system that can accurately 
detect & identify an unhealthy liver using various advanced supervised machine-
learning classiﬁcation techniques. Early detection & identiﬁcation will lead to timely

Predicting Liver Disease from MRI with Machine Learning ...
443
Table 1 Performance metrics used for evaluating & performing the relative comparison of 
classiﬁcation algorithms namely Decision Tree, SVM k-NN, and Random Forest 
Classiﬁcation algorithm
Performance metrics 
Accuracy (%)
Confusion matrix
F1-Score 
Decision Tree
75.00
[[9 3] 
[3 9]] 
0.75 
SVM
83.33
[[10 2] 
[2 10]] 
0.83 
K-NN
83.33
[[10 2] 
[2 10]] 
0.83 
Random Forest
91.67
[[11 1] 
[1 11]] 
0.91
and proper diagnosis and thus prevent the risk of the disease from becoming chronic 
or fatal. The traditional detection methods adopted by hospitals are expensive and 
time consuming. So the proposed method provides a cost-effective solution. 
This paper has presented a novel approach for predicting liver disease from 
MRI images using machine learning-based feature extraction and classiﬁcation algo-
rithms. The results showed that the proposed method is capable of accurately diag-
nosing liver disease and outperforms existing methods in terms of accuracy and 
efﬁciency. The use of feature extraction techniques to identify relevant features from 
MRI images and the comparison of various classiﬁcation algorithms were key factors 
in the success of this approach. 
This research also highlights the potential for this method to be applied to other 
medical imaging domains, further expanding the impact of this work. The creation 
of a large dataset of MRI images and corresponding disease labels will also enable 
future research in this ﬁeld. 
Overall, this study has made a signiﬁcant contribution to the ﬁeld of liver disease 
diagnosis, demonstrating the potential for machine learning algorithms to be used 
in medical imaging. This approach holds promise for improving the accuracy and 
efﬁciency of liver disease diagnosis, ultimately beneﬁting patients and the healthcare 
system. 
References 
1. Kumar V, Garg ML (2018) Predictive analytics: a review of trends and techniques. Int J Comput 
Appl 182:31–37. https://doi.org/10.5120/ijca2018917434 
2. Mokdad AA, Lopez AD, Shahraz S, Lozano R, Mokdad AH, Stanaway J et al (2014) Liver 
cirrhosis mortality in 187 countries between 1980 and 2010: a systematic analysis. BMC Med 
12:145 
3. Byass P (2014) The global burden of liver disease: a challenge for methods and for public 
health. BMC Med 12(1):159

444
S. V. Laddha et al.
4. Auxilia LA (2018) Accuracy prediction using machine learning techniques for Indian patient 
liver disease. In: 2018 2nd international conference on trends in electronics and informatics 
(ICOEI). IEEE 
5. Hashem EM, Mabrouk MS (2014) A study of support vector machine algorithm for liver disease 
diagnosis. Am J Intell Syst 4:9–14 
6. Sajda P (2006) Machine learning for detection and diagnosis of disease. Annu Rev Biomed 
Eng 8:537–565 
7. Mahmud SM et al (2018) Machine learning based uniﬁed framework for diabetes prediction. 
In: Proceedings of the 2018 international conference on big data engineering and technology. 
ACM 
8. Albregtsen F, Nielsen B, Danielsen HE (2000) Adaptive gray level run length features from 
class distance matrices. Pattern Recognit. Proceedings. 15th International Conference on (Vol. 
3, pp. 738–741). IEEE 
9. Sastry SS, Kumari TV, Rao CN, Mallika K, Lakshminarayana S, Tiong HS (2012) Transition 
temperatures of thermotropic liquid crystals from the local binary gray level cooccurrence 
matrix. Adv Condens Matter Phys 2012:1–9. 
10. Mohanaiah P, Sathyanarayana P, GuruKumar L (2013) Image texture feature extraction using 
GLCM approach. Int J Sci Res Publ 3(5):1 
11. Ojala T, Pietikainen M, Harwood D (1996) A comparative study of texture measures with 
classiﬁcation based on featured distributions. Pattern Recognit 29(1):51–59 
12. Ohanty AK, Beberta S, Lenka SK (2011) Classifying benign and malignant mass using GLCM 
and GLRLM based texture features from mammograms. Int J Eng Res Appl 1(3):687–693 
13. Leo B (2001) Random Forests. Mach Learn 45(1):5–32 
14. Cortes C, Vapnik V (1995) Support-vector networks. Mach Learn 20(3):273 
15. Lopez-Bernal D, Balderas D, Ponce P, Molina A (2021) Education 4.0: teaching the basics 
of KNN, LDA and simple perceptron algorithms for binary classiﬁcation problems. Future 
Internet 13:193–206 
16. Decision
Trees.
https://dataaspirant.com/2017/01/30/how-decision-treealgorithmworks/. 
Accessed 5 Oct 2019 
17. Nayeem MJ, Rana, S, Alam F, Rahman, MA (2021) Prediction of hepatitis disease using k-
nearest neighbors, Naive Bayes, support vector machine, multi-layer perceptron and Random 
Forest. In: 2021 international conference on information and communication technology for 
sustainable development (ICICT4SD), pp 280–284. https://doi.org/10.1109/ICICT4SD50815. 
2021.9397013 
18. Bhunia PK, Debnath A, Mondal P, Monalisa DE, Ganguly K, Rakshit P (2021) Heart disease 
prediction using machine learning. Int J Eng Res Technol (IJERT) NCETER – 2021 09(11) 
19. Pawlovsky P, Matsuhashi H (2017) The use of a novel genetic algorithm in component selection 
for a kNN method for breast cancer prognosis. In: 2017 global medical engineering physics 
exchanges/pan American health care exchanges (GMEPE/PAHCE), Tuxtla Gutierrez, Mexico, 
pp 1–5. https://doi.org/10.1109/GMEPE-PAHCE.2017.797208 
20. Poonguzharselvi B, Ashraf MMA, Subhash VVSS, Karunakaran S (2021) Prediction of liver 
disease using machine learning algorithm and genetic algorithm. Ann. RSCB, 2347 
21. Haque MR, Islam MM, Iqbal H, Reza MS, Hasan MK (2018) Performance evaluation of 
Random Forests and artiﬁcial neural networks for the classiﬁcation of liver disorder. In: 2018 
international conference on computer, communication, chemical, material and electronic engi-
neering (IC4ME2), Rajshahi, Bangladesh, pp 1–5. https://doi.org/10.1109/IC4ME2.2018.846 
5658 
22. Kuzhippallil MA, Joseph C, Kannan A (2020) Comparative analysis of machine learning 
techniques for Indian liver disease patients. In: 2020 6th international conference on advanced 
computing and communication systems (ICACCS), Coimbatore, India, pp 778–782. https:// 
doi.org/10.1109/ICACCS48705.2020.9074368

An Improved Genetic Algorithm Based 
on Chi-Square Crossover for Text 
Categorization 
Gyananjaya Tripathy and Aakanksha Sharaff 
Abstract Text classiﬁcation has gained importance due to the quickly rising content 
volume. During the text categorization process, it is necessary to complete tasks 
including extracting relevant information from different viewpoints, reducing the 
high feature space, and improving efﬁciency. Many studies on feature selection have 
been conducted, but increasing efﬁciency by reducing features remains a challenge. 
To evaluate its effectiveness, the Amazon review dataset is used in this proposed 
work. A chi-square-based enhanced genetic algorithm (CSEGA) approach is used 
in this paper to achieve the purpose of the study. The execution of the task includes 
the preprocessing task, followed by the optimization process for the selection of 
the optimal features. The unique crossover and selection process have made this 
work superior to other algorithms. With 93.0% accuracy, 94.5% precision, 90.5% 
recall, and a 92.47% F-score, the proposed approach outperforms other state-of-the 
art algorithms with fewer features. 
Keywords Chi-square · Two-phase crossover · Unique selection process · Feature 
selection 
1 
Introduction 
The volume of digitized text documents grows exponentially as internet usage rises. 
As a result, text data organization becomes more signiﬁcant. According to the classi-
ﬁcation categories, classiﬁcation divides texts into a variety of predetermined groups 
[1]. The process of extracting valuable and favorable information from the text is 
signiﬁcant. The difﬁculty of categorizing text is generally due to the high dimen-
sion of features, redundant and unnecessary terms, and noisy data [2]. The two
G. Tripathy envelope symbol · A. Sharaff 
National Institute of Technology, Raipur, India 
e-mail: gtripathy.phd2020.cse@nitrr.ac.in 
A. Sharaff 
e-mail: asharaff.cs@nitrr.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_38 
445

446
G. Tripathy and A. Sharaff
most widely used dimensionality reduction techniques in the research are feature 
extraction and selection, which aim to solve these issues and improve the efﬁcacy 
of text categorization [3]. The process of selecting a small set of features that are 
ideally required and enough to deﬁne the desired result is known as feature selection 
[4]. Additionally, feature selection approaches aim to select the best terms from the 
primary set rather than trying to invent new ones [5]. 
In the ﬁelds of data mining and pattern recognition, feature selection has been a 
popular research topic. The overall search space, including all conceivable subsets, 
to ﬁnd out the most pertinent and unique features is 2n, where n represents the total 
features [6]. The most suitable features are determined through systematic explo-
ration; however, even for average-sized datasets, this is typically not computationally 
possible. It is necessary to look for a solution that is computationally efﬁcient and 
valuable in terms of quality because evaluating all potential subgroups is quite expen-
sive [7]. Metaheuristic algorithms are frequently used in feature selection techniques 
to reduce rising computing complexity. These techniques are capable of accurately 
and quickly optimizing the challenge of feature selection. 
The major contributions of the CSEGA are:
●The preprocessing task has been carried out at the initial level to clean the input 
data for further processing.
●The development and effectiveness of the proposed CSEGA model used for the 
feature selection process.
●The effect of the proposed unique crossover process is discussed and analyzed.
●Improvement in performance with fewer features compared to other techniques. 
The rest of this paper is structured as follows: A literature review is demonstrated 
in Sect. 2, and an overview of the research technique (the pre-processing stage and 
the CSEGA model) is discussed in Sect. 3. The experimental designs and the review 
of the ﬁndings are presented in Sect. 4, and ﬁnally, Sect. 5 provides the conclusion. 
2 
Related Work 
Optimization algorithms have been demonstrated to be effective in a variety of 
applications in the literature. Additionally, text categorization has effectively used 
optimization and meta-heuristic algorithms. 
To minimize the feature space dimensionality for text classiﬁcation, most papers 
in the literature have used feature selection approaches. As a result, many meta-
heuristic techniques have been used in conjunction with feature selection techniques. 
An enhanced genetic algorithm-based feature selection technique was demonstrated 
by Sokhangoee, et al., [8] for categorizing tweets connected to disasters. The model 
performs better compared to others, but the number of features is still high. 
A novel PSO-based feature selection technique [9] is described for the investi-
gation of laser-induced breakdown spectroscopy. This approach tries to combine 
the beneﬁts of coating and ﬁltering techniques. A PSO-based feature selection

Title Suppressed Due to Excessive Length
447
method [10] with several classiﬁers is developed to enhance classiﬁcation perfor-
mance while lowering computational costs. In this study, the problem of feature 
selection in a complex dataset is addressed using a unique self-adaptive param-
eter and strategy. According to the results, these methods signiﬁcantly improved 
particle optimization algorithms’ capacity to explore large datasets. Reducing the 
number of features is still a challenge for this model. Additionally, a graph-based 
method [11] for selecting features is developed to improve disease detection preci-
sion. In this approach, a strategy for the particle initialization is put forth, utilizing the 
node centrality criterion. The model outperforms other models while compromising 
precision. 
A strategy for the ABC-based multi-objective feature selection method [12] uses  
two different operators to enhance search performance and ABC search strategy 
convergence. A multi-objective cognitive model and a sample feature selection mini-
mization technique are combined to provide an ABC-based feature selection method 
[13]. To evaluate the credit risk assessment, a hierarchy-based model [14] is devel-
oped that combines expert knowledge and GA for feature selection, which is utilized 
to rank the relative relevance of dataset attributes. A community detection-based 
approach has been put out to ﬁnd feature clusters, which are collections of related 
features. The proposed approach to picking redundant characteristics is prevented 
by grouping related features. 
Over the ﬁlter method, the wrapper method shows a little lower performance but 
with a huge margin of computational time. Different wrapper-based metaheuristic 
approaches have been used for feature selection purposes. The biggest challenge of 
using the metaheuristic method is the optimal feature set. Different selected feature 
set gives different performance results. Again, more features will take more compu-
tational time. As a result, a new metaheuristic method is needed, which will select 
fewer features to produce high performance. 
3 
Proposed Method 
The presence of many redundant and irrelevant characteristics in real-world datasets 
might negatively affect the learning rate and the effectiveness of the trained model. 
In order to eliminate duplicate and irrelevant features from a given dataset, feature 
selection is a crucial phase in data preparation for data mining. In this section, the 
description of the CSEGA model, along with the preprocessing task, is discussed. 
Evolutionary-based approaches are based on generation and population. In this 
paper, a new hybrid genetic algorithm (CSEGA) is developed to choose the nearly 
optimal feature set to solve the problem faced by other techniques. The main disad-
vantage of the genetic algorithm (GA) is the convergence problem. The unique 
crossover and selection process makes the proposed approach eligible to overcome 
the drawbacks in GA. The block diagram of the CSEGA model is shown in Fig. 1.

448
G. Tripathy and A. Sharaff
Fig. 1 Block diagram of the proposed work 
3.1 
Data Preprocessing 
The data used for the text classiﬁcation is raw data, and it cannot be fed into the 
model directly. So, some preprocessing tasks are required to prepare the data in the 
required format. The preprocessing task involves tokenization, stopword and garbage 
removal, and lemmatization, as shown in Fig. 1. 
Each review is divided into tokens in the process of tokenization. Tokens are the 
smallest part of the sentence. The tokenization process makes the text data ready for 
stopword and garbage removal. Stopwords are the most common words used in the 
sentence without carrying any sentiment. The words like “a”, “an”, “the”, “myself”, 
“yourself” etc. are some examples of stopwords. These words are required to be 
removed from the sentence to reduce its complexity, as they do not have any impact 
on sentiment evaluation. Similarly, garbage words like URLs need to be removed 
using regular expressions. Once the reviews are cleaned, it is required to fetch the 
root words of all the used words using lemmatization. 
3.2 
Vectorization 
The preprocessed data cannot be applied to the model directly. To make the prepro-
cessed data understandable to the model, the words need to be converted to vector 
form using the TF-IDF vectorizer. 
3.3 
Proposed CSEGA Approach 
The most important phase of the experiment is the execution of the feature selection 
model. The proposed CSEGA model is used for this purpose. The vector form of 
data collected from the vectorization phase is the input data to the proposed model.

Title Suppressed Due to Excessive Length
449
Fig. 2 Flowchart of the proposed CSEGA model 
The efﬁciency of the model basically reduces due to the high volume of features; 
hence, the key focus here is to minimize the number of features. The proposed model 
imposes the hybridization of the modiﬁed GA with chi-square. This combination is so 
effective in reducing the features while maintaining the performance of the model. 
Some parameters are initialized before applying the genetic operators. Parameter 
initialization for the proposed model is given in Table 1. 
Selection, crossover, and mutation are the genetic operators used in the given 
sequence for the GA. As mentioned in Table 1, the initial population is chosen 
as eight. The accuracy as a ﬁtness value for each solution is then calculated using
Table 1 Parameter 
initialization of CSEGA
Parameter
Explanation
Value 
RP
Initial random population size
8 
PS
Parent size
4 
max_iter
Maximum iterations
1500 
CR
Rate of crossover
0.5 
MR
Rate of mutation
0.01 
Kbest
Top k features for Chi-Square
480 

450
G. Tripathy and A. Sharaff
Fig. 3 Covariance graph 
support vector machines (SVM). The topmost four (Parent size) solutions are selected 
as parents on the basis of their ﬁtness value. 
Once the selection of the parents is made, the crossover and mutation are applied. 
The genetic operators are used for exploration and exploitation. The unique crossover 
design makes the model possible to get high exploration. In the proposed model, the 
two-phase crossover is implemented. In the ﬁrst phase crossover, the topmost solution 
is kept as it is, and the uniform crossover is applied to the remaining solutions. With 
the help of this approach, each time, new offspring are generated from the local best 
solution. This will lead the solution toward the global optimum with high exploration. 
In the second phase crossover, the chi-square is used. The main focus here is to 
minimize the number of features along with exploration. Here, the top k features 
(i.e., Kbest = 480 based on Fig. 3) are selected using chi-square. The covariance 
graph is shown in Fig. 3. Then the logical AND operation is applied between the 
parents and the selected Kbest from the chi-square to create a new parent, and a 
crossover probability of 0.5 is then applied to it to get the offspring. Finally, the new 
population is explored by applying the mutation operator to all the offspring derived 
from the ﬁrst and second phases of the crossover. The global best solution is modiﬁed 
after each iteration. The proposed model is executed for the ﬁxed number of iterations 
(i.e., 1500) as set at the initialization stage. The model returns the nearly optimal set 
of feature sets after the complete execution. The ﬂowchart of the proposed CSEGA 
model is shown in Fig. 2, and the global best ﬁtness value in terms of accuracy for 
each iteration is shown in Fig. 4.
4 
Experimental Setup and Result Analysis 
This section provides the details of the experimental setups before carrying out the 
execution and the complete result analysis. The complete experiment is accomplished 
using an Intel core-i3 processor, 12 GB RAM memory with 250 GB SSD on Windows 
10 machine.

Title Suppressed Due to Excessive Length
451
Fig. 4 Global accuracy
4.1 
Dataset 
Experiments are conducted using the Amazon Review dataset collected from the 
Kaggle repository [15], having multiple unique reviews with 0 and 1 labels. Labels 
0 and 1 represent negative and positive sentiments, respectively. 
4.2 
Result Analysis 
Optimal features selected using the CSEGA algorithm are used to measure the quality 
of the model with some evaluation metrics like accuracy, recall, precision, and F-
score value using Eqs. 1–4. To prove the efﬁcacy of the CSEGA model, the resulting 
outcome is compared with the outcomes of some other metaheuristic algorithms, 
like the Flower Pollination Algorithm (FPA), Differential Evolution (DE), Whale 
Optimization Algorithm (WOA), Parallel Particle Swarm Optimization (PPSO), 
and Social Spider Optimization (SSO) [16]. The performance comparison between 
different feature selection algorithms is shown in Table 2. The pseudocode of the 
proposed model is given in Algorithm 1. 
upper A c  c  u r  a  c y equ al s l ef t p ar ent
hesis upper T upper N plus upper T upper P right parenthesis divided by left parenthesis upper T upper N plus upper F upper N plus upper T upper P plus upper F upper P right parenthesis
upper P r e c i s i  o  n 
equals upper T upper P divided by left parenthesis upper T upper P plus upper F upper P right parenthesis
upper R  e  c a l l equ
als upper T upper P divided by left parenthesis upper T upper P plus upper F upper N right parenthesis
no rm al up er  F minu s s c o r  e  equ
als upper T upper P divided by left parenthesis upper T upper P plus 0.5 left parenthesis upper F upper N plus upper F upper P right parenthesis right parenthesis
The proposed algorithm produced effective results with fewer features compared 
to other metaheuristic algorithms as shown in Table 3, and the corresponding 
graphical representation is shown in Fig. 5.

452
G. Tripathy and A. Sharaff
Table 2 Performance comparison of different feature selection algorithms 
Metric
CSEGA
DE
WOA
FPA
PPSO
SSO 
Accuracy
93.00
83.00
84.50
76.50
74.00
76.50 
Precision
94.51
91.25
89.66
90.77
76.09
82.72 
Recall
90.53
73.00
78.00
59.00
70.00
67.00 
F-score
92.47
81.11
83.42
71.52
72.92
74.03 
AUC
0.92
0.86
0.89
0.77
0.78
0.83 
Average precision (AP)
0.91
0.87
0.87
0.75
0.76
0.84
Table 3 Selected features using different feature selection algorithms 
FS algorithms
CSEGA
DE
WOA
FPA
PPSO
SSO
Total features 
Number of Features
148
349
409
312
335
286
659 
Algorithm 1: CSEGA based feature selection technique 
Data: CR = 0.5, MR = 0.01, max_iter = 1500, PS = 4, RP = 8, Ip = 
Initialpopulation, Kbest = 480, Document D, g_best = 0 
Result: Nearly optimal selected features 
Calculate the vector matrix VM from D 
Create an initial population Ip of size RP 
result
; 
while i
 max iter do 
p.f it() # according to Eqn. (1) 
Ip
p) #in descending order of fitness 
select top 4 solutions of Ip as parent_1 
offspring_
 First phase crossover with CR rate on parent_1 except 1st 
parent 
-Square(VM, Kbest); 
for each solution  parent_1 do 
parent_
-square_best  solution; 
end 
 Second phase crossover with CR rate with parent_2 
Mutation of MR rate applied to offspring_1 and offspring_2 
Ip
; 
fitness 
 I 
sort(I 
1 
CS_best 
 Chi 
2 
chi 
offspring_2 
offspring_1.add(offspring_2) 
l_best
max(fitness); 
if g_best < l_best then 
g_best
 l_best; 
end 
result.add(g_best); 
end 
p.fit( ); 
fitness 
 I 
g_best
max(fitness); 
return solution(g_best)

Title Suppressed Due to Excessive Length
453
Fig. 5 Selected features 
Fig. 6 Performance 
evaluation 
As per Table 2, the proposed CSEGA model outperforms other algorithms with 
93.0% accuracy, 94.51% precision, 90.53% recall, 92.47% F-score, an AUC value 
of 0.92, and an average precision value of 0.91. The graphical representations of the 
performances are shown in Figs. 6, 7 and 8. 
Fig. 7 Average precision 
performance

454
G. Tripathy and A. Sharaff
Fig. 8 AUC performance 
5 
Conclusion 
This research introduced an efﬁcient feature selection technique that works with 
the population-based approach in collaboration with the ﬁlter-based approach. The 
features obtained from the proposed approach provide sufﬁciently high performance. 
The hybridization of the model makes it possible to serve efﬁciently with fewer 
features to reduce complexity. The major issue of the metaheuristic approach, i.e., 
convergence to local minima, is solved using the proposed method. The two-phase 
crossover method used in this work provides high levels of exploration and exploita-
tion. In terms of accuracy, recall, precision, F-score, AUC, and average precision 
value, the CSEGA outperforms other metaheuristic algorithms. The suggested work 
will eventually be expanded in order to address the unsupervised problem using the 
real-time dataset. 
Acknowledgements The authors would like to acknowledge the National Institute of Technology, 
Raipur, for providing the required infrastructure and facilities for doing research work. The authors 
truly appreciate all of the reviewers’ insightful remarks and recommendations, which helped to 
improve the quality of the manuscript. 
References 
1. Hu J, Gui W, Heidari AA, Cai Z, Liang G, Chen H, Pan Z (2022) Dispersed foraging slime 
mould algorithm: continuous and binary variants for global optimization and wrapper-based 
feature selection. Knowl Based Syst 237:107761 
2. Srinivasarao U, Sharaff A (2021) Sentiment analysis from email pattern using feature selection 
algorithm. Expert Syst 
3. Bommert A, Sun X, Bischl B, Rahnenführer J, Lang M (2020) Benchmark for ﬁlter methods for 
feature selection in high-dimensional classiﬁcation data. Comput Stat Data Anal 143:106839 
4. Houssein EH, Saber E, Ali AA, Wazery YM (2022) Centroid mutation-based search and rescue 
optimization algorithm for feature selection and classiﬁcation. Expert Syst Appl 191:116235 
5. Hu G, Du B, Wang X, Wei G (2022) An enhanced black widow optimization algorithm for 
feature selection. Knowl Based Syst 235:107638

Title Suppressed Due to Excessive Length
455
6. Alsahaf A, Petkov N, Shenoy V, Azzopardi G (2022) A framework for feature selection through 
boosting. Expert Syst Appl 187:115895 
7. Srinivasarao U, Sharaff A (2021) Email sentiment classiﬁcation using lexicon-based opinion 
labeling. In: Singh B, Coello Coello CA, Jindal P, Verma P (eds) Intelligent computing and 
communication systems, pp 211–218. Springer, Singapore 
8. Sokhangoee ZF, Rezapour A (2022) A novel approach for spam detection based on association 
rule mining and genetic algorithm. Comput Electr Eng 97:107655 
9. Yan C, Liang J, Zhao M, Zhang X, Zhang T, Li H (2019) A novel hybrid feature selection 
strategy in quantitative analysis of laser-induced breakdown spectroscopy. Anal Chim Acta 
1080:35–42 
10. Xue Y, Tang T, Pang W, Liu AX (2020) Self-adaptive parameter and strategy based particle 
swarm optimization for large-scale feature selection problems with multiple classiﬁers. Appl 
Soft Comput 88:106031 
11. Rostami M, Forouzandeh S, Berahmand K, Soltani M (2020) Integration of multi-objective 
PSO based feature selection and node centrality for medical datasets. Genomics 112:4370–4384 
12. Zhang Y, Cheng S, Shi Y, Gong D, Zhao X (2019) Cost-sensitive feature selection using 
two-archive multi-objective artiﬁcial bee colony algorithm. Expert Syst Appl 137:46–58 
13. Wang X, Zhang Y, Sun X, Wang Y, Du C (2020) Multi-objective feature selection based on 
artiﬁcial bee colony: an acceleration approach with variable sample size. Appl Soft Comput 
88:106041 
14. Shang R, Zhao K, Zhang W, Feng J, Li Y, Jiao L (2022) Evolutionary multiobjective overlap-
ping community detection based on similarity matrix and node correction. Appl Soft Comput 
127:109397 
15. Kaghazgarian M (2015) Sentiment labelled sentences data set. https://www.kaggle.com/dat 
asets/marklvl/sentiment-labelledsentences-data-set. Accessed 10 Dec 2022 
16. van Thieu N, Mirjalili S (2022) MEALPY: a framework of the state-of-the-art meta-heuristic 
algorithms in Python, 7068595

Tuna Optimization Algorithm-Based 
Data Placement and Scheduling in Edge 
Computing Environments 
P. Jayalakshmi and S. S. Subashka Ramesh 
Abstract Mobile edge computing (MEC) represents the promising technology that 
targets at facilitating different resources for processing and storing near the edge of 
mobile devices. Nevertheless, limited availability of resources in MECs possesses 
a necessity for adopting an appropriate management for preventing wastage of 
resources. In speciﬁc, the scheduling of workﬂow indicates a process that attempts in 
the mapping of tasks to the most suitable set of resources available in MECs based on 
the satisfaction of the objectives. In this paper, Tuna Optimization Algorithm-based 
Data Placement and Scheduling (TOA-DPS) is proposed with improved convergence 
by preventing the problems of local optima to map tasks to most suitable resources. 
It adopted a method of task prioritization for determining the order in which the 
tasks in the scientiﬁc workﬂows are executed. Further, Tuna Optimization Algorithm 
(TOA) is utilized for attaining data-demanding workﬂow scheduling along with data 
placement using Dynamic Voltage and Frequency Scaling (DVFS) in MEC environ-
ments. The performance evaluation of proposed TOA-DPS-based scheduling mech-
anism is conducted using extensive simulations carried out over renowned scien-
tiﬁc workﬂows of various sizes. The outcomes of the proposed TOA-DPS-based 
scheduling scheme conﬁrmed better performance of data access by 21.38%, and 
minimized energy consumption by 19.84%, better than the baseline approaches used 
for investigation. 
Keywords Mobile edge computing (MEC) · Tuna Optimization Algorithm 
(TOA) · Dynamic Voltage and Frequency Scaling (DVFS) · Workﬂow 
optimization · Data intensive tasks
P. Jayalakshmi envelope symbol · S. S. Subashka Ramesh 
Department of CSE, SRM Institute of Science and Technology, Ramapuram, Chennai, Tamilnadu, 
India 
e-mail: jayalakp@srmist.edu.in 
S. S. Subashka Ramesh 
e-mail: subashka@gmail.com 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_39 
457

458
P. Jayalakshmi and S. S. Subashka Ramesh
1 
Introduction 
Internet of Things (IoT) comprises of wirelessly connected devices including vehi-
cles, home appliances, RFID tags, sensors, mobile phones and so on [1, 2]. These 
devices are capable of cooperating among themselves by involving a distinct 
addressing scheme but suffer from reduced amount of computation resources and 
battery power. These devices may incessantly generate substantial quantity of data 
and transmit them over network [3]. Cloud computing [4] is efﬁcient in delivering 
mandatory computational as well as storage resources to IoT involving increased 
trafﬁc and delays. There is a demand for increased amounts of resources at edges of a 
network to ofﬂoad [5] IoT-based workload on them. Mobile Edge Computing (MEC) 
is a technology that is capable of handling these demands. It offers a distributed 
computational model for handling tasks, networking and storing data at network 
edges [6]. It is capable of dealing with resource demands of IoT applications and 
diminishes the communication overheads along with latency [7]. Every MEC server 
provides a virtual environment involving computational device, unit to support wire-
less communication along with data cards. The virtual resources in MEC servers 
handle requests of mobile or ﬁxed devices. Usually, a mobile device can connect to 
MEC servers by using a wireless connection involving a single-hop using wireless 
interfaces like 4G LTE devices, Wi-Fi, Bluetooth, etc. MEC servers are associated 
with cloud infrastructure through the Internet [8]. MEC drops latency and improves 
content delivery as well as data handling speed. It also affords increased scalability as 
well as reliability by stopping single points of failure in dealing with the requests of 
devices. Additionally, MEC reconstructs missing data due to noise and other issues 
in devices [9]. Nevertheless, it demands increased storage with advanced infrastruc-
ture that raises deployment cost of MECs. Moreover, it is susceptible to numerous 
security attacks [10]. 
Effective resource organization is tedious in MEC and provides improved services 
to MDs. In such environments, scheduling schemes allocate tasks to suitable collec-
tion of MEC servers [11]. These scheduling mechanisms handle virtual resources 
and emphasis on restraints as well as deadlines of workﬂows. The architecture-based 
scheduling mechanisms of MEC are categorized into centralized and decentralized 
scheduling schemes. Depending on their competencies in dealing with the tasks, they 
are categorized into online and ofﬂine schemes. Depending on the algorithm used, 
they are classiﬁed into heuristic and meta-heuristic classes which involve single or 
multiple objectives. These scheduling schemes focus on reducing cost, communica-
tion overhead, makespan and amount of energy consumed in addition to improving 
reliability, security as well as availability. 
Homogenous Virtual Machines (VMs) are used in MEC computing environments, 
whereas heterogeneous VMs are used by others. The amount of energy consumed is 
a critical factor that has to be taken into consideration in all scheduling mechanisms. 
Energy models used by the scheduling mechanisms may be traditional, renewable

Tuna Optimization Algorithm-Based Data Placement and Scheduling ...
459
and Dynamic Voltage and Frequency Scaling (DVFS)-based models. DVFS is a stim-
ulating scheme which reduces processor frequency to alleviate power usage. Never-
theless, dropping processor frequency raises time taken for execution. Frequency 
reduction must be focused by taking into consideration deadlines along with Quality 
of Service (QoS) features. An indispensable issue in scientiﬁc workﬂow scheduling 
is placement policy of data which can lessen data transmission as well as storage 
cost in MEC environment. Scheduling workﬂows and placement of data prove to be 
NP-hard problems [12]. Nevertheless, less amount of research is carried out on ideal 
placement of data combined with scheduling policies to deal with datasets mandatory 
for scientiﬁc workﬂows. Numerous heuristics as well as meta-heuristic scheduling 
schemes [13] deal with scheduling individual tasks. Less amount of work is carried 
out in workﬂow scheduling. 
Meta-heuristic algorithms should be categorized as evolutionary and SI algo-
rithms, physics as well as bio-inspired algorithms. Evolutionary algorithms like 
Genetic Algorithm (GA) simulate biological natures like natural selection, recom-
bination as well as mutation in determining an ideal solution. Swarm Intelligence 
(SI) algorithms are stimulated by collaborating swarms including birds, bees, ants 
or ﬁshes and impersonate their behaviors. Some SI-based schemes include Ant 
Colony Optimization (ACO), Gravitational Search Algorithm (GSA), Artiﬁcial Bee 
Colony (ABC) etc. Physics-based algorithms like gravitational search mimics natural 
phenomenon. Bio-inspired algorithms like Butterﬂy Optimization Algorithm (BOA) 
[14] mimics the nature of biological entities and diverse forms of algorithm are 
available. BOA is employed for handling incessant optimization problems. It is 
population-dependent stimulated by butterﬂies’ motion in food hunting behaviors. 
Nevertheless, notwithstanding BOA’s attainment in incessant optimization, it may 
not be used without altering discrete problems. Concerning the discrete behavior of 
scheduling problems, there is room for some enhancement. Algorithms are modiﬁed 
to make it appropriate for discrete optimization problems. 
In this paper, Tuna Optimization Algorithm-based Data Placement and Scheduling 
(TOA-DPS) is proposed with improved convergence by preventing the problems 
of local optima to map tasks to most suitable resources. It adopted a method of 
task prioritization for determining order in which the tasks in the scientiﬁc work-
ﬂows are executed. Further, Tuna Optimization Algorithm (TOA) is utilized for 
attaining data-intensive workﬂow scheduling and data placement using Dynamic 
Voltage and Frequency Scaling (DVFS) in MEC environments. The performance 
evaluation of proposed TOA-DPS-based scheduling mechanism is conducted using 
simulations conducted over the renowned scientiﬁc workﬂows of various sizes. From 
the outcomes of the proposed TOA-DPS-based scheduling scheme conﬁrmed better 
performance of data access by 21.38%, and minimized energy consumption by 
19.84% in contrast to standard schemes taken for investigation.

460
P. Jayalakshmi and S. S. Subashka Ramesh
2 
Related Work 
Initially, Li et al. [15] have presented cache-aware task scheduling mechanism in 
EC. Initially, a combined utility function is obtained based on transmission cost 
of data chunk, cache value along with replacement penalty. Data is cached on 
ideal edge servers to improve combined utility value. Once caches are positioned, 
a scheme is proposed to schedule tasks based on cache locality. The scheduling 
problem is modelled as a weighted Bipartite Graph. The edge weights on graphs 
are impacted by positions of the demanded data. Maximal weighted matching amid 
tasks as well as resources are obtained every moment. The proposed mechanisms 
involve polynomial time which is tolerable in EC. Further, several experiments show 
that cache-based task scheduling algorithms outdo other standard algorithms based 
on cache hit ratio, transmission and task response time, data locality along with cost 
of energy consumption. Shao et al. [16] have proposed a system to manage replica 
that comprises of dynamic duplication maker, a dedicated cost-based scheduler for 
placing data, a system viewer with data security tools for cooperative edge as well 
as cloud computing systems. By taking into consideration task dependence, data 
consistency as well as sharing, scheduling data for workﬂows is modelled as Integer 
Programming Problem (ILP). Rapid meta-heuristic mechanism is proposed to solve 
the same. From the results, it is evident that the algorithm can offer improved system 
performance in contrast to benchmarked schemes, creating appropriate amount of 
data copies and search for high quality copy positioning solution while dropping the 
complete data access costs with deadline. 
Lin et al. [17] have proposed self-dynamic Discrete Particle Swarm Optimiza-
tion algorithm with Genetic Algorithm operators (GA-DPSO) to reduce time taken 
for data transmission while positioning data for scientiﬁc workﬂows. This mech-
anism considers features of data positioning by uniting edge as well as cloud 
computing. Further, it considers the parameters which have an impact on trans-
mission delay including bandwidth amid data centers, along with quantity as well 
as storage volume of data centers. Crossover and mutation operators of GA are used 
for avoiding premature convergence of conventional PSO algorithm that improves 
variety of population evolution and efﬁciently reduces time taken for data transmis-
sion. From the outcomes, it is evident that data placement approach using GA-DPSO 
efﬁciently reduces the time involved in data transmission while executing workﬂows 
by combining edge and cloud computing. Hosseinzadeh et al. [18] have presented 
Discrete Butterﬂy Optimization Algorithm (DBOA) that implements Levy Flight 
for enhancing convergence speed and overcoming local optima problems. They have 
prioritized tasks to determine order in which task are to be executed in scientiﬁc 
workﬂows. DBOA for DVFS-based scheduling of data-demanding workﬂows and 
data positioning is proposed for MEC environments. For assessing performance of 
scheduling schemes, simulations are carried out on numerous workﬂows with diverse 
sizes. The investigational outcomes show that the method outperforms algorithms 
based on the amount of energy consumed, overheads related to data access etc.

Tuna Optimization Algorithm-Based Data Placement and Scheduling ...
461
Chen et al. [19] have combined the merits of MEC as well as cloud computing, 
and GAPSO based scheme is proposed for scientiﬁc workﬂows to determine ideal 
method of data placement. Initially, a combined model is proposed for placement of 
data to ﬁnd a cost-effective mechanism that takes into consideration diverse features 
amid MEC and cloud computing along with inﬂuence of latency on transmission 
costs. The merits of GA and PSO are combined to enhance the model that uses PSO’s 
quick convergence and GA’s crossover and mutation functions. The proposed scheme 
involves reduced data placement costs. Vivekanandan and Gnanasekaran (2021) [20] 
have proposed Hybrid Harris Hawk-Salp Swarm-based Optimization, Data Place-
ment and Task Scheduling (HHHSS-ODPTS) mechanism for enhancing experience 
in EC. Placement of data is efﬁciently performed by considering reputation and user 
inclination of data blocks, storage capacity and ratio of replacement linked with 
edge servers. This mechanism uses 2/3-approximation for crucial task mapping with 
produced containers of server. From the outcomes, it is evident that HHHSS-ODPTS 
mechanism offers improved response times of data and tasks, amount of substituted 
data blocks, task hit rate with varying capacities of data storage, amount of mandatory 
data blocks along with data popularity. 
Li et al. [21] have focused on these issues and have proposed a data placement 
scheme with efﬁcient task scheduling. Several performance indicators are used in 
the proposed data placement mechanism. Straggling nodes are detected, and sensible 
distribution of resources is considered when task is scheduled. About data placement, 
performance is analyzed for 800 ﬁles in terms of safety level. In the case of task 
scheduling, the proposed scheme involves the least time overhead. Qi et al. [22] have  
considered computation ofﬂoading of several varied edge servers in smart prosthesis. 
Initially, problem deﬁnition is considered along with an overview of MEC-dependent 
task ofﬂoading model for Deep Neural Network (DNN). By taking into considera-
tion amputees’ mobility, mobility-based energy consumption and latency model are 
designed. By including Deep Learning (DL)-based motion determined recognition 
mechanism on smart prosthesis, efﬁciency of task ofﬂoading as well as scheduling 
policy is established. From the investigational outcomes, it is seen that the proposed 
schemes are capable of determining ideal task ofﬂoading and scheduling decisions. 
3 
Proposed Tuna Model 
3.1 
Problem Formulation 
The formal deﬁnition of workﬂow model and environment utilized for the execu-
tion of the workﬂows in implementation of proposed mechanism is presented in 
this section. In the implementation environment, the collection of MECs is utilized 
as presented as upper  M upper E upper C Subscript normal i Baseline equals StartSet upper M upper E upper C 1 comma upper M upper E upper C Subscript 2 comma ellipsis ellipsis period comma Baseline upper M upper E upper C Subscript normal n Baseline EndSet. This individual MEC is 
assumed to possesses a set of computational resources as VMs representing using 
uppe r  V upper M Subscript normal j Baseline equals StartSet upper V upper M 1 comma upper V upper M Subscript 2 comma ellipsis ellipsis period comma Baseline upper V upper M Subscript normal m Baseline EndSet. Moreover, each VMs are considered to work at

462
P. Jayalakshmi and S. S. Subashka Ramesh
different levels like upper F  upper D upper V u p p e r  F upper S Subscript normal k Baseline equals StartSet upper F upper D upper V upper F upper S 1 comma upper F upper D upper V upper F upper S 2 comma ellipsis ellipsis period comma upper F upper D upper V upper F upper S Subscript normal upper R Baseline EndSet. In this case, 
normal i Superscript t h setting of the upper F upper D upper V upper F upper S Subscript normal k represents frequency and voltage of MEC processor as spec-
iﬁed as upper F  upper D upper V upper F upper S Subscript normal j Baseline equals left parenthesis upper F r e q comma upper V o l t right parenthesis. Moreover, it is considered that upper F  upper D upper V upper F upper S Subscript normal j Baseline less than upper F upper D upper V upper F upper S Subscript normal j
where no rmal i less than normal j. 
A dataset is considered on the data center of the cloud computing environment, 
such that different fragments of it can be deployed over the MEC scenario as repre-
sented through upper D  upper S Sub s c r ipt  left parenthesis normal l right parenthesis Baseline equals StartSet upper F r g 1 comma upper F r g 2 comma ellipsis period period comma upper F r g Subscript normal s Baseline EndSet. But the total number of fragments 
associated with each MECs need to be less than or equal to maximum storage capacity 
possesses by it. In this context, the number of VMs which can be allocated by each 
MEC to each IoT tasks is denoted as upper V upper M Subscript upper C o u n t left parenthesis upper M upper E upper C right parenthesis. Moreover, the total size of storage 
with respect to all, MECs is comparatively less than the dataset size.
sig
ma summation Un derscrip
t normal i equals 1 Overscript normal n Endscripts upper S t o r a g e Subscript upper M upper E upper C Baseline less than upper D upper S Subscript upper S i z e
where, upper D upper S Subscript upper S i z e represents the dataset size. 
In the implementation of the proposed scheme, workﬂows submitted to 
environment of MEC are represented as Directed Acyclic Graph (DAG). 
Further, the collection of workﬂows sent to MEC is represented as upper W 
upper F Su b s c r i p t  upper S e t Baseline equals left parenthesis upper W upper F 1 comma upper W upper F 2 comma ellipsis ellipsis comma upper W upper F Subscript normal t Baseline right parenthesis. This workﬂows in turn comprises of some tasks labelled 
as uppe r  W upper F Subscript normal i Baseline equals StartSet upper T upper S 1 comma upper T upper S Subscript 2 comma ellipsis ellipsis period comma Baseline upper T upper S Subscript normal z Baseline EndSet. Further, the workﬂow considered as DAG wherein 
every node and edge signiﬁes the tasks and control or data dependencies between the 
tasks. In particular, normal upper E Subscript i j represents edge between two tasks such as normal upper T Subscript normal i and normal upper T Subscript normal j, respec-
tively under no rmal upper T Subscript normal i Baseline not equals normal upper T Subscript normal j. Thus, the complete set of direct predecessors with respect to 
each workﬂow can be determined based on Eq. (2) 
normal u p per 
T
 Subsc
r
ipt no
rmal i minus upper P r e d Baseline equals left brace normal upper T Subscript normal j Baseline divided by left parenthesis normal upper T Subscript normal j Baseline comma normal upper T Subscript normal i Baseline right parenthesis element of normal upper E Subscript i j Baseline right brace
normal u p per 
T
 Subsc
r
ipt no
rmal i minus upper S u c c Baseline equals left brace normal upper T Subscript normal j Baseline divided by left parenthesis normal upper T Subscript normal i Baseline comma normal upper T Subscript normal j Baseline right parenthesis element of normal upper E Subscript i j Baseline right brace
Moreover, the complete set of successors of every task is calculated based on 
Eq. (3). However, the set of predecessors need to be empty when the tasks are entry 
tasks or tasks. 
In particular, the mean computation time with respect to each normal upper T Subscript normal i and normal upper T Subscript normal j is computed 
based on Eq. (4). 
upper T i m e Subscript
Lupper E x e c Baseline left parenthesis normal upper T Subscript normal i Baseline comma upper F upper D upper V upper F upper S Subscript normal k Baseline right parenthesis equals StartFraction upper L e n Subscript upper T a s k Baseline left parenthesis normal upper T Subscript normal i Baseline right parenthesis Over upper V upper M left parenthesis normal j comma upper F upper D upper V upper F upper S Subscript normal k Baseline right parenthesis EndFraction
upper T i m e 
Subscript upper E x e c Baseline left parenthesis normal upper T Subscript normal i Baseline comma upper F upper D upper V upper F upper S Subscript normal k Baseline right parenthesis equals StartFraction upper L e n Subscript upper T a s k Baseline left parenthesis normal upper T Subscript normal i Baseline right parenthesis Over upper V upper M left parenthesis normal j comma upper F upper D upper V upper F upper S Subscript normal k Baseline right parenthesis EndFraction
In addition, the makespan is computed based on Eq. (5). 
upper M upper  K Subscript up per S p
 a n Baseline left parenthesis upper W upper F Subscript normal i Baseline right parenthesis equals StartSet upper M a x left parenthesis upper F upper T left parenthesis normal upper T Subscript normal i Baseline right parenthesis divided by normal upper T Subscript normal upper J Baseline element of upper W upper F Subscript normal i Baseline right parenthesis EndSet
In addition, the ﬁtness function is formulated by minimizing the objective function 
which integrates the above-mentioned parameters into account.

Tuna Optimization Algorithm-Based Data Placement and Scheduling ...
463
3.2 
Inspiration 
This proposed strategy of minimizing the ﬁtness function using the Tuna Swarm 
Optimization (TSO) which is modelled as follows. The algorithm’s mathematical 
model is detailed below. 
Initialization 
Like many swarm-based meta-heuristics, TSO commences optimization by 
producing preliminary population arbitrarily uniformly in Search Space (SS). 
nor
mal u pper X Su bscript i n i t Super s c r ip
t n
where, 
nor
mal upper X Subscript i n i t Superscript normal i - ‘normal i Superscript t h’ preliminary individual 
ub and lb - Upper and lower limits of SS 
n - Tuna Population size 
Ran - Consistently distributed random vector in the range [0, 1] 
Spiral Foraging 
As sardines, small schooling ﬁshes and herring face predators, whole school of 
ﬁshes produce a thick establishment by continuously varying swimming direction 
by making it tedious for predators to focus on a target. Tuna crowd pursuits the prey 
by creating tight spiral. Along with spiraling after prey, these schools interchange 
information among themselves. Every tuna follows preceding ﬁsh thus facilitating 
sharing of information amid adjoining tuna. Depending on above principles, spiral 
foraging mechanism is illustrated as follows: 
nor
mal u
pper
 
X Subs
cr
ipt 
normal t
 p
lus 1
 
Su perscrip
t
 
n
or mal 
i B
a
seline
 e
qual
s StartL
ay
out E
n
la rged left 
b
r
ac e 1st Row  n o rma
l phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline right parenthesis comma normal i equals 1 2nd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline right parenthesis comma normal i equals 2 comma 3 comma ellipsis period normal n EndLayout
nor mal phi 1  equal s normal a plus left parenthesis 1 minus normal a right parenthesis period StartFraction normal t Over normal t Subscript max Baseline EndFraction
norma
l phi 1 equals normal a plus left parenthesis 1 minus normal a right parenthesis period StartFraction normal t Over normal t Subscript max Baseline EndFraction
nor mal p hi 2 equal s left  parenthesis 1 minus normal a right parenthesis plus left parenthesis 1 minus normal a right parenthesis period StartFraction normal t Over normal t Subscript max Baseline EndFraction
norma
l phi 2 equals left parenthesis 1 minus normal a right parenthesis plus left parenthesis 1 minus normal a right parenthesis period StartFraction normal t Over normal t Subscript max Baseline EndFraction
normal theta equa
ls normal e Superscript b l Baseline period cosine left parenthesis 2 pi normal b right parenthesis
no rmal l equals n ormal e Superscript 3 cosine left parenthesis left parenthesis left parenthesis normal t Super Subscript max Superscript plus StartFraction 1 Over normal t EndFraction right parenthesis minus 1 right parenthesis normal pi right parenthesis
no rmal l
 equals normal e Superscript 3 cosine left parenthesis left parenthesis left parenthesis normal t Super Subscript max Superscript plus StartFraction 1 Over normal t EndFraction right parenthesis minus 1 right parenthesis normal pi right parenthesis
where, 
nor
mal upper X Subscript normal t plus 1 Superscript normal i - ‘normal i Superscript t h’ individual of ‘t + 1’ iteration 
normal
 upper X Subscript normal t Superscript b e s t
- Present ideal individual (food)

464
P. Jayalakshmi and S. S. Subashka Ramesh
normal phi 1 and normal phi 2 - Weight coefﬁcients focusing on individual’s tendency to move to ideal 
and preceding individual 
a - Constant which ﬁnds the level to which Tuna follows ideal and preceding 
individual in preliminary stage 
t - Present iteration 
normal t Subscript max - Maximum amount of iterations 
b - Arbitrary number consistently distributed in the range [0, 1] 
As tuna hunts spirally around food, they offer improved exploitation capability 
for SS round about food. Nevertheless, when ideal individual is not able to determine 
food, simply following ideal individual to hunt is not favorable for group hunting. 
Hence, an arbitrary co-ordinate is determined from SS as a Reference Point (RP) 
for performing spiral search. It enables individuals to search in broader space and 
perform TSO with universal exploration capability. 
nor
mal u p
p er 
X
 Subs
cr
ipt 
normal 
i 
norma
l
 n 
normal i
 no rm al t
 Su
p
erscr
ip
t no
rmal i 
Ba
selin
e
 eq
uals Start
L
ay ou t Enlar g e d  le
ft brace 1st Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline comma normal i equals 1 2nd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline comma normal i equals 2 comma 3 comma ellipsis period normal n EndLayout
norma
l upper X Subscript normal t Superscript r a n
- Arbitrarily produced RP in SS 
In speciﬁc, meta-heuristic algorithms typically perform wide-ranging universal 
exploration in primary phase and progressively transit to detailed local exploita-
tion. Hence, TSO modiﬁes RPs of spiral foraging from arbitrary individuals to ideal 
individuals as iterations increase. Spiral foraging approach is given below. 
nor
mal u pp
er
 X Su
bscri
pt 
n
ormal
 i
 nor
mal n n
or
mal i
 
nor
mal t Su
persc rip t norm al
 i Baseline equals StartLayout Enlarged left brace 1st Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline comma normal i equals 1 normal i normal f normal r normal a normal n less than StartFraction normal t Over normal t Subscript normal m normal a normal x Baseline EndFraction 2nd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline comma normal i equals 2 comma 3 comma ellipsis period normal n normal i normal f normal r normal a normal n greater than or equals StartFraction normal t Over normal t Subscript normal m normal a normal x Baseline EndFraction 3rd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline right parenthesis comma normal i equals 1 4th Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline right parenthesis comma normal i equals 2 comma 3 comma ellipsis period normal n EndLayout
norma
l u
p
per X
 S
ubsc
ript no
rm
al i 
n
orm
al n norma
l
 i n ormal t  Su pe rsc ri pt n orm
al i Baseline equals StartLayout Enlarged left brace 1st Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline comma normal i equals 1 normal i normal f normal r normal a normal n less than StartFraction normal t Over normal t Subscript normal m normal a normal x Baseline EndFraction 2nd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline comma normal i equals 2 comma 3 comma ellipsis period normal n normal i normal f normal r normal a normal n greater than or equals StartFraction normal t Over normal t Subscript normal m normal a normal x Baseline EndFraction 3rd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline right parenthesis comma normal i equals 1 4th Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline right parenthesis comma normal i equals 2 comma 3 comma ellipsis period normal n EndLayout
norma
l u
p
per X 
Su
bscr
ipt norm
al
 i no
r
ma l n norm
a
l
 i n orma
l t
 
Supers
cr
ipt 
normal i
 B
aseli
n
e equals Sta
r
t
Layo ut Enla r ge d left
 brace 1st Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline comma normal i equals 1 normal i normal f normal r normal a normal n less than StartFraction normal t Over normal t Subscript normal m normal a normal x Baseline EndFraction 2nd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal r normal a normal n Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal r normal a normal n Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue right parenthesis plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline comma normal i equals 2 comma 3 comma ellipsis period normal n normal i normal f normal r normal a normal n greater than or equals StartFraction normal t Over normal t Subscript normal m normal a normal x Baseline EndFraction 3rd Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i Baseline right parenthesis comma normal i equals 1 4th Row normal phi 1 period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal theta period StartAbsoluteValue normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline EndAbsoluteValue plus normal phi 2 period normal upper X Subscript normal t Superscript normal i minus 1 Baseline right parenthesis comma normal i equals 2 comma 3 comma ellipsis period normal n EndLayout
Parabolic Foraging 
Besides nourishing by establishing a spiral, tunas form parabolic co-operative 
feeding. Tuna forms parabolas with food as RP. Further, tuna hunts for food by hunting 
around themselves. The methods are executed instantaneously with supposition that 
the selection probability is 50% for both cases. 
nor
mal u
pper X 
Su
bscri
p
t norm
al
 t pl
u
s
 1 Super
s
cript 
no
rmal 
i
 
Basel ine equals
 StartLay
ou
t E nlar ged le
ft brace 1st Row 1st Column normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline plus normal r normal a normal n left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline right parenthesis plus normal upper T normal upper F period normal p squared period left parenthesis normal upper X Subscript normal t Superscript normal b normal e normal s normal t Baseline minus normal upper X Subscript normal t Superscript normal i Baseline right parenthesis comma 2nd Column normal i normal f normal r normal a normal n less than 0.5 2nd Row 1st Column normal upper T normal upper F period normal p squared period normal upper X Subscript normal t Superscript normal i Baseline comma 2nd Column normal i normal f normal r normal a normal n greater than or equals 0.5 EndLayout
no r
m
al  p  equals left parenthesis 1 minus StartFraction normal t Over normal t Subscript max Baseline EndFraction right parenthesis Superscript left parenthesis StartFraction normal t Over normal t Super Subscript max Superscript EndFraction right parenthesis
norm
al
 p equals left parenthesis 1 minus StartFraction normal t Over normal t Subscript max Baseline EndFraction right parenthesis Superscript left parenthesis StartFraction normal t Over normal t Super Subscript max Superscript EndFraction right parenthesis
norm
a
l p equals left parenthesis 1 minus StartFraction normal t Over normal t Subscript max Baseline EndFraction right parenthesis Superscript left parenthesis StartFraction normal t Over normal t Super Subscript max Superscript EndFraction right parenthesis

Tuna Optimization Algorithm-Based Data Placement and Scheduling ...
465
where, TF represents the random number that possesses the value of 1 or –1. 
Tuna co-operatively hunts using either of the 2 foraging approaches to determine 
prey. For optimizing TSO, population is arbitrarily produced in SS. In every iteration, 
the individuals arbitrarily select a foraging approach or select to renew the locations 
in SS based on probability ‘z’. During whole process of optimization, the individuals 
are unceasingly updated and computed until ﬁnish condition is satisﬁed, and ideal 
individual and conforming ﬁtness values are output. 
TSO Algorithm 
Input: n, 
Output: Food position (best individual), fitness  
Set arbitrary Tuna population 
 (i = 1, 2, . . ., n) 
Allocate free factors ‘a’ and ‘z’ 
While (t < 
) 
Compute fitness of tunas 
Modify ‘
’ 
For (every tuna) do 
Modify ‘ 
’, ‘ 
’, p 
If (ran < z) then 
Update location ‘
’ by using Eq. (6) 
Else if (ran ≥ z) then 
If (ran < 0.5) then 
If
then 
Update location ‘
’ by using Eq. (12) 
Else if 
then 
Update location ‘
’ by using Eq. (7) 
Else if (rand ≥ 0.5) then 
Update location ‘
’ by using Eq. (14) 
End //for 
t = t + 1 
End //while 
Return (
 & F(
)) 
In addition, the algorithm considered during implementation of proposed mech-
anism is detailed below. 
Scheduling_MEC_Procedure () 
Input:
Workﬂow (upper W upper F Subscript left parenthesis normal i right parenthesis) 
Output: Assignment or mapping of tasks to appropriate VMs 
Step 1:
Identify the VMs count in the environment of MEC

466
P. Jayalakshmi and S. S. Subashka Ramesh
Step 2:
Identify the different levels of DVFS 
Step 3:
Determine the deadline associated with each workﬂow (upper W upper F Subscript left parenthesis normal i right parenthesis)) 
Step 4:
Use Eq. (4) for determining the rank of workﬂow 
Step 5:
Depending on the determined ranks, arrange the tasks in an increasing or 
der 
Step 6:
Estimate the slack time associated with each workﬂow 
Step 7:
Assign the slack time to different levels of workﬂow 
Step 8:
Initialize the objective function 
Step 9:
Utilize the optimization algorithm of TOA to determine the best solution 
Step 10: For each upper T upper S Subscript normal i in the workﬂow (upper W upper F Subscript normal i) 
Step 11: Determine levels of DVFS related to each VMs for identifying the best 
solution 
Step 12: Identify the set of predecessors related to each upper T upper S Subscript left parenthesis normal i right parenthesis
Step 13: If (Predecessor left parenthesis upper T upper S Subscript normal i Baseline right parenthesisis empty) then 
Step 14: While (Status of upp er V upper M Subscript normal i Baseline not equals i d l e) 
Step 15: 
Wait 
Step 16: End 
Step 17: Else 
Step 18: While (Status of upper  V upper M Subscript left parenthesis normal i right parenthesis Baseline not equals i d l e or all predecessors of upper T upper S Subscript left parenthesis normal i right parenthesis) are not 
executed) 
Step 19: Wait 
Step 20: End 
Step 21: End 
Step 22: Allocate the different levels of DVFS to each VM 
Step 23: Allocate the tasks (upper T upper S Subscript normal i) to each VM 
Step 24: End 
4 
Results and Discussion 
The performance evaluation of proposed TOA-DPS scheme and baseline DBOA-
DPS, HHHSS-ODPTS, and GAPSO-DPS approaches achieved using inter-task 
communication, data access ratio with the datasets of LIGO and Montage. 
Figures 1 and 2 present the plots of inter-task communication achieved by the 
proposed TOA-DPS scheme and the baseline DBOA-DPS, HHHSS-ODPTS, and 
GAPSO-DPS approaches with respect to the datasets of LIGO and Montage under 
different number of tasks. The proposed TOA-DPS scheme under LIGO dataset 
with different number of tasks in the workﬂow is minimized by 10.24, 12.98, and 
14.56%, better than the baseline DBOA-DPS, HHHSS-ODPTS, and GAPSO-DPS 
approaches. Moreover, the proposed TOA-DPS scheme under Montage dataset with 
different number of tasks in the workﬂow is minimized by 11.21, 13.48 and 15.86% 
in contrast to standard schemes taken for investigation.
In addition, Figs. 3 and 4 present the plots of data access ratio achieved by the 
proposed TOA-DPS scheme and the baseline DBOA-DPS, HHHSS-ODPTS, and

Tuna Optimization Algorithm-Based Data Placement and Scheduling ...
467
Fig. 1 Inter-task communication of TOA-DPS under the LIGO dataset 
Fig. 2 Inter-task communication of TOA-DPS under the Montage dataset
GAPSO-DPS approaches with respect to the datasets of LIGO and Montage under 
different number of tasks. The proposed TOA-DPS scheme under LIGO dataset 
with different number of tasks in the workﬂow minimized the data access rate by 
11.64, 13.86, and 16.79%, excellent than the baseline DBOA-DPS, HHHSS-ODPTS, 
and GAPSO-DPS approaches. Moreover, the proposed TOA-DPS scheme under 
Montage dataset with different number of tasks in the workﬂow also reduced data

468
P. Jayalakshmi and S. S. Subashka Ramesh
access rate by 10.94, 13.21, and 15.68%, better than the baseline approaches used 
for investigation. 
Fig. 3 Data access ratio of TOA-DPS under the LIGO dataset 
Fig. 4 Data access ratio of TOA-DPS under the Montage dataset

Tuna Optimization Algorithm-Based Data Placement and Scheduling ...
469
5 
Conclusion 
The proposed TOA-DPS achieved an improved convergence by preventing the prob-
lems of local optima to map tasks to most suitable resources. It adopted a method of 
task prioritization for determining order in which the tasks in the scientiﬁc workﬂows 
are executed. Further, Tuna Optimization Algorithm (TOA) is utilized for attaining 
data-exhaustive workﬂow scheduling as well as data placement using Dynamic 
Voltage and Frequency Scaling (DVFS) in MEC environments. The performance 
evaluation of proposed TOA-DPS-based scheduling mechanism is conducted using 
simulations conducted over the renowned scientiﬁc workﬂows of various sizes. The 
results of the proposed TOA-DPS-based scheduling scheme conﬁrmed better perfor-
mance of data access by 21.38%, and minimized energy consumption by 19.84%, 
better than the baseline approaches taken for investigation. In future, it is planned to 
formulate and implement a Dingo-based data placement mechanism and compare it 
with proposed scheme under homogeneous and heterogeneous conditions of MEC 
environment. 
References 
1. Masdari M, Bazarchi SM, Bidaki M (2013) Analysis of secure LEACH-based clustering 
protocols in wireless sensor networks. J Netw Comput Appl 36(4):1243–1260 
2. Masdari M, Barshande S, Ozdemir S (2019) CDABC: chaotic discrete artiﬁcial bee colony 
algorithm for multi-level clustering in large-scale WSNs. J Supercomput 75(11):7174–7208 
3. Wortmann F, Flüchter K (2015) Internet of things. Bus Inf Syst Eng 57(3):221–224 
4. Masdari M, Khoshnevis A (2020) A survey and classiﬁcation of the workload forecasting 
methods in cloud computing. Clust Comput 23(4):2399–2424 
5. Shakarami A, Ghobaei-Arani M, Masdari M, Hosseinzadeh M (2020) A survey on the compu-
tation ofﬂoading approaches in mobile edge/cloud computing environment: a stochastic-based 
perspective. J Grid Comput 18(4):639–671 
6. Bonomi F, Milito R, Zhu J, Addepalli S (August, 2012) Fog computing and its role in the 
internet of things. In: Proceedings of the ﬁrst edition of the MCC workshop on mobile cloud 
computing, pp 13–16 
7. Vaquero LM, Rodero-Merino L (2014) Finding your way in the fog: towards a comprehensive 
deﬁnition of fog computing. ACM SIGCOMM Comput Commun Rev 44(5):27–32 
8. Hu P, Dhelim S, Ning H, Qiu T (2017) Survey on fog computing: architecture, key technologies, 
applications and open issues. J Netw Comput Appl 98:27–42 
9. Rawassizadeh R, Keshavarz H, Pazzani M (2019) Ghost imputation: accurately reconstructing 
missing data of the off period. IEEE Trans Knowl Data Eng 32(11):2185–2197 
10. Keshavarz H, Abadeh MS, Rawassizadeh R (2020) SEFR: a fast linear-time classiﬁer for 
ultra-low power devices. arXiv preprint arXiv:2006.04620 
11. Yin L, Luo J, Luo H (2018) Tasks scheduling and resource allocation in fog computing based 
on containers for smart manufacturing. IEEE Trans Ind Inf 14(10):4712–4721 
12. Rahbari D, Nickray M (2019) Low-latency and energy-efﬁcient scheduling in fog-based IoT 
applications. Turk J Electr Eng Comput Sci 27(2):1406–1427 
13. Masdari M, Salehi F, Jalali M, Bidaki M (2017) A survey of PSO-based scheduling algorithms 
in cloud computing. J Netw Syst Manage 25(1):122–158 
14. Arora S, Singh S (2019) Butterﬂy optimization algorithm: a novel approach for global 
optimization. Soft Comput 23(3):715–734

470
P. Jayalakshmi and S. S. Subashka Ramesh
15. Li C, Tang J, Tang H, Luo Y (2019) Collaborative cache allocation and task scheduling for data-
intensive applications in edge computing environment. Future Gener Comput Syst 95:249–264 
16. Shao Y, Li C, Fu Z, Jia L, Luo Y (2019) Cost-effective replication management and scheduling 
in edge computing. J Netw Comput Appl 129:46–61 
17. Lin B, Zhu F, Zhang J, Chen J, Chen X, Xiong NN, Mauri JL (2019) A time-driven data 
placement strategy for a scientiﬁc workﬂow combining edge computing and cloud computing. 
IEEE Trans Ind Inf 15(7):4254–4265 
18. Hosseinzadeh M, Masdari M, Rahmani AM, Mohammadi M, Aldalwie AHM, Majeed 
MK, Karim SHT (2021) Improved butterﬂy optimization algorithm for data placement and 
scheduling in edge computing environments. J Grid Comput 19(2):1–27 
19. Chen Z, Hu J, Min G, Chen X (2021) Effective data placement for scientiﬁc workﬂows in 
mobile edge computing using genetic particle swarm optimization. Concur Comput Pract Exp 
33(8):e5413 
20. Vivekanandan N, Gnanasekaran A (2021) Hybrid Harris Hawk-Salp swarm optimization 
algorithm-based integrated optimal data placement and task scheduling for improving the user 
experience in edge computing. Concur Comput Pract Exp 33(24):e6455 
21. Li C, Zhang C, Ma B, Luo Y (2022) Efﬁcient multi-attribute precedence-based task scheduling 
for edge computing in geo-distributed cloud environment. Knowl Inf Syst 64(1):175–205 
22. Qi P (2022) Task ofﬂoading and scheduling strategy for intelligent prosthesis in mobile edge 
computing environment. Wirel Commun Mob Comput

Frequency Control of Single Area Hybrid 
Power System with DG 
Ashutosh Biswal, Prakash Dwivedi, and Sourav Bose 
Abstract Analysis of power output vs load proﬁle in a system with scattered 
generation resources connected to the existing traditional energy system is crit-
ical, because even a minor frequency shift might cause a total blackout. The load 
frequency management problem for a hybrid coal-based system incorporated with 
DG, consisting of fuel cells, diesel engine generators, wind turbine generators, aqua-
electrolyzer, and battery energy storage system is explored in this work. Due to the 
signiﬁcant output power variation of wind energy systems, integrating them into 
DG offers a challenge for the creation of an appropriate controller. The stochastic 
volatility of the load proﬁle makes this issue more difﬁcult. The study’s suggested 
control method relies on differential evolution (DE). For various disturbances, the 
efﬁciency of frequency stabilization is studied. The results demonstrate how the 
hybrid DG system’s PID controller was able to achieve the least amount of frequency 
variation. 
Keywords Differential evolution (DE) algorithm · Distribution generator (DG) ·
Proportional integral derivative (PID) · Renewable energy source (RES) 
1 
Introduction 
The two primary objectives of power system controls are to preserve system integrity 
and return the system to normal functioning following any kind of physical disrup-
tion. In other words, maintaining the system’s planned performance and restoring 
it following a disruption like a short circuit and loss of generation or load are both
A. Biswal envelope symbol · P. Dwivedi · S. Bose 
Department of Electrical Engineering, NIT Uttarakhand, Srinagar, India 
e-mail: linku.ashutosh@gmail.com 
P. Dwivedi 
e-mail: prakashdwivedi@nituk.ac.in 
S. Bose 
e-mail: souravbose@nituk.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_40 
471

472
A. Biswal et al.
included in power system control [1, 2]. Signiﬁcant advancements in both the tech-
nology of distribution generators (DGs) and renewable energy sources have been 
made in recent years (RESs). Utilization of RES/DG and micro-grids is increasing, 
which has a variety of technical effects and critical implications for how well-suited 
conventional power system control methods are to the brand-new operating environ-
ment. The implications of RESs/DGs on the dynamics and stability of power systems, 
as well as potential control measures, have recently attracted a lot of attention. 
Analysis of power output vs load proﬁle in a system with scattered generation 
resources connected to the existing traditional energy system is critical, because 
even a minor frequency shift might cause a total blackout [3, 4]. The load frequency 
control problem for a hybrid thermal power system that is integrated with DG is 
examined in this research as shown in Fig. 1. Due to the large output power ﬂuctu-
ation of wind energy systems, integrating them into DG offers a challenge for the 
development efﬁcient control scheme. This problem is made more challenging by 
the load demand’s stochastic instability. The proposed control strategy in the study 
is based on differential evolution (DE). Investigations on the efﬁciency of frequency 
stabilization for various disturbances are conducted. The results demonstrate how the 
hybrid DG system’s PID controller was able to achieve the least degree of frequency 
ﬂuctuation. 
Fig. 1 Modeled transfer function single-area power system with DGs

Frequency Control of Single Area Hybrid Power System with DG
473
2 
System Description 
Equations 1 to 5 illustrate a ﬁrst order lag transfer function that is used to model the 
fuel cells, diesel generators, wind generators, aqua-electrolyzer, and battery energy 
storage systems. 
u ppe r  G Subscript upper F upper C Baseline equals StartFraction normal upper Delta upper P Subscript upper F upper C Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper F upper C Baseline Over 1 plus s upper T Subscript upper F upper C Baseline EndFraction
up
e
r  G Subscript upper F upper C Baseline equals StartFraction normal upper Delta upper P Subscript upper F upper C Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper F upper C Baseline Over 1 plus s upper T Subscript upper F upper C Baseline EndFraction
up pe r G S
ubscript upper F upper C Baseline equals StartFraction normal upper Delta upper P Subscript upper F upper C Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper F upper C Baseline Over 1 plus s upper T Subscript upper F upper C Baseline EndFraction
up per G  Subscript upper D upper E upper G Baseline equals StartFraction normal upper Delta upper P Subscript upper D upper E upper G Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper D upper E upper G Baseline Over 1 plus s upper T Subscript upper D upper E upper G Baseline EndFraction
up
e
r  G Subscript upper D upper E upper G Baseline equals StartFraction normal upper Delta upper P Subscript upper D upper E upper G Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper D upper E upper G Baseline Over 1 plus s upper T Subscript upper D upper E upper G Baseline EndFraction
up pe r G Su
bscript upper D upper E upper G Baseline equals StartFraction normal upper Delta upper P Subscript upper D upper E upper G Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper D upper E upper G Baseline Over 1 plus s upper T Subscript upper D upper E upper G Baseline EndFraction
up p er  G  Su b script upper W upper T upper G Baseline equals StartFraction normal upper Delta upper P Subscript upper W upper T upper G Baseline Over normal upper Delta upper P Subscript upper W upper P Baseline EndFraction equals StartFraction upper K Subscript upper W upper T upper G Baseline Over 1 plus s upper T Subscript upper W upper T upper G Baseline EndFraction
upp e r
 
G S ub script upper W upper T upper G Baseline equals StartFraction normal upper Delta upper P Subscript upper W upper T upper G Baseline Over normal upper Delta upper P Subscript upper W upper P Baseline EndFraction equals StartFraction upper K Subscript upper W upper T upper G Baseline Over 1 plus s upper T Subscript upper W upper T upper G Baseline EndFraction
up pe r G  Su b
script upper W upper T upper G Baseline equals StartFraction normal upper Delta upper P Subscript upper W upper T upper G Baseline Over normal upper Delta upper P Subscript upper W upper P Baseline EndFraction equals StartFraction upper K Subscript upper W upper T upper G Baseline Over 1 plus s upper T Subscript upper W upper T upper G Baseline EndFraction
up per  G Subscript upper A upper E Baseline equals StartFraction normal upper Delta upper P Subscript upper A upper E Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper A upper E Baseline Over 1 plus s upper T Subscript upper A upper E Baseline EndFraction
up
e
r G Subscript upper A upper E Baseline equals StartFraction normal upper Delta upper P Subscript upper A upper E Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper A upper E Baseline Over 1 plus s upper T Subscript upper A upper E Baseline EndFraction
up pe r G S
ubscript upper A upper E Baseline equals StartFraction normal upper Delta upper P Subscript upper A upper E Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper A upper E Baseline Over 1 plus s upper T Subscript upper A upper E Baseline EndFraction
up per G S ubscript upper B upper E upper S upper S Baseline equals StartFraction normal upper Delta upper P Subscript upper B upper E upper S upper S Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper B upper E upper S upper S Baseline Over 1 plus s upper T Subscript upper B upper E upper S upper S Baseline EndFraction
up
e
r  G Subscript upper B upper E upper S upper S Baseline equals StartFraction normal upper Delta upper P Subscript upper B upper E upper S upper S Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper B upper E upper S upper S Baseline Over 1 plus s upper T Subscript upper B upper E upper S upper S Baseline EndFraction
up pe r G Subs
cript upper B upper E upper S upper S Baseline equals StartFraction normal upper Delta upper P Subscript upper B upper E upper S upper S Baseline Over upper U 2 EndFraction equals StartFraction upper K Subscript upper B upper E upper S upper S Baseline Over 1 plus s upper T Subscript upper B upper E upper S upper S Baseline EndFraction
2.1 
Structure of the Controller 
While a proportional controller shortens the rising time, it leaves the steady-state error 
in place. An integrated control can minimize transient responsiveness while reducing 
steady-state inaccuracy. By using derivative control to reduce overshoot and settling 
time, the system’s stability may be improved. A PID controller’s construction is 
shown in Fig. 2.
2.2 
Objective Function 
Based on development strategies, a controller is designed using the objective func-
tions, and the control parameter is adjusted based on performance indicators. Four 
objective functions are frequently used in control designs [5, 6]. In this work, the 
objective function ITAE is chosen since it decreases processing time and peak over-
shoot. However, it won’t be accomplished with IAE or ISE-based tuning [7]. Equation 
represents the ITAE mathematical expressions (6). 
inte
g
ra
l Subsc ri pt 0 S up erscript t Su bs cr
ipt s i m Baseline left parenthesis StartAbsoluteValue normal upper Delta upper F 1 EndAbsoluteValue plus StartAbsoluteValue normal upper Delta upper F 2 EndAbsoluteValue plus StartAbsoluteValue normal upper Delta upper P Subscript upper T i e Baseline EndAbsoluteValue right parenthesis dot t dot d t

474
A. Biswal et al.
P
K 
D
K 
I
K
s 
1 
+ 
+ 
+ 
Proportional gain 
Integral gain 
Derivative gain 
Integrator 
Input
Output 
dt 
du 
Derivative 
Fig. 2 PID controller structure
where
normal upper Delta f 1 and normal upper Delta f 2 are the variation in frequency deviations;
normal upper Delta upper P Subscript upper T i e is a simple change to tie line power; 
t Subscript s i m is the simulation’s time frame. 
3 
DE Algorithm 
A heuristic search DE algorithm, is user-friendly, effective, and trustworthy. The DE 
algorithm performs several actions, including parameter initialization, mutation, and 
cross over [8, 9]. 
4 
Sımulatıon Results 
In the MATLAB/SIMULINK environment, the hybrid model of the system under 
study illustrated in Fig. 1. is created, and a DE programme is created (in .m-ﬁle). 
The hybrid model in a speciﬁc area is considered for this investigation. The present 
investigation’s minimum and maximum values for KP, KD, and KI are set at − 
2.0 and 2.0, respectively. A 1% step load disturbance in the thermal and wind 
power module is taken into consideration when duplicating the built model in a 
different programme (by.m-ﬁle utilising starting population/controller settings). The 
optimization approach uses the objective function, which is calculated in the.m-ﬁle. 
A population size of NP = 100, generation number G = 100, step size F = 0.8, and

Frequency Control of Single Area Hybrid Power System with DG
475
crossover probability CR = 0.8 were employed in the current experiment [10, 11]. 
The optimization was conducted 50 times, and the 50 best ﬁnal solutions were used 
to determine the controller parameters. The 50 runs’ top ﬁnal results are displayed 
in Table 1. 
Five instances can be used to examine the system under consideration.
. In the ﬁrst scenario, the hybrid DG system is subjected to a 1% change in wind 
speed and load.
. In the second scenario, the thermal unit is subjected to a random step load ﬂuc-
tuation, while the hybrid system’s wind power module is subjected to a 1% load 
variation.
. In the third scenario, a thermal unit random noise signal and a 1% load ﬂuctuation 
are delivered to the hybrid system’s wind power module.
. In the fourth scenario, the hybrid system’s thermal unit receives a 1% load ﬂuc-
tuation in addition to a random step load variation delivered to the wind power 
module.
. In the ﬁfth scenario, a random noise signal is supplied to the wind power module 
and a 1% load ﬂuctuation is applied to the hybrid system’s thermal unit. 
Case 1 
The hybrid DG system is subjected to a 1% load disturbance and wind power ﬂuc-
tuation at t = 0 s. Figure 3 displays the system’s DE-optimized I/PI/PID controller’s 
performance under these conditions. As shown in Fig. 3, pick undershoot, overshoot, 
and settling time are signiﬁcantly decreased in PI controllers when compared to I 
controllers, and performance is further enhanced in PID controllers when compared 
to I/PI controllers.
Case 2 
A random step load variation in a thermal unit is shown in Fig. 4. Figure 5 shows 
the associated frequency ﬂuctuation when a 1% load from a wind power module is 
applied to the hybrid system.
Table 1 Controller optimized values 
Controller Parameters
I
PI
PID 
Controller 1
KP1
––
−1.4162
−1.6501 
KI1
−0.2876
−1.0132
−1.9088 
KD1
––
––
−0.6357 
Controller 2
KP2
––
−1.0667
−0.9571 
KI2
1.5799
−1.3189
−0.3037 
KD2
––
––
0.1645 

476
A. Biswal et al.
Fig. 3 Frequency deviation due to load disturbance and wind power variation of 1%
Fig. 4 Load change pattern for random step load
Case 3 
A thermal unit random noise signal and a 1% load applied to the wind power module 
are both applied to the hybrid system. Figure 6 depicts the resultant frequency 
deviation.

Frequency Control of Single Area Hybrid Power System with DG
477
Fig. 5 variation in frequency brought on by random changes in load
Fig. 6 frequency variation brought on by a signal of random noise 
Case 4 
Figure 7 depicts the wind power module with a random step load ﬂuctuation, and 
Fig. 8 depicts the hybrid system’s thermal unit with a 1% load and the accompanying 
frequency deviation.
Case 5 
The wind power module receives a random noise signal, and 1% of the thermal 
unit’s load is applied. Figure 9 shows the equivalent frequency deviation for the 
hybrid system.

478
A. Biswal et al.
Fig. 7 Random wind power ﬂuctuation load change pattern 
Fig. 8 Random variations in frequency caused by the wind
Fig. 9 Variation in frequency caused by a signal of random noise supplied to a wind power module

Frequency Control of Single Area Hybrid Power System with DG
479
5 
Conclusion 
In this paper, the load frequency management issue for a hybrid thermal power system 
integrated with DG is investigated. This system consists of fuel cells, diesel engine 
generators, wind turbine generators, aqua-electrolyzers, and battery energy storage 
systems. The integration of wind energy systems into DG presents a difﬁculty for the 
creation of an efﬁcient controller design because of the high output power ﬂuctuation 
of wind energy systems. The stochastic volatility of the load demand makes this issue 
more difﬁcult. The control strategy proposed in this study is based on differential 
evolution (DE). For various disturbances, the efﬁciency of frequency stabilization is 
studied. The results demonstrate the hybrid DG system’s PID controller was able to 
achieve the least amount of frequency variation. 
References 
1. Biswal A, Dwivedi P, Bose S (2022) DE optimized IPIDF controller for management frequency 
in a networked power system with SMES and HVDC link. Front Energy Res 10:1972 
2. Kothari DP, Nagrath IJ (2003) Modern power system analysis. Tata McGraw-Hill Publishing 
Company, New York 
3. Behera SP, Biswal A (2019) TID controller in two area multi unit power systems with SMES 
and HVDC link. In: Global Conference for Advancement in technology (GCAT). IEEE, pp 
1–5 
4. Biswal A, Behera SP (2019) Simulation study of two area multi unit power systems using 
PI-PD controller with RFB and UPFC. In: Global Conference for Advancement in Technology 
(GCAT). IEEE, pp 1–6 
5. Khuntia SR, Panda S (2012) Simulation study for automatic generation control of a multi-area 
power system by ANFIS approach. Appl Soft Comput 12(1):333–341 
6. Soni DK, Thapliyal R, Dwivedi P (2019) Load frequency control of two interconnected area 
hybrid microgrid system using various optimization for the robust controller. In: TENCON 
IEEE Region 10 Conference (TENCON). IEEE, pp 1019–1025 
7. Ramoji SK, Saikia LC, Dekaraja B, Behera MK, Bhagat SK (2022) Repercussions of SMES 
and HVDC link in amalgamated voltage and frequency regulation of multi-area multi-unit inter-
connected power system. IN: 4th International Conference on Energy, Power and Environment 
(ICEPE). IEEE, pp 1–6 
8. Das S, Suganthan PN (2010) Differential evolution: a survey of the state-of-the-art. IEEE Trans 
Evol Comput 15(1):4–31 
9. Storn R, Price K (1997) Differential evolution-a simple and efﬁcient heuristic for global 
optimization over continuous spaces. J Glob Optim 11(4):341 
10. Pradhan PC, Sahu RK, Panda S (2016) Fireﬂy algorithm optimized fuzzy PID controller for 
AGC of multi-area multi-source power systems with UPFC and SMES. Eng Sci Technol Int J 
19(1):338–354 
11. Rout UK, Sahu RK, Panda S (2013) Design and analysis of differential evolution algo-
rithm based automatic generation control for interconnected power system. Ain Shams Eng J 
4(3):409–421

Prediction of Heart Disease and Heart 
Failure Using Ensemble Machine 
Learning Models 
Abdullah Al Maruf, Aditi Golder, Abdullah Al Numan, 
Md. Mahmudul Haque, and Zeyar Aung 
Abstract Heart disease, commonly referred to as cardiovascular disease and heart 
failure, has been the leading cause of mortality globally. Many risk factors for heart 
disease are associated with prompt access to reliable, dependable, and practical early 
diagnosis and disease management procedures. Identifying heart disease through 
early-stage signs is challenging in today’s global climate. If not caught in time, 
this could result in death. When there are no heart specialist doctors in remote, 
semi-urban, or rural areas, precise risk prediction and analysis might be critical in 
the early-stage identiﬁcation of heart disorders. Machine learning (ML) and Deep 
learning (DL) approaches were employed in this study to assess massive volumes of 
complex medical data, supporting specialists in predicting heart illness and mortality 
from heart failure. This study used two datasets: one to forecast heart disease and the 
other to analyze and forecast death due to heart failure. Predicting cardiac illnesses 
using Artiﬁcial Neural Networks is 91.52% accurate (ANN). The bagging ensemble 
predicted heart failure with 90% accuracy. The primary contribution of this research 
is an ensemble strategy with high performance that multiple measurements have 
demonstrated to predict heart failure and cardiac disorders using ANN. 
Keywords Heart Disease · Heart Failure · Healthcare · Machine Learning ·
Ensemble · Artiﬁcial Neural Network 
A. Al Maruf · Md. M. Haque 
Department of Computer Science and Engineering, Bangladesh University of Business and 
Technology (BUBT), Dhaka, Bangladesh 
A. Golder 
Jahangirnagar University, Savar, Dhaka 1342, Bangladesh 
A. Al Numan 
Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh 
Z. Aung (B) 
Khalifa University, Abu Dhabi, United Arab Emirates 
e-mail: zeyar.aung@ku.ac.ae 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_41 
481

482
A. Al Maruf et al.
1 
Introduction 
The Heart is the crucial organ in the body that leverages a network of veins and 
arteries to transmit oxygen-rich blood to other organ systems. Cardiovascular illness 
or Heart disease (HD) is any disorder that impairs our hearts [ 37]. Heart problems 
are less common in women than in men, especially in middle or old age [ 3, 45], 
while children can also have similar health problems [ 43]. The most severe and life-
threatening ailment impacting humans has long been thought to be cardiovascular 
disease (CVD) [ 14]. 
Recent information from the World Health Organization (WHO) indicates nearly 
20.5 million deaths each year worldwide are attributed to cardiac diseases or 32% of 
all fatalities. By 2030, it is anticipated that there will be 24.2 million more fatalities 
per year [ 29]. Stroke and coronary artery disease for 85% of deaths from heart disease. 
The primary behavioral risk factors for heart disease and stroke comprise smoking, 
unhealthy eating patterns, inactivity, and alcohol addiction. 
The European Cardiology Society (ESC) states that 3.6 million individuals world-
wide receive a heart disease diagnosis yearly, totaling 26 million cases [ 39]. Approxi-
mately 3 % of the overall expenditure on healthcare to treat heart disease is spent, and 
over 50% of patients with heart disorders die within one to two years after diagnosis, 
a higher frequency in Asia [ 21]. False projections might result from medical staff 
members’ lack of experience [ 31]. Heart illness is often brought on by the body’s 
organs not receiving adequate blood ﬂow from the heart [ 51]. 
Early signs include occasional pulse, a chilly sweat, shortness of breath, swollen 
feet, chest pain, and sudden nausea. Early identiﬁcation of symptoms and alterations 
in behavior, such as increased workouts, quitting cigarette use, and expert medical 
evaluation, can aid in lower mortality [ 5]. 
Early detection can be challenging [ 6]. Surgery to treat heart disease is difﬁcult, 
especially in underdeveloped nations requiring more educated medical professionals, 
diagnostic tools, and other resources to accurately diagnose and treat heart disease 
patients [ 1]. The safety of patients would be increased, and severe heart attacks might 
be avoided with the accurate assessment of the cardiac failure risks [ 26]. 
In most cases, medical experts have trouble accurately predicting a patient’s car-
diac problems, and they can foretell up to 67% accuracy [ 12] because similar symp-
toms seen in patients who have already been diagnosed with a disease are currently 
used to determine the diagnosis of any disease [ 23]. To accurately forecast cardiac dis-
ease, the healthcare industry thus needs an automated intelligent system. This could 
be accomplished by combining an ML algorithm with the vast volume of patient 
information currently available in the medical industry [ 34]. ML algorithms can help 
recognize illnesses [ 36] when taught on appropriate data. Data is the ﬁnest source 
for learning new or additional information and obtaining essential facts. However, 
this data is entirely unprocessed, whether organized or without formal organization 
[ 41]. Big data must be utilized to obtain vital information by storing, processing, 
analyzing, managing, and visualizing this data through data analysis [ 48]. Prediction 
models may be compared using publicly available heart disease datasets. Preprocess-

Heart Disease and Heart Failure Prediction
483
ing is necessary to extract critical information, shorten the algorithm’s performance 
time, and increase effectiveness [ 42]. ML offers the most popular predictive modeling 
techniques to transcend constraints and is employed in many domains [ 10]. 
Therefore, in this study, we tried to evaluate all of the perils and variables that 
may affect the Heart and result in cardiac illness. We employed various techniques 
to resolve this issue and predicted heart diseases and heart failure using the ensemble 
method. 
The following is how this document is structured: Sect. 2 will review the back-
ground research. Section 3 will demonstrate the dataset deﬁnition, data preprocess-
ing, and our prediction model. In Sect. 4, the outcomes of our endeavor are depicted. 
Finally, we come to the end of the document. 
2 
Related Work 
This section covers the latest advanced methods for diagnosing heart illness and 
heart failure utilizing deep and machine learning approaches. Due to increased fore-
cast accuracy and efﬁciency, the practice of AI and ML techniques in the recent 
past has grown signiﬁcantly [ 4]. The reduction formed a cardiac disease forecast 
model employing the Cleveland dataset of 303 data by feature normalization and 
feature reduction [ 16]. Seven main components were found and utilized to aim for 
Machine Learning classiﬁers. They came to the end that Support Vector Machine 
(SVM) and Logistic Regression (LR) had nearly comparable accuracy (87% and 
85%, respectively) [ 27]. 
Heart Disease dataset of UCI, ML, and more traditional methods like SVM, Ran-
dom Forest (RF), and learning models have recently been put to the test, and with the 
structured model, the attained accuracy was 85.48% [ 2]. The voting-based model, 
together with other classiﬁers, increased accuracy. According to the research, the 
weak classiﬁers improved by 2.1%. 
Different ML classiﬁcation algorithms were utilized in the study to predict chronic 
illness [ 15]. Using nine characteristics, they created an accuracy of 85.48 utilizing 
majority vote with RF, Bayes Net, MLP, and NB classiﬁers. Using the ensemble 
technique, they increased the accuracy to 7.26%. 
The Cleveland dataset created three classiﬁcation models for heart disease foretell 
Logistic Regression(LR), Decision Tree (DT), and Gaussian NB [ 19]. Single-value 
decomposition went from 13 to 4 to minimize the number of features. They concluded 
both Gaussian NB and LR exhibited an area under the curve (AUC) of 0.87% and 
prognostic scores of 82.75% [ 13]. The suggested model AUC increased by 2% to 
6%. ML was used to apply to the signiﬁcant risk variables identiﬁed in the study and 
conduct a comparative analysis [ 50]. 
Five ML classiﬁers were trained using 303 records of the UCI dataset and ten 
characteristics, namely KNN, DT, SVM, RF, and LR, to foretell heart disease. The 
accuracy, sensitivity, and speciﬁcity of the MFSFSA and SVM combination were

484
A. Al Maruf et al.
81.19%, 72.92%, and 88.68%, respectively [ 30], and the best accuracy of 93.44% 
based on a weight matrix [ 44]. 
The Cleveland dataset, 297 entries with 13 criteria, created a heart disease predic-
tion system (HDPS) and assessed the Naive Bayes, RF, KNN, and SMO classiﬁer’s 
accuracy and recall metrics [ 47]. The Cleveland dataset from UCI, which contains 
303 records and 14 characteristics, was studied [ 20], and a forecast model for car-
diac disease was presented [ 24]. It was suggested that preprocessing replace missing 
values with the mean values to determine whether or not there is a heart illness [ 18]. 
An original hybrid model was created utilizing DT, RF, and hybrid (DT + RF) 
approaches employing the Cleveland heart dataset and 14 attributes with a 70:30 
training and testing ratio [ 49]. Rashmi [ 11] conducted their experiments using the 
Cleveland dataset subset 303. Decision Tree, the suggested method, has an accu-
racy rate of 75.55%. ML techniques were examined with the CVD-established risk 
measure Hellenic Score [ 38], which was used to evaluate the risks of hypertension, 
hyperglycemia, and hyperlipidemia [ 7]. In this analysis, The Framingham and Cleve-
land tests had categorization accuracies of 91% and 93%, respectively [ 40], and the 
Hoeffding classiﬁer had a CVD prediction accuracy of 88.56%, using ensemble mod-
els to predict the risk of heart disease [ 27]. The three distinct ensemble methods’ 
accuracy, sensitivity, and speciﬁcity were 93.44%, 89.28%, and 96.96%, respectively 
[ 9]. 
Using 280 instances of the UCI dataset introduced a novel hybrid classiﬁer selec-
tion method called HRFLC that combines random forest, AdaBoost, and linear cor-
relation [ 25]. Filter, wrapper, and embedding approaches were used to choose the 
eleven (11) features; the accuracy of the hybrid norm improved by 2% [ 32]. 
Saqlain [ 46] attained 68.6% perfection across the AFIC datasets, whereas Random 
Forest (RF) received 80.89% exactness [ 38]. The K-Nearest Neighbors technique has 
been used on the same dataset by Sharma [ 28] and Dwivedi [ 8] 90.16% and 80%, 
respectively, were the outcomes. 
The accuracy of 450 data from Kita Hospital Jakarta Enriko [ 33] was 46%. The 
Cleveland dataset enhanced the result by 56.13% by using AdaBoost [ 35]. Shetty 
[ 22] used the Starlog 270 data to obtain 89% accuracy, and The identical technique 
utilized by Chaurasia with a hybrid method produced 75.9% accuracy. 
According to Cheng and Chaurasia, an Artiﬁcial Neural Network model accuracy 
was 82.5% [ 17], and hybrid norm accuracy was 78.88% [ 27]. For this investigation, 
an ATTICA dataset was chosen. The Hellenic Score displayed 20% speciﬁcity, 58% 
NPV, 85% accuracy, 87% PPV, and 97% sensitivity, depending on the kind of clas-
siﬁer and training dataset. While the speciﬁcity, accuracy, sensitivity, PPV, and NPV 
of the ML algorithms ranged from 65% to 84%, 46% to 56%, 67% to 89%, and 24% 
to 45%, respectively.

Heart Disease and Heart Failure Prediction
485
3 
Methodology 
3.1 
Dataset Description 
In this research, two datasets were used. The ﬁrst was to forecast cardiac disease, and 
the second was to predict heart failure and analyze death from heart failure. Table 1 
underlines the datasets’ applicability and description. 
3.2 
Data Preprocessing 
The obtained data standard may be lower due to missing values and noisy data, which 
impacts the ﬁnal forecast quality. As a result, data preparation techniques cleaning, 
normalization, selections of features, and extracting data are necessary to make data 
suitable for mining and analysis. Several preprocessing strategies were utilized to 
prepare the data for training. We used a different technique for each dataset because 
we used two datasets with varying architectures of the feature. In Dataset 1, ﬁrst, 
we removed AgeCategory, Race, and GenHealth from the features before converting 
them into boolean values. For the remaining segments, if the answer is “no,” we put 
the value to 0, and if it’s “yes,” we set the value to 1. If a person is male, we assessed 
their gender to 1; otherwise, to 0. It can be challenging to handle when numerical and 
categorical data mix. Preprocessing procedures are necessary to convert categorical 
data into a numeric form. Therefore, we must manage categorical data. To collect our 
category data, we used one hot encoder. For model selection, we used the train test 
split method, where training data is 70%, and testing data is 30% for both datasets. 
Finally, we used the standardscaler to preprocess Dataset 2. 
3.3 
Prediction Model 
We used Artiﬁcial Neural Networks to predict cardiac disorders. To predict heart 
failure deaths, stacking and bagging ensemble techniques were used. The models 
utilized in the research are outlined below and illustrated in Fig. 1. 
Table 1 Datasets’ summary. 
Dataset
Application
Size 
Dataset 1
Predict heart disease
319795 data points with 18 
columns 
Dataset 2
Analysis and predict the death 
of heart failure 
299 data points with 13 
columns

486
A. Al Maruf et al.
Fig. 1 Work ﬂow diagram of the proposed system. 
i. Artiﬁcial Neural Network (ANN): ANN is used for complicated and intricate 
tasks. A neural network frequently consists of processing units that resemble neurons 
linked together using weighted connections. Artiﬁcial neural networks can function 
in two modes. The ﬁrst method involves the transmission of activation across the 
network. The second mode involves learning, in which the network arranges itself 
typically based on the most recent activation Transfer. In essence, ANN has an input, 
a hidden layer, and an output layer. We use Keras to create our ANN model. To create 
the neural network, the Sequential() method is used. The input layer, which has 
12 nodes, is initially made. The number of rows in our practice set is 12. The hidden 
layers are then added. We employ two hidden layers to make things straightforward. 
There are twelve nodes in the ﬁrst hidden layer and eight in the following layer. We 
used the relu activation function in the hidden part. We then include the output 
layer. The model is visualized in Fig. 2. To train the ANN, we carry out the following 
actions: To construct the model and employ the Adam optimizer. The binary cross-
entropy loss is employed. One hundred epochs are used to train the model. In Table 2, 
the ANN architecture is summarized. 
ii. Bagging Ensemble: The bootstrap aggregating algorithm is often known as the 
bagging algorithm. It is one of the earliest and most basic ML algorithms and works 
well for issues with tiny training datasets. In this approach, a collection of original 
models with replacement are trained using random subsets of data acquired using the 
bootstrap sampling method. The ﬁnal result is obtained by combining the individual 
models generated from the bootstrap samples through a process of majority voting. 
Bagging is used in regression and classiﬁcation to increase the accuracy of ML

Heart Disease and Heart Failure Prediction
487
Fig. 2 ANN architecture. 
Table 2 ANN training 
parameters. 
Type
Values 
optimizer
adam 
loss
binary_crossentropy 
batch_size 
32 
epochs
100 
Table 3 Bagging ensemble 
parameters. 
Type
Values 
base_estimator 
RF 
n_estimators
500 
max_samples
0.8 
bootstrap
True 
oob_score
True 
techniques. We used the bagging ensemble since the dataset we have to forecast heart 
failure is limited. The parameters of the bagging ensemble are shown in Table 3. 
iii. Stacking Ensemble: Stacking is an ensemble learning approach that employs 
multiple heterogeneous classiﬁers, the predictions of which are then integrated into 
a ﬁnal classiﬁer. The base model ﬁrst trained the model, and a meta-classiﬁer then 
trained the output of the base model. The stacking ensemble in this work consists 
of KNN, RF, DT, and XGB as basis classiﬁers, whose predictions were utilized 
for training by the LR meta-classiﬁer. To improve the performance of our stacking 
model, we gave numerous parameters to our base estimators.

488
A. Al Maruf et al.
4 
Result and Discussion 
The following four performance evaluation criteria were used in our study. 
upper P r e c i s i o n left parenthesis upper P right parenthesis equals StartFraction upper T upper P Over upper T upper P plus upper F upper P EndFraction semicolon upper R e c a l l left parenthesis upper R right parenthesis equals StartFraction upper T upper P Over upper T upper P plus upper F upper N EndFractionPrecision (P) =
T P
T P + F P ;
Recall (R) =
T P
T P + F N
(1) 
upper F minus m e a s u r e equals 2 times StartFraction upper P times upper R Over upper P plus upper R EndFraction semicolon upper A c c u r a c y equals StartFraction upper T upper N plus upper T upper P Over upper T upper N plus upper T upper P plus upper F upper N plus upper F upper P EndFractionF-measure = 2 × P × R
P + R ;
Accuracy =
T N + T P
T N + T P + F N + F P
(2) 
To predict heart diseases, we employed ANN. Table 4 summarizes the performance of 
heart disease (cardiac disorder) prediction obtained by ANN. The evaluation metrics 
used are listed in the equations above. 
Precision evaluates the accuracy of our heart disease prediction model in identify-
ing actual positive cases among the cases it has predicted as positive. In this case, the 
precision is 61%, which means that of all the instances the model predicted as having 
heart disease, 61% of them were actually positive. Recall assesses the capability of 
our heart disease prediction model to correctly identify all cases of heart disease. The 
recall is 53%, which means that the model was able to correctly identify 53% of all 
positive instances. The F-measure is a combined metric that balances the precision 
and the recall in our heart disease prediction model by taking a weighted average 
of both. It provides a single score that summarizes the performance of the model 
and F-measure is 57%. Accuracy measures the overall performance of the model in 
terms of correct predictions. In this case, the accuracy is 91.56%, this implies that 
our model was capable of correctly predicting 91.56% of all cases. 
In conclusion, the heart disease prediction model using ANN has relatively good 
accuracy, but there needs to be an improvement in precision and recall. 
To Predict heart failure, we applied the bagging and stacking method. Table 5 
shows the performance of predicting heart failure. From the table, it seems that the 
bagging model performs better in terms of precision, with a precision score of 94% 
compared with the stacking model (82%). However, the stacking model has a slightly 
better recall score of 73% compared to 69% for the bagging model. Regarding F-
measure, which balances precision and recall, both models perform similarly with a 
score of 80% for the bagging model and 78% for the stacking model. 
Table 4 Performance of 
heart diseases prediction 
(ANN). 
Evaluation_metric 
Performance 
Precision
61% 
Recall
53% 
F-measure
57% 
Accuracy
91.56%

Heart Disease and Heart Failure Prediction
489
Table 5 Performance of 
heart failure prediction. 
Evaluation_metric 
Bagging 
Stacking 
Precision
94%
82% 
Recall
69%
73% 
F-measure
80%
78% 
Accuracy
90%
87% 
AUC
92%
78% 
Fig. 3 Confusion matrix. 
When it comes to calculating the accuracy, the bagging model is slightly better 
than the stacking model, with a score of 90% compared to 87%. However, the bagging 
model also has a higher area under the curve (AUC) score (92%) compared to the 
stacking model (78%). So we calculate the confusion matrix of the bagging model 
as it gives a good performance. The confusion matrix is shown in Fig. 3. Overall, the 
bagging model proves good performance in precision and AUC, while the stacking 
model exhibits slightly better recall and F-measure. 
5 
Conclusion 
Heart illnesses endanger people’s lives. It is presently happening at an alarming rate. 
Aside from heart disease, heart failure is a harmful part of the human body that 
kills many people. The research had numerous objectives. Prediction of early-stage 
cardiac disease can help to minimize the worrisome rate. We created a technology 
that can forecast cardiac problems. ANN predicted heart illnesses with a 91.56% 
accuracy. Because of the uneven dataset, the accuracy and recall value is insufﬁcient 
to forecast cardiac illnesses. Our method may also be able to anticipate heart failure. 
The bagging ensemble has the highest accuracy in predicting heart failure (90%). 
Precision (82%), recall (73%), f-measure (80%), and AUC (92%) values are likewise 
adequate for predicting heart failure. Death from heart failure was studied statistically. 
This study can assist experts in making decisions for any patient. This model can 
assist specialists. Finally, collecting image data for further growth in this ﬁeld is a 
challenging but promising direction.

490
A. Al Maruf et al.
References 
1. Al-Shayea QK (2011) Artiﬁcial neural networks in medical diagnosis. Int J Comput Sci Issues 
8(2):150–154 
2. Alaa AM, Bolton T, Di Angelantonio E, Rudd JH, Van der Schaar M (2019) Cardiovascular 
disease risk prediction using automated machine learning: a prospective study of 423,604 UK 
Biobank participants. PLOS ONE 14(5):e0213653 
3. Amin MS, Chiam YK, Varathan KD (2019) Identiﬁcation of signiﬁcant features and data 
mining techniques in predicting heart disease. Telematics Inform 36:82–93 
4. Ananey-Obiri D, Sarku E (2020) Predicting the presence of heart diseases using comparative 
data mining and machine learning algorithms. Int J Comput Appl 176(11):17–21 
5. Andreotti F, et al (2020) Prediction of the onset of cardiovascular diseases from electronic 
health records using multi-task gated recurrent units. arXiv preprint arXiv:2007.08491 
6. Ashraf M et al (2021) Prediction of cardiovascular disease through cutting-edge deep learning 
technologies: an empirical study based on TENSORFLOW, PYTORCH and KERAS. In: Gupta 
D, Khanna A, Bhattacharyya S, Hassanien AE, Anand S, Jaiswal A (eds) International Con-
ference on Innovative Computing and Communications, vol 1165. AISC. Springer, Singapore, 
pp 239–255. https://doi.org/10.1007/978-981-15-5113-0_18 
7. Chen M, Hao Y, Hwang K, Wang L, Wang L (2017) Disease prediction by machine learning 
over big data from healthcare communities. IEEE Access 5:8869–8879 
8. Chicco D, Jurman G (2020) Machine learning can predict survival of patients with heart failure 
from serum creatinine and ejection fraction alone. BMC Med Inform Decis Making 20(1):1–16 
9. Dinesh KG, Arumugaraj K, Santhosh KD, Mareeswari V (2018) Prediction of cardiovascu-
lar disease using machine learning algorithms. In: 2018 International Conference on Current 
Trends towards Converging Technologies (ICCTCT). IEEE, pp 1–7 
10. Dwivedi AK (2018) Performance evaluation of different machine learning techniques for pre-
diction of heart disease. Neural Comput Appl 29(10):685–693 
11. Ed-Daoudy A, Maalmi K (2019) Performance evaluation of machine learning based big data 
processing framework for prediction of heart disease. In: 2019 International Conference on 
Intelligent Systems and Advanced Computing Sciences (ISACS). IEEE, pp 1–5 
12. Gazeloglu C (2020) Prediction of heart disease by classifying with feature selection and 
machine learning methods. Progr Nutrition 22(2):660–670 
13. Ghosh P et al (2021) Efﬁcient prediction of cardiovascular disease using machine learning 
algorithms with relief and LASSO feature selection techniques. IEEE Access 9:19304–19326 
14. Ghwanmeh S, Mohammad A, Al-Ibrahim A (2013) Innovative artiﬁcial neural networks-based 
decision support system for heart diseases diagnosis. J Intell Learn Syst Appl 5(3) 
15. Goel S, Deep A, Srivastava S, Tripathi A (2019) Comparative analysis of various techniques 
for heart disease prediction. In: 2019 4th International Conference on Information Systems and 
Computer Networks (ISCON). IEEE, pp 88–94 
16. Gupta A, Kumar R, Arora HS, Raman B (2019) MIFH: a machine intelligence framework for 
heart disease diagnosis. IEEE Access 8:14659–14674 
17. Haq AU, Li JP, Memon MH, Nazir S, Sun R (2018) A hybrid intelligent system framework for 
the prediction of heart disease using machine learning algorithms. Mob Inf Syst 2018 
18. Javid I, Alsaedi AKZ, Ghazali R (2020) Enhanced accuracy of heart disease prediction using 
machine learning and recurrent neural networks ensemble majority voting method. Int J Adv 
Comput Sci Appl 11(3) 
19. Jousilahti P, Vartiainen E, Tuomilehto J, Puska P (1999) Sex, age, cardiovascular risk factors, 
and coronary heart disease: a prospective follow-up study of 14 786 middle-aged men and 
women in Finland. Circulation 99(9):1165–1172 
20. Karthick D, Priyadharshini B (2018) Predicting the chances of occurrence of cardio vascular 
disease (CVD) in people using classiﬁcation techniques within ﬁfty years of age. In: 2018 2nd 
International Conference on Inventive Systems and Control (ICISC). IEEE, pp 1182–1186 
21. Kaur A (2017) A comprehensive approach to predict heart diseases using data mining. Int J 
Innov Eng Technol 8(2):2319–2358

Heart Disease and Heart Failure Prediction
491
22. Kavitha M, Gnaneswar G, Dinesh R, Sai YR, Suraj RS (2021) Heart disease prediction using 
hybrid machine learning model. In: 2021 6th International Conference on Inventive Computa-
tion Technologies (ICICT). IEEE, pp 1329–1333 
23. Kodati S, Vivekanandam DR (2018) Analysis of heart disease using in data mining tools Orange 
and Weka. Glob J Comput Sci Technol 
24. Kumar NK, Sikamani KT (2020) Prediction of chronic and infectious diseases using machine 
learning classiﬁers-a systematic approach. Int J Intell Syst 13(4):11–20 
25. Kumar NK, Sindhu GS, Prashanthi DK, Sulthana AS (2020) Analysis and prediction of cardio 
vascular disease using machine learning classiﬁers. In: 2020 6th International Conference on 
Advanced Computing and Communication Systems (ICACCS). IEEE, pp. 15–21 
26. Kumar R, Rani P (2020) Comparative analysis of decision support system for heart disease. 
Adv Math Sci J 9(6):3349–3356 
27. Latha CBC, Jeeva SC (2019) Improving the accuracy of prediction of heart disease risk based 
on ensemble classiﬁcation techniques. Inform Med Unlocked 16:100203 
28. Louridi N, Amar M, El Ouahidi B (2019) Identiﬁcation of cardiovascular diseases using 
machine learning. In: 2019 7th Mediterranean congress of telecommunications (CMT). IEEE, 
pp 1–6 
29. Mienye ID, Sun Y, Wang Z (2020) An improved ensemble learning approach for the prediction 
of heart disease risk. Inform Med Unlocked 20:100402 
30. Mishra J, Tarar S (2020) Chronic disease prediction using deep learning. In: Singh M, Gupta 
PK, Tyagi V, Flusser J, Ören T, Valentino G (eds) ICACDS 2020, vol 1244. CCIS. Springer, 
Singapore, pp 201–211. https://doi.org/10.1007/978-981-15-6634-9_19 
31. Mohan S, Thirumalai C, Srivastava G (2019) Effective heart disease prediction using hybrid 
machine learning techniques. IEEE Access 7:81542–81554 
32. Mourao-Miranda J, Bokde AL, Born C, Hampel H, Stetter M (2005) Classifying brain states 
and determining the discriminating activation patterns: support vector machine on functional 
MRI data. NeuroImage 28(4):980–995 
33. Obasi T, Shaﬁq MO (2019)Towards comparing and using machine learning techniques for 
detecting and predicting heart attack and diseases. In: 2019 IEEE International Conference on 
Big Data (Big Data). IEEE, pp 2393–2402 
34. Oh MS, Jeong MH (2020) Sex differences in cardiovascular disease risk factors among Korean 
adults. Korean J Med 95(4):266–275 
35. Pasha SJ, Mohamed ES (2020) Novel feature reduction (NFR) model with machine learning and 
data mining algorithms for effective disease risk prediction. IEEE Access 8:184087–184108 
36. Pavithra V, Jayalakshmi V (2021) Hybrid feature selection technique for prediction of cardio-
vascular diseases. In: Materials Today: Proceedings 
37. Perumal R, Kaladevi A (2020) Early prediction of coronary heart disease from Cleveland 
dataset using machine learning techniques. Int J Adv Sci Technol 29:4225–4234 
38. Pouriyeh S, Vahid S, Sannino G, De Pietro G, Arabnia H, Gutierrez J (2017) A comprehensive 
investigation and comparison of machine learning techniques in the domain of heart disease. 
In: 2017 IEEE Symposium on Computers and Communications (ISCC). IEEE, pp 204–207 
39. Ramalingam V, Dandapath A, Raja MK (2018) Heart disease prediction using machine learning 
techniques: a survey. Int J Eng Technol 7(2.8):684–687 
40. Rashmi G, Kumar U (2019) Machine learning methods for heart disease prediction. Int J Eng 
Adv Technol 8(5S):220–223 
41. Reddy KVV, Elamvazuthi I, Aziz AA, Paramasivam S, Chua HN, Pranavanand S (2021) Heart 
disease risk prediction using machine learning classiﬁers with attribute evaluators. Appl Sci 
11(18):8352 
42. Saqlain SM et al (2019) Fisher score and Matthews correlation coefﬁcient-based feature subset 
selection for heart disease diagnosis using support vector machines. Knowl Inf Syst 58(1):139– 
167 
43. Shamrat FJM, Raihan MA, Rahman AS, Mahmud I, Akter R et al (2020) An analysis on breast 
disease prediction using machine learning approaches. Int J Sci Technol Res 9(02):2450–2455

492
A. Al Maruf et al.
44. Sharma H, Rizvi M (2017) Prediction of heart disease using machine learning algorithms: a 
survey. Int J Recent Innov Trends Comput Commun 5(8):99–104 
45. Sharma S, Parmar M (2020) Heart diseases prediction using deep learning neural network 
model. Int J Innov Technol Explor Eng 9(3):124–137 
46. Tama BA, Im S, Lee S (2020) Improving an intelligent detection system for coronary heart 
disease using a two-tier classiﬁer ensemble. BioMed Res Int 2020 
47. Tougui I, Jilbab A, El Mhamdi J (2020) Heart disease classiﬁcation using data mining tools 
and machine learning techniques. Health Technol 10(5):1137–1144 
48. Trevisan C, Sergi G, Maggi S (2020) Gender differences in brain-heart connection. Brain Heart 
Dyn. 937–951 
49. Weng SF, Reps J, Kai J, Garibaldi JM, Qureshi N (2017) Can machine-learning improve 
cardiovascular risk prediction using routine clinical data? PLOS ONE 12(4):e0174944 
50. Yadav DC, Pal S (2020) Prediction of heart disease using feature selection and random forest 
ensemble method. Int J Pharm Res 12(4):56–66 
51. Zhao D (2021) Epidemiological features of cardiovascular disease in Asia. JACC: Asia 1(1):1– 
13

Veriﬁable Secret Image Sharing 
with Cheater Identiﬁcation 
Franco Debashis Ekka, Sourabh Debnath, Jitendra Kumar, 
and Ramesh Kumar Mohapatra 
Abstract As digital communication has grown in a variety of industries where data 
must be secure and conﬁdential, secret image transmission has grown in popular-
ity. It will be a severe issue if false data is produced in the medicine, military, or 
diplomacy sectors. The three actors in conventional secret image sharing are the 
dealer, the participants, and the combiner. There is a chance that the Combiner will 
fabricate information, a participant will alter the data and submit a manipulated 
share, or someone will pose as a dealer and provide false information. To prevent 
data tampering, all actors in the secret image-sharing process should be veriﬁed and 
authenticated during transmission. This issue introduces a new area of study on secret 
sharing known as veriﬁcation of secret sharing scheme, which addresses cheating 
activities by implementing various veriﬁcation which includes dealer veriﬁcation, 
cheating detection, and combiner veriﬁcation techniques. This paper implements a 
secret image sharing (SIS) scheme with the ability to detect the cheater. Bitwise OR 
secret sharing scheme is used to share the secrets, and for veriﬁcation one-way hash 
function, image hashing, and XOR operations are used. Encryption of the secret is 
done using the entropy of the image as the key. The dealer assigns shadow shares 
and shadow ID’s to the participants, which are then reconstructed by the combiner 
to create the original image. Participants validate the combiner’s authenticity, and 
the combiner validates the integrity of the shares before reconstruction. After the 
construction, the participants once again verify if the secret constructed is true or 
false data. 
F. D. Ekka (B) · S. Debnath · J. Kumar · R. K. Mohapatra 
Department of Computer Science and Engineering, National Institute Of Technology, Rourkela, 
Odisha, India 
e-mail: 221cs2065@nitrkl.ac.in 
S. Debnath 
e-mail: 519cs1014@nitrkl.ac.in 
J. Kumar 
e-mail: 520cs1001@nitrkl.ac.in 
R. K. Mohapatra 
e-mail: mohapatrark@nitrkl.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_42 
493

494
F. D. Ekka et al.
Keywords Secret Sharing · Image Encryption · Shadow Secret Sharing ·
Combiner Authentication · Cheating Detection · Cheater Identiﬁcation 
1 
Introduction 
The security of sensitive images from malicious users while they are transmitted over 
the network and making sure the information is incomprehensible to any unauthorized 
users is critical, especially in sectors such as the military, diplomatic, and medical 
ﬁelds [ 2]. Numerous cryptography or steganography techniques are used in an effort 
to safeguard information and adhere to conﬁdentiality criteria [ 1]. The Secret Sharing 
approach is preferred for sharing information within a group. Conventional secret-
sharing schemes have three main components: a dealer, participants, and a combiner. 
The ﬁrst secret sharing was introduced by Shamir [ 6], which was based on Lagrange 
interpolation. The goal was to divide a data D into n pieces so that with access to k 
or more than k pieces, the data can be computed but cannot be recovered even with 
k-1 pieces. This scheme was called (k,n) threshold scheme. The ﬁrst stage is for the 
dealer to divide the image into n shares and distribute them to the participants. The 
combiner collects each share, reconstructs it, and the image is then obtained. 
Tompa and Woll [ 10] introduced the need for image veriﬁcation and added an 
extension to a new ﬁeld of research in secret sharing called veriﬁable secret sharing 
(VSS). VSS are of three types: Dealer Veriﬁcation, i.e., the dealer should be veriﬁed 
by all the participants so that no false information should be circulated to them. 
Combiner Veriﬁcation, i.e., the combiner should be veriﬁed by all the participants to 
check the authenticity of the combiner to conﬁrm the genuineness of the reconstructed 
image. Cheating Detection, i.e., combiners are responsible for detecting if there is 
any manipulation of data and detecting the cheater. 
2 
Literature Survey 
In S. Kandar & B.C. Dhara proposed method [ 3], the participants receive shadow 
shares from the dealer where each participant can verify the authenticity of the com-
biner. This scheme consists of three phases. ,i.e., registration phase, share generation 
and distribution phase, reconstruction and veriﬁcation phase. 
J. Shao proposed a scheme [ 7] based on Shamir’s secret sharing [ 6] and hash 
function, which has (k,t,n) access structure. The dealer shares k secret among partic-
ipants, but before that, the dealer generates shadow shares (SW) and then provides 
upper S upper W Subscript iSWi to the participants. Combiner uses a recovery algorithm and veriﬁes the SW 
provided by the participants, and if all the shares satisfy the provided equation, then 
the secret is reconstructed. 
B. Rajabi & Z. Eslami proposed [ 4] a collision resistance and a threshold veriﬁable 
secret sharing scheme based on a lattice problem where the participants can verify

Veriﬁable Secret Image Sharing with Cheater Identiﬁcation
495
the authenticity and consistency of the shares without any communication. When the 
dealer generates the share and sends it to the participants through a secure channel, 
it also generates public information, which can be used for the veriﬁcation of shares 
by the user. 
A. Soreng & S. Kandar proposed a scheme [ 8] based on Bitwise OR operation 
for sharing secrets, and XOR and hash functions are performed for the veriﬁcation 
process. The dealer uses the entropy of the image as a key and also provides shadow 
shares to the participants instead of actual data with veriﬁcation messages for the 
veriﬁcation process. Participants verify the authenticity of the combiner before send-
ing secret shadow shares to the combiner. Before constructing the secret, combiner 
ﬁrst checks if the shares provided by the participants are manipulated or not. 
3 
Preliminaries 
3.1 
Generating Share Matrix 
After the secret image is generated, the secret is then divided into n equal halves, 
where n is the total number of participants present in the group. These equal halves 
of the secret image are known as shares. At the time of reconstruction, at least k 
numbers of shares are needed, where k is a threshold set by the Dealer and should be 
maintained. To maintain the bit value, if the original bit is ‘0’, then at the particular 
position of the secret image, the value should be set to ‘0’. If the original bit is 
‘1’, then at least (n-k+1) shares should set the bit to ‘1’ in that particular position. 
This way, only (kminus−1) shares will be left with ’0’ in them, and the total number of k 
is maintained throughout the process. 
Figure 1 is a simple example of the above-
discussed share matrix, which shows how a binary matrix M is generated from a 
secret with the value 5602. If the bit value in the original secret is ‘1’, a column in 
matrix M is chosen at random, and bit values are set to 1 in k or more than k position 
column-wise. As no image encryption is done in the above process, it can lead to 
the partial leaking of information, and to avoid this situation, image encryption is 
needed (Table 1). 
Fig. 1 Secret Image Sharing 
using Share Matrix

496
F. D. Ekka et al.
Table 1 Notation used throughout the proposed approach 
Notations 
Symbols
Deﬁnition 
h
One way hash function (SHA-256) 
H
Image Hashing function 
circled plus⊕
XOR method 
parallel to||
Concatenation Operation 
n
Total number of Participants 
k
Threshold value 
Img
Original Image 
upper I Subscript e n cIenc
Secret Image 
CID
Combiner’s Unique Identiﬁcation Number 
PWD
Combiner’s Password 
R
Random Number 
M
Random Matrix Generated using Combiner’s 
attribute 
PM
Participant Matrix 
ID
Participant’s ID 
SH
Generated Share 
SW_SH
Shadow Share 
SW_ID
Shadow ID 
upper V e r upper M Superscript 1V er M1
Veriﬁer message to identify Cheater 
upper V e r upper M squaredV er M2
Veriﬁer message to check Combiner’s 
authenticity 
upper F upper V Subscript iFVi
Veriﬁes the reconstructed secret 
Out_Img
Reconstructed Secret 
3.2 
Image Fingerprinting 
For checking the genuineness of an image, Image ﬁngerprinting is used. Image 
ﬁngerprinting is also known as Image hashing, which holds three basic properties 
that is robustness, discrimination, and the avalanche effect [ 9]. Any image hashing 
algorithm that returns a ﬁxed bit length can be used in the suggested method. We used 
the average image hashing approach, also known as a-hash, to demonstrate image 
hashing, which ﬁrst takes the image’s mean value and either assigns a ’0’ or a ’1’ on 
their respective position depending on the condition. 
upper M upper E upper A upper N equals StartFraction upper S u m o f a l l p i x e l s upper T o t a l Over upper P i x e l s EndFractionM E AN = Sum of all pixels T otal
Pixels
upper I f left parenthesis upper P i x e l v a l u e less than equals upper M upper E upper A upper N right parenthesis t h e n a s s i g n Baseline 0I f ( Pixel value <= M E AN ) then assign 0

Veriﬁable Secret Image Sharing with Cheater Identiﬁcation
497
upper I f left parenthesis upper P i x e l v a l u e greater than upper M upper E upper A upper N right parenthesis t h e n a s s i g n Baseline 1I f ( Pixel value > M E AN ) then assign 1
3.3 
One Way Hashing 
It is a primitive cryptographic algorithm that takes variable length input and returns 
a ﬁxed bit length output. One-way hashing functions satisfy the avalanche effect 
property. They are relatively simple to compute, but it is extremely difﬁcult to reverse 
the process and recover the original data from the hashed value. SHA-256 hash 
algorithm is used in our proposed approach, which accepts variable length input and 
returns 64 bits of output. 
3.4 
Chaotic Logistic Map 
A chaotic logistic map is used to generate a pseudo-random sequence [ 5]. The ran-
domness is achieved using a control parameter (r). Then the value of r is put in the 
equation given below to generate a random sequence. 
x Subscript n plus 1 Baseline equals r times x Subscript n Baseline times left parenthesis 1 minus x Subscript n Baseline right parenthesisxn+1 = r × xn × (1 −xn)
(1) 
4 
Proposed Work 
In previously mentioned schemes, many methods were proposed for verifying dealer, 
participants, and combiner in veriﬁable secret image sharing (VISS). Dealer were 
examined before the participant accepted the shares, the combiner was veriﬁed before 
participants sent their secret shares, and the combiner was employed to verify the 
shares sent by the participants before reconstructing and investigating for the identity 
of the cheater in case cheating was detected. However, if any entity has more than two 
roles, a problem will arise. For instance, if the combiner is also a participant. After 
Combiner veriﬁcation, the combiner can manipulate the data and construct false data 
during reconstruction. We address this issue in the proposed approach by adding an 
extra veriﬁcation layer in which the reconstructed data is once again veriﬁed by the 
participants by performing hash operations on the reconstructed secret generated by 
the combiner and shadow ID’s provided by the dealer during the share distribution 
phase. This approach contains three actors, the same as a conventional secret-sharing 
scheme, dealer, participants, and combiner. It consist of six phases:

498
F. D. Ekka et al.
4.1 
Initialization Phase 
The ﬁrst step in this process is to encrypt the secret to avoid partial leakage of 
information while generating a shared matrix using bitwise OR. The encryption is 
done using a chaotic logistic map which generates a pseudo-random sequence using 
Eq. 2. The key used in the equation is the entropy of the secret image, and the control 
parameters r lie between 3.85 to 3.99 and the initial value of x Subscript n Baseline element of left bracket 0 comma 1 right bracketxn ∈[0, 1]. 
upper H left parenthesis upper I m g right parenthesis equals StartFraction 1 Over upper P EndFraction sigma summation Underscript i equals 0 Overscript 255 Endscripts p r o b left parenthesis v a l Subscript i Baseline right parenthesis times l o g StartFraction 1 Over p r o b left parenthesis v a l Subscript i Baseline right parenthesis EndFractionH(Img) = 1
P
255
Σ
i=0
prob(vali) × log
1
prob(vali)
(2) 
upper P colon n u m b e r o f p i x e l s comma v a l Subscript i Baseline colon p i x e l v a l u e comma p r o b colon p r o b a b i l i t y o f p i x e l v a l u eP : number of pixels, vali : pixel value, prob : probability of pixel value
4.2 
Registration Phase 
In this phase, the combiner chooses an ID (CID), password (PWD), and a randomly 
chosen number R. This data is sent to the Dealer for registration, but before that, 
the combiner computes two operations and then sends the output to the Dealer for 
registration process through a secure channel. 
upper P upper K upper Y equals h left parenthesis upper C upper I upper D parallel to upper P upper W upper D right parenthesisPKY = h(C I D||PW D)
upper V upper R equals h left parenthesis upper C upper I upper D parallel to upper R right parenthesisV R = h(C I D||R)
4.3 
Share Generation Phase 
After the combiner registers itself, the dealer takes PSK as the key to generate a 
random matrix using a chaotic logistic map where each element has an integer value 
between 0–255. n shares (upper S upper H Subscript iSHi) are generated by the dealer by dividing upper I m g Subscript e n cImgenc
using share matrix scheme with bitwise OR. Distinct IDs (upper I upper D Subscript iI Di) are assigned to the 
participants by the dealer, and using the upper I upper D Subscript iI Di as keys, Participants Matrix (upper P upper M Subscript iPMi) is  
generated.

Veriﬁable Secret Image Sharing with Cheater Identiﬁcation
499
4.4 
Share Distribution Phase 
To add an extra degree of protection before issuing shares to participants, the dealer 
generates shadow sharesupper S upper W normal bar upper S upper H Subscript iSW_SHi and shadow IDsupper S upper W normal bar upper I upper D Subscript iSW_I Di instead ofupper S upper H Subscript iSHi andupper I upper D Subscript iI Di. 
It also generates three veriﬁcation messages that will be used later in the veriﬁcation 
phase to authenticate the actors in the veriﬁcation and reconstruction phase, as well 
as to validate the integrity of the reconstructed secret in the ﬁnal veriﬁcation phase. 
upper S upper W normal bar upper S upper H Subscript i Baseline equals upper S upper H Subscript i Baseline circled plus upper M circled plus upper P upper M Subscript iSW_SHi = SHi ⊕M ⊕PMi
upper S upper W normal bar upper I upper D Subscript i Baseline equals upper I upper D Subscript i Baseline circled plus upper H left parenthesis upper M right parenthesis circled plus upper H left parenthesis upper S upper W normal bar upper S upper H Subscript i Baseline right parenthesisSW_I Di = I Di ⊕H(M) ⊕H(SW_SHi)
upper F upper V Subscript i Baseline equals h left parenthesis upper I m g StartAbsoluteValue EndAbsoluteValue upper H left parenthesis upper S upper W normal bar upper I upper D Subscript i Baseline right parenthesis right parenthesisFVi = h(Img||H(SW_I Di))
upper V e r upper M Subscript i Superscript 1 Baseline equals h left parenthesis upper S upper W normal bar upper I upper D Subscript i Baseline right parenthesis circled plus h left parenthesis upper H left parenthesis upper S upper H Subscript i Baseline right parenthesis parallel to upper I upper D Subscript i Baseline right parenthesisV er M1
i = h(SW_I Di) ⊕h(H(SHi)||I Di)
upper V e r upper M Subscript i Superscript 2 Baseline equals h left parenthesis upper S upper W Subscript upper I Baseline upper D Subscript i Baseline right parenthesis circled plus upper V upper RV er M2
i = h(SWI Di) ⊕V R
After the computation, Dealer shares (upper S upper W normal bar upper S upper H Subscript i Baseline comma upper S upper W normal bar upper I upper D Subscript i Baseline comma upper F upper V Subscript i Baseline comma upper V e r upper M Subscript i Superscript 1 Baseline comma upper V e r upper M Subscript i Superscript 2upper S upper W normal bar upper S upper H Subscript i Baseline comma upper S upper W normal bar upper I upper D Subscript i Baseline comma upper F upper V Subscript i Baseline comma upper V e r upper M Subscript i Superscript 1 Baseline comma upper V e r upper M Subscript i Superscript 2SW_SHi, SW_I Di, FVi, V er M1
i , V er M2
i ) to 
the participants and then the dealer deletes all it belonging. 
4.5 
Veriﬁcation and Reconstruction Phase 
Combiner Veriﬁcation. The combiner initiates the veriﬁcation process by choosing 
k participants. Combiner sends VR value to the k participants for authentication. 
Participants computes upper V e r upper M Subscript iV er Mi: 
upper V e r upper M Subscript i Baseline equals h left parenthesis upper S upper W normal bar upper I upper D Subscript i Baseline right parenthesis circled plus upper V upper RV er Mi = h(SW_I Di) ⊕V R
If upper V e r upper M Subscript iV er Mi = upper V e r upper M Subscript i Superscript 2V er M2
i then the participant sends its upper S upper W normal bar upper I upper D Subscript iSW_I Di, upper S upper W normal bar upper S upper H Subscript iSW_SHi and upper V e r upper M Subscript i Superscript 1V er M1
i
to the combiner else combiner is not authenticated, and the process is terminated in 
this phase. 
Cheater Identiﬁcation. After Combiner Veriﬁcation, the Combiner computes a ran-
dom matrix (M) using its PKY as key. Then Combiner computes upper I upper D prime Subscript iI D'
i to generates 
Participants Matrix (upper P upper M prime Subscript iPM'
i) using  upper I upper D prime Subscript iI D'
i as key and upper S upper H prime Subscript iSH '
i and upper V e r upper M prime Subscript iV er M'
i for Participant 
veriﬁcation: 
upper I upper D Subscript i Superscript prime Baseline equals upper S upper W normal bar upper I upper D Subscript i Baseline circled plus upper H left parenthesis upper M right parenthesis circled plus upper H left parenthesis upper S upper W normal bar upper S upper H Subscript i Baseline right parenthesisI D'
i = SW_I Di ⊕H(M) ⊕H(SW_SHi)
upper S upper H Subscript i Superscript prime Baseline equals upper S upper W normal bar upper S upper H Subscript i Baseline circled plus upper M circled plus upper P upper M Subscript iSH '
i = SW_SHi ⊕M ⊕PMi
upper V e r upper M Subscript i Baseline equals h left parenthesis upper S upper W normal bar upper I upper D Subscript i Baseline right parenthesis circled plus h left parenthesis upper H left parenthesis upper S upper H Subscript i Baseline right parenthesis right parenthesis parallel to upper I upper D Subscript i BaselineV er Mi = h(SW_I Di) ⊕h(H(SHi))||I Di

500
F. D. Ekka et al.
[a]
[b]
[c]
[d] 
Fig. 2 (a) Original Image (b) Original Image Histogram (c) Master Image (d) Master Image His-
togram 
If upper V e r upper M Subscript iV er Mi = upper V e r upper M Subscript i Superscript 1V er M1
i , then the Participants are veriﬁed, and Combiner starts con-
structing the secret shares. 
Reconstruction. After all the Participants are veriﬁed, Combiner generatesupper I prime Subscript e n cI '
enc using 
upper S upper H prime Subscript iSH '
i . The entropy of upper I prime Subscript e n cI '
enc is calculated and the secret image is retrieved. 
4.6 
Final Veriﬁcation Phase 
– This is the last phase, each participant checks the integrity of the secret by com-
puting upper I m gImg. 
upper I m g equals h left parenthesis upper O u t Subscript upper I Baseline m g StartAbsoluteValue EndAbsoluteValue upper H left parenthesis upper S upper W normal bar upper I upper D Subscript i Baseline right parenthesis right parenthesisImg = h(OutImg||H(SW_I Di))
– If the computed value upper I m g equals upper F upper V Subscript iImg = FVi, then the constructed secret is accepted; else 
the reconstructed secret is discarded (Fig. 2). 
4.7 
Registration 
CID is ‘2212065,’ PWD is ‘nitrourkela,’ & R is ‘2931’ at the registration step, which 
is taken by the Combiner. Using these values, PKY and VR are computed, where PKY 
value 
is 
‘a62f603177a2ef8d1f93fae86c2b95cdfa284ba29a08a26d941a3252f6aa 
594f’ & VR value is ‘218f736a89d517cc54da55c43ba998f6c2cb20b6cd69eec8610 
11d01ee9031e0’. Following these calculations, the data are transmitted to the Dealer 
through a secret channel.

Veriﬁable Secret Image Sharing with Cheater Identiﬁcation
501
[a]
[b]
[c]
[d]
[e] 
Fig. 3 (a)upper S upper H 1SH1 (b)upper S upper H 2SH2 (c)upper S upper H 3SH3 (d)upper S upper H 4SH4 (e)upper S upper H 5SH5
4.8 
Share Generation 
In this example, the dealer generates participant IDs with the following values:upper I upper D Subscript iI Di = 
1,upper I upper D 2I D2 = 9,upper I upper D 3I D3 = 6,upper I upper D 4I D4 = 5, andupper I upper D 5I D5 = 2 then generates shares for each participant 
(Fig. 3). 
4.9 
Share Distribution 
In Fig. 4, it is demonstrated how a dealer generates Shadow shares and how their 
histogram differs from the histogram of the original image. Dealer also generates 
Shadow IDs for each participant and the hash value is mentioned below: 
upper S upper W normal bar upper I upper D 1SW_I D1 : 9950b6dda8dfb25013ddacf431623339c57b6e6c9ecbd65f460a159372b77874 
upper S upper W normal bar upper I upper D 2SW_I D2 : 506b005c1dcfd2183383ad3d00c98343b193486ccae8d5a877f55ddbb1f88571 
upper S upper W normal bar upper I upper D 3SW_I D3 : 92a5be02c4bcc973f397ba6b682f9691b64d093d509049d952613c097a1191ca 
upper S upper W normal bar upper I upper D 4SW_I D4 : 964299b1478fbb8d39b330f184b45a982f513aa4f24e2c9a6b023888a1ddd403 
upper S upper W normal bar upper I upper D 5SW_I D5 : 4b9a0476cd3f92d49c76597b4e1aae7442d51724d7ec490dd02187a06dd96faf 
4.10 
Veriﬁcation and Reconstruction 
In this phase, the combiner selects any k participant and initiates the veriﬁca-
tion phase by sending VR. For example, if the combiner selects 3 participants, 
Participant1, Participant2, and Participant3, then each participant will compute 
upper V e r upper M Subscript i Baseline equals h left parenthesis upper S upper W normal bar upper I upper D Subscript i Baseline right parenthesis circled plus upper V upper RV er Mi = h(SW_I Di) ⊕V R and check if it matches withupper V e r upper M Subscript i Superscript 2V er M2
i or not. Below is 
an example of Participant1 verifying the combiner. 
VerM = h(9950b6dda8dfb25013ddacf431623339c57b6e6c9ecbd65f460a159372b77874) circled plus⊕
218f736a89d517cc54da55c43ba998f6c2cb20b6cd69eec861011d01ee9031e0 
If the value generated by each selected participant are matched, then it proceeds to 
the next step; else, the process is stopped.

502
F. D. Ekka et al.
[a]
&
& [b]
&
& 
[c]
&
& [d]
&
& 
[e]
& 
Fig. 4 Shadow shares generated by the Dealer (a)upper S upper W 1SW1 (b)upper S upper W 2SW2 (c)upper S upper W 3SW3 (d)upper S upper W 4SW4 (e)upper S upper W 5SW5
After the combiner veriﬁcation, Combiner computesupper I upper D prime Subscript iI D'
i to generate Participants 
Matrix (upper P upper M prime Subscript iPM'
i) using  upper I upper D prime Subscript iI D'
i as key and upper S upper H prime Subscript iSH '
i and upper V e r upper M prime Subscript iV er M'
i for Participant veriﬁcation, 
and after the veriﬁcation process, all of the upper S upper H Subscript iSHi are used to produce the encrypted 
picture upper I Subscript e n cIenc via the Bitwise OR technique. The image’s entropy is determined, and 
the ‘lena’ image is obtained. 
4.11 
Reconstructed Image Veriﬁcation 
Below is the example of Participant1 verifying the reconstructed image. 
upper I m g equals h left parenthesis upper O u t Subscript upper I Baseline m g StartAbsoluteValue EndAbsoluteValue upper H left parenthesis upper S upper W normal bar upper I upper D Subscript i Baseline right parenthesis right parenthesisImg = h(OutImg||H(SW_I Di))
upper I m g 1Img1 = 58cb69d4875fbd2016702ea9ba2be0fb965c52204eb67d67ba92d7ca53f8927dupper F upper V 1FV1
= 58cb69d4875fbd2016702ea9ba2be0fb965c52204eb67d67ba92d7ca53f8927d 
If the computed valueupper I m g Subscript i Baseline equals upper F upper V Subscript iImgi = FVi, then the constructed secret is accepted; else, 
the reconstructed secret is discarded. 
4.12 
Structural Similarity Index Matrix (SSIM) 
It was possible to calculate the similarity between the original and reconstructed 
images using Eq. 3, and when the SSIM value between the original and reconstructed 
images was calculated, it was determined to be 1.

Veriﬁable Secret Image Sharing with Cheater Identiﬁcation
503
upper S upper S upper I upper M left parenthesis x comma y right parenthesis equals StartFraction left parenthesis 2 mu Subscript x Baseline mu Subscript y Baseline plus upper C 1 right parenthesis plus left parenthesis 2 delta Subscript x y Baseline plus upper C 2 right parenthesis Over mu Subscript x Superscript 2 Baseline plus mu Subscript y Superscript 2 Baseline plus upper C 1 left parenthesis delta Subscript x Superscript 2 Baseline plus delta Subscript y Superscript 2 Baseline plus upper C 2 right parenthesis EndFractionSSI M(x, y) = (2μxμy + C1) + (2δxy + C2)
μ2x + μ2y + C1(δ2x + δ2y + C2)
(3) 
4.13 
Histogram Analysis 
In Fig. 4, it is demonstrated how a Dealer generates Shadow shares and how their 
histogram differs from the histogram of the original image, making it immune to 
histogram attacks. 
5 
Conclusion and Future Work 
We discussed various veriﬁable secret image-sharing schemes (VSIS) and also 
addressed the problem regarding the previously proposed schemes. To overcome 
the problem, a new approach was proposed, which added a new layer of veriﬁcation 
phase to check the integrity of the reconstructed secret shares. Dealer authentication 
should be taken for future work. 
References 
1. Chanu OB, Neelima A (2019) A survey paper on secret image sharing schemes. Int J Multimedia 
Inf Retrieval 8:195–215 
2. Jain J, Jain A (2022) Securing e-healthcare images using an efﬁcient image encryption model. 
Sci Program 2022:6438331 
3. Kandar S, Dhara BC (2020) A veriﬁable secret sharing scheme with combiner veriﬁcation and 
cheater identiﬁcation. J Inf Secur Appl 51:102430 
4. Rajabi B, Eslami Z (2019) A veriﬁable threshold secret sharing scheme based on lattices. Inf 
Sci 501:655–661 
5. Shah AA, et al (2020) Efﬁcient image encryption scheme based on generalized logistic map 
for real time image processing. J Real-Time Image Process 17:2139–2151 
6. Shamir A (1979) How to share a secret. Commun ACM 22:612–613 
7. Shao J (2014) Efﬁcient veriﬁable multi-secret sharing scheme based on hash function. Inf Sci 
278:104–109 
8. Soreng A, Kandar S (2022) A veriﬁable threshold secret image sharing (sis) scheme with 
combiner veriﬁcation and cheater identiﬁcation. J Ambient Intell Human Comput 
9. Tang Z, et al (2019) Robust image hashing with tensor decomposition. IEEE Trans Knowl Data 
Eng 31:549–560 
10. Tompa M, Woll H (1989) How to share a secret with cheaters. J Cryptol 1:133–138

An ECC-Based Lightweight CPABE 
Scheme with Attribute Revocation 
Avinash Chandel, Sourabh Debnath, Jitendra Kumar, 
and Ramesh Kumar Mohapatra 
Abstract Data storage on cloud servers has become a common practice across busi-
nesses. Data security and accessibility in cloud environments are jeopardized when 
stored on unreliable cloud servers, making it necessary to convert user information 
into encrypted text that is difﬁcult to understand and analyze, even if it is compro-
mised. The novel encryption method known as Ciphertext Policy Attribute Based 
Encryption (CPABE) can be used to provide ﬁne-grained access control and privacy 
in a cloud environment. To avoid the costly bilinear pairings, several researchers 
suggested CPABE approaches based on the elliptic curve cryptosystem (ECC). But 
there is a key-escrow concern with these systems. Hence, in this paper, we propose a 
multi-authority lightweight CPABE methodology. In addition, time and geographical 
attributes are also taken into consideration to enhance ﬁne-grained access control. 
Our proposed solution also solves the issue related to dynamic attribute revocation 
by relegating repeated and tedious tasks to proxy servers. The detailed performance 
analysis demonstrates that the suggested method outperforms current schemes. 
Keywords CPABE · Multi-authorities · ECC · Lightweight · Dynamic attribute 
revocation · Geographical attributes · Time attributes 
A. Chandel (B) · S. Debnath · J. Kumar · R. K. Mohapatra 
Department of Computer Science and Engineering, National Institute of Technology, Rourkela, 
Odisha, India 
e-mail: avinashchandel1561@gmail.com 
S. Debnath 
e-mail: 519cs1014@nitrkl.ac.in 
J. Kumar 
e-mail: 520cs1001@nitrkl.ac.in 
R. K. Mohapatra 
e-mail: mohapatrark@nitrkl.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_43 
505

506
A. Chandel et al.
1 
Introduction 
Data storage on cloud servers has become a common practice across businesses. 
Several cloud providers serve customers with a variety of services. Cloud computing 
emphasises the efﬁcient use of distributed resources, ﬂexibility and efﬁcient use of 
time and money. Although this technique has many advantages, it also has some 
problems. As mentioned by Jadeja and Modi [ 1], the main concerns are security and 
data privacy. Because information is accessed over the internet, data is vulnerable to 
unauthorised access and trust issues. An untrusted cloud provider can leak sensitive 
information to adversaries. This makes it necessary to convert user information into 
encrypted text that is difﬁcult to understand and analyse even if cloud servers are 
compromised. 
Before we upload user data to our cloud servers, we encrypt it using standard 
encryption methods. The standard encryption methods that address privacy-related 
issues are inadequate for establishing access controls. Separate trusted servers are set 
up in contemporary scenarios to implement access control. This results in signiﬁcant 
maintenance expenses for numerous servers. 
To overcome the problems of scalability, access control and ﬂexibility, researchers 
have devised a new cryptographic technique, attribute-based encryption (ABE). In 
CPABE, the ciphertext is generated using the access tree, and secret keys are gener-
ated with the help of the attribute set. 
In addition to maintaining user and role-based access controls, it is also necessary 
limits data access based on location and temporal factors. For instance, a software 
engineer may access some critical code during business hours and from a speciﬁc 
location like the workplace. 
The major contributions of this paper are as follows. 
– To cut down on computational overheads, a lightweight ECC-based CPABE tech-
nique has been proposed. 
– Incorporating dynamic attribute revocation allows for the dynamic modiﬁcation 
of attributes. 
– Data access controls are expressed using linear secret sharing structures (LSSS). 
Many authors [ 6, 7, 10] proposed light-weight ECC-based CPABE schemes, but all 
the schemes lack the revocation mechanism. To the best of our knowledge, despite 
focusing on a practical context, this study is one of the ﬁrst to ofﬁcially address the 
problem of user/attribute revocation in ECC-based CPABE. 
2 
Related Work 
Bethencourt et al. [ 2] ﬁrst introduced CPABE in 2007. In this scheme, the ciphertext 
is generated using an access tree, and the private key is generated using a set of 
attributes.

An ECC-Based Lightweight CPABE Scheme with Attribute Revocation
507
This schema is appropriate for situations where data owners need to establish 
access rules so that only particular users are permitted to view the data. Separate 
trustworthy servers were employed before the inception of this method. Users of the 
data can only access the data if the set of user attributes matches the encryption’s 
access structure. This technique uses a monotonic access tree, and the encrypted text 
also depends linearly on the number of attributes. 
In subsequent studies, researchers focused on efﬁcient CPABE. They focused 
on shortening the ciphertext size and speeding up the encryption and decryption 
process. Waters [ 3] introduced a new technique from CPABE that uses a linear 
secret sharing scheme (LSSS) matrix over attributes to express access control. These 
schemes incorporate a single authority mechanism. Here, authorities can decrypt and 
access all messages. 
Later studies concentrated on replacing the expensive bilinear pairings with ellip-
tic curves and ABE. Yao et al. [ 4] coined an ECC-based KPABE technique for IOT 
without bilinear pairing based on ECC. But this scheme is not scalable. In 2016, Odelu 
et al. [ 5] unveiled a novel CPABE system based on ECC. Despite being superior to 
bilinear pairing-based systems, the AND gate structure used by this scheme cannot be 
expressed. In 2018, Ding et al. [ 6] also presented an approach that substituted scalar 
multiplication for bilinear pairing on elliptic curves. It reduces computational over-
head. The key-escrow issue crippled this approach. To address the collusion attack 
security issue, Raj et al. [ 7] suggested a CP-ABE solution based on the ECC with a 
constant-size secret key. While the number of users grows, the additional validation 
process can be the bottleneck. For IOT devices, Sana et al. [ 8] presented a multi-
authority CPABE scheme where costly operations are outsourced to reliable servers. 
However, this plan is deﬁcient in attribute revocation, which jeopardises data security. 
In 2019, A bilinear pairing free ecc-based KPABE technique for IOT was proposed 
by Sowjanya et al. [ 9], but this scheme cannot be utilised to create access control 
based on the preferences of the data owner. Additionally, this method lacks access 
control capabilities like multi-authority and non-monotonic access structures. For 
fog-based cyber systems, Aisha et al. [ 10] lightweight ABE method was introduced, 
which may be employed effectively on devices with limited resources. An expressive 
access structure is not supported by this method, though. A multiauthority access 
control approach for sharing healthcare data securely has been presented by Zhang 
et al. [ 12]. Data are encrypted in this approach before being sent to cloud servers. In 
2022, Sangjukta and Namasudra [ 13] introduced the multiauthority CPABE scheme 
for healthcare infrastructure. This scheme use multiauthority to reduce workload on 
central authority and also solve the key-escrow problem. But it lacks in dynamic 
attribute revocation which is major threat for security. 
However, the access restrictions of all the above approaches are based on static 
properties and ignore dynamic elements like time and place. A unique multi-authority 
CP-ABE approach was presented by Cheng et al. [ 11] on the basis of the ECC. For 
ﬁne-grained access control, this method additionally took into account dynamic vari-
ables like location and time. However, this technique has signiﬁcant drawbacks, such 
as the linear dependence of the ciphertext calculation on the number of characteristics.

508
A. Chandel et al.
The dynamic attribute revocation, a signiﬁcant privacy and authorisation concern, is 
also missing from this system. 
3 
Proposed Scheme Model 
The proposed system includes server roles, including data owners, users, proxy 
servers, and some other things. Figure 1 illustrates how the proposed technique actu-
ally works. Several roles and their functionalities are mentioned below. 
– Global Authority (GA): All roles fully trust GA, which is principally in full 
control of initialising the global parameters. 
– Attribute Authorities (AA): These are also completely dependable authorities 
in charge of upholding the collection of attributes. No two AA keep the same set 
of attributes due to the mechanism in which the attributes are kept. 
– Edge Nodes: The edge nodes serve as a gateway between cloud servers and 
consumers and are capable of high storage and computationally demanding oper-
ations. These nodes process real-time data, such as data uploads and downloads 
from servers, which minimises network latency. 
– Proxy Server: This is in charge of updating the attributes and parameters used to 
encrypt the data and dynamically revoke attributes. 
Fig. 1 Proposed Scheme

An ECC-Based Lightweight CPABE Scheme with Attribute Revocation
509
– Cloud Servers: As opposed to internal storage servers, cloud servers are storage 
servers offered by service providers at discounted premiums. 
– Data Owner: In the suggested scheme, these are the key players. Data encryption, 
building access control frameworks, and uploading information to a cloud server 
for other users to easily access are mostly the responsibilities of the data owner. 
– Data User: The beneﬁciaries of this strategy are the data users. They are in charge 
of collecting the encrypted message and decrypting the encrypted text that has been 
received in order to retrieve the desired data. On the basis of the secret keys they 
acquired from AA, they also construct the secret keys for edge nodes. 
The proposed methodology comprises a variety of algorithms that are accom-
plished by various scheme stakeholders. Instead of using pricey bilinear pairings, this 
technique made use of scalar operations. This encryption method includes real-time 
attribute revocation, preventing illegal access to data by exploiting expired attributes. 
Proxy servers are given the arduous chore of refreshing keys and re-encrypting pre-
existing ciphertext in an attempt to reduce the strain of dynamic revocation. Pre-
Decryption, carried out by edge nodes, and actual decryption, carried out at the Data 
User Site, are the two phases in which the ciphertext is decrypted. The following 
provides more details on the various algorithms employed in the scheme. 
– Global Parameter Generation: The suggested scheme’s initial phase has been 
put into action by a global authority. The elliptic curveupper E Subscript cEc over the ﬁnite ﬁeld and 
the generator upper E Subscript gEg are selected throughout the execution of this algorithm. 
A hash functionupper H Subscript fH f :StartSet 0 comma 1 EndSet Superscript asterisk{0, 1}∗=upper Z Subscript r Baseline Superscript asteriskZr ∗is used in conjunction with the input parameters 
mentioned above to translate the user’s GID to the elements inupper Z Subscript rZr. Global authority 
deﬁnes a collection of attributes B = {b1, b2,..., bn}, which are non-overlappingly 
handled by AA. 
With ver initialised to 1, this algorithm generates global parameters GP = {GF(q), 
upper E Subscript gEg, upper E Subscript cEc, B, upper H Subscript fH f , ver}, which the GA then transmits to the AA. 
– AA Setup: The methodology employs the GP that GA delivers as input to generate 
parameters for each authority based on a set of attributes that AA manages. Each 
AA keeps track of the list of characteristics pertaining to each user in addition 
to non-overlapping attributes. To generate the Master Key (MK) and Public Key 
(PK) for each attribute i AA chooses two integral values, {u Subscript ju j and v Subscript jv j}, which are 
then used to generate the Master Key (MK) = {u Subscript ju j,v Subscript jv j, ver,for all∀j} and the Public Key 
(PK) = {u Subscript ju j.upper E Subscript gEg, v Subscript jv j.upper E Subscript gEg, ver, for all∀j}. 
– Establishment of Secret Keys: This algorithm is executed by attribute authorities 
and data users and emphasizes on establishing the secret keys for data users and 
edge nodes. The secret keys are generated based on the category of attributes, 
namely time attributes, geographical attributes and normal attributes. 
i. Geographical attributes Keys 
The data user will ask for location keys for that region if the encrypted text must 
be accessed from a certain geographic place. Geographical key is created by 
AA using the formula upper G upper K Subscript i comma upper U upper I upper D primeGKi,U I D' = u Subscript iui + upper H Subscript fH f (UID)(v Subscript ivi + l Subscript ili) and is then entered as 
a UID-related entry. Additionally, AA creates temporary keys for edge nodes

510
A. Chandel et al.
in the form upper G upper K Subscript upper E upper N comma upper G upper I upper D primeGKE N,GI D' = {  upper G upper K Subscript i comma upper U upper I upper D Sub Superscript prime Subscript Baseline comma i element of upper S upper G Subscript j comma upper U upper I upper D BaselineGKi,U I D', i ∈SG j,U I D } and sends them to the 
appropriate data user. Using the formula upper G upper K Subscript i comma upper U upper I upper DGKi,U I D = u Subscript iui + upper H Subscript fH f (UID)(v Subscript ivi + l Subscript ili) + zetaζ
where zeta element of upper Z Subscript rζ ∈Zr, the data user calculates secret keys. 
ii. Normal attributes keys 
The key is created by AA using the formulaupper U upper K Subscript i comma upper U upper I upper D primeU Ki,U I D' =u Subscript iui +upper H Subscript fH f (UID)v Subscript ivi and is 
then entered as a UID-related entry. Additionally, AA creates temporary keys 
for edge nodes in the form upper U upper K Subscript upper E upper N comma upper G upper I upper D primeU KE N,GI D' = {  upper U upper K Subscript i comma upper U upper I upper D Sub Superscript prime Subscript Baseline comma i element of upper S Subscript j comma upper U upper I upper D BaselineU Ki,U I D', i ∈Sj,U I D } and sends 
them to the appropriate data user. Using the formulaupper U upper K Subscript i comma upper U upper I upper DU Ki,U I D =u Subscript iui +upper H Subscript fH f (UID)v Subscript ivi
+ zetaζ where zeta element of upper Z Subscript rζ ∈Zr, the data user calculates secret keys. 
iii. Time attributes keys 
The data user will ask for time keys for that interval if the encrypted text must 
be accessed within a certain time interval. Time key is created by AA using the 
formulaupper T upper K Subscript i comma upper U upper I upper D primeT Ki,U I D' =u Subscript iui +upper H Subscript fH f (UID)(v Subscript ivi +t Subscript iti) and is then entered as a UID-related 
entry. Additionally, AA creates temporary keys for edge nodes in the form 
upper T upper K Subscript upper E upper N comma upper G upper I upper D primeT KE N,GI D' = {upper T upper K Subscript i comma upper U upper I upper D Sub Superscript prime Subscript Baseline comma i element of upper S upper T Subscript j comma upper U upper I upper D BaselineT Ki,U I D', i ∈STj,U I D } and sends them to the appropriate data 
user. Using the formula upper T upper K Subscript i comma upper U upper I upper DT Ki,U I D = u Subscript iui + upper H Subscript fH f (UID)(v Subscript ivi + t Subscript iti) +  zetaζ where zeta element of upper Z Subscript rζ ∈Zr, 
the data user calculates secret keys. 
– Encrypting Data: 
i. Data owner ﬁrst encrypt the data using any of the existing symmetric key cipher 
technique. Data owner selects random key K to encrypt the plaintext PT and 
obtain encrypted text ET = upper E Subscript upper KEK(PT). Simultaneously, upper H Subscript f Baseline Subscript upper C upper TH f CT = upper H Subscript fH f (PT).upper E Subscript gEg is 
calculated to ensure the integrity of information. 
ii. Each encrypted text ET is associated with a different id, or upper E upper T Subscript i dETid. The time 
attributesupper S upper T Subscript i comma upper E upper T Sub Subscript i d SubscriptSTi,ETid ofupper E upper T Subscript i dETid are added if the encrypted message must be accessed 
within a speciﬁc time frame [upper T Subscript s t a r t Baseline comma upper T Subscript e n d BaselineTstart, Tend]. The time attribute’s symmetric key is 
calculated ast Subscript iti.upper E Subscript gEg, wheret Subscript iti is the data owner’s randomly chosen value. Similar 
to this, the time attribute upper S upper L Subscript i comma upper E upper T Sub Subscript i d SubscriptSLi,ETid of upper E upper T Subscript i dETid is expanded to include [upper L Subscript s t a r t Baseline comma upper L Subscript e n d BaselineLstart, Lend] 
if the encrypted message must be accessed within a speciﬁc geographic range. 
The symmetric key for the time attribute is calculated asl Subscript ili.upper E Subscript gEg, wherel Subscript ili i n upper Z Subscript rinZr is 
the data owner’s choice of a random value. 
iii. To provide ﬁne-grained access control, the data owner develops an access struc-
ture (upper A Subscript sAs,r h orho) based on LSSS. A matrix of size n Subscript a Baseline times lna × l is known as upper A Subscript sAs, where 
n Subscript ana denotes the total number of attributes andr h o left parenthesis x right parenthesisrho(x) denotes the attribute corre-
sponding to row x in the matrix. Following matrix construction, the access matrix 
is delivered to edge nodes alongside other parameters for further encryption. 
The edge nodes receive the partially encrypted data and create encrypted text that 
can be saved on a cloud server. At edge nodes, the data is encrypted using the 
methods listed below. 
black medium square∎A random value z element of upper Z Subscript r∈Zr is selected and calculates upper E 0E0 = K + z.upper E Subscript gEg. 
black medium square∎Two vectors ModifyingAbove w With right arrow−→
w = (z,w 2w2, ... ,w Subscript mwm) element of∈upper Z Subscript rZr and ModifyingAbove q With right arrow−→
q = (0,q 2q2, ... ,q Subscript mqm) element of∈upper Z Subscript rZr are selected 
and following parameters are calculated.

An ECC-Based Lightweight CPABE Scheme with Attribute Revocation
511
lamda Subscript x Baseline equals upper A Subscript x Baseline period ModifyingAbove w With right arrowλx = Ax.−→
w
omega Subscript x Baseline equals upper A Subscript x Baseline period ModifyingAbove q With right arrowωx = Ax.−→
q
upper E Subscript 1 comma x Baseline equals lamda Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline u Subscript rho left parenthesis x right parenthesis Baseline upper E Subscript gE1,x = λx Eg + γxuρ(x)Eg
upper E Subscript 2 comma x Baseline equals gamma Subscript x Baseline upper E Subscript g Baseline upper E Subscript 3 comma x Baseline equals StartLayout Enlarged left brace 1st Row 1st Column omega Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline v Subscript rho left parenthesis x right parenthesis Baseline upper E Subscript g Baseline comma 2nd Column if rho left parenthesis x right parenthesis element of normal attributes 2nd Row 1st Column omega Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline left parenthesis v Subscript rho left parenthesis x right parenthesis Baseline plus t Subscript rho left parenthesis x right parenthesis Baseline right parenthesis upper E Subscript g Baseline comma 2nd Column if rho left parenthesis x right parenthesis element of time attributes 3rd Row 1st Column omega Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline left parenthesis v Subscript rho left parenthesis x right parenthesis Baseline plus l Subscript rho left parenthesis x right parenthesis Baseline right parenthesis upper E Subscript g Baseline comma 2nd Column if rho left parenthesis x right parenthesis element of geographical attributes EndLayoutE2,x = γx Eg E3,x =
⎧
⎪⎨
⎪⎩
ωx Eg + γxvρ(x)Eg,
if ρ(x) ∈normal attributes
ωx Eg + γx(vρ(x) + tρ(x))Eg, if ρ(x) ∈time attributes
ωx Eg + γx(vρ(x) + lρ(x))Eg, if ρ(x) ∈geographical attributes
gamma Subscript xγx element of∈upper Z Subscript rZr , xelement of∈[1,l] andupper A Subscript xAx is row x of A. 
black medium square∎The ﬁnal encrypted text is produced by edge nodes asupper E upper T Subscript f i n a lET f inal = { ET,upper H Subscript f upper C upper TH f CT , (upper A Subscript sAs,rhoρ) , {upper E Subscript 1 comma xE1,x,upper E Subscript 2 comma xE2,x, 
upper E Subscript 3 comma xE3,x } ,upper E 0E0 , ver }, and is then uploaded to a cloud server. 
– Decrypting the Ciphertext: In proposed scheme, the decryption of the encrypted 
text happens in two phases. 
• Decryption at edge nodes 
The edge nodes receive the ciphertext from the cloud server and performs partial 
decryption of the encrypted text. 
Edge nodes recieve the attribute set S from the data user and calculates the 
vector psi equals StartSet x parallel to rho left parenthesis x right parenthesis element of upper S EndSetψ = {x||ρ(x) ∈S} and verify whether the create set match with access 
structure. In case of match found, a set left brace c Subscript x Baseline element of upper Z Subscript r Baseline right brace Subscript x element of upper X{cx ∈Zr}x∈X can be obtained which 
means sigma summation Underscript x element of upper X EndscriptsMathID119∑
x∈XcxλX = s and sigma summation Underscript x element of upper X EndscriptsMathID120∑
x∈XcxωX = 0. The partially decrypted text upper D upper T Subscript xDTx = 
upper E Subscript 1 comma xE1,x +upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesisH f (U I D).upper E Subscript 3 comma xE3,x -upper K Subscript rho left parenthesis x right parenthesis comma upper U upper I upper DKρ(x),U I D.upper E Subscript 2 comma xE2,x where the value ofupper K Subscript rho left parenthesis x right parenthesis comma upper U upper I upper DKρ(x),U I D is obtained 
by on the nature of attribute. 
i. Normal Attributes 
StartLayout 1st Row 1st Column Blank 2nd Column upper D upper T Subscript x Baseline equals upper E Subscript 1 comma x Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period upper E Subscript 3 comma x Baseline minus upper K Subscript rho left parenthesis x right parenthesis comma upper U upper I upper D Baseline period upper E Subscript 2 comma x Baseline 2nd Row 1st Column Blank 2nd Column equals lamda Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline u Subscript rho left parenthesis x right parenthesis Baseline upper E Subscript g Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period left parenthesis omega Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline v Subscript rho left parenthesis x right parenthesis Baseline upper E Subscript g Baseline right parenthesis minus left parenthesis u Subscript i Baseline plus upper H Subscript f Baseline left parenthesis normal upper U normal upper I normal upper D right parenthesis v Subscript i Baseline plus zeta right parenthesis period gamma Subscript x Baseline upper E Subscript g Baseline 3rd Row 1st Column Blank 2nd Column equals lamda Subscript x Baseline upper E Subscript g Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period omega Subscript x Baseline upper E Subscript g Baseline minus zeta period gamma Subscript x Baseline upper E Subscript g Baseline EndLayoutDTx = E1,x + H f (U I D).E3,x −Kρ(x),U I D.E2,x
= λx Eg + γxuρ(x)Eg + H f (U I D).(ωx Eg + γxvρ(x)Eg) −(ui + H f (UID)vi + ζ).γx Eg
= λx Eg + H f (U I D).ωx Eg −ζ.γx Eg
ii. Geographical Attributes 
StartLayout 1st Row 1st Column Blank 2nd Column upper D upper T Subscript x Baseline equals upper E Subscript 1 comma x Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period upper E Subscript 3 comma x Baseline minus upper K Subscript rho left parenthesis x right parenthesis comma upper U upper I upper D Baseline period upper E Subscript 2 comma x Baseline 2nd Row 1st Column Blank 2nd Column equals lamda Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline u Subscript rho left parenthesis x right parenthesis Baseline upper E Subscript g Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period left parenthesis omega Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline left parenthesis v Subscript rho left parenthesis x right parenthesis Baseline plus l Subscript rho left parenthesis x right parenthesis Baseline right parenthesis upper E Subscript g Baseline right parenthesis minus left parenthesis u Subscript i Baseline plus upper H Subscript f Baseline left parenthesis normal upper U normal upper I normal upper D right parenthesis left parenthesis v Subscript i Baseline plus l Subscript i Baseline right parenthesis plus zeta right parenthesis period gamma Subscript x Baseline upper E Subscript g Baseline 3rd Row 1st Column Blank 2nd Column equals lamda Subscript x Baseline upper E Subscript g Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period omega Subscript x Baseline upper E Subscript g Baseline minus zeta period gamma Subscript x Baseline upper E Subscript g Baseline EndLayoutDTx = E1,x + H f (U I D).E3,x −Kρ(x),U I D.E2,x
= λx Eg + γx uρ(x)Eg + H f (U I D).(ωx Eg + γx (vρ(x) + lρ(x))Eg) −(ui + H f (UID)(vi + li ) + ζ).γx Eg
= λx Eg + H f (U I D).ωx Eg −ζ.γx Eg
iii. Time Attributes 
StartLayout 1st Row 1st Column Blank 2nd Column upper D upper T Subscript x Baseline equals upper E Subscript 1 comma x Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period upper E Subscript 3 comma x Baseline minus upper K Subscript rho left parenthesis x right parenthesis comma upper U upper I upper D Baseline period upper E Subscript 2 comma x Baseline 2nd Row 1st Column Blank 2nd Column equals lamda Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline u Subscript rho left parenthesis x right parenthesis Baseline upper E Subscript g Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period left parenthesis omega Subscript x Baseline upper E Subscript g Baseline plus gamma Subscript x Baseline left parenthesis v Subscript rho left parenthesis x right parenthesis Baseline plus t Subscript rho left parenthesis x right parenthesis Baseline right parenthesis upper E Subscript g Baseline right parenthesis minus left parenthesis u Subscript i Baseline plus upper H Subscript f Baseline left parenthesis normal upper U normal upper I normal upper D right parenthesis left parenthesis v Subscript i Baseline plus t Subscript i Baseline right parenthesis plus zeta right parenthesis period gamma Subscript x Baseline upper E Subscript g Baseline 3rd Row 1st Column Blank 2nd Column equals lamda Subscript x Baseline upper E Subscript g Baseline plus upper H Subscript f Baseline left parenthesis upper U upper I upper D right parenthesis period omega Subscript x Baseline upper E Subscript g Baseline minus zeta period gamma Subscript x Baseline upper E Subscript g Baseline EndLayoutDTx = E1,x + H f (U I D).E3,x −Kρ(x),U I D.E2,x
= λx Eg + γx uρ(x)Eg + H f (U I D).(ωx Eg + γx (vρ(x) + tρ(x))Eg) −(ui + H f (UID)(vi + ti ) + ζ).γx Eg
= λx Eg + H f (U I D).ωx Eg −ζ.γx Eg

512
A. Chandel et al.
After the calculating the value ofupper D upper T Subscript xDTx, the edge calculatestau 1τ1 andtau 2τ2 as follows. 
StartLayout 1st Row 1st Column Blank 2nd Column tau 1 equals sigma summation Underscript x element of upper X Endscripts c Subscript x Baseline upper D upper T Subscript x Baseline equals s upper G minus zeta sigma summation Underscript x element of upper X Endscripts c Subscript x Baseline gamma Subscript x Baseline upper E Subscript g Baseline 2nd Row 1st Column Blank 2nd Column tau 2 equals sigma summation Underscript x element of upper X Endscripts c Subscript x Baseline upper E Subscript 2 comma x Baseline equals sigma summation Underscript x element of upper X Endscripts c Subscript x Baseline gamma Subscript x Baseline upper E Subscript g EndLayoutτ1 =
∑
x∈X cx DTx = sG −ζ
∑
x∈X cxγx Eg
τ2 =
∑
x∈X cx E2,x =
∑
x∈X cxγx Eg
The partially decrypted text is generated as DT’ = {upper E 0E0, ET,upper H Subscript f upper C upper TH f CT ,tau 1τ1 ,tau 2τ2} and 
transmitted to data user for further decryption. 
• Decryption at data user site 
Data user calculatesk Superscript prime Baseline equals upper E 0 minus tau 1 plus zeta tau 2k
' = E0 −τ1 + ζτ2 and calculatesupper H Subscript f upper C upper T Sub Superscript primeH f CT ' = H(upper E Subscript k Sub Superscript primeEk'(PT))upper E Subscript gEg
by using k’. If upper H Subscript f upper C upper T Sub Superscript primeH f CT ' = upper H Subscript f upper C upper TH f CT , then plain text is correctly obtained. 
– Revoking attributes: Unauthorised access to the server’s data may result from 
expired attributes on the cloud server. This approach is crucial for the implemen-
tation of ﬁne-grained access control and authorised access. 
Multiple tasks that must be completed by the proxy server as part of the revocation 
procedure are mentioned as follows. 
i. Global Parameter Updation 
Once the attribute set is modiﬁed, this algorithm comes into action. Global 
parameters are updated, and ver is incremented and transmitted to AA. 
GP’ = { GF(q), upper E Subscript gEg, upper E Subscript cEc, B’,  }  
ii. Attribute Authorities Re-initialization 
To generate the Master Key (MK) and Public Key (PK) for each attribute i AA 
chooses two integral values, {u Subscript ju j and v Subscript jv j}, which are then used to generate the 
Master Key (MK’) = {u Subscript ju j, v Subscript jv j, ver’,  for all∀j} and the Public Key (PK’) = {u Subscript ju j.upper E Subscript gEg, 
v Subscript jv j.upper E Subscript gEg, ver’, for all∀j}. 
iii. Re-Encrypt Cloud Data 
During this phase, the proxy server regenerates the new encrypted text utilising 
the most recent access structures and parameters from the cloud server’s existing 
encrypted text. The existing encrypted text is replaced with upper E upper T prime Subscript f i n a lET '
f inal, which is 
created using the formulasupper E upper T prime Subscript f i n a lET '
f inal = { ET,upper H Subscript f upper C upper TH f CT , (upper A prime Subscript sA'
s,rho primeρ') , {upper E prime Subscript 1 comma xE'
1,x,upper E prime Subscript 2 comma xE'
2,x,upper E prime Subscript 3 comma xE'
3,x } ,  
upper E 0E0 , ver’ } .  
iv. Secret Key Regeneration 
In order to construct new secret keys using the formula outlined in “Estab-
lishment of Secret Keys,” the proxy server uses the updated attribute set and 
key pairs, and ver is increased by 1. Then, for the purpose of encrypting fresh 
messages or data, these new keys are distributed to data users and edge nodes.

An ECC-Based Lightweight CPABE Scheme with Attribute Revocation
513
4 
Performance Analysis 
This section compares the proposed scheme’s performance with that of existing 
schemes to evaluate the proposed scheme’s competence. The feature-based compar-
ison of the schemes is shown in Table 1. 
It can be interpreted from Table 1 that schemes proposed by Odelu [ 5] and Yao 
[ 4] used AND gate and Access tree, respectively, which are less efﬁcient as com-
pared to LSSS for implementing ﬁne-grained access control used in the proposed 
scheme and other schemes. The dynamic attribute constraints should also be taken 
into consideration for managing ﬁne-grained access. Hence, time and geographical 
attributes are incorporated into the proposed scheme and scheme proposed by Cheng 
et al. [ 11] 
The encryption and decryption portions of the CPABE schemes account for the 
majority of the computing cost. We use scalar multiplication based on ECC as a unit 
of measurement to compare the computational overhead of our approach to other 
techniques. Table 2 compares the calculation overhead of the various approaches. 
Note: Nr: Row count in access matrix B, Na: Total attributes in the system, Ma: 
Minimum number of attributes satisfying the access policy,parallel to upper B parallel to||B||: number of attributes 
in the access policy, Sm: scalar multiplication based on ECC, Tr: Number of times 
attribute revoked. 
Table 1 Feature based comparison of schemes 
Odelu et al.
Ding et al.
Yao et al.
Cheng et al.
Proposed 
Scheme 
Access 
Structure 
AND Gate
LSSS
Access Tree
LSSS
LSSS 
Multi-
Authority 
No
No
No
Yes
Yes 
Dynamic 
Attributes 
No
No
No
Yes
Yes 
Decryption 
Outsourcing 
No
No
No
Yes
Yes 
Dynamic 
Attribute 
Revocation 
No
No
No
No
Yes 
Table 2 Computational overhead of various schemes 
Encryption
Pre-Decryption
Decryption 
Odelu et al.
(Na -parallel to||Bparallel to|| + 2) Sm
–
(Na -parallel to||Bparallel to|| + 3) Sm  
Ding et al.
(3 Nr + 1) Sm
(Ma + 1) Sm 
Yao et al.
(Nr + 1) Sm
(Ma + 1) Sm 
Cheng et al.
(4Nr + 1) Sm
(Ma + 1) Sm
(1) Sm 
Proposed Scheme
Tr (4 Nr + 1) Sm
(Ma + 1) Sm
Sm

514
A. Chandel et al.
From Table 2, it can be interpreted that scheme proposed by Odelu et al. [ 5] is com-
paratively low as compared with other schemes. The proposed scheme and Cheng et 
al. [ 11] scheme took dynamic attributes into consideration that result in additional 
overhead during encryption phase. Our scheme also incorporate the attribute revo-
cation and every attribute revocation require the re-encryption of the data. Thus the 
computational overhead is directly proportional to number of times attribute revoked. 
5 
Conclusions 
In this work, we presented a revolutionary lightweight CPABE system based on ECC 
and edge computing with dynamic attribute revocation. In this plan, basic scalar 
multiplication is used in place of expensive bilinear mappings. Proxy servers are 
given the laborious work of modiﬁcation and maintenance in order to lessen the load 
of attribute revocation. If an inappropriate or expired attribute still exists, the proxy 
servers update the parameters and keys and replace any existing ciphertext based on 
that ciphertext with new ciphertext. Multiple authorities are used in this method to 
prevent the issue of key escrow, and dynamic features like geographical attributes 
are also taken into account to achieve ﬁne-grained access control. The efﬁciency of 
the suggested plan is lastly demonstrated by the performance analysis. 
This strategy can still be enhanced, though, and that work will be done in the 
future. The system is less effective since the calculation of the ciphertext is linearly 
dependent on the number of attributes. The proxy servers, on the other hand, can be 
enhanced to update the keys without revealing the user’s information. 
References 
1. Jadeja Y, Modi K (2012) Cloud computing-concepts, architecture and challenges. In: 2012 
international conference on computing, electronics and electrical technologies (ICCEET), pp 
877–880. IEEE 
2. Bethencourt J, Sahai A, Waters B (2007) Ciphertext-policy attribute-based encryption. In: 2007 
IEEE symposium on security and privacy (SP 2007), pp 321–334. IEEE 
3. Waters B (2011) Ciphertext-policy attribute-based encryption: an expressive, efﬁcient, and 
provably secure realization. In: International workshop on public key cryptography, pp 53–70. 
Springer, Heidelberg 
4. Yao X, Chen Z, Tian Y (2015) A lightweight attribute-based encryption scheme for the Internet 
of Things. Futur Gener Comput Syst 49:104–112 
5. Odelu V, Das AK (2016) Design of a new CP-ABE with constant-size secret keys for lightweight 
devices using elliptic curve cryptography. Secur Commun Netw 9(17):4048–4059 
6. Ding S, Li C, Li H (2018) A novel efﬁcient pairing-free CP-ABE based on elliptic curve 
cryptography for IoT. IEEE Access 6:27336–27345 
7. Raj N, Pais AR (2020) CP-ABE scheme satisfying constant-size keys based on ECC. In: ICETE 
(2), pp 535–540

An ECC-Based Lightweight CPABE Scheme with Attribute Revocation
515
8. Belguith S, Kaaniche N, Laurent M, Jemai A, Attia R (2018) PHOABE: securely outsourcing 
multi-authority attribute based encryption with policy hidden for cloud assisted IoT. Comput 
Netw 133:141–156 
9. Sowjanya K, Dasgupta M, Ray S, Obaidat MS (2019) An efﬁcient elliptic curve cryptography-
based without pairing KPABE for Internet of Things. IEEE Syst J 14(2):2154–2163 
10. Junejo AK, Komninos N (2020) A lightweight Attribute-based security scheme for fog-enabled 
cyber physical systems. Wirel Commun Mobile Comput 
11. Cheng R, Kehe W, Yuling S, Li W, Cui W, Tong J (2021) An efﬁcient ECC-based CP-ABE 
scheme for power IoT. Processes 9(7):1176 
12. Zhang L, Ye Y, Yi M (2020) Multiauthority access control with anonymous authentication for 
personal health record. IEEE Internet Things J 8(1):156–167 
13. Das S, Namasudra S (2022) Multiauthority CP-ABE-based access control model for IoT-
enabled healthcare infrastructure. IEEE Trans Industr Inf 19(1):821–829

Prediction of Schizophrenia in Patients 
Using Fuzzy AHP and TOPSIS Methods 
R. Anoop, Impana Anand, Mohammed Rehan, R. Yashvanth, 
Ashwini Kodipalli, Trupthi Rao, and Shoaib Kamal 
Abstract Schizophrenia is a chronic illness that most frequently affects people 
between the ages of 16 and 30. There are many elements that lead to a patient 
receiving a diagnosis of the illness, but since the origin of the illness is unknown, 
fuzzy analysis can be an important tool in identifying the factors. The purpose of 
this study report is to inform readers of the factors that have the greatest inﬂuence 
on an individual. Fuzzy logic is a powerful tool for tackling a wide range of prob-
lems in research. It can be used to model complex systems, analyze decision-making 
processes, and develop systems that can recognize patterns in data. In addition, fuzzy 
logic can be used to model complex systems such as ﬁnancial markets and natural 
phenomena. As fuzzy logic is able to deal with uncertainty and imprecise data, it is 
particularly well-suited to a wide range of research. Fuzzy TOPSIS (Technique for 
Order Preference by Similarity to Ideal Solution) is a method used to determine the 
best of the options under consideration with respect to the weights and inﬂuence of 
each of the attributes associated with every option. Fuzzy AHP (Analytic Hierarchy 
Process) is a multi-criteria decision-making method that uses fuzzy set theory to 
evaluate alternatives. This method seeks to identify the best option from a set of 
alternatives based on the relative importance of the criteria. The goal of this study is 
to emphasize a through and in-depth literature evaluation of multi criteria decision 
making challenges in addition to pinpointing the aspects that contribute most signif-
icantly. We have implemented MCDM techniques to propose an effective approach 
towards decision making of various factors at multiple levels in patients suffering 
from schizophrenia and observed that the highest ranked Person (P4) with 0.8097
R. Anoop envelope symbol · I. Anand · M. Rehan · R. Yashvanth · A. Kodipalli · T. Rao · S. Kamal 
Department of Artiﬁcial Intelligence and Data Science, Global Academy of Technology, 
Bangalore, India 
e-mail: anoopr2592@gmail.com 
A. Kodipalli 
e-mail: dr.ashwini.k@gat.ac.in 
T. Rao 
e-mail: trupthirao@gat.ac.in 
S. Kamal 
e-mail: shoaib.kamal@gat.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_44 
517

518
R. Anoop et al.
CCi can be inferred to most likely be affected by Schizophrenia, while the person at 
the bottom of the rank hierarchy (P1) with 0.2813 CCi is least likely to be affected. 
Keywords Schizophrenia · Fuzzy Analytical Hierarchy Process (Fuzzy-AHP) ·
TOPSIS (Technique for order performance by similarity to ideal solution) · Fuzzy 
logic · Multi-criteria decision making (MCDM) · Analytical Hierarchy Process 
(AHP) 
1 
Introduction 
Schizophrenia causes psychosis and is associated with signiﬁcant impairments that 
affect life, like family, personal, educational, social, environmental, and professional 
aspects. Schizophrenia patients face widespread discrimination and human rights 
violations. More than three out of four of the mentally ill people worldwide don’t 
receive specialized health care. There are multiple efﬁcient treatments for people 
diagnosed with schizophrenia, and at least one out of three people with this disease 
makes a full recovery. Patients suffering from the disease often suffer from logical 
and thinking skills such as memory, attention and problem solving. Research shows 
that there is no single cause of schizophrenia in particular. Many genetic and environ-
mental factors are thought to contribute to schizophrenia. Heavy drugs, cannabis use 
is associated with raise in risk of the disorder [1–3]. Schizophrenia affects approxi-
mately 0.5–1% of the world over their lifetime, and is viewed as the less desirable 
result of a true “complex,” versatile psychotic syndrome, which can be linked to 
countable, age-dependent (young vs. old) expression of accountability in a sizable 
portion of the non-ill general population (around 10–20%) [2, 4]. There have been 
various machine learning methodologies used in the study of schizophrenia, but there 
have been very few fuzzy techniques implemented in this ﬁeld. 
The intent of this paper is to put forth an efﬁcient MCDM methodology for 
scrutinizing the symptoms and effects of schizophrenia. The ﬁrst MCDM (Multi-
Criteria Decision Making) method in this study is Fuzzy AHP, which incorporates 
deﬁning the signiﬁcance of each selected criterion. Developing hierarchical structure 
with objectives at the top, attributes in the next level, and options at the ultimate level. 
In this Fuzzy AHP is a pair wise comparison matrix created using relative importance 
scale. This relative importance scale of different attributes will give the respective 
goals. The main difference between Fuzzy AHP and AHP is, AHP is one of the best 
ways to decide between different levels of complex reference structures whereas 
in Fuzzy AHP, decision maker’s fuzziness is observed. Owing a holistic approach 
of Fuzzy AHP methods for qualitative and quantitative integration facts, it is very 
likely that this approach has a strong hold in selecting the schizophrenic patients. In 
Fuzzy TOPSIS, it is used in the situation where the performance values in decision 
matrix is not a crisp value but it is linguistic terms which are given by the decision 
matrix, evaluation of weights are performed by using AHP method, normalized 
the score for each criterion and the distance between each is calculated to ﬁnd the

Prediction of Schizophrenia in Patients Using Fuzzy AHP and TOPSIS …
519
schizophrenic patients. Some application areas of TOPSIS have capability in solving 
group decisions under fuzzy criteria, suitability in hospitals for the selecting proper 
attributes in a fuzzy environment and assistance to the industrial practitioners for the 
performance of the evaluation criteria [3, 4]. 
2 
Literature Review 
According to Bleuler (1911/1950), schizophrenia is a complicated and varied condi-
tion with a range of symptoms, premorbid histories, prognoses, and likely etiolo-
gies [3, 4]. While still serving as the foundation for American psychiatry’s proceed 
towards classiﬁcation, the schizophrenia subtypes described by Kraepelin (1923) 
earlier this century have not been sufﬁcient as a framework for conclusions regarding 
treatment or prognosis. The classiﬁcation of symptoms as either positive (produc-
tive) or negative has been one of the most signiﬁcant advancements in this ﬁeld of 
research. In Britain, Crow (1980) hypothesized that positive signs like delusions and 
hallucinations are linked to excessive transmission of dopamine and, consequently, 
a neurochemical abnormality that is sensitive to neuroleptics [5]. The condition’s 
non-uniformity has hampered attempts to study and treat it. There are several ways 
to get criteria weights [6, 7], but the Analytic Hierarchy Process [8–10] is the one that 
is most commonly utilized. Incorporating the fuzzy idea into AHP, Buckley [11, 12] 
created Fuzzy AHP. It was a new technique for determining fuzzy weights and was 
similar to direct fuzziﬁcation of the Saaty technique. On the basis of interval-valued 
fuzzy sets, Chen and Tsao [10, 13] continued the TOPSIS approach in 2008. In 2010, 
Chen and Lee proposed an interval type-2 fuzzy set-based fuzzy TOPSIS method 
[10]. An approach for solving MCDM problems using ratings of alternatives provided 
interval-valued precognition fuzzy sets, relative to criteria and its weight was devised 
by Li in an article [12, 14] in 2010.The AHP was developed by Saaty is utility theory-
based method. Vaidya and Kumar stated AHP to be one of the most frequently used 
MCDM methods. Holguin-Veras, Sadiq, and many others have presented on how 
AHP method can be implemented in civil and environmental engineering. AHP is 
an excellent platform for dealing with complexities in decision-making. The AHP 
hierarchy of levels is solved by combining global and local preference weights to 
determine the priority (Saaty 1980) [15, 16]. Many efforts have been performed in 
the research ﬁeld of AHP. The FAHP was ﬁrst suggested by Buckley et al. in 1985, 
and this is based on Zadeh’s [17, 18] fuzzy set theory. As a result of the ongoing 
development of fuzzy set theory, many FAHP methods, such as Pythagorean FAHP, 
triangular FAHP, Intuitionistic FAHP, trapezoidal FAHP, and Neutrosophic FAHP, 
have been proposed [19, 20]. Lokare et al. [21] worked on AHP and TOPSIS methods 
which was used for taking decisions. TOPSIS was chosen as the best solution for 
MCDM problems. Integrated neutrosophic-TOPSIS was proposed by Nabeeh et al., 
he evolved personnel selection applications [22–24]. The AHP processes data using 
impartial mathematics about people/’s personal and subjective preferences (Saaty 
2001).

520
R. Anoop et al.
3 
Methodology 
We have implemented three main fuzzy techniques to assess the various factors in 
the determination of schizophrenia. Namely, 
• Analytical Hierarchy Process (AHP) 
• Fuzzy Analytical Hierarchy Process (FUZZY-AHP) 
• Technique for Order Performance by Similarity to Ideal Solution (TOPSIS) 
3.1 
AHP 
AHP is an efﬁcient outlook in calculating the weights of criteria concerning a deﬁned 
goal. AHP is a multi-criterion decision analysis methodology that compares decision 
criteria in pairs to rank the importance of various attributes. In reference with a 
predeﬁned scale of relative importance a pair-wise comparison matrix is built to 
obtain the relative importance of different attributes. A python code is setup to process 
this comparison matrix and obtain the criterion weights for each of the attributes under 
consideration. 
STEP 1: Pick out a problem and deﬁne the criteria. 
STEP 2: Arbitrate the alternatives. 
The evaluation criteria in this case are made up of nine characteristics identiﬁed 
as the main symptoms of schizophrenia. The identiﬁed characteristics are, Halluci-
nation, Apathy, Disorganized thoughts, Extreme reactions, Cognitive Impedance, 
Insomnia, Concentration, Suicidal Thoughts, Depression. 
STEP 3: Use pairwise comparison to arbitrate the degree of importance for each 
of the alternatives and criterion. 
STEP 4: Verify the stability of each pairwise comparison. 
STEP 5: Analyze the pairwise comparison to determine the respective weights. 
STEP 6: Obtain the calculated overall priorities for the corresponding attributes. 
Consistency Index (CI) is an illustration of the degree of inconsistency in the 
decision matrix. The following formula is used to determine the consistency index 
(Fig. 1): 
Consistency Index left pa renthe sis CI right parenthesis equals StartFraction upper L a m d a minus n Over n minus 1 EndFraction
Co nsistency Index left parenthesis CI right parenthesis equals StartFraction upper L a m d a minus n Over n minus 1 EndFraction
NO 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
RCI 
0.00 
0.00 
0.520 
0.890  1.110 
1.250 
1.350 
1.400 
1.450 
1.490 
Fig. 1 Random consistency index

Prediction of Schizophrenia in Patients Using Fuzzy AHP and TOPSIS …
521
where Lambda is the average value of the criterion weights obtained, and n refers 
to the number of attributes considered. Based on the CI values of each attribute, 
they are ranked according to their priorities. Random index, the mean consistency 
indices of a given number of random number pair-wise comparison matrices 
are computed with the help of AHP. Consistency ratio of the decision matrix is 
obtained to infer about the consistency of the judgment matrix. The consistency 
ratio is computed from the following formula [25]: 
Consistency Ratio left par e nthesis CR right parenthesis equals StartFraction upper C upper I Over upper R upper I EndFraction
Consistency Ratio left parenthesis CR right parenthesis equals StartFraction upper C upper I Over upper R upper I EndFraction
3.2 
Fuzzy AHP 
The technique of integrating qualitative and quantitative methods is known as fuzzy 
analytic hierarchy process. Evaluation factors can be categorized by Fuzzy AHP into 
target, criteria, and factor levels. In many of the MCDM challenges [18, 26], Fuzzy 
AHP is a commonly utilized decision-making tool. It uses pair-wise comparisons of 
numerous options in relation to various criteria and hence offering a decision help 
tool for problems involving multiple criteria [12]. With reference to a predetermined 
scale of relative importance, a pair-wise comparison matrix is built to determine the 
relative relevance of various traits. 
A similar approach as in AHP, is used wherein a python code is designed to 
calculate the criterion weights. 
STEP 1:- Recognize a problem and assemble a team of related subject-matter 
authorities 
STEP 2:- Specify the AHP’s parameters and scope 
STEP 3:- Construct a scale with the membership function deﬁnition (Fig. 2) 
STEP 4:- Utilizing the scaled results from the questionnaire, conduct pairwise 
comparisons at each level. 
STEP 5:- Utilize fuzzy numbers to build a fuzzy comparison matrix. 
The following formula is used to compute the fuzzy weights:
Fig. 2 Membership Values 
[14]
Term                
Fuzzy Num-
ber 
Very Low 
[1,1,3] 
Low 
[1,3,5] 
Average 
[3,5,7] 
High 
[5,7,9] 
Very High 
[7,9,9] 

522
R. Anoop et al.
(
wi = (ri ⊗ ((ri ⊕(r2 ⊕ ..... ⊕(
rn)−1, Where wi is the fuzzy weight, and ri is the 
fuzzy geometric mean value 
STEP 6:- After solving the eigen vector, if the consistency index is obtained to 
be less than 0.10, we proceed to the next step. 
STEP 7:- If not, use sensitivity analysis to determine the variance 
STEP 8:- Ranking the criteria. 
3.3 
TOPSIS 
One of the most renown and efﬁcient methodology for resolving MCDM issues 
is the TOPSIS method, which was put forth by Hwang and Yoon [27, 28]. This 
approach is predicted on the idea that the selected alternative should be nearest to the 
Positive Ideal Solution, the solution that reduces costs while maximizing beneﬁts, 
and at a great distance from the Negative Ideal Solution [29, 30]. An excel method 
is implemented to calculate, using the following steps:-
STEP 1: Create a panel of decision makers. 
STEP 2: Choose an appropriate evaluation criterion. 
A total of nine attributes, mentioned above were identiﬁed as the main 
symptoms of schizophrenia together make up the evaluation criteria in this case. 
STEP 3: Choose the appropriate linguistic variable (Fig. 3) 
STEP 4: Assign weightage for each of the criteria. 
STEP 5: Constructing the decision matrix 
The aggregated fuzzy weights are calculated by the formulas: 
a = mink{ak}, b eq uals StartFraction 1 Over upper K EndFraction sigma summation Underscript k equals 1 Overscript upper K Endscripts b Subscript k
b 
eq
uals
 StartFraction 1 Over upper K EndFraction sigma summation Underscript k equals 1 Overscript upper K Endscripts b Subscript k, c  = maxk{ck}, K signiﬁes the number of decision 
makers.
Equal   
              [1,1,1]  
Moderate 
              [3,4,5]  
Strong  
              [5,6,7]  
Very strong 
              [7,8,9]  
Extremely strong 
              [9,9,9] 
Intermediate values 
              [1,2,3] 
              [2,3,4]  
              [4,5,6]  
              [6,7,8] 
Fig. 3 Linguistic Variables [16]

Prediction of Schizophrenia in Patients Using Fuzzy AHP and TOPSIS …
523
STEP 6: Normalize the fuzzy decision matrix. 
The most desirable attribute of all the attributes under consideration is acknowl-
edged as the positive criteria or beneﬁcial criteria, whereas the least desirable 
attribute is acknowledged as the cost criteria, negative criteria or nonbeneﬁcial 
attributes [17]. 
The normalized fuzzy decision matrix is uper S overTilde = [s o verTilde Subscript i j], where 
s o v erTilde Subscript i j =
l
ef t  parenthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
le
ft  p ar e nthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
le
ft  p ar e nthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
le
f
t parenthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
and z S
ubscript j Superscript asterisk = m a x Subscript i{z Subscript i j  Baseline right brace left parenthesis b e n i f i t c r i t e r i a right parenthesis. 
s o v erTilde Subscript i j =
l
ef t  parenthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
le
ft  p ar e nthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
le
ft  p ar e nthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
le
f
t parenthesis StartFraction x Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction y Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction comma StartFraction z Subscript i j Baseline Over z Subscript j Superscript asterisk Baseline EndFraction right parenthesis
and x S
ubscript j Superscript minus = m i n Subscript i{z Subscrip t i j Baseline right brace left parenthesis c o s t c r i t e r i a right parenthesis.
STEP 7: Build the Weighted Normalized Fuzzy Decision Matrix. 
Weights of the decision matrix is upper V overTilde = (v o v erTilde Subscript i j Baseline right parenthesis, where v o v e r Tilde Subscript i j Baseline equals r overTilde ij × w Subscript j Baseline period
STEP 8: Compute the Fuzzy positive ideal solution (FPIS) and Fuzzy negative 
ideal solution (FNIS) values. 
The TOPSIS method picks the most feasible alternative which is nearest to the 
FPIS and farthest from the FNIS. The best performing values for each attribute 
comprise the FPIS values whilst The least performing values comprise the FNIS 
values. 
upper A Superscript asterisk Baseline equals(M
od
ifyi
ng
Abo v e  v  1  S
uperscr ipt a s
ter
is k Baseline comma With tilde v 2 Superscript asterisk Baseline overTilde comma ellipsis ellipsis comma ModifyingAbove v Subscript n Superscript asterisk Baseline right parenthesis With tilde comma w h e r e v Subscript j Superscript asterisk Baseline overTilde equalsm a x Subscript i{v Subscript i j Baseline 3 Baseline right brace. 
upper A Superscript minus Baseline equals(M
odi
fy in
gAb
ov e  v  1  S u p
er
sc rip t min u
s B
as eline comma With tilde v 2 Superscript minus Baseline overTilde comma ellipsis ellipsis comma ModifyingAbove v Subscript n Superscript minus Baseline right parenthesis With tilde comma w h e r e v Subscript j Superscript minus Baseline overTilde equalsm i n Subscript i{v Subscript i j Baseline 1 Baseline right brace. 
STEP 9: Compute distance from every individual attribute to the FPIS and FNIS. 
The distance s is computed using the following formulas: 
s Su bscr ipt v Baseline left parenthesis p overTilde comma q overTilde right parenthesis equals StartRoot one third left bracket left parenthesis l 1 minus l 2 right parenthesis squared plus left parenthesis p 1 minus p 2 right parenthesis squared plus left parenthesis q 1 minus q 2 right parenthesis squared right bracket EndRoot
/
s Subscript v Baseline left parenthesis p overTilde comma q overTilde right parenthesis equals StartRoot one third left bracket left parenthesis l 1 minus l 2 right parenthesis squared plus left parenthesis p 1 minus p 2 right parenthesis squared plus left parenthesis q 1 minus q 2 right parenthesis squared right bracket EndRoot
s
 
Subs cript v  Basel ine lef t pare nthesis p overTilde comma q overTilde right parenthesis equals StartRoot one third left bracket left parenthesis l 1 minus l 2 right parenthesis squared plus left parenthesis p 1 minus p 2 right parenthesis squared plus left parenthesis q 1 minus q 2 right parenthesis squared right bracket EndRoot
s S
ub sc
ri
pt j
 Superscript asterisk Baseline equals sigma summation Underscript j equals 1 Overscript n Endscripts d Subscript v Baseline left parenthesis v o v er T
ild
e Subscript i j Baseline comma v Subscript j Superscript asterisk Baseline overTilde right parenthesis where i equals values ranging from 1 to m (1, 2……m). 
s S
ub sc
ri
pt j
 Superscript minus Baseline equals sigma summation Underscript j equals 1 Overscript n Endscripts d Subscript v Baseline left parenthesis v o v er T
ild
e Subscript i j Baseline comma v Subscript j Superscript asterisk Baseline overTilde right parenthesis where i equals values ranging from 1 to m (1, 2……m). 
STEP 10: For each of the alternatives, determine the closeness coefﬁcient (CCi) 
[31, 32]. 
CCi determines the similarity between each attribute and is calculated using 
the following formula: 
uppe r
 C 
upper C Subscript i Baseline equals StartFraction d Subscript i Superscript minus Baseline Over d Subscript i Superscript asterisk Baseline plus d Subscript i Superscript minus Baseline EndFraction
upp
er  C up
per C Subscript i Baseline equals StartFraction d Subscript i Superscript minus Baseline Over d Subscript i Superscript asterisk Baseline plus d Subscript i Superscript minus Baseline EndFraction

524
R. Anoop et al.
4 
Results 
The obtained results of the respective Multi Criteria Decision Making techniques 
are: 
AHP 
A satisfactory Random Consistency index value is obtained in the results as 0.129, 
hence proving the consistency of the judgment matrix [33]. 
The random consistency index is 0.12910565868336632, The judgment matrix 
has satisfactory consistency. 
The largest eigenvalue is: 10.49762564072705. 
The corresponding eigenvector is: [0.369, 0.139, 0.018, 0.071, 0.06, 0.045, 0.249, 
0.028, 0.02] that sums upto 1. 
The attribute with the highest eigenvector value 0.369 (Hallucination) proves to 
be the major affecting factor, whereas the attribute with the least eigenvector value 
0.018 (Apathy) proves to be the least affecting factor. 
Fuzzy AHP 
The Normalized weights of all the attributes obtained in the ﬁnal step of the Fuzzy 
AHP method are: 
a1: 0.358593748 (Hallucination) 
a2: 0.13926375 (Disorganised Thoughts) 
a3: 0.019975649 (Apathy) 
a4: 0.07462292 (Extreme Reactions) 
a5: 0.065234483 (Cognitive Impedance) 
a6: 0.050564207 (Insomnia) 
a7: 0.239576164 (Concentration) 
a8: 0.030331976 (Suicidal Thoughts) 
a9: 0.021837103 (Depression) 
It is similar to the respective eigenvector values of the correlating attributes 
obtained in the AHP method, proving the accurate consistency. 
5 
TOPSIS 
The person (Pi) with the highest CCi value is provided the highest rank. In this case, 
of all the individuals under study, the highest ranked Person (P4) with 0.8097 CCi 
can be inferred to most likely be affected by Schizophrenia, while the person at the 
bottom of the rank hierarchy (P1) with 0.2813 CCi is least likely to be affected.

Prediction of Schizophrenia in Patients Using Fuzzy AHP and TOPSIS …
525
Cci
RANK 
P1
0.281321485
9 
P2
0.527656636
4 
P3
0.436682920
6 
P4
0.809720889
1 
P5
0.435727013
7 
P6
0.290874238
8 
P7
0.499934006
5 
P8
0.745704951
2 
P9
0.567603256
3 
6 
Conclusion 
In this paper AHP, Fuzzy AHP and TOPSIS methods were estimated and compared 
to study the decision making of the main factors determining the symptoms of the 
disease in patients suffering from schizophrenia, and identiﬁcation of patients most 
likely to be suffering from the disease respectively and Fuzzy AHP works similar to 
one another. The main difference between AHP and Fuzzy AHP is that, AHP stands 
out in being one of the best ways to decide on the complex criterion structure at 
various levels whereas on the other hand, the Fuzzy AHP also handles the fuzziness 
in the process of decision making. AHP and Fuzzy AHP helps in exploring the 
various attributes of the disease, and inferring on the factor affecting the schizophrenic 
patients to the highest degree. TOPSIS method is used to make the best possible 
choice based on the magnitude of impact each of the factors affecting the decision 
have on the individual choices. Here the TOPSIS method helps in selection of the 
individual most likely to be suffering from schizophrenia amid all the individuals 
involved in the decision making, on the basis of the TOPSIS score calculated to each 
of the choices under consideration. The implementation of AHP, Fuzzy AHP and 
TOPSIS helps in selecting the criteria and ﬁnds the best option in this regard, which 
would be of great beneﬁt for the prediction of schizophrenia patients. 
References 
1. Luo Y, Tian Q, Wang C, Zhang K, Wang C, Zhang J (2020) Biomarkers for prediction of 
schizophrenia: insights from resting-state EEG microstates. IEEE Access 8:213078–213093. 
https://doi.org/10.1109/ACCESS.2020.3037658 
2. Van Os J, Kenis G, Rutten BP (2010) The environment and schizophrenia. Nature 
468(7321):203–212 
3. Kay SR, Sevy S (1990) Pyramidical model of schizophrenia. Schizophr Bull 16(3):537–545

526
R. Anoop et al.
4. Crow TJ (1980) Molecular pathology of schizophrenia: more than one disease process? BMJ 
280:66–68 
5. Zavadskas EK, Podvezko V (2016) Integrated determination of objective criteria weights in 
MCDM. Int J Inf Technol Decis Mak 15(2):267–283 
6. Zavadskas EK, Turskis Z, Bagocius V (2015) Multi-criteria selection of a deep-water port in 
the Eastern Baltic Sea. Appl Soft Comput 15:180–192 
7. Saaty TL (1980) The analytic hierarchy process: planning, priority setting, resource allocation. 
Mcgraw-Hill 
8. Buckley JJ (1985) Fuzzy hierarchical analysis. Fuzzy Sets Syst 17(3):233–247 
9. Chen TY, Tsao CY (2008) The interval-valued fuzzy TOPSIS method and experimental 
analysis. Fuzzy Sets Syst 159(11):1410–1428 
10. Chen SM, Lee LW (2010) Fuzzy multiple attributes group decision-making based on the interval 
type-2 TOPSIS method. Expert Syst Appl 37(4):2790–2798 
11. Li DF (2010) TOPSIS-based nonlinear-programming methodology for multiattribute decision 
making with interval-valued intuitionistic fuzzy sets. IEEE Trans Fuzzy Syst 18(2):299–311 
12. Bard JF, Sousk SF (1990) A trade analysis for rough terrain cargo handlers using the AHP: an 
example of group decision making. IEEE Trans Eng Manag 37(3):222–228 
13. Kahraman C, Öztay¸si B, Çevik Onar S (2016) A comprehensive literature review of 50 years 
of fuzzy set theory. Int J Comput Intell Syst 9:3–24 
14. Xu Z, Liao H (2013) Intuitionistic fuzzy analytic hierarchy process. IEEE Trans Fuzzy Syst 
22(4):749–761 
15. Hwang CL, Yoon KP (1981) Multiple attribute decision making: methods and applications, a 
state-of-the-art survey. Springer, Berlin 
16. Chen YT, Peng WC, Yu HY (2018) Identify key factors for career choice by using TOPSIS 
and fuzzy cognitive map. In: 2018 IEEE/ACIS 17th international conference on computer and 
information science (ICIS). IEEE. 
17. Park JH, Park IY, Kwun YC, Tan X (2011) Extension of the TOPSIS method for decision 
making problems under interval-valued intuitionistic fuzzy environment. Appl Math Model 
35:2544–2556 
18. Rachana PJ, Kodipalli A, Rao T (2022) Comparison between ResNet 16 and Inception 
V4 network for COVID-19 prediction. In: Emerging research in computing, information, 
communication and applications: proceedings of ERCICA 2022. Springer, Singapore, pp 
283–290 
19. Guha S, Kodipalli A, Rao T (2022) Computational deep learning models for detection of 
COVID-19 using chest X-Ray images 
20. Bhoomika R, Shahane S, Siri TC, Rao T, Ashwini K, Chodon PK (2022) Ensemble learning 
approaches for detecting Parkinson’s disease 
21. Kodipalli A, Guha S, Dasar S, Ismail T (2022) An inception-ResNet deep learning approach 
to classify tumours in the ovary as benign and malignant. Expert Syst e13215 
22. Ruchitha PJ, Richitha YS, Kodipalli A, Martis RJ (2021) Segmentation of ovarian cancer 
using active contour and random walker algorithm. In: 2021 5th international conference on 
electrical, electronics, communication, computer technologies and optimization techniques 
(ICEECCOT). IEEE, pp 238–241 
23. Kodipalli A, Devi S, Dasar S, Ismail T (2022) Segmentation and classiﬁcation of ovarian cancer 
based on conditional adversarial image to image translation approach. Expert Syst e13193 
24. Ruchitha PJ, Sai RY, Kodipalli A, Martis RJ, Dasar S, Ismail T (2022) Comparative analysis of 
active contour random walker and watershed algorithms in segmentation of ovarian cancer. In: 
2022 international conference on distributed computing, VLSI, electrical circuits and robotics 
(DISCOVER). IEEE, pp 234–238 
25. Gururaj V, Ramesh SV, Satheesh S, Kodipalli A, Thimmaraju K (2022) Analysis of deep 
learning frameworks for object detection in motion. Int J Knowl-Based Intell Eng Syst 26(1):7– 
16 
26. Guha S, Kodipalli A, Rao T (2022) Computational deep learning models for detection of 
COVID-19 using chest X-Ray images. In: Emerging research in computing, information,

Prediction of Schizophrenia in Patients Using Fuzzy AHP and TOPSIS …
527
communication and applications: proceedings of ERCICA 2022. Springer, Singapore, pp 
291–306 
27. Zacharia S, Kodipalli A (2022) COVID vaccine adverse side-effects prediction with sequence-
to-sequence model. In: Emerging research in computing, information, communication and 
applications: proceedings of ERCICA 2022. Springer, Singapore, pp 275–281 
28. Kodipalli A, Devi S (2021) Prediction of PCOS and mental health using fuzzy inference and 
SVM. Front Public Health 1804 
29. Bhagwani H, Agarwal S, Kodipalli A, Martis RJ (2021) Targeting class imbalance problem 
using GAN. In: 2021 5th international conference on electrical, electronics, communication, 
computer technologies and optimization techniques (ICEECCOT). IEEE, pp 318–322 
30. Dhanush N, Prajapati PR, Revanth M, Ramesh R, Kodipalli A, Martis RJ (2021) Prediction 
of gold price using deep learning. In: 2021 IEEE 9th region 10 humanitarian technology 
conference (R10-HTC). IEEE, pp 1–5 
31. Raj A, Umrani NR, Shilpashree GR, Audichya S, Kodipalli A, Martis RJ (2021) Forecast 
of COVID-19 using deep learning. In: 2021 IEEE international conference on electronics, 
computing and communication technologies (CONECCT). IEEE, pp 1–5 
32. Sanjana S, Sanjana S, Shriya VR, Vaishnavi G, Ashwini K (2021) A review on various method-
ologies used for vehicle classiﬁcation, helmet detection and number plate recognition. Evol 
Intel 14(2):979–987 
33. Gururaj V, Shriya VR, Ashwini K (2019) Stock market prediction using linear regression and 
support vector machines. Int J Appl Eng Res 14(8):1931–1934

Sports Activity Recognition - Shot Put, 
Discus, Hammer and Javelin Throw 
Swati Shilaskar, Gayatri Aurangabadkar, Chinmayee Awale, 
and Sakshi Awale 
Abstract This paper describes the classiﬁcation of sports activities using still 
images. Sports activities speciﬁcally include discus throw, shot put and javelin throw. 
This system is useful for athletes preparing for these sports and it will help them 
to understand and practice the right posture for these sports. Along with them, this 
model is also helpful to the coaches to train their players and increase their chances of 
winning. Dataset is created by collecting the different images of the following sports 
from different sources. Various machine learning algorithms such as random forest, 
KNN, SVM (Polynomial), voting classiﬁer has been applied to the model out of 
which SVM (poly) performed better than the rest with an accuracy of 98.7%. Voting 
classiﬁer and Random Forest has also performed well. BRISK feature descriptor 
method is found useful to get point of interest in the image dataset. Techniques 
like image augmentation used for creating artiﬁcial dataset to which is useful for 
training the machine learning model and Principal Component Analysis (PCA) used 
for dimensionality reduction is found helpful to increase the accuracy of the model. 
Keywords Discuss · shotput · hammer throw · javelin · machine learning ·
classiﬁcation · BRISK
S. Shilaskar · G. Aurangabadkar envelope symbol · C. Awale · S. Awale 
Vishwakarma Institute of Technology, Pune, India 
e-mail: gayatri.aurangabadkar20@vit.edu 
S. Shilaskar 
e-mail: swati.shilaskar@vit.edu 
C. Awale 
e-mail: chinmayee.awale20@vit.edu 
S. Awale 
e-mail: sakshi.awale20@vit.edu 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_45 
529

530
S. Shilaskar et al.
1 
Introduction 
The Olympic hurling events are Javelin, discus, shot put and discus throw where 
they toss the respective object by either swinging around at the end of the chain or 
hurling it with brute force. During the Olympics season, sports is a major part of 
the media, and its classiﬁcation can contribute to analysis and developing high-level 
visual understanding machines. Classiﬁcation of actions like throwing or hurling can 
be difﬁcult and a challenging task in computer vision. Recognition of activities from 
still images is somewhat more complex as compared to videos because in images there 
is restricted information that we can get and also varied backgrounds can increase 
the chances of false recognition. According to a survey on papers based on human 
activity recognition for images datasets. It is observed that deep learning methods 
has worked with better accuracy on larger datasets and for small datasets machine 
learning methods are more useful [1]. Also, the combined machine algorithms also 
give better accuracy. 
Since our paper only focuses on sports with similar actions in picture, the task 
for distinguishing can be tough and be determined mainly on the basis of which 
object is held in the hand. The primary approach for this problem statement would 
be through Machine learning. The model is build using various machine learning 
algorithms such as random forest, KNN, SVM (Polynomial), voting classiﬁer, etc. A 
feature descriptor technique named BRISK (Binary Robust Invariant Scalable Key 
points) is applied to the dataset along with PCA (Principal Component Analysis) for 
dimensionality reduction. 
2 
Literature Review 
Computer vision is one of the emerging ﬁelds in the today’s world. A lot of human 
work is made easy using different machine learning and deep learning techniques. 
Recognition of activities from still images is one of the trending and useful appli-
cations in the ﬁeld of computer vision. In this paper we have tried to make a sports 
activity recognition model based on images with the help of different machine 
learning algorithms. Various human activities recognition systems and classiﬁca-
tion models for human activities have been surveyed. Some of the papers that we 
have surveyed, and their insights are as given below: 
The Author has overviewed the different methods that are being used by different 
people for human activity recognition and gave his analysis based on them. Some 
of the methods include HMM, PCA, image-based rendering, CRF, approach using 
context-free grammar (CFG)-based representation etc. They also discussed the chal-
lenges faced during the recognition and areas where this human activity recognition 
is useful [1]. Comparison between the deep learning and machine learning methods 
for human activity recognition is done by the author. Deep learning methods include 
Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) and

Sports Activity Recognition - Shot Put, Discus, Hammer and Javelin Throw
531
machine learning methods includes Support Vector Machine and Random forest. 
Experiment analysis is done on two different datasets USC-HAD and WISDM with 
the accuracies 90 and 87% in DL and TML respectively. Lastly, it’s concluded that 
for small scale datasets TML is suitable and for large scale datasets DL approaches 
are suitable [2]. Comparative analysis of classiﬁcation of sport activities based on 
different machine learning algorithms is done by the author. The algorithms include 
decision tress, Random Forest Classiﬁer, K-Nearest Neighbors (K-NN), Support 
Vector Machines (SVM), Gradient boosting, Neural Networks, and its variants. These 
algorithms are performed on data collected from sensors for a total 19 different 
activities. It is stated that among all classiﬁers gradient boosting has given better 
performance [3]. Author proposed a human activity recognition system using depth 
silhouettes, context features and advance HMM. The input data is a real time data 
taken with the help of depth sensors. They compared the depth silhouettes features 
approach with approach using state of art features and observed that their approach 
of depth silhouettes gives better results [4]. Based on feature extraction from depth 
silhouettes, authors proposed human activity recognition. In this method they have 
proposed a combination of joints and body shape features. Training and testing data 
is processed using SOM. They have got the accuracy of 92.43% on their dataset [5]. 
To capture the interesting objects and recognize them correctly in a real time image, 
the author has proposed a method named centerpiece interaction framework. The 
model mainly consists of two stages. First one graph-based capturing of interesting 
objects modelled using 2.5 D spatial co-occurrence context objects and second is 
hierarchical modelling where relative features of interesting objects are automat-
ically learned. It has been observed that this framework performs better than the 
other method used i.e., state of art methods [6]. The author has proposed a survey 
on human action recognition from still images. They have covered various points in 
the survey such as different databases used for recognition of activities and shared 
the results of performance, algorithms and methods used in action learning, various 
high-level cues and low-level features in image analysis. This survey is helpful for 
get the insight of steps that must be performed for human activity recognition from 
still images [7]. A method to recognize human activities using a supervised machine 
learning model with the help of 2D skeletal data has been proposed by author. This 
model proposes four activities which are sit, stand, fall and walk. Firstly, the data 
is extracted from videos, next the shape and motion features are extracted and then 
the model is trained using different classiﬁers such as knn, svm, naive bayes, linear 
discriminant and feed-forward backpropagation neural network. Among these all, 
knn has given the best accuracy of 98% [8]. Author has proposed human activity 
recognition using combined SVM and HMM. Kinect sensor is used to take input data. 
Then the features are extracted such as motion features, structure features, polar coor-
dinate features. These features are combined in some way named as fusion features 
with much stronger descriptive ability to avoid false recognition between somewhat 
similar kinds of images. Firstly, SVM and HMM are separately implemented and then 
combined SVM & HMM is implemented. It has been observed that a combined algo-
rithm has given better results. Recognition rate reached 98% [8]. A semi-supervised 
learning method is used by the author for activity recognition. Accuracy of the model

532
S. Shilaskar et al.
is improved with the help of unlabeled data. Decision trees, Naive Bayes classiﬁer 
and K-Nearest neighbors (K = 3) are used in the experiments. and then compared 
the En-Co-training algorithm. It is noted that en-co- training gives better results [9]. 
A method to automatically classify sports using different cue techniques like neural 
net, texture code and combination of both is implemented. Classiﬁcation of ﬁve 
different sports i.e., tennis, track events, swimming, yachting, and cycling is done 
which are further cut into shots using automatic shot cut algorithm and the frames 
are extracted from these shots. It is stated that neural net has performed better than 
texture code with only 6% wrong classiﬁcation. Even the combination of both could 
not perform better than neural net [10]. Classiﬁcation of ﬁve different sports based 
on positional data of people is implemented. Heatmaps are plotted for summarizing 
the positional data which is being collected. These heatmaps are further classiﬁed 
after the dimensionality reduction using PCA and Fisher’s linear discriminant. For 
training and testing purpose these heatmaps are used. The model resulted in the total 
accuracy of 94.24% [11]. Deep learning approach with CNN an RNN is used for 
sports classiﬁcation with datasets containing 10 and 15 classes. Along with it, transfer 
learning method is also used with VGG-16 model which has given the accuracy of 
92 and 94% respectively for 15 and 10 classes respectively. 
3 
Methodology 
Image processing helps one to interpret a better result out of the given dataset of 
images. Some of the important concepts while working on image processing and 
machine learning one needs to consider few points so that the outcome gives a good 
result. A good dataset, feature detection to get the key features of the image. This step 
helps one even when the image is rotated at certain angle identifying all the features 
correctly. And a multiple machine learning models to compare your results with and 
then decide the best model depending on its accuracy. Figure 1 shows ﬂowchart of 
complete classiﬁcation model.
3.1 
Creation of Dataset 
The dataset is created by collecting images from various sources. Since this project 
solely focuses on discuss throw, javelin throw, hammer throw and shot put, ﬁrst few 
images for each group have been collected from ‘100 sports image classiﬁcation’ 
dataset easily available on Kaggle. Figures 2, 3, 4 and 5 are the images of each 
sport i.e., discuss, hammer throw, shotput and javelin respectively from the Kaggle 
dataset. Then the next few images were taken frame by frame from video dataset for 
the respective sports. The dataset is available on ‘Olympic Sports Dataset - Stanford 
Vision Lab’ in seq format. These videos were converted to Mp4 using ffmpeg library. 
and the later section of the dataset was collected from other online sources. We also

Sports Activity Recognition - Shot Put, Discus, Hammer and Javelin Throw
533
Fig. 1 Flowchart of 
Complete classiﬁcation 
model
used image augmentation technique to increase our dataset. Image augmentation 
technique artiﬁcially creates a greater number of images through the given images 
by performing operations like shifting, scaling, rotation, etc. So, in total there are 
8000 images in our dataset i.e., 2000 images of all four classes discuss, javelin, 
hammer throw, shotput. All these images were them resized to the dimension of 
224 × 224 and duplicate and redundant or corrupt images were manually deleted 
from the dataset. These images were then divided among a testing and training set.
3.2 
BRISK Feature Descriptor 
In this model, we mainly used the following modules on python:-NumPy, Matplotlib, 
Cv2, CSV, pickle and sklearn. To store the results after working on the pixels of the 
images, CSV ﬁles were created. For 4 different sports 4 csv ﬁles were created. 
The images were then converted from RGB channel to Gray since the computation 
in range 0 to 255 is much easier and less complex than when working on three 
different channels. These images were then re-sized to dimension 100 × 100 and

534
S. Shilaskar et al.
Fig. 2 Discuss 
Fig. 3 Hammer Throw 
Fig. 4 Shotput 
Fig. 5 Javelin

Sports Activity Recognition - Shot Put, Discus, Hammer and Javelin Throw
535
were applied with a Gaussian blur. This step helps reduce the noise in the image 
since few images have been captured in low light. These images were then equalized 
using the histogram where the image input should always be in grayscale. On these 
equalized pixel values, Prewitt operator was applied so that the edges in the image 
can be easier to detect. (1) & (2) given below speciﬁed the kernels used in Prewitt 
operator for X & Y axis respectively. 
Kernel for up pe r X mi
nu
s 
axi s c olo
n
 
S
t
a
r
t
 
3 
B
y 3 Matrix 1st Row 1st Column negative 1 2nd Column negative 1 3rd Column negative 1 2nd Row 1st Column 0 2nd Column 0 3rd Column 0 3rd Row 1st Column 1 2nd Column 1 3rd Column 1 EndMatrix
Kernel for up pe r Y mi
nu
s 
ax i s c
ol o n S
ta r t 3
 B
y
 3 Matrix 1st Row 1st Column negative 1 2nd Column 0 3rd Column 1 2nd Row 1st Column negative 1 2nd Column 0 3rd Column 1 3rd Row 1st Column negative 1 2nd Column 0 3rd Column 1 EndMatrix
Binary Robust Invariant Scalable Key points, BRISK then helps in feature point 
detection. BRISK is scale and rotation invariant. It adopts ﬁxed neighborhood 
sampling technique with the help of which the data points are plotted in form of 
concentric circles. This step helps us capture the key points and descriptors of the 
image which proves to be important as even if the images are rotated or even if its 
scale changed it is easy to detect the key features of the image. This is an alternative 
for SIFT. 
The csv ﬁles are then read into data frames so that the further data computation on 
the key points and descriptors can be done. K means is a clustering algorithm which 
divides the data into k clusters. A k means pretrained model was implemented on 
the data frame and predictions were noted down. All the above steps were repeated 
once again so that the k means prediction can give us the desired values. 
The data was then converted in int 64 using astype function. These key points 
and descriptors are then respectively stored in their csv ﬁles. The last column was 
stored with the labels of the respective sports which was found through the k means 
predictions. To reduce the dimensionality and avoid the dimensionality curse which 
leads to over-ﬁtting, principal component analysis (PCA) was applied to the dataset. 
A standard scale was created which would standardize the data in a standard format. 
Equation (3) represents the formula for standard scaler. 
z e
quals St artFraction value minus mean Over Standard deviation EndFraction
z equals StartFract
ion value minus mean Over Standard deviation EndFraction
Further a transform function is applied to it. In PCA a Covariance matrix is 
calculated so that the principal components can be found by computing the eigen 
values and eigen vectors. By doing so we get the maximum amount of information 
by reducing the variables enough that one wouldn’t compromise on the accuracy 
of the models. Figures 6 and 7 shows the ﬂowchart of pretraining operations and 
ﬂowchart of training & classiﬁcation model respectively.

536
S. Shilaskar et al.
Fig. 6 Flowchart of 
pretraining model 
implementation

Sports Activity Recognition - Shot Put, Discus, Hammer and Javelin Throw
537
Fig. 7 Flowchart of training and classiﬁcation model 
Fig. 8 Accuracy of 
Classiﬁer algorithms

538
S. Shilaskar et al.
3.3 
Training of the Model 
To train our classiﬁcation model using different machine learning algorithms, we 
used the standardized data which is preprocessed with PCA (Principal Component 
Analysis). Dataset is split in such a way that 80% of the data is used for training and 
20% is used to test the classiﬁcation model. To better visualize the performance of 
different algorithms, ROC curves are plotted. These are the algorithms that are used 
for training this classiﬁcation model. 
Random Forest. It is a machine learning algorithm based on the concept of ensemble 
learning i.e., to combine different classiﬁers in order to solve a complex problem. 
It is a supervised machine learning technique used for both classiﬁcation as well as 
regression. It is basically a bunch of n number decision trees where each decision 
tree. More the number of trees better is the accuracy. It gives the prediction for 
each decision tree and based on the majority of predictions; each new points are 
assigned to the relevant category. It is observed that this algorithm is capable of 
handling multiclass classiﬁcations and datasets with higher dimensionality and better 
in dealing with overﬁtting. 
KNN. It is a supervised machine learning algorithm which is used for classiﬁcation 
as well as regression but mainly used for classiﬁcation. In this algorithm, ﬁrstly the 
value of K is selected and then the K nearest neighbors is selected by calculating 
Euclidean distance between the different data points. Number of data points in each 
category is calculated among these k nearest neighbors and the new value is assigned 
to the category containing maximum numbers of neighbors. This algorithm does not 
learn anything from training dataset instead it stores the dataset and performs action 
directly while performing classiﬁcation. 
Voting Classiﬁer. In voting classiﬁer, various machine learning models are fed to 
the classiﬁer and predictions are made on the basis of highest probability of that class 
being predicted in each of those classiﬁers that are being fed to the voting classiﬁer. 
Rather than creating separate model, this classiﬁer helps to create a model which 
trains by the models fed to it and predict the outcome on the basis of their combined 
majority. 
Gaussian Naïve Bayes. Gaussian Naïve Bayes is one of the types of naïve bayes 
algorithm. It is a probabilistic machine learning algorithm based on bayes theorem of 
probability. In this classiﬁcation model, it is assumed that each class follows normal 
distribution. It means that the continuous values taken by predictors are sampled by 
gaussian distribution. 
SVM. Supervised machine learning algorithm which is used for both classiﬁcation 
as well as regression. In this algorithm, target classes in a hyperplane are separated 
in a multidimensional space. A decision boundary is created to separate different 
classes to make correct predictions. Kernel technique is used in SVM to input data 
space into desired form. Different kernel functions are available in SVM like linear,

Sports Activity Recognition - Shot Put, Discus, Hammer and Javelin Throw
539
sigmoid, polynomial, RBF, etc. We have implemented SVM with polynomial kernel 
in this classiﬁcation model. Equation (4) gives formula for SVM with polynomial 
kernel. 
upper  F le ft  paren the
sis upper X comma upper X Subscript i Baseline right parenthesis equals left parenthesis upper X period upper X Subscript i Baseline right parenthesis Superscript d
Here, upper  F left parenthesis upper X comma upper X Subscript i Baseline right parenthesis represents decision boundary for separating the classes, d denotes 
the degree & ‘.’ Represents dot product. 
3.4 
Testing of the Model 
As the model is now trained using the above-mentioned machine learning algorithms, 
it is then tested using remaining 20% of the dataset. Separate csv ﬁles are created 
for each class in the dataset. All the test images are ﬁrst resized, and gaussian noise 
removal technique is applied to the images. Then the Prewitt operator is implemented 
for edge detection. After that BRISK algorithm is applied to the testing dataset for 
feature extraction of test images. These features are then stored in separate csv ﬁles. 
To reduce the dimensionality and transform the dataset to the required form, principal 
component analysis (PCA) technique is used on the test images. This process is 
further followed by implementation of various machine learning algorithms such 
as random forest, KNN, SVM with polynomial kernel, poly, Naïve bayes, voting 
classiﬁer, etc. 
4 
Results and Discussions 
We have implemented various machine learning algorithms such as random forest, 
KNN, SVM with polynomial kernel, Naïve bayes, voting classiﬁer, etc. Among all 
of these algorithms, SVM with polynomial kernel has performed better than the rest 
algorithms with the accuracy of 98.75%. Apart from SVM, voting classiﬁer and 
random forest have also given good accuracies of 97.5 and 96.25%. Some other 
algorithms that we used in building our machine learning model are KNN and naïve 
bayes. They have given the accuracies 68.75 and 65% respectively (Fig. 8). 
All models are compared based on the accuracies and confusion matrices. Table 1. 
shows the training and testing accuracy, precision, recall and F1 score for various 
algorithms used.

540
S. Shilaskar et al.
Table 1 Performance evaluation of different models (in%) 
Model
Train Accuracy
Test Accuracy
Precision
Recall
F1 
Random Forest
100
96.25
96.25
96.26
96.25 
KNN
83
68.75
69.73
68.75
68.37 
SVM Poly
98.81
98.75
98.80
98.75
98.74 
Naive Bayes
72
65
65
65
65 
Voting Classiﬁer
99.8
97.50
97.61
97.50
97.49 
5 
Conclusion 
Different machine learning algorithms like random forest, SVM polynomial, Knn, 
voting classiﬁer, etc. for the classiﬁcation of sport activities like discuss, hammer 
throw, javelin and shotput are implemented. Out of which SVM poly has given the 
highest accuracy of 98.75%. Apart from these algorithms, Random Forest and voting 
classiﬁer has also performed good with an accuracy of 96.25 and 97.5% respectively. 
Brisk algorithm used for key points and feature. 
References 
1. Bai L, Li K, Pei J, Jiang S (2015) Main objects interaction activity recognition in real images. 
Neural Comput Appl 27:335–348 
2. Wu H, Pan W, Xiong X, Xu S (2014) Human activity recognition based on the combined 
SVM&HMM 
3. Balamurugan R, Prasanth NN, Prabakar MA, Devi KV (2021) Sports activity recognition using 
classiﬁcation techniques 
4. Jalal A, Kamal S, Kim D (2015) Individual detection-tracking-recognition using depth activity 
images 
5. Jalal A, Kim Y, Kamal S, Farooq A, Kim D (2015) Human daily activity recognition with joints 
plus body features representation using Kinect sensor 
6. Guo G, Lai A (2014) A survey on still image based human action recognition. Pattern Recogn 
47:3343–3361 
7. Ahad MAR, Tan JK, Kim HS, Ishikawa S (2008) Human activity recognition: various paradigms 
8. Guan D, Yuan W, Lee Y-K, Gavrilov A, Lee S (2007) Activity recognition based on semi-
supervised learning 
9. Hou C (2020) A study on IMU-based human activity recognition using deep learning and 
traditional machine learning 
10. Messer K, Christmas W, Kittler J (2002) Automatic sports classiﬁcation 
11. Gade R, Moeslund T (2013) Sports type classiﬁcation using signature heatmaps

User Acceptance of Contact Tracing 
Apps: A Study During the Covid-19 
Pandemic 
Inger Elisabeth Mathisen, Kanika Devi Mohan, Tor-Morten Grønli, 
Tacha Serif, and Gheorghita Ghinea 
Abstract The unpredicted coronavirus outbreak, termed COVID-19, has placed 
numerous governments worldwide in a difﬁcult position. The scarcity of resources 
to tackle the outbreak, combined with the fear of overburdening the healthcare 
system, has forced most countries into a state of lockdown. Many governments have 
shown great interest in digital contact tracing applications that can help automate the 
demanding task of tracking newly infected individuals’ recent contacts. However, 
these apps have created a great deal of discussion, especially regarding their tech-
nology, architecture, and the adoption rate needed. This study aims to contribute to an 
increased understanding of the acceptance of this technology in the Norwegian popu-
lation. Based on the uniﬁed theory of acceptance and use of technology (UTAUT) 
model, our research model incorporates the following ﬁve constructs: performance 
expectancy, effort expectancy, social inﬂuence, facilitating conditions, and privacy 
consideration. A survey was distributed amongst the Norwegian population, and 
the results were obtained from a sample of 258 respondents. The results from this 
study indicate that performance expectancy has the most signiﬁcant impact on the 
intention to use a contact tracing application. Privacy considerations are also impor-
tant, followed by effort expectancy and social inﬂuence. Facilitating conditions were 
found to be much less important. 
Keywords User acceptance · Contact tracing · App
I. E. Mathisen · K. D. Mohan · T.-M. Grønli · G. Ghinea envelope symbol
Kristiania University College, Oslo, Norway 
e-mail: george.ghinea@brunel.ac.uk 
T.-M. Grønli 
e-mail: tor-morten.gronli@kristiania.no 
T. Serif 
Yeditepe University, Istanbul, Turkey 
e-mail: tacha.serif@persystlab.org 
G. Ghinea 
Brunel University London, Uxbridge, UK 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_46 
541

542
I. E. Mathisen et al.
1 
Introduction 
Throughout history, several pandemics with varying magnitude, severity, and dura-
tion have been portrayed. Still, to ﬁnd a crisis at a similar scale the whole world 
is currently facing, we need to go back more than a hundred years to the Spanish 
ﬂu that lasted from 1918 to 1920. On 11th March 2020, the World Health Organi-
zation declared the outbreak, termed Covid-19, a global pandemic. Today, over a 
year later, we are still in the middle of the pandemic, and it has led the world into a 
state of uncertainty and fear. The Coronavirus disease 2019 (COVID-19) is a rapidly 
spreading infectious disease caused by severe acute respiratory syndrome coron-
avirus 2 (SARS-CoV-2). For months, the absence of treatment or vaccinations made 
the public health response rely on measures such as physical distancing, isolation 
of cases, and manual contact tracing. The virus has spread with increased fatalities 
worldwide, and the pandemic has made a signiﬁcant impact on our everyday life 
and caused disruption in ways few could have ever imagined. In order to “ﬂatten 
the curve” of Covid-19 cases, governments worldwide have mandated lockdown and 
self-isolation for people in infected areas, proposed work from home policies, intro-
duced strict social distancing criteria, and a new infrastructure for mass testing [1]. 
Most people infected with the virus will experience mild to moderate respiratory 
illness and recover without special treatment. However, older individuals and people 
with underlying medical issues, such as cardiovascular disease, diabetes, chronic 
respiratory disease, and cancer, are more likely to develop a severe illness. World-
wide, the number of Covid-19 cases has reached 166 million [2] (as of 21.05.22), 
and the numbers continue to rise. 
The potential use of contact tracing for the handling of Covid-19 has been 
discussed in previous studies [3, 4], and their ﬁndings suggest that contact tracing 
applications can reduce the spread of Covid-19 and further ease lockdown. After the 
emergence of Covid-19, the ﬁrst contact tracing application (hereafter referred to as 
CTA) was developed and deployed in China on 11th February 2020, and several coun-
tries quickly followed. Today, governments, healthcare agencies, and private orga-
nizations worldwide have requested the development and implementation of contact 
tracing technologies as a central part of their Covid-19 management strategy [5]. 
Although these CTAs can signiﬁcantly facilitate the management of the pandemic, 
they raise several technical and non-technical concerns. It can be challenging for 
policymakers to design one app that ﬁts all individuals in a society. It is also argued 
that understanding how to achieve mass acceptance in the population is a primary 
concern for the policymakers responsible for designing and promoting the app [5], 
especially when the willingness to accept such apps differs between critics, advocates, 
and undecided individuals in a society. 
The aim of this study and the main contribution will be to add to the body of 
existing knowledge and provide a deeper understanding of the acceptance of CTAs. 
Further, we want to examine the prevalence of these digital tools and how authorities 
in different countries have adopted this technology as part of their strategy to prevent 
the spread of the Covid-19 virus. Therefore, the research will be guided by the

User Acceptance of Contact Tracing Apps: A Study During …
543
following research question: What are the driving factors for acceptance of contract 
tracing apps during the Covid-19 pandemic? 
2 
Related Work 
2.1 
Governmental Approaches in Different Societies 
The rapid spread of the Covid-19 virus and the unusual circumstances have faced 
governments and authorities worldwide with many new challenges. As presented 
earlier, previous studies show how CTA works effectively if a large part of the popu-
lation download and use it. To governments, this poses a collective action problem, 
which refers to “the actions taken by a group of people to achieve a common goal” 
[6]. Consequently, signiﬁcant different approaches have been taken by governments 
worldwide to achieve a high adoption rate of CTA’s. The MIT project on Covid-19 
CTA’s lists over 49 apps at this stage, which governments have employed across the 
world for contact tracing during the pandemic. The project lists different applica-
tions ranging from those that are mandatory and those that are voluntary. The various 
applications differ signiﬁcantly in terms of transparency, data destruction, and the 
technology employed. The different design features in these apps have fueled heated 
debates and raised concerns regarding the data governance needed. Previous work 
[6–8] shows that there are no globally ideal approaches, only locally contextualized 
ones that depend on immediate health risk, prior experience with pandemics, societal 
values, and national culture. Also, the role of government, trust in government, and 
trust in technology in each society play an important part. 
A study in [4] explored the beneﬁt and drawbacks of government surveillance 
within a public health crisis, more speciﬁcally the Covid-19 outbreak. They collected 
data on new conﬁrmed Covid-19 cases per day from six different countries: China, 
Italy, Germany, Singapore, South Korea, and the US. Further, they observed two 
groups of different countries for two distinct periods, where one group was exposed 
to a treatment of mandated mobile apps in the second period but not in the ﬁrst. This 
group consisted of South Korea and Singapore. The second country group were not 
exposed to the treatment of mobile app tracing during either period. The control group 
included China, Germany, Italy, and the US, which did not mandate mobile tracing 
apps. Their study showed that having mandatory mobile tracing and monitoring may, 
on average, reduce new Covid-19 cases per day. Further, the researchers argued that 
the impact would dramatically ﬂatten the exponential growth of new cases, especially 
if a mandated policy is introduced in the early stage of an outbreak. This can indicate 
that required enforcement of mobile contact tracing could generate the best health 
outcome [6].

544
I. E. Mathisen et al.
2.2 
Smittestopp 
In eight months, the Norwegian government has launched two versions of their CTA 
“Smittestopp.” The ﬁrst version was launched on 16th April 2020 and was developed 
by the research company Simula on behalf of the Norwegian Institute for Public 
Health (Folkehelseinstituttet – hereafter referred to as FHI). The ﬁrst version of the 
CTA aimed to achieve infection tracing and use the data for further research. Although 
this was a voluntary app, it was heavily recommended by the Norwegian government, 
and prime minister Erna Solberg even said: “I believe that if we are to get our everyday 
life and freedom back, as many people as possible need to download the app”. The 
effect of this showed, and by the end of April 2020, 1.5 million Norwegians had 
downloaded the CTA. However, the ﬁrst version of the CTA quickly received a wide 
range of criticism from Norway’s technology and data protection communities. Even 
Amnesty International raised privacy concerns related to CTA’s. They classiﬁed the 
apps employed in Bahrain, Kuwait, and Norway (ﬁrst version) to be among the most 
intrusive and least safe for data privacy. They argued that “Smittestopp 1.0” stored 
data centrally and included GPS tracking in almost real-time. This gives authorities 
access to information about how individuals live their lives, making the app one of the 
most intrusive (Amnesty). Therefore, on 15th June 2020, FHI received notiﬁcation 
from the Norwegian data protection authorities (Datatilsynet), and they had to stop 
further data collection and delete previously collected data due to privacy breaches. 
Datatilsynet argued that the technology and architecture in the CTA and the low 
infection rates in the country (as of May 2020) did not justify the extensive breach 
of privacy. 
After what can be perceived as a “failure” of the ﬁrst version of “Smittestopp”, FHI 
opened up for other suppliers to submit a proposal for a new CTA. Six months after 
the ﬁrst version was halted, on the 21st of December 2020, FHI launched the “new 
and improved Smittestopp”. The latest version is still voluntary to use and available 
to anyone over sixteen. However, there are signiﬁcant changes in the technology 
used. “Smittestopp 2.0” is based on the technology from Apple and Google’s API. 
Unlike the previous version, which was based on GPS tracking, the new version uses 
Bluetooth proximity tracing. Compared to the ﬁrst version, the new CTA is only used 
for infection tracing and not for analysis or further research. 
3
Method
 
As described above, the primary aim of this research is to assess behavioral intention 
towards use of CTA’s during Covid-19 to achieve mass acceptance. When examining 
the topic from an individual point of view, it seems rational to choose a survey as the 
overall strategy for the data generation and make a questionnaire sent to a sample 
of the population. The idea of survey research is to obtain the same kind of data 
from many people in a standardized and systematic way. The main advantage of this

User Acceptance of Contact Tracing Apps: A Study During …
545
method is that it is quite cost-effective and can provide representations from a wider 
population. However, a limitation is that this method lacks in-depth or new insight 
on the research topic. 
3.1 
Measures 
Dependent Variable. The underlying theory for all the intention models is that 
behavioral intention will signiﬁcantly inﬂuence technology usage and actual use [8]. 
However, in our study, we have only retained behavioral intention as a dependent 
variable. This is because the population has no previous experience with CTA’s 
and we want to precisely investigate the behavioral intention to use this type of 
technology. User acceptance was thus operationalized as behavioral intention and 
was based on: “I intend to download an infection tracking app if a new pandemic 
occurs in the future”. The predictor of acceptance is measured using a ﬁve-point 
Likert scale, where 1 = “strongly disagree”, 2 = “disagree”, 3 = “neither agree nor 
disagree”, 4 = “agree”, and 5 = “strongly agree”. 
Predictors (Independent Variables). As stated above, we chose to use the UTAUT 
model as a framework for our research. However, the wording and content deviate 
somewhat from the UTAUT guidelines [8]. Still, the essence of each predictor is 
captured. These predictors are; Performance Expectancy (PE), Effort Expectancy 
(EE), Social Inﬂuence (SI), and Facilitating Conditions (FC). In addition, we included 
privacy considerations (PC) and demographic information. 
Most items in the questionnaire were measured using a ﬁve-point Likert scale, 
where respondents could choose their degree of agreement. Regarding the demo-
graphic information, age was measured using the following categories 1 = 16–25, 
2 = 26–35, 3 = 36–45, 4 = 46–55, 5 = 56–65, 6 = 66–75, 7 = 75+. Gender was 
coded using variables, where 1 represented male, and 2 represented female. Level 
of education was measured using six variables: “primary school”, “high school”, 
“vocational school”, “university/college: up to three years”, “university/college: 
4 years or more”, or “other”. Experience with mobile use and apps was measured 
using categories for average time spent on a mobile device daily, using the categories: 
“0–60 min”, “1–2 h”, “3–4 h”, “5 h or more”, or “I don’t know”. We also included 
variables that revealed whether the respondents had downloaded one, two, or none 
of “Smittestopp”.

546
I. E. Mathisen et al.
4 
Results 
4.1 
Demographics 
Two hundred ﬁfty-eight people participated in our study. Of the entire sample (n = 
258), 168 (65.1%) were women and 90 (34.9%) were men. Age was measured using 
categories, and most of the participants (n = 165, 64%) were in the age categories 
26–36 years (n = 106, 41.1%), followed by 16–25 years (n = 59, 22.9%). A large 
proportion of the sample consists of individuals with up to a three-year education 
from a university or college (n = 108, 41.9%), and individuals with a master’s degree 
or higher (n = 83, 32.2%). The remaining participants were distributed between 
individuals with vocational school or other 1–2-year education (n = 28, 10.9%), 
high school (n = 34, 13.2%), and primary school (n = 5, 1.9%). A big share of the 
sample uses their mobile phone between 3–4 h every day (n= 115, 44.6%). Regarding 
the Norwegian CTA “Smittestopp”, (n = 146, 56.6%) of the sample downloaded the 
ﬁrst version, and (n = 92, 35.2%) downloaded the second version. 
4.2 
Correlation Between Variables 
We conducted a correlation analysis to investigate the relationship between the vari-
ables: Performance Expectancy (PE), Effort Expectancy (EE) Social Inﬂuence (SI), 
Facilitating Conditions (FC), Privacy Considerations (PC) and Behavioral Intention 
(BI). The variables were investigated using Pearson Product-Moment Correlation 
Coefﬁcient. The magnitude of an effect size is small when its value is larger than 
or equal to 0.10 but less than 0.30, medium when its value is larger than or equal 
to 0.30 but less than 0.50, and large when its value is higher than or equal to 0.50 
[9]. A strong correlation existed between PE and EE (r = 0.54, p ≤ 0.001), between 
PE and PC (r = 0.55, p ≤ 0.001), and between EE and PC (r = 0.56, p ≤ 0.001). 
Further, a strong correlation was seen between BI and the variables PE (r = 0.64, p 
≤ 0.001), EE (r = 0.59, p ≤ 0.001) and PC (r = 0.64, p ≤ 0.001.) 
4.3 
Evaluation 
To evaluate our proposed model, we performed a standard multiple regression anal-
ysis. Multiple regression reveals how much variance in the dependent variable can be 
explained by the independent variables. It also indicates the relative contribution of 
each independent variable. Further, it allows determining the statistical signiﬁcance 
of the results, which applies to the model itself and the independent variables. The 
variance in the dependent variable (BI), explained by the model (independent vari-
ables) is 59% (adjusted R square = 0.590). The results identiﬁed PE, PC, EE and SI

User Acceptance of Contact Tracing Apps: A Study During …
547
as signiﬁcant predictors for user acceptance of CTA. The strongest predictor is PE 
(beta = 0.294, p ≤ 0.001) and PC (beta = 0.286, p ≤ 0.001), followed by EE (beta 
= 0.231, p ≤ 0.001) and SI (beta = 193, p < 0.001). Furthermore, we found that the 
remaining predictor FC, was a negative predictor and has the lowest effect on user 
acceptance (beta = −0.103, p = 0.015). 
4.4 
Smittestopp 
A large proportion of the respondents downloaded the ﬁrst version of “Smittestopp” 
(n = 146, 43.4%) compared to the latest version (n = 92, 35.7%). Through an open-
ended question in the questionnaire, those who had not downloaded “Smittestopp 
2.0” were provided with the opportunity to describe why with their own words. Of 
the entire sample (n = 258), 166 (64.3%) had not downloaded “Smittestopp 2.0”, and 
143 explained why. The largest share of respondents (n = 26, 18.2%) reported that 
they did not perceive “Smittestopp” as necessary to them, followed by apprehensions 
because the ﬁrst version was halted and the criticism it received (n = 19, 13.3%). 
Further, (n = 17, 11.9%) stated that they did not have any trust in the application or 
that it would have any effect, and (n = 16, 11.2%) reported that they forgot or did 
not know about the app. Privacy concerns was reported by (n = 13, 9.1%), and (n 
= 11, 7.7%) reported surveillance as the main inhibitor. The remaining participants 
stated that they had little knowledge about the app (n = 11, 7,7%), usability issues 
(n = 9, 6.3%), practical reasons (n = 2, 1.4%). Lastly, other reasons were reported 
by (n = 19, 13.3%). 
Smittestopp Version 1.0. To investigate differences between those who down-
loaded “Smittestopp 1.0”, and those who did not, a one-way MANOVA analysis 
was conducted. The dependent variables used were performance expectancy (PE), 
effort expectancy (EE), social inﬂuence (SI), facilitating conditions (FC), privacy 
considerations (PC) and behavioral intention (BI). A Wilks’ Lambada value = 0.768 
was obtained for the results with an associated signiﬁcance level ≤0.001. We, there-
fore, conclude that there is a statistically signiﬁcant difference between those who 
downloaded “Smittestopp 1.0” and those who did not, in terms of their perceptions 
of the dependent measures. 
Further, when the results for the dependent variables were considered separately, 
the differences to reach statistical signiﬁcance, using a Bonferroni adjusted alpha 
level of 0.008  was PE (p  ≤ 0.001, partial eta squared = 0.123), EE (p ≤ 0.001, 
partial eta squared = 0.197), SI (p = 0.006, partial eta squared = 0.030), PC (p 
≤ 0.001, partial eta squared = 0.109), and BI (p ≤ 0.001, partial eta squared = 
0.159). Results highlight that those who downloaded “Smittestopp 1.0” report signif-
icant higher levels of the variables PE, EE, SI, PC and BI. The variable with the 
largest difference was BI, where the respondents who downloaded “Smittestopp 
1.0” reported stronger acceptance (M = 4.15, SD = 1.03), than respondents that did 
not downloaded “Smittestopp 1.0” (M = 3.13, SD = 1.32).

548
I. E. Mathisen et al.
Smittestopp Version 2.0. A one-way MANOVA analysis was also applied to inves-
tigate differences between those who downloaded “Smittestopp 2.0” and those who 
did not. Our analysis gave a Wilks’ Lambada value = 0.761 and its associated signif-
icance level ≤0.001. Therefore, we conclude that there is a statistically signiﬁcant 
difference between those who downloaded “Smittestopp 2.0” and those who did not. 
When the result for the dependent variable were considered separately, the differ-
ences to reach statistical signiﬁcance, using a Bonferroni adjusted alpha level of 
0.008, was PE (p ≤ 0.001, partial eta squared = 0.135), EE (p ≤ 0.001, partial eta 
squared = 0.15.8), SI (p ≤ 0.001, partial eta squared = 0.057), PC (p ≤ 0.001, 
partial eta squared = 0.043), and BI (p ≤ 0.001, partial eta squared = 0.166). Mean 
scores reveal that those who downloaded “Smittestopp 2.0” report signiﬁcant higher 
levels of the variables PE, EE, SI, PC and BI. The variable with the largest difference 
was BI, where the respondents who downloaded “Smittestopp 2.0” reported higher 
acceptance (M = 4.40, SD = 0.902), then respondents who had not downloaded 
“Smittestopp 2.0” (M = 3.33, SD = 1.28). 
5 
Discussion 
We will discuss our results in relation to the theoretical background and based on our 
proposed model, with the goal of understanding what factors affect the acceptance 
of contact tracing applications. 
5.1 
Performance Expectancy 
The items used to measure performance expectancy in our survey addresses the 
perceived usefulness of contact tracing applications, the perceived contribution to 
society, and the perceived personal usefulness. Studies using the UTAUT model have 
indicated that the mentioned predictor will be the strongest construct to achieve user 
acceptance [9]. In a meta-analysis of the UTAUT model [10], it was found that of the 
74 studied publications, the component performance expectancy reveals the strongest 
effect size on behavior intention. This concurs with the results from our study, indi-
cating that performance expectancy is the strongest positive predictor of acceptance. 
Our analysis reveals that the relationship between performance expectancy on behav-
ioral intention is strong, with an effect size of 0.646. From these results, we were 
able to state that the link between PE and BI is the strongest among all relationship 
in our structural model. Cohen (1988) reported that the magnitude of an effect size 
is small when its value is larger than or equal to 0.10 but less than 0.30, medium 
when its value is larger than or equal to 0.30 but less than 0.50, and large when its 
value is larger than or equal to 0.50. Therefore, our results show that the more people 
perceive that a CTA is useful for tracking infection and that it is an effective tool in a

User Acceptance of Contact Tracing Apps: A Study During …
549
society during a pandemic, the more likely they are to engage in this type of digital 
tool. 
5.2 
Effort Expectancy 
Effort expectancy is deﬁned as the degree of ease an individual associate with using 
an information system or technology [9]. The items used to measure effort expectancy 
in our survey addresses perceived simplicity regarding the registration process and 
the willingness to provide and receive information from the app., as people are more 
likely to install an app with a high convenience design that involves less time and 
effort [5, 7, 8]. A poor interface and application design that requires extra time and 
effort may lead to higher effort expectancy, greater dissatisfaction, and negative user 
attitudes towards the app [5, 7, 8]. Previous meta-analysis [10] classiﬁed the relation-
ship between effort expectancy and behavioral intention as moderate. However, the 
results obtained from our study indicate that effort expectancy is a signiﬁcant positive 
predictor for behavioral intention (beta = 0.23) for CTA’s. Unlike previous studies 
[9, 10], our research suggests that the relationship between EE and BI is strong, with 
an effect size of 0.594. The difference may be due to our sample’s familiarity with 
smartphone devices and the installation of various apps for different purposes. 87.6% 
of the respondents reported good or very good experience downloading applications 
to their mobile devices. Using a smartphone appears to be standard practice for many 
of the respondents, and they may perceive using it for infection tracing as similar to 
using it for other tasks. 
6 
Conclusion 
The impact of the Covid-19 pandemic represents an extraordinary challenge to public 
health authorities and governments worldwide. To limit the spread of the virus, 
public health authorities around the world have considered and introduced contact 
tracing applications. The purpose of this study was to examine and gain a deeper 
understanding of the acceptance of CTA’s and to examine the predictors of these 
applications during the Covid-19 pandemic. Thus, this research seeks to answer the 
following research question: What are the driving factors for acceptance of contract 
tracing apps during the Covid-19 pandemic? 
To answer the research question, we tested the impact of ﬁve factors: performance 
expectancy, effort expectancy, social inﬂuence, facilitating condition, and privacy 
considerations using a sample of 258 participants. Of all the factors, performance 
expectancy has the strongest effect on the user’s intention to use a CTA. This indicates 
that the expected beneﬁts and perceived usefulness from using a CTA could increase 
the end-user’s intention to adopt these types of applications. Therefore, for the popu-
lation to accept a CTA, they must perceive that the app actually has some effect. In

550
I. E. Mathisen et al.
addition, our study indicates that privacy considerations and effort expectancy will 
have a signiﬁcant impact on user’s acceptance. People are more likely to engage in a 
CTA if they perceive that they are easy to use and have a high privacy design. Social 
inﬂuence is also a signiﬁcant positive predictor, with a moderate effect on user accep-
tance. Lastly, we found that facilitating conditions was not a signiﬁcant predictor for 
user acceptance. Overall, the results from this study indicate that our proposed model 
were able to explain 59% of the variance in behavioral intention to install and use 
a contact tracing application. Although our ﬁndings show that a majority of our 
respondents (60.9%) are willing to download a CTA if another pandemic occurs in 
the future, we argue that it is essential for policymakers to choose a strategy based 
on the driving factors tailored to the population. 
References 
1. Ahmed N et al (2020) A survey of covid-19 contact tracing apps. IEEE Access 8:134577– 
134601 
2. WHO
(2020)
Coronavirus.
https://www.who.int/health-topics/coronavirus#tab=tab_1. 
Accessed 05 Oct 2022 
3. Hinch R, et al (2020) Effective conﬁgurations of a digital contact tracing app: a report to NHSX 
4. Urbaczewski A, Lee YJ (2020) Information Technology and the pandemic: a preliminary 
multinational analysis of the impact of mobile tracking technology on the COVID-19 contagion 
control. Eur J Inf Syst 29(4):405–414 
5. Trang S, Trenz M, Weiger WH, Tarafdar M, Cheung CMK (2020) One app to trace them 
all? Examining app speciﬁcations for mass acceptance of contact-tracing apps. Eur J Inf Syst 
29(4):415–428 
6. Riemer K, Ciriello R, Peter S, Schlagwein D (2020) Digital contact-tracing adoption in the 
COVID-19 pandemic: IT governance for collective action at the societal level. Eur J Inf Syst 
29(6):731–745 
7. Grønli T-M, Ghinea G, Younas M (2014) Context-aware and automatic conﬁguration of mobile 
devices in cloud-enabled ubiquitous computing. Pers Ubiquit Comput 18:883–894 
8. Gulliver SR, Serif T, Ghinea G (2004) Pervasive and standalone computing: the perceptual 
effects of variable multimedia quality. Int J Hum Comput Stud 60(5–6):640–665 
9. Venkatesh V, Morris MG, Davis GB, Davis FD (2003) User acceptance of information 
technology: toward a uniﬁed view. MIS Q 27(3):425–478 
10. Khechine H, Lakhal S, Ndjambou P (2016) A meta-analysis of the UTAUT model: eleven years 
later. Can J Adm Sci/Revue Canadienne des Sciences de l’Administration 33(2):138–152

Digital Watermark Techniques and Its 
Embedded and Extraction Process 
Satya Narayan Das and Mrutyunjaya Panda 
Abstract The development of the digital communication system is increasing 
rapidly and all the manual data management process is massively converted to the 
digital processing system to make faster processing and reduce the valuable time 
of human users and increase productivity. Now the input and output system of the 
device is converted to a higher-level interface medium so that the device can commu-
nicate with the user and environment through audiovisual components and commu-
nicate like a human. Also in the present scenario devices are implemented by human 
society to deal with very secure and sensitive information like banking, the mili-
tary, the research environment, the medical sector, etc. Hence there is a chance of 
misutilization of information through the device. Many Security using mathematical 
techniques like cryptography, digital signature, etc. are used to make secure infor-
mation from the unauthentic person, but to protect the copywriting of the digital 
information watermark technology is used. Through the watermark, we can add the 
identiﬁcation of the owner hence the other person can not claim or tamper with the 
information for its misutilization. In this proposed work we reviewed different tech-
niques to create a watermark with the algorithm associated with it which helps the 
researcher extend the watermark technique in a better way. 
Keywords Image watermarking · DWT transform · DCT transform · Embedding 
technique
S. N. Das envelope symbol
Department of Computer Science and Engineering, GIET University, Gunupur, Odisha, India 
e-mail: sndas@giet.edu 
M. Panda 
Department of Computer Science and Applications, Utkal University, Bhubaneswar, Odisha, India 
e-mail: mrutyunjayapanda@yahoo.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_47 
551

552
S. N. Das and M. Panda
1 
Introduction 
To make information systems more user-friendly nowadays information is communi-
cated using multimedia data like audio, video, and images. Images play a very crucial 
role in the information communication system, but images can be easily tampered 
with and modiﬁed by a different user. To protect the image modiﬁcation from the 
unauthenticated user and achieve the owner’s intellectual property rights, the water-
marking system is used. Hence mostly it can be used for banknotes and copyright 
information. 
There are two types of watermarking systems. One is a traditional watermarking 
system another is a digital watermarking system. Tradition watermarking system is 
used for images only, but the digital watermarking system can be used in images, 
audio, video, 3D animation, etc. There is a lot of application of digital watermarking 
system given below 
1.1 Copyright Protection: The information can be embedded with copyright infor-
mation hence other users can not be claimed for that same and can be easily 
identiﬁed as the original owner. 
1.2 Tampering Detection: there is a concept called as Fragile watermark technique 
in which the watermark is destroyed if a person tries to tamper with the informa-
tion. Hence the information receiver can understand the information is tampered 
with by any person 
1.3 Authentication and Integrity veriﬁcation: The above two-point also explains 
that by using a watermark we can verify the authenticity of the information 
which helps to improve the integrity of the information system. 
Digital images are widely used as an information-sharing medium in the digital 
communication system. The watermark image added with the cover image is 
normally in a grayscale format in which each pixel contains 8-bit information and 
the intensity range is between 0 to 28–1 which is 0 to 255. The gray image is used 
for operation because it can be easy to compare and process with the color image. 
In the image watermarking system in the ﬁrst phase, the cover image is added to 
the watermarked image along with the secret key explained using Fig. 1. In the next 
phase, the cover image is extracted from the watermarked image pointed out in Fig. 2.
This article is further explained in the following order, the Sect. 2 explains the 
common technique used to transform the image into a matrix, the Sect. 3 explains 
the Embedding and Extraction Process performed to add a watermark in an image 
and extract the watermark from an image, the Sect. 4 analyzes the result of the 
different algorithm used by the different papers and the result and Sect. 5 contains 
the conclusion part.

Digital Watermark Techniques and Its Embedded and Extraction Process
553
Fig. 1 Watermark 
Embedding Process 
Fig. 2 Watermark 
Extraction Process
2 
Image-to-Matrix Transformation 
For making any operation with the image, the image will ﬁrst be converted to one 
frequency domain. Figure 3 indicates all the frequency domain transform methods 
used to convert images to the frequency domain. Some frequently used algorithms 
of different papers are explained below.

554
S. N. Das and M. Panda
Fig. 3 Different Frequency 
Domain used to transform 
the Image 
2.1 
Discrete Wavelet Transform 
A wavelet is a wave frequency such as oscillation. It contains amplitude and the 
signal begins at zero the graph is increased to a position and after reaching the 
increased position it again decreases back to zero just like a graph recorded by the 
heart monitor. It is used as a mathematical tool for extracting information from 
computational information like audio, image, etc. we can map all the pixels of a 
picture with a square-integrable function with certain orthonormal series created by 
a wavelet called a wavelet series. Hence a DWT is a wavelet transform in which the 
wavelets are sampled discretely. It is used for signal coding, image processing, and 
many others 
In DWT transform the signal x is calculated by crossing through the different 
stages of ﬁlters. First, the samples are crossed through a low-pass ﬁlter. Then the 
samples are decomposed through high and low-pass ﬁlters. The signals are passed 
through a low pass ﬁlter and again it is divided into two-phase using a low pass ﬁlter 
and a high pass ﬁlter. The Fig. 4 explained how the decomposition is performed as 
per the mathematical model 
y l o w le
f
t
 brac
ket n rig ht bracket equals sigma summation Underscript t equals negative normal infinity Overscript normal infinity Endscripts x left bracket t right bracket l left bracket 2 n minus t right bracket
y h i g h  l
e
f
t bra
cket n ri ght bracket equals sigma summation Underscript t equals negative normal infinity Overscript normal infinity Endscripts x left bracket t right bracket h left bracket 2 n minus t right bracket

Digital Watermark Techniques and Its Embedded and Extraction Process
555
Input Signal 
LPF
HPF 
LPF
HPF
LPF
HPF 
< 4Hz
> 4Hz 
< 2Hz
> 2Hz
< 6Hz
> 6Hz 
Signal Between 
6Hz to More 
Signal Between 
4Hz – 6Hz 
Signal Between 
2Hz – 4Hz 
Signal Between 
0 – 2Hz 
Yes
Yes
Yes
Yes 
Fig. 4 Filtering the Frequency 
2.2 
Discrete Cosine Transform 
By default an image in a special domain. Hence we can’t perform any image 
processing operation in the speciﬁed image. To make an operation image will be 
converted from a Special domain to a frequency domain. DCT is used for that purpose. 
It can be applied to both one-dimensional images as well as two-dimensional images. 
‘I’ is denoted as a special domain and F is denoted as a frequency domain. In notation 
form, it can be written as 
DCT
I
F
 
For a one-dimensional image F = c × f, Where c = Cosine Transformation Matrix, 
and f = Original image, For a two-dimensional image F = c × f × cT, where cT is 
the Transpose cosine matrix. To ﬁnd out cosine matrix (F(u)) equation is 
upper F left parenthesis u right parenthesis equals StartRoot 2 EndRoot StartFraction w left parenthesis u right parenthesis Over StartRoot n EndRoot EndFraction sigma summation Underscript x equals 0 Overscript upper N minus 1 Endscripts f left parenthesis x right parenthesis cosine left parenthesis StartFraction left parenthesis 2 x plus 1 right parenthesis Over 2 upper N EndFraction u pi right parenthesis
√
upper F left parenthesis u right parenthesis equals StartRoot 2 EndRoot StartFraction w left parenthesis u right parenthesis Over StartRoot n EndRoot EndFraction sigma summation Underscript x equals 0 Overscript upper N minus 1 Endscripts f left parenthesis x right parenthesis cosine left parenthesis StartFraction left parenthesis 2 x plus 1 right parenthesis Over 2 upper N EndFraction u pi right parenthesis
√up
per
 
F le
ft  parenthesis  u right parenthesis equals StartRoot 2 EndRoot StartFraction w left parenthesis u right parenthesis Over StartRoot n EndRoot EndFraction sigma summation Underscript x equals 0 Overscript upper N minus 1 Endscripts f left parenthesis x right parenthesis cosine left parenthesis StartFraction left parenthesis 2 x plus 1 right parenthesis Over 2 upper N EndFraction u pi right parenthesis
up
er F left parenthesis u right parenthesis equals StartRoot 2 EndRoot StartFraction w left parenthesis u right parenthesis Over StartRoot n EndRoot EndFraction sigma summation Underscript x equals 0 Overscript upper N minus 1 Endscripts f left parenthesis x right parenthesis cosine left parenthesis StartFraction left parenthesis 2 x plus 1 right parenthesis Over 2 upper N EndFraction u pi right parenthesis
f  o  r  u equals 0  c o mma  1 comma 2 ellipsis upper N minus 1
upper W h e 
r
 e w left parenthesis u right parenthesis equals StartLayout Enlarged left brace 1st Row StartFraction 1 Over square root 2 EndFraction i f u equals 0 2nd Row 1 i f u not equals 0 EndLayout
up e r  W  h e 
r e  w  left parenthesis u right parenthesis equals StartLayout Enlarged left brace 1st Row StartFraction 1 Over square root 2 EndFraction i f u equals 0 2nd Row 1 i f u not equals 0 EndLayout

556
S. N. Das and M. Panda
Table 1 Comparison of DWT, DCT, and SVD 
Considered 
Parameters 
DWT
DCT
SVD 
Complexity
More Complex
Less complex 
compare to DWT 
More complex 
Robustness
More robust to various 
attacks of image 
Robust to various 
attacks of image 
Resist against common 
attacks on image 
Applications
Science, engineering, 
mathematics and 
computer science 
Signal and Image 
processing 
Signal processing and 
statistics 
For ﬁnding out inverse function of F(u) that is f(x) is 
f left parenthesis x right parenthesis equals square root StartFraction 2 Over upper N EndFraction sigma summation Underscript u equals 0 Overscript upper N minus 1 Endscripts w left parenthesis u right parenthesis times upper F left parenthesis u right parenthesis times cosine left parenthesis StartFraction 2 x plus 1 Over 2 upper N EndFraction times u pi right parenthesis
f 
lef
t
 par
enthesis x right 
pare nthesis equals square root StartFraction 2 Over upper N EndFraction sigma summation Underscript u equals 0 Overscript upper N minus 1 Endscripts w left parenthesis u right parenthesis times upper F left parenthesis u right parenthesis times cosine left parenthesis StartFraction 2 x plus 1 Over 2 upper N EndFraction times u pi right parenthesis
f l
eft 
parenthesis x right parenthesis equals square root StartFraction 2 Over upper N EndFraction sigma summation Underscript u equals 0 Overscript upper N minus 1 Endscripts w left parenthesis u right parenthesis times upper F left parenthesis u right parenthesis times cosine left parenthesis StartFraction 2 x plus 1 Over 2 upper N EndFraction times u pi right parenthesis
f  o  r  x equals 0  c o m ma 1 comma 2 comma ellipsis comma upper N minus 1
upper W h e 
r
 e w left parenthesis u right parenthesis equals StartLayout Enlarged left brace 1st Row StartFraction 1 Over square root 2 EndFraction i f u equals 0 2nd Row 1 i f u not equals 0 EndLayout
up e r  W  h e 
r e  w  left parenthesis u right parenthesis equals StartLayout Enlarged left brace 1st Row StartFraction 1 Over square root 2 EndFraction i f u equals 0 2nd Row 1 i f u not equals 0 EndLayout
2.3 
Singular Value Decomposition (SVD) 
SVD is a numerical analysis tool that is used to analyze the matrices. In SVD trans-
formation, the matrix can be decomposed into three matrices which are of the same 
size as the original matrix and preserve both one-way and non-symmetric properties. 
When SVD is used in the digital image the size of the matrices is not ﬁxed. Singular 
values in digital images are less affected if general image processing is performed 
and singular values contain intrinsic algebraic image properties. The singular values 
of the host image are modiﬁed to embed the watermark image by employing multiple 
singular functions. Table 1 represents the comparison of DWT, DCT, and SVD.

Digital Watermark Techniques and Its Embedded and Extraction Process
557
3 
Embedding and Extraction Process 
The important procedure in the watermarking system is the embedding and extrac-
tion process. In the embedding procedure, the cover image is concatenated with a 
watermark image. In the process of extraction, the watermark image is extracted 
from the cover image and it is reconstructed again for improvement in its intensity. 
The embedding process and extraction process are explained using Figs. 5 and 6. 
The detailed procedure is discussed below. 
Fig. 5 Watermark Embedding Process 
Fig. 6 Watermark Extraction Process

558
S. N. Das and M. Panda
Fig. 7 Embedding and Encryption algorithm used for Images 
In the embedding procedure, the selected cover image of 512 × 512 resolution 
is converted to a grayscale image. Grayscale image information is a 2D matrix and 
values vary from 0 to 255 as per the intensity of the color. The white color is treated 
as a 0 intensity and the black color is treated as intensity of 255. Select the added 
watermark image of the same size as the covered image. Apply three levels of DWT 
decompositions of both pictures with the ‘k’ scaling factor and embedded both cover 
and watermark pictures. 
The resultant matrix is processed through the Inverse Discrete Wavelet Transform 
(IDWT) for getting the ﬁnal watermarked image. 
Get the watermarked image, and watermark image as an input for the extraction 
process. Processed it through DWT algorithm up to three-level. Use scaling factor to 
recover the image and process through IDWT for getting the cover image ﬁltered from 
the watermarked image. Figure 7 shows some embedded and encryption algorithms 
for watermarked image processing. 
4 
Results 
We go through some different watermark implemented papers. Table 2 provides the 
information regarding the algorithm used by the paper and what results the author 
ﬁnd out by implementing those algorithms.

Digital Watermark Techniques and Its Embedded and Extraction Process
559
Table 2 Comparative analysis of the algorithms 
S. 
No. 
Reference
Procedure 
used 
Images Type/Size
Result 
1
Singh et al. [13], 
2014 
DWT, SVM 
Greyscale images of 
Goldhill, Mandrill, and 
Peppers (512 × 512)/ 
logo image (64 × 64) 
PSNR = 45.94, NC = 0.961 
2
B. Jagadeesh 
et al. [14], 
2014 
DWT, SVM 
Greyscale images of 
Goldhill, Mandrill, and 
Peppers (512 × 512)/ 
logo image (64 × 64) 
PSNR = 45.94, NC = 0.961 
3
Yahya et al. [15], 
2015 
DCT, SVM
Lena, Baboon of size 
1024 bits/logo image 
PSNR = 49.86, 
NC = 1.0 
4
Mohindru et al. 
[16], 2016 
DWT, 
Fuzzy logic, 
and neural 
network 
Greyscale image of 
Lena, Airplane (512 × 
512)/logo image 
PSNR = 47.96, 
NC = 0.94 
5
Nerurkar et al. 
[6], 2018 
DWT, DCT, 
SVT 
Fireﬂy 
Algorithm 
(FA) 
Carrier Image 512 × 
512 
Hidden image 256 × 
256 
Extracted Carrier 
Image 
Attacks DWT SSIM/ DCT 
SSIM 
Salt and pepper 0.8414, 
0.7385 
Gaussian noise 0.7356, 
0.6786 
Sharpening 0.6490, 0.6233 
Speckle noise 0.7953, 
0.6027 
6
Chaughule et al. 
[1], 2019 
DWT, 
Arnold’s 
Transform 
Carrier image 256 × 56 
Hidden image 128 × 
128 
Extracted carrier image 
the PSNR value of the 
extracted and corrected 
hidden image remains above 
30 dB at 50% pixel 
tampering/error 
7
Ernawan et al. 
[4], 2021 
DWT, DCT, 
Arnold’s 
Transform, 
Carrier image 512 × 
512 
Hidden image 32 × 32 
Extracted carrier image 
PSNR value of 47 dB, 
SSIM value of about 0.987 
5 
Conclusion 
In the proposed paper we explain the different transform techniques used for adding a 
watermark in an image which helps to protect the owner’s intellectual property rights. 
Apart from that, the paper contains one comparative analysis of the algorithm as well 
as it also contains the result speciﬁed by different authors using that algorithm. In this 
paper, we intend to give complete technical information regarding the embedding 
and extraction of the watermark which helps the researcher a lot to get maximum 
knowledge of this concept.

560
S. N. Das and M. Panda
References 
1. Chaughule SS, Megherbi DB (2019) A robust, non-blind high capacity & secure digital 
watermarking scheme for image secret information, authentication, and tampering localiza-
tion and recovery via the discrete wavelet transform. In: 2019 IEEE international symposium 
on technologies for homeland security (HST), pp 1–5. IEEE 
2. Singh P, Chadha RS (2013) A survey of digital watermarking techniques, applications, and 
attacks. Int J Eng Innov Technol (IJEIT) 2(9):165–175 
3. Lee CF, Shen JJ, Chen ZR (2018) A survey of watermarking-based authentication for the 
digital image. In: 2018 3rd international conference on computer and communication systems 
(ICCCS), pp 207–211. IEEE 
4. Ernawan F, Ariatmanto D, Firdaus A (2021) An improved image watermarking by modifying 
selected DWT-DCT coefﬁcients. IEEE Access 9:45474–45485 
5. Yadav AS, Kumar S (2018) Comparative analysis of digital image watermarking based on 
DCT, DWT, and SVD with image scrambling technique for information security. In: 2018 
international conference on computational and characterization techniques in engineering & 
sciences (CCTES), pp 89–93. IEEE 
6. Nerurkar PP, Phadke AC (2018) Digital image watermarking using ﬁreﬂy algorithm. In: 
2018 fourth international conference on computing communication control and automation 
(ICCUBEA), pp 1–5. IEEE 
7. Evsutin OO, Melman AS, Meshcheryakov RV (2020) Digital steganography and watermarking 
for digital images: a review of current research directions. IEEE Access 
8. Yuan G, Hao Q (2020) Digital watermarking secure scheme for remote sensing image 
protection. China Commun 17(4):88–98 
9. Sharma S, Singh AK, Kumar P, Pradesh-India H (2016) digital image watermarking using 
machine learning techniques: a technical review. GRENZE Int J Comput Theory Eng 2(3):1–6 
10. Abdulla NB, Navas KA (2020) High-security watermarking techniques for digital rights 
management: a review. In: 2020 international conference on communication and signal 
processing (ICCSP), pp 162–166. IEEE 
11. Zainol Z, Teh JS, Alawida M, Alabdulatif A (2021) Hybrid SVD-based image watermarking 
schemes: a review. IEEE Access 9:32931–32968 
12. Pal P, Singh HV, Verma SK (2018) Study on watermarking techniques in digital images. In: 2018 
2nd international conference on trends in electronics and informatics (ICOEI), pp 372–376. 
IEEE 
13. Singh AK, Dave M, Mohan A (2014) Wavelet-based image watermarking: futuristic concepts 
in information security. Proc Natl Acad Sci India Sect A Phy Sci 84(3):345–359 
14. Jagadeesh B, Rajesh Kumar P, Chenna Reddy P (2013) Robust digital image watermarking 
scheme in discrete wavelet transform domain using support vector machine. Int J Comput Appl 
73(14) 
15. Yahya S, Hussain HS, Ali FHM (2015) DCT Domain Stega SVMshifted LSB Model for 
highly Imperceptible and robust cover image. In: International conference on computing and 
informatics, vol 43 
16. Mohindru P, Gill MS, Pooja (2014) A new image fusion algorithm based on wavelet transform 
and adaptive neuro-fuzzy logic approach. Int J Adv Res Electr Electron Instrum Eng 3(8)

Galvanic Skin Response-Based Mental 
Stress Identiﬁcation Using Machine 
Learning 
Padmini Sethi, Ramesh K. Sahoo, Ashima Rout, and M. Mufti 
Abstract Stress is vital in assessing the physical and mental state of the human 
body with signiﬁcant psychological and physiological changes. A proper and timely 
diagnosis of stress may make one healthier, happier, and more productive. In the 
workplace, undergoing many changes leads to stress, trauma, and anxiety. At the 
same time, hormonal changes in the human body due to stress can be reﬂected in terms 
of psychological and physiological changes. This paper has identiﬁed three different 
activities (normal, tension, and exercise) with varied positions (laying, sitting, and 
standing). Airﬂow, Temperature, and Galvanic Skin Response (GSR) are different 
sensors that sense data. This work has emphasized GSR sensors and conceptually 
connected them with other sensors. GSR values differ regarding the contact surface 
area with the body. Different machine learning algorithms such as; Naive Bayes, 
Support Vector Machine, Decision Tree (J48), and Random Forest have been used 
to analyze sensed datasets. Random Forest Algorithm has been observed to perform 
better in the proposed work. 
Keywords Physiological data · Psychological data · GSR Sensor · Machine 
learning
P. Sethi 
RDW University, Bhubaneswar, India 
R. K. Sahoo envelope symbol · A. Rout 
IGIT Sarang, Dhenkanal, Odisha, India 
e-mail: ramesh0986@gmail.com 
M. Mufti 
Nottingham University, Nottingham, UK 
e-mail: mufti.mahmud@ntu.ac.uk 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_48 
561

562
P. Sethi et al.
1 
Introduction 
Mental stress is one of the human body’s responses, which is reﬂected in anxiety, 
anger, depression, and fear. Different forms of situations/events can create stress 
levels in the human body. The human body sue generates stress hormones to stress 
in terms of psychological sweating, mostly observed in the face, palm, and soles 
of the human body [1]. It has three types of sweat glands with different functions. 
One of the glands named the eccrine gland present in the palm of the human body, 
and this is linked with emotional anxiety and stress [2]. So, the physiological and 
psychological activities of the human body generate critical and serious hazards that 
affect humans physically and mentally. Therefore, it is highly essential to observe 
and analyze the stress on the human body consistently. 
Stress turns to physical or mental sicknesses such as; tiredness, depression, and 
anxiety. Physiological changes make a signiﬁcant impact on the mental sickness of 
the human body. This may be connected with different situations with age groups that 
can be reﬂected through their behavior, performance, thinking ability, etc. [3]. Due 
to these variations, the resultant stress increases sweat gland activity. It happens due 
to the interrelation between the human body and the mind [4]. For example, physical 
sickness of the human body may turn into anxiety that affects the health conditions 
of humans in a negative way. Therefore, physiological data may be considered, and 
it is better suitable for proper analysis of mental stress. It can be used to predict the 
person’s stress level and its extent as per the current activity of a person. The stress 
level of the human body can be estimated and observed continuously through skin 
conductance, and it has been discussed experimentally by the authors in [5]. 
Heavy ﬂuids with the eccrine gland increase the skin conductance, and it is 
observed on the body part (face, palm, and sole) as sweat. So in the experiment, 
the conductance may be monitored and estimated by placing the two electrodes on 
the two ﬁngers of the human body. GSR is vital in monitoring the human body’s phys-
ical and mental stress. Sweat gland activity can be tracked through Skin conductance 
and GSR. It can be considered as an indication of psychological or physiological 
stimulation and its electrodermal response. 
2 
Background 
Nowadays, GSR sensor-based technology is gaining popularity as a wearable device 
for the detection of emotion and the day-to-day activity of humans. Various device 
has been discussed in [6] for emotion detection using a GSR sensor. Due to anxiety/ 
physical activity/stress situations, sweat gland activity in the neural system of the 
human body increases, that in turn increases the electrical characteristics of the human 
body, and it is measured using the GSR sensor by providing a constant 5-volt voltage 
to the human body in order to determine skin conductance. Therefore mental stress

Galvanic Skin Response-Based Mental Stress Identiﬁcation ...
563
is directly proportional to skin conductance [6, 7]. It is used for the early detection 
of stress levels in the human body, which will improve the quality of life. 
A device has been developed [8] using an Arduino Uno microcontroller, GSR 
Sensor, and e-Health sensor shield to detect the mental stress of a person. It works 
in ofﬂine mode, where data has been stored locally in the laptop/desktop through 
which the device was attached. GSR sensor has been used to detect skin conduc-
tance of the human body, and it reﬂects the mental stress of the person. Further, 
this work has been extended by authors in [9] by adding remote data collection in 
the Wi-Fi network through Arduino-based devices like Wi-Fi and GSM shields in 
which health data will be collected from the person and transmitted to a remote 
server for analysis. Authors in [10] discussed a data collection framework for data 
collection using various sensors attached to the human body automatically. Further, 
this sensed data has been transmitted to a sink node/remote server for analysis in 
a heterogeneous wireless network environment. Authors in [11] presented mental 
stress detection model in which mental stress has been estimated from the human 
body using skin conductance of human body sensed using GSR sensor. Due to the 
constant transmission of 5-volt voltage for the estimation of skin conductance, some 
noise has been added to the sensed data. Various methods have been discussed in 
[11] to eliminate noise from sensed data so that performance will be better and the 
accuracy of the model for mental stress detection has been enhanced. 
Personalization methods based on the decision tree algorithm have been discussed 
in [12] to detect wireless motion and the PDA for human activity recognition. Health 
data obtained using the GSR sensor and Heartbeat sensor can be analyzed using fuzzy 
logic for the detection of stress patterns in the human body [13]. In the proposed work, 
mental stress has been analyzed through a different machine learning algorithm. 
3 
Methodology 
In this section, experimental work with data and collection framework has been 
discussed. 
3.1 
Data Collection Framework 
Various components used for data collection and data communication in wireless 
mode are Arduino Uno microcontroller, GSR Sensor, temperature sensor, airﬂow 
sensor, e-Health sensor platform for data collection, GSM Shiels, and Wi-Fi shield 
for data communication. 
The temperature sensor is used to detect body temperature in degree centigrade, 
and the airﬂow sensor is used to detect the breathing rate of a person. Both these 
sensors are used to detect abnormal conditions that may be high breathing rates or 
low breathing rates.

564
P. Sethi et al.
GSR sensor is used to detect the skin conductance of a person that reﬂects the 
stress level of the person. It has been placed in the middle and index ﬁnger of our 
plan. It provides a 5-volt constant voltage to our body. A minor amount of current 
always ﬂows through our body, and it is regulated by our neural system. By using 
Constant voltage and current following through our body, skin conductance has 
been evaluated by the GSR sensor. Whenever the stress level of a person increases, 
sweat gland activity of our neural system increases, that in turn increases the amount 
of current following through the human body. In the case of the GSR sensor, a 
constant 5 V voltage has been provided; therefore, skin conductance depends on the 
current following through the body. Therefore, the Stress level of a person is directly 
proportional to skin conductance. 
GSR sensor, Temperature sensor, and airﬂow sensor will be attached to the e-health 
sensor platform. The E-health sensor platform is a motherboard that allows different 
sensors like a heartbeat, Glucose, Blood pressure, temperature, GSR, ECG, EMG, 
airﬂow, etc., to collect vital health data from the human body that will be analyzed 
further to detect actual health condition of human. This e-Health sensor platform 
will be attached to an Arduino Uno microcontroller. Arduino Uno is a widely used 
microcontroller in various IoT applications. It can be easily attached to any laptop 
or desktop using a USB cable to write and upload an Arduino sketch or program to 
control various attached sensors/devices. It also receives data from sensors/devices 
and stores it locally in the attached Laptop or desktop for further analysis. It also gets 
the required power for operation through the attached PC using a USB cable. For 
data communication, Wi-Fi Shield and GSM shield will be attached with Arduino 
Uno in wireless mode. Wi-Fi shields can detect available Wi-Fi networks in the 
desired location and connect with the desired Wi-Fi network for data communication 
automatically without human intervention. GSM shield is used to transmit data using 
the GSM network, and it is also attached to Arduino Uno. 
3.2 
Working Principle 
GSR, temperature, and airﬂow sensor has been used in the proposed work. All these 
sensors have been attached to the e-Health sensor platform for data collection from 
the human body. Further eHealth sensor platform will be attached to an Arduino Uno 
microcontroller. GSM and Wi-Fi both the shields have been attached with Arduino 
microcontroller to provide a heterogeneous data communication mechanism using 
both Wi-Fi and GSM as per the availability. Finally, the Arduino Uno microcontroller 
has been attached to a laptop to power up the device and also to upload the proposed 
Arduino sketch/program to the microcontroller. As per the program, the control 
signal will be transmitted to the sensor to collect vital health parameters from the 
human body, as reﬂected in Fig. 1(a). Further, this sensed data will be stored locally in 
comma-separated value (CSV) format in a text ﬁle on the micro sd card attached to an 
Arduino Wi-Fi shield. Finally, the stored data will be transmitted to the cloud platform 
for storage and analysis using either GSM Shield or Wi-Fi shield in a dynamic

Galvanic Skin Response-Based Mental Stress Identiﬁcation ...
565
 (b): Observing Node 
(a): Data acquisition Node                                    
Fig. 1 Data acquisition and observing Node setup [9] 
manner. If a reliable Wi-Fi network is available in the desired location, then data will 
be transmitted to the cloud platform using an Arduino Wi-Fi shield; otherwise, the 
sensed data will be transmitted to the cloud platform using a GSM network through 
a GSM shield attached to the microcontroller as reﬂected in Fig. 1(b).
Vital health data has been sensed and gathered from different persons with three 
different mental conditions Tension, Exercise, and Normal, with different positions 
like Standing, Laying, and Sitting. Further, this dataset has been analyzed using 
various machine learning (ML) algorithms to ﬁnd a person’s position and state of 
mind from the obtained health data. 
3.3 
Analysis Through Machine Learning Algorithms 
The obtained dataset has been analyzed using various machine learning (ML) algo-
rithms in the WEKA environment. In the proposed work, well-renowned supervised 
machine learning algorithms such as Naive Bayes [14], SVM [14], Decision Tree 
(J48) [14], and Random Forest [14] algorithms have been used. Full training set 
mode has been used to classify the dataset using various ML algorithms in which 
the dataset will be divided into training datasets only once for classiﬁcation using 
various ML algorithms in the WEKA environment. Further, the various algorithms 
have been compared to estimate the best ML algorithm for the proposed work. Various 
parameters like True Positive (TP) Rate, False Positive (FP) rate, Precision, Recall, 
F-Measure, Precision-Recall Curve (PRC), Mean absolute error (MAE), Relative

566
P. Sethi et al.
absolute error (RAE), root relative squared error (RRSE), root mean squared error 
(RMSE), the total number of correctly classiﬁed instances, the total number of incor-
rectly classiﬁed instances, kappa statistics, etc., has been used to evaluate various 
ML algorithms considered for analysis of proposed work. 
4 
Result Analysis 
Figure 2 and Table 1 provide a comparative view of the total number of actual 
records and a total number of predicted records in various categories. In the proposed 
work, temperature, stress level, and airﬂow data have been recorded from various 
users using Temperature, GSR, and Airﬂow sensors in various positions and moods 
like Stand_Tension reﬂects data collected in standing position with tension mood, 
Stand_Normal reﬂects data collected in standing position with normal mood, Stand_ 
Exercise reﬂects data collected in standing position with exercise mood, Sit_Tension 
reﬂects data collected in sitting position with tension mood, Sit_Normal reﬂects data 
collected in sitting position with normal mood, Sit_Exercise reﬂects data collected 
in sitting position with exercise mood, Lay_Tension reﬂects data collected in laying 
position with tension mood, Lay_Normal reﬂects data collected in laying position 
with normal mood, and Lay_Exercise reﬂects data collected in laying position with 
exercise mood. From Table 1, it is observed that total of 10,361 records have been 
recorded from users in different position and moods. 
In the Stand_Tension category, the actual number of records obtained is 1168 
but after classiﬁcation, the total no. of records predicted as Stand_Tension in Naive 
Bayes, SVM, Decision Tree (J48), and Random Forest algorithms are 2011, 1201, 
2449, and 1238 respectively. Results provided by SVM and Random Forest algo-
rithms are closer to actual records. In the case of the Stand_Norma category, an actual 
number of records recorded is 1168 but after classiﬁcation, the total no. of records 
predicted as Stand_Normal in Naive Bayes, SVM, Decision Tree (J48), and Random 
Forest algorithms are 337, 1282, 0 and 1099 respectively. Results provided by SVM 
and Random Forest algorithms are closer to actual records. Similarly, for the Stand_ 
Exercise category, the total number of actual records is 1201 but after classiﬁcation, 
the total no. of records predicted as Stand_Exercise in Naive Bayes, SVM, Decision 
Tree (J48), and Random Forest algorithms are 46, 838, 959 and 1208 respectively.
Fig. 2 Comparative analysis of actual and predicted no. of records in various categories

Galvanic Skin Response-Based Mental Stress Identiﬁcation ...
567
Table 1 Statistical data 
Class
Actual Quantity
Naive Bayes
SVM
Decision Tree 
(J48) 
Random Forest 
Stand Tension
1168
2011
1201
2449
1238 
Stand Normal
1168
337
1282
0
1099 
Stand Exercise
1201
46
838
959
1208 
Sit Tension
1164
2357
1332
1201
1165 
Sit Normal
1098
1500
1121
1100
1098 
Sit Exercise
1130
1122
1099
1186
1129 
Lay Tension
1148
72
917
1079
1143 
Lay Normal
1120
1312
1145
1130
1121 
Lay Exercise
1164
1604
1426
1257
1160 
Total Record
10,361
10,361
10,361
10,361
10,361
Results provided by Decision Tree (J48) and Random Forest algorithms are closer 
to actual records. 
The actual number of records for the Sit_Tension category is 1164 and the total 
number of predicted records as Sit_Tension using Naive Bayes, SVM, Decision Tree 
(J48), and Random Forest algorithms are 2357, 1332, 1201, and 1165 respectively. 
Hence SVM and Random forest algorithms have better results than others for the 
Sit_Tension category. The actual number of records of the Sit_Normal category is 
1098 and the total number of predicted records as Sit_Normal using Naive Bayes, 
SVM, Decision Tree (J48), and Random Forest algorithms are 1500, 1121, 1100, 
and 1098 respectively. Hence Decision Tree (J48) and Random forest algorithms 
have a better result than others for the Sit_Normal category. The actual number 
of records for the Sit_Exercise category is 1130 and the total number of predicted 
records as Sit_Exercise using Naive Bayes, SVM, Decision Tree (J48), and Random 
Forest algorithms are 1122, 1099, 1186, and 1129 respectively. Hence Naive Bayes 
and Random forest algorithms have better results than others for the Sit_Exercise 
category. 
In the Laying_Tension category, the total number of actual records is 1148 and 
the total number of predicted records as Laying_Tension using Naive Bayes, SVM, 
Decision Tree (J48), and Random Forest algorithms are 72, 917, 1079, and 1143 
respectively. Only Random Forest algorithms provide a better result for the Laying_ 
Tension category. Similarly in the Laying_Normal category, the total number of 
actual records is 1120 and the total number of predicted records as Laying_Normal 
using Naive Bayes, SVM, Decision Tree (J48), and Random Forest algorithms are 
1312, 1145, 1130, and 1121 respectively. Therefore, Decision Tree (J48) and Random 
Forest algorithms provide a better result for the Laying_Normal category. Finally, 
in the Laying_Exercise category, the total number of actual records is 1164 and the 
total number of predicted records as Laying_Exercise using Naive Bayes, SVM, 
Decision Tree (J48), and Random Forest algorithms are 1604, 1426, 1257, and

568
P. Sethi et al.
Fig. 3 Error report of various machine learning algorithms 
1160 respectively. Only Random Forest algorithms provide a better result for the 
Laying_Exercise category. Random Forest algorithms better result in all categories 
as observed in Fig. 1 and the statistical data available in Table 1. 
Figure 3 provides a comparative view of error reports after classiﬁcation using 
various machine learning algorithms such as Naive Bayes, SVM, Decision Tree (J48), 
and Random Forest. It is observed that the kappa statistic is maximum whereas, 
Mean absolute error and Root mean squared error is minimum in the Random Forest 
algorithm than others. Therefore random forest algorithm provides better accuracy 
for the proposed work. 
Figure 4 provides a comparative view of the classiﬁcation reports after classiﬁ-
cation using various machine learning algorithms such as Naive Bayes, SVM, Deci-
sion Tree (J48), and Random Forest. TP Rate, FP Rate, Precision, Recall, F-Measure, 
MCC, ROC area, and PRC area are the parameters used to evaluate machine learning 
algorithms. It is observed that TP Rate, Precision, Recall, F-Measure, MCC, ROC 
area, and PRC area are maximum, whereas FP Rate is minimum in the Random 
Forest algorithm than others. Therefore, as per the classiﬁcation report, the Random 
Forest algorithm performs better than others. 
Figure 5 provides a comparative view of the Accuracy report after classiﬁcation 
using various machine learning algorithms such as Naive Bayes, SVM, Decision 
Tree (J48), and Random Forest. It is observed that the number of correctly classiﬁed 
instances is maximum whereas, incorrectly classiﬁed instances, Relative absolute 
error, and root relative squared error is minimum in the Random Forest algorithm 
than others. Therefore Random forest algorithms provide maximum accuracy that is 
88.51%, whereas the accuracy level achieved by Naive Bayes, SVM, and Decision 
Tree (J48) are 42.28%, 73.06%, and 80.18%, respectively.
Fig. 4 Classiﬁcation report of various machine learning algorithms 

Galvanic Skin Response-Based Mental Stress Identiﬁcation ...
569
Fig. 5 Accuracy report of various machine learning algorithms 
It is observed that the performance of the Random Forest algorithm is better than 
Decision Tree (J48), SVM, and Naive Bayes algorithms for the proposed work, as 
reﬂected in Figs. 2, 3, 4, and 5. 
5 
Conclusions and Future Scope 
Mental stress has been estimated through GSR value, body temperature, and 
breathing rate from the human body using various sensors in various positions like 
standing, sitting, and lying with various moods like tension, exercise, and normal. 
Various machine learning algorithms have been used for attaining the models, and 
the best algorithm has been selected after proper evaluation. Further, the selected 
algorithm may be used for the estimation of the position and mental state of a person 
using their health record. It has been observed that the Random Forest Algorithm 
performs better in comparison with NB, SVM, and Decision tree. In the Future, we 
will try to enhance the accuracy of the ML algorithm. 
Acknowledgements The authors are thanking to Prof. Srinivas Sethi Principal Investigator, of 
SERB (DST) sponsored Project to carry out this work related to Stress Analysis in wireless sensor 
network environment at CSEA Department, IGIT Sarang. 
References 
1. Harker M (2013) Psychological sweating: a systematic review focused on aetiology and 
cutaneous response. Skin Pharmacol Physiol 26:92–100. https://doi.org/10.1159/000346930 
2. Asahina M, Poudel A, Hirano S (2015) Sweating on the palm and sole: physiological and 
clinical relevance. Clin Auton Res 25:153–159. https://doi.org/10.1007/S10286-015-0282-1 
3. Glanz K, Schwartz M (2008) Stress, coping, and health behavior. In Health behavior and health 
education: theory, research, and practice, pp 211–236 
4. Vijaya PA, Shivakumar G (2013) Galvanic skin response: a physiological sensor system for 
affective computing. Int J Mach Learn Comput (Springer) 3:1

570
P. Sethi et al.
5. Navea RF, Buenvenida PJ, Cruz CD (2019) Stress detection using Galvanic skin response: an 
Android application. J Phys Conf Ser 1372:012001 (2019). Iop Publishing, https://doi.org/10. 
1088/1742-6596/1372/1/012001 
6. Sanchez-Comas A et al (2021) Correlation analysis of different measurement places of Galvanic 
skin response in test groups facing pleasant and unpleasant stimuli. Sensors 21(12):4210. 
https://doi.org/10.3390/S21124210 
7. Fenz WD, Epstein S (1967) Gradients of a physiological arousal of experienced and novice 
parachutists as a function of an approaching jump. Psychomatic Med 29:33–51 
8. Sahoo R, Sethi S (2015) Functional analysis of mental stress based on physiological data of GSR 
sensor. In: Emerging ICT for bridging the future-proceedings of the 49th annual convention of 
the computer society of India (CSI), vol 1. Springer, Cham 
9. Sahoo R, Sethi S (2015) Remotely functional-analysis of mental stress based on GSR sensor 
physiological data in wireless environment. In: Information systems design and intelligent 
applications. Springer, New Delhi, pp 569–577 
10. Sethi S, Sahoo RK (2020) Design of WSN in real time application of health monitoring system. 
In: Virtual and mobile healthcare: breakthroughs in research and practice. IGI Global, pp 
643–658 
11. Sahoo RK et al (2022) Mental stress detection using GSR sensor data with ﬁltering methods. 
In: Intelligent systems. Springer, Singapore, pp 537–548 
12. Juha P, Luc C, Miikka E (2010) Personalization algorithm for real-time activity recognition 
using PDA, Wireless Motion Bands, and Binary Decision Tree. IEEE Trans Inf Technol Biomed 
14(5):1211–1215 
13. De Santos Sierra A, Avila CS, Del Pozo GB, Guerra Casanova J (2011) Stress detection by 
means of stress physiological template. Nat Biol Inspired Comput (Nabıc) 131–136 
14. Eligüzel N, Çetinkaya C, Dereli T (2020) Comparison of different machine learning techniques 
on location extraction by utilizing geo-tagged tweets: a case study. Adv Eng Inform 46:101151

A Federated Learning Based Connected 
Vehicular Framework for Smart Health 
Care 
Biswa Ranjan Senapati 
, Sipra Swain 
, Rakesh Ranjan Swain 
, 
and Pabitra Mohan Khilar 
Abstract Data privacy and data security are the main concerns in the digital era. 
The 3.5% centralized increase in annual digital data and the use of machine learning 
and deep learning approaches in the centralized computing environment endanger 
data privacy and security. The evolution of various body sensors also increases the 
digital health parameter data, which also demands privacy and security, which are 
difﬁcult to achieve in a centralized computing environment. In the article, a two-
level VANET-based federated learning framework is proposed for the classiﬁcation 
of health parameters, in which Road Side Unit (RSU) acts as the local server in the 
ﬁrst level and cloud networks act as the remote server in the second level. Health 
parameters considered for the proposed work are body temperature, heart rate, and 
systolic and diastolic blood pressure. The proposed work classiﬁes health parameters 
into four categories, such as normal, low-risk, medium-risk, and high-risk data. Accu-
racy, precision, recall, and loss are used to evaluate the proposed work. In addition, 
in terms of average false classiﬁcation rate and multi-class classiﬁcation accuracy, 
the proposed federated learning computing is compared to centrally managed com-
puting. 
Keywords Cloud network · Federated learning · V2X · VANET 
B. R. Senapati 
Institute of Technical Education and Research, Siksha ‘O’ Anusandhan University, Bhubaneswar, 
Odisha, India 
S. Swain (B) · P. M. Khilar 
National Institute of Technology, Rourkela, Rourkela, Odisha, India 
e-mail: swainsipra86@gmail.com 
P. M. Khilar 
e-mail: pmkhilar@nitrkl.ac.in 
R. R. Swain 
CSE Department, IIT Kanpur, Kanpur, India 
Present Address: 
R. R. Swain 
Institute of Technical Education and Research, Siksha ‘O’ Anusandhan (Deemed to be) 
University, Bhubaneswar, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_49 
571

572
B. R. Senapati et al.
1 
Introduction 
Technological advancement in the communication process, the availability of sensors 
to measure the health data at an affordable cost, and the ease of various machine 
learning (ML) and deep learning (DL) approaches to classify the disease all increase 
the demand for ML and DL in the health care industry [ 14]. However, manually 
collecting health data at the health centre for infectious diseases puts health workers 
at risk of infection. The manual and periodic supply of health services by specialists, 
nurses, and other paramedical professionals causes health warriors to worry about 
contracting illnesses. For example, many health workers lost their lives during the 
infectious disease COVID-19 [ 1]. This motivates the automatic transmission of health 
parameters from the home to the observation centre. For such purposes, the vehicular 
network plays a crucial role in the transmission of health data to the observation centre 
[ 19]. Vehicular network is an ad hoc network where the communication of a vehicle 
in the network is called vehicle to everything (V2X) [ 8, 11, 17, 20]. The scenario 
for V2X communication is presented in Fig. 1. In Fig.  1, V2V refers to vehicle-
to-vehicle communications, V2I refers to vehicle-to-infrastructure communications, 
V2N refers to vehicle-to-network communications, and V2P refers to vehicle-to-
pedestrian communications. The use of standards like wireless access in vehicular 
environments (WAVE) and dedicated short-range communication (DSRC) [ 6], and 
support for 6G enabled in VANET [ 4], and efﬁcient use of MAC layer 802.11p 
[ 2] support the transmission of health data through VANET. Even though data is 
transmitted to the observation centre through VANET, the classiﬁcation of health 
data through ML and DL at the observation centre is a centralised approach. The 
centralised approach to health data classiﬁcation at the observation centre has several 
limitations. These include limited storage, compute capability, a limited number of 
resource people to complete the task, vulnerability to various health data attacks, 
security issues, and so on. To address such issues, we go for the Federated Vehicular 
Network (FVN), which classiﬁes the health parameters automatically using a two-
level server architecture. The following are the manuscript’s main contributions: 
1. Use of a two-level server, i.e., a local server and a remote server, to automatically 
classify health parameters for infectious diseases. 
2. Provision of Global federated model from the remote server (cloud network) to 
the local server Road Side Unit (RSU). 
3. Evaluation of the proposed federated model in terms of accuracy, loss, precision, 
recall, False classiﬁcation rate. 
Thereafter, the document is structured as follows: In Sect. 2, we analyse the rel-
evant literature. In Sect. 3, we discuss the proposed approach. Lastly, in Sect. 4 and 
Sect. 5, respectively, the results obtained and the conclusion of the work are outlined.

A Federated Learning Based Connected Vehicular Framework.. . .
573 
Fig. 1 V2X Communication scenario for VANET 
2 
Literature Study 
This section addresses two factors. The ﬁrst aspect discusses the work done by 
researchers on the computational capability of RSU, which motivates its use in the 
health care industry. The second aspect discusses the role of federated learning for 
various applications. 
2.1 
Computational Capability of RSU 
The popularity of VANET has been raised because of the successful implementation 
of various applications and the efﬁcient computation by RSU. If multiple paths exist 
at the RSU, the meta-heuristic algorithm Ant Colony Optimization (ACO) is used 
to ﬁnd the optimal route from a junction to the destination location [ 9]. RSU plays 
a signiﬁcant role in monitoring the various aspects of environment (temperature, 
humidity, carbon monoxide, and methane) in the rural areas [ 10, 18]. Using the 
camera module in the RSU assists in automatically determining the available parking 
slot and is considered a smart city convenience application [ 15]. The determination 
of toll tax value by RSU contributes to the reduction of delays during transportation 
[ 16]. Looking at the above computational capability of RSU motivates the use of 
RSU for the classiﬁcation of health parameters. The next subsection discusses the 
use of federated learning in various applications.

574
B. R. Senapati et al.
2.2 
Various Applications of Federated Learning 
In 2016, Google was the ﬁrst to propose the idea of Federated Learning as a solution 
to the problems with centralised learning [ 3]. In terms of the general data protection 
law of the European Union [ 12], it becomes more difﬁcult to extract the private data 
of different users from different services. The federated base health care model’s 
primary focus is on protecting the conﬁdentiality of patient information. Federated 
learning is considered the key in the health sector due to the rapid increase in digital 
health data. Federated Learning successfully predicts the hospitalisation of a patient 
due to cardiac events and the ICU stay time for a patient based on digital data [ 13]. 
Federated learning is used effectively in the wireless network for 5G-based applica-
tions. The key characteristics of federated learning in the wireless environment are 
content caching in the edge network, proper management of the frequency spectrum 
for data transmission, and the provision of network data analytic functions to make 
the parameter suitable for machine learning model algorithms [ 7]. The successful 
use of federated learning in health care and the support of the above-mentioned char-
acteristics of federated learning for wireless networks to carry out machine learning 
algorithms motivate the use of federated learning-based vehicular ad hoc networks 
in the health sector. Limitation of the centralized architecture due to computational 
overhead and merits of decentralized architecture of federated vehicular network 
(FVN) encourage to use FVN in the health care domain. 
3 
Proposed Model 
The proposed model of federated learning works on two levels. The ﬁrst level is 
made up of VANET components, such as RSU, which can classify health data using 
a federated model. In the ﬁrst level, RSU serves as the local server. The second level 
consists of the cloud network, which acts as the remote server. The remote server has 
greater computational power than the local server. The overall two-level structure of 
federated learning is presented in Fig. 2. The classiﬁcation of health data is done at the 
local server through the global federated learning framework. The generation of the 
federated learning framework is described below. In the designed framework, there 
are two main entities, namely, a remote server (cloud) and local servers (RSUs), 
such as RSUs connected to a number of vehicular modules in different regions. 
The cloud server is enabled with sufﬁcient computing power and resources for the 
global training. The local servers are generally powerful as compared to the vehicular 
devices. The local servers store the data for local training. For the classiﬁcation 
of health symptoms, a federated learning approach, i.e., federated averaging, uses 
transformed data from the ﬁrst level of the framework. 
1. The cloud server manages the whole learning process, and the local data is stored 
on a set of RSUs. 
2. We move our model from the cloud server down to all the RSUs.

A Federated Learning Based Connected Vehicular Framework.. . .
575 
Fig. 2 Two level structure for federated learning framework 
3. The RSUs separately train the model with local data and send it to the server, 
which aggregates the learnt parameters [ 5]. 
4. The RSU sends the gradients, and the server will aggregate these gradients in a 
mini-batch fashion. 
The overall ﬂowchart for the generation of global federated learning framework 
is shown in Fig. 3. 
Algorithm 1 depicts the overall Federated learning process in a form of pseudo-
code. 
4 
Experimental Results 
We make the assumption that there are four critical metrics derived from the human 
body that can be measured by an inexpensive wearable device. These parameters ((a) 
body temperature, (b) heart rate, (c) systolic, and (d) diastolic blood pressure) have 
the potential to be employed in the creation and evaluation of health care models. We 
have synthesised our symptom data using the following ranges: Fig. 4a for normal 
data, Fig. 4b for low-risk data, Fig. 4c for medium-risk data, and Fig. 4d for high-risk 
data. Patients who fall into the third and fourth risk categories must be immediately 
isolated and, if necessary, get medical assistance in case of an emergency. In addition, 
the data records for each risk class were generated in such a way that they would 
follow the distribution appropriately in order to replicate the real-world scenario.

576
B. R. Senapati et al.
Fig. 3 Flowchart for the 
federated learning 
framework generation 
Start 
Cloud network gen-
erate FL framework 
using historical data 
Transmit the FL 
framework to RSU 
Each RSU improves the 
framework by local training 
Improved FL frame-
work is transmitted 
back to cloud network 
Cloud network updates the 
improved FL framework, 
sends back the global FL 
framework to each RSU 
Stop 
Prior to integrating a predictive model into a healthcare system, two phases must 
be completed: (a) model construction and (b) model evaluation. Model construction 
entails learning the structure and parameters of a federated learning model, while 
evaluation entails assessing the learned model’s generalisation ability. For the imple-
mentation, we have considered two local servers (RSUs) and a remote server (cloud). 
The total number of epochs is set to 100, and the batch size is 64. The model is made 
up of four input features and four multi-class outputs. The middle layer contains 
32, 64, and 32 layers sequentially. The output layer has a softmax activation func-
tion, whereas the middle layers contain ReLU activation function. The training and 
testing samples are splitted into (50%,50%), (60%,40%), (70%,30%), (80%,20%), 
and (90%,10%) for the performance measurement. Table 1 presents the evaluation 
metrics of federated learning in terms of categorical accuracy, precision, recall, and 
loss with different training and testing sizes. 
Figures 5a and 5b compare centralised learning to federated learning in terms of 
multi-class classiﬁcation accuracy and false-classiﬁcation rate. Federated learning 
has the highest classiﬁcation accuracy of 80.14%, while centralised learning has a 
classiﬁcation accuracy of 78.29%. Because federated learning trained their model

A Federated Learning Based Connected Vehicular Framework.. . .
577 
(a)
(b) 
(c)
(d) 
Fig. 4 Data Statistics (a) Normal data, (b) Low-risk data, (c) Medium-risk data, and (d) High-risk 
data parameters 
Table 1 Federated learning results with different training & testing sizes 
Federated Learning 
Training & 
Testing 
Classiﬁcation 
Accuracy 
Precision
Recall
Loss 
(0.9,0.1)
0.8014
0.8165
0.7682
0.5045 
(0.8,0.2)
0.7896
0.8065
0.766
0.5110 
(0.7,0.3)
0.7894
0.8062
0.7614
0.5179 
(0.6,0.4)
0.7808
0.7942
0.7616
0.5173 
(0.5,0.5)
0.7796
0.7937
0.7548
0.5249

578
B. R. Senapati et al.
Algorithm 1 Federated Averaging 
1: Input: N : no. of RSUs, Bl (batch size), El (number of epochs), and β (learning rate) 
2: Cloud_Server: 
3: Assigned model parameter ω0 
4: for each t = 1, 2, . . .  do 
5:
m ← max(C.N , 1) {C.N=fraction of RSU} 
6:
sett = random set of m RSUs 
7:
for each RSU k ∈ sett do 
8:
ωk 
t+1 ← RSU_update(k, ωt ) 
9:
end for 
10:
ωt+1 ←ΣN 
k=1 
nk 
n ωk 
t+1 
11: end for 
12: RSU_Update(k,ω): 
13: Bl ← (divide pk into batches of Bl) {pk is  set of indexes of data on  kth RSU} 
14: for each epoch i=1 to El do 
15:
for each batch b ∈ Bl do 
16:
ω ← ω − β∇l(ω; b) 
17:
end for 
18: end for 
19: return ω to cloud server 
(a)
(b) 
Fig. 5 Centralised Learning Vs Federated Learning (a) Multi-class classiﬁcation accuracy, and (b) 
False classiﬁcation rate 
locally and updated the global model through aggregation, it outperformed cen-
tralised learning. The false classiﬁcation rate gives the missed classiﬁcation rate. 
The average false classiﬁcation rate for federated learning is 0.212, while the rate 
for centralised learning is 0.228. Due to individual training of the local model and 
updating the global model periodically, the distributed approach gives more accu-
rate classiﬁcation. Federated learning enables local model training in parallel, which 
leads to faster training with less computational power and storage than a centralised 
approach. As a result, federated learning ensures data privacy without exposing sen-
sitive information to the central server.

A Federated Learning Based Connected Vehicular Framework.. . .
579 
5 
Conclusion 
The increased use of digital data in a centralised computing environment puts data 
privacy and security at risk. The evolution of various body sensors also increases the 
amount of digital health data, which also demands privacy. The proposed VANET-
based federated learning uses a two-level structure to classify the health parameter 
data. For different training and testing ratios, the proposed work is evaluated for accu-
racy, precision, recall, and loss. Also, the federated learning approach is compared 
with the centralised learning approach in terms of accuracy and false classiﬁcation 
rate. It has been found that federated learning has more accuracy than centralised 
learning. Also, the lower false-classiﬁcation rate for the federated learning approach 
compared to the centralised approach indicates the better performance of federated 
learning as compared to centralised learning. The strength of the federated learning 
is the provision of data privacy and accuracy in real time predictions. But the involve-
ment of the heterogeneous system and effective communication is the challenge for 
the proposed work. Use of homogeneous components in the local and global server 
could enhance the performance more in the federated learning system. In the future, 
the privacy and security of the federated learning model will be evaluated. 
References 
1. Bandyopadhyay S et al (2020) Infection and mortality of healthcare workers worldwide from 
Covid-19: a systematic review. BMJ Glob Health 5(12):e003097 
2. Cao S, Lee VC (2020) An accurate and complete performance modeling of the IEEE 802.11p 
MAC sublayer for VANET. Comput Commun 149:107–120 
3. Jiang JC, Kantarci B, Oktug S, Soyata T (2020) Federated learning in smart city sensing: 
challenges and opportunities. Sensors 20(21):6230 
4. Liao L, Zhao J, Hu H, Sun X (2022) Secure and efﬁcient message authentication scheme for 
6G-enabled VANETs. Electronics 11(15):2385 
5. McMahan B, Moore E, Ramage D, Hampson S, y Arcas BA (2017) Communication-efﬁcient 
learning of deep networks from decentralized data. In: Artiﬁcial intelligence and statistics. 
PMLR, pp 1273–1282 
6. Mihret ET, Yitayih KA (2021) Operation of VANET communications: the convergence of 
UAV system with LTE/4G and wave technologies. Int J Smart Veh Smart Transp (IJSVST) 
4(1):29–51 
7. Niknam S, Dhillon HS, Reed JH (2020) Federated learning for wireless communications: 
motivation, opportunities, and challenges. IEEE Commun Mag 58(6):46–51 
8. Noor-A-Rahim M et al (2022) 6G for vehicle-to-everything (V2X) communications: enabling 
technologies, challenges, and opportunities. In: Proceedings of the IEEE 
9. Ranjan Senapati B, Mohan Khilar P (2020) Optimization of performance parameter for vehic-
ular ad-hoc network (VANET) using swarm intelligence. In: Nature inspired computing for 
data science. Springer, Cham, pp 83–107 
10. Ranjan Senapati B, Mohan Khilar P, Ranjan Swain R (2021) Environmental monitoring 
through vehicular ad hoc network: a productive application for smart cities. Int J Commun 
Syst 34(18):e4988 
11. Ranjan Senapati B, Swain S, Ranjan Swain R, Mohan Khilar P (2023) A heterogeneous 
fault diagnosis approach to enhance performance of connected vehicles. Int J Commun Syst 
36(4):e5414

580
B. R. Senapati et al.
12. Regulation P (2016) Regulation (EU) 2016/679 of the European parliament and of the council. 
Regulation (EU) 2016/679 
13. Rieke N et al (2020) The future of digital health with federated learning. NPJ Digital Med 
3(1):1–7 
14. Saraswat D et al (2022) Explainable AI for healthcare 5.0: opportunities and challenges. IEEE 
Access 
15. Senapati BR, Khilar PM (2020) Automatic parking service through VANET: a convenience 
application. In: Progress in computing, analytics and networking. Springer, Cham, pp 151–159 
16. Senapati BR, Khilar PM, Sabat NK (2019) An automated toll gate system using VANET. 
In: 2019 IEEE 1st international conference on energy, systems and information processing 
(ICESIP). IEEE, pp 1–5 
17. Senapati BR, Khilar PM, Swain RR (2021) Composite fault diagnosis methodology for urban 
vehicular ad hoc network. Veh Commun 29:100337 
18. Senapati BR, Swain RR, Khilar PM (2020) Environmental monitoring under uncertainty using 
smart vehicular ad hoc network. In: Smart intelligent computing and applications. Springer, pp 
229–238 
19. Singh P, Raw RS, Khan SA (2021) Development of novel framework for patient health moni-
toring system using VANET: an Indian perspective. Int J Inf Technol 13(1):383–390 
20. Swain S, Senapati BR, Khilar PM (2023) Evolution of vehicular ad hoc network and ﬂying 
ad hoc network for real-life applications: role of VANET and FANET. In: Modelling and 
simulation of fast-moving ad-hoc networks (FANETs and VANETs). IGI Global, pp 43–73

ELECTRE I-based Zone Head Selection 
in WSN-Enabled Internet of Things 
Sengathir Janakiraman, M. Deva Priya, A. Christy Jeba Malar, 
and Suma Sira Jacob 
Abstract In Wireless Sensor Network (WSN)-enabled Internet of Things (IoT) 
environment, potential resource utilization and efﬁcient service delivery are of great 
interest. In speciﬁc, IoT-based networks completely depend on energy-efﬁcient clus-
tering architecture that is used for transferring data between heterogeneous devices, 
and optimal energy-aware deployment methods in WSN. Clustering-based energy-
efﬁcient routing and sensor node deployment are identiﬁed to extend network life-
time. Efﬁcient selection of Zone Heads (ZHs) during the process of partitioning 
the network into different zones or clusters is essential for maximizing the reach-
ability of nodes within clusters, and for achieving better communication with the 
Base Station (BS). In this paper, ELECTRE I-based Zone Head Selection (EZHS) 
scheme based on distinct factors such as number of times a node is selected as ZH, 
distance of the sensor node from the center, distance between adjacent nodes and 
level of energy is proposed, since they directly inﬂuence network lifetime and drain 
of node energy. It adopts the merits of ELECTRE I-based Multi-Criteria Decision 
Making (MCDM) model for determining the relative inﬂuence of impactful parame-
ters during the process of ZH selection. The results of the proposed EZHS approach 
conﬁrm an improvement in network lifetime by 21.98% and network stability by 
22.94% in contrast to the benchmarked strategies.
S. Janakiraman 
Department of Information Technology, CVR College of Engineering, Mangalpally, Vastunagar, 
Hyderabad, Telangana, India 
e-mail: j.sengathir@gmail.com 
M. D. Priya envelope symbol
Department of Computer Science and Engineering, Sri Eshwar College of Engineering, 
Coimbatore, Tamilnadu, India 
e-mail: devapriya.m@sece.ac.in 
A. C. J. Malar · S. S. Jacob 
Department of Information Technology, Sri Krishna College of Technology, Kovaipudur, 
Coimbatore, Tamilnadu, India 
e-mail: a.christyjebamalar@skct.edu.in 
S. S. Jacob 
e-mail: sumasirajacob@skct.edu.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_50 
581

582
S. Janakiraman et al.
Keywords Wireless Sensor Network (WSN) · Internet of Things (IoT) ·
ELECTRE I · Zone Head (ZH) · Network Lifetime 
1 
Introduction 
Wireless Sensor Networks (WSNs) are extensively applied in unattended environ-
ments that include border surveillance, volcano monitoring, surveillance, health 
monitoring, smart homes, Internet of Things (IoT), etc. They are capable of sensing 
data from the physical environment of application, aggregate and forward them to 
the Base Station (BS) for better decision making [1, 16, 17]. Sensors are identiﬁed to 
play an indispensable role in smart IoT-based applications that include smart homes, 
environmental monitoring, smart cities and healthcare due to attractive applications 
of WSN. The main objective of IoT-based networks targets in facilitating connec-
tivity, anywhere, any time and with everything [2, 18]. In speciﬁc, WSNs possess 
the potentiality of offering predominant connectivity of real and virtual worlds. The 
sensor nodes have limited amount of resources like battery, memory and processing 
capability [19, 20]. These resource constrained sensor nodes need to be intelligently 
utilized, since the replacement or recharging of battery is completely difﬁcult when 
they are frequently operated in an unattended environment on a large scale. More-
over, WSNs face the signiﬁcant challenges of localization, network coverage, data 
aggregation, security and energy efﬁciency [3, 21]. It is always important to wisely 
use the available energy as sensor nodes are powered by battery and it is essential to 
extend the network lifetime in WSN-enabled IoT. The amounts of energy incurred 
in data transmission and reception are always comparatively higher than the energy 
required for sensing and aggregating data. Hence, energy efﬁcient approaches are 
essential for prolonging the overall network lifetime and ensuring stability. 
1.1 
Motivation 
Diversiﬁed methods from nodes deployment to decision making are contributed 
to the literature for achieving energy efﬁciency in WSN-enabled IoT environment 
[4]. At this juncture, clustering techniques are widely utilized for reducing energy 
consumption, since the sensor nodes collaborate with one another in a group termed 
as clusters. In clustering-based schemes, selection of a speciﬁc node as Zone Head 
(ZH) based on some speciﬁc criteria is required for achieving maximized network 
lifespan through effective data forwarding, aggregation and collection processes. 
Diverse clustering schemes contributed to the literature are mainly classiﬁed into 
probability and non-probability strategies, both of them targeting on attaining energy 
efﬁciency. The process of ZH selection in the probability-based strategies completely 
depends on prior probability or random selection strategy. ZHs are chosen based 
on a number of criteria such as the number of times a sensor node is selected as

ELECTRE I-based Zone Head Selection in WSN-Enabled Internet ...
583
CH, distance between nodes, power of transmission and level of Residual Energy 
(RE). The non-probability strategies select efﬁcient and more reliable node as ZH 
for network lifetime improvement [5]. Majority of the non-probability approaches 
mainly target on achieving better energy efﬁciency. But ensuring energy efﬁciency 
in IoT like large networks is still an open issue due to the randomized characteristics 
of WSNs. 
1.2 
Objectives 
The main objective of the proposed ELECTRE I-based ZH Selection (EZHS) scheme 
concentrates on the formulation of a Multi-Criteria Decision-Making (MCDM) 
model that explores different factors of sensor nodes during the selection of cluster 
ZH. It focusses on the evaluation of different impact parameters that attributes 
towards better selection of sensor nodes as ZHs. It also targets on the evaluation of 
the proposed EZHS scheme based on performance metrics that aids in determining 
its efﬁcacy over the existing schemes. 
In this paper, EZHS scheme is proposed based on the distinct factors such as 
number of times a node is selected as ZH, distance of the sensor node from the center, 
distance between adjacent nodes and level of energy, since they directly inﬂuence 
network lifetime and drain of node energy. It adopts the merits of ELECTRE I-based 
MCDM model for determining the relative inﬂuence of each impactful parameter 
during the process of ZH selection. The simulation results of the proposed EZHS 
approach are assessed based on network lifetime and stability for varying number of 
rounds and sensor nodes considered for evaluation. 
The remaining sections of the paper are structured as follows. Section 2 presents 
the complete study of the works contributed to the literature along with their merits 
and limitations. Section 3 details on the complete view of the proposed EZHS scheme. 
Section 4 presents the results of the proposed EZHS approach with respect to network 
lifetime and stability for varying number of sensor nodes and rounds. Section 5 
concludes the paper with major contributions and future scope of enhancement. 
2 
Related Work 
In this section, works carried out for effective CH selection in WSNs are discussed. 
Farman et al. (2017) [6] have designed a mechanism to choose CHs in WSN 
based on distance from nodes and centroid, RE, number of times a node is chosen as 
CH and merged nodes. This MCDM-based system employs Analytical Network 
Process (ANP) model for ideal CH selection. Sensitivity analysis is carried out 
to ensure stability of alternatives and ranking in diverse scenarios. The proposed 
scheme outdoes energy-based clustering protocols based on ideal CH selection 
and reduction in CH reselection leading to prolonged network lifespan. Sahaaya

584
S. Janakiraman et al.
Arul Mary & Gnanadurai (2017) [7] have proposed Enhanced Zone Stable Election 
Protocol (ZSEP-E) which is based on fuzzy logic (FZSEP-E) to assess CH selec-
tion probability by considering factors like node’s RE, density and distance to BS. 
It relays data to far-away CHs in multiple hops. From the outcomes, it is evident 
that the scheme involves reduced energy and offers improved network lifespan and 
throughput in contrast to ZSEP-E and FSEP-E. Farman et al. (2018) [8] have designed 
multi-criterion-based non-probabilistic CH selection mechanism for IoT-based WSN 
based on parameters that affect node energy and network lifespan. The factors consid-
ered in their former work [6] have greater impact on network performance. It chooses 
the most effective node that is capable of sensing more amounts of data as the CH. 
It uses ANP to determine the inﬂuence of parameters in CH selection. The perfor-
mance is analyzed for varying parameters in CH selection, along with their inﬂuence 
on stability and network lifespan. 
Murugaanandam & Ganapathy (2019) [9] have proposed Reliability-based 
Enhanced Technique for the Ordering of Preference by Similarity Ideal Solution 
(RE-TOPSIS) jointly with fuzzy logic. This MCDM approach aids in making efﬁ-
cient as well as consistent choice of CHs. It uses traditional LEACH protocol to 
facilitate choice of CH or scheduling in every cluster depending on rank index. It 
removes the demand for CH selection in every round of setup-state cycle of LEACH. 
It considers the RE, distance amid neighboring nodes, distances amid sink and CHs 
in addition to distances amid CHs to members, energy utilization rates, reliability 
index and adjacent node availability. Shelebaf & Tabatabaei (2020) [10] have used  
TOPSIS, a MCDM-based clustering method for WSNs. It clusters nodes based 
on energy levels and chooses nodes with increased amounts of RE as CHs. Self-
mapping is performed in which nodes with less-energy are attracted by nodes with 
increased energy. Balanced CHs are chosen based on RE, neighborhood, distance to 
sink and workload. Simulations of the proposed scheme carried out by opnet show 
that the proposed scheme outperforms other protocols like IEEE802.15.4 based on 
the amount of power consumed along with network lifetime. 
Garg & Kaur (2021) [11] have proposed a zone divisional scheme based on fuzzy 
logic which forms clusters and assigns nodes, thus reducing the amount of energy 
consumed along with complexity, and extending the network lifetime. In WSN, 
unequal clustering has a greater overhead and is exposed to connectivity problems. 
The proposed algorithm outperforms the existing schemes for effective data aggre-
gation in multi-hop WSNs in terms of amount of energy consumed, balancing of 
loads and extension of network lifespan. It considers factors like coverage of CH, 
power, sink to CH connectivity, RE, node power, and distance amid CH and sink, 
and amid CH and nodes for choosing optimum CHs. Establishing collaboration of 
these opposing features is tedious. Kumar & Kumar (2022) [12] have proposed a 
scheme in which collaborations are established and numerous criteria are employed 
for choosing the best set of CHs in ideal clustering. Nevertheless, it uses Visekriteri-
jumsko KOmpromise Rangiranje (VIKOR) for establishing co-operation. In VIKOR, 
appropriate CHs are chosen for optimal clustering. From the simulation outcomes, 
it is seen that the suggested mechanism offers improved network lifetime in contrast 
to other benchmarked schemes.

ELECTRE I-based Zone Head Selection in WSN-Enabled Internet ...
585
3 
Proposed ELECTRE I-based Zone Head Selection 
(EZHS) Scheme 
An Enhanced Hybrid Network Deployment based on Grid (EHNDG) mechanism that 
makes use of multiple criteria for ZH selection in a grid-based WSN is proposed. 
Grid-based Hybrid Network Deployment (GHND) [13] is enhanced, wherein ZHs 
are chosen based on level of energy, and distances from other nodes and zone’s 
center. The proposed work enhances the process of ZH selection by taking different 
parameters into consideration for choosing optimal node as the ZH. This improves 
network stability as well as network lifespan. This process is split into 2 stages 
namely, topology construction and ZH selection. 
3.1 
Construction of Topology 
In EHNDG, network is split into grids of dimension ‘up per G times upper G’, where every grid 
signiﬁes a zone. Nodes (n) are arbitrarily positioned in zone, and each node is allotted 
a static Zone ID. It is assumed that both the Base Station (BS) and node are aware 
of the Zone ID, energy level along with node position (co-ordinates). Based on these 
coordinates, the BS ﬁnds the density of node per zone. Zones with high and low 
densities are determined initially. In case the quantity of nodes positioned in a zone 
is fewer than least threshold, the nodes are merged with the neighboring zone based 
on density as well as distance. In case the number of nodes surpasses the maximum 
threshold, the zone is split into sub-zones ensuing GHND. 
WMS equals  left br acket left pa
renthesis upper D times Dist right parenthesis plus left parenthesis normal phi times Dens right parenthesis right bracket
Once network topology is built, the BS determines the ZH for every zone. The 
ZH is accountable for collecting data from nodes and forwarding to BS. 
3.2 
Selection of ZHs 
Network performance is signiﬁcantly dependent on choice of appropriate ZH as 
it plays a dominant role in protocols for ensuringe energy efﬁciency in WSN as 
well as IoT networks. Inappropriate selection may cause instability of networks and 
dropping of network lifespan as it has a direct inﬂuence on complete network. As 
ZHs are accountable for gathering and forwarding data, it is signiﬁcant in choosing 
ideal node as ZH, thus prolonging network stability as well as lifespan. The following 
parameters are taken into consideration: Node’s RE, Distance from adjacent nodes 
in a zone (Dist Subscript Node), Distance from zone center (Dist Subscript CZ), Merged Node (upper M Subscript Node) and 
Number of Times a node acts as ZH (ZH Subscript upper T).

586
S. Janakiraman et al.
The factors are combined to form an Aggregated Score (AS) as presented in 
Eq. (3). Weights (w Su bscr ipt 1 Baseline comma w Subscript 2 Baseline comma w Subscript 3 Baseline comma w Subscript 4 Baseline) are allocated to every factor based on the priority 
using ANP. The total weight is determined using Eq. (2). Once investigational 
assessment is done using ANP, weights are such that w S ub scr ip t 1  B aseline greater than w Subscript 2 Baseline greater than w Subscript 3 Baseline greater than w Subscript 4. 
w S ubscr ipt 1  Base lin
e p
ANP, a decision tool based on multiple criteria is extensively used for selecting 
components in varied ﬁelds. The problem is split into sub-problems by presenting 
criteria as well as substitutes. Quantitative score in the range 1 to 9 [14] is used in  
scaling criteria as well as alternatives. Once scaling is done, criteria as well as alterna-
tive are compared to obtain relative signiﬁcance over one another. To ensure trustwor-
thiness, it is signiﬁcant to check the consistency. In case Consistency Ratio (CR) < 0.1, 
then comparison is steady; else comparison should be modiﬁed. Unweighted super-
matrix is got from comparisons and it is converted to column stochastic Weighted 
Supermatrix (WS). Limit matrix is obtained by stabilizing weights. It offers ﬁnal 
weights of criteria as well as alternatives. ANP aids in identifying association and 
signiﬁcance amid factors. The model deals with parameter prioritization. ‘RE’ has 
increased weight making it the most signiﬁcant followed by ‘Dist Subscript Avg’ as well as  
‘prob
ability Subscript ZH Superscript upper T’, while, ‘upper M Subscript Node’ has least priority. Based on the weights obtained using ANP 
model, they are assigned to parameters
l
eft 
p
arenthesis upper W Subscript i Superscript AS Baseline right parenthesis
. 
uppe
r 
W Subs cript i  
S
upe r
script AS Baseline equals left parenthesis w Subscript 1 Baseline times RE Subscript i Baseline right parenthesis plus left parenthesis w Subscript 2 Baseline times StartFraction 1 Over Dist Subscript Avg Superscript i Baseline EndFraction right parenthesis plus left parenthesis w Subscript 3 Baseline times probability Subscript ZH Superscript upper T Baseline right parenthesis plus left parenthesis w Subscript 4 Baseline times upper M Subscript Node Baseline right parenthesis
upper 
W S
u
b
s
cri pt i S
up
e
rscrip t AS Bas
eline equals left parenthesis w Subscript 1 Baseline times RE Subscript i Baseline right parenthesis plus left parenthesis w Subscript 2 Baseline times StartFraction 1 Over Dist Subscript Avg Superscript i Baseline EndFraction right parenthesis plus left parenthesis w Subscript 3 Baseline times probability Subscript ZH Superscript upper T Baseline right parenthesis plus left parenthesis w Subscript 4 Baseline times upper M Subscript Node Baseline right parenthesis
where, ‘RE Subscript i’ represents the RE of node ‘i’, ‘prob
ability Subscript ZH Superscript upper T’ signiﬁes the priority of a node to 
be the ZH and ‘Dist S
ubscript Avg Superscript i’ represents the Average distance of ‘i’ in the zone. Higher 
the value of ‘RE’, more is the chance of a node to be the ZH. In the initial round, 
‘RE’ is uniform as homogenous nodes have similar resources. ‘Dist S
ubscript Avg Superscript i’ is based on 
‘Dist Subscript Node’ and ‘Dist Subscript CZ’. Reduced value of ‘Dist S
ubscript Avg Superscript i’ increases the chances of becoming 
the ZH. It is the responsibility of the BS to determine ‘Dist Su bs
cript Node Superscript i comma j’ for all nodes in their 
corresponding zones. 
Dist Su bs
cript  Node Superscript i comma j Baseline equals StartRoot left parenthesis ix minus jx right parenthesis squared plus left parenthesis iy minus jy right parenthesis squared EndRoot
/
Dist  Subscr ipt No de Supe
rscript i comma j Baseline equals StartRoot left parenthesis ix minus jx right parenthesis squared plus left parenthesis iy minus jy right parenthesis squared EndRoot
Dist Sub
scrip t Cent Superscript i comma c Baseline equals StartRoot left parenthesis ix minus cx right parenthesis squared plus left parenthesis iy minus cy right parenthesis squared EndRoot
/
Dist  Subscr ipt Ce nt Supe
rscript i comma c Baseline equals StartRoot left parenthesis ix minus cx right parenthesis squared plus left parenthesis iy minus cy right parenthesis squared EndRoot
Dist S
ubsc ri
pt Avg Superscript i Baseline equals StartFraction normal gamma Over n negative 1 EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts Dist Subscript Node Superscript i comma j Baseline plus normal delta times Dist Subscript Cent Superscript i comma c Baseline comma j not equals i
Di st 
Sub
scri pt Avg Su
persc ript i Baseli ne
 equal s  Sta
rtF
raction normal gamma Over n negative 1 EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts Dist Subscript Node Superscript i comma j Baseline plus normal delta times Dist Subscript Cent Superscript i comma c Baseline comma j not equals i
where ‘normal gamma’ and ‘normal delta’ are weights allotted to ‘Dist Subscript Node’ and ‘Dist Subscript CZ’ based on signiﬁ-
cance. Initially, ‘prob
ability Subscript ZH Superscript upper T’ is the same for every node as no node is chosen as the ZH.

ELECTRE I-based Zone Head Selection in WSN-Enabled Internet ...
587
Nevertheless, during reselection, node with a chance of being a ZH will be assigned 
lesser priority. Furthermore, to evade inefﬁcient node selection and improve network 
stability and lifetime, it is seen that the increased value of ‘ZH Subscript upper T’ reduces the likelihood 
of the node being the ZH. 
prob
abi l
ity Subscript ZH Superscript upper T Baseline equals StartFraction 1 Over 1 plus ZH Subscript upper T Baseline EndFraction
pr obabil
ity Subscript ZH Superscript upper T Baseline equals StartFraction 1 Over 1 plus ZH Subscript upper T Baseline EndFraction
In case a node ﬁts into the same zone, the weight is increased in contrast to the 
one that is combined. Network topology is built using GHND wherein, less denser 
zones are combined based on a threshold called lower bound, while high denser ones 
are divided into sub-zones based on upper bound. In case a node is combined, there 
are chances for that node to be far from neighbors leading to inefﬁcient selection. 
This parameter is included so as to reduce likelihoods of far nodes. 
The BS computes ‘uppe
r W Subscript i Superscript AS ’ of nodes in the corresponding zones and sustains a list 
which is shared with respective ZHs, once ZH is chosen. This aids in avoiding control 
messages during reselection. Nodes with increased ‘uppe
r W Subscript i Superscript AS ’ is chosen as ZH for the 
particular zone. On getting the list, every ZH shares the transmission schedule with 
members based on TDMA policy. Members forward the sensed data to the respective 
ZH according to the schedule. 
Radio model [15] assesses the amount of energy consumed in nodes. For sending a 
message (k-bits) over ‘d’ meters, the amount of energy consumed at the transmitting 
end is given by ‘upper
 E Subscript Tx Superscript k comma d ’. 
upper
 E Subscr ipt Tx Sup ersc ript 
k comma d Baseline equals upper E Subscript El Baseline times k plus upper E Subscript Am Baseline times k times r squared
where, ‘upper E Subscript El’ represents the energy consumed to run transmitter as well as receiver 
circuitry set to 50 nJ slash bit comma ‘upper E Subscript Am’ assigned a value of 100 pJ slash bit slash m squared is the amount 
of Tx ampliﬁer energy consumed, as k-bits are transmittedperiod The amount of energy 
consumed at the receiver is given by, 
upp
er E Subs cri
pt 
Total energy is computed as shown below. 
up
er E  Subscr
ipt  Tot 
Su
perscript k Baseline equals upper E Subscript Tx Superscript k comma d Baseline plus upper E Subscript Rx Superscript k
3.3 
Adoption of ELECTRE I for ZH Selection 
ELECTRE I-based scheme is proposed to deal with the selection of ZHs. The steps 
are listed below.

588
S. Janakiraman et al.
• Selection of ZHs from group of Sensor Nodes: A group of ZHs is selected based 
on ‘RE’, ‘Dist Subscript Node’, ‘Dist Subscript CZ’, ‘upper M Subscript Node’ and ‘ZH Subscript upper T’from the available set of sensor 
nodes. 
• Finding the Fitness value of each Sensor Node to select ZHs: Potential sensor 
nodes depending on energy and distance are considered for the selection of ZHs 
selection. 
• Choice of Impactful Criteria: The selection criteria includes ‘RE’, ‘Dist Subscript Node’, 
‘Dist Subscript CZ’, ‘upper M Subscript Node’ and ‘ZH Subscript upper T’. In contrast to the chosen criteria, sensor nodes 
(alternatives) are assessed. 
• Assessment of Signiﬁcance Weight (SW): The SW by ‘k’ ZHs is assessed using 
scale measurement, followed by computation of criterion weights. 
SW S ubscript j Baseline equals StartFraction 1 Over k EndFraction left bracket SW Subscript j Superscript 1 Baseline plus SW Subscript j Superscript 2 Baseline plus ellipsis period period SW Subscript j Superscript k Baseline right bracket
S
W
 Sub
sc ript j
 B ase l i ne equ
a
l
s StartFraction 1 Over k EndFraction left bracket SW Subscript j Superscript 1 Baseline plus SW Subscript j Superscript 2 Baseline plus ellipsis period period SW Subscript j Superscript k Baseline right bracket
• Assessment of Sensor Nodes (alternatives) Rating: The rating of alternatives is 
assessed by ‘k’ ZHs using scale measurement for evaluating ratings and forming 
Decision Matrix (DM Subscript ij). 
d Su bscript ij Baseline equals StartFraction 1 Over k EndFraction left bracket d Subscript ij Superscript 1 Baseline plus d Subscript ij Superscript 2 Baseline plus ellipsis period period d Subscript ij Superscript k Baseline right bracket
d
 
Sub
scr ipt i
j B ase l i ne eq
ua
l
s StartFraction 1 Over k EndFraction left bracket d Subscript ij Superscript 1 Baseline plus d Subscript ij Superscript 2 Baseline plus ellipsis period period d Subscript ij Superscript k Baseline right bracket
The format of decision matrix can be expressed as follows: 
DM Su bs
cr
ipt ij 
Ba
se
lin
e e
q u a ls St
art
 3 
B y  3 Mat
ri
x 
1s
t Ro
w 
1s
t 
Colu
mn
 d
 S
u b s c
ri
pt
 1
1 Ba
se
line 2n
d 
Co
lumn d Subscript 12 Baseline 3rd Column StartLayout 1st Row 1st Column ellipsis 2nd Column d Subscript 1 n Baseline EndLayout 2nd Row 1st Column d Subscript 21 Baseline 2nd Column d Subscript 22 Baseline 3rd Column StartLayout 1st Row 1st Column ellipsis 2nd Column d Subscript 2 n Baseline EndLayout 3rd Row 1st Column StartLayout 1st Row period 2nd Row period 3rd Row StartLayout 1st Row period 2nd Row d Subscript m 1 Baseline EndLayout EndLayout 2nd Column StartLayout 1st Row period 2nd Row period 3rd Row StartLayout 1st Row period 2nd Row d Subscript m 2 Baseline EndLayout EndLayout 3rd Column StartLayout 1st Row 1st Column StartLayout 1st Row period 2nd Row period 3rd Row StartLayout 1st Row period 2nd Row ellipsis EndLayout EndLayout 2nd Column StartLayout 1st Row period 2nd Row period 3rd Row StartLayout 1st Row period 2nd Row d Subscript mn EndLayout EndLayout EndLayout EndMatrix
up per W eq u a l s le
ft bracket w Subscript 1 Baseline w Subscript 2 Baseline ellipsis w Subscript n Baseline right bracket
where, ‘d Subscript ij’co mma for all Subscript i comma j Baseline are the alternatives’ rates ‘Al Subscript i’ lef t par en t h e sis i equals 1 comma 2 comma ellipsis m right parenthesis based on criterion 
‘upper C Subscript j’. The weight of ‘upper C Subscript j’ is given by ‘w Subscript j’lef t par ent hesis  j equals 1 comma 2 comma period period period comma n right parenthesis. 
• Finding Relationship amid Alternatives: The association amid alternatives 
relating to every criterion is determined. Pairwise comparison of alternatives (‘Al Subscript i’ 
and ‘Al Subscript k’, ‘k’ in [i…m], k not equalsi) is given by, 
upper J Super s
c
r
ipt p lus Ba se line le
f
t parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals left brace j vertical bar upper C Subscript j Baseline left parenthesis Al Subscript i Baseline right parenthesis greater than upper C Subscript j Baseline left parenthesis Al Subscript k Baseline right parenthesis right brace
where, ‘upper J Superscript plus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis’ represents the criterion set for which ‘Al Subscript i’ is favoured over 
‘Al Subscript k’. 
upper J Super s
c
r
ipt e quals Baseline 
l
eft parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals left brace j vertical bar upper C Subscript j Baseline left parenthesis Al Subscript i Baseline right parenthesis equals upper C Subscript j Baseline left parenthesis Al Subscript k Baseline right parenthesis right brace

ELECTRE I-based Zone Head Selection in WSN-Enabled Internet ...
589
where, ‘upper J Superscript equals Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis’ represents the criterion set for which ‘Al Subscript i’ is equally 
favoured to ‘Al Subscript k’. 
upper J Super s
c
r
ipt m inus B as eline l
e
ft parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals left brace j vertical bar upper C Subscript j Baseline left parenthesis Al Subscript i Baseline right parenthesis less than upper C Subscript j Baseline left parenthesis Al Subscript k Baseline right parenthesis right brace
where, ‘upper J Superscript minus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis’ represents the criterion set for which ‘Al Subscript k’ is favoured over 
‘Al Subscript i’. 
• Conversion of Association amid Alternatives: The total criteria weights in every 
set of comparison are determined. 
upper P Super s
c
ri pt p lus  Baseline left
 parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals sigma summation Underscript j Endscripts w Subscript j Baseline comma for all Subscript j Baseline element of upper J Superscript plus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis
upper P Super s
c
ri pt e qua ls Baselin e le
ft parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals sigma summation Underscript j Endscripts w Subscript j Baseline comma for all Subscript j Baseline element of upper J Superscript equals Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis
upper P Super s
c
ri pt m inu s Baseline  lef
t parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals sigma summation Underscript j Endscripts w Subscript j Baseline comma for all Subscript j Baseline element of upper J Superscript minus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis
• Combining Numerical Values: The numerical values are combined by deter-
mining the Concordance Index (CI), Concordance set and Discordance Index 
(DI). 
• CI: It shows how much the hypothesis (‘Al Subscript i’ which outclasses ‘Al Subscript k’) is 
consistent with reality shown by assessments of alternatives,0 less tha n or equals CI Subscript i comma k Baseline less than or equals 1 period
CI Sub script i c omma k Baseline  equals StartFraction upper P Superscript plus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis plus upper P Superscript equals Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis Over upper P left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis EndFraction
CI Subs cript
 i comma k Baseline equals StartFraction upper P Superscript plus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis plus upper P Superscript equals Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis Over upper P left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis EndFraction
where, 
upper P  left  parenthes is Al  Subscript  i Ba seline com ma Al Subscript k Baseline right parenthesis equals upper P Superscript plus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis plus upper P Superscript equals Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis plus upper P Superscript minus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis
• Concordance Set: 
upper J  left  parenthes is Al  Subscript  i B
aseline comma Al Subscript k Baseline right parenthesis equals upper J Superscript plus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis union upper J Superscript equals Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis
• DI: 
DI Sub s
c
rip t i com a k B a s
eline equals StartLayout Enlarged left brace 1st Row 0 comma upper J Superscript minus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals normal empty set 2nd Row StartFraction 1 Over normal beta Subscript j Baseline EndFraction times m a x left parenthesis upper C Subscript j Baseline left parenthesis Al Subscript k Baseline right parenthesis minus upper C Subscript j Baseline left parenthesis Al Subscript i Baseline right parenthesis right parenthesis comma j element of upper J Superscript minus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis comma upper O t h e r w i s e EndLayout
DI Subsc
r
ipt i co mma k Bas
e
li ne  equals St artLay out Enlarg
ed left brace 1st Row 0 comma upper J Superscript minus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis equals normal empty set 2nd Row StartFraction 1 Over normal beta Subscript j Baseline EndFraction times m a x left parenthesis upper C Subscript j Baseline left parenthesis Al Subscript k Baseline right parenthesis minus upper C Subscript j Baseline left parenthesis Al Subscript i Baseline right parenthesis right parenthesis comma j element of upper J Superscript minus Baseline left parenthesis Al Subscript i Baseline comma Al Subscript k Baseline right parenthesis comma upper O t h e r w i s e EndLayout
where, ‘normal beta Subscript j’ represents the amplitude of scale related to ‘upper C Subscript j’, 0 less tha n or equals DI Subscript i comma k Baseline less than or equals 1
• Filtering of alternatives: From the beginning, the action sets are extracted. From 
the set, an action is preserved. It outclasses more alternatives.

590
S. Janakiraman et al.
CI Sub scri
pt i comma k Baseline greater than or equals ct
DI Sub scri
pt i comma k Baseline less than or equals dt
Al S u bscript i Baseline upper S Al Subscript k
where, 
S - Outranking association (Al S u bscript i Baseline upper S Al Subscript k means ‘Al Subscript i’ is as good as ‘Al Subscript k’). 
Finally, the outranking values of the sensor nodes are considered for identifying 
potential nodes as ZHs in the network. 
4 
Results and Discussion 
The simulation experiments of the proposed EZHS scheme and the compared ZSEP-
E, RE-TOPSIS and VIKOR-ZHS approaches are conducted using ns-2.34. The simu-
lation parameters considered for implementing the proposed EZHS comprises of 
network area of 100 × 100 m, network node density of 100, energy of 0.5 J and 
1.0 J, size of the packets set to 2000, the dimensions of gird equal to 4 and energy 
incurred for ampliﬁcation and data aggregation are 100 pJ/bit/m2 and 5 nJ/bit/signal) 
respectively. 
Initially, the performance of the proposed EZHS scheme and the compared ZSEP-
E, RE-TOPSIS and VIKOR-ZHS approaches are evaluated based on network stability 
with initial energy of sensor nodes set to 0.5 J and 1.0 J. This performance evaluation 
is mainly conducted for identifying the time at which the ﬁrst node dies in the network. 
This network stability is investigated based on the number of rounds as depicted in 
Figs. 1 and 2.
The results of the proposed EZHS scheme evaluated based on network stability 
with initial energy of sensor nodes set to 0.5 J and 1.0 J conﬁrm its predomi-
nance over the compared approaches, since it takes a greater number of rounds 
for the First Node Death (FND), Half Node Death (HND) and Last Node Death 
(LND), as it adopts multiple inﬂuential factors during the implementation of non-
probabilistic clustering schemes used for ZH selection. This implementation of the 
non-probabilistic clustering scheme helps in potential selection of ZH in the network 
for offering better stable network. In speciﬁc, the mean network stability achieved 
by the proposed EZHS scheme is 21.38% better on an average when compared to 
ZSEP-E, RE-TOPSIS and VIKOR-ZHS approaches taken for evaluation. 
Further, the performance of the proposed EZHS scheme and the compared ZSEP-
E, RE-TOPSIS and VIKOR-ZHS approaches are evaluated based on network lifetime 
(number of alive nodes vs number of rounds) with initial energy of sensor nodes set 
to 0.5 J and 1.0 J. This performance evaluation is mainly conducted for determining 
the number of nodes alive in the network during the implementation of different

ELECTRE I-based Zone Head Selection in WSN-Enabled Internet ...
591
Fig. 1 Network stability (FND, HND, and LND) achieved by the proposed EZHS scheme with 
initial energy of nodes set to 0.5 J 
Fig. 2 Network stability (FND, HND, and LND) achieved by the proposed EZHS scheme with 
initial energy of nodes set to 1.0 J

592
S. Janakiraman et al.
rounds ranging from 500 to 3000. In speciﬁc, Figs. 3 and 4 represent the network 
lifetime with initial energy of sensor nodes set to 0.5 J and 1.0 J respectively. 
Fig. 3 Network Lifetime (Number of Alive Nodes) attained by the proposed EZHS scheme for 
varying number of rounds and initial energy of nodes set to 0.5 J 
Fig. 4 Network Lifetime (Number of Alive Nodes) attained by the proposed EZHS scheme for 
varying number of rounds and initial energy of nodes set to 1.0 J

ELECTRE I-based Zone Head Selection in WSN-Enabled Internet ...
593
The results of the proposed EZHS scheme evaluated based on network lifetime 
with initial energy of sensor nodes set to 0.5 J and 1.0 J conﬁrm the predominance over 
the compared approaches, since the factors considered for ZH selection is comprehen-
sive. It explores only the energy-efﬁcient sensor nodes during the process. Moreover, 
average improvement in network lifetime achieved by the proposed EZHS scheme is 
19.84% superior in contrast to ZSEP-E, RE-TOPSIS and VIKOR-ZHS approaches 
taken for evaluation. 
5 
Conclusion 
The proposed EZHS scheme adopts the factors of frequent number of times a node is 
selected as ZH, distance of the sensor node from the center, distance between adjacent 
nodes and level of energy, and conﬁrm better network lifetime and energy stability in 
WSNs-enabled IoT. It inherits the merits of ELECTRE I-based MCDM model and 
achieves better ZH selection. The results of the proposed EZHS approach conﬁrm an 
average improvement in network lifetime by 21.98% and network stability by 22.94% 
in contrast to benchmarked strategies. As a part of future scope of research, it is 
planned to formulate and implement a PROMETHEE-based decision-making model 
and compare it with the proposed EZHS scheme under heterogeneous network condi-
tions. The proposed EZHS scheme can also be explored under different homogeneous 
and heterogeneous network conditions. 
References 
1. Chung-Shuo FAN (2013) Rich: region-based intelligent cluster-head selection and node 
deployment strategy in concentric-based WSNs. Adv Electr Comput Eng 13(4):3–8 
2. Chauhan V, Soni S (2020) Mobile sink-based energy efﬁcient cluster head selection strategy 
for wireless sensor networks. J Ambient Intell Humaniz Comput 11(11):4453–4466 
3. Jafarizadeh V, Keshavarzi A, Derikvand T (2017) Efﬁcient cluster head selection using Naïve 
Bayes classiﬁer for wireless sensor networks. Wireless Netw 23(3):779–785 
4. Anisi MH, Abdullah AH, Razak SA (2013) Energy-efﬁcient and reliable data delivery in 
wireless sensor networks. Wireless Netw 19(4):495–505 
5. Bagci H, Yazici A (2013) An energy aware fuzzy approach to unequal clustering in wireless 
sensor networks. Appl Soft Comput 13(4):1741–1749 
6. Farman H, Javed H, Jan B, Ahmad J, Ali S, Khalil FN, Khan M (2017) Analytical network 
process based optimum cluster head selection in wireless sensor network. PLoS ONE 
12(7):e0180848 
7. Sahaaya Arul Mary SA, Gnanadurai JB (2017) Enhanced zone stable election protocol based on 
fuzzy logic for cluster head election in wireless sensor networks. Int J Fuzzy Syst 19(3):799–812 
8. Farman H et al (2018) Multi-criteria based zone head selection in Internet of Things based 
wireless sensor networks. Futur Gener Comput Syst 87:364–371 
9. Murugaanandam S, Ganapathy V (2019) Reliability-based cluster head selection methodology 
using fuzzy logic for performance improvement in WSNs. IEEE Access 7:87357–87368 
10. Shelebaf A, Tabatabaei S (2020) A novel method for clustering in WSNs via TOPSIS multi-
criteria decision-making algorithm. Wireless Pers Commun 112(2):985–1001

594
S. Janakiraman et al.
11. Garg A, Kaur G (2021) Zone head selection algorithm based on fuzzy logic for wireless sensor 
networks. J Univ Shanghai Sci Technol 23(21) 
12. Kumar A, Kumar A (2022). Multi criteria decision making based energy efﬁcient clustered 
solution for wireless sensor networks. Int J Inf Technol 1–10 
13. Farman H, Javed H, Ahmad J, Jan B, Zeeshan M (2016) Grid-based hybrid network deployment 
approach for energy efﬁcient wireless sensor networks. J Sens 
14. Saaty TL (2004) Decision making-the analytic hierarchy and network processes (AHP/ANP). 
J Syst Sci Syst Eng 13(1):1–35 
15. Heinzelman WR, Chandrakasan A, Balakrishnan H (2000) Energy-efﬁcient communication 
protocol for wireless microsensor networks. In Proceedings of 33rd IEEE annual Hawaii 
international conference on system sciences, p 10 
16. Janakiraman S, Priya MD, Devi SS, Sandhya G, Niveditha G, Padmavathi S (2021) A markov 
process-based opportunistic trust factor estimation mechanism for efﬁcient cluster head selec-
tion for extending lifetime of wireless sensor networks. EAI Endorsed Trans Energy Web 
8(35):e5–e5. ISSN: 2032-944X 
17. Sengathir J, Deva Priya M, Nithiavathy R, Sam Peter S (2023) COPRAS-based decision-making 
strategy for optimal cluster head selection in WSNs. In: Mahapatra RP, Peddoju SK, Roy S, 
Parwekar P (eds) Proceedings of International Conference on Recent Trends in Computing. 
Lecture Notes in Networks and Systems, vol 600, pp 537–549. Springer, Singapore. https:// 
doi.org/10.1007/978-981-19-8825-7_46. ISBN: 978-981-19-8825-7 
18. Balamurugan A, Sengathir J, Deva Priya M, Christy Jeba Malar A (2022) Hybrid marine 
predators optimization and improved particle swarm optimization-based optimal cluster routing 
in wireless sensor networks (WSNs). China Commun 19(6):219–247. https://doi.org/10.1016/ 
j.suscom.2023.100875. ISSN: 1673-5447 
19. Balamurugan A, Janakiraman S, Deva Priya M (2022) Modiﬁed African buffalo and group 
teaching optimization algorithm-based clustering scheme for sustaining energy stability and 
network lifetime in wireless sensor networks. Trans Emerg Telecommun Technol 33(1). Wiley. 
ISSN: 2161-3915 
20. Balamurugan A, Deva Priya M, Christy Jeba Malar A, Janakiraman S (2021) Hybrid stochastic 
ranking and opposite differential evolution-based enhanced ﬁreﬂy optimization algorithm for 
extending network lifetime through efﬁcient clustering in WSNs. J Netw Syst Manag 29(33). 
Springer. https://doi.org/10.1007/s10922-021-09597-6. ISSN: 1573-7705 
21. Sengathir J, Deva Priya M (2020) An energy-proﬁcient clustering-inspired routing protocol 
using improved Bkd-tree for enhanced node stability and network lifetime in wireless sensor 
networks. Int J Commun Syst 33(16). Wiley. ISSN: 1099-1131

Fabrication of Metal Oxide Based Thick 
Film pH Sensor and Its Application 
for Sweat pH Measurement 
Vandana Pagar, Shweta Jagtap, Arvind Shaligram, and Pravin Bhadane 
Abstract Electrochemical pH sensors are progressively in demand for applications 
such as continuous monitoring of water quality and health monitoring. This is due to 
better stability over a wide pH range, ease of fabrication, mechanical robustness & 
miniaturization. In this work, a thick ﬁlm-based pH sensor is fabricated using pH-
sensitive oxides RuO2 & TiO2. An interdigitated conducting electrode’s surface 
was screen printed with the sensitive RuO2 - TiO2 paste for conductimetric pH 
monitoring. The constructed IDE pH sensor has the following main advantages: 
faster and less expensive fabrication. Performance analysis of fabricated sensors 
using impedance spectroscopy is presented and discussed. Optimized performance 
is obtained for thick ﬁlm pH sensor when we mix two metal oxides RuO2 & TiO2 in 
the proportion 80:20 wt.%. Its pertinency for measurements of sweat pH is checked 
by measuring the pH value of an artiﬁcial sweat i.e., human sweat equivalent solution. 
The pH sensors were characterized using electrochemical impedance spectroscopy 
in the 10 Hz to 8 MHz frequency range. 
Keywords Metal oxides · Thick ﬁlm · pH sensor · Screen printed technology ·
Sweat analysis
V. Pagar envelope symbol
MAEER’s MIT Art’s, Commerce and Science College, Alandi, Pune, Maharashtra 412105, India 
e-mail: vnpagar@mitacsc.ac.in 
S. Jagtap 
Department of Instrumentation Science, Savitribai Phule Pune University, Pune, Maharashtra, 
India 
A. Shaligram 
Department of Electronics, Savitribai Phule Pune University, Pune, Maharashtra, India 
P. Bhadane 
Department of Electronics, Nowrosjee Wadia College, Pune, Maharashtra, India 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_51 
595

596
V. Pagar et al.
1 
Introduction 
The pH measurement has wide applications in the food industry, environmental 
science, chemical engineering, biomedical science, soil measurements in agricul-
ture, chemistry, etc. Generally, glass electrodes are used for pH determination in 
various applications, but these sensors are bulky and don’t offer mechanical robust-
ness. Therefore, traditionally used glass electrodes are not suitable for applications 
such as continuous health monitoring devices. Sensors that are mechanically robust 
and can be miniaturized are required for these applications. Sensors based on metal 
oxide can meet this criterion. The best sensing electrodes for these sensors are made 
of metal oxides because they can be easily miniaturized and have a high level of 
chemical stability [1, 7]. Metal oxide pH sensors are more widely applicable than 
glass pH sensors, respond quickly to aqueous, non-aqueous, and harsh conditions, 
and have a low cost of fabrication, high sensitivity, and durability [1, 5]. pH sensors 
have previously been made from a variety of metal oxides, including PtO2, IrO2, 
RuO2, OsO2, Ta2O2, TiO2, SnO2, ZrO2, CO2O2, and PbO2. Out of these, RuO2 has a 
very low resistance in alkaline solutions [10, 11] but is a highly sensitive and highly 
conductive material. Excellent conductivity and high selectivity for hydrogen ions 
in RuO2 prevent the space charge accumulation and response close to the Nernstian 
equation [1, 14]. Oxide-based conductometric pH sensors do not require a reference 
electrode, unlike metal oxides-based sensors that operate on a potentiometric prin-
ciple. The TiO2-based ﬁlms are considered suitable for pH-sensitive layers due to 
their great chemical stability, and a few studies on TiO2 as a pH sensing layer have 
also been published [7, 12]. However, due to the inherent characteristics of TiO2 
sheets, the sensors in such realizations showed low sensitivity. To achieve the best 
performance from oxide-based conductometric pH sensors, we have combined RuO2 
and TiO2 here. TiO2 helps to keep RuO2 from corroding and increases the antifouling 
resilience of pH sensors in challenging environments [13]. Emerging technologies 
in the biomedical sciences include continuous health monitoring and non-invasive 
diagnosis. 
Recent studies in this area have revealed that biomarkers that are slightly portioned 
from blood can be found in bodily ﬂuids that are externally released by humans, 
such as tears, saliva, and perspiration. Out of this sweat is the most easily acces-
sible bioﬂuid for continuous health monitoring. Numerous researchers have proposed 
electrochemical sensor-based sweat monitoring devices. Continuous physiological 
monitoring of sweat is a vital diagnostic tool in accessing health status of an indi-
vidual person. To prevent dehydration during physical exercise, appropriate elec-
trolyte balance must be ensured, especially in athletes. Electrolyte concentrations 
in sweat, including salt, chloride, and potassium, serve as a measure of hydration. 
Continuous sweat monitoring can be used to detect changes in sweat composition. 
Electrolyte concentrations and sweat pH are recognized to be closely correlated 
[17]. Which means that pH measurements may be used to relate the build-up of 
acid in muscle cells during exercise which leads to muscle fatigue. pH measure-
ment is very crucial in sweat monitoring because it can imitate the variations in the

Fabrication of Metal Oxide Based Thick Film pH Sensor ...
597
concentration of various electrolytes in sweat which is an indication of disease and 
metabolic activity. A healthy person’s pH value, for instance, is between 4.5 and 
6.5, whereas people with cystic ﬁbrosis have alkaline sweat (up to pH 9), which is 
caused by a deﬁciency in bicarbonate-reabsorption (H+-secretion) (Nikolajek and 
Emrich, 1976). The pH level of perspiration is another sign of how dehydrated a 
person is. Ammonia in sweat changes during exercise into ammonium (NH4+), which 
lowers the concentration of ammonia in the sweat. Sweat pH rises as a result of the 
excess NH4+ molecules accumulating ions because NH4+ is less diffusible across 
cellular membranes than ammonia (Czarnowski and Gorski, 1991; Sonner et al., 
2015). Considering these, accurate sweat pH monitoring is crucial for applications 
including health monitoring and wellness. This is a vital step in the development 
of a multisensory functional system for comprehensive sweat-based health moni-
toring. The conductimetric based metal oxide pH sensors work by similar principle 
and they do not require a reference electrode for their operation. The pH sensitive 
semiconductor nanomaterials are mainly used for fabrication of conductimetric pH 
sensors. The H3O+/OH ions in the solution modify the electrical properties of this 
sort of pH sensor, speciﬁcally the resistance and conductance of the SE material. 
The degree of protonation has a signiﬁcant impact on how well certain types of 
pH sensors sense pH. Metal hydroxyl groups (M-OH) are created on the surface of 
metal oxides when they are exposed to solutions due to dissociative chemisorption 
[18]. When metal oxides are added to an aqueous solution, charged surface groups 
such as -O-, -OH, and -OH2+ are produced on the metal surface. Depending on the 
kind of electrolytic solution, H+ or OH- ions are then adsorbed on the oxide surface 
to determine the potential. The potential-determining H+ or OH- ions are adsorbed 
on the oxide surface and form an electrochemical double layer, depending on the 
type of electrolyte solution [19, 20]. The surface becomes hydroxylated because of 
the proton and hydroxide ions’ attraction to the surface cation and lattice oxygen 
in the solution, which formed OH- groups [19, 20]. The concentration of these 
potential-determining ions affects how various electrical properties of the double 
layer (capacitance, resistance, conductance, potential, etc.) change [20]. 
According to research by Lee WS et al., for conductive pH sensors based on TiO2 
nanowires, the conductance value drops linearly with a resolution of 5.68 ± 0.28 nS/ 
pH, as pH increases [16]. Due to reactions with H+/OH− ions, the depletion layer at 
the surface of the TiO2 NWs has increased, causing a drop in conductance [20, 21]. 
There is a very limited research work which is devoted to fabrication of conducti-
metric thick ﬁlm pH sensors using metal oxides. Here we have fabricated metal oxide 
based conductimetric pH sensor using low cost and easy screen-printing technology. 
Novelty of the research is the optimized performance of the metal oxide pH sensor 
and its use for measurement of pH of human sweat. Applicability of the fabricated 
sensor is examined by designing a health nonitoring system based on this pH sensor. 
The change in pH level can be used to monitor the hydration status of athlets or 
patients.

598
V. Pagar et al.
2 
Experimental 
2.1 
Sensor Fabrication 
Initially, thick ﬁlm conductimetric pH sensor was fabricated using Titanium dioxide 
(TiO2), a pH sensitive and highly resistive, low-cost metal oxide. We selected to test 
the performance of pure metal oxide on an alumina substrate. An alumina substrate 
of 15 mm was used, on which a conducting silver paste was screen printed in an 
interdigitated pattern. The ﬁrst step was depositing a planar IDE on it. The electrode 
pattern consists of 2 mm wide ﬁngers with 2 mm distance between the adjacent 
ﬁngers. This silver IDE patterned ﬁlm was ﬁred at 750 °C for 10 min (Figs. 1, 2 and 
3). 
pH sensitive metal oxide Titanium Dioxide and binder Bi2O3 are used in powder 
are weighed and then a ﬁne and homogeneous mixture of TiO2 & Bi2O3 is prepared 
by mixing it well for 30 min in an agate mortar. After that by adding organic vehicle 
into the mixture, a homogeneous thick paste was prepared by mixing it again for 
30 min in an agate mortar. The TiO2 sensitive layer was printed on the top of the 
IDE using screen printing. This Sensitive thick paste is then screen printed on the 
top of the IDE alumina substrate. Then the prepared ﬁlms were ﬁred at 750°C for 
10 min. When the ﬁlms get cooled, resistance of the ﬁlms was measured. As TiO2 
is a highly resistive material, this sensor offered a very high resistance. All the 
three ﬁlms which were prepared had the very high resistance values 255, 514 & 
350 MΩ. Standard pH solutions of 4.0, 7.0 and 9.0 were prepared in 100 ml DI water 
using standard pH capsules. Fabricated sensors were tested using Electrochemical 
Impedance Spectroscopy. 
It is observed that EIS response of these ﬁlms for conductance as well as 
impedance was not linear. TiO2 offered very high resistance, which was difﬁcult
Fig. 1 IDE alumina 
Substrate 
Fig. 2 Tio2 Based Thick 
Film 
Fig. 3 RuO2 & Tio2 Based 
pH Sensor 

Fabrication of Metal Oxide Based Thick Film pH Sensor ...
599
Table 1 Variations in 
resistance of pH sensor 
(80:20) for different pH 
values 
Sr. No
pH Value of testing Solution
Resistance in K ohm 
1
4.0
3.5 K 
2
7.0
1.9 K 
3
9.0
1 K  
to measure, and the EIS response was not acceptable. To overcome this limitation, 
thick ﬁlm was prepared using highly sensitive metal oxide i.e., Ruthenium oxide 
(RuO2), by following the same fabrication steps. Three Films were prepared, and all 
the ﬁlms had resistance about 1.166 Ω which is signiﬁcantly low for the measure-
ments. These thick ﬁlm sensors were tested for pH 4.0 & pH 7.0 but response was 
not linear. As the resistance was too low their variations were not easily detectable, 
and it was difﬁcult to distinguish the resistance variations due to change in pH of the 
solution or due to change in ambient temperature. 
Ruthenium oxide (RuO2) and titanium dioxide (TiO2) were combined to improve 
the performance and address the shortcomings of previously developed sensors. Here 
we combined two metal oxides RuO2 & TiO2 in the ratio 60:40 wt.% after weighing, 
the materials were mixed to obtain homogeneous mixture and the same fabrication 
steps were followed. Resistance of the bare ﬁlm was measured, and it was 3 M Ohm. 
Resistance was not in measurable range & EIS results were not up to the mark. Then 
to obtain the improved performance, RuO2 and TiO2 mixed in ratio 80:20 wt.%. 
Prepared ﬁlms were tested using impedance spectroscopy for different pH values. 
LCR meter is used for electrochemical impedance spectroscopy of the fabricated 
thick ﬁlm pH sensor for 10 Hz–8 MHz frequency range. EIS shown promising 
results for this combination of metal oxides. Hence, we get optimized performance 
when we mix two metal oxides RuO2 & TiO2 in the proportion 80:20 wt.%. Table 1. 
Shows the resistance values of this optimized sensor (Fig. 4). 
Fig. 4 Relative change in 
resistance of fabricated 
sensor for different pH 
values

600
V. Pagar et al.
2.2 
EIS Analysis of the Sensor 
In order to determine an electrochemical cell’s impedance, an AC voltage is typically 
applied, and the cell’s current is then measured. A circuit made up of resistive, 
capacitive, and inductive components has an impedance, which is an opposition 
to the AC current it offers. The impedance is frequency dependent and explains 
underlying electrochemical process in the system. A conductimetric pH sensor’s 
electrical properties are affected by changes to the sensitive layer of the electrode’s 
surface caused due to reaction with the solution (Fig. 5). 
Figure 6 depicts the variation of AC conductance of the fabricated thick ﬁlm pH 
sensor versus frequency in the range 10 Hz–8 MHz for different pH values of the test 
solution. It is observed that there is an increase in conductance with increasing pH 
values in the range of 4 to 9. For the frequencies up to 1 kHz, the conductance for all 
pH values is constant and above 1 kHz, it increases with an increase in frequency. It 
is possible to explain these variations in conductance by the electrochemical surface 
reactivity of H+/OH- ions with the oxide surface in TiO2 and RuO2 based sensors. 
Different electrochemical processes take place at the oxide-solution surface when a 
metal oxide is exposed to an electrolyte [1, 24]. The generation of a surface hydroxyl 
group occurs in the case of TiO2 as a result of the dissociative chemisorption of 
a water molecule [24]. By site binding theory, the charged surface of metal oxides 
develops an electrical double layer structure because of the diffusion of H+/OH- ions 
[25]. Both the proportion of H+ and OH- ions varies when a solution’s pH changes.
It can be observed from the graph (Fig. 7), that impedance exhibits a rapid increase 
in frequency up to 1 kHz. Then for a broad spectrum of 103–106 Hz, it remains 
constant. For lower pH values impedance is higher & decreases with an increase in 
pH value. For higher frequencies above 10 kHz, the impedance is almost equal for all 
the pH values. Hence impedance is more strongly dependent on pH values at lower 
frequencies below 1 kHz. The exhibited dependence is predominantly due to applied 
alkaline solutions’ lower resistance as compared to acidic solutions.
The sensor capacitance for buffer solutions with various pH levels is shown in 
Fig. 8 for different frequencies. At lower frequencies up to 100 Hz, it is seen that 
the capacitance value falls quickly with an increase in frequency. The graph shows
Fig. 5 Experimental arrangement for EIS 

Fabrication of Metal Oxide Based Thick Film pH Sensor ...
601
Fig. 6 Frequency 
dependence of conductance
Fig. 7 Variations of 
Impedance with frequency
10
100
1000
10000
100000 
1000000 
0 
200 
400 
600 
800 
Impedance (Ohm) 
Frequency (Hz)
 pH=4
 pH=7
 pH=9 
that the capacitance increases when pH rises, but at lower frequencies, pH inﬂuences 
capacitance more strongly. In the impedance spectroscopic analysis, the complex 
impedance data of the pH sensor is analysed using a Nyquist plot of the Imaginary 
part of the impedance versus the real part of the impedance. This plot presents the 
resistive, capacitive, and inductive features of electrodes & their chemical reactions. 
Figure 9 depicts a semicircle in the upper-frequency region and a straight line in the 
lower-frequency section. 
The initial straight line in the Nyquist plot depicted in Fig. 9 may have resulted 
out from the diffusion of H+/OH- ions into the electrode surface from the solution. 
After that, there is a sharp decrease in impedance with the increase in pH value. 
The frequency response of the pH sensor was analysed for the frequency range from 
(10 Hz–8 MHz). It can be observed that there is a sharp increase in impedance with 
an increase in frequency and then at a certain frequency it starts decreasing.
Fig. 8 Frequency 
dependence of Capacitance
101
102
103
104
105
106
-2.0µ 
0.0 
2.0µ 
4.0µ 
6.0µ 
8.0µ 
10.0µ 
12.0µ 
14.0µ 
16.0µ 
Capacitance (farad) 
Frequency (Hz)
 PH=4
 pH=7
 pH=9 

602
V. Pagar et al.
Fig. 9 Nyquist plot for pH 
sensor
0.0
200.0
400.0
600.0
800.0
1.0k 
0 
1k 
2k 
3k 
4k 
5k 
6k 
Z_imag (Ohm) 
Z_real (Ohm)
 pH=4
 pH=7
 pH=9 
2.3 
Testing Thick Film pH Sensor Using Test Solutions 
In accordance with the concentration in Table 3, a human sweat equivalent solution 
was prepared by combining NaCl, KCl, and glucose in deionized water. We prepared 
dissolved salt solutions of 10, 50 and 100 mM by dissolving NaCl, KCl, and glucose 
in deionized water to test the electrochemical performance of the sensor under the 
effect of ions and analytes, including Na+, K+, and glucose, which are frequently 
found in sweat. We produced three sets of NaCl concentrations to further explore how 
NaCl affects the electrical properties of the sensor (10, 50 and 100 mM). Variations in 
resistance were measured for different concentrations of NaCl testing solutions. The 
sensor was also examined for the effect of KCL on the sensor by using three sets of 
different concentrations. Then it was tested for an artiﬁcial sweat solution containing 
its major constituents, NaCl, KCl & Glucose. Signiﬁcant changes in resistance of the 
fabricated pH sensor were observed for different concentrations of equivalent sweat 
test solutions. It is seen that the resistance of the ﬁlm decreases with an increase in 
concentration from 10 to 100 mM. 
were measured for different concentrations of NaCl testing solutions. The sensor 
was also examined for the effect of KCL on the sensor by using three sets of different 
concentrations. Then it was tested for an artiﬁcial sweat solution containing its major 
constituents, NaCl, KCl & Glucose. Signiﬁcant changes in resistance of the fabri-
cated pH sensor were observed for different concentrations of equivalent sweat test 
solutions. It is seen that the resistance of the ﬁlm decreases with an increase in 
concentration from 10 to 100 mM (Table 2).
Human sweat pH measurement system using fabricated conductometric pH 
sensor: 
To use pH sensor for human sweat monitoring, a system is used which contains 
a Wheatstone’s bridge circuit, instrumentation ampliﬁer and its output is connected 
to microcontroller system Arduino nano (Fig. 10).
In this investigation, we assumed that there would be enough sweat for in-situ 
analysis. 
Typically, in-situ analysis requires >10 µl of sweat [23]. Even this quantity might 
not be available in the case of those leading sedentary lifestyles. Such situations 
could be treated with on-demand sweat extraction techniques such “iontophoresis”.

Fabrication of Metal Oxide Based Thick Film pH Sensor ...
603
Table 2 Response of thick 
ﬁlm sensor for artiﬁcial sweat
Test Solution
Concentration
Resistance of sensor 
Before
4.32 K 
NaCl
10 mM
1.46 K 
50 mM
1.1 K 
100 mM
0.66 K 
KCl
10 mM
2.34 K 
50 mM
1.21 K 
100 mM
1.63 K 
NaCl+KCl+Glucose
10 mM
1.22 k 
50 mM
0.83 K 
100 mM
0.48 K
Fig. 10 Block diagram of pH measurement system
3 
Conclusion 
In this work a fabrication and characterization of thick ﬁlm interdigitated TiO2 and 
RuO2-based conductive pH sensor is presented. Initially, we have fabricated a pH 
sensor using low-cost metal oxide TiO2, but due to its very high resistive property, 
it was not showing promising results. Then we fabricated a sensor using a highly 
sensitive metal oxide RuO2, but these ﬁlms were showing very low resistance, there-
fore we mixed these two metal oxides and tried various mixing proportions. We 
have got optimized performance for thick ﬁlm pH sensor when we mix two metal 
oxides RuO2 & TiO2 in the proportion 80:20 wt.%. This optimized conductometric 
pH sensor is further tested for the measurement of sweat pH. The outcome demon-
strated that both, impedance, conductance, and capacitances are frequency and pH 
dependent. The impedance spectroscopy analysis suggests that, in the low-frequency 
band, the electrical characteristics of the sensor greatly depend on the pH of the solu-
tion. As compared to other previous techniques in the literature, the constructed IDE 
pH sensor has following advantages: faster, less expensive fabrication & small size. 
Further work is needed to fabricate this sensor on a ﬂexible substrate which will 
enhance its use for continuous monitoring of sweat pH.

604
V. Pagar et al.
References 
1. Manjakkal L, Cvejin K, Kulawik J, Zaraska K, Szwagierczak D, Socha RP (2014) Fabrication 
of thick ﬁlm sensitive RuO2-TiO2 and Ag/AgCl/KCl reference electrodes and their application 
for pH measurements. Sens Actuators, B Chem 204:57–67 
2. Dang W, Manjakkal L, Navaraj WT, Lorenzelli L, Vinciguerra V, Dahiya R (2018) Stretchable 
wireless system for sweat pH monitoring. Biosens Bioelectron 107:192–202 
3. Glanc-Gostkiewicz M, Sophocleous M, Atkinson JK, Garcia-Breijo E (2013) Performance of 
miniaturised thick-ﬁlm solid state pH sensors. Sens Actuators, A 202:2–7 
4. Nikolajek WP, Emrich HM (1976) pH of sweat of patients with cystic ﬁbrosis. Klin Wochenschr 
54(6):287–288 
5. Manjakkal L, Vilouras A, Dahiya R (2018) Screen printed thick ﬁlm reference electrodes for 
electrochemical sensing. IEEE Sens J 18(19):7779–7785 
6. Uppuluri K, Lazouskaya M, Szwagierczak D, Zaraska K, Tamm M (2021) Fabrication, poten-
tiometric characterization, and application of screen-printed RuO2 pH electrodes for water 
quality testing. Sensors 21(16):5399 
7. Simi´c M, Manjakkal L, Zaraska K, Stojanovi´c GM, Dahiya R (2016) TiO2-based thick ﬁlm 
pH sensor. IEEE Sens J 17(2):248–255 
8. Kurzweil P (2009) Metal oxides and ion-exchanging surfaces as pH sensors in liquids: state-
of-the-art and outlook. Sensors 9:4955–4985 
9. Vonau W, Guth U (2006) pH monitoring: a review. J Solid State Electrochem 10:746–752 
10. Mayousse E, Maillard F, Fouda-Onana F, Sicardy O, Guillet N (2011) Synthesis and character-
ization of electrocatalysts for the oxygen evolution in PEM water electrolysis. Int J Hydrogen 
Energy 36:10474–10481 
11. da Silva GM, Lemos SG, Pocrifka LA, Marreto PD, Rosario AV, Pereira EC (2010) Develop-
ment of low-cost metal oxide pH electrodes based on the polymeric precursor method. Anal 
Chim Acta 616:36–41 
12. Chen Y, Mun SC, Kim J (2013) A wide range conductometric pH sensor made with titanium 
dioxide/multiwall carbon nanotube/cellulose hybrid nanocomposite. IEEE Sensor. J. 13:4157– 
4162 
13. Osman JR, Crayston JA, Pratt A, Richens DT (2008) RuO2-TiO2mixed oxides prepared from 
the hydrolysis of metal alkoxides. Mater Chem Phys 110:256–262 
14. Liao Y-H, Chou J-C (2008) Preparation and characteristics of ruthenium dioxide for pH array 
sensors with real-time measurement system. Sens Actuators B: Chem 128:603–612 
15. Manjakkal L, Szwagierczak D, Dahiya R (2020) Metal oxides based electrochemical pH 
sensors: current progress and future perspectives. Prog Mater Sci 109:100635 
16. Lee WS, Park Y-S, Cho Y-K (2014) Hierarchically structured suspended TiO2 nanoﬁbers for 
use in UV and pH sensor devices. ACS Appl Mater Interfaces 6:12189–12195 
17. Patterson M, Galloway S, Nimmo MA (2000) Variations in regional sweat composition in 
normal human males. Exp Physiol 85:869–876 
18. Blesa MA, Weisz AD, Morando PJ, Salﬁty JA, Magaz GE, Regazzoni AE (2000) The interaction 
of metal oxide surfaces with complexing agents dissolved in water. Coord Chem Rev 196(1):31– 
63 
19. Al-Hilli S, Willander M (2009) The pH response and sensing mechanism of n-type ZnO/ 
electrolyte interfaces. Sensors 9(9):7445–7480 
20. Manjakkal L, Djurdjic E, Cvejin K, Kulawik J, Zaraska K, Szwagierczak D (2015) Electro-
chemical impedance spectroscopic analysis of RuO2 based thick ﬁlm pH sensors. Electrochim 
Acta 168:246–255 
21. Lee WS, Park YS, Cho YK (2014) Hierarchically structured suspended TiO2 nanoﬁbers for 
use in UV and pH sensor devices. ACS Appl Mater Interfaces 12189–12195 
22. Lin K et al (2022) Self-adhesive and printable tannin–graphene supramolecular aggregates for 
wearable potentiometric pH sensing. Electrochem Commun 137:107261 
23. Nyein HYY et al (2018) A wearable microﬂuidic sensing patch for dynamic sweat secretion 
analysis. ACS Sens 3(5):944–952

Fabrication of Metal Oxide Based Thick Film pH Sensor ...
605
24. Fog A, Buck RP (1984) Electronic semiconducting oxides as pH sensors. Sensors Actuators 
B-Chem 5:137–146 
25. Yates DE, Levine S, Healy TW (1974) Site-binding model of the electrical double layer at the 
oxide/ water interface. J Chem Soc Faraday Trans 1 170:1807–1818

Reliable Data Delivery in Wireless Sensor 
Networks with Multiple Sinks and 
Optimal Routing 
Vasavi Junapudi and Siba K. Udgata 
Abstract Wireless sensor networks are equipped with energy constrained and lim-
ited communication range sensor nodes to transmit the sensed data. The constraints 
made prolonged network lifetime is one of the major concerns while dealing with 
the energy hole problem. In the presence of nodes whose behaviour is unpredictable 
as a relay node (unreliable nodes/URNodes) will raise the concern about reliable 
data delivery and makes enhancement of network lifetime difﬁcult. In this paper, 
the energy hole problem is addressed by looking for an Alternate Path (AP) by 
availing the residual energy of other sensor nodes who involves in data transmission 
rarely. In the presence of URNodes with various percentage of unreliability, the AP 
approach reaches almost same network lifetime when compared with the network 
without URNodes. Experiments have been done with increased density of the net-
work sensor nodes with varied number of unreliable node (URNodes) with various 
percentage of unreliability. The main concern of the work is reliable data delivery 
with extended network lifetime. So we also considered the adjustable communica-
tion range to ensure reliable data delivery. With an adjustable communication range, 
along with the enhanced network lifetime, the total number of dropped messages due 
to URNodes are also got reduced. 
Keywords Multiple sinks · Reliability · CASS · Alternate Path (AP) · Hot spot ·
Energy Hole 
1 
Introduction 
A sensor is an energy-constrained tiny device, uses wireless media such as radio, 
infrared, and optical media frequency to transmit the sensed data to the Data Col-
lector/Sink [ 1]. But the efﬁciency of a sensor encourages to use in wide variety of 
applications where the human can’t reach out [ 2, 3]. Due to its energy constraint, 
V. Junapudi (B) · S. K. Udgata 
School of Computer and Information Sciences, University of Hyderabad, Hyderabad, India 
e-mail: vasavi.singh@gmail.com 
S. K. Udgata 
e-mail: udgata@uohyd.ac.in 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
S. K. Udgata et al. (eds.), Intelligent Systems, Lecture Notes in Networks and Systems 
728, https://doi.org/10.1007/978-981-99-3932-9_52 
607

608
V. Junapudi and S. K. Udgata
the prolonged network lifetime became a primary concern and particularly in the 
presence of URNodes whose behaviour is unpredictable during data transmission. 
Therefore, the energy of sensor nodes must be utilized efﬁciently to enhance the net-
work lifetime while delivering the data in a reliable way while reducing the message 
drop due to unreliable behavior of URNodes. 
Using the multiple sinks and routing the data transmission are two popular meth-
ods to enhance the network lifetime. Deploying multiple sinks will increase the 
network lifetime at the increased sink maintenance cost. Cluster based Algorithm 
for Sink Selection (CASS) is the approach where a single sink is allowed to be active 
based on the network reach of the sink. The approach handled the extension of the 
network lifetime by minimizing the maintenance cost. But, energy depletion of the 
hot spots (energy hole), leaves network non-functional and many nodes left with 
their residual energy get unused. The Alternate Path (AP) is the approach addresses 
the energy hole problem efﬁciently to enhance the network lifetime by exploring the 
other possible path to reroute the data to sink. Doing this hot spots can be avoided 
in data transmission. 
In a dense network, some relay nodes can’t behave as reliable in data transmission 
due to various reasons such as heavy trafﬁc in the network, buffer overﬂow due to 
limited memory size of a sensor node. Such nodes referred as URNodes. The reliable 
data delivery depends on the number of URNodes and how much percentage it is 
behaving as unreliable during its lifetime. The AP approach with URNodes is able 
to reach the network lifetime as the lifetime of the network without URNodes. 
2 
Literature Survey 
To extend the network lifetime, deploying multiple sinks is one important approach 
among many approaches as reported in the literature. With the help of multiple; 
sinks, the sensor can send the data to its nearest sink so that the energy consumption 
gets reduced, and in turn, it improves the network lifetime [ 4– 6]. But, deploying 
multiple sinks increases the cost to maintain sinks. Mobile sink concept is one of the 
alternatives to minimize energy consumption, increasing the network lifetime and to 
reduce maintenance cost of multiple sinks also. One of the main challenges raised 
in mobile sinks is the quick update of the sink’s new location to sensor nodes so that 
the sensed data can reach the sink in the stipulated time [ 7]. To achieve this, one of 
the good options is to ﬂood [ 8] the new sink location which increases the trafﬁc ﬂow 
and leads to congestion, message drops, and so on. 
To reduce the sink maintenance cost without trafﬁc congestion [ 9] comes with the 
Cluster-based Algorithm for Sink Selection (CASS) model to shift the sink among 
multiple sinks which means deploying multiple sinks but activating a single sink 
based on the network reach-ability to the sink. The sink with the highest network 
reach will be the next active sink. The sink with a large cluster, more the network 
reach. With this approach, the maintenance cost will be reduced as a single sink 
will be active at a time and multiple sinks deployment allows the network lifetime

Reliable Data Delivery in WSN with Multiple Sinksellipsis. . .
609 
enhancement. But, CASS ends with many nodes residual energy get unused due to 
depletion of hot spots energy making the energy holes in the network. 
Hot spots are the nodes which involve more frequently in data transmission. 
Energy hole is created due to energy depletion of the hot spots as they are participating 
more frequently in data transmission. The neighbor nodes of the sink and other relay 
nodes which are greatly involves in data transmission comes under this category. 
An energy hole makes the network into multiple isolated chunks which makes the 
network non-functional due to dead hot spots. The energy depletion of hot spots 
affects the network lifetime because even though the nodes with a good amount of 
residual energy can’t send the sensed data as there is no relay node to forward it to 
the sink [ 1]. Instead of allowing the hot spots to drain quickly, avoid using the crucial 
nodes to forward the data by rerouting it through an alternate path to reach the sink. 
By doing this 
– the hot spots will continue to run for longer time. 
– network will not get isolated as no node is dead. 
– network lifetime will be enhanced. 
– utilizing the residual energy of the other nodes who participate in forwarding rarely 
to reroute the data in the Alternate path (AP) 
Reliability is one of the main factors for ensuring safe packet delivery to the sink. 
To ensure reliable data delivery several approaches are proposed in earlier literature. 
Few of them are end-to-end and hop-to-hop methods to recover lost data. In an end-
to-end method, the source and destination have to take responsibility to recover the 
data. Until the source receives an explicit ACK (eACK) from the sink, the source 
needs to maintain a copy of the message so that it can be used for data recovery. In a 
dense network with limited buffer capacity, it is difﬁcult to store the copy of multiple 
messages until the source gets ACK from the sink. Explicit ACK also increases the 
trafﬁc ﬂow in the network. 
In a hop-to-hop method, every hop node takes responsibility in delivering the 
data. Every hop maintains a copy of the messages until the data is forwarded by 
the next relay node. The broadcasting nature helps in monitoring the relay node’s 
behavior in forwarding the message. The implicit ACK (iACK) ensures reliable data 
delivery and also does not burden the network with trafﬁc for acknowledgments and 
also addresses the internal buffer problem which arises in the eACK [ 10, 11]. 
3 
System Model 
3.1 
Assumptions 
– Sensor Nodesupper SS are deployed randomly ensuring all sensor nodes can communicate 
at least one sink 
– Sensor nodeupper SS have limited energy and initial energyupper EE is same for all sensor nodes

610
V. Junapudi and S. K. Udgata
– Sink nodes upper NN with inﬁnite energy and can communicate with the outer world 
– Sensors identify their neighbors based on the Euclidean distance method 
• If d i s t left parenthesis u comma v right parenthesisdist(u, v) fall under the given communication range then node uu and vv are 
neighbors to each other 
• otherwise they are not neighbors and communication can be established only 
by using multi-hopping. 
– All sensor nodes equipped with iACK technique and hop-to-hop reliability 
– Random set of nodes declared as unreliable nodes (the nodes whose behavior is 
unpredictable as a relay node) 
– The energy consumption between any two nodes is considered based ond i s t left parenthesis u comma v right parenthesisdist(u, v)
– Lifetime of a network is deﬁned as the total working time of the network till all 
nodes are unable to communicate with any of the sink. In brief, the number of 
messages delivered to all sinks is the metric to measure the network lifetime [ 1]. 
– Initial path: The source follows the shortest path to reach the sink among the many 
available paths. 
– Alternate Path (AP): New path from the relay node to the sink to avoid using 
residual energy of the crucial nodes/hot spot. By doing this, the hot spot life can 
be extended and energy hole creation can be delayed. 
3.2 
Network Model 
The network is modeled as a graph upper G left parenthesis upper V comma upper E right parenthesisG(V, E), where upper VV is the set of all sensor nodes 
sayupper NN, and the base stations/sink nodesupper SS i.e.upper V equals upper N union upper SV = N ∪S. The sensor nodes and sink 
nodes are dropped randomly in the target region. Sink nodes start broadcasting a hello 
message to ﬁnd the reachable nodes in the network. In this process, the sensor nodes 
come to know about their neighbor nodes and establish a wireless sensor network. 
Considering the sensor (referring to source) say nodevv is not in the communication 
range to reach the sink then multi-hop communication is used for data transmission. 
With multi-hopping, the source vv will ﬁnd multiple paths through each neighbor to 
reach the sink sayupper P 1 comma upper P 2 comma period period period comma upper P Subscript k BaselineP1, P2, ..., Pk, wherek greater than or equals 1k ≥1. Among all possible paths, the source 
node will consider the shortest path (in number of hops) sayupper P Subscript jPj where1 less than or equals j less than or equals k1 ≤j ≤k for 
minimal usages of energy due to energy constraint, and the chosen path is referred 
as an initial path. Like this, many source nodes will have multiple paths to reach 
the sink. Considering a graph with six nodes say A, B, C, D, E and F (this naming 
applied to the adjacency matrix considering from top to bottom and left to right). 
Selecting AP for reliable data delivery is illustrated using the adjacency matrix given 
below.

Reliable Data Delivery in WSN with Multiple Sinksellipsis. . .
611 
upper M equals Start 6 By 6 Matrix 1st Row 1st Column 0 2nd Column 1 3rd Column 1 4th Column 0 5th Column 0 6th Column 0 2nd Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 1 5th Column 0 6th Column 0 3rd Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 1 5th Column 0 6th Column 1 4th Row 1st Column 0 2nd Column 1 3rd Column 1 4th Column 0 5th Column 1 6th Column 1 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 5th Column 0 6th Column 1 6th Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 1 5th Column 1 6th Column 0 EndMatrixM =
⎡
⎢⎢⎢⎢⎢⎢⎣
0 1 1 0 0 0
1 0 1 1 0 0
1 1 0 1 0 1
0 1 1 0 1 1
0 0 0 1 0 1
0 0 1 1 1 0
⎤
⎥⎥⎥⎥⎥⎥⎦
“1” refers the nodes corresponding row and column are in communication range and 
“0” indicates the nodes can’t reach each other directly. Considering the node A as a 
source and F as a sink, then using the above adjacency matrix the number of paths 
to reach the sink are 11. Among the explored paths 
upper A right arrow upper C right arrow upper FA →C →F
is the shortest path which involves only one relay node to reach the sink F from 
source A and considers as initial path. 
Once the network is established, the CASS approach will select a sink to be 
active. After establishing the network, all sinks will have good network connectivity, 
so no other criterion is considered to select the initial sink. Before starting data 
transmission, an energy threshold is set, so that the node that fails to satisfy the 
energy threshold condition is not allowed to participate in data forwarding. By doing 
this, the AP approach is not allowing the hot spots to drain its energy. Once all nodes 
fail to satisfy the energy threshold level, the threshold is reduced to half of its previous 
value [ 1]. 
Similarly, a random set of sensor nodes are considered as unreliable nodes (URN-
odes). The network will take care of URNodes not to behave as unreliable more than 
the deﬁned percentage of unreliability. 
3.3 
Energy Model 
An energy model on radio characteristics including energy consumption in transmit-
ting and receiving models to send a kk-bit message to a distance dd using the radio 
model is described in [ 1] 
In our simulations, we assumed that 
– The radio channel is symmetric, i.e., the energy consumption to transmit the data 
from node A to node B is the same as from node B to node A. 
– The nodes in the network would send the sensed data periodically to the sink [ 2].

612
V. Junapudi and S. K. Udgata
3.4 
CASS 
In [ 9], the authors proposed a method Cluster based Algorithm for Sink Selection 
(CASS) to handle increases in maintenance cost due to multiple sinks deployment 
for enhancing the lifetime of a network. CASS is the approach to activate a single 
sink based on the reachable of the network. Every sink node comes up with its reach-
ability to the network and the sink with the highest network reach will be considered 
to be an active sink. By allowing a single sink active at the time the maintenance 
cost will be reduced as well as multiple sinks are involved to receive the data, hence 
enhancing the network lifetime. To select the sink with high reach-ability to the 
network, the shift rate parameter plays an important role. It is deﬁned as below. 
StartLayout 1st Row upper S h i f t r a t e Subscript upper T Sub Subscript n Baseline equals StartFraction left parenthesis upper N u m b e r o f n o d e s c o n n e c t e d t o s i n k right parenthesis Subscript t Sub Subscript j Subscript Baseline Over left parenthesis upper N u m b e r o f n o d e s c o n n e c t e d t o s i n k right parenthesis Subscript t Sub Subscript i Subscript Baseline EndFraction EndLayoutShi f t rateTn =
(Number of nodes connected to sink )t j
(Number of nodes connected to sink )ti
where, t Subscript iti and t Subscript jt j are the time instance at ii and j j intervals, i less than ji < j and i not equals ji /= j
The CASS approach doesn’t deal with the energy hole problem. In this paper, the 
energy hole problem is addressed by taking the advantage of CASS in maintenance 
cost reduction with multiple sinks deployment and also keeping the congestion over-
head in consideration for reliable message delivery, we come up with Alternate path 
routing for Reliable Data Delivery in the presence of unreliable nodes with their 
unpredictable nature. 
3.5 
Alternate Path Model 
An alternate path comes into the picture when a relay node in the initial path is 
not satisfying the energy threshold requirement then the sender node reroutes the 
messages to the sink avoiding the relay node in further forwarding. By doing this 
we can delay the energy hole creation by not involving the relay node (hot spot) in 
message forwarding. Consider a source,upper AA follows the shortest path to reach the sink 
upper SS using h Subscript ihi, h Subscript jh j, h Subscript khk and h Subscript lhl as relay nodes and also referred as senders. 
upper I n i t i a l upper P a t h colon upper A right arrow h Subscript i Baseline right arrow h Subscript j Baseline right arrow h Subscript k Baseline right arrow h Subscript l Baseline right arrow upper S i n kInitial Path : A →hi →h j →hk →hl →Sink
say h Subscript ihi received a message from upper AA. Before forwarding the message to h Subscript jh j, the node 
h Subscript ihi checks the residual energy of the next node i.e., h Subscript jh j. If  h Subscript jh j satisﬁes the energy 
threshold requirement then the senderh Subscript ihi forwards the data to the next relay nodeh Subscript jh j. 
Otherwiseh Subscript ihi checks with other neighbor nodes to reroute the message to sink so that 
h Subscript jh j is not involved in forwarding. 
By doing this the lifetime of the hot spot h Subscript jh j is extended. It means delaying the 
energy hole and extending the network lifetime. By rerouting with AP, congestion 
in the network is avoiding and utilizing the residual energy of other nodes who 
participate rarely in data transmission.

Reliable Data Delivery in WSN with Multiple Sinksellipsis. . .
613 
3.6 
Reliability Model 
In the beginning of the network setup, randomly few nodes are considered as a set 
of unreliable nodes with the predeﬁned percentage of unreliability. 
Percentage of Unreliability: A node behaving unreliable for the predeﬁned per-
centage of time during its lifetime. For example a node upper U upper N Subscript iU Ni is deﬁned as URNode 
with percentage of unreliability as 15. It means that during it lifetime the node upper U upper N Subscript iU Ni
behaves as unreliable for 15% of its lifetime and remaining 85% of the time it behaves 
reliable. The reliable and unreliable behaviour of theupper U upper N Subscript iU Ni is unpredictable. Internally 
the network will take care of the unreliable node not to behave as unreliable beyond 
the deﬁned percentage of unreliability. 
3.7 
Adjustable Communication Range 
This paper addresses reliability in data transmission in the presence of URNodes 
means need to keep a count on minimizing the message drop, along with the net-
work lifetime enhancement. To minimize the message drop, we come up with a 
variation saying adjustable communication range. This variation applied next to AP 
with reliability model (AP-R). Sender will adjust its communication range to skip 
the unreliable node so that can reach the next next relay node when AP not found. 
By adjusting the communication range, the reliable data delivery can happens means 
prolonging network lifetime and minimizing the message drop. In [ 12, 13], adjusting 
the communication range can extend the lifetime of the network, delaying the energy 
hole and reduction in message drop. Hence makes the network more reliable. 
4 
Proposed Method 
All the sensor nodes start sending their sensed data to the sink if every hop node 
satisﬁes the energy threshold condition while considering URNodes with the deﬁned 
percentage of unreliability. The AP approach presented in Algorithm 1 is executed 
irrespective of the nature of the nodes. If any of the nodes in the initial path not 
satisfying the threshold criterion, look for AP with the node satisfying the threshold 
which can route to sink. Choose the shortest among all available alternate paths (AP). 
If no AP found for the above condition, we can op any of the following based on the 
capacity and availability of the sender node. 
– Either we can drop the message if buffer space is not available [ 14] 
– keep the message for aggregation if the buffer space is available 
– if the application is not time-sensitive then we can keep it for later to resend

614
V. Junapudi and S. K. Udgata
Once all nodes are unable to reach the sink with the deﬁned threshold then reduce 
to half of its value. This process will be repeated until the threshold value of energy 
reaches 10% of the initial energy. After this, the network allows the hot spot to use 
its residual energy. 
The Algorithm 2 explains how to deal when an unreliable node is encountered 
and showing unreliable behavior. If the initial path encounters a node with unreliable 
behavior then the sender node will look for AP to sink. If the sender ﬁnds a shortest 
AP among the explored possible path to sink then the path will be updated and the 
data transmission continues as per AP. If sender not able to ﬁnd AP then the message 
drop will be counted due to unreliable behaviour of the node in the path. Based on 
the availability, either the sender will update the path information and reroute the 
message for data transmission or the message drop to be considered due to unreliable 
behavior of the node. The nodes in the network continue to follow the two algorithms 
Algorithm 1 and Algorithm 2 till all sinks network reach comes to 1. It means no 
more reorganization is beneﬁcial to extend the network lifetime. 
The adjustable/increased communication range (ICR) approach is considered as a 
variation in AP with Reliable Data Delivery (AP-R). If no AP found after triggering 
node with unreliable behavior, the sender node will adjust its communication range 
to reach the next node to relay node by skipping the relay node. Doing this, the sender 
avoids message dropping. By doing this, the network 
Algorithm 1. Alternate Path 
Input: threshold_energy[ ],path[ ],AP[ ][ ],neigh[ ][ ], sender hi , relay_node h j 
Output: AP-R[ ] ,drop_AP,drop_UR 
//Alternate path routing to delay hot spots in the network 
1: repeat 
2:
sender hi check the threshold condition against relay node h j 
3:
if h j is satisfying threshold_energy[ ] then 
4:
update the energy 
5:
discard the copy from hi internal buffer 
6:
else 
7:
look for shortest AP 
8:
if AP found then 
9:
update the path[ ] 
10:
else 
11:
update drop_AP[ ] //Dropped message count for no AP found 
12:
end if 
13:
end if 
14: until the data packet received by the sink 
15: return drop_AP

Reliable Data Delivery in WSN with Multiple Sinksellipsis. . .
615 
Algorithm 2. Reliability model 
Input: URNodes[ ],perc_unreliability,path[ ],AP[ ][ ],neigh[ ][ ], sender hi , relay_node h j 
Output: AP-R[ ] ,drop_UR 
//Reliable transmission of data packet with rerouting to shortest AP 
1: repeat 
2:
After relay node h j satisﬁes the threshold 
3:
Check whether the hop node h j is reliable or not 
4:
if h j is reliable
forward the data packet
then 
5:
discard the copy from hi internal buffer 
6:
else 
7:
if h j behaves as reliable
forward the data packet
then 
8:
discard the copy from hi internal buffer 
9:
else 
10:
look for shortest AP 
11:
if AP found then 
12:
update the path[ ] 
13:
else 
14:
update drop_AP[ ] //Dropped message count for no AP found 
15:
end if 
16:
end if 
17:
end if 
18: until the data packet received by the sink 
19: return drop_U R  
lifetime is enhanced. The experiments have been done on this variation (AP-R-ICR) 
by considering hop skip to 1 node and 2 nodes. If the receiver nodes behaves unreliable 
after adjusting the communication range to skip single hop then sender checks with 
hop skip count and proceeds either to drop and to skip 2 hops. By skipping 2 hops, 
the network achieved zero message droppings. In case, with hop skip 1 the sender 
couldn’t make data forwarding then count it as dropping because of unreliability 
d r o p normal bar upper U upper Rdrop_U R. 
5 
Experimental Results 
The experiments were done in MATLAB with variable size of networks deployed 
in a 100times×100 grid. While running the simulation, all the time 1 Superscript s t1st sink is consid-
ered as an initial active sink. The simulation starts with randomly selected set of 
unreliable nodes. Other than the declared nodes remaining all nodes including sink 
nodes are considered to be reliable in nature. Experiments have been done on 40 test 
cases. Different data sets are generated by increasing number of sinks with respect 
to network density. Detailed parameters information is provided in Table 1.
The
 
experiments have been done on 100 sensors with 4 sinks, 200 sensors with 6 sinks, 
and 300 sensors with 8 sinks. The unreliable nodes are considered as 5%, 10%, 15%, 
20%, and 25% of the total number of deployed sensor nodes with 10%, 20%, 30%, 
40% and 50% as percentage of unreliability.

616
V. Junapudi and S. K. Udgata
Table 1 Input Data Information 
Grid Size
100times×100 
Initial Energy of Sensors 
10 J 
No. of Sensors
100, 200, 300 
No. of Sinks
4, 6, 8 
Commn. Range
15 units 
Shift Rate
90% 
URNodes
5%–25% on total deployed sensor nodes 
Node Unreliability
10%–50% of the sensor nodes 
A comparative study on the lifetime of the network is presented between the AP 
and AP-R. Alternate path (AP) approach in which only the energy hole problem 
with reduced maintenance cost using CASS is addressed and considers all nodes 
are reliable. Alternate path approach with reliability (AP-R) by considers a set of 
URNodes with varied percentage of unreliability while availing beneﬁts of AP. 
Figure 1a is showing comparison between AP and AP-R with 100 sensors and 
4 sinks by considering 5% of URNodes with increased percentage of unreliability 
from 10% to 50%. With 5% of URNodes and 10% of unreliability, AP-R delivering 
the same number of messages as AP. With the increased percentage of unreliability 
the network lifetime gets reduced by 1.5% than AP. Figure 1b and Fig. 1c shows  the  
network lifetime with more density i.e., 200 and 300 sensors with 6 and 8 sinks 
respectively. The network lifetime falls in the denser networks which are 0.58% and 
0.5% than the network without unreliable nodes. From this can be conclude that the 
approach AP not only extending the life of hot spots and also maintain almost same 
lifetime as the network without URNodes. It works more effectively in the dense 
network as the approach explores more alternate path for reliable data transmission. 
Figure 2a shows the comparison of network lifetime between the AP-R and AP-R-
ICR with variable network density and allowing the sender to skip a single hop. The 
inputs considering are variable number of URNodes varying from 5% to 25% with 
50% of unreliability. It means the selected nodes behave reliable for half of its life-
time and half time as unreliable. With such 50-50 chance of unpredictable behavior 
from URNodes, the adjustable communication range variation shows improvement 
in extending the network life time. 
The Fig. 2b showing the test results of 5 different test case networks with increased 
density of sensor nodes and number of sinks. In all the scenarios, with increased 
number of URNodes and 50% of unreliability as input for deﬁning unpredictable 
nature of URNodes, AP-R-ICR with single hop skip performing well in enhancing 
the lifetime and also reduction in message drop (happen to occur due to URNodes 
with its unpredictable behavior) than AP-R which has been shown in Fig. 2b.

Reliable Data Delivery in WSN with Multiple Sinksellipsis. . .
617 
89 
90 
91 
92 
93 
94 
95 
96 
97 
98 
1 
2 
3 
4 
5 
6 
7 
8 
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 
Network Lifetime 
Test Cases with 100 sensors and 4 sinks 
AP Vs AP - Reliability with 5% of unreliable nodes varying percentage of 
Unreliability 
Alternate Path (AP) 
AP-R 10% - Unreliability 
AP-R 20% - Unreliability 
AP-R 30% - Unreliability 
AP-R 40% - Unreliability 
AP-R 50% - Unreliability 
(a) 100 sensors and 4 sinks 
87 
88 
89 
90 
91 
92 
93 
94 
1
2
3
4
5 
Network Lifetime 
Test Cases with 200 sensors and 6 sinks 
AP Vs AP-R with 5% of UR nodes by varying 
percentage of Unreliability 
AP 
AP-R 10% Unreliability 
AP-R 20% Unreliability 
AP-R 30% Unreliability 
AP-R 40% Unreliability 
AP-R 50% Unreliability 
(b) 200 sensors and 6 sinks 
84 
85 
86 
87 
88 
89 
90 
91 
92 
1
2
3
4
5 
Network Lifetime 
Test Cases with 300 sensors and 8 sinks 
AP Vs AP - R 5% of UR nodes by varying 
percentage of Unreliability 
AP 
AP - R 10% - Unreliability 
AP  -R 20% - Unreliability 
AP - R 30% - Unreliability 
AP - R 40% - Unreliability 
AP - R 50% - Unreliability 
(c) 300 sensors and 8 sinks 
Fig. 1 Lifetime comparison between AP and AP-R 
With Adjustable communication range, the drop message count also reduced 
almost less than 0.5% with single hop skip. The Fig. 2b clearly shows how well 
the new variation reduced the dropping rate even with increased URNodes and the 
network density. 
78 
80 
82 
84 
86 
88 
90 
92 
94 
96 
98 
1
2
3
4
5
6
7
8
9
10 
11 
12 
13 
14 
15 
Lifetime 
Test Cases with variable sensors and sinks 
AP-R Vs AP-R-ICR with 50% of unreliability 
AP-R 5% URNodes 
AP-R-ICR 5% URNodes 
AP-R 10% URNodes 
AP-R-ICR 10% URNodes 
AP-R 15% URNodes 
AP-R-ICR 15% URNodes 
AP-R 20% URNodes 
AP-R-ICR 20% URNodes 
AP-R 25% URNodes 
AP-R-ICR 25% URNodes 
(a) Lifetime 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
1
2
3
4
5
6
7
8
9 
10 11 12 13 14 15 
Dropped messages due to Unreliability 
Test cases with varying sensors and sinks 
AP-R Vs AP-R-ICR with 50% of unreliability 
AP-R 5% URNodes 
AP-R-ICR 5% URNodes 
AP-R 10% URNodes 
AP-R-ICR 10% URNodes 
AP-R 15% URNodes 
AP-R-ICR 15% URNodes 
AP-R 20% URNodes 
AP-R-ICR 20% URNodes 
AP-R 25% URNodes 
AP-R-ICR 25% URNodes 
(b) Dropped Messages 
Fig. 2 AP-R with AP-R-Inc. Commn.Range (AP-R-ICR)

618
V. Junapudi and S. K. Udgata
6 
Conclusions and Future Scope 
This paper describes how extent the residual energy can be utilized using the approach 
Alternate Path. By involving other nodes in data transmission as relay nodes, the net-
work lifetime can be extended as well as the formation of energy hole can be delayed. 
Adjustable communication range is used to ensure the reliability of the network in 
the presence of variable number of URNodes with varied percentage of unreliable 
nature. By availing the beneﬁts of the approaches, experiments have been done on 
different kind of dense networks with different number of URNodes varying the 
percentage of unreliability. The results has shown that, increase in URNodes and 
behavior of Unreliability nature, the method AP-R well tried to reach lifetime as 
AP without URNodes and achieved with little fall ranging from 1.5% to 6.9% with 
increase in URNodes and percentage of unreliability. But, with Adjustable commu-
nication range variation, the network lifetime only reduced from 0.02% to 0.47% 
with variable input parameters. The dropped messages due to unreliable behavior of 
URNodes and the network failed to reroute the messages is also reduced almost less 
than 0.5% than AP-R approach by skipping one hop to reach the next next node of 
message forwarder. With hop skip count 2, the AP-R-ICR achieved zero message 
drops with prolonged network lifetime. These output parameter shows the AP is a 
good approach to delay the energy holes and delivering the data messages in a reli-
able way without increasing the trafﬁc ﬂow in the network. The variation adjustable 
communication range also increases the lifetime than AP-R and also reduces the 
number of dropped messages due to unreliable nature shown by URNodes. 
References 
1. Junapudi V, Udgata SK (2018) Lifetime maximisation of wireless sensor networks with multiple 
sinks using multiple paths and variable communication range. Int J Sens Netw 26(3):200–211. 
Indersceince Publisher (IEL) 
2. Lotf JJ, Ghazani SHHN (2011) Overview on wireless sensor networks. J Basic Appl Sci Res 
11(1):2811–2816 
3. Wang Q, Balasingham I (2010) Wireless sensor networks-an introduction. In: Tan YK (ed) 
Wireless sensor networks: application-centric design. INTECH Open Access Publisher. https:// 
doi.org/10.5772/13225 
4. Chen S, Coolbeth M, Dinh H, Kim Y-A, Wang B (2009) Data collection with multiple sinks in 
wireless sensor networks. In: Wireless algorithms, systems, and applications. LNCS, vol 5682. 
Springer, Heidelberg, pp 284–294. ISBN 978-3-642-03416-9, https://doi.org/10.1007/978-3-
642-03417-6_28 
5. Das A, Dutta D (2005) Data acquisition in multiple-sink sensor networks. SIGMOBILE 
Mob Comput Commun Rev 9(3):82–85. ISSN 1559-1662, https://doi.org/10.1145/1094549. 
1094561 
6. Vincze Z, Vida R, Vidacs A (2007) Deploying multiple sinks in multi-hop wireless sensor 
networks. In: IEEE international conference on pervasive services, Istanbul, Turkey, pp 55–63 
7. Maurya S, Jain VK, Chowdhury DR (2019) Delay aware energy efﬁcient reliable routing for 
data transmission in heterogeneous mobile sink wireless sensor network. J Netw Comput Appl 
144:118–137

Reliable Data Delivery in WSN with Multiple Sinksellipsis. . .
619 
8. Ye F, Zhong G, Lu S, Zhang L (2005) Gradient broadcast: a robust data delivery protocol for 
large scale sensor networks. Wireless Netw 11(3):285–298 
9. Boler C, Yenduri S, Ding W, Perkins L, Harris J (2011) To shift or not to shift: maximizing 
the efﬁciency of a wireless sensor network using multiple sinks. Int J Netw Comput Adv Inf 
Manage (IJNCM) 1(1):4 
10. Mahmood MA, Seah WKG, Welch I (2015) Reliability in wireless sensor networks: a survey 
and challenges ahead. Comput Netw 79:166–187 
11. Junapudi V, Udgata SK (2019) Extended lifetime and reliable data transmission in wireless sen-
sor networks with multiple sinks. In: International conference on smart intelligent computing 
and applications, pp 91–100 
12. Tran-Quang V, Miyoshi T (2010) A transmission range adjustment algorithm to avoid energy 
holes in wireless sensor networks. In: 8th Asia-Paciﬁc symposium on information and telecom-
munication technologies, pp 1–6 
13. Liu X (2016) A novel transmission range adjustment strategy for energy hole avoiding in 
wireless sensor networks. J Netw Comput Appl 67:43–52, ISSN 1084–8045 
14. Gudla S, Kuda NR (2022) Learning automata based energy efﬁcient and reliable data delivery 
routing mechanism in wireless sensor networks. J King Saud Univ - Comput Inf Sci 34(8)Part 
B:5759–5765, ISSN 1319-1578. https://doi.org/10.1016/j.jksuci.2021.04.006, https://www. 
sciencedirect.com/science/article/pii/S1319157821000926

