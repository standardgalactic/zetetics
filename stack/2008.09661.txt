arXiv:2008.09661v2  [cs.CV]  7 Apr 2021
TRAINING SPARSE NEURAL NETWORKS USING
COMPRESSED SENSING
Jonathan W. Siegel
Department of Mathematics
Pennsylvania State University
University Park, PA 16802
jus1949@psu.edu
Jianhong Chen
Department of Mathematics
Pennsylvania State University
University Park, PA 16802
jxc5788@psu.edu
Pengchuan Zhang
Microsoft Research
penzhan@microsoft.com
Jinchao Xu
Department of Mathematics
Pennsylvania State University
University Park, PA 16802
jxx1@psu.edu
April 8, 2021
ABSTRACT
Pruning the weights of neural networks is an effective and widely-used technique for reducing model
size and inference complexity. We develop and test a novel method based on compressed sensing
which combines the pruning and training into a single step. Speciﬁcally, we utilize an adaptively
weighted ℓ1 penalty on the weights during training, which we combine with a generalization of the
regularized dual averaging (RDA) algorithm in order to train sparse neural networks. The adap-
tive weighting we introduce corresponds to a novel regularizer based on the logarithm of the abso-
lute value of the weights. We perform a series of ablation studies demonstrating the improvement
provided by the adaptive weighting and generalized RDA algorithm. Furthermore, numerical ex-
periments on the CIFAR-10, CIFAR-100, and ImageNet datasets demonstrate that our method 1)
trains sparser, more accurate networks than existing state-of-the-art methods; 2) can be used to train
sparse networks from scratch, i.e. from a random initialization, as opposed to initializing with a
well-trained base model; 3) acts as an effective regularizer, improving generalization accuracy.
1
Introduction
Recently, neural networks have led to a breakthrough in a wide range of ﬁelds, especially computer vision and natural
language processing [32]. However, neural network models require large amounts of computational power both for
training and inference. In addition, trained models often contain an extremely large number of parameters, which
require a large amount of memory to store. This makes it difﬁcult or even impossible to employ trained networks
in computationally limited environments. Approaches to overcome this problem include quantizing the weights of
neural networks [9, 27, 51, 19, 18, 53], designing compressed architectures which are computationally less expensive
yet achieve comparable accuracy [25, 48, 28], and network pruning, which involves removing weights from a network
as part of the training process [19, 18, 20].
We focus on the problem of network pruning, for which a very popular and effective approach is the iterative pruning
and retraining method [20]. This method works by pruning small weights after training, then ﬁnetuning or retraining
the resulting model and repeating this process. There are also methods which combine the pruning and training into a
single step [13, 2, 1, 14]. While existing methods achieve impressive results, compressing networks by over an order
of magnitude with minimal loss in accuracy, we demonstrate that by utilizing a weighted ℓ1-norm penalty motivated
by compressed sensing [15] signiﬁcantly smaller networks which achieve the same or better accuracy can be obtained.

A PREPRINT - APRIL 8, 2021
Compressed sensing is concerned with the recovery of a sparse signal from a limited number of measurements. Im-
portantly, the support of the signal is not known and must be inferred from the measurements. It is well-known that
an ℓ1-norm regularizer can be used to recover the signal under a variety of assumptions [49, 6, 15]. This suggests
using an ℓ1-norm regularizer on the weights of a neural network to train sparse models. Indeed, the ℓ1-norm has been
used successfully in neural network training, although an ℓ2-norm penalty combined with pruning and retraining often
performs better for obtaining sparse networks[20]. In addition, when seeking structured sparsity, the closely related
group lasso regularizer [54] is often used effectively [50].
In the ﬁeld of compressed sensing, other regularizers have been shown to reconstruct sparser signals than the ℓ1-norm.
For instance, an adaptively weighted ℓ1-norm [7] and the ℓp-norm with 0 ≤p < 1[8]. These are also related, since
the ℓp-norm can be implemented via an iteratively reweighted ℓ1-norm penalty, although an iteratively reweighted
ℓ2-norm approach is computationally simpler [8]. However, such techniques have not, to the best of our knowledge,
been applied to the training of deep neural networks yet. In this work, we seek to ﬁll this gap.
In particular, motivated by compressed sensing, we design an algorithm for training sparse neural networks which is
based on an adaptively weighted ℓ1-norm penalty. There are two major problems we overcome in doing this. First,
we design an appropriate adaptive ℓ1 weighting scheme for neural network training. Second, we design an algorithm
which generates sparse iterates during training and also achieves good generalization accuracy. For convex objectives,
the regularized dual averaging (RDA) algorithm [52] was designed for this purpose, and we generalize it to the setting
of deep learning. These contributions are summarized as follows.
• We design an adaptively weighted ℓ1-regularization scheme which works well for training sparse neural
networks. We connect this regularization scheme with a novel logarithmic regularizer, and also show how to
adapt it to obtain structured sparsity.
• In order to use this scheme to train sparse neural networks, we propose the use of a modiﬁed version of the
regularized dual averaging (RDA) method which incorporates momentum.
• Experimental results indicate that, on a variety of datasets and architectures, our method trains networks
which generalize better and are signiﬁcantly sparser than existing state-of-the-art methods.
2
Related Work
It is widely accepted that highly overparameterized neural networks are easier to train and achieve better generalization
accuracy than smaller models [3, 24, 55]. However, large models are also computationally expensive to deploy. This
motivates the idea of network pruning, where most of the weights and complexity of the network are removed during
training. This idea was ﬁrst introduced in [34, 21] and shown to be highly effective in [20]. Since then, a variety of
approaches to training sparse neural networks have been proposed.
2.1
Unstructured pruning
Many successful approaches to network pruning follow the pruning and retraining methodology ﬁrst successfully used
in [20, 19]. In this approach, weights with small magnitude are removed after training and the remaining weights
are ﬁnetuned. This process is often repeated multiple times to compress networks with no loss in generalization
accuracy. A variety of heuristics for pruning have been introduced, for instance based on neuron outputs instead of
weight magnitudes [26], redundancy [39, 46], and second derivatives [14]. The difference between ﬁne-tuning the
remaining unpruned weights, retraining them from scratch, and retraining them from their initial values is studied in
[17, 36, 56], where speciﬁcally the “Lottery Ticket Hypothesis” is explored. Also, in [19], it is shown that pruning can
be effectively combined with weight quantization to further improve network compression.
In addition, many methods which combine pruning and training have been proposed. In particular, it has been shown
that variational dropout [40] can be used to train exceptionally sparse networks. An ℓ1-norm penalty on the weights
combined with the RDA algorithm [52] is applied to neural networks in [22]. In [13] a modiﬁed momentum method
is used to train sparse networks, which is particularly effective even for deep architectures. The use of momentum to
obtain sparsity is also considered in [12]. This paper builds upon these approaches by using an adaptively weighted
ℓ1-norm regularizer to combine the pruning and training steps. An important difference between our method and most
previous methods which combine training and pruning is that our algorithm trains the network from scratch, i.e. from
a random initialization, as opposed to initializing with a well-trained base model.
2

A PREPRINT - APRIL 8, 2021
3
Applying compressed sensing to neural network training
We begin by introducing some notation. The neural networks which we consider consist of a sequence of convolutional,
linear, and batch normalization [29] layers. Each of the convolutional and linear layers contain trainable weights W
and bias parameters b. In addition, the batch normalization layers contain trainable shift and scale parameters γ and
β. We group the weights of the network by layer and also separate weights W from biases b, and shifts β from scale
parameters γ in convolutional, linear and batch normalization layers. We denote the resulting groups of parameters
G1, ..., GN, where each group Gi is either W or b from a convolutional or linear layer, or is γ or β from a batch
normalization layer. Here N is the total number of groups, which will be twice the number of layers. We also let
Θ = {Gi}N
i=1 denote the collection of all parameters, D denote the training dataset, and
L(Θ) =
1
|D|
X
(x,y)∈D
l(x, y, Θ)
(1)
denote the empirical loss function, where l is typically taken to be the cross-entropy. Finally, we denote by ˜∇L(Θ) a
stochastic gradient sample obtained by considering a mini-batch of training data.
3.1
Adaptively weighting ℓ1-norm
The lasso [49], which involves adding an ℓ1-norm regularization to the regression loss function, is a well-known and
effective method for performing sparse regression and signal estimation in compressed sensing. In the context of
neural network training, this corresponds to solving
arg min
Θ L(Θ) + λ∥Θ∥1,
(2)
where λ is a hyperparameter controlling the trade-off between sparsity and training loss.
While the lasso regularizer can be effectively applied to neural network training, it can be considerably improved by
using an adaptive ℓ1 weight. An adaptively reweighted ℓ1-constraint was ﬁrst introduced in [7], where it is shown
to be considerably better at recovering sparse signals than the Danzig selector [5]. In this method, the ℓ1-norm on a
parameter θ ∈Θ is weighted by |˜θ|−1, where ˜θ is an estimate of the true value of θ obtained in the previous iteration.
We use a similar adaptively weighted ℓ1-norm on the weights of our neural networks. Our method differs in the
observation that parameters belonging to the same group Gi should be of a similar scale, while parameters belonging
to different Gi can have very different scales. This suggests using a scheme similar to that introduced in [7] within
each group Gi. Speciﬁcally, we weight the ℓ1-norm on a parameter θ ∈Gi with
λ(β + 1)

β + |θ|
Mi
−1
,
(3)
where β and λ are hyperparameters and Mi is the maximum absolute value of all parameters in Gi, i.e. Mi =
maxθ∈Gi |θ|. This weight is λ for the entry with largest absolute value in each group and increases to a maximum
of λ(1 + β−1) for entries which are 0. So the hyperparameter λ controls the total strength of the ℓ1 penalty, while β
controls how large the penalty can become for small weights.
The precise form of the weighting scheme 3 was determined through trial and error by running experiments on the
LeNet-5 model on MNIST [33]. However, it can also be related to a novel regularizer, via an argument given in
[8]. There, an ℓp-penalty for 0 ≤p < 1 is implemented using an adaptively weighted ℓ1-penalty, with appropriately
weights. Following this viewpoint, the weighting scheme in 3 can be understood as corresponding to the logarithmic
regularizer
R(Θ) = λ(β + 1)
N
X
i=1
X
θ∈Gi
Mi log

β + |θ|
Mi

,
(4)
where x = G1 ∪· · · ∪Gn represents all of the weights in the network. This is very similar to the case of an ℓ0 penalty
in [8].
Finally, in order to make the algorithm more robust, we calculate the weights in (3) based upon a running average of the
magnitudes of the parameters. In particular, we consider a running average of the absolute values of each parameter,
computed recursively according to
|θ|av
n = µ|θ|av
n−1 + (1 −µ)|θn|.
(5)
Here µ is a momentum parameter which effectively controls the number of iterations over which we average, and we
discuss it in more detail later. We use the averaged absolute values |θ|av
n in (3) to calculate the adaptive weights in
each iteration.
3

A PREPRINT - APRIL 8, 2021
3.2
Extended Regularized Dual Averaging
A key difﬁculty in using an ℓ1-norm regularizer for neural network training is that the naive forward-backward stochas-
tic gradient descent algorithm applied to the regularized loss function
f(Θ) = L(Θ) + λ∥Θ∥1
(6)
doesn’t generate sparse iterates, which takes the form
Θn+ 1
2 = Θn −sn ˜∇L(Θn)
Θn+1 ∈Θn+ 1
2 −sn∂(λ∥· ∥1)(Θn+1),
(7)
where sn is the step size in n-th iteration. Here the second line represents a backward step for the regularizer λ∥· ∥1,
whose solution can be given in closed form and is known as the soft-thresholding operator
xn+1 = arg min
y

λ∥y∥1 + 1
2∥xn+ 1
2 −y∥2
2

= sign(xn+ 1
2 ) max(0, |xn+ 1
2 | −snλ).
(8)
The fact that iteration (7) doesn’t generate sparse iterates, which was ﬁrst observed in [52], occurs because a small
step size sn is often necessary for convergence, or to obtain good generalization accuracy, and this means that the
soft-thresholding parameter snλ is very small. This phenomenon has also been observed in the context of training
compressed neural networks, where it has been termed magnitude plateau [13].
To overcome this issue for convex machine learning problems, the regularized dual averaging algorithm (RDA), which
is an extension of the dual averaging algorithm of Nesterov [42], is introduced in [52]. A special case of this algorithm,
which bears resemblance to (7), is
Θn+ 1
2 = Θn−1
2 −sn ˜∇L(Θn)
Θn+1 ∈Θn+ 1
2 −Sn∂(λ∥· ∥1)(Θn+1),
(9)
where the backward step is given by Sn = Pn
k=1 si and Θ−1
2 = Θ0. Note that this implies that the soft-thresholding
parameter, λSn, grows signiﬁcantly and this leads to sparse iterates. Extensive experiments showing the effectiveness
of this method on convex problems are given in [52].
The RDA algorithm has previously been applied to the training of neural networks in [22]. However, when applied to
highly non-convex problems such as neural network training, the RDA algorithm is not particularly robust and may not
always converge. This is due to the fact that the soft-thresholding parameter increases too rapidly and this makes the
algorithm highly sensitive to the initialization. To get around this issue, we use a generalization of the RDA algorithm,
ﬁrst proposed in [44] and called xRDA (eXtended RDA). This algorithm introduces an additional averaging parameter
αn and takes the form
Θn+ 1
2 = (1 −αn)Θn + αnΘn−1
2 −sn ˜∇L(Θn)
Θn+1 ∈Θn+ 1
2 −Sn∂(λ∥· ∥1)(Θn+1),
(10)
where the backward step size now satisﬁes Sn = αnSn−1 + sn and again Θ−1
2 = Θ0. Notice that if αn = 0 we
get SGD (7), and if αn = 1 we get RDA (9). Thus this method can be thought of as interpolating between the two.
In [44] it is shown that this method converges for convex objectives. By slowly increasing αn during the course of
training, the soft-thresholding parameter slowly increases, improving the robustness of the algorithm and leading to
better accuracy and sparsity.
3.3
Momentum for RDA
Finally, in order to improve the generalization accuracy of the trained sparse models, we introduce a method of adding
momentum to RDA. Momentum, which has an intuitive physical interpretation [43], has long been used to accelerate
convex optimization [41] and has been used extensively in deep learning to accelerate convergence and improve
generalization accuracy [47]. In the context of deep learning, training with momentum typically means the search
direction in each iteration is an exponentially weighted average of the gradient over the previous iterations, i.e.
vn = µvn−1 + ˜∇L(Θn)
Θn+1 = Θn −snvn,
(11)
where the momentum parameter µ effectively determines how many previous iterates the gradient is averaged over.
We note that in the context of convex optimization, the above iteration must be modiﬁed to guarantee convergence for
4

A PREPRINT - APRIL 8, 2021
arbitrary convex objectives [41]. A detailed investigation of (11) and Nesterov’s momentum for training deep neural
networks is given in [47].
We incorporate momentum in the xRDA algorithm (10) by replacing the sampled gradient ˜∇L(Θ) by an average over
the past gradients. This results in the algorithm
vn = µvn−1 + (1 −µ) ˜∇L(Θn)
Θn+ 1
2 = (1 −αn)Θn + αnΘn−1
2 −sn˜vn
Θn+1 ∈Θn+ 1
2 −Sn∂(λ∥· ∥1)(Θn+1),
(12)
where as before the backward step size now satisﬁes Sn = αnSn−1 + sn and again Θ−1
2 = Θ0.
The momentum parameter µ in (12) controls the scale over which we average our gradients to determine an appropriate
search direction. This parameter is often taken independent of the step size, which corresponds to averaging the
gradients over a ﬁxed number of iterations. However, we have observed that better generalization accuracy is obtained
if the gradients are averaged over a ﬁxed timescale. In particular, we take the momentum parameter µ = e−sn/T ,
where T determines the time scale over which we average. We note that this dependence of momentum on step size
corresponds to a discretization of Langevin dynamics with constant damping parameter [4].
We also use this value of µ to determine the average parameter magnitudes |θ|av
n . In other words, we set µ = e−sn/T
in equation (5) so that the parameter magnitudes are averaged over a ﬁxed time scale when calculating the adaptive ℓ1
weights. We have found that this signiﬁcantly improves the stability and robustness of the algorithm.
4
Experimental Results
In this section, we provide experimental evidence demonstrating the effectiveness of our compressed sensing based
approach to training sparse neural networks1.
4.1
Comparison with Existing State-of-the Art Methods
We evaluate the effectiveness of using compressed sensing to train sparse neural networks on several common bench-
mark models and datasets. Speciﬁcally, we consider the datasets CIFAR-10 and CIFAR-100 [31], and ImageNet [10].
In all of our tests, we run our algorithm from scratch, i.e. we do not need to initialize with a well-trained base model,
unlike previous approaches which combine training and pruning into a single step [13, 40].
4.1.1
CIFAR-10
We present the results of our ﬁrst set of experiments on CIFAR-10 in Table 1. We test a variety of architectures,
speciﬁcally VGG-16 and VGG-19 [45], and ResNet-18 and ResNet-56 [23]. We note that for the ResNet-18 structure
we use is much wider that the ResNet-56 network. In particular, the ResNet-56 architecture we use contains 3 blocks
with [16, 32, 64] channels, while the ResNet-18 architecture contains 4 blocks with [64, 128, 256] channels. We use
these two architectures to show that compressed sensing can be applied both to deep narrow and to shallow wide
residual networks.
For VGG-16 and VGG-19 we set the momentum timescale T = 9.5 and use a cosine learning rate schedule [37] with
initial learning rate 1.0. We have obtained the best results by setting the averaging parameter αn = 0, which corre-
sponds to a version of stochastic gradient descent, although similar results can be obtained with a slowly increasing
averaging parameter αn. Finally, we set the ℓ1-norm parameters λ = 10−6 and β = 2 · 10−3. For ResNet-16 and
ResNet-18 we use mostly the same hyperparameters, but we set the initial learning rate to 0.25 and set λ = 1.3 · 10−6.
We run the algorithm for 600 epochs in all cases. This follows the methodology in [13], and we have found that
this large number of epochs is necessary to give the algorithm time to ﬁnd advantageous sparse conﬁgurations. The
baseline accuracy is obtained by training with SGD using appropriately tuned hyperparameters for the same number
of epochs. For all models, we use a standard data augmentation strategy, consisting of padding to 40 by 40, followed
by a random crop back to 32 by 32, and random right-left ﬂipping.
We can see from these results that using compressed sensing (CS) results in signiﬁcantly sparser and more accurate
networks than existing state-of-the-art methods. For instance, we are able to reduce the number of parameters in VGG-
19 by more than two orders of magnitude while increasing the accuracy. In addition, this holds across a wide range of
architectures, including both deep and wide residual neural networks.
1Our code is available at https://github.com/jwsiegel2510/xrda-sparse-training
5

A PREPRINT - APRIL 8, 2021
4.1.2
CIFAR-100
Next, we present results on the CIFAR-100 dataset in Table 2. Here we again consider the VGG-19 network archi-
tecture. We used the same hyperparameters as for VGG-19 on CIFAR-10 and the same data augmentation strategy,
discussed previously.
We see from these results that compressed sensing (CS) is also effective on more complicated datasets. In particular,
we are able to reduce the size of VGG-19 by a factor of nearly 40 while attaining a signiﬁcant increase in accuracy.
This suggests that compressed sensing is an effective regularizer in addition to producing sparse networks.
Table 1: Unstructured sparsity results on CIFAR-10.
Model
Algorithm
Base
Sparse
Dense / Sparse
Compression
Non-Zero
Top1
Top1
Parameters
Ratio
Fraction
ResNet-18
CS
95.05
94.49
11.17M / 0.14M
81x
1.23
ResNet-18
RDA [22]
-
93.95
11.17M / 0.56M
20x
5.00
ResNet-20
CS
93.87
91.99
270K / 27K
10x
9.88
ResNet-20
Bayesian [11]
93.90
91.68
270K / 27K
10x
10.00
VGG-16
CS
93.79
94.13
14.73M / 0.18M
80x
1.25
VGG-16
Momentum [12]
93.41
93.31
14.73M / 0.74M
20x
5.00
VGG-16
Bayesian [38]
91.60
91.00
14.73M / 0.81M
18x
5.50
VGG-16
Var Dropout [40]
92.70
92.70
14.73M / 0.31M
48x
2.08
VGG-16
Slimming [35]
93.66
93.41
14.73M / 0.65M
22x
4.40
VGG-16
DST [30]
93.74
93.36
14.73M / 1.4M
10x
10.00
VGG-16
DST [30]
93.74
93.00
14.73M / 0.74M
20x
5.00
VGG-19
CS
93.60
94.18
20.04M / 0.19M
104x
0.97
VGG-19
Pruning [20]
93.50
93.34
20.04M / 1.00M
20x
5.00
VGG-19
Scratch-B [36]
93.50
93.63
20.04M / 1.00M
20x
5.00
Table 2: Unstructured sparsity results on CIFAR-100.
Model
Algorithm
Base
Sparse
Dense / Sparse
Compression
Non-Zero
Top1
Top1
Parameters
Ratio
Fraction
VGG-19
CS
73.83
75.93
20.09M / 0.46M
43x
2.30
VGG-19
Pruning [20]
71.70
70.22
20.09M / 1.00M
20x
5.00
VGG-19
Scratch-B [36]
71.70
72.08
20.09M / 1.00M
20x
5.00
4.1.3
ImageNet
Here we present the results of using compressed sensing to compress ResNet-50 [23] on the ImageNet [10] dataset.
The results can be found in table 3. We see from this that compressed sensing signiﬁcantly outperforms both pruning
[36] and global sparse momentum [13]. It is certainly competitive with and even slightly outperforms the state-of-the-
art rigging the lottery method [16].
Table 3: Unstructured sparsity results on ImageNet.
Model
Algorithm
Base
Sparse
Dense / Sparse
Compression
Non-Zero
Top1
Top1
Parameters
Ratio
Fraction
ResNet-50
CS
76.98
25M / 3.5M
7.14x
14.00
ResNet-50
CS
76.00
25M / 1.7M
14.7x
6.8
ResNet-50
Pruning [36]
76.15
76.06
25M / 17.5M
1.43x
70.00
ResNet-50
Pruning [36]
76.15
76.09
25M / 10M
2.5x
40.00
ResNet-50
RigL [16]
76.8
77.1
25M / 5M
5x
20.00
ResNet-50
RigL [16]
76.8
76.4
25M / 2.5M
10x
10.00
ResNet-50
GSM [13]
75.72
75.33
25M / 6.25M
4x
25.00
ResNet-50
GSM [13]
75.72
74.30
25M / 5M
5x
20.00
6

A PREPRINT - APRIL 8, 2021
5
Conclusion and future work
We have shown that compressed sensing can be used to effectively train sparse neural networks. In particular, by
developing an appropriate adaptively weighted ℓ1-norm penalty, combined with a novel optimization algorithm based
on the RDA algorithm, we are able to train signiﬁcantly smaller and sparser networks than previously possible on both
CIFAR-10 and CIFAR-100. A signiﬁcant increase in generalization accuracy also suggests that compressed sensing
has an important regularizing effect.
Further work involves more extensive testing and tuning of the algorithm, in addition to a theoretical analysis of
compressed neural networks. In particular, we propose testing this technique on larger and more complex datasets
such as ImageNet [10], and attempting to obtain better results with more extensive hyperparameter tuning. We also
propose testing different types of structured sparsity using the method, and expect that this technique will also be
effective for different types of machine learning models beyond feed-forward convolutional neural networks. Finally,
we hope that a theoretical analysis can explain the precise nature of the regularizing effect of compressed sensing
observed in our experiments.
6
Acknowledgements
We would like to thank Dr. Xiao Lin for helpful comments and discussion while preparing this work.
References
[1] Alvarez, J.M., Salzmann, M.: Learning the number of neurons in deep networks. In: Advances in Neural
Information Processing Systems, pp. 2270–2278 (2016)
[2] Alvarez, J.M., Salzmann, M.: Compression-aware training of deep networks. In: Advances in Neural Information
Processing Systems, pp. 856–867 (2017)
[3] Bengio, Y., Roux, N.L., Vincent, P., Delalleau, O., Marcotte, P.: Convex neural networks. In: Advances in neural
information processing systems, pp. 123–130 (2006)
[4] Bhattacharya, R.N., Waymire, E.C.: Stochastic processes with applications, vol. 61. Siam (2009)
[5] Candes, E., Tao, T., et al.: The dantzig selector: Statistical estimation when p is much larger than n. The annals
of Statistics 35(6), 2313–2351 (2007)
[6] Candès, E.J., Romberg, J., Tao, T.: Robust uncertainty principles: Exact signal reconstruction from highly in-
complete frequency information. IEEE Transactions on information theory 52(2), 489–509 (2006)
[7] Candès, E.J., Wakin, M.B., Boyd, S.P.: Enhancing sparsity by reweighted ℓ1 minimization. Journal of Fourier
analysis and applications 14(5-6), 877–905 (2008)
[8] Chartrand, R., Yin, W.: Iteratively reweighted algorithms for compressive sensing. In: 2008 IEEE International
Conference on Acoustics, Speech and Signal Processing, pp. 3869–3872. IEEE (2008)
[9] Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neural networks with binary weights
during propagations. In: Advances in neural information processing systems, pp. 3123–3131 (2015)
[10] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database.
In: 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee (2009)
[11] Deng, W., Zhang, X., Liang, F., Lin, G.: An adaptive empirical bayesian method for sparse deep learning. In:
Advances in Neural Information Processing Systems, pp. 5563–5573 (2019)
[12] Dettmers, T., Zettlemoyer, L.: Sparse networks from scratch: Faster training without losing performance. arXiv
preprint arXiv:1907.04840 (2019)
[13] Ding, X., Zhou, X., Guo, Y., Han, J., Liu, J., et al.: Global sparse momentum sgd for pruning very deep neural
networks. In: Advances in Neural Information Processing Systems, pp. 6379–6391 (2019)
[14] Dong, X., Chen, S., Pan, S.: Learning to prune deep neural networks via layer-wise optimal brain surgeon. In:
Advances in Neural Information Processing Systems, pp. 4857–4867 (2017)
[15] Donoho, D.L.: Compressed sensing. IEEE Transactions on information theory 52(4), 1289–1306 (2006)
[16] Evci, U., Gale, T., Menick, J., Castro, P.S., Elsen, E.: Rigging the lottery: Making all tickets winners. arXiv
preprint arXiv:1911.11134 (2019)
7

A PREPRINT - APRIL 8, 2021
[17] Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint
arXiv:1803.03635 (2018)
[18] Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.A., Dally, W.J.: Eie: efﬁcient inference engine on
compressed deep neural network. ACM SIGARCH Computer Architecture News 44(3), 243–254 (2016)
[19] Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural networks with pruning, trained
quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015)
[20] Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for efﬁcient neural network. In:
Advances in neural information processing systems, pp. 1135–1143 (2015)
[21] Hassibi, B., Stork, D.G.: Second order derivatives for network pruning: Optimal brain surgeon. In: Advances in
neural information processing systems, pp. 164–171 (1993)
[22] He, J., Jia, X., Xu, J., Zhang, L., Zhao, L.: Make ℓ1 regularization effective in training sparse cnn. arXiv pp.
arXiv–1807 (2018)
[23] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 770–778 (2016)
[24] Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531
(2015)
[25] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mo-
bilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861
(2017)
[26] Hu, H., Peng, R., Tai, Y.W., Tang, C.K.: Network trimming: A data-driven neuron pruning approach towards
efﬁcient deep architectures. arXiv preprint arXiv:1607.03250 (2016)
[27] Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.: Quantized neural networks: Training neural
networks with low precision weights and activations. The Journal of Machine Learning Research 18(1), 6869–
6898 (2017)
[28] Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.: Squeezenet: Alexnet-level
accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360 (2016)
[29] Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate
shift. arXiv preprint arXiv:1502.03167 (2015)
[30] Junjie, L., Zhe, X., Runbin, S., Cheung, R.C., So, H.K.: Dynamic sparse training: Find efﬁcient sparse network
from scratch with trainable masked layers. In: International Conference on Learning Representations (2019)
[31] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)
[32] LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444 (2015)
[33] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Pro-
ceedings of the IEEE 86(11), 2278–2324 (1998)
[34] LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Advances in neural information processing
systems, pp. 598–605 (1990)
[35] Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efﬁcient convolutional networks through
network slimming. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2736–2744
(2017)
[36] Liu, Z., Sun, M., Zhou, T., Huang, G., Darrell, T.: Rethinking the value of network pruning. arXiv preprint
arXiv:1810.05270 (2018)
[37] Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983
(2016)
[38] Louizos, C., Ullrich, K., Welling, M.: Bayesian compression for deep learning. In: Advances in Neural Informa-
tion Processing Systems, pp. 3288–3298 (2017)
[39] Mariet, Z., Sra, S.: Diversity networks: Neural network compression using determinantal point processes. arXiv
preprint arXiv:1511.05077 (2015)
[40] Molchanov, D., Ashukha, A., Vetrov, D.: Variational dropout sparsiﬁes deep neural networks. In: Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pp. 2498–2507. JMLR. org (2017)
[41] Nesterov, Y.: A method for unconstrained convex minimization problem with the rate of convergence o (1/kˆ 2).
In: Doklady an ussr, vol. 269, pp. 543–547 (1983)
8

A PREPRINT - APRIL 8, 2021
[42] Nesterov, Y.: Primal-dual subgradient methods for convex problems. Mathematical programming 120(1), 221–
259 (2009)
[43] Polyak, B.T.: Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki i Matematich-
eskoi Fiziki 3(4), 643–653 (1963)
[44] Siegel, J.W., Xu, J.: Extended regularized dual averaging. arXiv preprint arXiv:1904.02316 (2019)
[45] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition.
arXiv
preprint arXiv:1409.1556 (2014)
[46] Srinivas, S., Babu, R.V.: Data-free parameter pruning for deep neural networks. arXiv preprint arXiv:1507.06149
(2015)
[47] Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initialization and momentum in deep
learning. In: International conference on machine learning, pp. 1139–1147 (2013)
[48] Tan, M., Le, Q.V.: Efﬁcientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint
arXiv:1905.11946 (2019)
[49] Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series
B (Methodological) 58(1), 267–288 (1996)
[50] Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H.: Learning structured sparsity in deep neural networks. In: Advances
in neural information processing systems, pp. 2074–2082 (2016)
[51] Wu, S., Li, G., Chen, F., Shi, L.: Training and inference with integers in deep neural networks. arXiv preprint
arXiv:1802.04680 (2018)
[52] Xiao, L.: Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine
Learning Research 11(Oct), 2543–2596 (2010)
[53] Yin, P., Zhang, S., Lyu, J., Osher, S., Qi, Y., Xin, J.: Blended coarse gradient descent for full quantization of deep
neural networks. Research in the Mathematical Sciences 6(1), 14 (2019)
[54] Yuan, M., Lin, Y.: Model selection and estimation in regression with grouped variables. Journal of the Royal
Statistical Society: Series B (Statistical Methodology) 68(1), 49–67 (2006)
[55] Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.: Understanding deep learning requires rethinking
generalization. arXiv preprint arXiv:1611.03530 (2016)
[56] Zhou, H., Lan, J., Liu, R., Yosinski, J.: Deconstructing lottery tickets: Zeros, signs, and the supermask. In:
Advances in Neural Information Processing Systems, pp. 3592–3602 (2019)
9

