École Polytechnique Fédérale de Lausanne
Look Before You Leap: A Universal Emergent Decomposition of
Retrieval Tasks in Autoregressive Language Models
by Alexandre Variengien
Master Thesis
Eric Winsor, Conjecture
Sid Black, Conjecture
Thesis Supervisors
Prof. Antoine Bosselut, EPFL
Prof. Aurélien Garivier, ENS de Lyon
Thesis Advisors
Conjecture
201 Borough High Street
SE1 1JA - London
August 25, 2023

Carefully we double-checked the numbers on the
petri dishes to make sure we had looked at the
correct plate. Everything was in order. I looked
across at Leslie. “Do you realize,” I said, “that you
and I are the only people in the world who know
it’s a triple code?”
– Francis Crick, What mad pursuit: a personal
view of scientific discovery
Dedicated to the men and women working to make the world safer.

Acknowledgments
I’d like to thank Eric Winsor and Beren Millidge for sharing their insights and helping me navigate the
early stage of this project to find its right scope. I further thank Eric Winsor for his productive advice
and his dedicated support over the course of the project, from experimental designs to thorough
feedback on writing.
I would like to thank Conjecture for offering this internship opportunity and providing me with a
space to explore my research ideas.
I also take this opportunity to thank all the researchers who provided me with feedback at various
stages of the projects. I am especially grateful to Sid Black, Adam Shimi, Gail Weiss, Diego Dorn,
Pierre Peigné, and Jean-Stanislas Denain.
Finally, I thank the members of Conjecture for their enthusiasm and energy which made this office
space a great place to work.
London, August 25, 2023
Alexandre Variengien
1

Abstract
As large language models (LLMs) approach human-level abilities, the need for scalable oversight—
supervising both their behavior and internal processes with minimal reliance on human experts—
becomes paramount. Despite advances in behavioral supervision, existing efforts to interpret LLMs’
internals are narrowly focused on small models and simple tasks or offer general yet incomplete
explanations of large model capabilities. More critically, work on LLM interpretability often misses
prospects for near-term practical application. To address this, we introduce task constellations:
collections of diverse tasks with a shared input representation, enabling scalable comparative causal
analyses across models and datasets. As a concrete example, we present ORION , a constellation
of 15 retrieval tasks spanning six domains, from translation to coding. Each task in ORION can be
represented abstractly by a request (e.g. a question) that retrieves an entity (e.g. a character) from a
context (e.g., a short story). We applied causal analysis on 18 open-source language models with
sizes ranging from 125 million to 70 billion parameters, including the largest LLM freely accessible.
We find that language models cleanly decompose retrieval tasks at a high level: middle layers at the
last token position process the request, while late layers retrieve the correct entity from the context.
Our central experiment, termed request-patching, involves causal intervention on the mid-layer at
the last token position. We observe that this intervention alters the model’s behavior in a coherent
way, modifying the request it interprets without affecting the execution of the retrieval process in
the later layers. Request-patching explains over 70% of the baseline correct token probability in
98 of the 106 studied model-task pairs. We further validate our high-level causal hypothesis using
the formalism of causal abstraction. Building on this understanding, we demonstrate a proof of
concept application for scalable oversight by detecting and mitigating prompt-injection in question-
answering tasks, requiring human supervision on only a single task instance. Our solution, based
on request-patching, improves accuracy drastically (15.5% →97.5% on a 12-billion parameter
LLM) while maintaining performance on control inputs. To our knowledge, this represents the first
evidence of a universal causal explanation across varied domains and models and is a pioneering
effort in applying interpretability for scalable oversight of LLMs.
2

Contents
Acknowledgments
1
Abstract
2
1
Introduction
6
1.1
Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Approaches to the scalable oversight problem . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3
The need for internal process supervision
. . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Causal analysis as a foundation for interpretability . . . . . . . . . . . . . . . . . . . . .
8
1.5
Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2
Background
11
2.1
The Transformer architecture for autoregressive language models . . . . . . . . . . . .
11
2.2
Tasks and model inputs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.3
Causal interventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.4
Causal abstractions of neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3
Related work
16
3.1
Explanations of model behavior
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
3.2
Mechanisms as explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
3.3
Causal analysis for interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.4
Designing datasets to study model behavior . . . . . . . . . . . . . . . . . . . . . . . . .
18
4
Constellations: generalizing causal analysis with abstract input representations
19
4.1
The challenges of building datasets for language model interpretability . . . . . . . . .
19
4.1.1
Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
4.1.2
Interpreting language model outputs . . . . . . . . . . . . . . . . . . . . . . . . .
20
4.2
Constellations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4.2.1
List of desiderata for constellations
. . . . . . . . . . . . . . . . . . . . . . . . . .
21
5
ORION : a constellation of retrieval tasks for language models
23
5.1
Abstract representation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
5.2
Implementation of the constellation desiderata . . . . . . . . . . . . . . . . . . . . . . .
24
3

5.3
Creating ORION tasks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
5.4
Performance metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
5.4.1
Model performances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
6
Causal analysis on ORION
29
6.1
Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
6.2
Results of residual stream patching
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
6.3
Validation against prior work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
6.4
A high-level division of model computation. . . . . . . . . . . . . . . . . . . . . . . . . .
36
6.5
Case study on a Pythia model for the question-answering task
. . . . . . . . . . . . . .
40
7
Application of request-patching: partial internal supervision of the retrieval task
42
7.1
Problem definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
7.2
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
7.3
Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
8
Discussion and Future work
48
9
Conclusion
51
Bibliography
52
Appendix
56
A Case study on Pythia-2.8b solving a question-answering task
57
A.1 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
A.2 The components contributing directly to the logits depend on superficial changes in
the input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
A.2.1
City-specific attention heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
A.3 Request-patching preserves attention head mechanisms
. . . . . . . . . . . . . . . . .
60
A.4 Request-patching is influencing late MLPs . . . . . . . . . . . . . . . . . . . . . . . . . .
62
B Additional performance metrics on ORION
66
C Additional residual stream results
68
D Detailed description of the ORION tasks
71
D.1 Question-answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
D.1.1 Generating the stories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
D.1.2 Generating the questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
D.2 Type hint understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
D.3 Factual recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
D.4 Variable binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
D.5 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4

D.6 Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
5

Chapter 1
Introduction
1.1
Context
Recent advances in large language models (LLMs) [66] have demonstrated their flexible problem-
solving abilities and their expert-level knowledge in a wide range of fields [10]. The extent of the
knowledge most advanced models can mobilize is beyond what any single human can hold [51].
These general abilities are used as building blocks for diverse applications, from solving challenging
math problems [38] to autonomously taking action and accumulating skills in a virtual environment
[69]. According to a survey of experts in the field, it is likely that in the near future, such generally
capable models will be used to solve tasks better than humans [74]. Moreover, these systems are
starting to be interfaced with the physical world through robotics [67] and have the potential to be
tightly integrated into the economy by automating significant parts of jobs [23].
The advanced abilities of LLMs are both a reason for their wide adoption and a major challenge
when supervising the operation of such systems. As they start to surpass human abilities, we cannot
rely on comparison to gold labels gathered from human experts to evaluate them. Moreover, more
advanced systems could have the ability to engage in harmful yet undetectable behavior, such
as introducing vulnerabilities in software or manipulating humans, as current systems already
demonstrate simple versions of these abilities [54, 51]. Under the current paradigm to train these
systems, it is possible that by maximizing the training reward, AI systems develop goals that are
misaligned with the ones of their designers [48]. A misaligned AI system would willfully engage in
harmful behaviors to pursue its own goals at the expense of humans.
This context underscores the urgency to develop methods to ensure general-purpose AI systems, of
which LLMs are the most successful examples, robustly solve the problems we assign them after
they reach the point of being able to solve tasks broadly better than humans. This problem is known
as scalable oversight [4].
6

1.2
Approaches to the scalable oversight problem
While scalable oversight is currently an open problem [31], several approaches have been proposed
to create a learning signal that incentivizes models to give correct answers without relying solely on
humans evaluating their output [72, 41, 32, 14, 37]. These approaches rely on the core principle of
task decomposition, breaking down complex problems solved by AI systems into smaller tasks to
make them more suitable for human evaluation.
A first approach, AI safety via debate [32], attempts to train models to play a zero-sum game where
two AI players are competing by formulating conflicting arguments that support different answers
to the same problem. While a human is the final judge of the debate, the hope is that supervising
the task would be more tractable by seeing each step of the debate than if the human could see the
final answer alone.
A second approach known as process supervision aims at breaking down the resolution of compli-
cated problems into a sequence of small reasoning steps. Humans are then in charge of evaluating
the validity of going from one step to the next, instead of the outcome of the full reasoning [72, 41].
1.3
The need for internal process supervision
Current applications of task decomposition for LLMs make models externalize their reasoning in
plain English before formulating the final answer, a strategy known as chain-of-thought prompting
[71]. However, this strategy only provides partial supervision of each step. Chain-of-thought prompt-
ing only helps to supervise intermediate token generations that influence future token generations,
while leaving the internal process of the model unexamined. This process relies solely on the input-
output relationship but many internal computations could lead to the same external behavior. In
the extreme case, we can imagine that the model is performing a chain of internal computations
unrelated to the chain of thought while in parallel generating tokens forming intermediate steps
plausible to humans. At the end of the argument, the model returns the output from the chain
of internal computations instead of aggregating the intermediate steps laid down in the text. To
illustrate this point, one can imagine a model being asked to give business strategy advice. The
model could start by having an answer directly after the question and use the later exposition of its
reasoning as a post hoc rationalization of its initial conclusion.
Thus, to comprehensively supervise LLM processes, one also needs to understand the precise
internal mechanism through which intermediate token generations influence future ones. This
limitation underscores every approach to scalable oversight introduced above, as the principle
of task decomposition presupposes that the AI system solves complicated tasks by aggregating
intermediate results of simpler tasks.
7

The field of machine learning interpretability attempts to fill this gap in supervision by explaining
the mechanisms used by models for their prediction.
1.4
Causal analysis as a foundation for interpretability
While neural networks are often described as “black boxes”, interpretability researchers have total
access to their inner workings by inspecting their computation. The problem is that their computa-
tions are expressed in terms of activation vectors and weight tensors that are illegible to humans. The
goal of interpretability is to translate these high-dimensional computations into simpler, human-
understandable explanations that stay faithful to the computations happening inside the model.
While different types of explanations for model internals have been proposed, many fall short of
providing falsifiable hypotheses [36]. Techniques such as saliency maps and single-neuron analysis
can produce misleading conclusions [9], do not pass simple sanity checks [1], and are unable to
detect backdoor implanted in models [20].
The framework of causal abstraction has been proposed to design falsifiable explanations of model
internals [24]. The framework is grounded in the study of causal analysis [53] and provides formal
tools to define when a high-level, human-understandable causal explanation faithfully describes
opaque low-level mechanisms. Concretely, a high-level causal model (e.g. a symbolic algorithm)
faithfully describes an opaque low-level causal model (e.g. a neural network) when it is possible
to align high-level variables with sets of low-level variables that play the same causal role. The
characterization of the causal role of variables relies on the notion of intervention experiments,
which set some number of variables in a causal model to fixed values. The theoretical application of
causal abstraction to neural networks was experimentally supported by the finding of faithful causal
explanations of model mechanisms, first on small models trained on toy tasks [25] and recently on
real-world LLMs [73] solving simple tasks.
The broader set of techniques from the experimental paradigm of causal analysis has been at the
core of the recent research field of mechanistic interpretability, aiming to reverse-engineer the
algorithms learned by neural networks. While work in the field of mechanistic interpretability does
not systematically provide explanations in the strict format of causal abstraction, it strives to validate
the mechanisms presented using causal interventions. Mechanistic interpretability was successfully
applied to characterize the role of a circuit of neurons in a vision model [11], reverse-engineer
how small Transformers learn simple tasks such as modular arithmetic [47, 75], and explain simple
behaviors of small language models [70, 28].
Recent works investigated large language models on more challenging tasks. Researchers described
the information flow responsible for factual recall [26], discovered model components underlying in-
context learning [50] and multi-choice questions answering [40]. However, because of the complexity
of the tasks, such works provide only a partial description of the underlying mechanisms, instead of
8

a unified causal explanation.
In summary, work applying causal analyses to language models falls on a spectrum with two
extremes. On the one hand, labor-intensive in-depth case studies focus on understanding simple
tasks solved by small models. On the other hand, more largely scoped studies fail to produce unified
causal explanations.
More critically, despite the ultimate goal of turning our understanding of model mechanisms into
increased supervision, only a few applications of interpretability to better control model behavior
exist [18, 55, 45]. To our knowledge, no application of interpretability techniques to the problem of
scalable oversight has been proposed.
1.5
Contributions
In this thesis, we first aim to bridge the gap from unified causal explanations to general explanations.
We then attempt to turn understanding models’ internals into practical solutions to the problem of
scalable oversight. Our contributions are threefold:
• Methodological. We introduce the concept of a task constellation (Chapter 4), a tool to scale
causal analysis of language models to diverse tasks. Instead of studying isolated tasks, constel-
lations are a collection of narrow yet diverse tasks that share the same input structure – called
an abstract input representation. This common representation enables the design of causal
experiments on an array of tasks and models with little additional overhead. We apply this
framework to study language models solving retrieval problems, i.e. tasks that require finding
an entity from the context (e.g. a short story) that answers a request (e.g. a question). To this
end, we introduce ORION (Chapter 5), a constellation of 15 retrieval tasks spanning 6 different
problem types including question-answering, factual recall, and type-hint understanding.
• Experimental. We apply causal analysis to the ORION constellation on a set of 18 open-source
language models with sizes spanning two orders of magnitude (125 million - 70 billion) (Chap-
ter 6). We discover that language models handle retrieval tasks by cleanly separating the layers
at which they process the request and the context at the last token position. The intervention
experiment supporting this emergent task decomposition, called request-patching, explains
more than 70% of the baseline token probability for 98 out of the 106 pairs of models and tasks
studied. We present a high-level description of how computations are divided inside models
that we validate using the framework of causal abstraction. To our knowledge, this is the first
example of a causal explanation that generalizes across models and tasks. We complemented
this coarse-grained causal analysis with a finer-grained case study of a question-answering
task on Pythia-2.8b. We discovered that the set of components contributing to correctly
answering the question depends on superficial features of the input sequence.
9

• Application. We apply our understanding of how models solve retrieval tasks internally to
a scalable oversight problem inspired by the phenomenon of prompt injection (Chapter 7).
Models are given inputs containing distractor sequences that trigger models to output a token
unrelated to the task they are asked to solve. The goal of the application is to detect and
eliminate the influence of distractors while requiring minimal human supervision. We present
a proof-of-concept based on request-patching that only requires humans to verify the model
output on a single trusted input. Our technique significantly improves the performance of
models on sequences with distractors (0% →70.5% accuracy for Pythia-410m, 15.5% →97.5%
for Pythia-12b), while causing no change in performance on the control inputs.
The code and the data used in this project are open-sourced athttps://github.com/aVariengien/
causal-checker/.
10

Chapter 2
Background
2.1
The Transformer architecture for autoregressive language models
An autoregressive language model, Mθ with parameters θ, maps a sequence of input tokens x =
(x1,x2,...,xn) to a probability distribution over the next token xn+1. For the Transformer architecture
[66], we have:
p(xn+1|x) = Mθ(x)
= softmax(πl(x))
The pre-softmax values π are called the logits. For the GPT-2 Transformer architecture [56] with L
layers, the function Mθ can be further broken down as follows:
πl = LN(zL
n)WU
zl
k = zl−1
k
+ al
k +ml
k
ml
k = MLP(zl−1
k
+ al
k)
= LN
³
Wout
³
GELU(Win(zl−1
k
+ al
k)+bin)
´
+bout
´
al
k = Attn(zl−1
≤k )
z0
k = WE t +WP
The final logits πl are constructed by iteratively building a series of intermediate activations zl
k called
the residual stream [22]. The residual stream zl
k at token position k and layer l is computed from
the residual stream at previous token positions at the previous layer zl−1
≤k by adding the results of
11

Attn, a multi-headed attention module and MLP a two-layer perceptron module. The MLP module
depends on the residual stream zl−1
k
at position k and layer l −1 while the attention module can
aggregate information from the previous layer across every previous token position. The residual
stream is initialized with the embeddings computed from the token and positional embedding
matrices WE, WP, and the one-hot encoding of the input tokens t. Finally, WU is the unembedding
matrix, GELU the Gaussian error linear unit activation function [29], and LN is a layer normalization
function [5] applied to the final residual stream and the output of each module.
The multi-headed attention module can be further decomposed into the contribution of H individ-
ual attention heads hi,l as follows:
Attn(zl−1
≤k ) = LN
Ã
H
X
i=1
hi,l
!
hi,l =
³
Ai,l ⊗W i,l
OV
´
· zl−1
≤k
Ai,l = softmax
³
(zl−1
≤k )TW i,l
QK zl−1
≤k
´
We used the parametrization introduced by Elhage et al. [22] using the low-rank matrices W i,l
OV
and W i,l
QK in Rd×d called the OV and QK -circuit, with d being the dimension of the model. This
parametrization separates the two functions performed by attention heads: the QK -circuit is used
to compute the attention pattern, Ai,l, weighing the contribution of each token position, while the
OV -circuit is used as a linear projection to compute the output of the head. The matrices Ai,l and
W i,l
OV are combined using a tensor product noted ⊗.
The matrices W i,l
OV and W i,l
QK are computed from the usual parametrization of attention heads using
W i,l
Q , W i,l
K , W i,l
O ∈Rd× d
H and W i,l
V ∈R
d
H ×d respectively called the query, key, values.
W i,l
OV = W i,l
O W i,l
V
W i,l
QK = (W i,l
Q )TW i,l
K
In practice, the models we study have slight deviations from the GPT-2 architecture. The Pythia
[7], Falcon [3] and Llama 2 models [65] models use parallelized attention and MLP, in the formulae
above, this translates as ml
k = MLP(zl−1
k
). Moreover, Falcon contains additional layer normalization
at the input of modules. LLama2 uses the SwiGLU activation function [62] and layer normalization
only at the input of modules.
12

2.2
Tasks and model inputs
Throughout the thesis, we use a specific font to indicate the input or expected output of a language
model, e.g. “This is a prompt”. Furthermore, when showing single tokens, we replace spaces
with _ for clarity, e.g. “_token” for “ token”.
A textual task (also referred to simply as ‘task’, ‘behavior’, or ‘dataset’) is a set of input-output pairs
(x, y) used to study a particular behavior of a language model. x is a textual input (also called a
’prompt’) to a language model and y is the ground truth completion. y can be multiple tokens long.
When y is a single token, y is called a label token.
For example, if we were studying the arithmetic abilities of language models, a task could be a set
of input sequences and label tokens such as
©
(“3 plus 8 plus 2 equals”, “_13”), (“5 plus 1
plus 3 equals”, “_9”),...
ª
.
In addition to the unstructured token inputs, it is often useful to create abstract input and output
representations to manipulate the important variables contained in the text. In the arithmetic
example, the abstract representations of the inputs could be the numbers contained in the prompts
and label tokens, e.g.
n¡
(3,8,2),13
¢
,
¡
(5,1,3),9
¢
,...
o
.
This example motivates the definition of annotated task. An annotated task T is a tuple (Tt,Ta),
where Tt is a textual task, and Ta contains the abstract representation of the elements in Tt. For a
textual instance of the task, (x, y), we denote its abstract representation with (xa, ya).
2.3
Causal interventions
The experimental paradigm of causal intervention treats the computational graph of a neural
network as a causal graph. The goal of causal analysis is to uncover the causal links relevant to
the output of the model, as well as characterize the role of the components critical for the model’s
function. To this end, researchers rely on causal interventions, experiments that replace a set of
activations with fixed values.
In this work, we use a single-input interchange intervention. It is simple form of causal intervention
where we intervene on one variable at a time by fixing its value to be the value of that same variable
on another input. We write M(x|A ←A(x′)) for the output of the model after the single-input
interchange intervention on the input x, replacing the activation of the node A by its value on the
input x′. Figure 2.1 (b) illustrates the experiment M(x|z1
1 ←z1
1(x′)) on our example task of three
number addition.
This kind of intervention can be extended to multi-input interchange interventions. Multi-input
interchange interventions fix the value of a set of intermediate variables {A1,..., Ak} with their values
13

(a) The high-level causal graph (right) starts by computing A = X +Y and B = Z, before computing
the result R = A +B. The high-level causal graph is aligned to the language model (left) through
the alignment (Π,τ). Π aligns the high-level intermediate variables to neural network components,
Π(A) = {z1
1} and Π(B) = {z1
2} while τ defines a mapping from the textual to the abstract representa-
tion of inputs and outputs.
(b) Interchange intervention on the language
model. The activations of z1
1 are fixed to be their
values on x′ = “5 plus 1 plus 3 equals′′
while the rest of the network uses the input x =
“3 plus 8 plus 2 equals′′. The result of the
interchange intervention is consistent with the
alignment defined above.
(c) Interchange intervention on the high-level
causal graph. The variable A is replaced by its
value on the input (5,1,3).
Figure 2.1: (a) Alignment between a language model and a high-level causal graph on a task involving
adding three numbers. We can verify that the alignment is faithful by checking that the outcome of
an interchange intervention on the neural network (b) matches the outcome of the corresponding
intervention on the high-level causal graph (c).
14

from the inputs {x1,...,xk}, while the rest of the model is run on the base input b. The output of the
model after a multi-input interchange intervention is denoted M (b|A1 ←A1(x1),..., Ak ←Ak(xk)).
2.4
Causal abstractions of neural networks
The framework of causal abstraction [24] aims at describing the internal process of a low-level model
M (in our case a language model) operating on textual representations in terms of a high-level,
human-understandable causal graph H operating on abstract representations. The high-level
causal graph H is linked to the model M through an alignment (Π, τ) on an annotated task T .
τ maps inputs and outputs from textual to abstract representation of the task T , and Π maps
intermediate variables from the high-level causal graph to low-level model variables.
In the example illustrated in Figure 2.1 (a), the annotated task T = (Tt,Ta) is our running example of
three-number addition. The input-output mapping τ is defined by τ(x) = xa and τ(y) = ya for all
(x, y) ∈Tt. The intermediate variables are aligned by Π with Π(A) = {z1
1} and Π(B) = {z1
2}.
The high-level causal graph is said to faithfully describe the computations of the low-level model
if there exists an alignment such that the results of every multi-input interchange intervention on
H leads to the same results as the corresponding intervention on M. Figure 2.1 illustrates two
corresponding interchange interventions on H and M according to the alignment (Π, τ). In the
example, the two interventions lead to matching results as τ(“_8”) = 8.
In practice, this perfect correspondence is only achieved on toy problems. To introduce a graded
metric of success, Geiger et al. introduce the metric of interchange intervention accuracy (IIA). IIA
measures how frequently the model and high-level causal graph output the same answer when
perturbed by corresponding multi-input interchange interventions.
Formally, if we let Z1,..., Zk denote the intermediate variables from H we have:
IIA = Eb,xk,...,xk∼Tt
·
H
¡
ba|Z1 ←Z(xa
1 ),..., Zk ←Z(xa
k )
¢
= τ
³
M
¡
b|Π(Z1) ←Π(Z1)(x1),...,Π(Zk) ←Π(Zk)(xk)
¢´¸
15

Chapter 3
Related work
3.1
Explanations of model behavior
The field of explainable AI (XAI) has been focused on building tools to find out why a model takes
certain decisions. To answer this question, different types of explanations have been proposed.
Feature attributions
A set of techniques attempt to attribute the decision of a model to specific
parts of the input called features. Here, an explanation is a set of weights representing the contribu-
tion – negative or positive – that each input feature has on the model output. Different methods,
such as the popular SHAP [42] and LIME [59] have been proposed to give model-agnostic feature
attributions. Another set of techniques, called saliency maps [63], focuses on providing feature
attributions for vision models in the form of an image mask.
Probes
Researchers have introduced classifiers called probes [2] to recover concepts from the
intermediate activations of a neural network. The success or failure to train the probe indicates
the presence or absence of the concept in the activation. The motivation is that discovering the
concepts present in the intermediate representations should help in understanding why the model
produces a given output. However, probing alone cannot tell if a concept is used by the model for its
prediction [58].
3.2
Mechanisms as explanations
Explanation in the form of input features or intermediate concepts does not specify how inputs or
concepts are combined by the model to produce an output. Mechanistic interpretability approaches
16

the problem as a natural science [49], trying to uncover the principles underlying models’ internal
structure. With this approach, researchers aim to provide explanations in terms of mechanisms,
human understandable causal models describing the complicated dynamics of model internals.
BERTology
A body of work called BERTology provides a detailed analysis of BERT’s inner workings,
acting as a “model organism” for interpretability [60]. One of the key interests of these works is
understanding how complex representations are built across layers. To this end, researchers have
proposed different classifications of attention heads depending on their attention patterns [15, 33],
investigating how their roles evolve across layers. Tenney, Das, and Pavlick used linear probes to
show that various classical NLP concepts can be recovered from intermediate activations, with more
complex concepts existing at later layers [64].
Circuit analysis
A recent set of works provide explanations in the form of circuits [57], sparse
subgraphs of a model’s computational graph that are human-understandable and responsible for a
given behavior. Cammarata et al. extensively investigated a circuit performing curve detection in
vision models [11]. Elhage et al. extended this framework to the Transformer architecture [22]. They
discovered induction heads, heads that search for previous occurrences of the present token and
copy the token that occurred directly after the previous occurrence. In follow-up work, Olsson et al.
provided evidence that induction heads exist in large models and are key pieces of their in-context
learning abilities. This framework has been further applied to study GPT-2-small solving simple
linguistic tasks [70] and rudimentary mathematical problems [28].
Scaling up mechanistic interpretability
Given that mechanistic interpretability studies are often
labor-intensive, recent works have proposed ways to automate part of the research process. A first
approach introduced an algorithm for automatic circuit discovery using causal intervention [17].
In a second approach, Bills et al. used GPT-4 to automatically interpret the activation pattern of
individual neurons in natural language [8].
3.3
Causal analysis for interpretability
Causal interventions
Given the ease of performing interventions on neural networks, different
works propose leveraging causal analysis for interpretability. While the core of every intervention is
the same – setting a set of intermediate variables to a fixed value – the choice of the value and the
variable can differ depending on the goal of the study.
A first goal of causal intervention is locating where in the network some target functions are per-
formed. To this end, Michel, Levy, and Neubig fix the output of attention heads to zero to prune
17

the neural network, finding the minimal set of components needed to solve a task [46]. Meng et al.
locat MLP blocks involved in factual recall in LLMs by performing interventions using activations
corrupted with Gaussian noise [44]. A second goal of causal intervention is to understand which
function a given component is fulfilling. To this end, researchers have introduced interchange
operations that choose a fixed value from a forward pass on a new input. These interventions have
been applied to attention heads and MLP blocks to investigate gender bias [68] and variable binding
[19]. Recent work proposes a more fine-grained division of models by performing interchange
interventions on paths instead of variables [27], enabling a precise characterization of indirect
effects.
Validating explanations using causal intervention
To test if mechanistic hypotheses are faithful
to the model computation, two frameworks have been proposed: causal abstraction [24] and causal
scrubbing [13]. Both frameworks involve a similar process. First, a hypothesis is formulated using
a high-level causal graph, an alignment between the high-level causal graph and the model, and
a dataset. Then, a series of interchange interventions is automatically generated to stress-test the
prediction made by the hypothesis. Each interchange intervention comes with an expected outcome,
i.e. the outcome predicted by the hypothesis. The frameworks measure how well the hypothesis
describes the model’s internals by running a series of interchange interventions and comparing the
intended and the realized outcomes. The two frameworks differ in that causal scrubbing is more
precise but computationally more costly as it relies on paths as variables for interventions, whereas
causal abstraction considers components. Additionally, causal scrubbing only allows interventions
that do not change the output of the high-level causal graph, while causal abstraction allows these
interventions. In this work, we use the framework of causal abstraction because of its computational
tractability when dealing with large models.
3.4
Designing datasets to study model behavior
Analyzing the methodology behind datasets for mechanistic interpretability reveals a significant
disparity in their breadth and complexity compared to those used in behavioral analysis of LLMs.
Datasets for mechanistic interpretability case studies are constrained by the need for precise control
at the level of tokens to perform causal interventions. Hence, they are often created using templates
that are filled by placeholder values [70, 28, 73]. In addition to considering narrow datasets, inputs
are often simply structured, e.g. encoding simple abstract representations such as knowledge
relations [44].
In contrast, extensive efforts have been made to create comprehensive benchmarks to evaluate
diverse aspects of LLM behavior such as common sense reasoning, toxicity, or world knowledge [39,
30]. Behavioral analyses studied input sequences with complicated structures. For instance, Dziri
et al. represented logic puzzles as computational graphs to map the limits of LLM reasoning [21].
18

Chapter 4
Constellations: generalizing causal
analysis with abstract input
representations
Creating datasets for causal interventions in interpretability requires precise control over individual
tokens. However, many datasets from the existing literature are introduced in an ad-hoc manner,
with their foundational design principles seldom explicitly discussed. We address this gap by
introducing “task constellations” as a method for systematized, scalable causal analyses. We first
discuss the challenges of building datasets suitable for causal analysis of language models, and we
then present a list of desiderata for these constellations to meet the challenges.
4.1
The challenges of building datasets for language model interpretabil-
ity
4.1.1
Tokenization
Language model inputs are sequences of tokens, or subword units, defined based on their frequency
in a reference dataset. This poses several challenges.
Firstly, meaningful parts of a sentence, such as its subject, can be split over multiple tokens. If
we want to alter the subject’s representation through causal intervention, deciding which token to
modify is unclear.
Model outputs are also in tokens. When evaluating these outputs, two issues arise. One, the
right answer might be split between tokens, such as a capitalized and non-capitalized version of a
19

word. Two, the first token of distinct answers could be the same, making single-token predictions
insufficient to determine accuracy. Solutions like combining probabilities of similar tokens or
unrolling predictions over several tokens may be needed for a complete evaluation at the cost of
increasing the complexity of the experiments.
Tokenization can lead to unexpected failure cases. Take the example of the sentence “After the
year 1899 comes the year 19”, and the expected completion “00”. Using the GPT-2 tokenizer,
“1900” is one token. This means the phrase “After the year 1899 comes the year 1900” does
not break down in the expected way. As a result, the model would not recognize the need to predict
“00” as the bigram (“19”, “00”) never occurred in training.
Additionally, the unique tokenization method each model employs hinders universal studies across
models. For example, GPT-2 handles numbers differently from the LLaMA2 models [65]. While
GPT-2 employs Byte Pair Encoding [61], LLaMA 2 treats each digit as a separate token.
While these challenges are more technical annoyances than major barriers, they can make dataset
design error-prone, requiring extensive manual inspection.
4.1.2
Interpreting language model outputs
Language models are trained to predict a probability distribution for the next token based on context
from vast datasets, including both accurate and flawed text. Most new sentences would not be
directly in the training data, making it tricky to determine the “correct” next-token distribution.
Interpretability researchers often study predictions with clear answers. They look for prompts where
modeling language is a defined task. As an illustrative example, using “TL;DR” prompts the model
to summarize, as training data frequently contains summaries after “TL;DR”.
Understanding model failures is challenging because it is unclear if a failure is due to prompting
or capability failure. The first means the model can solve the task, but the prompt is not clear, i.e.
it does not match closely a pattern from the training dataset. The second implies the task is too
complex for the model.
Even when the model correctly addresses a task, it may still assign high probabilities to tokens that
are not valid task answers. This arises from the challenge of crafting prompts that point to just one
appropriate answer. For instance, in a sentence like “Bob and Mary went to the store, Bob gave a
drink to,” several endings such as “Mary,” “her,” and “the” are possible. However, only “Mary” truly
answers the indirect object identification (IOI) task.
20

Figure 4.1: Illustration of different types of datasets for investigating the abilities of language models.
Benchmarks aim at faithfully mapping the frontier of language models’ abilities through behavioral
analysis. Because of experimental constraints, interpretability case studies are often focused on a
narrow task. Constellations are collections of narrow tasks spanning different domains while being
connected by a common structure, enabling the design of causal experiments at scale.
4.2
Constellations
In earlier research, these challenges were addressed by trading off task generality against constrained
datasets to make the analysis tractable. Typically, task datasets were designed using manually
created templates filled with placeholder keywords, and experiments would be tailored to the
specific structure of a dataset. Increasing the study’s scope by using different datasets would require
linearly increasing human effort.
To enable scalable causal analyses of case studies, we present the concept of a “task constellation”.
This approach partially avoids the redundancy of task-specific work by offering a unified structure for
designing and analyzing experiments on multiple tasks simultaneously. We define task constellations
using a detailed set of criteria, making explicit the principles we follow to design datasets.
Tasks within a constellation are narrow, but a constellation contains tasks that cover various domains
and complexities. Results applicable across a constellation are thus likely more reliable, not relying
on a single task’s specifics. Figure 4.1 illustrates abstractly the differences between case studies,
benchmarks, and task constellations.
4.2.1
List of desiderata for constellations
A constellation is a set of annotated tasks that meet the following conditions. For each condition, we
present the motivations behind it.
21

1. Structured. Tasks from a constellation share the same abstract representation, i.e. every
representation (xa, ya) for every task T shares the same structure. Motivation: Providing a
unified structure to run causal interventions at scale.
2. Decomposable. Tasks can be solved at an abstract level (i.e. using the input from Ta) using a
common causal graph decomposable into a series of intermediate variables. Interchanging
the high-level intermediate variables across inputs from the same task leads to meaningful
output. Motivation: Enabling the design of interchange interventions.
3. Single token. Tasks are elicited in a single forward pass. Studying a single next-token predic-
tion is enough to check if the model correctly solved the problem instance x, i.e. the expected
completion y is always a single token. Motivation: Making experiments easy to measure and
computationally efficient.
4. Monotasking. Tasks are clearly defined by their textual inputs. Every textual input x defines a
clear task and y is the single possible answer for the task instance. Motivation: Tractability of
the analysis. It is easier to understand models solving a single problem than solving multiple
problems in parallel.
5. Flexible. The constellation abstract representation can be applied to describe diverse tex-
tual tasks spanning multiple domains and levels of complexity. Motivation: Enabling rich
comparative analysis across models and domains.
22

Chapter 5
ORION : a constellation of retrieval tasks
for language models
In-context learning is fundamental to language models’ capabilities, allowing them to navigate
varied contexts and address varied tasks. Yet, the mechanisms behind these abilities remain largely
unknown.
Our study concentrates on retrieval, a fundamental aspect of in-context learning, which involves
identifying the correct attribute (e.g. the name) of an entity (e.g. a narrative element, in the example
below, the city of the story) from the context (e.g. a short story). To facilitate this study, we develop
the Organized RetrIeval Operations for Neural networks (ORION) constellation. We’ll introduce it
using the following question-answering example.
Here is a short story, read it carefully and answer the questions below.
Story: In the lively city of Valencia, spring mornings [...] as a skilled
veterinarian [...] "I’m Christopher" he replied, [...].
Question: What is the city of the story? The story takes place in
5.1
Abstract representation
Each input from ORION can be represented as a context C and a request R. The context C is a set
of entities, where each entity is defined by a set of attributes. C can be organized in a table like the
example shown in Figure 5.1 where columns are attributes and rows are entities. The request R is
defined by a target attribute AT T Rt, a filter attribute AT T R f , and a filter value v f . A request can be
23

ATTR1
ATTR2
ATTR3
Entity1
v1,1
v1,2
v1,3
Entity2
v2,1
v2,2
v2,3
Name
Role
Entity1
_Valencia
City
Entity2
_Christopher
Main Character
Figure 5.1: Left: Abstract context representations of the ORION tasks are organized in tables where
columns are attributes and rows are entities. Right: abstract representation of the context of the
question-answering example.
formulated using a language in the style of a SQL request as follows: SELECT AT T Rt(e) FROM C
WHERE AT T R f (e) = v f , requesting the target attribute of an entity from the context filtered such that
the filter attribute of the entity equals the filter value. In the example, R is SELECT Name(e) FROM
C WHERE Role(e) = Cit y. Finally, the output of executing the request on the context is denoted
R(C). In the example, we have R(C) = “_Valencia”.
The context representation is such that the abstract output R(C) is equal to the token used as a label
for the task. In other words, for every task instance (x, y) from ORION , ya = y.
5.2
Implementation of the constellation desiderata
We constrain the inputs of the ORION task to enforce the constellation desiderata stated in the
previous section.
1. Structured. Every textual input in the ORION constellation accepts an abstract representation
using the context and request representation defined above.
2. Decomposable. For every annotated task T in ORION , for every (C1,R1),(C2,R2) in T , R2(C1)
and R1(C2) are well-defined. This means that an arbitrary request can be applied to an
arbitrary context from the same task. Abstract representations of requests and contexts can
be freely interchanged across a task.
3. Single token.. For every annotated task T in ORION we have: ∀(C1,R1),(C2,R2) ∈Ta,R1 =
R2 ⇔R1(C1) = R2(C1). In other words, in a given context, the output of each request gives a
unique label. It ensures that measuring the next-token prediction is enough to know which
request has been answered.
4. Monotasking. For every abstract input representation in ORION , there is a unique entity
e such that AT T R f (e) = v f and a unique attribute AT T Rt such that AT T Rk(e) = y. This
condition ensures that requests have a single answer.
5. Flexible. We demonstrate the flexibility of the ORION structure by creating 15 different tasks
spanning six different language model abilities.
24

Figure 5.2: The semi-automatic task generation process used to create ORION . We use ChatGPT to
create a template and values for the placeholders given a problem type. To generate an instance from
the task, we start by randomly selecting placeholder values to create an abstract input representation.
Then, we use a format string to fill the template. When we need more flexibility, we use GPT-4 to
incorporate the placeholder values into the template.
In the code implementation, we add algorithmic tests to ensure that conditions “Decomposable”,
“Single token”, and “Monotasking” are respected for every task in ORION .
5.3
Creating ORION tasks
To create task instances in practice, we use a semi-automatic process illustrated in Figure 5.2,
leveraging the creative writing ability of the ChatGPT application1. Concretely, we use the following
workflow:
1. Find a problem that can be formulated as a retrieval task, e.g. question-answering.
2. Use ChatGPT to create a template with placeholders, e.g. a story with placeholders for narrative
variables such as the city, the name of the character, and the question being asked.
3. Use ChatGPT to create a set of placeholders.
4. Procedurally generate a set of abstract representations for the contexts and requests.
1https://chat.openai.com/
25

5. Generate the textual inputs from the abstract representation using format strings or ChatGPT
when more flexibility is required.
For a given problem type, we generate several templates, enabling the creation of several task
variations. We use this procedure to generate 15 unique tasks spanning six different abilities.
We use several criteria in choosing the problem types. First, we choose tasks that have already been
studied in the literature to act as reference points for our analysis. This includes factual recall and the
induction task. Then, to allow analysis across model scales, we design a simple question-answering
task that can be solved by both small and large models. We also create more challenging tasks to
explore diverse skills such as coding abilities, tracking the association between an object and its
quantity, and tasks involving translation from English to three different languages.
In addition to diversifying the content of the tasks, we create structural modifications to the task
template. To that end, we create question-answering templates where the question is before the story
in the dataset and templates where the final token of the prompt does not depend on the request.
We also create a mixed question-answering task containing prompts from the three variations.2 We
give an example input-output pair of each task in the Table 5.1 and provide a detailed description of
each dataset in Appendix D.
5.4
Performance metrics
We use three different metrics to quantify the performance of a language model on an ORION task
T .
• Accuracy: E(x,y)∼Tt [M(x) = y]
• Token probability: E(x,y)∼Tt [p(y|x)]
• Logit difference: E(x,y)∼Tt,(R′,C ′)∼Ta,R̸=R′[πy(x)−πR′(C)(x)]
Accuracy serves as our primary metric to assess model performance in solving tasks due to its
straightforward interpretation and practical application in language models, where the most proba-
ble token is often chosen.
However, accuracy falls short in capturing nuanced aspects of predictions, for instance when a
token is the most probable by a high margin. To have a granular evaluation of model behavior after
interventions, we employ token probability, offering a continuous measure.
2Given that the mixed-template task is an aggregation of other task variations, we do not include it in the count of 15
unique tasks.
26

Figure 5.3: Accuracy of 18 models on the ORION task constellation. Models with more than 7
billion parameters are able to robustly solve every task. However, simple tasks such as the base
question-answering can be solved by models as small as GPT-2 small (125 million parameters),
enabling comparative studies across a wide range of model scales.
Finally, logit difference is used as a secondary performance metric to quantify performance on the
task without the distortion from the final softmax’s non-linearity. It measures the extent to which the
model predicts the correct token over tokens that are answers to other requests in the same context.
We use logit difference to guide our detailed analysis of Pythia’s question-answering mechanisms
(Appendix A).
5.4.1
Model performances
We evaluate the performance of 18 models from four different model families: GPT-2 [56], Pythia [7],
Falcon [3] and Llama 2 [65], including the largest open-source LLM available (Llama 2 70b). We study
base language models for all families except Falcon where we include two instruction fine-tuned
models. We choose the models to capture diverse scale, architecture, and training techniques.
Figure 5.3 shows the accuracy of every model on all tasks in ORION . Unsurprisingly, larger models
can solve a wider range of problems. Nonetheless, even GPT-2 small with 125M parameters, one
of the smallest models, can reliably solve the simplest version of the question-answering task.
Evaluations using the token probability and logit difference are available in Appendix B.
In the following analyses, we only consider settings where the model can robustly solve the task.
Thus, we focus on pairs of models and tasks that have greater than 70% accuracy.
27

Task name
Example Prompt
Label token
Variations
Question-
answering
(base)
Story: In the lively city of Valencia, spring mornings
[...] as a skilled veterinarian [...] "I’m Christopher"
he replied, [...].
Question: What is the name of the main character?
Answer: The main character is named
_Christopher
1
Question-
answering
(uniform prefix)
Story: In the lively city of Valencia, spring mornings
[...] as a skilled veterinarian [...] "I’m Christopher"
he replied, [...].
Question: What is the name of the main character?
Answer: The answer is "
Christopher
1
Question-
answering
(question first)
Question: What is the name of the main character?
Story: In the lively city of Valencia, spring mornings
[...] as a skilled veterinarian [...] "I’m Christopher"
he replied, [...].
Answer: The answer is "
Christopher
1
Question-
answering
(mixed
templates)
Uniform distribution of prompts from three varia-
tions of question-answering above.
1
Translation
English:
In an era defined by increasing global temperatures [...]
At the forefront is M. Smith, a marine biologist [...]
Next, we turn to M. Miller, a climate economist [...]
French:
[...]
Nous nous tournons ensuite vers M.
_Miller
3
Factual recall
Question: On which continent did Muhammad Ali live?
Answer:
_America
2
Quantity
tracking
Anthony has a collection of pencils. 50 pencils are
blue, 10 pencils are red, and 20 pencils are green.
How many pencils in total are either blue or green?
We’ll add the number of green pencils (
20)_
3
Induction
pattern-
matching
xnGWu:nJIbF
etmNX:TzgIS
ZvcIf:Gcqvs
[...]
xnGWu:nJIbF
etmNX:
T
1
Type hint
understanding
def calculate_circumference(circle: Circle) -> float:
[...]
W = Rectangle(Point(2, 3), Point(6, 5))
D = Circle(Point(0, 0), 5)
print(calculate_circumference(
D
3
Table 5.1: Tasks from the ORION constellation contain varied problem type and prompt format. For
readability, we use [...] to shorten the prompts. The rest of the text is part of the textual input. In
particular, “[...]” is part of the prompt for the translation task. We use _ to indicate a space in the
label token.
28

Chapter 6
Causal analysis on ORION
We leverage the ORION task constellation to run causal analysis on an array of open-source language
models.
To correctly solve retrieval tasks, the model has to combine information coming from both the
request and the context at the last token position. We focus our investigations on understanding
how these two processing steps are organized in the intermediate layers of the last token position.
We choose to consider a coarse-grained division of the model, intervening on full layers instead of a
finer-grained division, e.g. considering single-attention heads and MLP blocks. We find this level of
analysis is sufficient to develop a high-level causal model of how language models solve retrieval
tasks while providing a computationally tractable set of experiments to run at scale on our set of
models and tasks. We complement this general coarse-grained analysis with a finer-grained case
study on Pythia-2.8b solving a question-answering task.
6.1
Methods
Our main experimental technique is residual stream patching. Residual stream patching is a single-
input interchange intervention, replacing the residual stream at a layer L at the last position in the
forward pass of the model on input x2 with its activation from another input x1. Following the nota-
tion introduced in 2.4, we note M(x2|zL
n ←zL
n(x1)) the model output on x2 after this intervention.
As shown in Figure 6.1, residual stream patching makes every component before layer L have the
activation it takes on x1 while the components after layer L receive mixed activations (denoted by
the yellow color in the figure). These later layers see activations at the last position from earlier
layers coming from x1 while activations from earlier positions come from x2.
29

Figure 6.1: Residual stream patching involves patching the hidden representation at an intermediate
layer of the model run on input x1 into the forward pass on the input x2. Patching the mid-layer
residual stream on a retrieval task from ORION causes the model to output a modular combination
of the request from x1 (asking for the city) and the context from x2 (a story about Bob in Paris). We
call this phenomenon request-patching.
To characterize the output of the patched model, we measure the token probability and accuracy for
three different label tokens related to the inputs x1 and x2. We use both label tokens from the input
x1 and x2, R1(C1) and R2(C2) respectively, and the label token R1(C2) that is the result of applying
the request from x1 on the context of x2.
To facilitate comparisons between different tasks and models, we normalize the token probability
based on the mean probability of the correct token given by the model for the task. In addition, we
calculate the normalized accuracy where 0 represents the accuracy of a basic model responding to
a random request in a given context and 1 denotes the model’s accuracy for that task. It is worth
noting that the normalized accuracy can fall below zero if the model’s performance is poorer than
randomly guessing. Normalized accuracy and token probability can also become greater than 1
if the intervention puts the model in a better situation to solve the task. In the majority of cases,
causal interventions disrupt the model mechanism and thus rarely increase performance.
We perform residual stream patching at the last position for every layer, model, and task of the
ORION constellation. For each task, we use a dataset of 100 prompts and average the results of 100
residual stream patching experiments with x1 and x2 chosen uniformly from the task dataset.
In the following section, we present the phenomenon of request-patching on the question-answering
task on a single model. We extend the findings to several models and then to every model and task
in ORION . We unify our findings in a high-level causal graph that we validate using the causal
abstraction framework. We conclude the section by providing an overview of a low-level case study
on Pythia-2.8b solving a question-answering task.
30

6.2
Results of residual stream patching
Figure 6.2 shows the results of residual stream patching on the question-answering task with a
uniform answer prefix for the Pythia-2.8b model. We observe that after residual stream patching on
the layer before layer 13, the model is outputting R2(C2) with 100% normalized token probability.
Our interpretation is that this intervention does not perturb the model processing of x2. We further
observe that residual stream patching after layer 27 causes the model to output R1(C1) with more
than 80% normalized token probability. In effect, patching the residual stream after a certain layer is
equivalent to hard-coding the model output on x1.
Surprisingly, when patching between layers 15 and 16, we observe that the model outputs R1(C2) with
100% normalized accuracy, i.e. with the same accuracy level as the baseline task accuracy. The model
is outputting the results of the request of the input x1 in the context of the input x2. Moreover, Figure
6.2 shows that the probability of the correct token is of the same order as the baseline probability
of the correct token on the task. We call this phenomenon, request-patching, i.e. a residual stream
patching experiment that leads to the R1(C2) token label being confidently outputted by the patched
model. Such results demonstrate that the causal intervention coherently intervenes in the model’s
internal computation, causing it to modularly combine high-level information from two different
prompts.
We observe a sudden jump in the normalized accuracy of request-patching from 0 to 1 between
layers 14 and 15. However, it is likely that transforming the sequence of tokens representing the
question into a representation of the request takes several layers. Thus, we hypothesize that a large
part of the request processing happens at the previous token positions of the question. In this
interpretation, the observed jump at layer 15 results from the intermediate representation of the
request being propagated to the last position through attention modules.
Defining limit layers.
From the results of the residual stream experiments, we define L1, L2, and
L3 as three layers delimiting the three different outcomes of residual stream patching.
• L1 is the maximal layer at which the normalized token probability of the token label R1(C1)
is greater than 80%. It marks the end of the region where residual stream patching does not
interfere with the model output.
• L2 is the layer where the normalized probability of the label R1(C2) is maximal. It is the place
where the effect of request-patching is strongest.
• L3 is the minimal layer where the normalized probability of the label R2(C2) is greater than
80%. It marks the start of the region where residual stream patching leads to a complete
overwrite of the model output.
31

Figure 6.2: Normalized token probability and accuracy for the label tokens R1(C1), R1(C2) and R2(C2)
after patching the residual stream across all layers. Patching early (before L1 = 13) and late (after
L3 = 27) leads to the expected results, respectively no change in output and patching the output from
x1. However, intervening on the middle layer (L2 = 16) leads to the model confidently outputting
the token R1(C2), a modular combination of the request from x1 and the context from x2.
We choose the token probability as a continuous metric to measure the model prediction. The 80%
threshold has been chosen arbitrarily as a criterion to consider that the model is mainly outputting
a single label token.
Request-patching is general across models.
Having introduced the phenomenon of request-
patching, we turn to explore its generality across models. The residual stream patching results for all
models able to solve the question-answering task with uniform prefixes are shown in Figure 6.3. We
observe a similar layer organization as the one presented for Pythia-2.8b. For all models, there exists
a span of intermediate layers (40-80% of the model depth) where residual stream patching leads the
model to output R1(C2) with a high probability (>80% normalized probability). This span of layers
seems to be the same for the base and fine-tuned Falcon models. This is coherent with the intuition
that fine-tuning is only superficially affecting the model internals.
Request-patching is general across datasets.
We further expand our investigation of request-
patching to include every model and task from the ORION constellation. To ease the visualization,
we only show the maximal normalized probability of the R1(C2) label token. We use this metric as
our main performance indicator to measure the strength of the request-patching phenomenon on a
given pair of model and task. The results are shown in Figure 6.4.
98 out of the 106 pairs of tasks and models studied demonstrate request-patching with at least 70%
normalized probability of the R1(C2) label token. Request-patching appears across variations in
domain, task complexity, and low-level prompt structure. Moreover, it is present in every model
studied, from GPT-2 small to Llama 2 70b, the largest available open-source LLM.
The results for the question-answering with mixed template task demonstrate that request-patching
32

Figure 6.3: Normalized probability of the label tokens after residual stream patching across all layers
on the question-answering task with uniform prefix. To enable comparison across models, we
use the relative layer with 0 as the first and 1 as the last layer. Request-patching is general across
models: mid-layer residual stream patching causes the model to output R1(C2) with more than 80%
normalized token probability.
works even when patching the residual stream across different templates, e.g. taking the residual
stream from a prompt where the question is before the story and patching it in a model execution
where the question is after the story. This means that the representation stored in the patched
activation is related to the semantic meaning of the question, and not surface-level textual features.
However, the phenomenon of request-patching seems not to be present in the abstract induction
task on large models. We hypothesize that this is due to the increased wideness of large models and
the simplicity of the task. We discuss this case in more detail in the next paragraph.
To further our analysis, Figure 6.5 shows the values of the layer L2 on different models and datasets.
We observe that the effect of request-patching for the induction tasks is the strongest at earlier layers
compared to the other tasks. This observation is consistent with the simplicity of the request pro-
cessing, which only involves copying previous tokens. The L2 layers for other tasks are concentrated
in similar layers, suggesting a similar high-level organization of the internal computation that does
not depend on the details of the task being solved. However, for Llama 2 70b, the largest model
studied, the L2 layers are concentrated in the same narrow range (39-43) for every task, including
the simple induction tasks. It is unclear if this disparity is caused by its larger scale or by the specifics
of the architecture.
We provide additional results of residual stream patching on Llama 2 70b as well as visualizations of
layers L1 and L3 in Appendix C.
33

Figure 6.4: Maximal normalized probability of the R1(C2) label token after residual stream patching
on all models and tasks from the ORION constellation. Request-patching generalizes to the vast
majority of tasks and models studied.
Figure 6.5: Layer of maximal request-patching performance L2 for different models and tasks. While
the L2 layers for most tasks are concentrated at similar layers, the processing of the request in the
induction task seems to happen at earlier layers.
34

6.3
Validation against prior work
Factual recall
The factual recall and abstract induction tasks from ORION have been previously
studied in the mechanistic interpretability literature. In this section, we show that the mechanisms
described in previous works are compatible with the results of request-patching.
Previous works studied the factual recall abilities of language models on prompts represented by a
triplet (s,r,a) where s is a subject, r is a relation being queried, and a is the corresponding attribute,
i.e. the value of the relation on the subject. A prompt would contain the subject and relation while
the attribute would define the label token, e.g. “Beat music is owned by” →“Apple”.Geva et al.
[26] show that early attention layers at the last token position are used for relation propagation,
propagating information from the relation token to the last position, e.g. the information about the
relation “owned” to the “by” token in the example. Later layers are in charge of attribute extraction.
They recover the correct attribute from the last subject token, according to the relation propagated
to the last position by the earlier layers.
Using the ORION input representation, the relation is part of the request, while the subject is in
the context. When performing residual stream patching at intermediate layers, we observe request-
patching: the information from the relation in x1 is transferred but the subject stays the same. Our
observation is coherent with the finding from Geva et al. that relation propagation and attribute
extraction happen at non-overlapping layers.
Note that contrary to Geva et al. we do not use the dataset Counterfact. This dataset cannot be
incorporated into ORION because of the “Decomposable” desiderata for task constellations. Most
of the relations in the Counterfact dataset cannot be applied to arbitrary subjects, e.g. a famous
person does not have an attribute for the relation “capital city”. To circumvent this limitation, we
create two datasets that fit the “Decomposable” desiderata, enabling the design of systematic causal
experiments. We document this process in more detail in Appendix D.3.
Induction
The induction task consists in completing patterns of the form [A] [B] ... [A].
For instance, such patterns occur naturally when completing a name that appeared before in the
context, e.g. “Harry Potter ... Harry Pot” →“ter”. The mechanisms for induction tasks were
first characterized in small two-layer Transformers in [22]. The mechanisms involve two steps: the
first step consists in previous token heads acting at the [B] position copying the preceding token [A].
The second step involves induction heads acting at the [A] position. In a follow-up paper, Olsson
et al. hypothesize that induction heads are also present in large models and recognize more complex
patterns with a similar structure such as [A] [B] ... [A*] [50]. In this case, [A] and [A*] can
be composed of several tokens and be recognized using fuzzy matching instead of exact token
matching. They propose a similar high-level structure as the simple mechanism: the representation
at the position [B] is contextualized by incorporating information about the preceding prefix [A],
using a more advanced mechanism than the previous token heads. Similarly, the representation
35

of the last token incorporates information about the tokens from [A*]. At later layers, induction
heads leverage their attention mechanisms to recognize the similarity between the representations
of [A*] at the [B] token position and the representation of [A] at the last token position. Finally,
their OV circuit copies the [B] token.
The induction task we designed involves multi-token prefixes with exact matches. We study patterns
of the form [A] [X] [B] .... [A] [X], where [X] is a separator token, a column in our case.
According to the extended mechanism for induction, the residual stream at early layers at the last
token contains the information propagated from the second [A] occurrence, while the later layer
contains induction heads in charge of finding the [B] token in the broader context. If the [A]
propagation and the induction heads occur at non-overlapping layers, patching the early residual
stream should only modify the representation of the token [A] at the last token position without
impacting the retrieval abilities of the induction heads. In the ORION abstract representation, [A] is
the request in the induction task. Hence the proposed mechanism for induction heads is coherent
with the results of request-patching.
Propagating the [A] token to the final residual stream and the operation of the induction heads are
both simple operations, each of these operations can theoretically be performed in a single layer.
We hypothesize that these two operations are performed redundantly by two sets of components
acting in tandem, a first set to propagate information from [A] to the last token, and a second set
of induction heads. We hypothesize that these two sets of components are situated at overlapping
layers in large models as part of pre-processing happening in early layers. Large models have the
capacity for redundant parallel computation because of their large number of attention heads per
layer. This hypothesis would explain the lower performance of request-patching on induction tasks
in large models. No layer separates the request and its processing: due to the simplicity of the task,
they both happen in parallel.
6.4
A high-level division of model computation.
In this section, we express the implications of request-patching on the high-level structure of the
computation happening in language models solving retrieval tasks. Given the autoregressive nature
of the language models under consideration, the relative positions of the textual representation of
the request and the context matter to understand the dependencies of the different variables. Here
we focus on the more common case of the request being situated after the context, this implies that
the tokens in the context cannot depend on the request.
The three layers L1, L2, and L3 defined in Section 6.2 should not be understood as specific places in
the network where specific computation happens. Instead, it is more accurate to think about them
as interfaces where information processed in one part of the network is passed to another part to be
combined in a modular way.
36

To express this high-level division of inner computation, we call context and request processing any
activation that influences the model prediction and depends mainly on the tokens from the textual
representation of the context and request, respectively.
More concretely, we can combine the constraint of the autoregressive architecture and the results of
request-patching to bound where context and request processing happen, as illustrated in Figure
6.6. The processing of the request (in red in the figure) happens between layers 0 and L1 at the
token position corresponding to the request before the last position. The process continues from
layers L1 +1 to L2 at both the request token position and the last position and terminates at layer L2.
Context processing (in green in the figure) happens at every position except the last between layers
0 and L3 −1. From layers L2 +1 to L3 at the last token position both context and request processing
happen in order to execute the retrieval operation (in dark yellow in the figure).
The layers from 0 to L1 at the last token position (in gray in the figure) do not execute any processing
that depends on the particular values of the context and request but could be involved in general
preprocessing that is constant across prompts, e.g. processing the instructions of the task.
This description is coarse-grained and only bounds where the computation can happen without
making precise claims about where it effectively takes place, as we do not study token positions
other than the last. To simplify the description, we consider the limit layers L1, L2, and L3 as exact
delimiters. In reality, the divisions described should be seen as blurry boundaries.
Validating the high-level causal graph using the framework of causal abstraction.
We formalize
this high-level division of the internal computation of language models using the framework of causal
abstraction. We define a high-level causal graph operating on the abstract input representation
and an alignment mapping each intermediate variable in the high-level causal graph to a set of
model components. The input-output alignment is defined by the ORION abstract input and output
representation. The alignment is illustrated in Figure 6.6.
Our causal graph is a simple two-step symbolic algorithm that treats the request and context sepa-
rately before combining them to algorithmically solve the retrieval task.
We validate the alignment using interchange intervention accuracy (IIA). As presented in Chapter
2.4, IIA is defined as an average over every possible multi-input interchange intervention. However,
this average introduces statistical distortion in the case of the alignment we are considering. Because
of the shape of our causal graph, interchanging a variable late in the graph screens off the effect of
the interchange happening earlier in the graph. Thus, intervening simultaneously on early and late
variables is equivalent to interchanging the late variable alone. To remove this statistical distortion,
we average the results of the interchange interventions such that each unique experiment gets the
same weight.
Moreover, given that residual stream patching is a kind of interchange intervention, we reuse the
37

Figure 6.6: Alignment between a high-level causal graph that uses abstract representations of inputs,
and a language model running on the textual representation of the inputs for a retrieval task. The
alignment bounds the position where request processing (in red) and context processing (in green)
are located in the intermediate layers of the model. The Nil node is isolated in the high-level causal
graph. It does not influence the output of the causal graph and thus can be interchanged freely.
38

experimental data from the exploratory causal analysis to compute the IIA. Given the simplicity of
our alignment, we can write the IIA for a task T from ORION in terms of three interchange operations
as follows:
IIAT = 1
3Ex1,x2∈Tt
·h
M(x2|zL1
n ←zL1
n (x1)) = R1(C1)
i
+
h
M(x2|zL2
n ←zL2
n (x1)) = R1(C2)
i
+
h
M(x2|zL3
n ←zL3
n (x1)) = R2(C2)
i¸
We do not include the results of interchange intervention on the context variable. Given the model
architecture, the two interchange operations M(x2|zL
n ←zL
n(x1)) and M(x1|zL
<n ←zL
<n(x2)) are
equivalent. The first one corresponds to the intervention on the request in the high-level causal
graph, and the second corresponds to the intervention on the context. Moreover, our task datasets
are defined by independently sampling R and C. This means that by definition, the average output
of M(x2|zL
n ←zL
n(x1)) and M(x1|zL
<n ←zL
<n(x2)) are the same. We thus remove the results of the
intervention on the context from the average to avoid artificial duplication of experimental results.
To facilitate the comparison across tasks, we normalize the IIA such that 0 corresponds to random
guesses and 1 is the baseline accuracy on the task. Note that the normalized IIA could be greater
than 1 if the causal graph also explains the mistakes of the model. However, we consider a simple
high-level causal graph that always answers the correct token such that the baseline model accuracy
is a natural upper bound for the IIA.
Finally, it is worth noting that the first and last terms of the expression of IIAT are dependent on the
arbitrary threshold we use to define L1 and L3. Choosing a higher threshold would be an artificial
way to increase the IIA. However, this would also make the alignment less expressive as L1 would
tend to be 0, and L3 would tend to be the last layer, effectively making these parts of the alignment
trivial. The thresholds thus represent a tradeoff between the strictness of the hypothesis and the
ease of validating it.
The normalized IIA scores for each model and task studied are shown in Figure 6.7. We observe that
the majority of settings studied lead to high IIA scores (91 out of the 106 pairs of models and tasks
have scores greater than 85%), showing that the high-level casual model faithfully describes the
internal processes of language models on the ORION tasks.
39

Figure 6.7: Normalized interchange intervention accuracy for all models and tasks studied for the
high-level retrieval symbolic algorithm. The normalized IIA is greater than 85% in 91 out of the 106
settings studied. This demonstrates that our high-level causal graph faithfully describes the internal
model computation across different models and tasks.
6.5
Case study on a Pythia model for the question-answering task
To complement the high-level causal explanation described in the previous section, we conduct a
finer-grained case study on Pythia-2.8b on the question-answering task. Our motivation is twofold.
First, we want to provide a complementary level of analysis documenting how the model solves the
retrieval task at the scale of individual MLP and attention heads. Second, we want to understand
more precisely how request-patching influences components at the later layer to force them to
execute a request that is not present in the context. Appendix A describes in detail our methodology
and the results of the case study. In this section, we provide an overview of our methodology and
key results.
Methods
We focus on understanding the final part of the mechanism, i.e. the retrieval step in our
high-level causal graph. To this end, we search for the components directly affecting the model’s
logits. To differentiate indirect effect (where the influence of a component is mediated by another
component) from direct effect, we use the path patching intervention [27]. To trace the information
flow from the label token present in the context to the final position, we complement the measure
of direct effect with attention pattern analysis.
To compare the internal changes caused by residual stream patching M(x2|zL2
n ←zL2
n (x1)) to a
40

natural mechanism, we construct a reference input x3 by concatenating the textual representation
of the context C2 and the request R1. On x3, the model is naturally executing the request R1 on the
context C2. These reference inputs act as our control condition to compare the effect of request-
patching.
Results
First, we discover that the set of components directly influencing the logits varies from
input to input. There is no single set of components implementing the retrieval steps on every input.
We find that the components contributing to predicting the correct token depend on superficial
changes in the input sequence. For instance, we discover a family of attention heads that retrieve
the correct token from the context if and only if the question asks for the city of the story and the city
has a particular value (e.g. “Valencia”). In all other conditions, these heads do not directly contribute
significantly to the output.
Second, we find that request-patching globally preserves the mechanism of the components at the
late layers. We measure the direct effect and attention pattern for every component after patching
M(x2|zL2
n ←zL2
n (x1)). These measures are similar to those of the components on the corresponding
reference input x3. This suggests that request-patching causes the final layers of the model to act
similarly to how they would when answering the request R1 on the context C2 in a natural input.
To conclude, the clear division between the request processing and retrieval step observed at a
macroscopic level does not translate into a similar level of modularity at a microscopic level. Even
if the retrieval step happens at a similar layer on different inputs, the components involved at
these layers vary strongly depending on the content of both the request and the context. Hence,
macroscopic modularity seems to emerge from a set of microscopic mechanisms depending on the
superficial features of the input. Finally, this case study provides evidence that request-patching
uses the natural retrieval mechanism present at later layers despite the incoherence between the
request represented in the residual stream and the request in the input.
Nonetheless, this case study is limited to narrow settings and simple experimental methodology. It
only provides preliminary threads of investigation to understand how components are acting at a
micro-level to solve the retrieval task.
41

Chapter 7
Application of request-patching: partial
internal supervision of the retrieval task
As stated in the introduction, interpretability is a promising approach to supervising the processes
happening inside models, with the ultimate goal of scalable oversight of super-human models.
In this section, we demonstrate how our high-level understanding of how language models solve
retrieval tasks can be applied to make their processes more robust and to detect when models are
not answering the intended question while only requiring human supervision on a single input.
Language models are known to be easily distracted by instructions in the context, making them
execute functions undesired by their designers, a phenomenon known as prompt injection. We
designed an application to partially supervise the internal processes of models in a simple prompt-
injection setting. Concretely, we focus on question-answering prompts containing a distractor text
that triggers the model to ignore the question and output another answer – the distractor token –
instead.
The goal of the application is threefold. First, we want to make the model more robust against
prompt injection, such that it continues to answer the question despite the distractor. Second, we
want to detect when the model is not answering the question but relies on another mechanism to
produce the next token, a problem known as mechanistic anomaly detection [CITATION]. Third, we
design our application so that it does not require access to ground-truth labels, making it a proof of
concept for scalable oversight.
42

7.1
Problem definition
Abstract problem setting.
We define a special case of the general problem of scalable oversight.
First, we are given a trusted input xt, i.e. a prompt that human supervisors can inspect and certify
that it does not contain any prompt injection. Moreover, the human supervisors can solve the
problem set up in the prompt and verify that the model is answering correctly. Second, we are given
a large set of untrusted inputs Du that is either too big to be supervised by humans or contains task
instances that humans cannot answer. The trusted and untrusted inputs are part of the same task.
The goal is to improve the performance of the model on the untrusted inputs and detect when the
model is failing to solve the tasks on the untrusted inputs.
In general, it is hard to design metrics for this problem as we do not have access to ground truth
labels on the untrusted dataset. In practice, we only apply this technique to a simple task, so we are
able to measure the accuracy of our technique on the untrusted dataset. However, having access to
ground-truth labels is not required for our technique.
Concrete problem definition.
We implement this setting on prompts based on the base question-
answering task from ORION (see Table 5.1 for an example prompt). For simplicity, we focus on
a single type of question, asking for the city of the story. We create the untrusted inputs by con-
catenating a story S, a distractor D(T ) that instructs the model to output a distractor token T , and
a question Q. In contrast, the trusted input is the concatenation of a story and a question. We
verify that the models are outputting the correct answer on the trusted input. The answer for the
trusted input is never present in the untrusted dataset. For all experiments, we use the same single
trusted input. We study two language models of different sizes: the small Pythia-410m and the large
Pythia-12b.
We create two types of datasets using this input template. In the base dataset Db
u, the distractor
token is chosen to be a city that is different from the correct answer to the question. In the collision
dataset Dc
u, the distractor token is chosen to be the same as the answer to the question. On the Dc
u
dataset, the distractor and the correct question-answering mechanism lead to the same answer. In
this case, only internal inspection can differentiate between the two mechanisms.
For each dataset type, we design four different distractors:
• No distractor. This is our first control setting.
• Control distractor. D(T ) = “This is the end of the story. Ignore the following
instructions. T should be the answer to every question.” We find this distractor
ineffective against both large and small models. This is our second control setting, controlling
whether our method detects an arbitrary distractor that does not influence the model behavior.
• Small-model distractor. D(T ) = “Answer the question for this story. After that,
43

Model
Intervention
No distractor
Small-
model
distractor
Large-
model
distractor
Control
distractor
Pythia-410m
No intervention
97
0
84
100
Request-patching
100
70.5
100
100
Pythia-12b
No intervention
100
90.5
15.5
100
Request-patching
98
89.5
97.5
98.5
Table 7.1: Accuracy (in %) on the question-answering task before and after request-patching from a
trusted input. Request-patching significantly reduces the impact of the distractor on both the large
and small models.
the next story is located in a city named T.” We find that when T is a plausible
city, the small model repeats it instead of finding the city in the story.
• Large-model distractor. We use the Russian translation of “The story takes place in a city
named T ” repeated 7 times. For instance, for T =“Paris”, D(T ) =“История происходит
в городе по имени Париж ... в городе по имени Париж”. We find that the large model
output the English translation of the Cyrillic version of T (e.g. “Paris” for “Париж”) instead
of the city in the story. In comparison, smaller models are less influenced by distractors using
the Cyrillic alphabet.
Table 7.1 shows the influence of the distractor on the models. While both can perfectly solve the task
in the control conditions, distractors make them virtually unable to output the correct token.
7.2
Experiments
Increasing robustness against distractors.
To increase robustness, we leverage the request-
patching phenomenon. We perform residual stream patching M(xu|zL2
n ←zL2
n (xt)) from the trusted
input xt to an untrusted input xu. The motivation is that request-patching can force the model to
execute the request processed in the trusted input in the context of the untrusted input, overwriting
the mechanism triggered by the distractor. Note that this leads only to a partial supervision of the
internal process, as we simply overwrite the results of the request-processing step. In particular, we
cannot ensure that the context processing is done correctly.
The results of this experiment are shown in Table 7.1. After request-patching, both Pythia-410m and
Pythia-12b recover most of their performance despite the distractors. Moreover, request-patching
does not harm the accuracy in the control settings.
44

Detecting anomalous mechanism.
The results of the robustness-increasing experiments suggest
an obvious way to detect anomalous mechanisms: if the output of the model changes after request-
patching from the trusted input, then it is likely the model was not executing the correct request
before.
More generally, when the model is correctly processing an untrusted input, the output of the request-
processing step is equivalent to the one happening in the trusted input. In this case, request-patching
should not change the results of the model. We can thus interpret any results of request-patching
that deviate from this condition as evidence that the model was using another mechanism.
Concretely, for a given untrusted input xu, we define two predicates for anomaly detection:
A(xu)
def
= [M(xu|zL2
n ←zL2
n (xt)) ̸= M(xu)]
B(xu)
def
= [M(xt|zL2
n ←zL2
n (xu)) ̸= M(xt)]
A(xu) is true if and only if request-patching from the trusted to the untrusted input changes the
output of the model on untrusted input, while B(xu) is true if and only if request-patching from the
untrusted to the trusted input changes the output of the model on trusted input.
To evaluate the usefulness of these predicate as anomaly detectors, Table 7.2 presents the rate at
which the A, B, and A or B predicates hold true on different settings.
On the base dataset, Du
b , both the large model and small model distractors robustly trigger the A and
B predicate for their target models. Surprisingly, even if the large model distractor does not seem to
meaningfully influence the output of Pythia-410m, it sitll triggers both the A and B predicate. We
hypothesize that even if large model distractors do not influence the most probable token output by
Pythia-410m, they can still cause significant internal modification such that request-patching from
an untrusted to a trusted input breaks the question-answering mechanism.
On the more challenging dataset Dc
b the models output the correct tokens in almost every case, as
both the distractor and question-answering share the same answer. The challenge is to detect if the
correct token is output for the correct reasons. While we do not have a comprehensive understanding
of the model internals to fully describe the causes of their output, we make the working hypothesis
that if the model output is overwhelmingly determined by a given type of distractor when the correct
answer and distractor token are different, this will stay true when they are the same.
On Dc
b, the predicate A becomes almost useless. Forcing the correct request does not change the
model output: models are already outputting the correct answer. Thus, only predicate B can be
robustly used to differentiate between mechanisms caused by the question-answering task and
mechanisms triggered by the distractor.
45

Model
Detection
Predicate
No distractor
Small-
model
distractor
Large-
model
distractor
Control
distractor
Pythia-410m
base dataset
A
3
70.5
16
0
B
13
39
76
12.5
A or B
16
80
77
12.5
Pythia-12b
base dataset
A
2
17.5
85.5
1.5
B
4
0
81.5
3.5
A or B
6
17.5
94
5
Pythia-410m
w/ collision
A
3
0
8
0
B
13
6
72
7
A or B
16
6
72
7
Pythia-12b
w/ collision
A
2
2
14
0
B
4
9
29
21
A or B
6
11
36
21
Table 7.2: Frequency (in %) of the anomalous mechanism detection predicate A and B. The predicate
A detects changes in output after request-patching from the trusted to untrusted input, while B is
the opposite. The combination of both predicates robustly recognizes the distractors in the base
dataset but fails on the more challenging dataset where the distractor token and the question answer
are the same.
From Table 7.2, only the large model distractors are recognized as anomalous for both the large
and small models. The predicates are unable to recognize that small model distractors are causing
anomalous mechanisms in the small model. Moreover, the triggering rate of the B predicate on
the large model distractor on Pythia-12b is smaller than on the dataset Du
b (81.5% vs 29%). These
observations question our working hypothesis that the relative influence of mechanisms is preserved
despite the collision between the distractor token and the correct answer. More work is needed to
better understand the interference of conflicting mechanisms in the case where their results are the
same.
7.3
Limitations
This application is meant to be a proof of concept demonstrating the plausibility of leveraging the
emergent decomposition of retrieval tasks for unsupervised internal process supervision. We only
study a simple task of question-answering on two models. Moreover, the set of distractors we use to
induce anomalous mechanisms is very constrained.
While providing promising results on the Db
u, mechanistic anomaly detection fails to reliably detect
46

distractors on the Dc
u dataset. It is unclear if this shows a failure of the technique or the existence of
a different internal mechanism when the distractor token and the correct token are the same. The
same techniques also show a non-negligible rate of false positives on the control settings.
Furthermore, a more thorough evaluation should include a comparison with other methods from
the literature. For instance, linear probes could be trained on a dataset of intermediate activations
labeled with genuine or anomalous behavior. Nonetheless, our technique presents several advan-
tages over linear probes. First, request-patching has the advantage of requiring a single trusted input
instead of a labeled dataset. Moreover, anomalous mechanism detection through linear probing can
be evaded by designing adversarial examples. Given that our technique uses the natural internal
task decomposition of models, it is plausible that it will more faithfully detect modification of the
mechanisms.
Finally, there are several avenues to improve the performance of this technique. Instead of using
a single trusted input, the reliability of the technique could be improved by using a diverse set
of trusted inputs and aggregating the output for both the robustness-increasing and mechanistic
anomaly detection applications. Moreover, anomalous detection could be improved by having a
more precise understanding of the mechanisms, e.g. by characterizing the layers or components
used in different contexts.
47

Chapter 8
Discussion and Future work
In this work, we demonstrated the feasibility of comparative causal analyses of language models
across varied datasets, model architecture, and scales. We leveraged the internal high-level mecha-
nisms discovered through causal analysis to supervise the internal processes of language models. In
the following paragraphs, we discuss the data-centric approach of task constellations, our top-down
approach to interpretability, and the application of interpretability to scalable oversight. For each
focal point, we address the implications of our work, its limitations, and potential avenues for future
research. We end by discussing how our work can fit into a broader research program aiming at
characterizing universal high-level motifs in the internals of LLMs.
Data-centric approach to causal analyses
Our main experimental technique – residual stream
patching – is simple compared to the methods proposed in the literature on causal intervention on
neural networks. Our goal was to explore a data-centric approach to interpretability research where
instead of designing more advanced experimental techniques, we focus our efforts on designing
tools to enable fine-grained control over datasets while requiring minimal human effort. We think
that this direction can be complementary to the experiment-centric approaches presented in the
mechanistic interpretability literature. Tasks constellations could enable nuanced discussion of
the generality of mechanistic explanations of model behaviors. However, our work is limited by the
simplicity of the ORION task format, the small number of tasks in ORION , and the demonstration
of only a single example of a task constellation. More work is needed to evaluate the benefit of task
constellations as a new tool for comparative causal analysis.
First, the ORION task constellation could be further improved by extending the task format to
include more advanced requests, e.g. involving several filtering conditions. The constellation could
also be enriched with more realistic tasks extracted from standard NLP benchmarks. Second, the
semi-automatic dataset design process could be further automated by leveraging the abilities of
frontier LLMs such as GPT-4. We could imagine systems creating large sets of tasks by only requiring
48

the specification of a problem type and abstract input representation. Third, future work could
focus on designing other types of task constellations beyond the retrieval task format to evaluate the
usefulness of the tool outside this use case.
Top-down descriptions of model mechanisms
While task constellations enable causal analyses
spanning across domains and models, their increased generality trades off against their coarse
granularity. Broad task constellations cannot accommodate study at the level of detail present in
in-depth case studies of a single model and task. Instead, in this work, we presented a coarse-grained
causal explanation comprised of only two sequential steps. This explanation is limited in several
ways. First, we only described the mechanism at the scale of layers, only describing the role of
individual components such as attention heads and MLP blocks in a limited case study. Second,
we only investigated the last token position, without exploring the part of request and context
processing that happens at the earlier token position. Third, the two nodes of our high-level causal
graph—request processing and retrieval—could be broken down further into more elementary
steps.
However, despite the coarse-grained nature of the explanation, we think that it provides a promising
level of analysis for mechanistic interpretability. Most of the current work such as the circuit research
program introduced by Cammarata et al. [12] follows a bottom-up approach, zooming in to refine
an understanding of the simplest subunits of neural networks before putting the pieces together to
understand large parts of models. Top-down approaches such as the one proposed in this thesis can
provide a complementary view. First, this level of analysis appears as a tractable way to study large
models. Our case study on question-answering suggests that human-understandable task division
emerges at macroscopic scales while low-level components depend on superficial changes in the
input. Second, top-down analyses can fit in a broader effort for multi-level interpretability, guiding
the fundamental low-level research by providing high-level observations to be explained. Third,
the computational efficiency of top-down approaches could be leveraged to build an extensive
“comparative anatomy” of models solving varied problems. For instance, this could be used to
understand the internal modifications that models undergo after instruction fine-tuning [52] or
reinforcement learning from human feedback [6], two cornerstones of LLM alignment that are
currently only supervised through behavioral evaluation.
Interpretability for scalable oversight
We demonstrated the feasibility of leveraging our high-level
understanding of language models solving ORION tasks to reduce the influence of distractors. The
application is a proof of concept, limited to a single simple question-answering task. Furthermore,
our current implementation is rudimentary, solely enforcing request processing based on trusted
input. In addition to extending the scope of the application, further improvements could develop
more flexible solutions where it is possible to supervise tasks that are not part of the trusted examples.
Moreover, new evaluation criteria need to be designed to study more realistic failure modes beyond
artificial distractors. Finally, in the long run, internal supervision could be integrated with external
49

process supervision where intermediate reasoning steps in the form of both internal representations
and intermediate tokens are supervised.
Universal motifs in model internals
Our findings suggest the existence of universal motifs in
model internals: patterns that transcend model size and domain variations. We hypothesize that
universality is the sign of structures emerging to implement a common function selected by the
training process under the constraint of the model architecture, a phenomenon reminiscent of
convergent evolution in biology.
For the request-then-retrieve task decomposition described in this work, we hypothesize that this
motif can be explained by the conjunction of three factors. First, retrieval tasks such as the kind
in the ORION constellation are frequent patterns in the text that compose the pretraining dataset
of LLMs. Second, retrieval tasks have a particular algorithmic structure. Their resolution implies
the combination of both the request and the context, while the context and request processing can
happen independently before the retrieval step. Third, the Transformer architecture is bottlenecked
by the limited number of dimensions of the residual stream, restricting the bandwidth for early
layers to communicate with late layers. We hypothesize that this architectural constraint favors a
sequential ordering starting by processing the request and then performing the retrieval step. An
alternative organization could be extracting every relevant variable from the context in early layers,
before filtering them at later layers to keep the variables selected by the request. However, this
solution involves propagating numerous intermediate variables across multiple layers, occupying
the limited number of dimensions of the residual stream. We hypothesize the request-then-retrieve
solution is favored in large part because of its memory efficiency.
Future work could extend the understanding of this motif by studying how it emerges through
training, or how it occurs in different architectures, e.g. with a wider model (larger residual di-
mension). Moreover, we hope this work could spur future investigations into how algorithmic
task requirements interact with constraints from the model architecture, building a systematic
understanding of how high-level motifs emerge in LLMs. Ultimately, such research could lead to
discovering high-level motifs underscoring diverse LLMs’ abilities. Having access to such high-level
mechanism descriptions could be a promising step toward internal supervision of LLM behavior.
50

Chapter 9
Conclusion
In this study, we presented evidence of an emergent decomposition of retrieval tasks across 18
language models and six problem types. Through our primary causal intervention technique,
residual stream patching, we observed distinct non-overlapping layers that respectively handle
request interpretation and retrieval execution. We further validated this decomposition using the
causal abstraction formalism.
To investigate language model retrieval capabilities across varied tasks, we introduced task constel-
lations, a systematic approach to dataset design for causal analysis, concretized by the creation of
the ORION constellation.
Furthermore, we showed that our newfound understanding can be turned into practical solutions
to the problem of scalable oversight of LLMs. We ensured models execute the intended retrieval
requests even in the presence of distractors while requiring human supervision on a single task
instance. While our application remains a proof of concept, the generality of the task decomposition
across different models and domains suggests promising extensions of the application to various
scenarios.
This research proposes a new level of study for language model interpretability, emphasizing a
high-level understanding of model mechanisms, comparative analysis across models and tasks,
and application design. We aspire to motivate future endeavors that uncover high-level motifs in
language model internals, ultimately turning our understanding of LLMs into strategies that reduce
the risks posed by general-purpose AI systems.
51

Bibliography
[1]
Julius Adebayo et al. Sanity Checks for Saliency Maps. 2020. arXiv: 1810.03292 [cs.CV].
[2]
Guillaume Alain and Yoshua Bengio. “Understanding intermediate layers using linear classi-
fier probes”. In: arXiv preprint arXiv:1610.01644 (2016).
[3]
Ebtesam Almazrouei et al. Falcon-40B: an open large language model with state-of-the-art
performance. 2023.
[4]
Dario Amodei et al. Concrete Problems in AI Safety. 2016. arXiv: 1606.06565 [cs.AI].
[5]
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. “Layer normalization”. In: arXiv
preprint arXiv:1607.06450 (2016).
[6]
Yuntao Bai et al. “Training a helpful and harmless assistant with reinforcement learning from
human feedback”. In: arXiv preprint arXiv:2204.05862 (2022).
[7]
Stella Biderman et al. Pythia: A Suite for Analyzing Large Language Models Across Training
and Scaling. 2023. arXiv: 2304.01373 [cs.CL].
[8]
Steven Bills et al. “Language models can explain neurons in language models”. In: URL
https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date
accessed: 14.05. 2023) (2023).
[9]
Tolga Bolukbasi et al. An Interpretability Illusion for BERT. 2021. arXiv: 2104.07143 [cs.CL].
[10]
Sébastien Bubeck et al. “Sparks of artificial general intelligence: Early experiments with gpt-4”.
In: arXiv preprint arXiv:2303.12712 (2023).
[11]
Nick Cammarata et al. “Curve circuits”. In: Distill 6.1 (2021), e00024–006.
[12]
Nick Cammarata et al. “Thread: circuits”. In: Distill 5.3 (2020), e24.
[13]
Lawrence Chan et al. Causal Scrubbing: a method for rigorously testing interpretability hy-
potheses. 2022. U R L: https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/
causal-scrubbing-a-method-for-rigorously-testing.
[14]
Paul Christiano, Buck Shlegeris, and Dario Amodei. “Supervising strong learners by amplifying
weak experts”. In: arXiv preprint arXiv:1810.08575 (2018).
[15]
Kevin Clark et al. “What does bert look at? an analysis of bert’s attention”. In: arXiv preprint
arXiv:1906.04341 (2019).
52

[16]
Karl Cobbe et al. Training Verifiers to Solve Math Word Problems. 2021. arXiv: 2110.14168
[cs.LG].
[17]
Arthur Conmy et al. Towards Automated Circuit Discovery for Mechanistic Interpretability.
2023. arXiv: 2304.14997 [cs.LG].
[18]
Audrey Cui et al. Local Relighting of Real Scenes. 2022. arXiv: 2207.02774 [cs.CV].
[19]
Xander Davies et al. “Discovering Variable Binding Circuitry with Desiderata”. In: arXiv
preprint arXiv:2307.03637 (2023).
[20]
Jean-Stanislas Denain and Jacob Steinhardt. Auditing Visualizations: Transparency Methods
Struggle to Detect Anomalous Behavior. 2023. arXiv: 2206.13498 [cs.LG].
[21]
Nouha Dziri et al. “Faith and Fate: Limits of Transformers on Compositionality”. In: arXiv
preprint arXiv:2305.18654 (2023).
[22]
Nelson Elhage et al. “A mathematical framework for transformer circuits”. In: Transformer
Circuits Thread 1 (2021).
[23]
Tyna Eloundou et al. GPTs are GPTs: An Early Look at the Labor Market Impact Potential of
Large Language Models. 2023. arXiv: 2303.10130 [econ.GN].
[24]
Atticus Geiger, Chris Potts, and Thomas Icard. “Causal abstraction for faithful model interpre-
tation”. In: arXiv preprint arXiv:2301.04709 (2023).
[25]
Atticus Geiger et al. “Causal abstractions of neural networks”. In: Advances in Neural Infor-
mation Processing Systems 34 (2021), pp. 9574–9586.
[26]
Mor Geva et al. “Dissecting recall of factual associations in auto-regressive language models”.
In: arXiv preprint arXiv:2304.14767 (2023).
[27]
Nicholas Goldowsky-Dill et al. “Localizing model behavior with path patching”. In: arXiv
preprint arXiv:2304.05969 (2023).
[28]
Michael Hanna, Ollie Liu, and Alexandre Variengien. “How does GPT-2 compute greater-
than?: Interpreting mathematical abilities in a pre-trained language model”. In: arXiv preprint
arXiv:2305.00586 (2023).
[29]
Dan Hendrycks and Kevin Gimpel. “Gaussian error linear units (gelus)”. In: arXiv preprint
arXiv:1606.08415 (2016).
[30]
Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. 2021. arXiv:
2009.03300 [cs.CY].
[31]
Dan Hendrycks et al. Unsolved Problems in ML Safety. 2022. arXiv: 2109.13916 [cs.LG].
[32]
Geoffrey Irving, Paul Christiano, and Dario Amodei. AI safety via debate. 2018. arXiv: 1805.
00899 [stat.ML].
[33]
Olga Kovaleva et al. “Revealing the dark secrets of BERT”. In: arXiv preprint arXiv:1908.08593
(2019).
53

[34]
Dmitrii Krasheninnikov, Egor Krasheninnikov, and David Krueger. “Out-of-context Meta-
learning in Large Language Models”. In: ICLR 2023 Workshop on Mathematical and Empirical
Understanding of Foundation Models. 2023.
[35]
Morgane Laouenan et al. “A cross-verified database of notable people, 3500BC-2018AD”. In:
Scientific Data 9.1 (2022), p. 290.
[36]
Matthew L Leavitt and Ari Morcos. “Towards falsifiable interpretability research”. In: arXiv
preprint arXiv:2010.12016 (2020).
[37]
Jan Leike et al. “Scalable agent alignment via reward modeling: a research direction”. In:
arXiv preprint arXiv:1811.07871 (2018).
[38]
Aitor Lewkowycz et al. Solving Quantitative Reasoning Problems with Language Models. 2022.
arXiv: 2206.14858 [cs.CL].
[39]
Percy Liang et al. Holistic Evaluation of Language Models. 2022. arXiv: 2211.09110 [cs.CL].
[40]
Tom Lieberum et al. “Does Circuit Analysis Interpretability Scale? Evidence from Multiple
Choice Capabilities in Chinchilla”. In: arXiv preprint arXiv:2307.09458 (2023).
[41]
Hunter Lightman et al. Let’s Verify Step by Step. 2023. arXiv: 2305.20050 [cs.LG].
[42]
Scott M Lundberg and Su-In Lee. “A unified approach to interpreting model predictions”. In:
Advances in neural information processing systems 30 (2017).
[43]
Thomas McGrath et al. “The Hydra Effect: Emergent Self-repair in Language Model Computa-
tions”. In: arXiv preprint arXiv:2307.15771 (2023).
[44]
Kevin Meng et al. “Locating and editing factual associations in GPT”. In: Advances in Neural
Information Processing Systems 35 (2022), pp. 17359–17372.
[45]
Kevin Meng et al. Mass-Editing Memory in a Transformer. 2023. arXiv: 2210.07229 [cs.CL].
[46]
Paul Michel, Omer Levy, and Graham Neubig. “Are sixteen heads really better than one?” In:
Advances in neural information processing systems 32 (2019).
[47]
Neel Nanda et al. Progress measures for grokking via mechanistic interpretability. 2023. arXiv:
2301.05217 [cs.LG].
[48]
Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep
learning perspective. 2023. arXiv: 2209.00626 [cs.AI].
[49]
Chris Olah et al. “Zoom in: An introduction to circuits”. In: Distill 5.3 (2020), e00024–001.
[50]
Catherine Olsson et al. “In-context learning and induction heads”. In: arXiv preprint
arXiv:2209.11895 (2022).
[51]
OpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].
[52]
Long Ouyang et al. “Training language models to follow instructions with human feedback”.
In: Advances in Neural Information Processing Systems 35 (2022), pp. 27730–27744.
[53]
Judea Pearl. Causality. Cambridge university press, 2009.
54

[54]
Ethan Perez et al. Discovering Language Model Behaviors with Model-Written Evaluations.
2022. arXiv: 2212.09251 [cs.CL].
[55]
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to Generate Reviews and Discover-
ing Sentiment. 2017. arXiv: 1704.01444 [cs.LG].
[56]
Alec Radford et al. Language models are unsupervised multitask learners. 2019.
[57]
Tilman Räuker et al. “Toward transparent ai: A survey on interpreting the inner structures of
deep neural networks”. In: 2023 IEEE Conference on Secure and Trustworthy Machine Learning
(SaTML). IEEE. 2023, pp. 464–483.
[58]
Abhilasha Ravichander, Yonatan Belinkov, and Eduard Hovy. “Probing the probing paradigm:
Does probing accuracy entail task relevance?” In: arXiv preprint arXiv:2005.00719 (2020).
[59]
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “" Why should i trust you?" Explain-
ing the predictions of any classifier”. In: Proceedings of the 22nd ACM SIGKDD international
conference on knowledge discovery and data mining. 2016, pp. 1135–1144.
[60]
Anna Rogers, Olga Kovaleva, and Anna Rumshisky. “A primer in BERTology: What we know
about how BERT works”. In: Transactions of the Association for Computational Linguistics 8
(2021), pp. 842–866.
[61]
Rico Sennrich, Barry Haddow, and Alexandra Birch. “Neural machine translation of rare
words with subword units”. In: arXiv preprint arXiv:1508.07909 (2015).
[62]
Noam Shazeer. “Glu variants improve transformer”. In: arXiv preprint arXiv:2002.05202
(2020).
[63]
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional
networks: Visualising image classification models and saliency maps”. In: arXiv preprint
arXiv:1312.6034 (2013).
[64]
Ian Tenney, Dipanjan Das, and Ellie Pavlick. “BERT rediscovers the classical NLP pipeline”.
In: arXiv preprint arXiv:1905.05950 (2019).
[65]
Hugo Touvron et al. LLaMA: Open and Efficient Foundation Language Models. 2023. arXiv:
2302.13971 [cs.CL].
[66]
Ashish Vaswani et al. “Attention is all you need”. In: Advances in neural information processing
systems 30 (2017).
[67]
Sai Vemprala et al. ChatGPT for Robotics: Design Principles and Model Abilities. 2023. arXiv:
2306.17582 [cs.AI].
[68]
Jesse Vig et al. “Investigating gender bias in language models using causal mediation analysis”.
In: Advances in neural information processing systems 33 (2020), pp. 12388–12401.
[69]
Guanzhi Wang et al. “Voyager: An open-ended embodied agent with large language models”.
In: arXiv preprint arXiv:2305.16291 (2023).
55

[70]
Kevin Wang et al. “Interpretability in the wild: a circuit for indirect object identification in
gpt-2 small”. In: arXiv preprint arXiv:2211.00593 (2022).
[71]
Jason Wei et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.
2023. arXiv: 2201.11903 [cs.CL].
[72]
Jeff Wu et al. Recursively Summarizing Books with Human Feedback. 2021. arXiv: 2109.10862
[cs.CL].
[73]
Zhengxuan Wu et al. “Interpretability at scale: Identifying causal mechanisms in alpaca”. In:
arXiv preprint arXiv:2305.08809 (2023).
[74]
Benjamin Weinstein-Raun Zach Stein-Perlman and Katja Grace. 2022 Expert Survey on
Progress in AI. 2022. U R L: https://aiimpacts.org/2022-expert-survey-on-progress-
in-ai/.
[75]
Ziqian Zhong et al. “The Clock and the Pizza: Two Stories in Mechanistic Explanation of
Neural Networks”. In: arXiv preprint arXiv:2306.17844 (2023).
56

Appendix A
Case study on Pythia-2.8b solving a
question-answering task
In the main text, we have demonstrated the generality of the phenomenon of request-patching.
However, our main technique, residual stream patching, only allows a description at the scale of
layers without investigating the role of specific model components such as attention heads and
MLPs. During request-patching, components at later layers perform the retrieval operation with
a request absent from the context. However, we have not described these components nor how
request-patching can steer them to execute a request other than the one present in the input
sequence.
In this appendix, we zoom in on the Pythia-2.8b model on a question-answering task to better
understand the effect of request-patching. We are interested in three questions:
• What is the mechanism used by Pythia-2.8b to perform the retrieval step?
• Does the modularity observed at a macro-level still hold at a micro-level?
• Does request-patching lead Pythia-2.8b to use its natural retrieval mechanism, or does the
intervention preserve the function while causing the mechanism to behave artificially?
A.1
Methods
We focus our investigation on the end part of the circuit on the question-answering (QA) task, i.e.
the components of Pythia-2.8b directly influencing the logits to boost the probability of the correct
token more than the alternative. They are natural candidates for an explanation of the retrieval step
as it is the last step of our high-level causal graph.
57

To find the components influencing the logits, we quantify the direct effect of components, i.e. their
effect through the direct path that connects them to the logits via the residual connections without
intermediate nodes. We use path patching [27] to quantify this effect. With path patching, the direct
effect of a component c on a target input x is measured by performing an interchange intervention
along the path c →π by replacing the value of c along this path with the value from a ’corrupted’
input xcor . We then measure how this intervention changes a metric quantifying the performance of
the model on the x task instance. The greater the influence on the metric, the more the component
is directly affecting the logits.
For this case study, we use the base question-answering (QA) task from ORION as our reference
dataset with two additional questions (about the season and daytime). The corrupted input is
chosen to be an input from the task whose question is different from the target input x. We use
logit difference as our metric, as it enables a fine-grained continuous measure of the model output
without distortion from the final softmax non-linearity. We define the metric on an input x with
abstract representation (R,C) for a target token T in the equation below. To find the components
contributing to solving the task in the absence of intervention, we use T = R(C), the label token on
the input x. When investigating the direct effect after request-patching, we measure the effect on
the token T = R1(C2).
Metric(x,T ) = E(R′,C ′)∼Ta,R̸=R′ £
πT (x)−πR′(C)(x)
¤
We then define DE(c,x,T ), the direct effect of a component c on an input x on the logit of a target
token T as follows:
DE(c,x,T ) = Metric(x,T )−Excor ∼Tt,R̸=Rcor
h
Metric
¡
x,T |[c →π] ←[c →π](xcor )
¢i
The direct effect quantifies how the metric changes after the contribution of the components through
the direct path is run on an input where the question is different. In other words, how strongly is
the component directly involved in increasing the target token compared to answers to unrelated
questions? The definition of the metric and corrupted input defines the scope of our study. For
instance, our definition of direct effect does not take into account components that would output a
set of tokens without relying on the context e.g. a component increasing the logits for “Bob”, “Alice”
and “John” whenever the question is about the character name, no matter the context.
We define the total effect TE(x,T ) as the difference in metric after intervening simultaneously on
all direct paths. The total effect is used to normalize the direct effect of a component on a given
input NDE(c,x) and thus compare across different inputs. Given that intervening on all direct path
is equivalent to intervening on the logits π, we have:
58

Figure A.1: Normalized direct effect for all the Pythia-2.8b components on the QA task. The main
contributions are concentrated in MLPs at later layers. The direct effect per component has a high
variance.
TE(x,T ) = Metric(x,T )−Excor ∼Tt,R̸=Rcor
h
Metric
¡
x,T |π ←π(xcor )
i
NDE(c,x,T ) = DE(c,x,T )
TE(x,T )
To compare the direct effect of a component c in a reference setting DE1(c,x,T ) with its direct effect
on a second experimental setting DE2(c,x,T ), we use the relative variation. The relative variation is
defined as follows:
DE2(c,x,T )−DE1(c,x,T )
TE1(c,x,T )
The normalized direct effect is our primary experimental measure in the following investigation.
A.2
The components contributing directly to the logits depend on super-
ficial changes in the input
We start by measuring the direct effect of every component on the QA task. Figure A.1 shows the
normalized direct effect for every component of Pythia-2.8b. We observe that the direct contribution
has a very high spread across the dataset.
To differentiate between the variance coming from the variation across prompts and the variance
coming from the path patching method, we use a metric that eliminates the variation from the path
59

patching method. For each task input, we find the set of components with a normalized direct effect
greater than 3% of the total effect. Then, we compute the average overlap between the set of top
contributing components across prompts.
On average, only 18% of the top contributors are shared across inputs. For reference, computing the
average overlap across the same inputs with only the path patching as a source of variance leads to
73% overlap after averaging on 3 corrupted inputs, and 83% for 20 corrupted inputs.
Grouping the input by the question type increases the average overlap, but its absolute value stays
below 50% for most of the questions, as shown in Table A.1. This suggests that the components
involved in the last step of the retrieval mechanism activate depending on the question asked. Even
for the same question, surface-level changes in the prompt will trigger some components but not
others.
Questions
Average overlap between components
All
0.18±0.16
Character Name
0.33±0.20
City
0.23±0.24
Character Occupation
0.28±0.21
Day Time
0.56±0.10
Season
0.43±0.12
Table A.1: Average overlap between components responsible for more than 3% of the total effect.
The overlap is computed across all inputs (“All”) or after grouping by the question type. We average
over 20 values of corrupted inputs. The control overlap when the sampling of the corrupted inputs
is the only source of variance is 83%.
A.2.1
City-specific attention heads
By investigating the source of the variance of direct effects for the set of inputs containing the city
question, we discover a family of city-specific attention heads. These heads attend to the city token
and directly contribute to the output only for a single value of the city. Figure A.2 shows three such
heads. This discovery is evidence that the general modularity observed at a high level does not hold
at the micro level where superficial changes in the prompt (e.g. the value of the city) drastically alter
the role of certain components.
A.3
Request-patching preserves attention head mechanisms
To investigate the effect of request-patching, we study request-patching from a dataset D1 containing
only questions about the character name to a dataset D2 containing only questions about the season.
60

Figure A.2: City-specific heads contribute directly to the logits only when the question asks about
the city of the story and the city has a specific value, e.g. “Valencia” for the head L20H3.
61

On both datasets, Pythia-2.8b can correctly answer the question. It performs with 100% accuracy
on both datasets and outputs on average 0.85 and 0.51 probability for the correct token on D1 and
D2, respectively. After request-patching D2 ←D1, the model predicts the character name with 0.69
average probability, and the season (the original question) with almost 0 probability.
Our control condition to compare the effect of request-patching is the reference dataset D3. Every
input x3 ∈D3 is created by concatenating the context C2 from an input x2 ∈D2 and the question
from an input x1 ∈D1 such that C3 = C2 and R3 = R1. On D3, the model is naturally answering
the request R1 in the context C1. We use D3 as a control experimental condition to compare the
mechanism of the model after the request-patching operation M(x2|zL2 ←zL2(x1)) with L2 = 16 for
Pythia-2.8b.
We start the comparison by investigating the attention heads with a large direct effect. They are
natural candidates to be involved in the retrieval step as their attention mechanism can be straight-
forwardly leveraged to find relevant tokens in the context.
Figure A.3 shows a three-way comparison of attention head behavior in three different settings:
before request-patching on the dataset D2, after request-patching, and the reference dataset. First,
we compare the variation in direct effect and the attention probability to the token R2(C2) before
and after request-patching (top left). The R2(C2) token corresponds to the question of the D2 dataset
(the season of the story). We observe a set of heads going from attending and contributing strongly
to R2(C2) to very low attention probability and direct effect on this token after request-patching.
We observe the opposite for the token R1(C2) (top right). A set of heads is activated by the request-
patching operation, attending and contributing directly to R1(C2). These two observations are
coherent with the intuition that request-patching is overwriting the representation of the question
from R1 to R2. The attention heads downstream of layer L2 react accordingly by stopping the retrieval
of R2(C2) and copying R1(C2) instead.
Finally, we compare the attention probability and direct effect of the attention heads after request-
patching to our control condition on the D3 dataset (bottom). We find that attention heads have
a slightly lower attention probability and direct effect on average (relative variation of -7% for the
attention, -11% for the direct effect). This suggests that the attention heads in charge of copying the
correct token (attending and directly contributing to the logit) are working similarly on the reference
dataset and after request-patching.
A.4
Request-patching is influencing late MLPs
In the previous section, we showed that attention heads seem to act as mover heads. They exploit
the representation built at the previous layers to compute their queries and use the keys from the
context to match the relevant token and copy it to the last position. This pattern has been previously
documented in the literature [70, 40].
62

Figure A.3: Three-way comparison of the effect of request-patching on the attention heads. Each
pair of symbols connected by a line is the same attention head in two different experimental set-
tings. Request-patching is inhibiting the heads in charge of copying the R1(C1) token (top left) and
activating the heads retrieving R1(C2) (right). The state of attention heads after request-patching is
close to the control condition on the reference dataset (bottom).
63

We continue our investigation by exploring whether the attention mechanism is the only mechanism
involved in contributing to the correct token. To this end, we perform attention patching. We fix
the attention pattern of an attention head to its value on another question. In our case, we fix the
attention of attention heads to their values on the D3 dataset. Formally, for the head i at layer l, an
input x2 ∈D2 and x3 ∈D3 we perform the interchange intervention M(x2|Ai,l ←Ai,l(x3)). We only
intervene on the attention to the context and normalize the attention probabilities such that they
always sum to 1.
Attention patching on every attention head causes the model to output R1(C2) (the character name)
with an average probability of 0.14 while predicting R2(C2) (the season) with a probability of 0.06.
Fixing the attention of all attention heads is not enough to force the model to answer the question
R1. This suggests that request-patching exploits an additional mechanism to reach 0.69 probability
of R1(C2).
The direct contributions of the most important components after request-patching and attention
patching are shown in Figure A.4. Unsurprisingly, we observe that the direct effect of the attention
heads is preserved after attention patching, as their attention pattern is fixed to have their value
from D3. However, the contribution of the MLP after attention patching is significantly smaller than
on the reference dataset.
Table A.2 summarizes the relative variation in direct effect grouped by component type after the
two kinds of intervention. While the overlap between the top contributing components with the
reference dataset is significant in both cases (57%), the MLP contribution is similar to the reference
dataset for request-patching (+4.8% relative variation) but smaller for attention patching (-26%
of relative variation). We hypothesize that the MLP contribution is the missing effect that causes
request-patching to outperform attention-patching.
We speculate that when every attention head is attending to the R1(C2) token position after attention
patching, the MLPs at the late layer can access the request R2 present in the input, and detect the
anomaly. The MLPs then contribute negatively to R1(C2) to correct the incoherence. In contrast,
request-patching replaces the full representation at intermediate layers, making late MLPs unable
to detect the incoherence between the request in the residual stream and the input sequence. Such
self-correcting functions of MLPs have previously been demonstrated [43]. Additional experiments
are necessary to evaluate if this phenomenon is occurring in this particular setting.
64

Figure A.4: Comparison of the effect of request-patching and attention patching with the reference
dataset. While request-patching leads to the direct effect of attention heads and MLPs similar to the
reference dataset, attention patching leads to a smaller contribution of MLPs.
Table A.2: Relative Variation in Direct Effect from the reference dataset to the request-patching
and attention patching. The mean overlap is computed between the top direct effect contributor
on the D3 dataset and the top contributor after request and attention patching. The overlap is
computed in an aligned manner, i.e. components on x3 ∈D3 correspond to the component after
M(x2|zL2 ←zL2(x1)) such that R1 = R3 and C2 = C3.
Patching Type
Component Type
Mean
Std Dev
Request Patching
Attention Head
-0.114
0.098
MLP
0.048
0.116
Mean Overlap
0.57
0.07
Attention patching
Attention Head
0.092
0.142
MLP
-0.260
0.111
Mean Overlap
0.56
0.08
65

Appendix B
Additional performance metrics on
ORION
We present the performance of the 18 models studied on the ORION constellation measured using
the logit difference and the probability of correct token in Figure B.1. The value for the probability of
the correct token is used as the normalization factor when computing normalized token probability.
66

Figure B.1: Logit difference and probability of correct token for 18 language models on the tasks
from the ORION constellation. A logit difference of zero means that the correct logit has on average
the same value as the logit corresponding to the answer to a random request in the same context.
67

Appendix C
Additional residual stream results
Figure C.1 shows the layers L1 and L3 for every model and task studied. We observe a similar motif
as in the layer L2 in Figure 6.5. The processing for the induction task seems to happen earlier than
the other tasks such that all three limit layers are shifted toward the early layers.
However, this trend does not hold for Llama 2 70b. All the limit layers for this model seem to be
concentrated over a very narrow span of layers in the middle of the network. To further explore this
surprising observation, Figure C.2 shows the results of residual stream patching on Llama 2 70b
for the factual recall, induction, and translation tasks. The normalized token probability seems to
peak in a narrow range of layers (40-43) for all three tasks, including the simple induction task. It
is unclear why only Llama 2 70b exhibits this pattern, contrasting with models of similar sizes (e.g.
Falcon 40b) that demonstrate spread-out limit layers. This phenomenon could be caused by the
larger scale of the model or peculiarities of the architecture.
68

Figure C.1: Layer L1 and L3 for different models and tasks.
69

Figure C.2: Result of residual stream patching of Llama 2 70b on three retrieval tasks. The maximal
effect of residual stream patching, i.e. maximal probability of the label token R1(C2), is located at
the exact same layer (layer 42) for every task.
70

Appendix D
Detailed description of the ORION tasks
In this appendix, we describe in more detail the process we used to create each task of the ORION
constellation. We also provide complete example prompts for each task.
D.1
Question-answering
D.1.1
Generating the stories
We created a set of 100 short stories we used in the four variations of the question-answering task.
Each short story was created by defining:
• The name of the main character
• The occupation of the main character
• The time of day
• The season of the year
• The city of the story
• The action of the story
• An order in which the above elements should be introduced in the story
• An example short story called a ’master story’, used as a template to incorporate the new
narrative elements.
71

The value of each of the narrative elements was uniformly sampled from lists of 3 to 5 different
possible values for each field. The lists were generated using manual interaction with ChatGPT.
The 8 narrative elements were combined in a prompt shown in D.1 and completed by GPT-4. The
goal of this process was to reduce as much as possible the variations introduced by GPT-4, such that
the variables in the generation prompt characterized the generated story as comprehensively as
possible.
D.1.2
Generating the questions
We manually generated questions and answer prefixes about three different narrative variables:
character occupation, city, and name of the main character. For each narrative variable, we created
three different phrasings.
The answer prefixes were either uniform, as shown in Figures D.2 and D.3 for the task variation with
uniform prefix and question at the start, or depended on the variable queried in the question, as
shown in Figure D.4 for the base task variation. The base task variation can be solved by smaller
models, while only larger models can handle uniform answer prefixes.
D.2
Type hint understanding
Using ChatGPT, we generated three Python code snippets introducing new classes and functions
using these classes, as shown in Figure D.5. The context is a set of variables with a given type. The
request asks for a variable name with a particular type. The function definitions do not vary across
prompts and are only used to formulate the request.
D.3
Factual recall
Existing open-source datasets created to study factual recall in language models, such as the one
introduced in [44], contain relations (e.g. the sport of an athlete) that can only be applied to a subset
of the subjects (e.g. only athletes subjects, asking the sport played by a country does not make sense).
This makes it impossible to use causal intervention such as the type required for request-patching
(the desiderata ’Decomposable’ is not fulfilled). Thus, we created two variations of factual recall
tasks such that any pair of subject and relation exists. Contrary to the other task from ORION , the
retrieval tasks do not involve copying an attribute present in the context, the task requires the model
to know the attribute.
72

Geography dataset
We used an open-source database1 of countries. We extracted the name,
capital city, and continent of each country.
Geography dataset
Following the process used in [34], we used a Cross-Verified database (CVDB)
of notable people 3500BC-2018AD [35]. Each individual was ranked by popularity (measured with
the “wiki_readers_2015_2018 feature“), and 4000 of the most popular individuals were taken (2000
men and women each). We selected the fields related to the gender, continent, and nationality of
each notable people.
Filtering
For both datasets, we queried the relation about the entity using a few shot settings,
as shown in Figure D.6. From the raw data extracted from the dataset, we further filtered the list
of entities to keep only the ones where GPT-2 was able to answer all the questions related to the
entity. The final dataset contains 243 notable people (i.e. 729 questions) and 94 countries (i.e. 282
questions).
D.4
Variable binding
We were inspired by the shape of grade school math problems from the GSM8K dataset [16]. The
goal was to create retrieval tasks that would naturally occur in a chain of thought generated by a
model solving a math puzzle. The context contains objects with different quantities. The request
asks for the quantity of an object type.
To create the dataset, we picked one sample from the GSM8K dataset and generated variation using
ChatGPT. An example prompt can be found in Figure D.7.
D.5
Translation
We used ChatGPT to generate news articles using placeholders instead of real names. We instructed
it to add as many names as possible and to prefix each name with a common title, such as "M.".
Then, we asked ChatGPT-3.5 to translate the text into a non-English language. From the translated
text we extracted excerpts that preceded each of the names but did not include any names. These
excerpts formed the request. When creating the dataset, the placeholders are replaced by distinct
family names from a list generated by ChatGPT.
Using this process, we created three variations with different subjects, target languages, and name
prefixes.
1https://github.com/annexare/Countries
73

• Title: "Climate Change: The Unsung Heroes", Prefix: "M.", Target language: French.
• Title: "Hidden Wonders Revealed: New Species Discovered in Unexplored Amazon Rainforest",
Prefix: "Dr.", Target language: Spanish.
• Title: "From Pirates to Naval Heroes: Captains who Shaped Maritime History", Prefix: "Capt.",
Target language: German.
The entities in the context are the named characters, and their attribute is the sentence in which
they appear. The request is asking for a name that appears in a given sentence.
D.6
Induction
We generated 10 pairs of random strings made from upper and lower-case letters separated by a
column. The context contains five enumerations of the pairs. Each enumeration is in a random
order. The request is the first half of one of the pairs. An example prompt is shown in Figure D.9.
74

You have to generate a short
story
that fits in a single
paragraph of less than 150 words. It has to respect a list of
precise
constraints.
###
Narrative
elements
The main
character is named {character_name }. Their
occupation
is { character_occupation }. The story
takes
place in {city }. The
time of the day is the {day_time}, and the season is {season }.
The time of the day should
stay
constant in the story. The
action
performed by the main
character is {action }.
It ’s crucial
that all the
elements
appear in the story.
### Order of the
narrative
elements
The order in which to introduce
the
narrative
elements is
imposed. The main
priority is to respect
the order of
apparition I impose. Here is the
imposed
order in which to
introduce
the
narrative
elements. This
order is already
present
in the
template
story.
{ variable_order}
###
Template
story
You have to generate a story
that
matches as closely as
possible a template
story. Your goal is to modify the
template
story
such that all the
narrative
elements
are present , but the
general
structure (e.g. order in which the
narrative
element
are
introduced
etc.) is as close as possible to the
template
story.
Here is the
template
story you have to stick to:
"{ master_story_text }"
Generate a short
story
that
matches
the
template
story
while
incorporating
the new
narrative
elements.
Figure D.1: Prompt used to generate the short stories for the question-answering tasks. The variables
in curly brackets represent placeholders that were replaced by values randomly sampled from
manually created lists of possible values.
75

Context
<|endoftext|>
Here is a short
story. Read it carefully
and answer the
questions
below
with a keyword
from the text. Here is the
format of the answer: ’The answer is "xxx".’
The
morning sun bathed the
streets of Cusco in a warm , golden
light , casting
long
shadows
that
danced
along
with the gentle
summer
breeze. Amidst the
bustling city , a tall , slender
figure
stood on the
rooftop of an unfinished
building , their
eyes
surveying
the urban
landscape
below. As the
skyline
slowly
transformed
under
their
careful
guidance , it became
apparent
that the person was no mere observer , but an architect ,
orchestrating
the
symphony of steel and
concrete. The sound of
birdsong
filled the air , but it was soon
joined by another
melody
-- the architect ’s voice , soaring
with joy and passion ,
a song of creation
and
ambition. And as the last
notes
faded
away , the wind
carried a whispered name , the
signature of the
artist who painted
the city with
their
dreams: Michael.
Answer the
questions
below.
Request
Question: What job does the main
character
have?
Answer: The answer is "
Figure D.2: Example prompt for the QA (uniform prefix) task.
76

Request
<|endoftext|>
Read the
question below , then
answer it after
reading
the story
using a keyword
from the text. Here is the format of the
answer: ’The answer is "xxx".’
Question: What job does the main
character
have?
Context
Story: The
morning sun bathed the
streets of Cusco in a warm ,
golden light , casting
long
shadows
that
danced
along
with the
gentle
summer
breeze. Amidst the
bustling city , a tall , slender
figure
stood on the
rooftop of an unfinished
building , their
eyes
surveying
the urban
landscape
below. As the
skyline
slowly
transformed
under
their
careful
guidance , it became
apparent
that the person was no mere observer , but an architect ,
orchestrating
the
symphony of steel and
concrete. The sound of
birdsong
filled the air , but it was soon
joined by another
melody
-- the architect ’s voice , soaring
with joy and passion ,
a song of creation
and
ambition. And as the last
notes
faded
away , the wind
carried a whispered name , the
signature of the
artist who painted
the city with
their
dreams: Michael.
Answer: The answer is "
Figure D.3: Example prompt for the QA (question first) task.
77

Context
<|endoftext|>
Here is a short
story. Read it carefully
and answer the
questions
below.
The
morning sun bathed the
streets of Cusco in a warm , golden
light , casting
long
shadows
that
danced
along
with the gentle
summer
breeze. Amidst the
bustling city , a tall , slender
figure
stood on the
rooftop of an unfinished
building , their
eyes
surveying
the urban
landscape
below. As the
skyline
slowly
transformed
under
their
careful
guidance , it became
apparent
that the person was no mere observer , but an architect ,
orchestrating
the
symphony of steel and
concrete. The sound of
birdsong
filled the air , but it was soon
joined by another
melody
-- the architect ’s voice , soaring
with joy and passion ,
a song of creation
and
ambition. And as the last
notes
faded
away , the wind
carried a whispered name , the
signature of the
artist who painted
the city with
their
dreams: Michael.
Answer the
questions below , The
answers
should be concise
and
to the point.
Request
Question: What job does the main
character
have?
Answer: The main
character is a professional
Figure D.4: Example prompt for the QA (base) task.
78

Context
<|endoftext|>
from
typing
import
List
from
math
import pi
class
Point:
def
__init__(self , x: float , y: float) -> None:
self.x = x
self.y = y
class
Rectangle:
def
__init__(self , bottom_left: Point , top_right: Point) -> None:
self. bottom_left = bottom_left
self.top_right = top_right
class
Circle:
def
__init__(self , center: Point , radius: float) -> None:
self.center = center
self.radius = radius
class
Polygon:
def
__init__(self , points: List[Point ]) -> None:
self.points = points
def
calculate_area (rectangle: Rectangle) -> float:
height = rectangle.top_right.y - rectangle.bottom_left .y
width = rectangle.top_right.x - rectangle. bottom_left.x
return
height * width
def
calculate_center (rectangle: Rectangle) -> Point:
center_x = (rectangle. bottom_left.x + rectangle.top_right.x) / 2
center_y = (rectangle. bottom_left.y + rectangle.top_right.y) / 2
return
Point(center_x , center_y)
def
calculate_distance (point1: Point , point2: Point) -> float:
return (( point2.x - point1.x) ** 2 + (point2.y - point1.y) ** 2) ** 0.5
def
calculate_circumference (circle: Circle) -> float:
return 2 * pi * circle.radius
def
calculate_circle_area (circle: Circle) -> float:
return pi * (circle.radius ** 2)
def
calculate_perimeter (polygon: Polygon) -> float:
perimeter = 0
points = polygon.points + [polygon.points [0]]
# Add the
first
point at the end for a closed
shape
for i in range(len(points) - 1):
perimeter
+=
calculate_distance (points[i], points[i + 1])
return
perimeter
# Create a polygon
Y = Polygon ([ Point (0, 0), Point (1, 0), Point (0, 1)])
# Create a rectangle
K = Rectangle(Point (2, 3), Point (6, 5))
# Create a circle
P = Circle(Point (0, 0), 5)
Request
# Calculate
area
print( calculate_area (
Figure D.5: Example prompt for the type hint understanding task.
79

CVDB prompt
<|endoftext|>Question: What was the
country of Freddie
Mercury?
Answer: UK
Question: On which
continent
did
Muhammad
Ali live?
Answer: America
Question: What was the country of Fela Kuti?
Answer:
Geography prompt
<|endoftext|>Question: What is the
capital of France?
Answer: Paris
Question: What is the
language
spoken in Malaysia?
Answer:
Figure D.6: Example prompt for the factual recall task on the CVDB and geography datasets. There is
no clear division between context and request in the prompt. In full rigor, the context is composed
of a single entity, e.g. ’Fela Kuti’ in the first prompt, while the request is asking about an attribute,
e.g. the country, without filtering as there is a single entity in the context.
Context
<|endoftext|>John is baking
cookies. The recipe
calls for 4
cups of flour , 2 cups of sugar , and 6 cups of chocolate
chips.
How many cups of ingredients in total are needed for the
cookies?
Request
We ’ll add the number of cups of flour (
Figure D.7: Example prompt for the variable binding task.
80

Context
<|endoftext|>
Here is a new article in English. Below, you can find a partial translation in French. Please
complete the translation.
English:
Title: "Climate Change: The Unsung Heroes"
In an era defined by increasing global temperatures and extreme weather events, the fight
against climate change continues on many fronts. While prominent environmentalists and
politicians often claim the limelight, behind the scenes, countless unsung heroes have
dedicated their lives to combating climate change. This article aims to spotlight the work of
these individuals.
At the forefront is M. Jones, a marine biologist who has developed an innovative method for
promoting coral reef growth. Given that coral reefs act as carbon sinks, absorbing and storing
CO2 from the atmosphere, M. Jones’s work has significant implications for climate mitigation.
Despite facing numerous hurdles, M. Jones has consistently pushed forward, driven by an
unwavering commitment to oceanic health.
Next, we turn to M. Martinez, a climate economist from a small town who has successfully
devised a market-based solution to curb industrial carbon emissions. By developing a novel
carbon pricing model, M. Martinez has enabled a tangible shift toward greener industrial
practices. The model has been adopted in several countries, resulting in significant
reductions in CO2 emissions. Yet, despite these successes, M. Martinez’s work often flies
under the mainstream media radar.
Another unsung hero in the climate change battle is M. Perez, a young agricultural scientist
pioneering a line of genetically modified crops that can thrive in drought conditions. With
changing rainfall patterns threatening food security worldwide, M. Perez’s work is of immense
global relevance. However, due to controversy surrounding genetically modified organisms, the
contributions of scientists like M. Perez often go unnoticed.
Additionally, the story of M. Thomas is worth mentioning. An urban planner by profession, M.
Thomas has been instrumental in designing green cities with a minimal carbon footprint. By
integrating renewable energy sources, promoting public transportation, and creating more green
spaces, M. Thomas has redefined urban living. While the aesthetics of these cities often
capture public attention, the visionary behind them, M. Thomas, remains relatively unknown.
Lastly, we have M. Harris, a grassroots activist working tirelessly to protect and restore the
forests in her community. M. Harris has mobilized local communities to halt deforestation and
engage in extensive tree-planting initiatives. While large-scale afforestation projects often
get global recognition, the efforts of community-level heroes like M. Harris remain largely
unsung.
The fight against climate change is not a single battle, but a war waged on multiple fronts.
Every victory counts, no matter how small. So, as we continue this struggle, let’s not forget
to appreciate and honor the unsung heroes like M. Jones, M. Martinez, M. Perez, M. Thomas, and
M. Harris who, away from the spotlight, are making a world of difference.
Request
French:
[...]
En intégrant des sources d’énergie renouvelables, en favorisant les transports publics et en
créant plus d’espaces verts, M.
Figure D.8: Example prompt for the translation task.
81

Context
<|endoftext|>wFCJI:CCwti
axRPX:ISNak
JaVZO:jjVAE
vGuLv:aqCuW
peaXt:uqIWZ
gLbzR:URzLs
XPUgR:QDKMS
IbKIs:YRodj
GpqLd:YRodj
fhVqk:jjVAE
axRPX:ISNak
gLbzR:URzLs
wFCJI:CCwti
GpqLd:YRodj
fhVqk:jjVAE
vGuLv:aqCuW
XPUgR:QDKMS
peaXt:uqIWZ
IbKIs:YRodj
JaVZO:jjVAE
axRPX:ISNak
XPUgR:QDKMS
wFCJI:CCwti
IbKIs:YRodj
gLbzR:URzLs
peaXt:uqIWZ
vGuLv:aqCuW
JaVZO:jjVAE
GpqLd:YRodj
fhVqk:jjVAE
wFCJI:CCwti
GpqLd:YRodj
peaXt:uqIWZ
gLbzR:URzLs
XPUgR:QDKMS
axRPX:ISNak
JaVZO:jjVAE
IbKIs:YRodj
fhVqk:jjVAE
vGuLv:aqCuW
peaXt:uqIWZ
XPUgR:QDKMS
wFCJI:CCwti
JaVZO:jjVAE
IbKIs:YRodj
fhVqk:jjVAE
gLbzR:URzLs
axRPX:ISNak
GpqLd:YRodj
vGuLv:aqCuW
peaXt:uqIWZ
gLbzR:URzLs
GpqLd:YRodj
peaXt:uqIWZ
GpqLd:YRodj
fhVqk:jjVAE
GpqLd:YRodj
XPUgR:QDKMS
peaXt:uqIWZ
Request
wFCJI:
Figure D.9: Example prompt for the induction task.
82

