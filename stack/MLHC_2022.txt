Proceedings of Machine Learning Research 182:1–25, 2022
Machine Learning for Healthcare
Reducing Reliance on Spurious Features in Medical Image
Classification with Spatial Specificity
Khaled Saab
ksaab@stanford.edu
Department of Electrical Engineering, Stanford University, USA
Sarah Hooper
smhooper@stanford.edu
Department of Electrical Engineering, Stanford University, USA
Mayee Chen
mfchen@stanford.edu
Department of Computer Science, Stanford University, USA
Michael Zhang
mzhang@cs.stanford.edu
Department of Computer Science, Stanford University, USA
Daniel Rubin
dlrubin@stanford.edu
Department of Biomedical Data Science, Stanford University, USA
Christopher R´e
chrismre@stanford.edu
Department of Computer Science, Stanford University, USA
Abstract
A common failure mode of neural networks trained to classify abnormalities in medical
images is their reliance on spurious features, which are features that are associated with
the class label but are non-generalizable. In this work, we examine if supervising models
with increased spatial specificity (i.e., information about the location of the abnormality)
impacts model reliance on spurious features. We first propose a data model of spurious
features and theoretically analyze the impact of increasing spatial specificity. We find that
two properties of the data are impacted when we increase spatial specificity: the variance
of the positively-labeled input pixels decreases and the mutual information between abnor-
mal and spurious pixels decreases, both of which contribute to improved model robustness
to spurious features. However, supervising models with greater spatial specificity incurs
higher annotation costs, since training data must be labeled for the location of the abnor-
mality, leading to a trade-off between annotation costs and model robustness to spurious
features. We investigate this trade-off by varying the coarseness of the spatial specificity
supplied and sweeping the quantity of training samples that have information about the ab-
normality location. Further, we assess if semi-supervised and contrastive learning methods
improve the cost-robustness trade-off. We empirically examine the impact of supervising
models with increased spatial specificity on two medical image datasets known to have
spurious features: pneumothorax classification on chest x-rays and melanoma classification
from dermoscopic images. We find that while models supervised with binary labels have
near-random robust performance (robust AUROC of 0.46), increasing spatial specificity to
bounding box detection and image segmentation achieves a robust AUROC of 0.72 and
0.82, respectively, on the pneumothorax classification task. We also observe this trend for
the melanoma task, where segmentation models achieve a robust AUROC of 0.73, com-
pared to worse than random performance for models trained with binary labels. Moreover,
by leveraging semi-supervised and contrastive methods, models achieve a 5 point gain in
robust AUROC when we have access to very few training samples.
© 2022 K. Saab, S. Hooper, M. Chen, M. Zhang, D. Rubin & C. R´e.

Reducing Reliance on Spurious Features with Spatial Specificity
1. Introduction
One of the key roadblocks to deploying trustworthy machine learning models in clinical set-
tings is their hidden reliance on spurious features (Wiens et al., 2019). Spurious features are
characterized as features that are associated with, but not causally related to the target task,
and naturally occur in many real-world settings. A prominent example of machine learning
models relying on spurious features is that of pneumothorax (PMX) classification from chest
X-rays (CXRs). Oakden-Rayner et al. (2020) observed that a model with “expert-level”
performance actually relied on the presence of chest tubes—a spuriously correlated feature
used to treat PMX—to classify PMX, and the model dropped to first-year resident perfor-
mance when tested on CXRs without chest tubes. Similarly, Zech et al. (2018) observed
that a CXR model that predicted pneumonia relied on hospital-specific features, such as
scanner hardware, due to large differences in pneumonia prevalence among hospitals in the
training data. Winkler et al. (2019) observed that a model with near-perfect performance
for melanoma classification from dermoscopic images was sensitive to the presence of surgi-
cal skin markers, which are often present in suspicious lesions. These examples are part of
a growing body of work (DeGrave et al., 2021; Badgeley et al., 2019) revealing that models
with seemingly “expert-level” performance on medical tasks often rely on non-generalizable
features, resulting in systematic and potentially dangerous model errors.
Encouragingly, there have been several proposed methods to improve model robustness
to spurious features. A common technique is to estimate spurious labels in an unsupervised
manner (e.g., by clustering a model’s representations) to either manipulate the training
data (Goel et al., 2020) or use in robust optimization techniques (Liu et al., 2021; Sohoni
et al., 2020; Zhang et al., 2022). While such methods are promising, their success relies
on their ability to detect relevant subgroups in an unsupervised manner and have yet to
be shown effective on medical image classification tasks (Bissoto et al., 2020) (see also
Table 1). Another direction that has shown promise is to augment model supervision with
additional domain-specific knowledge. For example, Seah et al. (2021) train a chest X-
ray classification model supervised with a comprehensive class ontology tree. Graf et al.
(2020) tackle the pneumothorax classification problem with a customized pipeline that
ensembles classification and segmentation models for tubes, lungs, and pneumothoraces.
While both techniques lead to reduced reliance on spurious features, they require much
higher annotation costs and introduce task-specific training pipelines, which precludes them
from being readily applied in other scenarios.
However, these latter techniques share a
central theme that we build upon in this work: by supervising models with greater task
specificity, we can improve robustness to spurious features.
Specifically, we investigate the role of spatial specificity in a model’s reliance on spuri-
ous features. We define the spatial specificity of a task as the amount of information about
abnormality location in the supervision (see e.g., Figure 1). In binary classification, the
supervision provides one bit of information: is the abnormality present. This setting has
the lowest spatial specificity as no information is provided about the abnormality location.
At the other end of the spectrum is image segmentation, where the supervision provides
maximum spatial specificity by indicating the precise abnormality location. Patch-based
classification (e.g., identifying the quadrants with the abnormality) and bounding-box detec-
tion lie in the middle of the spatial specificity spectrum. We hypothesize that as we increase
2

Reducing Reliance on Spurious Features with Spatial Specificity
Figure 1: Varying levels of spatial specificity for pneumothorax classification. The yellow
region denotes the positively labeled pixels, where the pneumothorax is in the
upper left lung; the white arrow points at the spurious chest tube.
spatial specificity along this spectrum, the presence of a spurious feature gets disentangled
from the presence of an abnormality, weakening the spurious feature’s informativeness of
the task and, as a result, the model’s reliance on the spurious feature.
We first theoretically explore the hypothesis that increasing spatial specificity reduces
the model’s reliance on spurious features. Under a data model we propose for the spurious
features and abnormalities, we examine how two measures on the data—the variance over
the positively-labeled input pixels and the mutual information between the spurious feature
and abnormal label—are impacted by spatial specificity, and we connect these properties
to model robustness. We find that increasing spatial specificity reduces the variance of the
positively-labeled pixels, suggesting that the model will learn lower variance representations
for the positive class. This results in improved generalization to test samples where the spu-
rious correlation does not hold. Then, we describe conditions under which increasing spatial
specificity decreases the mutual information between the spurious label and the abnormal
label. As spatial specificity increases, the spurious feature becomes less informative about
the presence of an abnormality, and in this case, the model is less likely to learn based
on the spurious feature. We empirically support our theoretical findings with a synthetic
instantiation of our data model.
However, increasing spatial specificity comes at a cost: it takes clinicians more time
to annotate more specific abnormality locations. For example, after a clinician finds the
abnormality, they can provide a binary label with one click, a bounding box with a few
clicks, or a segmentation mask with around 100 clicks (Yi, 2020). Since increased clinician
time translates to increased annotation cost, there exists a cost-robustness trade-off where
the annotation cost depends on the amount of spatial specificity in the annotations and the
number of annotations needed to train a high-performing model. Motivated by our theo-
retical insights that increased spatial specificity improves model robustness due to tighter
class embeddings, we investigate the effectiveness of two techniques at reducing the number
of labeled training data in the setting of highest spatial specificity (i.e., segmentation): (1)
semi-supervised learning (SSL) with an unsupervised image-level similarity loss that pulls
3

Reducing Reliance on Spurious Features with Spatial Specificity
image embeddings with different augmentations closer to one another, and (2) a supervised
pixel-level similarity loss that pulls abnormal pixel embeddings closer to one another.
We study the impact of spatial specificity on two real-world datasets with known spuri-
ous features: CXR pneumothorax classification with chest tubes as a spurious feature, and
melanoma classification from dermoscopic images of skin lesions with multiple spurious fea-
tures (e.g., surgical markings). We find that increasing spatial specificity directly translates
to models which are substantially more robust to spurious features. For the pneumothorax
task, models trained with binary labels achieve a robust AUROC of 0.45, while models
trained for bounding box detection and segmentation achieve robust AUROCs of 0.72 and
0.82, respectively. Similarly for melanoma classification, a segmentation model achieves 0.73
robust AUROC while models trained with binary labels achieve worse than random robust
performance. Using semi-supervised and contrastive learning techniques, we improve the
cost-robustness trade-off by improving robust AUROC by 5 points in low data regimes.
Generalizable Insights about Machine Learning in the Context of Healthcare
Safely deploying machine learning systems in clinical settings requires understanding and
addressing their systematic errors. While spurious correlations are ubiquitous in real-world
data and are a major cause for such systematic errors, machine learning practitioners can
take actions during task specification, dataset curation, or model training to mitigate such
failure modes. We explore increasing the task specification to reduce model reliance on
spurious features and make the following contributions:
• We study the impact of the spatial specificity on robustness to spurious features, and
observe a relationship between increased spatial specificity and improved robustness.
• We share theoretical insights into why and when spatial specificity improves robustness
to spurious features.
• We validate that increasing spatial specificity improves spurious feature robustness on
the CXR pneumothorax and melanoma classification tasks.
• We explore the cost-robustness trade-off with spatial specificity and assess two tech-
niques to reduce the annotation burden when increasing spatial specificity.
Our study finds a clear trade-off between annotation cost of increasing task specificity
and robustness to spurious features. In settings where spurious features have low overlap
with the abnormal region, or self-, weakly-, or semi-supervised techniques are highly effective
in reducing labeled data, a clinical machine learning practitioner should keep in mind the
benefits of increasing spatial specificity.
2. Related Work
We stratify the many techniques that combat spurious features by the stage of intervention:
model training, training data, and task specificity.
4

Reducing Reliance on Spurious Features with Spatial Specificity
Intervening during model training. The vast majority of techniques combat spurious
features during model training, where the task and training dataset are given and the goal is
to alter the objective function to prevent the model from learning spurious features. Notable
lines of work include robust optimization, where the objective is to minimize the worst-case
subgroup performance (GDRO) (Sagawa et al., 2019), invariant risk minimization (IRM)
(Arjovsky et al., 2019), and adversarial learning (Alvi et al., 2018; Kim et al., 2019; Zhang
et al., 2018). Importantly, these techniques require access to the spurious labels, which
is a major limitation when systematic model errors are unknown prior to model training.
However, there have been works that propose a two-step process to circumvent the need
for spurious labels prior to model training. These approaches first estimate spurious labels
then use the estimated labels in robust objectives (such as GDRO). For example, GEORGE
estimates spurious labels by clustering an initial trained model’s representations and uses
these noisy labels in GDRO (Sohoni et al., 2020). Liu et al. (2021) estimate spurious labels
by grouping model errors on the training set and use these noisy labels to reweight training
samples. Zhang et al. (2022) also estimate spurious labels by grouping model errors, but
they leverage a contrastive approach to pull points with the same target label but differ-
ent spurious labels closer together. Other techniques that attempt to estimate spurious
labels to then de-bias a model via optimization include pseudo bias-balanced learning (Luo
et al., 2022), LfF (Nam et al., 2020), CVaR DRO (Levy et al., 2020), EIIL (Creager et al.,
2021), and PGI (Ahmed et al., 2020). While these methods have made great progress in
mitigating spurious features, they have primarily been validated on natural, as opposed to
medical, images and heavily rely on the ability of heuristic approaches to estimate spurious
labels. Another interesting direction is using pre-trained weights, or transfer learning, and
has shown to be effective at improving robustness to spurious features (Jabbour et al., 2020).
Intervening through the training data. Since the culprit for reliance on spurious fea-
tures is strong correlations in the training data, some techniques focus on weakening that
correlation through training data interventions. One theme in these techniques is learning
input transformations that can add or remove spurious features, allowing for the removal
of the correlation (Taghanaki et al., 2021; Goel et al., 2020). As opposed to data augmen-
tation, Maron et al. (2021) propose pre-processing input images by masking background
pixels using a pretrained image segmentation model. In similar spirit, Viviano et al. (2019)
propose to align model representations after shuffling background pixels.
Intervening via task specificity. At this stage of the pipeline, techniques may alter the
original task to combat spurious features. For example, Seah et al. (2021) increase task
specification by greatly increasing the number of class labels. By training a model on a
comprehensive class ontology tree, consisting of 127 CXR classes including five chest tube
classes, they show that the resulting model does not suffer a performance drop on examples
without chest tubes. Graf et al. (2020) directly tackle the spurious feature of chest tubes
in CXRs by hand-crafting a pipeline that consists of multiple tasks of varying specificity,
including a lung detector, pneumothorax segmentation model, and a chest tube classifier.
Our work falls under intervening via task specification. We take inspiration from these
techniques, which increase task specificity and as a result have shown improved robust-
5

Reducing Reliance on Spurious Features with Spatial Specificity
Figure 2: An example of an abnormal image with a spurious feature from our synthetic
dataset.
The darker square is the spurious feature, while the smaller lighter
square is the abnormal area. The red box (˜Y ) encompasses the pixels labeled
as positive. As we increase spatial specificity (left to right), the spurious feature
becomes less informative/correlated with the positive labels.
ness against spurious features, and dive into studying the impact of increasing the spatial
specificity in medical image classification.
3. Analyzing the Impact of Spatial Specificity on Robustness
We first set up the problem setting of image classification in the context of spatial speci-
ficity (Section 3.1). Then, we introduce a model of spurious features in images in order
to understand how robustness to these features changes with increased spatial specificity
(Section 3.2). Under this model, we describe how to construct a synthetic dataset that
captures key characteristics of medical image datasets with spurious features. Finally, we
theoretically analyze the relationship between spatial specificity and robustness by studying
two measures on the data (Section 3.3). We end the section by validating our theoretical
insights with empirical observations on the synthetic dataset.
3.1. Preliminaries
We formally describe the image classification task. We are given a dataset of m images each
containing n×n pixels, X ∈Rn×n, and pixel-level labels ˜Y ∈{0, 1}n×n with a certain spatial
specificity (red box in Figure 2). Hidden to us are the ground truth abnormal segmentation
masks, Y ∈{0, 1}n×n. We define the image-level label y ∈{0, 1}, where y = 0 if Yij = 0 for
all i, j (no abnormality), and y = 1 otherwise (abnormality present).
The spatial specificity of our supervision determines the granularity of the given pixel-
level labels ˜Y . We define the spatial specificity with a quantity r > 0, where ˜Yij = 1 if
there exists a positive-labeled pixel Ykl = 1 within ⌊n
r ⌋pixels of it, both horizontally and
vertically (formally, satisfying |i −k| < ⌊n
r ⌋and |j −l| < ⌊n
r ⌋). Then, a spatial specificity of
r = 1 reduces to the binary supervision setting where ˜Yij = 1 for all i, j if there exists any
abnormal pixel satisfying Ykl = 1. As we increase r, ˜Y approaches Y over the entire image,
and r = n reduces to ˜Y = Y , the segmentation setting (see Figure 2).
The problem we consider is binary image classification: given a dataset {(Xi, ˜Y i)}m
i=1,
we train a model w : Rn×n 7→{0, 1}, that predicts the existence of an abnormality in an
image, y.
6

Reducing Reliance on Spurious Features with Spatial Specificity
3.2. Modeling spurious features
We present a model for how spurious features and abnormalities are represented in images.
Using this model, we describe a synthetic dataset setup we use in the rest of our analysis.
3.2.1. Proposed Data Model
Some images may contain spurious features, which are distinct features that are correlated
with the abnormality but are non-generalizable.
We denote the presence of a spurious
feature with an image-level spurious label s ∈{0, 1} and pixel-level labels S ∈{0, 1}n×n.
The generative process of constructing X, Y, ˜Y and S is as follows: first, values of y and s
are picked. Then, Y, S are drawn from a distribution Py,s, where Y = 0 if y = 0 and S = 0
if s = 0. ˜Y is deterministically constructed as a function of Y , where we note that Yij = 1
implies ˜Yij = 1. Finally, X is drawn from the distribution Pr(X|Y, S).
We make two additional assumptions. First, we assume that the spurious and abnormal
regions of the image are non-overlapping, e.g.
Py,s satisfies Pr(Sij = 1|Yij = 1) = 0.
In practice, there are cases where these regions have little overlap (such as chest tubes
and PMX), though we make this simplifying assumption to better isolate the impact of
spatial specificity. Second, for simpler analysis we assume that at a pixel level, the spurious
feature and label are independent conditioned on if there is an abnormality or not. That
is, Sij ⊥˜Yij|Yij. This means the chance of a pixel being spurious only depends on whether
the pixel is in the background, regardless of the (lower resolution) label provided.
3.2.2. Generating a synthetic dataset
We present a particular synthetic data setup that reflects two key characteristics observed in
medical image classification tasks with spurious features: that there is a correlation between
s and y, and that the spurious feature S is more visible than the abnormality.
To construct Y and S, we first draw y ∼Bernoulli(µy). To construct a spurious feature
that is highly correlated with y, we sample s with Pr(s = 1|y = 1) = ρ with ρ > 0.5, and
Pr(s = 1|y = 0) = 0 on the training data. On the test data, we aim to evaluate samples
where the spurious feature is independent of y, so we let Pr(s = 1) = 0.5 for either value of
y. When y = 1, we choose an a × a block for the abnormal region Y uniformly at random.
When s = 1, we choose a b × b block for the spurious region S uniformly at random.
To construct images X where S is easy to identify relative to the abnormality, we sample
our images with each pixel drawn from a Gaussian distribution depending on Yij and Sij:
Xij|Yij = 0, Sij = 0 ∼N(0, σ2); Xij|Yij = 1, Sij = 0 ∼N(µa, σ2); Xij|Yij = 0, Sij = 1 ∼
N(µs, σ2), where σ2 is the variance and µa and µs satisfy |µa| < |µs|. We instantiate our
synthetic dataset with the following parameters: m = 150, n = 40, µy = 0.9, ρ = 1, µa = 2,
µs = −10, σ = 1, a = 8, b = 12 (see Figure 2).
3.3. Spatial Specificity Analysis
We now present two measures on the data that intuitively suggest better robustness, and we
theoretically show that they depend on spatial specificity under our proposed data model.
We then empirically validate the relationship between these two properties and robustness
of the model on synthetic data.
7

Reducing Reliance on Spurious Features with Spatial Specificity
3.3.1. Theoretical Results
We propose two measures to capture the spatial specificity of a task. First, we introduce
the variance measure on input image pixels.
Definition 1 (Variance measure) The variance measure is defined as Vari,j[Xij| ˜
Yij =
1], the variance in input data over the positively labeled input pixels.
Intuitively, the more specific the pixel label is in defining an abnormal pixel, fewer
“background” pixels are grouped with positively labeled pixels, which reduces their variance.
Our first theoretical result examines the variance measure on the training distribution of
our synthetic setup.
Proposition 2
The variance measure Vari,j[Xij| ˜Yij = 1] varies between Var[Xij|y = 1]
when r = 1 (low spatial specificity) and Var[Xij|Yij = 1] when r = n (high spatial speci-
ficity). If we assume Xij is sampled according to a Gaussian distribution based on Yij and
Sij as in Section 3.2, then increasing r quadratically decreases the proposed measure under
conditions defined in Appendix B.1.
Proposition 2 tells us that adjusting r impacts the variance measure by interpolating
between the variance of abnormal pixels and the variance of all pixels. In our synthetic
setup, we find that our measure monotonically decreases as we increase r, proving to be an
appropriate and practical measure for spatial specificity. Via standard deep learning theory
analyses, we can demonstrate that various classes of models will preserve the variance of
positive-labeled pixels when they are mapped into embedding space. Then, in conjunction
with Theorem 3.1 in Zhang et al. (2022), which describes how clustered class embeddings
help robustness, this characterization of the variance measure forms a connection to how
increasing spatial specificity improves spurious robustness.
Our characterization of when the variance measure decreases is specific to the Gaussian
model in our synthetic setup.
For a more general setting, we consider how the mutual
information between abnormal and spurious pixel labels changes with spatial specificity.
Definition 3 (MI measure) The mutual information (MI) measure is defined as I(Sij; Yij|
˜Yij = 1), which represents the amount of information shared between the spurious feature
and the abnormality, given that the pixel label is observed to be positive.
Our next result shows that the MI measure tends to 0 as spatial specificity increases.
Proposition 4
The MI measure I(Sij; Yij| ˜Yij = 1) varies between I(Sij; Yij) when r = 1,
which uses no information from ˜Yij, and 0, which means full independence of the spurious
feature and abnormality, when r = n. As the spatial resolution r increases for r ≥r0, where
r0 is defined in Appendix B.2, I(Sij; Yij| ˜Yij = 1) decreases.
Proposition 4 reflects our intuition that as we increase spatial specificity, the informa-
tiveness of the spurious label to the abnormal label is weakened. A less informative spurious
feature suggests that the model is less likely to use it for prediction and instead use the
information from the given labels.
8

Reducing Reliance on Spurious Features with Spatial Specificity
(a)
0
5
10
15
20
25
30
35
40
Resolution
0.0
0.2
0.4
0.6
0.8
1.0
Robust AUROC
Var(X|Y_tilde=1)
MI(Y_tilde,S)
(b)
(c)
Figure 3: In (a) we observe that as we increase spatial specificity, the robust performance
improves, while the variance (normalized to have maximum value 1) and mutual
information measures decrease. We plot the 2D UMAP projections of the image
embeddings for the binary (b) and segmentation (c) models, and observe that
increasing spatial specificity results in subgroups with the same label but different
spurious label becoming more aligned.
3.3.2. Empirical Validation
We conduct synthetic experiments to validate that the behavior of the variance measure
and the MI measure as spatial specificity changes matches our theoretical findings, and
that these observations are connected to spurious robustness. In particular, we evaluate the
model’s reliance on spurious features and the quality of the learned embeddings via how
“aligned” subgroups based on values of s and y are.
To measure reliance on spurious features, we report the AUROC on the subset of the
test set where the spurious correlation induced in the train set does not hold: normal images
contain the spurious feature while abnormal images do not (we refer to this as the Robust
AUROC). To measure alignment between subgroups, we report the average cosine similarity
between embeddings from the different subgroups.
For each level of spatial specificity, we train a fully connected ResNet50 from the torchvi-
sion segmentation models library, and minimize the cross entropy loss with adam optimizer
with a learning rate of 0.001 and a batch size of m, for 50 epochs and checkpoint at the
epoch with the smallest test loss.
We make the following observations as we increase spatial specificity (Figure 3):
1. The robust AUROC increases, and the variance measure and MI measure decrease,
suggesting we become less reliant on the spurious feature.
2. The model embeddings with the same class but different spurious label become more
aligned (Table 2 in Appendix A.3). This observation reflects better preservation of
intra-class variance in the data and weakened dependence on spurious features, which
are also suggested by our two measures.
9

Reducing Reliance on Spurious Features with Spatial Specificity
While these two measures do not fully characterize how spatial specificity improves
model robustness, by analyzing the relationship between these measures and spatial speci-
ficity, we hope to provide theoretical insight into how to better construct robust models.
4. The Cost-Robustness Trade-off for Spatial Specificity
While increasing spatial specificity may improve robustness to spurious features, it also
incurs higher annotation costs, resulting in a cost-robustness trade-off. In this section, we
first estimate the difference in annotation cost among three common supervision signals
with varying spatial specificity (binary labels, bounding boxes, and segmentation masks).
We then discuss approaches to reduce the annotation cost of increasing spatial specificity.
4.1. Estimating Annotation Cost for Increasing Spatial Specificity
Since medical image supervision is commonly provided in the form of binary labels, bound-
ing boxes, or segmentation masks, we now describe their relative annotation costs.
After a clinician has identified an abnormality, they must then generate the annotation.
We assess the cost differences among annotation types by analyzing the number of clicks
required to generate different annotations. For example, for a given image, providing a
binary label requires 1 click. Thus, to provide binary annotations for m positive samples
with a click time of c seconds, mc clicks will be required.
For a 2D image, bounding
boxes require 2 clicks per abnormality; assuming an average of a abnormalities per image,
bounding box annotation will require 2mca seconds. To generate segmentation masks, the
number of clicks can vary widely between different abnormalities and patients. If we let
n > 2 be the average number of clicks needed to generate a segmentation mask, then
generating a segmentation mask requires nmca seconds. Thus, for images with a single
abnormality, (i.e., a = 1), the estimated additional annotation time to provide bounding
boxes and segmentation masks is mc and (n −1)mc seconds, respectively. For example, if
we have m = 10,000 positive samples, n = 45 clicks (Yi, 2020), c = 1 second, and a clinician
is paid $100/hr, then we would pay an additional $280 for bounding boxes and $12,200 for
segmentation masks.
4.2. Improving the Cost-Robustness Trade-off
As discussed above, providing greater spatial specificity incurs greater annotation costs.
The additional cost strongly depends on both the number of required training samples and
the level of spatial specificity. By reducing the number of samples with location information
or by supplying bounding boxes instead of segmentation masks, the total annotation cost
can be reduced, though likely at the expense of robust performance. In Section 5.2, we
explore how robust and overall performance change along these two dimensions.
Additionally, we explore training techniques that are designed to reduce the amount
of labeled data needed to train machine learning models. Much prior work has focused
on developing methods to train segmentation models with limited labeled data, showing
that models can be trained with significantly less labeled data while maintaining strong
performance (Tajbakhsh et al., 2020). In this work, we explore if such methods can be
used to train models with greater spatial specificity while keeping labeling costs low but
10

Reducing Reliance on Spurious Features with Spatial Specificity
maintaining high robust performance. We guide our exploration of these techniques and
build upon them using the insights gained from our theoretical analysis in Section 3.
4.2.1. Unsupervised Image-level Similarity Loss
In Section 3.3, we formed the intuition that increasing spatial specificity results in embed-
ding spaces with tighter class clusters, which corresponds to models with reduced reliance
on spurious features.
Motivated by this intuition, we implement a previously proposed
semi-supervised training method that reduces reliance on labeled data by using a cosine
embedding loss over unlabeled data, encouraging similar model embeddings for a given
image with different augmentations (Hooper et al., 2022). Our motivation for using this
method is that since augmentations of an image do not change the label, the cosine embed-
ding loss would encourage tighter class embeddings.
The semi-supervised training is a two-step process, described in brief below and de-
scribed in detail in prior work (Hooper et al., 2022). In Step 1, an initial segmentation
network f′ is trained using both labeled and unlabeled data. A supervised cross-entropy
loss is implemented to learn from the labeled data, encouraging the outputs of f′ to match
the ground truth segmentation masks. Additionally, a self-supervised cosine embedding loss
is implemented to learn from the unlabeled data, encouraging similar model embeddings
for a given input image with two different augmentations applied. In Step 2, f′ is used
to generate pseudo labels for the unlabeled training set. Then, the pseudo labels and the
small set of manually labeled data are used to train the final segmentation network f. We
use the same losses described in Step 1, but compute the supervised cross entropy loss over
both the pseudo labels and the small set of manually labeled data.
4.2.2. Supervised Pixel-level Similarity Loss
We next build upon the previously proposed semi-supervised training approach by replacing
the self-supervised cosine embedding loss with a supervised similarity loss. Specifically, we
pull together the embeddings of positively-labeled pixels by computing the following loss:
−1
|B|
P
(xi,xj)∈B σ(xi, xj), where B is the set of positively-labeled pixels x1, . . . , x|B| in the
batch, and σ(xi, xj) = f(xi)⊤f(xj)
|f(xi)||f(xj)| is the cosine similarity between pixel embeddings for the
segmentation encoder f(·).
This alteration is also motivated by our theoretical insights in Section 3.3, where we ob-
served that spatial specificity improves robustness due to the reduced variance of positively-
labeled pixels. By using a supervised similarity loss among positively-labeled pixels instead
of the self-supervised similarity loss over all pixels (as is done in the image-level similar-
ity loss), we modify the training loss to optimize explicitly for reduced variance in the
embedding space of positively-labeled pixels.
5. Experiments on Medical Image Classification
We empirically validate our claim that increasing spatial specificity reduces reliance on spu-
rious features (Section 5.1) on two real-world medical image classification tasks. Further,
we explore the cost-robustness trade-off (Section 5.2). Details on model architecture and
11

Reducing Reliance on Spurious Features with Spatial Specificity
training for all models can be found in the Appendix (Section A.1 and A.2).
Medical image classification tasks
• Pneumothorax classification (CXR). We consider the task of classifying the pres-
ence of a pneumothorax from a CXR, where chest tubes are highly correlated with
pneumothorax. We use the SIIM-ACR Pneumothorax dataset (for Imaging Informat-
ics in Medicine, 2019), which consists of 12,047 chest X-rays, 2,669 of which contain
a pneumothorax. The dataset is also accompanied with expert-labeled segmentation
masks, allowing us to investigate tasks with varying levels of spatial specificity. We
randomly split the dataset to train (10,096), validation (951), and test (1,000) sets
while preserving class balance.
• Melanoma classification (ISIC). We consider the task of classifying benign versus
melanoma skin lesions from dermoscopic images. The dataset consists of 2,594 images
with segmentation masks provided by the ISIC 2018 challenge (Codella et al., 2019).
However, we use the train (1,822), validation (259), and test (513) splits provided
by Bissoto et al. (2020), which amplify correlations with 7 spurious features in the
train set: dark corners, hair, gel borders, gel bubbles, rulers, ink markings/staining,
and patches. The test sets provided reverse the spurious correlations. Therefore, the
robust and overall AUROC are equivalent for the ISIC dataset.
5.1. Increasing Spatial Specificity Improves Robustness to Spurious Features
We hypothesize that models trained with increased spatial specificity are more robust to
spurious features. To measure model robustness, we evaluate the AUROC on the subset
of the test set where the spurious correlation does not hold, e.g., X-rays that do not have
a pneumothorax but include a chest tube and X-rays that have a pneumothorax but do
not include a chest tube, which we term the “robust AUROC.” We also assess the overall
AUROC over the entire test set.
To validate our hypothesis, we train an empirical risk minimization (ERM) model with
increasing levels of spatial specificity, from binary labels, to bounding boxes, to segmentation
masks, and report the average AUROC and standard deviation over three random seeds.
In Table 1 (and Figure 5 in Appendix A.3), we show how the robust AUROC changes with
increasing spatial specificity for both the CXR and ISIC classification tasks. We find that
increasing spatial specificity greatly improves robustness to chest tubes in both tasks. While
the CXR binary ERM model achieves near random performance (robust AUROC of 0.46),
the bounding box model achieves an average robust AUROC of 0.72, and the segmentation
model achieves an average robust AUROC of 0.82.
We also compare to baselines that can be used for any classification task, including
GEORGE (Sohoni et al., 2020) and JTT (Liu et al., 2021). These baselines are methods
designed to reduce reliance on spurious features using binary labels. While other works have
tackled the CXR chest tube problem before (Graf et al., 2020), their pipelines are designed
specifically for CXR chest tubes and not easily generalized to other classification problems,
like ISIC. As shown in Table 1, while robust optimization techniques are able to improve
robustness over Binary-ERM, they are significantly less robust compared to methods that
increase spatial specificity.
12

Reducing Reliance on Spurious Features with Spatial Specificity
CXR
ISIC
Specificity-Method
Overall AUROC
Robust AUROC
Robust AUROC
Binary-ERM
0.909 ± 0.017
0.455 ± 0.000
0.359 ± 0.016
Binary-JTT
0.926 ± 0.003
0.559 ± 0.024
0.444 ± 0.027
Binary-GEORGE
0.920 ± 0.009
0.634 ± 0.009
0.514 ± 0.012
BBox-ERM
0.924 ± 0.011
0.720 ± 0.000
0.305 ± 0.084
Segmentation-ERM
0.933 ± 0.004
0.820 ± 0.039
0.727 ± 0.016
Table 1: Robust performance on CXR and ISIC for baselines versus increasing levels of
spatial specificity.
All methods assume no knowledge of the spurious feature.
Average and standard deviation are reported across three random seeds.
The ISIC dataset is a more challenging task since it involves multiple spurious features,
some of which have complete overlap with the skin lesion (e.g., hair). We still find that
training with the highest spatial specificity (i.e., segmentation) achieves significant robust-
ness gains over training with binary labels (see Table 1). To dig deeper into which spuri-
ous features the segmentation model relies upon the most, we analyzed the gap in model
performance between test samples with and without each individual spurious feature. In-
terestingly, we found that spurious features with large performance gaps corresponded to
the spurious features that generally have the largest spatial overlap with the skin lesions.
These included hair (always overlapped), gel bubbles (roughly 50% of images with gel bub-
bles had bubbles that overlapped with the lesion), and ruler (roughly 20% of images with
ruler markings were within the segmentation mask). Conversely, the spurious feature with
the lowest performance gap was dark corners, which rarely overlapped with the skin lesions.
Interestingly, bounding box detection does not achieve robustness gains as seen in CXR.
From manually comparing the images with bounding boxes versus segmentation masks, we
found that the bounding boxes covered a significantly larger area of the image (quantita-
tively, they covered roughly 50% more of the image), and as a result, all spurious features
had significantly larger overlap with the bounding box as compared to the segmentation
mask. We therefore suspect that the bounding boxes in ISIC do not add enough spatial
information to provide robustness gains.
Finally, to verify our findings in Section 3, we measure the variance of positively-labeled
pixels for the varying levels of spatial specificity on the CXR and ISIC datasets. As expected
from our theoretical analysis, the variance measure decreases as we go from binary labels,
to boudning boxes, to segmentation (Table 3 in Appendix A.3).
5.2. Evaluating the Cost-Robustness Trade-off
While we see that increasing spatial specificity increases robust and overall AUROC, sup-
plying such spatial specificity also results in increased annotation costs (Section 4.1). In this
section, we explore methods for reducing the annotation cost of increased spatial specificity.
First, we sweep the number of bounding boxes and segmentation masks supplied during
training, including 600, 1100, 2200, and 4400 training samples. In Figure 4, we show how
the robust and overall AUROC changes with reduced number of training samples. We find
13

Reducing Reliance on Spurious Features with Spatial Specificity
(a)
600
800
1000
1200
1400
1600
1800
2000
2200
Number of Training Samples
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Robust AUROC
Seg-ERM
Seg-SSL
Seg-SPL
Binary-ERM
(b)
600
800
1000
1200
1400
1600
1800
2000
2200
Number of Training Samples
0.82
0.84
0.86
0.88
0.90
0.92
0.94
Overall AUROC
Figure 4: We measure the cost-robustness trade-off by plotting the (a) robust and (b) over-
all AUROC versus the number of training samples, and compare the trade-off
for a segmentation model trained with ERM (Seg-ERM), SSL with an unsuper-
vised image-level similarity loss (Seg-SSL), and supervised pixel-wise similarity
loss (Seg-SPL). Average values are plotted across three random seeds (standard
deviations can be found in Table 4 in Appendix A.3).
that even at the most limited number of training samples supplied (i.e., 600), both the
segmenation and bounding box models have higher robust AUROC than the binary ERM
model. This implies that even supplying a limited number of training samples with higher
spatial specificity can help models learn more robust features.
Next, we evaluate two alterations to the training method that are aimed at maintaining
high segmentation performance even with reduced number of labels: a previously proposed
semi-supervised method that relies on an image-level cosine embedding loss (Seg-SSL), and
our modified method that uses a supervised pixel-level cosine embedding loss (Seg-SPL)
(Section 4.2.2). Since semi-supervised learning has shown success in reducing the amount
of masks required to train a high performing segmentation model, we hypothesize that such
tools will be effective for improving the cost-robustness trade-off. Each of these methods are
also trained with varying number of training samples. As shown in Figure 4, Seg-SSL and
Seg-SPL provide a 5 point lift in robust performance compared to the naive segmentation
model when we have access to few labeled training samples. We also find that Seg-SSL
simultaneously provides a lift in overall performance as opposed to Seg-SPL, indicating an
advantage of utilizing unlabeled data.
6. Discussion
In this work, we investigated how increasing task spatial specificity reduces model reliance
on spurious features. We found that as labels contain more precise information about the
abnormality location, the informativeness of the spurious feature to the abnormality label
decreases, which in turn reduces the model’s reliance on the spurious feature.
We theoretically analyzed the impact of increasing spatial specificity through our spuri-
ous feature data model and developed two intuitive takeaways: increasing spatial specificity
changes the data by (1) reducing the variance of positively-labeled pixels, thus tighten-
14

Reducing Reliance on Spurious Features with Spatial Specificity
ing class clusters in embedding space and improving model generalization as a result, and
(2) reducing the mutual information between abnormal and spurious labels, which in turn
weakens the likelihood that a model will rely on the spurious feature.
We validated our claims on the relationship between spatial specificity and spurious
feature robustness on two medical image classification datasets with known spurious fea-
tures. While we found significant improvements in robust AUROC as we trained models
with bounding boxes or segmentation masks (e.g., increasing 36 robust AUROC points on
CXR and 37 AUROC points for ISIC), it is interesting to consider why there remains a
performance gap between robust and overall performance. One reason for this gap may be
the spatial overlap between the abnormal and spurious features. For example, since chest
tubes are inserted into the pneumothorax to release the trapped air, the tip of the tube can
overlap with the abnormal region. Even without any spatial overlap, the tube’s positioning
in the chest is indicative of the abnormality location and, as a result, the tube may still
be helpful in finding the abnormal location. For ISIC, spurious features such as hair are
almost completely overlapping with the abnormal location. Additionally, the presence of
the chest tube may also correlate with visual features of PMX that are easier to segment,
as the visual features of PMXs with and without a chest tube may differ. Since PMX with
a chest tube have been treated, they may represent more extreme cases of PMX which are
easier to segment in an image, resulting in higher performance for PMX with chest tubes.
While we studied how spatial specificity improves robustness to spurious features, we
also discussed how providing additional spatial information results in longer annotation
times, resulting in a cost-robustness trade-off. We thus explored multiple ways of decreasing
annotation costs. For example, one can supply coarser spatial specificity (e.g., bounding
boxes instead of segmentation masks); fewer total segmentation masks (e.g., supplying only
300 positive segmentation masks still improves robust AUROC over naive training by 20
points); or using self- or semi-supervised segmentation methods. The latter approach is
particularly promising in the broader context of machine learning research. Weakly- and
semi-supervised training methods are continually being developed and refined, and large
self-supervised models have recently shown to be promising backbones for fine tuning many
downstream tasks with limited labeled data. We expect these evolving tools to continually
reduce the annotation costs associated with providing additional spatial specificity in model
supervision, allowing increased spatial specificity to be more practical and standard.
This exploration was inspired by the growing body of work that identifies systematic
failure modes of healthcare ML models. To continue making progress at improving model
robustness, it’s critical we also continue rigorous auditing (Eyuboglu et al., 2022; Liu et al.,
2022) to achieve widespread trustworthy adoption of machine learning in healthcare.
Limitations.
While we have shown the benefits of spatial specificity towards spurious
feature robustness, increasing spatial specificity results in more costly annotations, and
struggles to overcome spurious features that have high overlap with the abnormal region
(e.g., dataset shifts that are uniformly spread across the image).
15

Reducing Reliance on Spurious Features with Spatial Specificity
References
Faruk Ahmed, Yoshua Bengio, Harm van Seijen, and Aaron Courville. Systematic gen-
eralisation with group invariant predictions. In International Conference on Learning
Representations, 2020.
Mohsan Alvi, Andrew Zisserman, and Christoffer Nell˚aker. Turning a blind eye: Explicit
removal of biases and variation from deep neural network embeddings. In Proceedings of
the European Conference on Computer Vision (ECCV) Workshops, 2018.
Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz.
Invariant risk
minimization. arXiv preprint arXiv:1907.02893, 2019.
Marcus A Badgeley, John R Zech, Luke Oakden-Rayner, Benjamin S Glicksberg, Manway
Liu, William Gale, Michael V McConnell, Bethany Percha, Thomas M Snyder, and Joel T
Dudley. Deep learning predicts hip fracture using confounding patient and healthcare
variables. NPJ digital medicine, 2(1):1–10, 2019.
Alceu Bissoto, Eduardo Valle, and Sandra Avila. Debiasing skin lesion datasets and models?
not so fast. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pages 740–741, 2020.
Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza,
David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti,
et al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the
international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019.
Elliot Creager, J¨orn-Henrik Jacobsen, and Richard Zemel. Environment inference for in-
variant learning. In International Conference on Machine Learning, pages 2189–2200.
PMLR, 2021.
Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection
selects shortcuts over signal. Nature Machine Intelligence, 3(7):610–619, 2021.
Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-
Messer, Jared Dunnmon, James Zou, and Christopher R´e. Domino: Discovering sys-
tematic errors with cross-modal embeddings. arXiv preprint arXiv:2203.14960, 2022.
Society for Imaging Informatics in Medicine.
Siim-acr pneumothorax segmentation.
https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation, 2019.
Karan Goel, Albert Gu, Yixuan Li, and Christopher R´e.
Model patching: Closing the
subgroup performance gap with data augmentation. arXiv preprint arXiv:2008.06775,
2020.
Benedikt Graf, Arkadiusz Sitek, Amin Katouzian, Yen-Fu Lu, Arun Krishnan, Justin Rafael,
Kirstin Small, and Yiting Xie. Pneumothorax and chest tube classification on chest x-rays
for detection of missed pneumothorax. arXiv preprint arXiv:2011.07353, 2020.
16

Reducing Reliance on Spurious Features with Spatial Specificity
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for
image recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition, 2016.
Sarah Hooper, Sen Wu, Rhodri Davies, James Moon, Peter Kellman, Hui Xue, Curtis Lan-
glotz, and Christopher. R´e. Speeding up cardiac mr segmentation with semi-supervision:
applications in cine imaging. In Artificial Intelligence in CMR, Joint Summit of EACVI
and SCMR, 2022.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 7132–7141, 2018.
Sarah Jabbour, David Fouhey, Ella Kazerooni, Michael W Sjoding, and Jenna Wiens. Deep
learning applied to chest x-rays: Exploiting and preventing shortcuts. In Machine Learn-
ing for Healthcare Conference, pages 750–782. PMLR, 2020.
Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning
not to learn: Training deep neural networks with biased data.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9012–9020,
2019.
Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for
distributionally robust optimization. Advances in Neural Information Processing Systems,
33:8847–8860, 2020.
Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori
Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness
without training group information. In International Conference on Machine Learning,
pages 6781–6792. PMLR, 2021.
Xiaoxuan Liu, Ben Glocker, Melissa M McCradden, Marzyeh Ghassemi, Alastair K Den-
niston, and Lauren Oakden-Rayner. The medical algorithmic audit. The Lancet Digital
Health, 2022.
Luyang Luo, Dunyuan Xu, Hao Chen, Tien-Tsin Wong, and Pheng-Ann Heng. Pseudo bias-
balanced learning for debiased chest x-ray classification. arXiv preprint arXiv:2203.09860,
2022.
Roman C Maron, Achim Hekler, Eva Krieghoff-Henning, Max Schmitt, Justin G Schlager,
Jochen S Utikal, and Titus J Brinker. Reducing the impact of confounding factors on skin
cancer classification via image segmentation: technical model study. Journal of Medical
Internet Research, 23(3):e21695, 2021.
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from
failure: De-biasing classifier from biased classifier. In Advances in Neural Information
Processing Systems, volume 33, pages 20673–20684, 2020.
Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher R´e. Hidden
stratification causes clinically meaningful failures in machine learning for medical imaging.
17

Reducing Reliance on Spurious Features with Spatial Specificity
In Proceedings of the ACM conference on health, inference, and learning, pages 151–159,
2020.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedical image segmentation. In International Conference on Medical image computing
and computer-assisted intervention, pages 234–241. Springer, 2015.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally
robust neural networks for group shifts: On the importance of regularization for worst-
case generalization. arXiv preprint arXiv:1911.08731, 2019.
Jarrel CY Seah, Cyril HM Tang, Quinlan D Buchlak, Xavier G Holt, Jeffrey B Wardman,
Anuar Aimoldin, Nazanin Esmaili, Hassan Ahmad, Hung Pham, John F Lambert, et al.
Effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpre-
tation by radiologists: a retrospective, multireader multicase study. The Lancet Digital
Health, 3(8):e496–e506, 2021.
Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R´e. No sub-
class left behind: Fine-grained robustness in coarse-grained classification problems. Ad-
vances in Neural Information Processing Systems, 33:19339–19352, 2020.
Saeid A Taghanaki, Kristy Choi, Amir Hosein Khasahmadi, and Anirudh Goyal. Robust
representation learning via perceptual similarity metrics. In International Conference on
Machine Learning, pages 10043–10053. PMLR, 2021.
Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey N Chiang, Zhihao Wu, and Xiaowei
Ding. Embracing imperfect datasets: A review of deep learning solutions for medical
image segmentation. Medical Image Analysis, 63:101693, 2020.
Joseph D Viviano, Becks Simpson, Francis Dutil, Yoshua Bengio, and Joseph Paul Cohen.
Saliency is a possible red herring when diagnosing poor generalization. arXiv preprint
arXiv:1910.00199, 2019.
Jenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-
Velez, Kenneth Jung, Katherine Heller, David Kale, Mohammed Saeed, et al. Do no
harm: a roadmap for responsible machine learning for health care. Nature medicine, 25
(9):1337–1340, 2019.
Julia K Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein,
Rainer Hofmann-Wellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz,
et al. Association between surgical skin markings in dermoscopic images and diagnostic
performance of a deep learning convolutional neural network for melanoma recognition.
JAMA dermatology, 155(10):1135–1141, 2019.
Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1492–1500, 2017.
Pavel Yakubovskiy.
Segmentation models pytorch.
https://github.com/qubvel/
segmentation_models.pytorch, 2020.
18

Reducing Reliance on Spurious Features with Spatial Specificity
Darvin Yi. Improving Medical Image Segmentation by Designing Around Clinical Context.
Stanford University, 2020.
John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and
Eric Karl Oermann. Variable generalization performance of a deep learning model to
detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11):
e1002683, 2018.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with
adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,
and Society, pages 335–340, 2018.
Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christopher R´e.
Correct-n-contrast: A contrastive approach for improving robustness to spurious correla-
tions. arXiv preprint arXiv:2203.01517, 2022.
19

Reducing Reliance on Spurious Features with Spatial Specificity
Appendix A. Additional Empirical Support and Details
We provide additional details on the training procedure for the binary models, bounding
box models, and segmentation models (Section A.1), as well as for baselines GEORGE
(Sohoni et al., 2020) and JTT (Liu et al., 2021) (Section A.2). We also include additional
empirical results that were left out of the main body (Section A.3).
A.1. Model training details for binary ERM and spatial specificity models
To train models with binary labels, we use a ResNet-50 architecture (He et al., 2016) with
a learning rate of 0.00001, and checkpoint on the AUROC over the validation set. To train
the segmentation models, we use a UNet architecture (Ronneberger et al., 2015) with a
ResNeXt50 (32x4d) encoder (Xie et al., 2017) with a Squeeze-and-Excitation module (Hu
et al., 2018) with pretrained ImageNet weights; the architecture and weights are available
in the segmentation-models-pytorch Python package (Yakubovskiy, 2020). We trained the
model with a learning rate of 0.0005, and apply data augmentations during model training
(affine transforms, elastic transforms, brightness jitter, contrast jitter). We checkpoint on
the mean Dice coefficient over the validation set. To train the bounding box models, we
follow the same procedure as the segmentation models; however, the positively-labeled pixels
in the supervision mask reflect bounding boxes (e.g., Fig. 1). All models were trained in
Pytorch using a cross entropy loss and an Adam optimizer for 150 epochs. For segmentation
models trained with the pixel-level similarity loss, we additionally tuned the weight on the
similarity loss (relative to the cross entropy loss) that maximizes validation dice score. The
average time for a single epoch on an NVIDIA Titan RTX GPU was ∼2 minutes.
A.2. Model training details for binary baselines
For both GEORGE and JTT, we train with the same initial ERM model. For CXR, the
ERM model is an ImageNet-pretrained ResNet-50 (RN50), and we train with 30 epochs,
batch size 32, SGD optimizer, learning rate 0.0001, momentum 0.9, and weight decay 0.1.
For ISIC, the ERM model is an ImageNet-pretrained RN50, and we train with 15 epochs,
batch size 32, SGD optimizer, learning rate 0.0001, momentum 0.9, and weight decay 0.1.
We also balance the ground-truth classes.
For CXR, we initialize the GEORGE model
as an ImageNet-pretrained RN50, and train with SGD optimizer, learning rate 0.0001,
weight decay 0.001, gamma 0.1, momentum 0.9, batch size 32, generalization adjustment
parameter 3, and 100 epochs.
For CXR, we initialize the JTT model as an ImageNet-
pretrained RN50, and train with SGD optimizer, learning rate 0.0001, weight decay 0,
momentum 0.9, and batch size 32. We upsample datapoints such that each inferred group’s
size matches the largest inferred group’s size. For ISIC, we initialize the GEORGE model as
an ImageNet-pretrained RN50, and train with SGD optimizer, learning rate 0.00001, weight
decay 0.00001, gamma 0.1, momentum 0.9, batch size 32, generalization adjustment param-
eter 3, and 100 epochs. For ISIC, we initialize the JTT model as an ImageNet-pretrained
RN50, and train with SGD optimizer, learning rate 0.0001, weight decay 0, momentum
0.9, and batch size 32. We also upsample datapoints such that each inferred group’s size
matches the largest inferred group’s size.
20

Reducing Reliance on Spurious Features with Spatial Specificity
A.3. Additional empirical support
Avg. Cos. Sim. across Subgroups / r
1
2
4
20
40
(Y=0, S = 0) ⇔(Y=0, S = 1)
0.13
0.39
0.66
0.80
0.89
(Y=1, S = 0) ⇔(Y=1, S = 1)
0.62
0.57
0.60
0.93
0.92
Table 2: We find that as we increase spatial specificity the average cosine similarity between
the subgroups of the same class but different spurious label increases, signifying
weaker dependence on the spurious feature.
Spatial Specificity
Binary
Bounding Box
Segmentation
CXR
0.99
0.47
0.42
ISIC
1.28
1.14
0.88
Table 3: We verify that the proposed variance measure decreases with increased spatial
specificity on our two medical image datasets.
Binary
BBox
Segmentation
Spatial Specificity
0.3
0.4
0.5
0.6
0.7
0.8
Robust AUROC
CXR
ISIC
Figure 5: Robust performance at three levels of increasing spatial specificity: binary, bound-
ing box, and segmentation, for CXR and ISIC. We report the average performance
across three random seeds (standard deviations and baselines are in Table 1).
21

Reducing Reliance on Spurious Features with Spatial Specificity
# Training Samples
600 (8×)
1100 (4×)
2200 (2×)
Seg-ERM
0.656 ± 0.038
0.701 ± 0.007
0.749 ± 0.018
BBox-ERM
0.654 ± 0.035
0.685 ± 0.022
0.715 ± 0.043
Seg-SSL
0.679 ± 0.016
0.723 ± 0.003
0.741 ± 0.009
Seg-SPL
0.697 ± 0.008
0.704 ± 0.004
0.738 ± 0.008
Table 4: Robust performance on the CXR dataset as we vary the number of training samples
for segmentation (Seg-ERM), bounding box (BBox-ERM), segmentation with SSL
(Seg-SSL), and segmentation with pixel-wise similarity loss (Seg-SPL). We report
the average robust performance and standard deviation over three random runs.
Appendix B. Proofs
We provide the proofs of Proposition 2 (Section B.1) and Proposition 4 (Section B.2).
B.1. Proof of Proposition 2
We first explain the extreme values of our variance measure. When r = 1, ˜Yij = 1 means that
we receive a binary label of 1 over the image. Then, Vari,j[Xij| ˜Yij = 1] = Vari,j[Xij|y = 1],
since we are conditioning on a single variable over the entire image. When r = n, ˜Yij = 1
means that Yij = 1, so our variance measure is equivalent to Vari,j[Xij|Yij = 1] (note that
the condition y = 1 is implied here).
The variance measure can be written as Vari,j[Xij| ˜Yij = 1] = E[X2
ij| ˜Yij = 1]−E[Xij| ˜Yij=1]2.
From our synthetic, we have that
E[X2
ij| ˜Yij = 1]= Pr(Yij = 0, Sij = 0| ˜Yij = 1)σ2 + Pr(Yij = 1, Sij = 0| ˜Yij = 1)(σ2 + µ2
a) (1)
+ Pr(Yij = 0, Sij = 1| ˜Yij = 1)(σ2 + µ2
s)
=σ2 + Pr(Yij = 1, Sij = 0| ˜Yij = 1)µ2
a + Pr(Yij = 0, Sij = 1| ˜Yij = 1)µ2
s,
and
E[Xij| ˜Yij = 1] = Pr(Yij = 1, Sij = 0| ˜Yij = 1)µa + Pr(Yij = 0, Sij = 1| ˜Yij = 1)µs.
(2)
Next, we simplify the probabilities in the above expressions. By the chain rule and the
conditional independence of Sij and ˜Yij given Yij, we have
Pr(Yij = 0, Sij = 1| ˜Yij = 1) = Pr(Sij = 1|Yij = 0) Pr(Yij = 0| ˜Yij = 1).
(3)
Since Yij = 1 implies ˜Yij = 1 and the abnormality and spurious feature are nonoverlap-
ping, we have
Pr(Yij = 1, Sij = 0| ˜Yij = 1) = Pr(Sij = 0|Yij = 1, ˜Yij = 1) Pr(Yij = 1| ˜Yij = 1)
(4)
= Pr(Sij = 0|Yij = 1) Pr(Yij = 1| ˜Yij = 1)
= Pr(Yij = 1| ˜Yij = 1).
22

Reducing Reliance on Spurious Features with Spatial Specificity
We compute Pr(Yij = 1| ˜Yij = 1), which we denote as a function γ(r) of spatial specificity.
By construction of ˜Yij, this is equal to Pr(Yij=1)
Pr( ˜Yij=1). For our synthetic, the abnormal region
is a2 out of n2 pixels, and the points with positive label form a square with side length
min{n, a + 2(⌈n
r ⌉−1)}. Therefore, γ(r) = Pr(Yij = 1| ˜Yij = 1) =
a2
min{n,a+2(⌈n
r ⌉−1)}2 ∈[ a2
n2 , 1]
is increasing in r.
Define wij = Pr(Sij = 1|Yij = 0). Then,
E[X2
ij| ˜Yij = 1] = σ2 + γ(r)µ2
a + wij(1 −γ(r))µ2
s
(5)
E[Xij| ˜Yij = 1] = γ(r)µa + wij(1 −γ(r))µs.
(6)
Therefore,
Vari,j[Xij| ˜Yij = 1] = σ2 + µ2
aγ(r)(1 −γ(r)) + µ2
swij(1 −γ(r))(1 −wij(1 −γ(r)))
(7)
−2µaµswijγ(r)(1 −γ(r)).
The variance is quadratic in γ(r), the only variable that depends on spatial specificity.
Rearranging, we get that Vari,j[Xij| ˜Yij = 1] = −(µa −µswij)2γ(r)2 + ((µa −µswij)2 +
µ2
swij(wij−1))γ(r)+σ2+µ2
swij(1−wij). Now, we aim to understand when the variance mea-
sure strictly decreases in r. Vari,j[Xij| ˜Yij] is maximized by γ(r) = (µa−µswij)2−µ2
swij(1−wij)
2(µa−µswij)2
=
1
2 −wij(1−wij)
2

µs
µa−µswij
2
. Then, under the condition that
1
2 −wij(1 −wij)
2

µs
µa −µswij
2
≤γ(1),
(8)
the variance measure will be decreasing quadratically in r. We see that for large |µs|
and smaller |µa| (i.e., the spurious feature is very distinguishable from the background and
the abnormality is slightly distinguishable), this inequality can be satisfied.
As a verification, we plug in quantities from our synthetic setup. Recall that γ(1) =
Pr(Yij = 1) =
a2
n2 .
Furthermore, wij can be expressed as
b2
n2−a2 by construction of the
spurious and abnormal regions. We now plug in quantities n = 40, a = 8, b = 12 to get
that wij = 0.09375. Then, using µa = 2, µs = −10, the LHS is 0.01 which is less than
a2
n2 = 0.04. Since (8) is satisfied, our theoretical findings match with our synthetic results
in Figure 3.3.2 that the variance measure decreases with resolution.
B.2. Proof of Proposition 4
We first explain the extreme values of our MI measure. Using the same logic as in our proof
of Proposition 2, when r = 1 our measure becomes I(Sij; Yij) and when r = n our measure
becomes I(Sij; Yij|Yij = 1), which equals 1 because Yij is no longer random.
23

Reducing Reliance on Spurious Features with Spatial Specificity
Now we simplify I(Sij; Yij| ˜Yij = 1). We again use the conditional independence of Sij
and ˜Yij given Yij = 0 and the construction of ˜Yij via Yij to get:
I(Sij; Yij| ˜Yij = 1) =
X
Sij,Yij∈{0,1}
Pr(Sij, Yij| ˜Yij = 1) log
Pr(Sij, Yij| ˜Yij = 1)
Pr(Sij| ˜Yij = 1) Pr(Yij| ˜Yij = 1)
(9)
=
X
Sij,Yij∈{0,1}
Pr(Sij|Yij) Pr(Yij| ˜Yij = 1) log
Pr(Sij|Yij) Pr(Yij| ˜Yij = 1)
Pr(Sij| ˜Yij = 1) Pr(Yij| ˜Yij = 1)
=
X
Sij,Yij∈{0,1}
Pr(Sij|Yij) Pr(Yij| ˜Yij = 1) log
Pr(Sij|Yij)
Pr(Sij| ˜Yij = 1)
.
We now write out all combinations of (Sij, Yij). Recall that the spurious feature and
the abnormality are nonoverlapping.
I(Sij; Yij| ˜Yij = 1) = Pr(Sij = 1|Yij = 0)(1 −γ(r)) log Pr(Sij = 1|Yij = 0)
Pr(Sij = 1| ˜Yij = 1)
(10)
+ Pr(Sij = 0|Yij = 0)(1 −γ(r)) log Pr(Sij = 0|Yij = 0)
Pr(Sij = 0| ˜Yij = 1)
+ γ(r) log
1
Pr(Sij = 0| ˜Yij = 1)
.
From our previous proof, we define wij = Pr(Sij = 1|Yij = 0), γ(r) = Pr(Yij = 1| ˜Yij =
1). In addition, we can write Pr(Sij = 0| ˜Yij = 1) as
Pr(Sij = 0| ˜Yij = 1) = Pr(Sij = 0, Yij = 1| ˜Yij = 1) + Pr(Sij = 0, Yij = 0| ˜Yij = 1)
(11)
= Pr(Sij = 0|Yij = 1) Pr(Yij = 1| ˜Yij = 1) + Pr(Sij = 0|Yij = 0) Pr(Yij = 0| ˜Yij = 1)
= γ(r) + (1 −wij)(1 −γ(r)).
and similarly,
Pr(Sij = 1| ˜Yij = 1) = Pr(Sij = 1, Yij = 1| ˜Yij = 1) + Pr(Sij = 1, Yij = 0| ˜Yij = 1)
(12)
= Pr(Sij = 1|Yij = 1) Pr(Yij = 1| ˜Yij = 1) + Pr(Sij = 1|Yij = 0) Pr(Yij = 0| ˜Yij = 1)
= wij(1 −γ(r)).
Therefore, the MI measure is
I(Sij;Yij| ˜Yij = 1) =
(13)
wij(1 −γ(r)) log
1
1 −γ(r) + (1 −wij)(1 −γ(r)) log
1 −wij
γ(r) + (1 −wij)(1 −γ(r))
+ γ(r) log
1
γ(r) + (1 −wij)(1 −γ(r)).
We have written the MI measure as a function of γ(r). We make several observations
on this function; when γ(r) = 0 or 1, I(Sij; Yij| ˜Yij = 1) = 0. However, γ(r)’s minimum
value (when r = 1) is equal to Pr(Yij = 1) > 0.
24

Reducing Reliance on Spurious Features with Spatial Specificity
This function is also convex with a maximum at a point γ⋆that is bounded by γ⋆< 0.64
for any wij. As a result, for r0 satisfying γ(r0) ≥0.64, the MI measure is decreasing as the
spatial specificity increases starting from r0.
Therefore, if we sample spatial specificity at intervals starting from r = 1 to r = n, it is
possible to see a monotonic decrease in the MI measure if γ(1) is large enough; otherwise,
we may witness a slight increase in the MI measure followed by a decrease to 0. Therefore,
a high spatial specificity overall decreases the MI measure towards zero.
25

