Program Induction by Rationale Generation:
Learning to Solve and Explain Algebraic Word Problems
Wang Ling♠
Dani Yogatama♠
Chris Dyer♠
Phil Blunsom♠♦
♠DeepMind
♦University of Oxford
{lingwang,dyogatama,cdyer,pblunsom}@google.com
Abstract
Solving
algebraic
word
problems
re-
quires executing a series of arithmetic
operations—a program—to obtain a ﬁnal
answer.
However, since programs can
be arbitrarily complicated, inducing them
directly from question-answer pairs is a
formidable challenge. To make this task
more feasible, we solve these problems by
generating answer rationales, sequences
of natural language and human-readable
mathematical expressions that derive the
ﬁnal answer through a series of small
steps. Although rationales do not explic-
itly specify programs, they provide a scaf-
folding for their structure via intermedi-
ate milestones. To evaluate our approach,
we have created a new 100,000-sample
dataset of questions, answers and ratio-
nales. Experimental results show that in-
direct supervision of program learning via
answer rationales is a promising strategy
for inducing arithmetic programs.
1
Introduction
Behaving intelligently often requires mathemat-
ical reasoning.
Shopkeepers calculate change,
tax, and sale prices; agriculturists calculate the
proper amounts of fertilizers, pesticides, and wa-
ter for their crops; and managers analyze produc-
tivity. Even determining whether you have enough
money to pay for a list of items requires applying
addition, multiplication, and comparison. Solv-
ing these tasks is challenging as it involves rec-
ognizing how goals, entities, and quantities in the
real-world map onto a mathematical formaliza-
tion, computing the solution, and mapping the so-
lution back onto the world. As a proxy for the
richness of the real world, a series of papers have
used natural language speciﬁcations of algebraic
word problems, and solved these by either learn-
ing to ﬁll in templates that can be solved with
equation solvers (Hosseini et al., 2014; Kushman
et al., 2014) or inferring and modeling operation
sequences (programs) that lead to the ﬁnal an-
swer (Roy and Roth, 2015).
In this paper, we learn to solve algebraic word
problems by inducing and modeling programs that
generate not only the answer, but an answer ratio-
nale, a natural language explanation interspersed
with algebraic expressions justifying the overall
solution. Such rationales are what examiners re-
quire from students in order to demonstrate un-
derstanding of the problem solution; they play the
very same role in our task. Not only do natural
language rationales enhance model interpretabil-
ity, but they provide a coarse guide to the structure
of the arithmetic programs that must be executed.
In fact the learner we propose (which relies on a
heuristic search; §4) fails to solve this task with-
out modeling the rationales—the search space is
too unconstrained.
This work is thus related to models that can
explain or rationalize their decisions (Hendricks
et al., 2016; Harrison et al., 2017). However, the
use of rationales in this work is quite different
from the role they play in most prior work, where
interpretation models are trained to generate plau-
sible sounding (but not necessarily accurate) post-
hoc descriptions of the decision making process
they used. In this work, the rationale is generated
as a latent variable that gives rise to the answer—it
is thus a more faithful representation of the steps
used in computing the answer.
This paper makes three contributions. First, we
have created a new dataset with more than 100,000
algebraic word problems that includes both an-
swers and natural language answer rationales (§2).
Figure 1 illustrates three representative instances
arXiv:1705.04146v3  [cs.AI]  23 Oct 2017

Problem 1:
Question: Two trains running in opposite directions cross a
man standing on the platform in 27 seconds and 17 seconds
respectively and they cross each other in 23 seconds. The ratio
of their speeds is:
Options: A) 3/7 B) 3/2 C) 3/88 D) 3/8 E) 2/2
Rationale: Let the speeds of the two trains be x m/sec and y
m/sec respectively. Then, length of the ﬁrst train = 27x meters,
and length of the second train = 17 y meters. (27x + 17y) / (x +
y) = 23 →27x + 17y = 23x + 23y →4x = 6y →x/y = 3/2.
Correct Option: B
Problem 2:
Question: From a pack of 52 cards, two cards are drawn to-
gether at random. What is the probability of both the cards
being kings?
Options: A) 2/1223 B) 1/122 C) 1/221 D) 3/1253 E) 2/153
Rationale: Let s be the sample space.
Then n(s) = 52C2 = 1326
E = event of getting 2 kings out of 4
n(E) = 4C2 = 6
P(E) = 6/1326 = 1/221
Answer is C
Correct Option: C
Problem 3:
Question: For which of the following does p(a)−p(b) = p(a−
b) for all values of a and b?
Options:A) p(x) = x2, B) p(x) = x/2, C) p(x) = x + 5, D)
p(x) = 2x1, E) p(x) = |x|
Rationale: To solve this easiest way is just put the value and
see that if it equals or not.
with option A. p(a) = a2 and p(b) = b2
so L.H.S = a2 −b2
and R.H.S = (a −b)2 →a2 + b2 −2ab.
so L.H.S not equal to R.H.S
with option B. p(a) = a/2 and p(b) = b/2
L.H.S = a/2 −b/2 →1/2(a −b)
R.H.S = (a −b)/2
so L.H.S = R.H.S which is the correct answer.
answer:B
Correct Option: B
Figure 1: Examples of solved math problems.
from the dataset. Second, we propose a sequence
to sequence model that generates a sequence of in-
structions that, when executed, generates the ra-
tionale; only after this is the answer chosen (§3).
Since the target program is not given in the train-
ing data (most obviously, its speciﬁc form will de-
pend on the operations that are supported by the
program interpreter); the third contribution is thus
a technique for inferring programs that generate a
rationale and, ultimately, the answer. Even con-
strained by a text rationale, the search space of
possible programs is quite large, and we employ
a heuristic search to ﬁnd plausible next steps to
guide the search for programs (§4). Empirically,
we are able to show that state-of-the-art sequence
to sequence models are unable to perform above
chance on this task, but that our model doubles the
accuracy of the baseline (§6).
2
Dataset
We built a dataset1 with 100,000 problems with
the annotations shown in Figure 1. Each question
is decomposed in four parts, two inputs and two
outputs: the description of the problem, which we
will denote as the question, and the possible (mul-
tiple choice) answer options, denoted as options.
Our goal is to generate the description of the ratio-
nale used to reach the correct answer, denoted as
rationale and the correct option label. Problem
1 illustrates an example of an algebra problem,
which must be translated into an expression (i.e.,
(27x + 17y)/(x + y) = 23) and then the desired
quantity (x/y) solved for. Problem 2 is an exam-
ple that could be solved by multi-step arithmetic
operations proposed in (Roy and Roth, 2015). Fi-
nally, Problem 3 describes a problem that is solved
by testing each of the options, which has not been
addressed in the past.
2.1
Construction
We ﬁrst collect a set of 34,202 seed problems that
consist of multiple option math questions covering
a broad range of topics and difﬁculty levels. Ex-
amples of exams with such problems include the
GMAT (Graduate Management Admission Test)
and GRE (General Test). Many websites contain
example math questions in such exams, where the
answer is supported by a rationale.
Next, we turned to crowdsourcing to generate
new questions. We create a task where users are
presented with a set of 5 questions from our seed
dataset. Then, we ask the Turker to choose one
of the questions and write a similar question. We
also force the answers and rationale to differ from
the original question in order to avoid paraphrases
of the original question. Once again, we manually
check a subset of the jobs for each Turker for qual-
ity control. The type of questions generated us-
ing this method vary. Some turkers propose small
changes in the values of the questions (e.g., chang-
ing the equality p(a) −p(b) = p(a −b) in Prob-
lem 3 to a different equality is a valid question, as
long as the rationale and options are rewritten to
reﬂect the change). We designate these as replica
problems as the natural language used in the ques-
tion and rationales tend to be only minimally un-
altered. Others propose new problems in the same
topic where the generated questions tend to dif-
1Available at https://github.com/deepmind/
AQuA

Question
Rationale
Training Examples
100,949
Dev Examples
250
Test Examples
250
Numeric
Average Length
9.6
16.6
Vocab Size
21,009
14,745
Non-Numeric
Average Length
67.8
89.1
Vocab Size
17,849
25,034
All
Average Length
77.4
105.7
Vocab Size
38,858
39,779
Table 1: Descriptive statistics of our dataset.
fer more radically from existing ones. Some Turk-
ers also copy math problems available on the web,
and we deﬁne in the instructions that this is not
allowed, as it will generate multiple copies of the
same problem in the dataset if two or more Turkers
copy from the same resource. These Turkers can
be detected by checking the nearest neighbours
within the collected datasets as problems obtained
from online resources are frequently submitted by
more than one Turker. Using this method, we ob-
tained 70,318 additional questions.
2.2
Statistics
Descriptive statistics of the dataset is shown in
Figure 1. In total, we collected 104,519 problems
(34,202 seed problems and 70,318 crowdsourced
problems). We removed 500 problems as heldout
set (250 for development and 250 for testing). As
replicas of the heldout problems may be present in
the training set, these were removed manually by
listing for each heldout instance the closest prob-
lems in the training set in terms of character-based
Levenstein distance. After ﬁltering, 100,949 prob-
lems remained in the training set.
We also show the average number of tokens (to-
tal number of tokens in the question, options and
rationale) and the vocabulary size of the questions
and rationales. Finally, we provide the same statis-
tics exclusively for tokens that are numeric values
and tokens that are not.
Figure 2 shows the distribution of examples
based on the total number of tokens. We can see
that most examples consist of 30 to 500 tokens, but
there are also extremely long examples with more
than 1000 tokens in our dataset.
3
Model
Generating rationales for math problems is chal-
lenging as it requires models that learn to per-
form math operations at a ﬁner granularity as
0
200
400
600
800
1000
0
1000
2000
3000
frequency
length
Figure 2: Distribution of examples per length.
each step within the solution must be explained.
For instance, in Problem 1, the equation (27x +
17y)/(x + y) = 23 must be solved to obtain
the answer.
In previous work (Kushman et al.,
2014), this could be done by feeding the equation
into an expression solver to obtain x/y = 3/2.
However, this would skip the intermediate steps
27x+17y = 23x+23y and 4x = 6y, which must
also be generated in our problem. We propose a
model that jointly learns to generate the text in the
rationale, and to perform the math operations re-
quired to solve the problem. This is done by gener-
ating a program, containing both instructions that
generate output and instructions that simply gener-
ate intermediate values used by following instruc-
tions.
3.1
Problem Deﬁnition
In
traditional
sequence
to
sequence
mod-
els (Sutskever et al., 2014; Bahdanau et al.,
2014), the goal is to predict the output sequence
y
=
y1, . . . , y|y| from the input sequence
x = x1, . . . , x|x|, with lengths |y| and |x|.
In our particular problem, we are given the
problem and the set of options, and wish to pre-
dict the rationale and the correct option. We set x
as the sequence of words in the problem, concate-
nated with words in each of the options separated
by a special tag. Note that knowledge about the
possible options is required as some problems are
solved by the process of elimination or by testing
each of the options (e.g. Problem 3). We wish to
generate y, which is the sequence of words in the
rationale. We also append the correct option as the
last word in y, which is interpreted as the chosen
option. For example, y in Problem 1 is “Let the
. . . = 3/2 . ⟨EOR⟩B ⟨EOS⟩”, whereas in Problem
2 it is “Let s be . . . Answer is C ⟨EOR⟩C ⟨EOS⟩”,
where “⟨EOS⟩” is the end of sentence symbol and

i
x
z
v
r
1
From
Id(“Let”)
Let
y1
2
a
Id(“s”)
s
y2
3
pack
Id(“be”)
be
y3
4
of
Id(“the”)
the
y4
5
52
Id(“sample”)
sample
y5
6
cards
Id(“space”)
space
y6
7
,
Id(“.”)
.
y7
8
two
Id(“\n”)
\n
y8
9
cards
Id(“Then”)
Then
y9
10
are
Id(“n”)
n
y10
11
drawn
Id(“(”)
(
y11
12
together
Id(“s”)
s
y12
13
at
Id(“)”)
)
y13
14
random
Id(“=”)
=
y14
15
.
Str to Float(x5)
52
m1
16
What
Float to Str(m1)
52
y15
17
is
Id(“C”)
C
y16
18
the
Id(“2”)
2
y17
19
probability
Id(“=”)
=
y18
20
of
Str to Float(y17)
2
m2
21
both
Choose(m1,m2)
1326
m3
22
cards
Float to Str(m3)
1326
y19
23
being
Id(“E”)
E
y20
24
kings
Id(“=”)
=
y21
25
?
Id(“event”)
event
y22
26
<O>
Id(“of”)
of
y23
27
A)
Id(“getting”)
getting
y24
28
2/1223
Id(“2”)
2
y25
29
<O>
Id(“kings”)
kings
y26
30
B)
Id(“out”)
out
y27
31
1/122
Id(“of”)
of
y28
. . .
. . .
...
...
. . .
|z|
Id(“⟨EOS⟩”)
⟨EOS⟩
y|y|
Table 2: Example of a program z that would gen-
erate the output y. In v, italics indicates string
types; bold indicates ﬂoat types. Refer to §3.3 for
description of variable names.
“⟨EOR⟩” is the end of rationale symbol.
3.2
Generating Programs to Generate
Rationales
We wish to generate a latent sequence of program
instructions, z = z1, . . . , z|z|, with length |z|,
that will generate y when executed.
We express z as a program that can access x, y,
and the memory buffer m. Upon ﬁnishing execu-
tion we expect that the sequence of output tokens
to be placed in the output vector y.
Table 2 illustrates an example of a sequence of
instructions that would generate an excerpt from
Problem 2, where columns x, z, v, and r denote
the input sequence, the instruction sequence (pro-
gram), the values of executing the instruction, and
where each value vi is written (i.e., either to the
output or to the memory). In this example, instruc-
tions from indexes 1 to 14 simply ﬁll each position
with the observed output y1, . . . , y14 with a string,
where the Id operation simply returns its parame-
ter without applying any operation. As such, run-
ning this operation is analogous to generating a
word by sampling from a softmax over a vocabu-
lary. However, instruction z15 reads the input word
x5, 52, and applies the operation Str to Float,
which converts the word 52 into a ﬂoating point
number, and the same is done for instruction z20,
which reads a previously generated output word
y17.
Unlike, instructions z1, . . . , z14, these op-
erations write to the external memory m, which
stores intermediate values. A more sophisticated
instruction—which shows some of the power of
our model—is z21 = Choose(m1, m2) →m3
which evaluates
 m1
m2

and stores the result in m3.
This process repeats until the model generates the
end-of-sentence symbol. The last token of the pro-
gram as said previously must generate the correct
option value, from “A” to “E”.
By training a model to generate instructions that
can manipulate existing tokens, the model ben-
eﬁts from the additional expressiveness needed
to solve math problems within the generation
process.
In total we deﬁne 22 different oper-
ations, 13 of which are frequently used opera-
tions when solving math problems.
These are:
Id, Add, Subtract, Multiply, Divide,
Power, Log, Sqrt, Sine, Cosine, Tangent,
Factorial, and Choose (number of combi-
nations).
We also provide 2 operations to con-
vert between Radians and Degrees, as these
are needed for the sine, cosine and tangent opera-
tions. There are 6 operations that convert ﬂoating
point numbers into strings and vice-versa. These
include the Str to Float and Float to Str
operations described previously, as well as opera-
tions which convert between ﬂoating point num-
bers and fractions, since in many math problems
the answers are in the form “3/4”. For the same
reason, an operation to convert between a ﬂoat-
ing point number and number grouped in thou-
sands is also used (e.g. 1000000 to “1,000,000”
or “1.000.000”).
Finally, we deﬁne an opera-
tion (Check) that given the input string, searches
through the list of options and returns a string with
the option index in {“A”, “B”, “C”, “D”, “E”}. If
the input value does not match any of the options,
or more than one option contains that value, it can-
not be applied. For instance, in Problem 2, once
the correct probability “1/221” is generated, by ap-
plying the check operation to this number we can

hi
softmax
oi
ri
softmax
ri
qi,j=1
softmax
qij
softmax
copy 
input
aij
qi,j+1
copy 
output
hi+1
j < argc(oi)?
vi
execute
Figure 3: Illustration of the generation process of
a single instruction tuple at timestamp i.
obtain correct option “C”.
3.3
Generating and Executing Instructions
In our model, programs consist of sequences of
instructions, z. We turn now to how we model
each zi, conditional on the text program speciﬁ-
cation, and the program’s history. The instruction
zi is a tuple consisting of an operation (oi), an or-
dered sequence of its arguments (ai), and a deci-
sion about where its results will be placed (ri) (is
it appended in the output y or in a memory buffer
m?), and the result of applying the operation to its
arguments (vi). That is, zi = (oi, ri, ai, vi).
Formally, oi is an element of the pre-speciﬁed
set of operations O, which contains, for example
add, div, Str to Float, etc. The number of
arguments required by oi is given by argc(oi), e.g.,
argc(add) = 2 and argc(log) = 1. The argu-
ments are ai = ai,1, . . . , ai,argc(oi). An instruc-
tion will generate a return value vi upon execution,
which will either be placed in the output y or hid-
den. This decision is controlled by ri. We deﬁne
the instruction probability as:
p(oi, ai, ri,vi | z<i, x, y, m) =
p(oi | z<i, x) × p(ri | z<i, x, oi)×
argc(oi)
Y
j=1
p(ai,j | z<i, x, oi, m, y)×
[vi = apply(oi, a)],
where [p] evaluates to 1 if p is true and 0 otherwise,
and apply(f, x) evaluates the operation f with ar-
guments x. Note that the apply function is not
learned, but pre-deﬁned.
The network used to generate an instruction at
a given timestamp i is illustrated in Figure 3. We
ﬁrst use the recurrent state hi to generate p(oi |
z<i, x) = softmax
oi∈O
(hi), using a softmax over the
set of available operations O.
In order to predict ri, we generate a new hid-
den state ri, which is a function of the current pro-
gram context hi, and an embedding of the cur-
rent predicted operation, oi.
As the output can
either be placed in the memory m or the output
y, we compute the probability p(ri = OUTPUT |
z<i, x, oi) = σ(ri · wr + br), where σ is the lo-
gistic sigmoid function. If ri = OUTPUT, vi is
appended to the output y; otherwise it is appended
to the memory m.
Once we generate ri, we must predict ai, the
argc(oi)-length sequence of arguments that oper-
ation oi requires. The jth argument ai,j can be
either generated from a softmax over the vocab-
ulary, copied from the input vector x, or copied
from previously generated values in the output
y or memory m. This decision is modeled us-
ing a latent predictor network (Ling et al., 2016),
where the control over which method used to gen-
erate ai,j is governed by a latent variable qi,j ∈
{SOFTMAX, COPY-INPUT, COPY-OUTPUT}. Sim-
ilar to when predicting ri, in order to make this
choice, we also generate a new hidden state for
each argument slot j, denoted by qi,j with an
LSTM, feeding the previous argument in at each
time step, and initializing it with ri and by reading
the predicted value of the output ri.
• If qi,j = SOFTMAX, ai,j is generated by sam-
pling from a softmax over the vocabulary Y,
p(ai,j | qi,j = SOFTMAX) = softmax
ai,j∈Y (qi,j).
This corresponds to a case where a string is used
as argument (e.g. y1=“Let”).
• If qi,j = COPY-INPUT, ai,j is obtained by copy-
ing an element from the input vector with a
pointer network (Vinyals et al., 2015) over input
words x1, . . . , x|x|, represented by their encoder
LSTM state u1, . . . , u|x|. As such, we compute
the probability distribution over input words as:
p(ai,j | qi,j =COPY-INPUT) =
(1)
softmax
ai,j∈x1,...,x|x|
 f(uai,j, qi,j)

Function f computes the afﬁnity of each to-
ken xai,j and the current output context qi,j. A
common implementation of f, which we follow,
is to apply a linear projection from [uai,j; qi,j]

into a ﬁxed size vector (where [u; v] is vector
concatenation), followed by a tanh and a linear
projection into a single value.
• If qi,j = COPY-OUTPUT, the model copies from
either the output y or the memory m. This is
equivalent to ﬁnding the instruction zi, where
the value was generated. Once again, we de-
ﬁne a pointer network that points to the output
instructions and deﬁne the distribution over pre-
viously generated instructions as:
p(ai,j | qi,j =COPY-OUTPUT) =
softmax
ai,j∈z1,...,zi−1
 f(hai,j, qi,j)

Here, the afﬁnity is computed using the decoder
state hai,j and the current state qi,j.
Finally, we embed the argument ai,j2 and the
state qi,j to generate the next state qi,j+1. Once
all arguments for oi are generated, the operation
is executed to obtain vi. Then, the embedding of
vi, the ﬁnal state of the instruction qi,|ai| and the
previous state hi are used to generate the state at
the next timestamp hi+1.
4
Inducing Programs while Learning
The set of instructions z that will generate y is un-
observed. Thus, given x we optimize the marginal
probability function:
p(y | x) =
X
z∈Z
p(y | z)p(z | x) =
X
z∈Z(y)
p(z | x),
where p(y | z) is the Kronecker delta function
δe(z),y, which is 1 if the execution of z, denoted as
e(z), generates y and 0 otherwise. Thus, we can
redeﬁne p(y|x), the marginal over all programs Z,
as a marginal over programs that would generate
y, deﬁned as Z(y). As marginalizing over z ∈
Z(y) is intractable, we approximate the marginal
by generating samples from our model. Denote
the set of samples that are generated by ˆZ(y). We
maximize P z ∈ˆZ(y)p(z|x).
However, generating programs that generate y
is not trivial, as randomly sampling from the RNN
distribution over instructions at each timestamp is
unlikely to generate a sequence z ∈Z(y).
2 The embeddings of a given argument ai,j and the return
value vi are obtained with a lookup table embedding and two
ﬂags indicating whether it is a string and whether it is a ﬂoat.
Furthermore, if the the value is a ﬂoat we also add its numeric
value as a feature.
This is analogous to the question answering
work in Liang et al. (2016), where the query that
generates the correct answer must be found dur-
ing inference, and training proved to be difﬁcult
without supervision. In Roy and Roth (2015) this
problem is also addressed by adding prior knowl-
edge to constrain the exponential space.
In our work, we leverage the fact that we are
generating rationales, where there is a sense of
progression within the rationale. That is, we as-
sume that the rationale solves the problem step by
step. For instance, in Problem 2, the rationale ﬁrst
describes the number of combinations of two cards
in a deck of 52 cards, then describes the number
of combinations of two kings, and ﬁnally com-
putes the probability of drawing two kings. Thus,
while generating the ﬁnal answer without the ra-
tionale requires a long sequence of latent instruc-
tions, generating each of the tokens of the rationale
requires far less operations.
More formally, given the sequence z1, . . . , zi−1
generated so far, and the possible values for zi
given by the network, denoted Zi, we wish to ﬁlter
Zi to Zi(yk), which denotes a set of possible op-
tions that contain at least one path capable of gen-
erating the next token at index k. Finding the set
Zi(yk) is achieved by testing all combinations of
instructions that are possible with at most one level
of indirection, and keeping those that can generate
yk. This means that the model can only gener-
ate one intermediate value in memory (not includ-
ing the operations that convert strings into ﬂoating
point values and vice-versa).
Decoding.
During decoding we ﬁnd the most
likely sequence of instructions z given x, which
can be performed with a stack-based decoder.
However, it is important to refer that each gen-
erated instruction zi = (oi, ri, ai,1, . . . , ai,|ai|, vi)
must be executed to obtain vi. To avoid generating
unexecutable code—e.g., log(0)—each hypothesis
instruction is executed and removed if an error oc-
curs. Finally, once the “⟨EOR⟩” tag is generated,
we only allow instructions that would generate one
of the option “A” to “E” to be generated, which
guarantees that one of the options is chosen.
5
Staged Back-propagation
As it is shown in Figure 2, math rationales with
more than 200 tokens are not uncommon, and with
additional intermediate instructions, the size z can
easily exceed 400. This poses a practical challenge

for training the model.
For both the attention and copy mechanisms,
for each instruction zi, the model needs to com-
pute the probability distribution between all the at-
tendable units c conditioned on the previous state
hi−1.
For the attention model and input copy
mechanisms, c = x0,i−1 and for the output copy
mechanism c = z. These operations generally
involve an exponential number of matrix multi-
plications as the size of c and z grows. For in-
stance, during the computation of the probabilities
for the input copy mechanism in Equation 1, the
afﬁnity function f between the current context q
and a given input uk is generally implemented by
projecting u and q into a single vector followed
by a non-linearity, which is projected into a sin-
gle afﬁnity value. Thus, for each possible input
u, 3 matrix multiplications must be performed.
Furthermore, for RNN unrolling, parameters and
intermediate outputs for these operations must be
replicated for each timestamp. Thus, as z becomes
larger the attention and copy mechanisms quickly
become a memory bottleneck as the computation
graph becomes too large to ﬁt on the GPU. In con-
trast, the sequence-to-sequence model proposed in
(Sutskever et al., 2014), does not suffer from these
issues as each timestamp is dependent only on the
previous state hi−1.
To deal with this, we use a training method we
call staged back-propagation which saves mem-
ory by considering slices of K tokens in z, rather
than the full sequence. That is, to train on a mini-
batch where |z| = 300 with K = 100, we would
actually train on 3 mini-batches, where the ﬁrst
batch would optimize for the ﬁrst z1:100, the sec-
ond for z101:200 and the third for z201:300. The
advantage of this method is that memory intensive
operations, such as attention and the copy mecha-
nism, only need to be unrolled for K steps, and K
can be adjusted so that the computation graph ﬁts
in memory.
However, unlike truncated back-propagation for
language modeling, where context outside the
scope of K is ignored, sequence-to-sequence
models require global context. Thus, the sequence
of states h is still built for the whole sequence z.
Afterwards, we obtain a slice hj:j+K, and com-
pute the attention vector.3 Finally, the prediction
of the instruction is conditioned on the LSTM state
3This modeling strategy is sometimes known as late fu-
sion, as the attention vector is not used for state propagation,
it is incorporated “later”.
and the attention vector.
6
Experiments
We apply our model to the task of generating ratio-
nales for solutions to math problems, evaluating it
on both the quality of the rationale and the ability
of the model to obtain correct answers.
6.1
Baselines
As the baseline we use the attention-based se-
quence to sequence model proposed by Bahdanau
et al. (2014), and proposed augmentations, allow-
ing it to copy from the input (Ling et al., 2016) and
from the output (Merity et al., 2016).
6.2
Hyperparameters
We used a two-layer LSTM with a hidden size of
H = 200, and word embeddings with size 200.
The number of levels that the graph G is expanded
during sampling D is set to 5. Decoding is per-
formed with a beam of 200. As for the vocabulary
of the softmax and embeddings, we keep the most
frequent 20,000 word types, and replace the rest of
the words with an unknown token. During train-
ing, the model only learns to predict a word as an
unknown token, when there is no other alternative
to generate the word.
6.3
Evaluation Metrics
The evaluation of the rationales is performed
with average sentence level perplexity and BLEU-
4 (Papineni et al., 2002). When a model cannot
generate a token for perplexity computation, we
predict unknown token. This beneﬁts the baselines
as they are less expressive. As the perplexity of
our model is dependent on the latent program that
is generated, we force decode our model to gener-
ate the rationale, while maximizing the probability
of the program. This is analogous to the method
used to obtain sample programs described in Sec-
tion 4, but we choose the most likely instructions
at each timestamp instead of sampling. Finally,
the correctness of the answer is evaluated by com-
puting the percentage of the questions, where the
chosen option matches the correct one.
6.4
Results
The test set results, evaluated on perplexity,
BLEU, and accuracy, are presented in Table 3.

Model
Perplexity
BLEU
Accuracy
Seq2Seq
524.7
8.57
20.8
+Copy Input
46.8
21.3
20.4
+Copy Output
45.9
20.6
20.2
Our Model
28.5
27.2
36.4
Table 3: Results over the test set measured in Per-
plexity, BLEU and Accuracy.
Perplexity.
In terms of perplexity, we observe
that the regular sequence to sequence model fares
poorly on this dataset, as the model requires
the generation of many values that tend to be
sparse. Adding an input copy mechanism greatly
improves the perplexity as it allows the genera-
tion process to use values that were mentioned
in the question. The output copying mechanism
improves perplexity slightly over the input copy
mechanism, as many values are repeated after their
ﬁrst occurrence. For instance, in Problem 2, the
value “1326” is used twice, so even though the
model cannot generate it easily in the ﬁrst occur-
rence, the second one can simply be generated by
copying the ﬁrst one.
We can observe that our
model yields signiﬁcant improvements over the
baselines, demonstrating that the ability to gener-
ate new values by algebraic manipulation is essen-
tial in this task. An example of a program that is
inferred is shown in Figure 4. The graph was gen-
erated by ﬁnding the most likely program z that
generates y. Each node isolates a value in x, m,
or y, where arrows indicate an operation executed
with the outgoing nodes as arguments and incom-
ing node as the return of the operation. For sim-
plicity, operations that copy or convert values (e.g.
from string to ﬂoat) were not included, but nodes
that were copied/converted share the same color.
Examples of tokens where our model can obtain
the perplexity reduction are the values “0.025”,
“0.023”, “0.002” and ﬁnally the answer “E” , as
these cannot be copied from the input or output.
BLEU.
We observe that the regular sequence to
sequence model achieves a low BLEU score. In
fact, due to the high perplexities the model gener-
ates very short rationales, which frequently consist
of segments similar to “Answer should be D”, as
most rationales end with similar statements. By
applying the copy mechanism the BLEU score
improves substantially, as the model can deﬁne
the variables that are used in the rationale.
In-
terestingly, the output copy mechanism adds no
further improvement in the perplexity evaluation.
This is because during decoding all values that can
be copied from the output are values that could
have been generated by the model either from the
softmax or the input copy mechanism. As such,
adding an output copying mechanism adds little to
the expressiveness of the model during decoding.
Finally, our model can achieve the highest
BLEU score as it has the mechanism to generate
the intermediate and ﬁnal values in the rationale.
Accuracy.
In terms of accuracy, we see that
all baseline models obtain values close to chance
(20%), indicating that they are completely unable
to solve the problem. In contrast, we see that our
model can solve problems at a rate that is signiﬁ-
cantly higher than chance, demonstrating the value
of our program-driven approach, and its ability to
learn to generate programs.
In general, the problems we solve correctly cor-
respond to simple problems that can be solved in
one or two operations. Examples include ques-
tions such as “Billy cut up each cake into 10 slices,
and ended up with 120 slices altogether.
How
many cakes did she cut up? A) 9 B) 7 C) 12 D)
14 E) 16”, which can be solved in a single step. In
this case, our model predicts “120 / 10 = 12 cakes.
Answer is C” as the rationale, which is reasonable.
6.5
Discussion.
While we show that our model can outperform the
models built up to date, generating complex ratio-
nales as those shown in Figure 1 correctly is still
an unsolved problem, as each additional step adds
complexity to the problem both during inference
and decoding. Yet, this is the ﬁrst result showing
that it is possible to solve math problems in such
a manner, and we believe this modeling approach
and dataset will drive work on this problem.
7
Related Work
Extensive efforts have been made in the domain
of math problem solving (Hosseini et al., 2014;
Kushman et al., 2014; Roy and Roth, 2015), which
aim at obtaining the correct answer to a given math
problem. Other work has focused on learning to
map math expressions into formal languages (Roy
et al., 2016). We aim to generate natural language
rationales, where the bindings between variables
and the problem solving approach are mixed into

Bottle R contains 250 capsules and costs $ 6.25 . 
Bottle T contains 130 capsules and costs $ 2.99 . 
What is the difference between the cost per capsule for bottle R and the 
cost per capsule for bottle T ?
    (A) $ 0.25 
    (B) $ 0.12
    (C) $ 0.05 
    (D) $ 0.03
    (E) $ 0.002
Cost
per
capsule
in
R
is
6.25
/
250
=
0.025
Cost
per
capsule
in
T
is
2.99
/
130
=
0.023
The
diﬀerence
is
0.002
The
answer
is
E
250
6.25
0.025
2.99
130
0.023
0.002
E
div(m1,m2)
div(m4,m5)
sub(m6,m3)
check(m7)
\n
\n
\n
<EOS>
y
m
x
Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out
question-rationale pair.
a single generative model that attempts to solve the
problem while explaining the approach taken.
Our approach is strongly tied with the work
on sequence to sequence transduction using the
encoder-decoder paradigm (Sutskever et al., 2014;
Bahdanau et al., 2014; Kalchbrenner and Blun-
som, 2013), and inherits ideas from the extensive
literature on semantic parsing (Jones et al., 2012;
Berant et al., 2013; Andreas et al., 2013; Quirk
et al., 2015; Liang et al., 2016; Neelakantan et al.,
2016) and program generation (Reed and de Fre-
itas, 2016; Graves et al., 2016), namely, the usage
of an external memory, the application of differ-
ent operators over values in the memory and the
copying of stored values into the output sequence.
Providing textual explanations for classiﬁcation
decisions has begun to receive attention, as part of
increased interest in creating models whose deci-
sions can be interpreted. Lei et al. (2016), jointly
modeled both a classiﬁcation decision, and the se-
lection of the most relevant subsection of a docu-
ment for making the classiﬁcation decision. Hen-
dricks et al. (2016) generate textual explanations
for visual classiﬁcation problems, but in contrast
to our model, they ﬁrst generate an answer, and
then, conditional on the answer, generate an ex-
planation. This effectively creates a post-hoc jus-
tiﬁcation for a classiﬁcation decision rather than
a program for deducing an answer. These papers,
like ours, have jointly modeled rationales and an-
swer predictions; however, we are the ﬁrst to use
rationales to guide program induction.
8
Conclusion
In this work, we addressed the problem of generat-
ing rationales for math problems, where the task is
to not only obtain the correct answer of the prob-
lem, but also generate a description of the method
used to solve the problem. To this end, we collect
100,000 question and rationale pairs, and propose
a model that can generate natural language and
perform arithmetic operations in the same decod-
ing process. Experiments show that our method
outperforms existing neural models, in both the
ﬂuency of the rationales that are generated and the
ability to solve the problem.

References
Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
Proc. of ACL.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014.
Neural machine translation by jointly
learning to align and translate. arXiv 1409.0473.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013.
Semantic parsing on freebase from
question-answer pairs. In Proc. of EMNLP.
Alex
Graves,
Greg
Wayne,
Malcolm
Reynolds,
Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwiska,
Sergio Gmez Colmenarejo,
Edward
Grefenstette,
Tiago
Ramalho,
John
Agapiou,
Adri Puigdomnech Badia, Karl Moritz Hermann,
Yori Zwols, Georg Ostrovski, Adam Cain, Helen
King, Christopher Summerﬁeld, Phil Blunsom,
Koray Kavukcuoglu, and Demis Hassabis. 2016.
Hybrid computing using a neural network with
dynamic external memory. Nature 538(7626):471–
476.
Brent Harrison, Upol Ehsan, and Mark O. Riedl.
2017.
Rationalization:
A
neural
machine
translation approach to generating natural lan-
guage
explanations.
CoRR
abs/1702.07826.
http://arxiv.org/abs/1702.07826.
Lisa
Anne
Hendricks,
Zeynep
Akata,
Marcus
Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor
Darrell. 2016. Generating visual explanations. In
Proc. ECCV.
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In Proc. of EMNLP.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proc. of ACL.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proc. of EMNLP.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proc. of ACL.
Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
Rationalizing neural predictions.
In Proc. of
EMNLP.
Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2016.
Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. arXiv 1611.00020.
Wang Ling, Edward Grefenstette, Karl Moritz Her-
mann, Tom´as Kocisk´y, Andrew Senior, Fumin
Wang, and Phil Blunsom. 2016.
Latent predictor
networks for code generation. In Proc. of ACL.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016.
Pointer sentinel mixture
models. arXiv 1609.07843.
Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever.
2016.
Neural programmer: Inducing latent pro-
grams with gradient descent. In Proc. ICLR.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proc. of ACL.
Chris Quirk, Raymond Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In Proc. of ACL.
Scott E. Reed and Nando de Freitas. 2016.
Neural
programmer-interpreters. In Proc. of ICLR.
Subhro Roy and Dan Roth. 2015.
Solving general
arithmetic word problems. In Proc. of EMNLP.
Subhro Roy, Shyam Upadhyay, and Dan Roth. 2016.
Equation parsing: Mapping sentences to grounded
equations. In Proc. of EMNLP.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. arXiv 1409.3215.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Proc. of NIPS.

