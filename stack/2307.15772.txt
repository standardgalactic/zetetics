arXiv:2307.15772v1  [stat.ML]  28 Jul 2023
Weighted variation spaces and approximation by shallow ReLU
networksâˆ—
Ronald DeVore, Robert D. Nowak, Rahul Parhi, and Jonathan W. Siegel
Abstract
We investigate the approximation of functions f on a bounded domain â„¦âŠ‚Rd by the
outputs of single-hidden-layer ReLU neural networks of width n. This form of nonlinear n-
term dictionary approximation has been intensely studied since it is the simplest case of neural
network approximation (NNA). There are several celebrated approximation results for this form
of NNA that introduce novel model classes of functions on â„¦whose approximation rates avoid
the curse of dimensionality. These novel classes include Barron classes [2], and classes based on
sparsity or variation [20, 21] such as the Radon-domain BV classes.
The present paper is concerned with the deï¬nition of these novel model classes on domains â„¦.
The current deï¬nition of these model classes does not depend on the domain â„¦. A new and more
proper deï¬nition of model classes on domains is given by introducing the concept of weighted
variation spaces. These new model classes are intrinsic to the domain itself. The importance of
these new model classes is that they are strictly larger than the classical (domain-independent)
classes. Yet, it is shown that they maintain the same NNA rates.
Keywordsâ€”Neural networks, approximation rates, variation spaces, curse of dimensionality.
1
Introduction
Neural networks (NNs) are now the numerical method of choice for the development of learning
algorithms in regression and classiï¬cation, especially when dealing with functions of d variables
with d large. It is therefore important to understand, through mathematical theory, the reasons for
this success. In learning, we are tasked with approximating an unknown function f on a domain
â„¦âŠ‚Rd from some ï¬nite set of data observations of f. Thus, at least part of the success in using
NNs for such learning problems, must lie in their ability to eï¬€ectively approximate the functions
of interest. This has led to the study of approximation by NNs and, in particular, to understand
exactly which functions are well approximated by NNs and what is their approximation rate in
terms of the number of neurons employed (so-called n-term approximation).
A large number of papers have been written in recent years that give quantitative bounds on
the approximation rates of various model classes of functions when using neural networks. General
accountings of such results can be found in [3, 7, 10, 25].
Two types of results have emerged.
The ï¬rst is to show that deep NNs with ReLU activation functions (see, e.g., [15, 28, 29, 33]) are
surprisingly eï¬€ective in approximating functions from classical model classes such as ï¬nite balls
âˆ—This research was supported by the NSF grants DMS-2134077 and DMS-2134140 of the MoDL program as well
as the MURI ONR grant N00014-20-1-2787 (RD and RN). RP was supported by the European Research Council
(ERC Project FunLearn) under grant 101020573. JS was supported by NSF grants DMS-2111387 and CCF-2205004.
1

in a Sobolev or Besov space when the approximation error is measured in an Lp(â„¦) norm with
1 â‰¤p â‰¤âˆ. While such results are deep and interesting, they do not match the most common
setting of learning in high dimensions (d large) because these model classes necessarily suï¬€er the
curse of dimensionality. Indeed, the approximation rates for such smoothness classes is of the form
O(nâˆ’s/d) with s related to the smoothness assumption on f. Here and later n always refers to
the number of neurons used in the approximation. Thus, for large values of d, membership in
such a model class is not a realistic assumption to make on the target function f to be learned.
This negativity for classical smoothness as a model class asssumption for f can be ameliorated by
assuming that the input variable to f (and hence the data as well) is restricted by a probability
measure Âµ on â„¦supported on a low-dimensional submanifold.
The second type of approximation result introduces novel high-dimensional model classes for
which neural network approximation (NNA) does not suï¬€er this curse of dimensionality. Thus,
membership in these new model classes can be a realistic model class assumption for learning a
function of many variables. The most celebrated examples of such new model classes are the Barron
class Bs, s > 0, introduced in [2]. The set Bs = Bs(Rd) consists of all functions f deï¬ned on Rd
whose Fourier transform Ë†f satisï¬es
âˆ¥fâˆ¥Bs :=
Z
Rd
(1 + |Ï‰|)s| Ë†f(Ï‰)| dÏ‰ < +âˆ.
(1.1)
The original result of Barron showed that on a bounded domain â„¦âŠ‚Rd, any function f on â„¦
which is the restriction of a function from B1(Rd) can be approximated in the L2(â„¦) norm by single-
hidden-layer sigmoidal networks with n neurons to an accuracy of Câ„¦âˆ¥fâˆ¥B1nâˆ’1/2, n â‰¥1, where
the constant Câ„¦only depends upon the measure of â„¦. Notice that this approximation rate does
not deteriorate with increasing d in contrast with classical smoothness model classes. However, one
must note that the above deï¬nition of Barron spaces depend on d and indeed get more demanding
as d increases.
Barronâ€™s result spurred a lot of study and generalizations over the last decades. In particular,
new model classes of functions which have sparse representation of as linear combinations of neural
atoms were introduced. In the case of ReLU neurons, the sparsity class is larger than the (second-
order) Barron class and yet preserves the rate of approximation of n-term approximation [16].
These spaces based on sparsity are called variation spaces [1, 14, 18, 30, 31]. We summarize these
activities for ReLU neurons in the following two sections. For the moment, we only wish to focus
on the existing theory for these model classes and their approximation rates on domains â„¦âŠ‚Rd.
This is the typical setting in applications. The existing theory deï¬nes the corresponding model
classes on Rd and then extends the deï¬nition to domains as the restriction of functions deï¬ned on
Rd. As such, the theory and corresponding results are in a strong sense independent of the domain
â„¦. While this leads to a simple approximation theory on domains, these results never take into
consideration the nature of â„¦, e.g., its geometry.
The purpose of the present paper is to show there is a more satisfactory deï¬nition of these novel
model classes on domains â„¦that leads to domain-dependent results that are stronger than that
provided by the existing theory. We call these new model classes weighted variation spaces since
they generalize the classical variation space for ReLU neurons by introducing a domain-dependent
weighting of the ReLU atoms. These new model classes are strictly larger than the existing variation
spaces while still maintaining the same rate of approximation of n-term approximation. We develop
this domain-dependent theory primarily in the case when â„¦= Bd is the Euclidean unit ball in
2

Rd. To indicate how the theory would depend on the domain â„¦, we also consider the domain
Qd := [âˆ’1, 1]d and contrast the diï¬€erence in this case with that of Bd.
While we develop our results only for the case of ReLU neurons, it will be clear how they
generalize to the case of ReLUk neurons, k > 1. So, for the remainder of this paper the activation
function is
Ïƒ(t) = t+ = max{0, t}.
(1.2)
This paper is organized as follows. In the next two sections, we review some of the existing
results on ReLU neural network approximation. This will serve to frame the new results proved in
this paper. In Â§4 we introduce our new (domain-dependent) model classes. In Â§5, we prove our new
approximation results for â„¦= B2 the unit Euclidean ball in R2. We separate out this case since
it is the simplest setting to understand. The remaining sections of this paper formulate and prove
our results for â„¦= Bd which is the Euclidean unit ball in Rd. We also contrast how the results
change when â„¦= Qd. Finally, we discuss the possible signiï¬cance of these new model classes for
the problem of learning from data.
2
Approximation by shallow ReLU networks
In this paper, we concentrate on a very speciï¬c case of NNA, namely approximation by single-
hidden-layer ReLU NNs, i.e., the activation function Ïƒ is given by (1.2). We study neural network
approximation on a given bounded domain (the closure of an open connected set) â„¦of Rd. The most
natural choices for â„¦are the unit Euclidean ball Bd of Rd or the d-dimensional cube Qd := [âˆ’1, 1]d.
The case â„¦= Bd will be the primary example considered in this paper. In going further, we let
âˆ¥Â· âˆ¥denote the Euclidean norm on Rd.
We deï¬ne the ReLU atoms
Ï†(x; Î¾, t) := Ïƒ(Î¾ Â· x âˆ’t) = (Î¾ Â· x âˆ’t)+,
Î¾ âˆˆRd, âˆ¥Î¾âˆ¥= 1, t âˆˆR.
(2.1)
Given the atom Ï†, we let
HÏ† := {x âˆˆâ„¦: Î¾ Â· x = t}
(2.2)
be its hyperplane cut. HÏ† divides â„¦into two regions HÂ±
Ï† . The function Ï† is identically zero on
the region Hâˆ’
Ï† := {x âˆˆâ„¦:
Î¾ Â· x â‰¤t} and the linear function = Î¾ Â· x âˆ’t on the second region
H+
Ï† := {x âˆˆâ„¦: Î¾ Â· x > t}. Notice that for some values of t, the atom Ï† is identically zero on â„¦so
that Hâˆ’
Ï† = â„¦.
For each â„¦, there is a smallest interval T = T(â„¦) such that for t /âˆˆT, the dictionary element
Ï†(Â·; Î¾, t) is either identically zero on â„¦or a linear function on â„¦. Let D = D(â„¦) := {Ï†(Â·; Î¾, t)} be
the dictionary of all atoms Ï† for which t âˆˆT = T(â„¦). We are interested in n-term approximation
from the dictionary D. For n = 1, 2, . . . , let Î£n := Î£n(D) be the set of functions of the form
S(x) =
n
X
j=1
ajÏ†j(x),
x âˆˆâ„¦,
(2.3)
where the Ï†j are chosen arbitrarily from D and a1, . . . , an are real numbers. When n = 0, we deï¬ne
Î£0 := {0}. The functions S âˆˆÎ£n are precisely the functions on â„¦produced by a single-hidden-layer
ReLU network with n neurons, i.e., width n. The set Î£n is thus a (d + 2)n dimensional parametric
3

nonlinear manifold parameterized by the Î¾j âˆˆBd, j = 1, . . . , n, the tj âˆˆR, j = 1, . . . , n, and the
coeï¬ƒcients a1, . . . , an âˆˆR. Note that a given S âˆˆÎ£n has in general many representations of the
form (2.3). In other words, the dictionary D(â„¦) is redundant.
The above paragraph tells us that there are two ways to view shallow network approxima-
tion with ReLU activation. One view is that it is a special case of n-term approximation from a
dictionary of functions. Another view is that it is a special case of manifold approximation. There-
fore, a proper assessment of this form of NN approximation would be to compare it with other
approximation methods of either one of these forms.
Approximation by Î£n is one of the simplest examples of neural network approximation (NNA).
It is therefore a fundamental problem to completely understand the approximation properties of
Î£n, n â‰¥1, i.e., what are the properties of a function f that determine how well f is approximated
by the elements of Î£n. In the case d = 1 and â„¦is an interval, the set Î£n is the space of piecewise
linear function with n breakpoints. In this special case, approximation by Î£n is well understood
(see e.g. [7, 8]). So, we restrict ourselves to the case d â‰¥2 in going further in this paper.
For a function f in Lp(â„¦), 1 â‰¤p â‰¤âˆ, we deï¬ne
En(f)p := En(f)Lp(â„¦) := inf
SâˆˆÎ£n âˆ¥f âˆ’Sâˆ¥Lp(â„¦).
(2.4)
This is a form of nonlinear approximation since the set Î£n is not a linear space but rather a
nonlinear manifold. Rightfully, we often put this form of approximation in competition with other
examples of manifold approximation (see e.g. [6, 7]).
From the viewpoint of approximation theory, an understanding of the approximation properties
of Î£n would seek to precisely characterize the approximation classes for Î£n approximation. An ap-
proximation class is the collection of all functions whose approximation error decays at a prescribed
decay rate. For example, for a given Î± > 0, we seek a characterization of the set
AÎ± := AÎ±((Î£n)nâ‰¥0, Lp(â„¦))
(2.5)
of functions f âˆˆLp(â„¦) for which
En(f)p â‰¤M(n + 1)âˆ’Î±,
n = 0, 1, 2, . . . .
(2.6)
Note that by deï¬nition, Î£0 = {0} and hence E0(f)p = âˆ¥fâˆ¥Lp(â„¦) The smallest value of M for which
(2.6) holds is deï¬ned as âˆ¥fâˆ¥AÎ±. Notice that AÎ± is a quasi-normed linear space. While for most
classical methods of linear and nonlinear approximation, e.g. polynomials, splines, n-term wavelets,
there is a characterization of the spaces AÎ± (at least for a certain range of Î±), the case for neural
network approximation is much diï¬€erent. There is at present no known characterization of AÎ± for
any value of Î± > 0. There are however many suï¬ƒcient conditions that guarantee membership in
AÎ± (see [7]).
Another (less ambitious) viewpoint of approximation by Î£n is to propose model classes K, i.e.,
compact subsets K âŠ‚Lp(â„¦), and study how well the elements of K can be approximated by the
elements of Î£n. This leads to the study of
En(K)p := sup
fâˆˆK
En(f)p,
n â‰¥0.
(2.7)
If one comes up with a set K for which En(K) â‰¤Cnâˆ’Î±, n â‰¥1, then clearly K âŠ‚AÎ± and we
gain some information about AÎ±. Many interesting approximation results have been proven for
4

various classical model classes K such as Sobolev and Besov balls, however, the best approximation
rates are not known in all cases [7]. These results show no gain in approximation eï¬ƒciency when
compared with more classical methods of approximation such as those that use splines or wavelets.
Moreover, these classical model classes all suï¬€er the curse of dimensionality: smoothness of order
s gives rate decay En(K)p â‰¥Cnâˆ’s/d, n â‰¥0.
One of the celebrated accomplishments in the study of NNA was the introduction of new model
classes K which are known to not suï¬€er the curse of dimensionality and also give us information
on AÎ±. We discuss these model classes in the next two sections. In going further in this paper we
only treat the case of approximation in L2(â„¦). However, the case of Lp(â„¦) approximation has also
been well studied (see [30]).
3
Novel (non-classical) model classes
While the classical model classes based on smoothness all suï¬€er the curse of dimensionality, certain
novel model classes K have been introduced that avoid this curse. The discovery of these novel
model classes begin with the celebrated work of Barron [2]. We have already deï¬ned the Barron
spaces Bs(Rd) in the introduction.
Barronâ€™s original results on NNA were for sigmoidal activation and the Barron class B1(Rd)
where he showed that functions in this class, when restricted to a domain â„¦âŠ‚Rd, had an L2(â„¦)
approximation rate nâˆ’1/2, n â‰¥1. It was rather straightforward to extend his approach to proving
that functions in B2 had the same approximation rate when using ReLU activation. Several follow
up papers signiï¬cantly improved on these original results as we now describe.
Notice that the Barron classes are formulated for functions which are deï¬ned on all of Rd. Given
a bounded domain â„¦, it is not obvious how these classes should be deï¬ned on â„¦. The deï¬nition
employed in the literature is that the space Bs(â„¦) is the set of function f deï¬ned on â„¦which are
the restriction of a function F âˆˆBs(Rd) with norm given by
âˆ¥fâˆ¥Bs(â„¦) :=
inf
F |â„¦=f âˆ¥Fâˆ¥Bs(Rd),
s > 0.
(3.1)
With this deï¬nition, we have
En(U(B2(â„¦))L2(â„¦) â‰¤Cnâˆ’1/2,
n â‰¥1,
(3.2)
where C depends only on the diameter and measure of â„¦. Here and later we use the notation
U(Y ) to denote the unit ball of a normed space Y . This approximation rate was improved over
the years starting with Makovoz [17] and continuing on with the results of [1, 12, 30]. The current
best known approximation rate for n-term ReLU NNA is
En(U(B2(â„¦))L2(â„¦) â‰¤Cnâˆ’1
2âˆ’3
2d ,
n â‰¥1,
(3.3)
where again C depends only on d. We refer the reader to [30] for a more detailed discussion of
these approximation results. It is still not known if this rate can be improved for the Barron class
B2.
We turn next to a second family of novel model classes for NNA referred to as variation spaces.
Let D = D(â„¦) be the dictionary of ReLU atoms whose hyperplane cut intersects â„¦. Consider any
function S = Pn
j=1 ajÏ†j, i.e., S âˆˆÎ£n. Recall that this representation is not unique. We deï¬ne
5

V (S) := inf
ï£±
ï£²
ï£³
n
X
j=1
|aj| : S =
n
X
j=1
ajÏ†j
ï£¼
ï£½
ï£¾,
(3.4)
which is called the variation of S with respect to the dictionary D.
With this notation in hand, we can deï¬ne a new space V := V(â„¦) = V(â„¦, D) as the set of all f
in L2(â„¦) for which there is a sequence Sn âˆˆÎ£n, n â‰¥1, such that âˆ¥f âˆ’Snâˆ¥L2(â„¦) â†’0, n â†’âˆ, and
V (Sn) â‰¤M, n â‰¥1. Throughout the paper, we will use V when the domain â„¦and dictionary D
are clear from the context, and use V(â„¦), V(D), or V(â„¦, D) when we want to call attention to the
domain and/or dictionary. The smallest M for which this is true is deï¬ned as âˆ¥fâˆ¥V(â„¦). This space
is called the variation space of the dictionary D. The space V(â„¦) is a Banach space with respect to
this norm (see [31] for properties of variation spaces). A fundamental relation between the Barron
and variation space is the embedding
âˆ¥fâˆ¥V(â„¦) â‰¤Câ„¦âˆ¥fâˆ¥B2(â„¦),
f âˆˆB2(â„¦),
(3.5)
with Câ„¦the embedding constant (which depends only on the diameter of â„¦). The space V(â„¦) is
strictly larger than B2(â„¦).
The variation space V(â„¦) has been carefully studied and in particular it has been proven that
that (see [30])
En(U(V(â„¦)))L2(â„¦) â‰¤Cnâˆ’1
2âˆ’3
2d ,
n â‰¥1,
(3.6)
where C depends only on â„¦and d. This approximation rate also matches the decay rate of the metric
entropy of U(V(â„¦)) [30]. Notice that this gives the bound (3.3) and is in fact how approximation
rates for the Barron class are proved. The important thing to note here is that V is a larger space
than B2 but the current best known approximation rates (with shallow ReLU NNs) for both of
these classes is the same, namely O(nâˆ’1
2âˆ’3
2d ), n â‰¥1.
A major breakthrough in the understanding of V(â„¦) was made by characterizing membership
of a function f in V(â„¦) through the smoothness of its Radon transform. Namely, it was originally
proved in [20] that a function f is in V(â„¦) if and only if f has an extension F to all of Rd such that
the Radon transform R(F; Î¾, t) is in a certain smoothness space. Properties and generalizations
of this notion of smoothness were extensively studied in [21, 22, 24], giving rise to a new family
of Banach spaces, now referred to as the Radon BV spaces. These spaces are denoted by RBVk,
k âˆˆN.
The key result of [21] is the following representer theorem for these spaces.
Let xi âˆˆRd,
i = 1, . . . , m, and yi âˆˆR, i = 1, . . . , m. Then, there always exists a solution to the data-ï¬tting
problem
min
fâˆˆRBVk
m
X
i=1
L(yi, f(xi)) + Î»|f|RBVk
(3.7)
that takes the form of a function S which is the output of a single-hidden-layer neural network with
â‰¤m neurons and ReLUkâˆ’1 activation functions. Here, L is any loss function which is lower-semi-
continuous in its second argument and |f|RBVk is the semi-norm which deï¬nes the RBVk spaces,
which measures smoothness in the Radon domain. The Radon BV spaces are deï¬ned on domains
6

â„¦âŠ‚Rd via restrictions. For the case k = 2 (which corresponds to shallow ReLU NNs) it has been
shown in [24, Theorem 6] (see also [31, Theorem 2 and Corollary 1]) that
RBV2(â„¦) = V(â„¦),
(3.8)
with equivalent norms. It has also been shown that there exists a solution S to (3.7) which is in
Î£m(D) on any bounded domain â„¦âŠ‚Rd [24, Theorem 5].
4
Weighted variation model classes
One of the main points of the present paper is that one can derive improved results on approximation
by shallow ReLU networks if one considers new model classes that generalize the standard variation
space by including weights on the atoms. In this section, we introduce these new model classes for
the case when we want the error of approximation to be taken in the L2(â„¦) norm with â„¦a bounded
domain in Rd. We begin with the general principle of weighted variation spaces.
Let D be the dictionary of ReLU atoms. Let Sdâˆ’1 be the boundary of the unit Euclidean ball
Bd of Rd. That is, Sdâˆ’1 := {Î¾ âˆˆRd : âˆ¥Î¾âˆ¥= 1}. Any atom Ï† in D is of the form Ï†(x) = (Î¾ Â· x âˆ’t)+
where t âˆˆR. We are interested in the atoms Ï† whose hyperplane cut intersects â„¦(since otherwise
the atom is identically an aï¬ƒne function). Accordingly, we deï¬ne
Z(â„¦) := {(Î¾, t) : Î¾ âˆˆSdâˆ’1, t âˆˆR such that HÏ†(Â·;Î¾,t) âˆ©â„¦Ì¸= âˆ…}
(4.1)
and Â¯Z(â„¦) its closure in the Euclidean norm. Note that whenever Ï†(x) = Ï†(x; Î¾, t) is positive for
some x âˆˆâ„¦, it is positive in a neighborhood of x and hence âˆ¥Ï†âˆ¥L2(â„¦) > 0. Given the domain â„¦we
deï¬ne the dictionary
D(â„¦) := {Ï†(Â·, Î¾, t) : (Î¾, t) âˆˆÂ¯Z(â„¦)}.
(4.2)
The set Â¯Z(â„¦) is a compact subset of Sdâˆ’1 Ã—R. If we equip Â¯Z(â„¦) with the Euclidean norm topology
then the mapping (Î¾, t) 7â†’Ï†(Â·; Î¾, t) is a continuous mapping from Â¯Z(â„¦) into L2(â„¦).
Here is an important observation about the atoms in this dictionary which underlies the im-
proved approximation results of this paper. While each atom Ï† âˆˆD(â„¦) is in L2(â„¦) whenever â„¦is a
bounded domain, the L2(â„¦) norm of Ï† will depend heavily on Ï† and â„¦. Namely, if the support of Ï†
lies near the boundary of â„¦then this norm will be small and we expect that Ï† has a less important
role in approximating a given target function f âˆˆL2(â„¦).
As an example, consider the case when â„¦= Bd is the d-dimensional Euclidean ball. It is easy
to see that the atom Ï†(x) = (Î¾ Â· x âˆ’t)+, has L2(â„¦)-norm satisfying
âˆ¥Ï†âˆ¥L2(â„¦) â‰ˆ(1 âˆ’t)
3
2+ dâˆ’1
4 ,
âˆ’1 â‰¤t â‰¤1,
(4.3)
with constants of equivalence depending only on d. Indeed, the Lâˆ(â„¦) norm of Ï† is 1 âˆ’t and the
measure of its support â‰ˆ(1 âˆ’t)[âˆš1 âˆ’t]dâˆ’1. It follows that the norms of atoms get smaller as t
approaches one.
The compactness of Â¯Z(â„¦) implies that the dictionary D(â„¦) is a compact subset of L2(â„¦).
Thus, there is another useful characterization of the functions in V(â„¦). Consider the space M :=
M( Â¯Z(â„¦)) of all ï¬nite (signed) Radon measures on Â¯Z(â„¦), equipped with the variation norm âˆ¥Âµâˆ¥M :=
R
Â¯Z(â„¦)
d|Âµ|. For Âµ âˆˆM, we introduce the function
fÂµ :=
Z
Â¯Z(â„¦)
Ï†(Â·; Î¾, t) dÂµ(Î¾, t),
(4.4)
7

where the integral in (4.4) can be understood as a Bochner integral (see [31, Lemma 3] for more
details). Then, any f âˆˆV(â„¦) has a representation
f = fÂµ,
for some Âµ âˆˆM.
(4.5)
This representation is not unique in the sense that diï¬€erent measures Âµ can give rise to the same
f. It then follows (see [31]) that the V-norm can be alternatively speciï¬ed by
âˆ¥fâˆ¥V = inf{âˆ¥Âµâˆ¥M : f = fÂµ, Âµ âˆˆM}.
(4.6)
In order to simplify the geometry, in going further in this section, we assume that â„¦is a convex
subset of Rd and D := D(â„¦). We say that
w(Î¾, t),
(Î¾, t) âˆˆÂ¯Z(â„¦),
(4.7)
is a weight function if w is a non-negative continuous function on Â¯Z(â„¦). Given an atom Ï†(Â·; Î¾, t)
we will abuse notation and also write w(Ï†) or w(Ï†(Â·; Î¾, t)) for w(Î¾, t).
Admissible Weights: Given a weight function w deï¬ned on Â¯Z(â„¦), we deï¬ne
ËœÏ†(Â·; Î¾, t) := Ï†(Â·; Î¾, t)
w(Î¾, t) ,
(Î¾, t) âˆˆÂ¯Z(â„¦),
(4.8)
where ËœÏ†(Â·; Î¾, t) is deï¬ned to be the zero function whenever w(Î¾, t) = 0. We say that the weight
function w is admissible for â„¦, if the mapping (Î¾, t) â†’ËœÏ†(Â·; Î¾, t) is continuous as a mapping from
Â¯Z(â„¦) into L2(â„¦). It follows that
âˆ¥ËœÏ†(Â·; Î¾, t)âˆ¥L2(â„¦) â‰¤Cw,
(4.9)
with Cw an absolute constant. Notice that if a weight function w is admissible, then any larger
weight function Ëœw is also admissible.
When given an admissible weight w, the set of functions
Dw := Dw(â„¦) = {ËœÏ†(Â·; Î¾, t) : (Î¾, t) âˆˆÂ¯Z(â„¦)}.
(4.10)
is a new dictionary contained in L2(â„¦). Furthermore, this dictionary is compact in L2(â„¦). We
deï¬ne the weighted variation space Vw := Vw(â„¦) to be variation space of this new dictionary Dw.
Since the admissibility conditions ensure that the dictionary Dw is compact in L2(â„¦), we have,
from the discussion above, that, for every f âˆˆVw(â„¦), there exists a signed Radon measure Âµ = Âµf
on Â¯Z(â„¦) such that
f = ËœfÂµ :=
Z
Â¯Z(â„¦)
ËœÏ†(Â·; Î¾, t) dÂµ(Î¾, t)
with
âˆ¥fâˆ¥Vw(â„¦) = âˆ¥ËœfÂµâˆ¥Vw(â„¦) = âˆ¥Âµfâˆ¥M.
(4.11)
We also clearly have
âˆ¥fâˆ¥L2(â„¦) â‰¤Cwâˆ¥fâˆ¥Vw(â„¦),
f âˆˆVw(â„¦),
(4.12)
where Cw is the constant in (4.9).
We also have that, if Ëœw â‰¥w, then V Ëœw(â„¦) âŠ‚Vw(â„¦) and
âˆ¥fâˆ¥V Ëœ
w â‰¤âˆ¥fâˆ¥Vw(â„¦) which implies that
En(V Ëœw(â„¦)) â‰¤En(Vw(â„¦)),
n â‰¥0.
(4.13)
8

While Vw(â„¦) is deï¬ned for any nonnegative weight w which is admissible, there is a particular
choice of w which we will consider in this paper. Speciï¬cally, we show that the approximation rates
derived for shallow ReLU neural networks on the unweighted space V(â„¦) actually hold on the larger
space Vw(â„¦) for a certain collection of admissible weights w. As we will later see, the smallest
admissible weight with this property will depend upon the domain â„¦. This domain-dependent
smallest weight is related to the measure of the intersection of the hyperplane of Ï† restricted to
â„¦. To describe this particular weight w and our new approximation results, we start with the case
d = 2 where the proofs of approximation rates are simplest to understand. We consider the two
domains â„¦= B2 and â„¦= Q2. Later, we treat the general cases â„¦= Bd, d â‰¥2. We then explain
how the same theory carries over to â„¦= Qd (see Remark (6.8)).
Variation spaces V(D0) are deï¬ned as above for any dictionary D0 in any Hilbert space H
provided that the dictionary elements Ïˆ âˆˆD0 satisfy âˆ¥Ïˆâˆ¥H â‰¤Î´ for a ï¬xed value of Î´ > 0. Given
such a dictionary D0, we deï¬ne Î£n := Î£n(D0) as the set of all functions S âˆˆH that are a linear
combination of at most n terms of D0. For any f âˆˆH, we deï¬ne the error of n term approximation
to be
E(f, Î£n)H := inf
SâˆˆÎ£n âˆ¥f âˆ’Sâˆ¥H.
(4.14)
This n-term approximation error from a dictionary is well studied. A fundamental result for such
n-term approximation is the theorem of Maurey [26] (see also [2, 11]). Maureyâ€™s theorem says that
for each n â‰¥0 and f âˆˆV(D0) we have
inf
SnâˆˆÎ£n âˆ¥f âˆ’Snâˆ¥H â‰¤âˆ¥fâˆ¥V(D0)Î´nâˆ’1/2,
n â‰¥1.
(4.15)
In fact, Maureyâ€™s theorem can be generalized beyond the setting of a Hilbert space to the class of
type-2 Banach spaces (see [30] for the application to non-linear dictionary approximation). This
introduces an extra constant factor which depends upon the type-2 constant of the space. We shall
use this theorem going forward, but restrict ourselves to the Hilbert space setting.
5
Approximation in â„¦= B2
In this section, we develop our results in the case â„¦= B2 where B2 is the unit Euclidean ball in
R2. Here, Â¯Z(â„¦) = S1 Ã— [âˆ’1, 1]. This will illustrate, in their simplest form, all of the principles
needed to treat the more general case â„¦= Bd, d â‰¥2. The treatment of Bd is given in Â§6 but with
a signiï¬cant increase in the level of technicality.
In this section, we let D = D(â„¦) be the ReLU dictionary of atoms Ï† = Ï†(Â·; Î¾, t), Î¾ âˆˆS1 and
t âˆˆ[âˆ’1, 1].
Note that since d = 2, the hyperplane HÏ† associated to the atom Ï† is a line and
LÏ† := HÏ† âˆ©â„¦is a line segment whose length is |LÏ†| = (1 âˆ’t2)1/2. We deï¬ne the weight of this
atom by
w(Ï†) = w(Ï†(Â·; Î¾, t)) := 1 âˆ’t,
t âˆˆ[âˆ’1, 1].
(5.1)
It is easy to check that this weight is admissible since âˆ¥Ï†(Â·; Î¾, t)âˆ¥L2(â„¦) â‰ˆ(1 âˆ’t)7/4 (see (4.3)).
We ï¬rst want to prove results on the linear approximation of the atoms Ï†. Namely, for each
n = 1, 2, . . . , we want to construct an n dimensional linear space Xn which is good at approxating
all of the atoms Ï† âˆˆD(â„¦).
The linear space Xn will be the span of n well chosen atoms Ï†j,
j = 1, . . . , n, from D(â„¦). The construction we give for Xn is a modiï¬cation of ideas from [30]. Our
analysis of the approximation error in approximating Ï† by the elements of Xn is new in that it
gives an improved error estimate when the support of Ï† is near the boundary of â„¦.
9

To deï¬ne the space Xn := span{Ï†1, . . . , Ï†n}, we want to choose the atoms Ï†j, j = 1, . . . , n,
to have as a special discrete distribution from D. In the case d = 2, these atoms are rather easy
to describe geometrically as is given in the next paragraph.
When d > 2, we will need more
sophisticated arguments (see Â§6).
We ï¬x m â‰¥4 and let P = Pm be the set of points
Âµj = Âµj(m) := (cos Î¸j, sin Î¸j),
Î¸j = Î¸j,m := 2Ï€j
m , j âˆˆZ.
(5.2)
There are m distinct points and Âµj = Âµjâ€² if j and jâ€² are congruent modulo m, i.e., if j â‰¡jâ€². These
points are equally spaced on the circle.
Let Xn be the linear space spanned by the dictionary elements Ï† whose line segment LÏ† has end
points Âµi and Âµj, 1 â‰¤i < j â‰¤m. Notice that for each pair i, j there are two such atoms. Hence,
the dimension of Xn is n := m(m âˆ’1). We also note that Xn contains all linear functions on â„¦.
Given i, j âˆˆZ, we deï¬ne the distance between i and j by
d(i, j) := min{|iâ€² âˆ’jâ€²| : i â‰¡iâ€², j â‰¡jâ€²},
i.e. to be the periodic distance between the indices i and j.
Let Li,j = Li,j(m) be the set of all line segments L whose end points a, b are the points
(cos Î¸, sin Î¸) where Î¸ âˆˆ[Î¸i, Î¸i+1] in the case of a and Î¸ âˆˆ[Î¸j, Î¸j+1] in the case of b. We denote by
Si,j = Si,j(m) the union of all the line segments LÏ† in Li,j.
Note that the length Li,j and width Wi,j of Si,j satisfy
|Lij| â‰ˆd(i, j) + 1
m
,
|Wij| â‰ˆd(i, j) + 1
m2
,
1 â‰¤i â‰¤j â‰¤m.
(5.3)
Here and later in this section, all constants of equivalence are absolute. It follows that the measure
of Si,j satisï¬es
|Si,j| â‰²(d(i, j) + 1)2
m3
,
1 â‰¤i â‰¤j â‰¤m.
(5.4)
Lemma 5.1. Suppose that m â‰¥4 is an even integer, n = m(m âˆ’1), and Ï† = Ïƒ(Â·; Î¾, t) is any
dictionary element whose line segment LÏ† is in Li,j = Li,j(m) with Âµi Ì¸= Âµj. Then there is a
function g âˆˆXn such that
(i) Ï†(x) = g(x), x /âˆˆSi,j,
(ii) âˆ¥Ï† âˆ’gâˆ¥Lâˆ(â„¦) â‰¤C d(i,j)
m2 , with C an absolute constant.
(iii) âˆ¥Ï† âˆ’gâˆ¥L2(â„¦) â‰¤Cw(Ï†)nâˆ’3/4, with C an absolute constant.
If Ï† âˆˆLi,i for some i, then there is a g âˆˆXn such that statement (iii) holds.
Proof: We ï¬rst assume that 0 â‰¤i < j â‰¤m. Also, by reversing the roles of i and j if necessary, we
can also assume that j < m/2. Because of rotational symmetry we can assume that i = 0, i+1 = 1,
0 < j < m/2. Consider the linear function â„“(x) := Î¾ Â· x âˆ’t. Let the line segment LÏ† = HÏ† âˆ©â„¦
associated with Ï† be in Li,j. Let Âµi = Âµi(m), i âˆˆZ. We use the following three functions Ï†1, Ï†2, Ï†3
in Xn each of whose line segments LÏ†i are contained in Li,j. Here, LÏ†1 has endpoints Âµi, Âµj+1,
the second segment LÏ†2 has end points Âµi, Âµj, and the third function Ï†3 has line segment LÏ†3 with
endpoints Âµi+1, Âµj+1. The orientation of these three atoms matches that of Ï†. By this we mean
that whenever x âˆˆâ„¦is strictly outside Si,j and Ï†(x) > 0 then each of the functions Ï†i, i = 1, 2, 3,
10

will likewise be positive. Similarly, if x is strictly outside this strip and Ï†(x) = 0 the three functions
Ï†i, i = 1, 2, 3, will likewise vanish.
Consider the three linear functions â„“j, j = 1, 2, 3, corresponding to these line segments. That
is, we have â„“i(x) = Î¾â€²
i Â· x âˆ’tâ€²
i and Ï†i(x) = â„“i(x)+ with Î¾â€²
i âˆˆS1 and tâ€²
i âˆˆ[âˆ’1, 1]. Since these three
linear functions are linearly independent, we can write
â„“= c1â„“1 + c2â„“2 + c3â„“3.
(5.5)
Speciï¬cally, let Î¶ be the point where â„“2(Î¶) = â„“3(Î¶) = 0. Then,
c1 = â„“(Î¶)
â„“1(Î¶),
c2 = â„“(Âµj+1)
â„“2(Âµj+1),
c3 = â„“(Âµi)
â„“3(Âµi).
(5.6)
This follows by noting that with this choice (5.5) holds at the aï¬ƒnely independent set of points Î¶,
Âµj+1 and Âµi.
We claim that
|ci| â‰¤1,
i = 1, 2, 3.
(5.7)
Indeed, since the Î¾, Î¾i lie on the sphere it is clear that |â„“i(x)| = d(x, LÏ†i) and |â„“(x)| = d(x, LÏ†) for
any x âˆˆR2 (here d(x, L) denotes the distance from the point x to the line L). We will show that
d(Âµi, LÏ†) â‰¤d(Âµi, LÏ†3),
(5.8)
which implies |c3| â‰¤1. A completely analogous argument shows that |c2| â‰¤1.
For the proof of (5.8), we assume that j > i + 1. If j = i + 1, a similar argument applies
(which we leave to the reader). Consider the trapezoid whose vertices are Âµi, Âµi+1, Âµj, Âµj+1 and let
T denote its interior. Let Â¯Âµi denote the orthogonal projection of Âµi onto the line LÏ†3. The angle
formed by the vertices Âµj+1, Âµi+1, Âµi is larger than or equal to Ï€/2. This means that Â¯Âµi lies either
on or outside of the circle. By the deï¬ning property of LÏ† this line must intersect the segment
[Âµi, Â¯Âµi]. Therefore, d(Âµi, LÏ†) â‰¤d(Âµi, LÏ†3) which proves (5.8) as desired.
Next, we consider bounding |c1|. The line segments [Âµi, Âµj+1] and [Âµi+1, Âµj] are parallel, and
the intersection point Î¶ lies on the perpendicular line Lp connecting the midpoints of these two line
segments. Moreover, the lengths of these segments satisfy l([Âµi, Âµj+1]) > l([Âµi+1, Âµj]). This means
that the distance from Î¶ to LÏ†1 is greater than the distance to the parallel line segment [Âµi+1, Âµj].
Finally, since LÏ† âˆˆLi,j, LÏ† must intersect Lp, which implies that d(Î¶, LÏ†) â‰¤d(Î¶, LÏ†1). This means
that |c1| â‰¤1 as claimed.
Now, consider the function
g := c1Ï†1 + c2Ï†2 + c3Ï†3,
(5.9)
which is in the linear space Xn. This function agrees with Ï† outside Si,j so that (i) is satisï¬ed.
Each of the functions Ï† and g have Lâˆ(Si,j) norm not exceeding the width Wij â‰¤C d(i,j)
m2
(they are
1-Lipschitz and vanish on one edge) and so the upper bound in (ii) follows. The function Ï† âˆ’g is
supported on Si,j and we have
âˆ¥Ï† âˆ’gâˆ¥L2(â„¦) â‰¤âˆ¥Ï† âˆ’gâˆ¥Lâˆ(â„¦)|Si,j|1/2 â‰¤Cd(i, j)2mâˆ’2âˆ’3/2 â‰¤C|Lij|2mâˆ’1âˆ’1/2.
(5.10)
Note that in this calculation we have use that d(i, j) â‰ˆ(d(i, j) + 1). Since d(i, j) > 1, we easily see
that |Lij| â‰ˆ|LÏ†| = w(Ï†), which veriï¬es (iii).
Finally, if LÏ† is in Li,j with d(i, j) â‰¤1 then the conclusion follows in the same way we proved
(5.10) by taking either v = 0 or v = w Â· x + b to be linear function which matches the linear part
of Ï†.
âœ·
11

5.1
The approximation theorem
Throughout this section En(f) := En(f)L2(â„¦), n â‰¥1 for any f âˆˆL2(â„¦). We can now state the
main theorem to be proved in this section.
Theorem 5.2. Let â„¦= B2 and w(Ï†), Ï† âˆˆD(â„¦), be deï¬ned by (5.1). Then for any f âˆˆVw, we
have
En(f) â‰¤Câˆ¥fâˆ¥Vw(â„¦)nâˆ’5
4,
n â‰¥1,
(5.11)
where C is an absolute constant.
Proof: Since Î£n âŠ‚Î£n+1, n â‰¥0, it is enough to prove the theorem for any n = m(m âˆ’1) with
m â‰¥4 an even integer. This means that we can apply Lemma 5.1. It is enough to prove the
theorem for any function f from U(Vw(â„¦)). According to the deï¬nition of Vw(â„¦), for N suï¬ƒciently
large, there is an S âˆˆÎ£N with S = PN
j=1 ajÏ†j such that
âˆ¥f âˆ’Sâˆ¥L2(â„¦) â‰¤nâˆ’5/4
and
N
X
j=1
w(Ï†j)|aj| â‰¤1.
(5.12)
For each j, let gj âˆˆXn approximate the function Ï†j appearing in the representation of S according
to (iii) of Lemma 5.1. That is, we have
âˆ¥Ï†j âˆ’gjâˆ¥L2(â„¦) â‰¤C0w(Ï†j)nâˆ’3/4,
(5.13)
with C0 an absolute constant. The function g := PN
j=1 ajgj is in Xn and hence in Î£n. We write
f = f âˆ’S + h + g,
h := S âˆ’g.
(5.14)
Therefore,
E3n(f) â‰¤nâˆ’5/4 + E2n(h).
(5.15)
We want to bound E2n(h). We have h = PN
j=1 aj[Ï†j âˆ’vj]. We consider the dictionary Dâ€² = {Ïˆj}N
j=1
with Ïˆj := w(Ï†j)âˆ’1(Ï†j âˆ’gj). According to (5.12) and (5.13), each Ïˆj has L2(â„¦) norm at most
C0nâˆ’3/4 and h = PN
j=1 câ€²
jÏˆj with PN
j=1 |câ€²
j| â‰¤1. It follows from Maureyâ€™s theorem (see (4.15)) that
h can be approximated by a sum T of n terms from the dictionary Dâ€² with error
âˆ¥h âˆ’Tâˆ¥L2(â„¦) â‰¤Cnâˆ’3/4nâˆ’1/2 = Cnâˆ’5/4,
(5.16)
with C an absolute constant.
The function T is a sum of at most 2n terms from the original
dictionary D. Hence,
E2n(h) â‰¤Cnâˆ’5/4.
(5.17)
If we place this inequality back into (5.15), we obtain
E3n(f) â‰¤[1 + C]nâˆ’5/4
(5.18)
and the theorem follows.
âœ·
We close this section by again emphasizing that Theorem 5.2 is an improvement on the known
theorem that any f âˆˆV(D) satisï¬es En(f) â‰¤Cnâˆ’5/4 because the weighted variation space Vw is
strictly larger than the standard variation space V.
12

5.2
Weighted variation spaces for â„¦= Q2
Although we do not formulate a general result, it will be clear that the techniques of this paper can
be generalized to any convex domain â„¦. In this section, we want to point out what such a result is
for Q2 := [âˆ’1, 1]2 since this will allow us to see the eï¬€ect of the geometry of â„¦. So, in going further
in this section, we take â„¦= Q2.
If Ï† is a ReLU atom, then the line segment LÏ† relative to â„¦is HÏ† âˆ©â„¦. The length |LÏ†| can
now be large even if LÏ† is close to the boundary of â„¦, for example when LÏ† is parallel to one of
the sides of â„¦. In other words, many fewer atoms Ï† will have small |LÏ†|.
Let us sketch how the results and analysis for approximating general atoms Ï† given in Â§5.1 for
B2, changes in this case. We now take a set of m âˆ¼âˆšn equally spaced points on the boundary
of Q2. We can associate each Ï† to a Li,j similar to the case of B2 and create a linear space Xn
of dimension m2 âˆ¼n as before. Now the analogue of Lemma 5.1 says that any dictionary element
Ï† can be approximated by an element of g âˆˆXn to an accuracy (corresponding to (iii) in that
lemma)
âˆ¥Ï† âˆ’gâˆ¥L2(â„¦) â‰¤C0mâˆ’1[|LÏ†|mâˆ’1]1/2 = C0mâˆ’3/2|LÏ†|1/2.
(5.19)
Here the factor mâˆ’1 reï¬‚ects the Lâˆerror and the bracketed factor is the measure of the support
where Ï† and g diï¬€er.
Given the above calculations, we deï¬ne w(Ï†) := |LÏ†|1/2 as the weight of the atom Ï† and use
this weight to deï¬ne Vw(Q2). The proof of Theorem 5.2 now gives
Theorem 5.3. Let d = 2 and â„¦= Q2 and deï¬ne w(Ï†) := |LÏ†|1/2. Then for any f âˆˆVw, we have
En(f) â‰¤Câˆ¥fâˆ¥Vw(Q2)nâˆ’5
4 ,
n â‰¥1,
(5.20)
where C is an absolute constant.
6
Approximation in L2(Bd)
We turn now to the case of approximation on the domain â„¦= Bd, d > 2, i.e., â„¦is the unit
Euclidean ball of Rd. Recall that each atom Ï† = Ï†(Â·; Î¾, t), satisï¬es (Î¾, t) âˆˆÂ¯Z(â„¦) = Sdâˆ’1 Ã— [âˆ’1, 1].
To each atom, we assign the special weight
w(Î¾, t) := wâˆ—(Î¾, t) := (1 âˆ’t)
1
2 + d
4 .
(6.1)
From (4.3), we see that this weight is admissible for â„¦. Thus, w is taken to be given by (6.1)
throughout this section.
We recall the variation space Vw introduced and studied in Â§4. The main result of this paper is
the following theorem
Theorem 6.1. If f âˆˆVw = Vw(â„¦) then
En(f) := En(f)L2(â„¦) â‰¤Câˆ¥fâˆ¥Vwnâˆ’1
2 âˆ’3
2d ,
n â‰¥1,
(6.2)
where C depends only on d.
13

Notice that this theorem gives a stronger result than the previously known results on approxima-
tion by shallow neural networks with ReLU activation. Indeed, although the approximation rate
O(nâˆ’1
2âˆ’3
2d ) is the same as known whenever f âˆˆV, the assumption of membership in Vw is a strictly
weaker assumption than the membership in the traditional variation space V.
The remainder of this section is devoted to proving Theorem 6.1. The proof is similar, in spirit,
to the case d = 2 which was given in Theorem 5.2, but it is quite a bit more technical. Our ï¬rst goal
is to construct certain linear spaces Xn of dimension at most n, which can be used to eï¬€ectively
approximate general ReLU atoms. The space Xn will be the span of at most n well chosen atoms
from D(â„¦). The choice of the atoms used to deï¬ne Xn would intuitively be gotten by discretizing
the unit Euclidean sphere Sdâˆ’1 with mdâˆ’1 uniformly spaced vectors and then and to discretize
the oï¬€sets in T = [âˆ’1, 1] with m points. Here m is chosen so that n â‰ˆmd. The discretization
of T will not be uniform but instead will be be done in such a way that atoms whose support is
small, i.e., atoms whose associated hyperplane lies near the boundary Sdâˆ’1 of Bd will be very well
approximated.
Since there is no natural discretization of Sdâˆ’1, when d > 2, we proceed as follows.
Let
Q := Qd := [âˆ’1, 1]d and F be a face of Q. Each face F is gotten by setting one of the coordinates,
say, coordinate i, equal to either +1 or âˆ’1. Given one of these faces F, we shall use dyadic partitions
of F into d âˆ’1 dimensional cubes of side length 2âˆ’k. We let Vk(F) be the set of all vertices of this
partition. Thus, the cardinality of Vk(F) is (2k + 1)dâˆ’1. We deï¬ne Vk to be the union of all of the
sets Vk(F) as F runs through the 2d faces of Q. This gives a discrete set of points on the boundary
of Q with â„“âˆspacing 2âˆ’k. To obtain our discrete set of points on Sdâˆ’1, we simply renormalize.
Namely,
Wk :=

Î¾ =
Â¯Î¾
âˆ¥Â¯Î¾âˆ¥: Â¯Î¾ âˆˆVk

(6.3)
gives a set of points on the boundary of Bd that are quasi-uniformly spaced in the sense that
c02âˆ’k â‰¤dist(Î¾i, Wk \ {Î¾i}) â‰¤C02âˆ’k,
(6.4)
where the constants1 depend only on d. After adjusting for redundancy, we see that the cardinality
of Wk is 2d(2k)dâˆ’1. It is important to note that
Vk âŠ‚Vk+1
and
Wk âŠ‚Wk+1,
k â‰¥1.
(6.5)
We also want to discretize the oï¬€sets t. For this, we take
Tm := {âˆ’1 < t1 < Â· Â· Â· < t2m = 1}.
(6.6)
We take the ï¬rst m of these to be equally spaced in [âˆ’1, 0], i.e., tj := âˆ’1 + j/m, j = 1, . . . , m. For
the remainder of these points, we take
tj+m := cos Ï€(m âˆ’j)
2m
=: cos Î¸j,m,
j = 1, . . . , m.
(6.7)
Notice that the points in Tm have a ï¬ner spacing near one. Concerning this spacing, in going
further we will use the fact that for each m < j < 2m âˆ’1 and t âˆˆ[tj, tj+1] we have
Ï€
q
1 âˆ’t2
j+1
2m
â‰¤|tj+1 âˆ’tj| â‰¤
Ï€
q
1 âˆ’t2
j
2m
and
q
1 âˆ’t2
j â‰¤2
q
1 âˆ’t2
j+1 â‰¤2
p
1 âˆ’t2.
(6.8)
1In this paper, all constants depend only on d and may change from line to line. We use c for small constants and
C for large constants, sometimes with subscripts.
14

To prove this, we note that for ï¬xed j = i + m, 1 â‰¤i < m, we have
|tj+1 âˆ’tj| = Ï€
2m sin Î¶ = Ï€
2m
p
1 âˆ’cos2 Î¶
where Î¶ âˆˆ[Î¸i+1,m, Î¸i,m].
This gives the ï¬rst inequalities in (6.8).
The second inequalities are
proved similarly. The inequalities in (6.8) show that for any given t âˆˆ[tj, tj+1], j â‰¤2m âˆ’2, we
have |tj+1 âˆ’tj| â‰ˆ
âˆš
1 âˆ’t2/m with absolute constants in this comparison. We shall use this fact
repeatedly.
We now want to deï¬ne the linear space Xn. Consider the set of atoms given by
Î¦k,m := {Ï†(Â·; Î¾, t), Î¾ âˆˆWk, t âˆˆTm}.
(6.9)
This is a set of at most 4dm(2k)dâˆ’1 ReLU atoms. We choose k as the largest integer such that
4d2k(2k)dâˆ’1 â‰¤n and then take m = 2k. Then, Î¦n := Î¦k,m is a set of at most n atoms. We deï¬ne
Xn as the linear space
Xn := span(Î¦n),
(6.10)
Then, Xn is a linear space of dimension at most n.
We caution the reader that for the remainder of this paper, the integer n is always taken of the
form n = 4dmd, where m = 2k. It is enough to prove our approximation results for these n.
We now proceed to show that any atom Ï† := Ï†(Â·; Î¾, t) from D(â„¦) can be well approximated by
an element of the linear space Xn. We ï¬x Î¾, t and n. The approximation result we prove is given
in the following theorem.
Theorem 6.2. For any (Î¾, t) âˆˆÂ¯Z(â„¦) = Sdâˆ’1 Ã— [âˆ’1, 1], there is an element g = gÏ† âˆˆXn such that
âˆ¥Ï† âˆ’gâˆ¥L2(â„¦) â‰¤Cw(Ï†)nâˆ’3
2d ,
(6.11)
with the constant C depending only on d.
The proof of this theorem is a bit technical and given in the next subsection. After proving this
theorem we prove Theorem 6.1. In the constructions given below there are two important constants
A and L which depend only on d. It will be useful to the reader if we explain their role and their
deï¬nition. To prove Theorem 6.2, we are presented with an atom Ï†(x) = (Î¾ Â· x âˆ’t)+ and need to
construct an element g âˆˆXn that approximates Ï† to the given accuracy. From the deï¬nition of
Xn, the function g will take the form
g =
n
X
j=1
ajÏ†j,
(6.12)
where the Ï†j are the atoms Ï†j(x) = (Î¾j Â· x âˆ’Ï„j)+ used to deï¬ne Xn, where Ï„j = ti(j). The function
g that we construct to provide the approximation will agree with Ï† except on a certain set of small
measure. The only atoms active in the deï¬nition of g, i.e., which have nonzero coeï¬ƒcients, will
satisfy âˆ¥Î¾ âˆ’Î¾jâˆ¥â‰¤A
m, with A â‰¥2 a ï¬xed integer constant depending only on d. The size of A is
determined by the proof of Lemma 6.6 which is formulated later in this section and then proved
in the Appendix. Hence, in going forward, we can consider d arbitrary but ï¬xed and A depending
only on d to be ï¬xed as well.
The constant L is an integer which is chosen in the proof of Lemma 6.5. We will only have
to consider values of t such that t â‰¤t2mâˆ’L. This restriction can be applied on t because of the
following remark.
15

Remark 6.3. Let us note and record the following:
(i) If t â‰¥t2m or even t â‰¥tmâ€² with mâ€² = 2m âˆ’L with L a ï¬xed integer, then for any atom Ï†(Â·; Î¾, t)
the statement (6.11) holds by simply taking g = 0 and using the estimate (4.3) for the norm of the
atom.
(ii) If t â‰¤C < 1 with C ï¬xed then the weight w(Î¾, t) â‰¥c. In this case, the existence of a space
Xn spanned by n atoms that provides the estimate (6.11) was given in [30]. While our space Xn is
deï¬ned diï¬€erently (we use a diï¬€erent discretization of the oï¬€sets t), the proof in this case is simpler
and we exclude this case going forward.
(iii) If Î¾ is one of the discrete vectors from Wk, then the proof of the existence of a g for which
(6.11) is quite simple. Indeed, one can take g = aÏ†(Â·; Î¾, ti)+(1âˆ’a)Ï†(Â·; Î¾, ti+1) where ti is the closest
discrete oï¬€set to t and a is chosen so that ati + (1 âˆ’a)ti+1 = t.
In the proof of (6.11) we only need to provide a proof in the case that none of the special cases
(i-iii) holds.
6.1
The proof of Theorem 6.2
Obviously, we only need to prove the theorem for m suï¬ƒciently large, say m â‰¥mâˆ—where mâˆ—
depends on d. The integer mâˆ—will be speciï¬ed as we go along. Because of Remark 6.3 we only
need to prove the theorem in the case 1/2 < t â‰¤tmâˆ’L, where L is a ï¬xed integer depending only
on d. Again, we shall specify L as we proceed in the proof. Similarly, we can assume Î¾ is not in
Wk. We ï¬x such an Î¾ âˆˆSdâˆ’1 and such a t throughout this subsection.
We deï¬ne
H+ := {x : Î¾ Â· x â‰¥t}
and
Hâˆ’:= {x : Î¾ Â· x < t}.
(6.13)
So Ï† is identically zero on Hâˆ’and the linear function Î¾ Â· x âˆ’t on H+. For any one of the vectors
Î¾i appearing in the set Wk and any given a tj âˆˆTm, we similarly deï¬ne
H+
j := Hj(Î¾i) := {x âˆˆâ„¦: Î¾i Â· x â‰¥tj},
Hâˆ’
j := Hâˆ’
j (Î¾i) := {x âˆˆâ„¦: Î¾i Â· x < tj}.
(6.14)
Given Î¾i, we want to choose a tj with j â‰¤2m âˆ’1 (depending on i) that is close to t and so that
H+
j is a subset of H+. This is always possible whenever âˆ¥Î¾ âˆ’Î¾iâˆ¥â‰¤A/m and t â‰¤tmâˆ’L and L is
suï¬ƒciently large (depending only on A). One such choice for tj is to take
t+
i := t+(Î¾i) := min{tj âˆˆTm : H+
j âŠ‚H+}.
(6.15)
If t+
i = tj, we let
Ëœti := tj+1.
(6.16)
Then, we will also have H+
j+1 âŠ‚H+.
We now proceed to proving Theorem 6.2. We begin by recalling the following fact.
Lemma 6.4. If Î¾, Î¾â€² âˆˆSdâˆ’1 with âˆ¥Î¾ âˆ’Î¾â€²âˆ¥= Î´, then we have
Î¾ Â· Î¾â€² = 1 âˆ’Î´2/2.
(6.17)
Proof: By rotation, we can assume Î¾ = e1 = (1, 0, . . . , 0) and Î¾â€² = Î±e1 + Î· where Î· is orthogonal
to e1 and âˆ¥Î·âˆ¥2 = 1 âˆ’Î±2. Therefore,
Î´2 = (1 âˆ’Î±)2 + âˆ¥Î·âˆ¥2 = 2 âˆ’2Î± = 2 âˆ’2Î¾ Â· Î¾â€²
16

and so (6.17) follows.
âœ·
The last lemma allows us to compare t+
i with t.
Lemma 6.5. Given the integer A, we deï¬ne
L := (A + 1)2 = A2 + 2A + 1.
(6.18)
If mâˆ—is suï¬ƒciently large, depending only on d and m â‰¥mâˆ—, then whenever t âˆˆ[1/2, t2mâˆ’L] and
âˆ¥Î¾ âˆ’Î¾iâˆ¥â‰¤A
m, then t+
i and Ëœti are well deï¬ned, and we have
t â‰¤t+
i â‰¤Ëœti â‰¤t + C1
m
p
1 âˆ’t2,
(6.19)
where C1 depends only on d.
Proof: Consider ï¬rst the existence of t+
i , Ëœti. It is enough to show that if t â‰¤t2mâˆ’L, and Î¾i satisï¬es
âˆ¥Î¾ âˆ’Î¾iâˆ¥â‰¤
A
m, then there is a j â‰¤2m âˆ’2 such that H+
j
âŠ‚H+. Suppose that j is such that
tj â‰¥t but H+
j is not contained in H+. Then, there is an x = tjÎ¾i + Î· with Î· orthogonal to Î¾i and
âˆ¥Î·âˆ¥â‰¤
q
1 âˆ’t2
j and
x Â· Î¾ = tjÎ¾ Â· Î¾i + Î· Â· (Î¾ âˆ’Î¾i) < t.
From Lemma 6.4, we know that Î¾ Â· Î¾j â‰¥1 âˆ’A2
m2 and so we must have

1 âˆ’A2
m2

tj â‰¤t + âˆ¥Î¾ âˆ’Î¾iâˆ¥
q
1 âˆ’t2
j â‰¤t + A
m
q
1 âˆ’t2
j.
(6.20)
That is, we must have
tj â‰¤t + A
m
q
1 âˆ’t2
j + A2
m2 â‰¤t2mâˆ’L + A
m
q
1 âˆ’t2
j + A2
m2 .
(6.21)
If we write j = 2m âˆ’k, and use the deï¬nition of the tj (see (6.7)) we can rewrite (6.21) as
cos Ï€k
2m âˆ’cos Ï€L
2m â‰¤A
m sin Ï€k
2m + A2
m2 â‰¤Ï€Ak
2m2 + A2
m2 â‰¤A2 + 2Ak
m2
.
(6.22)
The left side of (6.22) is larger than k(Lâˆ’k)
m2
and so we see with the above deï¬nition of L, (6.22) is
violated when k = 2. This proves that t+
i and Ëœti are well deï¬ned.
We turn now to proving (6.19).
First note that if there is j âˆˆ{m + 1, . . . , 2m} such that
H+
j âŠ‚H+, then we must have tjÎ¾i Â· Î¾ â‰¥t which gives the left inequality in (6.19). To prove the
right inequality in (6.19), we let t+
i = tj, Ëœti = tj+1. It follows from the minimality in the deï¬nition
of t+
i that we must have H+
jâˆ’1 is not contained in H+. Thus, the inequality (6.20) holds with j
replaced by j âˆ’1. This gives

1 âˆ’A2
m2

tjâˆ’1 â‰¤t + âˆ¥Î¾ âˆ’Î¾iâˆ¥
q
1 âˆ’t2
jâˆ’1 â‰¤t + 2A
m
q
1 âˆ’t+
i
2,
(6.23)
where the last inequality uses (6.8). From (6.8), we also have
tj â‰¤tjâˆ’1 + (tj âˆ’tjâˆ’1) â‰¤tjâˆ’1 + Ï€
m
q
1 âˆ’t2
j.
17

If we multiply both sides of this last inequality by (1 âˆ’A2
m2 ) and use (6.23) we obtain

1 âˆ’A2
m2

t+
i â‰¤t + 2(A + 2)
m
p
1 âˆ’t2,
where we used that t+
i â‰¥t . We also have Ëœti â‰¤t+
i +
Ï€
2m
âˆš
1 âˆ’t2 because of (6.8). When these facts
are used in the last inequality we obtain the right inequality in (6.19).
âœ·
Now consider any Î¾ âˆˆSdâˆ’1 and deï¬ne
Wk(Î¾) :=

Î¾i âˆˆWk : âˆ¥Î¾ âˆ’Î¾iâˆ¥â‰¤A
m

and
t+ := t+(Î¾) :=
max
Î¾iâˆˆWk(Î¾) t+
i .
(6.24)
We can write t+
i = tj for some j â‰¤2m âˆ’1 and deï¬ne Ëœt := Ëœt(Î¾) := tj+1. From the previous lemma,
we know that
t â‰¤t+ â‰¤Ëœt â‰¤t + C1
m
p
1 âˆ’t2,
(6.25)
where C1 depends only on d.
For the construction of g, we use the following lemma.
Lemma 6.6. There is a constant mâˆ—depending only on d such that the following holds. Given any
m â‰¥mâˆ—and any Î¾ âˆˆSdâˆ’1 and any 1/2 â‰¤t â‰¤t2mâˆ’L, there exists (a+
i ), (Ëœai) such that
(i)
Î¾ = P
Î¾iâˆˆWk(Î¾) a+
i Î¾i + P
Î¾iâˆˆWk(Î¾) ËœaiÎ¾i,
(ii)
t+ P
Î¾iâˆˆWk(Î¾) a+
i + Ëœt P
Î¾iâˆˆWk(Î¾) Ëœai = t,
(iii)
P
Î¾iâˆˆWk(Î¾) |a+
i | + P
Î¾iâˆˆWk(Î¾) |Ëœai| â‰¤C1, where C1 depends only on d.
The proof of this lemma is technical and so we place it in the Appendix so as not to interrupt the
ï¬‚ow of the proof of Theorem 6.2.
We deï¬ne the following function g which will be used to approximate Ï† = Ï†(Â·; Î¾, t) in the case
1/2 â‰¤t â‰¤t2mâˆ’L
g(x) :=
X
Î¾iâˆˆWk(Î¾)
[a+
i (Î¾i Â· x âˆ’t+)+ + Ëœai(Î¾i Â· x âˆ’Ëœt)+]
(6.26)
where the coeï¬ƒcients come from Lemma 6.6. The functions appearing in the representation of g
are all in Xn and therefore g is also in Xn. From Lemma 6.6, we obtain
Ï†(x) âˆ’g(x) =
ï£®
ï£°
X
Î¾iâˆˆWk(Î¾)
a+
i (Î¾i Â· x âˆ’t+) + Ëœai(Î¾i Â· x âˆ’Ëœt)
ï£¹
ï£»
+
âˆ’
X
Î¾iâˆˆWk(Î¾)
[a+
i (Î¾i Â· x âˆ’t+)+ + Ëœai(Î¾i Â· x âˆ’Ëœt)+]
(6.27)
Before bounding the error in approximating Ï† by g, let us make some remarks to motivate the
idea of how to estimate this error. Notice that if x âˆˆâ„¦is such that Î¾i Â· x â‰¥Ëœt for all Î¾i âˆˆWk(Î¾),
then x is also in H+ and so g(x) = Ï†(x). Similarly, if x âˆˆHâˆ’then Ï†(x) = g(x) = 0. This means
that the only points x âˆˆâ„¦where Ï†(x) Ì¸= g(x) must be in one of the sets
Ëœâ„¦i := {x âˆˆâ„¦: Î¾ Â· x > t, Î¾i Â· x â‰¤Ëœt}.
(6.28)
We will now proceed to bound the measure of each of these sets and also bound the error |Ï†(x)âˆ’g(x)|
on each of these sets.
18

Lemma 6.7. There are constants C and mâˆ—depending only on d such that the following holds for
m â‰¥mâˆ—and any 1/2 â‰¤t â‰¤t2mâˆ’L:
(i) If x âˆˆËœâ„¦:= S
Î¾iâˆˆWk(Î¾) Ëœâ„¦i, then
|Ï†(x) âˆ’g(x)| â‰¤C
p
1 âˆ’t2/m.
(6.29)
(ii) The measure of Ëœâ„¦is bounded by
|Ëœâ„¦|d â‰¤C(1 âˆ’t2)d/2/m.
(6.30)
Proof: From Lemma 6.5 we have that
t â‰¤t+ â‰¤Ëœt â‰¤t + C1
âˆš
1 âˆ’t2
m
,
i = 1, . . . , M
(6.31)
where C1 depends only on d. Now, for any ï¬xed i consider any x âˆˆËœâ„¦i. Our goal is to estimate
the distance from x to the hyperplane H := {z : z Â· Î¾ = t}. From the deï¬nition of Wk(Î¾), we have
âˆ¥Î¾ âˆ’Î¾iâˆ¥â‰¤A/m. Since Î± := x Â· Î¾ > t, we can write
x = Î±Î¾ + Î·,
(6.32)
where Î· is orthogonal to Î¾ and
âˆ¥Î·âˆ¥â‰¤
p
1 âˆ’Î±2 â‰¤
p
1 âˆ’t2.
(6.33)
We want to show that Î± cannot be too large. Since x âˆˆËœâ„¦i, we know that
(x Â· Î¾i) = Î±Î¾ Â· Î¾i + Î· Â· Î¾i â‰¤Ëœt,
(6.34)
and
|Î· Â· Î¾i| = |Î· Â· (Î¾i âˆ’Î¾)| â‰¤A
âˆš
1 âˆ’t2
m
.
(6.35)
Using the inequality Î¾ Â· Î¾i â‰¥1 âˆ’A2/m2 (see Lemma 6.4) and (6.35) back in (6.34), we obtain
(1 âˆ’A2mâˆ’2)Î± â‰¤Ëœt + A
âˆš
1 âˆ’t2
m
â‰¤t + C1
âˆš
1 âˆ’t2
m
+ A
âˆš
1 âˆ’t2
m
,
(6.36)
where the last inequality used (6.31). Going further, we note that since by assumption t â‰¤t2mâˆ’L,
we have
(1 âˆ’A2mâˆ’2)âˆ’1 â‰¤1 + 2Amâˆ’2 â‰¤1 + 2A
âˆš
1 âˆ’t2
m
.
Using this estimate back in (6.36), we arrive at
Î± â‰¤t + C2
âˆš
1 âˆ’t2
m
,
(6.37)
with C2 depending only on d. It is important to note that this bound is independent of i. Therefore
any point x in Ëœâ„¦can be written as x = Î±Î¾ + Î· where Î± satisï¬es (6.37) and âˆ¥Î·âˆ¥â‰¤
âˆš
1 âˆ’t2. The
measure of the set of such Î· is â‰¤C3[
âˆš
1 âˆ’t2]dâˆ’1. Hence, we have proven (ii).
19

The inequality (6.37) also shows that
Ï†(x) â‰¤C2
âˆš
1 âˆ’t2
m
,
x âˆˆËœâ„¦.
(6.38)
Since the functions appearing in the sum for g are all smaller than Ï† from (iii) of Lemma 6.6 we
conclude that
|g(x)| â‰¤C3
âˆš
1 âˆ’t2
m
,
x âˆˆËœâ„¦.
(6.39)
This proves (i) and concludes the proof of the lemma.
âœ·
Proof of Theorem 6.2 According to Remark 6.3, we only need to consider the case Ï† = Ï†(Â·; Î¾, t)
when 1/2 â‰¤t < t2mâˆ’L. We return to our representation (6.27). As already mentioned Ï†(x) = g(x)
outside the set Ëœâ„¦. From Lemma 6.7 it follows that
âˆ¥Ï† âˆ’gâˆ¥L2(â„¦) â‰¤C|Ëœâ„¦|1/2
d
âˆš
1 âˆ’t2
m
â‰¤Cw(Ï†)mâˆ’3/2.
(6.40)
Since md â‰¥cdn, we have completed the proof of Theorem 6.2.
âœ·
6.2
Proof of Theorem 6.1
We can now prove Theorem 6.1 in the same way we proved the case d = 2. Let f be any ï¬xed
function from Vw(â„¦). According to the deï¬nition of Vw(â„¦), for N suï¬ƒciently large, there is an
S âˆˆÎ£N with S = PN
j=1 ajÏ†j such that
âˆ¥f âˆ’Sâˆ¥L2(â„¦) â‰¤Mnâˆ’1
2âˆ’3
2d
and
N
X
j=1
w(Ï†j)|aj| â‰¤âˆ¥fâˆ¥Vw =: M.
(6.41)
For each j, let gj âˆˆXn approximate the function Ï†j appearing in the representation of S according
to Theorem 6.2. That is, we have
âˆ¥Ï†j âˆ’gjâˆ¥L2(â„¦) â‰¤Cw(Ï†j)nâˆ’3
2d ,
(6.42)
with C depending only on d. The function g := PN
j=1 ajgj is in Xn and hence in Î£n. We write
f = f âˆ’S + h + g,
h := S âˆ’g =
N
X
j=1
aj[Ï†j âˆ’gj].
(6.43)
Therefore,
E3n(f) â‰¤Mnâˆ’1
2 âˆ’3
2d + E2n(h).
(6.44)
To bound E2n(h), we consider the dictionary Dâ€² = {Ïˆj}N
j=1 with Ïˆj := w(Ï†j)âˆ’1(Ï†j âˆ’gj).
According to Theorem 6.2 each Ïˆj has L2(â„¦) norm at most Cnâˆ’3
3d and h = PN
j=1 cjÏˆj with
PN
j=1 |cj| â‰¤M. It follows from Maureyâ€™s Theorem (see (4.15)) that h can be approximated by a
sum T of n terms from the dictionary Dâ€² with error
âˆ¥h âˆ’Tâˆ¥L2(â„¦) â‰¤CMnâˆ’3
2d nâˆ’1/2 = CMn
1
2âˆ’3
2d .
(6.45)
20

The function T is a sum of at most 2n terms from the original dictionary D. Hence,
E2n(h) â‰¤CMnâˆ’1
2âˆ’3
2d .
(6.46)
If we place this inequality back into (6.44), we obtain
E3n(f) â‰¤CMnâˆ’1
2âˆ’3
2d ,
(6.47)
and the theorem follows.
âœ·
Remark 6.8. If we consider â„¦= Qd in place of Bd then for the weight w(Ï†(Â·; Î¾, t)) = |LÏ†|1/2
where |LÏ†| is the (d âˆ’1)-dimensional measure of the intersection Qd âˆ©LÏ†, we obtain
En(f)L2(Qd) â‰¤Cnâˆ’1
2âˆ’3
2d âˆ¥fâˆ¥Vw,
f âˆˆVw.
(6.48)
We do not give the proof which follows along the lines of the case d = 2 given in Â§5.2.
7
Concluding remarks and implications to neural network training
The main theme of this paper was to introduce, for a bounded domain â„¦âŠ‚Rd, new model classes
Vw := Vw(D(â„¦)), called weighted variation spaces, and to prove bounds on how well functions in
these classes can be approximated by a linear combination of n terms of the ReLU dictionary D(â„¦).
That is, we provided bounds on the error En(f)L2(â„¦) in approximating f âˆˆVw(â„¦) in the L2(â„¦)
norm by the elements of Î£n := Î£n(D(â„¦)) where Î£n is the nonlinear parameterized manifold of
functions g that are a linear combination of n elements of the ReLU dictionary. We showed that
for certain choices of the weight w (dependent on â„¦) the functions in these new model classes have
the same approximation rate as those in the classical variation spaces V(â„¦). Since Vw is strictly
larger than the classical variation spaces V := V(â„¦), this gives stronger results on n-term ReLU
approximation than those in the literature. Thus, these new model classes Vw are important in
trying to understand which functions are well approximated by Î£n.
These approximation theoretic results may also have implications for training and regularization
when using shallow neural networks for learning an unknown function from data observations. We
outline these implications in the remainder of this section. We assume that d â‰¥2 and â„¦= Bd
throughout this section. Therefore, Â¯Z(â„¦) = Sdâˆ’1Ã—[âˆ’1, 1]. Let w be any admissible weight function
in the sense of (4.9) and let Dw be the weighted dictionary deï¬ned in (4.10). We use the results
and notation of Â§4 in going forward. In particular, the functions in Vw := Vw(â„¦) all take the form
(see (4.11))
ËœfÂµ :=
Z
Â¯Z(â„¦)
ËœÏ†(Â·; Î¾, t) dÂµ
and
âˆ¥ËœfÂµâˆ¥Vw(â„¦) = âˆ¥Âµâˆ¥M.
(7.1)
We consider the following data-ï¬tting problem. Suppose that xi, i = 1, . . . , m, are points from
the interior of â„¦and yi, i = 1, . . . , m, are real numbers. The data-ï¬tting problem
inf
fâˆˆVw(â„¦)
m
X
i=1
|yi âˆ’f(xi)|2 + Î» âˆ¥fâˆ¥Vw ,
(7.2)
21

with Î» > 0 is equivalent to the data-ï¬tting problem
inf
ÂµâˆˆM( Â¯Z(â„¦))
m
X
i=1
|yi âˆ’fÂµ(xi)|2 + Î» âˆ¥Âµâˆ¥M ,
(7.3)
in the sense that their inï¬mal values are the same and if Âµâ‹†is a minimizer of (7.3), then fÂµâˆ—is a
minimizer of (7.2). Note that the minimization problem (7.2) does not depend on the ambient space
L2(â„¦) in which we measure error of performance for the approximation problem. An important
property of the weighted variation spaces is the following representer theorem.
Theorem 7.1. Suppose that w is an admissible weight function and the {xi}m
i=1 lie in the interior
of â„¦. Then, there exists a solution f âˆ—to (7.2) that takes the form of a shallow ReLU network
f â‹†(x) =
n
X
j=1
ajÏ†(x; Î¾j, tj) =
n
X
j=1
aj(Î¾j Â· x âˆ’tj)+,
(7.4)
where the number of atoms satisï¬es n â‰¤m, {aj}n
j=1 âŠ‚R \ {0}, and {(Î¾j, tj)}n
j=1 âŠ‚Â¯Z(â„¦) are data-
dependent and not known a priori. Furthermore, the regularization cost is âˆ¥f â‹†âˆ¥Vw = Pn
j=1 w(Î¾j, tj) |aj|.
Proof: Let C( Â¯Z(â„¦)) denote the space of real valued functions on Â¯Z(â„¦). This is a Banach space
when equipped with the Lâˆ-norm. By the Rieszâ€“Markovâ€“Kakutani representation theorem [9,
Chapter 7], the dual of C( Â¯Z(â„¦)) can be identiï¬ed with the space of signed Radon measures M :=
M( Â¯Z(â„¦)). It is well-known that the extreme points of the unit ball
{Âµ âˆˆM : âˆ¥Âµâˆ¥M â‰¤1}
(7.5)
are the Dirac measures Â±Î´(Î¾,t), (Î¾, t) âˆˆÂ¯Z(â„¦) (see, e.g., [5, Proposition 4.1]).
Next, for i = 1, . . . , m, we introduce the functions
hi(Î¾, t) := ËœÏ†(xi; Î¾, t) = Ï†(xi; Î¾, t)
w(Î¾, t) ,
(Î¾, t) âˆˆÂ¯Z(â„¦).
(7.6)
We can rewrite (7.3) as
inf
ÂµâˆˆM
m
X
i=1
|yi âˆ’âŸ¨Âµ, hiâŸ©|2 + Î» âˆ¥Âµâˆ¥M ,
(7.7)
where âŸ¨Â·, Â·âŸ©denotes the duality pairing between C( Â¯Z(â„¦)) and M.
Since the functions hi, i =
1, . . . , m, are in C( Â¯Z(â„¦)), the mappings Âµ 7â†’âŸ¨Âµ, hiâŸ©are weakâˆ—continuous [27, Theorem IV.20, p.
114]. This shows that the hypothesis of the abstract representer theorem of [32, Theorems 2 and 3]
(see also [4, 5]) are satisï¬ed. That theorem shows that there exists a solution to (7.7) that takes
the form of a linear combination of the extreme points of the unit regularization ball. Thus, there
exists a solution that takes the form
Âµâ‹†=
n
X
j=1
cjÎ´(Î¾j,tj),
(7.8)
where the number of atoms satisï¬es n â‰¤m, {cj}n
j=1 âŠ‚R \ {0}, and {(Î¾j, tj)}n
j=1 âŠ‚Â¯Z(â„¦) are
distinct, data-dependent, and not known a priori. Clearly âˆ¥Âµâ‹†âˆ¥M = Pn
j=1 |cj|.
22

From the equivalence between (7.2) and (7.7), we see that the function
fÂµâ‹†=
Z
Â¯Z(â„¦)
ËœÏ†(Â·; Î¾, t)dÂµâ‹†(Î¾, t) =
n
X
j=1
cj
w(Î¾j, tj)Ï†(x; Î¾j, tj)
(7.9)
is a minimizer of (7.2). The theorem follows by the substitution aj := cj/w(Î¾j, tj).
âœ·
We have not indicated the fact that the solution (7.9) to the data ï¬tting problem depends on
Î». If we let Î» tend to zero then the solutions converge to a minimum-norm interpolant f # of the
data
f # âˆˆargmin{âˆ¥fâˆ¥Vw : f(xi) = yi, i = 1, . . . , m}.
(7.10)
In which case, there always exists an f # that has a representation
f # =
n
X
j=1
a#
j Ï†(x; Î¾#
j , t#
j ),
(7.11)
with n â‰¤m.
The theorem statement also holds when the ï¬rst term in the objective in (7.2) is replaced by
any loss function L(Â·, Â·) which is lower semi-continuous (see [22, Proof of Theorem 3.2]). In neural
network parlance, the Î¾j are referred to as the input weights, the aj are referred to as the output
weights and the tj are referred to as the biases.
This representer theorem provides insight into new forms of regularization for neural networks.
Indeed, ï¬rst notice that the norm of a single neuron Ï†(Â·; Î¾, t), where Î¾ âˆˆRd and t âˆˆR, takes the
form
âˆ¥Ï†(Â·; Î¾, t)âˆ¥Vw = âˆ¥Î¾âˆ¥w
 Î¾
âˆ¥Î¾âˆ¥,
t
âˆ¥Î¾âˆ¥

,
(7.12)
where we took advantage of the fact that the ReLU is positively homogeneous of degree 1. In this
form, the input weights are not restricted to be unit norm. The representer theorem (Theorem 7.1)
implies that a solution to the variational problem in (7.2) can be found by training a suï¬ƒciently
wide (ï¬xed width n â‰¥m) neural network to a global minimizer. In particular, by ï¬nding a solution
to the neural network training problem
min
Î¸
m
X
i=1
|yi âˆ’fÎ¸(xi)|2 + Î»
n
X
j=1
|aj| âˆ¥Î¾jâˆ¥w
 Î¾j
âˆ¥Î¾jâˆ¥,
tj
âˆ¥Î¾jâˆ¥

,
(7.13)
where
fÎ¸(x) =
n
X
j=1
ajÏ†(x; Î¾j, tj) =
n
X
j=1
aj(Î¾j Â· x âˆ’tj)+
(7.14)
is a shallow ReLU neural network and Î¸ = (aj, Î¾j, tj)n
j=1 denotes the neural network parameters.
When w is the weight speciï¬ed in (6.1) (which satisï¬es the hypotheses of Theorem 7.1), the
resulting regularizer takes the form
n
X
j=1
|aj| âˆ¥Î¾jâˆ¥

1 âˆ’
tj
âˆ¥Î¾jâˆ¥
 1
2+ d
4
(7.15)
23

This is a new regularizer for training neural networks which directly penalizes the biases. If we
assume the data sites {xi}m
i=1 are drawn i.i.d. uniformly on Bd, then this penalization reï¬‚ects the
volume of the subset of Bd where the neuron is â€œactiveâ€ (nonzero output). This suggests a new,
data-adaptive regularization scheme in which the the penalty on a given neuron is proportional to
the number of data in its support. This regularizer should be contrasted with the unweighted case
in which the regularizer takes the form
n
X
j=1
|aj| âˆ¥Î¾jâˆ¥,
(7.16)
which is sometimes referred to as the path-norm [19] of the neural network. Remarkably, path-
norm regularization is equivalent to the common procedure of training a neural network with weight
decay [13] which corresponds to a regularizer of the form
1
2
n
X
j=1
|aj|2 + âˆ¥Î¾jâˆ¥2 .
(7.17)
We refer the reader to [23] for more details about this equivalence. The new regularizer in (7.15)
requires further study in both theory and practice.
7.1
Open Problems
The results presented in this paper open the door to several new research directions.
(i) We have shown that the classical variation space V(â„¦) is not the approximation space AÎ± =
AÎ±(L2(â„¦)), Î± = 1
2 + 3
2d, since the (strictly larger) weighted variation space Vw(â„¦) admits the
same n-term approximation rate with shallow ReLU networks. Thus, the results of this paper
bring us one step closer to characterizing the approximation space AÎ±, Î± = 1
2 + 3
2d. Future
work will be devoted to ï¬nding a characterization of this approximation space.
(ii) The results of this paper only consider L2-approximation.
We conjecture that the same
rates hold for weighted variation spaces for all Lp, 1 â‰¤p â‰¤âˆ, where now the admissibility
condition on the weights will depend on p. That is to say, for each 1 â‰¤p â‰¤âˆ, there exists a
weight function wâˆ—
p such that the the optimal rate nâˆ’1
2âˆ’3
2d is achieved.
(iii) The weighted variation spaces lead to a new form of data-adaptive regularization for neu-
ral networks. Theoretical and experimental comparisons of this new form of regularization
compared with more conventional regularization techniques is a direction of future work. Fur-
thermore, extensions of this regularizer to deep neural networks is also a direction of future
work.
8
Appendix
In this appendix, we prove Lemma 6.6. We let Î¾ be arbitrary but ï¬xed throughout this section.
We begin by recalling some well known results on the representation of points x in a cube R âŠ‚Rd
in terms of the vertices of R. Given any cube R âŠ‚Rd, we let V (R) denote its set of vertices. Let
24

us ï¬rst consider the case R = U where U := U d := [0, 1]d. We denote the vertices in V (U) by e.
So e is a vector with d components e = (e1, . . . , ed) with each ej âˆˆ{0, 1}. There are 2d such e.
Let
â„“0(s) := (1 âˆ’s)
â„“1(s) = s,
s âˆˆR.
(8.1)
For each e âˆˆV , we deï¬ne
â„“e(x) :=
d
Y
j=1
â„“ej(xj),
x = (x1, . . . , xd) âˆˆU.
(8.2)
Then, â„“e(eâ€²) = 0, eâ€² Ì¸= e and â„“e(e) = 1. Any x âˆˆU is represented as
x =
X
eâˆˆV
â„“e(x)e.
(8.3)
This is a convex representation in that the coeï¬ƒcients â„“e(x) âˆˆ[0, 1], e âˆˆV (U), and they sum to
one.
Now consider an arbitrary cube R âŠ‚Rd. We can write R = v + Î±[0, 1]d = v + Î±U with Î± > 0.
This cube has vertices v + Î±e, e âˆˆV . Any point x = v + Î±y, y âˆˆU, from this cube, has the
representation
x = v + Î±
X
eâˆˆV
â„“e(y)e =
X
eâˆˆV
â„“e(y)[v + Î±e],
(8.4)
because P
eâˆˆV â„“e(y) = 1. Again this is the representation of x as a convex combination of the
vertices V (R) of R. Let us note that here we are taking v as the smallest vertex of R. We can
derive a similar decomposition by starting with any other vertex of R.
We use the above to ï¬nd a variety of representations of any x on the boundary of the cube
Qd := [âˆ’1, 1]d. Later, we shall apply these representations to x = Â¯Î¾ and subsequently to Î¾ âˆˆSdâˆ’1.
Let x be in the face F of Qd. We assume that the d âˆ’1 dimensional face F of Qd corresponds to
x1 = 1. We will derive representations for points x âˆˆF. Similar representations hold for any of
the other faces of Qd.
Any x âˆˆF takes the form x = (1, Ëœx) with Ëœx âˆˆ[âˆ’1, 1]dâˆ’1. Suppose now that R is any d âˆ’1
dimensional cube on F, i.e., R consist of points (1, Ëœx) where Ëœx is in a d âˆ’1 dimensional cube ËœR.
From the above, we can write Ëœx = Ëœv +Î±y, y âˆˆU dâˆ’1, where Ëœv is the smallest vertex of R. Therefore,
we have
Ëœx = Ëœv + Î±y = Ëœv + Î±
X
eâˆˆV (Udâˆ’1)
â„“e(y)e =
X
eâˆˆV (Udâˆ’1)
â„“e(y)[Ëœv + Î±e] =
X
Î½âˆˆV ( ËœR)
Î³Î½Î½,
(8.5)
where the Î½ are the vertices of ËœR and
Î³Î½ = â„“e(y),
when Î½ = Ëœv + Î±e.
(8.6)
This is a representation of Ëœx as a convex combination of the vertices V ( ËœR).
We turn now to representations of Î¾ âˆˆSdâˆ’1. We write Î¾ =
Â¯Î¾
âˆ¥Â¯Î¾âˆ¥where Â¯Î¾ lies on the boundary
of Qd = [âˆ’1, 1]d.
We assume Â¯Î¾ lies on the face F corresponding to ï¬rst coordinate equal to
one. All other cases are handled similarly. We write Â¯Î¾ = (1, Ëœx) with Ëœx âˆˆ[âˆ’1, 1]dâˆ’1 and use the
25

representations of Ëœx given above. Recall the discrete set of points Fk, with m = 2k. If kâ€² < k then
Fkâ€² âŠ‚Fk. We ï¬x such a kâ€² to be chosen in a moment.
We let A â‰¥1 be an integer whose value will be chosen below.
We place ourselves in the
following situation where Ëœx âˆˆËœv + Î´U dâˆ’1 =: ËœR âŠ‚ËœRâ€² := Ëœv + Î´â€²U dâˆ’1 where Ëœv âˆˆFk, Î´ = 2âˆ’k = 1/m
and Î´â€² = 2âˆ’kâ€² = AÎ´ with A = 2kâˆ’kâ€². The assumption that Ëœx âˆˆËœR for which there is such a ËœR
and ËœRâ€² is a restriction on the position of Ëœx in [âˆ’1, 1]dâˆ’1. When this is not the case, the argument
below needs to be adjusted by changing the choice of the initial vertex and the direction for the
representation. Since the adjustment is purely notational, we leave it to the reader.
We will give two representations of Ëœx, respectively Â¯Î¾; the one in terms of the vertices of ËœR and
the second in terms of the vertices of ËœRâ€². For the ï¬rst representation, we use (8.5) with Î± = Î´ to
write
Â¯Î¾ =
X
Î½âˆˆV ( ËœR)
Î³Î½(1, Î½) =
X
Î½âˆˆV ( ËœR)
Î³Î½
p
(1 + âˆ¥Î½âˆ¥2)Î¾Î½,
Î¾Î½ =
(1, Î½)
p
1 + âˆ¥Î½âˆ¥2 ,
(8.7)
with the coeï¬ƒcients Î³Î½ given by (8.6). Notice that the Î¾Î½ are all in Wk. This gives the representation
Î¾ =
X
Î½âˆˆV ( ËœR)
aÎ½Î¾Î½,
aÎ½ := Î³Î½
p
(1 + âˆ¥Î½âˆ¥2)
âˆ¥Â¯Î¾âˆ¥
.
(8.8)
We obtain a second representation as follows. We again write Ëœx = Ëœv + Î´y with y âˆˆU dâˆ’1. Then,
Ëœx = Ëœv +Î´
X
eâˆˆV (Udâˆ’1)
â„“e(y)e = Ëœv +
X
eâˆˆV (Udâˆ’1)
â„“e(y)
A AÎ´e =

1 âˆ’1
A

Ëœv +
X
eâˆˆV (Udâˆ’1)
â„“e(y)
A
[Ëœv +AÎ´e]. (8.9)
This gives the representation
Ëœx =
X
Î½âˆˆV ( ËœRâ€²)
Î³â€²
Î½Î½,
(8.10)
where
Î³â€²
Î½ = â„“e(y)
A
,
when Î½ = Ëœv + AÎ´e with e Ì¸= 0 and Î³â€²
0 = 1 âˆ’1
A + â„“0(y)
A
.
(8.11)
Notice that this representation of Ëœx is again a convex combination of the vertices of ËœRâ€². It follows
that
Î¾ =
X
Î½âˆˆV ( ËœRâ€²)
aâ€²
Î½Î¾â€²
Î½,
aâ€²
Î½ := Î³â€²
Î½
p
(1 + âˆ¥Î½âˆ¥2)
âˆ¥Â¯Î¾âˆ¥
.
Î¾â€²
Î½ =
(1, Î½)
p
1 + âˆ¥Î½âˆ¥2
(8.12)
We now want to estimate the sums
S :=
X
Î½âˆˆV ( ËœR)
aÎ½,
Sâ€² =
X
Î½âˆˆV ( ËœRâ€²)
aâ€²
Î½,
(8.13)
Lemma 8.1. There is an mâˆ—= mâˆ—(d), depending only on d, such that whenever m â‰¥mâˆ—and A
is suï¬ƒciently large (depending only on d), the following hold. Whenever Î¾ is not a vertex in Wk,
i.e., Â¯Î¾ = (1, Ëœx) where Ëœx = Ëœv + y where y Ì¸= 0, we have
S = 1 + Ç«
and
Sâ€² = 1 + Ç«â€²,
where
0 < 2|Ç«| < |Ç«â€²| â‰¤ÏƒÎ´2,
(8.14)
26

and
Ïƒ := Ïƒ(y) =
X
eÌ¸=0
â„“e(y) > 0,
(8.15)
where the strict inequality holds because y Ì¸= 0.
Proof: Let B2 := 1 + âˆ¥Ëœvâˆ¥2 and recall that Î´ := 1/m.
Observe that when Î½ = Ëœv + aÎ´e,
e âˆˆV (U dâˆ’1), we have
1 + âˆ¥Î½âˆ¥2 = B2 + 2aÎ´âŸ¨Ëœv, eâŸ©+ a2Î´2âˆ¥eâˆ¥2 = B2 + sÎ½(a),
(8.16)
where
sÎ½(a) := 2aÎ´âŸ¨Ëœv, eâŸ©+ a2Î´2âˆ¥eâˆ¥2.
(8.17)
We are interested in the cases, a = 1, A. Notice that sÎ½(a) = 0 when e = 0, i.e., Î½ = Ëœv, and
also |sÎ½(a)| â‰¤1/2 for these two values of a provided mâˆ—is large enough. These facts will be used
without further mention in what follows.
We will use the Taylor expansion of the function F(s) :=
âˆš
B2 + s. We have
F(s) = B + 1
2Bâˆ’1s âˆ’1
4Bâˆ’3s2 + O(s3),
|s| < 1.
(8.18)
This gives that
p
1 + âˆ¥Î½âˆ¥2 = F(sÎ½(a)) = B + 1
2Bâˆ’1sÎ½(a) âˆ’1
4Bâˆ’3sÎ½(a)2 + O(sÎ½(a)3)
(8.19)
From the above observations, we can write
âˆ¥Â¯Î¾âˆ¥Sâ€²
=
X
Î½âˆˆV ( ËœRâ€²)
Î³â€²
Î½F(sÎ½(A))
=
(1 âˆ’1/A)B +
X
eâˆˆV (Udâˆ’1)
â„“e(y)
A

B + 1
2Bâˆ’1sÎ½(A) âˆ’1
4Bâˆ’3sÎ½(A)2 + O(sÎ½(A)3)

=
B + Bâˆ’1
2
X
eâˆˆV (Udâˆ’1)
â„“e(y)sÎ½(A)
A
âˆ’Bâˆ’3
4
X
eâˆˆV (Udâˆ’1)
â„“e(y)sÎ½(A)2
A
+ O(A2ÏƒÎ´3).
(8.20)
Let us analyze the ï¬rst sum Î£1 in (8.20). Using the deï¬nition of the sÎ½, we see that this sum
equals
Î£1 = C1Î´ + C2AÎ´2,
where
C1 = Bâˆ’1 X
eÌ¸=0
â„“e(y)âŸ¨Ëœv, eâŸ©and C2 = Bâˆ’1
2
X
eÌ¸=0
â„“e(y)âˆ¥eâˆ¥2.
(8.21)
A similar analysis of the second sum Î£2 gives
Î£2 = C3AÎ´2 + C4A2Î´3 + C5A3Î´4,
where
C3 = Bâˆ’3 X
eÌ¸=0
â„“e(y)âŸ¨Ëœv, eâŸ©2,
and |C4|, |C5| â‰¤C0Ïƒ,
(8.22)
where C0 depends at most on d. In total, this gives
âˆ¥Â¯Î¾âˆ¥Sâ€² = B + C1Î´ + ËœCÏƒAÎ´2 + O(ÏƒA2Î´3),
(8.23)
27

where
Ïƒ ËœC = C2 + C3,
(8.24)
and where the constants in the â€Oâ€ term depend only on d. It is important to notice that
ËœC â‰¥Ïƒâˆ’1C2 â‰¥1/4.
(8.25)
Replacing A by one, we get
âˆ¥Â¯Î¾âˆ¥S = B + C1Î´ + ËœCÏƒÎ´2 + O(ÏƒÎ´3).
(8.26)
Notice that these constants are the same as those in (8.23) and again the constants in the â€Oâ€
term depend only on d.
Next, we want to compute âˆ¥Â¯Î¾âˆ¥and compare this number with B + C1Î´. We have Â¯Î¾ = (1, Ëœx)
where
Ëœx = Ëœv + Î´y = Ëœv + Î´
X
eâˆˆV (Udâˆ’1)
â„“e(y)e.
Therefore,
âˆ¥ËœÎ¾âˆ¥2 = 1 + âˆ¥Ëœvâˆ¥2 + 2Î´
X
eâˆˆV (Udâˆ’1)
â„“e(y)âŸ¨Ëœv, eâŸ©+ Î´2âˆ¥yâˆ¥2 = B2 + s.
(8.27)
If we now use (8.18), we obtain
âˆ¥Â¯Î¾âˆ¥= F(s) = B + Bâˆ’1
2 s âˆ’Bâˆ’3
4 s2 + O(s3) = B + C1Î´ + ËœCâ€²ÏƒÎ´2 + O(ÏƒÎ´3).
where
Ïƒ ËœCâ€² = Bâˆ’1
2 Î´2âˆ¥yâˆ¥2 + Bâˆ’3
ï£®
ï£°X
eÌ¸=0
â„“e(y)âŸ¨Ëœv, eâŸ©
ï£¹
ï£»
2
Î´2.
(8.28)
Here, we have also used the fact that âˆ¥yâˆ¥â‰¤Ïƒ. If we use this expression for âˆ¥Â¯Î¾âˆ¥in (8.26), we obtain
S = 1 + Câˆ—ÏƒÎ´2 + O(ÏƒÎ´3) =: 1 + Îµ,
Câˆ—= ËœC âˆ’ËœCâ€².
(8.29)
Similarly
Sâ€² = 1 + Câˆ—âˆ—ÏƒÎ´2 + O(ÏƒA2Î´3) =: 1 + Îµâ€²,
Câˆ—âˆ—= ËœCA2 âˆ’ËœCâ€².
(8.30)
If we choose m suï¬ƒciently large (m â‰¥mâˆ—with mâˆ—depending only on d and A as a suï¬ƒciently
large integer depending only on d we will have 0 < 2Îµ < Îµâ€² (see (8.25)). This completes the proof
of the Lemma.
âœ·
Note that the constant A of this lemma serves to deï¬ne A for this paper and then L = (A + 1)2
is deï¬ned as in Lemma 6.5.
Proof of Lemma 6.6:
Case Î¾ âˆˆWk: Let Î¾ = Î¾i âˆˆWk. Given t âˆˆ[1/2, t2mâˆ’L], we have t+
i = tj and we take Ëœti := tj+1.
We deï¬ne Î± by the requirement
Î±t+
i + (1 âˆ’Î±)Ëœti = t,
i.e.
Î± = t âˆ’Ëœti
t+
i âˆ’Ëœti
.
(8.31)
28

Then, Î¾ = Î±Î¾ + (1 âˆ’Î±)Î¾, which is the decomposition for Î¾ required in Lemma 6.6. Indeed, |Î±| â‰¤C
with C depending only on d because of Lemma 6.5 and (6.8).
Case Î¾ is not in Wk: We will use the constructions given above. We take A to be an integer as
given in Lemma 8.1. We have given two ways of representing Î¾ as given in (8.8) and (8.12). The
Î¾Î½ and Î¾â€²
Î½ appearing in these representations are all from Wk. We take Wk(Î¾) as the collection of
all these points. Property (ii) of Lemma 6.6 is satisï¬ed since âˆ¥Î¾ âˆ’Î¾iâˆ¥â‰¤A/m for each i. We deï¬ne
Î± by the requirement
Î±Ç« + (1 âˆ’Î±)Ç«â€² = 0,
i.e.
Î± =
Ç«â€²
Ç«â€² âˆ’Ç«.
(8.32)
It follows that
Î¾ = Î±
X
Î½âˆˆV (R)
aÎ½Î¾Î½ + (1 âˆ’Î±)
X
Î½âˆˆV (Râ€²)
aâ€²
Î½Î¾â€²
Î½, =
M
X
j=1
bjÎ¾j,
M
X
j=1
bj = 1,
(8.33)
where all of the Î¾j are in W(Î¾). The key here is that the coeï¬ƒcients in this representation sum to
one.
Now, given t âˆˆ[1/2, tmâˆ’L], we deï¬ne
t+ := max{t+
i : Î¾i âˆˆWk(Î¾)} = tj,
Ëœt := tj+1.
(8.34)
Similar to the above, we deï¬ne Î² by requiring that
Î²t+ + (1 âˆ’Î²)Ëœt = t,
i.e.
Î² = t âˆ’t+
t+ âˆ’Ëœt.
(8.35)
It follows that
Î¾ Â· x âˆ’t =
M
X
j=1
Î²bj(Î¾j Â· x âˆ’t+) +
M
X
j=1
(1 âˆ’Î²)bj(Î¾j Â· x âˆ’Ëœt).
(8.36)
This is the decomposition promised in Lemma 6.6 and thereby completes the proof of the lemma.
âœ·
References
[1] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of
Machine Learning Research, 18(1):629â€“681, 2017.
[2] Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory, 39(3):930â€“945, 1993.
[3] Helmut Bolcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approxima-
tion with sparsely connected deep neural networks. SIAM Journal on Mathematics of Data
Science, 1(1):8â€“45, 2019.
[4] Claire Boyer, Antonin Chambolle, Yohann De Castro, Vincent Duval, FrÂ´edÂ´eric de Gournay,
and Pierre Weiss.
On representer theorems and convex regularization.
SIAM Journal on
Optimization, 29(2):1260â€“1281, 2019.
29

[5] Kristian Bredies and Marcello Carioni.
Sparsity of solutions for variational inverse prob-
lems with ï¬nite-dimensional data. Calculus of Variations and Partial Diï¬€erential Equations,
59(1):Paper No. 14, 26, 2020.
[6] Albert Cohen, Ronald DeVore, Guergana Petrova, and Przemyslaw Wojtaszczyk.
Optimal
stable nonlinear approximation. Foundations of Computational Mathematics, 22(3):607â€“648,
2022.
[7] Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta
Numerica, 30:327â€“444, 2021.
[8] Ronald A. DeVore. Nonlinear approximation. Acta Numerica, 7:51â€“150, 1998.
[9] Gerald B. Folland. Real analysis: Modern techniques and their applications. Pure and Applied
Mathematics (New York). John Wiley & Sons, Inc., New York, second edition, 1999.
[10] RÂ´emi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender.
Approximation
spaces of deep neural networks. Constructive Approximation, 55(1):259â€“367, 2022.
[11] Lee K. Jones. A simple lemma on greedy approximation in Hilbert space and convergence
rates for projection pursuit regression and neural network training. The Annals of Statistics,
pages 608â€“613, 1992.
[12] Jason M. Klusowski and Andrew R. Barron. Approximation by combinations of ReLU and
squared ReLU ridge functions with â„“1 and â„“0 controls. IEEE Transactions on Information
Theory, 64(12):7649â€“7656, 2018.
[13] Anders Krogh and John Hertz. A simple weight decay can improve generalization. Advances
in neural information processing systems, 4, 1991.
[14] Vera KurkovÂ´a and Marcello Sanguineti. Bounds on rates of variable-basis and neural-network
approximation. IEEE Transactions on Information Theory, 47(6):2659â€“2665, 2001.
[15] Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for
smooth functions. SIAM Journal on Mathematical Analysis, 53(5):5465â€“5506, 2021.
[16] Chao Ma, Lei Wu, et al. The barron space and the ï¬‚ow-induced function spaces for neural
network models. Constructive Approximation, 55(1):369â€“406, 2022.
[17] Yuly Makovoz. Uniform approximation by neural networks. Journal of Approximation Theory,
95(2):215â€“228, 1998.
[18] Hrushikesh Narhar Mhaskar. On the tractability of multivariate integration and approximation
by neural networks. Journal of Complexity, 20(4):561â€“590, 2004.
[19] Behnam Neyshabur, Russ R. Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized
optimization in deep neural networks. Advances in neural information processing systems, 28,
2015.
[20] Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of
bounded norm inï¬nite width ReLU nets: The multivariate case. In International Conference
on Learning Representations, 2020.
30

[21] Rahul Parhi and Robert D. Nowak. Banach space representer theorems for neural networks
and ridge splines. Journal of Machine Learning Research, 22(43):1â€“40, 2021.
[22] Rahul Parhi and Robert D. Nowak. What kinds of functions do deep neural networks learn? In-
sights from variational spline theory. SIAM Journal on Mathematics of Data Science, 4(2):464â€“
489, 2022.
[23] Rahul Parhi and Robert D. Nowak.
Deep learning meets sparse regularization: A signal
processing perspective. arXiv preprint arXiv:2301.09554, 2023.
[24] Rahul Parhi and Robert D. Nowak. Near-minimax optimal estimation with shallow ReLU
neural networks. IEEE Transactions on Information Theory, 69(2):1125â€“1140, 2023.
[25] Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica,
8:143â€“195, 1999.
[26] Gilles Pisier.
Remarques sur un rÂ´esultat non publiÂ´e de B. Maurey.
SÂ´eminaire dâ€™Analyse
Fonctionnelle (dit â€œMaurey-Schwartzâ€), pages 1â€“12, April 1981.
[27] Michael Reed and Barry Simon. Methods of Modern Mathematical Physics I: Functional anal-
ysis. Academic Press, 1972.
[28] Zuowei Shen, Haizhao Yang, and Shijun Zhang. Optimal approximation rate of relu networks
in terms of width and depth. Journal de MathÂ´ematiques Pures et AppliquÂ´ees, 157:101â€“135,
2022.
[29] Jonathan W. Siegel. Optimal approximation rates for deep ReLU neural networks on Sobolev
spaces. arXiv preprint arXiv:2211.14400, 2022.
[30] Jonathan W. Siegel and Jinchao Xu. Sharp bounds on the approximation rates, metric entropy,
and n-widths of shallow neural networks. Foundations of Computational Mathematics, pages
1â€“57, 2022.
[31] Jonathan W. Siegel and Jinchao Xu. Characterization of the variation spaces corresponding
to shallow neural networks. Constructive Approximation, pages 1â€“24, 2023.
[32] Michael Unser. A unifying representer theorem for inverse problems and machine learning.
Foundations of Computational Mathematics, 21(4):941â€“960, 2021.
[33] Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Net-
works, 94:103â€“114, 2017.
Ronald DeVore, Department of Mathematics, Texas A&M University, College Station, TX 77843
Robert D. Nowak, Department of Electrical and Computer Engineering, University of Wisconsinâ€“
Madison, Madison, WI 53706
Rahul Parhi, Biomedical Imaging Group, Â´Ecole polytechnique fÂ´edÂ´erale de Lausanne, CH-1015 Lau-
sanne, Switzerland
Jonathan W. Siegel, Department of Mathematics, Texas A&M University, College Station, TX
77843
31

