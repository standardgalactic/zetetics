
Bayesian Modeling 
Using WinBUGS 

WILEY SERIES IN COMPUTATIONAL STATISTICS 
Consulting Editors: 
Paolo Giudici 
University of Pavia, Italy 
Geof H. Givens 
Colorado State University, USA 
Bani K. Mallick 
Texas A&M University, USA 
Wiley Series in Computational Statistics is comprised of practical guides and cut- 
ting edge research books on new developments in computational statistics. It fea- 
tures quality authors with a strong applications focus. The texts in the series provide 
detailed coverage of statistical concepts, methods and case studies in areas at the in- 
terface of statistics, computing, and numerics. 
With sound motivation and a wealth of practical examples, the books show in 
concrete terms how to select and to use appropriate ranges of statistical computing 
techniques in particular fields of study. Readers are assumed to have a basic under- 
standing of introductory terminology. 
The series concentrates on applications of computational methods in statistics to 
fields of bioinformatics, genomics, epidemiology, business, engineering, finance 
and applied statistics. 
A complete list of titles in this series appears at the end of the volume. 

Bayesian Modeling 
Using WinBUGS 
Ioannis Ntzoufras 
Department of Statistics 
Athens University of Economics and Business 
Athens, Greece 
WILEY 
A JOHN WILEY & SONS, INC., PUBLICATION 

Copyright Q 2009 by John Wiley & Sons, Inc. All rights reserved 
Published by John Wiley & Sons, Inc., Hoboken, New Jersey. 
Published simultaneously in Canada. 
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or 
by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to 
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax 
(978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should 
be addressed to the Permissions Department, John Wiley & Sons, Inc., 11 1 River Street, Hoboken, NJ 
07030, (201) 748-601 1, fax (201) 748-6008, or online at http://www.wiley.com/go/permission. 
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representations or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales 
representatives or written sales materials. The advice and strategies contained herein may not be suitable 
for your situation. You should consult with a professional where appropriate. Neither the publisher nor 
author shall be liable for any loss of profit or any other commercial damages, including but not limited 
to special, incidental, consequential, or other damages. 
For general information on our other products and services or for technical support, please contact our 
Customer Care Department within the United States at (800) 762-2974, outside the United States at 
(317) 572-3993 or fax (317) 572-4002. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may 
not be available in electronic format. For information about Wiley products, visit our web site at 
www.wiley.com. 
Library of Congress Cataloging-in-Publication Data is available. 
Ntzoufras, Ioannis, 1973- 
Bayesian modeling using WinBUGS / Ioannis Ntzoufras. 
Includes bibliographical references and index. 
1. Bayesian statistical decision theory. 2. WinBUGS. I. Title. 
QA279.5.N89 2009 
5 19.5'42Ac22 
2008033316 
p. cm. 
ISBN 978-0-470-141 14-4 (pbk.) 
Printed in the United States of America 
1 0 9 8 7 6 5 4 3 2 1  

To Ioanna and our baby daughter 

CONTENTS 
Preface 
Acknowledgments 
Acronyms 
1 Introduction to Bayesian Inference 
1.1 
1.2 Definition of statistical models 
1.3 Bayes theorem 
1.4 Model-based Bayesian inference 
1.5 
Introduction: Bayesian modeling in the 2 1 st century 
Inference using conjugate prior distributions 
1.5.1 
1.5.2 
1.5.3 
1.5.4 
1.5.5 
1.5.6 
Other conjugate prior distributions 
1.5.7 
Illustrative examples 
Inference for the Poisson rate of count data 
Inference for the success probability of binomial data 
Inference for the mean of normal data with known variance 
Inference for the mean and variance of normal data 
Inference for normal regression models 
1.6 
Nonconjugate analysis 
Problems 
xvii 
xix 
xxi 
1 
1 
3 
3 
4 
7 
7 
8 
9 
11 
12 
14 
14 
24 
27 
vii 

viii 
CONTENTS 
2 Markov Chain Monte Carlo Algorithms in Bayesian Inference 
2.1 
Simulation, Monte Carlo integration, and their implementation in 
Bayesian inference 
Markov chain Monte Carlo methods 
2.2.1 The algorithm 
2.2.2 
Terminology and implementation details 
2.3 
Popular MCMC algorithms 
2.3.1 
The Metropolis-Hastings algorithm 
2.3.2 
Componentwise Metropolis-Hastings 
2.3.3 
The Gibbs sampler 
2.3.4 
Metropolis within Gibbs 
2.3.5 The slice Gibbs sampler 
2.3.6 
Problems 
2.2 
A simple example using the slice sampler 
2.4 
Summary and closing remarks 
3 WinBUGS Software: Introduction, Setup, and Basic Analysis 
3.1 
Introduction and historical background 
3.2 
The WinBUGS environment 
3.2.1 
Downloading and installing WinBUGS 
3.2.2 
A short description of the menus 
3.3.1 
Code structure and type of parametersinodes 
3.3.2 
Scalar, vector, matrix, and array nodes 
Building Bayesian models in WinBUGS 
3.4.1 
Function description 
3.4.2 
3.4.3 
3.4.4 
3.4.5 
Model specification in WinBUGS 
3.4.6 
3.4.7 
3.4.8 
Data transformations 
Compiling the model and simulating values 
Basic output analysis using the sample monitor tool 
Chapter summary and concluding comments 
Problems 
3.3 
Preliminaries on using WinBUGS 
3.4 
Using the for syntax and array, matrix, and vector calculations 
Use of parentheses, brackets and curly braces in WinBUGS 
Differences between WinBUGS and R/Splus syntax 
Data and initial value specification 
An example of a complete model specification 
3.5 
3.6 
3.7 
Summarizing the procedure 
3.8 
4 WinBUGS Software: Illustration, Results, and Further Analysis 
31 
31 
35 
36 
37 
42 
42 
45 
71 
76 
76 
77 
81 
81 
83 
83 
84 
84 
85 
88 
88 
89 
93 
93 
97 
98 
98 
99 
100 
107 
108 
108 
117 
120 
121 
121 
125 
4.1 
A complete example of running MCMC in WinBUGS for a simple model 
125 

CONTENTS 
ix 
4.1.1 
The model 
4.1.2 
Data and initial values 
4.1.3 
Compiling and running the model 
4.1.4 
MCMC output analysis and results 
Further output analysis using the inference menu 
4.2.1 
Comparison of nodes 
4.2.2 
Calculation of correlations 
4.2.3 
Using the summary tool 
4.2.4 
4.2.5 
4.3.1 
Generation of multiple chains 
4.3.2 
Output analysis 
4.3.3 
The Gelman-Rubin convergence diagnostic 
Changing the properties of a figure 
4.4.1 
General graphical options 
4.4.2 
Special graphical options 
4.5.1 
The node i n f o  tool 
4.5.2 
4.5.3 
4.5.4 
4.5.5 
Problems 
4.2 
Evaluation and ranking of individuals 
Calculation of deviance information criterion 
4.3 
Multiple chains 
4.4 
4.5 
Other tools and menus 
Monitoring the acceptance rate of the Metropolis-Hastings 
algorithm 
Saving the current state of the chain 
Setting the starting seed number 
Running the model as a script 
4.6 
Summary and concluding remarks 
5 introduction to Bayesian Models: Normal Models 
5.1 
General modeling principles 
5.2 
Model specification in normal regression models 
5.2.1 
Specifying the likelihood 
5.2.2 
5.2.3 
5.2.4 
Using vectors and multivariate priors in normal regression models 
5.3.1 
5.3.2 
5.3.3 
5.3.4 
Continuation of Example 5.1 
5.4.1 
The one-way ANOVA model 
5.4.2 
Parametrization and parameter interpretation 
Specifying a simple independent prior distribution 
Interpretation of the regression coefficients 
A regression example using WinBUGS 
Defining the model using matrices 
Prior distributions for normal regression models 
Multivariate normal priors in WinBUGS 
5.3 
5.4 
Analysis of variance models 
125 
127 
127 
129 
132 
133 
136 
137 
138 
140 
141 
141 
142 
143 
145 
145 
145 
148 
148 
148 
149 
149 
149 
149 
150 
151 
151 
152 
153 
154 
154 
157 
161 
161 
162 
163 
164 
167 
167 
168 

CONTENTS 
X 
6 
7 
5.4.3 
5.4.4 
5.4.5 
Two-way ANOVA models 
5.4.6 
Multifactor analysis of variance 
Problems 
One-way ANOVA model in WinBUGS 
A one-way ANOVA example using WinBUGS 
169 
171 
173 
184 
184 
Incorporating Categorical Variables in Normal Models and Further 
Modeling Issues 
189 
6.1 
6.2 
Analysis of covariance models 
Analysis of variance models using dummy variables 
6.2.1 
6.2.2 
The parallel lines model 
6.2.3 
The separate lines model 
6.3.1 
Parallel lines analysis 
6.3.2 
Models using one quantitative variable and one qualitative variable 
6.3 
A bioassay example 
Slope ratio analysis: Models with common intercept and different 
slope 
Comparison of the two approaches 
Extending the simple ANCOVA model 
Using binary indicators to specify models in multiple regression 
Selection of variables using the deviance information criterion 
PIC) 
6.3.3 
6.4 
Further modeling issues 
6.4.1 
6.4.2 
6.4.3 
6.5 
Closing remarks 
Problems 
Introduction to Generalized Linear Models: Binomial and Poisson 
Data 
7.1 
Introduction 
7.1.1 
The exponential family 
7.1.2 
7.1.3 
Link functions 
7.1.4 
Common generalized linear models 
7.1.5 
Interpretation of GLM coefficients 
Common distributions as members of the exponential family 
7.2 
Prior distributions 
7.3 
Posterior inference 
7.3.1 
7.3.2 
GLM specification in WinBUGS 
7.4.1 
7.4.2 
The posterior distribution of a generalized linear model 
7.4 
Poisson regression models 
Interpretation of Poisson log-linear parameters 
A simple Poisson regression example 
191 
195 
197 
197 
20 1 
203 
204 
212 
217 
218 
218 
219 
219 
226 
226 
229 
229 
230 
23 1 
234 
236 
23 8 
239 
24 1 
24 1 
242 
242 
242 
245 

CONTENTS 
xi 
7.4.3 A Poisson regression model for modeling football data 
249 
7.5 
binomial response models 
255 
7.5.1 
Interpretation of model parameters in binomial response models 
257 
7.5.2 
A simple example 
263 
7.6 
Models for contingency tables 
269 
Problems 
270 
8 Models for Positive Continuous Data, Count Data, and Other GLM- 
Based Extensions 
8.1 
Models with nonstandard distributions 
8.1.1 
8.1.2 
The inverse Gaussian model 
Models for positive continuous response variables 
8.2.1 
The gamma model 
8.2.2 
Other models 
8.2.3 An example 
Additional models for count data 
8.3.1 
The negative binomial model 
8.3.2 The generalized Poisson model 
8.3.3 
Zero inflated models 
8.3.4 
The bivariate Poisson model 
8.3.5 
The Poisson difference model 
Further GLM-based models and extensions 
8.4.1 
Survival analysis models 
8.4.2 
Multinomial models 
8.4.3 
Problems 
Specification of arbitrary likelihood using the zeros-ones trick 
8.2 
8.3 
8.4 
Additional models and further reading 
9 Bayesian Hierarchical Models 
9.1 
Introduction 
9.1.1 
A simple motivating example 
9.1.2 
9.1.3 
Other advantages and characteristics 
9.2.1 
Repeated measures data 
9.2.2 
9.2.3 
9.2.4 
The generalized linear mixed model formulation 
9.3.1 
9.3.2 
Why use a hierarchical model? 
9.2 
Some simple examples 
Introducing random effects in performance parameters 
Poisson mixture models for count data 
The use of hierarchical models in meta-analysis 
A hierarchical normal model: A simple crossover trial 
Logit GLMM for correlated binary responses 
9.3 
275 
275 
276 
277 
279 
279 
280 
28 1 
282 
283 
286 
288 
29 1 
293 
296 
297 
298 
300 
301 
305 
305 
306 
307 
308 
308 
308 
313 
315 
318 
320 
32 1 
325 

xii 
CONTENTS 
9.3.3 
Discussion, closing remarks, and further reading 
Problems 
Poisson log-linear GLMMs for correlated count data 
9.4 
10 The Predictive Distribution and Model Checking 
10.1 Introduction 
10.1.1 Prediction within Bayesian framework 
10.1.2 Using posterior predictive densities for model evaluation and 
checking 
10.1.3 Cross-validation predictive densities 
10.2 Estimating the predictive distribution for future or missing observations 
using MCMC 
10.2.1 A simple example: Estimating missing observations 
10.2.2 An example of Bayesian prediction using a simple model 
10.3 Using the predictive distribution for model checking 
10.3.1 Comparison of actual and predictive frequencies for discrete data 
10.3.2 Comparison of cumulative frequencies for predictive and actual 
values for continuous data 
10.3.3 Comparison of ordered predictive and actual values for continuous 
data 
10.3.4 Estimation of the posterior predictive ordinate 
10.3.5 Checking individual observations using residuals 
10.3.6 Checking structural assumptions of the model 
10.3.7 Checking the goodness-of-fit of a model 
10.4 Using cross-validation predictive densities for model checking, evaluation, 
and comparison 
10.4.1 Estimating the conditional predictive ordinate 
10.4.2 Generating values from the leave-one-out cross-validatory 
predictive distributions 
10.5 Illustration of a complete predictive analysis: Normal regression models 
10.5.1 Checking structural assumptions of the model 
10.5.2 Detailed checks based on residual analysis 
10.5.3 Overall goodness-of-fit of the model 
10.5.4 Implementation using WinBUGS 
10.5.5 An Illustrative example 
10.5.6 Summary of the model checking procedure 
10.6 Discussion 
Problems 
333 
338 
340 
341 
341 
341 
342 
344 
344 
345 
347 
354 
354 
357 
358 
359 
362 
365 
368 
375 
375 
377 
378 
378 
379 
380 
380 
383 
386 
387 
387 
11 Bayesian Model and Variable Evaluation 
389 

CONTENTS 
11.1 Prior predictive distributions as measures of model comparison: Posterior 
model odds and Bayes factors 
1 1.2 Sensitivity of the posterior model probabilities: The Lindley-Bartlett 
paradox 
1 1.3 Computation of the marginal likelihood 
1 1.3.1 Approximations based on the normal distribution 
11.3.2 Sampling from the prior: A naive Monte Carlo estimator 
11.3.3 Sampling from the posterior: The harmonic mean estimator 
11.3.4 Importance sampling estimators 
11.3.5 Bridge sampling estimators 
11.3.6 Chib’s marginal likelihood estimator 
11.3.7 Additional details and further reading 
11.4 Computation of the marginal likelihood using WinBUGS 
1 1.4.1 A beta-binomial example 
11.4.2 A normal regression example with conjugate normal-inverse 
gamma prior 
1 1.5 Bayesian variable selection using Gibbs-based methods 
1 1.5.1 Prior distributions for variable selection in GLM 
11.5.2 Gibbs variable selection 
11.5.3 Other Gibbs-based methods for variable selection 
xiii 
389 
391 
392 
392 
392 
393 
3 94 
394 
395 
397 
397 
399 
403 
405 
406 
409 
410 
1 1.6 Posterior inference using the output of Bayesian variable selection samplers 412 
1 1.7 Implementation of Gibbs variable selection in WinBUGS using an 
illustrative example 
414 
11.8 The Carlin-Chib method 
419 
11.9 reversible jump MCMC (RJMCMC) 
420 
11.10 Using posterior predictive densities for model evaluation 
42 1 
423 
424 
424 
425 
426 
427 
1 1.10.1 Estimation from an MCMC output 
1 1.10.2 A simple example in WinBUGS 
1 1.1 1.1 The Bayes information criterion (BIC) 
1 1.1 1.2 The Akaike information criterion (AIC) 
1 1.1 1.3 Other criteria 
1 1.1 1.4 Calculation of penalized deviance measures from the MCMC 
output 
428 
1 1.1 1.5 Implementation in WinBUGS 
42 8 
1 1.1 1.6 A simple example in WinBUGS 
429 
432 
Problems 
432 
1 1.1 1 Information criteria 
1 1.12 Discussion and further reading 
Appendix A: Model Specification via Directed Acyclic Graphs: The DOODLE 
Menu 
435 
A. 1 Introduction: Starting with DOODLE 
435 

xiv 
CONTENTS 
A.2 Nodes 
A.3 Edges 
A.4 Panels 
A.5 A simple example 
43 6 
43 8 
43 8 
439 
Appendix B: The Batch Mode: Running a Model in the Background Using 
Scripts 
443 
B. 1 Introduction 
443 
B.2 
Basic commands: Compiling and running the model 
444 
Appendix C: Checking Convergence Using CODNBOA 
C. 1 Introduction 
C.2 A short historical review 
C.3 Diagnostics implemented by CODMBOA 
C.3.1 The Geweke diagnostic 
C.3.2 The Gelman-Rubin diagnostic 
C.3.3 The Raftery-Lewis diagnostic 
C.3.4 The Heidelberger-Welch diagnostic 
C.3.5 Final remarks 
A first look at CODMBOA 
C.4.1 CODA 
C.4.2 BOA 
C.5 A simple example 
(2.4 
C.5.1 Illustration in CODA 
C.5.2 Illustration in BOA 
Appendix D: Notation Summary 
D.l MCMC 
D.2 
Subscripts and indices 
D.3 Parameters 
D.4 Random variables and data 
D.5 Sample estimates 
D.6 
D.7 Distributions 
D.8 Distribution-related notation 
D.9 
D. 10 Variable and model specification 
D. 1 1 Deviance information criterion (DIC) 
D. 12 Predictive measures 
Special functions, vectors, and matrices 
Notation used in ANOVA and ANCOVA 
447 
447 
448 
448 
448 
449 
449 
449 
450 
450 
450 
45 1 
453 
453 
457 
461 
46 1 
462 
462 
463 
463 
464 
464 
465 
466 
466 
466 
467 

References 
Index 
CONTENTS 
XV 
469 
485 

PREFACE 
Since the mid- 1980s, the development of widely accessible powerfbl computers and the 
implementation of Markov chain Monte Carlo (MCMC) methods have led to an explosion 
of interest in Bayesian statistics and modeling. This was followed by an extensive re- 
search for new Bayesian methodologies generating the practical application of complicated 
models used over a wide range of sciences. During the late 199Os, BUGS emerged in the 
foreground. BUGS was a free software that could fit complicated models in a relatively 
easy manner, using standard MCMC methods. Since 1998 or so, WinBUGS , the Windows 
version of BUGS, has earned great popularity among researchers of diverse scientific fields. 
Therefore, an increased need for an introductory book related to Bayesian models and their 
implementation via WinBUGS has been realized. 
The objective of the present book is to offer an introduction to the principles of Bayesian 
modeling, with emphasis on model building and model implementation using WinBUGS . 
Detailed examples are provided, ranging from very simple to more advanced and real- 
istic ones. Generalized linear models (GLMs), which are familiar to most students and 
researchers, are discussed. Details concerning model building, prior specification, writing 
the WinBUGS code and the analysis and interpretation of the WinBUGS output are also 
provided. Because of the introductory character of the book, I focused on elementary mod- 
els, starting from the normal regression models and moving to generalized linear models. 
Even more advanced readers, familiar with such models, may benefit from the Bayesian 
implementation using WinBUGS , 
Basic knowledge ofprobability theory and statistics is assumed. Computations that could 
not be performed in WinBUGS are illustrated using R. Therefore, a minimum knowledge 
of R is also required. 
xvii 

xviii 
PREFACE 
This manuscript can be used as the main textbook in a second-level course of Bayesian 
statistics focusing on modeling and/or computation. Alternatively, it can serve as a compan- 
ion (to a main textbook) in an introductory course of a Bayesian statistics. Finally, because 
of its structure, postgraduate students and other researchers can complete a self-taught 
tutorial course on Bayesian modeling by following the material of this book. 
All datasets and code used in the book are available in the book’s Webpage: www. 
stat-athens.aueb.gr/”jbn/winbugs-book. 
IOANNIS 
NTZOUFRAS 
Athens, Greece 
June 29.2008 

ACKNOWLEDGMENTS 
I am indebted to the people at Wiley publications for their understanding and assistance 
during the preparation of the manuscript. Acknowledgments are due to the anonymous 
referees. Their suggestions and comments led to a substantial improvement of the present 
book. I would particularly like to thank Dimitris Fouskakis, colleague and good friend, for 
his valuable comments on an early version of chapters 1-6 and 10-1 1. I am also grateful 
to Professor Brani Vidakovic for proposing and motivating this book. Last but not least, I 
wish to thank my wife Ioanna for her love, support, and patience during the writing of this 
book as well as for her suggestions on the manuscript. 
I. N. 
xix 

ACRONYMS 
ACF 
AIC 
ANOVA 
ANCOVA 
AR 
BF 
BIC 
BOA 
BP 
BOD 
BUGS 
CDF 
COD 
CODA 
CPO 
CR 
cv 
Autocorrelation 
Akaike information criterion 
Analysis of variance 
Analysis of covariance 
Attributable risk 
Bayes factor 
Bayes information criterion 
Bayesian output analysis (R package) 
Bivariate Poisson 
Biological oxygen demand (data variable in example 6.3) 
Bayesian inference using Gibbs (software) 
Cumulative distribution function 
Chemical oxygen demand (data variable in example 6.3) 
Convergence diagnostics and output analysis software for 
Gibbs sampling analysis (R package) 
Conditional Predictive Ordinate 
corner (constraint) 
Cross-validation 
xxi 

xxii 
Acronyms 
cv- 1 
DAG 
DI 
DIBP 
DIC 
GLM 
GP 
GVS 
ICPO 
i.i.d. 
LS 
MAP 
MP model 
MCMC 
MCE 
ML 
MLE 
NB 
OR 
PBF 
PD 
p.d.f. 
PO 
PPO 
RJMCMC 
RR 
SD 
SE 
ssvs 
STZ 
TS 
TVS 
WinBUGS 
ZI 
ZID 
ZIP 
Leave-one-out cross-validation 
Directed acyclic graph 
Dispersion index 
Diagonal inflated bivariate Poisson distribution 
Deviance information criterion 
Generalized linear model 
Generalized Poisson 
Gibbs variable selection 
Inverse conditional predictive ordinate 
Independent identically distributed 
Logarithmic score 
Maximum a posteriori 
Median probability 
Markov chain Monte Carlo 
Monte Carlo error 
Maximum likelihood 
Maximum-likelihood estimate/estimator 
Negative binomial 
Odds ratio 
Posterior Bayes factor 
Poisson difference 
Probability density function 
Posterior model odds 
Posterior predictive ordinate 
Reversible jump Markov chain Monte Carlo 
Relative risk 
Standard deviation 
Standard error 
Stochastic search variable selection 
sum-to-zero (constraint) 
Total solids(data variable in example 6.3) 
Total volatile solids (data variable in example 6.3) 
Windows version of BUGS (software) 
Zero inflated 
Zero inflated distribution 
Zero inflated Poisson distribution 
ZINB 
Zero inflated negative binomial distribution 

Acronyms 
xxiii 
ZIGP 
ZIBP 
Zero inflated generalized Poisson distribution 
Zero inflated bivariate Poisson distribution 

CHAPTER 1 
INTRODUCTION TO BAYESIAN 
INFERENCE 
1 .I 
INTRODUCTION: BAYESIAN MODELING IN THE 21 ST CENTURY 
The beginning of the 2 1 st century found Bayesian statistics to be fashionable in science. But 
until the late 1980s, Bayesian statistics were considered only as an interesting alternative 
to the “classical” theory. The main difference between the classical statistical theory and 
the Bayesian approach is that the latter considers parameters as random variables that 
are characterized by a prior distribution. This prior distribution is combined with the 
traditional likelihood to obtain the posterior distribution of the parameter of interest on 
which the statistical inference is based. Although the main tool of Bayesian theory is 
probability theory, for many years Bayesians were considered as a heretic minority for 
several reasons. The main objection of “classical” statisticians was the subjective view point 
of the Bayesian approach introduced in the analysis via the prior distribution. However, 
as history had proved, the main reason why Bayesian theory was unable to establish a 
foothold as a well accepted quantitative approach for data analysis was the intractabilities 
involved in the calculation of the posterior distribution. Asymptotic methods had provided 
solutions to specific problems, but no generalization was possible. Until the early 1990s two 
groups of statisticians had (re)discovered Markov chain Monte Carlo (MCMC) methods 
(Gelfand and Smith, 1990; Gelfand et al., 1990). Physicists were familiar with MCMC 
methodology from the 1950s. Nick Metropolis and his associates had developed one of the 
first electronic supercomputers (for those days) and had been testing their theories in physics 
using Monte Carlo techniques. Implementation ofthe MCMC methods in combination with 
the rapid evolution of personal computers made the new computational tool popular within 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
1 

2 
INTRODUCTION TO BAYESIAN INFERENCE 
a few years. Bayesian statistics suddenly became fashionable, opening new highways for 
statistical research. Using MCMC, we can now set up and estimate complicated models 
that describe and solve problems that could not be solved with traditional methods. 
Since 1990, when MCMC first appeared in statistical science, many important related 
papers have appeared in the literature. During 1990-1 995, MCMC-related research focused 
on the implementation of new methods in various popular models [see, e.g., Gelman and 
Rubin (1 992), Gelfand, Smith and Lee (1992), Gilks and Wild (1 992), Dellaportas and Smith 
(1993)l. The development of MCMC methodology had also promoted the implementation 
of random effects and hierarchical models. 
Green's (1995) publication on reversible jump Markov chain Monte Carlo (RJMCMC) 
algorithm boosted research on model averaging, selection and model exploration algorithms 
[see, e g ,  Dellaportas and Forster (1999),Dellaportas et al. (2002), Sisson (2005), Hans et al. 
(2007)l. During the same period, the early versions of BUGS software appeared. BUGS 
was computing-language-oriented software in which the user only needed to specify the 
structure of the model. Then, BUGS was using MCMC methods to generate samples from 
the posterior distribution of the specified model. The most popular version of BUGS (v.05) 
was available via the Internet in 1996 [manual date August 14,1996; see, Spiegelhalter et al. 
(19964. Currently WinBUGS version 1.4.3 is available via the WinBUGS project Web- 
page (Spiegelhalter et al., 2003d). Many add-ons, ut 
es, and variations of the package are 
also available. The development of WinBUGS had proved valuable for the implementation 
of Bayesian models in a wide variety of scientific disciplines. In parallel, many workshops 
and courses have been organized on Bayesian inference, data analysis, and modeling us- 
ing WinBUGS software. WinBUGS is a key factor in the growing popularity of Bayesian 
methods in science. 
Development, extensions, and improvement of MCMC methods have also been consid- 
ered in statistical research since the mid-1990s. Automatic samplers, which will be directly 
applicable in any set of data, are within this frame of research and have led to the slice 
sampler (Higdon, 1998; Damien et al., 1999). Various samplers designed for model and 
variable evaluation have been also produced; for a comprehensive review, see Sisson (2005). 
Perfect sampling (Propp and Wilson, 1996; Merller, 1999) and population-based MCMC 
methods (Laskey and Myers, 2003; Jasra et al., 2007) can also be considered as interesting 
examples of the more recent development of MCMC algorithms. 
Finally, more recent advancements in genetics have given new impetus to Bayesian 
theory. The generally large amount of data (in terms of both sample size and variable size) 
have rendered the more traditional methods inapplicable. Hence Bayesian methods, with 
the help MCMC methodology, are appropriate for exploration of large model and parameter 
spaces and tracing the most important associations; see, e.g., Yi (2004). 
This book focuses on building statistical models using WinBUGS . It aims to assist 
students and practitioners in using WinBUGS for fitting models starting from the simpler 
generalized linear-type models and progressing to more realistic ones by incorporating more 
complicated structures in the model. 
The present chapter provides a comprehensive, short introduction to Bayesian theory. 
Only the most essential elements of the specific topic are emphasized. Nevertheless, since 
this is intended as a comprehensive overview for students and practitioners, detailed illus- 
tration using examples is also provided with emphasis on data analysis. 
'Version 1.4.1 appeared on September 22, 2004; version 1.4.2, on March 13, 2007; version 1.4.3, on August 6, 
2007 

DEFINITION OF STATISTICAL MODELS 
3 
1.2 DEFINITION OF STATISTICAL MODELS 
One of the most important issues in statistical science is the construction of probabilistic 
models that represent, or sufficiently approximate, the true generating mechanism of a 
phenomenon under study. The construction of such models is usually based on probabilistic 
and logical arguments concerning the nature and function of a given phenomenon. 
Assume a random variable Y ,  called response, which follows a probabilistic rule with 
density or probability function f(yl8), where 8 is the parameter vector. Consider an 
independent, identically distributed (i.i.d.) sample y = [yl, . . . . ynIT of size n of this 
variable; where the AT denotes the transpose of a vector or matrix A. The joint distribution 
n 
f(yI8) = rI f(Ytle) 
2 = 1  
is called the likelihood of the model and contains the available information provided by the 
observed sample. 
Usually models are constructed in order to assess or interpret causal relationships between 
the response variable Y and various characteristics expressed as variables X,, j E V ,  called 
covariates or explanatory variables; j indicates a covariate or model term and V ,  the set 
of all terms under consideration. In such cases, the explanatory variables are linked with 
the response variables via a deterministic function and part of the original parameter vector 
is substituted by an alternative set of parameters (denoted by 0) that usually encapsulate 
the effect of each covariate on the response variable. For example in a normal regression 
model with y - iV(Xp. a21) the parameter vector is given by BT = [pT. 02]. 
1.3 BAYES THEOREM 
Let us consider two possible outcomes A and B. 
Moreover, assume that A = A 1 U. . U A, 
for which A, n A, = 0 for every i # j. Then, Bayes ' theorem provides an expression for 
the conditional probability of A, given B, 
which is equal to 
In a simpler and more general form, for any outcome A and B, 
we can write 
This equation is also called Bayes ' rule, although it was originally found by Piere-Simon 
de Laplace (Hoffmann-Jsrgensen, 1994, p. 102). 
The above rule can be used for inverse inference. Assume that B is the finally observed 
outcome and that by A, we denote possible causes that provoke B. 
Then P(BIA,) can be 
interpreted as the probability that B will appear when A, cause is present while P(A, IB) is 
the probability that A, is responsible for the occurrence of B that we have already observed. 
Bayesian inference is based on this rationale. The preceding equation, which at a first 
glance is simple, offers a probabilistic mechanism of learning from data (Bernard0 and 
Smith, 1994, p. 2). Hence, after observing data (yl. yz, . . . , yn) we calculate the posterior 
distribution f(8iyl. . . . . yn), which combines prior and data information. This posterior 
distribution is the key element in Bayesian inference. 

4 
INTRODUCTION TO BAYESIAN INFERENCE 
Example 1.1. Suppose that in a case-control study, we trace 51 smokers in a 
group of 83 cases of lung cancer and 23 smokers in the control group of 70 disease- 
free subjects. The prevalence rate (estimate of the proportion of the disease at the 
population) of lung cancer is equal to 1%. The aim is to calculate the probability 
that a smoker will develop lung cancer. 
From this example we can estimate the following probabilities: 
51 
23 
83 
70 
P(smoker1case) = - = 0.615. P(smokerlcontro1) = - = 0.329 
and P(case) = 0.01 . From the Bayes theorem we calculate 
P( 
smokerlcase) P( case) 
P( 
smoker/ case)P( case) + P( 
smoker1 control) P( 
control) 
P(case1smoker) = 
0.615 x 0.01 
0.615 x 0.01 + 0.329 x 0.99 = 0.0185 
- 
- 
Hence the probability of a smoker to develop lung cancer is equal to 1.85% (approximately 
2 people over 100). 
Using similar arguments we can calculate the probability of a nonsmoker to develop the 
disease, which is equal to 0.0099 and the relative risk (RR) is equal to 
P( 
case I smoker) 
0.0185 
P (case 1 nonsmoker) 
0.0099 
Therefore, the probability for a smoker to develop lung cancer is 87% higher than the 
corresponding probability for nonsmokers. 
- 1.87. 
- 
RR = 
1.4 MODEL-BASED BAYESIAN INFERENCE 
Bayesian statistics differ from the classical statistical theory since all unknown parameters 
are considered as random variables. For this reason, prior distribution must be defined 
initially. This prior distribution expresses the information available to the researcher before 
any “data” are involved in the statistical analysis. Interest lies in calculation of the posterior 
distribution f(8ly) of the parameters 8 given the observed data y. According to the Bayes 
theorem, the posterior distribution can be written as 
The posterior distribution embodies both prior and observed data information, which is 
expressed by the prior distribution f(e) and the likelihood 
n 
respectively. Throughout this book, the marginal probability or density function of random 
variable X evaluated at z is denoted by f(z), while the corresponding conditional proba- 
bility or density function of random variable X evaluated at 2 given that Y = y is denoted 
by f(zly). With this notation, no distribution parameters are represented. If the random 

MODEL-BASED BAYESIAN INFERENCE 
5 
variable X follows a specific distribution D with parameters 8, the notation f o ( x  ; 8) is 
used to denote the corresponding probability or density function evaluated at X = z. For 
example, f ~ ( x  
; PO. F,”) denotes the density function of a normal distribution with mean 
equal to PO and variance equal to 00” evaluated at X = z. 
Specification of the prior distribution is important in Bayesian inference since it in- 
fluences the posterior inference. Usually, specification of the prior mean and variance is 
emphasized, The prior mean provides a prior point estimate for the parameter of interest, 
while the variance expresses our uncertainty concerning this estimate. When we a priori 
strongly believe that this estimate is accurate, then the variance must be set low, while igno- 
rance or great uncertainty concerning the prior mean can be expressed by large variance. If 
prior information is available, it should be appropriately summarized by the prior distribu- 
tion. This procedure is called elicitation of prior knowledge. Usually, no prior information 
is available. In this case we need to specify a prior that will not influence the posterior dis- 
tribution and “let the data speak for themselves”. Such distributions are frequently called 
noninformative or vague prior distributions. A usual vague improper prior distribution is 
f(8) x 1, which is the uniform prior over the parameter space. The term improper here 
refers to distributions that do not integrate to one. Such prior distributions can be used 
without any problem provided that the resulting posterior will be proper. A wide range of 
“noninformative” vague priors may be used; for details, see Kass and Wasserman (1995) 
and Yang and Berger (1 996). In this book, we use the term low informationprior for proper 
prior distributions with large variance. Such priors contribute negligible information to the 
posterior distribution. 
Summary measures such as the moments of the posterior distribution can be used for 
inference concerning the uncertainty of the parameter vector 8. To be more specific, mea- 
sures of central location such as the posterior mean, median, or mode can be used as point 
estimates, while the 4/2 and 1 - q / 2  posterior quantiles can be used as (1 - q )  100% posterior 
credible intervals. 
We can use the Bayes rule to infer for any parameter of interest 8 even when the observed 
data are collected sequentially at different timepoints (e.g., in prospective studies). Before 
any data are available, we use only the prior distribution f(8) 
for inference. When a set of 
data y(’) is observed, we can use the posterior distribution f (81y(l)) x f(y(l) le)f(8). 
When a second set of data is available, we can use the posterior from the first instance as 
a prior and incorporate the new data in a new updated posterior distribution. Hence the 
updated posterior distribution will be given by 
f ( 8 l Y ( l ) ,  d2)) = f ( d 2 ) l q f ( e / y ( l ) )  
f (Y(2) 18) f ( d l )  Jp)f(e). 
This equation can be generalized for data collected in t different time instances using the 
equation 
f(81y(l). . . . 
cx f(y(t))e)f(e/y(l), 
. . . , y(t-l)) 
fi f (P 
I e) f(Q). 
k=l 
Where it is obvious that Bayesian theory provides an easy-to-use mechanism to update our 
knowledge concerning the parameter of interest 8. 
In order to complete the definition of a Bayesian model, both the prior distribution and 
the likelihood must be fully specified. Having specified these two components, we then 
focus on describing the posterior distribution using density plots and descriptive measures, 

6 
INTRODUCTION TO BAYESIAN INFERENCE 
We may divide the whole procedure into four stages: model building, calculation of the 
posterior distribution, analysis of the posterior distribution, and inference - 
final conclu- 
sions concerning the problem under consideration. In the first stage we need to consider a 
model (likelihoodiparametersiprior) with reasonable assumptions. In the second stage, we 
calculate the posterior distribution of interest with the appropriate method of computation. 
Then we focus on the posterior analysis using descriptive measures, figures, and credible 
intervals. Finally, we draw conclusions concerning the problem which we are dealing with. 
More specifically, in the first stage (model building) we may follow the procedure de- 
scribed below: 
1. Identify the main variable of the problem (called response Y) 
and the corresponding 
data y. 
2. Find a distribution that adequately describes Y. 
3. Identify other variables that may influence the response variable Y (called covariates 
or explanatory variables). 
4. Build a structure for the parameters ofthe distribution (using deterministic functions). 
5. Specify the prior distribution (select the distributional family and specify the prior 
parameters; select between a noninformative prior or incorporate prior information 
andlor expert opinion). 
6. Write down the likelihood of the model. 
In the second stage we identify first the method of calculation of the posterior distribu- 
tion (analytically, asymptotically, or using simulation techniques) and then implement the 
selected method to estimate the posterior distribution. 
Concerning the analysis of the posterior distribution, we may proceed with (all or some 
of the) the measures proposed below: 
1. Visually inspect the marginal posterior distributions of interest. Possible plots that 
can be obtained are as follows. 
a. Marginal posterior density or probability plots if analytical or asymptotic meth- 
b. Marginal posterior histograms (or density estimates) for continuous variables 
c. Boxplots of the marginal posterior distributions 
d. Bivariate posterior plots (e.g. contour plots) to identify and study correlations 
2. Calculate posterior summaries (means, medians, standard deviations, correlations, 
ods are used 
and bar charts for discrete or categorical variables 
quantiles) and 95% or 99% posterior credible intervals 
3. Calculate the posterior mode and the area ofhighest posterior density (where possible) 
This description is only indicative and can be enriched with more details and further analysis. 
One further important issue is the implementation of diagnostic tests or checks concerning 
the appropriateness ofthe adopted model. Various techniques may be used to check whether 
the assumptions of the model are valid and whether the fit of the model is adequate, to test 
specific hypotheses leading to different conclusions, and to compare different models that 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
7 
may represent totally different scientific scenarios. All these procedures may lead to a new 
revised model and hence represent an important part of model building. We refer to this 
important aspect in Chapters 10 and 11. 
Another important issue is the robustness of the posterior distribution. We can assess 
how robust the posterior distribution is to the selection of the prior distribution via sensitivity 
analysis, in which we assess changes in the posterior distribution over different prior distri- 
butions. When prior information is available, sensitivity analysis focuses on the structure 
of the prior distribution; when noninformative priors are used, it focuses on how different 
choices of prior parameters may influence the posterior inference. 
Another important aspect is prediction. Bayesian theory provides a realistic and straight- 
forward theoretical frame for the prediction of future observations through the predictive 
distribution. The predictive distribution is equivalent to the fitted (or expected or predicted) 
values in classical theory with the difference that now we directly deal with a “distribution”. 
This distribution is also used for checking the assumptions as well as the fit of the model. 
We will refer at this important issue in Chapter 10. 
1.5 INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
Usually the target posterior distribution is not analytically tractable. In the past, intractability 
was avoided via the use of conjugate prior distributions. These prior distributions have 
the nice property of resulting to posteriors of the same distributional family. Extensive 
illustration of conjugate priors is provided by Bernard0 and Smith (1 994). 
A prior distribution that is a member of the distributional family D with parameters a 
is conjugate to the distribution f(y 18) if the resulting posterior distribution f(8iy) is also 
a member of the same distributional family. Therefore 
if 8 N D ( a )  then 81y N D(C.), 
where a and C. are the prior and posterior parameters of D. In many simple cases, the pos- 
terior parameters are expressed as weighted means of the prior parameters and maximum- 
likelihood estimators. In this section we focus on simple models with one or two parameters. 
Special attention is given to distributions that belong to the exponential family. 
1.5.1 Inference for the Poisson rate of count data 
Let us assume a set of discrete count data y in which we wish to estimate their mean A. 
Assuming a Poisson distribution with mean X for the data, we write 
yi N Poisson(X) for i = 1:. . . , n. 
In this simple example, the parameter of interest is the Poisson rate A, therefore 8 = A, 
while the likelihood is given by 
Let us now consider a gamma prior distribution for X with parameters a and b and density 
function 
In this setup the prior parameter vector is given by a = (a: b). 

8 
INTRODUCTION TO BAYESIAN INFERENCE 
The resulting posterior distribution is equal to 
where L is the sample mean. Since the density of any gamma distribution with parameters a 
and b, denoted by gamma(a, b), is proportional to xa-lepbz, we reach the conclusion that 
Xly N gamma(njj + a, n + b); 
(1.1) 
that is, the posterior distribution is a gamma distribution with parameters E = (ny+ a; n + 
b)T. Therefore the gamma distribution is conjugate to the Poisson distribution. 
The posterior mean of X is given by 
n y + a  
n + b ’  
E(Xly) = 
= - 
while the posterior variance is given by 
-2 
nF+a 
V(X1y) = OX = ~ (n + b)2 ’ 
These quantities can be rewritten in the following form 
a 
E(XIy) = ( & ) g +  (A) 
(i) 
= w j j + ( l - w ) ( ; )  
= w g + ( l - w ) E ( X )  
where w = n/ (n + b). The posterior mean is expressed as a weighted average of the prior 
of the sample mean (which is the maximum-likelihood estimator). 
The usually selected “low information” prior is a gamma distribution with low and equal 
prior parameters such as a = b = 
This prior is convenient since its mean is equal to 
one while the variance is given by l/a, which becomes large (expressing prior “ignorance”) 
for low values of a. Moreover, when a = b + 0, then w -+ 1, and the posterior mean of X 
coincides with the sample mean g. 
1.5.2 Inference for the success probability of binomial data 
Let us consider a set of binomial data y, that express the number of successes over N ,  
attempts (for i = 1,2.. . . , n). Hence y2 - binomial(.iT, Ni), resulting to a likelihood given 
by 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
9 
where N = cr=, 
lV2 is the total number of the Bernoulli experiments in the sample. 
beta(a. b), and density function 
If we consider a beta prior distribution with parameters a = (a, b) T ,  denoted by 
then the resulting posterior is also a beta distribution since 
with posterior parameters 6 = (ng + a. N - nji + b)T and posterior mean and variance 
- 
(ng + a)(N - nv + b) 
V(TlY) = 0; = 
(Ar + a + b)2 ( N  + a + b + 1) ' 
Similarly to the Poisson case, the posterior mean can be also expressed as a weighted 
average of the prior and the sample proportion since 
where w = N / ( N  + a + b), nY/N = CZ, yZ/N is the sample proportion and a/(a + b) 
is the mean a beta prior distribution with parameters a and b. 
A beta distribution with equal and low parameter values can be considered as a low- 
information prior (e.g., a = b = 
Other choices that are usually adopted are the 
beta(+, i) or the uniform distribution denoted by U(0. 1) [i.e., a beta(1,l) distribution]. 
The latter can be consider as a low-information prior distribution since it a priori gives the 
same probability to any interval of the same range. Nevertheless, this prior will be influential 
when the sample size is low. This might not necessarily be a disadvantage since, for small 
sample size, the posterior will also reflect the low available information concerning the 
parameter of interest 7r. 
1.5.3 
Inference for the mean of normal data with known variance 
When we are interested in the calculation of the posterior distribution of the mean of a 
response variable which takes all values in the set of real numbers, then a frequently used 
(and in several cases reasonable) assumption is that yi follows a normal distribution. In this 
case the likelihood is given by 

10 
INTRODUCTION TO BAYESIAN INFERENCE 
Using a normal N ( p 0 ,  a,”) 
prior distribution with f(pla2) = f ~ ( p  
; PO, a,”), 
the poste- 
rior distribution is given by 
f W 2 >  
Y) 0: 
f ( Y l P :  a2)f(llIa2) 
0: f(YIPL, a 2 ) f N ( P ;  PO: 0,”) 
Note that a2 and a,” 
are different known quantities that correspond to the variances of 
the random variable Y and the prior distribution of p, respectively [i.e. V(Yi) = a2 and 
The posterior described above is a normal distribution with mean and variance given by 
V ( P )  = .,”I. 
respectively, since the density of the N(G, Zz) is given by 
The above mentioned posterior mean and variance can also be written as 
where w = na,”/(na,” + a2). Consequently, the posterior mean is again expressed as a 
weighted average of the prior and the sample mean. If the prior variance is low (and hence 
the prior information concerning the parameter ,u is strong), then the posterior mean will be 
equal to the prior mean, while if the prior variance is high (and hence the prior information 
concerning the parameter p is low) then the posterior mean will be equal to the sample 
mean. Similarly, the posterior variance is expressed as a proportion of the standard error of 
the sample mean. When the prior variance is large, the posterior distribution is equivalent 
to the distribution of the sample mean. 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
11 
1.5.4 inference for the mean and variance of normal data 
The logic for normal data y when the variance is unknown is similar to that described 
in the previous section. In this case we need to estimate the posterior distribution for 
the parameters p and 02. The conjugate prior is the normal-inverse gamma distribution 
NIG(po> c. a, b) [see, e.g., Bemardo and Smith (1994)l with density 
fbL; 02) = f(Pla2)f(4 
where f(plo2) = N(p0. c o 2 )  and f(u2) = IG(a, b) when IG(a, b) is the inverse gamma 
distribution with parameters a and b. Note that if X N gamma(a, b), then X - IG(a, b). 
For this reason, for normal models the precision IT = u - 2  is used instead. This notation is 
also adopted by WinBUGS software. The precision parameter has a plausible interpretation 
since it measures the precision of the available information concerning the parameter of 
interest. If the variance is large, then the precision is low and hence the mean cannot be 
used as a precise estimate for the random variable Y. 
On the contrary, when the variance 
is low, the precision is high and hence the mean can be used as a precise summary of Y. 
With this approach, the prior for the parameter vector 8 = ( p ,  r )  * is a normal-gamma 
distribution, denoted by NG(p, c. a, b), with density 
Theresultingposteriordensity f(p, 02/y) is aNIG(jG; C, a, b) [orequivalentlyNG(jG, C! a, b) 
for p and r] with 
n c  
WB + (1 - w)po with w = - 
1 + n c  
= 
- 
W 
c
=
-
 
n 
- 
n 
a = a + ?  
- 
ss 
b = b + -  2 
since 
where SS is given by 
and s2 is the sample variance of Y :  s2 = cy=l(yi 
- j j ) ' / ( n  - 1). 

12 
INTRODUCTION TO BAYESIAN INFERENCE 
In multidimensional posterior distributions, focus is given in the analysis of the marginal 
posterior distributions and their corresponding descriptive measures. In the current example, 
we focus our attention on the marginals 
-(n+2a+1)/2 
n + 2,) 
- n n + 2 a  
= t p.-- 
( w S S  + 2b' 
- 
- 
where t(p, r. a )  is the noncentral Student's t distribution with density 
[for details, see Bemardo and Smith (1994)l. The posterior mean and variance of (p> 
T )  
are given by 
n / 2  + a 
(SS/2 + b)2 ' 
and V(rJy) = 
n / 2  + a 
E(rly) = S S / 2  + b 
Inference for the variance u2 can be directly based on the posterior distribution 
with posterior mean and variance 
1.5.5 Inference for normal regression models 
The results of the previous paragraph can easily be extended for the case of the normal 
regression model. Here we use the conjugate multivariate normal-inverse gamma prior and 
present the posterior densities directly without further details. 
Let us consider a n-dimensional vector of data y and their corresponding random variable 
Y .  Then the normal regression model can be summarized by 
Yip, u2 N !vn ( x p ,  21,) , 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
13 
where X is the n x p data matrix including the values of the covariates, P is the p x 1 
parameter vector specifying the effect of each covariate, and In 
it the R x n identity matrix. 
The multivariate normal-inverse gamma prior distribution is conjugate to the likelihood 
described above and is given by 
which can be expressed as a product of a multivariate normal distribution (for parameter 
vector 0) 
and an inverse gamma prior (for 0 2, given by 
N Np(pa, Vo2) and 0’ N IG(a, b). 
The resulting posterior is also a normal-inverse gamma with parameters 
3 = 5(xTy+v-lpp) 
c = ( x T X + v - l y  
- 
- 
n 
- + a  
2 
a
=
 
The posterior mean 3 can be written in a form equivalent to that for the weighted average 
of the prior and the sample mean in the univariate normal case since 
-1 
6 = W f J  + ( I ,  - W )  
po with W = (XTX + V-’) 
XTX, 
-1 
where ,b = (XTX) 
we can obtain the elegant expression of Atkinson (1978) 
XTy is the maximum-likelihood estimator. Using more algebra 
T 
where RSS = (y - Xp) (y - Xp) = yTy - pTXTXp is the residual sum of 
squares in classical regression analysis. This expression is useful because it gives insight 
concerning the meaning and the role of SS, which is equal to the traditional sum of squares 
and a measure of distance between the maximum-likelihood estimator (MLE) and the prior 
mean. 
The marginal distributions are similar as in the simpler case. The marginal posterior 
distribution o f 0  is amultivariate Student distribution with parameters 3, g ( S S + 2 b ) / ( n +  
2a) and n + 2a, where the multivariate Student distribution (denoted by MSt) is defined 
as 
1 --(a+p)’z. 
y 
MStp(P,Ela) * f(Y) cx 
- P)TC-l(Y - P) 
The marginal posterior distribution of 0’ is simply an inverse gamma distributions with the 
parameters iT and b given above; for more details, see Bernard0 and Smith (1994). 

14 
INTRODUCTION TO BAYESIAN INFERENCE 
1.5.6 Other conjugate prior distributions 
In this section we summarize some well-known conjugate distributions. We focus on the 
popular exponential family, which includes a wide range of well-known distributions and 
is also used in the generalized linear models setup. 
Let us consider the exponential dispersion family with Y 
N expf (3. @, u(), b(), c()) 
(1.3) 
for some specific functions a($), b(6), and c(y, o), where 29 and d are location and scale 
parameters, respectively. 
The likelihood function for a sample of size n is given by 
[for details, see McCullagh and Nelder (1989, pp. 28-29)]. Using a prior distribution of 
we end up with a posterior 
This posterior has the same form as the prior (1.4) with parameters ng + 29 0 and n + TO 
(instead of 290 and TO). The conjugate distribution shown above is conditional on the 
dispersion parameter @ assuming that it is known and fixed. 
For example, for the normal model, assuming that CT is known, we have 
3 = p, Q = CT', u ( 4 )  = g2, b(6) = -, 
and c(y,3) = -- - - 1 
Y2 
1 
2a2 
2 O d 2 ~ 4 .  
P2 
2 
Following (1.4), the conjugate prior is given by 
This prior is equivalent to the conjugate prior that we used in Section 1.5.3 with p 0 = 30,b-o 
and 00" = n2/To. Members of the exponential family are also the gamma, binomial, and 
Poisson distributions. Table 1.1 summarizes the most important conjugate distributions. 
1.5.7 Illustrative examples 
In this subsection we use realistic data examples to illustrate the results shown above, which 
are based on conjugate prior distributions. Examples of data analysis using the Poisson, 
binomial, and normal distributions are provided in detail. 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
15 
Table 1.1 Conjugate prior distributions for commonly used distributions 
Distribution 
Likelihood 
Prior distribution 
Posterior parameters 
Poisson 
Y, - Poisson(X) 
Binomial 
Y, -- binomial(p, N z )  
Normal (known u2) Y, - X ( p  u 2 )  
Gamma (v known) Y, N gamma(u. 0) 
Exponential 
Y, - exponential(8) 
= gamma(1,O) 
Negative binomial K - KB(p. K,) 
X N gamma(a. b) 
p N beta(a, b) 
[p, $1 
N NIG(p0. c. a. b) 
[N(po. cu2) x IG(a, b)] - 
a = n / 2  + a  
L 
J
-
 
- 
c = w / n ,  b = SS/2 -k b 
w = nc/(l + n c )  
a = n u + a ,  
b = n g + b  
ss = (n - l)s2 + (fj - po)2w/c 
- 
- 
- 
8lu N gamma(a, b) 
8 - garnma(a. b) 
a = n + a ,  b = n g + b  
I 
K, + a, 
yz + b 
, Yz + a0 
I 
I 
19. d. a(). b(). c()) exp { [830 - rob(vl)]/a(@)) 
19 = nfj + 290, 
T = n 4 
TO 
Example 1.2. Goals scored by the national football team of Greece in Euro 
2004 (Poisson data). Let us consider the number of goals scored by a team in 
association football (soccer). The Poisson distribution is widely used to model such 
data although a slight overdispersion is observed (Lee, 1997; Karlis and Ntzoufras, 
2000; Karlis and Ntzoufras, 2003~). In this example we consider the final scores 
of the National team of Greece in the Euro 2004 cup competition, where Greece 
surprisingly won the competition; see Table 1.2. We are interested in estimating the 
posterior distribution of expected number of goals scored and conceded by Greece 
as well as the total number of goals scored by both teams in each game played by 
Greece (the latter is frequently used for betting purposes). 
Table 1.2 Games of Greece in the Euro 2004 competition 
Goals scored 
Opponent 
(In favor - against) 
Total 
~ 
1 
Portugal 
2 - 1  
2 
Spain 
1 - 1  
3 
Russia 
1 - 2  
4 
France 
1-0 
5 
Czech 
0-O(l 
6 
Portugal 
1-0 
Republic 
- 0y 
3 
2 
3 
1 
0 (1) 
1 
Sum 
6 - 4  
10 
Sample mean Z 
1.00 - 0.67 
1.67 
aNormal time score = 0-0; score after 15 times of extra time = 
1-0. 

16 
INTRODUCTION TO BAYESIAN INFERENCE 
" 4  
r 
x 
Y 
To model the goals scored and conceded by Greece (denoted by y 9 and y;, 
respectively) 
and the total number of goals (y," = yt + y,"), we use the Poisson distribution. Hence we 
write 
y," - Poisson(@), for k E {s, c, t }  and i = 1, 2,. . . , 6  , 
where 0" and 8" respectively are the expected number of goals scored and conceded by 
Greece and Bt are the expected total number of goals scored in each game. 
If we wish to consider a low-information prior, then we can use a gamma(0.001,O.OOl) 
prior with mean equal to one and variance equal to 1000. Following (1. l), the posterior 
distributions for the mean goals scored by Greece, against Greece, and in total are expressed 
in the following gamma distributions: 
O'ly 
N 
gamma(6.001,6.001), 
8"ly 
N 
gamma(4.001,6.001), 
Bt(y 
N 
gamma(10.001,6.001) . 
The posterior means and standard deviations (SD) are given by 
Goals conceded 
,~ 
,~ 
,
,
 
,
,
 
6.001 
6.001 
6.001 
6.001 
E(8"ly) = - 
= 1.000, SD(Ply) = 7 
= 0.166 
4.001 
4.001 
6.001 
6.001 
10.001 
10.001 
6.001 
E(B"1y) = - 
- 
- 0.667, SD(B"1y) = 7 
= 0.111 
- 1.667, SD(BtJy) = - 
- 
6.0012 - 0'278 
E(BtJy) = - 
- 
Graphical representation of the posterior distributions is given in Figure 1.1 
Figure 1.1 
low-information prior. 
Posterior distributions of number of goals scored and conceded by Greece in Euro 2004; 
Using an informative prior distribution. Before the Euro 2004 competition, prior in- 
formation was available from the group qualifying stage of the competition. Although the 
quality of the opponents was not as high as in the Euro 2004 competition and all games 
were played in both "home" and "away" (that is, domestic and foreign) stadiums, we can 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
17 
extract information from these game and specify a prior more informative than the one used 
above. 
From these games, the Greek national team was first in its group scoring 8 goals and ac- 
cepting only 4 in 8 games. From this information we can construct the posterior distribution 
after the completion of the qualifying group stage. Hence, according to (1. l), our posterior 
after considering these games is a gamma(8,8) for goals scored in favor and a gamma(4,8) 
for goals scored against Greece. Although these posterior distributions can be directly used 
as prior distributions for analysis of the data in the Euro 2004 competition, we propose 
slightly modifying the prior by increasing the variance in order to reflect additional un- 
certainty because this information resulted under different conditions. The finally imposed 
prior assumes the same means (1 and 0.5) as the ones above, but the variance is multiplied by 
the sample size of the prior data. In this way, the prior approximately contributes informa- 
tion equivalent to one data point to the posterior distribution. Therefore, the finally imposed 
priors are respectively gamma( 1,l) and gamma( 1,2) for goals scored in favor and against 
Greece. Following (1. l), the posterior distribution for the goals scored by Greece is now 
given by Ply N gamma(7; 7)) with mean 1 and standard deviation 0.38 goals per game, 
while the posterior for the goals conceded by Greece is given by Q ly N gamma( 5,8), with 
mean 0.625 and standard deviation 0.078 per game. These posterior distributions are also 
illustrated in Figure 1.2. 
7 " 4  
Goals conceded 
I 
0.5 
1.0 
1.5 
2.0 
2.5 
3.0 
Expected number of goals 
Figure 1.2 
including prior information from qualifying group games. 
Posterior distributions of number of goals scored and conceded by Greece in Euro 2004 
In the analysis above we have excluded the goal scored in the extra time played in the 
game against the Czech Republic. This can be easily incorporated by considering the factor 
of time. In this case we can assume that yi N Poisson(tiB), where ti is equal to one for 
usual games of 90 minutes of normal play, ti = 1 + 
= 1.167 for games with 15 minutes 
of extra time and ti = 1 + 
= 1.33 for games with 30 minutes of extra time. After the 
inclusion of extra time goals, the posterior distribution is 
Using the data of our example, the posterior distributions are slightly changed to 
O'Iy - gamma(7.168,6.618) and 8"ly N gamma(4.001,6.618) 

18 
INTRODUCTION TO BAYESIAN INFERENCE 
in the noninformative case and in the informative case, to 
0"Iy - gamma(8.168,7.618) and 0"Iy - gamma(5.001,8.618). 
In such problems, we might also be interested in inferences about the difference of the means 
8" - 0" as well as making predictions based on the probability of winning a game. The first 
issue can easily be solved by simply sampling from each gamma distribution, calculating 
the difference, and estimating the posterior distribution. This is a simple implementation of 
simulation and Monte Carlo techniques, presented in the following sections. To describe 
the second issue, we firstis need to define the predictive distribution, which is presented in 
Chapter 10. 
Example 1.3. Estimating the prevalence of a disease (Bernoulli data). Suppose 
that we are interested in the estimation of the prevalence of a disease, for example, 
coronary heart disease, for men aged 30-40. Data from a prospective study of 
sample size equal to 1250 men have indicated 5 men experiencing at least one 
incident. We are interested on estimating the prevalence rate of the disease for this 
specific age group. 
Following (1.2), using a beta(0.01,O.Ol) low-information prior, we have 
nly - beta(5.01,1245.01), 
with mean equal to 4 incidents over 1000 men and standard deviation equal to 1.89 incidents 
over 1000 men; see Figure 1.3 for graphical representation of the posterior distribution of 
prevalence. Alternatively, we may use the uniform U(0,l) distribution as a noninformative 
prior. Under this prior setup, the posterior is .rrly N beta(6.1246); with posterior mean of 
the prevalence rate equal to 4.8 incidents per 1000 males and standard deviation equal to 
1.95 incidents per 1000 males. 
0 
I 
I 
I 
2 
4 
6 
8 
10 
Prevalence 
Figure 1.3 
solid line - 
beta(0.01,O.Ol) prior; dashed line -uniform U(O.1) prior. 
Posterior distribution of prevalence rate (per 1000 males) for coronary heart disease: 
A simple credible interval can be given by the 2.5% and 97.5% quantiles of the beta 
distribution. In our case, the prevalence will be in the interval (1.3; 8.2) with probability 
95% for the beta(0.01,O.Ol) and (1.76,9.3) for the uniform prior. 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
19 
Example 1.4. Kobe Bryant’s field goals in NBA (binomial data). In this example 
we consider the field goals and the corresponding attempts of a successful NBA 
player, Kobe Bryant. Data were downloaded from the Yahoo sports page and are 
presented in Table 1.3. 
Having observed the data of Table 1.3, we are interested in comparing the perfor- 
mance of the player in terms of the success probability (or percentage) of field goals 
across different seasons. 
Table 1.3 Kobe Bryant’s field goals for seasons 1999-2007 
Field goals 
Season 
Games 
Successes 
Attempts 
Percent (%) 
1999100 
200010 1 
2001102 
2002103 
2003104 
2004105 
2005106 
2006107 
66 
554 
68 
701 
80 
749 
82 
868 
65 
516 
66 
573 
80 
978 
42 
399 
1 I83 
1510 
1597 
1924 
1178 
1324 
2173 
845 
46.8 
46.4 
46.9 
45.1 
43.8 
43.3 
45.0 
47.2 
Total 
749 
5338 
11,734 
45.5 
Source: Yahoo sports. 
In Table 1.4, we present the posterior details, including the parameters of the beta dis- 
tribution, the means and standard deviations, as well as the 95% posterior intervals. In all 
calculations a beta prior distribution with parameters equal to 0.01 has been used; 95% 
posterior intervals (based on the 2.5% and 97.5% posterior percentiles) are also depicted 
using error bars in Figure 1.4 in order to clearly compare the Kobe Bryant’s performance 
across different seasons. 
Table 1.4 Posterior summaries for Kobe Bryant’s field goals for seasons 1999-2007 
Posterior 
Posterior summaries 
parameters 
of field goal success percentage 
- 
I 
Year 
Games 
Successes 
Attempts 
a 
b 
Mean 
SDa 
Q0.025 
Q0.975 
1999/00 
66 
2000/01 
68 
2001/02 
80 
2002/03 
82 
2003104 
65 
2004/05 
66 
2005106 
80 
2006/07 
42 
Total 
749 
554 
70 1 
749 
868 
516 
573 
978 
399 
5338 
1183 
1510 
1597 
1924 
1178 
1324 
2173 
845 
11734 
554.01 
701.01 
749.01 
868.01 
516.01 
573.01 
978.01 
399.01 
5338.01 
629.01 
809.01 
848.01 
1056.01 
662.01 
751.01 
1195.01 
446.01 
6396.01 
46.8 
1.5 
44.0 
49.7 
46.4 
1.3 
43.9 
48.9 
46.9 
1.2 
44.5 
49.4 
45.1 
1.1 
42.9 
47.3 
43.8 
1.4 
41.0 
46.6 
43.3 
1.4 
40.6 
46.0 
45.0 
1.1 
42.9 
47.1 
47.2 
1.7 
43.9 
50.6 
45.5 
0.5 
44.6 
46.4 
QStandard deviation. 
Source: Yahoo sports 

20 
INTRODUCTION TO BAYESIAN INFERENCE 
0 1  
d 
I 
I 
I 
I 
I 
I 
I 
I 
I 
1999/00 2000/01 2001102 2002/03 2003104 2004105 2005/06 2006107 
Total 
Season 
Figure 1.4 
prior =beta(O.Ol, 0.01). 
95% posterior credible intervals for Kobe Bryant’s field goals for seasons 1999-2007; 
Updating information over the seasons. In the analysis above we compared the per- 
formance of the player across different seasons. In Bayesian analysis, we may also update 
our posterior after the end of each season. This will result in improving our confidence for 
the player’s performance (although we do not consider other factors that may be important, 
such as the age of the player). 
In Table 1.5 and Figure 1.5 we can observe how posterior distribution is updated after 
incorporating sequentially data information of each year. As expected, the final posterior 
distribution coincides with the corresponding posterior, which combines all seasons’ data. 
Table 1.5 
seasons 1999-2007 
Summaries of the updated posterior distributions for Kobe Bryant’s field goals for 
Posterior 
Posterior summaries 
parameters 
of field goal success percentage 
I 
I 
Year 
Games 
Successes 
Attempts 
a 
b 
Mean 
SDa 
Q o . 0 2 5  
Qo.975 
1999100 
66 
2000101 
68 
2001102 
80 
2002103 
82 
2003104 
65 
2004/05 
66 
2005106 
80 
2006/07 
42 
a Standard deviation. 
Source: Yahoo sports. 
554 
701 
749 
868 
516 
573 
918 
399 
1183 
554 
629 46.8 
1.5 
44.0 
49.7 
1510 
1255 
1438 46.6 
1.0 
44.7 
48.5 
1597 
2004 
2286 46.7 
0.8 
45.2 
48.2 
1924 
2872 
3342 46.2 
0.6 
45.0 
47.5 
1178 
3388 
4004 45.8 
0.6 
44.7 
47.0 
1324 
3961 
4755 45.4 
0.5 
44.4 
46.5 
2173 
4939 
5950 45.4 
0.5 
44.4 
46.3 
845 
5338 
6396 45.5 
0.5 
44.6 
46.4 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
21 
0 
cn 
5
0
 
.z (D 
c 
0, 
-0 
L 
.- ?
S
 
:: 
0, 
a 
w 
0 
(u 
0 
42 
44 
46 
48 
50 
Percentage of field goals 
Figure 1.5 
1999-2007; prior=beta( 0.01 0.01). 
Updated posterior densities for Kobe Bryant's percentage of field goals for seasons 
Example 1.5. Body temperature data (normal data). In this example we con- 
sider data of Mackowiak et al. (1992), who examined whether the true mean body 
temperature is 98.6 O F .  The same data were reanalyzed and presented by Shoemaker 
(1996). In the present example, we consider the normal model for inferences about 
both the mean and the variance of the body temperature. 
We adopt the normal model with yz N N ( p .  c?) 
and a normal-gamma prior with pa- 
rameters p = 98.6, c = 100, a = 0.001 and b = 0.001. This is a low-information prior 
(since the variance is large) with the mean temperature centered around 98.6 O F  which is 
the widely accepted value. 
The sample size, mean, and variance are equal to 
n = 130. jj = 98.24923 
and 
s2 = 0.5375575 H SS = (130 - 1) x 0.5375575 = 69.34492, 
respectively, resulting in an NIG posterior distribution for the mean and the variance of 
temperature with parameters 
i; = 0.999923 x 98.24923 + (1 - 0.999923) x 98.6 = 98.249 
- 
- 0'999923 = 0.007692 
153 
c
=
 n 
2 
I 
150 
a = - 
f0.001 = 65.001 
I 
b
=
-
 69'34492 + 0.001 = 34.6735 
2 

22 
INTRODUCTION TO BAYESIAN INFERENCE 
Hence we can summarize the joint posterior density by 
p. a2/y N NIG(98.25, 0.0077, 65.0, 34.67). 
Theposteriorweightwofthedataisequaltow = (130x 100)/(130x l O O + l )  = 0.999923. 
Since w is very close to one (both sample size and prior variance are large), we deduce that 
the data likelihood dominates the posterior disrtibution. 
The marginal posterior distribution of p is given by ply - t(98.25, 243.725, 130.002) 
since 
130'002 
= 243.7247 
130 
7 =  - 
0.999923 0.02 + 69.345 
with mean and standard deviation 
0.999923 129 x 0.5375575 + 0.02 = o.06456. 
128.002 
.€?(ply) = 98.25 and SD(pL/y) = 
~ 
130 
The marginal posterior distribution of the precision parameter 7 is 
I 
~ l y  
N gamma(E = 65.001, b = 34.67) 
with mean and variance 
-= 
n'2 
+ a 
65.001/34.67346 = 1.875, 
SS/2 + b 
n/2+ a 
= 0.054. 
v(T'y) = (SS/2+6)2 
The corresponding posterior mean and variance for 0 are given by 
-- 
- 0.542, 
SS/2 + b - 34.67346 
- 
- 
E(a2'y) - n / 2 + a -  1 
64.001 
34.67346' 
64.0012 x 63.001 = 0.00466 
- 
- 
- 
(ss/2 + b)' 
v(a2'y) - (n/2 + a - 1)2(n/2 + a - 2) 
Graphical representations of the marginal and the joint posterior densities are given in Figure 
1.6. 
From the posterior density plot it is obvious that the prior value 98.6 OF is not supported 
since it corresponds to a value with low posterior density lying at the tails of the distribution 
(in fact, in our graph is out of the range of the plotted distribution). 
Sensitivity analysis. When low-information prior distributions are used, it is common 
practice to proceed to the implementation of sensitivity analysis with different values of the 
prior mean or variance. 
The sensitivity of the posterior mean over different values of the prior parameter c, which 
controls the prior variance, is depicted in Figure 1.7. For c we consider c = 10 IC for values 
of k = -2, -1,O. 1,2,3,4,5. From the graph we clearly see that the posterior mean is 
quite robust with values ranging from 98.25 to 98.4. Even for relatively low values of c 
(e.g., for c = l), the estimate is equal to 98.27, which is quite close to the value of 98.25 
resulted using large values of c. 
Even if we consider as prior mean the extreme value of zero (which is far away from 
realistic values), the posterior mean is still quite robust; for values of c 2 10 with posterior 
mean ranging from 98.17 - 98.25, see Figure 1.8. 

INFERENCE USING CONJUGATE PRIOR DISTRIBUTIONS 
23 
98.0 
98.1 
98.2 
98.3 
98.4 
98.5 
Mean temperature 
(a) Posterior density for mean body temperature 
1.0 
1.5 
20 
2 5  
3.0 
Temperature precision 
(b) Posterior density for precision 7 
& 
0.3 
0 4 
0 5 
0.6 
0 7 
0.8 
Temperature variance 
(c) Posterior density for variance d 
I
,
,
,
,
 
96 10 
9620 
9830 
9840 
Mean temperature 
(d) Contour plot ofjoint posterior density for p T 
(e) Joint posterior density plot fork, T 
Figure 1.6 Posterior plots for parameters of body temperature data 

24 
INTRODUCTION TO BAYESIAN INFERENCE 
0~ 
8 
-
2
-
1
0
 
1
2
 3 
4 
5 
Figure 1.7 
to 98.6; dotted lines represent the 2.5% and 97.5% quantilies of the distribution. 
Sensitivity plot of posterior mean for different values of log,,(c) with prior mean equal 
Figure 1.8 
to zero. 
Sensitivity plot of posterior mean for different values of log,, (c) with prior mean equal 
The same procedure for c = 100 and p E (98,99) led to a quite robust posterior mean 
ranging from 98.24921 to 98.24929. 
For illustration we have also considered informative priors with values of c = 10 for 
k = -4, -3, -2, -1 and for a = b = 0.1 or a = b = 1. In Figures 1.9 and 1.10 we 
see how the prior becomes increasingly informative for low values of c which controls the 
marginal prior variance. Also note that even for c = 0.1, the posterior is very close to the 
data (expressed in terms of the distribution of the sample mean G). 
1.6 NONCONJUGATE ANALYSIS 
In the example of the body temperature data we have expressed the amount of information 
as a proportion of the unknown variance of Y .  In many occasions we are interested in 
expressing our prior beliefs in a simpler and more straightforward manner. Usually such 
prior information is extracted by experts who are not familiar with simple probability notions 
such as dependence and correlation. Therefore, in the example above, we might need to 

NONCONJUGATE ANALYSIS 
25 
c = 10-1 
98.2 
98.4 
98.6 
98.8 
Temperature mean 
= 10-3 
98.0 
98.2 
98.4 
98.6 
98.8 
Temperature mean 
c = 10-2 
98.0 
98.2 
98.4 
98.6 
98.8 
Temperature mean 
= 10-4 
98.0 
98.2 
98.4 
98.6 
98.8 
Temperature mean 
Figure 1.9 
Comparison of prior, data, and posterior distribution over different values of c for 
a = b = 1. Lines from left to right: dashed-dotted line (- . -) represents sample distribution of J ;  
solid line (-) 
indicates posterior of p; dashed line (- - -) denotes prior of p. 

26 
INTRODUCTION TO BAYESIAN INFERENCE 
c = 10-1 
98.2 
98.4 
98.6 
Temperature mean 
= 10-3 
98.8 
98.0 
98.2 
98.4 
98.6 
98.8 
Temperature mean 
c = 10-2 
98.2 
98.4 
98.6 
98.8 
Temperature mean 
= 10-4 
0 1  - 
, 
- _ _ _  
,
I
 
. 
_ -  
, -  
I 
I 
980 
982 
984 
986 
988 
Temperature mean 
Figure 1.10 
Comparison of prior, data, and posterior distribution over different values of c for 
a = b = 0.1. Lines from left to right: dashed-dotted line (-.-) represents sample distribution ofy; 
solid line (-) 
indicates posterior of p; dashed line (- - -) denotes prior of p. 

PROBLEMS 
27 
simplify the prior structure using independent distributions for p and r (or equivalently o 2, 
and directly specify the prior precision of p instead of setting it proportional to g '. For 
example, we may consider 
f(p.a2) = f ( p ) f ( c r 2 )  with f(p) = N ( p ~ : o ; )  
and f ( g 2 )  = IG(a: h) . 
In this case, the resulting posterior distribution is of an unknown form. Consequently, it is 
difficult to evaluate the posterior summaries and their corresponding marginal densities. 
In cases where conjugate priors are considered to be unrealistic or are unavailable, 
either asymptotic approximations such as Laplace approximation [see, e.g., Tierney and 
Kadane (l986), Tierney et al. (l989), Erkanli (1994)] or numerical integration techniques 
[see, e.g., Evans and Swartz (1996)] can be used. Another appealing alternative would be 
to use simulation-based techniques. These methods generate samples from the posterior 
distribution and will be used throughout this book since WinBUGS facilitates such methods 
in order to indirectly estimate the posterior distributions of interest. In the next Chapter, the 
most important methods for generating samples from the posterior distribution are described 
and illustrated using simple data examples. 
Problems 
1.1 
Let us consider a medical diagnostic test with 90% sensitivity and 80% specificity 
for a disease with incidence equal to 1 in 10,000 individuals. Use the Bayes theorem 
to calculate the probability that an individual has the disease when the test is positive 
and when the test is negative. 
Let us consider the exponential distribution with density hnction f(yi8) = Be 
and an i.i.d. sample Yi N exponential( 0) for i = 1, . . . , n. 
a) Show that a gamma prior distribution is conjugate for 0. 
b) Calculate the posterior mean and variance under this conjugate prior. 
c) For the following data 
1.2 
0.4 0.0 0.2 0.1 2.1 0.1 0.9 2.4 0.1 0.2 
use the exponential distribution and 
(i) Plot the posterior distribution for gamma prior parameters a = h = 0.001. 
(ii) Perform sensitivity analysis for various values of a and b. Produce related 
plots depicting changes on the posterior mean and variance. 
1.3 
In the exponential distribution, consider directly the mean parameter p = l/B. 
a) What is the conjugate prior for p? 
b) What is the posterior distribution for p and B under this setup? 
c) Is the posterior analysis under this approach equivalent to the corresponding one 
For yi N gamma(v, 0) assuming that v is known 
a) Prove that the gamma distribution is a conjugate prior for 8. 
b) Find the posterior mean and variance for 0. 
c) Examine the effect on the posterior density of 0 
in Problem 1.2? 
1.4 
(i) Of the known parameter v 
(ii) Of the sample size n 
(iii) Of the prior parameters 
1.5 
Let us consider Yi (for i = 1, . . . , n) be an i.i.d. sample of categorical variables with 
n categories. 

28 
INTRODUCTION TO BAYESIAN INFERENCE 
a) Show that a Dirichlet prior is conjugate for the probability of each category. 
b) Calculate the posterior mean and variance for the probability of each category. 
c) Calculate the posterior correlations between the probabilities of two different 
Let us consider the following 2 x 2 contingency table: 
categories. 
1.6 
x: 
Y: disease status 
Marginal 
Risk factor 
1: present 
2: absent 
of X 
1: present 
211 
2: absent 
1 
343 
1301 
MarginalofY 1 
554 
1621 
I 
2175 
a) Consider the multinomial distribution and the conjugate Dirichlet distribution to 
b) Calculate the posterior means and variances for the cell probabilities. 
c) Obtain plots of the estimated density function and boxplots of the posterior dis- 
Let us consider the following 2 x 2 contingency table of the previous problem: 
a) Use the Poisson distribution, that is, assume that nt3 N Poisson(X,,), and the 
corresponding conjugate prior to analyze the data. 
b) Obtain the posterior mean and variance for the expected cell frequencies according 
to the two approaches and compare them. 
c) Produce plots of the posterior densities for the expected cell frequencies X 
under 
the Poisson assumption, and compare them graphically with the corresponding 
posteriors obtained by the multinomial model in Problem 1.6. 
Let us consider the soccerifootball World Cup 2006 final scores for Italy (winner of 
the cup): 
analyze the data tabulated above. 
tributions of the cell probabilities. 
1.7 
1.8 
Normal time 
Other 
Stage 
Opponents 
Score 
Comments 
Group 
Group 
Group 
Round of 16 
Quarter-final 
Semifinal 
Final 
Italy - Ghana 
2-0 
Italy - USA 
1-1 
Czech Republic - Italy 
0-2 
Italy - Australia 
1-0 
Germany - Italy 
0-0 
0-2 (a.e.t.)a 
Italy - France 
1-1 
1-1 (a.e.t.)", 5-3 (pen.lb 
Italy - Ukraine 
3-0 
ascore after extra time of 30 minutes 
bScore in penalty kicks. 
Use the Poisson distribution to analyze the data tabulated above. Compare the 
goals scored and conceded by the Italian national team. 
Produce similar analysis using the multinomial distribution for the score difference 
during the normal time of a game (90 minutes). 
Perform similar analysis for the corresponding results of national team of France 
which was the finalist team of the tournament (see data in the table below). Com- 

PROBLEMS 
29 
pare the results with the corresponding one for the Italian national team. Which 
team seems to be better according to your (simple) analysis? 
Normal time 
Other 
Stage 
Opponents 
score 
comments 
~ 
~~~ 
Group 
Group 
Group 
Round of 16 
Quarter-final 
Semifinal 
Final 
~~ 
France - Switzerland 
0-0 
France - South Korea 
1-1 
Togo - France 
0-2 
Spain - France 
1-3 
Brazil - France 
0- 1 
Portugal - France 
0- 1 
Italy - France 
1-1 
l-l(a.e.t.)", 5-3 (pen.)b 
a Score after extra time of 30 minutes. 
bScore in penalty kicks. 
1.9 
Assume that a friend tosses a coin 10 times and tells you that "heads" appeared less 
than 4 times. 
a) Calculate the posterior density for the success probability 7r using a beta(a, u )  
b) Plot the posterior density for a = 1 and a = 2. 
c) Calculate the posterior mean of T .  
d) Plot the posterior mean over different values of the prior parameter a and examine 
prior density. 
the sensitivity of the results. 
1.10 
Consider the following data: 
0.671 1.412 -2.119 1.224 -1.168 -0.860 1.936 3.396 4.808 -1.259 0.27 
1.820 2.417 2.929 7.020 0.483 6.483 2.966 0.942 2.846 -3.398 3.840 
6.640 1.018 2.747 1.857 7.270 2.734 4.325 -1.222 
a) Assuming the normal distribution for these data and the corresponding conjugate 
b) Produce graphical representations of the marginal posterior distributions for p 
prior, calculate the posterior mean and variance. 
and o2 as well as their corresponding joint posterior distribution. 

CHAPTER 2 
MARKOV CHAIN MONTE CARLO 
ALGORITHMS IN BAYESIAN INFERENCE 
The present chapter provides a short introduction concerning the use random-number gen- 
eration (or stochastic simulation) methods in Bayesian inference. The focus is on Markov 
chain Monte Carlo (MCMC) methods that are widely used in Bayesian inference. The 
description of the most popular MCMC methods is accompanied by detailed examples im- 
plemented in R statistical and programming software. Users not familiar with R can follow 
the comprehensive pseudocode that describes the corresponding code. 
The chapter is divided into three main sections. First the reader is introduced to the basic 
notions of simulation and Monte Carlo integration. In Section 2.2, Markov chain Monte 
Carlo methods and their general setup are described. Finally, the most popular methods are 
described and illustrated using simple examples. The methods are described and illustrated 
in detail to introduce the reader to the implementation logic of such algorithms. Readers 
who wish to learn more about the technical and theoretical details of this topic should refer 
to more specialized books such as those by Gilks et al. (1996), Givens and Hoeting (2005), 
and Gamerman and Lopes (2006). 
2.1 SIMULATION, MONTE CARLO INTEGRATION, AND THEIR 
IMPLEMENTATION IN BAYESIAN INFERENCE 
In quantitative sciences, the problem of evaluation of integrals of the type 
I = J g ( x ) d x  
X 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
31 

32 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
is often required. Several solutions have been proposed in the literature, including either 
approximations or computationally intensive methods. One of them is based on generating 
random samples and then obtaining the integral shown above by its statistical unbiased 
estimate, the sample mean. Hence let us assume that the density function f(z) 
of a random 
variable enables us to easily generate random values. This can be expressed as 
where g* (x) = g(z)/f(x). Hence the integral I can be efficiently estimated by 
1. 
2. 
Generating z(') 
~ d2), 
. . . , dT) from the target distribution with probability density 
function (p.d.f.) f(z) 
Calculating the sample mean 
This concept was known from the early days of the electronic computers and was originally 
adopted by the research team of Metropolis in Los Alamos (Anderson, 1986; Metropolis 
and Ulam, 1949). 
The main advantage of this approach is its simplicity. Even if integrals are tractable, 
nowadays it is much easier to generate samples than calculate high-dimensional integrals. 
The accuracy of sample mean (2.1) as an estimate of the integral I is of order O(T -1/2) 
(Givens and Hoeting, 2005, p. 144), which implies slow convergence toward the true value. 
Nevertheless, the generated sample size T here can be specified by the researcher. For a 
suitable large generated sample (say, e.g., T = 10,000), this approach is very accurate 
(Gamerman and Lopes, 2006, p. 96). 
The method described above is directly applicable to many problems in Bayesian in- 
ference. Hence for every function of the parameter of interest G(8), we can calculate the 
posterior mean and variance by simply 
1. Generating a sample 8('), 8(2)1 
. . . , dT) from the posterior distribution f(8iy). 
2. Calculating the sample mean of G(8) by simply calculating the quantity 
1 
f = - 1 
G(8(t)). 
t=l 
T 
Simulation can also be used to estimate and visualize the posterior distribution of G(0) itself. 
This can be done with the assistance of kernel estimates of the values G (8(l)) G (d2)), 
The main problem in the above mentioned procedure is how to generate from the posterior 
density f(8ly) which, in most cases, is not straightforward. The posterior distributions 
resulting from conjugate priors is the simplest possible case in which interest may lie in the 
computation of posterior summaries of a function G(8) of the parameters 8 that will not 
be analytically available. In order to generate random values, a number of approaches are 
available (see References list at the end of this book), such as the method using the inverse 
cumulative distribution function, rejection sampling algorithms, and importance sampling 
. . . ~ ( d ~ ) ) .  

SIMULATION, MONTE CARL0 INTEGRATION, AND THEIR IMPLEMENTATION IN BAYESIAN INFERENCE 
33 
methods; for further details, see Carlin and Louis (2000, pp. 129-137), Gelman et al. (1995, 
chap. lo), and Givens and Hoeting (2005, pp. 145-162). 
Example 2.1. A simple example: Risk measures in medical research. In bio- 
statistics a 2 x 2 contingency table frequently arises by the cross-tabulation of the 
disease under investigation (denoted by Y with Y = 1 to indicate a disease case 
and zero otherwise) and an exposure risk factor (denoted by X with value one if 
the subject is exposed to the risk factor and zero otherwise). In such cases, the re- 
searcher is interested in estimating certain measures of risk such as the attributable 
risk (AR), the relative risk (RR), and the odds ratio (OR). To be more specific, the 
attributable risk (or risk difference) is given by 
AR = P(Y = 11X = 1) - P(Y = 1/X = 0) = “1 - “ 0 ,  
the relative risk is defined as 
P(Y = 1/x = 1) 
P(Y = 1IX = 0) 
7r1 - 
- 
R R =  
- 
“0’ 
and finally the odds ratio is equal to 
P(Y = 1JX = l)P(Y = o/x = 0) 
P(Y = 01x = l)P(Y = 1IX = 0) 
“1(1 - “0) 
“o(1 - “1) ’ 
- 
OR = 
- 
In this problem we have a product binomial likelihood given by 
Y1 N binomial(r0. no) and YO 
N binomial(7r1, nl) 
with the parameters of interest as 8 = ( T O ,  “1). Assuming beta priors for the success 
probabilities TO, 
7r1 with parameters (UO. bo) and (ul. 
bl), respectively, we end up with 
posteriors given by 
TOJY 
N beta(y0 + U O ,  no + bo) and TI ly N beta(y1 + U I ,  
n1 + b l ) .  
The posterior distribution of AR, RR, and OR cannot be obtained in a straightforward man- 
ner. Derivation of approximate expressions may be based on large sample approximations 
of “0 and “1 and their logarithm as in the classical approach [see, e.g., Woodworth (2004, 
chap. 7)]. 
By simulating values directly from the posterior distributions f ( ~ o l y )  
and f(7r1 ly), 
the estimation of the posterior distribution of the risk measures and their corresponding 
summaries can be easily obtained without hrther analytical or approximate calculations. 
Let us consider a simple illustrative example where we have traced 25 cases over a sample 
of 300 subjects exposed to a risk factor (exposed group, X = 1) while we have traced 30 
cases over 900 subjects in the nonexposed group ( X  = 0). From these givens, y 1 = 25, 
n1 = 300, yo = 30, and no = 900. Hence, for prior parameters a0 = bo = a1 = bl = 1, 
the resulting posteriors are given by 
roly N beta(26.301) and 7r1 Jy 
N beta(31,gOl). 
We can estimate the posterior distributions of the risk measures AR, RR, and OR using 
the following simple steps. For t = 1, . . . T 

34 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
1. Generate 7rt) - beta(26,301) 
2. Generate T?) N beta(31,gOl). 
3. Calculate AR(t), RR(t) and OR(t) using the expressions 
After we completing this generation, we obtain a simulated sample of size T from the 
posterior distributions of AR, RR, and OR. Posterior summaries can be easily obtained 
using sample estimates from this sample. Implementation of this simple generation in 
pseudocode commands as well as in R code is outlined in Table 2.1. Results are summarized 
in Table 2.2 along with histograms of the generated values, and their corresponding kernel 
estimates of the posterior densities are depicted in Figure 2.1; for more details concerning 
kernel estimates, see, for example, Scott (1992). Later in this book, we directly plot the 
posterior density estimates instead of the corresponding histograms of the simulated values. 
Table 2.1 
corresponding posterior distributions in Example 2.1 
Pseudocode and R code (and results) for generation of AR, RR and OR from 
Algorithm (pseudocode) 
R code 
Set up prior parameters 
Set up data 
Generate 1000 observations from the posterior of 
Calculate the corresponding values for risk mea- 
Calculate posterior means 
(results) 
T O  and n1 
sures 
Calculate posterior standard deviations 
(resiil ts) 
Calculate posterior credible intervals for AR 
(rcsults) 
Calculate posterior credible intervals for RR 
(results) 
Calculate posterior credible intervals for OR 
(results) 
Plot the estimated posterior densities 
> a < - l ;  aO<-al<-bO<-bl<-a 
> yi<- 25; yo<- 30; nl<-300; n0<-900 
> PO <- rbeta(1000, yO+aO, nO+bO) 
> p i  c- rbeta(1000, y l + a i ,  n l + b l )  
> AR <- pl-PO; RR <- pl/pO 
> OR <- pl*(l-pO)/(pO*(l-pl)) 
> mean(AR) ; mean(RR) ; mean(0R) 
[ 11 0.04645852 
[I] 2.459779 
[1] 2.605217 
> sd(AR) ; sd(RR) ; sd(OR) 
[l] 0,01578586 
[I] 0.5198302 
[l] 0,6964608 
> quantile(AR,c(0.025, 0.975)) 
2.5% 97.5% 
0.01792363 0.07805458 
> quantile(RR,c(0.025, 0 . 9 7 5 ) ) ;  
2.5% 97.5% 
1.456754 3.770378 
> quantile(OR,c(0.025, 0.975)) 
2.5% 97.5% 
1.482756 4.100452 
> par(mfrow=c(3, 1 ) )  
> plot(density(AR), main=”, xlab=’Attributable Risk’, 
+ ylab=’Posterior Density’) 
> plot(density(RR), main=”, xlab=’Relative Risk’, 
+ ylab=’Posterior Density’) 
> plot(density(OR), main=”, xlab=’Odds R a t i o ’ ,  
+ ylab=’Posterior Density’) 

MARKOV CHAIN MONTE CARLO METHODS 
35 
Table 2.2 
Posterior summaries of risk measures obtained from simulation of 1000 values 
Standard 
Percentiles 
Risk measure 
Mean 
deviation 
2.5%-97.5% 
Attributable risk (AR) 
0.046 
0.016 
0.018 - 0.078 
Relative risk (RR) 
2.470 
0.620 
1.457 - 3.770 
Odds ratio (AR) 
2.605 
0.696 
1.483 -4.100 
000 
0 0 2  
004 
006 
008 
010 
0 1 2  
000 
002 
004 
006 
008 
010 
012 
Attributable risk 
Attributable risk 
:m 
0 
0
,
 
I 
I 
I 
I 
e
o
 
- 0 
1 
2 
3 
4 
5 
1 
2 
3 
4 
5 
Relative risk 
Relative risk 
1 
2 
3 
4 
5 
6 
1 
2 
3
4
 5
6
 
Odds ratio 
Odds ratio 
Figure 2.1 
Histograms of posterior samples and plots of estimated posterior densities of AR, RR, 
and OR using 1000 simulated values. [Note: Posterior densities have been estimated using kernel 
density estimates (command density in R); see example in Scott (1992) for details.] 
2.2 MARKOV CHAIN MONTE CARLO METHODS 
The simulation methods described in the previous section (also called methods of “direct” 
simulation) cannot be applied in all cases. They refer mostly to unidimensional distributions. 
Moreover, some of them are focused on the effective computation of specific integrals 
and cannot be used to obtain samples from any posterior distribution of interest (Givens 
and Hoeting, 2005, p. 183). Simulation techniques based on Markov chains overcome 
such problems because of their generality and flexibility. These characteristics, along 
with the massive development of computing facilities, have made Markov chain Monte 
Carlo (MCMC) techniques popular since the early 1990s. MCMC methods are not new, 

36 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
as they were introduced into physics in 1953 in a simplified version by Metropolis and 
his associates (Metropolis et al., 1953). Intermediate landmark publications include the 
generalization of Metropolis algorithm by Hastings (1970) and development of the Gibbs 
sampler by Geman and Geman (1984). Nevertheless, it took about 35 years until MCMC 
methods were rediscovered by Bayesian scientists (Tanner and Wong, 1987; Gelfand et al., 
1990; Gelfand and Smith, 1990) and became one of the main computational tools in modem 
statistical inference. 
Markov chain Monte Carlo techniques enabled quantitative researchers to use highly 
complicated models and estimate the corresponding posterior distributions with accuracy. 
In this way, MCMC methods have greatly contributed to the development and propagation 
of Bayesian theory. Extensive details of the use of MCMC methods can be found in Gilks 
et al. (1996). BUGS (Spiegelhalter et al., 1996a) and WinBUGS software (Spiegelhalter 
et al., 20036) use MCMC techniques to generate samples from posterior distribution of 
complicated models, providing an effective way to evaluate Bayesian models. 
In the following sections, we will provide the elementary notions of MCMC algorithms 
in order to introduce the reader to the terminology and the logic of MCMC simulation. 
MCMC techniques are based on the construction of a Markov chain that eventually “con- 
verges” to the target distribution (called stationary or equilibrium) which, in our case, is the 
posterior distribution f(8ly). This is the main way to distinguish MCMC algorithms from 
“direct” simulation methods, which provide samples directly from the target - 
posterior 
distribution. Moreover, the MCMC output is a dependent sample since it is generated from 
a Markov chain, in contrast to the output of “direct” methods, which is an independent 
sample. Finally, MCMC methods incorporate the notion of an iterative procedure (for this 
reason they are frequently called iterative methods) since in every step they produce values 
depending on the previous one, 
2.2.1 The algorithm 
A Murkov chain is a stochastic process {@(I), d2), 
. . . , 8(T)} such that 
f(e(t++l) 
le(t),, 
, , . ,,g(+l)) = f(e(t++l) 
e(t) . 
that is, the distribution of 8 at sequence t + 1 given all the preceding 6 values (for times 
t, t - 1, . . . , 1) depends only on the value 8(t) of the previous sequence t. Moreover, 
f(d”” Idt)) is independent of time t. Finally, when the Markov chain is irreducible, 
aperiodic, and positive-recurrent, as t -+ m the distribution of 6 ( t )  conver es to its equi- 
librium distribution, which is independent of the initial values of the chain Of’); for details, 
see Gilks et al. (1996). 
In order to generate a sample from f(8ly), we must construct a Markov chain with 
two desired properties: (1) f(8(t+1) 
Idt)’) 
should be “easy to generate from”, and (2) the 
equilibrium distribution of the selected Markov chain must be the posterior distribution of 
interest f (6ly). 
Assuming that we have constructed a Markov chain with these requirements, we then 
1. Select an initial value do). 
2. Generate T values until the equilibrium distribution is reached. 
3. Monitor the converge of the algorithm using convergence diagnostics. If convergence 
I 
1% 
diagnostics fail, we then generate more observations. 

MARKOV CHAIN MONTE CARL0 METHODS 
37 
4. Cut off the first B observations. 
5. Consider { dB+'), 
8(B+2), 
. . . 
~ dT)} 
as the sample for the posterior analysis. 
6. Plot the posterior distribution (usually focus is on the univariate marginal distribu- 
tions). 
7 .  Finally, obtain summaries of the posterior distribution (mean, median, standard de- 
viation, quantiles, correlations). 
In these steps, we refer to convergence diagnostics, which are statistical tests that attempt to 
identify cases where convergence is not achieved. More details follow in the next section, 
along with terminology and additional implementation details of the preceding steps. 
2.2.2 Terminology and implementation details 
In this section we present the basic concepts related to MCMC algorithms. It is divided 
into four subsections. Section 2.2.2.1 briefly introduces the reader to the basic terminology 
used in the topic, while Sections 2.2.2.2-2.2.2.4 focus on more specific matters such as 
the analysis of the MCMC sample, estimation of Monte Carlo variability measures, and 
convergence of the algorithm. 
2.2.2.1 Definitions and initial terminology. In this subsection we present epigram- 
matically the most important notions of MCMC methodology. We avoided unnecessary 
long discussion of the topic in order to to introduce readers in a straightforward manner. 
More detailed description of related topics can be found in more specialized books related to 
MCMC methodology; see for example in Gilks et al. (1 996), Gamerman and Lopes (2006), 
and Givens and Hoeting (2005). 
Equilibrium distribution. This is called the stationary or target distribution of the MCMC 
algorithm. The notion of the equilibrium distribution is related to the Markov chain used 
to construct the MCMC algorithm. Such chains stabilize to the equilibriudstationary 
distribution after a number of time sequences t > B. Therefore, in a Markov chain, the 
distribution of 8(t) and 8("') will be identical and equal to the equilibriumistationary 
distribution. Equivalently, once it reaches its equilibrium (distribution), an MCMC scheme 
generates dependent random values from the corresponding stationary distribution (Robert 
and Casella, 2004, pp. 206-207). 
Convergence of the algorithm. With the term convergence of an MCMC algorithm, 
we refer to situations where the algorithm has reached its equilibrium and generates values 
from the desired target distribution. Generally it is unclear how much we must run an 
algorithm to obtain samples from the correct target distributions. Several diagnostic tests 
have been developed to monitor the convergence of the algorithm; for more details, see 
Section 2.2.2.4. 
Iteration. Iteration refers to a cycle of the algorithm that generates a full set of parameter 
values from the posterior distribution. It is frequently used to denote an observation of 
simulated values. Here iteration is denoted by superscript number or the corresponding 
index t in parentheses. For example, 8(5) and dt) respectively denote the values of random 
vector 8 generated at the 5th and tth iterations of the algorithm. 

38 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
Total number of iterations T .  This refers to the total number of the iterations of the 
MCMC algorithm. 
lnitial values of the chain (-I(’). 
The starting values used to initialize the chain are simply 
called initial values. These initial values may influence the posterior summaries if they are 
far away from the highest posterior probability areas and the sample size of the simulated 
sample T is sufficient to eliminate its effect. We can mitigate or avoid the influence of 
the initial values by removing the first iterations of the algorithm or letting the algorithm 
run for a large number of iterations or obtain different samples with different starting 
points. Other researchers, in order to ensure that the values will be close to the center of the 
posterior distribution, select as starting points the posterior mode or the maximum likelihood 
values if they are easy to obtain. The latter choice often provides poor starting points 
according to many researchers, including Radford Neal (Kass et al., 1998, p. 96). Other 
reasonable starting points can be based on the mean or the mode of the prior distribution 
when informative priors are used. Finally, for problems with multiple modes or “ridges” 
(modes in highly correlated multivariate densities), using multiple chains with different 
starting points is highly recommended; for more details, see Brooks (1998) and the fruitful 
discussion by Kass et al. (1 998). 
Burnin period. In the burnin period the first B iterations are eliminated from the sample 
in order to avoid the influence of the initial values. If the generated sample is large enough, 
the effect of this period on the calculation of posterior summaries is minimal. 
Thinning interval or sampling lag. As has already been mentioned, the final MCMC 
generated sample is not independent. For this reason, we need to monitor the autocorrela- 
tions of the generated values and select a sampling lag L > 1 after which the corresponding 
autocorrelation are low. Then, we can produce an independent sample by keeping the first 
generated values in every batch of L iterations. Hence, if we consider a lag (or thin interval) 
of three iterations then we keep the first every three iterations (i.e., we keep observations 
1, 4, 7, etc.). This tactic is also followed to save storage space or computational speed in 
high-dimensional problems. 
Iterations kept, T’. These are the number of the iterations retained after discarding the 
initial burnin iterations (i.e., T’ = T - B). Ifwe also consider a sampling lag L > 1, then 
the total number of iterations kept refers to the final independent sample used for posterior 
analysis. 
MCMC output. This refers to the MCMC generated sample. We often refer to the MCMC 
output as the sample after removing the initial iterations (produced during the bumin period) 
and considering the appropriate lag. 
Output analysis. This refers to analysis of the MCMC output sample. It includes both 
the monitoring procedure of the algorithm’s convergence and analysis of the sample used for 
the description of the posterior distribution and inference about the parameters of interest; 
for more details, see Sections 2.2.2.2 and 2.2.2.4, which follow. 
2.2.2.2 Describing the target distribution using MCMC output. The MCMC 
output provides us with a random sample of the type 
dl), d2), 
. . . , dt), 
. . . , 

MARKOV CHAIN MONTE CARL0 METHODS 
39 
From this sample, for any hnction G(8) of the parameters of interest 8 we can 
1. Obtain a sample of the desired parameter G(8) by simply considering 
G ( dl)) 
, G ( d2)), 
. . . , G ( @)), . . . , G ( dT')) 
2. Obtain any posterior summary of G(8) from the sample using traditional sample 
estimates. For example, we can estimate the posterior mean by 
and the posterior standard deviation by 
Other measures of interest might be the posterior median or quantiles (e.g., 2.5% 
and 97.5% percentiles will provide a 95% credible interval). Finally, the posterior 
mode can be estimated by an MCMC sample by simply tracing the value of 8, which 
maximizes the posterior. This estimate cannot be considered as a reliable one, and 
hence it is preferable to directly consider optimization methods if interest lies in 
estimating the mode of the posterior distribution. 
3. Calculate and monitor correlations between parameters. 
4. Produce plots of the marginal posterior distributions (histograms, density plots, error 
bars, boxplots, etc.). 
2.2-2.3 Monte Carlo error. In the analysis of the MCMC output, an important mea- 
sure that must be reported and monitored is the Monte Carlo error (MC error), which 
measures the variability of each estimate due to the simulation. MC error must be low in 
order to calculate the parameter of interest with increased precision. It is proportional to 
the inverse of the generated sample size that can be controlled by the user. Therefore, for 
a sufficient number of iterations T ,  the quantity of interest can be estimated with increased 
precision. 
The two most common ways to estimate MC error are: the batch mean method and 
the window estimator method. The first one is simple and easy to implement and, for this 
reason, popular, while the latter is more precise. 
In order to calculate the MC error using the batch means method, we simply partition 
the resulting output sample in K batches (usually K = 30 or K = 50). Both the number 
of batches K and the sample size of each batch v = T ' / K  must be sufficiently large in 
order to enable us to estimate the variance consistently and also eliminate autocorrelations 
(Carlin and Louis, 2000, p. 172). 
To calculate - 
the Monte Carlo error of the posterior mean of G(8), we first calculate each 
batch mean G( 8) by 
. 
bu 
G(8), = 
G(dt)) 
t=(b-l)v+l 

40 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
for each batch b = 1; . . . , K ,  and the overall sample mean by 
. T' 
+
K
 
assuming that we keep O ( ' ) ,  . . . , 
simply given by the standard deviation of the batch means estimates G(8) 
observations. Then an estimate of the MC error is 
MCE[G(8)] = s^E [W] 
= /+% [Gob] 
1 5 
( G ( B j b - G / 8 ) ) 2  
b= 1 
K ( K  - 1) 
The procedure for calculating the MC error for any other posterior quantity of interest 
U = U ( € J c l ) ,  . . . . 8(T')) is equivalent. To estimate the corresponding Monte Carlo error, 
we calculate o b  = U(8((b-1)"f1), 
. . . , 8(bu)) 
from each batch b = 1. . . . . K and then the 
MC error by 
The batch mean estimator of the Monte Carlo error is discussed in more detail by Hastings 
(1970), Geyer (1992), Roberts (1996, p. 50), Carlin and Louis (2000, p. 172), and Givens 
and Hoeting (2005, p. 208). 
The second method (window estimator) is based on the expression of the variance in 
autocorrelated samples given by Roberts (1996, p. 50) 
where i k  [G(B)] is the estimated autocorrelation of lag k, that is, the correlation between 
parameters G(8(t)) and G(8(t+k)). 
Thus, it is obvious that for large k the autocorrelations 
will not be estimated reliably from the sample because of the small number of remaining 
observations. Moreover, in practice the autocorrelation will be close to zero for a suffi- 
ciently large k. For this reason, we identify a window w after which autocorrelations are 
considerably low [say, < 0.1 (Carlin and Louis, 2000)] and discard j3k with k > 21: from 
the preceding MC error estimate. Hence, this window based modified MC error estimate 
is given by 
VF 
k= 1 
MCE[G(8)] = 
The quantity ESS = N /  41 + 2 c;==, 
p k  [G(O)] is also referred to as the effective sample 
size (Kass et al., 1998). 
Other methods for estimating Monte Carlo error are discussed by Geyer (1992) and 
Carlin and Louis (2000, pp. 170-172). 

MARKOV CHAIN MONTE CARLO METHODS 
41 
2.2.2.4 Convergence of the algorithm. This term refers to whether the algorithm 
has reached its equilibrium (target) distribution. If this is true, then the generated sample 
comes from the correct target distribution. Hence, monitoring the convergence of the 
algorithm is essential for producing results from the posterior distribution of interest. 
There are many ways to monitor convergence. The simplest way is to monitor the MC 
error (calculated as described above) since small values of this error will indicate that we 
have calculated the quantity of interest with precision. Monitoring autocorrelations is also 
very useful since low or high values indicate fast or slow convergence, respectively. 
A second way is to monitor the trace plots: the plots of the iterations versus the gen- 
erated values. If all values are within a zone without strong periodicities and (especially) 
tendencies, then we can assume convergence. An example of trace plots from an MCMC 
run is provided in Figure 2.2. In the first trace plot, we can clearly see the burnin period 
(within the gray box), which must be discarded from the final sample. After this period 
the generated sampled values, are stabilized within a zone. In the second plot, the initial 
200 iterations have been discarded to monitor the sampled values which demonstrate much 
better behavior with small periodicities (up and down periods in the graph). Finally, gener- 
ated observations of the last trace plot are more convincing in terms of convergence, with 
all generated values within a parallel zone and no obvious tendencies or periodicities. 
Figure 2.2 
after discarding the first 200, and (c) 3000 iterations after discarding the first 200. 
Trace plots (iterations vs. generated values) for (a) 300 iterations, (b) 1000 iterations 
Another useful plot is produced by depicting the evolution of the ergodic mean of a 
quantity over the number of iterations. The term ergodic mean refers to the mean value 

42 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
until the current iteration. If the ergodic mean stabilizes after some iterations, then this is 
an indication of the convergence of the algorithm. 
Another tactic, which is very efficient in practice, is to run multiple chains with different 
starting points. When we observe that the lines of different chains mix or cross in trace 
andor the ergodic mean plots, then convergence is ensured. 
Finally, several statistical tests have been developed and used as convergence diagnostics 
(Cowles and Carlin, 1996; Brooks and Roberts, 1998). CODA (Best et al., 1996) and 
BOA (Smith, 2005) software programs have been developed in order to implement such 
diagnostics to the output of BUGS and WinBUGS software. Note that all convergence 
diagnostics work like alarms that sound when they detect an unexpected anomaly in the 
MCMC output. Each diagnostic test is constructed to detect different problems and hence, 
in most cases, all diagnostics must be applied to ensure that convergence has been reached. 
To summarize, experienced users follow simple and fast tactics for monitoring conver- 
gence, including plotting autocorrelations, trace plots, and ergodic means. More advanced 
techniques such as multiple-chain comparisons and convergence diagnostics must be used 
for high dimensional complicated posterior distributions; see Kass et al. (1 998, sec. 2.2) for 
further suggestions on the subject. We may improve the mixing of the chain, reducing in 
this way the time to reach convergence, by implementing approaches such as transformation 
of the parameters of interest or selecting an efficient method appropriate for the specific 
problem under consideration; for more details, see Gilks and Roberts (1996). 
2.3 POPULAR MCMC ALGORITHMS 
The two most popular MCMC methods are: the Metropolis-Hastings algorithm (Metropolis 
et al., 1953; Hastings, 1970) and the Gibbs sampling (Geman and Geman, 1984). 
Many variants and extensions of these algorithms have been developed. Although they 
are based on the principles of the original algorithms, most of these algorithms are more ad- 
vanced and complicated than the original ones andusually focus on specific problems. Some 
important more recent developments reported in the MCMC literature are the slice sampler 
(Higdon, 1998; Damien et al., 1999; Neal, 2003), the reversible jump MCMC (RJMCMC) 
algorithm (Green, 1995), and perfect sampling (Propp and Wilson, 1996; Merller, 1999). 
In the following subsections, we focus on these two most popular methods (Metropolis- 
Hastings algorithm and the Gibbs sampler). For additional information concerning MCMC- 
specific methods, see Gilks et al. (1996), Robert and Casella (2004), Givens and Hoeting 
(ZOOS), and Gamerman and Lopes (2006). 
2.3.1 The Metropolis-Hastings algorithm 
Metropolis et al. (1953) originally formulated the Metropolis algorithm, by introducing the 
Markov-chain-based simulation methods used in science. Later, Hastings (1 970) general- 
ized the original method in what is known as the Metropolis-Hustings algorithm. The latter 
is considered to be the general formulation of all MCMC methods. Green (1995) further 
generalized the Metropolis-Hastings algorithm by introducing reversible jump Metropolis- 
Hustings algorithms for sampling from parameter spaces with different dimensions. 
Let as assume a target distribution f (z) from which we wish to generate a sample of size 
T .  The Metropolis-Hastings algorithm can be described by the following iterative steps; 
where z ( ~ )  
is the vector of generated values in t iteration of the algorithm: 
1. Set initial values z ( O ) .  

POPULAR MCMC ALGORITHMS 
43 
2. For t = 1. . . . . T repeat the following steps 
a. Set z = ~ ( t - ~ )  
b. Generate new candidate values z’ from a proposal distribution q(z + z’) = 
c. Calculate 
q(x’/z). 
d. Update ~
(
~
1
 
= 2’ with probability Q and ~
(
~
1
 
= 2 = z(~-’) 
with probability 
1 - Q  
The Metropolis-Hastings algorithm will converge to its equilibrium distribution regardless 
of whatever proposal distribution q is selected. Nevertheless, in practice, choice of the 
proposal is importance since poor choices will considerably delay convergence towards the 
equilibrium distribution. 
The algorithm outlined above can be directly implemented in Bayesian framework by 
substituting z by the parameters of interest 8 and the target distribution f (x) 
by the posterior 
distribution f(8ly). Thus, in Bayesian inference, the algorithm is summarized as follows: 
1. Set initial values do). 
2. Fort = 1. . . . . T repeat the following steps 
a. Set e = d--l) 
b. Generate new candidate parameter values 8’ from a proposal distribution q( 8’ 1 8). 
c. Calculate 
d. Update 8(t) = 8’ with probability a; otherwise set 8(t) = 8. 
An important characteristic of the algorithm is that we do not need to evaluate the normal- 
izing constant f(y) involved in f (8ly) since it cancels out in a. Hence the acceptance 
probability is simplified to 
In order to simplify notation, in the following we denote the current state of the chain 
without any superscripts (e.g., by z or 8). 
Special cases of the Metropolis-Hastings algorithm are the random-walkMetropolis, the 
independence sampler, the single-component Metropolis-Hastings, and the Gibbs sampler. 
These frequently used algorithm adaptations are described below. 
2.3.7.7 
Random-walk Metropolis In the original Metropolis algorithm (Metropo- 
lis et al., 1953), only symmetric proposals of type q(8’18) = q(818’) were considered. 
Random-walk Metropolis is a special case with q(8’18) = q(i8’ - 81). Both cases result 
in an acceptance probability that depends only on the posterior (target) distribution 

44 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
A usual proposal of this type is a multivariate normal q(8’18) = Nd(8; so), 
where d is the 
dimension of 8. The covariance matrix 9, controls the convergence speed of the algorithm. 
The values of the variances s,, determine how close the proposed and current values will 
be. Small values of s,, will result in high acceptance rates but slow convergence since the 
algorithm will need a large number of iterations to explore the parameter space. In this case, 
large autocorrelations will appear in the output analysis. On the other hand, high values 
of the proposal variances g,$ will result in low acceptance rates. As a consequence, for a 
large number of iterations the algorithm will stick with the same values, again resulting in 
poor exploration of the parameter space and a highly autocorrelated sample. In order to 
specify the elements of 3 0 ,  the user has to test the performance of the sampler by using 
several values until the desired acceptance rate is achieved; see Roberts and Rosenthal 
(200 1) for details. This empirical procedure is called tuning ofthe proposal distribution. 
The corresponding quantities are called tuningparameters of the proposal. In random walk 
samplers the optimal acceptance rate according to Roberts et al. (1997) andNeal andRoberts 
(2008) is around 25%, ranging from 0.23 for large dimensions to 0.45 for the univariate 
case; see also in Roberts and Rosenthal(200 1). Here we recommend tuning the variance of 
the proposal density such that the acceptance rates lie within the interval of 20&40%, which 
are the values also proposed and used by Spiegelhalter et al. (2003d, p. 6). This range is 
in concordance with the range of 1040% also suggested by Roberts and Rosenthal(2001) 
after observing that “there is little to be gained from fine tuning of acceptance rates.” 
The correlations involved in the proposal covariance matrix 9 8  influence the direction 
of the proposed movement. In order to construct an effective Metropolis algorithm, the 
correlation matrix of the proposal and the target posterior distribution must be equivalent. 
A good strategy is to run a small pilot run using diagonal 9 0  (i.e., independent normal 
proposals) to roughly estimate the correlation structure of the target posterior distribution 
5nd then rerun the algorithm using the corresponding estimated variance-covariance matrix 
Xe. After the posterior variance-covariance matrix has been estimated, Gelman et al. 
(1995, pp. - 334-335) suggest that the proposal variance-covariance matrix be set equal to 
So = c2Xo with c2 = 5.8/d, where d is the dimension of the parameter vector. 
- 
2.3.1.2 The independence sampler, The independence sampler is a Metropolis- 
Hastings algorithm where the proposal distribution does not depend on the previous state 
d-’) of the chain. For example a frequent choice is a multivariate normal distribution 
of the type 8’ - Nd(8, 3 0 ) .  The parameters (mean and variance) of this proposal can 
be obtained using approximation methods or any available previous experience or expert 
information. 
The independence sampler is efficient when the proposal q(8) is a good approximation 
of the target posterior distribution f(8iy). Good independent proposal densities can be 
based on Laplace approximation (Tierney and Kadane, 1986; Tierney et al., 1989; Erkanli, 
1994). Thus, a generally successful proposal can be obtained by a multivariate normal 
distribution with mean equal to the posterior mode 
and precision matrix 
that is, minus the second derivative matrix of the log-posterior density 

POPULAR MCMC ALGORITHMS 
45 
evaluated at the posterior mode g. Consequently, an efficient proposal is given by 
The posterior mode can be evaluated by usual optimization methods. When low information 
prior is used, then an adequate proposal can be obtained by setting the mean equal to the 
corresponding maximum-likelihood estimator (MLE) and the precision equal to its observed 
Fisher information matrix. 
The acceptance probability, when proposing a transition from 8 to 8’, is given by 
which can be reexpressed as 
(Y = min 1, - 
( :;;>. 
where w(8) = f(OIy)/q(8) is the ratio between the target and the proposal distribution 
and is equivalent to the importance weight used in importance sampling. In fact, the two 
approaches are very close, with the latter giving more weight to points with high weights 
w(8) (Brooks, 1998). 
In contrast to the random-walk Metropolis, where the optimal acceptance rate is around 
0.25, in the independence sample the acceptance rate must be high enough to obtain an 
efficient algorithm. Although high acceptance rates indicate that the proposal is a sufficient 
approximation of the target posterior distribution, according to Chib and Greenberg (1 995, 
p, 330), the tails of the proposal density must be fatter than the corresponding ones of 
the posterior distribution in order to obtain an efficient algorithm; see also Gamerman and 
Lopes (2006, pp. 199-204) for an interesting discussion concerning the selection of the 
proposal in this case. 
2.3.2 Componentwise Metropolis-Hastings 
In componentwise Metropolis-Hastings algorithm, the parameter vector 8 is divided into 
subvectors that are updated sequentially using Metropolis-Hastings steps. Gibbs sampling, 
which is discussed in Section 2.3.3, is a special case of this algorithm. The algorithm is 
also called Metropolis within Gibbs (see Section 2.3.4 for details) or single-component 
Metropolis-Hastings algorithm when univariate components are updated sequentially. 
In each step of the single-component Metropolis algorithm, a candidate value 6 ;  of 
the jth component of the vector 8 is proposed by q j  (6$ldt-”). The algorithm can be 
summarized by the following steps: 
1. Set initial values do). 
2. For t = 1, . . . , T repeat the following steps 
a. Set e = @ - I ) .  
b. Forj = 1‘ . . . ,  d 
(1) Generate new candidate parameter values 6’; for j component of vector 8’ 
from a proposal distribution q( 8; 18). 

46 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
(2) Calculate 
where 81, is the vector 8 excluding its jth component B3 [i.e., 
= 
(w2,. 
. . :ej-l>eg+l,. . . A)I. 
(3) Update B j  = 84 with probability a. 
C. Set dt) = 8 
One generated observation 8(t) is obtained after updating all components of the parameter 
vector. Generally, the sequence of updating the elements of 8 does not influence the 
convergence of the algorithm. Nevertheless, to ensure randomness, random selection of the 
updating sequence may be also used; this is called random scan. 
The advantage of this MCMC scheme is that the sampler is decomposed in several 
univariate steps in which random-number generation is usually straightforward. On the 
other hand, convergence of the chain cannot be accelerated by using multivariate proposal 
densities with appropriate correlation structure. To improve convergence in such cases, 
sampling can be implemented by parameter blocking. The parameter vector is divided into 
subvectors with correlated elements, which are called blocks, and each of them is updated 
in separate Metropolis steps. More details concerning blocking can be found in Gilks et al. 
(1996). 
2-3.2. 
I 
Simple examples. Two simple examples illustrating how we can build effec- 
tive Metropolis-Hastings algorithms are presented in this subsection. In the first example 
we use a univariate binomial example for Kobe Bryant's field goals and in the second 
example we consider a simple logistic regression model with two parameters under consid- 
eration. Detailed implementation of all methods, comments concerning their performance, 
and interpretation of the results are provided. 
Example 2.2. Univariate example: Posterior distribution of odds and log-odds 
in binomial data. Consider the Kobe Bryant's success data for 2006-2007 season. 
As we have already mentioned, posterior distribution of the success probability 
can be easily obtained analytically using a conjugate prior distribution. Frequently, 
when binomial data are used, there is focus on the estimation of odds. Its posterior 
distribution can be easily evaluated either analytically or by using a sample derived 
from direct sampling. 
In this example, for illustration, the likelihood is expressed in terms of the 
log-odds 0 = log[7r/(1 - T ) ]  instead of the original parameter T ;  log-odds are 
a convenient parameter choice since they are defined in the set of real numbers R 
and, thus, a normal prior can be adopted. 
Estimating the log-odds using a random-walk Metropolis. Let us first rewrite the 
likelihood as a hnction of 0: 

POPULAR MCMC ALGORITHMS 
47 
Since 0 E R, it is natural to consider as a low-information prior a normal distribution with 
zero mean and large variance 0 N N(,uo. c$), which results in 
The above distribution is univariate and therefore we can easily plot it to obtain a general 
overview of the posterior distribution. Nevertheless, it is still difficult to summarize the 
properties of the posterior distribution since evaluation of its summaries are not straightfor- 
ward. Generating a random sample from (2.2) simplifies the evaluation of such posterior 
summaries. 
To avoid numeric overflow problems, it is highly recommended to use the log-scale in 
all statistical computations. Using the log-scale in the following, a simple random-walk 
Metropolis can be summarized by 
0 Specify initial value o(’) 
F o r t =  l . . . . ; T  
I. Set o = Oct-’) 
2. Propose a new value 0’ from N(8, z;) 
3. Calculate log a = min(0. A )  with A given by 
4. Set O ( t )  = 0’ with probability a and O ( t )  = 0 with the remaining probability. 
Parameter 2; is a tuning parameter that needs to be calibrated such that it achieves an 
acceptance rate approximately equal to 25%. Table 2.3 provides a detailed description of 
the algorithm on the left and the corresponding R commands on the right. 
The algorithm and the corresponding program can be divided into two main parts: the 
preamble, where all parameters are initialized and defined, and the main MCMC algorithm, 
where the iterative procedure takes place (the for loop part). In the preamble we define the 
data, and the prior parameters and initialize any vectors that will be used by the algorithm. 
In order to identify a well-performed value for gi, the algorithm needs to be run using 
several values until an acceptance rate close to 0.25 is achieved. For this reason, an additional 
counter must be added in the program. In order to initialize such counter in R, we add the 
following command 
and then update this counter within the i f  statement 

48 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
Table 2.3 
log-odds in binomial data 
Algorithm and R code for running a random walk Metropolis algorithm for 
Algorithm (pseudocode) 
R code 
Set up data 
Set number of iterations T 
Set prior parameters: pe = O , O ;  = 10, 000 
Set proposal parameter: So 
Initialize vector of sampled values f3 
Set initial current o(') 
Fort = 1,. 
, . ,Trepeat 
Propose 0' from N ( p 0 .  Xi) 
Calculate log 01 =loglikelihood( 0' ) 
- loglikelihood( B ) 
+ log-prior( 8' ) 
- log-prior( B ) 
Generate u from U ( 0 ,  1) 
Set u = log u 
If u < log 01 then set 0(') = 0' 
else e(') = B 
End off or loop 
> y<-399; N<-845 
> Iterations<-P5OO 
> mu.theta<-0; s.theta<-lOO 
> pr0p.s <- 0.35 
> theta<-numeric(1terations) 
> current .theta<-0 
> f o r  (t in 1:Iterations)I 
> 
prop.theta<-mom( 1, current.theta, pr0p.s 
> 
loga <-( (prop.theta*y - N*log(l+exp(prop.theta))) 
+ 
-(current.theta * y -N+log(l+exp(current.theta))) 
+ 
+dnorm(prop.theta, mu.theta, s.theta, log=TRUE) 
+ 
-dnorm(current.theta, rnu.theta, s.theta, 
log=TRUE) ) 
> 
u<-runif (1) 
> 
if ( u < loga ) current. theta<-prop. theta 
> 
theta[t]<-current.theta 
> 
u<-log(u) 
> I  
substituting line 16 of the code provided in Table 2.3. The object acc . prob provides the 
number of the accepted moves within the MCMC run. 
After tuning, SQ = 0.35 was finally adopted, achieving acceptance rates close to the target 
value of 0.25. Trace, ergodic mean, and autocorrelation function (ACF) plots for chains 
withso = 0.1.0.2.0.35 and0.85 (withcorresponding acceptance rates approximately equal 
to 62%, 38%, 23%, and ll%, respectively) are illustrated in Figure 2.3. For Be = 0.85, 
long periods of constant 6 are observed in the trace plot. High autocorrelations are observed 
in chains for So = 0.1 and So = 0.85. All posterior means appear to have converged to the 
same value. 
Figure 2.4 demonstrates trace plots for chains using starting values 6 ('1 = -3, -1.5,0, 
1,5. Trace plots for all observations and for observations after discarding a burnin period 
of 500 iterations are provided in the left and right columns, respectively, of Figure 2.4. The 
effect of the initial value is evident in the trace plots of the first and the last chains. This 
effect is also clear in Figure 2.5, where the evolution of the ergodic means for all iterations 
(on the left) and for observations after discarding the bumin period (on the right) is depicted. 
Monte Carlo error values for the posterior mean using 25 batches of size equal to 80 
iterations per batch (after discarding the initial 500 iterations) were found equal to 0.004, 
0.0032, 0.00397, and 0.0052 for So = 0.1, 0.2, 0.35, and 0.85 respectively. Hence the 
choice of 30 = 0.2 (with acceptance rate equal to 0.38) is the best among the four proposal 
variance values used in this example, achieving the lowest sampling variability as expressed 
by the calculated Monte Carlo errors. The estimated posterior density in contrast to the true 
one [given by Eq. (2.2)] is portrayed in Figure 2.6. 
Having generated the posterior sample for 6, we can easily obtain the corresponding 
sample for the odds o and the success probability 7r by equations 
and the corresponding R commands 

POPULAR MCMC ALGORITHMS 
49 
Ergodic mean plot (c=O.l) 
Autocorrelation plot (r=0.1) 
7 
B
Z
 
m
o
j
_
 
P
'
 
I 
I 
I 
I $I_
- 
F 
- - - - - - - 
_ _ _ _ _ _ _ _  - 
_ _  
2 
0 500 
1500 
2500 
0 5 
15 
25 
Iterations 
Lag 
Ergodic mean plot (c=O.2) 
Autocorrelation plot (c=O.2) 
0 500 
1500 
2500 
0 5 
15 
25 
Iterations 
Lag 
Ergodic mean plot ( c = ~ , 3 5 )  
Autocorrelation plot (c=0.35) 
0 500 
1500 
2500 
0 5 
15 
25 
Iterations 
Lag 
Ergodic mean plot (c= 0.85) 
Autocorrelation plot (c=0.85) 
0 500 
1500 
2500 
0 5 
15 
25 
Iterations 
Lag 
Figure 2.3 
c = go. 
Diagnostic plots of Metropolis random-walk algorithm for various proposal parameters 

50 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
Figure 2.4 
(St.=starting). 
Trace plots of Metropolis random-walk algorithm for various starting values 

POPULAR MCMC ALGORITHMS 
51 
Ergodic means (all iterations) 
Ergodic means (iterations 501-2500) 
Figure 2.5 
Ergodic mean plots of Metropolis random-walk algorithm for various starting values 
o(0). 
Density plots of the estimated posterior distributions for the success odds o and probability 
T are given in Figures 2.7 and 2.8, respectively. Finally, posterior descriptive measures are 
given in Table 2.4. 
Table 2.4 
Posterior summaries for Kobe Bryant field goal success rates for season 
2006-2007a 
~~ 
Posterior 
Parameter 
summary 
Log-odds (Q) Odds ( 0 )  Probability ( T )  
Mean 
-0.1 12 
0.896 
0.472 
Median 
-0.109 
0.897 
0.473 
Standard deviation 
0.072 
0.065 
0.018 
2.5% percentile 
-0.261 
0.770 
0.435 
97.5rC percentile 
0.026 
1.026 
0.507 
aRandom-walk, bumin B = 500 iterations; iterations kept T = 2000. 

52 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
Figure 2.6 
2006-2007; dotted line indicates the true posterior given by Equation (2.2). 
Posterior density plots of success log-odds B of Kobe Bryant’s field goals for season 
Figure 2.7 
season 2006-2007. 
Posterior density plots of success odds o = T / (  1 - T )  of Kobe Bryant’s field goals for 
Figure 2.8 
Posterior density plots of success probability 7r of Kobe Bryant’s field goals for season 
2006-2007; dotted line indicates the true posterior based on the posterior density beta(399.01,446.01) 
resulting from conjugate analysis. 

POPULAR MCMC ALGORITHMS 
53 
Working using the odds. Alternatively, we may work directly using the success odds as 
the parameter of interest in the model likelihood. In this case the likelihood is expressed by 
Since o > 0, we can use either a gamma or a log-normal prior distribution. Here we adopt a 
log-normal prior distribution that is equivalent to the one used in the log-odds-based analysis 
since 
0 = logo N N(ps, 0;) w o N LN(p6, a;), 
where LN(pL; 
cr2) denotes the log-normal distribution with parameters p and cr 2 .  With this 
prior, the posterior is given by 
Since the parameter of interest o is positive, we cannot use a normal proposal distribution 
directly. We can still use a normal random walk on 0 = logo, which results to a mul- 
tiplicative version of the random-walk algorithm. The iterative step can now be written 
as 
0 Fort = 1,. . . ,T 
Set 0 = o(t-1) 
Propose a new value 8’ from N(1og o, a;). Set 0’ = 8. 
Calculate log a = min(0. A )  with A given by 
Set o ( ~ )  = o’ with probability a and o ( ~ )  = o with probability 1 - a. 
Variations of this algorithm can be constructed using any distribution defined on the set of 
the positive real numbers with mean equal to the parameter value of the previous state of 
the chain. Using the logic described above, a gamma proposal of type 
0’ N gamma (i , i) 
with mean equal to the previous value o of the chain and variance equal to b o can be 
adopted. Parameter b is a tuning parameter of the proposal variance that must be calibrated 
to achieve appropriate acceptance rates. With this approach the acceptance probability is 
given by log a = min(0, A) with 
where f r ( z ;  a, b) is the probability density function of the gamma(a, b) distribution. 

54 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
An independence sampler for the log-odds for simple binomial data. An efficient 
independence Metropolis-Hastings sampler for 0 can be constructed by considering the 
posterior mode as the mean of the proposal density. The posterior mode can be obtained 
by solving the equation dlog f(Qly)/d0 = 0. In this example, the first derivative of the 
log-posterior density is given by 
for which an iterative numerical method can be used to obtain the posterior mode. In the case 
of a low-information prior distribution, the contribution of the prior part, appearing on the 
right side of(2.3), to the posteriorwill benegligible. Thus, the MLE 8 = log[y/(N-y)] can 
be directly considered as the mean 
of the proposal distribution. The proposal variance 
3; can be set equal to the posterior variance is; approximated by 
where Var(8) = [y-l+ ( N  - y)-’] is the approximate variance of the maximum likelihood 
estimator. 
The iterative step of the algorithm can be described as follows: 
0 F o r t =  l . . . . . T  
1. Set B = ~ ( ~ - l ) .  
2. Propose a new value 8’ from N ( l e ,  s2). 
3. Calculate log cy = rnin(0, A) with A given by 
1 + e” 
- Nlog - 
I + e e ’  
0’ + 0 - 2plj 
0’ + 0 - 2po 
+ 
2s; 
where fllr(x : p. u2) is the probabilitydensity function ofa N(p. u 2 )  evaluated 
at point x. 
4. Set 0(‘) = 8’ with probability a: and Q ( t )  = Q with probability 1 - a. 
Details of the algorithm are given on the left column, with the corresponding R commands 
in the right column of Table 2.5. 
The proposed scheme is very effective with acceptance probability higher than 90%. As 
we have already mentioned, in the independence sampler we wish to obtain high acceptance 
rates in contrast to the random-walk approach, where the optimal acceptance rates are around 
25%. Plots of the output are provided in Figure 2.9 and posterior summaries in the last 
column of Table 2.6. In the sampling scheme presented above we have selected as starting 
point the value of zero. An obvious “good” choice is the posterior mode g. If we initiate 

POPULAR MCMC ALGORITHMS 
55 
Table 2.5 
algorithm for log-odds in binomial data 
Algorithm and R code for running an independence Metropolis-Hastings 
Algorithm (pseudocode) 
R code 
Set up data 
Set number of iterations T 
Set prior parameters: pe = O,O; = 10, 000 
Set proposal parameters 
Initialize vector of sampled values 6 
Initialize acceptance probability counter 
Set initial current do) 
Fort = 1. , . . , T repeat 
Propose 0' from ~ ( p s . ; ; )  
Calculate log a =log-likelihood( 0' ) 
- log-likelihood( B ) 
+ log-prior( 8' ) 
- log-prior( 0 ) 
+ log-proposal( 8; 
p e ,  Z: 
) 
- log-proposal( 8'; C, . Z: 
) 
Generate u from U ( 0 ,  1) 
Set u = log u 
If u < logo then set O(') = 8' 
and update acceptance probability 
counter 
else ect) = e 
End off or loop 
> y<-399; N<-845 
> Iterations<-1500 
> mu.theta<-0; s.theta<-100 
> prop.mu <- log( y/(N-y) ) 
> mle.var <- l/y+l/(N-y); w<-l/(1+mle.var/s.thetaA2) 
> prop. s <- sqrt ( mle . var *w ) 
> theta <- numeric(1terations) 
> acc.prob <- 0 
> current.theta<-0 
> for (t in l:Iterations){ 
> 
prop.theta <- rnorm( 1, prop.mu, pr0p.s 
> 
loga <-( (prop.theta*y - N*log(l+exp(prop.theta))) 
+ 
-(current.theta * y -N*log(l+exp(current.theta))) 
+ 
t 
-dnorm(current .theta, mu.theta, 6 .  theta, 
log=TRUE) 
+ 
+dnorm(current.theta, prop.mu,  prop.^, log=TRUE) 
+ 
> 
u<-runif(l) 
> 
u<-log(u) 
> 
if( u < loga) { 
> 
current.theta<-prop.theta 
> 
acc.prob <- acc.prob+l 
> 
thetart]<-current.theta 
+dnorm (prop. theta, mu. theta, s .theta, log=TRUE) 
-dnorm(prop.theta, prop.mu,  prop.^, log=TRUE) ) 
>
I
 
> )  
the chain from remote values [e.g., 6' ('1 = 51, you will observe that the chain 'sticks' in this 
value. This can be avoided by considering a larger proposal variance of type 3 = cize, 
where ce is a tuning parameter and 5: is approximated using (2.4). This parameter must 
be specified to allow the chain to move even for remote values. For c g  > 1, we specify a 
proposal with larger variance and fatter tails than the target distribution as recommended 
by Chib and Greenberg (1995). 
Table 2.6 
Posterior summaries for Kobe Bryant's field goal success rates for season 
2006-2007" 
Posterior 
Parameter 
summary 
Log-odds (6') 
Odds (0) Probability ( T )  
Mean 
-0.111 
0.897 
0.472 
Median 
-0.110 
0.895 
0.472 
Standard deviation 
0.068 
0.061 
0.017 
2.5% percentile 
-0.247 
0.781 
0.438 
97.570 percentile 
0.021 
1.022 
0.505 
ahdependence sampler, bumin B = 500 iterations; iterations kept T = 1000 

56 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
(a) Convergence plots 
(b) Estimated posterior densities 
for log-odds 6' and success probability T 
Figure 2.9 
(independence sampler, burnin B = 500 iterations; iterations kept T' = 1000). 
Posterior plots for Kobe Bryant's field goal success rates for season 2006-2007 
Example 2.3. Metropolis-Hastings algorithms for the simple binary logistic 
regression: senility symptoms data. Let us consider the data of Agresti (1990, 
pp. 122-123), in which 54 elderly people completed a subtest of the Wechsler 
Adult Intelligence Scale (WAIS) resulting to a discrete score with range from 0 to 
20. The aim of this study was to identify people with senility symptoms (binary 
variable) using the WAIS score. Interest also lies in calculating WAIS scores that 
correspond to increased probability of senility symptoms (i.e., with T > 0.5). The 
data of this example can be found in the book's Website and are reproduced with 
the permission of John Wiley and Sons, Inc. 
In order to identify the effect of WAIS (xi) on the senility symptoms (yi), we use a 
simple logistic regression model 
" a  
Y, N binomial(T,. N, = 1). log - 
= / 3 0 + & x ~  for i =  1*...,54 
1 - 7rz 
The likelihood of this model is given by 
A normal prior is frequently used for the parameters of the logistic regression model. Here 
we consider independent normal prior distributions with zero mean and large variance to 
express prior ignorance. Hence we consider 

POPULAR MCMC ALGORITHMS 
57 
using prior mean p~g, = 0 and large prior standard deviation us3 = 100 to express our prior 
ignorance. This setup results in the posterior density 
A Metropolis-Hastings algorithm can be built using either a multivariate normal proposal 
with mean the previous step (random-walk) or with mean the posterior mode (independence 
sampler). 
Bivariate random- walk approach: Independent normals proposal. For this exam- 
ple, we firstly consider a simple proposal of type 
The iterative step of the algorithm can be summarized by the following steps 
0 F o r t =  l . . . . . T  
2. Propose new values p’ = (Bb, 
from N(PO.Z$~) 
and N ( P I ; S $ ~ ) ,  
respec- 
tively. 
3. Calculate log Q = min(0. A) with A given by 
4. Update P(t) = p’ with probability a! or keep the same values with probability 
1 - a!. 
Table 2.7 demonstrates in more detail the algorithm in parallel to the appropriate R code. 

58 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
Table 2.7 
independent normal proposals for logistic regression model in Example 2.3 
Algorithm and R code for running a random-walk Metropolis algorithm with 
Algorithm (pseudocode) 
R code 
Set up data 
Set number of iterations T 
Set prior parameters: poo = ppl = 0, 
Set proposal parameters 
Initialize vector of sampled values p 
Initialize acceptance probability counter 
Set initial current p(O) 
Fort = 1,. . . T repeat 
,Yoo = up1 = 100 
Propose p’ from N(O,, ?g3 ); j = 0.1 
Calculate the linear predictor 7 for current and 
proposal parameters 
Calculate log oi =log-likelihood( y; p‘ ) 
- log-likelihood( y; p ) 
+ log-prior( p’ ) 
- log-prior( p ) 
Generate u from U ( 0 ,  1) 
Set u = log u 
If u < log LY then set p(t) = p’ 
else ~
(
~
1
 
= p 
and update acceptance probability 
counter 
End off or loop 
> waisc-read. table (’wais. dat’ ,header=TRUE) 
> y<-wais$senility; x<-wais$wais 
> Iterations<-2500 
> mu.beta<-c(0.0) ; s .beta<-c(lOO,lOO) 
> prop. s<-c(O. 1,O. 1) 
> beta <- matrix(nrow=Iterations, ncol=2) 
> acc.prob <- 0 
> current.beta<-c(O,O) 
> for (t in l:Iterations){ 
> 
prop.beta<- m o m (  2, current.beta, pr0p.s ) 
> 
cur.eta<-current.beta[ll+current.beta[2l*x 
> 
prop.eta<-prop.beta[ll+prop.beta[Zl *x 
> 
loga <-(sum( y*prop.eta - log( l+exp(prop.eta) ) ) 
+ 
-sum( y+cur.eta - log( l+exp(cur.eta ) ) ) 
+ 
+sum(dnorm(prop,beta, mu.beta,s.beta,log=TRUE)) 
+ 
-sum(dnorm(current.beta,mu.beta,s.beta,log=TRUE))) 
> 
u<-runif (1) 
> 
u<-log(u) 
> 
if( u < loga) { 
> 
current.beta<-prop.beta 
> 
acc.prob <- acc.prob+l 
> 
beta[t,l<-current.beta 
’
}
 
> I  
In our case, tuning parameters are set equal to Fp, = 0.2 for both j = 0 and j = 1 
achieving acceptance rates of N 20%. The algorithm was initially run for 2500 iterations. 
Diagnostic plots in Figure 2.10i indicate high autocorrelation and lack of convergence since 
ergodic means have not stabilized. For this reason, the number of iterations was increased to 
55,000 to ensure convergence excluding the initial 8000 iterations as burnin period. In order 
to eliminate high autocorrelations, we consider a thinning interval equal to 650 iterations 
(see Figure 2.1 Oii). 

POPULAR MCMC ALGORITHMS 
59 
(a) Trace plot of 30 
(b) Trace plot of 31 
(a) Trace plot of 30 
(b) Trace plot of 31 
(c) Ergodic mean plot of 3O 
(c) Ergodic mean plot of 31 
(c) Ergodic mean plot of 3O 
(e) Ergodic mean plot of 0, 
0 
IW 
>wo 
~ S M  2wo 
2im 
0 
300 
lWP 
,500 i c e  
ZSW 
0 
lDom 
3
m
 
5 m o  
o 
I- 
m m  
im 
IIelatlonS 
llelaflOnS 
Iterations 
lleratlonl 
Autocorrelations for 30 
Autocorrelations for 3, 
Autocorrelations for 30 
auto correlation^ for 3, 
P. 
p: 
2 2  
~ 
....................... 
..................... 
, 
' 
I T  
9 
i I 
......................... 
........................ 
:L 
0 
5 : l , l !  
10 
1
,
 
15 
2 .  
4 -  
$ :  
o 
?w 
200 
Sm 
iaa 
o 
m 
2m 
zoo 
400 
0 
5 
10 
l i  
Lag 
(Thm=650 ,DIDlm"61 
Lag 
(Thm-650 ~lersmns) 
Lag 
Lag 
(i) 2000 iterations + 500 burnin 
(ii) 33,000 iterations + 8000 burnin 
Figure 2.10 
using a random-walk algorithm with independent normal proposals. 
MCMC diagnostic plots for logistic regression parameters p0 and PI of Example 2.3 

60 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
To visualize the evolution of the chain, the bivariate posterior contour is plotted along 
with the MCMC moves and the simulated values. In Figure 2.1 1, the moves of the chain 
can be monitored for the initial 100, 1000, and 10,000 iterations (with appropriate lags). A 
scatterplot of the generated sample superimposed on the posterior contour is also provided 
in Figure 2.1 Id. Although from the latter figure we can conclude that the algorithm has 
reached convergence, from plots 2.1 la-c it is evident that the exploration of the posterior 
distribution is slow. In the initial 100 iterations (Figure 2.11a), only a small portion of 
the posterior has been explored. Even for 1000 iterations (Figure 2.1 lb) the chain has not 
produced values from the whole space. 
Figure 2.11 
MCMC moves and generated values for logistic regression parameters of Example 2.3 
using a random walk algorithm with independent normal proposals; the contour on upper left of each 
graph refers to the proposal distribution at the initial (0,O) step. 

POPULAR MCMC ALGORITHMS 
61 
Extending the bivariate random- walk approach: Bivariate normal proposal, From 
Figure 2.11, we observe high negative correlation between PO and p1 with estimated value 
(from the previous MCMC scheme) equal to r = -0.964 . This is the reason for the slow 
convergence of the previous algorithm. Using an independent normal proposal in a highly 
correlated space results in an inefficient MCMC scheme with slow convergence even in 
simple cases such as the one illustrated here. 
The rate of convergence of the algorithm can be considerably increased by using a 
multivariate normal proposal distribution. The correlation of the proposal distribution must 
be similar to the posterior one to allow for proposed moves in the same direction. To 
solve this problem, the Fisher information matrix can be facilitated in order to construct an 
efficient algorithm which has similar logic as the random-walk algorithm. Thus, an efficient 
proposal distribution is given by 
where cg is a tuning parameter, specified such that the desired acceptance rate is achieved. 
the log-likelihood is written by 
In the logistic regression case withpregressors for binomial data with Y, 
N binomial(r,, Nt), 
The first order partial derivatives are given by 
and the second ones by 
Hence in the simple logistic regression we can write 
where E 
distribution. The matrix H(P) can be expressed in the form 
is the prior variance covariance matrix of P assuming a multivariate normal prior 
P 
H(P) = XTdiag(hi)X + Z-', 
P 

62 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
-2 
where h, = N, exp (c:=, 
xt3P3) [I + exp (c&, 
523&)] x is the n X P data Or 
design matrix of the model, and h, is related to the dispersion parameter of the model’s 
distributional structure (McCullagh and Nelder, 1989). In the simple regression case X = 
(1, z), 
where 1, is a n-dimensional vector of ones and z is a vector with elements 2 ,  
equal to the observed values of WAIS. 
via an optimization method 
is required. If we wish to avoid this optimization step, we may substitute components h , by 
some naive estimates originating from the response data y,. In the logistic regression case 
these components are equal to h, = N,n2 (1 - n,). A simple approach is to set probabilities 
equal to values directly estimated from the original data. If the model is well fitted, then the 
predicted values will be close to the sample estimates. This approach can be easily adopted 
for binomial but not for Bernoulli (zero-one) data, since, in the latter case, all estimated T 
values will be either one or zero, resulting to a poor approximation of the probabilities T 
needed in h,. An effective alternative is to use the current values of P to estimate structure 
of the variance covariance matrix. Using this approach, the proposal distribution is given 
In the expression above, calculation of the posterior mode 
by 
Note that this proposal is not symmetric since the variance-covariance matrix when moving 
from ,D -+ p’ is different from the corresponding one in the inverse move 0’ 
+ P. The 
iterative step of the algorithm can be described by the following steps: 
0 F o r t =  1, ..., T 
H ( P )  = X’h(p)X + C-l and Sp = c; [H(P)]-’ 
P 
3. Propose a new value p’ = (PA. Pi)T from NZ(p ~ S 
4. Calculate H(P’) = XTh(/3’)X + X i 1  and S p  = c$ [H(P’)] 
-I. 
5. Calculate log a = min(0. A) with A given by 
. 
p 
P )  
1 
2 
--(P - p’)T [SF’ - spl] (p - p’). 

POPULAR MCMC ALGORITHMS 
63 
6. Update ,B(t) = P' with probability cy or keep the same values for ,B(t) with 
probability 1 - a. 
Table 2.8 demonstrates the above algorithm in parallel to the appropriate R code. 
Table 2.8 
normal proposal distributions for logistic regression model in Example 2.3 (The MASS 
package must be loaded in order to be able to use the mvrnorm function) 
Algorithm and R code for the Metropolis-Hastings algorithm with dependent 
Algorithm (pseudocode) 
R code 
Set up data 
Set up matrix X 
Set number of iterations T 
Set prior parameters: po0 = pjl = 0, 
Set proposal parameters 
Initialize vector of sampled values p 
Initialize acceptance probability counter 
Set initial current do) 
Fort = 0. . . , . T - 1 repeat 
U B 0  = UO1 = 100 
Calculate log-likelihood and precision for p 
Propose p' from bivariate normal 
Calculate loglikelihood and precision for p' 
Calculate log a =log-likelibood(y; p') 
- log-likelihood(y: p) 
+ log-prior(p') 
- log-prior(p) 
+ log-proposal(p' -, p) 
- Iog-proposal(p - p') 
Generate u from C(0 1) 
Setu = logu 
If u < log a then 
set ~
(
~
1
 
= p' 
and update acceptance probability counter 
elsep(t) = p 
End off or loop 
Set up a function to calculate log-likelihood and 
precision 
Set up precision limit 
Calculate linear predictor qz for p 
Calculate log(1 + e") 
If qt > precision, set log( 1 + eqx ) = 7% 
Calculate log-likelihood 
Truncate qt in order to calculate h, 
Calculate h, 
Calculate matrix H 
Return the log-likelihood and €3 
> wais<-read.table('wais.dat',header=TRUE) 
> y<-wais$senility; x<-wais$wais 
> n<-length(y); X<-cbind( rep(l,n), x ) 
> Iterations<-2500 
> mu. beta<-c(O,O) ; s .beta<-c(lOO,IOO) 
> c.beta<- 1.75 
> beta <- matrix(nrow=Iterations, ncol=Z) 
> acc.prob c- 0 
> current.beta<-c(O,O) 
> f o r  (t in l:Iterations){ 
> 
cur<-calculate.loglike( current.beta, x, y) 
> 
> 
prop.beta<- rnvrnorm(l,current.beta,solve(cur.T)) 
> 
prop<-calculate.loglike( prop.beta, x, y) 
> 
pr0p.T <-(l/c .beta"Z)*(prop$H+diag(I/s. beta"2)) 
> 
loga <-( prop$loglike 
> 
- cur$loglike 
t 
tsum(dnorm(prop.beta, mu.beta,s.beta,log-TRUE)) 
t 
-sum(dnorm(current.beta,mu.beta,s.beta.log=TRUE)) 
t 
tas.numeric( 0.5*log( det(cur.T) ) 
-0.5*t(current.beta-prop.beta) %*% pr0p.T %*% 
(current.beta-prop.beta) 
t 
-0.5*log( det(cur.T) ) +0.5*t(prop.beta-current.beta) 
%e% cur.T %*% (prop.beta-current.beta ) ) )  
> 
u<-runif (1) 
> 
u<-log(u) 
> 
if( u < loga ) { 
> 
current.beta<-prop.beta 
> 
acc.prob <- acc.prob+l 
> 
beta[t,]<-current.beta 
># ------- FUNCTION calculate.loglike --------------- 
># input: beta, x, y 
># output: loglike and precision 
> calculate.loglike<-function( b, x, y){ 
> 
n<-length(x) ; X<-cbind( rep(l,n), x ) 
> 
precision<-700 
> 
eta<-b[lltb[2l*x 
> 
logq <- log(l+exp(eta)) 
> 
logq[eta>precisionl<-etaCeta>precisionl 
> 
loglike<- sum( y*eta - logq ) 
> 
eta[eta>precisionl<-precision 
> 
h <- l/((ltexp(-eta))*(l+exp(eta))) 
> 
H <- t(x) %*% diag( h ) %*% x 
> 
return(list( loglike=loglike, H=T ) )  
cur .T <- (Uc. 
beta"Z)*(cur$H+diag(l/s .beta%) 
>
}
 
> }  
In our example we consider binary (Bernoulli) data; hence N ,  = 1 for all i = 1, . . . , n. 
The scheme shown above was used to simulate samples of 1500 and 11000 iterations. 
Parameter cg was set equal to 2.5 achieving an acceptance rate around 20%. The diagnostic 
plots (in Figure 2.12) demonstrate much better behavior than do the algorithm of the previous 
paragraph. In Figure 2.12i we observe that the autocorrelation lag is about 30 iterations, 

64 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
while in Figure 2.12ii (chain with 1 1,000 iterations and burnin period of 1000 iterations), the 
ergodic means have been stabilized. Moreover, the contour plots in Figure 2.13 demonstrate 
that the chain is highly mobile and even in the initial 100 iterations (Figure 2.13a) the 
algorithm has explored an important portion of the posterior distribution. 
(a) Trace plot of 30 
(b) Trace plot of3, 
(a) Trace plot of JO 
(b) Trace plot of3, 
(c) Ergodic mean plot of 30 
(c) Ergodic mean plot of 31 
0 
sw 
I M O  
l i W  
0 
Iw 
tow 
,500 
iteratmns 
IteraflOns 
Autocorrelations for 30 
Autocorrelations for 3> 
" I  
:I 
i ! l  
6-111, 
(c) Ergodic mean plot of 30 
:It. . . . . . . .  ,- 
I:: 
-
1
 
. J l  
0 
2m 
1ow m 8om lowD 
lIelatC'nS 
Autocorrelations for 30 
(c) Ergodic mean plot of 31 
-
,
 
?
,
'
,
 , 
, 
, 
0 em 1m Bwo BiM IWM 
Iterations 
Autocorrelations for 8, 
............... ...,.... 
4L!25z= 
........................ 
Lag 
,Thl"=iO ilrrailonti 
Lag 
(rn>".X ,llrmo"%) 
Lag 
Lag 
(i) 1000 iterations + 500 burnin 
(ii) 10,000 iterations + 1000 burnin 
Figure 2.12 
using a bivariate normal proposal distribution. 
MCMC diagnostic plots for logistic regression parameters p0 and PI of Example 2.3 
Single-component random-walk approach. In practice, MCMC schemes are adopted 
in which one parameter at each time is updated iteratively. This procedure is called the 
single-component Metropolis-Hastings algorithm (or Metropolis within Gibbs or compo- 
nentwise Metropolis-Hustings algorithm) and is simpler to design and easier in terms of 
the specification of the tuning parameters since in each step we deal with univariate distri- 
butions. This iterative procedure is summarized as follows: 
0 F o r t = l ,  . . . .  T 
(t-1) 
(t 1) T 
1. S e t P =  (Po 
, P I -  
. 
2. Propose a new value PL from N ( P ~ , s ~ ~ ) .  
3. Set p' = (Pb, p1 
( t - I ) ) T ,  
4. Calculate log cy = min(0, A) with A given by 

POPULAR MCMC ALGORITHMS 
65 
Figure 2.13 
MCMC moves and generated values for logistic regression parameters of Example 
2.3 using a bivariate normal proposal; the contour on upper left of each graph refers to the proposal 
distribution at the initial (0,O) step. 

66 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
5. Update p = p' with probability a or keep the same values for pwith probability 
6. Propose a new value pi from N ( & ,  Z",). 
7 .  Set p' = ( 3 0 .  
8. Calculate log a = min(0. A) with A given by 
1 - a. 
9. Update p = 0' 
with probability a or keep the same values for @with probability 
1 - a. 
10. Set P(*) = p 
Table 2.9 demonstrates in more detail the algorithm in parallel to the appropriate R code as 
implemented for the logistic regression example using the senility symptoms data. 
Table 2.9 
for logistic regression parameters of Example 2.3 
Algorithm and R code for a single-component random-walk Metropolis algorithm 
Algorithm (pseudocode) 
R code 
Set up data 
Set number of iterations T 
Set prior parameters: @30 = p31 = 0, 
Set proposal parameters 
Initialize vector of sampled values p 
Initialize acceptance probability counter 
Set initial current / 3 ( O )  
F o r t =  l....,Trepeat 
u30 = UO1 = 100 
S e t @ ' =  
(/30,&) 
Propose 06 from N( 
00, Z& ) 
Calculate the linear predictor 7 for current ar 
proposal parameters 
Calculate log a =log-likelihood( y; p' ) 
- log-likelihood( y; p ) 
+ log-prior( p' ) 
- log-prior( p ) 
Generate u from C ( 0 ,  1) 
Set u = log u 
If u < log a then 
set p = p' 
and update acceptance probabilil 
counter 
Set@ = (30. al) 
Propose 3; from N(41, 
) 
Calculate linear predictor 77 for current ar 
Calculate log a =log-likelihood( y; p' ) 
- log-likelihood( y; p ) 
+ log-prior( p' ) 
- log-prior( p ) 
proposal parameters 
Generate u from L-(O. 1) 
Set u = log u 
If u < log a then 
set p = p' 
and update acceptance probabilil 
counter 
Set p(t) = p 
End off or loop 
> wais<-read.table('wais.dat',header=TRUE) 
> y<-wais$senility; x<-wais$wais 
> Iterations<-3500 
> mu.beta<-c(O,O); s.beta<-c(lOO,lOO) 
> prop.s<-c(l.5,0.15) 
> beta <- matrix(nrow=Iterations, ncol=Z) 
> acc.prob <- c(0,O) 
> current .beta<-c(O,O) 
> f o r  (t in ~:Iterations){ 
> 
prop.beta<- current .beta 
> 
prop.beta[ll<- rnorm( 1, current.beta[ll, 
> 
cur.eta <-current.beta[ll+current.beta[Zl*x 
> 
prop.eta<-prop.beta[l] +prop.beta[Zl *x 
> 
loga <-(sum( y*prop.eta - log( l+exp(prop.eta) ) 1 
t 
-sum( y*cur.eta - log( l+exp(cur.eta ) ) ) 
t 
tsum(dnorm(prop.beta, mu.beta,s.beta,log=TRUE)) 
+ 
> 
u<-runif (1) 
> 
u<-log(u) 
> 
if( u < loga) { 
> 
current.beta<-prop.beta 
> 
acc.prob[ll <- acc.prob[lltl 
> 
prop.beta<- current.beta 
> 
prop.beta[Z]<- n o r m (  1, current.beta[Zl, 
prop.s[ZI ) 
> 
cur.eta <-current .beta[ll+current .betaCZI*x 
> 
prop.eta<-prop.beta[ll tprop.beta[Zl *x 
> 
loga <-(sum( y*prop.eta - log( l+exp(prop.eta) ) ) 
+ 
-sum( ytcur.eta - log( l+exp(cur.eta) ) ) 
t 
t 
-sum(dnorm(current .beta,mu.beta,s 
.beta,log=TRUE))) 
> 
u<-runif (1) 
> 
u<-log(u) 
> 
if( u < loga ) { 
> 
current.beta<-prop.beta 
> 
acc.prob"21 <- acc.prob~ZIt1 
> 
beta[t,l<-current.beta 
prop.s[ll ) 
-sum(dnorm(current .beta,mu. 
beta, s .beta, 
log=TRUE))) 
>
}
 
+sum(dnorm(prop. beta, mu. beta, s .beta, 
log=TRUE) ) 
>
I
 
> )  

POPULAR MCMC ALGORITHMS 
67 
This algorithm will move more frequently than the corresponding one, which updates 
all parameters in a single step since it also allows us to change only specific parameters in 
each step. This is beneficial when high correlations between parameters exist, and we wish 
to avoid using multivariate proposals. 
Initially, a sample of 1500 iterations was generated excluding the first 500 observations 
as a burnin period using Sy, = 1.75 and Spl = 0.2, which result in acceptance rates 
approximately equal to 20%. 
As we can see from Figure 2.14i, the behavior of the chain was improved in comparison 
to the initial one-block random-walk algorithm. Nevertheless, it is still slightly worse than 
the second approach, which used a multivariate normal proposal distribution with variance- 
covariance matrix equivalent to the posterior one. According to Figure 2.14ii, the algorithm 
has reached convergence. A thinning interval equal to 90 iterations is required to obtain an 
independent sample. 
(a) Trace plot of 30 
(b) Trace plot of 31 
(a) Trace plot of 30 
(b) Trace plot of 9, 
(c) Ergodic mean plot of 30 
(c) Ergodic mean plot of 31 
(c) Ergodie mean plot of 30 
(e) Ergodic mean plot of Y1 
.- 
5 :],- 
_ _ I  
lteraflonr 
1teratmns 
1teratmns 
lIe*atlO"i 
Autocorrelations for 30 
Autocorrelations for 3, 
Autocorrelations for 30 
Autocorrelations for 3, 
y. y r  
.................... 
....................... 
1
1
,
 
, I  
I
,
,
,
,
 
2 
I
,
 
; 
.......................... 
......................... 
0 
20 
40 
eo 
80 
0 
20 
40 
60 
80 
0 
5 
10 
l i  
*o 
0 
5 
lo 
IS 
?O 
Lag 
(Thln-lo IIFllllO"iJ 
Lag 
(Thin-90 ~terdfionil 
La% 
:ag 
(i) 1000 iterations + 500 burnin 
(ii) 10,000 iterations + 1000 burnin 
Figure 2.14 
using a single-component random-walk algorithm. 
MCMC diagnostic plots for logistic regression parameters /% and 
of Example 2.3 
Moves of the algorithm in the bivariate space are depicted in Figure 2.15. In the first 
plot (iterations 1-30), both vertical and horizontal moves are visible in addition to the usual 
diagonal moves appearing in all previously illustrated algorithms. Such moves correspond 
to updating only one of the two parameters in the corresponding iteration. This depiction 
is typical of how single-component Metropolis-Hastings algorithms explore the parameter 
space. 

68 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
(a) Iterations 0-30 
(b) Iterations 0-100 
(c) Iterations 0-1000 
(d) All sampled values 
Figure 2.15 
MCMC moves and generated values for logistic regression parameters of Example 2.3 
using a single-component random-walk algorithm; the contour on upper left of each graph refers to 
the proposal distribution at the initial (0,O) step. 

POPULAR MCMC ALGORITHMS 
69 
Comparison of the three algorithms. Table 2.10 provides posterior summaries from 
the samples generated using the three algorithms described above. Additional details for 
the MCMC algorithms are given in Table 2.1 1. All approaches have reached convergence, 
and their posterior means and standard deviations are similar. Some discrepancies exist 
in the posterior quantiles, indicating that additional iterations might be required in order 
to estimate them more precisely. Monte Carlo errors, for the posterior means are also 
provided. For the second MCMC scheme, MC errors are considerably lower than for the 
other two algorithms. The first and the third MCMC schemes have similar MC errors but 
the length of the first chain is about 5 times as high as the length of the latter (47,000 vs. 
10,000 iterations kept). 
Table 2.10 
Posterior summaries of MCMC outputs for Example 2.3 
Parameter Algorithm" 
Percentiles 
Mean MCerror SDb 2.5% 97.5% 
90 
RW with independent normal 
2.658 
0.144 
1.322 0.192 5.319 
proposals 
MH with bivariate normal pro- 
2.624 
0.036 
1.217 0.437 5.226 
posal 
Single-component RW 
2.563 
0.120 
1.208 0.205 5.171 
P1 
RW with independent normal 
-0.353 
0.013 
0.127 -0.617 -0.121 
proposals 
MH with bivariate normal pro- 
-0.350 
0.004 
0.117 -0.607 -0.146 
posals 
Single Component RW 
-0.343 
0.012 
0.116 -0.601 -0.127 
RW=random-walk Metropolis algorithm; MH=Metropolis-Hastings algorithm 
bStandard deviation. 
Table 2.11 
MCMC details for algorithms used in Example 2.3 
Algorithm" 
Burnin Iterations 
Proposal Acceptance 
(B) kept (7") 
Lag parameters 
rate 
RW with independent normal proposals 8000 
47,000 
650 
= 0.20 
0.208 
MH with bivariate normal proposal 
1000 
10,000 
20 c s  = 2.50 
0.196 
Single-component RW 
1000 
10,000 
90 Soo = 1.75 
0.245b 
- 
S& = 0.20 
0.212b 
'RW=random-walk Metropolis algorithm; MH=Metropolis-Hastings algorithm. 
bOverall acceptance rate=0.398 (probability of updating at least one parameter). 
Finally, differences, in terms of convergence, between the three algorithms are depicted 
in Figure 2.16 via the evolution of the ergodic means of PO and PI. 
From these plots, it 
is clear that the means of the simple independent normal random-walk algorithm (solid 

70 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
line) converge to the correct value slower than the corresponding means for the other two 
methods. 
--
0 
10,000 
20,000 
10,wo 
4woo 
1000" 
0 
,o.ow 
20.000 
30,ow 
40,000 
50000 
Iterations 
Iterations 
(a) Ergodic means for DO (all iterations) 
(b) Ergodic means for PO (after discarding burnin) 
0 
I0,WO 
mow 
30,wo 
'0,000 
sow0 
0 
,ow0 
20,000 
3 o . m  
%OOO 
I0000 
Iterations 
Iterations 
(c) Ergodic means for ,!31 (all iterations) 
(d) Ergodic means for 
(after discarding burnin) 
Figure 2.16 
2.3. 
Ergodic mean plots for each algorithm for logistic regression parameters of Example 
Estimating WAlS with T > 0.5. In order to identify which scores of WAIS result in 
increased probability of senility symptoms, we can follow either of two approaches. The 
first approach is to estimate all 7r values under all (or some) possible values of WAIS and 
trace which posterior distributions are clearly over the threshold value of 0.5. 
Using this first approach for each set of sampled values ,L3(t), 
we can calculate 
and hence obtain a sample for each probability and estimate the posterior 95% or 99% cred- 
ible interval for each possible score. These details as well as the probability P(n(z) > 0.5) 
are provided in Table 2.12. If inference is based on the 95% credible intervals, then values 
of WAIS lower than 10 (WAIS< 10) indicate which subjects demonstrate high probability 
of having senility symptoms. 

POPULAR MCMC ALGORITHMS 
71 
Table 2.12 
values in Example 2.3 
Posterior summaries for probability of senility symptoms for all possible WAIS 
WAIS 
Percentiles 
score (x) Mean 
2.5% 
97.5% 
P ( T ( ~ )  
> 0.5) 
0 
0.894 
0.608 
0.995 
1 
0.869 
0.566 
0.990 
2 
0.837 
0.529 
0.983 
3 
0.797 
0.491 
0.969 
4 
0.748 
0.452 
0.945 
5 
0.688 
0.411 
0.907 
6 
0.619 
0.371 
0.848 
7 
0.541 
0.328 
0.763 
8 
0.459 
0.270 
0.659 
9 
0.376 
0.222 
0.548 
10 
0.300 
0.173 
0.446 
11 
0.234 
0.123 
0.369 
12 
0.180 
0.077 
0.304 
13 
0.138 
0.047 
0.254 
14 
0.105 
0.029 
0.218 
15 
0.080 
0.016 
0.187 
16 
0.061 
0.009 
0.158 
17 
0.047 
0.005 
0.136 
18 
0.037 
0.003 
0.117 
19 
0.028 
0.002 
0.100 
20 
0.022 
0.001 
0.088 
0.993 
0.991 
0.984 
0.972 
0.950 
0.912 
0.807 
0.630 
0.342 
0.075 
0.005 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
0.000 
Alternatively, the logistic regression equation may be used to obtain an expression for 
the threshold probability value of rr = 0.5, which results in zero logit. Thus, denoting by 
WAIS” the value that corresponds to probability of having senility symptoms equal to 0.5, 
we obtain the expression PO + /31 WAIS* = 0 (j WAIS* = -/30/p,. We obtain a posterior 
sample for WAIS* by simply setting WAIS*(t) = -/3t’/pit’, for t = 1,2,. . . , T .  This 
approach is easier to implement and can be used even when variable z is continuous. 
Implementing this transformation to the output ofthe second MCMC scheme, we obtain 
a posterior sample of WAIS” with posterior mean equal to 7.1 and 95th and 99th percentiles 
equal to 9.2 and 9.8, respectively. Hence, using these quantiles, we conclude that the 
value of WAIS equal to 10 (WAIS* = 10) safely discriminates high and low-risk subjects 
(using WAIS< 10 and WAIS? 10, respectively). This threshold value is the same as the 
corresponding one attained by the first approach for the calculation of WAIS *. 
2.3.3 The Gibbs sampler 
The Gibbs sampler was introduced by Geman and Geman (1984). It is a special case of 
single-component Metropolis-Hastings algorithm using as proposal density q (8’(dt)) 
the 
fullconditionalposteriordistributionf(B,18\j,y),where8\, = (01,. . . , 0 3 - ~ , 0 , + ~ ,  
. . . , , B d ) T .  

72 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
Such proposal distributions result in acceptance probability o = 1, and therefore the pro- 
posed move is accepted in all iterations. Although Gibbs sampling is a special case of 
Metropolis-Hasting algorithm, it is usually cited as a separate simulation technique be- 
cause of its popularity and convenience. One advantage of the Gibbs sampler is that, in 
each step, random values must be generated from unidimensional distributions for which 
a wide variety of computational tools exists (Gilks, 1996). Frequently, these conditional 
distributions have a known form and, thus, random numbers can be easily simulated using 
standard functions in statistical and computing software. Gibbs sampling is always moving 
to new values and, most importantly, does not require specification of proposal distribu- 
tions. On the other hand, it can be ineffective when the parameter space is complicated or 
the parameters are highly correlated. 
The algorithm can be summarized by the following steps: 
1. Set initial values do). 
2. Fort = 1. . . . .  T repeat the following steps 
a. Set 8 = d--l) 
b. Forj = 1  . . . . .  d,updateO,fromO, -f(Q, 16JlJ,y). 
c. Set dt) = 8 and save it as the generated set of values at t + 1 iteration of the 
algorithm. 
Hence, given a particular state of the chain 8 ( t ) ,  we generate the new parameter values by 
OF) from f ( e ,   OF-'), OF-''. . . . .  ef-? 
y), 
OF) from f(e21e?), OF-'). . . .  , e, ( t - 1 )  y), 
e!) 
from f(e3jej"), eF),eF-'), . . .  .o, 
,y), 
(t-1) 
.
.
.
.
.
.
.
.
.
.
.
 
.
.
.
.
.
.
.
.
.
.
.
 
.
.
.
.
.
.
.
.
.
.
.
 
.
.
.
.
.
.
.
.
.
.
.
 
.
.
.
.
.
.
.
.
.
.
.
 
.
.
.
.
.
.
.
.
.
.
.
 
0:) 
from 
f ( ~ , ~ ~ j t ) .  
OF). . . . .  gp-'. 
( t )  y). 
( t )  
(t-1) 
Generating values from f(O,l8\,, y) = f(0,Iej"'. . . .  .6'-,, 8,+1 . . . .  ,Of-'). y) is rela- 
tively easy since it is a univariate distribution and can be written as f(Q, l8,, . y) cc f(8iy), 
where all the variables except BJ are held constant at their given values. More detailed 
description of the Gibbs sampler is given by Casella and George (1992) and Smith and 
Roberts (1993), while early applications of the Gibbs sampling are provided by Gelfand 
and Smith (1990) and Gelfand et al. (1990). 
2.3.3.7 A simple example using the Gibbs sampler 
Example 2.4. Body temperature data revisited: Analysis with nonconjugate 
prior. Let us consider the data of example 1.5 but now with the following prior 
distribution 
1 N N (  po, 00' ) and n2 N IG(a0, bo) 
instead of the conjugate prior distribution presented in Section 1.5.7. 

POPULAR MCMC ALGORITHMS 
73 
In order to construct a Gibbs sampler for this model, we need to calculate the con- 
ditional distributions f(pla2: y) and f(021p, y) and sample sequentially from these two 
distributions. After some calculations, we obtain 
and 
Using these results, the Gibbs sampler is summarized as follows: 
0 F o r t = l ,  . . . .  T 
1. Set p = P(~-'), o = cd-l) and 8 = (p. ~
7
~
)
~
.
 
2. Calculate w = g,"/(02/n + cr,"), rn = wy + (1 - w)po, 
and s2 = wo2/n 
3. Generate p from N(m. s 2 )  
4. Set p(t) = p 
5. Calculate a = a0 + n/2 and b = bo + 
6. Generate r from G(a, b) 
7 .  Set crz = 1/r and ~
(
~
1
 
= o. 
Cr=l(yz 
- P ) ~ .  
Table 2.13 demonstrates in more detail the algorithm in parallel to the appropriate R code 
for the simple regression model of the body temperature data. 
Table 2.13 Algorithm and R code for Gibbs sampler of Example 2.4 
Algorithm (pseudocode) 
R code 
Set up data 
Set number of iterations T 
Set prior parameters: po = 0, 00 = 100, 
Initialize vectors of sampled values p, u 
Set initial current p(O' and a(') 
Fort = 1. . . , . T repeat 
no = bo = 0.001 
Calculate w = .,'/(.'/n 
+ 0 2 )  
Calculate rn = wg + (1 - w ) p o  
calculate s = J-2/n 
Generate p N N(m, s2) 
Calculate n = a0 + n / 2  
Calculate b = bo + 4 c:=,(yz 
- p)' 
Generate 7 - G(a. b) 
Set u = J1/T 
Set p ( t )  = p and u(t) = u 
End off or loop 
> y<-normtemp$temp; bary<-mean(y) ; n<-length(y) 
> Iterations<-3500 
> muO<-0; sO<-lOO; a0<-0.001; 
b0<-0.001 
> theta <- matrix(nrow=Iterations, ncol=2) 
> cur.mu<-0; cur.tau<-1; cur.s<-sqrt(l/cur.tau) 
> for (t in l:Iterations){ 
> 
w<- s0"2/( 
cur.s"2/n+ sO"2 ) 
> 
m <- wtbary + (l-w)*muO 
> 
s <- sqrt( w * cur.sh2/n ) 
> 
cur.mu <- m o m (  1, m, s ) 
> 
a <- a0 + 0.5*n 
> 
> 
cur.tau <- rgamma( 1, a, b ) 
> 
cur.5 <- sqrt(l/cur.tau) 
> 
thetait,]<-c( cur.mu, cur.s ) 
b <- bO + 0.5 * sum( (y-cur.mu)"2 
> )  
In Figure 2.17, trace, ergodic means, and autocorrelation plots for both p and o are 
provided. All convergence plots indicate that the algorithm has reached convergence since 
trace plots do not present irregularities and ergodic means have been stabilized. Moreover, 
only the first autocorrelation of the sample is high, indicating a well performed sampler. 

74 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
From the initial 10 moves of the Gibbs sampler (depicted in Figure 2.18 along with the 
intermediate steps between the updates of p and a), we observe that the chain is highly mo- 
bile, moving directly to the center of the target posterior distribution. The finally generated 
sample is provided in the scatterplot of Figure 2.19. 
(c) Ergodic mean plot of p 
(c) Ergodic mean plot of u 
c 
0 
500 
1000 
1500 
0 
500 
1000 
1500 
Iterallons 
lleratlOnE 
Autocorrelations for 1' 
Autocorrelations for o 
Figure 2.17 MCMC diagnostic plots for normal model using Gibbs sampler for Example 2.4. 

POPULAR MCMC ALGORITHMS 
75 
88 10 
88 15 
88.20 
88.25 
98.30 
88.35 
8840 
P 
Figure 2.18 
steps. 
Initial 10 moves of Gibbs sampler for Example 2.4; dashed lines indicate intermediate 
Figure 2.19 
Scatterplot of generated sample using Gibbs sampler in Example 2.4. 

76 
MARKOV CHAIN MONTE CARL0 ALGORITHMS IN BAYESIAN INFERENCE 
2.3.4 Metropolis within Gibbs 
Nowadays, the wide range of available computational tools for generating random values 
from univariate distributions allow us to implement the Gibbs sampler in a variety of cases, 
even when the resulting conditional posterior distribution is cumbersome. Nevertheless, on 
some occasions, it is convenient to use simple Metropolis-Hastings steps to generate from 
these univariate conditional posterior distributions. This approach is called the Metropolis 
within Gibbs algorithm and it is simply a componentwise Metropolis-Hastings algorithm 
in which some components of the parameter vector are directly generated from the cor- 
responding full conditional posterior distributions. This combination of Metropolis and 
Gibbs steps is frequently used in practice. In this way, the user can easily incorporate 
blocking, take advantage of specific easy-to-generate full conditionals, and generally build 
an efficient and flexible MCMC algorithm. 
2.3.5 The slice Gibbs sampler 
The slice sampler is essentially based on Gibbs sampling. It is used mainly when the fill 
conditional posterior distributions do not have a convenient form. This method augments 
the parameter space by adding a set of convenient random variables (called auxiliary vari- 
ables) that retain the marginal posterior distribution of interest unchanged but convert all 
conditionals to distributions of standard form. In this way a simple Gibbs sampler is di- 
rectly applicable. The method is also referred as the auxiliary variables method and was 
introduced in statistical physics in the late 1990s. It was further developed in the context 
of statistical science by Higdon (1998), Damien et al. (1999), and Neal (2003). 
The idea can be summarized as follows. Let us consider a target distribution g(x) that 
is difficult to generate from. We introduce a new variable u with conditional f(u1.). Then 
the joint distribution distribution can be written as 
f(u, 
2 )  = f(ul.)s(z). 
while the marginal distribution f(z) is equal to the original target distribution g(z) since 
In this way we can set up the following Gibbs sampler, which generates values from the 
joint distribution f(u. 
x) and the corresponding marginals f(u) 
and f(z) = g(x): 
1. Generate u - f(u1.) 
2. Generate z N f(uiz)g(z) 
Since f(u1.) 
participates in both Gibbs steps above, it must be specified in such way that 
both f(ulx) and f(uIx)g(x) are convenient in terms of simulation. A usual choice for 
f(ulz) 
is the uniform distribution U 0, g(x) since 
0 
and 

POPULAR MCMC ALGORITHMS 
77 
where I ( z )  is the indicator function taking value equal to one if z is true and zero otherwise. 
Then the Gibbs sampling is summarized by the following steps: 
1. Generateu(t) N U ( 0 , g  (dt-'))) 
2. Generate z ( ~ )  
N U ( z  : 0 5 ~
(
~
1
 
5 g(z)). 
When we focus on the Bayesian context, it is usual practice to facilitate u = (u 
1, . . . . u,) 
auxiliary variables coming from uniform distribution defined within the interval from zero 
to the likelihood ordinate f(yz16). Thus the joint distribution will be given by 
resulting in a Gibbs sampler of type 
1. Set 6 = 6(t-1). 
2. For z = 1,. . . . n, generate u ! ~ )  - U 0. f(yJ6) . 
3. Forj = 1,. . . , d, update Q3 N f(Q,) nr=l I ( 0  I uit) 5 f(yl16)). 
4. Set dt) = 6. 
0 
Clearly, in 2, we generate values for the parameters from the corresponding prior distribution 
truncated to satisfy the condition f ( y Z  16) 2 u implied by the auxiliary variables. According 
to Damien et al. (1999), the sampling scheme described above can be easily implemented 
in a wide variety of popular statistical models, including generalized linear models. 
Although, by using the slice sampler, we avoid the specification of the proposal densities 
embedded in Metropolis-Hastings algorithms, we still need to find a convenient augmen- 
tation scheme. The main advantage of the algorithm is that, after finding the appropriate 
augmentation scheme, the method is directly applicable in all sets of data without compu- 
tational difficulties. On the other hand, the resulting chain is usually highly autocorrelated. 
This is due to the fact that the sampling space is greatly extended which considerably delays 
convergence of the MCMC algorithm (Neal, 2003). Details of the slice sampler can also be 
found in Carlin and Louis (2000, pp. 167-170), Givens and Hoeting (2005, pp. 219-223), 
and Robert and Casella (2004, chap. 8). 
2.3.6 A simple example using the slice sampler 
Example 2.5. Slice sampler for binary logistic regression: Senility symptoms 
data revisited. Let us now consider the senility symptoms data presented in Ex- 
ample 2.3. We will now generate the posterior distribution using a simple slice 
sampler [for more details, see Damien et al. (1999)l. 
In order to build the slice sampler, we use u, 
such that 

78 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
This posterior has the “correct” marginal f(30.31 
iy) since 
Therefore we sample ui from 
30 from 
and 31 from 
The conditional posteriors described above are truncated normal distributions. The range 
oftruncation is defined by the inequalities ui I exp(Poy, + fllziyi)/[l+ exp(P0 + L?lzz)] 
which can be written as 
&n 
+?I 
zz 
u, 
=+ 
u, I 
1 + eOo+31z, 
=+ Po + 415, 2 log - 
For y, = 1 
1 - u, 
1 
1 - u, 
=+ Po + 012, I log - 
=+ 
u, 
1 + ePo+olz* 
u, 
For y, = 0 
resulting in 
By solving these inequalities with respect to PO and PI, we obtain the truncation intervals 
for these parameters 
PIXi) 
Ui 
1 - ui 
lo = max (log - 
- ~ 1 x i )  5 PO I 
uo = ,min (log - 
- 
i:y,=l 
1 - ui 
,:y,=O 
ui 
and 

POPULAR MCMC ALGORITHMS 
79 
since 2, > 0 in our example. Parameters .30 and& arefinallygeneratedfromN(p30, og0)I 
(lo, t/,0) 
and N ( ~ R ~ ,  
o&)I (11.211) respectively. 
Details of the implementation of the above mentioned slice sampler and the corresponding 
R code are given in Table 2.14 
Table 2.14 Algorithm and R code the slice sampler used in Example 2.5 
Algorithm (pseudocode) 
R code 
Set up data 
Set number of iterations T 
Set prior parameters: poo = pol = 0, 
Initialize vector of sampled values 0 
Initialize acceptance probability counter 
Set initial current @"), u 
Fort = 1.. . . , T repeat 
upo = u31 = 100 
Calculate 7 = 90 + 310 
Calculate U = [V, = eyt'It /(1 + e"c)] 
Generate u = [ul - L'(0. U,)] 
Calculatelu = {Zut = log[u/(l - u ) ] }  
Calculate lo = max ( / u 2  - $ 1 ~ ~ )  
for all i 
Calculate u g  = max(-lu, - 91s~) 
for 
Generate,5o - X ( p ~ 3 ~ .  
o & ) l ( l ~ .  
U O )  
with yt = 1 
all i with y, = 0 
Calculate11 = max[(lu, - 30)/z,] 
forall 
Calculateul = max[(-lu, - Bo)/z,] for 
Generate31 - A T ( p ~ l , u ~ l ) l ( l ~ , ~ ~ )  
zvvlthy, = 1 
all z with yt = 0 
= p 
End off o r  loop 
> wais<-read.table('wais.dat',header=TRUE) 
> y<-wais$senility; xc-wais$wais ; n<-length(y) 
> positive<- y==i 
> Iterations<-2500 
> mu.beta<-c(O,O); s.beta<-c(iOO,lOO) 
> beta <- matrix(nrow=Iterations, nCOl=2) 
> acc.prob <- 0 
> current .beta<-c(O,O); u<-nmeric(n) 
> for (t in l:Iterations){ 
> 
eta<-current .beta[ll+current .betar21 
*X 
> 
U<-exp(y*eta)/(l+exp(eta)) 
> 
u<-runif( n, rep(O,n), U) 
> 
logitu<-iog( u/(l-u) ) 
> 
logitul<- logitu[positivel 
> 
> 
iogitu2<- -1ogitu [ !  positive] 
> 
> 
unif .random<-runif(l,O,l) 
> 
fa<- pnorm(l0, mu.beta[ll, s.beta[ll) 
> 
fb<- pnorm(u0, mu.betaC11, s.beta[ll) 
> 
unif .random*(fb-fa), mu.betaC11, s.beta[il) 
> 
> 
> 
unif .random<-runif(i,O,i) 
> 
fa<- pnorm(li, mu.betaL21, s.betaC21) 
> 
fb<- pnorm(u1, mu.beta"21, s.beta"21) 
> 
current.betaL21 <- qnormc fa + 
unif ,randomt(fb-fa), mu.beta"21, s.beta[Zl) 
> 
beta[t,l<-current.beta 
lo<- max( logitul - current.beta[2l*xCpositivel ) 
uO<- mi=( logitu2 - current.betaC21*x[!positivel 1 
current.beta[ll <- qnormc fa + 
11<- max( (logitul-current .beta[ll )/x[positivel ) 
ulc- min( (logitu2-current .beta[il )/x[!positivel ) 
> }  

80 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
Using the slice sampler described above, we have generated a total of 55,000 iterations 
and discarded the initial 15,000 iterations as a burnin period. From the diagnostic plots 
(Figure 2.20), we conclude that the ergodic means have been stabilized. Estimated Monte 
Carlo errors for the posterior means of PO and 
are equal to 0.1 10 and 0.0105, respec- 
tively. As expected, high autocorrelations are present, indicating slow convergence. This is 
typical when using slice samplers since the parameter space is expanded. Using the above 
mentioned slice sampler in this example, we generate n + 2 parameters, since one ‘u is 
added for each observation i, instead of two parameters (PO and PI) 
used in the original 
(a) Trace plot of PO 
(c) Ergodic mean plot of 30 
0 
10,000 
30,000 
50.000 
Iterations 
Autocorrelations for 30 
:I1 
(b) Trace plot of 81 
(c) Ergodic mean plot of 13, 
0 
10,000 
30.000 
50,000 
Iterations 
Autocorrelations for 01 
11, 
0 
200 
400 
600 
800 
Lag 
0 
200 
400 
600 
600 
Lag 
Figure 2.20 
data (Example 2.5). 
Diagnostic plots for slice sampler of logistic regression parameters of senility symptoms 

SUMMARY AND CLOSING REMARKS 
81 
logistic regression model. The posterior means of $0 and 
were found equal to 2.62 and 
-0.35, respectively, with standard deviations 1.25 and 0.119 and 95% credible intervals 
(0.42. 5.28) and (-0.61. - 0.15), respectively. All these estimatedvalues are close to the 
ones estimated using the Metropolis-Hastings schemes (see Table 2. lo). 
2.4 SUMMARY AND CLOSING REMARKS 
In this chapter, Markov chain Monte Carlo algorithms are presented. Random-walk Metropo- 
lis, independent Metropolis-Hastings algorithms, the Gibbs sampler, and the slice sampler 
are described in detail accompanied with elaborate illustrations. The aim of this chapter 
is to introduce users to the logic of MCMC methods and enable them to understand how 
WinBUGS works and motivate them to construct their own MCMC algorithms (and codes). 
The basic MCMC algorithms described above were developed, improved, and massively 
implemented in Bayesian inference during 1990-2000. During this decade, such methods 
were considered as standard and more complicated algorithms, implemented in specific 
problems, and are under further development and assessment. 
Two interesting areas of active research regarding MCMC algorithms are (1) the devel- 
opment of varying dimension algorithms for model and variable selection methods (Green, 
1995; Sisson, 2005) and (2) the construction of efficient and “automatic” algorithms (in the 
sense that tuning parameters can be specified using mathematical arguments). 
In the following chapters we focus on the presentation and the technical details of Win- 
BUGS using the methods described here. 
Problems 
2.1 
Consider the contingency table data of Problem 1.6 and the model of independence 
between X and Y i.e. 7riTy = 7r:7r,’. 
Assuming a multinomial distribution for the 
cell frequencies yZJ with probabilities 7rZ3, and using beta priors for 7rp and 7rT : 
a) Estimate the posterior distributions for 7r: and 7rT. 
b) Estimate the posterior distributions for 7rt3 using direct sampling. 
c) Compare the posterior distributions of 7 r Z j  under the models, assuming and not 
For the World Cup 2006 soccer data of Italy and France provided in Problem 1.8, 
using the help of direct sampling, calculate the posterior probabilities 
a) Of winning a game 
b) Of playing overtime (i.e., in case of a draw) 
c) Owinning a game during the overtime. 
Consider the data of Problem 1.10 and the normal distribution with Y, 
N N ( p ,  0’)). 
Use a prior of the types p N N(O,oE) and o2 N IG(a, b). Construct the following 
MCMC schemes, and generate 5000 observations from the posterior distribution. 
a) Use a simple Gibbs scheme (all conditional posterior distributions are of known 
b) Use an independence Metropolis scheme to jointly sample p and log o ’. 
c) Use a bivariate random-walk scheme to jointly sample p and log o ’. 
assuming independence between X and Y. 
2.2 
2.3 
form). 

82 
MARKOV CHAIN MONTE CARLO ALGORITHMS IN BAYESIAN INFERENCE 
2.4 
For the data of Problem 1.10, assume the Student’s t distribution for Y,. Construct 
the following MCMC schemes to generate 5000 observations from the posterior 
distribution. 
a) Use an independence Metropolis scheme to jointly sample p and log CT ’. 
b) Use a bivariate random-walk scheme to jointly sample p and log 0’. 
c) Use the adaptive rejection sampling of Gilks and Wild (1 992) to implement Gibbs 
sampling (software for adaptive rejection sampling is available on the Web). 
Use the Weibull distribution to model the following survival times (in days): 
a) Construct a Metropolis within Gibbs algorithm to estimate the posterior distribu- 
b) Estimate the mean and median survival time under this model. 
c) Do you think that the exponential distribution is more appropriate for the data 
Use the gamma distribution to model the survival times (in days) of Problem 2.5. 
a) Construct a Metropolis within Gibbs algorithm to estimate the posterior distribu- 
tions of the parameters of the Weibull distribution. 
b) Estimate the mean survival time under this model. 
c) Compare the results with the corresponding ones from the Weibull and the expo- 
Consider the pumps data in Spiegelhalter et al. (2003~). Construct MCMC algorithms 
to estimate the posterior distributions for the Poisson models yz -, Poisson(A,) with 
a) Model 1: log A, = CI + log(&) 
b) Model 2: log A, = Q. + ,3log(t,) 
c) Model 3: A, = O,t, and 8, N gamma(a, b). 
For the output obtained from any MCMC scheme of Problems 2.3-2.7 
a) Monitor the convergence graphically (see Section 2.2.2.4). 
b) Estimate Monte Carlo errors for all parameters (see Section 2.2.2.3). 
c) Download CODA and/or BOA packages for R and implement the available con- 
2.5 
166 7 11 4 9 4  5 17 3 .  
tions of the parameters of the Weibull distribution. 
above mentioned. 
2.6 
nential distributions. 
2.7 
2.8 
vergence diagnostics. 

CHAPTER 3 
WinBUGS SOFTWARE: INTRODUCTION, 
SETUP, AND BASIC ANALYSIS 
3.1 INTRODUCTION AND HISTORICAL BACKGROUND 
WinBUGS is a programming language based software that is used to generate a random 
sample from the posterior distribution of the parameters of a Bayesian model. The user 
only has to specify the data, the structure of the model under consideration, and some initial 
values for the model parameters. BUGS, the ancestor of WinBUGS , became popular during 
the 199Os, and its last version (v0.6 for DOS and UNIX operating systems) is still available 
via the BUGS project Website.’ The acronym BUGS stands for the initials of the phrase 
“Bayesian inference Using Gibbs Sampling”. 
The BUGS project was initiated in 1989 by the MRC Biostatistics Unit. The last versions 
(v0.5 and v0.6) of the “classic” BUGS were available via the project’s Website in 1996 and 
1997, respectively. The first experimental version of BUGS for windows, WinBUGS , 
was presented in 1997, while the current version (vl.4.3) was developed jointly with the 
Imperial College School of Medicine at St. Mary’s, London. The project currently includes 
the development of OpenBUGS at the University of Helsinki in Finland, which is an open 
source experimental version of WinBUGS . In the project’s site2 a wide variety of add- 
in software, utilities, related papers, and course material is available. Some examples of 
related add-in software and WinBUGS expansions include PKBUGS for pharmacokinetic 
modeling, GeoBUGS for spatial modeling, and the WinBUGS jump interface for model 
‘http: //www.mrc-bsu. cam. ac .uk/bugs/classic/contents . shtml. 
’http: //www .mrc-bsu. cam. ac. uk/bugs/ 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
83 

84 
WINBUGS SOFTWARE: INTRODUCTION, 
SETUP, AND BASIC ANALYSIS 
and variable selection. Also independent contributors have provided plug-in software and 
useful utilities to further assist WinBUGS users. WinBUGS can now be run even from other 
software packages, such as R ,  Matlab, and Excel. 
The original aim of the WinBUGS project was to develop software for producing MCMC 
samples from the posterior distribution of the parameters of a desired model. Such model 
can be specified in WinBUGS using a relatively simple code (for users who are familiar with 
other languages) that is similar to the popular S language used by Splus and R statistical 
programs. Users not familiar with programming can specify the model structure by drawing 
its directed graphical structure in the DOODLE interface of WinBUGS . This tool automat- 
ically generates the corresponding model code. 
The wide range of models that can be implemented by WinBUGS is one of the main 
factors that has considerably increased its popularity. Moreover, it is free of charge with 
high quality support by the research team of the project. Documentation includes a detailed 
accompanying manual and the three volumes of examples that cover a wide range of ap- 
plications. WinBUGS users are only asked to register in the project’s Website in order to 
receive by email a “key” file that (currently) must be renewed every 6 months. 
OpenBugs is an open source version of WinBUGS . WinBUGS 1.4.3 is recommended 
by the BUGS team for standard use since OpenBUGS is still in experimental phase. Never- 
theless, the corresponding compiled versions of OpenBUGS for windows and LINUX are 
named WinBUGS versions 2.2 and LinBUGS, respectively. In this book we focus on the 
WinBUGS version 1.4.3. 
Finally, to close this short introduction, all WinBUGS users may have noticed the “health” 
warnings that always appear in the first page of BUGS and WinBUGS manuals: 
Beware - 
Gibbs sampling can be dangerous! 
This phrase resembles and reminds us of the health warnings that appear in all cigarette 
packets. Experienced MCMC users will consent with this comment for two reasons: (1) 
running a Gibbs sampler (and generally an MCMC algorithm) is usually a laborious proce- 
dure that requires careful computation of the conditional posterior densities and coding of 
the corresponding algorithm using a programming language, and (2) WinBUGS users must 
be familiar with the basic notions of Bayesian statistical theory and computation (focusing 
on MCMC); otherwise they may not specify the model correctly, or may interpret the pos- 
terior results incorrectly, or might not obtain results from a converged chain or even stick 
with the first computation problem of the algorithm (e.g., with a simple overflow problem). 
3.2 THE WinBUGS ENVIRONMENT 
3.2.1 Downloading and installing WinBUGS 
During the writing of this book, the latest version of WinBUGS (version 1.4.3) was freely 
available via the BUGS project Web~ite.~ 
In order to install it, you need to download the 
WinBUGS installation file and the additional cumulative patches (if available) 
After completing installation, the following license agreement appears when you open 
WinBUGS : 
3 h t t p :  //www .mrc-bsu. cam. ac .uk/bugs/. 

THE WINBUGS ENVIRONMENT 
85 
If the procedure is successfully completed, after restarting WinBUGS and retrieving the 
online help of the program (using the F1 key), the screen similar to the following must 
appear, with the new updated version appearing as highlighted below: 
3.2.2 A short description of the menus 
After completing installation, the following menu bar, which is identical in all Microsoft 
windows programs, appears in WinBUGS : 
They can categorized as follows according to their functionality: basic operations (File, 
Window and Help), file editing operations (Tools, Edit, and Text), MCMC functions 
(Inf 0, Model, Inference, and Options), and specialized operations (Doodle, and Map). 

86 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
File, Window, and Help menus may be included in the first category of WinBUGS 
procedures since all of them refer to functions common to every software. The File menu 
provides the basic file handling capabilities. Using this menu, the user can open, save, print, 
and perform other related file management operations. The Window menu controls opened 
windows within WinBUGS . The main operations are the Tile Horizontal/Vertical 
and Arrange Icons commands usually available in all Windows programs. Using the 
Help menu, the user can read details concerning the license and the package version as well 
as browse the manual and the two volumes of examples that are available in the current 
version of WinBUGS . 
File editing operations are performed using menus Tools, Edit, Attributes, and 
Text. All files edited in WinBUGS can be saved as compound document files that have an 
odc suffix. Text as well as graphics and plots produced by the WinBUGS output analysis 
can be saved in such files. The Tools menu provides commands for managing and editing 
compound document files, including encodeidecode commands used to run software up- 
grades and license keys. The Edit menu is equivalent to the corresponding menu included 
in most Windows packages. It includes basic file editing commands such as cut, copy 
and paste. Fonts size, type, and color can be controlled via the Attributes menu. The 
Text menu provides useful tools for the manipulation of text in a compound document. It 
includes findhearch basic operations and other text editing commands. 
In the third category of menus, MCMC-related menus Inf 0, Model, Inference and 
Options can be included. The Info menu provides information concerning the status of 
the current model and its components. It can be used to open and clear a log file or window. 
By log file or window, we refer to the location where results and messages produced by 
WinBUGS are printed. By default, WinBUGS opens an output window that can be saved as 
an odc file. Moreover, using the Node Inf 0, the user can monitor the current values of any 
component of the model (random or constant). Finally, using the components command, 
we can monitor how many WinBUGS modules are currently active. 
The Model menu is the most important menu of Win- 
BUGS since it checks the syntax of the model code, runs the 
MCMC algorithm, and obtains the posterior sample. In more 
detail, it is used to 
Specify the model (compile model code, load data and 
initial values) via the Specification tool 
0 Update the chain (Update), to monitor the acceptance rate 
of the Metropolis-Hastings algorithm (Monitor Met) 
Save the current values of the variables of interest (Save 
State) 
Specify the random seed number used to initialize the random number generation 
procedure (Seed) 
0 Run a script code (Script) that generates a list of WinBUGS commands without 
using the corresponding menus (see Appendix B for more details) 
The Inference menu is also a valuable menu since with its available set of operations 
that we can 

THE WINBUGS ENVIRONMENT 
87 
0 Monitor the MCMC output 
0 Check the convergence 
0 Obtain posterior summaries and plots 
Generally, with this menuwe can infer for the posterior distribu- 
tions of the parameters of interest using the generated sample. 
The corresponding set of inference operations was limited in 
classic BUGS and most of the output analysis was performed 
using other statistical programs. 
The menu item Samples is the most frequently used Using this tool, the user can perform 
a variety of posterior output analyses producing, among others, summaries and graphical 
representations of the posterior distribution and the corresponding MCMC sample. Further 
analysis can be obtained using the remaining items of the Inference menu, including 
0 Comparison plots (using Compare) 
0 The posterior correlation matrix estimates (via the Correlations item) 
0 Summary measures of the posterior distributions of interest (Summary) 
0 Evaluation of the posterior distribution of the orderinglranks of one vector (Rank) 
0 Calculation ofthe deviance information criterion (DIC) of Spiegelhalter et al. (2002) 
for checking the goodness of fit (DIC item) 
The Options menu has three items: 
0 Output options, which controls the location where the output will printed (screen 
or log file) as well as the number of digits for the numeric output 
0 Blocking options, which specifies whether the updating method of Gamerman 
(1 997) for the simulation of blocks of fixed-effect parameters will be used 
0 Update options, which selects the simulation method for each component 
Finally, two menus can be referred as specialized ones: the Doodle and Map menus. 
0 The Doodle menu is used for constructing the model by drawing directed acyclic 
graphs (DAG). Specification of the model is produced graphically. This tool is con- 
venient and friendly for practitioners who are not familiar with programming but they 
can alternatively conceive of the structure of the model in terms of graphical paths 
and links. Additional details and a simple example are provided in Appendix A. This 
graphical interface was not included in the early DOS and LINUX versions of BUGS. 
It can be considered as an advanced, specialized function since the user can work in 
WinBUGS without using this module. 
0 The Map menu refers to the GeoBUGS facilities used for spatial modeling. GeoBUGS 
was been initially developed as an add-in upgrade option but was embodied in Win- 
BUGS version 1.4. GeoBUGS provides convenient tools for the graphical represen- 
tation, on actual maps, of geographic indices estimated using a Bayesian model. 

88 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
3.3 PRELIMINARIES ON USING WinBUGS 
WinBUGS code and output (including graphs and tables) can be stored in a compound doc- 
ument with the suffix odc. The user can complete all the basic word processing operations 
using the Tools, Edit, Attributes and Text menus of WinBUGS. 
In order to start building a model, you need to open a new compound document using 
the path File>New. 
3.3.1 Code structure and type of parameterdnodes 
In order to specify a model, using WinBUGS code, we need to enclose all commands with 
the model statement. Hence, we write 
The model’s parametersivariables are grouped into three categories: 
1. Constants: fixed values usually specified in the data section, which is described 
below. Explanatory variables, which are prefixed by the study design, are also set as 
constant parameters. 
2. Stochastic or random components of the model: the random variables of the model 
that are characterized by a distribution. Both the model parameters 8 and the response 
data (or variables) yz are stochastic components since they are described by the prior 
f(8) 
and the response distribution f ( y z  lo), respectively. 
3. Logical components of the model: variables specified by a basic mathematical ex- 
pression (called logical in WinBUGS terminology). They are simple functions or 
transformations of other model parameters. In this book we will refer to them as 
deterministic components. Deterministic components can be either random (hence 
be randomly distributed) or fixed constants depending on the expression or function 
used to specify them. 
Model parameters (or variables or components) are also called nodes from the corresponding 
terminology in graphical models. In fact, this term is used in the WinBUGS manual since 
the project’s research team facilitates directed acyclic graphs (DAGs) to describe all models; 
for more details, see Appendix A. 
The stochastic nodes are specified using the following syntax 
The tilde sign .. is used to specify that a variable “fol1ows”a distribution. As Variable, any 
conventional name can be adopted provided that it is not constrained by WinBUGS for other 
use. As distribution, a command from the list of the random distributions available by 
WinBUGS (described in Tables 3.1 and 3.2) can be selected. Most distribution command 
names are similar to the corresponding ones used in Rand Splus packages. For example, 
the expression X - N ( p ,  0’ = l/r) in WinBUGS language is written as 
where tau is the precision parameter. Note that in WinBUGS , the normal distribution is 
defined using its precision parameter, not its variance or standard deviation. 

PRELIMINARIES ON USING WINBUGS 
89 
A deterministic node can be specified by setting it equal to a mathematical expression 
using the assignment sign <-. Thus <- stands for the equality sign (=) used in mathematical 
equations. All such computations are hereafter called as node assignments or simply 
assignments. For example, we may calculate y = 5 + z / 3  + l / w  using the syntax 
In the case of the normal distribution, we may define by 
the variance of the distribution given by g2 = l/r. If t a u  is a constant node then s2 will be 
also a constant node. If t a u  is a stochastic component [i.e., with a prior distribution f ( r )  
attached to it], then s2 will be also a random variable. In this case WinBUGS will produce 
a sample from its posterior distribution f(c2Iy). In order to facilitate the specification of 
deterministic nodes, a variety of function commands are available; for details, see Section 
3.4.1 and the related summary in Table 3.3. 
Generally a simple model of the type 
produces random numbers from the distribution used. For example, the following simple 
code will generate random values for X from the standardized normal distribution: 
Note that each nodeivariable can be only defined once within the WinBUGS model code. 
Exception is made only for transformations of response variables [see relevant section in 
Spiegelhalter et al. (2003A)]. 
Comments can be included in the model code using the character #. Text after this 
character is ignored by WinBUGS , Comments are useful for reminding to the author or 
describing to other users the structure of the model. 
3.3.2 Scalar, vector, matrix, and array nodes 
In the previous section, the basic notions of WinBUGS were described, including the def- 
inition of a node and internode differentiation according to type. In this section, we focus 
on the dimension of a node. The simplest, in terms of computation, node is unidimensional 
and it is called a scalar node or simply a node. In WinBUGS , we may also define vector, 
matrix, and array nodes. 
of length n and elements u2 (for i = 1,2. . . . , n) is denoted in WinBUGS by 
v [I and elements v [i] , respectively. Similarly, a matrix M of dimension I x J with 
elements MZJ is denoted by M [ , 1 and M [ i , j I , respectively. Finally, arrays are high- 
dimensional arrays and are denoted using the same logic as above. Each array element is 
denoted by its name followed by indices of each dimension separated by commas included 
in square brackets. Hence a four-dimensional array A with elements A Z 3 k l  is denoted in 
WinBUGS by A [ , , , ] and A [i , j , k ,1] , respectively. R/Splus users are familiar with 
this notation. Parts of vectors, matrices, or arrays can be extracted using the same syntax 
as in R/Splus . All elements of a dimension can be extracted by omitting indices in the 
corresponding index space of the node. Thus, by M [i ,I and M [ , j I we extract the i row 
and j column vectors, respectively, of matrix M. Finally, using the expression n:m (where 
A vector 

Table 3.1 
Univariate distributions available in WinBUGS 
Distribution name 
WinBUGS syntax 
Probability or density function f (2) 
Mean 
Variance 
Discrete distributions 
P 
P(1 - P )  
(1) Bernoulli 
x 
dbern(p) 
f ( 1  - p)' -x 
(2) Binomial 
x 
dbincp, n) 
n!pX(l - p ) " + " / [ z ! ( n  - z)!] 
TLP 
"P(1 - P )  
(4) Negative binomial 
x 
dnegbincp, r )  
(z + T - l)!p"(l - p ) ' / [ x ! ( r  - l)!] 
~ ( 1  
- p ) / p  
7
0
 - P ) / P 2  
(5) Poisson 
x 
dpois(1ambda) 
exp(-A)X'/z! 
X 
X 
K 
(3) Categorical 
x 
dcat(p I1 ) 
PX 
Ex=, c,"==, 
I.: - E(z)I2PJ 
Continuous distributions 
(6) Bcta 
x " dbeta(a, b) 
r(u + b)za-' (1 - z)*-'/[r(a)IY(b)] 
a/(. + b) 
ab/[(a + b)'(a + 6 + I)] 
(7) Chi-squared 
x 
dchisqrck) 
Seegamma(k/2, $) 
k 
21; 
(8) Douhleexponential 
x - ddexpcmu, tau) 
i ~ c x p ( - ~ ( z  
- pl) 
w 
&/7 
(9) Exponential 
x - dexpclambda) 
Xe-X= 
/r(a) 
x 
dgamma(a, b) 
b " z a - l e - b x  
(10) Gamma 
(11)Generalizedgamma x 
gen.gamma(a, b, r) rb(bz)"'-' ex]' [ - ( b ~ ) ~ ' ]  / q u )  
r(a + 1/r)/[hr(u)] [qa 
+ ~ / T ' ) ) I ' ( u )  
- r(a + i / ~ ) ~ ] / [ x r ( a ) ] ~  
(I 2) Log-normal 
x 
dlnom(mu, tau) 
m z - ' e x p  [--r/2(pz - / L ) ~ ]  c ~ + ' / ( ~ ~ )  
( e l / r  - I ) & + ~ / T  
(13) Logistic 
x 
dlogis(mu, t a u )  
-reT(,- p' [l + cT(J'-i') 
(14) Normal 
x 
dnormhu, tau) 
=exp[-7(1. 
- 11)"/2] 
(15) Pareto 
x 
dparca, c )  
ab/((I - 1) 
&/[(a - 1)'((1 - 2)] 
I" 
7r'/[32] 
P 
' / T  
1- 
(Icaz - <I - 1 
(16) Student's t 
x - dt(mu, t a u ,  v) 
r [(u + 1)/21 & Z G [ r ( 7 , / 2 ) 1 - ~  
p 
1JT- ' / ( U  - 2 )  
x 1 + 77,- 
I(. - /')2] 
[ 
(17) Uniform 
x 
dunif (a, b) 
- a )  
a/(. + b) 
&(b - U ) Z  
(18) Weihull 
x - dueibcv, lambda) 
uXz"--' t x p ( - k " )  
I/",. 
Key: p t (0, I); (1) z = 0, 
1; (2) z = 0, 1.. . , n, p E (0, 1); (3) p [ ]  is a vector of dimension K, z = 1,2,. . . , K and elcments p[z] = pr t (0, l ) ,  
Ex=, 
p ,  = 1; (4) z = 0 , 1 , 2 , .  . .; 1' = 1 , 2 , .  . .; (5) z = 0, I, 2 , .  . .; X > 0; (6) :c t (0, l ) ,  a, h > 0; (8) z t K, 
p E R, T > 0; (9) z > 0, X > 0; 
(10, 11) z > 0, (I, b, I' > 0; (12) :E > 0, p t K, 7 > (I; (13, 14, 16) z E R, /" E R, 7 , ' ~  
> 0; (15) z > c, u ,  c > 0; (17) z t (a, h), u, 
b t R. (I < b; 
(18) z > 0, u, 
X > 0. 
K 

Table 3.2 Multivariate distributions available in WinBUGS 
Distribution name 
WmHUGS syntax 
Probability or density [unction f (z) 
Mean 
Varianceicovariancc 
Continuous distributions 
Key: (19) z[] and p [ ]  are vectors ofdimension K with dements z[i] = z, = 0,1,2,. . . and p [ i ]  = p ,  t ( 0 , l )  with cK 
2% = N and czLl 
p7 = 1; (20) z[] and u [ ]  are vectors of 
dimensionKwithelementsz[z] = zi € (O,l)anda[i] = ai > O w i t h x K  
t r l  2% = l a n d a  = ELl a,;(21,22):c[]and~nu[](zandp)arevcctorsofdimensionK withelements 
z[i] = zz t (0, 1) and ~ [ i ]  
= pL t R. T [ ]  
(and T) is a K x K symmetric precision matrix with elements T[i, j] = T ~ ,  
> 0; II > 0 (23) z[, ] and R[,] (x and R) are K x K 
positivc-definitc(symmetric)matriccswithelementsr[i,j] = z~, 
andR(i,j] = FL,; A,, aretheelementofthematrix A = R - ' ; v  > 0. Alsop,: meanofX,; V(X,),Cov(X,, X z ) :  
variance of X,; Cov(X, , X ,  ): covariance bctwecn X, and X,J 
2=1 

92 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
n, m are positive integer numbers), we can extract n, n + 1, . . . , m components of a vector 
(or array). For example, from a vector z we may extract elements 5-10 using the syntax 
v [5 : 101. Equivalently, using the syntax M [3 : 6 , l  and M [ ,10 : 111, all elements of rows 
3 - 6, and all elements of columns 10 - 11 (respectively) of matrix M can be extracted. 
Similar syntax may be used for higher dimensional arrays. For example, A [I : 2, 3 : 4, 
5 : 61 extracts all components Aijh of the three dimensional array A withi = 1, 2, j = 3,4, 
and k = 5,6, resulting in a new 2 x 2 x 2 array. 
This nomenclature can be summarized by the following notation concerning vector 
nodes: 
1. v [I : all elements of vector w 
2. v [i] : wi; i-th element of ZI 
3. v [n : m] : wn, vn+l, . . . , w,; 
elements n, n + 1, . . . , m of vector w 
The syntax used for matrices and their elements is summarized as follows: 
1. M [ ,I : all elements of matrix M 
2. M [i, jl : M i j ;  element of ith row and jth column of matrix M 
3. M [i ,I : Mil, 11122, . . . , M ~ J ;  
elements of i-th row of matrix M 
4. M 1, jl : M l j ,  M,j, . . . , M1j; elements of j-th column of matrix M 
5. M h : m ,  I : elements of n, n + 1, . . . , m rows of matrix M 
6. M [: ,n:ml : elements of n: n + 1,. . . , m columns ofmatrix M 
7. M [n : m ,  j 1 : elements of n, n + 1, . . . , m rows of j column of matrix M 
8. M [ i , n : ml : elements of n, n + 1, . . . , m columns of i row of matrix M 
9. M h : m ,  k: 11 : elements inn, n+ 1. . . . , m rows and k ,  k+ 1, . . . , I columns ofmatrix 
Finally, some indicative examples for three dimensional arrays of dimension I x J x K 
are summarized as follows: 
M 
1. A [ , ,I : all elements of array A. 
2. A [i, j , k] : A i j k  element of array A. 
3. A [i , ,I, A C, j , I ,  A [, , kl : elements with first, second, and third dimension equal to 
4. A [i , j ,I : elements with first and second dimensions equal to i and j .  The result is 
a vector. The results for A [i , , kl and A [ , j , k] are equivalent to these by obtaining 
all elements of the second and first dimension, respectively, with specific values for 
the remaining dimensions. 
i, j ,  and k, respectively. The result of this syntax is a matrix. 
5. A [n: m ,  ,I : all elements of A having the first dimension equal to n, n + 1, . . . , m. 
The result is also an array. The result of the commands A [ , n : m, I and A [ , , n : ml 
is similar for the corresponding elements of the second and the third dimension, 
respectively. 

BUILDING BAYESIAN MODELS IN WINBUGS 
93 
6. A [nl :ml ,n2 :m2 ,n3 :m31: elements A z l k  with i = n1.711+1.. . . , ml, j = n2, 
n 2 +  
1. . . . m2 and k = 723.713 + 1. . . . , m3. The result is also an array. 
Within brackets, calculations using the basic operations (+ , - , *, and /) are allowed. 
Division should be avoided unless it is ensured that the result is a positive integer number. 
Nested indexing of the type x [y [ill can also be used. This is very convenient when 
categorical data of z observation (group membership) are stored in vector y. For example, 
in one-way ANOVA models we want to specify that the mean of the ith subject is equal to 
pLz = a + bgz, where gz is the group in which the i subject belongs. This can be effectively 
written in WinBUGS as 
The dimension of a node is defined automatically by the maximum indices used for a 
node within the WinBUGS model code. 
Finally, note that computations directly on vectors, matrices, or arrays are not available 
in WinBUGS . Hence most of the calculations are performed separately for each elements. 
Details on this subject are provided in Section 3.4.2. 
3.4 BUILDING BAYESIAN MODELS IN WinBUGS 
In this section, we focus on specification of a Bayesian model. A full description and 
summary of the commands are provided, followed by details concerning calculations related 
to vectors, matrices, and arrays. Definition of a Bayesian model, including the prior and 
the likelihood specification, is also provided. The section concludes with specification of 
the data and the initial values. 
3.4.1 Function description 
As we have already mentioned, a set of functions is available within WinBUGS syntax. A 
summary of the functions is given in Table 3.3, and the most frequently used commands 
are described below. 
Simple arithmetic functions. Various simple arithmetic functions are available in Win- 
BUGS, including 
0 The absolute value (abs) 
0 The sine and cosine functions (sin, cos) 
0 The exponent and the natural logarithm (exp , log) 
0 The logarithm of the factorial of an integer number (logf act) 
0 The logarithm of the gamma function (loggam) 
0 The square root value (sqrt) 
All these functions require as an argument a single scalar node. In order to set y equal to a 
function of x, the following syntax can be used: 

94 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
Table 3.3 Functions available in WinBUGS 
WinBUGS Syntax 
Function 
Description 
1. abs(x) 
2. cloglog(x) 
3. cos(x) 
4. cut (XI 
5 .  equals(x1, x2) 
6. exp(x) 
7. inprodcvl [I , v2 [I ) 
8. i n t e r p . l i n ( x ,  v l [ I ,  
8. i n v e r s e ( # [ , ] )  
9.1og(x) 
lO.logdet(M[,I) 
I l . l o g f a c t ( k )  
12.1oggam(x) 
13. l o g i t  (x) 
14. rnax(x1, x2) 
15. mean(v[l) 
16.rnin(xl, x2) 
17. phi (x) 
18.pow(x, 2) 
19. s i n ( x )  
20. s q r t  (x) 
21. r a n k ( v [ l ,  k) 
22. ranked(v [I , k) 
23. round(x) 
24.sd(v[l) 
25. s t e p ( x )  
26. sum(v[l) 
21. trunc(x) 
1x1 
log(- log(1 - 2 ) )  
CO.(X) 
log ( k !  ) 
l o g ( r ( z ) )  
1% & 
max(z1. 2 2 )  
iT = Eb, .,/a, 
where n is the 
length of vector z' 
min(s1, x 2 )  
P(X 5 2 ) .  x - N ( O . 1 )  
2 2  
sin(x) 
6 
c, 
I ( u ,  5 v k ) ,  where I(t) = 1 if 
2 true and 0 otherwise 
U~ : C ,  I 
u ~ )  
= L 
Absolute value 
Complementary log-log function 
Cosine function 
Posterior of z is not updated by the likelihood 
Binaly indicator function for equal nodes 
Exponent value 
Inner product of two vectors 
Interpolation line 
Inverse of a symmetric positive-definite matrix 
Logarithm (In) 
Logarithm of the determinant of a symmetric 
positive-definite matrix 
Log factorial function of an integer 
Log gamma function 
Logit function 
Maximum of two values 
Sample mean 
Minimum of two values 
CDF of standardized normal 
Pouer function 
Sine function 
Square root 
Rank of s component of a vector 
Element of a vector with rank s 
Round to the closest integer 
Sample standard deviation 
Binary indicator function of positive nodes 
Sum of a vector's components 
Truncation to the closest smaller than x integer 
Kty: x , z = single real value or logical or mathematical expression; k = single integer value; v = vector; M = 
matrix. 

BUILDING BAYESIAN MODELS IN WINBUGS 
95 
For example, if y = 1x1, then in WinBUGS we write 
Furthermore, commands round and t r u n c  are used to obtain the closest and the lower 
closest integer values, respectively. 
Arithmetic functions with two parameters include the maximum and the minimum be- 
tween two values (max , min) and the power function (pow). If we wish to compare two 
values 21, x2 and keep (in another variable y) the maximum one, then the following syntax 
must be used 
while for the computation of y = xz we write 
Statistical functions. Within WinBUGS , simple statistical functions can be calculated. 
The sample mean (mean), the sample standard deviation (sd), and the sum ( sum ) of a 
vector are available. The syntax of these commands is the same as in the one-parameter 
arithmetic functions. However, their argument must be a vector node v (denoted by v [I ). 
Two further commands (rank, ranked) are related to the ranking of the elements of a 
vector. If we wish to calculate the rank of the kth element of vector v, then we define 
while with the syntax 
we obtain the element of v with rank equal to k. Note that the minimum and the maximum 
value of a vector v of length n can be calculated using the functions given above by writing 
respectively. Similarly, the median value can be calculated using the syntax 
when n is an odd number and using the syntax 
when n is an even number. 
standardized normal variate. Hence the syntax 
Command phi is used to calculate the cumulative distribution function (CDF) of a 
will calculate the value 
“
1
 
of a scalar node x. If we wish to calculate the CDF of N ( p ,  02) evaluated at x, then we use 
the syntax 
Finally, the command inprod calculates the inner product of two vectors. Hence the syntax 

96 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
calculates the expression 
i=l 
where v and w are vectors of length n with elements vi and wi; 
i = 1,2, . . . , n. This 
function is not actually statistical but a simple vector function. It is included in this section 
because it is frequently used for calculation of the sample covariance between two vectors 
v and w, given by the syntax 
Binary indicators. Two binary indicator functions are currently available in WinBUGS . 
Command equals compares two values resulting one if these two values are equal and 
zero otherwise. Thus, we have 
y<-equals( x ,  z 1 * y =  { O i f x f z  
Command s t e p  checks whether a node is zero or positive, resulting in value one or zero if 
this statement is true or false, respectively. Hence, we have 
0 i f x < O  
1 i f x > O  
’ 
y<-step( x ) + y =  
These commands are used to evaluate “if” equalities and inequalities. For example, we 
may check whether inequality x > z is true by writing 
Both these commands may also be used to indirectly implement “if statements”, which 
are frequently encountered in programming languages. They can also be used to facilitate 
vector computations or for distribution truncations. For example, if we wish to calculate 
the variable y = x if x > a, y = 2 if z < -a and y = 3, otherwise then we can write 
This syntax calculates the desired quantity since for x 2 a we find that s t e p  (x-a) =I and 
step(x+a) =1, resulting in 
y = x  x 1fz x ( 1 - 1 ) + 3  x 1 x ( 1 - l ) = ~ .  
For -a 5 x < a, we obtain step(x-a)=O and step(x+a)=l, resulting in 
Y = Z  x O + Z  x ( 1 - 1 ) + 3  x 1 x ( 1 - 0 ) = 3  
and for x < a, we have step(x-a)=O and step(x+a)=O,resulting in 
y = x  x O + Z  x ( 1 - 0 ) + 3  x 0 x ( 1 - 0 ) = ~ .  

BUILDING BAYESIAN MODELS IN WINBUGS 
97 
Link functions. Commands cloglog, l o g i t ,  probit, and log can be used on the left 
side of assignment. They are used to specify the corresponding link functions used in 
generalized linear models; see Chapter 7 for more details. 
Mathematical expressions for each link function are provided in Table 3.3. The probit 
function is the inverse function of the CDF of the standardized normal variate. Thus, syntax 
is equivalent to 
Matrix functions. 
nant of a symmetric positive-definite matrix using the syntax 
We can directly calculate the inverse and the logarithm of the determi- 
respectively, where M1 and M2 are both symmetric positive-definite matrix nodes of dimen- 
sion K x K and y is a scalar node. The resulting matrix, which appears in the left part of 
the assignment for the inverse matrix calculation, must be defined, including its dimension 
indices within square brackets (ie., the matrix name must be followed by El: K ,  1 : Kl ). 
The cut function. This command is used when we do not wish the posterior of each 
parameter to be updated from the likelihood. It is recommended only for advanced users; 
for more details, see Spiegelhalter et al. (20034. 
3.4.2 Using the f o r  syntax and array, matrix, and vector calculations 
Direct calculations between vectors, matrices, and arrays are not available in WinBUGS except 
for specific functions described above, For example, the calculation of 2 T y ,  where both z 
and y are vectors of the same length, is given by 
Other functions concerning vectors are the sum, mean, sd, rank, and ranked. Also, the 
commands inverse and logdet can be directly applied to symmetric positive-definite 
matrices. 
Since functions for multidimensional nodes are limited, most of their calculations are 
completed separately for each element. For this reason, a syntax repeating similar expres- 
sions for each element of a multidimensional node is required. Such repetition is performed 
in WinBUGS using the f o r  syntax: 
which repeats all commands within curly brackets 12-11 + 1 times, where i is an index taking 
all integer values from 11 to 1 2  sequentially in each repetition. In terms of programming 
terminology, such syntax is called a “loop.” WinBUGSloop syntax is the same as in 
R/Splus but here is used to specify nodes (stochastically using - or deterministically using 
<-). For example, we can calculate the sum of two vectors of length n by 

98 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
or conclude that all elements xi of a vector 2 follow independent normal distributions with 
mean pi and common precision 7 by 
In order to perform similar actions for matrices (and arrays), we need to use “nested’ 
loops. Hence we can produce the sum of two matrices using the following syntax 
where I and J are the rows and columns of the matrices A, B, and C. 
The multiplication 
of two matrices A and B of dimensions I x K and K x J ,  respectively, is slightly more 
complicated. It can be accomplished by using the inprod function: 
The range limits I1 and I 2  of the index values must be defined as constant nodes in the 
data part or the odc file; see Section 3.4.6 for details concerning data specification. 
3.4.3 Use of parentheses, brackets and curly braces in WinBUGS 
Let us summarize the use of each type of brackets: 
1. Parentheses () are used 
0 In mathematical expressions or computations 
0 In functions surrounding their parametersiarguments [e.g., log (x) ] 
0 In for loops to declare the values of the index 
2. Square brackets [ ] are used to specify the elements of a vector or array. 
3. Curly brackets { } are used to declare the beginning and the end of the model and 
for statements. 
3.4.4 Differences between WinBUGS and R/Splus syntax 
At this point we summarize the main differences between the R/Splus and WinBUGS syntax. 
This is very useful for users of R/Splus, especially for those who they wish to use 
R/Splus for data manipulation and then import them in WinBUGS . 
First, the order of the commands is not important in WinBUGS since the model is com- 
piled altogether (simultaneously), unlike R/Splus , where the commands are executed 
sequentially. 
A further difference is that in WinBUGS we do not need to define a vector or array before 
using it. Its dimension is set equal to the maximum of indices used for the corresponding 
node within the code. 

BUILDING BAYESIAN MODELS IN WINBUGS 
99 
Mathematical expressions are implemented only in unidimensional scalar nodes in Win- 
BUGS. 
Finally, in WinBUGS , an entire vector is denoted by its name followed by square brackets 
(e.g., x [I ) while an array is denoted by its name followed by brackets, including commas 
to declare the number of dimensions (e.g., x [ , ,I indicates a three-dimensional array). 
3.4.5 Model specification in WinBUGS 
Likelihood specification. 
stored in a vector y with elements yi. The stochastic part of the model can be written as 
Let us assume a response variable Y with n observed values 
Y -- Distribution(6) 
where 6 is the parameter vector of assumed distribution. The parameter vector is “linked” 
with some explanatory variables X I  
~ X2 . . . . X ,  using a link function h 
6 = h(e, xl. 
xa1.. 
. xp). 
where 8 is a constrained set of parameters used to specify the link function and the final 
structure of the model. The vector 6 is the actual set of parameters to be estimated. More- 
over, each subject specific for observed values of z li. zzi, . . . , zpi will define a different 
set of parameters 8 for each subject i given by 
qi) 
= h(e: 51il 2 2 i l .  . . zpi) . 
In generalized linear models this function associates (or links) the parameters ofthe assumed 
distribution with a linear combination of the explanatory variables. The likelihood of the 
model is given by 
The corresponding WinBUGS syntax is given by 
where distribution. name is one of the prespecified WinBUGS distributions given in 
Tables 3.1 and 3.2 while parameter1 [i] , parameter2 [il , . . . refers to the elements 
of q z ) .  
Let us, for example, consider the case of a simple regression model with 
Y, -- X(pt.02) with pLz = Q: + 35% 
In this example we have 6ia) = ( p t .  7 )  (where 7 = a-2), 8 = (al b, 02), and h as the 
function 
The model described above is defined using the syntax 

100 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
Prior specification. To complete the Bayesian model specification, we further need to 
specify the prior distribution of the model parameters 8. Thus, we complete the specification 
by writing 
If the parameters 8 are stored in a single vector (of length K )  and all elements follow a 
prior distribution of the same type, then a f o r  loop can be used to specify them using the 
syntax 
Hierarchically structured models can also be specified by allowing the prior parameters 
of 8 to follow a prior distribution. Such specification allows us to define correlations or 
specify a wide variety of mixture distributions; see in Chapter 9 for more details. 
3.4.6 Data and initial value specification 
Data can be imported in WinBUGS using two different formats: the rectangular and the 
list formats. 
3.4.6.7 
Rectangular data format. The first data format is simple and similar to the 
text files used by most statistical packages. It can be used to specify a series of variables - 
vectors of the same length or a matrix (or array). Concerning simple vectors (for example 
variables denoted by y, xl, x2, x3), we only need to specify the names followed by square 
brackets in the first line and the values of each observation in each line, separated by empty 
spaces. Be careful: The data must conclude with the command END followed by at least 
one blank line. This detail has been added in version 1.4 and was not required in earlier 
versions of the program. Hence, an example is given by the following 
Similarly, a matrix Y with four columns (i.e., with dimension n x 3) is specified in the 
rectangular data format by 

BUILDING BAYESIAN MODELS IN WINBUGS 
101 
A higher-dimensional array is defined similarly by specifying all vectors resulting from 
leaving the first index empty. Therefore for a 4 x 2 x 2 array A with elements Aa3k = 
1OOi + l O j  + k is defined by 
3.4.6.2 List data format. The list data format is similar to the list objects used in 
R/Splus . This format can be used to specify single constant numbers (scalar nodes), vec- 
tors, matrices, and arrays. The syntax of this data format always begins with the command 
list followed by parentheses without any separating space. Within the parentheses we 
specify each variable separated by commas. We can define 
1. Scalarshingle numbers: using the syntax 
2. Vectors: using the syntax 
3. Matrices: using the syntax 
4. Arrays: using the same syntax as in matrices but the . D i m  argument will have at 
least three values specifying the length of each corresponding dimension. 
Matrix elements defined in .Data argument are arranged by row, which means that the 
elements of the first row are specified first, followed by the elements of the second row, and 
so on. Note that although this syntax is similar to the corresponding one used in R/Splus , 
data in these programs are arranged by column (defining initially the elements of the first 
column followed by those elements of the second column, etc.). Therefore, R/Splus users 
must be cautious when defining data using this syntax in WinBUGS . For example, syntax 
defines matrix A as 
while in WinBUGS , it defines matrix A as follows: 

102 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
For higher-dimensional arrays, the data in WinBUGS are organized such that the last 
index of the array changes faster, followed by dimensions from last to first (and indices 
from left to right). Hence, for a three-way table of dimension 5 x 3 x 2, we first define 
elements (zl1j;j = 1,2), then elements ( z l 2 j ; j  = 1,2), ( 2 1 3 j ; j  = 1,2), followedby 
elements ( ~ 1 j ;  
j = 1.2), ( z z 2 j ; j  = 1,2), ( 2 2 3 j ; j  = 1: 2), and so on. On the other hand, 
in R/Splus the first index changes faster, followed by indices from right to left. Hence we 
first specify xi11 for all i = 1,2.3.4,5 followed by 5 i 2 1 ,  x i3 1  2i12, 2 i2 2 , 2i32. Hence, 
syntax 
produces, in R/Splus , array A with the following structure 
while in WinBUGS , it produces array A given by 
3.4.6.3 Importing data from R/Splus . It is convenient to use R/Splus to specify 
data and then extract them in a format compatible to the WinBUGS list format. In order to 

BUILDING BAYESIAN MODELS IN WINBUGS 
103 
do that, we create a list object in R/Splus with all the desired components. Unfortunately, 
because of minor differences between WinBUGS and R/Splus syntax, some modifications 
are needed concerning matrices and arrays. 
For arrays we propose the following strategy. Let us assume, for example, that the 
following matrix x is available in R/Splus 
which can be defined using the command 
In R , commands 
will print on the commandwindow a syntax that is close to WinBUGS syntax. Alternatively, 
syntax dput (x , ' filename . txt ' ) can be used to save the syntax to a file instead ofbeing 
printed it at the command window of R/Splus . 
In more detail, in R (version 2.5), the preceding command produces 
We can modify this syntax as follows: 
1, Remove structure ( from the beginning and ) , .Names = "x") from the end 
of the syntax. 
2. Add .Data= betweenx = structure( and c (  
3. Reverse the dimension order. 
Hence substitute .Dim = c(3, 2) 
with 
.Dim = c(2,3). 
This procedure finally results in the following WinBUGS list data format: 
The corresponding result of dput command in Splus (version 6.1) is 
list( x = structure( .Data=c(l, 2, 3, 4, 5, 6),.Dim = c(2, 3) ) .  
In this syntax we make the following changes 
1. Replace matrix( with structure( .Data=. 
2. Remove quotes from the matrix name - 
replace "x" with x. 
3. Reverse the dimension order and use the .Dim expression; substitute nrow = 3, 
ncol = 2 by . D i m  = c(2,3). 
The result is the same syntax as above: 
list( x = structure( .Data= c(1, 2, 3, 4, 5, 6), .Dim = c(2, 3) 1. 
For higher-dimensional arrays, the data should be stored in reverse order of the indices. 
Then, using the command dput, we extract the code as above and proceed by making 

104 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
the appropriate changes. The most important part is to reverse the order of the dimen- 
sion argument. For example, see the model specification section of the WinBUGS manual 
(Spiegelhalter et al., 20034 . 
If an array is already stored in R/Splus , then we need to use nested f o r  loops to reverse 
the order of the dimensions. Hence, for a three-dimensional array we can use the syntax 
For higher dimensions, additional nested loops are needed. 
3.4.6.4 A simple example of data specification. Let us assume the following 
small dataset 
y xi x2 
gender age 
12 2 
0.3 
1 
20 
23 5 
0.2 
2 
21 
54 9 
0.9 
1 
23 
32 11 2.1 
2 
20 
We further need to specify the sample size n = 4 and the number of variables p = 5. This 
dataset can be defined using the following list syntax: 
while the same data in a matrix form will be specified using the following syntax: 
The data can be viewed within WinBUGS by the menu path Inf o>Node Info after 
compiling the model (see Section 3.5) and are given using the following sequence: 

BUILDING BAYESIAN MODELS IN WINBUGS 
105 
3.4.6.5 A simple example using arrays. Let us assume that we have the following 
array 
and we wish to insert it in WinBUGS including a vector of data with the dimensions of the 
array and the total number of cells and frequencies. Then, we need to write 
In order to ensure that you have correctly specified the data sequence, we propose inserting 
the table in R/Splus , using the f o r  syntax to reverse the dimension index sequence, and 
then exporting the data using the command dput as described in the previous subsection. 
The data imported will appear in the following sequence: 
3.4.6.6 Mixed and multiple data definition. In some cases, it is convenient to 
use a combination of both types of data formats. In this way, we avoid difficulties in the 

106 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
specification of matrix or arrays elements. Thus, some data can be loaded in a rectangular 
format and others in a list format. When using a rectangular format, we frequently need to 
specify some additional data (usually constants such as the data sample size) in a separate 
list format. In the case of multiple datasets, we need to load each set separately (one at a 
time) by pressing the load button (in model specification t o o l )  multiple times; for 
details, see Section 3.5. 
For example, in the first data example above, we can define variables y, 2 1,x2, gender, 
and age using the simple rectangular format 
and the constants n and p using the following list format 
list( n-4, p=5) 
This is similar to the syntax if we wish to load all data in a single array written in a 
rectangular data format. The only difference is that the first line (with the variable names) 
must be replaced by 
Alternatively, if we wish to specify the first column as the response with the name y and 
the rest of the variables in one matrix, then we write 
For the second case of the 3 x 4 x 2 matrix we can define the array by 
and the remainder using the following list format: 
For larger arrays it is convenient to specify each matrix defined by the combinations of the 
other dimensions in separate rectangular formats. For example, in the 3 x 4 x 2 array above 
we can write 

BUILDING BAYESIAN MODELS IN WINBUGS 
107 
In the latter, the user must specify the data multiple times (one for each data definition 
used). Note that the rectangular data format with the highest indics (here x [ ,4,21) must 
be read first. In this way WinBUGS will create a matrix of the correct dimension (here 
3 x 4 x 2) and then try to fill it in. 
3.4.6.7 Initial values. Initial values are used to initiate the MCMC sampler. Their 
format is the same as the list data format. Initial values must be provided for all stochastic 
nodes except for the response dataivariables. The user does not need to specify all initial 
values but can generate all or portion of them. Users must be careful in the case of using 
randomly generated initial values since they may face problems when certain parameters are 
initialized using inappropriate values resulting in numerical problems or slow convergence 
of the algorithm. 
3.4.6.8 Other details. Missing values are represented using the conventional name 
NA, which stands for a nonavailable value. 
All loaded data must be used in the model specification. For users who do not wish to 
use specific variables in the current model, we recommend to defining a simple model for 
these variables that will not be connected (and therefore it will not affect) the main model 
under consideration. 
Data are usually stored in the same odc file as the main model code. To conserve space, 
we can hide or suppress data using a WinBUGS "fold" following the menu path 
T o o l s  > Create F o l d  
Items written between the arrows of a fold can be hidden by clicking on these arrows. This 
option assists the user to keep the odc file small and easy to read. 
3.4.7 An example of a complete model specification 
Let us assume that we have a sample of 10 values from the normal distribution and we wish 
to estimate the posterior distribution of the mean and the variance. The parameter vector 
0 = (p. D ~ ) .  Moreover, we use a normal prior distribution with mean zero and variance 
equal to 100 for p, and the inverse gamma distribution for D with both parameters equal 
to &. The latter induces a gamma prior distribution for the precision parameter used in 
WinBUGS with mean equal to one and variance equal to 100. Both these priors can be 
regarded as low-information priors since their variance is large. The full model code is 
given by 
model{ 
# likelihood 
for (1 in l:n){ y[i] " dnorm( mu, t a u  ) 
> 
mu " dnorm( 0, 0.01 f 
# prior for m u  
tau" dgammac 0.01, 0.01 
# prior for tau 
# 
# deterministic definition of variance 
sigma.squared<-L/tau 
# deterministic definition of st. deviation 
sigma<-sqrt(sigma.squared) 
> 
and the data and the initial values by 
DATA 
list( n=10, y=c(-1.76, 0.38, 1.23, -0.67, -0.47, -1.36, 1.41, -0.07, 
INITS 
list( mu=l, tau=:! 

108 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
Initial values are needed for parameters p and 7 since they are the stochastic components of 
the model. Parameter 02, which is the one that we actually wish to estimate, is defined in 
WinBUGS as a deterministic node, and hence initial values for this parameter are defined 
indirectly via the initial values of “i. 
3.4.8 Data transformations 
When we wish to consider transformations of the response data, which are stochastic nodes, 
the simplest way is to insert directly transformed data. Nevertheless, WinBUGS allows for 
transformations of the response data. In fact, this is the only case where we can specify 
the same node twice: first, to define the transformation of the original data and then to 
assign the random distribution. In the example above, we can specify that the logarithm 
of the absolute value of y follows a normal distribution. Then the likelihood for the new 
transformed model can be express by the following WinBUGS syntax: 
These transformations are permitted only for functions of observed data. No stochastic 
nodes can participate in the transformations of the syntax shown above. Thus, missing 
values are also not allowed in the transformed data. 
From the WinBUGS team, we recommend specifying any transformations at the begin- 
ning of the model code in order to separate data transformations from the actual model 
definition and, in this way, simplify the model code structure. 
3.5 COMPILING THE MODEL AND SIMULATING VALUES 
After writing the full model code, the data and the initial values in an odc file we need to 
compile and run the model. This procedure is described using the following steps (also see 
Section 3.7 for a summary of the procedure): 
1. Open model specification tool. 
2. Check the model’s syntax. 
3. Load data. 
4. Compile model. 
5. Set initial values. 
6. Run the MCMC algorithm. 

COMPILING THE MODEL AND SIMULATING VALUES 
109 
To be more specific: 
1. Open the Model Specijication Tool. Follow the path 
Model> Specification 
to open the (model) specification tool. 
In this way, the model specification tool appears on the screen. This tool includes all 
the basic operations needed to initialize the MCMC algorithm (checking the model’s 
code syntax, loading the data, compiling the model, and setting the initial values) and 
specify the number of chains that we wish to generate. 
2. Check the syntax of the model. Highlight the command model and press the check 
model box of the model specification tool. 
WinBUGS checks the model’s syntax starting from the first character of the high- 
lighted command. If no command is highlighted, then the check begins at the top of 
the opened file or window. We recommend always highlighting the desired model 
command to avoid problems, especially if multiple model codes are included in the 
same odc file or window. If a problem in the syntax exists, then an indication is given 
in the lower left of the WinBUGS window while the cursor is placed at the location 
where the error was detected. 

11 0 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
Otherwise, the message model is s y n t a c t i c a l l y  c o r r e c t  appears in the same 
position. 

COMPILING THE MODEL AND SIMULATING VALUES 
11 1 
3. Load the data. Highlight the word list in the data list format and press the load 
d a t a  box of the model specification tool. 
When the data are defined in a rectangular format then we highlight the first row of 
the data where the names are declared. 
If a set of data is loaded successfully, then the message d a t a  loaded will appear at 
the bottom left of the WinBUGS window (status bar). Otherwise, an error message 
will appear in the same position. 

112 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
Finally, when multiple sets of data need to be declared, then we need to repeat the 
procedure (highlight data, load data) once for every set of data. For example, if the 
data are in the following format 
then the procedure is as described in Figure 3.1 
Figure 3.1 Example of multiple-dataset specification. 

COMPILING THE MODEL AND SIMULATING VALUES 
113 
4. Compile Model. After all data are loaded, press the compile box in the model 
specification tool. 
If the compilation is successful then the message model compiled will appear in 
the status bar otherwise an error message will appear in the same position. 

11 4 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
5. Set initial values. Initial values are set by following a procedure similar to the one 
used for data. We highlight the word list and then press the load i n i t s  box at 
the model specification tool. 
If all initial values are set successfully, the message model is initialized will 
appear in the status bar; otherwise an error message will be generated in the same 
position. 
If no initial values are specified for some 
stochastic nodes, 
we get the message 
the chain contains uninitialized 
variables in the status bar. 

COMPILING THE MODEL AND SIMULATING VALUES 
11 5 
In this case, we can generate random values 
for the remaining nodes using the gen i n i t s  
box of the model specification tool. This box 
is useful when the number of stochastic nodes 
is large, such as in the cases of missing val- 
ues, random effects, or latent data. This option 
should be avoided because it may generate in- 
appropriate values that, for some nodes, will 
cause overflows or simply delay the conver- 
gence of the algorithm. 
After setting (loading or generating) the initial values, the algorithm is initialized 
and is ready to simulate values using the update tool, which is described in the 
following step. 
6. Run the MCMC algorithm -generate random variables (burnin period). Follow the 
path Model> Update to open the update tool. 
The update tool has the following features: 
a. Updates: This refers to the number of additional iterations we want to generate 
(and store). 
b. Refresh: This refers to the number of iterations that WinBUGS will use to re- 
fresh the current iteration index in the update tool and update the online trace 
plots that are available via the Sample monitor tool. Using small refresh val- 
ues is useful for monitoring the chain and for stopping or pausing the algorithm. 
The refresh value must be changed according to the current MCMC algorithm. 
For simple models (where the algorithm generates large iterations rapidly), that 
we recommend using large refresh values, while for more complicated mod- 
els (where the MCMC algorithm will be slower) we recommend selecting low 
refresh values. 
c. Update butt on: Pressing this button will generate additional iterations equal 
to the number defined in the Updates box. If this button is pressed during 
the update procedure, then the algorithm will pause. During the pause of the 
MCMC run, specifications of the chain can be changed (e.g., the refresh rate) 
before restarting the algorithm by again clicking this button. 
d. Thin: This defines the thin (lag) of iterations kept. Hence, if we set the addi- 
tional updates equal to T' and the thin equal to k, then WinBUGS will generate 
k x T' iterations but will store only the last one in every sequence of k generated 
values. 

11 6 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
e. Iteration: This is a visual counter of the current iteration of the algorithm. 
This is not the actual number of iterations carried out by the MCMC algorithm; 
rather it is the number of iterations that have already been stored. Thus, if this 
counter is equal to TI iterations and the thin is specified to be equal to k, then 
the actual number of generated values will be equal to k x T ’. 
f. Overrelax: We may implement an overrelaxed MCMC algorithm (Neal, 
1998), whenever this is feasible, by checking this box. This approach may 
be used to reduce high autocorrelations. This can be achieved by generating 
multiple sets of values and selecting the one that is negatively correlated with 
the values ofthe current iteration. Since multiple values are generated, the sam- 
pling of each iteration is considerably increased. According to Spiegelhalter 
et al. (20034, “this method is not always effective and should be used with 
caution.” 
g. Adapting: This is an indicator box (like iterations) and is ticked during the 
tuning period when the Metropolis or the slice sampler algorithms are used. 
During the tuning period, parameters of the samplers are optimized in order 
to make the algorithms effective. The tuning period is set to 4000 and 500 
iterations for Metropolis and slice sampler, respectively. The iterations of the 
tuning phase cannot be used for posterior inference (summaries, plots, etc.). 
Note that the update tool will be available only after initializing correctly the MCMC 
algorithm for a given model. 
7. Set theparameters we wish to monitor: Follow the path Inference> Samples to 
open the sample monitor tool. 
Write the name of the parameter that we wish to monitor in the node box and 
then press the s e t  button. 
Using the procedure described above, we specify the parameters whose posterior 
distributions we wish to estimate via the MCMC generated values. The simulated 

BASIC OUTPUT ANALYSIS USING THE SAMPLE MONITOR TOOL 
11 7 
values of these parameters will be now stored in order to produce a detailed pos- 
terior analysis. In WinBUGS terminology, this procedure is “setting the monitored 
parameters”. 
8. Update the MCMC algorithm -generate andstore random variables. After setting 
the parameter we wish to monitor, we update the MCMC sampler by repeating the 
procedure described in step 6. After setting the parameters of interest and generating 
additional random values, we can monitor the posterior distribution by extracting 
posterior summary statistics and plots. Analysis of the MCMC output is made via 
the Inference menu and mainly by the sample monitor tool. Detailed description 
follows in Section 3.6. 
3.6 BASIC OUTPUT ANALYSIS USING THE SAMPLE MONITOR TOOL 
The main tool for output analysis is the sample monitor tool. It provides a variety of 
options for simple output analysis of the stored random values. Most of them can be applied 
after setting which parameters we wish to monitor and after generating additional random 
values from the posterior distribution of the model (see steps 7 and 8 ofthe previous section). 
The following choices are available: 
0 node box: In this text box, the user inserts the name of the node to be used (set or 
obtain summaries). If the text does not corresponds to a name of a node, then all 
options will remain disabled. When a node that has not been monitored yet is typed, 
then only the set option is enabled. 
On the other hand, when the node is included in the monitored ones, then all options 
are enabled. A list of the monitored nodes can be viewed by clicking on the arrow at 
the right of the box. Output analysis of a monitored node is obtained by selecting its 
name from this list. 

11 8 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
If we wish to obtain the same summary for all monitorednodes, then the star character 
(*) can be used in the node text box. An internal node named deviance is automat- 
ically generated by WinBUGS . It is equal to minus twice times the log-likelihood 
evaluated in the parameter values of each iteration. This can also be set as a node 
name at the sample monitor tool. This value is frequently used to compare or 
evaluate the fit of the current model. 
0 chains box: In this box, we specify which chains will be used for calculation of 
0 beg and end: Iterations included in the posterior analysis are declared here. 
0 t h i n  text box: The lag of the iterations used for the posterior analysis is defined in 
this box. It considers only one observation every k iterations. This is similar to the 
t h i n  in the update tool. The difference is that here t h i n  refers to the stored values 
and can be changed by the user after monitoring autocorrelations and MC errors. 
It refers only to the calculation of summaries and plots and not to the stored set of 
generated values as in the update tool. 
0 s e t  button: This button is used to specify which nodes we wish to monitor after the 
current state of the algorithm. Values generated after pressing the s e t  button are 
stored and can be used for posterior analysis. 
0 t r a c e  button: This button produces an online plot of the generated values against 
each iteration number. Points are plotted every k iteration; where k is the value 
declared in refresh button ofthe update tool. This plot is also referred as dynamic 
trace plot in the WinBUGS manual. 
summary statistics. 
0 density button: This but- 
ton produces an approximate 
visual kernel estimate of the 
posterior density or probabil- 
ity function. 

BASIC OUTPUT ANALYSIS USING THE SAMPLE MONITOR TOOL 
11 9 
0 history button: While the t r a c e  plotprovides anonline plot ofthe values generated, 
the history button draws a full trace plot of all stored values. 
percentiles selection box: In this text box, we specify which posterior percentiles 
we wish to calculate in the stats. 
0 stats button: This button estimates the posterior summary estimates by the MCMC 
output: mean, standard deviation, Monte Carlo (MC) error, and selected percentiles 
(default values are 2.5%, 50%, and 97.5%). It also provides details concerning 
the number of iterations discarded as bumin period and iterations finally kept for 
estimation. Monte Carlo error is estimated using the batch mean method; see Section 
2.2.2 for details. 
coda button: This button produces a set windows with the sampled values in a format 
compatible to one used by CODA software. CODA is an add-in package for Splus and 
R that is used for checking convergence of the algorithm using a variety of diagnostics. 
quantiles button: A plot of the evolution 
forthemedianandthe2.5% and97.5%per- 
centiles for each iteration of the algorithm 
is obtained by this button. 
0 bgr diag button: This button implements a modified version of the Gelman-Rubin 
convergence diagnostic (Gelman and Rubin, 1992; Brooks and Gelman, 1998). It 
can be applied only when multiple chains are used. 

120 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
auto cor button: This button plots the au- 
tocorrelations of the chain for each param- 
eter of interest. The default is to plot all au- 
tocorrelation using lag from 1 to 50. Values 
of the autocorrelations can be monitored by 
double-clicking on the plot and then press- 
ing Ctrl on the keyboard and the left mouse 
button. 
c l e a r button: This button clears and removes all stored sampled values of a given 
node. It is equivalent to restarting the algorithm with initial values equal to the ones 
of the current state of the algorithm. 
The density and stats options are used to obtain posterior (numerical and visual) 
estimates of the posterior distribution, while the remaining options are used them mainly 
to monitor the convergence of the chain. MC error, which is calculated in stats option, 
can also be used to check the convergence of the chain. The density, auto cor, stats, 
coda, quantiles, and bgr diag buttons will be disabled if the MCMC algorithm is 
in an adapting phase of a Metropolis or slice sampler (initial 4000 and 500 iterations, 
respectively). 
The convergence of the chain can be initially checked visually using trace plots obtained 
by the History option in the sample monitor tool. Values within a parallel zone without 
strong seasonalities will indicate convergence of the chain. Moreover, MC error can be 
checked. Ifthe value is low in comparison to its posterior summaries (especially its standard 
error), then the posterior density is estimated with accuracy ( s t a t s  option). For example, 
we can assume convergence when the MC error is lower than the 0.1% of the corresponding 
posterior standard deviation. We can further monitor convergence using autocorrelation 
plots. If autocorrelations are low, then convergence is obtained in a relatively low number 
of iterations (for details, see Section 2.2.2.4). 
A formal convergence diagnostic (Brooks and Gelman, 1998) can be implemented using 
the option bgr diag, but it requires multiple chain simulation (see Section 4.3). 
Finally, we can use CODA (Best et al., 1996) and BOA (Smith, 2005) packages in Rto 
obtain additional model diagnostics. In order to extract generated values in a file format 
compatible to this packages, we use the coda option. Files required by CODA or BOA are 
opened in separate windows that can be saved and imported in R (or Splus). 
3.7 SUMMARIZING THE PROCEDURE 
We can summarize the procedure of obtaining posterior sample of model in WinBUGS by 
the following steps 
1. Prepare the odc file - 
write the model code, specify data and initial values. 
2. Compile and initialize the model. 
3. Run the model/generate random values. 
4. Perform output analysis using WinBUGS , 
5. Apply convergence tests using BOAKODA or other software. 

CHAPTER SUMMARY AND CONCLUDING COMMENTS 
121 
6. Import WinBUGS output values to other package for additional analysis (plots, tests, 
etc.). 
Steps 1-4 are the basic ones, while steps 5 and 6 are useful but can be omitted in simple 
examples where convergence is obvious and no additional analysis is needed. The procedure 
is also summarized in Figure 3.2 
3.8 CHAPTER SUMMARY AND CONCLUDING COMMENTS 
In this chapter, the user was introduced to the basic functions of WinBUGS software. De- 
scription of the code structure and commands are provided in detail, as well as guidance 
for running and analyzing simple models. Hence, after this chapter, the reader must be 
able to write simple models in WinBUGS code, compile them, generate values, and, finally, 
produce simple analysis of the MCMC output. 
In the chapter that follows, a full example is provided, as well as more advanced or 
specialized functions of WinBUGS . 
Problems 
3.1 
Consider the following data: 
0.4 0 .0 1  0 .2  0 . 1  2 . 1  0 . 1  0.9 2 . 4  0 . 1  0.2 
Use the exponential distribution Y, N exponential(6') to model these data and impose 
a prior on log 6'. 
a) Define the data in WinBUGS using both rectangular and list formats. Use 6' = 1 
b) Write the model code. 
c) Compile the model and obtain a sample of 1000 iterations after discarding initial 
d) Monitor the convergence graphically using trace and autocorrelations plots. 
e) Calculate Monte Carlo errors using WinBUGS . 
f) Obtain posterior summaries and density plots for 8, l/O, and log 6'. 
g) Export MCMC values using the CODA option. Import them in another statistical 
Consider data of Problem 3.1. 
a) Use the gamma and the log-normal distributions to model the data. 
b) Use the normal distribution for the logarithms of the original values to model the 
c) In all models perform analysis similar to that in Problem 3.1. 
d) Compare the results obtained under all the models above. 
Use the Poisson and the negative binomial distributions to model the data of Problem 
1.8. 
a) Obtain a posterior sample for the model parameters. 
b) Calculate the probability for each team to win a game. 
Use the Poisson and the multinomial distributions to model the data of the 2 x 2 
contingency table of Problem 1.6. 
a) Define the data using both the rectangular and the list formats. 
as initial value. 
500 iterations as burnin. 
package and obtain the ergodic mean plot. 
3.2 
data. 
3.3 
3.4 

122 
WINBUGS SOFTWARE: INTRODUCTION, SETUP, AND BASIC ANALYSIS 
Figure 3.2 Summary chart for running a model in WinBUGS . 

PROBLEMS 
123 
b) Perform output analysis and obtain posterior summaries for the cell probabilities 
c) Estimate the posterior distribution of the odds ratio 
as well as for the expected counts under both distributions. 
under both formulations, where Eij are the expected counts corresponding to cell 
i,j. 
3.5 
Consider the following data: 
Samplel: 1.50 0.54 0.85 
2.04 -0.49 -0.65 
0.98 -0.63 -1.72 
Sample2: 0.80 
0.03 -0.37 
0.67 
0.76 
2.47 
1.36 2.29 
2.26 
-0.06 
0.60 
1.25 -0.72 
0.66 -0.39 
2.71 -0.28 
1.70 
1.88 -0.35 
1.90 
Use the normal and the Student’s t distributions to model these data assuming 
that the parameters of each sample are different. 
Obtain the posterior distributions for the means and variances for each sample 
and compare them. 
Estimate the posterior distributions of p1 - p2 and 0 1 / 0 2 .  Use the posterior 
sample to infer for the equality of the means and the variances of the two samples, 
where pk and q are the mean and standard deviation, respectively, of the kth 
sample. 

CHAPTER 4 
WinBUGS SOFTWARE: ILLUSTRATION, 
RESULTS, AND FURTHER ANALYSIS 
In this chapter we provide a full example into WinBUGS and additional, more specialized, 
functions, and tools in WinBUGS . The chapter is divided in five sections. In Section 4.1 
we analyze the data of Example 1.4 (Kobe Bryant’s’s data) using the basic functions of 
WinBUGS introduced in the previous chapter. In Section 4.2 we proceed to describe the 
remaining functions of the inference menu, which offers a variety of tools for the analysis 
of the MCMC output. Generation of multiple chains is described in Section 4.3. Finally, 
control of figure properties and the remaining tools, and menus are presented in the last two 
sections of the chapter. All functions, tools and menus are illustrated using Example 1.4 
(Kobe Bryant’s’s data) and the model fitted in Section 4.1. 
4.1 A COMPLETE EXAMPLE OF RUNNING MCMC IN WinBUGS FOR A 
SIMPLE MODEL 
4.1.1 The model 
In this section we will consider Example 1.4. Let us examine again the Kobe Bryant success 
probabilities but now, instead of the original parameterization, alternatively consider the 
following expression of the success probabilities 
Tk = T k - 1  X R k  
for k = 2, . . . ~ 8 (corresponding to seasons 2000101, . . . , 2006/07), while T 1 will be esti- 
mated individually and will denote the success probability for season 1999. Quantities R k 
(4.1) 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
125 

126 
WINBUGS SOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
denote the performance of the current season (in terms of success probabilities) in compar- 
ison to the season. We use a beta prior for the success probability of the first season and 
gamma priors for the relative success measures R k .  In both cases we use low parameter 
values to express prior ignorance. Thus, we use 
N beta(0.01, 0.01) and R k  N gamma(0.01, 0.01) for k = 2 , .  . . 8. 
This prior setup can be simply defined in WinBUGS code using the commands 
Note that if we use a vector to specify R k ,  then we also need to define R1. For this reason, 
we assign it a dummy value equal to one. 
The likelihood for this model is given by 
8 
f ( Y 1 T l . .  . . , T 8 )  = n T y ’ ( 1  -T)N’-’’, 
r = l  
with Tk given by (4.1). This likelihood will be written in WinBUGS code as 
while Eq. (4.1) will be expressed by the following syntax: 
Hence the complete model code is given as follows: 
In this code ,we can avoid multiple loops by including all commands related to the model 
in a single loop. Thus, the following code will have exactly the same effect as the previous 
code: 

A COMPLETE EXAMPLE OF RUNNING MCMC IN WINBUGS FOR A SIMPLE MODEL 
127 
Although the final result will be the same in terms of WinBUGS , the model code cannot 
be followed as easily as the first one. We recommend writing model codes that are easy 
to follow, including detailed and descriptive comments. A clear, well-written model code 
accompanied by meaninghl comments facilitates future reuse by its author and smooth 
implementation by other users. 
4.1.2 
Data and initial values 
For illustration, we use a mixed data format: a combination ofrectangular and list formatted 
data. In the list format, only the number of available seasons (i.e., the number of binomial 
experiments) is specified. The successes and the total attempts (yi and Ni) for each season 
are defined as vectors in the following rectangular data format: 
Alternatively, a single list format can be used: 
Initial values values must be specified for 571 and Rk ( k  = 2. :. . 8) using the following 
syntax: 
In this last syntax, initial values have been specified only for the first component of vector 
r and for all components of R except +he first one. No initial values can be specified for 
572, . . . .
~
8
 
and R1 since the they are'& .rrninistic and constant nodes, respectively. The 
value NA represents missing (unavailable) data. 
4.1.3 Compiling and running the model 
To compile and run the MCMC algorithm for 1000 burnin and 2000 iterations finally kept, 
we follow the steps below. 
1. Highlight the word model in the model code window. 
2. Open 
the 
model specification 
tool 
following 
the 
path 
Model> Specification. 
3. Click on check model box. 

128 
WINBUGS SOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
4. If the message model is s y n ta c tic a lly  correct appears in the left bottom bar 
of the WinBUGS window, then proceed to step 5 (data load). Otherwise, correct 
model code according to the indication given by WinBUGS (the cursor will indicate 
the position of the false code). 
5. Load the list format data by clicking on the list command and then press the load 
d a t a  button at the model specification tool. The message data loaded must appear 
if the data syntax is correct. 
6. Load the rectangular formatted data by clicking on the header row (i.e., the names 
of the columns - 
vectors) and then press the load d a t a  button at the model spec- 
ification tool. The message d a t a  loaded must appear (again) if the data syntax is 
correct. 
7. Click on the compile button of the model specification tool. If no error is found, 
then the message model compiled will appear in the bottom left of the window. 
Otherwise correct the error indicated by WinBUGS and rerun everything (from the 
beginning). 
8. Load initial values by highlighting the word list and then press the button load 
i n i t s  at the model specification tool. 
9. If the message model i n i t i a l i z e d  appears in the bottom left of the screen, then 
the algorithm is ready to run. 
10. Open the update tool by following the path ModeDUpdate . 
1 1. Select the number ofbumin iterations (updates), the refresh lag and thin, and press the 
update button (for this example we use updates=1000, refresh=100, thin=l). The 
index i t e r a t i o n  will begin counting the number of iterations. It will stop when the 
number of iterations specified in the updates text box is reached. 
12. Open the sample monitor tool in the path Inference>Samples . 
13. Set the parameters that we wish to monitor (vector ?r and components 2-8 of vector 
R). 
In the sample monitor tool, write in the node text box p i  (the name of ?r) and 
press the s e t  button. Repeat the same for components 2-8 of vector R by inserting 
the name R [2 : 81 in the node text box. Repeat the same procedure for all additional 
nodes and parameters that you wish to monitor. 
14. Open the online trace plots. In the sample monitor tool, type the star character 
(asterisk, *) in the node text box and press the t r a c e  button. A window with the 
online plots will appear. 
15. Return to the update tool and set the number of iterations equal to 2000. Then press 
the update button. The index i t e r a t i o n  will start (again), increasing until the 
desired number iterations is achieved. During the update the online plot will depict 
the generation of the monitored values. 

A COMPLETE EXAMPLE OF RUNNING MCMC IN WINBUGS FOR A SIMPLE MODEL 
129 
16. We can now use the sample monitor tool to obtain some initial output analysis or 
extract data in other statistical software. 
In order to get analysis for a single 
monitored parameter, we need to type 
the name of the corresponding Win- 
BUGSnode in the Node text box of the 
sample monitor tool and then press the 
option or button we wish to implement. 
The names of the monitored parameters 
can be also selected from the list of monitored nodes that is available at the right of 
the node text box. 
Ifwe wish to select specific components of a matrix 
or array, we can extract them using the I : J syntax. For 
example, R [2 : 81 will select components 2-8 of vector 
R .  
If we wish to implement the same analysis for all 
monitored parameters, then we type the asterisk (*) 
character in the node text box instead of the name of a 
specific node. 
4.1.4 MCMC output analysis and results 
4.1.4. I 
monitor tool in WinBUGS . 
We first produce trace plots for all monitoredparameters by typing the asterisk (*) in the 
node text box of the monitor tool. We then press the History button. The trace plot of 7r 1 
follows for illustration. 
Checking convergence. The following analysis can be obtainedusing sample 
No patterns or irregularities are observed, and therefore convergence can be assumed. (see 
sections 2.2.2.4 and 3.6 for details). 

130 
WINBUGSSOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
We then monitor autocorrelations by pressing the auto cor button. 
From this window, we observe that autocorrelations for 
all parameters become low only after considering a lag equal 
to 30. Thus, an independent sample can be obtained by re- 
running the algorithm with t h i n  set equal to 30 at the update 
tool. 
Numerical values of the calculated autocorrelations can be 
obtained by selecting the autocorrelation plot (by double- 
clicking on its picture) and then pressing the keyboard Con- 
trol key and the left mouse button. A window with all auto- 
correlation values of the selected parameter will appear on 
the screen. 
Then we monitor the evolution of selected quan- 
tiles using the quantiles button. This plot indi- 
cates that the requested quantiles have been stabi- 
lized, implying that the algorithm has converged in 
terms of T I .  Note that in this example we cannot use 
the option bgr diag since only a single chain was 
generated. 
Using the stats option, Monte Carlo errors can 
be calculated. They measure the variation of the mean of the parameter of interest due to the 
simulation. If MC errors are low in comparison to the corresponding estimated posterior 
standard deviations, then the estimated the posterior mean was estimated with high preci- 
sion. Increasing the number of iterations will decrease MC error. Here for parameter 7r 1 
we get MC error= 0.00116, while the posterior standard deviation is 0.0135 (for a sample 
of 2000 iterations after discarding an additional 1000 iterations as burnin), which is equal 
to the 8.6% of the posterior standard deviation. 
Increasing the number of iterations to 10,000 will considerably decrease MC error to 
0.00056 (48% decrease), while the posterior standard deviation will be relatively stable 
and equal to 0.0150. Note that the Monte Carlo variability of the remaining posterior sum- 
maries is minimal. 

A COMPLETE EXAMPLE OF RUNNING MCMC IN WINBUGSFOR A SIMPLE MODEL 
131 
Finally, the CODA button creates two windows for each chain. The first one is the output 
window (or file) with all the generated values that are stored sequentially for each mon- 
itored parameter with the corresponding iteration number. Hence the values generated 
for the first node will be stored first followed by the second ones and so on. This window 
must be stored as a text file with extension out to enable the user to import it directly in CODA. 
The second window refers to an index file with details 
related to the position of each node stored in the out file. 
Thus, the name of each node and the number of the start- 
ing and ending lines of the corresponding output are given 
in this window which must be stored as a text file with 
extension ind for direct use with the CODA package. 
Generally, transforming CODA-type data to usual R or 
Splus data frames is not difficult for a user of medium 
experience. Nevertheless, functions and routines are avail- 
able on the Web for converting CODA-type files inR/Splus da- 
ta frames directly. 
4.7.4.2 Calculation of posterior summaries. The main tool for the calculation 
of the posterior summaries in WinBUGS is the stats option, which provides estimates of 
the posterior mean, standard deviation, and quantiles (including the median) for the given 
generated sample. The total number of iterations (generated sample size) and the number 
of iterations that the generated sample started (hence the burnin period) are also provided. 
From these results we can infer that Kobe Bryant’s expected success field rate was equal 
to 46.7% for season 1999-2000. For the next two seasons (2000/01,2001/02), his success 
rate was about the same (posterior means for RP = 0.994 and R3 = 1.011). For seasons 

132 
WINBUGSSOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
2002103 and 2003104, his success rate was decreased by 4% and 3%, respectively (posterior 
expectations for Rd and R5 equal to 0.9615 and 0.9725). For the next season, the success 
rate was constant (posterior mean for R6 = 0.99), and finally for the last two seasons we 
observe increase of the success rate by 4% and 5%, respectively (posterior means of R 7 
and Rg equal to 1.038 and 1.051). 
We may also report the 95% posterior credible intervals for all Ri that quantify the 
relative performance of a current season in comparison to the previous one. All intervals lie 
around the reference value of one, which indicates a constant and stabilized performance 
of the player over the seasons studied. 
The density option provides a graphical representation of the posterior density estimate 
for each node. For parameter T 1 in the Kobe Bryant’s data, see Figure 4.1. 
Figure 4.1 
Density plot of Kobe Bryant’s success rate of field goals for the first season (TI). 
Additional analysis can be performed using the remaining available options of the 
inference menu. These menus are described in detail in Section 4.2. Moreover, us- 
ing the CODA option, the generated MCMC output can be exported in a text format and then 
imported in any statistical software for a more detailed output analysis. 
4.2 FURTHER OUTPUT ANALYSIS USING THE INFERENCE MENU 
In this section, details are provided concerning further output analysis within WinBUGS . 
We focus on the tools that are available in the inference menu while generating multiple 
chains, and the remaining operations available in WinBUGS are described in the sections 
that follow. 
All procedures are described using the example of Kobe Bryant’s 
field goal success rates presented in Section 4.1. 
The inference menu offers a variety of tools for a more de- 
tailed output analysis. In summary, the following tools are avail- 
able: 
Compare: This tool is used to compare graphically the elements 
of a monitored vector node. 
Correlations: This tool enables the user to obtain the corre- 
lation values and the scatterplots between two or more nodes. 
Summary: This tool provides summary statistics for a node. The results are similar to 
those obtained with stats option of the sample monitor tool. 

FURTHER OUTPUT ANALYSIS USING THE INFERENCE MENU 
133 
Rank: This tool provides useful analysis of the posterior distribution of ranks of the 
elements of a vector node. This tool is convenient when focus is on the evaluation of 
ranking of experimental units (organizations, teams, or subjects). 
DIC: This tool offers the possibility of calculating the deviance hformation criterion 
(DIC) (Spiegelhalter et al., 2002) used to compare competitive models. 
4.2.1 Comparison of nodes 
In order to compare the elements of a vector node, we need to open the comparison tool 
following the path Inf erence>Compare , 
We can plot the elements of a monitored vector by typing its name at the node text 
box and then pressing the box plot or the caterpillar buttons. The first will draw a 
boxplot for each node; the second one will draw horizontal lines or bars representing the 
95% credible intervals of each node. For graphical representation of the procedure, see 
Figures 4.2 and 4.3. 
Figure 4.2 
field goals) using boxplots. 
Comparison of posterior distributions of elements of vector R (relative performance in 
Boxplots produced in WinBUGS are slightly different from the traditional one. The 
limits of each box represent the posterior quartiles, while the middle bar the posterior mean 
(by default, there is an option that allows the user to depict the posterior medians instead). 
The ending of the whisker lincs represent the 2.5% and 97.5% posterior percentiles. The 
WinBUGS caterpillar plot essentially differs from the boxplot in two ways: (1) in the 
caterpillar plot, the box (referring to quartiles) is omitted, and ( 2 )  the bars are horizontal 
(parallel to the 5 axis). In both plots, the horizontal reference line represents the posterior 
mean estimated from all nodes depicted in the plot. 
The axis box can be used to define which values will be used in the x axis. The node 
used in this box must be of the same length as the corresponding node that defined the node 
box. It can be either stochastic or constant (e.g., the values of an explanatory variable). In 
cases where the axis node is stochastic, the posterior means are used as values for the 5 
axis. 

134 
WINBUGSSOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
Figure 4.3 
performance in field goals). 
Comparison of posterior 95% credible intervals of elements of vector R (relative 
To illustrate the last two options, we have also defined a new deterministic node called 
index simply indicating the year sequence (taking values from one to eight) using the 
command 
After compiling and running the new model, we activate the comparison tool. We can 
now plot the parameter pi (Kobe Bryant’s success rate) versus the time index by typing 
the corresponding names at the text boxes node and axis. Boxplot and caterpillar plots 
will not be affected by putting an additional node in the axis text box, but now the buttons 
model fit and scatterplot will also be activated. The first one creates a plot of the 
95% credible interval of pi (node argument) for each element with the corresponding IC 
axis values taken by index (axis argument). The 2.5% and 97.5% percentiles and mean 
points for successive values of index will be also connected in a plot that is equivalent to 
using a piecewise linear model. 

FURTHER OUTPUT ANALYSIS USING THE INFERENCE MENU 
135 
1.1 
1 0 -  
0.9 
0.8 
If we additionally type a node name in text box others, the values of this vector will be 
also printed on the plot. For example, using node Rin the ot her s  text box will create the 
following plot 
, _ - +  
+ ,  
_.+- 
A-- 
--. 
~ _ _ - * -  
--__c_- 
_ _ _ _ _  
_ _ . _ -  
- 
_-- 
, _ - -  
- - I  .. ~ _ _ _ _ _ _ _ _  
-
+
 
- 
- 
including the posterior means of node R . Note that the constant value of one for R [I] is 
also plotted. Generally the three arguments (node, other, axis) must be vectors of equal 
length. When stochastic nodes are used in other and axi s  then the posterior means are 
used. If we type R ,  R ,  and index in the text boxes node, other, and axis, respectively, 
we obtain the following plot: 
model fd R 
1 2  
. 
c _  
0.8 
00 
2 0  
40 
6 0  
8 0  
No posterior credible interval is plotted for R [ 11 since it refers to a constant node. If 
we eliminate node R [I by the others text box, then the corresponding points will not be 
plotted in the graph. 
model fit R 
1.2 
0 0  
20 
4 0  
60 
8 0  
The s c a t t e r p l o t  option provides a similar plot, with the values of node plotted against 
the values of axis. When stochastic nodes are used, then posterior means are used. Ad- 
ditionally, an exponentially weighted smoothed lined is fitted and plotted. Medians, bars, 
linear regression lines, and additional features can be plotted by changing the properties of 
the graph (click the right mouse button, then select p r oper t i es  and then special; for 

136 
WINBUGS SOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
more details, see Section 4.4); for examples of such plots, see Figure 4.4 . Finally, beg and 
end text boxes are used to specify the the beginning and the ending iterations used for the 
visual comparisons. 
Figure 4.4 
probabilities versus relative performance (pi vs. R )  for Example 1.4 (Kobe Bryant’s data). 
Scatterplots of(a) success probabilities versus time index (pi vs. index) and (b) success 
4.2.2 Calculation of correlations 
Correlations between the elements of a monitored vector node or between two monitored 
nodes can beobtainedusing the correlationtool(fol1ow the path Inference> Correlation: 
In the Correlation tool, the following fields and buttons and options are available: 
0 nodes text boxes: Here, the names of the nodes for which we wish to calculate 
posterior correlations are specified by the user. The second box can be left empty, 
provided that the node typed in the first one is a vector. In this case, correlations 
between all elements of the typed vector node are calculated. If two vector nodes are 
defined (one in each text box), then posterior correlations between the elements of 
the first and second nodes are calculated. 
0 beg and end text boxes: As usual, the range of the iterations that will be used for 
calculation of the posterior correlations can be defined here. 
0 s c a t t e r  button: This produces a scatterplot matrix between the elements of the 
requested nodes. 
0 matrix button: Produces a visual representation of the correlation matrix using 
different shades of black instead of the actual values. This simplifies comparisons of 
the correlations in multidimensional problems. 

FURTHER OUTPUT ANALYSIS USING THE INFERENCE MENU 
137 
0 print button: This provides the numerical estimates of the posterior correlations 
between the elements of the requested nodes. 
In Figure 4.5 we provide results of this tool for a single vector node (R; plots a-c) and 
for two vector nodes (Rand pi; plots d and e). 
Figure 4.5 
Results of the correlation tool using data of Example 1.4 (Kobe Bryant’s data). 
4.2.3 
Using the summary tool 
The summary monitor tool can be used independently from the sample monitor tool. 
We can open the corresponding dialog box by following the path Inf erence>Summary. 
Using this tool, summary statistics of the posterior distribution of a node can be obtained. It 
is equivalent to the stats option in sample monitor tool. The main difference from the 

138 
WINBUGS SOFTWARE: ILLUSTRATION, RESULTS, AN0 FURTHER ANALYSIS 
latter is that no values are stored here and hence less storage is required. This is convenient in 
high-dimensional problems where only some summary statistics may be needed for specific 
nodes. 
When we open the summary monitor tool, all four buttons are inactive. In order to 
activate them, we must define again which nodes we wish to monitor via this tool and then 
generate some additional iterations. Implementation of this procedure in Example 1.4 for 
p i  can be summarized by the following steps: 
0 Type p i  in the node text box and then press the s e t  button. 
0 Return to the update tool and generate additional values (e.g., 1000). 
Retype the node’s name (pi) and press the stats button. Then the following results 
will appear: 
As it is evident, the resulting output is the same as the corresponding one given by 
the stats option in the sample monitor tool. 
For p i  node, press the mean button. Then only the posterior means of this vector 
node will appear on a separate window. 
If the second step is skipped (i.e., the generation of additional iterations after set), then no 
action will be performed when pressing either the stats or the means button. 
4.2.4 
Evaluation and ranking of individuals 
In many problems there is interest in the evaluation and ranking of individual or units 
under study. Such problems typically arise in the evaluation of healthcare units (hospitals 
or medical centers), educational institutes (schools, colleges, universities, postgraduate 
programs), students, or athletic teams 

FURTHER OUTPUT ANALYSIS USING THE INFERENCE MENU 
139 
WinBUGS provides the facility to monitor some rank-related statistics on the basis of 
specific stochastic vector nodes that represent estimated or predicted performance. This 
can be done automatically using the rank monitor tool, which can be invoked following 
the path Inf erence>Rank. Using this tool, in each iteration of the MCMC algorithm the 
values of the vector node are rearranged in ascending order and the corresponding ranks 
are calculated and stored. Therefore, ranks indicate the sequence of the stochastic vector 
elements in ascending order; for example, rank 1 may indicate that this element had the 
lowest value. This is also an independent tool, where we need to specify separately which 
nodes will be monitored in terms of rank. The procedure (for Example 1.4) for evaluating 
Kobe Bryant’s performance using the ranks of node p i  can be summarized by the following 
steps. 
0 Type p i  in the node text box and then press the s e t  button (in rank monitor tool). 
0 Return to the update tool and generate additional values (e.g., 1000). 
0 Retype the node’s name (pi) and press the stats button. Then the following results 
will appear: 
As you can see, the median of the rank for each p ,  is given within the 2.5-97.5s 
percentiles. By this window we can infer which seasons were the best in terms of 
the player’s performance. For example, seasons 3 and 8 have the higher median rank 
(equal to 7 ) .  We can further infer that season 3 was the best since the 2.5% percentile 
is higher than the corresponding one for season 8 (3 vs. 2). In this way we can obtain 
an informal ordering of the player’s performance for each season. 

140 
WINBUGSSOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
0 The button histogram will provide a visual representation of the rank distribution 
for every node. Thus, using the histogram button, we obtain the following plots: 
0 Finally, the clear button deletes all simulated rank values for the selected node. If 
we wish to rerun some new rank values for the same node, then we have to repeat the 
procedure outlined above. 
Note that more detailed analysis of ranks can be obtained if we include corresponding 
code within the model definition; for details see the surgical example in Spiegelhalter et al. 
(1 996b, p. 1.8). In terms of storage, the rank monitor tool is more efficient since it reserves 
lower storage space than does the corresponding one, which calcul.ates ranks analytically 
within the model’s code. 
4.2.5 Calculation of deviance information criterion 
deviance 
as a measure of model comparison and adequacy. It is given by the expression 
The deviance information criterion (DIC) was introduced by Spiegelhalter et al. (2002) 
DIC(m) = 2D(Qm, m) - D(Gm: m) = D(Bm, m) + 2pm, 
where D(e,, m) is the usual deviance measure, which is equal to minus twice the log- 
likelihood 
D(e,,m) = -21ogf(yiem,m) 
and D(e,, m) is its posterior mean, p, 
can be interpreted as the number of “effective” 
parameters for model m given by 
pm = qe,, m) - D(Gm: m), 
and 8, is the posterior mean of the parameters involved in model rn. Smaller DIC values 
indicate a better-fitting model. DIC must be used with caution since it assumes that the 
posterior mean can be used as a “good” summary of central location for description of the 
posterior distribution. Problems have been reported in cases where posterior distributions 

MULTIPLE CHAINS 
141 
are not symmetric or unimodal. Details concerning DIC limitations and possible problem 
can be found in Spiegelhalter et al. (2003d, section entitled “Tricks: Advanced Use of 
the BUGS Language”). A detailed illustration of DIC can be found in Section 6.4.3; for 
additional details on the use of DIC and other methods for model comparison, see Chapter 
11. 
The DIG tool can be opened following the path Inf erence>DIC. The procedure for 
calculating DIC is similar to those for summary and ranks monitor tools. We first press 
the set button to start storing DIC values, then update the sampler and finally obtain the 
results by pressing the DIG button. 
4.3 MULTIPLE CHAINS 
4.3.1 
Generation of multiple chains 
The procedure described in Section 4.1.3 is followed when we wish to generate a single 
chain. If we wish to generate additional samples, then we follow steps 1-6 as in Section 
4.1.3. The remaining of the steps are slightly modified according to the following: 
I .  Set the number of chains you wish to gen- 
erate in the text box next to the compile 
button and then click on compile. For this 
example, three chains were used. 
8. Load initial values by highlighting the word 
list and then pressing the button load 
inits in the model specification tool. 
Repeat the procedure as many times as the number ofgenerated chains. For instance, 
for three chains we need to set three different sets of initial values. The generate 
button might be used for some chains but is recommended for specifying different 
starting values: with appropriate dispersion in order to ensure convergence. For Kobe 
Bryant’s data, we have used the following three sets of initial values. 
9-12. If the model is initialized, then update all chains by the given number of iterations 
(the same as in Section 4.1.3). Hence 1000 iterations means that each chain will be 
updated by 1000 iterations. 
13. Set the parameters to be monitored. By default, observations from all chains are 
monitored, but this can be modified by the chain. . .to. . . text boxes that are 

142 
WINBUGSSOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
present on the upper right of the sample monitor tool. These text boxes can be 
used to extract posterior summaries and output analysis only for selected chains. 
14. Open the online trace plots. Now one line 
for each chain is outlined on the dynamic 
trace plot. 
15. Update the sampler by 2000 iterations. 
16. The sample monitor tool can now be 
used to obtain initial output analysis and 
export data to other statistical programs. 
4.3.2 Output analysis 
In the output analysis we observe the following differences: 
0 In all plots (autocorrelations, trace, history, quantiles), results for all chains are vi- 
sually separately in the same graph. For example, in the h i s t o r y  plot one line for 
each chain is printed; see Figure 4.6 for the trace plot resulting from the model of 
Kobe Bryant’s data. Only in the density plot, a single estimate of the posterior 
distribution is calculated (and plotted) using values from all chains. 
(b) Quantiles plot 
(c) Autocorrelations plot 
(d) Density plot 
Figure 4.6 
WinBUGSplots when generating three chains in model for Kobe Bryant’s data. 
0 In the stats option, the posterior summaries using values from all requested chains 
are calculated. For an update of 2000 iterations using three chains, we obtain the 
following results. 

MULTIPLE CHAINS 
143 
In this output, 6000 iterations in total were used to calculate the posterior summaries 
(2000 from each of three generated samples). 
0 In the coda option, one output windowlfile with the generated values of each chain 
is obtained. The index windowifile is common for all chains; see Figure 4.7 
Figure 4.7 
CODA output when generating three chains in model for Kobe Braynt’s data. 
0 The Gelman-Rubin diagnostic (Gelman and Rubin, 1992) is now available via the 
bgr diag option (see Section 4.3.3 for details). 
4.3.3 The Gelman-Rubin convergence diagnostic 
The Gelman-Rubin diagnostic (Gelman and Rubin, 1992) of is available in WinBUGS via 
the bgr diag option, when multiple chains are generated in parallel, each one starting from 
different initial values. Then an ANOVA-type diagnostic test is implemented by calculating 
and comparing the between-sample and the within-sample variability (i.e., intersample and 
intrasample variability). The statistic R can be estimated by 
A 
V 
T’- 1 
BSS/T’n+l 
Rz- 
- 
- -+-- 
WSS 
TI 
wss 
n 

144 
WINBUGS SOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
where K is the number of generated samplesichains, T’ is the number of iterations kept in 
each samplekhain, BSS/T’ is the variance of the posterior mean values over all generated 
samplesichains (between-sample variance), WSS is the mean of the variances within each 
sample (within-sample variability), and 
A 
T’-1 
B S S R + l  
T‘ 
T 
V=- 
wss+ 77 
is the pooled posterior variance estimate. When convergence is achieved and the size of 
the generated data is large, then R -+ 1. Brooks and Gelman (1998) adopted a corrected 
version of this statistic given by 
- d + 3 R  
c - -  d + l  
where d is the estimated degrees of freedom for the pooled posterior variance estimate V ;  
for more details see Brooks and Gelman (1998, sec. 1.3). 
Using a line plot, the bgr diag option available in the sample monitor tool plots 
the evolution of the pooled posterior variance 1; (in green color), average within-sample 
variance WSS (in blue), and their ratio R. The dashed line denotes the reference value of 
one. Note that in WinBUGS the estimates are based on the ranges of the 80% posterior 
credible intervals. These quantities are calculated every 50 iterations. 
We generally expect R to be higher than one at the initial stage of the algorithm, provided 
that the starting points are suitably scattered over the parameter space. As the number of 
iterations increase, R tends to one and V and WSS will stabilize, indicating the convergence 
of the algorithm. 
Numerical estimates can be obtained by double clicking the left mouse button on the 
plot (opening the figure) and then pressing the left mouse and the keyboard Control buttons 
to open the window with the associated information. 
Normalized values are used so that all lines can be plotted in the same figure. These 
normalized values are calculated by dividing the original V and WSS by their maximum 
value (i.e., rnax{V.: WSS)). 

CHANGING THE PROPERTIES OF A FIGURE 
145 
4.4 CHANGING THE PROPERTIES OF A FIGURE 
4.4.1 General graphical options 
Options of each graph can be changed by following 
the steps below: 
1. Select the graph (left mouse click). 
2. Right mouse click to open the drop-down 
menu. 
3. Select properties in the menu. 
Afterthis procedure, the plot Properties menuopenswithfivedifferenttabs: margins, 
axis bounds, titles, all plots, and fonts. 
The margins, axis bounds, and titles tabs are straightforward to use. The fonts tab 
can be used to change the font family and size in titles and in the axes. Finally, by pressing 
the all plots tab, the user can apply the settings of the current plot (all ofthem or specific 
ones) to all graphs in the WinBUGS working window. 
Two buttons are also available in the plot properties menu: apply and special. 
The first one executes a requested change on the plot properties while the latter generates 
special figure properties that vary according to the type of the plot. In simple plots, this 
option is inactive since no special options are available. Special options of specific plots 
are illustrated below. 
4.4.2 
Special graphical options 
Special graphical options are available for the density plot of the inference tool, and for 
boxplots, caterpillar, model fit, and scatterplot in the compare tool. 
For the density plot, the only available special option is the smooth- 
ing parameter of the plot (default value = 0.2). Increasing the value 
of this parameter decreases the smoothing producing a higher num- 
ber of spikes on the plot, while decreasing this value increases the 
smoothing; see example in Figure 4.8 
From this menu, we can 
The available special properties in boxplots allow for several modifications in the graph. 
0 Eliminate the reference line or change its value (the default is the mean over all 
components). 

146 
WINBUGS SOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
(a) Smoothing parameter = 0.1 
(b) Smoothing parameter = 0.4 
Figure 4.8 Density plots using different smoothing parameters. 
0 Eliminate the boxplot labels. 
0 Depict the posterior mean (default) or the posterior median using the middle line of 
the boxplot. 
0 Order the boxplots according to their posterior meadmedian from the lowest (left) 
to the highest (right). 
0 Plot the boxplots parallel to either the y axis (vertical plot) or the 2 axis (horizontal 
plot). 
0 Use the logarithmic scale. 
0 Change the color of the boxes. 
see Figure 4.9, for a graphical description of the menu. 
plot special properties. 
to the use of logarithmic scale on 2 andor y axis. 
Special properties in caterpillar plots are similar to the box- 
The only available model f i t  special options are related 
Finally the scatterplot special properties allow us to 
0 Plot the posterior means or medians as points on the graph. 
0 Change the color of the points. 
Draw or eliminate the fitted line. 
0 Select the color of the fitted line. 
0 Select the type of fitted line (linear or smoothed) and their corresponding parameter 
values. 
0 Use logarithmic scale on 5 andor y axis. 
0 Plot credible intervals (for means of the variable plotted on y axis) as bars around the 
points. 
See also Figure 4.10 for a description of the scatterplot special menu. 

CHANGING THE PROPERTIES OF A FIGURE 
147 
Figure 4.9 
Special properties of WinBUGS boxplots. 
Figure 4.10 Special properties of WinBUGS scatterplots. 

148 
WINBUGSSOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
4.5 OTHER TOOLS AND MENUS 
4.5.1 The node info tool 
Information concerning the current state of a node can be extracted using the node i nf o 
tool (follow the path Inf o>Node inf 0). 
Using this tool, the current values of a node and the update method used for stochastic 
nodes is provided by the corresponding buttons. For example, for Kobe Bryant’s model, 
the current state for node p i  (after 3000 iterations) as follows: 
while the slice Gibbs method is used to update T I  and Rz, . . . , Rs nodes according to the 
information we obtain using the method option. 
4.5.2 Monitoring the acceptance rate of the Metropolis-Hastings algorithm 
The monitor Metropolis tool is activated only when the Metropolis method is used 
by WinBUGS . It can be opened following the path ModeDMonitor Met. It calculates 
statistics (minimum, maximum, and average) related to the acceptance rate of the random- 
walk Metropolis during the adaptive phase used by WinBUGS to tune the variance of the 
symmetric normal proposal. The adaptive phase is set by default equal to 4000 iterations, 
and all acceptance rates are calculated for batches of 100 iterations. The aim of the adaptive 
phase is to achieve an acceptance rate between 20% and 40%; for details, see Spiegelhalter 
et al. (2003d, pp. 6, 25). 

SUMMARY AND CONCLUDING REMARKS 
149 
4.5.3 Saving the current state of the chain 
Frequently, we get an initial sample from the MCMC algorithm and check whether it has 
converged. If there is an indication of nonconvergence, then we run the algorithm for 
an additional number of iterations. This is equivalent to starting the algorithm using as 
initial values the ones from the last iteration of the previous run. On some occasions, it 
is convenient to save the current state of the chain in case we wish to generate additional 
iterations in the future. This can be achieved in WinBUGS by the ModeDSave State 
path. The output is given in a list format and hence can be used directly as initial values 
for the next run of the model. For the example of Koby Bryant’s data, we get the following 
output: 
4.5.4 Setting the starting seed number 
All simulation algorithms that generate a sequence of 
pseudorandom numbers are called random generators. 
They require a starting number that is used to generate 
a sequence of pseudorandom numbers. Every starting 
seed generates a single sequence of random numbers. 
This property is usef’ul if we wish to reproduce exactly 
the same results multiple times. This starting seed can 
be set in WinBUGS following the path ModeDSeed. 
4.5.5 Running the model as a script 
As we have already seen, in order to run a model, we need to repeat a number of ac- 
tions (check model, define data, compile model, etc.). This procedure is laborious and 
time-consuming, especially when various different models are fitted. For this reason, Win- 
BUGS provides the facility to write a script code that generates all the actions required to 
simulate and monitor posterior values. This is equivalent to the backbugs command in the 
classic version of BUGS for DOS and UNIX. More details on this issue are provided in 
Appendix B. 
4.6 
SUMMARY AND CONCLUDING REMARKS 
In this chapter, we have illustrated both basic and more specialized functions of Win- 
BUGS using a simple example based on Kobe Bryant’s success rate in field goals. After 
this chapter, the reader must be able to construct simple models in WinBUGS , produce 
more specialized analysis (including boxplots of the posterior densities, analysis of ranks, 

150 
WINBUGSSOFTWARE: ILLUSTRATION, RESULTS, AND FURTHER ANALYSIS 
correlations, and calculation of DIC) and generate multiple chains and must be able to 
change the technical details and options of WinBUGS plots. Finally, brief details of the 
remaining tools and menus are also provided. 
Details about the construction of models using directed acyclic graphs and illustration 
of running WinBUGS models using scripts (instead of menus) are briefly discussed in this 
chapter. Further details can be found in Appendices A and B, respectively. 
In the chapter that follows, the normal linear regression model is introduced and illus- 
trated in detail. 
Problems 
4.1 
Following the guidelines presented in Section 4.1, perform the same analysis in Win- 
BUGS for the following survival times 
assuming 
a) Weibull distribution 
b) Gamma distribution 
c) Log-normal distribution 
Compare the models of Problem 4.1 using the DIC. Which model is preferable for 
those data? 
For each model of Problem 4.1, run five chains simultaneously and calculate the 
Gelman-Rubin convergence diagnostic. 
From WinBUGS examples, volume 1 (Spiegelhalter et al., 2003a), run the example 
Surgical. 
a) Produce a sample of 2000 iterations after discarding an additional 500 iterations 
b) Check for the convergence of the chain. 
c) Analyze the MCMC output and make inferences concerning the model parame- 
d) Produce an a posteriori analysis of the hospital ranking (using the rank tool). 
e) Compare the posterior distributions of the mortality rates of all hospitals, using 
For the data and implemented models of Problem 3.5 
a) Use the compare tool to produce scatterplots of the sampled values and estimate 
the posterior correlations between the parameters of interest. 
b) Run the MCMC chain for 5000 iterations plus 1000 burnin iterations. Save the 
current state of the algorithm and use these values to rerun the sampler using these 
values as initial values (no bumin is needed in the new sample). 
c) Follow the instructions in the WinBUGS manual (Spiegelhalter et al., 20034 and 
in Appendix B and rerun your MCMC in the scriptibackground mode. 
51 3 17 13 5 4 17 1 5 3 8 22 0 1 13 8 15 3 1 13 
4.2 
4.3 
4.4 
as bumin. 
ters. 
the compare tool. 
4.5 

CHAPTER 5 
INTRODUCTION TO BAYESIAN MODELS: 
NORMAL MODELS 
5.1 GENERAL MODELING PRINCIPLES 
Statistical models are nowadays used to describe parsimoniously real life problems observed 
under uncertainty. A statistical model is a collection of probabilistic statements (and equa- 
tions) that describe and interpret present behavior or predict future performance. It consists 
of three important components: the response variable (or variables) Y, 
the explanatory 
variables XI Xp, 
. . . X,, 
and a linking mechanism between the two sets of variables. 
The response variables Y are the main study variables, and they represent the stochastic 
part ofthe model. By the term stochastic we refer to random variables whose outcome is 
uncertain before it is observed. Concerning these variables, we are frequently interested in 
describing the mechanism underlying or leading to the appearance of a certain outcome of Y 
and predict a future outcome of Y. 
Since the response variable is the stochastic component 
of the model, we can write 
Y / X ~ ; X ~ , .  
. . ,xp q e )  
where D(0) is a distribution with parameter vector 8. For example, for normal regression 
models, the response (stochastic component of the model) is written as 
YIX1,Xp;. 
. . .x, 
N N(p,a2), 
where N(pi a2) is the normal distribution with mean p and variance a2. Models with one 
response variable are called univariate, while models with more than one response variables 
are called multivariate. In this book we focus on univariate models. 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
151 

152 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
As explanatory variables X I ,  . . . , X,, we consider all variables that potentially influ- 
ence the response variable Y. 
Inference concerning the significance, the type (negative or 
positive), and the magnitude ofthe effect of each X, 
on Y is the main focus in such models. 
Usually X, 
are considered as fixed, nonstochastic components, that is, deterministic nodes 
in WinBUGS . Hence, it is more precise to define the distribution of Y conditional on the 
observed explanatory variables 
Parameter vector 8 is expressed as a function of the explanatory variables and a new alter- 
native set ofparameters (@, @) that substitutes the original ones in terms of estimation and 
inference. Concerning the new set of parameters, vector @ summarizes the association be- 
tween the response and the explanatory variables, while @ refers to other characteristics of 
the distribution such as the variance or the shape. Usually, the mean of the response model 
is associated with the response variables, but in more complicated models, the variance or 
other moment functions can also be estimated via the explanatory variables. The function 
used to connect the stochastic and the deterministic part of the model (variables Y and X ,) 
can be referred to as the “generalized linking” function. The terminology and principles 
described above were originally introduced to define generalized linear models (McCullagh 
and Nelder, 1989), but they can be adopted for a wide range of models. For example, the 
simpler case of the above mentioned general linking function is to express the mean of the 
response variable given the explanatory variables as a function of the linear combination of 
the explanatory variables; hence we can write 
In this equation, the linear combination of of all explanatory variables is used to predict the 
expected value of the response variable Y and is often called the linearpredictor 7 of the 
model. This setup is introduced in generalized linear models and, within this context, g(p) 
is referred as the link function. 
To complete the formulation presented above, a prior distribution must be defined for 
the parameters under estimation. Thus, in this formulation a prior distribution f(@, 4) for 
the parameters (@, 4) remains to be specified. 
Finally, explanatory variables are usually defined as deterministic (fixed) quantities. In 
practice these variables are frequently random. Within the general framework of statistics 
it is generally easy to extend the model by either simply considering X ,  terms as random 
variables with additional parameters under estimation; see Ryan (1 997, pp. 34-35) for a dis- 
cussion. This can be specified in a straightforward manner within the Bayesian framework 
using additional hierarchical levels in our model (see Chapter 9 for details). 
5.2 MODEL SPECIFICATION IN NORMAL REGRESSION MODELS 
Normal regression models are the most popular models in statistical science. They are 
based on the initial work of Sir Francis Galton in the late years of the 19th century (Stanton, 
2001). In normal regression models, the response variable Y is considered to be a continuous 
random variable defined in the whole set of real numbers following the normal distribution 

MODEL SPECIFICATION IN NORMAL REGRESSION MODELS 
153 
with mean p and variance u2. Therefore, the model can be summarized by the following 
equations 
YIX1 ... . .x, N N(p(/3:X1.. . . .Xp):u’) 
(5.1) 
with 
p(P, x1, . , . . X,) 
= 0 0  + PlXl + ’ ’ ‘ + fiPXP 
(5.2) 
where u’ and /3 = ($0, 
. . . .13p)T is the set of regression parameters under estimation. 
Frequently, the following alternative representation of the regression model is adopted 
Y = 00 + 31x1 + . . . + ppxp + E :  E N N(O,u2) 
Although this formulation has a nice interpretation since the response variable is directly 
expressed as a function of the explanatory variables plus arandomnormal error with variance 
u2, the initial model expression given by (5.1) and (5.2) is more general and follows the 
model building principles described in Section 5.1. 
In order to simplify notation and make it compatible with the corresponding Win- 
BUGS notation, we hereafter remove the condition on the explanatory variables. Hence we 
denoteYIX1.. . . .X,simplybyYandthecorrespondingexpectedvalueE(YIXl,. . . ,X2) 
by E ( Y )  or p. 
5.2.1 
Specifying the likelihood 
Let us observe a sample of size n with response values y = (y1, . . . , Y,)~ and z,~, . . . . zzp, 
the values of the explanatory variables XI, . . . , X, for individuals z = 1. . . . , n. Then the 
model is expressed as 
(5.3) 
Y, 
N(PL,.02) 
p, = PO + &zZ1 + . . . + 13,xZp 
for i = 1,. . . . n . 
Within WinBUGS the normal distribution is defined in terms of its precision 7 = 
Hence the above likelihood in WinBUGS will be written as commands 
The last two commands are used to deterministically specify the connection between the 
variance, the standard deviation, and the precision parameter 7 = u - 2  used by the normal 
distribution in WinBUGS . Moreover, in this code all parameters Pj are defined separately 
as single scalar nodes, while explanatory variables are specified as vector nodes with names 
XI, . . . , xp of length n. When monitoring these parameters, each one of them must be 
set separately in the sample monitor tool of WinBUGS . 

154 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
5.2.2 Specifying a simple independent prior distribution 
In normal regression models, the simplest approach is to assume that all parameters are a 
priori independent having the structure 
The gamma prior of the precision parameter induces prior mean and variance given by 
a 
a 
b 
b2 
E(T) = - and Var(r) = -, 
respectively. In this prior setup, we have substituted the variance ~7 by the corresponding 
precision parameter T in order to make it compatible to the WinBUGS notation. The gamma 
prior used for T corresponds to an inverse gamma prior distribution for the original variance 
parameter with prior mean and variance given by 
b2 
and Var(a2) = 
b 
( a  - 1)2(a - 2) ’ 
E(O2) = a-l 
respectively. 
When no information is available, a usual choice for the prior mean is the zero value 
(pp3 = 0). This prior choice centers our prior beliefs around zero, which corresponds to 
the assumption of no effect of X ,  on Y .  In this way, we express our prior doubts about 
the effect of X ,  on Y, 
prompting Spiegelhalter et al. (2004, pp. 90, 158-160) to call this 
a “sceptical” prior. The prior variance c: of the effect P, is set equal to a large value (e.g., 
lo4) to represent high uncertainty or prior ignorance. Similarly, for T we use equal low 
prior parameter values, setting in this way its prior mean equal to one and its prior variance 
large. For example, we may use a = b = 0.01 which results in E(T) = 1 and V ( T )  = 100. 
This approach is also adopted in all illustrations of the WinBUGS manual and example 
volumes. More details concerning the specification of more complicated prior distributions 
are provided in Sections 5.3.2 and 5.3.3. 
Within WinBUGS , the prior setup described above can be incorporated by simply adding 
In this syntax, value 1.OE-4 is the scientific notation for 1.0 x lop4 = 0.001, which is the 
prior precision of each ,!?j and corresponds to prior variance equal to lo4. The definition 
above can be considerably simplified by using vectors instead of single nodes; see Section 
5.3.3 for details. 
5.2.3 Interpretation of the regression coefficients 
Each regression coefficient pertains to the effect of explanatory variable X j  on the ex- 
pectation of the response variable Y adjusted for the remaining covariates. The inference 
concerning the model parameter can be divided into three basic stages: 

MODEL SPECIFICATION IN NORMAL REGRESSION MODELS 
155 
1. Is the effect of X ,  important for the prediction or description of Y? 
2. What is the association between Y and X ,  (positive, negative, or other)? 
3. What is the magnitude of the effect of X ,  on Y ? 
Concerning query 1, we initially focus on examining whether the posterior distribution of 
Ll, is scattered around zero (or not). Posterior distributions far away from the zero value 
will indicate an important contribution of X ,  on the prediction of the response variable. 
Although formal Bayesian hypothesis testing is not based on simply examining the posterior 
distribution and their credible intervals, such analysis can offer a first and reliable tool for 
tracing important variables. 
In stage (query) 2, we identify whether the relationship is positive or negative. This 
can be based on the signs of the posterior summaries of central and relative location (e.g., 
mean, median, 2.5% and 97.5% percentiles). If all of them are positive or negative, then 
the corresponding association can be concluded. Positive association means that changes 
of the explanatory variable X ,  cause changes of the same direction for variable Y while 
negative association means that changes of the explanatory variable X ,  cause changes of 
the opposite direction for variable Y. 
Within this analysis, we can a posteriori calculate the 
posterior probability: 
When the zero value lies at the center of the posterior distribution, then the value shown 
above will be close to 
indicating that there is no clear positive or negative effect of X ,  on 
Y. 
When TO is low (e.g., lower than 2.5%, 1%, or 0.5%), then we may conclude positive 
or negative association depending on the sign of the posterior location summaries. Within 
WinBUGS we can calculate the posterior probability f(Pj > Oly) using the syntax 
which creates a binary node p . betaj taking values equal to one when p, is positive and 
zero otherwise. Obtaining the posterior mean via the sample monitor tool provides us the 
estimate of the posterior probability f(P, > Oly). 
In WinBUGS , it is also convenient to calculate the deviance information criterion (DIC) 
(Spiegelhalter et al., 2002) to compare models with different covariates and, in this way, 
evaluate their importance concerning their effect on Y. 
A brief description of DIC as well 
as illustration of its calculation in WinBUGS is provided in Section 4.2.5. In order to use 
DIC in regression models, we need to fit models including and excluding the variable of 
interest and then select the one with the lower value of DIC. In the case of a large number 
of covariates, this procedure can be quite tedious since a large number of models must be 
fitted before drawing conclusions regarding the model with the lowest value of DIC. More 
formal approaches concerning model checking, comparison, and selection are described in 
Chapters 10 and 11. 
Finally, the magnitude of the effect of variable X ,  on Y is given by the posterior distri- 
bution of D, (for j = 1. . . . . p )  since 

156 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
Hence the posterior mean or median of /3, will correspond to the corresponding posterior 
measures of the expected change of the response variable Y. 
Hence, an increase of one 
unit of X,, given that the remaining covariates will remain stable, induces an a posteriori 
average change on the expectation of Y equal to the posterior mean of S 3 ;  see Table 5.1 for 
further details. 
Table 5.1 Summary interpretation table for regression coefficients p., 
Concerning the constant parameter Bo, its interpretation corresponds to the expected 
value of the response variable Y when the observed values of all covariates are equal to 
zero. Frequently such combination lies outside the range of the observed covariate values. 
In such cases, the interpretation of 00 is not reliable since we infer or predict the behavior 
of Y for values of X, that have not been observed. Frequently, direct interpretation of 90 
does not lead to realistic and sensible interpretation. An alternative is to center around zero 
all explanatory variables X, by subtracting their sample mean. In this case, the constant 
3’6 represents the expected value of Y when all covariates are equal to its sample means, 
representing in this way the expected response Y for an “average” or “typical” subject 
according to our sample. In WinBUGS this quantity can be directly estimated using the 
command 
without changing the parametrization of the original model. The approach described above 
can also be used to calculate the expected values of Y for any combination of values of X j .  
Parameter precision T (and the variance a2) indicates the precision of the model. If 
the precision T is high (a2 low), then the model can accurately predict (or describe) the 
expected values of Y. 
Therefore, we can rescale this quantity using the sample variance of 
the response variable Y, 
namely, s$, using the R; statistic given by 
2 
7 - 1  
ff2 
4 
S$’ 
R, = 1- - 
= 1 - - 
where s$ is the sample variance of Y. 
This quantity can be interpreted as the proportional 
reduction of uncertainty concerning the response variable Y achieved by incorporating the 

MODEL SPECIFICATION IN NORMAL REGRESSION MODELS 
157 
explanatory variables X j  in the model. Moreover, it can be regarded as the Bayesian analog 
of the adjusted coefficient of determination Ridj (used in the frequentistic approach of the 
normal regression model), given by 
where 
where fiJ are the maximum likelihood estimates of PI. 
In order to calculate R i  in WinBUGS , we can use the commands 
or directly using the precision parameter T and the syntax 
5.2.4 A regression example using WinBUGS 
Example 5.1. Soft drink delivery times. The following example deals with the 
quality of the delivery system network of a soft drink company; see example 4.1 in 
Montgomery and Peck (1992). In this problem, we are interested in estimation of 
the required time needed by each employee to refill an automatic vending machine 
owned and served by the company. For this reason, a small quality assurance study 
was set up by an industrial engineer of the company. As the response variable, the 
engineer considered the total service time (measured in minutes) of each machine, 
including its stocking with beverage products and any required maintenance or 
housekeeping. After examining the problem, the industrial engineer recommended 
two important variables that affect delivery time: the number of cases of stocked 
products and the distance walked by the employee (measured in feet). A dataset 
of 25 observations was finally collected. This dataset is reproduced in the book’s 
Website with permission of John Wiley and Sons, Inc. 
Setting up the data and the model code. Following the approach described in Sections 
5.2.1 and 5.2.2, we define the data in either a rectangular or a list format. The rectangular 
format of the data are provided in Table 5.2, while the full model code of the example, 
including the list data format and the initial values, is given in Table 5.3. All three variables 
used in the model (time, cases, distance) are defined as separate vectors in the list data format, 
while in the initial values, each parameter T ,  PO, PI, and P 2  was initialized separately. 

158 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
Table 5.2 
5.1) 
Rectangular WinBUGS format of “soft drink delivery times” example (Example 

MODEL SPECIFICATION IN NORMAL REGRESSION MODELS 
159 
Table 5.3 Full model code for “soft drink delivery times” example (Example 5.1) 

160 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
Results. Posterior summaries and densities, after running the MCMC algorithm for 3000 
iterations and discarding the initial 1000 ones, are provided in Table 5.4 and Figure 5.1, re- 
spectively. Descriptive analysis of the posterior distribution of R g indicates a considerable 
improvement of the precision (posterior mean equal to 0.95) in the prediction of delivery 
times when including in the model covariates cases and distance. 
Table 5.4 
additional discarded 1000 burnin iterations 
WinBUGS posterior summaries for Example 5.1 after 2000 iterations and 
node 
mean 
sd 
MC error 2.5% 
median 97.5% start sample 
R2B 
0.9511 0.01743 5.118E-4 0.9064 0.9548 0.9734 1001 2000 
beta0 
2.356 
1.188 
0.03076 -0.03996 2.372 4.635 1001 2000 
beta1 
1.61 
0.1806 0.003737 1.272 
1.609 1.968 1001 2000 
beta2 
0.01447 0.003812 8.476E-5 0.006872 0.01446 0.02211 1001 2000 
p.beta0 
0.974 0.1591 0.004037 0.0 
1.0 
1.0 
1001 2000 
p.beta1 
1.0 
0.0 
2.236E-12 1.0 
1.0 
1.0 
1001 2000 
p.beta2 
1.0 
0.0 
2.236E-12 1.0 
1.0 
1.0 
1001 2000 
S 
3.386 0.5695 0.0168 
2.531 
3.302 4.749 1001 2000 
typica1.y 22.38 
0.683 
0.01701 21.09 
22.37 23.78 
1001 2000 
Figure 5.1 
Posterior densities of model parameters for Example 5.1 (soft drink delivery times). 
Concerning the posterior distribution of 0, 
we observe that with the current model we 
can predict the expected delivery time with with an a posteriori expectation equal to 3.4 
minutes. 
Considering as point estimates the posterior means, we end up with model 
Expected time = 2.36 + 1.6 x cases + 0.015 x distance. 

USING VECTORS AND MULTIVARIATE PRIORS IN NORMAL REGRESSION MODELS 
161 
Minor changes are observed in the regression equation if posterior medians are used as 
point estimates instead. 
Observing all parameters, we can infer that the effect of both explanatory variables 
(cases and distance) have an important contribution to the prediction of delivery time. All 
summary statistics and the posterior densities indicate that zero is far away from the posterior 
distribution with posterior probability of having positive association between each X 
j and 
Y equal to one. 
Furthermore, for each additional case stocked by the employee, the expected delivery 
time is a posteriori expected to increase by 1.6 minutes (96 seconds). The increase in 
expected delivery time for each additional case, lies between 1.3 and 2.0 minutes (76 and 
118 seconds) with probability 95%. For every increase of the walking distance by one 
foot, the delivery time is a posteriori expected to increase by 0.87 seconds, while every 100 
feet of additional walking distance increases by 1.5 minute the posterior expected delivery 
time (ranging between 0.7 and 2.2 minutes with probability 95%). In terms of meters, for 
every 100 m of walking distance the expected delivery time will increase by 4.7 minutes 
on posterior average (one foot is equal to 0.3048 m, resulting in an increase of the expected 
delivery time by 100/0.3048 * 0.01447 = 4.747 minutes). 
Parameter ,OO has no sensible interpretation in this example since the zero value is non- 
sense for both explanatory variables (the delivery employee will always have to stock some 
cases of products in the machine and walk at least a small distance to reach the delivery 
location). For this reason, no interpretation ofthis parameter is attempted. We only observe 
that the zero value lies at the left tail ofthe posterior distribution within the range of the 95% 
posterior interval. Moreover, the posterior probability of positive PO is equal to 97.4%. 
Since the interpretation for PO is meaningless, we can focus on the predicted value for 
the a typical or representative delivery route. According to the posterior summaries of node 
typical. y, a typical delivery route will take 22.4 minutes on average and will range from 
21.1 to 23.8 minutes with probability 95%. 
5.3 USING VECTORS AND MULTIVARIATE PRIORS IN NORMAL 
REGRESSION MODELS 
5.3.1 
The regression model described above can be defined in WinBUGS using vectors and ma- 
trices instead of scalar nodes. To achieve that, within the likelihood loop, we substitute the 
mean specification code line with the following syntax: 
Defining the model using matrices 
Matrix z (denoted by x[,l in WinBUGS) is a n x p matrix. Each column x3 of z 
corresponds to each explanatory variable X,, while each row z(%) 
corresponds to the ex- 
planatory variable values of the ith subject of the sample. Concerning the data definition, 
each column of the matrix can be defined using the rectangular data format with header 
x [ ,11 . . . x [ , PI. Otherwise, in list format, x must be defined as an array with dimen- 
sions n and p. 
Moreover, when the number of variables p is large, it is convenient to use the command 
inprod( b 13 , x [i ,I ) to express the linear combination of the explanatory variables 
Cy=, 3,xt3. Hence, the mean can be defined by the syntax 

162 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
where b [I is vector 
z] 
of dimension n x 
( p  + l), which is called the data matri-x; where 1, is a vector of length n with all elements 
equal to one corresponding to the “constant” term. The linear predictor in (5.3) can be now 
written as pi = ,BX(i), where X (i) is the ith row of X .  This expression can be coded in 
WinBUGS by 
= (pl, . . .,B,). 
Usually we incorporate the constant term in a matrix X = [l 
In this syntax, X [,I is a matrix of dimension n x ( p  + 1) with all components of the 
first column x [, 11 equal to one. Using this definition, beta [l] represents the constant 
coefficient $0, X [, jl refers to the vector of values of Xj-l, 
and beta[j] corresponds 
to coefficient 133-1 for j = 1. . . . , p + 1. In the rectangular format, data are defined 
in the usual manner with header names referring to columns from 2 to p + 1, that is, 
X [, 21 . . . X [ ,p+ll . This approach enables us to monitor all regression coefficients 
simultaneously by simply considering the vector node beta. 
5.3.2 Prior distributions for normal regression models 
Conjugate analysis for the normal regression model has been presented in Section 1.5.5. 
The conjugate prior for the normal regression model is considered if we specify [pi o 2] to 
a priori follow a normal-inverse gamma distribution. Hence we can write 
- n’p(p3> c2V02) and o2 N IG(a, b) . 
(5.5) 
where c2 is a parameter controlling the overall magnitude of the prior variance. 
popular Zellner (1986) g-prior, in which 
A special case of the family of prior distributions described above is the popular is 
v = (xTx)-1. 
Parameter c2 was denoted by g in Zellner’s original publication. The default chpice of 
c2 = n is usually adopted when no information is available since it has an interpretation 
of adding prior information equivalent to one data point (Kass and Wasserman, 1995; 
Fouskakis et al., 2008). This prior has been widely used because it considerably simplifies 
posterior computations and reduces the number of prior variance parameters that remain to 
be specified down to one. 
In the case where no prior information is available, we may simplify the prior by con- 
sidering independent normal distributions by setting 
v = c21p 
with P = p + 1 and c2 set large to express prior ignorance (e.g., c = 100). Hence we can 
simply rewrite the prior as 
(5.6) 
2
2
 
3J102 N 
iv(pO3. c o ) forj = 0. I,. . . , p ,  
where psJ are the components of the prior mean vector p s. 
Generally, the conjugate prior setup described above is very convenient for implementing 
Bayesian variable selection (Raftery et al., 1997). Moreover, Zellner’s g-priors were widely 
used within this context since they allow us for a sensible default choice ofprior distributions; 

USING VECTORS AND MULTIVARIATE PRIORS IN NORMAL REGRESSION MODELS 
163 
see Fernandez et al. (2000) for comparison between different values of c and Liang et al. 
(2008) for discussion and extensions concerning the g-priors. 
Another alternative is to consider the simpler prior setup of Section 5.2.2, where all 
parameters are a priori independent. This choice is usually selected when no information 
is available. It is not conjugate, and hence MCMC methods need to be implemented in 
order to estimate the posterior distribution. Nevertheless, this prior setup is conditionally 
conjugate, resulting in conditional posterior distributions for p and T that can be calculated 
analytically, allowing us to construct an efficient Gibbs sampler. 
Finally, other type of prior distributions for p have been proposed in the related literature. 
For example, the Student t distribution or the Cauchy distribution can be used instead of 
the normal prior, but obvious differences are seldom observed when no prior information 
is available. 
5.3.3 Multivariate normal priors in WinBUGS 
In the case that vector nodes are used in the model specification, the independence prior (5.4) 
can be defined in WinBUGS by specifying the priors for pj terms within a loop. Hence the 
P = p + 1 lines that define the prior distribution for each component of p can be substituted 
by 
If we wish to use the prior distribution (5.6), which is a conjugate prior, assuming that the 
elements of p vector are a priori independent, then we use the following the syntax: 
In order to define the conjugate prior ( 5 3 ,  we first need to specify in WinBUGS the 
precision matrix used in the multivariate normal prior distribution. The elements of the 
prior precision matrix T can be expressed as 
for1,j E {1,2: . . . , p +  l},whereV-'istheinverseofmatrixVand[V-']ijistheIth 
row and jth column element of V - l .  Hence, in WinBUGS , we need to first calculate the 
V-' and then the elements of the prior precision matrix using (5.7) within a double f o r  
loop. The multivariate prior can be specified by the following syntax: 
In this syntax P stands for the length of beta. The prior values V, po, and c2 can be 
specified either within the list of the data or directly within the WinBUGS model code. For 
example, the syntax 

164 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
defines pa = O p ,  c2 = 100, and V = I p ,  resulting in a ,!3J - N(O.1000~) prior. Node 
V [1, j I will take the value one if 1 = j (hence we have a diagonal element) and zero 
otherwise. Generally, it is easier to specify the components of T directly in the data than 
compute them inside the WinBUGS code. 
Finally, Zellner’s g-prior can be specified using the preceding syntax with matrix V 
calculated using the syntax 
This syntax calculates matrix A = V-’ with elements Al, = [V-’]l, = cr=l 
x21xz3, 
which are the elements of matrix ( X T X )  used in Zellner’s g-prior. 
5.3.4 Continuation of Example 5.1 
Here we rerun the same model using Zellner’s g-prior. The basic code of the model is given 
in Table 5.5, in which matrix V = ( X T X )  is defined within the model code. The first 
column X [ ,11 of matrix X is defined within the model code, and the remaining columns 
are defined within the data part. In Table 5.5, the first and last rows of the data are also 
given (in a rectangular data format) while the remainder have been denoted by dotted lines 
to save space. Additionally, the sample size n and the number of parameters P involved in 
the linear predictor are specified separately in a list format (hence these two types of data 
must be loaded separately). Alternatively, we can use the list format directly (with the help 
of R or Splus as described in Section 3.4.6.3). The list format that can be used alternatively 
in the code of Table 5.5 can be defined by the following code: 
As you may observe, some values (corresponding to the first column) are equal to NA (i.e., 
are defined as missing values). This is because the values of the first column are defined 
within the model’s code. Generally, ifthe list format is used, it is more convenient to specify 
the whole matrix x within the data format and remove command x [i ,11 C- 1 . 0 (line 19 
of Table 5.5) from the code. 
In order to define the prior parameters V, p a, and c2 in the data structure, the model 
code must be slightly changed by omitting the first lines appearing in the code of Table 5.5 
related to the definition of the corresponding parameters. Code for the general normal- 
inverse gamma prior setup in which all prior parameters are specified in a data list format 
is provided in Table 5.6. 

USING VECTORS AND MULTIVARIATE PRIORS IN NORMAL REGRESSION MODELS 
165 
Table 5.5 
WinBUGS code for Example 5.1 using Zellner’s g-prior and parameter vectorsa 
aData are compressed to conserve space 

166 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
Table 5.6 
prior" 
WinBUGS code for Example 5.1 using multivariate conjugate normal-gamma 
aAll prior parameters and data for matrix V are defined within the list data format. 

ANALYSIS OF VARIANCE MODELS 
167 
The model was run for c2 = n = 25 and for c2 = lo4, and results are presented in Tables 
5.7 and 5.8, respectively. Results (especially those concerning the variance parameter n ’) 
are sensitive to the choice of c2, indicating that the unit information choice c 2  = n = 25 
here is informative. 
Table 5.7 
additional discarded 1000 bumin iterations using Zellner’s g-prior (% = n = 25) 
WinBUGS posterior summaries for Example 5.1 after 2000 iterations and 
node 
mean 
sd 
MC error 2.5% 
median 
97.5% s t a r t  sample 
betar11 
2.228 
2.039 
0.04583 -1.974 
2.275 
6.238 
1001 2000 
betar21 
1.565 
0.3246 
0.007071 
0.9216 
1.563 
2.205 
1001 2000 
beta[3] 
0.01358 0.006783 1.423E-4 
4.297E-5 0.01348 0.02702 1001 2000 
S 
6.247 
0.938 
0.01813 
4.785 
6.134 
8.393 
1001 2000 
Table 5.8 
additional discarded 1000 bumin iterations using Zellner’s g-prior (% = lo4) 
WinBUGS posterior summaries for Example 5.1 after 2000 iterations and 
node 
mean 
sd 
MC error 2.5% 
median 
97.5% start sample 
beta[l] 2.329 
1.07 
0.02317 
0.1783 
2.341 
4.406 
1001 2000 
betar21 1.622 0.1695 
0.004654 1.297 
1.618 
1.945 1001 2000 
beta[3] 0.0143 0.003547 7.901E-5 0.007414 0.01427 
0.0214 1001 2000 
S 
3.153 0.4811 
0.01238 
2.375 
3.109 
4.253 
1001 2000 
5.4 ANALYSIS OF VARIANCE MODELS 
The normal models discussed in Sections 5.2 and 5.3 assess the association between con- 
tinuous variables. To be more specific, normal regression models identify which and how 
specific continuous explanatory variables influence a continuous response variable. Anal- 
ysis of variance models also assume a normal response variable, but now the explanatory 
variables are categorical. In this section we provide specific examples with one and two 
variables as well as details concerning their parametrization. A short discussion for multi- 
factor analysis of variance closes this section, while an example of a three-way analysis of 
variance model is provided in Section 6.1. 
5.4.1 The one-way ANOVA model 
Let us assume a categorical variable A (also calledfuctor) with levels ! 
= 1; 2 ,  . . . L.4 
and a continuous response variable Y .  When we assume that the effect of the categorical 
variable A is influencing the mean of the continuous variable Y ,  then this is equivalent to 
defining different means of Y for each category of A. Thus, assuming normal distributions 
for the response variable Y, 
the model can be summarized by 
where ! 
= 1 , 2 ,  . . . , LA indicates the group (category) of factor A from which Y originates 
and pi indicates the mean of Y for the ! 
category. An alternative method is to write 

168 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
This expression decomposes the original mean of each category level p: to an overall 
common parameter PO, called constant and group-specific parameters at, which are termed 
effects of E it level on the response variable Y. 
The interpretation of these parameters 
depends on the parametrization used for at; see next subsection for details. 
Let us now consider a random sample of n individuals resulting in ne subjects for each 
level L (E = 1; 2 ,  . . . . LA) of variable A. Then the model can be written as 
Yck 
N(PL, g2) and P i  = Po + ae 
(5.9) 
for k = 1; 2: . . . , ne and ! 
= 1! 2: . . . ~ LA. 
In practice, we usually observe n pairs (ai yi) that are realizations of the random vari- 
ables (Ai, K), where ai E (1; 2 , .  . . ; LA} is the group or level at which the ith subject 
belongs. In this case the model can be rewritten as 
~i N N(pi, g’) and pi = p.b, = PO + a a z  
(5.10) 
fori= 1.2, . . . .  n. 
5.4.2 
Parametrization and parameter interpretation 
From (5.9) it is evident that we are interested in estimating the mean values p i  of Y for 
each level of A. Thus, the original formulation p2. = ,& can be used to directly estimate 
the parameters of interest. Nevertheless, parametrization (5.10) is used for two reasons: 
(1) it separates the constant overall effect from the effect of the categorical variable A, 
and (2) it allows for generalization of the ANOVA formulation when additional categorical 
explanatory variables are involved in the model. 
In the direct estimation of the mean values p i ,  we estimate L A  parameters (one for 
each groupilevel). When the alternative parametrization (5.9) is used, then we need to 
estimate LA + 1 parameters. To make the model identifiable (i.e., the estimation feasible) 
and the two models equivalent, we impose one constraint on the new set of parameters. 
This constraint also specifies the interpretation and practical meaning of each parameter. 
Many parametrizations can be imposed by using different constraints, but two of them are 
most frequently met in statistical literature: the comer (CR) and the sum-to-zero .(STZ) 
constraints. These two parametrizations are described here in detail. 
5.4.2.1 Corner constraints. In corner constraints, the effect of a level r E { 1,2, . . . , 
LA} is set equal to zero: a, = 0. This level r is referred to as the baseline or reference 
category of factor A. Usually the first or the last (in order) level is used as the reference 
category. In medicine, placebo or standard (old) treatment are used as baseline levels. In 
the following discussion, we use the first level as the reference category: a 1 = 0. Under 
this parametrization the mean of Y will be summarized by 
pi =E(YIA= 1) = 
/LO 
pi = E(Y1A = k )  = 
where it is obvious that the constant parameter has a straightforward interpretation. It is 
simply the mean of Y for the reference category. Moreover, if we consider any difference 
- p i  = at, then we obtain the effect at of the Eth category of factor A. Hence, parameter 
at is the expected difference of Y for an individual belonging in t group (or level) of variable 
A in comparison to an individual from the reference group or category of A. 
PO +at for e 2 2 .  

ANALYSIS OF VARIANCE MODELS 
169 
5.4.2.2 Sum-to-zero constraints. According to the name of this parametrization, 
the following constraint is imposed: 
LA 
(5.1 1) 
C=1 
In practice, within the likelihood we substitute one parameter (usually the first or the last 
one) with the function resulting from the sum-to-zero constraint (5.1 1). When we substitute 
the first level, then 
L A  
a1 = -Eat. 
(5.12) 
The interpretation of this parametrization is different from the corresponding interpretation 
of the comer constrained parameters. In STZ, the constant term encapsulates an overall 
mean effect since 
e=2 
while parameter Q.8 describes deviations of each level from this overall mean effect. Positive 
effects induce an increased effect in comparison to the overall mean, while negative values 
induce effects lower than the overall mean level. 
5.4.3 One-way ANOVA model in WinBUGS 
In this section, we assume that the data are given by pairs (a i yi) referring to the character- 
istics of the ith individual. The stochastic part of the likelihood is the same as the one used 
for normal regression models above. The deterministic part of the likelihood is slightly 
changed since the mean must be specified as a function of each level of A [see Eq. (5. lo)]. 
Hence, the likelihood is defined in WinBUGS 
The imposed constraint must be set outside the likelihoodloop. Thus, for CRparametriza- 
tion, we set 
while, for STZ parametrization, we set 
Alternatively, the group means p; (denoted by m in the WinBUGS code) can be estimated 
directly. The desired effects can be simply calculated as contrasts of the group means p ;. 
Thus we can write 
po = p i ;  011 = 0 and at = pi - pi for l =  2 , .  . . , L A  
for CR parametrization and 
and ae = pi - - 
p-I for 
. . . .  L A  

170 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
for STZ parametrization. The model can be specified in WinBUGS using the following 
syntax 
Finally, we may adopt one parametrization and calculate the parameters of the other one as 
and 
for! = 1 , . . . , LA . Thus, in WinBUGS , we can calculate the STZ constraints by the model 
with corner constraints using the code 
$ R =  
C R - $ X  
at 
,;TZ 
= pt 
I - F' = pfR + a:FR - pfR - 
As a prior for p and a:! (for ! 
= 2, . . . , LA), we consider a simple normal distribution 
with mean zero and low precision to express prior ignorance. Hence, in the WinBUGS code 
the prior for the precision T is defined as in normal regression models. In the formulation 
above, no prior is imposed on the constrained parameter a: 1 since it is set equal to zero 
and therefore it does not appear in the likelihood equation. In this case, parameter cy 1 is a 
constant node, while, in STZ parametrization, it is a logicalideterministic node since it is 
defined as a function of the remaining parameters. 
When using the prior setup described above, we must be very careful since, under 
different parametrizations, we impose different prior distributions on the group means p i. 
In the case that the prior precision is small, differences due to this incompatibility of prior 
specification will be minor, but when prior information is used, these parameters must be 
specified carefully in order to lead to compatible prior beliefs. 

ANALYSIS OF VARIANCE MODELS 
171 
5.4.4 A one-way ANOVA example using WinBUGS 
Example 5.2. Evaluation of candidate school tutors. The director of a private 
school wishes to employ a new mathematics tutor. For this reason, the ability of 
four candidates is examinedusing the following small study. A group of 25 students 
was randomly divided into four classes. In all classes, the same mathematical topic 
was taught for 2 hours per day for 1 week. After completing the short course, all 
students had to take the same test. Their grades were recorded and compared (see 
Table 5.9). The administrator wishes to employ the tutor whose students attained 
higher performance at the given test. 
Table 5.9 Data for Example 5.2 (school tutors’ evaluation data) 
Candidate 
Students’ grades 
1 
8458100512889 
2 
97507683454283 
3 
644783 81 833461 
4 
776994805579 
Setting up the data and the model code. Data are coded in WinBUGSusing two 
columnsivariables: the first one with the student’s grades and the second one corresponding 
to which candidate tutor was teaching in each student’s class. Within the data we have also 
defined the number of cases (n = 25) and the number of tutors (TUTORS= 4). Data can be 
specified using either a list format or a rectangular format (see Table 5.10 for the list format 
of the data). 
Table 5.10 WinBUGS list format data for Example 5.2 
The model can be defined according to the guidelines in the previous subsection. Each 
group (tutor) mean will be equal to a constant term plus the effect with index specified by 
the variable classes. Initial values will be specified for the constant mu and the effect 
parameter alpha except from the first (baseline) level. The code and the initial values are 
provided in Table 5.1 1. STZ parametrization is used, but commands for CR parametrization 
are also provided as comments. 

172 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
Table 5.11 WinBUGS code and initial values for Example 5.2‘ 
=Corner parametrization can be fitted by removing the comment sign # in line 10 and adding it to line 8 of the 
code. 
Results. After generating 3000 iterations in total and discarding the initial 1000 iterations, 
the posterior summaries given in Table 5.12 have been calculated. In this example, we are 
interested in evaluating the overall performance of each tutor that is encapsulated by each 
parameter aJ (alpha [ j 1 ) for j = 1 2,3; 4 . Comparing the posterior means and medians 
of these parameters, we can see that the fourth tutor has a higher performance (close to 6 
while the remaining tutors have negative effects, indicating that their performance is below 
the overall mean). Nevertheless, from the graphical representations of Figures 5.2, we may 
conclude that the posterior distributions of Q~ are not clearly discriminated, indicating that 
the between-tutor differences are minor. Since the school needs to hire only one tutor, we 
recommend hiring the last one but keeping in mind that differences in the tutors’ performance 
in this small study did not indicate clear differences between tutors’ actual abilities. 
Table 5.12 
Posterior summaries for ANOVA parameters of Example 5.2 
node 
mean 
sd 
MC error 2.5% 
median 
97.5% s t a r t  sample 
alpha[l] 
-0.5661 8.016 0.1488 
-16.27 
-0.661 
15.43 1001 2000 
alphar21 
-1.218 
7.437 0.1442 
-15.77 
-1.317 
13.6 
1001 2000 
alphar31 
-4.17 
7.323 0.1497 
-18.04 
-4.158 
11.04 1001 2000 
alpha[4] 
5.955 8.345 0.1626 
-10.34 
6.066 
22.56 
1001 2000 
m 
68.96 
4.561 0.1109 
60.0 
68.98 
78.11 1001 2000 
S 
22.21 
3.638 0.08999 
16.54 21.74 
30.55 
1001 2000 

ANALYSIS OF VARIANCE MODELS 
173 
(a) Boxplots 
(b) Error bars 
Figure 5.2 
evaluation). 
Posterior boxplots and error bars for tutors’ effects in Example 5.2 (school tutors’ 
5.4.5 Two-way ANOVA models 
5.4.5.1 The main effects model, We can extend the analysis of variance model to 
accommodate additional categorical explanatory variables. In the following, we illustrate 
models with two categorical explanatory variables (two-way analysis of variance models) 
and then, in the Section 5.4.6 we briefly describe implementation of the general multifactor 
ANOVA models. 
Let as consider two categorical factors A and B with L A  and LB levels, respectively. 
Then a natural extension of model (5.8) is to include the additive effect of the additional 
variable in the expression of the mean p bb for a and b levels of A and B categorical variables, 
respectively. Hence pLb can be written as 
P L ~  
= PO + a a  + Pb 
fora = 1 , 2 , .  . . , LA and b =  1 , 2 , .  . . , Lg. 
then the model is given by 
Yabk 
When data are given in tabular format of size L A  x LB with n a b  observation per cell, 
nr(Phb, c2) and PLb = PO + a a  4- P b  
for iF = 1: 2 , .  . . > n a b  and a; b taking values as above. 
This tabular setup is restrictive and difficult to define within WinBUGSmodel code 
unless a sample with equal numbers of observations per cell is considered; see Section 
5.4.5.4 for more details. A more general setup, equivalent to the formulation for one-way 
models [see Eq. (5.10)], is given by the following equation 
~i N “pi; 0’) and 
= PO + Qac + P b ,  
for i = 1,2: . . . ~ n. In this model formulation, data are given in the form of (a i: bi; yz) for 
each subject i with ai and bi representing the groupsilevels of factors A and B, respectively, 
in which i subject belongs. 

174 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
5.4.5.2 Parametrization and parameter interpretation. Parametrization of the 
model described above can be completed by using STZ or CR constraints for both variables 
involved in the model. Mixed parametrization (e.g., CR for one variable and STZ for the 
other) can be used without any difficulty, but the user must be careful with the parameter 
interpretation. For this reason, such practice is not recommended for users with limited 
experience in modeling. In the following, we briefly present the parameter interpretation 
for each parametrization using the same logic as in Section 5.4.2. 
For the comer constraints, the constant parameter indicates the mean of Y for the refer- 
ence categories ofboth variables A and B since p{l = po (assuming here the first categories 
as the baseline levels). Effect aa indicates the expected group differences between subjects 
of level a and subjects belonging in the reference category of A when both being members 
of any group b of factor B since 
P k b  - P i b  = PO + aa + P b  - YO - a 1  - P b  
a, - a 1  = a, . 
- 
- 
Similarly, b b  is equal to the expected difference between individuals of level b and indi- 
viduals of the reference category of factor B belonging in the same group a of variable A 
since 
PLb - P i 1  = PO + &a + P b  - PO - a a  - PI 
= 
P b  - 
= !!3b . 
For the sum-to-zero constraints, the constant parameter is equal to an overall mean (grand 
mean) estimate since 
L A  L n  
L A  L n  
a = l  b=l 
L A  
LB 
LALBPO + LB C a a  + LA C 
= 
= L A L B ~ O  
a=l 
b=l 
1 
LA LB 
LALB a=l b=l 
* P O  
= 
- x x P a b .  
Moreover, since the mean of all effects a ,  is equal to zero, the effect a, indicates group 
differences between level a and the overall mean effect of A within each group b of variable 
B. Similarly, & represents differences between level b and the overall mean effect of B 
within each group a of variable A. 
In the “main effects” model defined above, the effect ofthe level of each factor is assumed 
constant across the levels of the second variable. This property is the main characteristic 
of the main effects model and, for this reason, such effects are called additive. 
5.4.5.3 The two-way interaction model. Frequently in practice the effect of an 
explanatory categorical variable A on a response variable Y is influenced by another factor 
B. In such cases, the two factors A and B interact in terms oftheir effect on Y and, therefore 
the additive model is no longer valid. A more complicated structure must be defined to 
encapsulate this “interaction” effect on the mean, which can be expressed by 
Pib = PO -k a a  + P b  + a P a b  

ANALYSIS OF VARIANCE MODELS 
175 
for a = 1.2.. . . .LA and b = 1 , 2 , .  . . , LB. Parameters a!&b 
now determine the way that 
the two categorical factors A and B interact and change their effects on Y. 
Constraints on interaction term a0 = (QPab) must be set in a manner similar to that 
for the main effects. In order to match the L A  x LB group means, we need to impose 
LA + LB - 1 constraints on the interaction parameters a@. 
In the CR parametrization we 
impose the following constraints 
at%,b = a3arb = 0 
foralla = 1 , 2  .... . L ~ a n d b =  
1.2 , . . . ,  L ~ , w h e r e r ,  E (1.2 ,..., L ~ } a n d r b  E 
{ 1.2,. . . . LB} are the reference categories for categorical factors A and B, respectively. 
In our case, we consider ra = Tb = 1 and hence aalb = aPal = 0. 
Similarly, for the STZ parametrization, we impose constraints 
(5.13) 
a=l 
b=l 
In order to estimate the model, we simply substitute L A  + LB - 1 parameters by the 
corresponding equations resulting from constraints (5.13). For example, by considering the 
substitution of the first levels of each factor, we end up setting 
L B  
apal = - C 
for all a = 1 , 2 . .  . . , L A ,  
(5.14) 
b=2 
L A  
aPlb = - x a B a b  forall b = 2 . 3  ...., L B .  
(5.15) 
Interpretation is similar to the one described in the previous section, but we now have to 
also consider the interaction term. In comer parametrization p 0 is still the mean of Y for 
experimental units in the reference categories of both categorical factors. Effects a a now 
denote to the mean difference between the reference and a level of variable A when B is 
set to its reference category r b  since 
a=2 
- 
p;rb - p;,Tb - PO + a a  + %b + a3arb - PO - 
PO + a a  - PO = aa . 
- P r b  - 
- 
- 
Similarly, $b denotes the mean difference between the reference and b level of variable B 
when A is set to its reference category ra. The interaction term Oflab denotes the additional 
effect due to the interaction between the two levels since, for a, b > 1, we obtain 
p;b - d a b  = PO + a a  + f i b  + a 3 a b  - 1 0  - ar, - P b  - a&,b 
- 
- PO + + o b  f QBab - PO - 3 b  = a a  + Qflab . 
The difference p i b  - Pknb is now not the same as the corresponding difference in the 
reference category (i.e., for b = Tb). This difference is now affected by the levels of the 
factor B, which is not the case in the main effects model. 
Interpretation of the STZ parameters is equivalent to the corresponding interpretation in 
CR parametrization, but now all comparisons are made with respect to the “grand mean”. 
It is slightly more difficult and, for this reason, corner constraints are frequently used to 
simplify interpretation. 

176 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
5.4.5.4 Data in tabular format (equal observations per cell). Frequently, data 
for analysis of variance models are provided in a tabular form. In this section we briefly 
demonstrate the WinBUGS model code when data are tabulated. 
Let us consider two categorical factors A and B with levels L A  and L g .  Then the 
data will be presented in a L A  x Lg tabular format with K observations in each level 
combination (cell). Here we assume equal observations per cell. An equivalent approach 
can be followed in cases of unbalanced data using missing values to fill in empty cells of 
the defined matrix. When unbalanced data are given in raw, individual-type format, then 
the approach described in Section 5.4.5.5 is recommended. 
Response data Yabk refer to the kth observation of Y for the a and b levels of the factors 
A and B, 
respectively. The data structure is depicted in Table 5.13. We use a L A x LB x K 
array to store the data in WinBUGS . In cases where K = 1 (i.e., one observation per cell), 
we can use a L A  x LB matrix. 
Table 5.13 
Tabular format for two-way ANOVA data with K observations per cell 
Factor B 
A 
1 
2 
. . .  
b 
. . .  
LB 
Setting up the data. The simplest approach is to use R to set up an array of appropriate 
dimension, then export it to WinBUGS and finally edit the file to meet the requirements of 
WinBUGS (see Section 3.4.6.3 for general instructions). 
To specify the data in a list format, we can use the syntax 
where #LA#, #LB#, and #K# are fixed numbers defining the number of levels for each 
variable and the number of observations per cell. In the syntax above we have specified first 
all observations for cell (1,l) followed by observations for cell (1,2), . . . .  (1, L B )  (levels 

ANALYSIS OF VARIANCE MODELS 
177 
of factor B changing first). After data for all categories of B are defined, then we insert all 
data for the next level of A using the same structure, and so on. 
A simpler way is to specify each column y [a, 
b ,I (or y [ , b , k] ) separately using the 
rectangular data format. This is easier to follow, although it requires specification of L A x 
LB columns. 
The model code. Having specified the data, we can specify the model likelihood using 
a triple loop in order to define all elements of the response array y. Thus the main effects 
model is defined by 
Yabk 
N(Pkbk; 7-l) and PLbk = PO + Qa + P b  
for a = 1: 2 , .  . . ,LA, b = 1, 2 , .  . . , Lg, and k = 1 , 2 , .  . . , K ,  while the corresponding 
WinBUGS code is given by 
Priors and constraints are defined as in the one-way ANOVA model. 
&, = PO + 
following command: 
For the interaction model, we only need to change the specification of the mean since 
+ P b  + QPab. Thus, the fifth line of the code above is substituted by the 
Note that the interaction term ab is defined as a matrix of dimension L A x LB. Priors must 
be defined for all components of ap except for the ones referring to the baseline levels T a 
and Tb. Hence we specify the prior 
LYPab - N ( O ,  lo4) for a = 2 , .  . . , L A  and b = 2 , .  . . , LB 
when T~ = r b  = 1. Similarly, for the other parameters, we use the common low-information 
priors 
PO N N ( O ,  lo4), a, - N ( O ,  lo4) and P b  N N(O, lo4) 
for all a = 2 , .  . . , LA and b = 2 , .  . . , LB. 
following syntax to specify the priors and the constraints: 
For the corner constraints, with the first level as baseline for both variables, we use the 
Sum-to-zero constraints will be defined by substituting the first four lines of the code 
above with the following syntax, which represents (5.15) and (5.14). 

178 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
5.4.5.5 A two-way ANOVA example. 
Example 5.3 . Schizotypal personality data. Let us consider the data presented in 
Table 5.14, inspired by a student survey examining the association between schizo- 
typal traits and impulsive and compulsive buying behavior of university students 
(Iliopoulou, 2004). Cell values of the table represent scores of a psychometric 
scale called schizotypalpersonality questionnaire [SPQ, Raine (1 99 l)] of individ- 
uals that are close to the median values of each group. In the original study by 
Iliopoulou (2004), no female students living in villages reported high economic 
status (indicated by NA). In Chapter 10 we illustrate how we can deal with missing 
response values using the Bayesian approach. Nevertheless, here, we have substi- 
tuted the missing values with two imaginary values (indicated within parentheses) 
for illustration purposes. In this example we are interested in the effect of the stu- 
dent’s family economic status (categorical with three levels: low, medium, high), 
area of residence (binary: City, Village), and gender (male, female) on Raine’s 
(1991) SPQ total score. 
Raine’s SPQ scale is a 74-item self-administered questionnaire used to measure the 
concepts related to schizotypal personality. The questionnaire consists of binary 
zero-one (yes-no) items. It provides subscales for nine schizotypal features as 
well as an overall scale for schizotypy. These nine specific characteristics of a 
“schizotypal personality” are defined in the DSM-111-R diagnostic and statistical 
manual ofmental disorders edited by the American Psychiatric Association (1 987), 
and are measured by an SPQ subscale calculated as the sum of the questionnaire 
items that refer to each schizotypal characteristic. 
A “schizotypal personality” suffers from minor episodes of “pseudoneurotic” prob- 
lems. In general, the prevalence rate of schizotypy is about 10% in the general 
population. The importance of schizotypal personality in psychiatric research is 
prominent for two reasons: (1) schizotypal subjects are at increased risk of devel- 
oping schizophrenia during their lifetimes, and (2) since they are healthy persons, 
they can participate in psychiatricipsychological research studies (by completing 
questionnaires - 
psychometric instruments) which tryly schizophrenic subjects 
are unable to do. In this example we focus on the total SPQ score. 
Table 5.14 
Data for Example 5.3 
Area of residence 
Citv 
Village 
Gender 
Gender 
Economic status 
Male 
Female 
Male 
Female 
Low (1) 
9 
18 
14 
29 
Medium (2) 
25 
22 
26 
25 
High (3) 
23 
12 
NA (24) 
NA (1 3) 
Two-way interaction model. In this subsection, we ignore the “city residence factor”, 
assuming that we have two observations for each combination of economic status (factor A) 
and gender (factor B). Hence we need to set up the data in 3 x 2 x 2 array ( L  A = 3, LB = 2 
and K = 2). 

ANALYSIS OF VARIANCE MODELS 
179 
Following the directions above, we first rearrange the data in the following way: 
Gender 
Male (1) 
Female (2) 
Economicstatus 
K = 1 K = 2 
K = 1 K = 2 
Low (1) 
9 
14 
18 
29 
Medium (2) 
25 
26 
22 
25 
High (3) 
23 
24 
12 
13 
We can now define the data in WinBUGS using the following syntax: 
To ensure that the data have been imported correctly in WinBUGS , we compile the model 
and go to Inf o>Node Inf 0, insert y to view the response data, and press the values box. 
The values of y will be printed in the log file for cross checking. The correct format in this 
example is as follows: 
Alternatively, we can use four columns of length equal to 3 to set up the data using the 
following rectangular data format: 
In the list data format of this syntax, we define the dimensions of the table needed for the 
specification of the likelihood in the model code. 
The full code for the interaction model is given in Table 5.15. Note that instead of 
alpha, beta, and ab we have used the names econ, gender, and econ. gender to reflect 
the actual names of the factors. 

180 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
Table 5.15 WinBUGS code and initial values for Example 5.3 

ANALYSIS OF VARIANCE MODELS 
181 
Results. Results obtained after 3000 iterations (discarding the initial 1000 iterations as 
burnin) are provided in Table 5.16 and Figure 5.3 . From the boxplots we can observe that 
0 Both interaction terms are far away from zero, indicating that this term must be 
included in the model. 
0 The 95% posterior intervals of the two interaction parameters (econ . gender 22 and 
econ . gender32 ) have common values, indicating that they can be considered as a 
posteriori equal. 
Both gender and economic status effects are far away from zero, indicating that they 
influence the mean SPQ score. 
0 Small differences are observed between the posterior distributions of the medium and 
high economic status effects. 
Table 5.16 
Example 5.3 
Posterior summaries for parameters of interaction in two-way ANOVA model of 
node 
econ [21 
econ[31 
econ. gender [2,21 
econ. gender [3,21 
gender [21 
mu0 
S 
mean 
14.09 
12.01 
-14.19 
-23.08 
12.08 
11.46 
4.081 
sd 
4.251 
4.287 
5.966 
6.095 
4.285 
3.012 
1.4 
MC error 2.5% 
median 
97.5% start sample 
0.2462 
5.74 
14.0 
22.43 
1001 2000 
0.2297 
3.497 12.0 
20.59 
1001 2000 
03869 -26.55 
-14.07 
-2.552 1001 2000 
0.375 -36.51 
-22.83 
-11.74 
1001 2000 
0.2705 
4.02 
11.94 
21.27 
1001 2000 
0.1542 
5.57 
11.43 
17.62 1001 2000 
0.06154 
2.3 
3.793 
7.522 1001 2000 
(a) Interaction 
(b) Economic status 
(c) Gender 
Figure 5.3 
personality data). 
Posterior boxplots for two-way ANOVA parameters in Example 5.3 (Schizotypal 
To be more specific, the mean SPQ score for a male student with low economic family 
status is a posteriori expected to be equal to 11.5. Female students with the same economic 
status are a posteriori expected to score about 12 units higher (Le., 23.5). Moreover, we 
observe a positive effect of economic status on male students since students with medium 
family economic status are a posteriori expected to score 14 points in addition to the ones 
scored by male students with low economic status. The corresponding increase is lower for 

182 
INTRODUCTION 
TO BAYESIAN MODELS: NORMAL MODELS 
the high economic group (about 12 points), indicating no separation in the effects of these 
two groups. 
The effect of economic status is opposite for female students since the interaction term 
indicates that increase in their economic status from low to medium does not change the 
a posteriori expected mean SPQ score (since econ [21 +econ. gender [2,21 = 14.09 - 
14.19 = -0.1). Moreover, a female student with high economic family status is aposteriori 
expected to get an SPQ score of 11 units lower than a female from low economic status 
(since econ[3] +econ.gender [3,21 = 12.01 - 23.08 = -11.07). 
The preceding comments can be depicted in an interaction plot based on the posterior 
means of expected SPQ scores (i.e., p values) for each combination of the categories of the 
two variables under consideration. In Figure 5.4 we have used one line for each gender. 
Note that in the main effects model, these two lines will not cross and will be parallel, 
indicating equal effects of the categorical variable across all levels of the other other one. 
From this plot it is clear that the economic status is positively associated with the mean SPQ 
score for males and negatively associated for females. Moreover, the difference between 
males and females is minor for the medium economic group. Finally, the mean SPQ score 
is a posteriori expected to be equal for males with low economic status and females with 
high economic status. 
10 
1 5  
20 
2 5  
3 0  
Economic status 
(a) Original plot 
(b) Graphical parameter representation 
Figure 5.4 
Interaction plot based on posterior means of expected SPQ score in Example 5.3. 
In Figure 5.4a the original interaction plot is provided, while in Figure 5.4b the same 
plot is annotated with comments on each parameter. The curly brackets refer to the inter- 
action terms, which indicate deviations from the main effects model. Hence the expected 
SPQ difference between females and male students for medium and high economic status 
is given from the differences econ. genderzz-gender2 and econ. gender32-gender2 
respectively. The plots in Figure 5.4 can be enriched by using more informative posterior 
boxplots or error bars for each group; see, for example, Figure 5.5. 
individual type data. Individual type data are usually available in practice. In this format 
data from each variable are provided in a tabular format with n rows andp+ 1 columns. Each 
row corresponds to one individual and each column, to one variable (as usually in statistical 
packages). Using this approach, we can easily fit an ANOVA model for unbalanced data. 
The main effects model can be easily programed using the syntax 

ANALYSIS OFVARIANCE MODELS 
183 
v) 
I 
I 
I 
I 
I 
1 .o 
1.5 
2.0 
2.5 
3.0 
Economic status 
Figure 5.5 
5.3. 
Interaction plot based on 95% posterior intervals of expected SPQ score in Example 
while for the interaction term we only substitute the expression for p by 
in order to also accommodate the interaction term ab. 
For Example 5.3 the data can be inserted using the following syntax 
in list format, or by 

184 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
when using the rectangular data format. In the data format displayed above, g [I and e [I 
denote the gender and economic status of each student of the data. The expression for the 
mean pi will be given by 
for the main effects model and by 
for the interaction model. 
Note that, since we can easily handle unbalanced data in this way, for Example 5.3 we can 
eliminate the two observations that correspond to missing values at the original dataset and 
rerun the MCMC algorithm. Table 5.17 presents results after removing these two values. 
Results are slightly different, but the main conclusions remain the same as in the full data 
analysis. 
Table 5.17 
Example 5.3 using unbalanced data 
Posterior summaries for parameters of interaction two-way ANOVA model of 
node 
econ[2l 
econ [31 
econ. gender [2,21 
econ. gender [3,21 
gender [2] 
mu0 
S 
mean sd 
MC error 
2.5% median 97.5% start 
14.23 6.16 
0.4092 
3.204 
14.07 
26.48 
1001 
11.32 7.72 
0.4144 
-3.831 
11.52 
26.4 
1001 
-14.63 9.385 0.7089 -34.23 
-14.11 
1.689 1001 
-23.09 11.08 0.674 
-45.34 
-22.87 
-2.465 1001 
12.32 6.563 0.4713 
1.033 
11.89 
24.5 
1001 
11.44 4.507 0.2476 
3.277 
11.41 
19.8 
1001 
5.473 3.123 0.222 
2.583 
4.741 12.43 
1001 
sample 
2000 
2000 
2000 
2000 
2000 
2000 
2000 
The individual-type data format is recommended since it is more general and can be 
implemented in most models by slightly modifying the model code shown above. Moreover, 
unbalanced data can be easily fitted using this approach. 
5.4.6 Multifactor analysis of variance 
ANOVA models can be extended to accommodate more than two categorical variables. 
The main effects model includes no interaction, and its interpretation is has similar to the 
one described for the two-way model. Including higher order interaction complicates the 
model, and the user must be careful when using them. 
Although we can extend the approach described in this section for multifactor ANOVA 
models, it is not recommended since the constraints we need to impose considerably com- 
plicate the model structure, resulting in a long and obscure model code in WinBUGS . Such 
models can be fitted in a more straightforward manner using dummy variables, which are 
described in detail in Section 6.1. 
Problems 
5.1 
Hubble’s constant data story.The following data refer to the distance in megaparsecs 
(Mpc) of 24 galaxies from earth and the velocity [in kilometers per second (km/s)] 
with which they appear to be moving away from us; note that 1 parsec = 3.26 light 
years. They were collected by Edwin Hubble in 1929 (Hubble, 1929) during his 

PROBLEMS 
185 
attempt to explain the past and predict the future behavior of the universe. Hubble 
formulated the following law 
Recession velocity = HO 
x distance 
where HO is Hubble’s constant estimated to be equal to 75 k m - I  Mpc-l. Both 
the data and the problem are available at the Data and Story Library (DASL) at 
http://lib.stat.cmu.edu/DASL/Datafiles/Hubble.html. 
a) Fit a simple regression model to identify whether the constant can be set equal to 
zero according to Hubble’s data. Use a low information prior for this model. 
b) Fit a simple regression model excluding the constant term and compare the pos- 
terior distribution of Hubble’s constant with Hubble’s estimated value. Use a 
low-information prior for this model. 
c) Interpret the regression coefficient estimated by both models. 
d) According to Hubble, the value l/Ho estimates that all galaxies originated in the 
same place (i.e., since the big bang) and hence provides an estimate of the age of 
the universe. Produce its posterior distribution. 
e) Produce sensitivity analysis in the model without intercept using a prior distribu- 
tion with N(0,o;). Compare the results for o$ E {100,1000, lo4}. Is the zero 
prior mean a sensible choice? 
f) Produce sensitivity analysis in the model without intercept using a prior distribu- 
tion with N(p0. lo4). Compare the results for p~g E {0.75,425}. 
g) Use different regression models for positive and negative velocities. Compare the 
posterior distributions of the estimated Ho values. Are they similar? 
5.2 
Consider the following data 
x: -5 
-4 -3 
-2 
-1 
0 
I 
2 
3 
4 
5 
y: -26.8 
9 27.07 22.87 8.43 4.38 3.64 4.41 37.80 95.96 179.10 
a) Use simple linear regression to fit data of the problem. 
b) Estimate the posterior distributions of the expected values of Y, for all i according 
to this model. 
c) Produce an error bar comparing the expected values of Y ,  for all i to the true 
observed values. 
d) Use polynomial regression to fit the above data. How many polynomial terms are 
needed to adequately fit the data? 
e) In the polynomial regression model, use Zellner’s g-prior and compare the results 
with the corresponding ones when an independent prior is used (in both cases 
assume that no prior information exists). 
Consider the simulated dataset of Dellaportas et al. (2002) (available at book’s Web- 
page). This dataset consists ofp = 15 covariates and n = 50 observations. Covari- 
ates X ,  (for j = 1, . , . . 15) were generated from a standardized normal distribution, 
while the response variable was generated from 
5.3 
Y, - N(X,, + Xz5. (2.5)’) for i = 1.2,. . . .50 
a) Estimate the full regression model using a simple independent low-information 
prior. 

186 
INTRODUCTION TO BAYESIAN MODELS: NORMAL MODELS 
b) Compare the estimates using boxplots and error bars. Which coefficients must be 
c) Are the true coefficient values placed in the center of the posterior distribution? 
Using the data of Problem 5.3 
a) Estimate the hll regression model using the Zellner’s g-prior with c 2  = n. Use 
b) Compare the results with the corresponding ones using an independent prior. 
Use a simple ANOVA model to compare the means of the following samples 
removed from the model, at least using these graphs? 
5.4 
vector and matrices whenever you can. 
5.5 
Group 1: 10.0 
9 . 6  8.8 
9 .1  11.8 9 . 6  9.0 10.7 10.0 11.6 
Group 2: 
9.7 
9 . 2  9.0 
9 .7  9 .0  9 .8  9 .6 8.3 
8 . 1  10.0 8.3 
8.2 
Group 3: 10.9 13.9 
5.6 11.5 12.4 
8 .6  15.2 
5.5 
Group 4: 10.3 10.6 
9.5 
9.4 10.8 
a) Compare the posterior distributions of the expected values for the four groups. 
b) Compare the posterior distributions of the model effects to those ofthe first group. 
c) Use unequal variances for each group. Compare their posterior distributions and 
Assume the following information to be available: 
Can they be considered equal? 
Can they be considered equal to zero? 
infer their equality or inequality. 
5.6 
Sample 
Sample 
Group 
Group 
mean 
variance 
size 
1 
40.8 
2.1 
30 
2 
40.1 
3.0 
100 
3 
33.5 
2.2 
56 
4 
28.9 
2.3 
16 
Use the available information tabulated above to fit in WinBUGS a simple one-way 
ANOVA model based on the distribution of the sample means. Produce posterior 
analysis of the model parameters and infer the equality or the inequality of the mean 
values. 
Download and read carehlly the paper by Kahn (2005) and the associated dataset. 
a) Fit a two-way ANOVA model (without interactions) with response to the FEV 
measurement and explanatory variables for the categorical variables gender and 
smoking status. 
5.7 
b) Interpret the posterior results of the model parameters. 
c) Examine whether an interaction term is needed for these data. If so, interpret the 
Albuquerque home prices data story.’ Download the home-priced data available 
athttp://lib.stat.cmu.edu/DASL/Datafiles/homedat.html. 
parameters of the interaction model. 
5.8 
‘Available at http://www.amstat, org/publications/jse/vl3n2/datasets.kahn.html. 
2Both the data and the problem are available from The Data and Stoly Library (DASL) 
(http: / / l i b .  stat, 
cmu. edu/DASL/) at Carnegie Mellon University. 

PROBLEMS 
187 
a) Compare the mean prices for the different number of features, the northeast loca- 
tion, and the corner location using simple one-way ANOVA models. 
b) Using a two-way ANOVA model, determine whether the effect of different num- 
bers of features (considered as categorical here) and the northeast location impact 
the home prices interactively. 
c) Using a two-way ANOVA model, determine whether the northeast or the corner 
location influence home prices. 
d) In all models, use error bars and boxplots to compare the posterior distributions 
of interest. 
e) For each model, plot error bars of the expected home prices for each observed 
value. 

CHAPTER 6 
INCORPORATING CATEGORICAL 
VARIABLES IN NORMAL MODELS AND 
FURTHER MODELING ISSUES 
In this chapter we focus on the incorporation of categorical covariates in the usual regres- 
sion models presented in the previous chapter. The use of dummy variables for the two 
parametrizations introduced in section 5.4 is first illustrated. Then we focus on ANOVA 
models using dummy variables and on the analysis of covariance (ANCOVA) models in 
which both qualitative and quantitative variables are used as covariates. 
Categorical variables can be incorporated in regression models via the use of dummy 
variables. These dummy variables identify which parameters must be added (and how) to 
the linear predictor. 
Let us consider the simple one-way ANOVA with a categorical factor A and L A  cate- 
gories. Under the CR constraints, the mean of an individual observation i with data (y i, ai) 
is given by 
Pi = Po 
when ai = 1 
pi = PO + aa, when ai > 1. 
The aim is to express the mean pi as a linear combination of the covariates equivalent to 
(5.2). Thus, we can write 
pi = PO + azD,Az + asD,P, + . . + + c Y L ~ D L ~ ,  
where D$; C = 1: 2 , .  . . , LA are dummy variables defined as 
D i  = 1 if ai = .t and D$ = 0 otherwise. 
(6.1) 
As we can observe, parameters (PO, CQ, . . . Q L ~ )  
play the same role as do parameters 
p = ( .OO ~ 31, . . . . Pp) in usual regression models. In all cases the number of dummies that 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
189 

190 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
we need to use in order to define a model will be equal to L - 1, where L are the number 
of levels of the variable under consideration. 
Similarly, for the STZ parametrization the mean of an individual observation i with data 
(yz, a,) is given by 
p, = po - a2 - as.. . - CYL* when a, = 1 
pz = Po + %% 
when a, > 1. 
The dummy variables here are slightly more complicated since 
Here we have expressed the first category as a function of the remaing of the parameters. If 
we wish to omit from the parameters a different category, then the dummy variables must 
be modified accordingly. Note that the STZ dummy variables can be easily expressed as the 
difference of the CR dummy variables. Hence, assuming that the first category will serve 
as baseline, we can write 
Di‘Stz = 0 2  - DA . 
(6.3) 
The use of dummy variables simplifies the model specification since its structure is the 
same regardless of the type of parametrization or data we use. Moreover, interaction terms 
are directly defined by the products of the corresponding dummy variables. For example, 
the product D$D$ will provide the parameter CY,!?,~ of the interaction between factors A 
and B for the corner constraints. Hence we can write 
where 06; is the dummy variable for the interaction between a and b levels of factors A 
and B for i subject. This multiplicative property is also true for higher interaction terms and 
is convenient in terms of WinBUGS programming since the definition of the constraints (as 
in Section 5.4) is avoided. Only actually used parameters are defined in the model, while 
omittedparameters (as in the STZ parametrization) can be directly monitored using a simple 
deterministicilogical node in WinBUGS model code. 
Dummy variables can also be specified within WinBUGSusing the following commands. 
For a categorical variable A with LA levels in which i subject belongs in a, category, we 
can specify the dummy variables for CR parametrization using the following syntax 
where LA is the number of levels of factor A, LA. The syntax displayed above can be 
additionally simplified if we use a matrix of dimension n x LA to specify the dummy 
variables for factor A. Then we can write 

ANALYSIS OF VARIANCE MODELS USING DUMMY VARIABLES 
191 
Using this syntax, we set D.A,e = 1 if ai = .t and zero otherwise following the definition 
of the dummy variables given in (6.1). Moreover, we have specified LA dummy variables 
instead of the required LA - 1. In the linear predictor (equation for pi), we must use 
only LA - 1 of the columns of matrix D . A. The column excluded by the linear predictor 
corresponds to the baseline (or reference) category. 
Similarly, following (6.2), the syntax 
produces the corresponding dummies for the STZ parametrization. The difference between 
STZ and CR dummy variables is in the first (baseline) category. In STZ parametrization, 
all dummies are set equal to minus one (- 1) for the reference category (DSTZ.A ze = - 1 
for all ! 
= 2, . . . , LA) instead of zero in the CR parametrization. Here, the first column 
must be omitted from the specification of pLZ. 
Changing the reference category also affects 
values for all dummy variables. Alternatively, the STZ dummy variable can be defined by 
(6.3) and, thus, specify them in WinBUGS by 
following the syntax used for the CR constraints. Alternatively, dummy variables can 
be defined in another statistical package and then imported in WinBUGS model code as 
ready-to-use data. 
Matrix X of dimension n x p with columns the constant term, the dummy variables of 
the factors, and their corresponding interactions involved in the model is called a design 
matrix. When mixed (dummy and continuous) types of variables are included in the linear 
predictor, it is called a data matrix as in the regression model. We can directly use matrix 
X to specify our model as described in Section 5.3.4 for the regression model. Such 
a strategy will considerably simplify the model specification. The design matrix can be 
easily constructed either within WinBUGS model code (following similar approach as for 
the construction of dummy variables) or outside WinBUGS and then by importing the design 
matrix in the data of the WinBUGS code; for a detailed example, see Section 6.1. 
Since matrix X can be defined regardless of the the type of variables we use, all prior 
distributions described in Section 5.3.2 can be used without difficulty. 
Moreover, the algebra related to dummy variables is intriguing since, in cases of bal- 
anced data, all design matrices can be calculated using Kronecker products. More specifi- 
cally, the STZ parametrization has interesting properties resulting in independent posterior 
distributions of parameters related to different model terms (main effects and interaction 
parameters); see Ntzoufras (1999b) for a brief discussion. 
6.1 ANALYSIS OF VARIANCE MODELS USING DUMMY VARIABLES 
Example 6.1. A three-way ANOVA model for schizotypal personality data. 
In the following illustration we specify an ANOVA model for all three variables 
(economic status, gender, city) involved in the unbalanced dataset presented in 
Table 5.14 of Example 5.3. We fit our three-way model in WinBUGS using dummy 
variables. 
Dummy variables for both parametrizations are given in Table 6.1. Code for the 
full three-way model is provided in Table 6.2. 

192 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Table 6.1 Dummy variables for Example 6.1 
Economic 
Corner constraints 
STZ constraints 
Gender Status City 
D y n d e r  Dgcon D;COn Diity 
D'$wder 
D;COn 
,?On 
DzitY 
1 
1 
1 
0 
0
0
0
 -1 
-1 
-1 
-1 
1 
2 
1 
0 
1
0
0
 -1 
1 
0 
-1 
1 
3 
1 
0 
0
1
0
 -1 
0 
1 
-1 
2 
1 
1 
1 
0
0
0
 
1 
-1 
-1 
-1 
2 
2 
1 
1 
1
0
0
 1 
1 
0 
-1 
2 
3 
1 
1 
0
1
0
 
1 
0 
1 
-1 
1 
1 
2 
0 
0
0
1
 -1 
-1 
-1 
1 
1 
2 
2 
0 
1
0
1
 -1 
1
0
1
 
2 
1 
2 
1 
0
0
1
 
1 
-1 
-1 
1 
2 
2 
2 
1 
1
0
1
 1 
1
0
1
 
To avoid definition of each parameter in separate scalar nodes, the design matrix X as 
well as a vector node for all parameters (beta) may be used instead. This approach requires 
a shorter code, which is presented in Table 6.3. The user must identify the location in which 
each parameter is stored in the vector node beta. Specific parameters can be additionally 
stored in a separate deterministic node vector in order to monitor only parameters of interest 
(e.g., the three-way interactions). 
Note that in this model, we have 10 observations and 13 parameters. This model is clearly 
overparametrized given the available set of data. Although such a model cannot be fitted in 
the classical modeling approach, within the Bayesian context the posterior distribution can 
be estimated for all parameters. The difference is that for the nonidentifiable parameters, 
their posterior coincides to their corresponding prior distribution. This implies that no 
additional information can be extracted from the data for estimation of these parameters, 
and therefore they can be assumed as redundant. 
In the following, we start by fitting the full three-way interaction model and then remove 
the terms that are not important since their posterior distributions are scattered around zero. 
Starting from the three-way model, we observe that the two parameters of the three-way 
interaction term lie around zero with large variance, indicating the likelihood that we may 
remove the parameters of this term. 
Table 6.4 provides posterior summaries for all parameters using the matrix-vector code. 
In the last column we provide the corresponding parameter name ifwe use the first (simpler) 
programming approach. In this table we observe two majorpoints: (1) both interaction terms 
are centered around zero, and hence we may remove them; and (2) parameters beta [121 
and beta [I01 (terms gender.econ.city232 and econ.city32, respectively) have posterior 
summaries that match with the corresponding summaries ofthe N(0,lO 4, prior distribution. 
This is a clear indication that the data do not contribute any information to the posterior 
distributions of these two parameters. 
In the second step, we remove the three-way interaction term (by eliminating X [i ,111 
and X [i ,121 and beta [I 1 : 121 from the model code) and rerun the algorithm. Results are 
similar as those above, and therefore we proceed by further removing the interaction term 
econ. city. We proceed by successively fitting models: gender*econ+gender*city, 

Table 6.2 
data of Table 5.14 
WinBUGS code for the full three-way model for Example 6.1 using unbalanced 

194 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Table 6.3 
X and parameter vector beta 
WinBUGS code for the full three-way model for Example 6.1 using design matrix 

ANALYSIS OF COVARIANCE MODELS 
195 
Table 6.4 
Example 6.1 
Posterior summaries for parameters of full three-way interaction model of 
node 
mean 
sd 
MC error 2.5% median 97.5% 
beta111 
9.204 
7.776 0.08657 -6.435 
9.003 26.28 
beta121 
8.735 
10.29 0.1017 -14.26 
9.001 29.78 
beta131 
15.59 
10.79 0.1359 -8.699 16.0 
36.85 
beta141 
13.48 
11.35 0.1528 -12.59 13.98 
35.85 
betaE51 
4.688 
10.74 0.1097 -18.52 
4.98 
25.98 
beta[6] -11.47 
13.98 0.1669 -39.8 -11.99 
21.0 
beta171 -19.36 
15.4 
0.229 
-48.3 -20.0 
15.45 
beta181 
6.32 
14.25 0.145 
-21.74 
6.014 37.41 
beta191 
-3.453 
14.85 0.1623 -33.49 -3.991 
30.36 
beta1101 
0.1041 99.89 1.039 -196.9 
0.445 193.3 
beta1111 -4.548 
19.01 0.2206 
-48.27 -4.024 
34.26 
beta1121 -0.3412 99.84 
1.041 -196.9 
-1.349 194.2 
S 
4.995 
8.582 0.5114 
0.102 1.607 30.57 
term 
mu0 
gender (f emale) 
econ[2] - med 
econ131 - high 
c i t y  
gender. econ 121 
gender. econ 131 
gender.city 
econ. c i t y  [21 
econ. city131 
gender. econ. c i t y  [2] 
gender. econ. c i t y  [2] 
st.dev. 
gender*econ+city, and gender*econ. The asterisk indicates that all lower interaction 
terms (and effects) are also included in the model. For example, gender*econ means that 
the main effects gender and econ are also included in the model. Boxplots ofparameters of 
the fitted models are provided in Figure 6.1. From this boxplot we observe that both models 
gender*econ+city and gender*econ seem to be satisfactory in the sense that the zero 
value lies at the tail areas for all parameters. Here we adopt model gender*econ+city 
since only for one parameter the zero value lies within the 95% posterior interval, and this 
is the lower boundary of this interval. Note that model gender*econ is the same as in 
the two-way analysis, but here we have considered the unbalanced data (10 observations 
instead of 12). Formal model comparisons follow in Chapters 10 and 11. 
In this strategy we start from the full model, including all higher order interaction terms, 
and proceed by removing higher-order interaction terms first. We do not remove lower- 
order interaction terms if these are nested within higher-order ones (hierarchically structured 
models). This approach leads to models that are easier to interpret and is provided only 
for convenience. The user may remove main effects or lower order interaction terms but 
must be very careful with interpretation. This approach it is not recommended unless the 
problem setup forces us to follow such strategy. 
Posterior summaries of the suggested model are provided in Table 6.5. Interpretation of 
the parameters is similar to that in the two-way analysis. Additionally, in this model, city 
resident students are a posteriori expected to score five SPQ points (05 = 5 )  higher than 
students of the same gender and economic status living in rural areas. 
6.2 ANALYSIS OF COVARIANCE MODELS 
Using both qualitative and quantitative variables as explanatory variables results in analysis 
of covariance (ANCOVA) models. ANCOVA models initially were used as an extension 
of ANOVA models in order to test for differences in the means of the response variable 
after adjusting for the effect of one or more numerical variables. In this way, variability 
predicted by numerical covariates can be removed before comparing between-groups dif- 
ferences. In practice such models do not demonstrate any difficulties in terms of estimation 

196 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Figure 6.1 
indicates that all lower interaction terms are also included in the model. 
Boxplots of parameters for full three-way interaction model for Example 6.1; asterisk 
Table 6.5 
6.1 with 2000 MCMC Iterations 
Posterior summaries for parameters of model gender*econ+city of Example 
node 
beta111 
beta 121 
beta131 
betaC41 
beta151 
beta C6l 
beta171 
S 
mean 
sd 
MC error 2.5% 
9.014 3.524 0.07379 2.146 
11.91 4.604 0.09902 2.496 
13.97 4.375 0.08849 4.647 
14.06 5.673 0.127 
2.833 
5.079 3.297 0.06297 -1.631 
-13.92 6.065 0.101 -26.04 
-23.0 7.878 0.1796 -37.84 
3.979 2.298 0.1045 1.741 
median 
9.075 
11.96 
13.99 
14.1 
5.001 
-14.0 
-23.25 
3 
* 377 
97.5% start 
16.27 1001 
20.76 1001 
22.74 
1001 
25.13 1001 
11.91 
1001 
-0.4573 1001 
-8.817 1001 
10.03 1001 
term 
(mu01 
(gender) 
(econ2) 
(e c on3) 
(city) 
(gender.econ2) 
(gender.econ3) 
(st. dev. ) 

ANALYSIS OF COVARIANCE MODELS 
197 
or model building. The interesting part is the interpretation of models and their structures 
when we incorporate interaction terms between categorical and numerical variables. Such 
interactions control the slopes of the regression lines for different groups. The two most 
important models are the parallel lines and direrent slopes models. 
We will briefly explain the results obtained with ANCOVA models by describing in more 
detail the simpler case with one quantitative variable and one qualitative variable. Additional 
explanatory variables can be incorporated in the model’s linear predictor similarly. 
6.2.1 Models using one quantitative variable and one qualitative variable 
Let us assume that we have available data ( X t ,  Ai, 
y Z )  with Xi, where Ai represents the 
numerical and categorical explanatory variables, respectively, and Yi is the continuous 
response variable for subject i. Then the following models can be fitted: 
1. Constant model: pi = PO 
2. Common line for all groups: pi = Po + PIXi 
3. Constant mean within each group: pi = PO + a,$ 
4. Parallel line (or common slope) model: pi = PO + PlXi + a,% 
5. Common intercept model: pi = PO + P1Xi + S,,X, 
6. Separate regression lines for each group: pi = ,30 + PlXi + a,, + &,Xi 
Models 1-3 are already known from simple regression and ANOVA models. More specifi- 
cally, model 1 is the simple constant model that “estimates” the grand mean of the sample. 
Model 2 is a simple regression model since it ignores the grouping variable A and fits a 
single regression line to the data. Finally, model 3 is a usual ANOVA model that estimates 
the means for each category of A by ignoring the numerical variable X .  Here we focus on 
models 4-6, which are the actual ANCOVA models. Model 4 assumes that the effect of 
each explanatory variable is additive. Moreover, each effect is not influenced by the effect 
of the other variable. On the contrary, model 6 assumes that the effect of the numerical 
variable is different for each group of the categorical variable. When considering ANCOVA 
models, we may also construct a common intercept model by making specific assumptions 
about parameters 
resulting from model 5. This aspect is discussed in further detail in the 
paragraphs that follow. Graphical representations of these models are provided in Figure 
6.2. 
6.2.2 The parallel lines model 
The parallel lines model can be summarized using the following equations 
(6.5) 
Y, 
N ( p , . g 2 )  
pz = P0+0lX,+a,, 
f o r i = 1 ,  . . . ,  n .  
The mean of the response variable in this model can be effectively summarized by 
where 
= ,30 + at. It is clear that the regression lines for each group of factor A will 
have different intercepts and share a common slope (i.e., will be parallel). Such models are 
also termed common slope models. 

198 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
(a) Constant model 
(c) Constant for each group 
(b) Single-regression-line model 
(d) Parallel lines model 
(e) Common intercept model 
(0 Separate regression lines for each model 
Figure 6.2 
qualitative explanatory variable. 
Illustration of the possible models with one quantitative explanatory variable and one 

ANALYSIS OF COVARIANCE MODELS 
199 
As in ANOVA models, we need to impose one constraint on the parameters at (t = 
1,2. . . . , LA) ofthe categorical variable. We canuse either CR or STZ constraints depending 
on the parameter interpretation we prefer. Hence the model parameters involved in the linear 
predictor in this model will be 
P = (RO,Pl.QZ,.”.QL*). 
omitting the a1 as the parameter corresponding to the baseline or the reference category. 
the regression line for the reference category to 
Using the CR parametrization with the first level as the reference category, we simplify 
Pt = Do + PlX,, 
providing a straightforward interpretation for PO that corresponds to the expected Y for the 
reference group when X = 0. Using similar arguments, we can interpret ae (e > 2) as 
the expected difference of Y between a subject of group t and a subject in the reference 
category having the same X since 
E(Y1X. A = !) - E(YIX, A = 1) = 30 + PIX + at - Po - Pix = C Y ~  . 
Similarly, 01 is the expected difference in Y when comparing two subjects differing by one 
unit in X and belonging in the same group since 
E(YIX = z+1. A = t)-E(YlX = z.A = t) = Bo+Pl(z+l)+ae-Po-Piz-ae 
= Pi. 
Interpretation for STZ parametrization is similar to that for CR, but our comparisons can 
be based on the property 
This equation indicates that in STZ parametrization we assume as a reference category the 
one that represents the average behavior across all factor levels. Following the relationships 
presented above, 00 under STZ parametrization provides an estimate of the expected value 
of Y when X = 0 averaged across all levels of A, while 
provides the expected deviation 
of Y for a subject belonging in .t group with X = z from the corresponding expected value 
of Y averaged across all levels of A and also having X = z. The STZ parametrization is 
equivalent to using covariates centered around zero (i.e., using Zi = X ,  - 5?) where the 
constant parameter Rg represents the expected Y for a “typical” individual of the observed 
data; see Section 5.2.3 for more details. 
The main property of this model is that the effect of X is the same in each group (equal 
to 01) and the effect of the grouping variable is the same when subjects of the same X are 
compared. This is not the case in the different slopes model, in which the effect of each 
covariate on Y depends on the values of the other covariate. Graphical representation and 
interpretation of the parallel lines model is provided in Figure 6.3. 
It is relatively straightforward to express the likelihood of the above model in WinBUGS . 
We can use either the direct approach, with the data in three vectors for y, x, and a, or the 
design matrix of the model as described in the previous sections. In the first approach the 
likelihood in WinBUGS can be written as 

200 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Figure 6.3 
Graphical representation of parallel lines model using comer constrained parameters; 
different lines represent the fitted regression models for different levels of the categorical variable 
under consideration. 
The constraints can be easily incorporated in the model using the same commands as in 
ANOVA models, namely 
for CR constraints and 
for STZ constraints. We can avoid imposing constraints by expressing the linear predictor 
as in (6.6) and hence write directly 
Finally, when using dummy variables, the linear predictor is written in the following 
form 
k
2
 
which can be easily coded in WinBUGS using the following expression to define the linear 
predictor 
or, alternatively, using the inprod command 

ANALYSIS OF COVARIANCE MODELS 
201 
In this syntax, we additionally need to specify alpha [I] by imposing the restriction cor- 
responding to the desired parametrization; for example, we set Q 1 = 0 for the comer 
parametrization. The inprod syntax can be also generalized for all variables included in 
the model as in the usual regression model. In such a case all data must be stored in a data 
matrix X of dimension n x ( L A  + 1). 
6.2.3 The separate lines model 
When assuming different regression lines for each level of the categorical variable, the mean 
of Y can be defined using the expression 
~z = PO + BlXz + @az + 
' 
(6.7) 
Alternatively, the model can be rewritten as 
Pz = 130*.,, + 3T,,*Xz. 
(6.8) 
where 5'$,! = $0 + 
= p1 + 6e for e = 1.2.. . . . LA. Thus, this model is 
equivalent to fitting one regression model for each group. 
Adopting a single ANCOVA model for the whole dataset instead of fitting different 
regression lines for each group allows us to monitor andor check for deviations from 
specific assumptions such as, for example, the common slopes assumption. Parameters 
6r denote the interaction parameters between variable X and factor A. Their magnitude 
reflects deviations from the parallel lines model and encapsulates changes in the effect of 
the quantitative covariate X on Y across the groups of factor A. STZ or CR parametrization 
may also be imposed to obtain an identifiable model. 
and 
The parameters involved in the linear predictor are given by 
p = (DO> 01. 
a2.. . . % Q L a ,  62.. . . . 6 L a )  . 
Under the CR parametrization, constraints a1 = 61 = 0 are imposed that result in 
P, = 9 0  + 31xz 
for the first reference category. Hence parameters PO and /31 will have the usual regression 
interpretation referring to subjects of the reference category. 
The main effects at denote the difference in the expected values of Y between subjects 
belonging in the !th and the reference category when X = 0 since 
E ( Y I X  = 0. A = e) - E ( Y I X  = 0. A = 1) = o0 + - po = 
. 
Finally, 31 + 6, is the expected difference of Y when comparing two subjects differing by 
one unit in X and belonging in the eth group since 
E(YJX = IC. A = k )  - E(YIX = 0. A = !) 
= 
= 3 0  + b e .  
00 + D~(z + 1) + at + 6!(~ + 1) 
- [a0 + 412 + QyE + Sex] 
Therefore Se provides the difference between the effects of one unit increase of X on Y in 
the eth group and the reference category. 

202 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Similarly, we can interpret model parameters in the STZ parametrization by comparing 
all parameters to the “mean” regression line (averaged over all groups) given by 
Parameters Lye and be will now represent differences for the intercept (i.e., for X = 0) and 
for the slope of each group from this “mean” regression line. 
Graphical representation and interpretation of the separate lines model is depicted in 
Figure 6.4. 
Figure 6.4 
Graphical representation of separate regression lines model using corner constrained 
parameters; different lines represent the fitted regression models for different levels of the categorical 
variable under consideration. 
The general ANCOVA model can be fitted in WinBUGS using the following syntax 
for the linear predictor expression (6.7). The intercept and the slope of each regression line 
can be calculated using the syntax 
Constraints for a1 and 61 must be imposed as usual. Alternatively, we may directly use 
equation (6.8) by writing 
The latter has the advantage that no constraints are needed since the model is directly 
identifiable. The original parameters of (6.7) can be extracted by adding the following 
commands in our model code 

A BIOASSAY EXAMPLE 
203 
for CR parametrization. For STZ parametrization, we need to define PO and P1 as the mean 
of all intercepts and slopes, respectively 
followed by the loop used for the specification of the CR parametrization. 
Using dummies is also straightforward since the multiplicative property (6.4) also holds 
for the interaction term between a quantitative variable and a qualitative variable. Hence 
each interaction parameter 6e will be the coefficient of a variable defined as 
The model can now be written 
Ld 
L A  
which can be coded in WinBUGS using the syntax 
The inprod command can be used to simplify the linear predictor 
where DAX is a matrix with each column representing the interaction term between factor 
A and variable X and can be calculated using the following loop 
nested within the likelihood loop (i.e., embeddedwithin a loop with i taking values from 1 to 
n). Use of the design matrix, in combination with the inprod command, hrther simplifies 
the model code as in usual regression models. 
Finally, removing the main effects at will result in the common intercept model, which 
assumes equal expected values of Y for all groups when X = 0. If this model is combined 
with centering X round zero, then it assumes that people with average X are expected to 
have equal Y in all groups. Common intercept models are rarely used in practice - 
they 
are usually adopted only if a scientific scenario supports them; see Section 6.3.2 for an 
example. 
6.3 A BIOASSAY EXAMPLE 
Bioussuys are experiments performed in the preclinical stage of a drug experiment. They 
ensure drug safety and determine appropriate drug dosage. The main objective is to estimate 
the unknown concentration of a substance of interest in a new drug in comparison to the 
standard preparation (drug) of known concentration. This unknown concentration is called 

204 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
the drugpotency and is usually estimated by the ratio of of the mean concentration of a 
standard treatment over the mean of the (new) test preparation: 
mean concentration of standard treatment 
mean concentration of test treatment 
Relative potency = 
' 
The mean corresponds to a response variable that may measure volume, weight, or dosage 
of each treatment required to achieve the same results. If the relative potency > 1, then 
a lower dose of the new drug produces the same results as does a higher dosage of the 
standard drug. Therefore, the new drug is more potent than the standard treatment. 
The response is usually quantitative or binary. In the following example we deal with 
an indirect bioassay in which we replicate an experiment in prespecified levels of dosage 
for the two treatments. In such assays, one or more drugs at different dosage levels are 
administered to experimental units such as cell cultures, tissues, organs, or living animals. 
Here we consider the case where the response is quantitative, hence ANCOVA is used to 
analyze the data and estimate the relative potency. The aim is, using statistical models, to 
estimate the relative potency of the new preparation. Two different approaches are popular: 
the parallel line analysis and the slope ratio analysis. Details concerning the parallel and 
slope ratio analysis can be found in Chen (2007) and references cited therein. Bayesian 
approaches to the parallel line and slope ratio analysis have been investigated by Darby 
(1980) and Mendoza (1 990), respectively. 
Example 6.2. Factor 8 example. Factor 8 (F8) is a blood clotting agent. Defi- 
ciency of F8 in the human body leads to hemophilia. The (fictitious) data of an 
experiment are given in Table 6.6, in which we wish to compare a newitest prepa- 
ration with the standard preparation of F8 having potency equal to 1.2 international 
units (IU). As a response Y we use the time until clotting occurs within seconds. 
Three dilutions (doses) were used (1 :40, 
1 :20, 1 : 10) with four measurements per 
dose (quantitative variable X )  and two test drugs (standard and test) are compared 
(using the categorical explanatory variable in our ANCOVA model). 
Table 6.6 Data for bioassay example (Example 6.2) 
Blood clotting time (in seconds) 
Dose 
Standard 
Test (new) 
1:40 
0.025 68.8,67.6,68.1, 67.6 
69.0,67.9,68.6,68.3 
1:20 
0.050 
61.4, 59.8,62.3,60.6 
60.9,60.3, 61.6, 61.8 
1:lO 
0.100 
53.5, 51.9, 53.6, 52.2 
53.8, 54.9, 54.1, 54.2 
6.3.1 Parallel lines analysis 
Model formulation. For a fixed dose d, of a standard drug, we assume that the same 
effect on Y is achieved with dose dt = pd, of the test preparation, where p is the relative 
potency of the two drugs. We adopt the model 
BO + 31 log(dose) for the standard treatment 
/30 + 91 log(p x dose) for the test treatment 
E ( Y )  = p = 

A BIOASSAY EXAMPLE 
205 
a 
Standard 
_ _ - - -  
_ _ - - -  
_ _ - - -  
_ _ - - -  
_ _ - -  
_ _ _ - - - -  
_ _ - - -  
_ _ - -  
log(dose) 
Figure 6.5 
Graphical representation of parallel lines analysis model. 
which is graphically represented in Figure 6.5. 
The model for the test treatment can be rewritten as 
pT = = Po + P1 l o d p  dose) 
= Po + Pi 
+ Pi log(dose) 
= PI, + PI log(dose), 
where ,$, = PO + P1 log(p). The two models have the same slope but different intercepts. 
The common slope is given by Pi, while the intercept of the standard treatment is denoted 
by PO and that for the test treatment, by PA. Thus, the relative potency can be calculated as 
follows: 
Using MCMC, it is straightforward to calculate the posterior distribution of p either directly 
by including it in the model or by simply calculating it from the values of a parallel lines 
ANCOVA model. 
In Table 6.7 we provide the WinBUGS code for fitting the parallel lines model. In this 
model code, different intercepts have been fitted directly to avoid contraints. Alternatively, 
the code using the design matrix approach for the CR parametrization is given in Table 
6.8. Commands for fitting the model using STZ parametrization are also provided as code 
comments in the same table. A normal prior distribution with mean zero and variance equal 
to 1000 was used for the parameters of the linear predictor and gamma with mean 1 and 
variance 1000 for the model’s precision. 

206 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Table 6.7 WinBUGS code for parallel lines model in Example 6.2 

A BIOASSAY EXAMPLE 
207 
Table 6.8 
approach" 
WinBUGS code for parallel lines model in Example 6.2 using design matrix 
aData are as specified in Table 6.7; substitute lines 7 and 14 by lines 8 and 15, respectively, to switch from CR to 
STZ parametrization. 

208 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Results. Posterior summaries of the parallel lines model have been calculated (see Table 
6.9) after running the MCMC algorithm for 2000 iterations and discarding additional 1000 
iterations as a burnin period. The following point estimate of the model can be provided 
using the posterior means: 
28.79 - 10.62 log(dose) for the standard treatment 
for the test treatment 
N(pz’ 0’772)‘ ” = 
29.46 - 10.62 log(dose) 
Table 6.9 Posterior summaries for parameters of parallel lines model in Example 6.2 
node 
mean 
sd 
MC error 
beta0 [l] 28.79 
0.865 
0.01804 
beta0[21 29.46 
0.8701 
0.01875 
betal 
-10.62 
0.2808 
0.006079 
potency 
1.127 0.03341 7.865E-4 
rho 
0.939 0.02785 6.554E-4 
S 
0.7731 0.1318 
0.003941 
tau 
1.81 
0.5676 
0.01645 
2.5% 
median 97.5% s t a r t  sample 
27.09 
28.8 
30.45 
1001 2000 
27.75 
29.46 
31.2 
1001 2000 
-11.18 
-10.62 -10.05 
1001 2000 
1.062 1.127 
1.195 1001 2000 
0.8846 0.9391 0.9961 1001 2000 
0.5753 0.7532 1.085 1001 2000 
0.8504 1.763 
3.027 
1001 2000 
Looking at the estimates of the model, we conclude the following: 
0 According to the intercepts PO and PA, the clotting time when the dose is equal 
to 1 (log-dose=O) is a posteriori expected to be equal to 28.8 and 29.5 seconds, 
respectively. This interpretation does not have any practical meaning since this value 
is far away from the range of dose values used in this experiment. To get reasonable 
estimates, we may transform the log-dose such that the zero value corresponds to a 
realistic dosage level (e.g., the lower level of dosage); see Table 6.10 for new rescaled 
results. 
Table 6.10 
dose [log2(40dose)] in Example 6.2 
Posterior summaries for parameters of parallel lines model after rescaling the 
node 
mean 
sd 
betaO[l] 67.96 
0.3024 
beta0 [21 68.63 
0.2973 
betal 
-7.353 
0.1971 
potency 
1.127 0.03467 
rho 
0.939 0.02889 
S 
0.7731 0.1318 
inter. dif 0.654 0.307 
MC error 
0.006875 
0.006413 
0.004279 
8.547E-4 
7.123E-4 
0.003943 
0.007264 
2.5% 
median 
97.5% 
s t a r t  
67.37 67.96 
68.55 
1001 
68.05 68.63 
69.23 
1001 
-7.736 -7.354 
-6.973 
1001 
1.058 1.126 
1.198 1001 
0.8816 0.9386 
0.9981 1001 
0.5754 0.7532 1.086 1001 
0.0543 0.6648 1.239 1001 
sample 
2000 
2000 
2000 
2000 
2000 
2000 
2000 
0 We observe a negative association between the drug dosage and the clotting time. 
Interpretation is not exactly the same as in usual regression models in terms of dosage 
increase, since the log-dose is used as an explanatory variable. One unit increase of 
the log-dose corresponds to multiplying the original dosage by e (= 2.7) since 
E(YIX = log(dose) + l , A  = k )  - E ( Y / X  = log(dose),A = k )  = D1 
and 
log(new dose) = 
new dose = e x dose. 
log(dose) + 1 = log(dose) + loge = log(e x dose) + 

A BIOASSAY EXAMPLE 
209 
Hence we can now interpret 
as the expected change in clotting time when the 
dosage increases the original one by 1.7 times (170%). In our data, when the dosage 
increases by 170%, the clotting time is a posteriori expected to decrease by 10.6 
seconds. 
To obtain more interpretable parameters, we propose using as explanatory variable the 
log2(40 x dose) instead of the log(dose). In this way the rescaled explanatory variable 
will assume values equal to 0,1,2. Now the intercept is directly linked with the expected 
clotting time of the lower dosage used in the experiment, while the new slope coefficient 
is associated with the expected decrease of clotting time when the dosage doubles. Results 
of this rescaled model are provided in Table 6.10. 
In order to calculate the proposed transformation in WinBUGS we need to express the 
new variable in terms of the natural logarithm log (or In). We write 
log(40 x dose) 
2 = 10g2(40 x dose) = 
log(2) 
using the command 
and then use x [il in the linear predictor instead of log(dose [il ). A slight change is also 
needed for the calculation of the relative potency since the new model is now given by 
The original parameters Pok and P1 are associated with the parameters of the new rescaled 
model by the equations 
resulting to relative potency 
Interpretation of the model parameters is more straight forward in this model. The clotting 
time for the lower dosage is a posteriori expected to be equal to 68 and 68.6 seconds for 
the standard and the test treatment, respectively. The difference between the two drugs is 
about 0.65 second with about 98% of the posterior values to be positive. 
When we double the dosage, we a posteriori expect a decrease of the clotting time by 
7.4 seconds. 
From the posterior densities for the two intercepts (see Figure 6.6), we observe that 
they lie in the same range of values, and thus we deduce equal potency between the two 
drugs. This comparison might be misleading since the two variables are highly correlated 
(see Figure 6.7). To get a more reliable picture, we consider the posterior density of their 
difference (0; - PO). Indeed, from the posterior density of the difference (see Figure 
6.8), we observe the zero value (equal intercepts) lying at the right tail area of the posterior 

21 0 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
(a) Original model [using log(dose)] 
(b) Rescaled model [using log, (40 x dose)] 
Figure 6.6 
Example 6.2. 
Posterior boxplots for intercepts under original and rescaled parallel lines models in 
(a) Original model [using log(dose)] 
(b) Rescaled model [using log , 
(40 x dose)] 
Figure 6.7 
Example 6.2. 
Posterior scatterplots for intercepts under original and rescaled parallel lines models in 
(a) Original model [using log(dose)] 
(b) Rescaled model [using log , 
(40 x dose)] 
Figure 6.8 
models in Example 6.2. 
Posterior densities for intercept difference under original and rescaled parallel lines 

A BIOASSAY EXAMPLE 
21 1 
distribution ofthe difference. This value corresponds to the 1.85% percentile ofthe posterior 
distribution. This indicates that the two treatments differ in terms of potency. 
We can reach similar conclusions if we monitor directly the relative potency p since it 
is a simple function of the above mentioned difference. We observe that the test treatment 
is a posteriori expected to be about 6% less potent than the standard treatment, ranging 
from 0.88 to 0.998 with probability 95%. The actual potency of the test treatment was a 
posteriori expected to be equal to 1.13 IU, ranging from 1.058 to 1.198 IU with probability 
equal to 95%; see Figure 6.9. 
Figure 6.9 
parallel lines model in Example 6.2. 
Posterior density of relative potency of new compared to standard treatments using the 
Checking the parallel lines assumption. We may check the parallelism assumption 
by simply fitting the separate lines model and comparing the posterior distribution of the 
differences between slopes. In order to fit this model in WinBUGS , we simply change the 
linear predictor to 
and define normal prior distributions for beta0 [I], beta0 [21, beta1 [I], and beta1 [21. 
We also define the slope difference using the command 
in order to monitor its posterior distribution and 
which calculates the posterior probability that the slope difference is lower than zero value 
(parallel lines assumption). Very high or low values provide an indication that these slopes 
are different. Results using 2000 iterations (using the rescaled log-dose) are given in Table 
6.1 1; see also Figure 6.10 for the posterior density plot. From these results we observe 
that the a posteriori expected slope difference is equal to 0.51, ranging from -0.22 to 1.28 
Table 6.11 
log(40 x dose)/ log(2) as explanatory variable 
Posterior summaries for parameters of separate lines model in Example 6.2 using 
node 
mean 
sd 
MC error 2.5% 
median 97.5% start sample 
P2 
0.084 0.2774 0.005963 0.0 
0.0 
1.0 
I001 
2000 
sloDe.difference 0.5096 0.3783 0.008266 -0.2171 0.4961 1.277 1001 
2000 

21 2 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
with probability 95%. We also estimate P(s1ope difference < 0) = 0.084, indicating that, 
although the zero value lies at the left tail area of the posterior distribution, it does not 
provide strong evidence against the parallel lines assumption. 
Figure 6.10 
Posterior density of slope difference in separate lines model in Example 6.2. 
6.3.2 Slope ratio analysis: Models with common intercept and different 
slope 
Model formulation. In this approach we assume again that for any fixed dose d , 
of the 
standard preparation we have the same effect with the dose d t  = pd, of the test preparation, 
but now we adopt the model 
$0 + Pldose 
PO + /31p dose 
for the standard treatment 
for the test treatment 
E(Y) 
= p = 
which is graphically represented in Figure 6.1 1. 
Dose 
Figure 6.11 Graphical representation of slope ratio analysis model. 

A BIOASSAY EXAMPLE 
213 
The model for the test treatment can be rewritten as 
where 9: = PIP. The two models now have the same intercept but different slopes. The 
common intercept is given by 130 while the slope of the standard treatment is given by ,3 
and that of the test treatment, by 3;. Therefore we can calculate the relative potency by 
(6.10) 
p = L .  
P' 
Bl 
It is straightforward to calculate the relative potency when the common intercept model 
is fitted using interaction terms under CR and STZ parametrization. The model can be 
generally expressed using the linear predictor 
pLL = 30 + &dosez + 6,,dose, 
with 61 = 0 and 61 = -62 for CR and STZ parametrizations, respectively. The slopes Do 
and Llh are equal to 31 and 31 + 62 in CR, resulting in 
while for STZ 30 = 31 - 62 and 0; = 31 + 62 with relative potency given by 
31 + 62 
Lll - 6 2  
Before we proceed, we rescale the dose in order to attain a model with parameters of 
p =  -. 
simple interpretation. For this reason, in this analysis, we use the transformation 
dose 
min(dose) ' 
z =  
(6.11) 
In this way, the two regression slopes provide the expected decrease ofthe clotting time when 
the dosage is increased by a quantity equal to the minimum dose used in the experiment. 
Note that in this analysis we must not change the zero point of the new transformed variable 
since such a transformation has a direct effect on the estimated model. For example, the 
transformation 
dose - min(dose) 
2 =  
min(dose) 
' 
results in z = 0 for dosage equal to the minimum dose of the experiment. With this 
transformation, the common intercept ,!?lo of the model corresponds to the expected clotting 
time for the lower dose. Therefore this model assumes equal effects of the lower dose for 
both treatments instead of the more realistic assumption of equal effect for the zero dosage 
of both treatments imposed by the original slope ratio analysis model. 
Under transformation (6.1 l), the relative potency is still given by the ratio of the two 
slopes. The minimum value of a vector is calculated in WinBUGS using the command 
ranked(dose [I ,1>. Hence the command 

21 4 
INCORPORATING 
CATEGORICAL 
VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
calculates the new rescaled dose used in the linear predictor of our model. 
WinBUGS code for the common intercept model is provided in Table 6.12 using the 
rescaled dosage (6.1 1). Different slopes have been used to avoid constraints. Alternatively, 
our model can be constructed using the design matrix approach as in the parallel lines 
model. The corresponding code for the CR parametrization is provided in Table 6.13. 
Commands for fitting the model using STZ parametrization are also provided in the same 
table as comments within the model code. Finally, syntax for specifying improper flat 
priors f ( / 3 j )  K 1 are also provided as model code comments. These improper priors were 
used to check the sensitivity of the posterior distribution. Minor differences were observed 
between the posteriors resulted by the improper and the N(0, lo3) prior distributions. 
Table 6.12 WinBUGS code for parallel lines model in Example 6.2 
Results. A normal prior distribution with mean zero and variance equal to 1000 was 
used for the parameters of the linear predictor and gamma with mean 1 and variance 1000 
for the model’s precision. Results using 2000 iterations and discarding the initial 1000 
iterations are provided in Table 6.14. When using the original dose, estimated posterior 
means and variances are found large for specific parameters. The normal prior distributions 
were proved to be informative in this case since results were sensitive to different values of 
the prior variance. Therefore the flat improper distribution [using the command df l a t  0 1  
was finally used. 

A BIOASSAY EXAMPLE 
21 5 
Table 6.13 
approach and rescaled dose" 
WinBUGS code for common intercept model in Example 6.2 using design matrix 
"Data are specified as in Table 6.7; substitute lines 8-9 and 17 by lines 10-1 1 and 18, respectively, to switch from 
CR to STZ parametrization. 
Table 6.14 
Posterior summaries for parameters of common slope model in Example 6.2" 
node 
mean 
beta0 
71.98 
beta1 [1] 
-4.885 
beta1 [2] 
-4.58 
more.potent21 0.0805 
potency 
1.126 
rho 
0.9382 
S 
1.362 
tau 
0.5829 
sd 
MC error 2.5% median 97.5% start sample 
0.6029 0.01443 70.75 71.99 73.2 
1001 2000 
0.2482 0.005378 -5.37 -4.886 -4.39 1001 2000 
0.2508 0.005515 -5.078 -4.58 -4.065 1001 2000 
0.2721 0.005348 0.0 
0.0 
1.0 
1001 2000 
0.05168 0,001109 1.027 1.125 1.232 1001 2000 
0.04307 9.243E-4 0.8555 0.9378 1.027 1001 2000 
0.2325 0.006969 1.014 1.33 1.915 1001 2000 
0.1828 0.005289 0.2727 0.5657 0.9737 1001 2000 
"Rescaled dose (6.1 1) is used as explanatory variable 

21 6 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
A point estimate of the model based on the posterior means is given by 
71.98 - 4.882, 
71.98 - 4.582, 
for the standard treatment 
for the test treatment 
yz N N(p,. 0.77’). pL2 = 
> 
where 2, is defined by (6.1 1). 
Thus we can infer that the clotting time is a posteriori expected to be equal to 72 seconds 
when no dilution is used. but to be reduced by 4.9 seconds when we increase the dose of 
the standard treatment by the minimum quantity (1:40) of the experiment. Moreover, the 
clotting time is a posteriori expected to be reduced by 4.6 seconds when increasing the dose 
of the test treatment by a quantity equal to the minimum quantity (1 :40) of the experiment. 
The posterior distribution of the relative potency under the common intercept model is 
given in Figure 6.12. The new (test) treatment is a posteriori expected to be 6.2% less potent 
than the standard treatment with values ranging from 0.86 to 1.03 with 95% probability. 
Although the posterior mean of the relative potency is similar to the corresponding one in 
the parallel analysis, the 95% posterior interval is wider in the current analysis, including the 
value of 1 which corresponds to the assumption of equal treatment potency. The posterior 
probability that the second treatment is more potent than the standard one was found equal 
to 0.08, which is considerably higher than the corresponding probability in the parallel lines 
analysis. Finally, The new (test) treatment is a posteriori expected to have potency equal to 
1.13 IU, with values ranging from 1.03 to 1.23 IU with 95% posterior probability. 
Figure 6.12 
Posterior density of relative potency using common intercept model in Example 6.2. 
Checking the assumption of common intercepts. In the preceding model we need to 
check for the equal intercepts assumption. The separate lines model can be used to check 
this assumption. From the posterior summaries of Table 6.15 and the posterior density plot 
of the intercept difference in Figure 6.13 we observe that the zero value (which corresponds 
to the parallel lines assumption) is close to the center of the posterior distribution since the 
posterior mean of the intercept difference is equal to -0.21, and the 95% percentile ranges 
from -2.56 to 2.18 . Moreover, the zero value corresponds to the 42% percentile of the 
posterior distribution, indicating that this assumption is a posteriori sensible. 

A BIOASSAY EXAMPLE 
217 
Table 6.15 
6.2“ 
Posterior summaries for intercept difference of separate lines model in Example 
node 
mean 
sd 
MC error 2.5% 
median 97.5% 
intercept.difference -0.2147 1.199 0.02616 -2.564 -0.2198 2.184 
P2 
0.424 0.4942 0.01217 0.0 
0.0 
1.0 
“Rescaled dose (6.1 1) is used as explanatory variable 
Figure 6.13 
Posterior density of intercept difference in separate lines model for Example 6.2. 
6.3.3 Comparison of the two approaches 
Two different approaches based on ANCOVA models have been implemented in order to 
estimate the relative potency of a new drug using a simple bioassay example. Both adopted 
models fitted above indicate that the new drug is about 6% less potent than the standard. The 
first analysis indicates clear differences between the two drugs since the posterior density of 
the estimated relative potency is far away from the value of 1, which corresponds to equally 
potent drugs. From the second model, the difference is not so clear since the posterior 
distribution of the estimated potency is more dispersed, indicating that the equal potency 
assumption might be plausible. In order to reach our final conclusion, we need to determine 
which model describes better the random behavior that we are studying. For this reason 
we may use a “naive” approach based on the Bayesian versions of R quantities described 
earlier in this chapter or more sophisticated techniques based on the predictive distributions 
to check the assumptions of each model and its goodness of fit. Also, Bayesian model 
comparison techniques can be used to identify which model is more appropriate in this 
case. All these issues are described in Chapters 10 and 1 1. 
Before closing, we provide the Bayesian measures of R i  in Table 6.16 in order to 
monitor the goodness of fit of the models. This measure indicates that do the models of the 
parallel lines analysis fit the data slightly better than the models of the slope ratio analysis. 
Moreover, the non-common-slope model (in parallel analysis) only slightly increases the 
posterior mean of this measure (posterior densities are identical). This is similar to the 
result in the non-common-intercept model in the slope ratio analysis since the posterior 
mean of R$ is slighlty lower that the corresponding one in the common slope model, while 
all posterior summaries are very close. 

21 8 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Table 6.16 Posterior summaries for RL in Example 6.2 
Analysis 
model 
mean 
sd 
MC e r r o r  2.5% median 97.5% 
Parallel line s 1 Paral l el  l i n e s  
0.9839 0.005795 1.715E-4 0.9693 0.9852 0.9914 
2 Non common slope 
0.9847 0.005355 1.341E-4 0.9714 0.9858 0.9919 
Slope Ratio 
3 Common intercept 
0.9501 0.01801 5.352E-4 0.9042 0.9538 0.9732 
4 Non common intercept 0.9479 0.01827 4.56E-4 0.9016 0.9515 0.9726 
6.4 FURTHER MODELING ISSUES 
6.4.1 Extending the simple ANCOVA model 
In practice, the number of quantitative and qualitative variables involved in a regression 
model is high, and the applied statistical analyst must select between them. Extending 
the simple ANCOVA models by incorporating interaction terms between more than two 
variables or factors results in highly complicated models. Nevertheless, their interpretation 
can be based on the three simple ANCOVA models (parallel lines, common intercept, and 
separate regression lines) presented in Section 6.2. 
When dealing with multiple regressiodANCOVA models, the user must bear in mind 
that 
0 
0 
0 
0 
0 
The most frequently used model is the main effects model, in which we do not 
include any interaction. This model corresponds to the simple model of parallel lines 
described in Section 6.2. It is simple to interpret and easily understood by specialists 
in other scientific fields. 
Generally interactions must be avoided unless the data or a scientific scenario of the 
problem at hand supports such an action. By avoiding interactions, we simplify the 
model and make it easily understandable to scientists not familiar with statistics. 
When we decide to use interactions in multiple regressiodANCOVA models, we 
usually restrict this use to two-way interactions. This is due mainly to their inter- 
pretation, which is simpler than in models with higher-order interactions. Moreover, 
the size of the data seldom allow us to extend the model by including higher-order 
interactions. 
An interaction between categorical variables imposes different intercepts for the level 
combinations of the factors involved in the corresponding interaction term. 
Higher-order interaction terms that involve one quantitative variable impose different 
slopes (effects) of the quantitative variable for each level combination of the factors 
involved in this interaction term. 
Usually, model selection methods are implemented to identify well-fittedmodels. Stepwise- 
like methods may be used within the Bayesian context, but such procedures will be intensive 
because of the computational effort needed to fit Bayesian models. This can be simplified 
in normal models since any posterior distribution of interest can be calculated analytically 
when using conjugate prior distributions. Alternatively, “automatic” model comparison 
and selection methods based on MCMC schemes can be applied. These methods are briefly 
presented and discussed in Chapter 1 1. 

FURTHER MODELING ISSUES 
21 9 
6.4.2 Using binary indicators to specify models in multiple regression 
When we wish to fit various models by trying to exclude or include different variables in 
the linear predictor, we need to write in WinBUGS different model codes for each model. 
A much simpler approach can be based on incorporating a binary vector y = (y 1 > . . . > yp) 
to the linear predictor qi, which can be now written as 
P 
and defined in WinBUGS using the syntax 
where X,, (and x [i , j]) is the j covariate or dummy variable. The binary indicator y3 is 
set equal to one if we wish to include the X ,  variable in our model and zero otherwise. 
This can be specified in the WinBUGS data section without changing the model code itself. 
Although we may extend the expression above using a binary indicator also for the constant 
term, Bo is usually included in the model. Moreover, in multiple regression models with 
large number of covariates it is convenient to use the data matrix X in combination with the 
inprod command to define the model. In this case the linear predictor can be expressed as 
P 
P 
j=1 
3=1 
where A,,, = ~ , 0 ,  and P = p + 1 the number of parameters involved in the linear 
predictor. The first column of X corresponds to the constant term, while the remaining 
columns represent the quantitative or dummy variables considered as covariates. 
The vector (Py,3) can be specified in WinBUGS using the syntax 
while the linear predictor will now be given using the inprod command by 
The binary vector y is a model indicator and is used in Bayesian variable selection 
methods [see, e.g., George and McCulloch (1993)l. It is relatively easy to extend our 
model to incorporate uncertainty concerning the inclusion of each covariate. We only have 
to use a Bernoulli prior for each yj. For example, yj N Bernoulli( i) can be used to express 
prior indifference concerning the inclusion or exclusion of the X j variable from the model. 
Other technical issues such as the selection of the prior distribution and the Bartlett-Lindley 
paradox (Lindley, 1957; Bartlett, 1957) hrther complicate implementation of the Bayesian 
variable selection. This issue is discussed in more detail in Chapter 1 1. 
6.4.3 Selection of variables using the deviance information criterion (DIC) 
As we have already mentioned in Section 4.2.5, the deviance information criterion (DIC) 
is defined as 
(6.12) 
It is used as a measure of model comparison and adequacy (Spiegelhalter et al., 2002). 
Low-value DICs indicate better-fitted models. Quantity p ,  is the number of “effective” 

220 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
parameters for model m. In normal models, presented in this chapter, p 
is approximately 
equal to the true number of parameters cl,. 
For this reason, the DIC here is approximately 
equal to Akaike’s information criterion (AIC) (Akaike, 1973, 1974). Generally DIC is 
considered as a generalization of AIC since in more complicated models, such as the 
hierarchical presented in Chapter 9, p m  is different from the number of parameters used in 
the model. 
We must further note that DIC must be used carefully since it assumes that the posterior 
mean is a good measure of central location. For this reason, DIC must not be used when the 
posterior distributions are highly skewed or bimodal (Spiegelhalter et al., 2003d, section 
entitled “Tricks: Advanced use of the BUGS language”). In normal models, the posterior 
distribution of parameters of the linear predictor is usually symmetric. Some problems may 
appear when the precision parameter 7 is low. 
According to Section4.2.5, the minimum DIC model identifies the model which offers the 
best short-term predictions. Nevertheless, Spiegelhalter et al. (2002) suggest the following 
rule of thumb: that models with DIC difference within the minimum value lower than 
two (2) deserve to be considered as equally well, while models with values ranging within 
2--7 have considerably less support. For this reason, we may report all models with DIC 
difference from the best one lower than a threshold value (e.g., 2,5, or 7) depending on the 
number of models we wish to report. 
In normal linear models, the simplest strategy is to fit all 2P models and identify the 
one with the minimum DIC value. This procedure is painful and time consuming within 
WinBUGS , even for a medium sized dataset. For example, when 5, 10, or 20 variables are 
used as covariates, then the number of models under consideration is equal to 32, 1024, and 
1,048. 576. Here we present a simple stepwise strategy that is suboptimal but can identify 
“well-fitted” models more rapidly. 
6.4.3.1 A stepwise method for DIC based variable selection in WinBUGS. 
First, we specify the starting model. Suitable starting models are the constant or null model 
(without any covariate in the model) and the full model (with all p covariates in the model). 
The latter can be used only when the number of model parameters is lower than the sample 
size 0, + 2 < n). A good starting point can be also obtained after fitting the full model 
and then adopt as the initial model the one including covariates for which the posterior 
distribution of the corresponding coefficients is away from zero. 
The procedure can be summarized by the following steps: 
1. Select initial model with binary variable indicator y(O) (see Section 6.4.2). Set 
y = y(0). 
2. Consider the currently selected model with binary indicatory such that 
Y, - N ( p t .  7-l) and pt = 90 + 
P 
“~3b’~X~~. 
(6.13) 
Further consider p candidate models characterized by their binary indicators y ; 
= 
( ~ f , ~ ,  
Y;,~. . . . . 
k ) T  for k = 1, . . . , p. Each of these p candidate models is given 
3=1 
by 

FURTHER MODELING ISSUES 
221 
Equation (6.15) defines each of the p different models such that it differs from y only 
in the status of the kth variable. For example, if X ,  is included in model y (y3 = l), 
then it is removed from jth candidate model (T;,:, 
= 1 - 2, = 0) but is included in 
all other candidate models (Y;,~ = y:, = 1 for all k # j). 
The models under consideration in this step will be denoted by 
3. Run the MCMC code and estimate DIC values for all models of step 2 (y and y for 
k = 1,. . . .p). 
4. From the fitted models of step 2, select model y Opt with the lowest DIC value defined 
as 
yopt = {y’ E  step : D I C ( ~ ’ )  = min { D I C ( ~ / / )  
for y” E M E ~ ~ P ) } .  
5. If yopt = y (i.e., the selected model is the same as the one indicated by the previous 
cycle ofthe procedure), then terminate the process, otherwise set y = y O p t  and return 
to step 2. 
This procedure can be set up within WinBUGS in a relatively straightforward manner. We 
only need to define the response variable of each model in a different vector in order to 
obtain separate DIC values for all fitted models in each cycle of the procedure described 
above. The use of the binary vectors y simplifies the implementation of this procedure 
in WinBUGS . We only need to set up the current model y and then run the associated 
WinBUGS code to identify the best model (yopt) in each cycle of the proposed procedure. 
Example 6.3. Oxygen uptake experiment. The data of this example come from 
an experiment that was conducted to examine the oxygen uptake (variable name: 
02UP) in milligrams of oxygen per minute given the following measurements: 
- X I  =BOD: biological oxygen demand 
- Xz =TKN: total Kjeldahl nitrogen (TKN) 
- X3 =TS : total solids (TS) 
- X4 =TVS: total volatile solids (TVS) 
- X5 =COD: chemical oxygen demand (COD) 
Each variable is measured in milligrams per liter. Data of this example were taken 
from Weisberg (2005, pp. 230-231). They are available in the book’s Website and 
are used and reproduced with permission of John Wiley and Sons, Inc. 
The aim here is to develop an efficient equation relating the logarithm of 02UP 
with the other available measurements for use in future predictions. 
In this example we facilitate DIC to identify which equation is needed for the predic- 
tion of log(O2UP). In each step we fit p models running in parallel within the same 
WinBUGS model code and then select the model with the lowest DIC value. 

222 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
WinBUGS model code for the stepwise procedure. We initially define the data of the 
model using the following syntax: 
Here, we define each variable that will be used in the model (BOD, TKN , TS , TVS , COD, 
02UP), the samples size (n), the number of covariates (p), and the structure of the current 
model (gamma). Within the model code we specify the transformed variable that will be 
used as response, and we also simplify the names of the covariates by setting them equal to 
xij using the syntax 
Following the description of the stepwise procedure described above, we need to define 
model (6.13) using the following WinBUGS code 
and the corresponding prior distribution by 
For the candidate models we first define their model structure as expressed by (6.15) 
using the syntax 

FURTHER MODELING ISSUES 
223 
Furthermore, one vector for each fitted model in each cycle must be defined in order to 
calculate each DIC separately (note that if a matrix is used instead, then a single DIC value 
will be calculated for all models). Hence we specify the response variables in WinBUGS by 
Finally, the linear predictor and the prior are defined using the following syntax: 
This syntax is similar to the one used for the current model with the difference that a double 
loop is used to define all p = 5 models fitted in each cycle. Hence, parameters are now 
stored in a matrix form instead of the separate vectors that we have used thus far. 
Results. Initially, the proposed stepwise procedure was implemented starting from the 
saturated model [y(O) = (1.1,l. 1, l)]. 
The procedure was terminated in four cycles, 
fitting 6 + 3 * 5 = 21 models in total. The model TS+COD was indicated as the best one 
with estimated DIC value equal to 40.47 and as covariates, the total solids (TS) and the 
chemical oxygen demand (COD). Raw WinBUGS results are provided in Table 6.17, and 
the procedure is summarized in Table 6.18. 
The same model was also selected when starting from the constant model. The procedure 
was terminated in three cycles fitting 16 models. First, the third variable (TS) was added 
followed by the fifth one (COD) in the second step. Results are summarized in Table 6.19 
When we fit the full model, then the zero value is included in the 95% posterior interval 
of the parameter effects of all covariates. Hence the ad hoc rule of starting from the model 
with covariates whose posterior intervals do not include zero indicates the null model as a 
good starting point. 
All starting points used here indicate model TS+COD ( X s  + Xg) as the best one according 
to DIC. The posterior distributions can be obtained after monitoringparameters ofthe current 
model in the final step of the stepwise procedures implemented here; see Table 6.20 for 

224 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
Table 6.17 
Stepwise WinBUGS DIC results for Example 6.3 starting from full model 
DIC STEPWISE STARTING FROM FULL MODEL 
Stcp 1: y - (11111) 
Dbar Dhat pD DIC 
y 38.164 30.659 7.505 45.669 
yl 36.655 30.273 6.382 43.037 
y2 37.986 31.707 6.279 44.265 
y3 40.191 33.874 6.318 46.509 
y4 37.030 30.687 6.343 43.374 
y5 41.274 34.941 6.333 47.607 
t o t a l  231.301 192.141 39.160 270.460 
Step2: REMOVE XI = h'OI1: y =1 (01171) 
Dbar Dhat pD DIC 
y 36.660 30.269 6.391 43.051 
yl 38.030 30.632 7.398 45.428 
y2 37.076 31.887 5.189 42.265 
y3 39.900 34.622 5.278 45.178 
y4 35.663 30.401 5.262 40.925 
y5 42.506 37.260 5.246 47.752 
t o t a l  229.835 195.070 34.765 264.599 
Step 3: REMOVE x4 = TI',Y: y = (01101) 
y 35.737 30.425 5.312 41.049 
yl 37.109 30.722 6.387 43.496 
y2 36.246 32.086 4.160 40.406 
y3 41.814 37.722 4.092 45.907 
y4 36.688 30.260 6.427 43.115 
y5 42.357 38.173 4.185 46.542 
t o t a l  229.952 199.389 30.563 260.515 
Step 4: REMOVE X z  - 7'K.Y: 
y - [ O O l O l )  
Dbar Dhat pD DIC 
y 36.283 32.094 4.189 40.471 
yl 37.189 31.918 5.271 42.460 
y2 35.643 30.408 5.235 40.878 
y3 42.237 39.158 3.079 45.317 
y4 37.219 31.907 5.312 42.531 
y5 42.059 38.913 3.146 45.205 
t o t a l  230.630 204.398 26.232 256.862 
FINAL MODEL: X3+X5, yopt = (00101) 

FURTHER MODELING ISSUES 
225 
Table 6.18 
model 
Tabulated summary of stepwise procedure for Example 6.3, starting from full 
Step 
(1) 
(2) 
(3) 
(4) 
Previous model: y 
- 
11111 
01111 
01101 
Action 
Initialize 
-X1 
-x4 
-x2 
Current model 
DIC 
45.7 
43.1 
41.0 
40.5 
7 
11111 
01111 
01101 
00101 
Candidate model 
variables 
X i :  BOD 
(-) 43.0 
(+) 45.4 
(+) 43.5 
(+) 42.5 
Xz: TKN 
(-) 44.3 
(-) 42.3 
(--) 40.4 
(+) 40.9 
X3: TS 
(-) 46.5 
(-) 45.2 
(-) 45.9 
(-) 45.3 
Xq: TVS 
(-) 43.4 
(-) 40.9 
(+) 43.1 
(+) 42.5 
X5: COD 
(-) 47.6 
(-) 47.8 
(-) 46.5 
(-) 45.2 
Key : (+) = add a variable to the current model; (-) = remove 
a variable from current model. 
Table 6.19 
nulliconstant model 
Tabulated summary of stepwise procedure for Example 6.3, starting from 
Step 
(1) 
(2) 
(3) 
Previous model: y 
- 
00000 
00100 
Action 
Initialize 
+X3 
+X5 
Current Model 
DIC 
66.8 
45.2 
40.5 
7 
00000 
00100 
00101 
Candidate Models 
Variable 
X i  : BOD 
(+) 50.8 
(+) 46.2 
(+) 42.5 
X2 : T K N  
(+) 68.8 
(+) 46.4 
(+) 40.8 
Xq : TVS 
(+) 54.9 
(+) 46.2 
(+) 42.6 
Key : (+) = add a variable to the current model; (-) = 
remove a variable from current model. 
X3 : T S  
(+) 45.0 
(-) 66.8 
(-) 45.3 
X5 : COD 
(+) 45.3 
(+) 40.6 
(-) 45.2 

226 
INCORPORATING CATEGORICAL VARIABLES IN NORMAL MODELS AND FURTHERMODELING ISSUES 
details. Since these parameters are not involved in the corresponding likelihood of the 
finally selected model, their posterior coincides with the corresponding prior distribution 
with zero prior mean and variance equal to 1000 (or standard deviation equal to 31.62). 
Table 6.20 
6.3" 
Posterior summaries for parameters of model Xs + X5 : TS+COD for Example 
node 
mean 
sd 
MC error 2.5% 
median 
97.5% 
s t a r t  sample 
beta[ll -0.06103 
32.14 
0.5393 -63.22 
0.2142 
63.51 
1001 4000 
beta[21 -0.408 
31.83 
0.4559 -63.2 
-0.4789 
61.82 
1001 4000 
beta[31 
3.448E-4 
1.308E-4 2.152E-6 
8.6E-5 
3.465E-4 5.991E-4 1001 4000 
beta[4] -0.09077 
31.83 
0.5583 -62.09 
-0.792 
63.23 
1001 4000 
beta[5] 
3.259E-4 
1.283E-4 2.022E-6 
6.738E-5 3.254E-4 5.76E-4 
1001 4000 
beta0 
-3.162 
0.493 
0.007998 -4.14 
-3.165 
-2.179 
1001 4000 
tau 
2.947 
1.031 
0.02212 
1.302 
2.843 
5.29 
1001 4000 
"Parameters 81, 82, 
and @4 are not included in the model; hence their posterior summaries are equal to their prior 
distribution, N(0. 1000). 
6.5 CLOSING REMARKS 
This chapter focuses on the extension of the simple normal regression model incorporating 
categorical variables, with emphasis on the simplest case of one qualitative variable and 
one quantitative variable. Further issues, such as model comparison using the deviance 
information criterion (DIC), are also discussed. 
After becoming familiar with the construction and analysis of the normal model, the 
reader can easily extend its formulationusing different distributions for the response variable 
(e.g., Student's t distribution), incorporating nonlinear components (such as polynomial 
functions of the original functions), higher order interactions of specific interest, or building 
models based on transformations of the original variables. 
In the following chapters we proceed with a description of the generalized linear models 
used for response variables of different types (quantitative continuous or discrete, qualita- 
tive, binary) and then extend our discussion to the Bayesian hierarchical models that account 
for data with more complicated structures. 
Problems 
6.1 
Construct the models described in Problem 5.5 using dummy variables under comer 
and sum-to-zero constraints. 
Use dummy variables for comer and sum-to-zero parametrizations to fit the models 
of Problem 5.6. 
Consider the paper of Kahn (2005) and the associated dataset, described in Problem 
5.7. 
a) Use WinBUGS to obtain posterior estimates of the parameters of the five models 
b) Compare results of the Bayesian analysis with the corresponding ones presented 
c) Interpret the estimated model coefficient for all models under consideration. 
6.2 
6.3 
presented by Kahn (2005). 
in Kahn's paper. 

PROBLEMS 
227 
6.4 
For Albuquerque home prices data presented in Problem 5.8, fit a three-way ANOVA 
model for the home prices using as factors the features of the house, the northeast 
location and the comer location, Use dummy variables for either the corner or the 
sum-to-zero parametrization. 
a) Fit the model with no interaction terms and compare it with the model with all 
b) Start from the full model and remove higher order interaction terms using DIC. 
For Albuquerque home prices data presented in Problem 5.8, include all available 
variables to model the price of a house: 
a) Fit in WinBUGS a model with no interaction terms. 
b) Interpret the model parameters. 
c) Remove variables that do not contribute to the model using DIC. 
d) Consider the model with all two-way interaction terms. Remove all terms that are 
Consider the following data for the bioassay example (Example 6.2) 
interaction terms using DIC. 
6.5 
not important using DIC. Interpret the parameters of the finally selected model. 
6.6 
Blood clotting time (in seconds) 
Dose 
Standard 
Test (new) 
1140 
0.025 
67.5 70.5 67.5 
68.8 67.1 67.0 - 
1~20 0.050 
- 62.1 62.4 
- 60.1 59.4 62.4 
1:lO 
0.100 
53.7 51.6 54.5 
53.9 51.7 53.4 52.7 
a) Perform the same analysis as in Section 6.3. 
b) Use y vector binary indicators to combine models in one WinBUGS code as 
c) Compare models using DIC. Be careful: All DICs must refer on the same response 
For Problem 6.3, incorporate all fitted models in a single WinBUGS model code file 
using the y binary vector described in Section 6.4.2. Compare all fitted models using 
DIC. 
For Problems 5.2 and 5.3, use DIC to select the appropriate variables that must be 
included in the model. 
Download the mshopl dataset from the book's Website (www . stat-athens . aueb. 
gr/" j bn/winbugs-book). The data were compiled from a customer satisfaction 
survey contacted in Chios, Greece (Sarantinidis, 2003). The given dataset is only part 
of the original dataset. Explanation of the variables is provided in file mshopl . txt. 
a) Build an ANCOVA model to identify which variables influence the total money 
b) Use DIC to finally select which variables are appropriate for the model. 
c) Interpret the parameters of the model on the basis of their posterior distributions. 
described in Section 6.4.2. 
Y with the same scale. 
6.7 
6.8 
6.9 
spent. 

CHAPTER 7 
INTRODUCTION TO GENERALIZED 
LINEAR MODELS: BINOMIAL AND 
POISSON DATA 
7.1 INTRODUCTION 
Generalized linear models (GLMs) constitute a wide class ofmodels encompassing stochas- 
tic representations used for the analysis of both quantitative (continuous or discrete) and 
qualitative response variables. They can be regarded as the natural extension of normal 
linear regression models and are based on the exponential family of distributions, which 
includes the most common distributions such as the normal, binomial and, Poisson. Gener- 
alized linear models have become very popular because of their generality and wide range of 
application. The can be considered as one of the most prominent and important components 
of modem statistical theory. They have provided not only a family of models that are widely 
used in practice but also a unified, general way of thinking concerning the formulation of 
statistical models. 
As we have already mentioned, three are the components of a GLM: the randomhtochas- 
tic component, the systematic component (or linear predictor), and the link fimction; see 
also Section 5.1. 
The stochastic component contains the response variable Y, 
and its assumed distribution 
D(0), which, within the GLM framework, is a member ofthe exponential dispersion family 
as defined in (1.3) of Section 1.5.6. The systematic component is a function of the explana- 
tory variables (or covariates) similarly as in normal regression models. Usually a linear 
combination of these variables is used, and for this reason this component is also called 
a linear predictor. Finally, the link function g(0) is the mathematical expression which 
connects the parameters of the response Y with the linear predictor and the covariates. In 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright @ZOO9 John Wiley & Sons, Inc. 
229 

230 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
GLM a location parameter (e.g., the mean) is usually linked with the linear predictor. Hence 
a GLM can be summarized by the following expressions 
Y, 
N 
expf(t?,, 4, a(), b(). c()) 
(stochastic component) 
qz 
= X(t,p = $0 + Cy=, xz3pJ (systematic component) 
19% 
= R(6'%) 
(canonical - 
distribution parameter function) 
8, 
= (@?+qT 
(model parameters), 
g(0,) = g(R-'(t?,)) = gd(29,) = q2 (link function) 
(7.1) 
where expfp,, d, a ( ) ,  b ( ) ,  c()) denotes the exponential family with location and dispersion 
parameters 29, and 4, respectively, and a(), b(), c() are functions needed to specify the 
structure of the specific distribution with density or probability function given by 
(7.2) 
In this setup, we denote by 0 the original location parameter of the distribution, by t? the 
canonical parameter of the exponential family, and by R(6') the function that connects the 
two parameters. Moreover, we denote byg(6') andgs(29) the link functions that associate the 
location parameter 6' and the canonical parameter 29, respectively, with the linear predictor 
q. Note that the latter is given by ge(t?) = g(R-'(t?)). 
In this section we introduce the user to the basic notions ofthe generalized linear models. 
In the following two sections we provide details concerning the exponential family and some 
common distributions that belong in this family. The section continues with a presentation 
of the most common link hnctions used and discussion of some of the more complicated 
ones proposed for binomial data. Finally, we close this section with a short description of 
the most common models for every type of data. 
Additional details concerning generalized linear models can be found in a variety of 
well-written books related to the topic such as McCullagh and Nelder (1989), Lindsey 
(1997), and Fahrmeir and Tutz (2001). A detailed illustration of Bayesian inference and 
analysis focusing on GLMs can be found in Dey et al. (2000). 
7.1.1 The exponential family 
A member of the exponential family has a probability or density function that can be 
expressed by the following form 
f(Yl4 = exp { R(W(Y) + B(8) + C(Y)}, 
(7.3) 
where 29 = R(6') is the canonical parameter and both 19 and 6' are location parameters; for 
this expression, see Bemardo and Smith (1994, p. 266), Lindsey (1997, p. lo), Gamerman 
and Lopes (2006, pp. 5 1-54), and Agresti (2002, p. 116) for equivalent expressions. 
The more general expression (7.2) of McCullagh and Nelder (1989) also incorporates 
in the formulation a scale parameter 4 in addition to the location parameter 19. These 
parameters are collectively called the exponential dispersionfumily. When the dispersion 
parameterq5isknown, then(7.2)becomesequalto(7.3)ifwesetR(19) = t?/a(4), T(y) = y, 
B(19) = -b(19)/a(4), and C(y) = c(y, 4). This expression is more convenient when one 
parameter distributions that are members of the exponential family are adopted. 

INTRODUCTION 
231 
The mean and the variance of Y with distribution in the exponential family with param- 
eters t9 and 4 are equal to 
a($) = b//(t9)u(+) . 
(7.4) 
d'b(t9) 
db(t9) - b'(19) and V(Y) = - 
dd2 
E ( Y )  = - 
- 
dt9 
7.1.2 Common distributions as members of the exponential family 
Members of the exponential family are popular distributions such as the normal, the bi- 
nomial, the Poisson, the gamma, and the inverse Gaussian distributions. We may further 
include distributions that are special cases of these distributions (e.g., the exponential and the 
Pareto distributions) or distributions that result as transformations of the above mentioned 
random variables (such as the log-normal or the inverse gamma distributions), which can be 
treated as GLMs without any methodological complications. In the following paragraphs 
we provide details concerning the most popular distributions of the exponential family; see 
Table 7.1 for a tabulated summary. 
Normal distribution. The density function of the normal distribution can be written as 
f(YIP> g2) = exp (7 
{ :  
2 
2 0 2  
y2 >I . 
1 
yp - p2 + - - log(27r) - - log 2 - - 
Hence the canonical parameter t9 is equal to the mean of the normal distribution p, the scale 
parameter 0 is equal to the variance n2, b(6) = b ( p )  = p2/2, a(4) = u(n2) = 02, and 
c(y, 4) = c(y, 2) 
= -; 
log(2r) - ; 
l o g 2  - ;y2/2. 
Binomial distribution. For the binomial distribution binomial(T, N), we can express the 
probability function as 
The canonical parameter is now given by t9 = log[.ir/(l - T)] 
while a(4) = 4 = 1 and 
which does not depend on either y or 19. Solving the expression of the canonical parameter 
in terms of T we have that T = e"/( 1 + e"') resulting in b(19) = N log( 1 + e'). Using (7.4), 
we obtain E(Y) = Ne"'/(l+ e"^) = NT and V(Y) = Ne"'/(l + e"')2 = Nn(1 - T) as 
expected in the binomial case. 
Negative binomial distribution. Similar to this is the logic for the negative binomial 
distribution NB(r;, k )  (for fixed k), where we can write t9 = log(1 - T) and b(6) = 
-klog(T) = -klog(l - e"'), a(q5) = 1 and 

232 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Poisson distribution. 
written as 
The probability function of the Poisson(X) distribution can be 
f(ylr, K) 
= exp y log(X) - x - log(y!)) . 
The canonical parameter is now given by 6 = log X while a(@) = d = 1, c(y. Q) = log(y!) 
and finally b(8) = X = e8, Using (7.4), we obtain both the mean and the variance equal to 
X as expected in the Poisson distribution. 
( 
Gamma distribution. 
mean p and variance p2/a can be written as 
The probability function of the gamma(a, a l p )  distribution with 
a 
1 
P 
P 
= exp ((a - 1) logy - -y + aloga + alog - - logr(a)) 
The canonical parameter is now given by 6 = -p -’ (or -b/a in the original parametriza- 
tion)whileo = a - l , a ( ~ )  = Q = a-1 andc(y,Q) = ~-~log(y~-~)-logy-log~(l/~). 
Finally, b(6) = - log(-[-l/p]) = - log(-.Ly). Using (7.4), we obtain E ( Y )  = p and 
V(Y) = p2/a as defined above. 
Inverse Gaussian distribution. The density of the inverse Gaussian distribution Y N 
IGaussian(p. A) is given by 
1 
= exp(-m--+- 
”” + - {log - log(27r) - 3 logy} 
2P2Y 
2P2Y 
2 
) 
A
1
 
= e x p ( - p - -  
Xy 
+ - + - {logX - log(27r) - 3logy) 
2Y 
P 
2 
where the canonical parameter is given by 6 = - p P 2  and the scale parameter by 0 = 
X-’. 
Moreover we have b(6) = -(p)-’ = -(-21Y)l/~ since p = (-26)-ll2, a(d) = 
d = X-l, and c(y, 0) = -; 
{ (@y)-’ + 10g(27r4y3)}. The mean and the variance of the 
preceding parametrized inverse Gaussian distribution is given by E ( Y )  = /I and V(Y) = 
p3/X. In GLMs the alternative parametrization IGaussian(p, X = 1/c2) is frequently 
used. 
Other well-known distributions. Other well known distributions that are reported as 
members of the exponential family are the beta, Weibull, multinomial, and he Dirichlet 
distributions. The last two can be regarded as members of the multivariate extension of the 
exponential family, while the Weibull distribution can be rewritten as in (7.3) for a given 
dispersion parameter. 

h 
a 
s 
v 
L 
h 
v 
93 
1 
a 
U 
J h 
v 
h 
v 
9 
4 
93 
a, 
.- 3 
2 
9 
2 
x 
Lc 
v: 
3 - 
9 
C 
.- 
I 
I 
z 
5 
.- 
I 
2 
I 
.- n 
9 
Y 
b 
m 
\ 
N 
93 
3. 
N 
b 
-i 
w 
p: 
h 
3 
b 
i 
2 
W 
- 
E 8 
h 
m 3 
I i 
\ 
9 
Y 
v 
- 
h 
e. 
iir 
I 
3 
v 
bc 
Y 
3 
I 
h 
k 
I 
c 
v 
ho 
i 
N R 
R 
I 
\ 
h 
i 
v 
Y 
9 
R 
I 
Y 
2 
3 
v 
-? 
m- 
0" 
I/ 
F 
ri 
v 
h 
Y 
k 
v 
INTRODUCTION 
233 

234 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
7.1.3 Link functions 
7.1.3.1 Common link functions. The link function is a monotonic and differentiable 
function used to match the parameters of the response variable with the systematic compo- 
nent, namely, the linear predictor and the associated covariates. Usually no restriction lies 
on the definition of such variables, but often we focus on the mean of the distribution be- 
cause the measures of central location are usually of main interest. GLM-based extensions 
in which dispersion or shape parameters are linked with covariates also exist in statistical 
literature [e.g., see Rigby and Stasinopoulos (2005)l. A desirable property of the link func- 
tion is to map the range of values in which the parameter of interest lies with the set of real 
numbers R in which the linear predictor takes values. For example, in the binomial case 
we wish to identify link functions that map the success probability 7-r from [0,1] to R. 
The simplest link function is the one that sets the linear predictor equal to the mean p. 
This is indeed the usual link function for the normal models. This function is not appropriate 
for other distributions such as the Poisson distribution since their mean is positive while 
77 E R. In cases where the parameter of interest is positive, we usually adopt the log-link, 
which implies a multiplicative relation between covariates and the parameter of interest, 
Usually the default choice of link function is provided by the canonical link, in which we 
set the canonical parameter (as an expression of the mean or other parameters of the distri- 
bution) equal to the linear predictor. The canonical link function for common distributions 
are summarized in Table 7.2. 
Table 7.2 
family 
Canonical link functions of most common members of exponential dispersion 
Link 
Link fimction 
Distribution 
name 
S(P) 
4 0 )  
(P/N)/(l - P / W ]  1% [n/P - 7r,] 
log(1 - 7r) 
Normal 
Identity 
Binomial 
Logit 
Negative binomial Complementary log 
Poisson 
Logarithmic 
log x 
Gamma 
Reciprocal 
1/P 
Inverse Gaussian 
Squared reciprocal 
l/pz 
For binomial (and negative binomial) models, a wide variety of link functions exist. The 
canonical link is the so-called logit link defined as 
Other popular alternatives are the probit link (frequently used in econometrics) defined as 
g(7-r) = @+(7r) 
and the complementary log-log link function given by 
where @(x) is the cumulative probability function of the standardized normal distribution 
and @-I(.) 
is its corresponding inverse function. 

INTRODUCTION 
235 
General links can be adopted by considering a family of functions indexed by one or 
more (unknown) continuous-valued parameters. For a binomial response, such functions 
can be easily obtained by considering the inverse cumulative probability function F 
(T: 
0) 
of a random variable 2 N D(0). Parameter Q gives rise to a range of possible link functions 
that can be used. Even the link functions mentioned above (logit and the complementary 
log-log) can be obtained by the inverse cumulative probability functions of the logistic and 
the extreme value distributions. 
Other link functions that may be used are the log-log link for binomial data, the square 
root g(p) = & (for distributions with positive mean), and the exponent g(p) = ( p  + c 1)Q 
when p > -c1 (Lindsey, 1997, p. 21). 
7.1.3.2 More complicated link functions for binomial data. A wide variety of 
link distributions have been proposed for binomial models. Here we briefly present some 
of the possible alternative link functions described in the related literature. For example, 
a straigtfonvard extension of the probit link is provided by considering the inverse t-link 
family proposed by Albert and Chib (1993) and the log-gamma link family proposed by 
Genter and Farewell (1985); see Ntzoufras et al. (2003) for the Bayesian implementation. 
The inverse t-link family is given by g ( T )  = F ~ ( 7 r ;  
v), where FT(Y 
; v) is the distribution 
function of a t distribution with v > 1 degrees of freedom. This link family includes as a 
special case the probit link function for large values of v (v -+ m). Furthermore, Albert 
and Chib (1993) argue that the t-link with v = 8 is a reasonable approximation to the logit 
link. The log-gamma link family is defined as 
1(0 > 0) 
7r + I(Q < 0)(1 - 7r); a = Q-*; b = 1 
This family includes as special cases the probit link (0 + 0), the log-log link (0 = -1) 
and the complementary log-log link (0 = 1). The link parameter 0 controls the symmetry 
of the link function in contrast to the t-link which is symmetric for all values of v. 
Further link functions are provided by using mixtures of beta distributions as prosed by 
Mallick and Gelfand (1 994) and normal scale mixtures as proposed by Basu and Mukhopad- 
hyay (2000). Lang (1999) considers the link function 
3 
where zck (p) are mixing proportions, p is a mixing parameter to be estimated, and Fk (74) 
are the cumulative distribution functions of the extreme minimum value, the logistic and 
extreme maximum value distribution for k = 1. 2,3, respectively. Aranda-Ordaz (1981) 
and Albert and Chib (1 997) used a family of symmetric links defined as 
2 
7rA - ( 1 - 7 r ) A  
T A  + (1 -.)A' 
g(X) = 
where X = 0.0,0.4.1.0 
correspond to the logit, (approximately) probit, and linear link 
functions, respectively. Aranda-Ordaz (1 98 1) also proposed an asymmetric family given 
bv 
(1 - 7 r - A  - 1 
X 
S(7.r) = 

236 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Other approaches include the link family based on the inverse distribution function of the 
logarithm of an F-distributed random variable (Prentice, 1976), the two-parameter link 
family of Pregibon (1 980) defined as 
the Box-Cox transformation based link family (Guerrero and Johnson, 1982) 
the generalization of the logit link suggested by Stukel (1988), and a family of robust link 
functions proposed by Haro-L6pez et al. (2000). More details concerning available link 
functions and their Bayesian analysis and comparison for binomial models are provided by 
Ntzoufras et al. (2003) and Czado and Raftery (2006). 
7.1.4 Common generalized linear models 
Different models of the exponential family are appropriate for different types of response 
variables. In this section, we summarize which are the most common models for each type 
of response variable. 
Response variables defined in R. In the case of continuous response with values 
defined over the whole range of real numbers, the normal regression model described 
Chapters 5 and 6 is the most popular choice. When the normality of error assumption is 
not appropriate, the normal model can be extended using errors that follow the Student’s 
t distribution. Although this model cannot be considered as a member of the exponential 
family, it can be easily fitted using WinBUGS , 
Positive continuous response variables. When positive defined continuous response 
variables are considered, then one initial approach is to transform them using the Box- 
Cox transformation or the logarithm. Using a common (normal) regression model for the 
transformed response variable may lead to a plausible model. Such models can be treated 
in the same way as usual regression models since, after transforming the response variable, 
inference is exactly the same. It is common practice to first consider the logarithm of the 
original response variable as a possible transformation since in several cases it may eliminate 
problems related to the assumptions of the model such as the normality or errors, linearity 
of the mean, or homoscedasticity. This very simple strategy (i.e., using the logarithm of 
a variable as the response in a normal model) is equivalent to assuming the log-normal 
distribution for the original response variable. Other common distributional choices for 
positive continuous responses are the gamma, the exponential, the inverse Gaussian, and 
the Weibull distributions. 
A positive response variable is the survival time, or more generally the time until an 
event of interest occurs, which is of central interest in medical studies, especially in clinical 
trials. When modeling survival times, censoring is an additional characteristic that must 
be considered in the model. A survival time is considered as censored when part of its 
information is not available. For example, we may know that a patient was alive for the 50 
first days of the study, but we might ignore the exact time of failure. The Weibull distribution 
is commonly used for modeling such response data. 

INTRODUCTION 
237 
Binary (success/fai/ure) responses. Binary (zero-one) data can be modeled using the 
Bernoulli distribution (i.e., y E (0, l}). Moreover, when the response variable measures 
the number of successes after the repetition of n such experiments, then the binomial 
distribution with success probability 7r and n replications must be considered. In such 
cases, the response variable takes values from zero to n, y E {0,1, . . . , n}. Note that in the 
case ofbinary data the Bernoulli distribution is equal to a binomial distribution with n = 1. 
The canonical link is the logit function log (7r/(1 - n)), 
which models the log-odds of 
success as linear combination of the covariates (Berkson, 1944, 1951). Logit models are 
the most popular stochastic formulations for such data and are cited as logistic regression 
models. Another popular link is the probit link (Bliss, 1935), which gives results similar to 
those for the logit link; see also Albert and Chib (1993) for the Bayesian implementation. 
Finally, complementary log-log link (Fisher, 1922) is a less popular link, but it models 
more efficiently the tails of the distribution, especially when asymmetry between low and 
high probability values is observed. 
Counts and responses definedin N . Response variables defined inN = { 0, 1,2, . . . , ) 
frequently represent number of events occurred within a prespecified time interval, namely, 
counts or frequencies. The Poisson distribution is naturally adopted in such cases result- 
ing in Poisson regression models that are also referred to as Poisson log-linear (or simply 
log-linear) models, due to the canonical log-link adopted in most cases. Poisson log-linear 
models are also used for the analysis of high-dimensional contingency data, which result 
from the cross-classification of several categorical variables as introduced by Birch (1 963). 
The Poisson distribution implies the restrictive assumption of equal mean and variance, 
which has generated much discussion within the statistical community, leading to less re- 
strictive models that allow for overdispersion (larger variance than mean) or underdispersion 
[e.g., seeLindsey(1997,sec. 2.3) andAgresti(2002,pp. 130-131)forarelateddiscussion]. 
A popular distribution that allows for overdispersion is the negative binomial distribution. 
Other response variables. For response variables that do not belong in the general cases 
described above, we propose as initial step transforming them in such way that they can be 
fitted using the precedingmodels. Otherwise, special models must be constructed following 
the logic of the generalized models. 
For example, for variables that are defined in a range y E (u, b), we may rescale them in 
the zero-one interval by setting y * = (y - a ) / ( b  - u )  and use the beta distribution for the 
stochastic component. Another alternative is to use a logit-like transformation by setting 
y** = log { (y - a ) / ( b  - y)} and use normal regression models. 
For categorical responses with k > 2 levels, the multinomial distribution may be used as 
a natural extension of the binomial models. The same distribution can be used for grouped 
categorical variables where the frequencies of k different outcomes will be recorded as 
responses. Finally, such responses can be modeled indirectly using the Poisson log-linear 
models for contingency tables when all covariates are categorical [see, e.g., Fienberg (1 98 1, 
chaps. 6, 7)]. 
Forresponsevariablesdefinedinthesetofintegernumbers (y E Z  = {. . . -3, -2, -1, 
0.1,2,3. . . .}) a model based on the differences of Poisson latent variables has been devel- 
oped by Karlis and Ntzoufras (2006,2008). This model cannot be considered as a member 
of the exponential family, but the conditional likelihood when the latent (under estimation) 
Poisson variables are known is a simple Poisson likelihood. To fit the model, we can use 
the EM algorithm to estimate the posterior mode (or the MLE estimate) or simple MCMC 
algorithms as described by Karlis and Ntzoufras (2006). 

238 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Finally, ordinal variables can be modeled using a variety of alternative approaches that 
have been introduced in the literature. A natural extension of the usual log-linear model used 
for contingency models can be adopted by Goodman’s (1979) association models, which 
were originally used for two-way contingency models. Such models cannot be considered as 
GLMs because of the multiplicative expression between the model parameters and Poisson’s 
expected values. 
7.1.5 Interpretation of GLM coefficients 
Interpretation of GLM’s coefficients is equivalent to the corresponding interpretation of the 
parameters in usual normal regression models. Thus, interest lies in (1) whether the effect 
of X, is important for the prediction or description of Y, 
(2) the type of association between 
Y and X, (positive, negative, linear, or other), and (3) the magnitude of the effect of X, on 
Y. 
Concerning the importance of the effect, we can simply monitor the posterior distribution 
and report whether the zero value is away from its center. Moreover, DIC may also be 
used to identify which model is more appropriate for the description of the available data, 
as implemented in the previous chapter; see Chapters 10 and 11 for other, more formal, 
approaches of model checking and comparison. 
The type of the association (negative or positive) is simply indicated by the corresponding 
sign of the posterior summaries for each coefficient as in common regression models. 
Concerning the interpretation of model parameters, interest lies in quantifying the effect 
of each covariate X, on the corresponding parameter of interest of the response variable Y 
(usually on the mean of Y). Although this is straightforward in normal regression models, 
since the canonical link is used and, therefore, the effect of each X, is linear to the mean of 
Y, 
it is slightly more complicated in GLMs and depends on the form of the link function. 
For this reason, we may interpret the effect of each covariate using one (or more) of the 
following approaches: 
Use the first-order differences or relative differences of the means 
Use the marginal effect of X, on the mean. 
Calculate and present the means of Y for specific plausible combinations of X, 
The mean of Y can be substituted by any other parameter of interest. Further details on 
these approaches can be found in Futing Liao (1994, pp. 6-9) and in Aldrich and Nelson 
Using first-order differences is the simplest approach. With this approach, we express 
model parameters 3, as the effect of one unit increase on the mean (or other parameter of 
interest). For example, in the simple case of the normal regression model with one covariate 
we have 
values. 
(1984, pp. 75-80). 
E(yiX = z + 1) = 30 + 01 
(X + 1) = & + 812 + (01 = p ( ~ )  
+ $1 * 
Ap = E(Y/X 
= x + 1) - E(YIX = 2 )  = Pi, 
where E(Y /X 
= z) is the mean of Y when X = 2. When the link function is a logarithmic 
expression of the parameter of interest, then the effect on this parameter is multiplicative 
and is usually expressed as a relative difference or as a percentage change. For example, in 
a Poisson log-linear model with one covariate, E(YIX = IC + 1) = E(YIX = z)eOl. 

PRIOR DISTRIBUTIONS 
239 
In specific cases, it is difficult to express the difference or the relative difference as a 
direct function of the model coefficients oj. In such cases, it is useful to consider the 
marginal effect of X j  on a parameter of interest of Y, 
which is given by the first derivative 
of the parameter of interest over X j  
where g(0) is the link function of the parameter of interest 0. This expresses the rate of 
increase or decrease of 8 when the covariate is equal to 2. It can be considered as a rough 
approximation of the increase of 0 when X j  increases by one unit and the other covariates 
remain the same; for details, see Futing Liao (1 994, pp. 6-9) and Aldrich and Nelson (1 984, 
In the case that we consider the canonical parameter 6 and the corresponding canonical 
link, the marginal effect on the canonical parameter is equal to pj for all members of the 
exponential family since 0 = 29 and g(6) = 6. 
Finally, it is convenient to provide the predicted means (or other parameters) under the 
estimated model for several scenarios or profiles of individual observations. Plausible sce- 
narios can be the average, the median, and the extreme low or high profiles (worst and 
best-case scenarios). The first two profiles refer to the calculation of 8 values for observa- 
tions with all covariates equal to the sample mean or medians. Alternatively, population 
means and medians can also be used if available. This produces an estimate of 0 for an av- 
erage or typical person in our sample (or population). The worst-case scenario (low profile) 
involves the computation of 8 by setting all covariates to the minimum or maximum values 
for covariates with positive or negative effect on Y ,  respectively. Similarly, the best-case 
scenario (high profile) involves the computation of 0 by setting all covariates to the maxi- 
mum or minimum values for covariates with positive or negative effect on Y, 
respectively. 
Similar estimates can be estimated by calculating all 8i (i = 1; . . . , n) and reporting the 
covariate values for mi& 
8i and maxy==l 0i, respectively. 
Additional intuition concerning the effect of each X j  on Y can be also obtained if we 
consider increasing or decreasing one Xj at a time from a basic profile (e.g., the mean one). 
Although this does not directly provide the effect of the model parameters, it does provide 
an overall view of how Y is affected by each Xj. An advantage of this approach is that it 
can be easily comprehended by people with no quantitative background, in contrast to the 
other two approaches, which may appear as more technical. 
PP. 77). 
7.2 PRIOR DISTRIBUTIONS 
In this section we focus on prior distributions ofparameters p involved in the linear predictor 
of (7.1) given the dispersion parameter d. 
Usually, in normal prior distributions of the type 
Id 
N ( P &  
4) ' 
The variance depends on the dispersion parameter 0 in order to achieve an appropriate 
scaling of the prior distribution. In the normal case, if 
= a 
N IG(a, b) then we endup 
with the corresponding conjugate prior distribution discussed previously. When no prior 
information is available, the prior mean is set equal to zero, while the corresponding variance 
is set large to express prior ignorance. Alternatively, a prior independent to the dispersion 

240 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
parameter can be considered. When the variance is set large to express prior ignorance, 
then no differences in the resulting posterior distribution will be observed. 
Independent priors are plausible when the design or data matrix is orthogonal since, in 
such cases, model parameters have similar interpretation over all models. We can easily 
incorporate such priors in ANOVA-type models with sum-to-zero constraints. When we are 
interested in prediction rather than description of variable relations, we may orthogonalize 
the design matrix and proceed with model selection in the new orthogonal model space 
(Clyde et al., 1996). In nonorthogonal cases, especially when high dependences among 
covariates exist, the use of such a prior setup may result in undesirable influence on the 
posterior distribution and hence must be avoided. Independent prior distributions also result 
if we consider the prior of Knuiman and Speed (1 988) for Poisson log-linear models in high 
dimensional contingency tables using the sum-to-zero parametrization. 
This prior assumes prior independence between model parameters. This may be prob- 
lematic when prior information is available or when collinear covariates exist, resulting in 
an undesirable effect to the posterior distribution. For this reason, a multivariate normal 
prior can be considered instead with 
An extension of the Zellner's g-prior considered in normal models can also be adopted here 
if we set the prior variance covariance matrix equal to 
Ep = c'( - 
(7.5) 
where 3 is the maximum-likelihood estimate and H ( P )  is the second derivative matrix of 
log f(yIP; Q), which in GLMs is given by 
-H(P) = X T H X ,  
where H is a n x n diagonal matrix with elements 
2
1
 
h, = (2) 
a,(d)b"(l9) . 
Details concerning h, for some popular distributions are provided in Table 7.3. 
Table 7.3 Generalized linear model weights hi 
Model 
Link 
GLM weights hi 
Normal 
Identity 
0 
-2 
Poisson 
Log 
A, 
Binomial 
Logit 
N,T,(1 - TZ) 
Probit" 
N, [4 
- Tz) {P(7G)}2] 
-l 
-N,(l - T,) (log(1 - Tz)}2 
n,l 
clog-log 
"p(z) is the density function of standardized normal distribution evaluated 
at 2. 

POSTERIOR INFERENCE 
241 
A special case of the preceding multivariate normal prior distribution when the variance- 
covariance matrix (7.5) is the unit information prior for c2 = n. This prior has precision 
approximately equal to the precision provided by one data point. More detailed discussion 
of this prior can be found in Spiegelhalter and Smith (1988) and Kass and Wasserman 
(1995). 
For the normal distribution, since h, are all equal to the precision gP2 of the re- 
gression model, the resulting prior will be equivalent to Zellner's g-prior. For the re- 
maining models, h, depends on estimated parameter values for each case [e.g., h ,  will 
be equal to A, = exp (X(,)p) in the Poisson case and a function of the success rate 
?, = [l + exp(-X(,)P)]-l in the binomial case], resulting in a data-dependent prior. 
When the variance inflation parameter c2 is set equal to large, the effect of this dependence 
will be minimal since the prior will be essentially noninformative. To avoid this data depen- 
dence, Ntzoufras et al. (2003) proposedusing the prior mean to obtain rough prior estimates 
of h,. For example, in binomial logistic regression models the prior model weight is set 
equal h, = N, exp (X(,),up) 
[1+ exp (X(,),us)] 
- , while for the common case of a zero 
mean h, = N,/4. The latter is even more simplified to a prior variance covariance matrix 
equal to ED = 4IV1c2(XTX)-' when N, = N for all i = 1,2,. . . , N .  
h 
h 
h 
2 
7.3 POSTERIOR INFERENCE 
7.3.1 The posterior distribution of a generalized linear model 
For generalized linear models under the general setup (7.1) and responses Yi with probability 
or density function (7.2), the likelihood is given by 
+ c 
C(YZ. @I 
' 1 
Yzgi'(x(,)P) - b(sB1(X(,)B)) 
d d )  
,=1 
where we have assumed a common dispersion parameter for all observations. When differ- 
ent dispersion parameters are assumed, then we simply substitute Q by @ %  in the likelihood. 
Using the multivariate normal prior described in Section 7.2, we end up with the posterior 
which simplifies to 
-\r/ 
\ i = l  
when the dispersion parameter is fixed, as, for example, in binomial models, where f(d) 
in the full posterior is the prior of the dispersion parameter 4. 
This posterior and its corresponding summaries cannot be evaluated analytically, except 
for the normal model when using the conjugate prior described in Section 1 S.5. Although 

242 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
approximation methods can be used, MCMC methods are now available and widely used for 
the computation of the posterior distribution. Specifically, the Gibbs sampler can be easily 
applied because of the result obtained by Dellaportas and Smith (1993), which allowed 
for implementation of the adaptive rejection method of Gilks and Wild (1992) since the 
posterior distributions of the parameters in specific GLMs is log-concave. Alternatively, 
Metropolis-Hastings algorithms or the slice sampler can be used; see Chapter 2 for an 
example in logistic regression models. This method is also used in WinBUGSfor the 
generation of random values from the posterior distribution of GLMs. 
7.3.2 GLM specification in WinBUGS 
WinBUGS code for the specification of a GLM is similar to the corresponding one for 
the specification of a normal regression model. Differences lie in the specification of the 
stochastic component and the link function. 
The stochastic component will be now defined using the appropriate distribution for 
the response variable. Details concerning the distributions of the most popular GLMs are 
summarized in Table 7.4. Note that the inverse Gaussian distribution is not included in 
the standard distributions of WinBUGS ; however, it can be modeled using an alternative 
approach presented in Section 8.1. 
The link function can be enhanced facilitated by four link functions that are available in 
WinBUGS : log, logit, probit, and the cloglog. Commands for the specification of 
link functions in WinBUGS can be used only in the left part of the definition of the linear 
predictor. The remaining link functions can be defined by setting the parameter of interest 
8, equal to g-l(v,). 
7.4 POISSON REGRESSION MODELS 
In this section we focus on Poisson regression models for response variables defined in N . 
Such variables usually express the number of successes (visits, telephone calls, number of 
scored goals in football) within a fixed time interval. They are frequently called Poisson 
log-linear models because of the canonical log-link, which is widely used. 
The Poisson log-linear model is summarized by the following expression: 
P 
Y, N Poisson(&) with log X i  = PO + 
Pjjzij = X ( i l p  . 
j=1 
7.4.1 Interpretation of Poisson log-linear parameters 
In Poisson log-linear models, the effect of each X ,  is linear to the log-mean of Y ,  resulting 
in an exponential effect of X, on the mean of Y .  
Let us first consider the simplest case where only one covariate involved. Then the mean 
of Y can be expressed as 
log A, 
= Po + DlX, H 
A, 
= 
e30eP1z, 
= BOB:' where B, = @J 
for j = 0.1, 

Table 7.4 
WinBUGS commands for distributions within thc exponential familf 
Distribution 
WinBUGS 
Probability or 
name 
syntax 
density function f(z) 
Mean 
Variance 
I. Normal 
y - dnorm(mu,tau) 
(Log-normal)h 
y 
dlnorm(mu, tau) 
2. Binomial 
y 
dbin(p,N) 
(Bernoulli)' 
y - dbern(p) 
3. Negative binomial y - dnegbin(p,r) 
4. Poisson 
y - dpois (lambda) 
5. Gamma 
y - dgamma (a, b) 
(Chi-squared)d 
y 
dchisqr(k) 
(Exponential)" 
y - dexp(1ambda) 
"Tcrms in parcnthcses can be considered as special cascs of the distributions shown abovc. 
* log(y) follows the normal distribution. 
"Binomial with N = 1. 
'Gamma with a = k / 2  and b = k. 
'Gamma with (I = 1 and h = A. 
N 
P 
w 

244 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
where Bo denotes the expected counts (or Y )  when the covariate is equal to zero ( X  = 0). 
Interpretation of /31 is slightly different from the corresponding one in normal models since 
relative mean differences are considered in the Poisson case. Let us denote by A(z) = 
E ( Y / X  = z) the expected counts (Y) for covariate with X = 2. Then 
log (A(z + 1)) - log (A(”,) 
= 01. 
resulting in 
A(Z + 1) = B ~ A ( Z )  
= eSIA(z). 
Hence, when the covariate X is increased by one unit, then the expected Y becomes equal 
to B1 times the corresponding value of Y for X = z. An even more comprehensive 
interpretation can be based on the percentage change of the expected Y given by ( B  1 - 
1) x 100 when X increases by one unit. The type of association between X and Y is 
highlighted by the sign of /31 (and B1 - 1) as in the normal regression models. 
When X is categorical with K levels, then the linear predictor is expressed as a linear 
function of K - 1 dummy variables denoted by D ,  for j = 2. . . . , K .  Let us consider the 
simpler case of comer parametrization with the first one (j = 1) as the referencebaseline 
category and hence setting 
= 0. Then we express the model by 
K 
log 
= Po + C PIDt, * 
,=2 
K 
= BO n 
Bft3 where B, = epJ for j E {0,2,3,. . . , K } .  
,=2 
When individual i belongs in the first category of X (i.e., X ,  = l), then A, = Bo, 
while 
when individual z belongs in the kth category (k > 1) of X (i.e., X ,  = j), then 
A(x = k )  = BoBk = BkA(X = 1). 
Therefore, quantity Bk = eSk can be now interpreted as the relative change of the Poisson 
expectation A when an individual belongs in I; category of X compared to the baselineirefer- 
ence category. The interpretation for the STZ parametrization is similar, but all coefficients 
express the relative change of the current level compared with an overall “average” level 
instead of the baseline category used in comer parametrization. 
This is similar to the interpretation of the parameters in the multiple Poisson regression 
case. The difference here is that in every change of a single explanatory variable (say, X,), 
other covariates need to remain the constant since 
P 
A, 
= Bo n 
BT” with B, = eo2 for j = 0.1,2,. . . , p  . 
,=1 
In Bayesian inference, a usual point estimate for the model parameters is provided by the 
posterior means (or medians). Concerning the estimation ofrelative difference, the posterior 
distribution of B, and its posterior summaries may be considered directly by using a simple 
deterministicilogical node in WinBUGS . Alternatively, the exponent of the posterior mean 

POISSON REGRESSION MODELS 
245 
or median of p, can be used as sensible estimates since they correspond to the posterior 
harmonic mean and the posterior median of B, , respectively. Moreover, exponentiating 
either the posterior mode or any posterior quantile provides the corresponding posterior 
summaries for B,. Finally, calculation of the posterior standard deviation itself must be 
calculated directly by using deterministic (logical) nodes in WinBUGS given by B, = eP2. 
7.4.2 A simple Poisson regression example 
Example 7.1. Aircraft damage dataset. Here we consider the aircraft damage 
dataset of Montgomery et al. (2006). The dataset refers to the number of aircraft 
damages in 30 strike missions during the Vietnam war. Hence it consists of 30 
observations and the following four variables: 
0 damage: the number of damaged locations of the aircraft 
0 type: binary variable which indicates the type of plane (0 for A4; 1 for A6) 
bombload: the aircraft bomb load in tons 
airexp: the total months of aircrew experience 
In this example we can use the Poisson distribution to monitor the number of 
damages after each mission. 
Data of this example are available in the book’s Website and are reproduced with 
permission of John Wiley and Sons, Inc. 
7.4.2.1 
ing structure 
Model specification in WinBUGS. The initial model will have the follow- 
damage, 
N Poisson(X,) 
log A, 
= 81 + PZ type, + /33 bombload, + 
airexp, 
for 
a = 1.2,. . . .30. 
Here, the index of B, takes values from 1 to 4 (instead from 0 to 3 as in the previous section) 
to be in concordance with the WinBUGS the code that follows. 
We follow the same structure as in the linear regression model with the difference that 
the likelihood is now defined using the following syntax: 
Moreover, the exponentiated parameters Bj can be easily defined using the syntax 
The usual independent normal prior with large variance (
7
~
~
 
= ~7;~’ 
= 
is consid- 
ered as prior distribution for pJ . The full code is available in this book’s Webpage. 

246 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
7.4.2.2 Results. 
while 95% posterior intervals are depicted in Figure 7.1. 
Posterior summaries of model parameters are given in Table 7.5, 
Table 7.5 Posterior summaries of Poisson model parameters for Example 7.1' 
node 
beta111 
beta [21 
beta131 
beta [41 
B [11 
B [21 
B [31 
B [41 
mean 
sd 
-0.766 1.089 
0.580 0.466 
0.177 0.068 
-0.011 0.010 
0.862 1.221 
1.993 0.996 
1.197 0.081 
0.989 0.010 
MC error 2.5% 
median 97.5% 
0.1762 -3.168 -0.835 
1.619 
0.0513 -0.302 
0.584 
1.537 
0.0099 
0.040 0.177 0.308 
0.0015 -0.033 -0.010 
0.007 
0.1829 
0.042 0.434 5.050 
0.1050 
0.739 1.793 4.652 
0.0118 
1.041 1.193 1.360 
0.0015 
0.968 0.990 
1.007 
s t a r t  sample harmonic 
1001 1000 
1001 1000 
1001 
1000 
1001 1000 
1001 1000 
0.465 
1001 1000 
1.786 
1001 1000 
1.194 
1001 1000 
0.989 
aThe harmonic means of Bj are calculated outside WinBUGS using the posterior means of 4. 
caterpillar pld: beta 
-4 0 
-2 0 
0 0  
2 0  
Figure 7.1 95% posterior intervals of Poisson model parameters for Example 7.1 
A point estimate of the model can be based on the posterior means. Hence the a posteriori 
estimated model can be summarized by 
log Xi = -0.77 + 0.58 type, - 0.18 bombload, - 0.011 airexp, I 
From the 95% posterior intervals of /3j, we observe that only the posterior distribution of 
the bombload coefficient is away from zero, indicating a significant effect of this variable 
on the amount of aircraft damage. 
7.4.2.3 lnterpretation of the model parameters. Interpretation of the model pa- 
rameters can be directly based on Bj values (B 11 in WinBUGS). From Table 7.5, we may 
conclude the following 

POISSON REGRESSION MODELS 
247 
0 The expected amount of damage for A6 (type=l) aircraft is twice as much as the 
corresponding damage for A4 (type=O) aircraft when the two aircraft return from 
missions with aircrew of the same experience and both carry the same bombload. 
If we base our inference on the medians or the harmonic means, then the A6 is a 
posteriori expected to have 79% more extensive damage than an A4 aircraft with the 
same bombload and aircrew exprerience. 
by 20%. 
locations by 1%. 
0 Every tone of bombload increases the expected number of damaged aircraft locations 
0 Every additional year of aircrew experience reduces the number of damaged aircraft 
7.4.2.4 Estimating specific profiles. The expected amount of damage for the two 
types of aircraft for the minimum, maximum, mean and median profiles have also been 
calculated. In the minimum and maximum profiles, the maximum and the minimum val- 
ues of crew experience was considered, respectively, since these variables are negatively 
associated with the number of damaged locations. 
Calculation of the expected value for a profile can be easily accommodated in WinBUGS . 
For example, for a profile of an A6 aircraft, the expected amount of damage is calculated 
by 
where bombload .profile and airexp .profile are the values of the two explanatory 
variables for the profile that we wish to consider. Substitution of these nodes by appropriate 
values provides the desired profiles; see Table 7.6 for the corresponding code. The profiles 
for A4 are obtained similarly by removing parameter pz. Note that the minimum and 
maximum values of a vector w can be obtained using the commands ranked(v [I ,I> and 
ranked (v [I , n) , respectively. Similarly, the median profile can be calculated using the 
command 
Table 7.6 
each profile for Example 7.1 
WinBUGS syntax for calculation of expected number of damaged locations for 

248 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
if n is odd and by 
if n is even. 
Posterior means and the corresponding standard deviations of these profiles are provided 
in Table 7.7. For a typical mission with A4 aircraft we expect 0.8 damaged locations, while 
for A6 the corresponding number of damaged locations is about 1.3. Note that the worst- 
case scenario (maximum profile) where missions with 14 tons of bombload and crew with 
the minimum flying experience (50 hours) corresponds to an expected number of 3.7 and 
5.9 damaged locations for A4 and A6 aircrafts, respectively. 
Table 7.7 
for minimum, mean, median, and maximum profiles for Example 7.1 
Posterior means (standard deviations) of expected number of damaged locations 
Expected damage 
A4 
A6 
Profile 
Bombload 
Experience 
mean (SD) 
mean (SD) 
Minimum 
4.0 
120.00 
0.27 (0.13) 0.50 (0.27) 
Median 
7.5 
80.25 
0.75 (0.24) 1.22 (0.94) 
Mean 
8.1 
80.77 
0.83 (0.27) 1.33 (0.40) 
Maximum 
14.0 
50.00 
3.68 (2.11) 5.90 (1.75) 
7.4.2.5 Selection of variables using DIC. Here only three covariates are consid- 
ered, resulting in eight possible models. All models can be simultaneously fitted in Win- 
BUGS and then be identified as the best one (i.e. with the lowest DIC value). Code for 
fitting all models in a single run is provided in this book’s Website. Results are summarized 
in Table 7.8 after 10,000 burnin and 10,000 additional iterations. Be careful to consider a 
sufficiently long burnin period because DIC is sensitive to initial values. 
Table 7.8 DIC values for all eight models under consideration for Example 7.1a 
Dbar 
Dhat pD 
DIC 
yl 
108.6 107.6 1.01 109.6 
y2 
94.0 92.0 1.99 96.0 
y3 
84.8 82.9 1.91 86.7 
y4 
85.3 82.4 2.95 88.3 
y5 
106.2 104.3 1.97 108.2 
y6 
88.9 85.9 3.01 92.0 
y7 
83.9 81.0 2.92 86.9 
y8 
83.7 79.7 3.98 87.7 
total 735.6 715.9 19.76 755.4 
Model 
Constant 
Bombload 
Type + Bombload 
Airexp 
Type + Airexp 
Bombload + Airexp 
Type + Bombload + Airexp 
Type 
aBurnin= 10,000; iterations kept=10,000 
According to the lowest DIC (86.7) value, only the bombload must be retained in the 
linear predictor. Moreover, the DIC value (86.9) of the model with covariates for both the 

POISSON REGRESSION MODELS 
249 
bombload and the crew experience is very close to the lowest DIC value. This is an indication 
that the two models have similar predictive abilities, and therefore crew experience may 
also be an important determinant of the number of damaged locations. 
7.4.3 A Poisson regression model for modeling football data 
Example 7.2. Modeling the English premiership football data. Modeling of 
football scores is becoming increasingly popular nowadays. In the present example 
we use the English premiership data for the season 2006-2007 to fit a simplified 
Poisson log-linear model for the prediction of model outcomes. Data were down- 
loaded from the Webpage http://soccernet-akamai .espn.go.com. 
7.4.3.1 Background information and the model. The model was by Maher (1982) 
and was used by other authors, including Lee (1997) and Karlis and Ntzoufras (2000). Let 
us denote by yL1 and yz2 respectively the goals scored by home and away teams (HT and 
AT) in the ith game. Then the model can be expressed by 
yZ3 
N Poisson(X,k) 
for j = 1 , 2  
lOg(&1) = p + home f 
~ H T ,  + dAT, 
log(Xz2) = p 
f UAT, f dHT, for i = 1 . 2 , .  . . ,n, 
where n is the number of games, p is a constant parameter; home is the home effect; HT 
and AT, are the home and away teams, respectively, competing in the ith game; a k and d k  
are the attacking and defensive effects-abilities of k team for k = 1 , 2 , .  . . , K ;  and K is 
the number of teams in the dataset under consideration (here K = 20). 
For attacking and defensive parameters ( a k  and d k ) ,  we use the sum-to-zero constraints 
in order to make the model identifiable and compare the ability of each team with an overall 
level of attacking and defensive abilities. Hence we set 
K 
K 
a k  = 0 and 
d k  = 0. 
(7.7) 
k=1 
k=l 
According to this parametrization, all parameters have a straightforward interpretation. 
Parameter p denotes an overall level of log-expected goals scored in away games, while 
parameter home encapsulates the home effect denoted by the difference between the log- 
expected goals scoredwhen two teams of equal strength compete with each other. Attacking 
and defensive parameters a k  and d k  can be interpreted as deviations of the attacking and 
defensive abilities from the average level in the league. Hence, a positive attacking param- 
eter indicates that the team under consideration has an offensive performance that is better 
than the average level of the teams competing in the league. Similarly, negative defensive 
parameters indicate teams with defensive performance better than the average level of the 
teams competing in the league. 
The Poisson regression model adopted for the goals scored by each team conceals two 
important assumptions that are questionable in the sports modeling literature: the indepen- 
dence between home and away goals and the equality between the mean and the variance. 
Empirical evidence and exploratory analysis has shown a (relatively low) correlation be- 
tween the goals in a football game. This correlation can be incorporated in the analysis 
by modeling the full score using the bivariate Poisson distribution (Karlis and Ntzoufras, 

250 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
2003~) or by modeling the goal differences using Skellam’s distribution (Karlis and Nt- 
zoufras, 2008). These models are natural extensions of the simplified model presented here. 
Concerning the Poisson assumption, slight overdispersion in literature; see Karlis and Nt- 
zoufras (2000) for a discussion. This can be incorporated to the model using a negative 
binomial distribution; see, for example, Reep and Benjamin (1968), Reep et al. (1971), and 
Baxter and Stevenson (1988). 
Another problem appearing in football data is the excess of specific scores, especially the 
0-0 and 1-1 draws. Dixon and Coles (1997) provided an extension based on the Poisson 
model allowing for extra probabilities in these scores. In a similar fashion, Karlis and 
Ntzoufras (2003~) have proposed using a diagonal inflated bivariate Poisson model and 
more recently a zero inflated model for the goal differences (Karlis and Ntzoufras, 2008). 
Other models include the dynamic model for paired data by Fahrmeir and Tutz (1994) and 
the state space model of Rue and Salvesen (2000). Related models and optimal prediction 
schemes have been applied by Kuonen (1996, 1997) for European national football club 
tournaments and by Kuonen and Roehrl(2000) for the France’98 World cup data. 
7.4.3.2 Model specification in WinBUGS. In this example we use four variables: 
the home and away goals and the home and away teams (using codes from 1 to 20), termed 
goalsl, goals2, ht, and a t  in WinBUGScode which follows. The model can be 
specified by 
Note that the STZ constraints (7.7) can be imposed in WinBUGS by setting one set of 
parameters effects (e.g., here we use k = 1) equal to 
K 
K 
a1 = - c 
ak and dl = - c 
d k  
k=2 
k=2 
using the WinBUGS syntax 
Prior distributions must be defined for the remaining parameters: p, home and a k ,  d k  
for k = 2, . . . , K .  The usual normal low-information prior with zero mean and large 
prior variance are used here (with prior precision equal to 10 -4). A large amount of 
historical data are available in sports. Such information can be used to form a plausible 
prior distribution, which will be extremely useful in the first weeks of a competition when 
a limited amount of data are available. Elicitation of such historical data and their potential 
usefulness, especially in the beginning of each season (where limited data are available), 
must be carefully examined. 
7.4.3.3 Results. Posterior summaries of the Poisson log-linear model parameters are 
provided in Table 7.9. Posterior credible intervals of attacking and defensive parameters 
for each team are depicted in Figures 7.2 and 7.3. As we can see from the estimated model 

POISSON REGRESSION MODELS 
251 
parameters, Manchester United had the highest attacking parameter while Chelsea had the 
lowest (i.e., best) defensive parameter. In a game where two average teams are competing 
each other, then the expected numbers of goals are equal to 1.32 for the home team and 
0.90 for the away team, resulting in an increase of 46% of each team scoring mean when 
playing in its home field. 
Table 7.9 Posterior summaries of expected number of goals for Example 7.2 
Posterior 
Posterior 
percentiles 
percentiles 
Team" 
Node 
Mean 
SD 
2.5% 97.5% Node 
Mean 
SD 
2.5% 97.5% 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
17. 
18. 
19. 
20. 
Arsenal 
Aston Villa 
Blackburn 
Bolton 
Charlton 
Chelsea 
Everton 
Fulham 
Liverpool 
Man City 
Man Utd 
Middlesbrough 
Newcastle 
Portsmouth 
Re ad i n g 
Sheff Utd 
Tottenham 
Watford 
West Ham 
Wigan 
0.33 
0.12 
0.08 
0.57 
-0.05 
0.15 
-0.34 
0.23 
0.16 
0.14 
-0.13 
0.42 
0.06 
0.14 -0.23 
0.32 
-0.26 
0.16 -0.61 
0.04 
0.33 
0.12 
0.08 
0.56 
0.14 
0.14 -0.13 
0.41 
-0.14 
0.16 
-0.47 
0.16 
0.22 
0.13 
-0.04 
0.47 
-0.44 
0.18 
-0.82 
-0.09 
0.60 
0.11 
0.38 
0.81 
-0.02 
0.15 
-0.33 
0.26 
-0.16 
0.16 
-0.48 
0.13 
0.00 
0.15 
-0.29 
0.29 
0.15 
0.14 
-0.13 
0.40 
-0.33 
0.17 
-0.68 
-0.01 
0.26 
0.13 -0.00 
0.51 
-0.42 
0.18 
-0.78 
-0.07 
-0.24 
0.16 
-0.56 
0.07 
-0.19 
0.16 
-0.51 
0.11 
-0.23 
0.16 -0.57 
0.08 
-0.10 
0.16 
-0.42 
0.19 
0.20 
0.14 
-0.08 
0.45 
0.15 
0.14 -0.13 
0.41 
0.28 
0.13 
0.03 
0.52 
-0.62 
0.20 
-1.04 
-0.25 
-0.22 
0.17 
-0.56 
0.09 
0.29 
0.13 
0.02 
0.53 
-0.51 
0.19 
-0.90 
-0.16 
-0.04 
0.15 
-0.34 
0.24 
-0.47 
0.19 -0.85 
-0.12 
0.09 
0.14 -0.20 
0.35 
0.03 
0.14 
-0.26 
0.31 
-0.07 
0.15 
-0.37 
0.22 
0.05 
0.15 -0.24 
0.33 
0.19 
0.13 
-0.08 
0.44 
0.20 
0.13 -0.06 
0.45 
0.25 
0.13 
-0.01 
0.50 
0.27 
0.13 
0.01 
0.51 
0.27 
0.13 
0.01 
0.51 
home 
0.38 
0.07 
0.25 
0.51 
p 
-0.10 
0.05 -0.20 
0.003 
Abbreviations: Man = Manchester; Utd = United; Sheff = Sheffield; Ham = Hampshire. 
7.4.3.4 Prediction of future games, Models in sports are used mainly for predic- 
tion. Here we briefly illustrate how we can obtain predictions for two future games. The 
approach can be easily generalized to additional games using the same approach. 
To illustrate the implementation in WinBUGS , we have substituted the scored goals 
in the last two games (Tottenham - Manchester City and Watford - Newcastle) by NA. 
WinBUGS automatically will generate values for the missing goals from the predictive 
distribution (see Chapter 10 for more details) and will provide estimates for each score by 
monitoring the nodes goals1 and goals2. 
Posterior summaries of the predicted scores are given in Table 7.10. As we can see in 
both of these games, posterior means indicate that the observed actual score was expected 
under the fitted model. 
Table 7.10 
Posterior summaries of the expected scores for last two games of Example 7.2 
Posterior summaries 
Actual 
Posterior 
of goal difference 
Home team Away team 
score 
Median 
Mean 
Mean 
SD 
95% CI 
379. Tottenham 
Manchester City 
2-1 
1-1 
1.65 - 0.72 
0.93 
1.59 
(-2,4) 
380. Watford 
Newcastle 
1-1 
1-1 
0.93 - 1.02 
-0.09 
1.43 
(-3, 3) 

252 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
:aterpillar plot: a 
-1 .o 
-0.5 
0.0 
0.5 
1 .o 
Abbiaeviations: Man = Manchester; Utd = United; Sheff = Sheffield; Ham = Hampshire. 
Figure 7.2 95% posterior intervals for team attacking parameters for Example 7.2. 
:aterpillar plot: d 
I2IArton Villa 
- 
1141 Portsmouth 
lW1 Man City 
,131 Newcaatle 
1151 Reading 
IU2lMlddlesbrough 
I41 Bolton 
1161 Sheff Uid 
131aLlaSkBKn 
I’’1TQuwLlm 
llal Watford 
lVQJ 
West Ham 
151 Charlton 
181 Fulham 
- 
1201 Wlgan 
-1.5 
-1 .o 
-0.5 
0.0 
0.5 
1 .o 
Abbreviations: Man = Manchester; Utd = United; Sheff = Sheffield; Ham = Hampshire. 
Figure 7.3 95% posterior intervals for team defensive parameters for Example 7.2. 

POISSON REGRESSION MODELS 
253 
Interest also lies in calculating in the probability of each outcome (winidrawiloss), which 
can be easily accommodated in WinBUGS using the following syntax: 
In this syntax, the elements of outcome are binary indicators denoting the win, draw, and 
loss of the home team in each column, respectively. Using similar syntax we can also 
estimate the probabilities of the expected differences. The syntax now is given by 
In this syntax pred . dif f . counts is again a matrix with binary elements indicating which 
difference appears in each MCMC iteration. Elements 2-12 denote differences from -5 
to 5, while the first and last elements denote differences lower than -5 and higher than 5, 
respectively. 
Posterior probabilities of each predicted outcome and each value of the goal difference 
are summarized in Tables 7.11 and 7.12. Outcome probabilities indicate that Tottenham’s 
probability of winning the game against Manchester City was about 60%, with a posterior 
mode of one goal difference. Concerning the second game (Watford vs. Newcastle), the 
posterior model probabilities confirm that the two teams have about equal probabilities of 
winning the game. 
Table 7.11 
Posterior probabilities of each game outcome for last two games of Example 7.2 
Posterior Probability 
Actual Home 
Away 
Home team Away team 
score 
wins 
Draw 
wins 
379. Tottenham 
Manchester City 
2-1 
0.59 
0.24 
0.17 
380. Watford 
Newcastle 
1-1 
0.33 
0.30 
0.37 
7.4.3.5 Regeneration of the full league. Interest also lies in reconstructing the 
league using the predictive distribution. Such practice is useful to evaluate whether the 
final observed ranking was plausible under the fitted model. It can be interpreted as the 
uncertainty involved in the final ranking if the league is repeated and the model is true; see 
Karlis and Ntzoufras (2008). 
In order to reconstruct the full table in WinBUGS , we need to replicate the full scores 
in a tabular K x K format and then calculate the number of points for each team. The 
replicated league (in tabular K x K format) is calculated using the following syntax: 

254 
Table 7.12 
Example 7.2 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Posterior probabilities of each game goal difference for last two games of 
Home 
Away 
Actual 
Posterior Probability of goal difference 
Team 
Team 
Score 5 -3 
-2 
-1 
0 
1 
2 
3 
4 
2 5 
379. Tottenham ManCity 
2-1 
0.012 0.036 0.119 0.242 0.257 0.185 0.090 0.037 
0.023 
380. Watford 
Newcastle 
1-1 
0.042 0.108 0.218 0.303 0.210 0.086 0.024 0.006 
0.002 
~ 
~~ 
~ 
aBoldface indicates the maximum probability and the corresponding posterior mode of the difference 
Abbreviations: Man = Manchester. 
The total number of points is calculated using the following syntax: 
In this syntax, points1 and points2 calculate the number of points in each game for 
the home and the away teams, respectively (arranged in i = 1.2.. . . , K rows and j = 
1.2, . . . , K columns). In the total points we calculate the total number of points for each 
team i earned in home games (sum of i row of node pointsl) and in away games (sum of 
j column of node points2). Diagonal elements points1 [ i ,  il and points2 [ i ,  il are 
removed since they refer to each team playing against itself. 
Posterior summaries of the total predicted earned points are obtained as usual 
(sample monitor tool), while posterior summaries of the ranks are obtained using the 
rank monitor tool. Results are summarized in Table 7.13 after 5000 iterations kept 
(and an additional 1000 iterations removed). Ranks refer to the total number of points in 
ascending order. Hence 20 refers to the team with the highest number of collected points 
(i-e., the champion), while one (1) refers to the team with the lowest number of collected 
points (i.e., the worst team in the league). The league was reproduced successfully. No 
differences are observed in the first four teams, while minor changes are observed for the 
remaining positions. 
Further, probabilities for each ranking can be obtained by calculating the ranks using the 
commands 
and then calculating 

BINOMIAL RESPONSE MODELS 
255 
Table 7.13 Observed and predicted (by model) points and rankings for all teams 
Predicted 
Posterior summaries for 
Posterior percentiles 
(actual) 
Actual 
total points 
for points ranksb 
ranking" Team 
points Mean SD 2.5% Median 97.5% 2.5% Median 97.5% 
1 (1) 
2 (2) 
3 (3) 
4 (3) 
5 (6) 
6 (8) 
7 (5) 
8 (9) 
9 (11) 
10 (10) 
11 (7) 
12 (12) 
13 (13) 
14 (14) 
15 (16) 
16 (17) 
17 (15) 
18 (18) 
19 (19) 
20 (20) 
Man Utd 
Chelsea 
Liverpool 
Arsenal 
Everton 
Reading 
Tottenham 
Portsmouth 
Aston Villa 
Blackbum 
Bolton 
Middlesbrough 
Newcastle 
Man City 
Fulham 
Wigan 
West Ham 
Sheff Utd 
Charlton 
Watford 
89 
84.7 8.1 68 
83 
78.3 8.8 
60 
68 
72.6 9.2 54 
68 
69.9 9.4 51 
58 
62.8 9.4 
44 
55 
55.8 9.6 
37 
60 
56.0 9.6 36 
54 
54.3 9.6 
36 
50 
53.4 9.7 34 
52 
51.9 9.6 33 
56 
49.6 9.3 
32 
46 
49.2 9.4 32 
43 
46.0 9.4 
28 
42 
40.8 9.0 
24 
39 
39.3 8.9 
22 
38 
38.8 9.1 
22 
41 
37.5 8.7 21 
38 
37.2 8.8 21 
34 
36.4 8.9 
19 
28 
33.2 8.6 
17 
85 
99 
16 
20 
20 
79 
94 
14 
19 
20 
73 
90 
12 
18 
20 
70 
88 
10 
17 
20 
63 
81 
7 
15 
19 
56 
75 
4 
13 
18 
55 
73 
4 
12 
18 
54 
73 
3 
12 
18 
53 
73 
3 
12 
18 
52 
70 
3 
11 
17 
49 
69 
2 
10 
17 
49 
68 
2 
10 
17 
46 
65 
1 
8 
16 
40 
59 
1 
6 
14 
39 
57 
1 
5 
13 
39 
57 
1 
5 
13 
37 
55 
1 
4 
13 
37 
55 
1 
4 
12 
36 
55 
1 
4 
12 
33 
51 
1 
3 
11 
aPredicted ranks are calculated using the median rank and then the mean points. 
bRanks here refers to the number of points in ascending order (e.g., 20 denotes the best team and 1, the 
worst team in terms of collected points). 
Abbreviations: Man = Manchester; Utd = United; Sheff = Sheffield; Ham = Hampshire. 
which can be used to calculate the probability of each team's ranking. From the results 
provided in Table 7.14, we observe that Manchester United was clearly better than the 
other teams since its probability of winning the league was about 60% versus 25% for 
Chelsea, which ended up second in the league. More detailed analysis on the topic, using 
the Skellam's Poisson difference distribution, can be found in Karlis and Ntzoufras (2008). 
7.5 BINOMIAL RESPONSE MODELS 
Binomial data are frequently encountered in modern science, especially in the field of 
medical research, where the response is usually binary, indicating whether a person has a 
specific disease. The most popular model in this case is the logistic regression model, in 
which the usual logit link is adopted. The logit link is not only the obvious choice since it is 
the canonical link but also has a smooth and nice interpretation based on the ratio T / (  1 - n), 
which is denoted the odds of Y = 1 versus Y = 0, where n is the probability of success 

256 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Table 7.14 
probabilities of each position (in %) 
Posterior mean, standard deviation of final league ranks, and posterior 
Posterior 
Posterior probability of each ranking' 
Node 
mean SD 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
Man Utd 
1.7 1.1 60 23 10 4 1 1 
Chelsea 
2.6 1.6 253321 11 6 2 1 1 
Liverpool 
3.7 2.0 10 21 25 18 11 7 3 3 1 1 
Arsenal 
4.3 2.3 6 15 21 21 13 9 6 4 2 1 1 1 
Everton 
6.2 2.9 
1 5 10 16 17 13 10 8 6 5 3 2 1 1 1 
Reading 
8.5 3.6 
2 4 6 1 0 1 2 1 1 1 1 9  9 7 6 4 3 3 2 1
1
 1 
Tottenham 
8.8 3.6 
1 3 6 10 11 12 10 10 9 7 6 4 4 3 2 1 1 1 
Portsmouth 
9.0 3.7 
1 2 6 8 11 10 11 9 9 8 6 5 4 3 2 2 1 1 
Astonvilla 
9.5 3.8 
1 2 5 7 9 10 11 10 9 8 7 6 5 4 3 2 2 1 
Blackbum 
9.9 3.8 
1 2 4 7 8 9 9 10 10 9 8 7 5 5 3 2 1 1 1 
Bolton 
10.8 3.8 
1
2
4
6
7
9
9
9
9
9
8
7
6
4
3
2
2
1
 
Middlesbrough 11 .O 3.9 
1
2
4
6
7
8
9
9
9
9
9
7
5
5
4
3
2
1
 
Newcastle 
12.3 3.9 
1
2
 3 5 6 8 8 9 9 1 0 8  9
6
 6
4
4
2
 
ManCity 
14.4 3.7 
1
1
 2 3 4 6 6 7 8 9 1 0 1 1 1 0 9  8 6 
Fulham 
14.9 3.6 
1 2 2 3 4 5 7 8 9 9 1 0 1 1 1 1 1 0  8 
Wigan 
15.1 3.6 
1 1 2 2 4 4 5 5 8 1 0 9  1 0 1 1 1 0 1 0 1 0  
WestHam 
15.6 3.4 
1
1
 1
2
 3 4 6 7 8 9 1 1 1 1 1 2 1 2 1 1  
SheffUtd 
15.8 3.3 
1
2
 3 3 4 5 7 8 9 1 0 1 2 1 3 1 2 1 2  
Charlton 
16.0 3.3 
1
2
 2 3 3 5 6 8 8 1 0 1 2 1 2 1 4 1 5  
Watford 
17.1 2.9 
1 1
2
 2 3 4 5 8 8 1 0 1 4 1 8 2 4  
aBoldface indicates highest probability for each team. Posterior percentages were rounded to 
the closest integer, while percentages < 0.5% were omitted. Sums of probabilities for each 
column are slightly higher than loo%, due to ties. 
Abbreviations: Man = Manchester; Utd = United; Sheff = Sheffield; Ham = Hampshire. 

BINOMIAL RESPONSE MODELS 
257 
for Y .  The logistic regression model model can be summarized by 
for i = 1,2, . . . 
~ TI . For Ni = 1, we have the case where Y, is binomial. Other frequently 
used link functions are the probit and clog-log links. 
7.5.1 Interpretation of model parameters in binomial response models 
7.5.7.7 
Interpretation ofthe parameters in logistic regression 
models is based in the notion of odds and odds ratios. We define as odds the relative 
probability of success (Y = 1) compared to the probability of failure (Y = 0) when we 
refer to binomial data. Hence 
Odds and odds ratios. 
T 
odds = - 
1-7T 
while the logistic model can be rewritten as 
using the odds representation, 
The interpretation of odds is relatively simple and straightforward. It provides the number 
we need to multiply the probability of failure in order to calculate the probability of success. 
For example, odds = 2 implies that the success probability is twice as high as the failure 
probability while odds = 0.6 implies that the success probability is equal to 60% of the 
failure probability. The value of 1 is of central interest since it implies that the probabilities 
of both outcomes are equal to 0.5. Values of odds higher than one (> 1) indicate an 
increased probability of success in contrast to the failure probability (T > 0.5), while 
values lower than one (< 1) indicate a probability of success lower than the probability of 
failure (T < 0.5). Quantity (odds - 1) x 100 provides the percentage increase or decrease 
(depending on the sign) of the success probability in comparison to the failure probability. 
For example, a value of odds = 1.6 indicates that the success probability is 60% higher 
than the corresponding failure probability. Similarly, a value of odds = 0.6 indicates that 
the success probability is 40% lower than the corresponding failure probability; see also 
Table 7.15, which summarizes the interpretation of odds. 
The ratio of two odds of two different outcomes are called odds ratios (OR) and provide 
the relative change of the odds under two different conditions (denoted by X = 1 , 2  and 
subscripts 1 and 2): 
odds(X = 1) 
odds(X = 2 )  * 
OR12 = 
where odds(X = z) denotes the conditional success odds given that X = z: 
P(Y = 1IX = z) 
P(Y = OIX = z) 
odds(X = z) = 
When OR12 = 1, then the conditional odds under comparison are equal, indicating no 
difference in the relative probability of Y under X = 1 and X = 2. Using similar approach, 
(OR12 - 1) x 100 provides the percentage change of the odds for X = 1 compared with 
the corresponding odds when X = 2 ;  see Table 7.16 for additional details. 

258 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Table 7.15 Summary interpretation table for odds 
Table 7.16 Summary interpretation table for odds ratios 

BINOMIAL RESPONSE MODELS 
259 
Odds ratios (and logistic regression models) became very popular, especially in medical 
research, because they are good approximations of the relative risk when the prevalence of 
the disease is low [e.g., < 10% (Rosner, 2005)l. Hence we can summarize relative risk as 
follows: 
RR12 = - x OR12 when TI, ~2 small. 
Moreover, the OR can be estimated in both prospective and retrospective studies while RR 
is limited to prospective studies; for more details on this aspect, see Schlesselman (1982, 
chap. 8) and Hosmer and Lemeshow (2000). These two facts findings odds ratios and 
logistic regression models as the main tools in medical research and biostatistics. 
7.5.1.2 Logistic regression parameters and odds ratios. Interpretation can be 
based on the exponents Bj of the original model parameters PJ in the same way as in the 
Poisson log-linear models. Now Bj are directly associated with odds ratios since 
Tl 
T 2  
in the simple logistic regression case with one arithmetic covariate. Therefore B 1 = e31 
denotes the relative odds magnitude when X increases by one unit. Note that for X ,  = 
-OO/&, 
we can calculate the value of X for which both probabilities are equal to 0.5. 
This value may be used as a threshold for prediction or, for example, for diagnosing future 
patients using the X variable directly. 
Similarly, when X is a categorical variable with K levels and the comer parametrization 
is adopted with the first level as baselineireference category, then 
K 
K 
log (odds(z)) = Po + C D31(z = j) = Po + C P 3 D 3  + 
3=2 
3=2 
K 
odds(z) = Bo 
BF + 
3=2 
Therefore, in logistic regression models, parameter B3 is the success odds ratio for the jth 
category of X versus the reference category of the same variable. 
Extension of the interpretation above to multiple logistic regression models is straight- 
forward. We only need to interpret each B, as the change of Y when a single covariate X j  
increases by one unit while the other covariates remain constant. Odds ratios estimated via 
multiple logistic regression models are often reported as “odds ratios adjusted for” the other 
covariates or “odds ratios after controlling for the effect” of the other covariates. Adjusted 
odds ratios estimate the joint effect of all covariates X ,  ( j  = 1,2, . . . , p )  on Y ,  and in this 
way we essentially calculate the effect of every covariate after the elimination of the effect 
of the other covariates. 
7.5.1.3 Parameter interpretation in probit models. Parameter interpretation in 
probit models is not as straightforward as the corresponding interpretation in logit models. 

260 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Interpretation in probit models can be based on the latent variable representation of the 
model [see, e.g., Powers and Xie (1999, pp. 59-61)]. With this approach, we assume a 
standardized normal latent unobserved variable 
P 
2, = qt + ~i = 00 + 1 
&Xi, + ~i = X(ilp + ~
i
,
 
with ~i N N(0: 1): 
(7.8) 
(for i = 1 . 2 , .  . . , n) which specifies the observables Yi by setting Y, = I(Zi > 0) (i.e., 
Y, = 1 for Zi > 0 and Y, = 0 when Zi < 0). From this, we obtain 
j=1 
P(K = lIX(2)) = P(Zi > 0) = 1 - P(Z2 5 0) = 1 - P(Zi - X ( q p  I -x,,,p) 
1 - @ (  - x,,,p) = @(x(i)p) 
= @(Vi) . 
= 
Note that in the simple case with one covariate, for T = 0 we have the critical value of 
xc = -Do/PI 
as in the logit function. 
The latent variable approach can also be used for the logit link (using the logistic distri- 
bution) and for the clog-log link (using the extreme value distribution), which follows. 
Under the normal latent variable approach, the coefficients of the probit model indicate 
the expected change of the latent variable Z with one unit increase of X. Generally we can 
write 
n(z + 1) = @(go + $l(Z + 1)) = @(Po + PlZ + Pl) = @ ( @-l (4.1) 
+ 31 ) i 
where ~ ( z )  
= P(Y = 1lX = z). A plausible interpretation of P1 can be obtained if we 
consider the value of x = x, = -Do/P1, for which we obtain 
n ( x = z c + l ) = T  ( x=--+1 
P1 
PO ) - @  
- ( a-1 (2, 
- +pl )=@(Pd 
For example, if 31 = 0.5,1,2,3 the probability of Y becomes equal to 0.69, 0.84, 0.977, 
and 0.99865, respectively, corresponding to an increase in the probability by 38%, 68%, 
95%, and 100% when X increases from -POIPI to -Po/Ol + 1. Therefore 
is directly 
associated with the probability of Y when X = z + 1. 
This logic can be easily generalized for calculating the deviations for changes of the 
linear predictor by one unit of X, in the multiple probit model. In such a case the linear 
predictor from q* will become equal to q* + P,, resulting in 
n(X, =z+LX,,) =@(v*+P,), 
where X\, denotes all variables except X, and n(z) = P(Y = 1lX = x). For q* = 0 
(i.e., the corresponding success is probability is equal to T = i), 
then the success probability 
becomes equal to 7r(q = P,) = @(I!?,). Hence D, is associated with the success probability 
when X, increases by one unit compared to an individual with success probability $ (and 
linear predictor q = 0). 
Alternatively, we may base our inference on the marginal effect of X, on 7r given by 
This quantity approximates the effect of one unit increase of X, on the success probability 
7r. Since this approximation is valid only for small deviations of X, from the current value 
of x, we may consider 
An x p(q)P, AX, for small AX,, 

BINOMIAL RESPONSE MODELS 
261 
for example, AX, = 0.1. For the case where 7 = 0, then the equation above becomes 
equal to 
where n = 3.14159. Hence, Pj is now directly related to the change of probability when 
7i = 0.5, and X, increases by a small change denoted by AXj. For the values of 0.5; 1,2, 
and 3 used above, the increase in probability is approximately equal to 0.0199, 0.0399, 
0.080, and 0.120 for an increases of X j  by 0.1 while the corresponding actual increases is 
equalto0.0199,0.0398,0.079,and0.118. 
In WinBUGS , the posterior distributions of both quantities discussed above can be 
calculated by specifying the corresponding logicalideterministic nodes. 
7.5.1.4 Relationship between logit and probit parameters. Both link functions 
have the same symmetry properties. Differences in the estimated probability values x i of 
the two models are very close. 
Following Aldrich and Nelson (1984, p. 41), we can obtain sufficient approximation of 
the logit parameters multiplying the probit parameters by a factor equal to n/ & = 1.81. 
This approximation results from considering the variance of the the latent variables for the 
logit and probit models, which are equal to n2/3 and one, respectively. Dividing the latent 
variable of the logit model by the factor n/ & provides the same variance as in the probit 
model. Then, assuming that Zprobit NN Zl0git/(n/&) results in 
$ogit - n probit - 
3 -$ - 1.81 x pTrobit 
A more accurate approximation can be based on the Taylor expansion, resulting in 
where L1 and L2 are the two link functions we wish to compare, gL(.rr) and g;(.ir) are the 
link function and the corresponding first derivative for L link, and x 0 is a constant used in 
the approximation; see eq. (6) in Ntzoufras et al. (2003). For the the logit and the probit 
link functions, this results in 
which for the default choice of T = 
which is the value also supported by Ameniya (1981, eq. 2.7). This approximation works 
reasonably well when no extreme probabilities exist. An even better approximation results 
using as T O  the proportion estimated by the available sample. Using either of these approx- 
imations, we can directly obtain an odds ratio interpretation for the parameters of the probit 
link (or any other link). 
Note that the association of the constant terms is slightly different [see eq. 5 in Ntzoufras 
et al. (2003)l but it is not of central interest, and hence we do not pursue this issue further 
in this section. 

262 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
7.5.1.5 Parameter interpretation in log-log and clog-log models, The com- 
plementary log-log (clog-log) model is given by the expression 
P 
log ( - log( 1 - Ti)) = 772 = Po + c 
pjxij . 
j=1 
Ifwe consider this model for the failure probability (i.e., set Y," = 1 - Yl), then the model 
becomes equal to 
3=1 
which is referred as the log-log model. Using the clog-log model for the success probability 
is equivalent to using the log-log link for the failure probability and vice versa. 
Complementary log-log link model assumes a latent variable Z, of the form (7.8) with 
errors E, following a standard extreme value distribution with density and distribution func- 
tion 
f ( ~ , )  
= e'% exp ( - e E z )  and F(E,) = 1 - exp (-eEt) . 
respectively. To be more specific, E ,  follows the Gumbel (or extreme value) distribution 
with parameters a = 0 and b = 1 and mean and variance equal to 0.577 and n 2/6 = 1.645, 
respectively (Agresti, 2002, pp. 248-250). 
For both the logit and the probit links, the corresponding errors of the latent variables are 
symmetric. This assumption is restrictive, and in cases where it is not realistic, the clog-log 
link may be more appropriate (Agresti, 2002, p. 248). 
Interpretation can be obtained by the expression 
[I - (x + 1) ] 
= exp (-ePO+P1(z+l) 
= exp ( - e ~ o + ~ l z + ~ 1  
1 
= exp (-eSo+PlzeP~) = exp (-eSo+Plz ) 
2 1  
= [l-.(x,] 
in the case with one covariate and by 
in case of p covariates. Hence for each probability T ( X )  with covariate values X ,  an 
increase of variable X j  by one unit changes the failure probability by 
For T = f and 
0.006 and 0.9 x 
resulting in 
= 0.5,1,2 and 3, the failure probability becomes equal to 0.32, 0.15, 
Association between the logit and clog-log link can be obtained using (7.9) with 7r 0 = f , 
decreasing by 36%, 69.6%, 98.8%, and 99.9%, respectively. 
clOg-lOg = 210&0git 
= 1.386 x /j'yt, 
6 

BINOMIAL RESPONSE MODELS 
263 
This approximation is accurate only for samples with no extreme values. Finally, the 
marginal effect is given by 
& 
a(1 - exp(-e’l)) 
DJ =exp(-eq++)B, = ( 1  - ~ ) l o g ( l - ~ ) P ~ ,  
817 
--Pj 
= 
a7 
which for i7 = and a change equal to AX, becomes equal to 
1 
1 
2 
AT (i) M -? log2pJAXj = -- log2&AXj = -0.34657 x & A X j  . 
A summary of the interpretation details for binomial response model coefficients pre- 
sented in Sections 7.5.1.2-7.5.1.5 is provided in Table 7.17. 
7.5.2 A simple example 
Example 7.3. Analysis of senility symptoms data using WinBUGS (data were 
originally analyzed in Example 2.3). In Section 2.3 we have illustrated the 
Metropolis-Hastings algorithm using a simple example in which 54 elderly people 
completed a subtest of the Wechsler Adult Intelligence Scale (WAIS) resulting to 
a discrete score with ranging from 0 to 20. The aim of this study was to identify 
people with senility symptoms (binary variable) using the WAIS score. Moreover, 
we were interested in calculating the threshold value of X for which T > 0.5 to 
enable us to identify possible patients directly using X. 
7.5.2.7 Model specification in WinBUGS . Here the response (senility symptoms) 
is binary, and hence the Bernoulli or the binomial with n = 1 distributions can be used 
to model the response variable. The explanatory variable 5 is the WAIS score, which is a 
discrete quantitative variable. 
Specification of the likelihood for the logit model proceeds as usual by the following 
syntax 
where n = 54. Alternatively, the Bernoulli distribution (command dbern (pi [il 1) can be 
used instead of the binomial with n = 1 without problem. 
Moreover, we may also define the e@J in order to interpret directly the posterior distri- 
butions of the odds and odds ratios. This can be easily achieved in WinBUGS by defining 
the corresponding logical (or deterministic) nodes using the commands 
The value of X = Z(T = 0.5), which corresponds to disease probability equal to 0.5, can 
be defined using the same manner since solving the equation 0 = + PIX = Z(T = 0.5) 
results in X = Z(T = 0.5) = -PO/&. WinBUGS syntax is now given by 

264 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Table 7.17 Summary interpretation table for binomial response model coefficients. 

BINOMIAL RESPONSE MODELS 
265 
Using the same approach, we may define X values for other probabilities (e.g., 0.25 or 
0.01). 
To define the probit and clog-log models, we only need to substitute the WinBUGS fbnction 
l o g i t  (pi [ill by 
the 
corresponding link 
commands probit (pi [ill and 
cloglog(pi [il ),respectively. Other, more complicated, link functions can be defined by 
expressing 7r2 as a function of the linear predictor 17%. Note that, for this example, arithmetic 
overflows occurred when using the probit link of WinBUGS . In order to avoid them, we 
have truncated the tails at (-[, <), [ > 0, of the probit link; see the computational note that 
follows. 
To facilitate parameter interpretation in probit models, we calculate the following quan- 
tities: 
Disease probability when 7r = $ and X j  is increased by one unit = @(pj) using the 
syntax 
Marginal effect from 7r = 
is defined to be equal to 0.4 x pj for AX, = 1 using 
the syntax 
The latter expression is the actual increase of the probability when 7r = 0.5 and x 
increases by one unit. 

266 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Approximate OR interpretation: <2 x ,!?j is given by the syntax 
The threshold value 2 ,  = -i30//3l for the probit link is the same as in the logit one and, 
therefore, can be obtained using exactly the same commands. 
Concerning the clog-log model, overflow problems appeared similar to those occurring 
for the probit link. Hence, we also propose to truncate the clog-log function outside the 
interval (-[I> <2), [I >> 6 2  > 0. This interval is not symmetric around zero since the 
complementary log-log function approaches zero much more slowly than it approaches 
one. The syntax needed for this truncation as well as details are presented in the following 
computational note. 
Finally, to facilitate interpretation of the clog-log parameters, we use the following 
incr.prob.pi.half <- exp( -exp(betal) ) 
0 marg.effect.pi.half <- -0.35 * betal 
WinBUGS syntax 
act.marg.effect.pi.ha1f <- exp( -exp(betal) ) - 0.5 
approx.or <- 1.39 * betal 
Using this syntax, we can calculate the disease probability when T = $ and WAIS is 
increased by one unit, the marginal effect of WAIS when T = $ and the approximate odds 
ratio. 

BINOMIAL RESPONSE MODELS 
267 
Finally, the threshold value z, is now given by z, = (log(1og 2) - /30)/,!?1 and is 
specified in WinBUGS using the syntax 
Finally, for comparison reasons, we may calculate the odds ratios and probability dif- 
ference for various values or of X .  Here the range of values is limited to integer values 
from zero to 20. Hence we can try all values to obtain a full overview of how the success 
probability changes for each value of z. The syntax for the probit link is 
while for the clog-log link we only need to change commands in the second line of this 
code by substituting probit with cloglog. Estimated probabilities for all possible values 
of z and hence all possible individual “profiles” of this example are also calculated using 
the syntax displayed above (given by node p i .  model). The usual low information priors 
& N N(0,lOOO) are used in this example. 
7.5.2.2 Results and parameter interpretation. Posterior summaries of the param- 
eters for each link are provided in Table 7.18. The a posteriori expected success probabilities 
along with the 2.5% and 97.5% posterior values for all three link functions are depicted in 
Figure 7.4. 
Table 7.18 
Posterior summaries for model parameters for each link function for Example 2.3 
Logit 
Probit 
clog-log 
Node 
Mean 
SD 
Mean 
SD 
Mean 
SD 
$00 
2.507 1.229 
1.402 0.661 
1.447 0.721 
OR” 
0.718 0.083 
0.748 0.074 
0.700 0.073 
WAIS(z7 = 0.5) 
6.975 2.104 
6.671 3.195 
6.152 1.575 
DIC 
55.105 
54.997 
54.998 
“Exact odds ratio in logit (= $1); approximate for probit and clog-log. 
P1 
-0.339 0.119 
-0.191 0.061 
-0.260 0.076 
All models indicate significant negative association between the WAIS score and the 
presence of senility symptoms. From the logit model, the odds of senility symptoms for 
individual scoring zero in WAIS are a posteriori expected to be equal to 12.27. Moreover, 
for each additional point of the WAIS score, a decrease in disease probability by 38% is a 
posteriori expected. Similarly, the corresponding approximate posterior odds for the other 
two models indicate decrease of 25% and 30% for the probit and clog-log links, respectively. 
These approximations are satisfactory summaries of the overall picture since the posterior 
means of the actual odds ratios for all WAIS values range from 0.61 to 0.74 for the probit 
link. Similar is the picture for most of the values in clog-log link with the odds ratios to 
range from 0.61 to 0.77 for z > 4. Generally, this approximation is more successful for 
the central T values as expected. 

268 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
Figure 7.4 
means; points represent 2.5% and 97.5% posterior percentiles for disease probabilities. 
Estimated binomial models for Example 7.3. Lines represent model based on posterior 
Concerning the threshold disease value, all three models indicate that cases are implied 
for values X 5 6; see Table 7.19 for a summary of posterior statistics. If we consider the 
median values, then the two first models indicate that X 5 7 (corresponding medians equal 
to 7.35 and 7.29) while for the clog-log link the corresponding median is equal to 6.998 
marginally indicating X 5 6. Finally, we may use the 95% posterior intervals to construct 
a more complicated decision rule. For example, for the logit link we may say that someone 
whose score 5 2 (lower than or equal to 2) is a case, or is healthy if X 2 10. Values 
within 3 - -9 form a nondecision zone since any value in this interval can be reasonably 
considered as a threshold value. Similarly, for the probit link we can form the following 
decision rule: X = 0 for case, X 2 10 for healthy, 1 5 X 5 9 for neither case or healthy 
(i.e., we cannot decide), whereas for the clog-log link, the corresponding decision rule is 
X 5 3 for case, X 2 9 for healthy, and 4 5 X 5 8 for neither (i.e., cannot decide). Note 
that for the last link, the neutral zone is narrower. 
Table 7.19 
discrimination rules for each link function for Example 2.3 
Posterior summaries for threshold value WAIS(7r = 0.5) and corresponding 
Decision rule 
WAIS(.n = 0.5) 
Logit 
Probit 
clog-log 
Node 
Logit Probit clog-log 
Case 
Healthy 
Case 
Healthy 
Case 
Healthy 
Mean 
6.975 6.671 
6.752 
X 5 6 
X 2 7 
X 5 6 
X 2 7 
X 5 6 
X 2 7 
Median 
7.353 7.291 
6.998 
X 5 7 
X 2 8 
X 5 7 
X 2 8 
X 5 6 
X 2 7 
95% posterior interval 2.149 0.028 
3.262 
X 5 2 
X 2 10 X = 0 
X 2 10 X 5 3 
X 2 9 
9.457 9.469 
8.968 
Finally, the DIC value for probit is the lowest with minor differences from clog-log, 
while logit seems to be the worst of the three models. Nevertheless, all DIC values are quite 
close, indicating minor differences in the fit of the three models. 

MODELS FOR CONTINGENCY 
TABLES 
269 
7.6 MODELS FOR CONTINGENCY TABLES 
Both binomial logit and Poisson log-linear models are also used for modeling the frequencies 
(counts) of high-dimensional contingency tables. The two models are associated, and using 
the Poisson log-linear models, we can derive the corresponding logit model. 
To be more specific, let us consider the Poisson log-linear model assuming corner con- 
straints in a 2 x 2 contingency table resulting from the cross-classification of two binary 
variables X and Y .  Then the Poisson log-linear model is given by 
?v:~ - Poisson(A,, ). log(A,,) = + A: 
+ A: 
+ A: 
(7.10) 
for i, j = 1,2, where N,, are the frequencies for any combination of X = i and Y = 3; A 
is 
the correspondingexpectednumber ofcounts; X O ,  A:, 
A:, 
and A;’ 
are the corresponding 
model parameters for the constant term, the effects X ,  the effect of Y and the interaction 
effect X Y ,  respectively. 
The Poisson likelihood can be written as 
2 
= n 
j B ( ~ t 2 ;  
~
i
,
 
~ i + ) f p ( ~ i + ;  
Xi+)! 
i=l 
where N,+ = 
N i j  N Poisson(Xi+), Xi+ = C s = l  Aij, and 7ri = A i z / ( A i l  + Aiz) 
and is equal to the conditional probability of P(Y = 21X = i). Hence the likelihood 
of a Poisson log-linear can be written as a product of two independent binomial models 
for N i 2  times a Poisson model for the marginal distribution of X with expected counts 
Xi+ = Xi1 + Xi2. Hence, model (7.10) is also referred to as the product binomial model. 
The parameters involved in the two parts of the likelihood are different, resulting in two 
independent likelihoods. Moreover, the logit expression resulting from the Poisson model 
above is given by 
Ti 
x i 2  
1 - Ti 
Ail 
logit(Ti) = log - 
= log - 
= x0 + 
+ A,Y + 
- x0 - AX 
= A; 
+ AZY = A,’ + A&YI(i = 2) . 
Hence Ar = PO and XgY = 01 in the corresponding logistic regression model for the same 
data given by 
Niz - binomial(Ti, Ni+), log 2 
= PO + PIXi, i = 1 , 2  . 
(1 
This logic can be generalized for more than two levels and for higher-dimensional contin- 
gency tables that involve additional categorical variables. If Y has more than two levels, 

270 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
then the model is a product multinomial following the same reasoning. In fact, it is com- 
mon practice to use log-linear models in order to fit the corresponding multinomial model 
for the response Y. 
The parameters of the corresponding logit model are given by all X ’, 
where S is a set of variables with Y E S. Moreover, parameters of the multinomial model 
for any term T are given by the corresponding parameter of the log-linear term for term 
T U Y, 
namely, the interaction term between T and Y. 
For example, if T = XlX2, then 
the corresponding multinomial parameter of the interaction between X 1 and Xz is given 
by the log-linear parameters X y x l X z .  In simpler words, we consider log-linear parameters 
for all terms, including Y, 
and we then match them to the logit parameters by eliminating 
Y from each term. 
Note that the Poisson log-linear model is a undirected model that does not assume any 
of the available variables as the response one. Associations between variables are indicated 
via the interaction terms included in the model. On the contrary, binomial (or multinomial) 
logit models are directed models assuming one variable as response. Additional details 
concerning this issue can be found in Agresti (2002). 
Examples of Poisson log-linear models for contingency tables using WinBUGS can be 
found in Spiegelhalter et al. (2003q dogs data example), Congdon (2003; 2005~; 
2006b), 
Dellaportas et al. (2000), and Ntzoufras (2002). 
Problems 
7.1 
Using the following simulated data 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
7 
4.7 
3 1.8 
0 
5 
2.3 
4 1.6 
3 
3 
6.4 
2 2.8 
64,235 
7 
7.6 
2 2.5 
5 
6 
3.0 
1 1.1 
0 
4 
6.6 
3 2.6 
3,269 
1 
4.9 
4 1.6 
7,045 
6 
5.5 
3 2.1 
5 
2 
1.6 
1 2.6 
162,124 
2 
4.1 
0 1.6 
979 
4 
5.7 
2 3.1 
38,319 
2 
7.1 
1 3.7 36,901,820 
2 
2.0 
0 1.7 
1,373 
3 
1.9 
3 1.9 
64 1 
5 
7.3 
5 1.7 
4 
2 
5.2 
3 1.0 
50 
4 
4.4 
4 1.2 
2 
2 
5.1 
3 1.7 
1,800 
6 10.0 
4 3.7 
14,04 1 
5 
5.8 
1 2.6 
377 
a) Model y in WinBUGS using a Poisson log-linear model. 
b) Use DIC to select which model is more appropriate for the case. 

PROBLEMS 
271 
c) Compare your results with the true expression 
K N Poisson(X,) with log X i  = 3 - 2X,2 + 5Xi5 
used to simulate Y,. 
7.2 
Download the mshopl dataset from the book's Website (www . stat-athens . aueb. 
gr/" j bn/winbugs-book), The data were compiled from a customer satisfaction 
survey conducted in Chios, Greece (Sarantinidis, 2003). The given dataset is only 
part of the original dataset. Explanation of the variables is given in file mshopl . t x t .  
a) Build a Poisson log-linear model to identify which variables influence the number 
b) Use DIC to select which variables are finally appropriate for the model. 
c) Interpret the parameters of the model on the basis of their posterior distributions. 
Use the Poisson log-linear model presented in Section 7.4.3 to model the water 
polo data available in the book's Webpage (www . stat-athens . aueb. gr/"jbn/ 
winbugs-book); see Karlis and Ntzoufras (2003~; 2005) for more details. 
a) Comment the performance of each team according to the model parameters. 
b) Interpret the model parameters for each team. 
c) Compare the model parameters, using the compare tool of WinBUGS . 
d) Estimate the final ranking, assuming that the tournament was based on the fact 
Use the Poisson log-linear model presented in Section 7.4.3 to model the soccer data 
from the Italian footballisoccer championship for season 2000-2001. The data are 
available in the book's Webpage (www . stat-athens. aueb. gr/"jbn/winbugs- 
book). 
a) Comment the performance of each team according to the model parameters. 
b) Interpret the model parameters for each team. 
c) Compare the model parameters using the compare tool of WinBUGS . 
d) Regenerate the full results. Use the ranks tool to monitor the ranking of the 
teams. Were the final observed rankings in agreement with the results obtained 
by the assumed model? 
Use the Poisson distribution to model the WinBUGS data example salm; see Spiegel- 
halter et al. (2003~). 
a) Fit the model described in Spiegelhalter et al. (2003u), ignoring the random effect 
b) Interpret the model coefficients of the fitted model. 
c) Use DIC to decide whether X ,  of log X, must be removed from the model. 
Use the binomial distribution to model the WinBUGSdata example seeds; see 
Spiegelhalter et al. (2003~). 
a) Use dummy variables under both comer and sum-to-zero parametrizations. 
b) Fit the model with and without the interaction term between the seed and the type 
of root extract. Compare the two models using DIC. In all models, ignore the 
random effect b, used in the corresponding description of the problem. 
c) Interpret the model coefficients underboth models. What is the difference between 
the two models? 
of purchased items. 
7.3 
that all teams were playing against all teams once. 
7.4 
7.5 
A,, 
. 
7.6 

272 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
d) Fit a probit model and a clog-log model. Interpret and compare the model pa- 
rameters with the ones obtained from the logit model. 
e) Compare the posterior distributions of the success probabilities obtained under 
the different link functions. 
Using the following simulated data 
7.7 
XI x2 
x3 x, x, Y N 
6 
12 
1 0.0 
0.8 
6 36 
12 
17 
1 0.3 
2.0 10 32 
8 
19 
1 0.8 
1.1 
4 41 
14 
12 
0 0.2 
1.0 13 37 
10 21 
0 0.9 
0.7 
4 33 
2 
16 
0 0.6 
0.7 
1 26 
10 
13 
1 0.9 
0.4 
1 29 
12 
12 
0 0.0 
3.3 18 31 
10 
19 
1 0.6 
0.6 
3 36 
8 
20 
1 1.1 
1.4 
3 27 
9 
19 
1 1.1 
0.6 
4 25 
9 
10 
0 0.2 
0.5 
4 30 
14 
15 
1 1.3 
0.6 
3 30 
11 
I5 
1 0.0 
0.2 14 40 
2 
18 
1 3.4 
1.0 
0 34 
6 
15 
1 0.2 
1.7 
2 21 
5 
10 
1 0.6 
1.7 
0 26 
9 
13 
1 1.5 -0.5 
3 26 
4 
18 
1 
1.1 -0.5 
3 36 
10 
18 
1 0.0 -2.2 
3 23 
a) Model y in WinBUGS using a logistic regression model. 
b) Use DIC to select which model is more appropriate for the case. 
c) Compare your results with the true expression 
Y, N binomial(-iri, Ni) with logit(-iri) = -3 + 0.2 * X i I  - 0.2 * Xi3 - Xi4 
used to simulate Yi. 
the estimated probabilities of the three models. 
d) Use the probit and clog-log links to model the data presented above. Compare 
e) Use DIC to compare the logit, probit, and clog-log models. 
Consider the covariates XI, 
. . . , X5 of Problem 7.7, but for the response Y, 
assume 
the following binary data: 
7.8 
1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0  
a) Model this response in WinBUGS using a logistic regression model. 
b) Use DIC to select which model is more appropriate for the case. 
c) Compare your results with the true expression 
Y, - binomial(ni, Ni) with logit(ni) = -3 + 0.2 * Xi1 - 0.2 * Xi3 - Xi4 

PROBLEMS 
273 
used to simulate Yi and with the posterior distributions obtained in Problem 7.7. 
d) Use the probit and clog-log links to model these data. Compare the estimated 
probabilities of the three models. 
e) Use DIC to compare the logit, probit, and clog-log models. 
Use the binomial distribution to model the data of WinBUGS example beetles; see 
Spiegelhalter et al. (2003b). 
a) Use the logit, probit, and clog-log links. 
b) Interpret the model coefficients for all models. 
c) Compare the posterior distributions of the model parameters and the success 
probabilities obtained using the three models. 
Use the water polo data of Problem 7.3 to model the probability for a team of winning 
a game. 
a) Comment the performance of each team according to the model parameters. 
b) Interpret the model parameters for each team. 
c) Compare the model parameters using the compare tool of WinBUGS 
d) Compare the results with the ones obtained from the Poisson log-linear analysis 
7.9 
7.10 
of Problem 7.3. 
7.11 
Consider the 2 x 2 contingency table data of Problem I .6. 
a) Use the following Poisson log-linear model 
n,j - Poisson(&) and log X i j  = ,LL + ai + bj + abij, 
where nij are the frequencies for the ith row and jth column of the Problem 1.6 
contingency table. 
b) Use data in a tabular format and a “nested double for” syntax to code in Win- 
BUGS the model of (a) using both comer and sum-to-zero parametrizations. 
c) Use dummy variables to code in WinBUGS the model of (a) using both comer 
and sum-to-zero parametrizations. 
d) Calculate the posterior distributions of the odds ratio under the assumed model 
given by 
under the two parametrizations. Can the OR be considered equal to one? 
e) Interpret the parameters of the model. 
f) Fit the independence model by removing the interaction term. Compare it with 
the full model using DIC. What is your conclusion? 
Consider the 2 x 2 contingency table data of Problem 1.6. 
7.12 
a) Use the binomial distribution to model the probability of having the disease under 
b) Use dummy variables to code in WinBUGS the model and both comer and sum- 
c) Calculate the posterior distributions of the odds ratio according to the assumed 
examination. 
to-zero parametrizations. 
model given by 
7r(X = 1)/{ 1 - 7r(X = 1)) 
?r(X = 0 ) / {  1 - 7r(X = 0 ) )  
OR = log 

274 
INTRODUCTION TO GENERALIZED LINEAR MODELS: BINOMIAL AND POISSONDATA 
with the two parametrizations, where T ( X  = 1) and T ( X  = 0) are the probabil- 
ities of having the disease when exposed or not exposed to the risk factor. Can 
the OR considered to be equal to zero? 
d) Interpret the parameters of the model. 
e) Compare the posterior distributions ofthe parameters with the corresponding ones 
obtained by the log-linear model. Are they similar? If so, Why? 

CHAPTER 8 
MODELS FOR POSITIVE CONTINUOUS 
DATA, COUNT DATA, AND OTHER 
GLM-BASED EXTENSIONS 
In this chapter we focus on models used for positive continuous data and on additional 
models used for count data. 
In the first case we include the log-normal, the gamma, and the inverse Gaussian models. 
Survival analysis data can be considered as a special case of such data. The Weibull 
distribution is frequently used to model such data, and, for this reason, it is also described 
here. 
We describe two models used for overdispersed count data: the negative binomial and 
the generalized Poisson models. The zero inflated versions of the Poisson and these two 
models are also described in detail. Bivariate Poisson models and the Skellam distribution 
for integer-valued variables are also discussed in this chapter. 
The chapter closes with a brief discussion of the extensions based on GLM, which can 
be easily fitted using WinBUGS . 
8.1 MODELS WITH NONSTANDARD DISTRIBUTIONS 
WinBUGS allows for modeling of nonstandard distributions (i.e., for a distribution that is 
not listed in WinBUGS ’ prespecified distributions) using the zeroones trick. Here we 
provide general guidelines and then illustrate this approach using an example by fitting the 
inverse Gaussian model. 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
275 

276 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
8.1.1 Specification of arbitrary likelihood using the zeros-ones trick 
We can use either the Bernoulli or the Poisson distribution to indirectly specify any arbitrary 
model likelihood. Let us assume a model with log-likelihood 1 = log f ( y i  16). Then the 
model likelihood can be written as 
Hence, the model likelihood can be written as the product of the densities of new pseudo- 
random variables Zi (i = 1, . . . , n), which follow the Poisson distribution with mean equal 
to minus the log-likelihood, and all observed values are set equal to zero. To ensure the 
positivity of the mean of each Ei, we add a positive constant term C to the mean. This 
is equivalent to multiplying each likelihood term by e P C .  This action does not affect the 
likelihood since it is equivalent to multiplying the resulting (unnormalized) posterior distri- 
bution by a constant term equal to ePnc. With this approach, the likelihood becomes equal 
to 
C must be selected in such way that -1i + C > 0 for all i = 1,2, . . . , n. 
This approach can by fitted in WinBUGS using the following code: 
In this syntax, 1 Cil must be specified accordingly for each model. For example, the normal 
model can be obtained if we set 
li = -0.5 log(2n) - 0.510g(a2) - 
using the syntax 
Instead of the Poisson-zeros strategy, the Bernoulli distribution can be also used for the 
same purpose. With this approach 
where f~ (1; ela, 1) is the binomial probability function with success probability e '% and 
N = 1. Hence, the model likelihood can be expressed as the product of the densities of 
new pseudorandom variables Zi, which now follow the Bernoulli distribution with success 
probability equal to el, with all observed values set equal to one. To ensure that this 

MODELS WITH NONSTANDARD DISTRIBUTIONS 
277 
probability is lower than one, we multiply each likelihood term by e-', 
where C is a 
positive large number. Now the likelihood is given by 
i=l 
i=l 
Now, the corresponding WinBUGS code is given by 
In this syntax, 1 [il must be specified accordingly for each model as in the Poisson-zeros 
approach described above. 
Although both approaches have the same effect, we recommend using the Poisson-zeros 
approach, which avoids overflow problems due to the simpler likelihood expression. 
In this section, we have demonstrated how to define a new sampling distribution (i.e., 
likelihood) function for distribution not included in the prespecified ones within WinBUGS . 
The same approach can be followed to specify a prior distribution of nonstandard form 
(Spiegelhalter et al., 20034. 
8.1.2 The inverse Gaussian model 
The density of the inverse Gaussian distribution Y - IGaussian(p, A) is given by 
The mean and the variance of this parametrized inverse Gaussian distribution is given by 
E ( Y )  = p and V ( Y )  = p3/X. In GLMs, the parametrization p, r2 = X - l  is frequently 
encountered. 
When X tends to infinity (or u z  to zero), the inverse Gaussian distribution becomes 
similar to a normal (Gaussian) distribution. As it is obvious from the fact that Y is positive, 
the inverse does not result in the inverse of a normal (Gaussian) distribution (Seshadri, 
1993). 
The model can be given by 
Y, - IGaussian(pL,, 
A), 
where pz linked with the linear predictor using the canonical (squared reciprocal) link which 
is given by 
or the log-link which is given by 
PF2 = 172 * Pz = m 
logpL, = qz H pL, = ellz . 
The log-link is preferred because of its the easier interpretation, which is similar to the one 
in Poisson log-linear models. 
In WinBUGS we may use either of the two parametrizations proposed above. The log- 
likelihood for the original parametrization is given by 

278 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
while the mean pi is specified by 
when the inverse squared link function is used, or by 
when the log-link function is used. Parameter u 2  is defined as a logical (deterministic) 
function using the following command 
while the priors can be defined as typically normal for the model coefficients /3j; for X we 
may use a gamma prior similar to the one used for the precision of the normal regression 
model. 
Example 8.1. An inverse Gaussian simulated dataset. To illustrate the approach 
described above, we have generated 100 random values from IGaussian(1ogp = 
3 + 2x1 - 1x2, 
X = 10). Four standardized normal variables X j ,  j = 1,2,3,4 
were generated as possible covariates. Data are available at the book’s Website. 
Both the Poisson-zeros and the Bernoulli-ones approaches worked satisfactorily with 
similar results. Results for the first model using 4000 burnin and an additional 5000 itera- 
tions finally kept are presented in Table 8.1. The constant C was set equal to 10,000 and 
100 for the Poisson and the Bernoulli approaches, respectively. The log-link was used in 
both cases. 
As we can see, the estimated model based on the posterior means is given by 
which is very closed to the actual model 
Y, N IGaussian(p = e3+2x1-xz , A  = 10) 
used to generate the data. 
Table 8.1 
Example 8.1 
Posterior summaries of inverse Gaussian model parameters for simulated data of 
node 
mean 
betaC11 
3.220 
betar21 
2.151 
beta[31 -0.950 
betar41 -0.081 
betaC51 
0.013 
lambda 
12.140 
s2 
0.084 
sd 
0.156 
0.083 
0.111 
0.083 
0.122 
1.779 
0.013 
MC error 2.5% median 
0.014 
2.936 
3.215 
0.007 
1.998 2.151 
0.006 
-1.183 -0.945 
0.004 
-0.242 -0.083 
0.004 
-0.242 
0.020 
0.033 
8.913 12.070 
0.0002 
0.063 0.083 
97.5% 
3.539 
2.324 
-0.745 
0.096 
0.247 
15.82 
0.112 
start sample 
4001 5000 
4001 5000 
4001 5000 
4001 5000 
4001 5000 
4001 
5000 
4001 5000 
WinBUGS code of the model using the zeros trick is given in Table 8.2. Additionally, 
the corresponding code using the ones trick is available at the Website of this book. 

MODELS FOR POSITIVE CONTINUOUS RESPONSE VARIABLES 
279 
Table 8.2 
8.1 
WinBUGS code for inverse Gaussian model used for simulated data of Example 
8.2 MODELS FOR POSITIVE CONTINUOUS RESPONSE VARIABLES 
In this section we focus on modeling positive continuous response variables. The normal 
distribution may be used without any major problems when the sampling distribution f(yl6) 
is relatively symmetric and far away from zero. The simplest approach is to use the logarithm 
of the original variable assuming that log Yi N N(pi, 02), which is equivalent to using the 
log-normal distribution for the original response variable Y. 
Alternatively, the gamma, the 
exponential (which is special case of the gamma), and the inverse Gaussian (described in 
the previous section) distributions can be adopted. Other distributions that may be used 
are the inverse gamma (which can be fitted via the gamma distribution) and the Weibull 
distribution. The latter is frequently used in the analysis of survival data where the response 
measures the time until an event of interest (usually death) occurs. Survival analysis models 
additionally have to account for incomplete data, which are called censored. Such data arise 
when specific timepoints (where the event of interest has occurred) are known, but the exact 
occurrence time is not available. 
8.2.1 The gamma model 
The gamma model is given by 
Y, - gamma(pir,r) 
log(pi) = Vi = Po + PlXil + ... + PPXiP . 
In this expression, the mean and the variance are given by E(Yi) = pi and Var(yZ) = r-l 
and the log-link is used. Covariate structure can be further imposed on the precision (or the 
variance) parameter. Alternatively, the canonical link p i 1  = qi, but problems may appear 
in the MCMC algorithms because pi must be positive. Moreover, interpretation is not as 
straightforward as in the log-link case. 
In WinBUGS , the gamma model can be implemented by 

280 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
If the canonical-link is used instead, then the fourth line is substituted by the command 
mu [il <- I/ (eta [il ) . 
8.2.2 Other models 
The log-normal model is essentially the same as the normal model. It can be easily imple- 
mented by transforming the response variable in WinBUGS using the syntax 
or by directly imposing the log-normal distribution on Y using the command 
Interpretation of such models is similar to the usual regression models, but now we focus 
on the mean of log Y instead or the original variable Y .  
Concerning the exponential distribution exponential(X), we can either use directly the 
dexp command or define it as a special case of the gamma distribution since exponential(X) = 
r( 1, A). Covariates can be imposed on the mean A-’ of the exponential distribution using 
either the inverse or the log-link, resulting in X i  = qi and log X i  = -q2, respectively. The 
case for the x2 distribution is similar. We can either use the dchisqr WinBUGS command 
or define it as a gamma(p/2, i) with mean p and variance 2p. 
Finally, the inverse gamma distribution is simply defined as 1/Y with Y N gamma(p.r, 7). 
Hence in WinBUGS we can easily define the inverse gamma distribution by transforming 
the original response variable. The corresponding WinBUGS syntax is given by 
In this model we can still model the mean of the inverse random variable. Note that the 
mean of Y-’ 
is equal to b/(a - 1) and is specified only for a > 1. 
Finally, the Weibull distribution with probability density function 
f(ylX, w) = wXy~-le--x~”, 
mean 
and 
variance 
equal 
to 
E ( Y )  
= 
X-’/’r(l + w-’), 
and 
V(Y) = 
(r(l + 2w-I) - r(l + u-’)’), respectively. It is implemented in Win- 
BUGS using the command y [il - dweib (v , lambda). The exponential distribution is 
also a special case of the Weibull distribution for w = 1. Parameter X is usually log-linked 
with the linear predictor. Alternatively, the mean can be directly log-linked with the linear 
predictor. This can be achieved by the following WinBUGS syntax 
since X i  = [r(l + 1/v)/E(Y,)IV, 
resulting in log(&) = vlogr(1 + 1/v) - vqi with qi 
the usual linear predictor log-linked with E(Yi), specifically, log E(Yi) = qi. 
= u log r(l + l/u) - ,b’o and /?,” = -w&, where 
P,” and ,b’j denote the regression coefficients imposed on log(X) and log(EY), respectively. 
Nevertheless, the prior distributions must be specified with caution in order to obtain the 
same posterior distributions. Exactly the same posterior distributions will be obtained only 
when we specify equivalent prior distributions under both parametrizations. 
The two models are equivalent to 

MODELS FOR POSITIVE CONTINUOUS RESPONSE VARIABLES 
281 
8.2.3 An example 
Example 8.2. Soft drink delivery data (Example 5.1 revisited). In the following 
we implement all the models described in this section and use the DIC to identify 
which model is the best on the soft drink delivery data presented in Chapter 5 
(Example 5.1). 
Note that DIC as calculated in WinBUGS can be used only if the same response Y is 
used. If transformation of the original variable is used, then DIC values are not comparable 
since the response used is essentially different. 
Table 8.3 presents the DIC values for various models fitted to the soft drink delivery 
data of Example 5.1. DIC values indicate the gamma and the Weibull models as the 
best ones. For the Weibull distribution, two models were fitted using covariates on the 
log(A) (parameter of the Weibull) and using covariates directly on the log-mean of the 
Weibull distribution. DIC values are slightly different, possibly because different priors 
are imposed on common parameters. Moreover, DIC values can be used for comparison 
only if we use exactly the same response variable. Hence, when using transformed data, 
DIC values reported by WinBUGS refer to the transformed and not to the original response 
variable. For this reason, a constant term based on the first derivative of the transformation 
function must be added. This term is given by C = -2 Cy=l log 1 Ji 1, where Ji is the first 
derivative of the transformation function. For the inverse gamma and the log-normal (when 
log-transformed data were used) distributions, WinBUGS DIC values were found equal to 
-12.51 and -159.150, respectively, in which the terms C = 2 cy=l 
logyi = 147.71 and 
C = 4 cy=l 
log yi = 295.42 must be added to obtain the correct values. Finally, when the 
log -p representation was used, a precision limit was set to avoid arithmetic overflows of 
A. 
Table 8.3 
Deviance values of various models fitted at soft drink delivery data of Example 8.2 
Model 
na Eb 
p ,  
DIC 
1. 
2. 
3. 
4. 
5. 
6. 
7a. 
7b. 
Normal 
131.2 127.0 4.2 135.4 
Log-normalcse 
131.0 126.8 4.2 135.2 
Exponential 
201.4 198.5 2.9 204.3 
X 2  
145.5 142.4 3.0 148.5 
Gamma 
129.4 125.4 4.1 133.5 
Inverse gammad,e 136.3 132.3 4.0 140.3 
Weibull (log A) 
129.6 125.9 3.7 133.4 
Weibull (logp) 
130.2 126.1 4.0 134.2 
aB = D(O,, m) = posterior mean of deviance. 
* D = D(8,, m) = deviance evaluated at posterior means of parameters. 
CWhen we use Z, = log(timei), then WinBUGSDIC estimate =-12.51. 
The term 
C = 2 cy=l 
log yz = 147.71 must be added in each deviance measure. 
dWinBUGSDICestimate =-159.150, The term C = -2cy='=1 
loglJ,I = 4 C G 1  logyi = 
295.42 must be added in each deviance measure. 
eWhen the response variable is transformed, the term C = -2cy=1 
log IJii must be added 
in each deviance in order to obtain a comparable DIC, where J, is the first derivative of the 
transformation function. 
h 

282 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
Table 8.4 presents the posterior summaries for the gamma and the Weibull model. The 
Weibull model is based on modeling log(p) to simplify interpretation. Results from both 
models are quite close, indicating no major differences. 
Gamma model 
Weibull model 
Posterior Posterior percentiles 
Posterior Posterior percentiles 
Parameter 
mean 
2.5% 
97.5% 
mean 
2.5% 
97.5% 
P O  
2.382 
2.276 
2.487 
2.299 
2.151 
2.44 
P1 
0.047 
0.031 
0.060 
0.051 
0.035 
0.070 
pz x 100 
0.046 
0.016 
0.078 
0.053 
0.024 
0.085 
r or v 
1.912 
0.940 
3.192 
6.878 
4.724 
9.335 
fl 
0.749 
0.560 
1.032 
- 
__ 
- 
epo 
10.850 
9.739 
12.020 
9.990 
8.592 
11.470 
ep1 
1.048 
1.032 
1.062 
1.053 
1.036 
1.072 
1.047 
1.016 
1.081 
1.055 
1.024 
1.088 
typical.ya 
19.67 
18.39 
21.07 
19.43 
18.01 
20.76 
elOOBz 
Table 8.4 
Deviance values of various models fitted at soft drink delivery data of Example 8.2. 
Interpretation can be based on the exponents of /3j coefficients of the gamma model. 
Posterior summaries of expected times for typical deliveries (with covariates set equal to 
their sample means) are also provided. For these values we can claim that 
0 When no cases of stockedproducts need to be replaced in the machine and no distance 
is walked, then the worker needs about 10 minutes to complete the delivery (9.7 - 12 
with the gamma model and 8.6 - 11.5 with Weibull model). Note that this estimate 
is quite different from the corresponding one for the normal linear model. 
0 Every additional case that needs to be stocked in the machine increases the delivery 
time by almost 5% (4.7 and 5.1 using the gamma and the Weibull models, respec- 
tively). 
0 Every additional 100 feet walking distance increases the delivery time by approxi- 
mately 5% (4.6 and 5.3 using the gamma and the Weibull models, respectively). 
0 A typical delivery is completed in about 19 minutes. 
8.3 ADDITIONAL MODELS FOR COUNT DATA 
As we have already mentioned, the Poisson distribution is frequently used for modeling 
discrete count data. This distribution imposes an important and restrictive assumption: the 
variance is restricted to be equal to the mean. This assumption is frequently violated in 
practice. In such cases, overiunderdispersed models must be used to capture this extra 
variability. Another problem frequently encountered is that the probability of zero in most 
real-life data is considerably underestimated by the standard Poisson model. For this reason, 
zero inflated models have been developed and used in practice. 

ADDITIONAL MODELS FOR COUNT DATA 
283 
Here we focus on two overdispersed distributions used to model count data. We further 
present zero inflated versions of such models. Finally we close this section with a discus- 
sion focused on other distributions used to model count data such as the bivariate Poisson 
distribution [see, e.g., Karlis and Ntzoufras (2003a)], and the Skellam distribution used to 
model differences of count data or, more generally, discrete integer-valued variables (Karlis 
and Ntzoufras, 2006). 
8.3.1 The negative binomial model 
A discrete random variable Y follows the negative binomial (NB) distribution (Y - 
NB(7r, r ) )  if the probability function is given by 
where y = 0,1,2, . . . , r and r > 0 a positive parameters. The mean and the variance of 
this distribution are equal to r(1 - T ) / T  and r(l - T ) / T ’ .  
Frequently r = N is an integer valued parameter. In such cases, this distribution is used 
to describe the number of times (equal to N + y) that we need to to repeat a Bernoulli 
experiment with success probability T until N successes are reached, for example, how 
many free throws a basketball player must attempt before scoring 10 times. In the more 
general case where r E R, r > 0, also referred to as Polya distribution, it is used to model 
overdispersed count data since the variance over mean ratio [i.e. the dispersion index (DI)] 
is given by 
When modeling count data, the parametrization X = r( 1 - T ) / T ,  or equivalently T = T / ( T +  
A), is frequentlyused instead. Underthis parametrization E(Y) = X and V(Y) = X(X+r), 
while the dispersion index is equal to DI= 1 + X/r. The negative binomial model with this 
parametrization can be also derived by the following Poisson-gamma mixture model 
Ylu N Poisson(Xu) and u - gamma(r, T ) .  
The resulting marginal distribution of Y is given by 
which is a negative binomial distribution with parameters r / ( r  + A) and r. 
The Poisson distribution with mean X = r(1 - T ) / T  is the limiting case of the negative 
binomial distribution for r --$ 00. Although the canonical link [complementary log (clog) on 
7r, log( 1 - T )  = 77) link can be used, we recommend, especially when modeling count data, 
using the log-linkon X [i.e., setting logit(7r) = -r - 771 in order to ensure a straightforward 
parameter interpretation and a direct comparison to the corresponding parameters of the 
Poisson model. 
This distribution is available in the current version of WinBUGS and can be modeled 
using the syntax 

284 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
The usual normal prior distributions can be usedon model coefficients p j, j = 0, 1,2, . . . , p. 
For parameter r, a gamma prior can used in a manner similar to that for the dispersion pa- 
rameters in the normal and the inverse Gaussian cases. Moreover, the model can be easily 
extended by adding structure on the dispersion parameter r. Such extensions can be ac- 
commodated in WinBUGS without any complications. 
Example 8.3. 1990 USA general social survey: Number of monthly sexual 
intercourses. In this example we consider the 1990 USA general social survey 
data concerning the number of sexual intercourses of each participant within the 
previous month. Accumulated data by gender are provided by Agresti (2002, pp. 
569-570) and are reproduced in the book’s Website with permission of John Wiley 
and Sons, Inc. Note that the sample means are 5.9 and 4.3 for males and females, 
respectively, while the variances are much higher (54.8 and 34.4, respectively), 
indicating that the data are clearly overdispersed. 
Here we use the negative binomial distribution to model these data and compare 
the model with the standard Poisson model using the DIC measure. 
The data consist of 550 observations. The data were inserted using ungrouped data and 
two variables: Gi, which is the gender for a subject (zero for males and one for females); 
and Yi, which is the responded number of sexual intercourses during the previous month. 
In the following we briefly present two approaches: (1) a simple approach using one 
separate negative binomial distribution for the data of each gender and (2) an approach 
structured as a generalized linear model with the gender as a covariate. 
Simple approach. The model can be easily summarized by 
- NB(TG%+I, 
TG,+I) for i = 1,2,. . . ,550 
with prior distributions 
~j - U(0,l) and rj N gamma(0.001,0.001) 
for j = 1,2 (males, females respectively). 
The corresponding WinBUGS code is given by 
Finally, the mean, variance, and dispersion index for each gender are estimated using simple 
logical (deterministic) nodes defined by the following syntax 

ADDITIONAL MODELS FOR COUNT DATA 
285 
Model-basedapproach. 
then we can express the model as 
Ifwe use the model-based approach with gender as a covariate, 
where 7rf, rf, and Xz are the parameters for each individual i (i = 1, . . . ,550) while 7 r j ,  
rj, and Xj are the parameters for each gender j ('j 
= 1 , 2) and are given by 
Finally, for PO and /31 we use the prior distributions 
/3j - N(0,lOOO) and rj - gamma(0.001,O.OOl) 
for j = 1,2 (males, females, respectively). 
The corresponding WinBUGS code is given by 
where p . ind, r . ind, and lambda. ind are the parameters 7rf, rf , and A:, 
respectively, of 
individual i. Finally, the dispersion index and the variances are specified as in the simple 
case described above. 
Results. Table 8.5 summarizes posterior results from the two approaches above and 
the simple Poisson model. Both negative binomial (NB) and Poisson models indicate a 
significant difference in the number of monthly sexual intercourses declared by people of 
different genders. Female sexual activity is lower by 25% than the corresponding activity 
of males. 
Dispersion indices of NB as well as DIC values indicate that the NB model is much 
better than the Poisson one. Concerning DIs, these range from 7.6 to 12.7 and from 9.3 
to 15.8 for males and females, respectively, indicating a clear overdispersion. Moreover, 
DIC values for the NB model is much lower than the corresponding value for the Poisson 
model. Note that, for the GLM approach DIC was not calculated by WinBUGS . For this 
reason the corresponding value was calculated outside WinBUGS using the posterior means 
of the deviance measure and 7rj and rj ( j  = I, 2); see Sections 4.2.5 and 6.4.3 for details 
concerning the computation of DIC. 
Note that the posterior means of X j  (expected counts) are similar for both Poisson and 
negative binomial models, except with regard to the dispersion of the sampling distribution, 
which also affects also the posterior variance of each parameter, producing larger 95% 
posterior intervals. 

286 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
Table 8.5 
binomial and Poisson model in Example 8.3 
Posterior means (95% posterior intervals) for model parameters of negative 
NB - 
simple 
NB - 
GLM 
Poisson 
1.76 ( 1.61, 1.93) 
5.84 (5.02, 6.88) 
4.38 (3.61, 5.26) 
0.75 (0.59, 0.95 ) 
0.67 (0.53, 0.83 ) 
0.40 (0.32, 0.48 ) 
0.10 (0.08, 0.13) 
0.08 ( 0.06, 0.1 1 ) 
9.85 ( 7.79, 12.67 ) 
12.16 (9.42, 15.80) 
2874.36" 
-0.29 (-0.53, -0.05 ) 
1.77 ( 1.71, 1.82) 
5.86 (4.98, 6.86) 
5.86 (5.55,6.17) 
4.31 (3.56, 5.17) 
4.30 (4.07,4.53) 
0.74 ( 0.68,0.79 ) 
0.67 (0.54, 0.84) 
0.40 (0.32, 0.49) 
0.10 (0.08, 0.13) 
0.09 ( 0.07, 0.1 1 ) 
9.80 ( 7.65, 12.47 ) 
11.89 (9.30, 15.32) 
-0.31 (-0.38,-0.23 ) 
2873.46 
5271.35 
aDIC is calculated outside WinBUGS in this case using the posterior means of deviance, 
~j and ~j (j=1,2; i.e., males and females, respectively). 
8.3.2 The generalized Poisson model 
Another distribution frequently used to model overdispersed insurance claim counts is the 
generalized Poisson distribution introduced by Consul (1 989); see also Consul and Famoye 
(1 992) for a thorough description and in Ter Berg (1 996) and Scollnik (1 998) for actuarial 
applications of this model. 
The generalized Poisson (GP) distribution is also known as Lagrangian Poisson distri- 
bufion, and its probability function is given by 
(8.1) 
where < and w are the distribution parameters. In this parametrization, the mean and the 
variance is given by E ( Y )  = ((1 - w)-' and by V(Y) = ((1 - w ) - ~ ,  
respectively. To 
make the parametrization equivalent to the corresponding Poisson model, Ntzoufras et al. 
(2005) have recommended an alternative parametrization by setting X = C( 1 - w) -'. Then 
the corresponding probability function is given by 
According to Ter Berg (1996), valid values for w are within the interval [O, 1). Typically, 
the distribution can be defined for y = 0,1,2, . . . , N with max{-1, -<IN} 5 w < 1 
with negative values leading to underdispersion; see Scollnik (1 998) for details. Note that 
for positive counts we set N = 00, leading to w E [0, I). For w = 0, the GP also reduces 
to the simple Poisson model with mean A. The mean of Y is equal to E(Y) = A, while the 
variance and the dispersion index are equal to V(Y) = X(1 - w) -' 
and DI = (1 - w ) - ~ ,  
respectively. 

ADDITIONAL MODELS FOR COUNT DATA 
287 
The generalized Poisson model can be fitted using the zeros-ones trick described in 
Section 8.1.1. Hence we may specify the log-likelihood components 1 i by 
where lambda. star [il is the WinBUGS node used to define Xy = (1 - w)Xi + wyi and 
in this way simplify the log-likelihood component 1 i .  
In the syntax displayed above, w is assumed constant across all parameters. Structure 
on w parameter can be added by substituting w by wi (omega[il in WinBUGS) and then 
specifying its structure in a separate line similarly to the specification of the mean. The log- 
link on X can be naturally used for an interpretation similar to that in the Poisson log-linear 
models. 
Concerning the prior distributions, the usual normal prior distributions can be used for 
Pj parameters while a beta prior can be used for w. When no information is available, a 
uniform defined on the interval (0,l) or a beta( f ,  i) prior can be used. Alternatively, if 
negative values of w are allowed (allowing also for underdispersion), then a rescaled beta 
distribution can be adopted. 
Here we use the GSS 1990 dataset (Example 8.3) to briefly illustrate results. The code 
can be found in this book’s Webpage. More details and actuarial oriented examples can be 
found in Katsis and Ntzoufras (2005). 
Example 8.3 (revisited-1): 
Fitting the generalized Poisson model on GSS 1990 data. 
The generalized Poisson model has been fitted using either the simple or the model-based 
approach as described in the illustration of negative binomial model. The constant parame- 
ter was set equal to zero since elz is the probability function ofthe GP distribution, resulting 
in -1i > 0 for all i = 1, . . . , n. With the simple approach, the DIC value, as reported 
by WinBUGS , is equal to 2926.44, which is lower than the corresponding value for the 
Poisson model but higher than the DIC value for the negative binomial one. With the pos- 
terior means of the dispersion indices, we observe that the GP model is far away from the 
value of one assumed by the Poisson distribution. Moreover, the GP model also indicates a 
considerable increase in variance with the 95% posterior intervals of DIs ranging from 8.5 
to 17.7 and from 16.2 to 26.9 for males and females, respectively; see also the last column 
Table 8.6 for detailed posterior summaries. 
Example 8.3 (continued): Using grouped data. In the previous example, the actual raw 
data were used with 550 observations. Aggregated data can be used instead in order to 
accelerate the MCMC algorithm. This can be easily achieved using the zeros-ones trick; 
see the following computational note for details. Using the aggregated data, only 3 seconds 
were required to generate 5000 iterations instead of 41 seconds needed when the raw data 
were used, increasing the speed of the algorithm by 13 times. Results are the same as in 
the case of using raw data since both the data and the model are the same (only rearranged 
in a more compact format). 

288 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
8.3.3 Zero inflated models 
Zero inflated models for count data have been widely used in statistical science to model a 
wide variety of real life count data such as manufacturing defects (Lambert, 1992); sexual 
behavior (Heibron, 1994); medical (Bohning, 1998; Cheung, 2002); dental (Bohning et al., 
1999; Mwalili et al., 2008; Karlis and Ntzoufras, 2006); spatial (Agarwal et al., 2002); 
violence-related (Famoye and Singh, 2006), and sports data (Karlis and Ntzoufras, 2003~; 
Karlis and Ntzoufras, 2006); see also Ridout et al. (1 998) and Gan (2000) and references 
cited therein. Such models introduce an extra probability parameter to capture an excess of 
zero values that cannot be estimated sufficiently by the assumed distribution or model. 
The zero inflated version of a distribution D of random variable Y - ZID(T 0,0) (where 
ZID = zero inflated distribution) has a probability function of the form 
where f ~ ( y l 0 )  
is theprobabilityfunctionofdistribution Dwithparameters OandfzID(y) is 
the zero inflated version of D with an additional parameter T O  as the proportion of additional 
zeros. From the equation above, the probability of zero is equal to T 0 + (I - T O ) ~ D  
(Ole), 
while the probability of y > 0 is given by (1 - ~ ~ ) f ~ ( y \ e ) .  
Moreover, the mean and the 
variance of this distribution are equal to 
E ( Y )  = (1 - TO)E(YD) 
v(y) = (1 - T O ) ( v ( y D )  + T O E ( y D ) ) >  
respectively, and the dispersion index is equal to DI = V(YD)/E(YD) 
+ T O ,  where YO - 
The zero inflated Poisson (ZIP) distribution is the simplest ZID and was introduced in 
the early 1960s by Cohen (1963), but a ZIP model was introduced in the statistical literature 
in the early 1990s by Lambert ( 1992) using covariates on both the Poisson rate X and on 
the zero excess probability T O .  Alternative terms used by some researchers developing 
independently similar models is the zero alteved (Heibron, 1994) or zero adjusted (Gupta 
D(0). 

ADDITIONAL MODELS FOR COUNT DATA 
289 
et al., 1996) Poisson models. The full ZIP model has the following representation: 
Y, N ZIP(noi, Xi) and log(&) = X ( Q P  . 
The excessive proportion of zeros T O  is usually assumed constant across all observations 
i = 1,2,. . . , n, but covariates can be also incorporated here with no difficulty. Hence the 
following structure 
where X ; )  and Pz are the values of the covariates and the corresponding coefficients 
involved in the linear predictor for T O ,  can be also incorporated in the model with no 
difficultly. 
Zero inflated models also exist for other distributions, such as the gamma distribution 
(Feuewerger, 1979). Nevertheless, interest mainly lies in discrete count data, resulting in a 
variety of zero inflated versions of the binomial (ZIB) (Hall, 2000), the negative binomial 
(ZINB) (Heibron, 1994; Mwalili et al., 2008), the generalized Poisson (ZIGP) (Gupta et al., 
1996; Famoye and Singh, 2006), the bivariate Poisson (ZIBP) (Wahlin, 2001), and the 
multivariate Poisson model (MZIP) (Li et al., 1999). More recently, Karlis and Ntzoufras 
(2003a, 2005) have introduced diagonal inflated bivariate Poisson (DIBP) models empha- 
sizing application in sports and the zero inflated version of Skellam’s (1 946) distribution [or 
Poisson difference distribution (ZIPD)] implemented in dental epidemiology (Karlis and 
Ntzoufras, 2006) and association football (Karlis and Ntzoufras, 2008) data. More details 
on zero inflated models can be found in Gan (2000). 
Bayesian analysis of zero inflated models has been described in detail by Ghosh et al. 
(2006), who also provide an easy-to-use WinBUGS code for fitting the ZIP model. Detailed 
MCMC algorithms for estimation of the posterior distributions of the parameters in the ZIPD 
model were provided by Karlis and Ntzoufras (2006), while the same authors (Karlis and 
Ntzoufras, 2008) used a generalized ZIPD model for association football data implemented 
using WinBUGS . 
Implementation of zero inflated distribution can be directly specified in WinBUGS using 
the zeros-ones trick using the syntax 
for the ZIP. For other distributions we need to specify appropriately f d  [il . 
Alternatively, the WinBUGS code can be based on the mixed Poisson representation 
Y / U  - Poisson(X(1 - U ) )  with U N Bernoulli(~io) 
resulting in 
fZIP(Y) = Tofp(Y; 0 )  + (1 - ro)fp(y; A) = r o q y  = 0 )  + (1 - no)fp(y; A) 
since the Poisson with mean equal to zero degenerates to I ( y  = 0), that is, to a distribution 
with probability of zero value equal to one and all other values having zero probability. 
Hence the corresponding model is defined using the commands 

290 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
This is the approach proposed by Ghosh et al. (2006). 
A beta prior for T O  (or a uniform prior when no information is available) can be used. 
The usual normal prior distributions can be facilitated for p and p when linear structure 
is assumed for the logit of TO. 
The zero inflated version of the negative binomial can be fitted in a similar manner in 
the case of ZIP. Using the first approach, we need to define the success probability T of the 
negative binomial by 
T i  
Ti = 
with Ui - BernouIli(To), 
ri + Xi(1 - Ui) 
which can be defined in WinBUGS by 
for i = 1,. . . , n. Finally, the zero inflated generalized Poisson distribution (or any other 
distribution not in included in the list of the WinBUGS predefined distributions) can be 
fitted using the zeros-ones trick as described above. 
Example 8.3 (revisited-2): 
Fitting the zero inflated models on GSS 1990 data. Zero 
inflated versions for the Poisson, negative binomial, and generalized Poisson distributions 
can be fitted using either the zeros-ones trick or the mixed Poisson model approach, with 
both approaches giving similar results. WinBUGS internal node of deviance (deviance) 
yields different results under both approaches since, in the mixed Poisson representation, 
the conditional deviance for each given ui is calculated. Hence, the deviance calculated by 
WinBUGS in this case is given by 
n 
n 
Deviance, 
= -2):logf(yi/ui,7~,A) 
= - 2 E I ( u i  = O)logfp(yi; TO,A) 
i=l 
i= 1 
for each iteration instead of the actual deviance 
n 
Deviance = -2 
logf(yi/ro, A) 
i= 1 
n 
which marginalizes out the augmented data ui. Hence we recommend specifying the latter 
as a deterministicAogica1 node in WinBUGS and using it directly. 
Posterior summaries of model parameters for the zero inflated models for each gender 
are provided in Table 8.6. Results of all ZI models indicate that 
0 The zero inflated component considerably improves the model fit according to the 
DIC value, while ZIGP seems to fit the data better than do the other models under 
consideration here. 
0 The proportion of excessive zeros is different for the two genders: about 22-27% 
and 3 5 4 1 %  (depending on the model) of males and females, respectively, remain- 
ing inactive during the last month in addition to what was expected from the main 
distributional assumption. 

ADDITIONAL MODELS FOR COUNT DATA 
291 
0 Overdispersion parameters are important for the model since both ZINB and ZIGP 
have much lower DIC values than do the corresponding Poisson models. 
0 Minor differences between the posterior distributions of r and w parameters of the 
ZINB and ZIGP are observed between the two genders. 
Table 8.6 
Poisson and zero inflated versions of Poisson, negative binomial, and generalized Poisson 
models for Example 8.3 
Posterior means (95% posterior intervals) for model parameters of generalized 
ParameteP 
ZIP 
ZINB 
ZIGP 
Generalized Poisson 
0.272 (0.219, 0.329) 0.226 (0.158, 0.291) 0.242 (0.184, 0.304) 
- 
0.413 (0.360, 0.469) 0.355 (0.283, 0.423) 0.377 (0.312, 0.438) 
- 
2.085 (2.029, 2.140) 2.026 (1.885, 2.162) 2.043 (1.907, 2.168) 1.767 (1.592, 1.966) 
0.910 (0.833, 0.984) 0.883 (0.710, 1.081) 0.900 (0.743, 1.076) 0.751 (0.536, 1.008) 
-0.094 (-0,183, -0.016) -0.129 (-0.342, 0.078) -0.1 10 (-0.297, 0.073) -0.298 (-0.624, 0.008) 
- 
1.65 
(1.15, 2.26) 
0.58 
(0.52, 0.64) 
0.71 
(0.65, 0.76) 
- 
1.42 
(0.96, 2.00) 
0.59 
(0.53, 0.65) 
0.75 
(0.70, 0.81) 
- 
0.17 
(0.13,0.23) 
2.84 
(2.36,3.34) 
1.07 
(0.93, 1.23) 
5.86 
(5.31,6.39) 
5.88 
(5.08, 6.77) 
5.85 
(5.08, 6.74) 
5.88 
(4.91, 7.14) 
4.30 
(3.84,4.77) 
4.30 
(3.69, 5.04) 
4.32 
(3.67, 5.02) 
4.38 
(3.51, 5.56) 
7.45 
(6.98,7.93) 
55.98 (42.70, 71.37) 35.45 (24.82, 51.84) 
71.47 (43.65, 122.4) 
6.07 
(5.59,6.53) 
36.54 (27.47,47.48) 27.84 (19.00,41.45) 
75.44 (40.71, 144.0) 
1.27 
(1.22, 1.33) 
9.48 
(8.15, 10.83) 
6.03 
(4.58, 8.05) 
12.00 
(8.38, 17.52) 
1.41 
(1.36, 1.47) 
8.46 
(7.15, 9.70) 
6.41 
(4.82, 8.70) 
16.87 (11.07,26.79) 
3663 
(3659,3670) 
2815 
(2810,2824) 
2809 
(2804,2817) 
2922 
(2919,2930) 
3667.412 
2821.139 
28 14.802 
2926.440 
~ 
0.18 
(0.13,0.23) 
3.23 
(2.75,3.74) 
1.71 
(1.47, 1.96) 
aParameter indices refer to gender (l=males, 2=females). 
*DIG values for the zero inflated models were calculated outside WinBUGS using the posterior means of 
the model parameters; DICs for Poisson and negative binomial models are equal to 5271.35 and 2873.46, 
respectively. 
8.3.4 The bivariate Poisson model 
Karlis and Ntzoufras (2003~) have constructed log-linear models based on the bivariate 
Poisson distribution [for details, see Kocherlakota and Kocherlakota (1 992) and references 
cited therein] for modeling sports outcomes; see also Karlis and Tsiamyrtzis (2008) for 
conjugate Bayesian analysis. 
Let us consider two discrete random variables Y1 and Y2, which jointly follow a bivariate 
Poisson distribution: 
(Yl, Y2) 
BP(X1, Xz, A3). 
Then, their joint probability function is given by 
f B P  (Y11 Y2) = PY, ,v2 (Y1, Yz) 

292 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
This bivariate distribution is derived by the following convenient latent variable repre- 
sentation 
Y1 = 21 + 2 3  & Y 2  = 22 + 2 3  with Z k  - Poisson(&) 
where z k  are independent. From this representation, we can easily calculate that E ( Y k )  = 
XI, + X3 and that v ( Y k )  = 
+ A3 while the covariance Cov(Y1, Y 2 )  = X3. Hence the 
distribution allows for a positive dependence between the two random variables Y1 and 
Y2. Note that for A3 = 0, the two variables are independent and the bivariate Poisson 
distribution reduces to the product of two independent Poisson distributions (also referred 
as a double-Poisson model) and is equivalent to the usual Poisson log-linear model; more 
details concerning the BP distribution can be found in Kocherlakota and Kocherlakota 
(1 992) and Johnson et al. (1 997). 
Covariates can be linked with the logarithms of X k  [see, e.g., Karlis and Ntzoufras 
(2003a)l directly on the marginal means X 1 + A3 and A2 + X 3 .  
To specify the BP model in WinBUGS , we can use the latent variable representation 
and define it using the zeros trick. Hence when Y1 and Y2 are observed, we can rewrite the 
probability function (8.3) as 
min(yl,yz) e - X 1 ~ y l - I ,  e - X Z A y - k  
e-X 
k 
A3 
c (y1 - k)! (y1 - k)! 
k! 
fBP(Ylr Y2) 
= 
k=O 
min(vl,vz) 
where 2 1  = y1 - k, 2 2  = y 2  - k and Z3 = k. To reproduce this function in WinBUGS , 
we can generate 23 latent data by considering them as Poisson distributed interval censored 
data at [O, min(y1, y ~ ) ] ,  
then set 21 = y1 - z3 and z2 = y2 - z3 and define the remaining 
likelihood using the zeros trick. Hence the WinBUGS code for this implementation is given 
by 
Note that initial values must be also defined for z3. We recommend avoiding random gen- 
eration of initial values in this case because noninteger values might be simulated resulting 
in logical errors within WinBUGS . A further problem that may appear with the addition of 
covariate structure on A3 is that the MCMC algorithm may adhere to very large values of 

ADDITIONAL MODELS FOR COUNT DATA 
293 
X3 indicating zero covariance between the two response variables for specific individuals. 
Hence, the user must monitor convergence and try alternative methods when suspicious 
behavior appears in the coefficients of the covariance term X 3. 
The zeros trick can also be used to implement the BP model. Additionally, a discrete 
uniform “pseudoprior” must be imposed on 2 3  latent data. An advantage of this approach 
is that we do not specify initial values for Z3. Using this approach, z3 is now defined by 
while the zeros part (which specifies the augmented likelihood) is now given by 
A short example using simulated data follows. Implementation in the association football 
data of Example 7.2 can be implemented in a straightforward manner; see Karlis and 
Ntzoufras (2003b) for more details. 
Example 8.4. Bivariate simulated count data. To illustrate the bivariate Poisson 
model, we have generated 100 observations from the following model 
(Kl, Kz) 
BP(X12, X 2 i ,  A3i) 
log(X1i) = 0.3 + 0.22i1 + O . l ~ i 3  
lOg(Xzi) = 0.5 - 0 . 3 ~ i z  - 0.5xi5 
log(Xsi) = xi4 for i = 1 , 2 , .  . . , 100, k = 1,2,3, 
where xij (j=1,2,3,4,5) are random samples from the standardized normal distri- 
bution. 
Posterior summaries of the full model and the true model are provided in Table 8.7. 
Normal prior distributions were used with zero mean and variance equal to 100. Results 
using both approaches described above give equivalent results. As we can see, estimated 
values are close to the actual ones. For this illustration normal prior distributions with zero 
mean and large variance were used. Model code of for this example is available in this 
book’s Webpage. 
8.3.5 The Poisson difference model 
Karlis and Ntzoufras (2006) have recast interest in Skellam’s (1 946) distribution implement- 
ing Bayesian inference for dental epidemiology data. A model based on this distribution 
was used for association football data by Karlis and Ntzoufras (2008). The zero inflated 
version of this distribution was also considered in both of these publications. Skellam’s 
(1946) distribution is not directly connected to the distribution of counts, but to their dif- 
ference. For this reason it is also called the Poisson difSerence (PD) distribution. It can 
be used for modeling discrete response variables defined inZ = {. . . , - 2 ,  -1,O, 1,2, . . .} 

294 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
Table 8.7 
models for simulated data of Example 8.4 
Posterior summaries for model parameters of full and true bivariate Poisson 
Node 
P I 0  
01 1 
P I 2  
PI3 
PI5 
020 
P2 1 
P22 
P24 
P14 
P23 
P2 5 
P30 
P3 1 
1032 
a33 
P34 
P35 
Full model 
Actual 
Percentiles 
value 
Mean 
SD 
2.5% 
97.5% 
0.3 
0.2 
0.1 
0.0 
0.0 
0.0 
0.5 
0.0 
-0.3 
0.0 
-0.5 
0.0 
0.0 
0.0 
0.0 
0.0 
1 .o 
0.0 
~ 
0.330 
0.102 
-0.081 
0.118 
-0.085 
-0.037 
0.446 
-0.087 
-0.416 
0.132 
-0.010 
-0.575 
-0.0 17 
0.016 
0.033 
-0.104 
0.979 
0.056 
- 
0.106 
0.085 
0.072 
0.079 
0.075 
0.070 
0.102 
0.063 
0.057 
0.059 
0.058 
0.060 
0.146 
0.073 
0.061 
0.073 
0.072 
0.076 
True model 
Percentiles 
Mean 
SD 
2.5% 
97.5% 
0.117 
0.532 
-0.070 
0.266 
-0.224 
0.053 
-0.036 
0.274 
-0.226 
0.061 
-0.178 
0.101 
0.245 
0.641 
-0.205 
0.041 
-0.534 
-0.307 
0.020 
0.251 
-0.119 
0.109 
-0.694 
-0.453 
-0.325 
0.243 
-0.128 
0.161 
-0.088 
0.152 
-0.249 
0.038 
0.842 
1.129 
-0.092 
0.210 
0.369 
0.077 
0.215 
0.519 
0.141 
0.073 -0.007 
0.285 
-0.079 
0.068 -0.208 
0.063 
- - - 
~ 
~ 
- 
- - 
0.502 
0.072 
0.356 
0.636 
-0.410 
0.056 -0.521 
-0.297 
-0.533 
0.049 
-0.627 -0.437 
~ 
- 
- 
- 
- 
- 
~ 
~ 
- 
- 
~ 
~ 
~ 
- 
- 
- 
0.962 
0.035 
0.892 
1.030 
- 
- 
~ 
~ 
Note: Linear predictor of full model: log & = PO + c:=, 
P h3 X 23 , k=l,2,3 
and differences in correlated count data. The advantage in the latter case is that we re- 
move additive correlation in much the same manner as in the usual paired t test in classical 
inference. In this way, the problem can now be solved using a univariate response model. 
The PD is defined as the distribution of a random variable Y with probability hnction 
for all y E Z ,  XI, 
A2 > 0, where I,(y) is the modified Bessel function of order T 
(Abramowitz and Stegun, 1974, p. 375) given by 
Generally, this can be expressed as the difference of two Poisson latent variables with 
parameters A1 and Xz: 
zk - Poisson(&), fork = 1 , 2  * Y = 21 - ZZ - PD(X1, XZ) . 
Although the Skellam's (1 946) distribution was originally derived as the difference be- 
tween two independent Poisson random variables, it can be also derived as the difference 
between distributions that have the following trivariate latent variable structure: 
Y k  = 21, 
+ 2 3  with 2, - D(&) & zk - Poisson(&) for k = 1,2. 
(8.5) 

ADDITIONAL MODELS FOR COUNT DATA 
295 
In other words Z3 may follow any distribution with parameter vector 6 3 .  An interesting 
property is that the joint distribution of Yl, Y2 is a bivariate distribution with correlation 
induced by the common stochastic component in both variables 2 3 .  For example, if 2 3  
follows a Poisson distribution, then the joint distribution is the bivariate Poisson distribution 
described in previous Section 8.3.4. Another interesting property is that the marginal 
distributions for Y1 and Yz will be Poisson only when 2 3  is a Poisson distributed random 
variable (or zero with probability one). Therefore, in the general formulation, the marginal 
distributions of Y1 and Y2 do not need to be Poisson distributed, but they can be any 
convolution of a Poisson and another discrete random variable. This assumption is less 
restrictive, allowing also for modeling overiunder-dispersed Y1 and Y2, in contrast to the 
assumption of the Poisson marginals of Y1 and Y2 imposed by the BP model. 
The expected value of the PD(X1, A,) distribution is equal to E(Y) = X 1  - X2, while 
the variance is Var(Y) = X1 + X2. For large values of the X 1  + Xz, the distribution can 
be sufficiently approximated by the normal distribution. If X 2 is very close to zero, then 
the distribution tends to a Poisson one; when A 1  approaches zero, then the distribution is 
a negative mirror of the Poisson distribution with mean A 2  (i.e., a Poisson distribution in 
the negative axis). Additional properties of the distribution are described by Karlis and 
Ntzoufras (2006). 
Finally, the trivariate reduction scheme (8.5) of PD provides a convenient data augmen- 
tation scheme that can be efficiently used for the construction of the MCMC algorithm for 
estimation of the posterior distribution; see Karlis and Ntzoufras (2006) for details. This 
latent variable representation can be used to specify the PD model in WinBUGS . 
Following the approach of Karlis and Ntzoufras (2006), for y 2 0 we can write 
fPD(Y; X1, XZ) 
= p(z1 - 2 2  = y/X1, XZ) 
= P(Z1 = Y + 22IX1,XZ) 
= c 
P(21 = y + 22,zz = wlX1, A,) 
w=o 
M 
w=o 
Similarly, for y < 0 we can write 
fPD(Y; xi, xz) = p(Zz = 21 - !//Xi, A,) 
00 
= c f P ( w ;  X l ) f P ( W  - y; X2). 
w=o 
We can use these expressions to generate latent data wi and then use the Poisson augmented 
likelihood. In WinBUGS , we can specify the PD model using using the following syntax: 

296 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
In order to use this approach, we have imposed a discrete uniform distribution on the latent 
data wi. This is achieved indirectly by truncating the continuous uniform distribution. Note 
that, since the uniform distribution is used, an upper limit must be adopted. In this way, we 
essentially used a truncated version of the PD distribution. However, if this limit is high, 
the effect of this truncation will be minimal. 
Example 8.5. Bivariate simulated count data of Example 8.4. Here we use 
the differences in the simulated data to fit the PD model on the difference in the 
simulated data of Example 8.4. 
Posterior summaries of the full model and the true model are provided in Table 8.8. 
Results are quite close to the actual ones for A1 and Az. Nevertheless, the structure of the 
covariance term A3 cannot be estimated now, since differences are considered. 
Table 8.8 
models for simulated data of Example 8.4 
Posterior summaries for model parameters of full and true Poisson difference 
Node 
- 
PI 0 
P1 1 
012 
013 
PI4 
01 5 
P20 
P2 1 
P22 
024 
P23 
P25 
Full model 
True model 
Actual 
Percentiles 
Percentiles 
value 
Mean 
SD 
2.5% 
97.5% 
Mean 
SD 
2.5% 
91.5% 
0.3 
0.2 
0.1 
0.0 
0.0 
0.0 
0.5 
0.0 
-0.3 
0.0 
-0.5 
0.0 
0.349 
0.05 1 
-0.138 
0.194 
-0.022 
-0.092 
0.437 
-0.116 
-0.438 
0.189 
0.034 
-0.607 
~ 
0.135 
0.116 
0.102 
0.109 
0.096 
0.104 
0.126 
0.083 
0.079 
0.079 
0.075 
0.072 
0.103 
-0.195 
-0.344 
-0.017 
-0.199 
-0.3 1 1 
0.204 
-0.278 
-0.600 
0.029 
-0.1 17 
-0.740 
0.617 
0.291 
0.069 
0.401 
0.161 
0.089 
0.671 
0.050 
-0.283 
0.342 
0. I72 
-0.470 
0.356 
0.166 
-0.121 
- 
0.484 
-0.444 
-0.53 
- 
- 
- 
0.1 18 
0.082 
0.110 
._ 
0.110 
0.076 
-0.535 
- 
- 
0.123 
0.013 
-0.358 
- 
- 
0.269 
-0.594 
0.061 
- 
- 
~ 
- 
0.586 
0.337 
0.088 
- 
- 
0.696 
-0.300 
- 
- 
-0.653 
- 
Note: Linear predictor in full model: log&% = PO + c,"=, 
j5'k3Xz3, k=1,2,3 
8.4 FURTHER GLM-BASED MODELS AND EXTENSIONS 
Generalized linear models can be regarded as the general foundation for building more 
advanced models. In this section we briefly discuss some GLM based extensions. 
The following sections focus on survival analysis models (which can be thought of 
as extensions of the models used for positive defined continuous response variables) and 
the multinomial regression models (which are natural extensions of the simple logistic 

FURTHER GLM-BASED MODELS AND EXTENSIONS 
297 
regression models used for binomial responses). The section closes with a brief discussion 
of other related models, how to treat ordinal variables, and related issues. 
8.4.1 
Survival analysis models 
Survival analysis refers to a family of statistical methods used to analyze duration of time 
until an event of interest (such as death) occurs. All models described in Section 8.2.2 
can be used to analyze such response variables since they are positively defined continuous 
random variables. The difference in such data is that we now focus on the hazard function 
instead of the the mean duration time. Moreover, we have to deal with incomplete duration 
time data, termed as censored, which must be incorporated in the model. 
In order to proceed, we need to specify the survival hnction S(y) = 1 - F(y) = 
P(Y 2 y), which provides the probability of surviving until timepoint y. Moreover, the 
hazard function denoted by h(y) is defined as 
and can be interpreted as the death (or event) rate of an individual, provided that this person 
survived until time y: 
As we have already mentioned, focus is on modeling this hazard rate. 
There are three types of censored data: 
0 Right censoring, referring to individuals who are followed from the beginning of the 
study until a time point where they are lost during the follow-up. The exact time of 
death is unavailable, but the know that the person survived until time y. This is the 
most frequent type of censoring encountered in survival analysis data. 
0 Left censoring, referring to cases where the exact time when the subject entered the 
study is unknown but the exact time of death is available. 
0 Interval censoring, referring to cases where both the exact times of death and the 
entry into the study are unknown. This type of censored data inform us that the 
individual was alive at specific timepoints, so we know that the survival time was 
greater than a specific value y. 
The data of censored survival time consist of the response duration times yi and the 
censoring indicators [i which take values of zero and one depending on whether the corre- 
sponding time was death or censoring time. The likelihood is now given by 
n 
f ( Y l L  0) = 
f(Yle)l-wYIw ' 
2= 1 
Hence the incomplete information is modeled via the survival hnction, which reflects the 
probability that the patient was alive for a duration greater than yi. 
In WinBUGS , censoring can be modeled using the commands I (a, 1, I ( , b), and 
I (a, b) for right, left, and interval censoring, respectively. In WinBUGS , for right censored 

298 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
data, we need two variables in order to set up the model. The actual survival times using NA 
values when censored data first appear and the censoring times, which take zero values when 
actual survival times, are observed. Then, assuming, for example, a Weibull distribution, 
the likelihood can be expressed as 
which results in (1) the uncensored Weibull distribution when actual survival times are 
observed and hence cens .time [il =O and (2) the appropriate censored Weibull distribu- 
tion with yi >cens . timei when cens . timei > 0. A detailed example is provided by 
Spiegelhalter et al. (2003~); see the mice data example. Similarly, we can treat other types 
of censored data, or we might build other survival analysis models. 
Another popular model used in survival analysis is Cox’s (1 972) proportional hazards 
model. The model is essentially semiparametric since no assumptions concerning the 
baseline hazard functions are made. In order to implement Cox’s model, we need to consider 
the formulation in the papers by Andersen and Gill (1982) and Clayton (1991) where the 
Cox model is represented as a counting process that was illustrated by Spiegelhalter et al. 
(2003~) 
using a detailed example (leuk data example). 
Finally, survival models can be divided in two major categories: proportional hazards 
models and accelerated failure time models. For the first ones we assume that h ( y i l ~ ( ~ ) )  
= 
X(z(i))ho(yi); that is, the hazard function of an individual i with survival time y i  and 
covariate values 2 (i) ) is proportional to the baseline hazard function, which does not depend 
on covariates. 
On the other hand, accelerated failure time models assume that the hazard function is 
given by h(yiIz(i)) = X(z(i))ho(X(z(i))yi); that is, the baseline hazard function depends 
on the covariate values. Moreover, accelerated failure time models can be written using the 
following representation 
logy = E(logY-0) - lOgX(z(2)) +log€ 
where Yo be the random variable when all covariates are zero (baseline survival time) and 
E is an error function with expected value equal to one (possible values are exponential, 
log-normal, gamma, and Weibull). Generally, for accelerated failure time models we have 
where 7 is the usual linear predictor. Hence the survival function for covariate values z is 
equal to the corresponding baseline survival function (i.e., for x = 0) accelerated by e -n. 
For more details, the reader is referred to specialized survival analysis textbooks, such as 
the classic book by Cox and Oakes (1984) or the more recent one by Hosmer et al. (2008). 
8.4.2 Multinomial models 
Multinomial logistic regression models are natural extensions of the binomial logistic re- 
gression models and are used when the response of interest are multicategorical variables. 
For example, such models can be used to model the final outcome (win/draw/loss) of a 
football game instead of the actual score. 
Assuming that the response variable Y i = (XI, . . . , X K )  has K levels, where yZk 
denotes the frequency of the kth level, the multinomial logistic regression model can be 
written as 
Y ,  N multinomial(7ri, Ni) 

FURTHER GLM-BASED MODELS AND EXTENSIONS 
299 
and 
k 
for k = 2, . . . , K ,  where 7ri = ( 7 r i 1 , 7 r ~ ,  . . . , 7 r i ~ ) ~  
is the vector of the probabilities for 
each level of variable Y for individual i with 7ril = 1 - cr=’=, 
7rik and y j k  are the usual 
binary indicators identifying the structure of the model and which variables specify or 
affect each odds. It is common practice to use similar structure for all odds ?rik/Til; see, 
for example, in Agresti (2002, chap. 7) for a comprehensive treatment of the subject. 
Solving (8.6) is terms of response probabilities results in 
This can be summarized by 
The restriction ~
i
l
 
= 0 can be indirectly imposed by setting all coefficients ofthe first linear 
predictor Pj1 equal to zero. This can be implemented in WinBUGS using the commands 
Usually individual data are observed, implying that Ni = 1. In such a case, the response 
variable will be given as a categorical variable with codes denoting each level. To specify 
this in terms of the multinomial distribution, it must be recoded using K dummy variable 
indicators that will identify which level appears in each case in a representation compatible 
with the multinomial notation. Alternatively, in WinBUGS , the categorical distribution 
(command dcat) can be used instead. In this way, the original variable (without transfor- 
mation or dummy variables) can be used. The corresponding expression in WinBUGS is 
given by 
A simple approach that is frequently used express the above mentioned model by separate 
K - 1 simple logistic regression models comparing each model with a baseline category. 
Although, this might be problematic in classical inference when certain constraints on 
specific parameters need to be incorporated to the different logistic models, this can be 
easily incorporated in the WinBUGS model specification. Moreover, Agresti (1990, pp. 
3 10-3 12,2002, pp. 273-274) cautions that estimates may be inefficient when the baseline 
category does not have sufficient observations. Hence it is advisable to select as baseline the 
category with the highest number of observations and then transform estimates as desired. 
To implement the separate regression models approach in WinBUGS , we need only to 
define & N binomial(p:k, N i k )  with Nik = x k  + x1. Hence the likelihood will be given 
by 

300 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
Note that 7r;k does not have the same meaning as in the original multinomial model since now 
it refers to the conditional probabilities given that only the category k and the baseline one 
are observed. Nevertheless, the ratio li~ik/li~il 
estimates the same odds as in the multinomial 
logistic model. 
Finally, when high-dimensional contingency tables are considered, we may use the 
connection between multinomial and log-linear models to indirectly fit the first one using the 
latter; see, for example, Agresti (2002) for more details. This was indeed the practice used 
for many years in conventional statistical packages. Nevertheless, it is not recommended 
in WinBUGS since direct fit of the multinomial model can be easily achieved. 
A detailed multinomial example is provided by the second volume of WinBUGS examples 
[see Spiegelhalter et al. (2003b), aligators data example; see also Spiegelhalter et al. (1996c, 
table 7.16, pp. 51-54)]. WinBUGS code for a similar alligators dataset (Agresti, 2002, p. 
304), with the alligators’ size as quantitative covariate, as well as Agresti’s (2002, table 
7.15) political party data are provided in this book’s Website. 
Other related examples are the biopsies and the endometrial (endo) data examples, which 
are also provided by Spiegelhalter et al. (2003b). The latter illustrates also the association 
between multinomial and Poisson log-linear models in a simple 2 x 2 contingency table; 
see also Spiegelhalter et al. (1996c, pp. 55-57). Details concerning multinomial regression 
models and extensive examples can be also found in the recent books of Congdon; see 
Congdon (2006b, chap. 7) and Congdon (2005~1, 
pp. 198-210). 
Finally, other link functions such as the probit can be easily accommodated in the multi- 
nomial models, but here we restrict ourselves in the logistic case, which is the most frequent 
one; see, for example, Congdon (2005~1, pp. 210-231) for details Concerning the probit 
model. 
8.4.3 
Additional models and further reading 
A wide variety of GLM-based models exist in the statistical bibliography. Unfortunately, 
not all of these models can be described in this introductory book. Hence, we focus on 
some basic models and their extensions. In this closing section, we briefly describe some 
additional models and provide some annotated references for hrther reading concerning 
GLM-based models. We also briefly review models for ordinal responses. 
In the previous sections we have described how to model continuous variables defined 
in the whole real line (normal models), positive continuous variables (log-normal, gamma, 
inverse Gaussian, etc.), binary and multinomial models (using logit and probit models), and 
discrete random variables (based on Poisson and negative binomial models). Multivariate 
extensions of such models have been reported in the literature that account for correlations 
between observations; see Fahrmeir and Tutz (2001) for a good overview of the topic. 

PROBLEMS 
301 
Concerning continuous variables, the multivariate normal or t distributions are frequently 
used [see, e.g., Tiao and Zellner (1964), Zellner (1976) and Box and Tiao (1973, sec. 
8.2, 8.3)] while other skewed multivariate distributions can be adopted as proposed by 
Sahu et al. (2003). Concerning discrete count data, the multivariate Poisson models have 
been introduced in the statistical literature (Tsionas, 2001 ; Karlis and Melikotzidou, 2005), 
while multivariate versions of logistic regression models also exist [see, e.g., Glonek (1 996), 
Movellan (2006) for a tutorial, and O’Brien and Dunson (2004) for the Bayesian approach]. 
Usually correlated repeated measurement data are modeled using random effects or 
hierarchical models, which are described in the following chapter. An alternative approach 
is modeling the marginals assuming a type of covariance between the variables. This is 
traditionally treated using generalized estimating equations (GEES), which are not discussed 
extensively within the Bayesian framework. For an smooth introduction to these two topics, 
see Agresti (2002, chaps. 11,12). More recently, copulas have been considered for the joint 
modeling of multivariate data; see Kolev et al. (2006) for a review of the topic and Huard 
et al. (2006), Pitt et al. (2006) for Bayesian implementation of such models. 
Another interesting topic is incorporation of ordinal variables in our model. Ordinal 
response variables have been extensively modeled using cumulative versions of multinomial 
logit or probit regression models, cumulative link models, or other variants of such models 
(e.g., continuation ratio, adjacent categories, or proportional odds models) [Agresti (2002, 
chap. 7) and Dobson (2002, sec. 8.4)]. Moreover, a full review of the topic is provided 
by Liu and Agresti (2005). In two-way contingency tables, Goodman’s (1979, 1981) 
association models can be used to estimate the column and row scores of ordinal variables 
using order restrictions; for the Bayesian implementation of such models, see Iliopoulos 
et al. (2007u,b) . Other models that can be used to reveal the ordinal structure of two cross- 
classified variables are the RC(M) models (Goodman, 1985), which are generalizations of 
the simple association models and are also related with the correlation models (i.e., the 
model-based analog of correspondence analysis); for more details, see Agresti (2002, secs. 
9.5,9.6) and Kateri et al. (2005) for the Bayesian implementation of RC(M) models. 
To summarize, the reader can refer to Lindsey (1 997) and Fahrmeir and Tutz (2001) for 
further variations and GLM extensions. More details on Bayesian inference of GLM-based 
models and their extensions can be found in the books by Congdon (2003; 2005~; 
2006b), 
while more advanced GLM based models are extensively presented and described by Dey 
et al. (2000). Finally, a wide collection of the available Bayesian models and approaches 
used for categorical data analysis is described in the excellent review provided by Agresti 
and Hitchcock (2005). 
Problems 
8.1 
Consider the following data: 
1.3 2.1 2.1 2.3 1.0 1.0 2.8 2.1 2.7 2.3 3.2 4.2 2.3 1.1 2.1 
2.4 1.7 1.7 2.1 1.8 2.0 3.0 2.0 2.3 1.3 1.0 2.2 2.1 1.9 1.8 
a) Use the inverse Gaussian, gamma, and log-normal distributions for these data. 
b) Compare the fitted distributions with the actual data. Which distribution do you 
c) Use the DIC to identify which distribution fits the data best. For the inverse 
think fits the data best? 
Gaussian distribution, calculate DIC using a software other than WinBUGS . 

302 
MODELS FOR POSITIVE CONTINUOUS DATA, COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
d) Use a normal distribution of the logarithms of the data above. Compare the 
posterior distributions of the parameters with those obtained by assuming the 
log-normal distribution for the data. 
e) Calculate DIC for the model of (d) and compare it with the corresponding one 
estimated using the log-normal distribution. How are they connected? 
Return to the data of Problem 6.9. 
a) Use the inverse Gaussian, gamma, and log-normal distributions to determine 
which variables influence the total money spent according to the receipt total. 
b) Use DIC to finally determine which variables are appropriate for each model. 
c) Interpret the parameters ofeach model on the basis of their posterior distributions. 
d) Compare the models assuming different distributions using DIC. Also compare 
your results with the corresponding ones assuming the normal distribution for our 
data. 
8.2 
8.3 
Consider the data of example mice of WinBUGS examples volume 1 (Spiegelhalter 
et al., 2003~). 
a) Use the Weibull, gamma, and log-normal distributions to model the survival times. 
For the incomplete censored times for predefined distributions, use the command 
1 (a, >; see Spiegelhalter et al. (2003q d )  for more details. 
b) Use DIC to identify which variables are appropriate for each model. 
c) Interpret the parameters ofeach model on the basis of their posterior distributions. 
d) Compare the models assuming different distributions using DIC. 
Consider the data of the example kidney of WinBUGS examples volume 1 (Spiegel- 
halter et al., 2003~). 
a) Use the Weibull, gamma, and log-normal distributions to model the survival times 
for the first instance only (ignore random effects). For the incomplete censored 
times, use the command I (a, ) ; see Spiegelhalter et al. (2003a, d )  for more details. 
b) Use DIC to determine which variables are appropriate for each model. 
c) Interpret the parameters of each model according to their posterior distributions. 
d) Compare the models assuming different distributions using DIC. 
Use the inverse Gaussian distribution to model the survival data mice and kidney 
analyzed in Problems 8.3 and 8.4. For the censored observation, use the survival 
function of the inverse Gaussian distribution given by 
8.4 
8.5 
a) Compare the results with the corresponding ones from the models using other 
b) Calculate DIC outside WinBUGS and compare it with the corresponding DIC 
distributions. 
values for the models based on other distributions. 
8.6 
Consider the following data 
Y 
1 0  1
2
 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 2 0  
Frequencies I31 37 37 35 23 30 31 8 25 13 8 2 5 6 3 5 1 

PROBLEMS 
303 
a) Use the Poisson, negative binomial, and generalized Poisson distributions to 
b) Compare the fitted distributions with the actual data. Which distribution do you 
c) Use the DIC to identify which distribution fits the data best. 
model these data. 
think fits the data best? 
8.7 
Consider the following data 
Y 
( - 7  -5 -4 -3 -2 -1 0 1 2 
3 
4 
5 
6 7 
8 
9 12 
Frequencies I 1 2 
4 
6 
7 14 21 28 30 29 20 10 14 5 
4 
4 
1 
a) Use the Poisson difference distribution to model these data. 
b) Compare the fitted distributions with the actual data. Which distribution do you 
c) Use the DIC to identify which distribution fits the data best. 
Consider the Epil data from WinBUGS examples volume I (Spiegelhalter et al., 
2003a). 
a) Construct a bivariate Poisson model comparing the first and the second instances. 
b) Interpret the model coefficients. 
c) Determine whether the covariance term can be assumed constant or whether co- 
Consider the Epil data from WinBUGS examples volume 1 (Spiegelhalter et al., 
2003a). 
a) For the differences Yz - Yl, 
Y3 - Yl, and Y4 - Y1, construct models on the basis 
b) Interpret the model coefficients. 
think fits the data best? 
8.8 
variates are needed to appropriately estimate this term. 
8.9 
of the Poisson difference distribution. 
8.10 
Use the alligators' primary foodchoice fromthedataofWinBUGSexample alligators 
(Spiegelhalter et al., 2003b) as the response variable in the following models. 
a) Fit binomial regression models comparing each type of food with fish (first level 
b) Fit a model for the food preference according to the multinomial distribution. 
Use the zero-one tricks to fit the folded normal distribution to the data of Problem 
8.1. The density function of the folded normal distribution is given by 
of the preferred choice of food). Interpret the model parameters. 
Compare the results with the ones from simple binomial-based models. 
8.11 
while the mean is given by 
a) Estimate the posterior distributions of E(Y), a, and b. 
b) Compare the fitted distribution with the observed data and the corresponding fitted 
Use a multinomial model for the data of Italian championship 2000 described in 
Problem 7.4 (consider only the final outcome of each game; widdrawiloss, and use 
structure similar to that in the Poisson regression model described in Section 7.4.3). 
distributions of Problem 8.1. 
8.12 

304 
MODELS FOR POSITIVE CONTINUOUS DATA. COUNT DATA, AND OTHER GLM-BASED EXTENSIONS 
a) Evaluate the performance of each team on the basis of the posterior distributions 
b) Reproduce the league. Compare the results with the corresponding ones from the 
Use a Poisson difference distribution to model the goal differences in the water polo 
data described in Problem 7.3; see also Karlis and Ntzoufras (2008) for details. 
a) Evaluate the performance of each team on the basis of the posterior distributions 
b) Reproduce the league using simulated values from the fitted model. Compare the 
of the parameters. 
Poisson model. 
8.13 
of the parameters. 
results with the corresponding ones from the Poisson model. 

CHAPTER 9 
BAYESIAN HIERARCHICAL MODELS 
9.1 INTRODUCTION 
Bayesian models have an inherently hierarchical structure. The prior distribution f(0la) of 
the model parameters 8 with prior parameters a can be considered one level of hierarchy, 
with the likelihood as the final stage of a Bayesian model resulting in the posterior dis- 
tribution f(0ly) c( f(yl0)f(0; a) via the Bayes theorem; see Figure 9.1 for a graphical 
representation of the hierarchical structure of a typical Bayesian model. 
Figure 9.1 
parameters, oval nodes refer to stochastic components of the model. 
Graphical representation of standard Bayesian model, Squared nodes refer to constant 
To capture the complicated structure of some data, the prior is frequently structured 
using a series of conditional distributions called hierarchicalstages of the prior distribution. 
Hence, a Bayesian hierarchical model is defined when a prior distribution is also assigned 
on the prior parameters a associated with the likelihood parameters 0. The posterior 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
305 

306 
BAYESIAN HIERARCHICAL MODELS 
distribution can be written as 
f(elY) 
f(ylW(@; a ) f ( a ;  
b) 
f ( Y l Q ) f ( ~ l a ) f ( 4 b )  
’ 
The prior distribution in this model formulation is characterized by two levels of hierarchy: 
f(8la) (first level) and f(alb) (second level). Prior distributions of the upper levels of a 
hierarchical prior are called hyperpriors and the corresponding parameters, hyperparume- 
ters; in the example above, f(alb) is the hyperprior and b are the hyperparameters of the 
prior parameters a. The structure can be extended by adding more levels of hierarchy if 
needed. 
This structure is easily depicted using directed acyclic graphs and can be designed within 
WinBUGS using the DOODLE tool; see Appendix A for details. Figure 9.2 depicts the 
simple two-stage hierarchical model described above. 
Figure 9.2 
refer to constant parameters, oval nodes refer to stochastic components of the model. 
Graphical representation of a two-stage Bayesian hierarchical model. Squared nodes 
Generally, any Bayesian model with parameters 8 and + and prior distribution f(O,+) 
can be written in a hierarchical structure if the joint prior distribution is decomposed to a 
series ofconditional distributions such as f(O,+) = f(OI+)f(+). 
In hierarchical models, 
hyperparameters + are rarely involved in the model likelihood. 
Hierarchical models can be considered as a large set of stochastic formulations that 
include many popular models such as the random effects, the variance components, the 
multilevel and the generalized linear mixed models (GLMM). Latent variable models can 
also be viewed as hierarchical models since random effects can also have a latent variable 
interpretation. 
Details concerning Bayesian hierarchical models can be found in many standard text- 
books; see, for example, Gelman et al. (2004, chaps. 5, 15) and references cited therein, 
Robert (2007, chap. lo), Lawson et al. (2003, chap. 2), and Woodworth (2004, chap. 11). 
An extensive treatment of the subject can also be found in Dey et al. (2000) and Gelman 
and Hill (2006). 
9.1 .l 
A motivation for introducing hierarchical structure in a simple model is provided by Spiegel- 
halter et al. (2004, pp. 91-94). Following their example, we assume the case of hav- 
ing K groups or units under investigation from which we collect a sample of responses 
Kk - D(8); i = 1,2,. . . , n k  and k = 1,. . . , K. Initially, we can assume two alternative 
models: 
1. One model that estimates a common mean effect p (pooled effect). With this ap- 
proach, when, for example, a normal distribution is assumed, we express the model 
as 
A simple motivating example 
- N(p,cr2) fori = 1,. . .,ni andk = 1,2,. . . , K .  

INTRODUCTION 
307 
2. One model that estimates different independent mean effects p k for each group or 
unit (fixed effects). In this approach, when a normal distribution is assumed, we 
express the model as 
X k  - N ( p k ,  2) for i = I,. . . , ni and IC = 1,2,. . . , K. 
Usually, we are interested in model 2, which estimates the expected performance of each 
unit. A disadvantage ofthis approach is that each mean effect p k is estimated independently 
from the other groups. Hence, in a group with small sample size, the posterior uncertainty 
will be large. 
In such cases it is logical to assume that all expected performances pk are observables 
from a population distribution with mean p, namely, an overall population average effect. 
Therefore, a two stage prior distribution may be adopted with hyperpriors: 
P k  
N(P,w2). 
In this way, all mean effects are associated, allowing for “borrowing strength” between 
groups or units and provide more accurate estimations, especially when the sample size 
of specific groups is small. The posterior mean of each pk is a weighted mean of the 
corresponding sample mean of the kth group and the overall mean effect p. 
9.1.2 
Why use a hierarchical model? 
Hierarchical models are inherently implied in population-based problems such the one 
described in Section 9.1.1. Within this context, hierarchical models are widely used in 
meta-analysis in medical research, where information from different sources or studies on 
the same topic is available; see Woodworth (2004, pp. 223-225) for an example. 
More generally, hierarchical models describe efficiently complex datasets incorporating 
correlation or including other properties in our model. Hence, when multivariate or re- 
peated responses are observed, correlation can be incorporated in the model via a common 
“random” effect for all measurements referring to the same individual. This introduces 
a marginal correlation between repeated data, while interpretation is based on the condi- 
tional means. Therefore, given the random effects, the structure and the interpretation are 
similar to common generalized linear models. Accordingly, hierarchical models naturally 
appear, for example, when modeling spatiotemporal data in which correlation between time 
and space can be added by using common random effects on adjacent (in time or space) 
responses. 
Hierarchical models can also be used to imply a complicated marginal distribution but 
(at the same time) keep the conditional structure as simple as possible. Such a characteristic 
example is the use of multiplicative gamma distributed random effects in the Poisson model, 
which implies a negative binomial marginal sampling distribution. In this way, in the 
Poisson case, we account for overdispersion, which may be present in our response data. 
Nevertheless, the conditional structure still consists of a series of Poisson-gamma models. 
Using random effects and the corresponding hierarchical structure to appropriately specify 
the marginal sampling distribution is frequently referred to as data augmentation. Such 
an approach considerably simplifies the MCMC scheme that can be used to estimate the 
posterior distributions of interest. 

308 
BAYESIAN HIERARCHICAL MODELS 
9.1.3 Other advantages and characteristics 
An important characteristic of hierarchical models is that each parameter referring to a 
specific group or unit borrows strength from the corresponding parameters of other groups or 
units with similar characteristics. In other words, a shrinkage effects towards the population 
mean is present with use of hierarchical models. The volume of shrinkage depends on the 
variance between the random parameters. This can be quite beneficial, especially when a 
small number of individuals is observed in some groups. In such cases, the reduction of the 
uncertainty is large since information from other groups or units with smaller variability is 
incorporated in the posterior estimates. The so-called exchangeability assumption plays a 
central role in this behavior. The exchangeability assumption is used when no information 
concerning the structure of the parameters of interest is available. With this approach, we 
assume that all random parameters come from a common population distribution and their 
ordering does not affect the model and the results, that is, the hyperprior is invariant to 
permutations of the random parameters. In general we can say that B j for j = 1, . . . , K are 
exchangeable if B j  Iq5 - D(q5) for all j .  For more details and an interesting discussion, see 
Gelman et al. (2004, sec. 5.2). 
Generally, hierarchical models are more flexible than are the typical nonhierarchical (or 
fixed effects) models since a more complicated structure is accommodated in the model. 
For this reason, they describe the data better, especially when the sample size is large. On 
the other hand, a complicated hierarchical formulation may lead to a model that overfits 
the data; that is, it may describes the current dataset better but might not allow for enough 
uncertainty in order to predict sufficiently future observations. 
Finally, Robert (2007, sec. 10.2.2) provides a series ofjustifications and advantages for 
using hierarchical models, including the fact that the prior is decomposed into two main 
parts: one referring to structural information or assumptions concerning the model and one 
referring to the actual subjective information of the model parameters. Another advantage, 
according to the same author, is that hierarchical structure leads to a more robust analysis, 
reducing subjectivism since posterior results are averaged across different prior choices of 
parameters of interest. Finally, the hierarchical structure simplifies both the interpretation 
and the computation ofthe model since the corresponding posterior distribution is simplified, 
resulting in conditional distributions of simpler form. This allows for the implementation of 
simpler Gibbs-based sampling schemes; for examples, see Sections 2.3.3 (Gibbs sampler) 
and 2.3.5 (slice sampler). 
9.2 SOME SIMPLE EXAMPLES 
9.2.1 Repeated measures data 
Example 9.1. Repeated measurements of blood pressure. Let us consider blood 
pressure measurements, which are well known for their variability. For illustration 
we consider the repeated measurements of blood pressure from 20 healthy individ- 
uals (see Table 9.1). The aim here is to estimate within-individual variability and 
between-individual variability. 
9.2.1.1 
In this example, we can split the overall variability into 
two sources: ( I )  between-subject variability and (2) within-subject variability. Moreover, 
there is an obvious correlation between the measurements of the same individual. To 
Model formulation. 

SOME SIMPLE EXAMPLES 
309 
Table 9.1 Blood pressure measurements of 20 healthy individuals 
Individual 
I 
2 3 
4 5 6 7 8 
9 10 
1 1  12 
13 14 
15 16 17 18 19 20 
Measurement 
I St 
108 91 93 104 99 95 93 99 90 92 101 97 97 96 106 100 90 88 92 100 
2nd 
98 94 96 99 97 98 97 96 100 95 89 97 100 95 100 98 99 98 92 101 
account for the within-persons variability (source 2) and the correlation, we may introduce 
a random effect ai for each individual. Hence we can formulate the model as 
y.. ,, - 
- p + ai + ~ i , j  with tij N N ( 0 ,  02) and ai N N(0, o:) 
or equivalently 
y Z j  - N ( p i j ,  02) with pij = p + ai and ai N N(0, o:) 
for i = 1,2, . . . , R and j = 1,2. Variance component at (the random effects variance) 
measures the between-subject variability (source l), while o accounts for the remaining 
within-subject variability. 
The usual prior distributions with large variances can be used when no information is 
available. Hence we can adopt 
1-1 - N(0, lOOO), o2 - IG(0.001,0.001), and a: N IG(0.001,0.001) 
to express our prior ignorance. 
This simple model is equivalent to assuming that 
where y, = (Kl, K2) and 12 = (1, l)T since yZ3 is expressed as the sum of two inde- 
pendent normal distributions and the parameters are given by 
E(K.7) = 
1-11 
Var(K,) 
= var(a,) + Var(e,,) = of + 2 
COV(K1, K2) = Cov(p +a, + €,l, p +a, + € a )  
- 
2 
- Var(a,) + Cov(a,, t,2) + COV(E,I, a,) + COV(E,~, 
€,a) = oa. 
Thus, the total variability of the response YzJ is equal to 02 + 02, while the covariance 
between two measurements of subject z is equal to the between-subject variability (the 
random effects variance). Thus, we can calculate the within-subject correlation by 
Values close to one imply large within-subject correlation, indicating the importance of 
hierarchicalimixed models, while values close to zero imply low within-subject correlation, 
indicating that random effects do not improve our model. 

310 
BAYESIAN HIERARCHICAL MODELS 
We can generalize these calculations for K (exchangeable) repeated measurement. In 
such case 
wherey, = (Y,l,Y,2, . . . , Y , ~ ) ~ , a n d l ~  
andl[lxJ] areavectoroflengthKandamatrix 
of dimension I x J ,  respectively, with all elements equal to one. 
Although the two model formulations are equivalent, the hierarchical structure facili- 
tates the model and also provides estimates of individual effects. This simpler structure 
considerably simplifies interpretation of the parameters and the required MCMC algorithm 
by augmenting the parameter space using the random effects. 
9.2.7.2 
To fit the hierarchical model described above, we need to set 
up a double nested loop (one for individuals and one for repeated measures). The following 
code can be used to define the model: 
YiIP;a*,a% - NK ( P l K , 4 1 , K x K ]  + 012 ) , 
WinBUGS code. 
Correlations, variances, and standard deviations can be calculated using simple logical/de- 
terministic nodes. 
The corresponding multivariate model can be directly fitted using the command dmnorm. 
The corresponding code for fitting the equivalent multivariate normal model directly is 
provided in the books Webpage (www . stat-athens . aueb. gr/” jbn/winbugs-book). 
Note that generation was slower when using the multivariate normal approach. 
9.2.7.3 Results. Posterior summaries of the hierarchical model used for the above- 
mentioned data are presented in Table 9.2. The posterior mean ofthe total variation is found 
equal to 20.72, from which 1.25 is attributed to between-individual variabilitity. Correlation 
between measurements is relatively low (posterior mean - 0.06), which can be interpreted 
that 6% of the total variation due to between-subject variability. Posterior intervals of indi- 
vidual random effects are depicted in Figure 9.3. The same results were obtained by direct 
application of the multivariate normal approach described in Section 9.2.1.1. 
9.2.7.4 Handling missing data. Hierarchical models can also contribute to the es- 
timation of missing values. If no random effects are assumed, then may use one of the 
following approaches. We can assume a simple model with Yij - N ( p ,  a2) ignoring in- 
dividual effects or a “fixed” effects model with Yij - N ( b  + ai, a2) and a simple prior 
distribution assigned directly on ai. In the first case all estimates of the missing values will 
be based on the posterior density of p ignoring individual differences. In the second case, 
an estimate of the individual effect will be taken directly from the prior when all individual 
values are missing, while in the hierarchical model presented in the Section 9.2.1.1, an 
estimate will be produced by borrowing information from observations in the remaining 
groups. For illustration, let as assume the dataset given in Table 9.3. 

SOME SIMPLE EXAMPLES 
31 1 
Table 9.2 Posterior summaries for Example 9.1 
Posterior 
Posterior 
percentiles 
Node 
Mean 
SD 
2.5% 
97.5% 
r 
0.05 
Ta 
109.50 
LJ2 
19.47 
1.25 
o2 +uz 
20.72 
Correlation 
0.06 
U 
4.38 
C a  
0.72 
2 
g a  
0.01 
265.40 
5.02 
2.63 
4.99 
0.1 1 
0.56 
0.86 
P 
96.70 
0.74 
95.21 
98.14 
0.03 
0.09 
0.11 876.20 
1.42 
31.20 
0.00 
9.16 
3.11 
32.29 
0.00 
0.40 
3.38 
5.59 
0.03 
3.03 
-5 0 
-2 
5 
00 
2 5  
5 0  
Figure 9.3 95% posterior intervals of random effects for Example 9.1. 
Table 9.3 Blood pressure measurements of 20 healthy individuals with missing values 
Individual: 
1 
2 
3 
4 
5 6 7 
8 
9 10 
11 12 
13 14 
15 
16 17 18 19 20 
Measurement 
1st 
108 91 - 104 99 95 93 99 90 92 101 97 97 96 106 100 90 88 92 100 
2nd 
_
_
~
 
~ 
97 98 97 96 100 95 89 97 100 95 100 98 99 98 92 101 

312 
BAYESIAN HIERARCHICAL MODELS 
The posterior distributions of within- and between-subject variabilities (0% and n2, re- 
spectively) are similar to those for the full dataset (posterior means equal to 1.39 and 21.73, 
respectively). Concerning the estimates, we observe that for individuals 2 and 3, posterior 
estimates of blood pressure are close to 96.4 and 96.8 while for individuals 1 and 4 are 
slightly higher (97.4 and 97.2, respectively). 
For the simpler model with a common mean, all posterior means of the missing values are 
equal (as expected) to 96.7. Finally, for the fixed effects model, estimates are significantly 
affected by the observed measurements. Hence for individuals 1 and4, estimates ofposterior 
means are quite high and close to the observed values of the first measurement, while for 
individual 2, the estimated measure is quite low. For individuals with no measurements, the 
posterior means are equal to 93.8, but the posterior variance is quite high, corresponding to 
the large prior variance (equal to 1000); see Table 9.4 for details. On the other hand, results 
from the random effects (hierarchical) model provides accurate estimates of the missing 
values in all cases with posterior intervals of lower range. 
Table 9.4 
Posterior summaries for missing data for incomplete dataset of Table 9.3 
Posterior 
Posterior 
percentiles 
Actual 
Individual Measurement Model Mean 
SD 2.5% 97.5% valuesa 
1 
2 
RE 
97.4 
5.0 
87.6 
107.5 
(98) 
FE 
107.7 
6.1 
95.6 
119.7 
108 
2 
2 
RE 
96.4 
4.9 
86.7 
105.9 
(94) 
FE 
91.1 
6.1 
79.0 
103.4 
91 
3 
1 2  
RE 
96.8 
4.9 
87.2 
106.4 
93 
FE 
93.8 32.6 
28.2 
156.8 
96 
4 
2 
RE 
97.3 
4.9 
87.7 
107.0 
(99) 
FE 
103.8 
6.0 
91.9 
115.9 
104 
All 
CM 
96.7 
4.8 
87.2 
106.3 
- 
aOmitted values are denoted in parentheses. 
Abbreviations: 
RE=hierarchical 
models 
(model using 
individual 
random 
effects); 
FE=nonhierarchical (model using individual fixed effects); CM=common mean (nonhierar- 
chical model with common mean). 
Hence the hierarchical model provides a reasonable compromise between the two ap- 
proaches. It shrinks estimates toward the overall mean by borrowing information from 
other individuals, but at the same time it accounts for individual differences by providing 
more reliable and accurate estimates. 

SOME SIMPLE EXAMPLES 
313 
9.2.2 Introducing random effects in performance parameters 
Example 9.2. Kobe Bryant's field goals in NBA (Example 1.4 revisited). Here 
we reconsider Example 1.4, in which Kobe Bryant's field goals in the NBA were 
examined. A simple logit model for the success probability of Kobe Bryant's field 
goal attempts is adopted with log-odds (or logits) from the same population mean. 
Hence we assume that the performance of the player is similar (exchangeable) from 
season to season. 
We use the following simple hierarchical model 
yt - binomial(Tt, N )  
Ot 
~0 - N(0,lOO) and a; - IG(0.01,0.01), 
- N(p0, a;) for t E (1999,2000,. . . ,2006) 
which can be compiled in WinBUGS using the following syntax: 
Table 9.5 provides estimates from the hierarchical and the corresponding fixed effects 
logit models. All success probabilities and log-odds have smaller variability in the hierar- 
chical model as we have already described; see also Figure 9.4. Finally, DIC indicates that 
the hierarchical model is better in terms of prediction by the corresponding fixed effects 
model (74.3 vs. 77.5). 
9.2-2.7 State space model. This model assumes that all performance parameters 
are exchangeable, that is, the performance remains constant across time and the observable 
seasonal success rate are simple a sample from a constant latent performance random 
variable. 
A model that is frequently used when data occur within a specific time duration, is the 
state space model, where the random coefficients are assumed to follow a normal distribution 
with the mean of the previous value of the performance parameter. Hence the model can 
be written as 
Yt - binomial(Tt, N )  

314 
BAYESIAN HIERARCHICAL MODELS 
0 
2 -  
a0 x -  
CD 
x 
x -  
x -  
x -  
* 
N 
0 
Table 9.5 
probability (%) using fixed effects and hierarchical logit models (Example 9.2) 
Posterior summaries for model parameters and Kobe Bryant’s field goal success 
I- 
I- 
11 1; 11 j I I I 
~~ 
:
+
 
- 8 .  
-~ 
.I 
., 
I 
I 
I 
I 
I 
I 
I 
I 
Fixed effects model 
Hierarchical model 
Node 
Mean 
SD 
2.5% 
97.5% 
Mean 
SD 
2.5% 
97.5% 
& 
-0.178 0.019 -0.216 -0.140 -0.179 0.039 -0.257 -0.102 
cri 
0.079 
0.019 0.0440 
0.118 
0.090 
0.032 
0.048 
0.172 
~ f ,  45.6 
0.5 
44.6 
46.5 
45.5 
1.0 
43.6 
47.5 
rig99 
46.9 
1.4 
44.0 
49.7 
46.4 
1.2 
44.0 
48.8 
TZOOO 
46.4 
1.3 
43.9 
48.9 
46.2 
1.1 
44.0 
48.4 
TZOOI 
46.9 
1.3 
44.4 
49.3 
46.5 
1.1 
44.4 
48.8 
TZOOZ 
45.1 
1.1 
42.9 
47.4 
45.2 
1.0 
43.3 
47.2 
T Z O O ~  43.8 
1.4 
41.0 
46.6 
44.4 
1.2 
41.9 
46.8 
~ 2 0 0 4  43.3 
1.4 
40.6 
46.0 
44.0 
1.2 
41.6 
46.3 
~ 2 0 0 5  45.0 
1.1 
42.9 
47.1 
45.1 
1.0 
43.2 
47.0 
T Z O O ~  47.2 
1.7 
43.9 
50.6 
46.5 
1.4 
43.9 
49.4 
in fixed effects model was calculated as the arithmetic mean ofall @ in each iteration. 
infixed effectsmodel was calculated as the standard deviation ofall @ in eachiteration. 
= 1/(1 + ew,) is each iteration. 
Season 
Figure 9.4 
hierarchical (dashed lines) models for Kobe Bryant’s field goals (Example 9.2). 
95% posterior intervals of model parameters for fixed effects (solid lines) and 

SOME SIMPLE EXAMPLES 
31 5 
Ot - N ( ~ 9 - 1 ,  
a;) for t E (2000,. . . ,2006} 
01999 - N(0,lOO) & U; - IG(0.01,O.Ol) 
The model defined above is more sensible in terms of interpretation, since the perfor- 
mance of the player depends on the previous season’s performance, incorporating in this 
way both physical and psychological effects, while the model defined in the previous section 
assumes exchangeable performances and hence stability across time. 
As expected, this model describes better the performance of the player. The DIC value 
was estimated at 67.8, which indicates a better fitted model than the exchangeable hier- 
archical model of the previous section. Estimates of the posterior success probabilities 
are presented in Fi 
random effects. 
ire-9.5 for comparison with the hierarchical model with exchangeable 
I 
I 
I 
I 
I 
I 
I 
I 
2000101 
2002/03 
2004105 
2006107 
1999100 
2001/02 
2003104 
2005106 
Season 
Figure 9.5 
and hierarchical model (dashed lines) models for Kobe Bryant’s field goals (Example 9.2). 
95% posterior intervals of model parameters for parameters of state space (solid lines) 
9.2.3 
Poisson mixture models for count data 
Example 9.3. 1990 USA general social survey (Example 8.3 revisited). In 
this section we consider two hierarchical models, which are frequently used for 
count data, for the data of Example 8.3: (1) the Poisson-gamma model and (2) the 
Poisson-log-normal model. Both models are simply finite mixtures of the Poisson 
distribution with different assumptions concerning the mixing distribution, resulting 
in a marginal sampling distribution that differs from the Poisson distribution, which 
is the standard assumed model for count data. 
9.2.3.7 
The Poisson-gamma model. The Poisson-gamma model is frequently 
used to model count data with overdispersion. The model is given by 
Y, - Poisson(Xiui) 

316 
BAYESIAN HIERARCHICAL MODELS 
U ,  - gamma(r,,r,) 
for i = 1,2, . . . . n. Covariates can be used to define both A, and u,. Here we consider only 
the gender as a possible covariate; hence we may simply replace X , by XG,, where G, is a 
gender index taking values 1 or 2 when the ith individual is male or female, respectively. 
Similarly, we may replace r, by TG, if we wish to assume different gender parameters for 
the mixing distribution. 
The marginal likelihood of this simple hierarchical model is given by integrating the 
random effects U ,  as described in Section 8.3.1, resulting in a negative binomial distribution 
with probability function 
The code for fitting this model is given by the following syntax: 
Results are the same as in the negative binomial illustration; see Section 8.3.1 for details. 
The only difference we observe is the DIC value (2122.03 instead of 2873.4) since it is now 
based on the conditional likelihood f(ylA 1, XI, 
T I ,  7-2, u) 
instead of the marginal likelihood 
f ( ~ l ~ i ~ ~ i , r ~ , r z )  
= Sf(~IXi,X1,ri,rz,u)f(ulrl,rz)du,, 
whereu = ( U I , . . . , ~ ~ ) ~ ;  
for details, see the computational note at the end of Section 9.2.3. 
9.2.3.2 The Poisson-log-normal model. A usual practice in building hierarchical 
models is to express the model as a typical GLM and then add random errors (or effects) at 
the linear predictor. Following this practice, we can formulate a log-linear Poisson model 
and add a random effect that is usually assumed to be normally distributed. Hence the 
model can be written using the following structure 
r, - Poisson(p,) 
log(pL,) = Po + PIX‘1 + . . + x,, + €2 
E, - N(O,u,2). 
In the current example only one covariate is assumed, which is the gender. A single dummy 
variable for the comer parametrization with “male” as the reference category can be defined 
by setting D, = G, - 1. 
The model formulation is equivalent to assuming 
Y, 
N Poisson(X,u,) 
U ,  = exp(6,) 
N LN(O,CT,~) 
log(X2) 
= Po + PIX,l + . . . + x,,. 

SOME SIMPLE EXAMPLES 
317 
This structure is identical to the one used for the Poisson-gammamodel, but now the mixing 
distribution is no longer gamma but log-normal. Hence the only difference between the 
two models lies in the assumption of the distribution of the random effects and the corre- 
sponding assumed data distribution, which is now Poisson-log-normal instead of negative 
binomial. Unfortunately, the probability function of the Poisson-log-normal distribution 
is not available analytically, making interpretation of this model more complicated than 
that of the negative binomial one. Nevertheless, the mean and the variance can be easily 
calculated and are given by 
E(YIX,a:) = Xe'p/2 
and V(Y/X,az) = Ad':/'+ 
X2ezu: - X2eu: 
In these data we have used a different variance for each gender. 
The code for the Poisson-log-normal model for this dataset is straightforward: 
The DIC value for this model is calculated to be equal to 2295.48, which is much higher 
than the corresponding DIC value for the hierarchical Poisson-gamma model (2122.03), 
indicating a better fit for the latter. Note that we can compare DIC values resulting only from 
hierarchical models since their values are based on the conditional likelihood as described 
in the computational note at the end of this section. 
The insufficient fit of the Poisson-log-normal model is also evident when we examine the 
posterior distributions of E(Y) and V ( Y )  for each gender. For example, the posterior means 
of E ( Y )  are equal to 6.9 and 6.6, respectively, which are far away from the corresponding 
sample estimates (equal to 5.85 and 4.30, respectively), while similar differences are also 
observed for the corresponding variances. 

318 
BAYESIAN HIERARCHICAL MODELS 
9.2.4 The use of hierarchical models in meta-analysis 
The hierarchical structure naturally arises in meta-analysis, where similar parameters are 
estimated in different studies under different circumstances. In the following paragraph we 
present a simple example where the odds ratios from different studies were estimated and 
the aim is to combine results from all studies and produce a common estimate of the risk 
factor for the disease under study. 

SOME SIMPLE EXAMPLES 
319 
Example 9.4. Analysis of odds ratios from various studies. Suppose the odds 
ratios measuring the effect of smoking on lung cancer presented in Table 9.6. In the 
seven studies listed in Table 9.6, only the 95% confidence intervals are available. 
For three additional studies, the full 2 x 2 contingency tables were also available 
given in Table 9.7. The aim in this illustrative example is to obtain an overall 
estimate of the odds ratio using information available from all 10 studies of Tables 
9.6 and 9.7. 
Table 9.6 
Odds ratios of lung cancer for smokers versus nonsmokers for studies 1-7 
Study Odds ratio 
95% CI 
1 
3.89 
0.92 - 16.30 
2 
3.97 
2.20- 7.16 
3 
3.88 
2.47- 6.08 
4 
17.47 
14.24-21.43 
5 
5.35 
2.44 - 11.74 
6 
9.10 
5.57- 14.86 
7 
12.41 
2.94- 3.96 
Table 9.7 2 x 2 contingency tables of lung cancer and smoking for studies 8-10 
Study Odds Ratio 
Cases Controls 
8 
3.48 
Smokers 
49 
29958 
Nonsmokers 
33 
70186 
9 
33.10 
Smokers 
12 
0 
Nonsmokers 
89 
118 
10 
3.43 
Smokers 
29 
4 
Nonsmokers 
17 1 
81 
Bear in mind that the sample estimates of the log-odds ratios are asymptotically normal, 
we may use the normal distribution to model the available information of Table 9.6. Hence 
we may use the following simple hierarchical model 
logO^Rk - N ( Q k , $ )  
~k 
N 
~ ( 0 , a , 2 )  
forIc=1,2 ,..., 7, i = 1 , 2 ,  
where O^Rk are the estimated odds ratio for the kth study as provided in Table 9.7, while & is 
the standard error of the corresponding log O^Rk calculated by Bk = log(U/L)/(2 x 1.96), 
with U and L, respectively, denoting the upper and the lower limits of the 95% confidence 
interval of the odds ratio of Table 9.6. The first equation approximates the likelihood of 
each study since the original data of the corresponding 2 x 2 contingency tables are not 

320 
BAYESIAN HIERARCHICAL MODELS 
available. For the last three studies the full data are available, and therefore the following 
model can be used 
ek - N(e,a,2) for k = 8,9,10, i = 1,2, 
where Yijk refers to the number of observations in the kth study with smoking and cancer 
status i (l=smokers, 2=nonsmokers) and j (l=case, 2=control), respectively. Parameter 0 k 
is the corresponding odds ratio, while a is the odds of the disease for the nonsmoking group. 
Usual noninformative prior distributions can be used for a k ,  0, and a;. 
The code for the first part (seven studies) of the hierarchical model can be specified in 
WinBUGS using the following syntax 
while for the second part (studies 8-10> the model is specified using the following syntax: 
Finally, the prior distributions are defined as usual by 
while estimated odds ratios for each study using the preceding model are given by e 'k . The 
overall estimate of the odds ratio is given by e'. 
The posterior mean of the overall odds ratio is found equal to 5.28, with 95% of the 
posterior values ranging from 3.33 to 9.05. Error bars of the estimated odds ratios of each 
study using the hierarchical model presented above are depicted in Figure 9.6. 
Additional details concerning hierarchical models and meta-analysis can be found in 
Woodworth (2004, chap. 11). 
9.3 THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
A popular hierarchical model formulation is that for the so called generalized linear mixed 
models, which is based on the GLM formulation also having a hierarchical structure by 
including random coefficients/effects in the usual linear predictor. 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
321 
caterpillar plot OR 
I
.
 
0 0  
100 
20 0 
30 0 
Figure 9.6 
nonsmokers for each study in Example 9.4. 
95% posterior intervals of adjusted odds ratios of lung cancer for smokers versus 
Hence the model can be formulated as 
where Y is a random response vector that may include repeated measures of the same 
variable or measurements of correlated variables and X,Z 
are the data or design matrices 
for fixed and random effects ,B and b, respectively. 
The assumption of normal random effects [Eq. (9.4) in the formulation above] can be 
easily substituted by another distribution without any difficulty within Bayesian inference, 
resulting in a different marginal sampling distribution for Y assumed by the adopted model. 
The normal model is a special case of the preceding formulation for V = normal and 
8 = (p, C). Note that some structure must be imposed on C in order to avoid an over- 
parametrizedmodel. The simplest case is to set C = c21. Variance components in normal 
models are calculated using logic similar to that followed in Section 9.2.1.1. The total vari- 
ability is calculated via Var(Yij), while partial variances are provided by the corresponding 
random effects variances. Correlations within each level of hierarchy also provide useful 
decomposition and interpretation concerning the source of variability and the necessity of 
the corresponding random effects and levels of hierarchy. 
9.3.1 
A frequent implementation of hierarchical models is within the context of crossover trials, 
in which different treatments are given with different sequences in groups of patients. Here 
we illustrate a simple two treatment two-period crossover trial in which patients are divided 
into two groups. The first group receives treatment A for the first period and treatment B 
A hierarchical normal model: A simple crossover trial 

322 
BAYESIAN HIERARCHICAL MODELS 
for the second period of the study, while the other group receives the same treatments in 
the reverse order. Random effects are used to capture the correlation that results from the 
within-patient variability. An additional feature that arises from this crossover design is 
that the treatment implemented in the first period might possibly influence the effectiveness 
of the treatment implemented in the second period. This effect is referred to as a carryover 
effect. This effect is sometimes eliminated if a treatment-free interval is introduced between 
the two treatment periods (called the washoutperiod). The carryover effect can be estimated 
and tested using the interaction between the period and the treatment effect in hierarchical 
models. 
Example 9.5. An AB/BA crossover trial. The data in this example are results 
from a study presented by Brown and Prescott (2006, pp. 275-279) comparing two 
diuretics in the treatment of mild and moderate heart failure. Baseline observations 
were also taken just before the first treatment period. The duration of each treat- 
ment period was 5 days without any washout period. To avoid carryover effects, 
measurements of the first 2 days were ignored. Although the primary outcome 
measurement was micturition, we analyze two secondary response variables that 
were available: edema status and diastolic blood pressure (DBP). The first variable 
was calculated as the sum of left and right ankle diameters, while the second vari- 
able was the sum of three DBP readings (here we will consider the corresponding 
means). These response variables were measured before the first treatment period 
and after the end of each treatment period. In total, 94 patients participated in the 
study. For 20 patients only one measurement is available in this dataset. The aim 
here is to compare the effectiveness of the two treatments. Data ofthis example are 
available in the book’s Website and are reproduced with permission of John Wiley 
and Sons, Inc. 
Following Brown and Prescott (2006), four models were fitted for each response (edema 
and DBP) and compared using DIC. In all four models, the treatment and the period effect 
were included in the analysis as fixed effects. The patient effect was also used in all models 
as either fixed or random effect. The baseline measurement was also introduced in two 
of the models to assess their importance for the model. Finally, an additional model was 
introduced in which the interaction effect between the period and the treatment was included 
in the linear predictor in order to account for possible carryover effects. 
We can incorporate all models using the usual y indicators in the model formulation. 
Hence all four models can be described by the following expression 
Y, - N ( P 2 P 2 )  
I; 
“0: 
gpatients) 
~i 
= PI + PZ period, + PYTZ + y l a r d o m  + (1 - y ~ ) a y  
+ y2P& 
arandom - 
2 
afixed 
~ ( 0 , 1 0 - ~ ) ,  
for i = 1,2, . . . , N and k = 1,2, . . . , n, where N now denotes the total number of obser- 
vations available, n is the number of individualsipatients participating in the study, period 
and Ti are dummy variables indicating the period and the treatment of observation i (or indi- 
vidual Pi) (corner constraints and zero/one indicator variables are used here), and a Tdom 
and aExed are the random and fixed effects for individual k (k = 1,2, . . . , n). The variable 
Bi refers to the baseline measurement of Y for observation i(or individual Pi). Since we 
consider two different responses (edema and DBP) here, an additional indicator must be 
added for the response variable used. The model structure is defined using binary indicators 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
323 
y1 (O=fixed effects, l=random effects) and 7 2  (1 or 0 depending on whether the baseline 
measurement is included in the linear predictor). The usual noninformative priors can be 
used for the remaining parameters. The WinBUGS code for this model, which encompasses 
all four different formulations, is given in Table 9.8. 
Table 9.8 WinBUGS code for models with edema and DBP responses in Example 9.5 
Key: y = (0,O) - 
fixed patient effects and no baseline measurement; y = (1,O) - 
fixed patient effects and 
baseline measurement, y = (0, 1) -random patient effects and no baseline measurement, 7 = (1,l) -random 
patient effects and baseline measurement. 

324 
BAYESIAN HIERARCHICAL MODELS 
DIC values of all models are presented in Table 9.9. All DIC values were calculated 
using 50,000 iterations since instability was observed for chains of length equal to 10,000 
iterations (especially for hierarchical models). From this table we observe that the fourth 
model (with random effects and the baseline measurement as covariate) has the lower DIC 
values. According to DIC, improvement ofthe fit is higher for edema than the corresponding 
one for DBP. The log-normal distribution was additionally used for the fourth model of 
Table 9.9 in order to check for deviations from the normality assumption, resulting in 
considerably higher DIC values (1095.5 and 494.1 for edema and DBP, respectively) than 
the corresponding ones under the normal assumption. Finally, carryover effects were also 
checked by including the interaction terms between the period and the treatment. Posterior 
distributions of the corresponding carry over effects lie around zero while DIC values are 
close to the ones for models with no carryover effects, indicating that these effects can be 
omitted from the model. 
Table 9.9 Fitted models and DIC values for Example 9.5 
DIC 
Model" 
y 
Patient effect Baseline Edema 
DBP 
1 
(0,O) 
Fixed 
No 
1094.1 
471.3 
2 
( 1 , O )  
Random 
No 
1084.8 
470.6 
3 
(0, I) 
Fixed 
Yes 
1093.9 
471.2 
4 
(1,l) 
Random 
Yes 
1065.4 
462.4 
all models the treatment and the period effect were included as fixed effects. 
Posterior summaries of the finally selected model are presented in Table 9.10. For DBP, 
both the period and the treatment effects are important since zero lies at the tails of the 
corresponding posterior distributions. On the other hand, the period and the treatment 
effects on edema are minor since their posterior distributions lie around the zero value. For 
both models the baseline measurement is important for determination of the corresponding 
values in each stage of the study. 
Table 9.10 Posterior summaries of parameters of model 4 in Table 9.9 for Example 9.5 
Edema 
Diastolic blood pressure 
Posterior 
Posterior 
Posterior 
percentiles 
Posterior 
percentiles 
Mean Median 
SD 2.5% 97.5% 
Mean Median 
SD 2.5% 97.5% 
Constant 
P1 
24.57 
24.59 5.29 
14.2 34.97 
-0.25 
-0.25 
1.51 
-3.19 
2.75 
Period 
P2 
-1.03 
-1.03 
0.77 
-2.54 
0.48 
-0.24 
-0.24 0.12 
-0.47 0.0016 
Baseline 
P4 
0.69 
0.69 0.06 
0.58 
0.81 
0.99 
0.99 0.03 
0.93 
1.04 
Variance components 
Treatment 
P3 
-1.01 
-1.01 0.76 
-2.53 
0.47 
-0.31 
-0.31 0.12 
-0.54 
-0.07 
Within-patient 2 
0.543 
0.532 0.093 0.391 
0.754 
23.12 
22.43 4.88 
16.38 33.43 
Between-patient 
3.833 
3.773 0.625 2.787 
5.229 25.30 
25.12 6.80 
13.36 39.11 
Within-patient 
~ 1 2  
0.874 
0.876 0.027 0.814 0.918 0.523 
0.529 0.085 0.341 
0.673 
Correlation 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
325 
Concerning the variance components, for both response variables the between-patient 
variability is large in comparison to the remaining within-patient variability. To be more 
specific for edema, the between-patient variability is about 7 times the within-patient vari- 
ability when comparing their posterior means, while for DBP, the two variance components 
are about the same. The within-subject correlation ~
1
2
 
[see Eq. (9.1)] ranges from 0.81 
to 0.92 with 95% posterior probability for edema measurements, while for DBP the cor- 
responding correlation is around 0.53, ranging from 0.34 to 0.67 with the same posterior 
probability. The large within-subject correlation indicates that using the crossover design is 
beneficial and allows us to successfully quantify and remove the corresponding variability. 
9.3.2 Logit GLMM for correlated binary responses 
When a series of repeated binary responses Yit fort = 1 2, . . . , T over the same individuals 
i = 1,2, . . . n are considered, the following model can be adopted 
Yit - Bernoulli(7rit) 
where Pot are fixed constants depending on time sequence t, X i j t  are time-varying covariates 
for individual i and time sequence j ,  P j t  are their corresponding coefficients, and hi are 
individual random effects capturing within-patient variability. The structure of the model 
formulated above can be slightly modified depending on the data and the problem at hand; 
for example, covariates or their effects may not depend on the time index t. 
A much simpler version of this model can be considered if no covariates are used. In 
such a case, the linear predictor can be simplified to 
log (5) 
= bi + Pt. 
1 - Tit 
(9.6) 
This model is frequently used in psychometrics and social sciences to model correlated 
questionnaire responses (items). The random effect b i depicts the individual’s ability to 
reply positively to each question, while fit reflects the frequency rate of a positive reply to 
the question t (and therefore easiness of the question in some cases). This type of model 
was initially introduced by Rasch (1961) and is referred in the relevant literature as Rasch 
or item response models (Agresti, 2002, p. 495). The simplest case of this model is the one 
for data with two binary measurements (T = 2). In such a case, the model is the analog 
of the one used by Cox (1958) to derive the conditional likelihood estimate of odds ratio 
for two correlated binary responses. Other link functions such as the probit or the clog-log 
can be alternatively used without any problem. 
Decomposition of the variance components is more complicated for models with bino- 
mial responses. One simple approach is to adopt the latent variable approach and express 
everything in terms of the variability of this latent variable. Hence, for the logit model, we 
assume that a latent variable Zi exists following the logistic distribution with mean pi and 
dispersion parameter equal to one. The model is summarized as follows: 

326 
BAYESIAN HIERARCHICAL MODELS 
P 
Pit 
Yit 
= Pot + X P j t X i j t  + bi 
= 
j=1 
1 if Zit > 0 and Yit = 0 otherwise. 
In this formulation, the variance of the latent variable given the random effects is equal to 
Var(Zit (pit) = Var(Zit lbi) = -ir2/3 from the logistic regression assumption. Moreover, 
the total variability of the latent variable is given by 
resulting in within-subject correlation of the latent measurement equal to 
5 2  
52 + -ir2/3 ' 
T, = 
Similar logic can be used for the rest ofthe link function. For example, for the probit model 
the latent variable is assumed to be a standardized normal distribution with Var(Zit I bi) = 1. 
Usually, we wish to express the variability in terms of the original response variable. For 
this reason alternative approaches have been proposed in the literature using either first- 
order Taylor-based approximations (model linearization approach) or simulation (Goldstein 
et al., 2002; Browne et al., 2005). Here we do not pursue this issue further but use a much 
simpler approach to estimate the changes in the variability of the original measurement by 
considering a simple R2 type measure and the corresponding modification of the measure 
with the addition of hierarchical levels to model structure. Here we define this measure as 
REin = 1 - o,",,/S$, where S$ denotes the sample variance of the response Y and a:ees 
denotes the variance measure of the unstandardized residuals, given by 
where Nit are the number of Bernoulli replications for the i,t combination and 
N = C,"=, Nit is the total number of Bernoulli observations over all available 
data. When binary data are used (as in the example here), then Nit = 1 for all i ,  t. 
In the following paragraphs, we present a simple example in a matched-pair clinical 
trial using two binary responses (Example 9.6) and a social sciences example using three 
correlated questionnaire items with gender as a covariate (Example 9.7). 
9.3.2.1 The logit model in 2 x 2 tables of dependent binary data. In this 
section we present a simple hierarchical model used when a simple 2 x 2 contingency table 
arises from dependent binary data. In the illustrative example that follows we compare two 
treatments given in a matched-pair clinical trial. 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
327 
Example 9.6. Treatment comparison in matched-pair clinical trials. Let us 
consider a matched-pair clinical trial in which 60 pairs of patients were considered. 
Each patient in the first treatment group was matched to one individual in the other 
treatment group according to age, gender, and severity of the disease. After 2 weeks 
of treatment, the presence or absence of the disease was measured and denoted by 
Y,1 and Yi2 for patients receiving treatments A and B, respectively. Table 9.1 1 
summarizes the data. Each entry of this table represents observed frequencies for 
each combination of the responses (Yi1, K2). 
The aim here is to estimate the efficiency of the new treatment (B) in comparison to 
the standard treatment (A). Traditional Poisson log-linear models or logit models 
presented in Chapter 7 cannot be used since measurements for each pair ofpatients 
are correlated and this must be accounted for in the model structure. 
Table 9.11 
Data of Example 9.6 
Treatment B 
Treatment A Not cured Cured 
Not cured 
7 
14 
Cured 
2 
37 
Here we the simplified version of model (9.5) using the linear predictor of the type (9.6) 
with T = 2 binary responses. Note that instead of using Pt parameters for each individual 
in the matched pair, we use the parameterization 
and PI+ /32 for each individual in order 
to directly estimate the log-odds ratio using ,& parameter. 
In order to import the aggregated data of Table 9.1 1 in WinBUGS , we have used vectors 
f = (n11,7~21,n12,n22)~ 
= (7,2,14,37)T 
Y1 = (0,1,0, 
Y2 
= (0,011, 
where nij are the frequencies of the contingency table (Table 9.1 1) and Y t ,  t = 1,2, are the 
values of the binary variables in the instances (here, each individual in the same matched 
pair) that correspond to each element of of frequency vector f. 
In WinBUGS we need to reconstruct individual data. One problem that arises is that the 
range of loop indices cannot be defined within WinBUGS code. Thus we construct a vector 
F with elements of the cumulative counts of vector f. Hence each element of F will be 
given by 
2 
F z = C f  k-1 for i = 1,2,3,4,5 
k=O 
with fo set equal to zero. In this specific case 
F = (0,7,9,23,60)*. 
Then we need to specify individual data yil and yi2 using the following loop: 
Yit=Ytk for t = 1 , 2 ;  i = F k + l ,  ..., Fk+l; k = l , 2 , 3 , 4 .  

328 
BAYESIAN HIERARCHICAL MODELS 
The model definition is completed using the following expressions 
Y i t  - Bernoulli(.irit) 
for t = 1,2 and i = 1, . . . , R, where R is the total number of matched pairs in the study. 
We further need to define random effects (i.e., the mixing distribution) and the prior dis- 
tributions. For the random effects we use anormal mixing distribution with b i - N(0, T - ' ) ,  
while the usual low-information prior distributions are used for the model parameters ,O 1, 
pz, and 7 :  ,Ot - N(0,lOOO) for t = 1 , 2  and 7 - gamma(10-4, 
Finally, we use 
logical nodes to define the odds ratio as OR = eD2 and the standard deviation of the random 
effects as 
= m. 
The full WinBUGS code and the data specification are provided in 
Table 9.12. 
Table 9.12 
matched-pair clinical trial data of Example 9.6 
WinBUGS code for modeling the two correlated binary responses in 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
329 
Posterior summaries of the estimated model parameters, using a sample of 3000 iterations 
and 1000 as burnin period are provided in Table 9.13. The log-odds ratio (p 2) is equal to 
-2.0 (mean equal to 2.1, median equal to 2.0, and rough estimate of the posterior mode 
equal to 1.87), which is very close to the sample estimate log (9) = 1.95. Since the 
posterior distribution of the odds ratio is highly skewed, we can use the posterior geometric 
mean, the median as point estimates, or the mode as point estimates giving values equal to 
e2.’ = 8.17,7.46 and 
= 6.49, respectively. The corresponding 95% posterior interval 
ranges from 1.95 to 54.04, indicating better performance of the second treatment, whereas 
using the posterior median, the second treatment is at least 6.5 times more effective than 
the first one with probability 0.5. 
Table 9.13 
responses in matched-pair clinical trial data of Example 9.6 
Posterior summaries for parameters of model used for the two correlated binary 
node 
mean 
sd 
MC error 2.5% median 97.5% start sample 
betar11 
1.25 0.65 0.043 
0.16 1.17 
2.67 
1001 3000 
beta[2] 
2.10 0.85 0.062 
0.67 2.00 
3.99 
1001 3000 
or 
12.77 21.77 1.260 
1.95 7.46 
54.04 
1001 3000 
sigma 
2.66 
1.16 0.115 
0.81 2.53 
5.48 1001 3000 
r 
0.62 0.19 0.021 
0.17 0.66 
0.90 
1001 3000 
R2bin 
0.45 0.13 0.012 
0.17 0.46 
0.66 1001 3000 
The posterior mean of the matched-pairrandom effect o is equal to 2.66 . Using the latent 
variance approach, the within-patient correlation of the latent “sickness” variable is equal 
to 0.62, indicating an important correlation between-subject that cannot be ignored. The 
R2-type statistic has posterior mean equal to 45%, which suggests a large improvement over 
the corresponding model with no random effects (posterior mean R ‘-type statistic equal to 
3.8%). 
Example 9.7. 
Modeling correlation in questionnaire binary responses: 
Schizotypal personality questionnaire. According to the DSM-111-R diagnostic 
and statistical manual of mental disorders edited by the American Psychiatric Asso- 
ciation (1 987), a “schizotypal” person suffers from minor episodes of “pseudoneu- 
rotic” problems. The prevalence rate of schizotypy is about 10% in the general pop- 
ulation. Study of the schizotypal personality in psychiatric research is of prominent 
importance for two reasons: (1) shizotypal subjects have increased risk of devel- 
oping schizophrenia during their lifetimes, and (2) since they are healthy persons, 
they can participate in psychiatric/psychological research studies (by completing 
questionnaires - 
psychometric instruments), which schizophrenics are unable to 
do. Although several scales have been proposed in the literature, Raine’s (1991) 
SPQ scale, a 74-item self-administered questionnaire, is the most popular question- 
naire used to measure the concepts of schizotypal personality. The questionnaire 
consists of binary zeroione (yesino) items. It provides subscales for nine schizoty- 
pal features as well as an overall scale for schizotypy. Each subscale is calculated 
as the sum of the questionnaire items that refer to each schizotypal subscale. 
Here we analyze the original binary responses of the SPQ scale administered within 
a small student survey completed in Greece examining the association between 
schizotypal traits and impulsive and compulsive buying behavior of university stu- 
dents (Iliopoulou, 2004). A total number of 167 students were finally considered 
here, which is subset of the original full dataset. Here we use the logit mixed model 
to quantify the within-subject variability. In a second stage, we use information on 
subscales to improve the model and estimate partial estimates of variability. 

330 
BAYESIAN HIERARCHICAL MODELS 
We adopt model (9.5) with linear predictor (9.6) with random individual effects b i and 
fixed item coefficients pj. We estimate within-subject variability and correlation using the 
simple latent variable interpretation. The WinBUGS code for this model is provided in 
Table 9.14. 
Table 9.14 
Example 9.7 
WinBUGS code for simple individual random effects model for SPQ data of 
According to our results, the within-subject correlation of the latent variable is equal 
to 0.19 . Nevertheless, the DIC value of the adopted model is found equal to 12,343.3, 
which is lower than the corresponding models with individual fixed or any individual effects 
(12,355.400and 13,553.9, respectively), indicating that nonzero correlationwithin-subject 
measurements exist. 
A similar analysis for the examination of random item effects lead to a nondecreasing 
DIC, indicating a minor correlation between individuals as expected. Adding individual 
effects increases the R2-type measure to 0.24 from 0.141, which was the corresponding 
value for the model with no individual effects. 
We can further introduce random effects clustering due to the different subscales which 
measure different characteristics of the schizotypal personality. These characteristics are: 
ideas of reference (9), odd beliefs (7), unusual perceptual experiences (9), odd speech (9), 
suspiciousness (8), constricted affect (8), odd behavior (7), no close friends (9), and social 
anxiety (8); the number of items of each corresponding subscale is denoted within paren- 
theses. The inclusion of each item within one of the these subscales is coded with values 
ranging from 1 to 9, respectively. The linear predictor of the model now accommodates 
individual random effects wis,, where sj E { 1,2, . . . ,9} denotes the subscale to which item 
j belongs. Hence the linear predictor (9.6) is now formulated as 
with 
2 
vis - N(0, c
~
~
~
~
~
~
~
,
~
)
 
for i = 1,. . . ,n = 167; s = 1,2,. . . , 9 .  

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
331 
This model assumes (under the simple latent variable interpretation) that within-subject 
variability for items within the same subscales is given by 
and within-subject variability for items within different subscales is given by 
0 2  
, where sJ # S ~ J .  
2
2
 
Cor(Z,, , ZZf) = 1/. + uscales,s, + r 2 ~ 3 + 2  + u L e s , s , ,  + r2/3 
The latter is formulated in WinBUGS by considering a matrix with ellements r k l  ,kz for 
k-1, k-2 E { 1,2, . . . ,9} using the formula 
ff2 + O:cales,kll(kl 
= k2) 
Ju2 + O:cales,kl + ?r2/32/02 + Cr&ales,kz + ? r 2 / 3  
r k i , k Z  = 
and the corresponding WinBUGS syntax 
where t o t .  s2. scales [k] is the total variability for each latent variable Zij. The full 
code for this model is provided in Table 9.15. 
DIC (11,356.2) is much lower for the model with clustering random effects of items 
because of the underlying subscales. Moreover, the posterior mean of the R 2-type statistic 
is equal to 0.36, indicating a decrease in residual variability of about 50% and 155% for 
the models with or without individual random effects, respectively. Posterior means of 
within-subject correlation is presented in Table 9.16. Within-subject correlations for items 
of different subscales range from 0.13 to 0.19, while the corresponding correlation of items 
in the same subscale range from 0.33 to 0.54. Items of odd behavior and social anxiety 
demonstrate the highest within-subject correlations with posterior means equal to 0.54 and 
0.52, respectively, followed by the corresponding correlations for items of odd beliefs and 
constricted affect with values around 0.42. Results of this model indicate that within-subject 
correlation of the items belonging in the same subscale changes from item to item and that 
subscales explain an important percent of the total SPQ variability. 

332 
BAYESIAN HIERARCHICAL MODELS 
Table 9.15 
SPQ data of Example 9.7 
WinBUGS code for random effects model, including intrasubscale variability for 
Table 9.16 
including intrasubscale variability for SPQ data of Example 9.7 
Posterior means of within-subject correlations for random effects model, 
1 Ideas of reference 
2 Odd beliefs 
3 Perceptual experiences 
4 Odd speech 
5 Suspiciousness 
6 Constricted affect 
7 Odd behavior 
8 No close friends 
9 Social anxiety 
1 
2 
3 
4
5
 6 
7 
8
9
 
Ideas of Odd Perceptual 
Odd Suspic? Constricted 
Odd 
No close Social 
reference beliefs experiences speech 
affect 
behavior friends anxiety 
0.359 
0.175 
0.426 
0.188 
0.178 
0.341 
0.188 
0.178 
0.191 
0.338 
0.181 
0.171 
0.184 
0.184 
0.389 
0.177 
0.167 
0.179 
0.179 0.172 
0.417 
0.157 
0.148 
0.159 
0.159 0.153 
0.149 
0.540 
0.185 
0.175 
0.188 
0.188 0.181 
0.176 
0.157 
0.359 
0.161 
0.152 
0.163 
0.163 0.157 
0.153 
0.136 
0.161 
0.518 
a Suspicoiousness. 
Key: Diagonal correlations ( r k k )  = within-subject correlations for items of same subscale k; l)clkz 
with kl # ka = within-subject correlations for items of the different subscales /Q and k2. 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
333 
9.3.3 Poisson log-linear GLMMs for correlated count data 
In this section we consider repeated count/discrete responses Yit for t = 1,2, . . . , T over 
the same individuals (or more generally conditions) i = 1,2, . . . n. As we have already 
mentioned in Section 9.2.3.2, random effects can be simply used to model overdispersion. 
Hence when we simply use the model 
Kt - Poisson(Xit) 
logkt = pit + Uzt 
P 
pit 
= Pot + C P j t X i j t  
(9.7) 
j=1 
Uit 
N ( 0 , n 2 ) ,  
where pit is the part of the linear predictor that involved only the usual fixed effects. This 
model assumes variance 
V(%) = Vu [E(Ktluit)] + E, [ V ( K t l ~ i t ) ]  
- 
- epzt { epzt v(euZt + E(euit I}, 
which simplifies to 
v(Kt) = epzt { eptt (eo2 - 1)eg2 + eo'iz} 
(9.8) 
when the usual log-normal random effects are used. The model is overdispersed since 
E(Kt) = ePitE(e"") = e f i * t e c 2 / 2  
with dispersion index 
DI&) = 1 + epztDI(eZLat) 
= 1 + ep"(eu2 - l)ea2/2 . 
(9.9) 
Although the model introduces overdispersion over the response variable, it assumes 
zero correlation between consequent measurements for individual observations i. For this 
reason, we may rewrite the linear predictor of the model described above as 
(9.10) 
(9.1 1) 
where bi are the individual random effects of observations Yit for t = 1,2, . . . , T. This 
model still induces overdispersion, resulting in the same variance and dispersion index as 
above [see Eqs. (9.8) and (9.9), respectively], but at the same time it introduces correlation 
between measurements Yzt, and Yzt, of the same individuaVobservation i since 

334 
BAYESIAN HIERARCHICAL MODELS 
resulting in 
1 
X 
el-1112 { eptta (ern2 - 1)eUz + e u ~ / 2 }  
= { (1 + we-p’tl) (1 + ue-p*tz)}-”’ 
with w = (eu2 - 
e p u 2 / 2 .  
From this expression, the within-subject correlation depends on the fixed linear predictor 
1-1,~ and therefore on the covariate values. Indicative values will be reported on the basis of 
sample means, sample minimum and maximum values of the observed covariates. 
Additional random effects can be added depending on the problem at hand. Although 
variance can be decomposed using arguments similar to those used above, more complicated 
hierarchical structure introduced in the model results in more complicated computations 
concerning the variance components. 
We now present a simple example with data from water polo and the corresponding 
scores from the World Cup of year 2000. We use the hierarchical formulations above to 
model overdispersion and correlation between the two competitors. 
Example 9.8. Modeling water polo World Cup 2000 data. Let us consider 
results of the water polo tournament held at the city of Fukuoka, Japan during July 
2001 for the “9th World Swimming Championships.” A total of 16 national water 
polo teams competed with each other. Initially four round-robin groups with four 
teams in each group were formed. The best two teams from each group qualified 
for a second round robin, and best two teams were qualified for the semifinals, and 
so on. The score of each team refers to the number of goals scored by each team 
within a game. Poisson seems to be a realistic assumption for such data. Usual 
scores are around eight goals for each team, with a strong correlation between the 
scores of the competing teams. Here we consider the following formulation: 
Here teamli and team2i indicate the two teams competing each other, the usual 
sum-to-zero constraints were imposed on both attacking and defensive parameters 
ak and d k ,  and parameters 71 and 7 2  are only binary indicators used to identify 
which model is fitted each time. Hence we consider four models: the fixed effects 
model with (rl,-yz) = (0, O), the overdispersed Poisson model with no correlation 
with ( 7 1 , ~ ~ )  
= (1,0), the overdispersed correlated Poisson model (71,72) = 
(0, l), and model with both game and individual random effects ( 7 1 , ~ ~ )  
= (1,l). 
The DIC results and R2-type statistics based on the residuals of each model are presented 
in Table 9.17. The latter was calculated using the following equation within each MCMC 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
335 
Table 9.17 
Comparison of models for water polo data of Example 9.8 
1 Fixed effects (0,O) 
590.3 
0.291 
2 Individual RE (1,O) 
591.0 
0.322 
3 Game RE (0,l) 
587.2 
0.376 
4 Game + individual RE (1,l) 587.9 
0.385 
Key: KE = random effects; E(R&,ly) = posterior mean 
iteration: 
RCois = 1 - 
n
2
 
i = l  t = l  
c c (Yit - & t ) 2  
5 i: (Yit - Y)2 ' 
i = l  t=l 
Thus we compare each model with the constant one. The third model, with game random 
effects, is indicated as the best-fitted model according to DIC. This implies that the goals 
scored in a game by the two competing teams demonstrate a nonzero correlation. The 
WinBUGS code for model with game specific random effects is given in Table 9.18. 
Table 9.18 
Example 9.8 
WinBUGS code for game-specific random effects model for water polo data of 

336 
BAYESIAN HIERARCHICAL MODELS 
The posterior mean of the constant is equal to 1.84, which corresponds to about 6.3 
expected goals when two teams of average attacking and defensive abilities compete with 
each other. Note that positive attacking and negative defensive parameters indicate teams 
with attacking and defensive skills better than those of an “average”-level team in the tour- 
nament (denoted by the zero value). Estimates of the attacking and defensive parameters 
are presented in Figures 9.7 and 9.8. From these figures we observe that Spain (the contest 
winner) had the best overall defensive parameter but only the fifth overall best attacking 
parameter. Russia (third in the competition) had the best attacking parameter but its defen- 
sive parameter is slightly worse than the average defensive level. Similarly, Italy (fourth 
in the competition) had the second best defensive parameter, but its attacking parameter 
was average. Yugoslavia (second in the league) had the fourth and the third best attacking 
and defensive parameters, respectively. We can compare the performance of the remaining 
teams in the same manner. Estimation of the full league under the fitted model can be per- 
formed using the predictive distribution; for mored details, see Section 7.4.3.5 and Chapter 
10. 
To understand the effect of the random coefficients on the imposed variance and corre- 
lation of the water polo data, we have calculated all individual correlations and dispersion 
indices using the following WinBUGS syntax: 
Finally, we have calculated the minimum, median, mean, maximum, and the quartiles 
over all games of the imposed dispersion index and the within-game correlations for each 
iteration. Their posterior summaries are presented in Table 9.19. The correlation averaged 
around 0.14, the maximum value is found equal to 0.20, while the variance is overdispersed 
by 19% on average or by 46% at the maximum value of the sample. 
Table 9.19 
for water polo data of Example 9.8 
Posterior summaries of selected within-game correlations and dispersion indices 
Posterior summaries 
Correlation 
Dispersion index 
Percentiles 
Percentiles 
Sample statistic 
Mean 2.5% 97.5% 
Mean 2.5% 
97.5% 
~~ 
~~ 
Minimum 
0.078 0.003 
0.217 
1.051 1.002 
1.171 
2.5%percentile 
0.116 0.005 
0.307 
1.132 1.005 
1.413 
Mean 
0.136 0.006 0.353 
1.188 1.007 
1.590 
Median 
0.136 0.006 0.353 
1.179 1.006 
1.559 
97.5% percentile 0.154 0.007 0.395 
1.23 1 1.008 
1.73 1 
Maximum 
0.198 0.010 0.486 
1.458 1.015 
2.528 
Note: Sample statistics related to the corresponding values of correlation 
and variance over all calculated values within each MCMC iteration. 

THE GENERALIZED LINEAR MIXED MODEL FORMULATION 
337 
8 
~ 
- 
caterrnllar plot an 
I 
#AUSTRALIA (10) 
1:iCANADA (15) 
I CiJAPAN ($6) 
it1 GERMANY (141 
11 BRAZIL (131 
IP'IKAZAKHSTAN (12) 
1' ITALY (4, 
II:INETHERLANDS (01 
- 
3 USA(7) 
.&LOVAKIA (1 11 
.'I GREECE (6) 
- 
Jbl YUGOSLAVIA 12) 
101 HUNGARY ( 5 )  
- 
131 CROATIA (81 
151 SPAIN (1 1 
''$RUSSIA (31 
. SPAIN (1) 
i lKALY(0 
. 
I
,
 
-1.0 
-0.5 
0.0 
0.5 
1 .o 
I~BIYUGOSLAVIA 
(2) 
1 5  AUSTRALIA (10) 
nHUNGARY (5) 
pICROATIA (8) 
IGREECE (6) 
'INETHERLANDS (8) 
I 8.riRUSSIA (3) 
ii~iUSA(7) 
.iBRAZIL (13) 
IwSLOVAKIA (11) 
Figure 9.7 
95% posterior intervals of attacking parameters for water polo data of Example 9.8 
catcrpikr pld: def 
Figure 9.8 
95% posterior intervals of defensive parameters for the water polo data of Example 9.8 

338 
BAYESIAN HIERARCHICAL MODELS 
9.4 DISCUSSION, CLOSING REMARKS, AND FURTHER READING 
In this chapter we have briefly introduced the reader to the basic notions of hierarchical 
modeling. The term hierarchical models refers more to a general set of modeling principles 
than to a specific family of models. The basic idea of hierarchical modeling (also known 
as multilevel, repeated measures, mixed, or longitudinal models) is to organize the model 
using a set of sequential statements of conditional relationships. The levels of these models 
are usually specified by the existence of several nested units of observations such as, for 
example, patients, hospitals, and geographic areas of the hospitals and countries. Each unit 
in this set may contribute effectively in the total observed variability and can be modeled by 
introducing a set of random effects coming from a general population distribution. Bayesian 
theory and hierarchical modeling are closely related since even simple Bayesian models 
embed the idea of hierarchy because of the conditional structure of the posterior distribution, 
which is decomposed to the data likelihood multiplied by the prior distribution. 
In this chapter we focused on simple hierarchical models that mainly included random 
intercept models. The use of hierarchical models (or multilevel, repeated measures, mixed, 
or longitudinal models) is valuable in statistics since they are based on the logic of simple 
generalized linear models by including random terms in the linear predictor that can be 
used to introduce dependence or overdispersion or to simply change the resulted marginal 
sampling distribution of the data (i.e., the likelihood). They are also used to combine 
information between different observations or studies or introduce a clustering effect on 
observations within the data. 
Details concerning classical hierarchical models can be found in Agresti (2002, chap. 
11) along with a very nice review at the end of the same chapter. Interesting books on the 
subject include the more recent books by Hedeker and Gibbons (2006), Brown and Prescott 
(2006), and de Leeuw and Meijer (2008), while an interesting selection of papers on the 
topic can be found in the related special issue of the Journal ofEducationaland Behavioral 
Statistics (vol. 20, issue no. 2, 1995), including the interesting and critical review by 
Professor D. Draper (Draper, 1995). 
Concerning Bayesian hierarchical models, their basic notions have been introduced by 
Lindley and Smith (1 972) within the framework ofthe normal linear model. Computational 
developments in the 1990s and MCMC algorithms have made Bayesian hierarchical models 
very popular; see Zeger and Karim (1 99 1) for implementation of the Gibbs sampler in 
hierarchical models. Bayesian generalized linear mixed, hierarchical longitudinal, and 
nonlinear hierarchical models are presented by Clayton (1996), Carlin (1996), and Bennet 
et al. (1996) in Gilks et al. (1996). A nice and comprehensive review can be found in 
Seltzer et al. (1996). Details on hierarchical modeling are also provided by Gelman et al. 
(2004, chaps. 5, 13) and Carlin and Louis (2000, sec. 7.4). Extensive treatment of 
the subject is also presented by several authors in Dey et al. (2000), while more recently 
Gelman and Hill (2006) have published a very nice book on the topic. Special topics within 
the framework of hierarchical models have been examined by Professor Gelman and his 
associates, including the specification of the prior distribution for variance components 
(Gelman, 2006), measures for variance components models (Gelman and Pardoe, 2006), 
and the use of redundant parametrizations (Gelman et al., 2008). 
In the wide class ofthe hierarchical model, we can also include the latent variables models 
[see Bartholomew and Knott (1 999) for a general treatment of the topic and Dunson (2000), 
Dunson and Herring (2005) for details concerning the Bayesian implementation], errors 
in variable models [for a nice review of the Bayesian implementation, see Wakefield and 

DISCUSSION, CLOSING REMARKS, AND FURTHER READING 
339 
Stephens (2000)], and models for time series data where the parameters change dynamically 
[see Aguilar et al. (1999) for a nice review and West and Harrison (1997) for more details]. 
Finally, additional examples of hierarchical models implemented in WinBUGS can be 
found in the three volumes of WinBUGS examples: 
1. In the first volume of examples (Spiegelhalter et al., 2003~) the following datasets 
are available 
0 rats (normal hierarchical model) 
0 pump (gamma-Poisson hierarchical model) 
0 seeds (random effects logistic regression) 
0 surgical, used for institutional ranking (the second model) 
0 salm, used to model extra-Poisson variation in a dose-response study 
0 equiv, used to model bioequivalence in a crossover trial data 
0 dyes (variance components model) 
0 epil (repeated measures on Poisson counts) 
0 blocker (random effects meta-analysis of clinical trials) 
0 Oxford (smooth fit to log-odds ratios in case control studies) 
0 LSAT (latent variable models for item-response data) 
0 inhalers (random effects model for ordinal responses from a crossover trial) 
0 kidney (Weibull regression with random effects) 
0 leuk (survival analysis using Cox regression with frailties) 
2. In the second volume (Spiegelhalter et al., 2003b), the following examples refer to 
hierarchical models: 
0 orange trees (a hierarchical, nonlinear model) 
0 air (covariate measurement error) 
0 cervix (case<ontrol study with errors in covariates) 
0 jaw (repeated measures analysis of variance) 
0 birats (a bivariate normal hierarchical model) 
0 schools (multivariate hierarchical model of examination results) 
3. The example hepatitis (random effects model with measurement error) can be 
found in the third volume of examples (Spiegelhalter et al., 2003~). 
Further examples of Bayesian hierarchical models for repeated ordinal data (implemented 
in WinBUGS) are also provided by Qiu et al. (2002). Lawson et al. (2003) present the 
basic notions of Bayesian hierarchical models and multilevel modeling (see chaps. 2 and 
3 respectively), while implementation using WinBUGS and MLWin is also illustrated in 
detail. WinBUGS code for the fitting hierarchical models for meta-analysis can be found 
in Woodworth (2004, chap. 11). Numerous examples are also provided in the Congdon’s 
books on Bayesian models Congdon (2003; 2005~; 
2006b). Finally, Lynch (2007, chap. 
9) provides examples of WinBUGS hierarchical models, including random coefficient and 
growth curve models. 

340 
BAYESIAN HIERARCHICAL MODELS 
Problems 
9.1 
9.2 
9.3 
9.4 
9.5 
9.6 
Use a Poisson log-linear model with random effects 
a) For the water polo data of Problem 7.3. 
b) For the Italian soccer/football data of Problem 7.4. 
Use a hierarchical model for the data of the WinBUGS example salm (Spiegelhalter 
et al., 2003~) 
used in Problem 7.5. 
a) Compare the results with the corresponding ones from a model without random 
b) What do we achieve here by using random effects? 
Use a hierarchical model for the data of the WinBUGS example seeds (Spiegelhalter 
et al., 2003~) used in Example 7.6. 
a) Compare the results with the corresponding ones from a model without the random 
b) What do we achieve here by adopting random effects? 
Use a hierarchical model for the data of the WinBUGS example rats (Spiegelhalter 
et al., 2003~). 
a) What do we achieve here by using random effects? 
b) Calculate the within-rat correlation. 
Use a hierarchical model for the data of the WinBUGS example surgical (Spiegel- 
halter et al., 2003~). 
a) Compare the random and the fixed effects models. 
b) What do we achieve here by using the random effects? 
Use the data of the WinBUGS example kidney to construct a hierarchical random 
effects model for the survival times. Estimate the within-subject variability and 
correlation in this case. 
effects. 
effects. 

CHAPTER 10 
THE PREDICTIVE DISTRIBUTION AND 
MODEL CHECKING 
10.1 INTRODUCTION 
10.1.1 Prediction within Bayesian framework 
In Bayesian theory, predictions of future observables are based on predictive distributions, 
that is, the distribution of the data averaged over all possible parameter values. For this 
reason, when data y have not been observed yet, predictions are based on the marginal 
likelihood 
f(Y) = J’f(Yl@)f(@)d@> 
(10.1) 
f(Y’lY) = / f(Y’l@)f(@lY)d@, 
(10.2) 
which is the likelihood averaged over all parameter values supported by our prior beliefs. 
Hence, f (y) is also called prior predictive distribution. 
Usually, after having observed data y, one finds the prediction of future data y ’ more 
interesting. Following this logic, we calculate the posterior predictive distribution 
which is the likelihood of the future data averaged over the posterior distribution f(0ly). 
This distribution is termed as the predictive distribution since prediction is usually attempted 
only after observation of a set of data y. 
Future observations y’ can be alternatively viewed as additional parameters under esti- 
mation. From this perspective, the joint posterior distribution is now given by f ( y  ’, @ly). 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
341 

342 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Inference on the future observations y‘ can be based on the marginal posterior distribu- 
tion f(y’ly) by integrating out all nuisance parameters, one of which in this case, is the 
parameter vector 8. Hence, the predictive distribution is given by 
resulting in (10.2) since past and future observables (yand y ’, respectively) are conditionally 
independent given the parameter vector 8. 
According to Press (1989, pp. 57-58), inference must be based on predictive distributions 
since they involve observables whilc the posterior distribution also involves parameters 
that are never observed. Hence, by using the predictive distribution, we can quantify our 
knowledge about future as well as measure the probability of again observing in the future 
each yz assuming that the adopted model is true. For this reason, we may use the predictive 
distribution not only to predict future observations but also to construct goodness-of-fit 
diagnostics and perform model checks for each model’s structural assumptions. 
10.1.2 Using posterior predictive densities for model evaluation and 
checking 
The posterior predictive density f(y’ly, m) of a model m is frequently used for checking 
the assumptions of a model and its goodness-of-fit. The main reason is that we can eas- 
ily generate replicated values yrep from the posterior predictive distribution by adding a 
single simple step within any MCMC sampler using the likelihood function f (yrePldt)) 
evaluated at parameter values 8(t) of the current state of the algorithm; see Section 10.2. 
The predictive data y r ” P  reflect the expected observations after replicating our experiment 
in the future, having already observed y and assuming that the adopted model is true. If the 
adopted model is appropriate for describing the observed data, then vectors y and y rep will 
be close. Hence, a comparison of these two vectors provides information concerning the 
fit of the model and possible outliers. Such a comparison can be facilitated by considering 
summary functions D(y, O ) ,  which play the role of a test statistic for checking the assump- 
tion under investigation and measure discrepancies between the data and the model (Gelman 
et al., 1996). Assessment of the posterior distributions of D(y 
8) and D(y, 8) provides 
individual as well as overall goodness-of-fit measures that can be summarized graphically 
or using tail area probabilities called posteriorpredictivep-values (Meng, 1994) given by 
Posterior p-values are posterior probabilities and can be directly interpreted as the probabil- 
ity of observing in the future samples with D(y, 8) higher than the one already observed. 
Values around 0.5 indicate that the distributions of the replicated and actual data are close, 
while values close to zero or one indicate differences between them (Gelman and Meng, 
1996, p. 19 1); additional details can also be found in Bayarri and Berger (2000). Posterior 
p-values must not be used or interpreted as the “probability that the current model is true” 
(Gelman and Meng, 1996, p. 192). Although the scaling of p-values is more familiar, it is 
not clear directly which cutpoints must be used to identify inadequate models, and further 
calibration is needed (Sellke et al., 2001). 

INTRODUCTION 
343 
In the preceding model checks, data are used twice: first for estimation of the posterior 
predictive density and second for comparison between the predictive density and the data. 
Although such comparisons clearly violate the likelihood principle, Meng (1994) argues 
in favor of “posterior predictive checks” provided that they are used only as measures of 
discrepancy between the model and the data in order to identify poorly fitted models (model 
adequacy) and not for model comparison and inference (Carlin and Louis, 2000, p. 48). 
We can divide model checks into individual and overall diagnostics. Individual checks 
are based on each yi and yYp separately, and their aim is to trace outliers or surprising 
observations under the assumed model. Overall predictive diagnostics aim to check more 
general assumptions of the model such as normality or the goodness-of-fit of the model. 
Steps in a selection of the possible posterior predictive checks are listed here: 
1. Plot and compare the frequency tabulations of y rep and y (for discrete data). 
2. Plot and compare the cumulative frequencies of y rep and y (for continuous data). 
3. Plot and compare ordered data y;rp, . . . , y;;) 
and ( ~ ( ~ 1 ,  
. . . , ~ ( ~ 1 )  
for continuous 
data. 
4. Plot estimated posterior predictive ordinate f ( y i  Iy) against yi to trace surprising 
values. The posterior predictive ordinate 
(10.3) 
provides us the probability of again observing y i  after having observed y. Low 
values of f(yi Iy) indicate observations originating from the tail areas of the assumed 
distribution, while extremely low values indicate possible outliers. A large amount 
of yi with small PPO may indicate a poorly fitted model. The scaling of PPO depends 
on the structure of the assumed data distribution. Thus, no general rule for identifying 
surprising values using PPOs can be adopted. For this reason, each PPO value must be 
monitored relative to other PPO values such as, for example, their maximum values. 
A graph of PPOs versus yi considerably simplifies their evaluation. 
5. Use test statistics and tail area probabilities (posterior p-values) to quantify differences 
concerning: 
a. Outliers: We may identify outliers using individual test statistics on the basis 
of residual values; see Section 10.3.5 for more details. 
b. Certain structural assumptions of the model: We may use global test statistics to 
assess the plausibility of structural assumptions of the model. For example, we 
may compare the skewness and the kurtosis of y re* with the corresponding ob- 
served measures in order to check for the plausibility of assuming a mesokurtic 
and symmetric distribution for y; see a similar example in Spiegelhalter et al. 
(19960, pp. 4347). 
c. Fitness of the model: Usual measures are the x2 type of statistic (Gelman et al., 
1995, p. 172) 

344 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
or the deviance measure 
n 
Deviance(y, Q) = -2 c l o g  f(yilf3) . 
For more details and further statistics used for checking the goodness-of-fit, see 
Section 10.3.7. 
i=l 
10.1.3 Cross-validation predictive densities 
Although the full predictive distribution f(y’ly) is useful for prediction, its use for model 
checking is questionable because of their double use of the data. For this reason, several 
authors (Gelfand, Dey and Chang, 1992; Gelfand, 1996; Vehtari and Lampinen, 2003; 
Draper and Krnjajik, 2006) have proposed the use of cross-validatory predictive densities. 
In this approach, the full set of data y is divided to in two subsets (y 1, yz). The first set 
y1 is used to fit the model and estimate the posterior distribution of interest, while the 
remaining observations y2 are used for model evaluation and checking by calculating the 
cross-validatory predictive density: 
One major difficulty in this approach is the selection of y1 and yz. Moreover, different splits 
of the data provide different results making the problem difficult to handle. A solution 
is proposed by Geisser and Eddy (1979), who proposed using the leave-one-out cross- 
validation (CV-1) predictive density 
where yi is the ith observation of y and yIi is y after omitting yi. This quantity is also known 
as the conditional predictive ordinate (CPO) (Gelfand, 1996). It can provide a quantitative 
measure for the effect of observation i on the overall prior predictive density f(y) since 
This quantity is equivalent to the posterior predictive ordinate (PPO) given in (10.3), and it 
also provides a measure for tracing outliers. Small CPOs indicate observations that are not 
expected under the cross-validation predictive distribution of the current model. 
An overall measure of fit can be constructed by by the product of CPOs (cross-validation 
predictive likelihood); for more details, see Chapter 11. 
Using CPOs, we can also construct an overall measure of fit given by their product 
(cross-validation predictive likelihood); for more details, see Chapter 1 1. 
10.2 ESTIMATING THE PREDICTIVE DISTRIBUTION FOR FUTURE OR 
MISSING OBSERVATIONS USING MCMC 
Let us consider a usual normal regression model as defined in Section 5.2 and a (unknown) 
fUtureobservationYn+l with(kn~wn)covariatevaluesz(,+~) = ( x ~ + ~ , ~ ,  
~ , + ~ , 2 , .  . . , z , + ~ , ~ )  

ESTIMATING THE PREDICTIVE DISTRIBUTION FOR FUTURE OR MISSING OBSERVATIONS USING MCMC 
345 
Then we can estimate its expected value E (Yn+l I y, z ( ~ + ~ ) )  
using the predictive distribu- 
tion 
f (~n+ilY, 
z ~ ( ~ + i ) )  
= J' f (vn+i lP, g2, q n + l ) )  f ( P ,  02/y) 
dP dg2 . 
Quantity yn+1 can considered as an additional parameter under estimation. Thus it can be 
generated within an MCMC scheme from the conditional posterior distribution 
f (!h+lIP! g2! 3, 
%+l)) = f (Yn+l IP>f12, z(n+l)) 
since Y, are independent, identically distributed (i.i.d.) random variables. Hence, we 
only need to generate yn+l from the distribution assumed by its model structure with the 
appropriate parameter values. For the usual normal regression model, we can generate y n+l 
in the tth iteration of the algorithm by 
In WinBUGS we need only to define an additional stochastic node ynew 
where xnew [I is the vector with elements of the explanatory values for the future 
(to-be-estimated) response. To complete the specification of the additional nodes, we need 
to specify xnew in the data of the WinBUGS model code. Moreover, in the data, we must 
specify that the value of ynew is not available by setting ynew=NA in the list data format. As 
we have already mentioned, ynew is treated in a manner similar to that used for parameters. 
A simple monitor of the posterior distribution of ynew produces a sample from the poste- 
rior predictive distribution, enabling us to calculate posterior summaries, density plots, and 
other properties. 
Otherwise, we may incorporate additional to-be-predicted observations in the data vec- 
tor y by substituting specific elements with NA values. Therefore, within the Bayesian 
framework (and WinBUGS ), both future and missing observations are handled in a similar 
manner using the predictive distribution. 
10.2.1 
Let us consider again the schizotypal personality data of Example 5.3 and the model fitted 
using the WinBUGS code of Table 5.15. In order to estimate the missing observations, we 
substitute values 24 and 13 with NA values. Hence the data can now be written as 
A simple example: Estimating missing observations 
in list format or as 

346 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
in rectangular (data) format. 
After compiling and running the model, posterior summaries of y provide values only 
for the missing (i.e., stochastic) components of this vector; see Table 10.1 and Figure 10.1 
for posterior summaries and densities, respectively. 
Table 10.1 Posterior summaries for missing values y312 and y322 in Example 5.3 
node 
mean 
sd 
MC error 2.5% 
median 
97.5% start sample 
y[3,1,2] 22.76 8.131 0.2267 
5.568 22.87 
39.09 1001 2000 
vC3.2.21 12.21 8.2 
0.187 
-4.321 
12.03 
29.99 1001 2000 
Figure 10.1 Posterior predictive densities for y312 and y322 in Example 5.3. 
According to this estimate, males and females living in villages with high income are a 
posteriori expected to score about 23 and 12 points in the SPQ scale, respectively. From 
the analysis presented above, a major deficiency of the proposed model is revealed. The 
normal distribution is not appropriate for such data since it assumes that the response variable 
also assumes negative values. Thus, especially for the second case, which corresponds to 
females, the model assigns to negative values a considerable posterior probability. For this 
reason, we may transform the response using the log-SPQ score to ensure that positive values 
for the predicted SPQ scores will be estimated; this is equivalent to using the log-normal 
distribution instead. 
In order to fit the log-transformeddata (with missing values), we have two alternatives to: 
either insert directly the log-transformed data with NAs in the missing observations or change 
the response distribution to log-normal using the command y [il "dlnorm(mu [il ,tau). 
Posterior summaries and densities of the predictive distributions of ~ 3 1 2  
and ~ 3 2 2  
using the 
log-transformed data are provided in Table 10.2 and Figure 10.2, respectively. According 
to the results, we expect that males and females leaving in villages with high income are a 
posteriori expected to score about 25 and 14 points in the SPQ scale respectively. The 95% 
posterior predictive intervals are much wider using this model, providing sensible values. 

ESTIMATING THE PREDICTIVE DISTRIBUTION FOR FUTURE OR MISSING OBSERVATIONS USING MCMC 
347 
From Figure 10.2 we observe that a problem similar to that in the case of the simple 
normal model arises since values higher than 74 have a minor but positive probability of 
appearing. Hence, an even more realistic model can be based on the binomial distribution 
and the logistic regression model, resulting in a discrete predictive distribution; see Chapter 
7 .  
Table 10.2 
log-transformed data 
Posterior summaries for missing values 7~312 and 7 ~ 3 2 2  in Example 5.3 using 
node 
mean 
sd 
MC error 2.5% 
median 97.5% start sample 
y[3,1,2] 
25.32 13.55 0.3768 
8.704 
22.89 
56.77 1001 2000 
y[3,2,21 
13.53 8.499 0.1969 
4.652 
12.08 
31.72 1001 2000 
Figure 10.2 
data. 
Posterior predictive densities for ?&I2 and y322 in Example 5.3 using log-transformed 
10.2.2 An example of Bayesian prediction using a simple model 
Example 10.1. Outstanding car insurance claim amounts. Table 10.3 lists the 
amounts (in thousand euros) paid per year for car accidents from an insurance 
company in Greece, while inflation factors are provided in Table 10.4 (Ntzoufras 
and Dellaportas, 2002). Original amounts were transformed to Euros to make their 
scale familiar to the current currency of the European Union. Data are provided in a 
tabular form, with rows indicating in the actual accident year of the claim amounts 
(factor A) and columns indicating years that the compensation was delayed (factor 
B). Thus, yij denotes the amount that the company paid in year i with delay o f j  - 1 
years. Delaying pending claims is a common practice for insurance companies. 
Such delays usually occur for various administrative reasons. It is of central interest 
in actuarial practice to be able to precisely estimate such quantities in order to 
reserve appropriate amounts and be able to pay its obligations to customers. For 
this reason, this problem has attracted the attention of many researchers. 
Assuming that these data were available at the end of year 1995, the lower triangle 
in Table 10.3 displays future payments that were not available at the time when the 
data were collected. Hence, in this example, we focus on the estimation of amounts 
missing from the lower triangle of Table 10.3. 

348 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
1989 
1990 
1991 
A 1992 
1993 
1994 
1995 
Table 10.3 Claim amounts data (in thousands of Euros) for Example 10.1 
1546.6 
647.5 
382.2 
246.8 211.8 
63.6 
146.3 
2099.0 1001.8 
487.2 
293.0 318.8 269.9 
3422.2 1257.1 
488.4 
456.0 562.4 
4948.8 1899.7 
984.0 1253.3 
8161.3 2820.3 1304.8 
10622.0 3897.7 
11744.9 
Table 10.4 
Inflation factors for Example 10.1 
Year 
1989" 
1990 
1991 
1992 
1993 
1994 
1995 
Inflation (%) 
100.0 120.4 143.9 166.6 190.6 214.2 
235.6 
Year 
1996 
1997 
1998 
1999 2000 
2001 
Inflation (%) 248.4 261.8 273.6 279.4 287.5 298.1 
aYear 1989 was used as the baseline year. 
Source: Ntzoufras and Dellaportas (2002). Copyright 2002 by the Society of Actuaries, 
Schaurnburg, Illinois. Reprinted with permission. 

ESTIMATING THE PREDICTIVE DISTRIBUTION FOR FUTURE OR MISSING OBSERVATIONS USING MCMC 
349 
70.2.2.7 Model formulation. We base our predictions on the simplest and most pop- 
ular model used in the relevant literature. The proposed model is a simple two-way main 
effects ANOVA model with response the log-amounts data. For details concerning this 
model and other related approaches, see Ntzoufras and Dellaportas (2002) and references 
cited therein. Hence the model is given by the formulation 
where Zab are the log-deinflated claim amounts, L are the number of years considered and 
infa+b- 1 is the inflation corresponding to the payment year of cell ab. A detailed description 
of building the above model in WinBUGS can be found in Scollnik (2002). 
The model is alternatively expressed by 
where LN(p, 0') are the log-normal distributions with parameters p and o 2  (which are the 
mean and the variance of log Y). This expression is convenient for WinBUGS since the 
likelihood of the model can be directly expressed using the syntax 
Monitoring of y directly provides us the predictive posterior distributions for the lower 
missing triangle of Table 10.3. 
We additionally need to calculate total payments for each year denoted by Ti for i = 
L + 1, . . . ,2L - 1 calculated by 
(a,b):a+b=i+l 
a=i-L+I 
Moreover, the total amount to be paid due to the outstanding claims is also of interest. It is 
calculated simply by the sum of the aggregated amounts 
2L-1 
T =  
Ti. 
i=L+l 
In WinBUGS these quantities are computed using the following syntax 
in which the matrix Y. T of dimension ( L  - 1) x L is first calculated by 
for a = 1,2, . . . , i - L 
(" 
Ya,i+l+u for a = i - L + 1,. . . , L 
Y.T~-L,~ 
= 
for i = L + 1, L + 2 ,  . . . , 2  L - 1. The annual sums are calculated by 
L 
Ti = C T.K,i+l-a 
a=l 

350 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
since 
L 
2-L 
L 
L 
and Cizt T.Y,,,+l-, = 0. 
To complete the model formulation, we adopt the usual comer constraints: a 1 = p 1  = 0. 
Consequently, expression (10.4) assumes that the expected log-adjusted claim amount p ,J,,, 
originated at year a and paid with delay of h - 1 years, is modeled via a linear predictor 
that consists of the expected log-adjusted claim amount p originating and paid within the 
first accident notification year, a factor that reflects expected changes due to the origin 
year a,, and a factor depending on the delay pattern P b .  These constraints imply that the 
parameters to be estimated are p, u2, a = ( a2,. . . , aT ) T ,  p = ( pz, . . . , pT ) T ,  and 
Y L  = { yZJ : i + j > r + 1}. Finally, the usual prior setup is adopted by 
p - N(0, gi), 01, - N(0, c:,), pb - N(0, ug3), a, b = 2, ..., L, 7 = K 2  - G(a,, h,). 
For the type of problem we are interested in, low-information prior distributions can be 
produced by 
u: = 1000, u2a = 100, a = 2, ..., L, C& = 100, h = 2, ..., r, a, = b, = 0.001. 
Data in WinBUGS code can be provided in a list format, with y given in L x L tabular 
format with the lower triangle filled with NAs; see Table 10.5 for the list data format. The 
WinBUGS code for the full model of this example is presented in Table 10.6. 
Table 10.5 
Data in list format for Example 10.1 

ESTIMATING THE PREDICTIVE DISTRIBUTION FOR FUTURE OR MISSING OBSERVATIONS USING MCMC 
351 
Table 10.6 
WinBUGS code for Example 10.1" 
"Data are given in Table 10.5. 

352 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Results. Posterior summaries of model parameters using 10,000 iterations (after discard- 
ing the initial 1000 iterations) are given in Table 10.7. Posterior summaries are close to the 
ones reported by Ntzoufras and Dellaportas (2002) and Scollnik (2002). Differences are 
observed to the intercept 
with posterior mean equal to 7.229. By adding the log-exchange 
rate between euros and drachmas to the constant term, we get 7.23 + log(340.75) = 13.06, 
which is the value reported by the authors cited above. 
From the posterior distributions of the model parameters we observe 
0 A positive effect of accident year to the deinflated amounts which shows that claim 
amounts increase from year to year (possibly because of the increased number of 
contracts by the insurance company). 
0 A negative effect of the delay year to the outstanding claims, which is plausible since 
the amount of unsettled claims reduces with time. 
Table 10.7 Posterior summaries for model parameters in Example 10.1" 
node 
mean 
sd 
MC error 2.5% 
median 97.5% start sample 
mu0 
7.229 0.1687 0.006309 
6.889 
7.229 7.563 1001 10000 
alpha[2] 0.3572 0.1655 0.004067 0.02533 0.3595 0.6804 1001 10000 
alphaC31 0.4682 0.1807 0.004547 
0.1056 0.4669 0.8315 1001 10000 
alpha[4] 0.917 0.1938 0.00479 
0.5273 0.9197 1.307 1001 10000 
alphaC51 1.068 0.2145 0.004901 0.6363 
1.069 1.491 1001 10000 
alpha[6] 1.271 0.2471 0.005466 0.7894 
1.273 1.758 1001 10000 
alphaC71 1.284 0.3317 0.006881 0.6192 
1.282 1.938 1001 10000 
betaC21 -1.085 
0.167 0.003729 -1.418 
-1.084 -0.7527 1001 10000 
betar31 -1.942 
0.1795 0.004298 -2.303 
-1.94 
-1.586 
1001 10000 
betaC41 -2.247 
0.1931 0.004576 -2.633 
-2.245 -1.862 
1001 10000 
betaC51 -2.441 
0.2139 0.004726 -2.866 
-2.442 -2.016 
1001 10000 
beta161 -3.342 
0.2501 0.005683 -3.833 
-3.34 
-2.852 
1001 10000 
beta[7] -3.099 
0.3293 0.007312 -3.758 
-3.095 -2.455 
1001 10000 
S 
0.282 0.0572 0.001141 
0.1962 0.2739 0.4198 1001 10000 
aTotal of 10,000 iterations kept; 1000 bumin. 
Returning to the estimated to-be-paid claim amounts, posterior predictive means for each 
future amount are presented in Table 10.8, while posterior predictive summaries for claim 
totals are provided in Table 10.9 and Figure 10.3. Therefore, the amounts that the company 
is expected to pay for car accidents that occurred during 1989-1 995 are equal to 8883,5 143, 
3767, 2430, 1282, and 769 thousand euros for years 1996,1997,. . . ,2001, respectively. 
The total corresponding amounts are found aposteriori equal to 22.27 million euros. Finally, 
if the company wishes to ensure that reserved accounts will cover its obligations with high 
probability, then reserved accounts can be based on posterior percentiles (e.g., 95th, 97.5th 
or 99th percentiles). Using this conservative approach, if the insurance company reserved 
the total amount of 34.1 million euros, then it would be able to fully cover the future 
obligations due to pending claims with probability equal to 97.5%. 

ESTIMATING THE PREDICTIVE DISTRIBUTION FOR FUTURE OR MISSING OBSERVATIONS USING MCMC 
353 
Table 10.8 
Example 10.1 
Posterior predictive means for claim amounts (in thousands of euros) for 
B 
3 
4 
5 
6 
7 
1989 
1990 
1991 
A 1992 
1993 
1994 
1995 
242.2 
208.4 287.0 
798.4 346.2 472.2 
1126.0 
979.9 421.1 562.1 
1882.0 1461.0 1272.0 531.3 715.2 
4626.0 2068.0 1602.0 1337.0 566.8 769.1 
Table 10.9 
10.1" 
Posterior predictive summaries for to-be-paid total claim amounts for Example 
node 
mean 
sd 
MC error 2.5% 
median 97.5% 
s t a r t  sample 
T[1] 
8883.0 
2412.0 33.9 
5488.0 8485.0 14760.0 1001 10000 (1996) 
T[2] 
5143.0 
1285.0 21.28 
3207.0 4961.0 
8069.0 1001 10000 (1997) 
T[3] 
3767.0 
1065.0 17.06 
2218.0 
3604.0 6283.0 1001 10000 (1998) 
T[4] 
2430.0 
765.4 
13.13 
1326.0 2301.0 4251.0 1001 10000 (1999) 
T[5] 
1282.0 
496.0 8.379 
604.5 
1204.0 2450.0 1001 10000 (2000) 
T[6] 
769.1 
462.5 7.028 
238.9 
677.2 
1835.0 1001 10000 (2001) 
Total 22270.0 4911.0 90.62 
14740.0 21600.0 34110.0 1001 10000 
"Total of 10,000 iterations kept; 1000 burnin. 
(a) Total claim amount 
(b) Total to-be-paid claim amounts ( T I ,  . . . , Te) 
Figure 10.3 
10,000 iterations kept, 1000 bumin). 
Posterior predictive plots for total outstanding claim counts for Example 10.1 (total of 

354 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
10.3 USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
10.3.1 Comparison of actual and predictive frequencies for discrete data 
As we have already mentioned, we can use the predictive distribution to re-generate the 
distributions of future (or replicated) data yrep. This distribution can be used to compare 
the predicted values under the assumed model with the actual data. Large deviations of 
the two distributions may indicate that the model is inappropriate for the data used. The 
comparison of the predictive distributions y rep is relative easy and convenient when discrete 
variables are considered. In the following we illustrate this by using a simple example with 
data from association football (soccer), which is popular in Europe. 
Example 10.2. The distribution of Manchester United's goals in home games 
for season 2006-2007. In this example we consider the number of goals scored 
and conceded by Manchester United in home games for Season 2006/07 in the 
English premiership of association football (soccer), presented in Table 10.10. The 
aim here is to estimate the expected number of goals using a simple Poisson model. 
Table 10.10 
games of Manchester United for premiership season 2006-2007 
Data for Example 10.2: Frequency of goals scored and conceded in home 
Number of goals 
0 
1 
2 3 4 5 Total games 
Frequencies for scoredgoals 
2 
3 
4 6 3 1 
19 
Frequencies for conceded goals 8 10 1 
Model formulation and WinBUGS code. A simple Poisson model with common mean 
yik - Poisson(Xk) is used for the scored ( k  = 1) and conceded (k = 2) goals. Although 
the usual (conjugate) gamma prior can be adopted, here we facilitate a log-normal prior 
with zero mean and large variance (lo3) in the log-scale, which is the default choice in the 
Poisson log-linear model presented in Chapter 7. 
Since the data are given in tabulated format, we can express the likelihood by 
where zij = yi, nf is the number of observed frequencies and f, is the frequency of Y = y. 
In WinBUGS it is possible to fit models for data given in a frequency table format using the 
syntax 
when the data for the scored goals are imported in the following list format 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
355 
where nf is the number of available frequencies. 
To generate replicated data yrep of the same size, we simply add the command 
within the double loop above. More generally, we may generate a set of data of size ri rep 
(n. rep in WinBUGS ) using a vector node by setting 
outside the above mentioned likelihood double loop. In the first case, the distribution of the 
predictive and observed counts can be directly compared; in the second case, the distribution 
of the corresponding relative frequencies must be used instead. The frequency table of each 
replicated set can be stored and monitored using the syntax 
In this syntax we first calculate matrix freq. bin of dimension n f  x n r e p  (here 6 x 19) 
with elements f req. bin [k, il taking value equal to one if y y p  = yk and zero otherwise. 
Then the frequencies are calculated as the sum of each row of this matrix. Concerning the 
conceded goals, the same model is used by substituting the data by the following list data 
format 
Results. Posterior summaries for the mean scored and conceded goals as well as the 
frequencies of the future (replicated) goals are presented in Tables 10.1 1 and 10.12. Error 
bars of the frequencies of the goals are given in Figure 10.4, produced in WinBUGS and 
Table 10.11 
Manchester United in Example 10.2" 
Posterior summaries for X and frequencies of replicated scored goals for 
node 
mean 
sd 
MC error 2.5% 
median 97.5% s t a r t  sample 
lambda 
2.407 
0.3432 
0.007838 1.762 
2.399 
3.094 
1001 2000 
freqC11 
1.821 
1.466 
0.03602 
0.0 
2.0 
5.0 
1001 2000 
freqC21 
4.19 
1.99 
0.04141 
1.0 
4.0 
9.0 
1001 2000 
freqC31 
4.822 
1.913 
0.04489 
1.0 
5.0 
9.0 
1001 2000 
freqC41 
3.933 
1.804 
0.03462 
1.0 
4.0 
8 . 0  
1001 2000 
freqC51 
2.277 
1.506 
0.02857 
0 . 0  
2.0 
6.0 
1001 2000 
freq[6] 
1.182 
1.103 
0.02522 
0 . 0  
1.0 
4.0 
1001 2000 
re1 . freq [I] 0.09584 0.07717 0.001896 0 . 0  
0.1053 0.2632 1001 2000 
rel.freqC21 0.2205 
0.1047 0.00218 
0.05263 0.2105 0.4737 1001 2000 
rel.freqC31 0.2538 
0.1007 0.002362 0.05263 0.2632 0.4737 1001 2000 
rel.freqC41 0.207 
0.09493 0.001822 0.05263 0.2105 0.4211 1001 2000 
rel.freqC51 0.1198 
0.07928 0.001504 0.0 
0.1053 0.3158 1001 2000 
rel.freqC61 0.06221 0.05804 0.001327 0.0 
0.05263 0.2105 1001 2000 
aTotal of 1000 iterations kept; 1000 bumin. 

356 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
compared to the original data superimposed (dashed lines). Although the model is sim- 
ple, we observe that the Poisson distribution seems to describe the original scored goals 
sufficiently since the replicated ones are quite close to them (all 95% posterior intervals 
include the observed data, indicating that the orginal dataset can be regenerated using the 
proposed model). Concerning the conceded goals, we observe that all actual counts (except 
for y = 0) lie at the tails of each predictive distribution, which indicates that a more sophis- 
ticated distribution is needed for these data. Finally, combining the two datasets and models 
in one WinBUGS model will also enable us to produce estimates of the goal difference and 
the probability of win, draw, and loss for Manchester United. 
Table 10.12 
Manchester United in Example 10.2 
Posterior summaries for X and frequencies of replicated conceded goals for 
node 
mean 
sd 
MC error 2.5% median 97.5% start sample 
lambda 
0.6301 0.1789 0.00393 0.335 0.6082 1.033 1001 2000 
freq[ll 
10.26 
2.721 0.06067 5.0 10.0 15.0 
1001 2000 
freq[2] 
6.178 
2.081 0.04327 2.0 
6.0 10.0 
1001 2000 
freq[3] 
1.974 
1.458 0.03346 0.0 
2.0 
5.0 
1001 2000 
freq[4] 
0.487 
0.7535 0.01591 0.0 
0.0 
2.0 
1001 2000 
freqC51 
0.0825 0.3061 0.007382 0.0 
0.0 
1.0 
1001 2000 
freq[6] 
0.015 
0.1256 0.002836 0.0 
0.0 
0.0 
1001 2000 
rel.freq[ll 0.5401 0.1432 0.003193 0.2632 0.5263 0.7895 1001 2000 
rel.freqL21 0.3251 0.1095 0.002277 0.1053 0.3158 0.5263 1001 2000 
rel.freqL31 0.1039 0.07676 0.001761 0.0 
0.1053 0.2632 1001 2000 
rel.freqL41 0.02563 0.03966 8.371E-4 0.0 
0.0 
0.1053 1001 2000 
rel.freqC51 0.004342 0.01611 3.885E-4 0.0 
0.0 
0.0526 1001 2000 
rel.freqC61 7.895E-4 0.00661 1.493E-4 0.0 
0.0 
0.0 
1001 2000 
aTotal of 1000 iterations kept; 1000 burnin. 
(a) Scored goals 
(b) Conceded goals 
Figure 10.4 
Manchester United in Example 10.2 (total of 1000 iterations kept, 1000 burnin). 
Posterior predictive plots for frequencies of replicated and actual goals conceded for 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
357 
10.3.2 Comparison of cumulative frequencies for predictive and actual 
values for continuous data 
When the data vector y is continuous, then we need to consider a more sophisticated 
approach to compare the actual and replicated values. Such a comparison can be based on 
the predictive cumulative relative frequencies FFd = P(Y < yily) compared with the 
observed cumulative relative frequencies Fvt. The predictive predictive cumulative relative 
frequencies are given by 
Fpred 
v’ = P(Y < Y’lY) 
= 1: J’ f(4e)f(elv) do dz 
= 1, J’ r ( Z  < Y’)f(zle).f(elY) 
dz 
This quantity can be estimated from an MCMC output by the expression 
assuming that we obtain yrep of size equal to the original sample n. Then a plot of the 95% 
posterior intervalus of FFred by F, will provide us with a good visual representation of the 
predictive cumulative frequencies in comparison to the observed data. 
Hence, in WinBUGS , if we use the syntax 
we first identify which yi > y? using the command step. Each comparison is stored in 
the binary matrix pred . lower. y i  of dimension n x n. Then the cumulative frequencies 
F;zred of the predictive values in each MCMC cycle are calculated by the sum of each row. 
An error bar plot of the 95% posterior intervals of FFd versus observed frequencies Fyt 
can be produced using either the compare tool of WinBUGS or another statistical program. 
Example 10.3. Simulated normal data. To demonstrate the approach described 
above, we have generated the following 19 values from the standardized normal 
distribution forming dataset 1; see Table 10.13. To further illustrate how the pre- 
dictive measures and plots behave in the presence of outliers, we have added the 
values of 5 and 10 in the data of the originally simulated dataset 1, forming in this 
way datasets 2 and 3, respectively. 
Table 10.13 Dataset 1 of Example 10.3: Simulated data from N ( 0 , l )  
~ 
0.51, 
0.10, -2.53, 
1.00, 
0.65, -0.95, 
2.76, 1.33, 0.25, 
1.48, 
-0.25, 
-0.45, 
2.11, -0.76, -1.51, 
-0.35, 0.18, 1.35, -0.18 

358 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Results. We have run a simple normal model using WinBUGS and calculated the pre- 
dictive cumulative frequencies for each observed value for the three datasets. Figure 10.5 
provides posterior predictive error bars of cumulative relative frequencies for each y i against 
its corresponding observed frequencies. If the model is appropriate, then these values must 
be close; hence the y = II: line is also plotted as a reference line. For the first dataset (Fig- 
ure 10.5a), we observe that both predicted and observed frequencies are close with small 
deviations from the reference line. For the second dataset (Figure 10.5b), larger deviations 
are observed with the last parameter having a narrow error bar indicating that very few 
predictive values were higher than y(zo) = 5. Finally, for the last dataset (Figure lOSc), 
deviations between predictive and observed frequencies are high (where the reference line 
is outside specific error bars), indicating poor fit of the model. Moreover, no value was 
generated from the predictive distribution with values higher than y (20) = 10, indicating 
that this value has a very low probability of observing this value if the current model holds. 
Finally, in Table 10.14, the effect of the artificial observations added in datasets 2 and 3 
on the posterior distribution of p and 0 is clear (both the posterior mean and the standard 
deviation were increased when a single outlier value was added in the dataset). 
Observed cumulative frequency 
7.- 
Observed cumulative frequency 
_ ~ _  
Observed cumulative frequency 
(a) Without outlier 
(b) With outlier y(20) = 5 
(c) With outlier ypo) = 10 
Figure 10.5 
frequencies for Example 10.3. 
Posterior predictive error bars of cumulative frequencies versus observed cumulative 
Table 10.14 
( p , ~ )  
for Example 10.3 
Actual values and posterior means (standard deviations) for model parameters 
Parameter True values 
Dataset 1 
Dataset 2 
Dataset 3 
~~ 
~ 
~~~~ 
F 
0.0 
0.26 (0.30) 0.48 (0.39) 0.73 (0.59) 
U 
1.0 
1.33 (0.25) 1.71 (0.30) 2.63 (0.47) 
10.3.3 Comparison of ordered predictive and actual values for continuous 
data 
An alternative and efficient predictive check can be based on the ordered data y (%) and yt:;. 
For each ordered observation, we compare ~
(
~
1
 
with the distribution f (y;:rly) estimated 
by the generated values y;;;. 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
359 
The distribution of f (y;:; 
[y) can be estimated within an MCMC cycle by adding the 
1. Generate TJ,"~'(~) from f (yi l8(')). 
following steps: 
2. Order yTp'(t) to get ( Y ( ~ )  
rep,(t) , . . . , y(nl 
r e P o ) ) ,  
After generating ( y ~ ~ ~ " " ' ,  
. . . , TJ;;"~)) 
we compare the distribution of the generated values 
yi:r'(t) with ~ ( ~ 1 .  
Graphical comparison of these two quantities can be produced by an error 
bar of ~f;;"~) 
for each value of y ( ~ .  
Example 10.3. Simulated normal data (continued). Error bars of the predicted 
ordered data against the corresponding observed ones for the simulated datasets of Example 
10.3 are given in Figure 10.6. Bars crossed by the diagonal line y = z indicate that observed 
and predicted ordered data are close. From the error bars, it is clear that for the first dataset 
(Figure 10.6a), the model is sufficient since all predicted and observed ordered data are 
very close. For the second dataset (Figure 10.6b), the picture is slightly worse with the 
outlier value y(20) = 5 found in the upper tail of the corresponding error bar. The overall 
picture indicates that y(20) = 5 might be an outlier, and not that the model must be rejected. 
Finally, for the third dataset, Figure 10.6~ 
indicates that the model must be rejected since we 
observe high deviations between predicted and observed data, while observation y (20) = 10 
is flagged as an outlier since the corresponding posterior interval does not contain this value. 
The latter also implies that the probability of observing this value is rather low under the 
assumed model. 
1 
., 
0 
I 
2 
-2 
0 
4 
0 
2 
I
6
 8 
I D  
Y 
Y 
Y 
(a) Without outlier 
(b) With outlier y(20) = 5 
(c) With outlier y(zO) = 10 
Figure 10.6 
Posterior predictive error bars of y;:; 
versus observed data y(%) for Example 10.3. 
10.3.4 Estimation of the posterior predictive ordinate 
Instead of replicating the data, we can directly estimate the ith posterior predictive ordinate 
PPOi = f ( ~ i l ~ )  
by 
where 8(') is the vector of parameter values generated in the tth iteration of the MCMC 
algorithm. Thus, we can directly calculate f^(yiJy) in WinBUGS by considering a deter- 
ministic node set equal to the likelihood evaluated at the current values of 8. For discrete 

360 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
distributions, we can compare f(yily) with the observed frequency for each yi, while for 
continuous data we can produce a rough estimate of the cumulative distribution on the basis 
of the observed y by calculating 
for k = 0,1,. . . , n, n + 1, where y(k) is the kth ordered observation and ~ ( ~ 1 ,  
y(n+l) are 
additional low and high values used to calculate the lower and upper tails of the distribu- 
tion. Here we have set y(o) = min(y) - SD(y) and y(n+l) = max(y) + SD(y) with 
SD(y) the sample standard deviation of the data y. The p(Y 5 y(k) ly) estimated above 
must be compared visually with the corresponding observed cumulative probability relative 
frequency of Y < y(%), which is equal to i/n. 
Example 70.2: ManChester United’s goals (continued). In the WinBUGS model 
code of the Poisson model used for Example 10.2, we have additionally defined node ppo 
using the syntax 
which calculates the probability under the simple Poisson model for all observed values 
yl, y2, . . . , ylLf under the sampled parameter values in each MCMC cycle. The probability 
function is written in log-scale form and then exponentiated: 
X+Y lodX)-log(Y!) . 
f ( Y l A )  = e- 
This is a usual practice in statistical computation, which is used to avoid arithmetic over- 
flows, and it is also recommended when defining nodes within WinBUGS model codes. 
Posterior means of ppo have been calculated after 2000 iterations (and an additional 
1000 iterations as burnin) and are given in Table 10.15. Comparison of estimated PPOs 
and observed relative frequencies is provided in Figure 10.7. As expected, PPOs using this 
approach are similar to the posterior means of the frequencies in predicted data replicated in 
Section 10.3.1. For scored goals, differences between PPOs and observed relative frequen- 
cies are small, with the exception of values for y = 3, while for conceded goals differences 
seem to be more severe, indicating that an alternative distribution may be more appropriate. 
Table 10.15 
Estimated posterior predictive ordinates (PPO) and observed relative 
frequencies for scored and conceded goals in home games of Manchester United for 
premiership season 2006-2007 (Example 10.2) 
0
1
2
3
4
5
 
Scored goals 
Observedrelative frequencies 
0.1 1 0.16 0.21 0.32 0.16 0.05 
Estimated PPO (posterior mean) 0.10 0.22 0.26 0.20 0.12 0.06 
Observed relative frequencies 
0.42 0.53 0.05 0.00 0.00 0.00 
Estimated PPO (posterior mean) 0.54 0.32 0.10 0.02 0.00 0.00 
Conceded goals 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
361 
:L 
0 
0 
0 
Figure 10.7 
1 
2 
3 
4 
5 
0 
?I 
0 
2 
3 
4 
5 
Y 
(a) Scored goals 
(b) Conceded goals 
- 
Estimated posterior predictive ordinates (solid line) and observed relative frequencies 
(dashed line) for scored and conceded goals in home games of Manchester United for premiership 
season 2006-2007 (Example 10.2). 
Example 10.3: Simulated normal data (continued). Returning to Example 10.3, 
we have to define the node ppo using the syntax 
in order to calculate the posterior predictive ordinates PPOi for i = 1,2, . . . , n. Two 
additional PPOs are also calculated using the syntax 
where ysmall and yhigh are the values y(o) and y(n+l) defined one standard deviation 
away from the minimum and maximum observed values in each dataset. These values are 
used to more accurately compute probabilities at the tails of the distribution required in 
calculation of the cumulative frequencies (10.6). 
Estimated cumulative function of the predictive distribution compared with the observed 
one is provided in Figure 10.8, while the estimated density function of the predictive dis- 
tribution is depicted in Figure 10.9 for all three datasets. From Figure 10.8 we observe 
that for dataset 1, the simple normal model fits the data satisfactorily. For dataset 2 we 
observe differences between the predictive and the observed distribution, which are even 
more severe for the third dataset. Therefore, improvement of the model may be needed for 
the second dataset, while for the third dataset the adopted model is clearly problematic and 
needs to be revised. 

362 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
y . ordered 
y,ardered 
y,ordered 
(a) Without outlier 
(b) With outlier yp0) = 5 
(c) With outlier ypo) = 10 
Figure 10.8 
for Example 10.3. 
Observed (dashed line) and estimated predictive (solid line) cumulative distribution 
-1 
4 
0 
2 
I 
4 
a 
0 
2 
4 
6 
Y 
I 
Y 
(a) Without outlier 
(b) With outlier y(20) = 5 
(c) With outlier ypo) = 10 
Figure 10.9 Estimated predictive density function for Example 10.3. 
10.3.5 Checking individual observations using residuals 
In addition to the quantities calculated in the previous sections, we facilitate residuals for 
checking the fit of each observation and the identification of outliers. 
Residual values can be based on the deviations of the data from the mean of the model; 
see Spiegelhalter et al. (1996~1, pp. 4347) for an example. Hence we define the simple 
(unstandardized) residual as 
and its standardized version by dividing it by the standard deviation under the adopted 
model: 
ri = yi - E(Y,lO) 
T i  
- Yi - E(Y,lO) 
ri = SD(Y,lO) - JTqqq . 
Finally, we can use the tail area probability 
p; = P(r,’eP > rzly) = P(y,‘eP > yily), 
where r,‘eP is the residual value, based on the predictiveheplicated values yFP. The value 
min (PI, 
1 - p;) = min 
yyp > yily), 1 - ~ ( y y p  
> y i ~ g ) }  
can be interpreted as the probability of “getting a more extreme observation.” Details con- 
cerning the use of these quantities can be found in the classic BUGS manual (Spiegelhalter 
et al., 1996~1, 
pp. 4047). 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
363 
Further residual values can be calculated using ordered statistics or frequencies. Here 
we additionally propose calculation of the ordered residual statistics 
or their corresponding ith ranked p-value 
These quantities measure differences between the expected and the observed ordered statis- 
tics and can also trace differences between the observed and assumed distributions of Y .  
Example 10.3 (continued): Residual analysis in the simple normal example. 
For the generated data of Example 10.3, we have calculated standardized residual values 
and their corresponding p-values using the following syntax: 
Standardized residuals with absolute posterior mean values higher than one are presented 
in Table 10.16, accompanied by detailed posterior summaries and their corrsponding tail 
area probabilities. Error bars of the residual values (produced using the compare menu 
of WinBUGS) are provided in Figure 10.10. Reference lines at values of -2 and 2 are 
also added in order to trace outlier values. Finally, in Table 10.17 we also present the 
distribution of min(pl, 1 - pr) and min(p?, 1 -pro). The first must be compared with the 
corresponding expected probabilities under the model under consideration. 
(a) Without outlier 
(b) With outlier ypo) = 5 
(c) With outlier ~ ( ~ 0 )  
= 10 
Figure 10.10 Error bars for standardized residuals of Example 10.3. 
For dataset 1, only two absolute values are close to the value of 2 with error bars in- 
cluding this value. Their corresponding probability values (0.026 and 0.039) are low but 
nonnegligible. Moreover, from Table 10.17, we observe no differences between expected 
and observed frequencies of the residual values. Probabilities based on order statistics are 
all high and thus do not indicate major differences. 

364 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Table 10.16 Posterior summaries for standardized residuals in Example 10.3" 
Dataset 1 
node 
mean 
sd 
MC error 2.5% 
median 97.5% 
P r  
minPr pro 
rs[31 
-2.171 
0.4461 0.01222 
-3.076 
-2.155 
-1.333 
0.974 0.026 0.675 
rsC151 -1.376 
0.3335 0.00917 
-2.054 
-1.36 
-0.734 
0.895 0.105 0.506 
rsC131 
1.445 0.3416 0.01211 
0.8127 
1.446 
2.197 
0.087 0.087 0.457 
rs[71 
1.951 0.4124 
0.01437 
1.169 
1.957 
2.864 0.039 0.039 0.436 
Dataset 2 
rs[3] 
-1.815 
0.3916 0.01055 -2.625 
-1.808 
-1.057 
0.957 0.043 0.481 
rs[15] -1.201 
0.3118 0.00801 -1.838 
-1.185 
-0.578 
0.860 
0.140 0.346 
rs[71 
1.368 0.3055 0.00824 
0.766 
1.377 
1.970 0.086 0.086 0.495 
rs[20] 
2.716 0.4902 0.01413 
1.763 
2.711 
3.717 0.011 0.011 0.123 
Dataset 3 
rs[31 
-1.278 
0.3211 0.00831 
-1.939 
-1.264 
-0.638 
0.886 0.114 0.177 
rsC201 
3.625 0.6304 0.01841 
2.437 
3.632 
4.901 0.002 0.002 0.020 
aOnly cases with ~ T , " I  > 1 are presented. 
Table 10.17 
10.3" 
Frequency tabulation of residual tail area probabilities for datasets of Example 
(0,0.025] (0.025,0.05] (0.05,0.1] (0.1,0.2] 
(0.2,0.5] 
~ 
~~ 
Frequency tabulation of min(p,, 1 - p z )  
Dataset I 
0 
2 
1 
3 
Dataset 2 
1 
1 
1 
2 
Dataset 3 
1 
0 
0 
1 
Expected 
1 
1 
2 
4 
Dataset 1 
0 
0 
0 
0 
Dataset 2 
0 
0 
0 
1 
Dataset 3 
1 
0 
3 
8 
Frequency tabulation of min(p?, 1 - p?) 
(20) 
(20) 
(8,103 16) 
13 
15 
18 
12 
19 
19 
8 
~~ 
" Values in parentheses indicate observations with corresponding probabilities. 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
365 
For the second dataset, results are slightly worse. Again, only two absolute values 
are close to the value of 2, with the corresponding error bars including this value. For 
observation y(zo) = 5 pl' is estimated equal to 0.01 1, which is low but nonnegligible. 
Moreover, from Table 10.17, we observe no differences in the expected and observed 
frequencies of the residual values, Probabilities based on order statistics are all high except 
for the artificial value of y(20) = 5, for which pro = 0.123, indicating a possible outlier. 
Finally, for the third dataset, observation y(20) = 10 is clearly spotted as an outlier since 
its error bar is far away from the value of 2; see Figure 10.10~. Moreover, both tail area 
probabilities are very low (p' = 0.002 and pro = 0.02), indicating an outlier. The first 
states that the probability of observing a value higher than 10 under the fitted model is equal 
to 0.002, while the latter states that the probability of observing the maximum value in a 
sample of 20 observations higher than 10 under the fitted model is equal to 0.02. Moreover, 
from Table 10.17 we can see that the distribution ofp l' is not as close as the expected one (the 
majority of observations have very high values in comparison to the expected ones), while 
three additional observations (indexedas 8,10,16) were also spotted with pro < 0.10. This 
may indicate an overall failure of the model resulted by the influential point y 
= 10. 
10.3.6 Checking structural assumptions of the model 
Posterior p-values can be used for checking the structural assumptions of the fitted model. 
For example, we can check whether the skewness and the kurtosis of the predictive and 
actual data are in agreement (which is particularly useful for normal models), or whether 
the assumption of equal mean and variance in Poisson models is valid. 
To be more specific, we can calculate 
1 "  ('2 
- 
gl(y) = skesess(y) = - 
i=l 
1 "  
gz(y) = kuzsis(y) = 
and their posterior p-valuesp,, = P(gk(yrep) > gi,(y)). Alternatively, we may use their 
Bayesian versions on the basis ofthe standardized residual values defined in Section 10.3.5 
(Spiegelhalter et al., 1996a, pp. 4347) 
gf(y, 0) = kurtosis(y, 0) = 
i= 1 
and their corresponding p-values p g f  = P g: 
(yreP) > 9: (y )) . 
Y/SD(Y)~ 
and its corresponding p-value 
( 
Similarly, for Poisson models we can calculate the sample dispersion index DI(y) = 
PDI = P ( D W ~ P )  > DW). 
We can select or construct any statistic to check for the plausibility of any assumption 
of interest. The selection of such a statistic heavily depends on the problem and the model 
at hand. 

366 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Example 10.2 (continued): Testing for over or underdispersion in Manchester 
United data. We use the DI(y) to test the assumption that the mean is equal to the 
variance. Hence we define the following in WinBUGS : 
Note that the mean and the variance for the actual data ware calculated using expressions 
(since the data are given in a tabulated format), where nf is the number of (different) 
observed values of y and fY is the observed frequency of y. 
Results are given in Table 10.18 indicating that for the scored goals the assumption 
of equal variance and mean is plausible (p-value = 0.29, observed DI = 1.26, posterior 
expected DI for predicted data = 1.12). On the other hand, for the conceded goal the 
observed dispersion index (1.77) is far away from what is expected under the simple Poisson 
model (posterior mean 1.09, 95% posterior interval ranging from 0.6 to 1.78) with low 
posterior value (equal to 0.03), indicating that an overdispersed model (e.g., the negative 
binomial distribution) might better describe the distribution of these data. 
Table 10.18 
Manchester United for premiership season 2006-2007 (Example 10.2y 
Dispersion index statistics for scored and conceded goals in home games of 
Dispersion index statistics 
Goals 
Observed 
Replicated 
Difference 
p-value 
Scored 
1.26 
1.12(0.57,2.18) -0.14(-0.69,0.92) 
0.294 
Conceded 
1.77 
1.09 (0.61, 1.78) -0.68 (-1.16,O.Ol) 
0.030 
aReplicated statistics are produced from g " P  and represent expected DI under the as- 
sumed model; posterior means (95% posterior intervals) are provided for columns 3 and 
4 (replicated and difference, respectively). 
Example 10.3 (continued): Checking for the skewness and the kurtosis of the 
model. Returning to the simulated data of Example 10.3, we calculate the p-values on 
the basis of the sample kurtosis and skewness coefficients as defined above. In order to 
specify 9 and g2 (and their p-values) in WinBUGS , we use the following syntax: 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
367 
and their Bayesian versions, by the following syntax: 
In this syntax we first calculate these elements used in the summations (defined in vectors 
m3, m4, mb3, and mb4) and then the desired measures using the sums of the above vectors. 
Results for the three datasets are summarized in Table 10.19. As we can see, the Bayesian 
p-values are more conservative, but inference is similar whatever measure we used. For the 
first dataset all p-values are close to 0.5, indicating that the observed and expected data have 
Table 10.19 
Skewness and kurtosis statistics for simulated normal data of Example 10.3" 
Observed 
Replicated 
p-value 
Skewness statistics 
Dataset 1 g1 -0.083 
0.009 (-0.902,0.940) 
g? 
-0.104 (-1.592,1.484) -0.012 (-1339,1337) 
g? 
0.784 (-0.502,2.579) -0.019 (-1,836,1398) 
gf 
2.412 ( 0.509,5.742) -0.019 (-1.836,1.898) 
Dataset 2 g1 
0.782 
0.018 (-0.804,0.824) 
Dataset 3 g1 
2.372 
0.01 8 (-0.804,0.824) 
Kurtosis statistics 
Dataset 1 
9 2  -0.370 
-0.558 (-1.478,1.222) 
gf 
0.328 (-2.376,6.821) 0.048 (-2.432,6.016) 
g! 
1.547 (-2.132,9.262) 0.020 (-2.507,6.023) 
9; 
7.579 (-1.104,26.59) 0.020 (-2.507,6.023) 
Dataset 2 
9 2  
0.952 
-0.562 (-1.453J.149) 
Dataset 3 g2 
6,555 
-0.562 (-1.453,l. 149) 
0.568 
0.53 1 
0.034 
0.217 
0.000 
0.040 
0.291 
0.462 
0.034 
0.33 1 
0.000 
0.096 
aReplicated statistics are produced from p
p
 and represent expected skewness and kurtosis coe- 
ficients under the assumed model; values represent posterior means (and 95% posterior intervals). 

368 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
the same skewness and kurtosis. For the second dataset, p-values for both g 1 and g2 are 
equal to 0.034, indicating deviations between observed and expected data. For g 
and gf, 
p-values are much higher (0.22 and 0.33, respectively), indicating minor deviations from 
the assumptions of the model. Finally, for the last dataset, all p-values are small (lower than 
0.001 for g1 and g2, 0.04 and 0.096 for g? and gf, respectively) indicating that the data 
have shape statistics different from those for the corresponding measures of the assumed 
model. 
The same approach can be used for any distribution. Moreover, these checks are useful 
for indirectly checking the assumption of the residual's normality in normal models; see, 
for example, Spiegelhalter et al. (1996a, pp. 4546) and Section 10.5 of the current book. 
10.3.7 Checking the goodness-of-fit of a model 
For the evaluation of the goodness-of-fit of a model, x and the deviance measures defined 
in Section 10.1.2 and their corresponding p-values are the most frequent statistics used. 
Calculation of the x2 within WinBUGS can be attained by defining nodes chisq. obs and 
chisq. rep, which will calculate x2 (y, @") and x2 ( Y ' ~ P > ( ~ ) ,  
d')) in each iteration t of 
the MCMC algorithm. Their difference x 2  ( ~ ' " p , ( ~ ) ,  
dt)) 
- x2 (y, dt)) 
must be monitored 
as well as their corresponding posterior p-value, given by the posterior mean of a node 
p. c h i s q  defined as 
which is a binary variable taking value 1 when ~ ~ ( y ' " p , ( ~ ) ,  
O ( t ) )  > x2 (y, dt)) 
and zero oth- 
erwise. Similar to this is the syntax for specification ofthe deviance or other related function. 
Note that the deviance (for observed data) is calculated internally in WinBUGS (termed as 
deviance) and, therefore, we do not need to define it in the WinBUGS model code. 
Poor fit is indicated for models with p-values close to zero since the observed statistic 
will be far away from what is expected under the assumed model. 
Example 70.2 (continued): Goodness-of-fit measures for Manchester United 
data. For the simple Poisson Example 10.2, we calculate x 2  using the following syntax: 
In this syntax, nodes chisq. rep. vec and chisq. obs . vec are initially used to calculate 
the x2 elements for each observation involved in its summation. Then the inprod command 
is used to calculate the final x2 value for the observed values. Using similar logic, we can 
define the corresponding values for the deviance measure using the following syntax: 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
369 
Finally, we may consider as a goodness-of-fit measure the rescaled deviance, which is 
given by the deviance of the current model after removing the deviance of the saturated 
model, that is, the Poisson model mean equal to each observation yi. This is a measure of 
the distance between the model estimates and the data. The rescaled version of the deviance 
for the simple Poisson model is given by 
n 
Y i  
f ( Y i l N  
- 
i=l 
f ( Y i l X  = Yi) 
Z Z l  
Deviance'(y) 
= -2 c 
log 
- -2 2 ((Yi - A) + yi log - 
This measure cannot be calculated if at least one observation is equal to zero. To avoid this 
problem, in the calculation of the rescaled deviance we may substitute y i by yi + E ,  where E 
is a small positive constant (e.g., in the following computations we have used E = 10 -I3). 
Using this approach, we can define the rescaled deviance by the WinBUGS syntax 
For the Manchester United data, we have estimated posterior p-values 0.65 and 0.54 
for x2 and deviance, respectively, for the goal scored, while the corresponding values for 
the conceded goals are higher (0.82 and 0.60). The probabilities for the rescaled deviance 
discussed above are similar (p-values 0.55 and 0.90, respectively). None of the results 
indicate any problem concerning the goodness-of-fit of the model. Figures of the x 
and 
deviance differences for the two sets of data are given in Figures 10.11 and 10.12. 
Finally, we can use a x 2  statistic to compare the predicted and observed frequencies 
using the statistic 
t=1 

370 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
(a) Scored goals 
(b) Conceded goals 
Figure 10.11 Differences in x2 statistics for Manchester United data (Example 10.2). 
(a) Scored goals 
(b) Conceded goals 
Figure 10.12 
Differences in deviance statistics for Manchester United data (Example 10.2). 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
371 
where we have used the assumption of multinomial sampling since the size of vector n 
is fixed by design. The expected frequency of each y under the model is given by its 
probability function f(yl0). 
In order to calculate this measure and its corresponding p-values in WinBUGS , we first 
calculate the relative frequencies (see Section 10.3.1 for details) and then add the following 
commands: 
The p-values based on x2 for frequencies are equal to 0.76 and 0.26 for scored and 
conceded goals, respectively. The latter indicates that, for conceded goals, differences 
between observed and expected frequencies exist but are not so severe as to reject the 
simple Poisson model. 
Example 10.3 (continued): Goodness-of-fit measures for simulated normal 
data. For testing the simple normal model, we propose using x measures for comparison 
of the cumulative frequencies rather than the actual frequencies as in the Poisson example 
above. 
For this reason we may define a X2-based statistic measuring the difference between 
observed and expected frequencies for given cutpoints y (k = 1, . . . n' with y; > y;-l) 
of the assumed distribution. The x2 statistic is given by 
where FY;-l,,; 
= FY; - FY;_, 
is the observed relative frequency of observations with 
values between y; and ykPl and 
F(ybl0) = 0, F(y6,+110) = 1 and F,;,+l = 1 . 
This expression was based in the assumption that FY;-,,y; 
will follow multinomial distri- 
bution success probabilities given by the probability of y E (ykPl, yk] under the assumed 
model. For the normal model, we consider the k/(n' + 1) quantiles of the standardized 
normal distribution, denoted by Zk, and define y(, as yk = p + zk/&. 
The expression is 
simplified to 
2 
n'+l pp+urk 
- Fp+uzk-l -@(a) 
+ @(Y-l)] 
X ; ( Y , @ J )  
= n C 
k=l ( @ ( z k )  - @(zk-1)) (1 - @(zk) + @(zk-1)) 
' 

372 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
where (z) is the cumulative distribution function of the standardized normal distribution. 
An important drawback of this statistic is its sensitivity to the selection of cutpoints y;. 
Moreover, in the case where n' is large, then a considerable number of observed frequencies 
Fy;- .y; will be equal to zero. For this reason, this statistic requires large samples to enable 
the use of numerous cutpoints and for accurate estimation. 
To avoid the observed sensitivity, we may use a x2 statistic based on the cumulative 
frequencies. We expect the p-values resulting from the corresponding statistic to be more 
robust since Fyk will not degenerate to zero as the number of cutpoints n' increases. Hence 
this version of x2 can be calculated by 
i=l 
which will be simplified to 
Finally, in order to avoid the use of arbitrary cutpoints, we may construct a x statistic 
based directly on the cumulative frequencies 
[FLk - L'(E.,t18)]2 =n>: 
[FY% 
- F(YZ1")I2 
x$(y,O) = n x  
z=l Var(F,* 18) 
z=l F(Yzie) (1 - F(yzie)) . 
For the normal case that we consider here, the x 2  statistic will be given by 
Finally, the Kolmogorov-Smirnov statistic (KS) 
can be adopted in order to evaluate the difference between the predictive and the observed 
cumulative frequencies. 
The original x2 as defined in the previous section cannot be used here since in the normal 
model we fit both the mean and the variance, and hence the abovementioned measure will 
not be able to trace deficiencies of the model; see Section 10.5 for more details. 
P-values are summarized in Table 10.20 for all three datasets. Model code for x $(y, e), 
KS(y, 8), x;(y, 8, z), and x$(y, 8, z )  statistics is presented in Tables 10.21 and 10.22. 
No p-values indicate any important deviation between predicted and observed data. For the 
third dataset, specific p-values [based on x$(y, 8, z )  and KS] are relatively low (compared 
to the rest), indicating a possible problem in the overall fit of the model. Finally, in the 
current example, x2f(y, 8, z )  is quite sensitive to the selection of cutpoints failing to trace 
possible differences, while x$(y, 8, z )  is quite robust over a variety of different number of 
cutpoints. 

USING THE PREDICTIVE DISTRIBUTION FOR MODEL CHECKING 
373 
Table 10.20 
frequencies 
P-values for comparison of differences between predictive and observed 
x: (?A 0, 
x$(?A 0, r) 
X$(Y> 0) 
W V ,  0) 
,, = ga ,/ 
= 196 n/ = 39c 
n/ = ga 12/ = 1gb 12’ = 39c 
Dataset I 
0.834 
0.805 
0.904 
0.635 
0.642 
0.650 
0.603 
0.661 
Dataset2 0.737 
0.778 
0.839 
0.486 
0.527 
0.526 
0.590 
0.509 
Dataset 3 0.233 
0.464 
0.564 
0.118 
0.126 
0.128 
0.240 
0.144 
a 10% percentiles. 
b5% percentiles. 
‘2.5% percentiles. 
Table 10.21 
statistic 
WinBUGS code for computation of x$(y, 0) and Kolmogorov-Smirnov 

374 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Table 10.22 
WinBUGScode for computation of Fv;-l,,t 
(denoted by f), Fv; 
(denoted by 
0, 
X?(Y, 0, z )  and x;(Y, 0, 
%cut and zcut correspond to n’, J and are loaded via a data list format. 

USING CROSS-VALIDATION PREDICTIVE DENSITIES FOR MODELCHECKING, EVALUATION, AND COMPARISON 
375 
10.4 USING CROSS-VALIDATION PREDICTIVE DENSITIES FOR MODEL 
CHECKING, EVALUATION, AND COMPARISON 
All predictive diagnostics presented above have the disadvantage of double usage of the 
data. In this way, the predictive performance is overestimated. On the other hand, cross- 
validation offers a well defined tool to evaluate the predictive ability of a model. In this 
section we focus on the leave-one-out (CV-I) cross-validation. We illustrate estimation 
of the cross-validatory conditional predictive ordinate and discuss generation of random 
values from the appropriate CV-1 predictive distribution. After the simulation of such a 
sample, CV-I predictive diagnostics can be calculated using a procedure similar to that 
followed for the full posterior predictive distribution described in the previous sections. 
10.4.1 
Estimation of the cross-validatory predictive ordinate can be performed in a straightforward 
manner using the MCMC output generated from the full posterior density. This can be 
achieved by the property 
Estimating the conditional predictive ordinate 
Thus, we can estimate the inverse predictive ordinate CPOi from the sample mean of the 
inverse density/probability function evaluated at yi for each O ( t )  generated from the full 
posterior distribution. A Monte Carlo estimate for CPO i is then given by 
which is the harmonic mean of the density (or probability) distribution function evaluated 
at yi for each tl(t) for t = 1,2, . . . , T‘. Further details can be found in Gelfand and Dey 
(1994, p. 51 l), Spiegelhalter et al. (1996a, p. 41), and Gelfand (1996, pp. 154-155). 
The preceding estimate can be easily obtained in WinBUGS by defining a deterministic- 
logical node that will enable calculation of [ f (yi I ect))] . Its posterior mean provides an 
estimate for CPOzyl, and therefore an estimate of CPOi is directly obtained by the inverse 
of these quantities. Thus, having calculated node ppo as illustrated in Section 10.3.4, we 
only need to add the command 
-1 
in order to calculate its posterior mean and obtain an estimate ofthe inverse ofthe conditional 
predictive ordinate. 
Since, for discrete data, CPOi can be used to estimate the probability of observing yi in 
the future when you have already observed y \i, it has a direct interpretation. Comparison 
of CPOi with the corresponding relative frequencies estimated from y \i provides a better 
picture of the model’s predictive ability. 
For continuous data, computation of F ( Y ( ~ )  
Iyii) in a manner similar to that in Section 
10.3.4 is computationally expensive and is not recommended. 

376 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Outliers can be easily traced for large values of ICPOs. In order to have a feeling of the 
scale of such quantities, we note that the inverse density for the 0.05,0.02,0.01, and 0.005 
quantiles of the standardized normal are equal to 9.7, 20.6, 37.5, and 69.1, respectively. 
Assuming approximate normality, ICPO values larger than 40 can be considered as possible 
outliers and higher than 70 as extreme values. 
Example 10.2: Manchester United’s goals (continued). MCMC-based estimates 
ofCPOs for Example 10.2 are provided in Table 10.23 and Figure 10.13. Higher differences 
are observed for the conceded goals as in the comparison of the PPOs in Section 10.3.4. 
Table 10.23 
Estimated leave-one-out conditional predictive ordinates (CPO) and observed 
relative frequencies for scored and conceded goals in home games of Manchester United for 
premiership season 2006-2007 (Example 10.2)” 
0
1
2
3
4
 5 
~ 
Scored goals 
Observed relative frequencies 
Actual 
0.11 0.16 0.21 0.32 0.16 
0.05 
Leave-one-out 
0.06 0.1 1 0.17 0.28 0.1 1 
0.00 
Estimated CPO (inverse posterior mean) 0.08 0.21 0.26 0.20 0.12 
0.05 
Estimated ICPO (posterior mean) 
11.79 4.76 3.92 4.95 8.53 
18.78 
Observed relative frequencies 
Conceded goals 
Actual 
0.42 0.53 0.05 0.00 0.00 
0.00 
Leave-one-out 
0.39 0.50 0.00 - - - 
Estimated CPO (inverse posterior mean) 0.52 0.32 0.09 - - - 
Estimated ICPO (posterior mean) 
1.91 3.12 11.19 - - - 
aLeave-one-out observed frequencies are calculated after removing one observation with the corresponding 
value. 
1 rp 
0 
T 
4 
7 
T 
0 
1 
2 
3 
4 
5 
0 
1 
2 
3 
4 
5 
8, 
Y 
(a) Scored goals 
(b) Conceded goals 
Figure 10.13 
Estimated conditional predictive ordinates (solid lines) and leave-one-out observed 
relative frequencies (dashed lines) for scored and conceded goals in home games of Manchester 
United for premiership season 2006-2007 (Example 10.2). 

USING CROSS-VALIDATION PREDICTIVE DENSITIES FOR MODELCHECKING, EVALUATION, AND COMPARISON 
377 
Example 10.3: Simulated normal data (continued). Using the simple rule of 
thumb proposed above, in Table 10.24 we present values with ICPO> 10. Note that one 
value (y3) was indicated as a possible outlier in dataset 1, while in datasets 2 and 3 only 
~
2
0
 
was flagged as an extremely outlying value. Especially for the last dataset, ICPO 
is extremely large, indicating that this observation cannot be actually observed under the 
assumed model. We also present the corresponding inverse PPOs, which have similar but 
milder behavior. Using inverse PPOs we can trace 920 only in the last dataset, where the 
extreme value of 10 was added. 
Table 10.24 
each dataset of Example 10.3; only for values with ICPO> 10 
Estimated conditional and posterior predictive densities of extreme values for 
Dataset 1 
Dataset 2 
Dataset 3 
Observation 
y3 
y7 
Y3 
Y20 
Yzo 
Rank 
1 
19 
1 
20 
20 
Value 
-2.53 
2.76 
-2.53 
5.00 
10.00 
ICPO 
54.54 25.30 
13.21 275.50 
60700.0 
IPPO 
13.11 
9.41 
6.02 
27.13 
99.7 
CPO 
0.018 0.040 
0.076 0.004 
1.6 x 
PPO 
0.076 0.106 
0.166 0.039 
0.0 10 
10.4.2 Generating values from the leave-one-out cross-validatory 
predictive distributions 
Generation of values from the leave-one-out cross-validation densities f (y i 1qi) can be 
achieved in a straightforward manner by generating values from the corresponding full 
predictive densities with data y\i. Thus we need to rerun n times the MCMC algorithm 
(and the corresponding WinBUGS code) for all possible leave-one-out datasets y \i (for 
i = l , 2  ,..., n). 
Gelfand (1 996) described an alternative approach in order to generate a sample (8’(t); 
t = 
1,2, . . . , T’) from the leave-one-out posterior density f ( O l ~ \ ~ ) .  
He proposed resampling 
with replacement from the MCMC sample (dt),; 
t = 1 , 2 , .  . . , T’) generated from the 
posterior distribution. For each 8(‘) the probability of selection w t  
is given by 
Usually random variables Yi are assumed to be independent given the parameter vector 8. 
Thus, the resampling weights are simplified to 

378 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Unfortunately, this approach cannot be performed within WinBUGS in a straightforward 
manner. Hence, in order to implement this approach, we need to generate a sample O ( t )  
from the full posterior distribution using WinBUGS , export the data in another statistical 
program, and finally perform the resampling as well as further calculations in this program. 
Nevertheless, the abovementionedunnormalized weights can be provided by the icpo node 
defined in the previous section. Once we have generated samples from the corresponding 
leave-one-out density, we can calculate corresponding residuals or statistics for the imple- 
mentation of other predictive checks. 
Although the leave-one-out cross validation approach avoids double usage of the data, 
we frequently use the full posterior predictive distribution which is a good approximation 
of the leave-one-out distribution (Carlin and Louis, 2000, pp. 205-206), provided that the 
size of data is not small. In this way we avoid extensive computation, making predictive 
inference straightforward. 
10.5 ILLUSTRATION OF A COMPLETE PREDICTIVE ANALYSIS: NORMAL 
REGRESSION MODELS 
In this section we focus on implementation of the predictive model checks in normal re- 
gression models. When fitting normal models, we are interested in checking for 
1. The structural assumptions of the model (independence, normality, and homoscedas- 
ticity of errors) 
2. Possible outliers or observations that are rarely observed from the assumed model 
3. The goodness-of-fit of the model 
All the proposed checks are illustrated using Example 5.1. 
10.5.1 Checking structural assumptions of the model 
The structural assumptions of the model are the independence of errors, normality, and 
homoscedasticity (i.e., constant error variance across all observations). In order to identify 
problems concerning these assumptions, we use Bayesian p-values based on measures used 
in classical significance tests. 
The independence of errors can be checked using the Durbin-Watson statistic 
which is essentially an estimate ofthe first order autocorrelation (i.e., for lag equal to one) of 
the errors. In order to be meaningful, observations must be arranged in a chronological order 
of collection. In this way we can identify dependencies due to the design or the sampling 
schme of the study; for example, sequential sampling of people visiting a specific place 
may result in collecting data from different members of the same family, which induces 
dependencies in the sample that are due to similar beliefs. 
Normality can be checked by 
1. Using the approaches proposed in Section 10.3.6, that is, using various versions of 
x2 statistics and the KS statistic. 

ILLUSTRATION OF A COMPLETE PREDICTIVE ANALYSIS: NORMAL REGRESSION MODELS 
379 
2. Testing for skewness and symmetry of the errors as described in Section 10.3.7. 
3. Calculating the number of observations with standardized residual values outside 
intervals ( - 2 , 2 )  and (-3,3) and compared with the 5% and 1% expected under the 
normality assumption. In this way, we may identify possible problems at the tails of 
the distribution. 
4. Visual evaluation of 
a. The 95% error bars of the cumulative frequencies F r e p  of the replicateapre- 
dictive values versus the observed cumulative frequency Fi for each residual 
value (as in Section 10.3.2), where 
l n  
l n  
$yp = - 
I(rTP 5 ri) and 
= - 
I(rk 5 T i ) .  
n 
n 
i=k 
i=k 
b. The 95% error bars of the ordered predictive standardized residual values T irr 
versus the posterior means of ordered standardized residuals T ( i )  (as in Section 
10.3.3). 
These measures focus on different aspects of the fitted distribution and thus may identify 
different problems concerning the normality of errors. 
Finally, to test homoscedasticity of errors, we propose dividing the sample into equal 
parts on the basis of covariates or the estimated mean in each iteration. Then variances 
must be calculated within each subsample and compared with the overall variance using 
the Levene test for the equality of variances. Hence we use the test statistic 
where ri = yi - pi is the residual for the i observation, T, is the mean residual value for 
the g group, K is the number of subsamples into which we have divided our data, z, is 
the mean of the gth subsample, and gi is the subsample indicator for the ith subject. We 
propose using four samples (split in the quantiles of the generated mean) or more if the 
sample size allows for further spliting. 
10.5.2 Detailed checks based on residual analysis 
Residual analysis can be performed in much the same way as described in Section 10.3.5. 
Additionally, PPO and CPO can be used to identify problematic cases. We can use the 
following techniques for the residual values: 
1. Error bars of the cumulative frequencies F r e p  of the replicatedpredictive values vs. 
the observed cumulative frequency Fi for each residual value (as in list item 4a in 
Section 10.5.1). 
2. Error bars of the ordered predictive standardized residual values T;:; 
versus the 
posterior means of ordered standardized residuals T ( i )  (as in list item 4b in Section 
10.5.1). 
3. Error bars of standardized residuals versus the observation index. 

380 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
4. Error bars of standardized residuals versus the posterior means of parameters ,u i or 
values of the explanatory variables. 
5 .  Calculation of the probability of more extreme observations. 
6. Calculation of PPO and CPO. Study values with ICPO> 10 and consider as possible 
outliers observations with ICPO> 40. 
All these techniques can be used to trace residuals and evaluate the overall fit of the model. 
To be more specific, calculations 5 and 6 focus on the identification of outliers while plots 
of items 1 and 2 evaluate normality, a plot of item 3 identifies autocorrelated errors, while 
a graph of item 4 traces problems of heteroscedasticity. 
10.5.3 Overall goodness-of-fit of the model 
In order to check the overall goodness-of-fit, we can calculate the R measures (as defined in 
Section 5.2.3) to identify the decrease of the error variance due to the explanatory variables 
included in the model. A rule of thumb indicates that models with acceptable predictive 
ability must have R2 higher than 0.7. 
A simple check of the importance of each model coefficient can be performed using the 
posterior tail area probability of value; hence we calculate the quantity 
Low values of this tail area probability indicate that the corresponding posterior distribution 
is far away from zero. Therefore the corresponding coefficient will be important for the 
model. This approach is only indicative and by no means should be considered as a formal 
model comparison or variable selection method. Such methods are briefly presented in the 
next chapter. 
10.5.4 Implementation using WinBUGS 
Calculation of the residual values: Standardized residual values based on actual and 
predictive values are calculated using the following syntax: 
Most of the model checks are based on standardized residual values. 
Calculation of the Durbin-Watson statistic: The DW statistic and the corresponding 
p-value can be calculated in WinBUGS using the following syntax 

ILLUSTRATION OF A COMPLETE PREDICTIVE ANALYSIS: NORMAL REGRESSION MODELS 
381 
where r [il and r . rep Cil are the standardized residuals calculated using the actual and 
predicted values. 
Calculation of the Levene statistic: Calculation ofthe Levene statistic is more complicated 
than the previous statistics. In the data format we define the number of groups using the 
deterministic node K and the ranks ofthe cutpoints in a vector node ranksK. In the following 
syntax we first calculate the ranks of pi (ranksmu), then the group membership of each 
observation (group [il ),and finally the means in each group (group. index) using a binary 
matrix. Each group mean is then calculated using the equation 
where Gik and gi correspond to the WinBUGS nodes group. index Ci , kl and group [il , 
respectively. The WinBUGS code for calculation of this statistic is given in Table 10.25. 
Calculation of the proportion of extreme residual values: Calculation of the percent- 
age of standardized residual values that lie outside an interval of type (-z, z )  are easily 
calculated using the expression 
n 
n 
P(' @ (-2,z)) 
= CI('z" 
5 -z) + CI('2" 
2 z )  
i = l  
i=l 
n. 
n 
= 1 
I (  - z - ',SO 2 0) + C I(?-% - z 2 0) 
i=l 
i= 1 
The binary indicator function I(" 2 0) corresponds to the WinBUGS command step(x). 
Hence, in WinBUGS we first calculate each element of the summation (10.7) using the 
syntax 
and then the proportion of residuals that lie outside the desired interval by 
Under the normality assumption, we expect 5% and 1% to lie outside intervals (-2,2) 
and (-3,3). To make the comparison more general, we can also calculate the corresponding 
proportion of the replicated residuals ',rep 
under the assumed model and calculate the 
corresponding p-value as usual. 
The full code for the intervals (-2,2) and (-3,3) is as follows: 

382 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Table 10.25 
WinBUGS code for implementation of Levene’s statistic. 

ILLUSTRATION OF A COMPLETE PREDICTIVE ANALYSIS: NORMAL REGRESSION MODELS 
383 
10.5.5 An Illustrative example 
Example 10.4. Model checking for the soft drink delivery times data (Example 
5.1 continued). Returning to the normal regression example (Example 5.1), tail 
area probabilities of the model checks suggested above are presented in Table 10.26. 
The proportion ofresidual values lying outside (-2,2) and (-3,3) intervals is given 
in Table 10.27. Figures 10.14 and 10.15 provide information concerning individual 
residual values as well as the overall fit of the model. Figure 10.16 offers a visual 
representation of individual residual values. Possible outliers with ICPO> 10 are 
given in Table 10.28, while R2 
values are listed in Table 10.29. 
Table 10.26 Posterior p-values and tail area probabilities for Example 10.4 
Assumption 
Statistic 
Node 
p-value 
Independence of errors 
Homoscedasticity 
Normality 
Durbin-Watson 
Levene (5 groups) 
X2-based tests 
Kolomogov-Smimov 
Symmety 
Skewness coefficient 
Kurtosis 
Kurtosis coefficient 
Tail area probabilities 
PO 
P1 
Pz 
dw.p 
levenes . p 
chisq. F . p 
chisq.F2.p 
chisq.f . p  
ks.p 
m3.p 
m4.p 
p. beta0 
p. beta1 
p. beta2 
0.970 
0.077 
0.325 
0.374 
0.236 
0.303 
0.497 
0.4 19 
0.979 
1.000 
0.999 
Concerning the normality assumption, no important problems exist since all correspond- 
ing p-values are reasonably high. The p-values for testing for symmetry and the mesokur- 
ticity of the distribution are very close to 0.5 indicating no major deviations from these 
two assumptions. This is similar to the scenarios in Figures 10.14 and 10.15, where small 
deviations are observed between expected and observed values. 
The proportion of observations outside (-2,2) is found equal to 5.3%, which is very 
close to the expected one (p-value 0.622), indicating no problem concerning the 95% tail 
area probability. Regarding interval (-3,3), 0.63% observations is found outside this 
interval, which is much higher than the corresponding expected percentage (p-value 0.853), 
possibly indicating an outlier. 
The Durbin-Watson posterior p-value is high (0.97), indicating strong error autocorrela- 
tions (DW posterior mean 1.17, expected posterior mean under the assumed model, 1.92). 

384 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
- 1  
T 
I 
I 
i 
1 
-2 
-1 
0 
1 
2 
Posterior mean of residuals T 
Figure 10.14 
posterior means of ordered standardized residuals T ( % )  for Example 10.4. 
95% posterior error bars of ordered predictive standardized residuals r;:; versus 
0.2 
0.4 
0.6 
0.8 
1 .o 
Posterior means 01 cumulative frequencies 01 residuals 
Figure 10.15 
frequencies Fi for Example 10.4. 
95% posterior error bars of cumulative predictive frequencies -E?,'ep versus cumulative 
Table 10.27 
p-values for Example 10.4 
Proportion of residuals outside (-2,2) and (-3,3) and their corresponding 
node 
mean 
sd 
MC error 2.5% median 97.5% start sample 
p95.obs 0.05324 0.04643 0.00111 0.0 0.04 
0.2 
1001 1000 
p95.rep 0.04824 0.04259 0.001294 0.0 0.04 
0.16 1001 1000 
p95.p 
0.622 
0.4849 
0.01389 0.0 1.0 
1.0 
1001 1000 
p99.rep 0.00296 0.01062 3.483E-4 0.0 0.0 
0.04 1001 1000 
p99.p 
0.853 
0.3541 
0.01137 0.0 1.0 
1.0 
1001 1000 
p99.obs 0.00632 0.01459 4.838E-4 0.0 0.0 
0.04 1001 1000 

ILLUSTRATION OF A COMPLETE PREDICTIVE 
ANALYSIS: NORMAL REGRESSION MODELS 
385 
The corresponding picture of possible sequential correlations is depicted in Figure 10.16a 
where we can spot batches of residuals with similar values. Nevertheless, such checks may 
be nonsense if the ordering of the collected observations has no sequential or time related 
interpretation. 
Similar problems are observed in the homogeneity of variance diagnostics. Levene 
statistic posterior p-value, after splitting the sample in five groups (of five observations each) 
according to the order of parameters pi, is equal to 0.077, suggesting possible differences 
in the residuals’ dispersion. The picture is similar in Figure 10.16b, where the cluster of 
the last five observations has clearly higher dispersion than do the remaing values. 
“1 
T 
‘1 
5 
10 
15 
20 
25 
observation index 
10 
20 
30 
40 
50 
MI 
70 
Posterior means of linear predictors ( p )  
(a) Observation index 
(b) Linear predictor pi 
Figure 10.16 
and (b) posterior mean of 11% for Example 10.4. 
95% posterior error bars of standardized residuals r,“ versus (a) observation index i 
Finally, individual residual checks, have indicated 13 observations (about 52% of the 
total observations) with inverse conditional predictive ordinate higher than the threshold 
value of 10 (ICPO> 10). The large number of observations is alarming, indicating possible 
problems in the normality of error assumption. Nevertheless, only observations 20 and 9 
are spotted with extremely large values (> 40), equal to 58.3 and 2703, respectively. The 
latter can certainly be considered as an outlier according to the values of all diagnostic 
checks presented in Table 10.28 (posterior mean of standardized residual 2.24, probability 
of more extreme 0.033, IPPO = 52.2). 
Finally, R2 values are high, close to 1.0, indicating a reasonably good fit of the model. 
Moreover, tail area probabilities P(pj > 0) (j = 0,1,2), given in Table 10.29, are high, 
indicating that the posterior distributions of all parameters are far away from zero. 
To summarize, small deviations from normality are present, and autocorrelated and 
heteroscedastic errors might be more appropriate for this model. Two values are traced 
as possible outliers, and one of them has extremely high diagnostic values. Finally, the 
overall fit of the model is good, with both explanatory variables having large impact on the 
response variable Y (delivery time). Possible use of the logarithms of the delivery time 
might correct problems in heteroscedasticity of errors and improve the fit of the model. 
Checks for independence may not have any practical meaning since the order of the values 
may not represent any actual time sequence. 

386 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
Table 10.28 Individual residual statistics for Example 10.4" 
Observation Standardized 
index(i) 
residual(r:) 
PEO 
PPO 
CPO IPPO 
ICPO 
19 
0.54 
0.294 0.100 0.096 
9.97 
10.42 
11 
0.67 
0.268 0.093 0.088 10.76 
11.37 
10 
0.72 
0.247 0.088 0.077 11.38 
13.03 
21 
-0.80 
0.207 0.084 0.074 11.96 
13.56 
18 
1.04 
0.149 0.069 0.062 14.48 
16.24 
23 
-1.40 
0.102 0.045 0.040 22.23 
24.90 
24 
-1.39 
0.112 0.047 0.035 21.27 
28.45 
4 
1.49 
0.078 0.041 0.032 24.34 
31.07 
22 
-1.13 
0.183 0.063 0.032 15.81 
3 1.07 
1 
-1.53 
0.077 0.039 0.028 25.61 
35.16 
9 
2.24 
0.033 0.019 0.000 52.22 
2703.00 
20 
-1.76 
0.049 0.028 0.017 35.44 
58.34 
aValues with ICPO> 10 only are presented. 
Key: PEO = probability of more extreme observation given by rnin{pr, 1 - p : }  with 
p: = P(Y,'"" > l'ly)); IPPO = inverse of PPO (posterior predictive ordinate); ICPO = 
inverse of CPO (conditional predictive ordinate). 
Table 10.29 Posterior summaries for R2 measures for Example 10.4 
node 
mean 
sd 
MC error 2.5% 
median 
97.5% start sample 
RB2adj 
0.9516 0.01649 
6.66E-4 
0.9091 0.9547 0.9745 
1001 1000 
RB2 
0.9556 0.01512 
6.105E-4 
0.9166 0.9585 0.9766 
1001 1000 
10.5.6 Summary of the model checking procedure 
In this section, we have illustrated the full procedure for predictive model checking in 
normal models as presented in Section 10.5. It is evident that a wide variety of approaches 
is available for model checking. For this reason it is essential to summarize them as a brief 
guideline for future use: 
1. Select appropriate measures and check the structural assumptions of the model (see 
Section 10.3.6). 
2. Perform outlier analysis using residual checks and by examining extreme PPOs and 
CPOs (see Sections 10.3.4, 10.3.5, and 10.4.1). 
3. Check the overall goodness-of-fit of the model using an appropriate measure (see for 
examples in Section 10.3.7). 
4. Revise the model if checks 1-3 indicate that the model is not valid (and rerun the 
Since the procedure is time-consuming, we suggest that users identify which measures 
they prefer and try to always implement the same measures for checking the model. Graph- 
previous analysis). 

DISCUSSION 
387 
ical representations should also be performed especially in the residual analysis or when 
comparing the goodness-of-fit of individual values, since they offer valuable information. 
Finally, cross-validation measures (such as CPO described in this chapter) must also be 
reported when the sample size is relatively small. 
10.6 DISCUSSION 
In this chapter, we have described the importance and the practical implementation of the 
predictive distribution in Bayesian inference using WinBUGS . First, predictions for fu- 
ture response data and estimation of missing response values are illustrated in detail using 
simple but descriptive examples. The use of the predictive distribution for checking the 
model fit and for the identification of outliers or surprising observations is further illus- 
trated. Posterior p-values and residual analysis have been also described in detail, while the 
leave-one-out cross-validation predictive distribution has also been discussed. The chapter 
closes with a detailed implementation of the described approaches in normal regression 
models. 
A further interesting area for implementation of predictive inference is in problems 
related to the evaluation and construction of performance ranking tables. In such cases 
we are interested in constructing league and ranking tables, which are particularly useful 
for the evaluation of institutions (hospitals, universities), individuals (students, athletes), or 
athletic teams. The predictive distribution can be easily used to rank institutions according 
to the collected data or even predict the future ranking in a sports contest; see Karlis and 
Ntzoufras (2008) for an implementation in association football (soccer) data. 
The predictive checks described or proposed in this chapter are by no means exhaustive. 
Many additional approaches exist in Bayesian literature since it is a highly active area 
of research. Each Bayesian researcher can use different statistics to check whether an 
assumption of interest is plausible under the light of the observed data. Moreover, the 
calibration of posterior p-values and related predictive checks is a very important issue that 
needs further investigation since no clear threshold points exist indicating low and high 
values (Sellke et al., 2001). Another interesting approach is the idea of predictive checks 
using the MCMC output of different models running in parallel. In this way, comparison 
of two models ml and m2 is feasible since we can check the distribution of a statistic when 
either of the two models under consideration are true; more details can be found in Congdon 
(2005b, 2006). 
Furthermore, predictive distributions have also been used for model comparison or se- 
lection. Extensive work on the topic has been published by Proffessor Ibrahim and his 
associates; see Ibrahim and Laud (1 994), Laud and Ibrahim (1 995), Hoeting and Ibrahim 
(1 998), and Meyer and Laud (2002) for some indicative examples. Decision theoretic ap- 
proaches to model selection and averaging have been also illustrated by Guti errez-Peiia and 
Walker (2001) and Walker et al. (2001), respectively. 
In the next and final chapter, we focus on the comparison of different models rather 
than checking for the adequacy of a single model. The purely Bayesian approach is briefly 
described by introducing the notions of prior predictive distribution, Bayes factor, and pos- 
terior model probabilities. Bayesian variable selection using MCMC and model averaging 
are also described and illustrated using a simple example. Finally, we focus on three in- 
formation criteria: the Akaike (AIC), the Bayesian (BIC), and the deviance (DIC) criteria. 
We illustrate them using simple examples in WinBUGS . 

388 
THE PREDICTIVE DISTRIBUTION AND MODEL CHECKING 
1989 
1990 
1991 
A 1992 
1993 
1994 
1995 
Problems 
10.1 
Let us consider the number of outstanding claims counts (i.e., contracts with pending 
claims) for Example 10.1 of Section 10.2.2. 
B 
Partial 
Year 
2 
3 
4 
5 
6 
7 11 
total 
527,003 183,260 
90,539 
50,471 37,875 10,110 21,165 
594,059 237,289 
99,640 
52,390 50,723 39,028 
810,593 257,122 
87,318 
72,538 81,336 
1,012,181 339,664 156,531 181,253 
1,459,202 448,651 188,698 
1,689,751 563,683 
1,698,534 
920,423 
1,073,129 
1,308,907 
1,689,629 
2,096,55 1 
2,253,434 
1,698,534 
Schaurnburg, 
a) Consider a Poisson log-linear model with linear predictor similar to the model 
used in Section 10.2.2 to estimate the number of contracts that will be paid in the 
missing part of the triangle. 
b) The last column (total) contains the total number of outstanding claims. Use this 
information to model the claims counts assuming a multinomial distribution. 
Check whether the fitted distributions are appropriate for the data of Problem 8.1. 
Check whether the fitted distributions are appropriate for the data of Problem 8.6. 
Check whether the Poisson distribution is appropriate for the data of Problem 8.7. 
Perform residual analysis for the models for 
a) Problems 5.1 - 5.8 of Chapter 5. 
b) Problems 6.3 - 6.9 of Chapter 6. 
c) Problems 7.1 - 7.10 of Chapter 7. 
d) Trace outliers and extreme values. 
e) Check whether residuals have a normal (or approximately normal) distribution. 
Check whether the assumptions of the normal model hold for 
10.2 
10.3 
10.4 
10.5 
10.6 
a) Problems 5.1 - 5.8 of Chapter 5. 
b) Problems 6.3 - 6.9 of Chapter 6. 
Implement goodness-of-fit tests for the models implemented in Problems 7.1 - 7.10 
of Chapter 7. 
Estimate the cross-validatory predictive measures for the data of Problems 8.1 and 
8.6. 
10.7 
10.8 

CHAPTER 11 
BAYESIAN MODEL AND VARIABLE 
EVALUATION 
11.1 PRIOR PREDICTIVE DISTRIBUTIONS AS MEASURES OF MODEL 
COMPARISON: POSTERIOR MODEL ODDS AND BAYES FACTORS 
Bayesian comparison of two competing models m l  and m2 (or the corresponding hy- 
potheses H1 and H2) is performed via the posterior model probabilities f ( m  k I y) and their 
corresponding ratio 
which is termed the posterior model odds of model m 1 versus model m2. B12 is the Bayes 
factor of model ml versus model m2 defined as the ratio of the “marginal” likelihoods 
f(ylm1) and f(ylm2). Hence we can summarize the preceding expression by 
Posterior model odds = Bayes factors x prior model odds, 
whereprior model odds is the ratio of the prior model probabilities f ( m  1) and f(m2). The 
marginal likelihood f(ylm) form E {ml, mz} is given by 
where f(yl8,, m) is the likelihood of model m with parameters 8, and f(0,lm) is the 
prior of 8, under model m. This definition of the marginal likelihood coincides with the 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
389 

390 
BAYESIAN MODEL A N 0  VARIABLE EVALUATION 
prior predictive density as defined in the previous chapter; see Eq. (1 0.1). Hence the Bayes 
factor is the ratio of the prior predictive densities under the compared models. Kass and 
Raftery (1995) call f(ylm) as thepredictiveprobability of the data under model m, that is, 
the probability of obtaining the actually observed data before any data are available under 
the assumption that model m is the true stochastic mechanism generating the observed data. 
The Bayes factor is of prominent importance within Bayesian theory since equal prior 
model probabilities are considered as a default choice when no information is available con- 
cerning the model structure. Hence, model comparison and model evaluation are frequently 
based solely on Bayes factors. If we consider the model comparison as a hypothesis testing 
problem where interest lies in evaluating the null hypothesis Ho (corresponding to a model 
mo) against the alternative H1 (corresponding to a model m I), then both the posterior model 
odds PO10 and the corresponding Bayes factor Blo evaluate the evidence against the null 
hypothesis, which is familiar to classical significance tests. On the other hand, PO 01 and 
Bol evaluate the evidence infavor of the null hypothesis, which is not feasible in classical 
significance tests. To summarize, using posterior model odds and Bayes factors, we can 
0 Evaluate the evidence in favor of the null hypothesis 
0 Compare two or more non-nested models 
0 Draw inferences without ignoring model uncertainty 
0 Determine which set of explanatory variables gives better predictive results 
Suggested interpretation of the Bayes factor is provided by Kass and Raftery (1995); see 
also Table 1 1.1. 
Table 11.1 Bayes factor interpretation according to Kass and Raftery (1995) 
log(Bl0) Blo 
Evidence against Ho 
0- 1 
1 - 3 
Negligible 
1 - 3  
3 - 20 
Positive 
3 - 5  
20-150 
Strong 
> 5  
> 150 
Verystrong 
Note that the integral involved in computation of the prior predictive density and Bayes 
factors is analytically tractable only in certain restricted examples, and therefore asymptotic 
approximations or Monte Carlo methods are frequently used instead. Moreover, these den- 
sities are sensitive to the dispersion ofthe prior distributions f(e,, l m k ) .  As a consequence, 
when large prior dispersion is specified for parameters that differ across compared models, 
then more parsimonious models are supported irrespective of which data we observe. For 
the same reason, flat improper prior distribution cannot be used since the integrals involved 
in the computation of Bayes factors are not tractable (i.e., tend to infinity). These prob- 
lems are widely known as the Lindley-Bartlett or Jeffreysparadox (Lindley, 1957; Bartlett, 
1957). Posterior model odds and Bayes factors cannot be generally calculated within Win- 
BUGS unless sophisticated approaches are used. 
The difficulties described above have initiated a wide discussion concerning the use 
of Bayes factor in Bayesian inference [see, e.g., Gelfand (1996)], resulting in numerous 

SENSITIVITY OF THE POSTERIOR MODEL PROBABILITIES: THE LINDLEY-BARTLETT PARADOX 
391 
versions of Bayes factor as well as different alternative approaches for model comparison 
and checking. Some of the most popular Bayes factor versions developed are the posterior, 
fractional, intrinsic and pseudo Bayes factors; see Aitkin (1991), O’Hagan (1999, Berger 
and Pericchi (1996a; 19966), and Geisser and Eddy (1979), respectively. 
11.2 SENSITIVITY OF THE POSTERIOR MODEL PROBABILITIES: THE 
LINDLEY-BARTLETT PARADOX 
Lindley (1957) reported a surprising behavior of the Bayes factor and, consequently, of the 
posterior odds. Let us consider the model Yi - N(0, u2) with u2 known in which we wish 
to test the following simple hypotheses: Ho : I9 = 00 versus H I  : I9 # 00. Let us consider 
the usual normal prior distribution for 0 under the null hypothesis centered around the value 
ofthe alternative Ho. Hence we consider f(0lH1) N N(&, u,”). Then the posterior model 
odds are given by 
The resulting posterior odds depends on the sample size n and@ Lindley considered samples 
being at the limit of rejection area of the usual significance test of lOOq% significance level. 
In these samples, ?j 
= 00 f 
~ ~ / ~ a / f i ,  
resulting in posterior odds equal to 
where zq is the q quantile of the standardized normal distribution. From here, we can 
observe that when n increases, then the posterior odds also increase and tend to infinity 
for a given significance level q supporting the simpler hypothesis in contrast to classical 
significance tests, which reject the null hypothesis for sufficiently large n. This leads to 
a paradox since for sufficiently large samples, Bayesian methods and significance tests 
support different hypotheses. 
Bartlett (1957) extended this paradox by observing that the prior variance used in the 
Bayes factor also affects the supporting hypothesis. He observed that the posterior Bayes 
factor in favor of Ho is sensitive to the magnitude of the prior variance a;. This behavior 
also appears for the Bayes factors of any two nested modelslhypotheses, in which case 
results are sensitive to the variances of the additional parameters. This phenomenon is of 
much concern since the usual improper priors cannot be determined because of unknown 
constants involved in the computation of posterior odds while large variance priors fully 
support the simplest model. For these reasons, improper priors cannot be used and a large 
prior variance must be carefully selected in order to avoid activating the paradox described 
above. Although Bartlett (1957) noted this behavior, the term “Lindley’s paradox” is used 
for any case where Bayesian and significance tests result in contradictive evidence (Shafer, 
1982). The term “Bartlett paradox” was used by a few researchers such as Kass and Raftery 
(1 999, while others refer to this phenomenon as “Jeffreys paradox” (Lindley, 1980; Berger 
and Delampady, 1987) or “Jeffreys-Lindley’s paradox” (Robert, 1993). Lindley ’s paradox 
is discussed in detail by Shafer (1982). 
Although Lindley (1993) noted that the sensitivity of the Bayes factor is natural, this 
drawback resulted in a series ofpublications that attempted to resolve the problem by finding 
good reference priors for model selection. In this category we may include Bayes factor 

392 
BAYESIAN MODEL AND VARIABLE EVALUATION 
variants (posterior, fractional and intrinsic); see Aitkin (1 991), O'Hagan (1 995), and Berger 
and Pericchi (1996~; 1996b), respectively. 
11.3 COMPUTATION OF THE MARGINAL LIKELIHOOD 
Various of alternative methods have been proposed in the literature in order to accurately 
estimate or approximate the marginal likelihood. Here we briefly present the most important 
ones and focus on attention to the simpler ones that can be obtained via WinBUGS . Detailed 
review of the related methods can be found in Kass and Raftery (1 995) and Gamerman and 
Lopes (2006, chap. 7). 
11.3.1 Approximations based on the normal distribution 
The most popular approximation of the marginal likelihood is the so called Laplace approx- 
imation based on the normal distribution. This approximation results in 
f(Ylrn) = ( 2 J 4 d m / 2  1 %  
""(Yl5,, 
m ) f ( k J m ) ,  
(11.2) 
where 5, is the posterior mode of the parameters of model m and Ern = (H,(G,)) 
, 
where Hm(Bm) is equal to the minus of the second derivative matrix of the log-posterior 
density log f ( e ( y ,  rn) evaluated at the posterior mode 5,. 
To avoid analytic calculation of C, and Om, we may use the Laplace-Metropolis es- 
timator proposed by Raftery (1 9964 and Lewis and Raftery (1 997). Using this approach, 
we estimate 5, and 5, from the output of a MCMC algorithm by the posterior mean 
and variance-covariance matrix of the simulated values, respectively. Hence the Laplace- 
Metropolis estimator is given by 
-1 
I 
- 
f(Ylrn) = f?(Ylrn) = (2n)d,!21S?,L)1/2f(YleTILi 
m)f(3rnlm) 
(1 1.3) 
Note that this approximation works efficiently when the posterior distributions are symmet- 
ric. For this reason, substituting the posterior mode by the corresponding mean will not 
affect results. 
11.3.2 Sampling from the prior: A naive Monte Carlo estimator 
Since the marginal likelihood is given by 
f(Ylrn) = 1 
f ( Y l e m ,  m)f(eTTlm)dem, 
a straightforward estimate can be provided by 
m 

COMPUTATION OF THE MARGINAL LIKELIHOOD 
393 
Note that in this last equation, we use a sample from the prior distribution denoted by 8 2‘) 
(fort = 1, . . . , T )  instead of the usual posterior sample denoted by 8:) (fort = 1, . . . , T). 
Unfortunately, this estimator is quite inefficient, especially when the prior distribution 
f(O,,/m) considerablydiffers from the posterior f(O,ly, m), for example, when the prior 
is relatively flat in comparison to the posterior. In such a case, the likelihood values for the 
majority of the generated (from the prior) values 8 will be almost zero and hence minimal 
contribution to the summation of the estimate above. This results in large standard errors 
of the estimate and very slow convergence to the true value (Kass and Raftery, 1995). 
11.3.3 Sampling from the posterior: The harmonic mean estimator 
We can construct a simple Monte Carlo estimate with values simulated from the posterior 
distribution by considering the following equation: 
Hence an estimate can be obtained on the basis of the harmonic mean of the likelihoods 
calculated in each step of an MCMC algorithm given by 
This estimator, introduced by Newton and Raftery (1 994), is called the harmonic mean 
estimator. Although this estimator is simple, it is quite unstable and sensitive to small 
likelihood values and hence is not recommended; see Raftery (1 9966, p. 169) and Raftery 
et al. (2007) for discussions on the efficiency of this estimator. Raftery et al. (2007) have 
proposed methodology for improving the efficiency of this estimator. 
The estimator derived above can be extended to Gelfand and Dey’s (1994) generalized 
harmonic mean estimator, which is based on the following identity: 
HenceanestimatorisgivenbytheharmonicmeanofW = g(8,)/ [f(ylem, rn)f(Omlm)] 
evaluated over values 8:), . . . , O K )  generated from the posterior distribution using an 
MCMC algorithm, and hence the generalized harmonic mean estimator is given by 
A special case of this estimator is the harmonic mean estimator for g ( 0 , )  = f(8,Im). 
The “importance” density g must be selected carehlly and must be close to the posterior 
density. In such a case, inflation of the components of the summation involved in the 
estimator derived above is avoided since small likelihood values will be moderated by 

394 
BAYESIAN MODEL AND VARIABLE EVALUATION 
low small values of the importance density. A multivariate normal or t distribution with 
mean and variance set equal to the posterior mean and variance (estimated from an MCMC 
algorithm) usually provides an accurate estimate. 
11.3.4 
Importance sampling estimators 
Sampling from an arbitrary importance sampling density g(e) results in the estimate 
since 
When the sampling density is known up to a constant [i.e., g(8 m )  = C g* (6,) oc g* (em)], 
then the estimator is slightly changed to 
&(Yim) = 
since 
Different selection of the importance sampling density results in different estimates: the 
naive estimator for g(8,) 
= f(8,lm) (importance density = prior), the harmonic mean 
estimator for g(8,) 
= f(e,/y, m) (importance density = posterior), and Newton and 
Raftery’s(1994)estimatorforg(Bm) = .1~f(e,lm)+(l-w)f(8,/y,m) for0 < w < 1 
(importance density = mixture of the prior and the posterior density). 
11.3.5 Bridge sampling estimators 
Another effective Monte Carlo estimator ofthe marginal likelihood was introduced by Meng 
and Wong (1 996) on the basis of a simulation sampling scheme called bridge sampling. 
Using this approach, we can write 
s h ( 8 m ) f ( e m l m ) f ( Y l e m ,  m ) g ( ~ m ) d & l  
s h(eTrL)g(em)f(evL 
IY, m)dern 

COMPUTATION OF THE MARGINAL LIKELIHOOD 
395 
Eg(e) { h(em)f(8mlm)f(~lern, 
m ) }  
~f (em, ,,) 
{ h (8, )g (8, ) } 
f(Yb) = 
? 
where g(f3,) is a proposal distribution and h(8,) is an arbitrary bridge function. A Monte 
Carlo estimator is given by 
where 8z1), . . . , 8Zt), . . . 
is a sample from the proposal distribution g(8,) and 
8:), . . . , 8:), . . . 8p) is a sample from the posterior distribution f(O,lg, m) (usually 
taken from an MCMC algorithm). 
Obviously the efficiency of this estimator depends on selection of the proposal distri- 
bution g and the bridge function h. The proposal distribution must be close to the tar- 
get posterior distribution. The closer are the two distributions, the more efficient is the 
estimator. Moreover, the function h plays the role of the bridge that links the two dis- 
tributions. For this reason, it must support both distributions. According to Meng and 
Wong (1996), an optimal function (under the mean square error) is the bridge function 
h(8,) = Tz/{T*g(8,) + T1f(8,ly, m)} which depends on the marginal likelihood. 
Hence an estimate of the marginal likelihood can be obtained using the iterative scheme 
proposed by Meng and Wong (1 996). Finally, a further generalization of the method was 
proposed by Gelman and Meng (1 998) using a series of functions called path, while the 
corresponding estimator was called the path sampling estimator (Gamerman and Lopes, 
2006). Although this method is quite efficient, the required specification of both the pro- 
posal distribution and the bridge function make this approach quite unattractive especially 
for inexperienced users. 
11.3.6 Chib’s marginal likelihood estimator 
Among the most popular estimators for calculation of the marginal likelihood are the esti- 
mators proposed by Chib (1 995) based on the Gibbs output and their extension by Chib and 
Jeliazkov (2001) using the output of Metropolis-Hastings algorithm. Both these estimators 
are also called candidate’s estimators and are based on the identity 
(11.5) 
for all data y and for any value of the parameters 8, (Besag, 1989). Any value 0; 
will provide the correct value for the marginal likelihood. The problem is that the posterior 
distribution is not available and hence must be estimated. Hence an estimate ofthe posterior 
predictive ordinate f^((e;ly, m) at a point of interest 8; will be required to estimate the 
marginal likelihood 
f(YlCn3 m)f(%lm) 
@:,IY> 
m) 
f^(Y Im) = 
When the full conditionals are known, then Gibbs sampling is facilitated. In this case 
the posterior distribution can be rewritten as 
f ( % h  m) = f ( O 1  IY, m)f(Oz, . . ., QdlOll Y, m) 

396 
BAYESIAN MODEL AND VARIABLE EVALUATION 
= f(~lIY,m)f(e21~l:Y,m)f(e31.~f 
lQdP110z,Y,m) 
d 
= f ( ~ l I Y , ~ ) ~ f ( ~ j l ~ l l . . . , ~ , - l l Y , ~ )  
, 
j=2 
where d is the dimension of the parameter vector O,, = (01, . . . , B d ) T .  These conditional 
distributions can also be written as 
and 
where Om,\j = (el,. . . Oj-ll Qjfl,. . . , 0 d ) .  Estimates can be obtained from the Gibbs 
output using the following simple estimators 
since the full conditional posterior densities are available in this sampling scheme. 
Chib (1995) proposed to estimate the marginal likelihood using 
with 
f^(e,py, . . . , q1, 
y1 m) = - 
l
T
 1 
f(e;(t)le;(t), 
. . . , ej!,', oiyll . . . led 
( t )  , Y, m). 
t=l 
T 
These formulations indicate that r(0; Iyl rn) will be estimated directly from the Gibbs 
sampler used to produce a sample from the posterior distribution while each of the remaining 

COMPUTATION OF THE MARGINAL LIKELIHOOD USING WINBUGS 
397 
conditional posterior predictive ordinates f^(Oj IOY, . . . , Oj-, , y, rn) will be estimated from 
the same sampling scheme but restricted to the selected values 0 T, . . . , 0;-, . Hence, in order 
to estimate the posterior predictive ordinate, we need to run our Gibbs sampling algorithm 
d separate times. Selection of Of is also important. Chib (1995) proposed using values 
close the center of the distribution; hence the posterior mean, median, or mode are usually 
good choices. 
Chib and Jeliazkov (200 1) implemented the same idea to the Metropolis-Hastings algo- 
rithm. Chen (2005) also proposed an extension of the method used when latent variables 
are available and that avoids some of the drawbacks of Chib’s estimators. Details concern- 
ing the implementation of the method using Matlab can be found in Congdon (2006b, sec. 
2.3). Calculation of the marginal likelihood using WinBUGS is possible, but it is not rec- 
ommended (at least from the author of this book) since the user does not know the sampling 
scheme implemented by the program and, furthermore, calculation of the full conditional 
posterior distributions and multiple runs of the algorithm are required. 
11.3.7 Additional details and further reading 
Review, comparison of several marginal likelihood estimators, and illustrative examples can 
be found in Gamerman and Lopes (2006). Congdon (2006b) provides details and examples 
for the implementation of several method using Matlab. Several other methods have been 
proposed in the literature for estimation of the marginal likelihood. As we have already 
mentioned, Raftety et al. (2007) have proposed a method to stabilize the harmonic mean 
estimator, Gelman and Meng (1 998) extended the bridge sampling estimator by proposing 
the path sampling estimator, Chib and Jeliazkov (2005) extended their method for posterior 
samples generated by the accept-reject 
Metropolis-Hastings algorithm, and Chen (2005) 
proposed a method that can be considered as an extension of Chib’s estimator. 
Neal (2001) used ideas from importance sampling and simulated annealing to construct 
an estimator of the marginal likelihood. Furthermore, Friel and Pettitt (2008) proposed an 
estimator based on power posteriors by combining ideas from simulated annealing and path 
sampling. 
These approaches entail running each model under comparison and estimating the 
marginal likelihood of each model separately. In real life model selection problems and 
especially in variable selection models, the set of models under consideration is usually 
large, and hence such methods cannot be implemented since a high computational burden 
is required. In such cases, we need to use an algorithm that can also work as a model 
search algorithm and will be able to trace the “good” models having high posterior model 
probabilities. Such methods are based on extensions of usual MCMC methods such as 
the reversible jump MCMC (Green, 1995); for more details, review, and comparison of 
such methods, see Han and Carlin (2001), Dellaportas et al. (2002), and Sisson (2005). 
In Section 11.5, we concentrate on the variable selection problem, presenting Gibbs-based 
methods that can be easily implemented in WinBUGS . A brief description of the reversible 
jump MCMC algorithm is also provided in Section 1 1.9 
11.4 COMPUTATION OF THE MARGINAL LIKELIHOOD USING WinBUGS 
In this section we illustrate the computation of the marginal likelihood models under con- 
sideration using the output generated using WinBUGS . More specifically, in this section we 
illustrate the Laplace-Metropolis estimator, the harmonic mean estimator, and the general- 

398 
BAYESIAN MODEL AND VARIABLE EVALUATION 
ized harmonic mean estimator. Two simple examples with conjugate prior setups were used 
in which the marginal likelihood was also available analytically. This allows us to present 
and compare the efficiency of the illustrated methods since the actual marginal likelihood 
value is known. The harmonic mean estimator fails even in simple cases presented for 
illustration in this chapter. The other two estimators give reasonable results, at least within 
the family of the generalized linear models. 
To estimate the Laplace-Metropolis estimator, we proceed with the following steps 
1. Produce an MCMC sample in WinBUGS . 
2. Estimate from the MCMC output produced by WinBUGS : 
a. The posterior means of the parameters of interest denoted by em 
b. The posterior standard deviations ofthe parameters of interest denoted by s 8, = 
c. The posterior correlation between the parameters of interest (from 
(sl,. . . I s d )  
Inf e r e n c e X o r r e l a t  ions) menu, denoted by Re, 
3. Calculate the expression 
n 
+ c 1% f(Yil%, 
m) + 1% f(e,Im), 
i= 1 
where s j  are the posterior standard deviations of 0, parameter estimated from the 
MCMC output. 
The harmonic mean estimator can be obtained by calculating the likelihood in each 
iteration. In order to do so, we first calculate the log-likelihood term e i for each observation i 
usingthenodelog.like[il (fori = 1 , 2 , .  . . ,n)andthentheinverselikelihoodinv.like 
by the expression exp (- cy=l 
t i )  using the WinBUGS syntax 
Finally, the estimate of the marginal likelihood is given by the inverse of the posterior 
mean of node inv. like: 
f(ylm) = 1/ inv.like. 
Similarly, we calculate the generalized harmonic mean estimate of the marginal likeli- 
hood. We calculate node w . l i k e  by the expression 
and then estimate the marginal likelihood by (1 1.4), that is, the inverse of the posterior mean 
of w . like: 
f^(ylm) = 1/ w.like. 

COMPUTATION OF THE MARGINAL LIKELIHOOD USING WINBUGS 
399 
11.4.1 A beta-binomial example 
Example 11.1. Kobe Bryant’s field goals in NBA (Example 1.4 revisited). Here 
we reconsider Kobe Bryant’s field goals in NBA (Example 1.4). Two models are 
fitted: the one with equal success probabilities across all seasons (model mo) and 
the one with different success probabilities (model m 1). Hence this is equivalent 
to testing HO : 7rk = 7r for all k E { 1999, . . . ,2006} versus the alternative that 
Ho: 7rk # 7rl for all k # 1; k,l E (1999,. . . ,2006) 
A simple beta-binomial model is adopted in which the marginal likelihood can be cal- 
culated analytically. 
Analytic computation of the marginal likelihoods. Under model ml, the success prob- 
abilities for each yeariseason are the parameters of interest 8 ml = T = ( ~ 1 9 9 9 ,  . . . ,7r2006), 
while the log-likelihood is given by 
We consider a set of independent (conjugate) beta prior distributions with parameters ai and 
bi 1 
2006 
f(eml = nlmi) = n 
f B ( X i ;  ai, bi). 
i=1999 
The marginal posterior likelihood for model ml is given by 
Similarly, for model mo we have a common parameter 8,, = 7r 
Finally, the Bayes factor of model mo versus model ml is given by the ratio of the 
two marginal likelihoods above. Here we consider the uniform distribution as a prior (i.e., 

400 
BAYESIAN MODEL AND VARIABLE EVALUATION 
a, = b, = 1 and a = b = 1). In this case the marginal likelihoods are equal to 
2006 
logf(ylmi) = - 
(N, + 1) = -58.0228 
z=1999 
= -39.2308, 
resulting in a log-Bayes factor equal to log Bol = -39.2308 + 58.0228 = 18.792, which 
provides support in favor of model m 0. 
lmplementation in WinBUGS . Illustration is described for model ml. Implementa- 
tion for model mo is similar, but one common parameter T is used to model the success 
probabilities instead of a vector. 
To calculate the harmonic mean estimator, we need to define the node log. l i k e  
and then set 
to calculate the inverse of the likelihood in each iteration. The harmonic mean estimator is 
given by the inverse of the posterior mean of inv. like. 
The procedure for the generalized inversed distribution is similar to the one described 
above. We need to define a reasonably well-behaved function, g(?r). Here we consider that 
To calculate the generalized harmonic mean estimator under this importance function we 
use the following syntax 
in addition to the log-likelihood defined as described above. An estimate of the marginal 
likelihood is given by the inverse of the posterior mean of node wlike. 
Finally, in order to calculate the Laplace-Metropolis estimator, we estimate the posterior 
means and standard deviations from an MCMC run. Then we calculate the posterior corre- 
lation matrix by the menu Inf erenceXorrelations; results for model m 1 are provided 
in Table 1 1.2. 
Then we calculate the marginal likelihood in an external software such as R. After import- 
ing the MCMC-based estimated posterior summaries (denoted by p, s, and R, respectively), 

COMPUTATION OF THE MARGINAL LIKELIHOOD USING WINBUGS 
401 
Table 11.2 
Example 1 1.1 
Posterior summaries and correlation of success probabilities for model rnl in 
Posterior 
Posterior correlations 
7r 
Mean 
SD 
r i g 9 9  
m o o o  
m o o 0  
m o o 0  
m o o 0  
m o o 0  
m o o 0  
moo0 
~ 1 g g g  0.468 0.0145 
7~2000 0.464 0.0131 
~2001 0.469 0.0125 
~ 2 0 0 2  0.451 0.0113 
~ 2 0 ~ 1 3  0.438 0.0146 
7r2004 
0.433 0.0138 
~ 2 0 0 5  0.450 0.0107 
~ 2 0 0 6  0.472 0.0170 
1.000 
0.141 1.000 
0.138 0.175 1.000 
0.176 0.152 0.185 1.000 
0.130 0.149 0.132 0.149 
1.000 
0.123 0.138 0.138 0.143 0.119 
1.000 
0.175 0.187 0.191 0.199 0.142 0.161 
1.000 
0.107 0.124 0.113 0.143 0.111 0.108 0.125 1.000 
we calculate the logarithm of the Laplace estimate for the marginal likelihood using the 
R commands 
The full code for model 7n 1 is given in Table 11.3. Code for model mo is similar, but p [tl 
is substituted by a single p. 
Table 11.3 
11.1 
WinBUGScode for estimation of marginal likelihood for model ml in Example 
Results. Results based on 5000 iterations (and an additional 1000 iterations as burnin 
period) are presented in Table 1 1.4. We observe that the generalized harmonic mean and 
the Laplace-Metropolis estimators are very close to the true values while the harmonic 
mean fails to provide a reasonably accurate estimate of the marginal likelihoods, especially 
for model ml . From Figure 1 1.1 we can also see that the latter is quite unstable since the 
ergodic estimators do not seem to stabilize. 

402 
BAYESIAN MODEL AND VARIABLE EVALUATION 
.- P 
I n -  
- 
0, 
- 
- 
"
2
 
Table 11.4 Estimated log-marginal likelihood and Bayes factor for Example 11.1 
'ir- 
Inverse likelihood 
inv. like 
Weighted inverse likelihood wlike 
3.917 x 1015 8.168 x 1OI6 
- 
1.091 x 1017 1.585 
loz5 - 
Log-marginal likelihood 
Harmonic mean 
-35.904 
-38.942 
3.038 
Generalized harmonic mean 
-39.231 
-58.025 
18.794 
Laplace-Metropolis 
-39.218 
-58.203 
18.985 
True value 
-39.231 
-58.023 
18.792 
h 
m 2 s - !  
I 
1 
I 
I 
I 
I 
I 
0 
1000 
3000 
5000 
0 
1000 
3000 
5000 
Iterations 
Iterations 
(a) Trace plot: inverse likelihood 
(b) Ergodic plot: harmonic mean estimator 
Iterations 
Iterations 
(c) Trace plot: weighted inverse likelihood 
(b) Ergodic plot: generalized harmonic mean 
Figure 11.1 
marginal likelihood for model ml. 
Trace and ergodic plots for simple and generalized harmonic mean estimators of the 

COMPUTATION OF THE MARGINAL LIKELIHOOD USING WINBUGS 
403 
11.4.2 A normal regression example with conjugate normal-inverse 
gamma prior 
Example 11.2. Soft drink delivery times data (Example 5.1 revisited). Here 
we reconsider Example 5.1, in which simple normal models with two covariates 
are considered, resulting in a total of four possible models under consideration. 
Analytic computation of the marginal likelihoods. Let us consider two models, mo 
and ml, given by 
where m E {mo, ml}, n is the sample size, p, is the P, x 1 vector of model parameters, 
X ,  is the n x P, design (or data) matrix of model m, P, is the number of parameters 
involved in the linear predictor of model m, N ( p ,  E) is the multivariate normal distribution 
with mean p and variance covariance matrix X, and I n  is an R x R identity matrix. 
Y N N (XrnP,, a21n) 
1 
We adopt the conjugate prior distribution given by 
and a gamma prior with parameters a and b for the residual variance f (a 
2, - IG(a, b). 
The resulting marginal likelihood for model m is given by 
while the corresponding posterior model odds for comparison between models m 0 and ml 
are provided by the expression 
where SS, are the posterior sum of squares 
c-2 
T v-1 
(11.8) 
(11.9) 
-l- --I- 
SSm = ~ ~ ~ - P r n E r n ~ r n +  
PD, 
m CL~,,,, 
P,, 
= Ern (XlXmPm + c 2 ~ k 1 p p m )  
> 
c, 
= X L X ,  +c-2v,11 
- 
- 
- 
(11.10) 
- -1 
pp, and V, are the prior mean vector and a prior matrix associated with the prior variance- 
covariance matrix of the parameter vector p , 
respectively, and p, and X, 
are the corre- 
sponding posterior measures. 
An alternative expression of the posterior sum of squares is given by Atkinson (1 978) 
and Pericchi (1 984), where 
I 
- 
-T 
and RSS, = yTy - p , X z X , p ,  
is the usual residual sum of squares. This quantity is 
the sum of two measures: a goodness of fit and a measure of distance between maximum 
likelihood estimates and the prior mean. 

404 
BAYESIAN MODEL AND VARIABLE EVALUATION 
lmplementafion and results. In this example we consider an independent normal prior 
distribution with V,, = Ip,,, and select various values for c2 E (10,100, lOOO}. For 
the inverse gamma distribution we set a = b = 0.001. Results based on the analytic 
computation of the marginal likelihoods are provided in Table 11.5. The second model 
(mz), with covariate the number of cases that the employee needs to load on the machine, 
is the one supported by prior setups within the range of c 2  E [lo, 10001. Lindley-Bartlett 
paradox is depicted in this table since the marginal likelihoods and the posterior model 
probabilities of the simpler models increase in contrast to the more complicated model 
when c2 increases. The effect of the covariate measuring the number cases is strong, 
resulting in posterior model probabilities that are quite robust for a wide range of values 
for the prior parameter c 2 .  The Lindley-Bartlett paradox eventually appears for large prior 
variances eliminating the actual data evidence and finally fully supporting the simplest 
(constant) model (see results for c2 = e7Oo). 
Table 11.5 
consideration in soft drink data (Example 5.1) 
Marginal log-likelihood and posterior model probabilities for models under 
ml 
m2 
m3 
m4 
c2 
(constant) 
(cases) 
(distance) 
(cases + distance) 
Marginal Log-likelihood 
10 
-1 13.65 
-84.93 
-102.28 
-86.10 
1000 
- I  15.84 
-89.48 
-106.85 
-92.96 
e7Oo 
-462.38 
-782.57 
-799.94 
-1132.59 
Posterior probabilities 
10 
2.6 x 
0.7627 2.2 x lops 
0.2373 
100 
1.0 x lo-'' 
0.9108 2.6 x 
0.0892 
1000 3.5 x lo-" 
0.9700 2.8 x lo-' 
0.0300 
100 
-114.70 
-87.18 
-104.55 
-89.51 
e70" 
1.000 
O.OOOO 
0.000 
0.0000 
The Laplace-Metropolis, the harmonic mean, and the generalized harmonic mean esti- 
mates of the marginal likelihoods for c2 = 1000 are presented in Table 11.6. Code for the 
four models under consideration are available in the book's Webpage. For the generalized 
harmonic mean estimates, we used the posterior means and standard deviations from each 
model in order to specify independent normal importance distributions for parameters ,B 
and a simple gamma for the precision parameter. These were similar to the results obtained 
when a multivariate normal importance function was used for all parameters. In all cases 
studied, the harmonic mean estimator proved inaccurate and unstable in contrast to the other 
two estimates, which provided accurate results. Note that the accuracy of the generalized 
harmonic mean estimator depends heavily on the choice of the importance function. In this 
example, the simple strategy of considering posterior summaries to construct independent 
importance functions has proved quite effective. 
Note that, because of the large values the exponents involved in the harmonic and the 
generalized harmonic mean estimator, it was not possible to calculate the inverse and the 
weighted inverse likelihood (inv . like and wlike) directly in WinBUGS . For this reason, 
in the example presented here, we subtracted a large value from each component before 
taking the exponent of the log-likelihood (here we have used C = 90). Hence, instead of 

BAYESIAN VARIABLE SELECTION USING GIBBS-BASED METHODS 
405 
Table 11.6 
c2 = 1000 for soft drink data (Example 5.1) 
Estimated marginal log-likelihoods and posterior model probabilities for 
Estimator 
ml 
m2 
m3 
m4 
(constant) 
(cases) 
(distance) 
(cases + distance) 
Marginal log-likelihood 
Harmonic mean 
Generalized harmonicmean 
Laplace-Metropolis 
True value 
Posterior probabilities 
Harmonic mean 
Generalized harmonic mean 
Lap lace-Metropolis 
True value 
-105.36 
-1 15.84 
-115.85 
-1 15.84 
1.3 x 
3.2 x lo-” 
3.0 x 10-l’ 
3.5 x 10-12 
-73.28 
-86.80 
-89.43 
-104.72 
-89.48 
-106.85 
0.0110 1.5 x lops 
0.9585 2.2 x 
0.9683 2.7 x lo-’ 
0.9701 2.8 x 
-89.35 
-106.73 
-68.78 
-92.57 
-92.77 
-92.96 
0.9890 
0.0415 
0.0317 
0.0299 
calculating 
in each iteration, we have calculated 
inv. like* = exp (- g e i  - C )  , 
n 
) 
wlike* = exp g(e,) - Ce, - f(e,lm) - c , 
( 
i= 1 
and then obtained the estimates of the log-marginal likelihood by 
inv. l i k e  = - log(inv. like*) - C and wlike = - log(wlike*) - c, 
where ei is f(gi[Orn, m) for i = 1, . . . , n. 
11.5 BAYESIAN VARIABLE SELECTION USING GIBBS-BASED METHODS 
In GLM-type models, which we have focused on in this introductory book, the main model 
selection problem is the selection of the covariates involved in the linear predictor. Even 
when the number of covariates is moderate, the number under consideration is large (e.g., 
for p = 20, it is equal to 2” = 1,048,576), making it difficult to estimate the marginal 
likelihoods of all models using the methods described in the previous section. In fact, we 
need to implement algorithms that efficiently search the model space, focus on the most 
probable a posteriori models, and provide estimates of their posterior model probabilities. 
This can be achieved by the methods described in this section. Here we focus on the Gibbs 
variable selection introduced by Dellaportas et al. (2002). Other Gibbs-based methods 
such as the stochastic search variable (George and McCulloch, 1993) selection and the 
Kuo and Mallick (1 998) sampler are discussed and compared. All these methods can 

406 
BAYESIAN MODEL AND VARIABLE EVALUATION 
be implemented in WinBUGS ; for additional details and examples, see Dellaportas et al. 
(2000) and Ntzoufras (2002). 
In variable selection, the set of models M under consideration can be represented by a 
vector of binary indicators 7 E (0, l}". This vector of binary indicators reveals which of 
the p possible sets of covariates are present in the model. For example, for a generalized 
linear model, the linear predictor can now be written as 
j = O  
where X and p are the design matrix and the parameter vector of the full model (including 
all p available covariates in the linear predictor), respectively: X j  is the vector or matrix 
with the elements X that correspond to the coefficients pj of the jth covariate under 
consideration; while XO = 1, is the column that corresponds to the constant term PO. 
There is a one-to-one transformation connecting the usual model indicator m with the 
vector of variable indicators y. One way to transform y to a unique model indicator 
m, is to consider y as a number in the binary numerical system and transform it to the 
corresponding number in the decimal numerical system; see Section 1 1.6 for more details. 
Finally, we consider the partition of /3 into (py, p\y) corresponding to those compo- 
nents of ,B that are included (ri = 1) or not included (-yi = 0) in the model. Hence the vector 
py corresponds to the active parameters of the model (i.e., 
,), 
while p\y corresponds 
to the remaining parameters, which are not included in the model defined by y. 
Before proceeding to a description of the Gibbs-based variable selection methods, we 
briefly provide details concerning a set of priors that can be used for Bayesian variable 
selection. 
11.5.1 Prior distributions for variable selection in GLM 
The specification of the prior distribution when no prior information is available is very 
important in variable selection in order to allow the data to determine which variables are 
important for the model without activating the Lindley-Bartlett paradox. Here we present 
a family of prior distributions based on the Zellner's g-prior (Zellner, 1986). We describe 
an interpretation based on imaginary data and the power prior of Ibrahim and Chen (2000) 
and Chen et al. (2000). The prior is presented for normal, Poisson, and binomial data, but 
it can be specified for the rest of the models using similar arguments. 
Zellner's (1 986) g-prior is simply defined if we specify V , 
= (XZX,) -' in the 
conjugate prior (1 1.6): 
(11.13) 
This prior can be interpreted using the power prior of Ibrahim and Chen (2000) and Chen 
et al. (2000). Let us assume imaginary data y * obtained under the same design matrix X ,. 
Then we set our prior proportional to a power of the likelihood obtained by the imaginary 
data: 
I / 2  
f(P,Iv*,m) 0: ( f ( Y * I P d ) )  
' 
This prior accounts for information equal to n/c2 data points. In the normal regression 
model, the above prior results in 

BAYESIAN VARIABLE SELECTION USING GIBBS-BASED METHODS 
407 
which is the Zellner g-prior for imaginary data equal to y * = X,pom. 
around 
zero and hence set porn = 0. Furthermore, a default choice for c2 (denoted by g in Zellner's 
original paper) is to set c2 = n, which corresponds to adding prior information equal to 
one data point, resulting to the prior 
If no prior information is available, it is logical to center our prior beliefs for p 
~ , l m - ~  
(
(
 
0, n xi,x,,)-loz). 
(11.14) 
In this way we a priori support the simplest model (by setting the means equal to zero) but 
in a minimal way, since our prior accounts only for one data point in the final analysis. This 
approach is also sensible in terms of the parsimony principle. Posterior model odds (and 
Bayes factors) penalize the model likelihood for deviations of the actual data from the prior 
distribution (Raftery, 1996a, eq. 12). Since the prior presented above can be generated 
using a set of minimally weighted imaginary data that fully support the constant model, it 
provides a sensible a priori support of more parsimonious models. 
Similar arguments were used by Fouskakis et al. (2008) to adopt the prior of Ntzoufras 
et al. (2003) 
f(P,,lm) = N(Porn>7L[m)1-1) 
(1 1.15) 
for the logistic regression case, where I@,) is the information matrix 
Z(P,) = x:wmxm. 
Here W,, is a diagonal matrix that in the binomial case (McCullagh and Nelder, 1989) 
takes the form 
W,, = diag N,ni(l - T , ) ) .  
This is the unit informationprior introduced by Kass and Wasserman (1995), which corre- 
sponds to adding one data point to the data. Here we use this prior as a base, but we specify 
7rt in the information matrix according to our prior information. In this manner we avoid 
(even minimal) reuse of the data in the prior. 
Similarly to the normal regression case, when little prior information is available, a 
reasonable choice for the prior mean of p, is poTL = 0. This corresponds to the assumption 
that a reasonable prior estimate (when no information is available) for all fitted probabilities 
of the binomial regression model is rt = f . With this choice and binary data (i.e., N ,  = 1 
for all z), Eq. (1 1.15) simplifies to 
( 
(11.16) 
where n is the total number of Bernoulli experiments. This prior distribution can also be 
motivated using the power prior approach of Chen et al. (2000). After observing the design 
matrix X, for any model m, we consider a set of imaginary data y: = N, and N: = 2N, 
for i = 1. . . . , TI that assigns probabilities 
for all binomial observations i and therefore 
supports the simplest (constant) model. We consider a prior that is generated using the 
likelihood of these imaginary data, 

408 
BAYESIAN MODEL AND VARIABLE EVALUATION 
where N = Cy=l Ni. Note when binary data are considered, then Ni = 1 for all i = 
1, . . . , R resulting to the setup of Fouskakis et al. (2008). Using this prior, the posterior 
becomes 
therefore this is equivalent to obtaining information from Cy=l (Ni + Ni/n) = ( N  + 1) 
Bernoulli experiments, instead of N when using a flat prior. Thus the proposed prior (1 1.17) 
introduces to the posterior distribution additional information equivalent to one data point. 
Using a Laplace approximation to (1 1.17) [see, e.g., Bernard0 and Smith (1 994, p. 286)], 
we obtain 
where Prn is the maximum likelihood estimate if the imaginary data y: were observed and 
Z(3,) 
is the observed information matrix given by 
I@,) = X;diag(2Ni%;(1 - %:))X,, 
A 
-1 
in which %,* = 1 + e~p(-X(~lP,)) 
is the fitted success probability for all i under 
model m when observing data y *. According to these imaginary data, p, = 0 and %i = f 
for all i, yielding I@,) 
= f ( X Z X , )  and therefore leading to the prior given by 
( 1 1.1 6). 
Similar arguments can be used for the Poisson case. If all imaginary data yy,* are equal 
to one, then the corresponding prior that accounts for one additional data points is given by 
( 
h 
Pmlm - N ( 0 ,  R(x;xmL)-l). 
This prior can be informative; for the constant parameter, we might use a vague prior for 
0 
(if it is included in all models) and use the prior formulated above for the parameters of the 
covariates, namely, ,B\o,,Jm - N ( 0 ,  R(x:z,)-~), 
where pie,, is the parameter vector 
p, without the constant parameter PO and x, is the data matrix X, after removing the 
first column that corresponds to the constant parameter. 
Using similar arguments, we may use an empirical B3yes approach by estimating the 
posterior means 
and the variance-covariance matrix Xe multiplied by the sample size 
R. In this way we assume that we have imaginary data such as the ones already observed, 
which account for one additional data point. Hence the effect of the double usage of data 
is low and simplifies the implementation of Bayesian variable selection. 
In model selection the uniform prior on model space it is typically used by setting 
1 
f ( m )  = - 
forall m E M .  
(MI 
When using the variable selection indicators 7, 
this prior is equivalent to specifying inde- 
pendent Bernoulli prior distributions with inclusion probability equal to f : 
~j N Bernoulli (i) for all j = 1,. . . , p  . 

BAYESIAN VARIABLE SELECTION USING GIBBS-BASED METHODS 
409 
Hence, in some cases, it is sensible to set f(~jIy\~) 
= f ( 7 j )  (where the subscript \j 
denotes all elements of a vector except the jth) , whereas in other cases (e.g., hierarchical 
or graphical log-linear models), it is required that f(7j IyIj) depends on y\j (Chipman, 
1996). 
Although, the prior for y with each term present or absent independently with probability 
$ may be considered noninformative in the sense that it gives the same weight to all possible 
models, George and Foster (2000) argue that this prior can be considered as informative 
since it puts more weight on models of size close top/2 supporting a priori overparametrized 
and complicated models. 
11.5.2 Gibbs variable selection 
Gibbs variable selection (GVS) was introduced by Dellaportas et al. (2002). In order to setup 
GVS, we need to specify the prior distribution for (y, P) with the following hierarchical 
structure: f(y, P) = f(y)f(Ply). If we consider the partition of P into (Py, P\-,), then 
the prior f(Ply) may be partitioned into model parameter prior f(Pyly) and pseudoprior 
f(P\ylPy; 7 ) .  
The full conditional posterior distributions for the model parameters are given by 
f(PyIP\y, 7 ,  Y) c( f(YIP, r)f(PylY)f(P\ylPy, 7) 
(11.17) 
f(P\ylPy,Y,Y) 
f(P\ylPy,Y), 
(11.18) 
and for the variable indicator yj by 
7 j  I@, y\j, y - Bernoulli - 
(1 ?&) 
(11.19) 
with 
o, - f(7j = 1 1 7 \ j A Y )  - f(YIP,7, = LT\j) f(Pl7j = Lr\j) f ( 7 j  = L 7 , j )  
- 
f ( 7 j  = 0l7\j3P>Y) f(YIP,7j = O,-f\j) 
f(Pl7j = O / \ j )  
f(7j = O,Y\j)’ 
3 -  
(1 1.20) 
where y\j denotes all terms of y except -yj. 
Note that the full conditional posterior distribution of the active model parameters P 
in 
(1 1.17) also depends on the pseudoprior f ( P  ly IPy, y), which seems awkward, at least on 
first glance at of this expression. Although this dependence may be useful, especially when 
correlated variables are considered as possible covariates, we may avoid it by assuming 
that the actual model parameters fly and the inactive parameters P\y are a priori indepen- 
dent. This plausible assumption implies that f(P\yl,By, y) =f(Piy IT), 
simplifying the 
conditionals posterior distributions (1 1.17) and (1 1.18) to 
f ( P y l P \ y J ,  Y) = f(YIP, M P y l Y )  
f(P\yJPy, 71 Y) c( f(P\ylr) 
The algorithm is further simplified when assuming prior conditional independence of 
all Pj terms for each model y. This is a rather restrictive assumption that may influence 
posterior model probabilities. Nevertheless, it is realistic in the case where a data matrix 
X with orthogonal columns X j  appearing in (1 1.12) and the imposed priors are intended 
to be noninformative. Then, each prior for P j  Iy consists of a mixture of two densities. The 
first, f ( P j / 7 j  = l , ~ \ ~ ) ,  
is the true priorforthe parameter, whereas the second, f ( P j [ 7 j  = 

41 0 
BAYESIAN MODEL AND VARIABLE EVALUATION 
0, T \ ~ ) ,  
is a pseudoprior. For example, considering normal prior and pseudoprior for each 
pj results in a prior setup that is a mixture of two normal distributions given by 
f ( P j l 7 j )  = y,”O, 
X j )  + (1 - ? j ) W P j , W  
(11.21) 
The prior with f(Pj 
17) = f(Pj 
I r j )  potentially makes the method less efficient, but it is 
appropriate in datasets where X is orthogonal. If prediction, rather than inference, is of 
primary interest, then X may always be chosen or appropriately transformed to be orthog- 
onal [see, e.g., Clyde et al. (1996)l. Under this simplified prior setup, the full conditional 
posterior distribution is now given by 
and a clear difference between this and SSVS is that the pseudoprior f (P, 17, = 0) does 
not affect the posterior distribution and may be chosen as a “linking density” to increase the 
efficiency of the sampler. Possible choices of p, and sJ may be obtained from a pilot run 
of the full model. An alternative is to set ,iT, equal to zero and set the proposal variance low 
but proportional to the actual prior variance g, = k,-*X, in a fashion similar to the prior 
setup used in the SSVS algorithm, which is described in Section 11.5.3. Empirical results 
have shown that this setup is efficient in specific cases and can be a good alternative when 
parameters vary across models, and therefore an overall proposal cannot be specified from 
a single pilot run from the model with all covariates (Ntzoufras, 1999~). 
11 5.3 Other Gibbs-based methods for variable selection 
Stochastic search variable selection. Stochastic search variable selection (SSVS) was 
introduced by George and McCulloch ( 1  993) for linear regression models and has been 
adapted for generalized linear models (George et al., 1996; George and McCulloch, 1997), 
Poisson log-linear models (Ntzoufras et al., 2000), and multivariate regression models 
(Brown et al., 1998). It has also been implemented in genetics-related problems [see, e g ,  
Oh et al. (2003) and Yi et al. (2003)l. 
The difference between SSVS and GVS described above is that the parameter vector 
retains its full dimension p under all models since the linear predictor is 
U 
(1 1.22) 
Therefore 17 = X P  for all models, where X contains all the potential explanatory variables. 
The indicator variables yJ are involved in the modeling process through a mixture prior of 
the type 
P31Y, 
%“O, 
E,) + (1 - r,)“0, lc/-2X,) 
(1 1.23) 
for prespecified prior parameters k, and prior variance-covariance matrix X J  . The prior 
parameters k, and X J  in (1 1.23) are chosen so that when y , = 0 (i.e., when the covariate is 
“absent” from the linear predictor), the prior distribution for P, ensures that 0, is constrained 
to be close to zero. 
The full conditional posterior distributions for 
are given by 

BAYESIAN VARIABLE SELECTION USING GIBBS-BASED METHODS 
41 1 
while for the variable indicators yj, the full conditional posterior distribution is again a 
Bernoulli distribution with success probability Oj/(l+ Oj) as in the corresponding Gibbs 
variable selection step (1 1.18) with 
If we use the prior distributions for P defined by (1 1.23) and assume that f ( 7 j  = 
0, yIj) = f(7.j = 1, Y \ ~ )  
for all j ,  then 
where dj is the dimension of P j .  
Posterior model probabilities calculated by this approach are slightly different from the 
corresponding probabilities calculated using the traditional posterior model probabilities, 
assuming that specific parameters pj are constraint to zero and therefore are fdly eliminated 
from the model. The resulting posterior model probabilities are heavily dependent on the 
choice of the prior parameters k; and Ej. One way of specifying these parameters is by 
setting Cj large (for "ij = 1) as in Section 1 1.5.1. Then, k: can be chosen by considering 
the value of iPj 1 at which the densities of the two components of the prior distribution are 
equal. This can be consideredas the smallest value of iPj 1 at which the term is considered to 
be significant in practice as proposed by George and McCulloch (1 993). See also Ntzoufras 
et al. (2000) for specification of the prior in the Poisson log-linear interaction models with 
multidimensional P j .  
Using unconditional prior distribution: The Kuo-Mallick sampler. Kuo and Mallick 
(1 998) approach is similar to GVS since the linear predictor has the form (1 1.12). They 
considered a prior distribution for the model parameters f ( P )  independent of the model 
structure 7 ,  resulting in the prior 
f(P,r) 
= f(P)f(r) 
= f(PyIP\y)f(P\y)f(Y). 
Therefore, the full conditional posterior distributions are given by 
(11.26) 
while, similarly to the corresponding step of GVS and SSVS, variable indicators yj con- 
ditional on the remaining parameters are a posteriori Bernoulli distributed with success 
probability Oj/(l + Oj) [see Eq. (11.18)] with 
The main strength of this approach is that it can be implemented in a straightforward 
manner. It is necessary only to specify the usual prior on P (for the full model), and the 
conditional prior distributions f (Pj lolj) replace the pseudopriors required by Carlin and 
Chib's method (Carlin and Chib, 1995). However, this is also a drawback in specific cases, 
since it does not allow for any flexibility of the method and, thereby for improvement of 
the efficiency of the sampler. 

41 2 
BAYESIAN MODEL AND VARIABLE EVALUATION 
Discussion and comparison of the variable selection algorithms. The three Gibbs 
sampling variable selection methods presented above can be easily summarized by inspect- 
ing the conditional probabilities (1 1.24), (1 1.27), and, in particular, (1 1.20). 
In SSVS, f(gIP, 
y) is not a function of y, and so the first ratio on the right side of 
(1 1.20) is absent in (1 1.24). For the “unconditional priors approach” of Kuo and Mallick 
(1 998), the second term on the right side of (1 1.20) is absent in (1 1.27) as P and y are a 
priori independent. For Gibbs variable selection, both likelihood and prior appear in the 
variable selection step. 
The key differences between the methods are their requirements in terms of prior andor 
linking densities. GVS requires linking densities whose sole function is to enhance the 
efficiency of the sampler. The prior parameters in SSVS all have an impact on the posterior, 
and therefore the densities cannot really be regarded as linking densities. The simplest 
method, described by Kuo and Mallick (l998), does not require one to specify anything 
other than the usual priors for the model parameters of the full model. The association 
between the variable selection Gibbs samplers is summarized in Table 1 1.7. 
Table 11.7 
each variable selection algorithm 
Components of full conditional posterior odds for inclusion of term j (0,) in 
oj 
Method 
rl 
PSRi LR.7 
PRi 
Key: PSR = Pseudoprior Ratio; LR = Likelihood Ra- 
tio; PR = Prior Density Ratio; SSVS = stochastic 
search variable selection; KM = Kuo-Mallick method; 
GVS = Gibbs variable selection. 
11.6 POSTERIOR INFERENCE USING THE OUTPUT OF BAYESIAN 
VARIABLE SELECTION SAMPLERS 
The main focus in Bayesian variable selection is to estimate the maximum a posteriori 
(MAP) model. This can be estimated by a simple frequency tabulation ofthe model indicator 
m or of the joint distribution y. Usually for every indicatory we assign the following model 
indicator 
j=k 
which transforms the y to a unique decimal number, where k = 1 when the constant term 
is included in all models under consideration and k = 0 otherwise. 
Hence we estimate the posterior model probabilities f(mly) simply using 
T 

POSTERIOR INFERENCE USING THE OUTPUT OF BAYESIAN VARIABLE SELECTION SAMPLERS 
41 3 
where T and B are the total and burnin iterations of the algorithm and m ( t )  is the model 
indicator value at iteration t. Note that only for the best models we accurately estimate 
the posterior model probabilities. Posterior model odds can be estimated using the ratios 
of the posterior model probabilities calculated by the MCMC output. These estimates are 
accurate only when both compared models have been visited in a sufficient number of 
iterations by the algorithm. An alternative is to use the approach of Ntzoufras et al. (2005) 
and calibrate prior model probabilities in such a way that all models under consideration 
are visited sufficiently often. Then, the Bayes factor and the true posterior model odds are 
calculated by some inverse calculations based on definition of the posterior model odds 
(1 1 .  I); see also in Katsis and Ntzoufras (2005) for implementation using WinBUGS . 
From the MCMC output, we can estimate the posterior inclusion probabilities of each 
variable 
f ( 7 j  = IlY) = 1 
f ( 7 j  = L 7 , j l Y )  
Y\,t{O,lp-' 
using the simple estimator 
Using these inclusion probabilities, we can trace the median probability model (MP), which 
includes all variables with f(rj = 119) > 0.5. The MP model has better predictive 
performance than the MAP model under certain conditions; for model details, see Barbieri 
and Berger (2004). 
An important task in variable selection is to identify a reasonable solution when the 
number of covariates p is large, especially in comparison to the sample size n. For large 
p ,  the model space becomes large, rendering the efficient implementation of the above 
algorithm difficult. For example, with 20, 50, and 100 variables under consideration, the 
model space includes more than lo6, 
and lo3" models. In such cases, it is rather 
unrealistic to expect any algorithm to accurately calculate the posterior probabilities of 
even the best models. Nevertheless, the algorithms described above can work as model 
search algorithms, which can identify the most probable models. Moreover, the posterior 
inclusion probabilities can be estimated efficiently. Thus we can follow the strategy of 
Fouskakis et al. (2008) and reduce the model space by removing variables with very low 
marginal inclusion probabilities - 
say, for example, with values lower than 0.2. 
Concerning the model parameters Py , the posterior distributions can f(P7 Iy, 
y) can be 
estimated but only for models that the algorithm visited in a sufficient number of iterations. 
In specific cases we do not wish to select a specific model, but make inference or 
predictions by considering model uncertainty also in our analysis. Hence we may obtain 
the model averaged posterior density of a quantity of interest E by 
f(ElY) = 1 
f(mlY)f(Elm,Y) 
mEM 
For example, might be a future observation yrep. 
Note that the marginal posterior distribution f(Ply) (i.e., under all models) does not 
have any meaning unless the parameters have the same interpretation across all models. In 
GLMs, this is true only for the case where X is close to orthogonality. 

41 4 
BAYESIAN MODEL AND VARIABLE EVALUATION 
11.7 IMPLEMENTATION OF GlBBS VARIABLE SELECTION IN 
WinBUGS USING AN ILLUSTRATIVE EXAMPLE 
Here we illustrate the implementation ofGVS in WinBUGS using a simple normal example. 
Additional examples (including binomial and Poisson data) and illustration of SSVS and 
the Kuo and Mallick (1998) sampler can be found in Ntzoufras et al. (2000). 
Example 11.3. Dellaportas et al. (2002) simulated data. Here we consider for 
illustration purposes the first simulated dataset due to Dellaportas et al. (2002) with 
p = 15 covariates and sample size equal to n = 50; data are available in the book’s 
Website with kind permission of Springer science and business media. Covariates 
X ,  (for j = I,. . . , 15) were generated from a standardized normal distribution, 
while the response variable was generated from 
Y, N N ( X z 4  + Xt5, (2.5)2) for i = 1,2,. . . 50. 
For this examples we use the following set of prior distributions: 
1. The prior originally used by Dellaportas et al. (2002): p3 - N(0,lOO) for 
2. Zellner’s g-prior with zero mean and c2 = n given by (1 1.14). 
3. Empirical Bayes independent prior distribution accounting for one data point. 
We set p, 
N N(i?,,n?$,) for j = 0,. . . ,15, where p3 and Sg3 are the 
posterior mean and variance of p, estimated by the full model. 
j = 0,1,2,. . . , 15 and uz 
N IG(10-41 lod4) for c-’. 
- 
- 
Implementation in WinBUGS . The likelihood is specified as usual in WinBUGS . The 
difference is that we now need to incorporate the binary indicators -yj in the linear predictor. 
Hence we can write 
This expresion can be written in WinBUGS in a more general way by avoiding the long 
summation involved in the linear predictor by using the command inprod. Therefore, the 
syntax can be written as 
In this code, node gb corresponds to the active values of fl \o under the current model since 
its elements assume values equal to bj when -yj = 1 and values equal to zero othenvize (i.e., 
when the corresponding variable is excluded from the model). All parameters (including 0 0) 
can be incorporated in a single node (e.g., B) using the following syntax for the likelihood 
definition: 

IMPLEMENTATION OF GIBBS VARIABLE SELECTION IN WINBUGS USING AN ILLUSTRATIVE EXAMPLE 
41 5 
In this code, X and x are the design matrices including or excluding the constant term. This 
setup should be used when a multivariate normal prior is used for p as in the second prior 
setup here: Zellner’s g-prior with zero mean and c2 = n. 
The prior for the inclusion indicators are easily defined here since y3 N Bernoulli(0.5) 
forallj = 0’1,. . . , p .  
Specification of the prior and the proposal densities in WinBUGS when we use indepen- 
dent priors are used (as in prior setups 1 and 3) is relatively easy since we need to specify 
that 
P3 - N(FCLrl >Pl 
1 s;,,0, 
1 
P?,,P, = Y3C1P3 + (1 - Y&3, 
and s;,,p, = Y3s;7 + (1 - Y&,? 
with 
where pp, and S;, are the prior mean and variance of p3 while ppl and sil are the corre- 
sponding proposal (or pseudoprior) parameters, respectiyely. Here, the proposal parameters 
were set equal to the posterior mean &pl and variance S;, of each P3 estimated by a pilot 
MCMC run of the full model. For the first prior setup, we set pp, = 0 and Sil = 100, 
resulting in 
PT,,P, =(l--Y,)&p, 
and s;,,p, =73100+(1-7.7)~;,? 
which can be modeled in WinBUGS using the syntax 
where prop. mean. beta [j 1 and prop. sd. beta [ j] are the proposal means 
and stan- 
dard deviations 30, of P3. For the empirical Bayes (third prior setup), the mean and variance 
of the mixture are given by 
P?,,P, = &P, and s;,,o, = (73n + (1 - Y J P 2  0, ’ 
which can specified in WinBUGS using the syntax 

416 
BAYESIAN MODEL AND VARIABLE EVALUATION 
In order to specify Zellner's g-prior, we need to work on the whole parameter vector 
P, which in the following code is denoted by B. Hence we use a multivariate normal prior 
distribution with mean p7,0 and precision T,,p: 
where ,B is denoted by B in the WinBUGS model code. The elements of the mean p D  are 
defined as in the case of independent prior distributions by 
P7,O.j = (1 - ?S)Lp,> 
while for the prior precision matrix T ,  each element T j k  is equal to the elements of matrix 
c - ~ Q - ~ X ~ X  
in the case where both variables X j  and XI, are included in the model. When 
at least one of them is not included in the model, then T j k  = 0 for j # k. Finally, diagonal 
elements Tjj denote the pseudoprior precision for yj = 0 that is, when X j  is excluded from 
the model. Hence we set 
for i, j = 1 , 2 ,  . . . , p and c2 = n. This mixture prior can be specified in WinBUGS using 
the following syntax: 
Before we conclude this section, we describe how we can obtain in WinBUGS a uniquely 
defined model indicator m for y and how we can calculate posterior model probabilities 
for specific models of interest. To transform y to a model indicator m, we use the simple 
formula that converts numbers defined in the binary numerical system to the corresponding 
numbers in the decimal numerical system. Hence we use the equation 

IMPLEMENTATION OF GlBBS VARIABLE SELECTION IN WINBUGS USING AN ILLUSTRATIVE EXAMPLE 
41 7 
which in WinBUGS is expressed using the syntax 
In this code, WinBUGS model is used insted of m. 
The model indicator m and the corresponding WinBUGSnode, model, can then be 
exported to a statistical software using the command CODA in order to obtain its frequency 
tabulation and, therefore, estimates of the posterior model probabilities. Nevertheless, 
posterior probabilities of specific models can also be calculated within WinBUGS using 
the command equals. In this way, we can trace whether a specific model is visited at 
each iteration of the MCMC run. For the current example, we monitor models X4 + X5 
and X4 + X5 + X12, 
which are the true and the maximum a posteriori ones according to 
Dellaportas et al. (2002) analysis, using the syntax 
This code uses a set binary indicators pmodel to identify which iteration m (model in 
WinBUGS) takes values equal to 24 + 25 = 48 or 24 + Z5 + 2" = 4144 that correspond 
to the target models Xq + X5 
and X4 + X5 + X12. 
When the number of all models is small ( e g ,  forp = 3, the models under consideration 
are only S), then all posterior model probabilities can be calculated using this approach. 
But in the case of a large model space, this approach is not recommended since it will slow 
down WinBUGS because of the large amount of values stored by using these binary model 
indicators. 
Results. Posterior variable inclusion probabilities and posterior model probabilities are 
presented in Tables 1 1.8 and 1 1.9 from MCMC runs of 2 1,000 iterations after discarding the 
Table 11.8 
(2002) 
Posterior variable inclusion probabilities for simulated data of Dellaportas et al. 
Prior 1 a 
Prior zb 
Prior 3c 
f(a = lly) MC error 
f
(
~
~
 
= lly) MC error 
f ( y J  = lly) MC error 
70 
0.042 
71 
0.03 1 
72 
0.039 
"/3 
0.033 
74 
0.970 
7 5  
0.999 
76 
0.046 
77 
0.037 
76 
0.041 
7s 
0.044 
710 
0.043 
711 
0.048 
712 
0.338 
713 
0.038 
714 
0.042 
715 
0.076 
0.0045 
0.0012 
0.0014 
0.0013 
0.0024 
0.0001 
0.0016 
0.0015 
0.0015 
0.0014 
0.0015 
0.0015 
0.0033 
0.0014 
0.0014 
0.0019 
0. I34 
0.128 
0.136 
0.127 
0.992 
1.000 
0.155 
0.138 
0.133 
0.168 
0.141 
0.184 
0.615 
0.137 
0.137 
0.277 
0.0025 
0.0025 
0.0027 
0.0024 
0.0001 
0.0000 
0.0028 
0.0028 
0.0023 
0.0027 
0.0029 
0.0030 
0.0040 
0.0024 
0.0023 
0.0037 
0.039 
0.106 
0.113 
0.101 
0.990 
1.000 
0.128 
0.117 
0.105 
0.138 
0.115 
0.147 
0.545 
0.106 
0.104 
0.243 
0.0016 
0.0022 
0.0023 
0.00 18 
0.0001 
0.0001 
0.0025 
0.0023 
0.0025 
0.0027 
0.002 1 
0.0027 
0.0034 
0.0024 
0.0021 
0.0032 
ahdependent PJ - N(0,lOO) used by Dellaportas et al. (2002). 
bZellner's g-prior with zero mean and c? = n given by (1 I .  14). 
'Empirical Bayes prior of type PJ N N ( P J ,  nSg, ), where OJ and S2 are the posterior mean and 
variance of DJ estimated by the full model. 
- -  
-
-
 
4 

41 8 
BAYESIAN MODEL AND VARIABLE EVALUATION 
Table 11.9 
Posterior model probabilities and odds for simulated data of Dellaportas et al. 
(2002) 
1 
48 
2 
4,144 
3 32,816 
4 
560 
5 
112 
6 16,432 
7 
2,096 
8 
1,072 
9 
8,240 
10 
49 
1 
4,144 
2 
48 
3 36,912 
4 
6,192 
5 32,816 
6 
4,208 
7 12,336 
8 
4,656 
9 
4,272 
10 
5,168 
1 
4,144 
2 
48 
3 36,912 
4 32,816 
5 
6,192 
6 
4,656 
7 
560 
8 
5,168 
9 
4,208 
10 
112 
0.3664 
0.1854 
0.0292 
0.0196 
0.0178 
0.0176 
0.0 172 
0.0157 
0.0150 
0.0149 
0.0679 
0.0453 
0.0252 
0.0 176 
0.0158 
0.01 18 
0.01 16 
0.01 15 
0.01 14 
0.01 12 
0.1014 
0.0896 
0.03 12 
0.0277 
0.0207 
0.0151 
0.0 142 
0.0138 
0.0136 
0.0133 
1 .oo 
1.98 
12.55 
18.69 
20.58 
20.82 
21.30 
23.34 
24.43 
24.59 
0.67 
1 .oo 
1.80 
2.57 
2.87 
3.84 
3.91 
3.94 
3.97 
4.04 
0.88 
1 .oo 
2.87 
3.23 
4.33 
5.93 
6.3 1 
6.49 
6.59 
6.74 
ahdependent P3 - N(0,lOO) used by Dellaportas et al. (2002). 
bZellner's g-prior with zero mean and c? = n given by ( 1  1.14). 
'Empirical Bayes prior of type P3 - N(P,, ngz, ), where p, and g;, are the posterior 
mean and variance of p3 estimated by the full model. 

THE CARLIN-CHI6 METHOD 
419 
initial 1000 iterations as burnin. Results under the three different prior setups differ because 
of the Lindley-Bartlett paradox described in a previous section. We managed to reproduce 
the results of Dellaportas et al. (2002) under prior setup 1 ; see also in Ntzoufras (1999a, pp. 
1 10-1 1 1). Under the first prior setup the true model X 4+X5 has posterior model probability 
0.366, which is about twice as high as the corresponding probability of model Xd + X s  + 
X ~ Z .  
Under this prior setup, more parsimonious models are supported than the ones obtained 
by using Zellner's g-prior with zero mean and c2 = n (prior setup 2) and the empirical prior 
setup (setup 3). Note that under all prior setups used in this illustration, variables X4 and 
X5 must be included in the model since their posterior inclusion probabilities are close to 
one. 
In a second stage, following the approach of Fouskakis et al. (2008), we can eliminate 
variables with posterior inclusion probabilities lower than 0.20 and rerun the MCMC al- 
gorithm. With this approach, we keep variables X4, Xg, and X12 for first prior setup and 
the same variables including variable X15 under prior setups 2 and 3. Posterior model 
probabilities in this reduced model space are presented in Table 1 1.10. 
Table 11.10 
(2002) in reduced model spaces 
Posterior model probabilities and odds for simulated data of Dellaportas et al. 
Posterior model probability 
Posterior model odds 
Model 
m Prior 1 a Prior 2b Prior 3" 
Prior 1" Prior 2b Prior 3" 
~~~~~~ 
~ 
x4 + x5 
4 
0.6505 
0.2987 
0.3503 
1 .oo 
1.00 
1.00 
x4 + x5 + XlZ 
8 
0.3265 
0.4338 
0.4118 
1.99 
0.69 
0.85 
x5 
3 
0.0127 
0.0017 
0.0013 
51.22 
175.71 
269.46 
x5 + XlZ 
7 
0.0102 
0.0025 
0.0035 
63.77 
119.48 
100.09 
x4 + x5 + XI5 
12 
- 
0.1032 
0.1055 
- 
2.89 
3.32 
X4 + X5 + 
+ X15 
16 
- 
0.1568 
0.1239 
- 
1.90 
2.83 
ahdependent ,OJ - N(0,lOO) used by Dellaportas et al. (2002). 
*Zellner's g-prior with zero mean and 2 = n given by (1 1.14). 
'Empirical Bayes prior of type ,OJ - N ( & ,  n,?z3 ), where p., and SgJ are the posterior mean and variance 
of pJ estimated by the full model. 
dEach model is compared with the true model X4 + X5. 
-
I
 
11.8 THE CARLIN-CHIB METHOD 
Carlin and Chib (1995) proposed a Gibbs sampling strategy used to generate from the poste- 
rior distribution f(m, P, 1 ~ ) .  In order to do this, we need to consider the joint distribution 
the model indicator m and parameter vectors P,,,, for all models m' under consideration, 
that is, for {P,, 
for all m' E M } .  Therefore, we need to define prior distributions f(m) 
and f(P,, im) for all m, m' E M .  Distributions f ( P ,  Im,) are the usual prior distributions 
while f (Pm,, lm) with m' # m are calledpseudopriors or linking densities, which are used 
to facilitate the Gibbs sampler and do not influence the posterior distribution. 
The conditional posterior distributions required for the Gibbs sampler are 

420 
BAYESIAN MODEL AND VARIABLE EVALUATION 
where 
sEM 
(1 1.29) 
Therefore, when m‘ = m, we generate from the usual conditional posterior for model 
m, and when m’ # m we generate from the corresponding pseudoprior, f (0 m, Im). The 
model indicator m is generated as a discrete random variable using (1 1.29). 
The pseudopriors have no influence on f(P , 
Im), the marginal posterior distribution 
of interest. They act only as linking densities. Their careful specification is essential for 
the mobility and the efficiency of the sampler. Ideally, f (P,, (m # m’) must resemble 
the marginal posterior distribution f (Pm, Im’, y); see Carlin and Chib (1995) for proposed 
strategies to achieve this. 
The flexibility of this method is due to the ability to specify and tune these pseudo- 
priors which assist the sampler move efficiently across models of different dimension. 
The number of pseudopriors can also be perceived as a drawback, especially in problems 
where the model space M is large since the specification of efficient pseudopriors may be 
time-consuming and cumbersome. A further drawback of the method is the requirement 
for generating parameter vectors Om, for all models under consideration m’ E M at each 
stage of the sampler. This may be avoided by using a Metropolis-Hastings step to generate 
m as proposed by Dellaportas et al. (2002). The description of this sampler is outside the 
scope of the current book; for more details, we refer the reader to the work of Dellaportas 
et al. (2002). 
An example of the implementation of this method is provided in Spiegelhalter et al. 
(1996c, pp. 47-59, pines example) and in Spiegelhalter et al. (2003c, pp. 38-39; pines 
example). In pines example, both models are normal with different mean and variance. 
Hence the overall model is simply expressed as a mixture of two normal distributions with 
different parameters. When the models under comparison are of different distributional 
form, then the zeros trick must be used as described in Section 8.1. Also note that the 
binary indicators approach used in variable selection be can also adopted here to simplify 
the problem of model specification. For an example comparing different distributions using 
WinBUGS , see Katsis and Ntzoufras (2005). 
11.9 REVERSIBLE JUMP MCMC (RJMCMC) 
Reversible jump Markov chain Monte Carlo (Green, 1995) is a flexible MCMC sampling 
strategy for generating observations from the joint posterior distribution f (m, ,B ,ly). The 
method is a natural extension of the simple Metropolis-Hastings algorithm and is based on 
creating a Markov chain that can “jump” between models with parameter spaces of different 
dimensions. 
Supposing that the current state of the Markov chain is (m, p m ) ,  where P, has dimen- 
sion d,, 
one version of the procedure would be as follows: 
0 Propose a new model m’ with probability j(m, m’). 
0 Generate u from a specified proposal density q(uIP,, m, m’). 

USING POSTERIOR PREDICTIVE DENSITIES FOR MODEL EVALUATION 
421 
Set (Pi,,), u’) 
= hm,mf (Om, u), where hTn,mt is a specified invertible function for 
which h,~,, = hit,,. 
This equation implies that d, + d(u) = dm, + d(u’), 
where d, and d ( u )  are the dimensions for the parameters of model m and vector u, 
respectively. 
Accept the proposed move to model m’ with probability 
There are many variations or simpler versions of reversible jump that can be applied in 
specific model selection problems. In particular, if all parameters of the proposed model are 
generated from a proposal distribution, then (PA,, u’) 
= (u, 
P,) 
with d, = d(u’) and 
dml = d ( u ) ,  and the Jacobian term in (1 1.30) is one. This version of the reversiblejump 
can be used for jumping between models for which no appropriate parameter transformation 
exists. When model m is nested to model m’, then a natural transformation function h,,,t 
may be the identity function such that d(u’) = 0 and Om, = (P,, u); see, for example, 
Dellaportas and Forster (1999). Finally, if m’ = m, then the corresponding step is the 
same as in a standard Metropolis-Hastings step. More details concerning RJMCMC can 
be found in Han and Carlin (2001), Dellaportas et al. (2002) and Sisson (2005). 
The WinBUGS jump interface, more recently developed by Dave Lunn, can be used for 
implementing variable selection and automatic spline models. The interface is available at 
the WinBUGS development site: 
http://www.winbugs-development.org.uk/main.html. 
For more details, see Lunn et al. (2005), Lunn et al. (2006) and the interface’s manual and 
examples. 
11.10 USING POSTERIOR PREDICTIVE DENSITIES FOR MODEL 
EVALUATION 
The posterior Bayes factor (PBF) is a natural variation of the Bayes factor based on the 
ratio of the posterior predictive densities of the observed data. Hence the PBF of model m 1 
versus mo is defined as 
Although PBF allows the use of improperpriors, it has been strongly criticized for its double 
usage of the data, leading to violation of the likelihood principle. Moreover, it does not 
correspond to any purely Bayesian measure as the posterior model probabilities f(mly) 
and their corresponding model odds. Finally, PBF supports more complicated models than 
do Bayes factors because of the double use of data. 
For this reason, it is more sensible to use the leave-one-out cross-validatory predictive 
density to construct an overall measure of model evaluation and comparison. Hence we 
may calculate the pseudo-Buyesfuctor (Geisser and Eddy, 1979; Gelfand and Dey, 1994), 
given by 

422 
BAYESIAN MODEL AND VARIABLE EVALUATION 
where CPOi (m) is the leave-one-out cross-validation predictive density for y i under model 
m. Computation of the pseudo Bayes factor can be achieved by calculating the cross-vali- 
dation predictive likelihood, defined as 
n 
n 
i=l 
i= 1 
Computation of each CPOi(m) is described in Section 10.4.1. Alternatively, we may use 
the negative cross-validatory log-likelihood (Spiegelhalter et al., 1996a, p. 42), given by 
n 
LScv(m) = - logegGd(m) = - c logCPOi(m) 
(1 1.31) 
i=l 
also referred as the cross-validatoiypredictive log-score (Draper and Kmjaji C, 2006). 
Although generation of the replicatedipredicted data y rep from the full predictive density 
and computation of CPOs in the leave-one-out cross-validation is straightforward, sampling 
from all R leave-one-out cross-validatory predictive densities is not as straightforward as 
sampling from the full predictive density (Gelfand, 1996). A simple but computationally 
expensive method is to run the MCMC sampler n times by omitting one observation each 
time from the sample. To simplify computations, we can approximate the leave-one-out 
cross-validatory conditional predictive ordinate f (y i IY,~, m) by the full posterior predictive 
ordinate f(yily, m) when the sample size n is large [see, e.g., Carlin and Louis (2000, pp. 
205-206)]. Hence, for the negative cross validation log-likelihood we can write 
i= 1 
i= 1 
The logarithmic score LS(m) is simpler to calculate using a single MCMC run; for details, 
see Section 10.3.4. Note that LS(m) is based on the full predictive density and does not 
correspond to the same supported models as in posterior Bayes factor. In the logarithmic 
score we first calculate all PPOi using expression (10.3) and then consider their product. 
Therefore, from each MCMC run we calculate the posterior means of f(yilOE), m) over 
all iterations t = 1,. . . , T and then consider the product of these posterior means. On the 
other hand, for the posterior Bayes factor, we calculate the full likelihood (i.e., we consider 
the product) and then integrate out over the predictive density; for details, see the appendix 
in f Draper and KmjajiC (2006). 
Other related methods are the approaches of the fractional and intrinsic Bayes factors 
introduced by O’Hagan (1 995) and Berger and Pericchi (1 996a,b). Both of these approaches 
try to avoid the double usage of the data and appropriately define the prior distribution in 
order to avoid the Lindley-Bartlett paradox. In the first approach, a fraction ofthe likelihood 
is used to construct the prior distribution and the remaining fraction to calculate Bayes factors 
and evaluate models, while in the latter a minimal training sample is used to construct the 
prior distribution and the remaining data to calculate the Bayes factor. The selection of 
the training sample is an important task in the second approach. This led to the proposal 
of median and mean intrinsic Bayes factors (and other variations), which have different 
properties and provide different values for comparison of the same models or hypotheses. 

USING POSTERIOR PREDICTIVE DENSITIES FOR MODEL EVALUATION 
423 
11.10.1 Estimation from an MCMC output 
For calculation of the posterior Bayes factor, we need to calculate the posterior predictive 
density evaluated at the observed data y: 
This can be estimated in a straightforward manner by the posterior mean of the likelihood 
over all sampled parameter values from the output of an MCMC sampler. Hence, an estimate 
is given by 
where 0:) , t = 1, . . . , T is a sample from the posterior distribution. In WinBUGS this can 
be estimated in a straightforward manner by defining a node L for the likelihood given by 
where 
= log f ( y i  lo,, 
rn) is the log-likelihood component for each observation i. Users 
must be careful with overflows and underflows of this node. For this reason, L may be 
divided by a constant term C to avoid such problems. 
Computation of the pseudo Bayes factors and the leave-one-out cross-validation like- 
lihood can be achieved by estimating CPOi(m) for i = 1,. . . , n from an MCMC output 
(i.e., within WinBUGS) as described in Section 10.4.1 and then calculate their product 
outside WinBUGS . Hence in WinBUGS we define the node ICPO [il as the inverse of the 
likelihood and estimate CPOs by the inverse of their means: 
-1 
CTOz(m) = [ r n i ( V ) ]  
The leave-one-out cross-validation likelihood is then estimated by 
The above quantity is frequently close to zero and, for this reason, its logarithm is usually 
reported instead expressed by 
h 
LSC"(rn) = -logZ&y(m) 
= -log 
ncTOi(m) = ~ l o g I C P O i ( m )  
(ilil 
) 
i l i 1  
which is called the negative cross-validatory log-likelihood for model m. 
expression 
Finally, the logarithmic score can be calculated in a software outside WinBUGS by the 
n 
n 
L s )  = - c l o g  P?Oi(m) 
= - 
log like [i] 
i= 1 
i=l 
where like [i] is the posterior mean of the likelihood component for observation i corre- 
sponding to the MCMC estimate of PPOi(rn); for details, see Section 10.3.4. 

424 
BAYESIAN MODEL AND VARIABLE EVALUATION 
11.10.2 A simple example in WinBUGS 
Example 11.4. Soft drink delivery times data (Example 5.1 revisited). Let 
us again reconsider Example 5.1. Two covariates are available, and hence four 
models can be fitted. Here we calculate the posterior and the pseudo-Bayes factors 
between the models under consideration as well as the negative cross-validatory 
log-likelihood and the logarithmic score for each model. 
The full WinBUGS code for calculation of the predictive likelihood L, the inverse CPO 
(ICPO), and PPO is available at the book’s Website. Results are presented in Table 1 1.1 1, 
where we present the logarithm of the predictive likelihood, the cross-validatory predictive 
log-score LScv (m), the log-score LS(m), and the logarithms of the posterior and pseudo 
Bayes factor of each model compared to the one with both covariates in the model (model 
m4). Finally, the last two columns correspond to the model weights based on the posterior 
and pseudo Bayes factors. All results have been generated with the same prior as in Section 
11.4.2 with c2 = 10, using 1000 iterations as burnin and an additional 5000 iterations as a 
final sample. 
Table 11.11 
Results based on the posterior predictive measures for soft drink data (Example 
1 1.4) 
ml (constant) 
-104.3 
-107.9 
-102.7 
-39.5 
-34.8 
0.0000 0.0000 
m3 (distance) 
-84.7 
-88.9 
-83.7 
-20.0 
-15.9 
0.0000 0.0000 
m4 (cases + distance) 
-64.8 
-73.0 
-63.1 
0.0 
0.0 
0.9985 0.9954 
m2 (cases) 
-7 1.2 
-78.4 
-69.4 
-6.5 
-5.4 
0.0015 0.0046 
Key: PBF,,,, 
and PSB,,,, 
= posterior and pseudo-Bayes factors of model m versus model m; 
W ~ B F  and w p , ~ ~  
= weights based on posterior and pseudo-Bayes factors calculated by @ B F ( ~ )  = 
PBF,,,,/ c“,=, 
P B F m , m 4  and ~ P S B ( ~ )  
= PSBm,m,/ c“,=, 
PSBmk,m4. 
The model with both covariates is fully supported. In particular, the weights of both 
posterior and pseudo-Bayes factors are concentrated (> 0.99) in this model, which is less 
parsimonious than the corresponding one supported by the analysis using the usual posterior 
model probabilities and the corresponding Bayes factors. Note that these measures are not 
sensitive to the prior variance controlled by c2. Results for both c2 = 10 and c2 = 1000 
are identical, and therefore we present results only for the first prior choice. 
11.1 1 INFORMATION CRITERIA 
The two most popular information criteria used in Bayesian statistics are: the Bayes informa- 
tion criterion (BIC), (Schwarz, 1978) and the Akaike information criterion (AIC), (Akaike, 
1973). More recently, the deviance information criterion was introduced by Spiegelhalter 
et al. (2002), as an extension of AIC; for details, see Section 6.4.3. In simple one-stage 
models, AIC and DIC are identical. Differences occur in hierarchical and latent variable 
models where DIC uses the number of “effective” parameters instead of the actual number 
of parameters used by AIC. Here we also focus on the Bayesian versions of AIC and BIC 
as described by Brooks (2002). 

INFORMATION CRITERIA 
425 
11 .I 1.1 The Bayes information criterion (BIC) 
The Bayes information criterion (BIC) is based on the criterion originally introduced by 
Schwarz (1 978), given by 
1 
s o l  = log(f(yIGrn,, ml) - log(f(ylgmol mo) - 2 (dml - dmo) log(n), 
(11.32) 
h 
where n is the sample size, 8, are the maximum likelihood estimates of parameters On, of 
model m and d, is the dimension of 8,. The main property of the Schwarz criterion S it 
that 
(11.33) 
and therefore it can be used to obtain a rough approximation of the log-Bayes factor under 
a wide family of prior distributions; see Kass and Wasserman (1 995) and Kass and Raftery 
(1995) for more details and Kuha (2004) for a nice review and description of BIC. The BIC 
value for a model m is defined as 
BIC(m) = D(Gm, m) + d, logn 
where D(Gm, m) is the deviance measure of model m as defined in Section 4.2.5. From 
this,it is evident that BIC is a penalized deviance (or log-likelihood) measure with penalty 
equal to logn for each parameter estimated by the model. From this last equation, the 
connection between BIC and the Schwarz criterion (1 1.32) is clear and can be expressed as 
whereas from (1 1.33) we obtain 
-2 log Bol M BIC(mo) - BIC(m1) = ABICol 
(1 1.34) 
for large n. Since BIC is related to the Bayes factor, we can obtain approximate posterior 
model probabilities by the expression 
exp ( - ; B I C ( ~ ) )  
f (mlv) = 
(1 1.35) 
c exp (- ;BIC(~!)) 
m'EM 
assuming the uniform prior distribution for all models under consideration. Moreover, 
we may use Table 1 1.1 (Kass and Raftery, 1995) to interpret differences between models. 
Hence a rule of thumb as follows: If the BIC difference between two models is lower than 
2, then we cannot discriminate between the two compared models. BIC differences from 
2 - 6,6 - 10 and higher than 10 express positive, strong, and very strong evidence in favor 
of the model with the lower BIC value. 
In the expression of BIC, although as sample size n we frequently use the dimension of 
the y vector, Raftery (1 9964 defines it as the dimension of y in normal models, as the sum 
of all Bernoulli trials in binomial models and as the sum of all counts in Poisson models. 
For further details on the Schwarz approximation, see Kass and Wasserman (1 995), Raftery 
(1 9964, and Pauler (1 998). 

426 
BAYESIAN MODEL AND VARIABLE EVALUATION 
11.1 1.2 The Akaike information criterion (AIC) 
The AIC statistic was introduced by Akaike (1973) as an approximation of the expected 
Kullback-Leibler distance between a true model and an estimated model. In his subsequent 
research, Akaike (1974) extends his work to time series models and proposes using the 
minimum AIC to select a model. 
Generally the AIC and BIC have different motivations. While BIC is a rough approxi- 
mation of the log-Bayes factor, AIC is an approximately unbiased estimator of the expected 
Kullback-Leibler (KL) distance between true and estimated models and supports models 
that have predictive performance equivalent to the true performance. Since AIC is one of 
the approximately unbiased estimators the KL distance, a wide variety of other estimators 
have been proposed in the literature; see Kuha (2003) for AIC and related methods. 
The Akaike information criterion is defined as 
h 
AIC(m) = D(f3,, m) + 2d,, 
and therefore it is also a penalized deviance measure with penalty equal to 2 for each 
estimated parameter. The penalty induced by AIC is lower than the corresponding one 
imposed by BIC for a reasonable sample size (n > 7), and hence AIC supports less 
parsimonious models than does BIC. AIC is closely related to the C, criterion of Mallows 
(1973), RZdj used in normal regression models and results obtainedusing the leave-one-out 
cross-validation method (Stone, 1977); see also Kuha (2004) for a related discussion. 
Model weights based on AIC can be obtained by the expression 
exp (- ; A I C ( ~ ) )  
f ( 4 Y )  
M 
(1 1.36) 
Cm’m 
exp ( - ; A m ) )  ’ 
with the Bayesian interpretation that it will correspond to posterior model odds when the 
prior model weights 
f(m) exp (${ lodn) - 1)) 
are adopted and BIC can be considered as a good approximation of the Bayes factor (Burn- 
ham and Anderson, 2004). 
Burnham and Anderson (2004) suggest that all models with AIC difference < 2 from the 
best one must be reported as equally “good” models having substantial support (evidence) 
against the remaining ones. Moreover, models with AIC difference ranging between 4 and 
7 are weakly supported by the data, while models with difference > 10 are not supported 
at all. This rule of thumb is comparable to the one proposed by Kass and Raftery (1995) 
for the Bayes factor and can be motivated by the Bayesian interpretation of AIC described 
in the previous paragraph. 
Finally, the deviance information criterion (DIC), more recently introduced by Spiegel- 
halter et al. (2002), can be considered as the Bayesian analog of AIC. Its justification is 
similar to that for AIC, but the expectations used in its derivation are now with respect to 
parameters instead of sampling distributions used in the original derivation of the latter. 
The importance of DIC is that it can be directly calculated from an MCMC output and can 
be applied in a much wider variety of models, including hierarchical and latent variable 
models, where the number of estimated parameters is unclear. 

INFORMATION CRITERIA 
427 
11.1 1.3 Other criteria 
A wide variety of penalized likelihood or deviance criteria is available in the statistical 
literature. The alphabet of such information criteria (IC) is described by Kuha (2004) and 
includes AIC, BIC, and DIC (already discussed); for more details, see Ntzoufras (1999q 
sec. 2.3), Kuha (2004), Konishi and Kitagawa (2008), and references cited therein. 
Generally, most information criteria minimize the quantity 
IC(m) = D(G,, m) + d,F, 
(1 1.37) 
where F is the penalty imposed on the deviance measure for each additional parameter used 
in the model. Different penalty functions result in different criteria such as for F = 2 and 
F = log(n), for which we obtain AIC and BIC, respectively. 
If we wish to compare two models, m o  and ml, then we select the one that has a lower 
value of IC, and therefore we may use the corresponding difference AIC 01 between the IC 
values for the two compared models 
A 
AICo1 = O(&n,, mo) - O(&n,, m1) - 
- d,")F. 
(11.38) 
Without loss of generality, we assume that d ( m 0 )  < d(m1). Note that if AICOI < 0 
we select model mo and if AICo1 > 0, we select model ml. We can generalize the 
abovementioned criterion difference by substituting the expression [d(m 
1) - d(mo)]F by 
a more complicated penalty function $. 
Shao (1993) divides the information criteria into two major categories: (1) criteria that 
are asymptotically valid under the assumption that a true model exists and (2) criteria that 
are asymptotically valid under the assumption that a true model does exists. Generally, 
information criteria with the penalty F fixed as n --f 00 (such as AIC) and criteria with 
F + rx as n + 00 (such as BIC) are two differently behaving categories ofcriteria, usually 
referred as AIC-like and BIC-like criteria. Criteria that attempt to compromise between 
the two approaches also exist. The main argument in favor of BIC-like criteria is that the 
existence of a true model is doubtful, and even if it does exists, we may prefer to select a 
simpler model that approximates sufficiently the true one. 
All information and model selection criteria consider both the goodness of fit ofthe model 
and the parsimony principle. The first is measured by the log-likelihood ratio and the latter, 
by the number of model parameters. In other words, they aim to facilitate selection of the 
model, which describes the data as accurately as possible but with the simplest possible 
model structure. Each information criterion differs in the weight that it gives to each of 
these two principles. 
Comparison of information criteria, posterior odds, and likelihood ratios and their con- 
nections are provided by Atkinson (198 1) and Chow (198 1). Note that Bayes factor variants 
and Bayesian predictive or utility-based criteria are (in most cases) equivalent to information 
criteria. For example, the predictive criterion L , 
of Ibrahim and Laud (1994) is equivalent 
to an information criterion (1 1.38) with penalty hnction 
n 
ri - d,, 
- 2 
F =  
while the posterior Bayes factor is approximately equal to a criterion (1 1.38) with penalty 
equal to F = log(2) (Aitkin, 1991; O'Hagan, 1995). Other interesting criteria have been 
introduced by George and Foster (2000) using empirical Bayes methods, Bernard0 (1 999) 
using Bayesian decision theory, and Gelfand and Ghosh (1998) using a predictive loss 
approach. 

428 
BAYESIAN MODEL AND VARIABLE EVALUATION 
11.1 1.4 Calculation of penalized deviance measures from the MCMC 
output 
In terms of the Bayesian approach, we can calculate the posterior distribution of any penal- 
ized deviance measure of the form 
IC(m) = D(O,, m) + d,F 
from the MCMC output and then obtain posterior summaries as usual. 
All information criteria of type (1 1.37), including AIC and BIC, can be approximately 
estimated from an MCMC output using the minimum deviance value over the generated 
parameter values O(t) for t = 1, . . . , T. Hence 
IC(m) M min 
D(Ok), m) + d,F 
. 
t=1, ..., T 
Note that this value may be close to the correct minimum value but it will never be exactly 
equal to the correct minimum value since MCMC is a sampling, and not an optimization, 
algorithm. Nevertheless, the minimum deviance value obtained from an MCMC output with 
a large number of generated iterations usually provides sufficiently accurate results. Even 
more accurate results can be obtained ifwe use the posterior mean or median obtained from 
a posterior distribution with a flat prior distribution instead of the maximum-likelihood 
estimates used in the formal definition of IC (Raftery, 19966). Hence we may estimate 
IC(m) with 
where a, 
is the posterior mean under model m. The posterior mean generally offers 
an accurate approximation, provided that the prior information is vague and the resulting 
posterior distribution is relatively symmetric to the mean. 
Another estimate of of the minimum deviance (and hence of AIC and BIC) can be 
obtained via calculation of the posterior Bayes factor. As we have described in Section 
1 1.10, the PBF can be simply estimated by the posterior mean of the likelihood f(ylO ,, 
m) 
from an MCMC output. Hence we may obtain an estimate of the minimum deviance by 
&k3(O,,m) = -2logPBF, 
-d,log(2) 
= -2logf(y1O,,m) 
-d,log(2) 
Z(m) = D(G,, m) + d,F, 
- 
and of an information criterion of type (1 1.37) by 
- - 
IC(m) = minD(O,, m) + d,F 
. 
Alternatively, we can obtain the whole posterior distribution of AIB or BIC as proposed 
by Brooks (2002) and compare, for example, their 95% posterior credible intervals to de- 
cide whether two models can be considered of equal value. Model weights can also be 
calculated for any posterior summary of AIC, BIC, or DIC, in order to obtain compara- 
ble measures inside the zero-one interval. Nevertheless, these weight do not have any 
theoretical justification. 
11.1 1.5 Implementation in WinBUGS 
Implementation in WinBUGS is straightforward. The sample from the posterior distribution 
of AIC and BIC can be obtained using the simple syntax 

INFORMATION CRITERIA 
429 
Although the deviance node is automatic, specified internally in WinBUGS , it cannot be 
used inside the model code to define other nodes or variables. For this reason we redefine 
a Deviance node as minus twice the sum of all log-likelihood components. The minimum 
value of nodes A I C  and BIC can be obtained after calculating the minimum deviance from 
the MCMC output in a statistical program outside WinBUGS . Similarly, after calculating 
the posterior means (or medians) using WinBUGS , the meadmedian-based AIC and BIC 
estimates can be calculated using software other than WinBUGS . 
11.1 1.6 A simple example in WinBUGS 
Example 11.5. Soft drink delivery times data (Example 5.1 revisited). 
Here we consider again Example 5.1 and compare the four models under consid- 
eration using the AIC and BIC values. 
The full WinBUGS code for calculation of AIC and BIC is available at the book's Web- 
site. MCMC-based estimates of the minimum deviance for each model are presented in 
Table 1 1.12, while the corresponding posterior summaries are given in Table 1 1.13. Simi- 
larly, estimates of the minimum values of AIC and BIC for each model are given in Table 
1 1.14, while their posterior distribution is depicted in Figures 1 1.2 and 1 1.3. All results 
have been generated with the same prior as in Section I 1.4.2 with c = 1000, using 1000 
iterations as burnin and an additional 5000 iterations as a final sample. The minimum 
deviance value was estimated using 20,000 and 100,000 iterations in order to monitor the 
improvement in estimated minimum deviance, based on the minimum value of the sample. 
From Table 1 1.12 we can see that the minimum value from an MCMC sample approaches 
sufficiently the actual minimum deviance; see the first column of Table 1 1.12. Although 
longer chains increase the accuracy of the estimate, even in samples of 5000 iterations, 
the correct values were calculated with accuracy of one decimal digit. Estimates based on 
the posterior mean and median are also very close to the actual minimum deviance value. 
Finally, although the PBF-based approach provides estimates close to the actual AIC and 
BIC values, they are less accurate than the estimates using the previous approaches. 
Table 11.12 
comparison for soft drink data (Example 1 1.5) 
Minimum deviance values estimated from MCMC output for models under 
Model (m) 
ml (constant) 
2 
207.049 
207.049 
207.049 
207.049 
207.049 207.058 207.263 
m2 (cases) 
3 
140.395 
140.401 
140.401 
140.395 
140.399 140.408 140.428 
mg (distance) 
3 
167.421 
167.431 
167.424 
167.423 
167.422 167.430 167.407 
m4 (cases +distance) 
4 
126.829 
126.872 
126.848 
126.847 
126.843 126.852 126.792 
Key: Min(SK), Min(20K), Min( 1 OOK) correspond to minimum deviance values from MCMC output oflength 
equal to 5000,20,000, and 100,000 iterations, respectively (with an additional 1000 iterations discarded as 
burnin). Estimates based on the mean, median, and PBF were obtained using an MCMC chain of 5000 
iterations (and 1000 iterations burnin). 

430 
BAYESIAN MODEL AND VARIABLE EVALUATION 
Table 11.13 
(Example 11.5); MCMC results based on 5000 iterations plus 1000 iterations as burnin 
Deviance posterior summaries for models under comparison for soft drink data 
Model (m) 
Minimum 2.5% percentile Mean 97.5% percentile 
ml (constant) 
207.05 
207.11 
209.08 
214.81 
m2 (cases) 
140.40 
140.61 
143.46 
150.13 
m3 (distance) 
167.43 
167.63 
170.40 
176.85 
m4 (cases + distance) 
126.87 
127.34 
130.79 
137.95 
Table 1 1.14 presents estimates of minimum AIC and BIC. All estimates indicate that the 
model with both variables is the best one. From Figures 1 1.2 and 1 1.3 we can clearly see 
that the posterior distributions of AIC and BIC for this model are clearly lower than those 
of remaining models, indicating that this model is better than the rest. We obtain a similar 
picture of posterior weights using Eqs. (1 1.35) and (1 1.36). For AIC, model 4 is supported 
with weight equal to 0.997 using any of the estimates of Table 1 I. 14, while model 2 has 
weight equal to 0.003. Similar model weights are obtained using the 25%, 50%, and 75% 
posterior percentiles of AIC (0.9964, 0.9952, and 0.9939, respectively). Finally, model 
4 is also supported with high model weight (equal to 0.994) from BIC, which is slightly 
lower than the corresponding one of AIC because of the increased penalty used by BIC, 
supporting in this way more parsimonious models in comparison with AIC. 
Table 11.14 
based on 5000 iterations plus 1000 iterations as burnin 
AIC and BIC estimates for soft drink data (Example 11 S); MCMC results 
Model (m) 
MLE 
Min 
Post. mean Post. median 
PBF 
Akaike information criterion 
ml (constant) 
211.049 211.049 
211.049 
m2 (cases) 
146.395 146.401 
146.399 
m3 (distance) 
173.421 173.431 
173.422 
m4 (cases + distance) 134.829 134.872 
134.843 
ml (constant) 
213.487 213.487 
213.487 
m2 (cases) 
150.052 150.058 
150.056 
m3 (distance) 
177.078 177.088 
177.079 
77'14 (cases + distance) 139.704 139.747 
139.718 
Bayes information criterion 
21 1.058 
146.408 
173.430 
134.852 
213.496 
150.065 
177.087 
139.727 
2 1 1.263 
146.428 
173.407 
134.792 
21 3.700 
150.085 
177.064 
139.667 
Key: Min = estimates based on the minimum deviance of an MCMC sample; MLE, Post. mean, 
Postmedian = estimates based on the maximum-likelihood estimates, posterior means, and posterior 
medians of the model parameters; PBF = estimates obtained indirectly via calculation of posterior 
Bayes factor. 

INFORMATION CRITERIA 
431 
"i 
P 
d - 
Model 1 
Model 2 
Model 3 
Model 4 
Figure 11.2 
(Example 5.1). 
Posterior error bars of AIC measures for models under comparison for soft drink data 
Model 2 
Model 3 
Model 4 
Model 1 
Figure 11.3 
(Example 11.5). 
Posterior error bars of BIC measures for models under comparison for soft drink data 

432 
BAYESIAN MODEL AND VARIABLE EVALUATION 
11.12 DISCUSSION AND FURTHER READING 
In this chapter we have briefly described Bayesian model selection methods with more 
focus on variable selection. We have described how to estimate the marginal likelihood of 
a model and the corresponding posterior model odds of two competing models. Variable 
selection methods are also described and illustrated using simple examples. Transdimen- 
sional methods for Bayesian model selection are briefly described. Finally, we conclude 
this chapter with illustration of model comparison methods based on posterior predictive 
distributions and on AIC and BIC criteria. This chapter is by no means exhaustive on the 
topic. It provides only a short introduction to the topic with emphasis on the implementation 
using WinBUGS . Advanced WinBUGS users can concentrate more on variable selection 
following details described by Dellaportas et al. (2000) and Ntzoufras (2002). Implemen- 
tation of more complicated model comparisons using Carlin and Chib’s (1995) sampler can 
be found in Spiegelhalter et al. (2003~) 
and Katsis and Ntzoufras (2005). Implementation of 
reversible jump MCMC in WinBUGS can be achieved via JUMP interface; for more details, 
see Lunn et al. (2005,2006) and the manual for this interface. 
Generally, implementation of variable and model selection methods is more complicated 
than simple estimation of model parameters and is recommended only for experienced users 
and scientists who have thoroughly comprehended at least all basic notions of Bayesian 
modeling presented in the previous chapters of this book. 
Problems 
11.1 
Estimate the marginal log-likelihood for the regression models including and ex- 
cluding the constant for the model implemented in Problem 5.1. 
11.2 
Estimate the posterior model odds of the model used in Problem 5.2 versus the 
Estimate the marginal likelihoods and the corresponding posterior weights of the 
For the data of the WinBUGS example seeds (Spiegelhalter et al., 2 0 0 3 ~ )  
used in 
Problem 7.6, estimate the marginal likelihoods of models with and without the dose 
and the log-dose. Also calculate the corresponding model weights. Use the prior 
suggested in the current chapter. 
For the data of the WinBUGS example beetles (Spiegelhalter et al., 2003b) used 
in Problem 7.9 
a) Estimate the marginal likelihoods of models with and without the drug’s concen- 
tration. 
b) Estimate model for the logit, probit, and clog-log links. 
c) Calculate the corresponding model weights. 
Use zero-one dummy variables to fit the model of Problem 5.5 and the variable 
selection methods to identify which groups can be considered equal. Use Zellner’s 
g-prior with c2 = n and an independence prior with prior variances equal to 100. 
and 6.9. 
constant model using the methods for estimating the marginal likelihoods. 
11.3 
models used in Problem 6.6. 
11.4 
11.5 
11.6 
11.7 
Apply the variable selection methods for the models implemented in Problems 6.5 

PROBLEMS 
433 
11.8 
Apply the Gibbs-based methods for variable selection for the models fitted in Prob- 
lems 7.1 and 7.2. 
Apply the Gibbs-based methods for variable selection for the models used in Prob- 
For Problems 8.1 and 8.2, calculate the marginal likelihoods for the models with 
Use the zero-ones trick and the approach of Katsis and Ntzoufras (2005) to compare 
Implement variable selection methods on the survival data of Problems 8.3 and 
Calculate the posterior Bayes factors and the negative log-likelihood for the models 
Calculate AIC and BIC measures for the models fitted in Problems 11 .l-11.4. 
11.9 
lems 7.7 and 7.8. 
11.10 
different distributional assumptions. Which distribution is more appropriate? 
the different distributions in Problems 8.1 and 8.2. 
11.1 1 
11.12 
8.4. 
11.13 
used in Problems I 1.1-1 1.4. 
11.14 

APPENDIX A 
MODEL SPECIFICATION VIA DIRECTED 
ACYCLIC GRAPHS: THE DOODLE MENU 
A.l 
INTRODUCTION: STARTING WITH DOODLE 
DOODLE is a graphical interface of WinBUGS in which we can represent our model via 
a directed acyclic graph (DAG) and then automatically generate the corresponding Win- 
BUGS code from this graph. The benefit of DOODLE is that a user, who is unfamiliar with 
programming can build a model without having to write the corresponding model code. 
In order to construct a DAG from a model, we need to specify 
The nodes representing the variables of the model 
0 The edges representing dependencies between the variables induced by the model 
Nodes and variables are depicted by rectangular or oval boxes depending on their type, 
while edges are depicted using uni-directional arrows. The head of the arrow points on the 
dependent variable, namely, the variable defined by the node or variable from which the 
arrow starts. 
To complete the model specification in WinBUGS, we further need to denote the repeat- 
ing structures, that is, actions or procedures that need to be repeated in a similar manner. An 
example of repeating structure in a statistical model is the likelihood, where every observa- 
tion is assumed to follow the same distribution with similar parameter pattern, for example, 
Y, - N(y,, 0’ for i = 1, . . . , n. These repeating structures are calledpanels in DOODLE. 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
435 

436 
MODEL SPECIFICATION VIA DIRECTED ACYCLIC GRAPHS: THE DOODLE MENU 
In order to start drawing a DAG in WinBUGS , we select Doodle > New. . . from the 
menu bar. Then, a dialog box asks the user to specify the dimensions of the window in 
which the DAG is drawn. After specifying the desired dimensions, click OK, and a window 
for drawing the graphical representation of the model will appear. 
Note that when the DAG is completed, we can generate the WinBUGS model code by 
selecting Write Code from the DOODLE menu. 
A.2 NODES 
A stochastic node is drawn when clicking (the left mouse button) in any position of the 
DOODLE window. This node is used to represent a variable of the model. When the node 
is highlighted, the properties of the specific node appear on the upper part of the window. 
The most important properties of a node are the following: 
0 Name: Insert the name of the node or variable here. 
0 Type: Set the type of the node. You can select from three types: stochastic, logical, 
and constant. Stochastic refers to random variables, logical refers to nodes that 
have been specified using deterministic expressions while constant refers to fixed 
quantities. Note that logical nodes can be either random or fixed, depending on the 
variables defining them. 
To select the type of node, click on the word type at its properties. Then a small menu 
The following node properties are type-specific: 
1. For the stochastic node we need to specify 
with the available node types will appear. 
a. Density: Set the density name from a prespecified list. Click on the word 
density for the menu to appear. 

NODES 
437 
b. Parameters: Type the values of the density parameters. For example, for the 
c. Upper and lower bounds: Set up the upper and lower bounds ofthe distribution 
normal density you have to define the mean and the precision. 
in case of censoring. 
2. For the logical node we need to specify 
a. Link: Set the link function g(z) for the logical node z. There are five available 
choices: 
(1) Identity: g(z) = z 
(2) Logit: g(z) = log(z/(l- z)) 
(3) Probit: g(z) = F1(z); where W1(z) is the inverse of the cumulative 
probability function of the standardized normal distribution 
(4) Cloglog: g(z) = log(- log(1 - x)) 
(5) log: g(z) = log(x) 
Click on the word l i n k  for the menu to appear. 

438 
MODEL SPECIFICATION VIA DIRECTED ACYCLIC GRAPHS: THE DOODLE MENU 
b. Value: Value or expression from which node x is defined. For example, if T/ is 
the value or expression of this option, we define x by the expression g(x) = 7. 
3. Finally, for a constant node we do not need to specify any additional value. The 
constant node is depicted using a rectangular box and its value is defined in the data 
section. 
The following operations pertain to nodes: 
0 Highlighting or changing the properties of a node: Just left-click on the desired 
node. Its properties will appear on the upper part of the working window. 
0 Moving a node: Click on a node and then move it by holding the left mouse button. 
0 Removing a node: Highlight the node and then press simultaneously the Control 
(Ctrl) and Delete (Del) keys. 
A.3 
EDGES 
Edges of a DAG are depicted using arrows starting at one node and ending at another one. 
They define conditional associations of the type Y IX if Y is stochastic, and of the type 
Y = g ( X )  if Y is a logical or deterministic node. 
Two nodes are connected with an edge by the following procedure 
0 Connect two nodes using an edge or arrow: Highlight the terminal node (where 
the arrow will point) and then, while holding the Control key, click on the starting 
node (where the arrow starts). 
An edge is deleted if we repeat the same procedure as above on two nodes that are already 
connected with an edge or arrow. 
a.4 PANELS 
Panels are rectangular frames representing a set of identical repeating operations (equivalent 
to the f o r  syntax). To create a panel, hold the Ctrl key and click on the point where you 
want it to appear. 

A SIMPLE EXAMPLE 
439 
The panel properties appear on the upper part of the window when the panel is selected 
or highlighted. In the properties of the panel, the user needs to specify the index name and 
its range of values. The index refers to a counter that controls how many times an operation 
within the panel is repeated. Any node included in panels will depend on the index of the 
surrounding panel. 
The full set of operations related to a panel are as follows: 
0 Create a panel: Ctrl key and left-click mouse button. 
0 Highlightkelect a panel: Left-click on the thick edges. 
0 Delete a panel: After highlighting the panel, press Ctrl and Del keys. 
0 Move a panel: After highlighting the panel, press and hold the left mouse button on 
the thick edge of the panel; move the panel using the mouse (while holding the left 
mouse button). Minor adjustments can be made using cursor keys. 
0 Resize a panel: After highlighting the panel, press and hold the left mouse button on 
the comer of the thick edge of the panel; change the size and the shape of the panel 
using the mouse (while holding the left mouse button). 
A.5 A SIMPLE EXAMPLE 
Let us consider the following simple model 
y, - N ( p i , T - I )  
a = N(p,,100) 
pi 
= a+bXi for i = l ,  ..., n 
b - N(O,100) 
T - gamma(0.1,0.1) 
u = I/& 
p, 
= 0.0 
n = 10 

440 
MODEL SPECIFICATION VIA DIRECTED ACYCLIC GRAPHS: THE DOODLE MENU 
In this model we have the following nodes: 
1. Yi for i = 1, . . . , n: stochastic nodes 
2. pi for i = 1 , . . . , n: logical nodes 
3. r: stochastic node 
4. Xi for i = 1, . . . , n: constant nodes 
5. n: constant node 
6. a, b: stochastic nodes 
7. X i  for i = 1 , . . . , n: constant nodes 
8. 0: logicalnode 
9. pa: constant node 
The relationships (edges in the graph) are defined as follows 
1. Y,lpi,r resultsin ( p i , r )  + Y,. 
2. pi is a function of a, b and X i  hence (Xi, 
a, b) + pi. 
3. a depends on pa; hence a ---f pa. 
4. u is a transformation of r hence r + u. 

A SIMPLE EXAMPLE 
441 
We further need one panel with index a moving from 1 to n that is used to specify the 
likelihood of the model. Nodes Yi, Xi, and pi must be embedded in this panel. 
Table 1.1 provides all details used for each parameter of the above model. 
Table 1.1 Detailed description of node options used in DAG of Appendix A 
Node 
Name 
Type 
Distributiona 
Parametersa 
Valueb 
y Cil 
m i  [il 
ta u 
x [il 
a 
b 
sigma 
mu.a 
Stochastic 
Logic a 1 
Stochastic 
Constant 
Stochastic 
Stochastic 
Logic a I 
Constant 
dnorm 
dgamma 
dnorm 
dnorm 
- 
~ 
- 
mean: muhl 
shape: 0.1 
- 
mean: mu.a 
mean: 0.0 
- 
- 
precision: t a u  
scale: 0 . 1  
precision: 0.01 
precision: 0.01 
- 
- 
- 
- 
- 
a+b*x [il 
- 
- 
l / s q r t  (tau) 
- 
aOptions for stochastic nodes. 
bOptions for logical nodes. 
Finally, wegenerate the WinBUGScodeofthemodelbythemenu Doodle>Write Code, 
which results in the following sequence: 
Further examples of DAGs can be found in WinBUGS examples (Spiegelhalter et al. 
2003a-c), where each example is accompanied by the corresponding graph. 

APPENDIX B 
THE BATCH MODE: RUNNING A MODEL 
IN THE BACKGROUND USING SCRIPTS 
B.l 
INTRODUCTION 
After having set up the model, we frequently need to perform the same analysis multiple 
times. The use of dialog menus described in Chapter 3 requires interaction by the user each 
time we run a model. Alternatively, the user may be facilitated by the batch mode in which a 
set of commands (script) performs the same operations as those performed via menus. The 
batch mode is the corresponding background running mode in the classic BUGS. A list of 
the available commands is provided in theWinBUGS manual (Spiegelhalter et al., 2003d). 
In the following we illustrate the most common commands using a simple example. 
Four different types of files must be prepared in order to run a procedure in the batch 
mode: 
1. A file with the model code and definition 
2. One (or more) files including the data 
3. One (or more) files including the initial values of the parameters 
4. A script file with the commands that correspond to the WinBUGS menus and dialog 
boxes. The user can run more than one script file with the assistance of the script 
command. 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
443 

444 
THE BATCH MODE: RUNNING A MODEL IN THE BACKGROUND USING SCRIPTS 
All files can be saved in either text or WinBUGS format, with . t x t  or . odc extensions, 
respectively. 
Finally, in the last version of WinBUGS , the shortcut BackBUGS runs directly all com- 
mands contained in the file s c r i p t .  odc, which is located in the directory of WinBUGS . 
B.2 BASIC COMMANDS: COMPILING AND RUNNING THE MODEL 
Lets us consider the simple example of Section 4.1. We first need to set up the files needed 
for the script mode. We save the model code under the name model. odc, the data under 
the name data. odc, and the initial values with the name i n i t i a l .  odc in a directory with 
path PATHNAME. Here we have considered the path C: /myexample/ . In order to run a 
WinBUGS full model in script session, you need to 
1. Open a log file (or window) where the WinBUGS output will be saved using the 
command display(’1og’). 
2. Check the model’s syntax, load the data, compile the model, and finally load the 
initial values using the commands 
3. Update the chain with burnin iterations (1000 iterations here) using the command 
update (1000). 
4. Set the monitored nodes using the command s e t  (nodename). 
5. Use the command s e t .  d i c  0 to start calculating DIC value. 
6. Update the chain with the iterations that will be kept for analysis. 
7. Produce posterior analysis. For example, the command s t a t s  (nodename) calcu- 
lates posterior summaries for nodename from the values sampled in step 6, while the 
command stats (*) provides posterior summaries for all monitored nodes. 
8. The command dic. s t a t s  0 calculates the DIC value. 
9. The command CODA saves the sampled values for a node in a file or in a window. 
The syntax is CODA (nodename, ’ filename ’ ) . If instead of the name of a specific 
node we use the asterisk (*), then the values of all monitored nodes are saved. The 
command saves at least two files: f ilenamei . t x t  and f ilenameIndex. t x t .  The 
first file includes the sampled values for the first chain. If two or more chains are 
generated, then the corresponding values are stored with different numbers. The 
format of a CODA file contains only two columns. The first refers to the iteration 

BASIC COMMANDS: COMPILING AND RUNNING THE MODEL 
445 
number and the second, to the sampled value. If more than one node is saved, then 
the values of the nodes are stacked sequentially. The ordering of the saved nodes 
is denoted in the second file (f ilenameIndex . txt), which has three columns: the 
node name and the position (starting line and the ending line) of the sampled values 
of each node. Finally, if the filename is left blank, then all values appear in separate 
windows instead of being saved them to a file. 
The full syntax for the example used here is given as follows: 

APPENDIX C 
CHECKING CONVERGENCE USING 
CODNBOA 
C.l INTRODUCTION 
The convergence of an MCMC algorithm is an important issue for the correct estimation of 
the posterior distribution of interest. A problem with MCMC methods is that convergence 
cannot always be diagnosed as clearly as in optimization methods. The user must specify 
both the length of the bumin period and the size of the MCMC output that will be used for the 
posterior analysis (i.e., the number of iterationsiobservations needed to keep for analysis). 
A secondary, but also important, problem is specification of the thinning interval, that 
is, the number of iterations we need to discard until two successive observations become 
independent. Graphical methods for monitoring convergence have been described in Section 
2.2.2.4. Furthermore, low Monte Carlo errors are also essential for precise estimation ofthe 
posterior distribution. More formal diagnostic tests have been developed in the literature 
and can be implemented using CODA [convergence diagnosis and output analysis software 
for Gibbs sampling analysis (Best et al., 1996; Plummer et al., 2006)] or BOA [Bayesian 
output analysis Smith (2005; 2007)] packages; both are available in R. 
In this appendix we briefly describe the diagnostics used by CODA and BOA and illus- 
trate their use with a simple example. 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
447 

448 
CHECKING CONVERGENCE USING CODNBOA 
C.2 A SHORT HISTORICAL REVIEW 
According to Plummer et al. (2006), the original version of CODA was written for Splus 
as a companion to a review by Cowles and Carlin (1996). CODA was later adopted by 
the BUGS team to serve as a companion to the classic BUGS (Spiegelhalter et al., 1996a), 
which had limited output facilities. Under this team, versions 0.30 (Best et al., 1996) and 
0.40 (Best et al., 1997) were released. With the development ofR and its growing popularity, 
CODA was rewritten in 2000 by Brian Smith (Smith, 2005) to form the package known 
as Bayesian output analysis (BOA). In parallel, the CODA package was also developed by 
Martyn Plummer in collaboration with the original CODA team (Plummer et al., 2006). 
Note that both packages are menu driven, with CODA also providing the possibility to 
separately call each diagnostic, making it extremely appealing to experienced users who 
wish to embody such commands in their own MCMC code. 
In the following, we briefly present the four diagnostics used by CODAiBOA in R and 
illustrate them (in R) using a simple example. 
C.3 DIAGNOSTICS IMPLEMENTED BY CODNBOA 
CODA provides four diagnostic tests suggested respectively by Geweke (1992), Gelman 
and Rubin (1992), Raftery and Lewis (1 992), and Heidelberger and Welch (1 992). The same 
tests are also implemented in BOA, but appear in a different ordering at the corresponding 
diagnostics menu. Details ofthese diagnostics can be found in Best et al. (1 996) and Cowles 
and Carlin (1 996). 
C.3.1 The Geweke diagnostic 
Geweke (1992) suggested a diagnostic test for checking the convergence of the mean of 
each parameter separately from the sampled values of a single chain. To construct this test, 
he proposed viewing the set of simulated values, obtained by the MCMC output, as a time 
series. This diagnostic applies a simple 2 test to check whether the means estimated from 
two different subsamples of the total MCMC output are equal. These subsamples refer to 
observations coming from the beginning and the end of the generated chain. Hence, if the 
means at the beginning and the end ofthe total MCMC output are rejected, then convergence 
of the chain cannot be assumed. CODAiBOA compare by default the initial 10% and the 
last 50% of the total iterations. For the asymptotic variance of the sample mean, an estimate 
from spectral density theory is adopted. Hence, for a set of generated values 
of a parameter of interest 8, the sample mean eis calculated, followed by calculation of the 
spectral density SQ(W) 
for these time series. Then, the standard error of the mean is given 
by d
m
 
and the corresponding 2 diagnostic by 
$A - p 
z= 
Jst (o)/TA + s/ (o)/TB ' 
-A 
-B 
which will asymptotically follow the standardized normal distribution, where B , B 
are 
the sample means for the two subsamples described above: TA,TB are the corresponding 

DIAGNOSTICS IMPLEMENTED BY COONBOA 
449 
sizes; and S$(O)/TA, 
S f ( 0 ) l T ~  
the corresponding variances of the sample means. This 
expression can also be used for any function g ( 0 ) .  
Since Z asymptotically follows the standardized normal distribution, values that lie in 
its tails provide an indication of nonconvergence (Best et al., 1996). To be more specific, 
parameters with / Z /  > 2 indicate differences in the means between the first and the last 
set of iterations and hence nonconvergence. Nevertheless, because of the type I error of 
classical significance tests, in multiparameter models, we allow a 5% of the calculated 2 s  
to lie outside this range. 
C.3.2 The Gelman-Rubin diagnostic 
The Gelman-Rubin diagnostic (Gelman and Rubin, 1992) involves checking the conver- 
gence of the chain using two or more samples generated in parallel. This test is an ANOVA- 
type diagnostic, calculating a shrinking factor R as described in Section 4.3.3. Values of R 
close to one indicate convergence. 
C.3.3 The Raftery-Lewis diagnostic 
The third diagnostic test was proposed by Raftery and Lewis (1992). It can be applied to 
output coming from a single chain, and it focuses on achieving a prespecified degree of 
accuracy of specific quantiles rather than the convergence of the mean. The default measure 
in CODA is the 2.5% percentile estimated with accuracy 0.005 and probability 0.95. CODA 
reports Nnlin, M ,  B, and I ;  more specifically: 
0 Nmin is the minimum number of iterations required to estimate the quantile of interest 
with the prespecified accuracy under the assumption of independence (i.e., with zero 
autocorrelation). 
0 N is the total number of iterations that the chain must run. 
0 M is the number of burnin iterations. 
0 I is the dependence factor given by I = N/Nmi,,, which indicates the relative 
increase of the total sample due to autocorrelations. If I is equal to one, then the 
generated values are independent. On the other hand, values greater than 5 often 
indicate a problematic behavior; for details, see Best et al. (1996). In general I can 
be considered as a rough estimate of the required thinning interval. 
Note that if the current number of generated observations T is lower than N,,i,, 
then an 
error is printed and the remaining values are not estimated. This diagnostic is very detailed, 
giving valuable information regarding the length of the sample. 
C.3.4 The Heidelberger-Welch diagnostic 
The last convergence diagnostic was proposed by Heidelberger and Welch (1992). It is 
used for the analysis of single chains from univariate observations. The test consists of two 
parts and is based on ideas from Brownian bridge theory. This diagnostic tests whether 
stationarity of the Markov chain is attained using the values from an MCMC output (null 
hypothesis). If this hypothesis is rejected, then the first 10% of the total iterations is 
discarded and the test is repeated on the remaining sample. The procedure is repeated by 

450 
CHECKING CONVERGENCE USING CODNBOA 
dropping additional 10% of the sample until the test of stationarity is not rejected or more 
than 50% of the sample is discarded. In the latter case, the null hypothesis is rejected and 
more iterations are required. If the test is not rejected (i.e., if it is passed following the 
terminology of CODA/BOA), then the number of iterations that have been used to pass the 
test, the discarded iterations, and the Cramer-von Mises statistic are reported. In the second 
part ofthe diagnostic, the halfwidth test is implemented. The portion of the chain that passed 
the stationarity test is treated as time series, from which we estimate the spectral density at 
zero S(0). Then, the asymptotic standard error of the mean is given by , / m a s  in the 
Geweke (1992) diagnostic; where N ,  is the length of the retained chain. If the halfwidth of 
the 95% confidence interval of the mean, evaluated with the asymptotic standard error, is 
less than E times the sample mean, the halfwidth test is passed, where E is a small fraction 
with CODA default value equal to 0.1. In the opposite case, the halfwidth test reports failure 
and a longer chain must be considered to achieve the required precision of the parameter of 
interest. 
C.3.5 Final remarks 
All convergence diagnostics work like “alarms” that detect certain problems concerning 
the convergence of the chain. Since the focus of each diagnostic is different, to ensure 
convergence all tests must be passed (not rejected). Nevertheless, the required precision of 
the quantiles of the diagnostic of Raftery and Lewis (1 992) must be adjusted according to 
the scaling of each variable. 
C.4 A FIRST LOOK AT CODNBOA 
Here we describe the main menus of CODA/BOA and present the required actions to import 
the data and obtain each diagnostic test. Additional descriptive measures and plots are 
available in both packages; for details, see Best et al. (1 996) and Smith (2005), respectively. 
C.4.1 CODA 
After downloading, installing, and loading the CODA package in R, we call the main menu 
by the command 
The following text menu appears where two selections are available: (1) importing data 
from a WinBUGS output file (obtained using CODA option in the Inf erence>Samples 
tool) or (2) data with the conventional CODA format (usually saved from previous CODA 
session). 
For the first selection, an index file (with the positions of each stored variable) and at 
least one output file must be provided by the user. In the second selection, the name of an 
R object must be provided. This object must have the same format as data saved after a 
CODA session. The data of a CODA session are saved by default with the name coda. dat. 

A FIRST LOOK AT CODABOA 
451 
This object is a list with the data of each chain stored in a matrix (each row is an iteration 
and each column a variablehode). 
Once the data are imported in CODA, the following menu appears on the screen 
Selection of the second option provides a list of available diagnostics 
Options 1 4  apply each of the diagnostics described in the previous section, while op- 
tions 5 and 6 provide some graphical details concerning autocorrelations and correlations 
across different parameters. For more details regarding the additional available analysis 
and options of CODA, see the CODA manual (Best et al., 1996) available through the 
WinBUGS Webpage. 
Exiting CODA will save the data of the current session under the name coda. dat. Store 
this file in another object in case you need to use it again. 
C.4.2 
BOA 
There are several differences between the menus of BOA and CODA. In the following 
we present the main menus of BOA. Note that detailed documentation of BOA (including 
examples) is provided in Smith (2005; 2007). 
After downloading, installing, and loading the BOA package in R, we call the main menu 
with the command 
Then the following initial menu of BOA appears 
To import WinBUGS files, select 1 (File) and from the next menu, select 3 (Import Data): 

452 
CHECKING CONVERGENCE USING CODNBOA 
WinBUGS output files are loaded with option 3 (CODA Output Files), after we specify 
the working directory in 7 (Options. . .). In BOA, unlike CODA, output and index files 
must have the same names with different extensions for each file: . out for the output files 
and . ind for the index file. Return to the BOA MAIN MENU by typing 1, enter twice, and 
then select 3 (analysis) to obtain the analysis menu: 
Select 4 to obtain the convergence diagnostics menu 
The diagnostics displayed above are the same as in CODA but in a different order (they are 
arranged alphabetically). 
In BOA, data are not stored by default in an R object as in CODA. We can store the 
whole BOA session by the Save Session option of the F i l e  Menu (option 1 in the main 
menu). By this option a list object is stored with extensive details concerning the current 

A SIMPLE EXAMPLE 
453 
BOA session. The data are available in boaobject$chain$master, which is a list like 
the one saved by CODA; where boaobject is the name of the R object where the BOA 
session is stored. 
C.5 A SIMPLE EXAMPLE 
In the following we briefly illustrate the use of CODA for obtaining the diagnostics of 
Geweke (1 992), Raftery and Lewis (1 992) and Heidelberger and Welch (1 992). Illustration 
of the Gelman-Rubin diagnostic (Gelman and Rubin, 1992) is omitted since it can be 
implemented in WinBUGS , as illustrated in Section 4.3.3. 
In the following we assume that we have generated one chain. The output file is savedun- 
der the name CODAoutput 1. t x t  and the index file, under the name CODAoutput Index. t x t .  
C.5.1 Illustration in CODA 
Firstly, we load the data using the following procedure 
1. Load the CODA package and then call the main menu with the command codamenu 0. 
2. In themenu 
select 1 (and press Enter): 
3. In the following request 
insert the name of the index file 
and then insert the name of the output file: 
The package expects a second output file, unless Enter is pressed without any other 
information. If the data are successfully loaded, then a list of the loaded nodes and 
the corresponding number of iterations will appear on the screen 

454 
CHECKING CONVERGENCE USING CODNBOA 
followed by CODA Main Menu: 
The data are now loaded and are ready to use. From the main menu, select 2 and then the 
desired model diagnostic from the following menu: 
Results of the Geweke (1992) diagnostic are obtained by selecting the first option: 

A SIMPLE EXAMPLE 
455 
From this output we observe that all 2 values are within -2 and 2, indicating no differences 
in the means for the first and last sets of iterations. Selecting the second option provides a 
graphical picture of 2 scores. 
Returning to the diagnostics menu (option 4) and selecting the Raftery-Lewis diagnostic 
(Raftery and Lewis, 1992) provides the following output: 
From this CODA output display, we conclude that a larger sample of size equal to 40,968 
iterations (which is the maximum of all N )  is required to obtain the prespecified accuracy 
for the 2.5% quantile. No additional bumin is needed since M is generally low (here 
1000 iterations have been already removed). I for some parameters is high, indicating 
high autocorrelations for these parameters and that a thinning interval may be required to 
make observations independent. For example, here we may finally consider to N x I ’ = 
3746 x 11 = 41,206 iterations with thinning interval equal to I’ = 11, finally keeping 

456 
CHECKING CONVERGENCE USING CODNBOA 
3746 independent observations; where I’ is the maximum I rounded to the closest larger 
integer number: I’ = rownd(max(1) + 0.5). 
Returning to the diagnostics menu (option 2) and requesting for the Heidelberg-Welch 
diagnostic (Heidelberger and Welch, 1992), we find the following output printed on the 
screen: 
This output indicates that all tests have been passed and convergence has been reached. 

A SIMPLE EXAMPLE 
457 
From the analysis above, some problems were traced using the Raftery-Lewis diagnos- 
tic. The user may argue that two out of three of the applied diagnostics have diagnosed 
convergence. Alternatively, if we wish to ensure convergence, we should obtain additional 
iterations according to the details of the Raftery-Lewis diagnostic. 
Finally, after returning to the main menu and exiting CODA, all values of the MCMC 
output are stored in the R object coda. dat. We recommend saving it in a different object 
for future use. For example, we can store the data in an object named exicoda with the 
command 
You may restart CODA using this object, by selecting the second option in the introductory 
menu 
and then insert the name of the object 
If the following message and the main CODA menu appear, then the data have been loaded 
successfully: 
C.5.2 
Illustration in BOA 
After downloading, installing, and loading the BOA package in R, we call the main menu 
using the command 
The initial menu of BOA gives the following options: 

458 
CHECKING CONVERGENCE USING CODAJBOA 
To import WinBUGSfiles, select 1 (File) and then 3 (Import Data): 
Selection: 
We then specify the working directory using 7 (Options. . . ): 

A SIMPLE EXAMPLE 
459 
Then load the WinBUGS data using selection 3 (CODA Output Files). The files must 
be saved under the same name with different extensions: . out and . ind for the output 
and index file, respectively. In the following the original files were renamed boa. out and 
boa. ind: 
If the data are successfully loaded, the following message appears: 
This is followed by the IMPRORT DATA MENU: 
Return to the BOA M A I N  MENU by typing 1, press the Enter key twice, and then select 3 
(analysis) to obtain the corresponding menu: 
Select 4 to obtain the convergence diagnostics menu: 

460 
CHECKING CONVERGENCE USING CODNBOA 
These diagnostics are the same as in BOA arranged in alphabetical order. Results are the 
same as in CODA. For the Geweke (1992) diagnostic, p-values are additionally provided. 

APPENDIX D 
NOTATION SUMMARY 
D.l 
MCMC 
0 B: number of burnin iterations 
0 T :  total number of iterations obtained by a simulation method 
0 T’ = T - B: total number of iterations retained after discarding the burnin period 
0 K :  number of batches used for estimation of Monte Carlo errors; also number of 
groups in implementation of Levene’s test in Chapter 10 
0 L: lagithin interval 
0 v = T’/K (or TIK): size of each batch used for the estimation of Monte Carlo error 
0 li = logf(yil@): log-likelihood 
0 IC: number of generated chains 
0 V = BSS/T’; R = V/WSS: Gelman-Rubin diagnostic 
0 WSWBSS: withinhetween-chain sum-of-squares (Gelman-Rubin diagnostic) 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
461 

462 
NOTATION SUMMARY 
D.2 SUBSCRIPTS AND INDICES 
0 i: index for subject/experimental unit (i = 1,2, . . . , n) 
0 j ,  1: indices for parameterdvariables in linear predictor 
0 e: index for levels in categorical variables taking values ! 
= 1,2, ..., L A  
0 k: subject index within each level of a categorical variable 
0 Subscript i: index for subjectdobservations 
0 Subscript \i: index for defining a vector without the ith element 
0 Subscript (i): index for i row of a matrix (used for design matrices to extract covariate 
values for i individuaVobservation); also 
8(i): parameters corresponding to i subject 
X(i): 
i row of a matrix (used for design matrices to extract covariate values for 
i individual/observation) 
y(i) : element of ith order of vector y 
0 Subscript j in a matrix ( X j ) :  j column of matrix X 
0 Subscript y, 8: random variable/parameter for which a measure (e.g., expected value 
or variance) is calculated 
0 Superscript (t): iteration number 
D.3 PARAMETERS 
0 8: parameter vector 
0 8,: parameter vector of model m 
0 8:): t observation of 8, generated from the posterior distribution 
0 8:Lt): t observation of 8, generated from the prior or an arbitrary distribution g(8 ,) 
0 8(i) : parameters for i individuaVobservation 
0 G(8): function of parameter vector 8 
0 h(8): link function 
0 ,B, ,B,: 
parameters involved in the linear predictor of the model 
0 P = ( P o , P 1 , . . . , P p ) T  
b =  (P1,...,Pp)T 

RANDOM VARIABLES AND DATA 
463 
D.4 RANDOM VARIABLES AND DATA 
0 n: samplesize 
0 p: number of explanatory variables 
0 P: number of parameters in linear predictor (usually P = p + 1) 
0 p ,  and P, : the corresponding p and P for model m 
0 d or d,: 
dimension of model m; in models where only linear predictor parameters 
are involved d = P = p + 1; normal models d = P + 1 = p + 2. 
X I ,  . . . , X,: explanatory variables 
0 zil, . . . , Xi,: data of i individual for explanatory variables X I ,  . . . , X ,  
0 x: n x p matrix with elements zi.j 
0 X = [ln, x] : desigddata matrix of dimension n x P in which the first column has 
each element equal to one (i.e. is a vector of ones: 1 ,), if the intercept is included in 
the model, and the remaining columns contain the values of each covariate X j 
0 X ( i ) ,  
~ ( ~ 1 :  
the ith row of matrices X and x, respectively 
0 Y :  response variable 
0 Y,: response variable for i individual 
0 yi: observed value of Yi for i individual 
0 y = (y1, . . . , Y,)~ vector of length n of the response data 
0 ~ ( ~ 1 :  
ith larger (order) observed value of vector y 
0 ( ?J(~), . . . , y(,)) 
vector of length n of the response data arranged in ascending order 
0 y,i = (yl, , . . 
yi+l . . . , Y,)~: 
vector of observed values without the ith ele- 
0 E(Y), V(Y) or Var(Y), SD(Y): expectation, variance, and standard deviation of Y: 
T 
ment 
E(O), Var(O), SD(O): corresponding prior measures 
E(Oly), Var(Oly), SD(O1y): corresponding posterior measures 
SD(y): sample standard deviation of vector y 
D.5 SAMPLE ESTIMATES 
0 3, si, 5;: sample mean, unbiased, and biased sample estimates of variance 
0 sy: sample standard deviation of y 
0 5 
(hat): maximum-likelihood estimate of parameter 8 
0 SS: sum of squares used in calculation of posterior quantities in normal models 

464 
NOTATION SUMMARY 
D.6 SPECIAL FUNCTIONS, VECTORS, AND MATRICES 
0 1(z): indicator function taking value equal to one if z is true and zero otherwise 
0 I k :  unit matrix of dimension k x k 
0 n: mathematical constant equal to 3.14159 
0 lk: vector of length k with all elements equal to one 
0 lTc,l[Txc~ 
: matrix of dimension r x c with all elements equal to one 
0 O k :  vector of length k with all elements equal to zero 
0 OTc: matrix of dimension r x c with all elements equal to zero 
D.7 DISTRIBUTIONS 
0 beta(a, b): beta distribution with parameters a and b 
0 Bernoulli(T): Bernoulli distribution with success probability T 
0 binomial(T, N ) ,  B(T, N ) :  binomial distribution with success probability 7r and N 
replications of a Bernoulli experiment 
0 Dirichlet(a): Dirichlet distribution with parameter vector (Y 
0 fo (y; 8): density or probability function evaluated at y of distribution D and param- 
eter vector 8 
0 fc (y; a, b): density function ofgamma distribution with parameters a and b evaluated 
at Y 
0 f ~ ( y ;  
p, c?): 
density function of normal distribution with mean ,u and variance cr 
evaluated at y 
0 gamma(a, b): gamma distribution with parameters a and b (mean a/b and variance 
alb2> 
0 IG(a, b): inverse gamma distribution with parameters a and b 
0 N ( p ,  cr’): univariate normal distribution with mean p and variance cr2 
0 Nd(p, C): d-dimensional normal distribution with mean p and variance C 
0 NB(T~ 
N ) :  negative binomial distribution with success probability 7r 
0 NG(p, c, a, b): normal gamma distribution with parameters p, c, a, b 
0 NIG(p, c, a, b): normal inverse gamma distribution with parameters p, c, a, b 
0 Poisson(X): Poisson distribution with parameter X 
U ( a ,  b): continuous uniform distribution defined in the interval (a, b) 

DISTRIBUTION-RELATED NOTATION 
465 
D.8 DISTRIBUTION-RELATED NOTATION 
0 a, b: parameters of gamma distribution with mean a/b and variance a/b2; also in 
gamma related distributions: 
ao, bo: prior parameters 
a, b: posterior parameters 
a; ;5: proposal parameters 
- 
- 
- 
0 c2, cg: variance multiplicators (used in prior or proposal) 
0 D ( a ) :  distribution 2) with parameter vector a 
0 b, u2, r: mean, variance, and precision of normal distribution 
0 PO, oi, 7 :  mean, variance, and precision of normal prior distribution 
0 ,LLL~, 
j2;: prior and posterior mean of I9 
0 j2, Z2, 7: mean, variance, and precision of normal posterior distribution 
0 p, a2, 7: mean, variance, and precision of normal proposal distribution 
0 p, E, T ,  R: mean vector, variance, precision, and correlation matrices ofmultivariate 
normal distribution 
0 pg: prior mean of ,tJ 
0 po, Co: prior mean vector and variance of a multivariate normal prior distribution 
0 6, C: posterior mean vector and variance matrix of multivariate normal posterior 
I 
distribution 
0 p, s: proposal mean vector and variance matrix of a multivariate normal proposal 
distribution 
0 N :  size in binomial distribution 
0 N,: size in i binomial case 
0 T :  success probability in binomial distribution 
0 n, : success probability in i binomial case 
0 n: mathematical constant equal to 3.14159 
0 a;, 5;: prior and posterior variances of t9 
0 og : prior variance of p 
0 zq: quantile of qlOO% of the normal distribution 
0 iT: posterior parameter 19 
0 g: maximum-likelihood estimate of t9 
0 e: t9 parameter of a proposal distribution 

466 
NOTATION SUMMARY 
D.9 NOTATION USED IN ANOVA AND ANCOVA 
0 yek: k observation of C group in ANOVA 
0 A, B, C: random factors used in ANOVA and ANCOVA models 
0 Ai, Bi, Ci: random variables for i subject/experimental unit 
0 ai, bi, ci: data codes for levelkategory of factors A, B, and C of subject/experimental 
unit 
0 at: effect of C level of factor A 
0 i: index for subjecb‘experimental unit (i = 1,2,. . . , n) 
0 j ,  1 :  indices for parametedvariables in linear predictor 
0 e: index for levels in categorical variables taking values e = 1 , 2, .. . , L A 
0 k: subject index within each level of a categorical variable 
0 ki: mean for i individual 
0 pi: mean for C group 
0 LA, LB, Lc: levels of factors A, B, and C, respectively 
0 D$, DG, DS: dummy variables using the corner constraint for factors A, B, and C, 
0 Di’STZ, DG, DZ: dummy variables using the sum-to-zero constraint for factors A, 
0 St: interaction parameters between quantitative and qualitative variables in ANCOVA 
respectively 
B, and C, respectively 
D.10 VARIABLE AND MODEL SPECIFICATION 
yj: binary variable indicators indicating inclusion of Xj in the linear predictor of the 
0 y = (yx,yz,. 
. . , yp): binary variable indicators indicating the inclusion of X j  in 
the linear predictor of the model (the length of y may be equal top or P depending 
on the model specification) 
model 
D.11 DEVIANCE INFORMATION CRITERION (DIC) 
0 DIC(m): deviance measure of model m 
0 D(O,, m) = -2 log f(ylO,, 
m): deviance measure of model m and parameters 
0 D(O,, m): posterior mean of deviance evaluated by an MCMC sample 
0 p, 
= D(O,, m) - D(G,, m): number of “effective” parameters for model m 
0, 

PREDICTIVE MEASURES 
467 
D.12 PREDICTIVE MEASURES 
0 yrep = (ypp, . . . , y?”pT vector of length n of the predictiveheplicated response 
data 
0 CV-1: leave-one-out cross-validation 
0 PPO;, CPOi: posterior and conditional predictive ordinates for individualiobserva- 
tion i 
0 ri: residual of ith observation 
0 rp: standardized residual of ith observation 
0 T : ~ ) :  residual of ith ordered observation y(i) 

REFERENCES 
Abramowitz, M. and Stegun, I. (1974), Handbook of Mathematical Functions, Dover, New York. 
Aganval, D., Gelfand, A. and Citron-Pousty, S. (2002), “Zero-inflated models with application to 
Agresti, A. (1990), Categorical Data Analysis, Wiley-Interscience, New York. 
Agresti, A. (2002), Categorical Data Analysis, 2nd ed., Wiley-Interscience, Hoboken, NJ. 
Agresti, A. and Hitchcock, D. (2005), “Bayesian inference for categorical data analysis”, Statistical 
Methods and Applications 14,297-330. 
Aguilar, O., Prado, R., Huerta, G. and West, M. (1999), “Bayesian inference on latent structure in 
time series (with discussion)”, in J. Bemardo, J. Berger, A. Dawid, and A. Smith, eds., Bayesian 
Statistics, Vol. 6, Oxford University Press, pp. 3-26. 
Aitkin, M. (1991), “Posterior Bayes factors”, Journal ofthe RoyalStatisticalSociety B 53, 11 1-142. 
Akaike, H. (1973), “lnformation theory and an extension of the maximum likelihood principle”, in B. 
Petrov and F. Csaki, eds., Proceedings of 2nd International Symposium on Information Theory, 
Academiai Kiado, Budapest, pp. 267-28 1. 
Akaike, H. (1974), “A new look at the statistical model identification”, IEEE Transactions on Auto- 
matic Control 19, 716-723. 
Albert, J. and Chib, S. (1993), “Bayesian analysis of binary and polychotomous response data”, 
Albert, J. and Chib, S. (1997), “Bayesian tests and model diagnostics in conditionally independent 
Aldrich, J. and Nelson, F. (1 984), Linear Probability, Logit, and Probit Models, Quantitative Appli- 
spatial count data”, Environmental and Ecological Statistics 9, 34 1-355. 
Journal of the American Statistical Association 88, 669-679. 
hierarchical models”, Journal ofthe American Statistical Association 92,9 16-925. 
cations in the Social Sciences, 07445, Sage Publications, Inc., Los Angeles, CA. 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 0 2 0 0 9  John Wiley & Sons, Inc. 
469 

470 
REFERENCES 
Ameniya, T. (1981), “Quantitative response models: A survey”, Journal of Economics Literature 
American Psychiatric Association (1 987), DSM-III- R: Diagnostic and Statistical Manual of Mental 
Andersen, P. and Gill, R. (l982), “Cox’s regression model for counting processes: A large sample 
Anderson, H. (1986), “Metropolis, Monte Carlo and the MANIAC”, Los Alamos Science pp. 96-107. 
Aranda-Ordaz, F. (1 981), “On two families of transformations to additivity for binary response data”, 
Atkinson, A. (1978), “Posterior probabilities for choosing a regression model”, Biometrika 65,3948. 
Atkinson, A. (1981), “Likelihood ratios, posterior odds and information criteria”, Journal of Econo- 
Barbieri, M. and Berger, J. (2004), “Optimal predictive model selection”, Annals of Statistics 32,870- 
Bartholomew, D. and Knott, M. (1999), Latent Variable Models and Factor Analysis, Kendall’s 
Bartlett, M. (1957), “Comment on D.V. Lindley’s statistical paradox”, Biometrika 44, 533-534. 
Basu, S. and Mukhopadhyay, S. (2000), “Binary response regression with normal scale mixture links”, 
in D. Dey, S. Ghosh, and B. Mallick, eds., Generalized Linear Models: A Bayesian Perspective, 
Marcel Dekker, New York, pp. 231-241. 
Baxter, M. and Stevenson, R. (1988), “Discriminating between the Poisson and negative binomial 
distributions: An application to goal scoring in association football”, Journal ofApplied Statistics 
15,347438. 
Bayarri, M. and Berger, J. (2000), “P-values for composite null models (with discussion)”, Journal 
of the American Statistical Association 95, 1127-1 142. 
Bennet, J., Racine-Poon, A. and Wakefield, J. (l996), “MCMC for nonlinear hierarchical models”, 
in W. Gilks, S. Richardson, and D. Spiegelhalter, eds., Markov Chain Monte Carlo in Practice, 
Chapman & Hall, Suffolk, UK, pp. 339-358. 
Berger, J. and Delampady, M. (1987), “Testing precise hypotheses”, Statistical Science 2,3 17-352. 
Berger, J. and Pericchi, L. (1996a), “The intrinsic Bayes factor for linear models”, in J. Bernardo, 
J. Berger, A. Dawid, and A. Smith, eds., Bayesian Statistics, Vol. 5 ,  Oxford University Press, 
pp. 2544. 
Berger, J. and Pericchi, L. (1996b), “The intrinsic Bayes factor for model selection and prediction”, 
Journal of the American Statistical Association 91, 109-122. 
Berkson, J. (1944), “Application of the logistic function to bio-assay”, Journal of the American 
Berkson, J. (1951), “Why I prefer logits to probits”, Biometrics 7,327-339. 
Bernardo, J. (1999), “Nested hypothesis testing: The Bayesian reference criterion”, in J. Bernardo, 
J. Berger, A. Dawid, and A. Smith, eds., Bayesian Statistics, Vol. 6, Oxford University Press, 
19, 1483-1536. 
Disorders, 3rd revised ed., APA, Washington, DC. 
study”, Annals of Statistics 10, 1100-1 120. 
Biometrika 68, 357-363. 
metrics 16, 15-20. 
897. 
Library of Statistics, Vol. 7, 2nd ed., Hodder Arnold Publications, UK. 
Statistical Association 39, 357-365. 
pp. 101-130. 
Bernardo, J. and Smith, A. (1994), Bayesian Theoty, Wiley, Chichester, UK. 
Besag, J. (1989), “A candidate’s formula: A curious result in Bayesian prediction”, Biometrika 
76, 183. 
Best, N., Cowles, M. and Vines, K. (1 996), CODA: Convergence Diagnostics and Output Analysis 
Software for Gibbs Sampling Output, Version 0.30, MRC Biostatistics Unit, Institute of Public 
Health, Cambridge, UK. 

REFERENCES 
471 
Best, N., Cowles, M. and Vines, K. (1997), CODA: Convergence Diagnostics and Output Analysis 
Sofmarefor Gibbs Sampling Output, Version 0.40 (addendum to manual), MRC Biostatistics 
Unit, Institute of Public Health, Cambridge, UK. 
Birch, M. W. (1963), “Maximum likelihood in three-way contingency tables”, Journal of the Royal 
Statistical Society B 25, 220-233. 
Bliss, C. I. (1935), “The calculation of the dosage-mortality curve”, Annals of Applied Biology 
Bohning, D. (1 998), “Zero-inflated Poisson models and c.a.man: A tutorial collection of evidence”, 
Bohning, D., Dietz, E., Schlattmann, P., Mendonca, L. and Kirchner, U. (1999), “The zero-inflated 
Poisson model and the decayed, missing and filled teeth index in dental epidemiology”, Journal 
of the Royal Statistical Society A 162, 195-209. 
Box, G. and Tiao, G. (1973), Bayesian Inference for Statistical Analysis, Wiley Classics Library, 
Wiley, New York. 
Brooks, S. (1998), “Markov chain Monte Carlo method and its application”, The Statistician 47, 69- 
100. 
Brooks, S. (2002), “Discussion of the paper by Spiegelhalter, Best, Carlin and van der Linde”, Journal 
of the Royal Statistical Society B 64, 6 1 6 6  18. 
Brooks, S. and Gelman, A. (l998), “Alternative methods for monitoring convergence of iterative 
simulations”, Journal of Computational and Graphical Statistics 7,434455. 
Brooks, S. and Roberts, G. (1998), “Assessing convergence of Markov chain Monte Carlo algorithms”, 
Statistics and Computing 8, 3 19-335. 
Brown, H. and Prescott, R. (2006), Applied Mixed Models in Medicine, Statistics in Practice, 2nd ed., 
Wiley, Chichester, UK. 
Brown, P., Vannucci, M. and Fearn, T. (1998), “Multivariate Bayesian variable selection and predic- 
tion”, Journal of the Royal Statistical Society B 60, 627-641. 
Browne, W., Subramanian, S., Jones, K. and Goldstein, H. (2005), “Variance partitioning in mul- 
tilevel logistic models that exhibit overdispersion”, Journal of the Royal Statistical Society A 
Burnham, K. and Anderson, D. (2004), “Multimodel inference: Understanding AIC and BIC in model 
selection”, Sociological Methods Research 33, 26 1-304. 
Carlin, B. (1996), “Hierarchical longitudinal modelling”, in W. Gilks, S. Richardson, and D. Spiegel- 
halter, eds., Markov Chain Monte Carlo in Practice, Chapman &Hall, Suffolk, UK, pp. 303-320. 
Carlin, B. and Chib, S. (1995), “Bayesian model choice via Markov chain Monte Carlo methods”, 
Carlin, B. and Louis, T. (2000), Bayes and Empirical Bayes Methods for Data Analysis, Texts in 
Casella, G. and George, E. (1992), “Explaining the Gibbs sampler”, The American Statistician 
Chen, D. (2007), “Bootstrapping estimation for estimating the relative potency in combinations of 
Chen, M.-H. (2005), “Computing marginal likelihoods from a single MCMC output”, Statistica 
Chen, M., Ibrahim, J. and Shao, Q. (2000), “Power prior distributions for generalized linear models”, 
Cheung, Y. (2002), “Zero-inflated models for regression analysis of count data: A study of growth 
22, 134-167. 
Biometrical Journal 40, 833-843. 
168,599-613. 
Journal of the Royal Statistical Society B 157, 473434. 
Statistical Science, Chapman & HaWCRC, New York. 
46, 167-174. 
bioassays”, Computational Statistics and Data Analysis 51,45974604. 
Neerlandica 59, 16-29. 
Journal ofStatistica1 Planning and Inference 84, 121-137. 
and development”, Statistics in Medicine 21, 1461-1469. 

472 
REFERENCES 
Chib, S. (1995), “Marginal likelihood from the Gibbs output”, Journal of the American Statistical 
Chib, S. and Greenberg, E. (1993, “Understanding the Metropolis-Hastings algorithm”, American 
Chib, S. and Jeliazkov, I. (2001), “Marginal likelihood from the Metropolis-Hastings output”, Journal 
Chib, S. and Jeliazkov, I. (2005), “Accept-reject Metropolis-Hastings sampling and marginal likeli- 
Chipman, H. (l996), “Bayesian variable selection with related predictors”, Canadian Journal of 
Chow, G. (1981), “A comparison of the information and posterior probability criteria for model 
Clayton, D. (1991), “A Monte Carlo method for Bayesian inference in frailty models”, Biometrics 
Clayton, D. (1996), “Generalized linear mixed models”, in W. Gilks, S. Richardson, and D. Spiegel- 
halter, eds., Markov Chain Monte Carlo in Practice, Chapman &Hall, Suffolk, UK, pp. 275-302. 
Clyde, M., DeSimone, H. and Parmigiani, G. (1996), “Prediction via orthogonalized model mixing”, 
Journal of the American Statistical Association 91, 1 197-1208. 
Cohen, A. C. (l963), “Estimation in mixtures of discrete distributions”, in Proceedings ofthe Inter- 
national Symposium on Discrete Distributions, Montreal, Canada, pp. 373-378. 
Congdon, P. (2003), Applied Bayesian Modelling, Wiley Series in Probability and Statistics, Wiley, 
Chichester, UK. 
Congdon, P. (2005a), Bayesian Models for Categorical Data, Wiley Series in Probability and Statis- 
tics, Wiley, Chichester, UK. 
Congdon, P. (2005b), “Bayesian predictive model comparison via parallel sampling”, Computational 
Statistics and Data Analysis 48, 735-753. 
Congdon, P. (2006a), “Bayesian model comparison via parallel model output”, Journal ofStatistica1 
Computation and Simulation 76, 149-165. 
Congdon, P. (2006b), Bayesian Statistical Modelling, Wiley Series in Probability and Statistics, 2nd 
ed., Wiley, Chichester, UK. 
Consul, P. (1 989), Generalized Poisson Distribution: Properties and Applications, Marcel Decker, 
New York. 
Consul, P. and Famoye, F. (1 992), “Generalized Poisson regression model”, Communications in 
Cowles, M. and Carlin, B. (l996), “Markov chain Monte Carlo convergence diagnostics: A compar- 
Cox, D. (1972), “Regression models and life-tables”, Journal of the Royal Statistical Sociev B 
Cox, D. and Oakes, D. (1984), Analysis ofSuwival Data, Chapman & Hall, Cambridge, UK. 
Cox, D. R. (1 958), “Two further applications of a model for binary regression”, Biometrika 45, 562- 
Czado, C. and Raftery, A. (2006), “Choosing the link function and accounting for link uncertainty in 
generalized linear models using Bayes factors”, Statistical Papers 47,4 19442. 
Damien, P., Wakefield, J. and Walker, S. (1999), “Gibbs sampling for Bayesian non-conjugate and 
hierarchical models by using auxiliary variable”, Journal of the Royal Statistical Sociew 
B 
Association 90, 13 13-1 32 1. 
Statistician 49, 327-335. 
of the American Statistical Association 96,270-281. 
hood estimation”, Statistica Neerlandica 59, 3044. 
Statistics 24, 17-36. 
selection”, Journal ofBconometrics 16, 2 1-33, 
47,467485. 
Statistics: Theory and Methods 21, 89-109. 
ative review”, Journal ofthe American Statistical Association 91, 883-904. 
34, 187-220. 
565. 
61,331-344. 

REFERENCES 
473 
Darby, S. (l980), “A Bayesian approach to parallel line bioassay”, Biometrika 67,607-612. 
de Leeuw, J. and Meijer, E. (ZOOS), Handbook of Multilevel Analysis, Springer-Verlag, New York. 
Dellaportas, P. and Forster, J. (1 999), “Markov chain Monte Carlo model determination for hierarchical 
and graphical log-linear models”, Biometrika 86, 6 15-633. 
Dellaportas, P., Forster, J. and Ntzoufras, I. (2000), “Bayesian variable selection using the Gibbs 
sampler”, in D. Dey, S. Ghosh, and B. Mallick, eds., Generalized Linear Models: A Bayesian 
Perspective, Marcel Dekker, New York, pp. 271-286. 
Dellaportas, P., Forster, J. and Ntzoufras, I. (2002), “On Bayesian model and variable selection using 
MCMC”, Statistics and Computing 12, 27-36. 
Dellaportas, P. and Smith, A. (1993), “Bayesian inference for generalized linear and proportional 
hazards models via Gibbs sampling”, Journal of the Royal Statistical Society C 42, 443460. 
Dey, D., Ghosh, S. and Mallick, B. (2000), Generalized Linear Models: A Bayesian Perspective, 
Marcel Dekker, New York. 
Dixon, M. and Coles, S. (1997), “Modelling association football scored and inefficiencies in football 
betting market”, Journal of the Royal Statistical Society C 46, 265-280. 
Dobson, A. (2002), An Introduction to Generalized Linear Models, 2nd ed., Chapman & Hall, New 
York. 
Draper, D. (1 995), “Inference and hierarchical modeling in the social sciences”, Journal of Educa- 
tional and Behavioral Statistics 20, 1 15-147. 
Draper, D. and KrnjajiC, M. (2006), Bayesian Model SpeciJcation, Technical Report, Department 
of Applied Mathematics and Statistics, Baskin School of Engineering, University of California, 
Santa Cruz. 
Dunson, D. (2000), “Bayesian latent variable models for clustered mixed outcomes”, Journal of 
the Royal Statistical Society B 62, 355-366. 
Dunson, D. and Herring, A. (2005), “Bayesian latent variable models for mixed discrete outcomes”, 
Biostatistics 6, 11-25. 
Erkanli, A. (1 994), “Laplace approximations for posterior expectation when the model occurs at the 
boundary of the parameter space”, Journal of the American Statistical Association 89,205-258. 
Evans, M. and Swartz, T. (1996), “Discussion of methods for approximating integrals in statistics 
with special emphasis on Bayesian integration problems”, Statistical Science 11, 5464. 
Fahrmeir, L. and Tutz, G. (1994), “Dynamic stochastic models for time-dependent ordered paired 
comparison system”, Journal of the American Statistical Association 89, 1438-1449. 
Fahrmeir, L. and Tutz, G. (200 I), Multivariate Statistical Modelling Based on Generalized Linear 
Models, Springer Series in Statistics, 2nd ed., Springer-Verlag, New York. 
Famoye, F. and Singh, K. (2006), “Zero-inflated generalized Poisson regression model with an appli- 
cation to domestic violence data”, Journal of Data Science 4, 1 17-130. 
Femandez, C., Ley, E. and Steel, M. (2000), “Benchmark priors for Bayesian model averaging”, 
Journal of Econometrics 100, 381427. 
Feuewerger, A. (1 979), “On some methods of analysis for weather experiments”, Biometrika 66,665- 
668. 
Fienberg, S. (1981), The Analysis of Cross-ClassiJed Categorical Data, 2nd revised ed., The MIT 
Press, Cambridge, MA. 
Fisher, R. A. ( I  922), “On the interpretation of chi-square from contingency tables, and the calculation 
of p”, Journal ofthe Royal Statistical Society B 85, 87-94. 
Fouskakis, D., Ntzoufras, I. and Draper, D. (2008), “Bayesian variable selection using cost-adjusted 
BIC, with application to cost-effective measurement of quality of health care”, Annals ofApplied 
Statistics (to appear) . 

474 
REFERENCES 
Friel, N. and Pettitt, A. (2008), “Marginal likelihood estimation via power posteriors”, Journal of 
the Royal Statistical Society B 70, 589-607. 
Futing Liao, T. (1 994), Interpreting Probability Models: Logit Probit, and Other Generalized Linear 
Models, Quantitative Applications in the Social Sciences, 007-101, Sage Publications, Inc., Los 
Angeles, CA. 
Gamerman, D. ( I  997), “Sampling from the posterior distribution in generalized linear mixed models”, 
Statistics and Computing 7, 57-68. 
Gamerman, D. and Lopes, H. (2006), Markov Chain Monte Carlo, Texts in Statistical Science, 2nd 
ed., Chapman & Hall, New York. 
Can, N. (2000), General Zero-Inflated Models and Their Applications, PhD thesis, North Carolina 
State University, at Releigh. 
Geisser, S. and Eddy, W. (1 979), “A predictive approach to model selection”, Journal of the American 
Statistical Association 74, 153-160. Corrigenda in Vol. 75, p. 765. 
Gelfand, A. (1996), “Model determination using sampling-based methods”, in W. Gilks, S. Richard- 
son, andD. Spiegelhalter, eds., Markov Chain Monte Carlo in Practice, Chapman & Hall, Suffolk, 
Gelfand, A. and Dey, D. (1 994), “Bayesian model choice: Asymptotic and exact calculations”, Journal 
of the Royal Statistical Society B 56, 50 1-5 14. 
Gelfand, A,, Dey, D. and Chang, H. (1992), “Model determination using predictive distributions 
with implementation via sampling-based methods (with discussion)”, in J. Bernardo, J. Berger, 
A. Dawid, and A. Smith, eds., Bayesian Statistics, Vol. 4, Oxford University Press, pp. 407425. 
Gelfand, A. and Ghosh, S. (1998), “Model choice: A minimum posterior predictive loss approach”, 
Biomefrika 85, 1-13. 
Gelfand, A,, Hills, S., Racine-Poon, A. and Smith, A. (1990), “Illustration of Bayesian inference 
in normal data models using Gibbs sampling”, Journal of the American Statistical Association 
Gelfand, A. and Smith, A. (1 990), “Sampling-based approaches to calculating marginal densities”, 
Journal of the American Statistical Association 85. 398409. 
Gelfand, A,, Smith, A. and Lee, T.-M. (1992), “Bayesian analysis of constrained parameter and 
truncated data problems using Gibbs sampling”, Journal ofthe American Statistical Association 
Gelman, A. (2006), “Prior distributions for variance parameters in hierarchical models”, Bayesian 
Analysis 1, 515--533. 
Gelman, A,, Carlin, J., Stern, H. and Rubin, D. (1995), Bayesian Data Analysis, Texts in Statistical 
Science, Chapman & Hall, London. 
Gelman, A,, Carlin, J., Stem, H. and Rubin, D. (2004), Bayesian Data Analysis, Texts in Statistical 
Science, 2nd ed., Chapman & Hall, London. 
Gelman, A. and Hill, J. (2006), Data Analysis Using Regression and MultileveNHierarchical Models, 
Cambridge University Press, New York. 
Gelman, A,, Huang, Z., van Dyk, D. and Boscardin, W. J. (2008), “Using redundant parameters to fit 
hierarchical models”, Journal of Computational and Graphical Statistics 17,95-122. 
Gelman, A. and Meng, X.-L. (1996), “Model checking and model improvement”, in W. Gilks, 
S. Richardson, and D. Spiegelhalter, eds., Markov Chain Monte Carlo in Practice, Chapman 
& Hall, Suffolk, UK, pp. 189-201. 
Gelman, A. and Meng, X.-L. (l998), “Simulating normalizing constants: From importance sampling 
to bridge sampling to path sampling”, Statistical Science 13, 163-185. 
UK, pp. 145-161. 
85,972-985. 
87,523-532. 

REFERENCES 
475 
Gelman, A., Meng, X.-L. and Stern, H. (1996), “Posterior predictive assessment of model fitness via 
realized discrepancies”, Statistica Sinica 6, 733-807. 
Gelman, A. and Pardoe, I. (2006), “Bayesian measures of explained variance and pooling in multilevel 
(hierarchical) models”, Technometrics 48, 241-25 1. 
Gelman, A. and Rubin, D. (1 992), “Inference from iterative simulation using multiple sequences”, 
Statistical Science 7, 457-5 1 1. 
Geman, S. and Geman, D. (1984), “Stochastic relaxation, Gibbs distributions and the Bayesian 
restoration of images”, IEEE Transactions on Pattern Analysis and Machine Intelligence 6,72 1- 
741. 
Center, F. and Farewell, V. (1985), “Goodness-of-link testing in ordinal regression models”, Canadian 
George, E. and Foster, D. (2000), “Calibration and empirical Bayes variable selection”, Biometrika 
George, E. and McCulloch, R. (1993), “Variable selection via Gibbs sampling”, Journal of the 
George, E. and McCulloch, R. (1 997), “Approaches for Bayesian variable selection”, Statistica Sinica 
7,339-373. 
George, E., McCulloch, R. and Tsay, R. (1996), “Two approaches to Bayesian model selection with 
applications”, in D. Berry, K. Chaloner, and J. Geweke, eds., Bayesian Analysis in Statistics and 
Econometrics: Essays in Honor ofArnold Zellner, Wiley, New York, pp. 339-348. 
Geweke, J. (1 992), “Evaluating the accuracy of sampling-based approaches to calculating posterior 
moments”, in J. Bernardo, J. Berger, A. Dawid, and A. Smith, eds., Bayesian Statistics, Vol. 4, 
Claredon Press, Oxford, pp. 169-194. 
Geyer, C. (1992), “Practical Markov chain Monte Carlo (with discussion)”, StatisticalScience 7,473- 
511. 
Ghosh, S., Mukhopadhyay, P. and Lu, J. (2006), “Bayesian analysis of zero-inflated regression mod- 
els”, Journal of Statistical Planning and Inference 136, 1360-1 375. 
Gilks, W. (1996), “Full conditional distributions”, in W. Gilks, S. Richardson, and D. Spiegelhalter, 
eds., Markov Chain Monte Carlo in Practice, Chapman & Hall, Suffolk, UK, pp. 75-88. 
Gilks, W., Richardson, S. and Spiegelhalter, D. (1996), Markov Chain Monte Carlo in Practice, 
Interdisciplinary Statistics, Chapman & Hall, Suffolk, UK. 
Gilks, W. and Roberts, G. (1996), “Strategies for improving MCMC”, in W. Gilks, S. Richardson, 
and D. Spiegelhalter, eds., Markov Chain Monte Carlo in Practice, Chapman & Hall, Suffolk, 
Gilks, W. and Wild, P. (1 992), “Adaptive rejection sampling for Gibbs sampling”, Journal of the Royal 
Givens, G. and Hoeting, J. (2005), Computational Statistics, Wiley Series in Probability and Statistics, 
Glonek, G. (1 996), “A class of regression models for multivariate categorical responses”, Biornetrika 
Goldstein, H., Browne, W. and Rasbash, J. (2002), “Partitioning variation in multilvevel models”, 
Understanding Statistics 1, 223-23 I .  
Goodman, L. (1979), “Simple models for the analysis of association in cross-classifications having 
ordered categories”, Journal of the American Statistical Association 74, 537-552. 
Goodman, L. (1 98 l), “Association models and canonical correlation in the analysis of cross- 
classifications having ordered categories”, Journal of the American Statistical Association 
76,320-334. 
Journal of Statistics 13, 3744. 
87,731-748. 
American Statistical Association 88, 88 1-889. 
UK, pp. 89-1 10. 
Statistical Sociev C 41, 337-348. 
Wiley, Hoboken, NJ. 
83, 15-28. 

476 
REFERENCES 
Goodman, L. (l985), “The analysis of cross-classified data having ordered and/or unordered cate- 
gories: Association models, correlation models and asymmetry models for contingency tables 
with or without missing entries”, Annals of Statistics 13, 10-69. 
Green, P. (1995), “Reversible jump Markov chain Monte Carlo computation and Bayesian model 
determination”, Biometrika 82, 71 1-732. 
Guerrero, V. and Johnson, R. (1982), “Use of the Box-Cox transformation with binary response 
Gupta, P., Gupta, R. and Tripathi, R. (l996), “Analysis of zero-adjusted count data”, Computational 
GutiCrrez-Pefia, E. and Walker, S. (200 I), “A Bayesian predictive approach to model selection”, 
Hall, D. (2000), “Zero-inflated Poisson and binomial regression with random effects: A case study”, 
Han, C. and Carlin, B. (2001), “Markov chain Monte Carlo methods for computing Bayes factors: a 
comparative review”, Journal of the American Statistical Association 96, 1 122-1 132. 
Hans, C., Dobra, A. and West, M. (2007), “Shotgun stochastic search for “large p” regression”, 
Journal of the American Statistical Association 102, 507-5 16. 
Haro-Lopez, R., Mallick, B. and Smith, A. (2000), “Binary regression using data adaptive robust link 
functions”, in D. Dey, S. Ghosh, and B. Mallick, eds., GeneralizedLinear Models: A Bayesian 
Perspective, Marcel Dekker, New York, pp. 243-253. 
Hastings, W. (l970), “Monte Carlo sampling methods using Markov chains and their applications”, 
Biometrika 57, 97-109. 
Hedeker, D. and Gibbons, R. (2006), Longitudinal Data Analysis, Wiley-Interscience, Hoboken, NJ. 
Heibron, D. (1994), “Zero-altered and other regression models for count data with added zeros”, 
Heidelberger, P. and Welch, P. (1992), “Simulation run length control in the presence of an initial 
transient”, Operations Research 31, 1109-1 144. 
Higdon, D. (1998), “Auxiliary variable methods for Markov chain Monte Carlo with applications”, 
Journal of the American Statistical Association 93, 585-595. 
Hoeting, J. and Ibrahim, J. (l998), “Bayesian predictive simultaneously variable and transformation 
selection in the linear model”, Journal of Computational Statistics andData Analysis 28,87-103. 
Hoffmann-Jargensen, J. (l994), Probabiliy with a View Towards Statistics, Vol. 1, Probability Series, 
Chapman & Hall, New York. 
Hosmer, D. and Lemeshow, S. (2000), Applied Logistic Regression, 2nd ed., Wiley, New York. 
Hosmer, D., Lemeshow, S. and May, S. (2008), Applied Survival Analysis: Regression Modeling of 
Time to Evenr Data, 2nd ed., Wiley, Hoboken, NJ. 
Huard, D., Evin, G. and Favre, A.-C. (2006), “Bayesian copula selection”, Computational Statistics 
and Data Analysis 51,809-822. 
Hubble, E. (l929), “A relationship between distance and radial velocity among extra-galactic neb- 
ulae”, in Proceedings of the National Academy of Science, Vol. 15, pp. 168-173, available at 
http://www.pnas.org/cgi/reprint/i5/3/168. 
Ibrahim, J. and Chen, M. (2000), “Power prior distributions for regression models”, StatisticalScience 
15,4660. 
Ibrahim, J. and Laud, P. (1994), “A predictive approach to the analysis of designed experiments”, 
models”, Biomefrika 69, 309-3 14. 
Statistics and Data Analysis 23, 207-218. 
Journal of Statistical Planning and Inference 93,259-276. 
Biometrics 56, 1030-1039. 
Biometrical Journal 36, 53 1-547. 
Journal of the American Statistical Association 89, 309-3 19. 

REFERENCES 
477 
Iliopoulos, G., Kateri, M. and Ntzoufras, I. (2007a), “Bayesian estimation of unrestricted and order- 
restricted association models for a two-way contingency table”, Computational Statistics and 
Data Analysis 51,46434655. 
Iliopoulos, G., Kateri, M. and Ntzoufras, I. (2007b), Bayesian Model Comparison for the Order 
Restricted RC Association Model, Technical Report, Department of Statistics, Athens University 
of Economics and Business, Athens, Greece, available at http: //stat-athens. aueb. gr/ 
” jbn/papers/paperl8. htm. 
Iliopoulou, K. (2004), Schizotypy and Consumer Behavior (in Greek), Master’s thesis, 
Department of Business Administration, 
University of the Aegean, Chios, Greece, 
available 
at 
http://stat-athens.aueb.gr/”jbn/courses/diplomatikes/ 
business/Iliopoulou(2004).pdf. 
Jasra, A., Stephens, D. and Holmes, C. (2007), “Population-based reversible jump Markov chain 
Monte Carlo”, Biometrika 94, 787-807. 
Johnson, N., Kotz, S. and Balakrishnan, N. (1997), Discrete Multivariate Distributions, Wiley, New 
York. 
Kahn, M. (2005), “An exhalent problem for teaching statistics”, Journal of Statistics Education 
13, available at www. amstat. org/publications/j se/vl3n2/datasets. kahn. html. 
Karlis, D. and Melikotzidou, L. (2005), “Multivariate Poisson regression with covariance structure”, 
Karlis, D. and Ntzoufras, I. (2000), “On modelling soccer data”, Student 3, 229-244. 
Karlis, D. and Ntzoufras, I. (2003a), “Analysis of sports data using bivariate Poisson models”, Journal 
ojthe Royal Statistical Society D 52, 381-393. 
Karlis, D. and Ntzoufras, I. (2003b), “Bayesian and non-Bayesian analysis of soccer data using bivari- 
ate Poisson regression models”, in Proceedings ojthe Greek Statistical Institute, Vol 16, pp. 605- 
612, available at http://www.stat-athens.aueb.gr/~jbn/tr/TR59_Greek~Soccer 
.ps. 
Karlis, D. and Ntzoufras, 1. (2005), “Bivariate Poisson and diagonal inflated bivariate Poisson regres- 
sion models in R’, Journal ojStatistical Sojmare 10, 1-37. 
Karlis, D. and Ntzoufras, I. (2006), “Bayesian analysis of the differences of count data”, Statistics in 
Medicine 25, 1885-1905. 
Karlis, D. and Ntzoufras, I. (2008), “Bayesian modelling of football outcomes: Using the Skellam’s 
distribution for the goal difference”, IM Journal ofManagement Mathematics (to appear) . 
Karlis, D. and Tsiamyrtzis, P. (2008), “Exact Bayesian modeling for bivariate Poisson data and 
extensions”, Statistics and Computing 18, 2740. 
Kass, R., Carlin, B., Gelman, A. and Neal, R. (l998), “Markov chain Monte Carlo in practice: A 
roundtable discussion”, The American Statistician 52, 93-100. 
Kass, R. and Raftery, A. (1995), “Bayes factors”, Journal of the American Statistical Association 
Kass, R. and Wasserman, L. (1 995), “A reference Bayesian test for nested hypotheses and its rela- 
tionship to the Schwarz criterion”, Journal ojthe American Statistical Association 90,928-934. 
Kateri, M., Nicolaou, A. and Ntzoufras, I. (2005), “Bayesian inference for the RC(m) association 
model”, Journal ojComputational and Graphical Statistics 14, 116-138. 
Katsis, A. and Ntzoufras, I. (2005), “Testing hypotheses for the distribution of insurance claim 
counts using the Gibbs sampler”, Journal ojComputationalMerhods in Sciences andEngineering 
Knuiman, M. and Speed, T. (1988), “Incorporating prior information into the analysis of contingency 
Statistics and Computing 15, 255-265. 
90,773-795. 
5,201-214. 
tables”, Biometries 44, 1061-1071. 

478 
REFERENCES 
Kocherlakota, S. and Kocherlakota, K. (1992), Bivariate Discrete Distributions, Marcel Dekker, New 
Kolev, N., Anjos, U. and Mendez, B. (2006), “Copulas: A review and recent developments”, Stochastic 
Models 22,617-660. 
Konishi, S. and Kitagawa, G. (2008), Information Criteria and Statistical Modeling, Springer Series 
in Statistics, Springer-Verlag, New York. 
Kuha, J. (2003), Model Assessment andModel Choice: An Annotated Bibliography, Technical Report, 
Department of Statistics and the Methodology Institute, London School of Economics, available 
athttp://stats.lse.ac.uk/kuha/msbib/biblio/. 
Kuha, J. (2004), “AIC and BIC: Comparisons of assumptions and performance”, Sociological Methods 
Kuo, L. and Mallick, B. (1998), “Variable selection for regression models”, SankhyZ B 60, 65-81. 
Kuonen, D. (1996), Modelling the Success ofFootball Teams in the European Championships, Tech- 
nical Report, No. 96.1, Department of Mathematics, Swiss Federal Institute of Technology, 
Lausanne, Switzerland. 
Kuonen, D. (1997), StatisticalModels for Knock-out Soccer Tournaments, Technical Report, No. 97.3, 
Department of Mathematics, Swiss Federal Institute of Technology, Lausanne, Switzerland. 
Kuonen, D. and Roehrl, A. (2000), “Was France’s world cup win pure chance?”, Student 3,153-166. 
Lambert, D. (1 992), “Zero-inflated Poisson regression, with applications to defects in manufacturing”, 
Lang, J. (1999), “Bayesian ordinal and binary regression models with a parametric family of mixture 
Laskey, K. and Myers, J. (2003), “Population Markov chain Monte Carlo”, Machine Learning 
Laud, P. and Ibrahim, J. (199S), “Predictive model selection”, Journal ofthe Royal Sfatistical Soci- 
Lawson, A,, Browne, W. and Vidal Rodeiro, C. (2003), Disease Mapping with WinBUGSand MLwiN, 
Lee, A. (1997), “Modeling scores in the premier league: Is Manchester United really the best?”, 
Lewis, S. and Raftery, A. (1 997), “Estimating Bayes factor via posterior simulation with the Laplace- 
Li, C., Lu, J., Park, J., Kim, K. and Peterson, J. (1999), “Multivariate zero-inflated Poisson models 
Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger, J. (2008), ‘Mixtures of g priors for Bayesian 
Lindley, D. (l957), “A statistical paradox”, Biometrika 44, 187-192. 
Lindley, D. (1980), “L.J.Savage - 
his work in probability and statistics”, Annals of Statistics 8, 1-24. 
Lindley, D. (1993), “On presentation of evidence”, Mathematical Scientist 18, 60-63. 
Lindley, D. and Smith, A. (1 972), “Bayes estimates for the linear model”, Journal of the Royal 
Lindsey, J. (1 997), Applying GeneralizedLinear Models, Springer Texts in Statistics, Springer-Verlag, 
Liu, I. and Agresti, A. (2005), “The analysis of ordered categorical data: An overview and a survey 
York. 
Research 33, 188-229. 
Technometrics 34, 1-14. 
links”, Computational Statistics and Data Analysis 31, 59-87. 
50, 175-196. 
ety B 57,247-262. 
Statistics in Practice, Wiley, Hoboken, NJ. 
Chance 10, 15-19. 
Metropolis estimator”, Journal of the American Statistical Association 92,648-655. 
and their applications”, Technometrics 41, 29-38. 
variable selection’, Journal ofthe American Statistical Association 103, 410423. 
Statistical Society B 34, 1-41, 
New York. 
of recent developments (with discussion)”, Test 14, 1-73. 

REFERENCES 
479 
Lunn, D. J., Best, N. and Whittaker, J. (2005), Generic Reversible Jump MCMC Using Graphical 
Models, Technical Report, No EPH-2005-01, Department of Epidemiology and Public Health, 
Imperial College, London, UK, available at https : //wwwl . imperial. ac . uk/resources/ 
8b3cf549-039e-4f96-8bec-cab969a0695ceph-2005-Ol.pdf. 
Lunn, D. J., Whittaker, J. C. and Best, N. (2006), “A Bayesian toolkit for genetic association studies”, 
Genetic Epidemiology 30,23 1-247. 
Lynch, S. (2007), Introduction to Applied Bayesian Statistics and Estimation for Social Scientists, 
Statistics for Social and Behavioral Sciences, Springer-Verlag, New York. 
Mackowiak, P. A., Wasserman, S. S. and Levine, M. M. (1992), “A critical appraisal of 98.6 degrees 
F, the upper limit of the normal body temperature, and other legacies of Carl Reinhold August 
Wunderlich”, Journal of the American Medical Association 268, 1578-1 580. 
Maher, M. (1982), “Modelling association football scores”, Statistica Neerlandica 36, 109-1 18. 
Mallick, B. and Gelfand, A. (1994), “Generalized linear models with unknown number of compo- 
Mallows, C. (1973), “Some comments on C,”, 
Technometrics 15, 661-675. 
McCullagh, P. and Nelder, J. (1989), Generalized Linear Models, Monographs on Statistics and 
Mendoza, M. (1990), “A Bayesian analysis of the slope ratio bioassay”, Biornetrics 46, 1059-1069. 
Meng, X.-L. (l994), “Posterior predictive p-values”, Annals ofstatistics 22, 1142-1 160. 
Meng, X.-L. and Wong, W. (1996), “Simulating ratios of normalizing constants via a simple identity: 
Metropolis, N., Rosenbluth, A,, Rosenbluth, M., Teller, A. and Teller, E. (1953), “Equations of state 
Metropolis, N. and Ulam, S. (l949), “The Monte Carlo method”, Journal ofthe American Statistical 
Meyer, M. and Laud, P. (2002), “Predictive variable selection in generalized linear models”, Journal 
Merller, J. (1 999), “Perfect simulation of conditionally specified models”, Journal ofthe Royal Sta- 
Montgomery, D. and Peck, E. (1992), Introduction to Regression Analysis, Wiley, New York. 
Montgomery, D., Peck, E. and Vining, G. (2006), Introduction to Linear Regression Analysis, 4th ed., 
Wiley, Hoboken, NJ. 
Movellan, J. (2006), Tutorial on Multivariate Logistic Regression, Machine Perception Laboratory, 
University of California, San Diego, available at the tutorial section of http: //mplab .ucsd. 
edu/wordpress/. 
Mwalili, S., Lesaffre, E. and Declerck, D. (2008), “The zero-inflated negative binomial regression 
model with correction for misclassification: An example in caries research”, Statistical Methods 
in Medical Research 17, 123-139. 
Neal, P. and Roberts, G. (2008), “Optimal scaling for random walk Metropolis on spherically con- 
strained target densities”, Methodology and Computing in Applied Probability 10,277-297. 
Neal, R. (1998), “Suppressing random walks in Markov chain Monte Carlo using ordered over- 
relaxation”, in M. Jordan, ed., Learning in Graphical Models, Kluwer Academic Publish- 
ers, Dordrecht, pp. 205-230; also available at http: //www . cs . utoronto. ca/-radf ord/ 
publications. html. 
nents”, Biometrika 81, 237-245. 
Applied Probability, Vol. 37,2nd ed., Chapman & Hall, Cambridge, UK. 
A theoretical exploration”, Statistica Sinica 6, 83 1-860. 
calculations by fast computing machine”, Journal ofchemical Physics 21, 1087-1092. 
Association 44, 335-341. 
of the American Statistical Association 97, 859-871. 
tistical Society B 61,251-264. 
Neal, R. (2001), “Annealed importance sampling”, Statistics and Computing 11, 125-1 39. 
Neal, R. (2003), “Slice sampling”, The Annals of Statistics 31, 705-767. 

480 
REFERENCES 
Newton, M. and Raftery, A. (1994), “Approximate Bayesian inference with the weighted likelihood 
bootstrap”, Journal of the Royal Statistical Society B 56, 3 4 8 .  
Ntzoufras, I. (1999a), Aspects of Bayesian Model and Variable Selection Using MCMC, PhD the- 
sis, Department of Statistics, Athens University of Economics and Business, Athens, Greece, 
available at http: //stat-athens .aueb.gr/-jbn/publications. htm. 
Ntzoufras, I. (1999b), “Discussion on Bayesian model averaging and model search strategies”, in 
J. Bernardo, J. Berger, A. Dawid, and A. Smith, eds., BayesianStatistics, Vol. 6, OxfordUniversity 
Press, pp. 178-179. 
Ntzoufras, I. (2002), “Gibbs variable selection using BUGS”, Journal ofStafistica1 Software 7, 1-19. 
Ntzoufras, I. and Dellaportas, P. (2002), “Bayesian modelling of outstanding liab 
claim count uncertainty (with discussion)”, North American Actuarial Journal 6, 1 13-128. 
Ntzoufras, I., Dellaportas, P. and Forster, J. (2000), “Stochastic search variable selection for log-linear 
models”, Journal of Statistical Computation and Simulation 68, 23-38. 
Ntzoufras, I., Dellaportas, P. and Forster, J. (2003), “Bayesian variable and link determination for 
generalized linear models”, Journal of Planning and Inference 111, 165-1 80. 
Ntzoufras, I., Katsis, A. and Karlis, D. (2005), “Bayesian assessment of the distribution of insurance 
claim counts using reversible jump MCMC”, North American Actuarial Journal 9, 90-10. 
O’Brien, S. and Dunson, D. (2004), “Bayesian multivariate logistic regression”, Biometrics 60,739- 
746. 
Oh, C., Ye, K., He, Q. and Mendell, N. (2003), “Locating disease genes using Bayesian variable 
selection with the Haseman-Elston method”, BMC Genetics 4, Supl.1 - S9, available at http: 
//www.biomedcentral.com/l47l-2i56/4/sl/S69. 
O’Hagan, A. (1995), “Fractional Bayes factors for model comparison”, Journalofthe RoyalStatistical 
Pauler, D. ( I  998), “The Schwarz criterion and related methods for normal linear models”, Biometrika 
Pericchi, L. (1984), “An alternative to the standard Bayesian procedure for discrimination between 
Pitt, M., Chan, D. and Kohn, R. (2006), “Efficient Bayesian inference for Gaussian copula regression 
Plummer, M., Best, N., Cowles, K. and Vines, K. (2006), “CODA: Convergence diagnosis and output 
analysis for MCMC”, R News 6(1), 7-1 1, available at http: //CRAN.R-project . org/doc/ 
Rnews/Rnews-2006-1.pdf. 
Powers, D. and Xie, Y. (1999), Statistical Methods for Categorical Data Analysis, Academic Press, 
San Diego, CA. 
Pregibon, D. (1980), “Goodness of link tests for generalized linear models”, Journal ofthe Royal 
Statistical Society C 29, 15-24. 
Prentice, R. (1976), “Generalization of the probit and logit methods for dose response curves”, 
Biometrics 32, 761-768. 
Press, S. (1989), Bayesian Statistics: Principles, Models and Applications, Wiley, New York. 
Propp, J. and Wilson, D. (1996), “Exact sampling with coupled Markov chains and applications to 
Qiu, Z., Song, P. X.-K. and Tan, M. (2002), “Bayesian hierarchical models for multi-level repeated 
Raftery, A. (19964, “Approximate Bayes factors and accounting for model uncertainty in generalized 
Society B 57, 57. 
85, 13-27. 
normal linear models”, Biometrika 71, 575-586. 
models”, Biometrika 93, 537-554. 
statistical mechanics”, Random Structures and Algorithms 9, 223-252. 
ordinal data using WinBUGS ”, Journal ofBiopharmaceutica1 Statistics 12, 121-135. 
linear models”, Biometrika 83, 25 1-266. 

REFERENCES 
481 
Raftery, A. (1996b), “Hypothesis testing and model selection”, in W. Gilks, S. Richardson, and 
D. Spiegelhalter, eds., Markov Chain Monte Carlo in Practice, Chapman & Hall, Suffolk, UK, 
Raftery, A. and Lewis, S. (1992), “How many iterations in the Gibbs sampler?’, in J. Bemardo, 
J. Berger, A. Dawid, and A. Smith, eds., Bayesian Statistics, Vol. 4, Claredon Press, Oxford, 
Raftery, A., Madigan, D. and Hoeting, J. (1997), “Bayesian model averaging for linear regression 
models”, Journal of the American Statistical Association 92, 179-191. 
Raftery, A., Newton, M., Satagopan, J. and Krivitsky, P. (2007), “Estimating the integrated likelihood 
via posterior simulation using the harmonic mean identity (with discussion)”, in J. Bemardo, 
J. Bayarri, and J. Berger, eds., Bayesian Statistics, Vol. 8, Oxford University Press, pp. 1 4 5 .  
Raine, A. (1991), “The SPQ: A scale for the assessment of schizotypal personality based on dsm-iii-r 
criteria”, Schizophrenia Bulletin 17, 555-564. 
Rasch, G. (1Y61), “On general laws and the meaning of measurement in psychology”, in Proceedings 
of4th Berkeley Symposium on Mathematics, Statistics, and Probability, University of California 
Press, Berkeley, pp. 321-333. 
Reep, C. and Benjamin, B. (1968), “Skill and chance in association football”, Journal ofthe Royal 
Statistical Society A 131, 581-585. 
Keep, C., Pollard, R. and Benjamin, B. (1Y71), “Skill and chance in ball games”, Journal of the Royal 
Statistical Society A 134, 623-629. 
Ridout, M., Demetrio, C. and Hinde, J. (1998), “Models for count data with many zeros”, in Proceed- 
ings of 19th International Biometric Conference, Cape Town, South Africa, pp. 179-190. 
Rigby, R. and Stasinopoulos, D. (2005), “Generalized additive models for location, scale and shape 
(with discussion)”, Journal ofthe Royal Statistical Society C 54, 507-554. 
Robert, C. (l9Y3), “A note on Jeffreys-Lindley paradox”, Statistica Sinica 3,601-608 
Robert, C. (2007), The Bayesian Choice, 2nd ed., Springer-Verlag, New York. 
Robert, C. and Casella, G. (2004), Monte Carlo Statistical Methods, Springer Texts in Statistics, 2nd 
ed., Springer-Verlag, New York. 
Roberts, G. (1996), “Markov chain concepts related to sampling algorithms”, in W. Gilks, S. Richard- 
son, and D. Spiegelhalter, eds., Markov Chain Monte Carlo in Practice, Chapman & Hall, Suffolk, 
Roberts, G., Gelman, A. and Gilks, W. (1997), “Weak convergence and optimal scaling of random 
Roberts, G. and Rosenthal, J. (2001), “Optimal scaling for various Metropolis-Hastings algorithms”, 
Rosner, B. (2005), Fundamentals of Biostatistics, 6th revised ed., Brooks Cole. 
Rue, H. and Salvesen, 0. (2000), “Prediction and retrospective analysis of soccer matches in a league”, 
Ryan, T. (1997), Modern Regression Methods, Wiley Series in Probability and Statistics, Wiley, New 
Sahu, S., Dey, D. and Branco, M. (2003), “A new class of multivariate skew distributions with 
applications to Bayesian regression models”, The Canadian Journal of Statistics 31, 129-1 50. 
Sarantinidis, M. (2003), A Suwty for the Development ofNatural Products Shops: The Case of 
“Masticha Shop ”(in Greek), Master’s thesis, Department of Business Administration, University 
of the Aegean, Chios, Greece, available at h t t p :  //stat-athens. aueb. gr/-jbn/courses/ 
diplomatikes/business/Sarantinidis(2003).pdf. 
pp. 163-188. 
pp. 763-774. 
UK, pp. 45-58. 
walk Metropolis algorithms”, The Annals ofStatistics 7, 1 10-120. 
Statistical Science 16, 351-367. 
Journal of the Royal Statistical Society D 49,399418. 
York. 

482 
REFERENCES 
Schlesselman, J. (1982), ‘Case-Control Studies: Design, Conduct, Analysis, Monographs in Epi- 
Schwarz, G. (1978), “Estimating the dimension of a model”, Annals of Statistics 6,461464. 
Scollnik, D. (1998), “On the analysis of the truncated generalized Poisson distribution using a Bayesian 
method”, ASTIN Bulletin 28, 135-152. 
Scollnik, D. (2002), “Implementation of four models for outstanding liabilities in WinBUGS : A 
discussion of a paper by Ntzoufras and Dellaportas”, North American Actuarial Journal 6, 128- 
136. 
demiology and Biostatistics, Oxford University Press, New York. 
Scott, D. W. (l992), Multivariate Density Estimation, Wiley, New York. 
Sellke, T., Bayarri, M. and Berger, J. (2001), “Calibration of p-values for testing precise null hypothe- 
Seltzer, M., Wong, W. and Bryk, A. (1996), “Bayesian analysis in applications of hierarchical models: 
Seshadri, V, (1993), The Inverse Gaussian Distribution: A Case Study in Exponential Families, 
Shafer, J. (1982), “Lindley’s paradox (with discussion)”, Journal of the American Statistical Associ- 
Shao, J. (1 993), “Linear model selection by cross-validation”, Journal of the American Statistical 
Shoemaker, A. L. (1996), “What’s normal? - temperature, gender, and heart rate”, Journal of 
Statistics Education 4(2), available at h t t p :  //www . amstat. org/publications/ j se/v4n2/ 
datasets.shoemaker.htm1. 
Sisson, S. (2005), “Trans-dimensional Markov chains: A decade of progress and future perspectives”, 
Journal of the American Statistical Association 100, 1077-1 089. 
Skellam, J. (1946), “The frequency distribution of the difference between two Poisson variates be- 
longing to different populations”, Journal ofthe Royal Statistical Society A 109, 296. 
Smith, A. and Roberts, G. (1993), “Bayesian computation via the Gibbs sampler and related Markov 
chain Monte Carlo methods”, Journal of the Royal Statistical Society B 55, 3-23. 
Smith, B. (2005), (B)ayesian (0)utput (A)nalysis Program (BOA) Version 1.1.5 User’s Manual, 
Technical Report, Department of Public Health, The University of Iowa, available at h t t p :  
//w.public-health.uiowa.edu/boa. 
Smith, B. (2007), “BOA: An R package for MCMC output convergence assessment and posterior 
inference”, Journal ofStatistical Sofhvare 21, available at h t t p :  //www . j statsof t . org/. 
Spiegelhalter, D., Abrams, K. and Myles, J. (2004), Bayesian Approaches to Clinical Trials and 
Health-Care Evaluation, Statistics in Practice, Wiley, Chichester, UK. 
Spiegelhalter, D., Best, N., Carlin, B. and van der Linde, A. (2002), “Bayesian measures of model 
complexity and fit (with discussion)”, Journal ofthe Royal Statistical Society B 64, 583-639. 
Spiegelhalter, D. and Smith, A. (1988), “Bayes factors for linear and log-linear models with vague 
prior information”, Journal of the Royal Statistical Society B 44, 377-387. 
Spiegelhalter, D., Thomas, A., Best, N. and Gilks, W. (1996a), BUGS 0.5: Bayesian Znference Using 
Gibbs Sampling Manual, MRC Biostatistics Unit, Institute of Public Health, Cambridge, UK. 
Spiegelhalter, D., Thomas, A., Best, N. and Gilks, W. (1996b), BUGS 0.5: Examples Volume 1, MRC 
Biostatistics Unit, Institute of Public Health, Cambridge, UK. 
Spiegelhalter, D., Thomas, A., Best, N. and Gilks, W. (1996c), BUGS 0.5: Examples Volume 2, MRC 
Biostatistics Unit, Institute of Public Health, Cambridge, UK. 
ses”, The American Statistician 55,62-7 1. 
Issues and methods”, Journal of Educational and Behavioral Statistics 21, 13 1-167. 
Oxford Science Publications, UK. 
ation 11,325-334. 
Association 88,486494. 

REFERENCES 
483 
Spiegelhalter, D., Thomas, A., Best, N. and Lunn, D. (2003a), WinBUGS Examples, Vol. 1, MRC 
Biostatistics Unit, Institute of Public Health and Department of Epidemiology and Public Health, 
Imperial College School of Medicine, UK, available at http : //www . mrc-bsu. cam. ac . uk/ 
bugs. 
Spiegelhalter, D., Thomas, A,, Best, N. and Lunn, D. (2003b), WinBUGS Examples, Vol. 2, MRC 
Biostatistics Unit, Institute of Public Health and Department of Epidemiology and Public Health, 
Imperial College School of Medicine, UK, available at http: //www .mrc-bsu. cam. ac. uk/ 
bugs. 
Spiegelhalter, D., Thomas, A,, Best, N. and Lunn, D. (2003c), WinBUGS Examples, Vol. 3, MRC 
Biostatistics Unit, Institute of Public Health and Department of Epidemiology and Public Health, 
Imperial College School of Medicine, UK, available at http : //www . mrc-bsu . cam. ac . uk/ 
bugs. 
Spiegelhalter, D., Thomas, A,, Best, N. and Lunn, D. (20034, WinBUGS User Manual, Version 1.4, 
MRC Biostatistics Unit, Institute of Public Health and Department of Epidemiology and Public 
Health, Imperial College School of Medicine, UK, available at http: //www .mrc-bsu. cam. 
ac . uk/bugs. 
Stanton, J. (2001), “Galton, Pearson, and the peas: A brief history of linear regression for statistics 
instructors”, Journal ofstatistics Education 9(3). 
Stone, M. (1977), “An asymptotic equivalence of choice of model by cross-validation and Akaike’s 
criterion”, Journal ofthe Royal Statistical Society B 39, 4447. 
Stukel, T. (1 988), “Generalized logistic models”, Journal of the American Statistical Association 
83,42643 1. 
Tanner, M. and Wong, W. (1 987), “The calculation ofthe posterior distributions by data augmentation”, 
Ter Berg, P. (1996), “A loglinear Lagrangian Poisson model”, ASTIN Bulletin 26, 123-129. 
Tiao, G. and Zellner, A. (1 964), “On the Bayesian estimation of multivariate regression”, Journal of 
the Royal Statistical Society B 26, 277-285. 
Tierney, L. and Kadane, J. (1986), “Accurate approximations for posterior moments and marginal 
densities”, Journal ofthe American Statistical Association 81, 82-86. 
Tierney, L., Kass, R. and Kadane, J. (1989), “Fully exponential Laplace approximations to expec- 
tations and variances of nonpositive functions”, Journal of the American Statistical Association 
Tsionas, E. (200 I), “Bayesian multivariate Poisson regression”, Communications in Statistics-The- 
ory and Methods 30,243-255. 
Vehtari, A. and Lampinen, J. (2003), “Expected utility estimation via cross-validation”, in J. Bernardo, 
M. Bayarri, J. Berger, A. Dawid, D. Heckerman, A. Smith, and M. West, eds., Bayesian Statistics, 
Vol. 7, Oxford University Press, pp. 701-710. 
Journal of the American Statistical Association 82, 528-549. 
84,710-716. 
Wahlin, J. (2001), “Bivariate zip models”, Biometrical Journal 43, 147-160. 
Wakefield, J. and Stephens, D. (2000), “Bayesian errors-in-variables modeling”, in D. Dey, S. Ghosh, 
and B. Mallick, eds., Generalized Linear Models: A Bayesian Perspective, Marcel Dekker, New 
York, pp. 33 1 - 348. 
Walker, S., Gutikrrez-Peiia, E. and Muliere, P. (2001), “A decision theoretic approach to model 
selection”, Journal of the Royal Statistical Society D (The Statistician) 50, 31-39. 
Weisberg, S. (2005), Applied Linear Regression, 3rd ed., Wiley-Interscience, New York. 
West, M. and Harrison, P. (1997), Bayesian Forecasting and Dynamic Models, Kendall’s Library of 
Statistics, Vol. 7,2nd ed., Springer-Verlag, New York. 

484 
REFERENCES 
Woodworth, G. (2004), Biostatistics: A Bayesian Introduction, Wiley Series in Probability and Statis- 
Yang, R. and Berger, J. (1996), A Catalog ofNoninformative Priors, Technical Report, Institute of 
Yi, N. (2004), “A unified Markov chain Monte Carlo framework for mapping multiple quantitative 
Yi, N., George, V. and Allison, D. (2003), “Stochastic search variable selection for identifying multiple 
quantitative trait loci”, Genetics 164, 1129-1 138. 
Zeger, S. and Karim, M. (1991), “Generalized linear models with random effects; a Gibbs sampling 
approach”, Journal of the American Statistical Association 86, 79-86. 
Zellner, A. (1976), “Bayesian and non-Bayesian analysis of the regression model with multivariate 
Student-t error terms”, Journal of the American Statistical Association 71,400405. 
Zellner, A. (1986), “On assessing prior distributions and Bayesian regression analysis using g-prior 
distributions”, in P. Goel and A. Zellner, eds., Bayesian Inference and Decision Techniques: 
Essays in Honor ofBruno de Finetti, North-Holland, Amsterdam, pp. 233-243. 
tics, Wiley, Hoboken, NJ. 
Statistics and Decision Sciences, Duke University, Durham, NC. 
trait loci”, Genetics 167, 967-975. 

INDEX 
A 
Akaike information criterion (AIC), 426,428 
analysis of covariance, see ANCOVA 
analysis of variance, see ANOVA 
ANCOVA, 195-2 18 
bioassay example, 2 0 6 2  17 
common slope model, 197,204-212 
different slopes model, 197, 199,212-216 
parallel lines model, 197, 204-212 
interaction model, 174-175, 177 
main effects, 173 
multifactor, I84 
one way, 167-168 
parametrization, 168-1 69, 174-175 
three way, 191-195 
two way, 173-175, 177 
using dummies, 191-195 
ANOVA, 167-184 
auxiliary variables, 76 
B 
baseline category, 168 
Bayes factor, 389-391 
fractional, 422 
interpretation, 390 
intrinsic, 422 
posterior, 42 1,423 
pseudo, 421 
sensitivity, 391-392 
variants, 39 I 
Bayes theorem, 3 4  
Bayesian inference, 4-7 
from MCMC output, 38 
using conjugate priors, 7-24 
Bayesian information criterion (BIC), 425, 428 
Bayesian model comparison 
Carlin-Chib sampler, 419420 
predictive model evaluation, 421424 
reversible jump MCMC, 420421 
Bayesian modeling, 1-2 
Bayesian variable selection, 4 0 5 4  19 
Gibbs variable selection (GVS), 409410 
Kuc-Mallick sampler, 41 1 
MCMC based posterior inference, 412413 
priors, 40W09 
stochastic search variable selection (SSVS). 
410411 
WinBUGS, 414-419 
Bernoulli distribution, 90 
beta distribution, 90 
binary response models, 56-7 I ,  237,263-268, 
See also binomial response models 
325-332 
binomial distribution, 8-9, 15, 18-20, 33,46, 56, 58, 
binomial response models, 5671,237,255-268, 
bivariate Poisson model, 291-293 
BOA, 120,447448,451453 
90,231,237 
325-332 
convergence diagnostics, 448449 
Bayesian Modeling Using WinBUGS, by Ioannis Ntzoufras 
Copyright 02009 John Wiley & Sons, Inc. 
485 

486 
INDEX 
illustration, 457460 
bridge sampling, 394-395 
BUGS, 83-84,87, 149 
bumin period, 38 
r 
c 
candidate’s estimator, 395 
canonical link, 234,237,242,277, 279-280,283 
Carlin-Chib sampler, 41 9 4 2 0  
censoring, 279,297 
chi-squared distribution, 90 
Chib’s marginal likelihood estimator, 395-397 
Clog-log, 97, 234,262-264 
parameter interpretation, 262-264 
convergence diagnostics, 448449 
CODA, 120,447448,450451 
Gelman-Rubin, 449 
Geweke, 448,454 
Heidelberger-Welch, 449,456 
Raftery-Lewis, 449, 455 
illustration, 453457 
complementary log-log model, see clog-log 
componentwise Metropolis-Hastings, 45-7 1 
compound document, 86,88 
computational notes, 58, 79, 156, 265-266, 288, 318 
conditional predictive ordinate, 344, 375 
conjugate Bayesian analysis, 7-24 
example, 14-24 
for normal regression models, 12-13 
for the mean and variance of normal data, 11-12, 
for the mean of normal data (known variance), 9-10 
for the Poisson rate, 7-8, 15-1 8 
for the probability of binomial data, 8-9, 18-21 
conjugate prior distributions, 7, 15 
constraints, 168-1 69, 174-1 75 
comer, 168-170, 175, 189-190 
sum-to-zero, 169-170, 175, 190 
21-24 
contingency tables, 268-270 
convergence, 37,40,42, 143-144 
convergence diagnostics, 37,447449 
Gelman-Rubin, 119, 143-144 
See also CODA 
See also CODA 
comer constraints, 168 
count data models, 237,242-255,282-293,315-317, 
3 3 3-3 3 6 
covariates, 3,6 
cross-validation, 344,375-378 
likelihood, 423 
negative log-likelihood, 423 
predictive log-score, 422 
n 
DAG, 87-88,435 
data augmentation, 77, 307 
data matrix, 162, 191 
data 
bioassay - 
factor 8 blood clotting times data, 204 
goals of the Greek national team in Euro 2004, 15 
Kobe Bryant’s field goals in NBA, 19 
matched-pair clinical trial, 327 
odds ratios from various studies, 3 19 
schizotypal personality data, 178 
design matrix, 189, 191 
deviance, 87, 118, 133, 140,344, 368 
deviance information criterion (DIC), 87, 140-141, 
directed acyclic graph, see DAG 
Dirichlet distribution, 91 
dispersion index, 283-286,288, 333,336 
distributions 
Bernoulli, 90 
beta, 90 
binomial, 8-9, 15, 18-20,33,46, 56, 58,90, 231, 
237 
bivariate Poisson, 291-293 
categorical (WinBUGS), 90 
chi-squared, 90 
Dirichlet, 91 
double exponential, 90, 230-233 
exponential, 15,90 
exponential family, 14-15 
gamma, 15,90,232 
generalized gamma, 90 
generalized Poisson, 286 
inverse Gaussian, 232,277-278 
Langrangian Poisson, 286 
log-normal, 90 
logistic, 90 
multinomial, 15, 91 
negative binomial, 15, 90, 231,283-285 
normal, 9-12,15,90-91,231 
Pareto, 90 
Poisson, 7-8, 15, 90,231 
Poisson difference, 293-296 
Polya, 283 
predictive, 341-342 
Student’s t, 90-91 
uniform, 90 
Weibull, 90 
Wishart, 91 
edges, 438 
example, 439441 
index, 439 
nodes, 436438 
panels, 435,438439 
double-Poisson model, 292 
double exponential distribution, 90 
dummy variables, 189-195,200, 203 
E 
219-226,248,318,426,428 
DOODLE, 85,87,435441 
equilibrium distribution, 37 
ergodic mean, 4 1 
examples (by subject) 
ANCOVA, 204-2 17 
ANOVA 
one way, 171-172 
three way, 191-195 
two way, 178-184 
Bayes theorem illustration, 4 

INDEX 
487 
Bayesian variable selection, 414419 
binomial response model, 263-268 
bivariate Poisson model, 293 
BOA illustration, 457460 
CODA illustration, 453457 
conditional predictive ordinate, 376-378 
conjugate analysis 
for normal data, 21-24 
for the Poisson rate, 15-18 
for the probability of binomial data, 18-21 
DIC, 221-226 
Doodle illustration. 439441 
gamma model, 281-282 
generalized linear mixed model, 322-325, 
generalized Poisson model, 287 
Gibbs sampling illustration, 72-74 
goodness-of-fit, 368-374 
grouped data, 287 
hierarchical model, 308-3 17,3 19-320,322-325, 
327-332,334-337 
information criteria, 42943 1 
inverse Gaussian model, 278 
log-normal model, 281-282 
logistic regression, 263-268 
logit GLMM, 329-332 
marginal likelihood computation, 399405 
matched-pair binary data, 327-329 
meta-analysis, 3 19-320 
Metropolis-Hastings illustration, 56-7 1 
missing values, 345-353 
models for counts, 245-255,284,287, 290-291, 
293,315-317,334-336 
models for integer responses, 296 
models for positive responses, 281-282 
Monte Carlo integration, 33-34 
negative binomial model, 284 
ordered predictive values, 359 
overdispersion, 366 
Poisson-gamma model, 3 15-3 16 
Poisson-log-normal model, 3 16-3 17 
Poisson difference model, 296 
Poisson GLMM, 334-337 
Poisson mixture model, 3 15-3 17 
Poisson regression, 245-255 
posterior predictive ordinate, 36&36 1 
prediction, 347-353 
predictive model evaluation, 424 
predictive 
327-331,334-336 
analysis for normal models, 383-386 
checks for the shape of the distribution, 366-368 
cumulative frequencies for continuous data, 
frequencies for discrete data, 354-356 
3 5 7-3 5 8 
random effects model, 3 13 
random walk illustration, 46-55 
regression, 157-161, 164-167, 221-226 
repeated measurement model, 308-3 12 
residuals, 363-365 
script illustration, 444445 
state mace model. 3 13-3 15 
stepwise variable selection using DIC, 221-226 
Weibull model, 281-282 
WinBUGS 
data specification, 104-105 
model specification, 107-108 
running MCMC, 125-132 
zero inflated Poisson model, 290-291 
1990 USA general social survey, 286285,287, 
ABiBA crossover trial, 322-325 
aircraft damage dataset, 245-249 
bioassay - 
factor 8 blood clotting times, 204-2 17 
bivariate simulated count data, 293, 296 
body temperature data, 21-24,72-74 
English premiership football data for season 
estimation of risk in medical studies using Monte 
estimation of the prevalence of a disease, 18 
evaluation of candidate school tutors, 171-172 
goals of the Greek national team in Euro 2004, 
inverse Gaussian simulated dataset, 278 
Kobe Bryant’s field goals in NBA, 19-20,46-55, 
125, 131-132,134, 136138, 141-143, 148, 
313-315,399402 
368-371,376,378 
examples 
290-291,315-316 
2006-2007,249-255 
Carlo, 33-34 
15-18 
Manchester United’s goals, 354-356,360, 366, 
odds ratios analysis from various studies, 3 19-320 
outstanding car insurance claim amounts, 347-353 
oxygen uptake experiment, 22 1-226 
relative risk using Bayes theorem, 4 
repeated measurements of blood pressure, 308-3 12 
schizotypal personality data, 178-184, 191-195, 
senility symptoms data, 56-71,77-81,263-268 
simulated data 
329-331,345-347 
binomial, 272 
bivariate Poisson, 293,296 
of Dellaportas et al. (2002), 414419 
inverse Gaussian, 278-279 
normal data, 357-359,361,363-368,371-374, 
Poisson, 270 
377 
slice sampling illustration, 77-81 
soft drink delivery times, 157-161, 164-167, 
treatment comparison in matched-pair clinical 
water polo World Cup 2000 data, 334-336 
281-282,383-386,403405,424,429431 
trials, 327-328 
exchangeability, 308-309,3 13, 315 
explanatory variables, 3, 6 
exponential distribution, 15, 90 
exponential family, 14-1 5,230-233 
F 
factor, 167 
G 
gamma distribution, 15, 90, 232, 279-280 

488 
INDEX 
Gelman-Rubin diagnostic, 119, 143-144,449 
generalized gamma distribution, 90 
generalized harmonic mean estimator, 393 
generalized linear mixed models, 306, 320-337 
logit GLMM, 325-332 
Poisson GLMM, 333-337 
binary response models, 237, 263-268 
binomial response models, 237, 255-268 
continuous positive response, 236 
continuous response, 236 
examples, 245-255,263-268 
exponential family, 230-23 1 
for counts, 237 
for positive continuous responses, 279-280, 282 
gamma distribution, 232 
gamma model, 279-280,282 
inverse Gaussian model, 277-278 
inverse Gaussian distribution, 232 
link function, 229, 232 
logistic regression, 237, 255, 257-259, 261, 
negative binomial, 23 1, 283-285 
normal distribution, 23 1 
normal regression, 236,378-385 
parameter interpretation, 238-239, 242,244-245, 
Poisson distribution, 23 1 
Poisson regression, 237, 242-255 
posterior, 241-242 
prior, 239-241 
probit regression, 237 
stochastic component, 229 
systematic component, 229 
WinBUGS, 242-243 
generalized Poisson model, 286 
GeoBUGS, 87 
Geweke diagnostic, 448 
Gibbs sampling, 42, 71-74 
Gibbs variable selection, 409410 
goodness-of-fit, 380 
generalized linear models, 229-239 
263-268 
257-258,262-264,267-268 
deviance (DIC), 87, 140-141,219-226,248,318, 
426,428 
WinBUGS, 428 
initial values of a chain, 38 
inverse Gaussian 
distribution, 232 
example, 278 
model, 277-278 
item response model, 325 
iteration, 37 
iterations kept, 38 
J 
jump WinBUGS interface, 421 
K 
Kuo-Mallick sampler, 41 1 
L 
lag, 38 
Langragian Poisson distribution, 286 
Laplace approximation, 392 
likelihood, 3 
cross-validation, 423 
marginal, 389-391 
Lindley-Bartlett paradox, 390-392 
linear predictor, 152, 161-162,219, 229 
link function, 97, 152, 230 
canonical, 234,237,242,277,279-280,283 
clog-log, 234,262-263 
logit, 234, 237,257 
probit, 234,237 
log-linear models, see Poisson log-linear models 
log-normal distribution, 90 
logistic distribution, 90 
logistic regression, 56-71,237, 257, 255, 257-259, 
261,263-268 
example, 263-268 
parameter interpretation, 257-259,264,267-268 
See also binomial response models 
logit, 97,234, 237, 257, 261 
logit GLMM, 325-332 
low information prior, 5 
H 
harmonic mean estimator, 393-394 
hazard function, 297 
Heidelberger-Welch diagnostic, 449 
hierarchical models, 305-339 
logit GLMM, 325-332 
normal, 308-312,321-325 
Poisson GLMM, 333-337 
stage, 305 
hyperprior, 306 
1 
importance sampling, 394 
improper, 5 
independence sampler, 4 4 4 5  
information criteria, 424427 
Akaike (AIC), 426 
Bayesian (BIC), 425 
calculation from MCMC output, 428 
M 
marginal effect, 239 
marginal likelihood, 389-397 
bridge sampling estimators, 394-395 
Chib’s estimator, 395-397 
computation, 392-397 
harmonic mean estimator, 393-394 
importance sampling estimators, 394 
WinBUGS, 397-398 
bridge sampling, 394-395 
convergence, 40,42 
describing the target distribution, 38 
Gibbs sampling, 71-74 
importance sampling, 394 
independence sampler, 4 4 4 5  
intoduction, 35 
MCMC, 31,35-37 

INDEX 
489 
iterations kept, 38 
Metropolis-Hastings, 42, 45 
componentwise, 45-71 
example, 56-71 
Metropolis within Gibbs, 45, 64-68, 76 
missing values, 344-345 
output, 38 
random walk Metropolis, 4 3 4 4  
example, 46-55 
reversible jump, 42,420-421 
slice sampler, 76-81 
example, 77-81 
terminology, 37 
the algorithm, 36 
bumin, 38 
convergence, 37 
equilibrium distribution, 37 
initial values, 38 
iteration, 37 
lag, 38 
thinning interval, 38 
total number of iterations, 37 
median probability model, 413 
meta-analysis, 3 18-320 
Metropolis-Hastings algorithm, 4 2 4 6  
componentwise, 45-71 
independence sampler, 4 4 4 5  
random walk, 4 3 4 4  
single-component, 64-67 
Metropolis within Gibbs, 45, 64-68, 76 
missing values, 344-345 
model, 3, 151-152 
ANCOVA, 195-218 
ANOVA, 167-184 
one way, 167-1 72 
three way, 191-195 
two way, 173-175, 177-184 
binomial response models, 56-71, 237, 255-268, 
325-332 
bivariate Poisson, 291-293 
common slope, 197,204-212 
defining the model using matrices, 161-162 
DIC, 87,140-141,219-226,248,318,426,428 
different slopes, 197, 199, 212-216 
double Poisson, 292 
examples, 245-255,263-268 
for count data, 282 
for integer responses, 294-296 
for positive continuous responses, 279-280,282 
gamma, 279-280 
generalized linear mixed, 320, 338 
generalized Poisson, 286 
hierarchical, 305-339 
inverse Gaussian, 277-278 
item response, 325 
logistic regression, 56-71, 237,257,255,257-259, 
261,263-268 
logit GLMM, 325-326,328-332 
longitudinal, 338 
median urobabilitv. 4 13 
Clog-log, 262-263 
multilevel, 338 
multinomial, 298-300 
negative binomial, 283-285 
normal regression, 152-167, 236, 378-385 
parallel lines, 197, 204-212 
parameter interpretation, 238-239, 242, 245-246, 
Poisson-gamma, 3 15-3 16 
Poisson-log-normal, 3 16-3 17 
Poisson difference, 293-296 
Poisson GLMM, 333-337 
Poisson log-linear, 237, 242-255,269-270 
Poisson regression, 242-255 
probit regression, 237 
product binomial, 269 
Rasch, 325 
negative binomial, 283-285 
regression, 152-167, 236, 378-385 
repeated measures, 308-312,321-325 
statistical, 15 1 
zero inflated, 288-291 
assumptions, 365-368, 378-379 
goodness-of&, 368-372,380 
residuals, 377, 379-381 
WinBUGS, 380-383 
information criteria, 424428 
257-268,282 
model checking, 342-344,354, 358-372 
model comparison 
See also Bayesian model comparison 
model odds 
posterior, 389-391 
prior, 389-391 
modeling principles, 15 1-1 52 
Monte Carlo error, 39-40 
batch mean method, 39 
window estimator, 40 
example, 33-34 
Monte Carlo integration, 31-34 
multilevel models, 306, 338 
multinomial distribution, 15,91 
multinomial models, 298-300 
N 
negative binomial 
distribution, 15, 90, 231 
model, 283 
negative cross-validatory log-likelihood, 423 
nodes 
Doodle, 436438 
WinBUGS, 88-89,92-93 
nonconjugate analysis, 24, 27 
noninformative, 5 
normal-inverse gamma prior, 13 
normal distribution, 9-12, 15, 90-91, 231 
normal regression, 152-167, 236, 378-385 
conjugate analysis, 12-1 3 
model checking, 378-385 
parameter interpretation, 154-1 57 
predictive analysis, 378-385 
I 
~ 
~~~~ 
,, 
~- 
priors, 154, 162-1 64 

490 
INDEX 
0 
odc file, 86,88,98, 107-109, 120 
odds, 257-258 
odds ratio, 257-259 
output analysis, 38 
overdispersion, 237, 250,282,285, 307,315, 
333-334,338 
P 
p-value, posterior, 342 
parameter interpretation 
one way, 168-169 
two way (interaction model), 175 
two way (main effects model), 174 
ANOVA 
binomial response models, 257-264,267-268 
clog-log model, 262-264,267-268 
gamma model, 282 
generalized linear models, 238-239 
logistic regression, 257-259,264,267-268 
Poisson regression, 242, 245-246 
probit regression, 258,264, 267-268 
regression, 154-157 
parametrization, 168-169, 174-175, 189-191 
Pareto distribution, 90 
path sampling, 395 
Poisson-gamma model, 3 15-3 16 
Poisson-log-gamma model, 316-3 17 
Poisson difference model, 294-296 
Poisson distribution, 7-8, 15, 90, 231 
Poisson log-linear GLMM, 333-337 
Poisson log-linear models, 237, 242-255,269-270 
Poisson mixture models, 315-317 
Poisson regression, 237, 242-255 
examples, 245-255 
parameter interpretation, 242-245 
bivariate, 291-293 
difference, 293-296 
double, 292 
generalized, 286 
Langragian, 286 
zero inflated, 288 
Poisson 
Polya, 283 
posterior Bayes factor, 421, 423 
posterior predictive ordinate, 343, 359-361 
posterior 
distribution, 4 
model odds, 389-391 
predictive p-value, 342 
checking model assumptions, 365-368,378-379 
cross-validation, 375-378 
goodness-of-fit, 368-372,380 
model checking, 354,358-372,375-385 
model evaluation, 421-424 
normal regression models, 378-385 
prior, 389-391 
residuals, 362-365, 377, 379-381 
WinBUGS, 380-383 
missing values, 344-345 
predictive distribution, 7, 341-345 
predictive 
cross-validatory log-score, 422 
p-values, 342 
prior, 389-391 
probability of the data, 389,-391 
prior, 4,239-241 
conjugate, 7 
for normal regression model, 13, 154, 162-164 
generalized linear models, 239-241 
hyper, 306 
improper, 5 
model odds, 389-391 
normal-inverse gamma, 13 
predictive, 341, 389-391 
pseudo, 419 
unit information, 240, 407 
vague, 5 
variable selection, 406-409 
Zellner’s g-prior, 162-164, 167 
prior predictive distribution, 341 
probit 
function (WinBUGS), 97 
link, 234, 237,261,265 
model, 237,259-261,264-265 
parameter interpretation, 259-26 1,264 
pseudo Bayes factor, 421 
pseudoprior, 419 
R 
Raftery-Lewis diagnostic, 449 
random-number generation, 3 1 
randomeffects, 306-307,310,313,315-316 
random scan, 46 
random walk Metropolis, 4 3 4 4  
example, 46-55 
Rasch model, 325 
reference category, 168 
regression, 152-167,236, 378-385 
conjugate analysis, 12-13 
model checking, 378-385 
parameter interpretation, 154-157 
predictive analysis, 378-385 
priors, 154, 162-164 
repeated measures, 308-312,321-325,338 
residuals, 362-365, 377, 379-381 
response, 3 
reversible jump MCMC, 42, 420421 
S 
scripts, 86, 149, 443445 
sensitivity analysis, 7 
sensitivity of Bayes factor, 391-392 
simulation, 31-34 
Skellam distribution, 294-296 
slice sampling, 76-81 
state space model, 3 13-3 14 
statistical model, 3, 15 1 
stepwise method, 220-226 
stochastic component, 230 
stochastic search variable selection (SSVS), 41041 1 
stochastic simulation. 3 1 

INDEX 
491 
Student’s t distribution, 90-91 
survival analysis, 297-298 
systematic component, 230 
T 
target distribution, 37 
thinning interval, 38 
trace plots, 41 
U 
underdispersion, 237, 286-287 
uniform distribution, 90 
unit information prior, 241,407 
V 
vague prior distributions, 5 
variable inclusion indicators, 218 
variable selection, 405419 
Gibbs variable selection (GVS), 409-410 
Kuo-Mallick sampler, 41 1 
MCMC based posterior inference, 412413 
priors, 406409 
stochastic search variable selection (SSVS), 
410411 
WinBUGS, 414-419 
variance components, 306,309, 321, 325, 334, 338 
W 
Weibull distribution, 90 
WinBUGS menus, tools and options 
adapting indicator, 116 
attributes menu, 86 
auto cor button, 120 
axis text box, 133-134 
beg and end text boxes, 118, 136 
bgr diag button, 119 
blocking options submenu, 87 
box plot button, 133 
caterpillar button, 133 
chains box, 11 8 
clear button, 120 
coda button, 1 19 
compare tool, 87, 132-134 
correlations tool, 87, 132, 136-137 
density button, 1 18, 120 
DIC tool, 87, 133, 140-141 
Doodle menu, 85, 87 
edit menu, 85 
file menu, 85-86 
help menu, 85-86 
history button, 119-120 
inference menu, 85-87, 11 7, 132 
info menu, 85-86 
iteration number indicator, 11 5 
map menu, 85,87 
matrix button, 136-137 
model menu, 85-86 
model specification tool, 86, 108-109 
monitor Meurnonitor Metropolis tool, 86, 148 
node info tool, 86, 148 
node text box, 117, 133-134, 136 
options menu, 85-87 
others text box, 135 
output options submenu, 87 
overrelax check box, 116 
percentiles selection box, 119 
quantiles button, 119 
rank monitor tool, 87, 132, 138-140 
refresh text box, 115 
sarnples/sample monitor tool, 87, 115-120, 129 
save state (menu option), 86, 149 
scatter button, 136 
scatterplot option, 135 
script (menu option), 86 
seed (menu option), 86, 149 
set button, 116, 11 8 
short description, 85-87 
stats button, 119-120 
summarymonitor tool, 87, 132, 137-138 
text menu, 85-86 
thin text box, 115, 118 
tools menu, 85-86 
trace button, 11 8 
update button, 115 
update tool, 115 
update options submenu, 87 
updates number indicator, 115 
window menu, 85-86 
WinBUGS procedures 
acceptance rate of the chain, 148 
arrange icons, 86 
boxplot, 133 
check model, 108-1 10 
comparison of nodes, 133 
comparison plots, 87 
compile model, 86, 108, 113-117 
components, 86 
convergence, 87,119 
copy, cut & paste, 86 
correlations, 87 
error bar, 133 
figure properties, 145-147 
Gelman-Rubin convergence diagnostic, 143-144 
generate random values, 108, 115-1 17 
initial values, 114 
initial values set, 108 
load data, 86, 108, 11 1-1 12 
load initial values, 86 
monitor acceptance rate, 86 
monitor MCMC output, 87 
monitor parameters (set), 1 16 
multiple chain generation, 141-144 
obtaining a posterior sample (summary), 120, 122 
output analysis, 87, 117, 132-133, 142-143 
plots, 87 
posterior summaries, 87 
rankings, 87 
run MCMC, 108, 115 
save MCMC outputirandom values, 117 
save the current state of the chain, 86, 149 
scatterplot, 134-135 
script code, 86, 149,443-445 

492 
INDEX 
seed number set, 86 
simulate random values, 108, 115-117 
starting seed set, 149 
tile horizontalivertical, 86 
update MCMC, 86, 117 
arrays, 89,92-93 
brackets, 98-99 
curly braces, 98-99 
data specification, 100-102, 107 
data transformations, 108 
distribution commands, 90-91 
for syntax, 97-98 
functions, 93-97 
initial values, 107 
likelihood specification, 99-1 00 
list data format, 101-105 
loops, 97-98 
matrices, 89, 92-93 
mixed data formats, 105-107 
model specification example, 107-108 
multiple data definitions, 105-107 
nodes, 89,92-93 
parentheses, 98-99 
prior specification, 100 
rectangular data format, 100-101 
scalars, 89,92-93 
script commands, 444-445 
vectors, 89, 92-93 
zeros-ones trick, 276-277 
ANOVA, 169-172, 176-184 
batch mode, 443445 
Bayesian variable selection, 414419 
binary response models, 263-268,327-332 
binomial response models, 263-268,327-332 
code structure, 88-89 
WinBUGS syntax 
WinBUGS 
Clog-log, 266 
compound document, 86 
data (list format), 101-105 
data (mixed format), 105-107 
data (multiple), 105-107 
data (rectangular format), 100-101 
defining the model using matrices, 161-162 
deviance information criterion (DIC), 87, 140-141, 
distribution commands, 90-91 
doodle, 435441 
example of running a chain, 125-1 32 
generalized linear models, 242-243 
GeoBUGS, 87 
historical background, 83-84 
information criteria (BIC, AIC), 428 
installation, 84-85 
jump interface, 421 
marginal likelihood, 397-398 
missing values, 344-345 
model checking, 380-383 
model specification example, 107-108 
models with nonstandard distributions, 275-278 
nodes, 88-89,92-93 
normal regression 
examples, 157-161, 164-1 67 
likelihood, 153 
priors, 154 
428 
Poisson regression, 245 
Poisson regression example, 250 
predictive analysis, 380-383 
probit, 265 
short description, 85-87 
syntax preliminaries, 88-93 
Wishart distribution, 91 
Z 
Zellner’s g-prior, 162-164, 167 
zero inflated model, 288-291 

WILEY SERIES IN COMPUTATIONAL STATISTICS 
Billard and Diday . Symbolic Data Analysis: Conceptual Statistics and Data Mining 
Dunne . A Statistical Approach to Neural Networks for Pattern Recognition 
Ntzoufras . Bayesian Modeling Using WinBUGS 
Bayesian Modeling Using WinB UGS, by Ioannis Ntzoufras 
Copyright 0 
2009 John Wiley & Sons, Inc. 

