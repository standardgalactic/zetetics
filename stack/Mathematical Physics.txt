
Bruce R. Kusse and Erik A. Westwig 
Mathematical Physics 
Applied Mathematics for Scientists and Engineers 
2nd Edition 
WILEY- 
VCH 
WILEY-VCH Verlag GmbH & Co. KGaA 

This Page Intentionally Left Blank

Bruce R. Kusse and ErikA. Westwig 
Mathematical Physics 

Related Titles 
Vaughn, M. T. 
Introduction to Mathematical Physics 
2006. Approx. 650 pages with 50 figures. 
Softcover 
ISBN 3-527-40627-1 
Lambourne, R., Tinker, M. 
Basic Mathematics for the Physical Sciences 
2000.688 pages. 
Softcover 
ISBN 0-47 1-85207-4 
Tinker, M., Lambourne, R. 
Further Mathematics for the Physical Sciences 
2000.744 pages. 
Softcover 
ISBN 0-471-86723-3 
Courant, R., Hilbert, D. 
Methods of Mathematical Physics 
Volume 1 
1989. 575 pages with 27 figures. 
Softcover 
ISBN 0-47 1-50447-5 
Volume 2 
1989. 852 pages with 61 figures. 
Softcover 
ISBN 0-471-50439-4 
Trigg, G. L. (ed.) 
Mathematical Tools for Physicists 
2005.686 pages with 98 figures and 29 tables. 
Hardcover 
ISBN 3-527-40548-8 

Bruce R. Kusse and Erik A. Westwig 
Mathematical Physics 
Applied Mathematics for Scientists and Engineers 
2nd Edition 
WILEY- 
VCH 
WILEY-VCH Verlag GmbH & Co. KGaA 

The Authors 
Bruce R. Kusse 
College of Engineering 
Cornell University 
Ithaca, NY 
brk2@cornell.edu 
Erik Westwig 
Palisade Corporation 
Ithaca, NY 
ewestwig@palisade.com 
For a Solution Manual, lecturers should contact the 
editorial department at physics@wiley-vch.de, stating their 
affiliation and the course in which they wish to use the 
book. 
All books published by Wiley-VCH are carefully 
produced. Nevertheless, authors, editors, and 
publisher do not warrant the information contained in 
these books, including this book, to be free of errors. 
Readers are advised to keep in mind that statements, 
data, illustrations, procedural details or other items 
may inadvertently be inaccurate. 
Library of Congress Card No.: 
applied for 
British Library Cataloguing-in-Publication Data 
A catalogue record for this book is available from the 
British Library. 
Bibliographic information published by 
Die Dentsehe Bibliothek 
Die Deutsche Bibliothek lists this publication in the 
Deutsche Nationalbibliografie; detailed bibliographic 
data is available in the Internet at <http://dnb.ddb.de>. 
0 
2006 WILEY-VCH Verlag GmbH & Co. KGaA, 
Weinheirn 
All rights reserved (including those of translation into 
other languages). No part of this book may be repro- 
duced in any form ~ by photoprinting, microfilm, or 
any other means - nor transmitted or translated into a 
machine language without written permission from 
the publishers. Registered names, trademarks, etc. 
used in this book, even when not specifically marked 
as such, are not to be considered unprotected by law. 
Printing Strauss GmbH, Morlenbach 
Binding J. Schaffer Buchbinderei GmbH, Griinstadt 
Printed in the Federal Republic of Germany 
Printed on acid-free paper 
ISBN-13: 978-3-52740672-2 
ISBN-10: 3-527-40672-7 

This book is the result of a sequence of two courses given in the School of Applied 
and Engineering Physics at Cornell University. The intent of these courses has been 
to cover a number of intermediate and advanced topics in applied mathematics that 
are needed by science and engineering majors. The courses were originally designed 
for junior level undergraduates enrolled in Applied Physics, but over the years they 
have attracted students from the other engineering departments, as well as physics, 
chemistry, astronomy and biophysics students. Course enrollment has also expanded 
to include freshman and sophomores with advanced placement and graduate students 
whose math background has needed some reinforcement. 
While teaching this course, we discovered a gap in the available textbooks we felt 
appropriate for Applied Physics undergraduates. There are many good introductory 
calculus books. One such example is Calculus andAnalytic Geometry by Thomas and 
Finney, which we consider to be a prerequisite for our book. There are also many good 
textbooks covering advanced topics in mathematical physics such as Mathematical 
Methods for Physicists by Arfken. Unfortunately, these advanced books are generally 
aimed at graduate students and do not work well for junior level undergraduates. It 
appeared that there was no intermediate book which could help the typical student 
make the transition between these two levels. Our goal was to create a book to fill 
this need. 
The material we cover includes intermediate topics in linear algebra, tensors, 
curvilinear coordinate systems, complex variables, Fourier series, Fourier and Laplace 
transforms, differential equations, Dirac delta-functions, and solutions to Laplace’s 
equation. In addition, we introduce the more advanced topics of contravariance and 
covariance in nonorthogonal systems, multi-valued complex functions described with 
branch cuts and Riemann sheets, the method of steepest descent, and group theory. 
These topics are presented in a unique way, with a generous use of illustrations and 
V 

vi 
PREFACE 
graphs and an informal writing style, so that students at the junior level can grasp and 
understand them. Throughout the text we attempt to strike a healthy balance between 
mathematical completeness and readability by keeping the number of formal proofs 
and theorems to a minimum. Applications for solving real, physical problems are 
stressed. There are many examples throughout the text and exercises for the students 
at the end of each chapter. 
Unlike many text books that cover these topics, we have used an organization that 
is fundamentally pedagogical. We consider the book to be primarily a teaching tool, 
although we have attempted to also make it acceptable as a reference. Consistent 
with this intent, the chapters are arranged as they have been taught in our two course 
sequence, rather than by topic. Consequently, you will find a chapter on tensors and 
a chapter on complex variables in the first half of the book and two more chapters, 
covering more advanced details of these same topics, in the second half. In our 
first semester course, we cover chapters one through nine, which we consider more 
important for the early part of the undergraduate curriculum. The last six chapters 
are taught in the second semester and cover the more advanced material. 
We would like to thank the many Cornell students who have taken the AEP 
3211322 course sequence for their assistance in finding errors in the text, examples, 
and exercises. E.A.W. would like to thank Ralph Westwig for his research help and 
the loan of many useful books. He is also indebted to his wife Karen and their son 
John for their infinite patience. 
BRUCE 
R. KUSSE 
ERIK 
A. WESTWIG 
Ithaca, New York 

CONTENTS 
1 A Review of Vector and Matrix Algebra Using 
SubscriptlSummation Conventions 
1.1 Notation, I 
1.2 Vector Operations, 5 
1 
2 Differential and Integral Operations on Vector and Scalar Fields 
18 
2.1 Plotting Scalar and Vector Fields, 18 
2.2 Integral Operators, 20 
2.3 Differential Operations, 23 
2.4 Integral Definitions of the Differential Operators, 34 
2.5 TheTheorems, 35 
3 Curvilinear Coordinate Systems 
3.1 The Position Vector, 44 
3.2 The Cylindrical System, 45 
3.3 The Spherical System, 48 
3.4 General Curvilinear Systems, 49 
3.5 The Gradient, Divergence, and Curl in Cylindrical and Spherical 
Systems, 58 
44 

viii 
CONTENTS 
67 
4 Introduction to Tensors 
4.1 The Conductivity Tensor and Ohm’s Law, 67 
4.2 General Tensor Notation and Terminology, 71 
4.3 Transformations Between Coordinate Systems, 7 1 
4.4 Tensor Diagonalization, 78 
4.5 Tensor Transformations in Curvilinear Coordinate Systems, 84 
4.6 Pseudo-Objects, 86 
5 The Dirac &Function 
5.1 Examples of Singular Functions in Physics, 100 
5.2 Two Definitions of &t), 103 
5.3 6-Functions with Complicated Arguments, 108 
5.4 Integrals and Derivatives of 6(t), 11 1 
5.5 Singular Density Functions, 114 
5.6 The Infinitesimal Electric Dipole, 121 
5.7 Riemann Integration and the Dirac &Function, 125 
6 Introduction to Complex Variables 
6.1 A Complex Number Refresher, 135 
6.2 Functions of a Complex Variable, 138 
6.3 Derivatives of Complex Functions, 140 
6.4 The Cauchy Integral Theorem, 144 
6.5 Contour Deformation, 146 
6.6 The Cauchy Integrd Formula, 147 
6.7 Taylor and Laurent Series, 150 
6.8 The Complex Taylor Series, 153 
6.9 The Complex Laurent Series, 159 
6.10 The Residue Theorem, 171 
6.1 1 Definite Integrals and Closure, 175 
6.12 Conformal Mapping, 189 
100 
135 

CONTENTS 
7 Fourier Series 
7.1 The Sine-Cosine Series, 219 
7.2 The Exponential Form of Fourier Series, 227 
7.3 Convergence of Fourier Series, 231 
7.4 The Discrete Fourier Series, 234 
8 Fourier Transforms 
8.1 Fourier Series as To -+ m, 250 
8.2 Orthogonality, 253 
8.3 Existence of the Fourier Transform, 254 
8.4 The Fourier Transform Circuit, 256 
8.5 Properties of the Fourier Transform, 258 
8.6 Fourier Transforms-Examples, 
267 
8.7 The Sampling Theorem, 290 
9 Laplace Transforms 
9.1 Limits of the Fourier Transform, 303 
9.2 The Modified Fourier Transform, 306 
9.3 The Laplace Transform, 313 
9.4 Laplace Transform Examples, 314 
9.5 Properties of the Laplace Transform, 318 
9.6 The Laplace Transform Circuit, 327 
9.7 Double-Sided or Bilateral Laplace Transforms, 331 
10 Differential Equations 
10.1 Terminology, 339 
10.2 Solutions for First-Order Equations, 342 
10.3 Techniques for Second-Order Equations, 347 
10.4 The Method of Frobenius, 354 
10.5 The Method of Quadrature, 358 
10.6 Fourier and Laplace Transform Solutions, 366 
10.7 Green’s Function Solutions, 376 
ix 
219 
250 
303 
339 

X 
11 Solutions to Laplace’s Equation 
11.1 Cartesian Solutions, 424 
1 1.2 Expansions With Eigenfunctions, 433 
11.3 Cylindrical Solutions, 441 
1 1.4 Spherical Solutions, 458 
12 Integral Equations 
12.1 Classification of Linear Integral Equations, 492 
12.2 The Connection Between Differential and 
Integral Equations, 493 
12.3 Methods of Solution, 498 
13 Advanced Topics in Complex Analysis 
13.1 Multivalued Functions, 509 
13.2 The Method of Steepest Descent, 542 
14 Tensors in Non-Orthogonal Coordinate Systems 
14.1 A Brief Review of Tensor Transformations, 562 
14.2 Non-Orthononnal Coordinate Systems, 564 
15 Introduction to Group Theory 
15.1 The Definition of a Group, 597 
15.2 Finite Groups and Their Representations, 598 
15.3 Subgroups, Cosets, Class, and Character, 607 
15.4 Irreducible Matrix Representations, 612 
15.5 Continuous Groups, 630 
Appendix A The Led-Cidta Identity 
Appendix B The Curvilinear Curl 
Appendiv C The Double Integral Identity 
Appendix D Green’s Function Solutions 
Appendix E Pseudovectors and the Mirror Test 
CONTENTS 
424 
491 
509 
562 
597 
639 
641 
645 
647 
653 

CONTENTS 
xi 
Appendix F Christoffel Symbols and Covariant Derivatives 
Appendix G Calculus of Variations 
Errata List 
Bibliography 
Index 
655 
661 
665 
671 
673 

This Page Intentionally Left Blank

1 
A REVIEW OF VECTOR AND 
MATRIX ALGEBRA USING 
SUBSCRIPTISUMMATION 
CONVENTIONS 
This chapter presents a quick review of vector and matrix algebra. The intent is not 
to cover these topics completely, but rather use them to introduce subscript notation 
and the Einstein summation convention. These tools simplify the often complicated 
manipulations of linear algebra. 
1.1 NOTATION 
Standard, consistent notation is a very important habit to form in mathematics. Good 
notation not only facilitates calculations but, like dimensional analysis, helps to catch 
and correct errors. Thus, we begin by summarizing the notational conventions that 
will be used throughout this book, as listed in Table 1 .l. 
TABLE 1.1. Notational Conventions 
Symbol 
Quantity 
a 
A real number 
A complex number 
A vector component 
A matrix or tensor element 
An entire matrix 
A vector 
@, 
A basis vector 
T 
A tensor 
L 
An operator 
- 
- 
1 

2 
A R E W W  OF VECTOR AND MATRIX ALGEBRA 
A three-dimensional vector 
can be expressed as 
v = VX& + VY&, + VZ&, 
(1.1) 
where the components (Vx, V,, V,) are called the Cartesian components of 
and 
(ex. e,, $) are the basis vectors of the coordinate system. This notation can be made 
more efficient by using subscript notation, which replaces the letters (x, y, z) with the 
numbers (1,2,3). That is, we define: 
Equation 1.1 becomes 
or more succinctly, 
i= 1,2,3 
Figure 1.1 shows this notational modification on a typical Cartesian coordinate sys- 
tem. 
Although subscript notation can be used in many different types of coordinate 
systems, in this chapter we limit our discussion to Cartesian systems. Cartesian 
basis vectors are orthonormal and position independent. Orthonoml means the 
magnitude of each basis vector is unity, and they are all perpendicular to one another. 
Position independent means the basis vectors do not change their orientations as we 
move around in space. Non-Cartesian coordinate systems are covered in detail in 
Chapter 3. 
Equation 1.4 can be compacted even further by introducing the Einstein summation 
convention, which assumes a summation any time a subscript is repeated in the same 
term. Therefore, 
i=1,2,3 
I 
I 
I 
Y 
I 
2 
Figure 1.1 The Standard Cartesian System 

NOTATION 
3 
We refer to this combination of the subscript notation and the summation convention 
as subscripthummation notation. 
Now imagine we want to write the simple vector relationship 
This equation is written in what we call vector notation. Notice how it does not 
depend on a choice of coordinate system. In a particular coordinate system, we can 
write the relationship between these vectors in terms of their components: 
C1 = A1 + B1 
C2 = A2 + B2 
C3 = A3 + B3. 
(1.7) 
With subscript notation, these three equations can be written in a single line, 
where the subscript i stands for any of the three values (1,2,3). As you will see 
in many examples at the end of this chapter, the use of the subscript/summation 
notation can drastically simplify the derivation of many physical and mathematical 
relationships. Results written in subscripthummation notation, however, are tied to 
a particular coordinate system, and are often difficult to interpret. For these reasons, 
we will convert our final results back into vector notation whenever possible. 
A matrix is a two-dimensional array of quantities that may or may not be associated 
with a particular coordinate system. Matrices can be expressed using several different 
types of notation. If we want to discuss a matrix in its entirety, without explicitly 
specifying all its elements, we write it in matrix notation as [MI. If we do need to 
list out the elements of [MI, we can write them as a rectangular array inside a pair of 
brackets: 
(1.9) 
We call this matrix array notation. The individual element in the second row and 
third column of [MI is written as M23. Notice how the row of a given element is 
always listed first, and the column second. Keep in mind, the array is not necessarily 
square. This means that for the matrix in Equation 1.9, r does not have to equal c. 
Multiplication between two matrices is only possible if the number of columns 
in the premultiplier equals the number of rows in the postmultiplier. The result of 
such a multiplication forms another matrix with the same number of rows as the 
premultiplier and the same number of columns as the postmultiplier. For example, 
the product between a 3 X 2 matrix [MI and a 2 X 3 matrix [N] forms the 3 X 3 matrix 

4 
A REVIEW OF VECTOR AND MATRIX ALGEBRA 
[PI, with the elements given by: 
7 
[PI 
The multiplication in Equation 1.10 can be written in the abbreviated matrix notation 
as 
[n/il"l = [PI, 
(1.11) 
Ml JNJk = PIk, 
(1.12) 
We can also use subscripthmmation notation to write the same product as 
with the implied sum over the j index keeping track of the summation. Notice j 
is in the second position of the Mtj term and the first position of the N,k term, so 
the summation is over the columns of [MI and the rows of [N], just as it was in 
Equation 1.10. Equation 1.12 is an expression for the iPh element of the matrix [PI. 
Matrix array notation is convenient for doing numerical calculations, especially 
when using a computer. When deriving the relationships between the various quan- 
tities in physics, however, matrix notation is often inadequate because it lacks a 
mechanism for keeping track of the geometry of the coordinate system. For example, 
in a particular coordinate system, the vector v might be written as 
V = lel + 3e2 + 2C3. 
(1.13) 
When performing calculations, it is sometimes convenient to use a matrix represen- 
tation of this vector by writing: 
v +  [V] = [;I. 
(1.14) 
The problem with this notation is that there is no convenient way to incorporate the 
basis vectors into the matrix. This is why we are careful to use an arrow (-) in 
Equation 1.14 instead of an equal sign (=). In this text, an equal sign between two 
quantities means that they are perfectly equivalent in every way. One quantity may 
be substituted for the other in any expression. For instance, Equation 1.13 implies 
that the quantity 1C1 + 3C2 + 2C3 can replace 
in any mathematical expression, and 
vice-versa. In contrast, the arrow in Equation 1.14 implies that [Vl can represent v, 
and that calculations can be performed using it, but we must be careful not to directly 
substitute one for the other without specifying the basis vectors associated with the 
components of [ Vl . 
- 
- 

VECTOR OPERATIONS 
5 
1.2 VECTOR OPERATIONS 
In this section, we investigate several vector operations. We will use all the different 
forms of notation discussed in the previous section in order to illustrate their dif- 
ferences. Initially, we will concentrate on matrix and matrix array notation. As we 
progress, the subscript/summation notation will be used more frequently. 
can be represented using a 
matrix. There are actually two ways to write this matrix. It can be either a (3 X 1) 
column matrix or a (1 X 3) row matrix, whose elements are the components of the 
vector in some Cartesian basis: 
As we discussed earlier, a three-dimensional vector 
- 
V+[V] = [ ;] 
or 
v-. [u+ 
= [ V, ~2 
V, I. 
(1.15) 
The standard notation [VJt has been used to indicate the transpose of [Vl, indicating 
an interchange of rows and columns. Remember the vector 
can have an infinite 
number of different matrix array representations, each written with respect to a 
different coordinate basis. 
1.2.1 Vector Rotation 
Consider the simple rotation of a vector in a Cartesian coordinate system. This 
example will be worked out, without any real loss of generality, in two dimensions. 
We start with the vector A, which is oriented at an angle 8 to the 1-axis, as shown 
in Figure 1.2. This vector can be written in terms of its Cartesian components as 
- 
A = A,& + A&, 
(1.16) 
where 
A~ = A C O S ~  
A2 = AsinO. 
(1.17) 
In these expressions A 3 1x1 = ,/A; 
+ A; is the magnitude of the vector A. The 
vector A' is generated by rotating the vectorx counterclockwise by an angle 4. This 
Figure 1.2 Geometry for Vector Rotation 

6 
A REVIEW OF VECTOR AND MATRIX ALGEBRA 
changes the orientation of the vector, but not its magnitude. Therefore, we can write 
A’ = A cos(8 + +)el + A sin(8 + 4)&. 
(1.18) 
-- 
A: 
A: 
The components A: and A; can be rewritten using the trigonometric identities for the 
cosine and sine of summed angles as 
A; = Acos(8 + 4) = Acos8cos4 - AsinOsin4 
- * 
(1.19) 
AI 
A2 
A; = Asin(8 + 4) = Acos8sin4 +Asin8cos4. 
+ * 
If we represent A and A’ with column matrices, 
- 
A -+ [A] = [::I 
x’ 
-+ [A‘] = 
, 
Equations 1.19 can be put into matrix array form as 
(1.20) 
(1.21) 
In the abbreviated matrix notation. we can write this as 
In this last expression, [R(4)] is called the rotation matrix, and is clearly defined as 
cos+ 
-sin+ 
(1.23) 
Notice that for Equation 1.22 to be the same as Equation 1.19, and for the matrix 
multiplication to make sense, the matrices [A] and [A’] must be column matrix arrays 
and [R(4)] must premultiply [A]. The result of Equation 1.19 can also be written 
using the row representations for A and A‘. In this case, the transposes of [R], [A] 
and [A’] must be used, and [RIt must postmultiply [AIt: 
[A’It = [AIt [RIt. 
(1.24) 
Written out using matrix arrays, this expression becomes 
(1.25) 
It is easy to see Equation 1.25 is entirely equivalent to Equation 1.2 1. 

VECTOR OPERATIONS 
7 
These same manipulations can be accomplished using subscriptlsummation nota- 
tion. For example, Equation 1.19 can be expressed as 
A; = R. 
1J .A J .. 
(1.26) 
The matrix multiplication in Equation 1.22 sums over the columns of the elements 
of [R]. This is accomplished in Equation 1.26 by the implied sum over j .  Unlike 
matrix notation, in subscriptlsummation notation the order of A, and Rij is no longer 
important, because 
R. 
‘I .A J . = A J .R.. 
‘I’ 
(1.27) 
The vector ;I’ 
can be written using the subscript notation by combining Equa- 
tion l .26 with the basis vectors 
This expression demonstrates a “notational bookkeeping” property of the subscript 
notation. Summing over a subscript removes its dependence from anexpression, much 
like integrating over a variable. For this reason, the process of subscript summation 
is often called contracting over an index. There are two sums on the right-hand side 
(RHS) of Equation 1.28, one over the i and another over j .  After contraction over 
both subscripts, the are no subscripts remaining on the RHS. This is consistent with 
the fact that there are no subscripts on the left-hand side (LHS). The only notation 
on the LHS is the “overbar” on h‘, indicating a vector, which also exists on the RHS 
in the form of a “hat” on the basis vector @ i .  This sort of notational analysis can be 
applied to all equations. The notation on the LHS of an equals sign must always agree 
with the notation on the RHS. This fact can be used to check equations for accuracy. 
For example, 
&’ f RijAj, 
(1.29) 
because a subscript i remains on the RHS after contracting over j ,  while there are no 
subscripts at all on the LHS. In addition, the notation indicates the LHS is a vector 
quantity, while the RHS is not. 
1.2.2 Vector Products 
We now consider the dot and cross products of two vectors using subscriptlsummation 
notation. These products occur frequently in physics calculations, at every level. The 
dot product is usually first encountered when calculating the work W done by a force 
F in the line integral 
- 
W = 1 d F - F .  
(1.30) 

8 
A REVIEW OF VECTOR AND MATRIX ALGEBRA 
In this equation, d? is a differential displacement vector. The cross product can be 
used to find the force on a particle of charge q moving with velocity t in an externally 
applied magnetic field B: 
F = q(V x B). 
The Dot Product The dot or inner product of two vectors, 
defined by 
(1.31) 
and B, is a scalar 
- 
A . B = IKl I@ cos 0, 
(1.32) 
where 0 is the angle between the two vectors, as shown in Figure 1.3. If we take the 
dot product of a vector with itself, we get the magnitude squared of that vector: 
- 
A .  h = &I2. 
(1.33) 
In subscripthmmation notation, Equation 1.32 is written as 
- -  
A.B=A.C..B.C. 
1
1
 
J J '  
(1.34) 
Notice we are using two different subscripts to form 
and B. This is necessary to 
keep the summations independent in the manipulations that follow. The notational 
bookkeeping is working here, because there are no subscripts on the LHS, and none 
left on the RHS after contraction over both i and j. Only the basis vectors are involved 
in the dot product, so Equation 1.34 can be rewritten as 
- -  
A .  B = AiBj(Ci * @ j ) .  
(1.35) 
Because we are restricting our attention to Cartesian systems where the basis vectors 
are orthonormal, we have 
The Kronecker delta, 
8.. Ee 
'J {A 
2 
rl" 
(1.36) 
(1.37) 
Figure 13 The Dot Product 

VECTOR OPERATIONS 
9 
facilitates calculations that involve dot products. Using it, we can write Ci . @; 
= a,;, 
and Equation 1.35 becomes 
- -  
A ' B = AiBj6ij. 
(1.38) 
Equation 1.38 can be expanded by explicitly doing the independent sums over both i 
and j 
A.B=AIB1611 +AlB2612 +A1B3613 +A2B1&1 + .... 
(1.39) 
Since the Kronecker delta is zero unless its subscripts are equal, Equation 1.39 reduces 
to only thee terms, - -  
A .  B = AlBl + AzB2 + A3B3 
AiBi. 
(1.40) 
As one becomes more familiar with the subscript/summation notation and the 
Kronecker delta, these last steps here are done automatically with the RHS of the 
brain. Anytime a Kronecker delta exists in a term, with one of its subscripts repeated 
somewhere else in the same term, the Kronecker delta can be removed, and every 
instance of the repeated subscript changed to the other subscript of the Kronecker 
symbol. For example, 
A . 6 . .  
1 'J = A .  
J -  
(1.41) 
In Equation 1.38 the Kronecker delta can be grouped with the B; factor, and contracted 
over j to give 
Ai(B;&j) = AiBi. 
(1.42) 
Just as well, we could group it with the Ai factor, and sum over i to give an equivalent 
result: 
Bj(Ai6i;) = Bj A;. 
(1.43) 
This is true for more complicated expressions as well. For example, 
Mij(Ak6ik) = MijAi 
or 
(1.44) 
Bi T;k(C, a;,,,) 
= Bi TjkC j .  
This flexibility is one of the things that makes calculations performed with sub- 
scripthmmation notation easier than working with matrix notation. 
We should point out that the Kronecker delta can also be viewed as a matrix or 
matrix array. In thee dimensions, this representation becomes 
6,j + [I] = 0 1 0 . 
(1.45) 
This matrix can be used to write Equation 1.38 in matrix notation. Notice the con- 
traction over the index i sums over the rows of the [ 13 matrix, while the contraction 
[: :] 

10 
A REVIEW OF VECTOR AND MATRIX ALGEBRA 
over j sums over the columns. Thus, Equation 1.38 in matrix notation is 
1
0
0
 
0
0
1
 
x.B-+[A]t[l][B] = [A1 
A2 
A31 [0 
1 01 [ii] 
= [AIt[B]. 
(1.46) 
The Cross Product The cross or vector product between two vectors 
a third vector c, which is written 
andB forms 
- 
C = A X B .  
( 1.47) 
The magnitude of the vector e is 
where 8 is the angle between the two vectors, as shown in Figure 1.4. The direction 
of c depends on the “handedness” of the coordinate system. By convention, the 
three-dimensional coordinate system in physics are usually “right-handed.’’ Extend 
the fingers of your right hand straight, aligned along the basis vector 61. Now, curl 
your fingers toward &he basis vector $2. If your thumb now points along 63, the 
coordinate system is right-handed. When the coordinate system is arranged this way, 
the direction of the cross product follows a similar rule. To determine the direction of 
C in Equation 1.47, point your fingers along A, and curl them to point along B. Your 
thumb is now pointing in the direction of e. This definition is usually called the right- 
hand mle. Notice, the direction of 
is always perpendicular to the plane formed 
by A and B. If, for some reason, we are using a left-handed coordinate system, 
the definition for the cross product changes, and we must instead use a left-hand 
rule. Because the definition of a cross product changes slightly when we move to 
- 
I 
I 
, 
\ 
\ 
\ 
\ , 
\ 
\ 
‘. ‘ 
Figure 1.4 
The Cross Product 
- 1  

VECTOR OPERATIONS 
11 
systems of different handedness, the cross product is not exactly a vector, but rather a 
pseudovector. We will discuss this distinction in more detail at the end of Chapter 4. 
For now, we will limit our discussion to right-handed coordinate systems, and treat 
the cross product as an ordinary vector. 
Another way to express the cross product is by using an unconventional determi- 
nant of a matrix, some of whose elements are basis vectors: 
Expanding the determinant of quation 1.49 gives 
( 1.49) 
This last expression can also be written using subscript/summation notation, with 
the introduction of the Levi-Civita symbol Gjk: 
where Eijk is defined as 
+ 1 
for ( i ,  j, k) = even permutations of (1,2,3) 
if two or more of the subscripts are equal 
1 
for (i, j, k) = odd permutations of (1,2,3) . 
(1.52) 
An odd permutation of (1,2,3) is any rearrangement of the three numbers that can be 
accomplished with an odd number of pair interchanges. Thus, the odd permutations 
of (1,2,3) are (2,1,3), (1,3,2), and (3,2,1). Similarly, the even permutations of 
(1,2,3) are (1,2,3), (2,3, l), and (3,1,2). Because the subscripts i ,  j ,  and k can each 
independently take the values (1,2,3), one way to visualize the Levi-Civita symbol 
is as the 3 X 3 X 3 array shown in Figure 1.5. 
Figurr 1.5 The 3 X 3 X 3 Levi-Civita Array 

12 
A REVIEW OF VECTOR AND MATRIX ALGEBRA 
The cross product, written using subscriptlsummation notation in Equation 1.51, 
and the dot product, written in the form of Equation 1.38 are very useful for manual 
calculations, as you will see in the following examples. 
1.2.3 calculations Using Subscript/summation Notation 
We now give two examples to demonstrate the use of subscript/summation notation. 
The first example shows that a vector’s magnitude is unaffected by rotations. The 
primary function of this example is to show how a derivation performed entirely 
with matrix notation can also be done using subscript notation. The second derives a 
common vector identity. This example shows how the subscript notation is a powerful 
tool for deriving complicated vector relationships. 
Example 1.1 
Refer back to the rotation picture of Figure 1.2, and consider the prod- 
ucts A - A and A - A , first using matrix notation and then using subscriptlsummation 
notation. Because A’ is generated by a simple rotation of A we know these two dot 
products, which represent the magnitude squared of the vectors, should be equal. 
-/ 
-/ 
_ _  
Using matrices: 
- 
A - A -+ [AIt[A] 
A’. A’ -+ [A’It[A’]. 
(1.53) 
( 1.54) 
But [A’] and [Allt can be expressed in terms of [A] and [AIf as 
M’I = [N4)I[Al 
[A’]+ = [Alt[R(4)lt, 
(1.55) 
where R(+) is the rotation matrix defined in Equation 1.23. If these two equations 
are substituted into Equation 1.54, we have 
A’ ’ A’ 
+ [Alt[R(4)lt[R(~,)I[Al. 
(1.56) 
The product between the two rotation matrices can be performed, 
and Equation 1.56 becomes 
-/ 
-/ 
A - A  -i 
[A]’[l][A] = [AIt[A] -+A*A. 
Our final conclusion is that 
(1.58) 
(1.59) 
To arrive at this result using matrices, care had to be taken to be sure that the matrix 
operations were in the proper order and that the row, column, and transpose matrices 
were all used correctly. 

VECTOR OPERATIONS 
13 
Now let’s repeat this derivation using the subscriptlsummation notation. Equa- 
tion l .40 allows us to write 
_ -  
A .  A = AiAi 
(1.60) 
(1.61) 
A . A  = A:A:. 
Notice how we have been careful to use different subscripts for the two sums in 
Equations 1.60 and 1.61. This ensures the sums will remain independent as they 
are manipulated in the following steps. The primed components can be expressed in 
terms of the unprimed components as 
-1 
-1 
A,! = R..A 
‘I 
I ., 
(1.62) 
where Rij is the ijth component of the rotation matrix R [ 4 ] .  Inserting this expression 
into Equation 1.61 gives 
(1.63) 
A * A  = R,A,R,,A,, 
where again, we have been careful to use the two different subscripts u and v .  This 
equation has three implicit sums, over the subscripts r ,  u, and u. 
In subscript notation, unlike matrix notation, the ordering of the terms is not 
important, so we rearrange Equation 1.63 to read 
--I 
--I 
(1.64) 
A * A  = A,A,R,R,,. 
Next concentrate on the sum over r ,  which only involves the [R] matrix elements, 
in the product R,R,,. What exactly does this product mean? Let’s compare it to an 
operation we discussed earlier. In Equation 1.12, we pointed out the subscripted ex- 
pression MijNjk represented the regular matrix product [M][N], because the summed 
subscript j is in the second position of the [MI matrix and the first position of the 
[N] matrix. The expression R,R,,, however, has a contraction over the first index of 
both matrices. In order to make sense of this product, we write the first instance of 
[R] using the transpose: 
--I 
--I 
RruRru + [Rlt [Rl- 
(1.65) 
Consequently, from Equation 1.57, 
R,R,, = &,. 
(1.66) 
Substituting this result into Equation 1.64 gives 
(1.67) 
Admittedly, this example is too easy. It does not demonstrate any significant 
advantage of using the subscriptlsummation notation over matrices. It does, how- 
ever, highlight the equivalence of the two approaches. In our next example, the 
subscriptlsummation notation will prove to be almost indispensable. 

14 
A REVIEW OF VECTOR AND MATRIX ALGEBRA 
~~~ 
~~ 
Example 1.2 
The subscript/summation notation allows the derivation of vector 
identities that seem almost impossible using any other approach. The example worked 
out here is the derivation of an identity for the double cross product between three 
vectors, 
X (B X 0. This one example essentially demonstrates all the common 
operations that occur in these types of manipulations. Other examples are suggested 
in the problems listed at the end of this chapter. 
The expression A X (B X c) is written in vector notation and is valid in any 
coordinate system. To derive our identity, we will convert this expression into sub- 
script/summation notation in a Cartesian coordinate system. In the end, however, we 
will return our answer to vector notation to obtain a result that does not depend upon 
any coordinate system. In this example, we will need to use the subscripted form for 
a vector 
- 
v = Vi&, 
(1.68) 
for a dot product between two vectors 
_ _  
A * B = AiBi, 
(1.69) 
and for a cross product 
To begin, let 
- 
D = B X C ,  
(1.71) 
which, written using the Levi-Civita symbol, is 
= BiCj&€ijk. 
(1.72) 
Substituting Equation 1.71 into the expressionx X (BX c), 
and using the Levi-Civita 
expression again, gives 
A x  (Bx C) = Ax D =  A ~ D ~ ~ + ~ E , . , ~ .  
(1.73) 
The sth component of D is obtained by dot multiplying both sides of Equation 1.72 
by ii, as follows: 
Substituting the result of Equation 1.74 into Equation 1.73 gives 
(1.74) 
(1.75) 

EXERCISES 
15 
which we rearrange slightly to read 
- 
A X (B X C) = ArBiCjCtErstEijs. 
(1.76) 
To proceed, some properties of the Levi-Civita symbol need to be developed. First, 
because of the definition of the Levi-Civita symbol given in Equations 1.52, it is clear 
that reversing any two of its subscripts just changes its sign, i.e., 
E . .  = - E .  1kJ . = E .  
Jk1. 
. 
(1.77) 
The second property involves the product of two Levi-Civita symbols that have a 
common last subscript: 
ZJk 
EijkEmnk = 8im8jn - 8in8jm. 
(1.78) 
With a considerable amount of effort, it can be shown that the RHS of Equation 1.78 
has all the properties described by the product of the two Levi-Civita symbols on 
the LHS, each governed by Equations 1.52. A proof of this identity is given in 
Appendix A. 
With Equations 1.77 and 1.78 we can return to Equation 1.76, which can now be 
rewritten 
- 
A X (B X C) = A r B , C j & , ( 8 , j & i  - & a t j ) .  
(1.79) 
After removing the Kronecker deltas, we obtain 
- 
A X (B X C) = A j B i C j C i  - A i B i C j C j .  
(1 30) 
At this point, you can really see the usefulness of the subscript/summation notation. 
The factors in the two terms on the F2HS of Equation 1.80 can now be rearranged, 
grouped according to the summations, and returned to vector notation in just two 
lines! The procedure is: 
(1.81) 
(1.82) 
Equation 1.81 is valid only in Cartesian systems. But because Equation 1.82 is in 
vector notation, it is valid in any coordinate system. 
K X (B X C) = ( A j C j ) ( B i C i )  - ( A i B i ) ( C j e j >  
= (K * C)B - (K- 
B)C 
In the exercises that follow, you will derive several other vector identities. These 
will illustrate the power of the subscript/summation notation and help you become 
more familiar with its use. 
EXERCISES FOR CHAPTER 1 
1. Consider the two matrices: 

16 
A REVIEW OF VECTOR AND MATRIX ALGEBRA 
With matrix notation, a product between these two matrices can be expressed as 
[kfI[Nl. Using subscript/summation notation, this same product is expressed as 
(a) With the elements of the [MJ and [N] matrices given above, evaluate the 
matrix products [MJ[Nl, [N[MJ and [MI[kfl, leaving the results in matrix 
array notation. 
(b) Express the matrix products of part (a) using the subscriptlsummation nota- 
tion. 
(c) Convert the following expressions, which are written in subscriptlsummation 
notation, to matrix notation: 
Mi j N j k .  
i. MjkNij. 
ii. MijNkj. 
fi. M ..N. 
Jl 
Jk* 
iV. MijNjk $. Tki. 
V. MjiNkj -k Tik. 
2. Consider the square 3 X 3 matrix [MI whose elements Mij are generated by the 
expression 
M . .  
11 = ij2 
and a vector v whose components in some basis are given by 
vk = k 
forc, j = 1,2,3, 
fork = 1,2,3. 
(a) Using a matrix representation for 
-+ [u, 
determine the components of 
(b) Determine the components of the vector that result from a premultiplication 
the vector that result from a premultiplication of [u 
by [MI. 
of [MI by [Ut. 
3. Thematrix 
1 
,Rl = [ cos8 
sin8 
-sin8 
cos8 
represents a rotation. Show that the matrices [RI2 = [R][R] and [RI3 = [RlER][Rl 
are matrices that correspond to rotations of 28 and 38 respectively. 
4. Let [D] be a 2 X 2 square matrix and [V] a 2 X 1 row matrix. Determine the 
conditions imposed on [D] by the requirement that 
[D" 
= [VI+[Dl 
for any [V]. 
5. The trace of a matrix is the sum of all its diagonal elements. Using the sub- 
scriptlsummation notation to represent the elements of the matrices [TI and 
lM1, 

EXERCISES 
17 
(a) Find an expression for the trace of [TI. 
(b) Show that the trace of the matrix formed by the product [ T ] [ M ]  is equal to 
the trace of the matrix formed by [MI[TI. 
6. Let [MI be a square matrix. Express the elements of the following matrix products 
using subscriptlsummation notation: 
(a) [M][M]+. 
(b) [Mlt[Ml. 
(4 
[MI + [MI t. 
7. Convert the following Cartesian, subscript/summation expressions into vector 
notation: 
(a) VfA,Bf@,. 
(b) cA f B, 6,. 
(c) AIBj@k%jk8b. 
(d) Af B, CIDm E f j k  %k. 
8. Expand each of the following expressions by explicitly writing out all the terms 
in the implied summations. Assume (i, j ,  k) can each take on the values (1,2,3). 
(a) 8 i j E f j k .  
(b) Tfj.4,. 
(4 
6f,Tf,Al. 
9. Express the value of the determinant of the matrix 
using subscriptlsummation notation and the Levi-Civita symbol. 
10. Prove the following vector identities using subscript/summation notation: 
(a) A. 
(Bx C) = ( A X  B>. c7 
(b) A X B = - B X A  
(c) (A 
X B) . ( C X  D) = (A- C)(B- D> - (A. D)(B. CT. 
(d) (A X B) X (c 
X D) = [(AX B> . D]c- [ ( A X  B>. CTDY 

2 
DIFF'ERENTIALANDINTEGRAL 
OPERATIONS ON VECTOR AND 
SCALAR FIELDS 
A field quantity is a function that depends upon space and possibly time. Electric 
potential, charge density, temperature, and pressure each have only a magnitude, and 
are described by scalar fields. In contrast, electric and magnetic fields, gravity, current 
density, and fluid velocity have both magnitude and direction, and are described by 
vector fields. 
Integral and differential operations on vector and scalar field quantities can be 
expressed concisely using subscript notation and the operator formalism which is 
introduced in this chapter. 
2.1 PL€YITING SCALAR AND VECTOR FIELDS 
Because it is frequently useful to picture vector and scalar fields, we begin with a 
discussion of plotting techniques. 
2.1.1 
Plotting scalar Fields 
Plots of scalar fields are much easier to construct than plots of vector fields, because 
scalar fields are characterized by only a single value at each location in time and space. 
Consider the specific example of the electric potential 
produced by two parallel, 
uniform line charges of ? h, coulombs per meter that are located at (x = 5 1, y = 0). 
In meter-kilogram-second (MKS) units, the electric potential produced by this charge 
distribution is 
a = -  
A0 
[ (x + 
+ Y * ]  
47TE, 
(x - 1)2 + y2 . 
18 

PLOTTING SCALAR AND VECTOR FIELDS 
19 
~ 
Electric field lines 
Equipotentials 
Figure 2.1 Equipotentials and Electric Field Lines of Two Parallel Line Charges 
Usually we want to construct surfaces of constant CP, which in this case are cylinders 
nested around the line charges. Because there is symmetry in the z direction, these 
surfaces can be plotted in two dimensions as the dashed, circular contours shown in 
Figure 2.1. The centers of these circles are located along the x-axis from 1 < x < 03 
for positive values of <P, and from --M < x < - 1 for negative values of CP. CP = 0 
lies on the y-axis, which you can think of as a circle of infinite radius with its center 
at infinity. If the contours have evenly spaced constant values of <P, the regions of 
highest line density show where the function is most rapidly varying with position. 
2.1.2 Plotting Vector Fields 
Because vectors have both magnitude and direction, plots for their fields are usually 
much more complicated than for scalar fields. For example, the Cartesian components 
of the electric field of the preceding example can be calculated to be 
A vector field is typically 
field vector at every point in 
(2.2) 
I 
"[ 
x2 - y2 - 1 
[(x - 1)2 + y2][(x + 1)2 + y2] 
7 E 0  
(2.3) 
I. 
"[ 
2XY 
7rE0 [(x - 1)2 + y2][(x + 1)2 + y2] 
drawn by constructing lines which are tangent to the 
space. By convention, the density of these field lines 

20 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
indicates the magnitude of the field, while arrows show its direction. If we suppose 
an electric field line for Equations 2.2 and 2.3 is given by the equation y = y(x), then 
With some work, Equation 2.4 can be integrated to give 
x2 + 0, - c)2 = 1 + 2, 
(2.5) 
where c is the constant of integration. This constant can be varied between --03 and 
-03 to generate the entire family of field lines. For this case, these lines are circles 
centered on the y-axis at y = c with radii given by I/=. 
They are shown as the 
solid lines in Figure 2.1. The arrows indicate how the field points from the positive 
to the negative charge. Notice the lines are most densely packed directly between the 
charges where the electric field is strongest. 
2.2 INTEGRALOPERATORS 
2.2.1 Integral Operator Notation 
The gradient, divergence, and curl operations, which we will review later in this 
chapter, are naturally in operator form. That is, they can be represented by a symbol 
that operates on another quantity. For example, the gradient of @ is written as v@. 
Here the operator is v, which acts on the operand @ to give us the gradient. 
In contrast, integral operations are comonly not written in operator form. The 
integral of f ( x )  over x is often expressed as 
which is not in operator form because the integral and the operand f(x) are inter- 
mingled. We can, however, put Equation 2.6 in operator form by reorganizing the 
terms in this equation: 
Now the operator dx acts on f (x) to form the integral, just as the v operator acts 
on @ to form the gradient. In practice, the integral operator is moved to the right, 
passing through all the terms of the integrand that do not depend on the integration 
variable. For example, 
/ dx x2(x + y)y2 = y 2  / dx x2(x + y ) .  
J 
J 

INTEGRAL OPERATORS 
21 
2.2.2 Line Integrals 
The process of taking an integral along a path is called line integration and is a 
common operation in all branches of physics. For example, the work that a force F 
performs as it moves along path C is 
W = L d F - F .  
(2.9) 
Here the line integral operator 
diacts on the force F. The differential displacement 
vector dF is tangent to each point along C, as shown in Figure 2.2. If C closes on 
itself, we write the operator with a circle on the integral sign: 
s, 
(2.10) 
Because Equation 2.9 is written in vector notation, it is valid in any coordinate 
system. In a Cartesian system, dF = dx$i and Equation 2.9 becomes 
(2.1 1) 
Notice that the quantity produced by this integration is a scalar, and the subscript i is 
summed implicitly. 
There are other less common line integral operations. For example, the operator 
in 
acts on the scalar @ to produce a vector. Another example, 
(2.12) 
(2.13) 
generates a vector using the cross product. Note that all these subscripted expressions 
are written in a Cartesian system where the basis vectors are orthonormal and position 
independent. 
Figure 2.2 The Line Integral 

22 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
223 Surfacehtegrals 
Surface integrals involve the integral operator, 
l d*, 
(2.14) 
where do is a vector that represents a differential area. This vector has a magnitude 
equal to a differential area of S, and a direction perpendicular to the surface. If we 
write the differential area as du and the unit normal ii, the differential area vector 
becomes d o  = i3 du. Because a surface has two sides, there is ambiguity in how 
to define b. For a simple, closed surface such as the one in Figure 2.3(a), we define 
ci to always point in the "outward" direction. If the surface is not closed, i.e., does 
not enclose a volume, the direction of b is typically determined by the closed path 
C that defines the boundaries of the surface, and the right-hand rule, as shown in 
Figure 2.3(b). 
Frequently, the surface integral operator acts on a vector field quantity by means 
of the dot product 
/dT.V. 
(2.15) 
S 
h Cartesian coordinates. 
where dui is positive or negative depending on the sign of ii. &i, as discussed in the 
previous paragraph. This surface integral becomes 
(2.17) 
There are less common surface integrals of the form 
2 
Y '  
(2.18) 
Figure 2.3 
Surface Integrals 

DIFFERENTIAL OPERATIONS 
23 
which is an operation on a scalar which produces a vector quantity, and 
P 
which also generates a vector. 
(2.19) 
2.2.4 
Volume Integrals 
The volume integral is the simplest integral operator because the variables of inte- 
gration are scalars. It is written 
(2.20) 
where dT is a differential volume, and V represents the total volume of integration. 
The most common volume integral acts on a scalar field quantity and, as a result, 
produces a scalar 
d r  @. 
In Cartesian coordinates, this is written 
dxl dx2 dx3 @. 
Volume integrals of vector quantities are also possible: 
(2.21) 
(2.22) 
(2.23) 
2.3 DIFFERENTIAL OPERATIONS 
By their definition, field quantities are functions of position. Analogous to how 
the change in a function of a single variable is described by its derivative, the 
position dependence of a scalar field can be described by its gradient, and the position 
dependence of a vector field by its curl and divergence. The Del operator v is used 
to describe all three of these fundamental operations. 
The operator v is written in coordinate-independent vector notation. It can be 
expressed in subscript/summation notation in a Cartesian coordinate system as 
(2.24) 
Keep in mind, this expression is only valid in Cartesian systems. It will need to be 
modified when we discuss non-Cartesian coordinate systems in Chapter 3. 

24 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
When operating on a scalar field, the v operator produces a vector quantity called 
the gradient: 
For example, in electromagnetism the electric field is equal 
of the electric potential: 
(2.25) 
to the negative gradient 
(2.26) 
The Del operator also acts upon vector fields via either the dot product or the cross 
product. The divergence of a vector field is a scalar quantity created using the dot 
product: 
(2.27) 
The charge density pc in a region of space can be calculated using the divergence 
from the relation 
- -  
pc = E,(V * E). 
(2.28) 
If instead we used the cross product, we produce a vector quantity called the curl, 
(2.29) 
where we have used the Levi-Civita symbol to express the cross product in sub- 
script/summation notation. One of Maxwell's equations relates the electric field to 
the rate of change of the magnetic field using the curl operator: 
- 
d B  
V x E = - - - .  
d t  
(2.30) 
2.3.1 
Physical Picture of the Gradient 
The gradient of a scalar field is a vector that describes, at each point, how the field 
changes with position. Dot multiplying both sides of Equation 2.25 by dT = dx& 
gives 
Manipulating the RHS of this expression gives 
(2.31) 
(2.32) 

DIFFERENTIAL OPERATIONS 
25 
The RHS of this expression can be recognized as the total differential change of Q, 
due to a differential change of position dF. The result can be written entirely in vector 
notation as 
dQ, = VQ, 
* dF. 
(2.33) 
From Equation 2.33, it is clear that the largest value of dQ, occurs when dT points in 
the same direction as VQ,. 
On the other hand, a displacement perpendicular to VQ, 
produces no change in Q, because d@ = 0. This means that the gradient will always 
point perpendicular to the surfaces of constant Q,. 
At the beginning of this chapter, we discussed the electric potential function 
generated by two line charges. The electric field was generated by taking the gradient 
of this scalar potential, and was plotted in Figure 2.1. While we could use this example 
as a model for developing a physical picture for the gradient operation, it is relatively 
complicated. Instead, we will look at the simpler two-dimensional function 
Q, = -xy. 
(2.34) 
A plot of the equipotential lines of @ in the xy-plane is shown in Figure 2.4. The 
gradient operating on this function generates the vector field 
- 
VQ, = -ye, -xi?,. 
(2.35) 
Now imagine we are at the point (1,2) and move to the right, by an infinitesi- 
mal amount dr along the positive x-axis. The corresponding change in Q, can be 
determined by calculating: 
dQ, =vQ, 
.dF 
= (-26, - 16,) . (dr C,) 
= - 2 d r .  
(2.36) 
Y 
Figure 2.4 
Surfaces of Constant @ = -xy 

26 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
This says CP decreases by 2 units for every unit moved in that direction. If instead, 
we were sitting at the point (3,4) and move an infinitesimal amount dr, at a 45-degree 
angle with respect to the x-axis, 
changes by an amount 
(2.37) 
-7 
= -dr. 
Jz 
Notice, these changes are for infinitesimal quantities. To calculate the change of CP 
over a finite path, where the gradient changes as we move from point to point, we 
need to use a line integral 
(2.38) 
When using the gradient to generate a vector field, we usually add a negative sign 
to the definition. For example, the electric field is generated from the electrostatic 
potential by 
- 
E = -V@. 
(2.39) 
Using this convention, moving against the field lines increases CP. For the potential 
of Equation 2.34, the negative gradient is 
- 
- VCP = yGx + xi?,. 
(2.40) 
The field lines for this function can be determined as follows: 
dY - x 
dx 
Y 
- _ -  
d y  y = d x  x 
y2 = x2 + c 
x2 - y2 = c. 
(2.41) 
These lines are perpendicular to the lines of constant @, as shown in Figure 2.5. 
Notice how the density of the vector field lines shows that the magnitude of the field 
increases as you move away from the origin. 
In summary, the gradient of a scalar function CP generates a vector field which, at 
each point, indicates the direction of the greatest increase of @ and lies perpendicular 
to the lines or surfaces of constant @. The example discussed above was worked 
out in two dimensions, but the process can also be visualized in three dimensions, 

DIFFERENTIAL OPERATIONS 
27 
Figure 2.5 
Field Lines for @ = -xy 
where CD = constant generates surfaces, and the gradient is always normal to these 
surfaces. Except for visualization, there are no limits to the number of dimensions 
for the gradient operation. 
2.3.2 Physical Picture of the Divergence 
The divergence operation will be described physically by developing the continuity 
equation, which describes the change in the local particle density of a fluid as a func- 
tion of time. Let p(x, y ,  z, t )  be the number of particles per unit volume andT(x, y. z, t )  
be the velocity of these particles, at the point (x, y, z) and the time t. Consider a dif- 
ferential volume d r  = dx dy dz located at (G, yo, G), as shown in Figure 2.6. The 
continuity equation is obtained by making the assumption that particles can be neither 
created nor destroyed in this volume, and then equating any net flow of particles into 
or out of the volume with a corresponding change in p. If we call the total number of 
particles in the infinitesimal volume N = p d ~ ,  
then 
(2.42) 
The rate of change of N in Equation 2.42 needs to be accounted for by the flow 
of particles across the six surfaces of dr. First, consider the bottom cross-hatched 
surface of Figure 2.6. The flow across this surface can be determined with the aid 
of Figure 2.7(a). The total number of particles that enter the volume in a time dt 
across this surface is equal to the number of particles in the shaded region dx dy ydt. 
Notice, a positive v, adds particles to the volume, while a negative v, removes them. 
Thus, the contribution of the bottom surface to dN/& is 
(2.43) 

DIFFERENTIAL AND INTEGRAL OPERATIONS 
dx 
Figwt 2.6 
Differential Volume 
Notice, both p and v, are evaluated at (xo, yo, G) in this expression. By defining the 
vector 5 = p3, which is called the current density, Equation 2.43 can be written more 
concisely as 
(2.44) 
This same type of calculation can be made for the top cross-hatched surface 
shown in Figure 2.6. Figure 2.7(b) shows that now a positive y carries the number 
of particles in the shaded region out of the volume. This side contributes 
JNbottom 
~- - JJxo, yo, ~ 0 ,  
t )  dx dY. 
at 
(2.45) 
to the total dN/at. Notice, in this case, we evaluate J, at the point (a. 
yo, zo + dz). 
Combining Equations 2.44 and 2.45 gives 
dN0p - -Jz(x0, yo, zo + dz, t )  dx d y  
- _  
at 
aNbottom 
aNtop - 
+-- 
[JZ(xo, 
yo, zo, t )  - Jz(xo, yo, zo + dz, t>] dx d y .  (2.46) 
d t  
at 
dx 
dx 
(a> 
(b) 
Figure 2.7 
Flow Across the Bottom and Top Surfaces 

DIFFERENTIAL OPERATIONS 
29 
This last expression can be written in terms of the derivative of Jz, because in the 
differential limit we have 
Jz(xo, Y O ,  zo + dz, t )  = Jz(xo, yo, zo? t )  + - 
(2.47) 
Substitution of Equation 2.47 into Equation 2.46 gives 
(2.48) 
Working on the other four surfaces produces similar results. The total flow into 
the differential volume is therefore 
- -  
which can be recognized as -V * J times d7. Combining this result with Equation 
2.42 gives us the continuity equation: 
(2.50) 
For a positive divergence of J, more particles are leaving than entering a region so 
dp/dt is negative. 
This exercise provides us with a physical interpretation of the divergence. If the 
divergence of a vector field is positive in a region, this region is a source. Field 
lines are “born” in source regions. On the other hand, if the divergence in a region is 
negative, the region is a sink. Field lines “terminate” in sink regions. If the divergence 
of a vector field is zero in a region, then all field lines that enter it must also leave. 
2.3.3 Physical Picture of the Curl 
The curl of a vector field is a vector, which describes on a local scale the field’s 
circulation. From the word curl, itself, it seems reasonable to conclude that if a vector 
field has a nonzero curl the field lines should be “curved,” while fields with zero curl 
should be “straight.” This is a common misconception. It is possible for vector field 
lines to appear as shown in Figure 2.8(a), clearly describing a “curved” situation, 
and have no curl. Also, the fields lines shown in Figure 2.8(b), which are definitely 
“straight,” can have a nonzero curl. To resolve this confusion, we must look at the 
curl on a differential scale. 
Consider a vector field v that is a function of only x and y. The curl of this field 
points in the z-direction, and according to Equation 2.29 is given by 
(2.51) 
for a Cartesian coordinate system. 

30 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
(4 
(b) 
Figure 2.8 
Circulating and Noncirculating Vector Fields 
Now consider the line integral of a vector field v around the closed path shown in 
Figure 2.9, 
/ d P . v .  
(2.52) 
The starting point for the integration is the point (h, 
yo). In this derivation, we need to 
be a little more careful with the treatment of infinitesimal quantities than we were in 
the discussion of the divergence. For this reason, we begin by letting the dimensions 
of the loop be Ax and Ay, as shown in the figure. Later in the derivation, we will 
shrink these to infinitesimal quantities to obtain our final result. 
The integral along C can be broken up into four parts. First consider the integration 
along C1, where y = yo and x varies from no to xo + Ax: 
C 
x,+Ax 
[ d ? - v =  [ 
dxVx. 
(2.53) 
JCl 
JXO 
Along this segment, we can expand V,(x, yo) with a Taylor series, keeping up to the 
linear term in x: 
(2.54) 
Ax 
Flgun? 2.9 Closed Path for Curl Integration 

DIFFERENTIAL OPERATIONS 
31 
Keeping higher-order terms in this expansion will not change the results of this deriva- 
tion. Substituting Equation 2.54 into Equation 2.53 and performing the integration 
gives 
(2.55) 
Next the integration along C3, the top section of the loop, is performed. Along this 
path, y is held fixed at y = yo + A y ,  while x varies from xo + Ax to x,. Therefore, 
L, 
d f .  V = l:Ax 
dx V,. 
(2.56) 
Again, we expand Vx(x, yo + A y )  to first order with a Taylor series: 
Substituting Equation 2.57 into Equation 2.56 and performing the integration gives 
Combining Equations 2.55 and 2.58 gives 
- 
L,d?*V+ 1 , d i . V ~  -31 A x A y .  
(2.59) 
After performing similar integrations along the C, and C, paths, we can combine all 
the results to obtain 
ay 
(X0,YO) 
(2.60) 
The error of Equation 2.60 vanishes as the dimensions of the loop shrink to the 
infinitesimal, Ax ----f 0 and A y  + 0. Also, using Equation 2.51, the term in the 
brackets on the RHS of Equation 2.60 can be identified as the z component of v X v. 
Therefore, we can write 
(2.61) 
where C is the contour that encloses S and duz = dx d y  is a differential area of that 
surface. 
What does all this tell us about the curl? The result in Equation 2.61 can be 
reorganized as: 
(2.62) 

32 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
- ~ -  
C 
Figure 2.10 Fields with Zero (a) and Nonzero (b) Curl 
This says the z component of V x V at a point is the line integral of V on a loop 
around that point, divided by the area of that loop, in the limit as the loop becomes 
vanishingly small. Thus, the curl does not directly tell us anything about the circulation 
on a macroscopic scale. 
The situations in Figure 2.8 can now be understood. If the “curved” field shown in 
Figure 2.10(a) has a magnitude that drops off as l/r, exactly enough to compensate 
for the increase in path length as r increases, then the integral around the closed 
differential path shown in the figure will be zero. Thus, the curl at that point is 
also zero. If the magnitude of the “straight” vector field in Figure 2.10(b) varies as 
indicated by the line density, the integral around the differential path shown cannot 
be zero and the field will have a nonzero curl. 
We derived Equation 2.61 in two dimensions and only picked out the z-component 
of the curl. The generalization of this result to three dimensions, and for any orienta- 
tion of the differential loop, is straightforward and is given by 
In this equation, S is still the area enclosed by the path C. Note that the direction of 
diF is determined by the direction of C and a right-hand convention. 
23.4 Differential Operator Identities 
Subscriptlsummation notation greatly facilitates the derivation of differential operator 
identities. The relations presented in this section are similar to the vector identities 
discussed in Chapter 1, except now care must be taken to obey the rules of differential 
calculus. As with the vector identities, a Cartesian coordinate system is used for 
the derivations, but the final results are expressed in coordinate-independent vector 
notation. 
Example 2.1 
subscriptlsummation notation, make the substitution 
Consider the operator expression, 7. 
(7@). 
To translate this into 
(2.64) 

DIFFERENTIAL OPERATIONS 
33 
The two v operators in the original expression must be written using independent 
subscripts: 
(2.65) 
Because the basis vectors of a Cartesian system are position independent, &j/dxi = 
0, and Equation 2.65 becomes 
- 
v . (Va) 
= (ei * e j ) -  (5) 
axi 
axj 
= ,,a(+,) 
ax, ax 
= -(-a) 
d
a
 
axi 
axi 
(2.66) 
In the last line, we have written out the implied summation explicitly to show you 
how it is working in this case. Equation 2.66 can be returned to vector notation by 
defining the Laplacian operator V2 as 
so that 
- 
v . (7,) 
= v2a. 
(2.68) 
Example 2.2 
which is the curl of the curl 
of v. This quantity arises when developing the electromagnetic wave equation from 
Maxwell’s equations. To write this in subscript notation in a Cartesian coordinate 
system, we need to use two Levi-Civita symbols: 
Consider the expression V X  V X  
Equation 2.69 can be manipulated as follows: 
(2.69) 

34 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
(2.70) 
Finally, the right-hand side of Equation 2.70 is converted back into vector notation 
to obtain the result 
v X V X V = V ( V - 5 )  -v2V. 
(2.7 1) 
Notice that the Laplacian operator can act on both vector and scalar fields. In Equation 
2.68, the Laplacian operates on a scalar field to product a scalar. In Equation 2.71, it 
operates on a vector field to produce a vector. 
- 
2.4 
INTEGRAL DEFJNITIONS OF THE DIFFERENTLAL OPERATORS 
In Equations 2.25,2.27, and 2.29, we provided expressions for calculating the gradi- 
ent, divergence, and curl. Each of these relations is valid only in a Cartesian coordinate 
system, and all are in terms of spatial derivatives of the field. Integal definitions of 
each of the differential operators also exist. We already derived one such definition 
for the curl in Equation 2.63. In this section, we present similar definitions for the 
gradient and divergence, as well as an alternate definition for the curl. Their deriva- 
tions, which are all similar to the derivation of Equation 2.63, are in most introductory 
calculus texts. We present only the results here. 
The gradient of a scalar field at a particular point can be generated from 
(2.72) 
where V is a volume that includes the point of interest, and S is the closed surface 
that surrounds V. Both S and V must be shrunk to infinitesimal size for this equation 
to hold. 
To get the divergence of a vector field at a point, we integrate the vector field over 
an infinitesimal surface S that encloses that point, and divide by the infinitesimal 
volume: 
We already derived the integral definition 
(2.73) 
(2.74) 
which generates the curl. This definition is a bit clumsy because it requires the 
calculation of three different integrals, each with a different orientation of S, to get 
all three components of the curl. The following integral definition does not have this 

THE THEOREMS 
35 
problem, but it uses an uncommon form of the surface integral: 
2.5 THE THEOREMS 
(2.75) 
The differential operators give us information about the variation of vector and 
scalar fields on an infinitesimal scale. To apply them on a macroscopic scale, we 
need to introduce four important theorems. Gauss’s Theorem, Green’s Theorem, 
Stokes’s Theorem, and Helmholtz’s Theorem can be derived directly from the integral 
definitions of the differential operators. We give special attention to the proof and 
discussion of Helmholtz’s Theorem because it is not covered adequately in many 
texts. 
2.5.1 Gauss’s Theorem 
Gauss’s Theorem is derived from Equation 2.73, written in a slightly different form: 
(2.76) 
In this equation, the closed surface S completely surrounds the volume dr, which we 
have written as an infinitesimal. 
Equation 2.76 can be applied to two adjacent differential volumes dr1 and d72 that 
have a common surface, as shown in Figure 2.11: 
- 
V . A d r 1  + V - A d r 2  = 
dZF-%+ 
(2.77) 
The contributions to the surface integral from the common surfaces cancel out, as 
depicted in the figure, so Equation 2.77 can be written as 
d 
izdrr.A. 
Figure 2.11 
The Sum of Two Differential Volumes 

36 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
Figure 2.12 The Sum of Two Differential Volumes 
where S1+2 is the outside surface enclosing both d q  and d72, as shown in Figure 
2.12. We can continue this process of adding up contiguous differential volume to 
form an arbitrary finite volume V enclosed by a closed surface S. The result, 
is called Gauss’s Theorem. 
(2.79) 
2.5.2 Green’s Theorem 
Green’s Theorem takes two forms and follows directly from Gauss’s Theorem and 
some vector operator manipulations. Start by considering the expression 
. ( u v v ) ,  
where u and u are both scalar fields. Using a vector operator identity, which you will 
prove in one of this chapter’s exercises, this expression can be rewritten as 
- 
v . ( U V U )  = v u  - vv + u v 2 v .  
(2.80) 
Switching u and u gives 
v . ( U V U )  = v u  
* vu + vv2u. 
(2.81) 
When Equation 2.81 is subtracted from Equation 2.80, the result is 
v . ( U V V )  - v . ( V V U )  = uv2v - vv2u. 
(2.82) 
Finally, we integrate both sides of Equation 2.82 over a volume V and apply Gauss’s 
Theorem, to generate one form of Green’s Theorem: 
j d B .  ( u v v  - v v u )  = 
dr[uV2v - vV2u]. 
S 
(2.83) 
In this expression, the closed surface S surrounds the volume V. The same process 
applied directly to Equation 2.80 produces a second form of Green’s Theorem: 
/ d Z F .  ( u v v )  = 
d r F u * v v  + u V 2 v ] .  
(2.84) 
S 

THE THEOREMS 
37 
2.5.3 Stokes’s Theorem 
Stokes’s Theorem derives from Equation 2.74, 
(2.85) 
where the path C encloses the differential surface d Z  in the right-hand sense. 
The development of Stokes’s theorem follows steps similar to those for Gauss’s 
theorem. Equation 2.85 is applied to two adjacent differential surfaces that have a 
common border, as shown in Figure 2.13. The result is 
(2.86) 
where the path C I + ~  
is the closed path surrounding both dal and daz. The line 
integrals along the common edge of C1 and C2 cancel exactly. Any number of these 
differential areas can be added up to give an arbitrary surface S and the closed contour 
C which surrounds S .  The result is Stokes’s Theorem: 
(2.87) 
There is an important consequence of Stokes’s Theorem for vector fields that have 
- 
zero curl. Such a field can always be derived from a scalar potential. That is to say, if 
V X x = 0 everywhere, then there exists a scalar function a@) 
such that A = -v@. 
To see this, consider the two points 1 and 2 and two arbitrary paths between them, 
Path A and Path B, as shown in Figure 2.14. A closed line integral can be formed 
by combining Path A and the reversal of path B. If 
X A = 0 everywhere, then 
Equation 2.87 lets us write 
(2.88) 
Figure 2.13 The Sum of Two Differential Surfaces 

38 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
Point 2 
B 
Point 1 
Figure 214 Stokes’s Theorem Implies a Scalar Potential 
(2.89) 
or 
Equation 2.89 says the line integral of A between the two points is independent of 
the path taken. This means that it is possible to define a scalar function of position 
@(f) such that its total differential is given by 
d@ = - d f * A .  
(2.90) 
It is conventional to add the negative sign here so Q, increases as you move against 
the field lines of A. Inserting Equation 2.90 into the line integrals of Equation 2.89 
shows that these integrals are both equal to 
1 
-d@ = @(1) - @(2). 
(2.91) 
Referring back to Equation 2.33, it is clear that the condition of Equation 2.90 can 
be rewritten as 
- 
A =  -V@. 
(2.92) 
In summary, if the curl of a vector field is everywhere zero, the field is deriv- 
able from a scalar potential. The line integral of this type of vector field is always 
independent of the path taken. Fields of this type are often called conservative. 
25.4 Helmhdtz’s Theorem 
Helmholtz’s Theorem states: 
A vector field, if it exists, is uniquely determined by specifying its divergence and curl 
everywhere within a region and its normal component on the closed surface surrounding 
that region. 

THE THEOREMS 
39 
There are two important parts of this statement. On one hand, it says if we have a 
field v that we are trying to determine, and we know the values of V . V and v X v 
at every point in some volume, plus the normal component of fi on the surface of 
that volume, there is only one fi that will make all this work. On the other hand, we 
have the disclaimer, “if it exists.” This qualification is necessary because it is entirely 
possible to specify values for the divergence, curl, and normal component of a vector 
field that cannot be satisfied by any field. 
To prove Helmholtz’s Theorem we will assume the two vector fields vl and v2 
have the same values for the divergence, curl, and normal component. Then we show 
that if this is the case, the two solutions must be equal. Let w = v1 - v,. Since 
the divergence, curl, and the dot product are all linear operators, %’ must have the 
following properties: 
- _  
- _  
V * W = 0 
V x w = 0 
ii. W = o 
in the region 
in the region 
on the surface. 
- 
(2.93) 
Because v X 
= 0, 
can be derived from a scalar potential 
w =  
-V@. 
(2.94) 
Green’s Theorem, in the form of Equation 2.84, is now applied with u = u = 
give 
to 
j d * . @ ( V @ )  = S,d+V. (U@) +V@..@ 
S 
Inserting Equation 2.94 into Equation 2.95 gives 
j 
d a  . CDW = k d r ( @ 8 .  w - w. 
W). 
S 
(2.96) 
Using Equations 2.93, the surface integral on the LHS and the volume integral of 
@v . Won the RHS are zero, with the result that 
- _  
(2.97) 
Because lWI2 is always a positive quantity, the only way to satisfy Equation 2.97 is 
to have W = 0 everywhere. Consequently, v1 = v2 and Helmholtz’s Theorem is 
proven. 
Helmholtz’s Theorem is useful for separating vectors fields into two parts, one 
with zero curl, and one with zero divergence. This discussion relies on two identities 
V . ( t T X K )  = o  
(2.98) 
VXT@ =o, 
(2.99) 
- 

40 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
which you will prove at the end of the chapter. Write v as 
(2.100) 
- 
V = V X A - V @ .  
Then we can write: 
- 
v .v = -V2@ 
- 
v x v = v x v x K. 
ii.v= ii- ( V X A - E D ) ,  
(2.101) 
Since the divergence, curl, and normal component are all specified if 
and @ 
are specified, Helmholtz’s Theorem says v is also uniquely specified. Notice the 
contribution to v that comes from 
has no divergence since v * (v X ;Ir> = 0. 
This is called the rotational or solenoidal part of the field and 
is called the vector 
potential. The portion of 
X v@ = 0. 
This is called the irrotational part of the field and @ is called the scalar potential. 
which arises from @ has no curl, because 
EXERCISES FOR CHAPTEX 2 
1. Consider the two-dimensional scalar potential function 
CP(x,y) = x3 - 3y2x. 
Make a plot of the equipotential contours for three positive and three negative 
values for CP. 
Find v@ 
. 
Show that the v@ 
field lines are given by setting 3x2y - y3 equal to a series 
of constants. 
Plot six representative V@ field lines. Be sure to indicate the direction of 
the field and comment on its magnitude. 
Find V * VCP, the divergence of the v@ 
vector field, and show that your 
field lines of part (d) agree with this divergence. 
electric dipole p is located at the origin of a Cartesian system. This dipole 
_ _  
creates an electric potential field @@) given by 
where F is the position vector, F = ni4. Let 
(a) Sketch the equipotential lines, @ = constant. 
(b) Find the electric field, a = -v@. 
(c) Sketch the electric field lines. 
= p&. 

EXERCISES 
41 
3. Perform the line integral 
/ d F X v ,  
C 
where C is the contour shown below 
Y 
C 
1 
-1 
and 
(a) V =  v,&,. 
(b) v =  V&. 
(c) v = v,r 
VOY 
(a) 7, 
(fA) = f(T-K) +A.Vj. 
- 
F 2 T  
X2 + y2 
+ - 
,&+ 
4. Use subscript/summation notation to verify the following identities: 
(b) VXvXA=V(V.-A;) -V2A. 
(c) v x  (fA) = f ( V X A )  + V f X A .  
(f) 
= fVg + gvf. 
(g) V(A.B) = A x  (VxB) + B x  (VXX) + (A.V)B+ ( B 3 ) A .  
(d) V - ( ( A X B ) = B * ( V X A ) - A * ( T X B ) .  
(e) A X  (VxB) = (VB) .A- (A.V)B. 
(h) 
X V,f = 0. 
(i) V .  (V x A) = O. 
5. Calculate the work done by following a straight line path from the Cartesian 
point (1, 1) to (3,3), if the force exerted is given by 
F = (x - y)Cx + (x + y)$. 
Can this force be derived from a scalar potential? Pick any other path that goes 
from (1,l) to (3,3) and calculate the work done. 

42 
DIFFERENTIAL AND INTEGRAL OPERATIONS 
6. Let a vector field v be given by 
- 
v = x b  + yey. 
(a) Use Stokes’s Theorem to evaluate the line integral, 
where C is the unit circle in the xy-plane centered at (0,O). 
centered at (1,O). 
themselves. 
(b) Use Stokes’s Theorem to determine the same line integral if C is a unit circle 
(c) Evaluate the two integrals above by actually performing the line integrals 
7. Show that if B = 7 
X x, 
then for any closed surface S, 
k d 5 . B  = 0. 
8. Use the differential operator theorems to evaluate 
j k d T . V ,  
S 
where S is the surface of a sphere of radius R centered at the origin, and v is 
(a) V = xQ + yzY + z&. 
(b) v = x3& + y3Cy + $6,. 
(c) v = x5Q + y5zy + 2$. 
9. Use an appropriate vector theorem to evaluate 
where C is any closed path and 
- 
v = (2y2 - 3x2y)Cx + (4xy - x3)@,. 
10. A force field is given by the relationship 
- 
F = (2x + 2y)& + (2y + 2x)iiy. 
(a) What is v - F? 
(c) Sketch the force field lines. 
What is V x F? 

EXERCISES 
43 
(d) This is a conservative field. What is its potential? 
(e) Sketch the equipotentials. 
11. The continuity equation for a fluid is 
JP 
at 
L
-
 
V . J + - = O ,  
where J = pv. 
(a) Show V - V = 0 if the fluid is incompressible (constant density). 
(b) Apply Gauss’s Theorem to the continuity equation and interpret the result. 
- -  
12. Prove these integral forms of the differential operators: 
13. Maxwell’s equations in vacuum are 
Manipulate these using subscript/summation notation to obtain the wave equation 
in vacuum: 

3 
CURVILINEAR COORDINATE 
SYSTEMS 
Up to this point, our discussions of vector, differential, and integral operations have 
been limited to Cartesian coordinate systems. While conceptually simple, these sys- 
tems often fail to utilize the natural symmetry of certain problems. Consider the 
electric field vector created by a point charge q, located at the origin of a Cartesian 
system. Using Cartesian basis vectors, this field is 
(3.1) 
In contrast, a spherical system, described by the coordinates (r, 8, +), fully exploits 
the symmetry of this field and simplifies Equation 3.1 to 
The spherical system belongs to the class of curvilinear coordinate systems. Basis 
vectors of a curvilinear system are orthonormal, just like those of Cartesian systems, 
but their directions can be functions of position. 
This chapter generalizes the concepts of the previous chapters to include curvi- 
linear coordinate systems. The two most common systems, spherical and cylindrical, 
are described first, in order to provide a framework for the more abstract discussion 
of generalized curvilinear coordinates that follows. 
3.1 THE POSITION VECTOR 
The position vector F(P) associated with a point P describes the offset of P from the 
origin of the coordinate system. It has a magnitude equal to the distance from the 
origin to P, and a direction that points from the origin to P. 
44 

THE CYLINDRICAL SYSTEM 
45 
Figure 3.1 The Position Vector 
It seems natural to draw the position vector between the origin and P ,  as shown 
in Figure 3.l(a). While this is fine for Cartesian coordinate systems, it can lead 
to difficulties in curvilinear systems. The problems arise because of the position 
dependence of the curvilinear basis vectors. When we draw any vector, we must 
always be careful to specify where it is located. If we did not do this, it would not 
be clear how to expand the vector in terms of the basis vectors. To get around this 
difficulty, both the vector and the basis vector should be drawn emanating from the 
point in question. The curvilinear vector components are then easily obtained by 
projecting the vector onto the basis vectors at that point. Consequently, to determine 
the components of the position vector, it is better to draw it, as well as the basis 
vectors, emanating from P .  This is shown in Figure 3.l(b). There are situations, 
however, when it is better to draw the position vector emanating from the origin. For 
example, line integrals, such as the one shown in Figure 2.2, are best described in this 
way, because then the tip of the position vector follows the path of the integration. 
We will place the position vector as shown in Figure 3.l(a) or (b), depending upon 
which is most appropriate for the given situation. 
In Cartesian coordinates, the expression for the position vector is intuitive and 
simple: 
The components (rl , rz, r3) are easily identified as the Cartesian coordinates 
(xl,xz,xj). Formally, r1 is obtained by dot multiplying the position vector f by 
the basis vector 
: 
While this may seem overly pedantic here, this technique can be used to find the 
vector components for any vector in any orthogonal coordinate system. 
3.2 THE CYLINDRICAL SYSTEM 
The coordinates of a point P described in a cylindrical system are (p, 4 , ~ ) .  
The 
equations 

46 
CURVILINEAR COORDINATE SYSTEMS 
x = pcos4 
y = psin+ 
(3.5) 
z = z  
and the corresponding inverse equations 
p = d
w
 
4 = tan-' ( Y h )  
(3.6) 
z = z  
govern the relationship between cylindrrcal coordinates and the coordinates of a 
superimposed Cartesian system, as shown in Figure 3.2(a). 
The unit basis vectors for the cylindrical system are shown in Figure 3.2(b). Each 
basis vector points in the direction that P moves when the corresponding coordinate 
is increased. For example, the direction of SP is found by watching how P moves as 
p is increased. This method can be used to determine the directions of basis vectors 
for any set of coordinates. Unlike the Cartesian system, the cylindrical basis vectors 
are not fixed. As the point P moves, the directions of i$ and i?+ both change. Also 
notice that if P lies exactly on the z-axis, i.e., p = 0, the directions of Sp and 6, are 
undefined. 
The cylindrical coordinates, taken in the order (p, 4, z), form a right-handed sys- 
tem. If you align your right hand along GP, and then curl your fingers to point in the 
direction of i?+, your thumb will point in the GZ direction. The basis vectors are also 
orthononnal since 
6, . 6, = 6, . Q = Q . e+ = 0 
e, . *p = e, . c, = c, . Q = 1. 
The position vector expressed in cylindrical coordinates is 
r" Ap 
Y 
X 
z 
6, 
X 
(4 
(b) 
Figure 3.2 
The Cylindrical System 
(3.7) 
(3.8) 

THE CYLINDRICAL SYSTEM 
47 
Y 
r 
$e@ 
6P 
Y 
eP 
X 
Figure 3.3 The Position Vector in a Cylindrical System 
Notice that C+ is always perpendicular to F, as shown in Figure 3.3, so Equation 3.8 
reduces to 
- 
r = rpCp + rtQ. 
(3.9) 
The two-dimensional version of the cylindrical system, with only the (p, 4) coor- 
dinates, is called a polar system. This system, shown in Figure 3.4(a), has basis vectors 
C, and C+. The position vector, shown in Figure 3.4(b), has only a p-component and 
is expressed as 
r; = $,. 
(3.10) 
Remember an arbitrary vector v, unlike the position vector, can have both p- and 4- 
components, as shown in Figure 3.5. 
(b) 
Figure 3.4 
The Polar System 
Y 
Figure 
v@ dV 
eP 
VP 
X 
P 
~~ 
3.5 Polar Components of a Vector 

48 
CURVILINEAR COORDINATE SYSTEMS 
3.3 THE SPHERICAL SYSTEM 
The three coordinates (r, 8,Cp) describe a point in a spherical coordinate system. 
Their relationship to a set of Cartesian coordinates is shown in Figure 3.6(a). The 
equations 
x = rsin8cos4 
y = rsin0sinCp 
(3.1 1) 
z = r cos 8 
and the inverses 
r = Jx2 + y2 + 22 
X 
Cp = cos-' ( 
) 
V G - T p  
(3.12) 
allow conversion between the coordinates of the two systems. 
The unit basis vectors for the spherical system are shown in Figure 3.6(b). As with 
cylindrical coordinates, we obtain the direction of each basis vector by increasing 
the associated coordinate and watching how P moves. Notice how the basis vectors 
change with the position of the point P. If P lies on the z-axis the directions for 20 
and 64 are not defined. If P lies at the origin er is also not defined. 
The spherical system, with the coordinates in the order (r, 0, Cp), is right-handed, 
just as in both the Cartesian and cylindrical systems. It is also an orthonormal system 
because 
X 
X 
(a) 
(b) 
Figure 3.6 The Spherical System 

GENERAL CURVILINEAR SYSTEMS 
49 
X 
Figure 3 
1 
Y 
The Position Vector in -2herical Coordinates 
The position vector, shown in Figure 3.7, is expressed in the spherical system as 
(3.14) 
- 
r = (F . C,)$ 
+ (F - &)CO + (F . @+)@+. 
Because F is always perpendicular to 20 and $4, Equation 3.14 simplifies to 
(3.15) 
- 
r = rC,. 
3.4 GENERAL CURVILINEAR SYSTEMS 
Although the most common, cylindrical and spherical coordinate systems are just 
two examples of the larger family of curvilinear systems. A system is classified as 
curvilinear if it has orthonormal, but not necessarily constant, basis vectors. Other 
more esoteric curvilinear systems include the toroidal, hyperbolic, and elliptical 
systems. Instead of individually working out the vector operations of the previous 
chapter for each of these systems, we present a general approach that can tackle any 
curvilinear geometry. 
3.4.1 
The coordinates (41, q 2 ,  q 3 )  and corresponding basis vectors ($1, q2, q 3 )  will be used 
to represent any generic curvilinear system, as shown in Figure 3.8. Because these 
Coordinates, Basis Vectors, and Scale Factors 
6 
Y 
X 
Figure 3.8 Curvilinear Coordinates and Basis Vectors 

50 
CURVILINEAR COORDINATE SYSTEMS 
basis vectors are functions of position, we should always be careful to draw them 
emanating from a particular point, as we mentioned earlier in this chapter. 
In both the cylindrical and spherical coordinate systems, a set of equations existed 
which related these coordinates to a “standard” set of Cartesian coordinates. For the 
general case, we write these equations as 
xi = xi(q19q27q3) 
(3.16) 
qi = qi(xl,x2,x3), 
(3.17) 
where the subscript notation has crept in to keep things concise. In both these equa- 
tions, the subscript i takes on the values (1,2,3). The variables xi always represent 
Cartesian coordinates, while the qi are general curvilinear coordinates. 
An expression for qi, the unit basis vector associated with the coordinate qi. can 
be constructed by increasing qi. watching how the position vector changes, and then 
normalizing: 
(3.18) 
where hi = ldF/dqiI. This equation is a little confusing, because there actually is 
no sum over the i index on the RHS, even though it appears twice. This is subtly 
implied by the notation, because there is an i subscript on the LHS. The hi, which 
are sometimes called scale factors, force the basis vectors to have unit length. They 
can be written in terms of the curvilinear coordinates. To see this, write the position 
vector in terms of its Cartesian components, which in turn are written as functions of 
the curvilinear coordinates: 
(3.19) 
Therefore, 
and 
hi = 121 = d
m
.
 
(3.21) 
The physical interpretation of the scale factors is quite simple. For a change dql 
of the coordinate 41, the position vector changes by a distance of ldql hl I. Therefore, 
using Equation 3.18, the displacement vector can be written in the curvilinear system 
as 
(3.22) 

GENERAL CURVILINEAR SYSTEMS 
51 
where there now is a sum over the index i on the RHS because there is no subscript on 
the LHS. Since the hi factors can change with position, a differential volume element 
in a curvilinear system is not necessarily a square cube as it is in Cartesian systems. 
Instead, as discussed in the next section, the sides of the differential volume in a 
curvilinear system vary in length and can pick up curvature. 
3.4.2 Differential Geometry 
Figure 3.9 depicts a differential surface enclosing an infinitesimal volume in a curvi- 
linear system. This figure will be the basis for the derivation, in general curvilinear 
coordinates, of the divergence and curl, as well as surface and volume integrals. 
The volume is formed by choosing a starting point (q,, q 2 ,  q3) and then constructing 
seven other vertices by moving away from this point with small changes of coordinates 
dql, dq2, or dq3. In the differential limit, the lengths along each edge of this “warped 
cube” are given by the appropriate dqi times its scale factor. The scale factor is 
evaluated at a set of coordinates that corresponds to their initial value on the edge. If 
the coordinate qi stays at qi all along an edge, it is set equal to qi. If the coordinate qi 
is equal to qi + dqi all along an edge, it is set equal to qr + dqi. If the coordinate qi 
Figure 3.9 Differential Volume of a Curvilinear Coordinate System 

52 
CURVILINEAR COORDINATE SYSTEMS 
goes from qi to qi + dqi on an edge, it is set equal to qi. This is a somewhat cavalier 
way to treat the position dependence of these factors, although it does give all the 
correct results for our derivations. A more rigorous approach, which evaluates the 
mean value of the scale factors on each edge, is presented in Appendix B in a detailed 
derivation of the curvilinear curl. 
Following this approach, the differential element’s volume is simply 
(3.23) 
where the hi’s are all evaluated at the point (ql, qz, q3). The differential surface of the 
bottom shaded side is 
d*baom = - d q l d q M ~ q 3 1  
, 
(3.24) 
where the minus sign occurs because the surface normal is antiparallel to q 3 .  In 
contrast, the differential surface of the top shaded side is 
(41.42.43) 
datop = dqldq2hlh243 
(3.25) 
(41.42143 + 4 3 )  
The minus sign is absent because now the surface normal is parallel to q 3 .  In th~s 
case, hl, hz, and the basis vector q 3  are evaluated at the point (ql, q2, q 3  + dq3). 
3.4.3 
The Displacement Vector 
The displacement vector di plays a central role in the mathematics of curvilinear 
systems. Once the form of dF is known, equations for most of the vector operations 
can be easily determined. From multivariable, differential calculus, di can be written 
(3.26) 
AS we showed in Equation 3.22, this can be written using the scale factors as 
dI; = dq.h. 
1 
1 % .  
A .  
(3.27) 
In a Cartesian system qi = xi, q i  = Ci, and hi = 1, so Equation 3.27 becomes the 
familiar 
dF = d&. 
(3.28) 
In cylindrical coordinates, hl = h, = 1, hz = h+ = p ,  and h3 = h, = 1 so 
(3.29) 

GENERAL CURVILINEAR SYSTEMS 
53 
3.4.4 Vector Products 
Because curvilinear systems are orthonormal, we have 
q. 
1 . q. 
J = 6.. 
'I' 
(3.30) 
This means that the dot product of two vectors, 
Cartesian system: 
and B, has the same form as in a 
Here Ai and B; are the curvilinear components of the vectors, which can be obtained 
by taking axis parallel projections of the vectors onto the basis vectors: 
- 
A .  
1
-
 = A .  4. 
I '  
(3.32) 
With proper ordering, we can always arrange our three curvilinear coordinates to 
be right-handed. Thus, the form of the cross product is also the same as in a Cartesian 
system. The cross product of 
and B expressed using the Levi-Civita symbol is 
A X B = Aiqi X B,Q, = AiBiqkeijk. 
(3.33) 
3.4.5 The Line Integral 
Using the expression for the displacement vector in Equation 3.27, line integrals in 
curvilinear systems are straightforward: 
(3.34) 
There is a sum over both i and j on the RHS of this equation. Because the curvilinear 
basis vectors are orthonormal, this line integral becomes 
(3.35) 
3.4.6 
The Surface Integral 
Curvilinear surface integrals are a bit more complicated, because the orientations of 
the surfaces must be considered. Recalling Figure 3.9, and Equations 3.24 and 3.25, 
the surface integral of a vector v is 
i d s .  V = J,' 5dqldq2hlh2V3 +- dq~dq3h2h3V1 +- dqldq3hlh3V2, 
(3.36) 
where each plus or minus sign must be chosen depending on the sign of d a .  q i .  

54 
CURVILINEAR COORDINATE SYSTEMS 
3.4.7 The Volume Integral 
The geometry of Figure 3.9 can be used to derive the form of volume integrals in 
curvilinear systems. The volume of the element, in the infinitesimal limit, is simply 
dT = dqldqzdq3hlhzh3. Therefore the integral of a function p(F) over any volume V 
is expressed as 
(3.37) 
3.4.8 The Gradient 
In Chapter 2, we showed how the gradient of a scalar function @ is defined such that 
d@ = v@ 
*dT. 
(3.38) 
Substitution of Equation 3.27 ford? yields 
d@ = V@ * dqjhjaj. 
Differential calculus implies d@ = (d@/dqi)dqi, so 
The only way Equation 3.40 can be satisfied is if 
(3.39) 
(3.40) 
(3.41) 
The brackets are necessary in Equation 3.41 to keep the d/dqi operator from acting 
on the basis vectors, because in general dqi/dqj f 0. 
3.4.9 The Divergence 
The divergence operation in a curvilinear system is more complicated than the gra- 
dient, and must be obtained from its integral definition 
- _  
JsdXF*K 
V * A =  lim 
s,v-+o 
JVdr ' 
(3.42) 
where S is the closed surface that encloses the vanishingly small volume V. 
Equation 3.42 for this volume in the infinitesimal iimit is straightforward: 
Once again, consider the differential volume of Figure 3.9. The denominator of 
l d r  = dqldqdq3hlhzh3. 
(3.43) 

GENERAL CURVILINEAR SYSTEMS 
55 
To evaluate the numerator, integration over all six surfaces of V must be performed. 
First, consider the two shaded sides of Figure 3.9, with normals aligned either parallel 
or antiparallel to q 3 .  The integral over the “bottom” surface is 
The minus sign arises because on this surface diF and $3 are antiparallel. Also notice 
that A3, hl , and h2 are all functions of the curvilinear coordinates and are evaluated 
at (41, q2, q3), the initial values of the coordinates on this surface. The integral over 
the “top” surface is 
In this case there is no minus sign because this surface normal is oriented parallel 
to $3. The initial value of the q3 coordinate for this surface has changed by an amount 
dq3 as compared to the bottom surface and thus A3, hl, and h2 must all be evaluated 
at the point (ql, q2, q3 + dq3). In the differential limit 
so the sum of Equations 3.44 and 3.45 is 
(3.47) 
Combining this result with similar integrations over the remaining four surfaces gives 
3.4.10 
The Curl 
The curl operation for a curvilinear coordinate system can also be derived from its 
integral definition: 
v X A .  lim 
d a  = lim 
dT;. A, 
s-0 
c-0 
(3.50) 
where C is a closed path surrounding the surface S, and the direction of d 5  is defined 
via C and a right-hand convention. 

56 
CURVILINEAR COORDINATE SYSTEMS 
1 
Figure 3.10 Orientation of Surface for Curvilinear Curl Integration 
A single component of the curl can be picked out by orienting d F  along the 
direction of a basis vector. Consider Figure 3.10, where d o  is oriented to pick out the 
q 1  component. In this case, d 3  = h2dq2h3dq3q1, so the left side of Equation 3.50 in 
the differential limit becomes 
The line integral on the right side of Equation 3.50 naturally divides into four parts 
along C,, C,, C,, and C,, as shown in Figure 3.11. The complete integral is then 
given by 
2 
1 
Figure 3.11 Differential Geometry for Curvilinear Curl Integrations 

GENERAL CURVILINEAR SYSTEMS 
57 
hi& 
h2$2 
h343 
Q X A = -  
a/aql 
a/dq2 d3s3 
hlA1 
h2A2 
h3A3 
- 
In the differential limit, the integral along C, evaluates to 
lo 
dq2h2A2 = (h2A2) 1 
dq2> 
(3.53) 
(41 ,42.43) 
where we evaluate both A2 and h2 at the point (q1,42, q3). Likewise, the contribution 
along C, is 
, 
der 
dq2, 
(3.54) 
(41~42.43 
+ 4 3 )  
where now we evaluate A2 and h2 at (41, 
q2,43 + dq3). In the differential limit, 
+ ?!!!?!dl 
dq3, 
(3.55) 
(41 4 2 . 4 3  + 4 3 )  
(9lr92.43) 
dq3 
(41.42.43) 
which allows the integrals along C, and C, to be combined to give 
(3.56) 
d T ; f  A = - d(h2A2) 
-1 
&2dq3. 
s 
C" + c c  
dq3 
(41r42r43) 
Similar integrations can be performed along Cb and Cd . The combination of all four 
pieces yields 
(3.57) 
Substituting Equations 3.57 and 3.51 into Equation 3.50 gives the 1-component of 
the curl of A: 
(3.58) 
The other components of v X A can be obtained by reorientating the surface shown 
in Figure 3.10. The results are 
(3.59) 
(3.60) 
(3.61) 

58 
CURVILINEAR COORDINATE SYSTEMS 
or using the Levi-Civita symbol and subscript notation, 
(3.62) 
3.5 THE GRADIENT, DIVERGENCE, AND CURL IN CYLINDRICAL 
AND SPHERICAL SYSTEMS 
3.5.1 Cylindrical Operations 
In the cylindrical system h, = h, = 1, h2 = hg = p, and h3 = h, = I. The gradient, 
divergence, and curl become 
3.5.2 The Spherical Operations 
(3.63) 
(3.64) 
(3.65) 
In the spherical system hl = h, = I, h2 = he = r ,  and h3 = h+ = r sin 0. The 
gradient, divergence, and curl become 
(3.66) 
- 
dQt 
1 dQt 
1 
dQ, 
VQ, = -4, + --qe 
+ ~- 
dr 
r 30 
r sin 0 a+ ‘4 
1 
dA+ 
+-- 
(3.67) 
- _  
I d(r2A,) 
I 
d(sin0Ae) 
V . A = - -  
+- 
r sin 0 J+ 
r2 
dr 
r sin 0 
d0 
(3.68) 

EXERCISES 
59 
EXERCISES FOR CHAPTER 3 
1. A vector expressed in a Cartesian system has the following form: 
- 
v = Cx + C, + CZ. 
(a) If this vector exists at the Cartesian point (1,2, l), what are its components 
(b) If this vector exists at the Cartesian point (1,2, l), what are its components 
(c) If this vector exists at the Cartesian point (0, 0, l), what are its components 
(d) If this vector exists at the Cartesian point (0, 0, l), what are its components 
2. In a cylindrical system, with coordinates (p, 4, z), the vector Vl exists at the 
point (1 , 0,O) and is given by 
= C, + 6,. A second vector '5, exists in the 
same coordinate system at the point (117r/2, 0) and is given by v2 = CP + 64. 
What is the value of V, . V,? 
expressed in a cylindrical system? 
expressed in a spherical system? 
expressed in a cylindrical system? 
expressed in a spherical system? 
3. A circular disk, centered at the origin of a coordinate system, is rotating at a 
constant angular velocity 0,. 
(a) In Cartesian coordinates, what is the velocity vector of the point P located 
(b) In polar coordinates, what is the velocity vector of the point P located at the 
at the Cartesian coordinates (1, l)? 
Cartesian coordinates (1, l)? 
4. Perform the integral 
where 
- 
F = 2 sin 0 sin 4 6, + 3 cos 0 cos 4 C+ + 4 cos 0 cos 4 60, 

60 
CURVILINEAR COORDINATE SYSTEMS 
and S is one-eighth of a spherical shell of radius 2, centered at the origin of 
the spherical system. The integration region lies in the positive xyz-octant of a 
Cartesian system, as shown below: 
2 
5. Perform the line integral 
dl; . G o ,  
where 60 is a spherical basis vector, and C is the quarter circle of unit radius lying 
in one quadrant of the Cartesian xz-plane shown below. Evaluate this integral in 
three ways, using Cartesian, cylindrical, and spherical coordinate systems. What 
happens to the value of the integral if the direction along C is reversed? 
Z 
1 
6. Perform the following line integrals along one quarter of a unit radius circle, 
traversed in a counterclockwise direction. The center of the circle is at the origin 
of a Cartesian coordinate system, and the quarter circle of interest is in the first 
xy -quadrant. 
(a) J, dF * v, where v = 2x ex. 
(b) Jc dI; a, where 
(c) Repeat parts (a) and (b) using a polar coordinate system. 
= 2x. 
7. Determine V . E, where E is the position vector 
- 
r = x GX + y gY + z GZ 
and show that V . I; = 3. Repeat this calculation using spherical coordinates. 

EXERCISES 
61 
8. Derive the hi’s for the cylindrical and spherical systems. Also derive the expres- 
sions given in the chapter for the gradient, divergence, and curl operators in these 
systems. 
9. Redo Exercise 8 of Chapter 2 using spherical coordinates. 
10. A Tokamak fusion device has a geometry that takes the shape of a doughnut 
or torus. Calculations for such a device are sometimes done with the toroidal 
coordinates shown in the figure below. 
1 - 
Ma.jor Axis 
/’ 
/ 
Minor Axis ,/>‘ .__- 
The “major axis” of the device is a vertical, straight line that forms the toroidal 
axis of symmetry. The “minor axis” is a circle of fixed radius R, that passes 
through the center of the doughnut. The position of a point is described by the 
coordinates (r, 8,+). The coordinates r and 8 are similar to a two-dimensional 
polar system, aligned perpendicular to the minor axis. The coordinate + measures 
the angular position along the minor axis. 
(a) Make a sketch of the unit basis vectors for this toroidal system. Are these 
(b) Obtain expressions relating the toroidal coordinates to a set of Cartesian 
(c) Obtain the hi scale factors for the toroidal system. 
(d) Write expressions for the displacement vector dF, a differential surface area 
(e) Write expressions for the gradient v@, 
divergence V * A, and curl v X 
(f) Laplace’s equation written in vector notation is V2@ = 0. What is Laplace’s 
vectors orthogonal? Do they form a right-handed system? 
coordinates. 
dV, 
and a differential volume d7 in this system. 
operations in this system. 
equation expressed in these toroidal coordinates? 
- _  

62 
CURVILINEAR COORDINATE SYSTEMS 
11. A second toroidal system using coordinates (5.7, 
cp) can be formed, with these 
coordinates related to the Cartesian coordinates by: 
a sinh q cos cp 
cash q - cos 5 
x =  
a sinh r) sin cp 
= coshq - cost 
(a) Describe the surfaces of constant and constant 17. 
(b) Pick a point and sketch the basis vectors. Is this a right-handed system? 
(c) Obtain the hi scale factors for this toroidal system. 
(d) Express the position and displacement vectors in this system. 
coordinates by the equations: 
12. The (u, u, z) coordinates of an elliptical system are related to a set of Cartesian 
x = acoshucosu 
y = asinhusinu 
z = z. 
(a) Sketch the lines of constant u and constant u on a two-dimensional xy-grid. 
(b) Obtain the hi scale factors associated with this elliptical system. 
(c) Obtain the form of a differential path length, as well as the area and volume 
elements used to form line, surface, and volume integrals in this elliptical 
system. 
(d) Obtain expressions for the gradient, divergence, and curl in the elliptical 
system. 
(e) Express the position and displacement vectors in the elliptical system. 
13. A hyperbolic (u, u, z) coordinate system is sometimes used in electrostatics and 
hydrodynamics. These coordinates are related to the "standard" Cartesian coor- 
dinates by the following equations: 
2xy = u 
x2 - y 2  = u 
z = z. 
(a) Sketch the lines of constant u and constant u in the xy-plane. Note that these 
(b) Indicate the directions of the basis vectors qu and qv in all four quadrants. 
lines become surfaces in three dimensions. 
Is this system orthogonal? 

EXERCISES 
63 
(c) This (u, ZI, z) system is left-handed. What can be done to make it right- 
(d) In the right-handed version of this system, obtain the hi scale factors. 
(e) Express the position and displacement vectors in this system. 
handed? 
14. An orthogonal (u, u, z) coordinate system is defined by the following set of 
equations that relate these coordinates to a "standard" Cartesian set: 
X 
u=- 
x2 + y2 
x= + y2 
-Y 
Z I = -  
z = z. 
(a) Sketch the lines of constant u and constant u in the xy-plane. 
(b) Pick a point in the xy-plane and sketch the qu, qu, and qz basis vectors. It 
is not necessary to solve for them in terms of the Cartesian basis vectors C,, 
eY, and C,. 
(c) Is this (u, u, z) system right-handed? 
(d) Invert the above expressions and solve for x and y in terms of u and u. 
(e) Express the position and displacement vectors in this system. 
(f) Express the gradient operation v@(u, u, z) in this system. 
15. A particle is moving in three-dimensional space. Describe its position F(t), ve- 
locity f, and acceleration in terms of the the spherical coordinates and basis 
vectors. To work this problem it will be necessary to take time derivatives of the 
basis vectors, because they will change as the particle moves in space. 
16. A particle is moving in a circular orbit with its position vector given by 
- 
r = r, cos(w,t) C, + r, sin(w,t) Cy, 
where r, and w, are constants. If and ? are the first and second derivatives of F, 
(a) Sketch the orbit in the xy-plane. 
(b) Evaluate F . k and discuss your result. 
(c) Show that f + w$ = 0. 
dinates ( p ,  4, z) such that @ = p cos 4. 
(a) Plot the surfaces of constant @ in the xy-plane. 
(b) Let the vectorv = v@, 
and find the radial V, and azimuthal V,+ components 
17. Consider a scalar function @ of position which depends on the cylindrical coor- 
of v. 

64 
CURVILINEAR COORDINATE SYSTEMS 
(c) The field lines for v can be described by a function p = ~(4). 
Show that 
p ( 4 )  satisfies the equation 
dP(4) - PV, 
d4 
v4' 
(d) Solve the above equation for ~ ( 4 ) .  
and plot the v field lines on the same 
18. A flat disk rotates about an axis normal to its plane and passing through its center. 
graph you plotted the surfaces of constant @ . 
Show that the velocity vector V of any point on the disk satisfies the equations 
v*Ti=o 
V X T i = 2 W ,  
- 
- 
where W is the angular velocity vector of the disk. This vector is defined as 
19. Consider a sphere of radius r,, rotating at a constant angular rate w, about its 
z-axis so that 
(a) Find an expression for the velocity vector V of points on the surface of the 
sphere using spherical coordinates and spherical basis vectors. Remember 
v = w x i .  
- 
(b) Perform the integration 
around the equator of the sphere. 
(c) Now perform the surface integral 
j d W X V  
over the entire surface of the sphere. 
20. The magnetic field inside an infinitely long solenoid is uniform 
B = BOG,. 
Determine the vector potential h such that v X h = B. For this situation, the 
magnetic field can also be derived from a scalar potential, B = --Fa. Why is 
this the case and what is @? 

EXERCISES 
65 
The magnetic field inside a straight wire, aligned with the z-axis and carrying 
a uniformly distributed current, can be expressed using cylindrical coordinates 
as 
where B, and po are constants. In this case the magnetic field can still be obtained 
from a vector potential v X A = g. Find this A. Now, however, it is no longer 
possible to find a scalar potential such that 
21. A static magnetic field is related to the current density by one of Maxwell’s 
equations, 
= -v@. 
Why is this the case? 
- 
v x B = poJ. 
(a) If the magnetic field for p < po is given, in cylindrical coordinates, by 
how is the current density 1 
distributed? 
shell at p = po? 
(b) If B = 0 for p > p,, how much current must be flowing in a cylindrical 
22. Classically, the angular momentum 
is given by 
- 
L = F X P ,  
where F is the position vector of an object and 
letting a vector differential operator Top act on a wave function q :  
is its linear momentum. 
In quantum mechanics, the value of the angular momentum is obtained by 
Angular Momentum = Top 9. 
The angular momentum operator can be obtained using the correspondence prin- 
ciple of quantum mechanics. This principle says that the operators are obtained 
by replacing classical objects in the classical equations with operators. Therefore, 
- 
.cop = RP 
x Fop. 
Rep = F. 
The position operator is just the position vector 
- 
The momentum operator is 
- 
pop = -iiiV, 
where i is the square root of - 1 and fi is Planck’s constant divided by 27-r. Show 
that, in Cartesian geometry, 

66 
CURVILINEAR COORDINATE SYSTEMS 
- 
d
d
 
dY 
Lop = -"[yz 
- z-]& 
-in z- 
- x -  
ey 
[ ax 
dz 
-in x- - y- 
e,. 
[ ; 
:XIA 
23. The derivation of the curvilinear divergence in this chapter is somewhat sloppy, 
because it treats the scale factors as constants along each surface of the volume. 
Derive the same expression for the divergence following the more rigorous 
method demonstrated in Appendix B for the curvilinear curl. 

INTRODUCTION TO TENSORS 
Tensors pervade all the branches of physics. In mechanics, the moment of inertia 
tensor is used to describe the rotation of rigid bodies, and the stress-strain tensor 
describes the deformation of elastic bodies. In electromagnetism, the conductivity 
tensor extends Ohm’s law to handle current flow in nonisotropic media, and Maxwell’s 
stress tensor is the most elegant way to deal with electromagnetic forces. The metric 
tensor of relativistic mechanics describes the strange geometry of space and time. 
This chapter presents an introduction to tensors and their manipulations, first 
strictly in Cartesian coordinates and then in generalized curvilinear coordinates. We 
will limit the material in this chapter to orthonormal coordinate systems. Tensors 
in nonorthonormal systems are discussed later, in Chapter 14. At the end of the 
chapter, we introduce the so-called ‘‘pseudo’’-objects, which arise when we consider 
transformations between right- and left-handed coordinate systems. 
4.1 THE CONDUCTIVITY TENSOR AND OHM’S LAW 
The need for tensors can easily be demonstrated by considering Ohm’s law. In an 
ideal resistor, Ohm’s law relates the current to the voltage in the linear expression 
V 
I = -  
R ’  
(4.1) 
In this equation, Z is the current through the resistor, and V is the voltage applied 
across it. Using MKS units, Z is measured in amperes, V in volts, and R in ohms. 
67 

68 
INTRODUCTION TO TENSORS 
Equation 4.1 describes the current flow through a discrete element. To apply Ohm’s 
law to a distributed medium, such as a crystalline solid, an alternative form of this 
equation is used: 
5 = UE. 
(4.2) 
Here 5 is the current density, E is the electric field, and u is the material’s conductivity. 
In MKS units, 5 is measured in amperes per meter squared, E in volts per meter, and 
u in inverse ohm-meters. 
Equation 4.2 describes a very simple physical relationship between the current 
density and the electric field, because the conductivity has been expressed as a scalar. 
With a scalar conductivity, the amount of current flow is governed solely by the 
magnitudes of (+ and E, while the direction of the flow is always parallel to E. But in 
some materials, this is not always the case. Many crystalline solids allow current to 
flow more easily in one direction than another. These nonisotropic materials must have 
different conductivities in different directions. In addition, these crystals can even 
experience current flow perpendicular to an applied electric field. Clearly Equation 
4.2, with a scalar conductivity, will not handle these situations. 
One solution is to construct an array of conductivity elements and express Ohm’s 
law using matrix notation as 
u 1 1  
u 1 2  
(+13 
[ 
= [ ;:: 
Uz2 Uz3] [ 
. 
(4.3) 
u 3 2  
a 3 3  
In Equation 4.3, the current density and electric field vectors are represented by 
column matrices and the conductivity is now a square matrix. This equation can be 
written in more compact matrix notation as 
or in subscriptlsummation notation as 
All these expressions produce the desired result. Any linear relationship between 
3 and E can be described. The 1-component of the current density is related to the 
1 -component of the electric field via u 1  1, while the 2-component of the current density 
is related to the 2-component of the electric field through u22. Perpendicular flow is 
described by the off-diagonal elements. For example, the u 1 2  element describes flow 
in the 1-direction due to an applied field in the 2-direction. 
The matrix representation for nonisotropic conductivity does, however, have a 
fundamental problem. The elements of the matrix obviously must depend on our 
choice of coordinate system. Just as with the components of a vector, if we reorient 
our coordinate system, the specific values in the matrix array must change. The matrix 
array itself, unfortunately, carries no identification of the coordinate system used. The 
way we solved this problem for vector quantities was to incorporate the basis vectors 

THE CONDUCTIVITY TENSOR AND OHM’S LAW 
69 
directly into the notation. That same approach can be used to improve the notation for 
the nonisotropic conductivity. We define a new object, called the conductivity tensor, 
which we write as ??. This tensor includes both the elements of the conductivity 
matrix array and the basis vectors of the coordinate system in which these elements 
are valid. Because this notation is motivated by the similar notation we have been 
using for vectors, we begin with a quick review. 
Remember that a vector quantity, such as the electric field, can be represented by 
a column matrix: 
The vector and matrix quantities are not equal, however, because the matrix cannot 
replace E in a vector equation and vice versa. Instead, the basis vectors of the 
coordinate system in which the vector is expressed must be included to form an 
equivalent expression: 
- 
E = Eii?;. 
(4.7) 
The nonisotropic conductivity tensor 
can be treated in a similar way. It can be 
represented by the matrix array 
but the matrix array and the tensor are not equivalent because the matrix array contains 
no information about the coordinate system. Following the pattern used for vectors 
and the expression for a vector given in Equation 4.7, we express the conductivity 
tensor as 
(4.9) 
The discussion that follows will show that this is a very powerful notation. It supports 
all the algebraic manipulations that the matrix array notation does and also handles 
the transformations between coordinate systems with ease. 
The expression for the conductivity tensor on the RHS of Equation 4.9 contains the 
elements of the conductivity matrix array and two basis vectors from the coordinate 
system in which the elements are valid. There is no operation between these basis 
vectors. They serve as “bins” into which the aij elements are placed. There is a 
double sum over the subscripts i and j and so, for a three-dimensional system, there 
will be 9 terms in this sum, each containing two of the basis vectors. In other words, 
we can expand the conductivity as 

70 
INTRODUCTION TO TENSORS 
This is analogous to how a vector can be expanded in terms of its basis vectors as 
- 
v = X V i &  = V1& + v,e, + v3g. 
(4.1 1) 
I 
Let’s see how this new notation handles Ohm’s law. Using the conductivity tensor, 
we can write it in coordinate-independent “vectorltensor” notation as 
- _ -  
J=ZF-E. 
(4.12) 
Notice the dot product between the conductivity tensor and the electric field vector 
on the RHS of this expression. We can write this out in subscript/summation notation 
as 
By convention, the dot product in Equation 4.13 operates between the second basis 
vector of 
and the single basis vector of E. We can manipulate Equation 4.13 as 
follows: 
(4.14) 
(4.15) 
(4.16) 
The quantities on the left- and right-hand sides of Equation 4.16 are vectors. The 
ith components of these vectors can be obtained by dot multiplying both sides of 
Equation 4.16 by 4 to give 
which is identical to Equations 4.3-4.5. Keep in mind that there is a difference 
between . and E - ??, The order of the terms matters because, in general, 
$j& * 
# 61 * ej&. 
(4.18) 
The basis vectors in this tensor notation serve several functions: 
1. They establish bins to separate the tensor components. 
2. They couple the components to a coordinate system. 
3. They set up the formalism for tensor algebra operations. 
4. As shown later in the chapter, they also simpllfy the formalism for transforma- 
tions between coordinate systems. 
Now that we have motivated our investigation of tensors with a specific example, 
we proceed to look at some of their more formal properties. 

71 
TRANSFORMATIONS BETWEEN COORDINATE SYSTEMS 
4.2 GENERAL TENSOR NOTATION AND TERMINOLOGY 
The conductivity tensor is a specific example of a tensor that uses two basis vectors 
and whose elements have two subscripts. In general, a tensor can have any number 
of subscripts, but the number of subscripts must always be equal to the number of 
basis vectors. So in general, 
(4.19) 
The number of basis vectors determines the rank of the tensor. Notice how the tensor 
notation is actually a generalization of the vector notation used in previous chapters. 
Vectors are simply tensors of rank one. Scalars can be considered tensors of rank 
zero. Keep in mind that the rank of the tensor and the dimension of the coordinate 
system are different quantities. The rank of the tensor identifies the number of basis 
vectors on the right-hand side of Equation 4.19, while the dimension of the coordinate 
system determines the number of different values a particular subscript can take. For 
a three-dimensional system, the subscripts (i, j ,  k, etc.) can each take on the values 
(1,233). 
This notation introduces the possibility of a new operation between vectors called 
the dyadic product. This product is either written as K:E or just A B. The dyadic 
product between vectors creates a second-rank tensor, 
(4.20) 
This type of operation can be extended to combine any two tensors of arbitrary rank. 
The result is a tensor with rank equal to the sum of the ranks of the two tensors in 
the product. Sometimes this operation is referred to as an outer product, as opposed 
to the dot product, which is often called an inner product. 
4.3 TRANSFORMATIONS BETWEEN COORDINATE SYSTEMS 
The new tensor notation of Equation 4.19 makes it easy to transform tensors between 
different coordinate systems. In fact, many texts formally define a tensor as “an object 
that transforms as a tensor.” While this appears to be a meaningless statement, as will 
be shown in this section, it is right to the point. 
In this chapter, only transformations between orthonormal systems are consid- 
ered. First, transformations between Cartesian systems are examined and then the 
results are generalized to curvilinear systems. The complications of transformations 
in nonorthonormal systems are deferred until Chapter 14. 
4.3.1 Vector Transformations Between Cartesian Systems 
We begin by looking at vector component transformations between two simple, two- 
dimensional Cartesian systems. A primed system is rotated by an angle @, 
with 

12 
INTRODUCTION TO TENSORS 
Figure 4.1 Rotated Systems 
respect to an unprimed system, as shown in Figure 4.1. A vector v can be expressed 
with unprimed or primed components as 
From the geometry of Figure 4.2, it can be seen that the vector components in the 
primed system are related to the vector components in the unprimed system by the 
set of equations: 
V: = cos 0,V, + sin OoV, 
v; = - sineov, + cos eov,. 
These equations can be written in matrix notation as 
rv‘1 = ral[Vl, 
(4.22) 
(4.23) 
where [V’] and [VJ are column matrices representing the vector v with primed and 
unprimed components, and [a] is the square matrix: 
(4.24) 
Figure 4.2 Vector Components 

TRANSFORMATIONS BETWEEN COORDINATE SYSTEMS 
73 
4.3.2 The Transformation Matrix 
In general, any linear coordinate transformation of a vector can be written, using 
subscript notation, as 
where [a] is called the transformation matrix. In the discussion that follows, two 
simple approaches for determining the elements of [a] are presented. The first assumes 
two systems with known basis vectors. The second assumes knowledge of only the 
coordinate equations relating the two systems. The choice between methods is simply 
convenience. While Cartesian coordinate systems are assumed in the derivations, 
these methods easily generalize to all coordinate transformations. 
Determining [a] from the Basis Vectors If the basis vectors of both coordinate 
systems are known, it is quite simple to determine the components of [a]. Consider 
a vector 
expressed with components in two different Cartesian systems, 
Substitute the expression for V: given in Equation 4.25 into Equation 4.26 to obtain 
v k 6 k  = aijvjs,!. 
(4.27) 
= km, one of the unprimed basis 
This must be true for any v. In particular, let 
vectors (in other words, Vk+-m = 0 and Vk=m = l), to obtain 
&, 
= a. 
rm &!. 
1 
(4.28) 
Dot multiplication of 2: on both sides yields 
Unm = (6; 
* C,). 
(4.29) 
Notice the elements of [a] are just the directional cosines between all the pairs of 
basis vectors in the primed and unprimed systems. 
Determining [a] from the Coordinate Eqclations If the basis vectors are not 
explicitly known, the coordinate equations relating the two systems provide the 
quickest method for determining the transformation matrix. Begin by considering the 
expressions for the displacement vector in the two systems. Because both the primed 
and unprimed systems are Cartesian, 
dF = dX.6. = d x,e,, 
' A '  
(4.30) 
where the dxl and dxi are the total differentials of the coordinates. Because Equation 
4.25 holds for the components of any vector, including the displacement vector, 
dx: = aijdx,. 
(4.3 1 )  

74 
INTRODUCTION TO TENSORS 
Equation 4.3 1 provides a general method for obtaining the elements of the matrix 
[a] using the equations relating the primed and unprimed coordinates. Working in 
three dimensions, assume these equations are 
or more compactly, 
xi' = x;(xI, x2. x3). 
Expanding the total differentials of Equations 4.32 gives 
Again, using subscript notation, this is written more succinctly as 
dxf = 
dxj. 
dX,'(Xl, x2. x3) 
an j 
(4.32) 
(4.33) 
(4.34) 
Comparison of Equation 4.3 1 and Equation 4.34 identifies the elements of [a] as 
(4.35) 
The OrthonormalProperty of [a] If the original and the primed coordinate systems 
are both orthonormal, a useful relationship exists among the elements of [a]. It is 
easily derived by dot multiplying both sides of Equation 4.28 by &: 
(4.36) 
Equation 4.36 written in matrix form is 
= ill, 
(4.37) 
where [a]' is the standard notation for the transpose of [a], 
and the matrix [l] is a 
square matrix, with 1's on the diagonal and 0's on the off-diagonal elements. 

TRANSFORMATIONS BETWEEN COORDINATE SYSTEMS 
75 
The Inverse of [a] The matrix [a] generates primed vector components from 
unprimed components, as indicated in Equation 4.25. This expression can be inverted 
with the inverse of [a], which is written as [a]-' and defined by 
or in subscript notation 
aI'ajk = a..aT' = 6. 
'J 
lJ 
J k  
l k .  
The subscript notation handles the inversion easily: 
V! = a..V. 
I 
1J 
J 
a,'V! = a-'a..V. 
I 
ki 
'J 
J 
- 
aki'V; = 6kjvj 
(4.39) 
(4.40) 
Transformation matrices which obey the orthonormality condition are simple to 
invert. Comparison of Equation 4.37 and Equation 4.38 shows that 
[a]-' = [a]', 
(4.41) 
or in subscript notation, 
al<' = aji. 
(4.42) 
The inverse relation then becomes 
v. = a J l  ..v! 
I' 
(4.43) 
Busis Vector Transformations The unprimed basis vectors were related to the 
primed basis vectors by Equation 4.28: 
(4.44) 
@. = 
1 
J l  j '  
Using the fact that the inverse of the [a] matrix is its transpose, this expression can 
be inverted to give the primed basis vectors in terms of the unprimed ones 
@! J = a..@. 
'J 
1 .  
(4.45) 
Remember that these expressions are only valid for transformations if both the primed 
and unprimed systems are orthonormal. 
4.3.3 Coordinate Transformation Summary 
The box below summarizes the transformation equations between two Cartesian 
coordinate systems: 

76 
INTRODUCTION TO TENSORS 
The functions x! = X ~ ( X ~ , X ~ , X ~ )  
relate the primed Cartesian coordinates to the 
unprimed Cartesian coordinates. To help keep things straight, notice there is a pattern 
to these transformation equations. Every time we convert from the unprimed system 
to the primed system, whether we are dealing with a basis vector or the components 
of some vector, we sum over the second subscript of ai,. In contrast, conversions 
from the primed system to the unprimed system always sum over thefirst subscript. 
4.3.4 
Tensor Transformations 
To understand why the elements of a tensor must change values when expressed 
in different coordinate systems, consider the conductivity tensor. If in one set of 
coordinates, current flows more easily in the 1-direction than the 2-direction, then 
u11 > a22. Observation of the same physical situation in a new, primed coordinate 
system where the 1'-direction is equivalent to the original 2-direction and the 2'- 
direction is the same as the original 1-direction, would show that ui1 < ui2. Clearly 
the elements of the conductivity tensor must take on different values in the two 
systems, even though they are describing the same physical situation. This is also 
true of a vector quantity; the same velocity vector will have different components in 
different coordinate systems. 
Tensor transformations follow the same pattern as vector transformations. A vector 
expressed in a primed and unprimed system is still the same vector: 
Likewise, using the notation of Equation 4.19, the expressions for a second-rank 
tensor in the two systems must obey 
Herein lies the beauty of this notation. The relationship between the elements, Ti, 
and T,$ is built into Equation 4.47 and is easily obtained by applying two successive 
dot products to both sides. The first dot product yields 
(4.48) 

TRANSFORMATIONS BETWEEN COORDINATE SYSTEMS 
77 
Applying a second dot product in the same manner yields 
Tlm = T:sariasm. 
(4.49) 
To invert Equation 4.49 use the inverted matrix [a]-' twice and remember, for 
orthonormal coordinate systems, a[;' = aji. This gives 
Ti; = Trsalra,. 
(4.50) 
In general, tensor transformations require one ai, factor for each subscript in 
the tensor. In other words, an rth rank tensor needs r different aij factors. If the 
transformation goes from the unprimed to the primed system, all the aij's are summed 
over their second subscript. For the inverse transformation, going from the primed to 
the unprimed system, the sums are over the first subscript. Tensor transformations, 
for arbitrary rank tensors, can be summarized as follows: 
where the elements of the mamx [a] are given by Equation 4.35. 
There is another important feature in the tensor notation of Equation 4.19. Unlike 
in a matrix equation where all terms must be in the same basis, the tensor/vector 
notation allows equations to have mixed bases. Imagine the elements of Ohm's law 
expressed in both a primed and unprimed coordinate system: 
(4.5 1) 
Ohm's law reads 
- _ -  
J = ? F . E ,  
(4.52) 
and any combination of the representations in Equations 4.51 can be used in the 
evaluation. For example: 
J1ef = (Ujkei@L) ' (El&[) = UjkEI$I(@L ' &) = U:kE@;Ukl. 
(4.53) 
The fact that elements of 
from the primed frame are combined with components of 
E from the unprimed frame presents no problem. The dot product of the basis vectors 
takes care of the mixed representations, as long as the order of the basis vectors is 
preserved. This is accomplished in Equation 4.53 by the fact that 2: . 21 # &. This 
type of an operation could not be performed using a matrix representation without 
explicitly converting everything to the same basis. 
The value of expressing a tensor in the form of 4.19 should now be clear. In addition 
to handling the same algebraic manipulations as a matrix array, it also contains all 

78 
INTRODUCTION TO TENSORS 
the information necessary to transform the elements from one coordinate system to 
another. Thus a tensor is truly a coordinate-independent, geometric object, just as a 
vector is. 
4.4 TENSOR DIAGONALEATION 
In physics and engineering problems we often want to diagonalize a tensor. What 
this means is we want to find a particular coordinate system in which the matrix 
array representation of a tensor has nonzero elements only along its diagonal. A rigid 
body will experience no vibration when rotated around any of the three axes of a 
coordinate system in which the moment of inertia tensor is diagonalized. The process 
of balancing a wheel of an automobile makes use of this fact. Small, asymmetrically 
placed weights are added to the rim until one of these special axes lies along the axle. 
Many students get lost in the mathematical process of diagonalization and forget 
that it is actually a transformation of coordinates. In this section, we derive the 
elements of the transformation matrix [a] 
that diagonalizes a given tensor. We start off 
with a completely theoretical treatment of the subject. Then two numerical examples, 
one nondegenerate and one degenerate, are worked out in detail. 
4.4.1 Diagonalization and the Eigenvalue Problem 
Based on the discussion of the previous section, a tensor 
system must be equivalent to the same tensor written in a primed system: 
written in an unprimed 
(4.54) 
We are interested in a very special primed system, a system where all the off-diagonal 
elements of S are zero. In this case, Equation 4.54 becomes 
(4.55) 
Both the tensor elements and the basis vectors of the unprimed system are presumed 
to be known. The problem is to find uis, the elements of the tensor in the primed 
system, and &;, 
the primed basis vectors, such that Equation 4.55 is satisfied. To do 
this, form the dot product of Equation 4.55 with the fist primed basis vector el as 
follows: 
(4.56) 
Equation 4.56 reveals an important property of the basis vectors of the system where 
the tensor is diagonal. They do not change direction when dot multiplied by the tensor. 
They can, however, change in magnitude. If we define hl = mi1, Equation 4.56 

TENSOR DIAGONALIZATION 
19 
becomes 
- 
- 
u . 6; = Al@{. 
(4.57) 
The A1 factor is called an eigenvalue of ?F. An eigenvalue results when an operation 
on an object produces a constant, the eigenvalue, times the original object. The primed 
basis vector is called an eigenvector. 
Now, we introduce the special unit tensor I, 
which is defined as 
- 
- 
1 = a,.@.@. 
‘I 1 J 
(4.58) 
so that 
- - -  
1 . V  =v. 
(4.59) 
- 
Represented as a matrix, T is simply 
- 
- 
1 + [ 1 ] =  
0 1 0 . 
(4.60) 
[: :] 
Using the 7 tensor, Equation 4.57 can be rearranged to read 
(2 - A l i )  
* 6; = 0. 
(4.61) 
in the unprimed system, Equation 4.61 can be written in subscript 
Expressing 
notation as 
Equation 4.29 and some rearrangement yields 
@i(cr,j - A1Gij)alj = 0, 
(4.63) 
where the al are three of the unknown elements of the transformation matrix relating 
the original coordinate systems to the one in which 
The LHS of Equation 4.63 is a vector, and for it to be zero, each of its components 
must be zero. Each component involves a sum over the index j. Equation 4.63 
therefore becomes three equations which can be written in matrix array notation as 
is diagonal. 
u13 
~
2
2
 
- AI 
c 2 3  ] [I!:] = [a] . 
(4.64) 
g 3 2  
u 3 3  -A1 
In order for a set of linear, homogeneous equations such as those in Equation 4.64 
to have a solution, the determinant of the coefficients must be zero: 

80 
INTRODUCTION TO TENSORS 
g 1 1  - A1 
u 1 2  
a 2 1  
~
2
2
 
- A1 
u13 
g 2 3  I 
= 0. 
(4.65) 
1 
a 3 1  
a 3 2  
u 3 3  - A1 det 
This results in a third-order equation for A1 which will generate three eigenvalues. 
Select one of these values, it does not matter which one because the other two will be 
used later, and call it A]. Inserting this value into Equation 4.64 allows a solution for 
a l l ,  a12, and a 1 3  to within an arbitrary constant. These are three of the elements of 
the transformation matrix between the primed and unprimed systems that we seek. 
These three elements also allow the determination of the 6; basis vector to within an 
arbitrary constant: 
e; = alje,. 
A 
(4.66) 
Requiring @[ to be a unit vector determines the arbitrary constant associated with all, 
a12 and ~ 1 3 :  
( a d 2  + (a12I2 + (d2 
= 1. 
(4.67) 
Except for an overall arbitrary sign and the degenerate situation discussed below, we 
have now uniquely determined 6;. 
The other primed basis vectors and elements of the transformation matrix are 
obtained in a similar way. The second primed basis vector is determined by forming 
the dot product in Equation 4.56 using 6;. A matrix equation equivalent to 4.64 
is written with A2, a21, az2, and ~ 2 3 .  The resulting determinant equation for A2 is 
identical to the one for Al, in Equation 4.65. One of the two remaining eigenvalues 
of this equation is selected for A2 and used to determine a21, a22, a23, and $4. In a 
similar way, the last eigenvalue of Equation 4.65 is used for A3 to determine ~ 3 1 ,  
~ 3 2 ,  
is diagonal, is defined by the basis 
vectors 6;, 64, and $:. The elements of ??in this primed system are just the eigenvalues 
determined from Equation 4.65, 
~ 3 3 ,  
and $4. 
The primed coordinate system, in which 
0 
0 A3 
(4.68) 
The matrices of interest in physics and engineering are typically Hermitian. If 
we allow the possibility for complex matrix elements, a matrix is Hermitian if it is 
equal to its complex conjugate transpose. That is, %j = eF. There are two important 
properties of Hermitian matrices. First, their eigenvalues are always pure real num- 
bers. Second, their eigenvectors are always orthogonal. Proofs of these statements 
are left as exercises at the end of this chapter. 
The only complication that can arise in the above diagonalization process is a 
degenerate situation that occurs when two or more of the eigenvalues are identical. 
Consider the case when Al # A2 = A3. The unique eigenvalue Al determines all, 

TENSOR DIAGONALIZATION 
81 
a12, a13 and the eigenvector e;, just as before. The degenerate eigenvalues, however, 
will not uniquely specify their eigenvectors. These eigenvectors may be chosen an 
infinite number of ways. An example with this type of degeneracy is discussed in one 
of the examples that follows. 
~~ 
~ 
~~ 
~ 
~ 
Example 4.1 
tivity tensor expressed in Cartesian coordinates: 
As an example of the diagonalization process, consider the conduc- 
Let this tensor have the matrix representation (ignoring units) 
10 
0 
1 
10 
(4.70) 
This matrix is Hermitian, so we can expect to find pure real eigenvalues and orthog- 
onal eigenvectors. The eigenvalues for the diagonalization are generated from the 
determinant equation 
10-A 
0 
0 
10-A ; I 
= O .  
(4.7 1) 
1 
10 - h der 
Expansion of the determinant gives the third-order polynomial equation 
(10 - A) [(lo - h)2 - 11 = 0, 
(4.72) 
which has three distinct roots: Al = 9, A2 = 11, and A3 = 10. 
to obtain 
The elements of al are determined by inserting the value of A1 into Equation 4.64 
(4.73) 
This equation requires aI2 = -a13 and all = 0. Thenormalization condition imposes 
the additional constraint that ( ~ 1 2 ) ~  
+ ( ~ 1 3 ) ~  
= 1 and results in 
a13 
The first eigenvector associated with the primed system becomes 
(4.74) 

82 
INTRODUCTION TO TENSORS 
The other components of [a] can be determined by performing similar calculations 
to give hz and h3. The complete transformation matrix is 
[a] = - [: : q. 
&
&
o
o
 
(4.76) 
The other two primed eigenvectors are 
6; = (1/&)6* 
+ (1/&)6, 
(4.77) 
and 
$4 = 6,. 
(4.78) 
It should be noted that there is an ordering ambiguity associated with the eigenvalues, 
as well as a sign ambiguity associated with each eigenvector. These ambiguities allow 
us to always set up the primed system to be right-handed. The ordering and sign 
choices made in this example give the primed basis vectors shown in Figure 4.3. 
The conductivity tensor elements expressed in the new, diagonal system are 
[d] 
= 0 11 0 . 
[: : :] 
(4.79) 
Example 4.2 
This example demonstrates the diagonalization process when two of 
the eigenvalues are degenerate. Again consider a conductivity tensor with Cartesian 
elements (ignoring units) 
-1 
0 
[a] = [ ;1 
'd pol. 
(4.80) 
Figure 4.3 
The Primed System Basis Vectors 

TENSOR DIAGONALIZATION 
83 
This is a Hermitian matrix and so we expect pure real eigenvalues and orthogonal 
eigenvectors. The determinant condition is 
-1 
11-A 
1 
=0, 
(4.81) 
1 1 - A  
-1 
0 
10 - A det 
which leads to the third-order polynomial equation 
(10 - A) [(ll - A)2 - 11 . 
(4.82) 
This third-order equation has three roots, but only two are distinct, A1 = 12 and 
A2 = A3 = 10. The A, root can be treated as before. When substituted into Equation 
4.64, the matrix relation becomes 
(4.83) 
This, when coupled with the normalization condition, gives 
(4.84) 
These elements of the transformation matrix define the first eigenvector 
c; = (l/JZ)Cl 
- (1/&)@2. 
(4.85) 
Now consider the degenerate eigenvalue. When A2 = 10 is substituted into Equa- 
tion 4.64 we obtain 
Substitution of A3 gives almost the same equation: 
(4.86) 
(4.87) 
Equation 4.86 requires a21 = u22, but gives no constraint on ~ 2 3 .  
The normalization 
condition forces the condition uil + ui2 + u:3 = 1. These conditions can be satisfied 
by many different eigenvectors. Since a23 is arbitrary, set it equal to zero. Now if the 
second eigenvector is to be orthogonal to Ci, we have 
(4.88) 

a4 
INTRODUCTION TO TENSORS 
This gives for the second eigenvector 
6.: = (1/&)2, + (1/&2. 
(4.89) 
The eigenvector associated with A3 is given by Equation 4.87 and has all the same 
constraints as the A2 eigenvector, namely, a31 = (132 and a 3 3  is arbitrary. If we want 
the eigenvectors to be orthogonal, however, 6: must be perpendicular to C/, and $4. 
The basis vectors 2; and 
are both in the original 12-plane and so, if C: is to be 
perpendicular to these two primed basis vectors, it must be along the 3-direction. 
This gives 
(4.90) 
and for the third eigenvector 
2; = 23. 
(4.91) 
A quick check will show that these three eigenvectors are orthonormal and define a 
right-handed coordinate system in which the elements of the conductivity tensor are 
diagonalized. 
4.5 TENSOR TRANSFORMATIONS IN CURVILINEAR 
COORDINATE SYSTEMS 
The transformations of the previous sections easily generalize to curvilinear coordi- 
nate systems. First consider the intermediate problem of a transformation between a 
Cartesian and a curvilinear system. 
Let the Cartesian system have primed coordinates (xi, x;, x;) and basis vectors 
(Ci, Ci, Ci), while the unprimed, curvilinear system has coordinates (q1,qz. q 3 ) ,  basis 
vectors (ql,q2,$3), and scale factors (hl,h2,h3). The set of equations relating the 
coordinates of the two systems can be written as 
= x:(q1, 929 q 3 ) .  
For example, the standard cylindrical system would use the equations 
x/ = 
- X I  = pcos8 
x; G z' = 2. 
xi = y' = psin6 
(4.92) 
(4.93) 

TENSOR TRANSFORMATIONS IN CURVILINEAR COORDINATE SYSTEMS 
85 
The transformation matrix [a] performs the same function as before. That is, it takes 
the unprimed, curvilinear components of a vector and generates the primed, Cartesian 
components: 
V! 
I = a . . ~ .  
' J  
J' 
(4.94) 
Recall from the previous chapter that the displacement vector for the two systems 
can be written 
d r  = 
= h I .d 41%. 
. A  
(4.95) 
The components of the displacement vector in the unprimed curvilinear system are 
given by the h,dq;, while its components in the primed Cartesian system are given 
by the dxl. These components must be related by the transformation matrix [a]. In 
subscript notation 
&! 
1 = a..h.d 
11 I 41. 
. 
(4.96) 
The total differential dx! can be formed from Equations 4.92 and becomes 
(4.97) 
Equation 4.97 can be placed in the form of Equation 4.96 by multiplying the RHS of 
4.97 by hJ/h;: 
Comparing Equations 4.98 and 4.96 gives 
[Curvilinear -+ Cartesian]. 
J x : ( q 1 9  
q29 q3) 
hjaqi 
a,, = 
(4.98) 
(4.99) 
The further generalization for the transformation between two curvilinear systems 
follows in a straightforward way. The elements for the transformation matrix [a] in 
this case become 
(4.100) 
Note there is no sum over i or j on the RHS of Equation 4.100 because both these 
subscripts appear on the LHS of the expression. 
Equation 4.100 is the most general form for the elements of the transformation 
matrix between two curvilinear coordinate systems. It simplifies to Equation 4.99 if 
the primed system is Cartesian since the hf -+. 1. It further degenerates to Equation 
4.35 if both the primed and unprimed systems are Cartesian since the h,' and the 
h; + 1. 

86 
INTRODUCTION TO TENSORS 
A X B =  
As before, the transformation matrix can also be determined from the basis vectors 
of the two coordinate systems. For the general curvilinear case, the elements of [a] 
are 
a.. 
1J = (q; . qj). 
(4.101) 
The above manipulations are fast and easy using subscript notation. It might be a 
useful exercise to go through the same steps using just matrices to convince yourself 
the subscript notation is more efficient. 
e, 
$2 
$3 
A, 
0 
0 
0 
B, 
0 
4.6 PSEUDO-OBJECTS 
If we consider only transformations that involve rigid rotations or translations, there 
is no way to change a right-handed system into a left-handed system, or vice-versa. 
To change handedness requires a reflection. Transformations that involve reflections 
require the introduction of the so-called “pseudo”-objects. Pseudoscalars, pseudo- 
vectors, and pseudotensors are very similar to their “regular” counterparts, except 
for their behavior when reflected. An easy way to demonstrate the distinction is 
by closely examining the cross product of two regular vectors in right-handed and 
left-handed systems. Another way that emphasizes the reflection properties of the 
transformation is the mirror test, which is presented in Appendix E. 
4.6.1 Pseudovectors 
Consider the right-handed Cartesian coordinate system shown in Figure 4.4. Picture 
two regular vectors in this system, oriented along the first two basis vectors: 
(4.102) 
(4.103) 
By “regular,” we mean that the components of these vectors obey Equation 4.25 when 
we transform the coordinates. 
The cross product between 
and 
can be formed using the determinant, 
= A,B,&, 
(4.104) 
iet 
Figure 4.4 
The Right-Handed System 

PSEUDO-OBJECTS 
87 
3 
2 
A x B  t 
Figure 4.5 
Vectors in the Right-Handed System 
or, equivalently, using the Levi-Civita symbol: 
(4.105) 
The resulting vector is shown in Figure 4.5. Notice how the direction of 
X B is given 
by the standard right-hand rule. If you point the fingers of your hand in the direction 
of x and then curl them to point along B, your thumb will point in the direction of the 
cross product. Keep in mind that the cross product is not commutative. If the order 
of the operation is reversed, that is if you form B X A, the result points in exactly the 
opposite direction. 
Now consider the left-handed system shown in Figure 4.6, with coordinates and 
basis vectors marked with primes to distinguish them from the coordinates and basis 
vectors of the right-handed system. This system results from a simple inversion of 
the I-axis of the unprimed system. It can also be looked at as a reflection of the right- 
handed system about its x2x3-plane. The equations relating the primed and unprimed 
coordinates are 
(4.106) 
3' 
Figure 4.6 The Left-Handed System 

88 
INTRODUCTION TO TENSORS 
so that the transformation matrix becomes 
-1 
0 0 
0 
0
1
 
[ a ] =  [ 0 
1 o j  
The regular vectors A and B in the primed coordinate system are simply 
= -A, 6; 
= B, 6;. 
(4.107) 
(4.108) 
(4.109) 
We just wrote these results down because they were obvious. Remember, formally, 
they can be obtained by applying [a] to the components of the unprimed vectors. The 
matrix multiplication gives 
(4.1 10) 
0 
0
1
 
and 
-1 
0 0 
[:I] 
= [ 0 
1 01 [lJ 
= [;,I 
0 
0
1
 
It is important to remember that the vectors are the same physical objects in both 
coordinate systems. They are just being expressed in terms of different components 
and different basis vectors. 
Now form the cross product of A and B in the left-handed system. To do this we 
will use the same determinant relation: 
(4.111) 
6; 
c; 
6; 
A X B =  -A, 
0 
0 
= -A, B,C&, 
(4.112) 
I O 
B, 
O Let 
or in terms of the Levi-Civita symbol: 
A X B = A{ BI 6; Eijk = A; B; 6; €123 = -A, B, 6:. 
(4.113) 
The vectors A and B and the cross product 
X B are shown in Figure 4.7 for the 
left-handed coordinate system. Notice now, the right-hand rule used earlier no longer 
works to find the direction of the cross product. If we define the cross product using 
the determinant in Equation 4.112, then we must use a left-hand rule if we are in a 
left-handed coordinate system. 
There is something peculiar here. Compare Figures 4.7 and 4.5 notice that while 
A and B point in the same directions in the two systems, their cross product does not! 
- 

PSEUDO-OBJECTS 
2' 
89 
I 
Figure 4.7 
Vectors in The Left Handed System 
By changing the handedness of the coordinate system, we have managed to change 
the vector 
X B. 
Let's look at this cross product from another point of view. If the unprimed 
components of the quantity 
X B, given in Equation 4.104, are transformed into the 
primed system using the [a] matrix, as one would do for regular vector components, 
we obtain 
[ 
y ]  [At,,] 
= [ A t , ]  
(4.1 14) 
Combining these components with the appropriate basis vectors gives for the cross 
product vector 
-1 
0 0 
A, B, $4. 
(4.1 15) 
This result disagrees with Equation 4.1 12 by a minus sign. To get around this difficulty, 
a quantity formed by the cross product of two regular vectors is called apsedovector. 
Pseudovectors are also commonly called axial vectors, while regular vectors are 
called polar vectors. If v is a regular vector it transforms according to Equation 4.25. 
However, if v is a pseudovector, its components transform according to 
V: = bidet Viari- 
(4.1 16) 
In this way Equation 4.1 14 becomes 
(A x B); 
-1 
0 0 
(A x B); 
0 
0 1 
A,B, 
- ' 4 3 "  
[ ( A X E 1 : ]  = -  [ 0 
1 .] [ : ] = [ : ] 
giving 
(4.117) 
X B = -A, B, $;, 
(4.1 18) 
in agreement with Equations 4.1 12 and 4.1 13. 

90 
INTRODUCTION TO TENSORS 
To summarize, if v is a regular vector its components transform as 
v: = vi a,i. 
If instead it is a pseudovector, it components transform as 
(4.1 19) 
(4.120) 
If the handedness of two orthonormal coordinate systems is the same, a transformation 
between them will have 
= 1 and vectors and pseudovectors will both transform 
normally. If the systems have opposite handedness, la/& = -1 and vectors will 
transform normally but pseudovectors will flip direction. A vector generated by a 
cross product of two regular vectors is actually a pseudovector. 
It is tempting to think that all this balderdash is somehow a subtle sign error 
embedded in the definition of the cross product. In some cases, this is correct. For 
example, when we define the direction of the magnetic field vector, which turns out 
to be a pseudovector, we have implicitly made an arbitrary choice of handedness 
that must be treated consistently. Another example is the angular momentum vector, 
which is defined using a cross product. While you could argue that the “pseudoness” 
of these two examples is just a problem with their definition, there are cases where 
you cannot simply explain this property away. It is possible to design situations where 
an experiment and its mirror image do not produce results which are simply the mirror 
images of each other. In fact, the Nobel Prize was won by Lee and Yang for analyzing 
these counterintuitive violations of purity conservation. The classic experiment was 
first performed by Wu, who showed this effect with the emission of beta particles 
from Cobalt-60, under the influence of the weak interaction. 
4.6.2 
Pseudoscalars 
The ideas that led us to the concept of pseudovectors apply to scalars as well. A proper 
scalar is invariant to any change of the coordinate system. In contrast, a pseudoscalar 
changes sign if the handedness of the coordinate system changes. A pseudoscalar 
involved in a transformation, governed by the transformation matrix [a], will obey 
(4.121) 
A good example of a pseudoscalar derives from the behavior of the cross product 
operation. The volume of a three-dimensional parallelogram, shown in Figure 4.8, 
can be written as 
Volume = (A x B) . C. 
(4.122) 
In a right-handed system, the vector formed by A X B will point in the upward 
direction. So in a right-handed system, 
(A x B) i2 > 0. 
(4.123) 

PSEUDO-OBJECTS 
91 
In a left-handed system, 
X B points downward, and consequently 
(h x B) . c < 0. 
(4.124) 
Interpreted in this way, the volume of a parallelogram is a pseudoscalar. 
4.6.3 Pseudotensors 
Pseudotensors are defined just as you would expect. Upon transformation, the com- 
ponents of a pseudotensor obey 
which is exactly the same as a regular tensor, except for the laid,, term. 
Again we turn to the cross product to find a good example. Consider two coordinate 
systems. One, the unprimed system, is a right-handed system, and the other, with 
primed coordinates, is left-handed. Using the Levi-Civita symbol in both coordinate 
systems to generate the cross product of A and B gives the relation 
The minus sign occurs because, as we showed earlier, the physical direction of the 
cross product is different in the two coordinate systems. Now the transformation 
properties of regular vectors can be used to find the relationship between Eijk and &. 
Because A, B, and the basis vectors are all regular vectors, they transform according 
to Equation 4.25. Writing the primed components of these vectors in terms of the 
unprimed, Equation 4.126 becomes 
_ -  

92 
INTRODUCTION TO TENSORS 
This expression is true for arbitrary a and B, so we obtain the result 
Eijk = -ariasjat&. 
(4.128) 
Keep in mind this applies only when the two systems have opposite handedness. 
If both systems have the same handedness, the minus sign disappears. Thus for 
the general case of arbitrary transformation between two orthonormal systems, the 
Levi-Civita symbol components obey 
Eijk = blder%%jatk'&t. 
(4.129) 
Consequently, the Levi-Civita symbol is a pseudotensor. 
EXERCISES FOR CHAPTER 4 
1. 
2. 
Determine the transformation matrix [a] that corresponds to a rotation of a two- 
dimensional Cartesian system by 30". Obtain the inverse of this matrix [a]-' and 
evaluate the following operations: 
(a) a;'ajm. 
(b) a i j ' a , j .  
(c) U j ' Q m j .  
Consider the transformation from a standard two-dimensional Cartesian system 
(xl, xz) to a primed system (xi, xi) that results from a reflection about the x1 -axis, 
as shown below: 
1 
i 
I 
.I 
Express the coordinates of a point in the primed system in terms of the 
coordinates of the same point in the unprimed system. 
Determine the elements of the transformation matrix [a] that takes vector 
components from the unprimed system to components in the primed system. 
Determine [a]-' in three ways: 
i. By inverting the [a] matrix found in part (b). 
ii. By inverting the coordinate equations of part (a). 
iii. By simply switching the primed and unprimed labels on the coordinates. 

EXERCISES 
93 
3. 
4. 
5. 
6. 
7. 
Determine the elements of the transformation matrix [a] that generates the Carte- 
sian components of a vector from its components in the toroidal system defined 
in Exercise 10 of Chapter 3. 
The coordinates of a hyperbolic system (u, u, z )  are related to a set of Cartesian 
coordinates (x, y ,  z )  by the equations 
= X L  - Y‘ 
73 = 2xy 
2 = z. 
Determine the elements of the transformation matrix [a] that takes the Cartesian 
components of a vector to the hyperbolic components. Using this transformation 
matrix and the position vector expressed in the Cartesian system, express the 
position vector in the hyperbolic system. 
Consider two curvilinear systems, an unprimed cylindrical system and a primed 
spherical system. 
(a) What are the equations relating the cylindrical coordinates to the spherical 
coordinates? 
(b) Find the elements of the transformation matrix [a] that generates the primed 
vector components from unprimed components. 
Consider a two-dimensional, primed Cartesian system that is shifted an amount x, 
and rotated an amount 0, with respect to a two-dimensional unprimed Cartesian 
system, as shown in the figure below. 
Y’ 
Y 
X’ 
A- 
,
\
 
(a) Identify the equations that generate the xy-coordinates from the x’y ’- 
(b) Determine the elements of the transformation mamx [a] that generate the 
Consider a two-dimensional Cartesian xy-system and a shifted polar p’O’-system, 
as shown in the figure below. The origin of the shifted polar system is located at 
x = r,,y = 0. 
coordinates. 
primed components of a vector from the unprimed components. 

94 
INTRODUCTION TO TENSORS 
(a) In a sketch, pick a point P well off the x-axis and draw the Cartesian and 
shifted polar basis vectors. 
(b) Express the Cartesian coordinates in terms of the shifted polar coordinates. 
Invert these equations and express the shifted polar coordinates in terms of 
the Cartesian coordinates. 
(c) Find the elements of the transformation matrix [a] that relates the shifted 
polar basis vectors to the Cartesian basis vectors. 
(d) Express the displacement vector dF first in the Cartesian system and then in 
the shifted polar system. 
8. A two-dimensional (u, v )  elliptical coordinate system can be related to an (x, y) 
Cartesian system by the coordinate equations 
x = coshucos v 
y = sinhusinv. 
Determine the elements of the transformation matrix [a] that converts vector 
components in the Cartesian system to vector components in this elliptical sys- 
tem. what is [a]-’? 
9. Consider three coordinate systems: (1) a Cartesian system with coordinates 
(XI, x2, x3) and basis vectors (el, 62, 
&3); (2) a curvilinear system with coordinates 
(91, q 2 ,  q 3 )  and basis vectors (81, &, q3); and (3) a second curvilinear system 
with coordinates (qi,q;, 4:) and basis vectors (qi, q:, a:). Let the relationships 
between the Cartesian coordinates and these curvilinear coordinates be given by 
x = xh3q29q3) 
z = z(q1, q27 q 3 )  
x = X’(q;,q:,q:) 
z = Z ’ M ,  s;. 4:). 
Y = Y(ql,q2>q3) 
Y = Y’(4;4;4:) 
(4.130) 
(a) Find the general expressions for the elements of the transformation matrix 
[a] that takes vector components from one curvilinear system to the other. 
(b) Take one of the curvilinear systems to be the cylindrical system, and the 
other to be the elliptical system of Exercise 8 and specifically determine the 
elements of [a] and its inverse. 

EXERCISES 
95 
10. Consider a two-dimensional Cartesian system with coordinates (x -+ XI, y -+ x2) 
and basis vectors (6, 
-+ 61,C, + 62) and a polar system with coordinates 
( p  -+ q1,4 + q2) and basis vectors (6, + &, 64 -+ &). 
(a) Express the Cartesian coordinates (xl, x2) in terms of the polar coordinates 
(b) Express the position vector ii;, first using the Cartesian basis vectors and then 
using the polar basis vectors. 
(c) Determine the elements of the transformation matrix [a] that takes vector 
components from the polar system to vector components in the Cartesian 
system. Show that this transformation matrix works for the expressions for 
the position vector of part (b) above. 
(d) What are the elements of [a]-’, the matrix that generates polar components 
from the Cartesian components? 
(e) Determine the elements of the displacement vector dii; in the Cartesian and 
polar systems. 
(f) A second-rank tensor expressed in the Cartesian system takes the following 
form 
(41, 
q2). 
What are its elements in the polar system? 
11. Consider the transformation from a two-dimensional Cartesian 12-system to a 
Cartesian 1’2’-system, which includes both an inversion and a rotation as shown 
in the drawing below. 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
2 
1’ 
/ , 
,/+t 
1 
\ 
\ \ 
\ 
\\ 2’ 
(a) Express the vector v = 36’ + 222 in the 1’2’-system. 
(b) Express T = ij@i6j in the 1 ’2’-system. Do not forget to sum over the repeated 
- 
i and j subscripts. 

96 
INTRODUCTION TO TENSORS 
12. Consider a two-dimensional conductivity tensor 
whose elements expressed in 
an (x, y) Cartesian system have the matrix representation 
The current density J is related to the electric field E through Ohm’s law: 
(a) If the electric field is given by E = &ex + EoeY, express the current density 
in this Cartesian system. 
(b) Now transform the components of a to a new orthogonal (x’,y’) system, 
where the basis vector i?: is parallel to the E given in part (a) above. The 
electric field in this new system is given as E = &E,,eL:. 
What are the 
elements of the transformation matrix that accomplishes this? What are the 
components of Z in this new system? 
(c) With a and E expressed in the primed system, form the product Ti. E in that 
system and show that it results in the same vector obtained in part (a) above. 
_ -  
13. Show, using matrix array and subscript/summation notation, that in general 
Z . E # E . Z .  
14. Consider the inner product of two second-rank tensors, each expressed in the 
same orthonormal system: 
- -  
- 
A - B .  
In subscripthmmation notation this product involves the term 
- -  
- -  
(a) Develop an expression for A * B using the Kronecker-6 and show that the 
result is also a second-rank tensor. 
(b) Show that A . B does not equal B . A. 
- -  
- -  
- _  
- -  
15. Let r be a second-rank tensor andv be a vector. Expressed in a Cartesian system, 
- 
- 
T == T..@.e. 
‘J 
I I 
v = vici. 
(a) Express the following using subscript/summation notation and, where nec- 
essary, the Levi-Civita symbol 

EXERCISES 
97 
- -  
i. V .  T. 
ii. T . 8. 
iii. (V . T) x V. 
iv. (V . T) x (1. 
v). 
v. (V . T) . (T . V). 
(V . T )  = 0 for any V? 
T) = 0 for any v? 
(b) What values for the elements of r, other than Ti, = 0, will result in V X 
(c) What values for the elements of T, other than Tij = 0, will result in V .  (V. 
- -  
- 
- 
16. Use subscriptlsummation notation to show that the expressions 
( T . C ) X D  
and 
T .  CxD 
= (  1 
are not necessarily equal 
17. An orthogonal (u, v,z) coordinate system is defined by the set of equations 
relating its coordinates to a standard set of Cartesian coordinates, 
X 
(a) Determine the elements of the transformation matrix [a]. 
(b) - 
At the Cartesian point ( 1 , 1 , 1 )  let the vector v = 1 8, + 2 
(c) At the Cartesian point ( 1 , 1 , 1 )  let the tensor 
+ 3 Sz. Express 
V at this point using (u, u, z) system vector components and basis vectors. 
= 1 6183 + 2 &2C1 + 3 6263 + 
46362. Express T at this point using (u, v,z) system tensor elements and 
basis vectors. 
18. Consider a dumbbell positioned in the xy-plane of a Cartesian system, as shown 
in the figure below. The moment of inertia tensor for this object expressed in this 
Cartesian system is 
- 
- - 
1 = I .  .&& 
'1 1 J 
-2 
0 
1 4 4 ,  [;2 ; (I]. 
Find the basis vectors of a coordinate system in which the moment of inertia 
tensor is diagonalized. Draw the dumbbell in that system. 

98 
INTRODUCTION TO TENSORS 
1 
-l / 
Y 
M 
1 
- -1 
- 
19. Let a tensor T = Ti,i$6, in a two-dimensional Cartesian system be represented 
by the matrix array 
(a) Find the eigenvalues and eigenvectors of this matrix array. 
(b) Plot the eigenvectors and show their relation to the Cartesian basis vectors. 
(c) Are the eigenvectors orthogonal? Is the matrix array Hermitian? 
(d) Repeat parts (a)-(c) for the tensor 
20. Let the components of the conductivity tensor in a Cartesian system be repre- 
sented by 
- 1  
0 
- 
iT;-[r]= ; ;I. 
Identify an electric field vector, by specifying its components, that will result in 
a current density that is parallel to this electric field. 
21. Using subscript/summation notation, show that the eigenvalues of a Hermitian 
matrix are pure real. Show that the eigenvectors generated by the diagonalization 
of a Hermitian matrix generate orthogonal eigenvectors. 
Start with a second-rank tensor that is assumed to have off-diagonal elements 
that may be complex, 
- 
- 
T = T .  .$.@. 
-4J I 
J '  

EXERCISES 
99 
Let the eigenvalues of the matrix representation of this tensor be Ai and the 
eigenvectors associated with these eigenvalues be @I = % j C j  so that 
z . j g n j  = An&i. 
Notice that although the n subscript is repeated on the RHS it is not summed over 
because it appears on the LHS, where it is clearly not summed over. Now form 
the quantities &zjsj 
and c&&ju-j 
and subtract one from the other. Apply the 
Hermitian condition and see what it implies. 
22. Is the dot product between two pseudovectors a scalar or a pseudoscalar? Give 
an example from classical mechanics of such an operation. 
23. Show that the elements of the Kronecker delta 6;j transform like elements of a 
tensor, not a pseudotensor. 

THE DIRAC 6-FUNCTION 
The Dirac 6-function is a strange, but useful function which has many applications in 
science, engineering, and mathematics. The &function was proposed in 1930 by Paul 
Dirac in the development of the mathematical formalism of quantum mechanics. He 
required a function whlch was zero everywhere, except at a single point, where it was 
discontinuous and behaved like an infinitely high, infinitely narrow spike of unit area. 
Mathematicians were quick to point out that, strictly speaking, there is no function 
which has these properties. But Dirac supposed there was, and proceeded to use it 
so successfully that a new branch of mathematics was developed in order to justify 
its use. This area of mathematics is called the theory of generalizedfunctions and 
develops, in complete detail, the foundation for the Dirac 6-function. This rigorous 
treatment is necessary to justify the use of these discontinuous functions, but for the 
physicist the simpler physical interpretations are just as important. We will take both 
approaches in this chapter. 
5.1 EXAMPLES OF SINGULAR FUNCTIONS IN PHYSICS 
Physical situations are usually described using equations and operations on contin- 
uous functions. Sometimes, however, it is useful to consider discontinuous ided- 
izations, such as the mass density of a point mass, or the force of an infinitely fast 
mechanical impulse. The functions that describe these ideas are obviously extremely 
discontinuous, because they and all their derivatives must diverge. For this reason 
they are often called singular functions. The Dirac 6-function was developed to de- 
scribe functions that involve these types of discontinuities and provide a method for 
handling them in equations which normally involve only continuous functions. 
100 

EXAMPLES OF SINGULAR FUNCTIONS IN PHYSICS 
I 
101 
Force 
Area =-A( m,v) 
time 
Figure 5.1 Force and Change in Momentum 
5.1.1 The Ideal Impulse 
Often a student's first encounter with the 6-function is the "ideal" impulse. In me- 
chanics, an impulse is a force which acts on an object over a finite period of time. 
Consider the realistic force depicted in Figure 5.l(a). It is zero until t = t1, when it 
increases smoothly from zero to its peak value, and then finally returns back to zero 
at t = t2. When this force is applied to an object of mass m,, the momentum in the 
direction of the applied force changes, as shown in Figure 5.l(b). The momentum 
remains constant until t = 21, when it begins to change continuously until reaching 
its final value at t = tz. The net momentum change A(rn,v) is equal to the integrated 
area of the force curve: 
1; dt F(t) = [ 
dt F(t) 
An ideal impulse produces all of its momentum change instantaneously, at the 
single point t = to, as shown in Figure 5.2(a). Of course this is not very realistic, 
since it requires an infinite force to change the momentum of a finite mass in zero 
time. But it is an acceptable thought experiment, because we might be considering 
the limit in which a physical process occurs faster than any measurement can detect. 
time 
~ Force O0 
, 
Area=A(m,v) 
1, 
Figure 5.2 An Instantaneous Change in Momentum 

102 
THE DIRAC &-FUNCTION 
The force of an ideal impulse cannot be graphed as a function of time in the normal 
sense. The force exists for only a single instant, and so is zero everywhere, except at 
t = to, when it is infinite. But this is not just any infinity. Since the total momentum 
change must be A(mo u), the force must diverge so that its integral area obeys (for all 
t- < t+) 
t- < to < t+ 
otherwise 
[ 
dt F(t) = 
(5.2) 
In other words, any integral which includes the point to gives a momentum change 
of A(mov). On the other hand, integrals which exclude to must give no momentum 
change. We graph this symbolically as shown in Figure 5.2(b). A spike of zero width 
with an arrow indicates the function goes to infinity, while the area of the impulse is 
usually indicated by a comment on the graph, as shown in the figure, or sometimes 
by the height of the arrow. 
The Dirac 8-function 8(t) was designed to represent exactly this kind of "patho- 
logical" function. 8(t) is zero everywhere, except at t = 0, when it is infinite. Again, 
this is not just any infinity. It diverges such that any integral area which includes 
t = 0 has the value of 1. That is (for all t- < t+), 
1 
t - < O < t +  
otherwise 
' 
(5.3) 
The symbolic plot is shown in Figure 5.3. The ideal impulse, discussed above, can 
be expressed in terms of a shrjkd Dirac 8-function: 
(5.4) 
F(t) = A(mov)8(t - to). 
The t - to argument simply translates the spike of the &function so that it occurs at 
to instead of 0. 
5.1.2 Point Masses and Point Charges 
Physical equations often involve the mass per unit volume p&) 
of a region of space. 
Normally p m 0  is a continuous function of position, but with 8-functions, it can also 
represent point masses. A point mass has a finite amount of mass stuffed inside a 
single point of space, so the density must be infinite at that point and zero everywhere 
else. 

TWO DEFINITIONS OF a(?) 
103 
Figure 5.4 A Single Point Mass at the Origin 
Integrating the mass density over a volume V gives the total mass enclosed: 
d7 p,(F) = total mass inside V .  
(5.5) 
Thus, if there is a single point of mass m, located at the origin, as shown in Figure 5.4, 
any volume integral which includes the origin must give the total mass as m,. Integrals 
which exclude the origin must give zero. In mathematical terms: 
(5.6) 
origin included in V 
origin excluded from V . 
l d 7 p m ( F )  = 
mo 
0 { 
Using Dirac &functions, this mass density function becomes 
(5.7) 
Equation 5.6 can easily be checked by expanding the integral as 
Application of Equation 5.3 on the three independent integrals gives m, only when 
V includes the origin. If, instead of the origin, the point mass is located at the point 
(x,, yo, z,), shifted arguments are used in each of the &functions: 
p,(F) 
= m,Wx - x&%y - Y , ) ~ ( z  - GI. 
(5.9) 
Dirac 6-functions can also be used, in a similar way, to represent point charges in 
electromagnetism. 
5.2 TWO DEFINITIONS OF S(t) 
There are two common ways to define the Dirac &function. The more rigorous 
approach, from the theory of generalized functions, defines it by its behavior inside 
integral operations. In fact, the &function is actually never supposed to exist outside 
an integral. In general, scientists and engineers are a bit more lax and use a second 
definition. They often define the &function as the limit of an infinite sequence of 

104 
THE DIRAC &FUNCTION 
-112n 
112n 
Figure 5.5 The Square Sequence Function 
continuous functions. Also, as demonstrated in the two examples of the previous 
section, they frequently manipulate the &function outside of integrals. Usually, there 
are no problems with this less rigorous approach. However, there are some cases, 
such as the oscillatory sequence functions described at the end of this section, where 
the more careful integral approach becomes essential. 
5.2.1 
The &function can be viewed as the limit of a sequence of functions. In other words, 
S(t) as the Limit of a Sequence of Functions 
8(t) = lim&(t), 
(5.10) 
n-m 
where &(t) is finite for all values oft. 
The simplest is the square function sequence defined by 
There are many function sequences that approach the Dirac &function in this way. 
and shown in Figure 5.5. Clearly, for any value of n 
1 : d t  &(t) = 1, 
(5.11) 
(5.12) 
and in the limit as n + m, &(t) = 0 for all f, except t = 0. The first three square 
sequence functions for n = 1,2, and 3 are shown in Figure 5.6. 
- 112 
-114 -116 
116 114 
112 
Figure 5.6 The First Three Square Sequence Functions 

TWO DEFINITIONS OF s(t) 
105 
Figure 5.7 The Resonance Sequence Function 
There are four other common function sequences that approach the Dirac 6- 
function. Three of them, the resonance, Gaussian, and sinc squared sequences, are 
shown in Figures 5.7-5.9 and are mathematically described by 
n/.rr 
Resonance: &(t) = ~ 1 + n2t2 
n 
2 2  
Gaussian: &(t) = -e-n fi 
(5.13) 
sin2 nt 
nn-t2 
Sinc Squared: &(t) = -. 
Each of these functions has unit area for any value of n, and it is easy to calculate 
that in the limit as n + 00, 6,(t) = 0 for all t # 0. 
There is one other common sequence, the sinc function sequence, which ap- 
proaches the &function in a completely different manner. The discussion of this 
requires more rigor, and is deferred until the end of this section. 
Figure 5.8 The Gaussian Sequence Function 

106 
THE DIRAC &FUNCTION 
L.. 
t 
nln 
Figure 5.9 The Sinc Squared Sequence Function 
5.2.2 Defining S(t) by Integral Operations 
In mathematics, the 6-function is defined by how it behaves inside an integral. Any 
function which behaves as 6(t) in the following equation is by dejnition a 6-function, 
t- < to < t+ 
otherwise 
’ 
L-“ 
dt 6(t - to)f(t) = 
(5.14) 
where t- < t+ and f (t) is any continuous, well-behaved function. This operation 
is sometimes called a sifiing integral because it selects the single value f ( t o )  out of 
Because this is a definition, it need not be proven, but its consistency with our 
previous definition and applications of the 6-function must be shown. If t- < to < t+ , 
the range of the integral can be changed to be an infinitesimal region of size 2~ centered 
around to, without changing the value of the integral. This is true because 6(t - to) 
vanishes everywhere except at t = to. This means 
L-‘+ 
dt 6(t - to)f(t) = s,-, dt 60 - to)f(t>- 
(5.15) 
Because f ( t )  is a continuous function, over the infinitesimal region it is effectively a 
constant, with the value f(to). Therefore, 
f (0. 
f0+E 
to+€ 
dt S(t - to) = f(to). 
(5.16) 
This “proves” the first part of Equation 5.14. The second part, when to is not inside 
the range t- < t < t+, easily follows because, in this case, 6(t - to) is zero for the 
entire range of the integrand. 
Figure 5.10 shows a representation of the integration in Equation 5.14. The inte- 
grand is a product of a shifted 6-function and the continuous function f ( t ) .  Because 
the &function is zero everywhere except at t = to, the integrand goes to a &function 
located at t = to, with an area scaled by the value of f(to). 
s,. 
[ 
dt 6 0  - t0)f ( t )  = f(t0) 

TWO DEFINITIONS OF S(f) 
107 
f(t> 
6( t- t o) with unit area 
t0 
Figure 5.10 A Graphical Intcrprctation of the Sifting Integral 
5.2.3 The Sinc Sequence Function 
The sinc function can be used to form another set of sequence functions which, in the 
limit as n -+ m, approach the behavior of a 6-function. The sinc function sequence 
is shown in Figure 5.1 1 and is defined by 
sin nt 
Sinc: 6,(t) = -. 
9l-t 
(5.17) 
Notice, however, that as n ---f m this &(t) does not approach zero for all t # 0, so the 
sinc sequence does not have the characteristic infinitely narrow, infinitely tall peak 
that we have come to expect. How, then, can we claim this sequence approaches a 
&function? 
The answer is that the sinc function approaches the &function behavior from a 
completely different route, which can only be understood in the context of the integral 
definition of the &function. In the limit as n + w, the sinc function oscillates infinitely 
fast except at t = 0. Thus, when the sequence is applied to a continuous function 
f ( t )  in a sifting integral, the only contribution which is not canceled out by rapid 
oscillations comes from the point t = 0. The result is, as you will prove in one of the 
7cin 
Figure 5.11 The Sinc Sequence Function 

108 
THE DIRAC &FUNCTION 
exercises at the end of this chapter, the same as you would expect for any &function: 
This means, in the limit n -+ a, the sinc sequence approaches the &function: 
sin nt 
lim - 
= 6(t). 
n - w  
n-t 
(5.18) 
(5.19) 
5.3 &FUNCTIONS WITH COMPLICATED ARGUMENTS 
So far, we have only considered the Dirac 6-function with either a single, independent 
variable as an argument, e.g., 6(t), or the shifted version 8(t - to). In general, however, 
the argument could be any function of the independent variable (or variables). It turns 
out that such a function can always be rewritten as a sum of simpler &functions. 
Three specific examples of how this is accomplished are presented in this section, 
and then the general case is explored. In each case, the process is the same. The 
complicated 6-function is inserted into an integral, manipulated, and then rewritten 
in terms of 6-functions with simpler arguments. 
5.3.1 s(-t) 
To determine the properties of 8(-t), make it operate on any continuous function 
f ( t )  inside a sifting integral, 
(5.20) 
where t- < t+. The substitution t' = -t transforms this to 
- 1;; dt' 6(t') f (-t') = sJ_:- dt' 8(t')f(-t') 
(5.21) 
where the last step follows from the integral definition of 6(t) in Equation 5.14. 
Therefore. we have 
-t+ < 0 < -t- 
otherwise 
= 
t- < 0 < ti 
otherwise 
j-'+ 
dt s(-t)f(t) = { fo) 
(5.22) 
But notice this is precisely the result obtained if a 8(t) were applied to the same 
function: 

109 
S-FUNCTIONS WITH COMPLICATED ARGUMENTS 
t- < 0 < t+ 
otherwise 
. 
lt 
dt s(t)f(t) = { i") 
(5.23) 
Remember, by definition, anything that behaves like a 6-function inside an integral 
is a &function, so S ( - t )  = i3(t). This implies the &function is an even function. 
5.3.2 S(at) 
Now consider another simple variation 6(at), where a is any positive constant. Again, 
start with a sifting integral in the form 
(5.24) 
With a variable change oft' = at, this becomes 
so that 
t- < 0 < t+ 
otherwise 
l-'+ 
dt f(t>&(at) = { L(0)" 
(5.26) 
Notice again, this is just the definition of the function 6(t) multiplied by the constant 
l/a, so 
W x )  = S(x)/a 
a > 0. 
(5.27) 
This derivation was made assuming a positive a. The same manipulations can be 
performed with a negative a, and combined with the previous result, to obtain the 
more general expression 
6(ax) = 6(n)/lal. 
(5.28) 
Notice that our first example, 6(t) = 8(-t), can be derived from Equation 5.28 by 
setting a = - 1 .  
5.3.3 
S(tZ - 2) 
As a bit more complicated argument for the Dirac &function, consider 6(t2 - aL). 
The argument of this function goes to zero when t = f a  and t = -a, which seems 
to imply two &functions. To test this theory, place the function in the sifting integral 
lr+ 
dt f(t)6(t2 - a2). 
(5.29) 

110 
THE DIRAC 6-FUNCTION 
There can be contributions to this integral only at the zeros of the argument of 
the 6-function. Assuming that the integral range includes both these zeros, i.e., 
t- < --a and t+ > +a, this integral becomes 
dt f(t)6(t2 - a2) + 1 
+a+€ 
dt f(t)6(t2 - a2), 
(5.30) 
fa-€ 
which is valid for any value of 0 < E < a. 
NOW t2 - a2 = (t - a)(t + a), which near the two zeros can be approximated by 
(5.31) 
(t + a)(-2a) 
(t - a ) ( + k )  
t --t -a 
t ---f +a . 
t2 - a2 = (t - a)(t + a) = 
Formally, these results are obtained by performing a Taylor series expansion of t2 - a2 
about both t = -a and t = +a and keeping terms up to the first order. The Taylor 
series expansion of t2 - a2 around an arbitrary point to is, to first order, given by 
to). 
(5.32) 
It=t', 
In the limit as E -+ 0, Integral 5.30 then becomes 
-a+€ 
+a+€ 
la-c 
d t f W ( - W  + 4) + la-, 
dt f ( t P ( k ( t  - a)). 
(5.33) 
Using the result from the previous section, 6(at) = 6(t)/lal, gives 
(5.34) 
1 
1 
2a 
1' dt f(t)6(t2 - a2) = --(-a) 
+ -f(+a). 
2a 
Therefore, S(t2 - a2) is equivalent to the sum of two 6-functions: 
1 
1 
2a 
2a 
6(t2 - a2) = - S ( t  - a) + -6(t + a), 
as shown in Figure 5.12. 
6(x2 - a2) 
00 
03 
area = 1/2a 
i -' 
X 
area = 1/2a 
- 
1,- 
a 
-a 
The Plot of S(x2 - a2) 
~~~~ 
_ _ _ _  
~~ 
_
~
~
_
 
- 
~ 
~~ 
~ 
- 
Figure 5.12 
(5.35) 

INTEGRALS AND DERIVATIVES OF 6(r) 
111 
5.3.4 The General Case of Scf(t)) 
Using the Taylor series approach, the previous example can be easily generalized to 
an arbitrary argument inside a 8-function: 
(5.36) 
where the sum over i is a sum over all the zeros of f ( t )  and ti is the value oft where 
each zero occurs. 
5.4 INTEGRALS AND DERIVATIVES OF S(t) 
Integrating the &-function is straightforward because the &function is defined by its 
behavior inside integrals. But, because the &function is extremely discontinuous, 
you might think that it is impossible to talk about its derivatives. While it is true that 
the derivatives cannot be treated like those of continuous functions, it is possible to 
talk about them inside integral operations or as limits of the derivatives of a sequence 
of functions. These manipulations are sometimes referred to as &function calculus. 
5.4.1 
Consider the function H ( x ) ,  which results from integrating the &function: 
The Heaviside Unit Step Function 
H(x) = 
dt s(t). 
.I_a 
(5.37) 
H ( x )  can be interpreted as the area under 8(t) in the range from t = --m to t = x ,  
as shown in Figure 5.13. If x < 0, the range of integration does not include t = 0, 
and H ( x )  = 0. As soon as x exceeds zero, the integration range includes t = 0, and 
H ( x )  = 1: 
0 
x < 0  
H(x) = [ = d t  6(t) = { 
x > 0 '  
f dt 
6(t> 
t 
~ 
~~ 
X 
Figure 5.13 The Integration for the Heaviside Step Function 
(5.38) 

112 
THE DIRAC &FUNCTION 
Figure 5.14 
The Heaviside Function 
Strictly, the value of H(0) is not well defined, but because S ( t )  is an even function, 
many people define H ( 0 )  = 1/2. A plot of the Heaviside function is shown in 
Figure 5.14. 
Ironically, the function does not get its name because it is heavy on one side but, 
instead, from the British mathematician and physicist, Oliver Heaviside. 
5.4.2 
The Derivative of S(t) 
The derivatives of a(?) are, themselves, very interesting functions with sifting prop- 
erties of their own. To explore these functions, picture the first derivative of S(t) as 
the limit of the derivatives of the Gaussian sequence functions: 
(5.39) 
Figure 5.15 depicts the limiting process. 
The first derivative of 8(t), often written S‘(t), is called a “doublet” because of 
its opposing pair of spikes, which are infinitely high, infinitely narrow, and infinitely 
close together. Like S(t), the doublet is rigorously defined by how it operates on other 
functions inside an integral. The sifting integral of the doublet is 
l-t+ 
dt f (t)a‘(t), 
(5.40) 
d6,( t)/dt 
-
,
 
Figure 5.15 The Derivative of s(t) 

INTEGRALS AND DERIVATIVES OF 6(r) 
113 
where f ( t )  is any continuous, well-behaved function. This integral can be evaluated 
using integration by parts. Recall that integration by parts is performed by identifying 
two functions, u(t) and v(t), and using them in the relation 
ib 
d(u(t))v(t) = ~ ( t ) v ( t ) ( ‘ = ~  
- lb 
d(v(t))u(r). 
(5.41) 
To evaluate Equation 5.40, let v ( t )  = f ( t )  and du = s’(t)dt. Then dv = (df/dt)dt 
and u(t) = 6(t) so that Equation 5.40 becomes 
f = a  
If t- # 0 and t+ f 0, Equation 5.42 reduces to 
(5.43) 
because 6(r) is zero for t # 0. This is now in the form of the standard sifting integral, 
so 
(5.44) 
The doublet is the sifting function for the negative of the derivative. 
This sifting property of the doublet, like the sifting property of S(t), has a graphical 
interpretation. The integrated area under the doublet is zero, so if it is multiplied by a 
constant, the resulting integral is also zero. However, if the doublet is multiplied by 
a function that has different values to the right and left of center (i.e., a function with 
a nonzero derivative at that point), then one side of the integrhd will have more area 
than the other. If the derivative of f ( t )  is positive, the negative area of the doublet is 
scaled more than the positive area and the result of the sifting integration is negative. 
This situation is shown in Figure 5.16. If, on the other hand, the derivative of f ( t )  
f(t) d6,(t)/dt 
/ 
Figure 5.16 The Sifting Property of the Doublet 

114 
THE DIRAC &FUNCTION 
is negative, the positive area of the doublet is enhanced more than the negative area, 
and the sifting integration produces a positive result. 
Similar arguments can be extended to higher derivatives of the &function. The 
appropriate sifting property is arrived at by applying integration by parts a number 
of times, until the integral is placed in the form of Equation 5.44. 
5.5 SINGULAR DENSITY FUNCTIONS 
One of the more common uses of the Dirac 6-function is to describe density functions 
for singular distributions. Typically, a distribution function describes a continuous, 
“per unit volume” quantity such as charge density, mass density, or number density. 
Singular distributions are not continuous, but describe distributions that are confined 
to sheets, lines, or points. The example of the density of a point mass was briefly 
described earlier in this chapter. 
5.5.1 Point Mass Distributions 
The simplest singular mass distribution describes a point of mass m, located at the 
origin of a Cartesian coordinate system, as shown in Figure 5.4. The mass density 
function p,(x, y, z) that describes this distribution must have the units of mass per 
volume and must be zero everywhere, except at the origin. In addition, any volume 
integral of the density which includes the origin must give a total mass of m,, while 
integrals that exclude the origin must give zero mass. The expression 
(5.45) 
satisfies all of these requirements. Clearly it is zero unless x, y ,  and z are zero. 
Therefore integrating pm over a volume that does not include the origin produces 
zero. Integrating over a volume that contains the origin results in 
prn(X3 Y 9 Z) = moS(xP(y)S(z) 
S,dr m06(46(y)6(z) = [ I ‘ d x  [:‘& 
SJ‘dz m & x ) ~ ( ~ ) W  
(5.46) 
Because 
dx 6(x) = 1, 6(x) has the inverse dimensions of its argument, or in this 
case, l/length. Therefore, m0S(x)6(y)6(z) has the proper dimensions of mass per 
unit volume. 
To shift the point mass to a different location than the origin, simply use shifted 
6-functions: 
- 
- m,. 
Pm = mo6(x - -G)S(Y - Y O ) G  - ~ 0 ) .  
(5.47) 
This function has the proper dimensions of mass per unit volume, and pm is zero 
except at the point (xo, yo, zo). The integral over a volume V correctly produces zero 
mass if V does not include the point (x,, yo, G) and m, if it does. 

SINGULAR DENSITY FUNCTIONS 
115 
Consider the very same point, but now try to write the mass density in cylindrical 
coordinates, i.e., p,(p, 4, z). In this system, the coordinates of the mass point are 
(pO, h, z,) where 
Po = 4 x 2  + y,' 
(5.48) 
A natural guess for pm(p, 4,z) might be 
%S(P - P m 4  - 40Pk - 70). 
(5.49) 
This density clearly goes to zero, unless p = po, 4 = 4o and z = zo, as it should. 
However, because 4 is dimensionless, Equation 5.49 has the dimensions of mass per 
unit area, which is not correct. Also, the integral over a volume V becomes 
This is zero if V does not contain the point (po, 
G), but when V does contain the 
point, the result is mopo, not m, as required. This is because the dp integration gives 
lo-€ 
dP PNP - Po) = Po. 
(5.51) 
Therefore, Equation 5.49 is not the proper expression for the mass density in cylin- 
drical coordinates, because it does not have the proper dimensions, and integration 
does not produce the total mass. From the discussion above, it is clear that the correct 
density function is 
P O + €  
This example demonstrates an important point. While the point distributions in 
a Cartesian system can be determined very intuitively, a little more care must be 
used with non-Cartesian coordinates. In generalized curvilinear coordinates, where 
dr = hlh&dqldqzdq3, the expression for a point mass at (qlo, qzO, q30) is 
(5.53) 
There is a common shorthand notation used for three-dimensional singular distri- 
butions. The three-dimensional Dirac &function is defined by 
(5.54) 

116 
THE DIRAC 6-FUNCTION 
In Cartesian coordinates, this is simply 
S3(F - To) = 6(x - x,)S(y - y,)S(z - z,). 
(5.55) 
Using this notation, the mass density of point mass located at ?, is, regardless of the 
coordinate system chosen, given by 
pm(T) = m,63(? - To). 
(5.56) 
If there are several points with mass mi at position Ti, the density function becomes 
a sum of 6-functions: 
pm(T) = Crni63(T - Ti). 
i 
Integrating over a volume V gives 
(5.57) 
(5.58) 
which, when evaluated, is simply the sum of all the masses enclosed in V. 
5.5.2 Sheet Distributions 
Imagine a two-dimensional planar sheet of uniform mass per unit area uo, located in 
the z = z, plane of a Cartesian system, as shown in Figure 5.17. The intuitive guess 
for the mass density pm in Cartesian coordinates is 
P ~ ( ~ > Y , Z )  
= aoa(z - 
~ 0 ) -  
(5.59) 
The &function makes sure all the mass is in the z = z, plane. The dimensions are 
correct, because a, has the dimensions of mass per area, and the &function adds 
another 1 /length, to give pm the dimensions of mass per unit volume. The real check, 
I 
X 
, 
Figure 5.17 Simple Infitute Planar Sheet 

SINGULAR DENSITY FUNCTIONS 
117 
however, is the volume integral. In order for Equation 5.59 to be correct, an integral 
of the surface mass density over some part of the sheet S must give the same total 
mass as a volume integral of pm, over a volume V ,  which encloses S: 
For the case described above, Equation 5.60 expands on both sides to 
(5.60) 
(5.61) 
which is indeed true, because the range of the z integration includes the zero of 
the &function. Thus the assumption of Equation 5.59 was correct. Notice how the 
&function effectively converts the volume integral into a surface integral. 
Unfortunately, things are not always this easy. The previous example was particu- 
larly simple, because the sheet was lying in the plane given by z = zo. Now consider 
the same sheet positioned in the plane y = x, as shown in Figure 5.18. In this case 
the mass is only located where y = x ,  and the intuitive guess for the mass density is 
- x). 
(5.62) 
But our intuition is incorrect in this case. The surface integral on the LHS of Equa- 
tion 5.60 expands to 
d a  crm(T) = 
ds 
dz ao, 
s 
J J  
(5.63) 
where ds is the differential length on the surface in the xy-plane, as shown in Fig- 
ure 5.19. Notice that 
ds = J(dx)* + (dy)2 
= h d x ,  
(5.64) 
Y 
Figure 5.18 A Tilted Planar Mass Sheet 

118 
THE DIRAC &FUNCTION 
Y 
dx 
Figure 5.19 The Differential Length ds Along the Surface x = y in the xy-Plane 
where the last step follows because dx = dy for this problem. Thus Equation 5.63 
becomes 
(5.65) 
If we use the expression in 5.62 for pm. the volume integral on the RHS of Equa- 
tion 5.60 is 
= J d x / d z u o ,  
(5.66) 
where the last step results from performing the integration over y. Comparing Equa- 
tions 5.66 and 5.65, we see that they are off by a factor of fi! 
Where does this discrepancy come from? The problem was our assumption in 
Equation 5.62 to use a 8-function in the form 8(y - x). We could have very well 
chosen [?]8(y - x), where [?I is some function that we need to determine. This still 
makes all the mass lie in the y = x plane. The correct choice of [?I is the one that 
makes both sides of Equation 5.60 equal. For example, when we make our “guess” 
for the distribution function for this example, we write 
Pm(x, y,z> = “?luo6(x - y)- 
(5.67) 
Then, when we evaluate the RHS of Equation 5.60, we get 
(5.68) 
In this case, we already showed that the value of [?I is simply the constant &. In 
more complicated problems, [?] can be a function of the coordinates. This can happen 

SINGULAR DENSITY FUNCTIONS 
119 
if either the mass per unit area of the sheet is not constant, or if the sheet is not flat. 
You will see some examples of this in the next section and in the problems at the end 
of this chapter. 
5.5.3 Line Distributions 
As a final example of singular density functions, consider the mass per unit volume 
of a one-dimensional wire of uniform mass per unit length A,. The wire is bent to 
follow the parabola y = Cx2 in the z = 0 plane, as shown in Figure 5.20. The factor 
C is a constant, which has units of l/length. We will follow the same procedure in 
constructing the mass density for this wire as we did for the previous example. In this 
case, the volume integral of the mass density must collapse to a line integral along 
the wire 
Ld.rp,(r) = 
ds A,(s). 
(5.69) 
h 
In this equation, s is a variable which indicates parametrically where we are on the 
wire, and A,(s) is the mass per unit length of the wire at the position s. 
Because all the mass must lie on the wire, we write the mass density function as 
Here we have used two &functions. The 6(z) term ensures all the mass lies in the z = 0 
plane, while S(y - Cx2) makes the mass lie along the parabola. As before, we include 
an unknown factor of [?I, which we will have to determine using Equation 5.69. 
In terms of Cartesian coordinates, the general expression for the differential arc 
length ds is 
Y 
Z 
Figure 5.20 Parabolic Line Distribution 

120 
THE DIRAC 8-FUNCTION 
Along the wire, z = 0 and y = Cx2, so that 
ds = dl + 4C2x2 dx. 
(5.72) 
Using Equation 5.70 and the fact that dr = dx dy dz, Equation 5.69 becomes 
/ 
dx / 
dy / d z  [?]A,S(y - Cx2)S(z) = 
dx d
w
 
A,. 
(5.73) 
S 
Let’s concentrate on the LHS of this equation. The integral over z is easy, because 
/ d Z S ( Z )  = 1. 
(5.74) 
Also, when we do the integral over y, x is held constant, and only one value of y 
makes the argument of the &function vanish, so we have 
/ d y  s(y - c x 2 )  = 1. 
Therefore, Equation 5.73 becomes 
(5.75) 
J d x  [?]A, = / d x  d
m
 
A,, 
(5.76) 
and the value of [?I clearly must be 
[?] = dl + 4c2x2. 
(5.77) 
Notice in this case that [?] is a function of position. The mass density for the wire is 
given by 
pm(x,y,z) = 41 + 4c2x2 A,S(y - Cx2)S(z). 
(5.78) 
When we converted the volume integral in Equation 5.73 to a line integral, it was 
easier to perform the dy integration before the dx integration. This was because when 
we integrated over y, holding x fixed, the only value of y which made the argument 
of the &function zero was y = Cx2. 
Another way to look at this is that there is a 
one-to-one relationship between dx and ds, as shown in Figure 5.21. 
If instead, we performed the x integration first, holding the value of y fixed, there 
are two values of x which zero the &function argument. In this case, the integral 
becomes 
- 
- & / d x  
[a(. - m) 
4- s ( x  + m)]. 
(5.79) 

THE INFINITESIMAL ELECTRIC DIPOLE 
Y 
121 
Figure 5.21 The Relation Between dx and ds Along the Parabola 
Figure 5.22 The Relation Between dy and ds Along the Parabola 
Looking at Figure 5.22 shows what is going on here. There is not a one-to-one 
relationship between dy and ds. Of course, evaluating the integral in this way produces 
exactly the same result for p,(Q, 
as you will prove in one of the exercises of this 
chapter. 
5.6 THE INFINITESIMAL ELECTRIC DIPOLE 
The example of the infinitesimal electric dipole is one of the more interesting appli- 
cations of the Dirac S-function and makes use of many of its properties. 
5.6.1 Moments of a Charge Distribution 
In electromagnetism, the distribution of charge density in space p,(F) can be ex- 
panded, in what is generally called a multipole expansion, into a sum of its moments. 
These moments are useful for approximating the potential fields associated with com- 
plicated charge distributions in the far field limit (that is, far away from the charges). 
Each moment is generated by calculating a different volume integral of the charge 
distribution over all space. Because our goal is not to derive the mathematics of 
multipole expansions, but rather to demonstrate the use of the Dirac 6-functions, the 
multipole expansion results are stated here without proof. Derivations can be found 

122 
THE DIRAC &FUNCTION 
in most any intermediate or advanced book on electromagnetism, such as Jackson’s 
Classical Electrodynamics. 
The lowest term in the expansion is a scalar called the monopole moment. It is 
just the total charge of the distribution and is determined by calculating the volume 
integral of pc: 
(5.80) 
The next highest moment is a vector quantity called the dipole moment, which is 
generated from the volume integral of the charge density times the position vector: 
(5.81) 
The next moment, referred to as the quadrapole moment, is a tensor quantity generated 
by the integral 
Q = J’ 
dT (3TF - lT12T)pc(T). 
(5.82) 
All space 
- 
In this equation, the quantity T T is a dyad, and T is the identity tensor. There are an 
infinite number of higher-order moments beyond these three, but they are used less 
frequently, usually only in cases where the first three moments are zero. 
Far away from the charges, the electric potential can be approximated by summing 
the contributions from each of the moments. The potential field Q, due to the first few 
moments is 
(5.83) 
It is quite useful to know what charge distributions generate just a single term in 
this expansion, and what potentials and electric fields are associated with them. For 
example, what charge distribgtion has just a dipole term (that is, 
# 0) while all 
other terms are zero (Q = 0, Q = 0, etc.). The Dirac &function turns out to be quite 
useful in describing these particular distributions. 
5.6.2 The Electric Monopole 
The distribution that generates just the Q / r  term in Equation 5.83 is called the electric 
monopole. As you may have suspected, it is simply the distribution of a point charge 
at the origin: 

THE INFINITESIMAL ELECTRIC DIPOLE 
Its monopole moment, 
123 
Q = 
d7q063(~;) 
= qo, 
(5.85) 
is simply equal to the total charge. The dipole, quadrapole, and higher moments of 
this distribution are all zero: 
All space 
P = / 
&q0F 63(F) = 0 
AU space 
- 
- 
Q = 
dr (3F F - r2T ) ti3@) = 0. 
All space 
The electric field of a monopole obeys Coulomb’s law: 
(5.86) 
(5.87) 
(5.88) 
5.6.3 The Electric Dipole 
An electric dipole consists of two equal point charges, of opposite sign, separated by 
some finite distance do, as shown in Figure 5.23. The charge density of this system, 
expressed using Dirac &functions, is 
Pdpl - 
- q0 [s3 (F - +GI) - 63 (F + + G X ) ]  
1 
(5.89) 
where in this case, the dipole is oriented along the x-axis. 
Indeed both Q and Q are zero, while the dipole moment is given by 
You might be tempted to believe that this distribution has only a dipole moment. 
= qodogx. 
The higher-order moments for the dipole distribution, however, do not vanish. 
Y 
Z 
(5.90) 
Figure 5.23 The Electric Dipole 

124 
THE DIRAC &FUNCTION 
The "ideal" dipole refers to a charge dstribution that has only a dipole moment, 
and no other. It is the limiting case of the "physical" dipole, described above. In 
this limit, the distance between the charges becomes vanishingly small, while the net 
dipole moment 
is held constant. In other words, do -+ 0 and qo ---f 60 such that 
doqo = po is held constant. The charge distribution for this situation is 
(5.91) 
Expansion of the 6-functions in terms of Cartesian coordinates gives 
ptacal 
= po6(y)6(z) 
Notice that this limit is just the definition of a derivative: 
This means the charge distribution of an ideal dipole, with a dipole moment of 
magnitude po oriented along the x axis, can be written 
(5.94) 
The electric field from the ideal dipole (and also from a physical dipole in the far 
field limit) is the solution of 
But we already know the solution for the electric monopole is 
(5.95) 
(5.96) 
Operating on both sides of Equation 5.96 with [ -dod/dx], shows the electric field in 
Equation 5.95 obeys 
Taking the derivative gives the result: 
(5.97) 
(5.98) 

RIEMANN INTEGRATION AND THE DIRAC &FUNCTION 
125 
This is the solution for a dipole oriented along the x-axis. In general, the dipole is a 
vector P, which can be oriented in any direction, and Equation 5.98 generalizes to 
(5.99) 
Notice that the magnitude of this field drops off faster with r than the field of 
the monopole. The higher the order of the moment, the faster its field decays with 
distance. 
5.6.4 Fields from Higher-Order Moments 
The technique described above can be used to find the electric field and charge 
distributions for the ideal electric quadrapole, as well as for all the higher moments. 
The charge distributions can be constructed using &functions, and the fields can be 
obtained by taking various derivatives of the monopole field. You can practice this 
technique with the quadrapole moment in an exercise at the end of this chapter. 
5.7 
RIEMANN INTEGRATION AND THE DIRAC &FUNCTION 
The &function provides a useful, conceptual technique for viewing integration which 
will become important when we discuss Green's functions in a later chapter. From 
Equation 5.14, the sifting integral definition of the &function, any continuous func- 
tion f(y) can be written 
f ( Y )  = 1; dx S(x - y)f(x). 
(5.100) 
The Riemann definition of integration says that an integral can be viewed as the limit 
of a discrete sum of rectangles, 
"+Z 
n= f m  
(5.101) 
where Ax is the width of the rectangular blocks which subdivide the area being 
integrated, and n is an integer which indexes each rectangle. The limiting process 
increases the number of rectangles, so for well-behaved functions, the approximation 
becomes more and more accurate as Ax ---f 0. The Riemann definition is pictured 
graphically in Figures 5.24(a) and (b). 
Using the Riemann definition, Equation 5.100 becomes 
f ( Y )  = /+-m dx f(x)@  - Y )  
--m 
(5.102) 

126 
THE DIRAC 6-FUNCTION 
(a) 
Ax 
co) 
Figure 5.24 Discrete Sum Representation of an Integral 
area of nb 6 - function 
= f(nAx)Ax 
- 
Ax 
Figure 5.25 The Construction of f(y) from an Infinite Sum of &Functions 
Equation 5.102 makes a very interesting statement: Any continuous function can be 
viewed as the sum of an infinite number of 6-functions. Both f ( y )  and the RHS of 
Equation 5.102 are functions of y, and are plotted in Figure 5.25 for finite Ax. In 
this figure, the 6-functions are located at y = nAx and have an area f(nAx)Ax. 
The function f ( y )  is generated by this sum of 6-functions as Ax + 0, i.e., as the 
spacing between the 6-functions and their areas go to zero. Thus an infinite number 
of infinitesimal area 6-functions, spaced arbitrarily close together, combine to form 
the continuous function f(y)! 
EXERCISES FOR CHAPTER 5 
1. Simplify the integral 
where a and b are real, positive constants. 

EXERCISES 
127 
2. 
3. 
4. 
Perform these integrations which involve the Dirac &function: 
i. J_9dx 6(x - 1). 
ii. 1: dx (x2 + 3)6(x - 5). 
iii. 1: dx x 6(x2 - 5 ) .  
iv. /:5 
dx x 6 ( x 2  + 5). 
r2v 
v. 
dx ~(COSX). 
vi. c’4 
dx x2 ~(COSX). 
d8(x - 5) 
10 
viii. /,,d. (x2 + 3) [ 
dx ] 
Determine the integral properties of the “triplet” d26(t)/dt2 by evaluating the 
integrals 
The function h(x) is generated from the function g(x) by the integral 
If h ( x )  is the triangular pulse shown below, find and plot g(x). 
-1 
1 

128 
5. Given that 
THE DIRAC 6-FUNCTION 
find and plot the first and second derivatives of f(x). 
6. An ideal impulse moving in space and time can be described by the function, 
f(x,t) = 106(x - vat), 
where v, is a constant. Make a three-dimensional plot for f ( x ,  t )  vs. x and t to 
show how this impulse propagates. What are the dimensions of vo? How does 
the plot change if 
where a, is a constant? What are the dimensions of a,? 
7. Consider the sinc function sequence: 
&(t) = sin (nn)/(rx). 
(a) On the same graph, plot three of these functions with n = 1, n = 10, and 
(b) Prove that the limit of the sinc sequence functions acts like a 6-function by 
n = 100. 
deriving the relation: 
As a first step, try making the substitution y = nt. You will need to use the 
identity: 
8. The function G(cosx) can be written as a sum of Dirac 6-functions 
n 
Find the range for n and the values for the a, and the xn. 
so that its charge density can be expressed as 
9. A single point charge qo is located at (1, 1,O) in a Cartesian coordinate system, 
pc(x, y ,  z) = qo 6(x - 1) w y  - 1) &z). 

EXERCISES 
129 
(a) What is pc(p, 8, z), its charge density in cylindrical coordinates? 
(b) What is pJr, 8, +), its charge density in spherical coordinates? 
10. An infinitely long, one-dimensional wire of mass per unit length A, is bent to 
follow the curve y = A cosh (Bx), 
as shown below. 
11 
Find an expression for the mass per unit volume p(x, y ,  z). Express your answer 
two ways: 
(a) As the product of two &functions. 
(b) As a sum of two terms, each the product of two &functions. 
Be sure to check the dimensions of your answers. 
An infinitely long, one-dimensional wire of mass per unit length A, is bent to 
follow the line formed by the intersection of the surface x = y with the surface 
y = z2, as shown in the figure below. Find an expression for pm(x, y ,  z), the mass 
per unit volume of the wire. 
Z 
Y 

130 
THE DIRAC &FUNCTION 
12. An infinitely long, one-dimensional wire with a constant mass per unit length A, 
is bent to follow the curve y = sinx in the z = 0 plane. 
Determine the mass density p,(x, y, z) that describes this mass distribution. 
13. A wire of mass per unit length A, is bent to follow the shape of a closed ellipse 
that lies in the xy-plane and is given by the expression 
x2 + 2y2 = 4. 
Express p,(x,y,z), 
the mass per unit volume of this object, using Dirac 6- 
functions. Show that your expression has the proper dimensions. There is more 
than one way to express the answer to this problem. Identify the most compact 
form. 
14. An infinite, one-dimensional bar of mass per unit length A, 
lies along the line 
y = m,x in the z = 0 plane. 
Y 
(a) Determine the mass per unit volume p(x, y, z) of this bar. 
(b) Now consider the situation where the bar is rotating about the z-axis at a 
constant angular velocity o, so that the angle the bar makes with respect to 
the x-axis is given by 8 = mot, as shown below. Find an expression for the 
time-dependent mass density p(x, y, z, t). 

EXERCISES 
131 
15. A charge Q, is evenly distributed along the x-axis fromx = -L,/2 to x = L,/2, 
as shown below. 
(a) Using the Heaviside step function, what is the charge density p&, y, z), 
(b) What is this charge density in cylindrical coordinates? 
expressed using Cartesian coordinates? 
Z 
16. Using &functions and the Heaviside step function, express the charge density 
p,(f) of a uniformly charged cylindrical shell of radius r, and length Lo. The 
total charge on the surface of the shell is Q,. 
17. An infinite, two-dimensional sheet with mass per unit area cr, is bent to follow 
the surface y = x3. 
(a) Make a plot of the curve made by the intersection of this sheet and the plane 
given by z = 0. 
(b) Determine the mass per unit volume pm(x, y, 2). 
18. Express the mass density pm(p, 8, z )  of a conical surface that is formed by cutting 
a pie-shaped piece from an infinite, uniform two-dimensional sheet of mass per 
unit area cr, and joining the cut edges. The conical surface that results lies on the 
surface p = a, z where (p, 8, z) are the standard cylindrical coordinates. 
Z 

132 
THE DIRAC &FUNCTION 
19. An infinite, two-dimensional sheet with mass per unit area a, is bent to follow 
the surface xy = 1 in a Cartesian coordinate system. 
(a) Using the hyperbolic coordinates developed in Exercise 13 of Chapter 3, 
express the mass density p,(u, u, z) for this sheet. 
(b) Using the equations relating the coordinates, convert your answer to part (a) 
above to Cartesian coordinates. 
(c) Now, working from scratch in a Cartesian system, obtain p,(x,y,z) by 
requiring that this density function take the volume integral over all space to 
a surface integral over the hyperbolic surface. 
20. Express the mass density p,@) for a spherical sheet of radius r,, with constant 
mass per unit area a,. 
21. A dipole electric field is generated outside the surface of a sphere, if the charge 
per unit area on the surface of that sphere is distributed proportionally to cos( 0). 
If the sphere has a radius r, and there is a total charge of +Qo on the upper 
hemisphere and -Q, on the lower hemisphere, what is the expression for the 
charge density pc(r, 0,4) in spherical coordinates? 
+ + + +  ;.-.: 
/ 
+ 
/ 
/ 
+ 
+ 
22. In a two-dimensional Cartesian coordinate system, the mass density pm(Q of a 
pair of point masses is given by 

EXERCISES 
133 
iii. S_: 1” cixldx2 [F * FI pm(x1, x2). 
iv. 1: 1: d x l d ~ 2  IF I;] pm(x1,xd. 
23. Prove that the monopole and quadrapole moments of any &pole (“physical” or 
24. A quadrapole charge distribution consists of four point charges in the xlx2-plane 
“ideal”) are zero. 
as shown below. 
(a) Using Dirac &functions express the charge density, pc(xl, x2, x3), of this 
(b) The quadrapole moment of this charge distribution is a second rank tensor 
distribution . 
given by 
- 
- 
Q = Qtj 
e l @ , .  
The elements of the quadrapole tensor are given by the general expression 
Q ,  = Im 
dxl /: dx2 1; dx3 p&l, 
x2, x3) [ ~ x J ,  - ( - Q % ) ~ J ]  
--m 
where a,, is the Kronecker delta. In particular, 
Q22 = /= 
dxl Irn 
dx2 Sy dx3 pc(xl, x2, x 3 )  [ k x z  - (x? + ~ 2 ’  
+ x?)] . 
Evaluate all the elements of the quadrapole tensor for the charge distribution 
shown in the figure above. 
- x  
-‘x 
--m 
(c) Does this charge distribution have a dipole moment? 
(d) Find the coordinate system in which this quadrapole tensor is diagonal. 
25. An ideal quadrapole has a charge density p,(x, y ,  y )  that is zero everywhere 
except at the origin. It has zero total charge, zero dipole moment, and a nonzero 
quadrapole moment. 
Express the elements of Q in this system. 

THE DIRAC 6-FUNCTION 
134 
(a) Show that if pc(x, y, t) has the form 
it satisfies the above requirements for an ideal quadrapole. Evaluate [?I SO 
that the elements of the quadrapole moment tensor for this distribution are 
the same as the quadrapole elements in Exercise 24. 
(b) Determine the electric field produced by this ideal quadrapole. 

INTRODUCTION TO 
COMPLEX VARIABLES 
One of the most useful mathematical skills to develop for solving physics and en- 
gineering problems is the ability to move with ease in the complex plane. For that 
reason, two chapters of this book are devoted to the subject of complex variable 
theory. This first chapter is primarily restricted to the fundamentals of complex vari- 
ables and single-valued complex functions. Some knowledge of complex numbers is 
assumed. 
6.1 A COMPLEX NUMBER REFRESHER 
Complex numbers are based on the imaginary number “i”, which is defined as the 
positive square root of - 1 : 
In general, a complex number z has a real and an imaginary part, 
z = x + iy, 
(6.2) 
where x and y are both real numbers. If y is zero for a particular complex number, 
that number is called pure real. If x is zero, the number is pure imaginary. 
6.1.1 The Complex Plane 
A complex number can be plotted as a point in the complex plane. This plane is 
simply a two-dimensional orthogonal coordinate system, where the real part of the 
135 

136 
INTRODUCTION TO COMPLEX VARIABLES 
Y 
imag 
z 
--. - 
real 
X 
Figure 6.1 A Plot of the Complex Number z = x + iy in the Complex Plane 
complex number is plotted on the horizontal axis and the imaginary part is plotted on 
the vertical axis. A plot of the complex number = x + iy is shown in Figure 6.1. 
The magnitude of the complex number z = x + iy is 
1g1 = d W ,  
(6.3) 
which is simply the distance from the origin to the point on the complex plane. 
6.1.2 Complex Conjugates 
The conjugate of a complex number can be viewed as the reflection of the number 
through the real axis of the complex plane. If g = x + iy, its complex conjugate is 
defined as 
z* 3 x - iy. 
(6.4) 
The complex conjugate is useful for determining the magnitude of a complex 
number: 
(6.5) 
2 
zz* = (x + iy)(x - iy) = x2 + y2 = JzJ . 
In addition, the real and imaginary parts of a complex variable can be isolated using 
the pair of expressions: 
g + g* - (x + iy) + (x - iy) - 
- x  
-- 
2 
2 
g - g* - (x + iy) - (x - iy) = y. 
-- 
2i 
2i 
6.1.3 The Exponential Function and Polar Representation 
There is another representation of complex quantities which follows from Euler’s 
equation, 
cos 0 + i sin 0. 
(6.8) 
,iO = 
To understand this expression, consider the Taylor series expansion of ex around zero: 

A COMPLEX NUMBER REFRESHER 
137 
(6.9) 
Now we make the assumption that this Taylor series is valid for all numbers including 
complex quantities. Actually, we dejne the complex exponential function such that 
it is equivalent to this series: 
e z z  1 + z +  ;+ z2 =_+g 
z3 
+ ..., 
- 
2! 
3! 
4! 
Now let g = i0 to obtain 
(iO>2 
  it^)^ 
( i ~ ) ~  
2! 
3! 
4! 
+ ~ 
+ ... 
eie = 1 + i8 + __ +- 
(6.10) 
(6.11) 
Breaking this into real and imaginary parts gives 
. 
(6.12) 
1 
2 !  
4! 
6! 
The first bracketed term in Equation 6.12 is the Taylor series expansion for cos 8, and 
the second is the expansion for sin 8. Thus Euler's equation is proven. 
Using this result, the complex variable z = x + iy can be written in the polar 
representation, 
z = re'' 
- 
= rcos8 + irsin8, 
(6.13) 
where the new variables r and 8 are defined by 
r = lz_l = Jm 
8 = tan-' y/x. 
The relationship between the two sets of variables is shown in Figure 6.2. 
imag 
r 
(6.14) 
(6.15) 
Figure 6.2 The Polar Variables 

138 
INTRODUCTION TO COMPLEX VARIABLES 
6.2 FUNCTIONS OF A COMPLEX VARIABLE 
Just as with real variables, complex variables can be related to one another using 
functions. Imagine two complex variables, and g. A general function relating these 
two quantities can be written as 
42I = E(2). 
(6.16) 
If y = u + iv and = a- + zy, then we can rewrite this relationship as 
where u(x, y) and v(x, y) are both real functions of real variables. 
As a simple example of a complex function, consider the relationshp 
Expanding z = x + iy gives 
- _  
w(t) = ( 2  - y2) + 2zxy. 
(6.19) 
The functions u(x, y) and v(x, y) from Equation 6.17 are easily identified as 
u(x, y )  = x2 - y2 
(6.20) 
v(x,y) = 2xy. 
(6.21) 
6.2.1 Visualization of Complex Functions 
A relationship between w and g can be visualized in two ways. The first interpretation 
is as a mapping of points from the complex ?-plane to the complex w-plane. The 
function 211 = 2 maps the point _z = 2i into the point 421 = -4 as shown in Figure 6.3. 
Another point z = 1 f i maps to y = 2i. By mapping the entire z-plane onto the 
w-plane in this way, we can obtain a qualitative picture of the function. 
A second, more abstract visualization is simply to plot the two functions u(x, y) 
and v(x,y) as the real and imaginary components of w. 
This method is shown in 
Figure 6.4 for the function w = sin z. 
Y 
~ z-plane 
&=2i 
X 
.~ 
V 
w-plane 
44b = Lo2 
wo= -4 
--.-- 
Figure 6.3 The Mapping Property of a Complex Function 

FUNCTIONS OF A COMPLEX VARlABLE 
139 
realy I 
imagy , 
0 
0 
, 
Figure 6.4 
The Plots of the Real and Imaginary Parts of &) as Functions of n and y 
6.2.2 Extending the Domain of Elementary Functions 
The elementary functions of real variables can be extended to deal with complex 
numbers. One example of this, the complex exponential function, was already intro- 
duced in the previous section. We defined that function by extending its Taylor series 
expansion to cover complex quantities. Series expansions of complex functions are 
covered in much greater detail later in this chapter. 
Trigonometric Functions We can perform a similar extension with the sine func- 
tion. The Taylor series of sin(x) expanded around zero is 
(6.22) 
where x is a real variable. The complex sine function can be defined by extending 
this series to complex values, 
where g is now a complex quantity. This series converges for all g. Similarly, the 
complex cosine function has the series 
z2 
z4 
z6 
cosz = 1 - = + L - L + ... 
- 
2! 
4! 
6! 
(6.24) 
which also converges for all values of g. 
to all complex quantities: 
With these extended definitions, Euler's equation (Equation 6.8) easily generalizes 
e'g = cosz - + ising. 
(6.25) 
This allows the derivation of the commonly used expressions for the sine and cosine 
of complex variables: 
,iz - e-iz 
2i 
ei< + e - i g  
2
,
 
sing = 
cosg = 
(6.26) 
(6.27) 

140 
INTRODUCTION TO COMPLEX VARIABLES 
The other trigonometric functions can be extended into &he complex plane using the 
standard relationships. For example, 
(6.28) 
Hyperbolic Functions 
are defined as 
The hyperbolic sine and cosine functions of real variables 
ex - e-x 
2 
sinhx = 
ex + ePx 
2
.
 
coshx = 
(6.29) 
(6.30) 
The complex hyperbolic functions are defined by following the same rules using the 
complex exponential function: 
(6.31) 
(6.32) 
The Logdhmic Function The definition for the logarithm of a complex variable 
comes directly from its polar representation: 
(6.33) 
6.3 DERIVATIVES OF COMPLEX FUNCTIONS 
The derivative of a complex function is defined in the same way as it is for a function 
of a real variable: 
This statement is not as simple as it appears, however, because Ag = Ax + iAy, 
and consequently Ag can be made up of any combination of Ax and iAy. That is 
to say, the point z can be approached from any direction in the complex z-plane. 
It is not obvious, and in fact not always the case, that the derivative, as defined in 
Equation 6.34, will have the same values for different choices of Az. 
Functions that have a unique derivative in a finite region are said to be analytic 
in that region. As will be shown, the Cauchy Integral Theorem, the Cauchy Integral 
Formula, and, in fact, the whole theory of complex integration are based on the 
analytic properties of complex functions. 

DERIVATIVES OF COMPLEX FUNCTIONS 
141 
6.3.1 The Cauchy-Riemann Conditions 
A necessary condition for the uniqueness of the derivative of ~ ( g )  
is that we obtain 
the same value for it whether AZ is made up of purely a Ax or purely an iAy. If 
A z  = Ax, Equation 6.34 becomes 
w(x + Ax + iy) - ~ ( x  
+ iy) 
(6.35) 
d w ( d  - lim - 
-- 
dg 
AX+O 
Ax 
On the other hand, if AZ = iAy, the derivative is 
(6.36) 
If I
&
)
 
is broken up into real and imaginary parts, ~ ( 2 )  
= u(x, y )  + iu(x, y ) ,  Equa- 
tions6.35 and 6.36 become 
u(x + Ax, y )  + iv(x + Ax, y )  - u(x, y )  - iu(x, y )  
lim 
dw(z) - 
- _  
dz 
A.r-0 
Ax 
and 
(6.37) 
(6.38) 
For these derivatives to be equal, their real and imaginary parts must be equal. 
This requirement leads to two equations involving the derivatives of the real and 
imaginary parts of &): 
(6.39) 
(6.40) 
Equations 6.39 and 6.40 are known as the Cauchy-Riemann conditions. They must 
be satisfied, at a point, if the derivative at that point is to be the same for AZ = Ax 
or AZ = iAy. Therefore, they are necessary conditions for the uniqueness of the 
derivative of a complex function. 
6.3.2 Analytic Functions 
The Cauchy-Riemann conditions are not enough to guarantee the uniqueness of the 
derivative. To obtain a sufficient condition, the function must meet some additional 
requirements. 

142 
INTRODUCTION TO COMPLEX VARIABLES 
If Az is made up of an arbitrary combination of Ax and iAy, then the derivative 
of ~ ( 2 )  
can be written as: 
(6.41) 
A Taylor series expansion of the real and imaginary parts of ~ ( z  
+ Ax + i h y )  about 
the point z = x + iy gives: 
dE(Z) - lim - 
w(g + Ax + i A y )  - ~ ( z )  
- _  
dg 
Ax,Ay+O 
A x  + iAy 
. 
(6.42) 
(&/ax + idv/dx)Ax + (aU/dy + idv/dy)Ay 
lim 
dw(z) - 
-- 
dZ 
A ~ , A ~ - o  
Ax + iAy 
For this equation to be valid, the partial derivatives of u(x, y )  and v(x, y )  must exist 
and be continuous in a neighborhood around the point g. A neighborhood is a region of 
arbitrarily small, but nonzero size surrounding a point in the complex plane. Dividing 
the numerator and denominator of the RHS by Ax before taking the limit gives 
If the derivative is to be independent of how the limit is taken with respect to Ax and 
A y ,  the RHS of Equation 6.43 must be independent of A x  and A y .  If the Cauchy- 
Riemann conditions are valid not just at the point I, but in the small region Ax A y  
about z, i.e., in the neighborhood of z, they can be substituted into the RHS of 
Equation 6.43 to give 
dW(Z) - lim 
(&/ax + idv/dx) + (-dv/dx + idu/dx)(Ay/Ax) 
-- 
dg 
A~,AY-+O 
1 + i A y / A x  
1 + i A y / A x  
1 + i A y / A x  
= 
lim (&/ax + idv/dx) 
Ax.Ay-0 
du 
dv 
dx 
dx 
+ i - ,  
- 
_ -  
which is independent of Ag. 
Consequently, if the partial derivatives of u(x, y )  and v(x, y) exist, are continuous, 
and obey the Cauchy-Riemann conditions in a neighborhood around a point z, the 
derivative of the function at that point is guaranteed to be unique. Functions that 
obey all these conditions are called analytic. A function may be analytic in the entire 
- 
z-plane or in isolated regions. But because the Cauchy-Riemann conditions must be 
satisfied in a neighborhood of the complex plane, a function cannot be analytic at an 
isolated point or along a line. The sum, product, quotient (as long as the denominator 
is not zero) and composition of two functions that are analytic in a region are analytic 
in that same region. Proofs of these statements are left as an exercise for the reader. 
There are many useful complex variable techniques which require the use of ana- 
lytic functions. In fact, the rest of the material in this chapter concerns the properties 
of analytic functions. 

DERIVATIVES OF COMPLEX FUNCTIONS 
143 
Example 6.1 First consider the function 
= z2 with 
u(x,y) = x2 - y2 
v ( x , y )  = 2xy. 
Taking the partial derivatives, 
and 
(6.45) 
(6.46) 
(6.47) 
(6.48) 
This function satisfies the Cauchy-Riemann conditions everywhere in the finite com- 
plex 2-plane. From either Equation 6.37 or 6.38, 
d w  
WX,Y) dv(x,y) - d&y) 
du(x,y) 
dZ 
dX 
dX 
dY 
dY 
+ i - - - -  
i
-
 
- 
= 2x + i2y 
(6.49) 
= 22, 
Notice that this derivative could have been obtained by treating g in the same manner 
as a real, differential variable, i.e., 
(6.50) 
Example 6.2 
real and imaginary parts of w are obtained by using g*, 
As another example, consider the function 
= 1 / z .  In this case, the 
so that 
(6.51) 
(6.52) 
(6.53) 
A check of the partial derivatives shows that the Cauchy-Riemann conditions are 
satisfied everywhere, except at 2 = 0, where the partial derivatives blow up. Therefore, 
1 /z is analytic everywhere except at z_ = 0, and its derivative is 
(6.54) 

144 
INTRODUCTION TO COMPLEX VARIABLES 
Example 6 3  As a final example, consider the function w = z z*, 
so that 
- 
w = (x + iy>(x - iy) = x2 + y2. 
(6.55) 
The real and imaginary parts are 
u(x,y) = x2 + y* 
V ( X , Y )  = 0 
(6.56) 
(6.57) 
so that 
and 
(6.58) 
(6.59) 
The Cauchy-Riemann conditions are not satisfied, except at the origin. Because this 
a single, isolated point, the function w = zg* is not an analytic function anywhere. 
This is made more obvious when the derivative of this function is formed. Using 
Equation 6.37, the derivative is 
while using Equation 6.38, 
(6.60) 
(6.61) 
6.3.3 Derivative Formulae 
The rules you have learned for real derivatives hold for the derivatives of complex 
analytic functions. The standard chain, product, and quotient rules still apply. All the 
elementary functions have the same derivative formulae. For example: 
6.4 THE CAUCHY INTEGRAL THEOREM 
(6.62) 
An important consequence of using analytic functions is the Cauchy Integral Theo- 
rem. Let w(z) be a complex function which is analytic in some region of the complex 

THE CAUCHY INTEGRAL THEOREM 
145 
imag 
z-plane 
n- 
u 
Figure 6.5 The Closed Cauchy Integral Path 
plane. Consider an integral 
(6.63) 
along a closed contour C which lies entirely within the analytic region, as shown in 
Figure 6.5. Because ~ ( g )  
= u(x, y )  + iv(x, y )  and dg = dx + idy, Equation 6.63 can 
be rewritten as 
Stokes’s theorem 
(6.65) 
can now be applied to both integrals on the RHS of Equation 6.64. For a closed line 
integral confined to the xy-plane, Equation 6.65 becomes 
(6.66) 
The first integral on the RHS of Equation 6.64 is handled by identifying V, = u(x, y ) ,  
v = -  v(x, y ) ,  and S as the surface enclosed by C, so 

146 
INTRODUCTION TO COMPLEX VARIABLES 
The second integral of Equation 6.64 is handled in a similar manner, by identifying 
V, = v ( x , y )  and V, = u(x, y), so 
v(x, y )  + dy u(x, y)] = 
d x d y  - - - . 
1 (2 :;) 
(6.68) 
Because &) is analytic and obeys the Cauchy-Riemann conditions inside the entire 
closed contour, &/ax = dv/@ and &/dy = -dv/dx, and we are left with the 
elegant result that 
(6.69) 
6.5 CONTOUR DEFORMATION 
There is an immediate, practical consequence of the Cauchy Integral Theorem. The 
contour of an complex integral can be arbitrarily deformed through an analytic region 
without changing the value of the integral. 
Consider the integral 
along the contour C, from a to b, drawn as the solid line in Figure 6.6(a). If we add 
to this an integral with the same integrand, but around the closed contour C, shown 
as the dotted line in Figure 6.6(a), the result will still be I, as long as the integrand is 
analytic in the entire region inside C: 
~ imag 
imag 
real 
real 
i 
(6.71) 
Figure 6.6 Contour Deformation 

THE CAUCHY INTEGRAL FORMULA 
147 
But notice the part of C that lies along the original contour has an equal, but opposite 
contribution to the value, and the contributions from the two segments cancel, leaving 
the contour shown in Figure 6.qb). Therefore, 
(6.72) 
Deformations of this kind will be very handy in the discussions that follow. 
6.6 THE CAUCHY INTEGRAL FORMULA 
The Cauchy Integral Formula is a natural extension of the Cauchy Integral Theorem 
discussed previously. Consider the integral 
(6.73) 
where f(z) is analytic everywhere in the z-plane, and C1 is a closed contour that 
does no? include the point G, 
as depicted in Figure 6.7. It has already been shown 
that the function 1 /g is analytic everywhere except at z = 0. Therefore the function 
1 /(z - ZJ 
is analytic everywhere, except at z = 5. Because CI does not enclose the 
point z = L, 
the integrand in Equation 6.73 is analytic everywhere inside C1 , and by 
Cauchy’s Integral Theorem, 
Now consider a second integral, 
(6.74) 
(6.75) 
similar to the first, except now the contour C2 encloses 5, as shown in Figure 6.8. The 
integrand in Equation 6.75 is not analytic inside C,, so we cannot invoke the Cauchy 
real 
~~~ 
~~ 
~ 
~ 
Figure 6.7 The Contour C1 for the Cauchy Integral Formula 

148 
INTRODUCTION TO COMPLEX VARIABLES 
I
-
 
____ 
real 
Figure 6.8 The Contour C, for the Cauchy Integral Formula 
Integral Theorem to argue that 12 = 0. However, the integrand is analytic everywhere, 
except at the point z = 5, so we can deform the contour into a infinitesimal circle of 
radius E ,  centered at b, 
without affecting the value: 
(6.76) 
This deformation is depicted in Figure 6.9. 
The integral expression of Equation 6.76 can be visualized most easily with the use 
of phasors, which are essentially vectors in the complex plane. Consider Figure 6.10. 
In Equation 6.76, z lies on the contour C, and is represented by a phasor from the 
origin to that point. The point 5 is represented by another phasor from the origin 
to L. 
In the same way, the quantity z - z, is represented by a phasor that goes from 
~0 to z. If the angle <b is defined as shown, using polar notation we have 
z - 5 = Ee'@. 
(6.77) 
On C,, E is a constant so 
(6.78) 
imag 
real 
Figure 6.9 The Equivalent Circular Contour for the Cauchy Integral Formula 

THE CAUCHY INTEGRAL FORMULA 
149 
real 
Figure 6.10 Phasors for the Cauchy Formula Integration 
and 4 goes from 0 to 27r. The integral in Equation 6.76 becomes 
(6.79) 
As E -+ 0, f(z + &') 
goes to f (z ) and can be taken outside the integral: 
- 4  
- 4  
2.n 
(6.80) 
Our final result is 
(6.81) 
where C is any closed, counterclockwise path that encloses G, and - 
f ( z )  is analytic 
inside C. 
Equation 6.81 is the simplest form of Cauchy's Integral Formula. A more general 
result can be obtained by considering the point 
to be a variable. If we restrict z+ 
to 
always lie inside the contour C, then Equation 6.81 can be differentiated with respect 
to % to give 
(6.82) 
In fact, Equation 6.81 can be differentiated any number of times, to give the most 
general form of Cauchy's Integral Formula: 
f (z) 
dg- 
(6.83) 
Here again, C is any closed, counterclockwise contour, 
- 
f(z) is analytic everywhere inside C. 
is any point inside C, and 

150 
INTRODUCTION TO COMPLEX VARIABLES 
Equation 6.83 says something quite important about the nature of analytic func- 
tions. If the value of an analytic function is known everywhere on some closed contour 
C, the value of that function and all its derivatives are uniquely specified everywhere 
inside C. 
6.7 TAYLOR AND LAURENT SERIES 
Just as with real functions, complex functions can be expanded using Taylor series. 
An extension of the standard Taylor series, called the Laurent series, is also frequently 
used to expand complex functions. Series expansions play a crucial role in the theory 
of complex functions, because they provide the fundamental tools for deriving and 
using the theory of residues, described in the next section. 
6.7.1 Convergence 
Before discussing Taylor and Laurent expansions for complex functions, it is appro- 
priate to say a few words about the convergence of complex series. We assume you 
are already familiar with the convergence properties of real series, so we will not 
attempt to cover that topic with any sort of mathematical rigor. 
An infinite summation of real numbers xn is said to converge to the value S if 
OD 
S = 
xn 4 A Finite Real Number. 
The series in Equation 6.84, is said to converge absolutely if 
n=O 
OD 
(6.84) 
lxnl + A Finite Real Number. 
(6.85) 
n=O 
Because 
cc 
m 
(6.86) 
n=O 
n=O 
absolute convergence is a more rigid requirement than convergence. That is, if a series 
converges absolutely it will always converge. However, a series may converge even 
though it does not converge absolutely. For example, the alternating harmonic series 
(6.87) 
converges to In 2, but the harmonic series 
(6.88) 
diverges. 

TAYLOR AND LAURENT SERIES 
151 
The ratio test can be used to establish the absolute convergence of a series. That 
is, if the terms of an infinite series satisfy 
(6.89) 
the series converges absolutely. Again, a series may converge even though it does not 
satisfy the ratio test. There are plenty of other tests for convergence, but the ratio test 
is the only one we need for our discussions. 
Things are not too different when we consider infinite series of complex numbers. 
A summation of complex numbers gfl converges to the complex value 8 if 
x 
- s = c-fl 
z + A Finite Complex Number. 
For convergence of the infinite complex series, both the real and imaginary parts must 
converge. That is, if we write I, = x, + iy, and 8 = S, + iSi, we must have 
(6.90) 
fl=O 
CD 
S, = 
xn -+ A Finite Real Number 
m 
(6.91) 
n=O 
Si = 
yn + A Finite Real Number. 
n=O 
The complex series defined in Equation 6.90 converges absolutely if 
a' 
/z, 
I -+ A Finite Complex Number. 
(6.92) 
n = O  
Because 
m 
m 
n = O  
fl=O 
and 
m 
m 
n=O 
fl=O 
if the series converges absolutely, the two series in Equations 6.91 will both converge, 
and consequently the original complex series in Equation 6.90 must also converge. 
The absolute convergence of a complex series can also be established by the ratio 
test. That is, if the terms of the series satisfy 
the series will converge absolutely. 
(6.95) 

152 
INTRODUCTION TO COMPLEX VARIABLES 
6.7.2 Taylor and Laurent Series of Real Functions 
Before dealing with expansions for complex functions, Taylor and Laurent series for 
real functions will be reviewed. Recall that the Taylor series expansion of f ( x ) ,  a real 
function of the variable x, around the point no is 
m 
f ( x )  = &(x 
- x,)" = c, + q ( x  - x,) + c*(x - x,)2 + * * . . 
(6.96) 
n=O 
The coefficients 
can be obtained from the expression 
(6.97) 
and depend upon both the function f ( x )  and the expansion point x,. The region of 
convergence of this series can be determined by using the ratio test. The series on the 
RHS of Equation 6.96 converges if 
(6.98) 
In general a Taylor series will converge for values of x in a symmetric range around 
x,, given by Ix - x,l < A, where A depends upon both the expansion point and the 
function f(x). 
A Laurent series expansion differs from a Taylor series because of the additional 
terms with negative n values. An expansion of f ( x )  around x, using a Laurent series 
has the form 
(6.99) 
Convergence for a Laurent series is slightly more complicated to verify than for 
a Taylor series, because we must check the convergence of both the positive and 
negative terms. In many cases, the negative terms of the series terminate at some 
finite value of n, and we only need to worry about the convergence of the positive 
terms. For the general case, however, we can perform two ratio tests, one for the 
positive terms and one for the negative terms. Clearly, if there are any terms with 
negative n values, the series cannot converge at x = x,. In general, a Laurent series 
will converge in a symmetric range of x given by Amin < Ix - x,l < A,,, 
where 
Amin and A- 
are the minimum and maximum allowed distances from the expansion 
point. 

THE COMPLEX TAYLOR SERIES 
153 
z" - z"" 
Zm - zm+l 
- 
0 
Figure 6.11 Polynomial Long Division 
6.7.3 A Basic Complex Taylor Expansion 
As part of the derivation of complex Taylor and Laurent series, the following basic 
complex Taylor expansion will prove to be valuable. This expansion, 
m 
(6.100) 
converges absolutely for all IzI < 1, a fact easily shown by applying Equation 6.95, 
the ratio test for complex series. This result derives from the rn + cc limit of the 
identity 
(6.101) 
which can be proved by multiplying both sides of this expression by (1 - z), or by 
the long division shown in Figure 6.1 1. 
6.8 THE COMPLEX TAYLOR SERIES 
Similar to a real Taylor series, a complex Taylor series attempts to expand a complex 
function around a point, using a sum of complex terms: 
z 
(6.102) 
2 
- 
f(z) = C c n ( g  - zJn = & + c1(z - ZJ 
+ c& - 5) + . . . . 
n=O 
The c, coefficients and the region of convergence of the series depend on the func- 
tion f(g) and the expansion point z. We will show how these coefficients and the 
region of convergence can be obtained in two ways, first by using Cauchy integra- 

154 
INTRODUCTION TO COMPLEX VARIABLES 
tion techniques, and then by applying the basic complex Taylor series developed in 
Section 6.7.3. 
6.8.1 
The process of determining the coefficients from a Cauchy integration begins by 
selecting an expansion point % and a counterclockwise, circular contour C centered 
around the expansion point, as shown in Figure 6.12. The function f(z) and its analytic 
properties are presumed to be known. Assuming f(z) is analytic everywhere inside C, 
Cauchy’s Integral Formula (Equation 6.81) state; that 
The Taylor Coefficients from Cauchy Integration 
(6.103) 
where z is any point inside C, and z/ is the complex variable of integration that lies 
along c. 5 can be inserted into the denominator of the integrand and manipulated as 
follows: 
(6.104) 
Because Iz - 51 < Iz’ - 
Equation 6.100 implies that 
for all z inside C, the basic complex Taylor series of 
imag 
z’ 
z-plane 
(
-
J
c
 
real 
(6.105) 
Figure 6.12 Contour for Taylor Series Coefficients 

THE COMPLEX TAYLOR SERIES 
155 
which converges for all z inside C. Substitution of Equation 6.105 into 6.104 gives 
(6.106) 
We will now interchange the order of integration and summation in Equation 6.106. 
It is not obvious that this is mathematically allowed, because we are manipulating 
an infinite series. An important theorem, however, says that the integration and 
summation of a series, or for that matter the differentiation and summation of a 
series, can be interchanged if the series in question converges uniformly. Uniform 
convergence is a property of infinite series of functions of one or more independent 
variables. We will not cover this topic in this text, but we refer you to almost any book 
on advanced calculus. See, for instance, Kaplan, Advanced Calculus or Sokolnikoff 
and Redheffer, Mathematics of Physics and Modem Engineering. The interchange 
of the integration and summation gives 
Comparing Equation 6.107 with Equation 6.102, the general form of the Taylor 
series for f ( z ) ,  identifies the coefficients as 
- 
(6.108) 
Using Equation 6.83, the general form of Cauchy’s Integral Formula, the coefficients 
can be recognized as 
(6.109) 
which is a satisfying extension of Equation 6.97 for the coefficients of real Taylor 
series. These coefficients will generate a complex Taylor series that will converge to 
f(z) for all g inside C .  The convergence properties of the series are best demonstrated 
by the example that follows. 
Example6.4 As an example of this process, let’s obtain the coefficients of the 
Taylor series for the complex function 
(6.110) 
This function is analytic everywhere in the complex plane, except at z = a. The 
Taylor series is to be expanded about z = z,. These points are shown in Figure 6.13. 
The circular path for the Cauchy integration is also shown in this figure. It is centered 
at z, and has a radius such that 1/(z - 4 is analytic everywhere inside. In other 
words, if z’ is a point along the contour, lz’ - < la - 51. The coefficients for the 

156 
INTRODUCTION TO COMPLEX VARIABLES 
. _ a  
Sinmilnritv 
zI '* 
z-plane 
I 
- 
\ \ 
\ 
\ 
\ 
\ 
I 
\ 
I 
\ 
/ 
\ 
/ 
=-I-- 
-I_ 
circle 
I '\ 
\ 
, 
/' 
red 
Figure 6.13 The Complex Plane for Determining the Taylor Coefficients for l/(z - UJ 
.__/ 
Taylor series can now be obtained from Equation 6.109 as 
c = _ A  
g=% 
--n 
=- ( 
)n+' 
a - J  
so that 
(6.111) 
(6.1 12) 
This series converges in the crosshatched region of Figure 6.13 where Iz - z, I < 
ig' - 5 I. In that same figure, we have drawn a dashed circle, centered at % and passing 
through the singularity at a. This circle, which we call a singularity circle, is the largest 
radius that the contour C can have, while still keeping f(g) analytic everywhere inside. 
Consequently, the region of convergence for the serierin Equation 6.1 1 1 can be made 
to extend up to the singularity circle, as shown in Figure 6.14. If a value of z is chosen 
$mag 
- 
z-plane 
Maximum region of 
convergence 
'; 
Figure 6.14 Region of Convergence 

THE COMPLEX TAYLOR SERIES 
157 
real 
Figure 6.15 The 
Dependence of the Convergence Region 
that lies inside this singularity circle, and is inserted into the LHS of Equation 6.112, 
we will obtain the same finite value that we get from inserting this same g into the 
RHS. On the other hand, if we select a point outside of the singularity circle, and 
use it in the same equation, the LHS will give a finite number, while the RHS will 
diverge. 
If we have a more complicated function, with more than one nonanalytic point, the 
region of convergence is inside the smallest of all the singularity circles. Also, notice 
that the Taylor coefficients and the region of convergence depend on the choice of 5. 
By selecting different ~ ’ s ,  
different Taylor series can be generated. Figure 6.15 shows 
two different expansion points and their corresponding regions of convergence. In 
this way, a Taylor series can be generated for this function that is valid for any Z, 
except z = a. From this picture it can also be seen that no single Taylor series can 
be made to converge in a region that surrounds a singularity point. This will be an 
important point in our discussion of Laurent series. 
6.8.2 The Taylor Coefficients Using the Basic Taylor Series 
Many times it is possible to determine the Taylor series coefficients and the region of 
convergence without using the technique described above, but instead by manipulat- 
ing f ( g )  to be in the form of the basic Taylor series given in Equation 6.100. This is 
bestdemonstrated by the following examples. 
Example 6.5 Again consider the function 
(6.1 13) 
where a is a complex constant. Dividing the numerator and denominator by -a gives 
(6.1 14) 

158 
INTRODUCTION TO COMPLEX VARIABLES 
As long as lg/gl < 1, the denominator can be expanded using Equation 6.100 to give 
- -' [I + 5 + ( z ) 2 +  (5)3+ 
- 
a 
. . a ] .  
(6.115) 
g - a  
a 
a
a
 
But Equation 6.115 is already in the form of a complex Taylor series, with an 
expansion point around the origin: 
m 
with 
n + l  
G = -(:) 
(6.1 16) 
(6.1 17) 
Convergence of this series can be checked by a ratio test 
(6.118) 
n-im 
which gives the condition 
IZI < la/, 
(6.119) 
which you might have guessed from the condition on the series expansion used in 
Equation 6.1 15. 
Notice the convergence region is a circle centered at the expansion point with a 
radius of Igl. This confirms the previous discussion which led to Figure 6.14. 
Example 6.6 
expanded about an arbitrary point 6 = 5. The series must take the form 
Now let's seek a Taylor series for the same function, but this time 
+m 
(6.120) 
If we manipulate the function as follows: 
(6.121) 
- -  
1 - 
1 
= (2) 
(+ 1 
g - a  
(g-z,)-@-zJ 
a - z ,  
1 - z  
we can use the same technique as before, and write 
with the region of convergence 
(6.123) 

THE COMPLEX LAURENT SERIES 
159 
which is the same region depicted in Figure 6.14. Thus we find the coefficients for 
this expansion are 
(6.124) 
Once again, the series converges in a circular region centered at 5. 
The conver- 
gence radius is the distance from the expansion point to the nonanalytic point a. 
Notice that these are the same coefficients and region of convergence we obtained 
using the Cauchy integration approach. 
6.9 THE COMPLEX LAURENT SERIES 
A complex Laurent series has the form 
(6.125) 
and is distinguished from a Taylor series because it contains negative n terms. As 
with the Taylor series, the coefficients and the region of convergence depend upon 
the function f(g) and the expansion point 5. These coefficients and the region of 
convergence for the Laurent series will be obtained from a Cauchy integral approach, 
and also by using the basic complex Taylor series developed in Equation 6.100. 
Laurent series are useful when an expansion is desired that converges in a region 
surrounding a nonanalytic point of - 
f(z). 
6.9.1 
Imagine we wanted to generate a series expansion for the function 1 /(z - a) about 5, 
as we did in the previous section, but this time, we want an expansion which is valid 
in a region outside of the singularity circle of g. We cannot use the contour shown 
in Figure 6.16, or any such contour which surrounds the singularity, and expect to 
The Laurent Coefficients from Cauchy Integration 
Singularity 
circle 
- 
z-plane 
Figure 6.16 Invalid Contour for Expanding for 1/(1 - a) around 5 

160 
INTRODUCTION TO COMPLEX VARIABLES 
Singularity 
circle 1 
1.. 
Figure 6.17 Contour for Calculating Laurent Coefficients 
generate a Taylor series using Equation 6.108, because that equation requires that 
- 
f (z) be analytic at all points inside C. 
Instead, consider the contour shown in Figure 6.17. In the crosshatched region 
interior to C, the function - 
f(z) - is analytic. Therefore, according to Cauchy's Integral 
Formula, 
(6.126) 
Using contour deformation, the two straight-end pieces of C can be brought arbitrarily 
close together, so their contributions to the integral cancel. Therefore, we can rewrite 
this integral as 
where C1 and C2 are shown in Figure 6.18. Notice the sneaky introduction of a minus 
sign into the contour C2, so both contours are now counterclockwise. Again, we 
require C1 and C2 to be circular, so we can use a similar expansion to the one used 
with the Taylor series. 
Figure 6.18 Double Circular Contour for Laurent Coefficient Integration 

THE COMPLEX LAURENT SERIES 
161 
Next manipulate the integrals of Equation 6.127. First insert G, 
(6.128) 
and then generate quantities that can be expanded using the basic Taylor series of 
Equation 6.100: 
(6.129) 
We have manipulated the two denominators differently in anticipation of the series 
expansions about to be performed. On C1, Iz - 51 < Iz’ - G I ,  and we use the 
expansion 
(6.130) 
On C,, 12 - GI > Jz’ - 51, so a different series is used: 
m 
(6.13 1) 
1 
When these expansions are substituted into Equation 6.129, and the orders of the 
integrations and summations are reversed, the result is 
Equation 6.132 is not quite in the form we are seeking. The first summation, involving 
the integrals over C1, is fine and can be identified with the n 2 0 terms of the Laurent 
series. In the second summation, m goes from 0 to + m. If the substitution m = - n - 1 
is made, this summation can be identified with the n < 0 terms of a Laurent series. 

162 
INTRODUCTION TO COMPLEX VARIABLES 
Equation 6.132 can be rewritten as 
--m 
(z - ZJ". 
(6.133) 
Comparing Equation 6.133 with Wuation 6.125, the general expression for the 
Laurent series, the 
for n 2 0 are given by integrals over C1 
while the 
for n < 0 are given by integrals over C2 
(6.134) 
(6.135) 
The Laurent series that is generated by these coefficients will be valid in the 
annular, crosshatched region of Figure 6.18. This region can be expanded by shrinking 
C1 and expanding C2, while keeping f(g) analytic between the two circles. Therefore 
C2 can be shrunk to the singularitycircle passing through g = a, and C1 can be 
expanded until it runs into another singularity off(@. Ifb is the first such singularity, 
the convergence region would be as shown in-igure 6.19. If - 
f (g) has no other 
singularities, C, can be expanded to infinity. 
Once we have determined the maximum contour C, and minimum contour C1, 
as described above, the coefficient expressions in Equations 6.134 and 6.135 can be 
combined into a single expression, 
(6.136) 
Figure 6.19 
lytic Points 
Convergence Region for the Laurent Series Expansion Between Two Nonana- 

THE COMPLEX LAURENT SERIES 
163 
1 imag 
Singularity 
circles 
Figure 6.20 Complex Plane for Determining the Laurent Coefficients 
where C is any contour which lies entirely within the two expanded contours. This 
last simplification is a direct consequence of the deformation theorem for analytic 
regions. 
For n < 0, the only nonanalytic points of the integrand in Equation 6.136 are 
those associated with f(& For n 2 0 the integrand has singularities associated with 
- 
f(z) as well as those generated by the (2' - &)"+' factor in the denominator. Notice 
that if f(z) is analytic everywhere inside C, all the c for n < 0 must be zero, and the 
LaureG series degenerates to a Taylor series, as expected. 
Example 6.7 As an example of this process, we will determine the coefficients of 
the Laurent series expansion for the complex function 
(6.137) 
This function is analytic everywhere in the complex plane, except at z = a and z = b. 
The Laurent series is to be expanded around the point z = L. 
These points are shown 
in Figure 6.20, where we have assumed that a is closer to L than b. For this example, 
the contour C used for determiningthe Laurent coefficients from Equation 6.136'must 
be centered at G, but can be located in three general regions: inside the singularity 
circle of a, between the g and b singularity circles, or outside the b singularity circle. 
These three situations will be treated separately. 
The Contour C Inside the Small SingurcVity Circle 
The case of the C contour 
lying inside the singularity circle of g is shown in Figure 6.21(a).The coefficients of 
the Laurent series are obtained from the Cauchy integration of Equation 6.136 as 
(6.138) 
For n < 0, the integrand of Equation 6.138 is analytic inside C and so for n < 0 all 
the 
are zero. For n 2 0, the only nonanalytic point of the integrand is at L, 
and so 

164 
INTRODUCTION TO COMPLEX VARIABLES 
n 2 0. 
(6.139) 
n+l 
Therefore, for this case, the series expansion becomes 
The region of convergence for this series can be determined by finding the largest 
and smallest circles around the point 5 that have f(z) analytic everywhere in between. 
In this case, the largest circle we can have lies Gst inside the singularity circle that 
passes through a. Because there are no other nonanalytic pints inside this circle, the 
smallest circle can be collapsed to a circle of radius zero. Thus the series converges 
for the entire crosshatched region shown in Figure 6.21(b). 
In this case, it can be seen that the series in Equation 6.140 is not a Laurent series 
at all, but just a simple Taylor series with the standard Taylor series convergence 
region. This will always be the case if f(g) is analytic inside C. 
- 
The Contour C Between the SingurcUity Circles Now let's look at the more 
interesting situation where the contour C lies between the singularity circles, as 
shown in Figure 6.22(a). The coefficients for the Laurent series can be obtained by 
Cauchy integration using an expression identical to Equation 6.138, except C is now 
between the singularity circles 

THE COMPLEX LAURENT SERIES 
165 
1 imag 
I h a g  
(a) 
(b) 
Figure 6.22 Complex Plane for C Between the Singularity Circles 
For n < 0, the integrand of Equation 6.14 1 is no longer analytic inside C because of 
the singularity at g = a. Therefore, 
for n < 0. 
1 
c n  = (7) 
a - G  
(6.142) 
For n 2 0, there are two singularities inside C ,  one at 5 and another at a. Therefore, 
after some work, we obtain 
(6.143) 
For this case, the series expansion becomes 
The region of convergence for this series is obtained by determining the largest 
and smallest circles, centered around L, 
that still have f(z) analytic everywhere in be- 
tween. In this case, the largest circle is just inside the singularity circle passing through 
- 
b, while the smallest is the circle that lies just outside the singularity circle passing 
through a. The resulting annular convergence region is shown in Figure 6.22(b). 
In this case, we have a true Laurent series with n < 0 terms in the expansion. 
Notice how the terms for n 2 0 make up the Taylor series expansion for the 1 /(z - b) 
part of f(& while the n < 0 terms are a Lament expansion for the l/(z - a) part. 
The n 7 0 terms diverge inside the singularity circle that passes through LZ, while 

166 
INTRODUCTION TO COMPLEX VARIABLES 
the n 2 0 terms diverge outside the singularity circle that passes through &. The 
combination only converges between the two singularity circles. 
The Contour C Out3ide the Large Singu- 
Circle As a final variation on 
this expansion, consider the situation where the contour C lies outside the large 
singularity circle that passes through b, as shown in Figure 6.23(a). The coefficients 
for the expansion can be obtained by a Cauchy integration using an expression 
identical to Equation 6.138, except C is now outside the singularity circle that passes 
through b: 
(6.145) 
For n < 0, the integrand of Equation 6.145 has two singularities, one at z = a and 
the other at z = b, and we have 
n+l 
n+l 
.=(=) 
+(=) 
forn<O. 
(6.146) 
For n 2 0, there are three singularities inside C, at 2 = a, z = b, and g = L. 
After 
more work than last time, we find the negative coefficients are all zero: 
4 = 0 
forn 2 0. 
(6.147) 
The series expansion, therefore, becomes 
As before, the region of convergence for this series is obtained by finding the largest 
and smallest circles centered at z, that keep f(z) analytic between them. In this case 
- 
(4 
Figure 6.23 The Complex Plane for C 
(b) 
Lying Outside the Large Singularity Circle 

THE COMPLEX LAURENT SERIES 
167 
the smallest circle lies just outside the singularity circle that passes through z = b. 
The largest circle can be extended to infinity because there are no other nonanalytic 
points of f(z_). Therefore, the convergence region is as shown in Figure 6.23(b). 
In thiscase we again obtain a true Laurent series. The coefficients of the series 
contain two terms. The first generates a Laurent series for l/(z - a) and the second 
a Laurent series for 1 /(z - b). The 1 /(z - uJ part of the series converges outside the 
singularity circle that passes through a. The 1 /(g - bJ part of the series converges out- 
side the singularity circle that passes through b. The combination converges outside 
the largest of the two singularity circles, the one passing through b. 
6.9.2 Laurent Coefficients Using the Basic Taylor Series 
Evaluating series coefficients from Equation 6.136 is not always an easy task. For- 
tunately, like the Taylor series, Laurent series often can be generated from the basic 
series expansion of Equation 6.100. This is best demonstrated by an example. 
Example 6.8 
Consider the function 
(6.149) 
1 
Z-a 
- 
f(z> = - 9  
which is analytic at every point except 
function expanded about 5 = 0 in the form 
= a. We seek a Laurent series for this 
(6.150) 
We can manipulate Equation 6.149 using Equation 6.100 to obtain 
L = ( ; ) ( & - ) = ( ~ ) [ l + ; + ( Q ) 2 + - . ]  
z - a  
(6.151) 
This is in the form of a Laurent series with only n < 0 terms. The coefficients of 
Equation 6.150 are 
0 
n r O  
Cn = { - 
a-"-' 
n < O '  
Because the series expansion in Equation 6.150 converges if 
lzl > lal, 
(6.152) 
(6.153) 
this Laurent series is valid in the region shown in Figure 6.24. This is the region 
outside the singularity circle that passes through the point a_. There is no outer limit 
to this convergence region. 

168 
INTRODUCTION TO COMPLEX VARIABLES 
Figure 6.24 Convergence Region for Laurent Series Expansion About 5 = 0 
The procedure for an expansion about an arbitrary point z = z, is similar. Manip- 
ulate Equation 6.149 as 
1 
- 
1 
-- 
z - g  
(z-z,)-@-z,) 
1 
(6.154) 
Expanding the second term gives 
1 
I+=---+ 
a - %  (-,:+...I. 
a - z ,  
z - z ,  
z - z ,  
which again is a Laurent series with coefficients 
0 
n 2 0  
= { (a - %)-n-l 
n < 0 * 
The series converges if 
Iz - 51 ’ 
la - 51 7 
imag 
z-plane 
Convergence regio 
1 
(6.155) 
(6.156) 
(6.157) 
Figure 6.25 
Convergence Region for Laurent Series Expanded about z, 

THE COMPLEX LAURENT SERIES 
169 
which is a region outside the singularity circle centered at z, and passing through 
- 
a, as shown in Figure 6.25. Notice, the region of convergence can be modified by 
changing the expansion point 5. 
As z, gets closer to a, the series becomes valid for 
more and more of the complex plane, but can never be made to converge exactly at 
z = a. 
- 
6.9.3 Classification of Singularities 
Laurent expansions allow singularities of functions to be categorized. Imagine the 
point z = a is a singularity of the function f ( 3 .  If the most negative term in the 
Laurent series expansion around the singular &int is -m, the singularity is called a 
pole of order m. For example, the function 
(6.158) 
is its own Laurent expansion around z = a, and thus this function has a pole of order 
one at that point. This is often calleda simple pole. The function 
(6.159) 
1 
f(z> = ~ 
- 
(z - d3 
again is its own Laurent Expansion around z = 
three. 
nation does not. An example is the function 
and therefore has a pole of order 
It is possible to have a function that appears to have a pole, but on closer exami- 
(6.160) 
This function has a singularity at the point g = 0, because the denominator goes to 
zero, but the Laurent expansion around z = 0 has no negative terms. This type of 
singularity is called removable because the function does not diverge as we approach 
the singularity. This can be seen in the case of Equation 6.160 because 
(6.161) 
even though the value exactly at z = 0 is undefined. 
It was quite easy to determine the order of the singularities in Equations 6.158 and 
6.159, because they were in the convenient form of 1/(g - d)". 
For a singularity of 
a more complicated function, you might think we would always have to determine 
the coefficients of the Laurent series. Fortunately, there is another approach, which is 
often much quicker. Notice for a pole of order m at z = a, multiplication by the factor 
(z - a)" will get rid of the divergence at a. In other words, if the original function 

170 
INTRODUCTION TO COMPLEX VARIABLES 
diverges because of a pole of order rn at z = a, i.e., 
the divergence can be removed by multiplying by the factor (z - UJ"" 
lim [f(g)(z - d,"] 
-+ 
a finite value. 
(6.163) 
z-%? 
Thus, one way to determine the order of a pole is to multiply by successive factors 
of (z - 13), until the result no longer diverges at a. The number of multiplying factors 
needed determines the order of the pole. 
and this diver- 
gence cannot be removed by multiplying by (z - uJm for any value of m. This is called 
an essential singularity. The Laurent expansion for a function about an essential sin- 
gularity point will have an infinite number of negative n terms. The function e '
'
2
 
has 
an essential singularity at z = 0. This function goes to infinity at z = 0 faster than 
any power of g goes to 0. 
A final type of singularity exists if the function diverges at z = 
Example 6.9 To illustrate this technique, consider the function 
(6.1 64) 
There are singularities everywhere cos z = 1. Let's consider just the singularity at 
z = 0. 
First we will demonstrate that the function diverges as we approach z = 0. Notice 
that both the numerator and denominator are zero at this point, so we must use 
1'Hopital's rule to determine the limiting behavior of the function. The complex 
version of 1'Hopital's rule is identical to the version for red functions: 
as long as both 
lim f (z) = 0 
lim [f (z)] = 0. 
%-I, [
I
 
-1 - 
5
'
1
 
-2 - 
Applying this to the z = 0 singularity of Equation 6.164 gives 
(6.165) 
(6.166) 
lim f(z) = lim ( 
z ) = lim (2) 
-+ 
-m. 
(6.167) 
2 4 0  - 
2-+0 
cosz - 1 
g+o 

THE RESIDUE THEOREM 
171 
Now let's determine the order of the pole. Start by multiplying the function by g 
and taking the limit: 
(6.168) 
To determine this limit, use 1'Hopital's rule: 
not once, but twice: 
Multiplying by z has removed the divergence, so the singularity at z = 0 is a pole of 
order one. 
6.10 THE RESIDUE THEOREM 
It could be argued that the most useful result of this chapter is the Residue Theorem. 
This theorem provides a powerful tool for calculating both complex contour integrals 
and real, definite integrals. 
6.10.1 
Consider an integral of %(I) on an arbitrary closed contour in the Z-plane: 
The Residue of a Single Pole 
Assume that y(z) has one singularity inside C, and that ~ ( g )  can be expanded in 
a Laurent series about a point G, such that the contour C is entirely within the 
convergence region, as shown in Figure 6.26. Notice that a Laurent series is necessary 
because &) 
is not analytic everywhere inside C. Consequently, the series can be 
substituted for ~ ( z )  
in the integral 
m 
Reversing the order of summation and integral gives 
(6.172) 
(6.173) 

172 
INTRODUCTION TO COMPLEX VARIABLES 
Figure 6.26 Contour and Convergence Annulus for Residue Theorem Integration 
Consider any one of the terms with n 2 0. Its contribution to I must be zero 
because the integrand is analytic everywhere: 
Now look at the n = -2 term. According to Cauchy’s Integral Formula, 
(6.174) 
(6.175) 
so the contribution of this term to I is also zero. This will be the case for all terms of 
the series with n 5 -2. For n = - 1, however, 
(6.176) 
where again, the last step follows from the Cauchy Integral Formula. 
the integral’s value is 
The c, 
coefficient is referred to as the residue at the singularity and, in general, 
dzy(g) = 2m‘ X the residue. 
(6.177) 
6.10.2 Residue Theorem for Integrals Surrounding Multiple Poles 
The extension of the above arguments to cover functions with several poles is straight- 
forward. Consider the complex function 
(6.178) 

THE RESIDUE THEOREM 
173 
, imag 
I 
c
c
 
real 
Figure 6.27 The Residue Theorem Applied to Multipoled Functions 
where f(z) is analytic in the entire g-plane. There are three poles at z = a, z - = b, and 
g = c. Imagine we are interested in evaluating an integral 
L = i d z w ( &  
(6.179) 
where C encloses two of the poles of ~ ( z ) ,  
as shown in Figure 6.27. Because the 
Cauchy Integral Theorem allows us to arbitrarily deform the contour through analytic 
regions, the integral can be broken up into the sum of two individual integrals, one 
around each pole: 
(6.180) 
as shown in Figure 6.28. Therefore, the value of the integral will be 2m' times the 
sum of the residues of ~ ( z )  
at g and b: 
= 2m"residue at Q + residue at b]. 
(6.18 1) 
d z f(z> 
.f 
c (z - a& - b)(z - c) 
The general form of the Residue Theorem for a function with an arbitrary number 
of poles is 
dzy(z) = 27ri 
Residues of poles inside C. 
(6.182) 
(7 8" 
real 
~- 
~~- 
Figure 6.28 Equivalent Multipole Contours 

174 
INTRODUCTION TO COMPLEX VARIABLES 
6.10.3 Residue Formulae 
Now that we have shown why residues are important, we need to develop some 
methods for calculating them. We showed earlier that the residue of a singularity is 
just the c-, term in a Laurent series expanded around the singularity itself. Thus, we 
can always determine the residue of a singularity once we know the Laurent series. 
Sometimes this is the only way possible, but in many cases we don’t have to do that 
much work. 
Imagine we already know that a singularity at g = 11 is a first-order pole. The 
Laurent series for such a pole, expanded about the pole, looks like this, 
(6.183) 
c- 1 
w(z) = - 
+ 5 + Cl(Z - UJ + & - d2 + * . ‘ ,  
(z - d 
- 
with no terms for n < - 1. Multiplying this series by (z - UJ gives 
(6.184) 
3 
- _  
w(z)(z 
- - UJ = c-1 + &(g - UJ + cl(z - UJ2 + c& - 4 + * * * . 
If then we take the limit as z 
--+ 11, we find that 
limk(& - 41 = c-1. 
(6.185) 
z-+g 
This gives a very simple method for determining the residue of a first-order pole. As 
an example, consider the function 
(6.186) 
which has first-order poles at both z = 
z = g: 
and z = b. Let’s calculate the residue at 
- 
(6.187) 
sing 
( 2 - U J  
=- 
1 
(g-bJ. 
limb(& - a)] = lim 
z-n 
This method can be extended for the residues of higher-order poles. The Laurent 
expansion of a second-order pole located at z = a, expanded around the pole, has the 
form 
(6.188) 
Multiplication by (Z - d2 gives 
Now we take the derivative 
(6.190) 

DEFINITE INTEGRALS AND CLOSURE 
175 
and finally take the limit as z + a: 
(6.191) 
This gives us a simple formula for calculating the residue of a second-order pole. For 
a pole of order n located at z = a, Equation 6.191 generalizes to 
(6.192) 
6.11 DEFINITE INTEGRALS AND CLOSURE 
The residue theorem can be used to evaluate certain types of real, definite integrals 
which appear frequently in physics and engineering problems. The method involves 
converting a real integral into a complex contour integral and then closing the con- 
tour. The residue theorem can then be used to evaluate the contour integral, and 
subsequently the original definite integral. 
Let x be a pure real variable, and consider an integral with the form 
(6.193) 
where w(x) is a real function of x .  Even though this integral involves quantities that 
are all pure real, it can be evaluated with x represented by a complex variable 2 by 
performing the integration along the real axis of the complex plane. That is 
dx w(x) = / dg w@, 
(6.194) 
where R is a straight-line path along the real axis from z = --oo to z = +m, as shown 
in Figure 6.29. To proceed, we need to define the function w(g) for values of z off 
the real axis. We can do this with a process called analytic continuation. Basically, 
this involves the determination of a complex function which, along the real axis, is 
the same as the original real function, and is analytic in as much of the complex 
plane as possible. In general, the continuation of a real function can be performed by 
z=1; 
R 
imag 
- 
z-plane 
real 
L - 
R 
Figure 6.29 Equivalent Path for Real Integral 

176 
INTRODUCTION TO COMPLEX VARIABLES 
1 
Figure 6.30 Semicircular Contour for Closure 
simply replacing all the x’s in a function with complex g’s. For example, the analytic 
continuation of w(x) = x2 would be 211(2) = g2. 
Because R is not a closed path, the residue theorem cannot be used to perform the 
complex integration of Equation 6.194. However, suppose 
where n is a semicircular path of infinite radius in the upper half g-plane, as shown in 
Figure 6.30. Because this additional section cannot change the value of the contour 
integral, we can just add it to the original complex integral to form a closed contour. 
That is, if the integral along 
is zero, then 
(6.196) 
The combination of the R and fl paths form the closed contour C shown in Figure 6.3 1. 
The original definite, real integral can now be written as a closed integral that can be 
evaluated by using the residue theorem: 
dx w(x) = f 
dgw(g) 
c 
= 2m’ 
Residues of ~ ( g )  
inside C. 
(6.197) 
~ imag 
Figure 631 Closed Contour for Definite Integral 

DEFINITE INTEGRALS AND CLOSURE 
177 
The technique of adding a section of contour to form a closed contour is referred 
to as closure. In the discussion above, closure was accomplished by adding a coun- 
terclockwise, infinite radius semicircular path in the upper half plane. Sometimes 
closure is accomplished by a similar semicircular path in the lower half plane. Clo- 
sure is most often performed with semicircles, but there are cases where rectangles 
or other shapes are necessary. 
6.11.1 
As mentioned, for Equation 6.197 to be valid, the integral along n must be zero. 
This integration can be viewed as a semicircular contour, as shown in Figure 6.30, in 
the limit of the radius approaching infinity. If we let r be the radius of the semicircle, 
then on n 
Conditions for Closure on a Semicircular Path 
where 0 < 8 < T for a semicircle in the upper complex plane. To move along fl, 
dg = ire"d8, 
(6.199) 
and the integral on n becomes 
(6.200) 
Thus to successfully use closure in the form described in the previous section, ~ ( z )  
must obey 
Using Equation 6.201 can be a little unwieldy, but a more usable condition can be 
derived by using the inequality 
(/dz&)I 
5 
Idzf(g)I. 
(6.202) 
which holds for any complex integral. Applying this to Equation 6.201, and noting 
leiel = 1 ,  gives 
5 7~ lim [ rWmx], 
(6.203) 
r-m 
where W,,, 
is the largest value of Iw(g)l on the interval 0 < 8 < T .  This gives us a 
useful result. If Wmx shrinks to zero faster than l / r ,  then the contribution of the f l  

178 
INTRODUCTION TO COMPLEX VARIABLES 
contour necessarily must vanish. Notice, it is not enough that I&)l -+ 0 as r ---$ m. 
Because of the extra factor of r. lw(g)I must go to zero faster than 1/r. 
Example 6.10 As an example of this closure technique, consider the real integral, 
(6.204) 
The value of the integral is equal to the area under the curve shown in Figure 6.32 and 
is clearly a positive, real quantity. It is easy to extend the integral into the complex 
plane 
(6.205) 
where the contour R is along the real _z-axis, as shown in Figure 6.29. Now consider 
the fl integral of Figure 6.30: 
On n, 
and 
(6.206) 
(6.207) 
(6.208) 
(6.209) 
Figure 632 Real Integral to Be Evaluated by Closure 

DEFINITE INTEGRALS AND CLOSURE 
179 
-1. 
Figure 6.33 The Closed Contour for the Integration of l/(x2 + 1) 
This falls off faster than 1 / r  for all values of 6, so in the limit as r -+ m, Z, = 0, and 
the original integral is equivalent to 
(6.210) 
where C is the closed contour shown in Figure 6.33. The two poles of the integrand 
at 2 i have also been indicated in this figure. From the residue theorem, the value of 
the original integral must be 
J m  dx 
- 
1 
- 2m 
Residues of- 
inside C .  
-= x2 + 1 
22 + 1 
Only the z = i pole is inside C, and a quick calculation shows the residul 
1/2i. Thus we obtain the final result 
dx 
- 
- 2n-i (&) = T. 
I 
x2 + 1 
(6.21 1) 
there is 
(6.212) 
This particular integral could also have been closed with a semicircle in cie lower 
half of the complex plane, as shown in Figure 6.34. In this figure, we see that now 
the z_ = - i  pole is enclosed and 
JE L 
- 
- -2m’(Residue at - i). 
-= x2 + 1 
(6.213) 

180 
INTRODUCTION TO COMPLEX VARIABLES 
+I pag - 
z-plane 
real 
I 
Figure 6.34 An Equivalent Closed Contour for the Integration of 1 /(x2 + 1) 
There is a minus sign out front because we enclosed the pole in a clockwise sense. 
The residue of this pole is - 1 /2i so the value of the integral becomes 
(6.214) 
which, fortunately, is the same as before. 
Not all real integrals can be closed in both the upper and lower half-plane, as 
this one could. Some functions diverge in one half-plane and thus force the closing 
contour to be drawn in the other. Some nasty functions diverge on both sides, and 
make evaluation with this type of contour impossible. 
6.11.2 Closure with Nonzero Contributions 
In some cases we can get away with closing a contour, even if the integral along the 
added segment is not zero. As before, we extend the real integral into the complex 
plane. When we evaluate the integral, however, we must subtract off the added 
contribution, 
where C is a closed contour, which includes both the path R along the real axis and 
the closing f l  path. 
Example 6.11 As an example, consider the definite integral 
,XI2 
1 +coshx' 
As usual, the conversion to a complex integral is simple: 
,x/2 
,212 
Smrdx 1 + coshx = Jdq 1 + coshg' 
(6.216) 
(6.217) 

DEFINITE INTEGRALS AND CLOSURE 
181 
If we try to close the contour with an infinite radius semicircle on either side of the 
real axis, we run into problems, because the complex integrand does not vanish as the 
radius approaches infinity. This can be seen most easily by evaluating the integrand 
along the imaginary axis, i.e., z = iy, where we find 
(6.2 1 8) 
Notice as y 3 ?=, the RHS of Equation 6.218 oscillates. Therefore, closing the 
contour with a semicircular loop as we did in the previous section will not work. 
By examining the poles of this function, you can see another big problem with using 
the infinite semicircular contour. A pole occurs every place where z = +(2n + 1)ir 
for all integer values of n. That means there are an infinite number of poles along the 
imaginary axis which would be enclosed by such a contour. 
A better method is to close the contour as shown in Figure 6.35, and then take 
the limit xo + m. This contour encloses only a single pole, the one at g = i r .  The 
integral around the contour C ,  in this limit, has four parts: 
For the right vertical segment, z = x, + iy and dg = idy, so 
x0+r2Tr 
&/2 
2?r 
lo d z l  +coshg 
imag 
+27c 
1 
I 
I 
I c  
i 
-x<, 
+xo 
Figure 6.35 Contour for Integration of ex/*/( 1 + cosh x )  
(6.219) 
(6.220) 

182 
INTRODUCTION TO COMPLEX VARIABLES 
As x,, --f +m, the exe term in the denominator dominates and forces this integral to 
zero. The same thing happens for left vertical segment. The integral along the top, 
horizontal segment (from x,, + i2m to -x,, + i2m) can be expressed in terms of the 
integral along the bottom segment: 
-x0+i27r 
ed2 
" 
1 + coshz 
(6.221) 
Consequently, 
r 
The right side of Equation 6.222 is just twice the real integral we started with, so 
(6.223) 
The contour C encloses the singularity at im, as shown in Figure 6.35. The residue 
of the integrand at i.rr can be determined by the techniques described earlier in this 
chapter. In this case, however, it may be simpler to look at the Taylor series for cosh z 
expanded about im. Using Equation 6.109, the first few terms of the expansion are 
1 
1 
2 -  
4! - 
coshg = - 1 - -(z - 
- -(z - i 7 ~ ) ~  
+ . . * . 
(6.224) 
This series converges to coshg in the entire 6-plane. From this Taylor series, you can 
infer that the complex integrand in Equation 6.223 has a second order pole at ir, 
because as we move arbitrarily close to this point, the denominator of the integrand 
is dominated by the (g - i
~
)
~
 
term in the series expansion. Equation 6.191 allows us 
to calculate the residue as 
Therefore, according to Equation 6.223, the value of the real integral is 
ex/2 
1 
= -(27T) = 7T. 
L d x  1 + coshx 
2 
(6.225) 
(6.226) 

DEFINITE INTEGRALS AND CLOSURE 
imag 
Poles of w(z) 
183 
imag 
imag 
imag 
Figure 6.36 Closure Results from Contour Deformation 
6.11.3 Another View of the Closure Process 
There is an insightful, alternative viewpoint one can take when looking at closure 
problems. Instead of adding a contour segment to close an integral, the original 
contour, which lies along the real axis, can be deformed as shown in the sequence of 
Figure 6.36. 
Consider a real integral 
m 
I = Lm 
dxw(x). 
(6.227) 
This integral converts to an equivalent complex integral along R, the real Z-axis, 
(6.228) 
as shown in the upper left-hand comer of Figure 6.36. Two poles of ~ ( g )  are also 
shown in this figure. The contour R can be deformed without changing the value of 
the integral as long as the deformation does not cross a nonanalytic point of &). 
Therefore the integrals on R’ and R”, shown in the upper right and lower left of 
Figure 6.36, are all equivalent to the original integral 
(6.229) 
We can now make the vertical line segments of R” overlap so they exactly cancel, as 
shown in the bottom right of Figure 6.36. The original integral can then be expressed 
as 
I = 1; 
dx w(x) = 
dz w(z) + 2m-i 
residues of poles crossed. 
(6.230) 

184 
INTRODUCTION TO COMPLEX VARIABLES 
If there are no other poles of w (5) then R”’ can be expanded to the infinite semicircle 
we used before. If the contribution along R”’ goes to zero as it is deformed to infinity, 
the result is the one found earlier in Equation 6.197. If, on the other hand, the 
contribution is nonzero, we must take that into account as we did in the previous 
section. 
6.11.4 The Principal Part of an Integral 
When the integrand of a real integral has a singularity for some value of x over the 
range of integration, the value of the integral is undefined. For instance the integrand 
of 
dx 
Lm 
2 
(6.231) 
behaves this way. However, since l/x3 is an odd function of x, it is tempting to claim 
that the value of this integral is zero, because the negative infinity on the left of x = 0 
is counteracted by the positive infinity on the right of x = 0. We can rescue our 
intuitive feel for this integral by defining the principal part of the integral: 
(6.232) 
In essence, the principal part takes the original integral and removes an infinitesimal 
region around the singularity. Evaluation of Equation 6.232 is straightforward: 
-1 
1 
2€2 
2€2 
+ -  
- 
- -  
= 0. 
(6.233) 
So, we in fact obtain the intuitive result 
PPs_rm$ 
= 0. 
(6.234) 
The principal part of many real integrals can be evaluated by closure and the 
residue theorem. Consider a real function w(n) that blows up at x = Q. The principal 
part of this integral is defined as 
dn w(x) = !% {/re 
dx w(x) + 
dn w(x)} . 
(6.235) 
a+€ 

DEFINITE INTEGRALS AND CLOSURE 
imag 
- 
z-plane 
PP 
real 
-- 
a-& 
a 
afE 
Figure 6.37 Principal Part Integration in the Z-Plane 
This is equivalent to an integral done in the complex g plane, 
185 
(6.236) 
where PP is a path of integration along the real z-axis from (-m, a - E )  and (a + E, m), 
in the limit of E --f 0, as shown in Figure 6.37. This integration can be handled with 
closure techniques by adding the standard infinite radius semicircular path in either 
the upper or lower complex plane, plus a semicircular path of radius E around the 
singularity at z = a. In other words, we write 
where U is the semicircular path of radius E ,  n is the semicircular path of infinite 
radius, and C is the closed contour made up of PP + U + n, as shown in Figure 6.38. 
The evaluation of the closed integral over the contour C is simply equal to 2m' 
times the sum of the residues of ~ ( z )  
inside C .  Most often, the contribution from fl 
is zero. The integral over U is a bit more tricky. If the singularity on the contour is a 
first-order pole, this added contribution is always half the residue of the pole. This is 
best demonstrated with an example. 
imag 
~ z-plane 
real 
Figure 6.38 Closed Contour for Evaluating the Principal Part 

186 
INTRODUCTION TO COMPLEX VARIABLES 
Example 6.12 
Let's evaluate 
1 
I-€ 
1 
pps_:dx (x2 + l)(x - 1) = !"o { L dx (x2 + I)(x - 1) 
. 
(6.238) 
1 
+ Jli, dx (x2 + l)(x - 1) 
Applying the above arguments, 
where the poles of the integrand and the various paths of integration 
Figure 6.39. 
(6.239) 
are shown in 
- 
The contribution from fl goes to zero because, for large lzl, the integrand falls off 
as 1 /z3. The closed integral on C is 
1 
= 2m 
residues inside C. 
(6.240) 
There are two enclosed poles, one at g = 1, with a residue of 1 /2, and the other at 
z = i, with a residue of (i - 1)/4. Therefore, 
imag 
- 
z-plane 
real 
Figure 639 Contours for PP Integration of I /(x2 + l)(x - 1) 

DEFINITE INTEGRALS AND CLOSURE 
187 
imag 
' 0  
- 
z-plane 
Figure 6.40 Expanded View of the U Integration 
The integration over the small semicircle U needs particular attention. Consider 
the expanded view shown in Figure 6.40 where we have drawn a phasor (g- 1) = Eei4. 
Along the path, dg = ie'@d+, and 
ranges from IT to 2 ~ .  
In the limit as E + 0, we 
have 
1 
lim - 
= 2, 
E'O 
22 + 1 
so the integral along U becomes 
irr 
2 
- 
- _  
The value of the principal part of the integral therefore becomes 
1 
1 
(x2 + l)(x - 1) 
PP 1; d.r 
IT 
-_ 
- 
- 
2 '  
(6.242) 
(6.243) 
(6.244) 
Note that the principal part in the example could have been evaluated with contours 
other than those shown in Figure 6.39. The semicircle of radius E could have been 

188 
INTRODUCTION TO COMPLEX VARIABLES 
taken above the singularity at g = 1, and/or the infinite radius contour could have 
been placed in the lower half of the 2-plane. The result obtained for the principal part 
of the integral is the same with any of these choices. 
6.11.5 Definite Integrals with sin 8 and cos 8 
Real integrals of the form 
r2a 
I = J, do w(sin 8, cos e), 
(6.245) 
where the integrand is a function of sin 8 and cos 8, can sometimes be converted to 
complex integrals along a circular contour of unit radius, as shown in Figure 6.41. 
On this contour, g = eie and 6 ranges from 0 to 2v. Therefore, on C we can write 
(6.246) 
and 
sine = L(eie - e-ie) = -(z 
1 - 1/g) 
1 
ie 
1 
cos e = -(e + e+’) = 2(g + 1/g). 
Substituting these quantities into Equation 6.245 allows the integral to be converted 
to one in the complex plane: 
(6.247) 
2i 
2i - 
2 
Example 6.13 As an example of this technique consider the real integral 
d6 
2“ 
I = I  
2 + C O S 6 ’  
Applying Equations 6.246 and 6.247, this integration becomes 
(6.249) 
(6.250) 
where C is the counterclockwise contour that lies along the unit circle shown in 
Figure 6.41. The integrand has two first-order poles at g = -2 * &. Only the pole 
at -2 + & is inside the contour, and it has a residue of 1/(2&), 
so the value of 

CONFORMAL MAPPING 
189 
, imag 
I 
- 
z-plane 
Figure 6.41 Contour for Real sin, cos Integrals 
the integral becomes 
= 2 m ( s )  
(6.251) 
(6.252) 
6.12 CONFORMAL MAPPING 
Conformal Mapping is a technique for finding solutions to Laplace’s equation in 
two dimensions. These solutions are of interest in fluid mechanics and electro- and 
magnetostatics. 
The mapping technique uses a complex function, such as w = I&), 
to take points 
from the Z-plane and “map” them onto points in the w-plane. If 4210 = y ( ~ ) ,  
then y(z) 
is said to map the point 
on to the point %, as shown in Figure 6.42. In this figure, 
we have continued to use the convention that the real and imaginary parts of z are 
given by x and y, and the real and imaginary parts of w by u and u. If one point in the 
~ 
I imag 
w-plane 
z-plane 
imag 
Yo I 
0 2, 
I 
v o  
real 
-* W” 
real 
Figure 6.42 
A Mapping of z, to % by w = w(z) 

190 
INTRODUCTION TO COMPLEX VARIABLES 
g-plane maps to only one point in the w-plane, the mapping function &) is said to be 
single valued. It is possible for the mapping function to take one point in the Z-plane 
to more than one point in the w-plane. This type of function is called multivalued. 
Mapping is a two-way street. The function ~ ( g )  
can be inverted to give a function 
of w, i.e., g o ,  that takes points from the w-plane and maps them back to points in 
the z-plane. If both ~ ( g )  
and ZW are single valued, the mapping between the two 
complex planes is called ‘‘oneto one:’ 
6.12.1 Mapping of Grid Lines 
Many times it is easier to discuss mapping properties with the use of a specific 
mapping function. For the purposes of this discussion, consider the mapping function 
w-plane 
N 
I1 
a 
2 
- 
w = g .  
- 
z-plane 
(6.253) 
v = 2  
, v  
v = l  
~ 
I 
v = -2 
Figure 6.43 Mapping of E-Plane Grid Lines onto z-Plane by w = g2 

CONFORMAL MAPPING 
191 
-plane 
X 
Figure 6.44 Perpendicular Intersection of Grid Line Mapping 
How do the grid lines in the w-plane, the lines of constant u and constant u, map onto 
the Z-plane? Substituting 441 = u + iv and = x + iy into this mapping function gives 
u(x, y )  = x2 - y2 
v ( x , y )  = 2xy. 
(6.254) 
Equations 6.254 implies that lines of constant u and u represent two sets of nested 
hyperbola, as shown in Figure 6.43. These hyperbola are plotted on the same graph 
in Figure 6.44. It can be seen that the hyperbola appear to intersect at right angles. 
Indeed, we will show that this is the case and is a general property associated with 
all analytic mapping functions. Notice that the mapping is not one to one, because a 
single grid line of the w-plane maps into two lines in the Z-plane. 
6.12.2 Conformal Maps 
A mapping function is said to be conformal if it preserves angles. This means that 
a conformal mapping function will map two lines that intersect at some angle into 
another pair of lines that intersect at the same angle. This is depicted in Figure 6.45, 
u 
1 
w-plane 
/ 1'" 
z-plane 
IY 
u 
w-plane 
I 
X 
V 
- 
Figure 6.45 
The Angle Preserving Feature of Conformal Mapping 

192 
INTRODUCTION TO COMPLEX VARIABLES 
V 
- 
X 
Figure 6.46 Analytic Functions and Conformal Mapping 
where the intersection point z, maps into the intersection point w-. 
In both planes, the 
angle of intersection is ao. 
Notice that the lines are not necessarily straight and so the 
intersection angle, in the case of curved lines, must be defined by the local tangents. 
The w = g2 mapping function just discussed appears to be conformal because the 
perpendicular grid lines of the g-plane map into hyperbola in the z-plane that intersect 
at right angles. 
All analytic mapping functions are conformal. To demonstrate this, let a mapping 
function take the point z, to the point w+,, and consider a nearby point z = ~0 + Ag that 
maps to the point g = w
-
 
+ Aw, in the limit Az -+ 0. This is shown in Figure 6.46. 
A line that passes through the two points z, and g maps into a line that passes through 
the points w- 
and g. If Ag = AzeiU and AE = Awe'P, we can write 
(6.255) 
But if the mapping function is analytic, we know the derivative at this point does not 
depend on the way Az approaches zero. Thus the right side of Equation 6.255 must 
be the same for any choice of a in the z-plane. This means that as a is changed, p 
must also change in such a way that 
p - a = c,, 
(6.256) 
where c, is a constant. All infinitesimal lines passing through zo will be rotated 
through the same angle as they are mapped onto the E-plane. Therefore, all complex 
functions which are analytic are conformal mapping functions. 
6.12.3 Solutions to Laplace's Equation 
Conformal mapping functions provide a method for solving Laplace's equation in 
two dimensions. In a two-dimensional Cartesian geometry, Laplace's equation is 
V2@(x, y) = (2 
+ $) 
@(x, y) = 0. 
If a mapping function w = ~ ( g )  
with 
(6.257) 
g = x + i y  
(6.258) 

CONFORMAL MAPPING 
and 
193 
- 
w = u(x, y )  + iv(x, y )  
(6.259) 
is analytic, the functions u(x, y) and v(x, y )  must satisfy the Cauchy-Riemann con- 
ditions 
(6.260) 
and these partial derivatives must all be continuous. Consequently, the second deriva- 
tives obey 
This says that 
(5 + $) 
u(x, y )  = V2u(x, y )  = 0. 
Equations 6.260 can be manipulated to give the same result for v ( x ,  y): 
($ + $) v ( x ,  y )  = V 2 v ( x ,  y )  = 0. 
(6.261) 
(6.262) 
(6.263) 
Therefore, if a mapping function is analytic, the functions u(x, y )  and v ( x ,  y) each 
satisfy a two-dimensional Laplace equation. Either one could be interpreted, for 
example, as an electrostatic potential or as the pressure in a fluid flow problem. 
Example 6.14 In electrostatics, the electric potential satisfies Laplace’s equation in 
a charge-free region. Conformal mapping functions can therefore be used to generate 
solutions to two-dimensional electrostatic problems. Consider the function 
(6.264) 
- 
w = z .  
As discussed above, the real part of I
&
)
 
is a solution to Laplace’s equation and can 
be identified with an electrostatic potential function @(x, y ) .  In this case, 
2 
@(x, y )  = u(x, y )  = x2 - y2. 
(6.265) 
The two-dimensional surfaces of constant potential are given by 
x2 - y2 = Constant. 
(6.266) 

194 
INTRODUCTION TO COMPLEX VARIABLES 
For example, the equipotential Q, = 5 surface obeys 
x2 - y2 = 5, 
and @ = 0 on the surface 
x2 - y2 = 0. 
This last surface is written more simply as 
x = 2y. 
(6.267) 
(6.268) 
(6.269) 
Both these surfaces have been shown in Figure 6.47. This means that if plates were 
placed on the x = ? y  and x2 - y 2  = 5 surfaces and held at 
= 0 and 
= 5 
respectively, the electric potential in the space between these plates would be given 
by 
@ = x2 - y2. 
(6.270) 
This is shown in Figure 6.48, where we have drawn the equipotential surfaces between 
the plates for @ = 1,2,3 and 4. 
The electric field lines between these plates can also be determined by the mapping 
function. The relationship between the electric potential and the electric field is 
- 
E = -V@. 
(6.271) 
Because Q, = u(x, y ) ,  
(6.272) 
Since field lines always run tangent to the field, if y = y(x) is a field line, then 
(6.273) 
Figure 6.47 
Boundary Conditions for the Electrostatic Potential Between Two Sheets 

CONFORMAL MAPPING 
195 
Figure 6.48 Equiptentid Lines Between the Two Sheets 
But this mapping function is analytic, and so its real and imaginary parts satisfy the 
Cauchy-Riemann conditions. Consequently, 
Equation 6.274 can be rearranged to give 
(6.274) 
(6.27 5) 
The left side of Equation 6.275 is just the total differential of v ( x , y ) ,  so the field 
lines are simply lines of constant v(x, y ) .  For this particular problem, the field lines 
are given by v(x, y) = 2xy and are shown in Figure 6.49. 
We could also interpret this solution as the laminar flow of a fluid, guided by the 
surfaces u = 0 and u = 5. In this case, the lines of constant u would be the flow 
lines, while the lines of constant v would be lines of constant pressure. 
Any conformal mapping function will generate solutions to Laplace’s equation. 
One approach to solving Laplace’s equation in various geometries would be to gen- 
erate a large table of mapping functions, with a corresponding list of boundary 
geometries, and then, much like using an integral table, find the solution to any given 
problem by looking it up. This is cumbersome, at best. A more direct way to anive at 
the proper mapping function, given the boundary geometry, is presented in the next 
section. 

1% 
INTRODUCTION TO COMPLEX VARIABLES 
Figure 6.49 Electric Field Lines Between the Two Sheets 
6.12.4 
Schwartz-Christoffel Mapping Functions 
The previously outlined method for obtaining solutions to Laplace's equation using 
conformal mapping techniques approached the problem from the wrong direction. 
The mapping function, with solutions to Laplace's equation given by its real and 
imaginary parts, was specified, and then the boundary conditions associated with 
the solutions were identified. These types of problems are usually posed the other 
way around. The Schwartz-Christoffel method uses conformal mapping techniques 
to design a mapping function from a particular set of boundary conditions. 
To see how this method works, begin by considering mapping function w = w(iJ 
and its inversion z = g,@ 
such that 
(6.276) 
where w, and k, are pure real numbers, and A is a complex number. We then ask, 
given the functional form imposed by Equation 6.276, how does the v = 0 line map 
onto the Z-plane? In other words, as y moves from --a, to +-a, along the real w-axis, 
as shown in Figure 6.50, what sort of line is traced out in the z-plane? The quantity 
imag 
y-plane 
- 
real 
I 
--t----------- 
~~ . 
w 
wo 
Figure 650 Real w Mapping 

CONFORMAL MAPPING 
197 
imag 
w-plane 
e = X  
Figure 6.51 Phase of @ - w,) for w to the Left of w, 
dz/dw, on the LHS of Equation 6.276, is complex and, in polar notation, has both a 
magnitude and a phase. Given the form of Equation 6.276, the phase of dg/dw is 
dz 
d w  
L-= = LA_ - k,L@ - w,), 
(6.277) 
where the operator L generates the phase of a complex number. Now consider the 
phase of d z / d w  as y moves from --M to +a0 along the real w-axis. For a pure real 
- 
w, with w to the left of w,, Figure 6.51 shows that L@ - w,) = 7 ~ .  
For pure real 441, 
with y to the right of w,, Figure 6.52 shows that L@ - w,) = 0. Therefore, for w 
on the real axis: 
(6.278) 
L A  - k,r 
for 111 to the left of w, 
for w to the right of w, ' 
As y moves along the real w-axis the mapping function will generate a line in 
the ?-plane. As w is incremented by an amount Aw, z will change by an amount 
Ag = ( d z / d y ) A y .  The phases of these Aw and Ag steps are related by 
As w moves to the right along the real axis, L Aw = 0 so that 
imag 
w-plane 
e=o 
real 
* 
wo 
w 
(w - wo) 
(6.279) 
(6.280) 
Figure 6.52 Phase of (w - w,) for 
to the Right of w, 

198 
INTRODUCTION TO COMPLEX VARIABLES 
Figure 6.53 The Mapping of the Real Axis from w, to +a onto the g-Plane 
Let the point w, map into 5. Then as moves away from w, toward fa, in steps 
of A y ,  moves away from z, in steps of Ag. The result of the mapping of this part 
of the real y-axis is shown in Figure 6.53. The series of Aw’s, all at an angle of 
zero and all of the same length, are mapped into a series of Ag’s. The Ag’s are not 
necessarily the same length but, by Equations 6.278 and 6.280, they all have the 
same phase, LA. The mapping of the real y-axis from --c4 to w, can be done in the 
same way, but now all the Az’s are at an angle equal to L A  minus kow, as shown 
in Figure 6.54. Consequently, the mapping function that obeys Equation 6.276 maps 
the real y-axis into a line on the g-plane made up of two straight-line segments, as 
shown in Figure 6.55. There is a break point in this line at 5, where the slope of the 
line changes by k,w. 
These ideas can be extended to a mapping function whose derivative has the form 
Figure 6.54 The Mapping of the Real Axis from ---a, to w, onto the g-Plane 
imag 
z-plane 
, 
Z O  
real 
Figure 6.55 Mapping of the Real W-Axis onto the 5-Plane 

CONFORMAL MAPPING 
199 
imag 
v = o  
I 
- 
z-plane 
Figure 6.56 The General Form of a Schwartz-Chnstoffel Mapping 
where wl, w2, w 3  . . . w, are successively increasing real numbers, and kl, k2, k3, 
* . . k,, are arbitrary real numbers. The mapping function that has this form for its 
derivative maps the real F-axis into connected straight-line segments in the z-plane, 
as shown in Figure 6.56. In this way, the derivative of the mapping function can 
be set up so that the real y-axis can be mapped into an arbitrary shape made up of 
straight-line segments. To obtain the mapping function y(g), Equation 6.281 must be 
integrated 
(6.282) 
and then inverted. 
The real F-axis can map into a set of straight-line segments that may close to form 
a polygon in the finite Z-plane, or they may extend to infinity. A variety of boundary 
shapes can be constructed. Some of the more complicated geometries arise from 
allowing w,, the last point along the real y-axis, to be located at -too. In this case, 
the (W - w,)-" factor in Equation 6.281 is eliminated, because it can never change 
the phase of dz/dw. Even though this point does not enter Equation 6.281, it can 
map into a point y, where there is a k,w angle change. In addition, the Schwartz- 
Christoffel method can be applied along any horizontal line in the y-plane. In this 
way the value of the function on the boundary can assume more than one value. 
Example 6.15 
In this example, we determine the electric potential and field around 
a sharp, two-dimensional spike. The spike and ground plane are held at zero volts, 
while the potential goes to infinity at an infinite distance above the ground plane. 
This geometry is shown in Figure 6.57. Specifically, @(x, 0) = 0 for all x, and also 
X 
Figure 6.57 
Grounded Spike Geometry 

200 
INTRODUCTION TO COMPLEX VARIABLES 
Figure 6.58 Mapping of the Real g-Axis for the Spike Problem 
@(O,y) = 0 for 0 < y < 1. We will try to find a solution for 
in the y > 0 half 
plane. 
To use the Schwartz-Christoffel mapping for this problem, the real w-axis must be 
mapped onto this complicated shape. The line in the Z-plane can be set up with three 
break points, as shown in Figure 6.58. These three break points, zl, g2, and g3, are the 
mapping of the points wl,w2, and w3, which all lie on the real w-axis, as shown in 
Figure 6.59. At this point, the exact positions of w1, w:! and w3 are arbitrary, except 
for the condition that w1 < w:! < w3. As w moves along the real w-axis, z moves 
along the path of Figure 6.58, first encountering the break point at zl, then the one 
at gz, and finally the one at g3. At the z, breakpoint, there is a ?r/2 counterclockwise 
bend, and so kl = 1/2. At the g2 breakpoint, there is a ?r clockwise bend, and so 
k2 = - 1. Finally, at the z3 breakpoint, there is another ?r/2 counterclockwise bend, 
and so k3 = 1 /2. The equation for dz/dw can therefore be written as 
(6.283) 
At this point, a bit of trial and error is necessary to figure out the constants 4, wl, 
w2, and w3. We must select these constants so the mapping function z = z&) has 
~ ( w I )  
= 0, g(w2) = i, and g(w3) = 0. Of course, we cannot check our guess until 
imag 
w-plane 
Figure 6.59 Mapping of Break Points onto Real y-axis 

CONFORMAL MAPPING 
201 
after Equation 6.283 is integrated. From the symmetry in the z-plane, 
- 
it makes sense 
to make w2 = 0 and require w3 = - W I .  As a trial set of constants, let w 1  = - 1 ,  
w2 = 0, and w3 = 1 to give 
(6.284) 
The constant 4 is just a scaling and rotating factor, which can initially be set equal to 
1 and changed later if necessary. Equation 6.284 can then be rewritten in the form 
which can be reorganized as the indefinite integral 
(6.285) 
(6.286) 
Ignoring the arbitrary constant of integration, the solution of this integral is 
g = d-. 
(6.287) 
Equation 6.287 can be inverted to give 
w = 1/22 + 1 .  
(6.288) 
Now this mapping must be checked to see if the constants were chosen properly. 
Whenw = w, = -1,g = g1 = 0. Whenw = w2 = 0,g = g2 = 6 
= 2i.When 
- 
w = w3 = + 1, g = z3 = 0. So it looks like all the constants are correct. Notice when 
- 
w = w2 = 0, two values of z2 are possible. The one that is correct for the geometry 
of this problem is g2 = + i .  The second value simply corresponds to a solution in the 
lower half of the g-plane. 
A check of Equation 6.288 will show that it is analytic everywhere in the Z-plane, 
except at the points - 
z = +i. Consequently, this mapping function will generate 
a solution to Laplace's equation everywhere, except at those two isolated points. 
To obtain the solution, the functions u(x,y) and v ( x , y )  must be determined. To 
accomplish this, rewrite Equation 6.288 as 
(u + ivl2 = (x + iy12 + 1. 
(6.289) 
Expand this equation, and separate its real and imaginary parts to get 
and 
2uv = 2xy. 
(6.291) 

202 
INTRODUCTION TO COMPLEX VARIABLES 
Y 
v = @ =  1.0 
Figure 6.60 Equipotentials above the Grounded Spike 
Solving Equation 6.291 for u and substituting the result into Equation 6.290 gives 
x2y2 - v 4  - x 2 v 2  + y2v2 - v 2  = 0. 
(6.292) 
This equation can be used to generate lines of constant u in the z-plane, wluch we 
interpret as lines of constant potential. The CP = u = 0 line is just the ground plane 
and spike of Figure 6.57. Several other, equally spaced, equipotential lines obtained 
from Equation 6.292 are shown in Figure 6.60. A computer is useful in making these 
The electric field lines are determined by setting u(n, y) = constant. The equation 
plots. 
for these lines is obtained by eliminating u from Equations 6.290 and 6.291: 
u4 - .zy2 - x*u2 + y2u2 - u2 = 0. 
(6.293) 
A plot of these field lines, for uniform steps of u between 0 and 1.4 is shown in 
Figure 6.61. Remember that in field line drawings such as Figure 6.61, the electric 
field is not necessarily constant along a field line. The intensity is proportional to the 
density of the field lines. So in Figure 6.61 it can be seen that the electric field is 
very strong near the tip of the spike, weakest at the bottom of the spike, and becomes 
constant far from the spike. 
Figure 6.61 Electric Field Lines above the Grounded Spike 

EXERCISES 
203 
EXERCISES FOR CHAPTER 6 
1. Use Euler's formula to show that 
cos(n0) + i sin(n0) = (cos 0 + i sin 0)". 
This result is called DeMoivre's formula. 
2. Find all the zeros of these complex functions: 
i. sing. 
iii. sinhz. 
iv. coshz. 
ii. cosz. 
3. Make a sketch in the complex g-plane of the following relationships: 
i. (g - 5)(z* -- 5) = 4. 
ii. (z - Si)(g* + 5 i )  = 4. 
iii. - 
z2 + (g21* = 0. 
4. Show that natural logarithms exist for real, negative numbers in the complex 
5. Determine all the values of i' and plot them in the complex plane. As a first step, 
6. Find the real and imaginary parts of the following complex functions: 
plane. In particular, find the value of In (- 10). 
write i in polar form. 
i. ~ ( z )  
= ec. 
ii. ~ ( g )  
= lnz. 
iii. y ( g )  = sing 
iv. w(z) = l/z. 
vi. ~ ( z )  
= In (g2 + I). 
vii. ~ ( 2 )  
= el';. 
viii. ~ ( z )  
= 1/cosg. 
ix. w(g) = z/z*. 
x. w(z) = z* Inz. 
v. w(z> = I/<? 
In what region of the complex plane are each of these functions analytic? 
7. Find an analytic complex function that has a real part of u(x, y) = x3 - 3xy2. 
8. Using the polar forms for z and IV, 

204 
INTRODUCTION TO COMPLEX VARIABLES 
a complex functional relationship between 411 and z can be written as 
- _  
W(Z) = R(r, 0) eiecr,@. 
The functions R(r, 6) and O(r, 0) are real functions of the real variables r and 8. 
These functions are the polar analogues of u(n, y) and v(x, y). If &) is analytic, 
show that the Cauchy Riemann conditions become 
and 
9. Show by direct integration that 
where C is any counterclockwise loop that encloses G, and n is an integer. 
10. Develop the Taylor series and idenw the region of convergence for the following 
functions: 
i. g(z) = z expanded about z = 2. 
ii. g(g) = cos g expanded around z = 0. 
iii. g(z) = cos g expanded around z = 7r/2. 
iv. ~ ( z )  = ln(z + 1) expanded around g = 0. 
v. g(z) = d- 
expanded about g = 0. (up to terms of 2) 
11. Develop the Laurent series and identify the region of convergence for the fol- 
lowing functions: 
i. ~ ( z )  
= e''Z 
ii. ~ ( z )  
= l/(z - 1) expanded about g = 2. 
iii. g(z) = ~ 
iv. g(2) = - 
expanded about z = 0. 
expanded about z = 2. 
expanded about g = 1. 
z' 
(z - a2 
Z2 
(z - 2>2 
12. Find the Taylor and Laurent series expansions around z = 0 and indicate the 
regions of convergence for the functions: 

EXERCISES 
205 
et 
iii. ~ ( z )  
= __ 
g2 - 1’ 
13. Use the Cauchy integral approach to determine the Taylor series expansion and 
region of convergence of the following functions around an arbitrary point 5: 
(a) ~ ( z )  
= z. 
(b) w(z) = g2. 
(4 w(z) = z - a. 
(4 
w(z> = z2/Q - @. 
(a) Find the Taylor series expansion about z = 1 + i for 
14. Using the expansion in Equation 6.100, 
(b) Find the Laurent series expansion about the same point for the same function. 
15. Using the Cauchy Integration approach, and the complex function 
(a) Determine the Taylor series expansion for ~ ( z )  
about z_ = 0 and indicate the 
(b) Determine the Laurent series expansion for ~ ( z )  
about z = 1 and indicate 
region of convergence. 
the region of convergence. 
16. Suppose we need to expand the function 
with a series that is valid on the closed contour C ,  a unit circle centered at g = 1, 
as shown below: 
imag 
- 
z-plane 
(a) First consider the Taylor series expansion for ~ ( z )  
about g = G, 
m 

206 
INTRODUCTION TO COMPLEX VARIABLES 
Identify the region of the z-plane where 5 can be placed so that this series 
will be a valid representati6n of the function on the contour C and determine 
the cn. 
(b) Now consider the Laurent series expansion for ~ ( g )  
about z = z,, 
Identify the region of the g-plane where z, can be placed so that this series 
will be a valid representation of the function on the contour C and determine 
the gn. 
17. Consider the complex function 
1 
1 
w(z) = - 
+ - 
2 - 3  
2 - 4 '  
- _  
(a) Find the Taylor series expansion for ~ ( 2 )  
about 2 = 1 and identify its region 
(b) Find the Laurent series expansion for ~ ( g )  
about z = 0, which converges in 
(c) Find the Laurent series expansion for ~ ( z )  
about z = 1, which converges in 
(d) Compare the g- coefficients in the series solutions to parts (b) and (c) above 
(e) Find the Laurent expansion for ~ ( g )  
about z = 1, which converges in the 
(f) How many different Taylor and Laurent series expansions for ~ ( z )  
can be 
of convergence. 
the annulus 3 < lgl < 4. 
the annulus 2 < 12 - 11 < 3. 
and comment. 
region 3 < I? - 11. 
made about the point 2 = 3.5? 
18. Consider the function 
(a) Find the function that generates the coefficients of the Taylor series expansion 
for this function about the general point z = 5. Indicate the region of 
convergence for this series. 
(b) Find the function that generates the coefficients of the Laurent series expan- 
sion for this function about the general point g = 5. Indicate the region of 
convergence for this series. 
19. Consider the complex function 

EXERCISES 
207 
20. 
21. 
22. 
23. 
(a) Find the Laurent series for ~ ( g )  
expanded about the point z = 1 and identify 
(b) Find the Taylor series for ~ ( g )  
expanded about the point z = 1 and identify 
(c) Find the Laurent series for I&) expanded about the point z = 0 and identify 
(d) Why is there no part (d) to this problem? 
the region of convergence. 
the region of convergence. 
the region of convergence. 
Verify that 
by comparing the Taylor series expansions for sinh z and ex. Comment on the 
convergence. 
Prove that a Laurent expansion of a function about a particular point is unique. 
That is, show if 
m 
m 
n=-n. 
n=-n, 
then gn must equal & for ail n. Use the Cauchy integral formula. 
Consider the function 
(a) Locate all the poles of &) 
in the -plane and identify their order. 
(b) Find the Taylor series expansion for ~ ( g )  
about z = 0. Identify the first three 
coefficients of this series and indicate the region of convergence. 
(c) How many different Laurent series for ~ ( z )  
can be constructed about z = O? 
(d) Find a series expansion around z = 0, either Taylor or Laurent, that is a valid 
representation of ~ ( z )  
at = r .  Indicate the region of convergence for this 
series. 
Consider the function 
(a) Where in the complex Z-plane are the poles of &)? 
(b) Determine the first three terms for the Taylor series expansion of I
&
)
 
about 
(c) Identify the region of convergence for the Taylor series of part (b). 
(d) Determine the general expression for the nrh coefficient of the Taylor series 
2 = 0. 
- 
expansion of part (b). 

208 
INTRODUCTION TO COMPLEX VARIABLES 
(e) There is a Laurent series expansion for I&) 
about z = 0 in the region 
1 < lzl 
- < 2 that has the form 
m 
n=-m 
Obtain the general expression for the &. 
24. Consider the function 
(a) Can this function be expanded in a Taylor series about g = O? 
(b) Find two different Laurent series expansions for this function, both expanded 
about g = 0. Indicate their regions of convergence. 
(c) Find the Taylor series for this function expanded about z = 1/2. Indicate 
the region of convergence. 
(d) Find the Laurent series for this function expanded about z = 1/2. Indicate 
the region of convergence. 
(e) What is the residue of this function at z = O? at z = l? at z = 1/2? 
(f) What is the value of the integral 
where C is a counterclockwise circle centered at the origin of radius 1/2? 
of radius 1.5? 
(g) What is the value of the integral in part (f) above if C is a clockwise circle 
centered at z = 1 of radius 0.5? of radius 1.5? 
25. Identify the locations and orders of the poles for the following functions. For 
each of the functions evaluate the residue associated with each of its poles. 
Z2 
g2 + 1’ 
i. ~ ( g )  = - 
Z 
iii. w(z) = e. 
sin z 
In g 
iv. y(z) = - 
g’ - 1’ 

EXERCISES 
209 
26. Each of the following functions has a pole at z = 1. In each case identify the 
order of the pole and determine its residue. 
1 
22 - 22 + 1 
1 
ii. ___ 
- 2 -  
1 ’  
1. --. 
- 
- 
I 
1 
iii. - 
In g 
1 - cos (z - 1) 
(z - 113 
. 
iv. 
v. cot(g - 1). 
27. Let C be a counterclockwise, circular contour of radius 2 centered at z = 0. 
Evaluate the complex integrals: 
(n = an integer). 
28. Evaluate the integral 
29. 
30. 
(a) where C is the clockwise, circular contour given by IzI = 2. 
(b) where C is the clockwise, circular contour given by lgI = 4. 
Evaluate the complex integral 
where C is the counterclockwise, circular contour given by lz - + ~ / 2 )  
= 1 
Evaluate the complex integral 
where C is the counterclockwise, circular contour lg - 11 = 2. 

210 
INTRODUCTION TO COMPLEX VARIABLES 
31. Evaluate the complex integral 
where C is the clockwise, circular contour lz - 24 = 1. 
32. Evaluate the complex integral 
where C is the counterclockwise, circular contour lgl = 1. 
33. Consider the complex integral 
sing 
A d z  (g - . I r / N z _  + .Ir/2) 
and the two contours C = C1 and C = C2 shown below. 
imag 
- 
z-plane 
Let Z, be the integral around C1 and 12 the integral around C2. Show by contour 
deformation that 12 - 11 = -2i. 
34. Consider the function 
(a) Find all the poles of I&) 
and identify their order. 
(b) Evaluate the integral 
where C is a counterclockwise, circular contour given by lzl = 1. 

EXERCISES 
211 
(c) Evaluate the integral of part (b) above if C is a counterclockwise, circular 
contour given by I? - T I  = 1. 
35. Consider the function 
Find all the poles of ~ ( z )  
and identify their order. 
Evaluate the integral 
where C is a counterclockwise, circular contour given by IzI 
- = 1. 
Evaluate the integral of part (b) above if C is a counterclockwise, circular 
contour given by lz - 274 = 1. 
36. Consider the integral 
Locate the poles of the integrand and identify their order. 
Evaluate the integral if C is a counterclockwise, circle of radius 1 centered 
at 
i. z 
~ = 0. 
ii. g = 27r. 
iii. z = 2m'. 
37. Consider the integral 
where n is a positive integer, and C is the counterclockwise circle lgl = 1. 
(a) Find the Laurent series expansion about z = 0 for the integrand. 
(b) Use the residue theorem to evaluate the integral. 
38. Convert each of the following integrals to an equivalent one in the complex 
plane. Identify all the poles of the complex integrands and their residues. Close 
the contours and evaluate the integrals. 
cos x 
x sin ?I 
ii. 1% 
dx ___ 
x2 + 1 '  

212 
INTRODUCTION TO COMPLEX VARIABLES 
x2 cos (2x) 
ex 
v. [Idx cosh (m) . 
sinh(ax) 
sinh ( m) 
--.rr<a<.rr. 
vi. 1 dx 
39. Convert each of the following real integrals into a complex integral and evaluate 
it using the residue theorem: 
1 
is l n d e  1 + sin2 6‘ 
1 
cos e 
cos (38) 
211 
iii. 1 d 0 5 + 4 ~ o s 0 ’  
40. Show that the integral 
where x is a pure real variable, can be evaluated by doing an integration in the 
- 
z-plane around the closed contour shown in the figure below. 
Identify the similar closed contour in the z-plane that can be used to evaluate 
the integral 
where m is any positive integer. 

EXERCISES 
213 
41. Consider the integral 
1 
m 
P P L r n k  (9 
+ l)(x + 1)’ 
where we must use the principal part because of the behavior of the integrand 
at x = - I. Convert this into an integral in the complex plane and determine an 
appropriate contour of integration to evaluate this integral. Show that you obtain 
the same answer regardless of which side of the 5 = - 1 pole you go around. 
42. Consider the integral 
- ixu 
= p 
- 9  
where x and u are real variables. 
(a) Without performing the integration, show that Z(u) is a pure real function 
(b) Convert this integral into the complex plane and evaluate it using closure 
of u. 
and the residue theorem. 
(c) Plot Z(u) vs. u. 
(d) The function Z(u) can be converted to a complex function of a complex 
for u, i.e., L(u) --+ Z(4li). Is 
variable by substituting the complex variable 
Lw analytic? 
43. Use complex integral techniques to evaluate the integral 
sin (kx) 
L d k 7 - 9  
where k and x are both real variables. 
44. Discuss the problems with trying to close the following integral in the complex 
plane: 
45. Consider the semicircular contour shown below in the limit as E -+ 0. 
- €  
, imag 
E 
real 
On this contour, evaluate 

214 
INTRODUCTION TO COMPLEX VARIABLES 
i. l d g - .  
1 
ii. L d z -  1 
iii. L d z -  1 
z 
z-i' 
- 
z(g - i)' 
46. Use the results of the previous problem to evaluate 
1 
where C is the closed contour shown below in the limit E -+ 0 and R, -+ w. 
real 
Based on this result, what is the principal part of the integral 
where x is a real variable? 
47. Consider the function 
(a) Evaluate the integral 
f 
dzw(z), 
C 
where C is the counterclockwise, circular contour given by lzl = 1 
(b) Evaluate the principal part of the integral 
PPl:dx,. cos x 

EXERCISES 
215 
18. How does the function y = g2 map the unit circle centered at the origin of the 
49. Using the mapping function 
z-plane onto the y-plane? 
(a) how does the unit circle 1441 - 1 I = 1 map onto the g-plane? 
(b) how does the line y = 1/2 map onto the z-plane? 
and u in the y-plane get mapped onto the Z-plane: 
50. For each of the mapping functions below, indicate how the lines of constant u 
i. ~ ( z )  
= ec. 
iii. ~ ( g )  
= lnz - 1. 
the origin of the E-plane for the following mapping functions? 
51. What part of the z-plane corresponds to the interior of the unit circle centered at 
2 - 1  
i. y(z) = = 
Z
f
 1' 
z - i  
ii. w(z) = = 
- 
z+i' 
52. Consider the mapping function 
(a) Is this mapping function analytic? 
(b) Show how this function maps the grid lines of the Z-plane onto the y-plane. 
Specifically map, on a labeled drawing, the grid lines shown below. 

216 
INTRODUCTION TO COMPLEX VARIABLES 
(c) Is this mapping function conformal? Can it be used to generate solutions to 
Laplace’s equation? 
53. Consider the mapping function 
(a) Where does the imaginary axis of the y-plane map onto the g-plane? 
(b) Where do the points in the right half of the y-plane map onto the z-plane? 
(c) Where do the points in the left half of the w-plane map onto the g-plane? 
54. The mapping function 
22 + 1 
- 
w(z> = - 
z 
can be used to analyze two-dimensional flow around a cylindrical obstacle. 
Uniform stream lines described in the w-plane by lines of constant u become, 
upon mapping onto the g-plane, the flow lines around a cylinder. The constant 
pressure lines for the flow around the cylinder are given by mapping the lines of 
constant u onto the Z-plane. 
(a) Plotthelinesv = -3,-2,-1,0,+1,+2,and 
f3ontotheg-plane. 
(b) Plot the lines of constant u onto the z-plane. 
(c) The velocity field for the flow is proportional to the gradient of the pressure 
distribution function, i.e., 
- 
v = -pVP(x,y), 
where p is a positive constant called the mobility, and P(x, 
y) is the pressure 
distribution function. What is the velocity field, V(x, y), for the flow around 
the cylinder? 
55. Use the Schwartz-Christoffel method to determine the functional relationship of 
d z / d y  that takes the real y-axis to the step in the g plane shown below. Integrate 
&d invert this expression to obtain the mapping function w = ~ ( g ) .  Make an 
attempt to plot the lines of constant u onto the g-plane. If you have access to 
computer plotting software, you should be able to make very accurate maps of 
the constant v lines. 
1 
w-plane 
-1 
1 
lU 
Y 
- 
z-plant 

EXERCISES 
217 
56. Use the Schwartz-Chnstoffel conformal mapping technique to find the stream 
function for flow over a surface that is initially flat and then rises at 45', as shown 
below. 
V 
Y 
w-plane 
z-plane 
- 
U 
X 
(a) Find the mapping function that takes the real y-axis into the line in the 
2-piane made up of the negative real axis and a segment out to infinity at 
(b) Find the function that takes stream lines in the w-plane to stream lines in the 
z-plane. NOTE: The last couple of lines of algebra in this calculation get a 
bit messy, but the answer is obtainable. 
57. Recall, the mapping function generated by the Schwartz-Christoffel method is 
0 = T/4. 
given by the expression 
d z  
- 
= A(w - w,)-kI(W - w*)-k'(y - Wg)-k' . . . . 
d w  
(a) How does the real g-axis map onto the Z-plane if: A = 
w1 = -1, 
kl = 1 /2, wz = 0, k2 = - 1, w3 = 1, and k3 = 1/2? Draw a sketch of this 
mapping. 
(b) How would your answer to part (a) change if A = 1, w 1  = - 1, w2 = 1/2, 
and w3 = 2, while all the k values stayed the same? 
58. Using the Schwartz-Christoffel method, design a mapping function that takes the 
real g-axis to a combination of the negative real z-axis and the positive imaginary 
z-axis with w = 0 mapping into z = 0. 
V 
y-plane 
Y 
~ z-plane 
X 
U 
I 
Using this mapping function, map the 45" lines shown below in the z-plane onto 
the y-plane. 

218 
INTRODUCTION TO COMPLEX VARIABLES 
- 
z-plane 
X 
-1 
59. Show that if 
then 

7 
FOURIER SERIES 
This chapter consists of four sections. In the first section, we present a quick refresher 
of Fourier series methods and a few examples. The second section demonstrates the 
utility of the complex, exponential form of the Fourier series. These two sections 
should provide a framework for the subsequent chapters on Fourier and Laplace 
transforms. The third section briefly discusses the convergence properties of Fourier 
series and introduces the concept of completeness. In the final section, we discuss the 
discrete Fourier series, which is a numerical technique for obtaining Fourier spectra. 
7.1 THE SINE-COSINE SERIES 
A general Fourier series expansion is the sum of a potentially infinite number of sine 
and cosine terms: 
m 
a: 
s ( t )  = 2 + 
a,coswnt + 
b,sino,t. 
n=1,2, ... 
n=1,2,.. 
2 
(7.1) 
Each term oscillates at an frequency on = 2 n / T 0 ,  an integer multiple of the 
fundamental frequency oo = 27r/TO. The value of To is determined either by the 
periodicity of the function the series is supposed to represent, or a periodicity we force 
the series expansion to take. The form of Equation 7.1 guarantees that the function 
generated by the series has a periodicity of To. In other words, it obeys the condition 
s(t) = s(t + To) 
for all t. 
(7.2) 
An example of such a function is shown in Figure 7.1. If the independent variable is 
time, then s(t) has gone on for all past time and must continue forever. 
219 

220 
FOURIER SERIES 
Figure 7.1 Periodic Function with Basic Period To 
In a typical Fourier series problem, we are given a particular function f ( t ) ,  and 
we want to determine both To and the a,, and b,, coefficients, so the series is equal to 
f ( t ) ,  for all t. For this to be possible, f ( t )  itself must be periodic and exist for all t, 
i.e., 
f ( t )  = f(t + To) 
for all t. 
(7.3) 
The smallest value of the constant To that satisfies Equation 7.3 is defined as the 
period of f ( t ) .  
If f ( t )  is not periodic, a Fourier series cannot be used to represent f ( t )  for all t. 
We can, however, make the series equal to f ( t )  over some finite interval. Consider 
the nonperiodic f ( t )  shown in Figure 7.2, and the interval tl < t < t2. We can define 
the basic period for the Fourier series to be To = t2 - t1, and the generated series 
can be made identical to f ( t )  over this interval, as shown in Figure 7.3. Outside the 
interval, the Fourier series s(t) is periodic and will not match the function f ( t ) .  
Figure 7.2 A Nonperiodic Function 
._ 
-_ 
Figure 7.3 Fourier Series Fit to a Nonperiodic Function 

THE SINE-COSINE SERIES 
221 
7.1.1 The Orthogonality Conditions 
To determine the coefficients an and b,,, we must first derive a set of equations called 
the orthogonality conditions. These relations involve integrals of products between 
the sine and cosine functions, i.e., (sin)(cos), (sin)(sin), and (cos)(cos). 
The derivation starts by considering what happens when either the sine or cosine 
functions are integrated over an integer multiple of a period. For any integer n # 0, 
it is obvious that 
[ + T o  dt sin (gt) 
= s,"'"dt 
cos (Ft) 
= 0. 
(7.4) 
The integrals in Equation 7.4, and the ones that follow, are valid for any to. For n = 0 
the first of the above integrals is still zero. However, the n = 0 integral for the cosine 
function takes on the special value 
b +To 
dt cos(0) = To 
.I 
(7.5) 
The first orthogonality condition involves an integral of the product of sine and 
cosine functions. Because 
sin(a + p) + sin(a - p) 
2 
(7.6) 
sin (Y cos p = 
Equation 7.4 implies 
dt sin (gt) 
cos (Ft) 
= 0 
(7.7) 
for all integer values of n and rn including zero. The other orthogonality conditions, 
for integrals involving the products of the sine function with other sine functions and 
the cosine function with other cosine functions, are obtained using 
cos(a - p) - cos(a + p) 
2 
sin a sin p = 
cos(a - p) + cos(a + p) 
(7.8) 
2 
cos a cos p = 
With Equations 7.7 and Equation 7.5, we can write for n > 0 and rn > 0 
and 
2 m  
2 m  
T O  
dt cos ( r t )  
cos ( r t )  
= S,,,,. 
I, +I"<, 
(7.10) 

222 
FOURIER SERIES 
7.1.2 Evaluating the Coefficients 
The orthogonality conditions allow us to isolate and evaluate the individual coeffi- 
cients of the Fourier series. If we want a series to equal f (t), we write 
m 
m 
f ( t )  = 3 
+ 
ancosont + 
b, sinwnt. 
n = 1,2,.. . 
n= 1.2. ... 
(7.1 1) 
The a, coefficient is obtained from 
1 
m 
m 
b+TO 
1 
dt f ( t )  = s,"'". [a + 
ancosont + 
bnsinqt . 
(7.12) 
n=1,2, ... 
n= 1.2, ... 
Using Equation 7.4, we can see that the only term in the expansion that does not 
integrate to zero is the a,/2 term. Thus 
or 
(7.13) 
(7.14) 
The remaining a, coefficients can be obtained in a similar manner. Operating 
on Equation 7.11 by an integral weighted by cos %t, where rn > 0, and using the 
orthogonality conditions gives 
m 
(7.15) 
T O  
to + To 
To - 
dt cos(w,t)f(t) = 
an&,,- - amT. 
2 
n= 1.2, ... 
n u s ,  the a," coefficient can be evaluated using the expression 
a, = $ s,"'" dt cos(%t) f (t). 
(7.16) 
Notice that Equation 7.16 is also valid for rn = 0. This was the reason the factor of 
2 was used in the a, term of Equation 7.11. With that convention, the a, term does 
not need to be treated differently from the other a, terms. The bn coefficients are 
obtained in exactly the same manner, except the integrals are performed with sin o,t. 
The orthogonality equations give 
m 
T O  
T O  
bnam-- = b m y  
dt sin(w,t)f(t) = 
2 
n= 1,Z. ... 
with the result that 
b m = $['" 
dt sin(o,t) f (t). 
(7.17) 
(7.18) 

THE SINE-COSINE SERIES 
223 
7.1.3 The Fourier Series Circuit 
There is an interesting way to view the summation of terms in a Fourier series 
which we will borrow from the field of electronics. We can model each term in 
the summation by a signal generator, which puts out a voltage signal at the proper 
frequency and amplitude for that term. This is shown in Figure 7.4. The a, term is 
I 
I 
Figure 7.4 Fourier Sine-Cosine Series Circuit 

224 
FOURIER SERIES 
a d.c. source of a0/2 volts. The bl term is a sine wave oscillating at frequency 01, 
with a signal amplitude of bl volts. The summation is accomplished by joining all 
the signals into one output from the circuit. This is the basis for what the electrical 
engineers call the frequency spectrum of a signal. Conceptually, they often break 
a signal into its various Fourier series components, analyze how each component 
behaves individually, and then join the results back together to generate their final 
solution. 
We will return to this analogy in more detail when we discuss Fourier and Laplace 
transforms. 
Example 7.1 As a very simple first example, consider the “expansion” of the func- 
tion, f(t) = sint. The function is obviously periodic, with To = 27r. The Fourier 
series takes the form 
m 
m 
sin t = 5 + 
a, cos(nt) + 
bn sin(nt). 
(7.19) 
n = l  
n= 1 
2 
We could struggle through the integral equations of the previous section to pick out 
the values of the a, and b,, coefficients. In this particular case, however, it is easy to 
see that the only nonzero coefficient is bl , which has a value of 1. 
The set of coefficients in the Fourier series is sometimes referred to as the spectrum 
off (t). The spectrum for f ( t )  = sin t is shown in Figure 7.5, where we have plotted 
the b, coefficients versus n. In this case, there is only one nonzero point on the graph. 
, 
n 
-*-*-- 
L .  -
~
-
 
-- 
~ 
1 
2 
3 
4 
5 
6 
Figure 7.5 The Spectrum for f ( t )  = sin t 
- 
~ 
~ 
~ 
Example 7.2 As a second example, let f ( t )  be the periodic triangular waveform 
shown in Figure 7.6. Notice that this function has the same amplitude and period 
as the previously discussed sine wave. On the interval -7r/2 < t < 37r/2, this 
triangular wave can be expressed as 
(7.20) 
As before, the basic period of this signal is To = 27r, so o, = n. Because this is an 
odd function oft, we can immediately see that all the a, coefficients are zero. The b, 

THE SINE-COSINE SERIES 
225 
Figure 7.6 The Triangle Wave 
coefficients can be calculated from 
3 n / 2  
b,= 11 dt fT(t) sin(nt) 
IT 
- n / 2  
sin(nt), 
(7.21) 
with the result that 
8 
. nrr 
b, = 2sin-. 
r r n  
2 
(7.22) 
A plot of the spectrum for the triangle wave is shown in Figure 7.7. 
Figure 7.7 The Spectrum for the Triangle Wave 
Example 7.3 As a final example, let’s calculate the Fourier series for the square 
wave function fs(t), shown in Figure 7.8. If this function has unit amplitude and a 
period of To = 27r, it can be expressed as 
(7.23) 
Again, this is an odd function of t and so all the a, coefficients are zero. The b, 
coefficients can be evaluated from Equation 7.18: 

226 
FOURIER SERIES 
with the result 
I 
Figure 7.8 The Square Wave 
2a 
b, = A / dt f~(t)sin(nt) 
T
o
 
a 
n = 1,3,5,7,. 
b,,={i;" 
n = 2,4,6,8,. 
(7.24) 
(7.25) 
The spectrum for the square wave is shown in Figure 7.9. 
By comparing Figures 7.5, 7.7, and 7.9, you will notice that as the waveform 
changes from a sine wave, to a triangular wave, and finally to a square wave, the 
relative amplitudes of the higher frequency components increase. 
The convergence of the Fourier series to the square wave function is demonstrated 
in Figure 7.10. Three plots are shown. In the fist, just the n = 1 term of the series is 
shown. The middle plot shows the result of the sum of the n = 1 and n = 3 terms. 
The last plot on the right is the sum of the n = 1, n = 3, and n = 5 terms. Notice 
that, while the square wave itself is not well defined at its discontinuities, its Fourier 
n 
I 
- -- 
' 
- 
1 
2 
3 
4 
5 
6 
Figure 7.9 The Spectrum for the Square Wave 

THE EXPONENTIAL FORM OF FOURIER SERIES 
227 
n = l  
i 
n =1 
i 
n =1 
Figure 7.10 The Convergence of the Fourier Series to a Square Wave 
series converges to a value equal to the average of the step at these points. There is 
another interesting convergence effect going on at this discontinuity called the Gibbs 
phenomenon, which will be discussed later in this chapter. 
7.2 THE EXPONENTIAL FORM OF FOURIER SERIES 
It turns out that representing the Fourier series with sines and cosines is not very con- 
venient. This is because the sine and cosine terms need to be manipulated separately. 
There is an alternative representation of the Fourier series, the complex exponential 
form, which combines the two sets of terms into one series. We can convert the 
Fourier series of Equation 7.1 into exponential form using the identities 
(7.26) 
1 
cos e = - (eie + e P )  
2 
8'). 
(7.27) 
1 
sine = - 
(,ie - 
2i 
7.2.1 The Exponential Series 
The exponential form of the Fourier series follows directly from the sine-cosine form 
m 
m 
s(r> = 7 + C a, cos(w,,t> + C b, sin(w,t). 
(7.28) 
I n=l 
,= 1 
Using Equations 7.26 and 7.27, we can write 
a, cos(o,t) + b, sin(w,t) = 5 (eiwmf + 
+ !!!! 
(ei*' - e-iwm') . 
(7.29) 
2 
2i 
The RHS of this expression can be rearranged to give 
a,COS(w,t) 
+ b, sin(w,t) = (7 + a) eiwm' + (T - $) e-iwm'. 
(7.30) 

228 
Equation 7.28 can then be placed in the form 
m 
n=-m 
where 
FOURIER SERIES 
(7.31) 
(7.32) 
Notice that if all the a,, and b, coefficients are real, Equations 7.32 imply h = c?,. 
In that case, the exponential form can also be written 
m 
s(t) = co + 2 Real 
&eion'. 
(7.33) 
n= 1 
7.2.2 
Orthogonality Conditions for Exponential Terms 
The & coefficients can always be obtained by determining the a, and bn coefficients 
using the techniques already described. Better yet, they can be determined directly 
using a new orthogonality condition: 
lo +To 
&lm T O .  
(7.34) 
dt eionfe-i"t 
= 
1 
This condition follows directly from the definition of the exponential function and 
the previous sine and cosine orthogonality conditions. It is true for any to, n, and m. 
The period To of f ( t )  is still related to the fundamental frequency by To = 27r/w0. 
If a function f ( t )  is to be expanded in an exponential Fourier series 
m 
the gn coefficients can be determined by using the orthogonality condition in Equa- 
tion 7.34. To do this, multiply both sides of Equation 7.35 by e-'%' and integrate 
over the period, To. This gives 
Exchanging the summation and integration on the RHS of this expression and using 
the orthogonality condition gives: 
= &To. 
(7.37) 

THE EXPONENTIAL FORM OF FOURIER SERIES 
Therefore, we can evaluate the G coefficient using the expression 
= s,"'" dt f(f)e-'".', 
229 
(7.38) 
which works for all the en's, including c+,. 
7.2.3 Properties of the Exponential Series 
There are three important properties of the exponential form of the Fourier series that 
can simplify calculations. The first deals with the G term of the expansion. Because 
(7.39) 
c+, is just the average value of f ( t )  over the period. This is often written as 
If f(t) is a real function, 5 must be a pure real number. 
function, then f( a) = f( - a). and 
The even or odd nature of f ( t )  places conditions on the cn. If f ( t )  is a real, even 
(7.41) 
If a change of variables is introduced, with a = -f and d a  = -dt, this equation 
becomes 
0 
= -L J d a  f(a)e'"n". 
'
0
 
-To 
The integrand f(a)eiwna is periodic in a with a period of To. Therefore, 
Now take the complex conjugate of both sides of this equation to get 
1 
To 
c; = T, 
d a  f*(a)e-iona. 
(7.42) 
(7.43) 
(7.44) 
If f ( t )  is a real function oft, then f * ( t )  = f ( t ) ,  and we obtain the result 
* 
(7.45) 
Therefore, if f ( t )  is a real, even function off, then the coefficients of its exponential 
Fourier series are all pure real. The Fourier series expressed with sines and cosines 
- 
C n  - Cn. 

230 
FOURIER SERIES 
actually has an analogous symmetry. A real, even function expressed in that form 
will not contain any sine terms. Similar arguments show that if f ( t )  is a real, odd 
function, then 
= -&, and the coefficients are pure imaginary. The analogous 
statement for the Fourier series expressed in terms of sines and cosines is that any 
real, odd function will not contain any cosine terms. If f ( t )  possesses neither odd nor 
even symmetry, then the coefficients of the exponential series are complex. In this 
case the sindcosine series is made up of both sine and cosine functions. 
7.2.4 
Convenience of the Exponential Form 
A major advantage of the exponential form of the Fourier series becomes evident 
when it is used with differential equations. If sines and cosines are used instead of the 
exponential notation, you must keep track of two sets of series, which can become 
horribly intertwined when derivatives are taken. With the exponential notation, all 
the terms are treated on equal footing, so the bookkeeping is much easier. 
Consider a nonhomogeneous, linear differential equation with constant coeffi- 
cients: 
(A$ 
+ B z  "
1
 
+ C f(t) = d(t), 
(7.46) 
where d(t) is a known function, and the problem is to find f ( t ) .  If d(t) is periodic, it 
can be represented by a Fourier series 
m 
d(t) = 
&eiont, 
(7.47) 
n=-m 
with 
(7.48) 
The coefficients Ll-, and the frequencies w,, are both known quantities. The unknown 
function f(t) can be set equal to its Fourier series 
OD 
f ( t >  = C F-,eiwnt. 
(7.49) 
n=-m 
Here, the F- 
are unknown, but the wn are the same frequencies used in Equation 7.48, 
which were determined by the periodicity of d(t). This will always be the case for 
linear differential equations. For nonlinear equations, f (t) can contain frequencies 
which are not in d (t). 
Substituting Equations 7.47 and 7.48 into the differential equation converts it into 
an algebraic equation, because all the dependencies on the independent variable t 
cancel out: 
( - w i A + i w n B + C ) &  =a. 
(7.50) 

CONVERGENCE OF FOURIER SERIES 
231 
Equation 7.50 can be solved for the unknown coefficients F- 
and used to express the 
solution in terms of a Fourier series: 
(7.5 1) 
L =  -4 
Ao: - iw,B - C 
(7.52) 
If sines and cosines had been used to expand the Fourier series for d(t) and f ( t ) ,  
the time dependence could not have been removed as easily. It might be a worthwhile 
exercise to try this for yourself. The Fourier series approach outlined in Equations 
7.49-7.52 will be described in more detail in Chapter 10, when we discuss solution 
methods for differential equations. 
7.3 CONVERGENCE OF FOURIER SERIES 
Up to this point we have assumed that a periodic f ( t )  could always be expressed with 
an exponential series 
with c, coefficients given by 
(7.54) 
Actually, the equivalence of f ( t )  and the series on the RHS of Equation 7.53 was just 
an assertion that we must justify by demonstrating that the Fourier series converges 
to the function f(t). In fact, this will not be possible for all types of functions. 
This section presents the conditions necessary for the convergence of Fourier series 
and introduces some new terminology. The proofs of the statements in this section 
are too involved for this short chapter. We present only a few important results here 
without proving them. A complete and rigorous reference for this material is the book 
Fourier Series by Georgi Tolstov. 
7.3.1 Qpes of Convergence 
In general, to show convergence, one starts with a function, f ( t ) ,  and calculates 
the Fourier coefficients 
using Equation 7.54. This step actually places a first, 
obvious condition on what types of functions can be represented by a Fourier series. 
It must be possible to perform the integration of Equation 7.54. Next, a version of the 
Fourier series with a finite number of terms is constructed using these coefficients. 

232 
FOURIER SERIES 
Convergence is proved by showing as the number of terms increases, the series 
gets arbitrarily close, by some measure, to f(t). There are several different types of 
convergence. We will discuss three in this section: Uniform, Pointwise, and Mean- 
Squared convergence. 
Pointwise convergence is simple to understand. Define the partial Fourier series 
s N ( t )  as 
n=+N 
(7.55) 
n=-N 
This sequence of functions is said to converge pointwise to the function f ( t )  in the 
rangea 5 t 5 bif 
lim [&(t) - f(t)] = 0 
(7.56) 
N-m 
for any given point t in that range. 
The same series is said to converge to f ( t )  in a mean-squared sense if 
(7.57) 
This is a less strict condition. Notice the sequence &(t) = tN converges to the 
function f ( t )  = 0 over the range 0 5 t 5 1 in the mean-squared sense, but not in a 
pointwise sense, because of the behavior of the function at t = 1. 
Uniform convergence is the most strict convergence of the three. The sequence 
&(t) converges uniformiy to the function f ( t )  on the range a 5 t 5 b if 
where max gives the largest value on the range. 
7.3.2 Uniform Convergence for Continuous Functions 
We state the following theorem without proof. If a periodic function is continuous 
and piecewise smooth, its Fourier series will always converge uniformly everywhere. 
A function is piecewise smooth if its derivative exists everywhere, except at isolated 
points. The triangle wave shown in Figure 7.6 is continuous and piecewise smooth, 
but the square wave of Figure 7.8 is not. 
7.3.3 Mean-Squared Convergence for Discontinuous Functions 
If a function is discontinuous, the Fourier series does not converge uniformly, but it 
may still converge in a mean-squared sense. A good example of a function with a 
discontinuity is the square wave function we discussed in an earlier example: 

CONVERGENCE OF FOURIER SERIES 
233 
(7.59) 
The sum of the first three terms of the Fourier series was shown progressively in 
Figure 7.10. Figure 7.1 1 shows the transition region for the summation of N = 40, 
60, and 200 terms of the partial Fourier series. The are two complications that arise 
with the convergence of the Fourier seces at points of discontinuity. The first problem 
is fairly obvious, while the second problem is quite subtle. 
The first problem is evident when you consider the point t = 0. According to 
the definition of f ( t )  in Equation 7S9, f(0) = 1. Clearly, looking at Figures 7.10 
and 7.11 shows the Fourier series does not converge to this value, even as N ---$. m, 
but rather to the average value of the step. In general, if a periodic function has a 
discontinuity where the function jumps from f, to f2, its Fourier series will converge 
to a value of 
f l  + f 2  
2 
(7.60) 
at the discontinuity. 
The second complication of the convergence at the discontinuity is less obvious. 
Again consider the summation of the lirst three terms of the Fourier series for the 
square wave, shown in Figure 7.10. As N is increased from 1 to 2 and then to 3, the 
partial sum S N ( ~ )  
looks more and more like a square wave, but there consistently is 
an oscillation before and after the rising step. You might expect this oscillation to 
shrink in size as the number of terms in the sum increases, but this does not happen. 
In Figure 7.1 1, notice the overshoot recovers faster as N increases, but its amplitude 
remains constant at a value around 1.17. 
This overshoot of the Fourier series at a discontinuity is referred to as the Gibbs 
phenomenon. In the region surrounding the discontinuity, the series is converging 
point by point, but not uniformly. Given any point f = to, except exactly the point of 
discontinuity, we can increase N until &(to) - f (to) is arbitrarily close to zero. We 
cannot, however, decrease I&(t) - f(t)l- 
below the value of 1.17 by increasing N. 
This leads us to a more general form of the convergence theorem of the previous 
section. If a function f (t) is piecewise smooth and piecewise continuous, the Fourier 
Figure 7.11 The Gibbs Phenomenon 

234 
FOURIER SERIES 
series converges pointwise to the value of f ( t )  at all points of continuity, and to the 
mean value of the step of any points of discontinuity. As a whole, the entire series 
converges to f ( t )  uniformly if the function is continuous, but only in a mean-squared 
sense if it has discontinuities. 
7.3.4 Completeness 
Mathematicians often like to phrase these convergence theorems using the idea of 
completeness. A set of functions is said to form a complete set over a functional class 
if a linear combination of them can express any of the functions in that class. The 
complete functions are often called basisfunctions, because they span the space of 
possible functions, much like the geometric basis vectors span all the possible vectors 
in space. 
Do the exponential terms of the Fourier series form a complete set? In the discus- 
sion above, we have said that they form a complete set over the class of piecewise 
smooth, continuous functions if we require uniform convergence. The theorem in the 
previous section shows they also form a complete set over the class of piecewise con- 
tinuous, piecewise smooth functions if we only require mean-squared convergence. 
Notice we must always declare what kind of convergence is being used before we 
can say whether a collection of basis functions is complete. 
7.4 THE DISCRETE FOURIER SERIES 
The coefficients of the Fourier series, either the a, and b, of Equation 7.1 or the 
gn of Equation 7.31, describe the frequency spectrum of a signal. Often, in physical 
experiments, there is an electrical signal which we would like to analyze in the 
frequency domain. Before the advent of the modem, digital era, electrical spectra 
were obtained using expensive analog spectrum analyzers. In simplified terms, these 
used an array of narrow-band, analog filters which split incoming signals into different 
frequency “bins” which were then reported on the output of the device. Modern 
frequency analyzers obtain their spectra by digitally recording the incoming signal at 
discrete time intervals, and then using a computer algorithm called the Fast Fourier 
Trunsfomz (FIT). 
When a computer is used to perform the Fourier series analysis of a signal, it cannot 
exactly carry out the operations of the previous sections. Because a computer cannot 
deal with either continuous functions or infinite series, integrals must be replaced 
by discrete sums, and all summations must be taken over a finite number of terms. 
The following sections describe a simplified version of the FIT, which adjusts our 
previous methods in an appropriate manner for computer calculations. 
7.4.1 Development of the Discrete Series Equations 
The first step in the development of a discrete Fourier series for f ( t )  is to determine 
To, the period of the signal. Next the computer takes 2N samples of f ( t ) ,  spread 

THE DISCRETE FOURIER SERIES 
235 
evenly over the period, to generate the values 
(7.61) 
as shown in Figure 7.12. The first sample fo is taken at to = 0, and the last sample 
f z N - l  is taken at t z ~ - l  = T0(2N - 1)/(2N). 
The standard exponential Fourier series is summarized by the pair of equations: 
TOk 
f k  = f ( t k )  
tk = - 
2N 
fork = 0, 1,2,. . .2N - 1, 
m 
1 
To 
= T, 1 dt f(t)e-i"n', 
(7.62) 
(7.63) 
where w, = 2 n / T 0 .  These equations need to be modified for the computer. The 
new versions of Equations 7.62 should generate the sampled values f k  from a new 
set of cn, via some kind of finite sum: 
(7.64) 
n=-m 
n=O 
In Equation 7.64, on is defined, as it was before, as on = (2nn)/T0, and n is an integer 
which we will arbitrarily start at zero and let run to a currently undefined, upper limit 
n-. 
The continuous variable t has been replaced by tk, the set of discretely sampled 
times. 
To evaluate the c, of Equation 7.64, an orthogonality condition similar to Equa- 
tion 7.34 is needed. We can try to construct one by converting the integral over t into 
a sum over all the tk values: 
Notice that the summation over the discrete time index k, unlike the frequency index 
n, has a well-defined upper limit determined by the number of sample points. The 

236 
FOURIER SERIES 
question now is, does the sum over k in Equation 7.65 lead to an orthogonality 
condition that allows a determination of the ~
'
s
 
in Equation 7.64? Answering this 
question will take some work. 
Equation 7.65 can be rewritten as 
2N-1 
Ur-1 
(7.66) 
k=O 
k=O 
where we have substituted on = 2m/T0. Defining 
eiR(n-m) 
(7.67) 
the sum in the RHS of Equation 7.66 can be recognized as a geometric series, which 
can be expressed in closed form as 
1 -IUr 
1 - r  
2N-1 
C r k = = .  
k=O 
The summation in Equation 7.66 can therefore be written as 
(7.68) 
(7.69) 
Because n and m are both integers, the numerator of Equation 7.69 is always zero. 
The LHS of this equation is therefore zero unless the denominator is also zero. This 
occurs when 
n - m  = 0,+2N,24N ,.... 
(7.70) 
From our original definition, the integers n and m range from zero to the unspecified 
limit n,, 
of the sum in Equation 7.64: 
(7.71) 
If we limit the sum in Equation 7.64 to UV terms, that is n,, 
= 2N - 1, we can force 
In - ml 5 2N - 1. 
(7.72) 
This important result makes the denominator of Equation 7.69 zero only when n = m. 
This is exactly what we need to construct the orthogonality condition. All that remains 
is to evaluate Equation 7.69 for n = m. We can easily accomplish this by returning 
to Equation 7.66 and substituting (n - m) = 0. Our final orthogonality condition 
becomes 
2h- 1 
(7.73) 

237 
THE DISCRETE FOURIER SERIES 
This orthogonality condition can now be used to invert Equation 7.64 and eval- 
uate the c, for the discrete Fourier series. To do this, we multiply both sides of 
Equation 7.64 by e-''+Jk and sum over k: 
2N-1 
2N- 1 
m- I 
k=O 
k=O 
n=O 
ur-1 
ur-1 
Using the orthogonality condition gives 
2N- 1 
2N-1 
e-iomgfk = 
~,6,,,,,2N, 
(7.74) 
k=O 
n=O 
with the result that 
(7.75) 
Thus the original equation pair which defines the Fourier series can, for the discrete 
Fourier series, be replaced by: 
m 
2N- 1 
(7.76) 
n = - m  
n=O 
7.4.2 Matrix Formulation 
Equations 7.76 and 7.77 are easily converted to a matrix format, which in turn, is 
easily handled by a computer. If we define the elements of the matrix [MI as 
(7.78) 
M~~ = eimnrk = e i $ n k  
Equation 7.76 can be written using subscript notation as 
f k  = GMlk. 
Likewise, Equation 7.77 becomes 
(7.79) 
1 
Cn - -MLfk. 
2N 
- 
(7.80) 

FOURIER SERIES 
238 
The orthogonality condition, Equation 7.73, is simply 
M&ML = 6,,,,,2N. 
(7.81) 
Keep in mind, the subscripts of all these expressions run from 0 to 2N - 1. 
Notice that the [MI ma& is the same for all functions, it only depends upon the 
number of sample points. The computer obtains the coefficients of the discrete Fourier 
series by inserting the sampled values of f(t) into Equation 7.80 and performing the 
summations. 
Example 7.4 
Consider a very simple signal, 
f ( t )  = cost. 
(7.82) 
The standard exponential Fourier series for this signal gives a spectrum with only 
two nonzero components, c1 = _c- I = 1 /2. These components occur at w = & 1, 
the natural frequency of the signal. A discrete Fourier series can be constructed for 
this signal by sampling it four times over its 2m period, as shown in Figure 7.13. 
Table 7.1 shows the sampled values and sampled times. 
Any signal sampled four times in a period has W = 4 and an [MI matrix given 
by 
Mnk = 
(7.83) 
I f(t) =cost 
I 
I , 
I 
8 
I 
I 
I 
- ____ 
I 
Figure 7.13 The cos I Function Sampled Four Times 
TABLE 7.1. Sample Times and Values of cost 
k 
tk 
.h 
0 
.rr/2 
7r 
3 1r/2 
1 
0 
-1 
0 

THE DISCRETE FOURIER SERIES 
239 
which in matrix array notation is 
Using Equation 7.80, we can evaluate the c,, 
1
1
 
with the result that 
(7.84) 
(7.85) 
(7.86) 
A spectral plot of these coefficients versus w, = n is shown in Figure 7.14. 
Parts of this spectrum make sense, and other parts do not. The fact that 
and c, 
are zero, while c1 is nonzero, agrees with the regular Fourier series spectrum. But 
the component at 0 3  = 3 makes no sense, because there obviously is no w = 3 
component in cos t. This suspicious extra component is referred to as an alias, an 
effect we will discuss in detail in the next section. 
-cn 
1 I2 
0 
0 
n = on 
* -  - -.-
~ 
0 
1 
2 
3 
Figure 7.14 Discrete Fourier Senes Specbum for cos t Sampled Four Times 

240 
FOURIER SERIES 
Now let’s see how the coefficients give us back the sampled values of the function. 
Equation 7.79 written for this example becomes 
(7.87) 
n=O 
n=O 
Inserting the values for & and noting that tk = kTo/(2N) = kv/2, 
The RHS of Equation 7.88 sums to the proper, real value of cos t at the sample times 
t = 0, v/2 , m, and 3v/2. At other vdues o f t  # tk, the ms of Equation 7.88 
does not equal f ( t )  and is not even pure real! In order for Equation 7.76 to be a good 
representation of the original f ( t ) ,  it seems we must sample the signal many times. 
We will quantify this statement in the following sections and in the next chapter. 
7.4.3 Aliasing 
In the previous section, the discrete Fourier series technique was applied to the 
function f ( t )  = cos t and it was sampled four times. The resulting spectrum, shown 
in Figure 7.14, had a legitimate component at w = 1 and an erroneous one at w = 3. 
We called the w = 3 component an alias. What is the source of this alias, and what 
parts of a discrete spectrum are to be trusted? 
The alias arises from the simple fact that when four sampling points are used, the 
two functions cos t and cos 3t give exactly the same values at the sampling points 
(0, v/2, v. 3v/2,2~). We can remove the alias at w = 3 by sampling more times per 
period. If cos t is sampled eight times, instead of four, the new spectrum appears as 
shown in Figure 7.15. The component at w = 1 remains unchanged, the component 
at w = 3 is now zero, but there is an alias at w = 7. And, as you probably guessed, 
the alias occurs because cos 7t and cos t have the same values at the eight sampling 
points (0, v/4, v/2,3~/4, T, 
5 ~ / 4 , 3 ~ / 2 , 7 ~ / 4 , 2 ~ ) .  
The sampling theorem, which we will prove in the next chapter, says in order to 
accurately portray a signal by sampling, you must sample at least twice the highest 
frequency present in the signal. If this is done with a discrete Fourier series, the 
resulting spectrum can be trusted(i.e., there will be no aliasing) up to half the sampling 
c n  
Figure 7.15 
The Spectrum of cos t Sampled Eight Times 

241 
THE DISCRETE FOURIER SERIES 
frequency. Therefore, the spectrum of Figure 7.14 is valid up to n = on = 2, while 
the spectrum of Figure 7.15 is valid up to n = on = 4. The signal 
f i t )  = cos f + cos 3t 
(7.89) 
has a period T,, = 2 ~ r  
and must be sampled at a frequency of 6, or six times over that 
interval, in order to trust the sampled spectrum up to n = on = 3. 
7.4.4 Positive and Negative Frequencies 
The normal exponential Fourier series contains terms for both positive and negative 
frequencies. The discrete Fourier series, the way it was developed in the discussion 
above, created a spectrum that ranged in n from 0 to (2N - 1) or 0 5 on 5 
27~(2N - l)/To. To establish this orthogonality condition of Equation 7.73, however, 
it is only necessary that n take on 2N consecutive values. The discrete series analysis 
can therefore be set up with all the sums over n starting at some arbitrary integer, say 
no, and ranging to (2N + no - 1) where 2N is still the number of sampled points in 
the period. Equation 7.76, for example, could have been written as 
2N+n,-1 
(7.90) 
n=no 
The elements of the [MI matrix are generated by the same set of k values, but a shifted 
set of n values: 
(7.91) 
Except for these modifications, the development of the discrete Fourier series would 
be unchanged. 
This flexibility in selecting the value of no, coupled with the sampling theorem, 
allow discrete Fourier series spectra to be set up more like the regular exponential 
Fourier spectra. If we set no to -N, then the sums over n range from -N to (N - 1) 
and the spectra that result will range in frequency from -2.rrN/TO to 2 7 i N  - 1)/To. 
If, in addition, the signal is sampled at a rate of at least twice the highest frequency 
present, there will be no aliasing in the spectrum that results from the discrete Fourier 
series analysis. Taking such a range for n, the two spectra shown in Figures 7.14 and 
7.15 would become as shown in Figure 7.16. In this way a discrete Fourier series 
spectrum can be generated that more truly represents the real Fourier spectrum of 
the signal. The crucial step here is to know the highest frequency present. This is 
sometimes accomplished by passing the signal through a filter that limits the hghest 
frequency to a known value and then sampling at twice this rate or faster. 
This way of looking at the discrete Fourier spectrum allows another interpretation 
of aliasing. The spectral component at n = 3 in the spectrum shown in Figure 7.14 
can be looked at as an “alias” of the true component at n = - 1 of the upper spectrum 
in Figure 7.16. The spectral component at n = 7 of Figure 7.15 is an alias of the 
true component at n = - 1 of the lower spectrum in Figure 7.16. These spectral 

242 
FOURIER SERIES 
[ -c. 
0 
112 c 
0 
N = 2  
n = w ,  
I 
+- 
L
.
,
 
-2 
-1 
0 
1 
, -Cn 
0 
112 1 
0 
N = 4  
I 
n = o ,  
I 
I 
* 
1 
a 
w 
I 
c 
w 
-4 
-3 
-2 
-1 
1 
2 
3 
Figure 7.16 The Discrete Fourier Series Spectrum for -N S n 5 (N - 1) 
components are being moved around by the shifting that occurs in the [MI matrix as 
no is changed, as described by Equation 7.91. 
Notice that with the almost symmetric choice for the range of n demonstrated by 
the spectra in Figure 7.16, the discrete Fourier Series representation for the sampled 
values of cos t becomes 
N -  1 
n=-N 
(7.92) 
The RHS of Equation 7.92 is not only pure real and equal to cost at the sampled 
values, t = tk, but is also pure real and e q d  to cos t for 
values of t ! 
EXERCISES FOR CHAPTER 7 
1. Show that if f(t) is a periodic function with period To, then 
f(t> = f(t + 3Td. 
A Fourier series for f(t) can be constructed that uses wn = 2m/3T0. Show that 
this is the same Fourier series that is generated using w, = 2nn/T0. 
2. Determine the coefficients of the Fourier series for the functions x(t), y(t), and 
z(t) shown below: 

EXERCISES 
243 
2 
3. Consider the exponential Fourier series for f ( x )  with the form 
n=-m 
Answer the following questions without actually determining the Fourier coeffi- 
cients: 
(a) If f ( x )  = sin2 x, what are the values for the on? 
(b) If f(x) = sin2 x, is Q, zero or nonzero? 
(c) If f(x) = sin2 x ,  are the c, pure red, pure imaginary, or complex? 
(d) If f ( x )  = sin2 (x - 7r/2), are the 
pure real, pure imaginary, or complex? 
(e) If f ( x )  = sin2 (x - 7r/4), are the 
pure real, pure imaginary, or complex? 
Now determine all the exponential Fourier series coefficients for f ( x )  = sin2 x, 
without doing any integrals. 
4. Determine the sinekosine Fourier series that represents the following periodic 
function: 

FOURIER SERIES 
244 
1 
4 
5 
-3 
I 
Determine the exponential Fourier series coefficients for this same function. 
5. Find the period and coefficients for a Fourier sinekosine series that represents 
the following nonperiodic functions over the specified range. Make a plot of the 
original function and the periodic Fourier series. 
i. f ( t )  = t3 o < t < T 
ii. f ( t )  = et 0 < t < 2~ 
6. Consider two Fourier series Sl(x) and S2(x). The first series Sl(x) represents the 
function yl(x) = x2 over the range 0 < x < 1, and &(x) is equal to the periodic 
function y2(x) where 
YZ(X) = x 
O < x < l  
y2(x) = y2(x + 1) for all x. 
(a) Make a labeled plot of S1 (x) for all x. 

EXERCISES 
245 
(b) Let 
nc 
m 
s ~ ( x )  = C aneiknx and 
&(x) = 
bneiknx. 
Determine the k, and, without evaluating the coefficients, find a relationship 
between the a, and b,. 
(c) Now evaluate the a, and b, coefficients. 
7. Consider the periodic function f ( t )  = f ( t  + To) where 
0 
- T 0 / 2 < t <  
-IT 
f ( t )  = cost - n < t <  +7r 
. 
{ 0 
+ T r < t <  
+T0/2 
This function can be expressed as an exponential Fourier series 
n=--pl 
(a) Make a plot of f ( t )  vs t .  
(b) What are the values for w,? 
(c) From your answer to part (a) determine G. 
(d) Are the c, pure real, pure imaginary, or complex? 
(e) Determine the cn. 
8. Consider the function 
m 
f ( t )  = c - nToh 
n=-m 
where n is an integer and 6 ( x )  is the Dirac &function. 
Make a labeled plot of f ( t )  vs t. 
Determine the fundamental frequency and the coefficients a, and b, of the 
sinelcosine Fourier series such that 
1 
m 
cc 
8(t - nTo) = 7 + 
an cos (w,t) + 
b, sin (w,t). 
n = l  
1 
n=-s 
n = I  
The &function is highly discontinuous, and based on the discussion of 
convergence in this chapter, we should suspect some odd behavior with the 
Fourier series representation of this function. What is unusual about the 
coefficients you determined in part (b)? 
9. Evaluate 
1" dt cos (w,t) eiam', 
where w, = 2nn/T0 and n and m are both integers. 

246 
FOURIER SERIES 
10. If a function can be expressed as a Fourier series 
m 
and its derivative can also be expressed as a Fourier series 
how are g,, and C-, 
related? How are w, and Rn related? 
11. I f g ( t )  is periodic with period To and is represented by a sinekosine Fourier series 
with coefficients a, and b,,, determine the coefficients of the Fourier series that 
represents the function 
Repeat this problem with g(t) represented by an exponential Fourier series. 
12. Consider the differential equation 
where f ( t )  is a periodic with period To and in the interval 0 < t < To is given by 
-1 
O < t < T o / 2  
f ( t ) =  { +1 
T0/2 < t < To ' 
(7.93) 
(a) Make a plot of f ( t ) .  
(b) Determine the exponential Fourier series for f ( t ) .  
(c) Assume x(t) can be expressed as an exponential Fourier series and find the 
(d) What electronic circuit demonstrates the action of this differential equation? 
coefficients. 
What type of electrical signals are represented by x ( t )  and f(t)? 
13. Consider the periodic triangular wave shown below: 

EXERCISES 
247 
(a) Find the sinekosine Fourier series for this function. 
(b) Find the exponential Fourier series for this function. 
(c) Assume that the function f ( r )  is the driving function for a damped harmonic 
oscillator, and the response of the oscillator is given by the function x(r) so 
that 
Find the Fourier series for the response x(r). Is it easier to use your answer 
to part (a) above and find the sinekosine series for f ( r )  or your answer to 
part (b) and find the exponential series for x(t)? 
(d) What happens to the response x(t) if To = 2.rr@? 
14. Go through the arguments for developing the discrete Fourier series and show 
that the orthogonality conditions holds as long as the range of n is no i n I 
(2N + no - l), where 2N is the number of samples taken and no is any integer. 
15. Consider the periodic function f ( r )  shown below: 
I 
1 
2 
3 
4 
(a) Determine the exponential Fouher series for this function and plot the 
vs. 
(b) Plot the function df(t)/dt. Determine its Fourier series and plot its spectrum. 
(c) Develop the discrete Fourier series for this function by sampling it six times 
during its period. Plot the discrete spectrum and discuss the aliasing effects. 
n to describe its spectrum. 
16. The function f ( t )  is the periodic triangular wave shown below. 

FOURIER SERIES 
248 
(a) Develop its discrete Fourier series by sampling it four times during a period. 
(b) Compare this discrete Fourier series to the one for a cosine function with the 
same period and sampled at the same rate. 
17. Let f(t) = t2 for all t. 
(a) Find the exponential Fourier series, S(t) that represents this function over 
theinterval0 < t < 1. 
(b) Develop the discrete Fourier series for the periodic function S(t) from part 
(a) above by sampling the function four times during its period. Compare 
h e  coefficients found for this discrete Fourier series with the actual ones 
found in part (a). 
(c) Now sample the function S(t) six times in one period and repeat the steps in 
(d) Discuss the aliasing for this problem. 
Part (b). 
18. Consider a periodic function with To = 4, given in the range - 1 < t < 3 by 
(7.94) 
Make a labeled sketch of f(t). 
Find the Fourier series that represents f ( t ) .  Note that d 2  f/dt2 = g(t), where 
g(t) is a familiar function. Find the Fourier series for f(t) by relating it to 
the Fourier series for g(t). 
Develop the discrete Fourier series for f ( t )  by sampling it four times during 
a period. Compare the coefficients of this discrete Fourier series to the 
coefficients obtained in part (b) above. 
Now find the discrete Fourier series by sampling f ( t )  six times during a 
period. 
19. Let SN be the partial Fourier series 
N 
SN(t) = C cneioJ. 
The mean-squared error between this partial sum and the function f ( t )  is defined 
as 
n=-N 
This error can be looked at as a function of the 2N + 1 cn coefficients. Assume 
S N ( ~ )  
is pure real and minimize the error with respect to the gn, i.e., set 

249 
EXERCISES 
to generate 2N + 1 equations for the gn. Show that these equations lead to 
the standard expressions for the Fourier series coefficients. 

FOURIER TRANSFORMS 
The Fourier series analysis of the previous chapter can only be used to obtain spectra 
for periodic functions. If the independent variable is time, periodic functions must 
repeat themselves for all time. But signals in the real world have both a beginning and 
an end, and consequently cannot be analyzed using Fourier series. There is, however, 
a continuous version of the Fourier series, called the Fourier transform, which can 
handle certain types of nonperiodic functions. In this chapter, we demonstrate how the 
Fourier transform is the limiting case of the Fourier series, as the period of repetition 
grows infinitely large. 
8.1 FOURIER SERIES AS To 4 m 
The Fourier series for a periodic function, such as the one in Figure 8.1, can be 
summarized by the equation pair 
+a 
,=-m 
To /2 
T o  -T,/2 
c, - 
- - 1 
dt f(t)e-i"nr, 
where a, = 2nn/T0. To simplify the discussion that follows, the limits of integration 
have been shifted to be symmetric about t = 0. The 
coefficients are the complex 
amplitudes of the discrete frequency components which make up f ( t ) .  
As the period To grows larger, as shown in Figure 8.2, the Fourier series still 
generates a spectrum, but the discrete frequencies a, = 2mT0 become packed 
250 

FOURIER SERIES AS To --+ m 
251 
-T0/2 
I T0/2 
Figure 8.1 Standard Periodic Function 
closer and closer together. If To is infinite, as shown in Figure 8.3, the function f ( t )  
is no longer periodic. Equation 8.2 can no longer be used to generate a spectrum 
because of the 1 /To factor, and the Fourier series analysis breaks down. 
Functions like the one in Figure 8.3 can be analyzed with the Fourier transform, 
which can be derived by carefully considering the Fourier series equations in the 
-T0/2 
I 
T0/2 
Figure 8.2 Periodic Signal with Increasing To 
Figure 8.3 Typical Nonperiodic Signal 

252 
FOURIER TRANSFORMS 
limit as To -+ ~ 0 .  In this limit, the discrete on frequencies merge into a continuous 
frequency parameter 
2n-n 
w, = - 
- + w  
T O  
and we can treat Am, the step size between the discrete frequencies, as a differential 
variable dw 
To find the equations equivalent to the Fourier series pair given by Equations 8.1 
and 8.2 in the limit of To -00, 
insert Equation 8.2 into Equation 8.1 to obtain 
Notice that in this equation a dummy variable of integration r has been introduced 
to avoid confusion with the independent variable t. Rearranging Equation 8.5 and 
substituting 1 /To = A w/2a gives 
As To goes to infinity, w,, -+ w and A w  --t do, so the sum over n becomes an integral 
over w. Thus in this limit, Equation 8.6 becomes 
where we have arbitrarily split up the factor of 1/2m in anticipation of things to 
come. 
The bracketed term on the RHS of Equation 8.7 is called the Fourier transform of 
f (t) and is a function of the continuous variable w. It is generally a complex function, 
so we represent it with the symbol E(w). With this definition for F(w), Equation 8.7 
can be broken into two equations 
f 0) 
(8.8) 
(8.9) 
which are often referred to as the Fourier transform pair. They are the continuous 
extensions of Equations 8.1 and 8.2 for nonperiodic functions. These equations are 
sometimes written using the operator symbol y: 

ORTHOGONALITY 
253 
where 
and 
(8.10) 
(8.1 I) 
(8.12) 
(8.13) 
are both operators. 
8.2 ORTHOGONALITY 
The orthogonality condition associated with the exponential Fourier series, which 
allowed the coefficients to be evaluated, took the form 
(8.14) 
where on = 27rn/T0 and n was an integer. A similar orthogonality expression exists 
for the Fourier transform. As long as the Fourier transform F(w) exists, Equation 8.7 
can be rearranged as 
Because the definition of the Dirac delta-function is 
f ( t )  = s_*, d7f(T)a(t - T), 
(8.15) 
(8.16) 
the bracketed term on the RHS of Equation 8.15 must act like a shifted &function: 
Interchanging the variables gives the inverted form: 
m 
& lm 
dt ei(o--o')t = 6(w - 0'). 
(8.17) 
(8.18) 
These two equations are the orthogonality conditions we seek. A function eiot is 
orthogonal to all other functions e-iO" when integrated over all t, as long as o # a'. 

254 
FOURIER TRANSFORMS 
The relations in Equations 8.17 and 8.18 cannot be viewed as rigorously correct, 
because they involve a 8-function which is not enclosed in an integral. The relations 
should be viewed as a shorthand notation and are mathematically valid only if they 
appear inside integral operations. 
The orthogonality relations given in Equations 8.17 and 8.18 can be used to gen- 
erate one of the equations of the Fourier transform pair (Equations 8.8 and 8.9) from 
the other. Operating on Equation 8.8 with J dw eiwr and then applying Equation 8.17 
gives 
1: d o  e'*'F(w) = - 
Srn 
dt f (t) /: dw ei(r-r)m 
6 
--m 
= G f ( r ) ,  
(8.19) 
which is simply Equation 8.9. The same process works in reverse by operating on 
Equation 8.9 with J df e-id and using Equation 8.18. 
8.3 EXISTENCE OF THE FOURIER TRANSFORM 
In order for the Fourier transform to exist, we must be able to perform two processes. 
First, the transform integral itself must exist. That is, given an f(t), we must be able 
to calculate F(w) from Equation 8.8. Second, when we apply the inverse transform, 
we must retrieve the original function. In other words, after calculating the E(w), we 
must be able to apply Equation 8.9 and get back f(t). 
In order for the Fourier transform of a function, f(t), to exist, it must be possible 
to perform the integration, /: dt e-id f (t). 
One simple condition derives from the fact that 
(8.20) 
so we can write 
(8.22) 
Thus a sufficient condition for the existence of F(w) is that the integral on the RHS 
of Equation 8.22 be finite. That is, 
1: dt If(t)l -=c 
Functions which obey Equation 8.23 are called absolutely integrable. 
(8.23) 

EXISTENCE OF THE FOURIER TRANSFORM 
255 
The second issue, whether the inverse transformation returns the appropriate orig- 
inal function, is more subtle. If E(m) exists, it can be substituted into Equation 8.9 to 
give Equation 8.15. Then by the orthogonality condition of Equation 8.17, it would 
appear that if F(w) exists, the inversion must return the original f ( t ) .  This indeed 
will be the case for all well-behaved, continuous functions. But the sifting action of 
8(t - T) is only really well defined for functions that are continuous at t = T. To 
determine what happens when the function is discontinuous at t = T takes a little 
more thought. 
One way to look at this situation is to substitute a sequence function 8,(t - T) for 
the bracketed term in Equation 8.15 and then take the limit as n ---f w. The Fourier 
inversion equation then becomes 
4. 
(8.24) 
If a Gaussian sequence function is used, the functions in the integrand can be graphed 
as shown in Figure 8.4. From this figure, it is clear that in the infinite limit we can 
write 
If f(t) is a continuous function, then the RHS of Equation 8.25 becomes f ( t )  and the 
Fourier inversion returns the original function. If the function f ( t )  is discontinuous 
at some point, say t = to, the inversion returns the average value of f(t): 
where f r  and fr are the values of f ( t )  just to the left and right of the discontinuity, as 
shown in Figure 8.5. 
Depending upon how f (t) is defined at the discontinuity, this may or may not be 
the correct value for f(t = to), For example, if f ( t )  were defined as 
(8.27) 
Figure 8.4 
Integrand Functions for the Fourier Inversion 

256 
FOURIER TRANSFORMS 
! 
f(T) 
j 
'\\ 
fir- F- 
to 
Figure 8.5 
Fourier Inversion at a Discontinuity 
so that f ( f J  = f i ,  the Fourier inversion returns the incorrect result given in Equa- 
tion 8.26. This same effect was seen in the discussion of the convergence of Fourier 
series in the previous chapter. 
8.4 THE FOURIER TRANSFORM CIRCUIT 
The expression for f ( t )  in terms of its Fourier transform E(w), 
f(4 = J2.lr Sm 
--m do eidE(o), 
(8.28) 
implies that the function f ( t )  can be made up of a continuous sum of oscillators. This 
can be seen by considering the integral in Equation 8.28 as the limit of a Riemann 
sum. Remember that a Riemann SM views an integral, such as the integral of g(x) 
over all of x, to be the limit of a sum of rectangular areas, 
(8.29) 
where Ax is the width of each rectangle. Applying this expression to Equation 8.28 
gives 
m 
f ( t )  = l h  
E(cnAW)ei(nAo)tAW. 
(8.30) 
Aw-0 
n=-m 
J2?r 
Equation 8.30 represents a sum of terms, each oscillating as ei(nAw)t, 
with an amplitude 
given by AoF(nAw)/&. 
Similar to what we did in the previous chapter with 
Fourier series, we can view this sum as the electric circuit shown in Figure 8.6. The 
value of f ( t )  is the sum of the voltages produced by all the oscillators. As Am goes 
to zero, both the spacing between the frequencies of the oscillators and the amplitude 
of each oscillator gets infinitesimally small. The sum, however, combines to form the 
finite value of f(t). 

THE FOURIER TRANSFORM CIRCUIT 
\ 
257 
J2 x 
L 
1 
\ 
I 
I 
I 
I 
I 
I 
Figure 8.6 The Fourier Transform Circuit 

FOURIER TRANSFORMS 
258 
8.5 PROPERTIES OF THE FOURIER TRANSFORM 
In this section, a number of mathematical properties of the Fourier transform are 
developed. Each derivation involves either one or both of the functions f (t) and g(t), 
which we will assume possess the valid Fourier transforms: 
(8.31) 
(8.32) 
8.5.1 
Delay 
The Fourier transform of f (t - to), which is f (t) delayed by to, can be expressed in 
terms of E((o). The Fourier transform of the delayed function is 
F[f(t - to)] = - 
[l dt eCid f (t - to). 
(8.33) 
Make the substitution a = t - to to obtain 
1
"
 
f (a) 
- 
- e-ioto- J2.rr 1" 
da eCioa 
= e-'"~[f(t)]. 
(8.34) 
So when a function is delayed by an amount to, its Fourier transform is multiplied by 
a factor of e-iofo. 
8.5.2 Time Inversion of Real Functions 
The Fourier transform of f( -t), if f (t) is a real function, is the complex conjugate 
of F(w). To see this, start by writing the Fourier transform of the inverted function as 
Substitution of a = -t gives 
1 
-" 
Y[f(-t)] 
= -- / da eioa f ( a )  
6 "  
with the result that, iff (t) is pure real, 
(8.35) 
(8.36) 
(8.37) 

PROPERTIES OF THE FOURIER TRANSFORM 
259 
8.5.3 
Even and Odd Functions 
If f (t) is a real, even function then 
f ( - t )  = f ( t h  
(8.38) 
and by Equations 8.31 and 8.37 
- 
F * ( W )  = E(w). 
(8.39) 
Thus, for even functions, F(w) must be a pure real function of w. 
If f (t) is a real, odd function then 
f(-O = -fW, 
(8.40) 
and by Equations 8.31 and 8.37 
F * ( W )  = -F(w). 
(8.41) 
Therefore, for odd functions, F(w) must be a pure imaginary function of w. 
8.5.4 
Spectra for Pure Real f(t) 
Functions which are pure real have transforms which obey an important symmetry. 
This can be derived easily by substituting -w into Equation 8.8 to give 
Consequently, if f ( t )  is pure real then 
- 
F ( - a )  = E'(w). 
(8.42) 
(8.43) 
This means that the negative frequency part of the spectrum can always be obtained 
from the positive frequency spectrum. The negative frequency part of the spectrum 
contains no new information. 
8.5.5 The Transform of df(t)/dt 
The Fourier transform of the derivative of f ( t )  can be expressed in terms of the 
transform of f (t). To see this, start with the transform of the derivative: 
The RHS of Equation 8.44 can be integrated by parts. Let u = e-'"' and dv = 
(df/dt) dt. Then 
m 
Y [ $1 
= ~2.rr 
1 { e-id f(t)lm - L m d t  (-iw)e-iwtf(t)}. 
(8.45) 
-m 

260 
FOURIER TRANSFORMS 
If f ( t )  -+ 0 as t --+ I m ,  which must occur for any function with a valid Fourier 
transform, then 
= iwE(w). 
(8.46) 
8.5.6 The Transformations of the product and Convolution 
of ' h o  Functions 
The Fourier transform of the product of two functions can be expressed in terms 
of the Fourier transforms of the individual functions. The Fourier transform of the 
product of f ( t )  and g(t) is 
1
"
 
F [f(t)gW] = - ,/ 
dt e-'"f(t)g(t). 
J2.rr --m 
To express this transform in terms of f ( w )  and G(w), insert the inverse transforms for 
f ( t )  and g(t) into Equation 8.47, making sure to use different variables of integration 
to keep all the integrals independent: 
(8.47) 
1
"
 
X J2.r Lrn 
dw" eiw"rF(w"). 
- 
(8.48) 
Now collect all the 1/J2?r factors, and combine the integrals over t, w' and w" into 
a three-dimensional integral. The RHS of Equation 8.48 becomes 
The order of integration is arbitrary. If we choose to do the t integral first, we are 
confronted with the integral 
J --m 
which is just the orthogonality condition for the Fourier transform. Using this in 
Equation 8.49 gives 

PROPERTIES OF THE FOURIER TRANSFORM 
261 
Performing the integration over dw” produces the final result 
3,{f(r)g(t)) = ___ /: dw’G(w’)F(w - w’). 
(8.52) 
The operation on the F2HS of Equation 8.52 is a common one and is called the 
convolution of G(w) and E(w). Convolution plays an important role in the application 
of Green’s functions for solutions of differential equations, which is discussed in a 
later chapter. The convolution of G(w) and F(o) is often written using the shorthand 
notation 
G(w) 0 
F(w) 
~w’G(w’)E((o 
- w‘). 
(8.53) 
- 
s_mm 
From the form of Equation 8.53, it is clear that 
G(w) 0 
F(w) = F(w) 0 am). 
(8.54) 
Due to the symmetry of the Fourier transform pair, it is clear that the Fourier 
inversion of the product of two transforms will also involve convolution: 
(8.55) 
1 
3 , - w 4 G ( 4 }  = ----f(t) 0 
g(t) 
fi 
or 
Convolution can be somewhat difficult to visualize. The process is facilitated by a 
graphical interpretation demonstrated in the following example. 
Example 8.1 Consider the convolution of two functions: 
W )  
_= f W  0 
s(i) = 
d7 f ( 7 M t  - 7) 
(8.57) 
Let f ( t )  be a square pulse and g(t) a triangular pulse, as shown in Figure 8.7. The value 
of h(t) at a particular value of r is equal to the area of the integrand of Equation 8.57. 
L 
1
2
3
 
,
1
2
3
4
5
 
Figure 8.7 
Example Functions for Convolution 

262 
FOURIER TRANSFORMS 
2pT 
ht 
2[i 
2
3
 
t-4 
t-2 
Figure 8.8 Convolution Functions Plotted vs. 7 
This area can be represented graphically as the area under the product of f ( 7 )  and 
g(t - 7) plotted vs. T .  The functions f ( ~ )  
and g(t - 7) plotted vs. 7 are shown in 
Figure 8.8. The function g(t - T )  plotted vs. 7 is the function g ( T )  inverted and shifted 
by an amount t. In Figure 8.8, g(t - r) has been plotted for a negative value of t. 
The integrand of the convolution integral is a product of these two functions, and the 
value of the convolution is the area under this product, 
To determine h(t), we must consider this process for all possible values of t. 
For this example, start with t Q 0. A combined plot off(7) and g(f - T )  for this 
condition is shown in Figure 8.9, which clearly indicates there is no overlap of the 
two functions. Their product is therefore zero, and h(t) is zero for this value of t. 
In fact, from this picture it is clear that h(t) = 0 for all t < 1 and all t > 7. The 
convolution of these two functions can be nonzero only in the range 1 < t < 7, 
where a little more analysis is needed. Within this range there are three subregions 
of t to consider: 1 < t < 2; 2 < t < 6; and 6 < t < 7. The nature of the overlap 
between the two functions is different in each case. 
For 1 < f < 2, the functions of Figure 8.9 only overlap in the range 1 < 7 < t, 
as shown in Figure 8.10. Therefore for 1 < t < 2, 
h(t) = f ( t )  @ g(t) = 1 
d7 f(T)g(t - 7). 
Over the integration range f ( ~ )  = 2. The function g(t - T )  is given by 
{f?/5)(f - 7 )  0 < (t - 7 )  < 5 
g(t - 7 )  = 
otherwise 
(8.58) 
(8.59) 
f(d 
,//’ 
/ 
z 
t-4 
t-2 
t 
2 
Figure. 8.9 
Convolution Functions fort 
0 

PROPERTIES OF THE FOURIER TRANSFORM 
263 
' 
l
t
2
 
Figure 8.10 Convolution Picture for 1 < f < 2 
The range 0 < (t - 7) < 5 is equivalent to (t - 5) < T < t. Therefore, over the 
range of the convolution integral in Equation 8.58, g(t - 7 )  = (2/5)(t - T), and 
The result is 
(8.61) 
2 
4
2
 
5 
5
5
 
h(t) = f ( t )  @g(t) = -t2 - -t + - 
1 < t < 2. 
Up to this point, h(t) has been obtained for the ranges --to < t < 2 and 7 < I, as 
shown in Figure 8.1 1. 
For 2 < t < 6, the triangle function is shifted further to the right, so that the 
square pulse is totally inside it, as shown in Figure 8.12. The convolution integral in 
this region becomes 
d T f ( T ) g ( t  - 7 )  
2 < t < 6. 
(8.62) 
2
2
 4 
2 
-t - -t+- 
\, 
5 
5
5
 
t 
12 
7 
Figure 8.11 Convolution Result for --m < r < 2 and 7 < t 

264 
FOURZER TRANSFORMS 
I
1
 2 
t 
Figure 8.12 Convolution picture for 2 < t < 6 
As before, f(7) = 2 and g(t - 7) = (2/5)(t - T), so Equation 8.62 integrates to 
4
6
 
h(t) = -t - - 
5
5
 2 < t < 6 .  
(8.63) 
The convolution function h(t) has now been determined for the ranges --to < t < 6 
and 7 < t as shown in Figure 8.13. 
For the last region, 6 < t < 7, the triangular pulse has moved to the right so only 
its trailing edge is inside the square pulse, as shown in Figure 8.14. The convolution 
integraI for this range becomes 
h(t) = 
d ~ f ( ~ ) g ( t  
- 7) 
6 < t < 7. 
(8.64) 
If, 
Again, f ( ~ )  
= 2 and g(t - 7) = (2/5)(t - T), so that the integration evaluates to 
(8.65) 
2 
8 
42 
h(t) = - - t 2  + -t + - 
5
5
5
 6 < I  < 7. 
2 2  
-t 
5 
4
6
 
5
5
 
-t- - 
‘ 1 2  
7 
Figure 8.13 Convolution Results for --oo < t < 6 and 7 < t 

PROPERTIES OF THE FOURIER TRANSFORM 
265 
1
2
 
t- 5 
t 
Figure 8.14 
Convolution picture for 6 < t < 7 
2 2  
- 
2 2  8 
- - t  
+ - t +  
4
6
 
-t- - 
5
5
 
5 , 5  
, 
4
2
 
5
5
 
- t + -  
__ 
t 
1 2  
67 
- 
42 
5 
Figure 8.15 
Convolution Results for --m < t < -m 
The final result for the convolution of f ( t )  with g(t) is shown in Figure 8.15. 
As you can see from this example, a good deal of care must be taken to perform a 
convolution integral properly! 
8.5.7 
Correlation 
The correlation process performs a measure of the similarity of two signals. It has 
several important practical applications. The most important is its ability to pick a 
known signal out of a sea of noise. The cross-correlation between f (t) and g ( t )  is 
defined as 
The cross-correlation of a function with itself, 
(8.66) 

266 
FOURIER TRANSFORMS 
g(7-t) 
Increasing t 
t 
t+l 
t+3 
t+5 
i
1
2
 
Figure 8.16 The Cross-Correlation Operation 
is called an autocorrelation. It is frequently used to measure the pulse width of fast 
signals. 
The correlation operation is quite similar to convolution described in the previous 
section, except the second function is not inverted. The t-variable shifts g(7) with 
respect to f(7) so that the functions of the integrand of the correlation operation 
appear as shown in Figure 8.16 if f(r) and g(r) are as defined in Figure 8.7. In 
general, the correlation operation requires the same care in breaking up the integral 
as is necessary for convolution. 
The Fourier transform of the cross-correlation of two functions can be expressed 
in terms of the individual Fourier transforms of the two functions. The derivation 
follows the same steps used for convolution, and the result is 
(8.68) 
8.5.8 Summary of 'kansform Properties 
The properties of the Fourier transform are summarized in the table below. In this 
table a double arrow c) is used to indicate the transform and its reverse process. 
The last four relations are true only if f ( t )  is a pure real function oft. The others are 
valid even if f ( t )  and g(t) are complex functions. 

FOURIER TRANSFORMS - 
EXAMPLES 
267 
8.6 FOURIER TRANSFORMS-EXAMPLES 
This section derives the Fourier transforms of several common functions. In each 
case, we have attempted to explore an important technique or principle associated 
with the Fourier transform. 
8.6.1 The Square Pulse 
Consider the square pulse shown in Figure 8.17 with 
I 
0 
otherwise 
-T/2 < t < T / 2  
f(f) = { 
Clearly 
(8.69) 
(8.70) 
so this function is absolutely integrable and should have a valid Fourier transform. 
Using the definition of the transform, 
dt e-i6Jt 
This integration is straightforward and evaluates to 
(8.71) 
(8.72) 
This spectrum is plotted in Figure 8.18. It sits inside an envelope given by d m .  
As w -+ 0, it has the finite limit of T/&, 
because both the numerator and 
denominator in Equation 8.72 are going to zero at the same rate. The first zero of 
- 
F( o) 
occurs when sin( wT/2) = 0 or at o = (27r)/T. The other zeros are periodic at 
-TI2 
TI2 
Figure 8.17 The Square Pulse 

268 
FOURIER TRANSFORMS 
2 dT' 
Figure 8.18 Spectrum of a Square Pulse 
w = 2 m / T  where n is an integer. This function is so common that it has been given 
its own name: 
sin x - 
= sinc x. 
X 
(8.73) 
We can easily find the area under any F(w) spectrum by evaluating the inverse 
Fourier transform of Equation 8.9 at t = 0: 
Therefore, for the square pulse 
(8.74) 
(8.75) 
The area under the spectrum of the square pulse is finite and independent of T. 
This has interesting consequences for the spectrum in the limit of T -+ 03. From 
Figure 8.18, notice that as T increases, the value of the spectrum at o = 0 increases, 
and all the zero crossings move toward o = 0. This happens in such a way that 
the area under the spectrum remains constant. In the limit of T -+ 
03, therefore, 
this spectrum has all the qualities of a Dirac &function. This can be confirmed by 
combining the formal expression for E(o) with the orthogonality condition derived 
in Equation 8.17: 
dt - iof 
F(w) = lim - 
- 
T4m 6 
S" 
-T/2 
= &a("). 
(8.76) 

FOURIER TRANSFORMS - 
EXAMPLES 
269 
Remember, however, that this is not a rigorous equation, because the &function is 
not inside an integral. The Fourier transform for the function f ( t )  = 1, is not really 
defined, since 
(8.77) 
does not converge. But in a casual sense, the &function in Equation 8.76 makes 
perfect sense, because when it is inserted into the inverse Fourier transform, we get 
back our original function f ( t )  = 1 : 
1
"
 lm 
dw ei'"'E(w) = - 
[I 
dw eiWf 
&8(w) 
We will explore the transform properties of the &function in greater detail in the 
following sections. 
8.6.2 
Transform of a &Function 
In the light of the previous example, it is appropriate to ask for the Fourier transform 
of 8(t). Again, we will suspend the rigorous treatment of the &function and allow it to 
exist outside of integral operations. The 6-function is certainly absolutely integrable, 
because 
and the transform itself is easy: 
1 
- 
_ _ _  6' 
The inversion of this transform function becomes 
ior 
(8.78) 
(8.79) 
(8.80) 
which, using Equation 8.17, can be interpreted as 8(t). 
The Fourier transform of a 6-function can be used to obtain the transform of a 
square pulse. If f ( t )  is the square pulse shown in Figure 8.17, its derivative is the sum 
of two &functions 
rlfo = 8(t + T/2) - 8(t - T/2). 
(8.81) 
dt 

270 
FOURIER TRANSFORMS 
The transform of these two shifted 6-functions, using Equation 8.34, is 
Because this is the transform of the derivative of the square pulse, Equation 8.46 says 
the spectrum of the square pulse is given by 
1 
iei0T/2 - e-i0T/2 
F(w) = ~ 
iw& 
- 
= 
[ s y 7 , / 2 ) ]  
which agrees with the result we found earlier in Equation 8.72. 
8.6.3 Tkansform of a Gaussian 
A normalized Gaussian pulse has the general form 
(8.83) 
(8.84) 
where the leading constant normalizes the function, as we will prove shortly. A graph 
of Equation 8.84 is shown in Figure 8.19. Notice how the parameter Q scales both 
the width and the height of the pulse. 
The area of the pulse is given by 
a 
2 2  
At) = 7 
e"' 
a/----- 
I 
&i 
- 
l/a 
Figure 8.19 The Gaussian Pulse 

FOURIER TRANSFORMS - 
EXAMPLES 
271 
and can be evaluated with a standard trick. First, make a substitution of x = at: 
(8.86) 
1
"
 
Area = - 
J dx e-". 
J.rr 
--m 
Now consider an equation for the square of the area: 
Notice the final integral of Equation 8.87 can be viewed as an integral over the entire 
Cartesian xy-plane. By changing the variables to a polar form with the substitutions 
x2 + y2 = p2 and dx dy = p dp do, Equation 8.87 becomes 
(8.88) 
The do integration simply gives a factor of 27r, and the dp integration that remains is 
straightforward. The final result is 
[ ~ r e a ] ~  
= 1. 
(8.89) 
Therefore, 
a 
2 2  
/:dt 
e P a t  = 1. 
(8.90) 
The area under the normalized Gaussian is one, independent of the value of a. 
The Fourier transform of the normalized Gaussian is 
- 
F(w) = - 
1 /" 
dt -e-a 
a 
2 2  
t 
- .  lot 
J 2 . r r - m  fi 
(8.91) 
This integration can be evaluated by completing the square. A quantity y is added 
and subtracted from the argument of exponential function, 
(a2t2 + iwt + y) - y, 
(8.92) 
so that the expression in the brackets of Equation 8.92 forms a perfect square. That is, 
we want to be able to write a2t2 + iwt + y as (at + p)2. The value of /3 is therefore 

272 
determined by 
FOURIER TRANSFORMS 
From Equation 8.93, 
so that 
and 
a2t2 + iwt + y = (at + p)2 
= a2t2 + 2apt i- 
p2 
2apt = iwt 
iw 
p = -  
2a 
(8.93) 
(8.94) 
(8.95) 
(8.96) 
Equation 8.91 can now be written as 
F(w) = - 
a 
e-oz/(4a2) 1: dt e-[at+io/(2a)1z 
(8.97) 
This integration is accomplished using the substitution x = at + io/(2a) and 
dx = a dt: 
4 
- 
(8.98) 
Thus we have shown the Fourier transform of a Gaussian is another Gaussian: 
(8.99) 
As a increases, the width of the Gaussian pulse decreases, while the width of 
its Gaussian spectrum increases, as shown in Figure 8.20. This demonstrates an 
important general property of the Fourier transform. The width of the function f(t) 
is always inversely proportional to the width of the transform F(w). In the case of the 
Gaussian, the width of f ( t )  is roughly 
1 
At zs -, 
a 
while the width of E(w) is approximately 
(8.100) 
Aw = 2a. 
(8.101) 

FOURIER TRANSFORMS - 
EXAMPLES 
273 
- 
0 
* 
1 /a 
2a 
Figure 8.20 The Gaussian and its Fourier Transform 
Figure 8.21 The Gaussian and its Fourier Transform in the Limit a + 
00 
Thus the product of the two is a constant: 
AhoAt = 2. 
(8.102) 
In general, for any functional shape, you will find that A wAt = c, where c is a constant 
determined by both the functional form and the precise definition of the “width” of a 
function. In the wave theory of quantum mechanics, this is the mathematical basis of 
the Heisenberg uncertainty principle. 
The Gaussian can also be used to obtain the Fourier transform of the Dirac 8- 
function by looking at the limit of the Gaussian as a ---t 00. In this limit, as can be 
seen from Figure 8.21, the Gaussian goes to 8(t) and its Fourier transform goes to 
1/&. 
8.6.4 
Periodic Functions 
There is no formal Fourier transform for a periodic function, because the integral of 
Equation 8.20 cannot be performed. This limitation vanishes, if you are willing to 
allow Dirac 8-functions to appear outside of integral operations. 

214 
FOURIER TRANSFORMS 
As an example, consider the function f ( t )  = sin t. Its Fourier transform is given 
by 
(8.103) 
where we have made use of the informal orthogonality condition, Equation 8.18. 
Figure 8.22 shows this transform pair. 
Fourier transforms which contain 8-functions, such as Equation 8.103, can be 
viewed as an alternative way of expressing the traditional Fourier series. Essentially, 
the 6-function converts the inverse Fourier integral into a Fourier series summation. 
For example, the inverse Fourier transform of the sine function is 
1
"
 
f ( t >  = - 
J2?r l m d w e i d  { i f i  [6(0 + 1) - 6(0 - l)] 
m 
n=-m 
(8.104) 
where, for this function, all the 
are zero except for c, = 1 /(2i) and c- = - 1 /(2i). 
This function has a periodicity of To = 2.rr and so o, = n. This is simply the Fourier 
sum for the sine function. 
Figure 8.22 Fourier Transform of sin t 

FOURIER TRANSFORMS - 
EXAMPLES 
275 
Consequently, if you are willing to consider Dirac &functions outside of integral 
operations, the Fourier transform of periodic functions can be taken. In general, this 
transform will be a series of 8-functions which, when inserted into the inversion 
integral, generates the normal Fourier series. With this interpretation, a separate 
formalism for the Fourier series is unnecessary, because all the properties of the 
Fourier series are included in the Fourier transform! 
8.6.5 An Infinite 'hain of 6-Functions 
An interesting transform to consider is that of an infinite train of 6-functions: 
m 
(8.105) 
Because this is a periodic function, its Fourier transform must be composed of a sum 
of &functions. This is, however, a fairly difficult function to transform and must be 
done in steps. 
We begin by taking the Fourier transform of a finite train of 6-functions: 
mo 
fm,(t> = C 6(t - nTo). 
This function is absolutely integrable in the sense of Equation 8.23, and thus has a 
legitimate Fourier transform: 
(8.106) 
n=-m, 
- 
- 1 
5 [I 
dt ePior6(t - nT,) 
J27. 
n=-m,, 
1 
m' 
- 
- - c 
e-ionToa 
J27. n=-m. 
(8.107) 
Because fm,(t) is a real, even function of t ,  its transform is pure real for all values 
of w. 
The frequency dependence of this Fourier transform can be interpreted graphically. 
The transform is a sum of phasors, each of unit magnitude and at an angle of - wnT, 
in the complex plane. One such phasor is shown in Figure 8.23. With o = 0 all the 
phasors are at an angle of zero radians, as shown in Figure 8.24. From this figure, it 
is clear that the result of the sum is a single phasor given by 
2m0 + 1 
Lo(@ 
= 0) = ___ 
6' 
(8.108) 

276 
FOURIER TRANSFORMS 
I 
Figure 8.23 One of the Phasors in the Fourier Transform of a Finite Train of &Functions 
m, 
I 
-2 
-1 
0 
1 
2 
I
__c 
n=-m, 
__c ..... 
( 2 4 +  1) 
real 
I 
Figure 8.24 
Fourier Transform for a Finite Train of &Functions Evaluated at w = 0 
As o increases from zero, the phasors in Figure 8.24 begin to curl up. The phasor for 
n = 0 is still at zero radians. The other phasors at +_n are at angles of T onT,. Because 
of the symmetry, the result is still a single phasor with a phase of zero radians, but 
its magnitude has decreased, as shown in Figure 8.25. When o = 27r/T0, all the 
phasors are. again aligned at zero radians so that &,(w = 27r/T0) = &,(o = 0). 
The Fourier transform repeats this maximum when wT, is any integer multiple of 2rr. 
A closed form expression for F-Jw), 
for arbitrary values of w, can be obtained 
graphically. To see this consider a simplified version of the phasor sum 
(8.109) 
Figure 8.25 Fourier Transform for a Finite Train of &Functions as w Increases from Zero 

FOURIER TRANSFORMS - 
EXAMPLES 
277 
E 
F 
Sint(2mo+ 1)x/21 
A 
J 
P 
Figure 8.26 Phasor Summation 
Equation 8.109 is a sum of 2m, + 1 phasors, each one of unit magnitude and at 
an angle of x radians relative to its predecessor in the complex plane. Figure 8.26 
shows a geometric method for evaluating this sum. A point P is chosen which is 
equidistant from every intermediate point on the diagram. This distance is easy to 
determine, because the length of segment 
is one, and the angle LAPB is x. This 
makes AP = 1/12 sin(x/2)]. Then, because angle LAPJ = (2m, + l)x, segment 
m, which is the sum we seek, is given by 
. 
sin [x(2m0 + 1)/2] 
m, 
- 
AJ = 
elnx = 
n=-m, 
sin(x/2) 
(8.1 10) 
Putting the result of Equation 8.1 10 back into Equation 8.107 shows that the Fourier 
transform of a finite train of 6-functions is given by 
(8.1 11) 
Figure 8.27 shows a graph of this function form, = 3. 
We obtain the transform of an infinite train of &functions by taking the m, - 
~0 
limit of Equation 8.1 11. Notice, as m, increases, the height of each peak in Figure 8.27 
increases, while the distance between the peak value and the first zero shrinks. The 
distance between peaks remains a constant. The frequency of the oscillations in 
between these peaks increases without limit, so their effect cancels out inside of any 
integral, much like the sinc-sequence function described in Chapter 5. 
It looks like we have all the characteristics of an infinite train of &functions! In 
order to validate this, we need to evaluate the integral area of one of the peaks. This 

278 
FOURIER TRANSFORMS 
TO 
First 
at 
= T0(2m0 +1) 
Figure 8.27 Fourier Transform for a Finite Number of &Functions 
is given by 
(8.1 12) 
sin[oTo(2mo + 1)/2] 
J 
do 6 
sin( oTo / 2 )  ' 
- d T 0  
Area = lim 
m,+m 
An easier method than grinding through the integral is to consider the triangular area 
shown in Figure 8.28. The magnitude of this area is G / T 0 .  As ma increases, the 
area of the triangle is a better and better approximation to the area of the integral. In 
the limit, they are the same. 
Thus, the Fourier transform of an infinite train of unit area &functions separated 
by A t  = To is another infinite train of 6-functions of area &/To, 
separated by 
A o = 27r/TO: 
(8.1 13) 
Figure 8.28 Triangular Solution for the &Function Area no 

FOURIER TRANSFORMS - 
EXAMPLES 
279 
Area 
..... 
..... _Illt 
I 
TO 
..... l l L  II 
..... 
w 
- 
Figure 8.29 Fourier Transform of an Infinite Train of &Functions 
rhis is shown in Figure 8.29. Notice that as the &functions get closer together in t ,  
he &functions of the spectrum move farther apart. 
3.6.6 
The transform of an infinite train of &functions can be coupled with convolution to 
find the Fourier transform of any periodic signal. Consider the function h(t), which 
is a periodic train of square pulses, as shown in Figure 8.30. The pulse width is to, 
;he pulse height is 1, and the distance between the start of one pulse and the start of 
:he next pulse is To. Because h(t) is a periodic function, its Fourier transform H ( w )  
must be composed of &functions and can be obtained using a Fourier series integral. 
However, h(t) can also be looked at as the convolution of two other functions 
Transform of a Periodic Function Using Convolution 
where f ( t )  is a single square pulse of height 1 and width to, centered at t = 0, and g(t) 
is an infinite train of unit area &functions separated by To, as shown in Figure 8.31. 
Because h(t) is the convolution of f ( t )  with g(t), its Fourier transform is just J2.r 
Limes the product of F(w) and G(w). These Fourier transforms have already been 
jetermined in previous examples: 
- 
I 
to 
To 
Figure 8.30 Periodic Train of Square Pulses 
(8.1 15) 

280 
FOURIER TRANSFORMS 
to 
TO 
Figure 831 The Signals f ( t )  and g(t) which Convolve to Form h(t) 
(8.1 16) 
Consequently, 
2
6
 
sin(wt0/2) 
OC 
- 
H(w) = - 
[ 
] 
6(w - 2 m / T O ) .  
(8.1 17) 
T O  
n=--m 
This is an infinite train of 6-functions with a 6-function located at every w = 2nn/T0, 
each with an area weighted by the sinc function. If it is assumed that the pulse width 
is small compared to the period, i.e., to < To, the spectrum for this function looks as 
shown in Figure 8.32. 
8.6.7 Periodic Burst 
In the real world, there is no such thing as a periodic signal going on for all time. Real 
signals must start and stop, even if they look periodic over a long interval of time. 
I 
Figure 8.32 Spectrum for a Train of Square Pulses 

FOURIER TRANSFORMS -EXAMPLES 
281 
Figure 8.33 A cos(o,t) Burst 
The spectrum for signals of this type can be obtained using convolution techniques. 
Consider the function 
cos(wot) 
-to < t < to 
otherwise 
h(t) = { 
(8.1 18) 
shown in Figure 8.33. We could calculate the Fourier transform of this signal using the 
standard Fourier integral. The transform is more easily obtained, however, by realizing 
that h(t) is the product of two functions whose transforms are already known. That 
is, h(t) = f(t)g(t), where f ( t )  is a square pulse of width 2t0 and unit amplitude, and 
g(t) is cos(w,,t). We already know the transforms of these two functions are 
2 sin(wto) 
F(w) = -~ 
- 
G ( w )  = fi 
[6(0 - wo) + 6(w + w,,)] . 
T
r
w
 
(8.119) 
- 
J 
Because h(t) = f(t)g(t), 
- 
1
"
 
- 
dw' F(w')G(w - w'). 
(8.120) 
Figure 8.34 shows a diagram of this convolution operation. 
In this convolution, G(w - w') is the sum of two 6-functions. This makes the 
convolution integral relatively easy, because the 6-functions will just sift out the 
value of F(w) where they overlap. If the expressions for F(w) and C(w) are inserted 
into Equation 8.120, the sifting property of the 6-functions results in 

FOURIER TRANSFORMS 
282 
Figure 8.34 Convolution Picture for the Burst Function 
Figure 8.35 
Transform of a Burst Signal 
as shown in Figure 8.35. Notice how each 6-hction creates a shifted version of 
- 
F(w), and the two are summed to giveH(w). If wo is much larger than 2m/t0, the two 
copies of F(w) are separated and are essentially distinct. On the other hand, if wo is 
on the order of 2m/t0, there is signiticant overIap. 
This transform has some interesting limits. One of these limits is when to + a. 
In this case, the burst signal becomes more like an ideal periodic cosine signal. As 
you would expect, the pair of sinc functions in the transform become more like S 
functions. Figure 8.36 depicts this limit. A second interesting limit is when wo + 0. 
Figure 836 Burst Transform as to + w 

FOURIER TRANSFORMS -EXAMPLES 
283 
Figure 8.37 Burst Transform as w, -+ 0 
In this limit, h(t) essentially becomes a square pulse, because cos(w,t) + 1. As you 
would expect from Equation 8.72, the transform approaches a single sinc function 
centered at w = 0. This is shown in Figure 8.37. 
8.6.8 Exponential Decay 
Consider the exponentially decaying function 
(8.122) 
where a, is a real, positive number, as shown in Figure 8.38. This function is 
absolutely integrable, and its Fourier transform is easy to calculate: 
dt e-iwre-aor 
F(w) = - 
- 
6
0
 
Jm 
(8.123) 
Notice that this transform has both a real and an imaginary part. This is because f ( t )  
has neither odd nor even symmetry. 
Figure 8.38 Exponential Decay 

284 
FOURIER TRANSFORMS 
.i 
Figure 839 Inversion of Exponential Transform in the *-Plane 
Now consider the inverse transform back to f ( t ) :  
1
"
 
iot 
=-I 
d o - .  
2m' 
--m 
o - ia, 
(8.124) 
In Equation 8.124, o is a pure real variable. The integral is easily calculated by 
extending the integral into the complex @-plane, and then closing the contour. The 
complex integral is 
igt 
1 
f ( t )  = - 1 do -, 
2m 
g - ia, 
(8.125) 
where the Fourier contour F is along the real axis, as shown in Figure 8.39. A single, 
first-order pole sits at o = ia,. 
If t > 0, we can get away with closing the contour using a semicircle of infinite 
radius even though the denominator of the integrand falls off only as fast as 1 /o. 
To 
justify this, we must show that the contribution along the path n, shown in Figure 8.40, 
vanishes as its radius goes to infinity. On n, g = Roeie and d g  = iR,eiedO, so 
igf 
iRoeieeiR0t cos Be-Rot 
sin 0 
d w -  
= lim 
@ - l a ,  
Ro-+m 
Roeie - ia, 
Figure 8.40 Upper Half-Plane Semicircle for Fourier Inversion 

FOURIER TRANSFORMS - 
EXAMPLES 
'TT 
iRoeieeiRot 
cos Be-R,t 
sin 8 
Roeie 
= lim 1 d0 
R,-m 
Now consider the magnitude of the contribution from fl : 
285 
(8.126) 
(8.127) 
The second step in this sequence comes from the fact that I ieiRotcose 
I = 1, while the 
last step is true because the function is symmetric about 0 = ~ / 2 .  
There is an important result, called Jordan's inequality, which can be used at this 
point. The inequality states that, for t > 0, 
(8.128) 
This is proved quite easily by noticing that sin 0 2 2 0 / ~  
for 0 5 0 5 ~ / 2 .  
Using 
this result in Equation 8.127 gives 
= 0. 
(8.129) 
This shows that for t > 0, the contribution from fl vanishes as the radius grows 
infinite, as long as Eb) 
falls off as 1 /lo1 or faster. 
For t < 0, we cannot close the contour in the upper plane as before because the 
integrand diverges for large, positive imaginary numbers. Instead, we must change 
the contour to close in the lower half plane. By arguments similar to the ones given 
above, you can show there is no contribution added when this contour is closed. That 
is, 
(8.130) 
where the contour U is a semicircle of infinite radius in the lower half g-plane. 
Consequently, the integration of Equation 8.125 can be performed by closure 
(8.131) 

286 
FOURZER TRANSFORMS 
Figure 8.41 
Closure of the Fourier Inversion Integration 
where the contour C is closed in the upper half @-plane for t > 0, and in the lower 
half @-plane for t < 0, as shown in Figure 8.41. 
For each contour, the integral can be determined by adding up the residues of 
the enclosed poles. The t < 0 contour has no enclosed singularities, and the t > 0 
contour has just one, with a residue of e-%'. Thus our final result is 
(8.132) 
which is our original function. 
Now, consider what happens as a, approaches zero. This makes the f ( t )  look 
more and more like a step function, as shown in Figure 8.42. For any a, bigger than 
zero, we can obtain a valid Fourier transform, but if q, = 0, we know we cannot take 
the Fourier transform, because the integral 
1
"
 
F(w) = -l dte-id 
2Tr 
(8.133) 
does not exist. 
1 
Figure 8.42 
The Exponential Function in the Limit of a, ---t 0 

FOURIER TRANSFORMS - 
EXAMPLES 
L - 
3' 
287 
imag 
@plane 
" a0 
11 
real 
- - 
LJ 
By looking at what happens to the inversion contour for this function in the a, + 0 
limit, we can gain a valuable alternative viewpoint. As long as a, is greater than zero, 
no matter how small, the pole of the integrand of Equation 8.125 is in the upper half 
- 
o-plane, and the F contour can remain on the real @-axis. But when a, = 0, the pole 
sits right on top of the F-contour and the integral cannot be performed. 
Notice, however, that if the T-contour is deformed to T' by dodging the pole, as 
shown in Figure 8.43, the inversion integral 
reproduces the unit step function 
0 
t < O  
1 
t > O  
f(t> = { 
(8.134) 
(8.135) 
This is no coincidence, and it will lead us to a more general integral transform called 
the Laplace transform. 
8.6.9 Damped Sinusoid 
As a last example, consider the damped sinusoid 
(8.136) 
where a, and o, are both positive numbers. Figure 8.44 shows this function. The 
Fourier transform is easy to calculate: 
(8.137) 

288 
FOURIER TRANSFORMS 
Figure 8.44 The Damped Sinusoid 
where g, = w, + ia, and g2 = -0, + ia,. Notice f ( w )  is a complex function 
because f ( t )  has neither odd nor even symmetry. 
The inversion transfom of Equation 8.137 is 
As with the exponential decay function of the previous section, the best way to 
evaluate this integral is to extend it into the complex plane, and close the complex 
contour. The complex version of the inversion integral is 
(8.139) 
where 7 is the Fourier contour along the real @-axis, as shown in Figure 8.45. Also, 
like the exponential decay example, we are forced to close the contour in the upper 
half-plane for t > 0 and the lower half-plane for t < 0. This is due to the influence 
of the eig* term in the numerator. As you will prove in one of the exercises at the end 
of the chapter, Equation 8.139 returns the original function f ( t ) ,  
as long as a, > 0. 
(8.140) 
Figure 8.45 Inversion of the Damped Sinusoid Transform in the g-Plane 

FOURIER TRANSFORMS - 
EXAMPLES 
F‘ 
- 0 0  
-
A
 
289 
*plane 
imag 
“0 
h a g  
- - 
- 
I 
Figure 8.46 The Undamped Sinusoid 
The Fourier transform and inversion for the damped sinusoid work correctly for 
all values of a, > 0, no matter how close to zero a, gets. Exactly at a, = 0, however, 
we know we cannot have a valid Fourier transform, because now f ( t )  becomes 
(8.141) 
as shown in Figure 8.46. The sinusoid is no longer damped, and the Fourier integral 
does not exist. This problem is also evident from looking at the inversion contour. 
With a, = 0, the Fourier transform in Equation 8.137 has become 
F(w) = - 
wJJ2.rr 
(8.142) 
(w - WoXW + 00)’ 
and the two poles in Figure 8.44 now lie on the Fourier contour. 
sinusoid 
Notice, however, Equation 8.142 still can be used to generate the undamped 
f(t) = - 
d w  eiot 
-00/J2.rr 
(8.143) 
if the Fourier contour is modified to y’, as shown in Figure 8.47. In fact, even an 
exponentially growing sinusoid, 
AL. 
- 
(w-w0)(4!+ wo)’ 

290 
3' 
- 0 0  
-
I
 
- 
v 
-% 
FOURIER TRANSFORMS 
imag 
*plane 
00 
real 
- 
- 
- 
- 
Figure 8.48 Exponentially Growing Sinusoid 
(8.144) 
such as the one shown in Figure 8.48, can be recovered from this inversion, if we let 
the Y'contour dip below the poles of Em, as shown in Figure 8.49. 
These deformed contours seem to extend the Fourier inversion into realms where 
you would not expect it to work. In the next chapter we formally introduce these 
extensions as the basis for the Laplace transform. 
8.7 THE SAMPLING THEOREM 
One of the more important results of Fourier transform theory is the Sampling The- 
orem. This theorem determines at what rate a signal must be sampled so enough 
information is available to properly reproduce it. This is important, because sampling 
too infrequently will cause the reproduced signal quality to degrade, while requiring 
a sampling rate which is too high is inefficient. 
Consider sampling an audio signal for recording onto a compact disc. Let the 
original microphone signal u(t) look as shown in Figure 8.50. Furthermore, let this 
analog signal have a Fourier transform &w), as shown in Figure 8.5 1. This spectrum 
vanishes above and below some cutoff frequency om, which is perhaps imposed by 

THE SAMPLING THEOREM 
291 
Figure 8.50 The Original Analog Audio Signal 
the mechanics of the microphone, For the purposes of this discussion, the transform 
will be assumed to be pure real. This assumption is not necessary for the validity of 
the Sampling Theorem, but is convenient for our drawings. 
A useful way to model the sampling process, and the signal recorded on the CD, 
is to let the sampled signal, which we will call a,(t), be equal to the original signal 
times a periodic set of &functions 
m 
as(t) = a(t) c s(t - nTo). 
The time between the &functions is the sampling interval. Figure 8.52 shows this 
representation. 
Because a,(t) is the product of two functions, the spectrum of a,(t), which we will 
call A,(o), is the convolution of A(w) with the transform of the train of $-functions: 
(8.145) 
"= -m 
J2?r 5 
6(o - 2nn/T0). 
(8.146) 
1 
A,(W) = -A(w) 
@ - 
6 
To 
-m 
When the sampling rate is very high, that is when To G 2?r/wm,, A,(o) has the form 
shown in Figure 8.53. The spectrum of as(t) is not the same as the spectrum of a(t), 
because it has many more high frequency components. If it were reproduced through 
-%ax 
%lax 
Figure 8.51 Fourier Transform of the Original Signal 

292 
FOURIER TRANSFORMS 
Figure 8.52 
The Sampled Signal 
an amplifier and speaker system, it would not sound like a(t). However, sitting in the 
spectrum of a&), from -w,- to +w,-, is the exact spectrum of a@). Thus, if we 
send this signal through a low-pass filter before amplification, the result should be a 
sound that reproduces a(t). 
A simple low-pass filter can be made from the capacitor and resistor circuit shown 
in Figure 8.54. The input voltage v d t )  and output voltage vout(t) are related by the 
differential equation 
(8.147) 
Applying a Fourier transform to both sides of this equation gives an algebraic equation 
relating the Fourier transforms of the input and output waveforms: 
K.Jw) = (iwRC + l)V-,ut(m) 
(8.148) 
01 
(8.149) 
0 
- 
-2f10 
2 f l O  
4f10 
Figure 8.53 Spectrum of Sampled Signal 
2anU 
- 4 f l O  

THE SAMPLING THEOREM 
293 
R 
+
-
+
 
Figure 8.54 Simple Low-Pass Filter 
The quantity V-,,(w)/~.,(o) is called the transfer function for the filter. The 
magnitude of this function is shown in Figure 8.55. For frequencies much less than 
w = 1/RC, the output voltage and input voltage are essentially identical. For fre- 
quencies much above o = I/RC, the output is zero. This transition frequency is 
called the cutoff frequency of the filter, wco. For a simple RC filter, such as this 
one, the transition at the cutoff is not very sharp. Better filters can be made with 
a more complicated combination of components, or with more sophisticated digital 
processing components. The ideal low-pass filter has the transfer function shown in 
Figure 8.56. 
If we pass the sampled signal through an ideal low-pass filter with o,, = om, 
the output of the filter will have a Fourier transform L&(w) that is exactly equal to 
- 
A(o), and consequently the signal coming out of the filter af(t) will sound exactly 
like the original a(t) signal from the microphone. The filter has removed the spurious 
high frequency components introduced by the sampling. This process is shown in 
-1mc 
j 
1mc 
Figure 8.55 Transfer Function for a Simple RC Low-Pass Filter 
-
0
0
 
I 
0, 
Figure 8.56 Ideal Low-Pass Filter 

294 
FOURIER TRANSFORMS 
Low Pass 
-1-J 
Figure 8.57 Frequency and Time Domain Pictures of the Filtering Process 
Figure 8.57. The top picture in Figure 8.57 shows the filtering process in the frequency 
domain, while the bottom picture shows the same process in the time domain. 
In the previous arguments, we made the important assumption that the original 
signal was sampled at a high rate. The spectrum for AJo) is composed of a series 
of A(o) spectra replicated every o = 2n?r/T0. As long as r/To > w,-, 
these 
spectrum do not overlap, and our previous analysis works fine. But when To grows so 
large that r / T ,  < (l)mar, the individual spectra overlap each other and &(w) looks as 
shown in Figure 8.58. Now, the output of the low-pass filter with w,, = o,, 
gives 
the spectrum shown in Figure 8.59. In this case the signal coming out of the filter 
would no longer sound like the signal from the microphone. 
Thus we see, in order to preserve the information in a signal, it must be sampled 
such that r/T0 > (l)mar. This means the sampling frequency 2r/T0 must be at least 
twice the highest frequency present in the sampled signal. The minimum sampling 
frequency is often called the Nyquist sampling rate. 
Keep in mind that we have drastically simplified the process of CD sampling in 
our previous argument. Some practical issues that introduce complications should 
be pointed out. First, in real CD sampling, the recorded audio signal is digitized 
..... 
..... 
Figure 8.58 Spectrum of an Undersampled Signal 

EXERCISES 
295 
-% 
%ux 
Figure 8.59 Spectrum of the Filtered, Undersampled Signal 
and can take on only discrete values. This requires the use of an analog-to-digital 
converter (ADC). 
The recreation of the original signal then requires the inverse step 
of a digital-to-analog conversion (DAC). Second, for an audio signal, the practical 
limit for wmx is about 20 kHz, the frequency limit of the human ear. The music 
industry has settled on a 44 Khz sampling standard, just above twice the maximum 
frequency we can hear. However, the microphone signal and its electronic handling 
before sampling may very well have frequency components well above 22 kHz. To 
prevent the overlap in the convolved spectra for &(o) that would result by sampling 
such a signal, a low-pass filter with a cutoff frequency near 22 lcHz is inserted just 
before the sampling electronics. Finally, there is no such thing as an ideal filter with 
a transfer characteristic like that shown in Figure 8.56; there will always be some 
“rolloff.” The better the filter, the better the sound reproduction. 
EXERCISES FOR CHAPTER 8 
1. What is the Fourier Transform of the function 
where a is real and positive? 
2. Find the Fourier transfonn of the function 
Is there a way to evaluate this without actually calculating the transform integral? 
3. Find the Fourier transform of the signal shown below, 

2% 
FOURIER TRANSFORMS 
-a 
(a) by directly calculating the transform integral. 
(b) by differentiating the signal and writing the transform in terms of transforms 
4. In this chapter, we determined the Fourier transform of the Dirac S-function. 
that were already derived in this chapter. 
Find the Fourier transform of the derivative of the Dirac 6-function, 
Compare these two transforms. 
5. Find the Fourier transform of the function 
(a) by directly calculating the transform integral using integration by parts. 
(b) by differentiating the signal three times and writing the transform in terms 
of the sum of two simpler transforms. 
6. The Fourier transform of f ( t )  is 
- 2  
= o2 - . 
12aw - a2' 
Determine the function f ( t )  and make a plot of it for positive and negative values 
oft. In the limit of a -+ 0, what happens to E(o) and f(t)? 
7. Obtain the Fourier transform of the function 
sin [o(t - T)] 
- 4 t - T )  
t > T 
f(t) = { 0 
t < r '  
Show that the inversion of this Fourier transform requires a slight modification 
to the standard t > 0, t < 0 closure semicircles discussed in this chapter. 

EXERCISES 
8. Consider the Fourier inversion 
eikx 
f(x) = s_m_dk - 
k - i ’  
where x and k are pure real variables. 
(a) For x > 0, determine how this integral can be closed in the complex &-plane 
(b) For x < 0, determine how this integral can be closed in the complex &-plane 
and evaluate f(x). 
and evaluate f(x). 
9. Prove that the inversion of Equation 8.139 gives Equation 8.140. 
10. Given the functions f ( t )  and g(t) shown below, 
(a) Make a labeled sketch of f ( t )  @ g(t) . 
(b) Make a labeled sketch of g(t) @ g(t) . 
(c) Make a labeled sketch of the autocorrelation of g(t). 

298 
FOURIER TRANSFORMS 
11. Consider the two functions 
-1 < t < +1 
otherwise 
g(t> = t e-12. 
(a) Plot f ( t )  and g(t) for all t. 
(b) Find an expression for h(t) where h(t) = f ( t )  0 
g(t). 
(c) Plot h(t). 
12. Consider the two functions: 
Make a labeled sketch of f ( t )  convolved with g(t). 
13. Let f ( t )  be a square pulse 
-T0/2 < t 
+T0/2 
= { i 
otherwise 
and g(t) be made up of a delayed version of f ( t )  plus a sinusoidal function of 
time: 
g(t) = f ( t  - lOT,) + sin(w,t), 
where w, = 207r/T,. Make a labeled sketch of the cross-correlation of f ( t )  with 
m. 
14. Show that the convolution integral is symmetric. That is, prove 
1: dT f ( M t  - 7) = 
dT g(T)f(t - 7). 
1: 
15. Show that if both g(t) and f ( t )  are zero for t < 0, then the convolution of the 
two functions g(t) 0 
f ( t )  is also zero for t < 0. 
16. Find the Fourier transform of the cross-correlation of f(t) with g(t) in terms of 
the Fourier transforms of the individual transforms F(w) and G(w). 
17. Use the fact that the Fourier transform of a Gaussian is a Gaussian, 

EXERCISES 
299 
to evaluate the integral 
1
"
 
h(t) = - 
~ 1" 
d r  e-' 
e-(t-T'2. 
On a single graph plot h(t) and Ft2. 
below. 
18. The function f ( t )  is a periodic train of Gaussian pulses with period T ,  as described 
m 
t 
-2T 
-T 
T 
2T 
This function can be looked at as the convolution of a single Gaussian pulse 
with a train of Dirac &functions. Using this, determine the Fourier transform of 
f(t). 
19. Find the Fourier transform for the periodic function f ( t )  shown below. 
I 
-2T 
-T 
T 
2T 
6 
Look at f(t) as the convolution of two functions. 
20. Find and plot the Fourier transform for the periodic function f ( t )  where 
e-"J sint 
f (t + 6 0 ~ )  
0 < t < 67r 
67r < t < 607r . 
all t 

FOURIER TRANSFORMS 
21. To evaluate real definite integrals, we have often used the technique of closure 
in the complex plane. We used a similar process to invert several of the complex 
Fourier transforms developed in this chapter. To be able to use closure, the 
functions being integrated must meet certain conditions. Show that the conditions 
for closure are different for a real, definite integral than they are for a Fourier 
inversion. 
For this problem, consider the function: 
0 2  
F ( 0 )  = - 
0 3  + 1 '  
(a) Show that, if the integral 
is extended into the complex g-plane, closure is not possible. 
(b) Show that the inverse transform 
f ( t )  = Lrn 
dx eim'F(o) 
can be closed in the complex g-plane. 
22. Ultrafast lasers have pulse widths that are on the order of 50 femptoseconds (a 
femptosecond is 
seconds). This pulse width is difficult to measure since 
the signal is so short that it cannot be displayed directly on an oscilloscope. It is 
often measured using an autocorrelation technique with an apparatus similar to 
the one shown below: 

EXERCISES 
Output Signal 
t 
Rowrtion~ to 
301 
oscillating 
Comer 
Reflector 
The original pulse is incident from the left on a 45 ', 50% mirror. A 50% mirror 
reflects half of the incident signal and transmits the other half. The 100% mirror 
is at a fixed position and reflects all of the incident light. The comer reflector 
reflects all of the light back along the incident direction, regardless of the angle 
of incidence. This reflector is mounted on a stage that oscillates back and forth at 
a frequency of 10Hz. The split signal is then recombined at a detector, which is 
usually a photomultiplier tube or a fast LED detector. The detector generates an 
electrical signal which is proportional to the total number of photons in several 
pulses of the combined [A(t) + B(t)J2 signal-i.e., 
it integrates the intensity of 
the signal over an interval long compared to the laser pulse width, but short 
compared to the period of the oscillating stage. Notice how the motion of the 
oscillating mirror causes the relative position of B(t) compared to A(t) to change. 
The output of the detector is applied to the vertical deflection of an oscilloscope, 

302 
FOURIER TRANSFORMS 
while the signal that drives the position of the comer reflector is applied to the 
horizontal deflection. 
(a) How far does light travel in 50 fs? For red light, how many oscillations of 
the electric field occur in 50 fs? 
(b) Assume a train of 50 fs pulses is incident on this device, and one pulse is 
separated from the next by 0.01 ps. Assuming the motion of the oscillating 
reflector and the display of the oscilloscope are adjusted so the B(t) signal 
moves from 200 fs before A(t) to 200 fs after A(t), what does the trace on 
the oscilloscope look like? To keep things simple, assume the pulse shape is 
square. 
(c) How should the 100% mirror and the corner reflector be positioned with 
respect to the 50% mirror so that the operation in part (b) above is obtained? 
(d) How is the width of the signal on the oscilloscope related to the pulse width 
of the incident signal? Treat the motion of the reflector as linear in time in 
the region where the two signals overlap. 

LAPLACE TRANSFORMS 
n this chapter, the Laplace transform is developed as a modification of the Fourier 
ransform. The Laplace transform extends the realm of functions which can be trans- 
ormed to include those which do not necessarily decay to zero as the independent 
rariable grows to infinity. This is accomplished by introducing an exponentially de- 
:aying term in the transform operation. The inverse Laplace operation can be viewed 
is a modification of the inverse Fourier transform, with the Fourier contour moved 
~ f f  
the real w-axis into the complex @-plane. 
B.1 
LIMITS OF THE FOURIER TRANSFORM 
We can motivate the development of the Laplace transform by examining a func- 
ion which clearly has no Fourier transform. Consider the ramp function shown in 
3gure 9.1 and described by 
rhis function does not go to zero as t goes to infinity and thus does not have a Fourier 
ransform. 
However, from the previous chapter, we know that a derivative operation in the 
ime domain is equivalent to multiplication by the factor of iw in the transformed 
requency domain. In other words, 
(9.2) 
303 

304 
LAPLACE TRANSFORMS 
I 
1 
t 
__ 
I 
1 
Figure 9.1 The Ramp Function 
Notice that the ramp function given by Equation 9.1 is related, by two derivatives, to 
the delta function, 
(9.3) 
as shown in Figure 9.2. In the previous chapter, we showed that the &function has 
the simple Fourier transform: 
1 
df(t)/dt 
i d?(t)/dt2= 6(t) 
1 
Area= 1 1 
- 
~ = - " ' " C w )  
Figure 9.2 Relationship Between the Ramp and a(?) 
(9.4) 

305 
LIMITS OF THE FOURIER TRANSFORM 
So if we suspend our rigor for a moment, we can naively suppose Equation 9.2 lets 
us write the transform of the step function of Figure 9.2 as 
-i 
w f i '  
(9.5) 
Applying Equation 9.2 one more time, with reckless abandon, gives the Fourier 
transform of the ramp function as 
-1 
w2J2.rr' 
(9.6) 
Of course, we know neither the step function nor the ramp have legitimate Fourier 
transforms. This is confirmed by examining the inverse Fourier integral of the trans- 
form functions given in Equations 9.5 and 9.6. The inversion for Equation 9.5 is 
and the inversion for Equation 9.6 is 
(9.7) 
Neither of these integrations can be performed because of the divergence of the 
integrands at w = 0. If you look at the representations of these integrals in the 
complex 9-plane, you will notice that a pole sits right on the Fourier contour. 
This is the problem we examined briefly at the end of the previous chapter. By 
modifying the contour to dip below the offending pole, as shown in Figure 9.3, 
we actually can recover the original function from the transform function given 
in Equation 9.6. We will show this in the next section. Our sloppy derivation of the 
ramp function's transform must contain some grain of truth. By examining the inverse 
process a little more closely, we can arrive at the basis for the Laplace transform. 
Second-Order Pole of 
imag 
elcot 
g-plane 
o2 
/- 
\ 2n 
- 
\ 
real 
- 
A 
- 
3 
Figure 9.3 A Modified Fourier Contour for the Ramp Function 

306 
LAPLACE TRANSFORMS 
9.2 THE MODIFIED FOURIER TRANSFORM 
In this section, we show how the standard Fourier transform can be modified by 
moving the inversion contour off the real axis. With this approach, we can transform 
functions that would not normally possess valid Fourier transforms. We start with 
some of the ideas introduced at the end of the previous chapter. 
9.2.1 The Modified Transform Pair 
In the previous section, we indicated that the ramp function of Equation 9.1 could be 
recovered from the transform given in Equation 9.6 using an inversion integral 
(9.9) 
on the modified contour y' shown in Figure 9.3. This contour lies along the real 0- 
axis, except at the origin where it dips below the pole. The discussion of the previous 
chapter shows that we can use closure in the upper half gplane for t > 0, and in the 
lower half for t < 0. Then, using the residue theorem, it is easy to show that 
t < O  
f ( t ) {  P 
t > O  
(9.10) 
The ramp is recovered by integrating the function given in Equation 9.6 along the 
modified Fourier contour 7'. 
This, however, is not the only path that will recover the ramp function. Any path 
lying in the lower half @-plane will work. Consider the path shown in Figure 9.4 that 
goes along the straight line oi = -p, where oi is the imaginary part of g: 
Second-Order Pole of 
' imag 
- 
w2Jiii 
\ \  
\-\ \ 
(9.1 1) 
%plane 
Figure 9.4 Another Modified Fourier Contour for the Ramp Function 

THE MODIFIED FOURIER TRANSFORM 
307 
Second-Order Pole 
@-plane 
', 
3 
real 
- 
c 
Figure 9.5 
Inversion for the Ramp Function 
This integration is easily performed by defining a transformation of variables with 
- 
w' = g + ip. With this transformation, the modified Fourier contour Y" 
that was 
along wi = -p becomes a standard Fourier contour along the real wi axis: 
(9.12) 
There is a second-order pole at g' = ip, as shown in Figure 9.5. Now standard Fourier 
closure techniques can be applied to recover the same ramp function as before. Notice 
that, with the pole at g' = ip, the ep' factor present in the numerator of Equation 9.12 
is exactly canceled out by the residue. From these arguments it can be seen that the 
modified Fourier contour Y' of Equation 9.9 can be placed anywhere in the lower 
half g-plane, as shown in Figure 9.6. 
The fact that the ramp function can be generated using Equation 9.6 in this inversion 
process raises the question as to whether this transform can be obtained directly from 
the ramp function itself, by extending the regular Fourier transform process into the 
complex g-plane. If we allow w to take on complex values, the Fourier transform 
becomes 
Second-order pole of 
~ F(w) , 
-\ 
Imag 
.. -, 
'.. 
Real 
Convergence region for 
modified Fourier transform - 
of ramp function 
(9.13) 
g-plane 
Figure 9.6 
Convergence Region for the Ramp Transform 

308 
LAPLACE TRANSFORMS 
If g = or + ioi, we can rewrite Quation 9.13 as 
(9.14) 
Notice the integral in Equation 9.14 looks just like a regular Fourier transform of the 
function f(t)eW'. 
Can this process generate Equation 9.6 from the ramp function? Inserting the ramp 
function into Equation 9.14 gives 
(9.15) 
The fact that the ramp function is zero for t < 0 makes the limits of integration range 
from 0 to +co. Integrating by parts, taking u = t and d v  = e-'*'dt, gives 
Both terms of this expression will converge only if oi < 0. In that case, Equation 9.16 
evaluates to 
(9.17) 
Thus, at least for the ramp function, it appears we can define a modified Fourier 
transform pair by 
1 
f ( t )  = - / d g  eio'F@, 
J2.I. F' 
(9.18) 
(9.19) 
with the requirement that the imaginary part of 0 be negative. The convergence region 
for this modified Fourier transform is identified in Figure 9.6. 
9.2.2 Modified Fourier Transforms of a Growing Exponential 
The modified Fourier transform pair given by Equations 9.18 and 9.19 has been 
shown to work for the ramp function. Will it work for other functions that do not 
possess regular Fourier transforms? Consider the exponentially growing function 
(9.20) 

THE MODIFIED FOURIER TRANSFORM 
309 
imag 
@-plane 
real 
Pole of F(o) 
~- ‘- 
.T I- 
\ -a 
I 
-1 
.. 
1. 
Convergence region for 
modified Fourier transform 
Figure 9.7 
Convergence Region for Modified Fourier Transform of the Exponential Function 
where a is a positive, real number. When this function is inserted into the modified 
transform of Equation 9.18, you can easily show, as long as oi < -a, that 
(9.21) 
Figure 9.7 shows this convergence region. Notice, as before, the convergence region 
is required to be below the pole of E(&. 
Let’s investigate the inverse operation, which must return the original function of 
Equation 9.20. Combining Equation 9.21 with the inversion integral of Equation 9.19 
gives 
(9.22) 
The modified Fourier contour y’ for this integration is shown in Figure 9.7. 
The same stunt we used for the ramp function can be applied here. A simple 
variable transformation, 0’ 
= g + ip, converts y’ to a contour on the real axis of 
the g’-plane: 
(9.23) 
Again, the integral in Equation 9.23 can be evaluated by closing the contour. The 
complex exponential forces us to close in the lower half plane for t < 0 and in the 
upper half plane for t > 0. There is a single pole at w’ = i(p - a )  with residue 
P e - 6 ‘ .  Thus. the residue theorem tells us 
(9.24) 
which is indeed the original function. 

310 
LAPLACE TRANSFORMS 
9.2.3 Closure for Contours off the Real Axis 
In the previous two examples, we made a variable change from 0 to g’, 
so the 
F‘ contour was changed into a contour along the real axis. The only reason we 
did this was so we could use the standard arguments developed in Chapters 6 and 
8 to show the contribution of the closing contour was zero. Actually, this step is 
not necessary. We can close directly in the g-plane, if we analyze the contribution 
from the closing contour a little more carefully. This is more work than the variable 
transformation approach, but once this general argument is understood, it simplifies 
the whole process. 
To see how this works, let’s return to the specific integration of Equation 9.22. For 
t < 0, consider the closed contour shown in Figure 9.8. In the limit of Ro --+ 00, the 
straight line segment from A to B is the modified Fourier contour F’ 
of Equation 9.22. 
This contour can be closed, as shown, with the circular segment in the lower @-plane. 
Using the standard closure arguments, it is easy to see the contribution from this 
circular segment as R, + 03 is zero. Because there are no poles of the integrand of 
Equation 9.22 below S’ 
we obtain 
f ( t )  = 0 
t < 0. 
(9.25) 
For t > 0 consider the closed, solid contour shown in Figure 9.9. In the limit of 
R, -+ 
00, the straight line-segment AB again is Y’, the modified Fourier contour. 
The contour is now closed by a circular segment BCDA that is mostly in the upper 
half &-plane, but has sections, BC and DA, that extend into the lower half g-plane. 
The contribution from this BCDA circular segment is also zero, but because of the 
sections BC and DA this fact is more difficult to show. 
g-plane 
real 
Figure 9.8 
Closure for the Modified Fourier Transform for t < 0 

311 
THE MODIFIED FOURIER TRANSFORM 
imag 
Convergence region for 
modified Fourier transform 
Figure 9.9 Closure for the Modified Fourier Transform for t > 0 
To show that this contribution is zero, we begin by identifying the points A and B. 
The point A is located at 0 = - r, - ip, and B is located at g = r, - ip, as shown in 
Figure 9.9. Notice that as R, + 00, r, + R,, while p remains fixed. The integration 
along the total circular segment can be integrated by parts: 
Evaluating the first term on the RHS gives 
As R, and r, + a, 
the RHS of Equation 9.27 goes to zero. The integration along the 
ircular segment can therefore be broken up into three parts: 
igt 
de- 
- 
The second integral, on segment CD, goes to zero as R, -+ 00 by the standard argu- 
nents used for the Fourier transform. This leaves only the integrals along segments 

312 
imag 
LAPLACE TRANSFORMS 
0 
real 
Figure 9.10 Closure of a Modified Fourier Contour 
BC and DA. Consider the BC segment. If g = or + iwi, everywhere along this 
segment w, 2 r, and oi 2 -/3. This implies 
eit(iJ& + h i )  
ept 
eigt 
1 
E* 
(9.29) 
As R, 4 a, the integrand is dropping off as 1/R: while the path length is limited 
to a length of /3. Consequently, as R, ---f to, the contribution from the BC section, 
for any fixed value oft, goes to zero. The same arguments can be made for the DA 
section. Therefore, for t > 0, the integration along the circular segment BCDA goes 
to zero and we can close the contour. The pole at 9 = -ia is inside the contour of 
Figure 9.9, and the residue theorem gives 
1 @ + ia)2 1 = I [wr + i(a + 
f ( t )  = eat 
t > 0. 
(9.30) 
To summarize, we can close the modified Fourier contour for the growing expo- 
nential function with the closed contours shown in Figure 9.10. This approach gives 
the same result as was obtained by first transforming to the @‘-plane. In the rest of 
this chapter, we will take this kind of reasoning for granted as we close different 
contours. 
9.2.4 General Properties of the Modified Fourier Transform 
The ramp and exponential function examples demonstrate the general conditions 
which allow the modified Fourier transform to exist. First, the function to be trans- 
formed must be exponentially bounded. The meaning of that expression is evident 

THE LAPLACE TRANSFORM 
313 
from looking at Equation 9.14. It must be possible to find an wi such that ewir f (t) goes 
to zero as t goes to infinity. If this is the case, the function e"'f(t) will have a regular 
Fourier transform. Second, the function we want to transform must be zero for t < 0. 
Actually, it is quite easy to develop the modified Fourier transform arguments so that 
f(t) must be zero for all t < to, where to is any real number. You can explore this 
idea further in one of the exercises at the end of this chapter. 
The modified Fourier transform of Equation 9.18 will always exist for values of 
- 
w with imaginary parts less than the imaginary parts of all the poles of Em. 
This 
implies the s' inversion contour of Equation 9.19 must also lie below all these poles. 
The inversion can be accomplished by closing above the modified Fourier contour 
fort > 0 and below it for t < 0. 
9.3 THE LAPLACE TRANSFORM 
The standard Laplace transform is really just the modified Fourier transform we have 
just discussed with a variable change and a rearrangement of the constants. The 
variable change involves the simple complex substitution 
s = ig. 
(9.31) 
Because the inversion equation is frequently evaluated using the residue theorem, 
the constants are juggled around so the leading constant of the inversion is 1 /(2m'). 
With these changes, the Laplace transform pair follows directly from Equations 9.18 
and 9.19: 
(9.32) 
(9.33) 
It is implicitly assumed by the form of Equation 9.32 that f(t) = 0 for all t < 0. 
These equations are sometimes written with a shorthand operator notation as: 
F(s) = -w(r)) 
(9.34) 
(9.35) 
The picture of the Laplace inversion in the complex s-plane looks very much 
like the pictures of the modified Fourier transform, except for a rotation of ~ / 2 .  
Figure 9.1 1 shows a typical convergence region and inversion contours. The Laplace 
transform converges in the region to the right of all the poles of E(s). The Laplace 
inversion contour L, which is called the Laplace or Bromwich contour, must lie 
in this convergence region and extends from g = p - i ~0 to s = p + i 00. The 
inversion contour is almost always evaluated by closing the complex integral in 
Equation 9.33. Using similar arguments given for the modified Fourier transform, the 
Laplace contour is closed to the right for t < 0 and to the left for t > 0. Because 1: 

LAPLACE TRANSFORMS 
h a g  
314 
0 
real 
Convergence region for 
to the right of all 
poles of F(2) 
' 
Laplace transform integration 
Figure 9.11 Complex i-Plane for the Laplace Inversion 
lies to the right of all the poles, the inversion always generates an f(t) that is zero for 
t < 0. 
It is the real part of s that allows the Laplace transform to converge for functions 
which do not have valid Fourier transforms. For an exponentially growing f ( t ) ,  as 
long as the real part of 8 is larger than the exponential growth rate of f ( t ) ,  the 
Laplace transform will converge. If f ( t )  is not exponentially growing, but in fact 
exponentially decaying, all the poles of I;@ are to the left of the imaginary axis, and 
the convergence region will include the imaginary z-axis. In a case like this, the entire 
Laplace contour can be placed on the imaginary 8-axis, and the Laplace transform is 
equivalent to a regular Fourier transform, with the exception of the fact that f ( t )  = 0 
for t < 0. This particular limitation will be removed when we consider the bilateral 
Laplace transform at the end of this chapter. Thus, we can really view the Fourier 
transform as a special case of the more general Laplace transform. 
9.4 LAPLACE TRANSFORM EXAMPLES 
In this section, the Laplace transform of several functions will be determined. With the 
exception of the 6-function example, these functions do not have Fourier transforms. 
9.4.1 The &Function 
The function 6(t - to) has a Fourier transform, and if to > 0, it is zero for t < 0. 
Therefore, we should expect its Laplace transform, except for an overall constant, 
to be the same as its Fourier transform with -is substituted for o. This is easily 

LAPLACE TRANSFORM EXAMPLES 
315 
confirmed by performing the transform: 
There is no limit on the real part of g for performing this integration, and therefore 
the region of convergence covers the entire complex $-plane. Consequently, for the 
inversion of the Laplace transform of the &function, the Laplace contour can be 
placed anywhere. That is, any value of p is acceptable. Incidentally, the fact that we 
can let the Laplace contour lie on the imaginary z-axis is the reason the &function 
possesses a Fourier transform. 
The inverse transform that returns the &function is 
(9.37) 
with the Laplace contour L located as shown in Figure 9.12. 
This is one of the few cases where the Laplace inversion cannot be done by closing 
the contour. Notice, however, because the convergence region is the entire complex 
$-plane, the Laplace contour can be placed on the imaginary x-axis. In that case, 
3 = iw and dg = idw, so the RHS of Equation 9.37 becomes 
imag 
real 
(9.38) 
Convergence region for 
Laplace transform integration 
of S(t - to) 
Figure 9.12 Laplace Inversion of the &Function 

316 
LAPLACE TRANSFORMS 
Figure 9.13 The Heaviside Step Function 
This final result comes from the orthogonality relation derived in the previous chapter. 
Equation 9.37 is actually a more general form of that orthogonality relation: 
(9.39) 
9.4.2 
The Heaviside Function 
The Heaviside step function does not possess a Fourier transform. It does, however, 
have a Laplace transform. Recall the definition of the Heaviside function is 
which is shown in Figure 9.13. 
The Laplace transform of this function is 
(9.40) 
(9.41) 
imag 
s-plane 
imag 
Convergence region for 
Heaviside transform inversion 
to the right of pole 
ats=O 
Figure 9.14 Inversion of the Heaviside Laplace Transform 

LAPLACE TRANSFORM EXAMPLES 
317 
This integration can be performed only when the real part of 3, which we call s,, is 
positive. The Laplace inversion of this transform is 
(9.42) 
The integrand has a first-order pole at 5 = 0, with a residue of 1. To be within the 
convergence region, the Laplace contour must fall to the right of this pole. It can be 
closed on the left for t > 0 and to the right for I < 0, as shown in Figure 9.14. The 
evaluation in either case is straightforward and recovers the original step function 
given by Equation 9.40. 
This example shows why the operation described in Figure 8.43 of the previous 
chapter worked. By deforming the Fourier contour to F‘, as shown in that figure, 
the operation was really a modified Fourier transform, equivalent to the Laplace 
transform of this example. 
9.4.3 
An Exponentially Growing Sinusoid 
Consider the exponentially growing sinusoid 
(9.43) 
with a, > 0, as shown in Figure 9.15. Again, this function does not possess a 
traditional Fourier transform. Its Laplace transform, however, is given by 
E(s> = Lrn 
dt e-sfeael sin(o,t). 
This integral exists only when s, > a,. In that region, 
(9.44) 
(9.45) 
Figure 9.15 The Exponentially Growing Sinusoid 

318 
imag 
imag 
LAPLACE TRANSFORMS 
s-plane 
Convergence region for the 
Laplace transform of the 
exponentially growing sinusoid 
1 
Figure 9.16 Inversion of the Laplace Transform for the Exponentially Growing Sinusoid 
where s, = a, - io, and s2 3 a, + io,. The inversion contour must fall to the 
right of the two poles at g, and g2, as shown in Figure 9.16. The integration for the 
inversion, 
generates the original exponentially growing sinusoid, as you will prove in an exercise 
at the end of this chapter. 
This example should be compared with the operation described in Figure 8.47 
of the previous chapter. We can now see why modifying the Fourier contour to dip 
below the poles gave the correct answer. Unwittingly, we were performing a Laplace 
transform! 
9.5 PROPERTIES OF THE LAPLACE TRANSFORM 
As we did with the properties of the Fourier transform, we will discuss the properties 
of the Laplace transform using the two functions f ( t )  and g(t). We assume both 
functions are zero for t < 0, and that they possess the Laplace transforms 
and 
(9.47) 
G(s) = 1 dt e-”g(t). 
(9.48) 

PROPERTIES OF THE LAPLACE TRANSFORM 
319 
These transform exist for the real part of s greater than some value, not necessarily the 
same for each function. Both these transforms can be inverted by integration along 
some Laplace contour 
and 
(9.49) 
(9.50) 
We use two different contours, Lf and L,, to allow for the possibility of two different 
convergence regions. 
9.5.1 Delay 
Consider the function f ( t  - to), which is the function f ( t )  delayed by to. The Laplace 
transform of this delayed function is 
m 
L{f(t - to)} = A dt e-xt f ( t  - to). 
Make the substitution a = t - to, so Equation 9.51 becomes 
L{f(t - to)} = [
:
o
 
d a  e-s(u+'o) f(a) 
(9.5 1) 
(9.52) 
The change in the lower limit in the second step is valid because f ( a )  = 0 for a < 0. 
A delay in time is equivalent to multiplying the Laplace transform by an exponential 
factor. 
9.5.2 
Differentiation 
The Laplace transform of df(t)/dt is 
(9.53) 
The RHS of this expression can be integrated by parts with g = e-3' and dv = 
(df(t j/dt j dt to give 
(9.54) 

320 
LAPLACE TRANSFORMS 
If the real part of .r is large enough to make f(t)e-s' go to zero as t --+ m, then 
L{Y} 
= -f(O)+gE(sJ. 
(9.55) 
Notice the difference between this result and the derivative relationship for the Fourier 
transform. 
9.5.3 
Convolution 
Now consider a function h(t), which is the convolution of f ( t )  with g(t): 
h(t) = 
d r  f(r)g(t - 7). 
L 
(9.56) 
The Laplace transform of h(t) can be written as 
r m  
= 1: dt ePSth(t) 
= 1: dt e-s' 1: 
d r  f (r)g(t - 7). 
(9.57) 
In the second step, we extended the lower limit to -m using the result of Exercise 8.15, 
which says if f ( t )  = 0 and g(t) = 0 for t < 0, then h(t) = 0 for t < 0. Now 
substitute the inverse Laplace transforms for f(~) 
and g(t - T) to convert the RHS of 
Equation 9.57 into 
f (7) 
g(f--7) 
We have been careful to keep the inversion integrations independent by using two 
distinct variables of integration, g' and s". Notice, the inversion for f( T) has been done 
on a Laplace contour Lf, 
which lies within the convergence region for the transform 
of f ( t ) .  The integration variable s' lies on Lf. 
Similarly, the inversion for g(t - r )  has 
been done on a contour L,, which must lie within the convergence region for G(s). 
The integration variable 5'' lies on L,. Figure 9.17 shows this situation, with the Lf 
and L, contours placed as far to the left in their respective convergence regions as 
possible. Remember, the contours are not necessarily in the same location because 
the convergence regions of the two transforms may be different. The convergence 
region for 230 falls to the right of all the poles of IYW. We will show that the poles of 
g(s> are just a combination of the poles of &YJ 
and GO, so the convergence region 

PROPERTIES OF THE LAPLACE TRANSFORM 
321 
Convergence region 
for transform of f(t) 
.- 
imag 
I 
s-plane 
c 
Convergence region 
for transform of g(t) 
Figure 9.17 Convergence Regions and Laplace Contours for f ( t )  and g(t) 
of li[(s) is just the overlap of the convergence regions of E(s) and c(s). The variable 
Because L, is to the right of all the poles of EsJ, and .L, is to the right of all the 
poles of G(s), contour deformation allows us to move both .Lf and Lg to the right 
without changing the values of their integrals. It will be very convenient if we move 
them to the right so that they become a common contour L that passes through the 
point s. Remember s is the point where we want to evaluate HsJ. Figure 9.18 shows 
this new configuration. In this situation, g, s‘, and $‘ will all have the same real part 
which we will call s,. 
For the integrations in Equation 9.58, sro is fixed, but the 
imaginary parts of 8’ and s”, which we call s: and s:, 
vary independently. 
dt operator to the right in 
Equation 9.58 gives 
of Equation 9.58 lies anywhere in this convergence region of @(sJ. 
Using the common Laplace contour and moving the 
Notice what happens to the last integral in this equation. Because s” and s have the 
same real part, that is 
= s, 
+ is, and s’’ = s, + is:, 
this integral is just the 
orthogonality relation: 
[I 
dt ert(s:-sc) = 2 7rS(S:’ - s,). 
(9.60) 

322 
LAPLACE TRANSFORMS 
s-plane 
j 
-7 
I 
Figure 9.18 The Common Region of Convergence and Laplace Contour 
Applying this to expression 9.59 gives 
where we have also made use of the fact that along L, ds“ = ids;, with s: 
ranging 
from minus to plus infinity. The 8-function makes the integral over s“ very easy and 
lets us write 
Interchange the remaining two integrals: 
(9.62) 
(9.63) 
We can now pull a similar stunt to the one used on the last integral in Equation 9.59. 
Because 2’ and 3 have the same real part, this looks like another instance of the 
orthogonality relation: 
J_mm dr ei7(s:-sJ) = 2T8(Sl - Si). 
(9.64) 
Again making use of the fact that ds’ = ids; along the L contour, we can write 
= - 
F(S)GsJ. 
(9.66) 

PROPERTIES OF THE LAPLACE TRANSFORM 
323 
This is an elegant final result. The Laplace transform of the convolution of two 
functions is the product of the Laplace transforms of the functions 
Because H(s) is the product of F(s) and GO, 
the poles of H(s) are just a combination 
of the poles of 
and G(s). The convergence region for H(sJ is therefore the 
intersection of the convergence regions of both E(s) and GO, as we assumed earlier. 
9.5.4 
Multiplication 
As a final situation, consider the product of two functions 
The Laplace transform of h(t) is given by 
r m  
H(S) = 
dt e-”’f(t)g(t) 
(9.68) 
(9.69) 
Again f ( t )  and g(t) have been assumed to be zero for t < 0, so that the dt integration 
of the Laplace transform can be extended to negative infiNty. As with the previous 
case, the Laplace contours Lf and L, are in their respective convergence regions and, 
initially, are placed as far to the left as possible, as shown in Figures 9.17 and 9.19. 
We begin the manipulation of Equation 9.69 by interchanging the order of inte- 
gration, to obtain: 
On Lf let $ = s:, 
+ is; and on L, let 5’’ = 
If s = s, + isi, then in order for the dt integral to exist, we must have 
+ is: where s:, 
and s;; are constants. 
s;, + s;, 
- s, = 0. 
(9.71) 
If the condition is not met, the dt integral diverges and I%($ 
does not exist. If this 
condition is met, however, the dt integral becomes a 8-function, 
(9.72) 
= 2rtqs; + s: - Si), 
(9.73) 
and H(s) can be evaluated. 
The condition of Equation 9.71 constrains the convergence region of H(s). If F($) 
converges for s i  > sf and G(g”) converges for s: > sg, then Equation 9.71 says H(sJ 
can exist only for s, > sf + sg. This situation is shown in Figure 9.19. To see why 

324 
Convergence region for F(s) 
LAPLACE TRANSFORMS 
I-- 
~ 
Convergence region for G(s) 
imag 
~ 
s-plane 
- t 
Convergence region 
for HW 
Figure 9.19 Convergence Regions for the Laplace Transform of a Product 
this is a reasonable result, imagine f(r) and g(t) are both exponentid functions. Their 
product is an exponential that is growing at a rate that is the sum of the growth rates 
Along Lf, 
dz' = idsf, and on Lg, 
d$' = ids:. Substituting these differentials into 
of f(0 and g(0. 
Equation 9.70 and using the result of Equation 9.72 gives: 
m 
m 
H(S) 
- = 1 
ds; E(si, + is;) 
ds: G(s:o + is:) 
S(si + s; - si). 
(9.74) 
2T 
--m 
These integrations can be performed in either order. Doing the 
dsf' first gives 
HSJ = - ds; ~(s:, + is;) ~(s:, + isi - is;). 
(9.75) 
2m Sm 
-m 
Using the condition that s:, + s
:
,
 
= sr, Equation 9.75 can be rewritten as 
This integration can now be converted back to one along the path Lf: 
(9.77) 

PROPERTIES OF THE LAPLACE TRANSFORM 
325 
Notice the result of Equation 9.77 is similar to the convolution integral discussed 
earlier, except now the integration is done along a complex path. The result of 
Equation 9.77 can be written in a shorthand notation as 
If the order of the integration of Equation 9.74 is reversed the result becomes 
(9.78) 
(9.79) 
We have obtained a result that is similar to the one for Fourier transforms. The 
Laplace transform of the product of two functions is the convolution of the separate 
transforms. In this case, however, the convolution must be integrated on one of the 
Laplace contours. 
In arriving at Equation 9.77 s, was determined by the locations of the Lf and Lg 
contours according to the condition s, = s:, 
+ s:~. As the Lf and L, contours are 
moved to the right, S, increases and consequently 8 also moves to the right, as shown 
in Figure 9.19. When using Equations 9.78 or 9.79 to evaluate HsJ, however, it is 
convenient to take another point of view. In particular, when using Equation 9.77, 
it makes more sense to select a fixed value for g and use this value to determine 
the location of the Lf contour. To do this, it must be realized that only values of s_ 
in the convergence region of H(S) can be used. In this convergence region, all the 
$-plane poles of G(,r - g’) must be to the right of all the $-plane poles of &I). 
This 
condition can be used to define the convergence region for fZsJ. The integration of 
Equation 9.77 is then evaluated by placing the Lf contour between the $-plane poles 
of E($) and G(s - s’). These ideas are demonstrated in the following example. 
Example 9.1 
strated by a simple example. Let 
The process of convolution along a Laplace contour can be demon- 
and 
The Laplace transforms of these functions are easily taken: 
1 
E N = -  
5 - 2 ’  
with a convergence region given by s, > 2, and 
(9.80) 
(9.8 1) 
(9.82) 
(9.83) 
1 
G O = -  
3 - 3 ’  
- 

326 
LAPLACE TRANSFORMS 
with a convergence region s, > 3. If h(t) = f(r)g(t) then 
t > O  
h(t) = 
and its Laplace transform can be taken directly 
1 
a @ = -  
- 
s - 5 '  
(9.84) 
(9.85) 
with a convergence region s, > 5. 
using Equation 9.77 and the convolution of 
This Laplace transform of h(t) and its convergence region can also be obtained 
with GsJ: 
(9.86) 
The integrand has two poles, one at g' = 2 and another at s' = g - 3, as shown in 
Figure 9.20. The Lf contour must lie in between these poles as indicated in the figure. 
This contour can be closed to the left or to the right to produce the same result we 
obtained in Equation 9.85: 
1 
IYsJ=- 
g - 5 '  
(9.87) 
The convolution arguments also give the same region of convergence for this trans- 
form. The pole at g - 3 must be to the right of the pole at 2, to make room for the Lf 
contour. Consequently, s, - 3 > 2 and the convergence region for HsJ is given by 
sr > 5. 
$-plane 
~ 
! 
- i  
i 
I , 
3 
region 5 
5 9 for ~ ( s )  $ 
4///////////// 
Figure 930 Convolution Integration in the Complex Plane 

THE LAPLACE TRANSFORM CIRCUIT 
327 
9.5.5 
Summary of Transform Properties 
Some properties of the Laplace transform operation are summarized in the table 
below. In this table the double arrow H 
is used to indicate the reversible transform 
process. 
9.6 THE LAPLACE TRANSFORM CIRCUIT 
As was the case with Fourier series and Fourier transforms, the inverse Laplace 
transform can be conceptually modeled with an additive electronic circuit. Consider 
the inverse Laplace transform integral 
(9.88) 
which is an integration along the contour L, as shown in Figure 9.21. The Laplace 
contour must lie within the convergence region of the transform function which is to 
the right of all the poles of EL!.). Along this contour, s = sro + isi, d& = idsi, and si 
ranges from --oo to 00. With these facts in mind, Equation 9.88 can be rewritten as 
f ( r )  = -L /+mdsi esrotefiltE(sro + isi). 
(9.89) 
If we think of the integral of Equation 9.89 in terms of its Riemann definition, it is 
the limit of a discrete sum of terms: 
27r 
-m 
f(t) = lim 2 F(s, + i n A s i ) ~ e S m r e i n A s J  
277 
n=-m 
As,-+O 
imag 
s-plane 
al 
- Convergence region 
Figure 9.21 The Standard Laplace Contour 
(9.90) 

328 
LAPLACE TRANSFORMS 
Equation 9.90 describes f ( t )  as a sum of functions, each with an exponentially 
growing envelope of (1/27r)17(sr0 + inAsi)Asiesmr, and oscillating as einAsir. 
9.6.1 A Growing Exponential 
Consider the growing exponential 
(9.91) 
with a > 0. Its Laplace transform is given by 
(9.92) 
1 
- 
F ( S ) = - .  
s - a  
The summation on the RHS of Equation 9.90 can be modeled with the circuit shown 
in Figure 9.22. There is a fair amount of information in this figure. The insert in 
the upper right-hand comer shows the $-plane with its pole, the convergence region 
\ 
I 
I 
I 
I 
I 
I 
Figure 9.22 The Basic Laplace Circuit 

THE LAPLACE TRANSFORM CIRCUIT 
329 
, 
\ 
\ 
I 
I 
I 
I 
I 
Figure 9.23 The Laplace Circuit with the Laplace Contour Shifted to the Right 
of the transform, and the inversion contour. Each term in the series on the RHS 
of Equation 9.90 is represented by an oscillator. In this figure only the first four 
oscillators for n 2 0 have been shown. All the oscillators have been going on for all 
time and continue to go on for all time. When added together they generate f ( t ) ,  a 
function that is zero for all t < 0, and grows exponentially for z > 0. The exponential 
growth rate of f ( t )  must be less than esm', the growth rate of the individual oscillators. 
The L contour can be moved to the right without affecting the result of the 
integration in Equation 9.89. This means we can increase sro, making each oscillator 
have an arbitrarily large growth rate. Nevertheless, when they are all added up, they 
generate the same f ( t ) .  This somewhat surprising result is depicted in Figure 9.23. 
9.6.2 A Decaying Exponential 
Things get even more interesting when we consider the case where the single pole of 
F(s) lies on the negative real axis. This happens in Equation 9.91 when a < 0. The 
region of convergence for the Laplace transform integration still lies to the right of 
the pole at s = a, and the inversion contour L can be placed anywhere to the right 

330 
, 
\ 
LAPLACE TRANSFORMS 
of this pole. Let’s start with it in the right half plane so that s, 
> 0. The circuit for 
this situation is similar to the previous ones and is shown in Figure 9.24. The inserted 
figure shows the Laplace contour and region of convergence. The sum of function 
generators produces an f ( t )  that is zero for t < 0. For t > 0, even though each 
function generator produces a signal that diverges as t increases, the sum is a damped 
exponential. Think about that last sentence for a second. We are adding up an infinite 
number of exponentially growing, oscillating functions which miraculously add in 
just the right way to generate a smooth, decaying exponential. Very strange, indeed. 
Because the convergence region of this transform extends into the left half plane, 
we can move L to the left, until it sits on top of the imaginary axis with s, 
= 0. In 
this case, the Laplace inversion has effectively gone to a Fourier inversion and F(isi) 
is really just the Fourier transform of f(t). The Laplace circuit is now composed 
of signal generators oscillating at a constant amplitude for all time, as shown in 
Figure 8.6. 
Let’s keep shifting L to the left, so that it lies in the left half-plane with s, < 0. 
The Laplace circuit for this case is shown in Figure 9.25. Now the circuit is composed 
of signal generators whose amplitudes are decaying, because s, 
is less than zero. For 

DOUBLE-SIDED OR BILATERAL LAPLACE TRANSFORMS 
331 
\ 
L 
P 
\ 
I 
I 
I 
I 
Figure 9.25 The Laplace Circuit with the Laplace Contour in the Left Half s-Plane 
t > 0 the signals from the generators sum to give the damped exponential of f ( t ) .  Its 
damping rate (Y is greater than the damping rate of the individual signal generators 
because a < s,~. For t < 0, even though the amplitudes of the individual oscillators 
are diverging as t goes to -a, 
their sum produces a signal that is zero for all t < 0. 
9.7 
DOUBLE-SIDED OR BILATERAL LAPLACE TRANSFORMS 
Early on, we made the statement that the Laplace transform is more general than the 
Fourier transform and can handle any function that the Fourier transform operation 
can. However, for the regular Laplace transform, f ( t )  must be zero for t < 0. The 
function 
(9.93) 
where a1 and a2 are both positive, real numbers, is absolutely integrable and clearly 
possesses a legitimate Fourier transform. But it is not zero for t < 0, and therefore 

LAPLACE TRANSFORMS 
332 
does not have a regular Laplace transform. So how can we make the claim that any 
function which can be Fourier transformed can also be Laplace transformed? 
It is true that the regular Laplace transform, with the restriction that f ( t )  must be 
zero for t < 0, cannot handle a function like the one in Equation 9.93. However, a 
small modification to the regular Laplace transform clears up this problem. 
Consider the Laplace transform type operation 
(9.94) 
where t < 0 is now included in the integration, and f (t) is given by Equation 9.93. 
The obvious question immediately arises. Is there is a region of convergence in the 
s-plane where the integral of Equation 9.94 exists? Inserting Equation 9.93 into 9.94 
gives 
EsJ = L m d t  e-s'ea*' + Lrn 
dt e-S'e-al' 
The first term on the RHS of Equation 9.95 converges for s, < a 2 ,  and the second 
term converges for sr > - a1 . Therefore 
(9.96) 
with a convergence region given by -a1 < s, < a2. The function E(S) has first- 
order poles at s = a2 and s = -a1 (remember that both a1 and a2 are positive, real 
numbers). This convergence region is therefore a strip between the poles of EN, as 
shown in Figure 9.26. 
It seems reasonable to try to set up the inversion integral along a Laplace contour 
in the convergence region as shown in Figure 9.26. This inversion integral becomes 
(9.97) 
-eJ'(al + a 2 )  
1 
(s+cu*)@-cu2)' 
imag 
1 
Convergence region 
Figure 9.26 Convergence Region for a Double-Sided Laplace Transform 

>OUBLE-SIDED OR BILATERAL LAPLACE TRANSFORMS 
\ 
333 
Figure 9.27 The Convergence Region for a Function that Grows Exponentially for t < 0 
Notice, there are poles of f(sJ on either side of L. Zn order to close this contour, we 
have the same requirements as we did for the regular Laplace transform. Because of 
the epr term, we must close to the left for t > 0 and to the right for r < 0. After 
evaluating the residues of the poles, it is quite easy to see the contour integral of 
Equation 9.97 generates the original f ( t )  in Equation 9.93. The Laplace contour can 
be placed anywhere in the convergence region with the same result. If it is placed 
on the imaginary ?-axis, this double-sided Laplace transform operation is equivalent, 
except for a &% factor, to a Fourier transform. 
If we let a1 or a2 become negative, the standard Fourier transform will fail. But, as 
long as a2 > - a1, there will be a finite convergence region and this special Laplace 
transform will exist. In cases like this, the convergence region does not include the 
imaginary z-axis, consistent with the fact that the Fourier transform operation fails. 
An example of this, with a2 < 0, is shown in Figure 9.27. 
Therefore, this more general approach to the Laplace transform, which is called a 
double-sided or bilateral Laplace transform, includes all the properties of the Fourier 
transform. To use this approach we must take more care specifying the Laplace 
contour and the region of convergence. This is important, as you will see in the 
example below, because a given E(d can generate different f(t)'s depending on how 
the convergence region is defined. 
~ 
~~ 
~ 
~ 
Example 9.2 
values ayl = cy2 = 1: 
Consider the transform given by Equation 9.96 with the specific 
2 
E M = -  
(g + 1)(s. - 1)' 
(9.98) 
This function has simple poles at 8 = + 1 and 5 = - 1. There are three possible 
regions of convergence: s, > 1 ; - 1 < s, < 1 ; and s, < - 1. Laplace inversions 
using these three different convergence regions result in three different functions oft. 
For the first case, with the region of convergence given by s, > 1, the Laplace 
contour is to the right of all the poles, as shown in Figure 9.28. This is just a regular 
Laplace transform. Because there are no poles to the right of the Laplace contour f(t) 

334 
- s-plane 
-1 
LAPLACE TRANSFORMS 
Figure 9.28 Inversion for the Region of Convergence Given by Real s > 1 
is zero for t < 0. For t > 0, f ( t )  is given by the sum of the residues of the two poles: 
(9.99) 
as shown in Figure 9.28. This f ( t )  has no Fourier transform analogue, because the 
region of convergence does not include the imaginary x-axis. 
The second region of convergence, given by - 1 < s, < 1, is the region between 
the two poles, as shown in Figure 9.29. This is a specific case of the situation we 
discussed previously. The Laplace contour lies between the two poles, and now f (t) 
is no longer zero for t < 0, but is given by minus the residue of the pole at 8 = 1. 
Similarly, f ( t )  for t > 0 is nonzero and equals the residue of the pole at 8 = - 1, 
(9. loo) 
as shown in Figure 9.29. In this case the imaginary x-axis is in the region of conver- 
gence, so there is a corresponding Fourier transform. 
The last region of convergence has s, < - 1, and is shown in Figure 9.30. In this 
case, the contour lies to the left of all the poles, and consequently f ( t )  is zero for 
t > 0. For t < 0, we sum the negative of the residues of the poles to get, 
f ( t )  = {; - 
e - f  
t < O  
t > O ’  
(9.101) 
Figure 9.29 
Inversion for the Convergence Region Given by - 1 < Real < 1 

EXERCISES 
335 
imag 
$-plane 
real 
t 
-1 
1 
Figure 9.30 Inversion for the Convergence Region Given by - I < Real 2 
as shown in Figure 9.30. This function does not possess a Fourier transform, because 
the imaginary x-axis is not in the region of convergence. 
EXERCISES FOR CHAPTER 9 
1. Find the Laplace transform of the function 
where a ,  and w, are positive real numbers. Demonstrate that you have obtained 
the correct transform by inverting it to obtain the original f ( t ) .  
2. Consider the function 
Sketch f ( t )  vs. r and determine its Laplace transform and region of convergence. 
Confirm that you have the right transform by inverting it to obtain the original 
f (t>. 
3. Show that the Laplace transform of 
t sin(kt) 
t > 0 
t < O  
is 
2kg 
= (s2 + k 2 ) 2 ’  
Determine the region of convergence for F(s). 
4. Find the Laplace transform of the function 
t3 e‘ 
t > O 
f @ ) = { o  
r < O  

336 
LAPLACE TRANSFORMS 
(a) using the standard transform integral and integrating by parts the appropriate 
(b) by treating the function as a product of two other functions. 
tions are zero for t < 0, and for t > 0 are given by: 
(a) sinh(kt). 
(b) cosh (kt). 
(c) sin(kt). 
(d) cos(kt). 
(e) e' cost 
(f) t". 
number of times. 
5. Determine the Laplace transforms of the following functions. Assume the func- 
6. Find the inverse Laplace transform of 
with the region of convergence s, > 0. Make a sketch of f(t). 
7. Find the function that has the Laplace transform 
1 
F(S)=--- 
- 
6- 1)*' 
with the convergence region s, > 1. 
8. Find the inverse transform of 
1 
z2(g - k )  ' 
F(sJ = 
- 
where k is a real positive number 
(a) by direct integration of EM. 
(b) by factoring E(s> and using the convolution theorem. 
(c) by expanding E(SJ as a sum of partial fractions. 
9. Find the inverse Laplace transform of 
where k is a positive real number 
(a) by direct integration of EQ). 
(b) by factoring E(sJ and using the convolution theorem. 
(c) by expanding 
as a sum of partial fractions. 

EXERCISES 
337 
10. Find the inverse Laplace transform of 
1.2 
(a) by direct integration of E(s). 
(b) by factoring F(s) and using the convolution theorem. 
(c) by expanding F(g) as a sum of partial fractions. 
11. Showthat 
where L[ f (t)] = fe), 
Develop a general expression for 
.[%I. 
12. If 
what is g(t), the inverse transform of GO, in terms of f(t), the inverse transform 
of FQ)? 
13. If the function 
is treated as a bilateral Laplace transform, it can have three different regions of 
convergence: 
(a) s, < -1. 
(b) - 1  < S, < 1. 
(c) 1 < s,. 
For each of these convergence regions, the inversion of F(s) gives a different 
function of time. Determine these three functions and make a plot of them. 
14. Find the Laplace transform of 
15. Find the inverse Laplace transform of 
assuming that the region of convergence is - 1 < s, < + 1. 

338 
LAPLACE TRANSFORMS 
16. 
17. 
Find the Laplace transform of f ( t )  where 
and identify its region of convergence in the g-plane. Invert this transform using 
closure to obtain the original f ( t ) .  
Consider the function 
where (Y and p are both positive, real numbers. 
(a) Make a sketch of this function. 
(b) Why does this function not have a standard Laplace transform? 
(c) Define a transform pair that will work for this function. Discuss the condi- 
tions on a and p that allow a finite region of convergence for the transform 
function. 

10 
DIFFERENTIAL EQUATIONS 
A differential equation contains terms involving either total (d/dx) or partial (a/dx) 
differential operations. In this chapter, several methods for obtaining solutions to 
differential equations will be presented. The emphasis is on solutions to linear prob- 
lems, although some techniques for solving nonlinear equations will be discussed. 
Particular attention will be paid to Fourier and Laplace transform approaches and 
Green’s function solutions. A basic knowledge of differential equations is assumed. 
10.1 TERMINOLOGY 
The study of differential equations involves a large number of definitions. This section 
presents a quick review of some of these definitions and other important terminology. 
10.1.1 Dependent versus Independent Variables 
Two types of variables appear in differential equations. A dependent variable repre- 
sents the unknown solution to the equation and typically appears in the numerator of 
the differential operation(s). An independent variable is a parameter of the solution. 
The dependent variables are functions of the independent variables. An independent 
variable typically appears in the denominator of the differential operation(s). For 
example, in the differential equation 
= y(x) + 5, 
dx 
(10.1) 
x is an independent variable and y is a dependent variable. The dependence of y on 
x is indicated by writing .y(x). 
339 

340 
DIFFERENTIAL EQUATIONS 
10.1.2 Ordinary versus Partial Differential Equations 
Ordinary differential equations involve only one independent variable and contain 
only total differential operations. For example, 
= 
+ y(x) = 0 
dx 
(10.2) 
is a typical, ordinary differential equation. There is a single dependent variable y, 
which depends on the single independent variable x. 
On the other hand, in a partial differential equation the dependent variable is 
a function of more than one independent variable. Consequently, these equations 
contain partial differential operations. For example, 
(10.3) 
is a partial differential equation. It involves one dependent variable y, which depends 
on the two independent variables x and t. This dependence is emphasized by writing 
Y ( X 9  t). 
10.1.3 Linear versus Nonlinear Equations 
A linear differential equation contains only terms that are linear in the dependent 
variables or their derivatives. In a linear equation, if y is a dependent variable, there 
can be no y2, y(dy/dt), (dy/dtp, etc. terms, nor terms where the dependent variable 
appears in the argument of a nonlinear function. There are no restrictions on the 
independent variables. Equations 10.2 and 10.3 are examples of linear differential 
equations. The equation 
(10.4) 
is also a linear, even though it contains an x2 factor and a sin@) term. 
ear functions of the dependent variable. For example, 
A nonlinear differential equation contains dependent variable products or nonlin- 
2 
d[Y2(X)] + dy(x) 
~ 
dx 
[-I 
+ x2y(x) = 0 
and 
* 
+ x2 sin(y> = o 
dx 
(10.5) 
(10.6) 
are nonlinear differential equations. The first is nonlinear because y(x) is squared in 
the numerator of the differential operation and because of the (dy/dx)’ term. The 
second is nonlinear because the dependent variable appears inside the nonlinear sine 
operation. 
Except in rare cases, nonlinear differential equations are much more difficult 
to solve than linear ones. Many times nonlinear equations require numerical or 
approximate solutions. 

TERMINOLOGY 
341 
10.1.4 Order 
The order of a differential equation, regardless of whether it involves partial or total 
differential operations, is determined by the highest derivative present in the equation. 
Equations 10.2 and 10.3 are examples of first-order differential equations, and 
(10.7) 
is an example of a third-order differential equation. 
10.1.5 Coupled Differential Equations 
Coupled differential equations involve more than one dependent variable. For ex- 
ample, 
d2X1(t) = q ( t )  - X 2 ( t )  
dt2 
(10.8) 
(10.9) 
form a set of coupled, second-order, linear differential equations. Both x1 and x2 are 
dependent variables and are functions of the single independent variable t. 
10.1.6 Homogeneous versus Nonhomogeneous Equations 
We will restrict our comments on homogeneous and nonhomogeneous properties to 
linear equations. A linear differential equation is homogeneous if every term contains 
the dependent variable. Homogeneous equations are usually arranged so that all the 
terms are on the LHS. Equation 10.7 is one example of a homogeneous equation. In 
general, an ordinary homogeneous differential equation can always be placed in the 
form 
-&pY(X> = 0, 
(10.10) 
where Lop is a linear differential operator, which acts on the dependent variable y. 
The operator for Equation 10.7 would be 
d 3  
d 
dx3 
dx 
Lop = - 
+ - - 1. 
(10.1 1) 
Solutions to homogeneous equations are sometimes called “undriven” solutions, 
because they can describe the response of a system which is not under the influence 
of an external force. The solutions of homogeneous equations are not unique, but can 
always be scaled by an arbitrary constant. A homogeneous equation will always have 
a trivial solution of y(x) = 0. 
The superposition principle applies to solutions of linear, homogeneous equations. 
It states that if y,(x) and y2(x) are solutions to a linear, homogeneous differential 

DIFFERENTIAL EQUAnONS 
342 
equation, then a linear sum of these solutions, y(x) = A, yl(x) + B, y2(x) where A, 
and B, are constants, is also a solution to the same differential equation. 
The order of a linear, homogeneous differential equation is especially important. 
It is equal to the number of nontrivial (nonzero) solutions to the equation, and 
consequently to the number of boundary conditions necessary to make the solution 
unique. 
In contrast, nonhomogeneous differential equations contain at least one term with- 
out a dependent variable. They are sometimes referred to as “driven” equations, be- 
cause they describe the response of a system to an external signal. Nonhomogeneous 
equations are usually arranged so that the terms that contain the dependent variable(s) 
are all on the left, and all other terms are on the right. Using the operator notation, an 
ordinary, linear, nonhomogeneous equation can be written 
LopY(X) = f ( x ) ,  
(10.12) 
where f ( x )  is often called the driving function. The equation 
(10.13) 
is an example of a linear, nonhomogeneous equation. 
f ( x )  can always be written as the sum of two parts 
The general solution to a linear nonhomogeneous differential equation LOpy(x) = 
Y ( X >  = Y h ( X )  + Y p W .  
(10.14) 
The first term, called the homogeneous part, is the general solution of the corre- 
sponding homogeneous equation LOpy(x) = 0. The second part, called the particular 
solution, is any solution of the nonhomogeneous equation. 
10.2 SOLUTIONS FOR FIRST-ORDER EQUATIONS 
Many first-order equations are relatively easy to solve because the solutions can be 
obtained simply by integrating the differential equations once. However, there are 
some first-order equations that are not immediately in integrable form. In this section, 
three standard approaches for these types of problems are presented: the separation of 
variables, the formation of an exact differential, and the use of an integrating factor. 
10.2.1 Separation of Variables 
Any first-order differential equation that can be placed in the form 
dY(4 - P(X) 
dx 
Qol) 
- 
- -- 
(10.15) 
may be able to be solved using the method of separation of variables. The minus sign 
in this equation is an arbitrary convention. The differential equation may be nonlinear 
through the Q(y) term. 

FIRST-ORDER EQUATIONS 
343 
Rewrite Equation 10.15 as 
Q(y) d y  = -P(x) 
dx. 
( 10.16) 
Now if both sides of Equation 10.16 can be integrated, we can generate a functional 
relationship between x and y, with one arbitrary constant of integration. In physi- 
cal problems, that constant is determined by one boundary condition that must be 
specified to make the solution unique. 
~ 
~ 
~ 
~~ 
~ 
Example 10.1 The nonlinear differential equation 
d y  - cos(x) 
d x  
Y ’ 
(10.17) 
with the boundary condition of y(0) = 0, can be solved by separation of variables. 
Equation 10.17 leads to the integral equation 
1 
d y  y = / dx cos(x). 
( 10.18) 
Both sides of Equation 10.18 easily integrate to give 
(10.19) 
YL 
- + C, = sin@), 
2 
where the two arbitrary constants of integration have been combined into the single 
constant C,. Imposing the boundary condition y(0) = 0 forces C, = 0. Thus we 
have the final solution 
y = d w .  
(10.20) 
10.2.2 Forming an Exact Differential 
If it is not possible to separate variables, an approach that sometimes works is to form 
an exact differential. Try to set up the equation in the form 
(10.21) 
Equation 10.21 can immediately be manipulated to read 
R(x, y) d x  + S(x, y) d y  = 0. 
(10.22) 
This equation is in the form of an exact differential if there is a function 4(x, y )  such 
that 
(10.23) 

344 
DIFFERENTIAL EQUATIONS 
and 
(10.24) 
If such a 4(x, y )  exists, the LHS of Equation 10.22 can be written as 
= d 4 ,  
(10.25) 
and Equation 10.22 becomes 
dcp = 0. 
(10.26) 
The solution to Equation 10.21, the original differential equation, is therefore 
4(x,y) = C,, where C, is some constant. As before, this constant is determined 
by the boundary condition for the problem. 
There is a simple test to see if a pair of functions forms an exact differential. If 
R(x, y )  and S(x, y )  form an exact differential, Equations 10.23 and 10.24 must be 
satisfied. If x and y are independent variables and the partial derivatives of +(x, y )  
exist, then 
(10.27) 
This means that for R(x, y) and S(x, y )  to form an exact differential, they must satisfy 
the condition 
(10.28) 
~~~ 
Example 10.2 Consider the differential equation 
d y = -  3x2 + 2y2 
dx 
4XY 
’ 
(10.29) 
with the boundary condition y( 1 )  = 1. Applying the test given by Equation 10.28 to 
the functions on the RHS of Equation 10.29 shows that an exact differential may be 
found for this problem. Indeed, the function 
4(x,y) = x3 + 2xy2 
(1 0.30) 
generates the two functions 
(10.31) 

FIRST-ORDER EQUATIONS 
345 
The solution to Equation 10.29 is therefore given by 
x3 + 2xy2 = c,. 
(10.32) 
The boundary condition y( 1) = 1 determines C, = 3, and our final result is 
y = Je 
2 x  
, 
(10.33) 
10.2.3 Integrating Factors 
integrating factors can sometimes be used to solve linear, nonhomogeneous, first- 
order differential equations of the form - 
+ P(x)y(x) = G(x). 
dx 
(10.34) 
In this equation, P(x) and G(x) are known functions of the independent variable x. 
An integrating factor is a function a ( x ) ,  by which we multiply both sides of Equation 
10.34, so the LHS is placed in the form 
in other words, we seek an a ( x )  such that 
Using the product rule of differentiation on the RHS of Equation 10.35 gives a simple 
first-order differential equation for a ( x ) :  
This equation can be rearranged as 
da - 
= P ( x ) ~ x  
a 
and solved for a ( x )  as follows: 
(10.36) 
(10.37) 
(10.38) 

346 
DIFFERENTIAL EQUATIONS 
Notice, it must be possible to calculate the integral of P(x) in order to use this method. 
It is not necessary to worry about the constants of integration associated with the two 
integrations performed in the steps above. The original differential equation we are 
trying to solve i s  first order, and therefore needs only one arbitrary constant. This 
constant will be introduced in the subsequent steps. 
With a(x) determined, the solution to the original differential equation can now 
be obtained. Multiplying both sides of Equation 10.34 by a ( x )  and using Equation 
10.35 gives 
Integrating both sides results in the expression 
a(x)y(x) = / d x  a(x)G(x) + C,. 
(10.40) 
The integration constant C, is determined by the single boundary condition that is 
necessary to make the solution unique. The final result is 
(10.41) 
Notice that to use this process, not only must we be able to integrate the function 
P(x) to get a(x), but we must also be able to integrate the function a(x)G(x). 
Example 10.3 As a simple example of using an integrating factor, consider the 
differential equation 
dY - + y = ex, 
dx 
with the boundary condition y(0) = 1. The integration factor is simply 
a(x) = eJdx = ex. 
Substitution into Equation 10.41 gives 
(10.42) 
(10.43) 
(10.44) 
1 
2 
= -ex + Coe-x. 
The boundary condition requires C, = 1/2, so 
y(x) = cosh(x). 
(10.45) 

TECHNIQUES FOR SECOND-ORDER EQUATIONS 
347 
10.3 TECHNIQUES FOR SECOND-ORDER EQUATIONS 
Second-order differential equations pervade physics and engineering problems. One 
of the most common equation in physics is the second-order equation 
(10.46) 
which describes the motion of a simple harmonic oscillator with a period of 2rr/w0. 
As mentioned earlier, the number of nontrivial, independent solutions of a linear 
differential equation is equal to the order of the equation. Thus, a linear second-order 
equation should have two different solutions. We can see that this is the case for 
Equation 10.46, since both 
y(x) = sin(o,x) 
(10.47) 
y(x) = cos(wox) 
(10.48) 
are valid solutions. 
There is no general analytic method for finding solutions to second-order and 
higher equations. Solutions are often obtained by educated guessing, or numerical 
methods. There is a general technique, however, for a specialized class of equations 
which have constant coefficients. In this section, we describe a simple technique 
for solving ordinary, second-order, linear differential equations with constant co- 
efficients. Then we introduce an important quantity called the Wronskian, which 
provides a powerful method for generating a second solution to a homogeneous dif- 
ferential equation when one solution is known. The Wronskian can also help generate 
the particular solution to a nonhomogeneous equation, if the general homogeneous 
solution is already known. 
10.3.1 Ordinary, Homogeneous Equations with Constant Coefficients 
The most general form of a second-order, linear, ordinary, homogeneous differential 
equation can be written as 
d2yo + P ( x ) W  + Q(x) y ( x )  = 0. 
dx2 
dx 
(10.49) 
Equation 10.49 is said to have constant coefficients, if the functions P(x) and Q(x) 
are constants, 
(10.50) 
where the factor of - 2 was used in the second term to simplify the algebra to come. 
The manipulation of the linear equations is assisted by defining the differential 
operator of order n as D& = d"/dx". In this notation, Equation 10.50 becomes 

348 
DIFFERENTIAL EQUATIONS 
We can now perform a fancy trick, and factor this as if it were a quadratic algebraic 
expression, 
( D o p  - r+>(Dop - r-)y(x) = 0, 
(10.52) 
- 
where r2 = p 2 d f i .  
This expression is completely equivalent to Equation 
10.51. 
This can be solved in two steps. First solve for s(x), the bracketed term in Equation 
10.52, 
(10.53) 
(Dop - r + ) S ( X )  = 0. 
This is a simple first-order equation with the solution 
s(x) = Ae‘+’, 
( 10.54) 
where A is our first undetermined constant. The second step is to solve the remainder 
of Equation 10.52: 
(DOp - r-)y(x) = s(x) = Aer+x. 
(10.55) 
This is another first-order equation. Its solution can be obtained by using an integration 
factor of e-r-x, with the result 
y(x) = er-x B + A 
d x e  r+-r--)x 
(10.56) 
where we have left the integral indefinite and introduced B, our second undetermined 
constant. 
At this point, there are two cases to consider. First, if p2 = 4, the two “roots” are 
equal: r- = r+ = p. In this case, Equation 10.56 becomes 
y(x) = (Ax + B)ePX 
forp2 = 4. 
(10.57) 
[ 
I (  1. 
On the other hand, if p2 # 4, then we get 
y(x) = Ae‘+* + Be’-’ 
forp # 4’. 
(10.58) 
In both cases, there are two arbitrary constants, which are determined by the boundary 
conditions of the problem. 
“What if the roots are imaginary?” we hear you asking. In other words, what 
happens if p2 < q, forcing r+ and r- to be complex? It turns out Equation 10.58 
is still entirely valid, if you remember the complex definition of the exponential 
function: 
e ( ~ + i ~ )  
= 
- eX[cos y + i sin y1. 
(10.59) 
If p2 < 4, r+ and r- will always be complex conjugates. In this case, define r-t = 
r, 2 iri where r, is the real part of r+, and ri is the imaginary part of r+ . Using this 
definition and redefining the arbitrary constants converts Equation 10.58 into 
y(x) = errx [ A  cos(rix) + B sin(rix)] for p2 < q. 
(10.60) 

TECHNIQUES FOR SECOND-ORDER EQUATIONS 
349 
Example 10.4 Consider the classic example of the harmonic oscillator, governed 
by the second-order equation 
(10.61) 
In operator notation, this equation is represented by 
Referring to Equation 10.50, p = 0 and q = 0,’. The roots associated with this 
equation are r-2 = -+iw,. When substituted into Equation 10.60, the general solution 
for the simple harmonic oscillator becomes 
y(t) = A cos(w,t) + B sin(w,t). 
(10.63) 
10.3.2 Finding a Second Solution Using the Wronskian 
The Wronskian is a useful tool for generating the second solution of a second- 
order, linear, homogeneous equation when the first solution is already known. The 
Wronskian, itself, is a function of the independent variable and can be determined 
directly from the differential equation. The Wronskian for higher-order equations is 
discussed in a later section of this chapter 
As we pointed out earlier, the most general form of a second-order, linear, ordinary, 
homogeneous differential equation can be written as 
d2y(x) + P(x)@ 
+ Q(x)y(x) = 0. 
dx2 
dx 
(10.64) 
Because this is a second-order, linear equation, there will be two nontrivial solutions, 
yl(x) and yz(x). Any linear combination of these solutions 
where C1 and C2 are arbitrary constants, is also a valid solution. 
The Wronskian W(x) is defined in terms of these two, nontrivial solutions as 
= Y l ( X > Y 2 ’ ( 4  - Y 2 ( X ) Y l / ( X ) ,  
(10.66) 
where we have used the shorthand notation y’(x) = dy/dx to represent derivatives. 
The Wronskian can also be written using a determinant: 
(10.67) 

350 
DIFFERENTIAL EQUATIONS 
sin(w,x) 
cos( wax) 
w(x) = 1 coocos(w,x) 
-coosin(m,x) 
The Wronskian can be used to tell whether the yl(x) and y2(x) solutions are 
independent. If two functions are linearly dependent, we can write 
Y l ( X )  = kOYZ(X) 
(10.68) 
for some choice of the constant k,. Taking the derivative of both sides gives 
Y l 7 4  = k, Y 2 W .  
(10.69) 
Consequently, if the two solutions are linearly dependent, 
de, 
and W(x) = 0. 
As a simple example, return to the harmonic oscillator equation 
d2y(x) + 0,2y(x) = 0. 
dx2 
We already have shown that the two nontrivial solutions are 
Y l ( 4  = sin(mox) 
y2(x) = cos(0,x). 
The Wronskian in this case is 
(10.70) 
(10.7 1) 
(10.72) 
(10.73) 

TECHNIQUES FOR SECOND-ORDER EQUATIONS 
351 
which are generated by plugging the two solutions into Equation 10.64. Subtracting 
these two equations gives 
Y l  Y2” - Y2YI’/ + P(x) [Yl y2l- Y Z Y l ’ ]  = 0. 
(10.78) 
Notice the dependence on Q(x) has conveniently canceled out. Equation 10.78 can 
now be written in terms of the Wronskian and its derivative, 
dW(x) + P(x) W(x) = 0. 
dx 
(10.79) 
So, without any knowledge of the solutions to the original differential equation, a 
simple first-order equation for the Wronskian has been derived. The solution for W(x) 
can be obtained by separation of variables: 
where W, is a constant, equal to W(x,). From this expression it is evident that if W(x) 
is zero for some value of x, it will be zero for all x. 
This ability to determine W(x) directly from the differential equation is important, 
because we can use it to generate the second solution of a differential equation if one 
solution is already known. To see this, assume that yl(x) is known and yz(x) is not. 
This situation might arise when it is possible to guess one solution. Start by forming 
the derivative of y2(x) divided by yl(x): 
Next express this derivative in terms of the Wronskian: 
Integrating both sides with respect to x gives 
(10.82) 
(10.83) 
10.3.3 The Wronskian and Nonhomogeneous Equations 
The Wronskian can also be used to obtain the particular solution to a second-order, 
linear, nonhomogeneous differential equation. Consider a general, linear, nonhomo- 
geneous equation with the form: 
d2y(x) + b(x)- 
+ c(x)y(x) = d(x). 
dx 
4
x
>
7
 
dx 
(10.84) 

352 
DIFFERENTIAL EQUATIONS 
The functions, a(x), b(x), c(x), and d (x) are known functions of the single independent 
variable. The general solution to Equation 10.84 can always be written as 
Y ( X )  = Y+) + ClYlh) + C2Y2b-h 
(10.85) 
where yl(x) and y2(x) are the two independent solutions to the corresponding second- 
order homogeneous equation, that is Equation 10.84 with d(x) = 0. The function, 
y&), is called the particular solution and is any one solution to the nonhomogeneous 
Equation 10.84. The constants C1 and C2 are determined by the boundary conditions. 
There must be two such conditions to determine these constants and uniquely specify 
the problem. 
We will now show that if u(x), b(x), c(x), d(x), and the two nontrivial, homoge- 
neous solutions yl(x) and y2(x) are known, the general solution represented by y(x) 
in Equation 10.85 can be obtained using the Wronskian. Actually, only one of the 
homogeneous solutions really needs to be given at the start, because Equation 10.83 
can be used to determine the second. The Wronskian can be obtained using either 
Equation 10.66 and the two homogeneous solutions, or from Equation 10.80 with 
It is convenient to reformat the problem. Instead of attempting to find the solution 
y,(x) and adding it to the two homogeneous solutions, we will try to find y(x), the 
function given by Equation 10.85, assuming it can be written as 
P(x) = b(x)/u(x). 
Y W  = cl(x)Yl(x) + C2(4Y2(X)- 
(10.86) 
We will seek solutions for the two functions, Cl(x) and C2(x). An important thing 
to notice right from the start is that C, (x) and G3x) are not unique. This is quickly 
evident if you assume the yp(x) of Equation 10.85 is known. On the one hand, 
Cl(x) could equal the constant C1, in which case C2(x) = C2 + yP(x)/y2(x). On 
the other hand, we could have just as easily made C2(x) the constant C2, forcing 
Cl(x) = C1 + yp(x)/yl(x). The fact that these functions are not unique will play an 
important role in the discussion that follows. 
Insert Equation 10.86 into the Equation 10.84. To do this, two derivatives of y(x) 
must be calculated. Using Equation 10.86, the first derivative is given by 
Because Cl(x) and C2(x) are not unique, we are free to impose a relationship between 
them, to make them unique. This relationship will be chosen to simplify Equation 
10.87. In mathematics, such a relationship is often referred to as a gauge. In this 
particular case, a useful gauge is the requirement that 
CI/WYl(4 + C2’(X>Y2(X) = 0. 
(10.88) 
This simplifies Equation 10.87 to 
= C,(X)Yl/(X) + C,(x)y;?’(x). 
dx 
(10.89) 

TECHNIQUES FOR SECOND-ORDER EQUATIONS 
353 
The second derivative of y(x) becomes 
Equations 10.86, 10.89 and 10.90 can now be substituted into Equation 10.84. Taking 
into account that y1 (x) and y2(x) both satisfy the homogeneous equation with d(x) = 
0, a simple first-order differential equation involving C1 (x) and C ~ ( X )  
is obtained: 
4 x )  [Cl’(X) Y l  ’(4 + C,’(X> YZ’(X)] = d(x). 
(10.91) 
This equation, plus the gauge given by Equation 10.88, results in a pair of coupled 
differential equations that determine C1 (x) and C ~ ( X )  
to within two arbitrary constants: 
(10.92) 
cl’(x)Yl(x) + C2’(X)Y2W = 0. 
(10.93) 
To solve these equations, they must be uncoupled. Equation 10.93 can be used to 
solve for C2’(x): 
C,’(x) = -C,’(x) - 
[;::::I 
Now substitute this back into Equation 10.92 to obtain 
(10.94) 
(10.95) 
The term in brackets is - W(x), the negative of the Wronskian. Thus we get the two 
relations, 
Integrating these two equations gives 
(10.96) 
(10.97) 
(10.98) 
yl(x)d(x) + another constant. 
(10.99) 
~ 2 ( x )  
= + J dx [ w(x) 
a ( x ) ]  
These expressions can now be used to generate the solution to the nonhomogeneous 
equation: 
(1 0.100) 
If the constants in Equations 10.98 and 10.99 are associated with the C1 and C2 of 
Equation 10.85, the particular solution can be written 
Y (XI = c1 ( x )  Y 1 ( x )  + C2(X) Y 2 ( X ) .  

DIFFERENTIAL EQUATIONS 
354 
Ylb) 
Y 2 W  
. * *  
Yn(x) 
Y l ' ( 4  
YZ'(X) 
. * * 
Yn'(X) 
W(x) == 
Y l ' W  
y 2 " W  
* * * 
y,"(x) 
Yl("'(X) 
Yz'"'(4 
* . . Yn'"'(X) 
(10.102) 
det 
10.4 THE METHOD OF FROBENIUS 
The method of Frobenius is a series approach to solving linear, homogeneous differ- 
ential equations of any order. It is usually a method of last resort, because it involves 
a good deal of algebra and many times leaves the solution as an infinite series that 
cannot be placed in closed form. However, the method can be used to obtain solutions 
to many equations that can be solved in no other way. 
The technique is probably best described by example. Consider the second-order 
differential equation 
(10.103) 
This is a form of Bessel's equation, which is common in physics and engineering 
problems with cylindrical geometries. We will try to find a solution using a power 
series of the form 
m 
y(x) = C C , X " + S ,  
(10.104) 
n=O 
where the c, are unknown coefficients, and s is a currently unknown integer, either 
positive or negative, which determines the starting power of the series. 
When the RHS of Equation 10.104 is inserted into Equation 10.103, we obtain 
m 
oc 
x2 c 
c,(n + s)(n + s - l)xn+s-Z + 2x c 
c,(n + s)x"+s-' 
n=O 
n=o 
m 
+ (2 - 2) c 
c, x"+s - 
- 0, 
(10.105) 
n=O 

THE METHOD OF FROBENIUS 
355 
where we have differentiated the series for y ( x )  term by term. This approach is valid 
if the resulting series converges uniformly, a condition that must be checked once s 
and the cn are determined. 
The terms can now be grouped based on the powers of x they contain: 
Equation 10.106 is of the form 
Camp 
= 0. 
(10.107) 
m 
The only way the LHS of this equation can be zero for all x is to have all the a, 
coefficients equal to zero. To isolate the various powers of x and their coefficients, 
rewrite Equation 10.106 as 
m 
m 
CC, 
[(n + s)(n + s - 1) + 2(n + S) - 21 x
~
+
~
 
+ ~
C
,
-
~
X
”
+
~
 
= 0. (10.108) 
n=O 
n=2 
The first series in Equation 10.108 has powers of x starting with xs, i.e., xs, xS+’, 
x ” ~ ,  . . .. The second sum has powers of x starting with 2+’, i.e., 2+2, 
2+3, 
. . .. So, 
in order to group according to powers of x ,  the first two terms of the first summation 
in Equation 10.108 have to be written out explicitly. The remaining terms can be 
written as a sum from n = 2 to n = m: 
c, 
[(s)(s - 1) + 2s - 21 xs + c1 [(s + l)(s) + 2(s + 1) - 21 X’+l 
= 0. 
(10.109) 
Each of the terms in Equation 10.109 must be equal to zero. The coefficients of 
the xs and the xs+l terms form what are called the indicial equations: 
c, [(s)(s 
- 1) + 2s - 21 = 0 
c1 [(s + l)(s) + 2(s + 1) - 21 = 0. 
(10.1 10) 
(10.1 11) 
These equations determine s. If Equation 10.104 is to generate a nontrivial solution, 
the c, coefficients cannot all be zero. There are several approaches that all generate 
the same solution. We will start off by assuming that c, # 0. That means in order to 
satisfy Equation 10.110, 
(s)(s - 1) + 2s - 2 = 0. 
( 10.1 12) 

356 
DIFFERENTIAL EQUATIONS 
This leads to two solutions for s: s = 1 and s = -2. These two different values for s 
mean that Equation 10.104 really represents two solutions, 
m 
yl(x) = C a , , x n f '  
for s = I 
n=O 
(10.1 13) 
m 
Y ~ ( x )  
= C b,, xRP2 
for s = -2, 
(10.1 14) 
where the c, have been replaced by a, and b, to distinguish between the two solutions. 
The fact that there are two nontrivial solutions is no surprise since we are dealing with 
a second-order differential equation. Notice that the order of the indicial equations 
determined the number of solutions. 
Forcing s = 1 makes Equation 10.110 true for any nonzero value of a,. In the 
same manner, when s = -2, Equation 10.110 holds for a nonzero value of b,. With 
s = 1, the only way Equation 10.11 1 can be true is if al = 0. Likewise, if s = -2, 
the only way Equation 10.11 1 works is if bl = 0. Thus the indicial equations show 
that there are two solutions. One starts off with q # 0 and a1 = 0. The second has 
bo f 0 and bl = 0. Notice we could have satisfied Equations 10.110 and 10.11 1 
with co = 0 and c1 # 0. This would have led to two different values for s, i.e., s = 0 
and s = -3. It is not hard to see that the solutions generated with this choice are 
identical to the solutions we will obtain below. 
What remains now is to determine the rest of the a, and b, coefficients. They are 
obtained from the n 2 2 terms of Equation 10.109. For s = 1 and n B 2 we obtain 
a,,[(n + l)(n) + 2(n + 1) - 21 + an-2 = 0. 
n=O 
(10.1 15) 
This forms a generating equation for the a,: 
( 10.1 16) 
4 - 2  
an = --- n2 + 3n 
n 5 2. 
A similar argument for the b, with s = -2 gives 
bn-2 
n 2 2. 
Evaluating the first five coefficients for each series gives 
b, = -___ 
n2 - 3n 
So the two series solutions become 
) 
( 
x3 
x5 
y1(x) = a0 
x -  - + - 
- * . -  
lo 
280 
(10.1 17) 
(10.1 18) 

357 
THE METHOD OF FROBENIUS 
and 
(10.1 19) 
Now that these series solutions have been obtained, we must check to see if they 
converge. For this example, convergence can be shown using the ratio test. For y1 (x), 
convergence occurs if 
From Equation 10.116, this condition is met because 
( 10.120) 
(10.121) 
for all x except x = 0. We must check the x = 0 point specifically, because at this 
point there is a division by zero in Equation 10.120. A quick look back at Equation 
10.1 18 with x = 0 shows that there are no problems here either. So y1 (x) converges 
for all values of x .  
For y2(x), convergence occurs if 
From Equation 10.1 17, we have 
(10.122) 
( 10.123) 
for all x # 0. Again, we need to return to the original series in Equation 10.119 to 
see what happens at x = 0. Because this series contains a term with a negative power 
of x, namely bo/x2, yz(x) will not converge at x = 0. 
Plots of y1 (x) and y&) are shown in Figure 10.1. These functions have been given 
special names. With a0 = 1/3, y l ( x )  becomes the spherical Bessel function of order 
one and with bo = - 1, yz(x) becomes the spherical Neumann function of order one. 
Figure 10.1 Spherical Bessel and Neumann Functions 

358 
DIFFERENTIAL EQUATIONS 
The example given above is one of the most straightforward applications of the 
method of Frobenius. Complications can arise if the two indicial equations have a 
common value for s. This is treated in one of the exercises at the end of this chapter. 
10.5 THE METHOD OF QUADRATURE 
The method of quadratwe sometimes works to find solutions to nonlinear, second- 
order differential equations of the form 
( 1 0.1 24) 
where f(4) 
can be a nonlinear function of the dependent variable 9. 
105.1 The General Method 
The general method of quadrature is fairly simple. Starting with a differential equation 
in the form given by Equation 10.124, both sides are multiplied by d+/dx: 
The LHS of this equation is manipulated to give 
Integrating both sides over x and additional manipulation gives 
(1 0.125) 
( 10.126) 
(10.127) 
(10.128) 
( 10.129) 
g(4) = x, 
(10.130) 
where the function g(4) has been used to represent the result of the integration on 
the LHS of Equation 10.129. The solution to Equation 10.124 becomes 
= g - l w .  
(10.131) 
To apply this method it must be possible to perform both the integrals on the RHS 
of Equation 10.127 and the LHS of Equation 10.129, and also to invert Equation 

THE METHOD OF QUADRATURE 
359 
10.130. In this derivation, the integration constants have been ignored. As will be 
shown in the following example, these constants must be accounted for and make the 
application of this method more difficult than the steps above imply. 
10.5.2 A Detailed Example: The Child-Langmuir Problem 
The solution of the Child-Langmuir problem is a classic example of the use of the 
method of quadrature. It also demonstrates the importance of the boundary conditions 
for these types of solutions. 
The Child-Langmuir problem describes electron flow in the anode-cathode gap 
of an electron gun. The geometry is shown in Figure 10.2. A heater boils electrons 
off the cathode, into the anode-cathode gap. In the gap, the electrons are accelerated 
by an electric field imposed by the applied voltage V,. In an actual electron gun the 
anode is a mesh or has a hole to allow most of the accelerated electrons to continue 
past the anode and, in most cases, to the screen of a cathode ray tube. In this problem, 
however, we will concentrate only on the dynamics inside the anode-cathode gap. 
The cathode is located at x = 0, the anode at x = d, and the problem will be treated 
one dimensionally. This is a valid assumption as long as the dimensions of apparatus, 
perpendicular to x ,  are large compared to the gap distance d. 
If very few electrons are in the gap at any time, their interactions with each other 
will be negligible. In this case, the motion of any one electron is easy to describe. It 
sees a uniform electric field E in the gap that arises from the linear potential #(x), 
shown in Figure 10.3: 
= -a.(;i-)G* 
d 
V , X  
= - (2) Q. 
Cathode 
-7 
I 
x = o  
(10.132) 
Anode 
x = d  h 
10 
- v, + 
Figure 10.2 The Anode Cathode Gap of the Child-Langmuir Problem 

360 
DIFFERENTIAL EQUATIONS 
d 
Figure 103 Potential Seen by an Isolated Electron 
In the gap, the electron feels a constant force given by 
(10.133) 
where -go is the electron charge. The equation of motion for the electron can be 
written as 
(10.134) 
where m, is the electron mass. The solution to this differential equation is easily 
obtained by integrating twice. If it is assumed that the electron leaves x = 0 with 
zero velocity, that is ux(0) = 0, its position as a function of time is given by 
x(t) = (") 
2m, d 
t2. 
(10.135) 
Using conservation of energy, we can write an equation that relates the velocity of 
the electron, as a function of position, to the potential: 
= 40 4(x). 
(10.136) 
To arrive at Equation 10.136 we assumed that 1$(0) = 0. This equation can now be 
solved for the velocity: 
(10.137) 
In the Child-Langmuir problem, the situation is more complicated because there 
is not just a single electron in the anode-cathode gap. Instead, many electrons are 
emitted at a high, continuous rate, so the gap can be filled with billions and billions 
of them. The presence of the electron charges modifies the electric potential in the 
gap so it no longer looks as shown in Figure 10.3. The field must now be treated 
self-consistently using Maxwell's equations coupled with the equation of motion for 
the electrons. 

rHE METHOD OF QUADRATURE 
361 
We will look for a steady-state solution, with no time dependence. In that case, 
Maxwell’s equations simplify to 
- 
V X E = O  
V x H = J  
v * E0E = pc 
- 
(10.138) 
(10.139) 
(10.140) 
V*poH=O 
(10.141) 
where E and Hare the electric and magnetic field vectors, J is the current density, pc is 
the charge density, and e0 and po are the dielectric constant and magnetic permiability 
of free space. Because there is no time dependence, the electric field has no curl and 
can still be expressed as the gradient of the scalar potential: 
- 
E = -V+. 
(10.142) 
Also, the equations involving the electric and magnetic fields uncouple, and Equations 
10.140 and 10.142 can be combined, with the result 
(10.143) 
Given the boundary conditions of the problem, we will assume that &(O) = 0 and 
There are two unknowns in Equation 10.143: the potential +(x) and the charge 
density pc(x), both functions of x. Consequently, to solve the problem another in- 
dependent equation relating pc and + is necessary. This equation comes from the 
mechanics of the electron motion. To obtain this equation, we must introduce the 
current density J = J,(x) CX, which is just the charge density times the local velocity: 
d 2 W )  - PAX) 
dx2 
€0 
--- 
- 
+(d) = vo. 
J,(x) = pc(x)vx(x). 
(10.144) 
Using the expression derived in Equation 10.137, the velocity can be removed 
from Equation 10.144, and the result can be solved for pc(x): 
(10.145) 
Although we now have an equation relating the potential and the charge density, it 
involves another unknown J,(x), so some other information is still necessary. For 
that, look at the continuity equation, which we derived in Chapter 2: 
(10.146) 
Since we have assumed a steady-state solution, dpc/dr = 0, and the current density 
must be a constant J = -Jo Q. 
We have added a minus sign to make the constant J, 
positive. The actual value of Jo will be determined later, after we apply the boundary 
dPc 
- -  
V * J +  - 
= O .  
at 

DIFFERENTIAL EQUATIONS 
362 
conditions. Equation 10.145 becomes 
Inserting Equation 10.147 into 10.143 gives: 
(10.147) 
(10.148) 
This is a nonlinear differential equation in the form of Equation 10.124, whch can 
now be solved using the method of quadrature. First, equation 10.148 is multiplied 
by d+/dx to obtain 
(10.149) 
Integrating both sides from the cathode to some arbitrary point n gives 
Because 4(0) is zero, we can simpllfy this to 
(10.151) 
The other constant of integration, ( d + / d ~ ) ~ I , = ~ ,  
still needs to be evaluated. In 
order to do this, we must leave mathematics and return to the physics of the problem. 
According to Equation 10.142, this constant is just the square of the electric field at 
the cathode 
(d4/dx>21x=o = &O). 
(10.152) 
If there were no electrons in the gap, the potential would behave as shown in Figure 
10.3, and ( d 4 / d ~ ) ~ I ~ = , ,  
= Vz/d2. With electrons in the gap, the electric field, and 
consequently the potenhal, are modified. With a single electron in the gap, the electric 
field of the electron adds to the vacuum electric field as shown in Figure 10.4. To 
the left of the electron, the total electric field is decreased slightly and to the right 
the field is slightly increased. As we add more electrons to the gap, regardless of 
how they are distributed, the electric field is always decreased at the cathode and 
increased at the anode, compared to the vacuum field. The modified potential has the 
general form shown in Figure 10.5. Of course, because electrons are point charges, 
the electric field changes discontinuously from one side of an electron to the other. 
With many electrons in the gap, however, the charge can be modeled by a continuous 
charge density, and the electric field and potential vary continuously. 
As electrons are thermally emitted from the cathode, they are accelerated into the 
gap. If the electrons are emitted at a slow rate, there will be few electrons in the gap 
at any one time, and the electric field at the cathode will vary only slightly from the 

THE METHOD OF QUADRATURE 
363 
1 IEl decreased 
IBI increased 
Cathode 
Anode 
Figure 10.4 Modification of the Vacuum Electric Field by a Single Electron 
vacuum case. As the temperature of the cathode is increased, electrons are emitted 
at a higher rate, and there will be more electrons in the gap. Now the electric field 
and the slope of the potential at the cathode become smaller. Therefore, there are 
many solutions for this problem, depending on the temperature of the cathode. The 
solution we will obtain is for the highest rate of electron injection possible. This will 
occur just as the electric field at the cathode goes to zero, because if the field were to 
reverse sign, electrons would be forced back into the cathode. For this solution, the 
second boundary condition is (d+/dx)l,=o = 0. 
We can now proceed with the solution. Equation 10.15 1 becomes 
(1 0.153) 
This equation can be set up as an integral, again from the cathode where + = 0, to 
the point x in the gap: 
d 
Figure 10.5 Potential Depressed by Electrons in the Gap 
(10.154) 

364 
DIFFERENTIAL EQUATIONS 
Performing the integration in Equation 10.154 gives the potential as a function of x: 
(10.155) 
The RHS of this equation still has the unknown quantity J,. but we have yet to impose 
the boundary condition +(d) = V,. This condition gives 
Solving this for Jo gives 
( 10.156) 
(10.157) 
The quantity JCl is referred to as the Child-Langmuir current density. It depends both 
upon the gap distance and the applied voltage. 
In this derivation, we assumed a particular cathode temperature, so that the electric 
field at the cathode was zero. If the cathode temperature is below this value, fewer 
electrons are injected into the gap, and the current density will be less than the Child- 
Langmuir amount given by Equation 10.157. The current density is then limited by 
the cathode temperature, and the gap is in what is called temperature limited flow. 
In this case, the current density increases exponentially with temperature until the 
Child-Langmuir limit is reached, as shown in Figure 10.6. This critical temperature 
is called the Child-Langmuir temperature T,I. Above Tcl, electrons are emitted from 
the cathode at a higher rate than can be supported by the Child-Langmuir solution. If 
there are too many electrons in the gap, the electric field near the cathode is reversed, 
as shown in Figure 10.7. In this case, the field tends to decelerate the electrons, which 
are emitted from the cathode with a thermal distribution of velocities. The slow ones 
are returned to the cathode. Those that have enough velocity to make it to the point 
where the slope of the potential goes to zero, and the electric field reverses sign, 
Current density 
J 
_ _ _ _ _  
- 
I 
cl 
Tc* 
Figure 10.6 Temperature Limited How 

THE METHOD OF QUADRATURE 
365 
vo 
/ 
1 *&on 
of the 
I virtual cathode 
a 
Figure 10.7 Potential Distribution With the Cathode Temperature Above T,l 
ire accelerated to the anode. The point where the slope of the potential goes to zero 
.s called a virtual cathode. To the right of the virtual cathode the Child-Langmuir 
jolution discussed above holds. In this case, however, the voltage used to determine 
Icl from Equation 10.157 will be slightly more than the applied voltage V,. It is 
increased by the magnitude of 4(x) at the virtual cathode. Also, the anode-cathode 
spacing used in Equation 10.157 will be less than the actual distance between the 
real cathode and anode. It is reduced by an amount equal to the distance from the 
real cathode to the virtual cathode. Consequently as the cathode temperature is raised 
above Tcl, the current density rises slightly above the J,I calculated using the values 
V, and d .  This is a small effect, however, and the graph of the current density as 
1 function of the cathode temperature becomes, qualitatively, as shown in Figure 
10.8. Assuming the voltage and gap spacing remain constant as the temperature of 
the cathode is raised from zero, the current density initially increases exponentially. 
rhis is the temperature limited flow region. When the temperature reaches Tcr, the 
Current density 
I 
I 
Cathode Temperature 
- 
~ 
~ 
- 
1 
I 
I 
Cathode Temperature 
- 
Tc, 
_-Temperature 
l i m i t e L  -Space 
charge limited-- 
Figure 10.8 Temperature and Space Charge Limited Flow 

DIFFERENTIAL EQUATIONS 
366 
solution of Equation 10.157 is valid. As the temperature is raised above Tcl, there is 
only a slight increase of the current density with temperature. This is called the space 
charge limited flow region. 
10.6 FOURIER AND LAPLACE TRANSFORM SOLUTIONS 
Linear differential equations with constant coefficients can many times be solved 
using Fourier or Laplace transform techniques. If the solutions are finite and asymp- 
totically go to zero, they can be obtained using Fourier transforms. If they grow 
asymptotically, but the growth is exponentially limited, they can be handled with 
Laplace transforms. Solutions that grow faster than any exponential cannot be solved 
with these techniques. 
10.6.1 Fourier Transform Solutions 
We will consider the solutions to differential equations with space and time as the 
independent variables. If the solutions are finite and integrable, Fourier transforms 
with respect to the independent variables can be taken. Consider a function f ( x ,  t) 
that depends on both a spatial dimension n, and the time t. The spatial-time transform 
of this function E(k, o) 
can be obtained by a two-step process. First consider the time 
transform pair: 
L ( x , w )  = - 
1: dt f (x, t)e-jU 
( 10.158) 
Next set up the spatial transform as 
(10.159) 
(10.160) 
(10.161) 
where we have purposely swapped the signs of the exponent from the standard Fourier 
transform for the reason described below. These transformations can be combined to 
form a pair of equations relating f ( x ,  t) and F(k, w): 
(10.162) 

OURIER AND LAPLACE SOLUTIONS 
367 
Jow you can see the purpose of the sign switch for the spatial transform. The various 
omponents which make up f ( x ,  r) with positive w and positive k can be viewed 
s plane waves traveling in the positive x direction, with a positive phase velocity 
if w/k. 
Many times, in physics and engineering problems, the dependent variable is a 
rector quantity and there are three spatial dimensions. This complicates Equations 
0.162 and 10.163 and motivates a shorthand notation for the transform pair, Consider 
he vector electric field E(x, y, z, t) 3 E(?, t), which is a function of three spatial 
limensions and time. The transform pair associated with this vector field quantity 
:an be written as 
(10.164) 
n these expressions a propagation vector k has been introduced. In Cartesian geom- 
:try, it would be written as 
k = k,@, + k,@, + kZ&,. 
( 10.166) 
The quantities d 3F and d 3k represent differential volume elements in real andk-space. 
Sgain, in Cartesian geometry, 
d3F = dxdydz 
( 10.167) 
(10.168) 
d3k = dk, dk, dk,. 
Votice, the integrals involving these differentials are actually three integrals spanning 
.he entire three dimensional range of F or k. 
Example 10.5 
Maxwell's equations are classic examples of differential equations 
nvolving space and time as the independent variables. They are used to describe the 
xopagation of electromagnetic waves. The four equations can be written as 
d 
- _  
at 
- 
V X H(F, t) = -eoE(r, t) + J(F, t )  
(10.169) 
d 
at 
- 
v x E(F, t) = - -pOR(F, t )  
v . poH(F, t )  = 0 
v . E,E(F,t) = pc(F,t). 
- 
rhese equations form a set of coupled, partial differential equations with constant 
:oefficients. If the current density 3 and charge density pc are specified, they can be 
looked at as driving terms, which make the equations nonhomogeneous. Alternatively, 

368 
DIFFERENTIAL EQUATIONS 
if the current density and charge density are proportional to the fields, as is the case 
in linear dielectric media, the equations can be put into homogeneous form. 
These equations can be transformed using the Fourier transform pair given in 
Equations 10.164 and 10.165. Using the fact that on transformation, d / d t  -+ iw and 
d/dx --+ -ikz, Equations 10.169 become a set of coupled algebraic equations: 
- _ _  
- ik X H(k, w) = iwcoE(k, 0) + J(k, 
- 
0) 
-iG x E(E, 0) = -iwkoH(k, 0) 
(1 0.170) 
-iK * p0E(k, 0) = 0 
-ik €ojz(G, 0 )  = &(E, 0). 
If the transforms $and &, are known, these equations can be solved algebraically for 
the transforms of the electric and magnetic fields. The real field quantities are then 
obtained by inverting these quantities. So you see, the task of solving the coupled 
partial differential equations has been simplified to simple algebra, as long and the 
Fourier transforms and inversions can be taken. 
Example 10.6 
Another problem whose solution is facilitated by the Fourier trans- 
form approach is the damped, driven, harmonic oscillator. The geometry of the prob- 
lem is shown in Figure 10.9. The mass M is driven by an applied force d(t). It also 
experiences the spring force - K x(t) and a frictional force -a (dx/dt), proportional 
to its velocity. The differential equation describing the motion is 
(10.171) 
If d (t) possesses a Fourier transform @( o), 
and the solution x(t) transforms into X( o), 
Equation 10.171 can be transformed to 
(10.172) 
\ 
-a k(t) 
Figure 10.9 The Damped, Driven, Mass-Spring Oscillator 

FOURIER AND LAPLACE SOLUTIONS 
369 
The denominator of the RHS of Equation 10.172 can be factored, and the solution is 
given by the Fourier inversion 
where 
(10.174) 
The response x(t) can be obtained once Q(o) 
is determined. Let's consider the 
particular case where d ( t )  is an ideal impulse, d(t) = Zo8(t). Its transform is simply 
1 
D(w) = I,- &' 
and the response becomes 
(10.175) 
(10.176) 
where the integral has been converted to one in the complex @-plane on the Fourier 
contour y, shown in Figure 10.10. Now we can evaluate the integral using closure 
and contour integration. For t > 0, the contour must be closed in the upper half plane, 
where it encloses the two poles of the integrand. For t < 0, it must be closed in the 
lower half plane and encloses no poles. A little algebra shows x(t) is given by 
l o  
t c o  
imag 
1 
g-plane 
l a  
0 
_
_
 
2M 
0 
3 
real 
Figure 10.10 Fourier Inversion for Damped Harmonic Oscillator 

370 
DIFFERENTIAL EQUATIONS 
Figure 10.11 Damped Harmonic Oscillator Impulse Response 
This response of Equation 10.177 is plotted in Figure 10.11. The instant after the 
application of the impulse force at r = 0, the mass has acquired a finite velocity, 
but has not changed its position. This velocity initiates the oscillation that is damped 
to zero by the frictional force, Notice there is no response before t = 0, as you 
would expect, because obviously the system cannot respond until after the impulse 
has occurred. This kind of behavior is often referred to as being causal. Causality, 
a characteristic of solutions involving time, requires that there can be no response 
before the application of a drive. 
Equation 10.171 is a second-order differential equation and needs two boundary 
conditions to make its solution unique. Notice that Equation 10.177 is a unique 
solution with no arbitrary constants. The boundary conditions must have somehow 
been included in the Fourier transformation and inversion process that arrived at this 
solution. 
What are the appropriate boundary conditions for this example? For a problem of 
this type, the boundary conditions are typically the initial values of x and dx/dt at 
t = 0. The initial conditions in this case, however, are complicated by the &function 
located exactly at t = 0. There are two ways we can look at these initial conditions. 
First, because causality implies that x = 0 for t < 0, we can say just before the 
application of the impulse, at t = 0-, these values are x = 0 and dx/dt = 0. Then 
the impulse occurs at t = 0 and instantaneously changes the velocity. The other view 
is to enforce the initial conditions just after the impulse is applied, at t = 0+, and 
remove the impulse drive from the differential equation. In this case, these t = O+ 
“effective” initial conditions are x = 0 and dx/dt = I,,/M. 
The solution in Equation 10.177, obtained from the Fourier analysis, satisfies both 
these conditions at t = 0- and t = O+. Because there are no poles of 2imJ in the 
lower half w-plane, the Fourier inversion in Equation 10.176 will always be zero for 
t < 0, and thus satisfies the r = 0- initial conditions. Looking at Equation 10.177 
and its derivative for t > 0 shows that the t = O+ conditions are also satisfied. The 
instantaneous change in velocity, built into these “effective” boundary conditions, is 
a result of the 0 0  
term in Equation 10.175. 
The Fourier approach worked in this example, because the driving force could 
be Fourier transformed, and because the damped response could also be Fourier 

FOURIER AND LAPLACE SOLUTIONS 
371 
ransformed. In many problems, it is not as obvious that the Fourier approach will 
work. For an unstable system, even if the driving term can be Fourier transformed, the 
-esponse can diverge in time and, therefore, will not have a valid Fourier transform. 
Zonsequently, it is always safer to use the Laplace transform approach described 
3elow, if the possibility of instability is present. 
10.6.2 Laplace Transform Solutions 
The Laplace transform can also be used for solving differential equations, and is 
%sentially the same as the Fourier approach described above. It has the advantage 
3f being able to treat unstable systems which grow exponentially. Unfortunately, it 
has an added complication because the initial conditions of the problem are present 
in the transforms themselves and need to be treated carefully. 
The standard Laplace transform pair is 
(10.178) 
(10.179) 
For the standard Laplace transform, it is assumed that f ( t )  is zero for t < 0. The 
inversion in Equation 10.179 is performed along a Laplace contour, to the right of all 
the poles of E(s). This position of the Laplace contour assures that the f ( t )  generated 
by the inversion is zero for t < 0. 
As with the Fourier approach, the first step is to Laplace transform the differential 
quation. Let the Laplace transform operation be represented by the L operator: 
L{x(t>} = x_(s). 
(10.180) 
rhen from the properties of the Laplace transform, 
1 { F} 
= .Tx_(sJ - x(0) 
(10.181) 
(10.182) 
The initial value terms on the RHS's of these equations complicate things. If, as is 
often the case, we know the solution and all its derivatives are zero at t = 0, we can 
drop these terms. Otherwise, they have to be carefully carried through the analysis, 
as discussed in the example that follows. 

372 
DIFFERENTIAL EQUATIONS - 
Example10.7 As an example of the Laplace transform approach, consider an 
undamped harmonic oscillator, a simplified version of the example discussed in the 
previous section. The system can be described by the configuration shown in Figure 
10.9, with no frictional force, i.e., a = 0. The differential equation for this system 
becomes 
M* 
+ Kx(t) = d(t). 
dt 
Applying the Laplace transformation to both sides gives 
( 10.183) 
(10.184) 
As before, let the driving force be equal to an impulse of magnitude I,, applied at 
t = 0: 
d(t) = Z&t). 
(10.185) 
The solution should be causal, so x(t) = 0 for all t < 0. What values should be used 
for x and dx/dt at t = 0 in Equation 10.184? As with the Fourier transform example 
of the previous section, because of the b-function drive at t = 0, there are two ways 
to think about these initial conditions. 
The first approach defines the initial conditions at t = 0- and includes the 
Laplace transform of the b-function drive in Equation 10.184. At t = 0-, x = 0 and 
dx/dt = 0 so that Equation 10.184 becomes 
Ms2XsJ + KXsJ = I,. 
(10.186) 
Now we can solve for the transformed solution: 
The final step is to invert this expression, 
(1 0.187) 
( 10.188) 
which can be evaluated using closure. The Laplace contour L is placed to the right 
of the two poles located at s = +im, 
as shown in Figure 10.12. For t < 0, 
the contour must be closed to the right, and no poles are enclosed. When t > 0, 
however, both poles are enclosed. The residue theorem and a little algebra give the 
final solution: 
( 0  
t < O  
(10.189) 

FOURIER AND LAPLACE SOLUTIONS 
373 
imag I
-
 
s-plane 
I 
Figure 10.12 Inversion Contour for the Harmonic Oscillator 
The second approach defines the initial conditions at r = O+. In this case, the 
Laplace transform of the &function drive is not included in Equation 10.184. The 
initial conditions must now be evaluated just after the impulse, so that x = 0 and 
dx/dt = Z,/M at t = O+. In this case, Equation 10.184 becomes 
( 10.190) 
This leads to an equation for X(s> that is identical to Equation 10.187, and conse- 
quently the same solution for x ( t )  given by Equation 10.189. Notice that if the f = O+ 
initial conditions as well as the Laplace transform of the 6-function drive were in- 
cluded in Equation 10.184, twice the correct answer for x(t) would be generated. 
In this example, the drive was a &function. Of course, this same approach works 
for other dnving functions as well, as long as it is possible to obtain their Laplace 
transforms. Notice that this particular solution requires the use of the Laplace trans- 
form, as opposed to a Fourier transform. Even though the driving term can be Fourier 
transformed, the solution cannot. If a Fourier transform of this solution were at- 
tempted, it would have poles lying on the real *-axis, and we could not invert it. 
Example 10.8 This example explores the behavior of the unstable electrical circuit 
shown in Figure 10.13. This is a simple RLC-circuit except for the negative resistance, 
-Ro. This oddity is a simplified model for an active element that can actually add 
energy to the circuit. The circuit is driven by a sinusoidal voltage source 
t < O  
t > 0 
V, sin(o, t )  
us@> = 
(10.191) 
For t < 0 the current and all voltages in the circuit are zero. 
voltage drops around the loop, 
To obtain the differential equation that describes this circuit, start by summing the 

374 
DIFFERENTIAL EQUATIONS 
l
-
 
v1 
f 
I 
L O  
Figure 10.13 Unstable Circuit 
di(t) 
1 
dt i(t) - Roi(t) + Lo - 
C O  
dt ’ 
= 
(10.192) 
where i(t) is the current in the loop, as shown in Figure 10.13. Equation 10.192 is a 
mixed integral-differential equation. We can convert it to a pure differential equation 
by taking the derivative of both sides: 
d2i(t) 
di(t) 
1 
&(t) 
Lo- 
- Ro- 
+ -i(t) = - 
dt2 
dt 
Co 
dt 
’ 
(10.193) 
This is a second-order, nonhomogeneous differential equation with i(t) as the depen- 
dent variable. The driving term of this equation is dv,(t)/dt. 
Neither vs(t) nor its derivative have a valid Fourier transform, so we cannot Fourier 
transform Equation 10.193. We can, however, still take the Laplace transform of both 
sides of Equation 10.193 to get 
where I@ and V&J 
are the Laplace transforms of i(t) and v,(t), respectively. The 
initial conditions for i and di/dt at t = 0 are zero, because causality requires the 
response does not occur before the application of the drive. 
The Laplace transform of v,(t) is 
( 10.195) 

375 
FOURIER AND LAPLACE SOLUTIONS 
so the expression for l(s) becomes 
V O  mos 
(10.196) 
(5 + iw,)@ - io,)(L,s2 - R,s + l/Co)' 
as) = 
The quadratic term in the denominator can be factored as 
Los2 - R,s + l/Co = Lo@ - s+)(s - K), 
(10.197) 
where 
(10.198) 
Notice that with a negative resistance, 5, and s- both have a positive real part. The 
Laplace inversion for i(t) is 
The Laplace contour and the poles of the integrand are shown in Figure 10.14. 
To solve Equation 10.199 for arbitrary o,, R,, Lo, and C, is a messy task, which 
is simplified by choosing specific values for these quantities. We will let o, = 1, 
Lo = 1, and choose the other values so that sl,z = 1 +- 2i. Equation 10.199 then 
becomes 
imag 
0 
s-plane 
C 
real 
Figure 10.14 Laplace Inversion for the Unstable Circuit 

376 
DIFFERENTIAL EQUATIONS 
t 
Figure 10.15 Response of the Unstable Circuit 
Using closure and residue techniques, the integration of Equation 10.200 can be 
performed to give 
t < O  
t > 0 
V U  
v o  t 
3vu 
f 
. (10.201) 
cost - - 
sint - -e 
cos(2t) + -e 
sin(2t) 
10 
5 
20 
For t > 0 the current response can be written as 
(10.202) 
The part of this response with a time dependence of cos(t + 0 . 1 5 ~ )  is a result of 
the driving voltage at w, = 1. It persists at a constant amplitude for all t > 0. The 
part of the response with a time dependence of er cos(2t + 0 . 2 ~ )  
is referred to as the 
characteristic response of the circuit, because it is determined entirely by R,, Co, and 
Lo. It grows exponentially in time because the negative resistance causes the circuit 
to be unstable. Because of this instability, the long-term behavior of the circuit is 
dominated by this characteristic exponential response, while the part oscillating at 
the drive frequency w, becomes negligible. 
A sketch of Equation 10.202 is shown in Figure 10.15. In this figure the exponential 
growth rate has been decreased by an order of magnitude, i.e., we have used e0." 
instead of e', so that the effects of the two parts of the response can be seen more 
clearly. For short times, both components are clearly visible, but for long times the 
unstable part dominates. 
"0 
f 
cos(t + 0.1%) - -e 
cos(2t + 0.217). 
4 
i(t) = - 
V U  
2 J J  
10.7 GREEN'S FUNCTION SOLUTIONS 
The Green's function approach is a powerful technique for finding solutions to lin- 
ear, homogeneous, and nonhomogeneous differential equations. Two methods for 
obtaining Green's function solutions are discussed below. The first uses the Laplace 
transform and is limited to linear differential equations with constant coefficients. 
The second method is more general and can be applied to any linear differential 
equation. 

GREEN'S FUNCTION SOLUTIONS 
377 
10.7.1 Derivation Using the Laplace Transform 
Consider an ordinary, second-order, nonhomogeneous, linear differential equation 
with constant coefficients 
d2r(t) + Bod") + Cor(t) = d(t), 
dt 
A, 2 
dt 
(10.203) 
where r ( t )  is the response to a known driving function d(t). The independent variable 
f is assumed to be time, so the concept of causality applies to the response. In other 
words, the response cannot occur before the application of the driving term. Assume 
the driving function is zero for t < 0, and that it has a standard Laplace transform 
D_(s). The causal boundary conditions are r(0) = 0 and dr/dt\t=o = 0. The Laplace 
transform of the differential equation becomes 
+ Bog + C,)R_O = LW, 
(10.204) 
where &s) is the Laplace transform of the response. Equation 10.204 can be solved 
for &): 
D O  
A,s2 + Bog + C, ' 
= 
which, in turn, can be inverted to give the response 
(10.205) 
(10.206) 
So far, this is nothing more than a recap of the Laplace transform approach to solving 
differential equations, which we presented in the previous section. At this point, 
however, we take a completely different tack. 
Returning to the Laplace transform of the response, it can be looked at as the 
product of two functions 
= QOG(d9 
(10.207) 
where C(s) = 1/(A,s2 + Box + C,). Consequently, from the product property of 
Laplace transforms, the time domain response is the convolution of two functions, 
where g(t) is the inverse Laplace transform of 
(10.209) 

378 
DIFFERENTIAL EQUAnONS 
The function g(t) needs some interpretation before jumping into the convolution 
integral of Equation 10.208. The function GsJ satisfies the algebraic equation 
(A,$* + B0s + Co) 
G(sJ = 1. 
(10.210) 
Therefore, the same reasoning that led to Equation 10.204 implies g(t) is a solution 
to the differential equation 
d2g(t) + B,-- d m  + Cog@) = 6(t), 
A0 dr2 
dt 
(10.21 1) 
with the same boundary conditions for g(t) as for r(t), namely g = 0 and dg/dt = 0 
at t = 0. The Laplace contour in Equation 10.209 is to the right of all of the poles 
of the integrand, so that g(t) is zero for all t < 0. Thus, the function g(t) is the 
response to a drive d(t) = 6(t), a unit impulse applied at t = 0. For this reason, g(t) 
is sometimes called the impulse response function. 
The function used in the convolution integral, Equation 10.208, is not g(t) but 
rather g(t - T). This is simply g(t) delayed by an amount 7. Because the A,, B,, C, 
coefficients of Equation 10.211 are not functions of time, g(t - T) is the solution 
to this equation with 6(t) replaced by 6(t - T), and with the causal condition that 
g(t - T) is zero for t < 7. In other words, delaying the 6-function drive just delays the 
impulse response by the same amount, without changing the functional shape. This 
behavior is called translational invariance. The function g(t - T )  is called the Green’s 
function. The translational invariance of this Green’s function is a consequence of 
the constant coefficients in the differential equation and the nature of the boundary 
conditions. As we will show, not all Green’s functions have this behavior. 
The convolution in Equation 10.208 is an operation involving the driving function 
and the Green’s function. It, in effect, sums up the contributions from all the parts of 
d(t) to give the total response r(t). Notice, once the Green’s function for a specific 
differential equation is known, the response to any drive function can be determined 
by evaluating the integral in Equation 10.208. This is powerful stuff! 
Notice this derivation required several things. First, the differential equation had to 
be linear, with constant coefficients. Second, the problem had to have causal boundary 
conditions. In the next section, we will explore a derivation which requires only that 
we have a linear differential equation and a particular set of homogeneous boundary 
conditions. Eventually, even this last condition will be relaxed when we will discuss 
Green’s function solutions for problems with nonhomogeneous boundary conditions. 
Example 10.9 As an example, again consider the undamped harmonic oscillator, 
shown in Figure 10.16, with a square pulse driving force: 
t < O  
To < t. 
(10.212) 

GREEN'S FUNCTION SOLUTIONS 
379 
Figure 10.16 
Undamped Harmonic Oscillator with Square Pulse Drive 
The nonhomogeneous differential equation describing the motion is 
M
d
2
 + K,x(t) = d(t). 
(10.213) 
As usual, we will assume that there is no response before the drive, and impose the 
boundary conditions x = 0 and dx/dt = 0 at t = 0. 
The first step in solving a problem of this type is to determine the Green's function. 
This is done by solving the equation 
+ K,g(t - T) = 6(t - T), 
Md2g:tl T, 
(10.214) 
with the same type of boundary conditions: 
g ( t - T ) = O  
t < T  
= o  
t < T .  
(10.215) 
dg(t - 7) 
dt 
Applying a Laplace transform to both sides of Equation 10.214 gives 
(MS* 
+ K,)G(s,T) = e-ST, 
(10.216) 
where G(s, 
7) is the Laplace transform of g(t - 7). This equation can be solved for 
- 
G(g, 7) and then Laplace inverted to give the Green's function 
(10.2 17) 
1 
(1 /M)eX(('-') 
(s + i J Z Z %  - iJKa/M)' 
g(t - 7) = g 
L d S  
Evaluating the integral using a Laplace contour on the right of both poles in the 
denominator gives 

380 
DIFFERENTIAL EQUATIONS 
I 
i 
2 
TO 
Figure 10.17 The Driving Force and Green’s Function vs. 7 
The next step is to use this Green’s function and the driving function in Equation 
10.208 to determine x(t): 
x(t) = 
dTd(T)& - 7). 
(10.2 19) 
To evaluate this convolution integral, it is instructive to plot d(t) and g(t - T) as 
functions of T, as shown in Figure 10.17. The integral of Equation 10.219 must be 
evaluated for three separate ranges of I: t < 0, 0 < t < To, 
and r > To. The first 
range is easy because, from Figure 10.17, you can see there is no overlap of d ( T) and 
g(t - T), so x(t) = 0 for t < 0. In the next range, when 0 < t < To, the functions 
overlap in the range 0 < T < t, as shown in Figure 10.18. The expression for x(t) 
becomes 
1: 
sin [ d m ( t  - T)] 
0 < f < To. 
(10.220) 
This integral is straightforward, and gives the result 
x(t) = 3 [l - cos ( d s t ) ]  0 < t < To. 
KO 
(10.221) 
Figure 10.18 Overlap for 0 < t < To 

GREEN’S FUNCTION SOLUTIONS 
381 
Figure 10.19 Overlap for t > To 
The overlap in the final range, when t > To, is shown in Figure 10.19. In this case, 
the integral for x(t) becomes 
which evaluates to 
- cos ( J G t ) ]  
t > To. 
(10.223) 
A plot of x(t) is shown in Figure 10.20. Notice that n(t) and dx(t)/dt are continuous 
everywhere, and d2x(t)/dt2 is continuous everywhere, except at t = 0 and t = To, 
where the driving force changes discontinuously. 
Figure 10.20 Response of Harmonic Oscillator to a Square Pulse of Force 

382 
DIFFERENTIAL EQUATIONS 
10.7.2 A More General Approach 
There is another approach to Green’s functions which requires only that the differ- 
ential equation be linear, and that its solutions satisfy what are called homogeneous 
boundary conditions. A general, second-order, nonhomogeneous equation has the 
form 
d2r(x) + B(x)* 
+ C(x)r(x) = d(x). 
A ( x ) F  
dx 
(10.224) 
We are not going to assume the independent variable is causal, and to emphasize 
this we have used x as the independent variable instead of f. The coefficients are 
no longer constants, but can be a function of the independent variable. The three 
functions A(x), B(x), C(x), and the driving function d(x) are presumed to be known. 
In addition to this differential equation, two boundary conditions are necessary to 
make the solution r(x) unique. These boundary conditions usually involve r(x) or its 
first derivative dr(x)/dx evaluated at the same or different values of x. We will begin 
by looking at the consequences of the linear nature of the equation and develop a 
Green’s function solution for r(x). Then the question of the boundary conditions will 
be addressed. 
Because the differential equation we are dealing with is linear, a superposition 
principle applies to its solutions. That is to say, if a drive dl ( x )  produces the response 
rl(x), and another drive dz(x) produces the response rz(x), then the response to 
a drive function d(x) = Aldl(x) + A&$$), 
where A1 and A2 are constants, is 
simply r(x) = Alrl(x) + Alrz(x). This, of course, assumes the boundary conditions 
are satisfied for all three responses. Now assume that the drive is a &function 
d(x) = 6(x), and that this drive produces a response r(x) = g(x). Next, shift the 
driving delta function to x = E, so that d(x) = 6(x - 6). The response to this shifted 
&function drive will depend upon 5, but because the coefficients of the differential 
equation are not constant, it cannot be assumed that the response is just g(x) shifted 
by 6. We will represent the response to the shifted 6-function drive by the Green’s 
function &It), which is pronounced “g of x given e. This function satisfies the 
differential equation 
+ B(x)% 
+ C(x)g(xlt) = 6(x - 6). 
dx 
(10.225) 
The superposition principle then says, if the drive is a discrete sum of weighted, 
shifted &functions, i.e., 
the response is a sum of Green’s functions weighted with the same An: 
(10.226) 

GREEN’S FUNCTION SOLUTIONS 
383 
t 
X 
This superposition action for a drive composed of the discrete sum of two &functions 
is shown in Figure 10.21. If d(x) goes to a continuous sum of weighted &functions, 
(10.228) 
the response becomes a continuous sum of Green’s function with the same weighting: 
(10.229) 
An arbitrary function d(x) can always be written as a continuous sum of 6- 
functions: 
(10.230) 
This means that, given any driving function d(x), the solution to Equation 10.224 can 
be written as 
(10.231) 
The Green’s function g(x16) is the solution to Equation 10.225, provided the boundary 
conditions work out. 

384 
DIFFERENTIAL EQUATIONS 
Equation 10.225 cannot be solved for the Green’s function without specifying 
boundary conditions for g(x15). Therefore, to obtain the Green’s function and eval- 
uate the response in terms of the Green’s function integral of Equation 10.231, the 
boundary conditions for &It) must be determined so that Equation 10.231 pro- 
duces an r(x) that still satisfies the original boundary conditions of the problem. Zero 
boundary conditions on r(x) are easy to handle. By zero boundary conditions we 
mean that r(x) or a derivative of r(x) is zero at some value of x, say x = x,. If this is 
the case, these boundary conditions on r(x) will be established by a Green’s function 
that obeys the same boundary conditions, that is g(x15) or its derivative is zero at the 
same x = x,, for any 5. These zero boundary conditions are specific examples of 
homogeneous boundary conditions, which are discussed below, list for the specific 
case of a second-order differential equations, and then in the general case. 
For a second-order linear differential equation, two boundary conditions must be 
specified. They generally involve the response andor its first derivative, evaluated at 
the same or two different values of the independent variable. The most general forms 
for homogeneous boundary conditions for a second-order equation are 
(10.232) 
(10.233) 
where a1, a2, PI and pZ are constants. If these types of boundary conditions are 
applied to a &I[), 
it is easy to show that the r(x) generated by Equation 10.227, 
and consequently Equation 10.231, will satisfy the same homogeneous boundary 
conditions. The homogeneous boundary conditions of the Laplace Example 10.9 fit 
thisformwitha=b=O,al = & = l r a n d a 2 = P 1 = O .  
For a linear, nonhomogeneous differential equation of nth order, there must be n 
boundary conditions that may involve the response up to its n - 1 derivative. For an 
nth-order equation, the general form of the homogeneous boundary conditions are n 
different equations: 
di-’ 
r (x) 
2 a
i
r
 = 0. 
i = l  
(10.234) 
The first equation involves a set of constants ai and the function andor its derivatives 
are evaluated at x = a; the second, a set of constants pi and terms evaluated at x = b; 
etc. 
The Green’s function argument is now complete. Given a linear, nonhomogeneous 
differential equation in the form of Equation 10.224 and a complete set of homo- 
geneous boundary conditions in the form of Equation 10.234, the response can be 
obtained from a Green’s function integral. The Green’s function g(x15) is the solution 
to the original differential equation with a &function drive and satisfies the same set 
of homogeneous boundary conditions. 

GREEN’S FUNCTION SOLUTIONS 
3885 
This derivation for the Green’s function solution is correct, but a little sloppy. 
In Appendix D there is a more formal, but far less intuitive proof for the existence 
of Green’s functions and for the properties of homogeneous boundary conditions. 
These proofs are for second-order differential equation, but are easily extended to 
linear equations of any order. 
~ 
~ 
~~ 
~~ 
~ 
~~ 
~ 
~ 
~ 
~~~ 
Example 10.10 We will now investigate a classic problem which uses a Green’s 
function solution. In this example, space is the independent variable instead of time. 
Imagine a uniform, elastic string, stretched between two fixed points, as shown in 
Figure 10.22. The ends of the string, at x = 0 and x = L, are both held at y = 0. A 
dlstributed force per unit length F(x) is applied along the string. Our sign convention 
is that a positive force points in the negative y direction. Assume the string has a 
uniform tension To, and that the displacement caused by F(x) is so small that this 
tension does not change. 
The differential equation describing the displacement of the string is 
(10.235) 
and the boundary conditions are homogeneous, with y(0) = y(L) = 0. Because 
this is a linear differential equation, it can be solved using a Green’s function. The 
displacement becomes 
where g(x16) is a solution to 
(10.236) 
(10.237) 
with g(O16) = g(L16) = 0. Because the Green’s function obeys homogeneous bound- 
ary conditions, the solution it generates will obey the same homogeneous boundary 
conditions. 
F(x) 
Figure 10.22 The Stretched String Problem 

386 
DIFFERENTIAL EQUATIONS 
x = L  
Figure 10.23 The Green’s Function Displacement of the String 
The interpretation of Equation 10.237 is simple and is depicted in Figure 10.23. 
The driving force is a single 6-function located at x = 6. The response to that 
driving term is the Green’s function &It), which is zero at x = 0 and x = L for all 
0 < 5 < L. Notice, in the small displacement approximation, the vertical components 
of the tension add to cancel the applied 6-function force per unit length. The fact that 
the horizontal components of the tension do not exactly cancel is ignored. 
The Green’s function for this problem can be obtained by solving Equation 10.237. 
Everywhere, except exactly at the point x = 6, the equation for g(x15) is 
(10.238) 
which has the simple solution 
g(x15) = Ax + B. 
(10.239) 
Because there are two different regions that have this solution, separated by the point 
x = 6, we can write the general solution for g(x16) as 
(10.240) 
where the constants A1 through B2 must be determined. 
constants, 
The boundary conditions at x = 0 and x = L give two equations involving these 
Al(0) + B1 = 0 
(10.241) 
and 
A2(L) + B2 = 0. 
(10.242) 
A third equation is obtained by requiring &I[) to be continuous at x = 6: 
A ] ( [ )  + BI = A2(6) + B2. 
(10.243) 

BOUNDARY CONDITIONS 
387 
In other words, the string does not break due to the force. The fourth and final equation 
takes a bit more work, and involves the driving term 6(x - 0, which has not yet been 
used. Start by integrating Equation 10.237 from x = 5 - E to x = 5 + E in the limit 
of E --+ 0: 
( 10.244) 
The LHS of this equation goes to To times the discontinuity in slope of g(xl5) at 
x = 6, and the RHS is one because we are integrating over a &function. The result 
is 
(10.245) 
Taking the limit as E + 0, and referring back to Equation 10.240, gives 
To (A2 - Al) = 1. 
(10.246) 
Notice that this equation is essentially a statement that the vertical forces at x = 5 
must cancel. Equations 10.241-10.243, along with Equation 10.246, constitute a set 
of four independent equations that uniquely determine Al, A2, B1, and Bz. Equation 
10.240 for the Green’s function becomes 
Notice that this Green’s function does not have the simple form g(x - 5) of a 
translationally invariant solution. A quick look at Figure 10.23 and the boundary 
conditions at the ends of the string show why this must be the case. 
This Green’s function can now be used in Equation 10.236 to obtain the displace- 
ment of the string for an arbitrary loading force density F(x). Some care, however, 
must be taken in setting up this integral. For this integration, x is held fixed, some- 
where between 0 and L, while the integration variable 5 ranges from 0 to L. According 
to Equation 10.247, g(x15) = g2(x15) when 5 < x and 
= g1(x15) when 5 > x. 
The integration of Equation 10.236 must therefore be broken up into two parts. The 
Green’s function solution for an arbitraxy F(x) becomes 
10.7.3 Nonhomogeneous Boundary Conditions 
Up to now, the Green’s function analysis has been for problems with homogeneous 
boundary conditions. In this section, we will review, briefly, why homogeneous 

388 
DIFFERENTIAL EQUATIONS 
Figure 10.24 Homogeneous Boundary Conditions 
boundary conditions are important. Then a method for dealing with nonhomogeneous 
boundary conditions will be introduced. 
In the case of the stretched string problem, the example of the previous section, 
the homogeneous boundary conditions specified that the response be zero at either 
end of the string. These boundary conditions fit the general homogeneous form 
of Equation 10.234. We required the Green’s function to obey the same boundary 
conditions. This works because when the Green’s function is constrained to be zero 
at x = 0 and x = L, as shown in Figure 10.23, an arbitrary sum of Green’s functions 
will automatically obey the same conditions. As an example of this, the sum of 
three Green’s functions is depicted in Figure 10.24. Therefore, forcing the Green’s 
function to obey the same boundary conditions as the original differential equation 
is the proper approach in this case. 
The difficulty with nonhomogeneous boundary conditions is easily demonstrated 
with the string problem by changing the second boundary condition so that y(L) = yo. 
A generic load and displacement which obeys this new condition is shown in Figure 
10.25. Problems immediately develop if these boundary conditions are applied to the 
Green’s function, and the response is generated with a summation of several of them. 
A sum of three of these Green’s functions is shown in in Figure 10.26. As with Figure 
10.24, here F(x) is composed of three weighted 6-functions. The response to each 

BOUNDARY CONDITIONS 
389 
Y = Yo 
= O  
Figure 10.25 Loaded String with Nonhomogeneous Boundary Conditions 
of these individual forces is shown on the left, each one proportional to a Green’s 
function which satisfies the nonhomogeneous boundary conditions. If these solutions 
are simply summed, as would be done by the Green’s function integral, the result 
would not satisfy the required boundary conditions. This is shown on the right of 
Figure 10.26 where the three terms have been summed to give y(L) = 3y0. Clearly, if 
Y = Y  _, 
~.~ 
Y =  
,/’ 
i 
- 
The Nonhomogeneous Boundary Condition Problem 
Y = 3 Y  
5,) 
I 

390 
DIFFERENTIAL EQUATIONS 
g(x16) satisfies the nonhomogeneous boundary conditions, the correct answer is not 
generated by the Green’s function sum. 
Fortunately, there is a way around this problem. It is clear from the picture above 
that to keep the generic Green’s function integral 
(10.249) 
from diverging for n = a or x = b, the Green’s function must be forced to have 
homogeneous boundary conditions. But we can add to this function another term, 
which makes the solution obey the nonhomogenwus boundary conditions. 
To see how this works, let’s return to the string problem with the nonhomogeneous 
boundary conditions y(0) = 0 and y(L) = yo. The solution y(x) still satisfies the 
differential equation 
d2Y(X> 
To- 
= F(x). 
dx2 
(10.250) 
Now break y(x) into two parts, 
Y ( 4  = Y l W  + y2(x), 
(10.25 1) 
where y1 (x) satisfies the nonhomogeneous differential equation, Equation 10.250, 
but has homogeneous boundary conditions, i.e., yl(0) = 0 and yl(L) = 0. Let y2(x) 
satisfy the homogeneous version of Equation 10.250 
(10.252) 
with the nonhomogeneous boundary conditions, y2(0) = 0 and y2(L) = yo. The 
sum of these two solutions, yl(x) + y2(x), will satisfy both the nonhomogeneous 
differential equation and the nonhomogeneous boundary conditions. The y1 (x) part 
of the solution can be found with the standard Green’s function approach, by requiring 
the Green’s function to satisfy homogeneous boundary conditions. The y2(x) part is 
obtained by solving a simple homogeneous differential equation and then applying 
the nonhomogeneous boundary conditions. 
In this case, yl(x) is the same solution found in Equation 10.248: 
L 
(10.253) 
Yl(4 = [ d S F ( O G ( L  
-6 
- x) + J: dtF(O&L 
- 0. 
The y2(x) part of the solution, the solution to Equation 10.252, is simply 
Y ~ ( x )  = AX + B. 
(10.254) 
Applying the nonhomogeneous boundary conditions forces the constants to be A = 
yo/L and B = 0, so that the total solution is 

BOUNDARY CONDITIONS 
391 
--x 
L 
Y ( X )  = L x d S f W T , L ( L  
-5 - x> + / &F(~)T,L(L - 0 + 
(10.255) 
~n 
summary, to solve a linear, nonhomogeneous differential equation with non- 
homogeneous boundary conditions, break the problem into two parts. Use a Green’s 
function approach to solve the original, nonhomogeneous differential equation using 
a Green’s function which satisfies homogeneous boundary conditions. The desired 
solution is then the sum of this solution and the solution to a homogeneous version 
of the differential equation, which obeys the nonhomogeneous boundary conditions. 
X 
L 
10.7.4 Multiple Independent Variables 
To this point, all our Green’s function examples have had a single independent vari- 
able. How are things modified when we consider higher dimensions? In this section, 
through examples, we will explore two-dimensional solutions for two common prob- 
lems, diffusion and wave propagation. Both examples have one space dimension and 
the time dimension. 
Example 10.11 The Green’s function approach can also be used to solve linear 
partial differential equations. Consider the diffusion of heat along the infinite, one- 
dimensional conducting rod shown in Figure 10.27, The homogeneous diffusion 
equation 
dZT(x, t )  
dT(x, t )  
D 2 - - -  
= o  
ax* 
at 
(10.256) 
describes the time and space dependence of the temperature T(n, t )  along the rod. 
The constant D is called the diffusion coefficient. A common way to formulate a 
diffusion problem is to specify T(x, 0) = T,(x), the initial temperature distribution 
at t = 0, as shown in Figure 10.27, and then solve for T(x, t) for t > 0. It will be 
assumed that T(x, t) -+ 0 as x + 503 for all t. 
- b  
d-+- 
- w  
Figure 10.27 One-Dimensional Conducting Rod with Initial Temperature Distribution 

392 
DIFFERENTIAL EQUATIONS 
At first, this does not look like a problem that could be solved using a Green’s 
function method, since the differential equation is homogeneous and there is no 
driving term. It is, however, a linear equation, so the superposition principle does 
apply. That is to say, if To(x) = 6(x - 5) results in T(x, t) = g(x15, t), then 
TAX) = C AnNx - e n )  
(10.257) 
n 
will result in 
(10.258) 
n 
for all t > 0. Taking these summations to continuous integrals allows us to argue 
that, if 
T&) = Srn 
d5 To(S)& - 51, 
(10.259) 
--m 
then 
(10.260) 
for all t > 0. Notice how we have set up the arguments of the Green’s function. The 
variables x and 6 have been used as standard Green’s function variables, while t is 
not involved in the Green’s function integral of Equation 10.260. This is indicated by 
the notation in the argument of the Green’s function. Only variables immediately to 
the right of a “I” will take part in Green’s function integrations. 
So the solution to the problem can be formulated using a Green’s function, with 
the initial temperature distribution acting as the weighting function in the Green’s 
function integral. The boundary conditions require T(x, I) -t 0 as x -+ +co and are 
homogeneous. Consequently, the same boundary conditions are applied to g(x15, t). 
This Green’s function must satisfy 
(10.261) 
with the initial condition at I = 0 
g(xl6,O) = 6(x - 5) 
(10.262) 
and the boundary conditions described above. Notice in this problem, the initial 
condition is treated very differently from the boundary conditions. 
One method to solve Equation 10.261 is to use a Fourier transform in space. A 
standard Laplace transform cannot be used because the solution must be valid for 
all x. We define 
- 
G(k(6,t) - 
1: 
dxg(X15, t)e-ik“, 
( 10.263) 

BOUNDARY CONDITIONS 
md the Fourier transform of Equation 10.261 becomes 
393 
(10.264) 
This is an easy first-order equation, with the general solution 
- 
G(k15, t )  = G
-
 
ePdk2', 
(10.265) 
where the complex constant G
-
 
= G(k15.0). But g(xl5, 0) = 6(x - t), so 
G = 3.'&15>0)) 
The Fourier transform of the Green's function becomes 
(10.266) 
(10.267) 
md the Green's function, itself, is the Fourier inversion of this expression: 
1 
s(xl57t) = - dk - e - i k t e - d k 2 t e r ~  
(10.268) 
This inversion can be done most easily using a convolution trick. The Fourier 
6
-
m
 
Irn J2.rr 
ransform of this Green's function can be looked at as the product of two functions 
W15, 
t )  = F(k15, t)Ll(k15.t>, 
(10.269) 
where 
(10.270) 
(10.271) 
The Green's function is therefore 1/& 
times the convolution of the Fourier 
nversions of Equations 10.270 and 10.271. Convolution is usually a messy process, 
wt not in this case because the inversion of 10.270 is just a &function 
(10.272) 

394 
DIPFERENTlAL EQUATIONS 
and 8-functions convolve easily. The second function, Equation 10.271, is a Gaussian 
and it inverts to another Gaussian: 
The Green’s function becomes a convolution over x: 
(10.273) 
(10.274) 
Quation 10.274 describes how an initial temperature distribution T(x,O) = 
S(x - 5) evolves in time and space. A graph of this function is shown in Fig- 
ure 10.28. Notice that in this case g(x16,t) has the form g(x - [,t), and thus the 
Green’s function is translationally invariant. As the initial 8-function shifts in space, 
the response shifts correspondingly. This happens because the differential equation 
has constant coefficients and the homogeneous boundary conditions are located at 
infinity. 
Figure 10.28 Green’s Function for the Diffusion Equation 

BOUNDARY CONDITIONS 
395 
The general response for an arbitrary initial temperature distribution To(x) is then 
T(x,t) = - 
d,$ To(,$) 
e - ( x - & 2 / ( 4 d I )  
(10.275) 
Notice the integration of Equation 10.275 is over only the x variable. The t variable 
just hangs around as a simple time dependence of the Green’s function. In the next 
example, both time and space are treated as Green’s function variables, and the 
response will involve an integral over both of these quantities. 
Ja 
Srn 
--m 
Example 10.12 We now consider the same one-dimensional, conducting rod as we 
used in the previous example, but this time with an external driving term. Imagine a 
special heating element, running through the center of the rod, which can add heat in 
an arbitrary way that depends both on x and t .  The heat source s(x, t) turns Equation 
10.256 into a nonhomogeneous differential equation: 
(10.276) 
The signs have been adjusted in this equation so that a positive source term produces 
a positive change in temperature. The solution still must obey the homogeneous 
boundary conditions, T ( x , t )  -+ 0 as x + fm. In addition, we will also assume 
that the rod obeys causality with respect to the time variable. That is, there can be 
no response in the rod before the heat source is applied. The source term s(x, t) is 
assumed to be zero for t < 0, so that T(x, t) = 0 for t < 0. 
Because Equation 10.276 is a linear equation with homogeneous boundary con- 
ditions, it must obey the superposition principle. Thus, if a drive of s(x,t) = 
S(x - ,$)S(t - 7) results in T(x, t) = g(xI,$, tIT), then a drive of 
n
m
 
causes a temperature response of 
(10.278) 
n
m
 
This reasoning can be extended for continuous quantities by converting the summa- 
tions into integrals. So if the drive is now 
s(x, t )  = 1: 
d5 1: 
d7s(,$, 7)Wx - OW - 4, 
(10.279) 
the response is 
T(x, t> = 1: 
d5 l:dTs(,$, 7)g(xI,$, tb). 
(10.280) 

3% 
DIFFERENTIAL EQUATIONS 
The Green’s function for this problem is the response of the rod to a &function 
impulse at position x = 5 and time t = 7. It obeys the partial differential equation: 
(10.281) 
The homogeneous boundary conditions T(x, 1) must satisfy are also applied to the 
Green’s function, so g(x15,rlT) --$ 0 as x + t w .  The causality condition requires 
that g(x15,?1T) = 0 for t < T. To solve Equation 10.281, we will take a Fourier 
transform in space, and a Laplace transform in time. Taking these transforms of the 
Green’s function in two steps gives 
G&$.tIT) 
= - hg(xlt, tlr)e-ib 
(10.282) 
i&L(klSr&) = 1 dtG&15,t1T)e-”. 
(10.283) 
The Fourier-Laplace transform of Equation 10.281, the original differential equation, 
gives the algebraic relation 
m 
(10.284) 
This allows us to solve for the Fourier-Laplace transform of the Green’s function 
(10.285) 
Now all that remains to obtain the Green’s function is the double inversion of 
Equation 10.285. In principle, the inversions could be done in either order. In this 
case, it is much easier to do the Laplace inversion first. 
The Laplace inversion can be evaluated from 
(10.286) 
There is a single pole at g = - d k 2 .  Because the variable k is always on a Fourier 
contour, it is pure real. The diffusion coefficient D is also pure real. Therefore, this 
pole is always on the negative, real axis in the complex g-plane, as shown in Figure 
10.29. The Laplace contour is always to the right of this pole. The integral in Equation 
10.286 can be performed using closure, closing to the right for t < 7 and to the left 
fort > T, with the result 
(10.287) 
Equation 10.287 can be written in more compact form using the Heaviside step 
function, 

IOUNDARY CONDlTIONS 
imag 
397 
db L 
real 
___c__ 
- DZk2 
?gum 10.29 Laplace Inversion for the Nonhomogeneous Diffusion Equation Green’s Func- 
ion 
0 
t < 7  
1 
f > T ’  
H(t - T )  = 
1s 
(10.288) 
(10.289) 
The space-time Green’s function is now obtained by applying a Fourier inversion to 
Quation 10.289: 
(10.290) 
h e  way to solve this integral is by completing the square of the integrand’s exponent. 
Instead, we will use the convolution trick again. Notice the function in Equation 
10.289 can be written as the product of two functions: 
(10.291) 
Zonsequently, the Green’s function can be written as 1/& 
times the convolution 
x e r  x of their inverse transforms: 

398 
DIFFERENTIAL EQUATIONS 
i g (  
Figure 10.30 Green's Function for the Diffusion Equation with a Distributed Heat Source 
Because of the 6-function, this convolution easily evaluates to 
(10.293) 
A plot of this Green's function is shown in Figure 10.30. 
There are some interesting things to notice about this Green's function. First, the 
tlr dependence has the form t - T and the XI( dependence has the form x - 5. These 
results make sense both mathematically and physically. Mathematically, this occurs 
because the problem has constant coefficients and homogeneous boundary conditions 
at x = 30. 
Physically, it is clear from symmetry, the problem must be translationally 
invariant in both time and space. Also notice there is a symmetry with respect to 
the x and 6 variables, with g(x15, t l ~ )  
= g(fln, tIT), while there is no such symmetry 
for the t and T variables, g(xl(, t17) f ?g(xl(, dr). This symmetry property will be 
explored in one of the exercises at the end of this chapter. 
~~~ 
Example 10.13 As a final example, the Green's function for a driven wave equation 
will be developed. The one-dimensional driven wave equation for propagation in a 

BOUNDARY CONDITIONS 
399 
uniform medium can be written as 
1 d2U(X,t) 
82u(x,t) 
c; 
dt2 
8x2 
= s(x, t). 
(10.294) 
The constant c, is the velocity of propagation of the wave. This is a nonhomogeneous 
equation, with the response u(x, t) generated by the distributed source term s(x, t). 
This problem can be treated with a spatial Fourier transform and a temporal 
Laplace transform, exactly like the diffusion equation of the previous example. There 
is no problem with this approach. We will demonstrate a different approach, however, 
because it is one commonly used in physics calculations. To avoid the tedious Laplace 
transform, we will assume that the time behavior of both the driving term and the 
response are in the sinusoidal steady state, so all the time dependence occurs with a 
constant amplitude at a fixed frequency a,. In other words, we assume that 
s(x,t) = Real [S(x)eimo'] 
, 
(10.295) 
and the response has a similar time dependence 
u(x,t) = Real [U(x)eimo'] . 
(10.296) 
As you will see, this is very much like taking a Fourier transform in time. Unfortu- 
nately, you will also find out it presents a few problems! 
In this sinusoidal steady state, the differential equation becomes 
0,' 
d 2 U 4  
- - 
U(x) - - 
= $(x). 
c; - 
dx2 
(10.297) 
Notice the time dependence completely cancels out, and we have a linear, nonhomo- 
geneous equation for U(x). Also, we now are dealing with only a single independent 
variable, so the partial derivative has been replaced by a regular derivative. The 
solution for _U(x) can be formulated in terms of a Green's function, 
where g(x16) satisfies the differential equation 
We can take the Fourier transform of both sides of this equation to give 
(10.298) 
(10.299) 
(10.300) 

400 
DIFFERENTIAL EQUATIONS 
~ 
IFbg= 
_real 
Figure 1031 Fourier Inversion for the Driven Wave Equation 
so that 
(10.301) 
The Green’s function is then obtained by the Fourier inversion 
And now you should see the problem. The Fourier contour must remain on the 
real k-axis, but if it does, it runs directly into the two poles of the integrand! This 
situation is shown in Figure 10.31. Why did this problem arise, and how do we 
evaluate the integration near the poles? The answer to the first question is simple. 
We assumed the response was sinusoidal steady state over time and then carelessly 
took the Fourier transform over space. This spatial Fourier transform does not exist 
because the response to a sinusoidal steady-state source located at x = 5 has a finite 
amplitude for all x from minus to plus infinity. We can’t take the Fourier transform 
of such a function. There are two ways to fix this problem. One approach is to 
“hand-wave” your way to the correct answer using physical arguments. The other is 
to modify the problem, so we actually can take the spatial Fourier transform. Both 
will give the same answer. 
The hand-waving approach begins with the observation that what is being de- 
scribed by Equation 10.302 are the waves propagating to the left and to the right 
from a sinusoidal steady-state source located at x = 5. We will make an attempt 
to connect the segments of the Fourier contour, shown in Figure 10.31, around the 
two poles. Then we will check to see if the result makes physical sense, given the 
physical situation. So let’s say the connections are made by small semicircles going 
above both poles, as shown in Figure 10.32. This contour can be closed in the upper 
half for x > 5 and in the lower half for x < 5 with the result that 
(10.303) 

BOUNDARY CONDITIONS 
401 
X 
-No 
Response- 
Figure 10.32 First Guess for the Fourier Contour 
The first term comes from the pole at k = wo/co, and the second term comes from 
the pole at k = - wo/co. 
Remember the Green’s function is supposed to be the response to a &function 
source at x = 5. If the sinusoidal steady-state time dependence is put back in the 
problem, then this solution would say that a source term of 
s(x, t )  = Real[S(x - 5) eioofl 
(10.304) 
would give, according to Equation 10.303, a response of 
where k, = w,/co is a positive constant. There is no response for x > 5, and for 
x < 5 there are two wave responses, one from the k = wo/co pole traveling to the left 
and one from the k = -wo/co pole traveling to the right, as shown in Figure 10.33. 
The only part of this solution that makes sense is the wave from the k = wo/co pole, 
traveling to the left, away from the source. The other wave in this region, x < 5, is 
traveling toward the source, which makes no physical sense. It also makes no sense 
that there is no wave traveling to the right, away from the source in the region x > 5. 
It is obvious, however, that all these shortcomings vanish if the “Fourier” contour 
is modified, as shown in Figure 10.34. Quotation marks are placed around Fourier 
because, strictly speaking, a Fourier contour exists only on the real axis. 
Figure 10.33 Response Using First-Guess Contour 

402 
- 
k-Plane 
L - 
-
w
 
- OJCO 
DIFFERENTIAL EQUATIONS 
x > s  
imag 
real 
- - 
- 
OO’CO 
3 
x < 5  
We can reach the same conclusion about how to modLfy the “Fourier” contour, a 
little more rigorously, if we introduce a damping krm into the original differential 
equation. If the wave decays as it propagates, we will be able to legitimately take the 
Fourier transform over x. A simple modification of Equation 10.294 does the trick 
(10.306) 
1 d 2 U ( X , t )  
d2U(X,t) 
du(x t )  
c; 
at2 
dX2 
dt 
+ a
2
 
= s(x,t). 
Now, if a! is positive, the waves are attenuated as they move away from the source. The 
calculation proceeds as before, except now the damping term modifies the positions 
of the &plane poles of Equation 10.302 and, for small positive a, places them as 
shown in Figure 10.35. There are no poles along the inversion contour, so everything 
evaluates easily. Then, finally, we can take the limit as a + 0 to see that the contour 
of Figure 10.34 is correct. In this limit, the poles in Figure 10.35 become arbitrarily 
close to the real &axis, but the Fourier contour stays below the pole on the right and 
above the pole on the left. This is exactly the result of the hand-waving discussion 
that lead to Figure 10.34. 
So if we proceed with the inversion of Equation 10.302, using the contour of 
Figure 10.34 for a! = 0, we obtain 
(1 0.307) 
acJ2 
1 .  
imag 
k-Plane 
Figure 1035 Fourier Contour and Complex Plane Picture with Small Wave Damping 

BOUNDARY CONDITIONS 
403 
1 
Source 
f-- '
t
 - 
Figure 10.36 Undamped Waves Moving Away from Source 
When coupled with the sinusoidal time dependence, this solution behaves as shown 
in Figure 10.36. This solution describes undamped waves moving in both directions, 
away from the &function source located at x = 5. 
For an arbitrary source S(x), the response is 
(10.308) 
m e  response to the original problem is obtained by coupling _U(x) with the sinusoidal 
steady-state time dependence and taking the real part: 
u(x,t) = Real _U(x)eiWo' . 
(10.309) 
The Green's function solution for this problem, Equation 10.308, is easier to 
[
I
 
understand if it is assumed that S(x) is pure real. Then 
s(n,t) = Real S(x)eiwo' 
1
1
 
= S(X) cos(o,t), 
(10.310) 
and the response becomes 
(10.311) 
From this expression, it can be seen that the response is a sum of waves propagating 
away from the distributed source, toward the observation point x. 
1
.
 
+ Lrn 
4 
s t o z  
[coot + ko(n - 5)]. 

404 
DIFFERENTIAL EQUAlTONS 
EXERCISES FOR CHAPTER 10 
1. Determine whether the following differential equations are homogeneous or 
nonhomogeneous, and linear or nonlinear: 
2. The rate of evaporation for a constant-density, spherical drop is proportional 
to its surface area. Assume this evaporation is the only way the drop can lose 
mass and detennine a differential equation that describes the time dependence 
of the radius of the drop. Solve this equation for the radius as a function of time, 
assuming that at t = 0 the radius is r,. 
3. Using separation of variables, solve the differential equation 
dyo 
2
2
 
= x y ,  
dx 
with the boundary condition y(0) = 5. What happens when you try to impose 
the boundary condition that y(0) = O? 
4. Consider the differential equation 
dY(X) - - y 
dx 
X ’  
with the boundary condition y (  1 )  = 1. 
(a) Solve this equation using separation of variables. 
(b) This equation can also be solved by converting it into an exact differential. 
Using the notation of Equations 10.21-10.24, identify the functions R(x, y ) ,  
S(x, y ) ,  and 4 ( x ,  y). Using the boundary condition, find a solution #(x, y )  = 
C, that is equivalent to your answer in part (a). 
5. Consider the differential equation 
dyo Y G )  
+ - 
= cos(x2). 
dx 
X 
(a) Determine an integrating factor for this equation. 
(b) Solve for y(x). 

EXERCISES 
405 
6. A resistor R and capacitor C are driven by a voltage source v(t), as shown in the 
circuit below: 
The charge on the capacitor is given by the function q(t). For t < 0, the voltage 
source is zero and the charge on the capacitor is zero. The differential equation 
that governs the charge is 
1 
R- 
+ -q(t) = v(t). 
dt 
C 
(a) Find the integrating factor for this differential equation. 
(b) Using the integrating factor, express q(t) in terms of v(t). 
(c) Solve the equation of part (b) above if v(t) = &t). 
(d) Solve the equation of part (b) above if v(t) is a unit step applied at t = 0. 
7. Find the integrating factor and the general solution, with no specific boundary 
condition, for the differential equation 
8. Consider the first-order differential equation 
(a) What is the integrating factor for this differential equation? 
(b) Find y(x) subject to the boundary condition y(0) = 0. 
9. Consider Bessel’s equation of order zero: 
d2Y(X) 
x - 
+ 
+ x y ( x )  = 0. 
dx2 
dx 

406 
DIFFERENTIAL EQUATIONS 
10. 
11. 
12. 
13. 
14. 
15. 
Show that the Wronskian for this homogeneous differential equation has the 
form 
co 
W(x) = -, 
X 
where C, is a constant. 
What is the Wronskian associated with the differential equation 
What is the Wronskian associated with the differential equation 
Given that one solution of 
is y(x) = xm, find the second solution. Using your result above, find a particular 
solution to the nonhomogeneous equation 
d2YW 
dy(x) 
1 
x2 - 
+ x - 
- y(x) = - 
d x 2  
dx 
1 - x ‘  
Use the method of Frobenius to find a series solution to 
The Legendre equation has the form 
( l - x ) - - 2 x -  
d2y(x) 
dy(x) + n(n + l)y(x) = 0. 
dx2 
dx 
Obtain the indicial equation used in the method of Frobenius to find a series 
representation for y(x). 
Show that the differential equation 
has Frobenius polynomial solutions, as long as A is a nonnegative integer. De- 
termine the solution for A = 4 and with the boundary condition y(0) = 1. 

EXERCISES 
407 
16. The differential equation 
possesses two independent solutions that are called the Auy functions. Use the 
method of Frobenius to evaluate these functions. What is the Wronskian of this 
differential equation? 
17. Using the method of Frobenius solve the differential equation 
18. Use the method of Frobenius to obtain two independent solutions to the equation 
Show that your solutions converge and prove they are independent. 
19. Use the method of Frobenius to generate a series solution for the differential 
equation 
d3YW 
dY(X) 
-x- 
=o. 
dx3 
dx 
Determine the generating function for the coefficients of the series and write out 
the first few nonzero terms. 
20. A tank car with a hole in the bottom is dumping its load so its total mass as a 
function of time is given by m(t) = m,( 1 - sot), where a, and m, are constants. 
The car sits on a frictionless track and is attached to a spring with a spring 
constant KO. 
The car is released at t = 0 from x = 0 with an initial velocity v,. Assuming the 
spring is at equilibrium when the car is at x = 0, 
(a) what is the second-order, homogeneous differential equation that describes 
the motion of the car? 

408 
DIFFERENTIAL EQUATIONS 
(b) is this a linear or nonlinear differential equation? 
(c) use the method of Frobenius to solve for x(t) over the interval 0 < r < 1 /a,. 
(d) check the convergence of your solution. 
(e) what happens at r = l/a,? 
21. The quantum mechanical interaction between two nuclear particles can be de- 
scribed by the one-dimensional interaction potential 
ePQ* 
V(x) = 
X 
Develop a series solution for the wave function q(x) that describes this system. 
"(x) satisfies the Schrodinger equation, 
-~ 
ti2 d2"(x) + [E - V(x)] "(x) = 0, 
2m 
dx2 
where E is a constant representing the energy of the particle and m is its mass. 
Explicitly write out the first three terms of the series you obtain. 
22. Consider an undamped, undriven, frictionless mass spring system described by 
the homogeneous differential equation 
m- d2y(t) + Ky(t) = 0, 
dt2 
where m is the mass and K is the spring constant. For this problem we will allow 
the spring to get stiffer as it is compressed. To accomplish this, let the spring 
constant K become dependent on the displacement such that 
K = Koy2, 
where KO is a positive constant. 
(a) At t = 0, the mass is released from x = x, with no initial velocity. Using 
the differential equation describing the motion, obtain an expression relating 
the velocity of the mass to its position. What is the acceleration of the mass 
whenx = x,? 
(b) Make a graph of the velocity vs. position to show how the mass oscillates. 
23. A mass spring oscillator is set up with a magic spring, with a negative spring 
constant. It is frictionless and driven by an external force f(t) that is a function 
of time. The force from the spring on the mass is given by K,x(t) rather than 
- Kox(t). The position of the mass is x(t) and the initial conditions are that x = 0 
and dx/dt = 0 for all t < 0. 
(a) Write down the differential equation of motion for x(t), the position of the 
mass. 

EXERCISES 
409 
(b) Let the external force f (t) be given by 
What is the Laplace transform of x(t)? 
(c) What is x(t) for large t? 
24. A driven, linear mass spring oscillator is described by the differential equation 
where x ( t )  is the position of the mass as a function of time, and f ( t )  is the driving 
force 
For t < 0 the mass is at rest. 
(a) If x(t) has a Laplace transform Xu, 
what is the Laplace transform of 
(b) What is x(t) for all t > O? 
d2x(t)/dt2? 
25. A linear, frictionless mass-spring oscillator with mass rn and spring constant K 
is at rest for t < 0 and is driven by an external force f (t). 
(a) Iff (t) = &(t), what initial conditions are establishedjust after this &function 
(b) If f ( t )  = d6(t)/dt, what are these initial conditions? 
(c) Finally, find the position and velocity of the mass for all t 2 0 if 
force is applied? 
26. Solve the nonlinear differential equation 
given the boundary conditions y = 0 and dy/dt = 0 at x = x,. 
27. Find the unique solution for y ( x )  to the nonlinear equation 
with the boundary conditions y(0) = 0 and dy/dxl0 = 0. 

410 
DIFFERENTIAL EQUATIONS 
28. Use the method of quadrature to solve the equation 
d2y(x> - [y(x)]3 = 0, 
dx2 
if y(0) = 1 and dy/dxlo = 1/&. 
29. Use the method of quadrature to solve the nonlinear equation 
- cos y sin y ,  
d2YW - 
dx2 
given the boundary conditions y(0) = 0 and dy/dxIo = 1. 
30. Use the method of quadrature to solve the differential equation 
d2y(x) - siny 
dx2 
c0s3y 
for y(x). Show, by substituting it back into the equation, that your solution works. 
Choose the constants of integration to give the simplest possible answer. 
31. This problem involves a Child-Langmuir solution for a two-component, anode- 
cathode gap. The geometry is described in the figure below. A large source 
of electrons exists at the cathode, and we assume the cathode temperature is 
sufficient for the electric field to be zero at t = 0. The same is true at the anode, 
except the particles there are positively charged ions. Let the electron have a 
mass of m- and a charge -qo, while the ion has a mass m+ and charge +qo. 
Electron 
Ion 
cloud 
cloud 
Look for a steady-state solution such that d/dt of all quantities are zero. Both 
electrons of charge density p- (x) and ions of charge density p+ (x) will be present 

EXERCISES 
411 
in the space between the electrodes, so the relevant Maxwell equation becomes 
v . €,E(X) = p+(x) + p-(x). 
Use the method of quadrature to obtain a relationship between the current I, and 
the voltage V,. 
(1) The example worked out in this chapter arrived at the equation: 
This problem is quite involved. Here are two hints to get you started: 
The solution to this exercise works out best if an indefinite integral, with its 
constants of integration, is used to impose the boundary conditions, insteadof 
the definite integral used in the example worked out in this chapter. Proceed 
as follows: 
The two constants can be combined 
and the single constant Co evaluated by the boundary conditions, 
b(0) = 0, 
= 0. 
x=o 
For the example worked out in this chapter, C, = 0. For this exercise, this 
constant will not be zero. 
In the chapter, we ended up with an integral in the form 
This integral was easy to perform. This problem does not work out as 
nicely, and the integral must be done numerically. For numerical integrations, 
variables are usually normalized so the results will be more general and can 
be easily scaled. For example, to normalize the above integral, we let 

412 
DIFFERENTIAL EQUATIONS 
so that the normalized form becomes 
The quantity in the brackets is referred to as a normalized integral because 
the variable q is dimensionless and the limits are from zero to one. The 
normalized integral for this exercise should come out to be 
Done numerically this integral has a value of about (4/3)d1.86. 
32. For the string problem worked out in this chapter, we found that the Green’s 
function solution took the form 
Show that the derivative of y(x) can be expressed as 
33. For the string problem worked out in this chapter, the displacement of the string 
was governed by the differential equation 
d2YW 
T - 
= f ( x ) .  
dx2 
Assume the homogeneous boundary conditions y(0) = y(L) = 0, and use the 
Green’s function to find the displacement of the string if 
f (x) = F, [(x - L/2)’ - L2/4] 
0 < x < L. 
34. Derive the Green’s function for the string problem again, using the boundary 
conditions 
(a) Y(0) = Y1; Y(L) = Y2. 
(b) y(0) = 0; dy/dx(r. = 0. 
(c) y(0) = 0; dy/dxlr. = A,. 
Show, with drawings, how the string can be fastened at its end points to impose 
the three different boundary conditions described above. Why do the boundary 
conditions 
d y / d ~ ( o  = 0 dy/dx(r. = 0 
cause problems with the Green’s function derivation? 

EXERCISES 
413 
35. Poisson‘s equation can be viewed as a multidimensional form of the string 
problem. Two spatial derivatives of the dependent variable generate a function 
proportional to the drive 
PC(Q 
V2@(F) = - -. 
Eo 
In this expression, @(F) is the electrostatic potential generated by a charge dis- 
tribution p@). 
Express a point charge in two dimensions using polar coordinates p and 4. 
Find the Green’s function for the two-dimensional Poisson’s equation using 
the polar coordinates. What boundary condition does this Green’s function 
satisfy? 
Using your Green’s function, find a solution for @(@ for an arbitrary pc(f). 
Repeat the above steps for a three-dimensional solution using spherical 
coordinates. 
Using your answer to part (d) above, find the solution for the potential of 
the dipole charge distribution shown below, where +q are point charges 
separated by a distance d. 
36. If time dependence is included in the description of an infinitely long, stretched 
string, the differential equation for its displacement becomes a wave equation 
= 0. 
d2y(x,t) -- 
1 a2y(x,t) 
3x2 
~2 
at2 
-- 
The string is stretched in the x-direction, and this equation describes perpen- 
dicular displacements in the y-direction. If there are no driving forces along the 
string, the differential equation is homogeneous, and excitations are established 
by the initial conditions at t = 0, which then propagate for t > 0. 
(a) Assume that the initial conditions are y(x, t = 0) = yo(x) and dy/dtl,=o = 0. 
Find the Green’s function that allows the displacement of the string to be 
written as 

414 
DIFFERENTIAL EQUATIONS 
(b) Plot your solution for the Green’s function vs. x for three values oft. 
(c) Find y(x. t) if 
(d) Now find the Green’s function for the initial conditions y(x, t = 0) = 0 and 
tly/atl,=o = vo(n). Plot this Green’s function vs. x for three different values 
oft. 
37. A linear, driven, mass-spring oscillator is acted upon by a standard - Kx spring 
force, an external force f (c), and a magic antifriction force a &/dt where a is 
small. Assume the mass is at rest for t < 0 and its position is given by x(f). 
(a) Write down the differential equation of motion for this system. 
(b) Take f ( t )  to be zero for t < 0 and equal to Fo sin (wet) for t > 0. Use 
(c) Sketch the Green’s function g(t - T )  vs. T. 
(d) If f ( t )  = t for t > 0, what is x(t)? 
Laplace transform techniques to obtain a solution for x(t) as t 4 m. 
38. Consider the differential equation 
+ r(t) = d(t), 
d3r(t) 
dt3 
where r(t) is the response to a drive d(t). The drive and response are assumed to 
be zero fort < 0. 
(a) What is the Green’s function for this differential equation?Can it be obtained 
with Fourier transform techniques? Can it be obtained with Laplace trans- 
form techniques? Discuss the initial conditions on r(t) and its derivatives. 
(b) Sketch the Green’s function g(t)T) vs. f. 
(c) If d(t) = t for t > 0, what is the response r(t)? 
39. Consider the differential equation 
where r(t) is the response to a source s(t), and the independent variable t rep- 
resents time. Since this is a fourth-order differential equation, four boundary 
conditions are necessary to obtain a unique solution. For all parts of this problem 
assume that r(t) and s(c) are zero fort C 0. 
(a) What initial conditions are established if s(t) = a(?)? Assume these condi- 
(b) Obtain an expression for the Green’s function g ( h )  for this problem. 
(c) Make a plot of g(tlT = 0) vs t. 
tions are imposed just after the &function occurs. 

EXERCISES 
415 
40. 
41. 
42. 
43. 
Find the Green’s function for the differential equation 
d2yo + y(x) = f ( x )  
dx2 
over the interval 0 < x < 1, with the boundary conditions y(0) = y(1) = 0. 
Solve for y ( x )  if f ( x )  = sin (m). 
Consider the linear operator 
d2 
d x 2  
Lop = - 
+ 1 
and the differential equation 
L p Y ( 4  = f ( x ) .  
For the initial conditions set y(0) = 0 and dy/dxl,=, = 0. 
(a) What is the Green’s function g(x15) for this operator and what are its bound- 
(b) Obtain the Green’s function solution for y ( x )  as specifically as you can, 
(c) Now set f ( x )  = 6(x - x,) and, using your answer to part (b), determine 
The equation of motion for a damped harmonic oscillator, driven by a force f(t), 
takes the form 
ary conditions? 
while still leaving f ( x )  arbitrary. 
Y ( X > .  
d2x(r) + aw + Kx(t) = f ( t )  
dt 
m- 
dt2 
where the damping constant a is greater than zero, and K is the positive spring 
constant. Assume the drive and response are zero fort < 0. 
(a) Use Laplace transform techniques to obtain the Green’s function for this 
problem. 
(b) What is x ( t )  if 
F, 
O < t < T , ?  
f ( t )  = { 0 
otherwise 
Consider the following circuit, where is(t) is a current source, which is zero for 
t < 0. Assume that the response vr(t), which is the voltage across the capacitor, 
is also zero for t < 0. 

DIFFERENTIAL EQUATIONS 
Obtain a nonhomogeneous differential equation which relates the response 
v,(t) to the drive is(t). 
Working in the time domain, determine the Green’s function for this problem. 
Using your Green’s function form part (b), determine vr(t) if 
Now find the solution working in the frequency domain. Obtain the Laplace 
transform of the response 
in terms of the Laplace transform of the drive 
Z&J. 
Use this result to obtain v,(t) for the drive given in part (c), above. Show 
that you obtain the same result as given by the Green’s function solution. 
Show that the relationship between V-,O 
and L(sJ can be obtained by treating 
the complex impedance of the capacitor as 1 /(C$ and forming the parallel 
combination with the impedance of the resistor R. 
Using the complex impedance idea of part (e). what is the complex impedance 
of an inductor, L? What is the ratio vsJ/L(sJ for the following circuit? 
+ 
L 
44. Consider the circuit, where vs(t) is a source voltage, and the current i,(t) is looked 
at as the response. 
R 

EXERCISES 
417 
Obtain the nonhomogeneous differential equation that describes the response 
i,(t) in terms of the source vs(t). 
Working in the time domain, determine the Green’s function for this differ- 
ential equation. Assume that the source and response are zero for t < 0. 
Using your Green’s function from part (b), determine i,(t) if 
Now find the solution working in the frequency domain. Obtain L,(S), the 
Laplace transform of i,(t), in terms of VJs), 
the Laplace transform of vs(t). 
Use the same function for vs(t) as was used in part (c). Invert the Laplace 
transform for i,(t) to obtain the same answer you got using the Green’s 
function approach of part (c). 
Show that by considering the impedance of the inductor to be Ls, the rela- 
tionship between I,@) and VJsJ 
can be obtained by inspection. Notice that 
this is just the Laplace transform of the impulse response. 
45. Using the Green’s function technique, solve the differential equation 
-- 
d2y(x) k,’ y(x) = f ( x )  
dx2 
over the interval 0 < x < Lo, subject to the boundary conditions y(0) = y(Lo) = 
0, using the following steps: 
(a) Show that the Green’s function for x < 5 is given by 
(b) Find the Green’s function for x > 6. 
(c) Find y(x) in terms of f ( x )  using the Green’s function integral. 
(d) Find y ( x )  under the above conditions if 
f ( x )  = 56(x - 5 )  + 106(x - 10). 
(e) How does your answer to part (c) change if y(0) = 0 and y(Lo) = 5? 
46. Consider the differential equation 
over the interval 0 < x < 1, with s(x) a known function of x. The solution y ( x )  
is to satisfy the following boundary conditions 
y(0) + finite ; y(1) = 0. 

DIFFERENTIAL EQUATIONS 
418 
(a) What differential equation must be satisfied by &XI(), 
the Green’s function 
(b) What is the general form of &I{) for x < { and x > t? 
(c) What conditions must g(x& and its derivative with respect to x satisfy at 
(d) Using your results from part (c) and the boundary conditions, determine 
(e) Sketch g(x1.E) vs. x for a fixed 5, over the range 0 < x < 1. 
for this problem? 
x = 5? 
&I&) in the range 0 < x < 1. 
47. Find the Green’s function for the differential equation 
subject to the boundary conditions y(0) = y( 1) = 0. The Green’s function must 
satisfy 
g(x/{) = 6(x - 5). 
dx2 
dx 
It may be easier, however, to solve for the Green’s function from 
48. This problem involves the diffusion of magnetic field into an electrically conduct- 
ing material. The material is characterized by a constant electrical conductivity 
mo and magnetic susceptibility po and exists in the half-plane x > 0. Free space 
fills the region x < 0. 
The electric E and the magnetic B fields in the conducting material are governed 
by Maxwell’s equations in the diffusion limit 

EXERCISES 
419 
Treat the material as if it were one-dimensional (i.e., all quantities have a/& = 0 
and d/dy = 0) and use these equations to obtain the diffusion equation 
a2B(x,t) - 
dB(x, t )  
=,Po 7. 
_ _ _ _  
dX2 
Now consider the situation where the magnetic field is controlled in the free 
space region so that B(x, t) = B,(t)C, for x < 0. A Green’s function solution for 
B(x, t )  in the region x 2 0 is sought, so that the solution for the magnetic field 
can be placed in the following form: 
dTB,(T)g(x, t)T)&. 
(a) What differential equation and what boundary conditions at x = 0 must 
(b) Determine G(x, WIT), the Fourier transform of g(x, t 1 ~ ) .  
(c) Describe the restrictions that must be placed on the function B,(t) in order 
g(n, t ) ~ )  
satisfy? 
for the above solution to be valid. 
49. Consider the one-dimensional wave equation 
- 
- Z] r(x, t )  = s(x, t). 
[;:2 
c: at2 
where the source term s(x, t) and the response r(x, t) are both zero for t < 0. 
(a) Obtain the Green’s function g(x15, t ) T )  for this problem that allows the re- 
sponse to be written as 
r(x, t )  = /: d5 
dTs(8, T>g(x15, tb). 
(b) Plot g(x16, r)T) vs. x for three different values oft, one value less than T and 
Note that if f ( x )  is a unit step from -a < x < a, the Fourier transform of f ( x )  
is 
two values greater than T. 

DIFFERENTIAL EQUATIONS 
420 
50. In the chapter, we derived the Green’s function for the wave equation in Equation 
10.307. 
(a) How is g(x15) modified if we have the additional boundary condition 
(b) Make a plot of this Real[g(x(&eioJ J for several different times-as 
many as 
(c) What physical situation does this new boundary condition represent? 
g(x = 015) = O? 
are needed to show its character. 
51. In free space, electromagnetic waves propagate according to Maxwell’s equations 
d€&(;Ci, t )  
v x E(F,t) = + 
at 
’ 
which can be combined to give the wave equation 
If, instead of free space, the wave is propagating in a dielectric medium that is 
spatially dependent, i.e., 
€0 --f €0 + em, 
(a) Show that the wave equation becomes 
(b) Consider a wave propagating at a pure real frequency w,, so that E(F, t )  + 
- 
E(F) e“’’.’. This is like taking a Fourier transform in time. Show that the wave 
equation becomes 
- 
- V 2- E(r) 
- - w ~ p o e o & ~ )  
= + w,~~~E(F)&F). 
Now let e(F) = 6(x) to model a sheet of infinite dielectric in the x = 0 plane. 
Assume there is no variation of the electric field in the y-direction, and that the 
spatial dependence of the field can be expressed as 
- 
4 
e 
eY* 
- 
E(F) = E eikIx 
ik,z A 
This is like taking Fourier transforms in the two spatial directions. 
(c) Find k, and k, for x > 0 and x < 0. 
(d) Can this solution for the electric field be used as a Green’s function for 
finding the solution for an arbitrary E(x)? 

EXERCISES 
421 
52. 
53. 
54. 
This problem looks at the diffusion of heat along an infinite rod with the temper- 
ature at x = 0 controlled for all time, 
t > O  
t < O ’  
T(x = 0,t) = {P 
The standard diffusion equation applies 
d2T(x, t )  
dT(x, t )  
D2- 
- 
= 0, 
dX2 
dt 
which, with the above boundary condition and the assumption that T(x, t )  ----f 0 
as 1x1 -+ m, uniquely determines T(x,r) for all x and r > 0. The solution for 
T(x, r )  can be obtained from a Green’s function integral 
(a) Identify the differential equation and boundary conditions that g(x, t 1 ~ )  
must 
(b) Determine g(x, t 1 ~ ) .  The following Laplace transform will probably be of 
satisfy. 
help: 
k 
_- 
e-k2/(4t) C-f e - k f i  
2J.ITt3 
(k > 0). 
A differential equation is said to be in Sturm-Liouville form if it can be written 
as 
where a(x), b(x), and s(x) are known functions of the independent variable. 
All linear, second-order differential equations can be placed in this form. This 
particular form has many advantages, as will be shown in the next chapter. The 
differential equation 
x2 d 2 y ( x )  
___ + (x2 cos x + 2x)- dy(x) + x y ( x )  = d ( x )  
dx2 
dx 
can be placed in Sturm-Liouville form by multiplying both sides of this equation 
by a function f(.x). 
(a) What f ( x )  will accomplish this? 
(b) What is the final Sturm-Liouville form for this equation? 
If a particular differential equation is not in Sturm-Liouville form (see Exercise 
10.53), the Green’s function for the equation in the non-Stunn-Liouville form 
will be different from the Green’s function for the equation in Sturm-Liouville 

422 
DIFFERENTIAL EQUATIONS 
form. Consider the differential equation: 
with the boundary conditions y(0) = y(1) = 0. 
(a) What is the Green’s function for this problem as described by the differential 
(b) Place the differential equation in Sturm-Liouville form. 
(c) What is the Green’s function for the differential equation in Sturm-Liouville 
form? 
(d) Formulate the response of the system to a drive d(t) using the Green’s 
functions obtained for parts (a) and (c) above and show that these responses 
are identical. 
55. In this chapter, we have seen cases where the Green’s function was symmetric 
equation given above? 
with respect to x and 5. That is to say, 
In this problem you are being asked to show the general conditions that are 
necessary for this symmetry to hold. Prove that if the differential equation is 
in Sturm-Liouville form (see Exercise 10.53) and the boundary conditions are 
homogeneous, the Green’s function will always have this symmetry. 
To show that this is the case, begin by writing the differential equation that 
satisfies for two different values of 5: 51 and 52. Multiply the first by 
g(&) 
and the second by g(x151) and subtract to obtain: 
Show that this expression becomes 
Now if the solution is to be valid over the range x1 < x < x2, and the boundary 
conditions are such that the function or its derivative must go to zero at x = xl 
and x = x2, show that the above equation can be integrated to give 

EXERCISES 
423 
56. Using the steps outlined in the previous problem, determine the symmetry prop- 
erties of the Green’s function that satisfies the differential equation 
x3 - 
d2y(x) + x2 dyo - x y ( x )  = d(x), 
dx2 
dx 
with the boundary conditions y(0) = y(1) = 0. Note that this differential 
equation is not in Sturm-Liouville form. 

11 
SOLUTIONS TO 
LAPLACE'S EQUATION 
It would be impossible to discuss all the different types of differential equations 
encountered in physics and engineering problems. In this chapter, we will concentrate 
on just one, Laplace's equation, because it occurs in so many types of practical 
problems. In vector notation, valid in any coordinate system, Laplace's equation is 
V 2 @  = 0. 
(11.1) 
We will investigate methods of its solution in the three most common coordinate 
systems: Cartesian, cylindrical, and spherical. The solution methods we present here 
will also be applicable to many other differential equations. 
11.1 CARTESIAN SOLUTIONS 
In Cartesian coordinates, Laplace's equation becomes 
(1 1.2) 
A standard approach to solving this equation is to use the method of separation of 
variables. In this method, we assume @(x, y ,  z) can be written as the product of three 
functions, each of which involves only one of the independent variables. That is, 
@(n, y ,  z )  = X(X)Y (Y)Z(Z). 
(11.3) 
It is important to note that not all the possible solutions of Laplace's equation in 
Cartesian coordinates have the special separated form of Equation 11.3. As it turns 
424 

CARTESIAN SOLUTIONS 
425 
out, however, all solutions can be written as a linear sum (of potentially infinite 
number of terms) of these separable solutions. You will see examples of this as the 
chapter progresses. Using this form for @(x, y ,  z), Laplace’s equation becomes 
(11.4) 
Each of the three terms on the LHS of Equation 11.4 is a function of only one 
of the independent variables. This means that, for example, the sum of the last two 
terms 
(1 1.5) 
cannot depend on x .  A quick look back at Equation 1 1.4 shows that this implies the 
first term, itself, also cannot be a function of x. It obviously is not a function of y or 
z either, so it must be equal to a constant: 
Similar arguments can be made to isolate the other terms: 
(11.6) 
(11.7) 
(11.8) 
The three quantities c,, cy, and cz are called separation constants. Notice that they 
are not completely independent of one another, because Laplace’s equation requires 
c, + cy + c, = 0. 
(11.9) 
Consequently, only two can be chosen arbitrarily. 
11.1.1 
Each of the three separated equations (Equations 11.6-1 1.8) is the same linear, 
second-order differential equation. The solution to Equation 1 1.6, for a particular, 
nonzero value of c,, is simply 
Solutions of the Separated Equations 
X ( x )  = x+e+@ + x-e-fi, 
c, # 0, 
(11.10) 
where X+ and X -  are arbitrary amplitudes, which are determined only when we apply 
the boundary conditions. When this general solution is combined with Y (y) and Z(z), 
these constants combine with the corresponding Yt and Zt constants to form other 
constants. For this reason, we will ignore these amplitude constants until the final 

426 
SOLUTIONS TO LAPLACE’S EQUATION 
form of @(x, y .  z) is constructed and initially just concentrate on the functional form 
of the separated solutions. Equation 1 1.10 will be written, using a shorthand notation, 
as 
X(x) -+ { e-&x 
e+fix 
c, # 0. 
(11.11) 
We now turn our attention to the solutions for X(x) with different separation 
constants. The constant c, may be negative and real, positive and real, or complex. 
Because the solution depends on the square mot of c,, it turns out the results of a 
complex c, are covered by a combination of solutions generated by real positive c, 
and real negative c,. Thus, we will limit our attention to real values of c,. There is 
also the special case of c, = 0 to consider. 
In the case with c, = 0, the differential equation becomes 
1 d2X(x) - 0, 
X(x) dx2 
(11.12) 
and the general solution has a special form 
X(x) = Ax + B, 
(11.13) 
where A and B are arbitrary amplitude constants. In our shorthand notation, this is 
written as 
X ( X >  + {: 
c, = 0. 
( 1 1.14) 
This is qualitatively very different than the form of Equation 11.11. Often these 
special linear solutions can be ignored because of some physical or mathematical 
reasoning, but not always, and one must be careful not to forget them. 
If c, is real and greater than zero, we can write fi 
= +a where a is real and 
positive. The solutions for X(x) take the form of growing or decaying exponentials 
(11.15) 
These solutions are shown in Figure 1 1.1. Often, it is convenient to rearrange things 
and use hyperbolic functions instead: 
(11.16) 
The two forms in Equations 1 1.15 and 1 1.16 are completely equivalent. Convenience 
dictates which set is the better choice. Some boundary conditions are more easily 
accommodated by the exponential solutions, and others by the hyperbolic functions. 
The sinh function has odd symmetry and is zero at x = 0. The cosh function has 
even symmetry, and is one at n = 0. These functions are shown in Figure 1 1.2. If a 

CARTESIAN SOLUTIONS 
427 
Figure 11.1 Exponential Solutions to Laplace’s Equation in Cartesian Geometry 
problem naturally has odd or even symmetry, the hyperbolic functions are usually a 
wise choice. 
= ik with 
k > 0. Therefore, for a negative separation constant, the solutions are oscillatory: 
If c, is a real negative number, its square root is imaginary. That is, 
(11.17) 
For solutions that must be pure real functions, this form is usually clumsy, because 
the amplitude constants need to be complex. Equation 11.17 can be rewritten using 
sin and cos functions: 
(11.18) 
Again, the choice between the two forms is one of convenience. 
In summary, the separated solutions to Laplace’s equation in Cartesian geometry 
have three possible forms. If the separation constant is zero, the solution has the 
linear form Ax + B, if the constant is negative, the solution oscillates, and finally, if 
the constant is positive, the solution is an exponential. Notice, because the separated 
differential equations for Y (y) and Z(z) are identical to the equation for X(x), their 
solutions follow a similar pattern. An interesting condition is imposed on these 
three solutions by Equation 11.9. If the separated solution in one direction has an 
Figure 11.2 The Hyperbolic Solutions to Laplace’s Equation in Cartesian Geometry 

4% 
SOLUTIONS TO LAPLACE’S EQUATION 
exponential form, at least one of the other solutions must have an oscillatory form 
and vice versa. We will discover that this is a general trait of the separated solutions 
in other coordinate systems as well. 
11.1.2 The General Solution 
The solution for +(x, y, x) is the product of the three separated solutions, as described 
by Equation 1 1.3. The amplitude and separation constants are determined by a set of 
boundary conditions. These boundary conditions may allow a particular separation 
constant to take on more than one value and, in many cases, an infinite set of 
discrete values. In such a situation, because Laplace’s equation is linear, the general 
solution will be composed of a hear sum over the acceptable values of the separation 
constants. 
Let the possible nonzero values of c, be indexed by m. That is, the mth allowed 
separation constant for x is c,. 
Likewise, let cyn be the nth nonzero separation 
constant for the y variable. Equation 11.9 implies that c, cannot be independently 
specified, because c, = -c, - cyn. Using the shorthand notation, we can write the 
general solution as a sum over the two indices n and m: 
+ The Special Linear Solutions. 
The sum on the RHS of this equation is a shorthand notation for writing all the 
combinations of solutions that have nonzero separation constants, each with an un- 
determined amplitude constant. For each value of m and n, there are eight possible 
combinations of the solution types. If you wanted to expand this out, it would look 
like this: 
cc 
m
n
 
(11.20) 
Many times the symmetry of the problem will eliminate some of the terms in the 
bracketed sum of Quation 11.20, and those particular amplitude constants will be 
zero. For this reason, it is convenient to use the shorthand notation of Equation 1 1.19 
and not address the amplitude constants until the symmetry arguments have placed 
the general solution in more tractable form. 

:ARTESIAN SOLUTIONS 
429 
The “special” solutions of Equation 1 1.19 are the ones that fall outside of normal 
rorm because one or more of the separation constants is zero. For example, if a 
iossible solution has c, = 0, but cy = c and cz = -c, there is a special set of linear 
jolutions 
When c, = cy = c, = 0, another set of special solutions is 
{: {; {: 
‘ 
(1 1.21) 
(1 1.22) 
The general solution will be unique, that is to say all the amplitude and separation 
:onstants will be determined, only if enough boundary conditions are specified. For 
Laplace’s equation this means that the solution will be unique in a region if the value 
3f @(x, y ,  z) or its normal derivative, or a combination of @(x, y, z) and its normal 
derivative is specified over the entire surface enclosing that region. These are referred 
to as the Neumann or Dirichlet boundary conditions, respectively. 
Example 11.1 To see how the boundary conditions are used to determine the con- 
stants of Equation 1 1.19, consider the two-dimensional problem depicted in Fig- 
ure 11.3. Two parallel plates exist in the planes y = +h and y = -h, with periodic 
square wave electric potentials applied as shown. The problem is two dimensional, 
because there are no variations in the z-direction. The potential along the x-direction 
repeats with a spatial period of 2d. The boundary conditions over the first period can 
be summarized as 
+1 
O < x < d  
-1 
d < ~ < 2 d  
@(x, +h) = 
@(x,+h) +1 
-1 
+1 
-1 
I 
+I 
-1 
+1 
-1 
(11.23) 
2d 
@(x,-h) 
-1 
+I 
-1 
+1 
1 
-1 
+I 
-1 
+1 
X 
Figure 11.3 
ditions 
Two-Dimensional Laplace Equation Problem with Square Wave Boundary Con- 

430 
SOLUTIONS TO LAPLACE’S EQUATION 
and 
(11.24) 
We want to find the solution for the potential between the plates, using the boundary 
conditions and the two-dimensional form of Laplace’s equation: 
(1 1.25) 
Based on our previous discussion, we will look for a two-dimensional solution with 
the form 
A quick look at the nature of the boundary conditions shows that the solution of X(x) 
must be oscillatory in order to satisfy the periodic nature of the problem. Therefore, 
it must use a negative, real separation constant: 
( 1 1.27) 
where we have put the constant in the form of a square to make the subsequent algebra 
simpler. The other solution must therefore obey 
(1 1.28) 
The possible solutions to muation 11.27 are constrained by two facts. First, 
because the boundary conditions of the problem have odd symmetry about x, X(x) 
must also have odd symmetry. Also, because the boundary conditions of the problem 
have a periodicity of 2d, each solution of X(x) must oscillate with a period of 2d/n 
where n is any positive integer. This ensures X(x + 2d) = X(x). All the possible 
solutions of X(x) can be written using n as an index, 
X(x) --f sin(k,,x), 
(11.29) 
where k,, = m / d .  Notice the k = 0 solution is not allowed, since the corresponding 
solution of X(x) = Ax + B would violate the combination of the symmetry and 
periodic requirements. This means we do not need to consider the special linear 
solutions we mentioned in the previous section. For the Y (y) solution, we will use 
the sinh function, because it matches the odd symmetry of the boundary conditions 
in the y-direction: 

CARTESIAN SOLUTIONS 
431 
The combination of these two solutions generates the general solution for @(x, y), 
m 
@(x, Y> = C An Sin(knX) sinh(kny), 
(11.31) 
n=1,2, ... 
where the amplitude constants A, are finally introduced. 
The A, coefficients are determined by requiring Equation 1 1.3 1 go to the applied 
potential on the boundaries. Since we have already imposed the odd symmetry in 
the y-direction, if the expression in Equation 11.31 goes to the proper potential at 
y = +h, it will automatically go to the correct value at y = -h as well. Thus to 
make everything work, the solution must satisfy 
a 
( 1 1.32) 
+1 O < x < d  
A,, sin (7) 
sinh (F) 
= { -1 
d < x < 2 d '  
n= 1,2, ... 
The LHS of Equation 11.32 is in the form of a Fourier sine series. A particular 
coefficient A, can be obtained by operating on both sides of this equation with 
6" 
dx sin ( y) 
Every term in the series drops out except the n = m term, giving the result: 
( 0  
m even 
modd . 
4 
Am = ( m?r sinh(m?rh/d) 
(1 1.33) 
( 1 1.34) 
The complete solution to the problem is then 
cc 
4 
sin(?) 
s i n h ( F ) .  
(11.35) 
n?r sinh(n?rh/d) 
@(X,Y) = c 
n=1,3, ... 
A plot of the first 21 terms of the RHS of Equation 11.35, for the region -2d < 
x < 2d and -h < y < h, is shown in Figure 11.4. Notice the Gibbs overshoot 
phenomenon occurring at the discontinuities. 
Example 11.2 The previous example was particularly simple, because the periodic 
boundary conditions immediately forced one of the separated solutions to have an 
oscillatory form. The boundary conditions shown in Figure 1 1.5 are not as obliging. 
This is again a two-dimensional problem, but now we are looking for the solution to 
Laplace's equation in the interior of an a X b box. On the two vertical surfaces, the 
potential is required to be zero, while on the top and bottom surfaces, CD = 2 1. In 
this case, it isn't obvious that either X(x) or Y (y) needs to have an oscillatory form, 
because none of the boundary conditions are periodic. We can use a common trick, 
however, and attempt to convert this problem into an equivalent periodic problem. 

432 
SOLUTIONS TO LAPLACE'S EQUATION 
I 
Y 
Q, 
Figure 11.4 The Electric Potential Distribution for Square Wave Boundary Conditions at 
y = ?1 
a 
@ = O  
i 
@ =  1 
@ = O  
@=-1 
b 
Figure 11.5 Square Box Boundary Conditions 
We will try to change the problem into the form shown in Figure 11.6. The position 
of the original a X b box is indicated by the shaded region shown. The top and bottom 
boundaries of this shaded region must be fixed at 
= 2 1 so they match the original 
boundary conditions. We need to apply periodic conditions along the rest of the top 
and bottom surfaces, where the ? marks appear in the figure, to try to force the vertical 
- 
b 
Figure 11.6 Attempt to Convert to a Problem Periodic in the x-Direction 

EXPANSIONS WITH EIGENFUNCTIONS 
433 
@ =1 
@ = - I  
@ =1 
@=-1 
@ =1 
a 
/ 
@=-1 
@ =I 
@ = - 1  
@ =1 
@ = - 1  
- - b p  
~- 
Figure 11.7 Conversion to a Periodic Problem in the x-Direction 
sides of the shaded region to have @ = 0. Looking back on the previous example, 
it is pretty easy to see that these requirements are met with the potential distribution 
shown in Figure 1 1.7. The solution inside the box is therefore identical to the solution 
to the previous example with the x-periodicity equal to 26. The only difference is, of 
course, that this solution is only valid inside the region of the original a X b box of 
Figure 11.5. 
We successfully made this problem periodic in the x-direction. Could we have 
done the same thing in the y-direction? Figure 11.8 shows what this would look like. 
Again, we need to apply periodic potentials to the areas where the ? marks occur in 
the figure, to try to force the horizontal sides of the shaded region to have @ = 2 1. 
It’s pretty obvious this cannot work. In this case, the x-direction must be chosen as 
the periodic one. 
An interesting twist is added to this problem if we modify the boundary conditions, 
as shown in Figure 1 1.9. In a case llke this, you will find it is impossible to simply 
make either the x- or y-direction periodic. Instead, the problem must be broken up 
into two parts, as shown in Figure 11.10, Because Laplace’s Equation is linear, the 
solution can be written as the sum of two solutions, @I (x, y) and @*(x, y ) ,  which 
each satisfy modified boundary conditions shown for the two boxes on the right. The 
function @ ~ ( x ,  
y) satisfies Laplace’s equation and has boundary conditions which are 
amenable to periodic extension in the x-direction. The function @(x, y) also satisfies 
Laplace’s equation, but with conditions which can be extended in the y-direction. 
The combination Q1(x, y )  + Q2(x, y) = @(x, y) satisfies Laplace’s equation inside 
the box and satisfies the original boundary conditions of Figure 1 1.9. 
11.2 EXPANSIONS WITH ELGENFUNCTIONS 
Before looking at solutions to Laplace’s equation in other coordinate systems, we need 
to establish some general techniques for expanding functions with linear summations 
of orthogonal functions. In the Cartesian problems we have discussed so far, we found 
that we could evaluate the individual amplitude constants in the general solution by 
using Fourier series type orthogonality conditions. This worked because the terms in 
our sums of the general solution contained sine and cosine functions. In this section, 
we will generalize this technique for other problems. We will begin by reviewing the 

434 
, @ = O  
I 
? 
SOLUTIONS TO LAPLACE’S EQUATION 
a , = O  
? 
Figure 11.8 Improper Conversion to a Periodic Problem in the y-Direction 
@ = 1  
a 
a,=l 
I 
I 
@(x,y) = ? 
@=-I 
a,=-1 
b 
Figure 11.9 Compound Cartesian Boundary Conditions 

EXPANSIONS WITH EIGENFUNCTIONS 
435 
O =  I 
8 =  
I 
8= 1 
O=O 
@=-I 
@=-I 
,#=O 
Periodic in x 
Period = 2b 
Periodic in y 
Period = 2a 
Figure 11.10 Dividing the Compound Problem into Two Periodic Problems 
:igenvalue and eigenvector concepts associated with the linear algebra of matrices, 
with which the reader is assumed to be familiar. These ideas will then be extended to 
Lhe eigenvalues and eigenfunctions of linear differential operators. 
11.2.1 Eigenvectors and Eigenvalues 
Eigenvectors and eigenvalues are concepts used frequently in matrix algebra. A matrix 
[MI is said to be associated with a set of eigenvectors [ Vn] and eigenvalues A,, if 
[W[Vnl = An[VnI- 
(1 1.36) 
[n other words, a vector is an eigenvector of a matrix if premultiplying it by the 
matrix results in a constant times the original vector. The proportionality constant is 
the eigenvalue. Given the matrix [MI, the eigenvalues are found from the condition 
I[Ml - [lIAnIdet = 0. 
(11.37) 
The eigenvectors, to within an arbitrary constant, can then be found by inserting the 
zigenvalues into Equation 11.36 and solving for the components. A 3 X 3 matrix will 
have three eigenvalues. If the eigenvalues are all different, there will be three distinct 
eigenvectors. If there is some degeneracy in the eigenvalues, then the eigenvectors 
associated with these degenerate eigenvalues will not be uniquely defined. These 
ideas were covered in detail in Chapter 4. 
A matrix [MI is Hermitian if 
M.. 
= M*.. 
(11.38) 
That is to say, a matrix is Hermitian if it is equal to its complex conjugate transpose. 
If a matrix is Hermitian, its eigenvalues are always pure real, and its eigenvectors are 
orthogonal. 
d
J
 --J 
11.2.2 Eigenfunctions and Eigenvalues 
These ideas can be extended to operators. Let L,,(x> be a linear differential operator. 
In direct analogy with the matrix definition, the set of eigenfunctions &(x) associated 
with this operator satisfy the condition 
Lop(x> &(X) = A n 4 n ( x ) .  
(1 1.39) 

436 
SOLUTIONS TO LAPLACE’S EQUATION 
In other words, when the operator acts on any of its eigenfunctions, the result is just a 
constant times the eigenfunction. The constant A, is still referred to as an eigenvalue. 
Actually, it is convenient to extend this definition a little further, to include what 
is called a weighting function. The eigenfunctions of an operator with a weighting 
function obey the equation 
-Lp(x> b ( x )  = Lw(x)4+l(x), 
(11.40) 
where w(x) is any real, positive definite function of x. Positive definite is a fancy way 
of saying w(x) > 0 for all x. Our original definition in Equation 11.39 is a just special 
case where w(x) = 1. 
11.23 Hermitian Operators 
An operator Lop(x) can be combined with a set of basis functions h ( x )  to form a set 
of matrix elements 
hi = l d x  g(x)LoPgj(x). 
(11.41) 
The functions %(x) may or may not be the eigenfunctions of the operator. In general, 
these functions are complex, as is the operator itself. The matrix elements generated 
by this integration can also be complex. The integration is done over some range of 
the variable x that we label with the symbol M. 
Following an analogy with matrix algebra, a linear operator is Hermitian if 
hj = LJi, 
(11.42) 
or equivalently 
(1 1.43) 
In general, satisfying the Hermitian condition of Equation 11.43 depends not only 
on the operator GP, but also on the functions &(x) and specifically the nature of 
their boundary conditions at the limits of M. Most often, it is convenient to use the 
eigenfunctions of the operator for the &(x), but in some cases other functions are 
used. 
11.2.4 Eigenvalues and Eigenfunctions of Hermitian Operators 
The eigenvalues and eigenfunctions of a Hermitian operator have three very useful 
properties. First the eigenvalues are always pure real. Second, the eigenfunctions 
will always obey an orthogonality condition. Third, the collection of eigenfunctions 
usually forms a complete set of functions. That is, any function can be expanded as 
a linear sum of the eigenfunctions. 
To prove the first two of these assertions, let &(x) be the nth eigenfunction of 
the Hermitian operator .Lop, with a corresponding eigenvalue of A,. We will use the 

EXPANSIONS WITH EIGENFUNCTIONS 
437 
extended version of the eigenvalueleigenfunction condition that includes a weighting 
function: 
Remember this weighting function is areal, positive definite function. If the Hermitian 
condition of Equation 1 1.43 is satisfied, then Equation 1 1.44 can be substituted into 
this condition to give 
which, on rearrangement. becomes 
(11.45) 
(1 1.46) 
This equation contains some very useful information. If m = n, the quantity 
4~(x)w(x>4,(x) 
= I+,(x)12w(x) is positive definite, so 
(I 1.47) 
Consequently, 
A, = A,*. 
(11.48) 
This shows the eigenvalues of a Hermitian operator are real. In addition, if m # n 
and A, # A,, 
the only way Equation 11.46 can work is if 
(1 1.49) 
Equation 11.49 is an orthogonality condition. Notice the range of the integral fl is 
the same range used in Equation 1 1.43 to produce the Hermitian condition. 
If two or more eigenfunctions have the same eigenvalue, that is, if A, = A,,,, 
the line of reasoning which leads to Equation 11.49 is not valid. We can, however, 
always arrange our choices for the degenerate eigenfunctions so that Equation 1 1.49 
still holds. This is directly analogous to the case of degenerate eigenvalues in ma- 
trix algebra, discussed in Chapter 4. The general method for constructing linearly 
independent, orthogonal eigenfunctions that have degenerate eigenvalues is called 
the Gram-Schmidt orthogonalization process. You can work out an example of this 
process in one of the exercises at the end of the chapter. 
We also claimed that the eigenfunctions of a Hermitian operator will usually form 
the basis of a complete set of functions. Completeness is a difficult property to prove 
and beyond the scope of this text. It can be shown that the eigenfunctions of a Sturm- 
Liouville operator always form a complete set. These types of operators are discussed 
in the following section. 

438 
SOLUTIONS TO LAPLACE’S EQUATION 
The consequences of having a complete set of orthogonal eigenfunctions are 
enormous. If this is the case, we have a method for expanding arbitrary functions in 
terms of these eigenfunctions. Imagine we wanted to expand the function s(x) using 
the eigenfunctions of an operator that obeys Equation 11.44. If the eigenfunctions 
are complete, we can write 
s(x) = CS, 
ACxX 
(11.50) 
n 
where the S, are (possibly complex) coefficients that we need to determine. To isolate 
the mth coefficient, multiply both sides of this expression by w ( x ) ~ & )  and integrate 
over the full range: 
The orthogonality condition will force every element of the sum on the right-hand 
side to vanish, except the n = m term. That single remaining term gives an expression 
for S-: 
(11.52) 
11.2.5 Sturn-Liouville Operators 
We will now prove that the Hermitian condition always holds for the so-called Sturm- 
Liouville operators, if a special set of boundary conditions is imposed on the functions 
used in the Hermitian integration. An operator is in Sturm-Liouville form if it can be 
written as 
(1 1.53) 
where p(x) and q(n) are two real functions. This is not a very restrictive condition, be- 
cause the differential operations in all second-order linear equations can be converted 
into this form by multiplying the differential equation by the proper function. 
To see how the Sturm-Liouville form of Equation 11.53 leads to the Hermitian 
condition, substitute this operator into the LHS of Equation 1 1.43 to obtain 
Likewise, the RHS of Equation 11.43 becomes 
(1 1.54) 
(1 1.55) 

EXPANSIONS WITH EIGENFUNCTIONS 
439 
The last terms of both these expressions are obviously the same, so to prove Equa- 
tion 11.43, we only need show the equality of the first two terms. Integrating these 
terms by parts and equating them gives 
The integrals in Equation 1 1.56 cancel, so if we limit our possible choice of functions 
to those that satisfy 
(1 1.57) 
for all possible choices of u,(x) and u,(x), the Hermitian condition will always be 
satisfied. 
It is important to note that an operator that is not in Sturm-Liouville form can still 
be Hermitian. A good example of this is the operator L = -iJ/dx, which you can 
easily show is Hermitian when appropriate boundary conditions are imposed on its 
eigenfunctions. We presented the proof for Sturm-Liouville systems here for several 
reasons. First, all the second-order, separated differential equations that are used to 
solve Laplace's equation can be put into Sturm-Liouville form. In addition, although 
we will not prove it here, if an operator is in Sturm-Liouville form, its eigenfunctions 
form a complete set of functions. The proof of this fact can be found in Griffel's 
book, Applied Functional Analysis. Consequently, a linear sum of these complete, 
orthogonal solutions can be used to satisfy any set of well posed boundary conditions. 
Example 11.3 Consider the simple differential operator 
d2 
dx2 
Lop = -. 
(1 1.58) 
For the simplest weighting function w(x) = 1, the eigenfunctions and eigenvalues of 
this operator satisfy the equation 
(11.59) 
We already encountered this equation when we separated Laplace's equation in 
Cartesian coordinates. Recall, there were several possible forms for the general 
solution of Equation 1 1.59. One form was 

440 
SOLUTIONS TO LAPLACE'S EQUATION 
where A,, and B,, are complex amplitude constants. Since we have not yet imposed any 
other requirements beyond the differential equation, the eigenvalues and amplitude 
constants are undetermined. 
What happens if we check to see if Lop is Hermitian? First, notice that this operator 
is in the Sturm-Liouville form of Equation 11.53, with p(x) = 1 and 4(x) = 0. This 
means the Hermitian condition is satisfied, using the eigenfunctions as the basis 
function, as long as the +,,(x) satisfy the condition of Equation 11.57: 
(11.61) 
Notice that the range of integration a has been defined by the two end points x = a 
and x = b. The easiest way to satisfy this expression is to require the +,,(x) or their 
first derivatives be zero at these endpoints. The only way to accomplish this is to make 
+,,(x) oscillatory, so the A,, must be real, negative numbers. If we define ik,, E 6, 
this puts Equation 1 1.0 into the form 
If we specifically force +,,(a) = 0 and +,,(b) = 0, the eigenvalues are limited to the 
discrete set of values 
A,, = - (">z n = 1,2,3 ..., 
b - a  
and the most succinct form for the eigenfunctions becomes 
(11.63) 
(1 1.64) 
The amplitude constant C- 
is arbitrary, but is usually chosen so the eigenfunction is 
normalized over the range of a. In this particular case, we can set 
(1 1.65) 
According to our previous discussion, these eigenfunction should obey an orthogo- 
nality condition. Indeed this is the case, because it is easy to show that 
(1 1.66) 

CYLINDRICAL SOLUTIONS 
441 
11.3 CYLINDRICAL SOLUTIONS 
Now that we know how to perform expansions using the eigenfunctions of Hennitian 
operators, we can proceed to solve Laplace’s equation in other coordinate systems. In 
this section, we explore the solutions to Laplace’s equation in cylindrical geometries. 
In cylindrical coordinates Laplace’s equation becomes 
We will again use the method of separation of variables and look for solutions of the 
form 
Inserting this form into Laplace’s equation gives 
11.3.1 
In cylindrical coordinates, things are quite a bit more complicated than they were in the 
Cartesian case, because the p and 8 dependencies are not isolated in Equation 11.69. 
In addition, unlike the Cartesian solutions, the p ,  8, and z separated solutions do not 
all have the same functional form. We will tackle these different solutions one at a 
time. 
Solutions of the Separated Equations 
Separating the z-Dependence The z-dependence is easy to separate because it is 
entirely contained in the last term of Equation 11.69. Setting this term equal to a 
separation constant c, gives 
This has the form of an eigenvalue problem 
-- 
a2z(z) - C z Z ( Z ) .  
8 2 2  
(1 1.70) 
(11.71) 
Just like the Cartesian examples we presented earlier in this chapter, the possible 
values that cz can take on will be limited by whatever boundary conditions are 
specified for the problem. For example, perhaps Q(p, 8, z) is required to be zero at 
z = a and z = b. We explored this case already in a previous example. This forces 
Z(z) to have an oscillatory form, and consequently c, must be negative and real. If 

442 
SOLUTIONS TO LAPLACE’S EQUATION 
we set c, = -k2 < 0, the form of Z(z) is 
We have already shown that this operator, when combined with the functions of 
Equation 1 1.72 and the proper boundary conditions, is Hermitian. Therefore, we can 
expand arbitrary functions of z using this set of solutions. 
In contrast, we might have a set of boundary conditions which requires @(p, 8, z) 
to vanish as z --+ 203. Oscillatory solutions of Z(z) are obviously not going to work 
in this case. In fact, the only way to get a solution that meets these conditions is to 
use a positive, real separation constant so the solutions of Z(z) are exponential. With 
c, = a2 > 0, 
These solutions, when combined with the d2/dz2 operator, do not satisfy the Hermi- 
tian condition and consequently do not obey an orthogonality condition. This means 
it is not possible to use these functions in an expansion. 
As always, the special case where c, = 0 needs to be considered. The form of 
Z(z) is linear: 
c, = 0. 
(1 1.74) 
Separating the @-Dependence 
by p2, Equation 11.69 becomes 
After removing the 2-dependence and multiplying 
The @-dependence is now isolated to the last term, which can be set equal to a second 
separation constant co: 
(1 1.76) 
This equation is also in the form of an eigenvalue problem, identical to the one 
describing the z-dependence. Taking co = -v2 < 0 gives orthogonal, oscillating 
solutions of the form 

CYLINDRICAL SOLUTIONS 
443 
Taking ce = p2 > 0 results in nonorthogonal solutions of the form 
The special case of ce = 0 gives the form 
cg = 0. 
(1 1.79) 
Most cylindrical solutions need to be periodic in 8, repeating every 27r. If this were 
not the case, H ( 8 )  # H(8 + 27r), and @(p, 8, z) would not be a single-valued function 
of position. For solutions that are periodic in 6, the form given in Equation 11.77 
with v = 1,2,3,. . . and the H(0) + 1 solution of Equation 11.79 are the only 
ones allowed. We can actually combine these two forms together into just the form 
of Equation 11.77 if we allow v = 0 to be included in that expression, so now 
ce = -v2 5 0. There are, however, some cases where the boundary conditions 
limit the range of 8 to something less than 0 to 27r. In these cases, c g  can be greater 
than zero, and the other solutions of Equation 11.78 plus the H ( 8 )  + 8 solution of 
Equation 1 1.79 are allowed. 
Separating the p-Dependence 
With the z- and 6-dependencies removed by insert- 
ing their separation constants, the differential equation for R(p) becomes 
(11.80) 
The form of the solution to this equation depends on the nature of the separation 
constants. When c~ I 
0, the most common case, the solutions of R(p) are either 
Bessel functions or modified Bessel functions depending on the sign of the c, constant. 
The sections that follow derive series representations for these important functions. 
11.3.2 Solutions for R(p) with c, > 0 and Cg I 
0 
The first situation we will consider is the case for ce = - v2 zs 0 and cz = a2 > 0. 
Notice we have included v = 0 in this category as we discussed when separating out 
the 8-dependence in the previous section. This is a solution that oscillates (except 
when v = 0) in 8 and exponentially grows or decays in z. For this case, Equation 1 1.80 
becomes 
(11.81) 
The procedure for solving this equation first involves scaling the p variable using 
the value of a. Notice that these two quantities have reciprocal dimensions so that 
the quantity ( a p )  is dimensionless. To accomplish this, rearrange Equation 11.8 1 into 
the form 
(11.82) 

444 
SOLUTIONS TO LAPLACE’S EQUATION 
Now define a new variable r = cup and a new function 
N P )  = &cup). 
(1 1.83) 
With these new quantities, Equation 11.82 can be written as 
(11.84) 
Once this equation is solved for k(r), the function R@) is obtained using Equa- 
tion 11.83. This result shows that the cu parameter acts like a scaling factor for the 
solution, because the qualitative behavior of R(p) does not depend on a. The v pa- 
rameter, however, is not just a scaling factor. The qualitative behavior of &r), and 
thus R(p), depends on the specific value of u. 
Bessel Functions of the First and Second Kind Equation 11.84 can be solved 
using the method of Frobenius, described in the previous chapter. Recall that, with 
this method, we look for series solutions of the form 
n=O 
By plugging this into Equation 11.84, and requiring co # 
equation for s: 
s2 - y2 = 0 
with the result 
s =  +v. 
This gives two solutions for k(r): 
03 
dl(r) = C a n r n + y  froms = t v  
n=O 
and 
m 
&(r) = C b n r n - u  from s = - v. 
n=O 
(1 1.85) 
0, we get the indicial 
(11.86) 
(1 1.87) 
(11.88) 
(11.89) 
Two different sets of coefficients, a,, and b,, have been used to distinguish between 
the solutions. 
When the first of these two series, Equation 11.88, is inserted back into the 
differential equation, the coefficients can be easily evaluated. The series is 
(11.90) 

CYLINDRICAL SOLUTIONS 
445 
where a, is an undetermined coefficient. If a, is chosen to be 1/2’u!, the RHS of 
Equation 1 1.90 is defined as the vth-order Bessel function of the first kind: 
(11.91) 
Unfortunately, if we try the same trick with the series in Equation 1 1.89, we run 
into trouble. If u is an integer, which is the most common situation, the b, coefficients 
will always diverge after some finite number of terms. Consequently, when v is an 
integer we cannot generate a second solution using this method. This divergence stems 
from a singularity of the original differential equation at r = 0. This second solution 
has a “logarithmic” singularity at r = 0, which means it cannot be represented by a 
power series. However, we can find the second solution if we modify the method of 
Frobenius slightly, by explicitly identifying the singularity with a logarithm factor. 
Now we look for a solution of the form, 
co 
&(r) = ln(r)J,(r> + 
b,r”+s, 
(1 1.92) 
where J,(r) is the first solution we found in Equation 11.91. Actually, because the 
differential equation is linear, any linear combination of k l ( r )  = J,(r) and the k*(r) 
obtained with this method is an equally legitimate second solution. This gives us 
some flexibility in choosing the second solution. A common choice is called the 
Bessel function of the second kind, or sometimes the Neumann function of order u. 
Deriving the result is straightforward for the v = 0 case. The general case for arbitrary 
v, however, is quite complicated. We will simply give the result here and recommend 
you investigate the classic reference by Watson, A Treatise on the Theory of Bessel 
Functions, for the details. The second solution is 
n=O 
where h, is the partial harmonic series defined by 
and y is 
y = lim (h, - Inn) = .5772, 
n-m 
(11.93) 
( 1  1.94) 
(11.95) 
which is called the Euler constant. 

446 
SOLUTIONS TO LAPLACE’S EQUATION 
Figure 11.11 Bessel Functions of the First and Second Kind 
Because these functions are so complicated, they have been extensively tabulated 
in reference books, such as the Handbook of Mathematical Functions by Abramowitz 
and Stegun. The u = 0 and Y = 1 versions of these functions are plotted in 
Figure 11.11. All the Jv(r) functions are zero at the origin, except for Jo, which 
has Jo(0) = 1. The functions all have the form of a damped oscillation, and the 
points where they cross zero turn out to be very important. These points are not 
evenly spaced, and we usually define the nth zero crossing of Jv(r) as the special 
constant am. These values are also extensively tabulated in the literature. The Yv(r) 
functions all go to minus infinity as r --+ 
0. Away from the origin, these functions 
are again in the form of damped oscillations. The nth zero crossing of Yv(r) 
occurs 
at r = &,,. 
After all this, it must be remembered that we origindy were seeking the general 
solution of R(p) in Equation 1 1.8 1. We have obtained two independent solutions for 
k(r) in Equation 11.84. These solutions can be related by to R(p) via Equation 1 1.83. 
Therefore, the general solution forR(p) is any linear combination of the two solutions: 
(11.96) 
The General Solution for @(p, 6,z) The general solution for @(p, 0, z), for the 
case where c, = a2 > 0 and ce = -u2 5 0, is a linear sum of all the possible 
products of the separated functions, 
(11.97) 
where the summation is over all the allowed values of v and a. In general, the 
boundary conditions will Emit these possible values, as you will see in the example 
that follows. 
Keep in mind, we could have just as easily chosen to write the @-dependence 
using complex exponentials, but that is not as convenient when we expect H(0) 
to be real. Likewise, the z-dependence has been written with the sinh and cosh 

CYLINDRICAL SOLUTIONS 
447 
functions, but could just as easily have been expressed with growing or decaying 
exponential functions. Computational convenience dictates which choice makes more 
sense. Notice the &dependence is periodic, while the z-dependence is exponential. 
This confirms the general rule for separated solutions we declared at the beginning of 
the chapter. If a solution oscillates in one direction, it must exponentiate in another. 
Applying the Boundary Conditions Consider the boundary conditions described 
in Figure 1 1.12. The potential is controlled on the surface of a cylinder of radius r, 
and length 22,. 
The sides of the cylinder are held at @(To, 8 , ~ )  
= 0. The top is held 
at @(p, 8, z,) 
= f~(p, 
O), a potential that varies with both p and 8, while the bottom 
is held at a different potential, @(p, 8, -z,J 
= f&, 8). The problem is to determine 
the potential inside the cylinder. 
It seems appropriate to start with the general solution given by Equation 11.97, 
because the range of 8 is 0 < 8 < 2.n, which requires the solution to be periodic 
in 8. Also, given the potentials on the top and bottom, the solution does not appear 
to be periodic in z. 
The geometry, boundary conditions, and a little physical reasoning allow the 
general form to be simplified somewhat. First, it would not make any physical sense 
for the potential to diverge as p t 0, so the R(p) solution will not have any of the 
YJap) solutions. Next, the boundary condition which forces @(T,, 8,z) = 0 limits 
the possible values of a to a discrete set. Remember, the zeros of a particular J,(ap) 
Bessel function occur when the argument is equal to one of the constants a,. SO to 
have R(r,) = 0, we must have 
ar0 = am, 
(11.98) 
where u = 0,1,2,. . . and n = 1,2,3,. . . . In this way, the summation over a in 
Equation 11.97 is converted to a summation over n, and the general solution is 
WP, 0, -zo) = f,(p,e) 
Figure 11.12 Cylindrical Boundary Conditions Nonperiodic in the z-Direction 

448 
SOLUTIONS TO LAPLACE’S EQUATION 
reduced to 
Notice that the value of LY in the argument of the hyperbolic functions is determined 
by a boundary condition associated with the Bessel function. 
If the shorthand notation on the RHS of Equation 11.99 is expanded, it is nec- 
essary to introduce four amplitude constants. In order to determine these constants, 
the top and bottom boundary conditions, yet unused, need to be introduced. These 
amplitude constants can be determined much like we found the amplitude constants 
in the Cartesian problems at the beginning of the chapter. By applying orthogonality 
conditions, we can isolate and evaluate one of the constants associated with a partic- 
ular v and a particular n. This will require two different orthogonality relations, one 
to isolate values of v. and one to isolate values of n. 
OrthogonaCity Relations We already know that the sin and cos functions which 
describe the &dependence obey an orthogonality condition, while the sinh and cosh 
functions do not. To evaluate all the amplitude constants, we need an additional 
orthogonality relation, which we can obtain from the Bessel functions. We can derive 
it by putting Bessel’s equation in the form of an eigenvalue, eigenfunction problem, 
identify a Hermitian operator, and then determine the orthogonality properties of its 
eigenfunctions. 
Originally, the Jv(a,p/ro) Bessel functions were constructed to satisfy Equa- 
tion 11.81: 
(11.100) 
This is in the form of a standard eigenvalue, eigenfunction problem with the weighting 
functionw(x) = 1, 
where 
d2 
1 d 
u2 
L = - - + - - - -  
OP 
dp2 
p a p  
p2 
(1 1.102) 
and 
2 
A, = -2. 
(11.104) 
Because u is inside the operator and fixed, different eigenfunctions of this operator 
are obtained by incrementing n, i.e., n = 1,2,3,. . . . 

CYLINDRICAL SOLUTIONS 
449 
These functions will obey an orthogonality condition if, when they are combined 
with Lop, they satisfy the Hermitian condition, that is, if 
dP JU(" 
m p/ro)Lop Ju("m P/ro> 
equals 
.I, 
d p  Ju (a 
,p/ro )Lop Ju (am 
p/r0 1. 
Unfortunately, it turns out that these two integrals are not equal and the Bessel 
functions, by themselves, are not orthogonal. 
We know, from our proof in the previous section, that an operator in Sturm- 
Liouville form, combined with the appropriate boundary conditions, will automati- 
cally lead to the Hermitian condition. The operator in Equation 11.102 is not, itself, 
in Sturm-Liouville form, but if Equation 11.100 is multiplied by p it becomes 
which can be written as 
The modified operator for this equation is 
( 1 1.106) 
(1 1.107) 
which is in Sturm-Liouville form. Equation 1 1.106 is in the form of a eigenfunction 
equation, with a weighting function of w ( p )  = p :  
= AnP4n- 
(1 1.108) 
Here again, the eigenfunctions and eigenvalues are 
+n = Ju(aunp/ro) 
2 
An = -2. 
(1 1.109) 
(11.110) 
Because the operator LLp is in Sturm-Liouville form, the Hermitian condition will 
be satisfied if Equation 11 5 7  is satisfied: 
(1 1.1 11) 

450 
SOLUTIONS TO LAPLACE'S EQUATION 
Because the range of interest for this problem has an obvious lower limit of p = 0 and 
an upper limit of p = r,, this condition is easily satisfied. The lower limit vanishes 
due to the presence of the p term, and the upper limit vanishes because Jv(av,) is by 
definition zero. Thus with fl covering the range 0 < p < r,, the modified operator 
of Equation 11.107 is indeed Hermitian. This means the Bessel functions form a 
complete set of functions and obey the orthogonality condition of Equation 1 1.49: 
( 1 I. 1 12) 
In order to perform expansions, we need one more tool. We need to also know the 
value of the integral of Equation 1 1.1 12 when m = n. We give the result, without the 
proof, that the general expression is 
where we have used the Kronecker delta symbol, a,,,,,. 
The orthogonality condition of Equation 11.113 now allows any function f(p), 
which obeys f ( p  = ro) = 0 and f(p) < a, 
to be expanded in the range 0 5 p I r, 
using the Bessel functions. In other words, we can write 
with 
( 1 1.1 14) 
(1 1.1 15) 
Evaluating the Amplitude Constants Now we can pick up the cylindrical problem 
at the point of Equation 11.99. The problem is further simplified if we impose odd 
symmetry in z by setting f&, 0) = -f&, 
0). This eliminates the hyperbolic cosine 
functions and Equation 11.99 becomes 
At this point, it is convenient to insert the amplitude constants and explicitly write 
out the summations of Equation 11.116: 
m
m
 
}. 
(11.117) 
A, sin(v6) sinh(a,z/ro)Jv(a,p/ro,) 
+B, cos(v0) sWa,z/ro)Jda,p/r0) 
w p m )  = xx{ 
v=O n=l 
Now we need to determine the A ,  and B, coefficients using the boundary con- 
ditions on the top and bottom surfaces. Because the odd symmetry in z has already 

CYLINDRICAL SOLUTIONS 
451 
been imposed, we can use either the top or bottom boundary. Picking the top, the 
series expression is evaluated at z = +zo: 
The amplitude constants can now be isolated and evaluated by using the orthogonality 
conditions of the trigonometric functions and of the Bessel functions. Because there 
are two summations, one over n and one over u, two orthogonality conditions are 
required. 
To evaluate the A,, 
operate on both sides of Equation 1 1.1 18 with 
1 2 n d 0  sin(u,0). 
This filters out all but the v = u, sine terms in Equation 1 1.1 18: 
(1 1.1 19) 
Notice how the order of the Bessel function u, has been selected, not by the orthog- 
onality of the Bessel functions, but by the orthogonality of the sine functions. 
A particular value of n is selected by operating on both sides of Equation 1 1.120 
with 
(1 1.121) 
This singles out the n, term to give 

452 
and 
SOLUTIONS TO LAPLACE'S EQUATION 
These amplitude constants are then inserted into Equation 11.117 to complete the 
solution. 
A simpler result is obtained if it is assumed that the potential on the top and bottom 
is not a function of 8, i.e., f ~ ( p ,  8) -+ f,(p). In a case like this, only v = 0 is allowed. 
There are no Aon terms because sin(0) = 0. Only Bg, amplitudes are present, and the 
solution for @(p, z) inside the cylinder becomes 
m 
@(P- Z> = C BOn s i ~ ( ~ ~ z / r ~ ) J o ( ~ . p / r , ) ,  (1 1.126) 
n= 1 
with 
(1 1.127) 
11.3.3 
We now turn our attention to cylindrical solutions of Laplace's equation that oscillate 
in both the z- and 8-directions. The general solution is still a product of the separated 
solutions 
Solutions for R(p) with c, < 0 and cg 5 0 
WP, 8, Z) = R(p)ww(z), 
(1 1.128) 
and the functions Z(z) and H (  8 )  still must satisfy Equations 1 1.70 and 1 1.76, respec- 
tively. Now, however, c, = -k2 < 0 and ce = -v2 5 0. This means that the form 
of solutions for these equations will be 
and 
(1 1.129) 
(1 1.130) 
The choice between the trigonometric and exponential forms of the solutions is, as 
usual, determined by whichever is more convenient for a particular set of boundary 
conditions. 
The separated equation for R(p) becomes 
P2 "I 
d2 
1 d 
- 
+ -- - k2 - - R(p) = 0. 
dP2 
PdP 
(1 1.131) 

CYLINDRICAL SOLUTIONS 
453 
Equation 11.131 is identical to Equation 11.81 with a2 --j -k2 or a -+ 
ik. The 
solutions to Equation 1 1.8 1 were found to be Bessel functions of the first and second 
kind: 
J u ( w )  
and 
Y,(ap). 
The solutions to Equation 1 1.13 1 can therefore be written as 
( 1 1.1 32) 
The imaginary arguments in the Bessel functions have a similar effect as imaginary 
arguments in the sine and cosine functions. These functions no longer oscillate, but 
they grow or decay with p. We might have expected this, because the general rule for 
the separated solutions of Laplace’s equation in Cartesian systems was that at least 
one separated solution was growing or decaying. 
The functions in Equation 11.132 are not in the most convenient form, how- 
ever, because they are not pure real functions of p. We can correct this problem by 
constructing two different solutions which are pure real: 
I,(kp) = i-”J,(ikp) 
(1 1.133) 
K,(kp) = (~r/2)i’+~[J~(ikp) 
+ iY,(ikp)]. 
(1 1.134) 
These two functions are called modified Bessel functions of the first and second kind. 
Again, like the Bessel functions, the modified Bessel functions are well tabulated 
in the literature. Graphs of the first few, Zo(r), Zl(r), Ko(r) and Kl(r), are shown in 
Figure 11.13. Notice, the Z,(r) functions are finite at r = 0 and blow up as r + m. 
In contrast, the K,(r) functions blow up as r -+ 0 and go to zero as r 
50. 
Now, we can write the general solution of R(p) as 
and the general form of @(p, 8, z) for this case becomes 
cos(v8) 
cos(kz) 
Z,(kp) 
{ sin(v8) { sin(kz) { K,(kp) 
”‘) 
= c 
u
K
 
(1 1.135) 
( 1 1.1 36) 
Figure 11.13 Modified Bessel Functions of the First and Second Kind 

454 
SOLUTIONS TO LAPLACE’S EQUATION 
In this expression, sine and cosine functions have been used for the z- and 8- depen- 
dencies. As always, complex exponential functions could have been used instead, but 
the sine-cosine functions are better when we expect the final answer to be pure real. 
Again notice the presence of at least one oscillating factor and one growingldecaying 
factor in each term. 
Example 11.4 
As an example of an electrostatic problem that uses the type of 
solution given in Equation 11.136, consider the geometry shown in Figure 11.14. 
This is quite similar to the problem discussed in Figure 11.7, which described a 
periodic boundary condition in Cartesian coordinates. In this problem, the potential 
is controlled on the p = r, surface of the cylinder. Its value is +V, for 0 < z < L/2 
and - V, for L/2 < z < L. We are interested in a solution for @(To, 8, z) inside the 
cylinder where p < r,. 
Notice that the boundary condition is periodic, so @(To. 8, z) = @(I,, 8, z + L) 
for all z. Consequently, the solution for this problem must also be periodic in z, and 
so the solution to Laplace’s equation for p < r, must fall into the general form of 
Equation 11.136. The symmetry of these particular boundary conditions allows us 
to simplify this general form tremendously. First of all, there is no 6-variation, so 
we can set H(8) = 1, and the sum over v only uses the v = 0 term. Second, the 
solution must be finite for p = 0, so there are no Ko(kp) terms, because they diverge 
as p --+ 0. Next, because the boundary conditions have odd symmetry about z, all the 
cos(kz) terms vanish, because they are all even functions. Finally, the periodicity of 
the boundary conditions forces k to be limited to the discrete set of values: 
(1 1.137) 
2 m  
k - + k , , = -  L 
where n = 0,1,2,. . . . 
With these simplifications, Equation 11.136 reduces to 
= 2 A,, sin (F) 
10 (7) 
2mP 
n=l 
( 1 1.1 38) 
(1 1.139) 
Notice the value of k appears not only as an argument of the sin&) term, but also in 
the argument of the modified Bessel function. 
As always, the A,, coefficients can be determined using an orthogonality condition 
and the boundary conditions at p = r,. Operate on both sides of Equation 11.138 by 
1‘ dz sin (F) 
to obtain 
(1 1.140) 

CYLINDRICAL SOLUTIONS 
+ m  
455 
Y 
X 
I 
I 
--oo 
Figure 11.14 Cylindrical Laplace Problem Periodic in the z-Direction 
Now, if we require 
(1 1.141) 
and perform the integral on the LHS of Equation 1 1.140, the A, amplitudes evaluate 
to 

456 
SOLUTIONS TO LAPLACE’S EQUATION 
( 0  
n even 
( 1 1.142) 
The final solution to this problem for p < r, becomes 
In this example, the modified Bessel functions did not combine with the p- 
dependent operator to satisfy a Hermitian condition. Consequently, they were not 
orthogonal functions and could not be used to select a particular value of n. Instead, 
we used the orthogonality of the sine functions to perform the expansion. 
Example 11.5 
As a final example, consider the cylindrical boundary value problem 
shown in Figure 11.15. The potential is held at zero on the top and bottom and at 
+V, on the sides. The solution to Laplace’s equation is sought for p < r, and 
The first thing to decide is the form of the general solution. Is it going to be 
Equation 11.97, with oscillating solutions in p and nonoscillating solutions in z, or 
Equation 1 1.136, with oscillating solutions in z and nonoscillating solutions in p? 
The nonoscillating solutions in z clearly cannot work, because if the amplitudes are 
adjusted to make @ = 0 at z = 0, this solution cannot also be zero at z = L/2. 
Therefore, we must choose the form of Equation 1 1.136. 
Now that we have determined that the solutions must be periodic in z, what is 
the spatial period? Remember we tackled a similar problem in Cartesian geometry, 
where we successfully converted a nonperiodic problem into a periodic one. Let’s 
try the same process here. We can construct an infinite cylinder, of which the region 
shown in Figure 1 1.15 is but a single “cell.” The zero potentials at the top and bottom 
0 < z < L/2. 
1 w 2  
i 
+vo 
~~ ,/----- 
@ = O  
Figure 11.15 Boundary Conditions for a Cylindrical Can 

CYLINDRICAL SOLUTIONS 
I 
, 
I 
457 
-vo I 
I 
I ’  
I I L/2 
1 ,  
I 
The Can 
,’ 
Y I’ 
, 
,- 
X 
” I 
I 
-vo I 
+vo 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-00 
Figure 11.16 The Cylindrical Can Converted to a Periodic Problem 
of this cell must be established by the periodic potential applied to the p = r, surface 
above and below this cell. This situation is shown in Figure 11.16. 
From Figure 11.16 it is clear that the solution inside the can will be identical to 
Equation 11.143, the solution obtained in the previous example. Of course, in this 
case the solution is only valid for 0 < t < L/2. 

458 
SOLUTIONS TO LAPLACE’S EQUATION 
11.4 SPHERICAL SOLUTIONS 
The solutions to Laplace’s equation in spherical geometry are fairly complicated, but 
they are important to understand because of their frequent use in quantum mechanics. 
The organization of the periodic table of elements is due primarily to the nature of 
these types of solutions. We will take the same approach we did with the other 
coordinate systems and look for separable solutions. 
Recall, the spherical coordinates (r, 8,4) of a point P are defined as shown in 
Figure 11.17. The coordinates are limited to the ranges 0 < r < +m, 0 < 6 < T, 
and 0 < 4 < 2rr. The Laplace operator in this system becomes 
1 a2 
l
a
 a 
1 
a2 
sine- + -- 
r dr2 
r2 sin 8 at3 
30 
r2 sin2 8 aC$2 
* 
v2 = --r 
+ ____ 
( 1 1.144) 
Again, we seek solution to 
V2cP(r, e,4) = o 
( 1 1.145) 
by the method of separation of variables. However, because of the r-factor in the first 
term of muation 11.144, it turns out to be more convenient to include a l/r factor 
in the separation process. That is, we will look for a solution with the form: 
(1 1.146) 
R(r) 
w. e,$) = ---PWF(+). 
r 
Substituting this into Laplace’s equation and multiplying by 
r3 sin2 8 
R(r)P(e)F(4) ’ 
we obtain 
r2 sin2 e a2R(r) 
sin8 a 
aP(e) 
1 a 2 ~ ( 4 )  
___- 
+-- 
sine- 
+-- 
= 0. 
R(r) 
dr2 
P(e)ae 
69 
F(4) d42 
( 1 1.147) 
Z 
Figure 11.17 The Spherical Coordinates 

SPHERICAL SOLUTIONS 
459 
Separating the +-Dependence 
Because the last term on the LHS of Equation 1 1.147 
contains all the +-dependence, it can be set equal to a separation constant. In the most 
common spherical problems, the range of + is 0 < + < 2 ~ ,  
and to keep the po- 
tential single-valued, we must have @(r, 8,+) = @(r, 8,+ + 27-r). This means the 
F(+) solutions must be periodic, and thus the separation constant cannot be positive. 
We will stick with the convention used by most scientists and define this separation 
constant as 
(1 1 .l48) 
We have discussed the solutions to this equation already. F(+) can take either of two 
forms: 
(11.149) 
In electrostatics, where the solutions need to be pure real, the second form of 
Equation 11.149 is usually preferred. In quantum mechanics, the solutions can be 
complex, and the first form is frequently used. Regardless of the form, the periodicity 
over the range 0 < 4 < 27-r forces m to take on the discrete values 
m = 0 , 1 , 2  ,.... 
(1 1.150) 
Separating the r-Dependence Now we can proceed to separate the r -dependence. 
Substituting -m2 for the +-dependent term, Equation 11.147 reduces to 
r2 sin2 e d2R(r) 
sin 8 d . 
dp(e) 
+-- 
sm8- 
- m2 = 0. 
~- 
R(r) 
dr2 
P ( e ) a e  
ae 
(1 1.151) 
After dividing this entire equation by sin2 8, the r-dependence is confined to the first 
term, which can be set equal to a second separation constant. For reasons that will 
become clear later, we will give this constant a rather peculiar form: 
r2 d2R(r) = 4(4 + 1). 
-- 
R(r) dr2 
( 1 1.1 52) 
What is the general solution for R(r)? One could go through the entire method 
of Frobenius to generate series solutions for this equation, but in this case, it is not 
difficult to see that the series will collapse to a single term. If we look for a solution 
with the form 
R(r) 0~ rn, 
(1 1.153) 
Equation 11.152 is satisfied if n satisfies the algebraic expression 
(n)(n - 1) = 4(4 + 1). 
( 1 1.154) 

460 
SOLUTIONS TO LAPLACE’S EQUATION 
This implies two solutions for n, 
n = 4 + l  
and 
n = -4. 
The solutions for R(r) then are 
(1 1.155) 
( 1 1.156) 
(1 1.157) 
This can easily be confirmed by substituting these solutions backinto Equation 1 1.152. 
At this point, we cannot say too much about these solutions, because we do not know 
if 4 is real, imaginary, or complex. To determine the nature of 4, the &dependent 
equation must first be solved. 
Separating the 8-Dependence Substituting 4(4 + 1) for the r-dependent term in 
Equation 11.151 gives 
( 1 1.158) 
This equation is called the associated Legendre equation and occurs often in quantum 
mechanics. 
The trigonometric functions in Equation 1 1.158 are difficult to manipulate, but we 
can simplify things quite a bit by introducing the variable change 
x = C O S ~ .  
(1 1.159) 
Because 0 I 
6 I 
T ,  the range for x is - 1 5 x 5 + 1. The extreme values must 
be included in the respective ranges to include the z-axis. This turns out to be a very 
important fact when we begin to solve this equation. With this change of variables, 
dx = - sin 8 de and sin2 8 = 1 - x2, and Equation 11.158 becomes 
m2 
( 1 1.160) 
This new function P(x) is related to our original function P(6) by P(0) = ~ ( C O S  
6). 
Notice Equation 11.160 is in Sturm-Liouville form. We can maneuver it to be in the 
form of an eigenvalueleigenfunction equation as well: 
m2 
{ 1 [(I - x 2 ) - 4  - .--.) P(x) = -4(4 + l)P(x). 
(1 1.161) 
Based on our previous experiences with equations in this form, we know the solutions 
will form a complete, orthogonal set. 
Because of its quantum mechanical significance, we will present a fairly detailed 
solution to Equation 11.161. We will begin by looking for solutions that have no 
+dependence, i.e., solutions with m = 0 that are symmetric around the z-axis. Then 
we will tackle the general case. 

SPHERICAL SOLUTIONS 
461 
11.4.1 Axially Symmetric Solutions with m = 0 
With m = 0, the +-dependence becomes 
F(+) + 1, 
the r-dependence remains the same 
and the equation for P(x) becomes 
- (1 - x )- 
= -4(4 + l)P(X). 
dX [ 
dP(x)l 
dX 
(1 1.162) 
(1 1.163) 
(1 1.164) 
At this point, it is clear that the solution for P(x) depends upon the choice of 4, 
so let's start labeling the functions with an 4 index: P(x) -+ P&). The solution to 
Equation 1 1.164 can be obtained using the method of Frobenius by setting 
m 
P&) 
= x a n x n + s .  
(1 1.165) 
n=O 
With this substitution, 
(1 1.166) 
and Equation 1 1.164 becomes 
2 { & [a,(n + s)xnfs-' - a,(n + S ) X ~ + ~ + '  +4(4 + I)u,x~+~ 
1 
n=O 
Performing the second differentiation gives 
a 
[a& + s)(n + s - 1)xn+s-2 - a,(n + s)(n + s + l)Xn+s 
n=O 
+ .e(4 + l)anXn+s] = 0. 
(1 1.168) 
The next step is to identify the coefficients of every power of x, and set them equal 
to zero: 
m 
ao(s)(s - I ) Y 2  + al(l + s)(s)x'-' + Zen(. 
+ s ) ( I ~  + s - 1) 
(11.169) 
n=2 
- an-2[(n + s - 2)(n + s - 1) - 4(4 + l)]}X*+s-2 
= 0. 

462 
SOLUTIONS TO LAPLACE'S EQUATION 
The coefficients for the 
form the indicial equations: for s 
and the 9-' terms are individually set equal to zero to 
ao(s)(s - 1) = 0 
(1 1.170) 
a,(s + l)(s) = 0. 
(1 1.171) 
Taking q, # 0, Equation 11.170 gives s = 0 and s = 1. These two values of s 
generate two solutions for Pt<x): 
m 
A p ( x )  = C u , x n + I  
froms = 1 
(1 1.172) 
n=O 
and 
m 
&(x) = 
b,xn 
from s = 0. 
n=O 
(1 1.173) 
Two different symbols, a, and b, have been used for the coefficients to distinguish 
between the two different solutions. 
The solution Pt(x) = A t ( x )  is generated with s = 1, a0 # 0, and a1 = 0. The 
solution Pt(x) = b ( x )  is generated with s = 0, bo # 0, and 61 = 0. It is worth 
mentioning that there are other possible choices. When s = 1, Equation 11.171 
requires a1 = 0, but with s = 0, Equation 11.171 can be satisfied with a nonzero bl. 
So an alternate form of &(x) could be generated with both bo and bl nonzero. This 
new solution, however, would be a just a linear combination of our original h ( x )  
and A&). Also, the indicial equations could be satisfied with a0 = bo = 0, and both 
a1 and bl nonzero. In this case, Equation 11.171 would require different values for 
s, namely s = - 1 and s = 0, but we would generate exactly the same solutions as 
our original choice. This makes sense, because the form of the series expansion in 
Equation 1 1.165 implicitly assumes that Q is the lowest nonzero term. 
The last term in Equation 1 1.169 provides generating equations for an and b, for 
n z 2: 
(n + s - 2)(n + s - 1) - .e(& + 1) 
(n + s)(n + s - 1) 
(n + s - 2)(n + s - 1) - .e(X + 1) 
(n + s)(n + s - 1) 
a, = 4 - 2  
( 1 1.174) 
IL 
Is=; 
b, = bn-2 
(11.175) 
We will start off both series with a0 = 1 and bo = 1. 
and b, = bn-2, so the ends of both series look like 
These series have very interesting convergence properties. For n S 1, a, = an-2 
A g ( x )  = * * .  + unsl {x" + xn+2 + xn+4 + * * * }  
(1 1.176) 
and 
= . . . + bnsl {x" + X"+2 + xni4 + . . .} . 
(11.177) 

SPHERICAL SOLUTIONS 
463 
Both these solutions must be valid for 0 I 
8 I 
T, 
or equivalently 1 2 x 2 - 1. 
From the ratio test on Equations 11.176 and 11.177, it is clear that the only way for 
these series to converge at the end points is to have them terminate, i.e., both a, and 
b, must be zero for some, not necessarily the same, value of n. So we must look for 
the conditions that allow these generating functions to start with nonzero a0 (or bo), 
but which eventually cause the series terminate. 
Starting with the a,, Equation 11.174 simplifies to 
(n - l)(n) - C(C + 1) 
a, = an-2 
(n + 
(1 1.178) 
Because a1 = 0, this equation is evaluated for only even values of n, i.e., n = 
2,4,6,. . . . Now, if a, is to be zero for some even value of n, the numerator must go 
to zero. That is to say, for some even value of n, 
(n - l)(n) - C(C + 1) = 0. 
(1 1.179) 
to be zero. Similar arguments conclude 
This will cause a, and all subsequent 
that the series representation for B&) 
terminates at an even value of n given by 
(n - 2)(n - 1) - C(C + 1) = 0. 
(1 1.180) 
Because n is a positive, even integer, the only way either Equation 1 1.179 or 1 1.180 
can be satisfied is if 4 is a real, but not necessarily positive, integer. This is our first 
condition on the separation constant C(X + 1) that isolated the r-dependence. Now 
we need to determine what these terminating solutions for k&) look like. We will 
individually determine different solutions as C is indexed, i.e., 4 = 0, ? 1, 22, . . . . 
Starting with X = 0, the A&) series terminates when Equation 1 1.179 is satisfied, 
i.e., when 
(n - l)(n) = 0 
for n = 2,4,6,. . . . 
(1 1.181) 
It is clear that no positive, even value of n will satisfy this condition. Thus, when 
4 = 0 the A&) 
series of Equation 1 1.172 does not converge for x = 2 1, and is 
therefore unacceptable if the solution must be valid on the z-axis (where x = 2 1). 
With 4 = 0 the &(x) series terminates when Equation 11.180 is satisfied, i.e., when 
(1 1.182) 
This equation is satisfied with n = 2. So with C = 0, the &(x) series of Equa- 
tion 1 1.173 terminates and therefore converges for x = t 1 with b, = 1, bl = 0, and 
bz = 0. Since the A&) 
solution was unacceptable, the only terminating solution for 
4 = 0is 
P&) = $(x) = 1. 
(n - 2)(n - 1) = 0 
for n = 2,4,6,. . . . 
(1 1.183) 
We will now work out the solution for 4 = 1 and a pattern for the solutions will 
become clear. Refemng to Equation 1 1.179, the A, ( x )  series terminates when 
(n - I)(n) - 2 = 0 
for n = 2,4,6,. . . . 
(1 1.184) 

464 
SOLUTIONS TO LAPLACE'S EQUATION 
The Bl(x) series terminates when 
(n - 2)(n - 1) - 2 = 0 
forn = 2,4,6 ,.... 
(1 1.185) 
As before, n must be a positive, even integer. Equation 11.185 can never be satisfied 
for any of the acceptable values of n, and so the series for h1 (x) never converges for 
x = 5 1. Equation 1 1.184, however, is satisfied with n = 2. So for 4 = 1, the only 
series that converges for x = t 
1 is the one for A,(x) and we have 
PI(X) = A,(x> = x. 
(1 1.186) 
This pattern will continue as 4 is increased. The terminating solutions alternate 
between the A&) 
series and the h&) series. The terminating solution is always in 
the form of a polynomial in x, with the order of the polynomial increasing with 4. 
What about negative values of 4? It turns out that negative, integer values of 4 
do not generate new solutions. For example, the 4 = - 1 solution is identical to the 
4 = 0 solution, and the 4 = -2 solution is the same as the 4 = 1 solution. This is 
not surprising because 4 appears as the quantity 4(4 + 1) in the differential equation. 
Setting 4 = -2 produces the same value of l(4 + 1) as does 4 = 1. So to cover all 
the possible terminating solutions, we only need consider integer values of 4 greater 
than or equal to zero. 
These terminating solutions are referred to as the Legendre polynomials. The first 
six are 
Po($ = 1 
Pl(X) = x 
P~(x) 
= (1/2)(3x2 - 1) 
P ~ ( x )  
= (1 /2)(5x3 - 3 ~ )  
P ~ ( x )  
= (l/8)(35x4 - 30x2 + 3). 
Each of these solutions has been normalized so that Pt(l) = 1. There is a clever 
equation, called Rodrigues's formula, for generating these polynomials: 
1 
d'(x2 - 1)' 
P'(X) = - 
2'4! 
d.x? 
' 
(1 1.187) 
The general solution to Laplace's equation with m = 0 is a linear combination of 
the ~ ~ ( C O S  
0) and the r-dependent solutions found in Ekpation 11.163: 
( 1 1.1 88) 
Notice, by the arguments given above, only integer values of 4 greater than or equal 
to zero appear in the sum. 

SPHERICAL SOLUTIONS 
465 
11.4.2 Orthogonality of the Legendre Polynomials 
From our experience with general solutions to Laplace's equation in Cartesian and 
cylindrical geometries, it is clear that, in order to satisfy arbitrary boundary conditions 
using an expansion, some of the functions in the expansion must exhibit orthogonality 
properties. If there are two sums in the general solution, two of the expansion functions 
must have orthogonality properties. If there is only one summation then only one of 
the functions needs to have orthogonality properties. In Equation 11.188 there is only 
one summation and so either the r-dependence functions or the Legendre polynomials 
must obey an orthogonality condition. It isn't difficult to see that the r-dependent 
functions will not be able to do this. So we must investigate the Legendre polynomials 
to determine their orthogonality properties. 
Remember the Legendre polynomials were the solutions to the differential equa- 
tion 
The linear operator associated with this equation is 
(11.189) 
(1 1.190) 
which is in Sturm-Liouville form. We learned earlier that an operator of this type is 
Hermitian if Equation 11.57 is obeyed. That equation, for this case with p(x) = 1 -x2, 
becomes 
jx- 
= 0, 
d P ~ , ( x )  
&(x)( 1 - 2)- 
dx 
(1 1.191) 
I Xm,, 
where 4 and 4' are two different positive integer values. We defined x,, 
and x,i, 
to be the upper and lower limits of the problem. Generally, if the problem includes 
the z-axis, then the range of 8 is 0 5 8 5 T, 
and thus xmi, = - 1 and x- 
= 1. 
Consequently, the zeroing of the LHS of Equation 11.191 is accomplished automat- 
ically through the (1 - x2> term. With this definition for fl and using the Legendre 
polynomials, the operator of Equation 11.190 is Hermitian. 
Equation 11.189 is also in the form of an eigenvalue equation. The eigenvalues are 
-(X)(k' + 1) and the eigenfunctions are the pt(x). Since we have already established 
the Hermitian property, we can immediately write down the orthogonality relation 
dx P~(X>?~/(X) 
= 0 
4 # 4'. 
(11.192) 
If we evaluate the above integral when 4 = 4', the orthogonality condition for these 
polynomials becomes 
P + l  
(1 1.193) 

466 
In terms of 8: 
SOLUTIONS TO LAPLACE’S EQUATION 
11.43 Solutions Valid off the z-Axis 
If a spherical problem excludes the z-axis from consideration, the requirement that the 
solutions be finite at 8 = 0 and 8 = T (x = 21) can be relaxed. A set of boundary 
conditions which leads to this type of solution is shown in Figure 11.18. These 
boundary conditions exist on a conical surface located at 8 = 8, and 8 = (m - 8,). 
The potential is held at + V, on the top part of the cone, and - V, on the bottom. A 
solution for the potential in the region 8, < 8 < (T - 8,) is desired. 
The general solution to Laplace’s equation for this type of problem develops along 
the lines of the previous section. If there is no +-variation, the separable solution can 
be written as 
where R(r) is the same function we developed previously, 
re 
r 
(1 1.195) 
(1 1.196) 
and Q( 8) is a new solution to Equation 11.158 with m = 0, which does not need to 
be finite on the z-axis. We use the function Q(8). because we want to reserve the P( 8) 
notation for the Legendre polynomial solutions we derived in the previous section. 
The Q( 8) solutions, of course, depend on the value of 4? and so we will, as before, 
use its value as an index: Q(8) ---f a(@. 
Again we will make the substitution 
Z 
/ /’ 
Figure 11.18 Spherical Boundary Conditions that Exclude the z-Axis 

467 
SPHERICAL SOLUTIONS 
x = cos 0 and Qg(cos 0) = a(@. 
The method of Frobenius can be used to develop 
the two solutions for &(x), and they will be identical in form to Equations 1 1.172 
and 11.173. The difference now, however, is that we must include the series solutions 
that do not terminate, the very ones we removed from the pt(6) solutions because 
they did not converge at x = 5 1. 
Fore = 0, we will define 
& o w  = A o ( 4 ,  
(1 1.197) 
where Adx) is the Frobenius series of Equation 11.172 which we previously found 
unacceptable: 
&O(X) = x + (1 /3)x3 + (1 / 3 x 5  + . * . . 
With some work this series can be expressed in closed form as 
(1 1.198) 
(1 1.199) 
Similarly for 4 = 1, define Q1 (x) as the nonterminating series, 
which in closed form can be written as 
Fore = 2: 
(1 1.201) 
(1 1.202) 
There is a generating function for the &(x) functions that is rather complicated. It 
involves the Legendre polynomials 
Notice how these solutions diverge for x = 2 1. 
Keep in mind, for the Qt(6) solutions, nothing says 4 must be an integer. The 
particular boundary conditions of the problem being solved will usually limit the 
possible values of 4 to some discrete set. Often this is a set of integers. With integer 
values for 4, the infinite series solutions for the &(x) solutions can be expressed in 
closed form and paired with the &t(x) solutions to give two independent solutions, 
as summarized in Table 1 1.1. The i)e(x) and &(x) solutions for 4 = 0, 1, and 2 are 
shown in Figures 1 1.19 and 1 1.20. 

468 
SOLUTIONS TO LAPLACE’S EQUATION 
TABLE 11.1. The i)((x) and Q&) Solutions to 
Laplace’s Equation in Cylindrical Geometry 
e 
s = 1 Solution 
s = 0 Solution 
0 
1 
2 
.., 
X 
I 
Figure 11.19 The First Four P&) Functions 
-1 
X 
I 
Figure 11.20 The First Three &(x) Functions 
11.4.4 
Solutions with +Dependence 
Now we return to the solution of Equation 1 1.161, this time allowing a +-dependence. 
Remember the separated solution to Laplace’s equation in spherical coordinates had 
the form 
(1 1.204) 
m-1 
W .  e,+) = ---F(4)P(eh 
r 
where 
-e- 1 
r 
(1 1.205) 

SPHERICAL SOLUTIONS 
and 
469 
(1 1.206) 
A substitution of x = cos 0 made P(0) = ~ ( C O S  
0), and P ( x )  solved the equation 
+4(4 + I) P(x) = 0. 
I 
m2 
(I 1.207) 
From the form of Equation 11.207, it is clear that P ( x )  depends on both rn and 4, so 
we will label the solutions of P(x) with two indices: P(x) -+ Ptm(x). 
A brute force Frobenius solution to Equation 11.207 is very messy. It is simplified, 
somewhat, by making the substitution 
(1 1.208) 
2 m / 2 0  
Pte,(x> = (1 - x 
With this, Equation 1 1.207 becomes 
Using the method of Frobenius, we will try to find a solution of the form: 
m 
Ut,(X) = C u , x n + s .  
( 1 1.2 10) 
When this is substituted into 11.209, equations for the a,, coefficients are obtained. 
For n = 0 and n = 1, the two indicial equations are 
n=O 
ao(s)(s - 1) = 0 
(1 1.21 1) 
and 
al(s + l)(s) = 0. 
( 1 1.2 12) 
For n 2 2, the generating equation for the an's is 
(n + s - 2)(n + s + 2m - 1) + m + m2 --t(-t + I) 
(n + s)(n + s - 1) 
a, = an-2 
. 
(11.213) 
There are two solutions for the indicial equations, s = 1 and s = 0. Thus we 
obtain two different solutions, 
m 
A&&) 
= c 
U,X"+l 
s = 1 
( 1 1.2 14) 
n=0,2, ... 

470 
SOLUTIONS TO LAPLACE'S EQUATION 
and 
m 
B~,,,(x) = C b,x" 
s = 0, 
(11.215) 
where, as usual, we have defined q, # 0, bo # 0, and a1 = bl = 0. The generating 
equation for the an's, for n 2 2 becomes 
n=0,2, ... 
( 1 1.21 6) 
(n - l)(n + 2rn) + m + m2 - t(t + 1) 
a,, = an-2 
(n + 
and the one for the bn's becomes 
(n - 2)(n + 2m - 1) + m + rn2 - t(4 + 1) 
b,, = bn-2 
(1 1.217) 
n(n - 1) 
As n + m, both a,, = an-2 and b, = bn-2. This means that for these solutions to be 
valid on the z-axis (where x = 2 l), the series solutions must terminate. This is the 
same argument used in the derivation of the Lengendre polynomials. 
Let's look at the termination of the s = 0 series, the one for &,,,(x). For this series 
to terminate 
(n - 2)(n + 2rn - 1) + rn + m2 - t(4 + 1) = 0. 
(1 1.21 8) 
With some rearrangement this condition for termination becomes 
(n + m - 2)(n + m - 1) = t(t + 1). 
(11.219) 
It should be noted that this simple condition for termination was what originally 
motivated the substitution of Equation 11.208. Had we not made that substitution, 
this condition would have been much more difficult to deal with! At this point, all we 
know about the terms in Equation 11.219 is that, according to Equation 11.150, m 
must be a nonnegative integer, and that n is a positive, even integer. This means that 
the first factor on the LHS of Equation 11.219, (n + rn - 2), is an integer and that 
the second factor, (n + rn - I), is equal to that same integer plus one. Therefore, if 
Equation 11.219 is to be satisfied, and the series is to terminate, 4 must be an integer. 
As was the case before, the solutions derived from considering negative values of 
4 are just repetitions of the positive t solutions. Therefore, we need only consider 
e =0,1,2 ,... . 
An interesting result of Equation 11.219 is a new condition on m. Pick some 
positive, integer value fort, say t = 4,. Termination then occurs when 
(n + m - 2)(n + m - 1) = t,(4, + 1). 
(1 1.220) 
If we set m = -to, termination can occur with n = 2. If m > 4, the LHS of 
Equation 11.220 is always greater than the RHS and termination can never occur. 
When m = 4 0  - 1, there is no way to satisfy Equation 11.220 with an even value of n. 

SPHERICAL SOLUTIONS 
471 
If we move m down another notch, and set m = 4, - 2, we can satisfy Equation 1 1.220 
again with a choice of n = 4. If we continue this process, we will find we can satisfy 
the condition in general for every other integer value of m until we reach either rn = 1 
or rn = 0, depending on whether 4, was even or odd. That is, the values of m can be 
(n = 2) 
4, - 2  
(n = 4) 
(1 1.221) 
1 or0 
(1 1.222) 
Remember this requirement is for the s = 0 series. 
we find the relationship 
If we determine, in a similar way, the termination condition for the s = 1 series, 
(n + m - l)(n + rn) = 4&?, + 1). 
The highest value that m can take on and still satisfy this equation is 4, - 1, with the 
n = 2. If m gets higher than this value, it is impossible to satisfy the relationship. 
If we try m = t, - 2, again we find we cannot satisfy Equation 11.222. But when 
we move down to m = 4,, - 3, we can satisfy it with n = 4. Much like we derived 
Equation 11.221, we find all the possible m values are given by 
4, - 1 
(n = 2) 
(n = 4) 
I 
4, - 3 
m =  { 4 , - 5  
(n = 6 )  
(1 1.223) 
where again, the lowest value of rn depends on the starting value of 4,. 
When we consider both the s = 1 and s = 0 series solutions collectively, we find an 
important fact. To have at least one valid solution, m must obey either Equation 1 1.221 
or Equation 11.223. In other words, 0 I 
m 5 4,. 
Now that we know the range of m, we need to determine what the terminating 
plm(x) solutions actually look like. We will start with the l = 0 solution. According 
to the previous discussion, with t = 0 there can only be a terminating series solution 
if rn = 0, and it must come from the s = 0 series with n = 2. Therefore, using 
Equation 1 1.208, 
PI&) 
= Uw(x) 
= Boo 
(1 1.224) 
= b,. 
The particular value of b, is usually chosen as 1. This solution is not terribly exciting, 
since it is just a constant. 

472 
SOLUTIONS TO LAPLACE'S EQUATION 
With 4 = 1, there are two possible m values: m = 1 and m = 0. The solution 
PI 1 (x) uses the s = o series with n = 2 so 
2 1 / 2 0  
+ l l ( X )  = (1 - x 1 
1 1 b )  
= (1 - X 2 ) ' / 2 B l l ( X )  
(1 1.225) 
= (1 - x2)lI2b0. 
As before, we usually set b, = 1. The Plo(x) solution uses the s = 1 series with 
n = 2, which gives 
(1 1.226) 
= aox. 
If we set 
In this way the solutions for all the terminating pem(x) solutions, valid on the 
z-axis, can be developed. The solutions are collectively referred to as the associated 
Legendre polynomials. A generating function for these polynomials, a generalization 
of Rodrigues's formula, has been developed: 
= 1, Pl, equals the &'1(x) Legendre polynomial we derived earlier. 
These functions obey the orthogonality condition 
(1 1.227) 
(1 1.228) 
which in terms of the variable 6 is 
Finally, the general solution to Laplace's equation in spherical coordinates using 
the R(r), F(+), and P(8) functions can be written down. The result is 
Example 11.6 We will now use these results to find the electric potential inside the 
spherical surface shown in Figure 11.21. Suppose that different electric potentials 
have been applied on each quadrant of the surface, as indicated in the figure. We will 
only show a general method of solution here and leave complete solutions for the 
exercises at the end of this chapter. 

SPHERICAL SOLUTIONS 
473 
+vo 0 < 0 < d 2 ,  O < @ < R  
Q(Ro,eY40= -vo 0<0<iT12, x<l$<2n: 
+Vo 
M 2 < 8 < 7 c ,  R < @ < ~ T I  
+vo M2<0<lc, O<l$<?K 
Figure 11.21 Boundary Conditions on a Spherical Surface 
Start with the general spherical solution in the form 
m
e
 
(1 1.231) 
sin(m4J) 
4=0 m=O 
We have chosen to use trigonometric functions for the +-dependence instead of 
complex exponentials, because the electrostatic potential should be a real quantity. 
Immediately, we can make some useful simplifications. All the r-'-' 
solutions 
must vanish because the potential must be finite at r = 0. In addition, the cos(rn4) 
solutions are also zero because the boundary conditions, and therefore the solution, 
have odd symmetry about 4. This reduces Equation 1 1.23 1 to 
m
e
 
D(r, o,+) = 
sin(rn+)Pt,(cos e). 
(1 1.232) 
O=O m=O 
Notice that there is a single amplitude constant Atrn, which is indexed by both 4 
and m. 
Values for this constant are determined by requiring the solution to go to the 
prescribed values at the boundary r = R,: 
A particular m value can be selected using the orthogonality of the sine functions. A 
particular 4 value can be isolated using the orthogonality of the associated Legendre 
polynomials. Using these orthogonality conditions allows a solution for Atrn: 

474 
SOLUTIONS TO LAPLACE'S EQUATION 
This integral determines the Aem, which are then inserted into Equation 11.232 for 
the solution inside the spherical surface. 
11.4.5 
Spherical Harmonics 
It is customary to combine the eim$ functions with the associated Legendre polyno- 
mials in Equation 1 1.230 to define a new function of c$ and 8, the so-called spherical 
harmonics: 
The complicated multiplying factor just normalizes the function, so that the orthog- 
onality condition, discussed shortly, is as simple as possible. The (- 1)"' factor is a 
convention used frequently in quantum mechanics. Notice, because we separated out 
the eim$ factor, m must now be allowed to vary between -4 and +4. To make this 
work, we define the associated Legendre polynomials with negative values of m to 
simply be proportional to the same polynomial with a positive m: 
(- l)"(.e - m)! . 
(4 + m)! 
Pem (x). 
Pe,-rn(X> = 
(11.236) 
The weird proportionality constant makes Equation 11.227 continue to hold for all 
values of m. 
Using the spherical harmonics, the general solution of Equation 11.230 becomes 
The spherical harmonics obey an orthogonality condition: 
[*&#J l T d 8  sin 8 zm(8, 
+)&,,f(8, 
4) = 6eedrn,. 
(1 1.238) 
Notice, since the spherical harmonics are complex functions, the orthogonality rela- 
tion involves a complex conjugate operation. 
The first few Ytm's are: 
&)(@,4> = 
xl-l(e, 
4) = *sin 
o c i +  
k'lo(8,4) = m
c
o
s
 
8 

SPHERICAL SOLUTIONS 
475 
These functions can be visualized by making three-dimensional, polar plots of 
1Gm(8, +)I2 vs. 8 and 4. The plot for &((I, 
4) is shown in Figure 11.22, where the 
magnitude squared of the spherical harmonic is represented by the distance from the 
origin to the surface. Similar surfaces are shown for the 4 = 1 and 4 = 2 spherical 
harmonics in Figures 11.23 and 11.24. In these figures, the z-axis is vertical, and the 
surfaces are always symmetric about this axis because le'"+l2 = 1. Also, the plots 
for +m are always exactly the same, because I Ye,,, I = I Ye-m I. 
The relevance of these spherical harmonics to the quantum mechanical wave 
functions for the hydrogen atom should be clear to anyone with a chemistry or 
physics background. It is important to realize that the restriction on m, i.e., that 
Iml 5 4, is a mathematical result, originating from our requirement that the solutions 
converge for all values of 8. 
Figure 11.22 Surface Representing &,(& 4) 

476 
SOLUTIONS TO LAPLACE’S EQUATION 
EXERCISES FOR CHAPTER 11 
1. Find a series solution for the electric potential inside a two-dimensional square 
box, with sides held at the potentials shown below: 
I 
I 
L 
@=+V, 
Q, = +V, 
2. Find the two-dimensional solution to Laplace’s equation inside a rectangular box 
of length Q and height b, if the boundary conditions for the solution are as shown 
in the figure below: 
2 
~ 
b
1
 
0 
1 
a 
3. Find the solution to Laplace’s equation inside a 1 X 1, two-dimensional square 
region, if the solution on the boundary is as described in the figure below: 

EXERCISES 
477 
Y 
@ = Y  
X 
4. Consider the L X 3L rectangular box shown below. 
01 L 
0 
+1 
0 
___ 3L 
0 
The electrostatic potential on the boundaries of this box is indicated in this figure. 
The end caps of the box are held at 4 = 0 zero, while the central section has 
Q, = 1. What is the potential everywhere inside the box? 
5. Find a series solution for the potential inside the two-dimensional rectangular 
box, with sides held at t- V,, as shown in the figure below: 
Q, = -Vo 
Lo Q,=+vo 
Q, = +Vo 
a, = -Vo 
2L0 
~ 
6. Find @(x, y ) ,  the solution to Laplace’s equation in two dimensions between the 
two parallel planes y = 0 and y = a,. Assume @(x, 0) = 0, and that @(x, a,) is 
periodic with period 2b,. Over one period, this function is given by 

478 
SOLUTIONS TO LAPLACE’S EQUATION 
+1 
O < x < b o  
-1 
bo < x < 2b0 ’ 
and is periodic with period 2b0 for all x .  
7. Find the two-dimensional solutions to Laplace’s equation inside the two isosceles 
right triangles, with the boundary conditions shown in the figures below: 
8. Consider the following two-dimensional boundary conditions and the solution 
to Laplace’s equation inside this boundary. 
(a) Set up the periodic boundary conditions that will generate a solution to 
Laplace’s equation inside this rectangular region and indicate the region 
of this periodic problem where its solution is equal to the solution of this 
rectangular problem. 
(b) Obtain a solution to Laplace’s equation inside the rectangular region that 
satisfies the above boundary conditions. 
(c) Repeat the above steps for the boundary conditions shown in the following 
figure. 
0 
VO 
0 

EXERCISES 
479 
9. For this problem, you are to develop the Green's function for a square drum head 
lying in the xy-plane, as shown below. The boundaries at x = 0, L and y = 0, L 
are held at h = 0. 
The displacement of the vibrating drum head h(x, y)eiW' is governed by the 
partial differential equation 
where d(x, y)ei"" is a driving function. The Green's function for this problem 
therefore satisfies 
(a) As the first step in finding the Green's function, turn this problem into one 
that is infinite in the x- and y-directions. Indicate the locations and nature of 
the driving &functions for a periodic problem whose solution is identical to 
the drum head problem in the region 0 < x < L and 0 < y < L. 
(b) Find a series solution for &I&, ylty). 
10. The potential between two concentric, electrically conducting cylindrical sur- 
faces is a simple electrostatic problem. If the inside cylinder of radius rl is held 
at a potential V 1  , and the outer cylinder at r2 is held at V2, the potential between 
the surfaces is 
Verify this expression and show how it is given by the general cylindrical solution 
presented in this chapter. 
11. Consider the two-dimensional polar form of Laplace's equation 
"'1 
[apz 
p a p  
p2ae2 
a* 
1 d 
- 
+ -- + -- u(p,e) = o 
and the separated solutions of the form u(p, 0) = R(p)H(B). 

480 
SOLUTIONS TO LAPLACE’S EQUATION 
(a) What are solutions for H(8)? 
(b) Determine the two independent solutions for R(p). 
(c) Using the results from parts (a) and (b), determine the two-dimensional 
solution to Laplace’s equation inside a circle of radius r,, if on the circle, 
the solution goes to u(ro, 0) = V, sin 8. What is the potential outside this 
circle, for p > r,? What assumptions have to be made to get the answer for 
p > r,? 
12. Find the solution to Laplace’s equation inside a cylinder of radius r, and length 
Lo with the boundary conditions shown in the figure below: 
VO 
~ 
g = 0  
$= -V0 
Now find the solution inside the same cylinder with the boundary conditions 
defined as shown in the figure below: 
L o /  $= VO B 
g = 0  
13. Find the solution to Laplace’s equation inside a cylinder of radius r, and length 
Lo with the following boundary conditions: The sides and bottom are held at 
@ = 0 and the top is held at @ = V,(1 - p/r,), where p = 0 is the axis of the 
cylinder. Use a reference such as Abramowitz and Stegun to at least make a stab 
at the integrals. 

EXERCISES 
481 
14. Find the solution to Laplace’s equation inside the hollow cylindrical region of 
length L between rl and 12, if the solution is to go to zero on the top and bottom 
and to + V, and - V, on the outer and inner surfaces, as shown below: 
cf, (rl) = +V, 
cf, (r2) = -Vo 
@ = O  
@ = O  
To solve this problem you must construct a new eigenfunction Mo(kp), which 
is a linear combination of Z&p) 
and K ~ ( k p ) ,  such that Mo(kr1) = +1 and 
15. Find the solution to Laplace’s equation for the same cylindrical region as de- 
scribed in the previous problem, but with the different boundary conditions shown 
below: 
Mo(kr-2) = - 1. 
Q (rl) = 0 
(r2) = 0 

482 
SOLUTIONS TO LAPLACE’S EQUATION 
16. For small, static displacements, the deflection of a drum head satisfies the fol- 
lowing two-dimensional differential equation in polar coordinates 
TV~Z(P, 
8) = F(P, e), 
where T is the tension in the drum membrane, z is the displacement, and F is the 
force per unit area applied to the membrane. Let the drum head have a radius r,, 
as shown in the figure below. 
Take the applied force per unit area to be independent of 8, so that the differential 
equation becomes 
(a) Determine the Green’s function g(p15) that allows the displacement to be 
written as 
(b) Plot APIO vs p. 
(c) If F(p) = 1 - p2/r:, 
set up the Green’s function integration for z(p). 
17. This problem considers the two-dimensional solution to Laplace’s equation 
in cylindrical coordinates from a Green’s function point of view. For no z- 
dependence, find the Green’s function g(p, 81 8,) that allows the solution 
2?r 
wp. 0) = 
do, .m,)g(p, m,). 
The function f ( 8 )  = @(r,, 0) defines the boundary condition at p = r,. In- 
side this boundary @(p, 0) satisfies a two-dimensional Laplace’s equation in 
cylindrical coordinates 

EXERCISES 
483 
18. 
19. 
20. 
21. 
The standard way to solve this problem sets @(p, 0) up as a series which must 
be shown to converge. 
Using similar steps to those in the previous problem, find the Green’s function 
for the two-dimensional homogeneous equation 
V ~ U ( ~ ,  
e) + k2u(p, e) = o 
inside a circle of radius p = R,, with the boundary condition that u(R,, 0) = 
.Lo(@>. 
The general form for the solution to Laplace’s equation in cylindrical coordinates 
that is not periodic in z was expressed in this chapter using a shorthand notation 
that did not include the amplitude constants: 
J,(kp) 
sin (ve) 
sinh (kz) 
@(” e’z) = { Y,(kp) { cos(v0) { cosh(kz) ‘ 
If this expression is expanded out, eight amplitude constants need to be intro- 
duced. If a solution is sought for p < r,, the Y,,(kp) solutions can be eliminated 
because they diverge as p + 0. Therefore, the number of amplitude constants is 
reduced to four. If, however we exclude the origin from the domain of the solu- 
tion, i.e., if we limit the problem to a range u < p < b, both the J,(kp) and Y,(kp) 
functions are necessary and all eight amplitude constants must be considered. 
The amplitude constants are not the only complication for this a < p < b 
situation, because the spatial factor k is also determined by boundary conditions. 
Consider a solution valid for u < p < b that is zero at p = a and p = b. 
To solve this problem, construct a new eigenfunction from the linear sum of the 
Bessel functions. Call this new eigenfunction R,(kp) where 
R,(kp) = A J,(kp) + B Y,(kp). 
Discuss how you would determine the set of values for k and the linear scaling 
factors A and B, so that this new eigenfunction is normalized and satisfies the 
zero boundary conditions at p = a and p = b. 
The potential between two concentric, electrically conducting spherical surfaces 
is a simple electrostatic problem. If the inside sphere of radius rl is held at a 
potential V1, and the outer sphere at r2 is held at V2, the potential between the 
surfaces is 
Verify this expression and show how it is given by the general spherical solution 
presented in this chapter. 
Find the potential between two concentric spherical surfaces of radius R, and 
2R,. The voltage on the upper hemisphere of the outer surface is held at + V,, 

484 
SOLUTIONS TO LAPLACE’S EQUATION 
and the voltage on the lower hemisphere of the outer surface is held at - V,. The 
voltage on the upper hemisphere of the inner surface is held at -V,, and the 
voltage on the lower hemisphere of the inner surface is held at + . V,. 
22. A hemispherical shell of radius Ro, held at @ = +V, is positioned on top of 
an infinite conducting ground plane held at @ = 0. Use symmetry, in this case 
sometimes referred to as the method of images, to find the solution to Laplace’s 
equation both inside and outside the hemisphere that satisfies these boundary 
conditions. 
23. Find a series solution for the potential inside a spherical shell whose Hemispheres 
are held at +2V, and -V, as shown below. 
Can you think of an easier way to solve this problem, using the result of the 
previous exercise? 
24. Helmholtz’s equation is written as 
V2 *@) + k2 W(F) = 0 
where k2 is a positive constant. Show that in spherical coordinates this equation 
separates and the differential equation the radially dependent component satisfies 
becomes 
where n is an integer. 
(a) Compare this to Bessel’s equation. 
(b) Show that R,(r) for different values of n can be combined with the operator 
and appropriate boundary conditions to satisfy the Hermitian condition. 
Identlfy those boundary conditions and the space over which the integration 
must occur to establish the orthogonality condition for these functions. 
(c) Show that the substitution 
gives a form of Bessel’s equation, with Zn(kr) being the Bessel function of 
order n + 1 /2. The functions R,(kr) are called Spherical Bessel Functions. 

EXERCISES 
485 
25. The quantum mechanical operators L+ and L- are given by 
and 
The spherical harmonics Ye,(B, 4) are not eigenfunctions of these operators. 
However, interesting things happen when these operators work on the Yxm(8, 4). 
Show that 
and 
Because of this property, L+ and L- are called raising and lowering operators. 
26. Find the potential between 60" < 8 < 120" of two infinite 60" cones, one held 
at +2V0 and the other at +V,, as shown below: 
27. Show that the eigenvalues of an Hennitian matrix are pure real quantities and 
that the eigenvectors are orthogonal. 

486 
SOLUTIONS TO LAPLACE'S EQUATION 
28. Because of their orthogonality and completeness properties the Legendre poly- 
nomials can be used to make series expansions of functions 
m 
f(x) = c CePe(4. 
e=o.l, ... 
Let f ( x )  be the triangular function over the range - 1 < x < 1 shown below: 
-1 
1 
(a) What are the first three coefficients of the Legendre polynomial expansion 
(b) What does the Legendre expansion for this function look like for 1x1 > l? 
(c) Consider a new function which is a similar triangular function, except now it 
extends from x = -2 to x = 2. Construct a Legendre polynomial expansion 
for this function and identify its first three coefficients. 
(a) If now f ( x )  = 1 - x2 over the interval - 1 < x < + 1, what are the first 
three coefficients of its Legendre polynomial expansion? 
29. The Legendre polynomials are determined to within arbitrary constants by the 
of this function? 
homogeneous differential equations they satisfy: 
Po@) = A0 
P,(x) = Alx 
P ~ ( x )  
= A2(l - 3x2) 
Determine the constants Ao. Al, and A2 for the first three Legendre polynomials, 
so that the functions obey the orthogonality condition 
30. The following differential equation is set up in eigenvalueJeigenfunction form, 

EXERCISES 
487 
where 8 ranges from zero to T, 
and Ph(8) is an eigenfunction of the differential 
operator with the eigenvalue A. This operator is not Hermitian. 
(a) Place this differential equation in Sturm-Liouville form and show that the 
(b) Using this new Hermitian operator show that if m # n 
operator that results is Hermitian. 
JG" de g ( w , w , ( o )  = 0 
and determine the weighting function g(8). 
31. Consider the differential equation 
where a is a positive constant. This equation is in eigenfunction, eigenvalue 
form. 
(a) If A, is assumed to be positive and real, show that the two solutions for the 
eigenfunctions become 
and that this form is valid for any value of A, f a2/4. 
(b) Now require that the eigenfunctions of part (a) satisfy the following boundary 
conditions: 4,(0) = 0 and +&) = 0. These boundary conditions determine 
the acceptable values for the A,, 
as well as the acceptable +&). Use the 
n-index to identify these acceptable values for the A, and make a plot of the 
first three acceptable eigenfunctions. 
(c) The differential equation given above is not in Sturm-Liouville form. Find 
the appropriate multiplying function and place it in S-L form. 
(d) Determine the orthogonality conditions for the eigenfunctions of part (a) 
that satisfy the boundary conditions of part (b). Be sure to identify the region 
over which the functions are orthogonal and explain how you arrived at your 
answer. 
32. The Gram-Schmidt orthogonalization procedure generates a set of orthonormal 
functions (or vectors) from a set of independent functions (or vectors). For this 
problem start with the set of independent functions y,(x) = x". A number of 
different orthonormal sets can be constructed from these y,(x), depending on 
the weighting function used. Let +&) be the set of functions generated from 
different sums of the y,(x), and let the orthonormalization be given by the 

488 
SOLUTIONS TO LAPLACE'S EQUATION 
33. 
following integration 
la 
dx +n(x)+m(x)e-x = snm, 
where a weighting function w(x) = e-x has been used. 
(a) The first function &(x) is determined by setting it equal to a constant times 
y~(x) and then normalizing: 
What is +&)? 
(b) The second function C$~(X) is set up as a linear sum of yl(x) and &(x): 
Find the constants al and blo that normalize 41 (x) and simultaneously make 
it orthogonal to &(x). 
(c) Continue this process and construct &(x). Set up &(x) as a linear sum of 
Y2(X), b ( x h  and 41w: 
h ( x )  = a22 Y 2 G )  + b20 b ( x )  + b 2 1  41(x). 
Find the constants a22, b20, and 
onal to &(x) and 41(x). 
that normalize &(x) and make it orthog- 
These (pm(x) functions are known as Leguerre polynomials. It is interesting to note 
the Legendre polynomials can be generated by the Gram-Schmidt process using 
the same set of independent functions yn(x) = xn and the orthonormalization 
given by 
The elliptical coordinates (u. u, z) are related to a set of Cartesian coordinates by 
the equations 
x = pocoshucosv 
y = p*sinhusinu 
z = z, 
where po is a positive constant needed to make the above equations dimensionally 
correct. Laplace's equation for U(u, v,z) becomes 

EXERCISES 
489 
Assume U(u, u, z) can be separated, i.e., that *(u, u, z) = f(u)g(v)h(z), and 
determine the three ordinary differential equations that f(u), g(v), and h(z) 
satisfy. Define the necessary separation constants. 
34. Consider a two-dimensional Cartesian coordinate system and a two-dimensional 
uv-system with the coordinates related by 
x = -uv 
y = (1/2)(u2 - 2). 
In general, Laplace's equation in two dimensions can be written as 
with 
h? = (g)2 
+ ( $ ) 2 .  
(a) In the xy-plane, sketch lines of constant u and constant v .  
(b) Express Laplace's equation using the uv-coordinates. 
(c) Use the method of separation of variables to separate Laplace's equation in 
35. Let n(y , t )  represent the number density (the number of particles per unit volume) 
of neutrons in a slab of nuclear reactor material. Take the slab to be infinite in 
the x and z-directions, so that there is no variation in the number density with 
x or z. The motion of the neutrons is described by a diffusion coefficient D. 
The neutrons can also collide with atoms in the slab, creating unstable isotopes. 
When an unstable isotope decays, more neutrons are released than were needed 
to make the atom's nucleus unstable. This results in a source of neutrons that is 
accounted for by a collision frequency v. Both D and v are positive, real numbers. 
The diffusion and source effects can be combined to give a partial differential 
equation that describes the evolution of n(y, t): 
the uv-system and obtain the general solution for "(u, v). 

490 
SOLUTIONS TO LAPLACE’S EQUATION 
At y = 0 and y = L, the neutrons can easily leave the slab, so the number 
density is always zero at these boundaries. 
(a) Using the method of separation of variables, obtain a series solution for 
n(y, t) that obeys the above boundary conditions and is valid for an arbitrary 
initial condition, i.e., n(y, 0) = no(y). 
(b) Using your answer to part (a), evaluate the series coefficients for the initial 
condition n,(y) = NoS(y - L/2). 
(c) The solution for an arbitrary initial condition can be formulated as a Green’s 
function integral. What is this Green’s function, g(y(6, t)? Express the solu- 
tion n(y, t) in terms of a Green’s function integral and show that n(y, t) goes 
to your answer to part (b) if n,(y) -+ &S(y - L/2). 
(d) Using your solution to part (a), determine the thickness L of the slab, which 
results in a nontrivial, steady-state solution for n(y, t) as t -+ 
00. 
(e) What are the consequences of having a slab thickness greater than the value 
you calculated in part (d)? 

12 
INTEGRAL EQUATIONS 
Differential equations have unknown functions or dependent variables inside differ- 
ential operators. In contrast, integral equations have unknown functions or dependent 
variables inside integral operations. For example, 
(12.1) 
is a typical differential equation, with one dependent variable 4. This differential 
equation can be easily converted to an integral equation using a Green’s function 
argument. The Green’s function solution to Equation 12.1 takes the form 
where g(x15) satisfies 
(12.2) 
(12.3) 
Equation 12.2 is an integral equation because the dependent variable 4 appears inside 
the integral operation, and there are no differential operations. 
In this chapter, we discuss four general classes of integral equations, the connec- 
tion between differential and integral equations and several methods of solution. In 
Appendix G, a briefsummary of thecalculus of variations is given. This is a technique 
that uses the structure of an integral equation and minimizes the value of a definite 
integral with respect to an initially unknown function embedded inside the integral 
itself. 
491 

492 
INTEGRAL EQUATIONS 
12.1 CLASSIFICATION OF LINEAR INTEGRAL EQUATIONS 
The integral equations discussed in this chapter are all linear in the dependent variable. 
They fall into four categories: Fredholm equations of the first and second kind, and 
Volterra equations of the first and second kind. These classifications are important 
because the approaches to solving these equations are different in each case. 
12.1.1 Fredholm Equations of the First Kind 
In a Fredholm equation of the first kind, the dependent variable 
the integral operation, and the limits of integration are constants. For example, 
exists only inside 
f ( x )  = lbdx‘I#I(x’)k(x,x’) 
(12.4) 
is a Fredholm equation of the first kind. Both f ( x )  and k(x, x’) are known functions 
of the independent variable x. The function k(x, x’) is referred to as a “kernel.” 
12.12 Fredholm Equations of the Second Kind 
Fredholm equations of the second kind differ from Fredholm equations of the first 
kind in that the dependent variable appears both inside and outside the integral 
operation. For example, 
b 
4 ( x )  = f ( x )  + 
dx‘cP(x’)k(x,x’) 
(12.5) 
is a Fredholm equation of the second kind. Again, I#I(x) is the unknown function, but 
both f ( x )  and the kernel k(x, x’) are known. The limits of the integral, a and b, and h 
are all constants. 
12.1.3 Volterra F.cluations of the First Kind 
Volterra equations of the first kind are similar to Fredholm equations of the first kind, 
except the limits of integration are not constants. The unknown function still appears 
only inside the integral. Therefore, 
f ( x )  = LX 
dx’ I#I(x’)k(x, x’) 
(12.6) 
is an example of a Volterra equation of the first kind, because the upper limit of the 
integral is the variable x. 
12.1.4 Volterra Equations of the Second Kind 
Finally, a Volterra equation of the second kind has the unknown function both inside 
and outside the integral operation, and the limits of the integral are not constants. The 

rHE CONNECTION BETWEEN DIFFERENTIAL AND INTEGRAL EQUATIONS 
493 
:quation 
(12.7) 
is an example of a Volterra equation of the second kind. 
12.2 THE CONNECTION BETWEEN DJFFERENTIAL 
AND INTEGRAL EQUATIONS 
Differential equations can be converted to integral equations. In this process, the 
boundary conditions associated with the differential equation are automatically in- 
corporated into the integral equation. Thus, unlike differential equations, integral 
equations will have unique solutions without specifying additional conditions. 
Let’s see how a ordinary second-order, linear differential equation can be converted 
to an integral equation. We start with the most general form 
+ A ( x ) S  + B(x)y(x) = s(x), 
d 2 y W  
dx2 
dx 
(12.8) 
where the functions A(x), B(x) and s(x) are presumed to be known. Because this is 
a second-order equation, two boundary conditions are required to make the solution 
y(x) unique. For this case, we will specify y(x) and its derivative at the point x = a: 
Y(a) = Yo 
(12.9) 
and 
(12.10) 
To convert Equation 12.8 into integral form, all the differential operations must 
be removed by successive integrations. The first such integration is accomplished by 
changing the variable in Equation 12.8 from x to x’, and then operating on both sides 
with 
[ 
dx‘. 
(12.11) 
After this integration, Equation 12.8 becomes 
(12.12) 
+ lx 
dx’B(x’)y(x’) = 1’ dx’s(x’). 

INTEGRAL EQUATIONS 
494 
Applying the boundary condition of Equation 12.10 and integrating the first integral 
on the LHS by parts gives 
dA(x‘) 
dx‘ - 
Y (XI) 
- 
dY (X) - y: + A(x’)y(x‘)l’ - 1 
dx 
a
a
 
dx‘ 
+ [ 
dx’B(x’)y(x’) = [ 
dx’s(x’). 
(12.13) 
Using the other boundary condition given in Equation 12.9, this equation can be 
rewritten as 
-- 
dy(x) 
y: + A(x)y(x) - A(u)yo 
dx 
+ [ d x ’ b ’ )  - - 
y(x‘) = [ 
dx’s(x’). 
(12.14) 
dx’ 
One more integration is necessary to remove the derivative in the first term. To do 
this, the variable x is changed to x‘‘ and the equation is operated on by 
dx”. 
(12.15) 
The dummy integration variable is labeled x’‘ to avoid confusion with x‘, the dummy 
variable of the first integration. This integration, and additional application of the 
boundary conditions, transforms Equation 12.13 to 
Y(X9 
1 
+ l’ 
dx“ l’” 
dx’ b ( x ‘ )  - dA(x’) 
7 
= Lx 
dx” L’” 
dx’ s(x’). 
Using the double integral identity 
which is proven in Appendix C, this equation becomes 
(1 2.16) 
(12.17) 
dx’(x - x’) [..’I 
- y] 
y(x‘) = 1 
dx’(x - x’)s(x’). (12.18) 
+I 

THE CONNECTION BETWEEN DIFFERENTIAL AND INTEGRAL EQUATIONS 
495 
This equation can be rearranged as 
y(x) = v, + yL(x - U )  + A(u)y,(x - U )  + 
~ x ’ ( x  - x/)s(x’) 
1 
which is in the form of a Volterra equation of the second kind, like Equation 12.7. 
The unknown function is 
the known function is 
and the kernel is 
k(x,x’) = (x - x’) [ - 
’:$’) 
- B(x’)] - A(x’). 
(1 2.22) 
The solution to Equation 12.19 is identical to the solution of Equation 12.8, 
when the boundary conditions given by Equations 12.9 and 12.10 are applied. To 
work with the differential equation, the boundary conditions must be specified in 
addition to the differential equation itself. With this integral equation, however, 
no additional information is necessary to arrive at the same unique solution. The 
boundary conditions are built in. 
Example 12.1 
an integral equation, consider the differential equation for a harmonic oscillator, 
As a specific example of the conversion of a differential equation to 
d2y(x) + k,2y(x) = 0, 
dx2 
(12.23) 
with the boundary conditions 
y(0) = y(1) = 0. 
(12.24) 
Notice we cannot use the general result derived in the last section, because the form of 
the boundary conditions is different in this case. Here we have specified the unknown 
function’s value at two end points. Before, we specified the function and its derivative 
at a single point. 

4% 
INTEGRAL EQUATIONS 
As we did before, we will change x --f x’ and then apply the operator 
1 
dx‘ 
to both sides of Equation 12.23 to give 
(12.25) 
The (dy/dx)lo term on the LHS is a constant, but it is not one of the boundary 
conditions. We label it as the constant y& but remember that it is not yet known. We 
integrate again, by transforming x -+ x“, and applying the operator 
[ 
dx“ 
to both sides to obtain 
(12.27) 
The quantity y(0) is one of the boundary conditions. Setting y(0) = 0 and using the 
double integral identity of Equation 12.17 gives 
(12.29) 
We are not finished yet, because the constant yh is not known. Also, we have not 
made use of the second boundary condition at x = 1. These two things are taken care 
of at once by evaluating Equation 12.29 at x = 1: 
I 
y ( l )  - y; + k q  
dx‘(1 - x’)y(x’) = 0. 
(12.30) 
0 
Because y( 1) = 0, this equation allows y: to be evaluated as 
1 
y; = k,’ 1 dX’(1 - x‘)y(x’). 
(12.31) 
Equation 12.29 becomes 
1 
y(x) = kzx h dx’(1 - x’)y(x’) - k,’ 
(12.32) 
This is clearly an integral equation. All the differential operators have been removed, 
and both the boundary conditions have been incorporated. The unknown function 
appears both inside and outside the integral operations, but one integral has constant 

THE CONNECTION BETWEEN DIFFERENTIAL AND INTEGRAL EQUATIONS 
497 
limits and the other has the independent variable as an upper limit. This appears to 
be a mixture of a Fredholm equation of the second kind and a Volterra equation of 
the second kind. A bit more work clears up this confusion. 
The integration from x’ = 0 to x‘ = 1 can be broken up into two parts. Because 
the independent variable x ranges from 0 to 1, Equation 12.32 can be rewritten as 
y ( x )  = kz 1’ dx’x(l - x’)y(x’) + ki 
dx’x(1 - x’)y(x’) 
6’ 
-kz lx 
dx’(x - x’)y(x’). 
(12.33) 
The two integrals from x’ = 0 to x’ = x can be combined: 
y ( x )  = k: 1‘ dx’x’( 1 - x)y(x’) + k,” 
dx’x( 1 - x’)y(x’). 
(12.34) 
I’ 
Equation 12.34 is now in the form of a Fredholm equation of the second kind: 
y(x) = k,’ / 
~ dx’k(x, x’)y(x’), 
J o  
with 
x’(1 - x )  
O I x ’ 5 x  
x(l - x’) 
x 5 x’ 5 1 . 
k(x,x’) = 
(12.35) 
(12.36) 
Notice that the independent variable x is a constant inside the integral operation, 
while the x’ is the variable of integration. 
The kernel for this problem is plotted in Figure 12.1. This kernel should look 
familiar to you. The differential equation for the string problem of the last chapter 
was solved using a Green’s function, which was essentially identical to the kernel 
described by Equation 12.36. The close link between this integral equation and the 
Green’s function solution isn’t too surprising when you consider Equation 12.23 from 
a Green’s function point of view. Rewrite Equation 12.23 as 
(12.37) 
X 
1 
Figure 12.1 Kernel for the Harmonic Problem 

498 
INTEGRAL EQUATIONS 
If the RHS of this equation is looked at as the drive, then the Green’s function solution 
for y(x) can be written as 
where 
operator. That is, g(x15) is the solution of 
is the Green’s function associated with the d2y(x)/dx2 differential 
(12.39) 
with the same homogeneous boundary conditions as y(x) 
g(0lS) = g(115) = 0. 
(12.40) 
This Green’s function solution, Equation 12.38, is in the form of an integral equation 
for y(x). It is identical to the integral equation we obtained by converting the original 
differential equation, Equation 12.37, to an integral equation with 
g(xlx’) = -k(x,x’). 
(12.41) 
12.3 METHODS OF SOLUTION 
In this section, several methods of solution for integral equations are presented. The 
different forms of integral equations generally require different solution techniques. 
For example, one takes a different approach for Volterra equations of the first kind 
than for Fredholm equations of the second kind. 
In these discussions, it is useful to define two classifications of kernels, which 
we dub “translationally invariant” and “causal” kernels. A translationally invariant 
kernel has the form 
k(x,x’) = k(x - x’). 
(12.42) 
The motivation for this terminology is fairly evident, when you consider the con- 
nection with the Green’s function kernels we discussed in the previous section. For 
a translationally invariant kernel, if the drive is displaced by a certain amount, the 
response will not change its character, but will just be displaced by the same amount. 
A causal kernel obeys the condition 
k(x,x’) = 0 
x < X I ,  
(12.43) 
which again using the Green’s function analogy, means that a response does not occur 
before the drive. Several of our solution techniques require kernels which obey one 
or both of these conditions. 

METHODS OF SOLUTION 
499 
12.3.1 Fourier Transform Solutions 
Recall, the standard Fourier transform operation is defined by the pair of expressions 
F(w) = - 
sp_ dt e-'"f (t) 
(12.44) 
Jm d o  ei"E(w). 
(12.45) 
A Fourier transform approach works well for finding solutions to Fredholm equations 
of the first kind, which have both translationally invariant kernels and infinite limits 
of integration: 
f ( t )  = Jz.r 
(1 2.46) 
In this expression, f ( t )  and k(t - 7) are known functions, and y(t) is unknown. 
Assume that the Fourier transform of each of these functions exists: 
(12.47) 
(12.48) 
(12.49) 
Notice, because the kernel is translationally invariant, we can describe its argument 
using only a single variable. 
Because Equation 12.46 is a convolution of k(t) and y(t), its Fourier transform is 
easy to evaluate: 
Solving for r( 
w )  gives 
The solution of y ( t )  can now be obtained with a Fourier inversion: 
12.3.2 Laplace Transform Solutions 
(12.51) 
(12.52) 
The Laplace transform approach works for solving Volterra equations of the first kind 
with causal, translationally invariant kernels and integration limits that range from 

500 
INTEGRAL EQUATIONS 
7 = 0 t0 7 = f :  
f ( t )  = p
k
(
t
 - 7)y(7). 
(12.53) 
Again, f ( t )  and the kernel are known functions, while y(t) is the unknown. We assume 
f ( f ) ,  y(t), and k(t) are all zero for t < 0 and possess the Laplace transforms 
(12.54) 
(12.55) 
(12.56) 
Recall that the standard Laplace operation is defined by the equation pair 
r m  
E(sJ = /o 
dte-”’f(t) 
(12.57) 
(12.58) 
with the Laplace contour L to the right of all the poles of Fb). 
As it stands now, the integral in Equation 12.53 is not a convolution integral 
because the upper limit of the integration is t rather than infinity. This can easily 
be corrected, since we assumed a causal kernel. Because k(t - 7) = 0 for t < T, 
we can extend the upper limit of the integral to +w without changing the value of 
the integral. Because y ( ~ )  
is zero for T < 0 the lower limit can be extended to -w 
without changing the value: 
f(t) = l d T k ( f  - ‘T)Y(T) = 
dTk(t - 7)y(7). 
(12.59) 
The integral equation is now in the form of a standard convolution and applying 
1: 
the Laplace transform operation to both sides gives 
Solving for Y (g) gives 
The solution for y(t) is obtained with a Laplace inversion: 
(12.61) 
(12.62) 
In this expression, the Laplace contour must be to the right of all the poles of the 
quantity EWKOl. 

METHODS OF SOLUTION 
501 
12.3.3 Series Solutions 
Fredholm equations of the second kind can be solved by developing what is known as 
a Neumann series solution. In quantum mechanics, this process is called perturbation 
theory. The general form of a Fredholm equation of the second kind is 
y(x) = f(x) + A 
dx’k(x,x’)y(x’), 
Ib 
(12.63) 
where f ( x )  and k(x, x’) are known functions, A is a constant, and y(x) is the unknown. 
The constant A is assumed to be “small” in some sense. To start, we will assume 
that A is small enough that 
(12.64) 
for all a < x < b. This really does not pin down the magnitude of A very well, because 
the integral on the RHS of Equation 12.64 cannot be evaluated without knowing y(x) 
in advance. Notice how f ( x )  cannot be zero anywhere. If it were, Equation 12.64 
could never be satisfied, regardless of the magnitude of A. Colloquially, we describe 
this requirement by saying the method requires a nonzero “seed’ function. 
If Equation 12.64 is satisfied, an iterative approach can be employed to obtain a 
solution for y(x) to arbitrary orders of A. As a first approximation, the solution can 
be given by 
We call this the zerorh-order solution because A does not appear in this expression. An 
improvement on this approximate solution is made if we substitute the zerorh-order 
solution into the integral of Equation 12.63 and then solve for a new value of y(x): 
(12.66) 
We call y1 ( x )  the first-order solution, because it has a term with A raised to the first 
power. Now you probably see what we are doing. To get the second-order solution, 
we substitute y1 (x) back into Equation 12.63 and solve for a new y(x): 
YZ(X) = f(x) + A 
dx’k(x,x’)yl(x’) 
I” 
b 
= f ( x )  + A 
dx’k(x,x‘)f(x’) 
(12.67) 
b 
+A2 lb 
dx’k(x, x’) .I d ~ ” k ( x ’ ,  
x”)f(x”). 

INTEGRAL EQUAnONS 
502 
And of course, you guessed it, this is called a second-order solution because the 
highest power of A that appears is A2. This process can be canied to any order. The 
@-order solution is 
(12.68) 
+ h2 Lb d x r l b  dx“ k(x, x‘)k(x‘, x“) f (x“) 
In this expression x”‘ is used to indicate an x with n primes. 
Now our hope is that 
y(x) = lim yn(x). 
(12.69) 
That is, if we keep performing the iterative process of substituting our approximate 
result for y(x) back into the equation, we will eventually converge on the correct 
solution. The standard convergence tests should be applied to check that the resulting 
infinite sum converges. Obviously, A must be small enough for this to occur. 
n-m 
Example 12.2 Most engineering and physics problems formulate naturally as dif- 
ferential equations as opposed to integral equations. That is why more emphasis, in 
this book and in others, is placed on solutions to differential equations rather than 
integral equations. There are some problems, however, that do formulate naturally as 
integral equations. The quantum mechanical scattering problem is one. 
Consider an electron incident on a scattering center, such as the hydrogen nucleus 
sketched in Figure 12.2. The scattering center is modeled by an electrostatic potential 
V(i) that is a scalar function of position. The electron is modeled by a wave function 
$(F), and its interaction with the potential is described by the Schrodinger wave 
WA- - 
e- 
Figure 12.2 The Quantum Mechanical Scattering Problem 

503 
METHODS OF SOLUTION 
equation: 
(12.70) 
Here E is the energy of the electron and m is its mass. If we define the constant 
k2 = 2mE/h2, Equation 12.70 can be rewritten as 
2m 
h2 
(V2 + k2) +(F) = -V(F)+(i-). 
(12.71) 
This last equation is a linear differential equation with +(I;) as the dependent variable. 
A Green's function approach to solving this equation gives the result 
(1 2.72) 
The first term on the RHS is the solution for +(F) when V(F) is zero, the homogeneous 
version of Equation 12.71. The second term is an integral over all space involving 
the Green's function g(F1i'). The Green's function itself is a solution to 
(V2 + k2) g(FIF') = S3(F - P). 
(12.73) 
It will be assumed that the Green's function vanishes at infinity. Applying this bound- 
ary condition gives 
(12.74) 
If $(F) were not sitting inside the Green's function integral of Equation 12.72, we 
would already have our desired solution. But since it does appear both inside and 
outside of the integral, we resort to the Neumann series method described earlier. The 
potential V(F) is assumed to be a small perturbation to the problem. The zero-order 
solution is just the wave function for a free electron 
+o(f) = eik.r 
(12.75) 
We obtain the first-order solution by forcing this solution back into Equation 12.72: 
(12.76) 
This solution is referred to as the first Born approximation. Higher-order approxirna- 
tions follow the Neumann series solution method. 
You may have noticed something a little odd about this example. In the derivation 
of the Neumann series, the constant A needed to be small enough for the series 
to converge. In this example, however, the constant A is nowhere to be seen. The 
convergence of this solution is controlled by the magnitude of V(i-). 

504 
INTEGRAL EQUATIONS 
12.3.4 Separable Kernel Solutions 
Fredholm equations of the second kind can be solved by the method of separable 
kernels, if the kernel can be made equal to the sum of several products of two 
functions. In mathematical terms, the kernel is separable when 
N 
k(x,x’) = c 
Mj(X)Nj(X’). 
(12.77) 
Notice M,(x) and Nj(x’) are functions of only one variable. This separation can be 
accomplished for a surprisingly large number of kernels, even, as we shall see, a 
translationally invariant one. 
If the kernel separates, we can write a Fredholm equation of the second kind as 
y(x) = f ( x )  + h 
dx‘ cMj(x)Nj(x’)y(x’). 
(12.78) 
I” 
j:l 
In this equation, f ( x )  and the Mj(x) and Nj(x) functions are known, and h is a 
constant. Interchanging the order of integration and summation gives 
Now this integration has no x-dependence. If we define 
b 
cj = 
dX”j(X’)Y(X’), 
(12.79) 
(12.80) 
Equation 12.78 can be rewritten as 
Keep in mind, the constants Cj are still unknowns because y(x) is unknown. 
Equation 12.81 is in a mixed form. The unknown elements are both the explicit 
y(x) and the Cj’s. The known elements are f ( x )  and theMj(x) functions. Some terms 
have x-dependence and some terms are constants. We can generate a more consistent 
form if we operate on both sides of Equation 12.81 by 
(12.82) 

METHODS OF SOLUTION 
505 
which yields 
According to Equation 12.80, the integral on the LHS is just the unknown constant Ci. 
The first integral on the RHS can be defined as a new, known constant with an i 
subscript: 
6 
Bi = 
dxN;(x)f(x). 
The last integral generates the known elements of a matrix [A]: 
(12.84) 
(12.85) 
Equation 12.83 can now be written in terms of known and unknown constants as 
N 
C, = B; + A 
CjAij 
j =  1 
(12.86) 
or in matrix notation as 
where the matrices [B] and [C] can be viewed as column matrices made up of the 
elements of Bi and C,. 
Working in matrix notation, and rearranging Equation 12.87 gives 
where [l] is the N X N diagonal, identity matrix. Solving for [C] gives 
[CI = m 1  - "l)-"Bl. 
(12.89) 
To determine the elements of [C] requires an inversion of the matrix 
[ I 1  - A M ] .  
(12.90) 
Once these elements of [C] are determined, y(x) is generated quite simply using 
Equation 12.81: 
N 
(12.91) 
j = 1  

506 
INTEGRAL EQUATIONS 
This matrix notation points out a very interesting result for these types of problems. 
If f ( x )  = 0, then all the elements of the [B] matrix are also zero. Therefore, from 
Equation 12.88, it can be seen that a nonzero [C] matrix, and therefore a nonzero 
y(x), can exist only if 
Given the [A] matrix, Equation 12.92 will be satisfied only for specific values of A. 
A nonzero solution to the original integral equation will exist only for these An 
eigenvalues, and the solutions yn(x) are eigenfunctions of the integral equation: 
(12.93) 
1 
A n  
lb 
dx’k(x, x’) y n ( ~ ’ )  = -yn(X). 
Example 123 It is possible to separate kernels that do not appear, at first glance, 
to be separable. As an example, consider the translationally invariant kernel 
k(t,t’) = cos(t - t’). 
(12.94) 
This kernel can be separated with the trigonometric identity 
cos(t - t’) = cos(t) cos(t’) + sin(t) sin(t‘), 
(12.95) 
so that 
L 
cos(t - t’) = x M j ( t ) N j ( t ‘ ) ,  
j =  1 
where 
Ml(t) = cos(t) 
Mz(t) = sin(t) 
Nl(t’) = cos(t’) N2(t’) = sin(t’) 
(12.96) 
(12.97) 
EXERCISES FOR CHAPTER 12 
1. 
2. 
Derive the Fredholm integral equation that corresponds to 
subject to the boundary conditions y(1) = y ( -  1) = 0. How does the integral 
equation change if the boundary conditions are changed to y( 1) = dy/dxl = l? 
Convert the following integral equation to a differential equation: 
y(x) = [dx’(x2 - xx’)y(x’). 

EXERCISES 
507 
Identify all the boundary conditions included in the integral equation, which 
must be specified to make the solution to the differential equation unique. 
3. Given the Fredholm equation of the second kind, 
where 
x)(t + 1)/2 
(1 - t)(x + 1)/2 
-1 < t < x 
x < t < 1 
k ( x , t )  = { (' - 
' 
obtain the associated second-order differential equation and the boundary con- 
ditions. 
4. Convert the integral equation 
Y(X) = 1 - x f 
dt ( X  - t)y(t) 
l 
to a differential equation, with the appropriate boundary conditions. 
5. Convert the integral equation 
+(x) = 1 + h2 [ 
d t  (x - t)+(t) 
to a differential equation and determine the appropriate boundary conditions. 
6. Use Fourier techniques to solve the following Fredholm integral equation for 
4 (x) : 
7. Use Laplace techniques to solve the following integral equation for +(x): 
8. Using the Neumann series approach, solve 
to obtain +(x) = epx2 

508 
INTEGRAL EQUATIONS 
9. Consider the Fredholm equation 
(a) Using the method of separable kernels, determine the values for h which 
(b) Determine all the nonzero solutions for +(x). 
allow solution for +(x). 
10. Consider the differential equation 
dx + A;x2y(x) = 0, 
with the boundary condition y(0) = 1. 
(a) Convert this differential equation and its boundary condition to an integral 
equation. 
(b) Using the Neumann series approach, obtain a solution to the integral equation 
assuming small A,. Can you place this solution in closed form, Le., not in 
terms of an infinite series? 
11. Solve 
+(x) = x + 
dt (1 + xt)+(t) 
I’ 
by three methods, 
(a) the Neumann series technique. 
(b) the separable kernel technique. 
(c) educated guessing. 
integral equations: 
12. Find the possible eigenvalues and the eigenfunction solutions of the following 
1 
i. 4(x) = h 
ii. +(x) = A ll 
dt (t - x)+(t) 
iii. +(x) = h 
d t ( x  - t)’+(x). 
s_, 
1 
1 
dt(xt + x)+(t). 
Ll 

13 
ADVANCED TOPICS IN 
COMPLEX ANALYSIS 
This chapter continues our discussion of complex variables with the investigation 
of two important, advanced topics. First, the complications associated with multi- 
valued complex functions are explored. We will introduce the concepts of branch 
points, branch cuts, and Riemann sheets, which will allow us to continue to use the 
residue theorem and other Cauchy integral techniques on these functions. The second 
topic describes the method of steepest descent, a technique useful for generating 
approximations for certain types of contour integrals. 
13.1 MULTIVALUED FUNCTIONS 
Up to this point, we have concentrated on complex functions that are single valued. 
A function &) is a single-valued function of z if it generates one value of w for a 
given value of z. The function 
- 
w = z  2 
(13.1) 
is example of a single-valued function of z. In contrast, the function 
- 
W = g  1 / 2  
(13.2) 
is an example of a multivalued function of z. This is easy to see, because z = 1 
generates w = + 1 or w = - 1. Another point, g = 2i, gives either w = fi 
eiTI4 or 
operation is performed 
only on a positive real quantity to produce a single, positive, real result. Operations 
like ( )'I2 can have positive, negative, or complex arguments and produce multivalued 
results. 
509 
- 
w = J Z e i 5 ~ / 4 .  
In this chapter, we will use the convention that the 

510 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
13.1.1 Contour Integration of Multivalued Functions 
In Chapter 6, special techniques were derived for performing integrals in the complex 
plane. Can we still use the residue theorem and the other Cauchy integral techniques 
on these multivalued functions? The answer is yes, if we are careful! 
From our previous work, we know that if the complex function I&) is analytic 
inside a closed contour C. we can write 
(13.3) 
The key requirement here is that the contour must be closed. For this path to be closed, 
the variable of integration g has to return to its starting value. For a single-valued 
function, this obviously also brings ~ ( g )  
back to its starting value. The same is not 
always true with a multivalued function, and this is the source of some difficulties, 
as you will see in the two examples that follow. 
Example 13.1 The single-valued function 
- 
W = $  
(13.4) 
is analytic everywhere in the complex z plane. Remember, for a function to be 
analytic, we must be able to write w = u(x, y )  + iv(x, y), with u(x, y )  and v(x, y )  
being continuous, real functions that obey the Cauchy-Riemann conditions. In this 
case, 
and 
(13.5) 
(13.6) 
So the Cauchy-Riemann conditions are satisfied for all g. If the closed contour C 
is the unit circle centered at z - = 0, as shown in Figure 13.1, then according to the 
Cauchy integral theorem, 
f dgg = 0. 
C 
(13.7) 
Let’s be skeptical about this last result and explicitly perform the integration of 
Equation 13.7. On the contour C, g = eie and dg = ieiedO. To go around the circle 
once and return g to its initial value, we let the range of 8 vary from 0 to 2 7 ~ .  
We could 
have made the limits on 8 different, perhaps T to 37r, but the starting and ending 

MULTIVALUED FUNCTIONS 
Y 
- 
z-plane 
511 
Figure 13.1 Contour for Unit Circle Integration 
values of 8 must differ by 27r so the circle is traversed exactly once. The integral 
becomes 
= (1/3)(ei6" - 1) 
= 0, 
(13.8) 
which is exactly the expected result. The integral around a closed path of this analytic 
function is zero. There are no surprises here. 
Example 13.2 Now consider the multivalued function 
(13.9) 
The single point 
simple way to see this is to write z - using an exponential representation 
= 1 will give two different values for y: w = 1 and w = - 1. A 
Notice that all these expressions identify the point g = 1, but do so using different 
polar angles. Now use the fact that (ei')3/2 = e3'l2 to obtain the corresponding values 
of y: 
1 
- 1  
,iO = &67r = ei12.rr. . . = 
- 
w = {  
or 
(13.11) 
,i3n = ,i97r = ei15.rr. . . = 
Two different values for w are generated by this process. 

512 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
Is this function analytic? If we write z - = x + iy, we have 
- 
w = (x + iyl3l2, 
(13.12) 
and with w = u(x, y )  + iu(x, y), 
1 
2 -  
1 
1 
2i 
2i 
u(x,y> = -(w + w*) = 
(x + iy)3/2 + (x - iy)3/2] 
V ( X , Y )  = --(klI - w*) = - 
[(x + iy)3/2 - (x - i ~ ) ~ / ' ] .  (13.13) 
The partial derivatives become 
(1 3.14) 
and so the Cauchy-Riemann conditions are satisfied for all g. Ths means the function 
given in Equation 13.9 is analytic everywhere in the g-plane, and consequently its 
closed line integral around the unit circle shown in Figure 13.1 should be zero: 
(13.15) 
Again, let's be skeptical about this result and check it explicitly by performing 
the same type of integration worked out in the previous example. As before, we set 
z - = ,i@ , dg = ieiedO, and let 0 range from zero to 2 a :  
6'" 
(13.16) 
J 
4 
5 '  
=-- 
Uh-oh! The Cauchy integral theorem appears to be violated. Our function is analytic 
inside C, but the integral is nonzero. Because the Cauchy integral theorem is the 
basis for complex integration using the residue theorem, its violation would be a big 
problem. Fortunately, there is a way to preserve the validity of the theorem. 

MULTIVALUED FUNCTIONS 
513 
Notice, although the 0 to 27r range of 8 returns z to its starting value in the above 
integration, it does not return g to its initial value. When 8 = 0, z = eiO = 1 and 
g =  1,butwhen8=27r,z=ei2"= 1 a n d w = e i 3 " =  
- 
-1 . B ut if we go around 
the unit circle twice, that is 0 < 8 < 4 ~ r ,  
both z and ~ ( z )  
= g312 will return to their 
starting values, and we obtain 
It appears that the definition of closed integral contour must take on new meaning 
when we are using multivalued functions. We cannot say a contour is closed unless 
both the variable of integration and the function being integrated simultaneously 
return to their initial values. We will investigate this idea in detail in the sections that 
follow. 
13.1.2 Riemann Sheets, Branch Points, and Branch Cuts 
Branch points, branch cuts, and Riemann sheets are all concepts developed so that 
the Cauchy integral theorem, the residue theorem, and all the other tools developed 
for analytic functions will still work with multivalued, complex functions. 
Riemann Sheets 
looking at their mapping properties. Consider the simple function, 
The nature of multivalued functions can best be explained by 
- 
w(z) = z1'2. 
( 13.18) 
It is easy to see this function is multivalued. The single point z = eim14 = ei9"14 maps 
onto two different points in the y-plane, w = ei"l8 and 211 = ei9"18. This mapping is 
shown in Figure 13.2. 
Y 
a 
X 
- 
z-plane 
V 
a 
U 
- 
a 
y-plane 
Figure 13.2 Mapping of a Multivalued Function 

514 
z-plane 3x 
Y 
X 
Figure 133 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
V 
y-plane "? 
V 
V 
Mapping of y = gl/* 
Let's look at the mapping in more detail by determining how each of the points on 
a unit circle, centered around the origin of the z-plane, gets mapped onto the w-plane. 
On the circle, z = eie and 
= eie/* = ei+. This mapping is shown, divided into 
three steps, in Figure 13.3. At the start, we set 8 = 0, so z = 1 and y = 1. In the first 

MULITVALUED FUNCTIONS 
515 
pair of pictures, we see that as the polar angle 8 increases from 0 to T, 4 increases 
from 0 to ~ / 2 .  
So w moves along its own unit circle, lagging z in angle by a factor 
of two. In the middle pair of pictures, 8 continues to increase from T to 2
~
,
 
so that 
z returns to z = ei2= = 1, its initial value. However, 
ends up at w = eiV = - 1, 
because 4 has only increased to T. 
In the last pair of pictures, 8 continues to increase 
from 2~ to 4 ~ ,  
repeating the loop it has already made, and once again returns to its 
initial value ofz = ei4= = 1. At the same time, w finally makes a complete circle 
and returns to its initial value, w = ei2= = 1. With the interpretation that a closed 
contour in the Z-plane must return both z and the function ~ ( g )  
to their starting values, 
the unit circle in the Z-plane must be traversed twice in order to form a closed path 
for the function 
= $I2. 
To facilitate contour integration and other tasks that involve multivalued functions, 
it is useful to extend our concept of the complex plane to include what are commonly 
called Riemann sheets. Simply put, this is a way to force the mapping of a multivalued 
function to be one-to-one, so we can keep track of what contours are truly closed. 
Imagine, in the case of the function w = z1I2, the complex g-plane is composed of 
two different “sheets,” as depicted in Figure 13.4. The contour that follows in Figure 
13.3 can be viewed not as the same circle twice, but one extended circle, which covers 
both sheets. In the 2-plane, as 8 is increased from zero to 2m, a circle is traced out 
on the upper sheet in the 2-plane and maps to the upper half circle in the !?-plane, 
Y 
Y 
X 
y-plane 
Multisheeted ?-plane 
Figure 13.4 Multisheeted g-Plane 

516 
I Y  
ADVANCED TOPICS IN COMPLEX ANALYSIS 
Y 
y-plane 
Multisheeted gplane 
Figure 13.5 Mapping of the Unit Circle with a Multisheeted Z-Plane 
as shown in Figure 13.5. Once 0 is increased beyond 2 ~ ,  
suddenly g "jumps" to the 
second sheet in the z-plane. Consequently, the z-plane contour has not closed on itself 
at this point. As 6 is increased from 271 to 471, now tracing out a circle on the lower 
z-plane sheet, y traces out the lower half circle in its plane, as shown in Figure 13.6. 
Finally, once 0 reaches 471, z jumps back to the upper sheet and the circular paths in 
both planes are complete. 
The picture, in Figure 13.2, of how a single point gets mapped from the z-plane to 
the w-plane is also modified when we are using Riemann sheets. The point = ei"/4 
is located on the upper Riemann sheet of the g-plane and maps into the point 3 = 
in the y-plane. The point z = ei9"l4, which is located on the lower Riemann sheet, 
maps onto the other point 
= ei9n/8 of the y-plane. In this way the mapping is 
one-to-one, as shown in Figure 13.7. 
Notice that a one-to-one mapping has been accomplished with only one sheet for 
the y-plane. This works because the inverted function z = y2 is not a multivalued 
function of 211. In more complicated functions, multiple Riemann sheets may be 
needed in both the z- and w-planes. 
Branch Points In the analysis of the previous section, we demonstrated the ne- 
cessity of having two Riemann sheets by determining how the points along a closed 
contour in the z-plane were mapped on the y-plane. We will use this method so often 

522 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
that connects the two branch points. It is for this reason that branch points must occur 
in pairs. They provide the end points for the branch cuts. If there is only a single 
branch point in the finite complex plane, it must be coupled with one at infinity. More 
will be said about relocating branch cuts in the examples that follow. 
13.1.3 Applications of Riemann Sheet Geometry 
Now that the fundamentals of branch points and branch cuts have been introduced, 
the details of Riemann sheet geometry are probably best understood by looking at 
examples of specific functions. The examples that follow discuss increasingly more 
complicated multivalued functions. In each case, we determine the location of the 
branch points and the number of Riemam sheets needed for a one-to-one mapping, 
locate the branch cuts, and make the Riemann sheet connections. 
~ 
Example 13.3 Consider the Riemann sheets associated with the function = z1I3. 
For this function, each value of z generates three different values of w, so it's likely 
that there must be three Riemann sheets to describe the z-plane. A tour in the g plane 
shows that g = 0 is a branch point, and it must be circled three times before w = z1I3 
returns to its starting value. This confirms that there must be three -plane sheets. 
Sheet 1 
I L 
d 
C 
f 
e 
Sheet 3 
C 
d 
e 
f 
Figure 13.14 Riemann Sheet Connections for w = g1I3 

518 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
that we give it a special name. The paths used to determine the multivalued nature 
of a function will be referred to as lours. With the function 
= g112, we discovered 
a single tour around the origin returned g to its initial value, but did not return 3 
to its starting value. W o  successive tours, however, brought both z and w back to 
their initial values. This is why we needed two Riemann sheets in order to make our 
mapping one-to-one. 
Still using the function w = g112, imagine another tour, this time not around the 
origin, but around another point in the first quadrant, as shown in Figure 13.8. We 
label the starting point for the tour as P, with coordinates r, and 0,. As we move 
around this circle once, z = r eie clearly returns to its initial value. But notice, w also 
returns to its starting value as well, because both r and 8 have returned back to their 
initial values. This will be the case for any tour of this function that does not surround 
the origin. For this pdcular function, the origin appears to be a very special point. 
Points of this type, those around which a single tow does not return the function 
to its initial value, are called brunch points. Branch points are isolated by reducing 
the radius of the touring circle to an arbitrary, small value. Usually the first task in 
making sense out of any multivalued complex integrand is to determine the locations 
of all its branch points. 
As you will see in the next section, branch points always occur in pairs. We 
have seen that the function w = z1I2 has a branch point located at the origin in the 
complex g-plane. Tours with this function, like the one in Figure 13.8, show that 
there are no other branch points of w = g112 in the finite g-plane. The second branch 
point for this function must be located at infinity. It may seem strange to call infinity 
a single point, but we can do so with the aid of the interesting mapping shown in 
Figure 13.9. This figure shows how points in the complex plane can be mapped onto 
points on the surface of a sphere of unit radius. The complex plane passes through 
the center of the sphere. To determine the point on the sphere that corresponds to 
a given complex number, draw a line from N, the north pole of the sphere, to the 
point in the complex plane. Where this line intersects the sphere is the mapped point. 
Points in the complex plane that lie outside the intersection of the sphere and the 
plane, such as the point PI shown in Figure 13.9, map onto the northern hemisphere. 
Figure 13.8 Displaced Unit Circle Contour 

MULTIVALUED FUNCTIONS 
519 
N 
S 
Figure 13.9 Mapping of the Complex Plane onto the Surface of a Unit Sphere 
Points inside this intersection, such as P2, map onto the southern hemisphere. Notice 
how the origin of the complex plane maps to the south pole of the sphere, while uny 
infinity in the complex plane, that is +m, -m, +im, -iw, etc. maps to the north pole. 
This single point at infinity is the second branch point of y = g112. With this picture, 
a small circular tour around the origin of the z-plane is equivalent to a small circular 
tour around the south pole. A tour around an infinitely large circle in the g-plane is 
equivalent to a small circular tour around the north pole. 
Brunch Cuts The two Riemann sheets associated with the function g112 have been 
described in Figures 13.4-13.7. The picture for the Riemann sheet geometry is not 
complete, however, until the sheets have been connected. This connection occurs 
where tours jump from one sheet to the other. The tour described in Figures 13.5 and 
13.6 was on a circle of constant radius. As the radius of this circle is changed, it can 
be seen that the jump between sheets occurs along a line that falls on the positive 
x-axis. This line forms what is referred to as a brunch cut and is shown in Figure 
13.10. It is called a cut, because you can imagine taking a pair of scissors and actually 
cutting the Z-plane sheets along this line to prevent a contour from crossing it. It can 
be seen that the cut goes between the two branch points of $I2. It is made on each 
sheet of the Z-plane, along the same line, as shown in Figure 13.1 1. The edges of the 
cut on one sheet have been labeled a-b and the edges of the other sheet have been 
labeled c-d. These edges are then connected as shown in Figure 13.12. As tours cross 
the branch cut they follow the connection and switch sheets. 
Figure 13.12 is quite clever because it simultaneously shows the nature of the 
z-plane from two different points of view. If the two g-plane Riemann sheets are 
stacked and viewed from the top, the point z = eiW14, on the top sheet, is exactly 
above the point z = ei9W/4, 
on the bottom sheet. Consequently, from this point of view 
these two points appear as the single point g = 1/& + i/&. 
But when the sheets 

520 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
Branch point 
' 
Branch cut 
,\\ 
/ 
/
x
 
-*--L--- 
'\ 1 
V 
1 
Branch point 
Branch cut 
y-plane 
Multisheeted ?-plane 
Figure 13.10 A Branch Cut in the ?-Plane 
b 
a 
Sheet 1 
Figure 13.11 Cut Riemann Sheets 

MULTIVALUED FUNCTIONS 
521 
Figure 13.12 Riemann Sheet Connections 
are viewed separately, z = 1 /fi 
+ i/& 
becomes two distinct points, one on each 
of the g-plane Riemani sheets. With many functions, however, making a drawing 
like Figure 13.12 becomes impossible. In these cases, the more abstract method of 
indicating the connections shown in Figure 13.13 is more convenient. 
For the function z1I2 we have placed the branch cut along the positive x-axis, 
between the branch points at the origin and at infinity. The arguments associated with 
Figures 13.5 and 13.6 led to this conclusion. With a little thought, it should be clear 
that the jump between sheets, and therefore the location of the branch cut for this 
function, does not have to be along the positive x-axis, but could be along any line 
Sheet 1 
Sheet 2 
C 
d 
C 
Figure 13.13 Another Way to Show Riemann Sheets Connections 

522 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
that connects the two branch points. It is for this reason that branch points must occur 
in pairs. They provide the end points for the branch cuts. If there is only a single 
branch point in the finite complex plane, it must be coupled with one at infinity. More 
will be said about relocating branch cuts in the examples that follow. 
13.1.3 Applications of Riemann Sheet Geometry 
Now that the fundamentals of branch points and branch cuts have been introduced, 
the details of Riemann sheet geometry are probably best understood by looking at 
examples of specific functions. The examples that follow discuss increasingly more 
complicated multivalued functions. In each case, we determine the location of the 
branch points and the number of Riemam sheets needed for a one-to-one mapping, 
locate the branch cuts, and make the Riemann sheet connections. 
~ 
Example 13.3 Consider the Riemann sheets associated with the function = z1I3. 
For this function, each value of z generates three different values of w, so it's likely 
that there must be three Riemann sheets to describe the z-plane. A tour in the g plane 
shows that g = 0 is a branch point, and it must be circled three times before w = z1I3 
returns to its starting value. This confirms that there must be three -plane sheets. 
Sheet 1 
I L 
d 
C 
f 
e 
Sheet 3 
C 
d 
e 
f 
Figure 13.14 Riemann Sheet Connections for w = g1I3 

MULTIVALUED FUNCTIONS 
523 
This function is so similar to w = g1l2, that it is fairly clear that the only other branch 
point is at infinity. There is a single branch cut connecting these two branch points. If, 
as before, the cut is made along the positive real axis, the connections of the Riemann 
sheets are as shown in Figure 13.14. If you start on the top sheet (sheet #l), a single 
loop around the origin drops you to the second sheet. A second tour drops you to the 
third sheet. A third loop returns you to the first sheet, and back to your starting value 
of g. This example points out the usefulness of the simple dot connection diagram 
for indicating sheet connections. It would be very difficult to make a drawing like 
Figure 13.12 for ths function. 
~ 
~ 
~ 
~~~~~ 
~~ 
Example 13.4 As the next example, consider the function y = In z. This function 
is more conveniently handled with polar representation: z_ = re'' and y = In r + i0. 
From this form. it is clear that all tours in the 2-plane around any point other than the 
origin will return w to its initial value. A tour around the origin, however, will not. To 
see this, start a tour around the origin at some initial point with r = r, and 0 = 0,. 
The initial value of the function is y = In rz + ill,. After a single lap around the origin, 
- 
w = In r, + i(0, + 27~). After two laps, w = In r, + i(0, + 4.n). No matter how many 
complete tours are made, g = In g will never return to its initial value! Consequently, 
for this function, there are an infinite number of g-plane Riemann sheets. If the sheets 
are cut along the positive real axis, they are connected as shown in Figure 13.15. 
Example 13.5 Now consider the function 
- 
w = (z2 
- - a2)1/2, 
(13.19) 
where a is a positive real number. This example is quite a bit more complicated, 
and requires the use of multiple complex phasors to check for branch points. One 
way to approach a function of this nature would be to break it up into two or more 
simple intermediate functions. For instance, in this case you might let 2 = g2 - u2 
so w = g112. Then you would analyze the two functions separately and combine 
the results. For this example, however, it is more instructive to work with the entire 
function directly. It does help to factor Equation 13.19 into two terms: 
g = (z - a ) q g  + a)? 
(13.20) 
Our first task is to find all the branch points in the g-plane. To accomplish this, we 
must perform lots of tours to determine where w does not return to its initial value. It 
is useful to define two phasors for the complex quantities 2 - a and z + a, as shown 
in Figure 13.16. These phasors are, in exponential notation, 
With these definitions, w can be written as 
(13.21) 
(13.22) 

/ 
/ 
/ 
/ 
k 
/ 
/ 
Figure 13.15 Riemann Sheets for 
= In z 
- 
z-plane 
z - 
b 
d 
-a 
a 
Figure 13.16 Phasors for Touring in the z-Plane 

MULTIVALUED FUNCTIONS 
525 
- 
z-plane 
I
Y
 
i 
- 
- 
z + a  
- 
z - a  
,' 
X 
~ 
-~~ __ 
-a 
a 
Figure 13.17 Tour Around z f +a 
or 
(13.23) 
For the first tour, pick any point P, which is not one of the two points +a, as shown 
in Figure 13.17. At the start of the tour, g = 5, with the corresponding initial values 
for the phasor components being rl,, el,, r2,, and 02,. A single counterclockwise 
tour around P returns both g and all these phasor components to their initial values. 
Consequently, w also returns to its initial value, and P is not a branch point of 
Now let's tour around the point z = u, again defining the starting position of the 
tour to be z_ = z,, 
as shown in Figure 13.18. The initial values for the phasors are 
again YI,, el,, r21, and 02,. As z makes a counterclockwise loop around this point and 
returns to its initial value, rl, 01, and r2 all return to their initial values, but this time 
02 has increased to 02, + 27~. 
Consequently, y after one loop is 
g = (52 - a 2 ) P  
= f i f i e 1 ( B 1 , + h + 2 ? r ) / 2  
= - J.1I fi 
e 4 h  + &< 
)/2 
(13.24) 
Y 
- 
z-plane 
-a 
a 
Figure 13.18 Tour Around the Point = a 

526 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
which is a factor of - 1 different from its initial value. This shows 5 = Q is a branch 
point of w = (z2 - u2)'I2. A tour around the point z = -a produces a similar result, 
so the two points z = +a are both branch points of y. 
We have already shown there are no other branch points in the finite z-plane, but 
there is the possibility that these two branch points do not fom a pair with a single cut 
between them. Instead, each of the branch points might have a separate cut extending 
out to infinity. In this case, you can visualize each of the two branch points paired 
with a different branch point at infinity. An easy way to determine if this is actually 
happening in this case is to make a large tour that encloses both branch points, as 
shown in Figure 13.19. If, at the end of this tour, we are on a different sheet from the 
start, we must have crossed some cut(s) extending to infinity. After performing this 
big tour, notice both rl and r2 have returned to their initial values, but both 81 and 62 
have increased by 2n-, so w becomes: 
(13.25) 
which is exactly the initial value of w. We did not have to cross a cut, so the two 
branch points at g = +a must form a pair with a single cut between them. 
To determine the complete nature of the sheets we need to know the results of a 
few more tours. It is easy to show that touring around either of the branch points at 
z = +a twice returns 
to its initial value. Also, going around one of the branch 
- 
z-plane 
-+-~- 
- 
z - a 
\ '% 
X 
l 
' 
-a 
a
,
 
Figure 13.19 Tour Around Both Branch Points 

MULTIVALUED FUNCTIONS 
527 
2 
Sheet 1 
Sheet 2 
Figure 13.20 Riemann Sheet Connections 
points once and then back around the other, in a figure-eight pattern, also returns 111 to 
its initial value. These facts lead us to conclude there are two Riemann sheets in the 
g-plane, and they are connected with a cut between them, as shown in Figure 13.20. 
This figure shows the cut placed along a straight line between z = ?-+a. 
As we 
mentioned earlier, the cut can be placed anywhere as long as it terminates at the 
appropriate branch points. We could legitimately deform the cut, in successive steps, 
as shown in Figure 13.21. The configuration of the cut at the bottom of this figure 
makes it look like there might now be two branch cuts. This is not the case. It is 
actually the same cut making a very large loop around the complex plane. With the 
cut positioned as shown at the bottom of Figure 13.21, a tour enclosing both branch 
points crosses the branch cut twice. Because this is the same cut, however, the second 
crossing of the cut returns us to the original sheet. 
Example 13.6 As a last example, consider the function 
(13.26) 
Although this looks very similar to the function in the previous example, it behaves 
quite differently. To find the branch points, tours must be made in the z-plane. The 
same phasors that were used in the previous problem, and shown in Figure 13.16, 
can be used again here: 
z + a = rl eiel 
z - a = r2e'h. 
With these phasors, the function can be rewritten 
(13.27) 
(13.28) 
In a single, counterclockwise lap around g = a, rl, 81 and 1-2 return to their initial 
values. The angle 192, however, increases by 27~. 
For simplicity, take the initial values 
of 81 and 02 to be zero, and the initial radial values to be rl = rli and rz = rzi. Then 

528 
-a 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
a 
-a 
a 
-a 
I 
a 
X 
---- 
- , 
-a 
a 
i 
l 
Figure 13.21 Deformation of the Branch Cut 
at the start of this tour, 
but at the end 
- 
w = J r , - J r , .  
(13.29) 
(13.30) 

MULTIVALUED FUNCTIONS 
529 
A tour around z = -a produces a similar result, so both points at z = t a  are branch 
points. Single loop tours around any other point in the finite _z-plane will return 
to 
its starting value, so there are no other branch points in the finite z-plane. 
As was the case in the previous example, we must check to see if there are any 
branch points at infinity by making a tour around both g = a and z = -a. Start the 
tour with both 8, and & equal to zero. At the end of this tour, 11 and rz will return to 
their initial values, but both 81 and 82 will increase by 27r. Thus, at the start of this 
tour 
w = J r , , + J r , , .  
while at the end 
- 
w = - J l l l -  
Jr,,. 
(13.31) 
( 13.32) 
Consequently, there must be two branch points at infinity, and the branch cuts that 
start at z = a and z = -a extend to infinity, as shown in Figure 13.22. Unlike the 
single cut, with the-ksleading appearance of two cuts, shown at the bottom of Figure 
13.21, these really are two distinct cuts extending out to infinity. 
We can determine the total number of Riemann sheets and how they are connected 
at the cuts by performing a few more tours, and watching how many different values 
of 
are generated at the same point of g, Consider the three different tours shown 
in Figure 13.23. These tours all start with the same value of g. This value, which we 
will call zi = ri e”1, is a pure real quantity, lying just above the cut with 8i = 0 and 
to the right of z = a. In terms of the phasor parameters, at the start of each of these 
tours 
w=Jr,+Jr,. 
We already determined that at the end of the topmost tour 
Y 
- 
z-plane 
X - 
-a 
a 
(13.33) 
(13.34) 
Figure 13.22 Branch Points and Branch Cuts for 
= (z_ - a)’/* + (z_ + 

530 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
e 
-a 
- 
z-plane 
I
y
 
i 
I 
I 
i- 
I 
I 
, 
a 
\ 
1 
/ '  
\ 
Y 
Y 
\ 
-a 
\ 
Figure 13.23 Tours to Determine the Number of Riemann Sheets 
which is different from the initial value, so at the end of this tour we must be on a 
second Riemann sheet. Now consider the tour shown in the middle of Figure 13.23. 
After one lap around this path, only 81 has changed by 277, and so 
- 
w = - f i + f i .  
(13.35) 

531 
MULTIVALUED FUNCTIONS 
rhis is a third value for w, different from both Equation 13.34 and 13.35, so at the 
:nd of this tour, we must be on a third Riemann sheet. A single lap around the tour 
shown on the bottom of Figure 13.23 changes both O1 and O2 by 2 ~ .  
At the end of 
:his tour 
g=-Jr,,-fi. 
(13.36) 
rhis is a fourth value for g and consequently requires a fourth Riemann sheet. 
Any combination of these tours or multiple laps of a single tour will not generate 
any new values for y, and so we conclude that there are four Riemann sheets. Because 
a two-lap tour around either of the branch points at z = +a would return us to the 
ariginal sheet, but a figure-eight pattern would not, the connection diagram must be 
as shown in Figure 13.24. 
I 
\ 
\ 
\ 
4 
b 
Figure 13.24 Riemann Sheet Connections 

532 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
A quicker way to see that there should be four Riemann sheets for this function is 
to realize that the 2 property of the square root operation, i.e., 
- 
w = ?fit Jrz, 
(13.37) 
allows four different possible combinations for forming y. This method is useful for 
determining the number of sheets, but does not guide you in determining how they 
are connected. In our previous example, with w = (3 - a)1/2(z + 2)'12, the square 
roots have only two combinations for _w, 
- 
w = 2fiJrz, 
and so there were only two Riemann sheets associated with this function. 
(13.38) 
13.1.4 Riemann Sheet Mapping 
Quite often, it is useful to see how an entire Riemann sheet gets mapped from one 
plane onto another. This is quite easy for the function w = z"~. If we define g = rzei' 
and 
= r,,,ei4, we can write 
rw = Jr, 
(13.39) 
(13.40) 
e 
+ P = -  
2' 
According to Figure 13.10, we are on the first Riemann sheet if 0 < 8 < 2rr. 
Therefore, the values of y we can generate from the first sheet will be limited to the 
angular range 0 < 4 < m, or equivalently the upper half plane. Likewise, the second 
Riemann sheet will map onto the lower half plane of the w-plane. This situation is 
shown in Figure 13.25. 
Now remember, we said the actual placement of the branch cut is arbitrary, as 
long as it connects the branch points. For this function, the branch cut could equally 
well have been placed along the positive imaginary axis of the z-plane. In that case, 
the z-plane sheets would map onto the _w-plane as shown in Figure 13.26. Now the 
first Riemann sheet is associated with the range 7r/2 < 8 < 5rr/2, and the second 
sheet with 5 ~ / 2  < 8 < 9m/2. In the w plane, the dividing line has been rotated by 
r/4. Keep in mind, there is nothing that says the branch cut has to be a straight line. 
It could be placed as shown in Figure 13.27. In this case, it is impossible to define 
the different Riemann sheets of the g-plane using a simple range of 8. Don't make 
the common mistake of assuming a Riemann sheet is always defined by an angular 
range. 
It is possible that, in order to get a one-to-one mapping, there must be multiple 
sheets in both the z- and y- planes. Consider the function y = z3l2. We investigated 
this function briefly at the beginning of this chapter. We showed there were two 
values of y for a single value of z, 
- so there must be two :-plane Riemann sheets. 

MULTIVALUED FUNCTIONS 
533 
w-plane 
Multisheeted ?-plane 
Figure 13.25 Riemann Sheet Mapping for 
= gl/* 
- 
w - Plane 
Multisheeted z - Plane 
Figure 13.26 Another Riemann Sheet Mapping for 
= g1l2 

534 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
Top sheet of the?-plane 
Figure 13.27 
Arbitrary-Shaped Branch Cut 
Quite similar to the square root function, tours in the g-plane reveal two branch points, 
one at z - = 0 and one at = 00. In this case, however, the inversion of this function 
z = - 
w2I3 is also multivalued. Each value of w produces three different values of z, so 
there are three Riemann sheets in the pplane. Tours in the g-plane show that there 
are two branch points, one at y = 0 and one at 
= m. If the cuts in both planes are 
taken along the positive, real axes, the _z-plane sheets map onto the y-plane sheets, as 
shown in Figure 13.28. The branch cut in the z-plane maps onto the real axis of the 
second Riemann sheet of the y-plane. 
13.1.5 Integrals of Multivalued Functions 
Remember, the primary reason we have spent all this effort defining branch points, 
branch cuts, and Riemann sheets is so we can use the Residue theorem and other 
Cauchy techniques to perform integrals on multivalued complex functions. 
Consider a contour integral in the complex g-plane of our favorite multivalued 
function: 
1 dgg1’2. 
c 
(13.41) 
From our previous experience with this function, we know there are two Riemann 
sheets, and two branch points in the g-plane, one at 5 = 0 and the other at z = a. 
For purposes of this discussion, let C extend across the lower half of the first sheet 
of the z-plane, so that the contour looks as shown in Figure 13.29. The integrand in 
Equation 13.41 is analytic everywhere except at z = 0, and so the contour C can be 
deformed without changing the value of the integral, as long as it is not moved across 
z = 0. Let’s attempt to move it up, into the upper half plane. As the contour is moved 
up it is snagged by the nonanalytic point at z = 0. But that is not all that happens. 
The left half of the contour, along which the real part of z is less than zero, moves up 
with no problem. But as the right half of this contour is moved across the positive real 

MULTIVALUED FUNCTIONS 
535 
Multisheeted - 
z-plane 
Figure 13.28 Mapping of Riemann Sheets for 
= z3I2 
Y 
Sheet #I 
Y 
Sheet #2 
X 
Figure 13.29 An Integration in a Multisheeted z-Plane 

536 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
Sheet #1 
Y 
X 
7- 
Y 
Sheet#2 
1 
Figure 13.30 Contour Deformation with Two Riemann Sheets 
axis, the deformed contour runs into the cut and must switch to the second Riemann 
sheet, as shown in Figure 13.30. 
Actually, it is possible to deform C and stay on a single Riemann sheet. Because 
the location of the branch cut is arbitrary, as long as it still connects the branch points 
we can m o w  it to extend along the positive imaginary axis, as shown in Figure 
13.3 1. Now as the contour C is raised into the upper half z-plane, it is deformed as 
shown in Figure 13.32. 
C 
Figure 1331 Integration with Repositioned Branch Cut 
I 
Sheet #1 
I 
Figure 13.32 Contour Deformation Remaining on a Single Riemann Sheet 

MULTIVALUED FUNCTIONS 
537 
Figure 13.33 Integration Along the Branch Cut and Around the Branch Point 
Continuing with this second approach to the deformation, consider the portion 
of this integration shown in detail in Figure 13.33. It is tempting to claim that the 
two vertical portions of this contour can be made to cancel each other, because they 
follow the same line, but in opposite directions. This would be the case if it weren’t 
for the branch cut, which causes the function zl/* to have different values on either 
side. To see this. let 
z - = r eie 
(13.42) 
with 8 = 0 being on sheet #l. Along the vertical segment just to the right of the 
branch cut, 8 = rr/2 and 
g1I2 = f i e i n I 4  = f i ( 1  + i )  
O < r < a .  
(1 3.43) 
Along the vertical segment just to the left of the branch cut, 8 = -3rr/2 and we have 
(1 3.44) 
The value of gl/’ 
on the left-hand side of the cut is not equal to its value on the 
right-hand side, and so the integrals along the vertical sections do not cancel. 
We should also look carefully at little circular part of the integral around the 
branch point in Figure 13.33. To perform this integration define z = Eeie. Then as 
the circular contour is shrunk around the branch point, 
lim & ei0I2 
- 3 ~ / 2  
< 8 < ~ / 2 .  
(13.45) 
Consequently, as E - 0, the integrand also goes to zero and the contribution from 
this circular segment is zero. In general, however, the integral around a branch point 
will not be zero and its contribution must be carefully evaluated for each particular 
situation. 
p! 
= 
- 
E’O 
These ideas are developed in the two examples that follow. 

538 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
Figure 13.34 Complex Plane Picture for Integration of Example 
Example 13.7 
As an example of how contours involving multivalued functions can 
be deformed in the complex plane, consider the integration 
(13.46) 
where 5 is some point in the complex plane, and C is the straight line contour 
on the first sheet, shown in Figure 13.34. The multivaluedness of the integrand is 
controlled by g1I2 in the numerator. The integrand can, as before, be represented on 
two Riemann sheets with branch points at g = 0 and z = 03, and a single branch 
cut between them. The addition of the term in the denominator does not change the 
arrangement of branch points or the nature of the Riemann sheets, but it does add 
a nonanalytic, second-order pole on both sheets, at g = 5. The branch cut is taken 
along the positive imaginary axis in anticipation of the contour deformation we are 
about to perform. 
A clever transformation of this integral can be arrived at by deforming C into the 
upper half plane as far as possible, while trying to stay entirely on the first Riemann 
sheet. As we do this, the contour “snags” on both the branch point and the pole at 
G .  This situation is shown in Figure 13.35. As the imaginary part of the horizontal 
sections of C’ approach +ia, their contribution to the integral vanishes, because the 
Figure 13.35 Deformed Inkgation Contour 

MULTIVALUED FUNCTIONS 
539 
Figure 13.36 Final Contours for Evaluating Integration 
integrand is approaching zero at a rate proportional to r-3/2. The two vertical sections 
extending down to 5 also cancel, because the integrand has the same value on these 
segments and they are in opposite directions. From our previous discussion, however, 
we know the integration along the vertical sections on either side of the branch cut do 
not cancel, and the contribution around the branch point must be evaluated carefully. 
The contour shown in Figure 13.34 is thus equivalent to the sum of the two contours 
shown in Figure 13.36. 
Example 13.8 
of the real integral 
Now let's work a specific, practical problem. Consider the evaluation 
1 
m 
I = b dx & x +  
1)' 
(13.47) 
Remember our convention that the 
we should obtain a positive result for this integral. 
2 for x, and then determining an appropriate contour. The integral can be written as 
symbol always gives us the positive root, so 
Our first step is to convert this integral into one in the complex plane by substituting 
( I  3.48) 
The integrand is multivalued because of the 2'12 term, and so there are two Riemann 
sheets, the branch points at zero and infinity, and the single branch cut between them. 
We place the branch cut on the positive real axis. To properly correspond to the real 
integral, the complex contour C must also run along the real axis, directly parallel to 
the cut, on one of the two sheets. To determine which one, let 
x -+ z = r eie. 
(13.49) 
Along C, we must have either 0 = 0 (on the first sheet) or 0 = 27r (the second 
sheet). But notice if 0 = 27r, we get the negative root in the denominator, which we 
explicitly said we did not want. Thus we take the contour to be on the first sheet, as 
shown in Figure 13.37. 

540 
Sheet #1 
c
x
 
X 
Figure 1337 Starting Contour for Integral Evaluation 
We are left with evaluating the integral in Equation 13.48 along the path shown in 
Figure 13.37. This integration can be evaluated by considering integration around the 
cleverly closed contour C, on sheet #1, shown in Figure 13.38. This contour is indeed 
closed, because we have made a special effort to stay on the first sheet by avoiding 
the branch cut entirely. The integration around C, is easy to evaluate, because the 
integrand is analytic everywhere inside C, except at g = - 1. Therefore, 
Y 
(13.50) 
Figure 13.38 Equivalent Closed Contour Integration 

MULTIVALUED FUNCTIONS 
541 
Y 
* 
-1 
Figure 13.39 The Four Parts of the C, Contour 
It is important to notice that the residue was evaluated at z = eiT and not at z = ei3=, 
because the integration is being performed on sheet #l. 
The problem now becomes one of relating the outcome of this integration back to 
the original integral of Equation 13.48. The integration around C, can be broken up 
into four parts: 
(13.5 1) 
where the paths 1-4 are shown in Figure 13.39. Along path 1, 8 = 0, z = re", and 
dz = dr so 
(13.52) 
The integral along path 1 is the same integral as in Equation 13.47, the very thing we 
hope to determine. Along path 2, we are moving around a circle of constant radius 
R, so z = Re" and dz = iReiedf3, with 8 ranging from 0 to 2
~
.
 
As we take the limit 
as R goes to infinity, path 2 contributes 
= 0. 
(13.53) 

542 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
Along path 3, 8 = 2
~
,
 
while r ranges from infinity to zero, so z = rei2= and 
dz - = rei2*dr. The integral on path 3 becomes 
which can be rewritten as 
(1 3.54) 
(13.55) 
Along path 4, we want to integrate around the branch point with a tiny circular 
contour whose radius will shrink to zero. If the radius is E ,  along the contour 3 = EeiB, 
dz = ieeied8 and, because we are going clockwise this time, 8 ranges from 27r to 0. 
This integral becomes 
= 0. 
(13.56) 
So the integration around the branch point vanishes. Remember this will not always 
be the case. There is no rule saying the integral around a branch point must vanish. 
Combining the results of these four integrals with Equation 13.50 gives 
(13.57) 
Therefore, our final result is 
- 
- T .  
(13.58) 
1 
m 
13.2 THE METHOD OF STEEPEST DESCENT 
The method of steepest descent, also known as saddle point integration, is a technique 
for approximating line integrals of the form 
(13.59) 
where C is a path in the complex g-plane, as shown in Figure 13.40. The method 
relies on analytic contour deformation to manipulate C so that only a small section 
on the contour actually contributes anything significant to the value of the integral. 
This deformation might be as shown in Figure 13.41. The modified contour C' passes 

THE METHOD OF STEEPEST DESCENT 
543 
IY 
- 
z-plane 
Figure 13.40 Initial Contour 
through a point L, 
where If(z)l has a steep maximum which dwarfs the values of f(z) 
at the other points on the contour. The path C‘ is called the path of steepest descent 
if no other contour can be drawn which has If(g)I decreasing more rapidly as one 
moves away from the maximum. Along this pgth, the integral can be approximated 
by the value obtained by integrating along C’ for only a short distance near G, 
as indicated in Figure 13.42. 
z-plane 
Y 
- 
(1 3.60) 
a 
Distance Along C’ 
Figure 13.41 Steepest-Descent Contour 

544 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
--f 
z-plane 
- 
Y 
Figure 13.42 The Steepest-Descent Approximation 
13.2.1 Finding the Locai Maximum 
To emphasize the steepness of the integrand and facilitate finding the local maximum, 
the integrand - 
f(z) is placed in the form 
(13.61) 
where g(g) 
- is supposed to be a relatively slowly varying function of g, while the 
exponential term &) 
changes magnitude rapidly. Given any - 
f(& the selection of 
g(z) 
- and ~ ( g )  
is not unique. You can always choose 
and 
(13.62) 
(13.63) 
In some cases, however, other choices are more appropriate. 
imaginary parts: 
Because ~ ( z )  
is a complex function, we can write it as the sum of its real and 
so that 
Because &) is varying slowly, the magnitude of f ( z )  is controlled predominately 
by u(x, y>7 Thus, a maximum in the magnitude of &) will occur at the maximum 
of u(x,y), but will be much steeper due to the effkt of the exponential function. 
Because u(x, y) is a function of two variables, its extreme points occur when both 
(13.66) 

THE METHOD OF STEEPEST DESCENT 
545 
X 
Figure 13.43 Local Maximum 
and 
- 0. 
au - -  
dY 
(13.67) 
If the values of x and y that allow the simultaneous solution of Equations 13.66 and 
13.67 are x, and yo, the location in the complex g-plane of an extremum of u(x, y )  is 
given by z, = x, + iy,. 
To determine the nature of a particular extreme point z, = x, + iy,, the second 
derivatives of u(x,y) must be investigated. If we define u, = d2u/dx2, uyy = 
d2u/dy2, and uxy = d2u/dxdy, there are four general possibilities (see for example, 
Thomas and Finney, Calculus and Analytical Geometry): 
u, < 0 and uxxuyy - u:y > 0 
u, > 0 and uxxuyy - u : ~  > 0 
u,uyy - u:y < 0 
u,uyy - u:y = 0 
cannot be determined. 
(13.68) 
You are no doubt familiar with the first two cases, which are simply local maxima 
and minima, as shown in Figures 13.43 and 13.44. The third case might be something 
new to you. For functions of two variables, it is possible to have an extremum which 
looks like a maximum in one direction, but a minimum from another. This type of 
z, is a maximum 
z, is a minimum 
z, is a saddle point 
V 
Figure 13.44 Local Minimum 

546 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
X 
Figure 13.45 Extremum for Analytic Functions 
extremum, is called a saddle point and is shown in Figure 13.45. The fourth condition 
listed in Equation 13.68 says that if the extreme point falls into this category, it is 
impossible to determine the nature of the extremum simply by looking at second 
derivatives. Higher-order terms must be considered. 
Usually in contour integral problems, we are dealing with functions which are 
analytic everywhere, except at isolated points. If we impose the condition that ~ ( z )  
be an analytic function, and consequently obeys the Cauchy-Riemam conditions, an 
interesting requirement is imposed on the extrema of u(x, y). The Cauchy-Riemann 
conditions specify that 
and 
This means that 
(13.69) 
(13.70) 
(13.7 1) 
Therefore, if u, > 0, then uyy < 0, and vice-versa. Looking back at the conditions of 
Equation 13.68, this means all the extrema for an analytic function fall into the third 
or fourth categories. Those in the third category are all saddle points. The ones in the 
fourth category occur when u, and uyy are both zero. These also turn out to be saddle 
points of a more complicated type. We defer the discussion of these cases until the 
end of the chapter. When applying the method of steepest descent to a saddle point, 
the deformed contour needs to cross the saddle such that the contour goes through a 
maxima at 5, not a minima. 

THE METHOD OF STEEPEST DESCENT 
547 
We already determined the extrema of 
are located where x and y satisfy 
Equations 13.66 and 13.67. If we are dealing with analytic functions, these equations 
also imply that &/dx and &/dy will also be zero at these points. Therefore, a simpler 
way to determine the location of the saddle points of analytic functions is by using 
the expression 
du 
dv 
dw 
- + i- 
= 
= 0 
at the saddle points. 
dx 
dy 
dg 
(13.72) 
13.2.2 
The path of steepest descent is the path through the saddle point that makes the peak of 
the curve, plotted in Figure 13.41, as narrow as possible. One simple way to visualize 
this is to think of the area around the saddle point as a mountain pass, and the path of 
steepest descent the shortest way through the pass. Alternatively, it is the path a skier 
would take to descend from the saddle point the fastest. Notice, the course the path 
takes is dependent on the details of the topology. It does not necessarily follow lines 
of constant y or constant x, and in general is not even a straight line. 
A useful way to picture the path of steepest descent is on a two-dimensional contour 
plot, such as the one shown in Figure 13.46. Lines of constant u(x, y) are plotted, with 
the solid lines used for u(x, y) > u(xo, yo) and dashed lines for u(x, y) < u(xo, yo). 
The saddle point (SP) is located at (xo,yo). The path of steepest descent passing 
through the saddle point has also been plotted. Notice how the path of steepest decent 
always intersects the dashed lines of constant u(x, y) at right angles. 
Determining the Path of Steepest Descent 
13.2.3 Evaluation of the Integral 
Now we will attempt to approximate the contour integral along this path by only 
considering the contribution of a short section of the path around the saddle point. 
Y 
X 
XO 
Figure 13.46 Lines of Constant u(x, y )  

548 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
X 
xo 
Figure 13.47 Contour for the Steepest-Descent Approximation 
Near the saddle point, the path can be approximated by a straight-line segment, 
tangent to the actual path of steepest descent at the saddle point. This is shown as the 
path C" in Figure 13.47. So the original integral, Equation 13.59 evaluated on the 
contour C, is exactly equivalent to the integral along C' of Figure 13.41, as long as 
the integrand is analytic between C and C'. This value is approximately equal to the 
integral along C" of Figure 13.47: 
(13.73) 
It is useful to modify the form of the integrand of Equation 13.61 slightly, by adding 
a constant factors > 0 in front of the function ~ ( 3 ) .  
This makes the integration we 
want to perform a function of s: 
(13.74) 
Essentially, the value of s is a measure of the validity of the approximation. As s 
increases, the steepness of the integrand's peak also increases, making the integral 
along C" a better approximation to the integration along the original contour C. 
Along the short contour CN, the value of z can be written 
3 = z, + 6eiao, 
(13.75) 
where z+, 
is the saddle point position, S is a small displacement, and a. is a constant 
angle. If the length of C" is 2 ~ ,  
as shown in Figure 13.48, then 6 ranges from - E  
to + E  as z moves between the end points of CN. The value of a, is the angle, with 
respect to the x-axis, that makes C" tangent to the path of steepest descent. We will 

THE METHOD OF STEEPEST DESCENT 
549 
Figure 13.48 Definition of the Angle a, 
present a method for determining this angle shortly. We can now change the variable 
of integration of Equation 13.74 from z_ to 6 by substituting Equation 13.75 and using 
dz_ = e'"Od8. 
(13.76) 
The approximation in Equation 13.74 becomes 
Since we assumed g(z> to be a slowly varying function, it can be approximated by 
a constant value &,,for 
the C" integration and therefore can be pulled outside of 
the integral. When a Taylor expansion for ~ ( g )  
is made about G,, 
we obtain the series 
which, written in terms of 6, becomes 
(13.79) 
Notice because d"t./dzlg = 0, the lowest-order &dependence is 6'. If we approxi- 
mate the function ~ ( 6 )  
by taking only terms up to the lowest order in 6, and combine 
this with the constant approximation for g(g), we obtain 
(13.80) 

550 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
This result assumes that the second derivative, which appears in the exponent of the 
integral, is not zero. Later in the chapter, we will briefly discuss a situation where 
this is not the case. 
The only task which remains is to determine the angle cw,. The path of steepest 
descent follows the line which is perpendicular to the line of constant u(x,y) that 
passes through the saddle point. From our previous work with analytic functions of 
complex variables, we know that this path is also along a line of constant u(x, y), 
because the lines of constant v(x,y) are always perpendicular to lines of constant 
u(x, y). This implies that, along the path of steepest descent, the imaginary part of 
w(z) is a constant equal to its value at the saddle point. Consequently, on the path of 
steepest descent, where we have approximated ~ ( 5 )  
by 
the imaginary part of ~ ( z )  
is entirely contained in the IV(G~) term and 
' 5 p  
must be a pure real quantity. Since 6 is a pure real quantity, that means that 
(13.81) 
(13.82) 
(13.83) 
must be a pure real quantity. In general, d2y/dg21Gp is a complex quantity, which we 
write using the exponential representation as 
In order for Equation 13.83 to be pure real, 
OO+2q,= 2 n v  
n = 0 , 1 , 2  ,.... 
If n is an odd integer, Equation 13.80 becomes 
while if n is zero or an even integer, 
(13.84) 
(13.85) 
(13.86) 
(13.87) 

THE METHOD OF STEEPEST DESCENT 
551 
Because we required s to be a positive real quantity, the integrand of Equation 13.86 
drops off as 6 moves from zero, that is, as z moves away from the saddle point. 
Therefore, this equation is an approximatation for the integration along the path of 
steepest descent. In contrast, the integrand of Equation 13.87 grows as S moves from 
zero, and approximates the integration along a path of steepest ascent. This is not the 
path we want. With n = 1, a, must be given by 
(13.88) 
The other odd values of n in Equation 13.85 will not generate any new values for 
a,. The ambiguity of the % sign must be determined by the direction taken by the 
undeformed contour C and the topology of u(x,y), which controls the way C is 
deformed to C’. The deformation must be such that C’ stays in the “lowland,” except 
when it crosses the saddle point. 
Equation 13.86 is almost the final result. The effect of the s parameter is now 
clear. The integrand of this expression is a Gaussian whose width is controlled 
by s. The larger s becomes, the narrower the Gaussian spike becomes, making the 
approximation more valid. This effect allows one last simplification of the result. 
If the Gaussian is narrow enough, the contributions from extending the limits of 
integration to minus and plus infinity will be negligible. With this approximation, the 
integration in Equation 13.86 can be evaluated to give the result 
(13.89) 
After all these approximations, you might wonder if the result could possibly 
bear any resemblance to the actual value of the original integral. If s is large enough 
and the deformation to a path of steepest descent can be accomplished, the approx- 
imations turn out to produce, in many cases, amazingly accurate results. Thls will 
be demonstrated in the examples that follow and in the exercises at the end of this 
chapter. 
Example 13.9 
evaluation of Laplace transforms and their inverses. For example, the transform 
One frequent application of the method of steepest descent is the 
r m  
(13.90) 
can be approximated using the method of steepest descent for positive real values 
of s. The first step is to place the integral in steepest descent form, i.e., 
To accomplish this, convert the transformation to an integral in the complex plane 
by letting t become z. The initial integration is then changed into a contour integral 

552 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
C 
X 
Figure 13.49 Contour for Laplace Transforms 
along the positive real z axis, 
as shown in Figure 13.49. 
possibilities. Probably the simplest approach is to get g(z) 
_ -  = 1 and 
Now we need to identify the two functions g(z) and &J. 
There are several 
so that 
(13.94) 
A more general approach would allow f(g) to be broken up into two parts, a slow- 
varying part g(z) and a fast-varying part h(z). Then we would set 
f(z) = g(z)h(z), 
(13.95) 
with 
(13.96) 
Sticking with our original choice of Equation 13.94, the Laplace transform integral 
becomes 
( 1 3.97) 
The next step is to find the saddle point. This is done be setting dw/dg = 0 and 
solving for z. In this case, the saddle point is determined from the condition 
(13.98) 

THE METHOD OF STEEPEST DESCENT 
553 
To complete the approximation, we need to determine the second derivative of ~ ( z )  
at the saddle point, and identify the values of W t  and a, to use in Equation 13.89. 
These steps are straightforward, but the details are of course determined by the nature 
of the function - 
f(g). 
Example 13.10 As a more specific example of the use of the method of steepest 
descent, we will work out an approximate expression for the r-function, which is 
defined by the integral expression 
(13.99) 
For positive integer values of s, this turns out to be intimately connected with the 
factorial function: 
r(s + 1) = s!. 
(1 3.100) 
The r-function can be used to define factorials of negative and noninteger numbers. 
To apply the method of steepest descent, we extend the integral into the complex 
plane by replacing p with I, 
r(s + 1) = J cigfe-z, 
(13.101) 
where again C is along the positive real axis, the same contour shown in Figure 13.49. 
The integral for the r-function must now be put into steepest descent form, so that 
w(z) and g(z) can be identified. Remember the desired form is 
C 
- 
- 
ryS + I) = 
dzg(z)eS”‘z)’. 
(1 3.102) 
Neither zs nor 0 
appear to be slowly varying functions and so we will set g(g) = 1. 
This is always a safe choice. That means that ~ ( 2 )  
in Equation 13.102 becomes 
1 -  
(1 3.103) 
Taking the derivative of this function with respect to g and setting it equal to zero 
gives the location of the saddle point: 
Z 
- 
w(z) = In@ - y .  
(13.104) 
- 
z, - s. 
Because s is supposed to be a positive real quantity, the saddle point is on the original 
contour, and no deformation is necessary to make the contour cross it. 
It is pretty clear that a, should be zero, but let’s see how this falls out of the 
method. To find a, we first evaluate the second derivative of ~ ( z )  
- at the saddle point. 

554 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
This gives 
( 13.105) 
This means that 
(13.106) 
and 
0, = n-. 
(1 3.107) 
With this value for 6,, Equation 13.88 gives two values for a,, cu, = 0 and a, = -T. 
If we were to use the value a, = -n-, the contour would pass through the saddle 
point from right to left. Since the original integration was from left to right, the a = 0 
solution should be selected. 
With these values for z+, WL’, 
a,, and the y(iJ and go 
- functions, Equation 13.89 
gives the result: 
(1 3.108) 
This is the simplest form of Stirling’s approximation. 
13.2.4 
Higher-Order Saddles 
Back in Equation 13.80, where we performed a Taylor series expansion of the function 
- 
w(z) around the saddle point, we assumed the first nonzero term involved the second 
derivative. The result of wuation 13.89 and the validity of all of our examples relied 
X 
Figure 13.50 A Three-Legged Saddle 

EXERCISES 
555 
on this fact. An interesting situation occurs if the second derivative, evaluated at 
the saddle point, turns out to be zero. Then we must go to higher-order terms in 
the Taylor series to get the lowest 6 dependent term needed for Equation 13.80. 
Interesting saddle structures occur in these cases. They are saddles for riders with 
more than two legs! An example of the saddle formed when the first nonzero term 
is the thud derivative is shown in Figure 13.50. Notice how a single, straight-line 
contour cannot approximate the line of steepest descent near the saddle point in this 
case. 
EXERCISES FOR CHAPTER 13 
1. Consider the complex function 
(a) How many z-plane Riemann sheets are required to describe this function? 
(b) Locate the branch points and branch cuts in the g-plane and show how the 
(c) How many w-plane Riemann sheets are required to describe this function? 
(d) Locate the branch points and branch cuts in the w-plane and show how the 
g-plane Riemann sheets are connected. 
w-plane Riemann sheets are connected. 
2. The function 
is represented on two sheets in the z-plane and two sheets in the y-plane. Place 
the cut on the real axis of the z-plane and indicate where each sheet of the g-plane 
maps onto the two E-plane sheets. 
3. Consider the complex function 
(a) Locate the branch points and branch cuts of this function in the g-plane. 
(b) How many Riemann sheets are required for the g-plane? 
(c) How are these Riemann sheets connected? 
(d) How many Riemann sheets are required for the y-plane? 
4. Consider the complex function 
w = In (-) 
z+ 1 
- 
g - 1  
(a) Identify the branch points and locate the branch cut(s) in the g-plane for this 
function. 

556 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
(b) How many z-plane Riemann sheets are required to represent this function? 
(c) How are the Riemann sheets connected? 
(d) Draw several closed contours, which span multiple sheets in the z-plane. 
5. Locate the branch points and branch cuts in the g-plane and describe the connec- 
tion of the Riemann sheets for the following functions: 
i. w = ln(2 - 1). 
ii. y = g lnz. 
iii. y = (2 - z)'/'. 
iv. w = z112 + (z - 1)'12. 
6. For each of the following functions: (1) Idenhfy the number of Riemann sheets 
in the complex z-plane and the complex y-plane. (2) Locate all the branch points 
and position the branch cuts. (3) Show where the z-plane maps onto the w-plane 
and vice versa. (4) Indicate how the Riemann sheets are connected: 
i . w = z  1/4 . 
ii. w = 213. 
iii. g = cosz. 
v. y = hl(2 + 1). 
2 
iv. g = (lnz) . 
7. The function (z - 1)'l2 has branch points at 2 = 1 and at infinity. For this 
problem, take the cut to be along the positive real axis and evaluate the integral 
The contour C is the circular path of radius 2, centered at z = 0, that starts 
just above the cut and ends just below the cut on the first Riemann sheet of the 
z-plane, as shown below: 
C
A
 
Sheet 1 
f 
! 
\ 
X 
To evaluate this integral show that the contour can be deformed to the one shown 
below: 

EXERCISES 
557 
Y 
Sheet 1 
Repeat the evaluation of the integral if C is assumed to be on the second kemann 
sheet of the z-plane. 
8. Consider the real integral 
Indicate a closed contour in the complex z-plane that can be used to evaluate 
this integral. Explain your reasoning by showing the pole(s), branch points, and 
branch cut(s) of the integrand and discussing the contributions from the segments 
used to accomplish closure. You do not have to actually evaluate the integral. 
9. Evaluate 
by extending the integral into the complex plane and using the contour C shown 
below. 
Y 
10. Consider the complex function 

558 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
(a) Identify the branch cuts, branch points Riemann sheets, and singularities of 
this function in the g-plane. 
(b) Now consider the real integral 
dx 
1 i (x2 - x)'/2(x + 2)' 
Extend this integral into the complex plane and identify an appropriate 
contour to evaluate it. 
(c) Evaluate the integral in part (b). 
the real integral 
11. Describe the branch cuts and branch points of the function (1 -g2)'/' 
and evaluate 
dx 
12. Evaluate the real integral 
dx 
b > a > O .  
J' 
-1 (x2 - 1)1/2(ax + b) 
13. Consider the function 
= g2 + 1. 
(a) Determine the location of the saddle point for this function. 
(b) Expand I&.) in a Taylor series about the saddle point and determine ao, 
the 
angle of a straight line passing through the saddle point tangent to the path 
of steepest descent. 
(c) Repeat steps (a) and (b) above for the function w = g3 + 1. 
14. Consider the function z 
22 
w = 
+cosg. 
-
2
 
(a) Show that this function satisfies the Cauchy-Riemann conditions. 
(b) Identify the location of a saddle point. 
(c) Describe the paths of steepest ascent and descent through this saddle point. 
15. Consider the function 
w = 23 + 3g. 
-
-
 
(a) Where in the complex z-plane does this function have saddle points? 
(b) Identify the lines in the z-plane along which Real M = 0. 
(c) The lines determined in part (b) divide the z-plane into six regions. For each 
region, indicate whether the real part of 411 is greater or less than zero. 

EXERCISES 
559 
(d) Near each of the saddle points, determine the directions of the paths of 
(e) Using the method of steepest descent, determine the approximate value of 
steepest descent and steepest ascent through these saddle points. 
the real integral 
where s is a large, positive real number. 
16. Let 
w = sing. 
(a) Where are the saddle points of this function in the g-plane? 
(b) Identify the paths of steepest descent and ascent through these saddle points. 
(c) Make a plot of the lines of constant Real W in the complex plane and label 
several of the contours to clearly show the topology. Also draw the lines of 
steepest descent and ascent on this plot. 
17. Consider the function 
w = sing - g. 
(a) Show that this function has saddle points at z = 2 m  where n = 0,+1, 
22, ... . 
(b) Expand the function in a Taylor series about the saddle point at z = 0. What 
is the second derivative of w at this point? 
(c) What is the value of the function at 
= O? Use this value to divide the 
?-plane into regions around g = 0, where the real part of the function is 
either larger than or smaller than its real part at z = 0. 
(d) Determine all the possible paths of steepest descent and steepest ascent that 
pass through g = 0. 
18. Use the method of steepest descent to approximate the value of the integral 
Z(x) = 
dtexr-" 
for large, positive x. 
19. Consider the real integral 
la 
dx xe-sxz, 
where s is a real positive number. 
(a) Evaluate this integral exactly using the fact that the integrand is an exact 
differential. 

560 
ADVANCED TOPICS IN COMPLEX ANALYSIS 
(b) Now evaluate the integral by the method of steepest descent and compare 
this approximate answer to the exact expression obtained in part (a). 
20. Use the method of steepest descent to approximate the Laplace transform of 
and compare it to the exact transform. Now repeat the same process for the 
transform of 
where n is any positive integer. 
21. Consider the integral 
y+im 
Z(x) = - ‘J ds-, 
where the path of integration is along a vertical line to the right of the origin in 
the 5-plane. 
(a) Locate the branch points of the integrand in the s-plane. 
(b) Place the branch cut so that the path of integration will not cross it. 
(c) Evaluate Z(x) for negative x. 
(d) For large positive x ,  use the method of steepst descent to approximate Z(x). 
2m’ 
y-im 
S 
22. According to transform tables, the inverse Laplace transform of 
is 0 for t < 0, and equals 
1 - 
sin (2Jcrt) 
d= 
for t > 0. Use the method of steepest descent to approximate 
Contour plots for the constant real part of WJSJ 
help to see how to deform the 
path of integration. When you deform the Laplace contour C remember to take 
into account the branch cut, branch points, and the singularity of the integrand. 
The integrand has a slowly varying function of 8, and both saddle points of 

EXERCISES 
561 
- 
w(s) should be used. How does your approximate answer compare to the correct 
answer for large values of t? What would have happened if you had used only 
one saddle point? If possible, use a computer to make a 3D plot of the real part 
of _w(s). 
23. Use the method of steepest descent to find an approximation for the inverse 
Laplace transform of 
Show the paths of steepest ascent and descent through the saddle point(s) and 
discuss the deformation of the path of integration. Also, include comments on 
any branch points and branch cuts in your solution. Compare your answer to the 
exact value of the inversion, which is 
Now repeat this process for the inversion of 

TENSORS IN NON-ORTHOGONAL 
COORDlNATE SYSTEMS 
This chapter continues our discussion of tensors. It begins with a brief review of tensor 
transformation properties, which was covered in more detail in Chapter 4. Then we 
tackle the complications that arise in non-orthononnal coordinate systems. The ideas 
of contravariance, covariance, and the metric tensor are developed to handle inner 
products in non-orthonormal systems. 
14.1 A BREF REVIEW OF TENSOR TRANSFORMATIONS 
In Chapter 4, we discussed how a tensor is defined by its behavior under coordinate 
transformations. With only a hint of sarcasm, the definition was given by the state- 
ment, “A tensor is any quantity that transforms as a tensor.” What this means is that 
transformation rules are enough to give tensors all of their special properties. If an 
object transforms between coordinate systems using tensor transformation rules, you 
can legitimately say the object is a tensor. 
Remember, the elements of a tensor can be transformed using a transformation 
matrix whose elements can be obtained from the equations relating the coordinates of 
the two systems under consideration, For transformations between Cartesian systems, 
the elements of this transformation matrix [a] are given by 
(14.1) 
In this equation, the original system has coordinates xi and basis vectors @i. The system 
being transformed to, the primed system, has coordinates xi‘ and basis vectors 6;. For 
orthonormal coordinate systems, the inverse of this transformation matrix is always 
562 

A BRIEF REVIEW OF TENSOR TRANSFORMATIONS 
563 
given by its transpose 
a;’ = aji. 
(14.2) 
An arbitrary tensor of rank n can be expressed in either the unprimed or primed 
coordinate system as 
- 
- 
(14.3) 
where there are n subscripts and n basis vectors in each term. Tijk... and Tist.,, are the 
elements of the tensor in the unprimed and primed coordinate systems, respectively. 
The two sets of elements are related to each other via the transformation matrix 
equation 
..I../../ 
T = T . .  
tjk ... &.@.& 
I 
1 k .  . . = TAt..,erese,. . . , 
T:sr... = Tijk ...aria 
sjatk. . . t 
(14.4) 
where the matrix [a] appears n times. The inverse transformation is 
Tm ... = Ti>k ...a irajsah + . . . 
(14.5) 
Our fundamental assertion is that any quantity that transforms in the manner described 
by Equation 14.4 is by definition a tensor. 
Because a vector is a first rank tensor, vector component transformations are also 
described by Equations 14.4 and 14.5. If we write the vector in the two coordinate 
systems as 
= V.&. 
1
1
 = V’@’ 
r r )  
(14.6) 
the relation between the components is given by 
V,! = Via,. 
(14.7) 
and the inverse 
v 
r = V!a. 
I 
ir. 
(14.8) 
Scalars are invariant under a coordinate transformation. We can think of a scalar 
as a zero rank tensor. The single element of a zero rank tensor has no subscripts and 
is not combined with any basis vectors. Equation 14.4 reduces to 
s = s’ 
(14.9) 
where S (or S’) is the single element of the scalar. 
Example 14.1 As an example of using transform properties to identify an object as 
a tensor, consider the Kronecker-6. Recall that this symbol was introduced to form dot 
products in orthonormal coordinate systems. The dot product between two vectors, 
a and B, written in two different orthonormal coordinate systems, one unprimed and 

564 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
one primed, can be written 
;C . B = A.B 
1 
I .6.. 
1J = ~ ' ~ ' 8 1  
r s rs' 
(14.10) 
Now we already know that Si, and a,!, 
can both be expressed as unit matrices, that 
is as 111. For the purposes of this discussion, however, let's look at the consequences 
of just requiring that both expressions for the inner product in Equation 14.10 be 
equal, and that Ai and Bi are vector components, and therefore transform according 
to Equation 14.7. Substituting into Equation 14.10 for A: and Bj gives 
A.B.6.. = a .A.a .B.6/ 
I 
I 
IJ 
n 1 SI 
J 
rs- 
= A.B.a 
1 
J n .a 
SJ .St 
rs' 
(14.1 1) 
Because this expression must be true for any 
and B, we can write 
6.. 
11 = a n .a SJ .a' rs. 
(14.12) 
Inverting this expression gives 
6/j = airajsars. 
(14.13) 
comparing Equations 14.12 and 14.13 with Equations 14.4 and 14.5, it can be seen 
that the elements of the Kronecker-8 transform like the elements of a second-rank 
tensor. Therefore, the Kronecker-8 symbol is a second-rank tensor, which can be 
expressed with basis vectors as - 
- 
6 = 8. 
I J ' J  
.&C. = a!.@!&!. 
1 1 1 1  
(14.14) 
14.2 NON-ORTHONORMAL COORDINATE SYSTEMS 
Up to this point, we have dealt only with orthonormal coordinate systems. In Cartesian 
systems, the basis vectors Ci are independent of position and orthonormal, SO Ci . 
C j  = &,. In curvilinear systems, the basis vectors qi are no longer independent of 
position, but they are still orthonormal, so qi * q, = 6,. Now we will consider non- 
orthonormal systems. To distinguish these systems, we will label the basis vectors 
of non-orthonormal coordinate systems as &, and the non-orthonormality condition 
becomes gi . g j  f 6ij. To keep the discussion and derivations as simple as possible in 
this chapter, we will limit ourselves to coordinate systems which have basis vectors 
that do not vary with position. This is obviously not the most general type of non- 
orthonormal coordinate system, but it is enough to demonstrate the important ideas 
of contravariance, covariance, and the metric tensor. In Appendix F, we present some 
of the generalizations required to describe non-orthonormal systems with position- 
dependent basis vectors. 
In physics, non-orthonormal coordinate systems appear, for example, in both 
special and general relativity. The basic postulate of special relativity is that the 
speed of light c,, is the same in all inertial reference frames. As a consequence of 

NON-ORTHONORMAL COORDINATE SYSTEMS 
565 
,’ ct’ 
i ct 
/ 
I 
/ 
An “event” 
/ 
/ 
Figure 14.1 The Coordinate Systems of Special Relativity 
this postulate, both the position and time coordinates of some physical phenomenon 
(an “event”) change as you change reference frames. This is very similar to how the 
components of a vector change as you transform the coordinate axes used to describe 
them. If we restrain the motion to be along a single spatial direction, an event can be 
described by two coordinates, a single position coordinate and time. As will be shown, 
the observation of an event in two different coordinate systems, one primed and one 
unprimed, can be plotted as a single point using the combined set of axes as shown in 
Figure 14.1. By taking its components with respect to both coordinate axes, we can 
get all the relationships imposed by special relativity. Notice how the x and ct axes 
intersect at right angles, but the x‘ and ct’ axes do not. While the unprimed system 
appears to be orthogonal, the primed system looks like a non-orthogonal, skewed 
system. We will discuss this picture, and its interpretation for special relativity, in 
some detail in the sections that follow. 
The basic postulate of general relativity is that gravity and acceleration are equiv- 
alent. Events observed in a gravitational field appear as if they were being observed 
in an accelerating coordinate system. This implies that light propagating through the 
gfavitational field of a massive object, such as a star, should be bent, as depicted in 
Figure 14.2. This would cause the apparent position of a star to deviate from its actual 
~ 
- - _  
Local nonorthogonal 
Apparent 
coordinate system 
star 
Figure 14.2 
A Coordinate System of General Relativity 

566 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
location. This phenomenon was first observed by Arthur Eddington, who measured 
the small deflection of stars by the sun during the total eclipse of 1919. The paths that 
light rays follow through space are called geodesics. A natural choice for the grid lines 
of a localized, coordinate system follows these geodesics, as shown in Figure 14.2. 
The basis vectors in such a coordinate system are both non-orthonormal and spatially 
dependent. We will not discuss examples of this type of system, but instead restrict 
our comments to "skewed" systems, where the basis vectors are not orthonormal, 
but are spatially invariant. The interested reader can investigate the very heavy book 
Gravitation by Misner, Thorne, and Wheeler for an introduction to general relativity. 
14.2.1 A Skewed Coordinate System 
Consider the two-dimensional, Cartesian system (XI, x2) and the non-orthonormal, 
primed system (n{,x:) shown in Figure 14.3. Two pairs of basis vectors and an 
arbitrary vector v are also represented in the figure. The basis vectors of the unprimed 
system are orthonormal: 
&. 
1 . & .  
J = 6.. 
I J .  
(14.15) 
The basis vectors of the skewed system are taken to be unit vectors, 
g; . g; = g; . g; = 1, 
but they are not orthonormal: 
2' 
I 
, 
1' 
1 
Figure 
@I 
Vl 
14.3 An Orthonormal and a Skewed Coordinate 
(14.16) 
(14.17) 
System 

NON-ORTHONORMAL COORDINATE SYSTEMS 
567 
As you will see, the formalism that is developed here does allow for systems with 
basis vectors that are not unit vectors. However, to start out as simply as possible, we 
assume the basis vectors of the skewed system satisfy Equation 14.16. 
In the orthonormal system, the vector v can be expressed as the sum of its axis 
parallel projected components, shown in Figure 14.3, coupled with the corresponding 
basis vectors: 
v = v, 61 + v, 62. 
(14.18) 
These vector components are just the projected lengths of v along the axes of the 
unprimed system and can be determined with simple trigonometry or by following 
straightforward vector manipulation. A particular component is obtained by the dot 
product between the vector v and the corresponding basis vector. For example, to 
find Vl: 
( 1 4.1 9) 
This works out nicely because of the orthonormality of the basis vectors. 
projected components and the primed basis vectors, as shown in Figure 14.3, 
In the primed system, the same vector can be written in terms of its axis-parallel 
- 
v = v: g; + v; g;. 
(14.20) 
These primed vector components can also be determined from trigonometry, but 
because of the skewed geometry, this is no longer as simple as it was in the orthogonal 
system. Because the primed basis vectors are not orthogonal, an attempt to isolate 
a particular primed component by a vector manipulation similar to Equation 14.19 
fails: 
- 
v .  g; = (Vl g; + v; g;). g; 
= v;<g; . g;) + v;(g; . g;) 
= v: + v;(& . g;) 
z v:. 
(14.21) 
It appears that vector manipulations in non-orthonormal coordinate systems are 
much more difficult than in orthonormal systems. Fortunately, there are some formal 
techniques which simplify this process. In the next section, we introduce the concepts 
of covariance, contravariance, and the metric tensor. Using these devices, the dot 
product between two vectors has the same form in an orthogonal system as in a 
non-orthogonal system. 

568 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
14.2.2 Covariance, Contravariance, and the Metric 
The basic complication introduced by a non-orthonormal coordinate system is evident 
in the dot product operation. In the two-dimensional orthonormal system described 
above, the inner product between two vectors is given by 
- -  
A .  B = A,@. . B . @ .  
1
1
 
I
1
 
= AiB .6.. 
J 
CJ 
= AlBl + A2B2. 
(14.22) 
If this same inner product is performed in the non-orthogonal system of Figure 14.3, 
the result contains some extra terms: 
A. B = 
B! g'. 
= A ! B ! ( ~ ; .  
g;) 
= A;& + AiBi + (AiBi + AiB[)(gi . gi). 
J
J
 
1
1
 
(14.23) 
The inner product formed in the non-orthononnal system, as expressed in Equa- 
tion 14.23, can be placed in the from of Equation 14.22 by rearranging as follows: 
x 
* B = Ai[Bi + Bi(gi . gi)] + Ai[B[(gi * gi) + Bi]. 
(14.24) 
Now define a new set of components for B as 
(14.25) 
These quantities are called the covuriunt components of B, while the original compo- 
nents are called contruvuriunt. Clearly, the vector cannot be expressed by combin- 
ing these new covariant components with the basis vectors g; and g:: 
E # Big: + Big;. 
(14.26) 
However, with these components, the inner product evaluated in the skewed system 
can be placed in the simple form 
x. 
B = @,! 
= A;B{ + A!$:. 
(14.27) 
Notice that the inner product could also have been written as 
_ -  
A * B = A,'BI, 
(14.28) 
with the covariant components of 
defined as 
A' , - 
- Ai + Ai(gL * g:) 
& = A;(g{ . gi) + A;. 
(14.29) 

NON-ORTHONORMAL COORDINATE SYSTEMS 
569 
The inner product needs to be formed with mixed contravariant and covariant com- 
ponents, but it does not matter which vector is expressed with which type. 
These arguments can be extended to non-orthogonal systems of arbitrary dimen- 
sions. The restriction that the basis vectors be normalized to unity can also be dropped. 
The covariant components can be generated from the contravariant components using 
the general expression 
= AS(g,' . g'.) 
J '  
(14.30) 
We have used the subscript summation convention here to imply a sum over the index 
j .  For an n-dimensional coordinate system, there will be n terms in each sum. Notice 
that if the coordinate system is orthonormal, Equation 14.30 reduces to 
= A;, and 
the covariant and contravariant components are equal. In this case, both Equations 
14.27 and 14.28 will revert back to Equation 14.22. This is important, because it 
implies this new notation is general enough to handle all of our previous Cartesian 
and curvilinear coordinate systems, as well as the new non-orthonormal ones. 
There is another way of expressing the inner product between two vectors in a 
non-orthonormal system that makes use of a quantity called the metric. As will be 
shown later, the metric turns out to be a second-rank tensor. The elements of the 
metric, in an unprimed system, are defined as 
M . .  
IJ E g . .  
1 & .  
J '  
(14.31) 
Notice this definition implies that the metric is symmetric: 
M . .  = M . .  
(14.32) 
'J 
J' ' 
Using the metric, Equation 14.30 can be written as 
Ai 
AjMij. 
(14.33) 
The metric converts the contravariant components to covariant components. 
Now the inner product between 
and B can be written as 
- -  
A * B = AiBjMij. 
(14.34) 
A sum over both i and j is indicated on the RHS of this equation. If the sum over i is 
performed first, Equation 14.34 becomes 
- -  
A * B = AjBj. 
(14.35) 
When the sum over j is performed first, Equation 14.34 instead becomes 
- -  
A . B = AiBi. 
(14.36) 
When Equation 14.34 is used for the inner product, the vector components are not 
mixed. Contravariant components are used for both. If the system is orthonormal, 
Mij = 6ij, and the standard inner product for orthonormal systems results. Notice 

570 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
that the metric is determined solely by the basis vectors of the coordinate system. 
This turns out to be an important fact, and will allow us to identify the metric as a 
second-rank tensor. 
In summary, there are two ways to perform the inner product between two vectors 
in a non-orthonormal system. One way is to use the covariant and contravariant 
components, as was done in Equations 14.27 and 14.28. A completely equivalent 
method is to use the metric and the regular contravariant vector components, as 
demonstrated in Equation 14.34. These arguments can be naturally extended to inner 
products between tensor quantities, but this generalization will be postponed until 
the transformation equations for non-orthonormal systems are worked out. 
14.2.3 Contravariant Vector Component Transformations 
Imagine two different skewed coordinate systems, as shown in Figure 14.4. We 
want to find out how the contravariant components of a vector expressed in the first 
system can be transformed into the second system. The first system has the unprimed 
coordinates xi and the basis vectors &. while the second system uses the primed 
coordinates xi and basis vectors gi. Remember, we are still limiting ourselves to 
coordinate systems with constant basis vectors. Let the general equations which 
relate the two sets of coordinates to one another be 
xi' = X((X1, x2, xg) 
xi = Xi(.[, xi, xi). 
(14.37) 
There will be one pair of equations for each dimension of the systems. 
In our previous work dealing with transformations between orthonormal coordi- 
nate systems, we were able to relate vector components of one system to another via 
1 
Figure 14.4 Two Skewed Coordinate Systems 

NON-ORTHONORMAL COORDINATE SYSTEMS 
571 
[a], the transformation matrix: 
v! 
I = a . . V .  
11 
1. 
(14.38) 
The restriction to orthonormal systems allowed us to invert this expression easily, 
because it turned out that a;' = aji. We can still write a relationship similar to 
Equation 14.38 for transformations between non-orthonormal systems, but we need to 
be more careful, because the inverse of the transformation matrix is no longer simply 
its transpose. To keep tabs on which transformations have this simple inversion and 
which do not, we will reserve the matrix [a] for transformations between orthonormal 
systems. The matrix [t] will represent transformations from unprimed coordinates to 
primed coordinates, where the systems may be non-orthonormal: 
v! 
I = t..V. 
11 
I '  
(14.39) 
The reverse operation, a transformation from primed coordinates to unprimed coor- 
dinates, will use the matrix [g], 
v. 
1 = g..v! 
11 
I' 
(14.40) 
where gij = tl<' # t j i .  By their very definition, it follows that ti$$ 
= &. We will 
discuss the relationship between the [t] and [g] matrices in more detail later. In both 
these expressions, the vector components are the regular, contravariant components 
of v, not the covariant components we introduced earlier. 
All the vectors at a given point transform using the same [t] matrix. To determine 
the ti, elements, it is easiest to consider the displacement vector dF, which in both 
coordinate systems is given by 
Applying this equality to Equation 14.39 gives 
dX; = tijdxj. 
(14.42) 
Referring to Equations 14.37 gives the relationship 
and the transformation matrix elements can be written as 
(14.43) 
(14.44) 
So far, these results look very similar to those of the Cartesian transformations of 
Chapter 4. In fact, the equation for the components of [t] given in Equation 14.44 is the 
same result obtained for the [a] matrix between Cartesian systems. The complications 

572 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
arise when we try to invert these equations. As we mentioned before, the inversion 
of [t] is no longer simply its transpose. A general way to obtain [tl-', what we are 
calling [g], is to use the expression 
where c j i  is the ji-cofactor of the tij matrix. From matrix algebra, this cofactor is 
defined as (- l)i+j times the determinant of the t i j  matrix, with the jth row and the 
ith column removed. The [g] matrix can also be obtained from the equations relating 
the coordinates in exactly the same manner that led to muation 14.44: 
(14.46) 
The [ t ]  and [g] matrices can also be used to relate the basis vectors to one another. 
Using the contravariant components, any vector v can be expressed in the primed or 
unprimed systems as 
Substituting Equation 14.39 into Equation 14.47 gives 
Since this expression must be true for any v, we must have 
g j  = t i j  g;. 
(14.49) 
Following the same steps above, using [g] instead of [ t ]  gives 
(14.50) 
Notice the contravariant vector components are transformed by contractions over 
the second subscript of either ti, or gi,, while the basis vectors are transformed by 
contracting over the first subscript. 
To summarize the results of this section, the transformation between the two 
non-orthonormal coordinate systems is governed by the relations 

NON-ORTHONORMAL COORDINATE SYSTEMS 
573 
14.2.4 
Subscript/Superscript Notation 
Before proceeding with a discussion of how covariant vector components transform, 
it turns out to be convenient to introduce some new notation. The tilde (vi) 
notation we 
have been using for the covariant vector components is clumsy. It is not obvious that 
the following conventions are much better, but they do provide a valuable mechanism 
for keeping track of which type of component (contravariant or covariant) should 
be used in an expression. The standard, axis-parallel projected vector components, 
which we have called the contravariant components, will now be labeled with a 
superscript, while the new covariant components will use a subscript instead of a 
tilde. For example, the contravariant components of the vector v are V', while the 
covariant components are Vi. 
One advantage of this new notation is evident by looking at the form of the inner 
product. With the superscripthubscript convention, we can write the dot product of 
XandBas 
Notice the index being summed over appears once as a subscript and once as a su- 
perscript. This, of course, is the same as saying that the sum is done over mixed 
contravariant and covariant quantities. This process of mixed superscripts and sub- 
scripts will persist for almost all contractions over a repeated index. It even works 
in forming a vector from its components with the proper interpretation of the ba- 
sis vectors. We know that the vector can be formed with the contravariant vector 
components and the basis vectors: 
- 
v = v'gi. 
(14.52) 
To be consistent with the subscriptlsuperscript convention, the basis vectors must be 
labeled with subscripts and be considered covariant. We will see, in the next section, 
that this conclusion is consistent with the way these basis vectors transform. 
This convention also prevents us from accidently forming a vector by combining 
its covariant vector components with the & basis vectors: 
- 
v # vigi. 
(14.53) 
The notation warns us that this is not correct because both indices appear as subscripts. 
In the previous section we generated several relations which described how the 
contravariant components of a vectorv transform between two skewed systems. How 
should the presentation of these results be modified to be consistent with the new 
superscriptkubscript convention? In the previous section we had written 
v! 
I = t..V. 
IJ 
I' 
(14.54) 
Now these vector components need to be superscripted. To be consistent with this 
new notation, one of the indices of the transformation matrix needs to be a subscript 

574 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
and one a superscript, 
where 
In a similar manner, the inversion of Equation 14.55 becomes 
where 
(14.56) 
(14.57) 
(14.58) 
In Equations 14.56 and 14.58, notice how the superscripted component in the de- 
nominator of the partial derivative results in a subscripted index on the LHS of 
this expression. This is a general property of partial derivatives with respect to con- 
travariant and covariant quantities. A partial derivative with respect to a contravariant 
quantity produces a covariant result, while a partial derivative with respect to a 
covariant quantity gives a contravariant result. We will prove this fact later in the 
chapter. 
These transformation matrices have what is called mixed contravariantkovariant 
properties. They are contravariant with respect to one index, but covariant with respect 
to another. We have not really defined what these words mean for quantities with 
more than one index yet, and we will defer this to a later section of this chapter, where 
we talk about tensor transformations. 
With the earlier notation, the reciprocal nature of [t] and [g] was indicated by 
the equation t i j g j k  = 6,. But now, to be consistent with the subscriptlsuperscript 
convention, we should write 
(14.59) 
The Kronecker-6, written in this way, also has mixed contravariant and covariant 
form. 
Equations 14.49 and 14.50, which indicate how the basis vectors transform, are 
written using superscripthubscript notation as 
(14.60) 
Notice how the horizontal positioning of the indices of the transformation matrix is 
important. In Equation 14.55 and 14.57, the sum was over the second index of the 
matrix, while these sums are over the first index. This prevents us from writing the 
[t] matrix elements as ti, since this would no longer indicate which index comes first. 

NON-ORTHONORMAL COORDINATE SYSTEMS 
575 
We should also rewrite the relations involving the metric using the new notation. 
Our previous definition of the metric was in terms of the covariant basis vectors. 
Consequently, Equation 14.3 1 remains unchanged: 
M . .  
IJ = &. 
I , g. 
I' 
(14.61) 
and both indices remain as subscripts. Formed this way, the metric elements are purely 
covariant because both indices are subscripts. The metric converts the contravariant 
components of a vector to its covariant components, within the same coordinate 
system. This operation can be written, using the superscripthbscript notation, as 
v. = M . . ~ J .  
1J 
(14.62) 
Notice how the summation convention continues to work. This same operation in a 
primed system uses a primed metric, 
M!. = g! . g! 
(14.63) 
IJ 
1 
I' 
and is written as 
V/ = M!.V'J. 
1J 
(14.64) 
In summary, the equations governing the transformations of the contravariant 
components of a vector v can be written using the new superscriptlsubscript notation 
as : 
g .  = ti, g! 
g! = g' . gi. 
J 
J
l
 
J
J
 
The covariant components of v can be obtained from the contravariant components 
using the metric: 
There are clearly some holes in this picture. First, there is the question of how 
the covariant components of a vector transform. Second, we said the basis vectors gi 
were covariant in nature. We need to prove this. Finally, can we define contravariant 
basis vectors? These issues are all intimately related to one another, and are addressed 
in the next section. 

576 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
14.2.5 
Covariant Vector Component lkansformations 
Return to the pair of skewed coordinate systems described in Figure 14.4. The 
covariant vector components of the vector V will transform according to some linear 
relation 
v; = [?]Vj. 
(14.65) 
To determine [?I in this expression, consider two equivalent forms of the inner product 
of two vectors, one in the primed system and one in the unprimed system: 
- 
A .  B = A' Bi = A'J B!. 
I 
(14.66) 
The contravariant components of & transform according to the rules determined 
earlier: 
A'j = t{A'. 
(14.67) 
Substituting this expression into the far RHS of Equation 14.66 gives 
A' Bi = A' t'. 1
1
 
B!. 
(14.68) 
Since this equation must be true for any K, we must have 
Bi = t'' BI. 
(14.69) 
This expression is easily inverted to give 
B; = gJi B,. 
(14.70) 
Notice the similarity between Equations 14.60, 14.69, and 14.70. This supports our 
conclusion that the axis-parallel basis vectors are covariant. 
We were able to combine the contravariant components of a vector with the 
covariant basis vectors to fonn the vector itself: 
v = V'g'. 
(14.71) 
It would be nice to come up with a new set of contravariant basis vectors, g ,  which 
could be combined with the covariant vector component to form the same vector. 
That is, 
- 
v = vj g .  
(14.72) 
In fact, we can use this expression to define these contravariant basis vectors, and 
look at the consequences. 
The basic properties of the contravariant basis vectors can be deduced by again 
considering the inner product between two vectors, x and B. If x is expressed using 
covariant basis vectors and contravariant components, while B is written with con- 

NON-ORTHONORMAL COORDINATE SYSTEMS 
577 
travariant basis vectors and covariant vector components, the inner product becomes 
A .  B = A' gi . B~ g j  
- A' B .  & _ .  
g j .  
- 
I g1 
According to Equation 14.51, this expression must be equal to A' Bi and so 
1 i = j  
0 i # j  ' 
(14.73) 
(14.74) 
or in terms of the Kronecker symbol, 
1 
(14.75) 
g. . gj = 6.j. 
This last condition allows both the magnitude and direction of the contravariant 
basis vectors to be determined, if the covariant basis vectors are already known. 
Working in two dimensions, g' 
g 2  = 0 and g1 - g 1  = 1. In words, g1 must be 
perpendicular to g2, while its projection along the 1-axis, parallel to 81, must be 
one. This uniquely determines g1 and, by similar arguments, g2. The conditions of 
Equation 14.75 can be pictured graphically as shown in Figure 14.5. The constructions 
in this figure have been made assuming that I& I = 1. 
The covariant and contravariant vector components can also be interpreted graphi- 
cally, as shown in Figure 14.6. Again, for this figure, it has been assumedthat (gi I = 1. 
The contravariant vector components are simply the magnitudes of the axis-parallel 
projections of the vector onto the skewed axes defined by the covariant basis vec- 
tors. The covariant components are the magnitudes of the projections of the vector 
onto the same coordinate axes, but following lines parallel to the new contravariant 
basis vectors. This makes the projection lines for the covariant vector components 
2 
1 
Figure 14.5 Determination of the Contravariant Basis Vectors 

578 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
1 
/--- 
- _  
- 
- 
- 
Figure 14.6 Covariant and Contrav~ant Component Projections of a Vector 
perpendicular to the axes, as shown in this figure. The geometry insures that 
- 
v = v' gi = vi g .  
(14.76) 
If the covariant basis vectors are not unit vectors, the constructions of Figures 14.5 and 
14.6 must be adjusted appropriately, following the requirements of Equations 14.75 
and 14.76. 
Transformations for the contravariant basis vectors fall directly from Equation 
14.76 by the techniques we have applied several times: 
g/i = tij g j  
(14.77) 
g. = gij gV. 
(14.78) 
This confirms our classification of these new basis vectors as contravariant, because 
they transform exactly like the contravariant components of a vector. 
The complete set of transformation rules for both contravariant and covariant 
vector components and basis vectors can be summarized by the symmetric set of 
relations: 
with 

NON-ORTHONORMAL COORDINATE SYSTEMS 
579 
Notice that contravariant quantities are always transformed by a summation over the 
second index of either tij or gjj, while covariant quantities transform by summing 
over the first index. For contravariant quantities, tij is used to go from the unprimed 
to the primed system, while g' is used to go from the primed to the unprimed system. 
For covariant quantities, the roles of tij and g' are reversed. 
The new contravariant basis vectors allow us to construct another version of the 
metric, this time with superscripts: 
Mii = @ . g j .  
(14.79) 
Application of this form of the metric converts covariant quantities into contravariant 
quantities. For example, 
v' = Mi' vj. 
(14.80) 
We will see in the next section that the two different metrics, Mi, and Mi', are simply 
different representations of the same object, the metric tensor. 
14.2.6 Covariance and Contravariance in Tensors 
The covariant and contravariant properties discussed above are easily extended to 
tensors. Just as a vector can be expressed with contravariant or covariant components, 
a tensor can be expressed using purely contravariant or covariant components: 
(14.82) 
However, higher-rank tensors are more flexible than vectors because they can also 
be expressed in a mixed form, with both contravariant and covariant indices. For 
example, 
- 
is another equivalent representation of T. 
All the tensor expressions in Equations 14.82 and 14.83 are equivalent, although 
the specific values of the components will be different in each case. Just as the 
covariant and contravariant Emponents of a vector are related via the metric, the 
different representations of T can be obtainedfrom one another using the same 
metric. For example, if the two expressions for T in Equation 14.82 are equal, we 
can write 
Tijk = Mi' MJm Mk" Timn. 
(14.84) 

580 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
The expression in Equation 14.83 gives the same tensor when 
Ti,k = Mj, Ti&. 
(14.85) 
To convert a set of tensor components from purely covariant to purely contravariant 
form, one metric operation is needed for each index. 
Coordinate system transfomtions of tensors follow the same pattern we estab- 
lished for vector transformations. One transformation matrix of the appropriate kind 
is used for each index. For example, we might write 
T'iJk = g'i t i  gnk Ti ", 
Ti; = tli gJm t; T I m  ,,. 
J 
(14.86) 
(14.87) 
Example 14.2 Reference has been made to the fact that the metric is a tensor, but 
we have not really proven this fact. The proof is straightforward. Consider the metric 
elements, expressed in pure covariant form as, 
M . .  
11 = g . .  
I g. 
I '  
(14.88) 
The inner product between two vectors, expressed in two different coordinate systems, 
can be written 
where MAn = g; - g;. The transformation equations can be used to express the primed 
vector components in terms of the unprimed components. This gives 
A' BJ M . .  
11 = A' BJ ty t"j MAn. 
(14.90) 
Since this expression must work for all possible A and B, we must have 
M . .  
EJ = t7 t"j MAn, 
(14.91) 
which easily inverts to give 
Mii = gT gni Mmn. 
(14.92) 
But this is exactly how the elements of a second-rank tensor must transform, and so 
by definition the metric is a tensor. This means that we can write 
- 
- 
M = M . .  
1Jg 
- i  g '  
-i 
(14.93) 
Since the metric is a tensor, we can modify its covariant or contravariant nature 
as we would for any tensor. Although it might seem a little odd to use the metric to 
modify itself, we can change a purely covariant metric to a purely contravariant form 
by applying the metric twice: 

NON-ORTHONORMAL COORDINATE SYSTEMS 
581 
We can also put the metric in mixed form by writing 
M i .  = Mi” M . 
J 
m i .  
Using the transformation equations, you can easily show that 
(14.95) 
M i .  
J = 6’. g. 
1 = 6’. 
J ’  
(14.96) 
This implies that the metric tensor is really just a generalization of the Kronecker-6 
tensor. 
14.2.7 The Contravariance and Covariance of Partial Derivatives 
When partial derivatives are taken with respect to a contravariant coordinate, the result 
is a covariant quantity. To see this, let the contravariant coordinates in an arbitrary 
pair of coordinate systems be xi and x ‘ ~ .  
The rules of calculus require that 
(14.97) 
where there is an implied summation over the index j .  But notice, the dxj/dx” term 
is exactly the definition of gJi. This lets us write 
d 
- 
d - 
- gJi-. 
dX” 
dx J 
(14.98) 
Comparing this expression with Equation 14.70 shows that the partial derivative 
operation transforms just like a covariant quantity. The same type of argument holds 
for partial derivatives with respect to covariant coordinates. In that case we find 
‘ a  
= tJi-, 
ax j 
(14.99) 
(14.100) 
which says that this partial derivative acts like a contravariant quantity. To be consis- 
tent with our superscripthubscript conventions, we impose the rule that a superscript 
in the “denominator” of the derivative operation acts like a subscript, while a sub- 
script in the denominator acts like a superscript. This idea was discussed briefly in 
connection with the transformation matrices in Equations 14.56 and 14.58. 
Example 14.3 
scalar potential: 
A static electric field is often calculated by taking the gradient of a 
(14.101) 

582 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
In an earlier chapter, we defined the gradient operator by the relation 
d 4  = v+ 
* dF. 
(14.102) 
Since the displacement vector can be written 
d i  = dxi g i ,  
(14.103) 
where dxi is a contravariant quantity, it is clear the gradient of 4 can be written as 
(14.104) 
To check the validity of this expression, insert Equations 14.104 and 14.103 into the 
FU-IS of Equation 14.102 to obtain 
= d+. 
When we write the components of the electric field as 
(14.105) 
( 14.106) 
(14.107) 
they are covariant and must transform according to the relation 
E,! = g3.E. 
I 
I' 
(14.108) 
Example 14.4 
A static magnetic field B can be calculated using Ampere's law: 
d d T - B  = p0Z. 
( 1 4.1 09) 
In this expression, Z is the total current flowing through the closed path C. Taking 
dT to be a differential vector quantity with contravariant components, as given in 
Equation 14.103, the components of the magnetic field used in this integration should 
be written in covariant form, so that 
i d x '  Bi = pol. 
(1 4.1 10) 

NON-ORTHONORMAL COORDINATE SYSTEMS 
583 
14.2.8 
Special Relativity 
Special relativity deals with the observation of events as seen from two different 
coordinate systems, one moving at constant velocity with respect to the other. For 
simplicity, we will limit most of this discussion to systems with one spatial dimension, 
as shown in Figure 14.7. The primed system is moving, relative to the unprimed 
system, with a velocity v, in the x-direction. Einstein's favorite example of this 
situation was a railway car moving at a constant velocity on straight, flat tracks. A 
coordinate system fastened to the tracks, which he called the embankment, is the 
unprimed system and a system attached to the moving car is the primed system. 
These two coordinate systems form different reference frames for the observation of 
events. 
Special relativity treats time as another dimension, on an equal footing with the 
spatial coordinates. To make this work out, the time coordinate needs to have the 
same units as the spatial coordinate. This is accomplished by multiplying the time 
parameters by the speed of light c,. An event occurs at a specific set of values of the 
space-time coordinates. For these coordinates in the unprimed system, instead of x1 
and x2, we will use x and cot: 
x* -+x 
x2 -+ cot. 
Even though we have not labeled these coordinates with superscripts, you should 
realize that they are actually contravariant quantities. In the primed system, this same 
event occurs at the primed coordinates: 
x" -+ x' 
XI2 + cot'. 
The Lorentz Transformation In the previous section, we showed two different sets 
of coordinates could describe the same event. Now we ask, what is the transformation 
that converts one set of these coordinates to another? To answer this, we need to briefly 
discuss the fundamental assumption of special relativity. 
The basic postulate of special relativity is that the speed of light must be constant, 
regardless of the reference frame from which it is being measured. This is a strange 
idea, apparently fraught with paradox. It implies, for example, that the light coming 
out of the headlights of a moving car, as observed by someone standing on the 
Figure 14.7 The Inertial Systems for Special Relativity 

584 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
ground outside the car, travels at the same speed, independent of the speed of the car. 
It also has other strange consequences, such as the dilation of time and contraction of 
lengths for fast-moving objects. Nevertheless, it has been confirmed by many different 
experiments. Our goal here is not to delve into these ideas and their associated 
paradoxes, but rather to show how the transfornation between inertial frames can be 
looked at using a skewed coordinate system. For a complete introduction to the ideas 
of relativity, the reader should investigate the excellent book, Spacetime Physics, by 
Edwin Taylor and John Wheeler. 
What does the invariance of the speed of light say about the transformation matrix 
that converts the unprimed coordinates to the primed coordinates? In terms of the 
coordinates defined above, if an object travels at the speed of light, following the line 
x = cot, 
(14.111) 
then the transformation matrix [t] must convert this into 
x' = cot', 
(14.1 12) 
so the object is still moving at the same speed in the primed coordinate system. The 
linear transformation that accomplishes this was worked out by Hendrick Lorentz, 
and bears his name. With one spatial dimension, the Lorentz transformation is given 
in matrix form as 
where 
and 
(14.1 13) 
(14.114) 
(14.1 15) 
The inversion of the matrix in Equation 14.113 is straightforward and gives the 
elements of the [g] matrix as 
(14.1 16) 
It should not be surprising that this matrix is simply the [t] matrix with the sign of vo 
reversed. 
The form of these transformations implies an important fact. Given the coordinates 
of an event observed in the unprimed frame (x, cot), the quantity x2 - (cot)* is invariant 
to transformation. That is, if we form this same quantity using the event's primed 

NON-ORTHONORMAL COORDINATE SYSTEMS 
585 
components, we find that 
(14.1 17) 
This statement is true for any x and cot, as long as the primed and unprimed coordinates 
are related by the Lorentz transformation. This is easily shown by substituting 
x’ = Yo x - YoPo cot 
(14.118) 
and 
cott = - yoPo x + yo cot 
(14.1 19) 
into the RHS of Equation 14.117. 
Skewed Coordinate Representation There is a clever way, using a skewed co- 
ordinate system, to visualize the relationship between the primed and unprimed 
coordinates that is imposed by the Lorentz transformation. Imagine the unprimed 
coordinate axes are perpendicular, and introduce the orthonormal, covariant basis 
vectors gl andg,, as shown in Figure 14.8. The covariant basis vectors of the primed 
system can be determined using Equation 14.60 and the elements of the gij matrix: 
(1 4.120) 
Notice we have arranged the basis vectors horizontally to be consistent with the fact 
that the summation is done over the first index of g‘ . According to these equations, 
the primed basis vectors form a skewed set of coordinate axes, with an angle of 
8 = tan-’ Po between the primed and corresponding unprimed axis. This is shown 
in Figure 14.8. As the relative velocity between the reference frames increases, 8 
increases and the prime system becomes more drastically skewed. 
This representation has several things going for it. An event, with coordinates 
(x, cot) in the unprimed system and (x’, cot‘) in the primed system, can be represented 
as a single point for both systems, as shown in Figure 14.9. Except for a small 
complication due to how the axes are scaled, the contravariant components in each 
coordinate system can be determined by axis-parallel projections of the point onto the 
appropriate axes. Notice, the line x = cot, called a light line, plots as the straight line 
at 45”. The same line could also be written as x’ = cott, graphically demonstrating 
that the speed of light is invariant. 
If an object is observed in the unprimed frame, moving at constant velocity 
x = vt, the motion plots as a straight line in the two-dimensional coordinate systems. 
If v < c,, this line must lie above the light line, as shown in Figure 14.10. The same 
line describes the motion as observed in the primed system, and so the motion in this 
frame is still at constant velocity, x‘ = v’t’. Notice, however, that v’ # v .  We can 

TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
g, 
Figure 14.8 
The Two-Dimensional Lorentz Coordinate Systems 
1’ 
_ _ _  
1 
- 
X 
Figure 14.9 
Lorentz Transformations of Events 

NON-ORTHONORMAL COORDINATE SYSTEMS 
587 
Figure 14.10 Motion at a Velocity Less Than the Speed of Light 
determine the primed velocity from: 
(14.121) 
This velocity can be calculated directly from the transformation equations as 
(14.122) 
As can be seen from this expression and from the figure, as v --+ co, v’ -+ co. 
Another classic consequence of special relativity is immediately evident from the 
graphical representation of coordinates in Figure 14.9. Two events which occur at 
the same time in the unprimed frame, occur at different times in the primed frame. 
Imagine two events with the same value of cot, but different values of x .  They would 
occur on a horizontal line in Figure 14.9. In the primed system, not only will they 
have different values of x’, but they also will have different values of cot’. 
The representation of Figure 14.8 for describing the transformations of special 
relativity does have problems, however. First, consider the magnitude of the basis 
vectors of the primed system. These covariant basis vectors are specified in terms of 
the unprimed basis vectors by Equation 14.120. Specifically for g; : 
g; = Yo g l  + Y O P O  g2. 
(14.123) 

588 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
The magnitude squared of this basis vector is obtained by an inner product of the 
basis vector with itself 
lgj(’ = g; * g;. 
= (Yo gl + %Po g2). (Yo g l  + %Po gz) 
= 2<1 + P 3 ,  
(14.124) 
where the last step follows because the unprimed basis vectors have unit magnitude. 
As long as the primed system is moving with respect to the unprimed system, that is 
if vo # 0, Equation 14.124 says that g{ and, by a similar argument gi, are not unit 
vectors. This in itself is not that serious a problem, because our formalism does easily 
handle nonunit basis vectors. It does, however, complicate the geometrical picture 
of Figure 14.8, because you cannot simply axis-parallel-project a vector onto a basis 
vector to obtain a vector component. You must also divide by the magnitude of the 
corresponding basis vector. 
A more serious problem arises when you consider the inner product in this system. 
In particular, let’s look at i - f, the inner product of the position vector with itself. 
This quantity is a scalar and should be invariant to transformations. In the unprimed 
frame, 
(1 4.125) 
- 
r = x gl + cot g2, 
so we expect the quantity f * T; to be given by the expression 
x2 + (cot)’. 
(14.126) 
But we already determined in Equation 14.117 that special relativity requires the 
quantity x2 - (cot)’ to be invariant. It is not possible for both quantities to be 
invariant. The problem lies in our assumption about the geometry of the space. The 
value of T; . i= is not given by Equation 14.126! This is discussed in the next section. 
The Metric of Special Relativity Both the problem with the magnitudes of the 
primed covariant basis vectors and the invariance of the magnitude of the position 
vector involve some assumptions about how to take an inner product. In both cases, 
we took the inner product of a vector with itself in the unprimed coordinate system 
assuming an Euclidian metric of the form 
In effect, this was the metric used in the calculation of Equation 14.126: 
( 14.127) 
(14.128) 

589 
NON-ORTHONORMAL COORDINATE SYSTEMS 
With a little reflection, it can be seen that the invariant quantity of Equation 14.1 17 
can be generated if we use an alternate metric: 
Mi,+ [' 
0 -1 " 1  . 
( 14.129) 
When this metric is used in the unprimed system, Equation 14.126 is modified to 
= x2 - (cot)? 
(14.130) 
How does ths new metric affect the orthogonality and the magnitudes of the basis 
vectors? Using the metric of Equation 14.129 to form the inner product of gl with 
itself gives 
= 1. 
(14.131) 
The g1 basis vector is still a unit vector. The inner product between gl and g2 is 
= 0. 
( 14.132) 
The covariant basis vectors of the unprimed frame are still orthogonal. But the 
magnitude of g2 is 
= -1. 
(14.133) 
This says that, while / g 2  I = 1 ,  g2 itself must be imaginary! Very odd. 
equations given by Equation 14.120, we can write 
What about the basis vectors of the primed frame? Using the transformation 
2; = Yo g l  + YoPo g2 
g; = YoPo gl + Yo g2. 
Using the unprimed metric, the magnitude of gi is determined by 
( 1 4.1 34) 
(14.135) 
= 1. 
(14.136) 

590 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
So gi has now become a unit vector. The inner product between gi and gi is 
= 0. 
(14.137) 
Now the primed, covariant basis vectors of the skewed system are also orthogonal! 
Finally, the magnitude of gi is determined by 
= -1. 
(14.138) 
The g; basis vector is an imaginary unit vector, similar to gz. 
The inner products in the primed system could have also been performed using 
Mij, the metric of the primed system. Since the metric is a tensor, the primed metric 
is related to the unprimed metric in the normal manner: 
M!. 
= gm g". M 
(14.139) 
1J 
1 
J 
mn. 
In matrix notation, the prjmed metric is given by 
[M'] 
= [ 
Y O P O  
'Yo 
?f'] [' 
0 -1 ] [Go Y t ]  
= [ '  0 -1 0 1  
( 14.140) 
Notice the form of the metric is identical in the primed and unprimed systems. With 
this metric, the inner product between the covariant primed basis vectors can be 
determined: 
= 1, 
= 0, 
and 
= -1. 
(14.141) 
(14.142) 
(14.143) 
These are exactly the same results obtained previously, using the unprimed metric. 

NON-ORTHONORMAL COORDINATE SYSTEMS 
591 
The fact that one of the basis vectors is imaginary seems like a very strange thing. 
But there is nothing mathematically inconsistent with it. The distance along the the 
axis is now simply measured in terms of “imaginary” meter sticks instead of “real” 
ones. The new metric, while not very intuitive, solves our problems with the simple 
picture in Figure 14.9. The basis vectors are now unit vectors and the invariance of 
x2 - 
is built into the inner product operation. This weird geometry is often 
called Minkowski space, while the new metric is called the Minkowski metric. 
We actually can devise an equivalent coordinate system which does not use the 
imaginary time axes. In this construction, we write the position vector as 
(14.144) 
- 
r = x 21 + ic,t Q, 
where the basis vectors 61 and 22 are now real, orthonormal, unit vectors. The same is 
true in the primed system. In this case inner products are performed with the “normal” 
Euclidian metric. This is equivalent to our earlier construction, using the imaginary 
basis vector. since all we have really done here is pull out a factor of i from the basis 
vector and put it into the coordinate. The problem with this view is that you lose the 
convenient representation of Figure 14.9. There is no way to interlace the two sets 
of axes, represent the same event in both systems as a single point, and visualize the 
transformation with axis-parallel projections. The best we can do is draw two distinct 
coordinate systems, as shown in Figure 14.1 1. 
Extension to Three Dimensions 
dimensions. The position vector, in the unprimed and primed systems, becomes 
These ideas can easily be extended to three spatial 
- 
r = x g l  + y g2 + z g 3  +cot &I 
= x’ g; + y’ g; + z‘ g; + cot‘ g. 
The Minkowski metric expands to 
[MI = 
, ic,t 
I 
0 
0
0
1
 0
’
 
0 0 0 - 1  
E: 
:I 
(14.145) 
(14.146) 
ic$ 
Figure 14.11 A Second Picture of the Coordinate Systems for Special Relativity 

592 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
The Lorentz transformation between the two systems becomes 
Yo 
0 0 --YoPo 
(14.147) 
1
0
 
-YoPo 
0 0 
yo 
In this expression, the relative motion between the coordinate systems is along the x 
direction. The appropriate transformation for an arbitrary dii-ection of relative motion 
can be obtained by a rotational transformation of this matrix. 
The position vector, as written in Equation 14.145, is usually called the position 
“four-vector.” There are many other important four-vectors, some of which are de- 
veloped in the exercises at the end of this chapter. One of their more useful properties 
is that the inner product between any two four-vectors is invariant to coordinate 
transformation. 
EXERCISES FOR CHAPTER 14 
1. Consider the primed and unprimed coordinate systems shown below: 
X2 
2 
2‘ 
--f 
I 
X’ 
Let the two systems have a common origin. Each system has a set of covariant 
basis vectors g ~ ,  
g2 and g:, gi. 
(a) Find the expressions for x1 and x2 in terms of x“ and x ’ ~  
and the inverse 
(b) Identify the covariant basis vectors in the primed system by drawing their 
(c) The vector v, expressed in the unprimed system, is given by 
relations. 
magnitudes and directions. 
- 
v = 2g, + g2. 
What are the contravariant and covariant components of this vector in the 
primed system? 
(d) Using its contravariant and covariant components, determine the magnitude 
of the vector 
by forming the inner product of the vector with itself. Show 

EXERCISES 
593 
that the same result is obtained if just the contravariant components are used 
with the appropriate metric. 
2. Consider the unprimed, two-dimensional Cartesian system and a primed, skewed 
system, as shown in the figure below: 
2 
2‘ 
(a) Determine the set of equations that relate the unprimed coordinates to the 
(b) Determine the elements of the transformation matrices ti, and g i j .  
(c) Determine the elements of the Mij metric in the primed system, using the 
elements of the transformation matrix gij and Equation 14.92. 
(d) Show that the elements of the metric in part (c) could also have been obtained 
by using Mil = g{ . g;. 
3. We have shown that the metric is a tensor. It can be expressed using purely 
contravariant components as M’J, or with purely covariant components as Mi;, 
primed coordinates. 
4. 
5. 
so that 
- 
- 
M = Milg’g2 = Milg1g2. 
(a) Evaluate the product MzkMk1. 
(b) Use the result for part (a) to simplify the following expressions: 
i. ViMiJMlk. 
ii. VkM”M,k. 
Show that the cross product A X B expressed in a skewed system can be written 
as 
A’B1&k€ilk 
and determine v. 
Starting with Equation 14.92 and taking the unprimed system to be orthononnal, 
show that 
M,!, = (s) (s) 
. 

594 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
The definition of the metric for the primed system is Mij = gi .g$, which depends 
only on the properties of the primed system. The above equation, however, seem 
to also imply that the primed metric depends on the unprimed system. Explain 
this apparent contradiction. 
6. In this problem, the ideas of contravariance and covariance will be used to analyze 
a two-dimensional polar system. Designate a two-dimensional, Cartesian system 
with coordinates (x, y) + (x', x2) as the unprimed system. Let the polar system 
be primed, with coordinates (p, 6) 4 (,I1, 
.I2). The relationships between the 
two sets of coordinates are 
XI1 = Jm 
(a) Obtain the elements of the metric for the primed polar system, Mij using the 
(b) Show for the metric you obtained in part (b) that 
expression Mij = g l i d j  where g', = dx[/dx/'. 
Z 6; * e;, 
where the $: are the traditional polar basis vectors (gP, 6%) defined in Chap- 
ter 3, 
(c) Using contravariantkovariant transformation techniques, obtain the con- 
travariant (@', ge) and covariant ($, ge) basis vectors of the polar system 
in terms of the Cartesian basis vectors (Q, $). Compare these contravariant 
and covariant polar basis vectors to the traditional polar basis vectors (&, 66). 
Pick a point in the first quadrant and plot Q, $, eP, &, gp, and be. 
(d) The displacement vector can be expressed using the covariant polar basis 
vectors as 
dI; = dx'g,! = dpgp + dogo. 
so that the contravariant components of the displacement vector in the polar 
system are simply d p  and do. What are the covariant components of the 
displacement vector in the polar system? 
(e) Using the contravariant and covariant components of the displacement vector 
in the polar system, form the inner product dT . dF. 
7. Show that if a set of primed coordinates (x, ct) are related to a set of unprimed 
coordinates (x', ct') by a Lorentz transformation, then 
x2 - (c2)2 = X I 2  - (dy. 
8. This problem illustrates the differences between Euclidian geometry and Min- 
kowski geometry. First for the Minkowski geometry: 

EXERCISES 
595 
(a) Draw an orthonormal set of axes for an (x,ct)-system and plot the point 
(b) On the same plot, draw the x’ and ct’ axes of a primed system, moving at 
(c) Transform the coordinates of the point in part (a) into the primed system and 
Now for Euclidian geometry: 
(a) Start with a (x1,x2)-system that is orthonormal. Make a labeled sketch of 
the axes of this system and plot the point x1 = 2cm, x2 = 3cm. 
(b) Determine the g’, and the tij transformation matrices that relate the (xi, 
x2)- 
system to a skewed primed system, where the x’l and xI2 axes lie in the same 
position as the x’ and ct’ axes of the above Minkowski system. 
(c) Transform the point in part (a) into the primed coordinate system and indicate 
the values of (x”, xI2) on the axes of your drawing. 
(d) Notice and comment on the differences in scaling of the x’ and ct’ axes of 
the Minkowski system and the scaling of the primed axes of the Euclidian 
system. 
9. Consider a wave propagating along the x-axis with the a space-time dependence 
x = 2cm, ct = 3cm. 
velocity v = .58c with respect to the unprimed system. 
indicate the values of (x’, ct’) on the axes of your drawing. 
of 
The time period of the wave is T = 2a/o, and the spatial wavelength is 
A = 2 a / k .  
(a) Assume that o and k are positive real numbers. Is the wave traveling in the 
positive or negative x-direction? 
(b) Now observe this wave from a coordinate system moving along the x- 
direction, with a positive velocity v = pc with respect to the original 
system. Should the frequency of the wave in the moving system be larger or 
smaller than o? 
(c) Call the moving frame the primed frame. In this system, the space-time 
dependence of the wave becomes 
&k’x’ +oft’) 
The primed quantities can be obtained from the unprimed quantities by using 
the transformation techniques of special relativity. First, it is necessary to 
construct a pair of “four-vectors” (actually, since we have only one spatial 
dimension in this problem, they are “two-vectors”). The terms in the expo- 
nential brackets can be expressed as the inner product between two of these 
vectors, R and K. R is the position two-vector, 
_ -  
- 
R = $1 + c&, 

596 
TENSORS IN NON-ORTHOGONAL COORDINATE SYSTEMS 
while E is a frequency two-vector with the components 
- 
K = kg' + (w/c)g. 
Why are these k and o/c components covariant quantities? 
(d) With these two-vectors defined, their inner product is 
- _  
K - R = kx + wt. 
which is invariant to transformation. Since we are working in Minkowski 
space, why wasn't the metric used to form this inner product? 
(e) The invariance of the quantity in part (d) says that 
kx + wt = k'x' + w't'. 
What is the physical interpretation of this invariance? 
(f) Use the Lorentz transformations to obtain expressions for k' and w'/c in 
terms of k and o/c and identlfy the relativistic Doppler shift of the frequency 
and the Lorentz contraction for the wavelength as observed in the moving 
frame. Does this agree with your intuitive answer to part (b)? 
(g) Show that if p 4 1 your answer to part (f) goes to the classical limit 
w' = w + kv 
k' = k. 
10. Why does Equation 14.122 imply that you cannot accelerate an object past the 
speed of light? 

15 
INTRODUCTION TO GROUP THEORY 
Group theory provides a formal framework for taking advantage of the different types 
of symmetries that pervade science. In this chapter, we introduce the mathematical 
definition of a group, and present several examples of both discrete and continuous 
groups that occur in physics and chemistry. 
15.1 THE DEFINITION OF A GROUP 
A group is a set of abstract elements which can be "multiplied" together. In this 
context, multiplication does not necessarily refer to the common operation between 
real or complex numbers, but rather a more generalized operation which combines 
two elements of the group together. To determine if something is actually a group, 
you must first know both the group elements and the rule for multiplication. 
We will typically denote an entire group using a capital letter, while the distinct 
elements that make up the group are written with a small letter and a subscript. For 
example, the group G might have the elements gl, g2.g3,. . . , gh. To talk about a 
single, but arbitrary group element, we will often use a letter subscript. For example, 
gi refers to any one of the elements gI,g2, g3, . . . 
In order for G to be called a group, its elements must obey the following four 
rules: 
1. If gi and g , 
are elements of G, then their product, g k  = gi g,, is also an element 
2. The multiplication operation is associative: (gi gj)gk = gi(gj g k ) .  
3. One element I serves as an identity element, such that Z gi = gi I = gi for all 
of G. 
gi in G. 
597 

598 
INTRODUCTION TO GROUP THEORY 
4. Each element of G has an inverse, g, = g;', that is also an element of G. The 
product of gi and its inverse equals the identity element: gi g;' = gi' gj = 1. 
Keep in mind that rule #2 does not require that the multiplication be commutative, 
although it may be. That is, gi gj does not have to equal g, gi. If the multiplication is 
commutative for all combinations of group elements, the group is called Abelian. 
Where possible, we will identify the first group element as the identity element, 
whose existence is required by rule #3. That is, gl = 1. Notice all operations with 
this particular element are necessarily commutative. 
15.2 FINITE GROUPS AND THEIR REPRESENTATIONS 
In this section we present several examples of groups. As we progress, we will 
gradually accumulate an arsenal of terminology to keep track of the different types 
of groups and their properties. 
15.2.1 The Cyclic Group C, 
Our first example of a group consists of the elements 
G = (1, 
g2, 
g3, 
g4). 
5.1) 
The order of a group, which we will call h, is the number of distinct elements it 
contains. The group described in Equation 15.1 has h = 4. If h is finite, as it is in 
this case, the elements form afinite group and the elements are discrete. In contrast, 
infnite groups have h -+ 
m. Infinite groups can have discrete or continuous elements. 
In this chapter, we will limit our discussions to discrete, finite groups and continuous, 
infinite groups. 
The multiplication properties of a finite group can be summarized concisely using 
a multiplication table. The row label of a table identifies the first element in the 
product, while the column label represents the second. If we always make the first 
group element the identity, it is trivial to construct the first row and column of any 
table, as shown for our four-element group in Table 15.1. The first row and column 
of all multiplication tables, for groups of the same order, will be identical. The rest of 
the table depends upon the particular group. For the C, group, the multiplication table 
is filled in with a cyclic arrangement of the group elements, as shown in Table 15.2. 
TABLE 15.1. Start of a Multiplication Table 

FINITE GROUPS AND THEIR REPRESENTATIONS 
599 
TABLE 15.2. Multiplication Table for Cd 
Notice this table is symmetric about the diagonal and therefore is Abelian. A good 
exercise would be to convince yourself that this table satisfies all the requirements of 
a group. 
Notice that each group element appears exactly once in each row and column of 
Table 15.2. This is not a coincidence, but rather a fundamental property of all group 
multiplication tables. To see this, assume the products of a group element gi with two 
different group elements, g, and gk, produce the same element gm. In other words, 
If this were the case, g, would appear more than once in the irh row of the table. If 
we multiply both these equations by g;', we get the pair of equations 
(15.4) 
(15.5) 
which imply 
g j  = gk. 
(15.6) 
This clearly violates our initial assumption that gj # gk. This proof shows that an 
element cannot appear twice in a single row. It is trivial to construct a similar proof for 
columns. The combination of both results is often called the rearrangement lemma, 
because each row and column of a table must contain all the group elements, but with 
different ordering. 
Figure 15.1 shows a simple two-dimensional picture that can be associated with 
the C, group. Each group element is a rigid rotation of the object in the plane of the 
figure, which leaves the square array in the same physical configuration it was initially. 
These operations are sometimes called symmetry operations. A rigid rotation requires 
the relative distances between the individual objects that make up the body remain 
constant. In the case of the square object of Figure 15.1, imagine sticks between 
the four objects in the comers of the square. The Z element clearly does nothing. 
The other elements are integer multiples of 7r/2 rotations of the figure around its 
center. The group multiplication table shows how two successive symmetry operations 
are equivalent to another symmetry operation. For example, two 7r/2 rotations are 

INTRODUCTION TO GROUP THEORY 
I 
I 
IY 
I 
--1) 
.iO 
.im/2 
.im 
. i 3 ~ / 2  
i0 
, p / 2  
.im 
.i3m/2 
.iO 
.i?r/Z 
,in 
. i 3 ~ / 2  
.im/2 
.im 
.i3m/2 
.iO 
.im 
.i3m/2 
i0 
.im/2 
.i3m/2 
i0 
, i d 2  
eim 
X 
~ 
Figure 15.1 A Graphical Interpretation of the C, Elements 
equivalent to a single T rotation. With this picture, the origin of the term cyclic should 
be a little more obvious. 
A representation of a group is a mapping between the group elements and a 
particular set of quantities, which obey the same multiplication table. The most useful 
type of representations use matrices as the group elements, with the multiplication 
between elements accomplished using standard matrix multiplication rules. Any finite 
group has an infinite number of different matrix representations, but as we will see 
later in the chapter, a finite number of ‘‘jmportant’’ ones. 
Our picture of the rotations of the square array in Figure 15.1 immediately leads 
to a simple 1 X 1 representation of the C4 group. We can associate four complex 
phasors with the C4 elements, as shown in Table 15.3. Using this representation for 
the elements of C,, the multiplication table fills in as shown in Table 15.4. Notice 
how this table has exactly the same structure as Table 15.2. If we wished, we could 
even drop the eie notation and write the elements of this representation as shown in 
Table 15.5. This does not change the multiplication table in any way. 
A different representation for the C4 group can also be devised by using Figure 
15.1 as a guide for constructing a set of two-dimensional transformation matrices. 
TABLE 153. A Representation for C4 
I 
g2 
g3 
g4 
eiO 
eim/2 
.im 
. i 3 ~ / 2  
TABLE 15.4. C, Representation Satisfging its Multiplication Table 

FINITE GROUPS AND THEIR REPRESENTATIONS 
601 
TABLE 15.5. Another Form of the C, Representation 
I 
g2 
g3 
g4 
1 
2 
-1 
-i 
Y 
X’ 
I 
*- 
X 
Y! 
- 
+ 
Figure 15.2 7r/2 Rotation for gp of the C4 Group 
Since the g2 element corresponds to a counterclockwise rotation of 72/2, its effect on 
a two-dimensional orthonormal coordinate system is shown in Figure 15.2. The 2 X 2 
transformation matrix that relates the primed coordinates to the unprimed coordinates 
for this rotation is simply 
(15.7) 
By associating the other group elements with their transformation matrices in a 
similar manner, we can generate another valid representation of C4. The elements of 
this 2 X 2 matrix representation for C4 are shown in Table 15.6. It is easy to check that 
the original multiplication table is still satisfied by multiplying the matrices together. 
It is clear from Figure 15.1, that we can define a cyclic group ch of arbitrary order 
h, by increasing the number of sides of the polygon. The representations discussed 
here are also easily extended in the same manner. 
15.2.2 The Vierergruppe D2 
The only other possible group of order four has the multiplication table shown in 
Table 15.7. This group is called the vierergruppe and does not demonstrate the cyclic 
nature of C4. Instead, each element is its own inverse, and the product of any two 
TABLE 15.6. A Different Representation for C4 

602 
INTRODUCTION TO GROUP THEORY 
TABLE 15.7. The Vierergruppe Multiplication Table 
of the elements, excluding the identity, generates the third. The symmetry about the 
diagonal still exists, so the vierergruppe is Abelian. 
As before, a simple physical picture can be associated with this group. Imagine 
an object, perhaps a molecule, made of three pairs of different atoms. The six atoms 
are arranged as shown in Figure 15.3. Each element of the vierergruppe can be 
associated with a rigid rotation of the system which leaves it in an identical state. 
Clearly, rotations of rn around any of the axes are group elements. The fourth element 
is, of course, the identity. We write these elements as R(O), RX(v), R,,(T), and RZ(m-). 
We will often make use of this “R” notation for rotations. The subscript refers to 
the axis of rotation, while the argument is the angle of rotation, counterclockwise 
about that axis. The correspondence between the vierergruppe multiplication table 
and this picture needs to be shown. Since applying a rn rotation twice is the same 
as no rotation at all, the fact that each element is its own inverse is obvious. That 
the product of any two nonidentity elements generates the third is demonstrated by 
Figure 15.4 for the RAT) RY(w) = R,(T) operation. The other permutations follow 
in an analogous manner. Because of this picture, the vierergruppe is often called Dz. 
The D stands for dihedral, a figure formed by the intersection of two planes, and the 
2 indicates there is a twofold symmetry around each axis. 
We can easily construct a 3 X 3 matrix representation of Dz by considering the 
effect of the symmetry operations on the coordinate axes shown in Figure 15.3. 
Y 
X 
figure 153 Symmetry for the Vierergruppe 

FINITE GROUPS AND THEIR REPRESENTATIONS 
603 
Z 
X 
Y 
Z 
Z 
z 
X 
- - 
Y 
Figure 15.4 Demonstration of Vierergmppe Element Multiplication 
For example, consider the group element Rx(m). Figure 15.5 shows its effect on the 
coordinate system. The matrix that converts the unprimed coordinates to the primed 
coordinates in this picture is 
[ij] = [a 11 pI] [;I 
(15.8) 
The matrices for the remaining elements can be generated in a similar manner, and 
the complete representation is given in Table 15.8. 
Z 
Y 
Z’ 
Figure 15.5 Vierergruppe Symmetry Rotation About the x-Axis 

604 
INTRODUCTION TO GROUP THEORY 
TABLE 15.8. A 3 X 3 Representation for the Vierergruppe. 
I 
IY 
/ // 
j 
‘\\ 
/ 
/ 
I 
‘\ 
/ 
I 
\ \ 
I 
\ 
X 
/ 
/ 
(-4312, 
~ 
-1/2) 
I 
I 
\ 
\ 
(4312, - ,112) 
Figure 15.6 Object for Threefold Symmetry 
15.2.3 The Threefold Symmetry Group 0 3  
Figure 15.6 shows an object with a threefold symmetry. Three identical objects are 
placed at the comers of an equilateral triangle situated in the xy-plane. The coordinates 
of each point are labeled in this figure. 
What are the rigid rotations that leave this object unchanged? If we restrict our- 
selves to rotations in the xy-plane, the transformations constitute the C3 group, a 
version of the cyclic group we investigated earlier. Using the “R” notation, the el- 
ements are R,(O), R2(2.rr/3), and Rz(4.rr/3). We can write down a simple matrix 
representation of these elements, by determining the 2 X 2 transfomtion matrices 
associated with each rotation, the C3 version of the representation given in Table 
15.6. The result is given in Table 15.9. 
TABLE 15.9. The Three Elements of C3 and a Representation 

FINITE GROUPS AND THEIR REPRESENTATIONS 
605 
Figure 15.7 Axes for Interchanging Two of the Threefold Symmetric Objects 
TABLE 15.10. The Six Group Elements of a DJ Representation 
There are other rigid body transformations that leave the configuration of this 
object unchanged. If rotations in three dimensions are allowed, there are three more 
symmetry operations that interchange only two vertices of the triangle. These can be 
viewed as rotations of T about the D, E, and F axes, shown in Figure 15.7. Now a 
total of six group elements have been generated, which are listed along with their 
matrix representation in Table 15.10. 
To finish the story, we need to confirm that these elements form a complete group, 
and determine the multiplication table. This can be done using pictures similar to 
Figure 15.4 or by multiplying each possible pair of the matrix representations together. 
With either method, the result is that these six elements do indeed form a complete 
group, and the multiplication table is given in Table 15.1 1. This group is called D3. 
Notice that it is not Abelian. 

INTRODUCTION TO GROUP THEORY 
figure 15.8 Labeling for Operator Representation of the 4 Group Elements 
25.24 Operator Representation and Permutation Groups 
Another useful type of group representation has operators as the group elements. 
As an example of this type of representation, consider the D3 group of the previous 
section. To develop the operator elements of this group we label the objects at the 
corners of the triangle with the letters a, b, and c, as shown in Figure 15.8. The 
g2 operation takes the sequence (abc) to the sequence (cub), while the g4 element 
changes the sequence (ubc) to (acb). We can define a “rearrangement” operator [ 1321 
such that 
[132](abc) --+ (ucb). 
(15.9) 
What this notation means is that the first value between the initial parentheses gets 
mapped onto the first value between the hal parentheses. The second and third values 
are interchanged. All of the elements of the D3 group can be expressed as a set of 
these rearrangement operators as described by Table 15.12. 
Multiplications for the elements of D3 are easily performed with these operators. 
The product of the two elements 
(1 5.10) 
g2 g4 = g5 
TABLE 15.12. Operator Representation for D3 
I 
g2 
g3 
g4 
g5 
g6 
l
b
 
a 
C 
C 
a 
b 
a 
a 
a 
a 
a 
m 
a 
a 
m 
a 
a 
a 
a 
a 
a 
c
a
 
b
c
 
a
b
 
b
a
 
c
b
 
a
c
 

SUBGROUPS, COSETS, CLASS, AND CHARACTER 
607 
I 
8 4  
is simply written as 
I 
g4 
I 
g4 
g4 
I 
[312][132] = [213]. 
(15.11) 
Notice how the product of two elements is now effectively built into the notation. 
We do not need to reference a multiplication table to determine the result given in 
Equation 15.1 1, because the first operator [312] applied directly to [132] gives the 
result [213]. 
The operator representation is very useful for discussing the general class of 
permutation groups. The S, permutation group describes the indistinguishable ways 
that n identical objects can be arranged. The identity element of this group can be 
represented by the operator [ 1234 . * * n], while the other elements have operators that 
are generated by all the possible permutations of the numbers 1 through n. The S, 
group is therefore of order n!. 
15.3 SUBGROUPS, COSETS, CLASS, AND CHARACTER 
There are several ways of sorting the elements of a group into categories. This section 
describes the four most important classifications for group theory: subgroups, cosets, 
class, and character. 
15.3.1 Subgroups 
A subgroup is a subset of the elements of a group that, by themselves, form a 
legitimate group. The 0 3  group, whose multiplication table was given in Table 15.1 1, 
has several subgroups. One, consisting of the elements I ,  g2, and g3, is easy to spot in 
the upper left-hand comer of the table, and its multiplication table is shown in Table 
15.13. There are three other subgroups that consist of two elements each; a subgroup 
formed by I and g4, one from Z and g5, and the last from I and gg. Each of these two 
element subgroups satisfies a multiplication table similar to the one shown in Table 
15.14.Notice the identity element must be a part of any subgroup. Actually, if we 
TABLE 15.13. Multiplication Table for a Subgroup 
of D3 

608 
INTRODUCTION TO GROUP THEORY 
want to be rigorous, 0 3  has two additional "trivial" subgroups. First, the entire group 
itself is considered a subgroup. Also, the identity element all by itself is a subgroup. 
These trivial cases occur for all groups, and are usually ignored. 
15.3.2 
Cosets 
A coset is the set of elements formed by multiplying all the elements of a subgroup 
by an element of the group. Let the group G have order h and consist of the elements 
gl, gz, * - - gh. For the purposes of this discussion we will use the symbol G to represent 
all the elements of the group, in any sequence: 
G = (sl. gz. g3. 
* - -  g d  = (g3, 
g h ,  gl, . - *  8 2 )  = etc. 
(15.12) 
This convention lets us write 
gi G = G, 
(1 5.13) 
which is just a compact way of writing the rearrangement lemma. Assume G has a 
subgroup S of order h', with elements 
S = {Sl, s2, s3, * -  * 
Sh'}. 
(1 5.14) 
Let gx be an element of G that may or may not be in S. If we premultiply gx by all 
the different members of S, we get a set of h' distinct elements called a right coset 
of S. We write the collection of products as S gx: 
S g x  = {Sl g x 9  s2 g x ,  s3 gx, - * * 
Sh' gxl. 
(15.15) 
Notice that this set of elements does not necessarily form a group. If gx is in S, then 
the coset is equivalent to the group S. Ifg, is not in S, every element of the coset must 
be an element that is not in S, and the coset cannot form a group because the identity 
element is missing. There are h possible choices for g,, so there are h possible right 
cosets of S. If instead, we put g, first in the product operation, we get a left coset: 
gx S = kx s1, gx s29 gx s33 . . ' gx Sh'}. 
(15.16) 
Again, there are h possible left cosets of S. 
It should be noted that some texts define cosets with the additional requirement 
that gx not be an element of S. The distinction is not very important, since it only 
eliminates the cosets that are equivalent to S. Our choice for forming the cosets helps 
to clarify the following discussion. 
The right and left cosets of S are not necessarily distinct. An important theorem 
says that the elements of two cosets of S either have all the same elements, or no 
elements in common. To see this, consider two right cosets, Sg, and Sg,. Now imagine 
these two cosets have an element in common. That is, for some choice of si and s,, 

SUBGROUPS, COSETS, CLASS, AND CHARACTER 
609 
Premultiplying this expression by sil and postmultiplying by g;' gives 
sy1 si = g, g;l. 
(15.18) 
The element generated on the LHS of this equation is an element of S, so the product 
gy gL1 on the RHS must be an expression for this same element of S. Thus we can 
write 
s gy g;' 
= s, 
(15.19) 
or equivalently 
s g y  = sgx. 
(15.20) 
Consequently, if the same element appears in two different cosets, the two cosets 
have all their elements in common. Conversely, if a particular element is present in 
one coset but not another coset, there can be no common elements between these two 
cosets. 
The elements in any single coset are distinct. That is, no element can appear more 
than once in any given coset. This is a direct result of the rearrangement lemma- 
every element must appear once and only once in every row and column of the 
table. Therefore, every coset contains exactly h' different group elements. Therefore, 
because any two cosets will either have all elements in common or no elements in 
common, the order of the group h must be an integer multiple of the number of 
elements in any of its subgroups. That is, 
h = nh' 
where n = a positive integer. 
(15.21) 
This is demonstrated quite nicely by the subgroups of D3, discussed earlier. The 0 3  
group has six elements, while its nontrivial subgroups have orders two and three. 
Example 15.1 As an example, consider the right cosets formed from one of the 
subgroups of 0 3 .  Remember 0 3  consists of six elements, which obey the multiplica- 
tion Table 15.1 1. Earlier we found a subgroup formed from the first three elements 
of this main group: 
S = {Z,gz,g3). 
(15.22) 
There are six right cosets associated with this subgroup, each with three elements, as 
shown in Table 15.15. The first three cosets are formed by multiplying the subgroup 
TABLE 15.15. Right Cosets of a Subgroup of D3 
I 
S g 1  
s g2 
s g3 
s g4 
s g5 
s g6 
1 

610 
INTRODUCTION TO GROUP THEORY 
by an element in the subgroup. These cosets therefore always contain the same three 
elements as the subgroup S. The last three cosets are formed by products of S with 
group elements that are not in S. Each of these cosets contain three distinct elements 
of G that are not in S. 
15.33 Class and Character 
Another way to sort group elements is by their class. If a and b are elements of a 
group, they are in the same class if we can find another group element g such that 
(15.23) 
- 1  
g a g = b .  
This type of process, which involves an operator and its inverse acting on a, is called 
a similarity transform of a. Notice that the identity element must always be in a 
class by itself, because any similarity transform of the identity element will always 
generate the identity element. 
In matrix algebra, the trace of a square matrix is the sum of all its diagonal 
elements. In subscript notation, it can be written 
Tr[b] = bii, 
(15.24) 
where the doubled subscript implies a sum over the index i. It is easy to prove the 
matrix representations of all the group elements in one class must have the same 
trace. Suppose [a] and [b] are two matrix elements of a group, related by a similarity 
transform using group matrix elements k] and k]-', 
in the matrix form of Equation 
15.23, 
[gl-'[al[gl = [bl, 
(15.25) 
or in subscript notation: 
The trace of [b] becomes 
(15.27) 
(15.28) 
(15.29) 
(15.30) 
Thus [a] and [b] must have the same trace if they are in the same class. 
This property turns out to be so important that the trace of a matrix representing a 
group element is given its own special name. In group theory, it is called the character 
of the element, and in this text is denoted as x. We have shown that matrix elements 

SUBGROUPS, COSETS, CLASS, AND CHARACTER 
611 
representing group elements of the same class will always have the same character. 
Keep in mind this does not always work the other way around. It is possible to have 
two matrix elements that are in different classes, but have the same character. 
Example 15.2 The group 0 3  has three different classes. As always, the identity 
element is in a class by itself. The two elements g2 and g3 are in a second class, 
while the elements g4, g5, and g6 form a third class. This can be easily confirmed 
by performing the similarity transformations. If you look back at Figure 15.6, it can 
be seen that these three different classes for 0 3  correspond to three very different 
types of symmetry operations. The identity element is unique because it is the only 
operation that represents no change to the orientation. The elements g2 and g3 are 
both operations which rotate the system in Figure 15.6 in the plane of the page, 
interchanging all three of the vertices of the triangle. The elements g4, g5 and g6 
are operations where only two of the objects are interchanged, while a third is held 
fixed. This gives an intuitive feel for what a class really is. It is a collection of all the 
symmetry operations which do “the same type of thing.” 
A list of the characters for a 2 X 2 matrix representation for 0 3  are shown in Table 
15.16. The first column of this table contains the 2 X 2 matrix representation of the 
group elements we found in Table 15.10. The second column indicates the character 
TABLE 15.16. List of Characters for a Ds 
Representation 
R 
X 
2 
1 
-1 
R
5
=
4
 A -  
J3] 
g 6 =  [ ;’ :] 
0 
0 
0 

612 
INTRODUCTION TO GROUP THEORY 
of each matrix element. Notice how we have subdivided the elements by their class. 
For this representation, each class has a different character, but remember this will 
not always be the case. 
15.4 IRREDUCIBLE MATRIX REPRESENTATIONS 
This section introduces irreducible representations, an enumerable set of important 
representations which exist for any finite group. In analogy with the basis vectors of 
a linear space, these irreducible representations are orthogonal and complete. This 
means, among other things, that any representation can be written as a sum of the 
irreducible representations, a fact that has great consequences in the application of 
group theory to physical problems. 
The derivation of many of the results of this section, including the comments in 
the previous paragraph, are too involved to present in this short introduction to group 
theory. In addition, there are many facets of the subject that we will not discuss. 
Only a few important results are presented here. The complete story, along with 
formal proofs, can be found in many textbooks devoted solely to group theory. An 
introductory, but complete treatise of group representation theory can be found in the 
third chapter of the book by Wu-Ki Tung. The books by Hamermesh and Wigner also 
give a very complete discussion of group theory. 
15.4.1 
Notation 
Because there are many possible matrix representations for a group, we need a 
notation for distinguishing them. When we discussed the group C4, we explored the 
two different representations given in Table 15.3 and Table 15.6. We will call the 
first Ci2] and the second CY], as shown in Table 15.17. The brackets are used as a 
reminder that this notation refers to matrix representations only. You will discover 
the justification for labeling some representations with bracketed numbers and some 
with bracketed letters in the sections that follow. Also shown in Table 15.17 is the 
representation C:'], a one-dimensional representation made up of all 1's. This trivial 
representation will work for any group, and we will always reserve the bracketed 
index ['I for it. 
TABLE 15.17. Three C.+ Representations 
I
I
I
 
g2 
8 3  
g4 

IRREDUCIBLE MATRIX REPRESENTATIONS 
613 
TABLE 15.18. The Direct Sum of Two C4 Representations 
I 
8 2  
g3 
g4 
I 
1 
i 
- 1  
-i 
[o 1
0
0
 
1 0 1  [k 
0
0
 
0 ‘1 [jl p 1  !l] 
[; 
B 3 
0
0
1
 
- 1  
0 
To refer to a single element in a particular matrix representation, we use a modi- 
fication of this notation. For example, the Cyl matrix representation of the g3 group 
element is written as @I. 
If we need to refer to one arbitrary element, we use a 
letter as the subscript. For example, gfA1 
refers to any one of the matrices in the Crl 
representation. 
15.4.2 Direct Sums of Matrix Representations 
The matrix representation of a group can be of any dimension. In fact, given any 
matrix representation of a group, it is easy to expand it to a higher dimension by 
adding “blocks” of another representation. This process can be demonstrated using 
the C4 cyclic group. Consider the Cfl and the Cyl representations shown in the first 
two rows of Table 15.18. The Ci2] representation can be looked at as a set of 1 X 1 
matrices. They can be added to the Crl matrices in a block diagonal sense to form 
the new 3 X 3 matrix representation listed in the last row of Table 15.18. The @ 
symbol, called the direct sum operation, is used to indicate this type of block diagonal 
addition: 
(15.3 1) 
It is easy to see that the set of Ci2] @ Cyl matrices form a valid representation 
for the C4 group. When any two of these elements are multiplied together, the 
block diagonal form insures there is no mixing between blocks. Since each block 
independently satisfies the C4 group multiplication table, the direct sum must also 
satisfy the same table. 
15.4.3 Reducible and Irreducible Representations 
Given a particular matrix representation GIA] for a group G, 
(15.32) 

614 
INTRODUCTION TO GROUP THEORY 
we can generate another representation GLEl by simply transforming the coordinate 
basis. If [TI is the transformation matrix, then the second representation is related to 
the first via the relations 
(15.33) 
Keep in mind, the same transformation matrix [TI must be applied to all the giA1 
elements. It is obvious that both representations are of the same dimension and obey 
the same multiplication table. Notice Equation 15.33 is in the form of a similarity 
transformation, like that of Equation 15.23, so the matrices of the new representation 
will have the same character as the original representation. 
Two matrix representations are said to be equivalent if you can obtain one from the 
other using the type of operation described in quation 15.33. In contrast, inequivalent 
representations cannot be generated from one another by simply transforming the 
coordinate basis. Obviously, two representations of different dimension will always 
be inequivalent, but it is also possible to have two inequivalent representations of the 
same dimension. 
If, using the same transformation matrix, all elements of a representation can be put 
into the form of a direct sum of two or more smaller blocks, the original representation 
is said to be reducible. If it cannot, it is irreducible. The irreducible representations 
of a group are important because any representation can be transformed to a direct 
sum of one or more irreducible representations. We will discuss this in more detail, 
later in this chapter. 
It turns out that the number of inequivalent irreducible representations is not 
infinite, but rather equal to the number of different classes in the group. In order to 
distinguish these enumerable irreducible representations from the infinite possible 
reducible ones, we use a slightly different notation for the two. If a representation is 
reducible, we use a letter in the brackets. For example, the reducible representation 
of C4 we developed in Equation 15.31 was called CiEJ. The representation Ci2] is 
obviously irreducible (because it is one-dimensional) and so is labeled with a number. 
~ 
~~ 
~ 
~~~ 
~ 
~ 
Example 15.3 As a simple example of the reduction of a representation, consider 
the Cyl representation of the C4 cyclic group, which was shown in Table 15.17. 
We would like to know if, using some similarity transformation, can we write this 
representation as the direct sum of two 1 X 1 matrices. In other words, is this 
representation reducible? 
Since these matrices are two-dimensional, to reduce them any further is equivalent 
to diagonalizing them. In Chapter 4, we discussed a general method for finding the 
transformation matrix which diagonalizes a given matrix. Let's apply this method to 
find the matrix [TI which diagonalizes the gyl matrix element of CY]. First, we find 
the eigenvalues using the determinant equation 
(1 5.34) 

IRREDUCIBLE MATRIX REPRESENTATIONS 
615 
TABLE 15.19. Block Diagonalization of CiA1 
g2 
g3 
g4 
which gives 
A = +i. 
(15.35) 
These eigenvalues generate the two eigenvectors 
"11 
; =&[!J 
1 
Jz 
From these we can construct the transformation matrix 
[TI=-[; 1 
li] 
Jz 
and its inverse 
[TI-' = - 
1 [; 3. 
Jz 
(15.36) 
(15.37) 
(15.38) 
Because we constructed it specifically to do so, applying [TI and [TI-' to the 
gp1 matrix in a similarity transformation will certainly diagonalize it. The real test, 
however, is if the same transformation simultaneously diagonalizes all the other 
elements. Table 15.19 shows the result when the same similarity transformation is 
applied to the rest of the matrix elements. Notice, these elements are also diagonalized, 
so CiA1 is reducible and can be written as the direct sum 
CY' @ cp, 
(15.39) 
where Cyl is the new representation shown in Table 15.20. Since Ci3] is one- 
dimensional, and is obviously different from Cyl and Ci2', it must be a third ir- 
reducible representation of C,. 
Earlier we stated that the number of irreducible representations is equal to the 
number of classes. The number of classes of C, is four, but so far we have only 
identified three different irreducible representations. One way to discover the fourth 
TABLE 15.20. Another Irreducible Representation for 
c4 
-i 
-1 

616 
INTRODUCTION TO GROUP THEORY 
set would be by trial and error. A more educational method is to derive it using the 
orthogonality relations of the irreducible representations, a topic we explore later in 
this chapter. 
Example 15.4 
As another example of representation reduction, consider the 0 3  
group. We showed a 2 X 2 matrix representation for this group in Table 15.10. An 
attempt to diagonalize this representation in the same way we did for the previous 
example fails. Therefore, it is an irreducible representation, which we will call DY]. 
As always, Dyl is the trivial one-dimensional irreducible representation. 
A 3 X 3 matrix representation for this group can be constructed from the operator 
representation developed in Table 15.12. For example, take the [231] operator. It 
generates the sequence ( h a )  from the sequence (ah). This operation can also be 
accomplished by a 3 X 3 matrix 
(15.40) 
This matrix is a valid representation for the g3 element of D3. The matrices for all 
the other group elements are constructed in a similar manner. The result is shown in 
Table 15.21 and we will refer to this as the DY] representation. Now we ask, is this 
representation reducible? 
We can attempt to reduce this matrix by a similar process to the one used earlier. 
We will diagonalize one of the group elements, and then check the other elements to 
see if the same transformation puts them into a reduced form. We have no guarantee 
that this will work in this case, because there is the possibility that none of the 
elements (besides the identity) are diagonal in the reduced representation. Later in 
the text, we will revisit this problem, using a more reliable technique based on the 
orthogonality and completeness properties of the irreducible representations. 
We have a small hint from looking back at the 2 X 2 Drl representation. Its gg 
element is diagonalized. Since the OF1 matrices are an irreducible representation, it 
would make sense to attempt diagonalization of the same element in the 3 X 3 DY1 
representation, and see what happens. Luckily, this process actually works, and the 
transformed version of the set of dtl matrices is given in Table 15.22. Notice this is 
just the direct sum DF1 @ D f ] .  
TABLE 15.21. The 3 X 3 0,"' Representation 
I 
g 2  
g3 
g4 
gs 
g6 
1
0
0
 
0
0
1
 
0
1
0
 
1
0
0
 
0
1
0
 
[x t :I [: : xl [; x tl [: : tl 1: x :I 
[8 x bl 

617 
IRREDUCIBLE MATRIX REPRESENTATIONS 
TABLE 15.22. Block Diagonalized of the D f l  Representation 
g 2  
g3 
I 
I 
g4 
We already determined that 0 3  has three different classes. This means that there 
should be three irreducible representations. So far, only two have been identified, DY1 
and DY1. In the next section, we determine the remaining irreducible representation 
DY1, using the orthogonality of the irreducible representations. 
15.4.4 Orthogonality of Irreducible Representations 
There are several important rules that the irreducible matrix representations obey: 
1. The number of irreducible matrix representations for a group is equal to the 
2. If 
is the dimension of the first irreducible matrix representation, ni2] the 
number of different classes of the group. 
dimension of the second, and so on, then 
where the summation is over all the irreducible representations and h is the 
order of the group. 
3. “Vectors” formed by the elements of the irreducible representation matrices are 
orthogonal. In this context, a vector is formed by combining the elements of 
a representation in a very particular way. The components of one such vector, 
for example, are formed by all the 1-2 matrix elements of the six DY1 matrices. 

618 
INTRODUCTION TO GROUP THEORY 
The components of another are formed by all the 2-2 elements of the six Dyl 
matrices, etc. 
4. The magnitude squared of each of the “vectors” described in rule #3 is equal to 
h/n[‘l where dil is the dimension of the matrix representation used to construct 
the vector. 
When checking orthogonality or checking the magnitude of the vectors described 
in rules #3 and #4, be aware that, if any of the components are complex, one set 
of the vector components should be complex conjugated before performing any dot 
products. 
In both the examples of the previous section, we found there was one missing 
irreducible representation. Instead of using guesswork to find the last representation, 
it is instructive to use the above rules to determine it. 
Example 15.5 For the C4 cyclic group, three irreducible representations were iden- 
tified: C?], CY], and Ci3]. There are four different classes in this group, and according 
to rule # 1, there must be another irreducible representation which we will call Cfl. 
First let’s determine the dimension of the Ci4] matrices. According to rule #2: 
(#1)2 
+ (n121)2 + (n[31)2 + (#J)2 
= 4 
1 + 1 + 1 + (n[41)2 = 4 
(15.42) 
(15.43) 
or 
n~41 = 1. 
(15.44) 
So the final irreducible representation for the C4 group is composed of four 1 X 1 
matrices. The identity element must be 1, and according to the multiplication table 
for ~
4
,
 
g3 is its own inverse, so grl = -t 1. MSO, g4 is the inverse of g2, so if we let 
grl = g, then gyl = l/g. Notice we have given a the flexibility of being complex. 
This information leads us to the partially specified Table 15.23. Now we just need to 
determine the value of g and the sign of gf]. 
A table of all of the four irreducible representations for C, is shown in Table 15.24. 
Notice all the representations are 1 X 1 matrices. Incidentally, this turns out to be a 
general property of any Abelian group. Following the prescription described in rule 
#3, we form four, four-dimensional vectors, which we call V , V , V , and Vf4]: 
-111 
-121 
-[31 
TABLE 15.23. The Form of the Missing Irreducible 
Representation of C, 
I 
‘
I
 
J 

IRREDUCIBLE MATRIX REPRESENTATIONS 
619 
TABLE 15.24. Partially Speci6ed Table of the 
Irreducible Representations of C4 
I 
I 
-1 
- 1  
1 
- 
a 
21 
1 /c 
1 
-i 
- 1  
i 
TABLE 15.25. The Irreducible Representations for C, 
Ci*] 
-1 
-1 
q 3 1  
1 
-i 
-1 
I 
Cf' 
1 
-1 
1 
- 1  
-[41 
. 
According to the rule, these vectors must be orthogonal. If V 
is orthogonal to V1l1 
with grl = - 1, we find that g = +i. Either choice for the sign of g does not lead 
us to a representation of C, that we have not seen before. But if instead we pick 
grJ = + 1, we obtain a new irreducible representation with g = - 1. Now our set of 
irreducible representations is complete. They are summarized in Table 15.25. 
According to rule #4, the magnitudes of V , V , V , and VL4] 
should all be 
equal to 4. Taking the dot product of each vector with itself, remembering to take the 
complex conjugate of one of the terms, shows this is indeed true. 
Example 15.6 The same procedure used above can be repeated for the 0 3  group. 
This group is of order six, and has three classes. We have already identified two 
inequivalent irreducible representations: D:'] and 0k3]. 
That leaves a third, the DYJ 
representation, for us to find. 
-111 
-[Zl 
3 3 1  
Using rule #2, the dimension of DrJ is given by 
(15.46) 
(15.47) 

620 
INTRODUCTION TO GROUP THEORY 
D f ]  
g1 
g2 
g3 
g4 
g5 
g6 
1 
1 
1 
?1 
21 
’1 
or 
( n q  = 1. 
(15.48) 
So the irreducible representation we seek is a set of six 1 X 1 matrices. According to 
the multiplication table for D3, the g4, g5 and g6 elements are d their own inverses, 
and are also in the same class. This lets us write gi2] = gyl = grl = 5 1. Also 
according to the multiplication table, gz g4 = g5 and g3 g4 = g6, so we must have 
gfl = grl = + 1. These properties give us the partially specified representation in 
Table 15.26. If we take the positive sign for all these terms, we just get a repeat of 
the trivial representation 0:’’. 
Therefore we take the negative signs instead, and the 
complete set of irreducible representations for 0 3  are listed in Table 15.27. 
It is educational to check that the other group rules hold for these representations. 
Rule #3 specifies that we can form “vectors” from the elements of the matrices, and 
that these vectors should all be orthogonal to one another. There will be six of these 
vectors. The 1 X 1 Dill representation forms the first vector: 
The second vector comes from the Dk2’ representation: 
TABLE 15.27. The Irreducible Representations for D3 
(15.49) 
I 
g2 
g3 
g4 
g5 
g6 
DY1 
1 
1 
1 
1 
1 
I 
Di2] 
1 
1 
1 
-1 
-1 
-1 

IRREDUCIBLE MATRIX REPRESENTATIONS 
-
-
 
1 
1 
1 
-1 
. 
-1 
- -1 
- 
- 
1 
-1/2 
-1/2 
1 / 2  
1 / 2  
-1 
621 
(15.50) 
- 
1 
- 
0 
- I -  0 -
-
 
-&/2 
&2 
- 1/2 
&/2 
-&/2 
-&2 
-1/2 
. 
&/2 
&/2 
-&/2 
-1/2 
(15.51) 
-1/2 
1 
0 
0 
- 
-
-
 
The 2 X 2 DY1 representation generates the last four: 
0 3  
gl 
g2 
g3 
g4 
g5 
g6 
xI1l 
x121 
xr3, 
1 
1 
2 
1 
1 
-1 
1 
1 
- 1  
1 
-1 
0 
1 
-1 
0 
1 
-1 
0 
Taking the dot product of all fifteen possible pairs of these vectors shows they are 
indeed orthogonal. According to rule #4, the vectors which come from OF1 and DY1 
should have magnitude squared values of 6 and those from DY1 should have 3. This 
is also obviously true, 
15.4.5 Character Tables and Character Orthogonality 
Once all the irreducible representations are known for a particular group, it is useful 
to construct a character table. This table lists the character of each group element for 
each of the irreducible representations. A character table for the 0 3  group is shown 
in Table 15.28. 
Similar to the orthogonality rules presented in the previous section, the character 
tables have orthogonality rules of their own. In this case, the “vectors” formed by 
the columns of these tables are orthogonal to one another, while their magnitudes 
squared are equal to the order of the group. It is easy to check that these rules hold 
for the 0 3  character table. 

622 
INTRODUCTION TO GROUP THEORY 
4 
TABLE 15.29. A More Compact Character Table for D3 
x[ll 
xr21 
x[31 
I 
g29 g3 
g4r gS9 g6 
1 
1 
2 
1 
1 
-1 
1 
- 1  
0 
Since elements in the same class always have the same character, the character 
table is often written more concisely by putting all the elements in the same class in a 
single row. The compact character table for D3 is given in Table 15.29. When forming 
dot products between the columns of these character tables, you need to weight each 
term by the number of elements in each class. 
15.4.6 Completeness of the Irreducible Representations 
The real power of the irreducible representations stems from the fact that, using 
the proper similarity transformation, any matrix representation can be written as a 
direct sum of one or more of the irreducible representations. A particular irreducible 
representation may appear once, more than once, or not at all. In other words, with 
some [TI, any group representations Gral, can be written 
This is often called the decomposition of a representation. 
There is a remarkable result of group theory which says that the number of times a 
particular irreducible representation appears in the direct sum of Equation 15.52 can 
be determined just by looking at the character table. Take the vector formed by the 
characters of the representation you want to decompose. Then form the dot product 
of this vector with one formed from the character values of a particular irreducible 
representation. Divide the value of this dot product by the order of the group, and the 
result will be equal to the number of times the particular irreducible representation 
appears in the decomposition. Notice how the irreducible representations are playing 
a role analogous to that of the basis vectors of a coordinate system. Because all matrix 
representations can be expanded this way, the irreducible representations are said to 
be complete. 
Example 15.7 In a previous example, we determined that the 3 X 3 Dyl matrix 
representation in Table 15.21 could be transformed and written as the direct sum 
Dyl @ Oh3]. We arrived at the result using the diagonalization process and a little 
guesswork. Let's see how the decomposition rule described above gives us this result 
with much less work. The character vector for the Table 15.21 representation for Drl 

IRREDUCIBLE MATRIX REPRESENTATIONS 
623 
is 
(15.53) 
Now take the dot product of this vector with each of the three vectors of the 0 3  
character table listed in Table 15.28, and divide by 6. The net result is that there is one 
DY1 representation, one DY1 representation, and no DY1 representation in the block 
diagonalization. This is exactly the result we found before. 
15.4.7 
Shur's Lemma 
Many practical applications of matrix representations center around an important 
group theory result known as Shur's lemma. Imagine we have a group G, and GLX1 
is 
one of its matrix representations: 
(15.54) 
Now suppose we have a matrix [ v] which is invariant under similarity transformations 
by ull these matrix elements of GEXl. That is, 
gj"I-l[~g,fXI = [VI for i = 1,2, * - .h. 
(15.55) 
Shur's lemma says if [V] satisfies Equation 15.55, and GLxl is an irreducible repre- 
sentation, then [ V] must be a multiple of the identity matrix. That is, 
[Vl = AVI, 
(15.56) 
where A is some constant. If [V] satisfies the conditions of Equation 15.55, it is said 
to commute with the elements of Grxl. This terminology is used because an equivalent 
way of writing Equation 15.55 is 
[Vlgjx] = gj"][V]. 
(15.57) 
If Grxl is reducible, we cannot use Shur's lemma directly. But remember from our 
previous discussion, any reducible representation can be decomposed into the direct 
sum of two or more irreducible representations by using the appropriate coordinate 
transformation. For example, imagine the decomposition of GIX1 looks like 
G['] 
0 
0 
[T]-'C["'[T] = [ 
G r  231] 
, 
(15.58) 

624 
INTRODUCTION TO GROUP THEORY 
where GI1], G['], and Gf31 are irreducible representations of dimension 1, 2, and 4 
respectively, and [TI is the transformation matrix that puts the matrices into the block 
diagonal form. If [VJ commutes with all the elements of Grx1, the transformed matrix 
[V'] = [T]-'[VJ[T] 
wiU commute with all the elements of the transformed Grxl 
representation. Then, by Shur's lemma, the [VJ' 
matrix must be diagonal, with the 
form 
[V'] = [T]-'[vJ[T] = 
A
l
O
O
O
O
O
O
 
O
A
~
O
O
O
O
O
 
O
O
h
z
O
O
O
O
 
0
0
0
h
~
0
0
0
 
0
0
0
0
h
3
0
0
 
0 0 
0 
0 
0 
h3 
0 
0
0
0
0
0
0
h
3
 
There are three different constants ;-mg the diagonal of [V'], i 
(15.59) 
lifferent one for 
each block of the decomposition of GLXl. This works because Shur's lemma applies 
separately to each irreducible block. 
Example 15.8 We now present an example that shows how many of the ideas of 
group theory can be applied to a problem from classical mechanics. Consider the 
two-dimensional system of balls and springs shown in Figure 15.9. Each of the three 
balls has mass m, and the three springs have the same spring constant k. What are the 
normal frequencies of vibration for this system? Because of the symmetry, we should 
expect group theory to be useful in solving this problem. We begin with a review 
of the standard method of determining the n o d  modes of such a system and then 
show how group theory simplifies the process. 
Since there are three masses and we are working in two dimensions, we need 
a total of six coordinates to describe an arbitrary state of the system. We will use 
the coordinates 41 and 42 to describe the displacement of the mass in the lower-left 
k 
Figure 15.9 Coupled Mass Spring System 

IRREDUCIBLE MATRIX REPRESENTATIONS 
- 5/4 
&/4 
1 
0 
-1/4 
-fi/4- 
&/4 
3/4 
0 
0 
-&/4 
-3/4 
0 
-&/4 
3/4 
&/4 
-3/4 
-1/4 
- 4 / 4  
-1/4 
&I4 
1/2 
0 
1 
0 
5/4 
-&/4 
-1/4 
&/4 
[VI = 
--&/4 
-3/4 
&/4 
-3/4 
0 
3/2 
A 
625 
' 
Figure.lS.10 Coordinates for the Mass Spring Problem 
To determine the equations of motion, we can use Newton's second law to write 
(15.63) 

626 
INTRODUCTION TO GROUP THEORY 
A normal mode is a state of motion that oscillates with constant frequency. To find 
the normal modes, we therefore require all the coordinates oscillate at the same 
frequency, i.e., 4i ---f seiw. Applying this to Equation 15.63 gives 
(15.64) 
But notice this is the same as a typical eigenvalue problem, 
where the eigenvalues are related to the normal frequencies by 
rnw2 
A=-. k 
(15.66) 
This means that to find the normal frequencies we just need to diagonalize the [Vl 
matrix. Unfortunately, diagonalizing a typical six-dimensional matrix is no easy task, 
since it involves finding the roots of a sixth-order polynomial equation. But the 
underlying symmetry of this problem, plus some physical reasoning, will let us find 
these eigenvalues much more easily. 
The physical system clearly has a D3-type symmetry. The representation of 0 3  
we need to consider for this problem is the set of 6 X 6 transformation matrices that 
correspond to each symmetry operation acting on the qi coordinates. Let’s work out 
the g2 element of this matrix representation in detail. Looking back at Table 15.10, 
this element corresponds to a rotation of 2m/3 counterclockwise around a z-axis 
sticking out of the page of Figure 15.10. Consequently, this operation rotates the 
qi coordinates into the. q; coordinates as shown in Figure 15.11. We can write the 
Figure 15.11 Coordinate Transformation for RZ(2n/3) Element 

IRREDUCIBLE MATRIX REPRESENTATIONS 
RZ(2.rr/3) + 
627 
- 0 
0 
0 
0 
-1/2 
-&2- 
0 
0 
0 
0 
f i / 2  
-1/2 
-1/2 
-&/2 
0 
0 
0 
0 
Js/2 
-1/2 
0 
0 
0 
0 
0 
0 
-1/2 
4
/
2
 
0 
0 
-
0
 
0 
&/2 
-1/2 
0 
O
A
 
relationships between the primed and unprimed coordinates as 
(15.67) 
so the matrix which transforms the primed coordinates into the unprimed coordinates 
is 
Repeating this process for the other 0 3  elements generates the complete representa- 
tion, given in Table 15.30. In addition, this table lists the character of each matrix. 
These matrices are reducible, and we can use the completeness and orthogonality 
of the character tables to determine how many times each irreducible representation 
of 0 3  occurs. Looking at Table 15.30, the character vector for this representation is 
(15.69) 
If we form the dot product with the columns of the irreducible character table for 0 3  
given in Table 15.28, we find the decomposition for the 6 X 6 representation is: 
DY1 0 
D f l  0 
D!j3] 0 
Db3]. 
(15.70) 
Because the potential matrix must obey the underlying symmetry of the system, 
it must be invariant to similarity transformations performed by all of the elements of 
the 6 X 6 representation for D3. By Shur’s lemma, the transformation matrix [TI that 

TABLE 15.30. 6 X 6 Matrix Representation and Character Values for the Spring 
Problem 
- 
0 
0 
-1/2 
&/2 
0 
0 
0 
0 
-&/2 
-1/2 
0 
0 
0 
0 
0 
0 
-1/2 
&/2 
0 
0 
0 
0 
-&/2 
-1/2 
-1/2 
&2 
0 
0 
0 
0 
-&2 
-1/2 
0 
0 
0 
0 - 
1 
r 0
1
0
0
0
0
 
1
0
0
0
0
0
 
- 
0 - 1 0  0 
0 
0
0
1
0
0
 
0
0
0
0
0
 
1
0
0
0
0
 
0 
0 
0 - 1 0  
0
0
0
0
1
 
- 
I 
0
0
1
0
0
0
 
0
0
0
1
0
0
 
R(O) -+ I 
1 
L 0
0
0
0
0
1
 
0
0
0
0
1
0
 
6 
0 
0 
0 
0 
-1/2 
7 / 3 2  
0 
0 
0 
0 
4 / 2  
-1/2 
-1/2 
-&2 
0 
0 
0 
0 
f i / 2  
-1/2 
0 
0 
0 
0 
0 
0 
-1/2 
-&2 
0 
0 
0 
0 
&/2 
-1/2 
0 
0 
-+ 
0 
0 
0 
0 
1/2 
-&2 
0 
0 
0 
0 
7 / 4 2  
-1/2 
0 
0 
1/2 
-&2 
0 
0 
0 
0 
7 / 3 2  
-1/2 
0 
0 
1/2 
7 / 3 2  
0 
0 
0 
0 
7 / 3 2  
-1/2 
0 
0 
0 
0 
1/2 
&/2 
&/2 
-1/2 
0 
0 
0 
0 
0 
0 
0 
0 
0 

IRREDUCIBLE MATRIX REPRESENTATIONS 
629 
- 
- 
A, 
0 
0 
0 
0 
0 
O
h
2
0
0
0
0
 
0 
0 
A3 
0 
0 
0 
0 
0 
0 
A3 
0 
0 
o
o
o
o
A
4
o
 
[V’] = [T]-”V][T] = 
- 0
0
0
0
0
A
‘
l
 - 
(15.71) 
’ 
where we have used [V’] to represent the [Vl matrix in this diagonalizing coordinate 
system. Already group theory has reduced the number of unknown eigenvalues of the 
[V] matrix, and therefore the number of normal mode frequencies for our mass spring 
problem, from six to four. Notice that this has been accomplished without actually 
finding the elements of the [TI matrix. 
The next simplification comes not from group theory, but rather from some general 
physical reasoning. When finding the normal modes of any unconstrained system, 
there will always be a number of modes with zero frequency. These correspond to 
rigid displacements of the system. In a two-dimensional system, such as this example, 
there must be exactly three such modes: two translational and one rotational. We can 
pick hl and the two h3’s to be the eigenvalues associated with these modes, which 
simplifies the [V’] matrix to 
[V’] = 
0
0
0
0
0
0
 
O h 2 0 0 0  0 
0
0
0
0
0
0
 
0
0
0
0
0
0
 
o o o o A ‘ l o  
o o o o o h ,  
(15.72) 
Now the problem has only two unknowns. 
We can determine the values of the remaining eigenvalues by remembering that 
the trace of a matrix is invariant to similarity transformations. Therefore, the trace of 
the original [V] matrix must equal the trace of [V’]: 
(15.73) 
There is only one free parameter left, and we need one more condition to specify 
things completely. The simplest way to obtain it is to consider the squares of the 
potential matrices: [VI2 and [V’I2. The trace of these two matrices must also be 
unaffected by similarity transformations, so we can write 
Tr ([V12) = Tr ([V’I’) 
(15.74) 

630 
INTRODUCTION TO GROUP THEORY 
The simultaneous solution of Equations 15.73 and 15.74 gives the results h2 = 3 and 
& = 312. 
Combining these results with Equation 15.66, we finally have the normal frequen- 
cies: 
The first two frequencies are degenerate, since o, occurs three times and wb twice. 
15.5 
CONTINUOUS GROUPS 
So far, all the groups we have discussed have had a finite number of discrete elements. 
There is another type of a group that has an infinite number of elements, described as 
a function of one or more continuous variables. This section briefly discusses three 
examples of this type of group. 
15.5.1 The 2D Rotation Group 
The C, group consisted of four elements that could be viewed physically as the 
rotations in a plane which left a square in a state indistinguishable from its initial 
state. We mentioned that this group could be generalized to C,,, to handle the symmetry 
of a polygon with n sides. 
What happens when we let n --f m? Now we really have an infinite number of 
elements which correspond to any rotation angle in the plane. If we use the "R' 
notation for rotations, the group elements can be written as continuous function of 
the angle, which we call 8, 
(15.76) 
with 0 5 8 < 277. Let's check to be sure that this definition of the group elements 
satisfies the requirements of a group: 
1. The two rotations applied in succession, R( Ol)R( q), 
is equivalent to R( 61 + Oz), 
which is also an element of the group. Keep in mind that if 81 + S, falls outside 
the range of 0 to 27r, it is necessary to subtract an integer multiple of 21r to put 
the result back into the 0 to 27r range. 
2. The multiplication is associative, because [R(81)R(&)]R(8d = R(61 + 02 + 
3. There is an identity element, Z = R(0). 
4. Each element R(8) has an inverse R(277 - 0). 
63) = R(@1)"82)R(83)1. 

631 
CONTINUOUS GROUPS 
We can also generate some matrix representations for these elements. An obvious 
one-dimensional choice is the complex exponentials: 
R(e) -+ ei@. 
(15.77) 
A different, two-dimensional representation is the set of matrices 
(15.78) 
It is a worthwhile exercise to convince yourself that both these representations obey 
the underlying group multiplication table. 
15.5.2 The 3D Rotation Group 0; 
The set of all possible rotations in three dimensions is another example of acontinuous 
group. It is easy to show that all four group axioms are still satisfied when we move 
into three dimensions. 
The matrix form for the continuous two-dimensional rotation group, given by 
Equation 15.78, can be generalized to three dimensions. If we restrict the rotation 
to be around the z-axis, as shown in Figure 15.12, the matrix representation of the 
rotation is given by the 3 X 3 matrix 
An arbitrary orientation in three dimensions can be accomplished using successive 
rotations around any three different axes. There are many ways to accomplish this, 
but the conventional choice is the rotation sequence 
Here the xl-axis is what the x-axis has become after the first rotation, and the z"-axis 
is what the z-axis has become after the second rotation. It is useful to also describe 
this process in words. First, rotate around the z-axis by an angle 4. This changes the 
Z 
Figure 15.12 Rotation Only About the z-Axis 

632 
INTRODUCTION TO GROUP THEORY 
X 
X’ 
Figure 15.13 
X” 
Euler’s Angles 
x-axis into a new axis, x’, but leaves the z-axis alone. Next, rotate around this x’-axis 
by an amount 8. This transforms the z’-axis into the z”-axis. Finally, rotate around 
the z”-axis with angle $. The three angles +, 8, and $ are called the Euler angles 
and are shown pictorially in Figure 15.13. 
We can write down the representations of each of the individual rotations that 
make up Equation 15.80. The rotation about z is the matrix 
(15.81) 
The rotation around x’ in terms of the primed coordinates is given by 
( 1 5.82) 

CONTINUOUS GROUPS 
633 
The final rotation around z” in terms of the double primed coordinates is 
Multiplication of these three matrices gives the intimidating 3 X 3 matrix: 
-- cos 8 sin + sin 4 : cos 0 sin I,!J cos 4 
: sin 8 sin + 
+ C O S ~ C O S +  
: 
+sin+cosI,!J 
: 
...................................................... 
- cos 0 cos + sin 4 : cos 0 cos + cos 4 : sin 0 cos + 
- sin+ccosc$ 
: 
- sin+sin+ 
: 
...................................................... 
sin 8 sin 4 
: 
-sin8cos4 
: 
cos8 
.84) 
Because this matrix represents a rigid rotation of coordinates, it is necessarily 
orthonormal. It also has a determinant of + 1. In fact, if +, 8, and 4 are allowed 
to roam over their entire 0 to 2.rr ranges, we have all the possible 3 X 3 orthogonal 
matrices with + 1 determinants. For this reason, when the group of three-dimensional 
rotations is written in this manner, it is called 0: , where the 0 stands for orthogonal, 
the 3 for the dimension, and the + to indicate the sign of the determinant. In the 
literature, it is also frequently called S0(3), where the S stands for the “special” 
condition of a +1 determinant. You also might run into the term unimodulur to 
describe matrices with + 1 determinants. 
15.5.3 The Group of Special Unitary Matrices SU(2) 
Our last example of a continuous group is the complete set of 2 X 2 unitary matrices, 
with determinants of + 1. This group is commonly called SU(2). A matrix is unitary 
if its inverse is equal to the complex conjugate of its transpose. The general form of 
a 2 X 2 matrix that satisfies these conditions is 
(15.85) 
with the requirement that g g” + b b* = 1 to ensure the unimodular condition. Notice 
there are really three free parameters here, because the real and complex parts of 
and b provide two each, but the unimodular condition reduces this number by one. 
To show that these matrices form a group, we need to prove that the multiplication 
of any two of them still has the general form of Equation 15.85. This can be verified 

634 
INTRODUCTION TO GROUP THEORY 
by forming the product 
- b& 
a19 + b e ;  ] . 
(15.86) 
-9 a 
- g 2 g  - a;&; -by$ + a;a; 
Now define g3 = g1a2 - l~,& and b3 = g 1 9  + blal, and notice the right-hand side 
of Equation 15.86 can now be written as 
(15.87) 
The unimodular condition still holds: 
This group can be generalized to SU(n), the set of n X n unimodular, unitary 
matrices. These matrices can be shown to have (n2 - 1) independent parameters. 
EXERCISES FOR CHAPTER 15 
1. Let S be a subgroup of a larger group G. If gi is an element in G that is not in S, 
2. Show that there are only two distinct fourth-order groups. 
3. How many classes does the vierergruppe have? 
4. Identify all the subgroups of D3. For each subgroup, indicate the elements of all 
its possible left and right cosets. Show that the ratio h/h' is an integer for all of 
the subgroups, where h is the order of 0 3  and h' is the order of the subgroup. 
show that gi' is also not in S. 
5. Three elements of a group are represented by the following 2 X 2 matrices, 
where w = ei2x'3. 
(a) What are the other elements of this group? 
(b) For each element identify its inverse. 
(c) Find all the subgroups and identify their elements. 

EXERCISES 
635 
6. In Table 15.21, we presented a 3 X 3 matrix representation of the D3 group. 
(a) Find the transformation matrix that puts this representation in block diagonal 
(b) What is the inverse of this transformation matrix? 
(c) Show that the character of the matrices in the block diagonal form are the 
7. Consider the twofold symmetric molecule composed of three pairs of atoms 
shown below. In the chapter, we developed the 0 2  group to represent the sym- 
metry of this object. The four elements of this group satisfy the vierergruppe 
multiplication table given in Table 15.7. 
form. 
same as the original matrices. 
The symmetry of this molecule allows the identification of a group of larger 
order. For example, the interchange of only the two largest atoms will also leave 
the structure unchanged and is not represented by an element of D2. The 0 2  
group describes only rigid body transformations. Interchanging the two largest 
atoms while leaving the others alone is not a rigid body transformation. 
(a) Identify all the elements of this higher-order group and determine its multi- 
(b) Show that D2 is a subgroup of this group. 
(c) Develop a 3 X 3 matrix representation for this group. 
(d) Using the [123456] operator representation, identify the elements of this 
(e) Verify that h/h' is an integer, where h is the order of the larger group and h' 
(f) Determine the left cosets of the 0 2  subgroup. 
plication table. 
higher-order group. 
is the order of 0 2 .  
8. Consider the symmetry group of the regular tetrahedron shown below. If nonrigid 
body transformations are allowed, this becomes the S4 group. 

636 
INTRODUCTION TO GROUP THEORY 
(a) What is the order of S4? 
(b) Obtain a nontrivial 4 X 4 matrix representation for this group. 
(c) Identify the classes of this group and show that the character of each of the 
(d) Identify the subgroup of S4 that corresponds to only the rigid rotations of the 
matrices in part (b) is the same for each class. 
regular tetrahedron. 
9. Construct a 4 x 4 matrix representation for the C4 group. The construction of this 
representation, which we will call C,["l, can be accomplished using the following 
method. According to the C4 multiplication table, g3 g2 = g4. We will require 
that the ghw element obey 
Likewise, g3 g3 = I ,  so we will also require 
(a) Continue this process and determine all four elements of the C:"] represen- 
tation. 
(b) Determine the characters of the Ciw representation. Compare them to the 
characters of the irreducible representations of C4. 
(c) The Cia representation is reducible. Using the completeness and orthog- 
onality of the character table of C4, determine the number of times each 
irreducible representation of C4 appears in the reduced form of Ciw. 
(d) Give the transformation matrix that puts the representation into the reduced 
form in part (c). 

EXERCISES 
637 
10. Consider an object consisting of four identical atoms, as shown below: 
I Y  
C 
I
b
 
(-lJ)* 
~ 
~ 
- I - -  
- 
(1,l) 
X 
-
1
 
I 
t--I,-l)* 
~- -1, 
(1,-1) 
d 
a 
Let the 0 4  group consist of the set of rigid body transformations that leave the 
appearance of this object unchanged. 
(a) Identify all the elements of this group using the [ 12341 operator notation. 
(b) Construct the multiplication table for 0 4 ,  
(c) Identify the inverse of each element of the group. 
(d) Identify all the subgroups of 0 4 .  
(e) Without performing any similarity transforms, identify the different classes 
associated with 0 4 .  
(f) How many possible irreducible matrix representations are there for this 
group? 
(g) A 2 X 2 matrix representation can be constructed for this group by associating 
a two-dimensional transformation matrix with each element. Determine all 
the elements of this 2 X 2 representation. 
If the restriction of rigid body transformations is relaxed, the symmetry of this 
object can be described by the S4 symmetry group, which consists of all the 
permutations of four objects. 
(h) Identify all the elements of S4 using [ 12341 operator notation. 
(i) Show that 0 4  is a subgroup of S4. 
11. Construct a 6 X 6 matrix representation for the 0 3  group, following the same 
prescription used in Exercise 9. We will call this representation OF1. 
(a) Determine the character of the elements of the DbN representations and 
include them as a fourth column in the character table for the 0 3  group, as 
shown below: 

638 
INTRODUCTION TO GROUP THEORY 
(b) Show that the vectors formed by the character values of the irreducible 
representations are all orthogonal to each other, but not to the vector formed 
by the character values of the Diw representation. 
(c) Determine how many times each of the irreducible representations appears 
in the decomposition of the Din representation. 
12. Consider the symmetry group associated with a diatomic molecule. This molecule 
has two identical atoms located in the xy-plane, as shown below: 
(a) Determine the four elements of the symmetry group, which correspond to 
(a) Construct a 2 X 2 matrix representation of this group by associating a trans- 
(e) Determine the multiplication table of this group. 
(d) Find the transformation that simultaneously diagonalizes all the elements 
of the representation found for part (b) and list all the diagonalized matrix 
elements. 
(e) The transformation you found in part (d) can be viewed as a coordinate sys- 
tem transformation. Sketch the locations of the atoms in this new coordinate 
system. 
rigid rotations in the plane and reflections. 
formation matrix with each element. 
13. In Equation 15.68, we derived a matrix representation for the g2 element of D3. 
Derive the other elements of this representation, which are listed in Table 15.30. 
14. Another way to write the general form of the continuous SU(2) group is 
eia cos y 
(15.89) 
where a, p, and y are all real quantities. Show that this form automatically 
satisfies both the unimodular and unitary conditions. 
cosy 1 ' 
eiP sin y 
-e-iP 
e-ia 
[ 

THE LEVI-CIVITA IDENTITY 
The three-dimensional Levi-Civita symbol is defined as 
+1 
fori,j,k = evenpermutationsof 1,2,3 
- 1 
for i, j ,  k = odd permutations of 1,2,3 
. 
(A.l) 
0 
if two or more of the subscripts are equal 
One useful identity associated with this symbol is 
EijkErsk = & 8 j s  - & s s j r .  
64.2) 
To prove Equation A.2, start by explicitly writing out the sum over k on the LHS: 
EijkErsk = 6 j I E r s l  + Eij2Ers2 + Eij3Ers3. 
64.3) 
Now consider eijl E , ] ,  the first term on the FWS of Equation A.3. This term is zero if 
i, j ,  I ,  or s is equal to 1. It is also zero if i = j or r = s. Therefore, ~ i j l  
E,I is nonzero 
for only four combinations of the indices (i, j ,  r, s): (2,3,2,3), (2,3,3,2), (3,2,2,3), 
and (3,2,3,2). Thus the first term on the RHS of Equation A.3 becomes 
+1 
for(i,j,r,s) = (2,3,2,3)or(3,2,3,2) 
~
j
~
l
~
~
~
~
 
= 
-1 
for(i,j,r,s)= (2,3,3,2)or(3,2,2,3) . 
(A.4) 
{ 0 
otherwise 
The other two terms on the RHS of Equation A.3 behave in a similar fashion, with 
the results: 
+1 
for(i,j,r,s) = (1,3,1,3)0r(3,1,3,1) 
-1 
for (i, j,r,s) = (1,3,3,1) or (3, 1, 1,3) 
(A.5) 
0 
otherwise 
639 

THE LEVI-CIVITA IDENTITY 
640 
and 
+1 
for(i,j,r,s)=(l,2,1,2)or(2,1,2,1) 
eij3erS3 = 
-1 
for(i,j,r,s) = (1,2,2,1)0r(2,1,1,2) . 
(A.6) 
I 0 
otherwise 
The LHS of Equation A.2 is the sum of the three terms described above. For a 
given set of values for i, j, r, and s with i # j and r # s, only one of the terms is 
nonzero. Therefore, 
+1 
fori # j , r  # s,i = r, j = s 
EijkE& = - 1 
for i # j, r # s, i = s, j = r . 
64.7) 
{ 0 
otherwise 
But notice, these conditions are also satisfied by the quantity 6 i r 6 j S  - 6 i s 6 j r ,  because 
+1 
fori# j , r  # s , j = s  
6.6. -6.6. = 
-1 
f o r i # j , r # s , j = r .  
(A.8) 
0 
otherwise 
ir 
j s  
IS j r  
Hence, we have our result: 

THE CURVILINEAR CURL 
A rigorous derivation of the form of the curl, gradient, and divergence operators in 
curvilinear coordinate systems requires a careful analysis of the scale factors and 
how they change as functions of position. In the text, this was treated in a somewhat 
cavalier manner. In this appendix, we present a more rigorous derivation of the 
curvilinear curl operation. The final result is, of course, the same as presented earlier. 
The rigorous derivation of the gradient and divergence operations follow a similar 
manner. 
The curl operation can be defined by the integral relation 
where C is a closed path surrounding the surface S, and the direction of d B  is defined 
by the direction of C and the right-hand rule. If we align the surface perpendicular to 
the 1-direction, as shown in Figure B. 1, and take the limit as the surface shrinks to a 
differential area, the LHS becomes 
This term is proportional to the 1-component of the curl. 
The RHS of Equation B.1 in this differential limit is 
641 

642 
THE CURVILINEAR CURL 
Figure B.l Orientation of Surface for Curvilinear Curl Integration 
where the loop has been broken up into four parts, as shown in Figure B.2. The 
contribution from C1 is given by 
420 + 4 2  
1 , d F . A  = Lb 
dq2A2hz. 
03.4) 
Along C1, both A2 and h2 can vary with q2. Since we are in the differential limit, we 
will consider only the lowest-order, linear variation of the integrand: 
Substituting Equation B.5 into Equation B.4 gives 
Next the integration along C, will be performed: 
-~ 
__---_ 
h2dq2 
Figure B.2 Differential Geometry for Curvilinear Curl Integrations 

THE CURVILINEAR CURL 
643 
hlcil 
h242 
h343 
hlAl 
h2A2 
h3A3 
V X A = - - -  
a/aq, 
d/%2 
d/aq3 
Here the expression for A2h2 picks up an extra term because q3 = q30 + dq3: 
I 
(B. 14) 
der 
Substituting Equation B.8 into Equation B.7 gives for the line integral along C3, 
Similar integrations can be performed along C2 and C,. When summed, these 
pieces give the value for the closed line integral: 
(B.lO) 
Notice how the higher-order terms have all canceled. Substituting Equations B. 10 
and B.2 into Equation B.l gives for the 1-component of the curl of A, 
(B.ll) 
The other components of 7 
X 
in Figure B. 1 to be along the other curvilinear axes: 
can be obtained by reorientating the surface shown 
(B.12) 
(B.13) 
(B.15) 

This Page Intentionally Left Blank

THE DOUBLE INTEGRAL IDENTITY 
In this appendix, the double integral identity 
[ 
dx” I” dx’ f(x’) = [ 
dx’ (x - x’)f(x’) 
is proven. To derive this relation, make the substitution 
Equation C. 1 becomes 
The first integration on the LHS, over the variable x’, can be done directly. The 
integral on the F2HS can be rewritten using integration by parts, with u = ( x  - x‘) 
and du = (dy/dx‘)dx’, to give 
.I‘ 
dx”[y(x”) - y(a)] = (x - x’)y(x’) lx‘=‘ 
x’=a + [ 
dx’ y(x’). 
(C.4) 
When integration of y(a) on the LHS is performed, and the first term on the RHS is 
evaluated at the limits, the result is 
(a - x)y(a) + l’ 
dx” y(x”) = (a - x)y(a) + lx 
dx’ y(x’). 
(C.5) 
Since x’ and x” are dummy variables of integration, the two sides of this equation are 
obviously equal, and the identity of Equation C. 1 is proven. 
645 

This Page Intentionally Left Blank

D 
GREEN’S FUNCTION SOLUTIONS 
In Chapter 10, the Green’s function solution for a linear, nonhomogeneous, second- 
order equation was obtained using a superposition argument. We first solved the 
special case of a &function drive for the nonhomogeneous term. Then we argued 
that the general solution, with an arbitrary nonhomogeneous term, could be obtained 
by simply forming an infinite weighted sum of these &function solutions. This proof 
relied on a certain amount of intuition. A formal proof of this solution can be obtained 
by directly inserting the Green’s function solution into the differential equation, and 
showing that the nonhomogeneous differential equation and the boundary conditions 
are satisfied. 
Consider the general linear, nonhomogeneous, second-order differential equation: 
This equation is in Sturm-Liouville form, but keep in mind any linear, second- 
order equation can be put into this form with a little manipulation. We want to find 
the function y(x) which solves Equation D.l and satisfies the set of homogeneous 
boundary conditions: 
Ix=a 

648 
GREEN'S FUNCTION SOLUTIONS 
We will prove that the integral 
satisfies these conditions, where g(x16) is a function that satisfies the differential 
equation 
and the same set of homogeneous boundary conditions: 
We begin by describing the properties of the Green's function g(x15). Notice 
everywhere, except at x = 6, g(xl6) satisfies the homogeneous differential equation 
If gI(x15) and gz(x15) are the solutions to this homogeneous equation on either side 
of x = 5, we can write 
Since Equation D.8 is a second-order equation, its general solution will contain two 
arbitrary constants. But these constants are not necessarily the same on the two sides 
of x = 6. so there really are a total of four unknown constants associated with Equa- 
tion D.9, two for gI(x15) and two for gz(x15). Therefore, we need four independent 
equations to completely specify g(x15). ' b o  come directly from the boundary con- 
ditions. Using gI(x15) in Equation D.6 establishes one equation. Equation D.7 with 
gz(xl&) establishes the second. A third equation comes from requiring that g(x16) be 
continuous at x = 5: 
This continuity is guaranteed by the form of Equation D.5. If g(x15) were discontin- 
uous at any point, the second derivative operation on the LHS of Equation D.5 would 
generate a term proportional to the derivative of a 8-function, a term that obviously 
does not exist on the RHS of Equation D.5. The last equation comes from integration 

GREEN'S FUNCTION SOLUTIONS 
of Equation D.5 across the &function: 
For arbitrarily small E this becomes 
649 
(D. 1 1) 
(D.12) 
where we have assumed that Q(x) and P(x) are well-behaved functions, so near 
x = f ,  Q(x) -+ Q(5) and P(x) -+ P(5). As E + 0, Equation D.12 becomes 
(D.13) 
The &function on the RHS of Equation D.5 causes a discontinuity in the derivative 
of g(x15) at x = ,$. 
Next we show that Equation D.4 is a solution to the original differential equation, 
by substituting it into Equation D.l. In terms of gl(x15) and g2(xI(), this solution is 
Y ( X )  = I d 5  h(5)g2(x15) + 
d5 h(Ogl(xl0. 
(D.14) 
Before putting this in the LHS of Equation D.l, we need to evaluate its first and 
second derivatives. This is a bit tricky, because the derivatives are taken with respect 
to a variable that is in both the integrand and the limits of integration. To evaluate 
such a derivative, use the result 
b 
(D.15) 
Therefore, 
(D. 16) 
The last two terms in this expression cancel because the Green's function is continu- 
ous, and we have 

650 
GREEN’S FUNCTION SOLUTIONS 
The second derivative of y(x) is 
(D.18) 
( X I 0  
- h(x)----- 
dg2 ( X I  5) 
+ h(x)--- 
dx 
dx 
The last two terms of this expression do not cancel because of the discontinuity in 
the derivative of the Green’s function we discovered earlier. Using Equation D. 13, 
however, these terms can be combined to give 
Substituting these expressions for y(x) and its derivatives into the LHS of Equa- 
tion D.l gives 
(D.20) 
Both gl(x(5) and gz(xl6) are solutions to the homogeneous Equation D.8. Because 
of this, all the terms in Equation D.20 cancel, except the single term h(x). Our final 
result is 
which shows that the Green’s function solution for y(x) indeed satisfies the original 
nonhomogeneous differential equation. 
It also easy to show that the Green’s function solution satisfies the original ho- 
mogeneous boundary conditions. The boundary condition at x = a is given by 
Equation D.2, and using Equation D.14 and D.17 evaluated at x = a, we can write 
= 0. 
(D.22) 

GREEN’S FUNCTION SOLUTIONS 
651 
Since we constructed the Green’s function to obey the same boundary condition, 
as given by Equation D.6, the last line in Equation D.22 is zero. A similar line of 
reasoning shows that the boundary condition at x = b, given by Equation D.7, also 
works. 

This Page Intentionally Left Blank

E 
PSEUDOVECTORS AND 
THE MIRROR TEST 
The mirror test provides an intuitive way to identify a pseudovector. A “regular” 
polar vector will reflect, as you would expect, in a mirror but a pseudovector will be 
reversed. 
An example of this test, taken from electromagnetism, is described in Figure E. 1. 
The real system on the left is a positively charged particle moving in a circular 
orbit. counterclockwise when viewed from above. This motion results in the average 
~ 
B /S+ 
r 
Real object 
! B  
Mirror image 
Figure E.l The Mirror Test for Pseudovectors 
653 

654 
PSEUDOVECTORS AND THE MIRROR TEST 
magnetic field pointing “up,” as indicated by the B vector. At a particular instant of 
time, this particle is at a position indicated by i and moving at a velocity V. Now view 
the image of this motion in the mirror shown in Figure E. 1. The image particle moves 
in a clockwise, circular orbit. This motion is properly described by the images of the 
position and velocity vectors. Hence, they are regular vectors. The reflected image of 
the magnetic field vector is still in the up direction. But notice, if a positively charged 
particle were really performing the motion of this image particle, the magnetic field 
would point down, as indicated by the dashed B vector. The magnetic field vector 
fails the mirror test, and is therefore a pseudovector. 

CHRISTOFFEL SYMBOLS AND 
COVARIANT DERIVATIVES 
Non-orthonormal coordinate systems become more complicated if the basis vectors 
are position dependent. An example of a two-dimensional coordinate system of this 
type is shown in Figure F. 1. In this system, the displacement vector can still be written 
as 
Since 
dT = (af) 
dX' 
dx' + (z) 
8x2 
dx2, 
the covariant basis vectors are still identified as 
but now are functions of position. The effect of nonconstant basis vectors is most 
evident when applying derivatives to vector and scalar fields. 
In Chapter 14, the gradient operation was discussed for skewed coordinate systems, 
where the basis vectors 
gradient was defined as 
were not orthonormal and had constant basis vectors. The 
655 

656 
CHRISTOFFEL SYMBOLS 
\ 
I 
--. 
i ; x1 
Figure El A Coordinate System in Curved Space 
and the del operator itself was identified as 
The derivative d+/dx', is the irh covariant component of the gradient vector. Notice 
how the contravariant basis vector g is not differentiated. This is important, because 
when we move to systems where the basis vectors are no longer constants, the gradient 
operation does not need to be modified in any way. 
Things are very different for the divergence of a vector field. Using the same 
definition for the del operator, the divergence of a vector v = Vi& becomes 
Now the spatial derivative operates both on the vector components V' and the gi basis 
vectors. Rearranging Equation F.6 gives 
- 
v . v = gj . 
(vie). 
F.7) 
axJ 

CHRISTOFFEL SYMBOLS 
657 
If the basis vectors are not constants, the RHS of Equation F.7 generates two terms 
The last term in Equation F.8 is usually defined in terms of the Christoffel symboE 
rkj: 
The definition in Equation F.9 implies the result of the differentiation on the LHS 
must be a vector quantity, expressed in terms of the covariant basis vectors &. 
The contravariant components of the vector quantity are given by the Christoffel 
symbol with a superscripted k. These components obviously also depend on which 
basis vector is being differentiated, given by the i index, and which coordinate the 
differentiation is being taken with respect to, given by the j index. Because of 
the nature of the LHS of Equation F.9, these indices are written as subscripts, so 
that the Christoffel symbol in Equation F.9 obeys the normal superscriptlsubscript 
conventions. It is important to note, however, the Christoffel symbol is not a tensor. 
Its elements do not transform like the elements of a tensor. 
In order to complete our discussion of the divergence, we must evaluate the 
Christoffel symbols in terms of the coordinate system geometry. Equation F.9 can be 
solved for rkj by dot multiplying both sides by g': 
or 
(F. 10) 
(F. 1 1) 
The basis vectors can still be written in terms of derivatives of the position vector, 
so the Christoffel symbol becomes 
(F.12) 
(F.13) 
This equation clearly indicates that the Christoffel symbol has a symmetry with 
respect to the subscripted indices 
Equation F. 13 provides a method for evaluating the Christoffel symbol, but it is 
not very useful because it is in terms of the derivatives of the position vector and 
the basis vectors of the coordinate system. A more useful relation can be derived by 

658 
CHRISTOFFEL SYMBOLS 
considering the metric. Remember the metric for a coordinate system is 
M . .  
1J = & . g. 
I' 
(F. 15) 
Even though the Christoffel symbol is not a tensor, this metric can be used to define 
a new set of quantities: 
This quantity, rbj, is often called a Christoffel symbol of the first kind, while rkj 
is a Christoffel symbol of the second kind. Notice the Christoffel symbol of the first 
kind exhibits the same symmetry with respect to the last two subscripts: 
Combining Equations F. 1 1 and F. 16 gives 
The spatial derivative of the metric, 
can now be written in terms of Christoffel symbols as 
Similarly, 
and 
(F. 18) 
(F. 19) 
(F.20) 
(F.21) 
(F.22) 
Adding Equation F.22 to F.2 1 and subtracting F.20, while malung use of the symmetry 
of the Christoffel symbols, gives 
(F.23) 

CHRISTOFFEL SYMBOLS 
659 
Raising the first index with the metric gives an expression for the Christoffel symbol 
of the second kind in terms of the coordinate system's metric: 
(F. 24) 
This equation allows us to evaluate the Christoffel symbol if we know the metric. 
Christoffel symbol as 
Returning to the divergence operation, Equation F.8 can now be written using the 
(F.25) 
The quantity in brackets on the RHS is referred to as the covariant derivative of a 
vector and can be written a bit more compactly as 
(F.26) 
where the Christoffel symbol can always be obtained from Equation F.24. If the basis 
vectors are constants, r;, = 0, and the covariant derivative simplifies to 
(F.27) 
as you would expect. 
V is 
The curl operation can be handled in a similar manner. The curl of the vector field 
d 
- 
v x v = gj- 
x p i g i ) ,  
ax] 
which, written in terms of the covariant derivative, is 
(F.28) 
(F.29) 

This Page Intentionally Left Blank

CALCULUS OF VARIATIONS 
The development of the calculus of variations begins with an integral in the following 
form: 
I = l; 
dxfb,Y(z),dY/dzl. 
(G.1) 
The function in the integrand depends upon the independent variable x and on the 
unknown function y(x) and its derivative dy/dz. Both y(z) and its derivative are 
taken to be functions of the independent variable 2. The problem is to determine the 
function y(x) so as to minimize the value of the integral. An example of such an 
integral is. 
This form looks like that of an integral equation because the unknown function is 
inside an integral operation. But it is unlike an integral equation in that the value of 
the integral is not known and also because the minimization of I does not uniquely 
determine y(z) unless a set of boundary conditions like 
((3.3) 
((3.4) 
is specified. Here y1 and y2 are known constants. 
The minimization process is accomplished by introducing a parameter a so that 
I ---f I ( a )  and requiring dI/dcr = 0 at Q = 0. The cr parameter is introduced by 
letting 
where y(z) is the function that minimizes I(Q) with (1y = 0. The idea is that adding 
any function ~ ( x )  
with cy # 0 causes I(a) to be larger than I(a = 0). 
Y(Z) + Y(Z, 
= Y(Z) + adz) 
((3.5) 
661 

662 
CALCULUS OF VARIATIONS 
The function ~ ( x ,  
a) and its derivative are then introduced into the integrand of 
Equation G. 1. 
The boundary conditions 
A shorthand notation 
(G.lO) 
(G. 1 1 )  
will be used to simplify the equations. 
The problem now becomes one of minimizing I ( a )  with respect to a! where 
(G. 12) 
(G.13) 
(G. 14) 
(G.15) 
(G.16) 
To accomplish this take d/da of the above expression and set it equal to zero at 
a! = 0. 
(G.17) 
or 
Now 
so 
(G.20) 

CALCULUS OF VARIATIONS 
663 
and 
so that 
((3.21) 
Notice that these partial derivatives of y(x, a) and yr(x. a) with respect to cy are no 
longer a function of 0. 
Substituting the5e values for the partial derivatives of y(x. 0) and gL (L. a )  into 
Equation G. 18 gives, 
The second integral 
is attacked by an integration by parts with 
and 
so that 
((3.24) 
((3.25) 
((3.26) 
(G.27) 
The first term on the right hand side of this equation vanishes because ~ ( r )  
is zero at 
X I  and .r2. 
The minimization condition then becomes 
(G.29) 
Since this must be true for any q(x) when (Y = 0, the condition for minimizing I(<?) 
becomes 
(G.30) 
((3.3 1 ) 

664 
CALCULUS OF VARIATIONS 
where 
Y = Y(X> a) = Y(X) + Q+) 
(G.32) 
and the function y(x) is the function we seek, the function that minimizes the original 
integral. In the limit (Y = 0 the above condition for minimization of the original 
integral becomes 
and 
dY (X) 
yx = -. dx 
Equation G.33 is solved for y(x) given the boundary conditions 
This is known as Eulers Equation. It can be written in equivalent form as 
(G.33) 
(G.34) 
(G.35) 
(G.36) 
(G.37) 
(G.38) 
This equivalence can be seen by expanding the total derivative with respect to x. 
Equation G.33 is useful if there is no explicit y-dependence in the integrand because 
it reduces to 
(G.39) 
Equation G.38 is useful iff has no explicit x-dependence because it reduces to 
(G.40) 

ERRATA LIST 
p. 10 Change Equation 1.48 to read: 
lc\ = (A1 IBJ sin0, 
p. 16 
(i) Exercise 2. first equation, change c, j to i, j :  
= i j 2  for 2,j = 1,2,3, 
(ii) Exercise 4, first line, change "row matrix" to "column matrix". 
(iii) Exercise 4, first equation, change equation to read: 
"Dl [VII + = Vl+ [Dl 
p. 49 Change Equation 3.14 to read: 
- 
r = (F . eT)& + (T . &)& + (F .6+)6+. 
p. 64 Exercise 20, change equation to read: 
- 
B = BOG,. 
p. 65 Exercise 20. change equation to read: 
(1.48) 
(3.14) 
665 

666 
ERRATALIST 
p. 75 Change Equation 4.45 to read: 
(4.45) 
p. 133 Exercise 25, first line, change pc(z, y, y) to pc(2, y, z). 
p. 152 In all equations on the page, change gn to c,, i.e., 
cc 
f (2) = C ~ " ( 2  
- zo), = C, + C ~ ( Z  - 2,) + C~(Z - 2,)' + . . . . 
(6.96) 
n = O  
(6.97) 
(6.98) 
(6.99) 
- 
- ...+ 
c-2 
+ 
c-1 
(2 - 2,)2 
+ co + c1(z - 2,) + c2(z - zo)2 + . . . , 
(2 - 2,) 
p. 162 Paragraph below Equation 6.135, 
(i) third line, change C1 to C2 and C2 to CI . 
(ii) seventh line, change C2 to C1. 
p. 215 Exercise 49. (b), change y = 1/2 to u = 1/2. 
p. 216 Exercise 55, change the sentences beginning in the second line of the para- 
graph to the end of the paragraph to read: 
Integrate to obtain 2 = &I). 
p. 241 Change Equation 7.90 to read: 
p. 242 Change the first line of Equation 7.92 to read: 
N-1 
(7.90) 
n=-N 

ERRATA LIST 
667 
p. 300 Exercise 21: 
(i) Change the last sentence of the first paragraph to read: 
“Show that the conditions for closure with zero contribution are different for a 
real, definite integral than they are for a Fourier inversion.” 
(ii) Change the first equation to read: 
W 2  
w3 + i  
F ( w )  = - 
(iii) Change the equation under (b) to read: 
p. 376 Figure 9.14, change the label on the abscissa from “imag” to “real”. 
p. 378 Figure 9.16, change the label on the abscissa from “imag” to “real” 
p. 354 Change the part of the sentence below Equation 10.104 to read: 
“. . .where the r,, are unknown coefficents, and s is a fixed number, not necessarily 
an integer or pure real. Often, however, it is a positive or negative integer.” 
p. 379 Figure 10.16, change -K x(t) to read -KO x(t). 
p. 383 Title of Figure 10.2 1, change “Principal” to “Principle”. 
p. 408 Exercise 22 (a), last line, change IC = z, to 3 = yo. 
p. 427 Exercise 53, first line below the equation, change a(z), b(z) top(z), q(s). 
p. 422 Exercise 54 (d), first line, change d(t) to d(z). 
p. 428 Change Equation 1 1.20 to read: 
cc 
III 
,2 
(11.20) 

668 
ERRATALIST 
p. 448 Split the last sentence of the paragraph Orthogonality Relations into two 
sentences: 
. . . eigenfunction problem. 
A Hermitian operator can then be identified and the orthogonality properties of the 
eigenfunctions determined. 
p. 449 Change Equation 11.11 1 to read: 
p, 453 Change Equation 1 1.136 to read: 
p. 462 Change Equation 1 1.176 to read: 
p. 463 
(i) Line above Equation 1 1.180, change Be( z) to Be ( X )  . 
(ii) Line aboveEquation 11.181,change Ao(z) to A,(%). 
p. 464 Third paragraph, change “six” to “five”. 
p. 473 Figure 11.21, last line of equation, change +VO to -VO. 
p. 474 Equation 1 1.237: replace closing brace by opening brace: 
p. 482 Exercise 16(a), change Equation to read: 
(1 1.11 1) 
(1 1.136) 
(1 1.176) 
(1 1.237) 
p. 484 Exercise 24, change the second and third line below the first equation to 
read: 
“. . . the differential equation for radially dependent component becomes” 

ERRATALIST 
669 
p. 489 Exercise 34: insert 0 on the right-hand side of Laplace’s equation in two 
dimensions: 
p. 507 Exercise 7, change equation to read: 
PX 
q(z) = x - 
dt (t - .)$(t). 
J! 
p. 508 Exercise 12.i., change equation to read: 
1 
?$(z) = x 
dt (. 
- t)”(t). 
1, 
p. 537 First line of the paragraph above Equation 13.45, change sentence to read: 
“We should also look carefully at the little circular part. . . ” 
p. 542 
(i) Second line from top of page, change term to read dz = e22.rrdr. 
(ii) Change Equation 13.57 to read: 
(13.57) 
p. 547 Change Equation 13.72 to read: 
(13.72) 
dU 
.i3v 
aw 
aw 
ax 
d z  
ax 
ag 
- 
+ 2- 
= z 
= = = 0 
at the saddle points. 
p. 581 Change Equation 14.100 to read: 
(14.100) 
d 
= tZ-, 
3 ax, 
p. 593 Exercise 3, change equation to read 
p. 611 In the first line of the paragraph above Table 15.16, change “are shown” to 
“is shown”. 
p. 614 In the fourth line from the top of the second paragraph, change “dimension” 
to “dimensions”. 

670 
ERRATALIST 
p. 619 Table 15.25, row Cf], column 94, change “-1” to “4’ 
p. 623 Change the sentence beginning in the 2nd line of the 2nd paragraph to read: 
“The net result is that there is one Dtl representation, no D f l  representation, and 
one Drl representation in the block diagonalization.” 
p. 625 Equation 15.62, change element (1,3) and (3,l) from “1” to “-1”. 

BIBLIOGRAPHY 
Abramowitz, Milton, and Irene Stegun, Handbook of Mathematical Function. Dover Publica- 
Arfken, George, Mathematical Methods of Physics. Academic Press, San Diego, CA, 1985. 
Butkov, Eugene, Mathematical Physics. Addison-Wesley, Reading, MA, 1968. 
Churchill, Rue1 V., and James Ward Brown, Complex Variables and Applications. McGraw- 
Griffel, D.H., Applied Functional Analysis. John Wiley and Sons, New York, 1981. 
Hamemesh, Morton, Group Theory and its Applications to Physical Problems. Adclison- 
Hildebrand, F.B., Methods ofApplied Mathematics. Prentice-Hall, Englewood Cliffs, NJ, 1958. 
Jackson, J.D, Classical Electrodynamics. John Wiley and Sons, New York, 1962. 
Kaplan, Wilfred, Advanced Calculus. Addison-Wesley, Reading, M A ,  1973. 
Mathews, Jon, and R.L. Walker, MathernaticalMethods of Physics. Addison-Wesley, Redwood 
Misner, Charles W., Kip S. Thome, and Jon Archibald Wheeler, Gravitation. W.H. Freeman, 
Sokolnikoff, Ivan, and R.M. Redheffer, Mathematics of Physics and Modem Engineering. 
Taylor, Edwin F., and Jon Archibald Wheeler, Spacetime Physics. W.H. Freeman, San Fran- 
Thomas, George B. Jr., and Ross L. Finney, Calculus andAnalytic Geometry. Addison-Wesley, 
Tolstov, Georgi P., Fourier Series, translated by Richard A. Silverman. Dover Publications. 
Tung, Wu-Ki, Gmup Theory in Physics. World Scientific Publishing, Philadelphia, PA, 1985. 
Watson, G.N., A Treatise on the Theory of Bessel Functions. Macmillian, New York, 1944. 
Wigner, Eugene Paul, Group Theory and its Applicatons to the Quantum Mechanics of Atomic 
tions, New York, 1964. 
Hill, New York, 1990. 
Wesley, Reading, MA, 1962. 
City, CA, 1970. 
San Francisco, CA, 1973. 
McGraw-Hill, New York, 1966. 
cisco, CA, 1963. 
Reading, MA, 195 1. 
New York, 1962. 
Spectra. Academic Press, New York, 1959. 
671 

This Page Intentionally Left Blank

INDEX 
Abelian group, 598,618 
Absolutely integrable function, 254 
Aliasing, 240-241 
Analytic continuation, 175 
Analytic function, see Complex functions, analytic 
Associated Legendre equation, 460 
Associated Legendre polynomials, 472 
Autocorrelation, 266 
Axial vector, 89 
Basis functions, 234 
Basis vectors 
contravariant, 576-579 
covariant, 573,576-579 
curvilinear coordinates, 49-5 1 
nonorthogonal, 564 
orthonormal, 2,44,75 
position dependent, 655 
position independent, 2,564 
Bessel equation, 444 
spherical, 354 
Bessel functions 
first kind, 444446 
modified, 453 
orthogonality, 448450 
second kind, 444-446 
spherical, 357 
Bilateral Laplace transform, see Laplace 
transform, double-sided 
Block diagonal mamces, 613 
Boundary conditions, 342 
Dirichlet, 429 
for Green’s functions, 384,387-391 
homogeneous, 384 
Neumann, 429 
nonhomogeneous, 387-39 1 
Branch cuts, 5 19-522 
Branchpoints, 516519 
Bromwich contour, 3 13,333 
Cartesian coordinates, 2 
Cauchy integral formula, 147-150 
Cauchy integral theorem, 144147,510-513 
Cauchy-Riemann conditions, 141-144,546 
Causality, 370 
Character, 610-612 
Character table, 621-622 
Child-Langmuir problem, 359-366,410-412 
Christoffel symbols, 655-659 
Class, 610-612.614 
Cleverly closed contour, 540 
Cofactor, 572 
Completeness 
C~OSUIE, 
175-189,284-285.310-312 
Fourier series, 234 
group representations, 622-623 
Sturm-Liouville eigenfunctions, 437 
Completing the square, 27 1 
Complex functions, 138-202.509-542 
analytic, 140-150 
derivatives of, 140-144 
hyperbolic, 140 
logarithm, 140 
multivalued functions, 509-542 
trigonometric, 139-140 
visualization of, 138 
673 

674 
INDEX 
Complex plane, 135-136 
Complex variables, 135-137 
conjugates, 136 
magnitude, 136 
polar representation, 136-137 
real and imaginary parts, 135, 136 
Conductivity tensor, 67-70,76 
Conformal mapping, see Mapping, conformal 
Conservative field, 38 
Continuity equation, 27-29,361 
Contour deformation, 146-147,532-539,542 
Contravariant components, 568-582 
Convergence 
absolute, 150, 151 
complex series, 150-151 
Fourier series, 23 1-234 
mean-squared, 232-234 
pointwise, 232 
ratio test, 151 
uniform, 155,232 
Fourier transform of, 261-280 
Laplace transform of, 320-323 
Convolution, 261-265,325 
Correlation, 265-266 
Cosets, 608-610 
Covariant components, 568-570,573-582 
Covariant derivatives, 655-659 
Cross-correlation, 265 
Cross product, 8, 10-12,53,86-92 
Curl 
Cartesian coordinates, 24 
curvilinear coordinates, 55-58 
cylindrical coordinates, 58 
integral definition, 34 
nonorthonormal coordinates, 659 
physical picture of, 29-32 
spherical coordinates, 58 
Current density, 28,68 
Curvilinear coordinates, 49-58 
Cylindrical coordinates, 45-47 
Decomposition of block matrices, 622 
Del operator 
Cartesian coordinates, 23-24 
curvilinear coordinates, 54-58 
identities, 32-34.41 
Delta function, Dirac, see Dirac delta function 
Delta, Kronecker, see Kronecker delta 
DeMoivre’s formula, 203 
Density functions, singular, 114-121 
Differential equations, 339-403 
boundary conditions, 342 
constant coefficients, 347-349 
coupled, 341 
exact differential, 343-345 
first-order, 342-346 
Fourier transform solutions, 366-371 
Green’s function solutions, 376403 
homogeneous, 341-342 
integrating factor, 345-346 
Laplace transform solutions, 371-376 
linear, 340-342 
nonhomogeneous, 341-342.35 1-354 
nonlinear, 340,358-359 
order, 341-342 
ordinary, 340 
partial, 340,391 
second-order, 347-354 
separation of variables, 342-343,424-475 
series solutions, 35&358 
terminology, 339-342 
Diffusion equation, 391-398 
Dipole, 123-125 
Dipole moment, 122 
Dirac delta function, 100-126 
complicated arguments, 108-1 11 
derivatives of, 112-1 14 
Fourier transform of, 269-270 
integral definition, 106-108 
integral of, 11 1-1 12 
Laplace transform of, 3 14-3 16 
sequence definition, 104-105 
shifted arguments, 102 
three-dimensional, 115 
use in Green’s function, 378,382 
use in orthogonality relations, 253-254 
Direct sum of block matrices, 613 
Dirichlet boundary conditions, 429 
Discrete Fourier series, 234-242 
Displacement vector, 52 
Divergence 
Cartesian coordinates, 24 
curvilinear coordinates, 54-55 
cylindrical coordinates, 58 
integral definition, 34 
nonorthonormal coordinates, 6 5 U 5 9  
physical picture of, 27-29 
spherical coordinates, 58 
nonorthonormal coordinates, 568-570,573 
tensors, 70 
Double-sided Laplace transform, see Laplace 
Doublet, 112-1 14 
Dyadic product, 7 1 
Dot product, 7-10,53 
transform, double-sided 
Eigenfunction, 433440 
Eigenvalue, 79,435-436 

INDEX 
675 
Eigenvector, 79,435 
Einstein summation convention, 2 
Electric dipole, 123-125 
Electric monopole, 122-123 
Electric quadrapole, 133 
Elliptical coordinates, 62 
Essential singularity, 170 
Euler angles, 632 
Euler constant, 445 
Euler’s equation, 1 3 6  I37 
Exact differential, 343-345 
Factorial function, 553 
Fast Fourier transform (FFT), 234 
Field lines, 20 
Fourier series, 219-242 
circuit analogy, 223-224 
convergence, 23 1-234 
discrete form, 234-242 
exponential form, 227-23 1 
orthogonality conditions, 221-223,228-229 
sine-cosine form, 219-223 
of a square wave, 225-227 
of a triangular wave, 224-225 
Fourier transform, 250-295 
circuit analogy, 256.257 
of a convolution, 261 
of a cross-correlation, 266 
of a damped sinusoid, 287-290 
of a decaying exponential, 283-287 
of a delayed function, 258 
of a delta function, 269-270 
of a derivative, 259-260 
differential equation solutions, 366-37 1 
of even and odd functions, 259 
existence, 254256 
of a Gaussian, 270-273 
integral equation solutions, 499 
limits of, 303-313 
orthogonality condition, 253-254 
of a periodic function, 273-275.279-280 
of a product, 260-261 
of pure real functions, 259 
relation to Fourier series, 250-253.275 
of a square pulse, 267-269 
transform pair, 252 
first kind, 492499 
second kind, 492,504 
Fredholm equation 
Frobenius, method of, 354358,444, 461, 
469 
Gamma function, 553-554 
Gauss’s theorem, 35-36 
Gaussian 
Fourier transform of, 270-273 
sequence function, 105 
General relativity, 565-566 
Generalized coordinates, see Curvilinear 
Generalized functions, 100, 103 
Geodesics, 566 
Gibbs phenomenon, 233 
Gradient 
coordinates 
Cartesian coordinates, 24 
curvilinear coordinates, 54 
cylindrical coordinates, 58 
integral definition, 34 
nonorthonormal coordinates, 582,656 
physical picture of, 24-27 
spherical coordinates, 58 
Gram-Schmidt orthogonalization, 437 
Green’s functions, 376403,647-651 
boundary conditions, 384,387-391 
for diffusion equation, 391-398 
for driven wave equation, 3 9 8 4 3  
multiple independent variables, 391-403 
for a stretched string, 385-391 
symmetry properties, 398,422423 
translational invariance, 378,398 
Green’s theorem, 3637 
Groups, 597-634 
Abelian, 598,618 
character, 610-612 
character table, 621-622 
class, 610-612, 614 
continuous, 630-634 
cosets, 608-610 
D2.601403 
D3.604-605 
definition, 597-598 
finite, 598-607 
multiplication table, 598-599 
order, 598 
permutation, 607 
rearrangement lemma, 599 
representations, see Representations, group 
rotation, 630-633 
SU(2). 633-634 
subgroups, 607-608 
C,, 598-601 
Harmonic oscillator 
damped, driven, 368-37 1 
simple, 347 
undamped, driven, 378-381 
Heaviside step function, 11 1-1 12, 316-317 
Heisenberg uncertainty principle, 273 

676 
INDEX 
Hemholtz's theorem, 38-40 
Hermitian 
matrix, 80,435 
operator, 436-440 
Homogeneous boundary conditions, see 
Homogeneous difbmtd 
- equation,see 
Hyperbolic coordinates, 62 
Boundary conditions. homogeneous 
Memntid equstions, homogeneous 
Impulse, ideal, 101-102 
hdicial equations, 355 
ImKr product, see Dot product 
classification of, 492493 
Follrer transfm SoIutiOns, 499 
Laplace transform solutions, 499-500 
relation to diffenatipl M o m .  
separable h
l
 
soluriom. 504-506 
series s o l u t i ~  
501-503 
Integral eq~ations, 491-506 
493-498 
Integrals 
analytic functions. 144-150 
closed, 21,175-189,510-513 
line, 7,21,53 
multivalued functions, 5 10-5 13,534-542 
operator form, 20-21 
principal part, 184-188 
volume, 23,54 
s&, 
22-23,53 
Integrating factor, 345-346 
Irreducible matrix representation, see 
Representations, group 
Irrotational vector field, 40 
Jordan's inequality, 285 
Kernel, 492 
causal, 498 
separable, 504-506 
translationally invariant, 498 
Kronecker delta, 8-10,563464,574,581 
L'Hopital's rule, 170 
Laguerre polynomials, 488 
Laplace contour, 313,333 
Laplace equation, 424475 
Cartesian coordham, 424-433 
confonnal map solutions, 1921% 
cylindrical coordinates. 441,457 
spherical coordinates, 458-475 
Laplace transform, 313-335 
circuit analogy, 326-331 
of a convolution. 320-323 
of a delayed function, 319 
of a delta function, 314-316 
of a de.rivative, 319-320 
double-sided, 331-335 
of a growing sinusoid, 3 17-3 1 8 
integral equation solutions, 499-500 
inversion contour, 313,333 
orthogonality condition, 316 
of a product, 323-326 
relation to Green"s function, 377-381 
of a step function, 316-317 
transform pair, 3 13 
differentid equation SOJU~~O~S, 371-376 
Laplacian, 33,34 
Laurent series 
complex functions, 159-171 
real functions, 152 
Left-hand rule, 10,88 
Left-handed COordiDBte system, 10,8692 
hge* 
polynomials, 464-466 
Levi-Civita symbol, 11-92,639-641 
Line distributions, singular, 119-121 
Linearly dependent functions, 350,354 
Lorentz transfdon, 582585.592 
Low-pass Mter, 292 
Mapping, 138 
conformal, 18P-202 
Riemann sheets, 532-534 
Schwiuiz-CbristoEel. 1!?6-202 
array notation, 3 
Hemitian, 80,435 
multiplication, 3-4 
notation, 3 
trace, 16,610 
transpose, 5 
unimodular, 633 
unitary, 633-634 
Matrices, 3-5 
Maxwell equations, 367-368 
Metric tensol; 569470,575,579,581, 
Minkowski space, 591 
Mhor test, forpseudovectors, 653-654 
Moment of inertia tensor, 78 
Moments of a distribution, 121-125 
Monopole, 122-123 
Monopole moment, 122 
Multiplication tabie, see Groups, multiplication 
Multiple expansion, 121-125 
Neighborhood, 142 
Neumann boundary conditions, 429 
588-592 
table 

INDEX 
677 
Neumann functions 
Bessel equation solution, 444-446 
spherical, 357 
Neumann series, 501-503 
Nonhomogeneous boundary conditions, see 
Nonhomogeneous differential equation, see 
Nonisotropic materials, 68 
Normal modes of vibration, 624630 
Notation, overview, 1-5 
Nyquist sampling rate, 294 
Boundary conditions, nonhomogeneous 
Differential equations, nonhomogeneous 
Ohm’s law, 67-70 
Operators 
group representations, 606607 
Hermitian, 436-440 
linear differential, 341 
SNrm-Liouvilk, 438439 
of a hfferential equation, 341-342 
of a group, 598 
associated Legendre polynomials, 472 
Bessel functions, 448450 
exponential Fourier series, 228-229 
Fourier series, 221-223 
Fourier transform, 253-254 
Laplace transform, 3 16 
Legendre polynomials, 4655466 
Order 
Orthogonality relations 
Outer product, see dyadic product 
Parity conservation, 90 
Period, 220 
Permutation group, 607 
Permutations, even and odd, 11 
Phasors, 148 
Piecewise smooth function, 232,233 
Point mass, 102-103, 114-1 16 
Polar vector, 89 
Pole, 169 
Position vector, 44-45 
cylindrical coordinates, 46 
spherical coordinates, 49 
Positive definite function, 436 
Principal part, 184-188 
Pseudo-objects, 86-92 
pseudoscalar, 90-9 1 
pseudotensor, 91-92 
pseudovector, 11,86-90,653-654 
Quadrapole, 133 
Quadrapole moment, I22 
Quadrature, method of, 358-366 
Ratio test, 151 
Rearrangement lemma, 599 
Reducible matrix representation, see 
Representations, group 
Relativity 
general, 565-566 
special, 564-565.583-592 
Removable singularity, 169 
Representations, group, 600 
completeness, 622-623 
decomposition of, 622 
equhlent vs inequivalent, 614 
irreducible, 6 13-623 
matrix notation, 6 12-6 14 
operator, 606-607 
orthogonality, 6 17-62 1 
reducible, 613-617 
Residue theorem 171-175.510-513 
Riemann integration, 125-126,256,327 
Riemann sheets, 513-516.532-534 
Right-hand rule, 10,22,87 
Right-handed coordinate system, 10,46,48,53, 
82,86-92 
Rodrigues’s formula 
associated Legendre polynomials, 472 
Legendre polynomials, 464 
Rotation matrix, 6, 12-13 
Saddle point, 545-547.554555 
Samplmg theorem, 290-295 
Scalar fields, 18-1 9 
Scalar potential, 38.40 
Scale factors, 49-5 1 
Schwartz-Christoffel mapping, see Mapping, 
Separation of variables 
Schwartz-Christoffel 
first-order differential equations, 342-343 
Laplace equation in Cartesian coordinates, 
Laplace equation in cylindrical coordinates, 
Laplace equation in spherical coordinates, 
424-433 
441457 
458-475 
Sequence functions, 104-105,107-108 
Sheet distributions, singular, 116-119 
Shur’s lemma, 623-630 
Sifting intern, 106,112 
Similarity aansform, 610 
Sinc function, 107-108,268 
Singularity circle, 156, 163-167 
Skewed coordinates, 565-567,585-588 
Solenoidal vector field, 40 
Special relativity, 564-565.583-592 
Speed of light, invariance, 564,585 

678 
INDEX 
Spherical coordinates, 4849 
Spherical harmonics, 474475 
Step function, see Heaviside step function 
Stirling’s approximation, 554 
Stokes’s theorem, 37-38 
String problem, 385-391 
Stunn-Liouville form, 438-439 
Subgroups, 607-608 
Subscript notation, 2 
Subscriphumation notation, 3,12-15.32-34 
Subscript/supedpt notation, 573-575,581 
Summation convention, Einstein, 2 
Superposition principle, 341, 382 
Symmetry operations, 599 
Taylor series 
steepest descent, method of, 542-555 
complex functions, 139-140,153-159 
real functions, 152 
Tensors, 6746,562-592 
contravariant vs. covariant, 579-581 
coordinate transformations, 76-78,84-86, 
diagonalization, 78-84 
metric, 569-570,575-579,581,58&592 
in non-orthogonal coordinates, 562-592 
notation, 69-71 
rank, 71 
562-564.580 
Tokamak, 61 
Toroidal coordinates, 61,62 
Tour, 5 17 
Trace, see Matrices, trace 
Transfer function, 293 
Transformation matrix, 73-75,8546.570-574 
Translational invariance, 378,398,498 
Transpose, 5 
Unimodular, 633 
Unitary matrix, 633-634 
Vector fields, 18-20 
Vector potential, 40 
Vector/te.nsor notation, 70,7 I 
Vectors 
coordinate transfomations, 71-76,90,570-573 
identities, 14-15,17 
notation, 3 
rotation of, 5-7, 12-13 
Vierergruppe, see Groups, D2 
Volterra equation 
fmt kind, 492499 
second kind. 492-493 
Wave equation, 398-403 
Weighting function, 436 
Wronskian, 349-354 

This Page Intentionally Left Blank

This Page Intentionally Left Blank

This Page Intentionally Left Blank

This Page Intentionally Left Blank

This Page Intentionally Left Blank

This Page Intentionally Left Blank

