Imagen Editor and EditBench: Advancing and Evaluating
Text-Guided Image Inpainting
Su Wang∗
Chitwan Saharia∗
Ceslee Montgomery∗
Jordi Pont-Tuset
Shai Noy
Stefano Pellegrini
Yasumasa Onoe
Sarah Laszlo
David J. Fleet
Radu Soricut
Jason Baldridge
Mohammad Norouzi†
Peter Anderson†
William Chan†
Google Research
Figure 1. A sequence of edits by Imagen Editor. Given an image, a user deﬁned mask, and a text prompt, Imagen Editor makes localized
edits to the designated areas. The model meaningfully incorporates the user’s intent and performs photorealistic edits.
Abstract
Text-guided image editing can have a transformative im-
pact in supporting creative applications. A key challenge
is to generate edits that are faithful to input text prompts,
while consistent with input images. We present Imagen Ed-
itor, a cascaded diffusion model built, by ﬁne-tuning Im-
agen [36] on text-guided image inpainting.
Imagen Ed-
itor’s edits are faithful to the text prompts, which is ac-
complished by using object detectors to propose inpaint-
ing masks during training. In addition, Imagen Editor cap-
tures ﬁne details in the input image by conditioning the cas-
caded pipeline on the original high resolution image. To im-
prove qualitative and quantitative evaluation, we introduce
EditBench, a systematic benchmark for text-guided image
inpainting. EditBench evaluates inpainting edits on natu-
ral and generated images exploring objects, attributes, and
scenes. Through extensive human evaluation on EditBench,
we ﬁnd that object-masking during training leads to across-
the-board improvements in text-image alignment – such that
Imagen Editor is preferred over DALL-E 2 [31] and Stable
Diffusion [33] – and, as a cohort, these models are better
at object-rendering than text-rendering, and handle mate-
rial/color/size attributes better than count/shape attributes.
∗Equal contribution. †Equal advisory contribution.
1. Introduction
Text-to-image generation has seen a surge of recent inter-
est [31, 33, 36, 50, 51]. While these generative models are
surprisingly effective, users with speciﬁc artistic and de-
sign needs do not typically obtain the desired outcome in a
single interaction with the model. Text-guided image edit-
ing can enhance the image generation experience by sup-
porting interactive reﬁnement [13,17,34,46]. We focus on
text-guided image inpainting, where a user provides an im-
age, a masked area, and a text prompt and the model ﬁlls
the masked area, consistent with both the prompt and the
image context (Fig. 1). This complements mask-free edit-
ing [13,17,46] with the precision of localized edits [5,27].
This paper contributes to the modeling and evaluation of
text-guided image inpainting. Our modeling contribution is
Imagen Editor,2 a text-guided image editor that combines
large scale language representations with ﬁne-grained con-
trol to produce high ﬁdelity outputs. Imagen Editor is a
cascaded diffusion model that extends Imagen [36] through
ﬁnetuning for text-guided image inpainting. Imagen Edi-
tor adds image and mask context to each diffusion stage via
three convolutional downsampling image encoders, shown
in Fig. 2.
A key challenge in text-guided image inpainting is ensur-
ing that generated outputs are faithful to the text prompts.
2https://imagen.research.google/editor/
arXiv:2212.06909v2  [cs.CV]  12 Apr 2023

Figure 2. Imagenator is an image editing model built by ﬁne-
tuning Imagen. All of the diffusion models, i.e., the base model
and super-resolution (SR) models, condition on high-resolution
1024×1024 image and mask inputs. To this end, new convolu-
tional image encoders are introduced.
The standard training procedure uses randomly masked re-
gions of input images [27, 35]. We hypothesize that this
leads to weak image-text alignment since randomly cho-
sen regions can often be plausibly inpainted using only the
image context, without much attention to the prompt. We
instead propose a novel object masking technique that en-
courages the model to rely more on the text prompt during
training (Fig. 3). This helps make Imagen Editor more con-
trollable and substantially improves text-image alignment.
Observing that there are no carefully-designed standard
datasets for evaluating text-guided image inpainting, we
propose EditBench, a curated evaluation dataset that cap-
tures a wide variety of language, types of images, and lev-
els of difﬁculty. Each EditBench example consists of (i)
a masked input image, (ii) an input text prompt, and (iii)
a high-quality output image that can be used as reference
for automatic metrics. To provide insight into the relative
strengths and weaknesses of different models, edit prompts
are categorized along three axes: attributes (material, color,
shape, size, count), objects (common, rare, text rendering),
and scenes (indoor, outdoor, realistic, paintings).
Finally, we perform extensive human evaluations on Ed-
itBench, probing Imagen Editor alongside Stable Diffu-
sion (SD) [33], and DALL-E 2 (DL2) [31]. Human an-
notators are asked to judge a) text-image alignment – how
well the prompt is realized (both overall and assessing the
presence of each object/attribute individually) and b) image
quality – visual quality regardless of the text prompt. In
terms of text-image alignment, Imagen Editor trained with
object-masking is preferred in 68% of comparisons with
its counterpart conﬁguration trained with random masking
(a commonly adopted method [27, 35, 41]). Improvements
are across-the-board in all object and attribute categories.
Imagen Editor is also preferred by human annotators rel-
ative to SD and DL2 (78% and 77% respectively).
As
a cohort, models are better at object-rendering than text-
Figure 3. Random masks (left) frequently capture background or
intersect object boundaries, deﬁning regions that can be plausibly
inpainted just from image context alone. Object masks (right) are
harder to inpaint from image context alone, encouraging models
to rely more on text inputs during training. (Note: This example
image was generated by Imagen and is not in the training data.)
rendering, and handle material/color/size attributes better
than count/shape attributes. Comparing automatic evalua-
tion metrics with human judgments, we conclude that while
human evaluation remains indispensable, CLIPScore [14] is
the most useful metric for hyperparameter tuning and model
selection.
In summary, our main contributions are: (i) Imagen Ed-
itor, a new state-of-the-art diffusion model for high ﬁdelity
text-guided image editing (Sec. 3); (ii) EditBench, a man-
ually curated evaluation benchmark for text-guided image
inpainting that assesses ﬁne-grained details such as object-
attribute combinations (Sec. 4); and (iii) a comprehensive
human evaluation on EditBench, highlighting the relative
strengths and weaknesses of current models, and the use-
fulness of various automated evaluation metrics for text-
guided image editing (Sec. 5).
2. Related Work
Text-Guided Image Editing. There has been much recent
work on text-guided image inpainting [1, 3, 5, 7, 19, 27, 31,
33]. Paint By Word [3] optimizes for a balance between a)
the consistency between the input and edited images, and
b) the consistency between the text guide and the edited
image. The technique has been used effectively more re-
cently in DiffusionCLIP [18]. Blended Diffusion [1] runs
CLIP-guided diffusion on the foreground (masked region)
and the background (the context) in parallel and separately,
and then blends the result by element-wise aggregation.
CogView2 [7] proposes an auto-regressive text-guided in-
ﬁlling technique powered by cross-modal language model-
ing. DiffEdit [5] presents a “masked mask-free” formula-
tion where masking segmentation and masked diffusion are
run in parallel to apply masked inpainting. Most relevant
to our work are Stable Diffusion [33] and GLIDE/DALL-E
2 [27,31], which are also diffusion models. Key differences

in our work are the use of an object detector for masking
plus architectural changes to enable high resolution editing.
There has also been much work in mask-free text-guided
image editing [2, 13, 17, 46]. Text2Live [2] operates on an
isolated edit-layer with semantic localization, which allows
for good context preservation yet does not lend itself well
to extensive modiﬁcations. Prompt-to-Prompt [13] presents
powerful manipulation techniques on the cross-attention in
the text-conditioning module. Imagic [17] optimizes a spe-
cial embedding to capture the semantics of the input image,
and produces textually faithful edits by interpolating the op-
timized embedding with the embedding of the target text.
Evaluation of Text-Guided Image Editing. Text-guided
image inpainting has primarily been evaluated with respect
to three aspects. (1) Image quality [1, 25, 27, 47] assesses
the standalone quality of an image, usually independent of
a (single) ground truth reference.
(2) Reconstruction ﬁ-
delity [10,18,23], on the other hand, calculates a similarity
between the evaluated image and a ground truth. (3) Text-
image alignment [28, 44, 53] measures similarity between
visual outputs and textual inputs. (1) and (3) are most rele-
vant to our work because text-guided image inpainting pro-
motes diverse coverage (contingent on semantic coherence)
rather than faithfulness to one particular reference.
Automatic evaluation.
The standard automatic met-
ric for image quality is Frech´et Inception Distance (FID),
which assesses the quality of images in the latent space of
a generative model with respect to the distribution of a set
of real images. For text-image alignment, metrics based on
text-image encoders (notably CLIP [30]) have been popu-
lar, e.g. CLIPScore [14] - distance between text and image
encodings; CLIP-R-Precision [28] - the retrieval rank of the
edited/synthesized image for the ground truth text among
distractors. In this work, we further explore the connec-
tion between automatic and human evaluation, gauging the
extent to which the automatic metrics agree with human as-
sessments of model performance (for which there is cur-
rently no substitute when judging model outputs).
Human evaluation.
The most typical formulation is
asking two questions about side-by-side outputs from com-
peting models – which has the better image quality?, and
which aligns with this ${text} better?
(paraphrased
in varied ways).
EditBench extends this paradigm by
constructing a benchmark along diverse feature axes (at-
tribute/object/scene), focusing the evaluation on the masked
area rather than the full image (which delineates the evalua-
tion of image editing from generation), and asking annota-
tors to assess the presence of each object and attribute men-
tioned in the prompt individually. This distinguishes our
work from the previous efforts in text-guided image inpaint-
ing, which typically evaluate in a less systematic manner or
merely share cherry-picked examples [1,18,27,53].
3. Imagen Editor
Imagen Editor is a text-guided image inpainting model tar-
geting improved representation and reﬂection of linguistic
inputs, ﬁne-grained control and high ﬁdelity outputs. Ima-
gen Editor takes three inputs from the user, 1) the image to
be edited, 2) a binary mask to specify the edit region, and 3)
a text prompt – and all three inputs are used to guide the out-
put samples. Imagen Editor is a diffusion-based model [6]
ﬁne-tuned from Imagen [36] for editing. See Figure 2 for
an illustration.
Object Detector Masking Policy. A natural question to
ask is: what kind of masks do we use to train models for
text-guided image inpainting? The masked regions should
be well aligned to the edit text prompt. Ideally, we would
have a large expert dataset of aligned mask-prompt edits to
train on; however, such a dataset does not exist and curating
a large one would be difﬁcult. One natural, simple policy to
use is a random mask distribution, for example random box
and/or random stroke masks; this has been successfully ap-
plied to prior inpainting models [35,48,49]. However, when
random masks are used during training, they may cover a re-
gion irrelevant to the text prompt (Fig 3 left). Training on
such examples can encourage the model to ignore the text
prompt. We ﬁnd this issue to be especially prevalent when
masked regions are small or only partially cover an object,
which was similarly observed for CogView2 [7].
Unlike simple text-unconditional inpainting, we need
generated regions (from the mask) to not only be realistic,
but also to relate coherently to the input text prompt. We
propose a simple, effective solution to this problem. We hy-
pothesize that masking out identiﬁed objects entirely will
induce a greater overlap with the text prompt (Fig 3), and
consequently encourage the model to pay more attention to
the text prompt when inpainting. We use an off-the-shelf
object detector to detect and localize objects, and use these
bounding object boxes to generate masks to be used during
training. The model we use is the lightweight SSD Mo-
bilenet v2 [39] 3 which can be easily run on-the-ﬂy and thus
offers the same ﬂexibility as random masking policies. Our
experiments show that this simple modiﬁcation to the mask-
ing policy works surprisingly well, and it alleviates most of
the issues faced by models trained with a random masking
policy. See the Appendix for implementation details.
High-Resolution Editing.
In Imagen Editor, we mod-
ify Imagen to condition on both the image and the mask
by concatenating them with the diffusion latents along
the channel dimension, similar to SR3 [38], Palette [35]
and GLIDE [27].
The conditioning image and the cor-
responding mask input to Imagen Editor are always at
1024×1024 resolution. The base diffusion 64×64 model
and the 64×64→256×256 super-resolution model oper-
3https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2

Figure 4. EditBench example. The full image is used as a ref-
erence for successful inpainting. The mask covers the target ob-
ject with a free-form, non-hinting shape. The three descriptions
types are: single-attribute description of the masked object (Mask-
Simple), multi-attribute description of the masked object (Mask-
Rich), or whole image (Full). Mask-Rich especially probes mod-
els’ ability to handle complex attribute binding and inclusion [12].
ate at a smaller resolution, and thus require some form of
downsampling to match the diffusion latent resolution (e.g.,
64×64 or 256×256). One method is to use a parameter-free
downsampling operation (e.g., bicubic); we instead apply
a parameterized downsampling convolution (e.g., convolu-
tion with a stride). In initial experiments we found this pa-
rameterized downsampling operation to be critical for high
ﬁdelity. Simple bicubic downsampling resulted in signif-
icant artifacts along the mask boundaries in the ﬁnal out-
put image, and switching to a parameterized downsampling
convolution resulted in much higher ﬁdelity. We also ini-
tialize the corresponding new input channel weights to zero
(like [27]); this means that at initialization the model is
identical to Imagen, as it ignores the conditioning image
and mask.
Classiﬁer-Free Guidance.
Classiﬁer-Free Guidance
(CFG) [16] is a technique to bias samples to a particu-
lar conditioning (e.g., text prompt), at the cost of mode
coverage. CFG has been found to be highly effective in
boosting text-image alignment as well as image ﬁdelity in
text→image models [9, 27, 36, 50]. We found CFG contin-
ues to be critical for ensuring strong alignment between the
generated image and the input text prompt for text-guided
image inpainting. We follow [15] and use high guidance
weights with guidance oscillation. In the base model, where
ensuring strong alignment with text is most critical, we use
a guidance weight schedule which oscillates between 1 and
30. We observe that high guidance weights combined with
oscillating guidance [15] result in the best trade-off between
sample ﬁdelity and text-image alignment.
4. EditBench
Overview. EditBench is a new benchmark for text-guided
image inpainting based on 240 images.
Each image is
paired with a mask that speciﬁes the image region to be
modiﬁed via inpainting.
For each image-mask pair, we
provide three different text prompts, representing different
approaches to specifying the edit (see Fig. 4). Similar to
the DrawBench [36] and PartiPrompts [50] benchmarks for
text-to-image generation, EditBench is hand-curated to cap-
ture a wide variety of categories and aspects of difﬁculty.
Image Collection.
EditBench includes both natural im-
ages drawn from existing computer vision datasets (Visual
Genome [20] and Open Images [21]), and synthetic im-
ages generated by text-to-image models (Imagen [36] and
Parti [50]) at 50:50 ratio. To construct EditBench, we ﬁrst
generate a wide variety of initial prompts to guide the image
collection process. Initial prompts are generated by enumer-
ating attribute-object-scene combinations from these cate-
gories:
• Attributes: {material, color, shape, size, count};
• Objects: {common, rare, text-rendering};
• Scenes: {indoor, outdoor, realistic, painting}.
The choice of object, attribute and scene categories was
inspired by studying image editing requests on Reddit.4
Natural images are selected by manually searching for im-
ages matching object, attribute, scene combinations, e.g.,
for ‘a=material|o=common|s=outdoor’, images of an out-
door patio made of wood could be selected, instantiating
‘a=wooden|o=patio|s=outdoor’. Synthetic images are cre-
ated by sampling an object and attribute within each cate-
gory (e.g., instantiating ‘a=material|o=common|s=outdoor’
as ‘a=metal|o=cat|s=outdoor’), writing a matching prompt
(e.g., a metal cat standing in the middle of a farm ﬁeld.),
sampling batches of images from text-to-image models as
candidates, and then manually identifying the generated im-
age that best matches the prompt. As shown in Figure 4,
synthetic images can capture object-attribute-scene combi-
nations that are unlikely to occur naturally, and editing these
images is an important use case as part of the workﬂow of
image creation combining descriptions and gesture.
Image Masks. For each image, we manually-annotate a
free-form mask that completely covers the target object.
We are careful not to too-closely segment the target ob-
ject, which could leak information about the object under-
neath via its shape. We also include masks with a range of
sizes (Fig. 5) to check the undesirable sensitivity to mask
sizes [32]. We check models’ robustness against the ten-
dency of painting-over small masks due to overwhelming
inﬂuence from the context; we also evaluate large-area in-
painting and uncropping where the challenge is to not com-
pletely disregard the relatively small context.
Creating Text Prompts. Prior work (e.g., GLIDE [27])
often demonstrate text-guided image inpainting using
prompts that describe the full image.
However, writing
full-image descriptions is unnatural for the use case of in-
4https://www.reddit.com/r/PhotoshopRequest/

Figure 5. EditBench encompasses a wide variety of mask sizes,
including large masks that contact the edges of the images (which
can amount to an uncropping task in some cases).
painting particular components of an image that depicts a
complex scene with multiple objects and characters. Con-
sequently, for each image-mask pair, we create three text
prompts to probe model behavior from different angles.
One type of prompt gives only a basic description (Mask-
Simple) for the mask, another gives much more details
(Mask-Rich), and the last describes the full image (Full,
disregarding the mask). The unmasked input image itself
serves as a reference image for inpainting in accordance
with the prompts. See Fig. 4 for a summary.
5. Evaluation
We conduct comprehensive human evaluations of both text-
image alignment and image quality on EditBench. We also
analyze human preferences relative to automatic metrics.
We evaluate four models:
• Imagen Editor (IM): Our full model described in Sec.
3;
• Imagen EditorRM (IMRM): Imagen Editor ﬁnetuned
with Random Masking instead of object masking;
• Stable Diffusion (SD): Version 1.5 of the model based
on Rombach et al. [33];5
• DALL-E 2 (DL2): A commercial web UI based on
Ramesh et al. [31], accessed in October 2022.6
We compare Imagen Editor with Imagen EditorRM to quan-
tify the beneﬁts of object masking during training.
We
include evaluations of Stable Diffusion and DALL-E 2 to
place our work in context with prior work and to more
broadly analyze the limitations of the current state of the
art.
5http://huggingface.co/runwayml/stable-diffusion-v1-5
6https://openai.com/dall-e-2/
Figure 6. Human evaluation for single model text-image align-
ment.
Full elicits annotators’ overall impression of text-image
alignment; Mask-Simple and Mask-Rich check for the correct in-
clusion of particular attributes and objects, and attribute binding.
5.1. Human Evaluation Protocol
We perform two types of human evaluations: single image
evaluations and forced choice side-by-side image evalua-
tions. The former allow us to ask ﬁne-grained questions
to establish if each individual object and attribute in the
prompt has been correctly rendered. Side-by-side evalua-
tions focus on comparisons between Imagen Editor and the
other models, capturing relative model performance. We
evaluate text-image alignment in both settings, and overall
image quality only in side-by-side evaluations. In all evalu-
ations we use a red box to highlight the image region edited
by the model and ask the annotator to pay special attention
to it (Fig. 6). Each model is evaluated based on four sam-
pled image edits for each prompt.
Single Image Evaluations. Our single image evaluations
are adapted to the given detail level for each type of prompt.
For the Full prompts (describing the full image), annotators
assess general text-image alignment by giving a binary an-
swer to the question Does the image match the caption?.
For Mask-Simple prompts describing the masked region
with one object and one attribute (e.g. a metal cat), eval-
uations are more ﬁne-grained (Fig. 6). Annotators answer
three binary questions, evaluating: (1) whether the object
(cat) is rendered, (2) whether the given attribute (metal) is
present in the image, and (3) whether the attribute (metal) is
depicted applied to the correct object (cat) [12,50]. Finally,
for Mask-Rich prompts, we extend the previous evaluation
to multiple attribute-object pairs. The annotator answers
three sets of three binary questions – about the attribute, the

Figure 7.
Single-image human evaluations of text-guided im-
age inpainting on EditBench by prompt type. In this ﬁgure, for
Mask-Simple and Mask-Rich prompts, text-image alignment is
only counted as correct if the edited image correctly includes every
attribute and object speciﬁed in the prompt, including the correct
attribute binding (setting a very high bar for correctness). Note that
due to different evaluation designs, Full vs Mask-only prompts re-
sults are less directly comparable.
object, and the attribute binding – making 9 binary judg-
ments in total. Compared to previous evaluations in im-
age generation that only assess general text-image align-
ment [1,27,53], our ﬁne-grained evaluations provide greater
insight into language ﬁdelity and also which categories of
objects and attributes present the most difﬁculties. Anno-
tators in total perform 11.5K single model evaluation tasks
(240 images × 3 prompts × 4 models × 4 samples).
Side-by-Side Evaluations.
Finally,
for Mask-Rich
prompts, we extend the previous evaluation to multiple
attribute-object pairs. The annotator answers three sets of
three binary questions – about the attribute, the object, and
the attribute binding – making 9 binary judgments in to-
tal. Compared to previous evaluations in image generation
that only assess general text-image alignment [1, 27, 53],
our ﬁne-grained evaluations provide greater insight into lan-
guage ﬁdelity and also which categories of objects and at-
tributes present the most difﬁculties. 18 (US-based) anno-
tators in total perform 11.5K single model evaluation tasks
(240 images × 3 prompts × 4 models × 4 samples). We
describe the human evaluation process in more detail in the
Appendix.
5.2. Human Evaluation Results
Overall. Fig. 7 presents the aggregated human ratings,
sliced by prompt types. % Correct Image-Text Alignment
is the proportion of positive judgments a model receives.
Each question is binary – in the case of Full, responses re-
ﬂect the overall impression, whereas for Mask-Simple and
Mask-Rich a positive response indicates the edited image at-
tributes are correctly bound to the correct objects. Across-
the-board, Imagen Editor receives the highest ratings (10-
13% higher than the 2nd highest). For the rest, the per-
Figure 8. Side-by-side human evaluation of image realism & text-
image alignment on EditBench Mask-Rich prompts.
For text-
image alignment, Imagen Editor is preferred in all comparisons.
Figure 9. Single-image human evaluations on EditBench Mask-
Simple by object type. As a cohort, models are better at object-
rendering than text-rendering.
Figure 10. Single-image human evaluations on EditBench Mask-
Simple by attribute type. Object masking improves adherence to
prompt attributes across-the-board (IM vs. IMRM).
formance order is IMRM > DL2 > SD (with 3-6% differ-
ence) except for with Mask-Simple, where IMRM falls 4-8%
behind. As relatively more semantic content is involved
in Full and Mask-Rich, we conjecture IMRM and IM are
beneﬁted by the higher performing T5 XXL text encoder
(see [13], D1).
An interesting observation is annotators rate models

Figure 11. Example model outputs for Mask-Simple vs. Mask-
Rich prompts. Object masking improves Imagen Editor’s ﬁne-
grained adherence to the prompt compared to the same model
trained with random masking. See Appendix for more examples.
higher with Full than Mask-Simple prompts, even though
the former involves more semantic content. There are two
likely reasons for this: a) models are trained by conditioning
on Full prompts rather than mask-only [31,33,37]; b) since
we do not change the context (unmasked), Full gains an
advantage of having correct associations for this portion be-
cause the unmasked (and correct) pixels remain unchanged.
Finally, note that with Mask-Rich, while IM retains
10+% lead over the rest (see Fig. 7 and examples in Fig.
11), the overall performance drops substantially – leaving
considerable room for future improvement.
Side-by-Side. In Fig. 8, compared with other models 1v1,
IM leads in text alignment with a substantial margin, being
preferred by annotators 78%, 77%, and 68% of the time
compared to SD, DL2, and IMRM respectively. These gains
were realized while achieving similar levels of performance
(0-6% delta) in image quality.
Breakdown by objects.
In Fig.
9, IM leads in all ob-
ject types: 10%, 11%, and 11% higher than the 2nd high-
est in common, rare, and text-rendering. For the rest, the
notable observation is SD’s performance plummets in text-
rendering (59% & 44% for common and rare, and only 26%
for text-rendering).
Breakdown by attributes. In Fig. 10, IM is rated much
higher (13-16%) than the 2nd highest, except for in count,
where DL2 is merely 1% behind. IM also improves the least
over IMRM in this attribute type (14%, with the 2nd lowest
22%), making count a particularly interesting category for
future study. In general models get rated lower in count and
shape, and the two happen to be the more abstract. The
result is intuitive yet runs counter to ﬁndings in the recog-
nition of abstract attributes [26]. The relatively similar per-
formance in size vs. material/count is slightly unexpected –
it is a relational category where the understanding requires
appropriate contextualization, while for the latter the object
itself is the only source of information wrt. other objects
or the general surrounding. One possibility is the simplic-
ity of our design: we include straightforward comparison
of different sizes but do not build adversarial cases where
seemingly small/large objects are in actuality the opposite.
5.3. Automatic Evaluation Metrics
Although human evaluations are widely adopted as the
gold standard for image realism and text-image alignment
evaluations, automatic evaluation metrics are valuable for
iterative hyperparameter tuning and model selection. We
compare human judgments with automatic metrics to iden-
tify the best metrics for model development.
Metrics.
We investigate text-image alignment metrics
based on CLIPScore [14] and CLIP-R-Prec(ision) [28].
CLIPScore
calculates
text-to-image
(T2I)
or
image-
to-image (I2I) similarity in the latent space of the
contrastively-trained CLIP model [29]. CLIP-R-Prec is a
ranking based approach (typically formulated as text R-
Precision [28]) that measures how well the generated image
retrieves the text prompt with CLIP from among a set of text
distractors. As text distractors we use all the other prompts
in EditBench with the same prompt type.
Comparison to human judgments. To evaluate agreement
between automatic metrics and human scores, a common
practice is to report correlation coefﬁcients [40]. However,
metrics such as Spearman’s ρ consider the ranking induced
by each score over all pairs of observations, which includes
ranking images with different prompts. Rather than com-
paring images with different prompts, which is a difﬁcult
judgment even for people, we focus on two questions: (1)
For a given prompt, can automatic metrics pick the image
preferred by people? and (2) Can automatic metrics identify
the model with the highest human evaluations?
In Tab. 1 we report the agreement between various met-
rics based on CLIPScore and human judgments when pick-
ing the best image from two model-generated images with
the same text prompt. We sample 10K image pairs and the
best image in each pair is determined by human single-
image evaluation scores (image pairs with the same hu-
man score are excluded). Metrics are calculated using both

Prompt
Image T2I
I2I
T2I+I2I R-Prec Rand
Full
Full
70.1 58.6
66.8
53.3
50.0
Full
Crop
68.1 55.8
62.4
57.7
50.0
Mask-Simple
Full
73.8 53.1
63.2
72.0
50.0
Mask-Simple
Crop
76.0 55.3
66.4
71.0
50.0
Mask-Rich
Full
66.7 55.2
63.4
62.3
50.0
Mask-Rich
Crop
68.4 56.4
64.1
63.3
50.0
Table 1. Percentage agreement between CLIPScore metrics and
human judgments when picking the best image out of two model-
generated images for the same text prompt. Text-to-image (T2I)
CLIPScore similarity outperforms CLIP-R-precision (R-Prec) and
image-to-image (I2I) similarity using a reference image.
Prompt
Image
T2I
I2I
T2I+I2I R-Prec Rand
Full
Full
38.5 30.8
35.7
28.7
25.0
Full
Crop
36.7 28.2
32.4
27.9
25.0
Mask-Simple
Full
45.7 28.2
37.1
40.2
25.0
Mask-Simple
Crop
47.1 29.4
38.5
39.7
25.0
Mask-Rich
Full
45.8 31.4
40.5
39.1
25.0
Mask-Rich
Crop
48.1 30.9
40.9
39.3
25.0
Table 2. Agreement between CLIPScore metrics and human judg-
ments when repeatedly picking the best model out of four hy-
brid models. Text-to-image (T2I) similarity outperforms CLIP-R-
precision (R-Prec) and reference image-to-image (I2I) similarity.
the full image (Full) and a cropped bounding box around
the masked region (Crop). We ﬁnd that CLIPScore based
on text-to-image (T2I) similarity has the highest agreement
with human judgments, identifying the best image in 68-
76% of pairs, depending on the prompt type. Unsurpris-
ingly, CLIPScore has higher agreement with humans on
simpler prompts (Mask-Simple) compared to more complex
prompts (Mask-Rich).
In Tab. 2 we report agreement with human judgments
when picking the best model based on evaluations aggre-
gated across EditBench. Each evaluation is a choice be-
tween 4 hybrid models created by randomly selecting one
sampled image from one of the available models for each
prompt [11, 52]. Scores for each hybrid model are created
by averaging the scores for the corresponding images in the
sampled data, and 100K evaluations are performed. Sim-
ilarly to Tab. 1, we ﬁnd that CLIPScore (T2I) is most re-
liable metric (identifying the best hybrid model out of 4
in 39-48% of instances). In both experiments CLIPScore
works best when the image region matches the prompt, i.e.,
full image (Full) when the prompt describes the full image,
and cropped bounding box around the masked region (Crop)
when the text prompt describes only the masked region. In
both Tab. 1 and Tab. 2 the 95% conﬁdence intervals calcu-
SD
DL2
IMRM
IM
Ref.
CLIPScore (↑)
T2I
29.7
29.1
29.6
31.5
31.0
I2I
74.9
76.1
75.8
76.6
-
T2I+I2I
52.3
52.6
53.1
53.6
-
CLIP-R-Prec (↑)
96.5
95.3
95.0
98.6
99.3
NIMA (↑)
4.44
4.33
4.56
4.63
4.89
Table 3. Aggregated automated metric scores. boldface: highest
scoring model; box : reference images rated highest.
lated with bootstrap resampling are below 1%.
Overall results.
In Tab. 3 we report automatic metrics
aggregated over all prompts for each model, and for the Ed-
itBench reference images. For CLIPScore metrics, the im-
age representation (Full or Crop) is aligned with the prompt
(Full or Mask). We also report NIMA [42] – a model-based
perceptual image quality metric. We ﬁnd that the reference
images receive the highest CLIP-R-Precision, and Imagen
Editor is ranked highest among the 4 models on each met-
ric.
6. Societal Impact
The image editing models presented are part of the growing
family of generative models which unlock new capabilities
in content creation, however, they also have the potential to
create content that is harmful to individuals or to society.
In language modeling, it is now well recognized [43, 45]
that text generation models are prone to recapitulating and
amplifying social biases that may be present in their training
sets. The risk of amplifying social harm also pertains to
text-to-image generation and text-guided image inpainting;
as discussed elsewhere, the data used to train these models
is equally fraught [4,36,50].
A particular risk that is exposed by text-guided image in-
painting, but is not present in text-to-image models is that
inpainting might enable the scaled and simple creation of
convincing misinformation– for example, editing an image
of a political ﬁgure to include a controlled substance. Two
approaches to mitigating this risk that we have taken in our
experimentation thus far are to (1) ensuring that distinctive
watermarks are present on each generated image, and (2) re-
fraining from photorealistic generation of human faces. To
extend the protections against misinformation, robust meth-
ods for proving image provenance such as steganographic
watermarking are helpful [24]. In addition, de-duplication
of text-image training datasets can reduce the likelihood
that a model reproduces a training set image [22].7 That
7https : / / openai . com / blog / dall - e - 2 - pre -
training-mitigations/

said, robust guardrails are needed to prevent recognizable
likenesses of people from being generated and exposed to
users.
7. Conclusion
We presented Imagen Editor and EditBench, making sig-
niﬁcant advancements in text-guided image inpainting and
the evaluation thereof. Imagen Editor is a text-guided image
inpainting ﬁnetuned from Imagen. Key to Imagen Editor
is adding new convolution layers to enable high-resolution
editing, and the use of a object masking policy for train-
ing. EditBench is a comprehensive systematic benchmark
for text-guided image inpainting. EditBench systematically
evaluates text-guided image inpainting across multiple di-
mensions: attributes, objects, and scenes. We ﬁnd Imagen
Editor to outperform DALL-E 2 and Stable Diffusion on
EditBench in both human evaluation and automatic metrics.
Acknowledgments
We would like to thank Gunjan Baid, Nicole Brichtova, Sara
Mahdavi, Kathy Meier-Hellstern, Zarana Parekh, Anusha
Ramesh, Tris Warkentin, Austin Waters, Vijay Vasudevan
for their generous help through the course of the project. We
give thanks to Igor Karpov, Isabel Kraus-Liang, Raghava
Ram Pamidigantam, Mahesh Maddinala, and all the anony-
mous human annotators for assisting us to coordinate and
complete the human evaluation tasks. We are grateful to
Huiwen Chang, Austin Tarango, Douglas Eck for reviewing
the paper and providing feedback. Thanks to Erica Moreira
and Victor Gomes for help with resource coordination. Fi-
nally, we would like to give our thanks and appreciation to
the authors of DALL-E 2 [31] for their permission for us to
use the outputs from their model for research purposes.
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. Proceed-
ings of CVPR, abs/2111.14818, 2022. 2, 3, 6, 12
[2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-
ten, and Tali Dekel. Text2live: Text-driven layered image
and video editing, 2022. 3
[3] David Bau, Alex Andonian, Audrey Cui, YeonHwan Park,
Ali Jahanian, Aude Oliva, and Antonio Torralba. Paint by
word. CoRR, abs/2103.10951, 2021. 2
[4] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahem-
bwe. Multimodal datasets: misogyny, pornography, and ma-
lignant stereotypes. In arXiv:2110.01963, 2021. 8
[5] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and
Matthieu Cord. Diffedit: Diffusion-based semantic image
editing with mask guidance, 2022. 1, 2
[6] Valentin De Bortoli, James Thornton, Jeremy Heng, and Ar-
naud Doucet. Diffusion schr¨odinger bridge with applications
to score-based generative modeling. Advances in Neural In-
formation Processing Systems, 34, 2021. 3
[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
Cogview2: Faster and better text-to-image generation via hi-
erarchical transformers. arXiv preprint arXiv:2204.14217,
2022. 2, 3
[8] Sara Dolnicar, Bettina Gr¨un, and Friedrich Leisch. Quick,
simple and reliable: forced binary survey questions. Inter-
national Journal of Market Research, 2011. 12
[9] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman.
Make-a-scene: Scene-
based text-to-image generation with human priors.
arXiv
preprint arXiv:2203.13131, 2022. 4
[10] Ying Gao and Qing Zhu. Text-guided image inpainting. In
Proceedings of IEEE, 2022. 3
[11] Yvette Graham and Qun Liu. Achieving accurate conclu-
sions in evaluation of automatic machine translation metrics.
In Proceedings of the 2016 Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, 2016. 8
[12] Klaus Greff, Sjoerd van Steenkiste, and J¨urgen Schmidhuber.
On the binding problem in artiﬁcial neural networks. CoRR,
abs/2012.05208, 2020. 4, 5
[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Im-
age Editing with Cross Attention Control. In arXiv preprint
arXiv:2208.01626, 2022. 1, 3, 6
[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. Clipscore: A reference-free evaluation met-
ric for image captioning. arXiv preprint arXiv:2104.08718,
2021. 2, 3, 7
[15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High deﬁnition video generation with diffusion mod-
els. arXiv preprint arXiv:2210.02303, 2022. 4
[16] Jonathan Ho and Tim Salimans.
Classiﬁer-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications, 2021. 4
[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models, 2022.
1, 3
[18] Gwanghyun Kim and Jong Chul Ye. Diffusionclip: Text-
guided image manipulation using diffusion models. arXiv
preprint arXiv:2110.02711, 2021. 2, 3, 12
[19] Yoon Kim and Alexander M. Rush. Sequence-Level Knowl-
edge Distillation. In EMNLP, 2016. 2
[20] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, Michael Bernstein, and
Li Fei-Fei. Visual Genome: Connecting Language and Vi-
sion Using Crowdsourced Dense Image Annotations. Inter-
national Journal of Computer Vision, 2017. 4
[21] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig,

and Vittorio Ferrari. The open images dataset v4: Uniﬁed
image classiﬁcation, object detection, and visual relationship
detection at scale. IJCV, 2020. 4
[22] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan
Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas
Carlini. Deduplicating training data makes language mod-
els better. CoRR, abs/2107.06499, 2021. 8
[23] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip
H. S. Torr. Manigan: Text-guided image manipulation, 2020.
3
[24] Xiyang Luo, Michael Goebel, Elnaz Barshan, and Feng
Yang. Leca: A learned approach for efﬁcient cover-agnostic
watermarking, 2022. 8
[25] Naila Murray, Luca Marchesotti, and Florent Perronnin.
Ava: A large-scale database for aesthetic visual analysis. In
Proceedings of CVPR, 2012. 3
[26] Zhixiong Nan, Yang Liu, Nanning Zheng, and Song-Chun
Zhu. Recognizing unseen attribute-object pair with genera-
tive model. Proceedings of AAAI, 2019. 7
[27] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Bob McGrew Pamela Mishkin, Ilya Sutskever, and
Mark Chen. GLIDE: Towards Photorealistic Image Gener-
ation and Editing with Text-Guided Diffusion Models. In
arXiv:2112.10741, 2021. 1, 2, 3, 4, 6, 12
[28] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,
and Anna Rohrbach. Benchmark for compositional text-to-
image synthesis. In Thirty-ﬁfth Conference on Neural Infor-
mation Processing Systems Datasets and Benchmarks Track
(Round 1), 2021. 3, 7
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021. 7
[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever.
Learning transferable vi-
sual models from natural language supervision.
CoRR,
abs/2103.00020, 2021. 3
[31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical Text-Conditional Image Gen-
eration with CLIP Latents. In arXiv:2204.06125, 2022. 1, 2,
5, 7, 9
[32] Walber Rodrigues, Felipe Walmsley, George Cavalcanti,
Jonysberg Quintino, and Helder Pinho. Grave artifacts in im-
age inpainting: Investigating the causes and untangling the
factors, 2021. 4
[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer.
High-Resolution Image
Synthesis with Latent Diffusion Models. In CVPR, 2022. 1,
2, 5, 7
[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. 2022. 1
[35] Chitwan Saharia, William Chan, Huiwen Chang, Chris A.
Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mo-
hammad Norouzi. Palette: Image-to-Image Diffusion Mod-
els. In arXiv:2111.05826, 2021. 2, 3, 13
[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi.
Photorealistic Text-to-
Image Diffusion Models with Deep Language Understand-
ing. In NeurIPS, 2022. 1, 3, 4, 8
[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding, 2022. 7
[38] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image Super-
Resolution via Iterative Reﬁnement. IEEE PAMI, 2022. 3
[39] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey
Zhmoginov, and Liang-Chieh Chen. Inverted residuals and
linear bottlenecks: Mobile networks for classiﬁcation, detec-
tion and segmentation. CoRR, abs/1801.04381, 2018. 3
[40] Lucia Specia, Fr´ed´eric Blain, Marina Fomicheva, Chrysoula
Zerva, Zhenhao Li, Vishrav Chaudhary, and Andr´e F. T. Mar-
tins. Findings of the WMT 2021 shared task on quality esti-
mation. In Proceedings of the Sixth Conference on Machine
Translation, 2021. 7
[41] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky.
Resolution-robust large mask inpainting with
fourier convolutions.
arXiv preprint arXiv:2109.07161,
2021. 2
[42] Hossein Talebi, Ehsan Amid, Peyman Milanfar, and Man-
fred K. Warmuth. Rank-smoothed pairwise learning in per-
ceptual quality assessment. In Proceedings of IEEE, 2021.
8
[43] Yi Chern Tan and L. Elisa Celis. Assessing Social and Inter-
sectional Biases in Contextualized Word Representations. In
NeurIPS, 2019. 8
[44] Ming Tao, Bing-Kun Bao, Hao Tang, Fei Wu, Longhui Wei,
and Qi Tian. De-net: Dynamic text-guided image editing
adversarial networks, 2022. 3
[45] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin,
Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae
Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Mene-
gali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,
James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen,
Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching
Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kath-
leen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee
Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker,
Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz,
Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin

Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar,
Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe
Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil,
Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi,
and Quoc Le. Lamda: Language models for dialog applica-
tions. CoRR, abs/2201.08239, 2022. 8
[46] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv
Leviathan. Unitune: Text-driven image editing by ﬁne tun-
ing an image generation model on a single image, 2022. 1,
3
[47] Jianan Wang, Guansong Lu, Hang Xu, Zhenguo Li, Chun-
jing Xu, and Yanwei Fu. Manitrans: Entity-level text-guided
image manipulation via token-wise semantic alignment and
generation, 2022. 3
[48] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with con-
textual attention. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 5505–5514,
2018. 3
[49] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Free-form image inpainting with gated
convolution. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 4471–4480, 2019. 3,
13
[50] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,
Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and
Yonghui Wu. Scaling Autoregressive Models for Content-
Rich Text-to-Image Generation. In arXiv:2206.10789, 2022.
1, 4, 5, 8
[51] Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Bo-
qiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu,
and Haifeng Wang.
Ernie-vilg: Uniﬁed generative pre-
training for bidirectional vision-language generation. CoRR,
abs/2112.15283, 2021. 1
[52] Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.
Weinberger, and Yoav Artzi. Bertscore: Evaluating text gen-
eration with bert. In ICLR, 2020. 8
[53] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Chris Tensmeyer,
Tong Yu, Changyou Chen, Jinhui Xu, and Tong Sun. Inter-
active image generation with natural-language feedback. In
Proceedings of AAAI, 2022. 3, 6, 12

A. Appendix
A.1. Human Evaluation Results
In this section we include further analysis and additional
details of the human evaluation results reported in the main
paper. In Fig. 12 we provide a breakdown of single-image
human evaluations for the EditBench Mask-Rich prompts,
illustrating the average proportion of objects and attributes
correctly rendered for each model. Focusing on a conser-
vative bar for performance, correctly rendering at least one
of the three objects-attribute pairs in a Mask Rich prompt,
Imagen Editor achieves over 85% across comparisons. This
further supports the conclusions of the main paper, object-
masking (IM vs.
IMRM) improves object rendering, at-
tribute rendering, and attribute binding (i.e., object and at-
tribute both correct, far right column).
Impact of Mask Size.
In Fig. 13 we report single-image
human evaluations by mask size (Small, Medium and
Large). In general, performance trends are consistent with
the aggregate results, however object-masking (IMRM vs.
IM) is more beneﬁcial for small and medium masks than
with large masks. For reference, Fig. 15 provides examples
of the different mask sizes in each bucket.
Single image vs. Side-by-Side.
A key consideration in
evaluating text alignment was whether to compare model
outputs side-by-side, in keeping with prior works [1,18,27,
53], or whether to evaluate each model separately – judging
a single image at a time. We report both but focus primarily
on single-image evaluations for two reasons:
• Fine-grained Evaluation. While it is reasonable to ask
the annotators simple comparative questions such as
which image matches the caption better?, the task be-
comes more cognitive taxing and prone to error when
multiple attributes and objects are compared [8].
• Avoiding Combinatorial Explosion. The single image
format facilitates pairwise comparison without elicit-
ing judgments for a (often impractically) large num-
ber of model pairs when more models are investigated.
In addition, we avoid exposing annotators to the same
outputs multiple times, which might introduce expo-
sure bias.
While the single image evaluation format may be subject to
calibration biases, i.e., not all annotators will have the same
threshold for judging correctness, we control for this by en-
suring that all model evaluations are performed in a batch
presented in random order to a large pool of annotators.
Signiﬁcance of single-model human evaluation.
We in-
clude 95% conﬁdence error bars calculated with bootstrap
Figure 12. Single-image human evaluations on EditBench Mask-
Rich illustrating the number of objects and attributes correct.
Comparing IM vs. IMRM, object-masking improves the rendering
of objects and attributes as well as attribute binding.
resampling in all single-model human evaluations (Figs. 7,
9 and 10). If error bars do not overlap, the difference in
scores is signiﬁcant. In particular, the difference between
Imagen Editor and the other models is signiﬁcant in all
cases. We conﬁrmed this, as suggested, using the one-sided,
two-sample proportions z test. Illustratively, when compar-
ing Imagen Editor’s overall image-text alignment perfor-
mance (Fig. 7) vs the next best model the p-values were
as follows for each prompt type: Full, P = 3.5 × 10−10,
Mask-Simple, P = 2.0 × 10−8, Rich, P = 5.2 × 10−7.
In Tab. 1 and Tab. 2 (correlation between automatic met-
rics and human evaluations) the 95% conﬁdence intervals
calculated with bootstrap resampling are <1%.
Sampling Strategy and Number of Evaluations.
In sin-
gle image evaluations we evaluated 4 edited image samples
for each prompt from each of the four models. In total this
gave: prompts (240) × prompt types (3) × image samples
(4) × models (4) = 11,520 outputs rated by annotators. In
the side-by-side evaluation of Mask-Rich prompts, we eval-
uated 3 model pairs (Imagen Editor vs. Stable Diffusion,
DALL-E 2 and Imagen EditorRM), resulting in: 3 × 240
(images) × 1 (prompt types) × 3 (votes from different an-
notators) = 2,160 ratings. In side-by-side evaluations, an
image was selected at random from the 4 samples from each
model.
Annotators.
We use a total of 18 US-based annotators
and the evaluation load was spread approximately equally.
Each annotator spent roughly ∼30s per prompt.
Crowdsourcing UI.
We illustrate the actual interface
used in our human evaluations in Figs. 16, 17.

Figure 13. Single-image human evaluations on EditBench by mask
size (columns) and prompt type (rows). Imagen Editor is preferred
in all comparisons and object-masking during training is particu-
larly beneﬁcial for small masks (IM vs. IMRM).
Figure 14. Bounding box based mask generation for Imagen Edi-
tor. We adapt the random mask policy used in [35,49].
A.2. Imagen Editor object masking
We apply a bounding box based masking strategy for Im-
agen Editor, which is an adaptation of random mask pol-
icy used in previous work. During training the mask is the
union of a random mask and an object detection bounding
box as described in Fig. 14.
A.3. Examples and Failure Cases
In Fig. 18 we provide further examples comparing out-
puts from Imagen Editor when trained with object-masking
vs. random masking. We ﬁnd that object masking makes
the model noticeably more robust when handling richer
prompts with more details of objects and their attributes. To
illustrate the variety of samples evaluated from each model,
in Figs. 19–22 we illustrate sampled outputs from Stable
Diffusion, DALL-E 2, Imagen EditorRM and Imagen Editor
respectively.
Imagen Editor Failure Cases.
In Fig. 23, we further ex-
plore Imagen Editor failure cases. We focus on attribute
types as Fig. 12 shows that, even in the case of more com-
plex, Mask Rich prompts, models are relatively strong at
getting the majority of objects mentioned correct. As is
consistent with our breakdown of Mask-Simple prompts by
Attribute type, a qualitative review of Imagen Editor failure
cases on Mask-Simple prompts supports that Imagen Edi-
tor is fairly strong on color and material. Where there were
failure cases the objects or the colors tended to be uncom-
mon (i.e. “butter-colored” letters or “silver” llama in the
ﬁgure). Size and shape are admittedly often more challeng-
ing because they can be more ambiguous. Yet still there
are a handful of cases where the size attribute appears to
be ignored (i.e. “tiny octopus” in ﬁgure). In almost all
cases, some object and often it’s more common attributes
are inpainted (an example of this is the “pentagon-shaped
block” instead of a cube-shaped block). Finally, count is
notoriously challenging and the most clear failure case of
the various models. Rarely do they render too many objects.
Almost always they render far too few, often over 50% of
objects are missing.

Figure 15. Examples of different mask sizes from the Small, Medium and Large buckets reported in Fig 13. Mask sizes were determined
by binning mask-to-image area ratios into 3 quantiles as follows: Small (5.7–21.5%), Medium (21.5–36.9%), and Large (>36.9%).

(a) Full prompt single image evaluation with binary selection for overall text-image alignment.
(b) Mask-Simple prompt evaluation where annotators assess existence of the correct object and attribute separately and together to measure correct attribute
binding.
Figure 16. Crowdsourcing UI illustration: Full prompts & Mask-Simple prompts.

(a) Mask-Rich prompt evaluation, which is similar to Mask-Simple (Fig. 16b) but involves 3 pairs of attributes and objects.
(b) Side-by-Side evaluation. The second (text-image alignment) question only appears after the ﬁrst (realism) question is
answered.
Figure 17. Crowdsourcing UI illustration: Mask-Rich prompts & Side-by-side evaluations.

Figure 18. Additional examples comparing the random and object-masking strategies on Mask-Simple and Mask-Rich prompts. Imagen
Editor is substantially more robust at handling richer attribute/object speciﬁcations, as conﬁrmed by human evaluations.

Figure 19. Stable Diffusion examples.

Figure 20. DALL-E 2 examples.

Figure 21. Imagen Editor (random masking) examples.

Figure 22. Imagen Editor (object masking) examples.

Figure 23. Imagen Editor failure cases by attribute. Material - the blocks don’t quite have the transparency property you’d expect of letters
encased in a glass box. Color - the llama is not quite silver colored in any case. The object “llama” is object type = “uncommon” denoting
that a “silver” llama is likely out of distribution for the model and therefore, more challenging. Size - the octopus is never quite “tiny” this
is wrong in at least two ways: not absolutely (with respect to the image size), nor relatively (compared to the car). Count - count is among
the more challenging attributes and anecdotally, models are often off by 50% or more. Shape - all image samples revert to the standard
shape for a block: a cube.

