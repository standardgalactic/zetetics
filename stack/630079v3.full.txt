A Fast and Scalable Framework for Large-scale and
1
Ultrahigh-dimensional Sparse Regression with Application to
2
the UK Biobank
3
Junyang Qian1, Yosuke Tanigawa2, Wenfei Du1, Matthew Aguirre2, Chris Chang3,
4
Robert Tibshirani1,2, Manuel A. Rivas2, and Trevor Hastie1,2
5
1Department of Statistics, Stanford University
6
2Department of Biomedical Data Science, Stanford University
7
3Grail, Inc.
8
Abstract
9
The UK Biobank (Bycroft et al., 2018) is a very large, prospective population-based cohort
10
study across the United Kingdom. It provides unprecedented opportunities for researchers to
11
investigate the relationship between genotypic information and phenotypes of interest. Multiple
12
regression methods, compared with GWAS, have already been showed to greatly improve the
13
prediction performance for a variety of phenotypes. In the high-dimensional settings, the lasso
14
(Tibshirani, 1996), since its ﬁrst proposal in statistics, has been proved to be an eﬀective method
15
for simultaneous variable selection and estimation.
However, the large scale and ultrahigh
16
dimension seen in the UK Biobank pose new challenges for applying the lasso method, as many
17
existing algorithms and their implementations are not scalable to large applications. In this
18
paper, we propose a computational framework called batch screening iterative lasso (BASIL)
19
that can take advantage of any existing lasso solver and easily build a scalable solution for very
20
large data, including those that are larger than the memory size. We introduce snpnet, an R
21
1
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

package that implements the proposed algorithm on top of glmnet (Friedman et al., 2010a)
22
and optimizes for single nucleotide polymorphism (SNP) datasets. It currently supports ℓ1-
23
penalized linear model, logistic regression, Cox model, and also extends to the elastic net
24
with ℓ1/ℓ2 penalty. We demonstrate results on the UK Biobank dataset, where we achieve
25
superior predictive performance on quantitative and qualitative traits including height, body
26
mass index, asthma and high cholesterol.
27
Author Summary
28
With the advent and evolution of large-scale and comprehensive biobanks, there come up unprece-
29
dented opportunities for researchers to further uncover the complex landscape of human genetics.
30
One major direction that attracts long-standing interest is the investigation of the relationships
31
between genotypes and phenotypes. This includes but doesn’t limit to the identiﬁcation of geno-
32
types that are signiﬁcantly associated with the phenotypes, and the prediction of phenotypic values
33
based on the genotypic information. Genome-wide association studies (GWAS) is a very powerful
34
and widely used framework for the former task, having produced a number of very impactful dis-
35
coveries. However, when it comes to the latter, its performance is fairly limited by the univariate
36
nature. To address this, multiple regression methods have been suggested to ﬁll in the gap. That
37
said, challenges emerge as the dimension and the size of datasets both become large nowadays.
38
In this paper, we present a novel computational framework that enables us to solve eﬃciently the
39
entire lasso or elastic-net solution path on large-scale and ultrahigh-dimensional data, and therefore
40
make simultaneous variable selection and prediction. Our approach can build on any existing lasso
41
solver for small or moderate-sized problems, scale it up to a big-data solution, and incorporate
42
other extensions easily. We provide a package snpnet that extends the glmnet package in R and
43
optimizes for large phenotype-genotype data. On the UK Biobank, we observe improved prediction
44
performance on height, body mass index (BMI), asthma and high cholesterol by the lasso over other
45
univariate and multiple regression methods. That said, the scope of our approach goes beyond ge-
46
netic studies. It can be applied to general sparse regression problems and build scalable solution
47
for a variety of distribution families based on existing solvers.
48
2
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

1
Introduction
49
The past two decades have witnessed rapid growth in the amount of data available to us. Many
50
areas such as genomics, neuroscience, economics and Internet services are producing big datasets
51
that have high dimension, large sample size, or both. A variety of statistical methods and computing
52
tools have been developed to accommodate this change. See, for example, Friedman et al. (2009);
53
Efron and Hastie (2016); Dean and Ghemawat (2008); Zaharia et al. (2010); Abadi et al. (2016)
54
and the references therein for more details.
55
In high-dimensional regression problems, we have a large number of predictors, and it is likely
56
that only a subset of them have a relationship with the response and will be useful for prediction.
57
Identifying such a subset is desirable for both scientiﬁc interests and the ability to predict outcomes
58
in the future. The lasso (Tibshirani, 1996) is a widely used and eﬀective method for simultaneous
59
estimation and variable selection. Given a continuous response y ∈Rn and a model matrix X ∈
60
Rn×p, it solves the following regularized regression problem.
61
ˆβ(λ) = argmin
β∈Rp
1
2n∥y −Xβ∥2
2 + λ∥β∥1,
(1)
where ∥x∥q = (Pn
i=1 |xi|q)1/q is the vector ℓq norm of x ∈Rn and λ ≥0 is the tuning parameter.
62
The ℓ1 penalty on β allows for selection as well as estimation. Normally there is an unpenalized
63
intercept in the model, but for ease of presentation we leave it out, or we may assume that both X
64
and y have been centered with mean 0. One typically solves the entire lasso solution path over a grid
65
of λ values λ1 ≥λ2 · · · ≥λL and chooses the best λ by cross-validation or by predictive performance
66
on an independent validation set. In R (R Core Team, 2017), several packages, such as glmnet
67
(Friedman et al., 2010a) and ncvreg (Breheny and Huang, 2011), provide eﬃcient procedures to
68
obtain the solution path for the Gaussian model (1), and for other generalized linear models with the
69
residual sum of squared replaced by the negative log-likelihood of the corresponding model. Among
70
them, glmnet, equipped with highly optimized Fortran subroutines, is widely considered the fastest
71
oﬀ-the-shelf lasso solver. It can, for example, ﬁt a sequence of 100 logistic regression models on a
72
sparse dataset with 54 million samples and 7 million predictors within only 2 hours (Hastie, 2015).
73
3
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

However, as the data become increasingly large, many existing methods and tools may not be
74
able to serve the need, especially if the size exceeds the memory size. Most packages, including
75
the ones mentioned above, assume that the data or at least its sparse representation can be fully
76
loaded in memory and that the remaining memory is suﬃcient to hold other intermediate results.
77
This becomes a real bottleneck for big datasets. For example, in our motivating application, the
78
UK Biobank genotypes and phenotypes dataset (Bycroft et al., 2018) contains about 500,000 indi-
79
viduals and more than 800,000 genotyped single nucleotide polymorphisms (SNPs) measurements
80
per person. This provides unprecedented opportunities to explore more comprehensive genotypic
81
relationships with phenotypes of interest. For polygenic traits such as height and body mass index
82
(BMI), speciﬁc variants discovered by genome-wide association studies (GWAS) used to explain
83
only a small proportion of the estimated heritability (Visscher et al., 2017), an upper bound of the
84
proportion of phenotypic variance explained by the genetic components. While GWAS with larger
85
sample size on the UK Biobank can be used to detect more SNPs and rare variants, their prediction
86
performance is fairly limited by univariate models. It is very interesting to see if full-scale multiple
87
regression methods such as the lasso or elastic-net can improve the prediction performance and
88
simultaneously select relevant variants for the phenotypes. That being said, the computational
89
challenges are two fold. First is the memory bound. Even though each bi-allelic SNP value can
90
be represented by only two bits and the PLINK library (Chang et al., 2015) stores such SNP
91
datasets in a binary compressed format, statistical packages such as glmnet and ncvreg require
92
that the data be loaded in memory in a normal double-precision format. Given its sample size and
93
dimension, the genotype matrix itself will take up around one terabyte of space, which may well
94
exceed the size of the memory available and is infeasible for the packages. Second is the eﬃciency
95
bound. For a larger-than-RAM dataset, it has to sit on the disk and we may only read part of it
96
into the memory. In such scenario, the overall eﬃciency of the algorithm is not only determined
97
by the number of basic arithmetic operations but also the disk I/O — data transfer between the
98
memory and the disk — an operation several magnitudes slower than in-memory operations.
99
In this paper, we propose an eﬃcient and scalable meta algorithm for the lasso called Batch
100
Screening Iterative Lasso (BASIL) that is applicable to larger-than-RAM datasets and designed
101
4
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

to tackle the memory and eﬃciency bound.
It computes the entire lasso path and
can easily
102
build on any existing package to make it a scalable solution. As the name suggests, it is done in
103
an iterative fashion on an adaptively screened subset of variables. At each iteration, we exploit an
104
eﬃcient, parallelizable screening operation to signiﬁcantly reduce the problem to one of manageable
105
size, solve the resulting smaller lasso problem, and then reconstruct and validate a full solution
106
through another eﬃcient, parallelizable step. In other words, the iterations have a screen-solve-
107
check substructure.
That being said, it is the goal and also the guarantee of the BASIL algorithm
108
that the ﬁnal solution exactly solves the full lasso problem (1) rather than any approximation, even
109
if the intermediate steps work repeatedly on subsets of variables.
110
The screen-solve-check substructure is inspired by Tibshirani et al. (2012) and especially the
111
proposed strong rules. The strong rules state: assume ˆβ(λk−1) is the lasso solution in (1) at λk−1,
112
then the jth predictor is discarded at λk if
113
|x⊤
j (y −X ˆβ(λk−1))| < λk −(λk−1 −λk).
(2)
The key idea is that the inner product above is almost “non-expansive” in λ and that the lasso
114
solution is characterized equivalently by the Karush-Kuhn-Tucker (KKT) condition (Boyd and
115
Vandenberghe, 2004). For the lasso, the KKT condition states that ˆβ ∈Rp is a solution to (1) if
116
for all 1 ≤j ≤p,
117
1
n · x⊤
j (y −X ˆβ)







= λ · sign(ˆβj), if ˆβj ̸= 0,
≤λ, if ˆβj = 0.
(3)
The KKT condition suggests that the variables discarded based on the strong rules would have
118
coeﬃcient 0 at the next λk. The checking step comes into play because this is not a guarantee. The
119
strong rules can fail, though failures occur rarely when p > n. In any case, the KKT condition will
120
be checked to see if the coeﬃcients of the left-out variables are indeed 0 at λk. If the check fails,
121
we add in the violated variables and repeat the process. Otherwise, we successfully reconstruct a
122
full solution and move to the next λ. This is the iterative algorithm proposed by these authors and
123
has been implemented eﬃcienly into the glmnet package.
124
5
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

The BASIL algorithm proceeds in a similar way but is designed to optimize for datasets that
125
are too big to ﬁt into the memory. Considering the fact that screening and KKT check need to scan
126
through the entire data and are thus costly in the disk Input/Output (I/O) operations, we attempt
127
to do batch screening and solve a series of models (at diﬀerent λ values) in each iteration, where a
128
single sweep over the full data would suﬃce. Followed by a checking step, we can obtain the lasso
129
solution for multiple λ’s in one iteration. This can eﬀectively reduce the total number of iterations
130
needed to compute the full solution path and thus reduce the expensive disk read operations that
131
often cause signiﬁcant delay in the computation. The process is illustrated in Figure 1 and will be
132
detailed in the next section.
133
2
Results
134
Overview of the BASIL algorithm
For convenience, we ﬁrst introduce some notation. Let
135
Ω= {1, 2, . . . , p} be the universe of variable indices. For 1 ≤ℓ≤L, let ˆβ(λℓ) be the lasso solution
136
at λ = λℓ, and A(λℓ) = {1 ≤j ≤p : ˆβj(λℓ) ̸= 0} be the active set. When X is a matrix, we use XS
137
to represent the submatrix including only columns indexed by S. Similarly when β is a vector, βS
138
represents the subvector including only elements indexed by S. Given any two vectors a, b ∈Rn,
139
the dot product or inner product can be written as a⊤b = ⟨a, b⟩= Pn
i=1 aibi. Throughout the
140
paper, we use predictors, features, variables and variants interchangeably. We use the strong set to
141
refer to the screened subset of variables on which the lasso ﬁt is computed at each iteration, and
142
the active set to refer to the subset of variables with nonzero lasso coeﬃcients.
143
Remember that our goal is to compute the exact lasso solution (1) for larger-than-RAM datasets
144
over a grid of regularization parameters λ1 > λ2 > · · · > λL ≥0. We describe the procedure for the
145
Gaussian family in this section and discuss extension to general problems in the next. A common
146
choice is L = 100 and λ1 = max1≤j≤p |x⊤
j r(0)|/n, the largest λ at which the estimated coeﬃcients
147
start to deviate from zero. Here r(0) = y if we do not include an intercept term and r(0) = y −¯y
148
if we do. In general, r(0) is the residual of regressing y on the unpenalized variables, if any. The
149
other λ’s can be determined, for example, by an equally spaced array on the log scale. The solution
150
6
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

0
10
20
30
40
−0.4
−0.2
0.0
0.2
0.4
Iteration 1
Lambda Index
Coefficients
0
10
20
30
40
−0.4
−0.2
0.0
0.2
0.4
Iteration 2
Lambda Index
Coefficients
0
10
20
30
40
−0.4
−0.2
0.0
0.2
0.4
Iteration 3
Lambda Index
Coefficients
0
10
20
30
40
−0.4
−0.2
0.0
0.2
0.4
Iteration 4
Lambda Index
Coefficients
completed fit
new valid fit
new invalid fit
Figure 1: The lasso coeﬃcient proﬁle that shows the progression of the BASIL algorithm. The previously
ﬁnished part of the path is colored grey, the newly completed and veriﬁed is in green, and the part that is
newly computed but failed the veriﬁcation is colored red.
path is found iteratively with a screening-solving-checking substructure similar to the one proposed
151
in Tibshirani et al. (2012). Designed for large-scale and ultrahigh-dimensional data, the BASIL
152
algorithm can be viewed as a batch version of the strong rules. At each iteration we attempt to
153
ﬁnd valid lasso solution for multiple λ values on the path and thus reduce the burden of disk reads
154
of the big dataset. Speciﬁcally, as summarized in Algorithm 1, we start with an empty strong set
155
S(0) = ∅and active set A(0) = ∅. Each of the following iterations consists of three steps: screening,
156
ﬁtting and checking.
157
7
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Algorithm 1 BASIL for the Gaussian Model
1: Initialization: active set A(0) = ∅, initial residual r(0) (with respect to the intercept or other
unpenalized variables) at λ1 = λmax, a short list of initial parameters Λ(0) = {λ1, . . . , λL(0)}.
2: for k = 0 to K do
3:
Screening: for each 1 ≤j ≤p, compute inner product with current residual c(k)
j
= ⟨xj, r(k)⟩.
Construct the strong set
S(k) = A(k) ∪E(k)
M ,
where E(k)
M is the set of M variables in Ω\ A(k) with largest |c(k)|.
4:
Fitting: for all λ ∈Λ(k), solve the lasso only on the strong set S(k), and ﬁnd the coeﬃcients
ˆβ(k)(λ) and the residuals r(k)(λ).
5:
Checking: search for the smallest λ such that the KKT conditions are satisﬁed, i.e.,
¯λ(k) = min

λ ∈Λ(k) :
max
j∈Ω\S(k)(1/n)|x⊤
j r(k)(λ)| < λ

.
For empty set, we deﬁne ¯λ(k) to be the immediate previous λ to Λ(k) but increment M by
∆M. Let the current active set A(k+1) and residuals r(k+1) deﬁned by the solution at ¯λ(k).
Deﬁne the next parameter list Λ(k+1) = {λ ∈Λ(k) : λ < ¯λ(k)}. Extend this list if it consists of
too few elements. For λ ∈Λ(k) \ Λ(k+1), we obtain exact lasso solutions for the full problem:
ˆβS(k)(λ) = ˆβ(k)(λ),
ˆβΩ\S(k)(λ) = 0.
6: end for
In the screening step, an updated strong set is found as the candidate for the subsequent ﬁtting.
158
Suppose that so far (valid) lasso solutions have been found for λ1, . . . , λℓbut not for λℓ+1. The new
159
set will be based on the lasso solution at λℓ. In particular, we will select the top M variables with
160
largest absolute inner products |⟨xj, y −X ˆβ(λℓ)|. They are the variables that are most likely to be
161
active in the lasso model for the next λ values. In addition, we include the ever-active variables at
162
λ1, . . . , λℓbecause they have been “important” variables and might continue to be important at a
163
later stage.
164
In the ﬁtting step, the lasso is ﬁt on the updated strong set for the next λ values λℓ+1, . . . , λℓ′.
165
Here ℓ′ is often smaller than L because we do not have to solve for all of the remaining λ values on
166
this strong set. The full lasso solutions at much smaller λ’s are very likely to have active variables
167
outside of the current strong set. In other words even if we were to compute solutions for those
168
very small λ values on the current strong set, they would probably fail the KKT test. These λ’s
169
8
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

are left to later iterations when the strong set is expanded.
170
In the checking step, we check if the newly obtained solutions on the strong set can be valid
171
part of the full solutions by evaluating the KKT condition. Given a solution ˆβS ∈R|S| to the
172
sub-problem at λ, if we can verify for every left-out variable j that (1/n)|⟨xj, y −XS ˆβS⟩| < λ, we
173
can then safely set their coeﬃcients to 0. The full lasso solution ˆβ(λ) ∈Rp is then assembled by
174
letting ˆβS(λ) = ˆβS and ˆβΩ\S(λ) = 0. We look for the λ value prior to the one that causes the ﬁrst
175
failure down the λ sequence and use its residual as the basis for the next screening. Nevertheless,
176
there is still chance that none of the solutions on the current strong set passes the KKT check
177
for the λ subsequence considered in this iterations. That suggests the number of previously added
178
variables in the current iteration was not suﬃcient. In this case, we are unable to move forward
179
along the λ sequence, but will fall back to the λ value where the strong set was last updated and
180
include ∆M more variables based on the sorted absolute inner product.
181
The three steps above can be applied repeatedly to roll out the complete lasso solution path
182
for the original problem. However, if our goal is choosing the best model along the path, we can
183
stop ﬁtting once an optimal model is found evidenced by the performance on a validation set. At a
184
high level, we run the iterative procedure on the training data, monitor the error on the validation
185
set, and stop when the model starts to overﬁt, or in other words, when the validation error shows
186
a clear upward trend.
187
Extension to general problems
It is straightforward to extend the algorithm from the Gaussian
188
case to more general problems. In fact, the only changes we need to make are the screening step
189
and the strong set update step. Wherever the strong rules can be applied, we have a corresponding
190
version of the iterative algorithm. In Tibshirani et al. (2012), the general problem is
191
ˆβ(λ) = argmin
β∈Rp
f(β) + λ
r
X
j=1
cj∥βj∥pj,
(4)
where f is a convex diﬀerentiable function, and for all 1 ≤j ≤r, cj ≥0, pj ≥1, and βj can be a
192
scalar or vector whose ℓpj-norm is represented by ∥βj∥pj. The general strong rule discards predictor
193
9
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

j if
194
∥∇jf(ˆβ(λk−1))∥qj < cj(2λk −λk−1),
(5)
where 1/pj + 1/qj = 1. Hence, our algorithm can adapt and screen by choosing variables with
195
large values of ∥∇jf(ˆβ(λk−1))∥qj that are not in the current active set. We expand in more detail
196
two important applications of the general rule: logistic regression and Cox’s proportional hazards
197
model in survival analysis.
198
Logistic regression
In the lasso penalized logistic regression (Friedman et al., 2010b) where the
observed outcome y ∈{0, 1}n, the convex diﬀerential function in (4) is
f(β) = −1
n
n
X
i=1
(yi log pi + (1 −yi) log(1 −pi)) .
where pi = 1/(1 + exp(−x⊤
i β)) for all 1 ≤i ≤n. The rule in (5) is reduced to
|x⊤
j (y −ˆp(λk−1))| < λk −(λk−1 −λk),
where ˆp(λk−1) is the predicted probabilities at λ = λk−1. Similar to the Gaussian case, we can still
199
ﬁt relaxed lasso and allow adjustment covariates in the model to adjust for confounding eﬀect.
200
Cox’s proportional hazards model
In the usual survival analysis framework, for each sample,
in addition to the predictors xi ∈Rp and the observed time yi, there is an associated right-censoring
indicator δi ∈{0, 1} such that δi = 0 if failure and δi = 1 if right-censored. Let t1 < t2 < ... < tm
be the increasing list of unique failure times, and j(i) denote the index of the observation failing
at time ti.
The Cox’s proportional hazards model (Cox, 1972) assumes the hazard for the ith
individual as hi(t) = h0(t) exp(x⊤
i β) where h0(t) is a shared baseline hazard at time t. We can let
f(β) be the negative log partial likelihood in (4) and screen based on its gradient at the most recent
10
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

lasso solution as suggested in (5). In particular,
f(β) = −1
m
m
X
i=1

x⊤
j(i)β −log

X
j∈Ri
exp(x⊤
j β)



,
where Ri is the set of indices j with yj ≥ti (those at risk at time ti). We can derive the associated
201
rule based on (5) and thus the survival BASIL algorithm. Further discussion and comprehensive
202
experiments are included in a follow-up paper (Li et al., 2020).
203
Extension to the elastic net
Our discussion so far focuses solely on the lasso penalty, which
204
aims to achieve a rather sparse set of linear coeﬃcients. In spite of good performance in many high-
205
dimensional settings, it has limitations. For example, when there is a group of highly correlated
206
variables, the lasso will often pick out one of them and ignore the others. This poses some hardness
207
in interpretation. Also, under high-correlation structure like that, it has been empirically observed
208
that when the predictors are highly correlated, the ridge can often outperform the lasso (Tibshirani,
209
1996).
210
The elastic net, proposed in Zou and Hastie (2005), extends the lasso and tries to ﬁnd a sweet
211
spot between the lasso and the ridge penalty. It can capture the grouping eﬀect of highly correlated
212
variables and sometimes perform better than both methods especially when the number of variables
213
is much larger than the number of samples. In particular, instead of imposing the ℓ1 penalty, the
214
elastic net solves the following regularized regression problem.
215
ˆβ(λ) = argmin
β∈Rp
f(β) + λ(α∥β∥1 + (1 −α)∥β∥2
2/2),
(6)
where the mixing parameter α ∈[0, 1] determines the proportion of lasso and ridge in the penalty
216
term.
217
It is straightforward to adapt the BASIL procedure to the elastic net. It follows from the gradient
218
motivation of the strong rules and KKT condition of convex optimization. We take the Gaussian
219
family as an example. The others are similar. In the screening step, it is easy to derive that we can
220
11
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

still rank among the currently inactive variables on their absolute inner product with the residual
221
|x⊤
j (y −X ˆβ(λk−1))| to determine the next candidate set. In the checking step, to verify that all the
222
left-out variables indeed have zero coeﬃcients, we need to make sure that (1/n)|x⊤
j (y−X ˆβ(λk−1))| ≤
223
λα holds for all such variables. It turns out that in our UK Biobank applications, the elastic-net
224
results (after selection of α and λ on the validation set) do not diﬀer signiﬁcantly from the lasso
225
results, which will be immediately seen in the next section.
226
UK Biobank analysis
We describe a real-data application on the UK Biobank that in fact
227
motivates our development of the BASIL algorithm.
228
The UK Biobank (Bycroft et al., 2018) is a very large, prospective population-based cohort
229
study with individuals collected from multiple sites across the United Kingdom. It contains exten-
230
sive genotypic and phenotypic detail such as genomewide genotyping, questionnaires and physical
231
measures for a wide range of health-related outcomes for over 500,000 participants, who were aged
232
40-69 years when recruited in 2006-2010. In this study, we are interested in the relationship between
233
an individual’s genotype and his/her phenotypic outcome. While GWAS focus on identifying SNPs
234
that may be marginally associated with the outcome using univariate tests, we would like to ﬁnd
235
relevant SNPs in a multivariate prediction model using the lasso. A recent study (Lello et al., 2018)
236
ﬁts the lasso on a subset of the variables after one-shot univariate p-value screening and suggests
237
improvement in explaining the variation in the phenotypes. However, the left-out variants with
238
relatively weak marginal association may still provide additional predictive power in a multiple
239
regression environment. The BASIL algorithm enables us to ﬁt the lasso model at full scale and
240
gives further improvement in the explained variance over the alternative models considered.
241
We focused on 337,199 White British unrelated individuals out of the full set of over 500,000 from
242
the UK Biobank dataset (Bycroft et al., 2018) that satisfy the same set of population stratiﬁcation
243
criteria as in DeBoever et al. (2018). The dataset is partitioned randomly into training, validation
244
and test subsets. Each individual has up to 805,426 measured variants, and each variant is encoded
245
by one of the four levels where 0 corresponds to homozygous major alleles, 1 to heterozygous alleles,
246
2 to homozygous minor alleles and NA to a missing genotype.
In addition, we have available
247
12
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

covariates such as age, sex, and forty pre-computed principal components of the SNP matrix.
248
To evaluate the predictive performance for quantitative response, we use a common measure
R-squared (R2). Given a linear estimator ˆβ and data (y, X), it is deﬁned as
R2 = 1 −∥y −X ˆβ∥2
2
∥y −¯y∥2
2
.
We evaluate this criteria for all the training, validation and test sets. For a dichotomous response,
249
misclassiﬁcation error could be used but it would depend on the calibration. Instead the receiver
250
operating characteristic (ROC) curve provides more information and illustrates the tradeoﬀbetween
251
true positive and false positive rates under diﬀerent thresholds. The AUC computes the area under
252
the ROC curve — a larger value indicates a generally better classiﬁer. Therefore, we will evaluate
253
AUCs on the training, validation and test sets for dichotomous responses.
254
We compare the performance of the lasso with related methods to have a sense of the contribution
255
of diﬀerent components. Starting from the baseline, we ﬁt a linear model that includes only age
256
and sex (Model 1 in the tables below), and then one that includes additionally the top 10 principal
257
components (Model 2). These are the adjustment covariates used in our main lasso ﬁtting and we
258
use these two models to highlight the contribution of the SNP information over and above that of
259
age, sex and the top 10 PCs. In addition, the strongest univariate model is also evaluated (Model
260
3). This includes the 12 adjustment covariates together with the single SNP that is most correlated
261
with the outcome after adjustment.
262
Toward multivariate models, we ﬁrst compare with a univariate method that has some multi-
263
variate ﬂavor (Models 4 and 5). We select a subset of the K most marginally signiﬁcant variants
264
(after adjusting for the covariates), and construct a new variable by linearly combining these vari-
265
ants using their univariate coeﬃcients. An OLS is then ﬁt on the new variable together with the
266
adjustment variables. It is similar to a one-step partial least squares (Wold, 1975) with p-value
267
based truncation. We take K = 10, 000 and 100, 000 in the experiments. We further compare with
268
a hierarchical sequence of multivariate models where each is ﬁt on a subset of the most signiﬁcant
269
SNPs. In particular, the ℓ-th model selects ℓ×1000 SNPs with the smallest univariate p-values, and
270
13
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

a multivariate linear or logistic regression is ﬁt on those variants jointly. The sequence of models
271
are evaluated on the validation set, and the one with the smallest validation error is chosen. We
272
call this method Sequential LR or SeqLR (Model 6) for convenience in the rest of the paper. As
273
a byproduct of the lasso, the relaxed lasso (Meinshausen, 2007) ﬁts a debiased model by reﬁtting
274
an OLS on the variables selected by the lasso. This can potentially recover some of the bias in-
275
troduced by lasso shrinkage. For the elastic-net, we ﬁt separate solution paths with varying λ’s at
276
α = 0.1, 0.5, 0.9, and evaluate their performance (R2 or AUC) on the validation set. The best pair
277
of hyperparameters (α, λ) is selected and the corresponding test performance is reported.
278
In addition, we make comparison with two other bayesian methods PRS-CS (Ge et al., 2019)
279
and SBayesR (Lloyd-Jones et al., 2019). For PRS-CS, we ﬁrst characterized the GWAS summary
280
statistics using the combined set of training and validation set (n = 269, 927) with age, sex, and
281
the top 10 PCs as covariates using PLINK v2.00a3LM (9 Apr 2020) (Chang et al., 2015). Using
282
the LD reference dataset precomputed for the European Ancestry using the 1000 genome samples
283
(https://github.com/getian107/PRScs), we applied PRS-CS with the default option. We took
284
the posterior eﬀect size estimates and computed the polygenic risk scores using PLINK2’s --score
285
subcommand (Chang et al., 2015). For SBayesR, we computed the sparse LD matrix using the com-
286
bined set of training and validation set individuals (n = 269, 927) using the -- make-sparse-ldm
287
subcommand implemented in GCTB version 2.0.1 (Zeng et al., 2018).
Using the GWAS sum-
288
mary statistics computed on the set of individuals and following the GCTB’s recommendations,
289
we applied SBayesR with the following options: gctb --sbayes R--ldm [the LD matrix] --pi
290
0.95,0.02,0.02,0.01 --gamma 0.0,0.01,0.1,1 --chain-length 10000 --burn-in 2000
291
--exclude-mhc --gwas-summary [the GWAS summary statistics]. We report the model per-
292
formance on the test set.
293
There are thousands of measured phenotypes in the dataset. For demonstration purpose, we
294
analyze four phenotypes that are known to be highly or moderately heritable and polygenic. For
295
these complex traits, univariate studies may not ﬁnd SNPs with smaller eﬀects, but the lasso model
296
may include them and predict the phenotype better. We look at two quantitative traits: standing
297
height and body mass index (BMI) (Tanigawa et al., 2019), and two qualitative traits: asthma and
298
14
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

high cholesterol (HC) (DeBoever et al., 2018).
299
We ﬁrst summarize the test performance of diﬀerent methods on the four phenotypes in Figure 2.
300
The lasso and elastic net show signiﬁcant improvement in test R2 and AUC over the other competing
301
methods. Details of the model for height are given in the next section and for the other phenotypes
302
(BMI, asthma and high cholesterol) in Appendix A. A comparison of the univariate p-values and
303
the lasso coeﬃcients for all these traits is shown in the form of Manhattan plots in the Appendix
304
B (Supplementary Figure 14, 15).
305
0.5336 0.5355
0.5444
0.5551
0.6601
0.6999
0.6727
0.6998
0.5615
0.5368
0.5367
0.5454
0.5531
0.558
0.5884
0.6126
0.5955
0.6131
0.5837
0.5491
0.0099 0.0124
0.021
0.0093
0.0395
0.1052
0.0537
0.1071
0.0315
0.0139
0.6889
0.6921
0.688
0.6547
0.7137
0.7191
0.7166
0.719
0.7027
0.6953
Asthma
High Cholesterol
Height
BMI
Covariates
Single SNP
10K−LR
100K−LR
SeqLR
Lasso
Relaxed Lasso
Elastic−Net
PRS−CS
SBayesR
Covariates
Single SNP
10K−LR
100K−LR
SeqLR
Lasso
Relaxed Lasso
Elastic−Net
PRS−CS
SBayesR
0.00
0.03
0.06
0.09
0.66
0.68
0.70
0.72
0.55
0.60
0.65
0.70
0.550
0.575
0.600
Method
Test Metric
Figure 2: Comparison of diﬀerent methods on the test set. R2 are evaluated for continuous phenotypes
height and BMI, and AUC evaluated for binary phenotypes asthma and high cholesterol.
Height is a polygenic and heritable trait that has been studied for a long time. It has been used
306
15
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

as a model for other quantitative traits, since it is easy to measure reliably. From twin and sibling
307
studies, the narrow sense heritability is estimated to be 70-80% (Silventoinen et al., 2003; Visscher
308
et al., 2006, 2010). Recent estimates controlling for shared environmental factors present in twin
309
studies calculate heritability at 0.69 (Zaitlen et al., 2013; Hemani et al., 2013). A linear based
310
model with common SNPs explains 45% of the variance (Yang et al., 2010) and a model including
311
imputed variants explains 56% of the variance, almost matching the estimated heritability (Yang
312
et al., 2015). So far, GWAS studies have discovered 697 associated variants that explain one ﬁfth
313
of the heritability (Lango Allen et al., 2010; Wood et al., 2014). Recently, a large sample study
314
was able to identify more variants with low frequencies that are associated with height (Marouli
315
et al., 2017). Using lasso with the larger UK Biobank dataset allows both a better estimate of the
316
proportion of variance that can be explained by genomic predictors and simultaneous selection of
317
SNPs that may be associated. The results are summarized in Table 1. The associated R2 curves for
318
the lasso and the relaxed lasso are shown in Figure 3. The residuals of the optimal lasso prediction
319
are plotted in Figure 4.
320
Model
Form
R2
train
R2
val
R2
test
Size
(1)
Age + Sex
0.5300
0.5260
0.5288
2
(2)
Age + Sex + 10 PCs
0.5344
0.5304
0.5336
12
(3)
Strong Single SNP
0.5364
0.5323
0.5355
13
(4)
10K Combined
0.5482
0.5408
0.5444
10,012
(5)
100K Combined
0.5833
0.5515
0.5551
100,012
(6)
Sequential LR
0.7416
0.6596
0.6601
17,012
(7)
Lasso
0.8304
0.6992
0.6999
47,673
(8)
Relaxed Lasso
0.7789
0.6718
0.6727
13,395
(9)
Elastic Net
0.8282
0.6991
0.6998
48,256
(10)
PRS-CS
0.5692
−
0.5615
148,052
(11)
SBayesR
0.5397
−
0.5368
667,045
Table 1: R2 values for height. For sequential LR, lasso and relaxed lasso, the chosen model is based on
maximum R2 on the validation set. Model (3) to (8) each includes Model (2) plus their own speciﬁcation
as stated in the Form column. The validation results for PRS-CS and SBayesR are not available because
we used a combined training and validation set for training.
A large number (47,673) of SNPs need to be selected in order to achieve the optimal R2
test =
321
0.6999 for the lasso and similarly for the elastic-net. Comparatively, the relaxed lasso sacriﬁces
322
some predictive performance by including a much smaller subset of variables (13,395). Past the
323
16
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

0
20
40
60
80
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
Lambda Index
R2
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G G G G G
G
G G G G G
G G
G G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G G G G G G G G G G G
G G G G G G
G G
G G
G
G
G
G G
G
G
G
G G G
G G
G
G
G
G
G
G
G
G
G G G
G G G
G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G
Lasso (train)
Lasso (val)
ReLasso (train)
ReLasso (val)
15
36
91
245
599
1413
3209
6888
13395
22803
34754
47673
Figure 3: R2 plot for height. The top axis shows the number of active variables in the model.
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
140
160
180
200
140
160
180
200
Predicted Height (cm)
Actual Height (cm)
0
2500
5000
7500
10000
−20
−10
0
10
20
30
Residual (cm)
Frequency
Figure 4: Left: actual height versus predicted height on 5000 random samples from the test set. The
correlation between actual height and predicted height is 0.9416. Right: histogram of the lasso residuals
for height. Standard deviation of the residual is 5.05 (cm).
17
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Method
R2
val
R2
test
h2
test
Cortest
Cortest−{age, sex}
Lasso
69.92%
69.99%
35.66%
0.8366
0.4079
Prescreened lasso
69.40%
69.56%
34.73%
0.8340
0.4025
Table 2: Comparison of prediction results on height with the model trained following the same procedure as
ours except for an additional prescreening step as done in Lello et al. (2018). In addition to R2, proportion
of residual variance explained (denoted by h2
test) and correlation between the ﬁtted values and actual values
are computed. We also compute an adjusted correlation between the residual after regressing age and sex
out from the prediction and the residual after regressing age and sex out from the true response, both on
the test set.
optimal point, the additional variance introduced by reﬁtting such large models may be larger than
324
the reduction in bias. The large models conﬁrm the extreme polygenicity of standing height.
325
In comparison to the other models, the lasso performs signiﬁcantly better in terms of R2
test
326
than all univariate methods, and outperforms multivariate methods based on univariate p-value
327
ordering. That demonstrates the value of simultaneous variable selection and estimation from a
328
multivariate perspective, and enables us to predict height to within 10 cm about 95% of the time
329
based only on SNP information (together with age and sex). We also notice that the sequential
330
linear regression approach does a good job, whose performance gets close to that of the relaxed
331
lasso. It is straightforward and easy to implement using existing softwares such as PLINK (Chang
332
et al., 2015).
333
Recently Lello et al. (2018) apply a lasso based method to predict height and other phenotypes
334
on the UK Biobank. Instead of ﬁtting on all QC-satisﬁed SNPs (as stated in Section 4), they
335
pre-screen 50K or 100K most signiﬁcant SNPs in terms of p-value and apply lasso on that set only.
336
In addition, although both datasets come from the same UK Biobank, the subset of individuals
337
they used is larger than ours. While we restrict the analysis to the unrelated individuals who have
338
self-reported white British ancestry, they look at Europeans including British, Irish and Any Other
339
White. For a fair comparison, we follow their procedure (pre-screening 100K SNPs) but run on
340
our subset of the dataset. The results are shown in Table 2. We see that the improvement of the
341
full lasso over the prescreened lasso is almost 0.5% in test R2, and 1% relative to the proportion of
342
residual variance explained after covariate adjustment.
343
Further, we compare the full lasso coeﬃcients and the univariate p-values from GWAS in Fig-
344
18
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

ure 5. The vertical grey dotted line indicates the top 100K cutoﬀin terms of p-value. We see
−0.3
−0.2
−0.1
0.0
0.1
0.2
0.3
0
100
200
300
400
500
0
1
2
3
4
5
Index
−log10(p)
Lasso Coefficient
GWAS
Lasso
Figure 5:
Comparison of the lasso coeﬃcients and univariate p-values for height.
The index on the
horizontal axis represents the SNPs sorted by their univariate p-values. The red curve associated with
the left vertical axis shows the −log10 of the univariate p-values. The blue bars associated with the right
vertical axis show the corresponding lasso coeﬃcients for each (sorted) SNP. The horizontal dotted lines in
gray identiﬁes lasso coeﬃcients of ±0.05. The vertical one represents the 100K cutoﬀused in Lello et al.
(2018).
345
although a general decreasing trend appears in the magnitude of the lasso coeﬃcients with respect
346
to increasing p-values (decreasing −log10(p)), there are a number of spikes even in the large p-value
347
region which is considered marginally insigniﬁcant. This shows that variants beyond the strongest
348
univariate ones contribute to prediction.
349
3
Discussion
350
In this paper, we propose a novel batch screening iterative lasso (BASIL) algorithm to ﬁt the full
351
lasso solution path for very large and high-dimensional datasets. It can be used, among the others,
352
for Gaussian linear model, logistic regression and Cox regression, and can be easily extended to ﬁt
353
the elastic-net with mixed ℓ1/ℓ2 penalty. It enjoys the advantages of high eﬃciency, ﬂexibility and
354
easy implementation. For SNP data as in our applications, we develop an R package snpnet that
355
19
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

incorporates SNP-speciﬁc optimizations and are able to process datasets of wide interest from the
356
UK Biobank.
357
In our algorithm, the choice of M is important for the practical performance. It trades oﬀ
358
between the number of iterations and the computation per iteration. With a small M or small
359
update of the strong set, it is very likely that we are unable to proceed fast along the λ sequence in
360
each iteration. Although the design of the BASIL algorithm guarantees that for any M, ∆M > 0,
361
we are able to obtain the full solution path after suﬃcient iterations, many iterations will be needed
362
if M is chosen too small, and the disk I/O cost will be dominant. In contrast, a large M will incur
363
more memory burden and more expensive lasso computation, but with the hope to ﬁnd more valid
364
lasso solutions in one iteration, save the number of iterations and the disk I/O. It is hard to identify
365
the optimal M a priori. It depends on the computing architecture, the size of the problem, the
366
nature of the phenotype, etc. For this reason, we tend to leave it as a subjective parameter to
367
the user’s choice. However in the meantime, we do plan to provide a more systematic option to
368
determine M, which leverages the strong rules again. Recall that in the simple setting with no
369
intercept and no covariates, the initial strong set is constructed by |x⊤
j y| ≤2λ −λmax. Since the
370
strong rules rarely make mistakes and are fairly eﬀective in discarding inactive variables, we can
371
guide the choice of batch size M by the number of λ values we want to cover in the ﬁrst iteration.
372
For example, one may want the strong set to be large enough to solve for the ﬁrst 10 λ’s in the
373
ﬁrst iteration. We can then let M = |{1 ≤j ≤p : |x⊤
j y| > 2λ10 −λmax}|. Despite being adaptive
374
to the data in some sense, this approach is by no means computationally optimal. It is more based
375
on heuristics that the iteration should make reasonable progress along the path.
376
Our numerical studies demonstrate that the iterative procedure eﬀectively reduces a big-n-big-
377
p lasso problem into one that is manageable by in-memory computation. In each iteration, we
378
are able to use parallel computing when applying screening rules to ﬁlter out a large number of
379
variables. After screening, we are left with only a small subset of data on which we are able to
380
conduct intensive computation like cyclical coordinate descent all in memory. For the subproblem,
381
we can use existing fast procedures for small or moderate-size lasso problems. Thus, our method
382
allows easy reuse of previous software with lightweight development eﬀort.
383
20
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

When a large number of variables is needed in the optimal predictive model, it may still require
384
either large memory or long computation time to solve the smaller subproblem. In that case, we
385
may consider more scalable and parallelizable methods like proximal gradient descent (Parikh and
386
Boyd, 2014) or dual averaging (Xiao, 2010; Duchi et al., 2012).
One may think why don’t we
387
directly use these methods for the original full problem? First, the ultra high dimension makes
388
the evaluation of gradients, even on mini-batch very expensive. Second, it can take a lot more
389
steps for such ﬁrst-order methods to converge to a good objective value. Moreover, the speed of
390
convergence depends on the choice of other parameters such as step size and additional constants
391
in dual averaging. For those reasons, we still prefer the tuning-free and fast coordinate descent
392
methods when the subproblem is manageable.
393
The lasso has nice variable selection and prediction properties if the linear model assumption
394
together with some additional assumptions such as the restricted eigenvalue condition (Bickel et al.,
395
2009) or the irrepresentable condition (Zhao and Yu, 2006) holds. In practice, such assumptions do
396
not always hold and are often hard to verify. In our UK Biobank application, we don’t attempt to
397
verify the exact conditions, and the selected model can be subject to false positives. However, we
398
demonstrate relevance of the selection via empirical consistency with the GWAS results. We have
399
seen superior prediction performance by the lasso as a regularized regression method compared to
400
other methods. More importantly, by leveraging the sparsity property of the lasso, we are able to
401
manage the ultrahigh-dimensional problem and obtain a computationally eﬃcient solution.
402
When comparing with other methods in the UK Biobank experiments, due to the large number
403
of test samples (60,000+), we are conﬁdent that the lasso and elastic-net methods are able to do
404
signiﬁcantly better than other methods. In fact, the standard error of R2 can be easily derived
405
by the delta method, and the standard error of the AUC can be estimated and upper bounded by
406
1/(4 min(m, n)) (DeLong et al., 1988; Cortes and Mohri, 2005), where m, n represents the number
407
of positive and negative samples. For height and BMI, it turns out that the standard errors are
408
roughly 0.001, or 0.1%. For asthma and high cholesterol, considering the case rate around 12%,
409
the standard errors can be upper bounded by 0.005, or 0.5%.
Therefore, on height, BMI and
410
asthma, the lasso and elastic net perform signiﬁcantly better than the other methods, while on
411
21
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

high cholesterol, the Sequential LR and the relaxed lasso have competitive performance as well.
412
4
Materials and Methods
413
Variants in the BASIL framework
Some other very useful components can be easily incorpo-
414
rated into the BASIL framework. We will discuss debiasing using the relaxed lasso and the inclusion
415
of adjustment covariates.
416
The lasso is known to shrink coeﬃcients to exclude noise variables, but sometimes such shrink-
age can degrade the predictive performance due to its eﬀect on actual signal variables. Meinshausen
(2007) introduces the relaxed lasso to correct for the potential over-shrinkage of the original lasso
estimator. They propose a reﬁtting step on the active set of the lasso solution with less regular-
ization, while a common way of using it is to ﬁt a standard OLS on the active set. The active set
coeﬃcients are then set to
ˆβA,Relax(λ) = argmin
βA∈R|A| ∥y −XAβA∥2
2,
whereas the coeﬃcients for the inactive set remain at 0. This reﬁtting step can revert some of the
417
shrinkage bias introduced by the vanilla lasso. It doesn’t always reduce prediction error due to the
418
accompanied increase in variance when there are many variables in the model or when the signals
419
are weak. That being said, we can still insert a relaxed lasso step with little eﬀort in our iterative
420
procedure: once a valid lasso solution is found for a new λ, we may reﬁt with OLS. As we iterate,
421
we can monitor validation error for the lasso and the relaxed lasso. The relaxed lasso will generally
422
end up choosing a smaller set of variables than the lasso solution in the optimal model.
423
In some applications such as GWAS, there may be confounding variables Z ∈Rn×q that we
424
want to adjust for in the model. Population stratiﬁcation, deﬁned as the existence of a systematic
425
ancestry diﬀerence in the sample data, is one of the common factors in GWAS that can lead to
426
spurious discoveries. This can be controlled for by including some leading principal components of
427
the SNP matrix as variables in the regression (Price et al., 2006). In the presence of such variables,
428
22
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

we instead solve
429
(ˆα(λ), ˆβ(λ)) =
argmin
α∈Rq,β∈Rp
1
2n∥y −Zα −Xβ∥2
2 + λ∥β∥1.
(7)
This variation can be easily handled with small changes in the algorithm. Instead of initializing
430
the residual with the response y, we set r(0) equal to the residual from the regression of y on the
431
covariates. In the ﬁtting step, in addition to the variables in the strong set, we include the covariates
432
but leave their coeﬃcients unpenalized as in (7). Notice that if we want to ﬁnd relaxed lasso ﬁt
433
with the presence of adjustment covariates, we need to include those covariates in the OLS as well,
434
i.e.,
435
(ˆαRelax(λ), ˆβA,Relax(λ)) =
argmin
α∈Rq,βA∈R|A| ∥y −Zα −XAβA∥2
2.
(8)
436
UK Biobank experiment details
We focused on 337,199 White British unrelated individuals
437
out of the full set of over 500,000 from the UK Biobank dataset (Bycroft et al., 2018) that satisfy
438
the same set of population stratiﬁcation criteria as in DeBoever et al. (2018): (1) self-reported
439
White British ancestry, (2) used to compute principal components, (3) not marked as outliers for
440
heterozygosity and missing rates, (4) do not show putative sex chromosome aneuploidy, and (5)
441
have at most 10 putative third-degree relatives. These criteria are meant to reduce the eﬀect of
442
confoundedness and unreliable observations.
443
The number of samples is large in the UK Biobank dataset, so we can aﬀord to set aside
444
an independent validation set without resorting to the costly cross-validation to ﬁnd an optimal
445
regularization parameter. We also leave out a subset of observations as test set to evaluate the ﬁnal
446
model. In particular, we randomly partition the original dataset so that 60% is used for training,
447
20% for validation and 20% for test. The lasso solution path is ﬁt on the training set, whereas the
448
desired regularization is selected on the validation set, and the resulting model is evaluated on the
449
test set.
450
We are going to further discuss some details in our application that one might also encounter
451
in practice. They include adjustment for confounders, missing value imputation and variable stan-
452
23
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

dardization in the algorithm.
453
In genetic studies, spurious associations are often found due to confounding factors. Among
454
the others, one major source is the so-called population stratiﬁcation (Patterson et al., 2006). To
455
adjust for that eﬀect, it is common is to introduce the top principal components and include them
456
in the regression model. Therefore in the lasso method, we are going to solve (7) where in addition
457
to the SNP matrix X, we let Z include covariates such as age, sex and the top 10 PCs of the SNP
458
matrix.
459
Missing values are present in the dataset. As quality control normally done in genetics, we
460
ﬁrst discard observations whose phenotypic value of interest is not available. We further exclude
461
variants whose missing rate is greater than 10% or the minor allele frequency (MAF) is less than
462
0.1%, which results in around 685,000 SNPs for height. In particulr, 685,362 for height, 685,371 for
463
BMI, 685,357 for asthma and 685,357 for HC. The number varies because the criteria are evaluated
464
on the subset of individuals whose phenotypic value is observed (after excluding the missing ones),
465
which can be diﬀerent across diﬀerent phenotypes. For those remaining variants, mean imputation
466
is conducted to ﬁll the missing SNP values; that is, the missing values in every SNP are imputed
467
with the mean observed level of that SNP in the population under study.
468
When it comes to the lasso ﬁtting, there are some subtleties that can aﬀect its variable selection
469
and prediction performance. One of them is variable standardization. It is often a step done without
470
much thought to deal with heterogeneity in variables so that they are treated fairly in the objective.
471
However in our studies, standardization may create some undesired eﬀect. To see this, notice that
472
all the SNPs can only take values in 0, 1, 2 and NA — they are already on the same scale by
473
nature. As we know, standardization would use the current standard deviation of each predictor
474
as the divisor to equalize the variance across all predictors in the lasso ﬁtting that follows. In this
475
case, standardization would unintentionally inﬂate the magnitude of rare variants and give them
476
an advantage in the selection process since their coeﬃcients eﬀectively receive less penalty after
477
standardization. In Figure 6, we can see the distribution of standard deviation across all variants in
478
our dataset. Hence, to avoid potential spurious ﬁndings, we choose not to standardize the variants
479
in the experiments.
480
24
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Histogram of SNP Standard Deviation
SD
Frequency
0.0
0.2
0.4
0.6
0.8
1.0
0
20000
40000
60000
80000
Figure 6: Histogram of the standard deviations of the SNPs. They are computed after mean imputation
of the missing values because they would be the exact standardization factors to be used if the lasso were
applied with variable standardization on the mean-imputed SNP matrix.
Computational optimization in software implementation
Among the iterative steps in
481
BASIL, screening and checking are where we need to deal with the full dataset. To deal with the
482
memory bound, we can use memory-mapped I/O. In R, bigmemory (Kane et al., 2013) provides
483
a convenient implementation for that purpose. That being said, we do not want to rely on that
484
for intensive computation modules such as cyclic coordinate descent, because frequent visits to the
485
on-disk data would still be slow. Instead, since the subset of strong variables would be small, we
486
can aﬀord to bring them to memory and do fast lasso ﬁtting there. We only use the full memory-
487
mapped dataset in KKT checking and screening. Moreover since checking in the current iteration
488
can be done together with the screening in the next iteration, eﬀectively only one expensive pass
489
over the full dataset is needed every iteration.
490
In addition, we use a set of techniques to speed up the computation. First, the KKT check can be
easily parallelized by splitting on the features when multi-core machines are available. The speedup
of this part is immediate and (slightly less than) proportional to the number of cores available.
Second, speciﬁc to the application, we exploit the fact that there are only 4 levels for each SNP
25
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Multiplication Method
n = 200, p = 800
n = 2000, p = 8000
Standard
3.20
306.01
SNP-Optimized
1.32
130.21
Table 3: Timing performance (milliseconds) on multiplication of SNP matrix and residual matrix. The
methods are all implemented in C++ and run on a Macbook with 2.9 GHz Intel Core i7 and 8 GB 1600
MHz DDR3.
value and design a faster inner product routine to replace normal ﬂoat number multiplication in
the KKT check step. In fact, given any SNP vector x ∈{0, 1, 2, µ}n where µ is the imputed value
for the missing ones, we can write the dot product with a vector r ∈Rn as
x⊤r =
n
X
i=1
xiri = 1 ·
X
i:xi=1
ri + 2 ·
X
i:xi=2
ri + µ ·
X
i:xi=µ
ri.
We see that the terms corresponding to 0 SNP value can be ignored because they don’t contribute
491
to the ﬁnal result. This will signiﬁcantly reduce the number of arithmetic operations needed to
492
compute the inner product with rare variants. Further, we only need to set up 3 registers, each
493
for one SNP value accumulating the corresponding terms in r. A series of multiplications is then
494
converted to summations. In our UK Biobank studies, although the SNP matrix is not sparse
495
enough to exploit sparse matrix representation, it still has around 70% 0’s. We conduct a small
496
experiment to compare the time needed to compute X⊤R, where X ∈{0, 1, 2, 3}n×p, R ∈Rp×k.
497
The proportions for the levels in X are about 70%, 10%, 10%, 10%, similar to the distribution of
498
SNP levels in our study, and R resembles the residual matrix when checking the KKT condition.
499
The number of residual vectors is k = 20. The mean time over 100 repetitions is shown in Table 3.
500
We implement the procedure with all the optimizations in an R package called snpnet, which is
501
currently available at https://github.com/junyangq/snpnet. It assumes pgen ﬁle format (Chang
502
et al., 2015) of the SNP matrix, ﬁts the lasso solution path and allows early stopping if a validation
503
dataset is provided. In order to achieve better eﬃciency, we suggest using snpnet together with
504
glmnetPlus, a warm-started version of glmnet, which is currently available at https://github.
505
com/junyangq/glmnetPlus. It allows one to provide a good initialization of the coeﬃcients to ﬁt
506
part of the solution path instead of always starting from the all-zero solution by glmnet.
507
26
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Related methods and packages
There are a number of existing screening rules for solving
508
big lasso problems. Sobel et al. (2009) use a screened set to scale down the logistic lasso problem
509
and check the KKT condition to validate the solution.
Their focus, however, is on selecting a
510
lasso model of particular size and only the initial screened set is expanded if the KKT condition is
511
violated. In contrast, we are interested in ﬁnding the whole solution path (before overﬁtting). We
512
adopt a sequential approach and keep updating the screened set at each iteration. This allows us
513
to potentially keep the screened set small as we move along the solution path. Other rules include
514
the SAFE rule (El Ghaoui et al., 2010), Sure Independence Screening (Fan and Lv, 2008), and the
515
DPP and EDPP rules (Wang et al., 2015).
516
We expand the discussion on these screening rules a bit. Fan and Lv (2008) exploits marginal
517
information of correlation to conduct screening but the focus there is not optimization algorithm.
518
Most of the screening rules mentioned above (except for EDPP) use inner product with the current
519
residual vector to measure the importance of each predictor at the next λ — those under a threshold
520
can be ignored. The key diﬀerence across those rules is the threshold deﬁned and whether the
521
resulting discard is safe. If it is safe, one can guarantee that only one iteration is needed for each λ
522
value, compared with others that would need more rounds if an active variable was falsely discarded.
523
Though the strong rules rarely make this mistake, safe screening is still a nice feature to have in
524
single-λ solutions. However, under the batch mode we consider due to the desire of reducing the
525
number of full passes over the dataset, the advantage of safe threshold may not be as much. In
526
fact, one way we might be able to leverage the safe rules in the batch mode is to ﬁrst ﬁnd out the
527
set of candidate predictors for the several λ values up to λk we wish to solve in the next iteration
528
based on the current inner products and the rules’ safe threshold, and then solve the lasso for these
529
parameters. Since these rules can often be conservative, we would then have strong incentive to
530
solve for, say, one further λ value λk+1 because if the current screening turns out to be a valid one
531
as well, we will ﬁnd one more lasso solution and move one step forward along the λ sequence we
532
want to solve for. This can potentially save one iteration of the procedure and thus one expensive
533
pass over the dataset. The only cost there is computing the lasso solution for one more λk+1 and
534
computing inner products with one more residual vector at λk+1 (to check the KKT condition).
535
27
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

The latter can be done in the same pass as we compute inner products at λk for preparing the
536
screening in the next iteration, and so no additional pass is needed. Thus under the batch mode,
537
the property of safe screening may not be as important due to the incentive of aggressive model
538
ﬁtting. Nevertheless it would be interesting to see in the future EDPP-type batch screening. It
539
uses inner products with a modiﬁcation of the residual vector. Our algorithm still focuses of inner
540
products with the vanilla residual vector.
541
To address the large-scale lasso problems, several packages have been developed such as biglasso
542
(Zeng and Breheny, 2017), bigstatsr (Priv´e et al., 2018), oem (Huling and Qian, 2018) and the
543
lasso routine from PLINK 1.9 (Chang et al., 2015).
544
Among them, oem specializes in tall data (big n) and can be slow when p > n. In many real
545
data applications including ours, the data are both large-sample and high-dimensional. However,
546
we might still be able to use oem for the small lasso subroutine since a large number of variables
547
have already been excluded. The other packages, biglasso, bigstatsr, PLINK 1.9, all provide
548
eﬃcient implementations of the pathwise coordinate descent with warm start.
PLINK 1.9 is
549
speciﬁcally developed for genetic datasets and is widely used in GWAS and research in population
550
genetics. In bigstatsr, the big spLinReg function adapts from the biglasso function in biglasso
551
and incorporates a Cross-Model Selection and Averaging (CMSA) procedure, which is a variant
552
of cross-validation that saves computation by directly averaging the results from diﬀerent folds
553
instead of retraining the model at the chosen optimal parameter. They both use memory-mapping to
554
process larger-than-RAM, on-disk datasets as if they were in memory, and based on that implement
555
coordinate descent with strong rules and warm start.
556
The main diﬀerence between BASIL and the algorithm these packages use is that BASIL tries to
557
solve a series of models every full scan of the dataset (at checking and screening) and thus eﬀectively
558
reduce the number of passes over the dataset. This diﬀerence may not be signiﬁcant in small or
559
moderate-sized problems, but can be critical in big data applications especially when the dataset
560
cannot be fully loaded into the memory. A full scan of a larger-than-RAM dataset can incur a lot
561
of swap-in/out between the memory and the disk, and thus a lot of disk I/O operations, which is
562
known to be orders of magnitude slower than in-memory operations. Thus reducing the number of
563
28
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

full scans can greatly improve the overall performance of the algorithm.
564
Aside from potential eﬃciency consideration, all of those packages aforementioned have to re-
565
implement a variety of features existent in many small-data solutions but for big-data context.
566
Nevertheless, currently they don’t provide as much functionality as needed in our real-data ap-
567
plication. First, in the current implementations, PLINK 1.9 only supports the Gaussian family,
568
biglasso and bigstatsr only supports the Gaussian and binomial families, whereas snpnet can
569
easily extend to other regression families and already built in Gaussian, binomial and Cox fami-
570
lies. Also, biglasso, bigstatsr and PLINK 1.9 all standardize the predictors beforehand, but in
571
many applications such as our UK Biobank studies, it is more reasonable to leave the predictors
572
unstandardized. In addition, it can take some eﬀort to convert the data to the desired format by
573
these packages. This would be a headache if the raw data is in some special format and one cannot
574
aﬀord to ﬁrst convert the full dataset into an intermediate format for which a tool is provided to
575
convert to the desired one by biglasso or bigstatsr. This can happen, for example, if the raw
576
data is highly compressed in a special format. For the BED binary format we work with in our
577
application, readRAW big.matrix function from BGData can convert a raw ﬁle to a big.matrix
578
object desired by biglasso, and snp readBed function from bigsnpr (Priv´e et al., 2018) allows one
579
to convert it to FBM object desired by bigstatsr. However, bigsnpr doesn’t take input data that
580
has any missing values, which can prevalent in an SNP matrix (as in our application). Although
581
PLINK 1.9 works directly with the BED binary ﬁle, its lasso solver currently only supports the
582
Gaussian family, and it doesn’t return the full solution path. Instead it returns the solution at the
583
smallest λ value computed and needs a good heritability estimate as input from the user, which
584
may not be immediately available.
585
We summarize the main advantages of the BASIL algorithm:
586
• Input data ﬂexibility. Our algorithm allows one to deal directly with any data type as
587
long as the screening and checking steps are implemented, which is often very lightweight
588
development work like matrix multiplication. This can be important in large-scale applications
589
especially when the data is stored in a compressed format or a distributed way since then
590
we would not need to unpack the full data and can conduct KKT check and screening on its
591
29
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

original format. Instead only a small screened subset of the data needs to be converted to the
592
desired format by the lasso solver in the ﬁtting step.
593
• Model ﬂexibility.
We can easily transfer the modeling ﬂexibility provided by existing
594
packages to the big data context, such as the options of standardization, sample weights,
595
lower/upper coeﬃcient limits and other families in generalized linear models provided by
596
existing packages such as glmnet. This can be useful, for example, when we may not want to
597
standardize predictors already in the same unit to avoid unintentionally diﬀerent penalization
598
of the predictors due to diﬀerence in their variance.
599
• Eﬀortless development. The BASIL algorithm allows one to maximally reuse the existing
600
lasso solutions for small or moderate-sized problems.
The main extra work would be an
601
implementation of batch screening and KKT check with respect to a particular data type.
602
For example, in the snpnet package, we are able to quickly extend the in-memory glmnet
603
solution to large-scale, ultrahigh-dimentional SNP data. Moreover, the existing convenient
604
data interface provided by the BEDMatrix package further facilitates our implementation.
605
• Computational eﬃciency. Our design reduces the number of visits to the original data
606
that sits on the disk, which is crucial to the overall eﬃciency as disk read can be orders of
607
magnitude slower than reading from the RAM. The key to achieving this is to bring batches
608
of promising variables into the main memory, hoping to ﬁnd the lasso solutions for more than
609
one λ value each iteration and check the KKT condition for those λ values in one pass of the
610
entire dataset.
611
Lastly, we are going to provide some timing comparison with existing packages. As mentioned
612
in previous sections, those packages provide diﬀerent functionalities and have diﬀerent restrictions
613
on the dataset. For example, most of them (biglasso, bigstatsr) assume that there are no missing
614
values, or the missing ones have already been imputed. In bigsnpr, for example, we shouldn’t have
615
SNPs with 0 MAF either. Some packages always standardize the variants before ﬁtting the lasso.
616
To provide a common playground, we create a synthetic dataset with no missing values, and follow
617
a standardized lasso procedure in the ﬁtting stage, simply to test the computation. The dataset has
618
30
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

R Package
Elapsed Time (minutes)
bigstatsr (Priv´e et al., 2018)
2.93 + 56.80
bigstatsr + CMSA (Priv´e et al., 2018)
2.93 + 101.75
biglasso(Zeng and Breheny, 2017)
4.55 + 54.27
PLINK (Chang et al., 2015)
53.52
snpnet
44.79
Table 4: Timing comparison on a synthetic dataset of size n = 50, 000 and p = 100, 000. The time for
bigstatsr and biglasso has two components: one for the conversion to the desired data type and the other
for the actual computation. The experiments are all run with 16 cores and 64 GB memory.
50,000 samples and 100,000 variables, and each takes value in the SNP range, i.e., in 0, 1, or 2. We
619
ﬁt the ﬁrst 50 lasso solutions along a preﬁx λ sequence that contains 100 initial λ values (like early
620
stopping for most phenotypes). The total time spent is displayed in Table 4. For bigstatsr, we
621
include two versions since it does cross-validation by default. In one version, we make it comply with
622
our single train/val/test split, while in the other version, we use its default 10-fold cross-validation
623
version — Cross-Model Selection and Averaging (CMSA). Notice that the ﬁnal solution of iCMSA
624
is diﬀerent from the exact lasso solution on the full data because the returned coeﬃcient vector is
625
a linear combination of the coeﬃcient vectors from the 10 folds rather than from a retrained model
626
on the full data. We uses 128GB memory and 16 cores for the computation.
627
From the table, we see that snpnet is at about 20% faster than other packages concerned. The
628
numbers before the “+” sign are the time spent on converting the raw data to the required data
629
format by those packages. The second numbers are time spent on actual computation.
630
It is important to note though that the performance relies not only on the algorithm, but also
631
heavily on the implementations. The other packages in comparison all have their major computation
632
done with C++ or Fortran. Ours, for the purpose of meta algorithm where users can easily integrate
633
with any lasso solver in R, still has a signiﬁcant portion (the iterations) in R and multiple rounds of
634
cross-language communication. That can degrade the timing performance to some degree. If there
635
is further pursuit of speed performance, there is still space for improvement by more designated
636
implementation.
637
31
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Acknowledgements
638
We thank Balasubramanian Narasimhan for helpful discussion on the package development, Ken-
639
neth Tay and the members of the Rivas lab for insightful feedback. J.Q. is partially supported by
640
the Two Sigma Graduate Fellowship. Y.T. is supported by a Funai Overseas Scholarship from the
641
Funai Foundation for Information Technology and the Stanford University School of Medicine.
642
M.A.R. is supported by Stanford University and a National Institute of Health center for Multi
643
and Trans-ethnic Mapping of Mendelian and Complex Diseases grant (5U01 HG009080). This work
644
was supported by National Human Genome Research Institute (NHGRI) of the National Institutes
645
of Health (NIH) under awards R01HG010140. The content is solely the responsibility of the authors
646
and does not necessarily represent the oﬃcial views of the National Institutes of Health.
647
R.T. was partially supported by NIH grant 5R01 EB001988-16 and NSF grant 19 DMS1208164.
648
T.H. was partially supported by grant DMS-1407548 from the National Science Foundation, and
649
grant 5R01 EB 001988-21 from the National Institutes of Health.
650
This research has been conducted using the UK Biobank Resource under application number
651
24983.
We thank all the participants in the study.
The primary and processed data used to
652
generate the analyses presented here are available in the UK Biobank access management system
653
(https://amsportal.ukbiobank.ac.uk/) for application 24983, ”Generating eﬀective therapeutic
654
hypotheses from genomic and hospital linkage data” (http://www.ukbiobank.ac.uk/wp-content/
655
uploads/2017/06/24983-Dr-Manuel-Rivas.pdf), and the results are displayed in the Global
656
Biobank Engine (https://biobankengine.stanford.edu).
657
Some of the computing for this project was performed on the Sherlock cluster. We would like
658
to thank Stanford University and the Stanford Research Computing Center for providing compu-
659
tational resources and support that contributed to these research results.
660
Author Contributions
661
Conceptualization: Junyang Qian, Trevor Hastie
662
Data curation: Yosuke Tanigawa, Matthew Aguirre, Manuel A. Rivas
663
32
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Formal Analysis: Junyang Qian, Wenfei Du, Robert Tibshirani, Trevor Hastie
664
Funding Acquisition: Robert Tibshirani, Manuel A. Rivas, Trevor Hastie
665
Methodology: Junyang Qian, Trevor Hastie
666
Software: Junyang Qian, Yosuke Tanigawa, Chris Chang
667
Supervision: Robert Tibshirani, Manuel A. Rivas, Trevor Hastie
668
Validation: Yosuke Tanigawa, Matthew Aguirre, Manuel A. Rivas
669
Visualization: Junyang Qian, Wenfei Du
670
Writing - Original Draft: Junyang Qian, Wenfei Du
671
Writing - Review & Editing: Yosuke Tanigawa, Matthew Aguirre, Robert Tibshirani, Manuel
672
A. Rivas, Trevor Hastie
673
References
674
Clare Bycroft, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T. Elliott, Kevin Sharp,
675
Allan Motyer, Damjan Vukcevic, Olivier Delaneau, Jared O?Connell, Adrian Cortes, Samantha
676
Welsh, Alan Young, Mark Eﬃngham, Gil McVean, Stephen Leslie, Naomi Allen, Peter Donnelly,
677
and Jonathan Marchini. The uk biobank resource with deep phenotyping and genomic data.
678
Nature, 562(7726):203–209, 2018. ISSN 1476-4687. doi: 10.1038/s41586-018-0579-z. URL https:
679
//doi.org/10.1038/s41586-018-0579-z.
680
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
681
Society. Series B (Methodological), 58(1):267–288, 1996. ISSN 00359246. URL http://www.
682
jstor.org/stable/2346178.
683
Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear
684
models via coordinate descent, 2010a. ISSN 1548-7660. URL https://www.jstatsoft.org/
685
v033/i01.
686
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning:
687
33
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Data Mining, Inference, and Prediction, 2nd Edition.
Springer series in statistics. Springer-
688
Verlag, 2009. doi: 10.1007/978-0-387-84858-7.
689
Bradley Efron and Trevor Hastie. Computer Age Statistical Inference: Algorithms, Evidence, and
690
Data Science, volume 5. Cambridge University Press, 2016.
691
Jeﬀrey Dean and Sanjay Ghemawat.
Mapreduce: Simpliﬁed data processing on large clusters.
692
Commun. ACM, 51(1):107–113, January 2008. ISSN 0001-0782. doi: 10.1145/1327452.1327492.
693
URL http://doi.acm.org/10.1145/1327452.1327492.
694
Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion Stoica. Spark:
695
Cluster computing with working sets. In Proceedings of the 2Nd USENIX Conference on Hot
696
Topics in Cloud Computing, HotCloud’10, pages 10–10, Berkeley, CA, USA, 2010. USENIX
697
Association. URL http://dl.acm.org/citation.cfm?id=1863103.1863113.
698
Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean, Matthieu
699
Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
700
Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan,
701
Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-
702
scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems De-
703
sign and Implementation, OSDI’16, pages 265–283, Berkeley, CA, USA, 2016. USENIX Associa-
704
tion. ISBN 978-1-931971-33-1. URL http://dl.acm.org/citation.cfm?id=3026877.3026899.
705
R Core Team.
R: A Language and Environment for Statistical Computing.
R Foundation for
706
Statistical Computing, Vienna, Austria, 2017. URL https://www.R-project.org/.
707
Patrick Breheny and Jian Huang. Coordinate descent algorithms for nonconvex penalized regression,
708
with applications to biological feature selection. The Annals of Applied Statistics, 5(1):232–253,
709
03 2011. doi: 10.1214/10-AOAS388. URL https://doi.org/10.1214/10-AOAS388.
710
Trevor Hastie. Statistical learning with big data. Presentation at Data Science at Stanford Seminar,
711
2015.
712
34
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Peter M. Visscher, Naomi R. Wray, Qian Zhang, Pamela Sklar, Mark I. McCarthy, Matthew A.
713
Brown, and Jian Yang. 10 years of gwas discovery: Biology, function, and translation. The
714
American Journal of Human Genetics, 101(1):5–22, 2017. ISSN 0002-9297. doi: 10.1016/j.ajhg.
715
2017.06.005. URL https://doi.org/10.1016/j.ajhg.2017.06.005.
716
Christopher C Chang, Carson C Chow, Laurent CAM Tellier, Shashaank Vattikuti, Shaun M
717
Purcell, and James J Lee. Second-generation PLINK: rising to the challenge of larger and richer
718
datasets. GigaScience, 4(1), 02 2015. ISSN 2047-217X. doi: 10.1186/s13742-015-0047-8. URL
719
https://doi.org/10.1186/s13742-015-0047-8.
720
Robert Tibshirani, Jacob Bien, Jerome Friedman, Trevor Hastie, Noah Simon, Jonathan Taylor,
721
and Ryan J. Tibshirani. Strong rules for discarding predictors in lasso-type problems. Journal
722
of the Royal Statistical Society. Series B (Statistical Methodology), 74(2):245–266, 2012. ISSN
723
13697412, 14679868. URL http://www.jstor.org/stable/41430939.
724
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge university press, 2004.
725
Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear
726
models via coordinate descent. Journal of Statistical Software, Articles, 33(1):1–22, 2010b. ISSN
727
1548-7660. doi: 10.18637/jss.v033.i01. URL https://www.jstatsoft.org/v033/i01.
728
D. R. Cox. Regression models and life-tables. Journal of the Royal Statistical Society. Series B
729
(Methodological), 34(2):187–220, 1972. ISSN 00359246. URL http://www.jstor.org/stable/
730
2985181.
731
Ruilin Li, Christopher Chang, Johanne Marie Justesen, Yosuke Tanigawa, Junyang Qian, Trevor
732
Hastie, Manuel A. Rivas, and Robert Tibshirani. Fast lasso method for large-scale and ultrahigh-
733
dimensional cox model with applications to uk biobank. bioRxiv, 2020. doi: 10.1101/2020.01.20.
734
913194. URL https://www.biorxiv.org/content/early/2020/01/21/2020.01.20.913194.
735
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the
736
Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005. doi: 10.1111/j.
737
35
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

1467-9868.2005.00503.x. URL https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.
738
1467-9868.2005.00503.x.
739
Louis Lello, Steven G. Avery, Laurent Tellier, Ana I. Vazquez, Gustavo de los Campos, and Stephen
740
D. H. Hsu. Accurate genomic prediction of human height. Genetics, 210(2):477–497, 2018. ISSN
741
0016-6731. doi: 10.1534/genetics.118.301267. URL http://www.genetics.org/content/210/
742
2/477.
743
Christopher DeBoever, Yosuke Tanigawa, Malene E. Lindholm, Greg McInnes, Adam Lavertu,
744
Erik Ingelsson, Chris Chang, Euan A. Ashley, Carlos D. Bustamante, Mark J. Daly, and
745
Manuel A. Rivas. Medical relevance of protein-truncating variants across 337,205 individuals
746
in the uk biobank study.
Nature Communications, 9(1):1612, 2018.
ISSN 2041-1723.
doi:
747
10.1038/s41467-018-03910-9. URL https://doi.org/10.1038/s41467-018-03910-9.
748
Herman Wold. Soft modelling by latent variables: The non-linear iterative partial least squares
749
(nipals) approach.
Journal of Applied Probability, 12(S1):117?142, 1975.
doi:
10.1017/
750
S0021900200047604.
751
Nicolai Meinshausen. Relaxed lasso. Computational Statistics & Data Analysis, 52(1):374 – 393,
752
2007.
ISSN 0167-9473.
doi: https://doi.org/10.1016/j.csda.2006.12.019.
URL http://www.
753
sciencedirect.com/science/article/pii/S0167947306004956.
754
Tian Ge, Chia-Yen Chen, Yang Ni, Yen-Chen Anne Feng, and Jordan W Smoller.
Polygenic
755
prediction via bayesian regression and continuous shrinkage priors. Nature Communications, 10
756
(1):1–10, 2019.
757
Luke R Lloyd-Jones, Jian Zeng, Julia Sidorenko, Lo¨ıc Yengo, Gerhard Moser, Kathryn E Kemper,
758
Huanwei Wang, Zhili Zheng, Reedik Magi, Tonu Esko, et al. Improved polygenic prediction by
759
bayesian multiple regression on summary statistics. Nature Communications, 10(1):1–11, 2019.
760
Jian Zeng, Ronald De Vlaming, Yang Wu, Matthew R Robinson, Luke R Lloyd-Jones, Loic Yengo,
761
Chloe X Yap, Angli Xue, Julia Sidorenko, Allan F McRae, et al. Signatures of negative selection
762
in the genetic architecture of human complex traits. Nature Genetics, 50(5):746–753, 2018.
763
36
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Yosuke Tanigawa, Jiehan Li, Johanne M Justesen, Heiko Horn, Matthew Aguirre, Christopher
764
DeBoever, Chris Chang, Balasubramanian Narasimhan, Kasper Lage, Trevor Hastie, et al. Com-
765
ponents of genetic associations across 2,138 phenotypes in the uk biobank highlight adipocyte
766
biology. Nature communications, 10(1):1–14, 2019.
767
Karri Silventoinen, Sampo Sammalisto, Markus Perola, Dorret I. Boomsma, Belinda K. Cornes,
768
Chayna Davis, Leo Dunkel, Marlies de Lange, Jennifer R. Harris, Jacob V.B. Hjelmborg, and
769
et al. Heritability of adult body height: A comparative study of twin cohorts in eight countries.
770
Twin Research, 6(5):399?408, 2003. doi: 10.1375/twin.6.5.399.
771
Peter M Visscher, Sarah E Medland, Manuel A. R Ferreira, Katherine I Morley, Gu Zhu, Belinda K
772
Cornes, Grant W Montgomery, and Nicholas G Martin. Assumption-free estimation of heritability
773
from genome-wide identity-by-descent sharing between full siblings. PLOS Genetics, 2(3):1–10,
774
03 2006. doi: 10.1371/journal.pgen.0020041. URL https://doi.org/10.1371/journal.pgen.
775
0020041.
776
Peter M. Visscher, Brian McEvoy, and Jian Yang. From galton to gwas: Quantitative genetics of
777
human height. Genetics Research, 92(5-6):371?379, 2010. doi: 10.1017/S0016672310000571.
778
Noah Zaitlen, Peter Kraft, Nick Patterson, Bogdan Pasaniuc, Gaurav Bhatia, Samuela Pollack,
779
and Alkes L. Price.
Using extended genealogy to estimate components of heritability for 23
780
quantitative and dichotomous traits. PLOS Genetics, 9(5):1–11, 05 2013. doi: 10.1371/journal.
781
pgen.1003520. URL https://doi.org/10.1371/journal.pgen.1003520.
782
Gibran Hemani, Jian Yang, Anna Vinkhuyzen, Joseph E Powell, Gonneke Willemsen, Jouke-Jan
783
Hottenga, Abdel Abdellaoui, Massimo Mangino, Ana M Valdes, Sarah E Medland, Pamela A
784
Madden, Andrew C Heath, Anjali K Henders, Dale R Nyholt, Eco J C. de Geus, Patrik K E.
785
Magnusson, Erik Ingelsson, Grant W Montgomery, Timothy D Spector, Dorret I Boomsma,
786
Nancy L Pedersen, Nicholas G Martin, and Peter M Visscher. Inference of the genetic architecture
787
underlying bmi and height with the use of 20,240 sibling pairs. The American Journal of Human
788
37
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Genetics, 93(5):865–875, 2013. ISSN 0002-9297. doi: 10.1016/j.ajhg.2013.10.005. URL https:
789
//doi.org/10.1016/j.ajhg.2013.10.005.
790
Jian Yang, Beben Benyamin, Brian P. McEvoy, Scott Gordon, Anjali K. Henders, Dale R. Nyholt,
791
Pamela A. Madden, Andrew C. Heath, Nicholas G. Martin, Grant W. Montgomery, Michael E.
792
Goddard, and Peter M. Visscher. Common snps explain a large proportion of the heritability for
793
human height. Nature Genetics, 42:565, 2010. doi: 10.1038/ng.608. URL https://doi.org/10.
794
1038/ng.608.
795
Jian Yang, Andrew Bakshi, Zhihong Zhu, Gibran Hemani, Anna A. E. Vinkhuyzen, Sang Hong
796
Lee, et al. Genetic variance estimation with imputed variants ﬁnds negligible missing heritability
797
for human height and body mass index. Nature Genetics, 47:1114, 2015. doi: 10.1038/ng.3390.
798
URL https://doi.org/10.1038/ng.3390.
799
Hana Lango Allen, Karol Estrada, Guillaume Lettre, Sonja I. Berndt, Michael N. Weedon, Fernando
800
Rivadeneira, et al. Hundreds of variants clustered in genomic loci and biological pathways aﬀect
801
human height. Nature, 467:832, 2010. doi: 10.1038/nature09410. URL https://doi.org/10.
802
1038/nature09410.
803
Andrew R. Wood, Tonu Esko, Jian Yang, Sailaja Vedantam, Tune H. Pers, Stefan Gustafsson,
804
et al. Deﬁning the role of common variation in the genomic and biological architecture of adult
805
human height. Nature Genetics, 46:1173, 2014. doi: 10.1038/ng.3097. URL https://doi.org/
806
10.1038/ng.3097.
807
Eirini Marouli, Mariaelisa Graﬀ, Carolina Medina-Gomez, Ken Sin Lo, Andrew R. Wood, Troels R.
808
Kjaer, et al. Rare and low-frequency coding variants alter human adult height. Nature, 542:186,
809
2017. doi: 10.1038/nature21039. URL https://doi.org/10.1038/nature21039.
810
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1
811
(3):127–239, January 2014. ISSN 2167-3888. doi: 10.1561/2400000003. URL http://dx.doi.
812
org.stanford.idm.oclc.org/10.1561/2400000003.
813
38
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Lin Xiao.
Dual averaging methods for regularized stochastic learning and online optimization.
814
Journal of Machine Learning Research, 11(Oct):2543–2596, 2010.
815
J. C. Duchi, A. Agarwal, and M. J. Wainwright.
Dual averaging for distributed optimization:
816
Convergence analysis and network scaling.
IEEE Transactions on Automatic Control, 57(3):
817
592–606, March 2012. ISSN 0018-9286. doi: 10.1109/TAC.2011.2161027.
818
Peter J. Bickel, Ya’acov Ritov, and Alexandre B. Tsybakov. Simultaneous analysis of lasso and
819
dantzig selector. Ann. Statist., 37(4):1705–1732, 08 2009. doi: 10.1214/08-AOS620. URL https:
820
//doi.org/10.1214/08-AOS620.
821
Peng Zhao and Bin Yu. On model selection consistency of lasso. Journal of Machine learning
822
research, 7(Nov):2541–2563, 2006.
823
Elizabeth R. DeLong, David M. DeLong, and Daniel L. Clarke-Pearson.
Comparing the areas
824
under two or more correlated receiver operating characteristic curves: A nonparametric approach.
825
Biometrics, 44(3):837–845, 1988.
ISSN 0006341X, 15410420.
URL http://www.jstor.org/
826
stable/2531595.
827
Corinna Cortes and Mehryar Mohri. Conﬁdence intervals for the area under the roc curve. In
828
Advances in Neural Information Processing Systems, pages 305–312, 2005.
829
Alkes L. Price, Nick J. Patterson, Robert M. Plenge, Michael E. Weinblatt, Nancy A. Shadick, and
830
David Reich. Principal components analysis corrects for stratiﬁcation in genome-wide association
831
studies. Nature Genetics, 38:904, 2006. doi: 10.1038/ng1847. URL https://doi.org/10.1038/
832
ng1847.
833
Nick Patterson, Alkes L Price, and David Reich. Population structure and eigenanalysis. PLOS
834
Genetics, 2(12):1–20, 12 2006. doi: 10.1371/journal.pgen.0020190. URL https://doi.org/10.
835
1371/journal.pgen.0020190.
836
Michael J. Kane, John Emerson, and Stephen Weston.
Scalable strategies for computing with
837
39
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

massive data. Journal of Statistical Software, 55(14):1–19, 2013. URL http://www.jstatsoft.
838
org/v55/i14/.
839
Eric Sobel, Kenneth Lange, Tong Tong Wu, Trevor Hastie, and Yi Fang Chen.
Genome-Wide
840
Association Analysis by Lasso Penalized Logistic Regression. Bioinformatics, 25(6):714–721, 01
841
2009. ISSN 1367-4803. doi: 10.1093/bioinformatics/btp041. URL https://doi.org/10.1093/
842
bioinformatics/btp041.
843
Laurent El Ghaoui, Vivian Viallon, and Tarek Rabbani. Safe feature elimination for the lasso and
844
sparse supervised learning problems. arXiv preprint arXiv:1009.4219, 2010.
845
Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space.
846
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849–911, 2008.
847
doi:
10.1111/j.1467-9868.2008.00674.x.
URL https://rss.onlinelibrary.wiley.com/doi/
848
abs/10.1111/j.1467-9868.2008.00674.x.
849
Jie Wang, Peter Wonka, and Jieping Ye. Lasso screening rules via dual polytope projection. Jour-
850
nal of Machine Learning Research, 16:1063–1101, 2015. URL http://jmlr.org/papers/v16/
851
wang15a.html.
852
Yaohui Zeng and Patrick Breheny. The biglasso package: A memory-and computation-eﬃcient
853
solver for lasso model ﬁtting with big data in R. arXiv preprint arXiv:1701.05936, 2017.
854
Florian Priv´e, Michael G B Blum, Hugues Aschard, and Andrey Ziyatdinov. Eﬃcient Analysis of
855
Large-Scale Genome-Wide Data with Two R packages: bigstatsr and bigsnpr. Bioinformatics,
856
34(16):2781–2787, 03 2018. ISSN 1367-4803. doi: 10.1093/bioinformatics/bty185.
857
Jared D Huling and Peter ZG Qian. Fast penalized regression and cross validation for tall data
858
with the oem package. arXiv preprint arXiv:1801.09661, 2018.
859
Elizabeth K. Speliotes, Cristen J. Willer, Sonja I. Berndt, Keri L. Monda, Gudmar Thorleifsson,
860
Anne U. Jackson, et al. Association analyses of 249,796 individuals reveal 18 new loci associated
861
40
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

with body mass index.
Nature Genetics, 42:937, 2010.
doi: 10.1038/ng.686.
URL https:
862
//doi.org/10.1038/ng.686.
863
Adam E. Locke, Bratati Kahali, Sonja I. Berndt, Anne E. Justice, Tune H. Pers, Felix R. Day, Corey
864
Powell, et al. Genetic studies of body mass index yield new insights for obesity biology. Nature,
865
518:197, 2015. doi: 10.1038/nature14177. URL https://doi.org/10.1038/nature14177.
866
Stephen D. Turner. qqman: An R package for visualizing gwas results using q-q and manhattan
867
plots. Journal of Open Source Software, 3(25):731, 2018. doi: 10.21105/joss.00731.
868
41
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

A
Results for Additional Phenotypes
869
A.1
Body Mass Index (BMI)
870
BMI is another polygenic trait that is widely studied. Like height, it is heritable and easily mea-
871
sured. It is also a trait of interest, since obesity is a risk factor for diseases such as type 2 diabetes
872
and cardiovasclar disease. Recent studies estimate heritability at 0.42 (Zaitlen et al., 2013; Hemani
873
et al., 2013) and 27% of the variance can be explained using a genomic model (Yang et al., 2015).
874
We expect the heritability to be lower than that for height, since intuitively speaking, one com-
875
ponent of the body mass, weight, should heavily depend on environmental factors, for example,
876
individual’s lifestyle. From GWAS studies, 97 associated loci have been identiﬁed, but they only
877
account for 2.7% of the variance (Speliotes et al., 2010; Locke et al., 2015). Although the estimates
878
of heritability are not precise, there may be more missing heritability for BMI than for height. We
879
also ﬁnd lower R2 values using the lasso. The results are summarized in Table 5. The R2 curves
880
for the lasso and the relaxed lasso are shown in Figure 7. From the table, we see that more than
881
26,000 variants are selected by the lasso to attain an R2 greater than 10%. In constrast, the relaxed
882
lasso and the sequential linear regression use around one-tenths of the variables, and end up with
883
degraded predictive performance both at around 5%. From Figure 8, we see further evidence that
884
the actual BMI is of high variability and hard to predict with the lasso model — the correlation
885
between the predicted value and the actual value is 0.3256. From the residual histogram on the
886
right, we also see the distribution is skewed to the right, suggesting a number of exceedingly high
887
observed values than the ones predicted by the model. Nevertheless, we are able to predict BMI
888
within 9 kg/m2 about 95% of the time.
889
A.2
Asthma
890
Asthma is a common respiratory disease characterized by inﬂammation of airways in the lungs and
891
diﬃculty breathing. It is another complex, polygenic trait that is associated with both genetic
892
and environmental factors. Our results are summarized in Table 6. The AUC curves for the lasso
893
and the relaxed lasso are shown in Figure 9. In addition, for each test sample, we compute the
894
42
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Model
Form
R2
train
R2
val
R2
test
Size
(1)
Age + Sex
0.0092
0.0089
0.0083
2
(2)
Age + Sex + 10 PCs
0.0104
0.0103
0.0099
12
(3)
(2) + Single SNP
0.0134
0.0128
0.0124
13
(4)
(2) + 10K Combined
0.0384
0.0195
0.0210
10,012
(5)
(2) + 100K Combined
0.1307
0.0064
0.0093
100,012
(6)
Sequential LR
0.0865
0.0385
0.0395
2,012
(7)
Lasso
0.3196
0.1017
0.1052
26,060
(8)
Relaxed Lasso
0.1609
0.0504
0.0537
2,585
(9)
Elastic Net
0.3923
0.1040
0.1071
29,548
(10)
PRS-CS
0.0490
−
0.0315
148,052
(11)
SBayesR
0.0231
−
0.0139
658,693
Table 5: R2 values for BMI. For lasso and relaxed lasso, the chosen model is based on maximum R2 on
the validation set. Model (3) to (8) each includes Model (2) plus their own speciﬁcation as stated in the
Form column.The validation results for PRS-CS and SBayesR are not available because we used a combined
training and validation set for training.
0
20
40
60
0.00
0.05
0.10
0.15
0.20
Lambda Index
R2
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G G G G G G G G G
G
G G G G G G G G G G G G G G G G G G G G G G
G G G G G G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
G
G G G G
G G G G
G G
G
G
G G G G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
Lasso (train)
Lasso (val)
ReLasso (train)
ReLasso (val)
13
13
16
39
109
480
2171
6699
14818
26060
33906
Figure 7: R2 plot for BMI. The top axis shows the number of active variables in the model.
percentile of its predicted score/probability among the entire test cohort, and create box plots of
895
such percentiles separately for the control group and the case group. We see on the left of Figure 10
896
that there is a signiﬁcant overlap between the box plots of the two groups, suggesting that asthma
897
43
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
20
30
40
50
60
25
30
Predicted BMI (kg/m2)
Actual BMI (kg/m2)
0
3000
6000
9000
−10
0
10
20
30
Residual (kg/m2)
Frequency
Figure 8:
Left: actual BMI versus predicted BMI on 5000 random samples from the test set.
The
correlation between actual BMI and predicted BMI is 0.3256. Right: residuals of lasso prediction for BMI.
Standard deviation of the residual is 4.51 kg/m2.
Model
Form
AUCtrain
AUCval
AUCtest
Size
(1)
Age + Sex
0.5293
0.5297
0.5320
2
(2)
Age + Sex + 10 PCs
0.5342
0.5344
0.5367
12
(3)
(2) + Single SNP
0.5463
0.5476
0.5454
13
(4)
(2) + 10K Combined
0.5783
0.5580
0.5531
10,012
(5)
(2) + 100K Combined
0.6884
0.5644
0.5580
100,012
(6)
Sequential LR
0.6601
0.5883
0.5884
2,012
(7)
Lasso
0.7692
0.6159
0.6126
5,936
(8)
Relaxed Lasso
0.6747
0.5988
0.5955
621
(9)
Elastic Net
0.7803
0.6167
0.6131
7,799
(10)
PRS-CS
0.6300
−
0.5837
148,052
(11)
SBayesR
0.6340
−
0.5491
658,693
Table 6: AUC values for asthma. For lasso and relaxed lasso, the chosen model is based on maximum
AUC on the validation set. Model (3) to (8) each includes Model (2) plus their own speciﬁcation as stated
in the Form column.The validation results for PRS-CS and SBayesR are not available because we used a
combined training and validation set for training.
is diﬃcult to predict. This can also be seen from the AUC value and the ROC curve in Figure 13.
898
That being said, the multivariate lasso still does much better than the baseline model and the
899
strongest univariate model. On the right of Figure 10, we stratify the prediction percentile into 10
900
bins, and compute the overall prevalence within each bin. We observe a clear upward trend that
901
provides further evidence that we manage to capture some genetic signal there.
902
44
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

0
10
20
30
40
50
0.50
0.55
0.60
0.65
0.70
Lambda Index
AUC
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
Lasso (train)
Lasso (val)
ReLasso (train)
ReLasso (val)
13
13
14
17
24
27
35
48
70
96
175
364
806
1815
3519
5936
Figure 9: AUC plot for asthma. The top axis shows the number of active variables in the model.
0
25
50
75
100
Control
Case
Asthma
Prediction percentile
G
G
G
G
G
G
G
G
G
G
0.10
0.12
0.14
0.16
0.18
5
15
25
35
45
55
65
75
85
95
Prediction percentile
Prevalence
Figure 10: Results for asthma based on the best lasso model. Left: box plot of the percentile of the linear
prediction score among cases versus controls. Right: the stratiﬁed prevalence across diﬀerent percentile
bins based on the predicted scores by the optimal lasso.
A.3
High Cholesterol
903
High cholesterol is characterized by high amounts of cholesterol present in the blood and is a risk
904
45
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

Model
Form
AUCtrain
AUCval
AUCtest
Size
(1)
Age + Sex
0.6918
0.6952
0.6883
2
(2)
Age + Sex + 10 PCs
0.6927
0.6959
0.6889
12
(3)
(2) + Single SNP
0.6963
6982
0.6921
13
(4)
(2) + 10K Combined
0.7402
0.6956
0.6880
10,012
(5)
(2) + 100K Combined
0.8518
0.6607
0.6547
100,012
(6)
Sequential LR
0.7540
0.7167
0.7137
1,012
(7)
Lasso
0.7832
0.7259
0.7191
1,371
(8)
Relaxed Lasso
0.7273
0.7220
0.7166
239
(9)
Elastic Net
0.7830
0.7259
0.7190
4,277
(10)
PRS-CS
0.7166
−
0.7027
148,052
(11)
SBayesR
0.7148
−
0.6953
658,693
Table 7: AUC values for high cholesterol.
For lasso and relaxed lasso, the chosen model is based on
maximum AUC on the validation set. Model (3) to (8) each includes Model (2) plus their own speciﬁcation
as stated in the Form column.The validation results for PRS-CS and SBayesR are not available because we
used a combined training and validation set for training.
factor for cardiovascular disease.
It is highly heritable and may be polygenic.
Our results are
905
summarized in Table 7. The AUC curves for the lasso and the relaxed lasso are shown in Figure 11.
906
Similarly the ROC curve for the best lasso model is shown in Figure 13, and box plots for the
907
two groups and a stratiﬁed prevalence plot are shown in Figure 12. We see that the distributions
908
of predictions made on non-HC individuals and on HC individuals are clearly diﬀerent from each
909
other, suggesting good classiﬁcation results. That is reﬂected in the AUC measure listed in the
910
table. Nevertheless, it is not much better than the result of the base model including only covariates
911
age and sex.
912
B
Manhattan Plots
913
The Manhattan plots in Figure 14 (generated using the qqman package (Turner, 2018)) show the
914
magnitude of the univariate p-values and the size of the lasso coeﬃcients for each gene for the
915
two quantitative traits and two binary traits. The coeﬃcients are plotted for the model with the
916
optimal R2 value on the validation set. The variants highlighted in green in both plots are those
917
that have coeﬃcient magnitudes above the 99th percentile of all coeﬃcient magnitudes for the trait.
918
The horizontal line in the p-value plot is plotted at the genome-wide Bonferroni corrected p-value
919
46
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

0
10
20
30
40
50
0.68
0.70
0.72
0.74
0.76
0.78
0.80
Lambda Index
AUC
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
Lasso (train)
Lasso (val)
ReLasso (train)
ReLasso (val)
14
14
18
24
32
33
38
48
72
125
216
490
1125
2353
4291
6993
Figure 11: AUC plot for high cholesterol. The top axis shows the number of active variables in the model.
GGGGGGGG
0
25
50
75
100
Control
Case
High cholesterol
Prediction percentile
G
G
G
G
G
G
G
G
G
G
0.0
0.1
0.2
0.3
5
15
25
35
45
55
65
75
85
95
Prediction percentile
Prevalence
Figure 12: Results for high cholesterol based on the best lasso model. Left: box plot of the percentile
of the linear prediction score among cases versus controls. Right: the stratiﬁed prevalence across diﬀerent
percentile bins based on the predicted scores by the optimal lasso.
threshold 5 × 10−8. There are two main points we would like to highlight:
920
• The lasso manages to capture signiﬁcant univariate predictors in each genetic region. Due
921
to possible correlation it does not pick up the variants with similarly small p-values located
922
47
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
False Positive Rate
True Positive Rate
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
False Positive Rate
True Positive Rate
Figure 13: ROC curves. Left: asthma. Right: high cholesterol.
nearby.
923
• Some of the variants with weak univariate signals are also identiﬁed and turn out to be crucial
924
to the predictive performance of the lasso.
925
For the two qualitative traits plotted in Figure 15, there are fewer p-values above the threshold,
926
and many of the signiﬁcant ones are located close to each other. The size of the lasso ﬁt is corre-
927
spondingly smaller, and the large coeﬃcients pick up the important locations as before. However,
928
the nonzero coeﬃcients are still spread across the whole genome.
929
48
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

(a)
(b)
(c)
(d)
Figure 14: Manhattan plots of the univariate p-values and lasso coeﬃcients for height (a, c) and BMI
(b, d). The vertical axis of the p-value plots shows −log10(p) for each SNP, while the vertical axis of the
coeﬃcient plots shows the magnitude of the coeﬃcients from snpnet. The SNPs with relatively large lasso
coeﬃcients are highlighted in green. The red horizontal line on the p-value plot represents a reference level
of p = 5 × 10−8.
49
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

(a)
(b)
(c)
(d)
Figure 15: Manhattan plots of the univariate p-values and lasso coeﬃcients for asthma (a, c) and high
cholesterol (b, d). The vertical axis of the p-value plots shows −log10(p) for each SNP, while the vertical
axis of the coeﬃcient plots shows the magnitude of the coeﬃcients from snpnet. The SNPs with relatively
large lasso coeﬃcients are highlighted in green. The red horizontal line on the p-value plot represents a
reference level of p = 5 × 10−8.
50
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted May 31, 2020. 
; 
https://doi.org/10.1101/630079
doi: 
bioRxiv preprint 

