
Fundamentals in Information Theory and Coding

Monica Borda
Fundamentals in
Information Theory and
Coding
ABC

Author
Prof. Dr. Eng. Monica Borda
Technical University of Cluj-Napoca
Dept. Communications
Str. Baritiu 26-28
400027 Cluj Napoca
Romania
Telephone: 0040-264401575
E-mail: Monica.Borda@com.utcluj.ro
ISBN 978-3-642-20346-6
e-ISBN 978-3-642-20347-3
DOI 10.1007/978-3-642-20347-3
Library of Congress Control Number: 2011925863
c⃝2011 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the mate-
rial is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlm or in any other way, and storage in data banks. Dupli-
cation of this publication or parts thereof is permitted only under the provisions of the German
Copyright Law of September 9, 1965, in its current version, and permission for use must always
be obtained from Springer. Violations are liable to prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does
not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
Typeset & Cover Design: Scientiﬁc Publishing Services Pvt. Ltd., Chennai, India.
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To my family 
 

Preface 
Motto:  We need to develop  
thinking, rather than  
too much knowledge. 
Democritus 
This book represents my 30 years continuing education courses for graduate and 
master degree students at the Electronics and Telecommunications Faculty from 
the Technical University of Cluj Napoca, Romania and partially my research ac-
tivity too. The presented topics are useful for engineers, M.Sc. and PhD students 
who need basics in information theory and coding.  
The work, organized in five Chapters and four Appendices, presents the fun-
damentals of Information Theory and Coding. 
Chapter 1 (Information Transmission Systems - ITS) is the introductory part and 
deals with terminology and definition of an ITS in its general sense (telecommuni-
cation or storage system) and its role. 
Chapter 2 (Statistical and Informational Model of an ITS) deals with the mathe-
matical and informational modeling of the main components of a digital ITS: the 
source (destination) and the transmission channel (storage medium). Both  
memoryless and memory (Markov) sources are analyzed and illustrated with ap-
plications. 
Chapter 3 (Source Coding) treats information representation codes (from the nu-
meral system to the genetic code), lossless and lossy (DPCM and Delta) compres-
sion algorithms. The main efficiency compression parameters are defined and a 
detailed presentation, illustrated with many examples, of the most important com-
pression algorithms is provided, starting with the classical Shannon-Fano or 
Huffman until the modern Lempel Ziv or arithmetic type. 
Chapter 4 (Cryptography Basics) is presenting basics of classic and modern sym-
metric and public key cryptography. Introduction in DNA cryptography and in 
digital watermarking are ending the chapter. Examples are illustrating all the pre-
sented chipers.  
Chapter 5 (Channel Coding) is the most extended part of the work dealing with  
error control coding: error detecting and forward error correcting codes. After de-
fining the aim of channel coding and ways to reach it as established by Shannon 
second theorem, the elements of the theory of block codes are given and Hamming 
group codes are presented in detail. 

VIII
Preface
 
Cyclic codes are a main part of Chapter 5. From this class, a detailed presenta-
tion of BCH, Reed-Solomon, Golay and Fire codes is given, with linear feedback 
shift register implementation. The algebraic decoding algorithms, Peterson and 
Berlekamp, are presented. 
Concerning the convolutional codes, after a short comparison with block codes, 
a detailed description of encoding and graphical representation as well as decod-
ing algorithms are given.  
Principles of interleaving and concatenation are also presented and exemplified 
with the CIRC standard used in audio CD error control. 
The principles of the modern and powerful Turbo Codes are ending the presen-
tation of error control codes. Channel Coding chapter also includes a presentation 
of Base-Band coding.  
The work also includes four Appendices (A, B, C and D) presenting: A – Alge-
bra elements and tables concerning some Galois fields and generator polynomials 
of BCH and RS codes; B – Tables for information and entropy computing; C – 
Signal detection elements and D – Synthesis example.  
I tried to reduce as much as possible the mathematical demonstrations, focusing 
on the conditions of theorems validity and on their interpretation. The examples 
were selected to be as simple as possible, but pointing out the essential aspects of 
the processing. Some of them are classic, others are taken from the literature, be-
ing currently standards in many real applications, but most of them are original 
and are based on typical examples taken from my lectures.  
The understanding of the phenomenon, of the aims of processing (compression, 
encryption and error control) in its generality, not necessarily linked to a specific 
application, the criteria of selecting a solution, the development of the “technical 
good sense”, is the logic thread guiding the whole work. 
 
 

Acknowledgements 
I want to thank to all those who directly or indirectly, contributed to my training as 
an engineer and professor. 
Special thanks are addressed to Professor Alexandru Spataru and to the school 
he created. He was my teacher in Theory of Information Transmission, during the 
time I was student at Polytechnic Institute of Bucharest.  
For the interest shown towards the modern algebraic decoding algorithms, for 
the simplicity and practical way in which he presented the abstract mathematics, I 
thank professor mathematician Nicolae Ghircoiasu, now pastaway. 
I am highly grateful to Dr. Graham Wade, from the University of Plymouth, 
UK, for the modern and practical orientation he gave to my whole course, for the 
valuable discussions and fruitful collaboration we had since 1993. His books im-
pressed me a lot, due to the practical features that he gave to a domain usually pre-
sented theoretically. 
Special thanks are given to my colleagues, professor Romulus Terebeú, teach-
ing assistant Raul Malutan and researcher Bogdan Belean for their observations 
and suggestions, and to my PhD student Olga Tornea for her contributions to 
DNA cryptography. 
I kindly acknowledge to all students who helped me in typewriting the book.  
Chapter 3 and 4 were partially supported by the Romanian National Research 
Council CNCSIS-UEFISCSU project no. PNII ID_909/2007.  
I would like to thank Dr. Christoph Baumann, Senior Editor Engineering at 
Springer (Germany), who motivated me in preparing the book. Finally, but not 
least, I thank Carmen Wolf, Editorial Assistant at Springer (Germany) and Ganesan 
Prabu from Scientific Publishing Services (India) for their wonderful help in the 
preparation and publication of the manuscript. 
 

Contents 
1   Information Transmission Systems.................................................................1 
     1.1   Terminology ...............................................................................................1 
     1.2   Role of an ITS.............................................................................................2 
     1.3   Model of an ITS..........................................................................................3 
References ..............................................................................................................5 
 
2   Statistical and Informational Model of an ITS ..............................................7 
     2.1   Memoryless Information Sources...............................................................7 
     2.2   Measure of Discrete Information................................................................8 
     2.3   Information Entropy for a DMS (Shannon Entropy) ................................11 
     2.4   Source Redundancy and Efficiency..........................................................13 
     2.5   Entropy of an Extended Discrete Memoryless Source: H(Xn).................14 
     2.6   Moments and Moment Rate......................................................................14 
     2.7   Information Rate, Decision Rate...............................................................15 
     2.8   Discrete Transmission Channels...............................................................16 
       2.8.1   Probabilities and Entropies in Discrete Channels ..........................16 
       2.8.2   Mutual Information and Transinformation.....................................21 
       2.8.3   Relationships between Entropies ...................................................21 
       2.8.4   Channel Capacity Using the Noise Matrix.....................................23 
       2.8.5   Shannon Capacity...........................................................................30 
     2.9   Memory Sources (Markov Sources) .........................................................38 
       2.9.1   Finite and Homogeneous Markov Chains......................................39 
       2.9.2   Entropy of m-th Order Markov Source ..........................................43 
       2.9.3   Applications ...................................................................................46 
References ............................................................................................................51 
 
3   Source Coding.................................................................................................53 
     3.1   What Is Coding and Why Is It Necessary? ...............................................53 
     3.2   Aim of Source Coding..............................................................................55 
     3.3   Information Representation Codes ...........................................................55 
       3.3.1   Short History..................................................................................55 
       3.3.2   Numeral Systems ...........................................................................56 
       3.3.3   Binary Codes Used in Data Transmission, Storage or  
                  Computing......................................................................................58 
 

XII 
Contents 
 
       3.3.4   Pulse Code Modulation (PCM)......................................................65 
       3.3.5   Genetic Code..................................................................................76 
     3.4   Coding Efficiency: Compression Ratio ....................................................80 
     3.5   Existence Theorem for Instantaneous and Uniquely Decodable Codes  
             (Kraft and McMillan Inequalities)............................................................82 
     3.6   Shannon First Theorem (1948).................................................................84 
     3.7   Losseless Compression Algorithms..........................................................85 
       3.7.1   Shannon-Fano Binary Coding........................................................85 
       3.7.2   Huffman Algorithms ......................................................................88 
       3.7.3   Run Length Coding (RLC).............................................................95 
       3.7.4   Comma Coding ............................................................................100 
       3.7.5   Dictionary Techniques [41]..........................................................101 
       3.7.6   Lempel-Ziv Type Algorithms (LZ)..............................................102 
       3.7.7   Arithmetic Coding........................................................................107 
     3.8   Lossy Compression in Differential Coding ............................................109 
       3.8.1   Differential Pulse Code Modulation (DPCM)..............................109 
       3.8.2   Delta Modulation (DM) ...............................................................111 
       3.8.3   Delta-Sigma Modulation..............................................................115 
       3.8.4   Comparison and Applications of Digital Modulations.................116 
     3.9   Conclusions on Compression..................................................................116 
References ..........................................................................................................118 
 
4   Cryptography Basics ....................................................................................121 
     4.1   Short History of Cryptography ...............................................................121 
     4.2   Terminology ...........................................................................................123 
     4.3   Cryptosystems: Role and Classification .................................................123 
     4.4   Cryptanalytic Attacks and Algorithms Security .....................................125 
     4.5   Classic Cryptography..............................................................................127 
       4.5.1   Definition and Classification........................................................127 
       4.5.2   Caesar Cipher...............................................................................128 
       4.5.3   Polybius Cipher............................................................................130 
       4.5.4   Playfair Cipher .............................................................................131 
       4.5.5   Trithemius Cipher ........................................................................132 
       4.5.6   Vigénère Cipher ...........................................................................134 
     4.6   Modern Symmetric (Conventional) Cryptography.................................135 
       4.6.1   Definitions and Classification ......................................................135 
       4.6.2   Block Ciphers...............................................................................137 
                  4.6.2.1   Main Features.................................................................137 
                  4.6.2.2   DES (Data Encryption Standard) ...................................139 
                  4.6.2.3   Some Other Block Ciphers.............................................147 
                  4.6.2.4   Block Cipher Operation Modes......................................150 
       4.6.3   Stream Ciphers.............................................................................153 
                  4.6.3.1   General Features.............................................................153 
                  4.6.3.2   Some Stream Ciphers.....................................................157 
       4.6.4   Authentication with Symmetric Cryptography ............................158 
 

Contents 
XIII
 
     4.7   Public Key Cryptography .......................................................................159 
       4.7.1   Principle of the Public Key Cryptography ...................................159 
       4.7.2   Public Keys Ciphers.....................................................................162 
     4.8   Digital Watermarking .............................................................................164 
       4.8.1   Introduction..................................................................................164 
       4.8.2   Short History................................................................................167 
       4.8.3   Watermarking Requirements........................................................168 
       4.8.4   Basic Principles of Watermarking................................................169 
       4.8.5   Specific Attacks ...........................................................................177 
                  4.8.5.1   Attack Definition and Classification ..............................177 
                  4.8.5.2   The Inversion Attack / IBM / Confusion /  
                                Deadlock / Fake Watermark / Fake Original..................178 
                  4.8.5.3   The Collusion Attack .....................................................180 
       4.8.6   Applications .................................................................................181 
                  4.8.6.1   Broadcast Monitoring.....................................................181 
                  4.8.6.2   Owner Identification: Proof of Ownership.....................182 
                  4.8.6.3   Fingerprinting (Transaction Tracking)...........................183 
                  4.8.6.4   Content Authentication (Fragile Watermarking)............183 
                  4.8.6.5   Copy Control..................................................................184 
       4.8.7   The Millennium Watermark System [53].....................................184 
                  4.8.7.1   Introduction....................................................................184 
                  4.8.7.2   Basic Principle of the Millennium System.....................185 
                  4.8.7.3   Millennium Standard Implementation............................188 
                  4.8.7.4   Unsolved Problems ........................................................188 
                  4.8.7.5   Copy Control [53] ..........................................................189 
                  4.8.7.6   Media Type Recognition................................................189 
       4.8.8   Some Remarks .............................................................................189 
     4.9   DNA Cryptography ................................................................................190 
       4.9.1   Introduction..................................................................................190 
       4.9.2   Backgrounds of Biomolecular Technologies ...............................190 
       4.9.3   Elements of Biomolecular Computation (BMC)..........................194 
                  4.9.3.1   DNA OTP Generation....................................................194 
                  4.9.3.2   Conversion of Binary Data to DNA Format and  
                                Vice Versa......................................................................194 
                  4.9.3.3   DNA Tiles and XOR with DNA Tiles ...........................195 
       4.9.4   DNA Based Steganography and Cryptographic Algorithms........197 
                  4.9.4.1   Steganography Technique Using DNA Hybridization...197 
                  4.9.4.2   Chromosome DNA Indexing Algorithm........................199 
                  4.9.4.3   DNA XOR OTP Using Tiles..........................................202 
References ..........................................................................................................204 
 
5   Channel Coding ............................................................................................209 
     5.1   Shannon Second Theorem (Noisy Channels Coding Theorem) .............209 
     5.2   Error Control Strategies..........................................................................213 
 
 

XIV
Contents 
 
     5.3   Classification of Error Control Codes.....................................................216 
     5.4   Representation of Binary Code Sequences .............................................216 
     5.5   Parameters of Detecting and Error Correcting Codes.............................218 
     5.6   Maximum Likelihood Decoding (MLD) ................................................220 
     5.7   Linear Block Codes ................................................................................224 
       5.7.1   Linear Block Codes: Definition and Matrix Description .............224 
       5.7.2   Error Syndrome............................................................................227 
       5.7.3   Dimensioning of Error Correcting Linear Block Codes...............228 
       5.7.4   Perfect and almost Perfect Codes.................................................228 
       5.7.5   Detection and Correction Capacity: Decoded BER .....................229 
       5.7.6   Relations between the Columns of H Matrix for Error  
                  Detection and Correction .............................................................230 
       5.7.7   Standard Array and Syndrome Decoding.....................................232 
       5.7.8   Comparison between Linear Error Detecting and Correcting  
                  Block Codes .................................................................................235 
       5.7.9   Hamming Group Codes................................................................238 
       5.7.10   Some Other Linear Block Codes................................................253 
     5.8   Cyclic Codes...........................................................................................255 
       5.8.1   Definition and Representation......................................................256 
       5.8.2   Algebraic Encoding of Cyclic Codes ...........................................257 
       5.8.3   Syndrome Calculation and Error Detection .................................265 
       5.8.4   Algebraic Decoding of Cyclic Codes...........................................269 
       5.8.5   Circuits for Cyclic Encoding and Decoding.................................274 
       5.8.6   Cyclic One Error Correcting Hamming Code..............................286 
       5.8.7   Golay Code ..................................................................................290 
       5.8.8   Fire Codes ....................................................................................292 
       5.8.9   Reed–Solomon Codes ..................................................................295 
     5.9   Convolutional Codes ..............................................................................312 
       5.9.1   Representation and Properties......................................................312 
       5.9.2   Convolutional Codes Encoding....................................................314 
       5.9.3   Graphic Representation of Convolutional Codes.........................320 
       5.9.4   Code Distance and d∞................................................................324 
       5.9.5   Decoding (Viterbi Algorithm, Threshold Decoding) ...................326 
     5.10   Code Interleaving and Concatenation...................................................340 
       5.10.1   Interleaving ................................................................................340 
       5.10.2   Concatenated Codes...................................................................342 
     5.11   Turbo Codes..........................................................................................346 
       5.11.1   Definition and Encoding ............................................................346 
       5.11.2   Decoding ....................................................................................348 
       5.11.2.1   Basic Principles.......................................................................348 
       5.11.2.2   Viterbi Algorithm (VA) [46], [48] ..........................................349 
       5.11.2.3   Bidirectional Soft Output Viterbi Algorithm  
                       (SOVA) [46]............................................................................357 
       5.11.2.4   MAP Algorithm ......................................................................364 
       5.11.2.5   MAX-LOG-MAP Algorithm ..................................................372 
 

Contents 
XV
 
       5.11.2.6   LOG-MAP Algorithm.............................................................373 
       5.11.2.7   Comparison of Decoding Algorithms .....................................374 
     5.12   Line (baseband) Codes..........................................................................374 
References ..........................................................................................................385 
 
Appendix A:   Algebra Elements......................................................................389 
     A1   Composition Laws ..................................................................................389 
       A1.1   Compositions Law Elements........................................................389 
       A1.2   Stable Part ....................................................................................389 
       A1.3   Properties......................................................................................389 
     A2   Modular Arithmetic ................................................................................391 
     A3   Algebraic Structures ...............................................................................392 
       A3.1   Group ...........................................................................................392 
       A3.2   Field .............................................................................................393 
       A3.3   Galois Field ..................................................................................393 
                  A.3.3.1   Field Characteristic........................................................394 
                  A.3.3.2   Order of an Element ......................................................395 
     A4   Arithmetics of Binary Fields...................................................................395 
     A5   Construction of Galois Fields GF(2m).....................................................398 
     A6   Basic Properties of Galois Fields, GF(2m) ..............................................402 
     A7   Matrices and Linear Equation Systems...................................................410 
     A8   Vector Spaces .........................................................................................412 
       A8.1   Defining Vector Space .................................................................412 
       A8.2   Linear Dependency and Independency ........................................413 
       A8.3   Vector Space ................................................................................414 
     A9    Table for Primitive Polynomials of Degree k (k max = 100).................417 
     A10   Representative Tables for Galois Fields GF(2k) ...................................418 
     A11   Tables of the Generator Polynomials for BCH Codes ..........................420 
     A12   Table of the Generator Polynomials for RS Codes...............................421 
 
Appendix B:   Tables for Information and Entropy Computing...................427 
     B1   Table for Computing Values of -log2(x), 0.01 ≤ x ≤ 0.99 .......................427 
     B2   Table for Computing Values of -x·log2(x), 0.001 ≤ x ≤ 0.999 ................428 
 
Appendix C:   Signal Detection Elements........................................................435 
     C.1   Detection Problem..................................................................................435 
     C.2   Signal Detection Criteria........................................................................438 
        C.2.1   Bayes Criterion...........................................................................438 
        C.2.2   Minimum Probability of Error Criterion  
                   (Kotelnikov- Siegert) ..................................................................440 
        C.2.3   Maximum a Posteriori Probability Criterion (MAP)..................441 
        C.2.4   Maximum Likelihood Criterion (R. Fisher) ...............................441 
     C.3   Signal Detection in Data Processing (K = 1) .........................................442 
        C.3.1   Discrete Detection of a Unipolar Signal.....................................442 
        C.3.2   Discrete Detection of Polar Signal .............................................446 
 

XVI
Contents 
 
        C.3.3   Continuous Detection of Known Signal.....................................448 
        C.3.4   Continuous Detection of Two Known Signals ...........................453 
References ..........................................................................................................459 
 
Appendix D:   Synthesis Example ....................................................................461 
 
Subject Index......................................................................................................475 
 
Acronyms............................................................................................................483 
 

List of Figures 
1.1   Block scheme of a general digital ITS.............................................................3 
1.2   Illustration of signal regeneration in digital communications: a) original  
signal, b) slightly distorted signal, c) distorted signal, d) intense distorted  
signal, e) regenerated signal (l - distance in transmission) ..............................4 
2.1   Graphical representation of the entropy corresponding to a binary  
source.............................................................................................................13 
2.2   Discrete information sources: a) unipolar binary source (m=2), b) polar  
binary source (m=2), c) quaternary source (m=4) .........................................15 
2.3   Discrete channel: a) graph representation, b) matrix representation..............17 
2.4   Graphical representation of the relationships between entropies: a) ordinary 
channel, b) noiseless channel, c) independent channel..................................23 
2.5   The graph corresponding to a binary erasure channel ...................................26 
2.6   Graphical representation of a binary transmission system.............................27 
2.7   Illustration of time and amplitude resolutions ...............................................31 
2.8   Graphical representation of channel capacity ................................................33 
2.9   Bandwidth - capacity dependency .................................................................34 
2.10   Bandwidth-efficiency diagram ....................................................................35 
2.11   Graphical representation of the relations between the information  
source and the channel.................................................................................36 
2.12   The graph corresponding to a Markov chain with two states ......................40 
2.13   Transition from si to sj in two steps.............................................................41 
2.14   Absorbent Markov chain .............................................................................42 
2.15   Graph corresponding to a 2-step memory binary source .............................45 
2.16   Differential Pulse Code Modulation for a black-and-white video signal.....49 
2.17   Example of burst..........................................................................................50 
2.18   The simplified model of a memory channel ................................................50 
3.1   Illustration of the transformation SåX realized through encoding...............54 
3.2   Coding tree associated to code D from Table 3.1..........................................55 
3.3   Block scheme illustrating the generation of PCM signal (PCM  
modulator and coder).....................................................................................65 
3.4   Block scheme illustrating the receiving PCM process (PCM demodulator /  
decoder) .........................................................................................................65 
3.5   Example of PCM generation..........................................................................66 
3.6   Representation of quantisation noise pdf.......................................................67 
3.7   Companding illustration ................................................................................71 
3.8   Ideal compression characteristic....................................................................72 
3.9   Threshold effect in PCM systems..................................................................75 

XVIII
List of Figures 
 
3.10   DNA structure..............................................................................................77 
3.11   The coding tree of Example 3.5...................................................................86 
3.12   Evolution of dynamic Huffman FGK tree for the message ‘abcbb’ ............92 
3.13   The effect of one error on the modified Huffman code: a) transmitted  
sequence, b) and c) received sequence affected by one error ......................98 
3.14   Uniform steps coding for black and white images.....................................100 
3.15   Text compression.......................................................................................100 
3.16   The LZ-77 algorithm illustration ...............................................................103 
3.17   Illustration of LZ-77 algorithm..................................................................104 
3.18   Illustration of LZ-78 algorithm..................................................................104 
3.19   Illustration of DPCM principle..................................................................109 
3.20   DPCM codec: a) DPCM encoder; b) DPCM decoder ...............................110 
3.21   Illustration of the DM ................................................................................112 
3.22   Slope-overload noise .................................................................................112 
3.23   Illustration of granular noise......................................................................113 
3.24   Granular noise in DM - pdf representation ................................................114 
3.25   Delta-Sigma modulation - demodulation block scheme ............................115 
3.26   Classification of compression....................................................................116 
4.1   Block-scheme of a cryptosystem where: A, B - entities which transmit,  
receive the information, E - encryption block, D - decryption block,  
M- plaintext, C - ciphertext, K - cryptographic key block,  
e
k - encryption key, 
d
k  - decryption key ...................................................123 
4.2   Illustration of confidentiality .......................................................................124 
4.3   Illustration of authentication........................................................................124 
4.4   Alberti cipher disk (formula) .......................................................................132 
4.5   Example of P box with n=7 .........................................................................138 
4.6   Example of  S box with n = 3: a) block scheme, b) truth table....................138 
4.7   Example of a product cipher (alternation of P and S boxes)........................139 
4.8   Generation of round keys in DES................................................................142 
4.9   DES encryption routine ...............................................................................144 
4.10   DES encryption/decryption function f.......................................................145 
4.11   Illustration of whitening technique: a- encryption, b- decryption..............149 
4.12   ECB mode: a) encryption; b) decryption...................................................150 
4.13   CBC mode : a) encryption, b) decryption..................................................152 
4.14   CFB mode: encryption...............................................................................152 
4.15   OFB mode : encryption .............................................................................153 
4.16   Block scheme of a pseudo-noise generator using LFSR............................154 
4.17   Pseudo-noise generator with LFSR and g(x)=x3+ x2 +1............................155 
4.18   Pseudo noise sequences generator implemented with two LFSRs ............156 
4.19   Nonlinear multiplexed system for generating pseudo-noise sequences.....157 
4.20   Block scheme of a public key system........................................................159 
4.21   Confidentiality in public key cryptosystems..............................................160 
4.22   Authentication in public key cryptosystems ..............................................160 
4.23   Confidentiality and authentication in public key cryptosystems ...............161 
4.24   Watermark principle block scheme: a) insertion (embedding),  
b) retrieval / detection................................................................................166 

List of Figures 
XIX 
 
4.25   Bloc scheme for watermark insertion ........................................................170 
4.26   Bloc scheme for watermark extraction and comparison............................171 
4.27   Line - scanning video signal ......................................................................173 
4.28   Video sequence watermark insertion model ..............................................174 
4.29   Video sequence watermark extraction model ............................................175 
4.30   Millennium Watermark block schemes: a) insertion, b) detection ............186 
4.31   Hybridization process ................................................................................191 
4.32   Gene representation ...................................................................................191 
4.33   Chromosome representation (http://www.ohiohealth.com/)......................192 
4.34   Amplifying process in PCR technique.......................................................192 
4.35   Illustration of DNA recombinant process..................................................193 
4.36   Illustration of microarray experiment ........................................................193 
4.37   Binding process between two ssDNA segments........................................194 
4.38   Triple-helix tile..........................................................................................195 
4.39   Tiles assembling through complementary sticky ends...............................196 
4.40   Tiles for START bits in a string ................................................................196 
4.41   Stacks with tiles for the rest of the bits in a string .....................................196 
4.42   XOR computation with tiles ......................................................................197 
4.43   Cipher text hiding: a) structure of cipher text inserted between two  
primers; b) dsDNA containing (hidden) the cipher text.............................198 
4.44   Illustration of OTP scanning process for message encryption...................200 
4.45   Design of the One Time Pad tiles ..............................................................203 
4.46   Example of message tiles binding..............................................................204 
4.47   Design of the one-time-pad tiles................................................................204 
5.1   Error exponent graphic ................................................................................210 
5.2   Input/output representation of a BSC obtained for C2.................................211 
5.3   Illustration of the relations between information bits and coded ones.........212 
5.4   Location of channel coding block in a complete transmission (storage)  
system..........................................................................................................213 
5.5   ARQ systems for N = 5: a) SW(Stop and Wait); b) GBN(go back N);  
c) SR(selective repeat).................................................................................215 
5.6   Minimum distance decoder principle (d=5).................................................222 
5.7   Coding gain representation..........................................................................238 
5.8   Shortened Hamming code for 16 bits memory protection...........................244 
5.9   Hamming (7,4) code block scheme: a) encoding unit; b) decoding unit .....247 
5.10   HDLC frame format ..................................................................................268 
5.11   LFSR with external modulo 2 adders ........................................................274 
5.12   LFSR with internal modulo 2 adders.........................................................276 
5.13   Cyclic systematic encoder with LFSR and external modulo two adders...277 
5.14   Cyclic systematic encoder with LFSR and internal modulo two adders....279 
5.15   Cyclic encoder with LFSR and external ⊕: a) block scheme;  
b) operation table for g(x)= x3 + x + 1 and  m = 4.....................................280 
5.16   Cyclic encoder with LFSR and internal ⊕: a) block scheme;  
b) operation table for g(x)= x3 + x + 1 and  m = 4.....................................281 
5.17   Error detection cyclic decoder with LFSR and external ⊕......................282 
5.18   Error detection cyclic decoder with LFSR and internal ⊕.......................283 

XX 
List of Figures 
 
5.19   a) Block scheme and b) the operating table of the cyclic decoder for  
g(x) = x3 + x + 1.........................................................................................284 
5.20   a) Block scheme and b) the operation table of a cyclic error detection  
decoder with LFSR and internal ⊕..........................................................285 
5.21   General block scheme of an error correction cyclic decoder.....................286 
5.22   a) SR block scheme and b) operation of the cyclic decoder from  
Fig. 5.21, for the cyclic Hamming code (7,4) with g(x) = x3 + x + 1;  
LFSR with external ⊕..............................................................................289 
5.23   a) SR block scheme and b) operation of the cyclic decoder from  
Fig. 5.21, for the cyclic Hamming code (7,4) with g(x) = x3 + x + 1;  
LFSR with internal ⊕...............................................................................290 
5.24   Block schemes for Fire code with g(x) = x10 + x7 + x2 + 1 and p = 3:  
a) encoder and b) decoder..........................................................................294 
5.25   Flow-chart corresponding to Berlekamp-Massey algorithm......................307 
5.26   Summating circuit over GF(2k)..................................................................309 
5.27   Multiplying circuit with α over GF(24) .....................................................310 
5.28   CD player – block scheme.........................................................................311 
5.29   Comparison between block and convolutional codes................................313 
5.30   Block scheme of: a) systematic and b) non systematic convolutional  
encoder; (ISR - Information Shift Register) ..............................................315 
5.31   Representation of the information at: a) encoder input; b) systematic  
encoder output; c) non-systematic encoder output.....................................316 
5.32   a) Convolutional systematic encoder - block scheme and operation;  
b) Convolutional non-systematic encoder - block scheme and  
operation....................................................................................................319 
5.33   State diagram of the convolutional code with R = 1/2, K = 3 for  
a) systematic and b) non-systematic type ..................................................321 
5.34   The graph corresponding to the systematic convolutional code R = 1/2,  
K = 3; ʊ the encoded structure for i = [0 1 1 0 1].....................................322 
5.35   Trellis corresponding to the convolutional code R = 1/2, K = 3:  
a) systematic with g(x) = 1 + x2; b) systematic with g(x) = 1 + x + x2;  
c) non-systematic with g1(x) = 1 + x2, g(x) = 1 + x + x2............................323 
5.36   Viterbi algorithm example for non-systematic convolutional code with 
R=1/2, K=3, g1(x)=1+x2, g2(x)=1+x+x2, on variable number of frames:  
N=3÷12......................................................................................................328 
5.37   Viterbi decoding process for a three error sequence in the first  
constraint length.........................................................................................332 
5.38   Graphical illustration of the hard decision (with 2 levels) and the  
soft decision (with 8 levels).......................................................................333 
5.39   Block-scheme of a threshold decoding system..........................................334 
5.40   Threshold decoder for the direct orthogonal code R = 1/2,  
g2(x)=1 + x2 + x5 + x6 ................................................................................337 
5.41   Threshold decoder for indirect orthogonal code R = 1/2,  
g2(x)=1 + x3 + x4 + x5 ................................................................................339 
5.42   Interleaving example: a) 4 codewords (of length 5) non-interleaved  
succession; b) interleaved succession (b = burst error length)...................340 

List of Figures 
XXI 
 
5.43   Block interleaving......................................................................................341 
5.44   Block-scheme of a convolutional interleaving made with shift registers  
(C-encoder for independent errors; dC-decoder for independent errors;  
I – interleaver; dI – de-interleaver) ............................................................342 
5.45   Block scheme of a concatenated system....................................................342 
5.46   Block-scheme of an interleaved concatenated system  
(C2 - RS + C1 - convolutional)...................................................................343 
5.47   Block scheme of CIRC system..................................................................344 
5.48   Encoding and interleaving in CIRC system: a) I1 - even samples B2p are 
separated from the odd ones B2p+1 with 2Tf ; b) C2 - RS (28,24) encoding; c) 
I2 - samples are delayed with different time periods to spread the errors; d) 
C1 -RS (32, 28) encoding; e) I3 - even samples (B2p,i) cross  
interleaving with the next frame odd samples (B2p+1,i+1)............................344 
5.49   Illustration of the decoding process in CIRC system.................................345 
5.50   Basic RSC code: R=1/2, g0 - feedback polynomial, g1- feed forward  
polynomial.................................................................................................347 
5.51   Basic turbo transmitter: turbo encoder with R=1/3, BPSK modulation  
and AWGN channel...................................................................................348 
5.52   Basic turbo-decoder...................................................................................349 
5.53   RSC encoder with R = 1/2 and K=2: a) block- scheme; b) state-diagram;  
c) trellis:— i=0 input, i=1 input.................................................................352 
5.54   Trellis representation on Ĳ=4 frames of RSC code with R=1/2  
and K=2 .....................................................................................................360 
5.55   Forward recursion; the thick line is representing the ML path  
(with minimum path metric 0.04)..............................................................362 
5.56   The backward recursion; ML path is presented with thick line.................363 
5.57   MAP decoding illustration: a) block scheme of a transmission system  
with RSC (R = 1/2, K = 3, G = [1, (1 + x2)/ (1 + x + x2)]) and MAP  
decoding; b) state transition diagram of RSC encoder with R = 1/2, K = 3,  
G = [1, (1 + x2)/ (1 + x + x2)]; c) trellis diagram of RSC code from b) .....365 
5.58   Graphical MAP representation: a) forward recursion,  
b) backward recursion................................................................................369 
5.59   Differential a) encoder and b) decoder ......................................................376 
5.60   Signal detection: a) block-scheme and b) illustration of BER...................381 
5.61   Examples of Base Band encoding..............................................................384 
C.1   Block scheme of a transmission system using signal detection. S- source,  
N- noise generator, SD- signal detection block, U- user, si(t) - transmitted 
signal, r(t) - received signal, n(t) - noise voltage, ŝi(t) - estimated  
signal..........................................................................................................435 
C.2   Binary decision splits observation space Δ into two disjoint spaces  
Δ0 and Δ1....................................................................................................436 
C.3   Block scheme of an optimal receiver (operating according to Bayes  
criterion, of minimum risk).........................................................................438 
C.4   Binary detection parameters: Pm- probability of miss, PD- probability  
of detection, Pf - probability of false detection ...........................................439 
C.5   Graphical representation of function Q(y) ..................................................440 

XXII 
List of Figures 
 
C.6   Block-scheme of the optimal receiver for unipolar signal and  
discrete observation ....................................................................................444 
C.7   Graphical representation of pdfs of decision variables for unipolar  
decision and discrete observation ...............................................................445 
C.8   Block Scheme of an optimal receiver with continuous observation  
decision for one known signal s(t): a) correlator based implementation;  
b) matched filter implementation................................................................451 
C.9   Graphical representation of 
)
p(z/s0 and 
)
p(z/s1
........................................452 
C.10   Observation space in dimensions (r1, r2) ...................................................455 
C.11   Block- scheme of an optimal receiver for continuous decision  
with two known signal (correlator based implementation).......................457 
C.12   Representation of the decision process in continuous detection of two 
known signals ...........................................................................................458 
D.1   Block-scheme of processing where: ...........................................................462 
D.2   Block scheme of cyclic encoder with LFSR with external modulo  
two sumators and g(x) = x3 + x + 1.............................................................468 
D.3   Non-systematic convolutional encoder for R=1/2 and  
K=3 ( g(1)(x) = 1 + x + x2, g(2)(x) = 1 + x)...................................................472 

List of Tables 
2.1   Prefixes and multiples for bit and byte ..........................................................10 
2.2   Illustration of time and amplitude resolution requirements ...........................37 
3.1   Binary codes associated to a quaternary source (M=4) .................................54 
3.2   Conversion between hex, dec, oct and binary numeral systems ....................58 
3.3   Tree corresponding to Morse code ................................................................59 
3.4   Baudot Code ..................................................................................................60 
3.5   The 7-bit ISO code (CCITT No 5, ASCII). Command characters:         -  
for national symbols, SP – Space, CR - Carriage Return, LF - Line Feed, 
EOT - End of Transmission, ESC – Escape, DEL - Delete ...........................61 
3.6   IBM BCD code – 6 bits lenght ......................................................................62 
3.7   EBCDIC code................................................................................................63 
3.8   Binary natural and Gray 4 bits length codes representation...........................64 
3.9   Example of 3 bit code in BN and Gray representation ..................................64 
3.10   Genetic code – encoding table.....................................................................78 
3.11   Genetic code – decoding table.....................................................................78 
3.12   Modified Huffman code used in fax ............................................................97 
3.13   LZW algorithm applied to Example 3.14 ..................................................106 
4.1   Classification of classic ciphers...................................................................128 
4.2   Caesar cipher ...............................................................................................129 
4.3   Polybius square............................................................................................130 
4.4   Illustration of Playfair cipher.......................................................................131 
4.5   Tabula recta of Trithemius cipher................................................................133 
4.6   Vigénère encryption with key word.............................................................134 
4.7   Vigénère encryption with trial- key letter and plaintext keyword ...............135 
4.8   Vigénère encryption with trial-key and ciphertext keyword........................135 
4.9   DNA to binary conversion...........................................................................194 
4.10   Truth table of XOR function......................................................................197 
5.1   Standard array for a C(n,m) code.................................................................232 
5.2   Standard array for C(5,3).............................................................................233 
5.3   Syndrome decoding for a linear code C(n,m)..............................................234 
5.5   Syndrome-decoding table for Hamming (7,4) code.....................................248 
5.4   Standard array for the Hamming (7,4) code ................................................249 
5.6   Encoding table for the cross parity check code............................................254 
5.7   Table of primitive polynomials up to degree 
5
k =
....................................260 
5.8   BCH codes generator polynomials up to 
31
n =
.........................................261 
5.9   GF(24) generated by p(x)= x4+x+1 ..............................................................263 
5.10   Coefficients of the error polynomials for BCH codes ...............................272 

XXIV 
List of Tables
 
5.11   Cyclic encoder with LFSR and external modulo two adders.....................278 
5.12   
t
j  coefficients of the error polynomial for RS codes...............................299 
5.13   Yk coefficients for RS codes (t=1,2)..........................................................300 
5.14   d∞ for the systematic and non-systematic codes: R = 1/2 and K∈[2, 8] ...326 
5.15   The most important direct orthogonal codes [47] for R = 1/2 ...................338 
5.16   The most important indirect orthogonal codes for R = 1/2........................339 
5.17   Example of differential coding ..................................................................376 
A.1   Minimal polynomials for GF(23) and generating polynomial  
1 + X+ X3....................................................................................................407 
A.2   Minimal polynomials for GF(24) and generating polynomial  
1 + X+ X4....................................................................................................407 
A.3   Minimal polynomials for GF(25) and generating polynomial  
1 + X+ X5....................................................................................................408 
D.1   Operation of the encoder from Fig. D.1 for the binary stream i  
(the output of the compression block) ........................................................473 

Chapter 1 
Information Transmission Systems 
Motto: 
When desiring to master  
science, nothing is worst 
than arrogance and more 
necessary than time. 
Zenon 
1.1   Terminology 
We call information “any message that brings a specification in a problem which 
involves a certain degree of uncertainty” [9]. The word information derived from 
the ancient Greek words “eidos” (idea) and “morphe” (shape, form), have thus, the 
meaning of form/shape of the mind. 
Taking this definition into consideration we may say that information is a fun-
damental, abstract notion, as energy in physics. 
Information has sense only when involves two correspondents: one generating 
it (the information source S) and another receiving it (the destination D, or the 
user U). Information can be transmitted at distance or stored (memorized) for later 
reading. The physical medium, including the contained equipment, that achieves 
the remote transmission of the information from S to D, is called transmission 
channel C; in the case of storage systems the channel is replaced by the storage 
medium, e.g. CD, tape etc. 
Information is an abstract notion. This is why, when stored or transmitted, it 
must be embedded into a physical form (current, voltage, electromagnetic wave) 
able to propagate through the channel or to be stored. What we call signal is pre-
cisely this physical embodiment carrying information. 
 
Remark 
Generally speaking, by signal we understand any physical phenomenon able to 
propagate itself through a medium. One should notice that this definition is restric-
tive: it rules out the signal that interferes with the information-carrying signal 
(useful signal); this signal is known as noise or perturbation (N). 
The information source can be discrete (digital source), or continuous (signal 
source). The discrete source generates a finite number of symbols (e.g. 0 and 1 
used in digital communications) while the continuous source, an infinite number 
of symbols (e.g. voice, television signal, measurement and control signals). 

2 
1   Information Transmission Systems
 
Remark 
The sources, as well the destinations, are supposed to have transducers included. 
By Information Transmission System (ITS) we will understand the ensemble of in-
terdependent elements (blocks) that are used to transfer the information from 
source to destination. 
 
Remarks 
• When transmitting the information from source to a remote destination through a 
channel, we deal with a transmission system; on the other hand, when storing the 
information, we deal with a storage system. The problems met in information 
processing for storage are similar in many aspects to those from transmission sys-
tems; therefore in the present work the term information transmission system 
(ITS) will be used for the general case (transmission as well storage system). 
• The signal, as well as the noise, are assumed to be random. 
1.2   Role of an ITS 
The role of an ITS is to ensure a high degree of fidelity for the information at destina-
tion, regardless to the imperfections and interferences occurring in the channel or stor-
age medium. The accuracy degree is estimated using a fidelity criterion, as follows: 
For analogue systems: 
 
• mean squared error ε: 
[
]2
y(t)
x(t)
:
ε
−
=
                                               (1.1) 
where x(t), y(t) are the signals generated by the source respectively received at 
destination;  the symbol “______” indicates the time averaging. 
• signal/noise ratio (SNR) ξ:  
[
]
[
]2
2
n(t)
y(t)
:
ξ =
                                                 (1.2) 
where n(t) indicates the noise. 
 
 
For digital systems: 
• bit error rate (BER): the probability of receiving an erroneous  bit 
The degree of signal processing for transmission or storage, depends on the 
source, destination, channel (storage medium), the required accuracy degree, and 
the system cost. 
When the source and destination are human beings, the processing may be re-
duced due to the physiological thresholds (hearing and vision), and also to the 
human brain processing, requiring a lower degree of fidelity. 
When dealing with data transmissions (machines as source and destination), the 
complexity of processing increases in order to achieve the required fidelity. 
For high quality data transmission/storage we may as well improve the channel 
(storage medium), the choice of the used method being made after comparing the 

1.3   Model of an ITS 
3
 
price of the receiver (the equipment used for processing) with the price of the  
channel (storage medium). The constant decrease of the LSI and VLSI circuits 
prices justifies the more and more increasing complexity of the terminal equipment, 
the ultimate purpose being the achievement of high quality transmission/storage. 
 
Remark 
In what follows we will exclusively analyze the numerical (digital) transmission 
systems taking into consideration their present evolution and the future perspec-
tives that show their absolute supremacy even for applications in which the source 
and the destination are analogue (e.g. digital television and telephony). 
1.3   Model of an ITS 
The general block scheme of an ITS is presented in Fig. 1.1 which shows the general 
processing involving information: coding, modulation, synchronization, detection. 
 
Fig. 1.1 Block scheme of a general digital ITS 
Legend: 
• S/D – source/destination;  
• CS/ DCS – source encoding/decoding blocks; 
• E/D – source encryption/decryption blocks; 

4 
1   Information Transmission Systems
 
• CC/DCC – channel encoding/decoding blocks; 
• i – information (signal) 
• v – encoded word 
•  n – noise 
•  r – received signal 
•  î – estimated information. 
The process named coding stands for both encoding and decoding and is used 
to achieve the followings: 
• matching the source to the channel/storage medium (if different as nature), us-
ing the source encoding block (CS) 
• ensuring efficiency in transmission/storage, which means minimum transmis-
sion time/minimal storage space, all these defining source compression (CS) 
• reliable transmission/storage despite channel/storage medium noise (error pro-
tection performed by the channel coding block  CC) 
• preserving information secrecy from unauthorized users, using the source en-
cryption/decryption blocks (E/D) 
Modulation is used to ensure propagation, to perform multiple access and to 
enhance the SNR (for angle modulation), as well as to achieve bandwidth com-
pression [8], [25].  
For digital systems, synchronization between transmitter and receiver is neces-
sary, and also signal detection, meaning that the receiver must decide, using the 
received signal, which of the digital signals has been sent [10]. 
In real applications, all the above-mentioned processes or only some of them 
could appear, depending on the processing degree required by the application. 
 
Why digital? 
 
There are several reasons why digital systems are widely used. Their main advan-
tage is high noise immunity, explained by signal regeneration: a digital signal, 
having only two levels corresponding to “0” and “1”, allows an easy regeneration 
of the original signal, even from a badly damaged signal (fig.1.2), without accu-
mulation of regenerative errors in transmission (in contrast to analogue transmis-
sion) [5], [18]. 
 
Fig. 1.2 Illustration of signal regeneration in digital communications: a) original signal, b) 
slightly distorted signal, c) distorted signal, d) intense distorted signal, e) regenerated signal 
(l - distance in transmission). 

References 
5
 
In analogue systems, the distortions, however small, cannot be eliminated by 
amplifiers (repeaters), the noise accumulating during transmission; therefore in 
order to ensure the required fidelity for a specific application, we must use a high 
SNR, unlike for digital systems in which (taking into account the possibility of er-
ror protection) we may use a very low SNR (lower than 10 dB, near Shannon limit 
[2]). 
Other advantages of digital systems are: 
 
• possibility of more flexible implementation using LSI and VLSI technologies 
• reliability and lower price than for analogue systems 
• identical analysis in transmission and switching for different information 
sources: data, telegraph, telephone, television, measurement and control signals 
(the principle of ISDN – Integrated Switching Digital Network) 
• good interference and jamming protection and also the possibility of ensuring 
information confidentiality. 
The main disadvantage of digital systems is the increased bandwidth compared 
to analogue ones. This disadvantage can be diminished through compression as 
well as through modulations, for spectrum compression.  
References 
[1] Angheloiu, I.: Teoria Codurilor. Editura Militara, Bucuresti (1972) 
[2] Berrou, C., Glavieux, A.: Near Shannon limit error-correcting coding and decoding 
turbo-codes. In: Proc. ICC 1993, Geneva, pp. 1064–1070 (1993) 
[3] Borda, M.: Teoria Transmiterii Informatiei. Editura Dacia, Cluj-Napoca (1999) 
[4] Borda, M.: Information Theory and Coding. U.T. Pres, Cluj-Napoca (2007) 
[5] Fontolliet, P.G.: Systhèmes de télècommunications. Editions Georgi, Lausanne 
(1983) 
[6] Gallager, R.G.: Information Theory and Reliable Communication. John Wiley & 
Sons, Chichester (1968) 
[7] Hamming, R.: Coding and Information Theory. Prentice-Hall, Englewood Cliffs 
(1980) 
[8] Haykin, S.: Communication Systems, 4th edn. John Wiley & Sons, Chichester (2001) 
[9] Ionescu, D.: Codificare si coduri. Editura Tehnica, Bucuresti (1981) 
[10] Kay, S.M.: Fundamentals of Statistical Signal Processing. In: Detection Theory, 
vol. II. Prentice-Hall, Englewood Cliffs (1998) 
[11] Lin, S., Costello, D.: Error Control Coding. Prentice-Hall, Englewood Cliffs (1983) 
[12] Mateescu, A., Banica, I., et al.: Manualul inginerului electronist, Transmisii de date. 
Editura Tehnica, Bucuresti (1983) 
[13] McEliece, R.J.: The Theory of Information and Coding, 2nd edn. Cambridge Univer-
sity Press, Cambridge (2002) 
[14] Murgan, A.: Principiile teoriei informatiei in ingineria informatiei si a comunicatiilor. 
Editura Academiei, Bucuresti (1998) 
[15] Peterson, W.W., Weldon, E.J.: Error-Correcting Codes, 2nd edn. MIT Press, Cam-
bridge (1972) 
[16] Proakis, J.: Digital Communications, 4th edn. Mc Gran-Hill (2001) 

6 
1   Information Transmission Systems
 
[17] Shannon, C.E.: A Mathematical Theory Of Communication. Bell System Technical 
Journal 27, 379–423 (1948); Reprinted in Shannon Collected Papers, IEEE Press 
(1993) 
[18] Sklar, B.: Digital Communications, 2nd edn. Prentice-Hall, Englewood Cliffs (2001) 
[19] Spataru, A.: Teoria transmisiunii informatiei. Editura Didactica si Pedagogica, Bu-
curesti (1983) 
[20] Spataru, A.: Fondements de la theorie de la transmission de l’information. Presses 
Polytechniques Romandes, Lausanne (1987) 
[21] Tomasi, W.: Advanced Electronic Communications. Prentice-Hall, Englewood Cliffs 
(1992) 
[22] Wade, G.: Signal Coding and Processing. Cambridge University Press, Cambridge 
(1994) 
[23] Wade, G.: Coding Techniques. Palgrave (2000) 
[24] Wozencraft, J.W., Jacobs, I.M.: Principles of Communication Engineering. Waveland 
Press, Prospect Heights (1990) 
[25] Xiong, F.: Digital Modulation Techniques. Artech House, Boston (2000) 

Chapter 2 
Statistical and Informational Model of an ITS 
Motto: 
Measure is the  
supreme well.  
(from the  wisdom of the peoples) 
2.1   Memoryless Information Sources 
Let us consider a discrete information source that generates a number of m dis-
tinct symbols (messages). The set of all distinct symbols generated by the source 
forms the source alphabet. 
A discrete source is called memoryless (discrete memoryless source DMS) if 
the emission of a symbol does not depend on the previous transmitted symbols. 
The statistical model of a DMS is a discrete random variable (r.v.) X; the val-
ues of this r.v. will be noted as xi, 
m
1,
i =
. By X=xi we understand the emission 
of xi from m possible. 
The m symbols of a DMS constitute a complete system of events, hence: 
j
i
Φ,
x
x
and
Ω
x
j
i
m
1
i
i
≠
∀
=
∩
=
=
∪
                              (2.1)  
In the previous formula, Ω signifies the certain event or the sample space and Φ 
the impossible event. 
Consider p(xi) = pi the emission probability of the symbol xi. All these prob-
abilities can be included in the emission probability matrix P(X): 
[
]
∑
=
=
=
m
1
i
i
m
i
1
1
p
 
 where
,
p
p
p
P(X)
"
"
                             (2.2) 
For a memoryless source we have: 
)
p(x
)
x
,
/x
p(x
i
2
i
1
i
i
=
−
−
…
                                    (2.3) 
For the r.v. X, which represents the statistical model for a DMS, we have the 
probability mass function (PMF) of r.v. X: 
∑
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
m
1
i
i
i
i
1
p
 ,
m
1,
i ,
p
x
:
X
                                    (2.4) 

8 
2   Statistical and Informational Model of an ITS 
 
Starting from a DMS, X, we can obtain a new source, having messages which 
are sequences of n symbols of the initial source X. This new source, Xn  , is called 
the n-th order extension of the source X. 
⎪⎩
⎪⎨
⎧
=
=
∑
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∑
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
=
)
p(x
)
)p(x
p(x
p
x
x
x
m
 
where
1
p
 ,
m
1,
j ,
p
m
:
X
1
p
 ,
m
1,
i ,
p
x
:
X
n
2
1
n
2
1
n
j
j
j
j
j
j
j
j
m
1
j
j
n
j
j
n
m
1
i
i
i
i
…
…
                           (2.5) 
The source Xn contains a number of mn distinct mj messages formed with al-
phabet X. 
 
Example 2.1 
Consider a binary memoryless source X: 
1
p
p ,
p 
p
 x
x
:
X
2
1
2
1
2
1
=
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
The 2nd order extension (n=2) of the binary source X is: 
∑
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
4
1
j
*
*
4
*
3
*
2
*
1
4
3
2
1
2
2
1
2
2
1
2
1
2
2
1
2
2
1
1
1
2
1
p
 , 
p
p
p
p
m
m
m
m
 
p
     
p
p
   
p
p
   
p
x
 x
x
 x
x
 x
x
x
:
X
j
 
2.2   Measure of Discrete Information 
As shown in 1.1, information is conditioned by uncertainty (non-determination). 
Consider the DMS, 
∑
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
m
1
i
i
i
i
1
p
 ,
m
1,
i ,
p
x
:
X
. Prior to the emission of a sym-
bol xi there is an uncertainty regarding its occurrence. After the emission of xi this 
uncertainty disappears, resulting the information about the emitted symbol. Infor-
mation and uncertainty are closely connected, but not identical. They are inversely 
proportional measures, information being a removed uncertainty. It results that the 
information varies oppositely to the uncertainty. 
The uncertainty regarding the occurrence of xi depends on its occurrence prob-
ability: pi, the both measures being connected through a function F(pi) that in-
creases as pi decreases.  
Defining the information i(xi) as a measure of the a priori uncertainty regarding 
the realization of xi, we can write: 
i(xi) = F(pi )                                              (2.6)  
In order to find a formula for the function F we impose that F carries all the 
properties that the information must have: 

2.2   Measure of Discrete Information 
9 
 
• information is positive or at least equal to 0 (
0
F(p) ≥
); we cannot get any in-
formation if the event is certain (p=1) or impossible (p=0) 
• information is additive: consider an event xi composed of two independent events 
xi1 and xi2; we have 
i2
i1
i
x
x
x
∩
=
; based on the additivity of information:  
 i(xi) = i(xi1) + i(xi2)  and therefore 
F[p(xi)] = F[p(xi1)] + F[p(xi2)]                                     (2.7) 
Due to the fact that xi1 and xi2 are independent events, we have: 
 
)
)p(x
p(x
)
x
p(x
i2
i1
i2
i1
=
∩
 
Relation (2.7) becomes: 
)]
F[p(x
)]
F[p(x
)
)p(x
F[p(x
i2
i1
i2
i1
+
=
                          (2.8) 
Taking into account the positivity of information, we obtain for the functional 
equation (2.8) the solution: 
)
i(x
p 
log
-
)
F(p
i
i
i
=
= λ
                                     (2.9) 
where λ is a positive constant. 
The information provided by relation (2.9) is called self information of xi. 
 
Units of information 
 
The unit of information was defined starting from the simplest choice, that of 
choosing one from two equally probable: 
1
2
1
log
-
)
i(x
)
i(x
 ,
1/2
  
1/2
     x
x
:
X
2
1
2
1
=
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
λ
 
If the logarithm is in base 2 and λ=1 we have:  
bit
 1
2
1
log
)
i(x
)
i(x
2
2
1
=
=
=
                                     (2.10) 
The unit previously defined is known as bit. According to this, relation (2.10) 
becomes: 
i
2
i
p
-log
)
i(x
=
                                              (2.11) 
The name of bit comes from the contraction of the words binary digit, made in 
1947 by J.W.Tuckey in a Bell Labs memorial and in 1948, Cl.E.Shannon first 
used the term in his famous paper “A Mathematical Theory of Communications” 
[31]. Apart from log2, other bases have been used at the beginnings of the infor-
mation theory: e and 10. In these cases the units are: 
- ln e
1  = ln e = 1 nit = 1 natural unit (Hartley)  

10 
2   Statistical and Informational Model of an ITS 
 
and represents the choice of 1 from e; this name was given in the memory of R. V. 
Hartley who first introduced the notion of information measure (1928). 
- lg 10
1 = lg10 = 1dit = 1 decimal unit, 
representing the choice of 1 from 10. 
The relations between these three units, using the logarithm base conversion: 
x
log
b
log
x
log
b
a
a
⋅
=
, are: 
 
1 nit = 1.44 bits 
1 dit = 3.32 bits 
 
In computing and telecommunications, another unit for information is com-
monly used: the byte (B), introduced in 1956 by W. Buchholz from IBM. A byte is 
an ordered set of bits, historically being the number of bits (typically 6, 7, 8 or 9), 
used to encode a character text in computer. The modern standard is 8 bits (byte). 
1 byte= 1 B= 8 bits 
Prefixes and multiples for bit and byte are given in Tab 2.1. 
Table 2.1 Prefixes and multiples for bit and byte 
Prefixes for bit and byte multiples 
Decimal 
Binary 
     Value 
        SI 
     Value 
       IEC 
JEDEC 
      1000 
      K  kilo 
      1024 
     Ki kibi 
K kilo 
      10002 
      M mega 
      10242 
     Mi mebi 
M mega 
      10003 
      G giga 
      10243 
     Gi gibi 
G giga 
      10004 
      T tera 
      10244 
     Ti tebi 
 
      10005 
      P peta 
      10245 
     Pi pebi 
 
      10006 
      E exa 
      10246 
     Ei exbi 
 
      10007 
      Z zetta 
      10247 
     Zi zebi 
 
      10008 
      Y yotta 
      10248 
     Yi yobi 
 
 
 
where the designation of the used acronyms is:  
 
SI- International System of Units 
IEC- International Electrotechnical Commission 
JEDEC- Joint Electronic Device Engineering Council (the voice of semicon-
ductor industry) 
 
The binary multiples of bit and byte are linked to 210 =1024, because digital 
systems are based on multiples of power of 2. However its usage was discouraged 
by the major standard organisations and a new prefix system was defined by IEC, 
which defines kibi, mebi, etc., for binary multiples. 

2.3   Information Entropy for a DMS (Shannon Entropy) 
11 
 
2.3   Information Entropy for a DMS (Shannon Entropy) 
The self information corresponding to a symbol xi can be computed using relation 
(2.11) as we have already seen. The average information per emitted symbol is 
called information entropy and is denoted with H(X): 
{
}
∑
=
∑
=
=
=
=
m
1
i
i
2
i
m
1
i
i
i
i
p
log
p
-
)
i(x
p
)
i(x
E
:
H(X)
                        (2.12) 
where E designates the average operator (expected value) of the self information 
of r.v. X. 
The entropy H(X) represents the average uncertainty that a priory exists con-
cerning the emission. 
 
Remarks 
• Equation (2.12), given by Claude E. Shannon in 1948 in his paper “A Mathe-
matical Theory of Communication”, has a perfect analogy with the entropy in 
thermodynamics, established by Boltzmann and J.W.Gibbs in 1870, therefore it 
was called information entropy. 
• Boltzmann-Gibbs formula represents the probabilistic interpretation of the sec-
ond principle of thermodynamics:  
∑
=
=
n
1
i
i
i
p 
log
p
-k
S
                                           (2.13) 
where k=1.38·10-23 J/K is Boltzmann constant, and pi are the system probabilities 
to be in the micro state i taken from an equilibrium ensemble of n possible. The 
thermodynamic entropy expresses the disorder degree of the particles in a physical 
system. 
Shannon formula indicates the system non-determination degree from an in-
formational point of view. 
• A. N. Kolmogorov [12] showed that the mathematical expression of the infor-
mation entropy is identical as form with the entropy in physics, but it would be 
an exaggeration to consider that all the theories from physics related to entropy 
contain, by themselves, elements of the information theory. Both notions have 
in common the fact that they measure the non-determination degree of a sys-
tem, but their applications can be found in completely different spheres of 
knowledge.   
– 
Relation (2.12) is a quantitative expression of the information. Besides the 
quantitative aspect, information has qualitative aspects as well, not empha-
sized in Shannon formula, very important for applications in the area of ar-
tificial intelligence [26]. 
– 
Formula (2.12) was given under some hypotheses: the source is a DMS, so 
from a mathematical point of view we have a complete system of events 
and there is a final equilibrium of states (events). These conditions are 
usually fulfilled in technical systems that have been used as models by 

12 
2   Statistical and Informational Model of an ITS 
 
Shannon, but are not always achieved in the biological, social, economic 
systems; therefore the information theory must be used with extreme care.  
 
Properties of the entropy 
• The entropy is a continuous function with respect to each variable pi because it 
is a sum of continuous functions. 
• Additivity: self information is additive and so it is H(X), which represents the 
mean value of self information. 
• Entropy is maximum when the symbols have equal probabilities; this maximum 
value is also called decision quantity, D(X): 
( )
( )
m
 
log
X
D
:
X
H
2
max
=
=
                                  (2.14) 
• Symmetry: H(X) is unchanged if the events xi are reordered. 
The maximum value of the entropy can be obtained calculating the maximum 
value of the function (2.12) with the constraint: 
∑
=
=
m
1
i
i
1
p
 
Using the Lagrange multipliers method it results: 
⎭
⎬
⎫
⎩
⎨
⎧
∑
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛∑
+
=
=
=
=
m
1
i
m
1
i
i
i
2
i
i
1
-
p
λ
p 
log
 
p
-
)
Φ(p
max 
H(X)
max 
                  (2.15) 
The maximum value of the function F(pi) is achieved for: 
, 
m
1,
i 
 
0,
p
)
Φ(p
i
i
=
∀
=
∂
∂
                                      (2.16) 
⎪
⎪
⎩
⎪
⎪
⎨
⎧
=
+
=
∂
∂
=
+
=
∂
∂
0
λ 
 e 
log
-
p 
-log
p
)
Φ(p
0
λ 
 e 
log
-
p 
-log
p
)
Φ(p
2
j
2
j
j
2
i
2
i
i
 
It follows that 
j
2
i
2
p
log
p
log
=
, hence pi=pj, 
 
m
1,
i 
 
=
∀
. Therefore, Hmax(X) is 
obtained if: 
p1 = p2 =…= pm = m
1   
and so  
Hmax(X) = D(X) = log2 m 
where D(X) is the decision quantity of X. 

2.4   Source Redundancy and Efficiency 
13 
 
This result can be deduced also intuitively; the average uncertainty corresponding 
to the transmission of a symbol is maximum when all the m symbols are equally 
probable, so when it is the hardest to predict which symbol will be transmitted. 
 
Example 2.2 
Compute and draw the entropy of a memoryless binary source: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
p
-
1
   
p
1
     
0
:
X
, 
p)
-
(1
p)log
-
(1
-
p
-plog
H(X)
2
2
=
                    (2.17) 
The graphical representation of the entropy of a binary memoryless source is 
given in Fig. 2.1. 
 
Fig. 2.1 Graphical representation of the entropy corresponding to a binary source  
From Fig 2.1 we can see that H(X)max  = 1 bit/symbol is obtained for p = 1/2. 
For p ≠ ½,  H(X)<1; for p=0 or p=1, H(X)=0, because in these cases the a priori 
uncertainty is zero, being known exactly that 1 will be emitted  in the first situa-
tion  and 0 in the second, meaning that the information in these cases is zero. 
2.4   Source Redundancy and Efficiency 
The source entropy deviation from its maximum value is called source redun-
dancy. This deviation can be absolute or relative, resulting an absolute or relative 
redundancy: 
Absolute redundancy: 
H(X)
-
D(X)
:
R X =
                                         (2.18) 
Relative redundancy: 
D(X)
H(X)
1
D(X)
R(X)
:
ρx
−
=
=
                                       (2.19) 

14 
2   Statistical and Informational Model of an ITS 
 
Source efficiency (ηx) is the ratio between the source entropy and its decision 
quantity: 
D(X)
H(X)
:
ηX =
                                                (2.20) 
2.5   Entropy of an Extended Discrete Memoryless Source: H(Xn) 
We consider the source Xn given by (2.5). 
The entropy of this source, H(Xn), is given by: 
∑
∑
=
∑
∑
=
=
=
=
=
=
=
n
n
n
n
2
i
i
n
2
1
n
n
n
2
1
n
2
1
m
1
i
m
1
i
j
2
j
j
j
j
2
j
j
j
m
1
j
m
1
j
j
j
j
2
j
j
j
j
2
j
n
p
log
p
p
p
-
-
p
log
p
p
p
-
p
p
p
log
p
p
p
-
p
log
p
-
)
H(X
…
…
…
…
…
         (2.21) 
in which:  
∑
∑
∑
=
∑
=
∑
=
∑
∑
∑
=
∑
=
=
=
=
=
=
=
=
=
=
m
1
j
m
1
j
m
1
j
m
1
j
j
2
j
2
j
j
j
j
j
2
m
1
j
j
m
1
j
m
1
j
m
1
j
m
1
j
j
2
j
j
j
j
2
j
1
2
m
i
i
i
i
m
2
1
i
i
i
n
1
2
n
i
n
2
1
i
H(S)
p
log
p
log
p
-
p
p
p
p
log
p
-
p
log
p
p
p
-
p
log
p
-
…
…
…
(2.22) 
Relation (2.21) contains n (2.22) terms, hence: 
nH(X)
)
H(Xn =
                                        (2.23) 
Example 2.3 
The entropy of the second order extension of a memoryless binary source. 
Be the binary source from Example 2.1 and its X2 extension. Applying (2.12) 
for X2 we obtain: 
(
)
(
) (
)
2H(X).
p
log
p
p
log
p
p
p
-2
p
log
p
p
p
log
p
2p
p
log
p
-
)
H(X
2
2
2
1
2
1
2
1
2
2
2
2
2
2
1
2
2
1
2
1
2
2
1
2
=
+
⋅
+
=
=
+
+
=
 
2.6   Moments and Moment Rate 
The signals used for carrying the numerical information are composed of time 
elementary signals, called moments. 
The characteristic parameter of any moment (amplitude, frequency, phase), re-
mains constant during the moment duration (TM) and represents the numerical in-
formation carried by that moment. This parameter can take m values. 
 

2.7   Information Rate, Decision Rate 
15 
 
Fig 2.2 shows some examples of information sources emphasizing the moments. 
 
Fig. 2.2 Discrete information sources: a) unipolar binary source (m=2), b) polar binary 
source (m=2), c) quaternary source (m=4). 
The decision quantity corresponding to a moment is: 
D = ld m                                                   (2.24)  
The moment rate 
•
M  (signalling speed / modulation speed / telegraphic speed) 
represents the number of moments transmitted in each time unit. 
M
T
1
:
M =
•
                                               (2.25) 
The unit for the moment rate is Baud (Bd), named after E. Baudot, the inventor 
of Baudot code for telegraphy.  
Bd
]
M
[
=
•
 
2.7   Information Rate, Decision Rate 
Information rate 
( )
X
H
•
of a source is the average information quantity generated 
by the source in time unit, or the information transmission speed. 
H(X)
M
T
H(X)
:
(X)
H
M
•
•
=
=
                                         (2.26) 

16 
2   Statistical and Informational Model of an ITS 
 
and its unit is:  
( )
d
bits/secon
X
H
=
•
 
Decision rate 
(X)
D
•
of a source, also known, as bit rate, is the decision quantity 
generated by the source in time unit. 
 
m
 
log
M
D(X)
M
T
D(X)
:
(X)
D
2
M
•
•
•
=
=
=
                          (2.27) 
Remarks 
• 
•
M expresses the speed changes of the signal physical parameters. As we will 
see in the 2.8, it is directly linked to the required channel bandwidth: 
•
M ∼B. 
• 
•
•
= M
D
 only for binary sources (m=2). From (2.25) it can be seen that we can 
obtain the same 
•
D  in two cases: working with a higher 
•
M  and a smaller m 
(high speed, reduced alphabet) or inversely. 
2.8   Discrete Transmission Channels 
2.8.1   Probabilities and Entropies in Discrete Channels 
As shown in 1.1 the transmission channel is the medium (including the equip-
ment) used for transmitting information from the source (transmitter) to the desti-
nation (receiver) (Fig 1.1). 
The channel is discrete if the symbols that are passing through it are discrete. 
A transmission channel is characterized by the followings: 
 
• input (transmission) alphabet: X={xi}, is the set of distinct symbols  transmit-
ted  by the source and which are accepted by the channel; P(X) is the transmis-
sion probability matrix. 
( ) [ ]
i
m
1
i
i
i
i
p
X
P
1
p
 ,
m
1,
i ,
p
x
:
X
=
∑
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
                                    (2.28) 
• output(receiving) alphabet: Y={yj}, being the set of all distinct symbols re-
ceived at channel output; one should notice that the two alphabets are not al-
ways identical. 
( ) [ ]
j
n
1
i
j
j
j
q
Y
P
1
q
 ,n
1,
j ,
q
y
:
Y
=
∑
=
=
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
=
                                 (2.29) 

2.8   Discrete Transmission Channels 
17 
 
The probability of receiving the symbol yj is qj=p(yj). P(Y) is the reception 
probability matrix. 
• the transition (noise) matrix of the channel, containing the conditional prob-
abilities: P(Y/X). 
(
) [
] ∑
=
∀
=
=
=
n
1
i
j/i
j,i
m
1,
i
 
1,
q
 , 
q
Y/X
P
                              (2.30) 
The element qj/i situated at the intersection of row i and column j, represents the 
reception probability of receiving yj conditioned by the emission of xi: qj/i = 
p(yj/xi). 
The transition matrix is a stochastic matrix, meaning that the sum of the ele-
ments on any row is 1: 
∑
=
∀
=
=
n
1
j
j/i
m
1,
i
 
1,
q
                                        (2.31) 
which intuitively represents the certainty in receiving a symbol yj, 
n
1,
j =
∀
if  xi 
was emitted, 
m
1,
i =
∀
. 
The matrix P(Y/X) represents the statistical model of the channel and is ob-
tained experimentally. 
Therefore the channel achieves the transition [X]→[Y]: 
P(X)→P(Y/X)→P(Y) 
The graph in Fig 2.3 presents this transition:  
 
Fig. 2.3 Discrete channel: a) graph representation, b) matrix representation 

18 
2   Statistical and Informational Model of an ITS 
 
If we the source is known by P(X) and the channel by its noise matrix P(Y/X), 
the receiving is found as: 
P(Y) = P(X) P(Y/X)                                          (2.32) 
In the previous relations the matrices P(X), P(Y), P(Y/X) have the dimensions 
as given in (2.28), (2.29), (2.30) respectively. 
One should notice that P(Y) depends on the source through P(X) and on the 
channel through P(Y/X). 
Having received the symbol yj and knowing P(X) and P(Y/X), all the a-
posteriori probabilities of the input symbols conditioned by yj may be computed, 
using Bayes formula: 
(
)
j
j/i
i
i
i
j
i
i/j
j
i
q
q
p
y
p
)
/x
)p(y
x
(
p
p
)
/y
p(x
=
=
=
                             (2.33) 
All pi/j probabilities will give the conditional probability matrix 
)
/
(
Y
X
P
 
(
)
(
)
(
)
(
)
(
) (
)
(
)⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
=
n
m
2
m
1
m
n
1
2
1
1
1
i/j
/y
x
p
    
   
/y
x
p
  
/y
x
p
/y
x
p
    
    
/y
x
p
   
/y
x
p
]
[p
X/Y
P
…
#
…
               (2.34) 
Remark 
The matrix P(X/Y) is obtained by calculus, unlike P(Y/X) which is determined 
experimentally and represents the channel model. 
The input (X) and the output (Y) of a channel can be seen as a joint r.v. (XY): 
the joint input–output, described by the joint probability mass function P(XY).  
P(X/Y) can be computed from P(X) and P(Y/X) using the relation: 
(
)
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
⋅
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
=
=
mn
m1
ij
1n
11
n/m
1/m
j/i
n/1
1/1
m
i
1
p
...
p
...
p
...
p
...
p
q
...
q
...
q
...
q
...
q
p
...
0
0
p
0
0
...
p
P(X)P(Y/X)
XY
P
           (2.35) 
Remark 
In (2.35), unlike (2.32), P(X) is written as a diagonal matrix. Multiplying the two 
matrices from (2.35), we obtain: 
(
)
j/i
i
ij
j
i
q
p
p
y
,
x
p
=
=
                                        (2.36) 
Using the joint (XY) PMF, we can calculate the PMF of r.v. X and Y as mar-
ginal probabilities, as shown in Fig. 2.3.b. 
∑
=
∀
=
=
n
1
j
ij
i
m
1,
i 
 ,
p
p
                                       (2.37) 

2.8   Discrete Transmission Channels 
19 
 
∑
=
∀
=
=
m
1
i
ij
j
n
1,
j 
 ,
p
q
                                          (2.38) 
The five probability matrices used for statistical description of the channel will 
generate the corresponding entropies: 
 
• input entropy: 
( )
( )
∑
=
→
=
m
1
i
i
2
i
p 
log
p
-
X
H
X
P
                                  (2.39) 
• output entropy: 
( )
( )
∑
=
→
=
m
1
i
i
2
i
q 
log
q
-
X
H
X
P
                                  (2.40) 
• conditional entropy (output conditioned by input) / average error: 
P(Y/X) → H(Y/X) 
• conditional entropy (input conditioned by output) / equivocation: 
P(X/Y) → H(X/Y) 
• joint input – output entropy: 
∑∑
=
→
= =
m
1
i
n
1
j
ij
ijldp
p
-:
H(XY)
P(XY)
                                   (2.41) 
Knowing the received symbols yj, we cannot state that the uncertainty regard-
ing the transmitted symbols was totally eliminated, because of the channel noise. 
Uncertainty still exists even after receiving the yj symbols. The average value of 
this residual uncertainty is denoted by H(X/Y) and signifies the input conditioned 
by output entropy; it is also called equivocation, being a measure of the equivocal 
that still exists regarding the input, when the output is known. 
The information quantity obtained about xi, when yj has been received is, ac-
cording to (2.11): 
i/j
2
j
i
p
-log
)
/y
i(x
=
                                       (2.42) 
The average quantity of information obtained about the input, when yj was re-
ceived will be: 
∑
∑
−
=
=
=
=
m
1
i
m
1
i
i/j
2
i/j
j
i
i/j
j
p
log
p
)
/y
i(x
p
)
H(X/y
 
The average quantity of information obtained about the input X, when we know 
the whole output Y, is: 
(
)
∑
∑∑
−
=
=
=
= =
n
1
j
m
1
i
n
1
j
i/j
2
i/j
j
j
j
p
log
p
q
)
X/y
(
H
q
X/Y
H
, 

20 
2   Statistical and Informational Model of an ITS 
 
which, by taking into consideration (2.36) becomes: 
(
)
∑∑
−
=
= =
m
1
i
n
1
j
i/j
2
ij
p
log
p
X/Y
H
                                      (2.43) 
Knowing the input symbols P(X), we cannot know with certainty the symbols 
that will be received, due to channel noise. It will always be an uncertainty whose 
average value is denoted with H(Y/X) and it represents the output conditioned by 
input entropy; it is also called average error. 
Using a similar reasoning with the one used to deduce (2.43), we obtain: 
(
)
∑∑
−
=
= =
m
1
i
n
1
j
j/i
2
ij
q
log
p
Y/X
H
                                       (2.44) 
In the case of a noiseless channel, i.e. no interference or perturbation, the struc-
ture of the noise matrix is: 
(
)
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
0
   
   
0
   
0
0
   
   
1
   
0
0
   
   
0
   
1
Y/X
P
…
#
…
…
,                                         (2.45) 
having only 0 and 1 as elements; when we transmit the symbol xi we know with 
certainty the received symbol. As a result: 
⎩
⎨
⎧
=
=
0
H(Y/X)
0
H(X/Y)
                                               (2.46) 
For a very noisy channel (independent), no relation can be established between 
transmission and receiver, these being independent: 
⎪⎩
⎪⎨
⎧
=
=
j
j/i
i
i/j
q
q
p
p
                                               (2.47) 
It follows that: 
⎩
⎨
⎧
=
=
H(Y)
H(Y/X)
H(X)
H(X/Y)
                                         (2.48) 
For a real channel, when the noise exists, but is not so high, the entropies will 
have values between the two extreme limits: 
⎩
⎨
⎧
≤
≤
≤
≤
H(Y)
H(Y/X)
0
H(X)
H(X/Y)
0
                                     (2.49)  

2.8   Discrete Transmission Channels 
21 
 
2.8.2   Mutual Information and Transinformation 
Mutual information between xi and yj, denoted with i(xi;yj), represents the non-
determination that remains about the transmission of xi after receiving yj. 
)
/y
x
(i
)
x
(i:
)
y
;
i(x
j
i
i
j
i
−
=
                                     (2.50) 
The a priori non-determination about the transmission of xi is i(xi). The receiv-
ing of yi removes part of this non-determination: i(xi/yj), the difference, given by 
the (2.50), being the mutual information. 
Replacing in (2.50) the corresponding expressions of i(xi), respectively i(xi/yj),  
and taking into consideration (2.33), we obtain: 
j
i
ij
2
i
i/j
2
i/j
2
i
2
j
i
q
p
p
log
p
p
log
p
log
p
log
)
y
;
x
(i
=
=
+
−
=
               (2.51) 
The mutual information between xi and yj is reciprocal: 
)
x
;
i(y
)
y
;
i(x
i
j
j
i
=
                                         (2.52) 
We invite the reader, as an exercise, to demonstrate the relation (2.52). 
The average value of the mutual information is called transinformation and it is 
denoted with I(X;Y); it represents the useful average quantity of information 
transmitted through the channel: 
(
)
∑∑
=
= =
m
1
i
n
1
j
j
i
ij
2
ij
q
p
p
log
p
Y
X;
I
                                 (2.53) 
Remark 
Even though i(xi;yj) can be negative, I(X;Y) is always positive (I(X;Y)≥0). 
2.8.3   Relationships between Entropies 
Relation (2.41) is: 
∑∑
=
= =
m
1
i
n
1
j
ij
2
ij
p
log
p
-
H(XY)
 
Using (2.36) and (2.41), we obtain: 
∑
∑∑
=
∑
∑∑
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∑
=
∑∑
=
=
= =
=
= =
=
= =
m
1
i
m
1
i
n
1
j
j/i
2
ij
i
2
i
m
1
i
m
1
i
n
1
j
j/i
2
ij
i
2
n
1
j
ij
m
1
i
n
1
j
j/i
ij
2
ij
q
log
p
-
p
log
p
-
q
log
p
-
p
log
p
-
q
p
log
p
-
H(XY)
 
 
 

22 
2   Statistical and Informational Model of an ITS 
 
Consequently, we obtain the following relation: 
H(XY)=H(X)+H(Y/X)                                     (2.54) 
Using a similar procedure, we obtain the result: 
H(XY)=H(Y)+H(X/Y)                                       (2.55) 
Remark 
The reader is invited to demonstrate the following relations (2.55), (2.57), (2.58). 
From (2.53) we obtain: 
(
)
∑∑
∑
∑
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛∑
−
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
∑
−
=
= =
=
=
=
=
m
1
i
n
1
j
m
1
i
n
1
j
j
2
m
1
i
ij
i
2
n
1
j
ij
ij
2
ij
q
log
p
p
log
p
p
log
p
Y
X;
I
  
 
I(X;Y)=H(X)+H(Y)-H(X,Y)                                (2.56) 
I(X;Y)=H(X)-H(X/Y)                                     (2.57) 
I(X;Y)=H(Y)-H(Y/X)                                     (2.58) 
The relationships established between different entropies have particular ex-
pressions for distinct types of channel: 
 
• Noiseless channel; from (2.46) and (2.57) we obtain: 
I(X;Y)=H(X)=H(Y)                                        (2.59) 
• Independent channel; from (2.48), (2.55), (2.57), (2.58) we get: 
H(X,Y)=H(X)+H(Y)                                      (2.60) 
I(X;Y)=0                                              (2.61) 
The relationships between entropies can be graphically represented using 
Venns diagram: 
We associate to the input r.v. X, the set A and to the output r.v. Y, the set B. 
We define the measure of the two sets: m(A); respectively  m(B) as the area of 
these sets. Now we make the following correspondences: 
 
• 
( )
( )
X
H
A
m
→
 
• 
( )
( )
Y
H
B
m
→
 
• 
(
)
(
)
XY
H
B
A
m
→
∪
 
• 
H(X/Y)
)
B
m(A
→
∩
 
• 
(
)
(
)
Y/X
H
B
A
m
→
∩
 
• 
(
)
(
)
Y
X,
I
B
A
m
→
∩
 
 
which are given in Fig. 2.4. 
 
 

2.8   Discrete Transmission Channels 
23 
 
 
Fig. 2.4 Graphical representation of the relationships between entropies: a) ordinary chan-
nel, b) noiseless channel, c) independent channel 
2.8.4   Channel Capacity Using the Noise Matrix 
In order to find a measure of efficiency in transmitting the information through a 
channel, Cl.E. Shannon introduced the concept of channel capacity, C: 
(
) [
]l
bits/symbo
  
Y
X;
I
max
C
io
i
p
p →
=
                                 (2.62) 
The maximum value of the transinformation is obtained for a certain set of 
probabilities [pi0] that defines a secondary source, which will be the channel input. 
Therefore, in order to transmit the maximum transinformation (which is, in fact 
the channel capacity) through the channel, it is compulsory that the primary source 
be transformed (through encoding) into a secondary source according to the opti-
mal PMF: [pi0]; which maximize (2.62). The entire process is called statistical ad-
aptation of the source to the channel.   
The maximum transinformation rate that can be transmitted through the  
channel is: 
[
]
bits/s
  ;
T
C
C
M
t =
                                         (2.63) 
Remark 
In many cases the channel capacity has the meaning of maximum information rate 
that can be transmitted through the channel (Ct → C). 
 
 

24 
2   Statistical and Informational Model of an ITS 
 
Channel redundancy and efficiency 
 
By analogy with source redundancy and efficiency we can define the followings: 
 
• channel redundancy, which expresses the deviation of the transinformation 
from its maximum value, in relative or absolute value: 
• channel absolute redundancy, Rc: 
(
)
Y
X;
I
C
:
R C
−
=
                                           (2.64) 
• channel relative redundancy, ρc: 
(
)
C
Y
X;
I
1
:
ρC
−
=
                                            (2.65) 
• channel efficiency, ηc: 
(
)
C
Y
X;
I
:
ηC =
                                            (2.66) 
 
Binary symmetric channel (BSC) capacity 
 
A binary symmetric channel, BSC, is a channel for which the error probability p is 
the same for each symbol. As a result, the noise matrix has the following structure: 
(
)
⎥⎦
⎤
⎢⎣
⎡
−
−
=
p
1
        
p
p
    
p
1
Y/X
PBSC
                                   (2.67) 
The capacity of a BSC can be computed starting from (2.62); expressing I(X;Y) 
by (2.58), we get: 
( )
(
)
[
]
Y/X
H
Y
H
max
C
ip
−
=
                                   (2.68) 
Considering a binary source as input: 
1
p
p
  ,
p
    
p
    x
x
:
X
2
1
2
1
2
1
=
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
       
we compute H(Y/X) using (2.44), where the elements pij are calculated using 
(2.35): 
⎥⎦
⎤
⎢⎣
⎡
−
−
=
⎥⎦
⎤
⎢⎣
⎡−
⎥⎦
⎤
⎢⎣
⎡
=
=
p)
(1
p
    
          
p
p
p
p
       
p)
(1
p
p
-
1
        
p
p
   
p
1
p
    
0
0
  
p
P(X)P(Y/X)
P(XY)
2
2
1
1
2
1
    (2.69) 
and we obtain: 
(
)
[
]
[
]
)
p
1(
log
)
p
1(
p
plog
)
p
p
(
p
plog
)
p
1(
p)log
1(
)
p
1(
p)log
1(
p
p
plog
p
p
plog
p
)
p
1(
p)log
1(
p
Y/X
H
2
2
2
1
2
2
2
2
2
2
2
1
2
1
−
−
−
−
=
+
+
−
−
−
=
=
−
−
+
+
+
−
−
−
=
=
   (2.70) 

2.8   Discrete Transmission Channels 
25 
 
One may notice that, for a BSC, the average error, H(Y/X), does not depend on 
the source, P(X), but only on channel noise matrix P(Y/X). 
Replacing H(Y/X) with (2.70) in (2.68) we get: 
( )
(
)
(
)
p
1
log
p
1
p
plog
Y
H
max
C
2
2
p
BSC
i
−
−
+
+
=
                (2.68.a) 
The maximum value for H(Y) is obtained if all the received symbols have equal 
probabilities: q1=q2=1/2. These values are obtained as marginal probabilities from 
(2.69) as follows: 
(
)
(
)
2
1
p
1
p
p
p
q
2
1
p
p
p
1
p
q
2
1
2
2
1
1
=
−
+
=
=
+
−
=
 
gives p1=p2=1/2. It means that the maximum value of the transinformation is ob-
tained when the symbols are used with equal probabilities and: 
ol]
[bits/symb
   
p)
(1
p)log
(1
p
plog
1
C
2
2
BCS
−
−
+
+
=
                (2.71) 
 
Symmetric channel capacity 
 
Symmetric channel is a channel that is symmetric at the input as well at the output. 
The noise matrix is as follows: 
(
)
m
1,
j
  ,
m
1,
i
   
,]
q
[
Y/X
P
j/i
=
=
=
 
where: 
⎪⎩
⎪⎨
⎧
≠
=
−
=
=
j
i
   
,
1
-
m
p
j
i
    
p,
1
)
/x
y
(
p
q
i
j
j/i
                             (2.72) 
Using (2.62) we have: 
(
)
(
)
  
1
-
m
p
log
 p
p
-
1
log
p
1
m
 
log
C
2
2
2
SC
+
−
+
=
                     (2.73) 
The reader is invited to demonstrate the relation. 
 
Binary erasure channel (BEC) capacity 
 
A BEC is a binary channel symmetric at the input, but asymmetric at the output. 
The graph corresponding to such a channel is shown in Fig. 2.5. When y3 is re-
ceived, the input signal can be 0 or 1 with the same probability. 
 
 

26 
2   Statistical and Informational Model of an ITS 
 
0
=
1
x
1
=
2
x
0
=
1
y
1
=
2
y
3
y
 
Fig. 2.5 The graph corresponding to a binary erasure channel 
The noise matrix is: 
⎥⎦
⎤
⎢⎣
⎡
−
−
−
−
=
q
       
q
p
1
     
          
p
q
   
          
p
        
q
p
1
(Y/X)
PBEC
                           (2.74) 
Using (2.62) the reader is invited to demonstrate: 
p
plog
q)
p
(1
q)log
p
(1
q)]
(1
log
q)[1
(1
C
2
2
2
BEC
+
−
−
−
−
+
−
−
−
=
       (2.75) 
Remark 
The errors that occur in memoryless channels are independent errors, caused by 
thermal noise. The formulae of error distribution are very complex and difficult to 
be computed; however, these formulae can be obtained by modelling the experi-
mental results [1], [14]. 
For a BSC, with given p, the binomial law [9], [24] allows to calculate the 
probability of having t independent errors in an n length word: 
( )
(
)
t
n
t
t
n
p
1
p
C
t
P
−
−
=
                                          (2.76) 
The probability of occurrence of t and less then t errors is given by: 
(
)
( )
(
)
∑
∑
−
=
=
≤
=
=
−
t
0
e
t
0
e
e
n
e
e
n
p
1
p
C
e
p
t
e
P
                           (2.77) 
 
Example 2.4 
The symbols 0 and 1 are emitted at the input of a binary transmission channel. Sta-
tistical measurements show that, because of channel noise, both symbols are 10% 
erroneous, the process being time invariant. Knowing that the symbols 0 and 1 are 
transmitted in a ratio of 3/7, the transmission of a symbol is independent of the 
previously transmitted symbols and that 1000 symbols per second are being emit-
ted (the duration of each symbol is the same), find: 
 
a) 
the statistical characterization of the transmission system 
b) 
the quantity of information obtained when 0 is emitted and the source aver-
age quantity of information  

2.8   Discrete Transmission Channels 
27 
 
c) 
source redundancy and efficiency 
d) 
rate of useful information transmitted through the channel 
e) 
channel efficiency. 
 
Solution 
The source and the receiver are modelled by the discrete r.v. X and Y, respec-
tively, described by the following PMFs: 
 1
q
q
   
,
q
    
q
y
   
y
:
Y
   
1,
p
p
   
,
p
   
p
   x
x
:
X
2
1
2
1
2
1
2
1
2
1
2
1
=
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
 
where x1 and x2 are the emission of  0 and 1 respectively, and p1 and p2 their cor-
responding emission probabilities. In the same way, y1 and y2 are the received 0 
and 1 and q1, q2 the corresponding probabilities at receiver. 
The modelling of the transmission channel is accomplished after computing the 
transition (noise) matrix (
)
[
]
j/i
q
Y/X
P
=
. The graphical representation of this 
transmission system is given in Fig 2.6. 
( )
1
2
x
( )
0
1
x
( )
1
2
y
( )
0
1
y
2/2
q
1/1
q
2/1
q
1/2
q
 
Fig. 2.6 Graphical representation of a binary transmission system  
From the problem hypothesis we have: 
⎪⎩
⎪⎨
⎧
=
+
=
1
p
p
7
3
p
p
2
1
2
1
 
It follows: 
[
]
0,7
0,3
P(X) =
 
The given data of the problem indicate a transmission error of 10 % for both 
symbols  (
)
0.1
q
q
1/2
2/1
=
=
and subsequently a correct transmission in 90% of the 
cases: 
0.9
q
q
2/2
1/1
=
=
. The transition (noise) matrix of the channel will be: 
⎥⎦
⎤
⎢⎣
⎡
=
0,9
0,1
0,1
0,9
P(Y/X)
 

28 
2   Statistical and Informational Model of an ITS 
 
Notice that P(Y/X) is a stochastic matrix (the sum on each line is 1), which in 
practice means that if a symbol xi is emitted, a symbol yj is received, correct or er-
roneous. We can easily recognize the binary symmetric type of channel with 
p=0,1.  
Knowing the source through P(X) and the channel through P(Y/X) we can eas-
ily calculate all the other PMFs. 
Using (2.35) we obtain the joint probability matrix: 
⎥⎦
⎤
⎢⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥⎦
⎤
⎢⎣
⎡
=
=
0,63
   
0,07
0,03
   
0,27
0,9
     
0,1
0,1
    
9,0
0,7
      
0
0
   
3,0
P(Y/X)
P(X)
P(XY)
 
We invite the reader to check the calculus, calculating as marginal probabilities 
P(X). 
The received PMF can be calculated, according to (2.38), by summation on 
columns:
[
]
0,66
0,34
P(Y) =
. 
 
Remark 
P(Y) can be calculated also using (2.32), as follows: 
( )
( ) (
) [
]
[
]
0,66
   
0,34
0,9
   
0,1
0,1
   
0,9
0,7
  
0,3
Y/X
P
X
P
Y
P
=
⎥⎦
⎤
⎢⎣
⎡⋅
=
=
 
In order to find the matrix P(X/Y), we use (2.33) to obtain pi/j elements. It  
results:  
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
0,66
0,63
0,34
0,07
0,66
0,03
0,34
0,27
P(X/Y)
 
It may be noticed that P(X/Y), unlike P(Y/X), is not a stochastic matrix. It was 
obtained through calculus and not experimentally, like P(Y/X).  
Using (2.11) (the definition of the self information), we have: 
1,737bits
p
log
)
x
(i
1
2
1
=
−
=
 
The source average quantity of information defines its entropy and is computed 
using (2.12):  
( )
(
)
∑
=
+
−
=
−
=
=
2
1
i
2
2
i
2
i
l
bits/symbo
  
0,88
0,7
0,7log
0,3
0,3log
p
log
p
X
H
 
c) Using the definitions, we have: 
• source absolute redundancy: 
( )
( )
( )
l
bits/symbo
 
0,12
0,88
1
X
H
ldm
X
H
X
D
R x
=
−
=
−
=
−
=
 
• source relative redundancy : 
( )
12%
or  
   
0,12
X
D
R
ρ
X
X
=
=
 

2.8   Discrete Transmission Channels 
29 
 
• source efficiency: 
( )
( )
88%
or 
   
0,88
X
D
X
H
ηx
=
=
 
d) The transinformation rate through the channel is: 
(
)
(
)
Y
X;
I
M
Y
X;
I
•
•
=
 
where 
•
M  = 103 symbols/second = 103 Bd. 
The transinformation can be calculated using (2.53) or the relationships be-
tween entropies, the last one being used as follows: 
I(X;Y)=H(Y)-H(Y/X) 
The binary channel is symmetric, so we have the following equations: 
H(Y/X)=-pldp-(1-p)ld(1-p),      
hence 
 
I(X;Y)=-(0,34ld0,34+0,66ld0,66)+0,1ld0,1+0,9ld0,9 =  
= 0,456 bits/symbol 
(
)
bits/sec
   
456
0,456
10
Y
X;
I
3
=
⋅
=
•
 
e) 
 
(
)
86%
or  
   
0,86
0,531
0,456
0,469
1
0,456
C
Y
X;
I
ηC
≈
=
−
=
=
 
The maximum efficiency 
1
ηC = , corresponding to a transformation that equals 
the channel capacity, can be obtained only when the source X is equally probable, 
which means (for the given system) that a source processing (encoding) must be 
performed before the transmission. 
 
Example 2.5 
Two binary symmetric channels given by  p1=10-1 and p2=10-2 are cascaded. Find 
the: 
 
a) equivalent noise matrix of the cascade 
b) equivalent channel capacity 
 
Solution 
 
a) The noise matrices of two BSC are: 
⎥⎦
⎤
⎢⎣
⎡
−
−
=
⎥⎦
⎤
⎢⎣
⎡
−
−
=
2
2
2
2
1
1
1
1
p
1
         
p
p
       
p
1
P(W/Z)
    
,
p
1
         
p
p
    
p
1
P(Y/X)
 
 

30 
2   Statistical and Informational Model of an ITS 
 
The equivalent noise matrix of the cascade is 
(
) (
)
(
)
(
)
(
)
(
)
(
) (
)⎥⎦
⎤
⎢⎣
⎡
−
⋅
−
+
−
+
−
−
+
−
+
−
⋅
−
=
=
⎥⎦
⎤
⎢⎣
⎡
−
−
⋅⎥⎦
⎤
⎢⎣
⎡
−
−
=
⋅
=
2
1
2
1
1
2
2
1
2
1
1
2
2
1
2
1
2
2
2
2
1
1
1
1
p
1
p
1
p
p
    
p
1
p
p
1
p
p
1
p
p
1
p
    
p
p
p
1
p
1
p
1
         
p
p
     
p
1
p
1
        
p
p
     
p
1
P(W/Z)
P(Y/X)
P(W/X)
 
 
It follows that:  
(
) (
)
(
)
⎩
⎨
⎧
−
+
=
=
+
+
−
=
−
⋅
−
+
=
=
2
1
2
1
2/1
1/2
2
1
2
1
2
1
2
1
2/2
1/1
p
2p
p
p
q
q
p
2p
p
p
1
p
1
p
1
p
p
q
q
 
meaning that the equivalent channel is BSC too, with   
pech=q2/1=q1/2=p1+p2-2p1p2 
The quality of the equivalent channel is worst than the worst channels in the 
cascade: 
pech=p1+p2-2p1p2≈p1+p2=10-1+10-2 
b)   
 
symbol
y 
bits/binar
  
0,4974
0,1996
0,3503
1
       
0,89
0,89log
0,11
0,11log
1
       
)
p
1(
log
)
p
1(
p
log
p
1
C
2
2
ech
2
ech
ech
2
ech
ech
=
−
−
=
=
+
+
=
=
−
−
+
+
=
 
2.8.5   Shannon Capacity 
Capacity of a transmission channel, modelled by P(Y/X) – the noise matrix, was 
defined in 2.8.4. In most practical cases the noise matrix is not known, the channel 
being specified by some parameters which can be measured experimentally much 
easier, like the bandwidth (B) and the signal/noise ratio (SNR - ξ). 
Theoretically it is possible to transmit any quantity of information on a channel. 
The maximum information rate that can be transmitted in real time is limited, and 
this limit defines the channel capacity. This limitation is determined by channel 
characteristics and it stands in both digital and analogue transmissions. We will 
compute channel capacity (without trying to impose it as a demonstration) for digital 
systems, as they are the most used [6]; as a matter of fact, the analogue information 
can be considered a limit case of the digital information (må∞) [32], [9]. 
At channel input we consider a discrete source described by: 
• decision rate: 
•
D  [bits/s], therefore the source is assumed to be equally probable 
(in practice this is obtained after encoding) 
• moment rate: 
•
M [Bd], which shows how fast the carrier signal varies. 
• source alphabet, formed by m specific states of a moment (these can be levels, 
frequencies or phases). 
 

2.8   Discrete Transmission Channels 
31 
 
As previously shown in 2.7, these three parameters are linked by: 
m
log
M
D
2
•
•
=
                                                (2.27) 
In real cases the receiver should be able to distinguish, in the presence of noise, 
two successive moments for which the characteristic parameter takes, in the worst 
case, two consecutive values from the m possible. 
In order to be able to transmit a decision rate
•
D , the channel must provide: 
 
• a time resolution, meaning it should allow the signals characteristic parameter 
to vary from one moment to another, or during one moment. 
• an amplitude resolution, such as the m possible values of the characteristic pa-
rameter may be distinguished even in the presence of noise. 
Figure 2.7 shows a graphical illustration of these two requirements: 
 
 
Fig. 2.7 Illustration of time and amplitude resolutions  
Time resolution 
 
Any real transmission channel contains reactance that oppose to fast signal varia-
tions, leading to an inertial behaviour of the channel. This phenomenon exists both 
in frequency and in time. 
In frequency: channel attenuation is a function of frequency, channel behaving 
as low-pass filter (LPF). 
In time: channel unit-step signal response has a finite slope defined by the rise 
time (tr) [15], [27]. When talking about real and ideal channels, the following em-
pirical relation stands between tr and B:   
0,45
0,35
Btc
÷
≅
                                       (2.78) 

32 
2   Statistical and Informational Model of an ITS 
 
which shows that for a channel of bandwidth B, the parameters cannot vary at any 
speed, being limited by tr. Therefore the moment duration 
•
=
M
/
1
TM
is also lim-
ited by tr: 
B
~
M
B
0,4
~
M
1
t
~
T
r
M
•
•
⇒
⇒
                                   (2.79) 
In 1928, H. Nyquist has proved the required relation between
•
M and B in order 
to achieve a time resolution (no inter-symbol interference) 
filter,
 
ideal
an 
for 
 
2B,
Mmax =
•
                                  (2.80) 
known in literature as the Nyquist theorem [20], [23]. 
For real channels, which are not ideal low-pass filters we have: 
r
M
2t
T
≅
                                                   (2.81) 
From (2.78) we get tc = 0.4/B and (2.81) becomes: 
B
0.8
TM =
, 
and for real channels, we have: 
B
1.25
M max
⋅
=
•
                                            (2.80a) 
 
Amplitude resolution 
 
Besides channel inertia that gives its low-pass filter behaviour, the noise on the 
channel added to the transmitted symbols, deteriorates the detection of the m val-
ues corresponding to a moment.  
The signal power PS being limited, it is impossible to recognize an infinity of 
different values of m in the presence of noise (the noise power is PN). 
In 1948, Shannon demonstrated that the theoretical limit for m in the presence 
of additive white Gaussian noise (AWGN), is: 
ξ
1
P
P
P
m
N
N
S
max
+
=
+
=
                                   (2.82) 
where 
N
S/P
P
ξ =
 is the signal/noise (SNR). 
 
Channel capacity 
 
Channel capacity is defined as the maximum decision rate that can be error free 
transmitted through that channel, assumed an ideal Low Pass Filter (LPF) with 
bandwidth B: 
(
)
(
)
ξ
1
log
B
ξ
1
log
2B
m
log
M
D
:
C
2
2
max
2
max
max
+
⋅
=
+
⋅
=
=
=
•
•
        (2.83) 
and is known as Shannon capacity formula (Shannon 3rd theorem). 

2.8   Discrete Transmission Channels 
33 
 
Remark 
If SNR( ξ ) is given in dB: 
[
]
10lgξ
ξ dB =
 
the relation (2.83) becomes: 
[
]
dB
Bξ
3
1
C ≅
                                            (2.83.a) 
Shannon capacity formula (2.83), given by Shannon in 1948, shows the theo-
retical limit of the maximum information rate through the channel in error free 
transmission.  
Despite the relation (2.83) is a theoretical limit, impossible to be reached in real 
transmissions, it is remarkably useful in applications, allowing a comparison and 
an evaluation of different transmission systems. 
Fig. 2.8 shows the graphical representation of this equation. 
 
Fig. 2.8 Graphical representation of channel capacity 
Interpretations of Shannon capacity formula 
 
1) C~B and C~log2(1+ ξ) , which means that reducing the bandwidth and keep-
ing C constant, ξ must be seriously improved, given the logarithmic dependence 
between C and ξ.  
2) Given the proportionality between capacity and bandwidth, the following 
question arises: is it possible to increase the capacity to infinity based on the in-
creasing of bandwidth? The answer is negative, the reason being obvious: increas-
ing the bandwidth, the noise power PN, increases too, causing a decrease of 
SNR( ξ ), for constant PS. The proof is immediate: the calculations is made under 
the assumption of AWGN with spectral density power N0 constant: 
(
)
ct
ln2
N
P
BN
P
1
Blog
lim
C
0
S
0
S
2
B
=
=
+
=
∞
→
∞
                       (2.84) 
 

34 
2   Statistical and Informational Model of an ITS 
 
 
Fig. 2.9 Bandwidth - capacity dependency 
Therefore, increasing the capacity beyond a certain limit is not rational, the cor-
responding capacity gain being very low (Fig 2.9). 
3) The formula (2.83), under the assumption of AWGN can be written as: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛+
⋅
=
0
S
2
BN
P
1
log
B
C
 
If the power of the signal is expressed as: 
M
b
S
T
E
P =
 
where Eb is the average value of the bit energy and we assume 
• binary transmission  (m=2): 
•
•
=
⇒
M
D
 
• transmission at capacity (ideal transmission ) : 
C
D =
⇒
•
, the former formula 
becomes : 
B
C
1
2
N
E
or 
  
N
E
B
 
C
1
log
B
C
B
C
0
b
0
b
2
−
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⋅
+
=
 
At limit Bå∞, it gives :  
0,693
ln2
B
C
1
2
lim
N
E
lim
B
C
B
0
b
B
=
=
−
=
∞
→
∞
→
. 
If expressed in dB, we obtain : 
-1,6dB
-1,59dB
N
E
lim
0
b
B
≅
=
∞
→
                                  (2.85) 

2.8   Discrete Transmission Channels 
35 
 
known as Shannon limit for an AWGN and gives the minimum required 
0
b
N
E
 ratio 
for error- free transmission 
The ratio: 
B
D
η
•
=
 [bits/sec/Hz]                                        (2.86) 
is defined as spectral efficiency (bandwidth efficiency) [34]. 
 
Interpretations of Shannon limit 
• it is a theoretical limit (
C
D =
•
), showing that for infinite bandwidth (
∞
→
B
), 
the ratio  
0
b
N
E
approaches the limit of -1,6 dB (in this case 
ln2
N
P
C
C
0
S
=
=
∞
 ) 
• the capacity boundary [9], defined by the curve for the critical bit rate 
C
D =
•
, 
separates combinations of system parameters that have the possibility for sup-
porting real-time error-free transmissions (
C
D <
•
), from those for which real-
time error-free transmissions are impossible (
C
D >
•
). 
η
]
[
0
dB
N
Eb
 
Fig. 2.10 Bandwidth-efficiency diagram  

36 
2   Statistical and Informational Model of an ITS 
 
Remark 
Turbo codes, invented in 1993 [3], with iterative decoding have almost closed the 
gap between capacity limit and real code performances. They get BER=10-5 at 
dB
 
0,7
N
E
0
b =
 with spectral efficiency of 0,5 bit/second/Hz [34]. 
4) The same capacity can be obtained using different values for ξ and B: we can 
use a narrow bandwidth B1 and a channel with a very good SNR ξ1 (this case cor-
responds to system 1 shown in Fig. 2.8); a noisy channel with a small ξ2 requires a 
wider bandwidth B2 (system 2, Fig. 2.8) to provide the same capacity. 
5) Relation (2.83) gives a theoretical limit of the maximum transmissible deci-
sion rate. On real channels: 
C
D
real
max 
<
•
. This limit is not obtained automati-
cally; it can be obtained by processing the source before transmission, that is, by 
matching the source to the channel (described by B and ξ) through coding and 
modulation. 
 
A suggestive graphical representation of the relations between the information 
source and the channel is provided in Fig. 2.11 [6]. 
If 
C
D >
•
, a real time transmission is no longer possible; in this case the same 
decision quantity 
T
D
D
⋅
=
•
 can be transmitted using an initial processing; the de-
cision quantity D is stored in a memory and then transmitted through a compatible 
channel (
C
D ≤
•
). It is obvious that the transmission time T increases, the trans-
mission not being performed in real time. In practice this situation occurs when 
images are transmitted by space probes and the channel capacity is much smaller 
compared to the source information rate. 
 
 
Fig. 2.11 Graphical representation of the relations between the information source and the 
channel 

2.8   Discrete Transmission Channels 
37 
 
Table 2.2 Illustration of time and amplitude resolution requirements  
Time resolution 
Moment rate                             Bandwidth 
M
T
1
M =
•
           
B
2
M ≤
•
              B
Amplitude resolution 
m values/moment              Signal/noise ratio 
(
)
ȟ
1
log
ldm
2
P
P
ȟ
ȟ
1
m
2
N
S
+
≤
=
+
≤
 
 Decision rate                               Capacity 
(
)
ȟ
1
log
B
C
C
D
m
log
M
D
2
2
+
=
≤
=
•
•
•
 
 
Example 2.6 [21] 
A black-and-white still image (a picture) is decomposed into 
5
10
4.8
n
⋅
=
 ele-
ments (pixels). Experiments have shown that if 100 levels quantify the brightness 
intensity of each pixel, the reconstructed image seems natural. The 100 levels of 
grey are assumed to be equally probable. Find the: 
 
a) average quantity of information supplied by one pixel and that of the whole  
image. 
b) minimum value of the required signal/noise ratio, and the transmission time for  
one picture on a voice channel with bandwidth (300 -  3400 )Hz . 
c) transmission time compared to b) if the transmission line has the same ξ but a 
double bandwidth. 
d) minimum bandwidth necessary for transmitting a dynamic image (video), 
knowing that 25 images/s (
•
M ) are needed by the human visual system in order 
to obtain a  moving image sensation; is it possible such a transmission on the 
voice channel used at point b)? 
 
Solution 
a) the information corresponding to one pixel is contained in its brightness; the 
100 levels being equally probable, the average information quantity for a pixel is 
maximum and using (2.13): 
Dpixel = log2 m = log2 100 = 6,64 bits/element (pixel) 
The average quantity of information of the entire image is: 
Dimage = n Dpixel = 4,8 ·105 ·6,64 = 3,2 Mb 
b) for the amplitude resolution, from (2.82) we have: 
40dB
10lgξ
ξ
  :
dB
in 
or 
  ,
10
ξ
ξ
1
m
dB
4
=
=
≅
⇒
+
=
 

38 
2   Statistical and Informational Model of an ITS 
 
Assuming that the transmission is made at channel full capacity, from (2.83a) 
we get: 
[
]
dB
b
b
b
ξ
B
3
1
T
D
D
C
=
=
=
•
 
where Tb represents the transmission time and 
3
b
10
3,1
300
3400
B
⋅
=
−
=
 Hz the 
bandwidth of the telephone line. From (2.83a) we obtain: 
77s
40
10
3,1
10
3,2
3
Bξ
3D
T
3
6
[dB]
b
≅
⋅
⋅
⋅
⋅
=
=
 
c) from (2.83), replacing B with 2B, we get: 
Tc = Tb / 2 = 38,5 s. 
d) decision rate corresponding to a dynamic image (video) is: 
Mb/s
80
10
3,2
25
D
M
D
6
d
=
⋅
⋅
=
⋅
=
•
•
•
 
Assuming that the channel is used at full capacity, this rate becomes: 
[dB]
d
d
d
ξ
B
3
1
C
D
=
=
•
 
giving: 
[
]
MHz
6
40
10
80
3
ξ
D
3
B
6
dB
d
d
=
⋅
⋅
=
=
•
 
Capacity of the telephone channel from point b) is: 
41kb/s
40
10
3,1
3
1
ξ
B
3
1
C
3
[dB]
b
b
≅
⋅
⋅
⋅
=
=
   
 
whereas: 
41kb/s
C
80Mb/s
D
b
d
=
>
=
•
, 
therefore, the real-time transmission is not possible. 
2.9   Memory Sources (Markov Sources) 
Taking into account the definition given in 2.1 we may say that most of the real 
information sources such as voice signals, TV signals a. s. o. are not memoryless 
sources. For all these sources the emission of a symbol depends on one or more 
previously emitted symbols. 
A source is an m-order memory source, if the generation of the (m+1) order 
symbol depends on the previously m generated symbols. 
The mathematical theory of memory sources is Markov chains theory [11], 
[20], [23], [24]. 

2.9   Memory Sources (Markov Sources) 
39 
 
2.9.1   Finite and Homogeneous Markov Chains  
Only few mathematical concepts share potentialities comparable to those offered 
by the concept of markovian dependency. Some fields where this theory has been 
applied are listed below: 
 
• biology  
• medicine  
• demography 
• meteorology 
• economy  
• reliability studies 
• research on pollution 
• marketing 
• financial transactions 
• pattern recognition theories 
• natural language modelling  
 
Brief historical overview 
 
The concept of markovian dependency appears explicitly for the first time in 1906 
in one of A. Markov papers, a Russian mathematician. He studied sequences of 
dependent variables that are named, in his memory, Markov chains. Markov him-
self studied the succession of vowels and consonants in Pushkin’s novel “Evgeni 
Onegin” and reached the conclusion that this succession may be seen as a two 
state, homogenous dependent chain. 
Nowadays there are an enormous number of papers that deal with Markov 
chains. It is necessary to emphasize the significant contribution brought to the the-
ory of finite Markov chains and its generalization by the godfathers of the Roma-
nian school of statistics and probability: academicians Octav Onicescu and Gheor-
ghe Mihoc. They initiated the research of generalizing the concepts of markovian 
dependency. 
Be a system (source) S with a finite alphabet
{
}
M
2
1
s,
,
s,
s
S
"
=
. The genera-
tion of a symbol is dependent of the m previous symbols. This source can be mod-
elled by a dependent random variable X(n), with values in S, where n indicates the 
time moment. Time evolution of the source is known statistically as: 
)
s
n)
(
X
,
,
s
X(i)
,
,
s
0)
(
X
(
P
k
j
i
=
=
=
…
…
 
We call a finite and homogenous Markov chain a sequence of dependent r.v. 
X(n) with the following properties:   
 
 
=
=
−
=
=
=
=
−
=
−
=
)
s
1)
/X(n
s
X(n)
(
P
)
s
X(1)
,
,
s
2)
X(n
,
s
1)
/X(n
s
X(n)
(
P
i
j
p
k
i
j
…
              
(2.87.a) 
n
,
p
)
/s
P(s
       
          
   
j/i
i
j
∀
=
=
                                     (2.87.b) 

40 
2   Statistical and Informational Model of an ITS 
 
Relation (2.87.a) defines the Markov property, revealing the one-step memory, 
meaning that the entire source history is contained in its most recent memory. 
Relation (2.87.b) indicates the homogeneity, that is, transitions from one state to 
another are not time dependent. 
All conditional probabilities 
j/i
i
j
p
)
/s
p(s
=
 generate the transition probability 
matrix (Markov matrix) as: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
=
M/M
j/M
1/M
j/i
M/1
j/1
1/1
i
j
p
p
p
  
p 
   
p
p
p
M
)
/s
P(s
…
…
…
…
…
…
…
…
                               (2.88) 
Markov matrix is a stochastic, squared matrix: 
∑
=
∀
=
=
M
1
j
j/i
M
1,
i
  
1,
p
                                          (2.89) 
For a Markov source we may also know the initial PMF (probabilities at zero 
moment): 
[
]
∑
=
=
=
=
M
1
i
i
(0)
i
(0)
1
p
  
and
  ,
M
1,
i  , 
p
P
                              (2.90) 
A Markov chain can be illustrated by a graph made up of nodes, that represent 
the states, and arrows, that connect the nodes, representing transition probabilities. 
 
Fig. 2.12 The graph corresponding to a Markov chain with two states  
 
Transition probabilities 
 
The probability of transitioning from the state si to the state sj in m steps is given 
by 
n
  ,
)
s
n)
(
/X
s
m)
n
(
p(X
p
i
j
(m)
j/i
∀
=
=
+
=
                         (2.91) 
One may notice that the homogeneity also extends to the m-step transition. 
Let us consider two states si and sj that can be reached in two steps m=2 on the 
links shown in Fig. 2.13. 

2.9   Memory Sources (Markov Sources) 
41 
 
 
Fig. 2.13 Transition from si to sj in two steps 
For the way shown in Fig. 2.13, the transition probability from si to sj in two 
steps is given by: 
j/k
k/i
(2)
j/i
p
p
p
⋅
=
                                            (2.92) 
The transition probability from si to sj in two steps on any way is given by: 
∑
⋅
=
=
M
1
k
j/k
k/i
(2)
j/i
p
p
p
                                      (2.93.a) 
We must notice that 
(2)
j/i
p
 are the elements of the second order power of the 
transition matrix M: 
2
(2)
M
P
=
                                             (2.93.b) 
By induction it can be demonstrated that: 
∑
∈
∀
∈
∀
⋅
=
=
+
M
1
k
(m)
j/k
(n)
k/i
m)
(n
j/i
S
j
i,
  
N,
m
n,
  ,
p
p
p
                    (2.94.a) 
Remark 
These equations are known as Chapman-Kolmogorov relations and can be written 
as follows: 
m
n
m)
(n
M
M
P
⋅
=
+
                                       (2.94.b) 
Markov chain evolution  
 
Given a Markov chain by the initial PMF, P(0) and the transition matrix M, the 
probability of the system to pass to a certain state in n steps starting from the ini-
tial state is: 
n
(0)
(n)
M
P
P
⋅
=
                                           (2.95) 
 
Markov chains classification 
 
A Markov chain is called regular if there is an n0∈N (natural set) such that 
o
n
M
is a regular matrix (all its elements are strictly positive). 

42 
2   Statistical and Informational Model of an ITS 
 
A Markov chain is called stationary if the n-step transition probabilities con-
verge, when 
∞
→
n
, towards limits independent of the initial state. 
( )
*
j
n
i
j
n
p
p
lim
=
∞
→
                                              (2.96) 
pj
* indicates that the starting state is of no importance. The stationary state PMF is: 
[ ]
∑
=
=
=
=
M
1
j
*
j
*
j
*
1
p
  ;
M
1,
j
   
,
p
P
                                     (2.97) 
and represents 
Π
P
P
M
P
*
*
n
(n)
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
=
#
                                         (2.98) 
Relation (2.98) indicates that we obtained a matrix having all the rows identical 
with the PMF of the stationary state. 
We compute this stationary state by solving the following system: 
⎪⎩
⎪⎨
⎧
∑
=
=
=
M
j
j
M
p
P
M
P
1
*
*
*
1                                               (2.99) 
In [11], [24] is demonstrated that: 
 
• for a stationary Markov chain, the stationary state PMF is unique. 
• a regular Markov chain accepts a stationary state. 
 
A Markov chain is called absorbent if it has at least an absorbent state for 
which
1
pi/i = . In [11] is demonstrated that an absorbent chain is not regular. 
 
Fig. 2.14 Absorbent Markov chain 
 

2.9   Memory Sources (Markov Sources) 
43 
 
The transition matrix for the chain shown in Fig. 2.14 is: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
0
  
0.5
  
0.5
     
0
0.5
   
0
    
0
   
0,5
0
    
0
    
1
      
0
0
    
0
    
0
      
1
M
 
We must notice that s1 and s2 are absorbent states meaning that if the system 
reaches one of these states it “hangs” on to it. 
 
Example 2.7 
One classical example of using Markov chains is weather forecasting. In meteoro-
logical stations the weather is observed and classified daily at the same hour, as 
follows: 
 
s1= S (sun) 
s2= C (clouds) 
s3= R (rain) 
 
Based on daily measurements, meteorologists computed the transition probabil-
ity matrix: 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
0,6
  
0,2
  
0,2
0,3
  
0,4
  
0,3
0,2
  
0,2
  
0,6
M
 
Some questions raised:  
• knowing that today is a sunny day, what is the probability that the forecast for 
the next seven days is: s1 s1 s2 s1 s2 s3 s1? 
The answer is: 
 
( )
(
)
(
)
(
) (
)
(
)
(
)
(
)
4
-
3
1
2
3
1
2
2
1
1
2
1
1
1
1
1
10
2,592
0,2
0,3
0,2
0,3
0,2
0,6
0,6
1
/s
s
p
/s
s
p
/s
s
p
/s
s
p
/s
s
p
/s
s
p
/s
s
p
s
p
⋅
=
⋅
⋅
⋅
⋅
⋅
⋅
⋅
=
=
⋅
⋅
⋅
⋅
⋅
⋅
 
• what is the probability that the weather becomes and remains sunny for three days, 
knowing that the previous day it had rained? 
(
)
(
)
(
)
(
)
(
)
[
]
(
)
2
1
1
1
1
1
1
3
1
3
10
2,88
0,6
1
0,6
0,6
0,2
1
/s
s
p
1
/s
s
p
/s
s
p
/s
s
p
s
p
−
⋅
=
−
⋅
⋅
⋅
⋅
=
=
−
⋅
⋅
⋅
⋅
 
2.9.2   Entropy of m-th Order Markov Source 
Consider a source having a finite alphabet: A= {a1…aM} and assume that the oc-
currence of the (m+1)th symbol depends on the previously m: 
im
i2
i1
j
a
a
/a
a
…
. 
We denote the sequence 
im
i2
i1
a
a
a
…
 by
(
)
i
im
i2
i1
i
s
a
a
a
  
s
=
…
. The total 
number of different sequences that can be created using an M size alphabet is Mm, 
so 
m
M
1,
i =
. 

44 
2   Statistical and Informational Model of an ITS 
 
The quantity of information obtained at the emission of aj conditioned by the 
sequence si, is: 
)
/s
a(
p
log
)
/s
a(i
i
j
2
i
j
−
=
                                    (2.100) 
The average quantity of information obtained at the emission of any aj condi-
tioned by si will be the conditional entropy: 
(
)
∑
−
=
=
M
1
j
i
j
2
i
j
i
)
/s
a(
p
)log
/s
p(a
A/s
H
                                (2.101) 
The average quantity of information obtained at the emission of any symbol aj 
conditioned by any sequence si of m-symbols, represents the entropy of m-step 
memory source 
∑∑
=
∑
∑∑
=
=
=
=
=
=
=
=
m
m
m
M
1
i
M
1
j
i
j
2
ji
M
1
i
M
1
i
M
1
j
i
j
2
i
j
i
i
i
m
)
/s
p(a
)log
p(s
-
)
/s
p(a
)log
/s
)p(a
p(s
-
)
)H(A/s
p(s
(A)
H
       (2.102) 
where   
i
j
im
i1
ji
s
a
a
a
s
…
=
. 
 
Remarks 
• In the particular case 
)
p(a
)
/s
a(
p
j
i
j
=
 corresponding to a memoryless source, 
we obtain (2.12) relation corresponding to a memoryless discrete source 
(MDS). 
• Hm(A) is always smaller compared to the one corresponding to the same source, 
but memoryless: the average non-determination quantity per symbol ai de-
creases because of the symbol correlation. 
 
Example 2.8 
Find the corresponding graph for a two-step binary source (second order memory 
source) and calculate its entropy if p(si) are the ones corresponding to the station-
ary state and the elements p(aj/si) are chosen randomly. 
 
Solution 
Consider the binary (M=2) source having the alphabet: 
A={0,1} 
The m = 2-step memory source will have Mm = 4 states: 
s1 = 00, s2 = 01, s3 = 10, s4 = 11. 
The graph corresponding to the source can be found taking into consideration 
that at the emission of symbol aj, from a state si only certain states can be 
reached.(Fig 2.15) 

2.9   Memory Sources (Markov Sources) 
45 
 
 
Fig. 2.15 Graph corresponding to a 2-step memory binary source  
We choose the transition matrix of this source as: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1/2
   
1/2
    
0
         
0
0
     
0
    
1/2
     
1/2
1/2
   
1/2
    
0
        
0
0
     
0
    
3/4
   
1/4
M
 
Notice that the matrix M contains zero elements, therefore in order to calculate 
the stationary distribution, we should first be certain it exists. Based on the theo-
rems stated in 2.9.1, we verify if the source is regular. For n0=2, M2 has all the 
elements strictly positive, so the source is regular, therefore it accepts a stationary 
state which can be established by solving the system (2.49). 
[
]
[
]
⎪⎩
⎪⎨
⎧
=
+
+
+
=
1
p
p
p
p
p
  
p
  
p
  
p
M
p
  
p
  
p
  
p
*
4
*
3
*
2
*
1
*
4
*
3
*
2
*
1
*
4
*
3
*
2
*
1
 
We obtain: 
 P * = [
]
11
3
11
3
11
3
11
2
 
The elements of this source, necessary for entropy computation are contained in 
the table: 
 
si 
 
 
ai1 
ai2 
aj 
p(si) = pi
*
 
p(aj/si) 
0 
0 
0 
2/11 
1/4 
0 
0 
1 
2/11 
3/4 
0 
1 
0 
3/11 
1/2 
0 
1 
1 
3/11 
1/2 
1 
0 
0 
3/11 
1/2 
1 
0 
1 
3/11 
1/2 
1 
1 
0 
3/11 
1/2 
1 
1 
1 
3/11 
1/2 
 
Replacing in (2.52) we find: H2(A) = 0,549 bits/binary symbol. 

46 
2   Statistical and Informational Model of an ITS 
 
Example 2.9 
A computer game implies the use of only two keys (a1, a2). If the pressing of a key 
depends on the key pressed before, a Markov chain may model the game, its tran-
sition matrix being: 
⎥⎦
⎤
⎢⎣
⎡
=
1/4
   
3/4
2/3
  
1/3
M
 
a) Which is the average quantity of information obtained at one key pressing? 
Assume that p(a1) and p(a2) are the stationary state probabilities. Compare this 
value with the one obtained for a memoryless source model. 
b) Draw the graph corresponding to the 2-nd order Markov source model. How 
will be the average quantity of information obtained at a key pressing compared to 
that from a)?  
 
Solution 
The stationary state PMF can be established by solving the system: 
[
]
[
]`
1
p
p
p
   
p
M
p
  
p
*
2
*
1
*
2
*
1
*
2
*
1
⎪⎩
⎪⎨
⎧
=
+
=
. 
We obtain:  
[
]
8/17
  
9/17
P* =
. The average quantity of information obtained for 
a key pressing represents the first order Markov entropy of the source. Using (2.102) 
we obtain: 
( )
l
bits/symbo
 
0.867
4
1
log
4
1
17
8
4
3
log
4
3
17
8
3
2
log
3
2
17
9
3
1
log
3
1
17
9
-
)
/s
p(a
)log
/s
)p(a
p(s
-
S
H
2
2
2
2
2
1
i
2
1
j
i
j
2
i
j
i
1
=
=
⎟
⎠
⎞
⎜
⎝
⎛
⋅
⋅
+
⋅
⋅
+
⋅
⋅
+
⋅
⋅
=
∑
=
∑
=
= =
 
In the case of a memoryless source modelling of the game, we have: 
( )
∑
=
⋅
−
⋅
−
=
−
=
=
2
1
i
2
2
i
2
i
/symbol
0,9974bits
17
8
log
17
8
17
9
log
17
9
p
log
p
S
H
 
so H1(S) < H(S), as expected. 
A two step memory binary source is modelled as shown in Fig. 2.14. The aver-
age quantity of information obtained at a key pressing will be the entropy of second 
order Markov source and will be smaller than the one corresponding to the first 
order Markov source model. 
2.9.3   Applications 
From the huge number of Markov chains applications we will choose some from 
the artificial intelligence and pattern recognition fields. 
 
 

2.9   Memory Sources (Markov Sources) 
47 
 
Natural language modelling [5] 
 
Any natural language has an alphabet, a dictionary and a syntax. Take for instance 
the Latin alphabet: it contains 26 letters and a space between words, so a total of 
27 symbols. The simplest model associated to such a language is the one corre-
sponding to a discrete memoryless source (the zero order approximation) in which 
all symbols are equally probable. The average quantity of information correspond-
ing to a symbol will be:  
( )
ymbol
4,75bits/s
ld27
S
D
=
=
•
. 
A sequence of symbols emitted by such a model will not mirror the structure of 
a certain language (French, English, Italian, Romanian etc). 
Considering the frequencies of the alphabet letters for a certain language, a slight 
reflection of the language can be obtained. It means the first order approximation, 
which leads to the average quantity of information per letter (for French [5]). 
( )
l
bits/symbo
4,75
27
ld
S
D
=
=
•
 
A better approximation of this information source is made with a first order 
Markov source in which case the joint probabilities  
j
i
ij
a
a
s
=
  and the condi-
tioned probabilities (
)
i
j/a
a
p
 must be known. In this case, in a letter sequence, we 
may recognize with certainty the language type. 
Better models of a natural language can be achieved using second order 
Markov sources. If there are constraints regarding the word length, if the symbols 
are replaced by words and the alphabet by the language dictionary, and if the syn-
tax is taken into consideration, we can find complex sentences in that language, 
with no special signification. 
Acting this way and imposing even more strict constraints, poems can be elabo-
rated on the computer, or literary works can be authenticated etc. 
The more constraints, the less system uncertainty and the number of expressible 
messages is reduced. The constraint degree in a system (language) can be esti-
mated by its redundancy R:  
u
2
i
u
2
u
2
p
2
M
log
I
M
log
M
log
M
log
R
=
−
=
                               (2.103) 
where Mp represents the number of possible messages without constraints and Mu 
the number of useful messages taking into consideration the constraints. Internal 
information of the system is defined (Ii) 
U
2
p
2
i
M
log
M
log
I
−
=
.                                      (2.104) 
In [5] it is shown that, for the French language, the average number of letters 
per word being 5, the number of 5-letter messages that can be achieved using 26 
symbols is:
6
5
p
10
12
26
M
⋅
≈
=
. The French dictionary contains approximately 
7000 5-letter words, so Mu=7000, from which using (2.103) and (2.104) results 

48 
2   Statistical and Informational Model of an ITS 
 
that the French language redundancy is ≈ 45% and the internal information is 2,15 
bits/letter. 
 
Voice modelling 
 
The voice average rate [6] is 80…200 words/minute. For 5-leter average length 
words, in a zero order approximation memoryless, results 
r
bits/lette
  
4,7
D ≅
 
therefore a rate
100bits/s
40
D
…
≅
•
. The real rate for a better approximation (supe-
rior order Markov sources), in which
( )
r
bits/lette
  
1,5
S
H
≅
, is much lower: 
( )
bits/s
30
12
S
H
…
=
•
. It is necessary to emphasize that this value represents only 
the semantic rate of the voice (the literal signification of the words). In reality the 
voice signal also bears subjective information (identity, quality, emotional state), 
which cannot be evaluated using the relations established by now [26]. 
 
Image modelling [17] [35] 
 
As previously shown in example 2.2 the brightness of each black-and-white TV 
image element (pixel) is quantified with m levels of grey assumed to be equally 
probable and then the average quantity of information contained in a pixel, in the 
independent elements approximation, was: 
D = Hmax = log2 m [bits/element pixel]  
In practice the pixels are not independent and the m levels of grey are not 
equally probable. If only the statistical relations between neighbouring elements 
are taken into consideration, the conditioned entropy will be: 
∑∑
−
=
= =
m
1
i
m
1
j
i
j
2
i
j
i
1
)
/a
p(a
)log
/a
)p(a
p(a
H
 
where p(ai) represents the i-level probability, p(aj/ai) is the probability of level j af-
ter level i. 
Calculations of this entropy have shown that, for typical television images, and 
considering only the image space statistics, two times lower values on average are 
obtained compared to the case when modelling with a discrete memoryless source 
(the image elements are considered independent) and for which we have:    
(
)
(
) [
]
∑
−
=
=
m
0
i
i
2
i
0
bits/pixel
   
a
p
log
a
p
H
 
H2 and H3 have also been calculated; they correspond to 2nd and 3rd order 
Markov models. But it was found that the decrease in entropy for these models is 
too little, being hence unpractical. 
A method for establishing conditional entropy is to process the correlated 
memory source in order to become independent (de-correlation). As we will see in 
chapter 3, statistical independence is a condition for an optimal coding.  

2.9   Memory Sources (Markov Sources) 
49 
 
A simple and efficient method for physical achievement of this condition is to 
use differential modulation [35] (see also 2.8). 
 
 
Fig. 2.16 Differential Pulse Code Modulation for a black-and-white video signal  
The analogue black-and-white video signal is converted into a digital signal in-
side PCM (Pulse Code Modulation) block so that each sample will be transmitted 
using n=5 bits. Assuming that samples are independent and using entropy formula 
for a discrete memoryless source we obtain: 
( )
e
bits/sampl
  
5
A
H
≅
. 
In fact video signal samples are strongly correlated, PCM output being in fact a 
memory source. For monochrome video signals it can be assumed that the differ-
ences between two successive samples dj are approximately independents, there-
fore the entropy corresponding to this differences, calculated with relation (2.12) 
will be close to the conditioned entropy of PCM signal. In this case 
( )
e
bits/sampl
  
3
A
Hd
≅
, a value twice lower than that of  PCM source. 
 
Remarks 
• The actual value of source entropy is very important when trying to achieve 
source compression, as we will see in chapter 3. 
• The differential transmission for colour video signals is thought similarly [35]. 
• Other important applications for Markov chains are: 
– 
synchronisation in digital telephony equipment [2], [29] 
– 
voice recognition [13], [28]. 
 
Memory channels 
 
A transmission channel is a memory channel if the output signals depend on the 
previously transmitted ones. 
Typical examples of memory channels are: 
 
• radio channels with burst errors caused by fading phenomenon   
• transmission wires and cables; transmitted sequences of symbols through the 
channel are affected by burst noise caused by switching and inter-modulation 
• magnetic recording media where recording gaps may appear because impurities 
and dust particles. 

50 
2   Statistical and Informational Model of an ITS 
 
Typical for these memory channels is the fact that noise does not affect inde-
pendently each transmitted symbol, but burst errors occur and that is why these 
channels are also called burst-error channels. 
We call a burst error a sequence of symbols (accurate or not) in which the first 
and last one are erroneous and the successive accurate symbols occur in groups 
smaller than r. 
Parameters characterizing burst error are: 
 
• r: between last erroneous symbol of a burst and the first erroneous symbol of 
the next burst there are more than r accurate successive symbols 
• l: burst error length is given by total number of symbols (erroneous and accu-
rate) forming the burst error 
• D: error density is defined as the ratio between the number of erroneous sym-
bols of the burst (t)  and the length of the burst(l) 
Here is a sample of a burst of length l = 7: 
 
Fig. 2.17 Example of burst 
Poisson law approximates burst error distribution. In order to obtain a better 
approximation of experimental curves, in channels where burst errors have a 
grouping tendency, other distribution laws have been searched for: hyperbolic law, 
Pareto distribution [1], [24]. 
A simplified model of a memory channel may be the one represented in Fig. 
2.18 [14]. 
 
Fig. 2.18 The simplified model of a memory channel 

References 
51 
 
This model has two states: s1 and s2, s1 being a “good state” in which there are 
few errors (p1=0), and s2 a “bad state” when errors occur very frequently (p2=1/2). 
The channel remains in state s1 almost all the time, its transition to state s2 are 
caused by transmission characteristics modifications, for instance strong fading. 
As a consequence errors occur in bursts, because of the high value of p2 (p2≈0.5). 
The study of these memory channels presents great interest because of their ap-
plications in mobile communications and satellite transmissions. 
References  
[1] Angheloiu, I.: Teoria Codurilor. Editura Militara, Bucuresti (1972) 
[2] Bellamy, J.C.: Digital Telephony, 2nd edn. Willey, Chichester (1991) 
[3] Berrou, C., Glavieux, A.: Near Shannon limit error-correcting coding and decoding 
turbo-codes. In: Proc. ICC 1993, Geneva, pp. 1064–1070 (1993) 
[4] Blahut, R.E.: Digital Transmission of Information. Addison-Wesley, Reading (1990) 
[5] Cullmaun, G.: Codage et transmission de l’ínformation. Editions Eyrolles, Paris 
(1972) 
[6] Fontolliet, P.G.: Systemes de t́elećommunícations. Editions Georgi, Lausanne (1983) 
[7] Gallager, R.G.: Information Theory and Reliable Communication. John Wiley & 
Sons, Chichester (1968) 
[8] Hamming, R.: Coding and Information Theory. Prentice-Hall, Englewood Cliffs 
(1980) 
[9] Haykin, S.: Communication Systems, 4th edn. John Wiley & Sons, Chichester (2001) 
[10] Ionescu, D.: Codificare si coduri. Editura Tehnica, Bucuresti (1981) 
[11] Iosifescu, M.: Lanturi Markov finite si aplicatii. Editura Tehnica Bucuresti (1977) 
[12] Kolmogorov, A.N.: Selected Works of A.N. Kolmogorov. In: Shiryayev, A.N. (ed.) 
Probability Theory and Mathematical Statistics, 1st edn., vol. 2. Springer, Heidelberg 
(1992) 
[13] Kriouile, A.: La reconnaissance automatique de la parole et les modeles markoviens 
caches. These de doctorat. Universite de Nancy I (1990) 
[14] Lin, S., Costello, D.: Error control coding. Prentice-Hall, Englewood Cliffs (1983) 
[15] Mateescu, A., Banica, I., et al.: Manualul inginerului electronist, Transmisii de date. 
Editura Tehnica, Bucuresti (1983) 
[16] McEliece, R.J.: The Theory of Information and Coding, 2nd edn. Cambridge Univer-
sity Press, Cambridge (2002) 
[17] Mitrofan, G.: Televiziune digitala. Editura Academiei RSR, Bucuresti (1986) 
[18] Mihoc, G., Micu, N.: Teoria probabilitatilor si statistica matematica. Editura Didac-
tica si Pedagogica, Bucuresti (1980) 
[19] Miller, M., Vucetic, B., Berry, L.: Satellite communications. In: Mobile and fixed 
Services. Kluwer Press, Dordrecht (1993) 
[20] Moon, T.K., Stirling, W.C.: Mathematical Methods and Algorithms for Signal Proc-
essing. Prentice-Hall, Englewood Cliffs (2000) 
[21] Murgan, A., et al.: Teoria Transmisiunii Informatiei. Probleme. Editura Didactica si 
Pedagogica, Bucuresti (1983) 
[22] Onicescu, O.: Probabilitati si procese aleatoare. Editura Stiintifica si Enciclopedica, 
Bucuresti (1977) 

52 
2   Statistical and Informational Model of an ITS 
 
[23] Oppenheim, A.V., Schafer, R.F.: Discrete-Time Signal Processing, 2nd edn. Prentice-
Hall, Englewood Cliffs (1999) 
[24] Papoulis, A.: Probability, Random Variables and Stochastic Processes, 2nd edn. 
McCraw Hill (1994) 
[25] Peterson, W.W., Weldon, E.J.: Error-Correcting Codes, 2nd edn. MIT Press, Cam-
bridge (1972) 
[26] Petrica, I., Stefanescu, V.: Aspecte noi ale teoriei informatiei. Editura Academiei 
RSR, Bucuresti (1982) 
[27] Proakis, J.: Digital Communications, 4th edn. Mc Gran-Hill (2001) 
[28] Rabiner, L., Schafer, R.W.: Introduction to Digital Speech Processing (Foundations 
and Trends in Signal Processing). Now Publishers Inc. (2007) 
[29] Radu, M.: Telefonie numerica. Editura Militara, Bucuresti (1988) 
[30] Reza, F.M.: An introduction to information theory (Magyar translation). Budapest 
(1966) 
[31] Shannon, C.E.: A Mathematical Theory Of Communication. Bell System Technical 
Journal 27, 379–423 (1948); Reprinted in Shannon Collected Papers, IEEE 
Press(1993) 
[32] Spataru, A.: Fondements de la theorie de la tránsmission de l’information. Presses 
Polytechniques Romandes, Lausanne (1987) 
[33] Stark, H., Woods, J.W.: Probability and Random Processes with Applications to Sig-
nal Processing, 3rd edn. Prentice-Hall, Englewood Cliffs (2002) 
[34] Vucetic, B., Yuan, J.: Turbo codes. Kluver Academic Publishers Group, Dordrecht 
(2001) (2nd Printing) 
[35] Wade, G.: Signal Coding and Processing. Cambridge University Press, Cambridge 
(1994) 
[36] Wozencraft, J.W., Jacobs, I.M.: Principles of Communication Engineering. Waveland 
Press, Prospect Heights (1990) 

Chapter 3 
Source Coding 
Motto: 
To defeat yourself is the first 
and the most beautiful of 
all victories. 
Democritus 
3.1   What Is Coding and Why Is It Necessary? 
Information is rarely transmitted directly to the receiver, without any processing, 
due to the followings: 
 
• source alphabet is different from channel one, therefore some  adaptation of the 
source to the channel is needed (information representation codes). 
• channel must be very efficiently used (close as possible to its full capacity), 
meaning that the source must be converted into an optimal one (pio) (statistical 
adaptation of the source to the channel from information theory point of view); 
keeping in mind the coding theory, we may say that for an efficient use of the 
channel - in order to minimize transmission time and/or storage space - source 
compression is needed (compression codes). 
• it is also necessary to adapt the source to the channel in order to match the 
spectrum to channels characteristics and also to ensure synchronization be-
tween transmitter and receiver (base band codes / line codes).  
• information transmitted over a noisy channel is distorted by channel noise; this 
is why error detecting and correcting codes are used in error control procedures 
(error control codes).  
• information confidentiality from unauthorized persons must be provided in 
some applications (encryption).  
The need for source processing prior to transmission (or storage) is leading to 
the processing known as coding. 
For a better understanding of coding, we will divide it into three main parts: 
 
1. source coding (Cs) or coding for noiseless channels. 
2. encryption (E), i.e. coding for keeping information confidentiality. 
3. channel coding (Cc) – protection to channel noise (error control) and channel 
adaptation (base band codes). 
In real transmission, one, two or all these aspects arise, the necessary process-
ing degree being imposed by the application itself. 

54 
3   Source Coding 
 
Coding is the process of finding a bijective correspondence between source (S) 
messages and codewords set (C) obtained using the alphabet X. 
Be a discreet memoryless source S and the corresponding codewords set C: 
∑
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
=
M
1
i
i
i
i
i
1
p
 ,
M
1,
i ,
)
p(s
p
s
        
:
S
                                   (3.1) 
)
p(s
)
p(c
 ,
M
1,
i ,
)
p(c
p
c
      
:
C
i
i
i
i
i
=
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
                                (3.2) 
where ci is a codeword and represents a finite row of symbols xj belonging to code 
alphabet X, assumed memoryless too: 
∑
=
=
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
=
=
m
1
j
j
j
j
j
1
p
  ,
m
1,
j
  ,
)
p(x
p
       x
:
X
                                 (3.3) 
By encoding, the initial source S is transformed into a secondary one X, enter-
ing the channel (Fig. 3.1) 
 
Fig. 3.1 Illustration of the transformation SåX realized through encoding  
For a source S and a given alphabet X, we may create a multitude of codes.  
Table 3.1 shows some binary codes put into correspondence with a quaternary  
(M = 4) source S. 
Table 3.1 Binary codes associated to a quaternary source (M=4)  
S 
A 
B 
C 
D 
E 
S1 
00 
0 
0 
0 
0 
S2 
01 
01 
10 
10 
100 
S3 
10 
011 
110 
110 
11 
S4 
11 
0111 
1110 
111 
110 
 
Next, we will define some codes frequently used in applications: 
• Uniform code is a code with same lengths of codewords (e.g.: code A). 
• Non-uniform code is a code with different lengths of codewords (e.g.: codes B, 
C, D, and E). 

3.2   Aim of Source Coding 
55
 
• Uniquely decodable code (UDC) is a code with a unique succession of source 
symbols for each codewords succession (e.g.: A, B, C, D are UDC). Code E is a 
not a UDC because the codeword c4, 110, can be decoded either s4 or s3s1. 
• Comma code is a UDC using demarcation symbols between words (e.g.: B – 
‘0’ indicates codewords beginning; C – ‘0’ indicates codewords ending). 
• Instantaneous code (IC) is a UDC for which a codeword is not the prefix of an-
other codeword (e.g.: A, C, D). Code B is not instantaneous because ‘0’ is pre-
fix for other words. 
Any code can be represented using the coding tree associated to codewords set. 
Such a tree is represented in Fig.3.2 for code D from table 3.1: 
 
Fig. 3.2 Coding tree associated to code D from Table 3.1 
3.2   Aim of Source Coding 
As shown in section 3.1 source coding aim reduces to: 
 
• adapting the source to the channel, if different as nature 
• ensuring channel most rational use by compression  (from coding theory point 
of view) and statistical adaptation of the source to the channel (from informa-
tion theory point of view); the latter is achieved by maximizing the transinfor-
mation which implies equally probable symbols 
m
1,
j
,
p
,
x
m
1
jo
j
=
∀
=
 for 
symmetric channels; therefore the channel must be used at its full capacity: 
m
 
ld
 
D(X)
 
(X)
H
 
C
max
 
channel
  
noiseless
=
=
=
                           (3.4) 
3.3   Information Representation Codes 
These codes are used for adapting the source to the channel as nature, in order to 
be able to transmit or store information. 
3.3.1   Short History 
Codes for information representation have been known since ancient times. Thus a 
series of symbols used in myths and legends, in heraldry and worship objects, are 
sending messages from long time-gone worlds. 

56 
3   Source Coding 
 
We illustrate this by quoting the symbol of the olive tree: 
”Olive tree [9]: A tree of many symbolic resources: peace, fertility, purity, 
power, victory and reward…. It has similar significance in all European and Ori-
ental countries. In Rome, it was dedicated to Jupiter and Minerva. According to a 
Chinese legend, the olive tree wood would neutralize certain poisons and venom; 
therefore it was highly valued for its curing properties. In Japan it symbolizes 
amiability as well as success in studies and war: it is the tree of victory. In Jewish 
and Christian traditions the olive tree is a symbol of peace: the pigeon brought to 
Noach an olive tree branch at the end of the Great Flood. According to an ancient 
legend, the cross on which Jesus was crucified was manufactured from olive and 
cedar wood. Furthermore, in the Middle Ages language, it also stands for a symbol 
for gold and love. In Islam the olive tree is a central tree, the “axis mundi”, a sym-
bol of the Universal Man, of the Prophet. Blessed tree, it is associated to Light, as 
the olive oil lightens the lamps… The olive tree symbolizes after all, the Heaven 
of the chosen ones.”    
In ancient Greece [41], for remote messages transmission, a kind of “telegraph” 
was made using torches, without compression, as later in Morse code.  
In 18th century, the Royal British Navy had used for transmission, signals on 6 
bits, a system of cabins with 6 shutters and some lamps inside, which allowed the 
coding of 
64
2
M
 6 =
=
 messages. These were used to encode 26 letters, 10 num-
bers and some other special commands, common words or phrases. In this way, 
they achieved, beside information representation, some sort of compression. In 
[41] it is shown that two of the 28 combinations, represented the command to  
execute or acquit a convict. It is mentioned the case of a convict executed due to a 
fatal transmission error of the message. This example emphasis compression 
weakness to error in transmission and/or storage.     
The acronyms used since ancient times are, in fact, compressed ways of repre-
senting information. Thus, on Roman funeral graves, for which the engraving cost 
was very high, it is frequently met STL (Sit Tibi Terra Levis), acronym corre-
sponding in English to ”May the earth rest lightly on you”. Regarding acronyms 
utilization in present times, we can refer to our era as ”civilization of acronyms”; 
each domain uses real dictionaries of acronyms. Lets take for example ITC (In-
formation Theory and Coding), SR (Shift Register), AOC (Absolute Optimal 
Code) a. s. o. But same acronyms could represent many other things: Informa-
tional Trade Center, Security Report, Airline Operation Center etc., illustrating in 
fact its vulnerability to errors. 
In what follows we will present some of the most used codes for information 
representation in data transmissions, analogue to digital converters including 
numbering systems and finally the genetic code, taking into account its universal-
ity and actuality. 
3.3.2   Numeral Systems 
A numeral system is a mathematical notation for representing numbers of a given 
set, using distinct symbols (a finite alphabet b). 

3.3   Information Representation Codes 
57
 
There is a diversity of numeral systems developed from ancient time to modern 
days.  
Unary system, is the simplest numeral system and it uses only one letter alpha-
bet: b=1, for example a symbol x. The representation of n symbols is n times x 
(e.g. three is represented: xxx). It was used in early days of human civilization. 
Sign – value system is a variation of unary system. It uses an enhanced alphabet 
by introducing different symbols for certain new values, for example for power of 
10, - , for power of 100, +. The number 213 could be represented: 
213ĺ ++–XXX 
The Egyptian numeral system was of this type and the Roman numeral system 
was a modification of this idea. 
System using special abbreviations for repetitions of symbols; for example: A – 
one occurrence, B – two occurrence,… , I – nine occurrence. In this case the num-
ber 205 will be written: B+EX.  
Positional systems (place – value notation) using base b. A number N in base b 
is represented using b symbols (digits) corresponding to the first b natural num-
bers, including zero: 
{
} { }
1
b
0,
i ,
a
 
 
1
b
...,
 
2,
 
1,
 
0,
A
i
−
=
=
−
=
                               (3.5) 
(
)
0
0
1
1
1
n
1
n
b
0
i
1
n
b
b
a
b
a
...
b
a
a 
...
 
a 
...
 
a
N
+
+
+
=
=
−
−
−
                 (3.6)  
Fractions in the positional system are written dividing the digits into two 
groups: 
(
)
∑
∑
+
=
−
=
∞
=
−
−
1
n
0
i
0
i
i
i
i
i
b
2
1
0
1
n
b
c
b
a
...
c
c
,
...a
a
                             (3.7) 
The numbers  
i
b  and  
i
b− are the weights of the corresponding digits. The po-
sition i is the logarithm of the corresponding weight: 
i
bb
log
i =
                                                   (3.8) 
According to the value of b, a lot of different numeral systems can be obtained, 
the most used in today life and in data communications and computing being: 
 
• decimal numeral system (dec) : b=10 , with the alphabet:  
{
}
9 
8,
 
7,
 
6,
 
5,
 
4,
 
3,
 
2,
 
1,
 
0,
A =
 
• octal numeral system (oct) : b=8 and the alphabet: 
{
}
7 
6,
 
5,
 
4,
 
3,
 
2,
 
1,
 
0,
A =
 
• hexadecimal numeral system (hex) : b=16 and the alphabet: 
{
}
F 
E,
 
D,
 
C,
 
B,
 
A,
 
9,
 
8,
 
7,
 
6,
 
5,
 
4,
 
3,
 
2,
 
1,
 
0,
A =
 

58 
3   Source Coding 
 
Conversion between bases: can be done using the method of successive  
division by b. 
Table 3.2 Conversion between hex, dec, oct and binary numeral systems  
Binary 
Nb 
hex 
dec 
oct 
b3 
b2 
b1 
b0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
1 
2 
2 
2 
2 
0 
0 
1 
0 
3 
3 
3 
3 
0 
0 
1 
1 
4 
4 
4 
4 
0 
1 
0 
0 
5 
5 
5 
5 
0 
1 
0 
1 
6 
6 
6 
6 
0 
1 
1 
0 
7 
7 
7 
7 
0 
1 
1 
1 
8 
8 
8 
10 
1 
0 
0 
0 
9 
9 
9 
11 
1 
0 
0 
1 
10 
A 
10 
12 
1 
0 
1 
0 
11 
B 
11 
13 
1 
0 
1 
1 
12 
C 
12 
14 
1 
1 
0 
0 
13 
D 
13 
15 
1 
1 
0 
1 
14 
E 
14 
16 
1 
1 
1 
0 
15 
F 
15 
17 
1 
1 
1 
1 
 
Remark 
The numeral systems used from ancient times to nowadays, show strong compres-
sion features, meaning less time in transmission and/or space in storage (writing, 
reading). 
3.3.3   Binary Codes Used in Data Transmission, Storage or 
Computing 
• Morse Code 
Although this code invented by Samuel Morse in 1837 for electric telegraph is not 
anymore actual, it still remains the universal code of amateur or professional radio 
operators (maritime links), especially in manually operating systems. 
Alphanumeric characters (26 letters of the Latin alphabet and 10 decimal num-
bers) are encoded using three symbols: dot, line and space. Morse alphabet also 
makes an ad-lib compression putting into correspondence the shortest words to 
letters with maximal frequency (from English language).      
The tree corresponding to Morse alphabet is shown in Table3.3. 

3.3   Information Representation Codes 
59
 
Table 3.3 Tree corresponding to Morse code  
● 
H 
● 
S 
▬ 
V 
● 
F 
● 
I 
▬ 
U 
▬ 
Ü 
● 
L 
● 
R 
▬ 
Ä 
● 
P 
● 
E 
▬ 
A 
▬ 
W 
▬ 
J 
● 
B 
● 
D 
▬ 
X 
● 
C 
● 
N 
▬ 
K 
▬ 
Y 
● 
Z 
● 
G 
▬ 
Q 
● 
Ö 
▬ 
T 
▬ 
M 
▬ 
O 
▬ 
Ch 
 
The “SOS” message in Morse code is “. . .  - - -  . . .” 
 
• Baudot Code 
Morse code, a non-uniform code, has the drawback of a difficult automatic decod-
ing. That is why Emile Baudot proposed in 1870 for telegraphic transmissions a 
five letters long uniform code (teleprinter code), known as ITA1 / CCITT2 / ITU3 
code (Table. 3.4). The 56 characters (26 letters, 10 numbers, 12 signs and 8 com-
mands) used in telegraphy cannot be uniquely encoded with 5 bits: 
56
32
25
<
=
=
M
. That is why the 56 character set was divided in two subsets: a 
lower set containing the letters and an upper set containing the numbers and other 
figures; commands are uniquely decodable. 
The same binary sequence is assigned to two different characters, but belonging 
to different sets.  
Any change from one set to the other, during a message, is preceded by an es-
cape codeword: 
 
1 1 1 1 1 indicates the lower set 
1 1 0 1 1 indicates the upper set. 
 
                                                           
1 ITA – International Telegraph Alphabet. 
2 CCITT – Comité Consultatif International Télephonique et Télegraphique. 
3 ITU – International Telecommunication Union. 

60 
3   Source Coding 
 
Remark 
The escape codes generally obtain a decrease in length of encoded message (there-
fore a compression), if there are not too many inter-sets changes in the message 
(condition achieved in most telegraphic transmissions). 
Table 3.4 Baudot Code  
The number 
of the combi-
nation 
Letters 
 
Numbers 
and special 
signs 
Code 
combina-
tion 
The
number 
of the 
combi-
nation
Letters 
Numbers 
and special 
signs 
Code 
combina-
tion 
1 
A 
- 
11000 
17 
Q 
1 
11101 
2 
B 
? 
10011 
18 
R 
4 
01010 
3 
C 
: 
01110 
19 
S 
; 
10100 
4 
D 
who are 
you 
10010 
20 
T 
5 
00001 
5 
E 
3 
10000 
21 
U 
7 
11100 
6 
F 
! 
10110 
22 
V 
= 
01111 
7 
G 
& 
01011 
23 
W 
2 
11001 
8 
H 
£ 
00101 
24 
X 
/ 
10111 
9 
I 
8 
01100 
25 
Y 
6 
10101 
10 
J 
ringer 
11010 
26 
Z 
+ 
10001 
11 
K 
( 
11110 
27 
Carriage Return (CR) 
00010 
12 
L 
) 
01001 
28 
New Letter (NL) 
01000 
13 
M 
. 
00111 
29 
Letter shift 
11111 
14 
N 
, 
00110 
30 
Figure shift 
11011 
15 
O 
9 
00011 
31 
Space (SP) 
00010 
16 
P 
0 
01101 
32 
Unusable 
00000 
• ASCII Code 
ASCII (American Standard Code for International Interchange) known also as 
CCITT 5 or ISO4 code is a 7 bit length code which allows letters, numbers and 
numerous special commands representation without escape code character as in 
Boudot code (
33
128
27
=
=
non – printing (control) characters + 94 printable +1 
space). It was proposed in 1960 and from then it is the most used code in data en-
coding. The code is given in Table. 3.5 
                                                           
4 ISO - International Organization for Standardization. 

3.3   Information Representation Codes 
61
 
Table 3.5 The 7-bit ISO code (CCITT No 5, ASCII). Command characters:          - for na-
tional symbols, SP – Space, CR - Carriage Return, LF - Line Feed, EOT - End of Transmis-
sion, ESC – Escape, DEL -  Delete. 
bit 7
0 
0 
0 
0 
1 
1 
1 
1 
bit 6
0 
0 
1 
1 
0 
0 
1 
1 
 
bit 5
0 
1 
0 
1 
0 
1 
0 
1 
bit 4 bit 3 
bit 2 bit 1 
 
0 
1 
2 
3 
4 
5 
6 
7 
0 
0 
0 
0 
0 
NUL 
DLE 
SP 
0 
 
P 
’ 
p 
0 
0 
0 
1 
1 
SOH 
DC1 
! 
1 
A 
Q 
a 
q 
0 
0 
1 
0 
2 
STX 
DC2 
” 
2 
B 
R 
b 
r 
0 
0 
1 
1 
3 
ETX 
DC3 
# 
3 
C 
S 
c 
s 
0 
1 
0 
0 
4 
EOT 
DC4 
¤ 
4 
D 
T 
d 
t 
0 
1 
0 
1 
5 
ENQ 
NAK 
% 
5 
E 
U 
e 
u 
0 
1 
1 
0 
6 
ACK 
SYN 
& 
6 
F 
V 
f 
v 
0 
1 
1 
1 
7 
BEL 
ETB 
, 
7 
G 
W 
g 
w 
1 
0 
0 
0 
8 
BS 
CAN 
( 
8 
H 
X 
h 
x 
1 
0 
0 
1 
9 
HT 
EM 
) 
9 
I 
Y 
i 
y 
1 
0 
1 
0 
10 
LF 
SUB 
* 
: 
J 
Z 
j 
z 
1 
0 
1 
1 
11 
VT 
ESC 
+ 
; 
K 
 
k 
 
1 
1 
0 
0 
12 
FF 
FS 
, 
< 
L 
 
l 
 
1 
1 
0 
1 
13 
CR 
GS 
- 
= 
M 
 
m 
 
1 
1 
1 
0 
14 
SO 
RS 
. 
> 
N 
^ 
n 
- 
1 
1 
1 
1 
15 
SI 
US 
/ 
? 
O 
- 
o 
DEL 
Decimal format of ASCII code is easily obtained taking into consideration the 
weights of the bits; as example: 
Letter s ĺ binary: 1110011 ĺ decimal: 26+25+24+21+20 = 115 
In many cases to the 7 data bits is added one more (the 8th bit), the parity con-
trol bit (optional); it results the ASCII-8 code able to detect odd errors (see 
5.7.10). 
• BCD Code 
BCD ( Binary Coded Decimal ) code, known sometimes NBCD (Natural BCD ) is 
a 4 – bits code for decimal numbers, in which each digit is represented by its own 
binary natural sequence. It allows easy conversion from decimal to binary for 
printing and display. This code is used in electronics in 7 – segments displays, as 
well in financial, commercial and industrial computing using decimal fixed point 
or floating – point calculus.  
IBM (International Business Machines Corporation), in its early computers 
used a BCD – 6 bits length code (Table. 3.6) 

62 
3   Source Coding 
 
Table 3.6 IBM BCD code – 6 bits length  
Information (characters) Code combination Information
(characters)
Code combination 
0
00 0000
BLANK
01 0000
1 
00  0001
/
01  0001
2 
00  0010
S
01 0010
3 
00  0011
T
01  0011
4 
00  0100
U
01  0100
5 
00  0101
V
01  0101
6 
00  0110
W
01  0110
7 
00  0111
X
01  0111
8 
00  1000
Y
01  1000
9 
00  1001
Z
01  1001
SPACE 
00  1010
= |
01  1010
= 
00  1011
,
01  1011
, 
00  1100
(
01  1100
 
00  1101
-
01  1101
√ 
00  1110
,
01  1110
> 
00  1110
CANCEL
01  1110
- 
10  0000
+
11  0000
J 
10  0001
A
11  0001
K 
10  0010
B
11  0010
L 
10  0011
C
11  0011
M 
10  0100
D
11  0100
N 
10  0101
E
11  0101
O 
10  0110
F
11  0110
P 
10  0111
G
11  0111
Q 
10  1000
H
11  1000
R 
10  1001
I
11  1001
! 
10  1010
?
11  1010
S 
10  1011
.
11  1011
* 
10  1100
)
11  1100
] 
10  1101
[
11  1101
; 
10  1110
<
11  1110
Δ 
10  1110
≠
11  1110
 
• EBCDIC code 
EBCDIC (Extended Binary Coded Decimal Interchange Code) is an extension of 
the BCD code to 8 bits length being proposed by IBM in the operating systems of 
its computers in the years of 1963 – 1964 (Table 3.7). 
 

3.3   Information Representation Codes 
63
 
Table 3.7 EBCDIC code 
b0 
0 
0 
0 
0 
0
0
0 
0 
1 
1 
1 
1 
1 
1 
1 
1 
b1 
0 
0 
0 
0 
1
1
1 
1 
0 
0 
0 
0 
1 
1 
1 
1 
b2 
0 
0 
1 
1 
0
0
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
 
b3 
0 
1 
0 
1 
0
1
0 
1 
0 
1 
0 
1 
0 
1 
0 
1 
b4 
b5 
b6 
b7 
 
0 
1 
2 
3 
4
5
6 
7 
8 
9 10 11 12 13 14 15 
0 
0 
0 
0 
0 NUL DLE DS
 
blk &
- 
 
 
 
 
 
 
 
 
0 
0 
0 
0 
1 
1 SOH DC1 SOS
 
 
 
/ 
 
a 
j 
 
 
A
J 
 
1 
0 
0 
1 
0 
2 
STX DC2 FS SYN
 
 
 
 
b 
k 
s 
 
B
K 
S 
2 
0 
0 
1 
1 
3 
ETX DC3 
 
 
 
 
 
 
c 
l 
t 
 
C
L 
T 
3 
0 
1 
0 
0 
4 
PF RES BYP PN
 
 
 
 
d m
u 
 
D
M U 
4 
0 
1 
0 
1 
5 
HT NL LF
RS
 
 
 
 
e 
n 
v
 
E
N 
V 
5 
0 
1 
1 
0 
6 
LC 
BS EOB UC
 
 
 
 
f 
o w
 
F
O W 
6 
0 
1 
1 
1 
7 
DEL IDL PRE EOT
 
 
 
 
g 
p 
x 
 
G
P 
X 
7 
1 
0 
0 
0 
8 
 
CAN 
 
 
 
 
 
 
h 
q 
y
 
H
Q 
Y 
8 
1 
0 
0 
1 
9 
 
EM 
 
 
 
 
 
 
i 
r 
z 
 
I 
R 
Z 
9 
1 
0 
1 
0 
10 SMM CC SM
 
Ç
! 
 
 
 
 
 
 
 
 
 
 
1 
0 
1 
1 
11 
VT 
 
 
 
. 
$
, 
Ü 
 
 
 
 
 
 
 
 
1 
1 
0 
0 
12 
FF 
IFS 
 
DC4 <
*
% @
 
 
 
 
 
 
 
 
1 
1 
0 
1 
13 
CR IGS ENQ NAK ( 
) 
- 
, 
 
 
 
 
 
 
 
 
1 
1 
1 
0 
14 
SO IRS ACK
 
+
; 
>
=
 
 
 
 
 
 
 
 
1 
1 
1 
1 
15 
SI 
IUS BEL SUB | 
¬
? 
“ 
 
 
 
 
 
 
 
 
 
• Gray Code 
In 1947, Frank Gray from Bell Laboratories introduced the term reflected binary 
code in a patent application, based on the fact that it may be built up from the con-
ventional binary code by a sort of reflexion process. 
The main feature of this code is that a transition from one state to a consecutive 
one, involves only one bit change (Table 3.8). 
The conversion procedure from binary natural to Gray is the following: the 
most significant bit (MSB) from the binary code is the same with the MSB from 
the Gray code.  Starting from the MSB towards the least significant bit (LSB) any 
bit change (0→1 or 1→0) in binary natural, generates an ’1’ and any lack of 
change generates a ’0’, in Gray code.  
The conversion from Gray to binary natural is the reverse: the MSB is the same 
in binary natural code as well as in Gray code. Further on, from MSB to LSB, the 
next bit in binary natural code will be the complement of the previous bit if the 
corresponding bit from Gray code is 1 or it will be identical with the previous bit 
if the corresponding bit from Gray code is 0. 

64 
3   Source Coding 
 
Table 3.8 Binary natural and Gray 4 bits length codes representation  
Decimal 
Binary 
Natural Code 
(BN) 
Gray Code 
Decimal 
Binary 
Natural Code 
Gray Code 
 
B3 B2 B1 B0 
G3 G2 G1 G0 
 
B3 B2 B1 B0 
G3 G2 G1 G0 
0 
0  0  0  0 
0  0  0  0 
8 
1  0  0  0 
1  1  0  0 
1 
0  0  0  1 
0  0  0  1 
9 
1  0  0  1 
1  1  0  1 
2 
0  0  1  0 
0  0  1  1 
10 
1  0  1  0 
1  1  1  1 
3 
0  0  1  1 
0  0  1  0 
11 
1  0  1  1 
1  1  1  0 
4 
0  1  0  0 
0  1  1  0 
12 
1  1  0  0 
1  0  1  0 
5 
0  1  0  1 
0  1  1  1 
13 
1  1  0  1 
1  0  1  1 
6 
0  1  1  0 
0  1  0  1 
14 
1  1  1  0 
1  0  0  1 
7 
0  1  1  1 
0  1  0  0 
15 
1  1  1  1 
1  0  0  0 
 
Gray code is used in position encoders (linear encoders, rotary encoders) that 
generate numerical code corresponding to a certain angle. Such transducer is made 
up with optical slot disks and for this reason it is impossible to modify simultane-
ously all the bits that could change between two consecutive values. This explains 
why the optical system that generates the number corresponding to a certain angle 
is encoded in Gray. 
In fast converters that generate continuous conversions, Gray code is used due 
to the same reasons. For these converters the problem that arises is storing the re-
sults; if the storage command arrives before all bits settle at final value, the errors 
in binary natural code are great, whereas the Gray code maximal error equals the 
LSB value.                
This code is also frequently used in modulation systems. Let us suppose we deal 
with an 8 levels amplitude modulation. Thus amplitude levels are assigned to each 
three bits, levels which are than transmitted in sequences. Let us see two examples, 
one with the binary natural code and other one using the Gray code (Table 3.9): 
Table 3.9 Example of 3 bit code in BN and Gray representation 
BN Code Assigned Level Gray Code
0  0  0 
1V 
0  0  0 
0  0  1 
2V 
0  0  1 
0  1  0 
3V 
0  1  1 
0  1  1 
4V 
0  1  0 
1  0  0 
5V 
1  1  0 
1  0  1 
6V 
1  1  1 
1  1  0 
7V 
1  0  1 
1  1  1 
8V 
1  0  0 

3.3   Information Representation Codes 
65
 
When errors occur, the transition to an adjacent level is most likely to occur. 
For example: a 4V level is transmitted and the receiver shows 5V.  Using binary 
natural code we have 3 errors whereas in case of Gray code we have only one er-
ror that can easily be corrected with one error-correcting code. 
3.3.4   Pulse Code Modulation (PCM) 
The concept of PCM was given in 1938 by Alec Reeves, 10 years before Shannon 
theory of communications and transistor invention, too early to demonstrate its 
importance. This is the basic principle of digital communications which involves 
an analog to digital conversion (ADC) of the carrying information signal x(t). The 
generated signal is a digital one, characterized by a decision rate (bit rate), thus 
PCM is an information representation code; it is a digital representation of an ana-
log signal x(t). 
PCM generation is illustrated by the block scheme given in Fig 3.3.  
Band 
limiting 
filter
quantizer
Digital 
encoder
n-bits
ADC
n
q
fs>=2fM
fM
xk
xkq
PAM
x(t)
Analog 
signal
DPCM=fsn
y(t)=PCM
sampler
●
 
Fig. 3.3 Block scheme illustrating the generation of PCM signal (PCM modulator and 
coder) 
At the receiver the processing is reversed, as illustrated in Fig 3.4 and repre-
sents a digital to analog conversion (DAC) followed by a low pass filtering (LPF) 
of recovered samples 
'
x qk . 
⊗
 
Fig. 3.4 Block scheme illustrating the receiving PCM process (PCM demodulator / decoder) 

66 
3   Source Coding 
 
Remark 
In transmission, the distorted PCM signal is regenerated using regenerative repe-
tor, allowing the complete regeneration of the digital signal for very small SNR 
(<15dB), without cumulative distortions, as in analog transmissions (main advan-
tage of digital).  
As illustrated in Fig 3.3, the generation of PCM implies:  
 
• low-pass filtering the analog signal x(t) by an antialiasing analog filter whose func-
tion is to remove all the frequencies above  fM (the maximum frequency of x(t)) 
• sampling the LP filtered signal at a rate a 
M
S
f
2
f
≥
(sampling theorem); in this 
point, the analog signal x(t) is discretized in time, resulting PAM (Pulse Ampli-
tude Modulation) signal xk being the values of x(t) at discrete moments kTS , 
where, 
S
f
1
S
T =
is the 
sampling period. 
• quantization: each sample 
K
x
is converted into one of a finite number of possi-
ble values of the amplitude (q signifies the number of quantization levels) 
q
k
x
 
• encoding: the quantitized samples 
q
k
x
are encoded using a binary alphabet 
(m=2), each quantized sample being represented, usually, by a binary code 
word of length n, called PCM word. 
Thus, the infinite number of possible amplitude levels of the sampled signal is 
converted into a finite number of possible PCM words. If n represents the length 
of PCM word, the total number of possible distinct words that can be generated 
(possible amplitude values of the quantized samples) are: 
q=2n                                                       (3.9) 
An example of PCM generation is illustrated in Fig 3.5. 
110
000
001
010
011
111
100
101
6
0
1
2
3
7
4
5
Δ
Decision 
levels
Reconstruction 
levels
y(t)
Encode
1
0
1 1 1
0
1 1 1
1 1
0
0
0 0 0
0 0 0 0
0
1 1
1
PCM
t
0
Ts
2Ts
3Ts
4Ts
5Ts
6Ts
7Ts
4
7
7
6
3
0
0
2
sampling
x(t)
xq(t)
nq3
quantization
 
Fig. 3.5 Example of PCM generation 

3.3   Information Representation Codes 
67
 
We may ask: which are the causes of distortions in PCM systems? If the sam-
pling frequency is 
M
S
f
f
2
≥
, theoretically, sampling do not introduce distortions 
(in reality, this is not true, because the sampling is not ideal, Dirac distributions 
being impossible to be realized). Quantization, always introduces distortions be-
cause the real amplitude of a sample is quantized by a unique amplitude value (the 
reconstruction level), generally being the average value of the quantized interval. 
The difference between the real value of a sample and its quantized value is called 
quantization noise: nq. 
)
(kT
x
)
x(kT
:
n
S
q
S
q
−
=
                                     (3.10) 
The step size (quantum) Δ, under the assumption of uniform quantization 
(Δ=ct), is: 
q
X
Δ u =
                                                (3.11) 
for unipolar signal : x(t)∈[0, X] , and  
q
2X
Δ b =
                                             (3.12) 
for polar signal: : x(t)∈[-X, X], where |X|  is the maximum level of x(t) and q in-
dicates the quantization levels. 
If the number of quantization levels is high (fine quantization) the probability den-
sity function (pdf) of the quantization noise, f(nq), is considered uniform. (Fig 3.6) 
nqi
K
f(nqi)
0
2
Δ
+
2
Δ
−
 
Fig. 3.6 Representation of quantisation noise pdf 
In Fig 3.6 the index i is designating the i-th quantization interval. 
Since, any random sample lies somewhere between – Δ/2 and + Δ/2 with prob-
ability 1, the pdf must satisfy: 
1
dn
)
f(n
qi
2
Δ
2
Δ
-
qi
=
∫
 
/
 
/
                                         (3.13) 

68 
3   Source Coding 
 
For a constant pdf: 
f(nq)=K                                                 (3.14) 
we obtain: 
f(nqi)=K=1/ Δ                                           (3.15)  
Encoding means to express in a binary code the quantized samples . The code 
could be: Binary Natural (BN), Binary Coded Decimal (BCD), Gray etc. 
If the quantization levels q are chosen, the length of the PCM word is:  
n=ld q∈Z                                             (3.16) 
Z meaning the integer set. 
Concluding, PCM generation implies: 
 
• band limiting filtering 
• sampling of x(t) with fS ≥ 2fM 
• quantization of samples with q levels  
• encoding (for uniform encoding) the q levels using words of length n=log2q €€  Z 
Thus, an analog signal x(t), band limited fM, is transformed into a binary stream 
of bit rate: 
n
f
D
S
PCM
⋅
=
•
                                              (3.17) 
PCM is thus an information representation code: the analog signal x(t) is  repre-
sented through PCM as a binary stream  
n
f
D
S
PCM
⋅
=
•
: 
n
f
D
f
x(t),
S
PCM
ADC
M
⋅
=
⎯
⎯→
⎯
•
 
The main advantage of PCM is its great immunity to noise: the required SNR 
in transmission is much lower compared to analogue transmission because at the 
receiver, the decision concerns only two levels and not infinity as in the analogue 
case. Beside this, digital offers many other facilities: compression, error control, 
encryption etc. 
For example: a SNR of 50dB at the user, in telephony is ensured with only ap-
proximately 15dB in transmission using PCM (for aprox. 15dB of SNR, a BER of 
10-5 is obtained without error control coding – see Appendix C).  
The main disadvantage of PCM is the increase of the requested bandwidth. Ex-
ample: for telephony, the bandwidth is [300, 3400]Hz; in digital telephony, the 
sampling frequency is fS =8KHz and the length of the PCM word is n=8. Conse-
quently, the PCM bit rate is: 64Kbps. The required bandwidth (baseband) is,  
according to Nyquist theorem:
51,12KHz
D
0,8
B
PCM
PCM
=
⋅
≈
•
, which is almost 
twenty times greater than that one corresponding to the analog signal (3,1KHz).  

3.3   Information Representation Codes 
69
 
Despite this disadvantage, which can be reduced, by processing (compression, 
modulation), PCM is widely used being the basic principle in digital communica-
tions (transmission and storage). 
The main advantages of the digital, compared to the analog, are: 
 
• high quality transmission/storage in channel/storage media with very poor sig-
nal/noise ratio (SNR) 
• multiple access by multiplexing (time division multiplex access - TDMA or 
code division multiplex access - CDMA) 
• digital facilities as: compression, error control coding, encryption etc. 
 
Noise in PCM systems 
 
In PCM systems, the noise has two main sources:  quantization (during PCM gen-
eration) and decision (made at the receiver, when a decision is made concerning 
the received value: 0 or 1) 
 
a) 
Quantization noise  [12] 
This noise is defined by relation (3.10): 
)
(kT
x
)
x(kT
:
n
S
q
S
q
−
=
                                       (3.10) 
The power of the quantitizing noise 
q
N is: 
2
q
q
n
R
1
N
=
                                                  (3.18) 
2
q
n
  meaning the expected value of the noise power for q quantitizing levels: 
∑∫
⋅
=
= −
q
1
i
Δ/2
Δ/2
i
qi
qi
2
qi
2
q
P
)dn
f(n
n
n
                                 (3.19) 
where Pi is the probability that the sample xk falls within the i-th interval. Assum-
ing that all the intervals are equally probable, we have: 
q
1,
i
,
q
1
Pi
=
∀
=
                                              (3.20) 
Replacing f(nqi) with (3.15) in (3.19), we obtain: 
(
)
12
Δ
q
1
q
Δ
1
12
Δ
q
1
Δ
1
dn
n
n
2
3
q
1
i
Δ/2
Δ/2
qi
2
qi
2
q
=
⋅
⋅
⋅
=
⋅
⋅
∑∫
=
=
−
                    (3.19.a) 
Consequently the power of the quantitizing noise for uniform quantization is: 
12
Δ
R
1
P
)dn
f(n
n
R
1
N
2
q
1
i
Δ/2
Δ/2
i
qi
qi
2
qi
q
⋅
=
∑∫
⋅
⋅
=
= −
                         
(3.18.a) 

70 
3   Source Coding 
 
The quantitizing SNR is: 
q
S
q
N
P
:
SNR
=
                                            (3.21) 
S
P   defining the signal power: 
2
rms
S
X
R
1
P =
                                            (3.22) 
which finally gives: 
2
2
rms
q
Δ
X
12
SNR
=
                                    (3.21.a) 
Using (3.21.a), some other formulae can be obtained for SNRq. Replacing Δ 
with relation (3.12) – the bipolar case, we have: 
2
2
rms
2
2
rms
q
q
X
X
3
4X
X
12
SNR
⋅
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
=
                             (3.21.b) 
If q is replaced with (3.9), (3.21.b) becomes: 
2n
2
rms
q
2
X
X
3
SNR
⋅
⎟
⎠
⎞
⎜
⎝
⎛
=
                                       (3.21.c) 
Defining as crest factor C the ratio: 
rms
X
X
:
C =
                                                    (3.23) 
it is possible to write: 
2
2
q
C
q
3
SNR
=
                                            (3.21.d) 
If the SNRq is expressed in dB, the relation (3.21.c) can be written as: 
[
]
X
X
20lg
6n
4,7dB
dB
SNR
rms
q
+
+
=
                            (3.21.e) 
Relation (3.21.e) is very practical, indicating that each supplementary bit of a 
PCM word enhance with 6dB the SNRq. 
 
Remarks 
Relation (3.21.a) shows that for an uniform quantization (Δ=ct), the quantization 
SNR is signal dependent, being an important disadvantage of uniform quantizers. 
The solutions to make a SNRq signal x(t) independent, are: 

3.3   Information Representation Codes 
71
 
• non-uniform quantization, meaning the use of small steps (ΔĻ) for weak signals 
(x(t) Ļ) and large steps(ΔĹ) for strong signals (x(t) Ĺ) 
• companding the signal dynamic range: it makes a compression before quantiza-
tion and an expansion after DAC in order to obtain the original dynamic range 
(Fig. 3.7) 
compression
ADC
DAC
expanssion
x(t)
z(t)
y(t)
y*(t)
f1
f-1
Uniform quantization
Non-uniform quantization
 
Fig. 3.7 Companding illustration 
Companding means: 
 
• compression 
y=f1(x)                                                   (3.24)  
• uniform quantization with q intervals 
y*=f2(y)=f2(f1(x))                                           (3.25) 
• extension: 
z=f-1(y*)=f-1(f2(f1(x)))=f3(x),                                   (3.26) 
representing the non-uniform quantization characteristic. The quantization step Δ 
is signal dependent and is given by the compression characteristic: 
Δ(x)
Δ
dy
dx
D
=
                                              (3.27) 
An ideal non-uniform quantization implies: 
Δ(x)=ct|x|                                               (3.28) 
Replacing Δ(x) in (3.27), we have: 
|
x
|
dx
Δ
K
dx
|
x
|
ct
Δ
dy
D
D
⋅
=
=
                                   (3.29) 
This differential equation has as solution a logarithmic characteristic 
X
|
x
|
lg
KΔ
X
|
y
|
D
+
=
,                                         (3.30) 
graphically represented in Fig 3.8. 

72 
3   Source Coding 
 
x
y
 
Fig. 3.8 Ideal compression characteristic 
Remark 
The characteristic (3.30) is impossible to be practically implemented because if 
xĺ0 => yĺ∞. The technical solutions for a non uniform quantization are ap-
proximations of this ideal characteristic. 
In PCM systems, the compression characteristics (standards) are [3]: A law – for 
Europe, μ law – for North America, Japan, with the corresponding characteristics: 
⎪
⎪
⎩
⎪⎪
⎨
⎧
≤
≤
+
+
≤
≤
+
=
1
x
A
1
,
lnA
1
lnAx
1
A
1
x
,0
lnA
1
Δx
y
                                     (3.31) 
(
)
(
)
μ
1
ln
μx
1
ln
y
+
+
=
                                               (3.32) 
b) 
Decision noise 
This noise occurs if PCM words are erroneous, meaning that the corresponding 
quantized samples will be erroneous too. We will discuss this problem under the 
following hypotheses: 
 
• independent errors 
• p is the BER of the channel. 

3.3   Information Representation Codes 
73
 
• the noise corrupts only one bit in a word of length n (fulfilled in usual  
application) 
• the code is BN (the worst from this point of view, but the most used) 
 
Example 3.1 
Be the correct PCM word: (1 1 1 1), the first left bit being MSB. 
The corresponding quantized sample is: 
15V
Δ
)
2
1
2
1
2
1
2
1(
0
1
2
3
=
⋅
⋅
+
⋅
+
⋅
+
⋅
 
if Δ is assumed to be 1V. 
Assume that 1 error occurs in a four bits length; the errors are very different, 
according the affected bit, as follows: 
 
Received word 
MSB       LSB 
kq
x
 
Error 
1   1   1   0 
14V 
1V 
1   1   0   1 
13V 
2V 
1   0   1   1 
11V 
3V 
0   1   1   1 
7V 
8V 
 
Decision noise 
d
n is defined as: 
'
x
x
:
n
kq
kq
d
−
=
                                             (3.33) 
where 
kq
x
 designates the correct quantized sample and 
'
x kq  the erroneous one. 
Under the given hypotheses, we have the probability of the erroneous word: 
np
p)
p(1
C
p
1
n
i
n
w
≈
−
=
−
                                    (3.34) 
Decision noise if the error is situated on the i-th position, 
1
n
0,
i
−
=
, is 
⎪⎩
⎪⎨
⎧
⋅
+
+
+
+
=
⋅
+
+
+
+
=
−
−
−
−
Δ
)
2
a
...
2
a
...
q
a(
'
x
Δ
)
2
a
...
2
a
...
q
a(
x
0
0
i
i
1
n
1
n
kq
0
0
i
i
1
n
1
n
kq
                          (3.35) 
Δ
)2
a
(a
n
i
i
i
di
⋅
−
=
                                         (3.36) 
Decision noise power Nd is: 
w
2
d
d
p
n
R
1
N
=
                                           (3.37) 
 

74 
3   Source Coding 
 
where 
2
d
n
means the expected value of the decision noise power: 
(
)
di
1
n
0
i
2
di
2
d
n
f
n
n
∑
=
−
=
                                          (3.38) 
where (
)
di
n
f
indicates the pdf of the decision noise, assumed equally probable for 
each position:  
(
)
n
1
n
f
di =
                                                 (3.39) 
Based on (3.36), (3.38), and (3.31), we obtain: 
n
Δ
3
1
2
2
n
Δ
Δ
2
n
1
n
2
2n
1
n
0
i
2i
2
2
1
n
0
i
2i
2
d
⋅
−
=
∑
=
∑
=
−
=
−
=
 
and  
np
3
1
2
n
Δ
R
1
N
2n
2
d
⋅
−
⋅
⋅
=
                                     (3.37.a) 
or 
(
)
2
2
2
2
d
q
3
Δ
p
R
1
1
q
3
Δ
p
R
1
N
⋅
⋅
⋅
≈
−
⋅
⋅
=
                          (3.37.b) 
The total noise in the PCM systems is: 
;
q
3
Δ
p
12
Δ
R
1
N
N
N
2
2
2
d
q
t
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
+
=
+
=
                             (3.40) 
with the mention that the two noise sources are independent. 
The total SNR in the PCM systems is: 
2
2
rms
2
2
2
2
2
rms
t
t
4pq
1
X
Δ
12
q
3
Δ
p
12
Δ
R
1
X
R
1
N
S
SNR
+
=
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
+
=
=
;                   (3.41) 
If we replace Δ  with (3.12) we obtain: 
)
4pq
(1
C
q
3
SNR
2
2
2
t
+
=
;                                    (3.42) 
 
 
 

3.3   Information Representation Codes 
75
 
Remarks 
• In PCM systems the total SNR (the detected SNR) is determined by: 
a) quantization noise and 
b) decision noise 
• The quantization noise is PCM is word length (n) dependent and can be re-
duced increasing the number of bits. Decision noise is channel dependent and 
can be reduced increasing the SNR input in the channel (SNRi). At low level of 
SNRi, the value of p is large and SNRt is dependent of the SNRi. This is the 
decision noise limited region, operating under a threshold (SNRi0 ≈15 dB). 
Over this threshold p is negligible and SNRt is limited to a value given by the 
quantization noise, based on the PCM codeword length. This is the quantization 
noise limited region (see Fig 3.9). 
 
Fig. 3.9 Threshold effect in PCM systems 
• Quantization noise in PCM cannot be eliminated, being linked to the quantiza-
tion process. The reconstructed signal 
(t)
x′
, obtained after the final lowpass fil-
tering of the received quantized samples (
kq
x
) assumed error free in transmis-
sion (without decision noise) is only an approximation of the original signal 
x(t) . The difference 
(t)
x - 
x(t)
′
 is called distortion. It can be reduced if n, the 
length of PCM word, is increased, which implies bandwidth increase also. A 
trade is made between fidelity and quality, known as the acceptable loss and 
described by rate distortion theory [4].   
 
Example 3.2 
Consider a 1kHz sinusoidal voltage of 1V amplitude. Determine the minimum 
number of quantizing levels required for a SNR of 33dB. Which is the length of 
the corresponding PCM word, assuming that in transmission BER is 
2
10−. Calcu-
late the total SNR. 
 
 
 

76 
3   Source Coding 
 
Solution 
The signal: 
t
10
sin2
ft
Xsin2
x(t)
3
π
π
=
=
 
The quantizing SNR given by (3.23.e) is: 
 
X
X
20lg
6n
4.7dB
[dB]
SNR
rms
q
+
+
=
= 
5
6
31.4
n
32
6n
1.6
33
1
2
1/
20lg
6n
4.7
≅
=
⇒
=
+
⇒
=
+
+
=
 
 
10kbps
5
10
2
n
f
D
2kHz;
2f
f
3
S
PCM
M
s
=
⋅
⋅
=
=
=
=
•
 
Based on (3.40) we have: 
15.64dB
)
2
10
4
2(1
2
3
)
4p2
2(1
2
3
)
4pq
c(1
q
3
SNR
10
2
10
2n
2n
2
2
t
=
⋅
⋅
+
=
+
=
+
=
−
 
3.3.5   Genetic Code 
Genetic information, meaning all the set of instructions necessary and sufficient to 
create life, is stored in the DNA (DeoxyriboNucleic Acid) of a cell [7]. 
Gene, located in DNA, is the basic unit of heredity and contains both coding 
sequences (exons) and non-coding sequences (introns), that determine when a 
gene is active (expressed). 
Gene expression [32] is given by the central dogma of molecular biology 
(given by Francis Crick in 1958): 
 
 
 
DNA, is a huge molecule (macro-molecule of more than 3,2*109 bp long for 
homo-sapiens), composed of two twisted complementary strands of nucleotides, 
linked in a double-helix structure, based on the hydrogen bonds: A=T and G≡T. 
This structure was discovered by J. Watson, F. Crick, M. Wilkins and Rosalind 
Franklyn in 1953 and priced with Nobel Prise in 1962.  
DNA 
mRNA
tRNA
protein 
transcription 
translation

3.3   Information Representation Codes 
77
 
 
Fig. 3.10 DNA structure 
 
DNA is a very stable medium, allowing long term storage of genetic informa-
tion, based on the lack of hydroxyl group OH and on the complementarity of 
bonds A=T, G≡T, offering thus error control properties (if a mutation occurs in 
one strand, it could be corrected based on the complementarity of strands). 
A nucleotide is composed of three groups: 
 
• a phosphate P group, linked to 
• a pentose sugar (D-deoxyribose) linked to 
• one of four nitrogenous N base: A – Adenine, C – Cytosine, G – Guanine and T 
– Thymine. 
 
The four nucleotides making DNA have different bodies, but all have the same 
pair of terminations: 5’ - P – phosphoryl and 3’ – OH – hydroxyl. 
The nucleotide stream is encoding the information necessary to generate 20  
distinct amino-acids, which are the building blocks of proteins. The identity of a 
protein is given not only by its composition, but also by the precise order of its 
constituent amino-acids. 
RNA (RiboNucleicAcid) is a polymer very similar to DNA, but having some 
main differences: 
 
• sugar deoxyribose (D) is replaced with ribose (R); it explains the great instabil-
ity of RNA: alcohol groups OH are highly instable to enzymes, allowing a 
quick adaptation of RNA molecule to the environment stimuli (hormones, nu-
trients). This explains its use for gene expression. 
• base Thymine (T) is replaced by Uracil (U); 
• its structure is simple stranded, having the capacity of making secondary struc-
tures like DNA (tRNA - transfer RNA), if the simple strand has polyndromic 
structure (that reads the same in both directions) regions [10].  
 
During transcription, when a gene is expressed, the coding information found 
in exons is copied in messenger RNA (mRNA).  
By translation, the genetic information of mRNA is decoded, using ribosomes 
as reading units, each codon being translated into an aminoacid (tRNA) and an 
aminoacid chain is giving a protein. 
Thus genetic code is defined either as DNA or RNA, using base T or U  
respectively. 
Proteins are macromolecules of 100 – 500 amino-acids known as residuus.  

78 
3   Source Coding 
 
In order to be able to encode 20 distinct aminoacids, using a 4 letter alphabet 
(A, T/U, C, G), a minimum 3 nucleotides sequence, called codon is necessary 
(42=16<20, 43=64>20). 
Genetic code [10] defines the mapping between the 64 codons and amino-acids. 
Table 3.10 represents the encoding table: codons – amino-acids and Table 3.11 the 
decoding table: amino-acids – codons. 
Table 3.10 Genetic code – encoding table 
 
U 
C 
A 
G 
 
U
UUU Phe (F) 
UUC Phe (F) 
UUA Leu (L) 
UUG Leu (L) 
UCU Ser (S) 
UCC Ser (S) 
UCA Ser (S) 
UCG Ser (S) 
UAU Tyr (T) 
UAC Tyr (T) 
UAA  Stop 
UAG  Stop 
UGU Cys (C) 
UGC Cys (C) 
UGA  Stop 
UGG Trp (W) 
 
C
CUU Leu (L) 
CUC Leu (L) 
CUA Leu (L) 
CUG Leu (L) 
CCU Pro (P) 
CCC Pro (P) 
CCA Pro (P) 
CCG Pro (P) 
CAU His (H) 
CAC His (H) 
CAA Gln (Q) 
CAG Gln (Q) 
CGU Arg (R) 
CGC Arg (R) 
CGA Arg (R) 
CGG Arg (R) 
 
A
AUU Ile (I) 
AUC Ile (I) 
AUA Ile (I) 
AUG Met (M) 
ACU Thr (T) 
ACC Thr (T) 
ACA Thr (T) 
ACG Thr (T) 
AAU Asn (N) 
AAC Asn (N) 
AAA Lys (K) 
AAG Lys (K) 
AGU Ser (S) 
AGC Ser (S) 
AGA Arg (R) 
AGG Arg (R) 
 
G
GUU Val (V) 
GUC Val (V) 
GUA Val (V) 
GUG Val (V) 
GCU Ala (A) 
GCC Ala (A) 
GCA Ala (A) 
GCG Ala (A)
GAU Asp (D) 
GAC Asp (D) 
GAA Glu (E) 
GAG Glu (E) 
GGU Gly (G) 
GGC Gly (G) 
GGA Gly (G) 
GGG Gly (G) 
 
Table 3.11 Genetic code – decoding table 
Ala/A GCU, GCC, GCA, GCG 
Leu/L  UUA, UUG, CUU, CUC, 
CUA, CUG 
Arg/R CGU, CGC, CGA, CGG, 
AGA,AGG 
Lys/K AAA, AAG 
Asn/N AAU, AAC 
Met/M AUG 
Asp/D GAU, GAC 
Phe/F UUU, UUC 
Cys/C UGU, UGC 
Pro/P CCU, CCC, CCA, CCG 
Gln/Q CAA, CAG 
Ser/S UCU, UCC, UCA, UCG, 
AGU, AGC 
Glu/E GAA, GAG 
Thr/T ACU, ACC, ACA, ACG 
Gly/G GGU, GGC, GGA, GGG 
Trp/W UGG 
His/H CAU, CAC 
Tyr/Y UAU, UAC 
Ile/T 
AUU, AUC, AUA 
Val/V GUU, GUC, GUA, GUG 
START AUG 
STOP UAA, UGA, UAG 
Legend: A – Alanine, F – Phenylanine, S – Serine, Y – Tyrosine, C – Cysteine, L – Leucine, W 
– Tryptophan, P – Proline, H – Histidine, R – Arginine, Q - Glutamine, I – Isoleucine, T – 
Threonine, N – Asparagine, K – Lysine, M - Methionine, V – Valine, D – Aspartic acid, G – 
Glycine, E – Glutamic acid. 

3.3   Information Representation Codes 
79
 
Table 3.9 and 3.10 represent the standard canonical genetic code. It is not uni-
versal: there are organisms where the synthesis of proteins relies on a genetic code 
that varies from the standard one. The discovery of genetic code, based on a triplet 
codon brought to A. Sanger the second Nobel Prize in 1970.  
The main features of the genetic code are: 
 
• Redundancy or degeneracy: the same aminoacid can be encoded by distinct 
codons: e.g. Alanine/A is specified by codons: GCU, GCC, GCA and GCG.  
X This situation defines the four fold degenerate site: the third position 
can be any of the four possible (A, C, G, U) and all the four codons are 
giving the same aminoacid. 
X A two-fold degenerate site is defined if only two of four possible nu-
cleotides in that position give the same aminoacid.  
E.g.: Asparagine (Asn/N) is specified by: AAU, AAC (two pyrimidi-
nes C/U) 
Aspartic acid (Asp/D) is specified by: GAU, GAC (two purines 
A/G) 
X Three-fold degenerate site is defined when changing 3 of four nucleo-
tides, the same aminoacid is obtained. 
E.g.: (the only one) Isoleucine (Ile/I) – AUU, AUC, AUA. 
X Six codons define the same aminoacid: 
e.g.: 
Arg/R, Ser/S, Leu/L. 
X Three STOP codons: UAA, UGA, UAG called also termination or non-
sense codons, which signal ending of polypeptide (protein) generated 
by translation. 
X Only two codons define a unique aminoacid: AUG – Methionine which 
also specify the START of translation and UGG – Tryptophane. 
• From coding point of view, redundancy means that the number of code words 
(codons): 43 = 64 is larger than the number of the messages to be encoded: M = 
20 amino-acids + 1 START + 1 STOP = 22. 
• The difference: 64 – 22 = 42 combinations (codons) are redundant and define 
the degeneracy of the code. Redundancy (see Cap. 5) allows error protection. 
Degeneracy of the genetic code makes it fault-tolerant for point mutations. 
e.g. four fold degenerate codons tolerate any point mutation on the third po-
sition meaning that an error (silent mutation) would not affect the protein. 
• Since 2004, 40 non-natural amino-acids has been added into protein, creating a 
uniquely decodable genetic code in order to be used as a tool in exploring pro-
teins structure and functions or to create novel enhanced proteins [44], [42]. 
Bioinformatics, the “computational branch of molecular biology” or “in silico-
biology [10], started 40 years ago when the early computers were used to analyse 
molecular segments as texts, has tremendously evolved since than, being at the 
centre of the greatest development in biology and other connected fields: deci-
phering of human genome, new biotechnologies, personalized medicine, bioar-
cheology, anthropology, evolution and human migration. 

80 
3   Source Coding 
 
3.4   Coding Efficiency: Compression Ratio 
The correspondence between si and ci achieved by coding can be accomplished in 
various ways using the alphabet X. To choose a code from more possible ones can 
be done if an optimisation criterion is used. The optimisation criterion for a noise-
less channel is the transmission cost, which is desired to be minimal, thus a mini-
mum number of symbols entering the channel. For information storage systems, 
the optimisation criterion is the storage space, which as in the transmissions case, 
is desired to be minimal. 
We define the length of a word (li) as the number of letters that compose a 
codeword, in order to be able to make remarks upon this criterion. Each letter is 
assumed to have the same duration, supposed 1. 
Average length of the codewords (l ), is the average value of these lengths: 
∑
=
=
M
1
i
i
il
p
l
                                                   (3.43) 
Taking into account that, by encoding, to every message si a correspondent 
codeword ci is made, we have: 
( )
( )
M
1,
i
   
,
p
c
p
s
p
i
i
=
=
=
i
                                     (3.44) 
The transmission cost is proportional to l ; therefore it is desirable to get min
l
 
by encoding. 
The question that raises now is: 
?
min =
l
 
Under the assumption of a DMS, the average information quantity per symbol 
is H(S) and it is equal to the average information quantity per codeword ci; hence: 
∑
−
=
=
=
M
1
i
i
2
i
p
log
p
H(C)
H(S)
                                    (3.45) 
The average quantity of information per each code letter is H(X): 
∑
∑
−
=
−
=
=
=
m
1
j
m
1
j
j
2
j
j
2
j
p
log
p
)
p(x
)log
p(x
H(X)
                       (3.46) 
It follows  
H(X)
l
H(S)
⋅
=
                                          (3.47) 
known as entropic or lossless compression relation. 
Finally, we get: 
H(X)
H(S)
l =
                                               (3.48) 

3.4   Coding Efficiency: Compression Ratio 
81
 
The minimal value for l  is obtained for 
D(X)
(X)
H
H(X)
max
=
=
, meaning 
the use of the alphabet X with equal probability: 
m
1,
i 
,
p 
m
1
io
=
∀
=
, which leads 
to: 
m
 
log
H(S)
l
2
min =
                                          (3.49) 
Remark 
1. The same condition
m
1,
i 
,
p 
m
1
io
=
∀
=
, known as statistical adaptation of the 
source to the channel (see relation 3.4) is obtained, this time from coding the-
ory point of view.   
2. Relation (3.49) is valid only when the alphabet X is memoryless; however, this 
is not always the case [6].  
3. Equations (3.47) and (3.49) show that encoding a DMS, its entropy is pre-
served, the source being changed into another one, X, with maximum entropy. 
This justifies the denomination of this type of compression as entropic or loss-
less compression. 
Coding efficiency (η ) is defined as: 
m
 
ld
 l
H(S)
l
l
η
min =
=
                                            (3.50) 
Another measure, more often used in compression techniques that also empha-
sises the coding efficiency, is the compression ratio RC defined as the ratio be-
tween the length of the codewords in uniform encoding lu and the average length 
of the codewords obtained by encoding: 
l
l
R
u
C =
                                               (3.51) 
The codes for which 
min
l
l =
 are named absolute optimal codes (AOC). For 
AOC we have 
ldm
D(X)
(X)
Hmax
=
=
 
so the symbols xj have equal probabilities: 
m
1,
j
  
1/m,
)
p(x j
=
∀
=
. 
Taking into account that 
)
p(c
)
p(s
i
i =
, it follows that for a DMS (symbols xj 
are assumed independent) we have: 
( )
il
i
m
1
c
p
⎟
⎠
⎞
⎜
⎝
⎛
=
                                           (3.52) 
Using the following equalities 
( )
( )
∑
∑
∑
=
∑
=
⎟
⎠
⎞
⎜
⎝
⎛
=
=
=
=
=
=
−
M
1
i
M
1
i
M
1
i
M
1
i
l
l
i
i
1
m
m
1
c
p
s
p
i
i
 

82 
3   Source Coding 
 
we also get: 
1
m
M
1
i
il =
∑
=
−
                                            (3.53) 
This relation is valid for AOC and it shows the connection that must exist be-
tween the lengths li and the code alphabet m for absolute optimal codes. 
 
Remark 
If (3.53) is fulfilled for a particular code, it does not necessarily mean that the 
code is an absolute optimal one; however, it states that for a given alphabet m, for 
words of lengths li that fulfils (3.53), an AOC can always be built. 
 
Example 3.3 
Source S is encoded in two different ways as follows: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
0,125
     
0,125
     
0,25
    
0,5
s 
          
s
         
s
       
s
:
S
4
3
2
1
 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
1
   
  
2 
    
3 
   
3
0
      
10
      
110
     
111
:
1
1
il
C
 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
3
   
  
3 
    
2
   
1
111
      
110
      
10
     
0
:
2
2
il
C
 
One must notice that (3.53) is fulfilled for both C1 and C2: 
2-1 + 2-2 + 2-3 + 2-3 = 1, 
but even so, only C2 is an AOC 
1
η
1
2,6
1,75
2
ld
 l
H(S)
η
2
1
=
<
=
=
 
3.5   Existence Theorem for Instantaneous and Uniquely 
Decodable Codes (Kraft and McMillan Inequalities) 
3.5   Existence Theorem for Instantaneous and U niquely Decodable Codes 
Theorem T1 
The necessary and sufficient condition for instantaneous codes existence is: 
1
m
M
1
i
il ≤
∑
=
−
                                                  (3.54) 
This relationship is also known as Kraft inequality. 
 
Proof 
 
a) 
necessity: 
We will show that (3.54) is fulfilled for an instantaneous code of base m with 
words of length l1 ≤ l2 ≤....... ≤ lM . 

3.5   Existence Theorem for Instantaneous and Uniquely Decodable Codes 
83
 
The code can be drawn as a graph of order m and size lM. Being an instantane-
ous code, it means that no codeword must be a prefix of another codeword, so that 
once a codeword is identified on the tree, it is impossible that another codeword 
exist on the incident branches representing the identified codeword. 
This mean that for every codeword of length li, m
il
M
l
− ends of the tree are  
excluded. 
For the given code, the total number of excluded ends is: 
∑
=
−
M
1
i
l
l
i
M
m
 
and taking into account that the number of ends for a tree of size lM is mlM , one 
of the following two inequalities must be true: 
 
M
i
M
l
M
1
i
l
l
m
m
≤
∑
=
−
, or: 
1
m
M
1
i
li ≤
∑
=
−
 
b) 
sufficiency: 
It will be shown that using an m base alphabet and taking lengths li so that l1 ≤ 
l2 ≤....... ≤ lM, an instantaneous code can be built, under the assumption that Kraft 
inequality holds. 
Let us imagine an m order graph of size lM. An end of this graph is considered 
to be a codeword of length l1, and thus m
1
M
l
l − ends eliminated. But taking into 
consideration that 
M
i
M
l
M
1
i
l
l
m
m
≤
∑
=
−
, at least one end that can be considered a 
codeword of length l2 still remains; therefore we have: 
m
m
m
M
2
M
1
M
l
l
l
l
l
≤
−
+
−
 
The algorithm is applied for codeword of lengths l3, l4, …. 
 
Theorem T2  
Existence theorem for uniquely decodable codes, McMillan theorem: Kraft ine-
quality, (3.54), is a necessary and sufficient condition for UDC existence. 
 
The proof of this theorem is similar to the one made for Kraft theorem [33]. 
 
Theorems T1 and T2, given by (3.54) are existence theorems; they show that for a 
given alphabet m, with lengths li, obeying (3.54), instantaneous or uniquely de-
codable codes can be built: notice that these theorems do not give algorithms for 
IC or UDC codes. 

84 
3   Source Coding 
 
3.6   Shannon First Theorem (1948) 
As shown in 3.4 (AOC), for the special case when source messages have certain 
particular values as: 
( )
( )
il
i
i
i
m
c
p
s
p
p
−
=
=
=
                                      (3.55) 
the coding efficiency is maximum (η=1). From (3.55) it follows that: 
set)
(integer 
 
Z
m
log
p
log
l
2
i
2
i
∈
−
=
                                  (3.56) 
For a source with a random set of probabilities, the ratio given by (3.56) is not 
an integer and therefore will be rounded upwards until obtaining the closest supe-
rior integer: 
i
2
i
2
i
2
i
2
p
1
m
log
p
log
l
m
log
p
log
⋅
+
−
<
≤
−
                                (3.57) 
∑
+
−
<
≤
−
=
M
1
i
i
2
i
2
i
i
i
2
i
2
i
p
m
log
p
log
p
p
l
m
log
p
log
p
 
∑
∑
∑
+
∑
−
≤
≤
−
=
=
=
=
M
1
i
M
1
i
M
1
i
i
M
1
i
2
i
2
i
i
i
2
i
2
i
p
m
log
p
log
p
p
l
m
log
p
log
p
 
1
m
log
H(S)
l
m
log
H(S)
2
2
+
<
≤
                                    (3.58) 
Relationship (3.58) is valid for any DMS, and in particular for the n-th exten-
sion of the source S, for which
nH(S)
)
H(Sn =
; it follows that:  
1
m
log
nH(S)
l
m
log
nH(S)
2
n
2
+
<
≤
,  
where ln  is the average length obtained encoding the source extension Sn. 
n
1
m
log
H(S)
l
n
l
m
log
H(S)
2
n
2
+
<
=
≤
                              (3.59) 
For
∞
→
n
, we obtain: 
min
2
n
n
l
m
log
H(S)
l
n
l
lim
=
=
=
∞
→
                                    (3.60) 
Relation (3.60) represents Shannon first theorem or noiseless channels coding 
theorem; it shows that when coding on groups of n symbols, an absolute optimal 
encoding can be achieved for any source S, under the assumption: 
∞
→
n
. This 

3.7   Losseless Compression Algorithms 
85
 
was expected due to the fact that when coding on groups of symbols, rounding 
upwards refers to the entire group and thus the rounding corresponding to only 
one symbol is smaller than the one obtained when coding symbol by symbol, so 
we will approach as much as possible to min
l
.    
 
Example 3.4 
Be 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
0.3
0.7
s
s
:
S
2
1
 encoded with Ca={0;1}.  
It follows that: 
 
 
0,88
η 
1,
l
l
bits/symbo
  
0,88
H(S)
l
a
a
min
=
=
=
=
 
 
The 2nd order extension of the source is: 
 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
0.09
0.21
0.21
0.49
s
s
s
s
s
s
s
s
:
S
2
2
1
2
2
1
1
1
2
encoded with  
Cb : (1 01 000 001) 
In this case we get: 
8,1
lb =
 
Relationship (3.21) becomes: 
a
b
n
l
0,9
2
1,8
n
l
n
l
<
=
=
=
; one may notice a de-
crease of the average length per symbol when coding on groups of two symbols. 
We invite the reader to check coding efficiency improvements when coding on 
groups of n=3 symbols. 
3.7   Losseless Compression Algorithms 
Shannon first theorem shows that for any source S, an absolute optimal coding is 
possible, if performed on the n-th order extension, with
∞
→
n
.  
In practice n is finite. The algorithms called optimal algorithms are ensuring an 
encoding for which 
min
l
l →
. 
The basic idea when dealing with optimal algorithms is to associate to high 
probabilities pi, short codewords (small li), and conversely fulfilling, obviously, 
Kraft inequality. 
 
Remark 
This basic idea for optimal codes is not new, a relevant example being Morse 
code, dating from 1837. 
3.7.1   Shannon-Fano Binary Coding 
Shortly after stating its first theorem, Shannon gave the first optimal coding algo-
rithm, in 1949; R.M. Fano, simultaneously, proposed the same algorithm and this 
is why the algorithm is known as Shannon – Fano algorithm. 

86 
3   Source Coding 
 
Although efficient, this algorithm was proved not to be the best for any kind  
of source; Huffman algorithm, invented later, proved to be the best one, and thus 
optimal. 
The main steps of Shannon-Fano algorithm are: 
 
1. Put the set 
M
1,
i
},
{s
S
i
=
=
 in decreasing order of probabilities. 
2. Partition S into two subsets (S0, S1) of equal or almost equal probabilities 
2
/
1
)
P(S
)
P(S
1
0
≅
≅
. 
3. Repeat step 2 for subsets S0 and S1 and get new subsets S00, S01 and S10, S11 re-
spectively, of probabilities as close to 1/4. Repeat until each subset contains 
only one message si. 
4. Assign 0 to S0 and 1 to S1 or vice versa such that the codewords assigned to the 
symbols corresponding to S0 begin with 0 and the codewords assigned to the 
symbols corresponding to S1 begin with 1. 
5. Repeat step 4 until the last subsets that contain one message. 
 
Example 3.5 
Be a DMS given by the probability mass function: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
1/8
    
1/8
     
1/4
    
1/2
s
     
s
      
s
     
s
:
S
4
3
2
1
 
Determine: 
a) binary Shannon-Fano code corresponding to the source. 
b) the code tree. 
c) encoding efficiency and the compression ratio. 
d) statistical adaptation of the source to the channel, made by encoding. 
 
Solution 
a) 
si 
pi 
Partitions 
 
ci 
li 
li0 
li1 
s1 
1/2 
0 
0 
1 
1 
0 
s2 
1/4 
0 
10 
2 
1 
1 
s3 
1/8 
0 
110 
3 
1 
2 
s4 
1/8 
 
1 
1 
1 
111 
3 
0 
3 
 
b) The graph corresponding to the code is given in Fig. 3.11 
 
 
10
110
111 
1
0 
1
1 
0 
0
0
11
1 
 
Fig. 3.11 The coding tree of Example 3.5 

3.7   Losseless Compression Algorithms 
87
 
One may notice that an instantaneous code is obtained. 
 
c) 
∑
=
×
+
×
+
×
+
×
=
=
=
M
1
i
i
i
1,75
3
1/8
3
1/8
2
1/4
1
1/2
l
p
l
 
1.75
p
log
p
H(S)
m
log
H(S)
l
i
2
M
1
i
i
2
min
=
∑
−
=
=
=
=
 
100%,
1
1.75
1.75
l
l
η
min
c
=
=
=
=
 
 
therefore the code is absolute optimal; this could have been easily noticed even 
from the beginning as the source probabilities lead to integer lengths li: 
 
- log2 p1= - log2 1/2=1=l1 
- log2 p2= - log2 1/4=2=l2 
- log2 p3= - log2 1/8=3=l3 
- log2 p4= - log2 1/8=3=l4 
 
l
l
R
u
C =
  
where lu is determined from: 
M
m ul
≥
                                               (3.61) 
It follows that: 
ldm
ldM
lu =
                                             (3.62) 
We get lu = 2, therefore RC = 2 / 1,75 = 1,14 
 
d) Statistical adaptation of the source to the channel involves the determination 
of the probabilities corresponding to the code alphabet, therefore of p(0) and p(1), 
which must be as close as possible; for an AOC we must have p(0)=p(1)=1/2. The 
two probabilities are computed using the equations: 
∑
∑
+
∑
=
+
=
=
=
=
M
1
i
M
1
i
1i
i
0
i
i
M
1
i
0
i
i
1
0
0
l
p
l
p
l
p
N
N
N
p(1)
                             (3.63) 
∑
∑
+
∑
=
+
=
=
=
=
M
1
i
M
1
i
1i
i
0
i
i
M
1
i
1i
i
1
0
1
l
p
l
p
l
p
N
N
N
p(1)
                            (3.64) 
where N0 and N1 are the average number of zeros and ones used in source coding. 
We obtain p(0)=p(1)=1/2 and therefore the statistical adaptation of the source to 
the channel was accomplished through encoding point of view; this means that 

88 
3   Source Coding 
 
such a source will use the binary symmetric channel at its full capacity (informa-
tion theory point of view). 
3.7.2   Huffman Algorithms 
Binary static Huffman coding (1952) 
 
Huffman algorithm (1952) is an optimal one, meaning that no other algorithm en-
sures a shorter average length. There are cases when other algorithms can provide 
an average length equal to Huffman one, but never smaller. 
The steps of this algorithm are: 
 
1. Put the source S={si} messages in decreasing order of probabilities. 
2. Combine the two least probable messages into a reduced message 
M
1
M
1
s
s
r
∪
=
−
 having the probability: 
)
p(s
)
p(s
)
p(r
M
1
M
1
+
=
−
. 
3. Include the message r1 into the remaining messages set and put this set in de-
creasing order of probabilities, obtaining the ordered stream R1. 
4. Repeat the reduction algorithm until the last ordered stream Rk contains only 
two messages Rk={rk-1 ,rk}. 
5. The codewords corresponding to each message are obtained as follows: 
– 
assign ‘0’ to the symbol rk-1 and ‘1’ to rk 
– 
assign ‘0’s and ‘1’s to each restriction until singular messages (si) are  
obtained. 
Example 3.6 
 
a) Encode using Huffman binary algorithm the source S: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
0.05
0.10
0.15
0.15
0.25
0.3
s
s
s
s
s
s
:
S
6
5
4
3
2
1
 
b) Calculate the compression efficiency and the compression ratio  
 
c) 
Show the fulfilment of lossless compression relation. Discussion. 
 
Solution 
a) 
 
 
 
 
Restrictions 
si 
pi 
ci 
li 
R1 
R2 
R3 
R4 
s1 
0,3 
00 
2 
0,3 
0,3 
0,4 
  0,6  0 
s2 
0,25 
10 
2 
0,25 
0,3 
0,3  00 
  0,4  1 
s3 
0,15 
11 
2 
0.15 
0,25 10 
0,3  01 
 
s4 
0,15 
010 
3 
0,15    010 
0,15 11 
 
 
s5 
0,10 
0110 
4 
0,15    011 
 
 
 
s6 
0,05 
0111 
4 
 
 
 
 

3.7   Losseless Compression Algorithms 
89
 
b) 
  
∑
=
=
=
6
1
i
i
i
45
2.
l
p
l
 
∑
=
−
=
=
6
1
i
i
2
i
2.39
p
log
p
H(S)
 
98%
0.98
2.45
9
2.3
η
=
=
=
 
1.225
2.45
3
l
l
R
u
c
=
=
=
 
c) Using (3.63) and (3.64), we obtain: p(0)≈0.6, p(1)≈0.4 and H(X)=0.9785.  
The lossless compression relation (3.47) becomes:  
2.3973.
0.9785
2.45
H(X)
l
H(S)
2.39
=
⋅
=
⋅
=
=
 
Discussion: 
Relation (3.47) was given under the assumption of S and X memoryless 
sources. Encoding introduces a slight memory, meaning that X is not com-
pletely memoryless. 
 
Non-binary static Huffman encoding (m>2) 
 
Huffman also presented the general case of its algorithm for a channel with an al-
phabet containing more than two symbols (m>2). Unlike for binary coding, when 
coding in a larger m base, each reduction will be formed from m symbols. In this 
case, after the first reduction we obtain a new source having M-(m-1) symbols, 
and after the last reduction (of order k) it has M-k(m-1) symbols. It must be under-
lined, that for an accurate encoding, the last reduction must contain exactly m 
elements, therefore: 
M-k(m-1)=m                                              (3.65) 
When the number of source messages does not allow an accurate encoding in 
the sense given by (3.65), this number is increased, the new messages having zero 
probabilities and therefore not affecting the initial source. 
 
Example 3.7 
Encode the source from Example 3.6 using a ternary alphabet (m=3) and deter-
mine the efficiency and the compression ratio. 
 
Solution 
The number of messages must check (3.65). 
Z
k
7,
3
2
2
3
1)
k(3
m
1)
k(m
M
∈
=
+
×
=
+
−
=
+
−
=
 
 
 
 

90 
3   Source Coding 
 
Note that a supplementary symbol s7 of zero probability must be added. 
 
si 
pi 
ci 
R1 
R2 
s1 0,3 
1 
s2 0,25 
2 
s3 0,15 
00 
s4 0,15 
01 
s5 0,10 
020 
s6 0,05 
021 
s7 0,0 
022 
0,3 
0,25 
0,15   00 
0,15   01 
0,15   02 
    0,45    0 
    0,3      1 
    0,25    2 
 
1.85
3
0.05
3
0.10
2
0.15
1
0.25
1
0.3
l
=
×
+
×
+
×
+
×
+
×
=
 
1.5
1.584
2.39
m
log
H(S)
l
2
min
=
=
=
 
81%
0.81
1.85
1.5
η
=
=
=
 
l
l
R
u
C =
 where lu is   
1.63
1.584
2.584
3
log
6
log
m
log
M
log
l
2
2
2
2
u
=
=
=
=
 
therefore we choose lu=2. 
Replacing lu=2 in the compression ratio formula, we get: 
1.33
1.5
2
RC
=
=
 
 
Remarks concerning static Huffman algorithm 
 
• The obtained codes are not unique. Interchanging 0 with 1 (for binary codes) 
we obtain a complementary code. Similarly, for messages with equal probabili-
ties, assigning the codewords is arbitrarily. However, even though the code is 
not unique, all the codes obtained with the same algorithm provide same aver-
age length, therefore same efficiency and compression ratio. 
• The last m codewords length is the same, if m-ary alphabet is used. 
• The codes, obtained using the described algorithms, are instantaneous; there-
fore no codeword is prefix for another codeword. 
• Symbol by symbol Huffman coding, in the case when the highest probability is 
close to 1, has as result a code efficiency decrease. This drawback can be over-
come using Huffman coding for streams of symbols (see Shannon’s first theo-
rem for n>1). In this case the information is divided in fixed length blocks that 
are then encoded. 
• Huffman coding is frequently used as the final processing (back-end coder), in 
a series of compression schemes. 
 
 

3.7   Losseless Compression Algorithms 
91
 
Multi-group Huffman coding [41] 
 
This type of coding is used for sources that generate streams of characters belong-
ing to disjunctive classes, for example sources generating burst letters followed by 
numbers and then by blank spaces. When designing such an encoder a tree is set 
up for each class of symbols. A failure symbol is then added to each tree. Each 
character is coded taking into consideration the current Huffman tree. In the case 
when the character can be found in that particular tree, its corresponding code is 
transmitted; otherwise the code corresponding to the failure character is transmit-
ted pointing to another tree. 
Advanced applications of this type can be found in database system drivers. 
 
Dynamic Huffman coding (1978) 
 
All Huffman-type algorithms that have been presented so far bear one major dis-
advantage: all require the source statistics (static algorithms). This drawback can 
be overcome using an adaptive (dynamic) algorithm. R.G. Gallager  [14] pre-
sented in 1978 three new theorems on dynamic Huffman codes. D.E. Knuth [24], 
using Gallager theorems, presented a Huffman algorithm with the capability of 
dynamically changing its own tree. 
The basic idea of dynamic Huffman algorithm is to use, for coding the symbol 
si+1, a coding tree (a coding dictionary set up as a binary tree) constructed using 
the first i symbols of the message. After transmitting the symbol si+1 the coding 
tree must be revised in order to code the next symbol si+2. There are more dynamic 
Huffman versions (FGK, ∆) [2] each of them using a different method for setting 
up the tree. 
In what follows we will introduce the pseudo-code corresponding to the dy-
namic Huffman compression algorithm. 
The general compression procedure for dynamic Huffman coding has the fol-
lowing pseudo-code: 
 
- initialise the coding tree with a root node; 
- transmit the first symbol as it is (using, for example, its ASCII code); 
- add 2 leafs to the root node (a left leaf,  empty leaf of weight 0, a right leaf 
of weight 1, which contains the current symbol) 
 
while (end of message)  
{ 
  
 - read a message symbol; 
if(the letter already belongs to the tree) 
 -  transmit its code from the tree;   
 else  
{ 
 
-  transmit the empty leaf code; 
     
-  transmit the ASCII code of the letter; 
} 
       update tree; 
} 

92 
3   Source Coding 
 
 
a 
1
0 
 
1 
1 
3 
2 
5 
4 
1 
1
0
3 
1 
0 
2 
a 
Right branch 
’1’ 
Left branch ’0’
Leaf node 
(corresponding
to the message 
characters) 
No. of 
the tree 
1 
0 
1 
0 
2 
b 
0 Leaf 
Node weight (equal to the no. of 
occurrences of the message symbol for 
leaf nodes, or with the sum of the nodes 
weights for the intermediary nodes) 
4
3 
1 
7 
5 
a 
6 
2 
1
3 
4 
b 
1 
0 
1 
2 
c 
4
2
2
7 
5 
b 
6 
1 
1
3 
4 
a 
1
0
1 
2 
c 
Associated 
character 
1 
1 
0 
3 
1 
0 
2 
c 
2 
1
5 
4 
b 
3
1
7 
6 
a 
3
2
1
7 
5 
a 
6 
1 
1
3 
4 
b 
1
0
1 
2 
c 
Node 
interchange
after ’b’ tree: 
after ’a’ tree
Initial tree:
after ’c’ tree: 
after ’b’ tree: 
 
Fig. 3.12 Evolution of dynamic Huffman FGK tree for the message ‘abcbb’ 
 
 
 

3.7   Losseless Compression Algorithms 
93
 
Remark 
The only difference between the numerous versions of dynamic coding is the way 
the coding tree is updated.  
One version of updating the tree is the FGK (Faller, Gallager, and Knuth) algo-
rithm. The basic idea for this method is the minimisation of 
∑
j
j
j
w 1
  (sum equivalent to the bit-length of the coded message), 
where wj represents the weight of the leaf j associated to the symbol j (the number 
of appearances of the j symbol) and lj is the code length associated to that leaf. 
This method is used in the MNP5 (Microcom Network Protocol 5) data com-
pression standard for modems. 
The pseudo-code corresponding to the FGK updating procedure is: 
 
- q = the leaf corresponding to the symbol si if this symbol can be found in 
the tree, or an empty leaf if it doesn’t belong to the tree; 
if (q is a 0 leaf)  
{  
- substitute the 0 leaf with a parent node with two 0 leafs sons num-
bered: 1 the left son, 2 the right son, 3 the parent; 
- increment the order of the other nodes; 
- q = the new created right son; 
} 
while (q is not a root node) 
{ 
- increment the weigh of the node q; 
  
- replace q with the highest ranked and smallest weighted node; 
- q = q node parent; 
   
} 
 
Huffman coding for Markov sources 
 
Until now Huffman coding was applied only for memoryless discrete sources. A 
question rises: how to apply this algorithm for memory sources? The answer to 
this question can be given in two ways: 
 
• the probability mass function of the stationary state P* is encoded obtaining a 
unique code. 
• the Markov source is encoded for all M states from where the source can 
evolve, therefore determining M codes. 
Let us now code the Markov source from Example 2.7 for which we have:  
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
0,6
    
0,2
   
0,2
0,3
    
0,4
   
0,3
0,2
    
0,2
   
0,6
M
 
and the stationary state probability mass function is: 
[
]
8
/
3
   
4
/
1
   
8
/
3
P* =
 

94 
3   Source Coding 
 
a. Stationary state Huffman coding: 
 
si 
pi 
ci 
s1 
s2 
s3 
3/8 
3/8 
1/4 
1 
00 
01 
 
The average length of the codewords is: 
1.62
2
4
/
1
2
8
/
3
1
8
/
3
l
=
×
+
×
+
×
=
 
b1. Huffman coding, the source starting from state s1: 
 
si 
pi 
ci 
s1 
s2 
s3 
0,6
0,2
0,2
0 
10 
11 
1.4
2
0.2
2
0.2
1
0.6
l1
=
×
+
×
+
×
=
 
b2. Huffman coding, the source starting from state s2: 
 
si 
pi 
ci 
s1 
s2 
s3 
0,4 
0,3 
0,3 
1 
00 
01 
6
1.
2
3
0.
2
3
0.
1
4
0.
l2
=
×
+
×
+
×
=
 
b3. Huffman coding, the source starting from state s3: 
 
si 
pi 
ci 
s1 
s2 
s3 
0,6 
0,2 
0,2 
0 
10 
11 
4
1.
2
0.2
2
0.2
1
0.6
l3
=
×
+
×
+
×
=
 
The average length of Huffman code applied to this source will be determined 
as the average value of the lengths determined previously: 
1.45
1.4
8
/
3
1.6
4
/
1
1.4
8
/
3
l
p
l
p
l
p
l
3
*
3
2
*
2
2
*
1
M
=
×
+
×
+
×
=
×
+
×
+
×
=
 
Remark 
If the number of states is high, the average length decreases slower and slower and 
the number of distinct codes increases so that coding the stationary state becomes 
more practical [16]. 

3.7   Losseless Compression Algorithms 
95
 
Conclusions concerning Huffman coding 
 
• This algorithm is probably one of the most used compression algorithms. 
• It is a rigorous, simple and elegant algorithm appreciated both by theorists and prac-
titioners. Despite its “age”, experimental studies in which it is used still appear. 
• The algorithm can be applied in three distinct versions: 
– 
static: knowledge of the source statistics P(S) is compulsory 
– 
quasi-dynamic (half-adaptive): as a first step finds the source PMF and 
then uses static encoding 
– 
dynamic (adaptive): it does not require the knowledge of source statistics 
in advance as the tree is constructed with each read symbol  
• The use of the static version of this compression algorithm in transmissions is 
limited to known sources for which the PMF is known: P(S) (e.g. for com-
pressed transmissions of texts written in different languages, the encoder-
decoder can be designed according to a well known statistic P(S)). 
• Huffman algorithm – being optimal at message level – was thought to be the 
best compression algorithm. However, in practical applications, this statement 
has limitations; it is forgotten that it was defined in restrictive conditions: 
– 
memoryless source 
– 
symbol by symbol encoding 
Most of the real sources are not memoryless and the encoding can be done on 
groups (using Shannon first theorem) therefore it is possible to achieve better 
compression ratios RC than the ones given by Huffman algorithms, although the 
former compressions are not optimal. 
Between 1952 and 1980, after Huffman work publication and computer spread-
ing, data compression had a major development both theoretically and practically. 
Some data compression techniques occurred and were named, although not always 
accurately, ad-hoc compression techniques. 
Many researches in this field have focused on coding source extensions for 
which each symbol corresponds to a stream of symbols of the initial source. 
3.7.3   Run Length Coding (RLC) 
The basic idea of run length coding consists in replacing a stream of repeating 
characters with a compressed stream containing fewer characters. This type of 
coding is especially used for memory sources (repetition means memory). 
Principle of run length coding: 
 
• successive samples are analysed and if a number of r successive samples differ 
with less than α quanta, it is said that we have a r-length step (α is named the 
compression algorithm aperture; identical samples implies α=0) 
• for transmissions or storage, it is enough to provide only the first sample ampli-
tude and the step length ‘r’, in binary. 
Huffman coding of steps (offers the best compression): 
• the first sample from every step is stored together with the codeword which 
represents the step length (ri) 
• the data statistics p(ri), necessary for encoding, is obtained experimentally. 

96 
3   Source Coding 
 
Example 3.8 [38] 
Consider a fax transmission (black and white image). Taking into consideration 
streams of the same type (black or white) with length ri, we can determine their 
PMF, for the whole image. 
In this case, the source alphabet will be: 
A = {r1, r2, …, rM }, 
where ri indicate streams of the same length ri (black or white).  
The corresponding PMF, obtained as a result of a statistical analysis of the im-
age, is given by: 
( )
∑
=
=
=
=
M
1
i
i
i
i
1
p
  ,
m
1,
i  
],
[p
r
P
 
Assuming independent steps, the average information quantity per step is: 
( )
∑
−
=
=
M
1
i
i
2
i
]
[bits/step
   
p
log
p
A
H
                                (3.66) 
These steps can be optimally encoded using the binary Huffman algorithm; it 
results, for A, the following average length: 
( )
( ) 1
A
H
l
A
H
+
<
≤
                                          (3.67) 
Dividing (3.67) by the average number of samples per step ( r ) we get: 
∑
=
=
M
1
i
i
i
/step]
ixels)
[samples(p
 ,
r
p
r
                                 (3.68)  
It follows that: 
( )
( )
r
1
A
H
D
r
l
r
A
H
b
+
<
=
≤
•
                                    (3.69) 
where  
b
D
•
 is the binary rate [bits/pixel (sample)]. 
r
l
Db =
•
                                                (3.70) 
For a typical fax image, corresponding to a weather forecast image [38], for this 
type of compression we get: 
( )
l]
[bits/pixe
  
0.2
r
A
H
≅
 compared to ‘1’ obtained 
without compression. 
 
Remark 
A better modelling of this information source can be made using a first order 
Markov model, in which case the compression is even better. 

3.7   Losseless Compression Algorithms 
97
 
Example 3.9 [38] 
The fax standard, proposed by the 3rd research group from CCITT, (Hunter and Rob-
inson, 1980) has a resolution of 1728 [samples/line] for A4 documents so the maxi-
mum number of codewords/line is 1728. Due to this great number of codewords the 
‘classic’ Huffman code becomes useless, therefore a modified Huffman code has been 
introduced; each step with length ri > 63 is divided in 2 steps: one having the value N 
x 64 (N is an integer) - “the root word”, and the other one - “the terminal part”, made 
of steps with values between 0 and 63. The black and white streams are encoded 
separately as they are independent, obtaining a lower rate (and thus a better compres-
sion). Table 3.12 shows the modified Huffman code used in facsimiles. 
Table 3.12 Modified Huffman code used in fax 
Codewords 
ri 
White 
Black 
ri 
White 
Black 
0 
00110101 
0000110111 
32 
00011011 
000001101010 
1 
000111 
010 
33 
00010010 
000001101011 
2 
0111 
11 
34 
00010011 
000011010010 
3 
1000 
10 
35 
00010100 
000011010011 
4 
1011 
011 
36 
00010101 
000011010100 
5 
1100 
0011 
37 
00010110 
000011010101 
6 
1110 
0010 
38 
00010111 
000011010110 
7 
1111 
00011 
39 
00101000 
000011010111 
8 
10011 
000101 
40 
00101001 
000001101100 
9 
10100 
000100 
41 
00101010 
000001101101 
10 
00111 
0000100 
42 
00101011 
000011011010 
11 
01000 
0000101 
43 
00101100 
000011011011 
12 
001000 
0000111 
44 
00101101 
000001010100 
13 
000011 
00000100 
45 
00000100 
000001010101 
14 
110100 
00000111 
46 
00000101 
000001010110 
15 
110101 
000011000 
47 
00001010 
000001010111 
16 
101010 
0000010111 
48 
00001011 
000001100100 
17 
101011 
000011000 
49 
01010010 
000001100101 
18 
0100111 
0000001000 
50 
01010011 
000001010010 
19 
0001100 
00001100111 
51 
01010100 
000001010011 
20 
0001000 
00001101000 
52 
01010101 
000000100100 
21 
0010111 
00001101100 
53 
00100100 
000000110111 
22 
0000011 
000001101111 
54 
00100101 
000000111000 
23 
0000100 
00000101000 
55 
01011000 
000000100111 
24 
0101000 
00000010111 
56 
01011001 
000000101000 
25 
0101011 
00000011000 
57 
01011010 
000001011000 

98 
3   Source Coding 
 
Table 3.12 (continued) 
26 
0010011 
000011001010
58 
01011011 
000001011001 
27 
0100100 
000011001011
59 
01001010 
000000101011 
28 
001000 
000011001100
60 
01001011 
000000101100 
29 
00000010 000011001101
61 
00110010 
000001011010 
30 
00000011 000001101000
62 
00110011 
000001100110 
31 
00011010 000001101001
63 
00110100 
000001100111 
“Root” codewords 
ri 
White 
Black 
ri 
White 
Black 
64 
11011 
0000001111 
960
011010100 
0000001110011 
128 
10010 
000011001000 1024
011010101 
0000001110100 
192 
0010111 
000011001001 1088
011010110 
0000001110101 
256 
0110111 
000001011011 1152
011010111 
0000001110110 
320 
00110110 000000110011 1216
011011000 
0000001110111 
384 
00110111 000000110100 1280
011011001 
0000001010010 
448 
01100100 000000110101 1344
011011010 
0000001010011 
512 
01100101 0000001101100 1408
011011011 
0000001010100 
576 
01101000 0000001101101 1472
010011000 
0000001010101 
640 
01100111 0000001001010 1536
010011001 
0000001011010 
704 
011001100 0000001001011 1600
010011010 
0000001011011 
768 
011001101 0000001001100 1664
011000 
0000001100100 
832 
011010010 0000001001101 1728
010011011 
0000001100101 
896 
011010011 0000001110010 EOL 000000000001 000000000001 
 
Remark 
If the compression is made using variable-length coding (Huffman coding), re-
moving the redundancy, the transmission becomes vulnerable to errors. In general 
(but not in all the cases) an error occurred in transmission will spread, leading to 
synchronisation loss and, implicitly, to an incorrect decoding. 
 
5W 
2B 
2W 
3B 
25W 
 
1 1 0 0 1 1 0 1 1
1
1 
0
0 
1 
0 
1 0
1
1 
-a 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5W 
3B 
2W 
3B 
25W 
 
1 1 0 0 1 0 0 1 1
1
1 
0
0 
1 
0 
1 0
1
1 -b 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5W 
1B 
7W 
6B 
4W 
 
1 1 0 0 0 1 0 1 1
1
1 
0
0 
1 
0 
1 0
1
1 
-c 
 
Fig. 3.13 The effect of one error on the modified Huffman code: a) transmitted sequence, b) 
and c) received sequence affected by one error 

3.7   Losseless Compression Algorithms 
99
 
Coding using a limited set if step lengths 
 
Another way of compressing identical streams, although not optimal as in Huff-
man coding, can be achieved using a limited set of codewords lengths correspond-
ing to each step. 
 
Example 3.10 [38] 
 
• Images in the visible and infrared spectrum taken by the weather forecast satel-
lites can be classified in three classes, according to the homogeneous areas they 
represent: the first class (water and land surfaces) and two classes of clouds 
(C1, C2). The homogenous regions are suitable for run length coding before 
transmission. Two sets of lengths, of 8 and 16 bits are used. 
• If the steps length ri<64=26, the images are coded using a word of 11=8 bits, the 
first two bits indicating the region: 
 
b1b2  = 00 indicates water + land 
01→C1 
 10→C2 
 
b1 
b2 
b3 
b4 
b5 
b6 
b7 
b8 
indicate the region 
6 bits for uniform coding of ri steps 
 
• If the steps length 
64
ri ≥
, the extended code is used, therefore l2=16 bits; the 
16 bits are used as shown below: 
 
b1 
b2 
b3 
b4 
b5 
b6 
..... 
b16 
11           
extended code indicate the region 12 bits for uniformly 
coding the steps ri 
 
It has been determined that, for this application, approximately 94% of streams 
have ri<64, therefore 8 bit coding is used. Average length of the codewords is  
8.5 bits/stream. For regular weather images a compression ratio of 
4
R c ≅
was  
obtained. 
 
Uniform step coding 
 
The third type of RLC, the poorest in quality, is using uniform step coding. For a 
given source, be r1,…,rM the steps lengths, where rM is the maximum length. All 
these steps will be uniformly encoded, the codewords length being determined by: 
l=log2 rM. 
At the beginning of each step a k-bit word is transmitted (stored) representing 
the value of each sample from the step; next we transmit the l bits representing the 
length of the step. 
 

100 
3   Source Coding 
 
Example 3.11 
In what follows we will present a method used in compressing facsimile black and 
white images; this method is extremely simple and quite efficient, although infe-
rior to Huffman coding. A facsimile system with 1728 samples/line generates 
identical streams having ri = 100 or higher (generally for white streams), therefore 
the steps are encoded with l = 8 bits (Fig. 3.14). 
 
Fig. 3.14 Uniform steps coding for black and white images 
Streams coding for texts 
 
When we are interested in processing texts, a stream of repeating characters can be 
transmitted in a compressed form following the algorithm: a special symbol S is 
transmitted at the beginning of the repeating characters stream to indicate repeti-
tion, next the repeating character X and finally a character C which is counting the 
repetitions (Fig. 3.15). 
 
Fig. 3.15 Text compression  
The stream of seven zeros was compressed transmitting CXS. 
3.7.4   Comma Coding 
Comma code is a uniquely decodable code which indicates through a symbol (0 or 
1) the ending of every codeword. 
 
Example 3.12 
The source 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
1/8
1/7
1/4
1/2
s
s
s
s
:
S
4
3
2
1
, is encoded using a comma code as: 
 
Si 
pi 
ciC 
ciH 
S1 
1/2 
0 
0 
S2 
1/4 
10 
10 
S3 
1/8 
110 
110 
S4 
1/8 
1110 
111 

3.7   Losseless Compression Algorithms 
101
 
We get the average length and the compression ratio: 
1.07
1.87
2
l
l
R
1.87
4
8
1
3
8
1
2
4
1
1
2
1
l
p
l
C
u
cC
m
1
i
i
i
C
≅
=
=
=
∑
⋅
+
⋅
+
⋅
+
⋅
=
=
=
 
The corresponding Huffman code is presented in the last column of the coding 
table (CiH). In this case we have: 
cH
cH
H
R
1.14
1.75
2
R
1.75
l
≅
=
=
 
It must be noticed that for the source under discussion the compression ratio is 
good enough compared to the maximum value obtained by Huffman coding. 
 
Remark 
For certain applications, comma coding can be more advantageous than Huffman 
coding due to the implementation simplicity and its robustness to errors (comma 
code is much more robust to errors unlike Huffman code). These advantages are 
paid by reducing coding efficiency (the compression ratio, respectively) in com-
parison to optimal codes, although in many cases the decrease can be small. 
3.7.5   Dictionary Techniques [41] 
Text files are characterised by the frequent repetition of some sub-streams. Nu-
merous compression algorithms perform detection and removal of the repeating 
sub-stream. 
The dictionary techniques build the dictionary of common sub-streams either at 
the same time with their detection or as a distinct step. Each dictionary sub-stream 
is put into correspondence with a codeword and the message is then transmitted 
through encoded sub-streams from the dictionary. 
According to the input/output messages dimension, the dictionary techniques 
can be classified in: 
 
• fixed – fixed: dictionary sub-streams have the same length and are uniformly 
encoded 
• fixed – variable: sub-streams are uniform but are not uniformly encoded, for 
example using a Huffman code 
• variable – fixed: at the compressor input the sub-streams are not uniform but 
are uniformly encoded 
• variable – variable: lengths of the dictionary sub-streams and the coding is not 
uniform 
 
 

102 
3   Source Coding 
 
After the dictionary has been built and the message encoded, the dictionary and 
the encoded message must be transmitted. 
The compression efficiency is better when the dictionary is also compressed. 
This is possible if the compressor and the de-compressor simultaneously build up 
their dictionaries, in the ongoing compression process. This idea can be found in 
LZ -type algorithms.  
3.7.6   Lempel-Ziv Type Algorithms (LZ) 
Adaptive dictionary techniques 
 
Dictionary techniques must allow the transmission of the dictionary from the 
compressor (transmitter), to the de-compressor (receiver). If the dictionary is 
static, it is not transmitted (static compression techniques). When using semi-
adaptive dictionary algorithms, the first step is to transmit the dictionary and then 
to compress the message. 
Adaptive dictionary techniques do not require explicit transmission of the dic-
tionary. Both transmitter and the receiver simultaneously build their dictionary as 
the message is transmitted. At each moment of the encoding process the current 
dictionary is used for transmitting the next part of the message. 
From adaptive dictionary techniques category, the most used ones are the algo-
rithms presented by J. Ziv and A. Lempel in two papers published in 1977 and 
1978; these algorithms are known as “LZ algorithms”. 
The basic idea of LZ algorithms is to replace some sub-streams of the message 
with codewords so that for their each new occurence only the associated codeword 
will be transmitted. 
“Lempel and Ziv have hidden this brilliant idea in a sea of math” [41]. The 
works of these two mathematicians were extremely theoretical, becoming accessi-
ble due to other authors descriptions, so that LZ coding is in fact a family of algo-
rithms built upon the same idea. 
 
LZ-77 algorithm 
 
LZ-77 algorithm uses the following two parameters: 
 
• 
]
Z[1,
N
∞
∈
, a window buffer length, which will move upon the transmitted 
message. 
• 
1]
N
Z[1,
F
−
∈
, the maximum length of encoded stream with F << N 
Typical values used in practice are 
13
2
N ≅
and
4
2
F ≅
, both expressed as pow-
ers of two and making the algorithm easier for implementation. 
The algorithm involves a window shifting register (buffer) of length N through 
which the message is shifted from right to left. The first N-F elements form the  
 
 

3.7   Losseless Compression Algorithms 
103
 
Lempel block, containing the most recently N-F transmitted letters, and the next F 
elements form Ziv block containing the next F letters to be transmitted (Fig. 3.16).  
 
 
Fig. 3.16 The LZ-77 algorithm illustration 
At the beginning, the algorithm initialises Lempel block with a predefined 
stream and loads Ziv block with message’s first characters. Encoding means to 
find the longest sub-stream from the register which has the first element in Lampel 
block and is identical to the stream starting from position N-F+1. This finding 
means to transmit a triplet (S, L, A), where 
[
]
F
N
1,
S
−
∈
 is the position in the 
Lempel block from where the stream begins, 
[
]
F
1,
L∈
 represents the length of the 
stream just found and A represents the letter where the similarity stopped. Next, 
the message is shifted from right to left in the shift register until the next letter, 
which has to be encoded, becomes the farthest element at the left from Ziv. 
Some remarks regarding the algorithm ingenuity: 
 
•  the third element from the transmitted triplet A makes possible to avoid the 
situation when a letter does not exist in the Lempel block (L = 0) 
• the stream from Lempel found to be similar to one from Ziv must start in Lem-
pel block but could be able to extended into Ziv block 
• the “dictionary” used in this technique is made up of every sub-stream from 
Lempel block; it must not be transmitted because it is dynamically resized both 
in encoder and decoder blocks 
• the algorithm is locally adaptive because it uses N-F previously transmitted let-
ters, at most 
• the search inside the table is linear and as a result, the encoding time is also de-
pendent on the lengths N and F 
• the decoding process is fast, because it doesn’t involve a linear search inside 
the buffer  
• due to the fact that N and F are finite numbers, S, L, A can be transmitted using 
an exact number of bits. 
 
Example 3.13 
An LZ77 coding example, starting from an empty Lempel buffer; the message is: 
”aabaabacabadadad” 

104 
3   Source Coding 
 
Lempel Buffer 
Ziv 
Transmitted 
code (S,L,A) 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
S 
L 
A 
0 
0 
0 
0 
0 
0 
0 
0 
a 
a 
b 
a 
0 
0 
a 
0 
0 
0 
0 
0 
0 
0 
a 
a 
b 
a 
a 
8 
1 
b 
0 
0 
0 
0 
0 
a 
a 
b 
a 
a 
b 
a 
6 
4 
0 
0 
a 
a 
b 
a 
a 
b 
a 
c 
a 
b 
a 
0 
0 
c 
a 
a 
b 
a 
a 
b 
a 
c 
a 
b 
a 
d 
2 
3 
d 
a 
b 
a 
c 
a 
b 
a 
d 
a 
d 
a 
d 
7 
4 
0 
 
Fig. 3.17 Illustration of LZ-77 algorithm 
LZ-78 algorithm 
 
LZ-78 version is similar to LZ-77, the difference being that the Lempel block is a 
continuously growing ‘dictionary’; having the dimension 
Z
d ∈
 theoretically 
unlimited and the strings are numbered from 0 to d-1. Furthermore, there is no 
limit imposed to the Ziv block length. 
For better understanding of the algorithm, let us consider Fig. 3.18: 
 
Fig. 3.18 Illustration of LZ-78 algorithm  
At the beginning, the algorithm initialises the dictionary with an all zero stream 
and sets d=1. At each step, the algorithm sends the longest stream from Lempel 
which is identical to the one from Ziv (actually its associated code) and adds the 
next letter from Ziv where the similarity has been lost. This way, p=‘ma’ is trans-
mitted by the equivalent of ‘m’ from the dictionary using [log2d] bits and by ‘a’ 
which is transmitted as it is. Next, the new stream (ma) is added to the dictionary, 
the message shifts in the Ziv block and the process starts all over again. 
The decoder must decode k+1 streams to be able to add the k-th stream to the 
dictionary.  
 
LZW algorithm (Lempel – Ziv – Welch ‘84) 
 
In 1984, T. Welch published a version of LZ-78 algorithm. This version abandons 
the idea of transmitting the letter at which the similarity between the message  

3.7   Losseless Compression Algorithms 
105
 
sub-streams and those from the ‘dictionary’ breaks. Explicit transmission of each 
stream last letter can be avoided by initialising the dictionary with all the symbols 
used in the message (e.g. the ASCII characters). This way, the letter that had to be 
transmitted explicitly will be the next stream first letter. The most important 
change of LZ-78 version is removing the explicit letter transmission. An improved 
version of the algorithm forms the base of the Compress Program under UNIX. 
LZW algorithm is based on the idea of transforming stream symbols 
S=s1s2s3s4… in sub-streams succession S=S1S2S3... Each sub-stream Si (sub-stream 
containing more si symbols) will be associated to a fixed length codeword. Ini-
tially, we will have a coding table where a codeword is associated to each symbol 
of the source alphabet: 
i
i
c
s →
. Next, the message will be read and when groups 
of symbols occur for the first time, they are added to the table (additional table) so 
that at a new occurrence they will be replaced with the codeword associated to 
their first occurrence. 
LZW algorithm has the following steps: 
 
• Encoding: 
 
– 
Initialise streams table with singular symbols (source alphabet) 
M
1,
i
},
{s
S
i
=
=
, and encodes them with fixed length words: 
i
i
c
s →
. 
– 
Read the first symbol s1 and sets P=s1, where P is the prefix (found in the 
prefixes stream) 
– 
Read the next symbol: E=sk 
̇ 
if PE can be found in the prefixes table, P=PE is made 
̇ 
if PE is not in the prefixes table, PE is added to the streams table 
(supplementary table), and E=P is added to the prefixes stream P 
• Decoding: 
– 
the decoder will build the same streams table as the one from encoding 
– 
each word received is shifted, using the table from encoding, in a prefixes 
stream and the extension symbol PE (which is stored), and this is recur-
sively repeated until the prefixes stream contains only one symbol 
 
Example 3.14 
Consider the source alphabet S={a,b,c,d}, so that the streams table is initialised 
with: 
 
si 
ci 
a 
1 
b 
2 
c 
3 
d 
4 
 
Encoder is fed with stream S = ”ababcbababaadaa”. 
Encoding begins by reading the first symbol s1=a and setting P=a. 

106 
3   Source Coding 
 
Next, the symbol s2 is read, E=b but because PE=ab is not in the table of 
streams, the stream PE=ab is added to the supplementary streams table, and 
P=E=b is added to the prefixes stream P. The third symbol s3=E=a is read, but PE 
= ba not being in the streams table, the stream PE is added to the supplementary 
streams table and P=E=a is added to the prefixes stream P. For the fourth symbol 
s4=b=E, we have PE=ab in the additional streams table, therefore P=PE and ab is 
added to the prefixes stream P a. s. o. 
Decoding: the decoder will use the same streams table as the one used for com-
pression. Each received codeword is moved from the similar prefixes table and an 
extension symbol (which is extracted and stored) is recursively repeated until the 
prefixes stream contains only one symbol. 
Let us consider, for example, the case when we receive “9”: 
 
9 = 6b → stores b 
6 = 2a → stores a       ⇒  the decoded sequence is bab 
2 = b   → stores b 
 
Table 3.13 shows how the LZW algorithm works for the sequence given in  
Example 3.14. 
Table 3.13 LZW algorithm applied to Example 3.14  
Input message 
a 
b 
a 
b 
c 
b 
a 
b 
a 
b 
a 
a 
d 
a 
a 
Stream added (to the 
coding table – PE) 
 
ab ba 
 
abc
cb
 
bab
 
 
baba
aa 
ad 
da 
 
Code added (to the 
coding table) 
 
5 
6 
 
7 
8 
 
9 
 
 
10 
11 12 13 
 
Transmitted stream P  
a 
b 
 
ab 
c 
 
ba 
 
 
bab
a 
a 
d 
aa 
Transmitted code 
 
1 
2 
 
5 
3 
 
6 
 
 
9 
1 
1 
4 
11 
 
Conclusions regarding the LZW algorithm 
 
• LZW is an universal compression algorithm which can be applied to any 
source, without even knowing it in advance (it is an adaptive algorithm) 
• coding/decoding are interleaved with a learning process developed while en-
coder/decoder builds and dynamically modifies the streams table; the decoder 
does not need coding tables, because it builds an identical table while receiving 
the compressed data. 
• dividing the input message S into sub-streams Si is not optimal (it doesn’t take 
into consideration the source statistics), due to which LZW is not optimal; the 
idea of Shannon first theorem is used here, the coding being performed on 
symbols groups (on sub-streams). 
• improved versions of LZW algorithm can be obtained making the followings:  
– 
setting a limit to the length of the streams included in the coding table  
– 
using a variable number of bits for codeword length (l), smaller at the be-
ginning, then higher as the transmitted codewords number is increased, 
which implies the use of a signalling character. 

3.7   Losseless Compression Algorithms 
107
 
• when coding table is full, there are two options: either to erase the table and 
start the process all over again or to continue with the same table but stopping 
the process of adding new characters. 
• like other compression algorithms, LZW is sensitive to transmission errors; the 
most used method to ensure protection when saving compressed data on disks 
or magnetic tape is to apply the CRC (Cyclic Redundancy Check). 
3.7.7   Arithmetic Coding 
Before getting into details of arithmetic compression, let us present a new point of 
view upon the source coding process. As we have already shown, the information 
given by a symbol si is -log2pi where pi is symbol probability. Now let us imagine 
that this information is a cube of volume –log2pi. In this context, the codeword ci 
associated to the symbol si will be a box through which that volume will be sent 
(transmitted through the channel). The problem is that we have only boxes of cer-
tain capacities and we want to work as efficiently possible (to avoid using big 
boxes for small volumes). In comparison with the uniform and entropic coding, in 
the first case we have boxes of fixed capacity (obviously a big waste of space), 
and in the second case we have boxes of different sizes. Looking at compression 
from this point of view, we get the idea of using boxes of the same capacity but 
filled with more cubes (the information of a symbol), in the order of their arrival. 
From an informational point of view, the basic idea is to encode more symbols 
using only one codeword, of fixed length. However, the problem that must be 
dealt with is how to extract the symbols from the received codeword. 
One solution to this problem is given by the arithmetic coding. 
The steps of arithmetic coding algorithm are: 
 
1. Put the symbols in decreasing order of probabilities: 
n
2
1
p
p
p
≤
≤
≤
…
. 
2. Divide the interval [0; 1) into n intervals of dimensions
n
2
1
p
,
,
p
,
p
…
. 
3. Read the first symbol of the message and memorise its associated interval.  
4. Divide this interval into n intervals, each of them proportional to
n
2
1
p
,
,
p
,
p
…
. 
5. Read the next symbol and memorise the interval associated to it.  
6. Continue the process following the same algorithm. 
7. Transmit one number from the last memorised interval. 
 
Example 3.15 
Encode the message “abac” using an arithmetical code designed for the source S: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
6
/
1
      
3
/
1
      
2
/
1
c
       
b
          
a
:
S
 
A new problem that must be solved is how to encode the sub-unitary numbers that 
must be transmitted. One way to tackle this problem may be the following: choose a 
minimum of the memorised interval and when this interval becomes smaller than that 
minimum, transmit a number contained in the previous interval. The transmitted 
number will not be a sub-unitary one, but an integer representing a multiple of the 
minimum quantum, multiple which can be found in the memorised interval. In this 

108 
3   Source Coding 
 
way, for example, if we consider the minimum interval 1/128, for the previous encod-
ing, we transmit 40 (40/128 is in the interval [23/72, 1/3]) in binary on 5 bits. 
Processed Interval 
Read 
symbol 
Memorized 
Interval 
1/2 (1-0) 
1/3 (1-0) 
1/6 (1-0) 
a 
[0,1/2) 
0 
a 
1/2 
b 
5/6 
1 
c 
 
 
1/2 (1/2 -0)=1/4 
1/3 (1/2-0)=1/6
1/12 
b 
[1/4,5/12) 
0 
a 
1/4 
b 
5/12 
1/2 
c 
 
 
1/2(5/12-1/4)=1/12 
1/18 
1/36 
a 
[1/4,5/3) 
1/4 
a 
1/3 
b 
7/18 
5/12 
c 
 
 
1/2(1/3-1/4)=1/24 
1/36 
1/72 
c 
[23/72,1/3) 
1/4 
a 
7/24 
b 
23/72 
1/3 
c 
 
 
 
 
We transmit a number from the interval [23/72; 1/3), for example 23/72. 
Processed Interval 
Interval  
containing
23/72 
Associated 
symbol    
1/2 (1-0) 
1/3 (1-0) 
1/6 (1-0) 
 
 
0 
a 
1/2 
b 
5/6 
1 
c 
[0,1/2) 
a 
1/2 (1/2 -0)=1/4 
1/3 (1/2-0)=1/6
1/12 
 
 
0 
a 
1/4 
b 
5/12 
1/2 
c 
[1/4,5/12)
b 
1/2(5/12-1/4)=1/12 
1/18 
1/36 
 
 
1/4 
a 
1/3 
b 
7/18 
5/12 
c 
[1/4,1/3) 
a 
1/2(1/3-1/4)=1/24 
1/36 
1/72 
 
 
1/4 
a 
7/24 
b 
23/72 
1/3 
c 
[23/72,1/3)
c 
 
 

3.8   Lossy Compression in Differential Coding 
109
 
3.8   Lossy Compression in Differential Coding 
3.8.1   Differential Pulse Code Modulation (DPCM) 
In PCM, a signal x(t), voice, video signal or else is sampled at a rate slightly 
higher than the Nyquist rate: fS = 2fM. For example, in digital telephony with 
maximum frequency fM=3.4 kHz the sampling frequency is fS = 8k Hz. These 
samples are very correlated, the signal x(t), not changing fast from a sample to the 
next one (it is a memory source). The corresponding PCM signal is therefore 
highly redundant. The removal of this redundancy before encoding is the basic 
idea of differential PCM (DPCM). This principle is illustrated in Fig. 3.19. 
 
>
 
Fig. 3.19 Illustration of DPCM principle  
In DPCM it is quantized the difference 
k
Δ between the sample 
k
x and the es-
timated value of it 
k
x
∧
 obtained from prediction from the previous samples of the 
signal x(t). The prediction is possible only if x(t) has memory, its statistical char-
acteristics being known. For a pure random signal the prediction is impossible. 
k
k
k
x
x
Δ
∧
−
=
                                             (3.71) 
Further we will use the following notations: 
k
k
k
Δ
,
x
,
x
∧
for analogic sizes 
'
Δ
,'
x
,'
x
k
k
k
∧
for digital sizes. 

110 
3   Source Coding 
 
The pair DPCM modulator (encoder)-demodulator DPCM (decoder) is known 
as DPCM codec. 
 
Remark 
DPCM makes a lossy source compression, caused by the quantizing noise; this 
loss, for human users, if is under the thresholds of human audio sense (HAS) or 
human visual sense (HVS), is without quality loss. 
Schemes for DPCM codecs are hundreds, thousands, depending on application 
and technology. Fig. 3.20 presents one block-scheme, illustrating the DPCM gen-
eration and detection. 
 
 
Fig. 3.20 DPCM codec: a) DPCM encoder; b) DPCM decoder 
For a linear prediction we have: 
i
1
k
r
0
i
i
k
x
a
x
−
−
=
∧
∑
=
                                             (3.72) 
r being the prediction order and 
ia  coefficients chosen to minimize the prediction 
mean square error: 
−
2
kmin
Δ
. 
If r=0 (zero order prediction) we have: 
1
k
k
x
x
−
∧
=
   and   
1
k
k
k
x
x
Δ
−
−
=
                                (3.73) 
 
Remark 
DPCM is advantageous only if: 
X
Δ
max
k
〈
 

3.8   Lossy Compression in Differential Coding 
111
 
E.g.: suppose a zero order prediction and two values corresponding to two con-
secutive samples: 
X
x k =
  
X
or −
and   
X
x
1
k
−
=
−
  or  X ; 
it results that : 
,
X
2
Δk =
 
meaning that for the same quantization noise, the length of the corresponding digi-
tal word
DPCM
n
 is higher than that one required in PCM (
PCM
n
). 
The degree of compression is appreciated by the compression ratio: 
DPCM
PCM
DPCM
PCM
c
D
D
n
n
:
R
•
•
=
=
                                    (3.74) 
with the mention that the sampling frequency is equal for both modulations:  
Another quality parameter practically used is prediction gain: 
2
krms
2
rms
diffrence
x
p
Δ
X
P
P
G
=
=
                                     (3.75) 
A better compression is realized if adaptive DPCM is used (ADPCM). A fully 
adaptive DPCM system will adapt both the predictor and quantizer from the ADC. 
DPCM is widely used for voice and video compression. For example, in digital 
telephony, a standard channel has
kbps
D PCM
64
=
D
. Using a linear prediction 
coding (LPC [3]) the bit rate is diminished to 2.4 kbps, meaning a compression ra-
tio of: 
27.
kbps
2.4
kbps
64
D
D
R
DPCM
PCM
c
≈
=
= •
•
 
3.8.2   Delta Modulation (DM) 
Delta modulation is 1 bit DPCM of a zero order prediction, r = 0: 
Δ
x
x
Δ
1
k
k
k
=
−
=
−
                                           (3.76) 
Being quantized with q = 2 number of levels (n=1): 
⎩
⎨
⎧
−
<
+
>
=
−
−
Δ)
(
x
x
if
0,
Δ)
(
x
x
if
1,
'
Δ
1
k
k
1
k
k
k
                                   (3.77) 
Consequently, the corresponding bit rate is: 
S
DM
f
D
=
•
                                            (3.78) 

112 
3   Source Coding 
 
Remark 
The sampling frequency ( S
f ) used in DM is much higher that the one used in 
PCM and DPCM in order to avoid an acceptable quantized SNR. The greatest ad-
vantage of the DM is its simplicity. 
Slope overload 
distorsion 
x(t)
xq(t)
granular 
noise
A
x(t)
t
1111111010000100010 – Binary sequence at modulation output
 
Fig. 3.21 Illustration of the DM  
We have the following notations: 
 
-x(t) is the input signal 
-
)
(t
xq
is the stare case approximation of the x(t) signal 
 
Noise in Delta Modulation 
 
In any differential PCM modulation, occur two types of noise: the slope overload 
noise, meaning difficulties of following the signal (put into evidence by long se-
ries of “0” or “1”), and granular noise corresponding to constant segments of x(t) 
and acting when 101010… occur. 
Both types of noise will be illustrated for DM. 
 
a) Slope-overload noise (Fig. 3.22) 
 
 
Fig. 3.22 Slope-overload noise  

3.8   Lossy Compression in Differential Coding 
113
 
In order to eliminate this type of noise, the necessary condition is: 
S
S
f
Δ
T
Δ
dt
dx
max
⋅
=
≤
                                        
(3.79) 
The relation (3.79) indicates that the slope-overload condition requires large 
steps Δ . 
 
Example 3.16 
Assume a sinusoidal signal x(t): 
ft
Xsin2π
x(t) =
                                             (3.80) 
The slope-overload condition in this case will be: 
πft
2
πfXcos
2
dt
dx =
 
S
max
f
Δ
πfX
2
dt
dx
⋅
<
=
                                     (3.81) 
The relation shows the dependency of the slope-overload condition on fre-
quency. This relation shows the limitations that differential modulations have at 
high frequencies. 
 
b) Granular noise (Fig. 3.23) 
 
Fig. 3.23 Illustration of granular noise  
Under the assumption of lack of the slope overload, the quantization noise 
q
n is 
a random signal with maximum amplitude Δ
±
and with uniform pdf.  
Δ
= 2
1
)
( q
n
f
 

114 
3   Source Coding 
 
2Δ
1
K =
 
Fig. 3.24 Granular noise in DM - pdf representation  
The power of the quantization noise is: 
3
Δ
R
1
)dn
f(n
n
R
1
N
2
Δ
Δ
q
q
2
q
q
=
∫
=
+
−
                              (3.82) 
Remarks 
 
• 
2
Δ
≈
q
N
, meaning that to reduce the noise, a small Δ  is required. This re-
quirement to reduce the granular noise is opposite to the condition necessary 
for the slope overload condition ( Δ  large). A possible solution for both situa-
tions is the use of adaptive differential modulations [Jayant, Song..], in which 
Δ  is the adapted to the signal x(t), thus, the 
q
SNR  remains independent of 
x(t). 
• Comparing the quantized noise power in DM and PCM, it is obvious that: 
12
Δ
R
1
N
3
Δ
R
1
N
qPCM
2
qDM
=
>
=
 
but it is important to remember that in DM, 
M
S
2f
f
>>
meaning that at the re-
ceiver, after the final low-pas filtering at 
M
f
, only a part of 
qDM
N
 is deliv-
ered to the user (
qDM
N'
) [12]: 
3
Δ
f
f
N
f
f
N
2
S
M
qDM
S
M
/
qDM
=
≅
                                    
(3.83) 
Consequently, at the user, the quantized SNR is: 
2
2
rms
M
S
2
S
M
2
rms
qDM
X
qDM
Δ
X
f
f
3
3
Δ
f
f
X
N
P
SNR
=
=
=
                        (3.84) 

3.8   Lossy Compression in Differential Coding 
115
 
According to the relation (3.84), the following remarks could be done: 
 
 
 
 
 
M
S
qDM
2
qDM
2
rms
qDM
f
f
~
SNR
Δ
1
~
SNR
X
~
SNR
 
 
Example 3.17 
Assuming a sinusoidal signal (3.80), the slope-overload condition (3.81), fulfilled 
at limit, will give: 
S
f
πXf
2
Δ =
                                               (3.85) 
Consequently, the quantizing SNR (3.84) will be: 
2
M
2
S
2
qDM
f
1
f
f
π
8
3
SNR
=
                                   (3.86) 
The relation (3.86) indicates the reverse proportionality of the quantized SNR 
with frequency, showing that the differential modulations are disadvantageous at 
high frequencies: 
↓
↑⇒
qDM
SNR
f
. 
3.8.3   Delta-Sigma Modulation 
One solution to eliminate the already mentioned disadvantage of differential 
modulations is to use delta-sigma modulation, which allow a quantized SNR inde-
pendent of the signal frequency. 
Before delta modulation, the signal x(t) is integrated, and after delta modula-
tion, by derivation, the initial spectrum of x(t) is recovered. 
The principle of delta-sigma modulation is illustrated in Fig. 3.25. 
 
Fig. 3.25 Delta-Sigma modulation - demodulation block scheme  

116 
3   Source Coding 
 
3.8.4   Comparison and Applications of Digital Modulations 
Comparison of digital modulation is made always for the same signal x(t). It is a 
non-sense to compare a voice codec with a television one. Comparison can be 
done in two ways: 
• same bit rate (
ct.
D =
•
): the best modulation will ensure the greatest quantized 
SNR (the highest quality) 
• same quality (
ct.
SNR q =
): the best modulation is that one which has the 
smallest bit rate (
•
D ) – best compression. 
According to these criteria, the following remarks can be made: 
• PCM is the most general digital modulation 
• DPCM is the best for both criteria: highest 
q
SNR  at minimum bit rate
•
D . 
• DM is the simplest and obviously the cheapest, useful for applications in which 
the quality is not essential, but requiring robustness as military communications. 
3.9   Conclusions on Compression 
As showed in 3.2, the main purpose of source coding is the compression. This is 
accomplished by reducing the source’s decision rate as a consequence of decreas-
ing the redundancy.  
One classification of compression can be made as shown in figure 3.26. 
 
Fig. 3.26 Classification of compression   

3.9   Conclusions on Compression 
117
 
Reversible compression (lossless) is the compression that uses transforms that 
preserve the source entropy but decreases the redundancy.  
Irreversible compression (lossy) is the compression that uses entropy-reducing 
transforms. To this category belong digital modulations (differential pulse-code 
modulation, delta modulation) and orthogonal transforms; in the present book 
were described only the digital modulations. 
Statistic compression is the compression that requires the knowledge of 
source’s statistics, P(s); in this case the algorithms are also known as static algo-
rithms. If P(s) is determined as the first step and effective coding is performed af-
ter, the algorithm is semi-adaptive (quasi-dynamic). Thus, for unknown sources 
the use of statistic algorithms like Huffman, Shannon-Fano, implies, as a first step, 
to determine P(s). 
For memory sources, the lossless compression algorithms used are Run Length 
Coding and LZ. In these cases, streams are generated using groups of symbols, 
which act as a memoryless source and can be optimally encoded (e.g. run length 
coding with Huffman coding of the streams).  
Dynamic algorithms (adaptive) such as dynamic Huffman or LZ are used for 
compressing unknown sources. 
For memory sources, Markov models can also be used. 
 
Some real applications of compression 
 
Until quite recently the practitioner point of view [41] was to ignore data com-
pression due to the following reasons: 
 
• most compression techniques are unable to compress different data types.  
• it is impossible to predict the compression degree.  
• some of them “narrow-minded” and “afraid” of the “mysticism” that surrounds 
the maths incorporated in compression. 
• “fear” of the complexity introduced by the level of compression. 
 
All these have led to a big gap between research and practice which can be 
filled either by commercial pressure (the case of fax compression) or by a research 
effort to provide as simple as possible, more concrete aspects of the data compres-
sion techniques. 
Current trends indicate a rapid expansion of the compression techniques, taking 
into consideration their applications: multimedia, HDTV, disk drivers, compres-
sion software, CD recordings etc. A comprehensive description of the domain is 
done in [31]. 
However, it must not be forgotten that besides its great advantages (decreasing 
the bit rate in transmission and reducing the space in storage), compression is ex-
tremely vulnerable to errors; this is the reason why, applications including com-
pression require error protection. 

118 
3   Source Coding 
 
References 
[1] Aaron, M.R.: PCM transmission in the exchange plant. Bell System Technical Jour-
nal 41(99), 99–141 (1962) 
[2] Apiki, S.: Lossless data compression. Byte, 309–314,386-387 (1991) 
[3] Bellamy, J.: Digital Telephony, 2nd edn. John Wiley and Sons, Chichester (1991) 
[4] Berger, T.: Rate Distortion Theory: Mathematical Basis for Data Compression. Pren-
tice-Hall, Englewood Cliffs (1971) 
[5] Berrou, C., Glavieux, A.: Near Shannon limit error-correcting coding and decoding 
turbo-codes. In: Proc. ICC 1993, Geneva, pp. 1064–1070 (1993) 
[6] Borda, M., Zahan, S.: A new algorithm associated to Huffman source coding. Acta 
Technica Napocensis 1, 61 (1994) 
[7] Bruce, A., et al.: Molecular Biology of the Cell, 5th edn. Garland Science (2008) 
[8] CCITT yellow book, Digital Networks Transmission systems and multiplexing 
equipment, vol. 3, Geneva, ITU (1981) 
[9] Chevalier, J., Gheerbrant, A.: Dictionnaire des symboles, Laffont, Paris (1995) 
[10] Claverie, J.-M., Notredame, C.: Bioinformatics For Dummies, 2nd edn (2007) 
[11] Crosier, A.: Introduction to pseudoternary transmision codes. IBM Journal of Re-
search and Developement, 354–367 (July1970) 
[12] Fontoillet, P.G.: Systemes de telecommunications, Lausanne (1983) 
[13] Gallager, R.G.: Information Theory and Reliable Communication. John Wiley & 
Sons, Chichester (1968) 
[14] Gallager, R.G.: Variation on a theme by Huffman. IEEE Transactions on Information 
Theory IT 24, 668–674 (1978) 
[15] Graef, G.L.: Graphics formats. Byte, 305–310 (September 1989) 
[16] Hamming, R.: Coding and Information Theory. Prentice-Hall, Englewood Cliffs 
(1980) 
[17] Held, G., Marshall, T.R.: Data compression-techniques and applications, Hardware 
and software considerations. John Wiley and Sons, Chichester (1987) 
[18] Hedit, M., Guida, A.: Delay Modulation. Proceedings IEEE 57(7), 1314–1316 (1969) 
[19] Ionescu, D.: Codificare si coduri. Editura Tehnica, Bucuresti (1981) 
[20] Jayant, N.S., Noll, P.: Digital Coding of Waveforms: Principles and Applications to 
Speech and Video. Prentice-Hall, Englewood Cliffs (1984) 
[21] Johnston, B., Johnston, W.: LD-4. A Digital Pioneer in Action. Telesis 5(3), 66–72 
(1977) 
[22] Johannes, V.I., Kaim, A.G., Walzman, T.: Bipolar pulse transmission with zero ex-
traction. IEEE Transactions on Communications Techniques 17(2), 303–310 (1969) 
[23] Kay, S.M.: Fundaments of statistical Signal Processing. In: Detection Theory, vol. II. 
Prentice-Hall. Englewood Cliffs (1998) 
[24] Knuth, D.E.: Dynamic Huffman coding. Journal of Algorithms 6, 163–180 (1985) 
[25] Lu, W.W., Gough, M.P.: A fast-adaptive Huffman coding algorithm. IEEE Transac-
tions on Communications 41(4), 535–543 (1993) 
[26] Mateescu, A., et al.: Manualul inginerului electronist, Transmisiuni de date. Editura 
Tehnica, Bucuresti (1983) 
[27] Moon, T.K., Stirling, W.C.: Mathematical Methods and Algorithms for Signal Proc-
essing. Prentice-Hall, Englewood Cliffs (2000) 
[28] Rabiner, L., Schafer, R.W.: Digital Processing of Speech Signals. Prentice-Hall, 
Englewood Cliffs (1978) 

References 
119
 
[29] Rabiner, L., Schafer, R.W.: Introduction to Digital Speech Processing (Foundations 
and Trends in Signal Processing). Now Publishers Inc. (2007) 
[30] Radu, M.: Telefonie numerica. Editura Militara, Bucuresti (1988) 
[31] Sayood, K.: Introduction to Data Compression, 2nd edn. Morgan Kaufmann, San 
Francisco (2000) 
[32] Schena, M.: Microarray Analysis. John Wiley and Sons, Inc., Chichester (2003) 
[33] Spataru, A.: Fondements de la theorie de la transmission de l’information. Presses 
Polytechniques Romandes, Lausanne (1987) 
[34] Storer, J.A.: Data Compression: Methods and Theory. Computer Science Press, 
Rockville (1988) 
[35] Takasaky, Y., et al.: Optical pulse formats for fibre optic digital communications. 
IEEE Transactions on Communications 24, 404–413 (1976) 
[36] Van Trees, H.: Detection, Estimation and Modulation Theory, Part. I. John Wiley and 
Sons, Inc., Chichester (1968) 
[37] Vucetic, B., Yuan, J.: Turbo codes. Kluver Academic Publishers Group, Dordrecht 
(2001) (2nd Printing) 
[38] Wade, G.: Signal Coding and Processing. Cambridge University Press, Cambridge 
(1994) 
[39] Wade, G.: Coding Techniques: An Introduction to Compression and Error Control. 
Palgrave Macmillan, Basingstoke (2000) 
[40] Welch, T.: A Technique for high-performance data compression. Computer, 8–19 
(June 1984) 
[41] Williams, R.N.: Adaptive data compression. Kluwer Academic Publishers, Dordrecht 
(1990) 
[42] Wang, Q., Parrish, A.R., Wang, L.: Expanding the genetic code for biological studies. 
Chemistry & Biology 16(3), 323–336 (2009) 
[43] Xiong, F.: Digital Modulation Techniques. Artech House, Boston (2000) 
[44] Xie, J., Schultsz, P.G.: Adding amino-acids to the genetic repertoire. Curr. Opin. 
Chem. Biol. 9(6), 548–554 (2005) 
[45] Ziv, J., Lempel, A.: A universal algorithm for sequential data compression. IEEE 
Transactions on Information Theory IT 23(3), 337–343 (1977) 

Chapter 4 
Cryptography Basics 
Motto: 
  Get to know yourself. 
  (aphorism written on 
   the frontispiece of 
   Apollo Temple in 
   Delphi) 
4.1   Short History of Cryptography 
Information (religious, military, economic, etc) was always power, this is why the 
desire to protect it in order to be accessible only to some elites, dates since ancient 
times. There were discovered ciphertexts older then 4000 years, coming from the 
Ancient Egypt. There are records that ciphered writings were used since the 5th 
century BC in Ancient Greece. Thus the Spartan military used a stick (scytalae) 
around which narrow ribbon lather, papyrus or parchment was rolled, spiral by 
spiral, on which the clear text was written along the axis. After finishing the writ-
ing, the ribbon was unrolled, the message becoming meaningless. Only the person 
who had an identical stick (playing the role of secret key) with the one used at 
writing, could have read it. 
The Greek historian Polybius (2nd century BC) is the inventor of a ciphering 
square table size 5x5, table found at the origin of a large number of ciphers, used 
in modern times. 
Steganography (from the Greek words steganos, which means covered and 
graphein, meaning to write), the technique of hiding secret messages, has its ori-
gin in the same time. Herodotus, the ancient Greek historian (5th century BC), 
tells in “The Histories” that the tyrant of Miletus shaved the head of a trusty slave 
and tattooed it with a  secret message and, after hair regrown, sent him to Athens 
to instigate a rebellion against the Persians [42]. 
In Ancient Rome the secrecy of political and military information was ensured 
by encryption, the most famous being Caesar cipher, used since the Gallic Wars 
(58-51 BC). 
All the Sacred Books of each religion contain parables and encrypted messages. 
Less known, the Arabian contribution to the development of cryptology is re-
markably important. David Kahn one of the greatest historians in the domain, under-
lines in his book [41], that cryptology is born in the Arabic world. The first three  
centuries of Islamic civilization (700- 1000 AC), besides a great political and military 

122
4   Cryptography Basics
 
extension, are a period of intensive translations in Arabic of the greatest works of 
Greek, Roman, Indian, Armenian, Hebrew and Syriac Antiquity. Some of them were 
written in already dead languages, meaning that they represented in fact ciphertexts. 
To read such texts is noting else than cryptanalysis. Al Kindi, an Arab scientist from 
9th century is considered the father of cryptology, his book on this subject being, at 
this moment, the oldest available [41]. That time, the scientific language was Arabic. 
The Arabs took over the Greek and Indian knowledge and introduced the decimal 
numeral system with the Arabic numbers (numerals). The terms zero, cipher, algo-
rithm, algebra are due to Arabs. Cipher comes from the Arabian “şifr” which is the 
Arabic translation of number, “zero” from Sanskrit language. When the concept 
“zero” was introduced in Europe, using that time the Roman numerals, it produced a 
great ambiguity. For this reason, those speaking unclear were baptised as “cipher 
speakers”. This meaningless sense of a message is known even today as cipher. 
During the Renaissance, besides the revival of the interest of the ancient civili-
sations, ancient cryptography was discovered. The expansion of diplomatic rela-
tions among different feudal states determined the progress of cryptography. To 
that period belong: 
 
• Leon Battista Alberti (1404- 1472), the inventor of the polyalphabetic cipher 
(Alberti cipher), using a cipher disk, considered the most important advance in 
cryptography since Antiquity. 
• Johannes Trithemius (1462- 1516), the author of the famous encrypted book 
Steganographia, in three volumes. 
• Giovani Batista Bellaso (1505- ?), the inventor of the polyalphabetic substitu-
tion with mixed alphabets, known later as the Vigénère cipher. 
• Giambattista della Porta (1535- 1615), the inventor of the digramic substitu-
tion cipher. 
 
The telegraph and radio inventions in the 19th century, as well the two World 
Wars from the past century, have been strongly stimulating the development of 
cryptographic algorithms and techniques [41]. 
The continuous development and spreading of computer use in our life, the exis-
tence and fast growing of communications networks, the existence of powerful  
databases, the introduction and development of e-commerce and web mail accounts 
indicate an uncommonly growth of the volume and importance of transmitted or 
stored data and implicitly of their vulnerability. Security, in these systems, refers to: 
 
• eliminate the possibility of deliberately or accidentally data destruction 
• guarantee communication confidentiality in order to prevent data leakage to 
unauthorized persons 
• authentication of data and entity in order to prevent unauthorized person from 
introducing or modifying data into the system 
• in some particular cases such as electronic funds transfers, contracts negotia-
tions, the existence of an electronic signature is important to avoid disputes be-
tween the transmitter and receiver about the sent message. 
 
All these objectives show a great expansion of cryptography from diplomatic, mili-
tary, political domains to the civil area, with strong economical and social features. 

4.2   Terminology 
123
 
4.2   Terminology 
Cryptography from Greek kryptos meaning hidden, secret and graph whide means 
writing is the science of secret writings. 
Plaintext/ cleartext (M) is the message requiring to be protected. In cryptogra-
phy it is known as text, however its nature: voice, image, video, data. 
Ciphertext (C) is the nonsense shape of the protected plaintext, inaccessible for 
adversaries. 
Encryption/ enciphering is the transformation (E) of the plaintext M into ci-
phertext: E(M)=C. 
 
Decryption/ deciphering is the reverse transformation (D) of encryption, i.e. the 
determination of the cleartext from the ciphertext: D(C)=D(E(M))=M. 
Both encryption and decryption are controlled by one or more cryptographic 
keys (ki). 
Cryptographic algorithm/ cipher is the mathematical function or functions used 
for encryption (E)/ decryption (D). 
Cryptanalysis is the art of breaking ciphers, the process of obtaining the plain-
text or the decryption key from the ciphertext only. 
Cryptology is the science of both cryptography and cryptanalysis. 
Cryptographer is the person dealing with cryptography. 
Cryptanalyst is the person dealing with cryptanalysis. 
Cryptologist is the person dealing with cryptology. 
Attack is the cryptanalytic attempt. 
Cryptosystem is the system where an encryption/ decryption process takes 
place. 
Steganography is the technique of hiding secret messages into harmless messages 
(hosts), such that the very existence of the secret message is hidden (invisible). 
4.3   Cryptosystems: Role and Classification 
The block- scheme of a cryptosystem is presented in Fig. 4.1. 
 
 
Fig. 4.1 Block-scheme of a cryptosystem where: A, B - entities which transmit, receive the 
information, E - encryption block, D - decryption block, M- plaintext, C - ciphertext, K - 
cryptographic key block,  
e
k - encryption key, 
d
k  - decryption key 

124
4   Cryptography Basics
 
Role of a cryptosystem 
 
The main tasks that a cryptosystem need to accomplish are: 
 
• Confidentiality/ secrecy/ privacy: an unauthorized user should not be able to de-
terminate 
d
k  from the ciphertext C even knowing the plaintext M. (Fig. 4.2); it 
means that the decryption (D) is protected. 
 
Fig. 4.2 Illustration of confidentiality 
• Authentication (Fig. 4.3) is applied to: 
 
– 
entities (persons, terminals, credit cards, etc) and in this case is known as 
entity authentication/ identification. 
– 
information: the transmitted or stored information need to be authentic as 
origin, content, time of transmission or storage and defines data. authenti-
cation/ data integrity. 
 
Fig. 4.3 Illustration of authentication  
• Digital signature  (S) is a mutual  authentication (both data and entity authenti-
cation) User B must be certain that the received message comes precisely from 
A, being signed by A and A must be certain that nobody will be able to forge 
his signature. 
 
Besides these three main objectives, the cryptosystem may ensure also, nonre-
pudiation, authorization, cancellation, access control, certification, confirmation, 
anonymity, etc. [64], [68]. 
 

4.4   Cryptanalytic Attacks and Algorithms Security 
125
 
Cryptosystems classification 
According to the type of keys used for encryption (
e
k ), respectively for decryp-
tion (
d
k ), the cryptosystems are: 
 
• symmetric (conventional): use the same key for encryption and decryption. 
k
k
k
d
e
=
=
                                                (4.1) 
and thus the key (k) is secret and requires a secure channel for transmission.  
C
(M)
E k
=
                                               (4.2) 
M
(M))
(E
D
(C)
D
k
k
k
=
=
                                      (4.3) 
Remark 
For simplicity the key is not always mentioned, thus is written: 
E(M)=C                                                (4.2.a) 
D(C)=D(E(M))=M                                         (4.2.b) 
• asymmetric (public-key) use distinct keys for encryption (
e
k ) and decryption 
(
d
k ): 
d
e
k
k
≠
                                                   (4.4) 
The encryption key, 
e
k , is public, whereas the decryption key, kd, is private 
(secret). Encryption is done with the public key of the correspondent X (
ex
k
) and 
decryption is made using the private key of the correspondent X (
dx
k
). For sim-
plicity the notation will be: 
keX
X
E
E
=
                                              (4.5) 
kdX
X
D
D
=
                                              (4.6) 
Thus, if A wants to communicate confidentially with B using a public key cryp-
tography (PKC), the protocol will be: 
 
1) A will encrypt the message M with the public key of B 
C
(M)
E B
=
                                                (4.7) 
2) B will decrypt the cryptogram C using its private key 
M
(M))
(E
D
(C)
D
B
B
B
=
=
                                    (4.8) 
4.4   Cryptanalytic Attacks and Algorithms Security 
Cryptanalytic attack is the attack made on the ciphertext in order to obtain the 
cleartext or the key necessary for decryption. Basically there are six types [64]: 

126
4   Cryptography Basics
 
1. ciphertext only attack: the cryptanalyst has the ciphertext 
)
(M
E
C
i
k
i =
 and 
must obtain the plaintext 
i
M  or the decryption key. 
2. known plaintext attack: the cryptanalyst has the ciphertexts 
i
C  and the corre-
sponding plaintexts 
i
M  and needs to find the key k or the algorithm to obtain 
1
i
M +  from
)
(M
E
C
1
i
k
1
i
+
+ =
. 
3. chosen plaintext attack:  it is possible to choose some plaintexts 
i
M  and to 
know the corresponding cryptograms:
)
(M
E
C
i
k
i =
; the cryptanalyst must  
find the key k or the algorithm of giving 
1
i
M +  from
)
(M
E
C
1
i
k
1
i
+
+ =
. 
4. adaptive chosen plaintext attack: the plaintexts 
i
M  can be chosen and are 
adaptive to the previous cryptanalitic attacks results and the ciphertexts 
)
(M
E
C
i
k
i =
 are known. It is required the key k or the algorithm to obtain 
1
i
M +  from
)
(M
E
C
1
i
k
1
i
+
+ =
. 
5. chosen ciphertext attack: the cryptanalyst can choose ciphertexts 
i
C  and plain-
texts 
)
(C
D
M
i
k
i =
, his task being that of finding the decryption key k; this at-
tack is used mainly in PKC (Public Key Cryptography). 
6. rubber-hose cryptanalysis/ purchase key attack, when the key is obtained with-
out cryptanalysis (blackmail and robbery being the main ways); this is one of 
the most powerful attacks. 
 
Algorithms security 
 
In 19th century, the Dutch Auguste Kerckhoffs stated the fundamental concept of 
cryptanalysis, known as Kerckhoffs low: the security of a cryptosystem need to lie 
only in the key, the algorithm and the implementation being known. It was refor-
mulated later [66] by Claude Shannon “as the enemy knows the system” and is 
opposite to the principle “security by obscurity”. 
Different algorithms provide different degree of security, according to the diffi-
culty of breaking them [64]. 
 
1. total break (zero security): the key is known 
2. global deduction: a cryptanalyst finds an alternative algorithm equivalent to 
D(C) without the knowledge of the key. 
3. local deduction: the cryptanalyst finds the cleartext from the intercepted cipher-
text. 
4. information deduction: the cryptanalyst gets some information about the key or 
the cleartext. 
5. computationally secure (strong) is the algorithm that cannot be broken with 
available resources in present and in the near future. 
6. unconditionally secure is the algorithm impossible to be broken.  
One time pad (OTP) is the only unconditionally secure algorithm. It was in-
vented in 1917 by Gilbert Vernam and in 1949 Claude Shannon proved OTP en-
sures perfect secrecy [66]:  
H(M)=H(M/C)                                               (4.9) 

4.5   Classic Cryptography 
127
 
where H(M) is the a priory entropy of the planetext and H(M/C) is the a posteriory 
entropy (conditional entropy of M). 
OTP is a random key with at least the same length as the plaintext (M) and is 
used only once. 
All other algorithms are breakable trying all the possible keys for a given ci-
phertext, until the plaintext is meaningful. This is the brute force attack (ex. If the 
key is n long, the number of possible keys is
n
2 ). 
Complexity of an attack [64] can be expressed in different ways: 
 
a) data complexity represents the amount of data required to fulfil the attack. 
b) processing complexity (work factor) is the time necessary to perform the attack.  
c) storage complexity is given by the required memory to do the attack. 
Rule: Complexity of an attack= min{a, b, c} 
 
 
Remark 
Many attacks are suitable for parallelisation, meaning that the complexity can be 
substantially reduced (see the possibility to use the graphics processor units - 
GPU, for cryptanalytic purposes, based on its parallelism). 
In cryptanalysis, when appreciating the size of an attack, we need to keep in 
mind Moore law [64]: computing power doubles approximately at 18 months and 
the costs diminish 10 times every five years. 
4.5   Classic Cryptography 
4.5.1   Definition and Classification 
By classic cryptography usually is understood the cryptography since Antiquity 
until modern times, defined by Shannon work “Communication Theory of Secrecy 
Systems” published in 1949 in the Bell System Technical Journal. 
Classic ciphers are symmetric ciphers, meaning the use of the same key, which 
is secret, for encryption and decryption. Basically they are hiding the plaintext us-
ing some elementary transformations. 
 
• substitutions, in order to create confusion (a very complex relation between the 
plaintext and the ciphertext) 
• transpositions, made by permutations, in order to diffuse the plaintext redun-
dancy into the statistics of the ciphertext. 
 
Some algorithms (ciphers) are applying these transformations iteratively in or-
der to enhance the security. 
 
 
 
 
 

128
4   Cryptography Basics
 
According to the type of transformation, classic ciphers are classified as given 
in Table 4.1: 
Table 4.1 Classification of classic ciphers  
Type of transformation 
Variety 
Cipher name 
Caesar 
monoalphabetic 
Polybius 
omophonic 
Example: 
E     ĺ B 
      ĺ J 
        ĺ Q 
polygramic 
Playfair 
Trithemius 
Substitution 
polyalphabetic 
Vigénère 
Transposition 
Ex: 
V 
E 
N 
I 
V 
I 
D 
I 
V 
I 
C 
I 
 
ĺVVVEIINDCIII 
ADFGVX 
 
C 
L 
A 
S 
S 
I 
C 
 
 
C 
I 
P 
H 
E 
R 
S 
 
Rotor machines 
electro-mechanical devices implementing Vigé-
nère versions 
Enigma 
 
Substitution ciphers: a character or a group of characters of the plaintext is sub-
stitute for another character or group of characters in the ciphertext. 
Transposition ciphers: the plaintext remains unchanged in the ciphertext, only 
the order of the characters is changed (a permutation is done). 
Rotor machines is an electro-medical device, invented in 1920 (a continuation 
of Leon Battista Alberti idea used in his cipher disk) used to automatize the com-
plicated iterated substitutions and permutations. Such a device contains a key-
board and a set of rotors, each one allowing the implementation of a Vigénère ver-
sion. The most famous rotor machine is Enigma, used during World War II [41]. 
In what follows, the most famous classic ciphers will be presented. 
4.5.2   Caesar Cipher 
Caesar cipher is a monoalphabetic substitution cipher. Each letter of the plaintext 
is replaced by a new one obtained shifting three positions to right. The encryption 
is given by the relation: 
3)mod26
(M
)
E(M
C
i
i
i
+
=
=
                              (4.10) 

4.5   Classic Cryptography 
129
 
and decryption is given by: 
3)mod26
(C
)
D(C
M
i
i
i
−
=
=
,                             (4.11) 
26 being the dimension of the Latin alphabet. 
Caesar cipher is represented in Table 4.2. 
Table 4.2 Caesar cipher  
Clear 
text A B C D E 
F G H I J
K L M N O P Q R S T U V W X Y Z 
Cipher 
text D E F G H I 
J K L M N O P Q R S T U V W X Y Z A B C 
 
Example 4.1 
Using Table 4.2, the well known sentence becomes the ciphertext:  
 
Cleartext: 
V E N I              V I D I  
V I C I 
Ciphertext 
Y H Q L  
Y LG L   
Y L F L 
 
Using the same Table 4.2, the deciphering is at once: ciphertextĺ plaintext. 
Caesar cipher can be generalized: 
k)mod26
(M
C
i
i
+
=
                                        (4.12) 
k)mod26
(C
M
i
i
−
=
                                       (4.13) 
where k, the shifting, can take 25 values: 
1,25
k =
                                               (4.14) 
meaning 25 possible keys. 
 
Remarks 
 
1. Caesar cipher is a simple linear congruential generator [64]: 
b)modN
(aX
X
1
n
n
+
=
−
                                      (4.15) 
A linear congruential generator is a pseudo-random generator given by relation 
(4.15), where a, b, N are constants and signify: 
 
a- multiplier (1 for Caesar cipher) 
b- increment (3 for Caser cipher) 
N- modulus (26 for Caesar cipher) 
 
2. Caesar and generalized Caesar ciphers are not at all secure, the cryptanalysis being 
very quick. The brute force attack consists of trying all the 25 keys (generalized 
Caesar), until the message becomes meaningful. If the alphabet of the plaintext is 
not known, the cryptanalysis is not easy. If the encryption is applied after com-
pression, cryptanalysis is much harder. For example if the plaintext is in the first 
step compressed using ZIP file format [68] which has an alphabet completely  
different (Latin letters, Greek letters, geometric symbols, etc.), and then encrypted 
using a simple substitution cipher, the brute force attack will not be successful. 

130
4   Cryptography Basics
 
3. monoalphabetic substitution ciphers, for which the alphabets of the plaintext 
and ciphertext are known, are very sensitive to relative frequency analysis [68]. 
For them, the relative frequency of letters in the ciphertext is the same with that 
of the plaintext (which is known if the language is known), meaning that the 
cryptanalysis is quite simple, and the steps are: 
 
– 
determination of relative frequency of letters in the a ciphertext. 
– 
comparison with the relative frequency of letters from the plaintext 
(known), resulting a sort of “skeleton” of the cleartext. 
– 
refinement of the analysis by search, until the result is a meaningful plaintext. 
4. ways to avoid the relative frequency analysis, so to make more difficult the 
cryptanalysis, are: to use omophonic substitutions, polygramic or polyalpha-
betic in order to make as possible, an uniform relative frequency for letters in 
the ciphertext (maximum entropy). 
5. a particular case of generalized Caesar cipher is ROT13, having k=13; thus 
ROT13 is its own inverse: applied twice, it gives the clear text. 
C=ROT13(C)                                               (4.16) 
M=ROT13(C)=ROT13(ROT13(M))                             (4.17) 
 
Remark 
This cipher is not used for security purposes (which is extremely poor), but for 
hiding potentially offensive messages (at users stations, in networks); it was in use 
in the early 1980, in the net.jokes newsgroup. 
4.5.3   Polybius Cipher 
The Ancient Greek historian Polybius (203-120 BC), being responsible with the 
operation of a “telegraph” used to send at distance messages, invented a substitu-
tion cipher, known since that as Polybius square (Table 4.3). The letters of the 
Latin alphabet (26) are arranged in a square of size 5x5. The letters I and J are 
combined in a unique character because the choice between them can be easily 
decided from the text meaning. The encryption consists of replacing each letter 
with the corresponding pair of numbers (the line and column crossing point). 
Table 4.3 Polybius square  
 
1 
2 
3 
4 
5 
1 
A 
B 
C 
D 
E 
2 
F 
G 
H 
I/J 
K 
3 
L 
M 
N 
O 
P 
4 
Q 
R 
S 
T 
U 
5 
V 
W 
X 
Y 
Z 

4.5   Classic Cryptography 
131
 
Example 4.2 
Using Polybius square (Table 4.3), the message S O S is encrypted 43 34 43. 
 
Remarks 
 
• this substitution cipher is using distinct alphabets for plaintext (letters) and ci-
phertext (numerals). 
• the cipher can be changed, changing the disposal of letters in the square, which 
plays the role of key (see Playfair cipher). 
4.5.4   Playfair Cipher 
Playfair cipher is one of the most famous polygramic ciphers. 
A polygramic cipher is using substitution of a group of characters in the plain-
text alphabet, known as “poligram”, with other groups of characters, for example: 
ABA ĺ RTQ, 
in order to obtain uniform relative frequencies for all the letters in the ciphertext, 
and thus to heavier the cryptanalysis. 
Playfair cipher was invented by Sir Charles Wheatstone in 1854, but is carrying 
the name of Lord Playfair, who was the promoter of its application [67]. 
• It uses a square of dimension 5x5 (as Polybius square), made using a keyword. 
• The square is filled with the letters of the keyword (without repeating  letters) 
downwards, from left to right; the letters I,J are put in the same box, as in Poly-
bius square. 
Encryption is done on groups of two letters, known as “digram”: 
• if the letters of the diagram are in the same line (l), the right side character is 
taken. 
• if the letters are in the same column (c) the bottom neighbour is taken. 
• if they are on different lines and columns the character found at the crossing 
point of line (l) and column (c) is chosen. 
 
Example 4.3 
Using Playfair cipher, encrypt the plaintext CRYPTOGRAPHY, using the key-
word LEONARD. 
 
Solution 
 
Table 4.4 Illustration of Playfair cipher  
 
CR YP TO GR AP HY       plaintext 
.       .            .     .     . 
.       .     Ļ     .     .     . 
.       .            .     .     . 
 
FD VT SN PG LU KW      ciphertext 
L 
E 
O 
N 
A 
R 
D 
B 
C 
F 
G 
H 
IJ 
K 
M 
P 
Q 
S 
T 
U 
V 
W 
X 
Y 
Z 

132
4   Cryptography Basics
 
Playfair cipher advantages 
 
Playfair cipher has some advantages compared to the monoalphabetic substitution 
ciphers: 
 
• the alphabet is the same  (N=26 letters), but the number of digrams is: 
26x26=676, meaning that the identification of distinct digrams is harder. 
• the relative frequency analysis is made more difficult 
• reasonably fast 
• requires no special equipment. 
 
For his advantages, long time Playfair cipher was thought unbreakable. 
The first break of Playfair cipher was given in 1914 by Joseph O. Maubergne, 
officer in US Army, and coinventor with Gilbert Vernam of OTP. 
Playfair cipher was extensively used by British Empire in the Second Boer War 
(1879- 1915) and World War I, and also by Australians and Germans in World 
War II [41]. 
4.5.5   Trithemius Cipher 
Trithemius cipher belongs to polyalphabetic substitution ciphers. 
A polyalphabetic substitution cipher is composed of more simple substitution 
ciphers. It was invented by Leon Battista Alberti (1404- 1472), one of the greatest 
Renaissance humanist polymaths (from the Greek polymates meaning “having 
learned much”): architect, poet, priest, linguist, philosopher and cryptographer. He 
is also the inventor of the cipher disk, named also formula. This device, the pre-
cursor of rotor machine, is the first polyalphabetic substitution example, using 
mixed alphabet and variable period. It consists of two concentric disks, fixed to-
gether by a pine and able to rotate one with respect to the other. 
 
Fig. 4.4 Alberti cipher disk (formula) 
 
Johannes Trithemius (1462- 1516), born in Trittenheim, Germany, was a Bene-
dictionne abbot, historian, cryptographer and occultist. As cryptographer he is  

4.5   Classic Cryptography 
133
 
famous for the cipher caring his name and for his work Steganographia in three 
volumes, written in 1499 and published in Frankfurt in 1606, an encrypted work, 
the last volume being recently decrypted, this is why a great aura of occultism sur-
rounded his author. After being totally decrypted, it showed that the whole book 
deals with cryptography and steganography. 
Trithemius cipher is a polyalphabetic substitution cipher. The 26 letter Latin al-
phabet is disposed in a rectangular square of dimension 25x25, known as tabula 
recta (Tab. 4.5). Each row, numbered from 0 to 25, contains the 26 letters alpha-
bet cyclically shifted to the right. The row numbered with 0 is the alphabet in ini-
tial order (without shifting). The row numbered with 1 is a cyclic shifting of the 
previous row (0), with one position to the right, and so on. The encryption is as 
follows: the first character of the plaintext is encrypted selecting it from the first 
row, the second character from the second row and so on. 
Table 4.5 Tabula recta of Trithemius cipher  
 
1 2 3
4
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 
0 A B C D E F G H I 
J K L M N O P Q R
S
T
U V W X Y Z 
1 B C D
E
F G H I J K
L M N O P Q R
S
T
U V W X Y Z A 
2 C D E
F
G H I 
J K L M N O P
Q R S
T
U V W X Y Z A B 
3 D E F
G H I 
J K L M N O P Q R
S T U V W X Y
Z A B C 
4 E F G H
I J K L M N O P Q R
S
T U V W X Y
Z
A B C D 
5 F G H
I 
J K L M N O
P Q R S
T U V W X Y
Z
A
B C D E 
6 G H I 
J K L M N O P
Q R S T U V W X Y
Z
A B
C D E F 
7 H I 
J K L M N O P Q R
S T U V W X Y
Z
A B
C
D E F G 
8 I J K
L M N O P Q R
S
T U V W X Y Z
A B
C D
E F G H 
9 J K L M N O P Q R S
T U V W X Y Z A
B
C D
E
F G H I 
10 K L M N O P Q R S T
U V W X Y Z A B
C D
E
F
G H I 
J 
11 L M N O P Q R S T U V W X Y Z A B C
D
E
F
G H I 
J K 
12 M N O
P
Q R S T U V W X Y Z A B C D
E
F
G H
I 
J K L 
13 N O P
Q R S T U V W X Y Z A B C D E
F
G H
I 
J K L M 
14 O P Q R
S T U V W X Y Z A B C D E
F
G H
I 
J K L M N 
15 P Q R
S
T U V W X Y
Z A B C D E F
G H
I 
J K
L M N O 
16 Q R S
T U V W X Y Z
A B C D E
F G H
I 
J K
L M N O P 
17 R S T
U V W X Y Z A B C D E
F G H
I 
J K
L M N O P Q 
18 S T U V W X Y Z A B
C D E F G H
I
J K
L M N O P Q R 
19 T U V W X Y Z A B C D E F G H
I
J
K
L M N O
P Q R S 
20 U V W X Y Z A B C D
E
F G H
I 
J
K L M N O
P
Q R S T 
21 V W X Y Z A B C D E
F G H
I
J K L M N O
P
Q
R S T U 
22 W X Y
Z A B C D E F
G H
I
J
K L M N O
P
Q R
S T U V 
23 X Y Z
A B C D E F G H
I
J
K L M N O
P
Q R
S
T U V W 
24 Y Z A B C D E F G H
I 
J
K L M N O
P
Q R
S
T
U V W X 
25 Z A B
C D E F G H I 
J K L M N O P
Q
R
S
T
U V W X Y 
 
1 2 3
4
5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  

134
4   Cryptography Basics
 
Example 4.4 
 1 2  3 4   
5  6 7 8   
 9 10 11 12 
Plaintext:  
 
 V E N I  
V I D I   
  V  I  C   I 
Ciphertext: 
 
W G O M 
A O K Q  
  E  S  N  U 
 
4.5.6   Vigénère Cipher 
Vigénère cipher is a polyalpahbetic substitution cipher using Trithemius tabula 
recta and a keyword which command row selection for encryption and decryption. 
This method belongs in fact to the Italian cryptologist Giovan Basttista Bellaso 
(1505- ?) and in 19th century was assigned to Blaise de Vigénère (1523- 1596), a 
French diplomat and cryptographer. Vigénère published the original cipher of Bat-
tista Bellaso and proposed a much stronger key, such that this cipher was consid-
ered unbreakable (le chiffre indéchiffrable) till 19th century when the first break 
was published. 
Being strong and simple enough, if used with cipher disks, it was used even 
during the American Civil War (1861- 1865). 
Gilbert Vernam tried to repair the broken Vigénère cipher (creating the Ver-
nam- Vigénère cipher (1918), unfortunately still vulnerable to cryptanalysis. But 
Vernan work led to the OTP, the unbreakable cipher, as demonstrated by Cl. 
Shannon [66]. 
 
Example 4.5 
A Vigénère encryption of the plaintext VENI VIDI VICI using the key MONA is 
presented in Table 4.6. 
Table 4.6 Vigénère encryption with key word  
Keyword 
M 
O 
N 
A 
M 
O 
N 
A 
M 
O 
N 
A 
Plaintext 
V 
E 
N 
I 
V 
I 
D 
I 
V 
I 
C 
I 
Ciphertext 
H 
S 
A 
I 
H 
W 
Q 
I 
H 
W 
P 
I 
 
An improved version of the presented Vigénère cipher is using a trial key. The 
trial key indicates the beginning line or lines for the first character or characters of 
the plaintext. Afterwards, the characters of the plaintext are used as keys. This 
means a feedback in the encryption process, the ciphertext being conditioned by 
the message itself. 
 
Example 4.6 
A Vigénère encryption with trial key: letter D and plaintext key word of the  
message: 
VENI VIDI VICI, 
is presented in Table 4.7 

4.6   Modern Symmetric (Conventional) Cryptography 
135
 
Table 4.7 Vigénère encryption with trial- key letter and plaintext keyword  
Keyword 
D 
V 
E 
N 
I 
V 
I 
D 
I 
V 
I 
C 
Plaintext 
V 
E 
N 
I 
V 
I 
D 
I 
V 
I 
C 
I 
Ciphertext 
Y 
Z 
R 
V 
D 
D 
L 
L 
D 
D 
K 
K 
 
Another version of Vigénère cipher is Vigénère ciphertext keyword. A trial-key 
is selected and afterward the characters of the ciphertext become keywords. 
 
Example 4.7 
A Vigénère encryption with trial-key letter D and ciphertext keyword of plaintext: 
VENI VIDI VICI, 
is given in Table 4.8. 
Table 4.8 Vigénère encryption with trial-key and ciphertext keyword 
Keyword
D 
Y 
C 
P 
X 
S 
A 
D 
L 
G 
O 
Q 
Plaintext
V 
E 
N 
I 
V 
I 
D 
I 
V 
I 
C 
I 
CiphertextY 
C 
P 
X 
S 
A 
D 
L 
G 
O 
Q 
Y 
 
Remarks 
 
• even though every character used as key can be found from the previous char-
acter of the ciphertext, it is functionally dependent on all the previous plaintext 
characters, including the trial key; the result is the diffusion effect of the statisti-
cal properties of the plaintext over the ciphertext, making thus the cryptanalysis 
very hard. 
• for present security requirements, Vigénère  cipher schemes are not very reliable; 
Vigénère important role lies in the discovery of non-repetition sequences genera-
tion (as key) using the message itself (plaintext or ciphertext) or parts of it. 
4.6   Modern Symmetric (Conventional) Cryptography 
4.6.1   Definitions and Classification 
The modern cryptography is considered to start with Claude Shannon work 
“Communication Theory of Secrecy Systems”, published in 1949 in Bell System 
Technical Journal, based on a classified version of “A Mathematical Theory of 
Cryptography” presented in September 1945 for Bell Telephone Labs [65]. 
In these works he gave the mathematical model of a cryptosystem, the conditions 
for a perfect secrecy, the mathematical definition of the basic processing in encryp-
tion: confusion and diffusion, the mathematical description of a product cipher. 

136
4   Cryptography Basics
 
Assume that all the plaintexts 
i
M , with 
m
1,
i =
 and their a priori, probabilities 
i
p  are known. By encryption, each 
i
M  is transformed in a ciphertext
i
C . Crypt-
analysis tries to calculate from 
i
C  the aposteriori probabilities of different plain-
texts: 
)
C
M
p(
i
i
. 
A cryptosystem is perfectly secret if: 
)
p(C
)
M
C
p(
i
i
i
=
,   
m
1,
i =
∀
                                  (4.18) 
)
p(M
)
C
M
p(
i
i
i
=
,   
m
1,
i =
∀
                                 (4.19) 
m being the number of distinct plaintexts (
i
M ), respectively ciphertexts (
i
C ). It 
means that the aposteriori probabilities equal the a priori ones. 
These conditions imply the followings: 
 
1) the number of keys (
i
k ) ≥ the number of plaintexts (
i
M ) 
(4.20.a) 
2) the length of keys (
i
k ) ≥ the length of plaintext (
i
M ) 
(4.20.b) 
3) a key need to be used only once 
(4.20.c) 
 
These three conditions define the one time pad (OTP) principle. 
The average quantity of information per plaintext
i
M , respectively ciphertext 
i
C  is the corresponding entropy: 
∑
−
=
=
m
1
i
i
2
i
)
p(M
)log
p(M
H(M)
                               (4.21) 
∑
−
=
=
m
1
i
i
2
i
)
p(C
)log
p(C
H(C)
                               (4.22) 
As shown in chapter 2, the maximum value of the entropy (the decision quan-
tity D, (2.14)) is obtained when the messages are equally probable: 
m
log
D(M)
H(M)
2
=
≤
                                    (4.23) 
This information is completely hidden [3], [65] when the non-determination of 
the key is the greatest, which means that: 
m
log
H(C)
2
=
                                          (4.24) 
Relation (4.24) shows that the maximum non-determination introduced in a 
cryptosystem cannot be greater than the size of the key space: 
m
log2
. It means 
that a cryptosystem using n bits length key will give 
n
2
m =
 distinct keys (the 
brute force attack dimension). As H(C) is grater, meaning m, respectively n 
greater, the breaking of a cryptosystem is harder. 
 
 
 

4.6   Modern Symmetric (Conventional) Cryptography 
137
 
Notice 
Cryptanalysis is using redundancy of the plaintext (see 2.9.3). As known (see 
chapter 3), compression removes redundancy, this is why in real cryptographic 
implementation [3], [64], the first step is to compress the plaintext and only after 
that to encrypt it. 
The basic processing in cryptography was defined mathematically also by 
Shannon: 
 
• confusion defines the relation between the plaintext 
i
M , and ciphertext  
i
C  which need to be as much possible complex and confused (chaotic);  
substitution (S box in modern ciphers) was identified as a primary confusion 
mechanism. 
• diffusion [64] refers to the property of diffusing the redundancy of the plaintext 
i
M  into the statistics of the ciphertext 
i
C ; transposition, made by permutation 
(P box in modern ciphers), and linear transformations in finite fields were iden-
tified as diffusion mechanisms. 
Product cipher (SPN- Substitution- Permutation- Network) defines the alterna-
tion of S and P boxes during r rounds (iterations) were explained mathematically 
also by Shannon [3], [65]. 
Classification of modern symmetric ciphers, is made according to the type of 
information processing (as in error control coding - see chapter 5): 
 
• block ciphers: the cleartext M is processed in blocks 
i
M  of constant length n, 
each block 
i
M  being encrypted once; the result is the ciphertext 
i
C  of the 
same length n. 
• stream ciphers: the encryption is character oriented (n=1 character), being in 
fact modern Vigénère versions. 
4.6.2   Block Ciphers 
4.6.2.1   Main Features 
• the cleartext M is divided in blocks of constant length n (usually 32 ÷ 128 bits), 
each block 
i
M  being independently encrypted. 
• each block cipher is a product cipher of S and P boxes, iterated r times; the 
number of iterations (rounds) r varies: 1 in classic ciphers, 16 in DES, Blow-
fish, 8 in IDEA, 10, 12 or 14 in AES, 48 in T-DES, etc. As r is greater, the 
cryptanalysis is harder, but also the speed of execution is greater. This is why a 
trade between security and speed of executing is done, the accepted value in 
most applications being
16
r0 =
. 
 
 
 

138
4   Cryptography Basics
 
P box (Fig. 4.5) ensures the diffusion by permutation. 
0
M
1
M
2
M
3
M
7
M
0
C
1
C
2
C
3
C
7
C
 
Fig. 4.5 Example of P box with n=7  
The transformations made by a P box are linear; the internal links (permutations) 
being easily determined putting a binary “1” at each input and finding the corre-
sponding output; if the input is of size n, the possible permutations of P box are (
n
2 )! 
S box, which ensures the confusion, is usually composed of a binary to decimal 
converter (BDC) at the input, followed by a P box making the transposition be-
tween decimal positions and having a decimal to binary converter (DBC) at the 
output. An example is given in Fig. 4.6. 
 
0
M
1
M
2
M
0
C
1
C
2
C
 
 
Input M0
M1
M2
C0 
C1 
C2 Output
0 
0 
0 
0 
0 
1 
0 
3 
1 
1 
0 
0 
1 
1 
1 
7 
2 
0 
1 
0 
0 
0 
0 
0 
3 
1 
1 
0 
0 
1 
1 
6 
4 
0 
0 
1 
0 
1 
0 
2 
5 
1 
0 
1 
0 
0 
1 
4 
6 
0 
1 
1 
1 
0 
1 
5 
7 
1 
1 
1 
1 
0 
0 
1 
      b) 
 
Fig. 4.6 Example of  S box with n = 3: a) block scheme, b) truth table  

4.6   Modern Symmetric (Conventional) Cryptography 
139
 
The operation of such a box is as follows: 
• the plaintext is expressed in binary in n bits and is applied to the input of binary 
to decimal converter (BDC) 
• the P box, situated between the two converters, make a permutation of the 
n
2  
inputs, the total number of combinations being (
n
2 )! 
In our example (Fig. 4.6), n being 3, there are 
)
2
( 3 != 40320 possibilities.  
If n is great, for example 128, the cryptanalysis is, with conventional technology, 
impossible today. 
A typical block cipher is a product cipher, made of an alternation of P and S 
boxes, known as SPN (Substitution Permutation Network) is given in Fig. 4.7: 
1
S
2
S
3
S
4
S
1
S
2
S
3
S
4
S
1
P
2
P
3
P
1
M
0
M
15
M
1
C
15
C
0
C
2
K
1
K
 
Fig. 4.7 Example of a product cipher (alternation of P and S boxes)  
As illustrated in Fig. 4.7, P boxes are fixed (without key) and are used to im-
plement the diffusion by permutation. S boxes receive the permuted text (for each 
box 4 bits) and also a key of 2 bits long to command one of the four possible sub-
stitutions. Thus the key for all the four S boxes 
)
K
,
(K
2
1
is 8 bits long and the 
whole key of Fig. 4.7 is 
2
1
K
K
K
+
=
 of 16 bits long. 
Block ciphers, based on SPN principles, proposed by Shannon, are using prod-
ucts of type 
r
2
1
P...S
PS
S
, P being a permutation without key and 
r
1
S
,....,
S
a sim-
ple cryptographic transform based on a cryptographic key (
r
K ). Each iteration 
(round) is repeated r times using the same routine. The output of the round i is in-
put to the round (i+1). At decryption the process is the same as for encryption, but 
in reverse order. 
In what follows, DES (Data Encryption Standard) the most studied and used 
until his replacement in May 2002 will be presented. 
4.6.2.2   DES (Data Encryption Standard) 
A very good presentation of DES history is made in [64] and samples of it will be 
presented in what follows, because that history represents an important step in the 
open / non-military / civilian cryptography development. 

140
4   Cryptography Basics
 
At the beginnings of 70th the non-military cryptography almost did not exist, 
despite a very good market for cryptographic products (especially governments of 
different outside countries), impossible to be interconnected and certified by an 
independent organisation. 
Besides this necessity, the development of computers and data communications 
at large scale for civilians too, required urgently security in processing and trans-
mission. It is true, IBM the greatest producer in the domain, had a group of valu-
able cryptographers who created Lucifer, a straightward algorithm for IBM, but 
the necessity to have a standard was imperative. 
This is why NBS (National Bureau of Standards) of USA, today NIST (Na-
tional Institute of Standards and Technology) initiated a programme to develop a 
standard for cryptographic algorithms; the requirements were to be easy to analyse 
and certify and the equipments using it could interoperate. It launched the public 
call in 1973, but the submissions were very far to the requirements. 
The second call was done in 1974, when they had the promise that IBM will 
submit Lucifer (this one fulfilled the requirements). 
NBS asked NSA (National Security Agency) to evaluate the algorithm and to 
determine if it is suitable as federal standard. 
In March 1975, the Federal Register published the details of the algorithm and 
the IBM requirements for nonexclusivity and in August it requested comments on 
it from agencies and the general public. As a consequence, in 1976 were organised 
two workshops, one for specialists, to debate the mathematics and the possibility 
of a trap in it and  the second one to discuss the problem of increasing the length 
of the key from 56 (NSA reduced the original length from 128 bits to 56). 
This was the first and until now the last “democratic” choice of a security stan-
dard. The “voice of designers, evaluators, implementers, vendors, users and crit-
ics” [65] was heard. 
Despite the criticism, especially related to the key size (56 bits plus 8 parity 
check), in 26.11.1976, DES was adopted as a federal standard and authorised for 
unclassified government communications. In 1977 was given the official descrip-
tion of the standard. 
DES was thought by NSA for hardware implementation because of real time 
applications, but the published details were enough for software implementation 
(despite 1000 times slower [64]). 
DES was the first, and until now the last cryptographic algorithm publicly 
evaluated by NSA, being thus an important catalyser of  civilian cryptography, be-
ing available for study a secure algorithm, certified by NSA. 
The main characteristics of DES are: 
 
• block size: n=64 
• key size: 
8
56
64
n k
+
=
=
 
 
 
 

4.6   Modern Symmetric (Conventional) Cryptography 
141
 
• number of rounds: r=16 
• SPN type 
 
In what follows we will present DES encryption standard, more precisely  
one of its versions. For a better understanding of the ciphering scheme we will  
explain:  
 
• round keys generation 
• encryption routine  
• encryption/decryption function f. 
 
Round keys generation 
 
The round keys are generated starting from a 64 bits key, 8 being parity-check 
bits. In this way the 8th bit is the parity-check bit for the first 7 bits, the 16th bit is 
the parity-check bit for the bits 9 to 15 bit and s. o. until the 64th bit which is the 
parity-check bit for the bits 57 to 63. 
The key generation is presented in Fig.  4.8. 
Key generation is obtained as follows: 
 
• the key (64 bits from which 8 are parity bits) provided by the user, is delivered 
to the permutation box P1, which provides at the output 56 bits, and then this 
stream is split in two: C0, D0 of 28 bits each. 
• C0 structure is: 
 
57 
49 
41 
33 
25 
17 
09 
01 
58 
50 
42 
34 
26 
18 
10 
02 
59 
51 
43 
35 
27 
19 
11 
03 
60 
52 
44 
36 
 
(bit 57 from the initial matrix will be the first bit from the C0 matrix, and bit 58 
will be bit 9 from C0 etc.) 
• D0 structure is: 
 
63 
55 
47 
39 
31 
23 
15 
07 
62 
54 
46 
38 
30 
22 
14 
06 
61 
53 
45 
37 
29 
21 
13 
05 
28 
20 
12 
04 
 
(bit 63 from the initial matrix will be the first from D0, bit 7 will be bit 8 in D0 
etc.) 

142
4   Cryptography Basics
 
Key
(64 bits)
Permutation
P1 (56 bits)
C0
(28 bits)
D0
(28 bits)
LCS
LCS
C1
D1
LCS
LCS
Permutation
P2 
K1
48
C15
D15
K15
48
LCS
LCS
C16
D16
Permutation
P2 
K16
48
Permutation
P2 
 
Fig. 4.8 Generation of round keys in DES  
 
 
 
 

4.6   Modern Symmetric (Conventional) Cryptography 
143
 
• the elements of C1…C16 and D1…D16 are obtained shifting to the left (LCS- 
left cyclic shift) the previous  inputs as given bellow:  
 
Round Number of
 shifted bits
Round Number of 
shifted bits
01 
1 
09 
1 
02 
1 
10 
2 
03 
2 
11 
2 
04 
2 
12 
2 
05 
2 
13 
2 
06 
2 
14 
2 
07 
2 
15 
2 
08 
2 
16 
1 
• the 16 keys Ki , 
1...16
i =
 are obtained from matrices Ci and Di. The two matri-
ces Ci and Di form a new matrix (the first 28 bits come from Ci and the next 
from Di), which is introduced in a permutation box (P2) delivering at the output 
48 bits with the following structure: 
 
14 
17 
11 
24 
01 
05 
03 
28 
15 
06 
21 
10 
23 
19 
12 
04 
26 
08 
16 
07 
27 
20 
13 
02 
41 
52 
31 
37 
47 
55 
30 
40 
51 
45 
33 
48 
44 
49 
39 
56 
34 
53 
46 
42 
50 
36 
29 
32 
     (the 14th bit of the input is the first at the output). 
 
Encryption routine 
 
Encryption routine is given in Fig. 4.9 and it has the following steps: 
 
• the initial transposition matrix IP has the structure: 
58 
50 
42 
34 
26 
18 
10 
02 
60 
52 
44 
36 
28 
20 
12 04 
62 
54 
46 
38 
30 
22 
14 
06 
64 
56 
48 
40 
32 
24 
16 08 
57 
49 
41 
33 
25 
17 
09 
01 
59 
51 
43 
35 
27 
19 
11 03 
61 
53 
45 
37 
29 
21 
13 
05 
63 
55 
47 
39 
31 
23 
15 07 
• L0 contains the first 32 bits and R0 the next 32 
• XOR is a block that performs an exclusive OR between the bits of the two input 
blocks 
• DES function f is a two variable function. Its representation is given in  
figure 4.11output transposition matrix (IP-1) is the inverse of IP 
 
 

144
4   Cryptography Basics
 
 
Final transposition matrix IP-1 
Encrypted output 
block (64 bits)
DES f function 
XOR 
K3 
Input block in clear (64 
bits)
Initial transposition 
matrix IP
R0
L0 
 DES f function
XOR 
K1 
R1=L0 XOR f(R0,K1) 
L1=R0 
DES f function 
XOR 
K2 
R2=L1 XOR f(R1,K2) 
L2=R1 
R15=L14 XOR f(R14,K15) 
R16=L15 XOR f(R15,K16)
L15=R14 
L16=R14 
DES f function 
XOR 
K16 
 
Fig. 4.9 DES encryption routine  
 
 
 

4.6   Modern Symmetric (Conventional) Cryptography 
145
 
DES encryption/decryption function f 
 
The analysis of function f is done following Fig. 4.10:  
 
Fig. 4.10 DES encryption/decryption function f  
The processing steps in function f are:  
 
• inputs of function f are the (Ri, Ki+1) pairs (see fig.4.10) 
• the keys Ki  correspond to 48 bits blocks, whereas Ri are 32 bits blocks; in order 
to correlate the two blocks dimensions, an extension of Ri is performed to a 48 
bits block, according to the following extension matrix E: 
32 
01 
02 
03 
04 
05 
04 
05 
06 
07 
08 
09 
08 
09 
10 
11 
12 
13 
12 
13 
14 
15 
16 
17 
16 
17 
18 
19 
20 
21 
20 
21 
22 
23 
24 
25 
24 
25 
26 
27 
28 
29 
28 
29 
30 
31 
32 
01 
 
• the block corresponding to key Ki+1 and the one corresponding to extended Ri 
are the inputs into an XOR block. The new block is divided into 8 sub-blocks 
of 6 bits each, sub-blocks processed through Si blocks, 
8
...
1
=
i
. The substitu-
tion blocks Si are distinct and correspond to the following matrices: 
 
 

146
4   Cryptography Basics
 
S1: 
14 
04 
13 
01 
02 
15 
11 
08 
03 
10 
06 
12 
05 
09 
00 
07 
00 
15 
07 
04 
14 
02 
13 
01 
10 
06 
12 
11 
09 
05 
03 
08 
04 
01 
14 
08 
13 
06 
02 
11 
15 
12 
09 
07 
03 
10 
05 
00 
15 
12 
08 
02 
04 
09 
01 
07 
05 
11 
03 
14 
10 
00 
06 
13 
 
S2: 
15 
01 
08 
14 
06 
11 
03 
04 
09 
07 
02 
13 
12 
00 
05 
10 
03 
13 
04 
07 
15 
02 
08 
14 
12 
00 
01 
10 
06 
09 
11 
05 
00 
14 
07 
11 
10 
04 
13 
01 
05 
08 
12 
06 
09 
03 
02 
15 
13 
08 
10 
01 
03 
15 
04 
02 
11 
06 
07 
12 
00 
05 
14 
09 
 
S3: 
10 
00 
09 
14 
06 
03 
15 
05 
01 
13 
12 
07 
11 
04 
02 
08 
13 
07 
00 
09 
03 
04 
06 
10 
02 
08 
05 
14 
12 
11 
15 
01 
13 
06 
04 
09 
08 
15 
03 
00 
11 
01 
02 
12 
05 
10 
14 
07 
01 
10 
13 
00 
06 
09 
08 
07 
04 
15 
14 
03 
11 
05 
02 
12 
 
S4: 
07 
13 
14 
03 
00 
06 
09 
10 
01 
02 
08 
05 
11 
12 
04 
15 
13 
08 
11 
05 
06 
15 
00 
03 
04 
07 
02 
12 
01 
10 
14 
09 
10 
06 
09 
00 
12 
11 
07 
13 
15 
01 
03 
14 
05 
02 
08 
04 
03 
15 
00 
06 
10 
01 
13 
08 
09 
04 
05 
11 
12 
07 
02 
14 
 
S5: 
02 
12 
04 
01 
07 
10 
11 
06 
08 
05 
03 
15 
13 
00 
14 
09 
14 
11 
02 
12 
04 
07 
13 
01 
05 
00 
15 
10 
03 
09 
08 
06 
04 
02 
01 
11 
10 
13 
07 
08 
15 
09 
12 
05 
06 
03 
00 
14 
11 
08 
12 
07 
01 
14 
02 
13 
06 
15 
00 
09 
10 
04 
05 
03 
 
S6: 
12 
01 
10 
15 
09 
02 
06 
08 
00 
13 
03 
04 
14 
07 
05 11 
10 
15 
04 
02 
07 
12 
09 
05 
06 
01 
13 
14 
00 
11 
03 08 
09 
14 
15 
05 
02 
08 
12 
03 
07 
00 
04 
10 
01 
13 
11 06 
04 
03 
02 
12 
09 
05 
15 
10 
11 
14 
01 
04 
06 
00 
08 13 
 
S7: 
04 
11 
02 
14 
15 
00 
08 
13 
03 
12 
09 
07 
05 
10 
06 
01 
13 
00 
11 
07 
04 
09 
01 
10 
14 
03 
05 
12 
02 
15 
08 
06 
01 
04 
11 
13 
12 
03 
07 
14 
10 
15 
06 
08 
00 
05 
09 
02 
06 
11 
13 
08 
01 
04 
10 
07 
09 
05 
00 
15 
14 
02 
03 
12 
 

4.6   Modern Symmetric (Conventional) Cryptography 
147
 
S8: 
13 
02 
08 
04 
06 
15 
11 
01 
10 
09 
03 
14 
05 
00 
12 
07 
01 
15 
13 
08 
10 
03 
07 
04 
12 
05 
06 
11 
00 
14 
09 
02 
07 
11 
04 
01 
09 
12 
14 
02 
00 
06 
10 
13 
15 
03 
05 
08 
02 
01 
14 
07 
04 
10 
08 
13 
15 
12 
09 
00 
03 
05 
06 
11 
• the algorithm corresponding to a block S is the following: the first and last bit 
from the 6 bits input block defines the row in the matrix and the others 4 the 
column. The element selected this way is a number between 0 and 15. The out-
put of block S provides the binary code of that number (4 bits). 
• the output transposition matrix P is: 
16 
07 
20 
21 
29 
12 
28 
17 
01 
05 
23 
26 
05 
18 
31 
10 
02 
08 
24 
14 
32 
27 
03 
09 
19 
13 
30 
06 
22 
11 
04 
25 
 
DES decrypting routine 
 
Concerning decryption, the same algorithm is used, the only difference being that 
the 16 keys are used in reverse order than for encryption. 
4.6.2.3   Some Other Block Ciphers 
• AES (Advanced Encryption Standard) was selected in 26 May 2002 to replace 
the old standard DES. 
 
It was the winner of NIST call for a new standard, being selected from 15 can-
didates. 
The algorithm, designed by the Belgian cryptographers Joan Daemen and  
Vincent Rijmen was presented in 1998 as Rijndael cipher, from the names of its 
authors. 
The main characteristics of the cipher are: 
 
– 
block size: n=128 (2x64) 
– 
key size: 
128
n k =
, or 192, or 256 (at choice) 
– 
number of rounds: r=10, or 12, or 14 (at choice) 
– 
SPN structure:-S boxes are based on multiplicative inverse over GF (
8
2 ) 
(see finite fields in Annex A) with good non-linearity properties; P boxes 
are based on permutations of lines and columns, treated as polynomial in 
GF (
8
2 ). 
 
 

148
4   Cryptography Basics
 
The USA government certified in June 2003 that AES can be used for classified 
information of level SECRET (with key of size 128) and TOP SECRET (with 
keys of size 192 and 256). 
Soon after becoming the official standard for symmetric cryptography, were re-
ported attacks on AES: at round 7 for key size 128, at round 8 for key size 192 and 
round 9 for key size 256 (it looks that strong debates as those made for DES, are 
important despite our rushing times). 
 
• IDEA (International Data Encryption Algorithm), invented in 1990 by X. Lai 
and J. Massay in Switzerland, is a very secure, flexible and quick cipher [64]; it 
was one of the most promising candidates for the new standard. Its main fea-
tures are: 
 
– 
block size: n=64 (as for DES) 
– 
key size: 
128
n k =
 
– 
number of rounds: r=8( less than DES which has 16, meaning that is much 
faster) 
– 
flexible and cheap software implementation  and very fast operation time 
in hardware implementation using FPGA facilities 
– 
the principle is based on modulo
16
2
 summation and modulo (
16
2
+1) 
multiplication, operations which are not distributive and associative, mak-
ing thus difficult the cryptanalysis. 
Despite IDEA lost the battle for the new standard, it remains a very secure, fast, 
flexible and cheap cipher, being widely used in PGP (Pretty Good Privacy) 
[64]. 
• Multiple DES [64], [68] 
The time when DES become wick because of his small size key (56 bits), alter-
natives were investigated, in order to preserve the existing equipment, but to en-
hance the security. A solution was to use multiple encryption with DES and multi-
ple keys. The most common are: Double- DES with two keys, using an equivalent 
key size of 56x2=112 bits and Triple- DES (T- DES) with two or three keys and 
an equivalent key size of 56x3=168 bits. 
 
Applications of T- DES with two keys: PEM (Privacy Enhanced Mail), ANSI 
(American National Standard Institute) and ISO 8732 for key generation. 
Applications of T- DES with three keys are in PGP and S/MIME (Secure/ Mul-
tiple purpose Internet Mail Extension). 
 
DES-X [64] is a DES improvement proposed by Ron Rivest from RSA Data 
Security; it uses the whitening technique, a cheep way to improve the security hid-
ing the input and the output of a cryptosystem (Fig. 4.11). 

4.6   Modern Symmetric (Conventional) Cryptography 
149
 
 
Fig. 4.11 Illustration of whitening technique: a- encryption, b- decryption  
Remark 
In practice: 
K
K
K
2
1
=
=
. 
 
• CAST- 128 or 256 is a symmetric block cipher used in Canada and invented in 
1997 by Carlisle Adams and Stafford Tavares. It was between the 15 candi-
dates for the selection of the new standard. The main features are: 
 
– 
block size: n=64 bits 
– 
key size is variable, being a multiple of 8 : 40 ÷ 256 bits 
– 
number of rounds: r=16 
• SAFER [64] (Secure and Fast Encryption Routine) invented by J. Massay, was 
a candidate too for AES. Its main characteristics are: 
 
– 
block size: n=128 bits 
– 
key size is variable: 128, 192 or 256 bits 
– 
is based on modular arithmetic. 
• FEAL [64] is the equivalent DES of Japan, having almost the same characteris-
tics as size: 
 
– 
block size: n=64 bits 
– 
key size: 
64
n k =
bits 
but operating in modular arithmetic: mod (
8
2 ). 
 
• GOST (Gosudarstvenîi Standard) is the governmental standard of Russia. It was 
adopted in 1989 as an equivalent for DES. Its characteristics are: 
 
– 
block size: n=64 bits (as DES) 
– 
key size: 
256
n k =
 (much greater than DES- 56, and obviously longer 
life) 

150
4   Cryptography Basics
 
– 
number of rounds: r=32 (greater than DES- 16) 
– 
the S and P boxes are secret, despite Kerckhoffs law. 
4.6.2.4   Block Cipher Operation Modes 
An operation mode of a block cipher is defined by the used algorithm (cipher), the 
feedback and some elementary transformations. The most used algorithm is DES, 
but the modes act the same for any block cipher. 
Basically there are four operation modes. 
 
1. Electronic Codebook (ECB) 
 
Each plaintext block 
i
M  is encoded independently using the same key K and 
giving the ciphertext 
i
C  (Fig. 4.12). Because the transformation 
i
i
C
M ↔
is bi-
univoc it is theoretically possible to create a code book of plaintexts 
i
M  and the 
correspondente ciphertexts 
i
C . If the size of the block is n, there will be 
n
2  dis-
tinct
i
M ; if n is great, this code book will be much too large to be stored and fur-
ther, for each key a distinct code book is obtained. 
 
Fig. 4.12 ECB mode: a) encryption; b) decryption  
 
 

4.6   Modern Symmetric (Conventional) Cryptography 
151
 
Advantages 
• the encryption of blocks being independent, is suitable for randomly accessed 
encrypted files (as an encrypted database); any record could be added, deleted, 
encrypted or decrypted independently [64]. 
• the process suit to parallelization, using multiple processors for encryption and 
decryption of distinct blocks. 
• each block is independently affected by errors 
 
Drawbacks 
• if the block 
i
M  is repeated, the ciphertext 
i
C  is repeating too, giving thus in-
formation to the cryptanalyst. Ways to avoid this is to have distinct block 
i
M  
(to avoid stereotyped beginnings and endings). 
• padding occurs when the block is smaller than n. 
• the most serious problem in ECB is when an adversary could modify the  
encrypted files (block reply [64]); to avoid this situation, block chaining is  
required. 
 
2. Cipher Block Chaining (CBC)(Fig. 4.13) 
 
Is used precisely for long plaintext (>n), where there are blocks  
i
M  which are 
repeated. In order to obtain distinct 
i
C  for identical
i
M , a feedback is introduced, 
such that the input of the encryption block will be the current plaintext 
i
M  added 
modulo two with the ciphertext of the previous encryption block (Fig. 4.13) 
1
i
i
1
i
i
C
C
C
M
−
−
≠
=
⊕
 if 
1
i
i
M
M
−
=
. 
 
Advantages 
• CBC mode, in order to have distinct ciphertexts 
i
C  for identical 
i
M , starting 
from the first block, is using in the first block an initialization vector (IV), 
which plays also an authentication role, but it can be transmitted in clear, only 
the key k requiring secure channel in transmission. 
 
Drawbacks 
• error propagation: errors in the ciphertext propagate, affecting more blocks in 
the recovered plaintext (error extension). 
• security problems, such birthday attack is discussed in [64]. 
• padding remains a problem as in ECB. 

152
4   Cryptography Basics
 
 
Fig. 4.13 CBC mode : a) encryption, b) decryption  
3. Cipher Feedback( CFB) 
 
CFB is used to transform a block cipher into a stream cipher. The encryption is 
done on j bits at once (j=1, 8, 32), thus the size of 
i
M  being also j. It requires in 
the first block an initialization vector (IV), as in CBC (Fig. 4.14). Decryption fol-
lows the same steps, but in reverse order. 
 
 
Fig. 4.14 CFB mode: encryption 

4.6   Modern Symmetric (Conventional) Cryptography 
153
 
As stated in previous chain modes, the existence of an identification vector 
(IV), in the first block, is used also for authentication. 
 
4. Output Feedback (OFB) 
 
OFB is identical with CFB as structure (Fig. 4.15), the difference being that for 
chaining are used the j bits and not 
i
C  as in CFB, which ensure that errors in 
transmission do not propagate. Decryption follows the same steps, but in reverse 
order. 
 
 
Fig. 4.15 OFB mode : encryption  
4.6.3   Stream Ciphers 
4.6.3.1   General Features 
Stream ciphers are modern versions of Vigénère cipher, using as basic transforma-
tion substitutions, to make confusion. 
 
• They process the information continuously bit by bit or character by character. 
• Are fast in hardware implementation (using linear feedback shift registers – 
LFSR, see chapter 5). 
• Are used in noisy applications, based on their advantage of do not propagate 
the errors, such radio channels. 
• They can simulate OTP principles, meaning the generation of long pseudo- 
random keys in two ways: 
 
1. Linear congruential generators 
 
A linear congruential generator is a generator of a pseudo- random (noise)  
sequence. 
b)modm
(aX
X
1-
n
n
+
=
                                      (4.15) 

154
4   Cryptography Basics
 
where 
n
X  is the n-th term of the sequence, 
1
n
X − the (n-1)-th term, and 
 
a- multiplier 
b- increment 
m- modulus 
 
The seed (key) is the value
0
X . 
The sequence generated by relation (4.15) is a sequence with a period no 
greater than m, which is the maximal period and is obtained if b is relatively prime 
to m. Details concerning such generators are given in [64]. 
 
2. LFSR as pseudo- random (noise) generators 
 
A pseudo-random sequence can be generated using a linear feedback shift reg-
ister (LFSR) having the connections in correspondence to the coefficients of 
primitive polynomials g(x), such that generated sequence length is maximum (see 
also 5.8.5). 
 
Fig. 4.16 Block scheme of a pseudo-noise generator using LFSR 
The sequence at the output, k, of the register will be: 
1
2
n
],
...a
a
[a
k
m
1
n
1
0
−
=
=
−
                                     (4.25) 
Each symbol ai is given by: 
0
i
i
S
σT
a =
                                                  (4.26) 
where 
• σ is the output selection matrix of the pseudo-random sequence (the output can 
be taken from any of the m LFSR cells); in Fig. 4.16 the expression of σ is: 
LSB
[10...0]
σ
↑
=
                                                 (4.27) 
• T is the register characteristic matrix  
• 
0
1
1
m
1
m
m
m
g
x
g
...
x
g
x
g
g(x)
+
+
+
+
=
−
−
is LFSR generator polynomial 
• So is initial state of LFSR; obviously, it cannot be zero 

4.6   Modern Symmetric (Conventional) Cryptography 
155
 
Changing the coefficients of the generator polynomial and the initial state of 
the shift register, we can modify the generated sequence and subsequently the 
keys k. 
At the receiver, an identical generator and synchronised with the one used at 
emission, generates the same pseudo-random sequence k, which added modulo 2 
with the received sequence gives the message in clear. 
 
Example 4.8 
Let us consider a pseudo-random sequence generator with the generator polyno-
mial:
1
x
x
g(x)
2
3
+
+
=
. The block scheme of this generator is presented in  
Fig. 4.17. 
 
Fig. 4.17 Pseudo-noise generator with LFSR and g(x)=x3+ x2 +1 
Considering the shift register initial state as 101, the pseudo-random sequence 
can be obtained by register time evolution: 
 
Clock tn 
SR tn+1 
Pseudo-noise se-
quence (PNS) tn 
 
C2 
C1 
C0 
PNS2 PNS1 PNS0
1 
1 
0 
1 
1 
0 
1 
2 
0 
1 
0 
0 
1 
0 
3 
0 
0 
1 
0 
0 
1 
4 
1 
0 
0 
1 
0 
0 
5 
1 
1 
0 
1 
1 
0 
6 
1 
1 
1 
1 
1 
1 
7 
0 
1 
1 
0 
1 
1 
 
Remarks 
• at the 8th clock, the register will contain the initial sequence 101, so the process 
will be repeated 
• the pseudo-noise sequence is 7 bits long: n=2m-1=7 

156
4   Cryptography Basics
 
Longer keys can be obtained easily multiplexing two or more LFSR of small 
lengths (Fig. 4.18). 
 
Fig. 4.18 Pseudo noise sequences generator implemented with two LFSRs  
According to (4.26), any element ai of the pseudo noise sequence k, will be: 
ai=σ1T1
i S01+σ2T2
i S02                                             (4.28) 
If n is the length of the k-th generated sequence (the period), then:  
02
i
2
2
01
i
1
1
02
n
i
2
2
01
n
i
1
1
S
T
σ
S
T
σ
S
T
σ
S
T
σ
+
=
+
+
+
                      (4.29) 
2
n
2
1
n
1
I
T
,
I
T
2
1
=
=
                                         (4.30) 
1
2
n
1,
2
n
2
1
m
2
m
1
−
=
−
=
                                     (4.31) 
In order to fulfil relation (4.28) we need to have: 
2
n
2
1
n
1
I
T
and
I
T
=
=
                                      (4.32) 
The smallest n that satisfies this condition is the least common multiple of 
numbers n1 and n2: 
n=l.c.m.{n1; n2}                                              (4.33)  
If n1 and n2 are prime numbers:  
2
1 n
n
n
⋅
=
                                              (4.33.a) 
The difficulty of the cryptanalysis is increased if a certain non-linearity is in-
troduced (Fig. 4.19). 
 
 
 

4.6   Modern Symmetric (Conventional) Cryptography 
157
 
 
Fig. 4.19 Nonlinear multiplexed system for generating pseudo-noise sequences  
Example 4.9 
 
For the particular case when p1 = 3 and p2 = 8 the output k of the multiplexing cir-
cuit is expressed, underlining the non-linearity introduced by multiplexing. 
 
3
x
2
x
1
x
Y 
0 
0 
0 
1
y
0 
0 
1 
2
y
0 
1 
0 
3
y
0 
1 
1 
4
y
1 
0 
0 
5
y
1 
0 
1 
6
y
1 
1 
0 
7
y
1 
1 
1 
8
y
 
8
3
2
1
7
3
2
1
6
3
2
1
5
3
2
1
4
3
2
1
3
3
2
1
2
3
2
1
1
3
2
1
y
x
x
x
y
x
x
x
y
x
x
x
y
x
x
x
y
x
x
x
y
x
x
x
y
x
x
x
y
x
x
x
k
+
+
+
+
+
+
+
+
=
 
4.6.3.2   Some Stream Ciphers 
A wider presentation of stream ciphers is given in [64], [68]. Some of them will be 
mentioned in what follows: 
 
A5 is used in GSM (Group Special Mobile) to encrypt the link between base 
station and subscribers. It was designed by French cryptographers in 1980 and 
contains three LFSRs with
19
m1 =
, 
22
m2 =
, 
23
m3 =
. In this way, using the 
principle described above, the length of the key, obtained by multiplexing (rela-
tion (4.33.a)) is: 
3
2
1
n
n
n
n
⋅
⋅
=
, 
where 
1
2
n
19
1
−
=
, 
1
2
n
22
2
−
=
, 
1
2
n
23
3
−
=
. 

158
4   Cryptography Basics
 
The security is medium, but it is quick, error resistant and chip. 
 
RC-4 was invented by Ron Rivest in 1987. It is a block cipher operating in 
OFB mode. In 1994 its source code was put on the internet, and for this reason it 
is extremely studied and used. 
 
SEAL is a stream cipher created at IBM, very fast in software implementation. 
 
Hughes XPD/ KPD was created in 1986 to protect portable devices (XPD) and 
is used by Hughes Aircraft Corporation for kinetic protection device (KPD). It is 
basically composed by a single LFSR of 61 bits long (m=61); at this degree, the 
number of primitive polynomials (see Appendix A) is great:
10
2
, meaning that the 
key is composed of two subkeys the choice of the characteristic polynomial ( one 
from 
10
2
 possible) and the initial state 
0
S  of the LFSR (there are (
1
261 −)). 
 
NANOTWO 
was used by the South African police for fax transmission pro-
tection. The principle is similar to the one used in Hughes XPD/ KPD: a unique 
LFSR 127 bits long, meaning that 
0
S  and g(x) are selected from greater sets. 
4.6.4   Authentication with Symmetric Cryptography 
In networks with several users and several data bases, a user must be convinced 
that he entered into the desired data-base, and the computer (database) has to be 
sure that that person is authorised to have access to these databases. In this case, 
between user A and computer C an authentication protocol is established: 
 
• the user gives his clear identity A and randomly composes a short data se-
quence X which is ciphered with his own key (kA):  
 (X⊕kA)                                                 (4.34) 
• the computer knows the key corresponding to A and deciphers the sequence X 
with the same key (kA):  
 (X⊕ kA) ⊕ kA =X                                         (4.35) 
• to this deciphered sequence, the computer adds its own random sequence Y and 
ciphers both sequences with key kA, transmitting the sequences to A:  
 (XY) ⊕kA                                             (4.36) 
• when deciphering the message, A compares the received sequence X to the trans-
mitted sequence X and makes sure of computer identity data (authentication):  
 [(XY) ⊕kA] ⊕kA=XY                                     (4.37) 

4.7   Public Key Cryptography 
159
 
• in the next messages transmitted by A, it adds the deciphered sequence Y, ci-
phers everything with its own key and transmits it to the computer:  
 (MXY) ⊕ kA                                               (4.38) 
• the computer compares the deciphered sequence Y with the sequence sent  
for the first time and makes sure that the correspondent is the right one (entity 
authentication):   
 (MXY) ⊕kA] ⊕kA=MXY                                      (4.39) 
4.7   Public Key Cryptography 
4.7.1   Principle of the Public Key Cryptography 
The year 1976 is considered the starting point of modern cryptography. In  
November 1976, W. Diffie and M. Hellman published in IEEE Transaction on  
Information Theory the paper: “New Direction in Cryptography” in which they  
introduced the PKC (Public Key Cryptography) concept. 
The principle of this concept is shown in fig. 4.20: 
 
Fig. 4.20 Block scheme of a public key system  
Each user has for encryption a public transform
1
k
E
, that can be memorized in 
a public folder and a transform for secret decryption
2
k
D
. 
The requirements of public key cryptography are: 
 
• for each pair of keys (k1,k2), the decryption function with key k2: 
2
k
D
 is the 
reverse of the encryption function with key k1 : 
1
k
E
  
• for any pair (k1,k2) and any M, the computation algorithms for 
1
k
E
and  
2
k
D
are easy and fast 
• for any pair (k1,k2), the computation algorithm for 
2
k
D
 cannot be obtained in 
reasonable  time starting from 
1
k
E
; decryption 
2
k
D
 (which is secret) is de-
rived from 
1
k
E
 through a transformation that cannot be easily reversed (one 
way function) 
• any pair (k1,k2) must be easily obtained starting from a unique and secret key 

160
4   Cryptography Basics
 
In public key systems, confidentiality and authentication are done by distinct 
transform. 
 
Fig. 4.21 Confidentiality in public key cryptosystems  
Let us assume that user A wants to transmit a message M to another user B.  
In this case, A, knowing the public transformof B (EB), transmits the following  
ciphertext:  
(M)
E
C
B
=
                                                (4.40) 
ensuring confidentiality (Fig 4.21) 
When receiving the message, B decrypts the ciphertext C using the secret trans-
form DB:  
M
(M))
(E
D
(C)
D
B
B
B
=
=
                                   (4.41) 
Such a system does not allow any authentication because any user has access to 
the public transform of B (EB) and can transmit a fake messages (M’): 
)
M
(
E
C
B
′
=
′
                                              (4.42) 
For authentication (Fig.4.22) we apply to M the secret transform DA of A. A 
will transmitte to B:  
(M)
D
C
A
=
                                              (4.43) 
When receives the encrypted message, B will apply the public transform EA 
corresponding to A: 
 
Fig. 4.22 Authentication in public key cryptosystems  
Authentication is done, because fake messages cannot be transmitted to B: 
C’=DA(M’), only A knows DA (the secret key). In this case, however, the confi-
dentiality is not ensured, due to the fact that M can be obtained by anyone just ap-
plying EA to C, EA being public. 
In order to simultaneously ensure confidentiality and authentication (Fig 4.23), 
the space M must be equivalent to the space C, so that any pair (EA,DA), (EB,DB) 
can operate not only on clear text but also on ciphered one. It is necessary for 
(EA,DA) and (EB,DB) to be mutually inverse: 

4.7   Public Key Cryptography 
161
 
M
(M))
(E
D
(M))
(D
E
A
A
A
A
=
=
                               (4.44) 
M
(M))
(E
D
(M))
(D
E
B
B
B
B
=
=
                               (4.45) 
 
Fig. 4.23 Confidentiality and authentication in public key cryptosystems  
User A will sign using his private key DA the message M and than will encrypt 
it using the public key of B:  
(M))
(D
E
C
A
B
=
                                          (4.46) 
B will obtain M applying to the received cipher text its own private key DB and 
than the public key of A (EA) to check his signature:  
M
(M))
(D
E
(M)))))
(D
((E
(D
E
(C))
(D
E
A
A
A
B
B
A
B
A
=
=
=
           (4.47) 
 
Digital signature in public key cryptosystems 
 
Let us consider that B is receiving a signed message from A. The signature of A 
needs to have the following proprieties: 
 
• B is able to validate A signature 
• it must be impossible for enyone, including B, to fake A signature 
• when A does not recognize the message M signature, it has to be a “judge” 
(Trusted Person - TP) to solve the argue between A and B 
The implementation of digital signature is very simple in public key systems. In 
this case DA can serve as a digital signature for A. The receiver B of the message 
M signed by A knows that transmitter and data are authentic. Because EA trans-
form is public, the receiver B can validate the signature. 
The protocol of digital signature can be developed as follows: 
 
• A signes M:   
S = DA(M)                                               (4.48) 
• A sends to B the cryptogram:   
C = EB(S)                                               (4.49) 
• B validates A signature, verifying if  
EA(S) = M                                             (4.50) 

162
4   Cryptography Basics
 
S
(S))
(E
D
(C))
(D
B
B
B
=
=
                                  (4.51) 
M
(M))
(D
E
(S)
E
A
A
A
=
=
                                 (4.52) 
• a “judge”(TP) solves the dispute that may occur between A and B checking if 
EA(S) belongs to M, in the same manner as B does. 
4.7.2   Public Keys Ciphers 
Rivest – Shamir - Adleman (RSA) cipher 
 
This algorithm was published in 1978. The security of the cipher is based on the 
fact that the factorisation of the product of two high prime numbers is, at least for 
now, unsolvable. Related to this aspect, Fermat and Legendre developed same fac-
torisation algorithms. The most efficient ones, used even now, are the ones devel-
oped by Legendre[96]. 
Encryption method implies exponential computation in a Galois Field (GF(n)).  
Ciphertext can be obtained from the cleartext through a block transform (encod-
ing). Be M such a cleartext block, having the property M∈(0,n-1). The encrypted 
block C corresponding to the cleartext block can be obtained computing the expo-
nential: 
n)
(mod
M
C
E
=
, E and n representing the public key. Decryption is done 
as: 
n)
(mod
C
M
D
=
, D being the secret decryption key (private key). 
The two keys, E and D, must satisfy the following relations: 
n)
(mod
M
n)
(mod
C
M
ED
D
=
=
                                 (4.53) 
This algorithm is based on Euler-Fermat Theorem: if n is prime number and a a 
positive integer, non divisible, with n, then 
n)
1(mod
a
1
n
=
−
 for any a∈[1,n). 
If we chose n a prime number, for any block 
1)
n
(0,
M
−
∈
we have:   
1
n)
 
(mod
M (n)
=
ϕ
                                           (4.54) 
where  
ϕ (n) = n-1                                                (4.55)  
is Euler totient. 
If E and D satisfy the relation: 
1
(n))
ED(mod
=
ϕ
                                          (4.56) 
then we may write: 
1
(n)
...
(n)
(n)
1
(n)
k
ED
+
+
+
+
=
+
=
ϕ
ϕ
ϕ
ϕ
                        (4.57) 
n)
...M(mod
M
M
M
M
(n)
(n)
1
(n)
...
(n)
(n)
ED
ϕ
ϕ
ϕ
ϕ
ϕ
⋅
=
=
+
+
+
+
            (4.58) 
 

4.7   Public Key Cryptography 
163
 
It follows: 
n)
M(mod
M
1
(n))
ED(mod
ED =
⇒
=
ϕ
                            (4.59) 
In this way we made a reversible transform based on exponential finite fields. 
We still have to solve the problem concerning the security of the decryption key. 
This key D must be almost impossible to be determined from the encryption key, 
but in the given case it is easy to determine this key using E and n and knowing 
that 
1
n
(n)
and
1
(n))
ED(mod
−
=
=
ϕ
ϕ
. 
The security is based on high number factorization. Starting from this idea the 
number n can be obtained from the product of two high prime numbers p and q: 
q
p
n
⋅
=
, such that Euler totient, in this case: 
1)
1)(q
(p
(n)
−
−
=
ϕ
,                                           (4.60) 
becomes harder to be found using only n. 
Using this method we can obtain a secure public key system. Such a system 
which ensures confidentiality has as elements the following pairs: 
(E,n) the public key 
(D,n) the private key 
A cryptanalyst who knows the pair (E, n) must determine D taking into account 
that
1
(n))
ED(mod
=
ϕ
. For this purpose he has to determine 
(n)
ϕ
 from 
1)
1)(q
(p
(n)
−
−
=
ϕ
, i.e. p and q; this problem reduces to a high number factori-
zation and it is very difficult to be solved.  
 
Example 4.10 
 
We choose two prime numbers: p = 47 and q = 97. 
n = p·q = 3713 
Choosing D = 97, E will be 37, to satisfy: 
1]/D
1)
1)(q
[(p
E
1
1))
1)(q
(p
D(mod
+
−
−
=
=
−
−
 
In order to encode the message “A SOSIT TIMPUL”, first we must transform 
the letters of the alphabet in numbers. For example A = 00, B = 01, …. 
The message becomes: 
001814180819190802152011 
In what follows we encode each 4 numbers smaller than n: 
0943
mod(3713)
1418
mod(n)
1418
3019
mod(3713)
0018
mod(n)
0018
37
E
37
E
=
=
=
=
 
 

164
4   Cryptography Basics
 
The encrypted message becomes:  
309109433366254501072965 
At the decryption we compute: 
0018
mod(3713)
3091
mod(n)
3091
97
D
=
=
 
thus obtaining the original message. 
Once the method is understood, it may easily hint its broad field of applications 
in authentication and digital signatures. 
However a problem that still may appear in developing such an algorithm is 
computing systems values: the number n and the two keys E and D; the computa-
tion is done at tens digits level in order to assume a high level of security. Until 
now lengths of (512 – 1024) bits are considered enough for current applications. 
The encryption system with public keys RSA is the most important among  
public key algorithms, offering a high level of security and being a standard in 
digital signatures field. RSA is known as the safest PKC algorithm of encryption 
and authentication commercially available, impossible to be broken even by gov-
ernmental agencies. The method has a great advantage because, compared to some 
other encryption methods, there are no traps for system breaking. The algorithm is 
used for confidentiality and data authentication, passwords, being used by a lot  
of companies as: DEC, Lotus, Novell, Motorola, and also a series of important  
institutions (the USA Defence Department, Boeing, the bank network SWIFT - 
Society for Worldwide Interbank Financial Telecommunication, the Belgium 
Government, etc). 
4.8   Digital Watermarking 
4.8.1   Introduction 
In the last 20 years we were witnesses of an outbreak of the digital multimedia 
technologies. The digital audio/ video information has several advantages over its 
analogical counterpart: 
 
• superior quality in transmission, processing and storage 
• simpler editing facilities, the desired fragments of the initial data  can be lo-
cated  with precision and modified  
• simpler lossless copying : the copy of a digital document is identical with the 
original.    
 
For producers and distributors of multimedia products, several of the above  
mentioned advantages are handicaps, leading to important financial losses. Unau-
thorized copying of CDs, DVDs is currently a major problem. Also the information  
 

4.8   Digital Watermarking 
165
 
contained in WebPages, books, and the broadcasted information, are frequently  
copied and used without any permission from the “editor”.   
The copyright in this domain is a problem of maximum urgency. Several at-
tempts in this sense exist, but we cannot speak of a generalized corresponding leg-
islation. In 28 Oct. 1998, the president of the United States signed an act [79] 
(Digital Millennium Copyright Act) that contains recommendations to be followed 
in order to protect the intellectual property and also the customers rights.  At its 
turn, the European Community is preparing several protection measures for digital 
multimedia products such as CDs and DVDs. 
The most important technologies used in copyright protection for authors or 
distributors are: encryption and watermarking.  
Encryption is used for protecting data in transmission and storage. Once the in-
formation was decrypted,  it is no longer protected  and can be copied without any 
restriction.  
Watermarking is an operation, which consists in embedding an imperceptible 
signal called watermark (WM) into the host information. The host information can 
be text, audio signal, still image or video. 
The name watermark comes form the words “water” and “mark” and designates 
a transparent, invisible mark like the water transparency. 
In general, the watermark contains information about the origin and destination 
of the host information. Event though it is not directly used in intellectual property 
protection, it helps identifying the host and the receiver, being useful in disputes 
over authors / distributors rights. 
From a theoretical point of view the watermark has to permanently protect the 
information, so it has to be robust, in such a way that any unauthorized removal 
will automatically lead to quality degradation. The watermark resembles to a sig-
nature, at the beginning it was called signature, but in order to eliminate the con-
fusions with the digital signatures from cryptography the original name was-
dropped . Taking into account the fact that it has to be transparent, imperceptible 
for hearing or seeing, the resemblance with the “invisible tattoo”, made by A. 
Tewfik [76], is suggestive.  
In order to insure copyright protection, the watermarking technologies need two 
operations (Fig. 4.24):  
 
• watermark insertion in host data, before transmission or storage; 
• watermark extraction from the received data and comparison between the 
original watermark and the extracted one, in case of dispute. 
• Watermarking is used especially for  information protection such as: 
•  Copyright protection. The owner inserts  a watermark containing information 
related to its intellectual rights. The watermarks resembles to ISBN (Interna-
tional Standard Book Numbering) - 10 characters or ISRC (International Stan-
dard Recording Code) - 12 alphanumerical characters.  
 

166
4   Cryptography Basics
 
watermark insertion
host data
watermarked data
watermark
watermark 
extractor
comparison
watermark received
detected 
watermark
original
watermark
N
Y
a)
b)
 
Fig. 4.24 Watermark principle block scheme: a) insertion (embedding), b) retrieval / detection 
The information inserted could be related to license rights, distribution agree-
ments, etc., in these cases watermark length being usually 60 ÷ 70 bits. 
 
• Copy protection; in this case the watermark is a single bit that allows (forbids) 
copying; this bit is computed in watermark detectors in storage devices and ac-
cordingly, information copying will be allowed (forbidden) [47]. 
• Fingerprinting, used for unauthorized copy detection. The data owner inserts 
information related to the customers that bought the license in the watermark. 
This information is like a serial number. When illegal copies are found, the 
source can be easily identified using the information embedded into the water-
mark.  
• Broadcast monitoring: using watermarking on commercials, a monitoring sys-
tem for commercials broadcasting according to the license agreements can be 
implemented. 
•  Data authentication: when the watermark is used for identification, it is called 
fragile watermark and it shows if the data have been altered, together with the 
place where the modification was done [87].  
 
Beside these protection goals, watermarking can be used also for: 
 
• Characteristic enrichment for the host signal, e.g. several language subtitling; 
there are several services that use this property. 
• Medicine applications: using watermarking techniques, patient data are inserted 
into the medical images. 
• Secret message transmission: there are countries where cryptographically ser-
vices are restricted; it follows that secret (private) messages can be inserted 
through watermarking.  

4.8   Digital Watermarking 
167
 
4.8.2   Short History 
Nowadays digital watermarking is a modern version of steganography (form the 
Greek words “stegano” which means covered and “graphos” meaning writing) - 
signifying covered writing. 
Steganography is a technique used for secret message hiding into other mes-
sages in such a way that the existence of the secret messages is hidden. The sender 
writes a secret message and hides it into an inoffensive one.  
Among the techniques used during the history of steganography we remind: 
 
• use  of invisible inks  
• thin holes for some characters, fine modifications of the space between words 
• the use of semagrams (from the Greek words  “sema” meaning sign and 
“gramma” meaning writing, drawing). 
 
These techniques were recently resumed and put into digital context for text 
watermarking [20]. 
Audio watermarking (audiosteganography) and still / dynamic image water-
marking (videosteganography) are using the same ideas as steganography. 
As an example for audiosteganography, we can remind Bach. He used invisible 
watermarks for copyright protection, writing  his name in several  works using in-
visible watermarking; for example he counted the  number of appearances of a 
musical note ( one appearance for A, two for B , three for C and eight for H). 
As for steganography, for graphical images for instance, using the least signifi-
cant bit, several secret messages can be hidden. The image rests almost the same 
and the secret message can be extracted at the receiver.  Proceeding like that for a 
1024x1024 black and white image one can insert 64 KB of secret messages (sev-
eral modern services are using this capacity). 
For digital imaging, the first invisible marks were used in 1990 in Japan [73] 
and independently, in  1993 in  Europe [23] and [77].  At the beginning the ter-
minology used for such invisible marks was “label” or “signature”; around 1993 
the words water mark were used , signifying a transparent,  invisible  mark. The 
combination of the two words, gave the word “watermark”, which will be used 
henceforward. 
Applications of digital watermarking for audio domain are known since 1996 
[11]. 
In 1995 the first applications for uncompressed and compressed still images 
are done [24]. 
1996 [33], 1997 [46] are marking the beginning for uncompressed, respectively 
compressed video signals.  
After several breakthroughs between 1995 and 1998 it seems that the last years 
can be viewed as a plateau in watermarking research. Simultaneously the industry 
had an increasing role in standards and recommendations elaboration. This phe-
nomenon resembles to the development of modern cryptography and the elabora-
tion of standards for civil applications. 

168
4   Cryptography Basics
 
4.8.3   Watermarking Requirements 
Each watermarking application has specific demands. However there are some 
general, intuitive requirements. 
 
• Perceptual transparency.  It is related to the fact that the watermark insertion 
must not affect the quality of the host data. The mark is invisible if one cannot 
distinguish between the original signal and the marked one, e.g. if the changes 
in the data are below the thresholds of human senses (hearing, seeing). Percep-
tual transparency test are made without knowing the input data. Original or 
marked data are presented independently to the subjects. If the selection per-
centage is equal for the two cases, this means that perceptual transparency is 
achieved. In real perceptual transparency applications, the subjects do not know 
the original data, having therefore correct testing conditions.  
• Robustness is the watermark property to resist to unintentional changes, due to 
the  inherent processing  related to the transmission / storage (unintentional   
attacks) or to intentional changes (intentional attacks) aiming to remove the  
watermark. 
There are some applications when robustness is not required. For data authenti-
cation for example, the fragile watermark needs not to be robust, an impossible 
watermark detection proving the fact that the data is altered, being no longer  
authentic. 
However, for most applications the watermark has to be robust, its extraction 
from the host data leading to a significant quality loss, making the host data  
unusable.  
 
• Watermark payload; the watermark payload is also known as watermark in-
formation. The watermark payload is defined as the information quantity in-
cluded in the watermark. It is application dependent [47] and some usual values 
are: 
 
• 
1 bit for copy protection  
• 
20 bits for audio signals 
• 
60 ÷ 70 bits for video signals 
Another important parameter related to the payload is the watermark granular-
ity. This parameter shows the required quantity of data necessary for the insertion 
of a single watermark information unit. In the above-mentioned example a water-
mark information unit has 20 bits for audio signals and, 60 ÷ 70 bits for video sig-
nals. These bits are inserted in 1 or 5 seconds for audio segments. For video sig-
nals the watermark information unit is inserted in a single frame or is spread over 
multiple frames.    
Watermark spreading improves the detection robustness [33]. For most video 
applications, the watermark information is inserted in less then a second for video 
signals (approx. 25 frames).  
 

4.8   Digital Watermarking 
169
 
• Detection with and without original signal. Depending on the presence of the 
original signal there are two methods for watermark detection [47]: 
 
– 
with the presence of the original signal : nonoblivious (informed) water-
marking  
– 
without original signal: oblivious (public, blind)  watermarking.  
The first type of detection that needs the original signal or a copy of it is used in 
copyright protection applications restraining the inversion attack [25], [84]. 
The second detection modality, not needing the original, is used in application 
where the presence of the original at detection is impossible, for example in copy 
protection. 
 
• Security in watermarking can be seen as in cryptography: contained in the en-
cryption key. Consequently the watermarking is robust if some unauthorized 
person cannot eliminate the watermark although this person knows the insertion 
and detection algorithm. Subsequently the watermark insertion process uses 
one or several cryptographic robust keys. The keys are used also in the water-
mark detection process.  There are applications, like covered communications, 
where encryption is necessary before marking. 
• Ownership deadlock. The ownership deadlock is known as the inversion attack, 
or IBM attack, [10]. Such an attack appears whenever in the same data there are 
several watermarks claiming the same copyright. Someone can easily insert his 
own watermark in the data already marked. 
Watermarking schemes capable of solving this problem (who is the “right” 
owner or who was the first that made the mark), without using at detection the 
original or a copy of it, are not known until now. 
Such a situation can be solved if the watermark is author and host dependent.  
In such a case the author will use at insertion and detection two keys: k1 - author 
dependent and k2 - signal/ host dependent. Using the keys he will generate a 
pseudo-random sequence k. The key k2, signal dependent, can be generated using 
one-way hash (OWH) functions. Such generators are including: RSA, MD4, SHA, 
Rabin, Blum/Blum/Shub [64]. The watermark extraction at the receiver is impos-
sible without knowing the keys k1 and k2. The k2 key, being host dependent, the 
counterfeiting   is extremely difficult. In copyright protection, the pirate will be 
able to give to a judge only his secret key k1 and not k2.  The last key is computed 
automatically using the original signal by the insertion algorithm. The hash func-
tion being noninvertible the pirate will not be able to produce a counterfeit identi-
cal with the original.  
4.8.4   Basic Principles of Watermarking 
As shown in the Introduction, watermarking has two basic processing: one at the 
sender and the other at the receiver: 
 
• Watermark insertion in the host data. The insertion is done respecting the per-
ceptual transparency and robustness requirements. 

170
4   Cryptography Basics
 
• Watermark extraction (detection) from the marked received signals (possibly 
altered) and the comparison between the extracted watermark and the original 
one, in case of deadlock. 
 
For the robustness demand the watermark will be inserted using one or several 
robust cryptographic keys (secret or public). The keys will be further used at wa-
termark detection. 
The perceptual transparency is done according to a perceptible criterion, the 
last one being implicit or explicit. Therefore the individual samples of the host 
signal (audio signals, pixels or transform coefficients) used for the insertion of the 
watermark information will be changed only between some limits situated below 
the perceptiveness thresholds of the human senses (seeing, hearing). 
Transparent insertion of the watermark in the digital host signal is possible only 
because the final user is a human being. His senses (hearing, seeing ) are imperfect 
detectors characterized by certain minimal perceptiveness thresholds and by the 
masquerade phenomenon. By masquerade, a component of a given signal may be-
come imperceptible in the presence of another signal called masquerading signal. 
Most of the coding techniques for audio and video signals are using directly or in-
directly the characteristics of the HAS - human audio system or HVS - human vis-
ual system [72].   
The watermarking techniques cannot, therefore, be used for data representing 
software or numbers, perceived by a computer (machines, not humans). 
According to the robustness demand the watermarking signal (despite the small 
amplitude required by the perceptual transparency demand) is spread over several 
samples according to the granularity demands. This makes possible the detection 
of the watermark signal although the data is noise affected.  
Fig. 4.25 and Fig. 4.26 are showing the bloc schemes of watermark insertion 
and detection. 
 
Fig. 4.25 Bloc scheme for watermark insertion  
 
 

4.8   Digital Watermarking 
171
 
D
Watermark
Extractor
(detector)
Tested 
data
Detected 
watermark
Î
Key
K
X
Original 
host data
Y
C
Comparison
No
Yes
I
Watermark 
information
Threshold
γ
 
Fig. 4.26 Bloc scheme for watermark extraction and comparison  
Watermarking (Fig. 4.25) consists in: 
 
• Watermark information (I) generation (payload) 
• Watermark generation (distinct from I – watermark information): W, that will 
be inserted into the host signal X; usually W depends on the watermark infor-
mation (I) and on the key K:  
W = E1 (I, K),                                           (4.61) 
where E1 is a function (in most cases modulation and  spreading). 
There are applications where, in order to limit the IBM attack, the watermark 
signal can be host signal X dependent): 
W = E1 (I, X, K)                                          (4.62) 
• Key generation; the key can be public or secret, leading to a possible classifica-
tion of the watermarking techniques in public keys systems and private keys 
systems.   
• Watermark signal (W)  insertion in the host signal (X). The insertion is made 
with respect to the robustness and perceptual transparency demands, giving the 
watermarked signal Y: 
Y = E2 (X, W),                                          (4.63) 
where E2 is a function (which usually makes a modulo 2 summation between  W 
and  X). 
As a conclusion, in order to fulfil the perceptual transparency demands, the two 
models HAS or HVS, are taken into account directly or indirectly for watermark-
ing. For the robustness requirements, the watermark information I is spread over 
the host data (see the granularity concept).   
Watermarking can be done in the transform domain or in the spatial domain.  
It follows that, before watermark insertion or extraction, the host data needs to be 
converted in the domain where the processing will take place: spatial, wavelet, DCT 
(Discrete Cosine Transform), DFT (Discrete Fourier Transform), fractals. Each  
domain has specific properties that can be used in the watermarking process [47]. 

172
4   Cryptography Basics
 
Watermarking can also be done for compressed or uncompressed host data; 
most applications are, however, for uncompressed data [34]. 
 
Remark 
Due to the perceptual transparency demands, the changes in the host data are rela-
tively small, so the watermark signal W, will be error vulnerable.  In order to 
overcome this drawback, in transmission or storage, several protection measures 
can be taken using error correcting codes before watermark insertion, [2] and [32]. 
 
Watermark extraction (Fig. 4.26) 
 
The watermark detector input signal is Y’ and it can be the result of a watermarked 
signal with errors or not. In order to extract the watermark information Î, the original 
signal X is necessary – or not - depending on the detection type: (Fig. 4.26): 
Î = D (X, Y′, K) - nonoblivious detection                         (4.64) 
Î = D (Y′, K)  - oblivious detection                              (4.65) 
In copyright applications, the detected watermark information Î is compared 
with the ownership original I: 
C (I, Î) = 
⎩
⎨
⎧
<
≥
γ
c
if
no,
γ
c
if
yes,
                                        (4.66) 
In practice, the comparison is made by a correlator that computes the cross-
correlation c between I and Î, and a threshold detector with γ  threshold value.  
Most watermarking techniques are based on the principle of spread-spectrum 
[80], [32]. 
The idea of a spread spectrum system is to transmit a low bandwidth signal (in 
this case the watermark information I) on a large bandwidth channel with interfer-
ences (the audio or the video signal X).  
The major advantages of a spread spectrum transmission are: 
 
– 
the interferences are reduced  
– 
the signal is hidden against interceptions.  
 
• Watermark insertion: 
The watermark information I is spread using a modulation with a pseudo-
random noise, the watermarked regions being hidden in this way. In order to avoid 
processing that can eliminate the watermark information the insertion regions 
must be known (or at least recognizable). The spread spectrum techniques ensure 
an efficient protection of the watermark especially against usual, non-intentional 
data manipulation in transmission or storage (compression, scaling etc). 
• Watermark extraction: 
In spread spectrum based watermarking, the authorized detection ( K is known) 
is easy to implement, even in the absence of the original X, using a correlator  

4.8   Digital Watermarking 
173
 
receiver. The lack of synchronization can be eliminated using a sliding correlator. 
The maximum value of the correlation functions – the true value of the watermark 
information I- is found in this case by sliding. 
We will illustrate the spread spectrum based watermarking principle for video 
signal in the spatial domain [32]. 
A video sequence can be represented as three-dimensional signal. Such a signal 
has two dimensions in the x-y axis and the third one is the time. For line scanning 
systems the signal can be viewed as a signal with a single dimension. 
 
Fig. 4.27 Line - scanning video signal  
• Video signal watermark insertion (Fig.4.28) 
Let I be the watermark information with 
1
N  bits 
j
a : 
{ }
{
}.
1
1,
a
,
N
1,
j
,
a
I
j
1
j
−
∈
=
=
                                  (4.67) 
1
N  is the watermark payload and as we have seen its maximal value is 60 ÷ 70 
bits. 
In order to make the watermark more robust this sequence I is spread over a 
large number of bits according to the chip rate (or spreading factor) m . Typical 
values for m are between (103 ÷ 106) [81]. The spread sequence is then: 
(
)
N
i
j,
1
m
i
jm
,
a
b
j
i
∈
+
<
≤
=
                               (4.68) 
The spread spectrum sequence, amplified with a local factor
0
Ȝi ≥
, modulates 
a pseudo-random signal, the key K with 
i
p bits, 
}
{p
K
i
=
: 
{
}
N
i,
1
1,
pi
∈
−
∈
                                           (4.69) 
The watermarked spread sequence 
{
}
i
w
W =
 
N
i,
p
b
Ȝ
w
i
i
i
i
∈
=
                                           (4.70) 

174
4   Cryptography Basics
 
is summed  with the video signal 
i
x . The video watermarked sequence is obtained:  
{ }
N
i
,
y
Y
i
∈
=
                                          (4.71) 
N
i
,
p
b
Ȝ
x
y
i
i
i
i
i
∈
+
=
                                  (4.72) 
Watermark information bits aj 
         I 
 
Spreading 
  m 
 t 
Modulation 
 
K 
PN- sequence 
X 
t
t 
Y
xi 
yi 
iλ  
i
i
i
i
w
b
p
=
λ
 
i
b
 
i
p
Insertion control 
according to HVS 
Additive insertion 
Original video sequence 
Watermarked video sequence 
 t 
spreading factor 
 
Fig. 4.28 Video sequence watermark insertion model  
The pseudo-random sequence 
{ }
ip
K =
 and the corresponding spread water-
marked sequence 
{
}
i
w
W =
) are pseudo-random  signals being therefore difficult 
to detect, localize and eliminate. 
The security of a watermarking system depends on the key K. The key is cho-
sen in order to insure the cryptographic security. Besides this, for correlative de-
tection, the key must have good correlation and orthogonality properties. In such a 
way several watermarks can be embedded in the same host signal and these dis-
tinct watermarks can be detected without ambiguity at the receiver [80], [81]. 
The amplification factor
i
Ȝ , depends on the local properties of the video signal. 
In order to make the amplitude of the local watermark amplitude maximal, with 
respect to the perceptual transparency demands, the amplification factor 
i
Ȝ  can 
use HV spatial and temporal masquerade phenomena. 
The watermark information is inserted in the least visible regions, for example 
in regions with fine details or contours. 
 
 

4.8   Digital Watermarking 
175
 
• Video signal watermark extraction (Fig. 4.29) 
Watermark information 
bits aj 
 Î 
 t 
 
K 
PN sequence 
Y 
t 
yi 
ip  
Video watermarked 
sequence 
Sumation  and 
thresholding 
 t 
Demodulation 
iy  
Filtering 
dj = aj 
 
Fig. 4.29 Video sequence watermark extraction model  
When the spread spectrum principle is used, the authorized extraction (the key 
K is known) can be easily done without knowing the original X, using a correlator. 
Before demodulation (correlator based), the watermarked video sequence Y´ 
(identical or different form the original one due to attacks or transmission process-
ing) is high-pass filtered. The high pass filtering eliminates the major components 
from the video signal. High pass filtering is not compulsory but enhances the per-
formances of the whole system, reducing the interferences between the video sig-
nal and the watermark signal.  
Watermark detection consists in a multiplication between the watermarked 
video signal (Y´) with the same key (K) used in the insertion process, followed by 
a correlation sum computation,
j
d for each inserted bit  and by a thresholding op-
eration. An estimate (âj) for the watermark information bit 
j
a is thus obtained. 
(
)
(
)
(
)
∑
∑
+
=
=
−
+
=
−
+
=
1
m
1
j
jm
i
1
m
1
j
jm
i
i
i
i
i
i
i
i
j
p
b
Ȝ
x
p
y
p
d
                        (4.73) 
where 
_
iy   is the value of the i - th high pass filtered video bit, supposed not af-
fected by errors. Consequently:  
i
i
i
i
w
x
y
y
+
=
=
′
                                           (4.74) 
2
1
S
1
1)m
(j
jm
i
i
i
i
i
S
1
1)m
(j
jm
i
i
i
S
S
p
b
Ȝ
p
x
p
2
1
+
=
∑
∑
+
−
+
=
−
+
=
−
*
* )
*
* (
'
*
* )
*
* (
'
                          (4.75) 
2
1 S
,
S
 are describing the contribution in the correlation sum of the filtered video 
signal and the filtered watermarked signal. 

176
4   Cryptography Basics
 
Considering that by high pass filtering the video signal 
i
x  has been practically 
eliminated from 
iy , the assumption 
0
S1 =
 is correct. Considering also that the 
influence of the high pass filtering on the watermarking signal is negligible, we 
have: 
i
i
i
i
i
i
p
b
Ȝ
p
b
Ȝ
≅
                                           (4.76) 
Under these assumptions: 
[ ]
(
)
∑
=
≅
−
+
=
1
m
1
j
jm
i
i
j
2
p
i
i
2
i
j
Ȝ
mE
a
σ
b
Ȝ
p
d
,                             (4.77) 
where  
2
p
σ  is the variance  of the pseudo-random noise (K), and  [ ]
i
Ȝ
E
 is the m 
pixels based average value of the coefficients 
i
Ȝ . 
The sign of the correlation sum gives the value of the watermark bit 
j
a  in hard 
decision schemes: 
)
(a
sign
)
Ȝ
m
σ
(a
sign
d
sign
j
0
i
2
p
j
j
=
=
>
)
(
'
                               (4.78) 
As seen from (4.78) the transmitted information bit is +1 if the correlation be-
tween the video signal containing the watermark bit and the pseudo-random se-
quence is positive, and –1 if the correlation  is negative.  
If the pseudo-random sequence used for the watermark extraction is different 
from the one used for watermark insertion (or if we do not have a synchronism be-
tween the two sequences) such a scheme will not work and the computed bits are 
random.  
The lack of synchronization between the two sequences can be eliminated using 
a sliding correlator: every single possible sliding is done experimentally, the true 
value for 
j
a corresponds to the maximum value of the correlation
j
s , obtained for 
a specific sliding. 
 
Remark 
The scheme given in Fig. 4.29 is an oblivious one, not needing the presence of the 
original X for watermark extraction. For a nonoblivious scheme, the use of the 
original, subtracted from Y before the demodulation – instead of filtering- will 
eliminate any interference between the video signal Y and the watermark W. The 
watermarking extraction in this case is much more robust. 
The performances of this scheme can be estimated by computing the bit error 
rate BER  (Bit Error Rate). One bit is in error if: 
(
)
(
)
2
2
1
S
sign
S
S
sign
≠
+
                                      (4.79) 
This situation corresponds to: 
2
1
S
sign
S
sign
≠
 and 
2
1
S
S
>
                                (4.80) 

4.8   Digital Watermarking 
177
 
In [32] it is shown that the bit error rate is:  
[ ]
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
+
=
2
x
2
x
i
p
ȝ
σ
2
Ȝ
E
m
σ
erfc
2
1
BER
,                                (4.81) 
where 
2
x
σ  and  
x
ȝ
 are the variance and the average value of the video signal, re-
spectively. 
From (4.81) we can see that the BER is small if m , 
p
σ  and [ ]
i
Ȝ
E
 have high 
values.  
In [32] - Table. 2. gives several examples of BER  computed and measured for 
different values of m , and 
i
Ȝ ,  with/without filtering.  
The watermark information bit rate is: 
factor
 
spreading
 
second
per 
 
pixels
 
luminance
 
of
number 
R WM =
                     (4.82) 
Remarks 
In order to maintain a certain value for the BER, even when attacks occur, the ar-
gument of the BER has to be raised using an insurance factor [32]. 
If m increases, 
WM
R
 decreases. If the number of the information bits per sec-
ond is lowered correspondingly the watermark still rests robust to intentional or 
non-intentional attacks. The robustness can be explained by the fact that each in-
formation bit 
j
a  is spread over a larger number of bits 
i
b . 
If the spreading factor m decreases, the BER also decreases. In order to keep 
the BER between some admissible limits, even if m is small and  
WM
R
 relatively 
big, soft decoding error correcting codes can be used [72]. 
The method shown above for spatial domain video signals watermarking, can 
be used in any transformed domain of the video signal. Every transformed domain 
has its own advantages and drawbacks. A very good presentation of the state of 
the art in watermarking is made in [47]. 
4.8.5   Specific Attacks 
4.8.5.1   Attack Definition and Classification 
The causes leading to errors in the watermark extraction process are called  
attacks. 
According to the way they were produced, the attacks can be classified in two 
major categories: 
 
– 
Unintentional attacks, due to the usual signal processing in transmission or 
storage: linear (nonlinear) filtering, JPEG compression, MPEG-2 compres-
sion, pixel quantisation , analog to digital conversions, digital to analog 
conversions for recording processes, γ correction. A detailed description of 
these attacks is done in [26]. 

178
4   Cryptography Basics
 
– 
Intentional attacks intentionally made in order to eliminate the watermark 
or to insert false watermark, keeping also the perceptual fidelity. 
There is other attacks classifications among them we refer to: [81], [34]: 
 
A. 
Simple attacks, the watermarked signal sustains some distortions, how-
ever the intention being not to eliminate the watermark. The majority of these at-
tacks are unintentional attacks described above. 
B. 
Detection disabling attacks, including the synchronization attack. These 
attacks are oriented towards the watermark extraction devices; the purpose of such  
an attack is to avoid watermark detection. A common characteristic for such  
attacks is the signal decorrelation, making the correlation based watermark extrac-
tion impossible.  In this case the most important distortions are geometric distor-
tions: zooming, frame rotation, sub-sampling, the insertion or extraction of a pixel 
or a group of pixels, pixel interchanges, spatial or temporal shifts. 
In the case of the Stir Mark [69], the jitter attack consists in the elimination  
of some columns and the multiplication of others, keeping unchanged the image 
dimensions. 
On the same category, frame modifications are included: frame removal, frame 
insertion or swapping. 
C. 
Ambiguity attacks, also known as confusion, deadlock/ inversion-IBM/ 
fake watermark/ fake original attacks. These attacks are trying to create some con-
fusion by producing a fake original. 
D. 
Removal attacks are trying to decompose the watermarked image Y in a 
watermark W and an original X, in order to eliminate the watermark. In this cate-
gory we mention the collusion attack, noise extraction and nonlinear filtering. 
 
In multimedia MPEG compression based applications the attacks can be done 
in the compressed domain (frequency - DCT), or in the spatial domain.  The most 
important attacks are done in the spatial domain, for uncompressed signals.  
There are computer programs for several kinds of attacks, among them we 
mention: 
 
– 
Stir - Mark, from Cambridge University, 
– 
Attack, from University of Essex, 
still images oriented useful also for dynamic images too. 
In the following paragraphs we will expose the principle for two of the most 
powerful attacks: the inversion (IBM) attack, and the collusion attack. 
4.8.5.2   The Inversion Attack / IBM / Confusion / Deadlock / Fake 
Watermark / Fake Original 
The goal of this attack is the insertion, by faking, of some watermarks in an al-
ready watermarked signal, followed by a copyright claim of the fake owners. 
The resulted signal contains several watermarks creating therefore a confusion 
related to the right owner (the first person who watermarked the document). 
 
 

4.8   Digital Watermarking 
179
 
From a mathematics point of view this situation can be modeled as follows: 
Let X be an original document watermarked by its owner (p): 
Yp = X + Wp                                               (4.83) 
and Yf  the fake document, obtained by the forger by adding its own watermark 
Wf  to the original already watermarked: 
Yf = Yp + Wf = X + Wp +Wf                                 (4.84) 
By proceeding on this manner, a new document with two watermarks is cre-
ated, for the new document both the right owner and the forger could claim the 
copyright, each one having its own watermark inserted. 
The question is, who is the right owner of the document X ? 
 
  
A simple solution consists in using the arbitrated protocol. Let Dp and Df be 
the watermark extraction algorithms used by the owner and the forger. 
The judge verifies Yp and Yf using at the time the two extraction algorithms: 
(
)
(
)
(
)
(
)
⎪⎩
⎪⎨
⎧
=
+
+
=
=
+
=
f
f
p
p
f
p
p
p
p
p
p
W
W
W
X
D
Y
D
W
W
X
D
Y
D
                           (4.85) 
and: 
(
)
(
)
(
)
(
)
⎪⎩
⎪⎨
⎧
=
+
+
=
=
+
=
f
f
p
f
f
f
p
f
p
f
W
W
W
X
D
Y
D
0
W
X
D
Y
D
                          (4.86) 
The forger is identified, and thus, the problem posed in [72], is solved.  
The above-mentioned problem becomes more difficult when the forger is able 
to produce a false (counterfeit) original: Xf - this case is known as the inversion 
attack or IBM attack. 
Starting from a watermarked original: 
Yp = X + Wp ,                                           (4.87) 
a false original Xf  is created by the forger, by extracting a false watermark Wf:  
Xf  = Yp - Wf                                            (4.88) 
It follows that: 
Yp = Xf +Wf ,                                           (4.89) 
Consequently the already marked image contains the forger’s watermark; the 
forger can pretend now that Yp is his, being created from the counterfeit original 
Xf and his watermark. 
The above-described arbitrated protocol can no longer tell who is the right 
owner: 
(
)
(
)
(
)
(
)
⎪⎩
⎪⎨
⎧
=
+
+
=
=
+
=
f
f
p
p
f
p
p
p
p
p
p
W
W
W
X
D
Y
D
W
W
X
D
Y
D
,                         (4.90) 
because Yp is the same and it contains both watermarks. 

180
4   Cryptography Basics
 
This attack cannot be avoided in oblivious schemes (needing the presence of 
the original for extraction). 
A necessary condition - but not sufficient - for avoiding the inversion attack is 
the use of nonoblivious watermark extraction schemes.  
Basically there are two major nonoblivious extractions schemes:  
 
1. the watermark has to be original dependent, using for example hash functions  
2. the original signal X has to be time stamped at creation, the time stamp will 
permit to establish who is the right owner according to the temporal stamp’s 
date. 
 
1. Oblivious watermark extraction schemes using hash functions. 
An original dependent watermark can be produced using one way hash func-
tions, so that: 
H(X)=h,                                               (4.91) 
h represents the value of the hash function (hash value). Some robust hash func-
tions based algorithms are MD4, MD5 and SHA [64]. It imposes the demand that 
any watermarking system has to be oblivious; it is evident that such a demand has 
to be also legally established in order to counterattack the IBM attack. In this case 
the forger can no longer generate a false original Xf because the watermark Wf 
cannot be dependent on the false original Xf.  
2. Time stamping means that the watermark information has to contain some 
time stamp with information related to the certified original X, the certification 
date T, and also the name of the owner P, so such a time stamp is: 
S = (h, T, P),                                            (4.92) 
where h is the hash value of the function H(X). Consequently the watermarked 
image is: 
Y = X + W(S)                                           (4.93) 
In this case the IBM attack is impossible, due to the fact that it is impossible to 
obtain: 
Xf  = Y - W(Sf)                                         (4.94) 
4.8.5.3   The Collusion Attack 
The goal of this attack is to eliminate the watermark, therefore to obtain an un-
marked signal X. 
Conditions: the existence of at least two copies of the same original differently 
watermarked: 
2
2
1
1
W
X
Y
W
X
Y
+
=
+
=
                                          (4.95) 
where W1, W2 are two digital fingerprints. 
 

4.8   Digital Watermarking 
181
 
Actions 
1. Collusion by subtraction. The difference between the two copies is done, so the 
watermarks are localized and therefore eliminated: 
Y1  - Y2 = W1 - W2                                         (4.96) 
2. Collusion by addition. This is the general case for this type of attack; in this 
case the collusion is viewed as a mean operation performed on the same origi-
nal marked with several different watermarks: 
2
2
1
1
W
X
Y
W
X
Y
+
=
+
=
                                              (4.97) 
The mean of the copies is: 
2
W
W
X
2
Y
Y
2
1
2
1
+
+
=
+
                                   (4.98) 
From the above relation it can be seen that the watermark is half reduced; for a 
larger number of watermarked versions of the same signal, the original X can be 
obtained. 
4.8.6   Applications 
The main applications of the watermarking were mentioned in introduction. As 
shown in 4.8.3, each application has specific demandes concerning the watermark-
ing requirements. However, three important features define watermarking from 
other solutions: 
 
• watermarks are perceptually transparent (imperceptible), 
• watermarks are inseparable from the host data in which they are inserted, mean-
ing that support the same processing as the host data. 
 
The quality of the watermarking system is mainly evaluated by its: 
• robustness, which indicates the capability of the watermark to “survive” to un-
intentional or intentional attacks, 
• fidelity, describing how transparent (imperceptible) a watermark is. 
 
The value of these quality parameters is highly dependent on the application. 
We will proceed presenting the most important applications, their requirements, 
the limitations of alternate techniques, emphasizing the necessity and advantage of 
the watermarking solution [27]. 
4.8.6.1   Broadcast Monitoring 
• Who is interested in it? 
– 
advertisers, which are paying to broadcasters for the airtime; in 1997 a 
huge scandal,  broke out in the Japanese television advertising (some sta-
tions usually had been overbooking air time), 

182
4   Cryptography Basics
 
– 
performers, to  receive the royalties due them by advertising firms; in 1999 
the Screen Actor Guild, after a check, found that the unpaid royalties in US 
television programs is aprox. 1000   USD/hour, 
– 
owners of copyrighted works, who want to be sure that their property is not 
illegally rebroadcast (major events, catastrophes, exclusive interviews, etc). 
 
• Broadcast monitoring ways: 
– 
using human observers to watch the broadcast and record what they see 
and hear; these techniques is expansive, low tech and predisposed to errors. 
– 
automated monitoring: 
- passive monitoring: the humans are replaced by a computer  that moni-
tors broadcasts and compares the received data with those found in a data 
base with known works; in case of matching the work is identified. The ad-
vantages of this system are: do not require cooperation between advertisers 
and broadcasters and is the least intrusive. Unfortunately it has major disad-
vantages related to the implementation (need of huge data base, impractical to 
search); if signatures of the works are used, the data base diminishes a lot, but 
the broadcast altering the work, will make the signature of  the received work 
different as those corresponding to the original (an exact matching is impossi-
ble). This system is applied for marketing, but not for verification services, 
because its error rate. 
 - active monitoring: it removes the main disadvantages of the passive 
monitoring. There are two ways to accomplish this aim: using computer rec-
ognizable identifiers (e.g. Vertical Blanking Interval for analog TV or file 
headers for digital format) along with the work or the watermarking. The 
first solution has as main disadvantage the miss of guarantee in transmission 
and the lack of resistance to format changes. The watermarking solution, 
even more complicated to be embedded (compared to file headers or VBI), 
has not any more the risk to be lost as identifiers are. 
4.8.6.2   Owner Identification: Proof of Ownership 
Textual Copyright notices, placed in visible places of the work, are the most used 
solutions for owner identification. An exemplification for visual works is: 
 
 
“Copyright data owner” 
 
“© data owner” (© 2002 by Academic Press) the textual copyright notice 
for “Copr. data owner” [27]. 
 
Unfortunately this widespread technique has some important limitations: the 
copyright notice is easily omitted when copied the document (e.g. photocopy of a 
book without the page containing the copyright notice), or cropped (Lena is a 
cropped version of November 1972 Playboy cover) and, if not removed, it is un-
aesthetic. The watermarking is a superior solution, eliminating the mentioned 
limitations due its proprieties of inseparability from the work (ca not be easily re-
moved) and imperceptibility (it is aesthetic). 

4.8   Digital Watermarking 
183
 
A practical implementation of a watermarking system for this application is 
Digimarc, distributed with Adobe Photoshop image processing software; when 
Digimarc’s detector recognize a watermark, it contacts a central data base over the 
Internet and uses this information to find the image owner. This available practical 
system is useful only for honest people. 
When malicious attacks are done, as collusion for removal and IBM for creat-
ing a fake owner, the proof of ownership is a major task. In a copyrighted work a 
forgery is very easy to be done in both cases presented before. As example: 
“© 2000 Alice” can easily be transformed by an adversary (pirate, traitor) Bob 
as: “© 2000 Bob” by a simple replacement of owner’s. Using the Digimarc sys-
tem, the replacement of Alice watermark with Bob watermark is not difficult (if a 
watermark can be detected, probably can be removed to). For this situation there 
are some solutions as using an arbitrated protocol (which is costly) or using work 
dependent watermark, as presented in chapter 4.8.5. 
4.8.6.3   Fingerprinting (Transaction Tracking) 
The digital technology allows making quickly cheap and identical (same quality) 
copies of the original and to distribute them easily and widely. Distribution of ille-
gal copies is a major problem of our days, implying always money (huge amount) 
loses of copyright owners. 
Fingerprinting is a new application, aiming to catch the illegal distributors or at 
least making its probability greater. A legal distributor distributes to a number of 
users some copies, each one being uniquely marked with a mark named finger-
print (like a serial number). A number of legal users, called pirates, cooperate in 
creating illegal copies for distribution. If an illegal copy is finding, the fingerprint 
could be extracted and traced back to the legal owner of that copy. 
The idea of transaction tracking is old, being used to unveil spies. To suspects 
were given different information, some of them false, and by the action of enemy 
those who revealed the information were caught. 
A practical implementation of a fingerprinting system was done by DIVX Cor-
poration, now defunct [27]. Each DIVX player placed a unique watermark (fin-
gerprint) into every played video. If such a video was recorded and than copies 
sold on the black market, the DIVX Corporation could identify the pirate by de-
coding the fingerprint. Unfortunately no transaction tracking was done during the 
life of DIVX Corporation. 
Transaction tracking in the distribution of movie dailies is another application. 
These dailies are very confidential, being not allowed to the press. Studios prefer 
to use fingerprinting, and not visible texts for marking, because the last one are 
very easily removed. The quality (fidelity) is not necessary to be very high in this 
application. 
4.8.6.4   Content Authentication (Fragile Watermarking) 
The aim of this application is to detect data tampering and to localize the  
modification. 
 

184
4   Cryptography Basics
 
Why is it so important? 
Because in digital, tampering is very easy and very hard to detect (see image 
modification with Adobe Photoshop). Consequences of such tampering could be 
dramatically in a police investigation! 
Message authentication [64] is a basically problem of cryptography, a classical 
solution being the digital signature, which is an encrypted summary of the message. 
This solution was used in “trustworthy digital camera” [27] by computing a 
signature inside the camera. These signatures act as a header (identifier) necessary 
to be transmitted along with the work. Unfortunately the integrity of this header is 
not guaranteed if format changes occur. As in broadcast monitoring, instead of a 
header, which could be lost, an incorporated signature in the work is preferred, re-
alized using watermarking. These embedded signatures are authentication marks. 
These marks become invalid at the smallest modification of the work and for this 
reason they are named fragile watermarks. 
In this application, robustness is not anymore a requirement; the watermark de-
signed for authentication should be fragile, as nominated. A detailed presentation 
of this application is done in [27], chapter 10. 
4.8.6.5   Copy Control 
If the former described applications had effects after an intentional forgery, the 
copy control is aimed to prevent people to make illegal copies of copyrighted 
works. The ways for this type of protection mentioned in the Introduction are en-
cryption and watermarking. The both, without an appropriate legislation will not 
solve the problem. 
In the next chapter the first trial for an appropriate standard proposal is  
presented. 
4.8.7   The Millennium Watermark System [53] 
4.8.7.1   Introduction 
The first attempt of standardization for the DVD`s copy protection is the Millen-
nium watermarking system introduced by Philips, Macrovision and Digimarc in 
USA; it was submitted to the approval of the USA Congress, and the result was 
the   “Digital Millennium Copyright Act” signed by the president of the USA in 
28. 10.1998. 
The main cause was the market explosion of digital products like DVDs, digital 
broadcasting of multimedia products and the producers’ exposure to potential 
huge financial losses, in case of the non-authorized copying. 
The standardization of the video DVD`s provoked unusual debates in copy pro-
tection (like the 1970 ÷ 1975 years for the DES standardization) influencing the 
whole multimedia world. 
On a DVD the information is encrypted (with secret algorithms) but in order to 
assure the copy protection the encryption is not enough. Using encryption on a 
storage device: CD, DVD, or in transmission on communication channels or open 

4.8   Digital Watermarking 
185
 
interfaces copy protection can be realized using an authentication system and a 
session key generating mechanism for all interfaces (end to end encryption). En-
cryption used on DVDs supposes that the players or recorders have incorporated 
compliant devices. When the content is displayed in clear on a monitor or played 
on speaker (to the human consumer) the encryption-based protection disappears. It 
is now when the need for watermarking becomes clear; the watermark assures that 
copying is allowed for a restricted number of copies (one copy) or prohibited 
(never copy).  
The basic demands for the DVD copy protection watermark system are:  
 
• invisible and hard to remove 
• fast watermark extraction (maximum 10 s), therefore real time processing  
• cheap watermark detector, with minimum additional hardware required for 
players and recorders  
• robust detection , the watermark has to resist  when usual processing of the  
signal are performed: compression, noise adding, shifts, format conversions etc. 
• the watermark ` s payload has to be at least  8 bits/ detection interval 
• the false alarm probability - watermark detection without watermark signal-  
has to be below 10-12. 
4.8.7.2   Basic Principle of the Millennium System 
For any practical implementation solution of the system, the basic demands are:  
 
- cheap and simple, 
- robustness with perceptual transparency fulfilled. 
 
According to these demands, from the wide spectrum of technical solutions, the 
following one has been chosen: the real time detection in the spatial domain using 
a simple spatial correlator. Even if the transformed domain (DCT or wavelet) of-
fers several advantages, the needed processing cannot be done in real time.  
In order to reduce the complexity, pure spatial watermarking is used, each wa-
termark sequence W is repeatedly inserted in each frame of the video signal. By 
proceeding on this manner one can view the video signal as a time accumulated 
still image. Watermark detection can be described consequently: 
i
i
t
t,i
w
y
T
N
1
d
∑
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛∑
=
,                                      (4.99) 
where: t  is the temporal position of a pixel 
 
i is the spatial position of a pixel 
           
N is the number of pixel from an image (frame), 
      
T is the number of frames used in the detection process. 
 
Using time accumulation, the number of multiplying is reduced, and conse-
quently the complexity and also the processing time are also reduced.  

186
4   Cryptography Basics
 
Block schemes for the watermark insertion and watermark detection are given 
in Fig. 4.30.a and Fig. 4.30.b, respectively. Both schemes are using the principle 
illustrated in Fig. 4.28 and Fig. 4.29:  
 
 
 
 
 
 
 
s 
xi 
λi 
wi 
yi 
image 
payload 
Y 
PN 
W(I) 
Λ(X) 
I 
X 
yi = xi + sλiwi 
key 
 
y 
N1 × N2 × T 
[d I ] 
PN 
Y 
M × M 
Î 
AF 
a)
b)
 
Fig. 4.30 Millennium Watermark block schemes: a) insertion, b) detection  
N1 = 720, N2 = 480, T = 27, M = 128. 
 
Watermark insertion (fig. 4.30.a) 
The insertion of the same watermark in a given number of consecutive video 
frames is a 1 bit (mark) information for a video sequence. 
A watermarked image Y = {yi} is obtained using: 
yi = xi + sλiwi ,                                           (4.100) 
s being a global scaling parameter, and  Λ = {λi} a local image dependent scaling 
factor.  The values for λi are smaller in the ``quiet`` regions within the image and 
bigger in the `` active`` ones (contours, peaks). A satisfactory local scaling factor 
is obtained when the image is Laplace high pass filtered and taking the absolute 
value: 
Λ = ⎪L⊗X⎪                                            (4.101) 
the sign ⊗ represents the cyclic convolution , and   
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
−
−
−
−
−
−
−
−
=
1
1
1
1
8
1
1
1
1
L
                                       (4.102) 

4.8   Digital Watermarking 
187
 
From (4.100) we can see that the (spread) watermark sequence W is a simple 
additive noise. Being formed using a pseudo-random sequence, its spectrum repre-
sents a white noise. 
 
Watermark detection (fig. 4.30.b) 
As shown above, the watermark detection is done using a spatial correlation, it 
follows that the watermarked image is: 
Y = X + s × Λ × W                                        (4.103) 
Then the correlation sum used for the detection is: 
∑
+
∑
=
i
2
i
w
i
Ȝ
s
N
1
i
i
w
i
x
N
1
d
                              (4.104) 
Remark 
In (4.104) it is assumed that during detection there is synchronization between the 
image and the watermark.  For a correct aligning, the correct position is searched 
taking into consideration all the allowed spatial shifts, the decision variable dk  
being: 
∑
=
−
i
k
i
i
k
w
y
N
1
d
                                     (4.105) 
For a real time detection the search for all the possible shifts is computationally 
prohibitive. The solution used in Millennium is the insertion of translation symme-
try in the structure of the watermark:  
wi+k = wi ,                                            (4.106) 
for any vector k, which components are, multiplies of M , M being the dimension 
(tile)  of the processing window support. The chosen practical value is M = 128. 
Under these hypotheses the exhaustive search for all the possible shifts greatly 
simplifies. Due to the fact that the watermark is repeated over values that are mul-
tiplies of M, the watermarked image is superposed on an M × M matrix B={bi}. In 
this case  
∑
=
−
i
k
i
i
2
k
w
b
M
1
d
                                       (4.107) 
Actually (4.107) represents a cyclic two-dimensional convolution. 
D = B ⊗ W *                                              (4.108) 
(4.108) can be easily computed in the frequency domain [53]:  
D = IFFT (FFT(B) × FFT ( W )* )                               (4.109) 
In (4.109) FFT (Fast Fourier Transform) and IFFT (Inverse FFT) are the dis-
crete Fourier transform and the inverse discrete Fourier transform, computed 
through a fast algorithm. 

188
4   Cryptography Basics
 
Remark 
The detection performances can be improved if an adaptive filtering precedes the 
convolution.  
From an experimental point of view the best detection is obtained by ignoring 
the amplitude information from W  and retaining only the phase information: 
D = IFFT (phase (FFT(B)) × phase (FFT ( W ))* )                   (4.110) 
Such a detection method is known in pattern recognition, as the symmetrical 
phase only filtering - SPOMF. 
4.8.7.3   Millennium Standard Implementation 
The real time watermark detector was built on three separate platforms: 
 
• on a high-end Silicon Graphics workstation, 
• on a Tri Media processor based board 
• on a  FPGA board. 
The author shows in [53] that for DVD the best implementation is the FPGA 
based one. The costs associated with the implementation of the Millennium stan-
dard are given below: 
 
 
IC 
ROM 
RAM 
FPGA 
17kG 
34kB 
36kB 
Silicon 
14kG 
1kB 
36kB/3,6mm2 
 
Remarks 
• the data from Table 1 are related  to a single watermark detector  
• in general the watermark detector has to be implemented on a MPEG decoder 
or a host-interface chip of a DVD-drive ; in both cases the functioning of the 
IC`s  - (MPEG - decoding and PCI - bus streaming integrated circuits) needing 
huge memory buffers.  
 
Even though there are no written information about the robustness of the Mil-
lennium system some conclusions tells that it fulfils the desired goal. 
4.8.7.4   Unsolved Problems 
The watermark detector can be placed in two places: 
 
• From a security point of view the place of the copy control is in the driver of 
the DVD player, in the closest place to the storage medium (DVD). The PC 
data bus receives data from these DVD drivers, writing them into sectors with-
out any interpretation possibility (audio, video or other). 
 

4.8   Digital Watermarking 
189
 
For such a placement of the watermark detector, the data written into sector 
has to be recognized: 
 
– 
data concatenation from different sectors  
– 
decryption 
– 
demultiplexing  
– 
partial MPEG decompression. 
The above-mentioned processing   are nor usually done in the driver. 
• The second solution is the MPEG decoder; in such a case the line between the 
driver and decoder has to be secure (encrypted). It seems that this solution is 
preferred. 
4.8.7.5   Copy Control [53] 
There are two basic principles: 
 
a) The remarking concept, consisting in the insertion of a second watermark 
by the recorder. 
b) The ticket concept, consisting in volatile information, lost in the copying 
process, like in the case of a bus ticket that looses its validity by obliteration.   
The ``ticket`` acts like an authorized identifier. In order to assure that the ticket 
is specific to certain information, and to a certain transfer - for example copying- 
the ticket is encryptionally tied with the payload.  
4.8.7.6   Media Type Recognition 
According to the usage type, several media types can be used: 
 
– 
pressed storage media (ROM), that can not be recorded  
– 
recordable storage media. 
The media type recognition can be regarded as a solution used in order to pre-
vent ROM type discs recording.   
4.8.8   Some Remarks 
Digital watermarking was presented as a solution for copyright protection and espe-
cially for multimedia product unauthorized copying. In fact, even though several so-
lutions were proposed, actually the domain rests untested, not experimented. 
Among the great lacks shown, we can remind in the first place: 
 
– 
the lack of standardization in algorithm testing and evaluation (lacks of 
benchmarking) (something like StirMark)  
– 
the lack of a suitable legislation.  
 

190
4   Cryptography Basics
 
The copyright protection problem [72] shows that watermarking is by no means 
an unfailing method. Any research teams (even the whole technical community) 
will not solve copyright protection, because it is related to several legal aspects in-
cluding a concise definition for similarity and subsequent work. Now we are in a 
period of great interdisciplinary efforts for national and international recommen-
dations and standard elaboration for ownership protection in the digital era, in 
which both the multimedia products manufacturers and the legislation (the politi-
cal factors) have to arrive to an agreement. 
4.9   DNA Cryptography 
4.9.1   Introduction 
DNA cryptography and steganography is a new field, born from L. M. Adleman 
research [1] and Viviana Risca project on DNA steganography [74]. 
The huge advantages that DNA structure offers for efficient parallel computation 
and its enormous storage capabilities, made from this field a very promising one for 
various applications despite today limitations: expensive or time consuming. 
For cryptographic purposes the interest is to generate very long one time pads 
(OTP) as cryptographic keys, which ensures the unbreakability of a crypto-system 
[66], to convert the classical cryptographic algorithms to DNA format and to find 
new algorithms for biomolecular computation. 
Taking into account the huge advances in DNA technology, especially in mi-
croarray, the bio-processor [63], obeying an accelerated Moor’s law [64], we must 
expect a faster repetition of microprocessor evolution and at larger scale. 
4.9.2   Backgrounds of Biomolecular Technologies 
The deoxyribonucleic acid (DNA), the major support part of genetic information 
of any organism in the biosphere, is composed by two long strands of nucleotides, 
each of them containing one of four bases (A – adenine, G -  guanine, C – cyto-
sine, T – thymine), a deoxyribose sugar and a phosphate group. The DNA strand 
leave chemical polarity, meaning that on each end of a molecule there are different 
groups (5’- top end and 3’- bottom end) [63]. 
A DNA molecule has double stranded structure obtained from two single-
stranded DNA chains, bonded together by hydrogen bonds: A=T double bond and 
C≡G triple bonds. The double helix structure is configured by two single antipar-
allel strands (Fig. 3.10 paragraph 3.3.5). 
The DNA strands can be chemically synthesized using a machine known as 
DNA synthesiser. The single stranded chains obtained artificially with the DNA 
synthesiser are called oligonucleotides, having usually 50-100 nucleotides in 
length. In what follows, the individual strands will be referenced as single  
 

4.9   DNA Cryptography 
191
 
stranded DNA (ssDNA) and the double helix as double-stranded DNA (dsDNA). 
Individual ssDNA can, under certain conditions, form dsDNA with complemen-
tary ssDNA. This process is named hybridization, because the double stranded 
molecules are hybrids of strands coming from different sources (Fig. 4.31). 
 
 
Fig. 4.31 Hybridization process  
Ribonucleic acid (RNA) is a single strand of ribonucleotides, identical with 
ssDNA strands expect thymine (T) which is replaced with uracil (U). 
The genetic information flows from DNA into mRNA (messenger RNA) proc-
ess known as transcription and from mRNA to protein, process called translation, 
defining what is known as the central dogma of molecular biology (Fig. 3.9 from 
paragraph 3.3.5) [63]. 
Gene is a segment of DNA that contains coding sequences (exons) and non-
coding sequences (introns). 
 
 
Fig. 4.32 Gene representation  
When a gene is active, the coding sequence is copied in the transcription proc-
ess in mRNA and, in the translation process, the mRNA directs the protein seg-
ments via genetic code (see 3.3.5). Transcription is governed by regulatory ele-
ments (enhancer, promoter), which are short (10 – 100 bp) DNA sequences that 
control gene expression. 
Chromosome is a large organized structure of DNA coiled around proteins, 
containing genes, regulatory elements and other nucleotide sequences. It replicates 
autonomously in the cell and segregates during cell division (Fig. 4.33). 

192
4   Cryptography Basics
 
 
Fig. 4.33 Chromosome representation (http://www.ohiohealth.com/)  
The entire DNA content of a cell, including nucleotides, genes and chromo-
somes, are known as the genome. Each organism contains a unique genomic se-
quence, with a unique structure.  
Polymerase chain reaction (PCR) is a molecular biology technique used to ex-
ponentially amplify certain regions of DNA using enzymatic replication and start-
ing with the DNA fragment (primer) to be amplified (Fig. 4.34). 
 
Fig. 4.34 Amplifying process in PCR technique  
Recombinant DNA technology (gene splicing, genetic engineering) uses en-
zymes to cut and paste DNA “recombinant” molecules. Recombinant DNA en-
ables the isolation and cloning of a gene and PCR its amplification (Fig. 4.35). 
dsDNA is cut with restriction enzymes (e. q. E.coli) that recognise specific nu-
cleotides (recognition sequence) in it and cleave its double strand in precise loca-
tion, leaving uncompleted “sticky ends” [63]. 
 

4.9   DNA Cryptography 
193
 
 
Fig. 4.35 Illustration of DNA recombinant process  
Gel electrophoresis is a technique used to separate charged and labelled parti-
cles located in the gel (DNA, RNA etc) when electric current is applied. The DNA 
fragments of interest can be cut out of the gel and extracted or can be removed 
from the gel using Southern Blotting [4]. 
Microarray is the biological microprocessor. It is an ordered array of micro-
scopic elements (spots) arranged in rows and columns on a planar substrate (glass 
or silicon). Each spot contains ssDNA molecules attached to the glass substrate 
(target). These molecules allow binding to them, by hybridization the fluorescent 
probe molecules. The gene expression information is measured by the fluorescent 
intensity of each spot (Fig. 4.36) [63]. 
 
1. Sample preparation
2. Fluorescent labeling
3. Hibridization
4. Surface washing
5. Image acquisition
Tissue 1
Tissue 2
ARNm
ARN Extraction
Sample preparation 
and labeling
Hibridization
Washing
Microarray 
image 
acquisition
 
Fig. 4.36 Illustration of microarray experiment  
This new concept presented in 1994, represents the collaboration of Mark 
Schena and Ron Davis (Stanford University and Affymetrix company). It is a 
revolutionary technique allowing an enormous increase in speed and precision of 

194
4   Cryptography Basics
 
gene quantitative analyses. A single microarray can be used to analyse the entire 
human genome (aprox. 25000 genes) in a single step of few minutes! 
4.9.3   Elements of Biomolecular Computation (BMC) 
BMC methods were proposed by Adleman [1] to solve difficult combinatorial 
search problems using the great available parallelism to the combinatorial search 
among a large number of possible solutions represented by DNA strands. In 1995 
there were proposals to use BMC methods for breaking DES [9]. Besides the com-
binatorial search, BMC has many other exciting applications, due to the exceptional 
storage capacity of DNA. In cryptography the interest is for long OTP, for classic 
cryptographic algorithms in DNA format and also for new BMC algorithms. 
4.9.3.1   DNA OTP Generation 
A OTP in DNA format can be generated in two main ways:  
 
• Assembling randomly long sequences from short oligonucleotide sequences. 
The ssDNA segments are bound together using a special protein (ligase) and 
a short complementary template (Fig. 4.37) 
 
Fig. 4.37 Binding process between two ssDNA segments 
• Using the chromosome sequence which is very large (thousands, millions 
bases), or segments of chromosomes delimitated by primers (short length; 
20 bp, DNA sequences) the distinct number of possible primers is 420, indi-
cating the order of the brut force attack in this case. 
4.9.3.2   Conversion of Binary Data to DNA Format and Vice Versa 
In order to use the classic cryptographic algorithms in DNA format, conversion from 
binary to DNA format are required. DNA alphabet being of four letters (A, C, G, T), 
it is obvious that two bits are needed for encoding the four letters (Table. 4.9).  
Table 4.9 DNA to binary conversion 
DNA Binary
ACSII - 7bits 
Decimal 
ASCII - 8 bits 
Decimal 
A 
C 
G 
T 
00 
01 
10 
11 
0 
1 
2 
3 
0+1 = 1 
1+1 = 2 
2+1 = 3 
3+1 = 4 

4.9   DNA Cryptography 
195
 
The plain text message is transformed in bits and after, in DNA format. If the 
message is a text, then the encryption unit is a character and it will be repre-
sented on ASCII code on 7 bits. If the message is an image than a pixel is the 
encryption unit and can be represented on 8 bits at least. Taking into account 
that each DNA letter (A, C, G, T) is represented with 2 bits, it means that an 
ASCII character or a pixel will be represented on 4 letters, being equivalent to  
a byte. 
 
Example 4.11 
Convert the message secret in ASCII, binary and DNA formats. 
 
Solution: 
 
ASCII 
Message 
Decimal 
Binary 7 bits 
Binary 8 bits 
DNA 
s 
115 
1110011 
01 11 00 11 
CTAT 
e 
101 
1100101 
01 10 01 01 
CGCC 
c 
99 
1100011 
01 10 00 11 
CGAT 
r 
114 
1110010 
01 11 00 10 
CTAG 
e 
101 
1100101 
01 10 01 01 
CGCC 
t 
116 
1110100 
01 11 01 00 
CTCA 
 
Remark 
This example was given to understand the ASCII ĺ binary ĺ DNA conversion 
and it is not used for cryptographic purposes (it would be too easy to be broken). 
4.9.3.3   DNA Tiles and XOR with DNA Tiles 
DNA tiles [30] were synthetically designed after Wang tiles [82], using individual 
oligonucleotide chains which might hybridize in one helix, than cross-over and 
hybridize in another helix (Fig. 4.38). 
 
Fig. 4.38 Triple-helix tile  
 
 

196
4   Cryptography Basics
 
Upper and lower helixes end with uncompleted sticky ends, used for binding 
other tiles with complementary sticky ends (Fig. 4.39). 
 
tile 1
tile 2
T C G A
A G C T
 
Fig. 4.39 Tiles assembling through complementary sticky ends  
Binary data can be encoded using a single tile for each bit. The difference be-
tween 0 and 1 is given by the group of nucleotides on sticky ends. 
 
Example 4.12 
Tiles binding in a string will be discussed. 
In order to make a string with DNA tiles, two steps are required: 
 
• Selection of START tiles (Fig. 4.40) 
 
Fig. 4.40 Tiles for START bits in a string  
• Selection of stacks with tiles for the rest of the bits in the string (Fig. 4.41). 
 
Fig. 4.41 Stacks with tiles for the rest of the bits in a string 

4.9   DNA Cryptography 
197
 
• DNA XOR with tiles 
In order to perform bit-wise XOR operation between two messages of given 
length (in cryptography it is XOR between the plain text and OTP key), a linear 
structure is obtained (Fig. 4.42) 
 
Fig. 4.42 XOR computation with tiles  
 
The truth table of XOR function is given in Table 4.10. 
Table 4.10 Truth table of XOR function 
Inputs 
Output 
x1 
x2 
y = x1 XOR x2 
0 
0 
1 
1 
0 
1 
0 
1 
0 
1 
1 
0 
 
4.9.4   DNA Based Steganography and Cryptographic Algorithms 
4.9.4.1   Steganography Technique Using DNA Hybridization 
The technique of hiding a message in a DNA medium (microdot) was presented in 
1999 in [74].  
Based on sticker model for DNA computation [62] and on the technique of hid-
ing a message in DNA microdots [74], a DNA encryption algorithm is presented 
[18]. A ssDNA OTP key is used to encode a text (plaintext). The technique to hide 
the encrypted message (ciphertext) could be the one presented in [74]. The steps 
of the algorithms are: 
 

198
4   Cryptography Basics
 
a) 
Conversion of the plain text in binary 
Message (TEXT) ĺ ASCII ĺ binary (n bits) 
b) 
Generation of ssDNA OTP 
–   each bit will be encoded with 10 nucleotides, chosen randomly; the 
length of OTP will be greater than 10xn. 
c) 
Encryption 
–   for a binary “1” in the message, a strand of 10 bases long, complemen-
tary to the ssDNA OTP created at b) is made. 
–   For a binary “0” in the message, neither operation is performed. 
d) 
Hiding the ciphertext using DNA steganography techniques; the ciphertext 
is placed between two primers and hidden in microdot, for example, as in 
[74], using as background fragmented and denaturated DNA (fig 4.13). 
 
Fig. 4.43 Cipher text hiding: a) structure of cipher text inserted between two primers; b) 
dsDNA containing (hidden) the cipher text  
e) 
Message recovery (decryption) 
The intended recipient requires the knowledge of the: 
–   medium containing the hidden message 
–   primers and OTP used for encryption 
Steps performed for the message recovery are: 
–   PCR performing using right primers to amplify the hidden ciphertext 
–   analyses of PCR product by gel electrophoresis or microarray analyses 
–   cleavage of the obtained strand in segments of 10 bases long, using re-
striction enzymes, in order to cut the strand 
–   hybridization between the obtained segments and the original OTP 
–   reading the message: hybridized segments are “1” and ssDNA are “0” 
–   OTP destruction. 
Example 4.13 
The previous algorithm will be exemplified for the message ZOO (except the steps 
requiring biomolecular technologies). 
 
a) Conversion from text ĺ binary 
ZOO ĺ ASCII:  90 79 79 ĺ binary: 10110101100111111001111 – 21 bits = n 

4.9   DNA Cryptography 
199
 
b) Generation of ssDNA OTP: for each binary digit 10 nucleotides, randomly 
selected (eq. using function “randseq (length)” from MatLab Bioinformatics 
Toolbox). The sequence can be generated in DNA, RNA or amino alphabet. 
In our example sequence of 220 bases (> 210 bases) was generated: 
TAAATATGTAACCCCCTGTAGCCAGCTTCCCTCCTCTACTGAGAG
TCGGTAGTCGGCCTAACGTCACTCCTCAGCCTTGTTCAGTAACAGTC
GACCCCTAAGTGTCCCGATCGTCGGGAGTGTATGAGAGAGCAAACT
TGTGATTAGCGCTAGCCCGAGTCCTTGCTTCTCACGCAATCAAGGAA
TTGGTGTGTATATGCACTCGCGGGGTAATAGAGCA 
c) Encryption: for binary “1”s, complementary ssDNA are created and for bi-
nary “0”s neither operation is performed. In our case the encoded stream ( 
corresponding to the 14 “1”s) is: 
TGGGGGACAT, 
GAGGAGATGA, 
CTCTCAGCCA, 
TGCAGTGAGG, AGTCATTGTC, AGCAGCCCTC, ACATACTCTC, 
TCGTTTGAAC, ACTAATCGCG, ATCGGGCTCA, TCCTTAACCA, 
CACATATACG, TGAGCGCCCC, ATTATCTCGT 
 
and the OTP containing the hybridized parts is: 
 
d) Message recovery (decryption). The decryption of the first “101101” bits 
from the encrypted message ZOO is:  
 
TAAATATGTA
ACCCCCTGTA
GCCAGCTTCC CTCCTCTACT
GAGAGTCGGT AGTCGGCCTA ACGTCACTCC
TGGGGGACAT
GAGGAGATGA
CTCTCAGCCA
TGCAGTGAGG
0
0
0
1
1
1
1
 
4.9.4.2   Chromosome DNA Indexing Algorithm 
As shown in 4.9.3.1, OTP can be generated using chromosomes segments, situated 
between two primers, 20 bps long. 

200
4   Cryptography Basics
 
The steps of the algorithm are: 
 
a) Generation of OTP as session key for symmetric encryption, using a se-
quence of DNA chromosomes selected from an available public data base 
[38] 
b) Encryption 
–   The plaintext, if text, is converted in ASCII code (on 8 bits: 0+7 bits), 
and, if image, in 8 bits at least; than the binary stream is converted in 
DNA format , as indicated in 4.6.3.2, meaning that each 8 bit character 
is transformed in 4 letters (A, C, G, T). 
The OTP sequence is analyzed in steps of 4 bases long and compared to the plain-
text in DNA format (Fig. 4.44). 
 
T T C C A A T A G ...
Step 1
Step 2
Step 3
i=1
i=2
i=2
 
Fig. 4.44 Illustration of OTP scanning process for message encryption 
–   If 4-letters sequence, representing a character of the plaintext was re-
trieved in the chromosomal sequence, then the starting point (index in 
chromosome) of identical 4-letters is memorized in an array. For each 4-
letters character an array of indexes in the chromosomal sequence is ob-
tained. The number of indexes allocated to a character depends on the 
number of occurrences of the character in the chromosomal sequence. 
From this index array, a number is selected randomly and will be the 
encrypted character. Thus, the ciphertext is an array of random indexes. 
c) Message recovery (decryption) 
–   The same OTP along with the primers used for encryption are required 
to decrypt the message. The steps at decryption, as for any symmetric 
algorithm are identical with those used for encryption, but made in re-
verse order. 
–   This algorithm is using OTP key, meaning that a key is used only once. 
It is necessary to be able to generate great number of very long keys in 
order to leave a truly OTP. Chromosomes indexing offers a huge variety 
of possible genes and chromosomes itself can be selected from different 
organism, being thus excellent materials for cryptography, offering ran-
domness and non-repeating OTPs. 

4.9   DNA Cryptography 
201
 
–   Transmission of the required OTP chromosome and primers, in fact the 
key management, which is the hardest problem in the symmetric cryp-
tography, becomes much easier. Only the type of the chromosome and 
primers are needed to be known, which, being extremely small in 
length, could be transmitted using classic PKC for example. 
The last two features look to be two great advantages of this algorithm. In fact 
this is not properly a DNA cryptographic algorithm because it is not using to DNA 
medium, but is using the huge randomness that DNA medium is offering. Serious 
studies to compare the classic algorithms towards this new one, will validate or 
not the efficiency for practical applications. 
 
Example 4.14 
Apply chromosome DNA indexing algorithm using Homo sapiens FOSMID clone 
ABC145190700J6 from chromosome X, available in NCBI database [38] for the 
message: secret. 
 
Solution 
a) A fragment from sequence file in FASTA format is: 
 
 
b) Encryption 
Be the plaintext: secret ĺ ASCII: 115 101 99 114 101 116 
 
s ĺ 115 ĺ 01110011 ĺ CTAT ĺ indexes: 
166    258   
789 
927 
1295 
2954  
3045 
 
3098 
3181 
3207 
3361 
3763 
4436 
 
4559 
5242 
5443 
5794 
5938 
5966 
 
7392 
7698 
7762 
7789 
7832   
8128 
8627 
9918 
11871 
12240 
12332 
12383 
 
12581 
13107 
13128 
13324 
14919  
15169 
 
15177 
15494 
15602 
15844 
16073 
16369 
16829 
16891 
16939 
17227 
17342 
17718 
 
17818 
18564 
19530 
20022 
20437 
20619 
 
21145 
21411 
21419 
21725 
22030 
22051 
 
23157 
23180 
23231 
23311 
23367 
23430 

202
4   Cryptography Basics
 
23434 
23556 
23811 
24005 
24038 
24182 
 
24568 
25871 
27176 
27208 
27896 
29321 
 
29642 
29848 
30087 
30097 
30110 
30438 
 
30472 
31090 
31487 
33204 
33226  
33321 
 
33378 
33612 
35520 
35530 
35646 
35768 
For each character was chosen a random index from its array of indexes. Below 
are established positions of random indexes inside character’s arrays: 
115 å 70th index 23811 
101 å 26th index 13981 
99 å 7th index 8011 
114 å 57th index 21195 
101 å 57th index 32741 
116 å 158th index 25264 
Final encrypted message is:  
23811       13981        8011       21195       32741       25264 
 
c) Message decryption 
The key is Homo sapiens FOSMID clone ABC14-50190700J6, from chromo-
some x complete sequence. First we read this sequence using functions from Bio-
informatics Toolbox: 
FASTAData = fastaread('homo_sapiensFosmid_clone.fasta') 
Each index from received encrypted message was used to point in chromoso-
mal sequence: 
SeqNT=FASTAData.Sequence(i:i+3) 
Using these pointers we extracted for each character a 4-bases DNA sequence. 
This variable was transformed in numerical value, using transformation offered by 
Matlab Bioinformatics Toolbox (A-1, C-2, G-3, T-4). As transformation starts 
with 1, at encryption to each digit was added a unit and after that it was trans-
formed in base (example, “00” binary ĺ 0 digit ĺ 0+1 ĺ A). At decryption from 
each obtained digit was subtracted a unit and after that transformed in 2 bits, for 
example: 
CCCA (bases) ĺ 2221(digits) ĺ 2-1, 2-1, 2-1, 1-1 ĺ 1110 (digits) ĺ 01 01 
01 00 (bits) 
Obtained binary numbers are the ASCII cods of the recovered message characters.   
4.9.4.3   DNA XOR OTP Using Tiles 
Based on principle described on 4.9.3.3 and using the idea presented in [30] an 
encryption algorithm is presented in [78]. The steps of the algorithm are: 
 
 
 

4.9   DNA Cryptography 
203
 
a) 
Message tiles binding in a string. 
b) 
XOR operation between message (plaintext) and OTP (encryption key) 
(Fig. 4.45). 
otp 0
otp 1
 
Fig. 4.45 Design of the One Time Pad tiles  
c) 
Encrypted message results: the tiles as result of DNA XOR operation will 
have a pair of sticky ends for binding between them and a pair for binding to en-
cryption key tiles (Fig. 8). 
d) 
Cleavage process to keep the ciphertext only. In order to cut the cipher-
text from the rest of tiles, we propose to use restrictive enzymes. Then the cipher-
text will be stored in a compact form and sent to the destination together with the 
string of labels.  
Since XOR is its own inverse; decryption is made using the same key and op-
erations. The receiver needs an identical pad and use the received string of labels 
in order to extract from this pad the encryption key. After self assembling of the 
same tiles used for encryption, but in reverse order, the plaintext is extracted. 
 
Example 4.15 
 
DNA XOR OTP using tiles algorithm: 
 
a) Message tiles binding in a string 
For the START tiles and the stacks for message binding the selection given 
in Example 4.2 is used. 
b) Microcontroller for tiles binding 
• acknowledgement of the message beginning  
• decision concerning the next bit from the message 
• verification of message ending 

204
4   Cryptography Basics
 
1
0
ACTG
TACC
ACTG
GTCA
otp
otp
TGAC
0
ACTG
GTCA
otp
TGAC
actual
next
next +1
 
Fig. 4.46 Example of message tiles binding  
c) XOR operation between message and OTP 
OTP is a great set of labeled tiles representing 0 and 1. Labeling could be a 
certain order of nucleotides from the inside structure of the tile.  
 
Fig. 4.47 Design of the one-time-pad tiles  
References 
[1] Adleman, L.M.: Molecular computation of solution to combinatorial problems. Sci-
ence 266, 1021–1024 (1994) 
[2] Ambroze, A., Wade, G., Serdean, C., Tomlinson, M., Stander, Y., Borda, M.: Turbo 
code protection of video watermark channel. IEE Proceedings Vision Image, Signal 
Processing 148(1), 54–58 (2001) 
[3] Angheloiu, I., Gyorfi, E., Patriciu, V.: Securitatea si protectia informatiei in sistemele 
electronice de calcul. Editura Militara, Bucuresti (1986) 
[4] Arizona Board of Regents and Center for Image Processing in Education, Gel 
Electrphoresis Notes What is it and how does it work (1999),  
  http://science.exeter.edu 
[5] Bajenescu, T., Borda, M.: Securitatea in informatica si telecomunicatii. Editura Dacia 
(2001) 
[6] Bassia, P., Pitas, I.: Robust audio watermarking in the time domain. In: Proc. Euro-
pean Signal Proc. Conf., Rhodes (1998) 

References 
205
 
[7] Berghel, H., German, L.O.: Protecting Ownership rights through digital watermark-
ing. IEEE Computer 29(7), 101–103 (1996) 
[8] Bojcovics, Z.S., Toma, C.I., Gui, V., Vasiu, R.: Advanced topics in digital image 
compression. Editura Politenhică, Timişoara (1997) 
[9] Boneh, D., Dumworth, C., Lipton, R.: Breaking DES using a molecular computer. In: 
Proceedings of DIMACS workshop on DNA computing (1995) 
[10] Boneh, D., Shaw, J.: Collusion-secure fingerprinting for digital data (963). In: Cop-
persmith, D. (ed.) CRYPTO 1995. LNCS, vol. 963, pp. 452–465. Springer, Heidel-
berg (1995) 
[11] Boney, L., Tewfik, A.H., Hamdy, K.H.: Digital watermarks for audio signals. In: 
Proc. Of 3rd IEEE ICMCS, pp. 473–480 (1996) 
[12] Borda, M.: Collaborative research agreement between the University of Plymouth UK 
and the Technical University of Cluj-Napoca Ro in Video Watemarking, nr. 33, No-
vember 11 (1999) 
[13] Borda, M.: Digital watermaking basics. Acta Technica Napocensis 42(1), 48–59 
(2002) 
[14] Borda, M.: Principles of Watermarking. In: Proceedings of Enelko, Cluj-Napoca, pp. 
31–40 (2003) 
[15] Borda, M., Major, R., Deac, V., Terebes, R.: Raport de faza la contract Contract 
ANSTI 6113/2001. Tema: Sistem de marcare a imaginilor numerice în scopul auten-
tificării proprietăţii intelectuale asupra acestora (2001) 
[16] Borda, M., Major, R., Deac, V., Terebes, T.: Raport de faza la contract Contract 
ANSTI 6113/2002. Tema: Sistem de marcare a imaginilor numerice în scopul auten-
tificării proprietăţii intelectuale asupra acestora (2002) 
[17] Borda, M., Tornea, O.: DNA Secret Writing Techniques. In: Proceedings of 8th Intl. 
Conf. on Communications, pp. 451–456 (2010) 
[18] Borda, M., Tornea, O., Hodorogea, T., Vaida, M.: Encryption system with indexing 
DNA chromosomes cryptographic. In: Proceedings of IASTED, pp. 12–15 (2010) 
[19] Borda, M., Vancea, F., Deac, V.: Raport intern de cercetare. Contract ANSTI 
6113/2000. Tema: Sistem de marcare a imaginilor numerice în scopul autentificării 
proprietăţii intelectuale asupra acestora (2000) 
[20] Brasil, J., Low, S., Maxemchuk, N., O´Gorman, L.: Electronic marking and identifi-
cation techniques to discourage document copying. In: IEEE Infocom 1994, pp. 
1278–1287 (1994) 
[21] Brasil, J., Low, S., Maxemchuk, N., O´Gorman, L.: Electronic marking and identifi-
cation techniques to discourage document copying. IEEE, J. Select. Areas Com-
mun. 13, 1495–1504 (1995) 
[22] Bultzer, P.L., Jansche, S.: Mellin Transform Theory and The Role of its Differential 
and Integral operators. In: Proceedings of Second International Workshop On Trans-
form Methods and Special Functions, Varna, pp. 63–83 (1996) 
[23] Caroni, J.: Assuring ownership rights for digital images VIS 1995. Session Realiable 
IT Systems, Veiweg (1995) 
[24] Cox, I.J., Kilian, J., Leighten, T., Shamoon, T.: Secure spread spectrum watermarking 
for multimedia. Technical Report 95-10, NEC Research Institute, Princeton, NY, 
USA (1995) 
[25] Cox, I.J., Linnartz, J.: Public watermark and resistance to tampering. In: IEEE Int. 
Conf. On Image Processing, vol. 3, pp. 3–6 (1997) 
[26] Cox, I.J., Linnartz, J.: Some general methods for tampering with watermarks. In: 
IEEE Int. Conf. On Image Processing, vol. 16(4), pp. 587–593 (1997) 

206
4   Cryptography Basics
 
[27] Cox, I.J., Miller, M., Bloom, J.: Digital watermarking. In: Principle and practice. 
Academic Press, London (2002) 
[28] Cox, I.J., Miller, M.L., Mckellips, A.L.: Watermarking as Communications with Side 
Information. Proceedings of IEEE 87(7), 1127–1141 (1999) 
[29] Craver, S., Memon, N., Yeo, B.L., Minerva, M.: Resolving Rightful Ownerships with 
Invisible Watermarking Techniques: Limitation, Attacks, and Implications. IEEE 
Journal on Selected Areas in Communications 16(4), 573–586 (1998) 
[30] Gehani, A., LaBean, T.H., Reif, J.H.: DNA-based cryptography. In: Jonoska, N., 
Păun, G., Rozenberg, G. (eds.) Aspects of Molecular Computing. LNCS, vol. 2950, 
pp. 167–188. Springer, Heidelberg (2003) 
[31] Guoan, B.: Fast algorithms for the 2-D discret W transform. Signal Processing 74, 
297–308 (1999) 
[32] Hartung, F., Girod, B.: Watermarking of uncompressed and compressed video. Signal 
Processing 66, 283–301 (1998) 
[33] Hartung, F., Girod, B.: Digital Watermarking of Raw and Compressed Video. Digital 
Compression Technologies and Systems for Video Communications 2952, 205–213 
(1996) 
[34] Hartung, F., Girod, B.: Digital Watermarking of MPEG-2 Coded Video in the Bit-
stream Domain. In: Proceedings of International Conference on Acoustic, Speech and 
Signal Processing, vol. 4, pp. 2985–2988 (1997) 
[35] Hartung, F., Kutter, M.: Multimedia Watermarking Techniques. Proceedings of 
IEEE 87(7), 1079–1106 (1999) 
[36] Hartung, F., Su, J.K., Girod, B.: Spread Spectrum Watermarking: Malicious Attacks 
and Counterattacks. In: Proc. Of SPIE Security and Watermarking of Multimedia 
Contents, vol. 3957, pp. 147–158 (1999) 
[37] Hernandez, J.R., Perez-Gonzales, F.: Statistical Analysis of Watermarking Schemes 
for Copyright Protection of Images. Proceedings of IEEE 87(7), 1142–1166 (1999) 
[38] http://www.ncbi.nlm.nih.gov 
[39] Jayant, N., Johnston, J., Safranek, R.: Signal Compression Based on Models of Hu-
man Perception. Proceedings of IEEE 81(10), 1385–1422 (1993) 
[40] Joseph, J.K., Ruanaaidh, O., Pun, T.: Rotation, scale and translation invariant spread 
spectrum digital image watermarking. Signal Processing 66, 303–317 (1998) 
[41] Kahn, D.: The Codebreakers. McMillan, New York (1967) 
[42] Katzenbeisser, S., Petitcolas, F.: Information hiding techniques for steganography and 
digital watermarking. Artech House, Boston (2000) 
[43] Kim, S.W., Suthaharan, S., Lee, H.K., Rao, K.R.: Image watermarking scheme using 
visual model and BN distribution. Electronics Letters 35(3), 212–214 (1999) 
[44] Kingsbury, N.G.: The Dual-Tree Complex Wavelet Transform: A new Tehnique for 
Shift Invariance and Directional Filters. In: Proc. 8th IEEE DSP Workshop, Bryce 
Canyon (1998) 
[45] Kobayaski, M.: Digital Watermarking: Hystorical roots. IBM Research. Apr. Tokyo 
Res. Lab. Technical Report (1997) 
[46] Langelaar, G.C., Lagendijk, R.L., Biemond, J.: Real time labeling methods for MPEG 
compressed video. In: Proc. 18th Symp. on Information Theory in the Benelux, Vold-
hoven, NL (1997) 
[47] Langelaar, G.C., Setyawan, I., Lagendijk, R.L.: Watermarking Digital Image and 
Video Data. Signal Processing Magazine 20-46 (2000) 
[48] Low, S., Maxemchuk, N.: Performance comparison of two text marking methods. 
IEEE J. Selected Areas Commun. 16, 561–571 (1998) 

References 
207
 
[49] Low, S., Maxemchuk, N., Brasil, J., O´ German, L.: Document marking and identifi-
cation using both line and word shifting. In: Proc. Infocom 1995, Boston (1995) 
[50] Lu, C.S., Huang, S.K., Sze, C.J., Liao, H.Y.M.: A New Watermarking Tehnique for 
Multimedia Protection. In: Guan, L., Kung, S.Y., Larsen, J. (eds.) Multimedia Image 
and Video Processing. CRC Press Inc., Boca Raton (1999) (to appear) 
[51] Lu, C.S., Liao, H.Y.M., Huang, S.K., Sze, C.J.: Cocktail Watermarking on Images. 
In: Pfitzmann, A. (ed.) IH 1999. LNCS, vol. 1768, pp. 333–347. Springer, Heidelberg 
(2000) 
[52] Lu, C.-S., Liao, H.-Y.M., Huang, S.-K., Sze, C.-J.: Highly Robust Image Watermark-
ing Using Complementary Modulations. In: Zheng, Y., Mambo, M. (eds.) ISW 1999. 
LNCS, vol. 1729, pp. 136–153. Springer, Heidelberg (1999) 
[53] Maes, M., Kalker, T., Linnartz, J.P., Talstra, J., Depovere, G.F., Haitsma: Digital wa-
ter-marking for DVD video copy protection. Signal Processing Magazine 17(5), 47–
57 (2000) 
[54] Major, R., Borda, M., Amadou, K.: Hash functions for spatial image watermarking. 
In: International Scientific Conference microCAD, Miskolc, Ungaria, pp. 39–44 
(2003) 
[55] Maxemchuk, N., Low, S.: Marking text documents. In: Proc. IEEE Int. Conf. Image 
Processing, Santa Barbara, pp. 13–16 (1997) 
[56] Menezes, A., van Oorschot, P., Vanstone, S.: Handbook of Applied Cryptography. 
CRC Press, Boca Raton (1996) 
[57] Nafornita, C., Borda, M., Cane, A.: Wavelet-based digital watermarking using sub-
band- adaptative thresholding for still images. In: Proceeding of microCAD, Miskolc, 
Hungary, pp. 87–91 (2004) 
[58] Nikolaidis, N., Pitas, I.: Robust image watermarking in the spatial domain. Signal 
Processing 66, 385–403 (1998) 
[59] Petitcolas, F.A.P.: Watermarking Schemes Evaluation. Signal Processing Magazine, 
58–67 (2000) 
[60] Petitcolas, F.A.P., Anderson, R.J., Kulu, M.G.: Information hiding - A survey. Pro-
ceedings of the IEEE 87(7), 1062–1078 (1999) 
[61] Podlichuk, C.I., Zeng, W.: Image-adaptive watermarking using visual models. IEEE 
J. Select. Area Commun. 16, 525–539 (1998) 
[62] Roweis, S., Winfree, E., Burgoyne, R., et al.: A sticker based architecture for DNA 
computation. In: DIMACS series in discrete mathematics and theoretical computer 
science, vol. 44, pp. 1–30 (1996) 
[63] Schena, M.: Microarray analyses. Wiley-Liss, Chichester (2003) 
[64] Schneier, B.: Applied cryptography: Protocols. In: Algorithms and Source Code in C. 
John Wiley & Sons, Inc., Chichester (1996) 
[65] Shannon, C.: A Mathematical Theory Of Communication. Bell System Technical 
Journal 27, 379–423 (1948); Reprinted in Shannon Collected Papers, IEEE Press 
(1993) 
[66] Shannon, C.: Communication Theory of Secrecy Systems. Bell System Technical 
Journal 28(4), 656–715 (1949) 
[67] Smith, M.: Station X: The Codebreakers of Bletchley Park. Channel 4 Books (2000) 
[68] Stallings, W.: Cryptography and Network security, Principles and Practice, 2nd edn. 
Prentice-Hall, Englewood Cliffs (1998) 
[69] StirMark, http://www.cl.cam.ac.uk/ 
[70] Swanson, M.D., Kobayashi, M., Tewfik, A.H.: Multimedia Data Embedding and Wa-
termarking Technologies. Proceedings of the IEEE 86(6), 1064–1087 (1998) 

208
4   Cryptography Basics
 
[71] Swanson, M.D., Zhu, B., Chau, B., Tewfik, A.H.: Object-Based Transparent Video 
Wa-termarking. In: IEEE First Workshop on Multimedia Signal Processing, pp. 369–
374 (1997) 
[72] Swanson, M.D., Zhu, B., Tewfik, A.H.: Multiresolution scene-based video water-
marking using perceptual models. IEEE J. Select. Area Commun. 16(4), 540–550 
(1998) 
[73] Tanaka, K., Nakamura, Y., Matsui, K.: Embedding secret information into a dithered 
multilevel image. In: Proc. IEEE Military Commun. Conference, pp. 216–220 (1990) 
[74] Taylor, C., Risca, V., Bancroft, C.: Hiding messages in DNA microdots. Nature 399, 
533–534 (1999) 
[75] Tewfik, A.H.: Digital Watermarking. IEEE Signal Processing Magazine 17(5), 17–18 
(2000) 
[76] Tewfik, A.H., Hamdy, K.H.: Digital watermarks for audio signals. In: Proc. 
EUSIPCO 1996, Trieste, Italy (1996) 
[77] Tirkel, A., van Schyndel, R., Osborne, C.: A two-dimensional watermark. In: Proc. 
DICTA, pp. 666–672 (1993) 
[78] Tornea, O., Borda, M.: DNA Cryptographic Algorithms. MediTech Cluj-Napoca 26, 
223–226 (2009) 
[79] U. S. Copyright office Summary, The Digital Millennium Copyright Act of 1998 
(December 1998), http://www.loc.gov/copyright/leg  
[80] Viterbi, A.J.: CDMA-Principles of spread spectrum communications. Addison –
Wesley, London (1996) 
[81] Wade, G.: Watermark Attacks. Research Report, Plymouth (1999) 
[82] Wang, H.: Prooving theorems by pattern recognition. Bell Systems Technical Jour-
nal 40, 1–42 (1961) 
[83] Wang, H.J.M., Su, P.C., Kuo, C.-C.J.: Wavelet-based digital image watermarking. 
Optics Express 3(12), 491–496 (1998) 
[84] Watson, A.T.J.: IBM Research Report. RC 20509 Computer science/ Mathematics 
(1996) 
[85] Wolfgang, R.B., Delp, E.J.: Overview of image security techniques with applications 
in multimedia systems. In: Proc. SPIE Int. Conf. Multimedia Networks: Security, 
Display, Terminals, and Gateways, vol. 3228(4/5), pp. 297–308 (1997) 
[86] Wolfgang, R.B., Podiluck, C.I., Delp, E.J.: Perceptual Watermarks for Digital Images 
and Video. Proceedings of IEEE 87(7), 1108–1126 (1999) 
[87] Wolgang, R.B., Delp, E.J.: Fragile watermarking using the VW2D watermark. Proc. 
Electronic Imaging 3657, 204–213 (1999) 
[88] Xia, X.G., Boncelet, C.G., Arce, G.R.: Wavelet transform based watermark for digital 
images. Optics Express 3(12), 497–511 (1998) 
[89] Borda, M., Tornea, O., Hodorogea, T.: Secrete Writing by DNA Hybridization. Acta. 
Technica Napocensis 50(2), 21–24 (2009) 
[90] Shparlinski, I.: Finite Fields: Theory and Computation. Kluver Academic Publishers, 
Boston (1999) 
[91] Lidl, R., Niederreiter, H., Cohn, P.M.: Encyclopedia of Mathematics and Its Applica-
tions. Cambridge University Press, Cambridge (2000) 
[92] Hankerson, D.C., et al.: Coding Theory and Cryptography, 2nd edn. Marcel Dekker, 
New York (2000) 
[93] Johnson, N.F., Duric, Z., Jajodia, S.: Information Hiding. Kluver Academic Publish-
ers, Boston (2001) 

Chapter 5 
Channel Coding 
Motto: 
The way towards truth is strewn 
with errors. Who does not make 
them, who does not touch them, 
does not fight with them and fi-
nally doesn’t obviate them with 
his own forces, does not reach 
the truth. 
     Constantin Tsatsos 
5.1   Shannon Second Theorem (Noisy Channels Coding 
Theorem) 
When transmitting (storing) information via noisy channels (storage media), the 
noise may alter the signals. This is why protection against this unwanted effect 
must be taken. This problem occurred since the very beginnings of communica-
tions, but it became a huge problem with the development of high speed networks 
and globalisations of digital communications. The advent of electronic era re-
quires high reliability in data transmission and/or storage, taking into account its 
impact in economical and social life. The history of error control coding shows 
how the field was born (mid 20th century) and grown from the one error correct-
ing codes (Hamming codes - 1950) until the most powerful (turbo-codes - 1993), 
trying to limit technically errors effect in applications. 
The answer to the question: how is it possible to achieve error protection when 
transmitting or storing information? was given by Cl. E. Shannon in his work “A 
mathematical theory of communication” (1948) and represents what is now known 
under the name of Shannon second theorem (noisy channels coding theorem). He 
proved that: 
• on any noisy channel of capacity C, a real time transmission can be performed 
(
C
D <
•
) with an error probability P(E) as small as wanted, using uniform codes 
with length n, such that: 
( )
)
D
ne(
2
E
P
•
−
≤
                                            (5.1) 

210 
5   Channel Coding
 
where: 
– 
n is the codewords length (uniform encoding) 
– 
)
D
e(
•
is a positive function of 
•
D , completely determined by channel  
characteristics; it is called error exponent (see Fig. 5.1). 
– 
2 – binary channel 
Remarks 
• the theorem was given under the assumption of binary channel, but it stands for 
any alphabet (>2)  
• the demonstration of (5.1) is found in [41] 
 
Fig. 5.1 Error exponent graphic  
Interpretations of Shannon second theorem 
• the theorem underlines a very surprising fact: no matter how noisy the channel 
is, a transmission with an error probability however small is always possible 
• without giving any algorithms, the theorem shows the ways to obtain P(E) 
however small: 
– 
transmission at low rate: in this case (see Fig. 5.1) if 
)
D
e(
•
 increases, P(E) 
decreases; this method is not used in practice because the channel is not ef-
ficiently used (
C
D <<
•
); 
– 
using large codewords (great n), which means deliberately introducing  
redundancy before transmission; this method is used in practice for error 
protection: redundant codes for error protection - error control codes 
• in real transmission channels (noisy), the error probability P(E) can not be 0, 
because it means n→∞, meaning infinite bandwidths (see 2.8.5 – Interpreta-
tions of Shannon limit).  
A simple example will be given to explain the necessity of introducing redun-
dancy in order to detect and correct errors. 
 
Example 5.1 
 
Let us consider 
}
s;
{s
S
2
1
=
 an equally probable binary source. Considering the 
transmission in a binary symmetric channel with p known, we shall analyze the 
two following cases: 

5.1   Shannon Second Theorem (Noisy Channels Coding Theorem) 
211
 
• Non-redundant coding: it follows that the codewords length is 
1
m
n
=
=
, where 
m designate the number of information symbols, necessary to encode the informa-
tion of the source S and so we have the code 
}
1;0
{
C1 =
. Transmitting over a  
binary symmetric channel BSC, we receive the same symbols {0; 1} without 
knowing if one of them is erroneous; the error probability in this case will be:  
.
p
(E)
P1
=
 
 
• Redundant coding: instead of transmitting one symbol, we transmit 
3
n =
 sym-
bols, introducing 
2
1
3
m
n
k
=
−
=
−
=
 redundant symbols (control symbols), 
the code becoming: 
}
111
;
000
{
C2 =
 
 
Fig. 5.2 Input/output representation of a BSC obtained for C2. 
When transmitting over a BSC we obtained the situation shown in Fig. 5.2. 
From 8 possible output sequences only 2 correspond to the codewards and the re-
maining 6 perform the error detection. If we chose as rule the majority criterion 
(that means we consider reliable sequences those containing two “0”s and a “1” or 
two “1”s and one “0” corresponding to the sequences 000 and 111), the correction 
is also possible. 
Obviously, the decisions taken at the receiver are exposed to a risk that can also 
be evaluated. Even when receiving the codewards 000 and 111 there is a risk that 
all the three bits are erroneous. For independent errors, this risk becomes 
3
3
p
p =
. 
Similarly the probability of two errors in a word of length 3 is: 
p)
-
(1
3p
)
p
-
(1
p
C
=
p
2
2
2
2
n
2
=
. 
The total error probability after decoding becomes in this case: 
2
2
3
2
3
2
2
p
3
p)
2
(3
p
p
p)
(1
p
3
p
p
(E)
P
≅
−
=
+
−
=
+
=
 
The initial risk has decreased from 
 p 
 
(E)
P1
=
to 
2
2
3p
(E)
P
=
. 

212 
5   Channel Coding
 
A second question rises: which is the price paid to obtain an error probability 
however small on the account of the redundancy increase? 
The answer to this question may be obtained reasoning as follows:  
If we want to keep unmodified the information source decision rate 
)
(Di
•
: 
bi
i
T
1
D =
•
                                                 (5.2) 
where Tbi is the information bit duration, than adding k redundant symbols  
(Fig. 5.3) we obtain: 
bi
bc
mT
nT
=
                                            (5.3) 
where Tbe is the coded bit duration. 
 
Fig. 5.3 Illustration of the relations between information bits and coded ones  
From (5.3) we obtain: 
bi
bc
T
n
m
T
=
                                                (5.4) 
The ratio: 
n
m
R =
                                                     (5.5) 
is named the coding rate.  
Equation (5.3) can be written also as: 
•
•
•
>
=
=
i
i
bc
c
D
R
D
T
1
D
                                         (5.6) 

5.2   Error Control Strategies 
213
 
so, encoding and keeping  
ct
Di =
•
, we obtain an increase of the coded decision 
rate 
•
c
D . Knowing that 
•
c
D and the required bandwidth B are directly proportional 
(see relation (2.82)), it results that the price paid for error protection when 
ct
Di =
•
, is an increase in the transmission bandwidth (the storing space) and con-
sequently an increased noise at the receiver. When the bandwidth enhancement is 
not possible, some coding and modulation procedures for bandwidth compression 
(m-ary phase modulations, trellis modulation [42]) are used. 
 
Remark 
The entire discussion in this paragraph has been done under constant power as-
sumption at transmission and without analyzing the modulation system effect and 
that of the input signal waveform.  
In a complete information transmission (storage) system, the channel coding 
(Cc) block is located as shown in Fig 5.4. 
s
C
c
C
c
DC
s
DC
 
Fig. 5.4 Location of channel coding block in a complete transmission (storage) system.  
5.2   Error Control Strategies 
Error control in transmission or storage can be achieved mainly in three ways: 
 
• Error detection: in this case the system requires a return channel through which 
the transmitter is informed about error detection and a repetition is necessary 
(ARQ – automatic repeat request) 
• Forward error correction (FEC) – the error correcting code automatically cor-
rects an amount of errors. The corresponding system is a unidirectional one 
(Fig. 5.4.); this type of transmission is used in storage systems, space commu-
nications etc.  

214 
5   Channel Coding
 
• Error correction and detection (ARQ hybrid systems): the error correcting code 
will correct the erroneous combinations according to its correcting capacity, the 
rest of erroneous combinations being corrected by the ARQ system; these sys-
tems are used in radio transmissions (satellite, mobile communications) 
The most frequently used ARQ procedures are the three ones (Fig. 5.5): 
• Stop and wait (SW) system: the transmitter sends a code sequence to the re-
ceiver and waits for the positive confirmation signal-acknowledge (ACK), 
which means that no errors were detected; when receiving the ACK signal, the 
transmitter sends the next sequence. When receiving a negative confirmation – 
not acknowledge (NAK) corresponding to an erroneous block, the transmitter 
repeats the NAK block until it receives a positive confirmation (ACK). This is 
a half-duplex-type communication. 
This system is extremely simple and it is used in numerous data transmission 
system such as binary symmetrical communication protocol (BISYNC). Even if it 
is extremely simple, this system is inefficient because of the break between the 
transmission and reception of the confirmation. 
• Go-back N blocks system (GBN) corresponds to a continuous blocks transmis-
sion, therefore full duplex communication. The transmitter does not wait for the 
transmitted block confirmation. This confirmation is received after a number of 
N blocks. During this interval the transmitter has already transmitted another 
N-1 blocks. When receiving a NAK, the transmitter returns to the not acknowl-
edged block and repeats this block and another N-1 blocks. 
This system is more efficient then SW system and it is less costly. The ARQ 
system is used in SDLC (Synchronous Data Link Control) and ADCCP (Ad-
vanced Data Communication Control Procedure). The inefficiency occurs because 
of numerous correct blocks that must be repeated. 
• Selective repeat system (SR) corresponds to a continuous transmission (full du-
plex communication) just like GBN system, with the difference that, for the 
former, only the negatively confirmed (NAK) blocks are repeated. This kind of 
system is the most efficient from the three presented, but the most expensive, in 
implementation; it is used in satellite communications.  
An ARQ system makes decoding errors if it accepts received words affected by 
non-detectable errors
en
P
P(E) =
. 
The performance of an ARQ system can be estimated by its total efficiency (η) 
defined as the ratio between the total number of information bits accepted by the 
receiver in time unit and the total number of transmitted bits in a time unit. 

5.2   Error Control Strategies 
215
 
 
Fig. 5.5 ARQ systems for N = 5: a) SW(Stop and Wait); b) GBN(go back N); c) 
SR(selective repeat). 
Remarks 
• all the three ARQ methods have the same P(E), but different efficiencies. 
• expression of ARQ systems efficiency are determined in [28] under an error-
free feedback-channel assumption and in [3] for the case of feedback-channel is 
noisy too.  
 
Comparison between error control strategies  
 
A detailed analysis of this issue is made in [28]. 
Compared to FECs, ARQ systems are much simpler. ARQ systems are adap-
tive, meaning that the information is repeated every time when errors occur. If the 
channel is very noisy, repetitions became very frequent and justify, in this case, 
the use of hybrid ARQ + FEC systems. 

216 
5   Channel Coding
 
5.3   Classification of Error Control Codes 
Classification of error control codes can be done using many criteria; some of 
them are listed bellow: 
 
• according to the nature of the processed information: 
 
– 
block codes: the information at the input is divided into blocks of  m symbols 
to which, by encoding, k control symbols are added resulting an encoded 
block of length n. 
– 
continuous (convolutional) codes: the information is processed continuously 
• according to channel errors type: 
 
– 
codes for independent error control 
– 
codes for burst error  
• according to the control strategy: 
 
– 
error detecting codes (ARQ) 
– 
error correcting codes (FEC) 
• according to the possibility of separating the information symbols from the con-
trol ones: 
 
– 
separable codes, in which the redundant symbols can be separated from 
the information symbols 
– 
non-separable codes (with implicit redundancy) in which the separation is 
not possible, for example the (m, n) codes 
• according to the possibility of having or not, the information and control sym-
bols in clear in a codeword, the separable codes can be classified in: 
 
– 
systematic codes: 
]
[ c i
v =
, where i is the matrix of information symbols 
and c the control symbol matrix 
– 
non-systematic codes: 
]
u
u
[
(n)
(1)…
=
v
 the information and the control 
symbols are not given in clear in the encoded structure. 
5.4   Representation of Binary Code Sequences 
The most frequent representations used for binary code sequences are matrix, vec-
tor, polynomial and geometrical representations. 
Matrix representation is in fact a matrix containing all the code words, with the 
exception of the all zero components. 

5.4   Representation of Binary Code Sequences 
217
 
If M is the number of n length codewords, than the matrix in which the whole 
code is included is: 
M
2
1
v
v
v
C
⇒
⇒
⇒
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
...
a
...
a
a
...
...
...
...
a
...
a
a
a
...
a
a
Mn
M2
M1
2n
22
21
1n
12
11
 
where 
}
1;0
{
aij ∈
 for binary codes.  
The matrix representation allows a compact writing by selecting the lines 
(codewords) that are linearly independent; all the other codewords vi can be ob-
tained as linear combinations of the linearly independent lines. 
Vector representation of a code is based on the fact that the set of all n-length 
words forms a vector space Vn. Each sequence of length n is represented by a  
vector: 
(
)
a
...
a
a
n
2
1
=
v
 
where 
}
1;0
{
ai ∈
 for binary codes. 
The vector space Vn has, in the binary case, 
n
2 code vectors formed by “0”s 
and “1”s. A group of vectors that have a common propriety form a vector sub-
space Vm with 
n
m <
. Such a mathematical model applied for a code allows, 
when studying the codes, to use the vector spaces properties. We remind some ba-
sic notions from the vector space theory used to study linear codes, the most im-
portant codes used in communication systems (see also Appendix A). 
The vector space order is given by the number of linearly independent vectors; 
they form a base of the vector space if any vector which belongs to the space can 
be expressed as a linear combination of linearly independent vectors.  
There is a strong dependency between matrix and vector representation as the 
lines of the matrix can be vectors in a vector space. 
The polynomial representation: a word of length n: 
1
n
0
a
,...,
a
−can be repre-
sented as an 
)
(
1
n−
 or smaller degree polynomial with the unknown x: 
0
1
2
n
2
n
1
n
1
n
a
x
a
...
x
a
x
a
v(x)
+
+
+
+
=
−
−
−
−
 
where 
}
1;0
{
ai ∈
 in the binary codes case; the power of the unknown x is used to 
locate symbol ai in the sequence. 
The polynomial representation allows using, when studying codes, the proprie-
ties of algebraic structures built in polynomials theory (see Appendix A – finite 
fields). 
Geometric representation: every word of length n can be considered as a point, 
which defines the peaks of a geometrical shape in a n-dimensional space. It allows 
using a series of well known proprieties of geometric figures to the codes. 

218 
5   Channel Coding
 
5.5   Parameters of Detecting and Error Correcting Codes 
In what follows we will define parameters that allow us to compare the performances 
of error detecting and correcting codes. 
 
Redundancy 
As it was shown in Shannon second theorem, error protection can be achieved 
only using redundancy. When coding, redundancy can be defined in absolute or 
relative value, for separable and non-separable codes. 
 
• absolute redundancy Ra: for separable codes it represents the number of redun-
dant symbols from a codeword 
]
bits
k[
Ra =
                                              (5.7) 
• relative redundancy Rr : for separable codes it is the ratio between the number 
of redundant symbols k and the length of code word n:  
100[%]
n
k
R
n
k
R
r
r
=
⇔
=
                                   (5.8) 
For non-separable codes (implicit redundancy codes) the two parameters can 
be defined as:  
[
]
[
]
codewords
 
of
 
no.
ld
sequences
length 
-
n 
of
 
no.
 
total
ld
R a
−
=
 
(5.9) 
[
]
sequences
length 
-
n 
of
number 
 
total
ld
R
R
a
r =
                       (5.10) 
Remark 
Equations (5.9) and (5.10) are more general than (5.7) and (5.8), the last two being 
obtained for a particular case of the former equations. The reader is invited to ob-
tained Ra and Rr for separable codes, starting from the equivalent definitions of the 
non-separable ones. 
 
Detection and correction capacity  
 
The ratio of detectable/correctable errors from the total number of possible errors 
is estimated using the detection/correction capacity 
c
d/C
C
defined as: 
sequences
  
erroneus
  
of
number 
 
total
sequences
  
erroneus
  
detectable
 
of
number 
d
C
=
                      (5.11)  
sequences
  
erroneus
  
of
number 
 
total
sequences
  
erroneus
  
e
correctabl
 
of
number 
c
C
=
                    (5.12) 
 

5.5   Parameters of Detecting and Error Correcting Codes 
219
 
Hamming weight and code distance 
• Hamming weight w of a word v is defined by the number of non-zero symbols 
of that word. 
 
Example 
5
)
(
1011011
=
⇒
=
v
v
w
 
• Hamming distance between two word 
iv  and 
j
v  denoted by 
)
,
(
d
j
i v
v
 is de-
fined as the number of positions for which 
iv  and 
j
v  differ: 
(
)
∑
=
⊕
=
n
1
k
kj
ki
j
i
a
a
:
,
d
v
v
                                      (5.13) 
where 
 )
a 
...,
 ,
a ,
(a
 
 
ni
2i
1i
i =
v
, 
 )
a 
...,
 ,
a ,
(a
 
 
nj
2j
1j
j =
v
and 
⊕ is modulo two  
summation. 
 
Example 
 
(1011011)
 
 i =
v
 
 
(0111101)
 
 j =
v
 
The distance between 
iv  and 
j
v  is: 
(
)
(
) (
) (
) (
) (
) (
)
4
0
1
1
0
0
1
1
1
1
1
0
1
1
1
1
1
0
0
1
,
d
j
i
=
+
+
+
+
+
+
=
=
⊕
+
⊕
+
⊕
+
⊕
+
⊕
+
⊕
=
v
v
 
For linear block codes modulo two summation of two binary words 
iv  and 
j
v  
is another word with “1”s on the positions in which 
iv  and 
j
v  differ. 
 
Theorem 
Hamming distance between two words 
iv  and 
j
v   equals the weight of the 
modulo two summation of the given words: 
(
)
(
)
(
)
k
j
i
j
i
w
w
,
d
v
v
v
v
v
=
⊕
=
                                     (5.14) 
For a code C having M codewords the code distance d is defined as the mini-
mum distance between any two codewords: 
(
)
C
,
 ,
,
d
min 
d
j
i
j
i
∈
∀
=
v
v
v
v
                                    (5.15) 

220 
5   Channel Coding
 
According to (5.14) the distance between two codewords equals to another 
codeword weight; therefore it results that the code distance d equals the minimum 
weight of that code: 
min
w
d =
                                                  (5.16) 
Remarks 
• equation (5.16) is very useful in practice due to the fact that it is very easy to 
compute the weights 
• the all zero codewords are not taken into consideration when calculating the 
weights and code distances. 
5.6   Maximum Likelihood Decoding (MLD) 
At the receiver the decoder must decide, based on the received sequence r, which 
was the transmitted sequence and so it estimates the transmission of vˆ  based on the 
reception. If  
v
v =
ˆ
 (code sequence), than there are no decoding errors. If the esti-
mated sequence ˆv  differs from the transmitted one v the decoding is erroneous. The 
probability of having errors after decoding, when r is received, 
)
p(E/r , is: 
)
/
ˆ
p(
:
)
p(E/
r
v
v
r
≠
=
                                        (5.17) 
The decoder error probability can also be expressed as:  
)
p(
)
/
p(
)
p(
r
r
r
∑
Ε
=
Ε
                                     (5.18) 
derived as marginal probability from the joint probability of 
)
(E,r :  
)
p(
)
/
p(
)
,
p(
r
r
r
Ε
=
Ε
 
According to (5.18), we will have a minimum decoding error probability, if 
)
P(E,r  is minimum, p(r) being independent of the coding process. Due to the fact 
that the minimization of p(E/r) is equivalent to the maximization of 
)
/
(
p
r
v
v =
 it 
results that p(E) min can be obtained for 
)
/
(
p
r
v
max; 
)
/
(
p
r
v
 can be expressed, 
according to Bayes formula:  
)
p(
)
)p(
p(
)
p(
r
r/v
v
v/r =
                                          (5.19) 
It results that the maximization of 
)
/
(
p
r
v
 is equivalent to the maximization of 
)
/
(
p
r
v
, in the hypothesis of equally probable codewords. For a memory-less 
channel, each received symbol depends only on the transmitted one; it follows: 
∏
=
=
n
1
i
i
i
)
/v
p(r
)
/
p(
v
r
                                        (5.20) 

5.6   Maximum Likelihood Decoding (MLD) 
221
 
where ri and vi, are the received/transmitted word components. The decoder that 
estimates the received sequence by maximizing (5.20) is called MLD, i.e. maxi-
mum likelihood decoder. 
∏
=
=
n
i 1
i
i
)
/v
p(r
max
)
/
p(
max 
v
r
                                
(5.21) 
where vi are the components of v. 
Due to the fact that (
)
x
log2
 is a monotone increasing function of x, the maxi-
mization of (5.21) is equivalent to the maximization of the logarithm: 
∑
=
i
i
i
2
)
/v
p(r
max
)
/
(
log
max  
v
r
                                 (5.22) 
Remark 
• if the codewords are not equally probable, the MLD decoder is not always op-
timal, in equation (5.20) 
)
/v
p(r
i
i
 being weighted with p(v); in numerous appli-
cation p(v) is not known; this is the reason why, the MLD decoding remains in 
practice the best, so optimal. 
For a BSC for which 
⎩
⎨
⎧
=
−
≠
=
i
i
i
i
i
i
v
r
:
if
   
p,
1
v
r
:
if
   
p,
)
/v
p(r
for an n-length word, the 
equation (5.22) becomes: 
]
p)
-
(1
log
n 
p
-
1
p
log
 )
,
d(
max[
     
          
          
p)}
-
(1
log
 
)]
,
d(
-
[n
p
log
 )
,
max{d(
])
/
p(
max[log
2
2
2
2
2
+
=
=
+
=
v
r
v
r
v
r
v
r
        (5.23) 
Because 
0
p
1
p
log2
<
−
 for 
2
/
1
p <
 and 
p)
-
(1
log
n
2
⋅
 is constant for any v, the 
MLD decoding rule for a BSC estimates vˆ  as that word v that minimizes 
)
,
d(
v
r
and that is why the MLD decoder is also called minimum distance  
decoder. 
])
,
d(
min[
)]
,
p(
[log
max 
2
v
r
v
r
=
                               (5.24) 
Consequently, a MLD decoder determines the distance between r and all the 
possible codewords vi and selects vi for which d(r, vi) is minimum (
M
1,
i =
). If 
this minimum is not unique, choosing between several vi words becomes arbitrary. 
Fig 5.6 shows the minimum distance MLD principle. Let us consider v1 and v2 
as two codewords for which 
5
)
,
d(
=
2
1 v
v
. 

222 
5   Channel Coding
 
1
v
1
v
1
v
1
v
2
v
2
v
2
v
2
v
1r
2r
3r
4r
 
Fig. 5.6 Minimum distance decoder principle (d=5). 
Let us consider four cases a, b, c, and d, corresponding to the reception of the 
words r1, r2, r3, r4 located at distance 1, 2, 3 and 4 from vi. For case a, correspond-
ing to one error in v1 the decoder will decide that r1 comes from v1 due to the fact 
that 
4
)
d(
1
)
d(
=
<
=
2
2
1
1
r,
v
r,
v
. The decoding would have been erroneous if r1 
had come from v2, by setting errors on 4 symbols. For case b, corresponding at 
two errors in v1, the decoder will decide that r2 comes from v1 as 
3
)
,
d(
2
)
,
d(
=
<
=
2
2
2
1
r
v
r
v
; decoded errors occur if r2 had come from v2 being er-
roneous by three symbols. For case c, the decoder will decide that r3 comes from 
v2 by the occurrence of two errors: 
3
)
d(
2
)
d(
=
<
=
3
1
3
2
r,
v
r,
v
. For case d, the 
decoder will decide that r4 comes from v2 under the assumption of one error: 
4
)
d(
1
)
d(
=
<
=
4
1
4
2
r,
v
r,
v
. For the last cases the same discussion is possible as in 
cases a and b. Fig. 5.6 shows that error detection is possible when r1, r2, r3, r4 are 
received corresponding to errors on 1, 2, 3 or 4 positions on v1 or v2. Non-
detectable errors occur when the reception is erroneous on 5 positions. 
This example shows that the code distance is a parameter able to indicate the 
code detection and correction capacity. In [28] it is demonstrated that: 
 
• the necessary and sufficient condition for  a code to correct maximum t errors 
is: 
1
t
2
d
+
≥
                                              (5.25) 
• the necessary and sufficient condition for  a code to detect maximum e errors 
is: 
1
e
d
+
≥
                                              (5.26) 

5.6   Maximum Likelihood Decoding (MLD) 
223
 
• the necessary and sufficient condition for  a code to simultaneously correct 
maximum t errors  and detect  maximum e errors is: 
t
e ,1
e
t
d
>
+
+
≥
                                        (5.27) 
Remark 
• Correction implies detection first; for example, for 
7
d =
we may have: 
 
e (detection) 
t (correction) 
3 
3 
4 
2 
5 
1 
6 
0 
 
• if three errors occur, all of them can be detected and corrected 
• if five errors occur, all of them can be detected but only one of them can be  
corrected. 
The same parameter d can be used to estimate the erasure correction capacity 
[42]. For erasure channels the channel output alphabet has a supplementary sym-
bol y3 called erasure symbol. The moment the decoder decides that a symbol is an 
erasure symbol, even though the correct value is unknown, its location is known, 
subsequently the correction of the erasure errors must be simpler than for other er-
rors (for the general case both the position and value must be known). It can be 
demonstrated that for correcting a number of s erasures the following condition 
must be fulfilled [42]: 
1
s
d
+
≥
                                                  (5.28) 
The condition for simultaneous correction of t errors and s erasures is: 
1
s
2t
d
+
+
≥
                                              (5.29) 
The simultaneous correction of s erasure errors and t errors is performed as fol-
lows. At the begining, the s erased positions are replaced by “0” and then the 
codeword is decoded with standard methods. The next step is to replace the s 
erased positions with “1” and repeat the decoding algorithm. From the two de-
coded words we select the one corresponding to the minimum number of corrected 
errors except for the s erased positions. 
 
Example 5.2 
Let us consider the code: 0000, 0101, 1010, 1111. It can easily be determined that 
2
d =
; according to (5.28) it follows that 
1
s =
 erasure may be corrected. 
Let us transmit the word 0101. At the receiver we assume that an erasure sym-
bol 
x101
=
r
 is detected. 
Replacing x with 0 we obtain r1=0101=v2, therefore 
0
)
,
d(
=
2
1 v
r
. Replacing x 
with 1 we obtain r2=1101 ∈ C; it follows 
1
)
d(
=
2
2 v
,
r
, 
3
)
d(
=
3
2 v
,
r
, 
1
)
d(
=
4
2 v
,
r
2
v
r =
⇒ˆ
. 

224 
5   Channel Coding
 
5.7   Linear Block Codes 
Short time after Shannon gave his second theorem, the first error protection codes 
were invented: the linear block-codes. The main steps in the history of evolution 
of these codes are: 1950 – R. Hamming and M. Golay introduced the systematic 
linear codes, 1957 –1960 – D. Slepian contributed to a unitary theory for linear 
codes; since 1960 many papers studied in detail the linear codes and proposed a 
series of practical codes. 
The binary codes, those having the symbols in GF(2), will be presented in what 
follows [2], [28]. 
5.7.1   Linear Block Codes: Definition and Matrix Description  
As shown in 5.3 - when dealing with block codes - the information originating in a 
binary source is divided into m bits blocks denoted with i (information block); to 
the m information symbols we add k redundant (control) symbols according to a 
certain coding law, thus giving a codeword v of length n; it follows that: 
k
m
n
+
=
                                                  (5.30) 
The number of messages that can be encoded and also the number of code-
words, for such a structure, is: 
m
2
M =
                                                 (5.31) 
From block codes, the linear structures are very important for practical  
applications. 
We call a linear block code (n,m) the n-length code for which the 
m
2
 (for bi-
nary codes) codewords form an m-dimensional subspace C of the n-dimensional 
space Vn that contains all the words of n bits (with coefficients in GF(2)).  
A block code is linear if and only if the modulo 2 summation of two codewords 
is still a code word (see the proprieties of vector spaces – Appendix A8). 
Defining such a structure it results that the n-dimensional vector space Vn con-
taining the set of distinct combinations that can be generated with n bits (
n
2 ) is 
divided into two distinct sub-sets: 
 
• C – the set of codewords, having 
m
2
 elements 
• F – the set of fake words, containing 
m
n
2
2 −
 elements 
Due to the fact that the linear code C(n,m) is an m-dimensional subspace of Vn, 
the entire set of codewords can be generated the linear combinations of the m lin-
ear independent vectors belonging to C. These linear independent codevectors de-
noted with 
m
1,
i , =
i
g
 can be written as a matrix; these vectors form the code 
generating matrix G[mxn]: 

5.7   Linear Block Codes 
225
 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
⇒
⇒
⇒
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
m
2
1
g
g
g
G
...
g
...
g
g
...
...
...
...
g
...
g
g
g
...
g
g
mn
m2
m1
2n
22
21
1n
12
11
                               (5.32) 
where gij ∈GF(2), meaning they are binary symbols. 
Subsequently, the encoding law is given by:  
[
]
n
n
2
2
1
1
m
2
1
...
  
i
...
i
i
g
i
g
i
g
i
g
g
g
iG
v
m
2
1
+
+
+
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
=
                 (5.33) 
To obtain a systematic structure: 
[
]c
i
v =
'
                                                 (5.34.a) 
or 
[
]I
c
v =
"
                                                (5.34.b) 
the generating matrix G must have one of the two canonical forms: 
[
]
P
I
G
m
=
'
                                             (5.32.a) 
[
]
m
I
P
G =
"
                                           (5.32.b) 
In this case the encoding relation, (5.33), becomes: 
[
]
[
]
( )
( )
( )
.
p
i
...
p
i
p
i
i
...
i
i
p
...
p
p
|
1
...
0
0
.
.
.
.
|
.
.
.
.
p
...
p
p
|
0
...
1
0
p
...
p
p
|
0
...
0
1
  
i
...
i
i
'
]
[
'
j
3
3
j
2
2
j
1
1
kxk
i
f
c
m
1
j
jk
j
i
f
c
m
1
j
j2
j
i
f
c
m
1
j
j1
j
m
2
1
        
          
          
          
          
mk
m2
m1
2k
22
21
1k
12
11
m
2
1
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
=
=
=
=
↓
=
=
↓
=
=
↓
=
∑
∑
∑
P
Im
iG
c
i
v
         (5.33.a) 

226 
5   Channel Coding
 
Looking at this equation we notice that the m information symbols are found 
unchanged in the codeword v structure and that the k control symbols (assigned to 
ci) are  linear combinations of information symbols; ∑
=
m
1
j
means modulo-two sum. 
m
1,
j
  ,k
1,
i  
),
i(
f
c
j
i
i
=
=
=
                                       (5.35) 
Equations (5.35) are known as encoding equations or parity-check equations. 
 
Example 5.3 
Let us consider the code C(5,3) with the encoding law: 
 
2
1
1
i
i
c
⊕
=
 
3
2
2
i
i
c
⊕
=
 
 
Taking into consideration the encoding equations, we shall determine the ca-
nonical form of the generating matrix G’ as follows: 
 
i1 
i2 
i3 
c1 
c2 
gi 
1 
0 
0 
1 
0 
g1=v1 
0 
1 
0 
1 
1 
g2=v2 
0 
0 
1 
0 
1 
g3=v3 
 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⇒
1
0
1
0
0
1
1
0
1
0
0
1
0
0
1
G
. 
From the linear combinations of the three linear independent vectors of the 
code gi we obtain the other four non-zero codewords:  
   
1]
  
0
  
0
  
1
  
1 [
=
⊕
=
2
1
4
g
g
v
 
   
1]
  
1
  
1
  
0
  
1 [
=
⊕
=
3
1
5
g
g
v
 
   
0]
  
1
  
1
  
1
  
0 [
=
⊕
=
3
2
6
g
g
v
 
   
0]
  
0
  
1
  
1
  
1 [
=
⊕
⊕
=
3
2
1
7
g
g
g
v
 
From the binary vector spaces properties (Appendix A.8) we know that if we 
consider a space C of dimension m, then always exists a null (orthogonal) space 
C* of dimension 
m
-
n
k =
 such that a codeword v∈C is orthogonal in C*. The 
linear independent k vectors belonging to the null space C* can be put in a matrix 
H[kxn] named control matrix (parity check matrix): 
[
]
n
2
1
h
...
h
h
H
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
kn
k2
k1
2n
22
21
1n
12
11
h
...
h
h
.
...
.
.
h
...
h
h
h
...
h
h
                      
(5.36) 
where 
GF(2)
hij ∈
and   
n
1,
i,i
=
h
 are the control matrix columns. 

5.7   Linear Block Codes 
227
 
Knowing that C and C* are orthogonal spaces, it results that matrices G and H 
are also orthogonal: 
0
HG
GH
T
T
=
=
                                          (5.37)  
Remark 
• Superscript T refers to “transposed matrix”. 
When dealing with systematic linear codes, H matrix also requires the corre-
spondent canonical forms: 
]
I
[P
H
k
T
=
′
                                            (5.37.a) 
]
P
[I
H
T
k
=
′′
                                           (5.37.b) 
The encoding equation (5.33) valid in the space C becomes in C*: 
0
HvT =
                                                 (5.38) 
5.7.2   Error Syndrome 
Let us consider the reception of word r, which is erroneous under the assumption 
of additive noise: 
e
v
r
⊕
=
                                                (5.39) 
where v is the transmitted codeword and e is the error word; ⊕ is the modulo two 
summation and indicates additive errors. 
In the case of matrix representation, e can be written as follows: 
[
]
n
2
1
e
...
e
e
=
e
                                        (5.40) 
where ei is “1” if on the position i appears an error and “0” if no error appears on 
the i position (obviously for binary codes). 
At the receiver, the encoding law is checked:  
S
HrT =
                                                 (5.41) 
where S is the syndrome, in fact a column matrix with k elements. 
Replacing r with (5.39), equation (5.41) can be written as:  
T
T
T
T
He
He
Hv
e
v
H
S
=
⊕
=
⊕
=
)
(
                             (5.42) 
Remarks 
• One may notice that the syndrome does not depend on the transmitted word v, 
but only on the errors e introduced by the channel. 
• No errors or undetectable errors implies 
0
S =
. In the case when v is transformed 
by errors into another codeword, the encoding equation is still fulfilled and the er-
ror cannot be detected. There are a number of 
1
2m − cases of undetectable errors 
corresponding to other codewords than those transmitted over the channel. 

228 
5   Channel Coding
 
• If 
0
S ≠
 the errors are detected and, in the case of ARQ systems, the retrans-
mission is requested. If the goal is to correct errors (FEC) than, in the binary 
codes case, it is necessary to determine the erroneous positions from S. The dis-
tinct and non zero syndromes number (the all zero syndrome corresponds to the 
absence of errors) is 
1
2k −; it follows that from the total number of possible  
errors (
)1
2n −
 only (
)1
2k −
 can be corrected. 
5.7.3   Dimensioning of Error Correcting Linear Block Codes 
Dimensioning a block code means, in fact, to determine the measures m, k and n 
necessary for encoding a source of M messages in order to correct t errors. 
The number of information symbols m is determined by the number of source 
messages M; for binary codes m is found from the following inequality: 
Μ
≥
m
2
                                                      (5.43) 
The necessary condition, however, not sufficient, for generating a t error cor-
recting code is: 
∑
=
≥
t
0
i
i
n
k
C
2
                                               (5.44) 
This inequality is also known as Hamming boundary. 
The sufficient condition, but not necessary, for generating a t error correcting 
code is:  
∑
>
=
1-
2t
0
i
i
1
-
n
k
C
2
,                                            (5.45) 
known as Varshamov-Gilbert boundary. 
For one error correction, the two boundaries lead to the same equation, which is 
the necessary and sufficient condition for generating a one error correcting code: 
n
1
2k
+
≥
                                                (5.46) 
Remark 
Demonstrations of the previous relations are given in [2] and [43],[50]. 
5.7.4   Perfect and almost Perfect Codes  
The perfect (tightly packed/lossless) codes are the ones that fulfil the following 
equation [2], [50]: 
1
2
C
t
1
i
k
i
n
−
=
∑
=
                                                (5.47) 
Perfect codes can correct exactly t errors, but any particular configuration with 
more than t errors results in even more errors. 

5.7   Linear Block Codes 
229
 
There are only few perfect codes; so far known are: Hamming codes with 
1
2
n
k −
=
, binary code with repetition for even n and two Golay codes [2], [50], 
[54]. 
The almost perfect codes correct all the t errors and some 
i
n
C
N ≤
 other com-
binations of t +1 errors, under the assumption that (for binary codes): 
1
2
N
C
k
t
1
i
i
n
−
=
+
∑
=
                                            (5.48) 
The perfect and almost perfect codes have a maximum probability of the  
correct reception, when transmitting over symmetrical channel with independent 
errors. 
5.7.5   Detection and Correction Capacity: Decoded BER 
Let us consider the error detecting linear block code C(n,m). The total number of 
errors 
te
N  is: 
1
2
N
n
e
t
−
=
                                              (5.49)  
and the number of undetectable errors is: 
1
2
N
m
ue
−
=
                                             (5.50)  
Detection capacity, according to (5.11) will be: 
k
n
m
n
m
te
ue
te
ue
te
te
de
d
2
1
2
2
1
1
2
1
2
1
N
N
1
N
N
N
N
N
C
−
−
=
−
≅
−
−
−
=
−
=
−
=
=
          (5.51) 
Similarly, using the definition (5.12) we can calculate the correction capacity 
Cc, with the remark that we must take into account if the code is a perfect or an 
almost perfect one. 
In the case of an error detecting code C(n,m) used in a BSC, the undetectable 
error probability Pu is calculated considering that the error is undetectable if an-
other non-zero codeword is obtained. It follows that [28]: 
i
n
n
1
i
i
u
p)
(1
ǹ
P
−
=
−
∑
=
                                          (5.52) 
where by Ai we denoted the codewords number with the weight i belonging to the 
set C. The numbers A0, …, An form the weights distribution of C. 
 
Remark 
• For a code C(n,m) having the code distance d we get  A0= … = Ad-1= 0. 
In theory we can compute weight distribution for any linear code (n,m) by 
evaluating the 
m
2
codewords. However, for high n and m the calculus becomes 

230 
5   Channel Coding
 
practically impossible; except for some short linear codes the weight distribution 
is not yet determined, so Pu is unknown. 
Even so, it is possible to determine the upper bound of the error probability af-
ter decoding [28] as follows: 
k
n
1
i
n
k
i
n
i
i
n
k
u
2
]
p)
(1
[1
2
p)
(1
p
C
2
P
−
=
−
−
−
≅
∑
−
−
=
−
≤
                    (5.53) 
since 
1
]
p)
(1
[1
n ≤
−
−
. 
Equation (5.53) shows that there exist linear codes (n,m) for which Pu decreases 
exponentially with the number of control bits k. 
 
Remark 
Although if the number of linear block codes is extremely great, only a restrained 
category of codes proved to have Pu satisfying the upper bound 
-k
2
(the Hamming 
codes, for example). 
A t error correcting block code C(n,m), excluding perfect codes, can correct 
numerous combinations of 
1
t +  errors and even more. The number of correctable 
combinations is 
1
2k −. When the transmission is performed over a BSC, a block 
decoding error probability has an upper bound given by [28]: 
∑
+
=
−
−
≤
n
1
t
i
i
n
i
i
n
p)
(1
p
C
P
                                        (5.54) 
In the previous formula, P represents the block erroneous decoding probability. 
For perfect codes, equation (5.54) is fulfilled at limit. In order to determine the af-
ter decoding bit error rate (pd) we start with the definition of pd: 
p
 x 
n
Ȗ
p
x 
n
block
per 
 
bits
  
erroneus
 
of
 
no.
n
 x 
blocks
 
of
 
no.
 
total
block
per 
 
bits
  
erronated
 
no.of
 x 
blocks
  
erroneus
 
no.of
bits
 
ed
 transmitt
of
 
no.
 
total
decoding
after 
 
bits
  
erroneus
no.of
pd
=
=
=
=
=
              (5.55) 
where γ is the number of erroneous bits from an erroneous block of length n. The 
value of Ȗ  depends on channel errors statistics and also on the code. For inde-
pendent errors, and one error correcting codes we have 
3
2
Ȗ
÷
=
 while for 
2
t =
 
errors correcting codes, 
4
3
Ȗ
÷
=
 [47]. 
5.7.6   Relations between the Columns of H Matrix for Error 
Detection and Correction  
We have shown in 5.7.2 that in error detection the syndrome S must be non zero. 
Let us now analyze the implications of this condition on the control matrix struc-
ture. Consider an erroneous word ei with t errors: 

5.7   Linear Block Codes 
231
 
[
]
0
   
...
   
e
   
...
e
...
0
t
1
i
i
i =
e
                                    (5.56) 
where 
1
e j
i, = in the positions on which errors occur. 
In this case equation (5.48) can be written as follows: 
[
]
n
1,
i
0
...
...
...
0
k =
∀
≠
⊕
⊕
⊕
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⋅
=
0,
h
...
h
h
e
e
h
...
h
h
S
t
1
t
i
2
i
i
i
i
n
2
1
        (5.57) 
This means that for t errors detection, the modulo two summation of any t col-
umns of the H matrix must be non zero. 
In particular, for detecting one error 
)1
(t =
, all the control matrix columns 
must be non zero; however, the columns may be equal. 
For correcting maximum t errors we must have distinct syndromes for all the t 
errors possible combinations; in this case equation (5.48) becomes: 
T
j
j
T
i
i
He
S
He
S
=
≠
=
                                      (5.58) 
where ei and ej are two distinct combinations of t errors. 
Expressing H by its columns, we obtain: 
[
]
[
]
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
≠
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
0
...
e
...
e
...
0
 
 
0
...
e
...
e
...
0
 
t
1
2
1
j
j
i
i
n
2
1
n
2
1
h
...
h
h
h
...
h
h
 
or: 
n
1,
j,
i
,
k
k
=
∀
⊕
⊕
⊕
≠
⊕
⊕
⊕
t
2
j1
t
2
1
j
j
i
i
i
i
h
...
h
h
h
...
h
h
         (5.58.a) 
Adding on both sides columns hj1, hj2, ... hjt and reordering the columns we  
obtain: 
n
1,
i
,0
k =
∀
≠
⊕
⊕
⊕
2t
2
1
i
i
i
h
...
h
h
                           (5.58.b) 
Equation (5.58.b) shows that for t errors correction the modulo two summation 
of any 2t columns of the H matrix must be non zero. 

232 
5   Channel Coding
 
For the particular case of one error correcting code, all the columns of the con-
trol matrix must be non zero and distinct from each other. 
5.7.7   Standard Array and Syndrome Decoding 
For linear block codes C(n,m), the codewords number is 
m
2
. No matter what 
word is transmitted, at receiver we can obtain any of the 
n
2 possible combinations 
of Vn. Any decoding algorithm should divide the set Vn into distinct subsets Di, 
each of them containing only one code word vi, 
m
2
i
1
≤
≤
. If the received word r 
is in the subset Di it will be decoded vi. 
This decoding method, which consists in partitioning the received words set Vn 
into distinct subsets Di that contain one codeword, is known as the standard array. 
The standard array represents the operation of partitioning the group Vn after the 
subgroup C by drawing a table with all the 
n
2  received words. The table is filled 
as follows: 
 
• first row (class) will contain all the 
m
2
 codewords starting from the all zero 
word placed in the first left column 
• each row forms a class; the first element from each class placed in the first col-
umn of the table is called the coset leader  
• from the remaining 
n
m
2
2
−
 combinations (not used in the first class) 
1
2k − 
elements are chosen to form the coset leaders of the next classes; we choose 
these elements (denoted with ej, 
k
1,2
j =
) in such a way that the decoding error 
probability is minimized, therefore we chose the error combinations with the 
highest probability of occurence. 
This algorithm is illustrated in table 5.1. 
 
Table 5.1 Standard array for a C(n,m) code. 
 
Coset leader 
e1 = v1= (0,0...,0) 
D2 
v2 
D3 
v3 
... 
Di 
vi 
... 
D2
m 
v2
m 
e2 
e2 ⊕v2 
e2 ⊕v3 
... 
e2 ⊕vi 
… 
e2 ⊕v2
m 
… 
… 
… 
 
… 
 
… 
ej 
ej ⊕v2 
ej ⊕v3 
... 
ej ⊕vi 
… 
ej ⊕v2
m 
… 
… 
… 
 
… 
 
… 
e2
k 
e2
k ⊕v2 
e2
k ⊕v3 
... 
e2
k ⊕vi 
… 
e2
k ⊕v2
m 
 
For a BSC the error words with minimum weight have a maximum probability 
of occurence, so the coset leaders of each class will be chosen from the n-
dimensional combinations which are not codewords and have a minimum weight 

5.7   Linear Block Codes 
233
 
(will be denoted with e2, …, e2
m ). Each class, i.e. the elements of each row will be 
obtained from 
j
i
e
v ⊕
 , 
m
1,2
i =
.  
Both the table and the decoding are based on the minimum distance algorithm 
MLD. Decoding errors occur only if the error combinations are not in the table of 
the coset leaders. 
For a BSC the probability of a decoding error (at word level) is [28]:  
i
n
i
n
0
i
i
p)
(1
p
α
1
P
−
=
−
−
=
∑
                                          (5.59) 
where the numbers αo,α1,…,αv  form the weight distribution of the main elements 
(from the first left column) and p is the BER of a BSC.  
 
Example 5.4 
Let us draw the table corresponding to the standard array for the code C(5,3) ana-
lysed in Example 5.3. The first class (first line) is made up by eight codewords, 
the 
first 
being 
the 
one 
with 
all 
zero 
elements. 
Next 
we 
select  
4
2
2
2
2
2
3
-
5
m
-
n
k
=
=
=
=
possible error combinations with minimum weight. Hav-
ing given e1=(0 0 0 0 0), it results that we must choose another three combinations 
with unit weight: e2=(1 0 0 0 0), e3=(0 1 0 0 0 0) and e4=(0 0 1 0 0). The table cor-
responding to the standard array is given in Table 5.2 
Table 5.2 Standard array for C(5,3) 
00000 
10010 
01011 
00101 
11001 
10111 
01110 
11100 
10000 
00010 
11011 
10101 
01001 
00111 
11110 
01100 
01000 
11010 
00011 
01101 
10001 
11111 
00110 
10100 
00100 
10110 
01111 
00001 
11101 
10011 
01010 
11000 
 
Let us consider the reception of sequence r=01101. Looking in the table we es-
timate r as v4=00101, obtained by the erronation of the second left sided bit. 
One may notice that this code can correct only some combinations from the to-
tal possible one error combinations; this was expected as the code distance of this 
code is 
2
d =
, so it does not fulfil the necessary condition to correct at least one 
error (
3
d =
 for 
1
t = ). 
 
Syndrome decoding (using lookup table) 
 
From the table containing the standard array for a linear code C(n,m) one must no-
tice that all the 
m
2
 elements from the same class have the same syndrome [28], so 
there are 
k
2  different syndromes corresponding to their classes. Therefore a bi-
univocal correspondence exists between the coset leader of a certain class and its 
syndrome; it results that a lookup table may be determined using the 
k
2  leaders 
(corresponding to the correctable error combinations) and their syndromes. This 

234 
5   Channel Coding
 
table can be memorized at receiver. In this case the decoding is performed as fol-
lows (table 5.3): 
 
• compute the syndrome corresponding to r:  
T
Hr
S =
 
• identify the coset leader ej for  which we have the same syndrome with the one 
determined for r; decide that r comes being erroneous by ej 
• decode r as:  
je
r
v
+
=
  
Table 5.3 Syndrome decoding for a linear code C(n,m) 
SI 
ei 
S1 
S2 
… 
S2
m 
e1 
e2 
… 
e2
m 
Example 5.5 
Use the syndrome decoding algorithm to decode the linear code C(5,3) from  
Example 5.3. 
In order to calculate the syndromes, according to (5.47), we need to generate H. 
The generating matrix has already been determined as: 
[
]
P
I
G
3
1
0
1
0
0
1
1
0
1
0
0
1
0
0
1
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
 
The matrix G is already in a canonical form; according to (5.36), H is: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
2
T
1
0
0
1
1
1
0
0
1
1
I
P
H
 
The syndromes corresponding to the three combinations of correctable errors  
e2 = (1 0 0 0 0), e3 = (0 1 0 0 0) and e4 = (0 0 1 0 0) are: 
⎥⎦
⎤
⎢⎣
⎡
= 0
1
1
S
, 
⎥⎦
⎤
⎢⎣
⎡
= 1
1
2
S
, 
⎥⎦
⎤
⎢⎣
⎡
= 1
0
3
S
. 
 
 

5.7   Linear Block Codes 
235
 
The syndrome decoding table is: 
 
Si 
ei 
00 
00000 
10 
10000 
11 
01000 
01 
00100 
5.7.8   Comparison between Linear Error Detecting and 
Correcting Block Codes 
An estimation of code efficiency is performed from the user point of view, the 
user giving the fidelity degree (either by the maximum error probability or by the 
minimum value of the signal/noise ratio SNR) taking into consideration a series of 
variables, such as: cost, transmission speed, complexity etc. 
The detection/correction capacity Cd/Cc defined in 5.5 as well as the BER equa-
tions defined in the previous paragraphs do not take into consideration the trans-
mission parameters: power of the signal PS, channel noise PN, information bit rate 
at the user 
)
(Di
•
. 
An efficient comparison from a practical point of view must take into account 
all these parameters. 
The hypothesis in which we compare the error detecting and correcting codes 
are: 
 
• same power at transmission: 
ct
PS =
 
• a transmission channel with the spectral density power of noise N0 known, as-
sumed to be a BSC with 
1
p <<  
• information bit rate at the receiver equal to the one of the uncoded source: 
ct
Di =
•
 
• receiving and transmission are assumed independent 
Comparison of error correcting codes 
Let us consider two error correcting codes that correct t1, respectively t2 errors. 
Considering perfect codes, equation (5.54) is valid at the limit (the most disadvan-
tageous case):
)
t,
m
,
n
(
C
  
and
  )
t,
m
,
n
(
C
2
2
2
2
1
1
1
1
. 
One question raises: which one of these two codes is better? It is not always 
true that the most redundant code is also the best. We have seen in 5.1 that by re-
dundancy increasing we obtain the receiver bandwidth increasing and subse-
quently power noise extension, therefore the error probability increasing. 
The criterion taken into consideration when estimating which one of the two 
codes is better (also taking into account the transmission parameters) is the correct  
 

236 
5   Channel Coding
 
decision probability for the same quantity of information transmitted over the 
channel. In this case:  
2
2
1
1
m
N
m
N
=
                                    (5.60)  
where N1, respectively N2, are the number of codewords transmitted though C1, C2 
codes.  
The most efficient code will be the one for which the correct transmission 
probability is greater: 
2
1
2
1
N
2
C
C
N
1
)
P
(1
)
P
(1
−
<
>
−
                                       (5.61) 
where P1 and P2 are the erroneous decoding probabilities for codes C1 and C2 and 
are determined with (5.54) at the limit: 
1
t
1
t
n
n
1
t
i
i
n
i
i
n
2
1
t
1
t
n
n
1
t
i
i
n
i
i
n
1
2
2
2
2
2
2
2
1
1
1
1
1
1
1
p
C
p)
(1
p
C
P
p
C
p)
(1
p
C
P
+
+
+
=
−
+
+
+
=
−
≅
−
=
≅
−
=
∑
∑
                          (5.62) 
The approximations in (5.62) stand for a BSC with 
1
p <<  case. 
The fact that the BER in the channel are different for the two codes is due to the 
different bit rates requiring different bandwidths; according to equations (2.80) 
and (5.6) we have: 
i
2
2
c
2
i
1
1
c
1
D
m
n
2
1
D
2
1
B
D
m
n
2
1
D
2
1
B
2
1
•
•
•
•
=
=
=
=
                                     (5.63) 
Relation (5.61) becomes (when 
1
P1,2 << ): 
2
2
C
C
1
1
P
N
1
P
N
1
1
2
−
<
>
−
 
or  
2
2
1
C
C
1
P
m
m
P
1
2
<
>
                                              (5.64) 
 

5.7   Linear Block Codes 
237
 
Replacing in (5.64) P1 and P2 with the expressions from (5.62) we have: 
1
t
1
t
n
1
C
C
1
t
1
t
n
2
2
2
2
1
2
1
1
1
p
C
m
p
C
m
+
+
+
+
<
>
                                  (5.65) 
The BER on the channel depends on the source bit rate (
1
c
D
•
,
2
c
D
•
) and on the 
modulation and decision systems used at receiver [15], [28] [Appendix C]. 
Be p given by the relation (C.40) from Appendix C: 
)
ξ
Q(
p =
                                                (5.66) 
where  ξ  is the SNR: 
B
N
P
P
P
ξ
0
S
N
S =
=
                                           (5.67) 
and Q is the coerror function [Appendix C]. 
∫
∞
−
=
1
2
y
y
2
1
1
dy
e
2p
1
)
Q(y
                                     (5.68) 
Under these circumstances equation (5.56) becomes: 
1
t
2
2
i
0
S
1
t
n
1
C
C
1
t
1
1
i
0
S
1
t
n
2
2
2
2
1
2
1
1
1
n
m
D
N
P
2
Q(
C
m
)
n
m
D
N
P
2
Q(
C
m
+
•
+
+
•
+
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
<
>
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
        (5.69) 
Comparison between error detecting codes 
Judging as in error correcting codes case, we use (5.69) replacing 
1
t
n
1
t
n
2
2
1
1
C
,
C
+
+
 
with N1 and N2 - the number of different sequences that can be received with 
1
t1 +  and, respectively 
1
t2 +  undetectable errors. It is obvious that all sequences 
with t1, t2 error are detected by the two codes. 
 
Coding gain Gc 
In practice, to estimate the code error protection efficiency, is used the parameter 
coding gain Gc, defined as the difference between the SNR necessary for an un-
coded transmission 
nc
ξ
 and the one corresponding to the encoded transmission 
c
ξ  for the same BER at the user (Fig. 5.7). 

238 
5   Channel Coding
 
not coded
encoded
p
)[dB]
/N
ξ(E
0
0
GC
lim
ξ
c
ξ
nc
ξ
 
Fig. 5.7 Coding gain representation 
c
nc
ct
p
c
ξ
ξ
G
−
=
=
                                            (5.70) 
From the graphic of Gc one may notice that there is a certain limit value of the 
SNR(
lim
ξ
)below which the uncoded transmission BER is less than for the coded 
one. One intuitive explanation is that an error protection code is redundant, mean-
ing that for 
ct
Di =
•
, 
c
D
•
 increases; it follows a bandwidth expansion and, subse-
quently a noise increase, therefore a decrease for ξ , having as consequence the 
increase of the error probability (BER). 
Concluding, to have a BER imposed by the application, at the user, we must 
decide upon one of the followings: 
 
• increasing the SNR (increasing the power at transmission, using efficient 
modulation systems) 
• using error protection codes, the price paid being a bandwidth increasing; gen-
erally speaking, this method proves to be more efficient in many applications. 
5.7.9   Hamming Group Codes 
Hamming codes are the first error correcting codes proposed after Shannon second 
theorem, by R. Hamming in 1950.  
 
 

5.7   Linear Block Codes 
239
 
One error correcting (t=1) Hamming group codes (perfect)  
 
The characteristics of this code are: 
 
• code length can be determined by:  
1
2
n
k −
=
                                                  (5.71)  
This equation is, in fact, a perfect code existence condition (5.43) for 
1
t = . 
 
• the code is separable, and has the structure: 
[
]
n
9
8
7
6
5
4
3
2
1
...a
a
c
a
a
a
c
a
c
c
=
v
                                 (5.72) 
where by ai we denoted the information symbols and by ci the control (parity 
check) symbols. 
• control symbols are placed on positions  
1
k
0,
i
   
,
2i
−
=
                                             (5.73) 
• control matrix H is:  
[
]
n
i
1
h
h
h
H
...
...
n]
[k
=
×
                               (5.74) 
where each column hi expresses in binary natural code (BN) its position with the 
less significant bit (LSB) on the k-th line. 
From the control matrix structure one may notice that all the hi columns are dis-
tinct, therefore condition (5.58.b) is fulfilled in the case of one error correcting 
(
1
t = ) codes. 
The encoding relations are determined using (5.38): 
0
T =
Hv
 
It follows that the control symbols are expressed as a linear combination of in-
formation symbols: 
( )
i
i
i
a
f
c =
 
 
Remark 
Relationship (5.38) expresses that even parity is checked (=0). 
Be an error on the i-th position: 
[
]
1
e
  , 
0
...
e
...
0
e
i
i
=
=
  
The syndrome is determined using (5.48):  
[
]
i
n
1
h
h
...
h
S
=
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
0
...
e
...
0
 
i
                                       (5.75) 

240 
5   Channel Coding
 
When only one error occurs, the syndrome indicates the error position in bi-
nary; by a binary-decimal conversion we may determine from S the error word e. 
Error correction is easy: 
v
e
e)
(v
e
r
v
=
⊕
⊕
=
⊕
=
                                       (5.76) 
Let us now see what happens when two errors occur (
2
t =
) on the i-th and j-th 
positions: 
[
]
0
...
...e
e
...
0
j
i
=
e
 
In this case the error syndrome S is: 
[
]
k
j
i
j
i
0
...
e
...
e
...
0
 
h
h
h
h
...
h
S
n
1
=
⊕
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
                              (5.77) 
It may be easily noticed that, if two errors occur, the code introduces a third er-
ror on the k-th position.  
For this code, a block error probability is determined with (5.54) for 
1
t = : 
2
2
n
2
2
n
n
2
i
i
n
i
i
n
p
2
1)
n(n
p)
(1
p
C
p)
(1
p
C
P
⋅
−
≅
−
≅
−
=
−
=
−
∑
                    (5.78) 
We remind to the reader that (5.78) has been determined under the assumption 
that the errors are independent and the channel is a BSC one with p as BER.  
The BER after Hamming decoding is obtained with (5.55): 
P
n
Ȗ
pd
⋅
=
. 
We have seen that for the perfect Hamming code, if two errors (
2
t =
) occur, 
another one is introduced. Therefore, the most probable number of errors is 
3
Ȗ =
, 
such that:  
2
2
d
p
1)
(n
2
3
p
2
1)
n(n
n
3
P
n
3
p
⋅
−
⋅
≅
⋅
−
⋅
=
⋅
=
                       (5.79) 
Modified Hamming Codes 
As we have already seen, the one error correcting Hamming code (the perfect 
code) introduces supplementary errors when overloaded with more errors than its 
correcting capacity (
1
t > ). In order to eliminate this disadvantage and to make it 
more useful in practical applications, this code has been modified by increasing 
the minimum distance from 
3
d =
 to 
4
d =
, allowing one error correction and 
double error detection. This modification is possible either extending or shorten-
ing the initial code. In what follows we shall briefly present both cases.  

5.7   Linear Block Codes 
241
 
Extended Hamming Codes 
 
Increasing the code distance from 3 to 4 is performed adding a supplementary 
control symbol (parity control symbol) c0. The codeword structure becomes: 
[
]
n
9
8
7
6
5
4
3
2
1
0
a
...
 
a
c
a
a
a
c
a
c
c
c
=
∗
v
                               (5.80) 
The control matrix will be: 
⎥⎦
⎤
⎢⎣
⎡
=
∗
1
1
H
0
H
                                               (5.81) 
where H is the Hamming perfect code matrix given by (5.74). 
In this case the syndrome is: 
⎥⎦
⎤
⎢⎣
⎡
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
=
=
∗
∗
2
n
1
0
S
a
.
c
c
1
T
S
1
1
H
0
v
H
S
                                (5.82) 
where S1 keeps its significance from the Hamming perfect code and S2 is a binary 
symbol, “0” or “1”; using S2 we may detect even errors (
0
S2 =
). 
We may have one of the following cases:  
⇒
⎭
⎬
⎫
=
0
=
S2
0
S1
 no errors or the errors are not detectable                 (5.83.a) 
⇒
⎭
⎬
⎫
≠
1
=
S2
0
S1
 one correctable error in the interval 
n
1÷
               (5.83.b) 
⇒
⎭
⎬
⎫
=
1
=
S2
0
S1
co symbol is erroneous                                              (5.83.c) 
⇒
⎭
⎬
⎫
≠
0
=
S2
0
S1
two errors are detected                                             (5.83.d) 
The code distance is 
4
d =
 and it corresponds to one error correction and two 
errors detection:
1
e
t
d
+
+
≥
 (4 = 1+2+1). 
 
Example 5.6 
The odd parity code H(8,4) used in teletext systems is given by the control matrix: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
∗
1
1
1
1
1
1
1
1
0
1
0
1
1
1
0
0
0
0
1
1
0
1
1
0
1
1
1
1
0
0
0
0
H
 

242 
5   Channel Coding
 
The structure of a codeword is: 
[
]
7
6
5
4
3
2
1
0
c
a
a
a
c
a
c
c
=
∗
v
 
Encoding relation, taking into account the odd parity, becomes:  
1
T =
∗
∗v
H
 
Encoding relations are determined from: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⋅
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
1
1
1
1
c
a
a
a
c
a
c
c
1
1
1
1
1
1
1
1
0
1
0
1
1
1
0
0
0
0
1
1
0
1
1
0
1
1
1
1
0
0
0
0
7
6
5
4
3
2
1
0
 
It follows that:  
 
1
a
a
a
c
2
4
5
1
⊕
⊕
⊕
=
 
1
a
a
a
c
2
4
6
3
⊕
⊕
⊕
=
 
1
a
a
a
c
4
5
6
7
⊕
⊕
⊕
=
 
1
c
a
a
a
c
a
c
c
7
6
5
4
3
2
1
0
⊕
⊕
⊕
⊕
⊕
⊕
⊕
=
 
 
The syndrome is determined from the equation: 
1
r
H
S
⊕
=
T
*
*
 
 
Shortened Hamming Codes  
 
The same increase of the code distance from 3 to 4 can be obtained shortening a 
perfect Hamming code. This can be obtained eliminating from the perfect code 
control matrix H all columns having an even number of ones, that is, the columns 
corresponding to the information positions. The new obtained code corrects one 
error and detects two. If only one error occurs during the transmission, the syn-
drome is not 0 and it contains an odd number of ones. Decoding is performed as 
follows [28]: 
 
• 
0
S =
⇒ no errors or undetectable errors 
• 
0
S ≠
 and containing an odd number of ones ⇒ one error; correction is per-
formed adding the syndrome corresponding to the error word to the received 
word (erroneous) 
• 
0
S ≠
 and containing an even number of ones ⇒ two errors are detected. 

5.7   Linear Block Codes 
243
 
Shortened Hamming codes have important applications in computers internal 
memory protection. These codes have been studied and proposed for applications 
in the memory protection field by Hsiao [23]; he also provided an algorithm for 
obtaining the optimal control matrix Ho with the following proprieties: 
• each column has an odd number of “1”s 
• the total number of “1”s is minimum 
• the number of “1”s from each  line in H0 must be equal or as close as possible 
to the average number of ones per line. 
 
Fulfilling these three conditions, an optimal H0 matrix with 
4
d ≥
 and a 
minimum hardware is obtained. 
When dealing with applications in computers field, these codes are encoded 
and decoded in parallel, taking into account that speed is the most critical problem 
of the on-line processing. 
 
Example 5.7 [47] 
For 16 bits memory protection, Hsiao introduced an optimal one error correcting 
and two error detecting (
)
4
d =
code 
)
16
,
22
(
H0
 obtained as follows: 
• start from the perfect 
)
57
,
63
(
H
 code with 
1
2
n
6 −
=
. 
• shorten the code eliminating the 31 columns that contain an even number of 
ones, and obtain the code: 
(32,26)
H
31)
31,57
H(63
s
=
−
−
 
• for 16 bits memory (
)
16
m =
, eliminate ten more columns, complying the op-
timal conditions for obtaining a minimum hardware [23]. 
Finally the obtained shortened Hamming code is H0(22,16): 
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
1
1
1
1
1
1
0
0
0
0
1
0
0
0
1
0
1
0
0
0
0
0
1
1
1
0
0
0
1
1
1
1
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
1
0
1
1
1
0
0
0
0
1
1
1
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
1
1
1
0
1
1
1
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
1
1
1
1
1
0
0
0
0
0
0
1
0
0
0
1
1
1
1
0
0
1
0
0
1
1
0
0
1
0
0
0
0
0
1
0
H
 
Figure 5.8 shows the way to use this code for 16 bits memory protection. 
In a memory writing cycle, 6 parity bits are generated and stored in a check 
memory. 
In a read cycle, new parity bits are generated from the received data and com-
pared to the ones found in the control memory. If errors are detected, the H(22, 
16) block disconnects the central unit (CU). 
 
 

244 
5   Channel Coding
 
 
Fig. 5.8 Shortened Hamming code for 16 bits memory protection 
The data format for the 
)
16
,
22
H(
 code is: 
 
Data 
control bits
Byte 1 
Byte 0 
6 
 
Similarly, for 32 bits memory protection the shortened Hamming code 
)
32
,
39
H(
 [23] is determined. 
 
Example 5.8 
Let us consider the error protection of a digital transmission performed with a one 
error correcting Hamming code with  
4
m =
. 
 
a. Dimension the code, determine the encoding relations using H and G matrices 
and design the encoding and decoding block schemes for a maximal processing 
speed; which is the code distance and how can it be determined? 
b. Is 
]
1101101
[
=
r
 a codeword? If not, determine the corresponding correct word 
assuming only one error in transmission (LSB is the first value from left in the 
word r). 
c. How much is the BER after decoding if the transmission is performed on a 
BSC with 
2
10
p
−
=
? 
d. Repeat point b) assuming standard array and syndrome-based decoding; discussion. 
e. Analyze the possibility of increasing the code distance from 3 to 4; discussion. 
 
Solution 
a)  For 
4
m =
, determine k using equation (5.71):  
 
=
−
=
1
2
n
k
3
k
k
m
=
⇒
+
 
from where it results the 
)
4,7
H(
code. 

5.7   Linear Block Codes 
245
 
According to (5.74) the control matrix has the following expression:   
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
1
0
1
0
1
0
1
1
1
0
0
1
1
0
1
1
1
1
0
0
0
7
6
5
4
3
2
1
H
 
In order to determine the correspondent generating matrix, it is necessary to de-
termine a canonical form of H matrix and then the correspondent canonical form 
for G; then, by rearranging the columns, one can obtain G matrix. 
P
   
          
I
      
1
1
1
0
0
1
1
0
1
1
1
0
 
  
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
1
2
4
7
6
5
3
I
   
          
P
      
1
0
0
0
1
0
0
0
1
 
  
1
0
1
1
1
1
0
1
1
1
1
0
1
2
4
7
6
5
3
4
3
T
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
′
⇒
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
′
G
H
 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1
0
0
1
0
1
1
0
1
0
1
0
0
0
0
0
1
1
0
0
1
0
0
0
0
1
1
1
7
6
5
4
3
2
1
G
 
 
Remark 
The computation accuracy is checked if the two matrices are orthogonal (relation 
(5.37)). We advice the reader to check this result. 
The encoding relations (5.38) and (5.33) are giving the control symbols as 
functions of the information symbols. 
According to (5.72), the codeword structure is the following: 
[
]
7
6
5
4
3
2
1
a
a
a
c
a
c
c
=
v
 
The encoding relations using the control matrix (5.38) are the followings: 
⎪⎩
⎪⎨
⎧
⊕
⊕
=
⊕
⊕
=
⊕
⊕
=
⇒
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
7
5
3
1
7
6
3
2
7
6
5
4
7
6
5
4
3
2
1
a
a
a
c
a
a
a
c
a
a
a
c
0
a
a
a
c
a
c
c
 
1
0
1
0
1
0
1
1
1
0
0
1
1
0
1
1
1
1
0
0
0
 
The encoding relations, using the generating matrix, (5.33), obviously lead to 
the same result: 

246 
5   Channel Coding
 
[
]
[
]
7
6
5
4
3
2
1
7
6
5
3
a
a
a
c
a
c
c
1
0
0
1
0
1
1
0
1
0
1
0
1
0
0
0
1
1
0
0
1
0
0
0
0
1
1
1
 
a
a
a
a
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
 
The code distance can be determined in more ways: 
 
• writing the codewords set C and determining their weights using equation 
(5.16); it is found 
3
w
d
min =
=
; we invite the reader to make this calculus, as 
an exercise.  
• one may notice that all the columns of H matrix are distinct, therefore, accord-
ing to (5.58.b) the code can correct one error; it follows, according to (5.25): 
3
1
1
2
d
=
+
⋅
=
 
The decoding equation (5.47) becomes: 
T
Hr
S =
; from which it follows: 
⎪⎩
⎪⎨
⎧
=
⊕
⊕
⊕
=
⊕
⊕
⊕
=
⊕
⊕
⊕
⇒
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⋅
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
3
7
5
3
1
2
7
6
3
2
1
7
6
5
4
3
2
1
7
6
5
4
3
2
1
s
r
r
r
r
s
r
r
r
r
s
r
r
r
r
s
s
s
r
r
r
r
r
r
r
1
0
1
0
1
0
1
1
1
0
0
1
1
0
1
1
1
1
0
0
0
 
The previously determined encoding and decoding equations lead to the block 
schemes of the encoding/decoding units (Fig 5.9). 
According to (5.76) the error correction consists in:  
e
r
v
⊕
=
 
where e is determined from S by a binary-natural to decimal conversion. 
For Hamming code, the correction is performed when r has been completely 
received (on a fraction from the duration of the n-th symbol), the seven modulo 
two adders being validated and allowing the writing of the correct word in the 
memory register (MR): 
e
r
v
⊕
=
. 
b)  To check whether r is or not a codeword, check the equation (5.47): 
S
Hr
=
T
 
5
1
0
1
1
0
1
1
0
1
1
 
1
0
1
0
1
0
1
1
1
0
0
1
1
0
1
1
1
1
0
0
0
h
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
 

5.7   Linear Block Codes 
247
 
Decoding h5  one can determine the error word:  
]
0 0 1 0 0 0 0 0
[
=
e
 
Correction consists in: 
] 1 0
  
0
  
1
  
0
  
1
  
1[
] 0
  
0
  
1
  
0
  
0
  
0
  
0
[
] 1
  
0
  
1
  
1
  
0
  
1
  
1[
=
⊕
=
⊕
=
e
r
v
 
1
c
2
c
4
c
7
a
6
a
5
a
3
a
 
1r
1r
2r
2r
3r
3r
4r
4r
5r
5r
6r
6r
7r
7r
1
s
2
s
3
s
1
c
2
c
3
a
4
c
5
a
6
a
7
a
1
e
2
e
3
e
4
e
5
e
6
e
7
e
 
Fig. 5.9 Hamming (7,4) code block scheme: a) encoding unit; b) decoding unit 

248 
5   Channel Coding
 
c) pd is determined with equation (5.79): 
3
4
2
2
d
10
10
9
6p
1,5
1)p
(n
2
3
p
−
−≅
⋅
=
⋅
=
−
≅
,  
so it decreased approximately 10 times, compared to the situation of not using the 
error correction code. 
d) For our code, the codewords set is formed by 
16
2m =
 combinations of 7 bits.  
 
[0 0 0 0 0 0 0], [1 1 0 1 0 0 1], [0 1 0 1 0 1 0], [1 0 0 0 0 1 1],  
[1 0 0 1 1 0 0], [0 1 0 0 1 0 1], [1 1 0 0 1 1 0], [0 0 0 1 1 1 1],  
[1 1 1 0 0 0 0], [0 0 1 1 0 0 1], [1 0 1 1 0 1 0], [0 1 1 0 0 1 1],  
[0 1 1 1 1 0 0], [1 0 1 0 1 0 1], [0 0 1 0 1 1 0], [1 1 1 1 1 1 1]. 
 
In order to form the standard array we choose from the rest of  
112
2
2
2
2
4
7
m
n
=
−
=
−
 combinations on 7 bits, a number of 
8
2k =
 combina-
tions of minimum weight except the combination with all zero elements; we shall 
choose the 7 combinations with unitary weight (see Table 5.4). 
The sequence 
]
1101101
[
=
r
 is identified at the intersection of line 6 with col-
umn 2, so the decoded word is 
1 0 0 1 0 1 1
. 
 
Remark 
Due to the fact that the table contains all possible combinations corresponding to 
one error, any singular error can be corrected. 
For syndrome-based decoding, we fill Table 5.5. 
Table 5.5 Syndrome-decoding table for Hamming (7,4) code. 
ST 
e 
0 0 0 
0 0 0 0 0 0 0 
0 0 1 
1 0 0 0 0 0 0 
0 1 0 
0 1 0 0 0 0 0 
0 1 1 
0 0 1 0 0 0 0 
1 0 0 
0 0 0 1 0 0 0 
1 0 1 
0 0 0 0 1 0 0 
1 1 0 
0 0 0 0 0 1 0 
1 1 1 
0 0 0 0 0 0 1 
 
One may notice from the previous table that for 
]1
0
1[
T =
S
 the error word is 
0]
 0 1 0 0 0 
[0
e =
. The correction is done as follows: 
] 1 0
  
0
  
1
  
0
  
1
  
1[
] 0
  
0
  
1
  
0
  
0
  
0
  
0
[
] 1
  
0
  
1
  
1
  
0
  
1
  
1[
=
⊕
=
⊕
=
e
r
v
 
e).  
Increasing the code distance from 3 to 4 can be done in two ways: ex-
tending or shortening the code. 

5.7   Linear Block Codes 
249
 
 
0000001
0000010
0000100
0001000
0010000
0100000
1000000
0000000
1101000
1101011
1101101
1100001
1111001
1001001
0101001
1101001
0101011
0101000
0101110
0100010
0111010
0001010
1101010
0101010
1000010
1000001
1000111
1001011
1010011
1100011
0000011
1000011
1001101
1001110
1001000
1000100
1011100
1101100
0001100
1001100
0100100
0100111
0100001
0101101
0110101
0000101
1100101
0100101
1100111
1100100
1100010
1101110
1110110
1000110
0100110
1100110
0101110
0101101
0101011
0100111
0111111
0001111
1101111
0101111
1110001
1110010
1110100
1111000
1100000
1010000
0110000
1110000
0011000
0011011
0011101
0010001
0001001
0111001
1011001
0011001
1011011
1011000
1011110
1010010
1001010
1111010
0011010
1011010
0110010
0110001
0110111
0111011
0100011
0010011
1110011
0110011
0111101
0111110
0111000
0110100
0101100
0011100
1111100
0111100
1010100
1010111
1010001
1011101
1000101
1110101
0010101
1010101
0010111
0010100
0010010
0011110
0000110
0110110
1010110
0010110
1111110
1111101
1111011
1110111
1101111
1011111
0111111
1111111
Table 5.4 Standard array for the Hamming (7,4) code. 

250 
5   Channel Coding
 
The H(7,4)  code extension is obtained using matrix 
*
H  given by (5.81) and a 
structure codeword (5.80). 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1
1
1
1
1
1
1
1
1
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
1
1
1
1
0
0
0
0
*
H
,  
[
]
7
6
5
4
3
2
1
0
*
a
a
a
c
a
c
c
c
=
v
 
The encoding relations for c1, c2, c4 are identical to the basic H(7.4) code, along 
with:  
7
6
5
4
3
2
1
0
a
a
a
c
a
c
c
c
⊕
⊕
⊕
⊕
⊕
⊕
=
 
Let us consider the following sequences are received: 
 
1
  
0
  
0
  
1
  
0
  
1
  
1
  
0
=
1r
   
1
  
0
  
0
  
1
  
0
  
1
  
0
  
0
=
2r
 
1
  
0
  
0
  
1
  
0
  
1
  
1
  
1
=
3r
  
1
  
0
  
0
  
1
  
0
  
0
  
0
  
0
=
4r
 
 
Determine, if possible, the correct sequences. 
1
1
*
0
...
0
0
0
r
r
H
⇒
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
is a codeword, meaning that no errors or the errors are 
not detectable. 
1
S
  ,
1
0
0
1
...
1
0
0
2
1
2
*
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⇒
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
S
r
H
, it follows that there is a correctable er-
ror on the first position.  
It results that: 
]1
  
0
  
0
  
1
  
0
  
1
  
1
  
0
[
]
0
  
0
  
0
  
0
  
0
  
0
  
1
  
0
[
2
=
⊕
= r
v
 
erroneous.
 
is
 
c 
ore
    theref
1
S
   
,0
1
...
0
0
0
0
2
1
3
*
=
=
⇒
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
S
r
H
 
 

5.7   Linear Block Codes 
251
 
It results: 
]1
  
0
  
0
  
1
  
0
  
1
  
1
  
0
[
]
0
  
0
  
0
  
0
  
0
  
0
  
0
  
1[
3
=
⊕
= r
v
 
 
,0
S
  ,
1
1
0
0
...
1
1
0
2
1
4
*
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⇒
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
S
r
H
 it results that there are two uncorrect-
able errors. However, the even errors have been detected. 
 
Code shortening 
 
It is well known that shortening a basic H(n,m) code is performed by reducing the 
information symbols number, so in our case, where m is fixed (4) we cannot start 
from the H(7,4) code but from the perfect 
)
11
,
15
(
H
code that follows immediately 
after: 
 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
H
 
                     ×         ×    ×              ×    ×         ×              × 
 
Eliminating the columns that contain an even number of ones we obtain:  
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
0
1
1
0
1
0
0
1
1
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
1
1
1
1
0
0
0
0
0
H
 
One may notice that this matrix has dimensions compatible to the data format 
(
4
m =
)and ensures 
4
d =
requirement. In this case the structure of the codeword 
is: 
[
]
8
7
6
5
4
3
2
1
0
a
a
a
c
a
c
c
c
=
v
 
 
 
 
 
 

252 
5   Channel Coding
 
The encoding relations can be determined with: 
0
0
v
H
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⋅
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
⇔
=
8
7
6
5
4
3
2
1
T
0
0
a
a
a
c
a
c
c
c
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
 
⎪
⎪
⎩
⎪⎪
⎨
⎧
⊕
⊕
=
⊕
⊕
=
⊕
⊕
=
⊕
⊕
=
⇒
7
6
4
1
8
6
4
2
8
7
4
3
8
7
6
5
a
a
a
c
a
a
a
c
a
a
a
c
a
a
a
c
 
 
Let us consider that the following sequences are received: 
 
0]
  
0
  
0
  
0
  
1
  
1
  
1
  
1[
1 =
r
 
0]
  
0
  
0
  
0
  
1
  
1
  
1
  
0
[
2 =
r
 
0]
  
0
  
0
  
0
  
1
  
1
  
0
  
0
[
3 =
r
 
 
An estimation of the received sequences can be done checking the following 
equation: 
 
S
r
H
=
T
0
 
 
 
0
...
0
0
0
T
1
0
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
r
H
, it follows that r1 is either correct or the errors can not be 
detected 
0)
S 
0,
(
2
1
=
=
S
. 
 
1
...
0
0
0
T
2
0
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
r
H
, it results that there is one error 
) 1 
 
S 
0,
 
 
(
2
1
=
=
S
. 
 
 

5.7   Linear Block Codes 
253
 
The syndrome value shows that it is located on the position c1, so the correct 
word is: 
 
]
11110000
[
]
10000000
[
2
=
⊕
= r
v
 
 
1
...
1
0
0
T
3
0
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
r
H
, the errors are double and can not be corrected 
) 1 
 
S 
0,
 
 
(
2
1
=
≠
S
, however the errors have been detected.  
5.7.10   Some Other Linear Block Codes 
Simple parity check codes 
 
These codes are among the most used error detection codes in data transmission 
systems; a simple parity code adds one supplementary bit (parity check bit) to the 
information bits of the code word. It follows that:  
1
m
n
+
=
                                                    (5.84) 
The parity check bit is determined depending on the used parity criterion: 
 
Odd parity criterion: 
1
a
n
1
i
i =
∑
=
                                                   (5.85) 
Even parity criterion: 
0
a
n
1
i
i =
∑
=
                                                 (5.86) 
Remark 
 
summation
 2 
modulo
 
 the
g
designatin
 
is
 - 
n
1
i
∑
=
 
 
This code can detect all the odd errors. The code relative redundancy is: 
n
1
n
k
R
=
=
 
Using the two parity criteria, determine the parity bit for the following se-
quence: 1011011. 
Using the odd parity criterion: 
1
c
1
1
0
1
1
0
1
=
⊕
⊕
⊕
⊕
⊕
⊕
⊕
; it results that 
0
c =
. 

254 
5   Channel Coding
 
Using the even parity criterion: 
1
c
1
1
0
1
1
0
1
=
⊕
⊕
⊕
⊕
⊕
⊕
⊕
; it results that 
1
c = . 
The codewords will be 10110110]
[
 and 10110111]
[
. 
Cross parity check code 
 
It is obtained iterating two simple parity check error detecting codes. 
The information received from the transmitter is arranged in blocks of informa-
tion sequences: a parity check bit is associated to each row and to each column 
(Ri), resulting a transversal parity check word (Table 5.6). 
 
Table 5.6 Encoding table for the cross parity check code. 
 
R1 
R2 
… 
Rm 
Cio 
v1 
i11 
i12 
… 
i1m 
C1o 
v2 
i21 
i22 
… 
i2m 
C2o 
 
… 
… 
… 
… 
… 
 
 
 
 
 
 
vp 
ip1 
ip2 
… 
ipm 
Cpo 
Cjv 
C1v 
C2v 
… 
Cmv 
 
 
The receiver checks the parity for each received word and then builds a trans-
versal parity check word of its own, which is compared to the received word. If 
the two words are identical and if the horizontal parity is checked for each word, 
the block is validated. 
This code can correct any singular error occurred inside the block, due to the 
fact that this error affects both the horizontal and vertical parity check. There are 
some cases in which the horizontal and vertical errors can be detected.  A detailed 
analysis of these particular cases is performed in [24]. This code was used for in-
formation protection in recording the information on magnetic tape or paper. 
 
Example 5.9 
Let us consider the block: 
 
 
R1 
R2 
R3 
R4 
R5 
R6 
R7 
Cio 
v1 
1 
0 
1 
1 
0 
0 
1 
1 
v2 
0 
1 
1 
0 
1 
1 
0 
1 
v3 
1 
1 
0 
0 
1 
0 
0 0⇒1
v4 
0 
0 
1 
0 
1 
0 
1 
0 
v5 
0 
1 
1 
0 
0 
1 
1 
1 
Ckv 
1 
0 
1 0⇒1 0 
1 
0 
1 
 
The odd parity criterion was used for encoding. We assume that an error occurs 
on position (v3, R4). It will affect C30 and C4v, therefore it will be corrected. 

5.8   Cyclic Codes 
255
 
Constant weight code (m, n) 
 
Unlike all the codes we have already studied, the constant weight code is a non 
separable code, i.e. with implicit redundancy. The length of the code words is n. 
The encoding law stipulates that all the words have the same weight 
m
w =
, al-
lowing the all odd errors detection. This code is used in information alpha numeric 
representations, for example code (3, 7) in telex. 
We will determine the redundancy and detection capacity taking into account 
relation (5.10): 
n
ldC
n
ld2
ldC
ld2
R
m
n
n
m
n
n
r
−
=
−
=
                              (5.87) 
te
ue
te
ue
te
d
N
N
1
N
N
N
C
−
=
−
=
 
where 
1
C
N
m
n
ue
−
=
 represents the number of undetectable erroneous combina-
tions and 
1
2
N
n
te
−
=
 is the total number of possible erroneous words. The total 
codewords number is 
m
n
C
M =
. In this case Cd is expressed as: 
1
2
1
C
1
C
n
m
n
d
−
−
−
=
                                         (5.88) 
 
Example 5.10 
Let us consider the code C(2, 5): 11000, 00011, 00110, 01010, 01100, 10001, 
10010, 10100. It follows that maximum ten messages can be encoded 
10)
C
(M
2
5 =
=
. 
The redundancy and code detection capacity are computed using (5.87) and 
(5.88). 
 
%
6,
33
336
,0
5
32
,3
5
5
10
5
=
=
−
=
−
=
ld
Rr
 
70%
0,7
31
22
1
2
1
10
1
C
5
d
=
=
=
−
−
−
=
. 
5.8   Cyclic Codes 
Cyclic codes are an important sub-set of linear block codes. They are very used in 
practice due to the simplicity in implementation using linear feedback shift regis-
ters (LFSR); the algebraic structure of these codes made possible a lot of practical 
decoding algorithms. 
From the history of this codes we mention: 1957- E. Prange is the first to study 
the cyclic codes, 1959 – A. Hocquenghem and 1960 – R. Bose and P. Chauduri 

256 
5   Channel Coding
 
are giving multiple errors correcting cyclic codes also known as BCH (Bose-
Chauduri- Hocquenghem) codes, 1960 – J. Reed and G. Solomon give the cyclic 
non-binary codes known as RS (Reed-Solomon) codes. W. Peterson wrote a 
monography about cyclic codes in 1961. Other important contributions in devel-
oping these codes belong to: G. Fornay, Y. Massey, R. Blahut, E. Berlekamp, T. 
Kasammi, L. Chen, S. Lin. 
5.8.1   Definition and Representation 
A linear block code 
)
,
(
m
n
C
 is cyclic if any cyclic permutation of a codeword is 
also a codeword. 
If: 
C
)
a
...
a
(a
n
1
0
∈
=
v
 
then any cyclic permutation of v is still a codeword:  
C
)
a
...
a
a
a
...
a
(a
...
C
)
a
...
a
(a
1
i-
n
1
0
1-
n
1
i-
n
i-
n
i)
(
1-
n
0
1-
n
)1(
∈
=
∈
=
+
+
v
v
 
For cyclic codes the words are represented by polynomials; for an n symbols 
sequence the corresponding polynomial is of degree 
1
-
n
or smaller: 
( )
1-
n
1-
n
2
2
1
0
x
a
...
x
a
x
a
a
x
v
+
+
+
=
                              (5.89)  
The encoding relation, giving the codewords set C, is to form codewords, poly-
nomially represented as multiples of k-th degree polynomial, known as the gen-
erator polynomial.  
From the cyclic codes definition we get that for any information polynomial 
i(x) of degree 
1
-
m
, the codewords are chosen multiples of a 
m
-
n
k =
 degree 
polynomial known as the generator polynomial of the code.  
( )
1-
m
1-
m
1
0
x
i
...
x
i
i
x
i
+
+
+
=
                                  (5.90) 
( )
1
g
g
,
x
g
...
x
g
g
x
g
0
k
k
k
1
0
=
=
+
+
+
=
                        (5.91)  
( )
i(x)g(x)
x
v
=
                                           (5.92) 
All distinct combinations set that can be formed with n symbols (codewords) 
forms an algebra. This set comprises the set of the residue classes of an n degree 
polynomial c(x) with coefficients in GF(2). 
The polynomial c(x) of degree n can be chosen randomly, but for obtaining a 
similar expression with the scalar product between two vectors [43], [53] it is cho-
sen as follows: 
0
1
x
c(x)
n
=
+
=
                                         (5.93) 

5.8   Cyclic Codes 
257
 
For binary codes the set residue classes of 
0
1
x
c(x)
n
=
+
=
 has 
n
2 elements. 
From this algebra we can choose 
m
2
elements subset (the codewords set), that are 
multiples of the generator polynomial (the ideal generated by g(x)). 
Due to the fact that the zero element belongs to the ideal, it results that exists a 
polynomial h(x) such that 
0
1
x
c(x)
h(x)
g(x)
n
=
+
=
=
⋅
                                 (5.94) 
It follows that ( )
x
g
 is chosen from the divisors of 
0
1
x
c(x)
n
=
+
=
, hence: 
g(x)
1
x
h(x)
n +
=
                                             (5.95) 
5.8.2   Algebraic Encoding of Cyclic Codes 
The codeword formed with relation (5.92) leads to a non-systematic cyclic code 
(the information is modified in the encoded structure)  
Relation (5.92) can be rewritten as follows: 
g(x)
x
i
...
xg(x)
i
g(x)
i
        
g(x)
)
x
i
...
x
i
(i
g(x)
i(x)
v(x)
1
m
1-
m
1
0
1-
m
1-
m
1
0
−
+
+
+
=
=
×
+
+
+
=
×
=
                   (5.96) 
Equation (5.96) shows that v(x) is formed by the linear combinations set of the 
generator G matrix line vectors, where: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
−
−
k
1
0
k
1
k
0
k
1
0
1
m
[mxn]
g
g
g
0
0
0
g
g
g
0
0
0
g
g
g
(x)
x
(x)
x
(x)
"
"
#
#
#
#
#
#
#
"
"
"
"
#
g
g
g
G
             (5.97) 
In order to obtain a systematic structure, i.e. the information to be unmodified 
on the most significant positions, we follow the next steps: 
 
1. 
( )
1-
n
1-
m
1
k
1
k
0
k
x
i
...
x
i
x
i
x
i
x
+
+
+
=
+
 
 
2. 
g(x)
r(x)
q(x)
g(x)
i(x)
xk
+
=
  
3.   
1
n
1
n
k
k
1
k
1
k
1
0
k
x
a
...
x
a
x
a
...
x
a
a
        
q(x)g(x)
r(x)
i(x)
x
v(x)
−
−
−
−
+
+
+
+
+
+
=
=
=
+
=
                           
(5.98) 
 

258 
5   Channel Coding
 
Therefore we obtain a cyclic codeword, multiple of g(x), with the information 
symbols placed on the most significant m positions: 
i(x)
xk
 and k control symbols 
given by r(x), the remainder after the division of  
i(x)
xk
 to ( )
x
g
, a polynomial of 
maximum degree 
1
k−. 
The encoding relationship (5.98) used to obtain a systematic cyclic code can be 
rewritten in such a way to obtain an equation similar to 
0
T =
Hv
. Equation (5.98) 
is multiplied with h(x) and due to the fact that h(x) and g(x) are orthogonal (rela-
tionship (5.94)) we get: 
0
x)
q(x)g(x)h(
v(x)h(x)
=
=
                                 (5.99) 
The product 
0
v(x)h(x) =
 can be written as the scalar product between two  
vectors: 
⎪
⎪
⎩
⎪⎪
⎨
⎧
=
=
=
−
−
−
−
−
−
−
0
0)
0
h
h
(h
)
a
a
(a
0
0)
h
h
h
h
h
0
(0
)
a
a
a(
  
          
0
)
h
h
h
h
0
0
(0
)
a
a
(a
0
1
m
m
1
n
1
0
0
1
2
m
1
m
m
1
n
1
0
0
1
1
m
m
1
n
1
0
"
"
"
#
"
"
"
"
"
"
            (5.100) 
Remark 
From the n equations given by the cyclic permutations we have written only the 
m
-
n
k =
 linear independent ones. 
The system (5.100) can be written also: 
0
a
a
a
0
0
0
0
h
h
h
0
h
h
h
h
0
0
h
h
h
h
0
0
0
1
n
1
0
0
1
m
m
0
2
m
1
m
m
0
1
1
m
m
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
⋅
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
−
−
−
−
−
#
"
"
#
#
#
#
#
#
#
#
#
"
"
"
"
       (5.101) 
One may easily notice that this system is of 
0
T =
Hv
 type, where H is identi-
fied as:     
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
−
−
−
−
×
0
0
0
0
h
h
h
0
h
h
h
h
0
0
h
h
h
h
0
0
0
0
1
m
m
0
2
m
1
m
m
0
1
1
m
m
n]
[k
"
"
#
#
#
#
#
#
#
#
#
"
"
"
"
H
          (5.102)  
 
Example 5.11 
Let us consider code C(7,4) with the generator polynomial 
1
x
x
g(x)
3
+
+
=
. 
 
 

5.8   Cyclic Codes 
259
 
According to (5.96), the generator matrix of this code is: 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1
0
1
1
0
0
0
0
1
0
1
1
0
0
0
0
1
0
1
1
0
0
0
0
1
0
1
1
G
 
the polynomial h(x) is determined from: 
1
x
x
x
g(x)
1
x
h(x)
2
4
7
+
+
+
=
+
=
 
where, according to (5.102), it results the control matrix structure. 
   
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
0
0
1
1
1
0
1
0
1
1
1
0
1
0
1
1
1
0
1
0
0
H
 
Using relation (5.101) we determine the encoding relations: 
 
⎪⎩
⎪⎨
⎧
+
+
=
+
+
=
+
+
=
⇒
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
4
3
2
0
5
4
3
1
6
5
4
2
6
5
4
3
2
1
0
a
a
a
a
a
a
a
a
a
a
a
a
0
a
a
a
a
a
a
a
 
0
0
1
1
1
0
1
0
1
1
1
0
1
0
1
1
1
0
1
0
0
 
 
The entire discussion regarding cyclic codes encoding was done supposing the 
existence of a generator polynomial g(x) of degree k. A question raises: 
 
How should be g(x) in order to correct a maximum number of t independent  
errors? 
 
The answer can be given taking into consideration the finite fields theory (Ga-
lois and extended Galois fields [Appendix A]), which represent the mathematical 
support for cyclic codes. 
From the encoding law, we know that v(x) must be a multiple of g(x), which, at 
its turn, according to (5.94), must divide 
0
1
xn
=
+
. 
In order to accomplish these requirements we proceed as follows: 
 
• We choose r roots of  
0
1
xn
=
+
 noted as: 
( )
r
0,
 
 i
or  
  
1
-
r
0,
 i ,
2
GF
 
 
 
k
i
i
=
=
∈
= α
β
                         (5.103) 

260 
5   Channel Coding
 
These βi roots are primitive elements (α) of the k-th order extension of the binary 
Galois field GF(2k). The order k of the extension is found from (Appendix A):   
( )
k
k
2
GF
 
and
k 
1
2
n
⇒
−
=
                                     (5.104) 
GF(
k
2 ) extension is generated by a primitive polynomial p(x) of degree k de-
termined with (5.104) (Table 5.7). Tables containing primitive polynomials for 
different degree until 100 can be found in Appendix A.9. 
 
Table 5.7 Table of primitive polynomials up to degree 
5
k =
. 
 
k 
0
1...a
a
a
k
k
−
 
1 
2 
3 
 
4 
 
5 
1 1 
1 1 1 
1 0 1 1  
1 1 0 1 
1 0 0 1 1 
1 1 0 0 1 
1 0 0 1 0 1 
1 0 1 0 0 1 
1 0 1 1 1 1 
1 1 0 1 1 1 
1 1 1 1 0 1 
 
Remark: k from equation (5.104) represents the extension order of GF(
k
2 ) field 
and not the number of the control symbols: 
m
n
k
−
=
, which will be denoted as 
k′  from this point on. As a consequence of this remark, there will be no misun-
derstandings regarding the use of k in these two cases. 
 
• We determine g(x) as the smallest common multiple of the minimal polynomi-
als corresponding to βi roots: 
} 
(x)
m
(x),...,
s.c.m{m
g(x)
r
1
=
                               (5.105) 
If all the r polynomials are primes, then: 
∏
=
=
r
1
i
i(x)
m
g(x)
                                         (5.105.a) 
A minimal polynomial corresponding to βi is defined as the irreducible poly-
nomial m(x) of minimal degree for which 
0
)
(
m
i
i
=
β
; it is obtained as: 
(
) (
)
⎟⎠
⎞
⎜⎝
⎛
+
+
⋅
+
=
1-
k
2
i
2
i
i
i
x
x
x
)
x
(
β
β
β
m
                            (5.106) 

5.8   Cyclic Codes 
261
 
• In order to obtain a t errors correcting code, we choose 
2t
r =
 roots 
( )
k
i
2
GF
∈
β
 and the code distance of the obtained code has the minimal dis-
tance 
1
2t
d
+
≥
[Appendix A] [29] [52]. 
• Based on the theorem (Appendix A) that states: for binary cyclic codes, if α  is 
a root 
1
k
2
2
2
2
,
,
,
−
α
α
α
…
 are also roots, to obtain the generator polynomial it is 
enough to select only the odd roots: 
1-t
2
1
2t
3
3
1
,...,
,
α
β
α
β
α
β
=
=
=
−
                               (5.107) 
In what follows we will enounce some theorems (without demonstration) useful 
in checking the calculus made for determining the polynomial g(x) [2], [29]: 
• The degree of any minimal polynomial 
)
x
(
mi
 is smaller than or equal to k (the 
extension order) 
( )
k
x
m
 
of
 
degree
i
≤
                                        (5.108) 
• The degree of the generator polynomial g(x) is smaller than or equal to k⋅t (we 
remind that the generator polynomial degree is equal to the control symbols. 
t
k
m
-
n
number
   
symbols
   
control
g(x)
 
of
 
degree
k
⋅
≤
=
=
=
′
        (5.109) 
in which k signifies the GF(
k
2 ) extension order 
• The number of non-zero terms of g(x) equal to the code distance: 
Non-zero terms [g(x)] = 
1
2t
d
+
≥
                          (5.110) 
The codes obtained as shown above (binary cyclic t errors correcting codes) are 
known as BCH codes (Bose–Chauduri–Hocquenghem). Tables with generator poly-
nomials for BCH codes for different n and t may be determined [28], [Appendix A]: 
Table 5.8 BCH codes generator polynomials up to 
31
n =
 
n 
m
t gk’ gk’-1… g0 
7 
 
15 
 
 
 
31 
4 
1 
11
7 
5 
1 
26
21
16
11
6 
1 
2 
1 
2 
3 
7 
1 
2 
3 
5 
7 
1 3 
1 7 7 
2 3 
7 2 1 
2 4 6 7 
7 7 7 7 7 
4 5 
3 5 5 1 
1 0 7 6 5 7 
5 4 2 3 3 2 5 
3 1 3 3 6 5 0 4 7

262 
5   Channel Coding
 
Remark 
The generator polynomial coefficients are given in octal for compression purposes. 
BCH encoding can be done in several ways: 
 
• using relation (5.92) for a non-systematic code  
• using relation (5.101) in which the control matrix structure is determined im-
posing v(x) to have as roots: 
,
,...,
,
1-t
2
1
2t
3
3
1
α
β
α
β
α
β
=
=
=
−
                               (5.111) 
in this case we have: 
(
)
1
-
2t
 
...,
 
5,
 
3,
 
1,
i
0
a
a
a
v
1
n
i
1
n
1
i
1
0
i
0
i
=
=
+
+
+
=
−
−β
β
β
β
"
                         (5.112) 
We obtain: 
(
)
(
)
(
)(
)⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
−
−
−
1
n
1
2t
1-
2t
1
2t
0
1-
n
3
6
3
0
1-
n
2
1
0
2
 
        
     
        
      
α
α 
α
α
α
α
α
α
α
α
α
α
H
"
#
"
"
                           (5.113) 
Remark 
Each element α is expressed in k digits, therefore the control matrix dimension is 
n]
[tk×
 
 
• using relation (5.98), for a systematic structure g(x) being determined as previ-
ously described, or chosen from available tables. 
 
Example 5.12 
Dimension the BCH error correcting codes of length 
15
n =
 for 
1
t = , 
2
t =
 and 
3
t =
. Determine the generator polynomials and the encoding relationships. De-
termine the codewords for a random information sequence. 
 
Solution 
Galois field is dimensioned using (5.104) 
)
GF(2
4
k
1
2
15
1
2
n
4
k
k
⇒
=
⇒
−
=
−
=
 
The elements belonging to GF
)
2
( 4  are the residues of polynomial p(x), a 
primitive polynomial of degree 
4
k =
. From Table 5.7 we choose: 
( )
1
x
x
x
p
4
+
+
=
 
as the generator polynomial of GF(2k). 

5.8   Cyclic Codes 
263
 
The primitive elements of the GF
)
2
( 4  field can be represented using polynomi-
als or matrices. We bear in mind that for any Galois field we have (Appendix A.5): 
1
n =
α
 
The elements of the GF
)
2
( 4  field generated by ( )
1
x
x
x
p
4
+
+
=
 are given in 
Table 5.9. 
For 
1
t =  we choose 
α
β =
1
 
( )
4
2
GF
∈
. The minimal polynomial of this root is, 
according to (5.106): 
( )
(
)(
)(
)(
)
1
x
x
x
x
x
x
x
m
4
8
4
2
1
+
+
=
+
+
+
+
=
α
α
α
α
 
and g(x) according to (5.105) is: 
( )
( )
1
x
x
x
m
x
g
4
1
+
+
=
=
 
One may notice that we obtained a polynomial of degree 4, so the number of 
control symbols will be 
k
4
k
=
=
′
, the order of the extension, because for 
1
t =  
g(x) is equal to p(x). Code dimensions for 
15
n =
 and 
1
t =  are: BCH(15,11,4) 
where n=15 represents the length of the codeword, 
11
m =
 represents the informa-
tion symbols, and k’=4=n-m the control symbols. 
Table 5.9 GF(24) generated by p(x)= x4+x+1 
0 and αi 
Polynomial representation 
 
Matrix representation  
0 
0 
 
 
 
 
 
 
0 0 0 0 
1 
1 
 
 
 
 
 
 
1 0 0 0 
α 
 
 
α 
 
 
 
 
0 1 0 0 
α2 
 
 
 
 
α2 
 
 
0 0 1 0 
α3 
 
 
 
 
 
 
α3 
0 0 0 1 
α4 
1 
+ 
α 
 
 
 
 
1 1 0 0 
α5 
 
 
α 
+ 
α2 
 
 
0 1 1 0 
α6 
 
 
 
 
α2 
+ 
α3 
0 0 1 1 
α7 
1 
+ 
α 
 
 
+ 
α3 
1 1 0 1 
α8 
1 
+ 
 
 
α2 
 
 
1 0 1 0 
α9 
 
 
α 
+ 
 
 
α3 
0 1 0 1 
α10 
1 
+ 
α 
+ 
α2 
 
 
1 1 1 0 
α11 
 
 
α 
+ 
α2 
+ 
α3 
0 1 1 1 
α12 
1 
+ 
α 
+ 
α2 
+ 
α3 
1 1 1 1 
α13 
1 
+ 
 
 
α2 
+ 
α3 
1 0 1 1 
α14 
1 
+ 
 
 
 
 
α3 
1 0 0 1 
 
 
 

264 
5   Channel Coding
 
The control matrix is expressed with (5.113) 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1
1
1
1
0
1
0
1
1
0
0
1
0
0
0
0
1
1
1
1
0
1
0
1
1
0
0
1
0
0
0
0
1
1
1
1
0
1
0
1
1
0
0
1
0
1
1
1
0
1
0
1
1
0
0
1
0
0
0
1
H
 
The structure of a codeword is: 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
=
′













	


	

symbols
n 
informatio
  
1
m
14
13
12
11
10
9
8
7
6
5
4
symbols
  
control
 4
=
k
3
2
1
0
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
v
 
The encoding relation obtained with (5.101) are: 
14
13
12
11
9
7
6
3
13
12
11
10
8
6
5
2
12
11
10
9
7
5
4
1
14
13
12
10
8
7
4
0
a
a
a
a
a
a
a
a 
a
a
a
a
a
a
a
a 
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a 
⊕
⊕
⊕
⊕
⊕
⊕
=
⊕
⊕
⊕
⊕
⊕
⊕
=
⊕
⊕
⊕
⊕
⊕
⊕
=
⊕
⊕
⊕
⊕
⊕
⊕
=
 
In order to determine the codeword we choose the following information  
sequence: 
⎥⎦
⎤
⎢⎣
⎡
=
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
1
LSB
   
i
 
The codeword can be determined either using the encoding relations already 
found or using (5.98): 
( )
( )
( )
( )
(
)1
x
x
/
x
x
g
/
x
i
x
x
x
i
x
1
x
i
4
4
k
4
k
+
+
=
=
=
 
It results: ( )
1
x
x
r
+
=
, so 
( )
( )
( )
1
x
x
x
r
x
i
x
x
v
4
k
+
+
=
+
=
, or in polynomial 
representation:  
⎥⎦
⎤
⎢⎣
⎡
=
1
  
1
  
0
  
0
  
1
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
0
MSB
v
 
For 
2
t =
 we choose the following roots: 
 
( )
(
)(
)(
)(
)
( ) (
)(
)(
)(
)
1
x
x
x
x
x
x
x
x
x
1
x
x
x
x
x
x
x
   
and
   
α
2
3
4
24
12
6
3
3
4
8
4
2
1
3
3
1
+
+
+
+
=
+
+
+
+
=
+
+
=
+
+
+
+
=
=
=
α
α
α
α
m
α
α
α
α
m
α
β
β
 

5.8   Cyclic Codes 
265
 
It can be noticed that both minimal polynomials are of degree 4, meaning that 
(5.109) is fulfilled. The two polynomials being relatively primes, g(x) is deter-
mined with (5.105.a) as: 
( )
( )
1
x
x
x
x
x
m
x
m
g(x)
4
6
7
8
3
1
+
+
+
+
=
=
 
and in binary:  
N
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛

	


	

1
1
  
0
  
0
  
2
0
  
1
  
0
  
7
1
  
1
  
1
=
g
 
The corresponding octal notation is: g = 721. 
Condition (5.110) is fulfilled for g(x) too: 
2
4
8
×
≤
. The non-zero terms of 
g(x) is 5, so according to (5.25) the code corrects two errors (the calculus is cor-
rect). The code dimensions are H(15,7,2) and the codeword structure is: 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
′







	








	

bits
n  
informatio
  
7
=
m
14
13
12
11
10
9
8
bits
  
control
  
8
=
k
7
6
5
4
3
2
1
0
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
v
 
The control matrix is: 
(
)
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
52
6
3
0
14
2
1
0
15,7,2
H
α
α
α
α
α
α
α
α
"
"
 
We ask the reader, as an exercise, to determine the encoding relations, the di-
mensions of the correcting code for 
3
t =
 as well g(x). 
Choosing the following information sequence: 
⎥⎦
⎤
⎢⎣
⎡
=
1
   
0
   
0
   
0
   
0
    
0
0
MSB
i
 
we determine the systematic codeword similar for 
1
t = . 
(
)
( )
⎥⎦
⎤
⎢⎣
⎡
+
+
+
+
=
+
+
+
+
=
=
=
1
   
0
   
0
   
0
   
1
   
0
   
1
   
1
   
1
   
0
   
0
   
0
   
0
   
0
0
or  
  
1
x
x
x
x
x
v
1
x
x
x
x
/
x
i(x)/g(x)
x
x
i(x)
x
1
i(x)
MSB
4
6
7
8
4
6
7
8
8
k
8
k
 
5.8.3   Syndrome Calculation and Error Detection  
Assuming we deal with additive errors, at receiver we get: 
( )
( )
( )
x
e
x
v
x
r
+
=
                                               (5.114) 
 

266 
5   Channel Coding
 
The encoding rule is checked, i.e. r(x) divides by g(x):  
( )
( )
( )
( )
( )
( )
( )
( )
( )
x
g
x
e
rem
x
g
x
e
rem
x
g
x
v
rem
x
s
x
g
x
r
rem
=
+
=
=
                   (5.115) 
where s(x) is the error syndrome. 
From relation (5.115) it follows that: 
 
• the syndrome does not depend on the transmitted codeword v(x), but only on 
the error word e(x); 
• if  ( )
0
x
s
=
, it means that there are not errors or the errors are not detectable; 
• if ( )
0
x
s
≠
, the error is detected; 
• the syndrome s(x) is a polynomial of maximum degree 
1
k −
′
, meaning that for 
binary codes there are 
k'
2
 distinct combinations; it means that from 
1
2n −pos-
sible errors combinations, maximum 
1
2k' − will be corrected. (zero combina-
tion is used for showing the lack of errors). 
Cyclic codes are the most used codes for detection of independent errors as 
well as burst errors. 
In what follows we will express the cyclic codes detection capacity starting 
from the definition (5.11): 
te
en
te
en
te
te
ed
d
N
N
1
N
N
N
N
N
C
−
=
−
=
=
, where Nue is deter-
mined by the undetectable errors combinations, so, for which the error word is a 
multiple (M) of g(x):  
( )
( )
x
g
M
x
e
⋅
=
 
Let us consider a burst of errors of length p, located between positions i and j: 
i
j
p
−
=
 
( )
(
)
i
i
j
j
i
i
i
j
j
e
x
e
x
x
e
x
e
x
e
+
+
=
+
+
=
−
"
"
                     (5.116) 
Further on, we analyze the three situations that may arise: p smaller, equal and 
higher than the generator polynomial degree, k′ . 
 
• 
k
p
′
<
 means that e(x) cannot be a multiple of g(x), so 
0
Nue =
; it results 
1
Cd = ; all burst errors, having lengths smaller than k′ will be detected. 
• if 
k
p
′
=
, there is only one error combination that may correspond to the gen-
erator polynomial:  
( )
( )
1
N
  
so
  ,
x
g
x
e
ue =
=
 
In this case 
k
p
1
p
1
p
d
j
i
1
p
te
2
1
2
1
2
1
1
C
1
e
e
  
because
  ,
2
N
′
−
+
−
−
−
−
=
−
=
−
=
=
=
=
                         (5.117) 

5.8   Cyclic Codes 
267
 
• if 
k
p
′
>
, the errors multiples of g(x) will not be detected: 
( )
( )
( )
( )
x
g
x
m
x
g
M
x
e
⋅
=
⋅
=
 
where m(x) is a polynomial of maximum degree 
k
p
′
−
; we calculate: 
1
p
te
1
k
p
ue
2
N
2
N
−
−
′
−
=
=
 
k
1
p
1
k
p
d
2
1
2
2
1
C
′
−
−
−
′
−
−
=
−
=
                                  (5.118) 
Remark 
Cyclic codes detection capacity depends only on the generator polynomial degree, 
no other condition being imposed on g(x). The notations used are: 
k
m
n
′
+
=
, 
where n is the length of the codeword, m the length of the information block and 
k′ is the number of redundant symbols, equal to the degree of the generator  
polynomial. 
 
Cyclic codes for error detection 
 
Cyclic codes are the most used codes in ARQ systems. A block of data forming a 
message M(x) is cyclically encoded using a generator polynomial of degree k’ de-
termined by the number of detectable errors. At receiver the block r(x) is divided 
to g(x). If the remainder is non zero, the error detection is performed and the re-
transmission is requested. This technique extremely often used in practice is 
known as CRC (Cyclic Redundancy Check). 
Some of its applications are given below: 
 
• In data transmissions, the CCITT V41 notification recommends CRC 
with ( )
1
x
x
x
x
g
5
12
16
+
+
+
=
. Data block lengths are 
3860
 
900,
 
500,
 
260,
n =
. 
The control symbols (CRC sequence) are two bytes long (15 degree polynomi-
als). In the IBM/360 network, the polynomial used in CRC–16 is 
1
x
x
x
g(x)
2
15
16
+
+
+
=
. 
• Error protection in digital telephony - European Standard for data synchroniza-
tion is done with CRC–4. 
• 22469/1–80 standard, equivalent to ISO 4335–79 for data transmission with 
high level connection control procedures (HDLC–High Data Link Control), es-
tablishes the message and check sequence structure (Fig. 5.10) which is ex-
plained further on. 
 
The m information bits (data + command + address) are represented by the 
polynomial i(x) and are included between the delimitation byte ant FCS The con-
trol sequence CRC–16 in the HDLC frame is called “frame checking sequence” 
and denoted with FCS (Frame Checking Sequence). The sequences formed by two 
control bytes at the frame level FLC (Frame Level Control) are used for protocol 
implementation at link level. 

268 
5   Channel Coding
 
 
Fig. 5.10 HDLC frame format 
The delimitation bytes are giving the length of the frame. If a delimitation se-
quence is not placed correctly between two frames, the receiver will assign them 
as a single frame. If the reception of the first frame is error free, the state of the 
LFSR (used for cyclic encoding and decoding) is 0, so the receiver will not detect 
the lack of the delimitation sequence for separating the frames. This problem oc-
curs because the LFSR state is the same (zero) before a frame error free reception 
and after its check. To avoid this problem we need to modify the FCS so that the 
two configurations of the LFSR are different for the two previously mentioned 
cases. 
The generator polynomial g(x) is used for dividing the polynomial M(x):  
( )
1
x
x
x
x
g
5
12
16
+
+
+
=
                                   (5.119) 
( )
( )
( )
x
L
x
x
i
x
x
M
m
16
+
=
                                 (5.120) 
where: 
( )
∑
=
=
15
0
j
j
x
x
L
                                           (5.121) 
is used for the inversion of the first 16 most significant bits from 
i(x)
x16
, determin-
ing the initialisation of the LFSR with “1”, in all binary positions. At the transmitter 
the original message i(x) is transmitted and concatenated with the complemented 
remainder of the division of 
i(x)
xk
 to g(x) (FCS): 
( )
( )
FCS
x
i
x
x
v
16
+
=
                                      (5.122) 
where FCS is the complement with respect to “1” of the remainder r(x) obtained 
from: 
( )
( )
( )
( )
( )
( )
x
g
x
r
x
q
x
g
x
L
x
x
i
x
m
16
+
=
+
                               (5.123) 

5.8   Cyclic Codes 
269
 
so 
( )
( )
( )
x
L
x
r
x
r
FCS
+
=
=
                                    (5.124) 
Adding the polynomial 
L(x)
xm
 to 
i(x)
x16
 is equivalent to setting the initial 
remainder as 1. By complementing with respect to 1 the r(x) by the transmitter, af-
ter performing the division, the error free received message will generate a unique 
non zero remainder; this ensures the protection in the case when there is no de-
limitation sequence at the message end. 
At receiver the division is done: 
( )
( )
( )
( )
( )
( )
[
]
( )
( )
( )
( )
( )
( )
x
g
x
L
x
x
q
x
x
g
x
L
x
x
g
x
r
x
L
x
x
i
x
x
x
g
x
L
x
x
v
x
16
16
16
m
16
16
m
16
16
+
=
=
+
+
+
=
+
+
      (5.125) 
If the transmission is not affected by errors, the receiver remainder is exactly 
( )
( )
x
g
x
L
x16
, which is 
1.
x
x
x
x
x
x
x
2
3
8
10
11
12
+
+
+
+
+
+
+
 
Using this method, the transmitter and the receiver, invert the first 16 most sig-
nificant bits by initialising the LFSR with “1” in all binary position. The remain-
der, after the reception of an error free block, is different from 0 and it has the 
configuration: 
1
   
1
   
1
   
1
   
0
   
0
   
0
   
0
   
1
   
0
   
1
   
1
   
1
   
0
   
0
0
MSB
, allowing the receiving 
of two correct frames with their FCS concatenated because of the lack of delimita-
tion sequence, without being considered as a unique frame. 
5.8.4   Algebraic Decoding of Cyclic Codes 
In what follows we will present the most used algorithm for cyclic decoding: Pe-
terson algorithm with Chien search [47]. 
As shown in 5.8.2, a cyclic t errors correcting code has codewords (vi) with 
2t
r =
 roots 
i
i
k
i
 ,)
GF(2
α
β
β
=
∈
as given in (5.112). 
For BCH codes we take into consideration only the odd roots: 
1
1,2t
i
−
=
 (see 
relation (5.111)). 
At receiver, the encoding relation is checked (5.112), which, for additive errors 
is written as follows: 
( )
( )
( )
( )
( )
1
-
1,3,...,2t
 
 i ,
e
e
e
v
r
i
i
i
i
i
i
=
=
=
=
+
=
S
α
β
β
β
β
                 (5.126)  
For 2t roots, the syndrome S is: 
(
)
1
2t
3
1
S
S 
S
S
−
=
…
                                          (5.127) 

270 
5   Channel Coding
 
For binary codes it is sufficient to determine the error positions from the  
syndrome. 
Let us now see how Si indicates the error position. For this we take randomly 
an error-word, with two errors, on positions 2 and 4: 
( )
4
2
x
x
x
e
+
=
 
The syndrome S, according to (5.126), is: 
( ) ( )
( )
( )
( )
i
2
i
1
i
4
i
2
4
i
2
i
i
i
e
X
X
α
α
α
α
α
S
+
=
+
=
+
=
=
 
where by Xk  we  denoted the error locator, expression that indicates the error  
position: 
1
-
n
0,
j
  
and
t
1,
k
  ,j
k
=
=
= α
X
                                (5.128) 
Therefore, it can be seen that the syndrome Si can be expressed as: 
∑
=
=
t
1
k
i
k
i
X
S
                                                 (5.129) 
which means that the error positions determination is nothing else but solving a 
system of non-linear equations with unknowns Xk. There are numerous methods to 
solve non-linear equation systems and all of them could be algebraic decoding al-
gorithms for cyclic codes. In what follows, we will show one of the first and most 
efficient decoding algorithms: Peterson algorithm with Chien search. 
 
Remark 
Encoding and decoding for cyclic codes can be done in time as well as in fre-
quency. Now we deal with the time approach, continuing to discuss the second 
method for Reed–Solomon codes (R S). 
Peterson algorithm with Chien search has the following steps: 
 
1. Error syndrome calculation 
(
)
( )
∑
=
=
=
=
−
t
1
k
i
k
i
i
1
2t
3
1
1
-
3,...,2t
 
1,
=
i
    
,
r
        
;
,
,
,
X
α
S
S
S
S
S
"
 
2. Finding the error polynomial ( )
x
σ
 with roots the Xk locators. 
( )
(
)
∏
+
+
+
=
+
=
=
−
t
1
k
t
1
t
1
t
k
x
x
x
x
σ
σ
X
σ
"
                     (5.130) 
The coefficients 
1
σ  will be determined taking into account the Si syndromes 
previously calculated. 
The Xk locators are the roots of ( )
x
σ
 polynomial, so: 
(
)
t
1,
k
,
k
=
∀
= 0
X
σ
                                     (5.131) 
 

5.8   Cyclic Codes 
271
 
Replacing in (5.131) x with Xk we obtain: 
0
σ
X
σ
X
=
+
+
+
t
1-t
k
1
t
k
"
                                    (5.132) 
Multiplying (5.132) with 
i
k
X  and summing for 
t
1,
=
k
, we have: 
0
X
σ
X
σ
X
=
∑
+
+
∑
+
∑
=
=
+
=
+
t
1
k
i
k
t
t
1
k
1-i
t
k
1
t
1
k
i
t
k
"
                       (5.133) 
Identifying Si (relationship (5.129)) we can write: 
t
1,
=
i
   
, 
i
t
1
i
t
1
i
t
0
S
σ
S
σ
S
=
+
+
+
−
+
+
"
                        (5.134) 
It can be noticed that (5.134) is, in fact, a linear system of t equations with t un-
knowns. We can solve this system applying Cramer rule: 
• Compute the determinant: 
t
2t
1
2t
2
t
1
t
1
1
t
t
D
S
S
S
S
S
S
S
S
S
"
#
"
"
−
+
−
=
                                   (5.135) 
• If  
0
D ≠
, then the system has a unique solution, given by: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
D
D
,
,
D
D
,
,
D
D
ı
t
j
1
t
"
"
                                  (5.136) 
where 
t
1,
j
  ,
Dj
=
, are characteristic determinants, obtained replacing in D the 
column j with the column formed by the free terms of the system (5.134). 
 
Remarks 
• If 
0
D =
, the solution is not determined, it is supposed that the received word 
contains less than t errors. 
• If 
0
D =
, we search for an integer as large as possible, but 
t
e ≤, such that 
0
De ≠
; it is supposed that in this case, the transmission was affected by e er-
rors. 
• If such an integer e does not exist, we may say that the transmission was not af-
fected by errors; this case is easily illustrated by 
0
i =
S
, 
1
1,2t
i
−
=
. 
• For the BCH codes we must take into account that, according to theorem 
(5.108), 
2
k
2k
S
S
=
. 

272 
5   Channel Coding
 
Table 5.10 Coefficients of the error polynomials for BCH codes. 
t 
i
σ  
1 
1
1
S
σ =
 
2 
(
)
1
3
1
3
2
1
1
S
S
S
σ
S
σ
+
=
=
 
3 
(
) (
)
(
)
2
1
3
3
1
3
3
3
1
5
3
2
1
2
1
1
σ
S
S
S
σ
S
S
S
S
S
σ
S
σ
+
+
=
+
+
=
=
 
 
Table 5.10 contains the coefficients 
(
)
i
i
f S
σ =
 for 
3
  
and
  
2 
1,
t =
; the table 
can be easily filled in for 
3
t >
 (practically up to 
5
t =
, the limit case for a practi-
cal usage of the algorithm). 
 
3. Chien Search 
Chien search algorithm indicates the error position at the moment when one er-
roneous symbol reaches the last cell of a memory register in which the received 
word is loaded. 
If the erroneous position is Xk, the error polynomial is given by (5.132). Divid-
ing (5.132) by 
t
k
X we obtain: 
1
t
1
i
i
k
i
=
∑
=
−
X
σ
                                            (5.137) 
index i showing the erroneous position. The error may occur on all n position, the 
maximum number of correctable errors being t. 
In Chien search, the search of erroneous symbol begins with rn–1, and in this 
case Xk is replaced with 
(
)
1
n−
α
. 
(
)
i
1
i
i
n
1
i
n
i
1
n
i
1
α
α
α
α
α
α
=
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
⋅
=
⋅
⋅
⋅
−
−
−
 
The symbol rn – j will occur in the search equation (5.137) as follows:  
(
)
j
i
j
i
n
i
j
n
i
⋅
⋅
⋅
−
−
−
=
⋅
=
α
α
α
α
 
Finally, Chien search equation (5.137) will be: 
n
1,
=
j ,
       
1
?
t
1
i
j
i
i
=
∑
=
⋅
α
σ
                                      (5.138) 
where to index 
1
j = corresponds rn–1 and to 
n
j =
 corresponds r0 (the reception is 
done beginning with rn–1).  

5.8   Cyclic Codes 
273
 
The j value for which equation (5.138) is satisfied will provide Xk 
j
n
k
−
= α
X
                                            (5.139) 
Example 5.13 
Let us assume the reception of a BCH(15,7) word: 
⎥⎦
⎤
⎢⎣
⎡
=
1
   
0
   
0
   
0
   
1
   
0
   
1
   
1
   
1
   
0
   
0
   
0
   
1
   
0
1
MSB
r
 
Using Peterson algorithm with Chien search find the decoded sequence. 
 
Solution 
( )
1
x
x
x
x
x
x
x
r
4
6
7
8
12
14
+
+
+
+
+
+
=
 
We apply the presented algorithm: 
• Calculating the error syndrome: 
( )
( )
4
12
18
21
24
36
42
3
3
5
4
6
7
8
12
14
1
r
r
α
1
α
α
α
α
α
α
α
S
α
1
α
α
α
α
α
α
α
S
=
+
+
+
+
+
+
=
=
=
+
+
+
+
+
+
=
=
 
• Determining the coefficients 
i
σ : 
From table 5.10, for 
2
t =
, we obtain the formulas for 
1
σ  and 
2
σ : 
11
5
5
15
4
1
3
1
3
2
5
1
1
α
α
α
α
α
α
S
S
S
σ
α
S
σ
=
=
+
=
+
=
=
=
 
• Chien search beginning with r14 using relationship (5.138): 
,
 
   
      
1
j
2
11
5
1
2
2
1
1
1
1
α
α
α
α
α
σ
α
σ
=
⋅
+
⋅
=
+
→
=
⋅
⋅
 
This means that the symbol rn–j is erroneous, so 
14
1-
15
r
r
=
is erroneous. 
1
α
σ
α
σ
1
α
σ
α
σ
1
α
σ
α
σ
r
r
r
1
α
α
α
α
α
α
α
σ
α
σ
1
α
α
σ
α
σ
≠
→
=
≠
→
=
≠
→
=
=
=
+
=
⋅
+
⋅
=
+
→
=
≠
=
→
=
⋅
⋅
⋅
⋅
⋅
⋅
−
⋅
⋅
⋅
⋅
 
+ 
      
   
14
j
 
+
 
      
     
5
j
 +
 
     
     
4
=
j
erronated
 
is
  
 
so
,
 
     
      
3
j
 + 
     
      
2
j
5
2
2
5
1
1
5
2
2
5
1
1
4
2
2
4
1
1
12
3
15
j-
n
17
8
6
11
3
5
3
2
2
3
1
1
9
2
2
2
2
1
1
#
 
 

274 
5   Channel Coding
 
It follows that the error word is: 
[
] 0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
1
  
0
  
1
=
e
 
The decoded word will be:  
[
]
[
]
⎥⎦
⎤
⎢⎣
⎡
=
+
+
=
⊕
=
1
  
0
  
0
  
0
  
1
  
0
  
1
  
1
  
1
  
0
  
0
  
0
  
0
  
0
0
 
=
   
          
 0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
0
  
1
  
0
  
1
1
  
0
  
0
  
0
  
1
  
0
  
1
  
1
  
1
  
0
  
0
  
0
  
1
  
0
  
1
MSB
e
r
v
 
The information sequence at the decoder output is:  
[
]1
  
0 0
  
0
  
0
  
0
  
0
=
i
. 
5.8.5   Circuits for Cyclic Encoding and Decoding 
As shown in 5.8.1 and 5.8.3, coding and decoding for systematic cyclic codes are 
done by division: 
i(x)
xk
 and r(x) respectively, to g(x). 
In what follows we will focus on two methods of dividing the polynomials, im-
plying two methods of coding and decoding: using linear feedback shift registers 
(LFSR) with external or internal modulo two adders. 
 
LFSR with external modulo two adders 
 
A LFSR is a linear sequential circuit that can operate independently, without any 
input signal except the feedback signal. The register connections depend on the 
characteristic (generator) polynomial: 
( )
0
1
1
k
1
k
k
k
g
x
g
x
g
x
g
x
g
+
+
+
+
=
−
−
"
 
in which 
{
}
0;1
gi ∈
 except for 
k
g which is always “1”. The block scheme of a 
LFSR with external modulo two adders is given in figure 5.11. 
Ck-1
C0
C1
Ck-2
gk=1
gk-1
gk-2
g2
g1
g0
• • •
 
Fig. 5.11 LFSR with external modulo 2 adders 

5.8   Cyclic Codes 
275
 
The register operation is described by the equations (5.140), where 
j
iC
st
, 
j
1-i C
st
 respectively, are the cell Cj states at the moment i, i-1 respectively. 
⎪
⎪
⎩
⎪⎪
⎨
⎧
+
+
+
=
=
=
−
−
−
−
−
−
−
−
1
k
1
i
1
k
1
1
i
1
0
1
i
0
1
k
i
2
1
i
1
i
1
1
i
0
i
C
st
g
C
st
g
C
st
g
C
st
C
st
 
          
   
C
st
C
st
   
C
st
"
#
              (5.140) 
In a matrix description we have: 
1
i
i
−
= TS
S
                                               (5.140.a) 
where  
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
−
−
−
−
−
−
1
k
2
1
0
1
k
1
i
0
1
i
1
i
1
k
i
0
i
i
g
...
g
g
g
1
...
0
0
0
0
...
1
0
0
0
...
0
1
0
    
and
 , 
C
st
C
st
    
, 
C
st
C
st
#
#
#
#
#
#
#
T
S
S
    (5.141) 
T being the register characteristic matrix. 
 
If the initial state of the LFSR is 
0
0 ≠
S
, its evolution in time will be the fol-
lowing: 
0
0
n 
0
0
,...,
,
S
S
T
TS
S
=
, so after a number of n steps it loads again the ini-
tial state; n is the register period. In order to have distinct states, 
1
−
T
must exist, 
this being equivalent to have 
1
g0 = . The total number of 
1
2k − non-zero states of 
the LFSR can be generated in one cycle or more. 
The characteristic polynomial of the matrix T is defined by: 
( )
k
1
k
1
k
1
0
1
k
2
1
0
x
x
g
...
x
g
g
x
g
...
g
g
g
1
x
...
0
0
0
0
0
...
1
x
0
0
0
...
0
1
x
x
det
x
Φ
+
+
+
+
=
=
−
−
−
−
=
−
=
−
−
−
#
I
T
                 (5.142) 
It can be seen that the characteristic polynomial of the matrix T is the generator 
polynomial g(x) such that the LFSR is uniquely determined. 
The characteristic matrix T is a root of the generator polynomial: 
( )
0
g
=
T
                                                   (5.143) 

276 
5   Channel Coding
 
The period of the characteristic matrix T, in other words the cycle length, is the 
smallest integer n for which: 
I
T
T
=
=
0
n
                                           (5.144) 
If U is the matrix:  
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1
0
0
#
U
                                               (5.145) 
The n non-zero states of the LFSR are: 
U
U
T
U
T
TU
U
U
T
=
=
n
2
0
,...,
,
,
                            (5.146) 
Using the notation: 
U
T
α
i
i =
                                             (5.147) 
and if T is a root of g(x) and 
i
α  is a root of g(x), we get: 
( )
(
)
0
g
g
i
i
=
=
U
T
α
                                     (5.148) 
From the theory of residues modulo g(x) of degree k, we know that n is maxi-
mum if g(x) is primitive (T is called a primitive element of the field GF
)
(2k  gen-
erated by g(x) of degree k, primitive) 
1
2
n
k −
=
 
It follows that the LFSR generates all the non-zero states in one cycle, under 
the assumption that g(x) is primitive. 
 
LFSR with internal modulo two adders  
 
The block scheme of LFSR with internal modulo 2 adders is given in Fig. 5.12. 
 
Fig. 5.12 LFSR with internal modulo 2 adders 
 

5.8   Cyclic Codes 
277
 
In this case the characteristic matrix is: 
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
−
−
0
...
0
0
g
1
...
0
0
g
0
...
1
0
g
0
...
0
1
g
0
1
2
k
1
k
#
#
#
#
#
T
                                  (5.149) 
LFSRs from Fig. 5.11 and Fig. 5.12 are equivalents as they have the same char-
acteristic polynomial. 
 
Encoders for systematic cyclic codes implemented with LFSR 
 
The process of encoding systematic cycle codes can be realized using LFSR with 
internal or external modulo two adders. 
 
Encoder with LFSR  
 
The block scheme of a cyclic encoder with LFSR (with external ⊕) is shown in 
Fig 5.13: 
 
Fig. 5.13 Cyclic systematic encoder with LFSR and external modulo two adders 
Comparing this scheme with Fig. 5.11 it can be easily observed that there is an 
input signal for the information symbols: 
]
a 
 
a[
m
-
n
1-
n …
i
, a modulo two adder S2 
and the switch K. 
The input signal modifies the LFSR operating equation as follows: 
U
TS
S
i
1
i
i
a
+ 
=
−
                                       (5.150) 
where ai is the  input symbol at the moment i. 

278 
5   Channel Coding
 
The switch K is on position 1 for m clocks, while the information symbols are 
delivered to the register; after m clocks it switches to position 2 for k clocks and 
the LFSR calculates the control symbols dividing 
i(x)
xk
 to g(x). At the output we 
obtain the systematic cyclic code word: 
r(x)
i(x)
x
v(x)
k
+
=
, where r(x) is the  
remainder of 
/g(x)
i(x)
xk
. 
Due to the fact that, during the last k clocks the switch K is on position 2, the S2 
output will be 0 (its two input signals are identical). This means that at the end of 
n clocks the LFSR will be in the 0 state (all the cells will be 0). 
Table 5.11 Cyclic encoder with LFSR and external modulo two adders 
Ck 
K 
input i 
Si 
Output v 
1 
1-
n
a
 
U
1-
n
a
 
1-
n
a
 
2 
2
n
a
− 
TU
U
1-
n
2
-
n
a
a
+
 
2
n
a
−
 
… 
… 
 
 
m 
 
 
1 
m
n
a
−
 
U
T
U
1
m
1-
n
m
-
n
a
a
−
+
+…
 
m
n
a
−
 
m+1 
 
U
T
U
m
1-
n
1-
m
-
n
a
a
+
+…
 
1
-
m
n
a
−
 
… 
 
 
 
m+k = n 
 
2 
 
U
T
TU
U
1-
n
1
-
n
1
0
a
a
a
+
+
+
…
0
a
 
 
As it was already explained, at moment n the LFSR state will be zero: 
[
]
U
T
TU
U
S
1
n
1
n
1
0
n
a
+
+
a
+
a
0
−
−
=
=
"
                               (5.151) 
Relation (5.151) can also be written as a matrix product: 
[
]
0
a
a
a
1
n
1
0
1
n
=
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
⋅
−
−
#
U
...T
 
TU
  
U
                                   (5.151.a) 
and more generally: 
0
T =
⋅v
H
 
By identification we get for the control matrix the following structure: 
[
]
U
T
 
...
 
TU
 
U
=
H
1
n−
                                          (5.152) 
where 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
1
0
0
#
U
and T is determined by (5.141). 

5.8   Cyclic Codes 
279
 
Encoder with LFSR and internal modulo two adders 
 
The block scheme of the cyclic systematic encoder with LFSR and internal 
modulo two adders is given in figure 5.14. 
 
Fig. 5.14 Cyclic systematic encoder with LFSR and internal modulo two adders 
Comparing this scheme with the one from Fig. 5.12, there are some changes: 
the modulo two adder S which allows us to supply the encoder with information 
symbols during the first m time periods, the gate P which is open during the first 
m periods and blocked during the last k periods, the switch K which is on position 
1 during the first m clocks and on 2 during the last k clocks. 
The encoder operation is given by relation (5.150):  
U
TS
S
i
1
i
i
a
+
=
−
 
At the end of the m clocks (as long we have information symbols) the register 
cells will be loaded with the remainder 
r(x)
/g(x)]
i(x)
[xk
=
; it follows that after 
the next k clocks the register will be in the zero state: 
0
S
=
n
. 
Similar to the case described in figure 5.13, the encoding relation 
0
S
=
n
 leads 
to 
0
v
H
=
⋅
T
; at this point we identify H with expression (5.152). 
 
Example 5.14 
We will illustrate a systematic cyclic encoder with LFSR and external modulo two 
adders, for H(7,4) and 
1
x
x
g(x)
3
+
+
=
. We will calculate the encoding relations 
and solve them for 
⎥⎦
⎤
⎢⎣
⎡
=
1
  
0
  
0
0
MSB
i
. 
For a given sequence i, the systematic cyclic codeword, according to (5.98), is: 
( )
( )
( )
( )
or
   
1
x
x
x
v
  
so
 
1,
x
1
x
x
x
rem
x
r
x
x
i
x
1
x
i
3
3
3
3
k
+
+
=
+
=
+
+
=
=
=
 
 

280 
5   Channel Coding
 
in a matrix form: 
⎥⎦
⎤
⎢⎣
⎡
1
   
1
   
0
   
1
   
0
   
0
0
=
MSB
v
 
The block scheme of a cyclic encoder with LFSR and external modulo two ad-
ders LFSR and its operation are given in figure 5.15. 
 
 
 
tn 
tn+1 
tn 
Ck 
K 
input i 
C2 
C1 
C0 
output v 
1 
a6 
0 
0 
0 
a6 
2 
a5 
a5 
a6 
0 
a5 
3 
a4 
a4+a6 
a5 
a6 
a4 
4 
 
1 
 
a3 
a3+a5+a6 
a4+a6 
a5 
a3 
5 
 
0 
a3+a5+a6 
a4+a6 
a2=a4+a5+a6 
6 
 
0 
0 
a3+a5+a6 
a1=a3+a4+a5 
7 
 
2 
 
0 
0 
0 
a0=a3+a5+a6 
 
b) 
 
 
 
 
 
Fig. 5.15 Cyclic encoder with LFSR and external ⊕: a) block scheme; b) operation table 
for g(x)= x3 + x + 1 and  m = 4. 
The scheme from Fig. 5.15.a) operates according to table from Fig. 5.15.b); we 
have already seen that equation 
0
v
H
=
⋅
T
 describes the scheme behaviour where 
H is given by (5.152) and T and U are by (5.141) and (5.145). 
The register characteristic matrix T is: 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
0
1
1
1
0
0
0
1
0
g
g
g
1
0
0
0
1
0
2
1
0
T
 and 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
1
0
0
U
.  

5.8   Cyclic Codes 
281
 
It follows easily H: 
 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
0
0
1
1
1
0
1
0
1
1
1
0
1
0
1
1
1
0
1
0
0
H
  and then, 
0
Hv
=
T
  leads to: 
⎪⎩
⎪⎨
⎧
⊕
⊕
=
⊕
⊕
⊕
⊕
=
⊕
⊕
=
⊕
⊕
=
⊕
⊕
=
6
5
3
4
3
6
5
4
4
3
2
0
5
4
3
1
6
5
4
2
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
, 
 
obviously identical with those previously determined. 
For 
] 1
  
0
  
0
  
0
[
=
i
one can easily check (either using the operation table or tak-
ing into account the encoding system) that  
1]
  
1
  
0
  
1
  
0
  
0
  
[0
=
v
. 
The block scheme of a cyclic encoder with LFSR and internal modulo two ad-
ders and its operation table are given in Fig. 5.16. 
 
 
 
 
tn 
 
 
tn+1 
 
tn 
Ck 
input i 
P 
C0 
C1 
C2 
Output v 
1 
a6 
 
a6 
a6 
0 
a6 
2 
a5 
ON 
a5 
a5+ a6 
a6 
a5 
3 
a4 
 
a4+ a6 
a4 + a6+ a5 
a5+ a6 
a4 
4 
a3 
 
a3 + a5+ a6 
a3 + a4+ a5 
a4 + a5+ a6 
a3 
5 
 
 
0 
a3 + a5+ a6 
a3 + a4+ a5 
a2 = a4 + a5+ a6 
6 
 
OFF 
0 
0 
a3 + a5+ a6 
a1 = a3 + a4+ a5 
7 
 
 
0 
0 
0 
a0 = a3 + a5+ a6 
b) 
 
 
 
 
 
 
Fig. 5.16 Cyclic encoder with LFSR and internal ⊕: a) block scheme; b) operation table 
for g(x)= x3 + x + 1 and  m = 4. 
From the operation table it can be seen that r(x), the remainder of 
/g(x)
i(x)
xk
 
is calculated by the LFSR at the end of 
4
m =
 clocks, being downloaded from the 
register during the last 
3
k =
 clocks, when the gate P is OFF. 
 

282 
5   Channel Coding
 
The register characteristic matrix is given by (5.149): 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
−
−
0
0
1
1
0
1
0
1
0
0
...
0
g
1
...
0
g
0
...
0
g
0
...
1
g
0
1
2
k
1
k
#
#
#
#
T
, 
U is calculated according to (5.145), and the control matrix (5.152) will be:  
[
]
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
0
0
1
1
1
0
1
0
1
0
0
1
1
1
1
0
0
1
1
1
0
  
  
  
  
  
  
=
6
5
4
3
2
U
T
U
T
U
T
U
T
U
T
TU
U
H
 
 
⎪⎩
⎪⎨
⎧
=
⊕
⊕
⊕
=
⊕
⊕
⊕
=
⊕
⊕
⊕
⇒
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⋅
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
0
a
a
a
a
0
a
a
a
a
0
a
a
a
a
0
a
a
a
a
a
a
a
0
0
1
1
1
0
1
0
1
0
0
1
1
1
1
0
0
1
1
1
0
4
3
2
0
5
2
1
0
6
3
2
1
6
5
4
3
2
1
0
T
Hv
 
Processing the last equations one can obtain the encoding relationships for a0, 
a1, and a2. 
 
Error detection cyclic decoders  
 
As shown in 5.8.3, relation (5.115), the error detection condition is that the syndrome 
s(x) (the remainder of 
/g(x)
r(x)
) is non-zero. This state has to be underlined by the 
decoder. We have already seen, when encoding, that the LFSR divides 
i(x)
xk
to 
g(x), so it can also be used for decoding. In both cases we saw that, at the end of the n 
clocks, the LFSR state is zero; it follows that, when detecting the error, it is enough to 
test the LFSR final state.  If this state is non-zero, the error is detected (see Fig. 5.17). 
 
 
Fig. 5.17 Error detection cyclic decoder with LFSR and external ⊕ 

5.8   Cyclic Codes 
283
 
The syndrome S, determined by the LFSR state at the moment n: 
0
n ≠
S
 is not 
the remainder of 
/g(x)
r(x)
, but a modified form of it which allows the error  
detection. 
Ck-1
C0
C1
Ck-2
gk=1
gk-1
gk-2
g2
g1
g0=1
Detector Sn ≠ 0
• • •
r
 
Fig. 5.18 Error detection cyclic decoder with LFSR and internal ⊕ 
For this decoder, the syndrome S is calculated as the remainder of 
/g(x)
r(x)
 
and is determined by the LFSR state at the moment n. 
 
Example 5.15 
Consider the cyclic code H(7,4) with 
1
x
x
g(x)
3
+
+
=
 and the received words: 
]1
  
1
  
0
  
1
  
0
  
0
  
1[
1 =
r
 and 
]1
  
1
  
0
  
1
  
0
  
0
  
0
[
2 =
r
. Using a LFSR with external ⊕, 
check if the two words have been correctly received. 
 
Solution 
( )
( )
1
x
x
x
r
1
x
x
x
x
r
3
2
3
6
1
+
+
=
+
+
+
=
 
It can be seen that 
( )
x
r2
 is the codeword
( )
x
g
=
. 
( ) ( ) (
) (
)
( )
( )   ,
x
g
1
x
1
x
x
1
x
x
/
1
x
x
x
x
/g
x
r
2
x
q
3
3
3
6
1
+
+
+
+
=
+
+
+
+
+
=

	

 
 
 

284 
5   Channel Coding
 
so ( )
1
x
x
r
2 +
=
; this means that r1(x) is erroneous. It can be emphasized also by 
the LFSR state 
)
0
(≠
 at the moment 
7
n =
. The block scheme and the operation 
table of the decoder are given in Fig. 5.19: 
 
 
 
 
tn 
tn+1 
r 
C2 
C1 
C0 
T 
1 
2 
1 
2 
1 
2 
1 
2 
1 
1 
0 
1 
0 
0 
0 
0 
0 
2 
0 
0 
0 
0 
1 
0 
0 
0 
3 
0 
0 
1 
0 
0 
0 
1 
0 
4 
1 
1 
0 
1 
1 
0 
0 
0 
5 
0 
0 
1 
0 
0 
1 
1 
0 
6 
1 
1 
0 
0 
1 
0 
0 
1 
7 
1 
1 
0 
0 
0 
0 
1 
0 
           b) 
 
Fig. 5.19 a) Block scheme and b) the operating table of the cyclic decoder for  
g(x) = x3 + x + 1 
From the operating table we see that at the moment 
7
n =
, for r1, the register 
state is (
)
0
S
≠
=
n
1 0 0
; this state is not that one corresponding to the remainder 
)1
  
0
  
(1
. 
The block scheme and the operation table of a cyclic error detection decoder 
with LFSR and internal modulo two adders is given in Fig. 5.20. 
 
 

5.8   Cyclic Codes 
285
 
 
 
 
tn 
 
 
 
 
tn + 1 
 
 
T 
   r 
 
          C0 
 
          C1 
 
               C2 
 
 
1 
2 
1 
2 
1 
2 
1 
2 
1 
1 
0 
1 
0 
0 
0 
0 
0 
2 
0 
0 
0 
0 
1 
0 
0 
0 
3 
0 
0 
0 
0 
0 
0 
1 
0 
4 
1 
1 
0 
1 
1 
0 
0 
0 
5 
0 
0 
0 
0 
0 
1 
1 
0 
6 
1 
1 
0 
1 
1 
0 
0 
1 
7 
1 
1 
1 
0 
0 
0 
1 
0 
                               b) 
Fig. 5.20 a) Block scheme and b) the operation table of a cyclic error detection decoder 
with LFSR and internal ⊕ 
Error Correction Cyclic Decoder (Meggitt Decoder) 
 
Decoding for binary error correction must allow determining the erroneous  
position from the syndrome expression.  
As we have already seen, error detection is possible at the end of the entire 
word reception, i.e. after n clocks. For binary codes correction it is necessary to 
know the position of the error. If it is possible to determine a fixed state of the 
syndrome register (SR) during 
)
2n
  
1;
n
( +
, when the erroneous bit is found in the 
last cell of an n-bit memory register (MR) in which the received word is serially 
loaded, the correction is immediately done summating modulo a “1” on the deter-
mined position. The correction is possible during 
)
2n
  
1;
n
( +
, therefore, the error 
correction with cyclic codes takes 2n clocks. One word length breaks are avoided 
during transmission using two identical decoders that operate in push-pull. The 
block scheme of an error correction cyclic decoder is given in Fig. 5.21. 
The legend for Fig. 5.21 is: 
 
• MR – memory register (n cells) 
• LFSR (SR) – syndrome register (a LFSR identical with that one used for  
encoding) 

286 
5   Channel Coding
 
• D – fix state detector: detects the SR unique state (fix state) when the erroneous 
bit is in the last cell (n) of the MR. 
• C – correction cell (modulo two adder) 
• 
*
2
*
1
2
1
P
,
P
,
P
,
P
- gates ensuring the decoders push-pull operation. 
 
 
Fig. 5.21 General block scheme of an error correction cyclic decoder 
The way this block scheme operates is described below: 
• During (1÷n):  
ON
P
OFF
P
  
OFF
P 
ON
P
*
2
2
*
1
1
−
−
−
−
.  
the received word enters the MR and simultaneously the first decoder. 
• During
)
2n
 
1
n
(
÷
+
: 
OFF
P 
ON
P
ON
P
OFF
P 
*
2
2
*
1
1
−
−
−
−
 
it is possible to correct the received word during 
)
2n
  
1;
n
( +
 in the correction cell 
C, detecting the SR fix state. The next word is received and simultaneously loaded 
in the second decoder and in the MR too, as this register becomes available when 
the previous word has been corrected too. 
5.8.6   Cyclic One Error Correcting Hamming Code 
The Hamming perfect code, studied in 5.4.5 can be cyclically encoded too. The re-
lation (5.71) defining the perfect one error correcting code remains: 
1
2
n
k −
=
.  
 
 

5.8   Cyclic Codes 
287
 
The differences between this code and the Hamming group code studied in 
5.7.9 are the codeword structure and the encoding / decoding relations.  
The structure of the cyclic Hamming code is:  
]
a
a
a
a
a[
symbols
n
informatio
 
m
1
n
k
symbols
control
 
k
1
k
1
0

	
 "



	

"
−
−
=
v
, (see relation (5.98) ) 
For encoding, a primitive generator polynomial g(x) of degree k is chosen. The 
encoding can be done using LFSR, as shown in the Fig. 5.13, respectively Fig. 
5.14. The encoding relations are obtained with: 
,0
T =
⋅v
H
 
where 
[
]
U
T
 
 
TU
  
U
=
H
1
n
...
−
 (see relation (5.152)), with T and U determined as 
indicated in 5.8.5. 
In what follows we will present in detail the error correction decoding proce-
dures, determining that the syndrome register fix state is highlighted by the detec-
tor D, when the erroneous bit reaches the MR last cell of the decoder presented in 
Fig. 5.21.  
Decoder with LFSR and external modulo 2 adders 
Suppose an error occurred on position 
1
n
0,
i  ,ri
−
=
during transmission: 
[
]
0
e
0
i"
"
=
e
 
At moment n, the syndrome Sn is: 
[
]
U
T
U
T
 
TU
  
U
=
He
=
Hr
=
S
i
i
1
n
T
T
0
e
0
 
...
 
=
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⋅
−
#
#
                       (5.153) 
At this moment (n), the erroneous bit is in the (
1
i + )th cell of the MR: 
 
 
 
 
 
 
 
MR 
t = n 
r0 
r1 
… 
ri 
… 
rn–1 
 
 
1 
2 
… 
i+1 
… 
n 
 
The symbol ri will reach the MR last cell after 
1)
 - i -
(n 
 clocks, therefore taking 
into account also the n clocks necessary to charge r in MR, at the moment 
1)
 - i -
(n 
 
n +
. We are interested to find out the SR state at that moment. For this 
purpose, the SR will freely operate (without input, P1 being blocked): 

288 
5   Channel Coding
 
(
)
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
−
−
+
+
−
−
−
−
−
−
−
+
0
0
1
=
=
=
=
=
:
1
i
n
n
=
   
:1
n
=
          
:n
1
1
1
n
i
1
i
n
1
i
n
1
i
i
#
#
U
T
U
IT
U
T
T
U
T
T
S
T
U
T
TS
U
T
S
    (5.154) 
n
T being the k-th order unit matrix and 
U
T-1
 = 
T
]
0 
...
 0
  
1[
, T indicating the 
transposition. 
So, for a decoder with LFSR with external ⊕, the SR fix state is 
]
0 
...
 0
  
1[
, 
meaning that all the k cells, except C0, will be 0. This state will be detected by D 
which will provide a ‘1’, added in the correction cell C with ri: 
(
)
i
i
i
a
1
1
a
1
r
=
+
+
=
+
                                       (5.155) 
implying the complementation of the bit value, so correcting  ri.  
 
Remark 
The occurence of this state 
]
0 
...
 0
  
1[
 during (
n
,1
) would cause false corrections, 
therefore it is necessary to block P2 for all this period. 
 
Decoder with LFSR and internal modulo two adders 
 
For LFSR with internal ⊕the state of the LFSR indicates precisely the remainder, 
meaning that when the erroneous symbol reaches the last cell of the MR, the error 
word e(x) corresponds to 
1-
n
x
, meaning that the state of the SR is given by: 
( )
state
fix 
 
SR
x
g
x
rem
1
n
=
−
                                  (5.156) 
Example 5.16 
Consider the cyclic one error correcting Hamming code H(7,4) with 
1
x
x
g(x)
3
+
+
=
; its encoding was analysed in example 5.15. 
We choose the following transmitted word:  
⎥
⎦
⎤
⎢
⎣
⎡
=
0
1
2
3
4
5
6
a
a
a
a
a
a
a
1
1
0
1
0
0
 
0
v
 
and assume that a4 (i = 4) is erroneous, so the reception is: 
[
].
1
  
1
  
0
  
1
  
1
  
0
  
0
=
r
 
 
 

5.8   Cyclic Codes 
289
 
We will illustrate for this r the error correction procedure using LFSR  
(figure 5.22). 
                    SR                                                  MR                                         D0
Ck 
r 
C2 
C1 
C0 
 
 
1 
2 
3 
4 
5 
6 
7 
 
 
 
1 
0 
0 
0 
0 
 
 
0 
- 
- 
- 
- 
- 
- 
 
 
- 
2 
0 
0 
0 
0 
 
 
0 
0 
- 
- 
- 
- 
- 
 
 
- 
3 
1 
1 
0 
0 
 
 
1 
0 
0 
- 
- 
- 
- 
 
 
- 
4 
1 
1 
1 
0 
 
 
1 
1 
0 
0 
- 
- 
- 
 
 
- 
5 
0 
1 
1 
1 
 
 
0 
1 
1 
0 
0 
- 
- 
 
 
- 
6 
1 
1 
1 
1 
 
 
1 
0 
1 
1 
0 
0 
- 
 
 
- 
7 
1 
1 
1 
1 
 
 
1 
1 
0 
1 
1 
0 
0 
 
 
- 
8 
- 
0 
1 
1 
 
 
- 
1 
1 
0 
1 
1 
0 
 
 
0 
9 
- 
0 
0 
1 
 
 
- 
- 
1 
1 
0 
1 
1 
 
 
1 
 
b) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 5.22 a) SR block scheme and b) operation of the cyclic decoder from Fig. 5.21, for the 
cyclic Hamming code (7,4) with g(x) = x3 + x + 1; LFSR with external ⊕ 
 

290 
5   Channel Coding
 
 
 
SR 
 
 
 
MR 
 
            D0
Ck 
r 
C0 
C1 
C2 
 
1 
2 
3 
4 
5 
6 
7 
 
 
1 
0 
0 
0 
0 
 
0 
- 
- 
- 
- 
- 
- 
 
- 
2 
0 
0 
0 
0 
 
0 
0 
- 
- 
- 
- 
- 
 
- 
3 
1 
1 
0 
0 
 
1 
0 
0 
- 
- 
- 
- 
 
- 
4 
1 
1 
1 
0 
 
1 
1 
0 
0 
- 
- 
- 
 
- 
5 
0 
0 
1 
1 
 
0 
1 
1 
0 
0 
- 
- 
 
- 
6 
1 
0 
1 
1 
 
1 
0 
1 
1 
0 
0 
- 
 
- 
7 
1 
0 
1 
1 
 
1 
1 
0 
1 
1 
0 
0 
 
- 
8 
- 
1 
1 
1 
 
- 
1 
1 
0 
1 
1 
0 
 
0 
9 
- 
1 
0 
1 
 
- 
- 
1 
1 
0 
1 
1 
 
1 
b) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 5.23 a) SR block scheme and b) operation of the cyclic decoder from Fig. 5.21, for the 
cyclic Hamming code (7,4) with g(x) = x3 + x + 1; LFSR with internal ⊕ 
5.8.7   Golay Code 
The Golay codes are binary perfect codes, as Hamming codes. 
The parameters of the Golay code are: 
3
  t
11,
k
  
12,
m
  
23,
n
=
=
=
=
                                (5.157) 
Being a perfect code, the number of non–zero syndromes is equal to that of the 
correctable errors; therefore relation (5.47): 
1
2
C
k
t
1
i
i
n
−
=
∑
=
  is satisfied for 
3
t =
. 
N
1
2
C
C
C
11
3
23
2
23
1
23
=
−
=
+
+
                               (5.158)  
It results that the codeword roots are primitive elements α of GF(211) generated 
by a primitive polynomial p(x) of degree 
11
k =
 for which: 
11
 N =
α
                                            (5.159) 

5.8   Cyclic Codes 
291
 
Due to the fact that the codeword length is 
23
n =
, the generator polynomial of 
degree 11 must have the same period 
23
n =
, so all its α roots must be of the 
same order 23. 
From 
1
2
N
11 −
=
 elements of GF (211), we will choose a root αi to be a root of: 
0
1
Xn
=
+
                                                 (5.160) 
and at the same time root of g(x). Replacing αi in (5.160) and taking into account 
(5.159), we obtain: 
1
 
N
n
i
=
=
⋅
α
α
                                             (5.161) 
from which results: 
89
23
1
2
n
N
i
11
=
−
=
=
                                       (5.162) 
In order to obtain a minimum number of control symbols, g(x) must be a mini-
mal polynomial of
89
 
α
β =
. Depending on how we choose p(x) as generator for 
GF
)
2
( 11 , we obtain [43], [54]: 
( )
11
10
6
5
4
2
1
x
x
x
x
x
x
1
x
g
+
+
+
+
+
+
=
                         (5.163) 
or: 
( )
11
9
7
6
5
2
x
x
x
x
x
x
1
x
g
+
+
+
+
+
+
=
                           (5.164) 
It is obvious that the two polynomials are dividers of 
1
x23 + : 
(
) ( )
( )
x
g
x
g
x
1
1
x
2
1
23
+
=
+
                                     (5.165) 
Decoding can be done using the correspondence table, (the syndrome based  
decoding table), or applying: 
0
v
H
=
⋅
T
 
where  
[
]
1
n
1
0
 
 
 
−
=
β
β
β
H
…
                                           (5.166) 
Elements
i
 β are represented by the table of modulo p(x) residue, p(x) polyno-
mial of degree 11, and each element being an 11 bits matrix. 
Observing the generator polynomials g1(x) and g2(x), it can be seen that the 
non-zero terms is 7, so according (5.110), the code distance is: 
3
t
1
2t
d
=
⇒
+
=
 
It follows that this code can correct maximum 3 errors. 

292 
5   Channel Coding
 
5.8.8   Fire Codes 
Fire codes (1959) are the first error correcting cyclic codes used for burst error 
correction [28], [43]. 
An error correcting Fire code for bursts of length p is generated by the  
polynomial: 
( )
(x)
1)g
(x
x
g
1
1
2p
+
=
−
                                         (5.167) 
where g1(x) is a primitive polynomial of degree m over GF(2). By nm we denote 
the period of g1(x). The length n of the code is: 
n = smallest common multiplier {
}
p
n
1,
p
2 −
                         (5.168) 
The number of control symbols is: 
1
p
2
m
k
−
+
=
                                             (5.169) 
Fire encoding is done using LFSRs with external or internal modulo two  
adders. 
Let us consider a burst error of length p: 
]
e
ε
ε
e
[
e
t
n
1
t
n
i
n
1
p
t
n
1
…
…
…
…
−
−
−
−
+
−
−
=
             (5.170) 
or, written polynomially: 
]
x
e
x
ε
 
          
x
ε
x
e[
(x)
e
t
n
t
n
1
t
n
1
t
n
i
n
i
n
1
p
t
n
1
p
t
n
l
−
−
−
−
−
−
−
−
+
−
−
+
−
−
+
+
+
+
+
+
=
…
…
                    (5.170.a) 
where 
1
ei = represents the error position i. 
jε  can be an erroneous position 
)1
ε( j =
 or an error free position
)
0
ε( j =
. 
In order to include the case 
1
p = , when the burst has one error, we put: 
⎩
⎨
⎧
>
=
=
=
+
−
−
+
−
−
1
p
for  
  ,1
1
p
for  
  ,0
ε
e
1
p
r
n
1
p
t
n
                              (5.171) 
In this case the error burst is: 
)
x
x
ε
(ε
x
x
x
ε
x
ε
x
e
(x)
e
1
p
2
p
1
t
n
1
p
t
n
1
p
t
n
t
n
1
t
n
1
t
n
i
n
i
n
1
p
t
n
1
p
t
n
l
−
−
−
−
+
−
−
+
−
−
−
−
−
−
−
−
−
+
−
−
+
−
−
+
+
+
=
=
+
+
+
+
+
=
…
…
…
  (5.172) 
 
 

5.8   Cyclic Codes 
293
 
The syndrome is: 
U
T
I
T
U
I
T
T
T
U
T
U
T
U
T
S
)
e
+
(
=
=
)
e
(e
=
=
+
e
e
=
j
1
l
1
j
j
t
n
t-
1
1
t
n
1
p
1
p
t
n
t-
t
n
i-
n
i
n
1
p
t
n
1
p
t
n
−
−
=
−
−
−
−
−
+
−
+
−
−
−
−
+
−
−
+
−
−
∑
+
+
+
+
+
…
…
               (5.173) 
where T is the LFSR characteristic matrix used for encoding, and U the SR input 
matrix (both depending on the type of LFSR). 
When the first erroneous symbol 
1-
nr
 reaches the MR last cell, which is after 
1
-
t
 clocks, the SR state is: 
∑
−
=
−
−
−
−
−
1
l
1
j
j
j
t
n
1
1
t
)
ε
+
(
=
U
T
I
T
S
T
                             (5.174) 
 
Example 5.17 
Design a Fire error correcting code for burst errors of length 
3
p =
 and determine 
the block schemes for the encoding and decoding unites with LFSR and external 
modulo two adders. 
 
Solution 
We choose a primitive polynomial, 
( )
x
g1
, of degree 
5
m =
: 
( )
5
2
1
x
x
1
x
g
+
+
=
 
The code generator polynomial, according to (5.167), will be: 
( )
(
)(
)
10
7
2
5
2
5
1
1
2p
x
x
x
1
x
x
1
1
x
(x)
1)g
(x
x
g
+
+
+
=
+
+
+
=
+
=
−
 
Polynomial g1(x) is a primitive one; it follows that its period is maximum and 
equals to: 
31
1
2
1
2
n
5
m
=
−
=
−
=
 
From (5.168), the code length n is: 
155
31
5
{5;31}
 
multiplier
common 
smallest 
n
=
⋅
=
=
 
The control symbols number being (5.169): 
10
5
5
1
p
2
m
k
=
+
=
−
+
=
, 
it follows that 145 is the number of  information symbols. 
 

294 
5   Channel Coding
 
The block schemes of Fire encoder and decoder implemented with LFSR and 
external adders are: 
 
 
 
 
 
Fig. 5.24 Block schemes for Fire code with g(x) = x10 + x7 + x2 + 1 and p = 3: a) encoder 
and b) decoder 
We will analyze the expression of the syndrome, given by (5.174), for bursts of 
lengths
1
p = , 
2
p =
 and 
3
p =
. 
 
• 
1
p =  
The syndrome register state when the error is in the MR last cell (155) is: 
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
−
0
0
0
1
1
0
0
0
0
0
1
0
0
0
0
1
0
1
1
1
0
0
0
0
0
1
0
0
0
0
0
1
0
1
#
#
"
"
#
#
#
#
#
#
#
"
"
"
"
U
T
 
Therefore, the detector D detects the state 1 of the cell C0 and the 0 states of all 
the other cells. 
• 
2
p =
  
The SR state that must be detected when the first erroneous symbol of the burst 
is in the last MR cell: 
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
+
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
+
−
−
0
0
1
1
0
0
1
0
0
0
0
1
2
1
#
#
#
U
T
U
T
 

5.8   Cyclic Codes 
295
 
so the detector D detects the states 1 of the cells C0 and C1, all the other cells being 
in 0. 
• 
3
p =
 
The SR state that must be detected is: 
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
+
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
+
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
+
+
−
−
−
0
0
1
ε
0
0
0
1
0
1
0
0
0
1
0
ε
0
0
0
0
1
ε
3
2
1
#
#
#
#
U
T
U
T
U
T
 
which means that for a burst as
1
  
0
  
1 
, the symbol 
0
ε =
, so the state detected by 
the SR is:  
T
0]
   
...
   
0
   
1
   
0
   
[0
 
and for the burst 
1
  
1
  
1 
, 
0
ε =
, and the detected state is:  
T
0]
   
...
   
0
   
1
   
1
   
[0
. 
5.8.9   Reed–Solomon Codes 
Reed-Solomon (RS) codes, proposed in 1960, are a variety of non-binary cyclic 
codes (the bits are replaced with characters), extremely convenient in many charac-
ter oriented applications; they are used for independent and burst errors correction. 
A RS word (block) of length n can be expressed as a vector: 
)
v
,...,
v
,
v
,
(v
v
1
n
2
1
0
−
=
                                    (5.175) 
or as a polynomial: 
0
1
1
n
1
n
x
...
x
v(x)
v
v
v
+
+
+
=
−
−
                             (5.176) 
where each character vi is an element of Galois field GF (
k
2
q =
), so it can be ex-
pressed with k bits. 
We can associate decimals to GF(
k
2 ) elements, as follows: 
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
−
=
−
1
n
3
2
1
0
1
0
)
GF(2
2
2
2
k
k
"
"
α
α
α
                            (5.177) 
where α is a primitive element of the GF(
k
2 ) field. 
The parameters of a t errors correcting RS code are: 
• codeword length n given by: 
1
2
n
k −
=
                                           (5.178) 
where k represents the binary Galois field extension: GF(2k). 

296 
5   Channel Coding
 
We have: 
k
m
n
′
+
=
                                         (5.179) 
where m is the information characters number and k′  the control characters  
number.  
 
• control characters number 
)
k
( ′  can be determined with: 
t
2
k =
′
                                           (5.180) 
and the code distance is: 
1
2t
d
+
=
. 
Relation (5.180) indicates the high efficiency of these codes, compared to the bi-
nary BCH codes; at the same relative redundancy the correction capacity is higher.  
 
Example 5.18 
The BCH code (15,7) can correct 
4
8/2
/k
k
t
=
=
′
=
 errors (see relation (5.110)). 
According to (5.180), the RS code (15,7) can correct 
4
8/2
/k
k
t
=
=
′
=
 errone-
ous characters, at the same relative redundancy 
8/15
/n
k
=
′
. 
RS encoding and decoding are very much similar to the binary BCH ones, the 
last one being a particular case of RS codes for 
2
q =
. 
We will analyse RS encoding and decoding both in time and in frequency [6]. 
 
Encoding 
 
• Time encoding 
The t errors correcting RS codes, as well as the BCH codes, are based on the 
existence of a generator polynomial, defined as the smallest common multiple as-
sociated to a number of 2t consecutive elements of the field GF
)
2
(
k .  
The encoding algorithm is based on the fact that v(x) is divisible with g(x), so 
the two polynomials have the same 2t roots: 
)
x
)...(
x
)(
x
(
x)
(
g
1-
2t
p
1
p
p
+
+
+
+
+
=
α
α
α
                         (5.181) 
where p is an arbitrary integer, usually 0 or 1. 
The encoding process, as for all cyclic codes, can lead to a systematic or non-
systematic code. Systematic encoding results in obtaining code words v(x), in 
which the information symbols are on the first m most significant positions and 
the control symbols are on the last k' positions. 
 
Example 5.19 
Design a RS code having length
7
n =
, error correcting capacity 
2
t =
 and sys-
tematically encode an arbitrary information sequence. 
 
Solution 
GF(
k
2 ) is determined with (5.178): 
)
GF(2
3
k
1
2
n
3
k
⇒
=
⇒
−
=
 

5.8   Cyclic Codes 
297
 
The representation of the GF(
3
2 ) field is given in table found in Appendix A.10. 
The number of control characters using (5.180) is: 
4
2t
k
=
=
′
 
It follows RS(7,3), where 3 represents the information characters number (m). 
The generator polynomial is determined using relationship (5.184): 
4
x
2
x
x
4
x
        
x
x
x
x
)
x
)(
)(x
x
)(
x
(
g(x)
2
3
4
3
2
3
4
4
3
2
1
+
+
+
+
=
=
+
+
+
+
=
+
+
+
+
=
α
α
α
α
α
α
α
 
In order to perform the encoding we choose an information sequence:  
⎟⎠
⎞
⎜⎝
⎛
=
1
   
5
3
MSC
i
 
with polynomial representation: 
1
α
α
+
+
=
+
+
=
x
x
1
x
5
x
3
i(x)
4
2
2
2
 . 
Using (5.98) we determine the systematic codeword:  
4
5
4
6
2
4
2
2
4
2t
k'
x
x
x
)
x
x
(
x
i(x)
x
i(x)
x
+
+
=
+
+
=
=
α
α
1
α
α
 
g(x)
/)
x
x
x
(
x
x
          
          
)
x
x
x
x
/(
)
x
x
(
g(x)
/
i(x)
x
2
2
3
3
3
4
2
2
3
2
3
3
4
4
6
2
2t
1
α
α
α
α
α
α
α
α
1
α
α
+
+
+
+
+
+
=
=
+
+
+
+
+
+
=
 
( )
( )
( )
( )
( ) ( )
x
g
x
q
x
x
x
x
x
x
x
v
x
g
x
i
x
 
reamind
2
2
3
3
3
x
i
x
4
5
4
6
2
2t
2t
=
+
+
+
+
+
+
=









	








	

1
α
α
α
α
α
 
(
)1
3
4
4
1
5
3
 
2
3
3
4
MSC
2
=
⎟⎠
⎞
⎜⎝
⎛
=
1
α
α
α
1
α
α
v
 
 
Remark 
Tables for RS generator polynomials having different lengths n and t can be found 
in Appendix A.12. 
 
• Frequency encoding [6] 
All the frequency expressions are obtained using the Fourier transform in the 
Galois field. The frequency domain offers, in certain situations (especially when 
decoding), a series of advantages: easier computing and simpler implementation 
(we use fast computation algorithms for the Fourier transform and also digital sig-
nal processors DSPs). 
The discrete Fourier transform (DFT) of v is a vector V of length n with sym-
bols 
)
GF(2
V
k
k ∈
 given by: 
∑
=
−
=
1
n
0
i
i
ik
k
:
v
α
V
                                         (5.182) 
where α is a primitive element of 
)
GF(2k . 

298 
5   Channel Coding
 
The polynomial associated to an n length word is, in frequency: 
∑
=
−
=
1
n
0
k
k
kx
(x)
V
V
                                      (5.183) 
The Reverse Discrete Fourier Transform (RDFT) is defined as follows: 
k
1
n
0
k
ik
i
n
1
:
V
α
v
∑
=
−
=
−
                                    (5.184) 
so, taking into consideration (5.183) we can write: 
)
V(
n
1
i
i
−
=
α
v
                          (5.185)  
g(x) must divide the codeword, so they have the same roots: 
0
α
α
α
0
α
α
α
=
=
+
+
+
=
=
+
+
+
=
+
−
+
+
+
−
1-
2t
p
1)
1)(n
-
2t
(p
1-
n
1-
2t
p
1
0
1-
2t
p
1
n
1-
n
1
0
p
V
v
...
v
v
)
v(
...
v
...
v
v
)
v(
      (5.186) 
It follows that the first 2t components of V are zero. 
 
Example 5.20 
For RS(7,3) from example 5.19 we determined: 
1
α
α
α
α
α
+
+
+
+
+
+
=
x
x
x
x
x
x
(x)
v
2
2
3
3
3
4
5
4
6
6
 
The frequency expressions of the n components are:   
6
9
17
24
28
39
44
7
7
6
8
15
21
24
34
38
6
6
7
13
18
20
29
32
5
5
6
11
15
16
24
26
4
4
5
9
12
12
19
20
3
3
3
7
9
4
14
14
2
2
3
5
6
4
9
8
1
1
)
(
v
V
)
(
v
V
)
(
v
V
)
(
v
V
)
(
v
V
)
(
v
V
)
v(
V
α
1
α
α
α
α
α
α
α
α
1
α
α
α
α
α
α
α
α
1
α
α
α
α
α
α
α
0
1
α
α
α
α
α
α
α
0
1
α
α
α
α
α
α
α
0
1
α
α
α
α
α
α
α
0
1
α
α
α
α
α
α
α
=
+
+
+
+
+
+
=
=
=
+
+
+
+
+
+
=
=
=
+
+
+
+
+
+
=
=
=
+
+
+
+
+
+
=
=
=
+
+
+
+
+
+
=
=
=
+
+
+
+
+
+
=
=
=
+
+
+
+
+
+
=
=
 
 
Algebraic decoding for RS codes 
 
For RS codes, besides the knowledge of error position given by the locators Xk 
(sufficient for BCH decoding) it is also necessary to determine the error value Yk. 
The error value, added to the erroneous value allows its correction. 

5.8   Cyclic Codes 
299
 
In what follows we will present two of the most efficient decoding algorithms: 
Peterson with Chien search and Berlekamp. The first algorithm will be presented 
in time domain, while the second in frequency. 
Peterson algorithm with Chien search was described at BCH decoding. How-
ever there are some differences between RS and BCH decoding algorithms, and 
all these differences will be emphasised step by step. 
 
1. Error syndrome calculation:  
)
,...,
,
(
2t
2
1
S
S
S
S =
 
2t
i
1
;
)
r(
i
k
t
1
k
k
i
i
≤
≤
∑
=
=
=
X
Y
α
S
                              (5.187) 
2. Determination of 
t
σ coefficients of the error polynomial 
(x)
σ
 as functions of 
the syndromes Sj calculated to 1: 
t
1,
i
,
...
i
t
1
i
t
1
i
t
=
=
+
+
+
−
+
+
0
S
σ
S
σ
S
 
The coefficients table 
)
f( i
t
S
σ =
 valid for BCH codes can not be used for RS 
codes because the equality 
2
k
2k
α
α
=
 is not valid any more. We give a coeffi-
cients table for 
1
t =  and 
2
t =
, and the reader can easily determine the coeffi-
cients 
t
α  for 
2
t >
. 
Table 5.12 
t
σ  coefficients of the error polynomial for RS codes 
t 
i
σ  
1 
2
1
1
S
S
σ =
 
2 
(
) (
)
(
) (
)
3
1
2
2
2
3
4
2
2
3
1
2
2
2
3
4
1
1
S
S
S
S
S
S
σ
S
S
S
S
S
S
S
σ
+
+
=
+
+
=
 
 
3. Determination of locators using Chien search, identical for BCH and RS codes: 
j
n
k
?
ij
t
1
i
i
    
n
1,
j
   
1,
−
=
=
=
=
∑
α
X
α
σ
 
4. Determination of  
k
Y  – the error values  
The value 
k
Y  is found starting from (5.187): 
i
k
t
1
k
k
i
i
)
(
X
Y
α
r
S
∑
=
=
=
 
 
 
 
    
where the locators 
k
X  were determined at 3 and the syndromes Si calculated at 1. 
 

300 
5   Channel Coding
 
This relation represents a linear system of t equations with t unknowns Yk: 
t
t
t
t
t
2
2
t
1
1
2
2
t
t
2
2
2
2
1
1
1
1
t
t
1
2
2
1
1
1
...
...
...
...
S
X
Y
X
Y
X
Y
S
X
Y
X
Y
X
Y
S
X
Y
X
Y
X
Y
=
+
+
=
+
+
=
+
+
⎪
⎪
⎩
⎪⎪
⎨
⎧
                             (5.188) 
Using Cramer rule, the solution is determined as follows:  
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
′
′
′
′
′
′
=
D
D
D
D
D
D
Y
t
2
1
...
                                   (5.189) 
in which: 
t
t
t
2
t
1
2
t
2
2
2
1
t
2
1
...
.
.
.
.
...
...
X
X
X
X
X
X
X
X
X
D =
′
                                     (5.190) 
and 
t
1,
j , j
=
′
D
 are characteristic determinants of the system (5.188). 
The expressions for the coefficients 
k
Y  are given in table 5.13, for 
1
k =  and 
2
k =
; the table can be easily completed for 
2
k >
 using (5.188) to (5.190). 
Table 5.13 Yk coefficients for RS codes (t=1,2) 
t 
k
Y  
1 
2
2
1
1
S
S
Y =
 
2 
2
2
2
1
2
1
1
2
2
1
2
1
2
2
1
1
X
X
X
S
X
S
Y
X
X
X
S
X
S
Y
+
+
=
+
+
=
 
 
5. Error correction 
Correcting the character 
j
n−
r
 whose position 
k
X  was determined at step 3 is 
performed with: 
k
j-
n
j-
n
Y
r
v
⊕
=
                                       (5.191) 
 

5.8   Cyclic Codes 
301
 
Example 5.21 
Let us consider RS(7,3) with g(x) determined in example 5.19 and the received 
word:  
⎟⎠
⎞
⎜⎝
⎛
=
1
  
3
  
4
  
4
  
2
  
5
  
0
 
MSC
r
  
Determine whether r is erroneous or not. If errors do exist, apply Peterson algo-
rithm with Chien search and determine the correct word. 
 
Solution 
( )
1
α
α
α
α
α
r
+
+
+
+
+
=
x
x
x
x
x
x
2
2
3
3
3
4
5
4
 
1. 
( )
( )
( )
( )
(
)
0
α
α 
α
S
0
α
α
α
5
5
3
4
4
5
3
3
5
2
2
3
1
=
⇒
=
=
=
=
=
=
=
=
α
r
S
α
r
S
α
r
S
α
r
S
 
 
2.  
(
) (
)
(
)
3
5
3
10
5
5
3
1
2
2
2
3
4
1
1
α
α
α
α
/
α
α
/
=
+
=
+
+
=
S
S
S
S
S
S
S
σ
 
(
) (
)
(
)
3
5
3
10
10
3
1
2
2
2
3
4
2
2
α
α
α
α
/
α
/
=
+
=
+
+
=
S
S
S
S
S
S
σ
 
3.  
6
1
1
2
2
1
1
1
α
 :
erroneous
 
is
 6
1
-
7 
 j
-
n
position 
 
so
   
1
 
  
1
j
=
=
=
=
+
=
⋅
⋅
X
α
σ
α
σ
 
1 
 
  
7 
j
1 
 
  
6 
j
1 
 
  
5 
j
1 
 
  
4 
j
α 
 
 :
erroneous
 
is
 4 
 3
- 7 
 j -
n 
position 
 
so
  
1 
  
3 
j
α
  
2
j
7
2
2
7
1
1
6
2
2
6
1
1
5
2
2
5
1
1
4
2
2
4
1
1
4
1
3
2
2
3
1
1
2
2
2
2
2
1
1
≠
+
=
≠
+
=
≠
+
=
≠
+
=
=
=
=
=
+
=
=
+
=
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
α
σ
α
σ
α
σ
α
σ
α
σ
α
σ
α
σ
α
σ
X
α
σ
α
σ
α
σ
α
σ
 
4. 
2
2
2
2
1
2
2
1
1
α
X
X
X
S
X
S
Y
=
+
+
=
  
2
2
2
2
1
2
1
1
2
α
X
X
X
S
X
S
Y
=
+
+
=
 
5.  
1
3
3
2
4
4
2
1
6
6
=
⊕
=
⊕
=
=
⊕
=
⊕
=
α
α
Y
r
v
α
0
Y
r
v
 
 
The correct word is 
(
)1
  
3
  
4
  
4
  
1
  
5
  
3
=
v
. 
 

302 
5   Channel Coding
 
Berlekamp – Massey algorithm 
 
Improvements of Peterson algorithm were brought by Berlekamp and Massey [6]. 
The algorithm can be implemented in time as well as in frequency, being preferred 
the frequency approach for powerful correcting codes 
5)
(t >
, because of its high 
processing speed. As we will see later on, in the frequency domain we deal with 
vectors having the dimension 2t, whereas in time the vectors dimension is n; n is 
usually much larger than 2t, resulting a higher processing volume and subse-
quently, a longer decoding time. 
In what follows we give a brief presentation of Berlekamp algorithm in fre-
quency omitting the demonstrations, which can be found in [6]. 
Let us consider a received word affected by t additive errors: 
1
-
n
i
0 ;i
i
i
≤
≤
+
=
e
v
r
                                        (5.192) 
where 
0
e ≠
i
 on the t erroneous positions and zero elsewhere. 
Applying the DFT (see 5.182), we obtain: 
1
n
i
0
  ;i
i
i
−
≤
≤
+
=
E
V
R
                                   (5.193) 
As shown in (5.186), the first 2t components of the vector V are zero, so: 
1
2t
k
0
  ;
k
k
k
−
≤
≤
=
=
E
R
S
                                (5.194) 
This relation shows that, at receiver, one can easily determine the 2t compo-
nents of the error word from the received word transposed in the frequency  
domain. 
The idea used in Berlekamp algorithm is to generate the other 
t
2
n−
 compo-
nents of the error word from the free evolution of a feedback shift register initial-
ized with the first 2t components of this error word. 
The algorithm allows determining the error locator polynomial 
( )
x
Λ
 which is 
exactly the polynomial of the LFSR initialized with the first 2t components Sk: 
∏
+
=
=
t
1
i
i
)
(x
:
(x)
1
α
Λ
                                         (5.195) 
If i is an erroneous position, than 
-i
α  is a root of the locator polynomial: 
(
)
0
α
=
−i
Λ
                                               (5.196) 
According to (5.184), the time components of the error vector are: 
0
α
E
α
e
≠
=
∑
=
−
−
=
⋅
−
)
E(
n
1
i
k
1
n
0
k
k
i
i
                               (5.197) 
If i is not an erroneous position, 
-i
α  is not a root of the locator polynomial, so: 
(
)
0
α
≠
−i
Λ
                                              (5.198) 

5.8   Cyclic Codes 
303
 
and the error value, in time, will be: 
0
α
e
=
=
−)
E(
i
i
                                          (5.199) 
It results: 
1
n
0,
i
;
)
)E(
Λ(
i
i
−
=
=
−
−
0
α
α
                                 (5.200) 
so ( ) ( )
x
E
x
Λ
 must be a multiple of the polynomial: 
)
(x
1
x
n
0
i
i
n
∏
+
=
+
=
−
α
                                        (5.201) 
or: 
( ) ( )
(
)1
x
  
mod
  
0
x
E
x
n +
=
Λ
                                    (5.202) 
Starting from (5.202) we get the following equation system: 
1
n
0,
j
,
E
t
1
k
k
j
k
j
−
=
∑
=
=
−
E
Λ
                                 (5.203) 
The system (5.203) can be divided into two sub-systems: 
1
0,2t
j
,
t
1
k
k
j
k
j
−
=
∑
=
=
−
E
Λ
E
                              (5.203.a) 
a system with t known coefficients 
i
E  and as unknowns the locator polynomial 
coefficients 
k
Λ , respectively: 
1
n
2t,
j
,
t
1
k
k
j
k
j
−
=
∑
=
=
−
E
Λ
E
                            (5.203.b) 
The subsystem (5.203.b) determines the other 
t
2
n−
 components of the error 
vector in the frequency domain, obtained by the free evolution of the feedback 
shift register. 
The free evolution of this LFSR leads to extremely long computation time for 
large codeword lengths. 
This can be overcome in the following way: 
 
• re-write relation (5.202) as: 
1)
Γ(x)(x
Λ(x)E(x)
n +
=
                                      (5.204) 
• computing (5.204) at 
i
−
α
, index i expressing the erroneous position, we obtain 
a non-determination which can be removed by derivation: 
(x)
Γ'
Γ(x)
nx
1)
(x)(x
Γ'
(x)
Λ(x)E'
(x)E(x)
Λ'
1
n
n
+
+
+
=
+
−
            (5.205) 

304 
5   Channel Coding
 
Replacing x with 
i
−
α
, it results: 
)
Γ(
n
)
)E(
(
Λ'
i
i
i
i
−
−
−
=
α
α
α
α
                                   (5.206) 
and according to (5.184), we have: 
)
(
Λ'
)
Γ(
)
E(
n
1
i
i
i
i
i
−
−
−
=
=
α
α
α
α
e
                                   (5.207) 
These are exactly the error vector components in time domain. 
The polynomials 
( )
x
Λ
 and 
( )
x
Γ
 are calculated applying Berlekamp and 
Massey theorem, which states the followings: 
 
• Let us consider 
1-
2t
1
0
 v
...,
 ,
v
,
v
 given and A(x), B(x) two polynomials. For the 
following initial conditions:  
0
r 
0,
L
,
x
(x)
A
0,
(x)
Γ
1,
(x)
B
1,
(x)
Λ
0
1
(0)
(0)
(0)
(0)
=
=
−
=
=
=
=
−
,  
the next 2t iterations: 
 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
⎥⎦
⎤
⎢⎣
⎡
∂
−
∂
=
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
⎥⎦
⎤
⎢⎣
⎡
∂
−
∂
=
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
−
−
−
−
−
−
(x)
A
(x)
Γ
)x
(1
Δ
Δx
1
(x)
A
(x)
Γ
(x)
B
(x)
Λ
)x
(1
Δ
Δx
1
(x)
B
(x)
Λ
1)
(r
1)
(r
r
1
r
(r)
(r)
1)
(r
1)
(r
r
1
r
(r)
(r)
                        (5.208) 
where 
⎩
⎨
⎧
−
≤
≠
=
∂
−
         
          
otherwise
   
0,
1
r
2L
or 
 
0,
Δ
 
if
 
1,
1
r
r
r
                              (5.209) 
∑
=
=
−
−
r
0
k
k
1
r
r
Δ
E
Λ
                                        (5.210) 
Δ  is also known as the discrepancy between the calculated LFSR output and the 
known values of this output: 
(
)
1
r
1
r
r
L
r,
L
max
L
−
−
−
=
                                    (5.211) 
determine the two polynomials. 
In other words, we have: 
Λ(x)
(x)
Λ(2t)
=
 and 
Γ(x)
(x)
Γ(2t)
=
. 
Although we have already discussed the case in which 
0
p =
, this algorithm 
can also be implemented in the general case, when 
0
p ≠
, but with some minor 
changes: 
 

5.8   Cyclic Codes 
305
 
– 
when initializing:
( )( )
1-
-p
0
x
x
A
−
=
 
– 
when computing the discrepancy: 
∑
=
=
+
−
−
r
0
k
p
k
1
r
r
r
E
Λ
Δ
. 
The flow-chart of Berlekamp-Massey algorithm is presented in Fig. 5.25. 
 
Example 5.22 
We will decode the word r from example 5.21, using Berlekamp–Massey algorithm. 
⎟⎠
⎞
⎜⎝
⎛
=
1
  
3
  
4
  
4
  
2
  
5
  
0
 
MSC
r
 
1. Syndrome calculation: 
1,2t
i
   
,
E
S
i
i
=
=
 
( )
3
1
α
α
r
S
=
=
 
 
( )
5
2
2
α
α
r
S
=
=
 
( )
5
3
3
α
α
r
S
=
=
 
( )
0
α
r
S
=
=
4
4
 
(
)
0
α
α
α
S
,
,
,
5
5
3
=
 
 
Initializing the algorithm: 
 
3
1
1
α
S
Ε
=
=
 
 
5
2
2
α
S
Ε
=
=
 
 
5
3
3
α
S
Ε
=
=
 
0
S
Ε
=
=
4
4
 
( )
1
x
x
ǹ
1
p
=
=
−
, 
( )
1
x
Λ
=
, ( )
0
x
Γ
=
, ( )
1
x
Ǻ
=  
0
r
0,
L
=
=
 
 
Iteration 
1
r =  
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡+
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
=
∂
=
−
=
≠
=
=
0
α
0
α
α
1
α
α
1
0
α
α
1
0
α
E
x
B(x)
Λ(x)
x
A(x)
Γ(x)
x
B(x)
Λ(x)
x
B(x)
Λ(x)
1
1
L
r
L
 ;
Δ
Λ
Δ
3
4
3
4
3
4
3
1
3
1
0
 
 
 

306 
5   Channel Coding
 
Iteration 
2
r =
 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥⎦
⎤
⎢⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡+
=
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
+
+
=
⎥⎦
⎤
⎢⎣
⎡
⎥⎦
⎤
⎢⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
=
∂
⇒
−
>
≠
=
+
=
0
α
0
α
1
α
α
1
α
α
α
1
0
α
1
0
α
E
E
x
B(x)
Λ(x)
x
x
A(x)
Γ(x)
x
x
x
x
x
B(x)
Λ(x)
x
x
B(x)
Λ(x)
0
1
r
2L
;
Δ
Λ
Λ
Δ
3
4
2
4
5
3
2
1
1
2
0
 
Iteration 
3
r =
 
4
2
1
3
0
Λ
Λ
Δ
α
E
E
=
+
=
 
2
L
r
L
1,
1
r
2L
 ;
Δ
3
=
−
=
=
∂
⇒
−
≤
≠0
 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
+
+
+
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
x
x
B(x)
Λ(x)
x
A(x)
Γ(x)
x
x
x
B(x)
Λ(x)
x
B(x)
Λ(x)
6
3
3
4
5
3
2
3
3
4
α
α
0
α
α
1
α
α
α
α
1
0
α
α
1
 
Iteration 
4
r =
 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
+
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
+
+
+
=
⎥⎦
⎤
⎢⎣
⎡
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
⎥⎦
⎤
⎢⎣
⎡
=
∂
⇒
−
>
≠
=
+
=
2
6
2
3
2
2
5
3
2
3
3
2
3
2
2
2
3
1
x
x
x
B(x)
Λ(x)
x
x
A(x)
Γ(x)
x
x
x
x
B(x)
Λ(x)
x
x
B(x)
Λ(x)
0
1
r
2L
 ;
Δ
Λ
Λ
Δ
α
α
α
0
α
1
α
α
α
α
1
0
α
1
0
α
E
E
 
 
 
 
 
 
 
 
 
 
 
 

5.8   Cyclic Codes 
307
 
0
L
,
x
(x)
A
0,
(x)
Γ
1,
(x)
B
1,
(x)
Λ
0
1
(0)
(0)
(0)
(0)
=
−
=
=
=
=
−
0
r;
1
0,2t
k
;
S
E
k
k
=
−
=
=
∑
=
−
−
=
r
0
k
k
1
r
r
r
E
Λ
Δ
⎥
⎦
⎤
⎢
⎣
⎡
⎥⎦
⎤
⎢⎣
⎡
∂
=
⎥
⎦
⎤
⎢
⎣
⎡
−
−
−
(x)
B
(x)
Λ
0
Δ
Δx
1
(x)
B
(x)
Λ
1)
(r
1)
(r
1
r
(r)
(r)
⎥
⎦
⎤
⎢
⎣
⎡
⎥⎦
⎤
⎢⎣
⎡
∂
−
∂
=
⎥
⎦
⎤
⎢
⎣
⎡
−
−
−
(x)
A
(x)
Γ
)x
(1
Δ
Δx
1
(x)
A
(x)
Γ
1)
(r
1)
(r
r
1
r
(r)
(r)
0
)
Λ(α
i =
−
)
(α
)/Λ/
Γ(α
α
r
v
t
i
i
i
i
−
−
+
=
 
Fig. 5.25 Flow-chart corresponding to Berlekamp-Massey algorithm.  

308 
5   Channel Coding
 
The two polynomials are: 
 
( )
2
3
3
x
x
x
Λ
α
α
1
+
+
=
, 
( )
2
3
x
x
x
Γ
α
α
+
=
 , 
(
)
( )
0
α
α
1
α
α
=
+
+
=
=
−
5
4
6
Λ
Λ
 . 
0
α
0
α
0
α
0
α
0
α
α
1
α
0
α
≠
≠
≠
≠
=
+
+
=
≠
−
−
−
−
−
−
)
Λ(
)
Λ(
)
Λ(
)
Λ(
)
Λ(
)
Λ(
0
1
2
3
2
6
4
5
 
It follows that the erroneous components are 6r  and 4r ; the set of the errone-
ous positions is 
}
4,6
{
=
Ε
. 
The first order formal derivative of the 
( )
x
Λ
 polynomial is: 
 
)
x
(
(x)
Λ'
j
i
j
E
i
i
1
α
α
+
∏
∑
=
≠
∈
 
3
4
6
6
4
4
6
)
x
(
)
x
(
(x)
Λ'
α
α
α
1
α
α
1
α
α
=
+
=
+
+
+
=
 
6
6
)
Γ(
)
Γ(
α
α
α
=
=
−
 
2
4)
Γ(
α
α
=
−
 
2
2
3
6
6
6
6
α
α
0
α
α
α
r
v
=
+
=
+
=
 
1
α
α
α
α
α
r
v
=
+
=
+
=
3
3
2
4
4
4
 
The corrected word is:  
(
)1
  
3
  
4
  
4
  
1
  
5
  
3
=
v
. 
 
RS coding and decoding using linear feedback shift registers LFSR 
 
For binary codes there are various ways of implementing the encoder and decoder 
using LFSRs. 
The RS codes are character oriented and require mathematical operations over 
GF
)
2
( k . The practical implementations require implementing these operations 
and expressing the characters in the Galois fields. All mathematical operations  
 
 

5.8   Cyclic Codes 
309
 
over GF
)
2
( k  are done by specialised circuits that perform additions and multipli-
cation over the same fields. 
 
• Addition circuit over GF(2k) 
One possible circuit to realise the summation over GF
)
2
(
k  is given in Fig. 5.26: 
 
Fig. 5.26 Summating circuit over GF(2k) 
The addition of two elements from GF
)
2
( k  is done summating the binary 
numbers on k digit: 
(
)
(
).
b 
...
 
b 
b
b
a 
...
 
a 
a
a
k
1
0
k
1
0
=
=
 
The circuit operates as follows:  
 
1. Load the k cells of the register with the binary number on k digits correspond-
ing to the element. 
2. At each clock cycle the register is loaded with the adder parallel output. 
3. Put the results in the register. 
 
Example 5.23 
Let us consider 
( )
3
3
2
GF
 ,
∈
α
α
. The shift register will have 3 cells. The two ele-
ments are expressed as follows: 
(
)
(
)
0
  
1
  
0
1
  
1
  
0
3
=
=
α
α
 
Adding the two numbers, the register will contain at step 2 the following array: 
(
)1
  
0
  
0
3
=
+ α
α
 
 

310 
5   Channel Coding
 
• Multiplication circuit over GF(2k) 
Multiplying two elements in Galois fields is, in fact, multiplying the polyno-
mial representation of the two numbers (i.e. two polynomials of maximum degree 
1
-
k
) followed by a k-th order the modulo generator polynomial reduction. 
 
Example 5.24 
 
• Multiplication circuit over GF
)
2
( 4  
Let us consider ( ) (
)
3
2
1
0
b
b
b 
b
x
b
=
 and ( )
x
x
a
=
.  
We have  ( )
( )
x
x
a
  
and
  
x
b
x
b
x
b
b
x
b
3
3
2
2
1
0
=
+
+
+
=
 
The generator polynomial of GF
)
2
( 4  is: ( )
1
x
x
x
p
4
+
+
=
 
( )
( )
(
)
(
)
3
2
2
1
3
0
3
3
3
2
2
1
0
4
3
3
2
2
1
0
x
b
x
b
x
b
b
b
  
       
          
1
x
b
x
b
x
b
x
b
x
b
x
b
x
b
x
b
x
b
x
a
+
+
+
+
=
=
+
+
+
+
=
+
+
+
=
⋅
 
This relation determines the following way of implementation: 
⊕
 
Fig. 5.27 Multiplying circuit with α over GF(24) 
There are several steps that need to be followed in order to get the final result: 
 
1. Loading in parallel the register with the bits corresponding to the element b. 
2. Shift to the right the register content. 
3. The register parallel output gives the multiplication result. 
In practice, high speed RS encoders/decoders are implemented using special-
ised circuits by Advanced Hardware Architecture Inc: the AHA 4510 series 
(
)
5
e =
, AHA 4010 (
)
10
e =
, AHA 4600 (
)
16
e =
. 
 
Applications 
Reed–Solomon codes are widely used for error protection in satellite transmis-
sions, in storage on CDs and DVDs, applications in which powerful correcting 
codes are required in order to correct both independent and burst errors. In most 
applications RS codes are concatenated with convolutional codes. 
 
 
 

5.8   Cyclic Codes 
311
 
Example 5.25 
The block scheme of a CD player is given bellow: 
R
D
D
c
i
×
=
•
•
R
 
 
D
D
t
c
/
•
•
=
 
Fig. 5.28 CD player – block scheme 
Among others, the command computer plays the songs following the options 
selected by the listener on the control panel. 
When recording sounds on CDs, the error protection is realised using a RS(7,3) 
code. 
Knowing that the sound bandwidth is 5Hz–20kHz and that sampling is done at 
Nyquist frequency, determine the information rate of the encoded source (under 
the assumption of equally probable source), as well as the capacity of the trans-
mission channel at the digital output. Which is the capacity of a CD, if it can be 
played for 74 minutes? What is the decoder output sequence if the reading laser 
provides 
⎥⎦
⎤
⎢⎣
⎡
=
0
  
0
  
2
  
4
  
0
  
2
  
0
 
MSC
r
? 
 
Solution 
 
a. 
kHz
 
40
f
2
f
max
s
=
=
– sampling frequency 
(
)
)
GF(2
 :
extension
 
of
order 
 
k the
       
          
          
 
and
 
symbols
n 
informatio
 
of
 
no.
 
m
 
 where
,
D
mk
f
D
assumption
 
ideal
 
D
C
3
c
s
i
i
=
=
=
=
•
•
•
 
Mb/s
 
0,36
C
bits
 
21
3
7
k
n
n
Mb/s
 
0,84
n
f
D
word
word
s
i
=
=
⋅
=
⋅
=
=
=
•
 

312 
5   Channel Coding
 
b. The codeword duration is given by the sampling period, so the transmission 
time for 
21
n =
 bits is 
25μ5
f
1
T
s
s
=
=
 
The memory capacity of a CD is determined by the decision quantity corre-
sponding to the encoded information: 
es)
MB(megabyt
 
466,2
60
74
 
Mb/s
 
84
,0
D
  
so
  
t
D
D
c
c
c
=
⋅
⋅
=
=
•
 
c. Having used RS(7,3), one may easily determine that each character 
( )
3
i
2
GF
∈
v
, so it will be expressed with 
3
k =
 bits 
We determine: 
( )
3
1
α
α
r
S
=
=
 
(
)
6
2
2
α
α
r
S
=
=
 
( )
α
α
r
S
=
=
3
3
 
   
(
)
5
4
4
α
α
r
S
=
=
 
Due to the fact that 
0
α
α
S
S
S
≠
+
=
+
4
12
3
1
2
2
, the received sequence is af-
fected by two errors (t = 2). 
Using the formula from Table 5.13 to compute the 
i
σ coefficients, we get: 
α
σ
α
σ
=
=
2
3
1
 
Chien search formula provides the erroneous positions r1 and r2: 
 
α
Χ =
1
 
1
α
Χ
=
=
0
2
 
1
Υ =
1
 
1
α
Υ
=
=
0
2
 
 
It follows that errors occur on control positions; the decoder output (Peterson 
algorithm with Chien search decoding) is: 
[
] 0 0 0  0 1 0  0 0 0 
 0
  
2
  
0
 
MSC
=
⎥⎦
⎤
⎢⎣
⎡
=
i
 
5.9   Convolutional Codes  
5.9.1   Representation and Properties 
Convolutional codes have been introduced by P. Elias in 1955 as an alternative to 
block codes. Unlike the block codes, convolutional codes have encoding memory, 

5.9   Convolutional Codes 
313
 
that is, at a certain moment of time, the n encoder outputs depend not only on the 
m inputs at that moment, but also on the previous M information blocks (an in-
formation block contains m bits). Important contributions to the development of 
these codes were brought by: J. Wozencraft who, in 1961, proposed the sequential 
decoding, J. Massay who presented in 1963 the threshold decoding and A. Viterbi 
who proposed in 1967 the minimum distance decoding algorithm, known as 
Viterbi algorithm. 
These codes are used in practice due to some essential advantages: great detec-
tion and correction capacity for both independent and burst errors and, in many 
cases, simplicity in implementation. The disadvantage, due to the great redun-
dancy, imply by the inefficient use of the band, which this is why these codes are 
used in applications for which the bandwidth is not critical, especially for satellite 
transmissions and space communications. 
Figure 5.29 shows an intuitive presentation of these codes, in comparison with 
the block ones. 
 
Convolutional codes – characteristic parameters 
 
Constraint (M) represents the number of information blocks needed for determin-
ing a control symbol. 
Constraint length (K) represents the number of information symbols needed for 
determining a control symbol. 
m
M
K
⋅
=
                                            (5.212) 
The code distance has the same significance as for block codes, with the re-
mark that it is defined on a number of frames (blocks) N (used notation dN instead 
of d). 
Coding rate (R) has the same significance as for block codes (5.5), and it repre-
sents the relation between the number of information bits (m) and the length of an 
encoded block (n). 
Code type
block
convo-
lutional
systematic
non-
systematic
systematic
non-
systematic
Encoder-
input
Encoder-
output
m+k=n
n
m+k=n  n
M
M
...
...
...
...
Decoder-
input
Decoder-
output
m+k=n
n
n          n
...
...
N≥M
n
n
N≥M
m
m
m  m     m
m  m     m
n
n
m  m     m
m  m     m
m
m
M
M
 
Fig. 5.29 Comparison between block and convolutional codes 

314 
5   Channel Coding
 
Remarks 
The value of R is different for the two types of codes; for block codes R is big 
(>0.95) due to the low relative redundancy; for the convolutional codes R is  
usually small (typical values are 1/2, 1/3, 3/4) reflecting their high relative  
redundancy. 
As we have already shown in 5.1, the coded bit rate 
)
D
(
c
•
depends on R: 
R
D
D
i
c
•
•
=
 
This justifies the need for a larger bandwidth, from where the possibility to  
use such encoding for transmissions on channels for which the bandwidth is not 
critical. 
If the information sequence is of finite length, containing a number of L  
blocks, the encoded sequence is of length: 
M)
n(L +
. In this case the coding rate 
will be: 
(
)
M
L
n
L
m
R
+
⋅
=
                                           (5.213) 
If L>>M, then 
1
M)
L/(L
≅
+
, so (5.213), valid for convolutional codes is iden-
tical with expression (5.5) for block codes. 
If 
1/2
R =
, it results 
1
m = , so basically, at the input the information is not  
divided in blocks but processed continuously, hence the name of continuous 
codes. 
The name of convolutional codes comes from the fact that the k control sym-
bols for systematic codes (the n symbols for non–systematic structures) are ob-
tained from the digital convolution between the information sequence (i) and the 
generator polynomial (g). 
5.9.2   Convolutional Codes Encoding 
For convolutional codes each of the k control symbols (systematic codes) and each 
of the n symbols (for the non-systematic type) are obtained from K information 
symbols by multiplying the information sequence with the corresponding genera-
tor polynomials (Fig. 5.30). 
 
 

5.9   Convolutional Codes 
315
 
(x)
i(1)
(x)
i(2)
(x)
i(m)
(x)
c (1)
(x)
c (2)
(x)
c (k)
 
(x)
i(1)
(x)
i(2)
(x)
i(m)
(x)
u(1)
(x)
u(2)
(x)
u(n)
 
 
Fig. 5.30 Block scheme of: a) systematic and b) non systematic convolutional encoder; 
(ISR - Information Shift Register) 
Unlike for block codes, for convolutional codes the information as well as the 
encoding process take place continuously; it follows that, for polynomial represen-
tation, the polynomials will be continuous. 
( )( )
( )
( )
( )
( )( )
( )
( )
( )
( )( )
(
)
( )
( )
( )( )
( )
( )
( )
( )( )
( )
( )
( )
( )( )
( )
( )
( )
...
x
c
x
c
c
x
c
...
x
c
x
c
c
x
c
...
x
c
x
c
c
x
c
...
x
i
x
i
i
x
i
...
x
i
x
i
i
x
i
...
x
i
x
i
i
x
i
2
k
2
k
1
k
0
k
2
2
2
2
1
2
0
2
2
1
2
1
1
1
0
1
2
m
2
m
1
m,
0
m
2
2
2
2
1
2
0
2
2
1
2
1
1
1
0
1
+
+
+
=
+
+
+
=
+
+
+
=
+
+
+
=
+
+
+
=
+
+
+
=
#
#
                              (5.214) 
( )( )
( )
( )
( )
( )( )
( )
( )
( )
( )( )
( )
( )
( )
...
x
u
x
u
u
x
u
...
x
u
x
u
u
x
u
...
x
u
x
u
u
x
u
2
n
2
n
1
n
0
n
2
2
2
2
1
2
0
2
2
1
2
1
1
1
0
1
+
+
+
=
+
+
+
=
+
+
+
=
#
                           (5.215) 

316 
5   Channel Coding
 
For this representation the bits succession at encoder input and output will be: 
(Fig. 5.31) 

	

m
m
0
2
0
1
0 ...i
i
i

	

m
m
1
2
1
1
1 ...i
ii

	


	

k
k
0
2
0
1
0
m
m
0
2
0
1
0
...c
c
c
...i
i
i

	


	

k
k
1
2
1
1
1
m
m
1
2
1
1
1
...c
c
c
...i
ii

	

n
n
0
2
0
1
0
...u
u
u

	

n
n
1
2
1
1
1
...u
u
u
N
N
 
 
Fig. 5.31 Representation of the information at: a) encoder input; b) systematic encoder out-
put; c) non-systematic encoder output 
The control symbols
(x)
c(j)
 and the encoded symbols 
(x)
u(i)
are determined 
multiplying the information polynomial i(x)  with a generator polynomial 
(x)
g(j)
, 
multiplication which represents the numerical convolution between © and 
(j)
g
. 
The number of generator polynomials necessary for encoding is: 
k
m×
                                               (5.216) 
for systematic codes and 
n
m×
                                              (5.217) 
for non-systematic codes. 
From these generator polynomials, at least one must be of degree K–1, due to the 
fact that we must use K information symbols to determine one (j)
c
or 
(j)
u
symbol: 
( )( )
( )
( )
( )
1
-
K
k
  
...,
x
g
x
g
g
x
g
k
j
k
j
1
j
0
j
≤
+
+
+
+
=
"
                     (5.218) 
In this case, for a systematic code, we have: 
( )( )
( ) ( )( )
x
g
x
i
x
c
j
j
=
                                         (5.219) 
or 
( )
( )j
j
g
i
c
∗
=
                                           (5.219.a) 
where ∗ represents the numerical convolution between © and g, all the operations 
being modulo 2.The term of order j of the convolution is: 
( )
( )
( )
( )
( )j
k
k
l
j
1
1
l
j
0
l
k
0
i
j
i
i
l
j
l
g
i
g
i
g
i
g
i
c
−
−
=
−
+
+
+
=
∑
=
"
                      (5.220) 
where 
i
1
  
for  
  
0
:
i i-1
<
∀
=
. 

5.9   Convolutional Codes 
317
 
In the same way, for a non-systematic code, we have: 
( )( )
( ) ( )( )
x
g
x
i
x
u
j
j
=
                                           (5.221) 
( )
( )j
j
g
i
u
∗
=
                                             (5.221.a) 
Encoding relations (5.219) and (5.221) as well as their corresponding convolu-
tions are linear, justifying why convolutional codes are linear codes. 
One can easily built a convolutional encoder using a LSR with K cells and 
modulo two adders with inputs connected to the LSR according to the generator 
polynomials 
(x)
g(j)
. 
 
Example 5.26 
Be the convolutional code with parameters 
2
/
1
R =
 and  
3
K =
; encode the fol-
lowing information sequence in both cases, systematic and non-systematic:  
]
0
0
1
0
1
1
0
[
=
LSB
i
. 
 
Solution 
2
/
1
m/n
R
=
=
, so the codeword contains two symbols: one information symbol 
(
1
m = ) and one control symbol (
1
k = ) (for the systematic code). 
Knowing that the constraint length is 
3
M
m
K
=
×
=
and 
1
m = , it results that 
in order to perform the encoding we need 3 information bits (blocks). The genera-
tor polynomials will have the maximum degree:  
2
1
3
1
K
=
−
=
−
 
• systematic code: we have only one generator polynomial 
1
1
1
k
m
=
×
=
×
 and it 
must be of degree 2; we choose: 
2
2
x
x
1
or  
  
x
1
g(x)
+
+
+
=
 
• non-systematic code: we have
2
2
1
n
m
=
×
=
×
generator polynomials, from 
which at least one must be of degree 2;  
x
1
or   
  
x
x
1
(x)
g
  
x
1
(x)
g
2
(2)
2
(1)
+
+
+
=
+
=
 
Remarks 
• For non-systematic codes, depending on how we choose the generator polyno-
mials, catastrophic errors may occur. An error is defined as being catastrophic 
if a finite number of errors in transmission produce an infinite decoding errors. 
One sufficient and necessary condition [42] to determine catastrophic errors 
(for codes with 
/n
1
R =
) is that generator polynomials have a common divisor; 
x
1
(x)
g
  
and
  
x
1
(x)
g
(2)
2
(1)
+
=
+
=
 have 
x
1+
 as their common divisor, so 
this determines a catastrophic code. 
(
)(
).
x
1
x
1
x
1
2
+
+
=
+
 

318 
5   Channel Coding
 
• Systematic codes can never be catastrophic and this is another advantage, be-
sides their simplicity. 
In what follows we will determine the encoded sequence for the given © in 
both encoding types using the encoding relations (5.219) and (5.221). 
( )
( )
( ) ( ) (
)(
)
6
3
2
2
4
2
4
2
x
x
x
x
x
1
x
x
x
x
g
x
i
x
c
x
x
x
x
i
+
+
+
=
+
+
+
=
=
+
+
=
 
The vector expression of the control sequence is: 
[
]1
0
0
1
1
1
0
=
c
 
For the systematic code, the encoded sequence is: 
⎥
⎦
⎤
⎢
⎣
⎡
1
0 | 0
0 | 0
1 | 1
0 | 1
1 | 1
1
|
0
 
0
=
0
0 c
i
v
 
Using relation (5.222.a), we will determine the control sequence by numerical 
convolution: 
(
)1
0
0
1
1
1
0
1
0
1
0
0
1
0
1
1
0
=
=
2
1
0
6
5
4
3
2
1
0
g
g
g
i
i
i
i
i
i
i
=
=
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
∗
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
∗g
i
c
 
The components of the control sequence are determined with (5.220): 
1
g
i
g
i
g
i
c
0
g
i
g
i
g
i
c
0
g
i
g
i
g
i
c
1
g
i
g
i
g
i
c
1
g
i
g
i
g
i
c
1
g
i
g
i
c
0
g
i
c
2
4
1
5
0
6
6
2
3
1
4
0
5
5
2
2
1
3
0
4
4
2
1
1
2
0
3
3
2
0
1
1
0
2
2
1
0
0
1
1
0
0
0
=
+
+
=
=
+
+
=
=
+
+
=
=
+
+
=
=
+
+
=
=
+
=
=
=
 
 
Similarly, we encode in the non-systematic case: 
( )( )
( ) ( )( ) (
)(
)
( )( )
( ) ( )( ) (
)(
)
6
5
2
4
2
2
2
6
3
2
2
4
2
1
1
x
x
x
x
x
1
x
x
x
x
g
x
i
x
u
x
x
x
x
x
1
x
x
x
x
g
x
i
x
u
+
+
=
+
+
+
+
=
=
+
+
+
=
+
+
+
=
=
 
The code sequence will be: 
( )
( )
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
1
1 | 1
0 | 0
0 | 0
1 | 0
1 | 1
1
|
0
0
=
2
0
1
0 u
u
v
 
We suggest to the reader to determine the sequence v using the convolution 
(see 5.221.a). 

5.9   Convolutional Codes 
319
 
The encoders with LSRs and their operation is given in Fig. 5.32 a), and  
respectively 5.32 b). 
 
Tn
tn+1
Tn
V
i 
C1
C2
1(i)
2©
1 
0
0
0
0
0
2 
1
1
0
1
1
3 
1
1
1
1
1
4 
0
0
1
0
1
5 
1
1
0
1
0
6 
0
0
1
0
0
7 
0
0
0
0
1
a)
V
T
I 
C1
C2
1(u(1)) 
2(u(2))
1 
0 
0 
0 
0 
0 
2 
1 
1 
0 
1 
1 
3 
1 
1 
1 
1 
0 
4 
0 
0 
1 
1 
0 
5 
1 
1 
0 
0 
0 
6 
0 
0 
1 
0 
1 
7 
0 
0 
0 
1 
1 
b) 
 
 
 
 
 
 
Fig. 5.32 a) Convolutional systematic encoder - block scheme and operation; b) Convolu-
tional non-systematic encoder - block scheme and operation 

320 
5   Channel Coding
 
Remarks 
• All the information sequences must be ended with (
1
K−) zeros, necessary to 
decode the last frame corresponding to the given sequence as well as for flush-
ing the encoding register (trellis termination). 
• From the given example, one may notice the extreme simplicity of the convolu-
tional encoders, a real advantage in their implementation. 
In this example the information sequence length is small: 
5
L =
 and for this 
reason the encoding rate R can not be calculated with (5.5), but with (5.213): 
(
)
(
)
2
1
n
m
R
0,31
16
5
3
5
2
5
1
M
L
n
mL
R
=
=
<
=
=
+
⋅
=
+
=
 
If we had a bigger length for the information sequence, for example 
500
L =
, 
then the two relations would have been identical: 
(
)
(
)
n
m
2
1
3
500
2
500
M
L
n
mL
R
=
≅
+
=
+
=
 
A matrix description of the convolutional codes, using the control matrix H or 
the generator matrix G is given in [2], [28] and [43]. 
5.9.3   Graphic Representation of Convolutional Codes 
A convolutional code can be graphically represented in many ways: with state 
diagrams, tree diagrams and trellis diagrams. 
 
State diagram 
 
A convolutional encoder is made up with a shift register having 
1
K− cells which 
define at one moment of time ti, the encoder state  
(
)
(i)
1
K
(i)
2
(i)
1
(i)
C
C
C
X
−
=
"
 
(
(i)
j
C
 denotes the state of the cell Cj at the moment i). Knowing the register state at 
moment i and the information symbol transmitted at (
1
i+ ) we may determine the 
shift register state at the moment (
1
i+ ). The code sequence generated at the mo-
ment i is completely determined the register state at the moment i:
(i)
X
 and by the 
information symbol ii, so the state 
(i)
X
 represents the encoder history: 
( )
(
)
1
K
i
2
i
1
i
i
i
i
i
X
+
−
−
−
=
"
                                 (5.222) 
The register evolution is a Markov chain, meaning that the transition from one 
state to another is determined only by the previous state: 
(
)
( )
(
)
( )
(
)
(
)
( )
(
)
i
1
i
0
1
i
i
1
i
/X
X
P
X
,
,
X
,
/X
X
P
+
−
+
=
"
                       (5.223) 
The state diagram includes all possible states of the shift register as well as the 
encoded structure when transitioning from one state to another. There are only two 

5.9   Convolutional Codes 
321
 
possible transitions from any state, corresponding to the emission of 0 or 1; it fol-
lows that it is not possible that during one transition to jump from a certain state in 
any other state. 
 
Example 5.27 
We will determine the state diagrams corresponding to the two codes given in  
Example 5.26. 
The shift register used for encoding has 
2
1
K
=
−
 cells, so it will have four dis-
tinct states:  a =  00, b = 10, c = 01, d = 11 
On each branch connecting two states are written the information symbol and 
the encoded structure: i/v. 
 
Fig. 5.33 State diagram of the convolutional code with R = 1/2, K = 3 for a) systematic and 
b) non-systematic type 
Tree Diagram (Encoding Graph) 
 
Although the state diagram completely describes the encoder, it does not give any 
information about its history. The encoding graph adds to the state diagram the 
time dimension. 
The graph is drown beginning with the initial state (t0) zero of the shift register 
(state a). From this state two branches emerge, ongoing up and corresponding to 
the emission of a 0, and another one going down, corresponding to the emission of 
a 1. At the moment t1 from each branch two other branches will emerge, corre-
sponding to the emission of a 0 or 1 and so on. Adding the time dimension to the 
state diagram one may also know the decoder time evolution. 
If the length of the information sequence (L) is big, the branches number grows 
exponentially: 
L
2 , and limits the use of the graph in practice. 
 
Example 5.28 
We will draw the graph corresponding to the systematic code from example 5.26 
emphasizing the evolution of the encoder for the following information sequence: 
[
]1
  
0
  
1
  
1
  
0
=
i
 
 

322 
5   Channel Coding
 
 
Fig. 5.34 The graph corresponding to the systematic convolutional code R = 1/2, K = 3; ― 
the encoded structure for i = [0 1 1 0 1] 

5.9   Convolutional Codes 
323
 
Trellis Diagram 
 
One may notice in Fig. 5.34 that the graph is repeating beginning with t4; for the 
general case, the diagram repeats after K frames (K being the constraint length). 
The first ramification (at moment t1) leads to nodes a and b. At each next rami-
fication the number of nodes doubles; four nodes occur at t2: a, b, c and d and 
eight at t3: 2 nodes a, 2 b, 2 c and 2 nodes d. It can be easily noticed that all the 
branches which emerge from the same state generate the same code sequences and 
this is the reason why the two diagram halves (superior and inferior) are identical. 
It follows that, in Example 5.28 (
3
K =
), the 4-th bit is fed to the encoder from the 
left, while the first bit (i0) is removed from the register without having any influ-
ence on the codeword. 
 
 
Fig. 5.35 Trellis corresponding to the convolutional code R = 1/2, K = 3: a) systematic with 
g(x) = 1 + x2; b) systematic with g(x) = 1 + x + x2; c) non-systematic with g1(x) = 1 + x2, 
g(x) = 1 + x + x2 
 

324 
5   Channel Coding
 
 
Fig. 5.35 (continued) 
Subsequently, the sequences 0 1 1 x y and 1 1 1 x y generate the same code-
words after 
3
K =
 ramifications (frames). This means that any two nodes that 
have the same state at the same moment 
)
K
i(
ti
>
can be connected as they gener-
ate identical sequences. 
In this way, starting from the encoding graph we obtain another graphic repre-
sentation called trellis diagram, from the trellis aspect of the diagram. In the trellis 
representation the encoded sequence will be marked with a continuous line when 
we apply “0”, and with a dot line if the input is “1”. The trellis nodes show the en-
coding register state. At a certain moment of time ti the trellis contains 
1
K
2
− 
nodes, determined by all register distinct states at encoding. Two branches enter 
each node starting with moment tk (Fig. 5.35). 
5.9.4   Code Distance and d∞ 
Compared to block codes, for convolutional codes the code distance depends on 
the number of frames N used during the decoding process. 
For a convolutional code the N-th order code distance (dN) represents the 
minimum value of Hamming distance between any two possible code sequences 
on N frames and different in the initial frame. 
(
)
  
   
,
   
,
d
min  
:
d
1
1
N
N
H
N
v
u
v
u
≠
=
                              (5.224) 
where uN and vN are two code sequences on N frames. 
 
Remarks 
• As for block codes, the branches containing exclusively zeros in the first frame 
are not taken into consideration 
• Relation (5.16): 
min
w
d =
is valid 
• dN  provides the same information about the code control capacity  
The relation: 
1
t
2
dN
+
≥
                                               (5.225) 

5.9   Convolutional Codes 
325
 
shows that the code will correct any combination of t errors and less than t errors 
on N successive frames. 
The minimum number of frames used in the decoding process is M (the con-
straint). In this case: 
min
M
N
d
d
d
=
=
                                        (5.226) 
represents the minimum code distance. 
Unlike for block codes, the decoding process of a convolutional code can be re-
alized using a number of frames N higher than the number of frames used during 
encoding (M); at limit 
∞
→
N
The code distance defined for convolutional codes 
in this last case is 
∞
d
 or d free and it is used for Viterbi or sequential decoding, 
where the decoding memory is basically, unlimited. 
In order to define 
∞
d
 [47], we search branches that start from the zero state 
and return to the same state (except for all zero branch in the first frame). The way 
with the minimum Hamming weight will be 
∞
d
. Obviously, 
N
d
d
≥
∞
, which jus-
tifies the advantage of decoding on the number of frames on which 
∞
d
 operates. 
For most codes (with average or small K), 
∞
d
is obtained in several constraints 
or even in the first constraint. Practically, 
∞
d
 (for high values of K) is obtained 
for (
)K
5
4 ÷
. 
 
Example 5.29 
We will determine
K
min
d
d
=
 and 
∞
d
for the convolutional codes represented in 
Fig. 5.35. 
 
• For the systematic code (a) we have: 
2
/
1
R =
, 
3
K =
,  
2
x
1
g(x)
+
=
 and we 
determine: 
 
Frame  N 
1 
2 
K = 3 
Weight 
dK 
d∞ 
Sequence v 
v1 
v2 
v3 
wi 
 
 
11 
00
01
3
11 
00
10
3
11 
11
01
5
 
11 
11
10
5
3 
3 
 
• For the systematic code (b) : 
2
/
1
R =
, 
3
K =
, 
2
1
x
x
1
(x)
g
+
+
=
 
 
Frame  N 
1 
2 
K = 3 
Weight 
dK 
d∞ 
Sequence v 
v1 
v2 
v3 
wi 
 
 
11 
01 
01 
4 
11 
01 
10 
4 
11 
10 
00 
3 
 
11 
10 
11 
5 
3 
4 

326 
5   Channel Coding
 
• For the non-systematic code (c):
2
/
1
R =
, 
3
K =
, 
2
1
x
1
(x)
g
+
=
 and 
2
2
x
x
1
(x)
g
+
+
=
 
 
Frame  N 
1 
2 
K = 3 
Weight 
dK 
d∞ 
Sequence v 
v1 
v2 
v3 
wi 
 
 
 
11 
01 
11 
5 
 
11 
01 
00 
3 
 
11 
10 
10 
4 
 
11 
10 
01 
4 
3 
5 
 
One must notice that in all three cases, 
∞
d
 is reached in the first constraint 
length 
3
K =
. 
For systematic codes, d∞ is smaller than for non-systematic codes with identi-
cal parameters (R, K). Table 5.14 [42] presents a comparison between these codes 
from 
∞
d
 point of view: 
Table 5.14 d∞ for the systematic and non-systematic codes: R = 1/2 and K∈[2, 8] 
K 
d∞ 
 
systematic 
non-systematic 
2 
3 
3 
3 
4 
5 
4 
4 
6 
5 
5 
7 
6 
6 
8 
7 
6 
10 
8 
7 
10 
5.9.5   Decoding (Viterbi Algorithm, Threshold Decoding) 
Viterbi algorithm (1967) 
 
This algorithm is based on the principle of minimum distance, which was shown 
in 5.6 that is obtained from maximum likelihood decoding (MLD), so it is an op-
timal algorithm. 
Assume i is a message encoded as vi (the code sequence) and received as r. If 
all messages i are equally probable, subsequently vi are equally probable, the error 
probability is minimum if the principle of maximum likelihood decoding is used. 
 
 

5.9   Convolutional Codes 
327
 
If  
(
)
(
)
j
i
   
,
p
p
j
i
≠
>
r/v
r/v
                                     (5.227) 
then the receiver decides that r comes from vi. 
This decoding rule may be implemented quite easily. In 1967, Viterbi showed 
that for a BSC (a hard decoder), the decoding can be achieved by choosing for r 
the sequence vi that has the minimum Hamming distance, thus using a minimum 
Hamming distance decoder. In 1969, Omura demonstrated that Viterbi algorithm 
is, in fact, the maximum likelihood decision (MLD) algorithm. In this case, rela-
tion (5.227) is equivalent to: 
(
)
(
)
j
i  ,
,
d
,
d
j
H
i
H
≠
<
v
r
v
r
                                   (5.228) 
Viterbi algorithm operates on the trellis frame by frame, on a finite number of 
frames, in order to find the encoding path. At each node it calculates the distances 
between the received sequence and all the sequences on the trellis, cumulating the 
distance from one node to another (cumulated distance). In frame K, each node 
has two branches, so there will be two cumulated distances for each node; from 
these we keep the path with the minimum distance (survivor). If in a node there 
are two ways with equal distances, one of them is chosen randomly (the survivor). 
The frame K+1 is analyzed with the survivors from the node K, and so on. The 
analysis is continued on so many frames until there is only one path left; this path 
is considered the correct sequence. 
The number of frames on which the decoding process takes place is called de-
coding window (W). W must be high enough in order to ensure the correct deci-
sion on the oldest transmitted frame. 
Obtaining a unique route is a random variable. Computer simulations have 
shown that: 
(
)K
5
4
W
÷
≅
                                          (5.229) 
gives negligible inaccuracies compared to an infinite memory 
)
W
(
∞
→
 of the 
decoder; this is the reason for dimensioning the decoding windows of Viterbi de-
coders with relation (5.229).  
 
Example 5.30 
Applying Viterbi algorithm, decode the sequence r given by: 
(
)
00
00
11
01
00
1
1
00
01
00
1
1
11
=
r
 
and knowing that the code parameters are: 
2
/
1
R =
, 
3
K =
,  
2
1
x
1
(x)
g
+
=
 and 
2
2
x
x
1
(x)
g
+
+
=
. Suppose r contains two errors situated on the underlined  
positions. 

328 
5   Channel Coding
 
11
11
10
01
11
11
00
01
10
10
01
2
0
4
2
1
1
4
3
6
1
3
2
3
2
00
00
00
00
11
a) 
11 
 
 
11 
10
01
11
00
01
10
11
11
00
00
01
10
10
01
0
1
1
3
1
2
2
4
3
4
3
1
4
3
2
01
00
11
11 
b) 
 
00
01
00
11
11 
0
1
1
1
2
2
3
3
1
2
11 
10
01
00
01
10
11
00
01
01
11
11
00
00
01
10
10
01
3
5
5
1
4
3
4
3
c)
 
Fig. 5.36 Viterbi algorithm example for non-systematic convolutional code with R=1/2, 
K=3, g1(x)=1+x2, g2(x)=1+x+x2 , on variable number of frames: N=3÷12. 

5.9   Convolutional Codes 
329
 
d) 
11 
00
01
00
11 
11
11
10 
01 
00
01
10
11
01
01
00
00
01
10
11 
11 
00 
00 
01 
10 
10 
01 
0 
1 
1 
1
2
2
3
1
2
3
1
3
3
5 
3 
3 
5 
2 
4 
2 
4 
 
 
e) 
11
00 
00
01
00 
11 
11 
0
1
1
1
2
2
3
1
2
3
1
3
3
3
2
2
11
10 
01 
00
01
10
11
01
01
00
00
10
11
11
10
01
11 
11 
00 
00 
01 
10 
10 
01 
3
4 
5 
2 
4
3
4
3
 
 
11
10 
01 
00 
01 
01
01
00
10
11
10
01
00
00
01
10
11 
11 
00 
00 
01 
10 
10 
01 
0
1
1
1
2
1
2
1
3
3
2
2
3
2
3
3
4
4
4
4
2
5
4
3
f) 
11
00
00
01 
01
00 
11 
11 
 
 
 
Fig. 5.36 (continued) 

330 
5   Channel Coding
 
g) 
11
11 
00
00
01
01
00 
11 
11 
0
1
1
1
2
1
2
1
3
3
2
2
3
2
4
4
2
6
2
4
4
5
4
5
4
11
10 
01 
00 
01 
01
01
00
1
1
10
01
0
00
01
11
0
01
01
11 
1
0
00 
01 
10 
1
01 
 
 
10 
11 
11 
00 
00 
01 
10 
2
6
4
4
5
5
5
5
01
01
2
3
10
01
1
11 0
00 
1
01 1
00 1
01 2
2
00
01
2
3
11
00 
01 
10
2
4
4
4
01 
h) 
11
11
00 
00
00
01
01 
00 
11
11 
 
 
i) 
11
11
00
00 
00
00
01
01 
00 
11
11 
1
1
0
0
0
0
0
0
0
1
0
1
1
0
1
1
0
0
1
1
0
0
1
1
1
1
2
2
2
2
4
4
5
5
2
7
4
5
5
6
5
6
0
0
 
 
Fig. 5.36 (continued) 
 

5.9   Convolutional Codes 
331
 
j) 
11
11
00
00 
00
00
01
01 
00 
11
11 
0
1
1
1
1
2
2
2
2
2
2
4
5
2
7
4
5
5
6
5
1
0
0
0
0
1
0
0
0
0
0
1
0
1
0
0
1
1
1
0
0
0
1
1
0
3
3
4
5
6
 
 
1
0
0
0
0
0
0
0
1
1
0
0
1
1
0
1
1
0
0
1
1
0
0
1
1
1
1
2
2
2
2
4
4
5
5
2
7
4
5
5
6
5
6
k
)
11
11
00
00 
00 
00
00
01
01 
00 
11
11 
0
 
 
Fig. 5.36 (continued) 
 
Solution 
We will consider the trellis from Fig. 5.35.c. The decoding process will start by 
calculating the cumulated distances in each node between the received sequence 
and all the sequences on the trellis. After 
3
K =
 frames, each node will contain 
two branches, from which only the survivors will be selected for the frame 
1
K+  
(corresponding to the minimum distances in each node). One must notice that in 
Fig. 5.36.c, there are two nodes that contain paths of equal distances, the survivor 
being chosen randomly. The same situation occurs in f, g, h, i; for Fig.5.36.h we 
illustrated two possible choices. 
In Fig. 5.36.h, a unique route is obtained, after the first 5 frames, and 9 frames, 
respectively, in Fig.5.36.k. In this example, we analyzed the possibility of correct-
ing two errors on twelve frames. The decoding window W has 12 frames, so 
5
d
=
∞
 may be used; this implies that it is possible to correct any combination of 
two errors or less than two errors occurred on 
(
)
15
12
K
5
4
W
÷
=
÷
≅
frames. 
 

332 
5   Channel Coding
 
Example 5.31 
In what follows assume the occurrence of three errors in the first constraint length. 
(
)
00
01
00
01
11
00
1
0
1
1
0
1
=
r
 
 
01 
11 
10 
1
1
3 
2 
1 
2 
11
00
11 
00 
10 
01 
11
11
00
00
01
10
10
01
4
3
4
1
3
4
3
2
a)
b)
10
00 
01
11
0
1
1
3
3
1
2
3
3
4
5
1
4
3
11
10
01
11
00
01
11 
11 
01 
00 
10 
00
10 
01 
00
11
01
 
 
c)
00
01
11
10
00 
11 
10
01
10
11
00
01
1
2
1
2
3
1
3
3
11
01
 
2
3
c)
 
Fig. 5.37 Viterbi decoding process for a three error sequence in the first constraint length 
From Fig. 5.37.c, we may notice that it is impossible to correct three errors on 
the first constraint length. For convolutional codes, the correction capacity cannot 
be expressed as easily as for block codes, due to the fact that it depends on the er-
ror distribution. 
 
Soft Decision Decoding 
 
The algorithm that has just been described corresponds to the transmission on a 
BSC, so on a memoryless channel. This model (BSC) corresponds to a hard deci-
sion channel, which means that, although the signals received by the demodulator 
are continuous (Gaussian variables, due to the Gaussian noise), at the demodulator 
output the signals are binary (the decision is binary or hard [28], [43]). If the  

5.9   Convolutional Codes 
333
 
demodulator output is quantified by more than two levels, the decoding process is 
called soft (decision) decoding process (Fig. 5.38). 
N
000 N
001 N
010 N
011 N
100 N
101 N
110 N
111
)
p(z/s0
)
p(z/s1
 
Fig. 5.38 Graphical illustration of the hard decision (with 2 levels) and the soft decision 
(with 8 levels) 
For hard decoding process, the demodulator sends to the decoder only one 
symbol (0 or 1). For soft decoding with 8 quantizing levels, the demodulator sends 
to the decoder 3 bits, for each T time sample, which is equivalent with transmit-
ting to the decoder a measure of the trust/confidence degree. Thus, transmitting 
the code 111 is equivalent with the transmission of a “1” with a very high degree 
of confidence, while transmitting 100 signifies the fact that “1” has a very low 
confidence. For the soft decoding process, Hamming distances are replaced with 
Euclidian distances between the received word and all possible code words [47]. 
Examples are given in 5.11 (turbo codes). 
For a Gaussian channel and 8 levels (3 bits) soft decoding, the encoding gain is 
increased by 2 dB, compared to hard decoding; the coding gain for analog deci-
sion (with an infinite number of quantizing levels) is 2,2 dB, which means that for 
soft decision with 8 levels there is a loss of only 0,2 dB. This argument justifies, in 
the soft decision case, the use of maximum 8 levels. The price paid for 2 dB gain 
is the increased dimension of the memory needed in the decoding process (and 
possible speed losses as well) [42]. Soft decoding is frequently used in the Viterbi 
algorithm because it produces a slight increase in the computations volume [47]. 
 
Conclusions regarding Viterbi decoding algorithm 
 
– 
it is an optimal algorithm 
– 
it can be applied to any convolutional  code (systematic or non-systematic) 
– 
it can be used with hard decision or 8 quantizing levels (the encoding gain 
obtained in this case is of 2 dB compared to the hard decoding) 
– 
there are VLSI specialized circuits: for example the Q 1401 circuit manu-
factured by Qualcomm [38] is a Viterbi decoder for a convolutional code 
with 
2
/
1
R =
 and 
7
K =
, operating with hard or soft decoding with 8 

334 
5   Channel Coding
 
quantification levels, ensuring a 
17Mb/s
Di =
•
, Gc=5,2 dB at 
5
10
p
−
=
 
with BPSK or QPSK modulator and 8 levels soft decoding. 
– 
the practical implementation is limited by K up to maximum values of 
approx. 10 (the calculation volume increases exponentially with K). 
 
Threshold decoding (1963)  
 
Threshold decoding, although not optimal compared to the Viterbi decoding, is an 
algebraic method of decoding, conceptually closer to block codes (the error syn-
drome is calculated in a similar manner), having the advantage of a very simple 
implementation. At the beginning it was used for BCH decoding and it was pro-
posed by Massay in 1963 for convolutional decoding.  
The decoding process takes place at the end of the constraint length K, which 
leads to inferior performances than those obtained with Viterbi algorithm (we re-
mind that 
∞
< d
dK
, so  the code control capacity on length K is reduced too). 
The implementation great simplicity imposed this method in a series of applica-
tions that accept lower coding gains, but also low cost: in telephony and satellite 
radio transmissions [28].  
The algorithm can be applied only to systematic convolutional codes. 
The block-scheme of a threshold decoding system is presented in Fig. 5.39. 
2
J
A
J
1
j
j >
∑
=
i
0,
eˆ
 
Fig. 5.39 Block-scheme of a threshold decoding system 
The transmission was represented in parallel for information i(x) and control 
c(x) sequences in order to enable an easier understanding of the decoding process 
(in real serial transmission, the information and control bits are affected by errors 
independently). 

5.9   Convolutional Codes 
335
 
In Fig. 5.39 are illustrated the followings: 
 
– 
ISR - information shift register with K cells in which the information sym-
bols are loaded; these symbols are then used for determining the control 
symbols 
– 
)
x
(
ei
- error polynomial that alters the information symbols in the encoded 
sequence 
– 
)
x
(
ec
- error polynomial that changes the control symbols in the encoded 
sequence 
– 
)
x
(
c′′
- polynomial corresponding to the control symbols determined from 
the received information symbols 
)
x
(
i′
 
– 
)
x
(s
 - error syndrome polynomial 
– 
)
x
(
i′
 - polynomial corresponding to the received information 
– 
)
x
(
c′
 - polynomial corresponding to the received control bits 
– 
TLD- threshold logic device (majority logic device); its output is “1” if the 
most of the inputs A1, ..., AJ  are “1”. If: 
2
J
A
J
1
j
j >
∑
=
                                           (5.230) 
then the output is “1”. 
SR - syndrome register; it is a K cells shift register 
Correction starts from the first symbol introduced in ISR (i0), the TLD output 
being “1” if i0 is erroneous. 
The equations describing the operation of the scheme are the followings: 
( )
( )
( )
x
e
x
i
x
i
i
+
=
′
                                          (5.231) 
( )
( )
( )
x
e
x
c
x
c
c
+
=
′
                                         (5.232) 
( )
( ) ( )
( ) ( )
( )
[
]
x
e
x
i
x
g
x
i
x
g
x
c
i
+
=
′
=
′′
                             (5.233) 
( )
( ) ( )
x
i
x
g
x
c
=
                                            (5.234) 
( )
( )
( )
( )
( )
( )
( ) ( )
( )
( ) ( )
( )
x
e
x
g
x
e
x
s
x
e
x
g
x
c
x
e
x
c
x
c
x
c
x
s
c
i
i
c
+
=
+
+
+
=
′′
+
′
=
                 (5.235) 
From (5.235), we notice that the error syndrome does not depend on the trans-
mitted word, but only on the error word: ei (x) and ec (x) and on g(x). 
The convolutional codes, as we have already seen, are continuous codes so 
their polynomial representation will be made with infinite polynomials: 
( )
∑
=
∞
=0
n
n
nx
s
x
s
                                           (5.236) 

336 
5   Channel Coding
 
Depending on g(x), the convolutional codes are divided into direct orthogonal 
codes (DOC) and indirect orthogonal codes (IOC). 
We call direct orthogonal codes (DOC) the codes that allow to directly deter-
mine a set of J equations si that are orthogonal on a given symbol. 
A set of J equations is orthogonal on a given symbol if all the equations con-
tain that symbol and any other symbol cannot be found in more than one equation, 
at the most. 
 
Example 5.32 
 
Analyze the threshold decoding algorithm for the convolutional code given by the 
following parameters: 
2
/
1
R =
, 
7
K =
, ( )
6
5
2
x
x
x
1
x
g
+
+
+
=
 
 
Solution 
We express the error syndrome in polynomial form, according to relation (5.235): 
( ) (
) ( )
( )
x
e
x
e
x
x
x
1
x
s
c
i
6
5
2
+
+
+
+
=
                               (5.237) 
The n-th order coefficient of the polynomial s(x) will be: 
c
n,
i
6,
n
i
5,
n
i
2,
n
i
n,
n
e
e
e
e
e
s
+
+
+
+
=
−
−
−
                          (5.238) 
The first 7 coefficients: 
6
0
s 
...,
 ,
s
will be loaded in SR (syndrome register). 
We determine the following system: 
⎪
⎪
⎪
⎪
⎪
⎩
⎪⎪
⎪
⎪
⎪
⎨
⎧
+
+
+
+
=
+
+
+
=
+
+
=
+
+
=
+
+
=
+
=
+
=
c
6,
i
0,
i
1,
i
4,
i
6,
6
c
5,
i
0,
i
3,
i
5,
5
c
4,
i
2,
i
4,
4
c
3,
i
1,
i`
3,
3
c
2,
i
0,
i
2,
2
c
1,
i
1,
1
c
0,
i
0,
0
e
e
e
e
e
s
e
e
e
e
s
e
e
e
s
e
e
e
s
e
e
e
s
e
e
s
e
e
s
                           (5.238.a) 
From the 7 equations of this system we identify equations s0, s2, s5, s6 being or-
thogonal on i0; all of them contain e0,i and any other symbol is not found in more 
than one equation: 
⎪
⎪
⎩
⎪
⎪
⎨
⎧
+
+
+
+
=
=
+
+
+
=
=
+
+
=
=
+
=
=
c
6,
i
6,
i
4,
i
1,
i
0,
6
4
c
5,
i
5,
i
3,
i
0,
5
3
c
2,
i
2,
i
0,
2
2
c
0,
i
0,
0
1
e
e
e
e
e
s
A
e
e
e
e
s
A
e
e
e
s
A
e
e
s
A
                        (5.239) 

5.9   Convolutional Codes 
337
 
The terms 
4
1
A
 
...,
 ,
A
are the 
4
J =
 inputs of the threshold logic device TLD. 
We wish to evaluate the correction capability of this code. 
 
• Assume one error on position 
1
e
:
i
i
0,
0
=
′
. We get 
1
A
A
A
A
4
3
2
1
=
=
=
=
, so 
the TLD output, according to (5.230), is “1”; it follows that 
0i′  will be cor-
rected. 
• Assume two errors: one on 0i′  and another position, either information or con-
trol; in this case, one of the equations 
0
A j =
, the other three are “1”, so 0i′  
will be corrected. 
• If three errors occur: 0i′  and another two on either information or control sym-
bols, only one or two 
j
A  inputs will be “1”; the output of the majority logic 
device will be “0”, therefore i0’ will not be corrected. 
Resuming, we may say that the maximum number of errors that can be cor-
rected (tmax) is: 
2
J
tmax =
                                              (5.240) 
Remark 
g(x) must be carefully chosen, such that it does not decrease the code correction 
capacity given by (5.241): 
1
2t
dK
+
≥
                                           (5.241) 
It follows that: 
1
d
J
K −
=
                                           (5.242) 
The block scheme of the threshold decoder is the following: 
 
Fig. 5.40 Threshold decoder for the direct orthogonal code R = 1/2, g2(x)=1 + x2  + x5 + x6 

338 
5   Channel Coding
 
The generator polynomials for direct orthogonal codes DOC can be easily de-
termined using the control triangles [2], [47]. 
Table 5.19 contains the most important DOCs for 
1/2
R =
: 
Indirect orthogonal codes (IOC) are the codes for which the orthogonal system 
of equations on a given symbol cannot be obtained directly, but by linear combi-
nations of the syndrome equations. These codes are more efficient than the DOCs 
in the way that the same J (so the same power of correction) is ensured by a lower 
K (these polynomials are determined by computer trials - 1963 Massey). It must 
be underlined the fact that the syndrome register SR must be used with feedback. 
Table 5.15 The most important direct orthogonal codes [47] for R = 1/2 
J 
K 
g(x) 
2 
4 
6 
8 
2 
7 
18 
36 
1+x 
1+x2+x5+x6 
1+x2+x7+x13+x16+x17 
1+x7+x10+x16+x18+x30+x31+x35 
 
Example 5.33 
Analyze the threshold decoding for the indirect orthogonal code given by 
2
/
1
R =
, 
6
K =
,  ( )
5
4
3
x
x
x
1
x
g
+
+
+
=
. 
 
Solution 
The n-th order coefficient of the polynomial s(x) is: 
c
n,
i
5,
n
i
4,
n
i
3,
n
i
n,
n
e
e
e
e
e
s
+
+
+
+
=
−
−
−
                            (5.243) 
The first six coefficients loaded in the syndrome register are: 
⎪
⎪
⎪
⎪
⎩
⎪
⎪
⎪
⎪
⎨
⎧
+
+
+
+
=
+
+
+
=
+
+
=
+
=
+
=
+
=
c
5,
i
0,
i
1,
i
2,
i
5,
5
c
4,
i
0,
i
1,
i
4,
4
c
3,
i
0,
i`
3,
3
c
2,
i
2,
2
c
1,
i
1,
1
c
0,
i
0,
0
e
e
e
e
e
s
e
e
e
e
s
e
e
e
s
e
e
s
e
e
s
e
e
s
                              (5.244) 
The system (5.244) is not an orthogonal system on 0i , although the equations 
5
4
3
0
s 
and
s
,
s
,
s
 all contain 
i
0,
e
  and 
5
4
s 
and
s
 contain 
i
1,
e
. In order to make it 
orthogonal, we combine equations s1 and s5. Thus we obtain an orthogonal system 
on 0i : 

5.9   Convolutional Codes 
339
 
⎪
⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
+
+
+
+
=
+
=
+
+
+
=
=
+
+
=
=
+
=
=
c
5,
c
1,
i
5,
i
2,
i
0,
5
1
4
c
4,
i
0,
i
1,
i
4,
4
3
c
3,
i
0,
i
3,
3
2
c
0,
i
0,
0
1
e
e
e
e
e
s
s
A
e
e
e
e
s
A
e
e
e
s
A
e
e
s
A
                      (5.245) 
The block-scheme of the decoding unit is that illustrated in Fig.5.41 and the 
most important indirect orthogonal codes for 
2
/
1
R =
 are given in Table 5.16 
[47]. 
 
Fig. 5.41 Threshold decoder for indirect orthogonal code R = 1/2, g2(x)=1 + x3 + x4 + x5 
Table 5.16 The most important indirect orthogonal codes for R = 1/2 
J 
K 
AI 
g(x) 
4 
6 
 
 
8 
 
 
 
6 
12 
 
 
22 
s0, s3, s4, s1+ s5 
s0, s6, s7, s9 
s1+ s3+ s10 
s4+ s8+ s11 
s0, s11, s13, s16, s17 
s2+ s3+ s6+ s19 
s4+ s14+ s20 
s1+ s5+ s8+ s15+ s21 
1+x3+x4+x5 
1+x6+x7+x9+x10+x11 
 
 
1+x11+x13+x16+x17+x19+x20+x21 
 
Remark 
Comparing Table 5.15 and Table 5.16, we remark the advantage of the indirect or-
thogonal codes: at the same correction capacity J, their K values are much lower. 
 

340 
5   Channel Coding
 
The convolutional codes can also be decoded sequentially. This method is in 
fact the first proposed method for convolutional codes decoding and was proposed 
in 1957 by J. M. Wozencraft. Basically, a sequential decoder works by generating 
hypotheses regarding the transmitted code sequences; it measures the distances be-
tween these hypotheses and the received signal and goes on until the distances re-
main reasonable. When these are too high, there is a turn back, so a modification 
of the hypotheses, until, through attempts, the suitable hypotheses are found [28], 
[42]. From the sequential decoding process advantages, we mention the independ-
ence from the constraint length, which enables the use of this method for high K 
values (
41
K =
) [42]. The main disadvantage is that the number of the “weak” 
hypotheses and the turn backs depend on the channel signal/noise ratio (SNR). For 
low SNRs, the number of hypotheses that have to be tested is higher than for high 
values of this ratio, which in certain cases lead to exceeding the decoding storage 
capacity. 
5.10   Code Interleaving and Concatenation 
5.10.1   Interleaving 
The process of interleaving allows the use of independent error correction codes 
for burst error correction of lengths much higher than the number of independent 
errors which occur in a received word. 
The idea is to send the codeword symbols interleaved with other codewords 
symbols, such that the distance between two successive symbols of the same 
codeword is higher than the length of the burst (b). In this way, a burst of errors 
cannot affect more symbols in the same word and subsequently, a certain code-
word symbols are erroneous due to different bursts (independent bursts); their ef-
fect is that of the independent errors (Fig. 5.42). 
a1
a1
a1
a1
a0
a0
a0
a0
a2
a2
a2
a2
a3
a2
a1
a0
a2
a1
a0
a4
a3
a4
a3
a2
a1
a0
a2
a1
a0
a4
a3
a4
b)
a)
X            X         X         X
b
(1)
(1)
(1)
(1)
(1)
(1)
(1)
(2)
(2)
(2)
(2)
(2)
(2)
(2)
(2)
(1)
(3)
(4)
(3)
(3)
(3)
(3)
(4)
(4)
(4)
(4)
(4)
(4)
(4)
(3)
(3)
(3)
 
Fig. 5.42 Interleaving example: a) 4 codewords (of length 5) non-interleaved succession; b) 
interleaved succession (b = burst error length) 
Interleaving can be made in two ways: block interleaving and convolutional in-
terleaving [42]. 

5.10   Code Interleaving and Concatenation 
341
 
Block interleaving 
 
When transmiting, the symbols of an independent error correcting block code are 
disposed column by column (or line by line) in a block of N lines and M columns 
and then sent to the modulator on lines (or columns). At the receiver the opera-
tions occur in reverse order: the received symbols enter the demodulator by lines 
and are sent to the decoder by columns (or viceversa). 
 
Fig. 5.43 Block interleaving 
Block interleaving characteristics are: 
 
• Any burst error of length 
M
b ≤
 will lead to singular errors at the decoder  
input. 
• The delay caused by interleaving (taking into account the processing that takes 
place both at transmission and receiver) is approximately
T
M
N
2
⋅
⋅
⋅
(T is the 
symbol duration). More precisely, in order to initiate the transmission (immedi-
ately after loading the first symbol of the last column: 31 in Fig. 5.43), it is 
enough to load the memory in
1
1)
-
N(M
+ cells. The same minimum number of 
cells is necessary to initiate the decoding at the reception. It results that the en-
tire minimum delay (at the transmission and reception) is: 
2
2N
-
2NM
+
 
• The necessary memory must have 
M
N⋅
 cells; generally, the memory imple-
mented both at the transmission and receiver is double: 
M
N
2
⋅
⋅
, allowing to 
simultaneously load one block and flush another. 
• If the block code corrects only singular errors, the number of block columns M 
is chosen higher than the error burst length: 
b
M ≥
 
• The number of lines (N) depends on the type of code used: for block codes: 
n
N >
 (the block code length), and for convolutional codes 
K
N >
 (the con-
straint length). In this way, a burst error of length 
M
b =
 will determine one er-
ror, at the most, in one code word, or in any constraint length K. If we use t er-
rors correcting codes, M is chosen such that: 
b/t
M >
 

342 
5   Channel Coding
 
The convolutional interleaving was proposed by Ramsey (1970) [59] and For-
nay (1971) [60]. The structure proposed by Fornay is presented in Fig. 5.44. 
 
Fig. 5.44 Block-scheme of a convolutional interleaving made with shift registers ( C-
encoder for independent errors; dC-decoder for independent errors; I – interleaver; dI – de-
interleaver). 
The characteristics of convolutional interleaving are similar to those of block 
interleaving. The essential advantage of convolutional interleaving is that the total 
delay (transmission-receiver) is half of the block interleaving delay: 
2
/
1)
N(M−
, 
so the memory is reduced to half as well. 
5.10.2   Concatenated Codes 
Concatenated codes are using two levels: an inner C1 and an outer C2 (Fig. 5.45). 
 
Fig. 5.45 Block scheme of a concatenated system 
The resulting code is of dimension (n1n2,m1m2), and its code distance is 
2
1d
d
d ≥
, where d1 and d2 are the corresponding code distances for C1 and C2 
[28]. 
Generally, the outer code C2 is a RS (Reed-Solomon) code, and C1 is binary 
(usually a convolutional code) or RS as well. 
Concatenated codes are useful when dealing with mixed errors: independent 
and burst errors. The inner code C1 is usually dimensioned such that it corrects 
most of the independent errors on the channel. The outer code C2 reduces the error 
probability to the desired value (it corrects some of the errors missed by C1). 
The purpose of concatenating is to obtain a low bit error rate with simplicity in 
implementation (lower than the complexity required by the use of a single code), 
and higher redundancy, i.e. a more powerful code. 

5.10   Code Interleaving and Concatenation 
343
 
One of the most popular concatenated systems is C2 RS code, C1- convolutional 
code (implemented with Viterbi soft decoding) [28] (Fig. 5.46), for which 
5
10
p
−
=
corresponding to 
2,5)dB
(2
/N
E
0
b
÷
=
(the planetary NASA standard for 
codes). 
 
Fig. 5.46 Block-scheme of an interleaved concatenated system (C2 - RS + C1 - convolutional) 
Example 5.34  
The digital audio encoding system for CD (CIRC) [42] 
 
In 1979, Philips Corporation from Netherlands and Sony from Japan defined a 
standard for the audio digital system using CDs. 
The CD is a plastic disc of 120 mm used for audio signals storage. The sam-
pling is performed at 
kHz
 1.
44
fs =
(corresponding to an audio bandwidth of 20 
kHz). Each audio sample is quantized with 
16
2
 levels (16 bits or 2B bytes per 
sample); it results a dynamic of 96dB/s and harmonic distortions of 0,005% . One 
disc ( ≅70 minutes) contains 
10
10
 bits. The disc is laser written and read. 
The error sources on a CD are: 
 
• small, unwanted air bubbles inside the plastic or irregularities in recording; 
• fingerprints or scratches when handling the disc 
 
These errors are similar to burst errors, as they affect more bits. The high fidel-
ity of the system is due to a correction scheme based on concatenating two inter-
leaved RS codes (CIRC - Cross Interleave Reed-Solomon Codes). By interleaving, 
data are redistributed such that the digits belonging to neighbouring samples are 
spread in space; subsequently, the burst errors will occur as independent (singular) 
errors. The error protection is ensured by shortened RS codes. 
In digital audio applications, an undetected error is very important because it 
generates cracking noises, whereas detected errors are not so annoying as they can 
be “hidden”. 

344 
5   Channel Coding
 
The block-scheme of the CIRC system is presented in Fig. 5.47 
 
Fig. 5.47 Block scheme of CIRC system 
The CIRC system controls the errors in a hierarchical structure: 
 
1. The decoder provides a certain level of correctable errors; 
2. If the error correction capacity is exceeded, the decoder provides a correction 
level of erasures; 
3. If the level of erasures is exceeded, the decoder tries to mask the erroneous 
samples by interpolating the neighbouring unaltered samples; 
4. If the interpolation capacity is exceeded, the decoder turns on the mute option 
during the deteriorated samples. 
Fig. 5.48 and Fig. 5.49 illustrate the processes that occur at encoding and de-
coding, respectively. 
 
 
Fig. 5.48 Encoding and interleaving in CIRC system: a) I1 - even samples B2p are separated 
from the odd ones B2p+1 with 2Tf ; b) C2 - RS (28,24) encoding; c) I2 - samples are delayed 
with different time periods to spread the errors; d) C1 -RS (32, 28) encoding; e) I3 - even 
samples (B2p,i) cross interleaving with the next frame odd samples (B2p+1,i+1) 

5.10   Code Interleaving and Concatenation 
345
 
Fig. 5.48 (a) shows that a frame includes six sampling sequences, each sample 
made up of a stereo pair (left L, right R) quantized on 16 bits (2B). This results in 
a frame length of 24B. At encoding, the processing has five steps, as follows: 
 
a. I1 - even samples are separated from the odd ones with two frame lengths (2Tf) 
to “mask” the detectable but not correctable errors without difficulty, by inter-
polation (due to the correct neighbouring samples). 
b. C2 - it is an RS (28,24) encoder: 4 control bytes (4BC) are added to 24B. The 
RS (28,24) code is a shortened code: 
255
1
2
1
2
n
8
k
=
−
=
−
=
: RS(255,251). 
We remind that by shortening, the code maintains its control capacity
)
5
(d =
, 
which corresponds to the correction of maximum 2 erroneous characters 
(bytes). 
c. I2 - each of the 28B is differently delayed, so that the errors in one word are 
spread into different words (interleaving I2). C2 and I2 have the function of cor-
recting burst and independent errors that cannot be corrected by C1. 
d. C1 - is an RS(32,28) encoder, derived just like C2 by shortening the 
RS(255,251) code; therefore the codes have the same distance 
5
d =
. 
e. I3 – performs the cross interleaving between the even bytes (B2p,i) of one frame 
and the odd bytes of the next frame (B2p+1,i+1). Using this method, two consecu-
tive bytes belong to different codewords. 
C1 and I3 will correct most of the singular errors and will detect the long burst 
errors. 
The processing that takes place at the decoder (player) is shown in Fig. 5.49. 
 
Fig. 5.49 Illustration of the decoding process in CIRC system 
 
 

346 
5   Channel Coding
 
At receiver, the processing has also five steps, as follows: 
 
a. dI3 – de-interleaving  is accomplished by the alternate introduction of delay 
cells D. The 32 inputs (B1 ÷  B32) are loaded in parallel at the input of the  
de-interleaving circuit. The delays D take one byte. 
b. dC1 - RS(28) decoder; dI3 and dC1 are designed for correcting one character in a 
block of 32B and detecting long burst errors. If multiple errors occur, the dC1 de-
coder leaves them uncorrected, attaching to the 28B an erasure flag transmitted 
with dotted lines (see Fig. 5.49). 
c. dI2 - the delay lines D1, ..., D27 spread the errors on a number of words at the 
dC2 decoder input, which reduces the number of errors on a C2 codeword, so 
that C2 can correct them. 
d. dC2 - corrects the burst errors missed by dC1. If the errors cannot be corrected 
but only detected by dC2, the errors pass unchanged through dI1; however, dI1 
associates them an erasure flag (dotted lines B1, ... B24). 
e. dI1 - the uncorrected but detected errors (flagged errors) will be masked by  
interpolation from the correct neighbouring samples. 
Interpolation and mute interpolation 
 
The samples uncorrected by dC2 cause distortions that can be heard as cracks. 
The interpolator dI1 inserts new samples from the unaltered neighbouring samples, 
replacing the incorrect ones. If burst errors with 
48
b >
 frames occur and two or 
more successive samples are not corrected, the system goes mute for several ms, 
undetectable by the human ear [32], [42]. 
5.11   Turbo Codes 
5.11.1   Definition and Encoding 
Turbo-codes (TC), invented in 1993 by Claude Berrou and Alain Glavieux [5] at 
ENST (Telecomme) Bretagne, France, are a novel and revolutionary error- control 
coding technique, almost closing the gap between Shannon limit and real code 
performance. 
Turbo code consists of a parallel concatenation of two recursive systematic 
convolutional codes. 
A recursive convolutional code is a convolutional code (with feed forward) 
provided with feedback too. 
A recursive systematic convolutional (RSC) code is a systematic recursive con-
volutional code of rate R=1/n. A RSC with R=1/2 is given in Fig. 5.50. 

5.11   Turbo Codes 
347
 
0
g
1
g
1
...
K-1
i
0
v
1
v
[
]
1
0 v 
v
v
MR
 
 
Fig. 5.50 Basic RSC code: R=1/2, g0 - feedback polynomial, g1- feed forward polynomial 
In a polynomial representation, we have: 
i(x)
(x)
v0
=
                                             (5.246) 
(x)
g
(x)
g
i(x)
(x)
v
0
1
1
=
                                        (5.247) 
or: 
[
]
)
x
(
i(x)
(x)
g
(x)
g
1,
 
i(x)
 
(x)
g
(x)
g
i(x)
 
i(x),
(x)
 v
(x),
v
v(x)
0
1
0
1
1
0
G
=
⎥
⎦
⎤
⎢
⎣
⎡
=
=
⎥
⎦
⎤
⎢
⎣
⎡
=
=
                       (5.248) 
where G(x), the generator matrix, is identified as : 
⎥
⎦
⎤
⎢
⎣
⎡
=
(x)
g
(x)
g
 
1,
(x)
0
1
G
                                           (5.249) 
The variable x is representing the delay operator (D). 
If the coding rate is 
n
1
R =
, the generator matrix of the corresponding n
1 RSC 
code is: 
⎥
⎦
⎤
⎢
⎣
⎡
=
−
(x)
g
(x)
g
,
,
(x)
g
(x)
g
1,
(x)
0
1
n
0
1
…
G
                                  (5.250) 
If the polynomials 
(x)
gi
 
1)
(i ≥
and 
(x)
g0
are relatively prime (no common 
factor), the ratio
(x)
gi
,
(x)
g0
 can be written as [48]: 
...
x
a
x
a
a
(x)
g
(x)
g
2
2
1
0
0
i
+
+
+
=
                                (5.251) 
and corresponds to the encoded sequence 
(x)
vi
,
0)
(i ≠
, the control sequence. 

348 
5   Channel Coding
 
For iterative decoding, 
(x)
g0
- the feedback polynomial is selected to be a 
primitive polynomial of degree K-1 (the memory of the encoder). 
In this parallel concatenation a random interleaver is used exploating Forney 
concatenation idea. The information bits for the second code are not transmitted, 
thus incrementing the code rate (R). 
The main reason to use a long interleaver is to generate a concatenated code 
with a large block length which leads to a large coding gain [46]. 
A basic turbo coder of R=1/3 with BPSK (Binary Phase Shift Keying) modula-
tion and AWGN noise is presented in Fig 5.51. 
 
RSC
Encoder 1
RSC
Encoder 2
BPSK
BPSK
BPSK
+
+
Interliever
+
0r
1r
2r
0
x
AWGN
AWGN
AWGN
1
x
2
x
0
v
1
v
2
v
i
turbo-encoder
modulator
channel
^
i
 
Fig. 5.51 Basic turbo transmitter: turbo encoder with R=1/3, BPSK modulation and AWGN 
channel. 
5.11.2   Decoding 
5.11.2.1   Basic Principles 
A turbo decoder (Fig. 5.52) consists of two serially concatenated decoders sepa-
rated by the same interleaver. The decoding algorithm is an iterative one (MAP or 
SOVA- soft output Viterbi algorithm). The iterative process performs information 
exchange between the two decoders. Increasing the number of iterations in turbo-
decoding, a BER as low (
7
5
10
10
−
−÷
) can be achieved at a SNR very close to 
Shannon limit (-1.6 dB). 

5.11   Turbo Codes 
349
 
Decoder
1
Decoder
2
Interliever
Deinterliever
Interliever
Interliever
^
i
0
Λ
1
Λ
2
Λ
I ex 12
I ex 21
r0
~
r0
r1
r2
 
Fig. 5.52 Basic turbo-decoder 
The first decoder has as inputs 0r  and 1r . The decoder 1 produces a soft output 
1
Λ  which is interleaved and used to produce an improved estimate of the a priori 
probabilities of the information sequence (Iex12 extrinsic information of the first 
decoder is the priori probability estimate of the second decoder). The decoder per-
formance can be improved by this iterative operation relative to a single operation 
of the serial concatenated decoder. After a number of iterations, the soft outputs of 
both decoders stop to produce further performance improvements and the last de-
coding stage makes a hard decision after deinterleaving. 
The feedback loop is a distinguishing feature to the decoder and the name 
“turbo” is given precisely to the similarities, as concept and performance, with the 
thermodynamic turbo engine [19]. The two decoders cooperate similar with the 
pistons of a turbo engine: each decoder supplies its pair with extrinsic information, 
as the pistons supply each other with mechanical energy through the turbine. 
Iterative decoding algorithms are numerous and complex: Viterbi, SOVA, 
MAP, LOG-MAP and Max-LOG-MAP [46], [21]. Brifley they will be presented 
in what follows. 
5.11.2.2   Viterbi Algorithm (VA) [46], [48] 
This algorithm, presented in 5.9.5, was originally proposed for convolutional 
codes decoding. 
Decoding time is assumed Ĳ, and the sequence starts from zero state (at moment 
0) and reach the same state at the final moment (this is why the trellis termination 
is required). 
0)
S
,
S
,...,
S
0,
(S
S
0
1
Ĳ
1
0
=
=
=
−
 
VA estimate the information î that corresponds to the modulated sequence xˆ  in 
the trellis, such that the word (sequence) error probability: sP  is minimized. 
c
s
P
1
P
−
=
                                                (5.252) 

350 
5   Channel Coding
 
where 
c
P  is the probability of a correct sequence decoding: 
∫
=
r
r
r
i
r
)d
/
)p(
p(
Pc
                                        (5.253) 
r being the space of received sequences. 
s
P minimum, implies 
c
P  maximum, and according to (5.253) p(i/r), the a pos-
teriori probability maximum. The algorithm which Maximizes the Aposteriori 
Probability is known as MAP decoding algorithm. 
Using Bayes formula (see relation (2.32)) we have: 
)
p(
)
p(
)
)p(
p(
)
p(
r/i
r
r/i
i
i/r
=
=
                                      (5.254) 
based on the fact that the a priori probability of the information sequence 
)
p(i is 
equal with 
)
p(r  (encoding is a biunivoque correspondence: 
)
p(
)
p(
r
i =
). 
A decoder which maximizes 
)
p(r/i  is called maximum likelihood decoder 
(MLD) (see also 5.6). Under the assumption of equally probable r (equally prob-
able sequences), MAP and MLD are equivalent in terms of word (sequence) error 
probability. 
Under the assumption of AWGN with 
2
n
ı  dispersion (see Appendix D), and Ĳ 
length sequence, we have: 
2n
2
k
t,
k
t,
2ı
)
x
(r
Ĳ
1
t
1
n
0
k
2
n
Ĳ
1
t
e
2π
1
)
p(
)
p(
)
p(
−
−
=
−
=
=
∑∏
=
∑
=
=
ı
t
t/i
r
r/i
i/r
                (5.255) 
Taking into account that in relation (5.255) there are exponentials, the 
)
p(
log
i/r  will be used: 
∑∑
−
−
−
−
=
=
∑
∏
=
=
−
=
−
−
=
−
=
Ĳ
1
t
1
n
0
k
2
n
2
k
t,
k
t,
n
2ı
)
x
(r
Ĳ
1
t
1
n
0
k
2
n
2ı
)
x
(r
logı
log2
2
n
e
2π
1
log
)
logp(
2
n
2
k
t,
k
t,
nĲ
ı
π
τ
i/r
                      (5.256) 
Maximizing 
)
p(r/i is equivalent, based on (5.256) to minimizing the Euclidian 
distance: 
∑
−
=
=
1-
n
0
k
2
k
t,
k
t,
E
)
x
(r
d
                                    (5.257) 
 

5.11   Turbo Codes 
351
 
It follows that, for AWGN channels, the MLD (maximum likelihood decoder) 
and obviously MAP and VA, reduce to a minimum distance (Euclidian) decoder 
(the same demonstration, but for hard decoding, using Hamming distance, is given 
in 5.6, relation (5.24)). 
The squared Euclidian distance is called branch metric (
x
t
mb ) and is: 
∑
−
=
=
1-
n
0
k
2
k
t,
k
t,
x
t
)
x
(r
:
mb
                                      (5.258) 
The path metric (
x
t
mp ) corresponding to the sequence x is: 
x
t
t
1
j
x
1
t
x
j
x
t
mb
mp
mb
mp
+
∑
=
=
=
−
                                 (5.259) 
Thus, VA is an effective way of finding a path in the trellis with the minimum 
path metric. The computation is based on keeping only one path per node, the one 
with the minimum metric at each time instant (the survivor). 
The decision on the message estimation î is made at the final time Ĳ. The 
maximum likelihood path is chosen as the survivor in the final node. If the mini-
mum path metric corresponds to a path xˆ  , the decoder will select the binary se-
quence î on this path as the hard estimate of the transmitted sequence i. 
 
Example 5.35 
 
a) Design a RSC encoder with coding rate R=1/2 and constraint length K=2. 
b) Encode the information sequence: i=[0 1 0 0 1] 
c) Draw the state diagram and the trellis 
d) If the received sequence is: r=|1,-1|0.8,1|-1,1|-1,1|1,-1|, applying VA with 
Euclidian distance, find the maximum likelihood path. 
 
Solution 
a) From the input data 
RSC
2
1
 with K=2, we identify: 
n
m
R =
=> m=1, n=2 => k=1 
According to (5.216) it follows that only one generator polynomial is required 
and K being 2, it means that its degree is 1: 
x
1
(x)
g0
+
=
 and 
⎥⎦
⎤
⎢⎣
⎡
+
=
x
1
1
 
1,
G(x)
 
The block scheme of the RSC encoder is given in Fig. 5.53a. 
 
 
 
 

352 
5   Channel Coding
 
b) The encoding can be done algebraically, using (5.246), (5.247) or (5.248); the 
same result is obtain using the encoder operation (Fig. 5.53). 
– 
Algebraic encoding: 
(x)
v
x
x
i(x)
0
4 =
+
=
 
3
2
4
0
1
x
x
x
1
x
x
x
(x)
g
1
i(x)
(x)
v
+
+
=
+
+
=
=
 
10]
|
01
|
01
|
11
|
00
 [
]
v
[v
v
1
0
=
=
 
 
The encoder operation (Fig. 5.53) is: 
 
nt
 
nt
 
1
nt +  
]
v
[v
v
1
0
=
 
i 
1
C  
n
0
i
v
=
 
1
n
1,
n
1
C
i
v
−
⊕
=
 
0 
1 
0 
0 
1 
0 
1 
1 
1 
0 
0 
1 
0 
0 
1 
0 
1 
1 
1 
0 
 
Remark: the encoder starts and ends at zero state (
0
S ) 
 
c) The state diagram and the trellis are represented in Fig. 5.53.b, respectively c. 
 
1
c
0
v
1
v
[
]
1
0 v 
v
v =
 
Fig. 5.53 RSC encoder with R = 1/2 and K=2: a) block- scheme; b) state-diagram;  
c) trellis: — i=0 input, ····· i=1 input 
 

5.11   Turbo Codes 
353
 
 
 
 
Fig. 5.53 (continued) 
 
d) We calculate the branch metric 
x
t
mb  in each frame and starting with the sec-
ond frame, the path metric 
x
t
mp . Taking into account that from the second 
frame in each node enter two path metrics, the survivor will be selected based 
on its minimum Euclidian distance: 
– 
In the first frame, the branch metrics are: 
4
4
0t
0
S
00
11
1t
(1,-1)
 
1
1
2
2
1
1
mp
4
1)]
(
1
[
1)]
(
[1
mb
=
=
−
−
−
+
−
−
=
 
2
1
2
2
2
1
mp
4
1]
1
[
1]
[1
mb
=
=
−
−
+
−
=
 
 
• 
In the second frame, the branch metrics and the path metrics are: 
 
7.24
1)]
(
[1
1)]
(
[0.8
mb
2
2
1
2
=
−
−
+
−
−
=
 
0.04
1)]
(
[1
1]
[0.8
mb
2
2
2
2
=
+
−
+
−
=
 

354 
5   Channel Coding
 
4.04
1)]
(
[1
1]
[0.8
mb
2
2
3
2
=
−
−
+
−
=
 
3.24
1]
[1
1)]
(
[0.8
mb
2
2
4
2
=
−
+
−
−
=
 
0t
0
S
1t
2t
 
11.24
7.24
4
mp1
2
=
+
=
 
4.04
0.04
4
mp2
2
=
+
=
 
8.04
4.04
4
mp3
2
=
+
=
 
7.24
3.24
4
mp4
2
=
+
=
 
In each node the survivor is selected accordingly to the minimum path metric: 
8.04, respectively 4.04. 
 
– 
In the third frame the branch metrics and the path metrics are: 
0t
0
S
00
11
1t
00
01
11
10
12.04
2t
10
11
8.04
4.04
3t
12.04
4.04
12.04
r: (1, -1)          (0.8, 1)            (-1, 1)
 

5.11   Turbo Codes 
355
 
4
1)]
(
[1
1)]
(
1
[
mb
2
2
1
3
=
−
−
+
−
−
−
=
 
4
1]
[1
1]
1
[
mb
2
2
2
3
=
−
+
−
−
=
 
8
1)]
(
[1
1]
1
[
mb
2
2
3
3
=
−
−
+
−
−
=
 
0
1]
[1
1)]
(
1
[
mb
2
2
4
3
=
−
+
−
−
−
=
 
 
12.04
4
8.04
mp1
3
=
+
=
 
12.04
4
8.04
mp2
3
=
+
=
 
12.04
8
4.04
mp3
3
=
+
=
 
4.04
0
4.04
mp4
3
=
+
=
 
Selection of the survivors: in the first node the path metrics being the same 
(12.04) for the both branches, the selection is random; in the second node the sur-
vivor has the path metric 4.04. The remained ways are: 
12.04
4.04
00
11
10
01
0
S
r: (1, -1)          (0.8, 1)         (-1, 1)
 
– 
In the fourth frame we have: 
4
1)]
(
[1
1)]
(
1
[
mb
2
2
1
4
=
−
−
+
−
−
−
=
 
4
1]
[1
1]
1
[
mb
2
2
2
4
=
−
+
−
−
=
 
8
1)]
(
[1
1]
1
[
mb
2
2
3
4
=
−
−
+
−
−
=
 
0
1]
[1
1)]
(
1
[
mb
2
2
4
4
=
−
+
−
−
−
=
 
0
S
 

356 
5   Channel Coding
 
16.04
4
12.04
mp1
4
=
+
=
 
16.04
4
12.04
mp2
4
=
+
=
 
12.04
8
4.04
mp3
4
=
+
=
 
4.04
0
4.04
mp4
4
=
+
=
 
 
After the choice of survivor, the remaining ways are: 
12.04
4.04
00
11
10
01
01
0
S
r: (1, -1)          (0.8, 1)         (-1, 1)             (-1, 1)
 
– 
In the fifth frame we have: 
8.04
16.04
12.04
16.04
12.04
4.04
00
11
10
01
0
S
01
00
01
10
11
r: (1, -1)           (0.8, 1)            (-1, 1)           (-1, 1)            (1, -1)
 
4
1)]
(
1
[
1)]
(
[1
mb
2
2
1
5
=
−
−
−
+
−
−
=
 
4
1]
1
[
1]
[1
mb
2
2
2
5
=
−
−
+
−
=
 
4
1]
1
[
1]
[1
mb
2
2
3
5
=
−
−
+
−
=
 
8
1]
1
[
1)]
(
[1
mb
2
2
4
5
=
−
−
+
−
−
=
 
  
16.04
4
12.04
mp1
5
=
+
=
 
16.04
4
12.04
mp2
5
=
+
=
 
8.04
4
4.04
mp3
5
=
+
=
 
12.04
8
4.04
mp4
5
=
+
=
 
 
 
 
 
 

5.11   Turbo Codes 
357
 
After the choice of the way with the minimum path distance, we obtain: 
0
S
 
 
thus the information sequence at the output of VA decoder is: 
î= [0 1 0 0 1]. 
5.11.2.3   Bidirectional Soft Output Viterbi Algorithm (SOVA) [46] 
The main disadvantage of VA is its output is binary (hard estimation), meaning 
that performance loss occurs in multistage decoding. In order to avoid this, soft 
outputs are required. SOVA is able to deliver soft outputs estimating for each 
transmitted binary symbol ( ti ) the log- likelihood function
)
Λ(it . If the decision 
is made on finite length blocks (as for block codes), SOVA can be implemented 
bidirectionally, with forward and backward recursions. 
 
• SOVA estimates the soft output information calculating the log - likelihood 
function 
)
Λ(it . 
)
0/
P(
)
1/
P(
log
)
Λ(i
Ĳ
t
Ĳ
t
t
r
i
r
i
=
=
=
                                      (5.260) 
where 
)
j/
P(i
Ĳ
t
r
=
with j=1,0, is the aposteriori probability of the transmitted 
symbol, and 
τr  is the received sequence of length Ĳ. 
• SOVA compares 
)
Λ(it  with a zero threshold (hard decision): 
⎩
⎨
⎧
≥
=
otherwise
 
0,
0
)
Λ(i
  
if
  
1,
i
t
t
                                         (5.261) 
• The decoder selects the path xˆ  (respectively î) with the minimum path metric 
(mp) as the maximum likelihood path (ML) similar with VA. 
The probability of selecting xˆ  is proportional with
)
/
P(i
Ĳ
t r
, and based on rela-
tion (5.255), to 
Ĳ
min,
mp
e−
, where 
Ĳ
min,
mp
 represents the minimum path metric 
on Ĳ frames. For 
1
it =
, respectively 0 (the complementary symbol) we have. 
Ĳ
min,
mp
Ĳ
t
e
~
)
1/
P(i
−
=
r
                                    (5.262) 
c
t,
mp
Ĳ
t
e
~
)
0/
P(i
−
=
r
                                      (5.263) 

358 
5   Channel Coding
 
where 
c
t,
mp
 represents the minimum path metric of the paths with complemen-
tary symbol to the ML symbol at time t. 
In this case, the log-likelihood function (5.260) becomes: 
Ĳ
min,
c
t,
mp
mp
Ĳ
t
Ĳ
t
t
mp
mp
e
e
~
)
0/
P(i
)
1/
P(i
log
)
Λ(
c
t,
Ĳ
min,
−
=
=
=
=
−
−
r
r
i
                 (5.264) 
If we introduce the notations: 
⎪⎩
⎪⎨
⎧
=
=
c
t,
0
t
Ĳ
min,
1
t
mp
mp
mp
mp
                                        (5.265) 
it follows that (5.264) can be written as: 
0
t
1
t
Ĳ
t
Ĳ
t
t
mp
mp
~
)
0/
P(i
)
1/
P(i
log
)
Λ(i
−
=
=
=
r
r
                            (5.266) 
If the ML estimate at time t is 0, we have the relations: 
⎪⎩
⎪⎨
⎧
=
=
c
t,
1
t
Ĳ
min,
0
t
mp
mp
mp
mp
                                           (5.267) 
and log-likelihood relation becomes: 
a,
mp
mp
mp
mp
e
e
~
)
0/
P(i
)
1/
P(i
log
)
Λ(i
1
t
0
t
c
t,
Ĳ
min,
mp
mp
Ĳ
t
Ĳ
t
t
Ĳ
min,
c
t,
−
=
=
−
=
=
=
=
−
−
r
r
            (5.268) 
meaning that the soft output of the decoder is obtained as the difference of the 
minimum path metric (
Ĳ
min,
mp
) among the paths having 0 information at the time 
t and the minimum path metric (
c
t,
mp
) among all paths with symbol 1 at time t. 
The sign of 
)
Λ(it  determines the hard estimate at time t and its absolute value 
represents the soft output information which can be used in a next stage. 
If the decision is made at the end of the finite block length (as in block codes), 
SOVA can be implemented as a bidirectional recursive method with forward and 
backward recursions 
 
Bidirectional SOVA steps: 
 
a.  Forward recursion 
1) set the initial value: t=0, 
0
S0 =
, 
0
0)
(S
mp
0
x
0
=
=
, 
0
SĲ =
 (the infor-
mation block of length Ĳ starts from S0 and reaches 0 state at t = Ĳ; it has 
trellis termination) 

5.11   Turbo Codes 
359
 
2) apply Euclidian VA until the frame Ĳ (
0
SĲ =
) 
3) the survivor in frame Ĳ is the maximum likelihood path and its metric is 
Ĳ
min,
mp
 
b. Backward recursion: it applies the same steps as in forward recursion, but in 
opposite sense, from t = Ĳ to t = 0. 
1) set the initial values: t= Ĳ, 
0
SĲ =
, 
0
0)
(S
mp
0
x
Ĳ
=
=
,
0
S0 =
 
2) apply Euclidian VA from t= Ĳ to t=0. 
c. Soft decision 
1) start at t = 0 and continue until t = Ĳ 
2) at moment t, identify the ML estimate 
• 
0
it =
 or 1 
• 
determine: 
Ĳ
min,
i
t
mp
mp =
 
• 
find the path metric of its best competitor, the complementary 
one: 
:
1
i ⊕
 
)}
(S
mp
)
S
,
(S
mb
)
(S
min{mp
mp
j
b
t
j
k
c
t
k
f
1
t
c
t
+
+
=
−
                   (5.269) 
where 
k
S , 
j
S  are the distinct states of the trellis 
1)
(2
0,1,...,
S
,
S
1
K
j
k
−
=
−
 
– 
)
(S
mp
k
f
1
t−
- the path metric of the forward survivor at time t-1 and node 
k
S  
– 
)
S
,
(S
mb
j
k
c
t
- the branch metric at time t for complementary symbols from 
node 
k
S to  
j
S  
– 
)
(S
mp
j
b
t
- the backward survivor path metric at time t and node 
j
S . 
• 
compute   
1
t
0
t
t
mp
mp
)
Λ(i
−
=
                                       (5.270) 
 
Remarks concerning bidirectional SOVA 
 
• the complexity of forward recursion is the same to that of VA. 
• the complexity of backward recursion is less than VA because the survivors are 
not stored. 
 
Example 5.36 
Using the RSC code given in Example 5.35: R=1/2 and K=2, find the output of the 
decoder for the received sequence: 
1)]
1,
( 
1),
(0.8,
 
1,1),
( 
(1,1),
 [
r
r
4
Ĳ
−
−
−
−
=
=
 
1. using Viterbi algorithm (VA) 
2. using a bidirectional SOVA 

360 
5   Channel Coding
 
Solution 
 
1. The trellis of the code is given in Fig. 5.53.c and will be used for both decoding. 
00
11
10
01
01
0
S
11
11
11
00
00
00
01
10
10
1
2
3
4
0
r:   (1, 1)             (-1, 1)          (0.8, -1)          (-1, -1)
 
Fig. 5.54 Trellis representation on Ĳ=4 frames of RSC code with R=1/2 and K=2 
 
• In the first frame: t=1 
8
0
8
0
(1, 1)
 
1
1
2
2
1
1
mp
8
1)]
(
[1
1)]
(
[1
mb
=
=
−
−
+
−
−
=
 
2
1
2
2
2
1
mp
0
1]
[1
1]
[1
mb
=
=
−
+
−
=
 
• In the second frame: t=2 
 

5.11   Turbo Codes 
361
 
4
1)]
(
[1
1)]
(
1
[
mb
2
2
1
2
=
−
−
+
−
−
−
=
 
12
4
8
mp1
2
=
+
=
 
8
1)]
(
[1
1]
1
[
mb
2
2
2
2
=
−
−
+
−
−
=
 
18
8
0
mp2
2
=
+
=
 
4
1]
[1
1]
1
[
mb
2
2
3
2
=
−
+
−
−
=
 
12
4
8
mp3
2
=
+
=
 
0
1]
[1
1)]
(
1
[
mb
2
2
4
2
=
−
+
−
−
−
=
 
 
• In the third frame: t=3 
 
3.24
1)]
(
1
[
1)]
(
[0.8
mb
2
2
1
3
=
−
−
−
+
−
−
=
 
11.24
3.24
8
mp1
3
=
+
=
 
0.04
1)]
(
1
[
1]
[0.8
mb
2
2
2
3
=
−
−
−
+
−
=
 
0.04
0.04
0
mp2
3
=
+
=
 
4.04
1]
1
[
1]
[0.8
mb
2
2
3
3
=
−
−
+
−
=
 
12.04
4.04
8
mp3
3
=
+
=
 
7.24
1]
1
[
1)]
(
[0.8
mb
2
2
4
3
=
−
−
+
−
−
=
 
7.24
7.24
0
mp4
3
=
+
=
 

362 
5   Channel Coding
 
• In the fourth frame: t=4 
 
0
1)]
(
[1
1)]
(
1
[
mb
2
2
1
4
=
−
−
+
−
−
−
=
 
4.04
4
0.04
mp1
4
=
+
=
 
4
1)]
(
1
[
1]
1
[
mb
2
2
2
4
=
−
−
−
+
−
−
=
 
11.24
4
7.24
mp2
4
=
+
=
 
 
The ML path, with the minimum path metric is: 
00]
|
10
|
01
|
[11
 
 vˆ =
 corre-
sponding to the estimation: 
î= [1 0 1 0] 
 
2. SOVA 
a. 
Forward recursion is obtained applying VA starting from 
0
S0 =
 until 
4
SĲ =
 
The result of VA for the forward recursion is presented in Fig. 5.55. 
 
 
Fig. 5.55 Forward recursion; the thick line is representing the ML path (with minimum path 
metric 0.04) 

5.11   Turbo Codes 
363
 
The survivor path metrics are indicated above each node. The final survivor is 
the ML path with the metric 
0.04
mp
4
Ĳ
min,
=
=
. 
 
b. Backward recursion 
VA is applied starting with t = Ĳ = 4, until t = 0, in reverse sense. The backward 
recursion is shown in Fig. 5.56. The first number above each node indicates the 
forward survivor path metric and the second one the backward path metrics. 
 
r: (1,1) (-1,1) (0.8,-1) (-1,-1) 
0.04/0
01
10
0.04/0
8/4.04
8/3.24
0.04/0
0.04/0
7.24/4
 
Fig. 5.56 The backward recursion; ML path is presented with thick line. 
Remark: we propose to the reader to apply VA in the reverse sense and to check 
the results with the ones given in Fig. 5.56. 
 
c. 
Soft decision 
• 
We start at t = 0 
• 
At moment t = 1, we identify the ML estimate (hard estimate): 
1
i1 =
 and 
thus 
0.04
mp
mp
min,4
1
1
=
=
 
The complementary path metric is 
0
1
mp  and is the minimum path metric of the 
path that have 0 at time t = 1; its path metrics is calculated from Fig. 5.56 as the 
sum of the forward and backward survivor path metrics at node 0: 
12.04
4.04
8
mp0
1
=
+
=
 
Accordingly to (5.270), the log-likelihood ratio at time t = 1 is: 
12
0.04
12.04
mp
mp
)
Λ(i
1
1
0
1
1
=
−
=
−
=
 
• 
at moment t = 2, the ML estimate is 
0
i2 =
and the minimum path metric is 
0.04
mp
mp
min,4
0
2
=
=
 
- 
the best complementary competitor path at time t = 2, noted 
1
2
mp , is 
according to (5.269): 
)}
(S
mp
)
S
,
(S
mb
)
(S
{mp
min
mp
j
b
2
j
k
1
2
k
f
1
S,
S
1
2
j
k
+
+
=
 

364 
5   Channel Coding
 
where 
{0,1}
S
,
S
j
i
=
, 
)
(S
mp
k
f
1
is the forward survivor path metric at time 1 and 
node 
0}
 
{1,
Sk =
, 
)
S
,
(S
mb
j
k
1
2
is the branch metric at time t = 2 for the input in-
formation 1 and 
)
(S
mp
j
b
2
is the backward survivor path metric at moment t = 2 
and node 
{0,1}
Sj =
 
 
11.24
0.04)}
4
(8
3.24),
8
min{(0
mp1
2
=
+
+
+
+
=
 
- 
the log-likelihood ratio at time t = 2 is: 
11.2
11.24
0.04
mp
mp
)
Λ(i
1
2
0
2
2
−
=
−
=
−
=
 
In the same way we obtain: 
- 
at t = 3: 
0.04
0)}
0.04
(0
4),
4.04
min{(8
mp1
3
=
+
+
+
+
=
 
11.24
0)}
3.24
(8
4),
7.24
min{(0
mp0
3
=
+
+
+
+
=
 
11.2
0.04
11.24
mp
mp
)
Λ(i
1
3
0
3
3
=
−
=
−
=
 
- 
at t = 4: 
0.04
0)}
0
min{(0.04
mp0
4
=
+
+
=
 
11.24
0)}
4
min{(7.24
mp1
4
=
+
+
=
 
11.2
11.24
0.04
mp
mp
)
Λ(i
1
4
0
4
4
−
=
−
=
−
=
 
Thus, comparing 
)
Λ(it with zero threshold, the estimated analogue outputs are: 
}
2.
11
,
2.
11
,
2.
12
,
12
{
−
−
, corresponding to the digital outputs : { 1 0 1 0 }. 
We check easily that, if hard decision would be used, the decoded sequence is 
the same with the one obtained previously using VA: î= [1 0 1 0]. 
5.11.2.4   MAP Algorithm 
MAP is an optimal decoding algorithm which minimizes the symbol or bit error 
probability. It computes the log-likelihood ratio (5.260), which is a soft informa-
tion and can be used in further decoding stages. This value is used to generate a 
hard estimate using a comparison with zero threshold as in relation (5.261). 
We intend to illustrate MAP decoding using an example. 
 
Example 5.37 
MAP decoding for a RSC code with R = 1/2 and K = 3 
The block-scheme of the system is presented in Fig. 5.57. 
 
 
 

5.11   Turbo Codes 
365
 
 
 
 
 
00
11
00
11
11
11
00
00
10
10
10
10
10
11
11
00
00
01
01
01
01
01
S0=00
S1=10
S2=01
S3=11
c)
 
Fig. 5.57 MAP decoding illustration: a) block scheme of a transmission system with RSC 
(R = 1/2, K = 3, G = [1, (1 + x2)/ (1 + x + x2)]) and MAP decoding; b) state transition dia-
gram of RSC encoder with R = 1/2, K = 3, G = [1, (1 + x2)/ (1 + x + x2)]; c) trellis diagram 
of RSC code from b). 

366 
5   Channel Coding
 
We assume that the generated bits 
(1)
(0)  , v
v
 are BPSK modulated 
(
)1
 x
1,
x
(1)
(0)
+
=
−
=
 before entering the channel (AWGN with dispersion 
2
n
ı ). 
The decoding is block mode, meaning that decoding starts after a complete 
block of length Ĳ receiving 
)
( Ĳ
r
. It is assumed that decoding starts from all zero 
state of the encoder 
)
( 0
S
 and at the end of decoding (after Ĳ frames), it reaches 
the same state 
0
S . 
The transition probabilities of the AWGN channel are: 
(
)
(
)
∏
=
=
Ĳ
1
j
j
j
Ĳ
Ĳ
x
r
P
P
x
r
                                        (5.271) 
(
)
(
)
∏
=
=
1-
n
0
k
k
j,
k
j,
j
j
x
r
P
P
x
r
                                    (5.272) 
(
)
(
)
2n
2
k
j,
2ı
1
r
2
n
k
j,
k
j,
e
2π
1
1
x
r
P
+
−
=
−
=
ı
                          (5.273) 
(
)
(
)
2n
2
k
j,
2ı
1
r
2
n
k
j,
k
j,
e
2π
1
1
x
r
P
−
−
=
+
=
ı
                         (5.274) 
Be ti  the information bit associated with the transition 
t
1
t
S 
 to
S −
 and produc-
ing the output vt. The decoder will give an estimate tˆi , examining 
Ĳ
tr . 
The MAP algorithm calculates for each 
ti  the log likelihood ratio ( ( )
ti
Λ
) 
based on the received sequence 
Ĳ
r , according to the relation (5.260). MAP de-
coder makes a decision comparing 
( )
ti
Λ
 to a threshold equal to zero (relation 
(5.261)).  
The aposteriori probabilities (APP) can be calculated using Bayes relations 
(5.19): 
(
)
(
)
(
)
(
)
(
)
∑
=
′
=
=
=
∑
=
′
=
=
=
∈
′
−
∈
′
−
0t
0
t
l,l
Ĳ
Ĳ
t
1
t
l,l
Ĳ
t
1
t
Ĳ
t
)
P(
 l,
,l
P
l/ 
,l
P
0
i
P
T
T
r
r
S
S
r
S
S
r
                       (5.275) 
where 
0
t
T  is the set of transitions 
l
 
 to
l
t
1
t
=
′
=
−
S
S
 that are caused by informa-
tion bit 
0
it =
. 
 

5.11   Turbo Codes 
367
 
Analogue is calculated the APP of the information 
1
it =
: 
(
)
(
)
(
)
(
)
(
)
∑
=
′
=
=
=
∑
=
′
=
=
=
∈
′
−
∈
′
−
1t
1
t
l,l
Ĳ
Ĳ
t
1
t
l,l
Ĳ
t
1
t
Ĳ
t
)
P(
 l,
 ,l
P
 l,
 ,l
P
1
i
P
T
T
r
r
S
S
r
S
S
r
                         (5.276) 
where 
1
t
T  is the set of transitions 
l
S 
 to
l
S
t
1
t
=
′
=
−
 that are caused by information 
bit 
1
it =
. 
The joint probabilities will be noted with  
(
)
(
)
Ĳ
t
1
t
t
 l,
 ,l
P
l,l
ı
r
S
S
=
′
=
=
′
−
                                     (5.277) 
and (5.275) and (5.276) will be: 
(
)
(
)
(
)
∑
′
=
=
∈
′
0
t
l,l
Ĳ
t
Ĳ
t
)
P(
l,l
ı
0
i
P
T
r
r
                                    (5.275a) 
(
)
(
)
(
)
∑
′
=
=
∈
′
1
t
l,l
Ĳ
t
Ĳ
t
)
P(
l,l
ı
1
i
P
T
r
r
                                    (5.276a) 
The log-likelihood ratio can be expressed as: 
( )
(
)
(
)
(
)
(
)
∑
′
∑
′
=
Λ
∈
′
∈
′
0
t
1
t
l,l
t
l,l
t
t
l,l
ı
l,l
ı
log
i
T
T
                                         (5.278) 
The calculation of joint probabilities 
(
)l,l
ıt ′
 can be done introducing the fol-
lowing probabilities: 
( )
(
)
Ĳ
t
t
 l,
P
l
α
r
S =
=
                                         (5.279) 
( )
(
)l
P
l
ȕ
t
Ĳ
1
t
t
=
=
+
S
r
                                       (5.280) 
( )
(
)
{
}
1 
0,
 
  ,
l
 l,
 ,
P
l
Ȗ
1
t
Ĳ
t
t
t
=
′
=
=
=
=
−
i
S
r
S
i
i
i
                       (5.281) 
According to these notations, 
t
ı  and ( )
ti
Λ
 can be written: 
(
)
( ) ( )
(
)
∑
′
′
=
′
=
−
0,1
t
t
1
t
t
l,l
Ȗ
l
ȕ
l
α
l,l
ı
i
i
                                   (5.282) 
( )
( ) (
) ( )
( )
(
) ( )
(
)
∑
′
′
′
′
=
Λ
∈
′
−
−
1
,
t
0
t
1
t
t
1
t
1
t
t
l
ȕ
l,l
Ȗ
l
α
l
ȕ
l,l
Ȗ
l
α
log
i
tT
l
l
                             (5.283) 

368 
5   Channel Coding
 
and also: 
( )
( )
(
)
{
}
∑
∑
′
′
=
∈
′
=
−
S
i
i
l
0,1
t
1
t
i
l,l
Ȗ
l
α
l
α
                                    (5.284) 
( )
( )
(
)
{
}
∑
∑
′
′
=
∈
′
=
+
+
S
i
i
l
0,1
1
t
1
t
i
l,l
Ȗ
l
ȕ
l
ȕ
                                 (5.285) 
(
)
( )
[
]
(
)
⎪
⎪
⎪
⎩
⎪
⎪
⎪
⎨
⎧
∈
′
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
∑
−
−
=
′
−
=
−
otherwise
  
0,
  
l
l,
for 
 ,
2ı
)l(
x
r
exp
P
  
l,l
Ȗ
t
2
n
2
1
n
0
j
i
j
t,
1
n
j
i,
t
t
i
i
T
i
               (5.286) 
( )i
Pt
 is the apriori probability of information bit at moment t: 
i
it = , and 
( )l
xi
t
 
is the output of the encoder (after BPSK modulation), associated with transition 
l
 
 to
l
t
1
t
=
′
=
−
S
S
 and input 
i
it = . S is representing the distinct states of the en-
coder (in number of 2K-1, noted from 0 to 2K-1-1). 
In summary, MAP algorithm has the followings routines: 
 
1. Forward recursion (Fig. 5.58.a) 
– 
initialization of ( )l
α
 with 
S
∈
l
 
{
}
1
2 ,
1,
 
0,
1
K
−
=
−
…
S
, 
( )
( )
0
l
for 
 0
l
α 
and
 1
0
α
0
0
≠
=
=
. 
– 
for 
S
∈
=
l 
and
 Ĳ
1,
t
 calculate for all l branches of the trellis: 
(
)
( )
(
)
{
}
1 
0,
 ,
2ı
x
,
d
exp
P
l,l
Ȗ
2
n
t
t
2
t
t
=
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡−
=
′
i
r
i
i
                           (5.285a) 
where 
( )i
tP
 is the a apriori probabilities of each information bit (in most cases the 
equally probability is assumed), and 
(
)
t
t
2
x
,
d
r
 is the squared Euclidean distance 
between 
t
t
 x
and
 
r
 (the modulated it). 
– 
for 
{
}
(
) l,l
 Ȗ
store
 ,
1 
0,
t ′
=
i
i
 
– 
for 
S
∈
=
l 
and
 Ĳ
1,
t
 calculated and store 
( )l
αt
 given by (5.284). 
A representation of forward recursion is given in Fig. 5.57.a. 
 
2. Backward recursion (Fig. 5.57.b) 
– 
initialization of 
( )
S
∈
l  ,
l
ȕĲ
 
( )
( )
0 
 l
for 
 0 
 l
ȕ 
and
 1 
 
0
ȕ
Ĳ
Ĳ
≠
=
=
 

5.11   Turbo Codes 
369
 
– 
for 
S
∈
=
l 
and
 Ĳ
1,
t
, calculate 
( )l
ȕĲ
 with (5.285) 
– 
for 
Ĳ
t <
 calculate the log-likelihood ( )
ti
Λ
 with (5.283) 
 
Fig. 5.58 Graphical MAP representation: a) forward recursion, b) backward recursion 
Example 5.38 
Be the RSC code from Example 5.35 (
⎥⎦
⎤
⎢⎣
⎡
+
=
=
x
1
1
1,
G
 ,
2
1
R
), with BPSK modu-
lation and AWGN channel with 
2dB
N
E
0
b =
. The information sequence is 
(
) 1
  
0
  
0
  
1
  
1 
i =
. 
After Ĳ = 5, the received sequence is: 
( )
507154
,0
525812
,0
744790
,0
38405
,0
570849
,0
030041
,0
r 0
−
−
−
=
 
( )
591323
,1
904994
,1
495092
,0
107597
,1
753015
,0
726249
,0
r 1
−
−
−
−
=
 
The power of the signal is Ps = 0,5 W. 
Find the soft and hard outputs of a MAP decoder. 
 
Solution 
The encoder is presented in Fig. 5.53a and the state-diagram, respectively the trel-
lis are given in Fig. 5.53b, respectively c. 
In order to apply MAP, we need the trellis, the received sequence and also the 
knowledge of channel dispersion 
2
n
ı . 
 
 
 

370 
5   Channel Coding
 
The signal per noise ratio (SNR) can be expressed as: 
0
b
N
S
N
E
R
P
P
N
S
=
=
                                        (5.287) 
for BPSK modulation, where R is the coding rate. Taking into account that the 
length of the sequence 
5
Ĳ
L
=
=
 is small compared to the constant length (K = 2), 
R is expressed using (5.213): 
0,416
2
5
2
5
1
K
nL
mL
R
=
+
⋅
⋅
=
+
=
 
It follows from (5.287) that: 
758
,0
10
416
,0
5,0
N
E
R
P
 
ı
P
2,0
0
b
S
2
n
N
=
⋅
=
⋅
=
=
 
Then, MAP steps are applied: 
 
1. forward recursion 
According to the initialization, we have: 
( )
( )
0
l
for 
 0
l
α 
and
 1
0
α
0
0
≠
=
=
 
For 
5 
1,
t =
, according to (5.284) we have: 
( )
( )
(
)
{
}
∑
′
∑
′
=
=
=
−
0,1
t
1
0
l
1
t
t
l,l
Ȗ
l
α
l
α
i
i
i
 
– 
for t = 1, we have 
( )
( )
(
)
(
)
[
]
( )
(
)
(
)
[
]
0 
1,
Ȗ
0 
1,
Ȗ
1
α
0 
0,
Ȗ
0 
0,
Ȗ
0
α
0
α
1
1
0
1
0
1
1
0
1
0
1
+
+
+
=
 
We do not have 
(
)
0 
0,
Ȗ1
1
 and 
(
)
0 
1,
Ȗ0
1
, because in our case 
0
t
T  consists of (0, 0) 
and (1, 1) (if at the input is “0” we can have transitions only from 
0 
 l 
 to
0 
 l
=
=
′
or 
from 
1 
 l 
 to
1 
 l
=
=
′
). 
For 
1
t
T  it consists of (1, 0) and (0, 1), if at the input we have “1” there are tran-
sitions from 
0 
 l 
 to
1 
 l
=
=
′
or 
1 
 l 
 to
0 
 l
=
=
′
. 
( )
( )
(
)
(
)
[
]
( )
(
)
(
)
[
]
1 
1,
Ȗ
1 
1,
Ȗ
1
α
1 
0,
Ȗ
1 
0,
Ȗ
0
α
1
α
1
1
0
1
0
1
1
0
1
0
1
+
+
+
=
 
According to the above explanation, we obtain: 
( )
( )
(
)
( ) (
)
( )
( ) (
)
( )
(
)
⎪⎩
⎪⎨
⎧
+
=
+
=
1 
1,
Ȗ
1
α
1 
0,
Ȗ
0
α
1
α
0 
1,
Ȗ
1
α
0 
0,
Ȗ
0
α
0
α 
0
t
1-t
1
t
1-t
t
1
t
1-t
0
t
1-t
t
 
 
 

5.11   Turbo Codes 
371
 
– 
for 
⇒
=
6 
1,
t
  
( )
( )
(
)
( ) (
)
( )
( ) (
)
( )
(
)
⎪⎩
⎪⎨
⎧
=
+
⋅
=
+
=
=
+
⋅
=
+
=
255655
,0
0
255655
,0
1
1 
1,
Ȗ
1
α
1 
0,
Ȗ
0
α
1
α
034678
,0
0
034678
,0
1
0 
1,
Ȗ
1
α
0 
0,
Ȗ
0
α
0
α 
0
1
0
1
1
0
1
1
1
0
0
1
0
1
 
( )
( )
(
)
( )
(
)
( )
( )
(
)
( )
(
)
⎪⎪
⎩
⎪⎪
⎨
⎧
=
+
=
=
⋅
+
+
⋅
=
+
=
015322
,0
1 
1,
Ȗ
1
α
1 
0,
Ȗ
0
α
1
α
038815
,0
094143
,0
255655
,0 
   
          
425261
,0
034678
,0
0 
1,
Ȗ
1
α
0 
0,
Ȗ
0
α
0
α
0
2
1
1
2
1
2
1
2
1
0
2
1
2
 
and so on, until: 
 
( )
( )
(
)
( )
(
)
( )
⎪⎩
⎪⎨
⎧
=
=
+
=
001770
,0
1
α
00000338
,0
0 
1,
Ȗ
1
α
0 
0,
Ȗ
0
α
0
α
5
1
5
4
0
5
4
5
 
( )
( )
(
)
( )
(
)
( )
( )
(
)
( )
(
)
⎪⎩
⎪⎨
⎧
=
+
=
=
+
=
000002356
,0
1 
1,
Ȗ
1
α
1 
0,
Ȗ
0
α
1
α
000599
,0
0 
1,
Ȗ
1
α
0 
0,
Ȗ
0
α
0
α
0
6
5
1
6
5
6
1
6
5
0
6
5
6
 
2. backward recursion, applying relation (5.285) 
( )
( )
(
)
{
}
∑
′
∑
′
=
=
+
=
+
0,1
i
1
t
1
0
l
1
t
t
l,l
Ȗ
l
ȕ
l
ȕ
i
 
( )
( )
0 
 l
for 
 0 
 l
ȕ 
and
 1 
 
0
ȕ
Ĳ
Ĳ
≠
=
=
 (initialization) 
In our example: 
(
)
( )
( )
(
)
(
)
[
]
( )
(
)
(
)
[
]
( )
(
)
( )
(
)1 
0,
Ȗ
1
ȕ
0 
0,
Ȗ
0
ȕ 
       
          
          
1 
0,
Ȗ
1 
0,
Ȗ
1
ȕ
         
          
          
0 
0,
Ȗ
0 
0,
Ȗ
0
ȕ
0
ȕ
0
l
ȕ
1
1
t
1
t
0
1
t
1
t
1
1
t
0
1
t
1
t
1
1
t
0
1
t
1
t
t
t
+
+
+
+
+
+
+
+
+
+
+
=
=
+
+
+
+
=
=
=
 
( )
( )
(
)
(
)
[
]
( )
(
)
(
)
[
]
( )
(
)
( )
(
)1 
1,
Ȗ
1
ȕ
0 
1,
Ȗ
0
ȕ 
       
          
          
1 
1,
Ȗ
1 
1,
Ȗ
1
ȕ
0 
1,
Ȗ
0 
1,
Ȗ
0
ȕ
1
ȕ
0
1
t
1
t
1
1
t
1
t
1
1
t
0
1
t
1
t
1
1
t
0
1
t
1
t
t
+
+
+
+
+
+
+
+
+
+
+
=
=
+
+
+
=
 
( )
( )
⎭
⎬
⎫
⎩
⎨
⎧
=
=
0
1
ȕ
1
0
ȕ
6
6
 initialization 
( )
( )
(
)
( )
(
)
( )
( )
(
)
( )
(
)
⎪⎩
⎪⎨
⎧
=
+
⋅
=
+
=
=
+
⋅
=
+
=
338085
,0
0
338085
,0
1
1 
1,
Ȗ
1
ȕ
0 
1,
Ȗ
0
ȕ
1
ȕ
088558
,0
0
088558
,0
1
1 
0,
Ȗ
1
ȕ
0 
0,
Ȗ
0
ȕ
0
ȕ
0
6
6
1
6
6
5
1
6
6
0
6
6
5
 
and so, until 
( )
( )
(
)
( )
(
)
( )
( )
(
)
( )
(
)
⎪⎩
⎪⎨
⎧
=
+
=
=
+
=
001557
,0
1 
1,
Ȗ
1
ȕ
0 
1,
Ȗ
0
ȕ
1
ȕ
005783
,0
1 
0,
Ȗ
1
ȕ
0 
0,
Ȗ
0
ȕ
0
ȕ
0
2
2
1
2
2
1
1
2
2
0
2
2
1
 

372 
5   Channel Coding
 
With 
t
t ȕ ,
α
above calculated, we can calculate log-likelihood ratio according 
with (5.283) 
( )
( ) (
)
( )
( )
(
)
( )
( )
(
)
( )
( )
(
)
( )1
ȕ 
1 
1,
 Ȗ
1
α
0
ȕ 
0 
0,
 Ȗ
0
α
0
ȕ 
0 
1,
 Ȗ
1
α
1
ȕ 
1 
0,
Ȗ
0
α
log
i
t
0
t
1
t
t
0
t
1
t
t
1
t
1
t
t
1
t
1
t
t
−
−
−
−
+
+
=
Λ
 
( )
( ) (
)
( )
( )
(
)
( )
( )
(
)
( )
( )
(
)
( )
685564
,0
1
ȕ 
1 
1,
 Ȗ
1
α
0
ȕ 
0 
0,
 Ȗ
0
α
0
ȕ 
0 
1,
 Ȗ
1
α
1
ȕ 
1 
0,
Ȗ
0
α
ln
i
1
0
1
0
1
0
1
0
1
1
1
0
1
1
1
0
1
≈
+
+
=
Λ
 
and so on, until 
( )
( )
(
)
( )
( )
(
)
( )
( )
(
)
( )
( )
(
)
( )
72990
,7
1
ȕ 
1 
1,
 Ȗ
1
α
0
ȕ 
0 
0,
 Ȗ
0
α
0
ȕ 
0 
1,
 Ȗ
1
α
1
ȕ 
1 
0,
Ȗ
0
α
ln
i
6
0
6
5
6
0
6
5
6
1
6
5
6
1
6
5
6
≈
+
+
=
Λ
 
Comparing the soft outputs 
( )
ti
Λ
 to the threshold 0, we obtain the hard  
estimates: 
(
) 1 0 0 1 1 
iˆ =
. 
5.11.2.5   MAX-LOG-MAP Algorithm 
MAP decoding algorithm is considered too complex for implementation in many 
applications because of the required large memory and of the large time necessary 
to calculate a numerous multiplications and exponentials.  
A solution to avoid these drawbacks of MAP is to use the logarithms of 
(
)
( )
( )l
ȕ ,
l
α ,
l,l
Ȗ
t
t
i
t ′
: 
(
)
(
)l,l
Ȗ
l,l
 Ȗ
log
i
t
i
t
′
=
′
                                           (5.288) 
( )
( )
( )
(
)
{
}
∑
∑
=
=
∈
′
=
′
+
′
S i
l
0,1
l,l
Ȗ
l
α
t
t
it
1-t
e
 
log
l
α
l
α 
log
                      (5.289) 
with initial conditions 
( )
( )
0
l 
if
 ,
l
α 
and
 0
0
α
0
0
≠
−∞
=
=
. 
( )
( )
( )
(
)
{
}
∑
∑
=
=
∈
′
=
′
+
+
+
S i
l
0,1
l,l
Ȗ
l
ȕ
t
t
1
t
1
t
e
 
log
l
ȕ
l
ȕ 
log
                     (5.290) 
with initial conditions 
( )
( )
0
l 
if
 ,
l
ȕ 
and
 0
0
ȕ
Ĳ
Ĳ
≠
−∞
=
=
. 
The relation (5.283) can be written as: 
( )
( )
(
)
( )
( )
(
)
( )
∑
∑
=
Λ
∈
+
′
+
′
∈
+
′
+
′
−
−
S
S
i
l
l
ȕ
l,l
Ȗ
l
α
l
l
ȕ
l,l
Ȗ
l
α
t
t
0t
1
t
t
1t
1
t
e
e
log
                               (5.291) 

5.11   Turbo Codes 
373
 
The above expression can be simplified using the approximation  
(
)
i
n
1,
i
x
x
x
x
max
e
e
e
log
n
2
1
=
≈
+
+
+
…
                               (5.292) 
The relation (5.291) can be approximated by: 
( )
( )
(
)
( )
( )
(
)
( )⎥⎦
⎤
⎢⎣
⎡
+
′
+
′
−
⎥⎦
⎤
⎢⎣
⎡
+
′
+
′
≅
Λ
−
∈
−
∈
l
ȕ
l,l
Ȗ
l
α
max
l
ȕ
l,l
Ȗ
l
α
max
t
0
t
1
t
l
t
1
t
1
t
l
t
S
S
i
     (5.293) 
The computation of 
( )
( )l
ȕ 
and
 
l
α
t
t ′
 in (5.293) is equivalent to the computation 
of forward and backward recursion from Viterbi algorithm, since they can be  
written as: 
( )
( )
(
)
{
}
1 
0,
 i ,
l ,
l ,l
Ȗ
l
α
max
l
α
i
t
1
t
l
t
=
∈
⎥⎦
⎤
⎢⎣
⎡
′
+
′
=
−
∈
S
S
                      (5.294) 
( )
( )
(
)
{
}
1 
0,
 i ,
l,l ,
l ,l
Ȗ
l
ȕ
max
l
ȕ
i
1
t
1
t
l
t
=
∈
′
⎥⎦
⎤
⎢⎣
⎡
′
+
′
=
+
+
∈
S
S
                  (5.295) 
For each bit, the MAX-LOG-MAP algorithm calculates two Viterbi metrics and 
takes the largest one.  
5.11.2.6   LOG-MAP Algorithm 
Using the approximation relation (5.293), MAX-LOG-MAP algorithm is not 
anymore optimal, as MAP it is. An improvement can be obtained if Jacobian algo-
rithm [13] is used.  
It is based on a better approximation of logarithm of a sum of exponentials: 
(
)
(
)
(
)
(
)
1
2
2
1
z
z
2
1
z
z
z
z
fc
z ,
z
max
  
          
          
e
1
log
z ,
z
max
e
e
log
1
2
2
1
−
+
=
=
⎟⎠
⎞
⎜⎝
⎛+
+
=
+
−
−
                   (5.296) 
where (
)
1
2
z
z
fc
−
 is a correction function, that can be implemented using look-up 
table. 
For a sum of n exponentials, the approximation relation (5.296) can be com-
puted by a recursive relation: 
(
)
(
)
n
n
1
z
z
z
e
log
e
e
log
+
Δ
=
+
+…
                           (5.297) 
with 
z
z
z
e
e
e
1-
n
1
=
+
+
=
Δ
…
                                 (5.298) 
such that: 
(
)
(
)
(
)
(
)
(
) 
z
-
logz
fc
z 
logz,
max 
 
          
          
          
z
-
log
fc
z ,
log
max 
 
e
e
log
n
n
n
n
z
z
n
1
+
=
=
Δ
+
Δ
=
+
+…
                 (5.299) 

374 
5   Channel Coding
 
Such a recursive relation (5.299) can be used to evaluate 
( )
ti
Λ
 in relation 
(5.293). We identify zn: 
( )
(
)
( )
{
}
1 
0,
 
 i 
and
 
 
 l ,l 
are
n 
 ,
n 
 where
,
n
ȕ
n
,
n
Ȗ
n
α
z
t
i
t
1
t
n
=
∈
′
′
+
′
+
′
=
−
S
       (5.300) 
The performance of LOG-MAP is the same with MAP algorithm, but the com-
plexity is higher than that of MAX-LOG-MAP [46]. 
5.11.2.7   Comparison of Decoding Algorithms 
A very good comparison of all decoding algorithms is made in [46]. Some of these 
ideas will be presented in what follows: 
 
• MAP, the optimal decoding algorithm, in order to calculate 
( )
ti
Λ
, for each ti , 
considers all paths in the trellis, divided in two complementation sets (one that 
has 
1
it =
 and the second which has 
0
it =
). 
• MAX-LOG-MAP considers only two paths for each 
ti : the best path with 
0
it =
 and the best path with 
1
it =
. It computes 
( )
ti
Λ
 for each paths and re-
turns its difference. 
• SOVA takes two paths for each ti : one is the ML path and the other one is the 
best path with 
ti  (complementary 
ti ) to ML path, meaning that these two 
paths are identical to those considered by MAX-LOG-MAP. 
A comparison of complexity between all these algorithms is given in [46]. 
 
Remarks 
 
– 
the performance of soft output algorithms cannot be done in only one itera-
tion of decoding (BER); BER is defined only for hard estimation; 
– 
in [46], [21] are given performance comparison of distinct decoding  
algorithms. 
5.12   Line (baseband) Codes 
Baseband (BB) is defined as the bandwidth of the message signal, without fre-
quency transform, whereas the radiofrequency (RF) bandwidth refers to the band-
width of the band pass modulation. 
Baseband codes are called also line codes, transmission modes, baseband for-
mats/wave formats. These codes where developed for digital transmission over 
telephone cables(coaxial twisted pairs) or digital recording on magnetic media(in 
the 60’s) and recently for optic fiber transmission systems. In this category are in-
cluded also the codes used on free space optics (infrared remote control), codebase 
for paper printing, magnetic and optic recording, but these one will not be  
presented on the present book. 
Line codes acts as clothes that information wears in transmission or storage in 
different media (cables, wireless, optic fibber, magnetic tape, CD, DVD, etc.) 

5.12   Line (baseband) Codes 
375
 
The choice of a line code from a large family (line codes wardrobe) is done tak-
ing into account several aspects, which need to be appropriate to the application (it 
is analog to put a beautiful dressing for a wedding and not to take a swim suit): 
 
• signal spectrum (in fact the PSD - power spectral density): are important the 
bandwidth and the absence of the DC component. 
The presence of DC component requires DC couplings, meanwhile its absence 
allows AC couplings (transformers), the latter one ensures electrical isolation, this 
minimizing interferences. In storage, for magnetic recordings, the absence of DC 
component is also desired. 
The bandwidth of line codes need to be small compared to channel bandwidth 
to avoid ISI (intersymbol interference –see 2.8.5). 
PSD of distinct line codes is obtained as Fourier transform of the autocorrelation 
function ( )Ĳ
R
, and their graphical representation is given in [31], [39] and [49]. 
A coarse estimation of the bandwidth can be done using relation (2.32):  
1,25B
M
T
1
=
=
•
 
it follows that narrow pulses in the encoded sequence require larger bandwidth 
and vice versa. 
• bit or symbol synchronization (timing) is done using the received signal, based 
on signal transitions from high to low or low to high levels, meaning that line 
code format providing enough transition density in the coded sequence will be 
preferred. 
• code efficiency [49], η is defined as: 
[
]
100
x 
rate
n 
informatio
maximum
 l
theoretica
rate
n 
informatio
 
actual
:
η =
                    (5.301) 
• ratio 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
•
•
 
speed
 
signalling
 
rate
decision 
M
D
; certain codes provide an increase of 
•
D  for the 
same signalling speed, so without increasing the frequency bandwidth of the 
signal 
• error performance: BER expressed as a function of SNR 
• error detection capacity: without being error control codes (as presented in 5), 
certain BB codes have features that can be used in error detection. 
• immunity to polarity inversion: same BB codes, the differential ones, are im-
mune to the accidental inversion of the wires in a pair, causing the inversion of  
“0” and “1” 
• complexity and costs of the encoding-decoding equipment 
Terminology used for these codes states that to a binary unit we assign a volt-
age or current level: mark (M) and to a binary zero we assign zero level: space(S): 
S
0 
M,
1
→
→
. 

376 
5   Channel Coding
 
Differential coding is a technique used in BB coding as well as band pass 
modulation in order to obtain immunity to polarity inversion of the signal. 
The block-schemes of the differential encoder and decoder are presented in  
Fig. 5.59 
 
 
Fig. 5.59 Differential a) encoder and b) decoder;  
where:  
 
– 
ak – the original binary data; 
– 
dk – the differentially encoded data;  
– 
⊕ – modulo 2 adder 
– 
k
d′  – received differentially encoded data 
– 
k
a′  – decoded data 
An example of differential coding is given in Table 5.17 
 
Table 5.17 Example of differential coding. 
k
a
 
 
1 
0 
0 
1 
1 
0 
1 
0 
0 
1 
k
1
k
k
d
d
a
=
⊕
−
 
1 
0 
0 
0 
1 
0 
0 
1 
1 
1 
0 
k
d′ correct polarity 
1 
0 
0 
0 
1 
0 
0 
1 
1 
1 
0 
k
1
k
k
a
d
d
′
=
⊕
′
−
 
 
1 
0 
0 
1 
1 
0 
1 
0 
0 
1 
k
d′ inverse polarity 
0 
1 
1 
1 
0 
1 
1 
0 
0 
0 
1 
k
1
k
k
a
d
d
′
=
⊕
′
−
 
 
1 
0 
0 
1 
1 
0 
1 
0 
0 
1 
 
In differential decoding the signal is decoded pointing out the transitions, and 
not the absolute values of the signals, which allow high noise immunity. Another 
great advantage of differential coding is its insensitivity to the absolute polarity of 
the signal, meaning that an accurate decoding is possible although the polarity of a 
pair is accidentally converted. 
 
 

5.12   Line (baseband) Codes 
377
 
Majority of BB codes can be classified in the following groups: 
 
a. non-return to zero(NRZ;NRZ-L,NRZ-M,NRZ-S) 
b. return to zero(RZ) 
c. biphase(B, BL, BM, BS, Miller) 
d. pseudoternary(AMI, twined binary: dicode NRZ, dicode RZ) 
e. substitution(BNZS,HDBn,CMI,DMI ) 
In the followings the most important BB codes will be described. 
a. NON-Return to Zero: NRZ-L, NRZ-M,NRZ-S 
• NRZ-L (Non Return to Zero Level): “1” is represented by a constant level dur-
ing the bit duration T, and “0” by another constant level during T; the code can 
be unipolar (0, A), (0, -A) or polar/antipodal signal (-A, +A); 
• NRZ-M (Non Return to Zero Mark) for a “1” there is a transition at the begin-
ning of the bit interval: for “0” there is not level change; 
• NRZ-S (Non Return to Zero Space): for a “0” there is a transition at the begin-
ning of the bit interval; for “1” there is not level change; 
• NRZ-M and NRZ-S are differential versions of NRZ-L and have the advan-
tages of any differential code over NRZ-L; 
– 
if the “0”, and “1” are equally probable in the signal and the signalling is 
polar (+A, -A), NRZ has no DC component; if the signal is unipolar (0,A), 
DC component is A/2. 
– 
applications of NRZ codes: NRZ-L is the basic code for digital logic (the 
terminals, for example); NRZ-M-is used in magnetic tape recording; 
– 
the lack of transitions for long strings of “0”  or “1” means poor bit syn-
chronization capacity and therefore, NRZ codes have limited applications 
in telecommunications. 
b. Return to Zero (RZ): for a “1” a positive pulse of T/2 width is transmitted, thus 
the name of Return to Zero; for a “0”, zero level is transmitted during T, the bit 
duration, in a unipolar RZ code, or a negative pulse of T/2 width is transmitted 
for a polar RZ code 
– 
if “0”and “1” are equally probable, no DC component for a polar RZ and 
A/4 DC component for unipolar RZ; 
– 
synchronization capacity is good for polar RZ (there are two transitions per 
bit) but poor for unipolar RZ, when for long strings of “0” there are not 
transitions; 
– 
the pulse duration being T/2, and not T as for NRZ codes it means that the 
bandwidth of RZ codes is doubled compared to NRZ 
c. Biphase codes  
• B-L (Biphase Level-Manchester): a ”0” is represented by 01 and a ”1” by 10; it 
means that each bit contains two states of T/2 duration. At each bit the signal 
phase changes with π, therefore the name biphase. This code is known also as 
Manchester code; 

378 
5   Channel Coding
 
• B-M (Biphase Mark): there is a transition at the beginning of each bit. For”1” 
the second transition occurs after T/2 and for “0”, there is not the second transi-
tion until the beginning of the next bit. 
• B-S (Biphase Space): there is a transition at the beginning of each bit. For “0” 
the second transition occurs after T/2 and for “1” there is not the second transi-
tion until the beginning of the next bit. 
• Differential BL: a transition takes place in the middle of each bit regardless it is 
0 or 1. For “0” the first transition takes place at the beginning of the bit and the 
second occurs after T/2. For “1” the unique transition is at T/2. 
• Delay Modulation (Miller code) is a variety of biphase code; for “1” a transi-
tion occurs at T/2. For “0”, a transition takes place at the end of the bit if it is 
followed by a “0”; if a “1” comes next, there is no transition at the end of the 
bit. As shows in Fig 5.61, Miller code is obtained from BL (Manchester) by 
suppressing each second transition. For Miller code there exist at least one tran-
sition every 2 bits (corresponding to the most unfavorable sequence:101) and 
there is never more than one transition per bit, which means that it has a good 
bit synchronization capacity and very small DC component, making from it an 
excellent code for magnetic recordings [20]. 
Main features of biphase codes: 
– 
B-M and B-S are differential versions of B-L 
– 
all the three biphase codes have at least one transition in a bit duration (T), 
this providing a very good bit synchronization; 
– 
all the three biphase codes have not DC component if the signalling is polar; 
– 
applications: magnetic recording, optical and satellite communications. 
d. Pseudo ternary line codes 
These binary codes are using three levels: +A, -A and 0, which explains the name 
of pseudo ternary (in telecommunications they are called often bipolar codes).  
• AMI (Alternate Mark Inversion) is the most important code of this group. A “0” 
is encoded with zero level, during the whole bit duration. A “1” is encoded with 
level +A or –A, alternatively, from where the name of alternate mark inversion. 
AMI can be encoded both in NRZ and RZ forms; however the most used form 
is RZ. AMI has no DC component; identifying its use in digital telephony with 
scrambling. 
– 
scramblers are pseudo noise sequences, generated with LFSR (see 5.8.5) 
used to ensure p(0) ≈ p(1) in any binary  sequence, this improving the bit 
synchronization  capacity of the signal and subsequently eliminating the 
main disadvantage; 
– 
redundancy in the waveform allows the possibility to monitor the quality 
of the line without knowing the nature of the traffic being transmitted. 
Since the “1”s alternate as polarity it means that any violation of this rule 
implies an error. In this way, the line code, without being properly an error 
control code (see cap.5), acts for error detection. The numbers of bipolar 

5.12   Line (baseband) Codes 
379
 
violation are counted and if the frequency of their occurrence exceeds 
some threshold, an alarm is set. This error detection is done in the traffic 
supervising the line code 
– 
AMI has been widely used in digital coaxial or pair cable systems (in early 
T1-digital multiplex of 1,544 Mbps and E1-European primary multiplex of 
2,048Mbps) [4], [9]; 
• Dicode (twinned binary) [11], [49] are related to differential coding. If the data 
sequence to be encoded is {ak}, a dicode is obtained encoding AMI the se-
quence {dk}, where dk=ak-1-ak. 
 
Example: {ak}={ 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 1 } and a0=1 
        {dk}={ 0 , 1 , -1 , 1 , -1 , 1 , 0 , 0 , 0 , -1 , 0 , 1 , 0 , 0 , 0 , -1 } 
– 
Dicode can be represented in NRZ or RZ (Fig. 5.61) 
e. Substitution line codes 
AMI code, despite its great advantages: no DC component and error detection 
capacity has a great draw-back: without scrambler, bit synchronization capacity is 
very poor; long strings of “0”s, which is very frequent in telephony, means no bit 
timing recovery. In order to avoid this major disadvantages without scrambling 
there are two popular zero substitution codes: binary N-zero substitution (BNZS) 
and high density bipolar n( HDB n) codes. 
 
– 
Binary N-Zero Substitution Codes (BNZS), were proposed by V.I.Johannes 
in 1969 [25]. It replaces a string of N zeros in AMI format with a special 
N-bit waveform with at least one bipolar violation. Thus density of pulses 
is increased while the original data are obtained recognizing the bipolar 
violation and replacing them at receiver with N zeros. 
Example: B3ZS, a code of three zero substitution, used in DS-3 signal interface 
in North America standard and in LD-4 coaxial transmission system in Canada [4]. 
 
0 0 0 → 000V, where V means bipolar violation 
→ B0V, where B means a single pulse with bipolar alternation 
 
In both substitution, the bipolar violation occurs in the last position of the three 
zeros replaced by the code, and this position is easily identified. The decision to 
substitute with 00V or B0V is made according to the number of bipolar pulses (B) 
transmitted since the last substitution: 
 
• if odd  
→ 
0 0 V 
• if even  
→ 
B 0 V 
 

380 
5   Channel Coding
 
In this way the encoded signal has an odd number of bipolar violation. Also, 
bipolar violation alternate in polarity, so the DC component is missing. 
An even number of bipolar pulses between bipolar violations occurs only as a 
result of channel error. 
Other BNZS used in telephony are: B6ZS [4], B8ZS [4]. 
• HDBn (High Density Bipolar n) was proposed by A.Croisier [11] in 1970. 
These codes limit the number of consecutive zeros to n, replacing the (n+1) 
consecutive zeros by a sequence with bipolar violation. 
 
Example HDB-3: 
 
• 0000 → 0 0 0 V if m is odd 
• 0000 → B 0 0 V if m is even 
where m represents the number of consecutive marks since the last substitution. 
HDB3 is used in 2.048Mbps, 8.448Mbps and 34.368 Mbps multiplex within the 
European digital hierarchy [9]. 
In optical communication systems a symbol is represented by the intensity of 
the light given by the optical source, the laser diode. This is why a ternary code (as 
AMI), can not be used. A solution to avoid this difficulty was to replace the zero 
level of AMI with two-level waveforms. 
 
• Coded Mark Inversion (CMI), proposed by Takasaki [44] in 1976, uses alter-
nate signalling (A, -A) for “1”s in NRZ format, and for “0” a sequence 01. CMI 
enhances the transition density and thus the timing capacity and based on the 
alternation of “1”s, has error detection capacity through the monitoring of po-
larity violation (as AMI). It is used in the 139.246 Mbps multiplex within the 
European digital hierarchy [9].  
• Differential Mode Inversion Code (DMI), proposed also by Takasaki [44] is 
similar to CMI for coding “1”s and for “0”s differ in such a way to avoid pulses 
larger than T: 0->01 or 10. 
f. Multilevel Signalling 
Many of the baseband codes presented so far are binary, using two-levels 
(
•
•
= M
D
). There are applications with limited bandwidth and higher data rates re-
quirements. Data rate can be enhanced increasing the number of signalling levels 
(increasing the alphabet m>2) while keeping the same signalling rate 
T
1
M =
•
. 
Data rate (
•
D ) of a multilevel system is (2.27): 
m
log
T
1
m
log
M
D
2
2
=
⋅
=
•
•
 

5.12   Line (baseband) Codes 
381
 
In this case the moment rate 
•
M  is known as symbol rate. An example of qua-
ternary signalling: m=4 is achieved by 2 bits per signal interval (T): 0 0 → -3A, 0 
1 → -A, 10 → +A  and  1 1 → + 3 A. 
Error performance of line codes. Each application requires a minimum BER at 
the user. In transmission systems, if this value is obtained at lower SNR, the re-
generative repeaters can be spaced farther, this reducing installation and mainte-
nance costs [4]. 
A thorough analysis and demonstrations of BER for most line codes can be 
found in [49]. 
Further we present a short description of signal detection at receiver. Basiscs of 
signal detection are found in Appendix C. For binary symbols, detection means to 
take decision of which signal was transmitted, by comparing at the appropriate 
time (at T/2, T, according to the type of decision) the measurement with a thresh-
old located halfway between the two levels (0 in polar case, A/2 for unipolar and 
0, +A/2, -A/2 for bipolar signalling). The BER, after detections is a function of 
SNR. 
 
 
0
μ
)
p(r/s1
)
p(r/s0
)
/s
P(D
1
0
)
/s
P(D
0
1
+
⋅
=
=
)
/s
P(D
P
P
BER
0
1
0
e
Q(SNR)
)
/s
P(D
P
1
0
1
=
⋅
+
 
Fig. 5.60 Signal detection: a) block-scheme and b) illustration of BER 
 
 
 

382 
5   Channel Coding
 
The significance of the notations in Fig. 5.60 is: 
 
– 
( )
( )
( )
symbols
binary 
 
 the
- 
t
s
t
s
t
s
0
1
i
⎩
⎨
⎧
=
 
– 
n(t)=noise, assumed AWGN (Additive White Gaussian Noise) 
– 
r(t) = si(t) + n(t) – received signal 
– 
Eb = energy per bit 
– 
N0 = spectral density power of AWGN 
– 
V2 = power of the signal at the decision block (comparator) 
– 
2
n
ı  = power of the noise (R is assumed unitary) 
– 
p(r/si) = probability density function of r/si 
– 
D0, D1 = decision 0
sˆ , respectively 1sˆ  
– 
0
sˆ , 1sˆ = estimated binary signals after decision 
– 
Pe = bit error rate (BER) 
– 
Q = erfc (see Appendix C) 
The optimal receiver for binary signals is using a continuous observation and can 
be implemented using a correlated or a matched filter [27], [45] and (Appendix C). 
Polar signalling ( A, -A) is optimal because it maximizes the SNR; in this case 
the two signals are identical, but opposite(antipodal signalling). For this reason it 
is used often as basis for comparisons. The line codes with polar signalling are 
NRZ and biphase. 
Unipolar signalling codes use the same detector as for polar signal detection. 
The only difference is that the decision threshold is moved from 0 to A/2. To keep 
the same BER as in polar case, the power of the signal need to be doubled, mean-
ing that unipolar signalling is 3dB less with respect to polar performance (see  
Appendix C). 
Bipolar signalling (AMI and derived: decode, BNZS, HDBn) from error per-
formance point of view is identical to unipolar code. During any interval the re-
ceiver must decide between 0 and ±A/2. A slightly higher BER than in unipolar 
signalling occurs because both positive and negative noise may cause an errone-
ous threshold, crossing when a zero-level signal is transmitted. A graphical repre-
sentation of BER for these three types of signalling is given in Fig. 4.25, pag. 198 
from [4]. 
Multilevel signalling requires the same bandwidth as binary systems (
T
1
M =
•
 is 
the same), but uses m > 2 levels instead of two. The penalty is the increase of sig-
nal power in order to keep the same BER (see quaternary signalling Fig.5.61, 
which uses ±3A levels in addition). A graphical representation is given in Fig. 
4.26, pag.201 from [4]. 
 
Comparative analysis of line codes 
 
Comparison of line codes is made having in mind the aspects mentioned at the  
beginning of this chapter: 

5.12   Line (baseband) Codes 
383
 
• signal spectrum 
– 
no DC component (advantage): biphase codes, AMI, dicode, HDB-n, 
BNZS, CMI, DMI 
– 
with DC component (drawback): NRZ, RZ 
– 
bandwidth in increasing order: multilevel coding, Miller, NRZ, biphase, 
RZ, dicode, CMI, DMI, BNZS 
• bit synchronization capacity:  
– 
good: biphase codes, BNZS, CMI, DMI 
– 
poor: NRZ, RZ, AMI 
• code efficiency (η), defined by (5.301);  
– 
NRZ: 1 bit is encoded into 1 binary symbol 
(100%)
 1
2
log
2
log
η
2
2
NRZ
=
=
 
– 
Manchester:1 bit is encoded into 2 binary symbols 
(50%)
 
0,5
 
 
2
1
2
log
2
2
log
η
2
2
M
=
=
⋅
=
 
– 
AMI:1 bit is encoded into 1 ternary symbol 
(63%)
 
0,63
 
 
59
,1
1
3
log
2
log
η
2
2
AMI
≈
=
=
 
• error performance, as presented before, is the best for polar (antipodal) codes, 
than follow unipolar codes and bipolar codes. 
• error detection capacity is possible for AMI, HDBn, CMI, and DMI, based on 
the violation of encoding laws, without being properly error control codes. 
• immunity to polarity inversion is given by all differential codes. 
 
Example 5.38 
Encode in all presented base-band codes the sequence:  
 
101010100001100001 
 
The encoding is presented in Fig. 5.61. 

384 
5   Channel Coding
 
 
Fig. 5.61 Examples of Base Band encoding 
 
 
 
 
 

References 
385
 
References 
[1] Aaron, M.R.: PCM transmission in the exchange plant. Bell System Technical Jour-
nal 41, 99–141 (1962) 
[2] Angheloiu, I.: Teoria codurilor. Editura Militara, Bucuresti (1972) 
[3] Benice, R., Frey, A.: An analysis of retransmission systems. IEEE Transactions on 
Comm. Tech., 135–145 (1964) 
[4] Bellamy, J.: Digital Telephony, 2nd edn. John Willey & Sons, New York (1991) 
[5] Berrou, C., Glavieux, A.: Near Shannon limit error-correcting coding and decoding 
turbo-codes. In: Proc. ICC 1993, Geneva, pp. 1064–1070 (1993) 
[6] Blahut, R.: Algebraic Methods for Signal Processing and Communication Coding. 
Springer, Heidelberg (1992) 
[7] Bose, R., Chandhuri, D.R.: On a class of error correcting binary group codes. Inf. 
Control 3, 68–70 (1960) 
[8] CCITT yellow book, Digital Networks-Transmission systems and multiplexing 
equipment vol. III.3, Geneva, ITU (1981) 
[9] Chen, C.: High-speed decoding of BCH codes. IEEE Trans. on Inf. Theory IT 2, 254–
256 (1981) 
[10] Chien, R.: Cyclic decoding procedure for the BCH codes. IEEE Trans. Inf. Theory 
IT 10, 357–363 (1964) 
[11] Croisier, A.: Introduction to pseudo ternary transmission codes. IBM Journal of Re-
search and Development, 354–367 (1970) 
[12] Cullmann, G.: Codage et transmission de l’information. Editions Eyrolles, Paris 
(1972) 
[13] Erfanian, J., Pasupathy, S., Gulak, G.: Reduced complexity symbol detectors with 
parallel structure for ISI channels. IEEE Transactions on Communications 42(234), 
1661–1671 (1994) 
[14] Evans, M.: Nelson Matrix Can Pin Down 2 Errors per Word. Electronics 55(11), 
158–162 (1982) 
[15] Fontolliet, P.: Systemes de telecommunications. Editions Georgi, Lausanne (1983) 
[16] Forney, G.: On decoding binary BCH codes. IEEE Trans. on Inf. Theory IT 11, 580–
585 (1965) 
[17] Forney Jr., G.D.: Concatenated codes. MIT Press, Cambridge Mass (1966) 
[18] Hamming, R.: Coding and information theory. Prentice-Hall, Englewood Cliffs 
(1980) 
[19] Haykin, S.: Communication Systems, 4th edn. John Wiley & Sons, Chichester (2001) 
[20] Hecht, M., Guida, A.: Delay modulation. Proceedings IEEE 57(7), 1314–1316 (1969) 
[21] Hanzo, L., Liew, T.H., Yeap, B.L.: Turbo Coding. In: Turbo Equalisation and Space-
Time Coding for Transmission over Fading Channels, Wiley-IEEE Press, Chichester 
(2002) 
[22] Hocquenghem, A.: Codes corecteurs d’erreurs. Cliffres 2, 147–156 (1959) 
[23] Hsiao, M.: A class of optimal mińiḿum odd-weight-column SEC-DED codes. IBM 
Journal of Research and Development 14(4), 395–401 (1970) 
[24] Ionescu, D.: Codificare si coduri. Editura Tehnica, Bucuresti (1981) 
[25] Johannes, V.I., Kaim, A.G., Walzman, T.: Bipolar pulse transmission with zero ex-
traction. IEEE Transactions in Communications Techniques 17, 303 (1969) 
[26] Johnston, B., Johnston, W.: LD-4. A Digital Pioneer in Action. Telesis 5(3), 66–72 
(1977) 

386 
5   Channel Coding
 
[27] Kay, S.M.: Fundamentals of Statistical Signal Processing. In: Kay, S.M. (ed.) Detec-
tion Theory, vol. 1, Prentice Hall, Englewood Cliffs (1998) 
[28] Lin, S., Costello, D.: Error control coding. Prentice-Hall, Englewood Cliffs (1983) 
[29] Lidle, R., Niederreiter, H., Cohn, P.M.: Finite Fields. Cambridge University Press, 
Cambridge (1997) 
[30] Massey, J.: Shift register synthesis and BCH decoding. IEEE Trans. on Inf. Theory 
IT 17, 464–466 (1971) 
[31] Mateescu, A.: Manualul inginerului electronist. Transmisuni de date, Editura Tehnica, 
Bucuresti (1983) 
[32] Odenwalder, J.P.: Error control coding handbook. Linkabit Corporation, San Diego 
(1976) 
[33] Omura, J.K.: On the Viterbi decoding algorithm. IEEE Trans. on Inf. Theory IT 15, 
177–179 (1969) 
[34] Patel, A., Hong, S.: Optimal rectangular code for high density magnetic tapes. IBM J. 
Res. Dev. 18, 579–588 (1974) 
[35] Patel, A.M.: Error recovery scheme for the IBM 3850 mass storage system. IBM J. 
Res. Dev. 24, 32–44 (1980) 
[36] Peterson, W., Weldon, W.: Error correcting codes. MIT Press, Cambridge (1972) 
[37] Price, W., Barber, D.: Teleinformatica - Retele de calculatoare si protocoalele lor. 
Editura Tehnica, Bucuresti (1983) 
[38] Qualcomm, Inc., Q1401, Viterbi decoder device information (1987) 
[39] Radu, M., et al.: Telefonie numerica. Editura Militara, Bucuresti (1988) 
[40] Reed, I., Solomon, G.: Polynomial codes over certain finite fields. J. Soc. Ind. Appl. 
Math. 8, 300–304 (1960) 
[41] Shannon, C.: A Mathematical Theory Of Communication. Bell System Technical 
Journal 27, 379–423 (1948); Reprinted in Shannon Collected Papers, IEEE Press 
(1993) 
[42] Sklar, B.: Digital communications. Prentice-Hall, Englewood Cliffs (1988) 
[43] Spataru, A.: Fondements de la theorie de la transmission de linformation. Presses 
Polytechniques Romandes, Lausanne (1987) 
[44] Takasaki, Y., et al.: Optical pulse formats for fiber optic digital communications. 
IEEE Transactions on Communications 24, 404–413 (1976) 
[45] Van Trees, H.L.: Detection, Estimation and Modulation Theory, Part.I. John Wiley & 
Sons, Chichester (1968) 
[46] Vucetic, B., Yuan, J.: Turbo codes. Kluver Academic Publishers Group, Boston 
(2001) (2nd printing) 
[47] Wade, G.: Signal coding and processing. Cambridge University Press, Cambridge 
(1994) 
[48] Wade, G.: Coding Techniques. Palgrave (2000) 
[49] Xiong, F.: Digital Modulation Techniques. Artech House (2000) 
[50] Gallager, R.G.: Information Theory and Reliable Communication. John Willey & 
Sons, New York (1968) 
[51] Slepian, D.: Key Papers in the Development of Information Theory. IEEE Press, Los 
Alamitos (1974) 
[52] Shparlinski, I.: Finite Fields: Theory and Computation. Kluver Academic Publishers 
Group, Boston (1999) 
[53] Lidl, R., Niederreiter, H.: Finite Fields, Encyclopedia of Mathematics and Its Appli-
cations, vol. 20. Addison-Wesley Publ. Co., Reading (1983) 

References 
387
 
[54] McEliece, R.J.: The Theory of Information and Coding, 2nd edn. Cambridge Univer-
sity Press, Cambridge (2002) 
[55] Heegard, C., Wicker, S.B.: Turbo Coding. Kluver Academic Publishers Group, Bos-
ton (1999) 
[56] Lee, C.L.H.: Convolutional Coding – Fundamentals and Applications. Artech House, 
Boston (1997) 
[57] Wozencraft, J.M., Jacobs, I.M.: Principles of Communication Engineering. Waveland 
Press, Prospect Heights (1990) 
[58] Peterson, W.W., Weldon Jr., E.J.: Error-correcting Codes, 2nd edn. MIT Press, Cam-
bridge (1972) 
[59] Ramsey, J.: Realization of optimum interleavers. IEEE Transactions on Information 
Theory 16(3), 338–345 (1970) 
[60] Forney Jr., G.: Burst-Correcting Codes for the Classic Bursty Channel. IEEE Trans-
actions on Communication Technology 19(5), 772–781 (1971) 

Appendix A: Algebra Elements 
A1   Composition Laws 
A1.1   Compositions Law Elements 
Let us consider M a non empty set. An application ϕ defined on the Cartesian 
product M x M with values in M: 
(
)
(
)
y
x,
y
x,
 
Ȃ,
Ȃ
 x 
Ȃ
ϕ
→
→
                                   (A.1) 
is called composition law on M; it defines the effective law by which to any or-
dered pair (x, y) of M elements is associated a unique element 
y)
(x,
ϕ
, which be-
longs to the set M as well. 
The mathematical operation in such a law can be noted in different ways: 
0 ,
 ,
 ,
∗
−
+
 etc. We underline the fact that the operations may have no link with the 
addition or the multiplication of numbers. 
A1.2   Stable Part 
Let us consider M a set for which a composition law is defined and H a subset of 
M. The set H is a stable part of M related to the composition law, or is closed to-
wards that law if: 
H
y)
(x,
:
H
y
x,
∈
∈
∀
ϕ
 
where ϕ  is the composition law. 
 
Example 
• The set Z of integer numbers is a stable part of the real numbers set R towards 
addition and multiplication. 
• The natural numbers set N is not a stable part of the real numbers set R towards 
subtraction. 
A1.3   Properties 
The notion of composition law presents a high degree of generality by the fact that 
the elements nature upon which we act and the effective way in which we act are 
ignored. 

390 
Appendix A: Algebra Elements 
 
The study of composition laws based only on their definition has poor results. 
The idea of studying composition laws with certain properties proved to be useful, 
and these properties will be presented further on. For the future, we will assume 
the fulfilment of the law: 
(
)
y
x
y
x,
 
Ȃ,
Ȃ
 x 
Ȃ
∗
→
→
 
Associativity: the law is associative if for
M
z
y,
x,
∈
∀
: 
z)
(y
x
z
y)
(x
∗
∗
=
∗
∗
                                          (A.2) 
If the law is additive, we have: 
z)
(y
x
z
y)
(x
+
+
=
+
+
 
and if it is multiplicative, we have: 
(
)
(
)
yx
x
z
xy
=
 
Commutativity: the law is commutative if for 
M
z
y,
x,
∈
∀
: 
x
y
y
x
∗
=
∗
                                                  (A.3) 
Neutral element: the element 
M
e∈
is called neutral element if: 
x
e
x
x
e
=
∗
=
∗
, 
M
x ∈
∀
                                      (A.4) 
It can be demonstrated that if it exists, then it is unique. 
For real numbers, the neutral element is 0 in addition and 1 in multiplication 
and we have: 
x
x
1
1
    x
x;
0
0
x
=
⋅
=
⋅
+
=
+
 
Symmetrical element: an element 
M
x ∈
 has a symmetrical element referred to 
the composition law ∗, if there is an  
M
x ∈
′
 such that: 
e
x
x
x
x
=
′
∗
=
∗′
                                            (A.5) 
where e is the neutral element. 
The element x′  is the symmetrical (with respect to x) of x. 
From the operation table (a table with n rows and n columns for a set M with n 
elements, we can easily deduce whether the law is commutative, whether it has 
neutral element or whether it has symmetrical element. Thus: 
 
• if the table is symmetrical to the main diagonal, the law is commutative 
• if the line of an element is identical with the title line, the element is neutral one 
• if the line of an element contains the neutral element, the symmetrical of that 
element is to be found on the title line on that column where the neutral element 
belongs. 
 
 

A2   Modular Arithmetic 
391
 
Example 
Be the operation table: 
 
* 
1 
2 
3 
4 
1 
1 
2 
3 
4 
2 
2 
4 
1 
3 
3 
3 
1 
4 
2 
4 
4 
3 
2 
1 
 
– 
neutral element is 1 
– 
operation is commutative 
– 
symmetrical of 2 is 3 and so on. 
A2   Modular Arithmetic 
In technical applications, the necessity of grouping integer numbers in setss accord-
ing to remainders obtained by division to a natural number n, frequently occurs. 
Thus, it is known that for any 
Z
∈
a
, there is a q, 
Z
∈
r
, uniquely determined, 
so that: 
r
q
n
a
+
⋅
=
,  
1
-
n 
...,
 
1,
 
0,
r =
                                     (A.6) 
The set of numbers divisible to n, contains the numbers which have the remain-
ders 1, …, the remainder n-1, and they are noted with 
1
nˆ
...
,1ˆ,0ˆ
−; they are giving 
the congruence modulo n classes, denoted with 
n
Z . 
The addition and multiplication are usually noted with ⊕ and ⊗. The addition 
and multiplication are done as in regular arithmetic. 
 
Example 
For 
5
Z , we have: 
 
⊕ 
0 
1 
2 
3 
4 
0 
0 
1 
2 
3 
4 
1 
1 
2 
3 
4 
0 
2 
2 
3 
4 
0 
1 
3 
3 
4 
0 
1 
2 
4 
4 
0 
1 
2 
3 
 
⊗ 
0 
1 
2 
3 
4 
0 
0 
1 
2 
3 
4 
1 
1 
1 
2 
3 
4 
2 
2 
2 
4 
1 
3 
3 
3 
3 
1 
4 
2 
4 
4 
4 
3 
2 
1 

392 
Appendix A: Algebra Elements 
 
For subtraction, the additive inverse is added: 
3
1
2
4
2
=
+
=
−
, because the in-
verse of 4 is 1. It is similar for division: 
4
2
2
3
/
1
2
3
/
2
=
⋅
=
⋅
=
, because the 
multiplicative inverse of 3 is 2. 
 
Remark 
These procedures will be used for more general sets than the integer numbers. 
A3   Algebraic Structures 
By algebraic structure, we understand a non zero set M characterized by one or 
more composition laws and which satisfy several properties, from the above men-
tioned ones, known as structure axioms. For the problems that we are interested in, 
we will used two structures, one with a composition law called group and the 
other one with two composition laws, called field. Some other related structures 
will be mentioned too. 
A3.1   Group 
A joint (
)
∗
G,
formed by a non empty set G and with a composition law on G: 
G
y 
 
y with x,
x
y)
(x,
  
G,
G
G
∈
∗
→
→
×
 
is called group if the following axioms are met: 
 
• associativity: 
z)
(y
x
z
y)
(x
∗
∗
=
∗
∗
, 
G
z
y,
x,
∈
∀
 
• neutral element: 
G
x
x
e
x
x
e
,
G
e
∈
∀
=
∗
=
∗
∈
∃
                                (A.7) 
• symmetrical element; when the commutativity axiom is valid as well: 
G
 
y 
 
 x,
x,
y
y
x
∈
∀
∗
=
∗
the group is called commutative or Abelian group. 
 
If G has a finite number of elements, the group is called finite group of order m, 
where m represents the elements number. 
 
Remarks 
1. In a group, we have the simplification rules to the right and to the left: 
c
b
c
a
b
a
=
⇒
∗
=
∗
                                         (A.8) 
c
b
a
c
a
b
=
⇒
∗
=
∗
                                         (A.9) 
2. If in a group (
)
∗
G,
, there is a set 
G
H ⊂
, so that (
)
∗
H,
 should at its turn form 
a group, this is called subgroup of G, having the same neutral element and in-
verse as G. 
3. If the structure contains only the associative axiom and the neutral element, it is 
called monoid. 

A3   Algebraic Structures 
393
 
Example 
The integer numbers form a group related to addition, but not related to multipli-
cation, because the inverse of integer k is /k
1
, which is not an integer for 
1
k ≠
. 
The congruence modulo any n classes, are Abelian groups related to addition, 
and the ones related to multiplication are not Abelian groups unless the module n 
is prime, as we see in table for 
5
Z . When the module is not prime, the neutral 
element is 1 as well, but there are elements that do not have symmetrical number, 
for example element 2 in 
4
Z : 
 
⊗ 
1 
2 
3 
1 
1 
2 
3 
2 
2 
0 
2 
3 
3 
2 
1 
 
A3.2   Field 
A non empty set A with two composition laws (conventionally named addition 
and multiplication), and symbolised with +  and •  , is called field if: 
 
• (
)
+
Α,
 is an Abelian group 
• (
)
•
Α ,
1
is an  Abelian group, 
{ }
0
ǹ/
ǹ1 =
 where “0” is the neutral element of 
(
)
+
Α,
 
• Distributivity of multiplication related to addition: 
xz
xy
z)
x(y
+
=
+
   
    
Remarks: 
• if (
)
•
Α ,
1
 is a group, without being Abelian, the structure is called body; so the 
field is a commutative body. If (
)
•
Α ,
1
is monoid only, then the structure is ring.  
• the congruence modulo p classes, with p prime, form a ring. Rings may contain 
divisors of 0, so non zero elements with zero product. In the multiplication ex-
ample 
4
Z  we have 
0
2
2
=
∗
, so 2 is a divisor of 0. These divisors of 0 do not 
appear in bodies. 
A3.3   Galois Field 
A field can have a finite number m of elements in A. In this case, the field is 
called m degree finite field. The minimum number of elements is 2, namely the 
neutral elements of the two operations, so with the additive and multiplicative no-
tations: 0 and 1. In this case, the second group contains a single element, the unit 
element 1. The operation tables for both elements are in 
2
Z : 
 
⊕ 
0 
1 
 
 
 
⊗ 
0 
1 
0 
0 
1 
 
 
 
0 
0 
0 
1 
1 
0 
 
 
 
1 
0 
1 

394 
Appendix A: Algebra Elements 
 
This is the binary field, noted with GF(2), a very used one in digital processing. 
If p is a prime number, 
p
Z  is a field, because 
}
1
-
p 
...,
 
2,
 
1,
{
form a group with 
modulo p multiplication. 
So the set 
}
1
-
p 
...,
 
2,
 
1,
{
forms a field related to modulo p addition and multipli-
cation. 
This field is called prime field and is noted by GF(p). 
There is a generalisation which says that, for each positive integer m, we should 
extend the previous field into a field with pm elements, called the extension of the 
field GF(p), noted by GF(
m
p
). 
Finite fields are also called Galois fields, which justifies the initials of the nota-
tion GF (Galois Field). 
A great part of the algebraic coding theory is built around finite fields. We will 
examine some of the basic structures of these fields, their arithmetic, as well as the 
field construction and extension, starting from prime fields.  
A.3.3.1   Field Characteristic 
We consider the finite field with q elements GF(q), where q is a natural number. 
If 1 is the neutral element for addition, be the summations: 
k terms
1
1
1
1
,
2,
1
1
1
 
1,
1
k
1
i
2
1
i
1
1
i
↓
=
=
=
+
+
+
=
∑
=
+
=
∑
=
∑
…
…
 
As the field is closed with respect to addition, these summations must be ele-
ments of the field. 
The field having a finite number of elements, these summations cannot all be 
distinct, so they must repeat somewhere; there are two integers m and n (
)
n
m <
, 
so that 
∑
=
∑
⇒
∑
=
=
−
=
=
m
1
i
m
n
1
i
n
1
i
0
1
1
1
 
There is the smallest integer Ȝ  so that 
0
1
Ȝ
1
i
=
∑
=
. This integer is called the char-
acteristic of the field GF(q).  
The characteristic of the binary field GF(2) is 2, because the smallest Ȝ  for 
which         
0
1
Ȝ
1
i
=
∑
=
 is 2, meaning 
0
1
1
=
+
 
The characteristic of the prime field GF(p) is p. It results that: 
 
• the characteristic of a finite field is a prime number 
• for n, 
Ȝ
m <
 ,  ∑
=
n
1
i
1 ≠∑
=
m
1
i
1  

A4   Arithmetics of Binary Fields 
395
 
• the summations: 1, ∑
=
2
1
i
1 , ∑
=
3
1
i
1 , . . . , ∑
−
=
1
Ȝ
1
i
1  ,
0
1
Ȝ
1
i
=
∑
=
 : are Ȝ  distinct elements in 
GF(q), which form a field with Ȝ  elements GF( Ȝ ), called subfield of GF(q). 
Subsequently, any finite field GF(q) of characteristic Ȝ  contains a subfield 
with Ȝ  elements and it can be shown that if 
Ȝ
q ≠
 then q is an exponent of Ȝ . 
A.3.3.2   Order of an Element 
We proceed in a similar manner for multiplication: if a is a non zero element in 
GF(q), the smallest positive integer, n, so that 
1
an =  gives the order of the  
element a. 
This means that 
1
a , . . . ,
a
  
a,
n
2
= are all distinct, so they form a multiplicative 
group in GF(q). 
A group is called cyclic group if it contains an element whose successive expo-
nents should give all the elements of the group. If in the multiplicative group, 
there are q-1 elements, we have 
1
a
1-
q
= for any element, so the order n of the 
group divides 
1
-
q
. 
In a finite field GF(q) an element a is called primitive element if its order is 
1
-
q
. The exponents of such an element generate all the non zero elements of 
GF(q). Any finite field has a primitive element. 
 
Example 
Let us consider the field GF(5), we have: 
 
1
2
,3
2
,4
2
,2
2
4
3
2
1
=
=
=
=
so 2 is primitive 
1
3
,2
3
,4
3
,3
3
4
3
2
1
=
=
=
=
so 3 is primitive 
1
4
,4
4
2
1
=
=
, so 4 is not primitive. 
A4   Arithmetics of Binary Fields 
We may build a power of p. We will use only binary codes in GF(2) or in the ex-
tension GF(
m
2
). Solving equations and equation systems in 
2
Z  is not a problem, 
as 
0
1
1
=
+
, so 
1
1
−
=
. 
The calculations with polynomials in GF(2) are simple too, as the coefficients 
are only 0 and 1. 
The first degree polynomials are X and 
1
X + , the second degree ones are 
2
2
2
2
X
X
1
,
X
X
,
X
1
  ,
X
+
+
+
+
. Generally, there are 
n
2  polynomials of degree 
n, the general form of the n degree polynomial being: 
 
 

396 
Appendix A: Algebra Elements 
 
n
n
1
n
1-
n
1
0
Χ
f
Χ
f
...
Χ
f
f
+
+
+
+
−
, meaning n coefficients, so that: 
n
0
n
1
n
n
n
n
2
C
...
C
C
=
+
+
+
−
                                        (A.10) 
We notice that a polynomial is divisible by 
1
+
Χ
 only if it has an even number 
of non zero terms. An m degree polynomial is irreducible polynomial on GF(2) 
only if it does not have any divisor with a smaller degree than m, but bigger than 
zero. From the four second degree polynomials, only the last one is irreducible; 
the others being divided by X or 
1
+
Χ
. The polynomial 
1
X
X3
+
+
 is irreducible, 
as it does not have roots 0 or 1, it cannot have a second degree divisor; 
3
2
X
X
1
+
+
 is also irreducible. 
We present further 4th and 5th degree irreducible polynomials.  
The polynomial 
1
X
X4
+
+
 is not divided by X or 
1
+
Χ
, so it does not have 
first degree factors. It is obvious that it is not divided by 
2
X  either. If it should be 
divided by 
1
2 +
Χ
, it should be zero for
1
2 =
Χ
, which, by replacement, leads to 
0
X
1
X
1
≠
=
+
+
; it cannot be divided by 
X
Χ2 +
either, as this one is 
1)
X(X +
. 
Finally, when we divide it by  
1
X
2
+
+
Χ
 we find the remainder 1.There is no 
need for us to look for 3rd degree divisors, because then it should also have first 
degree divisors. So the polynomial 
1
X
X4
+
+
 is irreducible. 
There is a theorem stating that any irreducible polynomial on GF(2) of degree 
m divides 
1
X
1
2m
+
−
. 
We 
can 
easily 
check 
whether 
the 
polynomial 
1
X
X3
+
+
divides 
1
X
1
X
7
1
23
+
=
+
−
; 
as 
1
X
X3
=
+
 
we 
have 
1
X
X
2
6
+
=
and 
1
X
1
X
X
X
X
3
7
=
+
+
=
+
=
, so 
0
1
X7
=
+
 
An irreducible polynomial p(X) of degree m is primitive, if the smallest positive 
integer n for which p(X) divides 
1
X n +
 is 
1
2m −. In other words, p(X) must be 
the simultaneous solution of the binomial equations 
0
1
X
1
2m
=
+
−
 and 
0
1
Xn
=
+
, with 
1
2
n
m −
≤
. This does not occur except if n is a proper divisor of  
1
2m −, as we shall show further on. If 
1
2m − is prime, it does not have own divi-
sors (except 
1
2m −and 1), so any irreducible polynomial is primitive as well. 
Thus we may see that 
1
X
X4
+
+
divides 
1
X15 + , but it does not divide any 
polynomial 
1
Xn + , with 
15
n
1
≤
≤
, so it is primitive. The irreducible polynomial 
1
X
X
X
X
2
3
4
+
+
+
+
 is not primitive because it divides 
1
X5 + . But if 
5
m =
 , 
we have 
31
1
25
=
−
, which is prime number, so all irreducible polynomials of 5th 
degree are primitive as well. 
For a certain m there are several primitive polynomials. Sometimes (for cod-
ing), the tables mention only one that has the smallest number of terms. 

A4   Arithmetics of Binary Fields 
397
 
We will demonstrate the affirmation that the binomial equations  
0
1
Xm
=
+
 
and 
0
1
Xn
=
+
 with 
n
m <
, do not have common roots unless m divides n. In 
fact, it is known that the m degree roots, n respectively, of the unit are: 
m
2kπ
sin
i
m
2kπ
cos
X
⋅
+
=
, 
1
m
0,
k
−
=
                            (A.11) 
m
π
2k
sin
i
m
π
2k
cos
X
1
1
1
⋅
+
=
, 
1
m
0,
k1
−
=
                         (A.12) 
In order to have a common root, besides 
1
=
Χ
, we should have: 
n
m
k
k
,
m
π
2k
m
2kπ
1
1
⋅
=
→
=
                                   (A.13) 
But k∈Z, which is possible only if m and n have a common divisor noted d. 
The common roots are the roots of the binomial equation
0
1
Xd
=
+
, the other 
ones are distinct, d being the biggest common divisor of m and n. 
In order to find the irreducible polynomials related to a polynomial, these ones 
must be irreducible in the modulo-two arithmetic, so they should not have a divi-
sor smaller than them. 
The fist degree irreducible polynomials are X and 
1
+
Χ
. In order that a polyno-
mial be not divisible to X, the free term must be 1, and in order not to be divisible to 
1
+
Χ
, it should have an odd number of terms. 
For the 2nd degree polynomials, the only irreducible one is 
1
X
2
+
+
Χ
. 
For the 3rd, 4th and 5th degree polynomials, we shall take into account the pre-
vious notes and we shall look for those polynomials which are not divided by 
1
X
2
+
+
Χ
. 
The remainder of the division is obtained replacing in the polynomial: 
1
X
2
+
+
Χ
,  
1
3 =
Χ
, 
X
Χ4 =
, etc. 
For the 3rd degree irreducible polynomials, which should divide:  
1
ȕX
αΧ
X
2
3
+
+
+
, one of the coefficients must be zero, otherwise the total num-
ber of terms would be even. Similarly, taking into account the previous notes, the 
remainder of the polynomial divided by 
1
X
2
+
+
Χ
 is: 
α
ȕ)X
(α
+
+
. 
We will have the following table: 
 
α 
β 
(α+β)X+α 
Polynomial 
Irreducible 
Primitive 
1 
0 
X+1≠0 
X3+X2+1 
YES 
YES 
0 
1 
X≠0 
X3+X+1 
YES 
YES 
 
For each of the two cases, as the remainder is non zero, the polynomial is irre-
ducible. It is obtained by replacing in the general form the corresponding values of 
the coefficients α and β. 

398 
Appendix A: Algebra Elements 
 
For the 4th degree irreducible polynomials, we note the polynomial by: 
1
ȖX
ȕX
αΧ
X
2
3
4
+
+
+
+
.  
In order to have the total number of terms odd, all coefficients, or only one of 
them, must equal 1. As the remainder of the division by 
1
X
2
+
+
Χ
 is: 
1
ȕ
α
Ȗ)
ȕ
X(1
+
+
+
+
+
, we have the following table: 
 
α 
β 
γ 
X(1+β+γ)+α+β+1 
Polynomial 
Irreducible Primitive 
1 
1 
1 
X+1≠0 
X4+X3+X2+X+1 
YES 
NO 
1 
0 
0 
X≠0 
X4+X3+1 
YES 
YES 
0 
1 
0 
X•2+2= 0 
X4+X2+1 =  
(X2+X+1)2 
NO 
NO 
0 
0 
1 
X≠0 
X4+X+1 
YES 
YES 
 
The first and the third are not primitive, as they divide 
1
X5 − and 
1
X3 −, re-
spectively, with a lower degree than 
1
X15 −. Indeed, 3 and 5 are divisors of 15.  
Further on, we will present an important property of the polynomials on GF(2): 
( )
[
]
(
)
2
2
X
f
X
f
=
                                             (A.14) 
Proof 
Be 
n
n
1
0
X
...f
X
f
f
f(X)
+
+
=
, we will have: 
,)
f(X
)
(X
f
...
)
(X
f
X
f
f
X
...f
X
f
f
(X)
f
2
n
2
n
2
2
2
2
1
0
2n
2
n
2
2
1
2
0
2
=
+
+
+
+
=
+
+
=
  
as 
0
1
1
=
+
 and 
i
2
i
f
f
=
. 
A5   Construction of Galois Fields GF(2m) 
We want to set up a Galois field with 
m
2
 elements (
)1
m >
 in the binary field 
GF(2), starting from its elements 0, 1 with the help of a new symbol α, as follows: 
j
i
i
j
j
i
  times
j
j
2
 ,
  ,
,
  ,
 
 
  ,
  ,
 ,
+
=
⋅
=
⋅
⋅
⋅
⋅
=
⋅
=
=
⋅
=
⋅
=
⋅
=
⋅
=
⋅
=
⋅
=
⋅
α
α
α
α
α
α
α
α
α
α
α
α
α
1
α
α
1
0
α
0
1
1
1
0
0
1
1
0
0
0
0

	

…
…
                 (A.15) 
We will consider the set: 
{
} . . . ,
 , . . . ,
 ,
 ,
 
F
j
α
α
1
0
=
                                        (A.16) 
Be the set F={0, 1, α,…, αj,…} to contain 
m
2
 elements and to be closed re-
lated to the above multiplication. Be p(X) a primitive polynomial of degree m with 
coefficients in GF(2). We suppose that ( )
0
p
=
α
 . As p(X)  divides: 
1
X
1
2m
+
−
  

A5   Construction of Galois Fields GF(2m) 
399
 
( )
( ) ( )
X
p
X
q
1
X
  :
X
q 
1
2m
=
+
∃
−
                                 (A.17) 
Replacing X by α in the relationship (A.17), we will obtain: 
( )
( )
0
p
q
1
1
2m
=
⋅
=
+
−
α
α
α
                                    (A.18) 
so: 
1
1
2m
=
−
α
                                               (A.19) 
With the condition ( )
0
p
=
α
, the set F becomes finite and will contain the  
elements: 
⎭⎬⎫
⎩⎨⎧
=
−
∗
2
2
2
m
F
α
α
α
1
0
"
                            (A.20) 
The non zero elements of 
*
F are closed to the previously defined multiplica-
tion, which is easily demonstrated as follows: 
Be  
1
2
j
i  
if
    
1;
2 
  j
  
i,
      
,
F
  
 ,
m
m
j
i
−
>
+
−
<
∈
∗
α
α
 we have: 
(
)
1
2
  
,
  
r,
1
2
j
i
m
r
1)
(2
j
i
j
i
m
m
−
<
=
+
=
=
⋅
+
−
=
+
−
+
r
α
r
α
α
α
α
          (A.21) 
The set F is thus closed with respect to multiplication. In order that this set to 
be field, it needs to fulfil the field axioms.  
From (A.15) and (A.16), we can see that the multiplication is commutative and 
associative having the neutral element 1. The inverse of αi is 
i
1)
(2m
−
−
α
. 
The elements 
2
2
2
m
 ,
 ,
,
−
α
α
α
1
 being distinct, they determine a group with the 
operation •. 
We will further define the addition operation "+" on 
*
F , so that the elements 
should form a group with the operation "+". 
In order to facilitate the definition, we will first express the elements of the set 
*
F  with the help of polynomials, checking the group axioms. 
Be p(X) a primitive polynomial of degree m and  
1
2
i
0
m −
≤
≤
, where i is the 
degree of a polynomial 
i
X . We divide the polynomial by p(X): 
( ) ( )
( )
Χ
a
Χ
p
Χ
q
Χ
i
i
i
+
=
, 
1
m
a
deg
i
−
≤
                           (A.22) 
The form of the remainder ai(X) is: 
( )
(
)
1
m
1
m
i
1i
0
i
i
Χ
a
...
Χ
a
a
Χ
a
−
−
+
+
+
=
                              (A.23) 
Since 
i
X  and p(X) are relatively prime (from the definition of primitive poly-
nomials), we have: 
( )
0
ai
≠
Χ
                                                 (A.24) 

400 
Appendix A: Algebra Elements 
 
For 
1
2
j
  ,i
0
m −
<
≤
 and 
j
i ≠, we can show that:
( )
( )
Χ
a
Χ
a
j
i
≠
. If they should 
be equal: 
( )
( )
[
] ( )
( )
( )
( )
( )
[
] ( )
X
p 
X
q
X
q
X
a
X
a
X
p 
X
q
X
q
X
X
j
i
j
i
j
i
j
i
+
=
+
+
+
=
+
      (A.25) 
It results that p(X) should divide 
(
)
i-j
i
j
i
X
1
X
X
X
+
=
+
. 
Since p(X) and 
i
X  are relatively prime, p(X) should divide 
i-j
X
1+
, which 
contradicts the definition of the prime polynomial p(X), not to divide any polyno-
mial with a smaller degree than
1
2m −, or 
1
2
i
j
m −
≤
−
. The hypothesis is fake, 
so for any 
1
2
j
  ,i
0
m −
<
≤
 and 
j
i ≠  we must have: 
( )
( )
Χ
a
Χ
a
j
i
≠
                                           (A.26) 
For 
2
2 ,
1,
 
0,
m −
…
, we obtain 
1
2m − non zero distinct polynomials ai(X) of 
degree m-1 or smaller. 
Replacing X by α, in the relation (A.22), and taking into account the fact that 
( )
0
α
p
=
 , we obtain: 
( )
(
)
1
m
1
m
i
1
i
0
i
i
i
a
...
a
a
a
−
−
+
+
+
=
=
α
α
α
α
                      (A.27) 
The 
1
2m − non zero elements
2
2
2
1
0
m
  . . .  ,
   
,
  ,
−
α
α
α
α
of 
*
F  may be repre-
sented by 
1
2m − distinct non zero polynomials over GF(2) of degree (m – 1) or 
smaller. The 0 element in 
*
F  may be represented by the null polynomial. 
In the following, we will define addition “+” on 
*
F : 
0
0
0
=
+
                                               (A.28) 
and for 
1
2
j
  ,i
0
m −
<
≤
 
i
i
i
α
0
α
α
0
=
+
=
+
                                       (A.29) 
(
)
(
)
1
m
1)
j(m
1
m
i
1j
1
i
0
j
0
i
j
i
)
a
(a
...
)
a
(a
a
a
−
−
−+
+
+
+
+
+
=
+
α
α
α
α
        (A.30) 
the additions 
je
ie
a
a +
 being modulo-two summations. 
From above, for 
j
i = , it results 
0
i
i
=
+ α
α
 and for 
j
i ≠, we have: 
(
)
(
)
1
m
1)
j(m
1
m
i
1j
1
i
0
j
0
i
)
a
(a
...
)
a
(a
a
a
−
−
−+
+
+
+
+
+
α
α
, non zero.  
The relation (A.29) must be the polynomial expression for a certain 
k
α  in 
*
F . 
So the set F is closed to addition operation “+”, previously defined. 

A5   Construction of Galois Fields GF(2m) 
401
 
We can immediately check that 
*
F  is a commutative group for “+” operation. 
We notice that 0 is the additive identity; the addition modulo two being commuta-
tive and associative, the same thing happens for F*. From (A.29) for 
j
i =
 we no-
tice that the additive inverse (the opposite) is the element itself in 
*
F . 
It was shown that 
⎭⎬⎫
⎩⎨⎧
=
−
∗
2
2
2
m
  . . .  
   
   
   
F
α
α
α
1
0
 is a commutative group for 
addition “+” and that the non zero elements in 
*
F  form a commutative group for 
multiplication “•”. Using the polynomial representation for the elements in 
*
F  and 
taking into account the fact that the polynomial multiplication satisfies the law of 
distributivity related to addition, it is easily shown that multiplication in 
*
F  is dis-
tributive towards to addition in 
*
F . 
So, the set 
*
F  is a Galois field with 
m
2
 elements, GF
)
2
( m . All the addition 
and multiplication operations defined in 
*
F =GF
)
2
( m  are done modulo two. It is 
thus noticed that (0, 1) form a subfield of GF
)
2
( m , so GF(2) is a subfield of 
GF
)
2
( m , the first one being called the basic field of GF
)
2
( m . The characteristic 
of GF
)
2
( m is 2.  
When constructing GF
)
2
( m from GF(2), we have developed two representa-
tions for the non zero elements in GF
)
2
( m , an exponential representation and a 
polynomial one. The first one is convenient for multiplication, and the second one, 
for addition. There is also a third representation, matrix-type, as the following  
examples will show. 
 
Remarks 
In determining GF
)
2
( m , we act as follows: 
 
• we set the degree m of the primitive polynomial p(X) 
• we calculate 
2
2m −
, which will give the maximum number of powers of α ob-
tained from the primitive polynomial, after which this one is repeated 
1
1
2m
=
−
α
 
• from the equation ( )
0
p
=
α
we obtain 
m
α
, after which any exponent is obtained 
from the previous one, taking into consideration the reduction ( )
0
p
=
α
 
 
Example 
We will determine the elements of GF
)
2
(
3 , generated by the primitive polyno-
mial 
3
X
X
1
p(X)
+
+
=
. 
 

402 
Appendix A: Algebra Elements 
 
We have 
3
m =
 and 
6
2
2m
=
−
, so 
1
7 =
α
, α being a root of p(X) for which  
0
α
α
1
=
+
+
3
 so: 
 
1
α
1
α
α
α
α
α
1
α
α
α
α
α
α
1
α
α
α
α
α
α
α
1
α
=
+
+
=
+
=
+
=
+
+
=
+
+
=
+
=
+
=
+
=
3
7
2
3
2
6
2
3
2
5
2
4
3
 
 
For the matrix representation, we consider a linear matrix (a1 a2 a3) in which a1, 
a2, a3 are the coefficients of the terms α0, α1, and α2, respectively. So, for 
α
1
α
+
=
3
, we will have the matrix representation (1 1 0). Similarly for the other 
exponents of α.The table below presents the elements of the field GF
)
2
( 3 , gener-
ated by the polynomial 
3
X
X
1
p(X)
+
+
=
. 
 
α 
power representation 
Polynomial 
representation 
Matrix 
representation 
0 
0 
0    0    0 
1 
1 
1    0    0 
α 
         α 
0    1    0 
α2 
                    α2 
0    0    1 
α3 
1  +   α 
1    1    0 
α4 
     α  +    α2 
0    1    1 
α5 
1  +  α  +    α2 
1    1    1 
α6 
1  +              α2 
1    0    1 
α7 
1 
1    0    0 
 
Appendix A10 includes the tables for the representation of the fields GF
)
2
( 3  
GF
)
2
( 4 , GF
)
2
( 5 , GF
)
2
( 6 . 
A6   Basic Properties of Galois Fields, GF(2m)  
In the common algebra, we have seen that a polynomial with real coefficients does 
not have roots in the field of real numbers, but in that of complex numbers, which 
contains the previous one as a subfield. This observation is true as well for the 
polynomials with coefficients in GF(2) which may not have roots from this one, 
but from an extension of the field GF
)
2
( m . 
 

A6   Basic Properties of Galois Fields, GF(2m) 
403
 
Example 
The polynomial 
1
X
X
3
4
+
+
 is irreducible on GF(2), so it does not have roots in 
GF(2). Nevertheless, it has 4 roots in the extension GF
)
2
( 4 , namely, by replacing 
the exponents of α (see A.10), in the polynomial, we find that 
2
8
4
  ,
  ,
  ,
α
α
α
α
  are 
roots, so the polynomial can be written: 
(
)(
)(
)(
)
8
4
2
3
4
α
X
α
X
α
X
α
X
1
X
X
+
+
+
+
=
+
+
 
Let now be p(X), a polynomial with coefficients in GF(2). If β, an element in 
GF
)
2
( m  is a root of p(X), then we question whether p(X) has other roots in 
GF
)
2
( m  and what are those roots. The answer lies in the following property: 
 
Property 1: Be p(X), a polynomial with coefficients in GF(2). Be β an element of 
the extension of GF(2). If β is a root of p(X), then for any 
l
2
0,
1
β
≥
   is also a 
root of p(X). 
This is easily demonstrated taking into account relation (A.14): 
( )
[
]
⎟⎠
⎞
⎜⎝
⎛
=
l
l
2
2
X
p
X
p
 by replacing X by β we have:  
( )
[
]
⎟⎠
⎞
⎜⎝
⎛
=
l
l
2
2
p
p
β
β
 
So, if ( )
0
β =
p
, it results that  and so 
0
β
=
)
p(
l2
 so 
l
2
β
is also root of p(X). 
This can be easily noticed from the previous example. The element 
i
2
β
is called 
the conjugate of β . Property 1 says that if β is an element in GF
)
2
( m  and a root 
of the polynomial p(X) in GF(2), then all the distinct conjugates of β, elements of 
GF
)
2
( m , are roots of p(X). 
For example, the polynomial ( )
6
5
4
3
X
X
X
X
1
X
p
+
+
+
+
=
 has 
4
α
 , as root in 
GF
)
2
( 4 : 
( )
( )
( )
( )
( )
(
)
(
) (
)
0
α
α
α
α
α
α
α
α
1
1
α
α
α
α
1
α
α
α
α
1
α
α
α
α
1
α
=
+
+
+
+
+
+
+
+
+
=
+
+
+
+
=
=
+
+
+
+
=
+
+
+
+
=
3
2
9
2
9
5
12
24
20
16
12
4
6
4
5
4
4
4
3
4
         
p
 
The conjugates of  
4
α  are: 
( )
( )
( )
2
32
2
4
16
2
4
8
2
4
3
2
,
,
α
α
α
α
α
α
α
α
=
=
=
=
=
 

404 
Appendix A: Algebra Elements 
 
We note that ( )
0
64
2
4
4
α
α
α
=
=
, so if we go on, we shall see that the values 
found above repeat. It results, according to property 1, that 
2
8
  
and
  
,
α
α
α
 must 
also be roots of  ( )
6
5
4
3
X
X
X
X
1
X
p
+
+
+
+
=
. 
Similarly, the same polynomial has the roots 
5
α as well, because indeed  
( )
0
α
α
1
α
α
1
1
α
α
1
1
α
α
α
α
1
α
=
+
+
+
+
+
=
+
+
+
+
=
+
+
+
+
=
2
2
10
5
30
25
20
15
5
p
 
So, this one and its conjugates ( )
( )
5
20
2
5
10
2
5
2
  ,
α
α
α
α
α
=
=
=
 are roots of  
( )
6
5
4
3
X
X
X
X
1
X
p
+
+
+
+
=
. 
In this way we have obtained all the 6 roots of p(X): 
10
5
2
8
4
  ,
  ,
  ,
,
 ,
α
α
α
α
α
α
. 
If β is an non zero element in the field GF
)
2
( m , we have 
0
1
β
1
β
=
+
=
−
−
1
2
1
2
m
m
  
e
  therefor
,
, so β is a root of  
1
X
1
2m
+
−
. It follows that 
any non zero element from GF
)
2
( m  is root of  
1
X
1
2m
+
−
. As its degree is 
1
2m −, it results that the 
1
2m − non zero elements of GF
)
2
( m  are all roots of 
1
X
1
2m
+
−
. 
The results lead to: 
 
Property 2: The 
1
2m − non zero elements of GF
)
2
( m  are all roots of 
1
X
1
2m
+
−
, or, all the elements of GF
)
2
( m  are all roots of the polynomial   
X
X
m
2
+
. 
Since any β element in GF
)
2
( m  is root of the polynomial 
X
X
m
2
+
, β can be 
root of a polynomial on GF(2), the degree of which should be smaller that 
m
2
. 
Be 
( )
X
Φ
 the smallest degree polynomial on GF(2), so that 
( )
0
β =
Φ
. This poly-
nomial is called minimal polynomial of β . 
 
Example 
The minimal polynomial of  the zero element in GF
)
2
( m  is X, and that of the unit 
element 1, is 
1
+
Χ
. 
Further on, we will demonstrate a number of properties of the minimal  
polynomials. 
 
• The minimal polynomial Φ(X) of an element of the field is irreducible. 
We 
suppose 
that 
( )
X
Φ
 
is 
not 
irreducible 
and 
that 
( )
( )
( )
Χ
Φ
⋅
Χ
Φ
=
Χ
Φ
2
1
( )
( )
( )
β
β
β
2
1
Φ
Φ
Φ
⋅
=
⇒
, 
so 
either 
( )
0
β =
1
Φ
, 
or 

A6   Basic Properties of Galois Fields, GF(2m) 
405
 
( )
0
ȕ
Φ2
=
, which contradicts the hypothesis that 
( )
Χ
Φ
 is the polynomial with the 
smallest degree for which 
( )
0
ȕ
Φ
=
. It results that the minimal polynomial 
( )
Χ
Φ
 
is irreducible. 
• Be ( )
Χ
p
 a polynomial on GF(2), and 
( )
Χ
Φ
 the minimal polynomial of the 
element β in the field. 
As β is a root of ( )
Χ
p
, then ( )
Χ
p
 is divisible by ( )
Χ
Φ
. 
After division, it results: ( )
( ) ( )
( )
X
r
X
Φ
X
a
X
p
+
=
, where the remainder degree 
is smaller than the degree of 
( )
Χ
Φ
. After replacing β in the above relation, we 
will obtain: 
( )
( )
( )
( )
β
β
β
β
r
Φ
a
p
+
⋅
=
,  ( )
0
β =
p
, 
( )
0
β =
Φ
 
( )
0
β =
⇒r
 
The remainder being zero, it results that ( )
Χ
Φ
  divides p(X). 
• The minimal polynomial Φ(X) of an element β in GF(2m) divides 
X
X
m
2
+
 
meaning all the roots of ( )
Χ
Φ
 are in GF
)
(2m . 
Property 3: Let β an element in GF
)
(2m , and e the smallest non zero integer so 
that  
β
β
=
e
2
. Then: 
( )
∏
⎟⎠
⎞
⎜⎝
⎛
+
=
−
=
1
e
0
i
2i
x
x
p
β
                                       (A.31) 
is an irreducible polynomial on GF(2). 
In order to demonstrate this property, we consider: 
( )
[
]
⎟⎠
⎞
⎜⎝
⎛
+
⎥⎦
⎤
⎢⎣
⎡∏
⎟⎠
⎞
⎜⎝
⎛
+
=
∏
∏
∏
⎟⎠
⎞
⎜⎝
⎛
+
=
⎟⎠
⎞
⎜⎝
⎛
+
=
⎟⎠
⎞
⎜⎝
⎛
+
=
⎥⎦
⎤
⎢⎣
⎡∏
⎟⎠
⎞
⎜⎝
⎛
+
=
−
=
−
=
−
=
=
−
=
+
e
i
i
1
i
i
i
2
2
1
e
1
i
2
2
1
e
0
i
1
e
0
i
e
1
i
2
2
2
2
2
2
2
1
e
0
i
2
2
x
x
=
          
x
x
x
x
x
p
β
β
β
β
β
β
 
As 
β
β
=
e
2
, we have: 
( )
[
]
( )
∑
=
=
∏
⎟⎠
⎞
⎜⎝
⎛
+
=
=
−
=
e
0
i
2i
i
2
1
e
0
i
2
2
2
x
p
x
p
x
x
p
i
β
                         (A.32) 
Let the polynomial ( )
1
p
,
X
p
...
X
p
p
X
p
e
e
e
1
0
=
+
+
+
=
. Taking A.32 into ac-
count, we have: 
( )
[
]
[
] =
+
+
+
=
2
e
e
1
0
2
X
p
...
X
p
p
X
p
 ∑
=
e
0
i
2i
2
i X
p
                     (A.33) 
 

406 
Appendix A: Algebra Elements 
 
From the relations (A.32) and (A.33), we obtain: 
∑
∑
=
=
=
e
0
i
e
0
i
2i
2
i
2i
i
X
p
X
p
                                    (A.34) 
In order to have the equality, we must have: 
2
i
i
p
p =
, so 
1
or  
  
0
pi =
. 
So it is compulsory that p(X) has the coefficients in GF(2). 
We suppose that p(X) would not be irreducible on GF(2) and that 
( )
( )
( )
X
b
X
a
X
p
⋅
=
. If 
( )
0
β =
p
, then automatically 
( )
0
β =
a
 or 
( )
0
β =
b
. 
If ( )
0
β =
a
, a(X) has the roots
1
2
2
e
 , . . .,
 ,
−
β
β
β
 so its degree is e and ( )
( )
X
a
X
p
=
. 
Similarly for b(X), so p(X) is irreducible. A direct consequence of the last two 
properties is the following property: 
 
Property 4: Let 
( )
Χ
Φ
the minimal polynomial of the element β in GF
)
(2m . Let 
e be the smallest integer so that 
β
β
=
e
2
 . Then we have: 
( )
∏
⎟⎠
⎞
⎜⎝
⎛
+
=
−
=
1
e
0
i
2i
X
X
Φ
β
                                        (A.35) 
Examples 
 
1. Let be the Galois field GF
)
(24 , given in Appendix A10. 
Let 
3
α
β =
 . The conjugates of β are: 
9
24
2
12
2
6
2
3
2
,
,
α
α
β
α
β
α
β
=
=
=
=
 
The minimal polynomial of 
3
α
β =
  is: 
( ) (
)(
)(
)(
) (
)(
)
1
α
α
α
α
α
α
α
α
+
+
+
+
=
+
+
+
+
=
+
+
+
+
=
X
X
X
X
        
=
X
X
X
X
X
X
X
X
X
Φ
2
3
4
6
8
2
9
2
2
9
12
6
3
 
There is also another possibility of obtaining the minimal polynomial of an 
element in the field, as we shall see further on: 
2. We want to find the minimal polynomial, 
( )
X
Φ
, of the element 
7
α
β =
 in 
GF
)
(24  from A10. 
The distinct conjugates are: 
11
56
2
13
28
2
14
2
3
2
,
,
α
α
β
α
α
β
α
β
=
=
=
=
=
 
 

A6   Basic Properties of Galois Fields, GF(2m) 
407
 
As 
( )
X
Φ
 must be of 4th degree, it must have the form: 
( )
4
3
3
2
2
1
0
Χ
Χ
a
Χ
a
Χ
a
a
Χ
Φ
+
+
+
+
=
 
Replacing X by β we obtain: 
( )
0
β
β
β
β
β
=
+
+
+
+
=
4
3
3
2
2
1
0
a
a
a
a
Φ
 
Using the polynomial representations for 
4
3
2
  
and
  
,
 ,
β
β
β
β
we obtain: 
0
α
α
1
α
α
α
1
α
α
1
=
+
+
+
+
+
+
+
+
+
+
)
(
)
(
a
)
(
a
)
(
a
a
3
3
2
3
3
2
3
1
0
 
0
α
1
α
1
α
1
=
+
+
+
+
+
+
+
+
+
+
3
3
2
1
2
3
1
2
1
0
)
a
a
(a
)
(a
a
)
a
a
(a
 
All coefficients must be zero: 
⎪⎩
⎪⎨
⎧
=
=
=
=
⇒
⎪
⎪
⎩
⎪⎪
⎨
⎧
=
+
+
+
=
+
=
+
+
+
1
a
0
a
a
1
a
0
1
a
a
a
0
1
a
    
          
0
=
  
          
a
       
0
1
a
a
a
3
2
1
0
3
2
1
3
1
2
1
0
 
So for 
7
α
β =
 the minimal polynomial is: 
( )
4
3
Χ
Χ
1
Χ
Φ
+
+
=
 
In what follows we shall present tables of minimal polynomials in GF
)
(2m  for 
3
m =
, 
4
m =
and 
5
m =
. 
Table A.1 Minimal polynomials for GF(23) and generating polynomial 1 + X+ X3 
Conjugated Roots 
Minimal Polynomials 
0
1
Į, Į 2, Į 4 
Į 3, Į 6, Į 12, Į 5 
0
X
X3 + X + 1 
X3 + X2 +1
 
Table A.2 Minimal polynomials for GF(24) and generating polynomial 1 + X+ X4 
Conjugated Roots 
Minimal Polynomials 
0
1
Į, Į 2, Į 4, Į 8
Į 3, Į 6, Į 9, Į 12
Į 5, Į 10Ң
Į 7, Į 11, Į 13, Į 14
0
X
X4 + X + 1 
X4 + X3 + X2 + 1 
X2 + X + 1 
X4 + X3 +1
 

408 
Appendix A: Algebra Elements 
 
Table A.3 Minimal polynomials for GF(25) and generating polynomial 1 + X+ X5 
Conjugated Roots 
Minimal Polynomials 
0 
0 
1
X
Į, Į 2, Į 4, Į 8 , Į 16 
X5 +X2 +1
Į 3, Į 6, Į 12, Į 24 , Į 48 = Į 17 
X5 +X4 +X3 +X2 +1 
Į 5, Į 10, Į 20, Į 40 = Į 9, Į 18 
X5 +X4 +X2 +X+1 
Į 7, Į 14, Į 28, Į 56 = Į 25, Į 50 = Į 21 
X5 +X3 +X2 +X+1 
Į 15, Į 30, Į 60 = Į 29, Į 58 = Į 28, Į 54 = Į 23
X5 +X3 +1
Į 11, Į 22, Į 44 = Į 13, Į 26, Į 52 = Į 21 
X5 +X4 +X3 +X+1 
 
 
Some explanations for Table A.2: α  being root of the polynomial 
( )
1
Χ
Χ
X
p
4
+
+
=
, it has the conjugates 
8
4
2
 ,
 ,
α
α
α
 which are also roots, so for all 
of them, the minimal polynomial is : 
1
Χ
Χ4
+
+
.  
Among the exponents of α , given above, the smallest one that did not appear is 
α3, which has the conjugates 
9
24
12
6
 ,
 ,
α
α
α
α
=
and for all of them, the minimal 
polynomial is 
1
Χ
Χ
Χ
Χ
2
3
4
+
+
+
+
. For 
5
α  we have only the conjugate 
20
10
  
as
 
α
α
. The corresponding minimal polynomial is 
1
Χ
Χ2
+
+
. Finally, 
for
7
α we have the conjugates 
11
α
, 
13
α
, 
14
α
  to which corresponds the minimal 
polynomial 
1
Χ
Χ
3
4
+
+
. 
In order to find the minimal polynomial for root α , we have assumed that the 
other roots are exponents of α . The justification consists in that the primitive 
polynomial divides 
1
Χ
m
2
+ . 
But if m is not prime, each roots of the binomial equation (except 1), raised to 
1
m−powers repeat all the others. When it is not prime (
4
m =
), some of the 
minimal polynomials can have smaller degrees than the primitive polynomial. As 
the tables for 
3
m =
 and 
5
m =
 (prime numbers) show, the minimal polynomials 
are the primitive polynomials of degree 3 and 5. 
Since the two polynomials in GF
)
2
( m  cannot have a common root (because 
they would coincide), the minimal polynomials must be prime pairs. It results that 
the 
1
2m − roots must be distributed among m degree polynomials or even smaller 
degree ones. Thus, if 
4
m =
 
15
1
24
=
−
, it results that there will be three 4th de-
gree polynomials, one 2nd degree polynomial and one first degree polynomial. 
For 
3
m =
 
7
1
23
=
−
, we will have two third degree polynomials and one first 
degree polynomial. For  
5
m =
 
31
1
25
=
−
, there will be six fifth degree polyno-
mials and one first degree polynomial. 

A6   Basic Properties of Galois Fields, GF(2m) 
409
 
Property 5: It is a direct consequence of the previous one and stipulates that if 
( )
X
Φ
 is the minimal polynomial of an element β in GF
)
(2m  and if e is the degree 
of 
( )
X
Φ
, then e is the smallest integer such that 
β
β
=
e
2
 Obviously, 
m
e ≤
. 
In particular, it can be shown that the minimal polynomial degree of each ele-
ment in GF
)
(2m  divides m. The tables prove this affirmation. 
When constructing the Galois field GF
)
(2m , we use a minimal polynomial 
p(X) of m degree and we have the element α which is p(X) root. As the exponents 
of α generate all the non zero elements of GF
)
(2m , α is a primitive element. 
All the conjugates of α are primitive elements of GF
)
(2m . In order to see this, 
let n be the order of 
0
1
  ,
1
2
>
α
, and we have: 
1
α
α
=
=
⎟⎠
⎞
⎜⎝
⎛
⋅1
1
2
n
n
2
                                        (A.36) 
As α is a primitive element of GF
)
(2m , its order is 
1
2m −. From the above re-
lation, it results that 
1
2 must be divisible to 
1
2m −, 
(
)1
2
q
n
m −
=
. But 
(
)
1
2
n
:
1
2
k
n
m
m
−
=
−
=
, so α2 is a primitive element of  GF
)
(2m . 
Generally, if β is a primitive element of GF
)
(2m , all its conjugates 
...,
,
e
2
2 β
β
are also primitive elements of GF
)
(2m . 
Using the tables for GF
)
(2m , linear equation systems can be solved. Be the 
following system: 
 
⎪⎩
⎪⎨
⎧
=
+
=
+
)
 
of
 
inverse
 
 the
is
(which 
  
Y
X
Y
X
12
3
4
8
12
2
7
α
α
α
α
α
α
α
 
⎪⎩
⎪⎨
⎧
=
+
=
+
7
11
2
7
Y
X
Y
X
α
α
α
α
 
 
By addition and expressing 
7
11   , α
α
and reducing the terms, we obtain:  
3
2
2 Υ
)
(1
α
α
α
1
α
+
+
+
=
+
, but 
2
8
α
1
α
+
=
 and the inverse of  
7
8
  
is
 
α
α
 
So, multiplying this equation by 
7
α , we obtain: 
9
7
3
8
7
Υ
α
α
α
α
+
+
+
=
+
=
4
α
α
1
=
+
 
It follows that 
4
Υ
α
=
. 
Similarly for 
9
α
=
Χ
. 

410 
Appendix A: Algebra Elements 
 
If we want to solve the equation ( )
0
α
α
=
+
+
=
X
X
X
f
7
2
, we cannot do this 
by the regular formula, as we are working in modulo 2. Then, if ( )
0
X
f
=
, it has a 
solution in GF
)
2
( 4 ; the solution is obtained by replacing X by all the elements 
from A10. We find ( )
0
f
6 =
α
 and (
)
0
f
10 =
α
, so 
6
α  and 
10
α
 are roots. 
A7   Matrices and Linear Equation Systems 
A matrix is a table of elements for which two operations were defined: addition 
and multiplication. If we have m lines and n columns, the matrix is called a 
n
m×
matrix. 
Two matrices are equal when all the corresponding elements are equal. Matrix 
addition is done only for matrixes having the same dimensions, and the result is 
obtained by adding the corresponding elements. The null matrix is the matrix with 
all elements zero. 
The set of all matrices with identical dimensions forms a commutative group 
related to addition. 
The null element is the null matrix. 
Particular cases of matrices are the linear matrix [
] . . .  
b
  
a
 and column  
matrix
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
#
b
a
. 
Multiplication of two matrices cannot be done unless the columns number  
of the first matrix equals the lines number of the second one. Multiplication is  
not commutative. For square matrixes, mxm of degree m, we have the unit matrix 
that has all elements null, except those ones on the main diagonal which equal 1. 
Its determinant can be defined, according to the known rule, only for these  
ones. For such a matrix, we have the inverse matrix only when its determinant is 
not null. 
For any matrix the notion of rank is defined as follows: by suppressing lines 
and columns in the given matrix and keeping the order of the elements left, we ob-
tain determinants of orders from min (n, m) to 1. From these ones, the maximum 
order of non zero determinants is the matrix rank. 
There are three operations that change the matrix elements, but maintain its 
rank, meaning: switching lines with columns, multiplying a line (column) by an 
non zero number, adding the elements of a line (column) to another line (column). 
These operations allow us to get a matrix for which, on each line and column 
we have one non zero element the most. The number of these elements gives us 
the matrix rank. 
 
 
 
 

A7   Matrices and Linear Equation Systems 
411
 
Example: Calculate the rank of the following matrix: 
 
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
−
−
−
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
−
−
−
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
0
0
0
0
0
0
1
0
0
0
0
1
~
0
0
0
0
0
0
1
0
0
0
0
1
~
2
0
1
0
2
0
1
0
0
0
0
1
~
~
10
0
5
0
6
0
3
0
0
0
0
1
~
10
0
5
0
6
0
3
0
4
2
2
1
~
2
6
1
3
10
8
5
4
4
2
2
1
 
 
We have two non zero elements, so the rank of the matrix is 2. 
 
Linear equations systems 
 
Be the system: 
⎪
⎪
⎩
⎪⎪
⎨
⎧
=
+
+
+
=
+
+
+
=
+
+
+
2
n
mn
2
m2
1
m1
2
n
2n
2
22
1
21
1
n
1n
2
12
1
11
b
x
a
x
a
x
a
b
x
a
x
a
x
a
b
x
a
x
a
x
a
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
                            (A.37) 
For such a system, the rank r of the matrix 
n
m×
corresponding to the coeffi-
cients of the unknowns 
n
1
 x
, . . . ,
x
. At the same time, we choose a non zero de-
terminant of order r, which is called principal determinant. The equations and the 
unknowns that give the elements in this determinant are called principal equa-
tions, and the others, secondary. For each secondary equation, a characteristic de-
terminant is set up, by bounding the principal determinant by a line and a column. 
The line contains the coefficients of the main unknowns, and the column, the free 
terms of the principal and secondary equations. 
We have Rouche theorem, which says that the system is compatible (it has solu-
tions) if and only if all the characteristic determinants are null, the solutions being 
obtained by Cramer rule. Secondary unknowns are considered parameters, case in 
which we have an infinite number of solutions.  
If the rank r equals the number n of the unknowns we have a unique solution; 
on the contrary, we have secondary unknowns, so an infinite number of solutions. 
If n=m and the rank is n, the system is called Cramer system and there is a rule 
for expressing the solution using determinants. But, as calculating the determi-
nants involve a high volume of operations, in application we use Gauss algorithm. 
It starts from the extended matrix of the equation system (the coefficients matrix 
of the unknowns to which the column of free terms is added), on which the de-
scribed operations for determining the rank of a matrix are clearly made, working  
 

412 
Appendix A: Algebra Elements 
 
only on lines. Thus the operations number to be solved is much smaller than the 
one required for calculating the determinants. A simple example will illustrate this 
method which can be applied to any binary field. 
Be the system: 
⎪⎩
⎪⎨
⎧
−
=
+
+
−
−
=
+
−
−
=
+
+
4
2z
y
x
1
z
y
2x
3
3z
2y
x
 
We have: 
( )
( )
( )
( )
( )
( )
( )
( )
( )
( )
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
−
−
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
−
−
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
−
−
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
−
−
−
−
⎟
⎟
⎟
⎠
⎞
⎜
⎜
⎜
⎝
⎛
−
−
−
−
−
2
1
0
0
1
0
1
0
1
0
0
1
~
2
1
0
0
1
1
1
0
1
1
0
1
~
4
2
0
0
1
1
1
0
1
1
0
1
~
~
7
5
3
0
1
1
1
0
3
3
2
1
~
7
5
3
0
5
5
5
0
3
3
2
1
~
4
2
1
1
1
1
1
2
3
3
2
1
 
The solution is  x = 1, y = 1, z = 2. 
A8   Vector Spaces 
A8.1   Defining Vector Space 
Let (V,+) be an Abelian group, with elements called vectors and denoted 
…
 ,x ,v
. 
Let (F,+,⋅) be a field, with elements called scalars, having as neutral elements 0 
and 1. 
We call vector space over field F, the Abelian group (V,+), on which an  
outer composition law with operators in F is given, called the multiplication of 
vectors by scalars: 
V
u
  
and
  
V
a 
 
V,
V
F
∈
∈
∀
→
×
 and which satisfies the  
axioms: 
(
)
(
)
(
)
0
v
0
v
v
1
)
u
a(b
u
b
a
v
a 
 u
a
v
u
a
u
b 
 u
a
u
b
a
=
=
=
+
=
+
+
=
+
                                       (A.38) 
 
 
 

A8   Vector Spaces 
413
 
Example 
The space vectors and the real numbers form a vector space. The set of vectors in 
the three dimension space is determined by three coordinates, which can be as-
sembled in a linear matrix: 
(
)
z 
y,
 
x,
v =
. 
The set of linear matrices, with a dimension of 
3
1×
, forms a vector space. 
More general, the set of matrices with a dimension of 
n
1×
 forms a vector space. 
A8.2   Linear Dependency and Independency 
Be the vectors 
n
2
1
v , . . . ,
v ,
v
 in the vector space 
V
F×
 and the scalars 
n
2
1
Ȝ , . . .,
Ȝ
,
Ȝ
. Any expression having the form  
n
n
2
2
1
1
v
Ȝ , . . .,
v
Ȝ
,
v
Ȝ
 is called 
linear combination of the respective vectors. If this combination is zero, not all of 
the scalars being null, then: 
0
v
Ȝ 
 . . .,
v
Ȝ
v
Ȝ
n
n
2
2
1
1
=
+
+
                                   (A.39) 
we say that the vectors: 
n
2
1
v , . . . ,
v ,
v
 are linear independent. In the opposite 
situation, they are called linear dependent. 
Thus, two vectors in the plane can be linear independent, but three cannot. The 
maximum number of linear independent vectors in a vector space is called space 
dimension, and any system of linear independent vectors forms a space base; all 
the other space vectors can be expressed with the base ones. So, if the base vectors 
are noted by 
n
2
1
e , . . . ,
e ,
e
, any vector from that space can be written as: 
n
n
2
2
1
1
e
α
 . . . 
e
α 
e
α
V
+
+
+
=
                                    (A.40) 
in which the numbers 
n
2
1
α
, . . . ,
α ,
α
are called vector coordinates. 
If the space dimension is n, then the component matrix of the n vectors that 
form a base has the rank n. 
In an n-dimension space we can always have a number 
n
m <
of vectors linear 
independent. Their linear combinations generate only some of the space vectors, 
so they form a subspace of the given space, the dimension of which equals the ma-
trix rank of their components. 
Coming back to the Galois field, the vector space on GF(2) plays a central part 
in the coding theory. We consider the string (
)
n
2
1
a
, . . . ,
a ,
a
, in which each com-
ponent ai is an element of the binary field GF(2). This string is called n-tuple on 
GF(2). As each element can be 0 or 1, it results that we can set up distinct n-
tuples, which form the set Vn. We define addition "+" on Vn , for     
 
 
 

414 
Appendix A: Algebra Elements 
 
(
)
(
)
n
1-
n
1
0
1-
n
1
0
V
 
 v ,u
,
 v
, . . . ,
v
,
v
v ,
u , . . . ,
u
,
u
u
∈
=
=
  by the relation: 
(
)
1-
n
1-
n
1
1
0
0
v
u , . . . ,
v
u
,
v
u
v
u
+
+
+
=
+
                        (A.41) 
the additions being made modulo two. 
It is obvious that 
v
u +
 is an n-tuple on GF(2) as well. As Vn is closed to addi-
tion, we will easily check that Vn is a commutative group to the above defined  
addition. 
Taking into account that addition modulo two is commutative and associative, 
it results the same thing for the above defined addition. Having in view 
that
(
)
0 , . . . 
0,
,0
v =
 is the additive identity and that:  
(
)
(
)
0
0 , . . . 
0,
 
0,
v
 v
, . . . ,
v
v
,
v
v
v
v
1-
n
1-
n
1
1
0
0
=
=
+
+
+
=
+
, 
the inverse of each elements being itself, Vn is a commutative group to the above 
defined addition. 
We define the scalar multiplication of an n-tuple, v  in Vn with an element a in 
GF(2), by: (
)
(
)
1
-
n
1
0
1-
n
1
0
 v
a , . . . ,
 v
a,
 v
a
 v
, . . . ,
v
,
v
a
=
  modulo two multiplication. 
If 
1
a =  we have: 
v
v
a
=
. It is easy to see that the two identical operations  
defined above satisfy the distributivity and associativity laws. And the set Vn of  
n-tuples on GF(2) forms a vector space over GF(2). 
A8.3   Vector Space 
V being a vector space on field F, it may happen that a subset S of V be vector 
space on F as well. This one will be called a subspace of V. 
So, if S is an non zero subset of space V in the field F, S is subspace of V if: 
 
1. For any two vectors 
v
  
and
  
u
 in S, 
v
u +
 must be in S. 
2. For any element a ∈ F and any vector in S, a u  must be in S. 
Or, if we have the vector set 
k
2
1
v , . . . ,
v ,
v
 in the space V, then the set of all 
linear combinations of these vectors form a subspace of V. 
We consider the vector space Vn of all n-tuples on GF(2). We form the follow-
ing n-tuples: 
(
)
(
)
(
)1 , . . . 
0,
 
0,
e
...
0 , . . . 
1,
 
0,
e
0 , . . . 
0,
 
1,
e
1-
n
1
0
=
=
=
                                        (A.42) 
Then any n-tuples (
)
1-
n
1
0
a
, . . . ,
a ,
a
in Vn can be expressed with these ones; it 
follows that vectors (A.42) generate the entire space Vn of n-tuples over GF(2). 

A8   Vector Spaces 
415
 
They are linear independent and thus form a base of Vn and its dimension is n. If 
n
k <
, the linear independent vectors 
k
2
1
v , . . . ,
v ,
v
 generate a subspace of  Vn. 
Be
(
)
(
)
n
1-
n
1
0
1-
n
1
0
V
 
 v ,u
,
 v
, . . . ,
v
,
v
v ,
u , . . . ,
u
,
u
u
∈
=
=
. By inner product / dot 
product of u  and v  we understand the scalar:  
1-
n
1-
n
1
1
0
0
v
u 
 . . . 
v
u
v
u
v
u
+
+
+
=
                             (A.42) 
all calculated in modulo 2. If 
0
v
u
=
, u   and v  are orthogonal. 
The inner product / dot product has the properties: 
u
v
v
u
=
                                                 (A.44) 
w
u
v
u
)
w
v
(
u
+
=
+
                                        (A.45) 
( )
( )
v
u
a
v
u
a
=
                                           (A.46) 
Let be S a subspace of dimension k over Vn and Sd the vector set in Vn, such 
that for 
d
S
v
  
and
  
S
u
∈
∈
∀
 we have 
 
0
v
u
=
                                                  (A.47) 
For any element a in GF(2) and any 
d
S
v∈
 we have: 
⎪⎩
⎪⎨
⎧
=
=
=
1
a
  
if
  
v
0
a
  
if
  
0
v
a
                                          (A.48) 
It follows that v
a
 is also in Sd. 
Let v  and w  two vectors in Sd. For any vector 
S
u ∈
we have: 
0
0
0
w
u
v
u
)
w
v
(
u
=
+
=
+
=
+
                                (A.49) 
This means that if v  and w  are orthogonal with u , the sum vector  
w
u +
 is 
also orthogonal with u , so 
w
v +
is a vector in Sd. So Sd is also a vector space, be-
sides it is a subspace of Vn. This subspace, Sd, is called dual space (or null) of S. 
Its dimension is 
k
-
n
, where n is the dimension of the space Vn, and k the dimen-
sion of the space S: 
n
)
dim(S
dim(S)
d =
+
                                     (A.50) 
In order to determine the space, and the dual subspace, we look for the base of 
orthogonal vectors. Only those vectors which are orthogonal with all the subspace 
vectors from which we have started, are selected as base of the dual space. 
 
 
 

416 
Appendix A: Algebra Elements 
 
Example 
From the set of 32 elements of 5-tuples (a1,….,a5), we consider 7 of them and we 
look for the space dimension that they form. We have: 
 
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
~
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
1
0
1
0
1
1
1
0
0
~
~
1
1
1
0
0
0
0
0
0
0
1
1
1
0
0
1
1
1
0
0
1
0
0
0
1
0
1
0
1
0
1
1
1
0
0
~
1
1
1
0
0
0
1
0
1
0
1
0
1
1
0
1
1
1
0
0
1
0
0
0
1
0
1
0
1
0
0
0
1
1
0
~
1
1
1
0
0
1
1
0
1
1
1
0
1
1
0
0
1
1
0
1
1
0
0
0
1
0
1
0
1
0
0
0
1
1
1
 
 
It follows that the rank is 3 and a base is formed by the vectors that correspond 
to the lines that contain 1: (11100), (01010), (10001). 
Now we look for the orthogonal vectors in the subspace considered and we use 
them to form the subspace. 
 
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎟⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎞
⎜⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0
0
~
0
0
0
0
0
0
0
1
0
0
0
1
0
0
1
1
1
0
0
0
0
1
0
1
0
1
1
0
0
0
0
1
0
0
0
~
~
0
0
1
0
0
0
0
1
0
0
0
1
0
0
1
1
1
1
0
0
0
1
1
1
0
1
1
1
0
0
0
1
0
0
0
~
0
1
0
1
0
0
0
1
0
0
0
1
0
0
1
1
0
0
1
0
0
1
1
1
0
1
1
1
0
0
0
1
0
0
0
~
1
1
1
0
0
0
0
1
0
0
0
1
0
0
1
1
0
0
1
0
0
1
1
1
0
1
0
1
0
1
1
1
0
0
0
 

A9   Table for Primitive Polynomials of Degree k (k max = 100) 
417
 
We get that the space dimension is 4, and one of its bases is: (10101), (01110), 
(10010), (00100) and (00000). 
A9   Table for Primitive Polynomials of Degree k (k max = 100) 
Remark 
The table lists only one primitive polynomial for each degree k ≤ 100. In this table 
only the degrees of the terms in the polynomial are given; thus 7 1 0 stands for  
x7 + x + 1. 
 
1 
0 
 
 
 
 
 
 
51 
6 
3 
1 
0 
 
 
2 
1 
0 
 
 
 
 
 
52 
3 
0 
 
 
 
 
3 
1 
0 
 
 
 
 
 
53 
6 
2 
1 
0 
 
 
4 
1 
0 
 
 
 
 
 
54 
6 
5 
4 
3 
2 
0 
5 
2 
0 
 
 
 
 
 
55 
6 
2 
1 
0 
 
 
6 
1 
0 
 
 
 
 
 
56 
7 
4 
2 
0 
 
 
7 
1 
0 
 
 
 
 
 
57 
5 
3 
2 
0 
 
 
8 
4 
3 
2 
0 
 
 
 
58 
6 
5 
1 
0 
 
 
9 
4 
0 
 
 
 
 
 
59 
6 
5 
4 
3 
1 
0 
10 
3 
0 
 
 
 
 
 
60 
1 
0 
 
 
 
 
11 
2 
0 
 
 
 
 
 
61 
5 
2 
1 
0 
 
 
12 
6 
4 
1 
0 
 
 
 
62 
5 
3 
0 
 
 
 
13 
4 
3 
1 
0 
 
 
 
63 
1 
0 
 
 
 
 
14 
5 
3 
1 
0 
 
 
 
64 
4 
3 
1 
0 
 
 
15 
1 
0 
 
 
 
 
 
65 
4 
3 
1 
0 
 
 
16 
5 
3 
2 
0 
 
 
 
66 
8 
6 
5 
3 
2 
0 
17 
3 
0 
 
 
 
 
 
67 
5 
2 
1 
0 
 
 
18 
5 
2 
1 
0 
 
 
 
68 
7 
5 
1 
0 
 
 
19 
5 
2 
1 
0 
 
 
 
69 
6 
5 
2 
0 
 
 
20 
3 
0 
 
 
 
 
 
70 
5 
3 
1 
0 
 
 
21 
2 
0 
 
 
 
 
 
71 
5 
3 
1 
0 
 
 
22 
1 
0 
 
 
 
 
 
72 
6 
4 
3 
2 
1 
0 
23 
5 
0 
 
 
 
 
 
73 
4 
3 
2 
0 
 
 
24 
4 
3 
1 
0 
 
 
 
74 
7 
4 
3 
0 
 
 
25 
3 
0 
 
 
 
 
 
75 
6 
3 
1 
0 
 
 
26 
6 
2 
1 
0 
 
 
 
76 
5 
4 
2 
0 
 
 
27 
5 
2 
1 
0 
 
 
 
77 
6 
5 
2 
0 
 
 
28 
3 
0 
 
 
 
 
 
78 
7 
2 
1 
0 
 
 
29 
2 
0 
 
 
 
 
 
79 
4 
3 
2 
0 
 
 
30 
6 
4 
1 
0 
 
 
 
80 
7 
5 
3 
2 
1 
0 
31 
3 
0 
 
 
 
 
 
81 
4 
0 
 
 
 
 

418 
Appendix A: Algebra Elements 
 
32 
7 
5 
3 
2 
1 
0 
 
82 
8 
7 
6 
4 
1 
0 
33 
6 
4 
1 
0 
 
 
 
83 
7 
4 
2 
0 
 
 
34 
7 
6 
5 
2 
1 
0 
 
84 
8 
7 
5 
3 
1 
0 
35 
2 
0 
 
 
 
 
 
85 
8 
2 
1 
0 
 
 
36 
6 
5 
4 
2 
1 
0 
 
86 
6 
5 
2 
0 
 
 
37 
5 
4 
3 
2 
1 
0 
 
87 
7 
5 
1 
0 
 
 
38 
6 
5 
1 
0 
 
 
 
88 
8 
5 
4 
3 
1 
0 
39 
4 
0 
 
 
 
 
 
89 
6 
5 
3 
0 
 
 
40 
5 
4 
3 
0 
 
 
 
90 
5 
3 
2 
0 
 
 
41 
3 
0 
 
 
 
 
 
91 
7 
6 
5 
3 
2 
0 
42 
5 
4 
3 
2 
1 
0 
 
92 
6 
5 
2 
0 
 
 
43 
6 
4 
3 
0 
 
 
 
93 
2 
0 
 
 
 
 
44 
6 
5 
2 
0 
 
 
 
94 
6 
5 
1 
0 
 
 
45 
4 
3 
1 
0 
 
 
 
95 
6 
5 
4 
2 
1 
0 
46 
8 
5 
3 
2 
1 
0 
 
96 
7 
6 
4 
3 
2 
0 
47 
5 
0 
 
 
 
 
 
97 
6 
0 
 
 
 
 
48 
7 
5 
4 
2 
1 
0 
 
98 
7 
4 
3 
2 
1 
0 
49 
6 
5 
4 
0 
 
 
 
99 
7 
5 
4 
0 
 
 
50 
4 
3 
2 
0 
 
 
 
100 
8 
7 
2 
0 
 
 
A10   Representative Tables for Galois Fields GF(2k) 
Remark 
The tables list the powers of α and the matrix representation for Galois Fields 
GF(2k), k ≤ 6. For example, in the first table 3   1 1 0  stands for 
α
1
α3
+
=
. 
 
1. GF(23) generated by p(x) = 1+x+x3 
 - 
000 
 0 
100 
 1 
010 
 2 
001 
 3 
110 
 4 
011 
 5 
111 
 6 
101 
 
2. GF(24) generated by p(x) = 1+x+x4 
 - 
0000 
 
8 
1010 
 0 
1000 
 
9 
0101 
 1 
0100 
 
10 
1110 
 2 
0010 
 
11 
0111 
 3 
0001 
 
12 
1111 

A10   Representative Tables for Galois Fields GF(2k) 
419
 
 4 
1100 
 
13 
1011 
 5 
0110 
 
14 
1001 
 6 
0011 
 7 
1101 
 
3.  GF(25) generated by p(x) = 1+x2+x5 
- 
00000 
10 
10001 
21 
00011 
0 
10000 
11 
11100 
22 
10101 
1 
01000 
12 
01110 
23 
11110 
2 
00100 
13 
00111 
24 
01111 
3 
00010 
14 
10111 
25 
10011 
4 
00001 
15 
11111 
26 
11101 
5 
10100 
16 
11011 
27 
11010 
6 
01010 
17 
11001 
28 
01101 
7 
00101 
18 
11000 
29 
10010 
8 
10110 
19 
01100 
30 
01001 
9 
01011 
20 
00110 
 
 
 
4. GF(26) generated by p(x) = 1+x+x6 
 
- 
000000 
21 
101011 
43 
111011 
0 
100000 
22 
100101 
44 
101101 
1 
010000 
23 
100101 
45 
100110 
2 
001000 
24 
100010 
46 
010011 
3 
000100 
25 
010001 
47 
111001 
4 
000010 
26 
111000 
48 
101100 
5 
000001 
27 
011100 
49 
010110 
6 
110000 
28 
001110 
50 
001011 
7 
011000 
29 
000111 
51 
110101 
8 
001100 
30 
110011 
52 
101010 
9 
000110 
31 
101001 
53 
010101 
10 
000011 
32 
100100 
54 
111010 
11 
110001 
33 
010010 
55 
011101 
12 
101000 
34 
001001 
56 
111110 
13 
010100 
35 
110100 
57 
011111 
14 
001010 
36 
011010 
58 
111111 
15 
000101 
37 
001101 
59 
101111 
16 
110010 
38 
110110 
60 
100111 
17 
011001 
39 
011011 
61 
100011 
18 
111100 
40 
111101 
62 
100001 
19 
011110 
41 
101110 
 
 
20 
001111 
42 
010111 
 
 
 
 

420 
Appendix A: Algebra Elements 
 
A11   Tables of the Generator Polynomials for BCH Codes 
Remark 
The table lists the corresponding generator polynomial coefficients for BCH codes 
of different length n ≤ 127, with different correctable errors (t). The coefficients 
are written in octal.  
For example for a BCH(15,7) code, from the table results: n = 15, m = 7, t = 2 
and g(x) =: 7 2 1
N
N
N ⇒
⇒
1
2
7
001
010
111
g(x) = x8 + x7 + x6 + x4 + 1  
 
n 
 
M 
 
t 
 
Generator polynomial coefficients 
gk’ gk’-1 … g1 g0 
7 
 
4 
 
1  
13 
15 
 
11 
 
1  
23 
15 
 
7 
 
2  
721 
15 
 
5 
 
3  
2467 
31 
 
26 
 
1  
45 
31 
 
21 
 
2  
3551 
31 
 
16 
 
3  
107657 
31 
 
11 
 
5  
5423325 
31 
 
6 
 
7  
313365047 
63 
 
57 
 
1  
103 
63 
 
51 
 
2  
12471 
63 
 
45 
 
3  
1701317 
63 
 
39 
 
4  
166623567 
63 
 
36 
 
5  
1033500423 
63 
 
30 
 
6  
157464165547 
63 
 
24 
 
7  
17323260404441 
63 
 
18 
 
10  
1363026512351725 
63 
 
16 
 
11  
6331141367235453 
63 
 
10 
 
13  
472622305527250155 
63 
 
7 
 
15  
52310455435033271737 
127 
 
120 
 
1  
211 
127 
 
113 
 
2  
41567 
127 
 
106 
 
3  
11554743 
127 
 
99 
 
4  
3447023271 
127 
 
92 
 
5  
624730022327 
127 
 
85 
 
6  
1307044763222273 

A12   Table of the Generator Polynomials for RS Codes 
421
 
n 
 
M 
 
t 
 
Generator polynomial coefficients 
gk’ gk’-1 … g1 g0 
127 
 
78 
 
7  
26230002166130115 
127 
 
71 
 
9  
6255010713253127753 
127 
 
64 
 
10  
1206534025570773100045 
127 
 
57 
 
11  
335265252505705053517721 
127 
 
50 
 
13  
54446512523314012421501421 
127 
 
43 
 
14  
17721772213651227521220574343 
127 
 
36 
 
15  
314607466522075044764574721735 
127 
 
29 
 
21  
40311446136767062366753014176155 
127 
 
22 
 
23  
123376070404722522435445626637647043 
127 
 
15 
 
27  
22057042445604554770523013762217604353 
127 
 
8 
 
31  
7047264052751030651476224271567733130217
A12   Table of the Generator Polynomials for RS Codes 
Remark 
The table lists the generator polynomial coefficients for RS codes of different 
lengths n ≤ 511 and different correctable errors (t). The coefficients are given as 
the decimals associated to GF(2k) elements and the generator polynomial has the 
expression:  
0
1
2t
1
2t
2t
1-
2t
p
1
p
p
g
...
x
g
x
g(x)
or  
  
)
α
)...(x
α
)(x
α
(x
g(x)
+
+
+
=
+
+
+
=
−
−
+
+
 
For example, the generator polynomial for RS(7, 3) code with p = 1 is:  
 
( )
( )
⇒
+
+
+
+
=
⇒
=
4
2x
x
4x
x
x
g
4
2
1
4
1
:
x
g
2
3
4
 
( )
3
2
3
3
4
α
αx
x
x
α
x
x
g
+
+
+
+
=
⇒
 
 
RS(7, 3)  
t=2 
p=0  
1    3     6     6     7  
p=1  
1    4     1     2     4   
RS(15, 11) 
t=2 
p=0  
1   13     5     1     7  
p=1  
1   14     7     4    11  
RS(15, 9) 
t=3 
p=0   
1   10    13     2     3     5     1  
p=1   
1   11    15     5     7    10     7  
RS(15, 7) 
t=4 
p=0   
1   14     1     2    14     9    15     5    14  
p=1  
 1   15     3     5     3    14     6    12     7  

422 
Appendix A: Algebra Elements 
 
RS(15, 5) 
t=5 
p=0   
1    2     2     7     3    10    12    10    14     8     1  
p=1   
1    3     4    10     7    15     3     2     7     2    11  
RS(31, 27)  
t=2 
p=0   
1   24    18    27     7  
p=1   
1   25    20    30    11  
RS(31, 25) 
t=3 
p=0   
1   10     8    22    13    20    16  
p=1   
1   11    10    25    17    25    22  
RS(31, 23) 
t=4 
p=0   
1    3    21    21    16    28     4    24    29  
p=1   
1    4    23    24    20     2    10    31     6 
RS(31, 21) 
t=5 
p=0   
1   18    30    23     7     5    16    10    26    23    15  
p=1   
1   19     1    26    11    10    22    17     3     1    25 
RS(31, 19) 
t=6 
p=0   
1    6    21    29     7    29    29     9    29    31     3    30     5  
p=1   
1    7    23     1    11     3     4    16     6     9    13    10    17 
RS(31, 17) 
t=7 
p=0   
1   27     6     2    14    20    14    18    27    15    22    23     9    12    30  
p=1   
1   28     8     5    18    25    20    25     4    24     1     3    21    25    13 
RS(31, 15) 
t=8 
p=0   
1   23    12    29     5    30    27    15    18    30    26    13     3    11     9     4 
 
28  
p=1   
1   24    14     1     9     4     2    22    26     8     5    24    15    24    23    19 
 
13 
RS(31, 13) 
t=9 
p=0 
1   15    10    23     9    24    16    23    29    25    15    26     5    30     1    1 
 
5    27    30  
p=1 
1   16    12    26    13    29    22    30     6     3    25     6    17    12    15    16 
 
21    13    17 
RS(31, 11) 
t=10 
p=0  
1   22    29     3    26     6     8     5     6    21    14     9    13    31    22     8  
 
16    12    26     7     5  
p=1  
1   23    31     6    30    11    14    12    14    30    24    20    25    13     5 
 
23     1    29    13    26    25 
RS(63, 59) 
t=2 
p=0 
1   19    40    22     7  
p=1 
1   20    42    25    11 
RS(63, 57) 
t=3 
p=0   
1   59    47    41    52     6    16  
p=1   
1   60    49    44    56    11    22  
 
 

A12   Table of the Generator Polynomials for RS Codes 
423
 
RS(63, 55) 
t=4 
p=0 
1   43    58    29     7    36     9     1    29  
p=1   
1   44    60    32    11    41    15     8    37  
RS(63, 53) 
t=5 
p=0 
1   56    27    45    50    56    59    63    54    29    46  
p=1 
1   57    29    48    54    61     2     7    62    38    56  
RS(63, 51) 
t=6 
p=0 
1   60    11    42     3    56    23     4    25    12    55    52     4 
p=1 
1   61    13    45     7    61    29    11    33    21     2    63    16 
RS(63, 49) 
t=7 
p=0   
1   47     8    43    47    50    36     1    49    13    23    32    10    62    29  
p=1   
1  48    10    46    51    55    42     8    57    22    33    43    22    12    43 
RS(63, 47) 
t=8 
p=0 
1   28    40    62    13    20    49    27    31    42    16     2    10    11     4 
 
7    58  
p=1 
1   29    42     2    17    25    55    34    39    51    26    13    22    24    18 
 
22    11 
RS(63, 45) 
t=9 
p=0 
1   22    58    61    63    57    33    15    62    23    16    49    21    62    22 
 
37    51    32    28  
p=1 
1   23    60     1     4    62    39    22     7    32    26    60    33    12    36     
 
52    4    49    46 
RS(63,43) 
t=10 
p=0 
1   54    36    33    59    34    61    30    24    52    25     8    62    24    11     
 
3    47    40    62    36     2  
p=1 
1   55    38    36    63    39     4    37    32    61    35    19    11    37    25 
 
18    63    57    17    55    22 
RS(127, 123) 
t=2 
p=0   
1   94    40    97     7  
p=1   
1   95    42   100    11 
RS(127, 121) 
t=3 
p=0   
1  111     5   124    10   121    16  
p=1   
1  112     7   127    14   126    22  
RS(127, 119) 
t=4 
p=0   
1   91    33    42     3    49    47   112    29  
p=1   
1   92    35    45     7    54    53   119    37  
RS(127, 117) 
t=5 
p=0   
1    7   117   106   115    51   124   124    17    43    46  
p=1   
1    8   119   109   119    56     3     4    25    52    56  
RS(127, 115) 
t=6 
p=0  
 1  125    98     3    53    96    90   107    75    36    15    53    67  
p=1   
1  126   100     6    57   101    96   114    83    45    25    64    79  
 
 

424 
Appendix A: Algebra Elements 
 
RS(127, 113) 
t=7 
p=0   
1  103     6    29    69    28    63    60    76    54   108    81    71    54    92  
p=1   
1  104     8    32    73    33    69    67    84    63   118    92    83    67   106  
RS(127, 111) 
t=8 
p=0 
1   85    87    88    58     8    33    73     3    88    63    53   118    36    50 
 
63   121  
p=1   
1   86    89    91    62    13    39    80    11    97    73    64     3    49    64 
 
78    10  
RS(127, 109) 
t=9 
p=0 
1   58    66    49   118    46     1    32    79    80    96    66    52   114    76 
 
24    58    67    27  
p=1 
1   59    68    52   122    51     7    39    87    89   106    77    64   127    90 
 
39    74    84    45  
RS( 127,107) 
t=10 
p=0 
1   44    89    45   120    30    84    93    70    62    68    81   108    23    33 
 
125   107    51   114    88    64  
p=1 
1   45    91    48   124    35    90   100    78    71    78    92   120    36    47 
 
13   123    68     5   107    84 
RS(255, 251) 
t=2 
p=0   
1  239    27   117     7  
p=1   
1   77   252    82    11  
RS(255, 249) 
t=3 
p=0 
1  121   175   178   166    98    16  
p=1 
1  168     3   138    10   182    22  
RS(255, 247)  
t=4 
p=0   
1  235   188   101   201    24     4   117    29  
p=1   
1  177   241   212   254   221     4   204    37  
RS(255, 245) 
t=5 
p=0   
1  188    75    73   221   171   112   159     4     2    46  
p=1   
1  253    70    50    66   124    77    72   103    42    56  
RS(255, 243) 
t=6 
p=0   
1    7   122    75    16    31   182   251   136   205    85   130    67  
p=1   
1  104    46   102   126   193   120   206   152   141    98   169    79 
RS(255, 241) 
t=7 
p=0 
1   52     2     89   156   214    69    82   111   225   158   249   177    87 
 
92  
p=1   
1   201   252   159   53   196   131   226   146   226    98   219    72    36 
 
106 
RS(255, 239) 
t=8 
p=0 
1   167   118    59    63    19    64   167   234   162   223    39     5    21 
 
186   149   121  
p=1 
1   122  107  111  114  108  168   84    12   101   202   159   182   196 
 
209   241   137  
 

A12   Table of the Generator Polynomials for RS Codes 
425
 
RS(255, 237) 
t=9 
p=0 
1  236   251   109   105    27   166   202    50   129   225    84    90    24 
 
225   195   254   204   154  
p=1   
1  217   237   162    99   190   104   126   179    89   198   164   161    11 
 
194    21   115   114   172  
RS(255,235) 
t=10 
p=0   
1   36   216   188    10   183   179   197    27   158     5    82   119    38 
 
142    49    50    46    46   157   191  
p=1   
1   19    63   171    58    67   170   34   196   212   191   233   238    97 
 
254   172   181   230   231    208   211  
RS( 511, 507) 
t=2 
p=0   
1  391    41   394     7  
p=1   
1  392    43   397    11 
RS(511, 505) 
t=3 
p=0 
1  200   448    39   453   210    16  
p=1 
1  201   450    42   457   215    22  
RS(511, 503) 
t=4 
p=0 
1  400   208   119   109   126   222   421    29  
p=1 
 1  401   210   122   113   131   228   428    37  
RS(511, 501) 
t=5 
p=0   
1  374   119   230   291   117   300   248   146   410    46  
p=1   
1  375   121   233   295   122   306   255   154   419    56  
RS(511, 499) 
t=6 
p=0 
1   18   229   314   312   338    81   349   334   347   273    73    67  
p=1 
1   19   231   317   316   343    87   356   342   356   283    84    79  
RS(511, 497) 
t=7 
p=0 
1   5  395   124   77    77   268   225   281   103   116   176   460    83    92  
p=1 
1   6   397   127   81   82    274   232   289    112   126   187   472    96   106 
RS(511, 495) 
t=8 
p=0 
1  418   285     1   133   288   434   365   358   380   464   333   193    76 
 
375    12   121 
p=1 
1  419   287     4   137   293   440   372   366   389   474   344   205    89 
 
389    27   137 
RS(511, 493) 
t=9 
p=0 
1  390   472    90   210   352   166   252   200   196   217   286   217   420 
 
295   192    80    15   154  
p=1   
1  391   474    93   214   357   172   259   208   205   227   297   229   433 
 
309   207    96    32   172  
RS(511,491) 
t=10 
p=0 
1  366    17   118   453   497   299   372   499   139   115   158    26   429 
 
375    81    56   251   169    26   191 
p=1 
1  367    19   121   457   502   305   379   507   148   125   169    38   442 
 
389    96    72   268   187    45   211 
 

Appendix B: Tables for Information and 
Entropy Computing 
B1   Table for Computing Values of -log2(x), 0.01 ≤ x ≤ 0.99 
x 
-log2(x) 
 
x 
-log2(x)
 
x 
-log2(x)
 
x 
-log2(x) 
0.00  
 
0.25 2.0000 
 
0.50 1.0000 
 
0.75 0.4150 
0.01 6.6439 
 
0.26 1.9434 
 
0.51 0.9714 
 
0.76 0.3959 
0.02 5.6439 
 
0.27 1.8890 
 
0.52 0.9434 
 
0.77 0.3771 
0.03 5.0589 
 
0.28 1.8365 
 
0.53 0.9159 
 
0.78 0.3585 
0.04 4.6439 
 
0.29 1.7859 
 
0.54 0.8890 
 
0.79 0.3401 
0.05 4.3219 
 
0.30 1.7370 
 
0.55 0.8625 
 
0.80 0.3219 
0.06 4.0589 
 
0.31 1.6897 
 
0.56 0.8365 
 
0.81 0.3040 
0.07 3.8365 
 
0.32 1.6439 
 
0.57 0.8110 
 
0.82 0.2863 
0.08 3.6439 
 
0.33 1.5995 
 
0.58 0.7859 
 
0.83 0.2688 
0.09 3.4739 
 
0.34 1.5564 
 
0.59 0.7612 
 
0.84 0.2515 
0.10 3.3219 
 
0.35 1.5146 
 
0.60 0.7370 
 
0.85 0.2345 
0.11 3.1844 
 
0.36 1.4739 
 
0.61 0.7131 
 
0.86 0.2176 
0.12 3.0589 
 
0.37 1.4344 
 
0.62 0.6897 
 
0.87 0.2009 
0.13 2.9434 
 
0.38 1.3959 
 
0.63 0.6666 
 
0.88 0.1844 
0.14 2.8365 
 
0.39 1.3585 
 
0.64 0.6439 
 
0.89 0.1681 
0.15 2.7370 
 
0.40 1.3219 
 
0.65 0.6215 
 
0.90 0.1520 
0.16 2.6439 
 
0.41 1.2863 
 
0.66 0.5995 
 
0.91 0.1361 
0.17 2.5564 
 
0.42 1.2515 
 
0.67 0.5778 
 
0.92 0.1203 
0.18 2.4739 
 
0.43 1.2176 
 
0.68 0.5564 
 
0.93 0.1047 
0.19 2.3959 
 
0.44 1.1844 
 
0.69 0.5353 
 
0.94 0.0893 
0.20 2.3219 
 
0.45 1.1520 
 
0.70 0.5146 
 
0.95 0.0740 
0.21 2.2515 
 
0.46 1.1203 
 
0.71 0.4941 
 
0.96 0.0589 
0.22 2.1844 
 
0.47 1.0893 
 
0.72 0.4739 
 
0.97 0.0439 
0.23 2.1203 
 
0.48 1.0589 
 
0.73 0.4540 
 
0.98 0.0291 
0.24 2.0589 
 
0.49 1.0291 
 
0.74 0.4344 
 
0.99 0.0145 
 
 
 

428
Appendix B: Tables for Information and Entropy Computing
 
B2   Table for Computing Values of -x·log2(x), 0.001 ≤ x ≤ 0.999 
x 
 -xlog2(x)  
x 
 -xlog2(x) 
x 
 -xlog2(x) 
x 
 -xlog2(x) 
0.000
- 
 
0.250 0.5000  
0.500 0.5000  
0.750 0.3113 
0.001 0.0100  
0.251 0.5006  
0.501 0.4996  
0.751 0.3102 
0.002 0.0179  
0.252 0.5011  
0.502 0.4991  
0.752 0.3092 
0.003 0.0251  
0.253 0.5016  
0.503 0.4987  
0.753 0.3082 
0.004 0.0319  
0.254 0.5022  
0.504 0.4982  
0.754 0.3072 
0.005 0.0382  
0.255 0.5027  
0.505 0.4978  
0.755 0.3061 
0.006 0.0443  
0.256 0.5032  
0.506 0.4973  
0.756 0.3051 
0.007 0.0501  
0.257 0.5038  
0.507 0.4968  
0.757 0.3040 
0.008 0.0557  
0.258 0.5043  
0.508 0.4964  
0.758 0.3030 
0.009 0.0612  
0.259 0.5048  
0.509 0.4959  
0.759 0.3020 
0.010 0.0664  
0.260 0.5053  
0.510 0.4954  
0.760 0.3009 
0.011 0.0716  
0.261 0.5058  
0.511 0.4950  
0.761 0.2999 
0.012 0.0766  
0.262 0.5063  
0.512 0.4945  
0.762 0.2988 
0.013 0.0814  
0.263 0.5068  
0.513 0.4940  
0.763 0.2978 
0.014 0.0862  
0.264 0.5072  
0.514 0.4935  
0.764 0.2967 
0.015 0.0909  
0.265 0.5077  
0.515 0.4930  
0.765 0.2956 
0.016 0.0955  
0.266 0.5082  
0.516 0.4926  
0.766 0.2946 
0.017 0.0999  
0.267 0.5087  
0.517 0.4921  
0.767 0.2935 
0.018 0.1043  
0.268 0.5091  
0.518 0.4916  
0.768 0.2925 
0.019 0.1086  
0.269 0.5096  
0.519 0.4911  
0.769 0.2914 
0.020 0.1129  
0.270 0.5100  
0.520 0.4906  
0.770 0.2903 
0.021 0.1170  
0.271 0.5105  
0.521 0.4901  
0.771 0.2893 
0.022 0.1211  
0.272 0.5109  
0.522 0.4896  
0.772 0.2882 
0.023 0.1252  
0.273 0.5113  
0.523 0.4891  
0.773 0.2871 
0.024 0.1291  
0.274 0.5118  
0.524 0.4886  
0.774 0.2861 
0.025 0.1330  
0.275 0.5122  
0.525 0.4880  
0.775 0.2850 
0.026 0.1369  
0.276 0.5126  
0.526 0.4875  
0.776 0.2839 
0.027 0.1407  
0.277 0.5130  
0.527 0.4870  
0.777 0.2828 
0.028 0.1444  
0.278 0.5134  
0.528 0.4865  
0.778 0.2818 
0.029 0.1481  
0.279 0.5138  
0.529 0.4860  
0.779 0.2807 
0.030 0.1518  
0.280 0.5142  
0.530 0.4854  
0.780 0.2796 
0.031 0.1554  
0.281 0.5146  
0.531 0.4849  
0.781 0.2785 
0.032 0.1589  
0.282 0.5150  
0.532 0.4844  
0.782 0.2774 
0.033 0.1624  
0.283 0.5154  
0.533 0.4839  
0.783 0.2763 
0.034 0.1659  
0.284 0.5158  
0.534 0.4833  
0.784 0.2752 
0.035 0.1693  
0.285 0.5161  
0.535 0.4828  
0.785 0.2741 
0.036 0.1727  
0.286 0.5165  
0.536 0.4822  
0.786 0.2731 
0.037 0.1760  
0.287 0.5169  
0.537 0.4817  
0.787 0.2720 

B2   Table for Computing Values of -x·log2(x), 0.001 ≤ x ≤ 0.999 
429
 
x 
 -xlog2(x)  
x 
 -xlog2(x) 
x 
 -xlog2(x) 
x 
 -xlog2(x) 
0.038 0.1793  
0.288 0.5172  
0.538 0.4811  
0.788 0.2709 
0.039 0.1825  
0.289 0.5176  
0.539 0.4806  
0.789 0.2698 
0.040 0.1858  
0.290 0.5179  
0.540 0.4800  
0.790 0.2687 
0.041 0.1889  
0.291 0.5182  
0.541 0.4795  
0.791 0.2676 
0.042 0.1921  
0.292 0.5186  
0.542 0.4789  
0.792 0.2665 
0.043 0.1952  
0.293 0.5189  
0.543 0.4784  
0.793 0.2653 
0.044 0.1983  
0.294 0.5192  
0.544 0.4778  
0.794 0.2642 
0.045 0.2013  
0.295 0.5196  
0.545 0.4772  
0.795 0.2631 
0.046 0.2043  
0.296 0.5199  
0.546 0.4767  
0.796 0.2620 
0.047 0.2073  
0.297 0.5202  
0.547 0.4761  
0.797 0.2609 
0.048 0.2103  
0.298 0.5205  
0.548 0.4755  
0.798 0.2598 
0.049 0.2132  
0.299 0.5208  
0.549 0.4750  
0.799 0.2587 
0.050 0.2161  
0.300 0.5211  
0.550 0.4744  
0.800 0.2575 
0.051 0.2190  
0.301 0.5214  
0.551 0.4738  
0.801 0.2564 
0.052 0.2218  
0.302 0.5217  
0.552 0.4732  
0.802 0.2553 
0.053 0.2246  
0.303 0.5220  
0.553 0.4726  
0.803 0.2542 
0.054 0.2274  
0.304 0.5222  
0.554 0.4720  
0.804 0.2530 
0.055 0.2301  
0.305 0.5225  
0.555 0.4714  
0.805 0.2519 
0.056 0.2329  
0.306 0.5228  
0.556 0.4708  
0.806 0.2508 
0.057 0.2356  
0.307 0.5230  
0.557 0.4702  
0.807 0.2497 
0.058 0.2383  
0.308 0.5233  
0.558 0.4696  
0.808 0.2485 
0.059 0.2409  
0.309 0.5235  
0.559 0.4690  
0.809 0.2474 
0.060 0.2435  
0.310 0.5238  
0.560 0.4684  
0.810 0.2462 
0.061 0.2461  
0.311 0.5240  
0.561 0.4678  
0.811 0.2451 
0.062 0.2487  
0.312 0.5243  
0.562 0.4672  
0.812 0.2440 
0.063 0.2513  
0.313 0.5245  
0.563 0.4666  
0.813 0.2428 
0.064 0.2538  
0.314 0.5247  
0.564 0.4660  
0.814 0.2417 
0.065 0.2563  
0.315 0.5250  
0.565 0.4654  
0.815 0.2405 
0.066 0.2588  
0.316 0.5252  
0.566 0.4648  
0.816 0.2394 
0.067 0.2613  
0.317 0.5254  
0.567 0.4641  
0.817 0.2382 
0.068 0.2637  
0.318 0.5256  
0.568 0.4635  
0.818 0.2371 
0.069 0.2662  
0.319 0.5258  
0.569 0.4629  
0.819 0.2359 
0.070 0.2686  
0.320 0.5260  
0.570 0.4623  
0.820 0.2348 
0.071 0.2709  
0.321 0.5262  
0.571 0.4616  
0.821 0.2336 
0.072 0.2733  
0.322 0.5264  
0.572 0.4610  
0.822 0.2325 
0.073 0.2756  
0.323 0.5266  
0.573 0.4603  
0.823 0.2313 
0.074 0.2780  
0.324 0.5268  
0.574 0.4597  
0.824 0.2301 
0.075 0.2803  
0.325 0.5270  
0.575 0.4591  
0.825 0.2290 
0.076 0.2826  
0.326 0.5272  
0.576 0.4584  
0.826 0.2278 
0.077 0.2848  
0.327 0.5273  
0.577 0.4578  
0.827 0.2266 

430
Appendix B: Tables for Information and Entropy Computing
 
x 
 -xlog2(x)  
x 
 -xlog2(x) 
x 
 -xlog2(x) 
x 
 -xlog2(x) 
0.078 0.2871  
0.328 0.5275  
0.578 0.4571  
0.828 0.2255 
0.079 0.2893  
0.329 0.5277  
0.579 0.4565  
0.829 0.2243 
0.080 0.2915  
0.330 0.5278  
0.580 0.4558  
0.830 0.2231 
0.081 0.2937  
0.331 0.5280  
0.581 0.4551  
0.831 0.2219 
0.082 0.2959  
0.332 0.5281  
0.582 0.4545  
0.832 0.2208 
0.083 0.2980  
0.333 0.5283  
0.583 0.4538  
0.833 0.2196 
0.084 0.3002  
0.334 0.5284  
0.584 0.4532  
0.834 0.2184 
0.085 0.3023  
0.335 0.5286  
0.585 0.4525  
0.835 0.2172 
0.086 0.3044  
0.336 0.5287  
0.586 0.4518  
0.836 0.2160 
0.087 0.3065  
0.337 0.5288  
0.587 0.4511  
0.837 0.2149 
0.088 0.3086  
0.338 0.5289  
0.588 0.4505  
0.838 0.2137 
0.089 0.3106  
0.339 0.5291  
0.589 0.4498  
0.839 0.2125 
0.090 0.3127  
0.340 0.5292  
0.590 0.4491  
0.840 0.2113 
0.091 0.3147  
0.341 0.5293  
0.591 0.4484  
0.841 0.2101 
0.092 0.3167  
0.342 0.5294  
0.592 0.4477  
0.842 0.2089 
0.093 0.3187  
0.343 0.5295  
0.593 0.4471  
0.843 0.2077 
0.094 0.3207  
0.344 0.5296  
0.594 0.4464  
0.844 0.2065 
0.095 0.3226  
0.345 0.5297  
0.595 0.4457  
0.845 0.2053 
0.096 0.3246  
0.346 0.5298  
0.596 0.4450  
0.846 0.2041 
0.097 0.3265  
0.347 0.5299  
0.597 0.4443  
0.847 0.2029 
0.098 0.3284  
0.348 0.5299  
0.598 0.4436  
0.848 0.2017 
0.099 0.3303  
0.349 0.5300  
0.599 0.4429  
0.849 0.2005 
0.100 0.3322  
0.350 0.5301  
0.600 0.4422  
0.850 0.1993 
0.101 0.3341  
0.351 0.5302  
0.601 0.4415  
0.851 0.1981 
0.102 0.3359  
0.352 0.5302  
0.602 0.4408  
0.852 0.1969 
0.103 0.3378  
0.353 0.5303  
0.603 0.4401  
0.853 0.1957 
0.104 0.3396  
0.354 0.5304  
0.604 0.4393  
0.854 0.1944 
0.105 0.3414  
0.355 0.5304  
0.605 0.4386  
0.855 0.1932 
0.106 0.3432  
0.356 0.5305  
0.606 0.4379  
0.856 0.1920 
0.107 0.3450  
0.357 0.5305  
0.607 0.4372  
0.857 0.1908 
0.108 0.3468  
0.358 0.5305  
0.608 0.4365  
0.858 0.1896 
0.109 0.3485  
0.359 0.5306  
0.609 0.4357  
0.859 0.1884 
0.110 0.3503  
0.360 0.5306  
0.610 0.4350  
0.860 0.1871 
0.111 0.3520  
0.361 0.5306  
0.611 0.4343  
0.861 0.1859 
0.112 0.3537  
0.362 0.5307  
0.612 0.4335  
0.862 0.1847 
0.113 0.3555  
0.363 0.5307  
0.613 0.4328  
0.863 0.1834 
0.114 0.3571  
0.364 0.5307  
0.614 0.4321  
0.864 0.1822 
0.115 0.3588  
0.365 0.5307  
0.615 0.4313  
0.865 0.1810 
0.116 0.3605  
0.366 0.5307  
0.616 0.4306  
0.866 0.1797 
0.117 0.3622  
0.367 0.5307  
0.617 0.4298  
0.867 0.1785 

B2   Table for Computing Values of -x·log2(x), 0.001 ≤ x ≤ 0.999 
431
 
x 
 -xlog2(x)  
x 
 -xlog2(x) 
x 
 -xlog2(x) 
x 
 -xlog2(x) 
0.118 0.3638  
0.368 0.5307  
0.618 0.4291  
0.868 0.1773 
0.119 0.3654  
0.369 0.5307  
0.619 0.4283  
0.869 0.1760 
0.120 0.3671  
0.370 0.5307  
0.620 0.4276  
0.870 0.1748 
0.121 0.3687  
0.371 0.5307  
0.621 0.4268  
0.871 0.1736 
0.122 0.3703  
0.372 0.5307  
0.622 0.4261  
0.872 0.1723 
0.123 0.3719  
0.373 0.5307  
0.623 0.4253  
0.873 0.1711 
0.124 0.3734  
0.374 0.5307  
0.624 0.4246  
0.874 0.1698 
0.125 0.3750  
0.375 0.5306  
0.625 0.4238  
0.875 0.1686 
0.126 0.3766  
0.376 0.5306  
0.626 0.4230  
0.876 0.1673 
0.127 0.3781  
0.377 0.5306  
0.627 0.4223  
0.877 0.1661 
0.128 0.3796  
0.378 0.5305  
0.628 0.4215  
0.878 0.1648 
0.129 0.3811  
0.379 0.5305  
0.629 0.4207  
0.879 0.1636 
0.130 0.3826  
0.380 0.5305  
0.630 0.4199  
0.880 0.1623 
0.131 0.3841  
0.381 0.5304  
0.631 0.4192  
0.881 0.1610 
0.132 0.3856  
0.382 0.5304  
0.632 0.4184  
0.882 0.1598 
0.133 0.3871  
0.383 0.5303  
0.633 0.4176  
0.883 0.1585 
0.134 0.3886  
0.384 0.5302  
0.634 0.4168  
0.884 0.1572 
0.135 0.3900  
0.385 0.5302  
0.635 0.4160  
0.885 0.1560 
0.136 0.3915  
0.386 0.5301  
0.636 0.4152  
0.886 0.1547 
0.137 0.3929  
0.387 0.5300  
0.637 0.4145  
0.887 0.1534 
0.138 0.3943  
0.388 0.5300  
0.638 0.4137  
0.888 0.1522 
0.139 0.3957  
0.389 0.5299  
0.639 0.4129  
0.889 0.1509 
0.140 0.3971  
0.390 0.5298  
0.640 0.4121  
0.890 0.1496 
0.141 0.3985  
0.391 0.5297  
0.641 0.4113  
0.891 0.1484 
0.142 0.3999  
0.392 0.5296  
0.642 0.4105  
0.892 0.1471 
0.143 0.4012  
0.393 0.5295  
0.643 0.4097  
0.893 0.1458 
0.144 0.4026  
0.394 0.5294  
0.644 0.4089  
0.894 0.1445 
0.145 0.4040  
0.395 0.5293  
0.645 0.4080  
0.895 0.1432 
0.146 0.4053  
0.396 0.5292  
0.646 0.4072  
0.896 0.1420 
0.147 0.4066  
0.397 0.5291  
0.647 0.4064  
0.897 0.1407 
0.148 0.4079  
0.398 0.5290  
0.648 0.4056  
0.898 0.1394 
0.149 0.4092  
0.399 0.5289  
0.649 0.4048  
0.899 0.1381 
0.150 0.4105  
0.400 0.5288  
0.650 0.4040  
0.900 0.1368 
0.151 0.4118  
0.401 0.5286  
0.651 0.4031  
0.901 0.1355 
0.152 0.4131  
0.402 0.5285  
0.652 0.4023  
0.902 0.1342 
0.153 0.4144  
0.403 0.5284  
0.653 0.4015  
0.903 0.1329 
0.154 0.4156  
0.404 0.5283  
0.654 0.4007  
0.904 0.1316 
0.155 0.4169  
0.405 0.5281  
0.655 0.3998  
0.905 0.1303 
0.156 0.4181  
0.406 0.5280  
0.656 0.3990  
0.906 0.1290 
0.157 0.4194  
0.407 0.5278  
0.657 0.3982  
0.907 0.1277 

432
Appendix B: Tables for Information and Entropy Computing
 
x 
 -xlog2(x)  
x 
 -xlog2(x) 
x 
 -xlog2(x) 
x 
 -xlog2(x) 
0.158 0.4206  
0.408 0.5277  
0.658 0.3973  
0.908 0.1264 
0.159 0.4218  
0.409 0.5275  
0.659 0.3965  
0.909 0.1251 
0.160 0.4230  
0.410 0.5274  
0.660 0.3956  
0.910 0.1238 
0.161 0.4242  
0.411 0.5272  
0.661 0.3948  
0.911 0.1225 
0.162 0.4254  
0.412 0.5271  
0.662 0.3940  
0.912 0.1212 
0.163 0.4266  
0.413 0.5269  
0.663 0.3931  
0.913 0.1199 
0.164 0.4278  
0.414 0.5267  
0.664 0.3923  
0.914 0.1186 
0.165 0.4289  
0.415 0.5266  
0.665 0.3914  
0.915 0.1173 
0.166 0.4301  
0.416 0.5264  
0.666 0.3905  
0.916 0.1159 
0.167 0.4312  
0.417 0.5262  
0.667 0.3897  
0.917 0.1146 
0.168 0.4323  
0.418 0.5260  
0.668 0.3888  
0.918 0.1133 
0.169 0.4335  
0.419 0.5258  
0.669 0.3880  
0.919 0.1120 
0.170 0.4346  
0.420 0.5256  
0.670 0.3871  
0.920 0.1107 
0.171 0.4357  
0.421 0.5255  
0.671 0.3862  
0.921 0.1093 
0.172 0.4368  
0.422 0.5253  
0.672 0.3854  
0.922 0.1080 
0.173 0.4379  
0.423 0.5251  
0.673 0.3845  
0.923 0.1067 
0.174 0.4390  
0.424 0.5249  
0.674 0.3836  
0.924 0.1054 
0.175 0.4401  
0.425 0.5246  
0.675 0.3828  
0.925 0.1040 
0.176 0.4411  
0.426 0.5244  
0.676 0.3819  
0.926 0.1027 
0.177 0.4422  
0.427 0.5242  
0.677 0.3810  
0.927 0.1014 
0.178 0.4432  
0.428 0.5240  
0.678 0.3801  
0.928 0.1000 
0.179 0.4443  
0.429 0.5238  
0.679 0.3792  
0.929 0.0987 
0.180 0.4453  
0.430 0.5236  
0.680 0.3783  
0.930 0.0974 
0.181 0.4463  
0.431 0.5233  
0.681 0.3775  
0.931 0.0960 
0.182 0.4474  
0.432 0.5231  
0.682 0.3766  
0.932 0.0947 
0.183 0.4484  
0.433 0.5229  
0.683 0.3757  
0.933 0.0933 
0.184 0.4494  
0.434 0.5226  
0.684 0.3748  
0.934 0.0920 
0.185 0.4504  
0.435 0.5224  
0.685 0.3739  
0.935 0.0907 
0.186 0.4514  
0.436 0.5222  
0.686 0.3730  
0.936 0.0893 
0.187 0.4523  
0.437 0.5219  
0.687 0.3721  
0.937 0.0880 
0.188 0.4533  
0.438 0.5217  
0.688 0.3712  
0.938 0.0866 
0.189 0.4543  
0.439 0.5214  
0.689 0.3703  
0.939 0.0853 
0.190 0.4552  
0.440 0.5211  
0.690 0.3694  
0.940 0.0839 
0.191 0.4562  
0.441 0.5209  
0.691 0.3685  
0.941 0.0826 
0.192 0.4571  
0.442 0.5206  
0.692 0.3676  
0.942 0.0812 
0.193 0.4581  
0.443 0.5204  
0.693 0.3666  
0.943 0.0798 
0.194 0.4590  
0.444 0.5201  
0.694 0.3657  
0.944 0.0785 
0.195 0.4599  
0.445 0.5198  
0.695 0.3648  
0.945 0.0771 
0.196 0.4608  
0.446 0.5195  
0.696 0.3639  
0.946 0.0758 
0.197 0.4617  
0.447 0.5193  
0.697 0.3630  
0.947 0.0744 

B2   Table for Computing Values of -x·log2(x), 0.001 ≤ x ≤ 0.999 
433
 
x 
 -xlog2(x)  
x 
 -xlog2(x) 
x 
 -xlog2(x) 
x 
 -xlog2(x) 
0.198 0.4626  
0.448 0.5190  
0.698 0.3621  
0.948 0.0730 
0.199 0.4635  
0.449 0.5187  
0.699 0.3611  
0.949 0.0717 
0.200 0.4644  
0.450 0.5184  
0.700 0.3602  
0.950 0.0703 
0.201 0.4653  
0.451 0.5181  
0.701 0.3593  
0.951 0.0689 
0.202 0.4661  
0.452 0.5178  
0.702 0.3583  
0.952 0.0676 
0.203 0.4670  
0.453 0.5175  
0.703 0.3574  
0.953 0.0662 
0.204 0.4678  
0.454 0.5172  
0.704 0.3565  
0.954 0.0648 
0.205 0.4687  
0.455 0.5169  
0.705 0.3555  
0.955 0.0634 
0.206 0.4695  
0.456 0.5166  
0.706 0.3546  
0.956 0.0621 
0.207 0.4704  
0.457 0.5163  
0.707 0.3537  
0.957 0.0607 
0.208 0.4712  
0.458 0.5160  
0.708 0.3527  
0.958 0.0593 
0.209 0.4720  
0.459 0.5157  
0.709 0.3518  
0.959 0.0579 
0.210 0.4728  
0.460 0.5153  
0.710 0.3508  
0.960 0.0565 
0.211 0.4736  
0.461 0.5150  
0.711 0.3499  
0.961 0.0552 
0.212 0.4744  
0.462 0.5147  
0.712 0.3489  
0.962 0.0538 
0.213 0.4752  
0.463 0.5144  
0.713 0.3480  
0.963 0.0524 
0.214 0.4760  
0.464 0.5140  
0.714 0.3470  
0.964 0.0510 
0.215 0.4768  
0.465 0.5137  
0.715 0.3460  
0.965 0.0496 
0.216 0.4776  
0.466 0.5133  
0.716 0.3451  
0.966 0.0482 
0.217 0.4783  
0.467 0.5130  
0.717 0.3441  
0.967 0.0468 
0.218 0.4791  
0.468 0.5127  
0.718 0.3432  
0.968 0.0454 
0.219 0.4798  
0.469 0.5123  
0.719 0.3422  
0.969 0.0440 
0.220 0.4806  
0.470 0.5120  
0.720 0.3412  
0.970 0.0426 
0.221 0.4813  
0.471 0.5116  
0.721 0.3403  
0.971 0.0412 
0.222 0.4820  
0.472 0.5112  
0.722 0.3393  
0.972 0.0398 
0.223 0.4828  
0.473 0.5109  
0.723 0.3383  
0.973 0.0384 
0.224 0.4835  
0.474 0.5105  
0.724 0.3373  
0.974 0.0370 
0.225 0.4842  
0.475 0.5102  
0.725 0.3364  
0.975 0.0356 
0.226 0.4849  
0.476 0.5098  
0.726 0.3354  
0.976 0.0342 
0.227 0.4856  
0.477 0.5094  
0.727 0.3344  
0.977 0.0328 
0.228 0.4863  
0.478 0.5090  
0.728 0.3334  
0.978 0.0314 
0.229 0.4870  
0.479 0.5087  
0.729 0.3324  
0.979 0.0300 
0.230 0.4877  
0.480 0.5083  
0.730 0.3314  
0.980 0.0286 
0.231 0.4883  
0.481 0.5079  
0.731 0.3305  
0.981 0.0271 
0.232 0.4890  
0.482 0.5075  
0.732 0.3295  
0.982 0.0257 
0.233 0.4897  
0.483 0.5071  
0.733 0.3285  
0.983 0.0243 
0.234 0.4903  
0.484 0.5067  
0.734 0.3275  
0.984 0.0229 
0.235 0.4910  
0.485 0.5063  
0.735 0.3265  
0.985 0.0215 
0.236 0.4916  
0.486 0.5059  
0.736 0.3255  
0.986 0.0201 
0.237 0.4923  
0.487 0.5055  
0.737 0.3245  
0.987 0.0186 

434
Appendix B: Tables for Information and Entropy Computing
 
x 
 -xlog2(x)  
x 
 -xlog2(x) 
x 
 -xlog2(x) 
x 
 -xlog2(x) 
0.238 0.4929  
0.488 0.5051  
0.738 0.3235  
0.988 0.0172 
0.239 0.4935  
0.489 0.5047  
0.739 0.3225  
0.989 0.0158 
0.240 0.4941  
0.490 0.5043  
0.740 0.3215  
0.990 0.0144 
0.241 0.4947  
0.491 0.5039  
0.741 0.3204  
0.991 0.0129 
0.242 0.4954  
0.492 0.5034  
0.742 0.3194  
0.992 0.0115 
0.243 0.4960  
0.493 0.5030  
0.743 0.3184  
0.993 0.0101 
0.244 0.4966  
0.494 0.5026  
0.744 0.3174  
0.994 0.0086 
0.245 0.4971  
0.495 0.5022  
0.745 0.3164  
0.995 0.0072 
0.246 0.4977  
0.496 0.5017  
0.746 0.3154  
0.996 0.0058 
0.247 0.4983  
0.497 0.5013  
0.747 0.3144  
0.997 0.0043 
0.248 0.4989  
0.498 0.5009  
0.748 0.3133  
0.998 0.0029 
0.249 0.4994  
0.499 0.5004  
0.749 0.3123  
0.999 0.0014 
 
 

Appendix C: Signal Detection Elements 
C.1   Detection Problem 
Signal detection is part of the statistical decision theory or hypotheses testing the-
ory. The aim of this processing, made at the receiver, is to decide which was the 
sent signal, based on the observation of the received signal (observation space). A 
block- scheme of a system using signal detection is given in Fig C.1. 
 
Fig. C.1 Block scheme of a transmission system using signal detection. S- source, N- noise 
generator, SD- signal detection block, U- user, si(t) - transmitted signal, r(t) - received sig-
nal, n(t) - noise voltage, ŝi(t) - estimated signal. 
In signal detection block (SD), the received signal r(t) (observation space) is 
observed and, using a decision criterion, a decision is made concerning which is 
the transmitted signal. Decision taken is on thus the affirmation of a hypothesis 
(Hi). The observation of r(t) can be: 
 
• discrete observation: at discrete moments 
it , 
N
1,
i =
samples from 
)
(t
r
are 
taken ( )
ir , the decision being taken on 
(
)
N
1
r
,...,
r
r =
. If N is variable, the de-
tection is called sequential. 
• continuous observation: r(t) is observed continuously during the observation 
time T, and the decision is taken based on 
dt
r(t)
T
0
∫
. It represents the discrete 
case at limit: 
∞
→
N
. 
If the source S is binary, the decision is binary, otherwise M-ary (when the 
source is M-ary). We will focus only on binary detection, the M-ary case being a 
generalization of the binary one [1], [4], [6], [7]. 

436 
Appendix C: Signal Detection Elements
 
The binary source is: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
1
0
1
0
P
P
(t)
s
(t)
s
:
S
,
1
P
P
1
0
=
+
                                     (C.1) 
assumed memoryless, 
0
P and 1
P  being the a priori probabilities. 
Under the assumption of AWGN, the received signal (observation space Δ) is: 
0
0
0
r/s
or 
 
n(t)
(t)
s
r(t)
  :
H
+
=
                                  (C.2.a) 
1
1
1
r/s
or 
 
n(t)
(t)
s
r(t)
  :
H
+
=
                                   (C.2.b) 
 
Fig. C.2 Binary decision splits observation space Δ into two disjoint spaces Δ0 and Δ1. 
We may have four situations: 
 
• 
)
D
  ,
(s
0
0
- correct decision in the case of 0
s  
• 
)
D
  ,
(s
1
1
- correct decision in the case of 1s  
• 
)
D
  ,
(s
1
0
- wrong decision in the case of 0
s  
• 
)
D
  ,
(s
0
1
- wrong decision in the case of 1s  
The consequences of these decisions are different and application linked; they 
can be valued with coefficients named costs, 
ij
C : the cost of deciding 
i
D when 
js  
was transmitted. For binary decision there are four costs which can be included in 
cost matrix C: 
⎥⎦
⎤
⎢⎣
⎡
=
11
01
10
00
C
C
C
C
C
                                           (C.3) 

C.1   Detection Problem 
437
 
Concerning the costs, always the cost of wrong decisions is higher than those of 
good decisions (we pay for mistakes): 
00
10
C
C
>>
 and 
11
01
C
C
>>
 
In data transmission 
0
C
C
11
00
=
=
and 
10
01
C
C
=
 (the consequence of an er-
ror on ‘0’ or on ‘1’ is the same). 
Then, for binary decision an average cost, named risk can be obtained: 
)
/s
P(D
P
C
)
/s
P(D
P
C
)
/s
P(D
P
C
)
/s
P(D
P
C
    
)
s
P(D
C
)
s
P(D
C
)
s
P(D
C
)
s
P(D
C
    
)
s
P(D
C
C
:
R
0
1
0
10
1
0
1
01
1
1
1
11
0
0
0
00
0
1
10
1
0
01
1
1
11
0
0
00
1
0
i
1
0
j
j
i
ij
+
+
+
=
=
+
+
+
=
=
∑∑
=
=
=
=
    (C.4) 
Conditional probabilities 
)
/s
(D
P
j
i
=
 can be calculated based on conditional 
pdfs (probability density functions):
)
p(r/s j : 
)dr
p(r/s
)
/s
P(D
0
Δ
0
0
0
∫
=
                                       (C.5.a) 
)dr
p(r/s
)
/s
P(D
1
Δ
0
0
1
∫
=
                                      (C.5.b) 
)dr
p(r/s
)
/s
P(D
0
Δ
1
1
0
∫
=
                                      (C.5.c) 
)dr
p(r/s
)
/s
P(D
1
Δ
1
1
1
∫
=
                                      (C.5.d) 
Taking into account that the domains 
0
Δ  and 
1
Δ  are disjoint, we have: 
1
)dr
p(r/s
)dr
p(r/s
1
0
Δ
0
Δ
0
=
∫
+
∫
                                  (C.6.a) 
1
)dr
p(r/s
)dr
p(r/s
1
0
Δ
1
Δ
1
=
∫
+
∫
                                  (C.6.b) 
Replacing the conditional probabilities 
)
/s
P(D
j
i
 with (C.5.a÷d), and taking 
into consideration (C.6.a and b), the risk can be expressed only with one domain 
0
Δ , or 
1
Δ : 
)]dr
C
)(C
p(r/s
)P
C
)(C
[p(r/s
P
C
P
C
R
00
10
0
1
11
01
Δ
1
0
10
1
11
0
−
−
−
∫
+
+
=
     (C.4.a) 

438 
Appendix C: Signal Detection Elements
 
C.2   Signal Detection Criteria 
C.2.1   Bayes Criterion 
Bayes criterion is the minimum risk criterion and is obtained minimising (C.4.a): 
11
01
00
10
1
0
Δ
Δ
0
1
C
C
C
C
P
P
)
p(r/s
)
p(r/s
0
1
−
−
>
<
                                      (C.7) 
where  
ȁ(r)
:
)
p(r/s
)
p(r/s
0
1
=
=: likelihood ratio                               (C.8) 
)
p(r/s1  and 
)
p(r/s0 being known as likelihood functions and  
K
C
C
C
C
P
P
11
01
00
10
1
0
=
−
−
 =: threshold                                (C.9) 
Then Bayes criterion can be expressed as: 
K
ln 
 
(r)
ln 
or  
K  
ȁ(r)
1
0
>
<
Λ
>
<
Δ
Δ
                                (C.7.a) 
and it gives the block scheme of an optimal receiver( Fig. C.3) . 
 
Fig. C.3 Block scheme of an optimal receiver (operating according to Bayes criterion, of 
minimum risk) 
The quality of signal detection processing is appreciated by: 
 
• Error probability: 
E
P (BER) 
)
/s
P(D
P
)
/s
P(D
P
P
1
0
1
0
1
0
E
+
=
                                  (C.10) 

C.2   Signal Detection Criteria 
439
 
Under the assumption of AWGN, the pdf of the noise is
)
,0
(
2
n
N
σ
: 
2
2n
n
2ı
1
2n
e
2π
1
p(n)
−
=
σ
                                      (C.11) 
and the conditional pdf:
)
p(r/si are also of Gaussian type (Fig. C.4) 
 
Fig. C.4 Binary detection parameters: Pm- probability of miss, PD- probability of detection, 
Pf - probability of false detection 
In engineering, the terminology, originating from radar [1] is: 
 
– 
probability of false alarm: 
fP  
)dr
p(r/s
P
1
Δ
0
f
∫
=
                                         (C.12) 
– 
probability of miss: 
m
P  
)dr
p(r/s
P
0
Δ
1
m
∫
=
                                        (C.13) 
– 
probability of detection: 
D
P  
)dr
p(r/s
P
1
Δ
1
D
∫
=
                                        (C.14) 
• Integrals from normal pdfs can be calculated in many ways, one of them being 
function Q(y), also called complementary error function (co-error function: erfc). 
∫
=
∞
1
y
1
f(y)dy
:
)
Q(y
                                       (C.15) 

440 
Appendix C: Signal Detection Elements
 
where f(y) is a normal standard pdf, N(0,1): 
2
y
2
1
e
2π
1
:
f(y)
−
=
 and 
1
{y}
ı2
=
                            (C.16) 
with average value E{y} = y = 0 under the assumption of ergodicity [2]. It’s 
graphical representation is given in Fig. C.5. 
 
Fig. C.5 Graphical representation of function Q(y) 
The properties of function Q(y) are: 
(
)
(
)
( )
(
)
( )
y
Q
 - 1
y
Q
2
1
0
Q
0
Q
1
Q
=
−
=
=
∞
+
=
∞
−
                                          (C.17) 
If the Gaussian pdf is not normal standard, a variable change is used: 
y
ı
y
y
t
−
=
, with E{y} =
0
y ≠
 and
1
ın ≠                        (C.18) 
C.2.2   Minimum Probability of Error Criterion (Kotelnikov- Siegert) 
Under the assumption of: 
⎪⎩
⎪⎨
⎧
=
=
=
=
=
+
=
1
C
C
0
C
C
known
 - 
1)
P
(P
  ,
P
P
10
01
11
00
1
0
1
0
                            (C.19) 

C.2   Signal Detection Criteria 
441
 
The threshold (C.9) becomes: 
1
0
P
P
K =
 
and Bayes risk (C.4), the minimum risk is: 
min
E
f
0
m
1
min
P
P
P
P
P
R
=
+
=
 
from where the name of minimum error probability criteria. Bayes test (C.7)  
becomes: 
1
0
D
D
P
P
ȁ(r)
0
1
>
<
. 
C.2.3   Maximum a Posteriori Probability Criterion (MAP) 
Using Bayes probability relation (2.32), we have: 
)
)p(r/s
p(s
/r)
p(r)p(s
)
p(rs
i
i
i
i
=
=
                              (C.21) 
which gives: 
1
/r)
p(s
/r)
p(s
)P
p(r/s
)P
p(r/s
Δ
Δ
0
1
0
0
1
1
0
1
>
<
=
                                    (C.22) 
It can be written as: 
)
p(r/s
P
)
p(r/s
P
0
0
Δ
Δ
1
1
0
1
>
<
                                    (C.22.a) 
where 
)
p(r/s0  and 
)
p(r/s1  are known as Maximum A Posteriori pdfs. 
 
Remark 
(C.22), respectively (C.22.a) are in fact minimum error probability test, showing 
that MAP for error correction codes is an optimal decoding algorithm: it gives the 
minimum error probability. 
C.2.4   Maximum Likelihood Criterion (R. Fisher) 
If to the assumptions (C.19) we add also: 
1
0
P
P =
, the threshold (C.9) becomes: 
K = 1 
 

442 
Appendix C: Signal Detection Elements
 
and Bayesian test is: 
)
p(r/s
)
p(r/s
0
Δ
Δ
1
0
1
>
<
                                       (C.23) 
 
Remark 
The assumptions 
⎪
⎪
⎩
⎪⎪
⎨
⎧
=
=
=
=
=
=
2
1
P
P
1
C
C
0
C
C
1
0
10
01
11
00
 are basically those form data processing, this is 
why maximum likelihood criterion (K = 1) is the used decision criterion in data 
processing. 
C.3   Signal Detection in Data Processing (K = 1) 
C.3.1   Discrete Detection of a Unipolar Signal 
Hypotheses: 
 
• unipolar signal (in baseband): 
⎩
⎨
⎧
=
=
=
ct
A
(t)
s
0
(t)
s
1
0
 
• AWGN: N(0,
2
n
ı ); 
n(t)
(t)
s
r(t)
i
+
=
 
• T = bit duration = observation time 
• Discrete observation with N samples per observation time (T) 
(
)
N
1
r
,...,
r
r =
⇒G
 
• 
0
C
C
11
00
=
=
, 
1
C
C
10
01
=
=
, 
0
P , 1
P , with 
2
1
P
P
1
0
=
=
 
a. Likelihood ratio calculation 
n(t)
r/s
n(t)
n(t)
(t)
s
r(t)
:
H
0
0
0
=
→
=
+
=
 
A 
sample 
)
ı
N(0,
n
r
2
n
i
i
∈
=
 
and 
the 
N 
samples 
are 
giving 
(
)
(
)
N
1
N
1
n
,...,
n
r
,...,
r
r
=
=
G
 
2
i
2
n
r
2ı
1
2
n
0
i
e
2π
1
)
/s
p(r
−
=
σ
 
∑
−
=
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
N
1
i
2
i
2
n
r
2ı
1
N
2
n
0
e
2π
1
)
/s
r
p(
σ
G
 

C.3   Signal Detection in Data Processing (K = 1) 
443
 
n(t)
A
n(t)
(t)
s
r(t)
:
H
1
1
+
=
+
=
 
2
i
2n
A)
(r
2ı
1
2
n
1
i
2
n
i
i
e
2π
1
)
/s
p(r
)
ı
N(A,
n
A
r
−
−
=
⇒
∈
+
=
σ
 
∑
−
−
=
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
N
1
i
2
i
2
n
A)
(r
2ı
1
N
2
n
1
e
2
1
)
/s
r
p(
πσ
G
 
2
n
2
N
1
i
i
2
n
2ı
N
A
r
ı
A
e
e
ȁ(r)
−
∑
=
=
 
b. Minimum error probability test, applied to the logarithmic relation: 
lnK
)r(
ln
Δ
Δ
0
1
>
<
Λ
 
lnK
2ı
N
A
r
ı
A
Δ
Δ
2
n
2
N
1
i
i
2
n
0
1
>
<
−
∑
=
, or 
2
AN
lnK
A
ı
r
2
n
Δ
Δ
N
1
i
i
0
1
+
>
<
∑
=
                                    (C.24) 
where ∑
=
N
1
i
ir represents a sufficient statistics, meaning that it is sufficient to take the 
decisions and 
K
2
AN
lnK
A
ı2
n
′
=
+
                                     (C.25) 
represents a threshold depending on: the power of the noise on the channel (
2
n
ı ), the 
level of the signal (A), the number of samples (N) and 
0
P  , 1
P (through 
1
0 P
P
K =
). 
 
 
 
 
 
 
 
 
 

444 
Appendix C: Signal Detection Elements
 
Relation (C.24) direction to the block-scheme of an optimal receiver: 
 
Fig. C.6 Block-scheme of the optimal receiver for unipolar signal and discrete observation. 
Remark 
If K=1 and N=1 (one sample per bit, taken at 
2
T
) and 
1
0
P
P =
 (K=1), the deci-
sion relation (C.24) becomes: 
2
A
r
Δ
Δ
i
0
0
>
<
                                              (C.24.a) 
c. Error probability of the optimal receiver variable is 
According to (C.24), the decision variable is 
)
ı
n(E[y],
r
N
1
i
2
i
∑
∈
=
, E[y] being the 
variable and 
2
ı  the dispersion. Making the variable change 
N
ı
r
y
n
N
1
i
i
∑
=
=
                                            (C.26) 
a normalization is obtained: 
1
[y]
ı2
= . 
Using this new variable, the decision relation becomes: 
n
n
Δ
Δ
n
N
1
i
i
2ı
N
A
lnK
N
A
ı
N
ı
r
0
1
+
>
<
∑
=
                            (C.24.b) 
If we note: 
ȝ
2ı
N
A
lnK
N
A
ı
n
n
=
+
                                   (C.27) 
 
 

C.3   Signal Detection in Data Processing (K = 1) 
445
 
the decision relation (C.24.b) is: 
ȝ
y
Δ
Δ
0
1
>
<
                                                (C.24.c) 
Under the two assumptions 
0
H  and 
1
H , the pdfs of y are: 
2
y
2
1
0
e
2
1
)
p(y/s
−
=
π
                                    (C.28.a) 
2
n
)
ı
N
A
(y
2
1
1
e
2π
1
)
p(y/s
−
−
=
                            (C.28.b) 
graphically represented in Fig. C.7. 
 
Fig. C.7 Graphical representation of pdfs of decision variables for unipolar decision and 
discrete observation. 
( )
∫
=
=
∞
ȝ
0
f
ȝ
Q
)dy
p(y/s
P
                                    (C.29) 
)
ı
N
A
Q(ȝ
1
)dy
p(y/s
P
ȝ
n
1
m
∫
−
−
=
=
∞
−
                          (C.30) 
It is a particular value 
0
ȝ  for which 
m
f
P
P =
: 
)
ı
N
A
Q(ȝ
1
)
Q(ȝ
n
0
0
−
−
=
 
 

446 
Appendix C: Signal Detection Elements
 
It follows that: 
n
0
ı
N
A
2
1
ȝ
=
                                             (C.31) 
and according to (C.27), 
0
ȝ  is obtained if K=1, which means that 
2
1
P
P
1
0
=
=
 
and: 
)
ı
N
A
2
1
Q(
P
n
E =
                                        (C.32) 
or 
)
ξN
2
1
Q(
PE =
                                         (C.33) 
where ξ designates the SNR: 
2
n
2
2
n
2
n
s
ı
A
2
1
R
ı
2R
A
P
P
ξ
SNR
=
=
=
=
                                (C.34) 
It follows that the minimum required SNR for 
5
E
10
P
−
=
 for N=1 (the required 
value in PCM systems, see 3.3.5) is approximately 15dB (15,6 dB), which is the 
threshold value of the required input SNR: ξi0 which separate the regions of deci-
sion noise to that one of quantisation noise- Fig. 3.8. 
C.3.2   Discrete Detection of Polar Signal 
Hypotheses:  
 
• Polar signal in baseband: 
A
- 
 
B
 
A,
 
 
B
 
A,
(t)
s 
B,
(t)
s
1
0
=
<
=
=
 
• AWGN: 
)
ı
N(0,
2
n  
• T- observation time = bit duration 
• Discrete observation with N samples per T 
• 
0
C
C
11
00
=
=
, 
1
C
C
10
01
=
=
 
Following the steps similar to those from C.3.1, we obtain: 
a.  
⎥
⎦
⎤
⎢
⎣
⎡∑
∑
−
−
−
−
=
=
=
N
1
i
N
1
i
2
i
2
i
2
n
B)
(r
A)
(r
2ı
1
e
)r
ȁ(G
                               (C.35) 

C.3   Signal Detection in Data Processing (K = 1) 
447
 
b. 
( )
lnK
r
ln
0
1
Δ
Δ
>
<
Λ
 
2
B)
N(A
lnK
B
A
ı
r
2
n
N
1
i
i
0
1
+
+
−
>
<
∑
Δ
Δ
=
                                (C.36) 
For polar signal: B = -A, and K=1, the threshold of the comparator is 
0
K =
′
; if 
N=1, the comparator will decide ‘1’ for positive samples and ‘0’ for negative 
ones. 
c. In order to calculate the quality parameters: 
E
P , a variable change for normali-
zation of the decision variable is done: 











	


	

ȝ
n
n
y
n
N
1
i
i
2ı
N
B)
(A
lnK
N
B)
(A
ı
N
ı
r
0
1
+
+
−
>
<
∑
Δ
Δ
=
                            (C.37) 
The decision variable pdf under the two hypotheses is: 
2
n
)
ı
N
B
(y
2
1
0
e
2π
1
)
p(y/s
−
−
=
                                   (C.38.a) 
2
n
)
ı
N
A
(y
2
1
1
e
2π
1
)
p(y/s
−
−
=
                                  (C.38.b) 
The threshold 
0
μ  for which 
m
f
P
P =
 is: 
n
0
2ı
N
B)
(A
ȝ
+
=
                                             (C.39) 
which implies K=1 (
2
1
P
P
1
0
=
=
). 
If B = -A, the polar case, 
)
ξN
Q(
)
ı
N
A
Q(
P
n
E
=
=
                                     (C.40) 
Compared with the unipolar case, relation (C.33), we may notice that the same 
BER (
E
P ) is obtained in the polar case with 3dB less SNR. 

448 
Appendix C: Signal Detection Elements
 
C.3.3   Continuous Detection of Known Signal 
Hypotheses: 
 
• 
⎪⎩
⎪⎨
⎧
∫
=
=
=
T
0
2
1
0
(t)dt
s
E
energy 
 
finite
 
of
 
s(t),
(t)
s
0
(t)
s
 
• T- observation time 
• continuous observation: 
(
)
∞
→
=
N
1
r
,...,
r
rG
 
• AWGN: 
)
ı
N(0,
n(t)
2
n
∈
,
n(t)
(t)
s
r(t)
i
+
=
 
a. Calculation of 
)
p(r/s
)
p(r/s
ȁ(r)
0
1
=
 
Continuous observation means Nĺ∞. We shall express the received signal r(t) 
as a series of orthogonal functions 
)
(t
vi
 (Karhunen-Loeve expansion [2]) in such 
a way that the decision could be taken using only one function (coordinate), mean-
ing that it represents the sufficient statistics. 
∑
=
=
∞
→
N
1
i
i
i
N
(t)
v
r
lim
r(t)
                                         (C.41) 
The functions 
(t)
vi
 are chosen to represent an orthonormal (orthogonal and 
normalised) system. 
∫
⎩
⎨
⎧
≠
=
=
T
0
j
i
j
i 
if
 
0,
j
i 
if
 
1,
(t)dt
(t)v
v
                                    (C.42) 
The coefficients ir  are given by: 
(t)dt
r(t)v
:
r
T
0
i
i
∫
=
                                            (C.43) 
and represent the coordinates of r(t) on the observation interval [0,T]. In order to 
have 
(t)
v1
 as sufficient statistics, we chose: 
E
s(t)
(t)
v1
=
                                               (C.44) 
and 1r  is: 
∫
=
∫
=
T
0
T
0
1
1
r(t)s(t)dt
E
1
(t)dt
r(t)v
r
                               (C.45) 

C.3   Signal Detection in Data Processing (K = 1) 
449
 
We show that higher order coefficients: ir with i > 1, do not affect the likeli-
hood ration: 
)
/s
p(r
)
/s
p(r
)
/s
p(r
)
/s
p(r
)
/s
r
p(
)
/s
r
p(
)r
ȁ(
0
1
1
1
N
1
i
0
i
N
1
i
1
i
0
1
=
∏
∏
=
=
∞
→
=
∞
→
=
G
G
G
                            (C.46) 
the contribution of higher order coefficients being equal in the likelihood ratio: 
 
     
∫
=
T
0
i
0
i
(t)dt
n(t)v
/s
r
 
0
i
T
0
i
T
0
i
T
0
i
T
0
i
1
i
/s
r
(t)dt
n(t)v
         
          
          
          
(t)dt
n(t)v
(t)dt
s(t)v
(t)dt
n(t)]v
[s(t)
/s
r
=
∫
=
=
∫
+
∫
=
∫
+
=
               (C.47) 
because ∫
∫
=
=
T
0
T
0
i
1
i
0
(t)dt
(t)v
v
E
(t)
s(t)v
, based on the orthogonality of 
(t)
v1
 and 
(t)
vi
. 
Then, 
E
s(t)
(t)
v1
=
is a sufficient statistics. 
)
/s
p(r
)
/s
p(r
)r
ȁ(
0
1
1
1
=
G
 
n(t)
n(t)
(t)
s
r(t)
  :
H
0
0
=
+
=
 
∫
=
∫
=
=
T
0
T
0
1
0
1
Ȗ
n(t)s(t)dt
E
1
(t)dt
n(t)v
/s
r
                          (C.48) 
and has a normal pdf. 
The average value of 
0
/s
r
:
/s
r
0
1
0
1
=
 based on 
)
ı
N(0,
n(t)
2
n
∈
 and 
T
ı
TE
ı
E
1
]
n(t)s(t)dt
E
1
[
ı
]
/s
[r
ı
2
n
2
n
T
0
2
0
1
2
=
∫
=
=
 
It follows that: 
2
1
2n
r
T
2ı
1
2
n
0
1
e
T
2
1
)
/s
p(r
−
=
πσ
                                 (C.49) 
 
 

450 
Appendix C: Signal Detection Elements
 
n(t)
s(t)
n(t)
(t)
s
r(t)
  :
H
1
1
+
=
+
=
 
Ȗ
E
n(t)s(t)
E
1
(t)dt
s
E
1
        
t
n(t)]s(t)d
[s(t)
E
1
/s
r
T
0
T
0
2
T
0
1
1
+
=
∫
+
∫
=
∫
=
+
=
                          (C.50) 
(
)2
1
2n
E
r
T
2ı
1
2
n
1
1
e
T
2
1
)
/s
p(r
−
−
=
πσ
                                 (C.51) 
)
r
E
r
E
2
(r
T
2ı
1
2
1
1
2
1
2
n
e
)r
ȁ(
−
+
−
−
=
G
 
b. The decision criterion is: 
( )
lnK
r
ln
Δ
Δ
0
1
>
<
Λ
 
2
E
lnK
E
T
ı
r
2
n
Δ
Δ
1
0
1
+
>
<
                                           (C.52) 
If 1r  is replaced with (C.45), the decision relation becomes: 
2
E
TlnK
ı
r(t)s(t)dt
2
n
Δ
Δ
T
0
0
1
+
>
<
∫
                                (C.53) 
where
K
2
E
TlnK
ı2
n
′
=
+
. 
The block- scheme of the optimal receiver can be implemented in two ways: 
correlator-base (Fig. C.8.a), or matched filter-based (Fig. C.8.b). 

C.3   Signal Detection in Data Processing (K = 1) 
451
 
 
 
Fig. C.8 Block Scheme of an optimal receiver with continuous observation decision for one 
known signal s(t): a) correlator based implementation; b) matched filter implementation 
c. Decision relation is (C.52). Making a variable change to obtain unitary disper-
sion, we get: 
T
ı
E
2
1
lnK
E
T
ı
T
ı
r
2
n
2
n
Δ
Δ
2
n
1
0
1
+
>
<
                                 (C.54) 
Using the notations: 
T
ı
r
z
2
n
1
=
                                               (C.55) 
T
ı
E
2
1
lnK
E
T
ı
ȝ
2
n
2
n
+
=
                                      (C.56) 
the pdfs of the new variable z are: 
2
z
2
1
0
e
2π
1
)
p(z/s
−
=
                                        (C.57) 
2
2n
)
T
ı
E
(z
2
1
1
e
2π
1
)
p(z/s
−
−
=
                                 (C.58) 
which are represented in Fig. C.9. 

452 
Appendix C: Signal Detection Elements
 
 
Fig. C.9 Graphical representation of 
)
p(z/s0 and 
)
p(z/s1 . 
The probabilities occurring after decision are: 
( )
∫
−
=
=
∞
−
ȝ
0
0
0
ȝ
Q
1
)dz
p(z/s
)
/s
P(D
                              (C.59.a) 
∫
−
=
=
=
∞
ȝ
2
n
1
D
1
1
)
T
ı
E
Q(ȝ
)dz
p(z/s
P
)
/s
P(D
                       (C.59.b) 
∫
−
−
=
=
=
∞
−
ȝ
2
n
1
m
1
0
)
T
ı
E
Q(ȝ
1
)dz
p(z/s
P
)
/s
P(D
                 (C.59.c) 
( )
∫
=
=
=
∞
ȝ
0
f
0
1
ȝ
Q
)dz
p(z/s
P
)
/s
P(D
                            (C.59.d) 
The particular value 
0
ȝ  for which 
m
f
P
P =
 is: 
T
ı
E
2
1
ȝ
2
n
0 =
                                               (C.60) 
and according to (C.56) is obtained for K=1. 
In this case, K=1, the bit error rate is: 
)
T
ı
E
2
1
Q(
P
2
n
E =
                                          (C.61) 
 
 
 

C.3   Signal Detection in Data Processing (K = 1) 
453
 
which can be expressed also as a function of the ratio 
0
b N
E
 
⎪⎪
⎩
⎪⎪
⎨
⎧
=
=
=
=
2
N
T
2T
1
N
BT
N
T
ı
2
E
E
0
0
0
2
n
b
 
)
N
E
Q(
)
N
2
2
1
2
E
Q(
P
0
b
0
E
=
⋅
⋅
=
                              (C.62) 
It follows that the required 
0
b N
E
for 
5
10− BER is 12.6dB, with 3dB less 
than the required ξ in the discrete observation with 1 sample per bit. 
C.3.4   Continuous Detection of Two Known Signals 
Hypotheses: 
 
• 
⎪
⎪
⎩
⎪
⎪
⎨
⎧
∫
=
∫
=
T
0
2
1
1
1
T
0
2
0
0
0
(t)dt
s
E
energy 
 
finite
 
of
 
(t)
s
(t)dt
s
E
energy 
 
finite
 
of
 
(t)
s
 
• T- observation time 
• continuous observation : 
(
)
∞
→
=
N
1
r
,...,
r
rG
 
• AWGN: 
)
ı
N(0,
n(t)
2
n
∈
,
n(t)
(t)
s
r(t)
i
+
=
 
a.  
)
/s
r
p(
)
/s
r
p(
)r
ȁ(
0
1
G
G
G =
 
∑
=
=
∞
→
N
1
i
i
i
N
(t)
v
r
lim
r(t)
 
If the first two functions: 
(t)
v1
 and 
(t)
v2
 are properly chosen, 
)r
ȁ(G can be ex-
pressed only by the coordinates 1r  and 2r , which represent the sufficient statistics. 
(t)
s
E
1
(t)
v
1
1
1
=
                                           (C.63) 
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
−
−
=
1
1
0
0
2
2
E
(t)
s
ρ
E
(t)
s
ρ
1
1
(t)
v
                                (C.64) 
where ρ is the correlation coefficient. 
∫
=
T
0
1
0
1
0
(t)dt
(t)s
s
E
E
1
:
ρ
                                     (C.65) 

454 
Appendix C: Signal Detection Elements
 
Easily can be checked that: 
1
(t)dt
v
T
0
2
1
=
∫
                                            (C.66.a) 
1
(t)dt
v
T
0
2
2
=
∫
                                           (C.66.b) 
0
(t)dt
(t)v
v
T
0
2
1
=
∫
                                        (C.66.c) 
Higher order functions
(t)
vi
, with i > 2, if orthogonal can be any; it means that 
the coordinates ir , with i > 2 do not depend on the hypotheses 
0
H , 
1
H  and then 
1r and 2r  are the sufficient statistics. 
∫
=
∫
+
=
T
0
i
T
0
i
0
0
i
0
(t)dt
n(t)v
(t)dt
n(t)]v
(t)
[s
/s
r
:  
H
                     (C.67.a) 
∫
=
∫
+
=
T
0
i
T
0
i
1
1
i
1
(t)dt
n(t)v
(t)dt
n(t)]v
(t)
[s
/s
r
:  
H
                     (C.67.b) 
Consequently, the likelihood ratio is: 
)
/s
)p(r
/s
p(r
)
/s
)p(r
/s
p(r
)r
ȁ(
0
2
0
1
1
2
1
1
=
G
                                      (C.68) 
The coordinates 1r and 2r  are: 
∫
=
T
0
1
1
1
(t)dt
r(t)s
E
1
r
                                        (C.69) 
]
(t)dt
r(t)s
E
ρ
(t)dt
r(t)s
E
1
[
ρ
1
1
(t)
r
T
0
1
1
T
0
0
0
2
2
∫
−
∫
−
=
                  (C.70) 
Under the two hypotheses, we have: 
1
0
T
0
1
0
1
0
1
ρ
ρ
E
(t)dt
n(t)]s
(t)
[s
E
1
/s
r
+
=
∫
+
=
                     (C.71) 
with 
∫
=
T
0
1
1
1
(t)dt
n(t)s
E
1
ρ
                                       (C.72) 

C.3   Signal Detection in Data Processing (K = 1) 
455
 
1
1
T
0
1
1
1
1
1
ρ
E
(t)dt
n(t)]s
(t)
[s
E
1
/s
r
+
=
∫
+
=
                        (C.73) 
2
1
0
2
0
T
0
1
0
1
T
0
0
0
0
2
0
2
ρ
1
ρρ
ρ
ρ
1
E
(t)dt
n(t)]s
(t)
[s
E
ρ
(t)dt
n(t)]s
(t)
[s
E
1
ρ
1
1
/s
r
−
−
+
−
=
=
⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧
∫
+
−
∫
+
−
=
(C.74) 
where  
∫
=
T
0
0
0
0
(t)dt
n(t)s
E
1
ρ
                                         (C.75) 
2
1
0
T
0
1
1
1
T
0
0
1
0
2
1
2
ρ
1
ρρ
ρ
(t)dt
n(t)]s
(t)
[s
E
ρ
          
(t)dt
n(t)]s
(t)
[s
E
1
ρ
1
1
/s
r
−
−
=
⎪⎭
⎪⎬
⎫
∫
+
−
⎪⎩
⎪⎨
⎧
−
∫
+
−
=
                        (C.76) 
Under 
the 
assumption 
of 
noise 
absence: 
n(t)=0, 
(
ρ
E
/s
r
0
0
1
=
, 
)
ρ
1
E
/s
r
2
0
0
2
−
=
 are the coordinate of point 
0
M , and (
1
1
1
E
/s
r
=
, 
0
/s
r
1
2
=
) the coordinate of point 
1
M , represented in the space ( 1r , 2r ), Fig. C.10. 
 
Fig. C.10 Observation space in dimensions (r1, r2). 

456 
Appendix C: Signal Detection Elements
 
The separation line between 
0
Δ  and 
1
Δ  is the line 
)
( x
x ′  orthogonal on 
1
0M
M
. If we rotate the coordinates such that l be parallel with 
1
0M
M
, the in-
formation necessary to take the decision is contained only in the coordinate l 
which plays the role of sufficient statistics. Assume that the received vector is the 
point R ( 1r , 2r ). 
sinα
r
cosα
r
l
2
1
−
=
                                          (C.77) 
which is a Gaussian, based on the Gaussianity of 1r  and 2r . 
0
1
0
1
0
1
2
0
1
2
2
0
0
1
E
E
E
2ρ
E
E
ρ
E
        
)
E
ρ
E
(
)
ρ
1
E
(
E
ρ
E
cosα
+
−
−
=
=
−
+
−
−
=
                        (C.78) 
0
1
0
1
2
0
E
E
E
2ρ
E
ρ
1
E
sinα
+
−
−
=
                                  (C.79) 
If we introduce the notation: 
T
ı
l
z
2
n
=
                                                 (C.80) 
and l is replaced with (C.77), (C.78) and (C.79), we obtain: 
⎥
⎦
⎤
⎢
⎣
⎡
∫
−
∫
+
−
=
T
0
0
T
0
1
1
1
0
0
2
n
(t)dt
r(t)s
(t)dt
r(t)s
E
E
E
2
E
T
ı
1
z
           (C.81) 
The likelihood ratio can be written as a function of z, which plays the role of 
sufficient statistics. 
2
2
n
0
2
2
n
1
)
T
ı
a
(z
2
1
)
T
ı
a
(z
2
1
e
2π
1
e
2π
1
ȁ(z)
−
−
−
−
=
                                   (C.82) 
with 
0
1
0
1
1
0
1
1
1
E
E
E
2ρ
E
E
E
ρ
E
s
l
a
+
−
−
=
=
                              (C.83) 

C.3   Signal Detection in Data Processing (K = 1) 
457
 
0
1
0
1
1
0
0
0
0
E
E
E
2ρ
E
E
E
ρ
E
s
l
a
+
−
−
−
=
=
                             (C.84) 
b. The decision criterion, applied to the logarithmic relation, gives 
T
ı
a
a
2
1
lnK
a
a
T
ı
z
2
n
0
1
0
1
2
n
Δ
Δ
0
1
+
+
−
>
<
                                  (C.85) 
If we note: 
T
ı
a
a
2
1
lnK
a
a
T
ı
ȝ
2
n
0
1
0
1
2
n
+
+
−
=
                                  (C.86) 
The decision relation is: 
ȝ
z >
<
                                                  (C.85.a) 
which can be written, based on (C.81): 
2
E
E
TlnK
ı
(t)dt
r(t)s
(t)dt
r(t)s
0
1
2
n
T
0
Δ
Δ
0
T
0
1
0
1
−
+
∫
>
<
−
∫
                    (C.87) 
and represents the decision relation. 
K
2
E
E
TlnK
ı
0
1
2
n
′
=
−
+
                                   (C.88) 
represents the threshold of the comparator. 
The implementation of the decision relation (C.87) gives the block - scheme of 
the optimal receiver (Fig. C.11). 
 
Fig. C.11 Block- scheme of an optimal receiver for continuous decision with two known 
signal (correlator based implementation). 

458 
Appendix C: Signal Detection Elements
 
As presented in Fig. C.8, the correlator can be replaced with matched filters 
(Fig. C.8.b) 
 
c. The decision variable z, under the two hypotheses is represented in Fig. C.12 
 
Fig. C.12 Representation of the decision process in continuous detection of two known  
signals 
The distance between the maxima of 
)
p(z/s0  and 
)
p(z/s1  is: 
T
ı
E
E
E
2ρ
E
T
ı
a
T
ı
a
Ȗ
2
n
0
1
0
1
2
n
0
2
n
1
+
−
=
−
=
                         (C.89) 
fP  and 
m
P  are decreasing, which means
E
P  is decreasing, if Ȗ is greater. The 
greatest Ȗ, for 
ct
E
E
1
0
=
+
, is obtained when 
1
ρ
−
=
and  
E
E
E
1
0
=
+
, respec-
tively when the performance of the optimal receiver depends only on its energy. 
(t)
s 
(t)
s
1
0
−
=
                                              (C.90) 
We can notice that the shape of the signal has no importance, the performance 
of the optimal receiver depending on its energy. 
In this case  
E
a
a
0
1
=
−
=
                                           (C.91) 
0
ȝ , the value of the threshold corresponding to 
m
f
P
P =
 is: 
T
ı
2
a
a
ȝ
2
n
0
1
0
+
=
                                            (C.92) 

References 
459
 
and, based on (C.86), is obtained when K=1; it follows that: 
)
T
ı
2
a
a
Q(
P
2
n
0
1
E
+
=
                                           (C.93) 
In the particular case (C.90) 
⎪
⎪
⎩
⎪
⎪
⎨
⎧
=
=
−
=
−
=
=
=
1 
K 
 
E
a 
a
 1
ρ
 
E
E
E
0
1
1
0
 
the bit error rate is: 
)
N
E
2
Q(
)
T
ı
E
Q(
P
0
b
2
n
E
=
=
                                (C.94) 
and show that the same BER is obtained with 3dB less 
0
b N
E
than in the case of 
one signal (0, s(t)) - see relation (C.62). 
References 
[1] Kay, S.M.: Fundamentals of Statistical Signal Processing, Detection Theory. Prentice-
Hall, Englewood Cliffs (1998) 
[2] Papoulis, A.: Probability, Random Variables and Stochastic Processes, 3rd edn. 
McGraw-Hill Companies, New York (1991) 
[3] Sklar, B.: Digital Comunications. Prentice-Hall, Englewood Cliffs (1988) 
[4] Spataru, A.: Fundaments de la théorie de la transmission de l’information. Presse Poly-
thechnique Romandes, Lausanne (1987) 
[5] Stanley, W.D.: Electronic Communications Systems. Prentice-Hall, Englewood Cliffs 
(1982) 
[6] Van Trees, H.L.: Detection, Estimation and Modulation Theory, Part 1. J. Willey, New 
York (1968) 
[7] Xiong, F.: Digital Modulation Techniques. Artch House, Boston (2000) 

Appendix D: Synthesis Example 
We think that an example, trying to synthesize the main processings exposed in 
the present book: source modelling, compression and error protection, is helpful 
for those who already acquired the basics of information theory and coding and 
also for the very beginners, in order to understand the logic of processing in any 
communication/ storage system. Such examples are also suitable for examination 
from the topic above. 
The message VENI_VIDI_VIVI is compressed using a binary optimal lossless 
algorithm. 
 
1. Calculate the efficiency parameters of the compression and find the binary 
stream at the output of the compression block. 
2. Which are the quantity of information corresponding to letter V, the quantity of 
information per letter and the information of the whole message? 
3. Which is the quantity of information corresponding to a zero, respectively a one 
of the encoded message? Show the fulfilment of lossless compression relation. 
The binary stream from 1, assumed to have 64kbps, is transmitted through a 
BSC with 
2
10
p
−
=
. 
4. Determine the channel efficiency and the required bandwidth in transmission. 
Assume that before transmission/storage, the binary stream from 1 is error pro-
tected using different codes. Find the first non-zero codeword and the required 
storage capacity of the encoded stream. The error-protection codes are: 
5. Hamming group with m = 4 (information block length) of all three varieties 
(perfect, extended and shortened). 
6. Cyclic, one error correcting code, with m=4, using LFSR. 
7. BCH with n = 15 and t = 2 (number of correctable errors). 
8. RS with n = 7 and t = 2. 
9. Convolutional non-systematic code with R=1/2 and K=3. 
All the answers need to be argued with hypothesis of the theoretical develop-
ment and comments are suitable. 
 

462 
Appendix D: Synthesis Example
 
Solution 
A block scheme of the presented processing is given in Fig. D.1. 
 
Fig. D.1 Block-scheme of processing where:  
– 
S represents the message (its statistical model) 
– 
S
C - compression block 
– 
C
C - channel coding block (error control coding) 
1. The message VENI_VIDI_VICI, under the assumption of being memoryless 
(not true for a language) is modelled by the PMF: 
⎟
⎟
⎠
⎞
⎜
⎜
⎝
⎛
14
1
14
1
14
2
14
5
14
1
14
1
14
3
C
D
_
I
N
E
V
:
S
 
For compression is chosen the binary Huffman static algorithm which ful-
fils the requirements: lossless, optimal, binary and PMF known (previously 
determined). 
As described in 3.7.2 (Remarks concerning Huffman algorithm) the obtained 
codes are not unique, meaning that distinct codes could be obtained, but all  
ensure the same efficiency ( l ). 
In what follows two codes are presented, obtained using the same S and  
algorithm. 
 

Appendix D: Synthesis Example 
463
 
 
 
2.57
14
36
14
16
6
9
5
4
14
1
4
3
14
2
3
14
3
14
5
l
p
l
7
1
i
i
i
a
=
=
+
+
+
=
⋅
⋅
+
⋅
+
⋅
+
=
∑
=
=
 
2.57
14
36
4
14
1
2
14
2
2
14
3
2
14
5
lb
=
=
⋅
+
⋅
+
⋅
+
⋅
=
 
 
The output of the compression block (
S
C ), using (a) code is: 
 
000 0110 0111 1 001 000 1 0100 1 001 000 1 0111 1 
  V     E      N    I   _     V   I    D    I   _     V   I   C     I 
 
Efficiency parameters: coding efficiency η and compression ratio
C
R , given by 
(3.46), respectively (3.49) are: 
(97.7%)
0.968
2.57
2.49
2.57
H(S)
l
l
η
min
≅
=
=
=
 
H(S), source entropy, is given by (2.12): 
2.80
7
log
D(S)
(S)
H
2.49
p
log
p
H(S)
2
max
i
2
7
1
i
i
=
=
=
<
=
∑
−
=
=
 
Remark: Always is good to check the calculus and to compare to limits that are 
known and easy to compute. 
l
l
R
u
C =
, where ul  is the length in uniform encoding and is obtained using 
(3.58.a) 
 

464 
Appendix D: Synthesis Example
 
3
1
2.80
2
log
7
log
m
log
M
log
l
2
2
2
2
u
≈
=
=
=
 (the first superior integer) 
1.2
2.57
3
RC
≈
=
 
2. Under the assumption of a memoryless source, we have: 
• the 
self 
information 
of 
letter 
V, 
according 
to 
(2.11) 
is: 
bits
 
2.2
14
3
log
p(V)
log
i(V)
2
2
≈
−
=
−
=
 
• the average quantity of information  per letter is H(S)=2.49 bits/letter 
• the information of the whole message (14 letters) can be obtained in several 
ways; the simplest way is to multiply the number of letters (N=14) of the mes-
sage with the average quantity of information per letter (H(S)), using the aditiv-
ity property of the information. 
bits
 
34.86
rs
bits/lette
 
2.49
*
letters
 
14
H(S)
N
IM
=
=
×
=
 
Another possibility, longer as calculus, is to calculate the self information of 
each letter and then to multiply with the number of occurrence in the message. 
The result will be the same, or very close (small differences occur because of the 
logarithm calculus, on the rounding we do). The reader is invited to check it. 
 
3. By compression the message (source S) is transformed in a secondary binary 
source (X). Under the assumption that this new source is also memoryless, we 
can model it statistically with the PMF: 
 
1
p(1)
p(0)
 ,
p(1)
p(0)
1
0
X
=
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
=
 
1
0
0
N
N
N
p(0)
+
=
 
1
0
1
N
N
N
p(1)
+
=
 
where 
0
N , respectively 
1
N  represent the number of “0”s, respectively “1”s in the 
encoded sequence. Counting on the stream determined at 1, we have: 
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
⇒
⎪
⎭
⎪
⎬
⎫
≅
=
≅
=
0.45
0.55
1
0
:
X
0.45
36
16
p(1)
0.55
36
20
p(0)
 
• 
0.5
p(1)
p(0)
=
≈
 the condition of statistical adaptation of the source to the 
channel obtained by encoding is only approximately obtained; the encoding al-
gorithm, an optimal one, is introducing, by its rules, a slight memory. 
 

Appendix D: Synthesis Example 
465
 
• the information corresponding to a zero is: 
bits
1.86
0.55
log
p(0)
log
i(0)
2
2
=
−
=
−
=
 
• based on the same reason, as in 2, the information of the whole encoded mes-
sage is: 
)H(X)
N
(N
I
1
0
Me
+
=
 
simbol
y 
bits/binar
 
0.9928
0.5184
0.4744
0.45
0.45log
0.55
0.55log
p(1)
p(1)log
p(0)
p(0)log
H(X)
2
2
2
2
=
+
=
=
−
−
=
−
−
=
 
bits
 
35.64
0.99
*
36
IMe
=
=
 
 
Remarks 
The equality (approximation in calculus give basically the difference between them) 
between 
35.64
I
I
35.14
Me
M
=
≈
=
 shows the conservation of the entropy in loss-
less compression. The same condition can be expressed using relation (3.47): 
 
H(X)
l
H(S) =
 
2.54
0.99
2.57
2.49
=
⋅
=
 
4. Channel efficiency is expressed using (2.66): 
C
Y)
I(X;
ηC =
 
where the transinformation I(X;Y), can be obtained using (2.58): 
H(Y/X)
H(Y)
Y)
I(X;
−
=
 
Taking into account that the average error H(Y/X) for a BSC was calculated in 
(2.70): 
l
bits/symbo
 
0.0822
0.99)
0.99log
0.01
(0.01log
  
          
p)
(1
p)log
(1
p
plog
H(Y/X)
2
2
2
2
=
+
−
=
=
−
−
−
−
=
 
H(Y) requires the knowledge of PMF. It can be obtained in more ways (see 2.8.1). 
The simplest in this case is using (2.31): 
[
]
[
]
[
]
0.451
0.549
0.99
0.01
0.01
0.99
 
0.45
0.55
        
p
1
p
p
p
1
 
p(1)
p(0)
P(X)P(Y/X)
P(Y)
=
⎥⎦
⎤
⎢⎣
⎡
=
=
⎥⎦
⎤
⎢⎣
⎡
−
−
=
=
 
0.9931
0.5181
0.4750
0.451
0.451log
0.549
0.549log
H(Y)
2
2
=
+
=
−
−
=
 
It follows that 
0.9109
0.0822
0.9931
H(Y/X)
H(Y)
Y)
I(X;
=
−
=
−
=
 

466 
Appendix D: Synthesis Example
 
Capacity of BSC is given by (2.71): 
l
bits/symbo
 
0.9178
H(Y/X)
1
         
p)
(1
p)log
(1
p
plog
1
C
2
2
BSC
=
−
=
−
−
+
+
=
 
Now the channel efficiency can be obtained: 
0.993
0.9178
0.9109
C
Y)
I(X;
ηC
≈
=
=
 
The required bandwidth in transmission assuming base-band transmission, de-
pends of the type of the code is used. In principle there are two main types, con-
cerning BB coding (NRZ- and RZ- see 5.12) 
Using the relation (2.28), real channels, we have: 
⎩
⎨
⎧
−
−
=
⎪⎩
⎪⎨
⎧
=
≅
coding)
 
BB
RZ
(for 
 
102.4Khz
coding)
 
BB
NRZ
(for 
 
51.2Khz
   
RZ)
(for 
 
10
*
64
*
0.8
*
2
NRZ)
(for 
 
10
*
64
*
0.8
0.8M
B
3
3
 
For channel encoding, the binary information from 1 is processed according to 
the type of the code. 
101111
0011001000
1001000101
0001100111
0
MSB
 
5. Encoding being with Hamming group code with m=4, the whole input stream 
(N=36 bits) is split in blocks of length m=4, with the MSB first in the left. If 
necessary, padding (supplementary bits with 0 values) is used. 
In our case: 
9
4
36
m
N
NH
=
=
=
 codewords (no need for padding). 
• Perfect Hamming with m=4 is given by (5.71), (5.72), (5.73) and (5.74). 
1
2
k
m
1
2
n
k
k
−
=
+
=
−
=
 where n = m + k. 
For m=4 it follows that k=3 and n=7. The codeword structure is: 
[
]
7
6
5
4
3
2
1
a
a
a
c
a
c
c
v =
 
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
1
0
1
0
1
0
1
1
1
0
0
1
1
0
1
1
1
1
0
0
0
H
 
The encoding relations, according to (5.38) are: 
7
5
3
1
7
6
3
2
7
6
5
4
T
a
a
a
c
a
a
a
c
a
a
a
c
0
Hv
⊕
⊕
=
⊕
⊕
=
⊕
⊕
=
⇒
=
 

Appendix D: Synthesis Example 
467
 
The first codeword, corresponding to the first 4 bits block, starting from the 
right to left: 
]
a
a
a
[a
1]
1
1
[1
i
3
5
6
7
=
=
 
[
]1
1
1
1
1
1
1
v =
 
The required capacity to store the encoded stream is: 
bits
 
63
7
9
n
N
C
H
H
=
×
=
×
=
 
• Extended Hamming for m=4 is: 
[
]
7
6
5
4
3
2
1
0
*
a
a
a
c
a
c
c
c
v =
 
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
1
1
1
1
1
1
1
1
1
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
1
1
1
1
0
0
0
0
H*
"
"
"
"
"
"
"
#
#
#
 
0
v
H
T
*
*
=
⋅
 is giving 
1
2
4
c
,
c,
c
as for the perfect code and 
1
a
a
a
c
a
c
c
c
7
6
5
4
3
2
1
0
=
⊕
⊕
⊕
⊕
⊕
⊕
=
, 
and thus, the first non zero extended codeword is: 
 
[
]1
1
1
1
1
1
1
1
v* =
 
The required capacity to store the encoded stream is: 
bits
 
72
8
9
n
N
C
*
H
*
H
=
×
=
×
=
 
• Shortened Hamming, with m=4 is (see Example 5.8), obtained starting from the 
perfect Hamming with n=15 and deleting the columns with even “1”s: 
⎥
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎢
⎣
⎡
=
x
x
x
x
x
x
x
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
H
 
⎥
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎢
⎣
⎡
=
0
1
1
0
1
0
0
1
1
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
1
1
1
1
0
0
0
0
HS
 
The codeword structure is: 
[
]
8
7
6
5
4
3
2
1
S
a
a
a
c
a
c
c
c
v =
 
 

468 
Appendix D: Synthesis Example
 
and the encoding relations, obtained from 
0
v
H
S
S
=
 
are: 
 
8
7
6
5
a
a
a
c
⊕
⊕
=
 
8
7
4
3
a
a
a
c
⊕
⊕
=
 
8
6
4
2
a
a
a
c
⊕
⊕
=
 
7
6
4
1
a
a
a
c
⊕
⊕
=
 
 
For the four information bits: 
1]
1
1
[1
]
a
a
a
[a
i
8
7
6
4
=
=
, the first codeword is: 
[
]1
1
1
1
1
1
1
1
vS =
 
The storage capacity of the encoded stream is: 
bits
 
72
8
9
n
N
C
S
H
HS
=
×
=
×
=
 
6. For a cyclic one error correcting code, meaning a BCH code of m=4, t=1, from 
Table. 5.8 we choose the generator polynomial: 
1
x
x
g(x)
011]
[001
3]
[1
g
3
+
+
=
→
=
=
 
The block scheme of the encoder, using LFSR is (see Fig. 5.13) given in  
Fig. D.2. 
 
Fig. D.2 Block scheme of cyclic encoder with LFSR with external modulo two sumators 
and g(x) = x3 + x + 1 
 
The structure of the codeword is: 
]
a
a
a
a
a
a
a[
c]
[i
v
3
k'
:c
0
1
2
4
m
:i
3
4
5
6

	


	

=
=
=
=
 

Appendix D: Synthesis Example 
469
 
For the first four information bits: i=[1 1 1 1] the operation of the LFSR is 
given in the following table: 
 
tn 
tn+1 
tn
Ck 
K 
i 
C2 
C1 
C0 
v 
1 
1 
1 
0 
0 
1 
2 
1 
1 
1 
0 
1 
3 
1 
0 
1 
1 
1 
4
1
1 
1 
0 
1 
1 
5 
 
0 
1 
0 
1 
6 
 
0 
0 
1 
1 
7
2
 
0 
0 
0 
1 
 
 
Concerning the number of codewords for the binary input from 1, this is the 
same as for Hamming codes, m being the same.  
The storage capacity is: 
bits
 
63
7
9
n
N
C
BCH1
BCH1
BCH1
=
×
=
×
=
 
7.  For BCH, n=15, t=2 we choose from Table. 5.8: g= [7 2 1]= [111010001] 
7
m
8
k'
1
x
x
x
x
g(x)
4
6
7
8
=
⇒
=
⇒
+
+
+
+
=
 
A systematic structure is obtained using the algorithm described by relation 
(5.98); we choose, as information block, the first m=7 bits, starting from right to 
left: 
 
] 1
  
1
  
1
  
1
  
0
  
1 
0
[
i
MSB
=
 
• 
1
x
x
x
x
i(x)
2
3
5
+
+
+
+
=
 
• 
8
9
10
11
13
2
3
5
8
k
x
x
x
x
x
1)
x
x
x
(x
x
i(x)
x
+
+
+
+
=
+
+
+
+
=
′
 
• 

 

 	








	

g(x)
i(x)
x
rem
2
3
5
q(x)
2
3
4
5
k
k
k'
1
x
x
x
1
x
x
x
x
g(x)
i(x)
x
rem
q(x)
g(x)
i(x)
x
+
+
+
+
+
+
+
+
=
+
=
′
′
 
• 
=
⋅
=
+
=
′
′
 
g(x)
q(x)
g(x)
i(x)
x
rem
i(x)
x
v(x)
k
k
 
1
x
x
x
x
x
x
x
x
        
2
3
5
8
9
10
11
13
+
+
+
+
+
+
+
+
=
 
In matrix expression: 
]
  
1
  
0
  
1
  
1
  
0
  
1
  
0
  
0  1
  
1
  
1
  
1
  
0
  
1 
0
[
v
MSB
#
=
. 
 
 

470 
Appendix D: Synthesis Example
 
The same result can be obtained using a LFSR with the characteristic polyno-
mial g(x). The reader is invited to check this way too. 
The number of codewords corresponding to the stream 1 for m=7 is: 
5.1
7
36
m
N
NBCH2
=
=
=
 meaning that padding is necessary to obtain 6 
codewords: the required padding bits of “0” are 6. 
The storage capacity is: 
bits
 
90
15
6
n
N
C
BCH2
BCH2
=
×
=
×
=
 
8. RS with n=7 and t=2 
The dimensioning of this code was given in Example 5.19. 
)
GF(2
3
k
7
1
2
n
3
k
⇒
=
⇒
=
−
=
 is the corresponding Galois field of this 
code and it is given in Appendix A.10. 
For t = 2 (the number of correctable symbols), the generator polynomial is: 
3
2
3
4
4
3
2
x
x
x
x
)
)(x
)(x
)(x
(x
g(x)
α
α
α
α
α
α
α
+
+
+
+
=
+
+
+
+
=
 
which means that 
4
k =
′
 and it represents the corresponding number of control 
characters. It follows that the number of information characters is: 
3
4
-
7
k
n
m
=
=
′
−
=
 
We notice that each character is expressed in k bits, k being the extension of the 
Galois field 
)
GF(2k , which is in our case 3. It means that for encoding we need 
blocks of: 
bits
 9
3
3
k
m
=
×
=
×
 
The first 9 bits corresponding to the binary stream from 1 are: 
] 1
  
1
  
1  1
  
0
  
1  0
  
0
  
0 [
MSC
#
#
↑
=
i
 
Using the table giving 
)
GF(23  from A.10 we identify:  
 
000 ĺ 0 
101 ĺ 
6
α  
111 ĺ 
5
α  
 
 
 

Appendix D: Synthesis Example 
471
 
Now we apply the algebraic encoding for systematic codes (5.98), with the ob-
servation that the calculus is in
)
GF(23 . 
• 
5
6x
(x)
α
α
i
+
=
 
• 
4
5
5
6
5
6
4
k
x
x
)
x
(
x
(x)
x
α
α
α
α
i
+
=
+
=
′
 
• 







	


	

(x)
(x)
x
rem
3
2
5
3
(x)
4
6
k
k
1
x
x
x
x
(x)
(x)
x
g
i
q
α
α
α
α
α
g
i
′
+
+
+
+
+
=
′
 
• 
1
α
α
α
α
α
g
i
i
+
+
+
+
+
=
+
=
′
x
x
x
x
x
(x)
(x)
x
rem
(x)
x
v(x)
3
2
5
3
4
5
5
6
k'
k
 
In a matrix representation, v is: 
( )
[
]
binary
in 
  -
  
 
1
 0
  
0  1
  
1
  
0  1
  
1
  
1  0
  
1
  
0  1
  
1
  
1  1
  
0
  
1  0
  
0
  
0 
 
   
decimal
in 
  -
  
1
4
6
2
6
7
0
 
   
2
GF
in 
  -
  ]
[
LSB
3
⎥⎦
⎤
⎢⎣
⎡
=
=
=
#
#
#
#
#
#
1
α
α
α
α
α
0
v
3
5
5
6
 
The number of codewords, corresponding to the binary stream 1 is: 
4
3
3
36
k
m
N
NRS
=
×
=
×
=
 (no necessary for padding) 
The required capacity for storage of the encoded stream 1 is: 
bits
 
84
3
7
4
k
n
N
C
RS
RS
RS
=
×
×
=
×
×
=
 
9. Non-systematic convolutional code with R=1/2 and K=3 
The dimensioning and encoding of the code is presented in 5.92. 
• 
2
n
2
1
R
=
⇒
=
: two generator polynomials are required, from which at least 
one need to be of degree K-1=2 
We choose: 
 
2
(1)
x
x
1
(x)
g
+
+
=
 
x
1
(x)
g(2)
+
=
 
 
• the information stream is the binary stream from 1 ending with four “0”s, 
which means that with trellis termination. 
• the simplest way to obtain the encoded stream is to use the SR implementation 
of the encoder (Fig. D.3) 

472 
Appendix D: Synthesis Example
 
 
Fig. D.3 Non-systematic convolutional encoder for R=1/2 and K=3 ( g(1)(x) = 1 + x + x2, 
g(2)(x) = 1 + x) 
The result obtained by encoding with the non-systematic convolutional encoder 
from Fig. D.3, for the information stream:  
⎥⎦
⎤
⎢⎣
⎡
=
101111
0011001000
1001000101
0001100111
0
i
MSB
 is shown in the table D1. 
The required capacity to store the encoded stream from 1 is: 
bits
 
72
2
36
n
N
Cconv
=
×
=
×
=
 
Remark 
In our example only the encoding was presented, but we assume that the reverse 
process, decoding, is easily understood by the reader. Many useful examples  
for each type of processing found in chapter 3 and 5 could guide the full  
understanding. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Appendix D: Synthesis Example 
473
 
Table D.1 Operation of the encoder from Fig. D.1 for the binary stream i (the output of the 
compression block) 
tn 
tn+1 
tn
V
Ck
i
C1
C2
u(1)
u(2)
1 
1 
1 
0 
1 
1 
2 
1 
1 
1 
0 
0 
3 
1 
1 
1 
1 
0 
4 
1 
1 
1 
1 
0 
5 
0 
0 
1 
0 
1 
6 
1 
1 
0 
0 
1 
7 
0 
0 
1 
1 
1 
8 
0 
0 
0 
1 
0 
9 
0 
0 
0 
0 
0 
10 
1 
1 
0 
1 
1 
11 
0 
0 
1 
1 
1 
12 
0 
0 
0 
1 
0 
13 
1 
1 
0 
1 
1 
14 
0 
0 
1 
1 
1 
15 
0 
0 
0 
1 
0 
16 
1 
1 
0 
1 
1 
17 
0 
0 
1 
1 
1 
18 
1 
1 
0 
0 
1 
19 
0 
0 
1 
1 
1 
20 
0 
0 
0 
1 
0 
21 
0 
0 
0 
0 
0 
22 
1 
1 
0 
1 
1 
23 
0 
0 
1 
1 
1 
24 
0 
0 
0 
1 
0 
25 
1 
1 
0 
1 
1 
26 
1 
1 
1 
0 
0 
27 
1 
1 
1 
1 
0 
28 
1 
1 
1 
1 
0 
29 
0 
0 
1 
0 
1 
30 
0 
0 
0 
1 
0 
31 
1 
1 
0 
1 
1 
32 
1 
1 
1 
0 
0 
33 
0 
1 
1 
0 
1 
34 
0 
0 
1 
0 
1 
35 
0 
0 
0 
1 
0 
36 
0 
0 
0 
0 
0 
 

Subject Index 
absolute optimal codes   81 
absolute redundancy   13, 218 
absorbent   42, 43 
absorbent chain   42 
acceptable loss   75 
access control   124 
active monitoring   182 
adaptive chosen plaintext attack   126 
addition circuit over GF(2k)   309 
ad-hoc compression   95 
AES (Advanced Encryption Standard)   147 
Alberti cipher   122, 132 
algebra   122, 256, 389,  
algebraic decoding for RS codes   298 
algorithm   85-96, 100-106, 123–126,  
147-150 
almost perfect codes   228-229 
ambiguity attacks   178 
AMI (Alternate Mark Inversion)   378 
amino-acids   77, 78, 79 
amplitude resolution   31, 32, 37 
anonymity   124 
arabian contribution   121 
arbitrated protocol   179, 183 
arithmetic coding algorithm   107 
ARQ - automatic repeat request    
213–215, 228, 267 
asymmetric (public-key)   125 
attack   123, 125–127, 129, 136, 151, 169, 
177–181, 194 
audiosteganography   167 
authentication   122, 124, 153, 158–160, 
164, 183-185 
authentication role   151 
authentication/ identification   124 
authorization   124 
automated monitoring   182 
average length of the codewords   80, 81,  
94, 99 
 
backward recursion   357, 363, 368-371,  
373 
base band codes / line codes   53, 383 
basic idea   85, 91-93, 95, 102, 107 
basic idea of differential PCM   109 
basic principle of digital communications    
65 
BCH codes (Bose–Chauduri– 
Hocquenghem)   261, 269, 271,  
296, 299, 420 
BCH encoding   262 
Berlekamp   256, 299 
Berlekamp – Massey algorithm   302 
bidirectional SOVA steps   358 
Binary N-Zero Substitution Codes   379 
Biphase codes   377, 378, 383 
Bipolar signalling   381, 382 
bit error rate (BER)   2, 36, 68, 72,  
176, 229, 230, 233, 236-238, 342, 348, 
374, 375, 381, 382, 438, 447, 453, 459 
bit or symbol synchronization   375 
bit synchronization capacity   377-379, 383 
B-L (Biphase Level-Manchester)   377 
block ciphers   137, 139, 147  
block codes   216, 219, 224, 228, 232, 253, 
312-315, 324, 325, 341, 357-358 
block interleaving   340-342 
B-M (Biphase Mark)   378 
branch metric   351-353, 359, 364 
broadcast monitoring   166, 181, 184 
brute force attack   127, 129 
brute force attack dimension   136 
B-S (Biphase Space)   378 
burst error   49, 50, 216, 292, 340, 341 
burst error length   50, 340 
burst-error channels   50 
 
Caesar cipher   121, 128-130 
cancellation   124 

476
Subject Index
 
canonical forms   225, 227 
capacity   23-25, 30, 32-38, 53, 55, 107, 
167, 218, 311, 461 
capacity boundary   35 
capacity formula   32 
CAST   149 
catastrophic   317 
catastrophic code   317 
central dogma of molecular biology   76,  
191 
certification   124, 180 
channel   1-4  
channel absolute redundancy   24 
channel capacity   23-26, 29, 30, 32, 36 
channel efficiency   24, 27, 461 
channel redundancy   24 
channel relative redundancy   24 
Chapman-Kolmogorov relations   41 
characteristic matrix   154, 275-277,  
280-282, 293 
chiffre indéchiffrable   134 
chosen ciphertext attack   126 
chosen plaintext attack   126 
chromosome   191, 194, 199-202 
cipher   121-123, 127-137, 147-151,  
160-162, 198 
cipher disk   122, 128, 132 
ciphertext   150, 151, 123, 125-137,  
197, 198, 200, 203 
ciphertext only attack   126 
classic cryptography   127 
CMI Coded Mark Inversion   377, 380,  
383 
code distance   219, 222, 233, 241-246, 
291, 313, 324, 325, 342 
code efficiency   90, 235, 375, 383 
code extension   250 
code generating matrix   224 
code shortening   251 
codes   53-55, 81-83 
codes for burst error   216, 340,  
codes for independent error control   216 
codeword   54, 55, 59, 75, 80-83 
codewords set   246, 248, 256  
coding   3, 48, 53 
coding efficiency   80, 81, 84, 85, 101, 463 
coding gain Gc   237, 333, 348 
 
coding rate   212, 313, 314, 347, 351, 370 
coding theory point of view   55, 81 
coding tree   55, 86, 91 
codon   77-79 
collusion by addition   181 
collusion by subtraction   181 
comma code   55, 100 
comparative analysis of line codes   382 
complexity and costs   375 
complexity of an attack   127 
compression   4, 5, 49, 53-56, 58-60,  
68, 69, 71, 72, 80, 81, 85, 86,  
88-93, 95-98, 99-101, 106, 107,  
109-111, 116, 117, 129, 137, 172,  
177, 213 
compression codes   53 
compression ratio   80, 81, 86, 90,  
101, 111 
conditional entropy (input conditioned by 
output) / equivocation   19 
conditional entropy (output conditioned by 
input) / average error   19 
confidentiality and authentication   160 
confidentiality/ secrecy/ privacy   124 
confirmation   124, 214 
confusion   135, 137, 138, 178 
constant weight code (m, n)   255 
constraint (M)   313 
constraint length (K)   313, 317, 323, 326, 
332, 334, 340, 341 
continuous (convolutional) codes   216 
continuous codes   314, 335 
control matrix   226, 230, 231, 239-242, 
245, 259, 264, 265, 278, 282, 320 
control symbols   211, 216, 224, 226,  
239, 258, 263, 264, 278, 291-293,  
314-316, 335-337 
convolutional codes   312-314, 317, 320, 
324, 325, 332-335, 340, 341, 346, 349 
convolutional interleaving   340-342 
copy protection   166, 168, 184 
copyright   165, 169-172, 178, 182,  
183, 189 
copyright notice   182 
copyright protection   165-167, 169, 189 
correlator   172, 175, 176, 185, 450, 451, 
457 
 

Subject Index 
477
 
coset leader   232-234 
CRC (Cyclic Redundancy Check)   107, 
267 
crest factor   70 
Cross parity check code   254 
cryptanalysis   122, 123, 126, 127, 129-131, 
134-139, 156 
cryptanalyst   123, 126, 151, 163 
cryptanalytic attack   125 
cryptographer   123, 132, 133, 140 
cryptographic algorithm/ cipher   123 
cryptographic keys   123, 170, 190 
cryptography   121-123, 125, 127, 131-133, 
135-137, 139, 158, 159, 167-169, 184, 
190, 197, 200 
cryptologist   123, 134 
cryptology   122, 123 
cryptosystem   123, 124, 126, 135, 136, 
148, 160, 161 
cumulated distance   327, 331 
cyclic codes for error detection   267 
 
d free (d∞)   325 
data authentication   124, 164, 166 
data complexity   127 
decimal format of ASCII code   61 
decimal numeral system   57, 122 
decision noise   72-75, 446 
decoder with LFSR and external modulo  
2 adders   287 
decoder with LFSR and internal modulo 
two adders   288 
decoding window   327, 331 
decryption/deciphering   123-127, 129,  
134, 139-141, 145, 147-152, 162-164,  
189, 198-203 
degeneracy   79 
Delay Modulation   378 
Delta modulation   111, 115, 117 
DES-X   148 
detection disabling attacks   178 
detection with and without original signal   
169 
detection/correction capacity   218, 235 
dicode (twinned binary)   377, 379, 383 
differential BL   378 
differential coding   109, 376, 379 
diffusion   135, 137-139 
diffusion effect   135 
Digimarc   183, 184 
Digital Millennium Copyright Act   165, 
184 
digital signature   124, 161, 164, 165, 184 
digital watermarking   164, 167, 189 
digram   131, 132 
direct orthogonal codes   336, 338, 339 
discrepancy   304 
discrete   1, 7, 8, 14-17, 27, 30, 44-47,  
48, 49, 66, 93, 435 
distortion   5, 66, 67, 75, 178, 343, 346 
distribution of movie dailies   183 
DIVX Corporation   183 
DMI Differential Mode Inversion Code   
377, 380, 383 
DNA (DeoxyriboNucleicAcid)   76, 77, 
190-200 
DNA synthesiser   190 
DNA tiles   195-196 
DNA XOR with tiles   197 
DPCM codec   110 
 
electronic Codebook   150 
Encoder with LFSR   277 
Encoder with LFSR and internal modulo 
two adders   279, 281 
Encoders for systematic cyclic codes  
implemented with LFSR   277 
encoding equations   226 
encryption   53, 68, 69, 121, 123-125,  
127, 152, 162-165, 184, 197-202 
encryption/ enciphering   123 
enrichment for the host signal   166 
entropic or lossless compression relation   
80 
entropy of m-step memory source   44 
erasure channels   223 
erasure correction   223 
erasure symbol   223 
error control codes   53, 210, 216, 375, 383 
error correcting codes   172, 177, 209, 216, 
218, 235, 238 
error correction   228, 240, 285-289 
Error correction and detection  
(ARQ hybrid systems)   214 
Error Correction Cyclic Decoder  
(Meggitt Decoder)   285, 286 

478
Subject Index
 
error density   50 
error detecting codes   216, 237, 254 
error detection   213, 222, 230, 253,  
265-267, 282-284 
error detection capacity   375, 379, 380, 
383 
error detection cyclic decoders   282 
error exponent   210 
error extensión   151 
error locator   270, 302 
error performance   375, 382, 383 
error performance of line codes   381 
error polynomial   270, 272, 299, 335 
error propagation   151 
error value   298, 303 
escape codeword   59 
Euler totient   162, 163 
Euler-Fermat Theorem   162 
even parity criterion   253, 254 
exons   76, 77, 191 
extended Hamming Codes   241 
extrinsic information   349 
 
fault-tolerant for point mutations   79 
FEAL   149 
feedback shift registers   153 
fingerprint   183 
fingerprinting   166, 183 
finite and homogenous Markov chain   39 
fixed – fixed   101 
fixed – variable   101 
Forward error correction (FEC)   213, 216, 
228 
forward recursion   358, 359, 362, 368-370 
four fold degenerate site   79 
fragile watermark   166, 168 
frequency encoding   297 
 
gel electrophoresis   193, 198 
gene   76, 77, 191-194 
gene expression   76, 191-193 
generalized   129 
generates all the non-zero states in  
one cycle   276 
generator polynomial   154, 256, 258, 259, 
261, 262, 266-268, 275, 287, 291, 293, 
310, 314, 420, 421 
 
genetic code   76, 77-79, 119, 191 
genome   79, 192, 194  
geometric distortions   178 
geometric representation   217 
Gilbert Vernam   126, 132, 134 
Go-back N blocks system (GBN)   214, 215 
GOST   149 
granular noise   112-114 
graph   17, 26, 40, 83, 123, 321, 323, 324 
graphein   121 
 
half-duplex-type communication   214 
Hamming boundary   228 
Hamming distance   219, 324, 327, 351 
Hamming weight w   219, 325 
hard decision channel   332 
HAS - human audio system or  
HVS - human visual system   170 
HDBn (High Density Bipolar n)   380 
hexadecimal numeral system   57 
homogeneity   40 
hybridization   191, 193, 197, 198 
 
IBM attack   169, 171, 178-180 
IDEA   137, 148 
Identifiers   182 
immunity to polarity inversion   375, 383 
implicit redundancy   216, 218, 255 
independent   9, 20 
independent channel   22, 23 
indirect orthogonal codes   336, 338, 339 
information   1-5, 8-16, 21-23, 30 
information block   224, 267, 313, 469 
information representation codes   53, 55  
information symbols   211, 216, 224, 228, 
239, 245, 258, 264, 277-279, 311, 314, 
335 
Information Transmission System (ITS)    
2 
initialization vector (IV)   151, 152 
input (transmission) alphabet   16 
input entropy   19, 20 
insertion   165, 168-178, 186, 187 
instantaneous code   55, 82, 83, 87 
Intentional attacks   168, 177, 181 
Interleaving   340-344 
 

Subject Index 
479
 
International Standard Book Numbering   
165 
International Standard Recording Code   
165 
Interpolation   345, 346 
introns   76, 191 
inversion   169, 178, 268 
irreversible compression   117 
 
joint input – output entropy   19 
joint input–output   18 
joint probability mass function   18 
 
Kerckhoffs law   126 
key generation   141, 148, 171 
key management   201 
known plaintext attack   126 
Kraft inequality   82, 83, 85 
Kryptos   123 
 
length of a word   80 
LFSR   153 
LFSR with external modulo two adders   
274 
LFSR with internal modulo two adders   
276 
Limited   30, 32, 68, 75 
line codes, transmission modes, baseband 
formats/wave formats   374 
linear block code   224, 229, 255 
linear transformations in finite fields   137 
locators   270, 298, 299 
log - likelihood function   357 
 
main advantage   68 
main disadvantage   68, 182, 340 
majority criterion   211 
MAP decoding algorithm   350, 372 
Markov property   40 
Masquerade   170, 174 
masquerading signal   170 
matrix representation   216, 227, 263,  
402, 418 
maximum likelihood decoder   221,  
350, 351 
maximum likelihood path (ML)   351,  
357, 358 
McMillan theorem   83 
mean squared error ε   2 
medicine   39, 79, 166 
microarray   190, 193, 198 
Miller code   378 
minimal polynomial   260-263, 291, 404, 
405-408 
MLD decoder   221 
modified Hamming Codes   240 
modified Huffman code   97, 98 
Modulation   3, 4, 15, 36, 49, 64, 66, 69, 
109, 115, 116, 213, 348, 368-370,  
374-378 
Moore law   127 
m-order memory source   38 
multilevel signalling   380, 382 
multiple DES   148 
Multiplication circuit over GF(2k)    
310 
mute interpolation   346 
Mutual information   21 
 
necessary condition   113, 180, 228, 233, 
317 
noise   1-4, 17-20, 22-33, 37, 49-53,  
67-75, 97-102 
noiseless channel   20, 22, 23, 80 
noisy channels coding theorem   209 
Non-redundant coding   211 
Nonrepudiation   124 
NON-Return to Zero   377 
Nonsense   79, 123 
non-separable codes   216, 218 
non-systematic codes   216, 316, 317, 326 
non-uniform code   54, 59 
NRZ-L (Non Return to Zero Level)    
377, 378 
NRZ-M (Non Return to Zero Mark)    
377, 378 
NRZ-S (Non Return to Zero Space)    
377, 378 
N-th order code distance   324 
Nucleotide   76, 191 
numeral system   56, 57 
Nyquist theorem   32, 68 
 
octal numeral system   57 
Odd parity criterion   253, 254 
Oligonucleotides   190 

480
Subject Index
 
one error correcting (t=1) Hamming group 
codes (perfect)   209, 228, 239 
One time pad   126 
one time pad (OTP) principle   136 
one-way hash   169 
operation mode   150 
optimal algorithms   85 
output entropy   19 
output(receiving) alphabet   16 
ownership deadlock   169 
 
P box   137-139 
parity check matrix   226 
parity-check   141, 226 
passive monitoring   182 
path metric   351-354, 357-359, 362, 363 
perceptual transparency   168, 169-171, 
174, 185 
perfect secrecy   126, 135 
period of the characteristic matrix   276 
permutation   128, 137-143, 256 
perturbation   1, 20 
Peterson algorithm with Chien search   269, 
270, 273, 299, 301, 312 
plaintext/ cleartext   123 
Polar signalling   382 
polyalphabetic substitution cipher    
132, 133 
Polybius   121, 128, 130, 131 
Polybius square   130, 131 
polygramic cipher   131 
polymates   132 
polymerase chain reaction   192 
polynomial representation   217, 263, 264, 
297, 310, 315, 335, 347, 402 
positional systems   57 
prediction gain   111 
primer   192 
primitive   154, 158, 260-262, 276,  
290-297, 348, 395-401, 408-409, 417 
principle of spread-spectrum   172 
probe   193 
processing complexity (work factor)   127 
product cipher   135, 137, 139 
proof of ownership   182 
proteins   77-79, 191 
protocol of digital signature   161 
 
pseudo ternary line codes   378 
public key cryptography (PKC)   125, 159, 
164, 201 
 
quantization noise   67, 69, 75, 111, 113 
quantization noise limited region   75 
 
randomness and non-repeating OTPs   200 
rate distortion theory   75 
ratio   14, 26, 35, 50, 70, 84, 214, 218, 347, 
375, 453 
receiver   3, 4, 16, 20, 31, 53, 65, 102, 122, 
161, 165, 167, 169, 211-214, 269, 341, 
379-382, 457 
reception probability matrix   17 
reciprocal   21 
recognition sequence   192 
recombinant DNA technology   192 
recursive systematic convolutional codes   
346 
redundancy   13, 24, 27, 28, 47, 79, 98, 
107, 116, 117, 137, 210, 218, 235, 253, 
255, 296, 378 
redundant coding   211 
redundant symbols   211, 212, 216, 218, 
267 
Reed-Solomon (RS) codes   255, 295-301, 
308-312, 342-346, 421 
reflected binary code   63 
region   99 
regular   41, 45, 99, 391, 410 
regulatory elements   191 
relative frequency analysis   130-132 
relative redundancy   13, 24, 28, 218, 253, 
296, 314 
remarking concept   189 
removal attacks   178 
renaissance   122 
Return to Zero(RZ)   377 
reversible compression   117 
ribonucleic acid   190 
Rijndael   147 
Rivest – Shamir - Adleman (RSA) cipher   
162, 164, 169 
RNA (RiboNucleicAcid)   77, 191-193, 199 
Robustness   101, 116, 168-171, 177, 181, 
184, 185, 188 
 

Subject Index 
481
 
rotor machines   128 
rounds   137, 141, 147-149 
RS coding and decoding using linear  
feedback shift registers LFSR   308 
rubber-hose cryptanalysis/ purchase  
key attack   126 
 
S box   137, 138 
Sacred Books   121 
SAFER   149 
Scramblers   378 
Scytalae   121 
Secret   121-125, 127, 136, 148-150, 159, 
160, 162, 169, 184, 201 
Secret message transmission   166 
Security   122, 125-127, 130, 135, 137,  
140, 158, 162-164, 174, 188 
selective repeat system (SR)   214 
separable codes   216, 218 
Shannon   5, 9-11, 23, 30, 32-35,  
84, 85, 95, 106, 117, 126, 127,  
134-137, 209-218, 346, 348 
Shannon – Fano algorithm   85 
Shannon first theorem or noiseless channels 
coding theorem   84 
Shannon limit for an AWGN   35 
Shannon second theorem   209, 210, 218, 
238 
Shortened Hamming Codes   242 
Sign – value system   57 
Signal   1, 2, 4 
signal detection   381, 435, 438, 442 
signal spectrum   375, 383 
signal/noise ratio (SNR) ξ   2, 5, 30,  
32, 33, 36, 66, 68-70, 74, 75, 112,  
114, 115, 235-238, 370, 375, 382,  
446, 447 
signature   122, 124, 161, 165, 167,  
182-184 
Simple attacks   178 
Simple parity check codes   253 
sliding correlator   173, 176 
Slope-overload noise   112 
soft (decision) decoding   333, 334 
soft decision   333, 359, 363 
spectral efficiency (bandwidth efficiency)   
35, 36 
 
standard array   232, 233, 248, 249 
state diagram   320, 351, 352 
static algorithms   91, 117 
stationary   42, 44-46, 93, 94 
stationary state PMF   42 
statistic compression   117 
statistical adaptation of the source to the 
channel   23, 53, 55, 81, 86, 464 
steganographia   122, 133 
steganography   122, 123, 133, 167, 190, 
197, 198 
steganos   121 
step size   67 
Stop and wait (SW) system   214 
Storage   1-4, 53, 56, 58, 64, 69, 77, 80,  
95, 117, 127, 166, 172, 189, 194,  
213, 343 
storage complexity   127 
storage medium   1-4, 188 
stream ciphers   137, 153, 157 
substitution   122, 127-134, 137, 145, 377, 
379, 380 
substitution ciphers   128-130, 132 
substitution line codes   379 
substitution Permutation Network   137, 
139 
sufficient condition   82, 83, 222, 228 
survivor   327, 331, 351, 353, 354, 356,  
359, 363, 364 
symmetric (conventional)   135 
symmetric channel   24, 25  
synchronization   3, 4, 53, 173, 176, 178, 
187, 377-379, 383 
syndrome   227, 232-235, 239-242, 265, 
269, 270, 282, 283, 291, 305, 338 
syndrome decoding (using lookup table)   
232, 233-235, 248 
system   2, 11, 29, 36, 39, 42-47, 56, 57, 
100, 111, 123, 126, 172, 213 
system using special abbreviations for  
repetitions of symbols   57 
systematic codes   216, 314, 317, 471 
 
t error correcting block code   230 
t errors correction   231 
t errors detection   231 
tabula recta   133, 134 
 

482
Subject Index
 
target   193 
termination   79  
textual Copyright notices   182 
the basic demands   185 
the collusion attack   178, 180 
The digital audio encoding system for CD 
(CIRC)   343 
the encoding law   225-227, 255, 259 
The Millennium Watermark System   184 
three STOP codons   79 
three-fold degenerate site   79 
Threshold decoding   313, 326, 334, 336, 
338  
ticket concept   189 
tightly packed/lossless   228 
time encoding   296 
time resolution   31, 32, 37 
total efficiency   214 
transcription   76, 77, 101 
transinformation   21, 23, 25, 29, 55 
transition (noise) matrix   17, 27, 29  
translation   76-79, 187, 191 
transmission channel   1, 10, 26, 27, 30, 31, 
235, 311 
transmission probability matrix   16 
transmission system   2, 26, 214, 365,  
379, 435  
transposed matrix   227 
transposition   128, 137, 138, 143, 147, 288 
transposition ciphers   128 
Tree Diagram   321 
Trellis Diagram   323, 324, 365 
trellis termination   320, 349, 358, 471 
trial key   134, 135 
Trithemius cipher   132, 133 
trustworthy digital camera   184 
 
 
 
 
 
 
 
 
 
 
 
 
Turbo codes   36, 209, 333 
two-fold degenerate site   79 
 
unaesthetic   182 
unary system   57 
undetectable   227, 229, 237, 255, 346 
uniform code   54 
unintentional attacks   168, 177 
Unipolar signalling   382 
uniquely decodable code   55, 100  
upper bound of the error probability after 
decoding   230  
 
variable – fixed   101 
variable – variable   101  
vector representation   217 
very noisy channel (independent)   20 
videosteganography   167 
Vigénère ciphertext keyword   135 
Viterbi algorithm   313, 326, 327, 328,  
333, 334, 348, 349, 373 
 
Watermark   165, 167-181, 182 
watermark detection   185, 187 
watermark extraction (detection)   165, 
169-172, 175-180, 185 
watermark granularity   168 
watermark information   168-177, 180  
watermark insertion   165, 168-176, 186 
watermark payload   168, 173  
watermark signal   170, 171, 175 
watermarking   164-174, 176, 177,  
180-185, 189 
watermarking techniques   166, 170,  
171, 172 
weights distributio   229 
whitening technique   148, 149 

Acronyms 
AC – Alternating Current 
ACK – Acknowledge 
ADC – Analog to Digital  
Converter/Conversion 
ADCCP – Advanced Data  
Communication Control Procedure 
ADPCM – Adaptive DPCM 
AES – Advanced Encryption Standard 
AHS – Audio Human Sense 
AMI – Alternate Mark Inversion 
ANSI – American National Standard 
Institute 
AOC – Absolute Optimal Code 
APP – Aposteriori Probability 
ARQ – Automatic Repeat Request 
ASCII – American Standard Code for 
Information Interchange 
AWGN – Additive White Gaussian 
Noise 
BB – Baseband 
BCD – Binary Coded Decimal 
BCH – Bose–Chauduri– Hocquenghem 
code 
BEC – Binary Erasure Channel 
BER – Bit Error Rate  
BISYNC – Binary Symmetrical  
Communication Protocol 
B–L – Biphase Level–Manchester 
B–M – Biphase Mark 
BMC – Biomolecular Computation 
BN – Binary Natural 
BNZS – Binary N–Zero Substitution 
Codes 
BPSK – Binary Phase Shift Keying 
B–S – Biphase Space 
BSC – Binary Symetric Channel 
CBC – Cipher Block Chaining  
CC – Channel Encoding Block 
CCITT – International Telegraph and 
Telephone Consultative Committee 
CD – Compact Disc 
CDMA – Code Division Multiplex  
Access 
CFB – Cipher Feedback 
CIRC – Cross Interleaved  
Reed–Solomon Code 
CMI – Coded Mark Inversion 
CR – Carriage Return 
CRC – Cyclic Redundancy Check  
CS – Source Encoding Block 
CU – Control Unit 
DAC – Digital to Analog  
Converter/Conversion 
DC – Direct Current 
DCC – Channel Decoding Block 
DCS – Source Decoding Block 
DCT – Discrete Cosine Transform 
DEL – Delete 
DES – Data Encryption Standard 
DFT – Discrete Fourier Transform  
DM – Delta Modulation  
DMI – Differential Mode Inversion 
Code 
DMS – Discreet Memoryless Source 
DNA – DeoxyriboNucleic Acid 
DOC – Direct Orthogonal Codes  
DPCM – Differential PCM  
dsDNA – double stranded DNA 
DVD – Digital Versatile Disc 
EBCDIC – Extended Binary Coded 
Decimal Interchange Code 
ECB – Electronic Codebook 
EOT – End of Transmission 
ESC – Escape 

484
Acronyms
 
FCS – Frame Checking Sequence 
FEC – Forward error correction 
FFT – Fast Fourier Transform 
FGK – Faller, Gallager and Knuth  
algorithm 
FLC – Frame Level Control 
FPGA – Filed Programmable Gate  
Array 
GBN – Go–back N blocks system 
GF – Galois Fields 
GOST – Gosudarstvenîi Standard 
GSM – Group Special Mobile 
HAS – Human Audio System   
HDBn – High Density Bipolar n 
HDLC – High Data Link Control 
HDTV – High Definition Television 
HVS – Human Visual System  
IBM – International Business  
Machines Corporation 
IC – Instantaneous Code 
IC – Integrate Circuit 
IDEA – International Data Encryption 
Algorithm 
IEEE – Institute of Electrical and  
Electronics Engineers 
IFFT – Inverse FFT 
IOC – Indirect Orthogonal Codes 
IP – Inverse Permutation 
ISBN – International Standard Book 
Numbering  
ISDN – Integrated Services Digital 
Network 
ISI – InterSymbol Interference 
ISO – International Organization for 
Standardization 
ISR – Information Shift Register 
ISRC – International Standard  
Recording Code  
ITC – Information Theory and Coding 
ITS – Information Transmission  
System 
ITU – International  
Telecommunication Union 
JPEG – Joint Photographic Experts 
Group 
KPD – Kinetic Protection Device 
LF – Line Feed 
LFSR – Linear Feedback Shift Register 
LOG–MAP – Logarithmic MAP 
LP – Low Pass 
LPC – Linear Prediction Coding  
LPF – Low Pass Filter 
LSB – Least Significant Bit 
LSI – Large–Scale Integration 
LSR – Linear Shift Register 
LZ – Lempel–Ziv Algorithm  
LZW – Lempel – Ziv – Welch  
algorithm 
MAP – Maximum Aposteriori  
Probability algorithm 
MAX–LOG–MAP – Maximum  
LOG–MAP 
MD–4/5 – Message–Digest  
algorithm 4/5 
ML – Maximum Likelihood 
MLD – Maximum Likelihood  
Decoding 
MNP5 – Microcom Network  
Protocol 5 
MPEG – Moving Picture Experts 
Group 
MR – Memory Register 
mRNA – messenger RNA 
MSB – Most Significant Bit 
NAK – Not Acknowledge 
NASA – National Aeronautics and 
Space Administration 
NBCD – Natural BCD 
NBS – National Bureau of Standards  
NCBI – National Center for  
Biotechnology Information 
NIST –National Institute of Standards 
and Technology 
NRZ – Non–return to zero 
NRZ–L – Non Return to Zero Level  
NRZ–M – Non Return to Zero Mark) 
NRZ–S – Non Return to Zero Space 
NSA – National Security Agency 
OFB – Output Feedback 
OTP – One Time Pad 
OWH – One–Way Hash  
PAM – Pulse Amplitude Modulation 

Acronyms 
485
 
PC – Personal Computer 
PCI – Peripheral Component  
Interconnect 
PCM – Pulse Code Modulation 
PCR – Polymerase Chain Reaction  
PEM – Privacy Enhanced Mail 
PGP – Pretty Good Privacy 
PKC – Public Key Cryptography 
PMF – Probability Mass Function 
PS – Signal Power 
PSD – Power Spectral Density 
QPSK – Quaternary Phase Shift  
Keying  
RAM – Random Access Memory 
RC – Compression Ratio 
RDFT – Reverse DFT 
Return to Zero (RZ) 
RF – Radio Frequency 
RLC – Run Length Coding 
RNA (RiboNucleicAcid)  
ROM – Read Only Memory 
RS – Reed–Solomon code 
RSA – Rivest – Shamir – Adleman 
RSC – Recursive Systematic  
Convolutional codes 
S/MIME – Secure/ Multiple purpose 
Internet Mail Extension 
SAFER – Secure and Fast Encryption 
Routine 
SDLC – Synchronous Data Link  
Control 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
SHA – Secure Hash Algorithm 
SNR – Signal to Noise Ratio 
SOVA – Soft Output Viterbi  
Algorithm  
SP – Space 
SPN – Substitution Permutation  
Network 
SPOMF – symmetrical phase only  
filtering –  
SR – Syndrome Register  
ssDNA – single stranded DNA 
SW – Stop and Wait 
SWIFT – Society for Worldwide  
Interbank Financial  
Telecommunication 
T– DES – Triple DES 
TC – Turbo Codes 
TDMA – time division multiplex  
access 
TLD – Threshold Logic Device 
tRNA – transfer RNA 
TV – Television 
UDC – Uniquely Decodable Code 
VA – Viterbi Algorithm 
VBI – vertical blanking interval  
VHS – Video Human Sense 
VLSI – Very Large–Scale Integration 
WM – watermark 
XPD – protect portable devices 
ZIP – ZIP file format 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
“I grow old learning something new everyday.” 
Solon 
 

Monica BORDA received the Ph.D degree from 
“Politehnica” University of Bucharest, Romania, in 
1987. She has held faculty positions at the Technical 
University of Cluj-Napoca (TUC-N), Romania, where 
she is an Advisor for Ph.D. candidates since 2000. She 
is a Professor of Information Theory and Coding, Cryp-
tography and Genomic Signal Processing with the De-
partment of Communications, Faculty of Electronics, 
Telecommunications and Information Technology, 
TUC-N. She is also the Director of the Data Processing 
and Security Research Center, TUC-N. She has conducted research in coding the-
ory, nonlinear signal and image processing, image watermarking, genomic signal 
processing and computer vision having authored and coauthored more than 100 
research papers in referred national and international journals and conference pro-
ceedings. She is the author and coauthor of five books. Her research interests are 
in the areas of information theory and coding, signal, image and genomic signal 
processing. 
 

