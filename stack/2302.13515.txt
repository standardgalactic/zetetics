From Classical to Quantum Information Geometry, an
Introductory Guide
J. Lambert1, ∗and E. S. Sørensen
1, †
1Department of Physics & Astronomy,
McMaster University 1280 Main St. W., Hamilton ON L8S 4M1, Canada
(Dated: November 2022)
Recently, geometrical aspects of quantum information have attracted consider-
able attention due to their applicability in promising new quantum technologies.
In particular, the quantum Fisher information has gained prominence for its utility
within the ﬁeld of quantum sensing through the Cram´er-Rao bound. Closely related
concepts, such as quantum variance and co-variance, ﬁdelity susceptibility, entan-
glement entropy and Berry curvature have also been extensively studied.
While
classical information geometry has been developed as a sub-ﬁeld of mathematical
statistics since the late 1950s, it has had a more limited impact within the ﬁeld of
condensed matter physics. Despite this, the generalization of information geometry
to incorporate quantum mechanics provides a natural framework for understanding
all the above concepts in a uniﬁed manner. Starting from an introduction to classical
information geometry and Shannon information, we discuss the diﬃculties in gen-
eralizing the classical results to the quantum case and present a pedagogical guide
to quantum information geometry. Rather than a thorough review of the by now
extensive literature, the intention is to emphasize the unifying framework relating
the diﬀerent concepts and to discuss their applicability to quantum sensing, as well
as to the detection of quantum criticality via ﬁdelity susceptibility, and to the un-
derstanding of topological properties of ground state manifolds. General relations
between the quantum Fisher information and response functions are derived, and
scaling relations are discussed, along with bounds relevant to quantum enhanced
metrology. Several examples are given for simple one and two qubit systems.
∗lambej3@mcmaster.ca
† sorensen@mcmaster.ca
arXiv:2302.13515v1  [quant-ph]  27 Feb 2023

2
CONTENTS
I. Introduction
3
II. Information Geometry
8
A. Classical Information Geometry
9
1. Shannon Information
9
2. Fisher Information
11
3. From Information to Geometry
13
B. Quantum Information Geometry
18
1. von Neumann Entropy
18
2. Quantum Fisher Information
19
3. Quantum Geometric Tensor
20
4. Topological Invariants and Volumes
25
III. Information Geometry in Practice
26
1. Relationship to Linear Response
26
2. The Quantum Variance
27
3. Quantum Fisher Information and Entanglement
31
4. Fidelity Susceptibility
37
5. Quantum Sensing
41
6. Euler Characteristic, Curvature, and Many-body Phases
42
IV. Summary and Outlook
43
A. Properties of Shannon Information
44
B. Example Calculations
45
1. Single Qubit
45
References
47

3
I.
INTRODUCTION
Starting with the work of Amari [1, 2] and Nentsov it was realized that Fisher informa-
tion [3] could be regarded as a Riemannian metric on the space of probability distributions,
giving rise to the notion of distance and curvature on such a space. In a more general sense
we can regard a probability distribution as a measure of information and hence it makes
sense to talk about the geometry of information. If instead of classical probability distri-
butions, we consider density matrices deﬁned on the state space of a quantum system, we
arrive at what is termed quantum information geometry. Quantum information geometry is
a powerful language that allows us to understand deep connections between several diﬀer-
ent ﬁelds. The classiﬁcation of topologically ordered phases [4–6], the scaling exponents of
classical [7], quantum phase transitions [8, 9], dynamical phase transitions [10], the design
of sensors that surpass classical limits [11], AdS/CFT correspondence [12] and the quan-
tiﬁcation of resources for information processing tasks [13], all cohere in the geometrical
structure of the state space. Each of these subjects is well-developed in its own right, and
we make no attempt to review the by now extensive literature in these ﬁelds. This work is
intended, as its title suggests, as a pedagogical guide to those would be explorers of the rich
landscapes furnished by this geometry. We emphasize the landmarks and pitfalls, and try
to transmit our sense of excitement at this ever evolving picture of quantum mechanics. In
the remainder of this introduction, we review some of the statistical structure of quantum
mechanics and how this structure relates to the principles of superposition and uncertainty.
In Sec. II, we give an overview of the classical and quantum theories of information geome-
try with some motivating examples. In Sec. III, we turn to the applications of information
geometry to several domains of physics before oﬀering a concluding summary in Sec. IV
Much of this work we will be motivated by understanding the diﬀerences between ﬂuc-
tuations in a system governed by classical mechanics, and those in a system governed by
quantum mechanics, and the various implications of this distinction in many-body systems.
To get a sense of what is meant by this diﬀerence we introduce the following example, which
we refer to as the classical case. Imagine a collection of N rotors obeying the laws of classi-
cal mechanics, with orientations, ⃗n, and angular momenta, ˆL, in three dimensions given by,
x ≡{⃗n1, ...,⃗nN, ˆL1, ..., ˆLN} ∈Ω, where Ωis the classical phase space. The time dependence
of the state can be found by solving the classical equations of motion for a given set of initial

4
conditions, yielding a particular trajectory χ(t). Then, any macroscopic property of the sys-
tem can be described by the function, Λ{χ(t)}. In practice, it is intractable in most cases to
specify the initial conditions for any macroscopic collection of particles. Even theoretically,
solving the equations of motion is only possible for systems with suﬃcient symmetry. Thus,
in both theory and practice, we consider averages of this function over either ﬁxed intervals
of time, or ensembles of possible states, which are equivalent as long as the conditions for
ergodicity are met [14]. Classical systems are thus modelled by a probability distribution
p(x), called a state. Observable properties are characterized by their averages over this state,
⟨Λ⟩p =
Z
p(x)Λ(x)dx
(1)
and their variances and covariances,
Varp{Λ} = ⟨Λ2⟩p −⟨Λ⟩2
p
(2a)
Covp{Λ1, Λ2} = ⟨Λ1, Λ2⟩p −⟨Λ1⟩p⟨Λ2⟩p.
(2b)
In the classical case, ﬂuctuations arise out of our ignorance of the precise conditions of the
system at any given instant. We call these ﬂuctuations classical ﬂuctuations, or sometimes,
incoherent ﬂuctuations. The distribution for a system of rotors in equilibrium with a bath
of inverse temperature β is captured by the Gibbs distribution,
p(x) = e−βE(x)
Z
(3)
where,
Z =
Z
e−βE(x) dx.
(4)
Fluctuations that arise due to equilibration with a bath (whether or particles, energy, vol-
ume, etc...) are termed thermal ﬂuctuations. All thermal ﬂuctuations are classical, though
not all classical ﬂuctuations are thermal.
Classically, a pure state is one that is speciﬁed without any degree of uncertainty (so far
as our physical theory allows). For any particular solutions of the equations of motion χ(t),
we have the pure state,
p(x; χ(t)) = δ(x −χ(t))
(5)

5
An important albeit obvious property of classical pure states is that they are pure on all
subsystems. Consider a subsystem V and its complement ¯V , such that the phase space is
decomposed into Ω= ΩV ⊕Ω¯V , with points in phase space factorizing as x = xV ⊕x ¯V and
trajectories as χ(t) = χV (t) ⊕χ ¯V (t). Then the state on the subsystem will be given by,
pV (xV ; χV (t)) =
Z
Ω¯V
p(x; χ(t))dx ¯V = δ(xV −χV (t)).
(6)
As long as our above factorizations are possible, the above state will also be pure. This is
what we mean when we say that a classical pure state is pure on its subsystems.
The revelation of quantum mechanics is that, at the time-energy scales of Planck’s con-
stant, the properties of even a single particle undergoing the simplest imaginable dynamics
are subject to persistent, irrevocable ﬂuctuations. Where classically we deﬁned a point in
phase space explicitly in terms of the microscopic degrees of freedom that we might, in
principle, know and measure, the quantum state space is deﬁned by a unit norm vector
in Hilbert space called the wavefunction, |Ψ⟩in a Hilbert space H [15]. The wavefunction
induces a probability distribution on an observable represented a Hermitian operator ˆΛ via
the amplitude of projection onto the eigenbasis of ˆΛ, p(λ) = |⟨Ψ|λ⟩|2.
Once we grant that the state of a quantum system is given by a vector in Hilbert space
rather than by a complete set of observable values, all the reasoning of the classical case
can be applied. Namely, it is practically very diﬃcult to determine precisely what state
a quantum many-body system is in at any given moment, and consequently we consider
mixtures of quantum systems represented by the density matrix,
ˆρ =
X
j
pj |j⟩⟨j|
(7)
where pj is a classical probability distribution over the pure states labelled by j.
Such
mixtures introduce incoherent ﬂuctuations, that is, ﬂuctuations not caused by the wave-like
nature of quantum phenomena. Just as in the classical case, the mixture in Eq. 7 can be
reduced to a pure state, |ψ⟩by taking pj = δj,ψ. In this case ﬂuctuations arise entirely from
quantum mechanical eﬀects, as we shall see.
The transition from classical mechanics to quantum mechanics then corresponds to the

6
substitutions,
p(x) →ˆρ,
Λ(x) →ˆΛ,
Z
(·) dx →Tr{·}.
(8)
In direct analogy with the classical Gibbs state, a quantum system in thermal equilibrium
with a bath of inverse temperature β is described by the density matrix,
ˆρ = e−β ˆH
Z
(9)
where ˆH is the Hamiltonian. In general, the matrix ˆρ can have non-zero oﬀ-diagonal elements
(some of which could be negative). In contrast, Eq. (4), is always a positive number. We
will return to the state, ˆρ, frequently as our primary example of a quantum mixed state.
In fact, Eq. 9, applies to many systems beyond just those in thermal equilibrium, as any
density matrix can be expressed in the form of Eq. 9 for a satisfactory choice of Hamiltonian
operator. Moreover, the Gibbs state can recover pure states in the limit β →∞. Whenever
we wish to remember that a density matrix corresponds to a pure state, we will denote this
fact with the superscript labelling the state, ˆρψ = |ψ⟩⟨ψ|.
One important measure of the degree to which a state is mixed is the purity,
P = Tr{ˆρ2}
(10)
Notice that for a pure state P = 1, while for a mixed state P < 1. For a Hilbert space of
dimension d, the maximally mixed state is the uniform distribution over all states, ˆρ = 1
dId×d,
where Id×d is the d × d identity. In this case the purity is 1
d.
This formal transition from points in classical phase space to vectors in Hilbert space,
facilitates the introduction of two new physical principles. The ﬁrst is the principle of super-
position. Whereas the pure states on the right hand side of Eq. 7 are combined incoherently,
quantum states can also combine coherently in superposition,
|ψ⟩=
X
j
cj |j⟩
(11)
where cj = ⟨j|ψ⟩are complex numbers such that P
j |cj|2 = 1. Unlike the mixture of states

7
in Eq. 7, the superposition of states in Eq. 11 preserves the wave-like nature of quantum
eﬀects, allowing for the resulting ﬂuctuations to become correlated in ways not possible when
considering incoherent superpositions. One way to understand the diﬀerence is the mixture
decreases purity, whereas super position does not. It is the principle of superposition that
gives rise to entanglement. The second is the principle of uncertainty. That is the fact that
for pairs of observables, say ˆΛ1 and ˆΛ2, have a minimum amount of simultaneous ﬂuctuation,
Varˆρ{ˆΛ1}Varˆρ{ˆΛ2} ≥1
4

Dh
ˆΛ1, ˆΛ2
iE
ˆρ

2
(12)
Hence, if ˆΛ1 and ˆΛ2 do not commute,
h
ˆΛ1, ˆΛ2
i
̸== 0, this establishes limits on Varˆρ{ˆΛ1}
and Varˆρ{ˆΛ2}. The ﬂuctuations that arise from these two principles are collectively termed
quantum ﬂuctuations.
An important implication of the principle of superposition is the emergence of entangle-
ment. In direct analogy with the discussion surrounding Eq. 6, we may consider the state of
a many-body pure state on one of its subsystems, V . In direct analogy with Eq. 6 we have,
ˆρψ
V = Tr ¯V {ˆρψ}
(13)
Unlike in the case of classical pure states, the state ˆρψ
V will only have unit purity if the state
|ψ⟩admits the decomposition |ψ⟩= |φV ⟩|φ ¯V ⟩. If ˆρψ is not pure, we say that the subsystems
V and ¯V are entangled in the state |ψ⟩.
Entanglement comes down to the producibility of a state, that is, the extent to which it
can be factored into subsystems. Since the study of entanglement is a major application of
information geometry we take a moment to formalize this idea. Let |Ψ⟩be an N body state.
Now conside a factorization of this state into M subsystems,
|Ψ⟩=
M
O
l=1
|ψl⟩
(14)
where each subsystem has Nl particles with P
l Nl = N. Now let m be an integer such that
m ≥Nl for all ℓ. We then say that |Ψ⟩is m-producible. If a state is m-producible but no
(m −1)-producible, we say that |Ψ⟩is m-partite entangled.
Thus we have two kinds of ﬂuctuations that coexist in many-body systems. The classical

8
,
(a) Statistical structure of classical mechanics
with all observable subordinated to a single
classical probability distribution.
(b) Statistical structure of quantum
mechanics, showing the subordination of
diﬀerent sets of observables to diﬀerent
distributions generated by the state ˆρ
(thermal) ﬂuctuations caused by limitations in our precise knowledge of the state of the
system at a given instant, and the quantum ﬂuctuations that seem to be intrinsic to the
deﬁnition of the states themselves. These quantum ﬂuctuations can give rise to correlations
beyond classical physics, of which entanglement is a particular example. The Relationship
between the two is depicted schematically in Fig. 1a and Fig. 1b. Using information theory,
we can understand these ﬂuctuations, both classical and quantum, as the notion of distance
that induces the aforementioned geometrical structure, and in turn, reveal the presence of
quantum correlations.
II.
INFORMATION GEOMETRY
Information is a concept that, like energy, appears in many contexts, some more rigorous
than others. Here, we are concerned with two notions of information that are intimately
related to one another. The ﬁrst is the Shannon information, which emerges naturally in the
theory of communication as a quantiﬁer of the information capacity of a communications
channel. The second is the Fisher information, which quantiﬁes the precision with which a
parameter modelling a random process can be estimated from available measurements. In
this section we elaborate on these two notions of information, introducing them ﬁrst in the
classical context and then deriving their quantum counterparts.

9
A.
Classical Information Geometry
1.
Shannon Information
In developing our notions of information, it helps to consider a particular problem. We
imagine a meteorological laboratory with a number of ﬁeld stations in various diﬀerent loca-
tions. Each hour, the stations transmit the current weather conditions. The transmissions
occur in binary, with each possible forecast in the set {xi}D
i=1 having encodings with lengths
{Li}D
i=1. For example, x1, which might correspond to clear skies, could be encoded as 001
(with L1 = 3), while x2, which might correspond to overcast skies, could be encoded as
0100 (with L2 = 4), and so on. When determining an encoding of forecasts into binary, it is
important that we ensure that no forecast’s encoding is contained in the preﬁx of another
forecast’s encoding, lest we confuse our receiver (for example we don’t want to use 1 as an
encoding for clear skies if 10 is an encoding for cloudy skies). Codes with this property are
called preﬁx codes, and they are always possible provided that the lengths of the encodings,
{L1, ..., LD}, satisfy the inequality [16],
D
X
i=1
2−Li ≤1
(15)
This inequality can be generalized to alphabets of any size by replacing 2 with the length
of the alphabet. In the interest of eﬃciency, we should aim to assign the shortest encoding
to the most likely forecast, so that on average we transmit as few bits as possible. If the
probabilities for each forecast are given by the set {pi}D
i=1, then we want to minimize the
expected length of a transmission,
⟨L⟩=
D
X
i=1
piLi
(16)
subject to the constraint given by Eq. 15. In doing so, we ﬁnd that optimal encoding length
of a forecast occurring with probability pi is,
ℓi = −log2(pi).
(17)

10
In general we might imagine that our probabilities, pi = p(xi|⃗λ) depend on some other
parameters, ⃗λ (such as temperature). In this case Eq. 17 is called the log-likelihood. It can
be read as the logarithm of the likelihood that ⃗λ has a particular value given the observed
result xi. We will encounter it again shortly. From Eq. 16 we can see that the optimal
average message length is bounded from below by,
⟨L⟩≥
D
X
i=1
piℓi.
(18)
This lower bound is our ﬁrst measure of information,
S = −
X
i
pi log2(pi)
(19)
and, as it was introduced by Claude Shannon [17] a little less than a year before Kraft
introduced the inequality in Eq. 15 [16], we call it the Shannon Information. To gain an
understanding of its properties, let’s ﬁrst consider the trivial case where there is a single
possible forecast with probability one hundred percent, i.e. pi = δ1i where δ1i is the Kronecker
delta function. Then the shortest possible encoding has average length zero. After all, what
would be the point in even transmitting? Assuming now a uniform probability distribution
on si, we see that the average length of the encoding will grow monotonically with the
number of forecasts, N. This property is called monotonicity, and it is a natural property
to demand from a measure of information.
The Shannon Information can tell us much more, however, than the average length of an
optimal encoding. Considering the case of two possible forecasts, it is clear that the minimal
encoding will be one bit (say dot for x1 and dash x2), regardless of the relative values of p1
and p2. If, however, we know that p1 is very nearly 1 and p2 is very close to zero, we can
see that the Shannon information tells us something about how much the receiver learns
from reading the message. If, at station P, the forecast is sunny nine out of every ten days,
we learn relatively little when we see receive a forecast of sunshine. The power of Shannon
information is that it captures an intuitive aspect of information: the less probable an event
is thought to be, the more information is gained when that event is observed.
For the mathematically inclined reader, one may wonder to what degree Shannon infor-
mation is really unique. In 1961, Alfred R´enyi [18] found an entire family of information

11
measures by imposing a number of conditions (see App. A) that are considered sensible for
a measure of information. These so-called R´enyi entropies contain the Shannon information
as a special case. The Shannon information is singled out by its property of recursion (its
appearance in the preﬁx coding problem above).
Physicists will recognize that the Shannon information is a generalization of the concept
of entropy. There is no particular reason why we should have to measure information in
bits, and we can just as well take the logarithm in Eq. 19 to have base e, (in which case we
say we are measuring information in nats). Given a thermodynamic ensemble of W possible
microstates, all taken to be equally likely with pi =
1
W , we see that the Shannon information
is simply the Boltzmann entropy,
S = k ln(W).
(20)
For this reason we tend to use the words entropy and information interchangeably. Since
changing the base of the logarithm simply amount to a redeﬁnition of the units of informa-
tion, we will do so frequently in the following text as a matter of convenience.
2.
Fisher Information
At our meteorological laboratory, we are interested in producing models that determine
the distribution of forecasts from our ﬁeld stations. To this end, we make a statistical model
P(⃗λ) = {pi(λ)} where ⃗λ ∈Rd is a parameterization of possible distributions. As we receive
data from the ﬁeld stations we want to determine as accurately as possible the values of ⃗λ
that ﬁt the data. For simplicity let’s consider the case of only one parameter. Let λ represent
a parameter such as a temperature, and let ˜λ be an estimator for this parameter (such as
the sample mean of some number of temperature measurements). The function p(x|λ) gives
the likelihood of the temperature x given the value of the mean temperature in the model
λ. We assume our estimator is unbiased, which means that ⟨˜λ⟩= λ (this condition can be
relaxed without substantially changing the discussion),
0 =
Z
dx (˜λ(x) −λ)p(x|λ)
(21)

12
Diﬀerentiating both sides with respect to λ, we can write,
0 =
Z
dx (˜λ(x) −λ)∂λp(x|λ) −
Z
dx p(x|λ).
(22)
Notice that we assume that the estimator itself does not depend on λ. This makes sense
since the estimator is the processing that we do to our data to estimate λ and if it depending
on λ it would be ill deﬁned. Performing the integral in the second term and rearranging a
bit we arrive at,
Z
dx (˜λ(x) −λ)p(x|λ)

1
p(x|λ)∂λp(x|λ)

= 1.
(23)
Now we notice that the left-hand side of the above equation is just the covariance of the
estimator with the logarithmic derivative of the probability.
Squaring both sides of the
above equation and applying the Cauchy-Schwarz inequality, yields
1 ≤VarP{˜λ}VarP{∂λ ln(p(x|λ))}.
(24)
Notice here that we are assuming that the expected value of the derivative of the log-
likelihood is zero. This is straightforward to show,
⟨∂λ ln(p(x|λ))⟩=
Z
p(x|λ) (∂λ ln(p(x|λ))) dx = ∂λ
Z
p(x|λ)dx = 0
(25)
The variance of ˜λ is thus bounded from below as,
VarP(˜λ) ≥
1
CovP
n
˜λ, ∂λ ln(p(x|λ))
o
(26)
Reintroducing the multidimensional nature of the problem and the deﬁnition of the Fisher
information, we can write,
CovP{˜λµ, ˜λν} ≥
1
4Fµν
(27)
where,
Fµν = 1
4
Z
Ω
dx
1
p(x|⃗λ)
∂p(x|⃗λ)
∂λa
∂p(x|⃗λ)
∂λb
.
(28)

13
is the Fisher information [19]. We’ve added the factor of 1
4 for reasons that will become
apparent in a moment. The inequality in Eq. 27 is known as the Cram´er-Rao bound [20].
It quantiﬁes how much information can learned about the parameters ⃗λ by sampling from
p(x|⃗λ). The appearance of the derivative of the log-likelihood implies a connection between
the Shannon information, which is the expectation value of the log-likelihood, and the Fisher
information which is the covariance of the partial derivatives of the log-likelihood, and we
will explore this connection on more detail momentarily.
We can begin to get a sense of the meaning of the Fisher information by considering the
case of a weather station with only two forecasts. Let λ be the probability of sun and λ −1
be the probability of rain. We can see that when λ = 0.5, the Shannon information – the
information we learn by reading the forecast – is maximal, whereas the Fisher information
– our ability to estimate λ – is minimal. As λ approaches 1 or 0, the Shannon information
decreases while the Fisher information grows. This behaviour is shown in Fig. 2 where the
Shannon information is given by,
S = −λ log2(λ) −(1 −λ) log2(1 −λ),
(29)
and the Fisher information for each measurement is,
Fλλ = 1
4
1
λ(1 −λ).
(30)
3.
From Information to Geometry
To better understand the connection between Shannon information, Fisher information
and geometry we consider the above scenario, that of estimating the value of the parameter
λ, from a slightly diﬀerent perspective. Considering again the model p(x|λ), we can ask
how distinguishable P = p(x|λ) is from Q = p(x|λ′). The degree of distinguishability can
be quantiﬁed in terms of the informational diﬀerence between P and Q by introducing the
relative entropy [21],
S(P|Q) =
D
X
j=1
pj log2
pj
qj

(31)

14
0.0
0.2
0.4
0.6
0.8
1.0
λ
0.0
0.2
0.4
0.6
0.8
1.0
S2(λ)
0
20
40
60
80
100
Fλλ
Shannon Information
Fλλ
FIG. 2: Shannon information (red) in bits and Fisher information (blue) for a coin toss
with pH = λ. The fair coin with λ = 0.5 is simultaneously the least predictable (high
Shannon information), and the hardest to distinguish from its neighbours (low Fisher
information).
If P = Q then the relative entropy is zero, and the distributions are indistinguishable. One
the other hand, we can imagine that P corresponds to a place that is always sunny, while
Q corresponds to a place that is always rainy. Letting x1 be sunshine and x2 be rain. Then
pi = δ1i and qi = δ2i, and we can see that the relative entropy is inﬁnite.
Let us imagine now that P and Q correspond to the information arriving from diﬀerent
ﬁeld stations. What are the chances that we might mistake one for the other? The relative
entropy can used to bound the probability, E, that we would obtain a sequence of reports
corresponding to the expected sequences for the distribution, P, when sampling N times
from the distribution Q,
E ≤(N + 1)D2−NS(P|Q)
(32)
This result is called Sanov’s theorem [22]. In our above example, there is no chance, even
on a single transmission, of mistaking the forecast for a town that always rains for a town
that is always sunny.
Still considering a situation where the only two possible forecasts are sun or rain, we
consider a place P which has, on any given day a ﬁfty percent chance of either, and compare
it to a place Q where the probability of sun is λ. If we take the limit λ →1, then the
relative entropy S(P|Q) →∞. The asymmetry of the relative entropy is apparent here, as
S(Q|P) = 1. We interpret this to mean that there is zero chance that the typical sequence

15
of N reports from P would be recovered by Q if Q is always sunny. By contrast, there is a
chance, which decays with 2N, that the typical sequence of Q will be produced by P, albeit
one that is vanishingly small.
This notion of distinguishability is at the heart of information geometry. Intuitively, the
more easily distinguishable two distributions, the further they ought to be. This geometry
is interesting because it arises naturally in the course scientiﬁc investigation. The example
of weather modelling is just one example, and will see several more in the quantum context.
But ﬁrst, we must explain exactly how distinguishability can be used as a metric.
The relative entropy is, asymmetric, and thus unsuitable as a metric in and of itself.
However we can do slightly better if we consider the case where P is varied only inﬁnites-
imally. Concretely we deﬁne the distributions, P = {pi} and P + dP = {pi + dpi}, and
expand the relative entropy in powers of dpi,
S(P|P + dP) =
D
X
i=1
pi ln

pi
pi + dpi

≈1
2
D
X
i=1
dpidpi
pi
.
(33)
Here we’ve changed to measuring our relative entropy in nats for convenience. This inﬁnites-
imal expansion is symmetric, with the asymmetry of the relative entropy entering only at
higher order. If we imagine that this inﬁnitesimal expansion is a distance, then we can infer
the line element,
ds2 =
D
X
i,j=1
Fijdpidpj = 1
4
D
X
i,j=1
dpidpi
pi
(34)
with the Fisher-Rao metric,
Fij = 1
4
δij
pi
.
(35)
We can normalize the metric in any way we like, but the choice of 1
4 is particularly convenient.
To see why, let’s make a change of variables ui = √pi. Now we ﬁnd, that the line element
is,
ds2 =
D
X
i,j=1
duidui
(36)
subject to the constraint,
1 =
D
X
i=1
u2
i
(37)
coming from the normalization of the probability distribution. We recognize immediately the

16
equation for an N-sphere of unit radius. The constraint that 0 ≤ui ≤1 conﬁnes us to ﬁrst
octant of this sphere. Each point on the octant corresponds to a probability distribution,
and the geodesic distance between two distributions in terms of the probabilities is simply,
DB(P, Q) = cos−1
 D
X
i=1
√piqi
!
(38)
where we recognize the argument of the inverse cosine as the standard dot product in terms
uj variables. We call this dot product the classical ﬁdelity and the corresponding distance
the Bhattacharyya distance [23]. From Eq. 38 we can see that pure states always separated
by π
2. It is equivalent to the statement that all classical mixtures of pure points are unique
to say that the classical state space is a simplex. We make these points because they will
no longer be true when we pass from classical to quantum mechanics.
The Fisher-Rao metric is unique in the sense that it is the only Riemannian metric on
the space of probability distributions that is also monotone. The meaning of this is as fol-
lows. Let T be a stochastic map, then a monotone function is one for which, given any
distributions P and Q, DB(TP, TQ) ≤DB(P, Q). This property corresponds to the in-
troduction of noise over the channels connecting our laboratory to the ﬁeld stations. As
the channels become noisier, all the incoming transmissions should become less distinguish-
able. If distinguishability is our notion of distance, then under such stochastic mappings,
limn→∞DB(T nP, T n, Q) →0.
To complete the connection between the Fisher-Rao metric and the Fisher information
derived in the previous section, let us re-express the Fisher-Rao metric in terms of the
parameters ⃗λ. By the chain rule, the metric in Eq. 35 induces a metric on the space of
parameterizations as,
Fµν = 1
4
D
X
i,j=1
∂pi
∂λµ
∂pj
∂λν
Fij
(39)
Using our deﬁnition of the Fisher-Rao metric we arrive at the classical Fisher information

17
metric (CFIM), which we give in both discrete and continuous form,
Fµν = 1
4
D
X
i=1
1
pi(⃗λ)
∂pi(⃗λ)
∂λµ
∂pi(⃗λ)
∂λν
,
(40a)
Fµν = 1
4
Z
Ω
dx
1
p(x|⃗λ)
∂p(x|⃗λ)
∂λµ
∂p(x|⃗λ)
∂λν
.
(40b)
here Ωis sample space if x is continuous [24]. Up to the normalization, this metric is the
Fisher information that appears in the Cram´er-Rao bound from Eq. 27. We change our
notation from F to g to reﬂect our inclusion of the normalization factor. From now on we
will write
∂
∂λµ ≡∂µ
There are two other ways of expressing the metric above that we will ﬁnd useful later.
The ﬁrst takes advantage of the log-likelihood introduced in Eq. 17. Notice that Eqs. 40
are, in fact, covariances in the log-likelihood’s partial derivatives (recalling Eq. 25),
Fµν = CovP {∂µℓ, ∂νℓ} .
(41)
The covariance can be taken as an inner product, and the derivatives of the log-likelihoods as
tangent vectors, allowing us to recover a familiar form of the metric from diﬀerential geom-
etry. What is the base space? The Shannon information is the mean of the log-likelihoods,
and so we can imagine that the log-likelihood of a probability pi is the information conveyed
by that particular event. The derivatives of the log-likelihood are then tangent vectors on
this space of information.
This leads to our third and ﬁnal form of the metric [24],
Fµν = −1
4∂µ∂νS,
(42)
which makes most explicit the connection between the Shannon information, S and the
CFIM, with the Shannon information playing the role of a potential. This equation can be
checked for the two forecast example discussed earlier by comparing Eq. 29 and Eq. 30. We
will make some comments about this form of the metric in the conclusion. It should be
stressed here that Eqs. 40, Eq. 41 and Eq. 42 are all completely equivalent for our purposes,
and we will ﬁnd it useful to think of the CFIM in one form or another depending on the

18
context.
B.
Quantum Information Geometry
We now generalize the concepts developed above in a classical setting to the quantum
case by making explicit use of the correspondence in Eq. 8.
1.
von Neumann Entropy
We ﬁrst consider the quantum analogue of the Shannon information, which will allow
us to ultimately quantify the entanglement between subsystems. Starting from Eq. 19 and
applying the mapping in Eq. 8 we immediately arrive at the von Neumann entropy,
S = −Tr{ˆρ ln(ˆρ)}
(43)
where we measure the information in nats. In doing so, we introduce our convention that
the quantum analogue of a classical information measure is written in script font. We can
see that the von Neumann entropy will be zero for pure quantum states, just as the Shannon
entropy was zero for pure classical states.
At ﬁrst glance, the von Neumann entropy doesn’t seem particularly useful for detecting
entanglement, since it will give zero for all pure states. In fact, this is exactly its utility.
If the von Neumann entropy of the subsystem state ˆρΨ
V is non-zero, then we know that the
subsystem V is entangled with its complement, ¯V as per our earlier discussion. For this
reason, the von Neumann entropy of a subsystem SV is called the entanglement entropy.
Useful in this interpretation is the fact that SV = S ¯V , that is, V is entangled with ¯V exactly
as much as ¯V is entangled with V . This is true even if V and ¯V are not the same size, as
long as they form a disjoint and complete partition of the system.
To see how the entanglement entropy works, we can consider the simple example of the
entanglement between the two spins for the state,
|GHZN⟩= 1
√
2

|↑⟩⊗N + |↓⟩⊗N
(44)
with N = 2. In this case, the von Neumann entropy between the subsystems will be 1 bit.

19
The entanglement entropy has seen extensive application in quantum information and
many-body physics. A signiﬁcant triumph is the scaling laws of entanglement entropy with
subsystem size, which characterize the central charge of conformal ﬁeld theories in models
where such theories describe the low energy degrees of freedom [25–27]. For many-body
systems with local interactions, the entanglement entropy tends to scale with the area of
the subsystem under consideration [28]. This fact is crucial to the eﬃciency of the density
matrix renormalization group family of algorithms.
Taking Eq. 43 as the expectation value of the operator −ln(ˆρ), we deﬁne the latter as
the entanglement Hamiltonian.
Phases exhibiting symmetry protected topological order
have been shown to exhibit non-trivial degeneracies in the spectrum of the entanglement
Hamiltonian (the entanglement spectrum) [29, 30]. Another signiﬁcant aspect of the en-
tanglement entropy is that, for phases with long range topological order in two dimensions
and greater, the area law scaling of the entanglement entropy is modiﬁed by a negative
subleading correction, the topological entanglement entropy [31].
These facts have made the entanglement entropy ubiquitous as a theoretical probe of
quantum ﬂuctuations and correlations. Experimentally, however, the application of Eq. 43
is quite limited. In particular, computing the entanglement entropy requires complete knowl-
edge of the density matrix, something which is only possible in highly controlled experimental
settings.
2.
Quantum Fisher Information
When generalizing the concept of Fisher information to the quantum case we must address
the fact that quantum mechanics allows us to choose between sets of measurements that are
decoherent with one another. For example, we might orient our Stern-Gerlach devices along
any spin axis we like, and in doing so will be sampling from distinct probability distributions
corresponding to the diﬀerent choices of measurement basis. To formalize this, we introduce
the concept of the prositive operator valued measure (POVM). See for instance [32]. Let
{|ξ⟩⟨ξ|}M
ξ=1 be a set of positive semi deﬁnite operators (hereafter projectors) acting on the

20
Hilbert space satisfying the resolution of the identity,
I =
X
ξ
|ξ⟩⟨ξ|
(45)
In general we do not require that the projectors be orthogonal as they would be if they
represented the spectrum of a Hermitian operator. Eq 45 represents the most general possible
kind of measurement that can be made on a quantum state. For each set of observables that
commute with each other, there exists a corresponding POVM which we might label with
an additional superscript, α. Referring back to Fig. 1b, each of the probability distribution
branching oﬀof the density matrix correspond formally to,
p(ξα) = Tr{|ξα⟩⟨ξα| ˆρ}
(46)
When considering the quantum generalization of the Fisher information, we see that we
would in fact have a QFI for each possible choice of POVM. What is conventionally called
the QFI then corresponds to maximizing the classical Fisher information over all possible
choices of POVM [33, 34]. We make this point only brieﬂy because it is relevant in the case
of quantum sensing which we return to later. For now we move straight to the quantum
generalization of the geometrical structure introduce in Sec. II A 3.
3.
Quantum Geometric Tensor
The formal mapping in Eq. 8 allows quantum states to change in two distinct ways. To
see this we consider a general mixed state depending on the parameter λ,
ˆρ(λ) =
X
j
pj(λ) |j(λ)⟩⟨j(λ)| .
(47)
A change in the parameter λ might cause a change in the eigenvalues and/or a change in the
eigenvectors. The former of these changes induces a distance identical to the kind that we
considered in the classical case. Those parameterizations that change the eigenvectors give
rise to quantum information geometry. We can study this case in detail by ﬁrst considering
a pure state,
Ψ(⃗λ)
E
. Now we can expand this state to leading order for a small change in

21
the component of the parameter λµ,
|Ψ(λµ + dλµ)⟩= |Ψ(λµ)⟩+ dλµ |∂µΨ(λ)⟩,
(48)
where ∂µ ≡∂λµ. Intuitively, the distance between states is the size of the change in the
state. We need to take care however to ensure that our notion of distance is independent
of the global phase, since physical states are deﬁned by rays in the Hilbert space. We thus
deﬁne the quantum geometric tensor (QGT),
Qµν = ⟨∂µΨ| (I −|Ψ⟩⟨Ψ|) |∂µΨ⟩
(49)
where we’ve subtracted oﬀthe projection of the change in the state |∂µΨ⟩onto the subspace
of the unchanged state deﬁned by the projector |Ψ⟩⟨Ψ|.
The object in Eq. 49 admits a decomposition into a real symmetric component and an
imaginary antisymmetric component,
Qµν = Fµν + iΩµν.
(50)
The real part, Fµν gives the Riemannian metric on the quantum state space [35, 36], while
the imaginary antisymmetric part, Ωµν gives the Berry curvature [37].
From this point
on we focus in particular on the real symmetric component, which we term the quantum
Fisher information metric (QFIM). The reason for this decomposition is that the Hilbert
space in which we’ve formulated our geometry is complex. Consequently, the metric tensor
is required to be Hermitian, but not purely real. Such manifolds then have two parallel
structures. On the one hand, the real Riemannian structure, Fµν, and on the other hand
the symplectic structure determined by Ωµν
The distance on the space of pure states is a direct analogue of the Bhattacharyya distance
in Eq. 38 [38],
DQB(Ψ1, Ψ2) = cos−1 (|⟨Ψ1|Ψ2⟩|)
(51)
where the argument of the inverse cosine is now the quantum ﬁdelity in place of the classical
ﬁdelity. The distance between pure states is sometimes called the quantum angle. Where
classically, pure states are always separated by the maximal distance of π
2, now it is orthogo-

22
nal states that are separated by π
2. Instead of the positive octant of a hypersphere, the space
of quantum pure states can be visualized as a Bloch hypersphere, with anti-podal points
identiﬁed as orthogonal states.
All physical pure states must be normalized, and so in the pure state limit only unitary
parameterizations are relevant. Without loss of generality we can always express a unitary
parameterization via the Schr¨odinger equation,
∂µ |Ψ⟩= −iˆΛµ |Ψ⟩
(52)
where ˆΛµ is an Hermitian operator corresponding to a change in the parameter λµ. By
substituting Eq. 52 into Eq. 49 we see that the QFIM for unitary parameterizations is given
by,
Funitary
µν
= 4 Re[CovΨ{ˆΛµ, ˆΛν}]
(53)
For mixed states, unitary parameterizations are slightly more subtle. As a ﬁrst attempt,
we may try to take Eq. 40 and directly apply the mapping in Eq. 8. The problem with
this is that, in general, [ˆρ, ∂µˆρ] ̸= 0. Resolving this ambiguity is, to a degree, a matter
of convention. We proceed along the lines of Braunstein and Caves [33], who resolve this
ambiguity by introducing the superoperator,
L−1
ˆρ { ˆO} = 1
2(ˆρ ˆO + ˆOˆρ).
(54)
In the eigenspace of the density matrix, the inverse of this operator is given by,
Lˆρ( ˆO) =
X
j,k
2
pj + pk
Ojk |j⟩⟨k| ,
(55)
with the sum running only over terms with pj + pk > 0. Notice that the above form is the
inverse of Eq. 54 in the sense that Lˆρ{L−1
ˆρ { ˆO}} = ˆO. Applying the inverse to the derivative
of the density matrix,
Lˆρ,µ = Lρ
 ∂ˆρ
∂λµ

(56)
gives us a logarithmic derivative wherein the ambiguity in the ordering of the operators is
resolved symmetrically, hence we call Eq. 56 the symmetric logarithmic derivative (SLD).

23
Now we can write the QFIM for mixed states as,
Fµν = 4 Re [Covˆρ{Lˆρ,µ, Lˆρ,ν}] .
(57)
The above equation can be taken as the quantum analogue of Eq. 41. It includes the special
case where only the eigenvalues of the density matrix are modiﬁed, and in that instance
reduces to the classical Fisher information matrix (CFIM). Parameterizations that modify
the eigenvectors of the density matrix are captured by the von Neumann equation,
∂µˆρ = i
h
ˆρ, ˆΛµ
i
.
(58)
which described unitary evolutions generated by the Hermitian operator ˆΛµ
As we mentioned, the choice of resolving the ambiguity in ordering through the SLD is
just one choice. Morozova and ˇCencov [39] demonstrated that for every operator monotone
function we can deﬁne a metric on the space of mixed states. Skipping the rigorous details,
we present here the relationship between metrics and operator monotone functions. First
we introduce the superoperators [40],
Rˆρ( ˆO) = ˆOˆρ
(59a)
Lˆρ( ˆO) = ˆρ ˆO
(59b)
(Lf
ˆρ)−1 = f(LˆρR−1
ˆρ )Rˆρ.
(59c)
Here f is the operator monotone function. For the case of the SLD, we can ﬁnd that,
f SLD(x) = 1 + x
2
.
(60)
There are many other possible choices (we list several in Tab. I) , but the QFIM is distin-
guished by the fact that it ﬁnds the maximal distance between states, and is, in this sense,
unique [24].
The SLD QFIM can be used to deﬁne the Bures distance between mixed states in analogy
with Eq. 51 [41],
DBures(ˆρ1, ˆρ2) = cos−1 p
F(ˆρ1, ˆρ2)

,
(61)

24
QFIM
monotone function
Symmetric logarithmic derivative
1+x
2
Bogoliubov-Kubo-Mori
x−1
log(x)
right logarithmic derivative
x
left logarithmic derivative
1
Skew information (α = 1)
(√x+1)2
4
TABLE I: A partial reproduction of the table in [40] listing several diﬀerent forms of the
QFIM along with the associated monotone function.
where
F(ˆρ1, ˆρ2) = Tr
qp
ˆρ1ˆρ2
p
ˆρ1
2
(62)
is the Uhlmann ﬁdelity [42].
The above expression can be interpreted as the transition
amplitude between mixed states, in exactly the same way as the quantum ﬁdelity can be
interpreted as the transition amplitude between pure states. Taking the SLD QFIM for
granted, the Bures distance is the broadest generalization of Riemannian distance on the
quantum state space.
Focusing on the symmetric part of Eq. 49 it is useful to explicitly decompose the QFIM
into a component that arises due to changes in the eigenvectors of the density matrix (the
unitary component), and a component that arises due to changes in the eigenvalues (the
non-unitary component) (see for example the supplementary material of Ref. [8]),
Fµν = Fnon-unitary
µν
+ Funitary
µν
,
(63)
where,
Fnon-unitary
µν
= 1
4
X
j
1
pj
∂µpj∂νpj,
(64)
and
Funitary
µν
= 2
X
j,k
(pj −pk)2
pj + pk
[Λµ]jk[Λν]∗
kj.
(65)
Eq. 64 is the direct analogue of Eqs. 40. Eq. 65 has no direct classical analogue. It emerges
speciﬁcally due to the non-commutativity of quantum observables. Often times, the ex-
pression in Eq. 65 is called the quantum Fisher information, due to this fact. To maintain
our symmetry with the notation used in the discussion of von Neumann entropy, we prefer

25
to call the entire metric, containing both unitary and non-unitary components the QFIM,
and instead refer to the unitary and non-unitary components as needed. Because the Berry
curvature is antisymmetric there is no such ambiguity.
4.
Topological Invariants and Volumes
The QFIM allows for the construction of two distinct but related topological invariants.
We discuss these invariants speciﬁcally in the case of a two-dimensional submanifold of states,
M, which might be given by the parameters of a Hamiltonian, or by the ﬁrst Brillouin zone.
The ﬁrst emerges from the Berry curvature, which, when integrated over a two-dimensional
manifold, gives the Chern number of the manifold,
C = 1
2π
Z
M
Ωµνdλ1dλ2.
(66)
The second is the Euler characteristic, which is given by Gauss-Bonnet theorem [43],
χ = 1
2π
Z
M
K dA +
I
∂M
kg dℓ

.
(67)
In the above expression, K is the Gaussian curvature and kg the geodesic curvature of the
boundary. In order to express these explicitly in terms of the QFIM, we model our notation
on Ref. [44]. Let us write the metric in ﬁrst fundamental form, by expressing the line element
as
ds2 = Edλ2
1 + 2Fdλ1dλ2 + Gdλ2.
(68)
We denote the determinant of the metric by omitting the Greek indices,
F = det{Fµν} = EG −F 2.
(69)
We also recall the Christoﬀel symbols,
Γk
ij = 1
2Fkm  ∂jFim + ∂iFjm −∂mFij

.
(70)

26
Now we can express the Gaussian curvature and the geodesic curvature,
K =
1
√
F
 
∂2
 √
F Γ2
11
E
!
−∂1
 √
F Γ2
12
E
!!
(71a)
kg =
√
FG−3
2Γ1
22,
(71b)
along with the line and area elements,
dA =
√
Fdλ1dλ2
(72a)
dℓ=
√
Gdλ2
(72b)
In Sec. III 6 we oﬀer further discussion of the analysis of these topological invariants in
several systems.
III.
INFORMATION GEOMETRY IN PRACTICE
So far, we have laid out the geometrical structure of the quantum state space. In this
section, we try to understand what this structure means from a physical perspective, with a
particular focus on the applications of this structure to many-body systems. There are two
main topics to be addressed here. The ﬁrst is understanding what, in physical terms, the
quantum information geometry is actually describing. A hint is already given by the fact
that the metric is the real part of the covariance. The second is understanding several of
the ways in which
1.
Relationship to Linear Response
Considering the fact that the QFIM quantiﬁes the degree of distinguishability along
the path generated by the hermitian operator ˆΛµ, it is not surprising that the unitary
component of QFI has a convenient representation in terms of the dissipative part of the
linear response [8],
Funitary
µν
= 4
π
Z ∞
0
dω tanh
 ω
2T

χ′′
µν(ω).
(73)

27
where,
χ(ω) =
Z ∞
0
dte−iωt Tr
n
ˆρ
h
ˆΛµ(t), ˆΛν
io
.
(74)
Analogous formulae can be shown to hold for all of the QFIM’s (that is, for any choice of
monotone function f). The general relationship is given by,
Funitary
µν
{f} = 2
π
Z ∞
0
dω 1 −e−βω
f(e−βω) χ′′
µν.
(75)
The connection between quantum geometry and response functions has been exploited in a
number of proposed [45–47] and realized [48–50] experiments.
This connection is particularly signiﬁcant in light of the relationship between quantum
geometry and entanglement, which we discuss further in Sec. III 3. In particular, the low
temperature properties of the entanglement are strongly controlled by the static structure
factor, as demonstrated in Ref. [51] and Ref. [52].
2.
The Quantum Variance
In the introduction, we promised that quantum information geometry would oﬀer us a
means by which to separate the classical ﬂuctuations from the quantum ﬂuctuations in a
given system. The quantum covariance [53, 54] makes this concept explicit by decomposing
the covariance between two operators as,
Covˆρ{ˆΛµ, ˆΛν} = CovQ
ˆρ {ˆΛµ, ˆΛν} + CovC
ˆρ{ˆΛµ, ˆΛν}
(76)
where CovQ
ˆρ {·, ·} is the quantum component of the covariance and CovC
ˆρ{·, ·} is the classical
component of the covariance.
To deﬁne the classical covariance, we begin by considering the mixed state deﬁned in
Eq. 9,
ˆρ = e−β ˆH
Z
(77)
where,
Z = Tr{e−β ˆH}
(78)
which can be used to describe any mixed state for a suitable choice of ˆH. Now add to the

28
Hamiltonian the operators ˆ⃗Λ which are coupled via the parameters ˆ⃗λ
ˆρ(⃗λ) = e−β( ˆH+⃗λ·ˆ⃗Λ)
Z
(79a)
Z(⃗λ) = Tr
n
e−β( ˆH+⃗λ·ˆ⃗Λ)o
.
(79b)
Using the free energy,
F = −1
β ln(Z(h)).
(80)
we can now deﬁne the classical covariance as,
CovC
ˆρ{ˆΛµ, ˆΛν} = 1
β
∂2F
∂λµ∂λν
(81)
where the second derivative of the free energy can be recognized as the static thermal
susceptibility. The quantum covariance can now be deﬁned directly by rearranging Eq. 76,
CovQ
ˆρ {ˆΛµ, ˆΛν} = Covˆρ{ˆΛµ, ˆΛν} −CovC
ˆρ{ˆΛµ, ˆΛν}
(82)
To understand the meaning of this deﬁnition, let’s ﬁrst consider the case where
h
ˆH, ˆ⃗Λ
i
=
0.
From the perspective of information geometry, this kind of perturbation will change
the eigenvalues of the density matrix, but not the eigenvectors. In this case, the classical
covariance will be precisely the covariance, and the quantum covariance will be zero.
In the case where the commutator of the Hamiltonian with the perturbation is non-zero.
The evaluation of the second derivative in Eq. 81 must now be performed by taking a Taylor
series expansion of the exponential about the point where ⃗λ = ⃗0. In general the thermal
covariance will be strictly less than the total covariance (except at inﬁnite temperature),
and the diﬀerence is made up for by the quantum ﬂuctuations quantiﬁed by the quantum
covariance. Notice that the quantum variance is just the diagonal of the quantum covariance
matrix,
VarQ
ˆρ {ˆΛµ} ≡CovQ
ˆρ {ˆΛµ, ˆΛµ}
(83)
The physical meaning of the quantum covariance at ﬁnite temperature can be gleaned
by considering the case of a free particle in equilibrium with a thermal bath. The quantum

29
variance associated with the position operator is then [53],
VarQ
ˆρ (ˆx; ˆρ) =
1
24πλ2
dB
(84)
where λdB =
q
2πℏ2
mkBT is the thermal de Broglie wavelength. It decreases monotonically to
zero as the temperature increases, and thus indicates the falling oﬀof quantum ﬂuctuations
at ﬁnite temperature.
The quantum covariance appears to be behaving exactly as we would expect the unitary
component of the QGT to behave. To better understand the relationship between the two,
we recall a special case of the QGT, the Wigner-Yanase-Dyson (WYD) skew information,
[55, 56],
Iα,ˆρ(ˆΛµ, ˆΛν) = −1
2 tr
h
ˆΛµ, ˆραih
ˆΛν,
ˆ
ρ1−α
i
.
(85)
This QGT corresponds to the choice of monotone function [40],
fα(x) = α(1 −α)
(x −1)2
(xα −1)(x1−α −1).
(86)
It is shown in Ref. [53] that the quantum covariance may be written as,
CovQ
ˆρ (ˆΛµ, ˆΛν) =
Z 1
0
dα Iα(ˆΛ; ˆρ).
(87)
So we see that the quantum variance is the average over a particular family of QGT’s. The
diagonal entries of the SLD QFIM are bounded from above and below by the diagonal entries
of the quantum covariance (recall that the Berry curvature doesn’t have diagonal entries),
4VarQ
ˆρ (ˆΛµ) ≤Fµµ ≤12VarQ
ˆρ (ˆΛµ)
(88)
Using the representation of the WYD information in terms of response functions, Eq. 87
can be written as [57],
VarQ(ˆΛ; ˆρ) = ℏ
Z ∞
0
dω
π L
βℏω
2

χ′′
ˆΛ(ω)
(89)

30
where,
L (x) = coth(x) −1
x
(90)
is the Langevin function. We thus see that the quantum variance corresponds directly to
the unitary component of the QFIM described in Eq. 65, making explicit the fact that this
component corresponds to quantum ﬂuctuations by subtracting oﬀthe thermal ﬂuctuations
that would give rise to the non-unitary component of the QFIM in Eq. 64.
To understand the diﬀerence between the quantum variance and SLD QFIM we consider
a single qubit in thermal equilibrium with a bath of inverse temperature β. We denote
explicitly the gap of the qubit with ∆.
ρ = 1
Z e−β∆σz
(91)
It is easy to rewrite the density matrix explicitly as,
ρ = 1
2(I −tanh(|∆|β)ˆσz)
(92)
We consider parameterizations of the Hilbert space generated unitarily by the magnetization
operator in direction ˆn,
ˆΛ(θ, φ) = ˆn · ⃗σ = cos(θ) sin(φ)ˆσx + sin(θ) sin(φ)ˆσy + cos(φ)ˆσz
(93)
Below we calculate the variance, its quantum and thermal components, and the QFI as a
function of β, θ, and φ. It is helpful for this purpose to compute the partition function with
a source term,
Z(h) = tr{exp

−β(∆ˆσz −hˆΛ(θ, φ))

}
The variance, QV (from Eq. 82), and QFI (from Eq. 65), for φ = π
2 and φ = π
4 are shown
in Fig. 3 with details of the calculations given in App. B 1. We can see that regardless of
the orientation, the variance will always saturate at its maximal value in the limit of inﬁnite
temperature (β →0). If we imagine the Bloch sphere with its interior as the prototypical
representation of the space of pure and mixed states, then the inﬁnite temperature limit

31
corresponds to the center of this sphere. From an information theory perspective, it is the
equivalent of a black hole, as all states taken to this limit become indistinguishable. This
manifests in the QFI going to zero. We see that the quantum variance also tends to zero.
While both the QV and QFIM must go to zero in this limit, they do so at slightly diﬀerent
rates. Their diﬀerence is not constant but peaks at some value of β that scales with the
energy gap of the system.
0
1
2
3
4
4Var(ˆΛ(φ = π
2))
F(ˆΛ(φ = π
2))
4VarQ(ˆΛ(φ = π
2))
0
2
4
6
8
10
T
0
1
2
3
4
4Var(ˆΛ(φ = π
4))
F(ˆΛ(φ = π
4))
4VarQ(ˆΛ(φ = π
4))
FIG. 3: Plots of the variance, quantum variance, and quantum Fisher information for two
choices of the magnetization operator.
3.
Quantum Fisher Information and Entanglement
Among the most signiﬁcant aspects of the geometrical structure of quantum state space
is its explicit relationship to multipartite entanglement [58]. The diagonal components of
the QFIM may be related explicitly to bounds on the amount of multipartite entanglement.
These bounds are signiﬁcant for three reasons. First, they apply even at ﬁnite tempera-
ture. Second, they are directly accessible through the linear response functions discussion
in Sec. III 1. Third, the detection of entanglement depends in particular on the direction
in state space that we choose to examine, thus allowing us to consider a richer structure

32
of entanglement than would be accessible if we only considered the von Neumann entropy.
Below we derive the bounds explicitly following the discussion in Ref. [58].
Consider an N body system and a Hermitian operator composed of a sum of one-body
operators,
ˆΛ =
N
X
j=1
ˆAj
(94)
each with maximum eigenvalue amax, minimum eigenvalue amin. Let ∆a = amax −amin be
the spectral width. The ﬁrst bound we derive relies only on the component of the QFIM
associated with the operator in Eq. 94, which we hereafter refer to simply as the QFI, F,
omitting the indices for simplicity. We also note the following
a The set of k-producible states is convex.
That is to say the producibility is non-
increasing under mixture.
b The Fisher information is convex under mixture, F{pˆρ1 + (1 −p)ˆρ2} ≤pF{ˆρ1} + (1 −
p)F{ˆρ2} for p ∈[0, 1]
c For an N body state |Ψ⟩= |ψA⟩⊗|ψB⟩we have Var|Ψ⟩(ˆΛ) = Var|ψA⟩(ˆΛ(A)) +
Var|ψB⟩(ˆΛ(B))
d For an N body pure state |Ψ⟩, 4Var|Ψ⟩(ˆΛ) ≤∆aN 2 with the maximum achieved by
the GHZ state,
|GHZN⟩= 1
√
2

|amin⟩⊗N + |amax⟩⊗N
(95)
By (a) and (b) the maximal QFI will occur for pure states so we limit our discussion to that
case. Now we consider the k producible state,
|Ψ⟩=
M
O
ℓ=1
|ψℓ⟩
(96)
where each factor |ψℓ⟩contains Nℓparticles and the largest Nl is equal to k. For a given
value of k, the largest possible QFI is achieved by a state with s = ⌊N
k ⌋factors in a |GHZk⟩
state with the remaining particles in a |GHZr⟩state where r = N −sk. The QFI for a
k-producible state is thus bounded from above,
F ≤∆a(sk2 + r2)
(97)

33
with the bound for separable states given by,
F(ˆρseparable) ≤N
(98)
In Ref. [58], a second inequality is derived which is aimed speciﬁcally at systems whose
degrees of freedom can be represented in terms of the spin. Let’s consider the case where
the local operators in Eq. 94 are given by,
Aj = nj · Sj
(99)
where nj = (cos(θj) sin(φj), sin(θj) sin(φj), cos(φj)) where θ ∈[0, 2π) and φ ∈[0, π). Let us
consider the case where the orientation on each site is the same by dropping the subscript j
on the unit vector. We denote the corresponding generator, ˆΛn and the corresponding QFI
Fn. We can now consider the direction averaged QFI,
¯F =
Z
P(n)Fn sin(φ)dθdφ
(100)
where P(n) is a function such that 1 =
R
P(n) sin(φ)dθdφ. If we choose the case of P(n) =
δ(n −a) we ﬁnd the QFI for the particular orientation a of the generator and the inequality
in Eq. 97 would apply. Instead we consider the uniform case were P(n) =
1
4π, for which
¯F = Fx + Fy + Fz
3
(101)
In Ref. [58] the above expression is given the following bound for k-producible states,
¯F(ˆρk-producible) = 1
3
 s
 k2 + 2k −δk,1

+ r2 + 2r −δr,1

(102)
where s = ⌊N
k ⌋and r = N −sk are deﬁned as earlier. For fully separable states the above
bound reduces to,
¯F(ˆρseparable) = 2
3N
(103)
To illustrate the applications of Eq. 97 and Eq. 103 we consider the case of 2 partite states.
In the following discussion, we consider the QFI density, which we indicate with f = F
N or
¯f =
¯
F
N .

34
The extent to which the above inequalities detect entanglement depend sensitively on the
choice of the operator ˆΛ in Eq. 94, or equivalently, on the direction in state space in which
we choose to perturb the state. To see this consider the following two states,
|ψ(α)⟩= cos(α) |↑A↓B⟩−sin(α) |↓A↑B⟩,
(104a)
|φ(α)⟩= cos(α) |↑A↓B⟩+ sin(α) |↓A↓B⟩
(104b)
on the interval α ∈[0, π
2]. At α = π
4 the ﬁrst state is the singlet which we know is maximally
entangled.. This state is maximally entangled, containing one bit of entanglement entropy,
as can be seen from the solid red curve in Fig. 4. The second state is separable for all
possible values of λ and therefor has an entanglement entropy of zero. For both states we
can compute the QFI associated with the parameter λ and ﬁnd that they are identical. The
QFI associated with this parameter tells us nothing about the entanglement content of the
state. It’s easy to see that any single parameter family of states that interpolates smoothly
between two orthogonal states in the form given about will always yield the QFI,
Fαα = sec2(α) csc2(α)
(105)
To distinguish between entangled and separable spin states we must consider a path
generated by an operator that is local in the spin degrees of freedom. Even for two spins
there is considerable freedom in which operators we might choose. Two natural choices are
the ferromagnetic and anti ferromagnetic combinations of each of the three spin operators,
ˆΛ±
µ = ˆSµ
A ± ˆSµ
B
(106)
where µ ∈{x, y, z}.
Each of these generators deﬁnes a unitary operator, which induces an inﬁnitesimal dis-
placement in the state along a path parameterized by λ,
|α, dλµ⟩= eidλµΛ±
µ |α⟩
(107)

35
0.0
0.1
0.2
0.3
0.4
0.5
α
π
0.0
0.2
0.4
0.6
0.8
1.0
SA
0
20
40
60
80
100
Fαα
SA{ψ(α)}
SA{φ(α)}
Fαα{ψ(α)}
Fαα{φ(α)}
FIG. 4: Fisher information and Entanglement entropy of the states in Eq. 104a and
Eq. 104b. Here the Fisher information is taken with respect to the parameter λ itself, and
is identical for both states. The EE for |ψ⟩attains a maximum of one bit when the state is
maximally entangled, while the EE for |φ⟩is always zero.
The distance between |λ, 0⟩and |λ, dh⟩is then given by (following Eq. 53),
F±
µµ{ψ} = 4Covψ(ˆΛ±
µ , ˆΛ±
µ )
(108)
The QFI densities generated by the ferromagnetic and anti-ferromagnetic operators are give
in Fig. 5.
While the separable state, |φ⟩never exceeds the bound given in Eq. 98, we can see the for
the entangled state |ψ⟩the bound is exceeded by three of the diﬀerent paths chosen. First,
for the path given in terms of the z component for the anti-ferromagnetic spin combination,
bi-partite entanglement is detected for a window the parameter space λ ∈
π
8, 3π
8

. The
bound is also always violated for the parameterizations generated by the anti-ferromagnetic
x and y operators for the entire span of λ except at the separable points.
The three ferromagnetic operators deﬁned in Eq. 106 (or, equivalently, the three anti-
ferromagnetic spin operators), are related to each other by global rotations. We can thus
consider the QFIM corresponding to the three-dimensional submanifold whose parameteri-
zation is generated by these operators and take its trace. The results for the ferromagnetic

36
0.0
0.5
1.0
1.5
2.0
f −
µµ
ψ
φ
f −
xx
f −
yy
f −
zz
0.0
0.1
0.2
0.3
0.4
0.5
α
π
0.0
0.5
1.0
1.5
2.0
f +
µµ
0.0
0.1
0.2
0.3
0.4
0.5
α
π
FIG. 5: Diagonal entries of the QFIM matrix associated with the three ferromagnetic
(blue) and anti-ferromagnetic (red) generators deﬁned in Eq. 106. The three line styles
correspond to the x (dashed), y (dotted), and z (solid) lab frame directions. The grey
region indicates the bound given in Eq. 103.
and anti-ferromagnetic operators are given in Fig. 6. The bound corresponding to Eq. 103 is
again given in grey. Here we see that only the anti-ferromagnetic spin combinations detect
entanglement.
Notice that at the point α = π
4 all three components of the QFIM for the ferromagnetic
combination are zero. The singlet is an eigenstate of the magnetization operator, having
total magnetization zero in any of the three spin directions. This special case implies a
general principle. If a state |ψ⟩is an eigenstate of the generator of a parameterization, then
its QFI along that parameterization will be zero. This is easy to see if one considers the fact
that the QFI for a pure state is just the covariance of the generator for that state.
More generally, if the Hamiltonian of a system commutes with the generator, then the
ground state of that Hamiltonian will tend to exhibit zero QFI. An exception is in the case
where the ground state is degenerate. For example, the ferromagnetic Heisenberg chain has a

37
0.0
0.1
0.2
0.3
0.4
0.5
α
π
0.0
0.5
1.0
1.5
2.0
¯f
ψ
0.0
0.1
0.2
0.3
0.4
0.5
α
π
φ
¯f −
¯f +
FIG. 6: Trace of the QFIM matrix for the three symmetric (blue) and antisymmetric (red)
generators deﬁned in Eq. 106. The grey region indicates the bound on entanglement given
in Eq. 103 for detecting 2-partite entanglement. Notice that only the symmetric choice
detects the bi-partite entanglement in the state |ψ⟩.
three-fold degenerate ground state manifold, which can be used to construct entangled states
for which the QFI associated with the ferromagnetic generator does detect entanglement.
4.
Fidelity Susceptibility
The geometrical structure of the Hilbert space introduced in Sec. II applies to any pa-
rameterization of the state space.
One case of particular physical interest is when that
parameterization is generated by considering the ground state of the Hamiltonian,
ˆH(⃗Λ) = H0 + ⃗λ · ˆ⃗Λ
(109)
where the parameters ⃗λ can be used to drive phase transitions in the ground state manifold.
In this case, the QFIM is called the ﬁdelity susceptibility [59–61] (up to a conventional factor
of four, which we omit). It is also possible to consider the ﬁdelity susceptibility with respect
to parameters that are not driving the phase transition, such as a twist in the boundary
conditions [62–64], in which case it would be more appropriate to associate it with the QFIM.
To construct the QGT explicitly, let |Ψ0(λ)⟩be the ground state of Eq. 109 (we consider

38
one component for simplicity). The inﬁnitesimally neighboring state |Ψ(λ + dλ)⟩can be
found by applying perturbation theory,
|Ψ0(λµ + dλµ)⟩= |Ψ0(λµ)⟩+ dλµ
X
n̸=0
⟨Ψn| ˆΛµ |Ψ0⟩|Ψn⟩
En −E0
(110)
where, for simplicity, we consider one parameter at a time. Then, applying Eq. 49 and
taking the real part, we recognize the QGT as,
˜Qµν =
X
n̸=0
⟨Ψ0| ˆΛµ |Ψn⟩⟨Ψn| ˆΛν |Ψ0⟩
(En −E0)2
(111)
Provided that we avoid level crossings, the change in the parameters λ can be represented
by the unitary operator [9],
ˆU =
X
n
Ψ(⃗λ + d⃗λ)
E D
Ψ(⃗λ)

(112)
We may identify the associated Hermitian operator via,
ˆ˜Λµ = i(∂µ ˆU) ˆU
(113)
and rewrite the QGT in the covariance form,
˜Qµν = CovΨ0
nˆ˜Λµ, ˆ˜Λν
o
(114)
We denote the above QGT with a tilde in order to make a distinction between two diﬀerent
but related approaches to quantum metrology.
Both approaches involve ﬁrst deﬁning a
Hermitian operator ˆΛ. In the ﬁrst approach, the associated unitary exp
n
−iλˆΛ
o
is applied
directly to our initial state. The resulting QGT is denoted Qµν, where
Qµν = ⟨Ψ0| ˆΛµˆΛν |Ψ0⟩−⟨Ψ0| ˆΛµ |Ψ0⟩⟨Ψ0| ˆΛν |Ψ0⟩
= CovΨ0
n
ˆΛµ, ˆΛν
o
.
(115)
In the second approach, the initial state is taken to be the ground state of a Hamiltonian to

39
which ˆΛ is applied as a perturbation. This is the QGT denoted ˜Qµν in Eq. 114. The two
are in fact related by a series of inequalities [9] which we reproduce here for completeness.
Let ϵnm = En −Em and let ϵ10 be the smallest spectral gap. Now we may rewrite Eq. 111
as
˜Qµν =
X
n̸=0
⟨Ψ0| ˆΛµ |Ψn⟩⟨Ψn| ˆΛν |Ψ0⟩
ϵ2
n0
=
1
ϵ2
n′0
X
n̸=0
ϵ2
10
ϵ2
n0
⟨Ψ0| ˆΛµ |Ψn⟩⟨Ψn| ˆΛν |Ψ0⟩
≤1
ϵ2
10
X
n̸=0
⟨Ψ0| ˆΛµ |Ψn⟩⟨Ψn| ˆΛν |Ψ0⟩
= 1
ϵ2
10
 X
n
⟨Ψ0| ˆΛµ |Ψn⟩⟨Ψn| ˆΛν |Ψ0⟩−⟨Ψ0| ˆΛµ |Ψ0⟩⟨Ψ0| ˆΛν |Ψ⟩0
!
= 1
ϵ2
10
Qµν
(116)
Thus we ﬁnd that, ˜Qµν ≤
1
ϵ2
10Qµν.
At a level crossing critical point, we expect the QGT to diverge. The nature of this
divergence was determined in Ref. [65–67]. In order to analyze the scaling of the QGT it is
useful to consider the QGT density,
˜qµν = L−d ˜Qµν
(117)
where d is the spatial dimension of the system. Following Ref. [65], we consider a scale
transform x →αx and τ →αζτ. Here ζ is the dynamical critical exponent. Let ∆µ be the
scaling dimension of the operator ˆΛµ. If λc is the critical point, the correlation length will
diverge as ξ ∼|λ −λc|−ν. In the region approaching the critical point the QGT density will
diverge as,
˜qµν ∼|λ −λc|ν(∆µ+∆ν−2ζ−d)
(118)
This also leads to a ﬁnite size scaling relation at the critical point itself where,
˜qµν ∼L−(∆µ+∆ν−2ζ−d).
(119)
If the operator ˆΛµ drives the transition, as is the case when considering the ﬁdelity sus-

40
ceptibility, then ∆µ = d + ζ −1/ν and the diagonal terms in Eq. 119 become [66, 67]:
˜q ∼L2/ν−d.
(120)
These scaling relations can be contrasted with the scaling of the QGT for a unitary per-
turbation generated directly by the operator ˆΛ. In this case the dynamical critical exponent
doesn’t enter in, and we instead have,
qµν ∼|λ −λc|ν(∆µ+∆ν−d)
(121)
and at the critical point itself the ﬁnite size scaling,
qµν ∼L−(∆µ+∆ν−d).
(122)
As above, if we assume ∆µ = ∆ν = 2 + ζ −1/ν, we obtain for the diagonal term:
q ∼L2/ν−d−2ζ.
(123)
For comparison, it is useful to note that the closely related energy susceptibility, χe =
−∂2e0/∂λ2, with e0 the ground-state energy per site, scales as
χe ∼L2/ν−d−ζ.
(124)
This result follows immediately from the scaling of the critical free energy density, fs ∼
|λ −λc|ν(d+ζ), with χe the second derivative of fs. Notice that the dynamical critical ex-
ponent appears in Eq. 118 and Eq. 119 but not in Eq. 121 and Eq. 122. This is due to
the fact that the two metrics consider diﬀerent parameterizations. In particular, the metric
˜Qµν determines the distance between the ground state of a Hamiltonian at some value of
the Hamiltonian parameter λµ and some nearby value λµ + dλµ, where λµ couples to the
operator ˆΛµ. The metric Qµν, however, determines the distance between a state |ψ⟩and
the inﬁnitesimally neighboring state exp
n
−idλµˆΛµ
o
|ψ⟩. As we describe in Eq. 113, the
perturbation to the Hamiltonian may be recast as a unitary transformation on the ground
state provided that the change in parameters occurs adiabatically. The contribution of the

41
dynamical critical exponent in the Fidelity susceptibility reﬂects the necessity of the trans-
formation being adiabatic if we are to keep ourselves in the ground state. This diﬀerence in
scaling turns out to have important implications for quantum sensing [68].
The above scaling discussion coupled with the relationship between the QFIM and gen-
uine multipartite entanglement implies that as long as ˆΛ is relevant in the sense of the
renormalization group, the associated entanglement will diverge.
5.
Quantum Sensing
Quantum sensing is the utilization of quantum resources, such as entanglement, to en-
hance the precision and sensitivity of measurements [11, 69]. The example of the weather
stations describing in Sec. III can be mapped directly onto a general quantum framework,
whereby a probe is made to interact with a system for some amount of time, during which
the probe acquires a parameter dependent phase. The probe is then measured and the phase
estimated. For an extensive review of quantum sensing see Ref. [70].
The bounds in Eq. 98 and Eq. 103 imply that the sensitivity of a probe to quantum mea-
surements is enhanced by the presence of entanglement. This fact can be used to exploit
quantum critical regions for increased measurement sensitivity, as discussed in Ref. [71]. As
mentioned in Sec. III 4, the diﬀerence in the scaling relations for parameterizations controlled
adiabatically by a parent Hamiltonian, and those generated directly by a bounded unitary
operator lead to diﬀerent scalings in measurement sensitivity at the critical point [68]. Topo-
logical spins chains [72] and atomic ensembles have also been proposed as useful platforms
for quantum sensing due to their geometrical properties [73]
Recent work has also examined the applications of the QFIM to the detection of peri-
odically oscillating magnetic ﬁelds, using spin chains as intermediates [74]. Recently, the
dynamical phase corresponding to a time crystal has also been shown to have a robust
quantum advantage in measurement sensitivity.

42
6.
Euler Characteristic, Curvature, and Many-body Phases
In the context of quantum many-body phases the QFIM has been applied in several
diﬀerent contexts. Topological insulators, for example, have long been known to be char-
acterized by a non-trivial Chern number when considering a single particle state integrated
over the ﬁrst Brillouin zone [75]. Studies have found that the Euler Characteristic in the
ﬁrst Brillouin zone is also indicative of topologically non-trivial phases [5, 76]. Recent work
used ideas from quantum geometry to classify all N level fermionic and bosonic systems
according to a generalization of Bloch-sphere [4].
The curvature associated with diﬀerent many-body phases has also been examined in
several contexts [12, 77, 78]. While the physical implications of the state space curvature are
not yet fully understood, it has been shown to clearly indicate quantum criticality, including
in systems that lack a local order parameter [79]. In Ref. [77] it was shown that the state
space curvature–with inverse temperature as one parameter, and the driving parameter of
a quantum phase transition as the other–detects diﬀerent scaling regimes in the vicinity of
the critical point.
In Ref. [78] some progress was made in interpreting the state space curvature through its
apparent relationship to multipartite entanglement. By considering spherical slices of state
space generated by perturbing the Hamiltonian with the staggered magnetization operator,
ˆΛn(θ,φ) =
X
r
(−1)rn(θ, φ) · S.
(125)
where ˆn = (cos(θ) sin(φ), sin(θ) sin(φ), cos(φ)), coupled to a source Hamiltonian via the ﬁeld
h,
H = H0 + hˆΛn(θ,φ)
(126)
The size of these manifolds can then be computed using the formula,
Vψ =
Z
Ω
q
det(Fµν {ψ(θ, φ)})dθdφ
(127)

43
where Fµν is the metric for the sphere with tangent operators,
ˆΛθ =
X
r
(−1)r∂θn · Sr
(128)
ˆΛφ =
X
r
(−1)r∂φn · Sr
(129)
and the curvature is estimated by comparing the scaling of this volume as a function of the
coupling ﬁeld to the scaling of the volume (surface area) of a sphere in ﬂat space. For the
2-manifold parameterized by the orientation of n, we expect the volume to scale as,
Vﬂat = 4π(ah)2
(130)
where a is some constant that depends on the spin moment. The ratio of the volume centered
on the ground state, ψ of the source Hamiltonian in Eq. 126 to the volume ﬂat space is then
given by,
Vψ
Vﬂat
≈1 −bRh2
(131)
with b a constant and R the scalar curvature.
In the XY chain, the Euler characteristic was examined for a manifold parameterized
by the spin anisotropy and by a global rotation. The topology of the resulting manifold
was shown to change depending on whether or not the model was in a paramagnetic or
ordered phase, with the former phase resulting in an irrational Euler characteristic due to
the emergence of a cusp like singular [80].
Another application of quantum curvature is in the context of variational quantum Monte
Carlo, where the QFIM is used to ﬁnd the energetic minimum using stochastic reconﬁgu-
ration [81]. In this case, the local curvature of the state space can be used to infer the
robustness of the variational calculation, as discussed in Ref. [82].
IV.
SUMMARY AND OUTLOOK
In this paper, we have presented a pedagogical introduction to information geometry.
Though the formal foundations of information geometry were laid over a century ago, it is
in the last two decades that we have seen a rapid growth in research interest in the many ﬁelds

44
to which this geometry is applicable. In particular, there are now an abundance of studies
examining Fidelity susceptibility and Fisher information in various models from condensed
matter systems to cold atom setups. We expect that this interest will only grow. Beyond
equilibrium physics, concepts from quantum information geometry are ﬁnding application
in the study of dynamical phases [83] and quantum chaos [84].
There is still much to be learned about the geometrical structure of quantum state space
and in particular about interpretations of geometric invariants such as the curvature as
discussed in
[12].
If one recalls Eq. 42, we see that in classical information geometry,
the metric is sourced by the Shannon information. In quantum information geometry, the
analogous quantity is the K¨ahler potential (see, for example, Ref. [24]). Investigation of this
quantity in many-body systems is only just beginning, and there are no doubt many exciting
results to be found in this direction.
Appendix A: Properties of Shannon Information
In this appendix we discuss some properties of information measures.
We use P =
{p1, ..., pN} to denote a discrete distribution. The Shannon information deﬁned in Eq. 19
has the following properties [24],
• Positivity, S(P) ≥0
• Continuity, S(P) will vary continuous with the distribution P.
• Expansibility, S(p1, ..., pN) = S(p1, ..., pN, 0).
• Concavity, S(αP1 + (1 −α)P2) ≥αS(P1) + (1 −α)S(P2). The mixing of distributions
always increases the information.
• Subadditivity For two random variables, not necessarily independent, where p12 is the
joint probability distribution,
S(P12) ≤S(P1) + S(P2)
(A1)
with equality only if the two distributions are independent.

45
• Recursion. We can coarse grain a distribution P = p1, . . . , pN, via,
qj =
kj
X
i=kj−1+1
pi
(A2)
where j = 1, . . . , r and k0 = 0. The discrete distribution has been partition according
to the sum N = Pr
i=1 ki. Then the Shannon information is given by,
S(P) = S(Q) + q1S
p1
q1
, . . . , pk1
q1

+ . . . + qrS
pkN−kr+1
qr
, . . . , pN
qr

(A3)
It is the property of recursion that makes the Shannon information unique amongst all
possible choices of information measure. It appears in many other interesting contexts. In
particular in the ﬁeld of signal transmission where it was originally developed.
Appendix B: Example Calculations
1.
Single Qubit
We may compute the trace explicitly by diagonalizing the Hamiltonian with source term.
We denote the coeﬃcients of the magnetization operator as mα (no hat) and temporarily
suppress the explicit dependence on θ and φ
ˆH .=

∆−hmz
h(mx −imy)
h(mx + imy) −(∆−hmz)


We can immediately read oﬀthe eigenvalues for this matrix,
λ± = ±
q
(∆−hmz)2 + h2(mx −imy)(mx + imy)
= ±
p
∆2 −2∆mzh + h2
and thus ﬁnd,
Z(h) = 2 cosh

β
p
∆2 −2∆mzh + h2

(B1)

46
The variance is given by,
Var( ˆ
M) = tr

ρ ˆ
M 2
−tr

ρ ˆ
M
2
= 1 −tanh2(β|∆|) cos2(φ)
(B2)
The thermal variance can be evaluated by ﬁrst computing the ﬁrst and second derivatives
of the partition function with respect to the driving ﬁeld,
∂h=0Z = ∂h=02 cosh

β
p
∆2 −2∆mzh + h2

= 2β
 
sinh
 β√∆2 −2∆mzh + h2
√∆2 −2∆mzh + h2
(h −∆mz)
!
h=0
= −2βsgn(∆)mz sinh(β|∆|)
(B3a)
∂2
h=0Z =
 
2β2cosh
 β√∆2 −2∆mzh + h2
∆2 −2∆mzh + h2
(h −∆mz)2
−2β sinh
 β√∆2 −2∆mzh + h2
(∆2 −2∆mzh + h2)
3
2
(h −∆mz)2
+ 2β sinh
 β√∆2 −2∆mzh + h2
√∆2 −2∆mzh + h2
!
h=0
= 2β2 cosh(β|∆|)m2
z −2β sinh(β|∆|)
|∆|
m2
z + 2β sinh(β|∆|)
|∆|
= 2β2 cosh(β|∆|)m2
z + 2β(1 −m2
z)sinh(β|∆|)
|∆|
(B3b)
No we substitute these relations into the deﬁnition of the thermal variance which we restate
below,
VarT ( ˆ
M; ˆρ) = 1
β2
 
1
Z(h = 0)
∂2Z
∂h2

h=0
−
1
Z2(h = 0)
∂Z
∂h
2
h=0
!
= 1
β2

β2m2
z + β(1 −m2
z)tanh(β|∆|)
|∆|
−β2m2
z tanh2(β|∆|)

= m2
z + (1 −m2
z)tanh(β|∆|)
β|∆|
−m2
z tanh2(β|∆|)
= m2
z(1 −tanh2(β|∆|)) + (1 −m2
z)tanh(β|∆|)
β|∆|
(B4)

47
The QV can be computed by substituting Eq. B2 and Eq. B4 into Eq. 82,
VarQ( ˆ
M; ˆρ) = 1 −tanh2(β|∆|)m2
z −m2
z(1 −tanh2(β|∆|)) −(1 −m2
z)tanh(β|∆|)
β|∆|
= (1 −m2
z)

1 −tanh(β|∆|)
β|∆|

(B5)
Using Eq. 65, we can calculate an explicit form of the QFI for the single qubit as well.
Since diagonal entries don’t contribute to the QFI, we consider only oﬀdiagonal terms, and
denote by λ± = ±∆the two eigenvalues and eigenstates of the Hamiltonian. We denote the
corresponding eigenstates of σz by |±⟩. The QFI given by,
F( ˆ
M; ˆρ) = 2
X
λ,λ′
(pλ −pλ′)2
pλ + pλ′
⟨λ| ˆ
M |λ′⟩

2
= 4(p+ −p−)2
p+ + p−
⟨+| ˆ
M |−⟩

2
= 4
Z
(e−β|∆| −eβ|∆|)2
e−β|∆| + eβ|∆| (m2
x + m2
y)
= 4
Z
4 sinh2(β|∆|)
2 cosh(β|∆|) (1 −m2
z)
= 4 tanh2(β|∆|)(1 −m2
z)
= 4 tanh2(β|∆|) sin2(φ)
(B6)
[1] Shun-ichi Amari and H. Nagaoka. Methods of Information Geometry. American Mathematical
Society and Oxford University Press, 2000.
[2] Shun-ichi Amari. Information Geometry and its Applications. Springer, 2016.
[3] B. Roy Frieden. Science from Fisher Information. Cambridge University Press, 1998.
[4] Cameron JD Kemp, Nigel R Cooper, and F Nur ¨Unal. Nested spheres description of the n-
level chern number and the generalized bloch hypersphere. arXiv preprint arXiv:2110.06934,
2021.
[5] Anwei Zhang.
Revealing chern number from quantum metric.
Chinese Physics B,
31(4):040201, 2022.

48
[6] Tomoki Ozawa and Bruno Mera. Relations between topology and the quantum metric for
chern insulators. arXiv preprint arXiv:2103.11582, 2021.
[7] Mikhail Prokopenko, Joseph T Lizier, Oliver Obst, and X Rosalind Wang. Relating ﬁsher
information to order parameters. Physical Review E, 84(4):041116, 2011.
[8] Philipp Hauke, Markus Heyl, Luca Tagliacozzo, and Peter Zoller. Measuring multipartite
entanglement through dynamic susceptibilities. Nature Physics, 12(8):778–782, 2016.
[9] Paolo Zanardi, Paolo Giorda, and Marco Cozzini. Information-theoretic diﬀerential geometry
of quantum phase transitions. Physical review letters, 99(10):100603, 2007.
[10] Johannes Lang, Bernhard Frank, and Jad C. Halimeh. Dynamical quantum phase transitions:
A geometric picture. Phys. Rev. Lett., 121:130603, Sep 2018.
[11] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum metrology. Physical review
letters, 96(1):010401, 2006.
[12] Johanna Erdmenger, Kevin Grosvenor, and Ro Jeﬀerson. Information geometry in quantum
ﬁeld theory: lessons from simple examples. SciPost Physics, 8(5):073, 2020.
[13] Kok Chuan Tan, Varun Narasimhachar, and Bartosz Regula. Fisher information universally
identiﬁes quantum resources. Physical Review Letters, 127(20):200402, 2021.
[14] George D Birkhoﬀ. Proof of the ergodic theorem. Proceedings of the National Academy of
Sciences, 17(12):656–660, 1931.
[15] In fact the particular formalism is not so important. We can deﬁne quantum systems in
classical state space through the Wigner quasi probability distribution function, [?
?
],
but, the Kolmogorov axioms must be relaxed to allow for negative probabilities. We can also
deﬁne quantum states in real spaces as opposed to complex spaces, by imposing additional
constraints on the allowed observables [24].
[16] Leon Gordon Kraft. A device for quantizing, grouping, and coding amplitude-modulated pulses.
PhD thesis, Massachusetts Institute of Technology, 1949.
[17] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical
journal, 27(3):379–423, 1948.
[18] Alfr´ed R´enyi et al. On measures of entropy and information. In Proceedings of the fourth
Berkeley symposium on mathematical statistics and probability, volume 1. Berkeley, California,
USA, 1961.

49
[19] Ronald A Fisher. On the mathematical foundations of theoretical statistics. Philosophical
transactions of the Royal Society of London. Series A, containing papers of a mathematical
or physical character, 222(594-604):309–368, 1922.
[20] C Radhakrishna Rao. Information and the accuracy attainable in the estimation of statistical
parameters. Reson. J. Sci. Educ, 20:78–90, 1945.
[21] Solomon Kullback and Richard A Leibler. On information and suﬃciency. The annals of
mathematical statistics, 22(1):79–86, 1951.
[22] Ivan N Sanov. On the probability of large deviations of random variables. United States Air
Force, Oﬃce of Scientiﬁc Research, 1958.
[23] Anil Bhattacharyya.
On a measure of divergence between two multinomial populations.
Sankhy¯a: the indian journal of statistics, pages 401–406, 1946.
[24] Ingemar Bengtsson and Karol ˙Zyczkowski. Geometry of quantum states: an introduction to
quantum entanglement. Cambridge university press, 2017.
[25] Christoph Holzhey, Finn Larsen, and Frank Wilczek. Geometric and renormalized entropy in
conformal ﬁeld theory. Nucl. Phys. B, 424:443–467, 1994.
[26] Pasquale Calabrese and John Cardy. Entanglement entropy and quantum ﬁeld theory. Journal
of Statistical Mechanics: Theory and Experiment, 2004(06):P06002, jun 2004.
[27] Pasquale Calabrese and John Cardy. Entanglement entropy and conformal ﬁeld theory. Jour-
nal of physics a: mathematical and theoretical, 42(50):504005, 2009.
[28] Jens Eisert, Marcus Cramer, and Martin B Plenio. Colloquium: Area laws for the entangle-
ment entropy. Reviews of modern physics, 82(1):277, 2010.
[29] Hui Li and F. D. M. Haldane. Entanglement spectrum as a generalization of entanglement
entropy: Identiﬁcation of topological order in non-abelian fractional quantum hall eﬀect states.
Phys. Rev. Lett., 101:010504, Jul 2008.
[30] Frank Pollmann, Ari M Turner, Erez Berg, and Masaki Oshikawa. Entanglement spectrum of
a topological phase in one dimension. Physical review b, 81(6):064439, 2010.
[31] Alexei Kitaev and John Preskill. Topological entanglement entropy. Physical review letters,
96(11):110404, 2006.
[32] Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Information.
Cambridge University Press, 2000.

50
[33] Samuel L Braunstein and Carlton M Caves. Statistical distance and the geometry of quantum
states. Physical Review Letters, 72(22):3439, 1994.
[34] Samuel L Braunstein, Carlton M Caves, and Gerard J Milburn.
Generalized uncertainty
relations: theory, examples, and lorentz invariance. annals of physics, 247(1):135–173, 1996.
[35] Guido Fubini. Sulle metriche deﬁnite da una forma hermitiana: nota. Oﬃce graf. C. Ferrari,
1904.
[36] Eduard Study. K¨urzeste wege im komplexen gebiet. Mathematische Annalen, 60(3):321–378,
1905.
[37] Michael Victor Berry. Quantal phase factors accompanying adiabatic changes. Proceedings of
the Royal Society of London. A. Mathematical and Physical Sciences, 392(1802):45–57, 1984.
[38] William K Wootters. Statistical distance and hilbert space. Physical Review D, 23(2):357,
1981.
[39] Elena Aleksandrovna Morozova and Nikolai Nikolaevich Chentsov. Markov invariant geome-
try on state manifolds. Itogi Nauki i Tekhniki. Seriya” Sovremennye Problemy Matematiki.
Noveishie Dostizheniya”, 36:69–102, 1989.
[40] Tomohiro Shitara and Masahito Ueda. Determining the continuous family of quantum ﬁsher
information from linear-response theory. Physical Review A, 94(6):062316, 2016.
[41] Donald Bures. An extension of kakutani’s theorem on inﬁnite product measures to the ten-
sor product of semiﬁnite w*-algebras. Transactions of the American Mathematical Society,
135:199–212, 1969.
[42] Armin Uhlmann. The “transition probability” in the state space of a ⋆-algebra. Reports on
Mathematical Physics, 9(2):273–279, 1976.
[43] Manfredo P Do Carmo. Diﬀerential geometry of curves and surfaces: revised and updated
second edition. Courier Dover Publications, 2016.
[44] Michael Kolodrubetz, Vladimir Gritsev, and Anatoli Polkovnikov. Classifying and measuring
geometry of a quantum ground state manifold. Physical Review B, 88(6):064304, 2013.
[45] Olivier Bleu, DD Solnyshkov, and Guillaume Malpuech. Measuring the quantum geomet-
ric tensor in two-dimensional photonic and exciton-polariton systems. Physical Review B,
97(19):195422, 2018.
[46] Raﬀael L Klees, Gianluca Rastelli, Juan Carlos Cuevas, and Wolfgang Belzig. Microwave
spectroscopy reveals the quantum geometric tensor of topological josephson matter. Physical

51
Review Letters, 124(19):197002, 2020.
[47] Raﬀael L Klees, Juan Carlos Cuevas, Wolfgang Belzig, and Gianluca Rastelli. Ground-state
quantum geometry in superconductor–quantum dot chains. Physical Review B, 103(1):014516,
2021.
[48] Min Yu, Pengcheng Yang, Musang Gong, Qingyun Cao, Qiuyu Lu, Haibin Liu, Shaoliang
Zhang, Martin B Plenio, Fedor Jelezko, Tomoki Ozawa, et al. Experimental measurement
of the quantum geometric tensor using coupled qubits in diamond. National science review,
7(2):254–260, 2020.
[49] A Gianfrate, O Bleu, L Dominici, V Ardizzone, M De Giorgi, D Ballarini, K West, LN Pfeiﬀer,
DD Solnyshkov, D Sanvitto, et al. Direct measurement of the quantum geometric tensor in a
two-dimensional continuous medium. arXiv preprint arXiv:1901.03219, 2019.
[50] A Gianfrate, O Bleu, L Dominici, V Ardizzone, M De Giorgi, D Ballarini, G Lerario, KW West,
LN Pfeiﬀer, DD Solnyshkov, et al. Measurement of the quantum geometric tensor and of the
anomalous hall drift. Nature, 578(7795):381–385, 2020.
[51] James Lambert and Erik S. Sørensen. Estimates of the quantum ﬁsher information in the
s= 1 antiferromagnetic heisenberg spin chain with uniaxial anisotropy. Physical Review B,
99(4):045117, 2019.
[52] Varun Menon, Nicholas E Sherman, Maxime Dupont, Allen O Scheie, D Alan Tennant, and
Joel E Moore. Multipartite entanglement in the one-dimensional spin-1 2 heisenberg antifer-
romagnet. Physical Review B, 107(5):054422, 2023.
[53] Ir´en´ee Fr´erot and Tommaso Roscilde. Quantum variance: a measure of quantum coherence
and quantum correlations for many-body systems. Physical Review B, 94(7):075121, 2016.
[54] Daniele Malpetti and Tommaso Roscilde. Quantum correlations, separability, and quantum
coherence length in equilibrium many-body systems. Physical review letters, 117(13):130401,
2016.
[55] Eugene P Wigner and Mutsuo M Yanase. Information contents of distributions. In Part I:
Particles and Fields. Part II: Foundations of Quantum Mechanics, pages 452–460. Springer,
1997.
[56] Eugene P Wigner and Mutsuo M Yanase. On the positive semideﬁnite nature of a certain
matrix expression. Canadian journal of mathematics, 16:397–406, 1964.

52
[57] Ir´en´ee Fr´erot and Tommaso Roscilde. Reconstructing the quantum critical fan of strongly
correlated systems using quantum correlations. Nature communications, 10(1):1–7, 2019.
[58] Philipp Hyllus, Wies law Laskowski, Roland Krischek, Christian Schwemmer, Witlef Wiec-
zorek, Harald Weinfurter, Luca Pezz´e, and Augusto Smerzi. Fisher information and multipar-
ticle entanglement. Physical Review A, 85(2):022321, 2012.
[59] Paolo Zanardi and Nikola Paunkovi´c. Ground state overlap and quantum phase transitions.
Physical Review E, 74(3):031123, 2006.
[60] Shi-Jian Gu, Ho-Man Kwok, Wen-Qiang Ning, Hai-Qing Lin, et al. Fidelity susceptibility,
scaling, and universality in quantum critical phenomena. Physical Review B, 77(24):245109,
2008.
[61] Shi-Jian Gu. Fidelity approach to quantum phase transitions. International Journal of Modern
Physics B, 24(23):4371–4458, 2010.
[62] Nicolas Laﬂorencie, Sylvain Capponi, , and Erik S. Sørensen. Finite size scaling of the spin
stiﬀness of the antiferromagnetic s=1/2 xxz chain. The European Physical Journal B - Con-
densed Matter and Complex Systems, 24:77–84, 2001.
[63] Mischa Thesberg and Erik S. Sørensen. General quantum ﬁdelity susceptibilities for the j1-j2
chain. Phys. Rev. B, 84:224435, Dec 2011.
[64] Mischa Thesberg and Erik S. Sørensen. A quantum ﬁdelity study of the anisotropic next-
nearest-neighbour triangular lattice heisenberg model. Journal of Physics: Condensed Matter,
26(42):425602, oct 2014.
[65] Lorenzo Campos Venuti and Paolo Zanardi. Quantum critical scaling of the geometric tensors.
Physical review letters, 99(9):095701, 2007.
[66] David Schwandt, Fabien Alet, and Sylvain Capponi. Quantum monte carlo simulations of
ﬁdelity at magnetic quantum phase transitions. Phys. Rev. Lett., 103:170501, Oct 2009.
[67] A. Fabricio Albuquerque, Fabien Alet, Cl´ement Sire, and Sylvain Capponi. Quantum critical
scaling of ﬁdelity susceptibility. Phys. Rev. B, 81:064418, Feb 2010.
[68] Marek M Rams, Piotr Sierant, Omyoti Dutta, Pawe l Horodecki, and Jakub Zakrzewski. At
the limits of criticality-based quantum metrology: Apparent super-heisenberg scaling revisited.
Physical Review X, 8(2):021022, 2018.
[69] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Advances in quantum metrology.
Nature photonics, 5(4):222–229, 2011.

53
[70] Christian L Degen, Friedemann Reinhard, and Paola Cappellaro. Quantum sensing. Reviews
of modern physics, 89(3):035002, 2017.
[71] Ir´en´ee Fr´erot and Tommaso Roscilde. Quantum critical metrology. Physical review letters,
121(2):020402, 2018.
[72] Luca Pezze, Marco Gabbrielli, Luca Lepori, and Augusto Smerzi. Multipartite entanglement
in topological quantum phases. Physical review letters, 119(25):250401, 2017.
[73] Luca Pezze, Augusto Smerzi, Markus K Oberthaler, Roman Schmied, and Philipp Treutlein.
Quantum metrology with nonclassical states of atomic ensembles. Reviews of Modern Physics,
90(3):035005, 2018.
[74] Utkarsh Mishra and Abolfazl Bayat. Driving enhanced quantum sensing in partially accessible
many-body systems. Physical Review Letters, 127(8):080504, 2021.
[75] M Zahid Hasan and Charles L Kane. Colloquium: topological insulators. Reviews of modern
physics, 82(4):3045, 2010.
[76] Yu-Quan Ma, Shi-Jian Gu, Shu Chen, Heng Fan, and Wu-Ming Liu. The euler number of
bloch states manifold and the quantum phases in gapped fermionic systems. EPL (Europhysics
Letters), 103(1):10008, 2013.
[77] Paolo Zanardi, Lorenzo Campos Venuti, and Paolo Giorda. Bures metric over thermal state
manifolds and quantum criticality. Physical Review A, 76(6):062318, 2007.
[78] James Lambert and Erik S. Sørensen. State space geometry of the spin-1 antiferromagnetic
heisenberg chain. arXiv preprint arXiv:2209.05005, 2022.
[79] James Lambert and Erik S. Sørensen. Revealing divergent length scales using quantum ﬁsher
information in the kitaev honeycomb model. Physical Review B, 102(22):224401, 2020.
[80] Michael Kolodrubetz, Dries Sels, Pankaj Mehta, and Anatoli Polkovnikov. Geometry and
non-adiabatic response in quantum and classical systems. Physics Reports, 697:1–87, 2017.
[81] Sandro Sorella. Generalized lanczos algorithm for variational quantum monte carlo. Physical
Review B, 64(2):024512, 2001.
[82] Chae-Yeun Park and Michael J Kastoryano. Geometry of learning neural quantum states.
Physical Review Research, 2(2):023232, 2020.
[83] Bruno Mera, Chrysoula Vlachou, Nikola Paunkovi´c, V´ıtor R Vieira, and Oscar Viyuela. Dy-
namical phase transitions at ﬁnite temperature from ﬁdelity and interferometric loschmidt
echo induced metrics. Physical Review B, 97(9):094110, 2018.

54
[84] Xiao-Qian Wang, Jian Ma, Xi-He Zhang, and Xiao-Guang Wang. Chaos and quantum ﬁsher
information in the quantum kicked top. Chinese Physics B, 20(5):050510, 2011.

