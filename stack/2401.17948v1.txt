HyperZ·Z·W Operator Connects Slow-Fast Networks for Full Context
Interaction
Harvie Zhang
HyperEvol AI Lab
harvie.zhang@hyperevol.com
Abstract
The self-attention mechanism utilizes large implicit
weight matrices, programmed through dot product-based
activations with very few trainable parameters, to enable
long sequence modeling. In this paper, we investigate the
possibility of discarding residual learning by employing
large implicit kernels to achieve full context interaction at
each layer of the network.
To accomplish it, we intro-
duce coordinate-based implicit MLPs as a slow network to
generate hyper-kernels1 for another fast convolutional net-
work2. To get context-varying weights for fast dynamic en-
coding, we propose a HyperZ·Z·W operator that connects
hyper-kernels (W) and hidden activations (Z) through sim-
ple elementwise multiplication, followed by convolution of
Z using the context-dependent W. Based on this design,
we present a novel Terminator architecture that integrates
hyper-kernels of different sizes to produce multi-branch hid-
den representations for enhancing the feature extraction ca-
pability of each layer. Additionally, a bottleneck layer is
employed to compress the concatenated channels, allow-
ing only valuable information to propagate to the subse-
quent layers. Notably, our model incorporates several in-
novative components and exhibits excellent properties, such
as introducing local feedback error for updating the slow
network, stable zero-mean features, faster training con-
vergence, and fewer model parameters. Extensive experi-
mental results on pixel-level 1D and 2D image classifica-
tion benchmarks demonstrate the superior performance of
our architecture. Our code will be publicly available at
https://github.com/hyperevolnet/Terminator.
1. Introduction
In the past decade, significant advancements have been
made in neural networks, particularly with the emergence
1The definition of Hyper-Kernel comes from HyperNEAT [39].
2The slow network serves as a hyper-kernel generator, while the fast
network either interacts with the context or has context-dependent fast
weights. Our definitions of slow and fast networks differ from [16, 35, 36].
64 × H
2 × W
2
128 × H
4 × W
4
C × H × W
256 × H
8 × W
8
Conv-BN-ReLU
Conv-BN-ReLU
pool, /2
Conv-BN-ReLU
Conv-BN-ReLU
pool, /2
Conv-BN-ReLU
Conv-BN-ReLU
pool, /2
Conv-BN-ReLU
Avg pool
FC layer
CE Loss
4C × H × W
12C × H × W
24C × H × W
C × H × W
SFNE Block
SFNE Block
SFNE Block
Avg pool
FC layer
CE Loss
Slow Neural Loss
Figure 1. Comparison between the residual network and our Ter-
minator architecture. 1) Our Slow-Fast Neural Encoding (SFNE)
block employs a multi-branch structure, eliminating the need for
residual learning (Figure 3). 2) Our hidden layers do not utilize
pooling layers for downsampling feature resolution. 3) We intro-
duce a novel local feedback error for updating the slow network.
of ResNet [15] and Transformer [44]. However, the addi-
tive connections between layers in residual learning, while
addressing the performance degeneration, pose challenges
for the exploration of interpretable architectures [33, 45]
and the adoption of greedy layerwise training [27]. Main-
stream residual networks [15, 18, 24, 41, 49] typically rely
on single-branch architectures with small convolution ker-
nels, which significantly diminish the feature extraction ca-
pabilities of individual layers. To compensate information
loss, previous layer outputs are added to subsequent layers.
However, increasing the size of convolution kernels leads to
an exponential growth in model parameters. Consequently,
it is crucial to address the following questions in order to
eliminate the reliance on residual learning:
How can we generate large convolution kernels with
fewer parameters and build a multi-branch network?
1
arXiv:2401.17948v1  [cs.CV]  31 Jan 2024

In the Transformer models [8, 23, 44], the self-attention
weights can be viewed as context-dependent weight matri-
ces or neural network-programmed fast weights [1, 35, 36].
These models employ large implicit weight matrices, de-
rived from key-query activations with few trainable param-
eters, to encode long sequence inputs. However, the uti-
lization of dot product-based attention matrices introduces
quadratic time and space complexity, making it challeng-
ing to scale Transformers to inputs with large context sizes.
Therefore, this paper aims to identify a simpler and more
effective method than dot product-based attention to obtain
context-varying weights through network activations.
How can we obtain context-dependent fast weights
through simpler elementwise multiplication?
This paper not only investigates methods to leverage net-
work activations for generating context-dependent large im-
plicit convolution kernels but also proposes a novel multi-
branch architecture to explore the possibility of abandon-
ing residual learning. Specifically, we employ coordinate-
based implicit MLPs [9] with few model parameters as a
slow network to generate hyper-kernels for another fast con-
volutional network. The weights of slow network is data-
independent and remain fixed after training. To make the
generated hyper-kernels to be context-dependent, we pro-
pose a HyperZ·Z·W operator, connecting hyper-kernels
(W) and hidden activations (Z) through simple element-
wise multiplication, a more efficient alternative to the dot
product attention [44]. The fast network utilizes context-
varying weights to perform convolution operations on Z.
Consequently, the weights of fast network are context-
dependent and can change at test time. Based on above
design, we present a new architecture called Terminator,
which integrates context-dependent hyper-kernels of differ-
ent sizes to produce multi-branch hidden representations,
enhancing the feature extraction capability of each layer.
Additionally, a bottleneck layer is employed to compress
the concatenated channels, allowing only valuable informa-
tion to propagate to the subsequent layers.
Furthermore, several novel components are proposed
to build the multi-branch architecture, including Recursive
Gated Unit (RGU), hyper-channel interaction and hyper in-
teraction (see Figure 5). We also introduce a local feed-
back loss for updating the slow network and replace the nor-
malization layer that discards affine parameters and momen-
tum argument with a standardization layer. Our architecture
exhibits excellent properties such as stable zero-mean fea-
tures, faster training convergence, and fewer model param-
eters. Extensive experimental results on pixel-level 1D and
2D image classification benchmarks demonstrate the supe-
rior performance of our architecture.
Notation: Let x ∈RC×H×W be the 2D input, where H,
W and C are height, width, and channel dimension. The
HyperZ·Z·W can be abbreviated as HyperZZW.
Block 0
Block 1
Block 2
Block 3
Block 4
Input
ResNet-152
Terminator
Figure 2. Visualization of the feature maps in each block. For
the convenience of comparison, we enlarge the output of the 2∼4
blocks of ResNet-152.
2. Method
In this section, we first provide an overview of the key prop-
erties of our overall architecture. Subsequently, we present
the specifics of the coordinate-based implicit MLPs served
as a slow network for generating hyper-kernels, and eluci-
date the role played by the HyperZ·Z·W operator. Fol-
lowing that, we elaborate on the process of constructing a
multi-branch SFNE block using our proposed novel compo-
nents. Lastly, we introduce the slow neural loss.
2.1. Key Properties
□✗[No Residual Learning.]
Residual learning has been
widely employed to address the performance degenera-
tion problem in very deep neural networks [15, 24, 41,
48, 49]. In this paper, we content that the crucial moti-
vation behind the adoption of residual connections is to
compensate for the limited representation learning capac-
ity of individual layers, particularly in the case of resid-
ual networks with its single-branch architecture and small
convolution kernels. To overcome these limitations, we
incorporate large implicit convolution kernels of varying
sizes to construct a multi-branch network, enabling full
context interaction at each layer and obviating the neces-
sity for residual learning. Based on above analysis, we
introduce a novel Terminator architecture, which is com-
posed of a stack of SFNE blocks shown in Figure 3.
□✗[No Dot Product Attention.] The quadratic increase in
computational cost, based on the number of pixels, neces-
sitates the division of images into patches in order to facil-
itate the successful implementation of vision transform-
ers [8]. However, the conversion of patch embeddings
into fixed-length representations not only limits the inclu-
sion of local context information but also hinders the es-
timation of attention scores for individual pixels. In con-
trast, our proposed HyperZ·Z·W operator enables the
interaction between generated hyper-kernels and image
pixels through a simple elementwise multiplication, fa-
cilitating the acquisition of precise pixel-level (i.e. token-
level) scores (see Figure 4). Notably, this approach is
more efficient than the attention-free transformer [51] as
2

Input
Channel 
Mixer
RGU 
Channel Mixer
HyperZZW 
K = 7
HyperZZW  
K = 9
HyperZZW 
K = N
RGU  
Channel Mixer
Si-GLU
Channel Concatenation
Channel Bottleneck Layer
G-IBS
GeLU
Dropout
Output
HyperZZW 
K = 5
BS
GeLU
IS
IS
IS
Hyper-Channel 
Interaction
Hyper- 
Interaction
BS
BS
GeLU
GeLU
Slow Neuron
Fast Neuron
Z
W
Hidden 
feature
Hyper-
kernel
Context-
dependent kernel
Output 
Z
HyperZZW Operator
Hyper-kernels
Channel  
Bottleneck
Channel Bottleneck Layer
Mutual Hyper-Kernel 
Generation
Slow Neural Loss
Context-dependent 
Hyper-kernel
⊛
⊛
⊛
Elementwise Multiplication
Convolution
⊛
Dot Product
⊛
⊙⊗
Figure 3. The overall framework of our Slow-Fast Neural Encoding (SFNE) block, which utilizes channel mixers and multi-scale hyper-
kernels (e.g. N is the input size) to construct a nine-branch structure. The ovals represent slow networks used to generate hyper-kernels,
and the rectangles represent fast networks that interact directly with the input. The HyperZ·Z·W operator is formed by combining ⊙, ⊗,
and ⊛, enabling context-dependent fast weights. RGU and Si-GLU represent the recursive gated unit and the simplified gated linear unit.
it eliminates the reliance on the Sigmoid function.
□✗[No Intermediate Pooling Layer.] Intermediate pooling
layers enhance the receptive field by reducing feature map
size, enabling better capture of global information. How-
ever, our visualizations in Figure 2 demonstrate that the
intermediate pooling layers can distort the overall struc-
ture of objects.3 Therefore, this hampers final average
pooling layer to generate accurate image descriptors. In
our architecture, the utilization of large implicit convo-
lutional kernels enables global contextual interaction at
each layer, obviating the need for pooling layers to ex-
pand the receptive field. Moreover, the feature maps pro-
duced by our architecture exhibit a notable continuous de-
3To obtain 2D heat map, we sum the values of all channels in the feature
map. The visualization in Figure 4 follows the same process.
noising process (see Figure 2), which not only alleviates
the model’s learning complexity but also facilitates the
development of interpretable neural networks.
□✗[No Normalization.] Traditionally, normalization lay-
ers such as BN [19], IN [43], LN [2], GN [47] incor-
porate learnable affine parameters, including scaling fac-
tor γ and shift factor β.
Previous work [19] asserted
that these parameters restore the network’s representation
power. However, our experimental findings, as illustrated
in Figure 6, reveal that their primary function is to cor-
rect batch statistics (mean and variance) to ensure stable
activations. BN [19] and IN [43] also employ a momen-
tum argument to reduce volatility between batches, but it
can hinder the in-context learning in each batch. In this
paper, we remove the affine and momentum parameters,
3

transforming normalization into a standardized operation
called z-score standardization. The new standardization
can be written as:
ˆx =
x −E[x]
p
V ar[x] + ϵ
,
(1)
where ϵ is a constant added to the mini-batch variance for
numerical stability. Consequently, we replace the original
BN and IN layers with Batch Standardization (BS) and
Instance Standardization (IS) respectively.
□✓[Slow-Fast Networks.] In our architecture, we employ
coordinate-based implicit MLPs [9] as a slow network to
generate large convolution kernels (hyper-kernels) for a
fast network. The slow network has minimal trainable
parameters (see Table 1) but can generate large hyper-
kernels to achieve full context interaction at each layer.
Notably, the weights of the slow network remain fixed
after training and do not directly interact with the con-
text [35]. In contrast, as depicted in Figure 3, the fast net-
work consists of two components. The first component
is the channel mixers and bottleneck layer, which directly
interact with the feature maps. The second component
involves the context-dependent fast weights generated by
the HyperZ·Z·W operator, enabling convolution and dot
product operations. These fast weights can adapt in real-
time during testing based on the input.
2.2. Generation of Hyper-kernels
To address the first question presented in the introduction
section, we generate two distinct hyper-kernels using slow
networks to construct multi-branch networks. The first one,
denoted as global hyper-kernel Kg ∈R1×C×H×W , has the
same size as the input x. The second one, referred to as lo-
cal hyper-kernel is Kl ∈RC×1×k×k for depth-wise convo-
lution, has small kernel size k. By combining these hyper-
kernels, our architecture can extract global features while
preserving the input’s fine details. Below we elaborate on
the generation process of Kg.
The global hyper-kernel Kg ∈R1×C×H×W is generated
using coordinate-based implicit MLPs, which is also known
as multiplicative filter networks [9]. The generation process
is described by the following equations:
h(1) = g

c; θ(1)
h(i+1) =

W(i)h(i) + b(i)
⊙g

c; θ(i+1)
, i = 1, ..., l
h(o) = W(o)h(i+1) + b(o),
(2)
where ⊙denotes elementwise multiplication,
c
∈
R1×2×H×W represents the normalized coordinates on the
H and W dimensions, which lie in a uniform linear space
of [−1, 1]H and [−1, 1]W . The weight matrix for the i-th
layer is denoted by W(i) ∈Rd×d, and the hidden unit of the
Block 0
Block 1
Block 2
Block 3
Block 4
Global Hyper-Kernels
Context-dependent Global Hyper-Kernels
Slow Neural Loss
Slow Neural Loss
Figure 4. Visualization of global hyper-kernels in each block. By
performing elementwise multiplication between the sample activa-
tions and the global hyper-kernels Kg, the model can effectively
leverage context-dependent hyper-kernels ˆKg to obtain pixel-level
scores, especially when trained with the slow neural loss.
i-th layer is hi ∈R1×d×H×W . The weight W (o) ∈RC×d
in the output layer transforms hi+1 to ho ∈R1×C×H×W .
The function g
 c; θ(i)
is a simple sinusoidal filter defined
as:
g

c; θ(i)
= sin(w (i)c + ϕ(i)),
(3)
where θ(i) = w (i) ∈Rd×2, ϕ(i) ∈Rd.
In addition, we use a simple s-renormalization4 opera-
tion on the hidden output h(i+1) to stabilize training of the
slow network. This operation is defined as follows:
ˆh(i+1) = diag(a)I + diag(b)h(i+1)
norm ,
(4)
where a ∈Rd and b ∈Rd are scaling vectors learned dur-
ing training. h(i+1)
norm denotes the Euclidean norm of h(i+1).
We initialize a and b to 1.0 and 0.1, respectively, following
the setup described in [50].
2.3. HyperZZW Operator
Our proposed HyperZ·Z·W operator enables the gener-
ated global and local hyper-kernels to interact with the con-
text, resulting in context-dependent fast weights. The uti-
lization of HyperZ·Z·W involves two steps. First, hidden
activation Z and hyper-kernel W are multiplied element-
wise to obtain context-varying weights ˆ
W. Subsequently,
ˆ
W can be used to perform dot product or convolution op-
erations on Z for feature extraction. Unlike dot product-
based attention [44], our HyperZ·Z·W operator achieves
context-dependent fast weights with linear time and space
4The s-renormalization is called Weight Normalization in [34], and we
use it to renormalize the hidden output of the slow network.
4

complexity. The following steps outline the specific usage
HyperZ·Z·W on hyper-kernels Kg and Kl.
[Global HyperZZW.] For the global hyper-kernel Kg,
we employ a simple elementwise multiplication to get
context-dependent ˆKg:
ˆKg = Z ⊙Kg,
(5)
where Z ∈RB×C×H×W represents B samples in a batch,
and ˆKg ∈RB×C×H×W due to broadcasting Kg along
the batch dimension. The visualization results, shown in
Figure 4, demonstrate the effectiveness of ˆKg in captur-
ing pixel-level weighting scores for each batch sample, es-
pecially when the model is trained with slow neural loss
(as described in Section 2.5).
In contrast, the context-
independent Kg generated by the slow neural network ap-
pears to resemble random noise. Subsequently, instead of
using sliding window-based convolution, we employ the dot
product ⊗operation with ˆKg for global feature extraction:
Zg = Z ⊗ˆKg,
(6)
where Zg ∈RB×C×H×W . For 1D samples Z ∈RB×C×L
(e.g. L is the sequence length), we can utilize the following
Fast Fourier Transform (FFT) to extract global features:
Zg = iFFT(FFT(Z) ⊙FFT( ˆKg)),
(7)
[Local HyperZZW.] To introduce context-dependency
to the local hyper-kernel Kl ∈RC×1×k×k, we incorporate
the HyperZ·Z·W operator into its generation process. The
process is described by the following equations:
ˆZ(1) = T (Z)
h(1) = g

c; θ(1)
h(i+1) =

W(i)h(i) + b(i)
⊙g

c; θ(i+1)
, i = 1, ..., l
h(i+1) =

W(i)
z ˆZ(i) + b(i)
z

⊙h(i+1)
(8)
h(o) = W(o)h(l) + b(o)
where c ∈R1×2×k×k represents the normalized coordi-
nates on the k dimension, which range from −1 to 1 in a
uniform linear space. The hidden unit of the i-th layer is
hi ∈R1×d×k×k, and the weight W (o) ∈RC×d in the out-
put layer transforms hi+1 to ho ∈R1×C×k×k. The trans-
formation T converts Z ∈RB×C×H×W to ˆZ ∈R1×d×k×k
through average pooling (B×C ×k×k), channel reduction
(B×d×k×k) and batch mean (1×d×k×k). By performing
elementwise multiplication between ˆZ(i) and h(i+1), the
context-dependent local hyper-kernels are generated. We
can then utilize ˆKl ∈RC×1×k×k to extract local features
through sliding window-based convolution operations:
Zl = Z ⊛ˆKl,
(9)
where ⊛represents depth-wise convolution.
Our visualizations in the appendix show that the global
features Zg primarily capture the overall outline of object,
providing a rough representation. On the other hand, lo-
cal features Zl excel at preserving finer details, particularly
when smaller convolution kernels are utilized. To address
the challenge of information loss at each layer, the combi-
nation between global and local features through channel
stacking is used to enhance the comprehensive representa-
tion of the network. As a result, the necessity for residual
connections to compensate for this loss becomes obsolete.
2.4. Slow-Fast Neural Encoding Block
Our SFNE block, as illustrated in Figure 3, plays a cru-
cial role in simultaneously achieving full context interac-
tion in channel and spatial dimensions, so that the net-
work does not lose information at each layer.
The Fig-
ure 3 presents a nine-branch architecture, composed of three
branches from global HyperZ·Z·W, three branches from
local HyperZ·Z·W, one branch from the Si-GLU 5, one
branch from the middle layer, and the final one from hy-
per interaction. Notably, the global branches share a com-
mon hyper-kernel. Moreover, the nine-branch architecture
is flexible. To handle higher resolution images, we can ad-
just the number and kernel size of the local HyperZ·Z·W
while keeping the other modules unchanged. To facilitate
communication across channels, as the dot product and con-
volution operations in HyperZ·Z·W are channel-wise, we
introduce three types of channel transformations: 1) Chan-
nel Mixer is a straightforward one-layer MLP; 2) Recursive
Gated Unit (RGU); 3) Hyper-Channel Interaction. Follow-
ing these transformations, instance standardization (Mixer-
IS) is applied to ensure the independence of the channel di-
mension and the sample dimension. In the subsequent sec-
tions, we provide a detailed introduction to our proposed
novel components.
Recursive Gated Unit is an extension of the gated linear
unit (GLU) [7] that incorporates recursion.
K = WKx,
V = WV x,
Q = σ (S (K ⊙WV)) + b,
Y = WY Q,
(10)
where σ represents the Gaussian Error Linear Unit (GeLU)
nonlinearity, and S denotes instance standardization. The
outputs K, V, Q, and Y have the same size as the input
x. We apply the RGU to the input x and context-dependent
ˆKg and then use their sum as the output of a channel mixer.
Hyper-Channel
Interaction
module
integrates
HyperZ·Z·W operator, which can be viewed as a more
5The parameters-free Si-GLU can be written as Si-GLU(x)
=
σ(x) ⊙x, where σ denotes the Sigmoid function.
5

(a) Hyper-Channel Interaction
C
H
W
Avg Pool
C × 1 × 1
σ
HyperZ ⋅Z ⋅W
Hyper-Channel Weights
𝕏
̂𝕏
(b) Hyper Interaction
Hyper-Channel Weights
C
H
W
Avg Pool
C × 1 × 1
HyperZ ⋅Z ⋅W
Mean
HyperZ ⋅Z ⋅W
1 × H × W
σ
Hyper-Spatial Weights
𝕐
̂𝕏
𝕏
: Input
𝕏
: Output
̂𝕏
: High-level Output
𝕐
Figure 5. Diagrams illustrate our proposed hyper-channel interaction and hyper interaction mechanisms in our architecture.
Block
0
1
2
3
4
All
Slow Network
73K
77K
96K
153K
237K
636K
8%
Fast Network
Channel mixers
20K
27K
92K
650K
2.7M
3.5M
45%
Bottleneck layer
816
19K
186K
1.1M
2.4M
3.7M
47%
All
93K
123K
374K
1.9M
5.3M
7.8M
1.2%
1.6%
4.8%
24.4%
68.0%
100%
Table 1. Statistics of model parameters in each block, including slow network and fast network. In order to better know the parameters of
each module, we list channel mixers and bottleneck layer separately from the fast network.
effective version of channel attention mechanisms [17, 46].
As depicted in Figure 5(a), we begin by aggregating spatial
information from a feature map x ∈RC×H×W using an
average-pooling operation to generate a channel descriptor
zc ∈RC×1×1.
Next, our HyperZ·Z·W operator con-
nects the channel-wise hyper-weights wc ∈RC×1×1 and
zc generated from a slow network through two simple
elementwise multiplications, obtaining gating scores for
the channels.
Finally, the channel weights are obtained
by applying a Sigmoid function, and these weights are
broadcasted across the spatial dimension for multiplication
with x.
Through global cross-channel interaction, our
hyper-channel interaction module achieves channel mixing.
Hyper Interaction combines hyper-channel and hyper-
spatial interactions into a single module, leveraging the
gating scores of the high-level output to automatically ex-
tract features from input x. Figure 5(b) illustrates this pro-
cess.
To extract channel weights, we employ the same
mechanism as described in the hyper-channel interaction.
Additionally, for generating the spatial descriptor zs ∈
R1×H×W , we aggregate the channel information of a fea-
ture map x ∈RC×H×W using a channel mean opera-
tion. Our HyperZ·Z·W operator connects the spatial-wise
hyper-weights ws ∈R1×H×W with zs through two sim-
ple elementwise multiplications, resulting in the spatial gat-
ing scores. Next, these scores are multiplied with the chan-
nel scores, after broadcasting, to pass through the Sigmoid
function for obtaining the channel-spatial weights. Finally,
these weights calculated from the high-level output are uti-
lized for pixel-level filtering on the input x. This integration
enables the hyper-interaction module to enhance the feature
extraction capability of the SFNE block as a new branch.
Mutual Hyper-Kernel Generation (MuHKGen) mod-
ule can increases the model’s complexity and helps prevent
over-fitting. For instance, the slow network integrates z2
to produce a local hyper-kernel K1
l using Eqn. 8, and vice
versa. Subsequently, the fast networks perform convolution
operations on z1 and z2 using K2
l and K1
l , respectively, to
extract multi-scale features.
Channel Bottleneck Layer. The SFNE block concate-
nate the multi-scale features along the channel dimension
to achieve a comprehensive representation of the input. The
objective of the bottleneck layer is to compress the concate-
nated global and local feature maps, effectively eliminat-
ing noise present in the feature maps. To accomplish this,
we introduce a bottleneck coefficient λ to control the de-
gree of channel compression, determining the reduction in
the number of concatenated channels. On the other hand, it
scales up the channel dimension of the input by a factor of
λ (λ ≥1). Specifically, when the input is x ∈RC×H×W ,
passing through the bottleneck layer results in an output
y ∈RλC×H×W .
Group-based Standardization. After the channel bot-
tleneck layer, we introduce a Group-based Instance-Batch
Standardization (G-IBS) operation, as illustrated in Fig-
ure 3.
Specifically, the output feature maps are divided
into non-overlapping groups based on channels, and Group-
based Batch Standardization (G-BS) and Group-based In-
stance Standardization (G-IS) are applied alternately to each
corresponding group. Our visualized results shown in Fig-
ure 6(b-c) demonstrate that combining G-IS and G-BS can
enhance the model’s expressive power by increasing diver-
6

Method
#Param.
sMNIST pMNIST sCIFAR10
DilRNN [5]
44K
98.0
97.2
-
LSTM [3]
70K
87.2
85.7
-
GRU [3]
70K
96.2
87.3
-
TCN [3]
70K
99.0
97.2
-
FlexTCN-6 [30]
375K
99.6
98.6
80.8
r-LSTM [42]
500K
98.4
95.2
72.2
Transformer [42]
500K
98.9
97.9
62.2
HiPPO [10]
500K
98.9
98.3
61.1
CKCNN [31]
1M
99.32
98.54
63.74
CCNN [32]
2M
99.72
98.84
93.08
LSSL [12]
2M
99.53
98.76
84.65
S4 [11]
7.8M
99.63
98.70
91.13
Hyena [29]
7.8M
-
-
91.10
TrellisNet [4]
8M
99.20
98.13
73.42
LRU [28]
-
-
-
89.00
Liquid-S4 [14]
-
-
-
92.02
S4D-Inv [13]
-
-
-
90.69
S5 [38]
-
99.65
98.67
90.10
Terminator
1.3M
99.72
99.12
93.21
Table 2. Pixel-level 1D image classification. The underlined result
is the second best method.
sities of the channel variances.
2.5. Slow Neural Loss
The slow neural loss serves as a local feedback error for
updating the slow network (i.e. large hyper-kernel gener-
ator). We compute the Mean Squared Error (MSE) of the
j-th SFNE block as follows:
Lj
s =
j−1
X
t=0
∥Kj
g −E(Kt
g)∥2, j = 1, ..., J
(11)
Ls =
J
X
j=1
Lj
s,
(12)
where J is the number of SFNE blocks, and E represents
a channel expansion operation achieved through repeated
concatenations for different blocks (t = 0, ..., j −1). It can
exploit the channel-spatial consistency of Kg in deep and
shallow blocks to promote context-dependent ˆKg to obtain
more accurate pixel-level scores (see Figure 4). The total
slow neural loss Ls is obtained by summing up the individ-
ual block losses, Lj
s (j = 1, ..., J). In our experiments, we
combine the Ls loss with the cross-entropy loss to train the
model end-to-end using backpropagation.
3. Experiments
In this section, we provide quantitative and qualitative re-
sults to demonstrate the superior performance of our Ter-
minator architecture. The experimental settings, dataset in-
troduction, architectural details, ablation studies and more
discussions are presented in the appendix.
Method
#Param.
CIFAR10
CIFAR100
InceptionV2 [40]
65M
-
72.49
Xception [6]
21M
-
74.93
VGG16 [37]
20M
93.91
74.08
ViT [8]
6.3M
90.92
66.54
ConvNeXt [24]
6.3M
92.20
-
WRN-40-2 [49]
2.2M
94.67
-
ResNet-110 [15]
1.7M
93.57
74.84
Hyena-ISO [29]
202K
91.20
-
CKCNN [31]
670K
86.80
-
FlexNet-16 [30]
670K
92.20
-
CCNN† [32]
2.0M
94.56
73.58
S4ND-ISO [26]
5.3M
94.10
-
Terminator
1.3M
95.22
75.38
Table 3.
2D image classification on CIFAR10 and CIFAR100
datasets. † represents the results obtained from the code released
by [32].
3.1. Results on 1D Image Classification
The results shown in Table 2 highlight the superior per-
formance of our Terminator architecture on the sMNIST,
pMNIST [21], and sCIFAR10 [20] datasets.
Our net-
work achieves state-of-the-art results, surpassing temporal
convolutional networks [4], continuous convolutional net-
works [30–32] and state space models [10–14, 29, 38] in
terms of accuracy, despite having fewer model parameters
and not utilizing residual learning. One of the key factors
contributing to our model’s success is the HyperZ·Z·W
operator, which effectively models long-term dependencies
through context-dependent fast weights.
3.2. Results on 2D Image Classification
Table 3 shows the performance comparison of our Termi-
nator with various models on the CIFAR10 and CIFAR100
datasets. Remarkably, our Terminator model surpasses In-
ception models [6, 40], residual networks [15, 24, 49], and
vision transformers [8] in terms of accuracy while utilizing
fewer model parameters. Moreover, our network outper-
forms continuous convolutional networks [30–32] and state
space models [26, 29], establishing state-of-the-art results
without relying on residual learning.
Furthermore, the results in Table 5 demonstrate that
our Terminator achieves superior performance compared to
ResNet-152 while utilizing only 1/7 of the model param-
eters.
Notably, the Terminator model doubles the com-
putation while efficiently maintaining full context interac-
tion at each layer without information loss, even on large-
resolution images.
Consequently, the need for residual
learning is eliminated.
Additionally, our HyperZ·Z·W
operator utilizes hidden activations to generate context-
dependent fast weights, thereby enhancing the model’s ca-
pacity for in-context learning.
7

0
36
100
200
Epoch
0.1
0.3
0.5
0.7
0.9
Test Accuracy
Xception
ResNet-152
ResNeXt-152
DenseNet-201
Ours - Terminator
(a) Training convergence
0.0
0.1
0.2
0.3
0.4
0.5
Batch Mean
0
5
10
15
20
25
30
Frequency
ResNet-152 w/ BS
ResNet-152 w/ BN
Terminator w/ G-BS
Terminator w/ G-IS
(b) Channel mean
0
50
100
150
200
250
# of Batch
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Variance
ResNet-152 w/ BS
ResNet-152 w/ BN
Terminator w/ G-BS
Terminator w/ G-IS
(c) Channel variance
Figure 6. (a) shows fast training convergence of our Terminator. (b) and (c) visualize the batch-based channel mean and channel variance
statistics of the last hidden layer in ResNet-152 and Terminator. The experiments are constructed on the STL10 test set.
Method
ResNet-152 [15] w/ BN [19]
ResNet-152 [15] w/ BS
Terminator
w/ Affine
w/ Momentum
Test Accuracy
85.41
77.82
86.32
85.93
85.55
Table 4. Comparison of test accuracy of ResNet-152 and Terminator on STL10 dataset when using normalization and standardization
operations. ResNet-152 performs poorly when removing the affine and momentum parameters, while our Terminator does the opposite.
Method
Size
#Param.
FLOPs
Acc.
FlexNet-16 [30]
962
670K
-
68.67
CCNN† [32]
2562
2M
115G
78.01
ResNet-18 [25]
962
11.2M
5G
78.65
DenseNet-201 [18]
962
18.1M
13G
80.60
Xception [6]
962
20.8M
10G
84.15
ResNeXt-152 [48]
962
33.3M
18G
84.95
ResNet-152 [15]
962
58.2M
34G
85.41
Terminator
962
1.5M
10G
80.13
962
7.8M
67G
86.32
Table 5. Image classification on STL10 dataset. † represents the
reproduced results obtained from the code released by [32].
3.3. Analysis of Excellent Properties
Training Convergence. The visualization in Figure 6(a)
demonstrates the faster training convergence of our Termi-
nator architecture. It achieves the same test accuracy as
ResNet-152 in approximately 1/6 of the epochs, requiring
only 1/2 of the training epochs. Moreover, it is notewor-
thy that our Terminator achieves satisfactory results on the
sMNIST dataset with just 50 training epochs, which is only
1/4 of the epochs required by other networks [11, 12, 30–
32]. The fast convergence of our model is mainly due to
the zero-mean features introduced below, which is a well-
known conclusion [19, 22].
Zero-mean Features.
The results presented in Fig-
ure 6(b) and Table 4 demonstrate that our network achieves
stable zero-mean features and improved performance even
without the affine and momentum parameters in standard-
ization layers. In contrast, training ResNet-152 with batch
standardization (BS) leads to a significant decline in per-
formance, resulting in an accuracy of only 77.82%. Addi-
tionally, the channel mean and channel variance experience
notable increases. These elevated batch statistics introduce
higher volatility, which detrimentally affects the model’s
convergence and generalization capabilities.
Model Parameters. The statistical findings in Table 1
reveal that as the model depth increases, the number of
model parameters in each block also increases, with the last
block being particularly significant, accounting for 68% of
the total model parameters. This growth in parameters is
primarily attributed to the escalating number of channels in
the feature maps. Notably, the slow network has a mere 8%
of the model parameters. We also would like to highlight
that, as demonstrated in Figure 2, our Terminator network
no longer need to be very deep, because it can achieve effec-
tive image denoising by utilizing only a few SFNE blocks.
4. Conclusion
This paper introduces the Terminator architecture, which
offers a novel approach to network design by abandoning
residual learning and leveraging large implicit convolution
kernels. The SFNE block provides a new perspective for
building multi-branch networks, while the HyperZ·Z·W
operator enables dynamic encoding by providing context-
dependent weights for the fast networks through element-
wise multiplication between hyper-kernels and hidden acti-
vations. Our network brings several advantages, including
faster training convergence, stable zero-mean features, and
fewer model parameters. The experimental results demon-
strate its superiority in capturing long-range dependencies
and achieving state-of-the-art performance on pixel-level
1D and 2D image classification benchmarks. Due to lim-
ited computing resources, we were unable to conduct exper-
iments on ImageNet dataset. However, in future research,
we plan to prioritize the exploration of more effective slow
neural loss to improve the accuracy of pixel-level scores.
8

References
[1] Jimmy Ba, Geoffrey E. Hinton, Volodymyr Mnih, Joel Z.
Leibo, and Catalin Ionescu. Using fast weights to attend to
the recent past. In Neural Information Processing Systems,
2016. 2
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450,
2016. 3
[3] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical
evaluation of generic convolutional and recurrent networks
for sequence modeling. arXiv preprint arXiv:1803.01271,
2018. 7
[4] Shaojie Bai, J Zico Kolter, and Vladlen Koltun.
Trel-
lis networks for sequence modeling.
arXiv preprint
arXiv:1810.06682, 2018. 7
[5] Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao
Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A
Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent
neural networks. Advances in neural information processing
systems, 30, 2017. 7
[6] Franc¸ois Chollet. Xception: Deep learning with depthwise
separable convolutions.
In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
1251–1258, 2017. 7, 8
[7] Yann N Dauphin, Angela Fan, Michael Auli, and David
Grangier. Language modeling with gated convolutional net-
works.
In International conference on machine learning,
pages 933–941. PMLR, 2017. 5
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 2, 7
[9] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico
Kolter. Multiplicative filter networks. In International Con-
ference on Learning Representations, 2020. 2, 4
[10] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christo-
pher R´e. Hippo: Recurrent memory with optimal polynomial
projections. Advances in neural information processing sys-
tems, 33:1474–1487, 2020. 7
[11] Albert Gu, Karan Goel, and Christopher R´e.
Efficiently
modeling long sequences with structured state spaces. arXiv
preprint arXiv:2111.00396, 2021. 7, 8
[12] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri
Dao, Atri Rudra, and Christopher R´e. Combining recurrent,
convolutional, and continuous-time models with linear state
space layers. Advances in neural information processing sys-
tems, 34:572–585, 2021. 7, 8
[13] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R´e.
On the parameterization and initialization of diagonal state
space models. Advances in Neural Information Processing
Systems, 35:35971–35983, 2022. 7
[14] Ramin
Hasani,
Mathias
Lechner,
Tsun-Hsuan
Wang,
Makram Chahine, Alexander Amini, and Daniela Rus.
Liquid structural state-space models.
arXiv preprint
arXiv:2209.12951, 2022. 7
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 1, 2, 7, 8
[16] Geoffrey E Hinton and David C Plaut. Using fast weights
to deblur old memories. In Proceedings of the ninth annual
conference of the Cognitive Science Society, pages 177–186,
1987. 1
[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7132–7141, 2018. 6
[18] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger.
Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708, 2017. 1, 8
[19] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In International conference on machine learn-
ing, pages 448–456. pmlr, 2015. 3, 8
[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 7
[21] Yann LeCun.
The mnist database of handwritten digits.
http://yann. lecun. com/exdb/mnist/, 1998. 7
[22] Yann LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-
Robert M¨uller.
Efficient backprop.
In Neural networks:
Tricks of the trade, pages 9–50. Springer, 2002. 8
[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision, pages 10012–10022, 2021. 2
[24] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition, pages 11976–11986,
2022. 1, 2, 7
[25] Chunjie Luo, Jianfeng Zhan, Lei Wang, and Wanling
Gao.
Extended batch normalization.
arXiv preprint
arXiv:2003.05569, 2020. 8
[26] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey
Shah, Tri Dao, Stephen Baccus, and Christopher R´e. S4nd:
Modeling images and videos as multidimensional signals
with state spaces. Advances in neural information processing
systems, 35:2846–2861, 2022. 7
[27] Arild Nøkland and Lars Hiller Eidnes. Training neural net-
works with local error signals. In International conference
on machine learning, pages 4839–4850. PMLR, 2019. 1
[28] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fer-
nando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
Resurrecting recurrent neural networks for long sequences.
arXiv preprint arXiv:2303.06349, 2023. 7
[29] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu,
Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,
and Christopher R´e. Hyena hierarchy: Towards larger convo-
lutional language models. arXiv preprint arXiv:2302.10866,
2023. 7
9

[30] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak,
Erik J Bekkers, Mark Hoogendoorn, and Jan C van Gemert.
Flexconv: Continuous kernel convolutions with differen-
tiable kernel sizes. arXiv preprint arXiv:2110.08059, 2021.
7, 8
[31] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M
Tomczak, and Mark Hoogendoorn.
Ckconv:
Continu-
ous kernel convolution for sequential data. arXiv preprint
arXiv:2102.02611, 2021. 7
[32] David W Romero, David M Knigge, Albert Gu, Erik J
Bekkers, Efstratios Gavves, Jakub M Tomczak, and Mark
Hoogendoorn. Towards a general purpose cnn for long range
dependencies in n d. 2022. 7, 8
[33] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang,
Lesia Semenova, and Chudi Zhong. Interpretable machine
learning: Fundamental principles and 10 grand challenges.
Statistic Surveys, 16:1–85, 2022. 1
[34] Tim Salimans and Durk P Kingma. Weight normalization:
A simple reparameterization to accelerate training of deep
neural networks. Advances in neural information processing
systems, 29, 2016. 4
[35] Imanol Schlag, Kazuki Irie, and J¨urgen Schmidhuber. Lin-
ear transformers are secretly fast weight programmers. In In-
ternational Conference on Machine Learning, pages 9355–
9366. PMLR, 2021. 1, 2, 4
[36] J¨urgen Schmidhuber. Learning to control fast-weight mem-
ories: An alternative to dynamic recurrent networks. Neural
Computation, 4(1):131–139, 1992. 1, 2
[37] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 7
[38] Jimmy TH Smith, Andrew Warrington, and Scott W Linder-
man. Simplified state space layers for sequence modeling.
arXiv preprint arXiv:2208.04933, 2022. 7
[39] Kenneth O Stanley, David B D’Ambrosio, and Jason Gauci.
A hypercube-based encoding for evolving large-scale neural
networks. Artificial life, 15(2):185–212, 2009. 1
[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
2818–2826, 2016. 7
[41] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In International
conference on machine learning, pages 6105–6114. PMLR,
2019. 1, 2
[42] Trieu Trinh, Andrew Dai, Thang Luong, and Quoc Le.
Learning longer-term dependencies in rnns with auxiliary
losses. In International Conference on Machine Learning,
pages 4965–4974. PMLR, 2018. 7
[43] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-
stance normalization: The missing ingredient for fast styliza-
tion. arXiv preprint arXiv:1607.08022, 2016. 3
[44] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and
Illia Polosukhin. Attention is all you need. In Neural Infor-
mation Processing Systems, 2017. 1, 2, 4
[45] Andreas Veit, Michael J Wilber, and Serge Belongie. Resid-
ual networks behave like ensembles of relatively shallow net-
works. Advances in neural information processing systems,
29, 2016. 1
[46] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-
meng Zuo, and Qinghua Hu. Eca-net: Efficient channel at-
tention for deep convolutional neural networks. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 11534–11542, 2020. 6
[47] Yuxin Wu and Kaiming He. Group normalization. In Pro-
ceedings of the European conference on computer vision
(ECCV), pages 3–19, 2018. 3
[48] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1492–1500,
2017. 2, 8
[49] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. arXiv preprint arXiv:1605.07146, 2016. 1, 2, 7
[50] Sergey Zagoruyko and Nikos Komodakis. Diracnets: Train-
ing very deep neural networks without skip-connections.
arXiv preprint arXiv:1706.00388, 2017. 4
[51] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen
Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An
attention free transformer. arXiv preprint arXiv:2105.14103,
2021. 2
10

