Invariant Risk Minimization
Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, David Lopez-Paz
1
Introduction
Machine learning suﬀers from a fundamental problem. While machines are able to
learn complex prediction rules by minimizing their training error, data are often
marred by selection biases, confounding factors, and other peculiarities [49, 48, 23].
As such, machines justiﬁably inherit these data biases. This limitation plays an
essential role in the situations where machine learning fails to fulﬁll the promises of
artiﬁcial intelligence. More speciﬁcally, minimizing training error leads machines into
recklessly absorbing all the correlations found in training data. Understanding which
patterns are useful has been previously studied as a correlation-versus-causation
dilemma, since spurious correlations stemming from data biases are unrelated to the
causal explanation of interest [31, 27, 35, 52]. Following this line, we leverage tools
from causation to develop the mathematics of spurious and invariant correlations, in
order to alleviate the excessive reliance of machine learning systems on data biases,
allowing them to generalize to new test distributions.
As a thought experiment, consider the problem of classifying images of cows and
camels [4]. To address this task, we label images of both types of animals. Due to a
selection bias, most pictures of cows are taken in green pastures, while most pictures
of camels happen to be in deserts. After training a convolutional neural network
on this dataset, we observe that the model fails to classify easy examples of images
of cows when they are taken on sandy beaches. Bewildered, we later realize that
our neural network successfully minimized its training error using a simple cheat:
classify green landscapes as cows, and beige landscapes as camels.
To solve the problem described above, we need to identify which properties of the
training data describe spurious correlations (landscapes and contexts), and which
properties represent the phenomenon of interest (animal shapes). Intuitively, a
correlation is spurious when we do not expect it to hold in the future in the same
manner as it held in the past. In other words, spurious correlations do not appear
to be stable properties [54]. Unfortunately, most datasets are not provided in a form
amenable to discover stable properties. Because most machine learning algorithms
depend on the assumption that training and testing data are sampled independently
from the same distribution [51], it is common practice to shuﬄe at random the
training and testing examples. For instance, whereas the original NIST handwritten
data was collected from diﬀerent writers under diﬀerent conditions [19], the popular
MNIST training and testing sets [8] were carefully shuﬄed to represent similar mixes
of writers. Shuﬄing brings the training and testing distributions closer together, but
1
arXiv:1907.02893v3  [stat.ML]  27 Mar 2020

discards what information is stable across writers. However, shuﬄing the data is
something that we do, not something that Nature does for us. When shuﬄing, we
destroy information about how the data distribution changes when one varies the
data sources or collection speciﬁcs. Yet, this information is precisely what tells us
whether a property of the data is spurious or stable.
Here we take a step back, and assume that the training data is collected into
distinct, separate environments. These could represent diﬀerent measuring circum-
stances, locations, times, experimental conditions, external interventions, contexts,
and so forth. Then, we promote learning correlations that are stable across training
environments, as these should (under conditions that we will study) also hold in
novel testing environments.
Returning to our motivational example, we would like to label pictures of cows
and camels under diﬀerent environments. For instance, the pictures of cows taken
in the ﬁrst environment may be located in green pastures 80% of the time. In
the second environment, this proportion could be slightly diﬀerent, say 90% of the
time (since pictures were taken in a diﬀerent country). These two datasets reveal
that “cow” and “green background” are linked by a strong, but varying (spurious)
correlation, which should be discarded in order to generalize to new environments.
Learning machines which pool the data from the two environments together may still
rely on the background bias when addressing the prediction task. But, we believe
that all cows exhibit features that allow us to recognize them as so, regardless of
their context.
This suggests that invariant descriptions of objects relate to the causal explanation
of the object itself (“Why is it a cow?”) [32]. As shown by [40, 22], there exists an
intimate link between invariance and causation useful for generalization. However,
[40] assumes a meaningful causal graph relating the observed variables, an awkward
assumption when dealing with perceptual inputs such as pixels. Furthermore, [40]
only applies to linear models, and scales exponentially with respect to the number
of variables in the learning problem. As such, the seamless integration of causation
tools [41] into machine learning pipelines remains cumbersome, disallowing what we
believe to be a powerful synergy. Here, we work to address these concerns.
Contributions
We propose Invariant Risk Minimization (IRM), a novel learning
paradigm that estimates nonlinear, invariant, causal predictors from multiple training
environments, to enable out-of-distribution (OOD) generalization. To this end, we
ﬁrst analyze in Section 2 how diﬀerent learning techniques fail to generalize OOD.
From this analysis, we derive our IRM principle in Section 3:
To learn invariances across environments, ﬁnd a data representation such that the
optimal classiﬁer on top of that representation matches for all environments.
Section 4 examines the fundamental links between causation, invariance, and OOD
generalization. Section 5 contains basic numerical simulations to validate our claims
empirically. Section 6 concludes with a Socratic dialogue discussing directions for
future research.
2

2
The many faces of generalization
Following [40], we consider datasets De := {(xe
i, ye
i )}ne
i=1 collected under multiple
training environments e ∈Etr. These environments describe the same pair of random
variables measured under diﬀerent conditions. The dataset De, from environment
e, contains examples identically and independently distributed according to some
probability distribution P(Xe, Y e).1 Then, our goal is to use these multiple datasets
to learn a predictor Y ≈f(X), which performs well across a large set of unseen but
related environments Eall ⊃Etr. Namely, we wish to minimize
ROOD(f) = max
e∈Eall Re(f)
where Re(f) := EXe,Y e[ℓ(f(Xe), Y e)] is the risk under environment e. Here, the set
of all environments Eall contains all possible experimental conditions concerning our
system of variables, both observable and hypothetical. This is in the spirit of modal
realism and possible worlds [29], where we could consider, for instance, environments
where we switch oﬀthe Sun. An example clariﬁes our intentions.
Example 1. Consider the structural equation model [55]:
X1 ←Gaussian(0, σ2),
Y ←X1 + Gaussian(0, σ2),
X2 ←Y + Gaussian(0, 1).
As we formalize in Section 4, the set of all environments Eall contains all modiﬁ-
cations of the structural equations for X1 and X2, and those varying the noise of Y
within a ﬁnite range [0, σ2
MAX]. For instance, e ∈Eall may replace the equation of
X2 by Xe
2 ←106, or vary σ2 within this ﬁnite range . To ease exposition consider:
Etr = {replace σ2 by 10, replace σ2 by 20}.
Then, to predict Y from (X1, X2) using a least-squares predictor ˆY e = Xe
1 ˆα1 +Xe
2 ˆα2
for environment e, we can:
• regress from Xe
1, to obtain ˆα1 = 1 and ˆα2 = 0,
• regress from Xe
2, to obtain ˆα1 = 0 and ˆα2 = σ(e)2/(σ(e)2 + 1
2),
• regress from (Xe
1, Xe
2), to obtain ˆα1 = 1/(σ(e)2+1) and ˆα2 = σ(e)2/(σ(e)2+1).
The regression using X1 is our ﬁrst example of an invariant correlation: this is the
only regression whose coeﬃcients do not depend on the environment e. Conversely,
the second and third regressions exhibit coeﬃcients that vary from environment to
environment. These varying (spurious) correlations would not generalize well to
novel test environments. Also, not all invariances are interesting: the regression
from the empty set of features into Y is invariant, but of weak predictive power.
1We omit the superscripts “e” when referring to a random variable regardless of the environment.
3

The invariant rule ˆY = 1 · X1 + 0 · X2 is the only predictor with ﬁnite ROOD
across Eall (to see this, let X2 →∞). Furthermore, this predictor is the causal
explanation about how the target variable takes values across environments. In
other words, it provides the correct description about how the target variable reacts
in response to interventions on each of the inputs. This is compelling, as invariance
is a statistically testable quantity that we can measure to discover causation. We
elaborate on the relationship between invariance and causation in Section 4. But
ﬁrst, how can we learn the invariant, causal regression? Let us review four techniques
commonly discussed in prior work, as well as their limitations.
First, we could merge the data from all the training environments and learn a
predictor that minimizes the training error across the pooled data, using all features.
This is the ubiquitous Empirical Risk Minimization (ERM) principle [50]. In this
example, ERM would grant a large positive coeﬃcient to X2 if the pooled training
environments lead to large σ2(e) (as in our example), departing from invariance.
Second, we could minimize Rrob(f) = maxe∈Etr Re(f) −re, a robust learning
objective where the constants re serve as environment baselines [2, 6, 15, 46]. Setting
these baselines to zero leads to minimizing the maximum error across environments.
Selecting these baselines adequately prevents noisy environments from dominating
optimization.
For example, [37] selects re = V[Y e] to maximize the minimal
explained variance across environments. While promising, robust learning turns out
to be equivalent to minimizing a weighted average of environment training errors:
Proposition 2. Given KKT diﬀerentiability and qualiﬁcation conditions, ∃λe ≥0
such that the minimizer of Rrob is a ﬁrst-order stationary point of P
e∈Etr λeRe(f).
This proposition shows that robust learning and ERM (a special case of robust
learning with λe =
1
|Etr|) would never discover the desired invariance, obtaining
inﬁnite ROOD. This is because minimizing the risk of any mixture of environments
associated to large σ2(e) yields a predictor with a large weight on X2. Unfortunately,
this correlation will vanish for testing environments associated to small σ2(e).
Third, we could adopt a domain adaptation strategy, and estimate a data repre-
sentation Φ(X1, X2) that follows the same distribution for all environments [16, 33].
This would fail to ﬁnd the true invariance in Example 1, since the distribution of the
true causal feature X1 (and the one of the target Y ) can change across environments.
This illustrates why techniques matching feature distributions sometimes attempt
to enforce the wrong type of invariance, as discussed in Appendix C.
Fourth, we could follow invariant causal prediction techniques [40]. These search
for the subset of variables that, when used to estimate individual regressions for
each environment, produce regression residuals with equal distribution across all
environments. Matching residual distributions is unsuited for our example, since the
noise variance in Y may change across environments.
In sum, ﬁnding invariant predictors even on simple problems such as Example 1 is
surprisingly diﬃcult. To address this issue, we propose Invariant Risk Minimization
(IRM), a learning paradigm to extract nonlinear invariant predictors across multiple
environments, enabling OOD generalization.
4

3
Algorithms for invariant risk minimization
In statistical parlance, our goal is to learn correlations invariant across training
environments. For prediction problems, this means ﬁnding a data representation
such that the optimal classiﬁer,2 on top of that data representation, is the same for
all environments. More formally:
Deﬁnition 3. We say that a data representation Φ : X →H elicits an invariant
predictor w◦Φ across environments E if there is a classiﬁer w : H →Y simultaneously
optimal for all environments, that is, w ∈arg min ¯
w:H→Y Re( ¯w ◦Φ) for all e ∈E.
Why is Deﬁnition 3 equivalent to learning features whose correlations with the
target variable are stable? For loss functions such as the mean squared error and
the cross-entropy, optimal classiﬁers can be written as conditional expectations. In
these cases, a data representation function Φ elicits an invariant predictor across
environments E if and only if for all h in the intersection of the supports of Φ(Xe)
we have E[Y e|Φ(Xe) = h] = E[Y e′|Φ(Xe′) = h], for all e, e′ ∈E.
We believe that this concept of invariance clariﬁes common induction methods
in science. Indeed, some scientiﬁc discoveries can be traced to the realization that
distinct but potentially related phenomena, once described with the correct variables,
appear to obey the same exact physical laws. The precise conservation of these
laws suggests that they remain valid on a far broader range of conditions. If both
Newton’s apple and the planets obey the same equations, chances are that gravitation
is a thing.
To discover these invariances from empirical data, we introduce Invariant Risk
Minimization (IRM), a learning paradigm to estimate data representations eliciting
invariant predictors w ◦Φ across multiple environments. To this end, recall that we
have two goals in mind for the data representation Φ: we want it to be useful to
predict well, and elicit an invariant predictor across Etr. Mathematically, we phrase
these goals as the constrained optimization problem:
min
Φ:X→H
w:H→Y
X
e∈Etr
Re(w ◦Φ)
subject to
w ∈arg min
¯
w:H→Y
Re( ¯w ◦Φ), for all e ∈Etr.
(IRM)
This is a challenging, bi-leveled optimization problem, since each constraint calls an
inner optimization routine. So, we instantiate (IRM) into the practical version:
min
Φ:X→Y
X
e∈Etr
Re(Φ) + λ · ∥∇w|w=1.0 Re(w · Φ)∥2,
(IRMv1)
where Φ becomes the entire invariant predictor, w = 1.0 is a scalar and ﬁxed “dummy”
classiﬁer, the gradient norm penalty is used to measure the optimality of the dummy
classiﬁer at each environment e, and λ ∈[0, ∞) is a regularizer balancing between
predictive power (an ERM term), and the invariance of the predictor 1 · Φ(x).
2We will also use the term “classiﬁer” to denote the last layer w for regression problems.
5

3.1
From (IRM) to (IRMv1)
This section is a voyage circumventing the subtle optimization issues lurking behind
the idealistic objective (IRM), to arrive to the eﬃcient proposal (IRMv1).
3.1.1
Phrasing the constraints as a penalty
We translate the hard constraints in (IRM) into the penalized loss
LIRM(Φ, w) =
X
e∈Etr
Re(w ◦Φ) + λ · D(w, Φ, e)
(1)
where Φ : X →H, the function D(w, Φ, e) measures how close w is to minimizing
Re(w ◦Φ), and λ ∈[0, ∞) is a hyper-parameter balancing predictive power and
invariance. In practice, we would like D(w, Φ, e) to be diﬀerentiable with respect to
Φ and w. Next, we consider linear classiﬁers w to propose one alternative.
3.1.2
Choosing a penalty D for linear classiﬁers w
Consider learning an invariant predictor w ◦Φ, where w is a linear-least squares
regression, and Φ is a nonlinear data representation. In the sequel, all vectors
v ∈Rd are by default in column form, and we denote by v⊤∈R1×d the row form.
By the normal equations, and given a ﬁxed data representation Φ, we can write
we
Φ ∈arg min ¯
w Re( ¯w ◦Φ) as:
we
Φ = EXe 
Φ(Xe)Φ(Xe)⊤−1 EXe,Y e [Φ(Xe)Y e] ,
(2)
where we assumed invertibility. This analytic expression would suggest a simple
discrepancy between two linear least-squares classiﬁers:
Ddist(w, Φ, e) = ∥w −we
Φ∥2.
(3)
Figure 1 uses Example 1 to show why Ddist is a poor discrepancy. The blue
curve shows (3) as we vary the coeﬃcient c for a linear data representation Φ(x) =
x·Diag([1, c]), and w = (1, 0). The coeﬃcient c controls how much the representation
depends on the variable X2, responsible for the spurious correlations in Example 1.
We observe that (3) is discontinuous at c = 0, the value eliciting the invariant
predictor. This happens because when c approaches zero without being exactly
zero, the least-squares rule (2) compensates this change by creating vectors we
Φ
whose second coeﬃcient grows to inﬁnity.
This causes a second problem, the
penalty approaching zero as ∥c∥→∞. The orange curve shows that adding severe
regularization to the least-squares regression does not ﬁx these numerical problems.
To circumvent these issues, we can undo the matrix inversion in (2) to construct:
Dlin(w, Φ, e) =
EXe 
Φ(Xe)Φ(Xe)⊤
w −EXe,Y e [Φ(Xe)Y e]
2 ,
(4)
which measures how much does the classiﬁer w violate the normal equations. The
green curve in Figure 1 shows Dlin as we vary c, when setting w = (1, 0). The
6

−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
c, the weight of Φ on the input with varying correlation
0
2
4
6
8
10
12
invariance penalty
Ddist((1, 0), Φ, e)
Ddist (heavy regularization)
Dlin((1, 0), Φ, e)
Figure 1: Diﬀerent measures of invariance lead to diﬀerent optimization landscapes in our
Example 1. The na¨ıve approach of measuring the distance between optimal classiﬁers Ddist
leads to a discontinuous penalty (solid blue unregularized, dashed orange regularized). In
contrast, the penalty Dlin does not exhibit these problems.
penalty Dlin is smooth (it is a polynomial on both Φ and w), and achieves an
easy-to-reach minimum at c = 0 —the data representation eliciting the invariant
predictor. Furthermore, Dlin(w, Φ, e) = 0 if and only if w ∈arg min ¯
w Re( ¯w ◦Φ). As
a word of caution, we note that the penalty Dlin is non-convex for general Φ.
3.1.3
Fixing the linear classiﬁer w
Even when minimizing (1) over (Φ, w) using Dlin, we encounter one issue. When
considering a pair (γΦ, 1
γ w), it is possible to let Dlin tend to zero without impacting
the ERM term, by letting γ tend to zero. This problem arises because (1) is severely
over-parametrized. In particular, for any invertible mapping Ψ, we can re-write our
invariant predictor as
w ◦Φ =
 w ◦Ψ−1
|
{z
}
˜
w
◦(Ψ ◦Φ)
| {z }
˜Φ
.
This means that we can re-parametrize our invariant predictor as to give w any
non-zero value ˜w of our choosing. Thus, we may restrict our search to the data
representations for which all the environment optimal classiﬁers are equal to the same
ﬁxed vector ˜w. In words, we are relaxing our recipe for invariance into ﬁnding a data
representation such that the optimal classiﬁer, on top of that data representation,
is “ ˜w” for all environments. This turns (1) into a relaxed version of IRM, where
optimization only happens over Φ:
LIRM,w= ˜
w(Φ) =
X
e∈Etr
Re( ˜w ◦Φ) + λ · Dlin( ˜w, Φ, e).
(5)
7

As λ →∞, solutions (Φ∗
λ, ˜w) of (5) tend to solutions (Φ∗, ˜w) of (IRM) for linear ˜w.
3.1.4
Scalar ﬁxed classiﬁers ˜w are suﬃcient to monitor invariance
Perhaps surprisingly, the previous section suggests that ˜w = (1, 0, . . . , 0) would be a
valid choice for our ﬁxed classiﬁer. In this case, only the ﬁrst component of the data
representation would matter! We illustrate this apparent paradox by providing a
complete characterization for the case of linear invariant predictors. In the following
theorem, matrix Φ ∈Rp×d parametrizes the data representation function, vector
w ∈Rp the simultaneously optimal classiﬁer, and v = Φ⊤w the predictor w ◦Φ.
Theorem 4. For all e ∈E, let Re : Rd →R be convex diﬀerentiable cost functions.
A vector v ∈Rd can be written v = Φ⊤w, where Φ ∈Rp×d, and where w ∈Rp
simultaneously minimize Re(w ◦Φ) for all e ∈E, if and only if v⊤∇Re(v) = 0 for
all e ∈E. Furthermore, the matrices Φ for which such a decomposition exists are the
matrices whose nullspace Ker(Φ) is orthogonal to v and contains all the ∇Re(v).
So, any linear invariant predictor can be decomposed as linear data representations
of diﬀerent ranks. In particular, we can restrict our search to matrices Φ ∈R1×d
and let ˜w ∈R1 be the ﬁxed scalar 1.0. This translates (5) into:
LIRM,w=1.0(Φ⊤) =
X
e∈Etr
Re(Φ⊤) + λ · Dlin(1.0, Φ⊤, e).
(6)
Section 4 shows that the existence of decompositions with high-rank data representa-
tion matrices Φ⊤are key to out-of-distribution generalization, regardless of whether
we restrict IRM to search for rank-1 Φ⊤.
Geometrically, each orthogonality condition v⊤∇Re(v) = 0 in Theorem 4 deﬁnes
a (d−1)-dimensional manifold in Rd.
Their intersection is itself a manifold of
dimension greater than d−m, where m is the number of environments. When using
the squared loss, each condition is a quadratic equation whose solutions form an
ellipsoid in Rd. Figure 2 shows how their intersection is composed of multiple
connected components, one of which contains the trivial solution v = 0. This shows
that (6) remains nonconvex, and therefore sensitive to initialization.
3.1.5
Extending to general losses and multivariate outputs
Continuing from (6), we obtain our ﬁnal algorithm (IRMv1) by realizing that the
invariance penalty (4), introduced for the least-squares case, can be written as a
general function of the risk, namely D(1.0, Φ, e) = ∥∇w|w=1.0Re(w · Φ)∥2, where Φ
is again a possibly nonlinear data representation. This expression measures the
optimality of the ﬁxed scalar classiﬁer w = 1.0 for any convex loss, such as the
cross-entropy. If the target space Y returned by Φ has multiple outputs, we multiply
all of them by the ﬁxed scalar classiﬁer w = 1.0.
8

• v⋆
1
2
•
v⋆
2
2
Solutions
intersections of ellipsoids
:
zero is a solution
Figure 2:
The solutions of the invariant linear predictors v = Φ⊤w coincide with the
intersection of the ellipsoids representing the orthogonality condition v⊤∇Re(v) = 0.
3.2
Implementation details
When estimating the objective (IRMv1) using mini-batches for stochastic gradient
descent, one can obtain an unbiased estimate of the squared gradient norm as
b
X
k=1
h
∇w|w=1.0ℓ(w · Φ(Xe,i
k ), Y e,i
k ) · ∇w|w=1.0ℓ(w · Φ(Xe,j
k ), Y e,j
k
)
i
,
where (Xe,i, Y e,i) and (Xe,j, Y e,j) are two random mini-batches of size b from
environment e, and ℓis a loss function. We oﬀer a PyTorch example in Appendix D.
3.3
About nonlinear invariances w
How restrictive is it to assume that the invariant optimal classiﬁer w is linear? One
may argue that given a suﬃciently ﬂexible data representation Φ, it is possible to
write any invariant predictor as 1.0 · Φ. However, enforcing a linear invariance may
grant non-invariant predictors a penalty Dlin equal to zero. For instance, the null
data representation Φ0(Xe) = 0 admits any w as optimal amongst all the linear
classiﬁers for all environments. But, the elicited predictor w ◦Φ0 is not invariant in
cases where E[Y e] ̸= 0. Such null predictor would be discarded by the ERM term in
the IRM objective. In general, minimizing the ERM term Re( ˜w ◦Φ) will drive Φ so
that ˜w is optimal amongst all predictors, even if ˜w is linear.
We leave for future work several questions related to this issue. Are there non-
invariant predictors that would not be discarded by either the ERM or the invariance
term in IRM? What are the beneﬁts of enforcing non-linear invariances w belonging
to larger hypothesis classes W? How can we construct invariance penalties D for
non-linear invariances?
9

4
Invariance, causality and generalization
The newly introduced IRM principle promotes low error and invariance across training
environments Etr. When do these conditions imply invariance across all environments
Eall? More importantly, when do these conditions lead to low error across Eall,
and consequently out-of-distribution generalization? And at a more fundamental
level, how does statistical invariance and out-of-distribution generalization relate to
concepts from the theory of causation?
So far, we have omitted how diﬀerent environments should relate to enable
out-of-distribution generalization. The answer to this question is rooted in the
theory of causation. We begin by assuming that the data from all the environments
share the same underlying Structural Equation Model, or SEM [55, 39]:
Deﬁnition 5. A Structural Equation Model (SEM) C := (S, N) governing the
random vector X = (X1, . . . , Xd) is a set of structural equations:
Si : Xi ←fi(Pa(Xi), Ni),
where Pa(Xi) ⊆{X1, . . . , Xd} \ {Xi} are called the parents of Xi, and the Ni are
independent noise random variables. We say that “Xi causes Xj” if Xi ∈Pa(Xj).
We call causal graph of X to the graph obtained by drawing i) one node for each Xi,
and ii) one edge from Xi to Xj if Xi ∈Pa(Xj). We assume acyclic causal graphs.
By running the structural equations of a SEM C according to the topological
ordering of its causal graph, we can draw samples from the observational distribution
P(X). In addition, we can manipulate (intervene) an unique SEM in diﬀerent ways,
indexed by e, to obtain diﬀerent but related SEMs Ce.
Deﬁnition 6. Consider a SEM C = (S, N). An intervention e on C consists of
replacing one or several of its structural equations to obtain an intervened SEM
Ce = (Se, N e), with structural equations:
Se
i : Xe
i ←f e
i (Pae(Xe
i ), N e
i ),
The variable Xe is intervened if Si ̸= Se
i or Ni ̸= N e
i .
Similarly, by running the structural equations of the intervened SEM Ce, we
can draw samples from the interventional distribution P(Xe). For instance, we
may consider Example 1 and intervene on X2, by holding it constant to zero, thus
replacing the structural equation of X2 by Xe
2 ←0. Admitting a slight abuse of
notation, each intervention e generates a new environment e with interventional
distribution P(Xe, Y e). Valid interventions e, those that do not destroy too much
information about the target variable Y , form the set of all environments Eall.
Prior work [40] considered valid interventions as those that do not change the
structural equation of Y , since arbitrary interventions on this equation render
prediction impossible. In this work, we also allow changes in the noise variance of
Y , since varying noise levels appear in real problems, and these do not aﬀect the
optimal prediction rule. We formalize this as follows.
10

Deﬁnition 7. Consider a SEM C governing the random vector (X1, . . . , Xd, Y ),
and the learning goal of predicting Y from X. Then, the set of all environments
Eall(C) indexes all the interventional distributions P(Xe, Y e) obtainable by valid
interventions e. An intervention e ∈Eall(C) is valid as long as (i) the causal graph
remains acyclic, (ii) E[Y e|Pa(Y )] = E[Y |Pa(Y )], and (iii) V[Y e|Pa(Y )] remains
within a ﬁnite range.
Condition (iii) can be waived if one takes into account environment speciﬁc
baselines into the deﬁnition of ROOD, similar to those appearing in the robust
learning objective Rrob. We leave additional quantiﬁcations of out-of-distribution
generalization for future work.
The previous deﬁnitions establish fundamental links between causation and
invariance. Moreover, one can show that a predictor v : X →Y is invariant across
Eall(C) if and only if it attains optimal ROOD, and if and only if it uses only the
direct causal parents of Y to predict, that is, v(x) = ENY [fY (Pa(Y ), NY )]. The
rest of this section follows on these ideas to showcase how invariance across training
environments can enable out-of-distribution generalization across all environments.
4.1
Generalization theory for IRM
The goal of IRM is to build predictors that generalize out-of-distribution, that is,
achieving low error across Eall. To this end, IRM enforces low error and invariance
across Etr. The bridge from low error and invariance across Etr to low error across
Eall can be traversed in two steps.
First, one can show that low error across Etr and invariance across Eall leads
to low error across Eall. This is because, once the data representation Φ eliciting
an invariant predictor w ◦Φ across Eall is estimated, the generalization error of
w ◦Φ respects standard error bounds. Second, we examine the remaining condition
towards low error across Eall: namely, under which conditions does invariance across
training environments Etr imply invariance across all environments Eall?
For linear IRM, our starting point to answer this question is the theory of
Invariant Causal Prediction (ICP) [40, Theorem 2]. There, the authors prove that
ICP recovers the target invariance as long as the data (i) is Gaussian, (ii) satisﬁes a
linear SEM, and (iii) is obtained by certain types of interventions. Theorem 9 shows
that IRM learns such invariances even when these three assumptions fail to hold. In
particular, we allow for non-Gaussian data, dealing with observations produced as a
linear transformation of the variables with stable and spurious correlations, and do
not require speciﬁc types of interventions or the existence of a causal graph.
The setting of the theorem is as follows. Y e has an invariant correlation with
an unobserved latent variable Ze
1 by a linear relationship Y e = Ze
1 · γ + ϵe, with ϵe
independent of Ze
1. What we observe is Xe, which is a scrambled combination of Ze
1
and another variable Ze
2 that can be arbitrarily correlated with Ze
1 and ϵe. Simply
regressing using all of Xe will then recklessly exploit Ze
2 (since it gives extra, but
spurious, information on ϵe and thus Y e). A particular instance of this setting is
when Ze
1 is the cause of Y e, Ze
2 is an eﬀect, and Xe contains both causes and eﬀects.
To generalize out of distribution the representation has to discard Ze
2 and keep Ze
1.
11

Before showing Theorem 9, we need to make our assumptions precise.
To
learn useful invariances, one must require some degree of diversity across training
environments. On the one hand, extracting two random subsets of examples from a
large dataset does not lead to diverse environments, as both subsets would follow the
same distribution. On the other hand, splitting a large dataset by conditioning on
arbitrary variables can generate diverse environments, but may introduce spurious
correlations and destroy the invariance of interest [40, Section 3.3].
Therefore,
we will require sets of training environments containing suﬃcient diversity and
satisfying an underlying invariance. We formalize the diversity requirement as
needing envirnments to lie in linear general position.
Assumption 8. A set of training environments Etr lie in linear general position of
degree r if |Etr| > d −r + d
r for some r ∈N, and for all non-zero x ∈Rd:
dim

span
n
EXe
h
XeXe⊤i
x −EXe,ϵe [Xeϵe]
o
e∈Etr

> d −r.
Intuitively, the assumption of linear general position limits the extent to which
the training environments are co-linear. Each new environment laying in linear
general position will remove one degree of freedom in the space of invariant solutions.
Fortunately, Theorem 10 shows that the set of cross-products EXe[XeXe⊤] not
satisfying a linear general position has measure zero. Using the assumption of linear
general position, we can show that the invariances that IRM learns across training
environments transfer to all environments.
In words, the next theorem states the following. If one ﬁnds a representation Φ
of rank r eliciting an invariant predictor w ◦Φ across Etr, and Etr lie in linear general
position of degree r, then w ◦Φ is invariant across Eall.
Theorem 9. Assume that
Y e = Ze
1 · γ + ϵe,
Ze
1 ⊥ϵe,
E[ϵe] = 0,
Xe = S(Ze
1, Ze
2).
Here, γ ∈Rc, Ze
1 takes values in Rc, Ze
2 takes values in Rq, and S ∈Rd×(c+q).
Assume that the Z1 component of S is invertible: that there exists ˜S ∈Rc×d such
that ˜S (S(z1, z2)) = z1, for all z1 ∈Rc, z2 ∈Rq. Let Φ ∈Rd×d have rank r > 0.
Then, if at least d −r + d
r training environments Etr ⊆Eall lie in linear general
position of degree r, we have that
Φ EXe
h
XeXe⊤i
Φ⊤w = Φ EXe,Y e [XeY e]
(7)
holds for all e ∈Etr iﬀΦ elicits the invariant predictor Φ⊤w for all e ∈Eall.
The assumptions about linearity, centered noise, and independence between
the noise ϵe and the causal variables Z1 from Theorem 9 also appear in ICP [40,
Assumption 1], implying the invariance E[Y e|Ze
1 = z1] = z1 · γ. As in ICP, we allow
12

correlations between ϵe and the non-causal variables Ze
2, which leads ERM into
absorbing spurious correlations (as in our Example 1, where S = I and Ze
2 = Xe
2).
In addition, our result contains several novelties. First, we do not assume that the
data is Gaussian, the existence of a causal graph, or that the training environments
arise from speciﬁc types of interventions. Second, the result extends to “scrambled
setups” where S ̸= I. These are situations where the causal relations are not deﬁned
on the observable features X, but on a latent variable (Z1, Z2) that IRM needs to
recover and ﬁlter. Third, we show that representations Φ with higher rank need
fewer training environments to generalize. This is encouraging, as representations
with higher rank destroy less information about the learning problem at hand.
We close this section with two important observations. First, while robust learning
generalizes across interpolations of training environments (recall Proposition 2),
learning invariances with IRM buys extrapolation powers. We can observe this in
Example 1 where, using two training environments, robust learning yields predictors
that work well for σ ∈[10, 20], while IRM yields predictors that work well for all σ.
Finally, IRM is a diﬀerentiable function with respect to the covariances of the training
environments. Therefore, in cases when the data follows an approximately invariant
model, IRM should return an approximately invariant solution, being robust to mild
model misspeciﬁcation. This is in contrast to common causal discovery methods
based on thresholding statistical hypothesis tests.
4.2
On the nonlinear case and the number of environments
In the same vein as the linear case, we could attempt to provide IRM with guar-
antees for the nonlinear regime. Namely, we could assume that each constraint
∥∇w|w=1.0Re(w · Φ)∥= 0 removes one degree of freedom from the possible set of
solutions Φ. Then, for a suﬃciently large number of diverse training environments,
we would elicit the invariant predictor. Unfortunately, we were unable to phrase such
a “nonlinear general position” assumption and prove that it holds almost everywhere,
as we did in Theorem 10 for the linear case. We leave this eﬀort for future work.
While general, Theorem 9 is pessimistic, since it requires the number of training
environments to scale linearly with the number of parameters in the representation
matrix Φ. Fortunately, as we will observe in our experiments from Section 5, it is often
the case that two environments are suﬃcient to recover invariances. We believe that
these are problems where E[Y e|Φ(Xe)] cannot match for two diﬀerent environments
e ̸= e′ unless Φ extracts the causal invariance. The discussion from Section 3.3
gains relevance here, since enforcing W-invariance for larger families W should allow
discarding more non-invariant predictors with fewer training environments. All in all,
studying what problems allow the discovery of invariances from few environments is
a promising line of work towards a learning theory of invariance.
4.3
Causation as invariance
We promote invariance as the main feature of causation. Unsurprisingly, we are not
pioneers in doing so. To predict the outcome of an intervention, we rely on (i) the
13

properties of our intervention and (ii) the properties assumed invariant after the
intervention. Pearl’s do-calculus [39] on causal graphs is a framework that tells which
conditionals remain invariant after an intervention. Rubin’s ignorability [44] plays
the same role. What’s often described as autonomy of causal mechanisms [20, 1] is
a speciﬁcation of invariance under intervention. A large body of philosophical work
[47, 42, 38, 12, 54, 13] studies the close link between invariance and causation. Some
works in machine learning [45, 18, 21, 26, 36, 43, 34, 7] pursue similar questions.
The invariance view of causation transcends some of the diﬃculties of working
with causal graphs. For instance, the ideal gas law PV = nRT or Newton’s universal
gravitation F = G m1m2
r2
are diﬃcult to describe using structural equation models
(What causes what?), but are prominent examples of laws that are invariant across
experimental conditions. When collecting data about gases or celestial bodies, the
universality of these laws will manifest as invariant correlations, which will sponsor
valid predictions across environments, as well as the conception of scientiﬁc theories.
Another motivation supporting the invariance view of causation are the problems
studied in machine learning. For instance, consider the task of image classiﬁcation.
Here, the observed variables are hundreds of thousands of correlated pixels. What is
the causal graph governing them? It is reasonable to assume that causation does not
happen between pixels, but between the real-world concepts captured by the camera.
In these cases, invariant correlations in images are a proxy into the causation at play
in the real world. To ﬁnd those invariant correlations, we need methods which can
disentangle the observed pixels into latent variables closer to the realm of causation,
such as IRM. In rare occasions we are truly interested in the entire causal graph
governing all the variables in our learning problem. Rather, our focus is often on the
causal invariances improving generalization across novel distributions of examples.
5
Experiments
We perform two experiments to assess the generalization abilities of IRM across
multiple environments. The source-code is available at
https://github.com/facebookresearch/InvariantRiskMinimization.
5.1
Synthetic data
As a ﬁrst experiment, we extend our motivating Example 1. First, we increase the
dimensionality of each of the two input features in X = (X1, X2) to 10 dimensions.
Second, as a form of model misspeciﬁcation, we allow the existence of a 10-dimensional
hidden confounder variable H. Third, in some cases the features Z will not be
directly observed, but only a scrambled version X = S(Z). Figure 3 summarizes
the SEM generating the data (Xe, Y e) for all environments e in these experiments.
More speciﬁcally, for environment e ∈R, we consider the following variations:
• Scrambled (S) observations, where S is an orthogonal matrix, or
unscrambled (U) observations, where S = I.
14

• Fully-observed (F) graphs, where Wh→1 = Wh→y = Wh→2 = 0, or
partially-observed (P) graphs, where (Wh→1, Wh→y, Wh→2) are Gaussian.
• Homoskedastic (O) Y -noise, where σ2
y = e2 and σ2
2 = 1, or
heteroskedastic (E) Y -noise, where σ2
y = 1 and σ2
2 = e2.
He
Ze
1
Y e
Ze
2
He ←N(0, e2)
Ze
1 ←N(0, e2) + Wh→1He
Y e ←Ze
1 · W1→y + N(0, σ2
y) + Wh→yHe
Ze
2 ←Wy→2Y e + N(0, σ2
2) + Wh→2He
Figure 3: In our synthetic experiments, the task is to predict Y e from Xe = S(Ze
1, Ze
2).
These variations lead to eight setups referred to by their initials. For instance, the
setup “FOS” considers fully-observed (F), homoskedastic Y -noise (O), and scrambled
observations (S). For all variants, (W1→y, Wy→2) have Gaussian entries.
Each
experiment draws 1000 samples from the three training environments Etr = {0.2, 2, 5}.
IRM follows the variant (IRMv1), and uses the environment e = 5 to cross-validate
the invariance regularizer λ. We compare to ERM and ICP [40].
Figure 4 summarizes the results of our experiments. We show two metrics for each
estimated prediction rule ˆY = X1 · ˆW1→y +X2 · ˆWy→2. To this end, we consider a de-
scrambled version of the estimated coeﬃcients ( ˆ
M1→y, ˆ
My→2) = ( ˆW1→y, ˆWy→2)⊤S⊤.
First, the plain barplots shows the average squared error between ˆ
M1→y and W1→y.
This measures how well does a predictor recover the weights associated to the causal
variables. Second, each striped barplot shows the norm of estimated weights ˆ
My→2
associated to the non-causal variable. We would like this norm to be zero, as the
desired invariant causal predictor is ˆY e = (W1→y, 0)⊤S⊤(Xe
1, Xe
2). In summary,
IRM is able to estimate the most accurate causal and non-causal weights across
all experimental conditions.
In most cases, IRM is orders of magnitude more
accurate than ERM (our y-axes are in log-scale). IRM also out-performs ICP, the
previous state-of-the-art method, by a large margin. Our experiments also show the
conservative behaviour of ICP (preferring to reject most covariates as direct causes),
leading to large errors on causal weights and small errors on non-causal weights.
5.2
Colored MNIST
We validate IRM at learning nonlinear invariant predictors with a synthetic binary
classiﬁcation task derived from MNIST. The goal is to predict a binary label assigned
to each image based on the digit. Whereas MNIST images are grayscale, we color
each image either red or green in a way that correlates strongly (but spuriously) with
the class label. By construction, the label is more strongly correlated with the color
than with the digit, so any algorithm purely minimizing training error will tend to
exploit the color. Such algorithms will fail at test time because the direction of the
15

FOU
10−2
10−1
causal error
FOS
10−2
100
FEU
100
3 × 10−1
4 × 10−1
6 × 10−1
FES
100
3 × 10−1
4 × 10−1
6 × 10−1
10−4
10−2
non-causal error
10−3
10−2
100
3 × 10−1
4 × 10−1
6 × 10−1
10−1
2 × 10−1
3 × 10−1
4 × 10−1
6 × 10−1
POU
10−1
causal error
POS
10−1
100
PEU
100
2 × 10−1
3 × 10−1
4 × 10−1
6 × 10−1
PES
100
2 × 10−1
3 × 10−1
4 × 10−1
6 × 10−1
10−1
10−2
non-causal error
2 × 10−2
3 × 10−2
4 × 10−1
6 × 10−1
4 × 10−1
6 × 10−1
ERM
ICP
IRM
Figure 4: Average errors on causal (plain bars) and non-causal (striped bars) weights for
our synthetic experiments. The y-axes are in log-scale. See main text for details.
correlation is reversed in the test environment. By observing that the strength of the
correlation between color and label varies between the two training environments, we
can hope to eliminate color as a predictive feature, resulting in better generalization.
We deﬁne three environments (two training, one test) from MNIST transforming
each example as follows: ﬁrst, assign a preliminary binary label ˜y to the image based
on the digit: ˜y = 0 for digits 0-4 and ˜y = 1 for 5-9. Second, obtain the ﬁnal label y
by ﬂipping ˜y with probability 0.25. Third, sample the color id z by ﬂipping y with
probability pe, where pe is 0.2 in the ﬁrst environment, 0.1 in the second, and 0.9 in
the test one. Finally, color the image red if z = 1 or green if z = 0.
We train MLPs on the colored MNIST training environments using diﬀerent
objectives and report results in Table 1. For each result we report the mean and
standard deviation across ten runs. Training with ERM returns a model with
high accuracy in the training environments but below-chance accuracy in the test
environment, since the ERM model classiﬁes mainly based on color. Training with
IRM results in a model that performs worse on the training environments, but relies
less on the color and hence generalizes better to the test environments. An oracle
that ignores color information by construction outperforms IRM only slightly.
To better understand the behavior of these models, we take advantage of the fact
that h = Φ(x) (the logit) is one-dimensional and y is binary, and plot P(y = 1|h, e)
as a function of h for each environment and each model in Figure 5. We show each
algorithm in a separate plot, and each environment in a separate color. The ﬁgure
shows that, whether considering only the two training environments or all three
16

Algorithm
Acc. train envs.
Acc. test env.
ERM
87.4 ± 0.2
17.1 ± 0.6
IRM (ours)
70.8 ± 0.9
66.9 ± 2.5
Random guessing (hypothetical)
50
50
Optimal invariant model (hypothetical)
75
75
ERM, grayscale model (oracle)
73.5 ± 0.2
73.0 ± 0.4
Table 1: Accuracy (%) of diﬀerent algorithms on the Colored MNIST synthetic task. ERM
fails in the test environment because it relies on spurious color correlations to classify digits.
IRM detects that the color has a spurious correlation with the label and thus uses only the
digit to predict, obtaining better generalization to the new unseen test environment.
−5
0
5
0.0
0.5
1.0
P(y = 1|h)
ERM
−5
0
5
h
IRM
Train env. 1 (e=0.2)
Train env. 2 (e=0.1)
Test env. (e=0.9)
−5
0
5
Oracle
Figure 5: P(y = 1|h) as a function of h for diﬀerent models trained on Colored MNIST: (left)
an ERM-trained model, (center) an IRM-trained model, and (right) an ERM-trained model
which only sees grayscale images and therefore is perfectly invariant by construction. IRM
learns approximate invariance from data alone and generalizes well to the test environment.
environments, the IRM model is closer to achieving invariance than the ERM model.
Notably, the IRM model does not achieve perfect invariance, particularly at the tails
of the P(h). We suspect this is due to ﬁnite sample issues: given the small sample
size at the tails, estimating (and hence minimizing) the small diﬀerences in P(y|h, e)
between training environments can be quite diﬃcult, regardless of the method.
We note that conditional domain adaptation techniques which match P(h|y, e)
across environments could in principle solve this task equally well to IRM, which
matches P(y|h, e). This is because the distribution of the causal features (the digit
shapes) and P(y|e) both happen to be identical across environments. However,
unlike IRM, conditional domain adaptation will fail if, for example, the distribution
of the digits changes across environments. We discuss this further in Appendix C.
Finally, Figure 5 shows that P(y = 1|h) cannot always be expressed with a linear
classiﬁer w. Enforcing nonlinear invariances (Section 3.3) could prove useful here.
17

6
Looking forward: a concluding dialogue
[ Eric and Irma are two graduate students studying the Invariant Risk Minimization (IRM)
manuscript. Over a cup of coﬀee at a caf´e in Palais-Royal, they discuss the advantages and caveats
that invariance brings to Empirical Risk Minimization (ERM). ]
Irma: I have observed that predictors trained with ERM sometimes absorb
biases and spurious correlations from data. This leads to undesirable
behaviours when predicting about examples that do not follow the
distribution of the training data.
Eric: I have observed that too, and I wonder what are the reasons behind such
phenomena. After all, ERM is an optimal principle to learn predictors
from empirical data!
Irma: It is, indeed. But even when your hypothesis class allows you to ﬁnd the
empirical risk minimizer eﬃciently, there are some assumptions at play.
First, ERM assumes that training and testing data are identically and
independently distributed according to the same distribution. Second,
generalization bounds require that the ratio between the capacity of our
hypothesis class and the number of training examples n tends to zero, as
n →∞. Third, ERM achieves zero test error only in the realizable case
—that is, when there exists a function in our hypothesis class able to
achieve zero error. I suspect that violating these assumptions leads ERM
into absorbing spurious correlations, and that this is where invariance
may prove useful.
Eric: Interesting. Should we study the three possibilities in turn?
Irma: Sure thing! But ﬁrst, let’s grab another cup of coﬀee.
[We also encourage the reader to grab a cup of coﬀee.]
∼
Irma: First and foremost, we have the “identically and independently dis-
tributed” (iid) assumption. I once heard Professor Ghahramani refer to
this assumption as “the big lie in machine learning”. This is to say that
all training and testing examples are drawn from the same distribution
P(X, Y ) = P(Y |X)P(X).
Eric: I see.
This is obviously not the case when learning from multiple
environments, as in IRM. Given this factorization, I guess two things are
subject to change: either the marginal distribution P(X) of my inputs,
or the conditional distribution P(Y |X) mapping those inputs into my
targets.
Irma: That’s correct. Let’s focus ﬁrst on the case where P(Xe) changes across
environments e. Some researchers from the ﬁeld of domain adaptation
call this covariate shift. This situation is challenging when the supports
of P(Xe) are disjoint across environments. Actually, without a-priori
knowledge, there is no reason to believe that our predictor will generalize
outside the union of the supports of the training environments.
18

Eric: A daunting challenge, indeed. How could invariance help here?
Irma: Two things come to mind. On the one hand, we could try to transform
our inputs into some features Φ(Xe), as to match the support of all
the training environments. Then, we could learn an invariant classiﬁer
w(Φ(Xe)) on top of the transformed inputs.
[Appendix D studies the
shortcomings of this idea.] On the other hand, we could assume that the
invariant predictor w has a simple structure, that we can estimate given
limited supports. The authors of IRM follow this route, by assuming
linear classiﬁers on top of representations.
Eric: I see! Even though the P(Xe) may be disjoint, if there is a simple
invariance satisﬁed for all training environments separately, it may also
hold in unobserved regions of the space. I wonder if we could go further
by assuming some sort of compositional structure in w, the linear as-
sumption of IRM is just the simplest kind. I say this since compositional
assumptions often enable learning in one part of the input space, and
evaluating on another.
Irma: It sounds reasonable! What about the case where P(Y e|Xe) changes?
Does this happen in normal supervised learning? I remember attending a
lecture by Professor Sch¨olkopf [45, 25] where he mentioned that P(Y e|Xe)
is often invariant across environments when Xe is a cause of Y e, and
that it often varies when Xe is an eﬀect of Y e. For instance, he explains
that MNIST classiﬁcation is anticausal: as in, the observed pixels are an
eﬀect of the mental concept that led the writer to draw the digit in the
ﬁrst place. IRM insists on this relation between invariance and causation,
what do you think?
Eric: I saw that lecture too. Contrary to Professor Sch¨olkopf, I believe that
most supervised learning problems, such as image classiﬁcation, are causal.
In these problems we predict human annotations Y e from pixels Xe,
hoping that the machine imitates this cognitive process. Furthermore,
the annotation process often involves multiple humans in the interest of
making P(Y e|Xe) deterministic. If the annotation process is close to
deterministic and shared across environments, predicting annotations is
a causal problem, with an invariant conditional expectation.
Irma: Oh! This means that in supervised learning problems about predicting
annotations, P(Y e|Xe) is often stable across environments, so ERM has
great chances of succeeding. This is good news: it explains why ERM is
so good at supervised learning, and leaves less to worry about.
Eric: However, if any of the other problems appear (disjoint P(Xe), not
enough data, not enough capacity), ERM could get in trouble, right?
Irma: Indeed! Furthermore, in some supervised learning problems, the label
is not necessarily created from the input. For instance, the input could
be an X-ray image, and the target could be the result of a tumor biopsy
on the same patient. Also, there are problems where we predict parts of
the input from other parts of the input, like in self-supervised learning
[14]. In some other cases, we don’t even have labels! This could include
19

the unsupervised learning of the causal factors of variation behind Xe,
which involves inverting the causal generative process of the data. In
all of these cases, we could be dealing with anticausal problems, where
the conditional distribution is subject to change across environments.
Then, I expect searching for invariance may help by focusing on invariant
predictors that generalize out-of-distribution.
Eric: That is an interesting divide between supervised and unsupervised
learning! [Figure 6 illustrates the main elements of this discussion.]
Nature variables
pixels
Nature causal
mechanisms
label


0
1
...
0


“cat”
human
cognition
supervised/causal learning
unsupervised/anticausal learning?
self-supervised
Figure 6: All learning problems use empirical observations, here referred to as “pixels”. Fol-
lowing a causal and cognitive process, humans produce labels. Therefore, supervised learning
problems predicting annotations from observations are causal, and therefore P(label | pixel)
is often invariant. Conversely, types of unsupervised and self-supervised learning trying
to disentangle the underlying data causal factors of variation (Nature variables) should to
some extent reverse the process generating observations (Nature mechanisms). This leads to
anticausal learning problems, possibly with varying conditional distributions; an opportunity
to leverage invariance. Cat picture by www. flickr. com/ photos/ pustovit .
∼
Eric: Secondly, what about the ratio between the capacity of our classiﬁer
and the number of training examples n? Neural networks often have a
number of parameters on the same order of magnitude, or even greater,
than the number of training examples [56]. In these cases, such ratio
will not tend to zero as n →∞. So, ERM may be in trouble.
Irma: That is correct. Neural networks are often over-parametrized, and
over-parametrization carries subtle consequences. For instance, consider
that we are using the pseudo-inverse to solve an over-parametrized linear
least-squares problem, or using SGD to train an over-parametrized neural
network. Amongst all the zero training error solutions, these procedures
will prefer the solution with the smallest capacity [53, 3]. Unfortunately,
spurious correlations and biases are often simpler to detect than the true
phenomenon of interest [17, 9, 10, 11]. Therefore, low capacity solutions
prefer exploiting those simple but spurious correlations. For instance,
think about relying on large green textures to declare the presence of a
cow on an image.
20

Eric: The cows again!
Irma: Always. Although I can give you a more concrete example. Consider
predicting Y e from Xe = (Xe
1, Xe
2), where:
Y e ←106 · Xe
1α1,
Xe
2 ←106 · Y eα⊤
2 · e,
the coeﬃcients satisfy ∥α1∥= ∥α2∥= 1, the training environments are
e = {1, 10}, and we have n samples for the 2n-dimensional input X. In
this over-parametrized problem, the invariant regression from the cause
X1 requires large capacity, while the spurious regression from the eﬀect
X2 requires low capacity.
Eric: Oh!
Then, the inductive bias of SGD would prefer to exploit the
spurious correlation for prediction. In a nutshell, a deﬁcit of training
examples forces us into regularization, and regularization comes with
the danger of absorbing easy spurious correlations. But, methods based
on invariance should realize that, after removing the nuisance variable
X2, the regression from X1 is invariant, and thus interesting for out-of-
distribution generalization. This means that invariance could sometimes
help ﬁght the issues of small data and over-parametrization. Neat!
∼
Irma: As a ﬁnal obstacle to ERM, we have the case where the capacity of our
hypothesis class is insuﬃcient to solve the learning problem at hand.
Eric: This sounds related to the previous point, in the sense that a model
with low capacity will stick to spurious correlations, if these are easier to
capture.
Irma: That is correct, although I can see an additional problem arising from
insuﬃcient capacity. For instance, the only linear invariant prediction
rule to estimate the quadratic Y e = (Xe)2, where Xe ∼Gaussian(0, e),
is the null predictor Y = 0 · X. Even though X is the only, causal, and
invariance-eliciting covariate!
Eric: Got it. Then, we should expect invariance to have a larger chance of
success when allowing high capacity. For low-capacity problems, I would
rely on cross-validation to lower the importance of the invariance penalty
in IRM, and fall back to good old ERM.
Irma: ERM is really withstanding the test of time, isn’t it?
Eric: Deﬁnitely. From what we have discussed before, I think ERM is specially
useful in the realizable case, when there is a predictor in my hypothesis
class achieving zero error.
Irma: Why so?
Eric: In the realizable case, the optimal invariant predictor has zero error
across all environments.
Therefore it makes sense, as an empirical
principle, to look for zero training error across training environments.
This possibly moves towards an optimal prediction rule on the union of
21

the supports of the training environments. This means that achieving
invariance across all environments using ERM is possible in the realizable
case, although it would require data from lots of environments!
Irma: Wait a minute. Are you saying that achieving zero training error makes
sense from an invariance perspective?
Eric: In the realizable case, I would say so! Turns out all these people training
neural networks to zero training error were onto something!
∼
[ The barista approaches Eric and Irma to let them know that the caf´e is closing. ]
Eric: Thank you for the interesting chat, Irma.
Irma: The pleasure is mine!
Eric: One of my takeaways is that discarding spurious correlations is some-
thing doable even when we have access only to two environments. The
remaining, invariant correlations sketch the core pieces of natural phe-
nomena, which in turn form a simpler model.
Irma: Simple models for a complex world. Why bother with the details, right?
Eric: Hah, right. It seems like regularization is more interesting than we
thought. IRM is a learning principle to discover unknown invariances
from data. This diﬀers from typical regularization techniques to enforce
known invariances, often done by architectural choices (using convolutions
to achieve translation invariance) and data augmentation.
I wonder what other applications we can ﬁnd for invariance. Perhaps
we could think of reinforcement learning episodes as diﬀerent environ-
ments, so we can learn robust policies that leverage the invariant part of
behaviour leading to reward.
Irma: That is an interesting one. I was also thinking that invariance has
something to say about fairness. For instance, we could consider diﬀerent
groups as environments. Then, learning an invariant predictor means
ﬁnding a representation such that the best way to treat individuals with
similar relevant features is shared across groups.
Eric: Interesting! I was also thinking that it may be possible to formalize
IRM in terms of invariance and equivariance concepts from group theory.
Do you want to take a stab at these things tomorrow at the lab?
Irma: Surely. See you tomorrow, Eric.
Eric: See you tomorrow!
[ The students pay their bill, leave the caf´e, and stroll down the streets of Paris, quiet and warm
during the Summer evening. ]
22

Acknowledgements
We are thankful to Francis Bach, Marco Baroni, Ishmael Belghazi, Diane Boucha-
court, Fran¸cois Charton, Yoshua Bengio, Charles Blundell, Joan Bruna, Lars Buesing,
Soumith Chintala, Kyunghyun Cho, Jonathan Gordon, Christina Heinze-Deml, Fer-
enc Husz´ar, Alyosha Efros, Luke Metz, Cijo Jose, Anna Klimovskaia, Yann Ollivier,
Maxime Oquab, Jonas Peters, Alec Radford, Cinjon Resnick, Uri Shalit, Pablo
Sprechmann, S´onar festival, Rachel Ward, and Will Whitney for their help.
References
[1] John Aldrich. Autonomy. Oxford Economic Papers, 1989.
[2] James Andrew Bagnell. Robust supervised learning. In AAAI, 2005.
[3] Peter L. Bartlett, Philip M. Long, G´abor Lugosi, and Alexander Tsigler. Benign
Overﬁtting in Linear Regression. arXiv, 2019.
[4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita.
In ECCV, 2018.
[5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis
of representations for domain adaptation. In NIPS. 2007.
[6] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimiza-
tion. Princeton University Press, 2009.
[7] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, S´ebastien
Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-
transfer objective for learning to disentangle causal mechanisms. arXiv, 2019.
[8] L´eon Bottou, Corinna Cortes, John S. Denker, Harris Drucker, Isabelle Guyon,
Lawrence D. Jackel, Yann Le Cun, Urs A. Muller, Eduard S¨ackinger, Patrice
Simard, and Vladimir Vapnik. Comparison of classiﬁer methods: a case study
in handwritten digit recognition. In ICPR, 1994.
[9] Wieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-
features models works surprisingly well on imagenet. In ICLR, 2019.
[10] Joan Bruna and Stephane Mallat. Invariant scattering convolution networks.
TPAMI, 2013.
[11] Joan Bruna, Stephane Mallat, Emmanuel Bacry, and Jean-Franois Muzy. In-
termittent process analysis with scattering moments. The Annals of Statistics,
2015.
[12] Nancy Cartwright. Two theorems on invariance and causality. Philosophy of
Science, 2003.
23

[13] Patricia W. Cheng and Hongjing Lu. Causal invariance as an essential constraint
for creating a causal representation of the world. The Oxford handbook of causal
reasoning, 2017.
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for language understanding.
NAACL, 2019.
[15] John Duchi, Peter Glynn, and Hongseok Namkoong.
Statistics of robust
optimization: A generalized empirical likelihood approach. arXiv, 2016.
[16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, Fran¸cois Laviolette, Mario March, and Victor Lempitsky. Domain-
adversarial training of neural networks. JMLR, 2016.
[17] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A.
Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards
texture; increasing shape bias improves accuracy and robustness. ICLR, 2019.
[18] AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Kun Zhang.
Learning causal structures using regression invariance. In NIPS, 2017.
[19] Patrick J. Grother. NIST Special Database 19: Handprinted forms and char-
acters database. https://www.nist.gov/srd/nist-special-database-19,
1995. File doc/doc.ps in the 1995 NIST CD ROM NIST Special Database 19.
[20] Trygve Haavelmo. The probability approach in econometrics. Econometrica:
Journal of the Econometric Society, 1944.
[21] Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties
and domain shift robustness. arXiv, 2017.
[22] Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal
prediction for nonlinear models. Journal of Causal Inference, 2018.
[23] Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. Revisiting visual
question answering baselines. In ECCV, 2016.
[24] Fredrik D. Johansson, David A. Sontag, and Rajesh Ranganath. Support and
invertibility in domain-invariant representations. AISTATS, 2019.
[25] Niki Kilbertus, Giambattista Parascandolo, and Bernhard Sch¨olkopf. General-
ization in anti-causal learning. arXiv, 2018.
[26] Kun Kuang, Peng Cui, Susan Athey, Ruoxuan Xiong, and Bo Li.
Stable
prediction across unknown environments. In SIGKDD, 2018.
[27] Brenden M. Lake, Tomer D. Ullman, Joshua B Tenenbaum, and Samuel J.
Gershman. Building machines that learn and think like people. Behavioral and
brain sciences, 2017.
24

[28] James M. Lee. Introduction to Smooth Manifolds. Springer, 2003.
[29] David Lewis. Counterfactuals. John Wiley & Sons, 2013.
[30] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang,
and Dacheng Tao. Deep domain generalization via conditional invariant adver-
sarial networks. In ECCV, 2018.
[31] David Lopez-Paz. From dependence to causation. PhD thesis, University of
Cambridge, 2016.
[32] David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf,
and L´eon Bottou. Discovering causal signals in images. In CVPR, 2017.
[33] Gilles Louppe, Michael Kagan, and Kyle Cranmer. Learning to pivot with
adversarial networks. In Advances in neural information processing systems,
pages 981–990, 2017.
[34] Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip
Versteeg, and Joris M Mooij. Domain adaptation by using causal inference to
predict invariant conditional distributions. In NIPS, 2018.
[35] Gary Marcus. Deep learning: A critical appraisal. arXiv, 2018.
[36] Nicolai Meinshausen. Causality from a distributional robustness point of view.
In Data Science Workshop (DSW), 2018.
[37] Nicolai Meinshausen and Peter B¨uhlmann. Maximin eﬀects in inhomogeneous
large-scale data. The Annals of Statistics, 2015.
[38] Sandra D. Mitchell. Dimensions of scientiﬁc law. Philosophy of Science, 2000.
[39] Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University
Press, 2nd edition, 2009.
[40] Jonas Peters, Peter B¨uhlmann, and Nicolai Meinshausen. Causal inference
using invariant prediction: identiﬁcation and conﬁdence intervals. JRSS B,
2016.
[41] Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of causal
inference: foundations and learning algorithms. MIT press, 2017.
[42] Michael Redhead. Incompleteness, non locality and realism. a prolegomenon to
the philosophy of quantum mechanics. 1987.
[43] Mateo Rojas-Carulla, Bernhard Sch¨olkopf, Richard Turner, and Jonas Peters.
Invariant models for causal transfer learning. JMLR, 2018.
[44] Donald B. Rubin. Estimating causal eﬀects of treatments in randomized and
nonrandomized studies. Journal of educational Psychology, 1974.
25

[45] Bernhard Sch¨olkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun
Zhang, and Joris Mooij. On causal and anticausal learning. In ICML, 2012.
[46] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distribu-
tional robustness with principled adversarial training. ICLR, 2018.
[47] Brian Skyrms. Causal necessity: a pragmatic investigation of the necessity of
laws. Yale University Press, 1980.
[48] Bob L. Sturm. A simple method to determine if a music information retrieval
system is a “horse”. IEEE Transactions on Multimedia, 2014.
[49] Antonio Torralba and Alexei Efros. Unbiased look at dataset bias. In CVPR,
2011.
[50] Vladimir Vapnik. Principles of risk minimization for learning theory. In NIPS.
1992.
[51] Vladimir N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.
[52] Max Welling. Do we still need models or just more data and compute?, 2019.
[53] Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
Recht. The marginal value of adaptive gradient methods in machine learning.
In NIPS. 2017.
[54] James Woodward.
Making things happen: A theory of causal explanation.
Oxford university press, 2005.
[55] Sewall Wright. Correlation and causation. Journal of agricultural research,
1921.
[56] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
Understanding deep learning requires rethinking generalization. ICLR, 2016.
26

A
Additional theorems
Theorem 10. Let Σe
X,X := EXe[XeXe⊤] ∈Sd×d
+
, with Sd×d
+
the space of symmetric
positive semi-deﬁnite matrices, and Σe
X,ϵ := EXe[Xeϵe] ∈Rd.
Then, for any
arbitrary tuple
 Σe
X,ϵ

e∈Etr ∈
 Rd|Etr|, the set
{(Σe
X,X)e∈Etr such that Etr does not satisfy general position}
has measure zero in (Sd×d
+
)|Etr|.
B
Proofs
B.1
Proof of Proposition 2
Let
f ⋆∈min
f
max
e∈Etr Re(f) −re,
M ⋆= max
e∈Etr Re(f ⋆) −re.
Then, the pair (f ⋆, M ⋆) solves the constrained optimization problem
min
f,M
M
s.t.
Re(f) −re ≤M
for all e ∈Etr,
with Lagrangian L(f, M, λ) = M + P
e∈Etr λe(Re(f) −re −M). If the problem
above satisﬁes the KKT diﬀerentiability and qualiﬁcation conditions, then there
exist λe ≥0 with ∇fL(f ⋆, M ⋆, λ) = 0, such that
∇f|f=f ⋆
X
e∈Etr
λeRe(f) = 0.
B.2
Proof of Theorem 4
Let Φ ∈Rp×d, w ∈Rp, and v = Φ⊤w. The simultaneous optimization
∀e
w⋆∈arg min
w∈Rp
Re(w ◦Φ)
(8)
is equivalent to
∀e
v⋆∈arg min
v∈GΦ
Re(v),
(9)
where GΦ = {Φ⊤w : w ∈Rp} ⊂Rd is the set of vectors v = Φ⊤w reachable
by picking any w ∈Rp. It turns out that GΦ = Ker(Φ)⊥, that is, the subspace
orthogonal to the nullspace of Φ. Indeed, for all v = Φ⊤w ∈GΦ and all x ∈Ker(Φ),
27

we have x⊤v = x⊤Φ⊤w = (Φx)⊤w = 0. Therefore GΦ ⊂Ker(Φ)⊥. Since both
subspaces have dimension rank(Φ) = d −dim(Ker(Φ)), they must be equal.
We now prove the theorem: let v = Φ⊤w where Φ ∈Rp×d and w ∈Rp minimizes
all Re(w ◦Φ). Since v ∈GΦ, we have v ∈Ker(Φ)⊥. Since w minimizes Re(Φ⊤w),
we can also write
∂
∂w Re(Φ⊤w) = Φ ∇Re(Φ⊤w) = Φ∇Re(v) = 0 .
(10)
Therefore ∇Re(v) ∈Ker(Φ). Finally v⊤∇Re(v) = w⊤Φ ∇Re(Φ⊤w) = 0.
Conversely, let v ∈Rd satisfy v⊤∇Re(v) = 0 for all e ∈E. Thanks to these
orthogonality conditions, we can construct a subspace that contains all the ∇Re(v)
and is orthogonal to v. Let Φ be any matrix whose nullspace satisﬁes these conditions.
Since v ⊥Ker(Φ), that is, v ∈Ker(Φ)⊥= GΦ, there is a vector w ∈Rp such that
v = Φ⊤w. Finally, since ∇Re(v) ∈Ker(Φ), the derivative (10) is zero.
B.3
Proof of Theorem 9
Observing that Φ EXe,Y e [XeY e] = Φ EXe,ϵe[Xe(( ˜SXe)
⊤γ + ϵe)], we re-write (7) as
Φ



EXe
h
XeXe⊤i
(Φ⊤w −˜S⊤γ) −EXe,ϵe [Xeϵe]
|
{z
}
:=qe



= 0.
(11)
To show that Φ leads to the desired invariant predictor Φ⊤w = ˜S⊤γ, we as-
sume Φ⊤w ̸= ˜S⊤γ and reach a contradiction. First, by Assumption 8, we have
dim(span({qe}e∈Etr)) > d −r. Second, by (11), each qe ∈Ker(Φ). Therefore, it
would follow that dim(Ker(Φ)) > d −r, which contradicts the assumption that
rank(Φ) = r.
B.4
Proof of Theorem 10
Let m = |Etr|, and deﬁne G : Rd \ {0} →Rm×d as (G(x))e,i =
 Σe
X,Xx −Σe
X,ϵ

i .
Let W = G
 Rd \ {0}

⊆Rm×d, which is a linear manifold of dimension at most
d, missing a single point (since G is aﬃne, and its input has dimension d).
For the rest of the proof, let (Σe
X,ϵ)e∈Etr ∈Rd|Etr| be arbitrary and ﬁxed. We
want to show that for generic (Σe
X,X)e∈Etr, if m > d
r + d −r, the matrices G(x)
have rank larger than d −r. Analogously, if LR(m, d, k) ⊆Rm×d is the set of m × d
matrices with rank k, we want to show that W ∩LR(m, d, k) = ∅for all k < d −r.
We need to prove two statements. First, that for generic(Σe
X,X)e∈Etr W and
LR(m, d, k) intersect transversally as manifolds, or don’t intersect at all.
This
will be a standard argument using Thom’s transversality theorem. Second, by
dimension counting, that if W and LR(m, d, k) intersect transversally, and k < d−r,
m >
d
r + d −r, then the dimension of the intersection is negative, which is a
contradiction and thus W and LR(m, d, k) cannot intersect.
28

We then claim that W and LR(m, d, k) are transversal for generic (Σe
X,X)e∈Etr.
To do so, deﬁne
F : (Rd \ {0}) ×
 Sd×d
+
m →Rm×d,
F

x,
 Σe
X,X

e∈Etr
e′
l =

Σe′
X,Xx −Σe′
X,ϵ

l
If we show that ∇x,ΣX,XF : Rd × (Sd×d)m →Rm×d is a surjective linear trans-
formation, then F is transversal to any submanifold of Rm×d (and in particular
to LR(m, d, k)). By the Thom transversality theorem, this implies that the set of
 Σe
X,X

e∈Etr such that W is not transversal to LR(m, d, k) has measure zero in Sd×d
+
,
proving our ﬁrst statement.
Next, we show that ∇x,ΣX,XF is surjective. This follows by by showing that
∇ΣX,XF : (Sd×d)m →Rm×d is surjective, since adding more columns to this
matrix can only increase its rank. We then want to show that the linear map
∇ΣX,XF : (Sd×d)m →Rm×d is surjective. To this end, we can write: ∂Σe
i,jF e′
l
=
δe,e′ (δl,ixj + δl,jxi) , and let C ∈Rm×d. We want to construct a D ∈
 Sd×dm such
that
Ce′
l =
X
i,j,e
δe,e′ (δl,ixj + δl,jxi) De
i,j.
The right hand side equals
X
i,j,e
δe,e′ (δl,ixj + δl,jxi) De
i,j =
X
j
De′
l,jxj +
X
i
De′
i,lxi = (De′x)l + (xDe′)l
If De′ is symmetric, this equals (2Dex)l. Therefore, we only need to show that for
any vector Ce ∈Rd, there is a symmetric matrix De ∈Sd×d with Ce = Dex. To
see this, let O ∈Rd×d be an orthogonal transformation such that Ox has no zero
entries, and name v = Ox, we = OCe. Furthermore, let Ee ∈Rd×d be the diagonal
matrix with entries Ee
i,i = we
i
vi . Then, Ce = OT EeOx. By the spectral theorem,
OT EeO is symmetric, showing that ∇ΣX,XF : (Sd×d)m →Rm×d is surjective, and
thus that W and LR(m, d, k) are transversal for almost any
 Σe
X,X

e∈Etr.
By transversality, we know that W cannot intersect LR(m, d, k) if dim(W) +
dim (LR(m, d, k)) −dim
 Rm×d
< 0. By a dimensional argument (see [28], example
5.30), it follows that codim(LR(m, d, k)) = dim
 Rm×d
−dim (LR(m, d, k)) = (m −
k)(d −k). Therefore, if k < d −r and m > d
r + d −r, it follows that
dim(W) + dim (LR(m, d, k)) −dim
 Rm×d
= dim(W) −codim (LR(m, d, k))
≤d −(m −k)(d −k)
≤d −(m −(d −r))(d −(d −r))
= d −r(m −d + r)
< d −r
d
r + d −r

−d + r

= d −d = 0.
Therefore, W ∩LR(m, d, k) = ∅under these conditions, ﬁnishing the proof.
29

C
Failure cases for Domain Adaptation
Domain adaptation [5] considers labeled data from a source environment es and
unlabeled data from a target environment et with the goal of training a classiﬁer that
works well on et. Many domain adaptation techniques, including the popular Adver-
sarial Domain Adaptation [16, ADA], proceed by learning a feature representation
Φ such that (i) the input marginals P(Φ(Xes)) = P(Φ(Xet)), and (ii) the classiﬁer
w on top of Φ predicts well the labeled data from es. Thus, are domain adaptation
techniques applicable to ﬁnding invariances across multiple environments?
One shall proceed cautiously, as there are important caveats. For instance, con-
sider a binary classiﬁcation problem, where the only diﬀerence between environments
is that P(Y es = 1) = 1
2, but P(Y et = 1) =
9
10. Using these data and the domain
adaptation recipe outlined above, we build a classiﬁer w◦Φ. Since domain adaptation
enforces P(Φ(Xes)) = P(Φ(Xet)), it consequently enforces P( ˆY es) = P( ˆY et), where
ˆY e = w(Φ(Xe)), for all e ∈{es, et}. Then, the classiﬁcation accuracy will be at
most 20%. This is worse than random guessing, in a problem where simply training
on the source domain leads to a classiﬁer that generalizes to the target domain.
Following on this example, we could think of applying conditional domain
adaptation techniques [30, C-ADA]. These enforce one invariance P(Φ(Xes)|Y es) =
P(Φ(Xet)|Y et) per value of Y e. Using Bayes rule, it follows that C-ADA enforces
a stronger condition than invariant prediction when P(Y es) = P(Y et). However,
there are general problems where the invariant predictor cannot be identiﬁed by
C-ADA.
To see this, consider a discrete input feature Xe ∼P(Xe), and a binary target
Y e = F(Xe) ⊕Bernoulli(p). This model represents a generic binary classiﬁcation
problem with label noise. Since the distribution P(Xe) is the only moving part across
environments, the trivial representation Φ(x) = x elicits an invariant prediction rule.
Assuming that the discrete variable Xe takes n values, we can summarize P(Xe) as
the probability n-vector px,e. Then, Φ(Xe) is also discrete, and we can summarize
its distribution as the probability vector pφ,e = Aφpx,e, where Aφ is a matrix of
zeros and ones. By Bayes rule,
πφ,e := P(Φ(Xe)|Y e = 1) = P(Y e = 1|Φ(Xe)) ⊙pφ,e
⟨P(Y e = 1|Φ(Xe)), pφ,e⟩= (AΦ (v ⊙px,e)) ⊙(AΦpx,e)
⟨(AΦ (v ⊙px,e)) , AΦpx,e⟩,
where ⊙is the entry-wise multiplication, ⟨, ⟩is the dot product, and v := P(Y e =
1|Xe) does not depend on e. Unfortunately for C-ADA, it can be shown that the set
Πφ := {(px,e, px,e′) : πφ,e = πφ,e′} has measure zero. Since the union of sets with
zero measure has zero measure, and there exists only a ﬁnite amount of possible Aφ,
the set Πφ has measure zero for any Φ. In conclusion and almost surely, C-ADA
disregards any non-zero data representation eliciting an invariant prediction rule,
regardless of the fact that the trivial representation Φ(x) = x achieves such goal.
As a general remark, domain adaptation is often justiﬁed using the bound [5]:
Erroret(w ◦Φ) ≤Errores(w ◦Φ) + Distance(Φ(Xes), Φ(Xet)) + λ⋆.
30

Here, λ⋆is the error of the optimal classiﬁer in our hypothesis class, operating on
top of Φ, summed over the two domains. Crucially, λ⋆is often disregarded as a
constant, justifying the DA goals (i, ii) outlined above. But, λ⋆depends on the data
representation Φ, instantiating a third trade-oﬀthat it is often ignored. For a more
in depth analysis of this issue, we recommend [24].
D
Minimal implementation of IRM in PyTorch
import
torch
from
torch.autograd
import
grad
def
compute_penalty (losses , dummy_w ):
g1 = grad(losses [0::2]. mean(), dummy_w , create_graph =True )[0]
g2 = grad(losses [1::2]. mean(), dummy_w , create_graph =True )[0]
return (g1 * g2).sum ()
def
example_1(n=10000 , d=2, env =1):
x = torch.randn(n, d) * env
y = x + torch.randn(n, d) * env
z = y + torch.randn(n, d)
return
torch.cat((x, z), 1), y.sum(1, keepdim=True)
phi = torch.nn.Parameter(torch.ones(4, 1))
dummy_w = torch.nn.Parameter(torch.Tensor ([1.0]))
opt = torch.optim.SGD([ phi], lr=1e -3)
mse = torch.nn.MSELoss(reduction="none")
environments = [example_1(env =0.1) ,
example_1(env =1.0)]
for
iteration
in range (50000):
error = 0
penalty = 0
for x_e , y_e in
environments :
p = torch.randperm(len(x_e ))
error_e = mse(x_e[p] @ phi * dummy_w , y_e[p])
penalty
+=
compute_penalty (error_e , dummy_w)
error +=
error_e.mean ()
opt.zero_grad ()
(1e-5 * error + penalty ). backward ()
opt.step ()
if
iteration % 1000 == 0:
print(phi)
31

