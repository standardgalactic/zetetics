WIT: Wikipedia-based Image Text Dataset for Multimodal
Multilingual Machine Learning
Krishna Srinivasan
Google
krishnaps@google.com
Karthik Raman
Google
karthikraman@google.com
Jiecao Chen
Google
chenjiecao@google.com
Michael Bendersky
Google
bemike@google.com
Marc Najork
Google
najork@google.com
ABSTRACT
The milestone improvements brought about by deep representation
learning and pre-training techniques have led to large performance
gains across downstream NLP, IR and Vision tasks. Multimodal mod-
eling techniques aim to leverage large high-quality visio-linguistic
datasets for learning complementary information (across image
and text modalities). In this paper, we introduce the Wikipedia-
based Image Text (WIT) Dataset1 to better facilitate multimodal,
multilingual learning. WIT is composed of a curated set of 37.6
million entity rich image-text examples with 11.5 million unique
images across 108 Wikipedia languages. Its size enables WIT to be
used as a pretraining dataset for multimodal models, as we show
when applied to downstream tasks such as image-text retrieval.
WIT has four main and unique advantages. First, WIT is the largest
multimodal dataset by the number of image-text examples by 3x (at
the time of writing). Second, WIT is massively multilingual (first of
its kind) with coverage over 100+ languages (each of which has at
least 12K examples) and provides cross-lingual texts for many im-
ages. Third, WIT represents a more diverse set of concepts and real
world entities relative to what previous datasets cover. Lastly, WIT
provides a very challenging real-world test set, as we empirically
illustrate using an image-text retrieval task as an example.
KEYWORDS
machine learning, neural networks, multi-modal, multi-lingual,
image-text retrieval
ACM Reference Format:
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc
Najork. 2021. WIT: Wikipedia-based Image Text Dataset for Multimodal
Multilingual Machine Learning. In Proceedings of SIGIR Resource Track. ACM,
New York, NY, USA, 16 pages. https://doi.org/10.1145/1122445.1122456
1https://github.com/google-research-datasets/wit
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR Resource Track, 2021, Virtual
© 2021 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/10.1145/1122445.1122456
1
INTRODUCTION
Deep learning has fundamentally revolutionized the fields of NLP,
IR and Vision via our ability to have a rich semantic understanding
of texts and images. Notable examples of this include Deep CNN
models [29, 34] which set the bar for standard vision tasks like image
recognition and image classification. Attention based transformer
models [35] like BERT [8] have likewise enabled achieving new
benchmark performance across a myriad of text understanding /
NLP / IR tasks. These transformational advances have also found
their way to multimodal tasks such as image-text retrieval / search
[14] and image captioning [36, 41]. Multimodal models – such
as ViLBERT [22], UNITER [6], Unicoder-VL [17] amongst others
[1, 19, 32] – are able to jointly model the complex relationships
between text and visual inputs leading to wins in downstream tasks
like image search, Visual Question Answering (VQA) [2] and Visual
Commonsense Reasoning (VCR) [40].
Figure 1: The Wikipedia page for Half Dome, Yosemite, Cal-
ifornia via Wikimedia Commons.
Accompanying the modeling improvements across these ad-
vancements, an equally critical aspect is the leveraging of massive
datasets to enrich representation learning – often via unsupervised
pretraining. Increasingly, the efficacy of a model correlates strongly
with the size and quality of pretraining data used. For instance,
cutting-edge language models like BERT [8] and T5 [27] rely on in-
creasingly larger text datasets spanning from those in the O(100M)
range like Wikipedia, BooksCorpus [43] to datasets with billions
arXiv:2103.01913v2  [cs.CV]  3 Mar 2021

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
of examples like C4 [27] and mC4 [38]. Similarly, vision models
[9] are reliant on large corpora, such as ImageNet-21k [7] – which
with 14M images is among the largest public datasets. This scale is
important since studies have shown performance increases logarith-
mically with dataset size [33]. Another key dimension of language
datasets is the number of languages covered. By transitioning from
English-only to highly multilingual language datasets, models like
mT5 [38] and mBERT [37], are an important step for researchers
driving globally, equitable availability of information.
Multimodal visio-linguistic models are no different, and rely on
a rich dataset to help them learn to model the relationship between
images and texts. However as seen in Table 1, the scale of current
public datasets pales in comparison to image-only or text-only
ones, with the 30K-sized Flickr [39] and 3.3M-sized Conceptual
Captions (CC) [28] being among the largest ones. Having large
image-text datasets can significantly improve performance, as a
couple of recent works [15, 26] have shown by leveraging larger
noisy (proprietary) datasets. Furthermore the lack of language cov-
erage in these existing datasets (which are mostly only in English)
also impedes research in the multilingual multimodal space – which
we consider a lost opportunity given the potential shown in lever-
aging images (as a language-agnostic medium) to help improve our
multilingual textual understanding [30] or even translate [12].
To address these challenges and advance research on multilin-
gual, multimodal learning we present the Wikipedia-based Image
Text (WIT) Dataset. WIT is created by extracting multiple differ-
ent texts associated with an image (e.g., the reference description
seen in Fig 2) from Wikipedia articles and Wikimedia image links.
This was accompanied by rigorous filtering to only retain high
quality image-text associations. The resulting dataset contains over
37.6 million image-text sets and spans 11.5 million unique images –
making WIT the largest multimodal dataset at the time of writing.
Furthermore WIT provides unparalleled multilingual coverage –
with 12K+ examples in each of 108 languages (53 languages have
100K+ image-text pairs).
Figure 2: The Wikipedia page for Half Dome, Yosemite, Cal-
ifornia via Wikimedia Commons with examples of the dif-
ferent fields extracted and provided in WIT.
Table 1: Existing publicly available image-text datasets
pale in comparison to text-only datasets (e.g., mC4 with
O(Billions) of examples in 100+ languages) and image-only
datasets (e.g., 14M in ImageNet-21k).
Dataset
Images
Text
Languages
Flickr30K [39]
32K
158K
< 8
SBU Captions [24]
∼1M
∼1M
1
MS-COCO [21]
∼330K
∼1.5M
< 4
CC [5]
∼3.3M
∼3.3M
1
WIT
11.5M
37.6M
108
It is worth pointing out that by leveraging Wikipedia’s editing,
verification and correction mechanism, WIT is able to ensure a high-
quality bar. In particular, this use of a curated source like Wikipedia
contrasts with the approach used to create other existing datasets
(e.g. CC [28]) which rely on extracting annotations from web crawls.
We verified the curated quality of the WIT dataset via an extensive
human-annotation process (nearly 4400 image-text examples and
13K judgments across 7 languages), with an overwhelming major-
ity (98.5%) judging the randomly sampled image-text associations
favorably.
Empirical results on image-text retrieval tasks (both zero-shot i.e.,
pretrained model, as well as finetuned model evaluations) demon-
strate the potency of the data. The vast richness of Wikipedia texts
and images (grounded in a diverse set of real-world entities and
attributes) also means that WIT provides for a realistic evaluation
set – one that we demonstrate to be challenging for models trained
using existing datasets.
2
RELATED WORK
Visio-Linguistic (VL) datasets: Flickr30K [39] was among the
first datasets that helped drive early research in this space. Similar
to other such early datasets (e.g. the 330k example MS-COCO), it
was created by having crowd sourced (Mechanical Turk) workers
provide captions for ∼30K images (sampled from Flickr). While the
explicit human-based captioning helps ensure quality, the result-
ing datasets have been recognized as insufficient for significant
real-world improvements given that they are small and expensive
to construct [10, 42]. Furthermore, this manual effort has meant
extending to other languages has proven to be quite challenging.
Consequently there exists only a handful of non-English data collec-
tions such as Multi30K-DE (German) [11], DeCOCO (German) [13],
Multi30K-FR (French) [10], Multi30K-CS (Czech) [3], YJCaptions26k
(Japanese) [23] and MS-COCO-CN (Chinese) [20].
An alternative paradigm to creating such datasets is demon-
strated by the Conceptual Captions (CC) dataset [28]. By leveraging
the alt-text annotations for images from a web crawl, the resulting
dataset was significantly larger than previous ones (∼3.3M image-
text pairs). The drawback with this approach is the reliance on
complex filtering rules and systems to ensure data quality. Unfortu-
nately this makes these extraction-based datasets – like CC and the
recently proposed CC12M [5] – hard to extend and significantly
impacts their coverage and diversity. Perhaps unsurprisingly, the
complex filtering logic has meant that this approach has so far only
been successfully applied to curate English data collections.

WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
SIGIR Resource Track, 2021, Virtual
Table 2: Example of texts extracted for Half Dome example
Field Name
Text
Page Title
Half Dome, Yosemite
Canonical Page URL
en.wikipedia.org/wiki/Half_Dome
Page Description
Half Dome is a granite dome at the
eastern end of Yosemite Valley in
Yosemite National Park, California.
It is a well-known rock formation ...
Reference Description
Sunset over Half Dome from Glacier
Point
Attribution Description
English: Half Dome as viewed from
Glacier Point, Yosemite National
Park, California, United States.
WIT looks to achieve the best of both worlds by leveraging an
extractive approach on a clean, curated multilingual repository of
human knowledge with its accompanying images, illustrations and
detailed text descriptions (Wikipedia).
VL models: A slew of models have been proposed to leverage the
above datasets (either for unsupervised pretraining or finetuning).
For example, ViLBERT [22] uses MS-COCO and CC for pretraining
a multimodal transformer based model. UNITER [6] leverages these
datasets and pretrains on tasks like image-text matching and word
region alignment. Similarly, models like VL-BERT [32], VisualBERT
[19], ImageBERT [25], B2T2 [1] and Unicoder-VL [18], all pretrain
on CC or similar datasets using a variety of objectives and tasks.
Efficacy of these models is often studied on downstream tasks like
image-text retrieval, referring expressions, image captioning, etc
using Flickr30K, MS-COCO and similar curated collections. These
models have also shown that a larger and more varied data collec-
tion, results in better performance across the board in downstream
tasks.
3
WIT: WIKIPEDIA IMAGE TEXT DATASET
We would like to marry the benefits of curated datasets like Flickr30K
and MS-COCO (consistent, high quality image text pairs) with those
of extractive datasets like CC (automatically created and scalable),
while also creating a multilingual and heterogeneous dataset. To do
so, we leverage Wikipedia, which inherently uses crowd-sourcing
in the data creation process – via its editorial review process – to
ensure quality, freshness and accuracy of content. However, even
Wikipedia extractions cannot be directly used as is, due to a plethora
of low-information (e.g., generic) image-text associations which
would not help VL learning. In the remainder of this section, we
describe the WIT creation process and detail the filtering processes
we introduced to ensure that only the most useful data is selected.
3.1
Wikipedia Crawl Data
We started with all Wikipedia content pages (i.e., ignoring other
pages that have discussions, comments and such). These num-
ber about ∼124M pages across 279 languages. We used a Flume
[4] pipeline to programatically process, filter, clean and store the
Wikipedia data. We next extracted images and different texts related
to the image along with some contextual metadata (such as the
page URL, the page title, description ...). This yielded about ∼150M
Table 3: Statistics of the final WIT dataset and availability
of different fields. Tuple refers to one entry in the dataset
comprising the image, the three different possible texts and
the context. Context texts include the page and (hierarchi-
cal) section titles and their respective descriptions
Type
Train
Val
Test
Total / Unique
Rows / Tuples
37.13M
261.8K
210.7K
37.6M
Unique Images
11.4M
58K
57K
11.5M
Ref. Text
16.9M
150K
104K
17.2M / 16.7M
Attr. Text
34.8M
193K
200K
35.2M / 10.9M
Alt Text
5.3M
29K
29K
5.4M / 5.3M
Context Texts
-
-
-
119.8M
tuples of (image data, texts data, contextual data), which were the
input to the different filters described in the subsequent sections.
Note that there tends to be a wide variance of HTML formatting /
layouts used for image captions across (and sometimes even within)
Wikipedias in different languages, and hence our extraction rules
needed to be particularly robust to ensure high coverage.
3.2
The Texts used in WIT
The texts describing the images come from multiple different sources.
The three directly associated with the image are:
(1) Reference description (abbreviated as ref ): This is the cap-
tion that is visible on the wiki page directly below the image.
This is the least common among the three (present in ∼24M
of the tuples) but tends to be the most topical and relevant.
(2) Attribution description (abbreviated as attr): This is the
text found on the Wikimedia page of the image. This text is
common to all occurrences of that image across all Wikipedias
and thus can be in a language different to the original page
article. Often this text is multilingual i.e., with image descrip-
tions in multiple languages. 138M+ of the 150M tuples have
this field – though the vast majority of these are uninfor-
mative or noisy. However the remaining have rich semantic
descriptions of the images that we would like to extract.
(3) Alt-text description (abbreviated as alt): This is the “alt”
text associated with the image. While not visible in general,
it is commonly used for accessibility / screen readers. De-
spite this (surprisingly) we discovered that this was the least
informative of the three texts and in most cases was simply
the image file name (We found that of the 121M+ tuples
containing this text, only a small fraction to be meaningful
descriptions of the image).
In addition to these, we also note that the context part of the
tuple contains additional texts indirectly associated with the image
(such as the section text or page title). A complete example of these
texts, along with other metadata fields (as illustrated in Table 12)
we provide and more detailed statistics are available on the WIT
dataset Github page.
3.3
Text-based Filtering
To clean the low-information texts, we:
(1) Only retained texts that were at least of length 3.

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
(2) Removed any alt-text containing generic phrases such as
’.png’, ’.jpg’, ‘icon’ or ‘stub’ and also phrases with “refer to”,
“alt text” .. etc.
(3) For attributions and alt-text we enforced that
• Image is either JPG or PNG (since these texts for other
image types were almost always unhelpful). GIF images
with a reference description were retained.
• For tuples without a reference description, we enforced
the image is not found in the last sections of the page (i.e.,
the bibliography, external links and such).
3.4
Image & Image-Text based Filtering
We applied the following filters on the images in the tuples:
(1) To ensure rich images, we required that image height and
width were at least 100 pixels.
(2) Based on a detailed analysis, we eliminated images which
were either generic or didn’t have meaningful text associa-
tions. For example, images of maps are prevalent on Wikipedia
for denoting locations. However since these are generic and
not specific to the actual location, the text association is
often incorrect and hence we removed them. Other such
noise patterns included common images (e.g., tiny icons),
placeholder images and generic missing images.
(3) We only retained images that have a research-permissive
license such as Creative Commons (the text of Wikipedia is
licensed under a CC-BY-SA license).
(4) Lastly we found that certain image-text pairs occurred very
frequently. These were often generic images that did not have
much to do with the main article page. Common examples
included flags, logos, maps, insignia and such. To prevent
biasing the data, we heavily under-sampled all such images.
3.5
Additional Filtering
To ensure a high-quality dataset free of inappropriate content, we
removed tuples with questionable images or texts as done by previ-
ous works [28]. In particular we aimed to remove pornographic /
profane / violent / ...content using multiple techniques based on
sophisticated image understanding and multilingual text under-
standing models. Overall these filters help improve data quality
while only eliminating < 0.2% of all tuples.
Akin to other multilingual datasets (e.g., mC4 [38]), we restricted
our initial version to only the top 100 languages and hence only
retained tuples for languages with 12K+ tuples. Lastly we created
partitioned the data into training, validation and test splits (with 50K
images for the latter two) by ensuring that each image only occurs
in a single split.
3.6
Analyzing the WIT Data
As seen in Table 1, the resulting dataset is significantly larger
than previous ones with over 37M (image, text(s), context) tuples,
spanning 108 languages and covering 11.5 million unique images.
Among its many unique aspects and firsts:
• Multiple texts per image: WIT provides for multiple dif-
ferent kinds of texts per image. More than half of the tu-
ples (19.4M) have two or more of reference, attribution and
alt-texts. Table 3 provides some more detailed statistics of
Table 4: WIT: Image-Text Stats by Language
Image-Text
# Lang
Uniq. Images
# Lang
total > 1M
9
images > 1M
6
total > 500K
10
images > 500K
12
total > 100K
36
images > 100K
35
total > 50K
15
images > 50K
17
total > 14K
38
images > 13K
38
the coverage of the different texts. Overall with nearly 32M
unique image-text pairs, WIT is nearly an order of magnitude
larger than prior datasets.
• Highly multilingual: As seen in Table 4, WIT has broad
multilingual coverage. Nearly half of the 100+ languages,
contain 100K+ unique image-text tuples and 100K+ unique
images.
• Large cross-lingual coverage: Images have shown great
promise in helping build cross-lingual models [30, 31]. WIT
can be used to generate 50M+ cross-lingual pairs (i.e., text
descriptions in different languages for the same image) from
3.1M different images using just the reference and alt texts.
We expect this number to be even higher when counting
attributes, many of which are inherently multilingual.
• Contextual understanding: WIT is also the first dataset,
providing for understanding image captions in the context
of the page and surrounding text (incl. ∼120𝑀contextual
texts). For the sake of brevity we explore this in future work.
Figure 3: Human Annotation Template Example
3.7
Human Annotator Validation
To further verify the quality of the WIT dataset we performed a
study using (crowd-sourced) human annotators. As seen in Fig. 3,
we asked raters to answer 3 questions. Given an image and the page
title, raters first evaluate the quality of the attribution description
and reference description in the first two questions (order random-
ized). The third question understands the contextual quality of these
text descriptions given the page description and caption. Each re-
sponse is on a 3-point scale: "Yes" if the text perfectly describes
the image, "Maybe" if it is sufficiently explanatory and "No" if it is
irrelevant or the image is inappropriate.

WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
SIGIR Resource Track, 2021, Virtual
Table 5: Results of the human annotations of data quality.
These examples and ratings are included with the dataset.
Text
EN
non-EN
%Yes
%Maybe
%No
%Yes
%Maybe
%No
Reference
92.2
4.4
3.3
94.1
2.9
2.9
Attribute
92.2
3.3
4.6
93.1
0.8
6.2
Contextual
98.7
0.7
0.6
96.6
1.8
1.6
We randomly sampled nearly 4.4k examples for this evaluation.
To maximize rating quality we used a language identification filter
on the attribution to show raters examples in the language of their
expertise. In addition to rating ∼3𝑘examples in English, we also
rated 300 examples in German, French, Spanish, Russian, Chinese
and 100 examples for Hindi. (We chose these languages to capture
different language families and different sizes – Hindi is only 65𝑡ℎ
in size). Each example was rated by three raters and majority label
was used (Maybe being selected if no majority). As seen from the
results in Table 5, an overwhelming majority of examples were
found to be very helpful. Both reference and attribution were found
to be high-quality (with a slight edge to reference description). The
responses to the third question (which provides the page context)
also validated our hypothesis that the relevance of image captions
is influenced by the context as seen by the near-perfect ratings
when considering the context. Lastly we found no major difference
in performance across the different languages demonstrating the
multilingual data quality.
4
MULTIMODAL EXPERIMENTS WITH WIT
In this section, we empirically demonstrate the efficacy of the WIT
dataset both as a pretraining dataset as well as an evaluation set
for a new image-text retrieval task.
4.1
Experiment Details
Model: For this analyses, we leveraged a two-tower or dual-encoder
model, inspired by previous works that used them to learn multilin-
gual, multimodal models [30]. As the name suggests, the model has
two encoders – one to encoder the text and the other to represent
the images. While the text input to the model was a bag of words,
the image tower, the image was first embedded in a manner similar
to [16]. The final embeddings of these two towers is then combined
using their cosine similarity, which in turn is optimized using a
batch softmax loss. The dual-encoder architecture is illustrated in
Figure 4. Specifically, for a batch of 𝑛image-text embedding pairs,
the complete 𝑛× 𝑛similarity matrix is computed (the (𝑖, 𝑗) entry
being the cosine of the 𝑖𝑡ℎimage embedding and 𝑗𝑡ℎtext embed-
ding) and a softmax loss applied on each of the row. Note that only
the diagonal entries are considered as positive pairs.
Setup: We used a batch size of 128 for training and a batch size of
1000 for evaluation. The learning rate was set to 5e-7 The optimizer
we used was SGD with Momentum. For the text encoder, we used a
bag of words model (with ngrams of size 1 and 2). Each ngram was
mapped to an a one amongst a million vocabulary buckets using a
hash-function to get a 200D embedding. These ngram embeddings
were then summed and passed through a simple FFNN and projected
to a final 64D embedding, to match the size of the image encoder
embedding. The final activation function we used was ReLU.
Figure 4: WIT Dual Encoder Model for Training.
Table 6: Zero-shot evaluation for models using different text
fields on WIT Image-Text Retrieval test sets
Pretrain setup
WIT-All
WIT-EN
WIT-I18N
Data
Text
R@1
R@5
R@1
R@5
R@1
R@5
WIT
ref
0.126
0.258
0.169
0.358
0.114
0.236
WIT
attr
0.293
0.55
0.272
0.523
0.293
0.523
WIT
ref+attr
0.346
0.642
0.344
0.64
0.344
0.633
CC
text
0.048
0.122
0.072
0.186
0.041
0.11
Evaluation: We evaluated the models on the Flickr30K, Multi30K
and MS-COCO test sets, as well as the dedicated test sets released
as part of WIT. We also spliced the WIT test sets into English-only
and i18n (non-English) to understand any performance differences.
In all experiments using WIT for pretraining, we use the entire
training set (i.e., data for all languages). We also pretrained a model
with Conceptual Captions (CC) dataset to compare against. We
used Recall@K (K = 1, 3, 5) as the evaluation metric.
4.2
Evaluating a zero-shot pretrained model
A common evaluation of image-text datasets is as a pretraining
dataset for a model, which is then directly applied to a downstream
task – in our case image-text retrieval – without any finetuning (i.e.,
zero-shot). Since WIT contains multiple different texts associated
with an image, we first set about understanding the effect of pre-
training models on different fields. As seen in Table 6, the different
WIT models all perform quite well on both English and non-English
sets. The strongest performance was consistently obtained by the
concatenation of reference and attribution descriptions – which
we now default to for subsequent experiments. It is worth noting
that the model pretrained on CC lags behind those trained on WIT,
even on the English-only test set.
To better understand this, we next evaluated the WIT and CC
models (in this zero-shot manner) on popular English test collec-
tions from Flickr30K and MS-COCO which are more similar to CC.
As seen in Table 7, the multilingual WIT model trails the English

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
Table 7: Zero-shot Evaluation on Flickr30K, MS-COCO and
WIT test sets for Image-Text Retrieval Task
Pretrain
MS-COCO
Flickr30K
WIT-ALL
R@1
R@5
R@1
R@5
R@1
R@5
WIT-ALL
0.074
0.228
0.054
0.165
0.346
0.642
CC
0.145
0.385
0.111
0.32
0.048
0.122
Table 8: Zero-shot Evaluation on Multi30K and WIT I18N
test sets (CS, DE, FR) for Image-Text Retrieval Task
Exp
Multi30K-R@5
WIT-R@5
CS
DE
FR
CS
DE
FR
WIT-ALL
0.006
0.005
0.006
0.553
0.562
0.599
CC
0.004
0.005
0.004
0.096
0.084
0.104
Table 9: Zero-shot and Finetuned Evaluation on Wiki (Im-
age, Page Title) test set for Retrieval Task
Exp
Finetuning
WIT-All
R@1
R@3
R@5
WIT-EN
None
0.067
0.122
0.152
CC
None
0.012
0.024
0.032
WIT-ALL
WIT-ALL
0.1
0.174
0.214
CC
CC
0.01
0.021
0.029
CC model on these collections, though not as significantly as the
gap between WIT and CC on the heldout WIT test sets.
4.3
Understanding multilingual performance
Since WIT encompasses examples from 100+ languages, we next
evaluated how multilingual the WIT-based models are. For this, we
used Multi30K’s three language test sets (Czech (CS), German (DE)
and French (FR)). We generated similar language subset datasets
from the WIT test set for the same languages (CS, DE, FR) and used
that for evaluation. As shown in Table 8, both models struggle on
the Multi30K dataset, though again the WIT model shines on the
held-out WIT test set. Similar to the Flickr30k dataset, the Multi30k
datasets are quite different from the WIT datasets (as we discuss in
Sec. 4.5) which may explain this behavior.
4.4
Evaluation On (Image, Wiki Page Title)
Retrieval Task
Lastly, we evaluated on a real-world task that’s based on Wikipedia.
This retrieval task requires identifying images that can be found on
a given Wikipedia page, using only the page title. We ran this evalu-
ation in both a zero-shot setting (i.e., pretrained model directly) and
with finetuning on the training set. Unlike the above experiments,
here the input to the text encoder was the page title directly. The
evaluation was done with the held-out WIT test split using the page
title as text. From Table 9, we clearly observe a large performance
gain on this task using WIT relative to the CC model both with and
without finetuning.
Table 10: Vocabulary Comparison
Dataset
Unigrams
freq <= 3
pct freq <= 3
CC
149,924
63,800
42.55%
WIT (ref)
867,906
625,100
72.02%
Table 11: Language Model Comparison
Dataset A vs B
JSD
Flickr vs Flickr Test
0.1679
COCO vs COCO Test
0.1008
CC vs Flickr Test
0.4844
CC vs COCO Test
0.4746
CC vs WIT
0.3825
WIT vs Flickr Test
0.6007
WIT vs COCO Test
0.5957
4.5
Discussion
The above experiments clearly demonstrated that WIT-based pre-
trained models perform extremely well (5x+ gains) on the evalu-
ation sets based on Wikipedia data. However, the models do not
do as well on other image-text datasets (Flickr30K/Multi30k and
MS-COCO). Since the WIT dataset is not lacking in size or diver-
sity, we probed further into what makes these evaluation sets so
different from each other.
4.5.1
Vocabulary Analysis. We first analyzed the vocabulary of the
two datasets we used for pretraining : WIT and CC. Since Wikipedia
is entity heavy with a diverse concept pool, we suspected that the
vocabulary of the WIT dataset may reflect this. As shown in Table
10, this was the case with over 72% of WIT unigrams occurring 3
times or less (vs. 43% for CC).
4.5.2
Language Model. This difference is even more stark when
compared to the test collections used for evaluation (COCO and
Flickr). When we compared the unigram distributions of different
data sets using the Jensen-Shannon Divergence (JSD), we found
a massive difference in the vocabularies and concept coverage of
the data (see Table 11). While the fact that less than a sixth of
WIT is English skews these results slightly, the gap between the
English-only slice and other datasets remains sizeable.
4.5.3
Image entity Analysis. Part of the reason for this difference is
the broad coverage of entities in the WIT dataset. Using an image
classification model to tag all WIT images with entities, we found
that amongst the ∼4.5M entities identified, a large number (≥80%
i.e., ∼3.68M) of the entities occur 3 times or less. Thus similar to the
texts, the image data too is very diverse with not much repetition.
4.5.4
Key differences in texts. Text fields in WIT often tend to be
descriptive, verbose and use specific terminology. However this
causes a mismatch when evaluated on the test collections, which
are often terse single line captions of common words and objects.
The choice of bag of words likely exacerbates this issue. Perhaps the
most important difference is the use of specifics vs general words.
As found in the CC work [28], text hypernymization was crucial to
creating a dataset closer to those used for evaluation. For example
a text like Two sculptures by artist Duncan McKellar adorn

WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
SIGIR Resource Track, 2021, Virtual
trees outside the derelict Norwich Union offices in
Bristol, UK would be transformed to sculptures by person
adorn trees outside the derelict offices so as to remove
specifics (person names, locations, times etc ..). This is likely the
biggest reason why our trained models underperformed on the
existing collections. While there are benefits and drawbacks of
such hypernymization, we would like to add this in future ver-
sions. However there remains significant challenges doing such
replacements for a 100+ language dataset consistently and with
high quality across languages.
5
FUTURE WORK
In our eagerness and excitement to share the WIT Dataset with the
research community, we have just touched the tip of the iceberg by
starting out with an image-text retrieval task using a simple dual
encoder model. Given the superior performance of cross-attention
multimodal transformer models, WIT can potentially be used in
lieu of or in addition to the existing pretraining datasets in models
as illustrated by UNITER, Unicoder-VL, VL-BERT, ... etc. A range
of new i18n tasks can be formulated with WIT as the basis for VQA,
VCR and many others. Similarly, more specific i18n retrieval or
captioning tasks for low resource languages are yet to be explored.
There is also the possibility of using multimodality to enhance
multilingual performance. WIT Dataset provides a crosslingual
corpus of text for the same image which could aid in this idea. We
also hope to leverage the knowledge base and entities and attributes
of WIT to improve Q&A tasks.
6
CONCLUSION
In this paper we introduced the Wikipedia Image Text (WIT) dataset
– the largest (at time of writing), multilingual, multimodal, context-
rich dataset. By extracting texts associated with images and their
surrounding contexts from over a 100 languages, WIT provides for a
rich and diverse dataset. As a result, it is well suited for use in a myr-
iad of ways including pretraining multimodal models, finetuning
image-text retrieval models or building cross-lingual representa-
tions to name a few. Our detailed analysis and quality evaluation,
validate that WIT is a high quality dataset with strong image-text
alignment. We also empirically demonstrated the use of this dataset
as both a pretraining and finetuning set, and in the process un-
covered some shortcomings of existing datasets. We believe this
can serve as a rich resource to drive research in the multilingual,
multimodal space for years to come and enable the community to
building better and more robust visio-linguistic models well suited
to real world tasks.
ACKNOWLEDGMENTS
We thank Beer Changpinyo, Corinna Cortes, Joshua Gang, Chao
Jia, Ashwin Kakarla, Mohammad Khan, Mike Lee, Zhen Li, Piyush
Sharma, Radu Soricut, Yunhsuan Sung, Ashish Vaswani, Yinfei Yang,
and many others for their insightful feedback and help.
REFERENCES
[1] Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. 2019.
Fu-
sion of detected objects in text for visual question answering. arXiv preprint
arXiv:1908.05054 (2019).
[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In
Proceedings of the IEEE international conference on computer vision. 2425–2433.
[3] Loïc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and
Stella Frank. 2018. Findings of the third shared task on multimodal machine
translation. In THIRD CONFERENCE ON MACHINE TRANSLATION (WMT18),
Vol. 2. 308–327.
[4] Craig Chambers, Ashish Raniwala, Frances Perry, Stephen Adams, Robert R
Henry, Robert Bradshaw, and Nathan Weizenbaum. 2010. FlumeJava: easy, effi-
cient data-parallel pipelines. ACM Sigplan Notices 45, 6 (2010), 363–375.
[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Concep-
tual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail
Visual Concepts. arXiv preprint arXiv:2102.08981 (2021).
[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe
Gan, Yu Cheng, and Jingjing Liu. 2019. Uniter: Learning universal image-text
representations. arXiv preprint arXiv:1909.11740 (2019).
[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:
A large-scale hierarchical image database. In 2009 IEEE conference on computer
vision and pattern recognition. Ieee, 248–255.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[10] Desmond Elliott, Stella Frank, Loïc Barrault, Fethi Bougares, and Lucia Specia.
2017. Findings of the second shared task on multimodal machine translation and
multilingual image description. arXiv preprint arXiv:1710.07177 (2017).
[11] Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016. Multi30k:
Multilingual english-german image descriptions. arXiv preprint arXiv:1605.00459
(2016).
[12] John Hewitt, Daphne Ippolito, Brendan Callahan, Reno Kriz, Derry Tanti Wijaya,
and Chris Callison-Burch. 2018. Learning translations via images with a massively
multilingual image dataset. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). 2566–2576.
[13] Julian Hitschler, Shigehiko Schamoni, and Stefan Riezler. 2016. Multimodal pivots
for image caption translation. arXiv preprint arXiv:1601.03916 (2016).
[14] Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image de-
scription as a ranking task: Data, models and evaluation metrics. Journal of
Artificial Intelligence Research 47 (2013), 853–899.
[15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V
Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling Up Visual and
Vision-Language Representation Learning With Noisy Text Supervision. arXiv
preprint arXiv:2102.05918 (2021).
[16] Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev, Yi-Ting
Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, and Sujith Ravi. 2019. Graph-rise:
Graph-regularized image semantic embedding. arXiv preprint arXiv:1902.10814
(2019).
[17] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. 2019.
Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
Pre-training. arXiv e-prints, page. arXiv preprint arXiv:1908.06066 (2019).
[18] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. 2020.
Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal
Pre-Training.. In AAAI. 11336–11344.
[19] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
2019. Visualbert: A simple and performant baseline for vision and language.
arXiv preprint arXiv:1908.03557 (2019).
[20] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang,
and Jieping Xu. 2019. COCO-CN for Cross-Lingual Image Tagging, Captioning,
and Retrieval. IEEE Transactions on Multimedia 21, 9 (2019), 2347–2360.
[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common
objects in context. In European conference on computer vision. Springer, 740–755.
[22] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining
task-agnostic visiolinguistic representations for vision-and-language tasks. In
Advances in Neural Information Processing Systems. 13–23.
[23] Takashi Miyazaki and Nobuyuki Shimizu. 2016. Cross-Lingual Image Caption
Generation. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics, 1780–1790. https://doi.org/10.18653/v1/P16-1168
[24] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011. Im2text: Describing
images using 1 million captioned photographs. Advances in neural information
processing systems 24 (2011), 1143–1151.
[25] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. 2020.
Imagebert: Cross-modal pre-training with large-scale weak-supervised image-
text data. arXiv preprint arXiv:2001.07966 (2020).

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from natural language supervision.
Image 2 (2021), T2.
[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-
its of transfer learning with a unified text-to-text transformer. arXiv preprint
arXiv:1910.10683 (2019).
[28] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Con-
ceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic
image captioning. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). 2556–2565.
[29] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[30] Karan Singhal, Karthik Raman, and Balder ten Cate. 2019. Learning multilingual
word embeddings using image-text data. arXiv preprint arXiv:1905.12260 (2019).
[31] Lucia Specia, Stella Frank, Khalil Sima’An, and Desmond Elliott. 2016. A shared
task on multimodal machine translation and crosslingual image description. In
Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task
Papers. 543–553.
[32] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2019.
Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint
arXiv:1908.08530 (2019).
[33] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
2017.
Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.
arXiv:cs.CV/1707.02968
[34] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
2818–2826.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. Advances in neural information processing systems 30 (2017), 5998–6008.
[36] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show
and tell: A neural image caption generator. In Proceedings of the IEEE conference
on computer vision and pattern recognition. 3156–3164.
[37] Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual
effectiveness of BERT. arXiv preprint arXiv:1904.09077 (2019).
[38] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual
pre-trained text-to-text transformer. arXiv:cs.CL/2010.11934
[39] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image
descriptions to visual denotations: New similarity metrics for semantic infer-
ence over event descriptions. Transactions of the Association for Computational
Linguistics 2 (2014), 67–78.
[40] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition
to cognition: Visual commonsense reasoning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 6720–6731.
[41] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jian-
feng Gao. 2020. Unified Vision-Language Pre-Training for Image Captioning and
VQA.. In AAAI. 13041–13049.
[42] Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, and Zhou Yu. 2018. A visual
attention grounding neural model for multimodal machine translation. arXiv
preprint arXiv:1808.08266 (2018).
[43] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies and reading books. In Proceed-
ings of the IEEE international conference on computer vision. 19–27.
A
APPENDIX

WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
SIGIR Resource Track, 2021, Virtual
Figure 5: WIT Image-Text Example with All Text Annotations

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
Table 12: WIT: A Full Example Illustration : Half Dome, Yosemite
FIELD NAME
FIELD TYPE
VALUE
canonical_page_url
string
https://en.wikipedia.org/wiki/Half_Dome
image_url
string
Link to Yosemite JPEG media
page_title
string
Half Dome
page_description
string
Half Dome is a granite dome at the eastern end of Yosemite
Valley in Yosemite National Park, California. It is a well-known
rock formation in the park, named for its distinct shape. One
side is a sheer face while the other three sides are smooth and
round, making it appear like a dome cut in half. The granite
crest rises more than 4,737 ft above the valley floor.
section_text
string
Half Dome is a granite dome at the eastern end of Yosemite
Valley in Yosemite National Park, California. It is a well-known
rock formation in the park, named for its distinct shape. One
side is a sheer face while the other three sides are smooth and
round, making it appear like a dome cut in half. The granite
crest rises more than 4,737 ft (1,444 m) above the valley floor.
language
string
en
is_main_image
bool
TRUE
mime_type
string
image/jpeg
original_height
int
2988
original_width
int
4752
filtered_reference_description
string
Sunset over Half Dome from Glacier Point
filtered_caption
string
empty value
filtered_attribution_description
string
English: Half Dome as viewed from Glacier Point, Yosemite
National Park, California, United States.
attribution_passes_lang_id
bool
TRUE
page_changed_recently
bool
TRUE
section_title
string
empty value
hierarchical_section_title
string
Half Dome
page_url
string
http://en.wikipedia.org/wiki/Half_Dome

WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
SIGIR Resource Track, 2021, Virtual
Table 13: Language Stats
Lang
Dataset
# Examples
# Uniq. Img
# Ref
# Attr
# Cap
af
val
603
424
371
467
86
af
train
93,755
85,728
47,173
85,180
18,403
af
test
542
497
276
512
116
an
val
158
127
72
124
43
an
train
27,134
24,904
9,317
24,900
7,584
an
test
186
172
73
178
47
ar
val
3,888
2,630
1,520
3,173
98
ar
train
616,618
534,440
172,069
590,016
22,228
ar
test
3,595
3,015
1,065
3,487
134
arz
val
1,288
1,188
68
1,262
7
arz
train
247,771
243,215
9,387
245,931
1,700
arz
test
1,376
1,349
64
1,364
10
ast
val
1,091
754
656
800
16
ast
train
163,466
151,164
86,392
149,505
2,385
ast
test
978
875
532
930
15
az
val
752
541
315
608
47
az
train
122,522
109,663
34,090
117,342
9,815
az
test
681
587
206
663
60
azb
val
1,141
887
308
991
180
azb
train
185,055
180,778
32,959
181,264
28,173
azb
test
1,080
1,035
213
1,062
179
ba
val
247
187
122
187
43
ba
train
39,870
37,115
16,325
37,453
7,996
ba
test
239
214
110
233
35
bar
val
199
143
115
141
24
bar
train
27,762
25,995
14,813
25,307
4,551
bar
test
177
155
93
168
34
be
val
896
679
362
763
112
be
train
147,532
129,774
51,508
138,881
19,117
be
test
845
736
308
812
112
be-tarask
val
385
285
149
325
38
be-tarask
train
64,362
56,142
22,373
60,059
7,673
be-tarask
test
336
303
138
318
46
bg
val
2,601
1,291
1,741
1,453
153
bg
train
282,629
246,330
137,138
257,584
29,052
bg
test
1,635
1,367
884
1,508
169
bn
val
643
411
415
467
45
bn
train
92,497
81,882
51,035
86,040
6,670
bn
test
590
493
343
563
48
br
val
435
298
315
299
25
br
train
65,252
60,404
41,839
55,855
4,228
br
test
410
363
279
363
21
bs
val
506
339
341
346
48
bs
train
72,527
66,194
39,723
65,378
8,072
bs
test
417
374
247
379
52
ca
val
4,310
2,980
1,913
3,455
267
ca
train
698,364
592,765
246,834
654,920
53,396
ca
test
3,818
3,185
1,410
3,637
252
ce
val
330
308
33
317
272
ce
train
54,669
54,043
2,983
54,122
47,050
ce
test
278
276
20
273
237
ceb
val
1,467
1,278
19
1,458
3
ceb
train
273,344
251,059
3,830
272,904
2,082
ceb
test
1,388
1,277
17
1,386
4
ckb
val
138
76
99
81
11
ckb
train
15,424
14,321
8,811
13,800
1,283
ckb
test
109
102
65
103
6

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
Table 14: Language Stats
Lang
Dataset
# Examples
# Uniq. Img
# Ref
# Attr
# Cap
cs
val
4,291
2,855
2,528
3,499
877
cs
train
652,034
557,020
332,616
613,059
147,478
cs
test
3,447
2,886
1,767
3,273
724
cv
val
118
89
60
87
18
cv
train
14,742
14,013
6,467
13,337
1,867
cv
test
103
96
51
94
11
cy
val
773
535
221
656
19
cy
train
122,238
106,448
21,917
117,650
2,966
cy
test
720
618
159
697
25
da
val
1,707
1,117
1,177
1,211
79
da
train
235,285
209,386
142,343
212,290
16,701
da
test
1,397
1,211
864
1,272
96
de
val
23,589
13,374
14,377
17,954
3,973
de
train
3,350,887
2,632,431
1,720,948
3,192,611
721,886
de
test
17,507
13,205
9,413
16,806
3,539
el
val
1,312
808
766
945
88
el
train
187,283
165,482
94,120
170,918
5,664
el
test
1,119
959
589
1,055
41
en
val
45,542
19,979
33,173
29,385
3,193
en
train
5,422,027
3,940,784
3,264,650
5,094,378
561,756
en
test
33,177
21,870
22,439
31,699
3,662
eo
val
1,771
1,247
1,103
1,343
232
eo
train
286,521
254,160
156,601
260,429
44,127
eo
test
1,589
1,339
885
1,484
265
es
val
13,390
7,378
9,644
8,552
221
es
train
1,741,015
1,433,075
1,092,875
1,530,281
37,151
es
test
9,876
7,658
6,461
9,001
198
et
val
1,372
771
1,009
865
49
et
train
169,304
150,267
109,891
150,847
9,761
et
test
966
835
666
872
50
eu
val
1,951
1,520
549
1,715
27
eu
train
321,311
294,256
63,662
310,440
4,914
eu
test
1,771
1,604
393
1,731
15
fa
val
3,162
2,215
1,305
2,562
389
fa
train
487,183
429,833
148,916
468,121
73,117
fa
test
2,839
2,447
962
2,758
437
fi
val
2,493
1,709
1,490
1,846
343
fi
train
377,569
333,999
188,244
348,713
64,404
fi
test
2,166
1,897
1,100
2,032
401
fil
val
273
193
149
221
21
fil
train
37,964
35,009
18,968
34,446
5,221
fil
test
353
242
207
269
33
fr
val
16,822
10,221
8,243
13,463
3,579
fr
train
2,561,756
2,018,324
976,576
2,444,258
658,134
fr
test
13,943
10,479
5,851
13,444
3,521
fy
val
353
218
248
258
17
fy
train
49,108
44,420
31,821
44,169
2,944
fy
test
328
292
227
308
23
ga
val
501
159
461
171
6
ga
train
33,094
31,344
25,471
28,807
1,404
ga
test
232
217
180
212
9
gl
val
1,244
743
817
827
36
gl
train
176,492
151,579
96,982
157,170
6,876
gl
test
1,032
873
579
953
46
hi
val
486
327
306
358
24
hi
train
69,397
61,813
38,280
62,967
4,929
hi
test
542
448
347
522
54

WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
SIGIR Resource Track, 2021, Virtual
Table 15: Language Stats
Lang
Dataset
# Examples
# Uniq. Img
# Ref
# Attr
# Cap
hr
val
918
602
615
631
97
hr
train
139,111
118,919
79,178
123,083
18,866
hr
test
900
722
541
803
124
hsb
val
91
72
29
85
30
hsb
train
16,945
15,571
4,610
16,360
4,898
hsb
test
102
92
33
101
32
ht
val
142
120
35
119
69
ht
train
23,751
23,376
3,745
23,305
12,889
ht
test
148
144
21
147
90
hu
val
3,655
2,564
1,654
2,969
508
hu
train
563,738
495,429
209,013
519,752
86,780
hu
test
3,144
2,700
1,186
2,920
465
hy
val
1,713
1,235
670
1,379
37
hy
train
258,993
237,008
71,621
247,576
6,886
hy
test
1,454
1,289
419
1,401
52
ia
val
116
99
17
104
7
ia
train
16,666
16,250
2,075
16,112
698
ia
test
105
100
13
103
8
id
val
1,865
1,144
1,179
1,316
122
id
train
271,133
234,762
146,112
247,845
22,030
id
test
1,593
1,338
938
1,483
143
io
val
127
84
77
90
20
io
train
21,704
20,045
9,684
19,776
4,813
io
test
143
125
70
133
25
is
val
242
171
190
165
18
is
train
35,104
32,701
24,350
30,276
3,213
is
test
219
201
167
198
16
it
val
10,111
5,858
5,990
7,236
1,126
it
train
1,405,878
1,155,676
649,558
1,323,265
205,415
it
test
7,717
6,106
3,683
7,358
1,081
iw
val
2,775
1,610
1,636
2,168
152
iw
train
362,493
312,224
174,576
337,418
25,760
iw
test
2,091
1,740
1,052
1,949
137
ja
val
8,851
4,313
6,410
5,367
504
ja
train
1,084,871
874,699
624,537
1,003,353
92,839
ja
test
6,023
4,647
3,598
5,590
556
jv
val
130
99
83
97
6
jv
train
20,119
19,143
12,097
18,088
1,274
jv
test
122
116
73
119
8
ka
val
629
468
270
513
29
ka
train
104,856
94,129
36,443
97,517
8,248
ka
test
656
566
244
620
63
kk
val
325
245
150
266
69
kk
train
57,517
52,867
23,358
53,446
12,249
kk
test
336
297
158
320
63
kn
val
214
150
160
162
17
kn
train
30,406
28,060
21,687
27,315
1,566
kn
test
231
201
184
215
16
ko
val
2,065
1,386
1,138
1,573
183
ko
train
308,740
274,285
137,209
285,007
27,979
ko
test
1,802
1,586
881
1,674
165
la
val
1,209
680
1,027
658
96
la
train
142,674
134,587
108,875
124,441
18,944
la
test
742
690
569
674
101
lah
val
170
125
70
140
9
lah
train
30,825
27,456
9,590
29,383
1,634
lah
test
206
183
69
204
13

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
Table 16: Language Stats
Lang
Dataset
# Examples
# Uniq. Img
# Ref
# Attr
# Cap
lb
val
324
227
195
243
14
lb
train
49,401
43,998
22,042
46,565
3,278
lb
test
274
251
130
260
19
lmo
val
156
122
71
112
31
lmo
train
23,815
23,058
9,675
21,607
4,387
lmo
test
124
118
56
113
26
lt
val
967
723
491
774
77
lt
train
150,747
137,380
62,512
139,776
12,861
lt
test
817
728
351
772
71
lv
val
609
451
311
501
139
lv
train
95,165
83,567
43,276
88,648
25,237
lv
test
573
489
278
522
172
mg
val
115
96
41
81
45
mg
train
21,397
21,200
5,077
17,919
57
mg
test
143
142
23
127
15,277
mk
val
723
485
431
492
75
mk
train
113,482
97,839
59,047
103,236
25
mk
test
676
591
366
626
6,041
ml
val
564
371
322
424
54
ml
train
90,149
80,683
42,723
84,472
10
ml
test
570
494
292
546
1,990
mn
val
128
104
73
100
8
mn
train
19,695
18,715
8,167
18,307
14
mn
test
109
102
48
105
1,621
mr
val
169
131
93
141
6
mr
train
24,262
22,343
12,210
22,664
63
mr
test
165
148
85
162
11,330
ms
val
728
492
444
562
88
ms
train
113,190
103,204
57,894
103,981
3
ms
test
687
620
389
630
1,537
my
val
107
67
71
79
11
my
train
17,259
16,083
10,015
15,961
739
my
test
104
97
59
97
131,801
nan
val
1,387
831
603
840
631
nan
train
161,287
159,564
19,725
152,725
3
nan
test
809
792
126
765
888
nds
val
144
106
127
99
11
nds
train
19,024
17,954
15,735
15,998
7
nds
test
118
105
97
113
1,554
ne
val
128
100
66
108
9
ne
train
21,415
19,197
9,177
20,430
1,993
ne
test
150
134
72
150
360,360
nl
val
7,888
5,014
3,434
6,524
1,825
nl
train
1,225,211
969,736
405,788
1,169,330
87
nl
test
6,511
4,975
2,300
6,244
15,892
nn
val
796
535
498
589
101
nn
train
117,036
106,463
62,556
105,519
365
nn
test
757
639
413
700
64,330
no
val
2,971
1,940
1,759
2,316
452
no
train
430,419
362,192
219,405
397,910
2
no
test
2,662
2,072
1,510
2,496
6
nv
val
118
106
20
117
2,265
nv
train
20,691
18,386
3,300
20,479
13
nv
test
92
83
21
89
5
oc
val
347
251
215
249
1,354
oc
train
62,915
59,145
32,093
56,856
22
oc
test
363
326
209
325
2,790

WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
SIGIR Resource Track, 2021, Virtual
Table 17: Language Stats
Lang
Dataset
# Examples
# Uniq. Img
# Ref
# Attr
# Cap
pa
val
193
127
126
136
505,890
pa
train
27,122
25,126
15,053
25,113
2,642
pa
test
211
184
120
206
416
pl
val
7,036
4,773
3,308
5,783
73,480
pl
train
1,114,489
938,346
429,129
1,054,164
396
pl
test
5,747
4,798
2,235
5,507
16
pt
val
6,515
3,354
3,920
4,134
3,093
pt
train
804,395
672,754
332,753
754,910
26
pt
test
4,592
3,684
2,017
4,372
544
qu
val
117
75
61
94
102,809
qu
train
18,900
15,381
8,596
17,116
540
qu
test
134
103
65
128
797
ro
val
2,294
1,636
924
1,939
141,748
ro
train
375,263
324,990
116,427
357,865
764
ro
test
2,095
1,772
719
2,013
43
ru
val
9,808
6,225
4,445
8,067
8,537
ru
train
1,534,893
1,240,599
569,666
1,467,699
54
ru
test
8,780
6,792
3,610
8,485
7
sco
val
329
240
208
246
1,073
sco
train
47,619
45,243
28,392
42,170
12
sco
test
304
277
197
275
330
si
val
95
66
68
63
66,022
si
train
14,903
13,392
9,344
13,250
326
si
test
124
108
76
115
131
sk
val
1,433
1,111
520
1,234
27,002
sk
train
229,755
209,237
62,766
218,817
167
sk
test
1,233
1,087
385
1,180
59
sl
val
1,055
673
638
746
11,113
sl
train
146,551
130,395
71,919
134,643
51
sl
test
839
735
429
793
100
sq
val
355
262
214
263
18,810
sq
train
48,942
44,433
22,455
45,258
103
sq
test
273
243
133
252
58
sr
val
2,649
1,538
1,617
1,719
11,953
sr
train
358,505
314,625
166,598
324,088
81
sr
test
2,000
1,720
1,010
1,827
450
sr-Latn
val
1,007
723
563
707
87,196
sr-Latn
train
165,373
150,072
75,782
143,264
520
sr-Latn
test
957
840
463
847
11
sv
val
6,330
4,058
3,252
4,817
2,259
sv
train
918,732
791,029
363,165
877,774
10
sv
test
4,914
4,171
2,130
4,732
33
sw
val
201
136
153
131
5,963
sw
train
28,993
27,225
22,141
24,424
31
sw
test
164
151
131
145
17
ta
val
608
362
410
424
2,501
ta
train
85,453
75,394
48,154
77,670
17
ta
test
526
446
318
496
11
te
val
279
160
190
189
1,823
te
train
37,589
33,063
23,336
34,776
14
te
test
267
223
168
255
37
tg
val
155
125
57
132
8,480
tg
train
22,594
21,557
6,660
21,883
58
tg
test
143
137
38
142
142
th
val
793
544
437
579
25,437
th
train
120,592
105,618
54,853
110,325
151
th
test
750
633
366
697
12

SIGIR Resource Track, 2021, Virtual
Srinivasan and Raman, et al.
Table 18: Language Stats
Lang
Dataset
# Examples
# Uniq. Img
# Ref
# Attr
# Cap
tr
val
1,840
1,265
1,023
1,413
2,340
tr
train
270,292
237,703
126,042
250,380
10
tr
test
1,591
1,366
762
1,509
430
tt
val
274
225
85
242
78,021
tt
train
44,622
42,005
9,852
42,849
464
tt
test
285
268
74
274
173
uk
val
5,990
4,135
2,316
4,983
27,968
uk
train
924,811
787,944
279,548
885,861
176
uk
test
4,985
4,177
1,503
4,837
165
ur
val
567
413
291
475
27,394
ur
train
91,264
82,093
36,659
86,497
142
ur
test
580
489
246
558
251
uz
val
272
243
64
249
42,060
uz
train
42,808
42,084
7,722
41,971
205
uz
test
239
238
59
239
310
vec
val
290
268
25
278
61,086
vec
train
48,684
48,262
3,728
47,735
330
vec
test
265
264
40
256
3
vi
val
3,351
2,321
1,467
2,719
619
vi
train
515,937
453,326
177,565
491,288
4
vi
test
2,696
2,311
1,051
2,593
18
vo
val
180
157
167
86
4,636
vo
train
30,564
29,293
27,485
16,248
17
vo
test
134
127
116
83
2
war
val
443
354
72
400
1,213
war
train
75,167
68,979
9,022
71,917
13
war
test
334
312
45
316
11
xmf
val
86
77
33
68
1,807
xmf
train
15,221
14,405
5,562
13,801
11
xmf
test
92
89
27
89
292
yue
val
333
260
243
255
58,212
yue
train
52,494
49,464
34,196
46,744
357
yue
test
309
293
215
286
291
zh
val
5,493
3,504
3,164
4,306
58,289
zh
train
834,324
691,440
411,939
784,829
358
zh
test
4,947
3,905
2,733
4,716
zh-TW
val
5,493
3,508
3,157
4,312
zh-TW
train
834,941
691,337
412,317
785,378
zh-TW
test
4,955
3,903
2,736
4,721

