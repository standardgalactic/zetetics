Władysław Homenda, Witold Pedrycz
Automata Theory and Formal Languages

Also of Interest
Algorithms
Design and Analysis
Sushil C. Dimri, Preeti Malik, Mangey Ram, 2021
ISBN 978-3-11-069341-6, e-ISBN (PDF) 978-3-11-067669-3,
e-ISBN (EPUB) 978-3-11-067677-8
De Gruyter Series on the Applications of Mathematics in Engineering
and Information Sciences
Edited by Mangey Ram
ISSN 2626-5427, e-ISSN 2626-5435
Robotic Process Automation
Management, Technology, Applications
Edited by Christian Czarnecki, Peter Fettke, 2021
ISBN 978-3-11-067668-6, e-ISBN (PDF) 978-3-11-067669-3,
e-ISBN (EPUB) 978-3-11-067677-8
AutomationML
A Practical Guide
Edited by Rainer Drath, 2021
ISBN 978-3-11-074622-8, e-ISBN (PDF) 978-3-11-074623-5,
e-ISBN (EPUB) 978-3-11-074659-4

Władysław Homenda, Witold Pedrycz
Automata Theory
and Formal
Languages
|

Authors
Prof. Władysław Homenda
Warsaw University of Technology
Faculty of Mathematics and
Information Science
ul. Koszykowa 75
00-662 Warsaw
Poland
University of Information Technology and
Management
Faculty of Applied Information Technology
ul. Sucharskiego 2
35-225 Rzeszów
Poland
homenda@mini.pw.edu.pl
Prof. Witold Pedrycz
University of Alberta
Dept. of Electrical &
Computer Engineering
ECERF Building
Edmonton T6R 2V4
Canada
Polish Academy of Sciences
Systems Research Institute
ul. Newelska 6
01-447 Warsaw
Poland
wpedrycz@ualberta.ca
ISBN 978-3-11-075227-4
e-ISBN (PDF) 978-3-11-075230-4
e-ISBN (EPUB) 978-3-11-075231-1
Library of Congress Control Number: 2021947389
Bibliographic information published by the Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek lists this publication in the Deutsche Nationalbibliografie;
detailed bibliographic data are available on the Internet at http://dnb.dnb.de.
© 2022 Walter de Gruyter GmbH, Berlin/Boston
Cover image: Suebsiri / iStock / Getty Images Plus
Typesetting: VTeX UAB, Lithuania
Printing and binding: CPI books GmbH, Leck
www.degruyter.com

Foreword
What is Automata Theory and what are its relations to formal languages? What can
they do? How do they work? How can one use them? This excellent book by Władysław
Homenda and Witold Pedrycz provides the reader a systematic entry path into the an-
swers of those questions. Every effort has been made to produce a manuscript that
is very easily understood, without oversimplification of the subject matter. Here is a
clear, step-by-step Introduction to Formal Languages and Automata Theory address-
ing the most important elements in the field.
To view the developments in this field, a bit of personal history is in order. When
Sam Lee and I wrote our book “Fuzzy Switching and Automata: Theory and Applica-
tions” in 1979, our expectation was the applicability of the contents. However, it is the
seminal work of many researchers since then that showed how Automata Theory and
Formal Languages could be translated into a working reality. It is within the past few
years that it has become increasingly clear that a symbiotic relationship between this
field and AI, for example, as well with a variety of techniques from Uncertainty Man-
agement can be exploited to conceive, design, and build systems and products having
high MIQ (Machine Intelligence Quotient).
Viewed with those recentdevelopments,the importanceof the presentmanuscript
is easily understood. Professors Homenda and Pedrycz have assembled an impres-
sive authoritative treatment of some of the basic issues arising in the field. Professors
Homenda and Pedrycz deserve much credit as well as our congratulations on pro-
ducing an excellent text with authority, originality, and skill, which make this work a
pleasure to study and read, and it is only my hope that this outstanding text will serve
as an impetus for continued interest in the study and research in this exciting field.
Tampa, Florida, September 11, 2021
Abraham Kandel
https://doi.org/10.1515/9783110752304-201


Preface
Automata theory and formal languages are the cornerstone of computer science and
all information processing pursuits. These fundamentals are at the center of any com-
puter science, computer engineering and information technologies curriculum. Writ-
ing a textbook in this area is a challenging endeavor. The area is well established yet
the ongoing technological trends and the advancements in application areas call for a
creative exposure of the fundamental material. This textbook is aimed at students in
senior years of undergraduate programs and the first year of graduate programs (both
at the MSc and PhD level).
When writing this text, we strived for a systematic and clear exposure of the key
ideas by positioning them in the context of the overall picture and a spectrum of pos-
sible applications.
There are several outstanding features that will appeal to a broad spectrum of
instructors and students, which makes this text unique in comparison with the books
available on the market:
(i)
material is prudently selected to cater to a spectrum adequate for course well
adopted by the audience;
(ii) material is made self-contained and motivated;
(iii) the length of the text makes it ideal for a one-semester course by focusing on the
required well-structured and motivated ideas;
(iv) in-depth explanation supported by a wealth of solved examples and problems;
(v) a systematic exposure of the material;
(vi) easy expansion facilitating the usage of the text in a variety of courses.
The textbook could be also regarded as a sound and systematic prerequisite to more
advanced subjects in the areas of advanced algorithms, computability and computa-
tional complexity.
The book is structured into three parts preceded with all required prerequisites
offering a concise and focused background to make the exposure of the material self-
contained. The three main parts are structured to reflect to the general Chomsky hier-
archy of languages, namely:
(i)
we start from the class of regular languages forming the simplest class of sim-
plest languages, and then move toward the class of recursively enumerable lan-
guages, the most complex class of languages (Chapters 2, 3 and 4). We cover tools
that generate languages: regular expressions and regular grammars (Chapter 2),
context-free grammars (Chapter 3) and context-sensitive and unrestricted gram-
mars (Chapter 4);
(ii) we go back to the Chomsky hierarchy by studying tools used to accept languages
starting from the most complex class of languages and ending with the simplest
class of languages (Chapters 5, 6 and 7). Here considered are Turing machines with
https://doi.org/10.1515/9783110752304-202

VIII
|
Preface
their diverse categories, Turing machines with stop property and linear bounded
automata, which accept recursively enumerable languages, recursive languages
and context-sensitive languages (Chapter 5). Then we discuss push-down au-
tomata, which accept context-free languages (Chapter 6). Finally, we study finite
automata, which accept regular languages (Chapter 7);
(iii) finally, we revisit both paths by unifying tools to generate and accept languages
(Chapters 8 and 9).
Figure. Organization of the material.
The text is structured in a way it delivers a great deal of flexibility and makes the ma-
terial suitable for a broad spectrum of possible courses depending upon the audience
and the assumed position of the course in the curriculum. The road map shown in the
figure above displays a number of possibilities highlighting how the material could be
covered:
(i)
Chapters 2–7 are independent from each other; the reader can study selected ones
in any order. In general, minor linkages between Chapters 2–4 and between Chap-
ters 5–7 are not detrimental to the individual studies;
(ii) Chapters 8 and 9 require the mastery of the knowledge of previous chapters for all
of them.
It is worth to draw attention to Chapter 5, where the concept of nondeterminism is
introduced and extensively discussed. This concept is the only obstacle prerequisite
to study some topics covered in Chapters 6 and 7.
While the exposed of the material is self-contained, we assume some familiarity
of basic concepts and methods of algebra, set theory and logic and algorithms and
data structures. These concepts are included in Chapter 1. It is recommended to revise
these concepts prior to studying the remaining parts.

Preface
|
IX
We hope that the textbook will appeal to a broad audience of undergraduate and
graduate students and all of those interested in the systematic and concise exposure
to the fundamentals of automata and formal languages.
Edmonton, Warsaw, August 2021
Władysław Homenda
Witold Pedrycz


Contents
Foreword | V
Preface | VII
1
Preliminaries | 1
1.1
Sets | 1
1.2
Relations | 3
1.2.1
Multiplace relations | 3
1.2.2
Two place relations | 4
1.2.3
Binary relations | 6
1.2.4
Equivalence relations | 7
1.2.5
Closure of relations | 7
1.3
Mathematical induction | 9
1.4
Graphs and trees | 10
1.4.1
Graphs | 11
1.4.2
Trees | 11
1.5
Languages | 13
1.5.1
Basic definitions | 13
1.5.2
Operations on languages | 14
1.6
Grammars | 17
1.7
Problems | 19
Part I: Grammars and generating languages
2
Regular expressions and regular languages | 25
2.1
Regular expressions and regular languages | 25
2.1.1
Regular expressions | 25
2.1.2
Regular languages | 27
2.1.3
The Myhill–Nerode lemma | 29
2.1.4
The pumping lemma | 30
2.1.5
Regular grammars | 32
2.2
Problems | 35
3
Context-free grammars | 37
3.1
Context-free grammars – basics | 37
3.2
Simplification of context-free grammars | 42
3.2.1
Useless symbols | 42
3.2.2
Nullable symbols and ε-productions | 45
3.2.3
Unit productions | 48

XII
|
Contents
3.3
Normal forms of context-free grammars | 50
3.3.1
Chomsky normal form | 50
3.3.2
Greibach normal form | 53
3.4
Pumping and Ogden lemmas | 57
3.4.1
The pumping lemma | 57
3.4.2
The Ogden lemma | 61
3.5
Context-free language membership | 63
3.6
Applications | 69
3.6.1
Translation grammars | 69
3.6.2
LL(1) grammars | 70
3.7
Problems | 76
4
Context-sensitive grammars and unrestricted grammars | 81
4.1
Context-sensitive grammars | 81
4.2
Unrestricted grammars | 87
Part II: Automata and accepting languages
5
Turing machines | 93
5.1
Deterministic Turing machines | 93
5.1.1
Basic model of Turing machines | 94
5.1.2
Turing machine with the stop property | 100
5.1.3
Simplifying the stop condition | 102
5.1.4
Guarding the tape beginning | 103
5.1.5
Turing machines with a multitrack tape | 106
5.1.6
Turing machines with two-way infinite tape | 108
5.1.7
Multitape Turing machines | 111
5.1.8
Programming with Turing machines | 117
5.2
Nondeterministic Turing machines | 122
5.3
Linear bounded automata | 130
5.4
Problems | 134
6
Pushdown automata | 139
6.1
Nondeterministic pushdown automata | 139
6.2
Deterministic pushdown automata | 145
6.3
Accepting states versus empty stack | 148
6.4
Pushdown automata as Turing machines | 150
6.5
Problems | 152
7
Finite automata | 156
7.1
Deterministic finite automata | 156

Contents
|
XIII
7.2
Nondeterministic finite automata | 162
7.3
Finite automata with ε-transitions | 171
7.4
Finite automata as Turing machines | 180
7.5
Problems | 182
Part III: Revisited: languages, grammars, automata
8
Grammars versus automata | 187
8.1
Regular expressions, regular grammars and finite automata | 187
8.1.1
Regular expressions versus finite automata | 187
8.1.2
Regular grammars versus finite automata | 193
8.1.3
The pumping lemma | 197
8.1.4
The Myhill–Nerode theorem | 199
8.1.5
Minimization of deterministic finite automata | 200
8.2
More grammars and automata | 201
8.2.1
Context-free grammars versus pushdown automata | 201
8.2.2
Unrestricted grammars versus Turing machines | 205
8.2.3
Context-sensitive grammars versus linear bounded automata | 207
9
Around the hierarchy of languages | 208
9.1
More operations on languages | 208
9.1.1
Substitutions, homomorphisms | 208
9.1.2
Quotients | 211
9.1.3
Building automata with quotients | 212
9.2
Closure | 214
9.3
The hierarchy of languages | 219
Bibliography | 227
Index | 229


1 Preliminaries
This chapter offers all required prerequisites and elaborates on the notation used
throughout the book. As such, the content presented here becomes helpful in the
systematic exposure of the material covered in the consecutive chapters.
1.1 Sets
Finite sets include a finite number of members. The empty set, that is, the set which
includes no members, is also finite. The empty set is denoted by 0. Infinite sets include
infinitely many members. Infinite sets are countable or uncountable.
We say that a set is countable if and only if its members could be arranged in
a particular sequence. In other words, a set is countable if and only if we can enu-
merate (list) its members assigning natural numbers to them. Such an arrangement
guarantees to find a number assigned to any of its members. Of course, finite sets are
also countable. Thus the conclusion that any subset of a countable set is countable
becomes obvious.
A set is uncountable if and only if there is no enumeration of its members. Any set
including an uncountable subset is uncountable.
Example 1.1. Here are some important sets of numbers:
–
P = {2, 3, 5, 7, 11, 13, 17, . . .} – the set of prime numbers;
–
N = {0, 1, 2, 3, 4, 5, 6, 7, 8, . . .} – the set of natural numbers;
–
Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .} – the set of integer numbers;
–
Q = {a/b : a, b ∈Z, b
̸= 0} – the set of rational numbers, which are fractions of
integers with nonzero denominator;
–
R – the set of real numbers. It includes rational numbers as well as irrational num-
bers as, for instance, π, e, √2.
Note that all but the last set of numbers are countable. The last one is uncountable.
The following notes concerning the above sets show arrangements of their elements
in sequences, that is, justify countability of these sets:
–
comparing Example 1.1, it is straightforward that the set of prime numbers and
the set of natural numbers are countable;
–
the following enumeration of the set of integer numbers Z = {0, −1, 1, −2, 2, −3, 3, . . .}
shows its countability;
–
an intuitive depiction of Cantor enumeration of the set of pairs of natural numbers
is shown in Table 1.1. This intuition can be explicitly expressed by the formula
π(x, y) = (x + y) ⋅(x + y + 1)/2 + x, where (x, y) is an enumerated pair of natural
numbers.
https://doi.org/10.1515/9783110752304-001

2
|
1 Preliminaries
Table 1.1: Cantor enumeration of the set of pairs (x, y) of integer numbers. An intuitive depiction is
shown below where x enumerates rows and y – columns.
0
1
2
3
4
5
…
0
0
1
3
6
10
15
…
1
2
4
7
11
16
22
…
2
5
8
12
17
23
…
…
3
9
13
18
24
…
…
…
4
14
19
25
24
…
…
…
5
20
26
…
…
…
…
…
…
…
…
…
…
…
…
…
The Cantor enumeration of pairs of natural numbers reveals some interesting char-
acteristics. We note that any rational number can be represented as a fraction of two
integer numbers. Any nonnegative fraction is formed by taking a column’s label as the
numerator and treating a label of a row (except the first one) as its denominator. In this
enumeration, any fraction appears once in irreducible form and infinitely many times
in reducible form. Cantor enumeration of pairs of natural numbers could be adapted
to an enumeration of nonnegative rational numbers by skipping reducible fractions.
On the other hand, since the set of rational numbers is a (proper) subset of pairs of
natural numbers, the countability of the set of non-negative rational numbers is a di-
rect result of the countability of the set of pairs of natural numbers. Finally, we can
apply the way of enumeration of the set of integer numbers to all rational numbers.
Let us consider the unit interval of real numbers. Any member of this set could
be represented in the form of infinite expansion. Assume that all real numbers are
arranged in the following sequence:
b00 b01 b02 b03 b04 b05 b06 b07 b08 b09 . . .
b10 b11 b12 b13 b14 b15 b16 b17 b18 b19 . . .
b20 b21 b22 b23 b24 b25 b26 b27 b28 b29 . . .
b30 b31 b32 b33 b34 b35 b36 b37 b38 b39 . . .
. . .
The following real number of the unit interval does not appear in this sequence,
where ∼b is the binary digit opposite to the digit b, that is, ∼b = 0 if and only if b
̸= 0:
∼b00 ∼b01 ∼b02 ∼b03 ∼b04 ∼b05 . . .
Therefore, the assumption that all real numbers of the unit interval could be ar-
ranged in a sequence is false.
Example 1.2. Let us consider the set Σ = {0, 1} of binary digits and the set Σ∗= {0, 1}∗
of all binary strings of finite length. Binary strings can be put (arranged) in a so called

1.2 Relations
|
3
canonical order, that is,
–
shorter strings precede longer ones and
–
given two strings of the same length, this string comes first, which have 0 at the
leftmost position that is different in both strings.
Denoting the empty string (of length 0) by ε we get the following sequence of binary
strings in canonical order:
Σ∗= {ε, 0, 1, 00, 01, 10, 11, 000, 010, 011, 100, 101, 110, 111, 0000, 0001, . . .}
Canonical order of binary strings creates enumeration, which assigns natural numbers
to consecutive strings of this family. The existence of such enumeration directly proves
that the set of binary strings is countable.
Elementary operations performed on sets are: taking subset, union, intersection,
complement of a subset and Cartesian product.
Let us recall that Cartesian product X1 × X2 × ⋅⋅⋅× Xn of sets X1, X2, . . . , Xn is the set
of so called n-tuples {(x1, x2, . . . , xn) : x1 ∈X1, x2 ∈X2, . . . , xn ∈Xn}.
Note that finite sets and countable sets are closed under the above operations.
For instance, the result of the complement of a subset of a countable set is also count-
able.
1.2 Relations
Relations play a fundamental role in this book. They are exploited in interpretation
of key ideas and concepts such as graphs, grammars, productions and derivations
in grammars, transitions and computations of automata and machines. Relations are
summoned up in this section while more specific relations concerning concepts cov-
ered in the book are brought up and discussed in consecutive sections.
1.2.1 Multiplace relations
Multiplace relations are also called multidimensional relations or n-ary relations. A re-
lation is a subset of a given set of objects, usually a subset of the Cartesian product of
a number of sets. In this book, relations are exclusively identified with Cartesian prod-
ucts of a number of nonempty sets. Elements of a relation are called tuples (or n-tuples
in the case of elements of the Cartesian product composed of n sets). In particular, the
n-tuples are referred to as follows: single (1-tuple), pair (2-tuple), triple, quadruple,
quintuple, sextuple, etc. A multiplace relation defined in the Cartesian product of n
sets is named an n-place relation or n-ary relation.

4
|
1 Preliminaries
Definition 1.1. Any subset R of the Cartesian product X1 × X2 × ⋅⋅⋅× Xn of nonempty
sets X1, X2, . . . , Xn, R ⊂X1 × X2 × ⋅⋅⋅× Xn, is an n-ary relation (n-place or n-dimensional
relation) over sets X1, X2, . . . , Xn.
A tuple (x1, x2, . . . , xn) where x1 ∈X1, x2 ∈X2, . . . , xn ∈Xn, is said to be an element of
the relation R or to satisfy the relation R if and only if (x1, x2, . . . , xn) ∈R. Inclusion of the
tuple (x1, x2, . . . , xn) ∈R into the relation R is denoted equivalently as R(x1, x2, . . . , xn).
We also say that elements x1, x2, . . . , xn are R-related (or related, if this does lead to
confusion). The tuple x1, x2, . . . , xn is not an element of the relation R or does not satisfy
the relation R if and only if (x1, x2, . . . , xn) ∉R. Exclusion of the tuple from the relation
is also denoted as ∼R(x1, x2, . . . , xn). Here, we also say that elements x1, x2, . . . , xn are
not related.
Example 1.3. Let us consider the Cartesian product Z × Z × Z × Z, where Z is the set
of integer numbers. The relation R ⊂Z × Z × Z × Z, defined as the set of quadru-
ples R = {(k, l, m, n) : k = l ⋅m + n, 0 < l, 0 ≤n < |l|}, identifies the operation of
integer division with dividend k, divisor l, quotient m and remainder n. For instance,
(4, 3, 1, 1) ∈R, (−4, 3, −2, 2) ∈R, (5, −3, −1, 2) ∈R and (−5, −3, 2, 1) ∈R while (2, 0, 2, 2) ∉R
and (−5, 3, −1, −2) ∉R. Let us recall that remainder is a non-negative integer less than
absolute value of divisor.
Note that an n-dimensional relation forms a system of n + 1 components: n sets
X1, X2, . . . , Xn and a subset of the Cartesian product X1 × X2 × ⋅⋅⋅× Xn. So then relations
may differ either in one or more sets, or in a subset of the Cartesian product, or in both.
1.2.2 Two place relations
Two place relations are special cases of multi place relations. Any subset ρ of the Carte-
sian product X × Y of nonempty sets X and Y, ρ ⊂X × Y, is a two place relation. A pair
(x, y) where x ∈X and y ∈Y, is an element of the relation ρ if and only if (x, y) ∈ρ.
Inclusion of the pair (x, y) into the relation ρ is equivalently denoted as ρ(x, y) or x ρ y.
We say that such elements x, y are ρ-related (or related, for short). The pair (x, y) is not
an element of the relation ρ or it does not satisfy the relation if and only if (x, y) ∉ρ.
Exclusion of the pair (x, y) from the relation ρ is denoted ∼ρ(x, y) or ∼x ρ y. We also
say that such elements x, y are not ρ-related.
Definition 1.2. For a given two place relation ρ ⊂X × Y, we introduce the following
terminology:
–
the set X is the domain of the relation ρ;
–
the relation is defined for an element x ∈X if and only if there exists an element
y ∈Y such that ρ(x, y). If for given element x ∈X, an element y ∈Y such that
ρ(x, y) does not exist, then the relation ρ is undefined for that element x ∈X;

1.2 Relations
|
5
–
the set of all elements x ∈X, for which the relation is defined, is alternatively
called a domain of the relation. However, in this book, the whole set X is consid-
ered to be the domain of a two-place relation;
–
the set Y is the codomain of the relation ρ;
–
the set of all y ∈Y, such that for a given y there exists x ∈X related to y, that is,
ρ(x, y), is the range of a relation;
–
an element x is called the argument and an element y is called the value of the
pair (x, y)
–
a relation is:
–
total (left-total) if and only if for all x ∈X there exists y ∈Y such that
ρ(x, y);
–
surjective (right-total) if and only if for all y ∈Y there exists x ∈X such that
ρ(x, y);
–
injective (left-unique) if and only if for all x1, x2 ∈X and y ∈Y it holds
that if ρ(x1, y) and ρ(x2, y) then x1 = x2. Injective relation is a mapping (func-
tion);
–
a function if and only if the relation is injective. Injective and total relation is
a total function;
–
bijective if and only if is left-total, surjective and injective. Bijective relation
is also called 1-to-1 relation. Obviously, bijective relation is a (1-to-1) mapping
also called bijection.
Example 1.4. Let us define a relation L ⊂Σ∗× N, where Σ = {0, 1} denotes the set of
binary digits, Σ∗denotes the set of binary strings (see Example 1.2) and N denotes the
set of natural numbers, that is, N = 0, 1, 2, 3, . . . . The relation includes pairs of a string
and its length: for any string s ∈Σ∗and for any natural number n ∈N, (s, n) ∈L, if and
only if |s| = n, where |s| denotes length of the string s, that is, the number of symbols
in the string s. Note that L is left-total and right-total (surjective) but not injective. This
observation is straightforward.
Example 1.5. Two-place relations with both countable domain and codomain can be
represented in the form of two-dimensional tables. The rows of the table are labeled
by the domain members, while the codomain members label the columns. The entries
of this table are equal to 1 where the corresponding pair of row and column labels
belong to the relation or is equal to 0 where the pair does not belong to the relation.
The relation L defined in Example 1.4 is shown in Table 1.2.
Note that all binary strings can be enumerated according to canonical order. Con-
secutive strings are denoted by si, where indices enumerate strings, as is shown in the
first column of Table 1.2. For instance, s0 = ε, s1 = 0, s2 = 1, s3 = 00, etc. This enumer-
ation assigns to any string of length l an index not less than 2l −1 and not greater than
2l+1 −2.

6
|
1 Preliminaries
Table 1.2: Representation of the relation L defined in Example 1.4 is shown below (note: 0s are
dropped for the sake of readability).
0
1
2
3
4
…
s0
ε
1
…
s1
0
1
…
s2
1
1
…
s3
00
1
…
s4
01
1
…
s5
10
1
…
s6
11
1
…
s7
000
1
…
s8
001
1
…
s9
010
1
…
s10
011
1
…
s11
100
1
…
s12
101
1
…
s13
110
1
…
s14
111
1
…
s15
0000
1
…
s16
0001
1
…
s17
0010
1
…
…
…
…
…
…
…
…
…
1.2.3 Binary relations
A particular case of two-place relations are relations with the same domain and
codomain. In this book, such relations are called binary relations. In other words,
a relation ρ ∈X × X over X, where X is a nonempty set, is a binary relation.
We distinguish several important classes of binary relations where their taxonomy
takes into consideration the following fundamental properties:
–
reflexivity: (∀x ∈X) xρx
–
irreflexivity: (∀x ∈X) ∼xρx
–
symmetry: (∀x, y ∈X) xρy ⇒yρx
–
antisymmetry: (∀x, y ∈X) xρy ⇒∼yρx
–
asymmetry: (∀x, y ∈X) xρy and yρx ⇒x = y
–
transitivity: (∀x, y, z ∈X) xρy and yρz ⇒xρz
–
totality: (∀x, y ∈X) xρy or yρx
–
partial order: a relation which is reflexive, asymmetric and transitive
–
total order: a relation which is asymmetric, transitive and total
Example 1.6. The relation ≤∈Z ×Z (less or equal) over the set Z of integers is reflexive,
asymmetric, transitive, total and possesses the property of total order. The relation
<∈Z × Z (less than) over the set Z of integers is irreflexive, antisymmetric and transi-

1.2 Relations
|
7
tive. The relation is a divisor of over the set of positive natural numbers is reflexive,
asymmetric, transitive and partial order.
1.2.4 Equivalence relations
Equivalence relations formally exhibit the similarity of objects of a given set. In fact,
equivalence relations identify groups of elements that are not distinguishable with
regard to some aspects or properties.
Definition 1.3. Equivalence relation over a set X of objects is a binary relation that is
reflexive, symmetric and transitive.
For instance, graph isomorphism is an equivalence relation defined in the set of
graphs (it is elementary to prove reflexivity, symmetry and transitivity of graph iso-
morphism). Indeed, isomorphic graphs have the same structure and may only have
different labels of vertices. Such differences are unimportant from the point of view of
the features (properties).
Definition 1.4. A subset A of the set X is an equivalence class of an equivalence rela-
tion ρ ∈X × X if and only if
(∀x, y ∈A)xρy ∧(∀x ∈A)(∀y ∉A) ∼xρy
For a given a ∈X and a given equivalence relation ρ, the set of all members of X
related to a is an equivalence class denoted [a]ρ.
It is straightforward to note that equivalence classes (of an equivalence relation)
split the set X, that is, they are pairwise disjoint and every member of X belongs to one
of them.
An equivalence relation can be outlined by defining its equivalence classes.
Example 1.7. Let us consider the set {0, 1}∗of binary strings and the binary relation
over this set, which relates any two strings of the same length. This relation is reflex-
ive, symmetric and transitive, that is, it is an equivalence relation. The relation is il-
lustrated in Table 1.3. Due to the canonical ordering of binary strings, the equivalence
classes come as blocks of 1’s shown in this table.
1.2.5 Closure of relations
A question about a relation that includes arbitrarily given pairs of elements and is
desired to exhibit given properties invokes the notion of closure of relation.
Definition 1.5. Let R ⊂X × X is a binary relation and P is a set of properties. Closure of
the relation R with respect to a set P of properties, P-closure of the relation for short, is

8
|
1 Preliminaries
Table 1.3: Representation of the relation defined in Example 1.7.
ε
0
1
00
01
10
11
000
001
…
ε
1
…
0
1
1
…
1
1
1
…
00
1
1
1
1
…
01
1
1
1
1
…
10
1
1
1
1
…
11
1
1
1
1
…
000
1
1
…
001
1
1
…
…
…
…
…
…
…
…
…
…
…
…
the smallest binary relation RP, which includes the relation R and possesses all prop-
erties of P.
The reflexive closure R{r} of a binary relation R ⊂X × X requires that every pair
(x, x), x ∈X is R{r}, that is, R{r} comes as the union R{r} = R ∪{(x, x) : x ∈X}. In the
corresponding tabular representation, the reflexive closure sets to 1 all entries on the
main diagonal.
Similarly, the symmetric closure necessitates the inclusion of any pair (x, y),
x, y ∈X such that (y, x) ∈R : R{s} = R ∪{(x, y) : (y, x) ∈R}. Concerning the tabular
representation, the symmetric closure implies that the table is made symmetrical
with regard to the main diagonal; that is, zero’s entries must be set to one if they are
symmetrical with respect to one’s entries.
Example 1.8. Let us consider a binary relation over the set of all binary strings,
R ⊂{0, 1}∗× {0, 1}∗. The relation includes every pair of two successive strings in canon-
ical order. Find the closure R{r,t} of R concerning both reflexivity and transitivity.
Solution. The relation R is shown in Table 1.4 while the closure R{r,t} is outlined in
Table 1.5. To prove that R{r,t} is the closure of the relation R with respect to the set
of reflexivity and transitivity, let us notice that R{r,t} is reflexive (possesses 1’s on the
main diagonal). It is transitive, as well. Note that a pair (si, sj) ∈R{r,t} if and only if i ≤j.
Having (si, sj) ∈R{r,t} and (sj, sk) ∈R{r,t} we get i ≤k, since i ≤j and j ≤k. Therefore,
(si, sk) ∈R{r,t}.
To complete the proof, it is enough to show that R{r,t} is the smallest reflexive and
transitive relation including the relation R. Assume that this is not satisfied. As a result
of this assumption, there exists a reflexive and transitive relation R′, which includes
R and does not include a pair (si, si+k), i ≥0, k ≥0. If k = 0, then R′ cannot be reflex-
ive. If k = 1, then R ⊈R′. Then we have (si, si+1) ∈R, (si+1, si+2) ∈R, . . . , (si+k−1, si+k) ∈R.
Subsequently, applying the property of transitivity k times we get (si, si+k) should be in-
cluded into transitive closure (being very formal, an inductive proof would be applied,

1.3 Mathematical induction
|
9
Table 1.4: Representation of the relation R defined in Example 1.8 (as before 0s are not shown here).
s0
s1
s2
s3
s4
s5
s6
s7
…
ε
0
1
00
01
10
11
000
…
s0
ε
1
…
s1
0
1
…
s2
1
1
…
s3
00
1
…
s4
01
1
…
s5
10
1
…
s6
11
1
…
s7
000
…
…
…
…
…
…
…
…
…
…
…
…
Table 1.5: Representation of the relation R{r,t} defined in Example 1.8 (0s are not shown here).
s0
s1
s2
s3
s4
s5
s6
s7
…
ε
0
1
00
01
10
11
000
…
s0
ε
1
1
1
1
1
1
1
1
…
s1
0
1
1
1
1
1
1
1
…
s2
1
1
1
1
1
1
1
…
s3
00
1
1
1
1
1
…
s4
01
1
1
1
1
…
s5
10
1
1
1
…
s6
11
1
1
…
s7
000
1
…
…
…
…
…
…
…
…
…
…
…
…
cf. Section 1.3). Afterward the assumption that the relation R{r,t} is not the smallest one
is false. This ends the proof.
1.3 Mathematical induction
Mathematical induction is a generic tool to prove properties involving natural num-
bers. It is used to establish that a property is satisfied for all natural numbers based on
its satisfaction with the first natural number (0 or 1). Then it is proved that satisfaction
for any natural number implies satisfaction for the next one.
The simplest and the most common formulation of mathematical induction is
given in Definition 1.6. It can be applied in solving simple problems based on the whole
set of natural numbers.
Definition 1.6. Let assume that W is a property defined in the set N of natural numbers
such that:

10
|
1 Preliminaries
–
W(0) the basis of induction (0 satisfies the property W);
–
(∀n ∈N)(W(n) ⇒W(n + 1)) the inductive step (for any natural number n the
following holds: if n satisfies the property W, then so does n + 1)
then all natural numbers satisfy the property W.
An equivalent formulation of mathematical induction given in Definition 1.7
allows us to use it in problems, in which a proof of the strictly consecutive law
W(n) ⇒W(n + 1) is difficult to be employed. However, it is easier to prove the cor-
responding law: if the property is satisfied for natural numbers smaller than or equal
to n then so it is satisfied for n + 1.
Definition 1.7. Let assume that W is a property defined in the set N of natural numbers
such that:
–
W(0) the basis of induction (0 satisfies the property W);
–
(∀n ∈N)(((∀k = 1, 2, . . . , n)W(k)) ⇒W(n + 1)) the inductive step (for any natural
number n it holds that if every k = 0, 1, . . . , n satisfies the property W, then n + 1
satisfies W)
then all natural numbers satisfy the property W.
The assumption in the inductive step, that is, that a property holds for a given
natural number n or it holds for all numbers not greater than n, is also called the in-
ductive hypothesis. Based on the assumption that the inductive hypothesis is true, the
satisfaction of the property for n + 1 is drawn in the inductive step.
The principle of mathematical induction can be used for proving problems related
to any countable set. Suppose a countable set is enumerated by making use of natural
numbers. In that case, the problem can be related to indices, which are natural num-
bers rather than original members of the countable set. For instance, the principle of
mathematical induction can be used in proofs of properties of a set of natural num-
bers greater or equal to some given number n0, of the set of prime numbers, of the set
of odd integers, of the set of rational numbers, of the set of all binary strings of even
length, of graphs, etc. Inductive proofs involving both the above formulations of math-
ematical induction as well as different countable sets will be presented in consecutive
sections.
1.4 Graphs and trees
Graphs are essential mathematical structures used to model problems in theory and
practice. Trees are a particular type of graphs. In this book, graphs are mostly utilized
for modeling automata and trees. They are also employed to represent computations
realized by automata and to illustrate derivations of words in context-free grammars.

1.4 Graphs and trees
|
11
1.4.1 Graphs
Definition 1.8. A graph G is a system of two components G = (V, E) where
V = {v1, v2, . . . , vn} is a finite set of vertices (vertices are also called nodes) and E ⊂V×V
is a binary relation over the set of vertices. Elements of E = {e1, e2, . . . , em} are called
edges. Edges connect vertices. Sets of vertices and edges of a graph G are usually
denoted V(G) and E(G) or V and E when there is no danger of confusion. A graph
G = (V, E) is undirected if and only if the relation E is symmetrical. Otherwise, a graph
is directed.
In directed graphs, an edge connecting a vertex vi to a vertex vj is denoted as (vi, vj).
In undirected graphs, an edge connecting a vertex vi and a vertex vj is denoted {vi, vj}
and can be interpreted as the pair of directed edges (vi, vj) and (vj, vi).
Graphs are commonly represented in a graphical form, as adjacency matrices or
as adjacency lists.
Rows and columns of an adjacency matrix are indexed by vertices. Entries of the
matrix are equal to 0 or 1. An entry is equal to 0 if and only if there is no edge connect-
ing vertices corresponding to labels of the row and the column.
Two types of adjacency lists are constructed for every vertex. For a given vertex vj,
a list before includes all vertices connected to vj (such that there is an edge going from
the given vertex to vj). For a given vertex vj, a list after includes all vertices to which
vj is connected (such that there is an edge going from vj to the given vertex). Note that
for each vertex lists before and after are identical in undirected graphs.
The graphical representation of adjacency matrices and adjacency lists of a di-
rected graph and an undirected graph are shown in Figure 1.1.
A path in a graph G = (V, E) is a sequence of vertices vi1, vi2, . . . , vik that are pairwise
different and such that (vij−1, vij) ∈E for j = 2, 3, . . . , k (in case of undirected graphs
{vij−1, vij} is considered instead of (vij−1, vij)).
A path with vi1 = vik is a cycle. A graph with no cycle is called acyclic.
A graph is connected if and only if there is a path between any two vertices.
1.4.2 Trees
In graph theory, a tree T is defined as a connected acyclic graph G. In this book, we
employ an inductive definition of a tree.
Definition 1.9. A tree is an undirected connected graph G having the following prop-
erties:
–
the graph having one vertex T = (V = {v}, E = 0) forms a tree with the root v;
–
if T1 = (V1, E1), T2 = (V2, E2), . . . , Tk = (Vk, Ek) form trees with roots v1, v2, . . . , vk,
respectively, and a vertex v ∉V1 ∪V2 ∪⋅⋅⋅∪Vk
then T = (V1 ∪V2 ∪⋅⋅⋅∪Vk ∪{v}, E1 ∪E2 ∪⋅⋅⋅∪Ek ∪{{v, v1}, {v, v2}, . . . , {v, vk}}) is the

12
|
1 Preliminaries
Figure 1.1: Graphs and their representation: (a) examples of directed and undirected graphs, (b) ad-
jacency matrices, (c) adjacency lists (notice that lists before were dropped in case of the undirected
graph).
tree with the root v. Vertices v1, v2, . . . , vk are children of the vertex v and v is their
parent. T1, T2, . . . , Tk are subtrees of the tree T;
–
a graph is a tree if and only if it is constructed employing the two above rules for
finite number of times.
Vertices with no child are leaves. The set of all leaves of a tree is called crop of the
tree.
A tree that is built using this definition is an undirected acyclic graph. The def-
inition could be easily adapted for directed trees with edges connecting parents to
children.
A tree T is called k-tree if and only if every node has no more than k children. The
simplest trees are 2 −trees, also called binary trees.
Definition 1.10. Height of a tree is defined as follows:
–
height of the tree T = (V = {v}, E = 0) is equal to 0;
–
if T1, T2, . . . , Tk are trees of height h1, h2, . . . , hk, respectively, then the tree T with
subtrees T1, T2, . . . , Tk has height equal to 1 + max{h1, h2, . . . , hk}.
Note that the height of a tree is equal to the length of the longest path from the
root to a leaf.
Now we can prove the following important property of trees.

1.5 Languages
|
13
Lemma 1.1. A k-tree of height h has no more than kh leaves.
Proof. By induction:
–
a k-tree T = (V = {v}, E = 0) of one vertex has 1 leaf, its height is equal to 0. Of
course, 1 ≤k0.
–
if T1, T2, . . . , Tl, l ≤k, are k-trees of height not greater than h and – by inductive
assumption – they have N1 ≤kh, N2 ≤kh, . . . , Nl ≤kh leaves, respectively, the tree
T with subtrees T1, T2, . . . , Tl of height h+1 has N1+N2+⋅⋅⋅+Nl ≤l⋅kh ≤k⋅kh = kh+1
leaves.
–
since the basis (the first condition) and the inductive step (the second condition)
hold, then the lemma has been proved.
1.5 Languages
Languages are the main subjects of this study. Let us recall that languages are subsets
of the set of all words over some finite alphabet. An alphabet is a finite set of sym-
bols, while a word is a finite sequence (finite string) of symbols of this alphabet. This
description of languages has little in common with a typical understanding of lan-
guages such as natural languages: English, Spanish, Polish, Japanese or even such as
programming languages. As regarded here, languages are objects of set theory rather
than natural languages or programming languages. However, the discussion in this
book and conclusions drawn here are fundamental to various areas of research and
practice. Such areas include design and implementation of compilers, construction of
algorithms, analysis of the complexity of algorithms and characterization of the space
of problems from a perspective of the complexity of algorithms used to solve them.
1.5.1 Basic definitions
Definition 1.11. Alphabet is a finite set of symbols: Σ = {a(1), a(2), . . . , a(ρ)}. Symbols of
an alphabet are also called letters.
Word over an alphabet Σ is a finite sequence w = a1 a2 . . . an of letters. Length of
a word is equal to the number of symbols of the word and is denoted |w|. Words are
also called strings.
The set of all words over alphabet Σ is denoted as Σ∗.
Note:
–
symbols in a word are denoted using subscripts while symbols of an alphabet
are indicated with superscripts in brackets. This is just to differentiate symbols
of an alphabet from symbols of a word;
–
the set Σ∗of all words over alphabet Σ = {a(1), a(2), . . . , a(ρ)} is infinite and count-
able. Words over alphabet Σ can be arranged in the canonical order: longer words

14
|
1 Preliminaries
follow shorter ones and words of the same length are arranged alphabetically, that
is,
Σ∗= {ε, a(1), a(2), . . . , a(ρ), a(1)a(1), a(1)a(2), . . . , a(1)a(ρ),
a(2)a(1), a(2)a(2), . . . , a(ρ)a(ρ), a(1)a(1)a(1), . . .}
–
words can be enumerated consistently with the canonical order, that is;
–
the empty word w0 = ε (of length 0) gets the number 0;
–
words w1 = a(1), . . . , wρ = a(ρ) (of length 1) get numbers 1 to ρ;
–
words of length 2 get numbers ρ + 1 to ρ + ρ2;
–
words of length 3 get numbers ρ + ρ2 + 1 to ρ + ρ2 + ρ3;
–
words of length 4 get numbers ρ + ρ2 + ρ3 + 1 to ρ + ρ2 + ρ3 + ρ4;
–
etc.
Example 1.9. Let us consider the Latin alphabet {a, b, c, . . . , z} consisting of 26 letters.
Examples of words enumerated in canonical order are: w0 = ε, w1 = a, w2 = b, w23 = w,
w24 = x, w27 = aa, w702 = zz, w703 = aaa and so on.
Definition 1.12. A language L over an alphabet Σ is a subset of the set of all words Σ∗
over this alphabet.
Example 1.10. Examples of languages over the Latin alphabet:
–
the empty language 0;
–
the language of words including only letter a: {a, aa, aaa, aaaa, . . .};
–
the language of one letter words: {a, b, c, . . . , z};
–
the language of all correct English words;
–
the language of palindromes (a palindrome is a word which is identical to its re-
verse, for example, abcdcba, aaabbaaa).
Note: a language is finite (i. e., it has a finite number of words) if and only if there is
some constant such that the length of any word in this language is not greater than this
constant. Indeed, if the length of words of the language is bounded by a constant n,
then the number of all words of this language of length not greater than n is finite and
not greater than 1+ρ+ρ2 +⋅⋅⋅+ρn. So then, the language is finite. On the other hand, if
a language is finite, there is the longest word (more than one word may have the same
length). The length of the longest word is the constant bounding length of all words
of this language.
1.5.2 Operations on languages
In this section, operations on languages are discussed. Languages themselves are
sets, so then intuitively appealing set operations (taking subset, union, intersec-

1.5 Languages
|
15
tion, complement) are considered here. More operations are discussed below and in
Part III.
Concatenation is an important operation realized on languages. This opera-
tion relies on joining (concatenating) words. Let us consider two words, say u =
a1a2 . . . ak and v = b1b2 . . . bl. The concatenation of these two words is the word
w = a1a2 . . . akb1b2 . . . bl denoted as w = uv or w = u ∘v.
Concatenation of two languages L1 and L2 is the language L1 ∘L2 = {uv : u ∈L1,
v ∈L2}. Concatenation of two languages over different alphabets produces a language
over the union of both alphabets.
Concatenation of two languages could be easily generalized to concatenation of
a finite sequence of languages. Concatenation of languages L1, L2, . . . , Ln is the lan-
guage (. . . ((L1 ∘L2) ∘L3) . . .) ∘Ln.
Let us notice that concatenation is an associative operation both on words and on
languages. The proof of this is straightforward.
A special focus is placed on the concatenation of the same language. n-concate-
nation of a language L, denoted by Ln, is defined inductively in the following way:
L0 = {ε}
Ln = Ln−1 ∘L
This operation gives rise to the so-called Kleene closure or star closure. The Kleene
closure of a language L is the language L∗:
L∗=
∞
⋃
k=0
Lk
For the sake of simplicity, we use the symbol of positive closure:
L+ =
∞
⋃
k=1
Lk
Note that either L+ = L∗when ε ∈L or L+ = L∗−{ε} when ε ∉L.
Example 1.11. Let us consider the language L = {0, 00, 000}. Multiconcatenation of
the language L produces the following results:
–
L0 = {ε};
–
L1 = L0 ∘L = {ε} ∘{0, 00, 000} = {ε ∘0, ε ∘00, ε ∘000} = {0, 00, 000};
–
L2 = L1 ∘L = {0, 00, 000} ∘{0, 00, 000} = {0 ∘0, 0 ∘00, 0 ∘000, 00 ∘0, 00 ∘00, 00 ∘
000, 000 ∘0, 000 ∘00, 000 ∘000} = {00, 000, 0000, 00000, 000000};
–
L3 = L2 ∘L = {000, 0000, 00000, 000000, 0000000, 00000000, 000000000};
–
etc.
–
L∗= {ε, 0, 00, 000, 0000, 00000, . . .};
–
L+ = {0, 00, 000, 0000, 00000, . . .}.

16
|
1 Preliminaries
To simplify the notation, the following symbols are used:
–
an = aa . . . a
⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟
n times
– the word of n letters a;
–
a∗= {a0, a1, a2, a3, . . .} = {ε, a, aa, aaa, . . .}. That is, a∗is the language L = {a}∗;
–
♯aw denotes the number of letters a in a word w. For example, for w = abacbb,
♯aw = 2, ♯bw = 3, ♯dw = 0 where a, b, c, d are letters of the alphabet.
Now we consider two special binary relations defined on the set of all words Σ∗over
a given alphabet. The first one, the right invariant relation, is independent on any
language. The second one, called the relation induced by a language, is defined for
any language L.
Definition 1.13. A relation R ⊂Σ∗× Σ∗is right invariant if and only if
(∀u, v ∈Σ∗) (u R v ⇒(∀z ∈Σ∗)uz R vz)
Definition 1.14. A relation RL ⊂Σ∗× Σ∗is induced by a language L ⊂Σ∗if and only if
(∀u, v ∈Σ∗) (u RL v ⇔((∀z ∈Σ∗)uz ∈L ⇔vz ∈L))
Remark 1.1. Relation RL induced by a language L is right invariant relation.
Proof. This observation is valid since u RL v implies that for any z ∈Σ∗, uz RL vz. In-
deed, uz RL vz if and only if (∀y ∈Σ∗)(uz)y ∈L ⇔(vz)y ∈L. On the other hand, the
concatenation of words is associative, so ((uz)y ∈L ⇔(vz)y ∈L) ⇔(u(zy) ∈L ⇔
v(zy) ∈L). Denoting zy by x we get ux ∈L ⇔vx ∈L, which satisfies the definition
of RL.
Remark 1.2. Relation RL induced by a language L is an equivalence relation.
Proof. Reflexivity and symmetry of the relation RL is obvious (is directly derived from
the same properties of the equivalence relation). Transitivity of the RL is derived from
properties of the equivalence relation as well. That is, the relation RL is transitive if and
only if for any u, v, w ∈Σ∗the following implication holds: u RL v and v RL w implies
u RL w. From definition of RL, we have u RL v ⇔((∀x ∈Σ∗)ux ∈L ⇔vx ∈L) and
v RL w ⇔((∀y ∈Σ∗)uy ∈L ⇔vy ∈L) RL. Assuming x = y = z for any z ∈Σ∗we obtain
uz ∈L ⇔vz ∈L and vz ∈L ⇔wz ∈L, and finally, uz ∈L ⇔wz ∈L. This completes
the proof.
Remark 1.3. Any language L is a union of some equivalence classes of the relation RL
induced by L.
Proof. Notice that each equivalence class either is included in the language L or is
disjoint with it and no equivalence class can be shared between the language L and
its complement. This is because u RL v implies that u ∈L ⇔v ∈L.

1.6 Grammars
|
17
Remark 1.4. It is easy to notice that right invariant relation may not satisfy properties
of equivalence relation. For instance, by relating words of different lengths, we get
a right invariant relation. However, this relation cannot be an equivalence relation
since it is neither reflexive nor transitive. Since any relation induced by a language
is an equivalence relation, then this particular relation cannot be induced by any
language.
1.6 Grammars
Grammars are essential tools used for describing, defining and analyzing languages.
At first glance, grammars can be seen as tools describing syntactic rules that man-
age the analysis and processing of natural languages. However, a complete and pre-
cise description of natural languages is impossible due to their inherent complexity.
Grammars of natural languages describe rules though the rules are neither accurate,
nor complete, nor rigorous. Correctness rules of natural languages are often not strict,
and thus can be used more flexibly.
In contrast, the languages studied in this book are often less complex than natu-
ral languages and are always precisely defined. They can be called formal languages
or artificial languages. The correctness rules of such languages are always strict. Any
construct (word, text) can be exclusively qualified either as a correct structure of the
language or as not belonging to this language. Grammars are important tools describ-
ing artificial languages. Certainly, grammars defining artificial languages are much
less complex than grammars of natural language and have rules precisely defined.
Definition 1.15. A grammar is a system
G = (V, T, P, S)
where:
–
V is a finite set of variables (or nonterminals), will also be called alphabet of vari-
ables (alphabet of nonterminals);
–
T is a finite set of terminals, will be also called alphabet of terminals;
–
P is a finite set of productions;
–
S is a variable called initial (or starting) symbols of the grammar.
Production is a pair (α, β) of words over alphabets of nonterminals and terminals as-
suming that α is a nonempty word:
(α, β) ∈(V ∪T)+ × (V ∪T)∗
The set of grammar productions can be interpreted as a finite binary relation in
the set of all words over the union of alphabets of nonterminals and terminals. Pro-

18
|
1 Preliminaries
ductions can also be interpreted as rules that allow us to change words by replacing
a part of it identical to the first element of production with the second element of this
production. This interpretation carries out another symbol of the production (α, β),
that is,
α →β
Intuitively, straightforward interpretation of productions as rules of words ex-
change leads to a notion of direct derivation and derivation:
Definition 1.16. A direct derivation 󳨀→
G in a grammar G = (V, T, P, S) is a binary rela-
tion:
󳨀→
G ⊂(V ∪T)∗× (V ∪T)∗
where: (η, ζ ) ∈󳨀→
G if and only if (∃α, β, γ, δ ∈(V ∪T)∗) such that η = γαδ, ζ = γβδ and
α →β is a production. Note: the grammar name G will be omitted in the derivation
symbol if this does not lead to confusion.
Definition 1.17. A derivation
∗󳨀→
G in a grammar G = (V, T, P, S) is the transitive closure
of a direct derivation. Obviously, a derivation is a binary relation in the set of all words
over the union of sets of nonterminals and terminals.
A derivation allows verifying whether a word can be derived from another one by
utilizing a sequence of productions.
Terminals of a grammar could be seen as an alphabet of letters used for the con-
struction of words of a language over this alphabet. Nonterminals represent more gen-
eral concepts. Such concepts should be developed by applying relevant productions
as far as a final word over the terminal alphabet is derived.
Example 1.12. Let us consider the grammar G = (V, T, P, S) where:
P:
S →0
(1)
S →A
(2)
A →A0
(3)
A →A1
(4)
A →1
(5)
Note that several productions have the same left-hand side. Such productions are
often arranged together in a single line with the same left-hand side. The correspond-
ing right-hand sides are separated by vertical bars, say
P:
S →0|A
(1), (2)
A →A0|A1|1
(3), (4), (5)

1.7 Problems
|
19
Finally, as was mentioned above, grammar is a tool to define a language, which is
outlined in the following definition.
Definition 1.18. A language L(G) generated by the grammar (or generated in the gram-
mar) G is the set of words over the alphabet of terminals, which could be derived from
starting symbol of the grammar:
L(G) = {w ∈T∗: S
∗󳨀→
G w}
Example 1.13. Let us consider the grammar G = (V, T, P, S) defined in the previous
example. Observe that this grammar generates the language
L(G) = {0, 1, 10, 11, 100, 101, 110, 111, 1000, 1001, 1010, 1011, . . .},
that is, it generates binary numbers without non-significant zeros.1
If not stated explicitly, the following symbols will be used by default to indicate
constructs of grammars:
A, B, C, … – nonterminal symbols;
a, b, c, …, 0, 1, 2, …, 9 – terminal symbols;
Z, Y, X, … – terminal or nonterminal symbol;
z, y, x, … – words over alphabet of terminals;
α, β, γ, … – sequences of terminals and nonterminals.
1.7 Problems
Problem 1.1. Determine the number of all reflexive relations in a set of n elements
{a1, a2, . . . , an}.
Solution. Let us represent relations as tables with rows and columns labeled with
elements of the set. The entries of this table are equal to 1 if the row label is related to
the column label; cf. Example 1.5
The reflexive relation must have all entries on the main diagonal being equal to 1.
Any other entry of the table may be either 0 or to 1. The number of entries outside
the main diagonal is equal to n2 −n; see Table 1.6. These entries may have any value
(0 or 1). Therefore, the number of different reflexive relations is equal to 2n2−n.
Problem 1.2. Find the number of all symmetric relations in a set of n elements
{a1, a2, . . . , an}.
Solution. Find the number of all symmetric relations in a set of n elements. Answer:
2(n2+n)/2
1 A digit 0 in a number is non-significant if it can be removed without affecting the value of the num-
ber.

20
|
1 Preliminaries
Table 1.6: A reflexive relation defined on n elements.
a1
a2
a3
…
an
a1
1
?
?
…
?
a2
?
1
?
…
?
a3
?
?
1
…
?
…
…
…
…
…
…
an
?
?
?
…
1
Problem 1.3. A relation ρ over binary alphabet Σ = {0, 1} relates words having the same
numerical value. Check if the relation ρ is the right invariant one. Find its equivalence
closure and equivalence classes of the closure.
Solution. Let us check if this is an equivalence relation. Of course, the property of
having the same numerical value is reflexive, symmetric and transitive. So then, its
equivalence closure keeps the relation unchanged. Also, equivalence classes of this
relation could be easily listed:
A0 = {0, 00, 000, 0000, 00000, . . .}
A1 = {1, 01, 001, 0001, 00001, . . .}
A2 = {10, 010, 0010, 00010, 000010, . . .}
A3 = {11, 011, 0011, 00011, 000011, . . .}
A4 = {100, 0100, 00100, 000100, 0000100, . . .},
etc.
As we see, the relation creates an infinite set of infinite equivalence classes.
Two related words have the same numerical value. If any binary string is attached
to both words, then their numerical values will be equal. This observation fulfills the
definition of right invariant relation.
Problem 1.4. A relation ρ over binary alphabet Σ = {0, 1} relates to words having a com-
mon digit. For instance: 001 ρ 10, 1010 ρ 010101, ∼00 ρ 111. Check if ρ is an equivalence
relation. If not, find its equivalence closure ρ{eq}. Find equivalence classes of the equiv-
alence relation.
Solution. The relation is not an equivalence relation:
–
it is not reflexive: any word is related to itself except the empty word, which is not
related to itself;
–
it is symmetrical since the property of having common letter is symmetrical;
–
it is not transitive, for instance. 00 ρ 01 ∧01 ρ 11 and ∼00 ρ 11.
Let us find the equivalence closure of the relation ρ. First of all, we will find its reflexive
closure and its transitive closure. Then we will check if the union of both closures
remains reflexive, symmetric, transitive and is the smallest relation possessing these
properties and including the relation ρ:

1.7 Problems
|
21
–
it is obvious that the relation ρ{r} = ρ ∪{(ε, ε)} closes ρ over reflexivity: ρ{r} in-
cludes ρ, is reflexive and is the smallest relation satisfying these two conditions;
–
note that any word including both digits is related to any nonempty word. Since
for w = 01 and (∀u, v ∈Σ+)u ρ w ∧w ρ v, then the pair (u, v) should be included
into transitive closure ρ{t} of the relation ρ. Indeed, any two nonempty words are
related in the transitive closure of the relation ρ. On the other hand, empty word
is not ρ-related to any other word. Concluding, we have ρ{t} = Σ+ × Σ+.
–
we claim that ρ{r}∪ρ{t} is the smallest equivalence relation including the relation ρ.
In fact, the relation ρ{r} ∪ρ{t} is reflexive, symmetrical and transitive, so it is an
equivalence relation. Furthermore, it is not possible to remove any pair from the
relation ρ{r} ∪ρ{t} without violation of closure conditions: removing the pair (ε, ε)
violates reflexivity, removing a pair of words having common digit violates inclu-
sion of the original relation, while removing any pair of nonempty words not hav-
ing a common digit violates the transitivity property. In conclusion, the relation:
ρ{eq} = ρ{r} ∪ρ{t} = {(u, v) : u = ε = v ∨u
̸= ε ∧v
̸= ε} = {(ε, ε)} ∪Σ+ × Σ+
is the equivalence closure of the original relation ρ.
In addition, let us note that ρ{eq} results in two equivalence classes: E1 = {ε} and
E2 = Σ+.
Problem 1.5. Given is an alphabet Σ = {a(1), a(2), . . . , a(r)}, r
> 2. The set of let-
ters appearing in a word w ∈Σ∗is called support of the word and is denoted as
supp(w). For instance, supp(a(1)a(1)a(1)a(2))
=
{a(1), a(2)}. Words over the alpha-
bet Σ are ρ-related if and only if they have exactly two common letters, that is,
(∀u, v ∈Σ∗)u ρ v ⇔| supp(u) ∩supp(v)| = 2. For instance, 001 ρ 10, 1010 ρ 011,
∼1010 ρ 11, ∼ε ρ 101. Check if ρ is an equivalence relation. If not, find its equiva-
lence closure ρ{eq}. Find equivalence classes of the equivalence relation.
Solution. The relation is symmetric but is neither reflexive, nor transitive, for instance
∼X ρ X for X ∈Σ ∪{ε}; for a1, a2 and a3 pairwise different letters a1a2a3 ρ a1a2 ∧
a1a2 ρ a1a2a3 and ∼a1a2a3 ρ a1a2a3. The reflexive closure is ρ{r} = ρ ∪{(u, u) : u ∈Σ∗}
while the transitive closure is ρt = {(u, v) : | supp(u)∩supp(v)| ≥2} and the equivalence
closure is equal to ρ{eq} = ρr ∪ρt.
Equivalent classes of this closure are:
–
{ε}
–
{ai} such that a ∈Σ and i ∈{1, 2, . . .}, that is, all sets including a single word with
one letter support;
–
{w ∈Σ∗: | supp(w)| ≥2}, that is, the set of all words having at least two different
letters.
The reader can easily prove that the closures and equivalent classes are correctly fig-
ured out.


|
Part I: Grammars and generating languages


2 Regular expressions and regular languages
Regular languages form the simplest class of formal languages. They are inductively
defined by regular expressions. Regular languages also outline simple algebraic struc-
tures in the set of all words over a given alphabet. Due to this simplicity, they can be
easily distinguished and identified. Regular languages are generated by regular gram-
mars as well and – what will be discussed in Chapter 7 – they are accepted by finite
automata.
In this chapter, fundamental properties of regular expressions and regular lan-
guages are studied. We cover a discussion on the following topics: operation on reg-
ular expressions, algebraic properties of the relation induced by regular languages
(portrayed by so-called Myhill–Nerode lemma, which is a part of Myhill–Nerode the-
orem), the structure of words of regular languages (delineated by pumping lemma).
The study is supplemented by a section focused on regular grammars. All these topics
are revisited in Chapter 8.
2.1 Regular expressions and regular languages
2.1.1 Regular expressions
The definition of regular expressions is inductive, that is, basic regular expressions
are defined explicitly, while more complex regular expressions could be designed ac-
cording to given rules.
Definition 2.1. Regular expressions over an alphabet Σ are constructs defined as fol-
lows:
–
Φ is a regular expression;
–
ε is a regular expression;
–
for each a in Σ, a is a regular expression;
–
if r and s are regular expressions then
–
(r + s), the sum of regular expressions;
–
(rs), the concatenation of regular expressions and
–
(r∗), the Kleene closure of the regular expression, called also star closure
are regular expressions.
Regular expressions are strings of basic symbols connected with sum, concatena-
tion and Kleene operator enclosed in brackets. Basic symbols are shown in boldface
in order to distinguish between the symbols of basic regular expressions and letters of
the alphabet. Normal fonts will be used instead of boldface if the meaning of a symbol
is obvious from the context it is used in.
https://doi.org/10.1515/9783110752304-002

26
|
2 Regular expressions and regular languages
Example 2.1. The following strings are regular expressions over the alphabet Σ =
{a, b}:
–
Φ;
–
ε;
–
a;
–
b;
–
(Φ + a);
–
(b + ε);
–
((aa)(ba));
–
(((aa)b)(b + a));
–
(a + (a((bb)∗))).
Definition 2.2. Regular expressions over an alphabet Σ generate languages:
–
Φ generates the empty language 0:
–
ε generates the language {ε}, that is, the empty word sets up this language;
–
for each a in Σ, a generates the language {a}, that is, the one letter word sets up
such a language;
–
if regular expressions r and s generate languages R and S then
–
(r + s) generates the language R ∪S (union of languages R and S);
–
(rs) generates the language R ∘S (concatenation of languages R and S);
–
(r)∗generates the language R∗(Kleene closure of the language R).
Example 2.2. The following languages are generated by regular expressions used in
Example 2.1:
–
0, the empty language;
–
{ε} – the language having only the empty word;
–
{a} – the language having only one word of a single letter;
–
{b};
–
(0 ∪{a}) = {a};
–
({b} ∪{ε}) = {ε, b};
–
(({a}{a})({b}{a}))= {aaba};
–
((({a}{a}){b})({a} ∪{b})) = {aabb, aaba};
–
({a} ∪({a}(({b}{b})∗))) = {a, abb, abbbb, abbbbbb, . . .}.
Remark 2.1. Regular expressions are finite constructs. In fact, they are words over the
alphabet Σ ∪{Φ, ε, +, ∘,∗, (, )}, where Σ is an alphabet of a regular expression. On the
other hand, languages generated by regular expressions can be infinite. An example of
infinite language generated by the (finite) regular expression is given in Example 2.2,
refer to the last case.
Remark 2.2. Regular expressions and formulas describing languages generated by
regular expressions include a large number of brackets, what makes them hardly read-

2.1 Regular expressions and regular languages
|
27
able. On the other hand, algebraic operators, logic operators as well as set theoretic
operators are assumed to have priorities. This assumption allows dropping most of the
brackets of algebraic, logic as well as set-theoretic expressions. By analogy, the same
simplification is assumed for regular expressions. It is assumed that the sum opera-
tor has the lowest priority, concatenation has higher priority and the Kleene operator
has the highest priority. The same assumption is adapted for operators on languages.
Union has the lowest priority, concatenation has higher priority and the Kleene clo-
sure is of the highest priority. We will drop any pair of brackets if it does not change
the order of operators when priorities are applied. As an example, a simplified form
of regular expressions of Example 2.1 is presented in Example 2.3.
Example 2.3. Simplified form of regular expressions given in Example 2.1:
–
Φ;
–
ε;
–
a;
–
b;
–
Φ + a;
–
b + ε;
–
aaba;
–
aab(b + a);
–
a + a(bb)∗.
Remark 2.3. Regular expressions can be interpreted as strings of symbols. Therefore,
a simplified form of a regular expression is not equal to its original form in terms of
strings’ equality. Yet, languages generated by both forms of a regular expression are
identical. So then, both forms are considered to be equivalent. From now on, if not
stated otherwise, equivalent regular expressions will be deemed to be equal.
2.1.2 Regular languages
Definition 2.3. Regular languages are those and only those generated by regular ex-
pressions.
Example 2.4. The set of binary natural numbers without nonsignificant zeros L =
{0, 1, 10, 11, 100, 101, 110, 111, 1000, 1001, . . .} is a language over the binary alphabet. Jus-
tify that it is a regular language.
Solution. Let assume that Σ = {0, 1} is the alphabet. The regular expression 0 + 1(0 +
1)∗is claimed to generate the language L. Indeed, the regular expression 0 + 1(0 +
1)∗generates the language {0} ∪{1} ∘{0, 1}∗, which includes either word 0 or words
beginning with 1. This proves that words generated by the regular expression 0+1(0+
1)∗belong to the language L. On the other hand, any word of this language is generated
by the expression. Obviously, 0 is generated by the expression. Furthermore, any word

28
|
2 Regular expressions and regular languages
of length n and beginning with 1 belongs to the set {1} ∘{0, 1}n−1, which is a subset of
the language {0} ∪{1} ∘{0, 1}∗.
Example 2.5. Justify that the set L of binary words having exactly one sequence of
three 1’s 111 is a regular language. For example, 0000111 ∈L, 1111 ∉L (two sequences
of 111 are included: the first three digits and the last three digits) 11011001110001 ∈L,
0110110 ∉L (no sequence 111 included).
Solution. The following regular expression generates the language L:
r = 0∗((ε + 1)100∗)
∗111(00∗1(ε + 1))
∗0∗
The expression can be split into three parts: the middle 111, the beginning 0∗((ε +
1)100∗)∗and the ending (00∗1(ε+1))∗0∗. The middle part generates required three 1s.
The first part 0∗of 0∗((ε + 1)100∗)∗generates a sequence of 0’s. The second part
generates a sequence of binary words having one or two 1’s followed by a nonempty
sequence of 0’s. Notice that each single or double digit 1 is followed by at least one 0.
The ending part (suffix) is the reversed beginning part (prefix). So then words gen-
erated by the ending part are reversed versions of words generated by the beginning
part.
Finally, a concatenation of any words generated by the beginning, middle and
ending part of the regular expression, r create words having exactly one sequence of
three 1s, that is why they belong to the language L.
On the other hand, any word of the language L can be split into three parts: the
middle sequence of three 1’s, the sequence of digits (possibly empty) followed by the
middle part and the sequence of digits that follows the middle part.
The beginning part of every nonempty word ends with digit 0 and, if have 1’s,
any single digit 1 or pair of them must be separated by at least a single 0. So then, the
beginning part can be split into leading 0’s and then sequences of single or double
digits 1 followed by at least one 0. This observation makes it clear that the beginning
part of any word of the language L is generated by the first part 0∗((ε + 1)100∗)∗of the
expression r.
By analogy, it can be drawn that the ending part (00∗1(ε + 1))∗0∗of the regular
expressions r generates the ending part of any word of the language L.
In conclusion, any word of the language L is generated by the regular expression r.
This completes the proof.
Remark 2.4. The following equalities hold (in terms of Remark 2.3):
1.
0 + r = r + 0 = r
2.
0 r = r 0 = 0
3.
εr = rε = r
4.
ε + r = r + ε
5.
r + s = s + r
6.
(r + s) + t = r + (s + t) = r + s + t

2.1 Regular expressions and regular languages
|
29
7.
(rs)t = r(st) = rst
8.
r(s + t) = rs + rt
9.
(r + s)t = rt + st
10. (r∗)∗= r∗
11. (r∗s∗)∗= (r + s)∗
12. (r∗+ s∗)∗= (r + s)∗
2.1.3 The Myhill–Nerode lemma
The Myhill–Nerode theorem is the most crucial tool characterizing regular languages.
The theorem is given and proved in Chapter 8. In this chapter, a limited version of
the Myhill–Nerode theorem is formulated. It will be referred to as the Myhill–Nerode
lemma. The proof of the lemma is a direct consequence of a proof of the Myhill–Nerode
theorem. The lemma is not proved here since important topics used in the proof have
not been discussed yet. The lemma is formulated here since it is a crucial tool used in
the identification of regular languages. It will be used in this and other chapters.
Lemma 2.1 (The Myhill–Nerode lemma). A language L is regular if and only if the rela-
tion RL induced by the language L has a finite number of equivalence classes.
Example 2.6. Verify if the language L of binary numbers without nonsignificant 0’s is
regular (this language was defined in Example 1.13).
Solution. Let check if the following sets are equivalence classes of the relation RL in-
duced by the language:
–
E1 = {ε};
–
E2 = {0};
–
E3 = {1w : w ∈{0, 1}∗}, that is, the set of all binary words beginning with the
digit 1;
–
E4 = {0, 1}∗−(E1 ∪E2 ∪E3), that is, the set of all words not included in the previous
sets.
To prove that these sets are equivalence classes is sufficient to show that:
–
they partition the set {0, 1}∗of all binary words. Certainly, because sets E1, E2 and
E3 are pairwise disjoint, then the definition of the set E4 guarantees that these four
sets create a partition of {0, 1}∗;
–
any two words of any set are RL-related
–
this condition is obvious for sets E1 and E2;
–
any two words u and v of the set E3 belong to the language L (both words do
not have nonsignificant 0). Moreover, for any binary word z, words uz and vz
also belong to the language L (none of them have nonsignificant 0);
–
any two words u and v of the set E4 do not belong to the language L (both
words begin with nonsignificant 0). Moreover, for any binary word z, words

30
|
2 Regular expressions and regular languages
uz and vz also do not belong to the language L (both of them begin with non-
significant 0’s);
–
any two words of two different sets are not RL-related:
–
the empty word ε:
*
is not related to 0: for z = ε, we get εz = εε = ε and 0z = 0ε = 0. Of course,
ε ∉L and 0 ∈L;
*
is not related to any word u ∈E3: for z = ε, we get εz ∉L and uz ∈L;
*
is not related to any word u ∈E4: for z = 0, we get εz ∈L and uz ∉L;
–
the word 0
*
is not related to any word u ∈E3: for z = 0, we get uz = 00, so then 0z ∉L
and uz ∈L;
*
is not related to any word u ∈E4: for z = ε, we get uz = 0, so then 0z ∈L
and uz ∉L;
–
any word u ∈E3
*
is not related to any word v ∈E4: for z = ε, we get uz = u, so then uz ∈L
and vz ∉L.
Finally, by the Myhill–Nerode lemma, since the number of equivalence classes of the
relation RL induced by the language L is finite, the language L is regular.
Example 2.7. Check if the language L of nonempty binary words having the same
number of a’s and b’s is regular. Namely, the language is defined as follows: L = {w ∈
{a, b}∗: #aw = #bw > 0}. For instance, aabb ∈L, babaab ∈L, bab ∉L.
Solution. The number of equivalence classes of the relation RL induced by the lan-
guage L is infinite. Consider the following infinite sequence of words:
w1 = a
w2 = aa
. . .
wn = an
. . .
Notice that any two different words u and v of this sequence cannot belong to the same
equivalence class: if u = ak, v = al, k
̸= l, then for z = bk we get uz = akbk ∈L and
vz = albk ∉L. Consequently, the relation RL creates at least equivalence classes corre-
sponding to words of the sequence, that is, infinitely many classes. In concluding, in
light of the Myhill–Nerode lemma, the language L is not regular.
2.1.4 The pumping lemma
In addition to regular expressions and the Myhill–Nerode lemma, the pumping lemma
is another tool that can be used to characterize regular languages. Regular expres-

2.1 Regular expressions and regular languages
|
31
sions and the Myhill–Nerode lemma (and regular grammars, which are discussed in
the next section) are aimed at proving that a language is regular. The Myhill–Nerode
lemma and the pumping lemma can be used to prove that a language is not regular.
As in the case of the Myhill–Nerode lemma, the pumping lemma is not proven here
since important topics used in the proof have not been discussed yet. The lemma is
formulated here since it is a crucial to the identification of regular languages. It will
be proven in Chapter 8.
Lemma 2.2 (The pumping lemma for regular languages).
If
a language L is regular
then
there exists a constant nL such that for any word z ∈L the following condition
holds:
(|z| ≥nL) ⇒[( ⋁
u,v,w
z = uvw ∧|uv| ≤nL ∧|v| ≥1)
⋀
i=0,1,2,...
zi = uviw ∈L]
The pumping lemma formulates conditions necessary for a regular language. It
shows that the nature of regular languages is finite and the length of words of a given
regular language is limited by some constant nL determined by the pumping lemma.
If a regular language includes a word of length greater than or equal to nL, then it is
an infinite language. However, a structure of words that are longer than or equal to nL
is fairly simple. Such words are generated by inserting strings of a length limited by
the constant nL into words of the language that are shorter than nL. That is, any word
of a regular language not shorter than nL has a floating part of being deleted, leaving
the remaining part in the language.
Since the pumping lemma formulates necessary conditions, it is of limited prac-
tical usefulness in its direct form. It can be used for the analysis of the structure of
words of the language. If words of a language satisfy the conclusion of the pumping
lemma, then the language could be intuitively presumed to be regular. Then, based
on such a supposition, the language could be proven to be regular. In practice, we use
contrapositive of the pumping lemma rather than its generic version. The contrapos-
itive of the pumping lemma formulates sufficient conditions for a language not to be
regular. This makes contrapositive of the pumping lemma to help prove that specific
languages are not regular.
Lemma 2.3 (Contrapositive of the pumping lemma for regular languages).
If
for any constant nL there exists a word z ∈L such that
(|z| ≥nL) ∧[( ⋀
u,v,w
z = uvw ∧|uv| ≤nL ∧|v| ≥1)
⋁
i∈{0,1,2,...}
zi = uviw ∉L]
then
the language L is not regular.

32
|
2 Regular expressions and regular languages
Example 2.8. Check that the language L defined in Example 2.7: L = {w ∈{a, b}∗:
#aw = #bw > 0} is not regular one.
Solution. Let us employ the contrapositive of the pumping lemma. We assume a word
z = aNbN for a constant N not less than the constant nL from the pumping lemma.
For any split z = uvw of the word z satisfying assumptions of contrapositive of the
pumping lemma, the first two parts uv can include only a’s: u = ap, v = aq, where 0 < q
and p+q ≤N. Iterated words zi = apa(i−1)paN−(p+q)bN = bN−(i−1)qbN do not belong to the
language L for any iteration value i
̸= 1 (for instance, for i = 0 we get z0 = aN−qbN). By
contrapositive of the pumping lemma, we conclude that the language L is not regular.
Example 2.9. Check if the language of binary words having number of zeros equal to
square of a natural number is not regular:
L = {w ∈{0, 1}∗: #0w = n2, n ∈{0, 1, 2, 3, . . .}}
Solution. Let us apply contrapositive of the pumping lemma. We assume a word
z = 0N2 ∈L for a constant N ≥nL. For any split z = uvw of the word z satisfying
antecedent of contrapositive of the pumping lemma, let us take a particular iterated
word z2 = 0N2+|v|. The word z2 ∉L for the reason that N2 < N2 + |q| ≤N2 + N < (N + 1)2.
Again, by contrapositive of the pumping lemma, the language L is not regular.
2.1.5 Regular grammars
Regular grammars constitute the most straightforward class of grammars. They gener-
ate regular languages, so then they can be used for proving that languages are regular.
In this section, we formulate a definition of regular grammars and use them as tools to
generate regular languages. The formal proof that regular grammars generate regular
languages will be given in Chapter 8.
Definition 2.4. A grammar G = (V, T, P, S) is:
–
left-linear if and only if all its productions take the form A →Bw or A →w, where
A, B are nonterminals, that is, A, B ∈V, w is a string of terminals, possibly empty,
that is, w ∈T∗;
–
right-linear if and only if all its productions of the form a →wB or A →w, where
A, B are nonterminals, w is a string of terminals, possibly empty;
–
regular if and only if it is either left-linear or right-linear.
Remark 2.5. Each left-linear grammar has its equivalent right-linear counterpart, and
oppositely, each right-linear grammar has an equivalent left-linear counterpart. Equiv-
alence of grammars is understood as the identity of generated languages. This obser-
vation will be proven in Chapter 8.

2.1 Regular expressions and regular languages
|
33
Example 2.10. The grammar given in Example 1.12 generates the language L of binary
numbers without nonsignificant 0’s. The grammar in this example is left linear, so
then the language is regular. The following right linear grammar G = ({S, A}, {0, 1}, P, S)
generates the same language, where:
P :
S →0 | 1A
(1), (2)
A →0A | 1A | ε
(3), (4), (5)
Solution. Indeed,
–
words generated by the grammar G are included into the language L for the reason
that they are:
–
either 0 (derived directly with the production (1) applied to the initial symbol
S of the grammar) or
–
strings beginning with 1. Such strings are derived with the production (2) ap-
plied to the initial symbol S and then with any number of productions (3) and
(4) applied to the nonterminal A. The derivation is terminated with the pro-
duction (5);
–
every word of the language L is generated in the grammar G. Notice that words of
this language are either 0 or they begin with the digit 1. Hence:
–
0 is derived as explained above: S →0;
–
any other word 1ak−1ak−2 . . . a1a0 (a binary number with leading digit 1) can
be derived applying the production (2) to the initial symbol S and then pro-
duction (3) or (4) should be used for every next digit 0 or 1, respectively. The
derivation is terminated with the production (5), when all letters have been
generated. That is, a string 1ak−1ak−2ak−3 . . . a1a0 is generated as follows:
S
1→1A
ak−1+3
󳨀→1ak−1 A
ak−2+3
󳨀→1ak−1ak−2 A
ak−3+3
󳨀→1ak−1ak−2ak−3 A ⋅⋅⋅
a1+3
󳨀→1ak−1ak−2 . . . a1 A
a0+3
󳨀→1ak−1ak−2ak−3 . . . a1a0 A
5→1ak−1ak−2ak−3 . . . a1a0
Example 2.11. Prove that the language of binary strings divisible by 5 is generated by
a regular grammar. Note that binary strings permit nonsignificant 0’s; for instance, 1,
01, 001 are valid binary strings.
Solution. Let assume that reminder of integer division of a binary number akak−1 . . .
a1a0 by 5 is equal to r. Observe that the binary number akak−1 . . . a1a0a (a digit a at-
tached as least significant digit) divided by 5 gives remainder (2r + a) mod 5. Based on
this observation, we can build the following right-linear grammar generating natural
numbers divisible by 5:
G = ({S, A0, A1, A2, A2, A3, A4}, {0, 1}, P, S)

34
|
2 Regular expressions and regular languages
P :
S →0A0 | 1A1
(1), (2)
A0 →0A0 | 1A1 | ε
(3), (4), (5)
A1 →0A2 | 1A3
(6), (7)
A2 →0A4 | 1A0
(8), (9)
A3 →0A1 | 1A2
(10), (11)
A4 →0A3 | 1A4
(12), (13)
The productions can be rewritten in shortened form:
P :
S →0A0 | 1A1
A0 →ε
Ai →0A(2i) mod 5 | 1A(2i+1) mod 5
for i = 0, 1, 2, 3, 4
For example, the derivation of the binary number 01010 is carried out as follows:
S →0 A0 →01 A1 →010 A2 →0101 A0 →01010 A0 →01010
This grammar generates the language L:
–
information about the remainder of the division of the part derived so far is kept
as the nonterminal’s index. Derivation can be terminated with the production (5),
which leaves a binary number divisible by 5. This confirms that any word gener-
ated in this grammar belongs to the language;
–
simple inductive evidence shows that any word of the language can be generated
in this grammar:
–
first of all, the grammar allows for generating all strings having one binary
digit and supplemented by a nonterminal, that is, 0A0 and 1A1. It also allows
to generate all strings of length one divisible by 5, that is, the only one string 0;
–
by the inductive hypothesis, let assume that the grammar allows for generat-
ing all binary strings of length n appended with respective nonterminal and
allows for generating all binary strings of length n divisible by 5. Any binary
string of length n appended to a nonterminal Ai can be extended to a binary
string of length n+1 by adding any digit (0 if the first Ai production is used and
1 for the second Ai production). This shows that any binary string of length
n + 1 is generated. Then the production (5) applied to the nonterminal A0
generates a binary number of the length divisible by 5. Of course, all binary
numbers of length n + 1 can be generated;
–
finally, due to the satisfaction of both inductive conditions, we can state that
every word of the language is generated in the grammar.
Example 2.12. Design a left-linear grammar generating the language of binary strings
divisible by 5; cf. Examples 2.10 and 2.11.
Solution. The right-linear grammar of Example 2.11 adds symbols at the end of a bi-
nary string. This grammar controls remainder of division by 5. A left-linear grammar

2.2 Problems
|
35
adds symbols at the beginning of the string. Let us observe how the value of a binary
number and the value of the remainder of integer division changes when a binary
digit is added at the beginning of a binary string. Assume that a reminder of division
of a binary number ak ak−1 ak−2 . . . a1 a0 by 5 is equal to r.
The digit 1 added at the beginning of a binary number ak ak−1 ak−2 . . . a1 a0 in-
creases its value by 2k+1 and changes reminder of division by 5 to (r + 2k+1) mod 5.
Notice that 2i mod 5 is equal to 1, 2, 4, 3 when i mod 4 is equal to 0, 1, 2, 3, respectively.
Notice also that 2i+1 mod 5 = (2 ∗(2i mod 5)) mod 5.
The digit 0 added at the beginning of the numbers neither change its value nor
remainder. However, it affects changes when the next possible digit is added.
Therefore, in order to be able to control remainder of division by 5, a left-linear
grammar must remember position of added digit (more exactly – a reminderof division
of this position’s value by 5) and a reminder of division by 5 of the already generated
string, and then apply conclusions of the above discussion. The discussion leads to
the following grammar:
G = ({S} ∪{Ai,j : i, j ∈{0, 1, 2, 3, 4}, i
̸= 0}, {0, 1}, P, S)
P :
S →A2,0 0 | A2,1 1
Ai,0 →ε
Ai,j →A(2i) mod 5,j 0 | A(2i) mod 5,(i+j) mod 5 1
for i, j ∈{0, 1, 2, 3, 4}, i
̸= 0
There are 21 nonterminals: the initial symbol of the grammar S and 20 indexed
nonterminals Ai,j. There are 46 productions: 2 are given explicitly, 4 Ai,0 nullable
productions and 40 Ai,j productions. The first index of a nonterminal Ai,j stores re-
mainder of division by 5 of the binary position of the nonterminal. The second index
keeps remainder of division the remaining string of binary digits by 5. For instance,
for Ai,j ak ak−1 . . . a1 a0 : i = 2k+1 mod 5 and j = akak−1 . . . a1a0 mod 5.
An example derivation of the binary number comes as follows:
A →A2,00 →A4,210 →A3,2010 →A101010 →A2,001010 →01010
The proof that this grammar generates the given language is similar to the proof
we outlined in Example 2.11. It is left to the reader.
2.2 Problems
Problem 2.1. Prove equalities given in Remark 2.4.
Solution. All equivalences are understood in terms of equality of languages. Let us
consider the equivalence r(s + t) = rs + rt. Let assume that regular expressions r, s, t
generate languages R, S, T, respectively. The regular expression r(s + t) generates the

36
|
2 Regular expressions and regular languages
language R ∘(S ∪T) = {uv : u ∈R ∧v ∈S ∪T}, which is equal to R ∘(S ∪T) = {uv : u ∈
R ∧v ∈S ∨u ∈R ∧v ∈T} and then it is equal to {uv : u ∈R ∧v ∈S} ∪{uv : u ∈R ∧v ∈
T} = R ∘S ∪R ∘T. The latest language is generated by the regular expression rs + rt.
Problem 2.2. Prove that the language L = {w ∈{a, b}∗: #aw > #bw > 0} is not
a regular one.
Solution. We apply the contrapositive of the pumping lemma to prove that L is not
regular. Let N is a constant from the lemma and let z = aN+1bN. We consider any split
z = uvw satisfying antecedent of the lemma, that is, |uv| ≤N and |v| ≥1. Thus, uv
cannot include the letter b and v includes at least one letter a, that is, u = ap and
v = ar where 1 ≤r ≤p + r ≤N. The word z0 = uv0w = aN+1−rbN is not included in L.
This observation completes the proof that the language is not regular.
Problem 2.3. Prove that the language L = {w ∈{a, b}∗: #aw
̸= #bw} is not regular.
Solution. Let apply the Myhill–Nerode lemma. Consider the sequence of words: u1 =
a, u2 = aa, . . ., un = an, . . .. Any two words of this sequence are not related in the
relation RL induced by L. Indeed, for any uk and ul, k
̸= l, if, for instance, z = bk, then
ukz = akbk ∉L and ulz = albk ∈L. This means that such two words belong to different
equivalent classes of RL, and as a result, we have an infinite sequence of equivalent
classes of RL.
Problem 2.4. Prove that the following language is regular:
L = {w ∈{a, b}∗: #aw mod 3 = #bw mod 3}.
Hint. Note: justify that there are three equivalence classes of the RL relation:
Ei = {w ∈{a, b}∗: (#aw −#bw) mod 3 = i}, i = 0, 1, 2. Apply the Myhill–Nerode lemma.
Problem 2.5. Prove that the complement of a regular language L ⊂Σ∗, that is, the
language L = Σ∗−L, is a regular language.
Hint. Notice that every equivalence class of the relation RL induced by the lan-
guage L is either included in L or disjoint with L and apply the Myhill–Nerode lemma.

3 Context-free grammars
The class of context-free languages is the next simplest class of languages, besides the
class of regular languages. Context-free languages are generated by context-free gram-
mars, which correspond to regular grammars and regular expressions. Context-free
grammars are one of the few most important tools used for the analysis of context-free
languages. They are incomparably simpler than context-sensitive and unrestricted
grammars. On the other hand, they define a wider class of languages than simpler reg-
ular grammars. The structure of words they generate is rich in this sense that two parts
of wordscan besimultaneously iterated,unlikein the case of regularlanguages, where
one part could be iterated. Moreover, context-free grammars are well elaborated. They
provide effective methods of analysis and generation of languages. Furthermore, there
are effective methods of automatic analysis and processing of context-free grammars.
Due to these advantages, they are widely applied in practice, for example, in natu-
ral language processing, processing of programming languages, translation of formal
languages, pattern recognition, etc.
Algebraic structures of context-free languages created in the set of all words over
an alphabet are much more complex than those created by regular languages. For this
reason, the algebraic analysis of context-free languages is limited. For instance, there
is no tool corresponding to the Myhill–Nerode theorem.
This chapter is devoted to a discussion on basic properties of context-free lan-
guages, especially those properties, which arise out of the analysis of context-free
grammars. Let us emphasize that an analysis of context-free grammars is well estab-
lished, given their practical relevance.
3.1 Context-free grammars – basics
Context-free grammars have a simple form of productions: a nonterminal creates pro-
duction’s left-hand side while a sequence of terminals and nonterminals figures its
right-hand side. This form of productions allows for a simple illustration of deriva-
tion. A derivation of a word can be demonstrated as a tree, which eases the proofs of
such important properties as the pumping lemma, the Ogden lemma and a decision
algorithm for context-free languages.
Definition 3.1. A grammar G = (V, T, P, S) is a context-free grammar if and only if left
side of any its production is a nonterminal, that is, any p ∈P is of a form A →α, where
A ∈V and α ∈(V ∪T)∗.
Notice that productions of regular grammars may have only one nonterminal in
their right-hand side and those nonterminals must be at either left or right edge. So
then, regular grammars are a special case, straightforward case, of context-free gram-
mars.
https://doi.org/10.1515/9783110752304-003

38
|
3 Context-free grammars
Definition 3.2. A language L ⊂Σ∗is context-free if and only if it is generated by a
context-free grammar.
Example 3.1. The following context-free grammar generates arithmetic expressions:
G = ({E}, {+.∗, id}, P, E)
P:
E →E + E
(1)
E →E ∗E
(2)
E →id
(3)
The grammar generates a restricted form of arithmetic expressions with addition
and multiplication only, not using brackets and with only a single argument id. Such
grammar has limited practical importance. However, it will allow us for a convenient
illustration of structures related to context-free grammars. On the other hand, it is
easy to extend the grammar for subtraction and division (including new productions
similar to the first two). Moreover, if the terminal id is replaced by a nonterminal and
productions for this nonterminal are added, we get a more realistic grammar for arith-
metic expressions.
Example 3.2. Let check that the word id ∗id + id is derivable from the initial symbol of
the grammar provided in Example 3.1, that is, let build a derivation E
∗󳨀→id ∗id + id.
In the following derivation, productions are applied to the underlined nontermi-
nals:
E
1→E + E
2→E ∗E + E
3→id ∗E + E
3→id ∗id +E
3→id ∗id + id
where a number above the derivation symbol (right arrow) refers to an applied pro-
duction, for example,
1→informs that the first production is employed.
In this derivation, productions are always applied to the leftmost nonterminal
of intermediate words of derivation. Such a derivation is called leftmost derivation.
A derivation with rightmost nonterminals yielding productions is called right-
most derivation. The following derivation of the word id ∗id + id is the rightmost
one:
E
2→E ∗E
1→E ∗E + E
3→E ∗E + id
3→E ∗id + id
3→id ∗id + id
We can build another rightmost derivation of the word id ∗id + id:
E →E + E →E + id →E ∗E + id →E ∗id + id →id ∗id + id
and yet another leftmost derivation of this word:
E →E ∗E →id ∗E →id ∗E + E →id ∗id +E →id ∗id + id

3.1 Context-free grammars – basics
|
39
In addition to the leftmost and rightmost derivations, there are several mixed
derivations, that is, derivations in which productions are applied to nonsystemati-
cally chosen nonterminals.
For context-free grammar, a word’s derivation can also be presented in the form
of a tree.
Definition 3.3. A derivation (parsing) tree in a context-free grammar G = (V, T, P, S)
is a tree T = (W, E ⊂W × W) satisfying the following properties:
1.
it is compatible with Definition 1.9, but the set of children of every vertex is or-
dered;
2.
each vertex w ∈W of the tree is labeled with a nonterminal symbol or a terminal
symbol or the empty word: w ∈W = V ∪T ∪{ε};
3.
the root of the tree is labeled with the initial symbols S of the grammar;
4.
internal vertices of the tree (i. e., vertices different than leaves) are labeled with
nonterminals;
5.
if a nonterminal A labels an internal vertex and children of this vertex are labeled
with symbols (terminals and nonterminals) X1 X2 . . . Xk in this order or with the
empty word ε, then there exists a production A →X1 X2 . . . Xk or the production
A →ε, respectively;
6.
a vertex labeled with the empty word ε is the only child of its parent.
Notice that in this definition, we say a vertex is labeled with a symbol instead of a
vertex is a symbol in order to handle cases of several occurrences of the same symbol
in a tree.
We have two derivation trees for the word id ∗id + id, as shown in Figure 3.1. Both
trees have the same crop, so they generate the same word. Notice that each deriva-
tion tree corresponds to one leftmost derivation and one rightmost derivation of this
word.
Figure 3.1: Derivation trees for the word id ∗id + id in Example 3.2.

40
|
3 Context-free grammars
Remark 3.1. Let us notice that the first derivation tree in Figure 3.1 corresponds to the
first leftmost derivation and to the second rightmost derivation shown in Example 3.2.
The second derivation tree of this figure corresponds to the second leftmost derivation
and to the first rightmost derivation of the example. In general, every derivation tree
has a corresponding leftmost derivation and rightmost derivation.
Derivation trees corresponding to derivations are independent of the order of non-
terminals used for productions employed in a derivation. So then, there is no analogy
between derivation trees and sided (leftmost/rightmost/mixed) derivations. However,
if there is more than one derivation tree for a given word, then each derivation tree cor-
responds to a leftmost associated derivation and corresponds to an associated right-
most derivation. Therefore, in this meaning, there is correspondence between deriva-
tion trees, leftmost derivations and rightmost derivations.
We say that a word is ambiguous if it has more than a single derivation tree.
Equivalently, we can say that a word is ambiguous if and only if it has more than
one leftmost derivation or if it has more than one rightmost derivation. A context-free
grammar is said to be ambiguous if and only if there is an ambiguous word. A context-
free language is inherently ambiguous if and only if its every context-free grammar is
ambiguous.
The grammar discussed in Example 3.1 is ambiguous. However, the language of
arithmetic expressions is not inherently ambiguous. Later on in this chapter, we show
an example of a nonambiguous context-free grammar for arithmetic expressions.
Example 3.3. The following language is inherently ambiguous:
L = {akblcm : k, l, m, > 0 ∧(k = l ∨l = m)}
= {akbkcm : k, m, > 0} ∪{akbmcm : k, m, > 0}
Solution. A grammar G = (V, T, P, S) for this language may have productions such as
shown below:
P:
S →L | R
(1)(2)
L →Lc | Ac
(3)(4)
A →aAb | ab
(5)(6)
R →aR | aC
(7)(8)
C →bCc | bc
(9)(10)
Any word of the form w = akbkck has two leftmost derivations (notice that pro-
ductions have at most one nonterminal on the right-hand side):
S
1→L
(k−1)∗3
󳨀→
Lck−1
4→Ack (k−1)∗5
󳨀→
ak−1Abk−1ck
6→akbkck
and
S
2→R
(k−1)∗7
󳨀→
ak−1 R
8→ak C
(k−1)∗9
󳨀→
akAbk−1 C ck−1
10
→akbkck

3.1 Context-free grammars – basics
|
41
where formulas placed above derivation symbols (right arrows) identify a production
employed and, in some cases, how many times it is yielded. For instance,
(k−1)∗3
󳨀→means
that production number 3 is applied k −1 times.
Words generated here are nonempty sequences of symbols a followed by symbols
b and then followed by symbols c. There are two cases: lengths of the first two se-
quences (composed of symbols a and b) are equal or lengths of the last two sequences
(composed of symbols b and c) are equal. Notice that the direct derivation from the
initial symbol of the grammar (production number 1 or production number 2) decides
which case is considered.
No formal proof of inherent ambiguity of this language is given here since this
property is of minor importance for this book. The proof that some language is inher-
ently ambiguous is given in [1]. Intuitively, every grammar for the language must have
a mechanism deciding about the equality of lengths of the first two sequences or of
the last two sequences. This mechanism raises ambiguity about such grammar and
subsequently of the language itself.
Example 3.4. Prove that the language L = {w ∈{a, b}∗: #aw = #bw > 0} is context-
free.
Solution. The following context-free grammar generates L:
G = ({S}, {a, b}, { S →ab | ba | aSb | bSa | SS }, S)
To show that the grammar G generates L, we will prove that any word of L has a
derivation in G and then we will prove that any word generated in G is in L.
Before we attempt to do this, let as show that for any word z ∈L, if it begins and
ends with the same symbol, then it may be split to two parts, such that both belong
to L. Formally, if za = awaa, zb = bwbb, |wa|, |wb| > 0 and za, zb ∈L, then there exist
nonempty ua, va, ub, vb such that za = uava, zb = ubvb and ua, va, ub, vb ∈L.
For a given word za = a1 a2 . . . an (where a1 = an = a and a2, . . . , an−1 are ei-
ther a or b) define a mapping μ : {1, 2, . . . , n} →N and for k = 1, 2, . . . , n, μ(k) =
#a(a1 a2 . . . ak) −#b(a1 a2 . . . ak), that is, μ gives superiority of as over bs in a prefix of
length k. Notice that μ(n) = 0, what implies that μ(n −1) = −1. Moreover, μ(1) = 1 and
μ(k) can increase or decrease its value by 1 for successive values of k. Consequently, μ
must cross 0 somewhere in between 1 and n−1, that is, there must be p ∈{2, 3, . . . , n−2},
for which μ(p) = 0. As a result, ua = a1 . . . ap and va = ap+1 . . . an.
Consider a word zb likewise.
Now, let us prove that any word w ∈L has a derivation in G:
–
the shortest words of L are ab and ba. They are derivable with the first or the sec-
ond production directly from the initial symbol S of G;
–
for a longer word z = a1wan+2 consider its first and last symbols. There are four
cases: (1) a1 = a, an+2 = b, (2) a1 = b, an+2 = a, (3) a1 = a = an+2 and (4) a1 = b =

42
|
3 Context-free grammars
an+2. The first and the second cases are similar and the third and the fourth cases
are similar:
–
in the first and the second cases the subword w is in L. So, by the inductive
assumption, there is a derivation S
∗→w of w in G. So then S →aSb
∗→awb
and S →bSa
∗→bwa are derivations of z in these cases;
–
in the third case z = za = uava, where ua, va ∈L, cf. the above discussion.
Since |ua|, |va| < |z|, then by inductive assumption, there exist derivations
S
∗→ua and S
∗→va. Hence, S →SS
∗→uaS
∗→uava is a derivation of z in G;
–
the fourth case is similar to the third one;
–
based on mathematical induction we conclude that each word from the language
L is generated in the grammar G.
Afterward, we prove that any word derivable in G belongs to L. Let S
n→z, z ∈{a, b}∗
is a derivation in G, where
n→denotes derivation of length n (productions are applied
n times).
–
for n = 1 a derivation produces either ab or ba, both in L;
–
a derivation A
n+1
→z, z ∈{a, b}∗can be decomposed to:
–
either A →aSb
n→awb or
–
S →bSa
n→bwa or
–
S →SS
k→uS
l→uv, where 1 ≤k, l ≤n.
By inductive assumption w, u, v ∈L, that is, numbers of as and bs are equal in
each of these words. Consequently, numbers of as and bs are equal in z = awb,
z = bwa and z = uv;
–
based on mathematical induction we conclude that each word derivable in the
grammar G is in the language L.
3.2 Simplification of context-free grammars
The definition of context-free grammars does not include optimization mechanisms,
which would allow for simplification of grammars. In this section, we discuss some
methods of simplification of context free grammars. For instance, a context-free gram-
mar including symbols and productions, which are never used in derivation of any
word, may be transformed to a form simpler to use. Also, a context-free grammar can
be restructured to a form more suitable for a given application.
3.2.1 Useless symbols
In particular, context-free grammar can include symbols that are useless for gener-
ated language. A set of nonterminals that will never produce a sequence of terminals

3.2 Simplification of context-free grammars
|
43
is the first type of useless symbols, let us call them not generative ones. A set of sym-
bols (terminals or nonterminals) never used in derivation from the initial symbol of
the grammar forms the second type of useless symbols, call them unreachable. Both
types of useless symbols can be removed together with productions including such
symbols (such productions are invalid in terms of reduced sets of nonterminals and
terminals). Both grammars, the initial one and the grammar with removed useless
symbols and removed invalid productions, generate the same language. This obser-
vation is obvious since a production, which includes useless symbols, can never be
used in the derivation of any word over the terminal alphabet.
Proposition 3.1. For any context-free grammar G = (V, T, P, S) generating a nonempty
language L(G) there exists an equivalent (i. e., generating the same language) context-
free grammar G′ = (V′, T, P′, S) such that every nonterminal A ∈V′ generates a se-
quence of terminals (possibly empty). The following algorithm shows how to remove
useless not generative symbols (symbols of the first type):
begin
Vold:=Vnew:=0
for each production p: A→α, p ∈P do
if α ∈T∗then Vnew:=Vnew ∪{A}
{start with nonterminals producing a string of terminals}
while Vold
̸=Vnew do
begin
Vold:=Vnew
for each production p: A→α, p ∈P do
if α ∈(T ∪Vold)∗then Vnew:=Vnew ∪{A}
end
V′:=Vnew
{new set of nonterminals}
P′:= {A→α ∈P : A ∈V′, supp(α) ⊂(V′ ∪T)}
{nonterminals of productions must be included into new set of nonterminals}
end
Notice that the initial symbol would be classified as useless if context-free gram-
mar generates the empty language. However, any grammar must include an initial
symbol, so then the initial symbol cannot be classified as a useless one.
Proposition 3.2. For any context-free grammar G = (V, T, P, S) generating a nonempty
language L(G) there exists an equivalent (i. e., generating the same language) context-
free grammar G′′ = (V′′, T′′, P′′, S) such that every nonterminal symbol A ∈V′′ and
every terminal symbol a ∈T′′ could be derived from the initial symbol of the grammar.
The following algorithm allows for removing useless unreachable symbols (symbols of
the second type):

44
|
3 Context-free grammars
begin
Vold:=Told:=0
Vnew:= {S}, Tnew:=0
while Vold
̸=Vnew or Told
̸=Tnew do
begin
Vold:=Vnew, Told:=Tnew
for A∈Vold do
begin
Vnew:=Vnew∪{B∈V : exists A→α and B∈supp(α)}
Tnew:=Tnew∪{a∈T : exists A→α and a∈supp(α)}
end
end
V′′:=Vnew
{new set of nonterminals}
T′′:=Tnew
{new set of terminals}
P′′:= {A→α ∈P : A ∈V′, supp(α) ⊂(V′′ ∪T′′)}
{nonterminals of productions must be included into new set of nonterminals}
end
Proof. We have Proposition 3.1 and Proposition 3.2. Let us observe that:
–
a derivation tree of any word w ∈L(G′) in the grammar G′ is also a valid derivation
tree in the grammar G. The same concerns the grammar G′′. Therefore, L(G′) ⊂
L(G) and L(G′′) ⊂L(G);
–
a derivation tree of any word w ∈L(G) cannot have an internal node labeled with
a useless not generative nonterminal (otherwise, there will be a leaf labeled with
a nonterminal in a finite derivation tree). As a result, the derivation tree in G is
also a valid derivation tree in G′, that is, L(G) ⊂L(G′);
–
a derivation tree of any word w ∈L(G) cannot have a node (an internal node or
a leaf) labeled with a useless unreachable symbol (otherwise, there will be no
path from the root labeled with the initial symbol of the grammar G to this useless
symbol). For that reason, the derivation tree in G is also a valid derivation tree in
the grammar G′′, that is, L(G) ⊂L(G′′).
The above observation leads to the conclusion that L(G) = L(G′) and L(G) = L(G′′),
what completes the proof.
Removing useless symbols requires the application of the first algorithm followed
by the second one; cf. [1] or alternatively in e.g. [2]. The use of the algorithm in the op-
posite order may not remove all useless symbols, as shown in the following example.
Example 3.5. The following grammar:
G = ({S, A, B}, {a}, {A →SB | a, B →AB, A →a}, S)

3.2 Simplification of context-free grammars
|
45
includes the not generative symbol B. Employing the first algorithm to remove useless
symbols we get the grammar:
G′′ = ({S, A}, {a}, {S →a, A →a}, S)
with a useless symbol A. The second algorithm produces the grammar with no useless
symbols:
G′′ = ({S}, {a}, {S →a}, S)
The second algorithm employed prior to the first one does not change the gram-
mar since it does not include any unreachable symbol of the second type. The first
algorithm applied then produces the grammar
G = ({S, A}, {a}, {S →a, A →a}, S)
with a useless symbol A. The second algorithm employed again removes the remaining
useless symbol.
Proposition 3.3. The following property can be directly drawn from Proposition 3.1
and Proposition 3.2: any nonempty context-free language is generated by a context-free
grammar without useless symbols.
3.2.2 Nullable symbols and ε-productions
Elimination of ε-productions and nullable symbols is the next step of simplification of
context-free grammars. We rather change the status of nullable symbols rather than
eliminate them from grammars. Anyway, for the sake of simplicity, we will be using
the term eliminate, but having this in mind that this means change status of nullable
symbols.
ε-production is a production of the form A →ε and nullable symbol is a nontermi-
nal symbol producing the empty word A
∗→ε. Of course, if the empty word is derivable
from the initial symbol in a context-free grammar G, then it is not possible to elim-
inate all ε-production and nullable symbols. However, removing ε-productions and
nullable symbols from the grammar G turns it to the grammar generating the language
L(G)−{ε}. Therefore, the process of elimination of nullable symbols and ε-productions
will be applied to context-free grammars with the awareness that the empty word may
be removed from the generated language.
The following algorithm finds nullable symbols in a context-free grammar G =
(V, T, P, S):

46
|
3 Context-free grammars
begin
Vold:=0
{begin with no nullable symbols}
Vnew:= {A∈V : A→ε is a production}
{add all nonterminals producing directly the empty word}
while Vold
̸=Vnew do
begin
Vold:=Vnew
Vnew:=Vnew∪{A∈V : exists A→α, where α ∈V∗
old}
{add all nonterminals producing directly a word over nullable symbols}
end
end
Having the set of nullable symbols, we will be able to remove ε-productions. Observe
that nullable symbol in a production (its right-hand side) either can generate a string
of terminal symbols or can be turned to the empty word. As a consequence, a nul-
lable symbol can either be left on the right side of a production (when it produces a
nonempty sequence of terminal symbols) or it can be dropped from the right-hand
side of a production (when it generates the empty word). This observation leads to the
following method.
Proposition 3.4. Let G = (V, T, P, S) is a context-free grammar with no useless symbols
generating a language without the empty word. If A →X1 X2 . . . Xn is a production, then
this production is replaced with a set of all productions of a form A →α1 α2 . . . αn that
for all i = 1, 2, . . . , n satisfies conditions:
–
αi = Xi, if Xi is not nullable (i. e., it is a terminal symbol or a not nullable nonterminal
symbol);
–
αi = Xi or αi = ε, if Xi is a nullable symbol, that is, for a nullable symbol Xi we get two
productions, one with Xi left at the right-hand side and another one with Xi dropped
from the right-hand side;
–
not all α1, α2, . . . , αn are equal to the empty word (this condition eliminates ε-produc-
tions). Of course, the existing ε-productions are removed.
Notice that this method turns status of nullable nonterminals to not nullable ones rather
then removes them from the grammar. Finally, we get a grammar G′ = (V, T, P′, S) with-
out nullable symbols and ε-productions. This grammar has a modified set of productions
and is equivalent to the grammar G, that is, generates the same language.
Proof. Proof of equivalence of both grammars is based on equivalence of derivations
in both grammars G and G′:
–
both grammars have the same sets of terminals and nonterminals;
–
a derivation in the grammar G of any word w ∈L(G) can be turned to a derivation
of the same word in the grammar G′:

3.2 Simplification of context-free grammars
|
47
–
if a derivation does not employ ε-productions, then this is also a derivation in
the grammar G′;
–
if an ε-production X →ε is applied in a part of derivation with a production
Y →α X β utilized prior to the X →ε:
⋅⋅⋅→γ Y δ →γα X βδ →γαβδ →⋅⋅⋅
then this part cannot be included in any derivation in G′, but it can be turned
to a fragment of a derivation in G′ shown below. Here, the production Y →αβ
with the nullable symbol X dropped is employed,
⋅⋅⋅→γ Y δ →γαβδ →⋅⋅⋅
Finally, if we apply analogous replacement for every ε-production, the deriva-
tion of the word w ∈L(G) in the grammar G is turned into a derivation of the
same word in the grammar G′;
–
a derivation in the grammar G′ of any word w ∈L(G′) can be turned to a derivation
of the same word in the grammar G:
–
if a derivation utilizes only productions of the grammar G, then it is a valid
derivation of the word w in G;
–
otherwise, a derivation employs at least one production of a form Y →αβ of
the grammar G′ gotten from a production of a form Y →α′ X β′. Then a part
of derivation
⋅⋅⋅→γ Y δ →γαβδ →⋅⋅⋅
can be replaced by a fragment of a derivation in a grammar G with inserted a
series of ε-productions:
⋅⋅⋅→γ Y δ →γα′ X β′δ →⋅⋅⋅→γαβδ →⋅⋅⋅
where X →ε is an ε-productions in the grammar G and strings α and β are
gotten by applying the same scheme to all nullable symbols of both strings.
This method applied to all productions of the derivation of the word w ∈L(G′)
in the grammar G′ turns this derivation to a derivation of the same word in the
grammar G.
Finally, comparing languages generated by a context-free grammar G and by its trans-
formed form G′ without nullable symbols and ε-productions, we can state that L(G′) =
L(G) −{ε}.

48
|
3 Context-free grammars
Example 3.6. Remove nullable symbols and ε-productions from the grammar
G = ({E, El, T, Tl, P}, {+, ∗, id}, P, E) :
P′:
E →TEl
(1)
El →+TEl
(2)
El →ε
(3)
T →PTl
(4)
Tl →∗PTl
(5)
Tl →ε
(6)
P →id
(7)
Solution. It is straightforward to show that the grammar does not include useless
symbols and that the empty word cannot be derived. We have two nullable symbols:
El and Tl. We take advantage of Proposition 3.4 to remove ε-productions and change
the status of nullable symbols transforming productions of the grammar G to the fol-
lowing set of productions:
P:
E →TEl | T
(1)
El →+TEl | +T
(2)
not included
(3)
T →PTl | P
(4)
Tl →∗PTl | ∗P
(5)
not included
(6)
P →id
(7)
3.2.3 Unit productions
A context-free grammar G = (V, T, P, S) may have unit productions. Unit productions
are of a form A →B, where A, B ∈V. Unit productions are confusing and do not pro-
vide any new abilities for language generation. Elimination of unit productions is the
next step of grammar simplification. The method of removing unit productions is con-
cerned with substituting a unit production A →B with a series of productions A →αi
for all B-productions B →αi. The method is outlined in the form of the following al-
gorithm:
begin
while there exists a unit production A→B do
begin
if (A=B) then remove the production
else
replace the production A→B with
productions A→γ1|...|γk

3.2 Simplification of context-free grammars
|
49
where γ1,...,γk are right-hand side
of all B-productions
end
end
The new grammar G′′′ = (V, T, P′′′, S) produced by this algorithm may have useless
nonterminal symbols, which need to be removed. For instance, compare [1], the gram-
mar
G = ({S, A, B}, {a}, {S →A|a, A →B, B →a}, S)
is turned to the grammar without unit production, but with useless nonterminal sym-
bols A and B:
G′′′ = ({S, A, B}, {a}, {S →a, A →a, B →a}, S)
Note that elimination of the unit production S →A introduces the production S →a,
which already exists in the grammar. The process of removing useless symbols yields
the following grammar:
G∗= ({S}, {a}, {S →a}, S)
A grammar without unit productions is equivalent to the former one. To prove
this, let us consider a derivation tree in the former grammar. If a part of derivation
tree matching a derivation Ai1 →Ai2 →⋅⋅⋅→Air →α with all productions except
the last one are unit (the last production is not unit, the vertex Air has at least two
children or one leaf) includes a repeating nonterminal A′ in the path Ai1 →⋅⋅⋅Aip →
A′ →A′ →⋅⋅⋅→A′ →A′ →Air →⋅⋅⋅Air, then this path can be shortened to
Ai1 →⋅⋅⋅Aip →A′ →Air →α.
Let assume that a derivation Ai1 →Ai2 →⋅⋅⋅→Air →α with all productions
except the last one are unit. The method of elimination of unit productions provides
a production Ai1 →α, so then this derivation can be cut to Ai1 →α, which creates
a part of a derivation tree in the grammar G′′′. And vice versa, if we have a part of
derivation tree in the grammar G′′′ matching a production Ai1 →α in this grammar,
then this production either belongs to the grammar G or it can be turned to a derivation
Ai1 →Ai2 →⋅⋅⋅→Air →α in the grammar G.
In conclusion, a derivation tree in the grammar G can be turned to a derivation tree
in the grammar G′′′ by replacing all such transformations. And vice versa, a derivation
tree in the grammar G′′′ can be turned to a derivation tree in the grammar G.
Example 3.7. Remove unit productions from the grammar G′ considered in Exam-
ple 3.6.

50
|
3 Context-free grammars
Solution. There are two unit productions: the first one is in the group (1) of produc-
tions, the secondone in the group(4) ofproductions. The first unit productionE →T is
replaced with two productions E →PTl and E →P. The production E →P is still unit
one and is finally turned into E →id. The second unit production T →P is replaced
with the production T →id. As a result, we get the following set of productions:
P:
E →TEl | PTl | id
(1)
El →+TEl | +T
(2)
not included
(3)
T →PTl | id
(4)
Tl →∗PTl | ∗P
(5)
not included
(6)
P →id
(7)
3.3 Normal forms of context-free grammars
We discuss two normal forms of context-free grammars, that is, Chomsky normal form
and Greibach normal form. Conversions of context-free grammars to normal forms
come as a further step in the simplification of grammars. Normal forms are gram-
mars with restrictions imposed on the form of productions. Grammars in normal forms
produce languages without the empty word, so then only grammars not generating
the empty word could be transformed into normal forms. This is why a context-free
grammar, in which the empty word is derivable, should be turned to a form gener-
ating the same language without the empty word. Since both normal forms do not
admit ε-productions, removal of ε-productions and nullable symbols will convert any
context-free grammar to a form generating the language without the empty word. Nor-
mal forms do not necessarily require the removal of useless symbols. Anyway, it is
recommended to simplify a grammar by removing useless symbols first.
3.3.1 Chomsky normal form
Definition 3.4. A context-free grammar G = (V, T, P, S) is in Chomsky normal form if
and only if its productions are of the form A →BC or A →a, where A, B, C ∈V, a ∈T
(i. e., any production turns a nonterminal to two nonterminals or to one terminal).
Proposition 3.5. Any context-free grammar without ε-productions and unit productions
can be transformed to Chomsky normal form.
Proof. Let assume that a context-free grammar G = (V, T, P, S) does not have ε-pro-
ductions and unit productions, so then a right-hand side of any production is a ter-
minal symbol or is a string of at least two symbols. Productions with one terminal
symbol on the right-hand side are in Chomsky normal form. Such productions will
not be changed.

3.3 Normal forms of context-free grammars
|
51
Productions having at least two symbols on the right-hand side will be trans-
formed into a set of productions according to the following rules:
1.
every production of a form A →α1 a α2 is substituted with two productions
A →α1 A′ α2 and A′ →a, where: A ∈V, a ∈T, α1α2 ∈(V ∪T)+ and A′ is a new
nonterminal. That is, each terminal (on the right-hand side) is replaced with a new
nonterminal and the production from this new nonterminal to replaced terminal
is added;
2.
every production of a form A →A1 A2 A3 . . . An, n > 2, A1, A2, A3, . . . An ∈V is
substituted with two productions A →A1,2 A3 . . . An and A1,2 →A1 A2, where
A, A1, A2, . . . , An ∈V, A1,2 ∈V is a new nonterminal. Notice that a production with
right-hand side of length n > 2 is replaced with a production in Chomsky normal
form (two nonterminals) and a production with right-hand side of length n −1.
Therefore, applying this operation n −2 times to a production with right- hand
side of length n we turn it to n −1 productions in Chomsky normal form.
The new grammar G′ = (V′, T, P′, S) includes newly added nonterminals. All produc-
tions of the grammar G not in Chomsky form are replaced with sets of new productions
in Chomsky normal form.
Both grammars G and G′ are equivalent, that is, they generate the same language.
To show this, let us consider a derivation tree of a word in grammars G and G′:
–
a local fragment of a derivation tree in the grammar G matching a production of a
form A →α1 a α2 is shown in part (i) of Figure 3.2. This fragment can be replaced
with a fragment matching two productions A →α1 A′ α2 and A′ →a (both substi-
tute the former production) as shown in part (ii) of Figure 3.2. The new tree gener-
ates the same crop. On the other hand, a production A →α1 A′ α2 of the grammar
G′ forces the production A′ →a since the nonterminal symbol A′ is unique in the
grammar G′ and appears only in former two productions. Both later productions
correspond to a part of a derivation tree in the grammar G′ shown in part (ii) of
Figure 3.2. This part can be turned to a structure shown in part (i) of Figure 3.2,
which corresponds to a production A →α1 a α2 of the grammar G;
–
parts of derivation trees corresponding to a production of a form A →A1 A2 . . . An
and to its substitutions A →A1,2 . . . An and A1,2 →A1 A2 is shown in parts (iii) and
(iv) of Figure 3.2;
–
finally,
–
substituting all fragments of a derivation tree shown in parts (i) and (iii) of
Figure 3.2 turns a derivation tree in the grammar G to a derivation tree in the
grammar G′;
–
opposite substitutions turns a derivation tree in the grammar G′ to a deriva-
tion tree in the grammar G;
–
substitutions does not change the crop of subjected trees;
what justifies equivalence of grammars G and G′.

52
|
3 Context-free grammars
Figure 3.2: Equivalence of a context-free grammar and its Chomsky normal form.
Example 3.8. Find a grammar in Chomsky normal form for the set of production P′′
considered in Example 3.7.
Solution.
–
substitutions as in point 1 of Proposition 3.5 turns productions as shown below.
Note that enumeration of groups of productions is inherited from the one shown
in Example 3.6,
P′′:
E →TEl | PTl | id
(1)
El →⊕TEl | ⊕T
(2)
⊕→+
(2′)
T →PTl | id
(4)
Tl →⊙PTl | ⊙P
(5)
⊙→∗
(5′)
P →id
(7)
–
substitutions as in point 2 of Proposition 3.5 turns productions to Chomsky normal
form:
GC = ({E, El, T, Tl, P, T⊕, P⊙, ⊕, ⊙}, {+, ∗, id}, PC, E)

3.3 Normal forms of context-free grammars
|
53
PC:
E →TEl | PTl | id
(1)
El →T⊕El | ⊕T
(2)
T⊕→⊕T
(2′′)
⊕→+
(2′)
T →PTl | id
(4)
Tl →P⊙Tl | ⊙P
(5)
P⊙→⊙P
(5′′)
⊙→∗
(5′)
P →id
(7)
Chomsky normal form of a context-free grammar is an important and powerful
theoretical tool. The desired property of grammar in Chomsky normal form is that
derivation trees are binary. Chomsky normal form will be exploited in the pumping
lemma for context-free languages, in the Ogden lemma and in a decision algorithm
for context-free languages.
3.3.2 Greibach normal form
Definition 3.5. A context-free grammar G = (V, T, P, S) is in Greibach normal form if
and only if its productions are of the form A →a α where A ∈V, a ∈T, α ∈V∗,
i. e., any production turns a nonterminal to a terminal and a string (possibly empty)
of nonterminals.
Proposition 3.6. Any context-free grammar without ε-productions and unit productions
could be transformed to Greibach normal form.
Proof. The following method can be used to transform a context-free grammar G =
(V, T, P, S) to Greibach normal form:
1.
transform the grammar to Chomsky normal form GC = (VC, T, PC, S);
2.
enumerate nonterminal symbols in VC = {A1, A2, . . . , An};
3.
modify the grammar GC such that the right-hand side of every Ai-production be-
gins with a terminal symbol or with a nonterminal symbol of greater index:
(∗)
{ Ai →aα
or
Ai →Ajα
where a ∈T, Ai, Aj ∈VC and j > i, α ∈V∗
C
Let assume that for i = 1, 2, 3, . . . , k −1 all Ai productions satisfy the condition
(∗) given above and that some Ak production does not satisfy this condition. If a
production Ak →Aj α, where Ak, Aj ∈VC and α ∈V∗
C , does not satisfy (∗), then
either (i) k > j or (ii) k = j. Then perform the following operations: (a) in the case
(i) and (b) in the case (ii):
(a) for every Aj-production, Aj →β, replace the production Ak →Aj α with the
production Ak →β α. The right-hand sides of new productions Ak →βlα ei-

54
|
3 Context-free grammars
ther begin with a terminal or with a nonterminal Al with index greater than j.
Observe that every such substitution increases the value of the index of the
right-hand side nonterminal. Repeating substitutions at most k −j times, we
obtain productions replacing Ak →Aj α with right-hand sides beginning with
either a terminal symbol Ak or with the nonterminal symbol or with a non-
terminal symbol Al with l > k. Applying this operation to all Ak productions
satisfying (i), we eliminate this case;
(b) let there is the following set of Ak-productions:
Ak →Ak α1 | Ak α2 | ⋅⋅⋅| Ak αp | β1 | β2 | ⋅⋅⋅| βr
where: α1, . . . , αp ∈V∗
C , β1, . . . , βr ∈(T ∪{Ak+1, . . . An}) ∘V∗
C
that is, βl is a terminal or a nonterminal with index greater than k followed by
a sequence (possibly empty) of nonterminals.
This set of Ak-productions is replaced with the following set of new produc-
tions:
Ak →β1 | β2 | ⋅⋅⋅| βr | β1Bk | β2Bk | ⋅⋅⋅| βrBk
Bk →α1 | α2 | ⋅⋅⋅| αp | α1Bk | α2Bk | ⋅⋅⋅| αpBk
where: Bk, is a new nonterminal. Nonterminals Bi are ordered according to
their indexes and are followed by all nonterminals Ai.
Notice that all newly included productions satisfy the condition (∗). There-
fore, the condition (∗) is satisfied for all Ai-productions and Bi-productions
for i = 1, 2, . . . , k;
4.
the process of the recent point repeated for successive nonterminals guarantee
the satisfaction of the condition (∗) for all productions. Moreover, the right-hand
side of every An-production must begin with a terminal symbol since An is the
last nonterminal in the introduced order, that is, every An-production is in the
Greibach normal form;
5.
for backward values of k = n −1, n −2, . . . , 2, 1, for every Ak-production with right-
hand side with a leading nonterminal symbol Aj, j ∈{n, n−1, . . . , k+1, }, replace this
production with a new set of productions. Productions of this new set are obtained
by replacing the leading nonterminal Aj with right-hand sides of Aj-productions.
We do the same with Bk-productions for decreasing values of index k. Productions
of the newly created set are in Greibach normal form because all Aj-productions
and Bj-productions have already been turned to Greibach normal form;
6.
as a result, we get a grammar GG = (VG, T, PG, S) in Greibach normal form, where
VG is a set of nonterminal symbols VC supplemented with nonterminal symbols
Bk created in point 3(b).

3.3 Normal forms of context-free grammars
|
55
It has already been noted that the conversion to Chomsky normal form does not
change the language being generated. The transformation described in point 3(a) also
keeps the generated language without any changes – justification is the same as for
conversion to Chomsky normal form and for the elimination of unit productions.
We justify that elimination of looped productions in point 3(b) does not change
the language. Let assume that a part of a derivation tree in the grammar G employs
several productions of a form Ai →Aiαj, that is, a nonterminal symbol Ai is substituted
by a string of nonterminal symbols Aiαi several times and finally Ai is substituted by
a string β. The following example concerning a fragment of a derivation tree in the
grammar G shown in part (a) of Figure 3.3 is considered. This fragment is equivalent
to the following derivation G:
Ai →Aiαi3 →Aiαi2αi3 →Aiαi1αi2αi3 →βαi1αi2αi3
The rules in point 3(a) of Proposition 3.6 employed to the above fragment of deriva-
tion tree produces a fragment of a derivation tree in the grammar GG. This fragment
is shown in Figure 3.3(b). It is equivalent to the following derivation completed in the
grammar GG:
Ai →βBi →βαi1Bi →βαi1αi2Bi →βαi1αi2αi3
Notice that other productions may distort derivations presented here. Never-
theless, the altered derivations are equivalent to the same derivation trees; cf. Re-
mark 3.1.
Figure 3.3: Invariability of unlooping method of a context-free grammar.

56
|
3 Context-free grammars
Example 3.9. Transform the given context-free grammar to Greibach normal form
G = ({S, A, B}, {a, b}, P, S)
P:
S →AB
A →BS | a
B →SA | b
Solution. Assume the order S, A, B of nonterminals. Notice that all productions except
B →SA satisfy the (∗) condition of Proposition 3.6 Let us transform the grammar G to
the Greibach normal form according to Proposition 3.6:
–
the method 3 (a) produces the following set of productions:
P′:
S →AB
A →BS | a
B →ABA | b
–
the method 3 (a) applied again produces the following set of productions:
P′′:
S →AB
A →BS|a
B →BSBA|aBA|b
–
removing looped B-productions according to point 3 (b) gives rise to the following
set of productions with an extra nonterminal B:
P′′′:
S →AB
A →BS | a
B →aBA | b | aBAB | bB
B →SBA | SBAB
–
now nonterminals are ordered as follows: B, S, A, B. The backward process of point
5 brings Greibach normal form of the grammar G:
GG = ({S, A, B, B}, {a, b}, PG, S)
PG:
B →aBA | b | aBAB | bB
A →aBAS | bS | aBABS | bBS | a
S →aBASB | bSB | aBABSB | bBSB | aB
B →aBASBBA | bSBBA | aBABSBBA | bBSBBA | aBBA|
aBASBBAB | bSBBAB | aBABSBBAB | bBSBBAB | aBBAB
Example 3.10. Transform to Greibach normal form the given context-free grammar of
Example 3.6:
G = ({E, El, T, Tl, P}, {+, ∗, id}, P, E)

3.4 Pumping and Ogden lemmas
|
57
P:
E →TEl | T
El →+TEl | +T
T →PTl | P
Tl →∗PTl | ∗P
P →id
Solution. Transformation of this grammar to Chomsky normal form will make the
process more complex. So, we give up on the transformation of the grammar to the
Chomsky normal form. Let order nonterminals as follows: E, El, T, Tl, P. Notice that all
productions satisfy the (∗) condition in Proposition 3.6. So then, employing the back-
ward process of point 5 of Proposition 3.6 we get the following set of productions in
the Greibach normal form:
P′:
E →id TEl | id El | id Tl | id
El →+TEl | +T
T →id Tl | id
Tl →∗PTl | ∗P
P →id
Finally, a grammar in Greibach normal form is called simple, if for any nontermi-
nal symbol A ∈V and for any terminal symbol a ∈T, there is at most one production
T →a α, that is, there is at most one A-production with right-hand side beginning
with given symbol a. This condition is called the Greibach uniqueness condition.
3.4 Pumping and Ogden lemmas
The pumping lemma for context-free languages and the Ogden lemma characterize
the structure of words. Both lemmas are essential tools used in the identification of
context-free languages.
3.4.1 The pumping lemma
The pumping lemma formulates conditions necessary for a language to be context-
free. It shows that the nature of context-free languages is finite and the length of words
of a given context-free language is limited by some constant nL, whose value is deter-
mined in the pumping lemma. If a context-free language includes a word of length
greater than or equal to nL, then it is an infinite language. However, a structure of
words that are longer than or equal to nL, is fairly simple. Such words are generated
by inserting strings of a length limited by the constant nL into words of the language
that are shorter than nL. We can also say that any word of a context-free language, not
shorter than nL, have two floating parts that can be deleted simultaneously, leaving
the remaining part in the language (let us recall that words of regular languages have
only one part that can be subjected to deletion).

58
|
3 Context-free grammars
Lemma 3.1 (The pumping lemma for context-free languages).
If
a language L is context-free
then
there exists a constant nL such that for any word z ∈L the following condition
holds:
(|z| ≥nL) ⇒[(
⋁
u,v,w,x,y
z = uvwxy ∧|vwx| ≤nL ∧|vx| ≥1)
⋀
i=0,1,2,...
zi = uviwxiy ∈L]
Proof. If a language L is finite, then a constant nL greater than the length of the longest
word of this language satisfies the lemma.
Consider the case of infinite languages. Let us assume that a context-free grammar
G = (V, T, P, S) in Chomsky normal form generates the language L. Derivation trees in
such a grammar are binary trees. We use the property that the height (length of the
longest path from the root to a leaf) of any binary tree with k leaves is not less than
⌈log2 k⌉. All vertexes of such a path, except the last one, are labeled by nonterminal
symbols of the grammar. The last vertex of this path is a leaf of the tree and is labeled
by a terminal symbol.
Let |V| = N. If we set the constant nL = 2N +1, then the height of a derivation tree of
a word z not shorter than nL is not less than N + 1. Therefore, there exists a path from
the root S to a leaf not shorter than N + 1. Consequently, N + 1 vertexes of this path
are labeled by N nonterminal symbols and the leaf is labeled by a terminal symbol a;
cf. Figure 3.4. As a result, there exist two (maybe more) vertexes labeled by the same
nonterminal symbol. Let us consider the pair of such vertexes closest to the leaf a that
are labeled by the same nonterminal symbol A. To distinguish labels of vertexes of this
pair, the nonterminal symbol A is denoted A′ and A′′, respectively. The crop z of the
derivation tree is divided into five parts: u, v, w, x and y. The part w is the crop of the
subtree with the root A′′ while vwx is the crop of the subtree with the root A′.
Figure 3.4: The derivation tree of a word z of length 2|V|+1.

3.4 Pumping and Ogden lemmas
|
59
If we replace the subtree with the root A′ by the subtree with the root A′′, we will get
a valid derivation tree with the crop z0 = uwy = uv0wx0y. This tree is shown at the
left part of Figure 3.5. If we replace the subtree with the root A′′ by the subtree with
the root A′, we will obtain the tree shown at the right part of Figure 3.5, which is still
a valid derivation tree with the crop z2 = uvvwxxy = uv2wx2y. Replacing the subtree
with the root A′′ in the last tree by the subtree with the root A′, we get a derivation
tree with the crop z3 = uvvvwxxxy = uv3wx3y. This iterative process can be contin-
ued.
Figure 3.5: The derivation trees obtained from the tree shown in Figure 3.4.
In this way, we show that the pumping lemma for context-free languages is satisfied.
To fulfill formal requirements, mathematical induction should be applied based on
the number of replacements of the subtree with the root A′′ by the subtree with the
root A′. Details of an inductive proof are left for the reader.
Since the pumping lemma formulates necessary conditions, it is of limited practi-
cal importance in its direct form. It can be employed to analyze the structure of words
of the language. If words satisfy the conclusion of the pumping lemma, then the lan-
guage could be intuitively presumed to be context-free. Then, based on such a sup-
position, the language could be formally proved to be context-free. In practice, we
use a contrapositive of the pumping lemma rather than its generic version, like in the
case of the pumping lemma for regular languages. The contrapositive of the pump-
ing lemma formulates sufficient conditions for a language not to be context-free. This
makes the contrapositive to be very useful in proving that specific languages are not
context-free.

60
|
3 Context-free grammars
Remark 3.2. Let us notice that the pumping lemma for regular languages is a special
case of the pumping lemma for context-free languages. The assumption is that uv = ε
turns the pumping lemma for context-free languages to the pumping lemma for regu-
lar languages.
Lemma 3.2 (Contrapositive of the pumping lemma for context-free languages).
If
for any constant nL, there exists a word z ∈L such that
(|z| ≥nL) ∧[(
⋀
u,v,w,x,y
z = uvwxy ∧|vwx| ≤nL ∧|vx| ≥1)
⋁
i∈{0,1,2,...}
zi = uviwxiy ∉L]
then
the language L is not context-free.
Example 3.11. Prove that the language L = {anbn+pcn+p+q : n, p, q ≥1} is not context-
free.
Proof. We apply contrapositive of the pumping lemma. Let us take the word
z = aNbN+1cN+2, where N is the constant from the lemma. Note that vwx cannot in-
clude three different symbols, that is, a, b and c in any split of z = uvwxy satisfying
assumption of the lemma. Such a split satisfies one of the following conditions that
are in pairs mutually exclusive:
–
either v or x includes two different symbols;
–
neither v, nor x includes the symbol a;
–
neither v, nor x includes the symbol b or c. Therefore, they may include only
as.
If a split satisfies the first condition, then z2 = uv2wx2y does not belong to the language
L because this word has more than three sequences of symbols. In contrast, any word
of the language L has exactly three sequences of symbols (namely, any word of the
language has a sequence of symbols a followed by a sequence of symbols b followed
by a sequence of symbols c).
If a split satisfies the second condition, then iterated parts v and x include either
bs, or cs, or both symbols. Taking z0 = uv0wx0y = uwy, we decrease either the number
of bs, or the number of cs, or the numbers of both symbols. Of course, the number of
as is unchanged. In the first and the third cases, there are no more bs than as in z0. In
the second case, there is no more cs than bs in z0. Therefore, z0 ∉L.
For the third condition, iterated parts v and x include as or bs, or both symbols.
Taking z2 = uv2wx2y = uwy, we increase the number of as. Of course, the numbers of
bs and cs are unchanged. In consequence, there is no more bs than as in z0. Finally,
z2 ∉L.
Finally, since premises of contrapositive of the pumping lemma are satisfied, we
conclude that the language is not context-free.

3.4 Pumping and Ogden lemmas
|
61
3.4.2 The Ogden lemma
The pumping lemma, its contrapositive, is a powerful tool used to prove that lan-
guages are not context-free. However, the contrapositive of the pumping lemma can
hardly be applied for some types of languages. In such complex cases, the Ogden
lemma may help in proving that a language is not context-free. The pumping lemma
is a particular case of the Ogden lemma. Nevertheless, the pumping lemma is easier
to be applied than the Ogden lemma. This is why the pumping lemma is used for more
straightforward problems.
Lemma 3.3 (The Ogden lemma).
If
a language L is context-free
then
there exists a constant nL such that for any word z ∈L and for at least nL symbols
marked in z there exists a split z = uvwxy satisfying the conditions:
– vx includes at least one marked symbol;
– vwx includes no more than nL marked symbols
and such that zi = uviwxiy ∈L for any i = 0, 1, 2, . . . .
Proof. Let us notice that the height of a derivation tree is, of course, not less than
⌈log2 nL⌉. This means that there exists a path from the root to a leaf not shorter than
⌈log2 nL⌉. Such a path starts in the root and then, for each its vertex, goes to this child,
which has no less marked leaves in its subtree than another one has. Having such a
path, we can do the same replacements in the derivation tree as we did in the proof of
the pumping lemma.
Notice that by marking all symbols of a word, we turn the Ogden lemma into the
pumping lemma. Thus, the pumping lemma is a special case of the Ogden lemma.
As in the case of the pumping lemma, the Ogden lemma formulates necessary
conditions for a language to be context-free. This is why the Ogden lemma in its direct
form is hardly applicable, like pumping lemmas. In practice, we use the contrapositive
of the Ogden lemma as well.
Lemma 3.4 (Contrapositive of the Ogden lemma).
If
for any constant nL there exists a word z ∈L with at least nL symbols marked
symbols marked such that for any split z = uvwxy holding conditions:
– vx includes at least one marked symbol;
– vwx includes no more than nL marked symbols
and such that there exists a constant i ∈{0, 1, 2, . . .} for which zi = uviwxiy ∉L
then
the language is not context-free.
Example 3.12. Prove that the following language is not context-free; cf. [1]:
L = {akblcm : k, l, m > 1 and m
̸= kr ∧m
̸= ls where r, s ∈{2, 3, 4, . . .}}.

62
|
3 Context-free grammars
Solution. The contrapositive of the pumping lemma is not applicable to this language
(or, at least, we do not know how to employ it). On the other hand, trials to find a
context-free grammar generating this language fail. This suggests (but does not prove)
that the language might not be context-free, despite that the pumping lemma is not
applicable for it. So then, let us apply the contrapositive of the Ogden lemma to prove
that this language is not context-free. The idea of this proof relies on iterating a se-
quence of symbols c in order to get their number to be a multiple of the number of
symbols a or the number of symbols b. We can do it since the Ogden lemma allows
to force symbols c to be included in iterated parts vx of the split uvwxy, while in the
pumping lemma, such a forcing is impossible.
Assuming that nL is the constant form contrapositive of the Ogden lemma, let us
take a prime number N such that it is greater than nL and greater than 3. In the word
z = aNbNc(N−1)!, all symbols c are marked. Since N is prime, then it is not a divisor of
(N −1)!. This is why z ∈L.
Let consider splits of the word z = uvwxy. Any iterated part (v or x) can include
(multiples of) three different symbols or (multiplies of) two different symbols or a
(multiple of) one symbol or is empty (though one of v and x must be a nonempty word).
We can have the following types of splits:
1.
an iterated part (v or x) includes three or two different symbols, that is, a and b
and c or a and c or b and c. Notice that only symbols c are marked, then they must
appear in at least one repeated part;
2.
iterated parts include only symbols c;
3.
the iterated part v contains only symbols a and the iterated part x contains only
symbols c. Notice that the case that two different symbols appear in a repeated
part is considered in the first point. Notice also that since symbols c must appear,
they must be in x;
4.
the iterated part v contains only symbols b and the iterated part x contains only
symbols c.
For any type of splits, there exists an iteration coefficient i, which takes the word zi =
uviwxiy out of the language L. Following iterations take the iterated word out of the
language L for splits listed above:
1.
the word z2 = uv2wx2y ∉L because symbol b precedes symbols a or symbol b
precede symbols c;
2.
let observe that iterated parts vx includes q symbol c, where 1 ≤q ≤nL. We look
for such a coefficient i that zi = uviwxiy ∉L. Notice that zi = aNbNc(N−1)!+(i−1)q,
what means that we wish N dividing (N −1)! + (i −1)q, that is, we look for such
a factor r that rN = (N −1)! + (i −1)q. A simple conversion gives the value of the
iteration coefficient equal to
i = rN −(N −1)!
q
+ 1

3.5 Context-free language membership
|
63
and for r = (N −1)! we get
i = (N −1)(N −1)!
q
+ 1
Since 1 ≤q ≤nL < N, then q divides (N −1)!, so then the last fraction is a positive
integer number, which provides an iteration coefficient i excluding zi from the
language L.
3.
iterations do not change the number of symbols b. So then, the value of the iter-
ation coefficient, as computed in case 2, gives the number of symbols c to be the
multiple of the number of symbols b;
4.
the same as in the case 3, but with regard to symbols c and a.
Finally, based on the contrapositive of the Ogden lemma, since for any split we can
take the iterated word out of the language, this language is not context-free.
3.5 Context-free language membership
The central question is how to check if a word belongs to a language. Having context-
free grammar, we can answer the question if a word is generated in this grammar.
Moreover, we will be able to build a derivation of a given the word if it is generated in
the grammar.
First of all, let assume that grammar is in Greibach normal form. Note that any
production applied to a derivation adds a single terminal symbol. This means that any
derivation of a word of length n has length n (i. e., productions are applied n-times in
a derivation). A method of building a derivation is simple. Of course, we start with the
initial symbols S of the grammar and apply a S-production with the right-hand side
beginning with the first symbol of the word. In the next steps, we take the leftmost
nonterminal symbol A of an intermediate derivation word and the next consecutive
symbol a of the word and apply an A-production beginning with the terminal sym-
bol a.
If a grammar satisfies Greibach uniqueness condition, that is, for every nontermi-
nal symbol A and for every terminal symbol a, there is at most one A-production with
the right-hand side beginning with a; then there is no ambiguity in the choice of pro-
ductions. Otherwise, when the grammar does not satisfy the uniqueness condition,
there is a question of how to choose a production. We either can make a nondetermin-
istic choice between all A-production with the right-hand side starting with a given ter-
minal symbol, or can check if any possible derivation produces the given the word w.
In case of checking all possible derivations, assuming that we have no more than k
productions for the choice, we may have up to kn derivations, where the length of the
word w is equal to n. This means that the computational complexity of this method is

64
|
3 Context-free grammars
exponential, which makes it useless in practice. Note: a concept of nondeterminism
will be discussed in further parts of the book.
There are more algorithms for membership tests, many of them being appli-
cable to special forms of context-free grammars. We discuss here an algorithm in-
vented by J. Cocke, H. Younger and T. Kasami. The algorithm is called the Cocke–
Younger–Kasami algorithm or the CYK algorithm for short. The CYK algorithm oper-
ates on context-free grammar in Chomsky normal form.
The way of determining whether a word w of length n is generated in a grammar
G in Chomsky normal form is outlined as follows:
1.
split the word w into n substrings of length one (each symbol of the word w makes
up a substring of length one, in this case) and find out nonterminal symbols gen-
erating each substring. This operation is a simple lookup for productions of the
form A →a, where a is a given substring of length one;
2.
having nonterminal symbols generating substrings of the word w not longer
than k, we can find nonterminals generating substrings of length k + 1 as follows:
a.
split a substring of length k + 1 to all possible pairs of substrings (i. e., pre-
fixes of lengths 1, 2, 3, . . . , k and the corresponding suffixes of length k, k −1,
k −2, . . . , 2, 1);
b.
for every pair of a prefix and the corresponding suffix sets of nonterminal sym-
bols generating them have already been determined;
c.
find out the set of all nonterminals A such that there is a production A →BC,
where B and C are nonterminals generating the prefix and the corresponding
suffix;
d.
nonterminals found out in the point c generate the given string of length k + 1
and no other nonterminal does;
3.
finally, we get the set of all nonterminal symbols generating the substring of
length n, that is, generating the word w. The word w is generated in the grammar
if and only if the initial symbol of the grammar is in this set.
Assuming that w = a1a2 . . . an is an analyzed word and Vj
i is the set of all nonterminals
generating the substring aiai+1 . . . ai+j−1 of the word w, for j = 1, 2, . . . , n, i = 1, 2, . . . ,
n −j + 1, the Cocke–Younger–Kasami algorithm can be formulated as follows:
begin
1. find out all sets V1
i of nonterminal symbols
generating the symbol ai of the word w
2. for consecutive length values j = 2, 3, . . . , n
of substrings of the word w do
3.
for k = 1, 2, . . . , n −(j −1) do
begin
4.
initialize sets Vj
k to the empty set
and take the substring ak . . . ak+(j−1)

3.5 Context-free language membership
|
65
5.
for splits of ak . . . ak+(j−1) to prefix ak . . . ak+l−j)
and suffix ak+l . . . ak+(j−1), l = 1, 2 . . . j −1 do
begin
6.
find out all productions A →BC
s.t. B ∈Vl
k and C ∈Vj−l
k+l
7.
include all such A′s into the set Vj
k
end
end
8. the word w is generated in the grammar
if and only if the initial symbol of the grammar
is included into the set Vn
1
Example 3.13. Verify if the word w
=
abcab is generated in the grammar
G = ({S, A, B, C, D}, {a, b, c, d}, P, S) with productions:
P:
S →AB | CD | DB
A →BC | a
B →CD | b
C →AA | DC | c
D →AB | d
Solution. The result of the CYK algorithm is shown in Table 3.1. Entries of the table are
filled in with sets of nonterminal symbols Vj
k. The word is generated in the grammar
since the set V5
1 (bottom right entry of the table) includes the initial symbols of the
grammar.
Table 3.1: Results of computation of the CYK algorithm for the word abcab in Example 3.13.
a
b
c
a
b
V1
k
A
B
C
A
B
V2
k
S, D
A
0
S, D
V3
k
C
C
S, B
V4
k
0
0
V5
k
S, B
Let us analyze the computational complexity of the CYK algorithm. Observe that costs
of the following operations are upper-bounded by constants:
–
initialization of sets V1
i in operation 1;
–
getting a substring in operation 3;
–
initialization of sets Vj
k to empty sets in operation 4;

66
|
3 Context-free grammars
–
splitting a string into a prefix and a suffix in operation 5;
–
finding out productions in operation 6 (which requires checking all production of
the grammar and since the number of production is fixed, then cost of this oper-
ation is bounded by a constant);
–
including left-hand sides A of productions into sets in operation 7.
Finding out productions in operation 6 is a dominant operation of this algorithm. The
number of executions of this operation is equal to
n
∑
j=2
n−j+1
∑
k=1
(j −1) =
n
∑
j=2
(n −j + 1)(j −1)
= −
n
∑
j=2
j2 + (n + 2)
n
∑
j=2
j −(n −1)(n + 1)
= −(n3
3 + n2
2 + n
6 −1) + (n + 2)(n + 2)(n −1)
2
−n2 + 1 = n3 −n
6
what means that the complexity of the CYK algorithm is of the range Θ(n3) with regard
to the length of an analyzed word.
The basic version of the CYK algorithm is used to find out if a word is generated in
the grammar, but it does not allow for finding a derivation tree. A modified version of
the CYK algorithm, with the extended version of operations 6 and 7, gathers informa-
tion necessary for building derivation trees:
6.
find out all productions A →BC
s.t. B ∈Vl
k and C ∈Vj−l
k+l
7.
include all such A′s into the set Vj
k
and include all such A′s into the set Vj
k,
7a. store right-hand side BC of the production A →BC
and parameter l in the set A→attached to A
(left-hand side of the production)
The above operation 7a attaches the right-hand side of every A-production determined
by operation 6 to the nonterminal symbol A. It means that every nonterminal symbol
A in every set Vj
k has some associated set A→of the right-hand side of an A-production
generating the corresponding substring of the word w. The following algorithm gener-
ates a derivation tree based on the results of the extended CYK algorithm. Parameters
of the function generate (generate a subtree of the derivation tree) denote k – the po-
sition in the word of the first symbol of the substring, j – length of the substring, A –
the nonterminal generating the substring, here – the position of the nonterminal A in
the tree.

3.5 Context-free language membership
|
67
generate(k,j,A,here)
begin
if j=1 then
begin
put the symbol A at the place here,
draw an edge from A down to the terminal symbol
end
else
begin
newTree:=false
/*currently built tree is the current copy*/
for every BC, l ∈A→do
begin
if newTree then
begin
create a copy of the derivation tree
built before the current call
of this function, make newly created copy
to be the current copy,
end
apply subsequent operations to the current copy
put the symbol A at the place here
draw a left edge to a leftVertex
call generate(k,l,B, leftVertex)
draw a right edge to a rightVertex
call generate(k+l,j-l,C, rightVertex)
newTree := true
end
end
end
The first call of the function generate requests building the whole derivation tree for
an investigated word w of length n and is as follows:
generate(1,n,S,position-of-the-root).
Example 3.14. Find derivation trees for Example 3.13.
Solution. Results of computation of the extended CYK algorithms are displayed in
Table 3.2. Sets A→are displayed as subscripts of nonterminals symbols of sets Vj
k. The
word w has three derivation trees shown in Figure 3.6, that is, derivation of this word
is ambiguous.

68
|
3 Context-free grammars
Table 3.2: Results of computation of the extended CYK algorithm for the word abcab in Example 3.13.
a
b
c
a
b
V1
k
A{a}
B{b}
C{c}
A{a}
B{b}
V2
k
S{AB,1}, D{BC,1}
A{BC,1}
0
S{AB,1}, D{AB,1}
V3
k
C{AA,1,DC,2}
C{AA,2}
S{CD,1}, B{CD,1}
V4
k
0
0
V5
k
S{CD,3,DB,2}, B{CD,3}
Figure 3.6: The derivation trees produced by the extended CYK algorithm.
The first and the second derivation trees are built on the basis of the production S →
CD applied to the initial symbol of the grammar, which is the root of the derivation
tree. The third derivation tree is built on the production S →DB applied to the initial
symbols of the grammar. The difference between the first and the second derivation
trees stems from the ambiguity of the left nonterminal C derived from the initial sym-
bol of the grammar; cf. left subtrees of the first and the second trees.

3.6 Applications
|
69
3.6 Applications
This section is devoted to selected applications of context-free grammars. The section
is a roadside of the main discussion on formal languages, automata and computabil-
ity. Topics included in this section are a small part of a compiler’s practice. They can
be used in an elementary project on parsing basic constructions like arithmetical ex-
pressions, which are among the most complex parts of programming languages. This
section is not aimed at a complete and detailed presentation of parsing. It is rather a
signalization of the theme.
3.6.1 Translation grammars
This section is focused on some modifications of context-free grammars. Despite
that modifications presented here are not included in the main flow of discussion
on context-free grammars, the associated practical importance makes them valuable
and justifies their presentation in the book. Two extensions of context-free grammars
are presented, namely translation grammars and LL(1) grammars. These types of
grammars will be used to parse arithmetic expressions and translate them to postfix
form.
Translation grammars stem from context-free grammars. They could be seen as
context-free grammars with the simultaneous derivation of related words.
Definition 3.6. Translation grammar is a context-free grammar G = (V, T, P, S) with
the set T of terminal symbols split into two disjoint subsets T′ and T′′ of primary and
secondary symbols, that is, T = T′ ∪T′′, T′ ∩T′′ = 0.
A translation grammar is a context-free grammar producing a context-free lan-
guage L(G) over the alphabet T of terminal symbols. On the other hand, we can say that
the translation grammar produces two languages: the primary language L′(G) and the
translation language L′′(G). The primary language is obtained from the language L(G)
by removing translation symbols from its words. Then again, translation language is
obtained from the language L(G) by removing primary symbols from its words.
Example 3.15. The following grammar generates nonempty words over the alphabet
Σ = {a, b} having the same numbers of symbols a and b; cf. Problem 3.1.
G = ({S, A, B}, {a, b}, {S →aB | bA, A →a | aS | bAA, B →b | bS | aBB}, S)
Solution. The following translation grammar GT generates the primary language
L′(G) equal to L(G) and the translation language L′′(G), which has words with sym-
bols a shifted to the left:
GT = ({S, A, B}, {a, b, a, b})

70
|
3 Context-free grammars
where we have the following set P(T) of productions:
PT:
S →aaB | bAb
A →aa | aaS | bAAb
B →bb | bSb | aaBB
The set of terminal symbols {a, b, a, b} has two primary symbols {a, b} and the set
of translation symbols {a, b}. Productions of the grammar GT directly correspond to
the productions of the grammar G.
Let us consider the word w = baba. Its derivation tree (in the grammar G) is shown
in the left part of Figure 3.7. The corresponding derivation tree in the grammar GT is
shown in the right part of Figure 3.7. Its crop is equal to wT = baabaabb and is split to
the primary word w′ = baba and the secondary word w′′ = aabb.
Figure 3.7: The derivation trees obtained in a translation grammar.
This example can be interpreted as follows. For the given grammar G and a word w, we
can find its derivation in the grammar G. Then we design the corresponding deriva-
tion in the grammar GT. The generated word wT is split to the primary w′, which is
equal to the word w, and the secondary w′′, which is the translation of its primary
counterpart w′.
3.6.2 LL(1) grammars
In this section, we assume that, for a context-free grammar G = (V, T, P, S), any word
w ∈T∗has a special end-of-word symbol ◁appended. This symbol is neither a nonter-
minal symbol nor a terminal symbol. It is used for marking the end of any intermediate
and terminal word of a derivation of the word w.

3.6 Applications
|
71
Assume that p : A →α is a production in a context-free grammar G = (V, T, P, S),
where A ∈V, α ∈{V ∪T}∗. Let us define the following sets of symbols:
–
FIRST(p) = FIRST(α) is the set of those terminal symbols, which may open any
intermediate word derivable from α. Note that if the right-hand side α of the pro-
duction p begins with a terminal symbol a, then FIRST(p) = FIRST(α) = {a}. On
the other hand, FIRST(1) = FIRST(L) = {a} for the production 1 : S →L of the
grammar of Example 3.3;
–
FOLLOW(p) = FOLLOW(A) is the set of those terminal symbols and end-of-word
symbol, which may directly follow A in any intermediate word derived from the
beginning symbols S of the grammar G. Note that FOLLOW(3) = FOLLOW(L) =
{c, ◁} for the production 3 : L →Lc of the grammar of Example 3.3;
–
SELECT(p) = { FIRST(p) ∪FOLLOW(p)
if p is nullable
FIRST(p)
otherwise
Definition 3.7. A context-free grammar G = (V, T, P, S) is LL(1) grammar if and only
if for every nonterminal symbol A ∈V all A-productions have SELECT sets pairwise
disjoint. This condition is called LL(1) uniqueness condition.
LL(1) grammars are tools for building a top-down membership analyzer (top-
down parser). LL(1) grammars are tools for designing Leftmost derivation of the input
word, processing the word from Left to right. The derivation is designed based on 1
input symbol at a time.
Note that the uniqueness condition of the LL(1) grammars is similar to the
Greibach uniqueness condition. This allows for an easy designing of the derivation of
a word: for a given leftmost nonterminal symbol A ∈V in an intermediate derivation
word and a given input symbol a ∈T, we apply this A-production to the nontermi-
nal A, which has the terminal a in its SELECT set. When the right-hand side of the
applied production begins with the terminal symbol a, the input is shifted to the
next input symbol. Translation symbols in intermediate derivation words are skipped
during this processing.
Example 3.16. Let us consider the following grammar generating arithmetic expres-
sions with addition, subtraction, multiplication, division, power, change of sign
(negation) and brackets:
G = ({E, El, T, Tl, P, Pl, Q}, {+, −, ∗, /, ↑, (, ), id}, P, E)
The set of productions is given in Table 3.3. Productions are supplemented with FIRST,
FOLLOW and SELECT sets (FOLLOW sets are needed only for nullable productions).
SELECT sets show that the grammar is LL(1) one.

72
|
3 Context-free grammars
Table 3.3: A LL(1) grammar generating arithmetic expressions.
No.
Production
FIRST
FOLLOW
SELECT
1.
E →TEl
{−, (, id}
{−, (, id}
2.
El →+TEl
{+}
{+}
3.
El →−TEl
{−}
{−}
4.
El →ε
0
{), ⊲}
{), ⊲}
5.
T →PTl
{−, (, id}
{−, (, id}
6.
Tl →∗PTl
{∗}
{∗}
7.
Tl →/PTl
{/}
{/}
8.
Tl →ε
0
{+, −, ), ⊲}
{+, −, ), ⊲}
9.
P →QPl
{−, (, id}
{−, (, id}
10.
Pl →↑QPl
↑
{↑}
11.
Pl →ε
0
{+, −, ∗, /, ), ⊲}
{+, −, ∗, /, ), ⊲}
12.
Q →−Q
{−}
{−}
13.
Q →(Q)
{(}
{(}
14.
Q →id
{id}
{id}
Example 3.17. Below we present a translation grammar, which converts arithmetic ex-
pressions to a postfix form (Reverse Polish Notation). The translation grammar stems
from the grammar G of Example 3.16. The set of terminal symbols of the grammar is
supplemented with translation symbols T′′ = {+, −, =, ∗, /, ↑, id}. Productions are sup-
plemented with translation symbols as shown in Table 3.4. The uniqueness property
of the LL(1) grammar is inherited from the property of the original grammar of Exam-
ple 3.16. Since this property is associated with the primary language, so then FIRST,
FOLLOW and SELECT sets are unaffected and stay the same as in the grammar of Ex-
ample 3.16.
Table 3.4: A LL(1) grammar generating arithmetic expressions and their conversion to postfix form.
No.
Production
FIRST
FOLLOW
SELECT
1.
E →TEl
{−, (, id}
{−, (, id}
2.
El →+T + El
{+}
{+}
3.
El →−T −El
{−}
{−}
4.
El →ε
0
{), ⊲}
{), ⊲}
5.
T →PTl
{−, (, id}
{−, (, id}
6.
Tl →∗P ∗Tl
{∗}
{∗}
7.
Tl →/P / Tl
{/}
{/}
8.
Tl →ε
0
{+, −, ), ⊲}
{+, −, ), ⊲}
9.
P →QPl
{−, (, id}
{−, (, id}
10.
Pl →↑QPl ↑
↑
{↑}
11.
Pl →ε
0
{+, −, ∗, /, ), ⊲}
{+, −, ∗, /, ), ⊲}
12.
Q →−Q ≡
{−}
{−}
13.
Q →(Q)
{(}
{(}
14.
Q →id id
{id}
{id}

3.6 Applications
|
73
The set of productions can be split into several groups concerning their functions:
–
productions 1–4 generate additive operations (addition, subtraction);
–
productions 5–8 generate multiplicative operations (multiplication, division);
–
productions 9–11 generate power operation;
–
production 12 generates change of sign operation;
–
production 13 generates brackets;
–
production 14 generates arguments of operations (numbers, variables) repre-
sented by the terminal symbol id.
This split of operations reflects the precedence of operators. The following hierar-
chy is utilized when an expression is evaluated:
–
additive operators (addition and subtraction) have the lowest priority;
–
multiplicative operators (multiplication and division) have higher priority than
additive operators do;
–
the power operator has higher priority than multiplicative operators do;
–
the change of sign operators has the higher priority than the power operator
does;
–
brackets give the highest priority for included subexpression;
–
operators of the same priority are evaluated:
–
left to right in case of additive operators;
–
left to right in case of multiplicative operators;
–
right to left in case the power and the change of sign operator.
This hierarchy is preserved in the analysis of arithmetic expressions and their conver-
sion to other forms.
Analysis of nest three arithmetic expressions employs the grammar with produc-
tions outlined in Table 3.4 with production no 14 replaced with the following ones.
No.
Production
FIRST
FOLLOW
SELECT
14.
Q →a a
{a}
{a}
Q →b b
{b}
{b}
Q →c c
{c}
{c}
Q →d d
{d}
{d}
The derivation of the expression −a + b ∗c −d is designed as follows:
–
we start from the initial symbol E of the grammar of Example 3.17;
–
there is only one E-production to be applied to the input symbol and the initial
symbol of the grammar E: E
1→TEl;
–
there is only one T-production to be applied to the input symbol and the leftmost
nonterminal T: E
1→TEl
5→PTlEl;

74
|
3 Context-free grammars
–
there is only one P-production to be applied to the input and to the leftmost non-
terminal P: E
1→TEl
5→PTlEl
9→QPlTlEl;
–
there are three Q-productions to be applied to the input symbol and the leftmost
nonterminal Q, but SELECT set of only one Q-production includes the input sym-
bol and this production is applied: E
∗→QPlTlEl
12→−Q = PlTlEl
–
since in the previous step the input symbol has been added to the intermediate
derivation word by the production no 12, now input is shifted to the symbol a and
there are three Q-productions to apply to the input symbol and the leftmost non-
terminal Q, but SELECT set of only one Q-production includes the input symbol a
and this production is applied: E
∗→−Q = PlTlEl
14
→−a = PlTlEl;
–
now input is shifted to the symbol +, the translation symbol = in the intermediate
word of derivation is skipped, so then there are three Pl-productions to apply to
the input symbol + and the leftmost nonterminal Pl, but SELECT set of only one
Pl -production includes the input symbol + and this production is applied: E
∗→
−a = PlTlEl
11→−a = TlEl;
–
there are three Tl-productions to apply to the input symbol + and the leftmost non-
terminal Tl, but SELECT set of only one Tl -production includes the input symbol
+ and this production is applied: E
∗→−a = PlTlEl
11→−a = TlEl
8→−a = El;
–
there are three El-productions to apply to the input symbol + and the leftmost non-
terminal El, but SELECT set of only one El -production includes the input symbol
+ and this production is applied: E
∗→−a = TlEl
8→−a = El
2→−a = + T ∓El;
–
now input is shifted to the symbol b and there is only one T-productions to apply to
the input symbol and the leftmost nonterminal T: E
∗→−a = El
2→−a = +T + El
5→
−a = + PTl ∓El;
–
etc.
The derivation tree equivalent for the above derivation is shown in Figure 3.8. The
primary word of this derivation tree is - of course - w′ = w = −a + b ∗c −d. The
secondary (translated) word is equal to w′′ = a = b c ∗+ d −. Note that ordinary
symbols are marked with an upper dash in secondary word. Moreover, subtract oper-
ator and change of sign operator are distinguished in the secondary word. The former
one is shown in the form of an overlined minus sign −and the latter is shown as an
overlined equality sign =. A distinction between these two operators is required since
their meaning cannot be drawn from the context of expression in postfix form. Unlike,
expressions in traditional form (viz., infix form) allow for differentiating meaning of
the minus sign, which stands for both operators: subtraction and changing of the sign.
Two additive operators are utilized in order from the left one to the right one, that is,
they are left-to-right associative.
Unlike in the infix form, arithmetic expressions in postfix form do not need brack-
ets to change priorities of operators. This property is illustrated in Figure 3.9. The pri-

3.6 Applications
|
75
Figure 3.8: The derivation tree of the expression −a + b ∗c −d in the LL(1) translation grammar of
Example 3.17 and the translated expression a = b c ∗+ d −.
Figure 3.9: The derivation tree of the expression −(a + b) ∗c −d in the LL(1) translation grammar of
Example 3.17 and the translated expression a b + = c ∗d −.

76
|
3 Context-free grammars
mary word of this derivation tree is w′ = w = −(a+b)∗c−d. The secondary (translated)
word is equal to w′′ = a b + = c ∗d −.
In Figure 3.10, the right to left associativity of power and change of sign operators
is illustrated. The primary word of this derivation tree is w′ = w = −−a ↑−−b ↑c.
The secondary (translated) word is equal to w′′ = a = = b = = c ↑↑.
Figure 3.10: The derivation tree of the expression −−a ↑−−b ↑c in the LL(1) translation grammar of
Example 3.17 and the translated expression a = = b = = c ↑↑.
3.7 Problems
Problem 3.1. Design context-free grammars generating the following languages:
1.
the set of palindromes over the alphabet {a, b};
2.
the set of correct sequences of parentheses;
3.
the set of nested brackets (brackets, square brackets, round brackets);
4.
the set of nonempty binary words having the same numbers of as and bs;
5.
the set of binary words having twice as much as than bs;
6.
the set of regular expressions over the alphabet {a, b}.
Solution. 4. Compare also solution of Example 3.4. The following formula L = {w ∈
{a, b}∗: #aw = #bw > 0} defines the language. The following inquiry leads to a solu-
tion. The initial symbol S of the grammar generates words having the same numbers
of as and bs, that is, S →∗w. In a word w = a a2 . . . an ∈L, which begins with a, the
string a2 . . . an has the number of bs exceeding the number of as by 1. Assume that
strings with the number of bs exceed the number of as by 1 are generated by the non-
terminal B. By analogy, consider the case w = ba2 . . . an ∈L and assume that strings

3.7 Problems
|
77
with the number of as exceeds the number of bs by 1 are generated by the nontermi-
nal A. So, the following productions S →aB | bA could be employed.
Now, consider strings u = a a2 a3 . . . an generated by the nonterminal A, that is,
A →u. In this case, the string a2 a3 . . . an has the same number of as and bs. On the
other hand, in a string u = ba2 a3 . . . an generated by the nonterminal A, the number of
as exceeds the number of bs by 2. These cases employ productions A →a | aS | bAA.
Likewise, productions B →b | bS | aBB should be considered.
Finally, the above intuitive investigation brings the following grammar G
=
({S, A, B}, {a, b}, P, S) with the following set P of productions:
P:
S →aB | bA
A →a | aS | bAA
B →b | bS | aBB
A proof that this grammar generates the language L can use mathematical induc-
tion concerning words with the same number of as and bs, with the number of as
exceeding the number of bs by 1 and with the number of bs exceeding the number of
as by 1. We claim that all such words and only such words are generated by nontermi-
nals S, A, B, respectively. Notice that words with the same number of as and bs have
even length, that is, length equal to 2n for n = 1, 2, 3, . . .. Similarly, length of words of
other two types is odd, that is, is equal to 2n −1 for n = 1, 2, 3, . . ..
For n = 1, we have the following words of the above three types: a, b, ab, ba. These
words are derivable from corresponding nonterminals as follows: A →a, B →b,
S →aB →ab and S →bA →ba. Of course, no other word of the assumed lengths
could be derived.
Based on inductive hypothesis, we assume that for a given n:
–
for all k = 1, 2, . . . , n, all words of length 2k −1 having k symbols a and k −1 sym-
bols b, and only such words, are derivable from A;
–
for all k = 1, 2, . . . , n, all words of length 2k −1 having k −1 symbols a and k sym-
bols b, and only such words, are derivable from B;
–
all words of length 2k having k symbols a and k symbols b, and only such words,
are derivable from S.
Doing inductive step, we get:
–
any word of length 2n + 1 having n + 1 symbols a and n symbols b is derivable
from A. Such a word
–
either begins with a and has n (equal numbers) of as and bs in the remaining
part. So, based on the inductive assumption, we get that the production A →
aS allows to generate it;
–
or it begins with b and has n+1 symbols a and n−1 symbols b in the remaining
part. The remaining part can be split into two parts, in which the number
of as exceeds the number of bs by 1 (the reader can employ the function μ

78
|
3 Context-free grammars
used in Example 3.4 to justify such a split). Hence, based on the inductive
assumption, the production A →bAA allows to generate it.
Based on the inductive assumption, we can see that there is no way to derive from
A any word of length 2n + 1 with numbers of as and bs different than n + 1 and n;
–
likewise, we can show that any word of length 2n + 1 having n symbols a and n + 1
symbols b are derivable from B, and only such words;
–
any word of length 2n+2 having symbols a and n+1 symbols b is derivable from S.
Such a word
–
either begins with a and have n symbols a and n+1 symbols b in the remaining
part. Hence, the production S →aB allows to generate it;
–
or begins with b and have n + 1 symbols a and n symbols b in the remaining
part. In this cases the production S →bA is employed to generate it.
Based on the inductive assumption, we can see that only word of length 2n+1 with
equal numbers of as and bs can be derived form S.
The proof is complete.
5. The formula L = {w ∈{a, b}∗: #aw = 2 ∗#bw > 0} defines the language. The
reader can prove that the following grammar generates it: G = ({S, A, B}{a, b}, P, S) with
the following set P of productions:
P:
S →aAB | aBA | bAA
A →a | aS | bAAA
B →b | bS | aaBB | abABB | abBAB | abBBA
Hint. Employ mathematical induction to prove correctness of the grammar, consider
the function μ(w) = #wa −2 ∗#wb.
Problem 3.2. Prove that the language L is context-free.
L = {w ∈{a, b, c}∗: w = akbkcl, k, l > 0}.
Solution. The following grammar generates this language: G = ({S, A, B}{a, b}, P, S)
with the following set P of productions:
P:
S →AC
A →aAb | ab
C →Cc | c
Problem 3.3. Prove that the language L is not context-free:
L = {w ∈{a, b, c}∗: w = akbkck, k > 0}.

3.7 Problems
|
79
Problem 3.4. Prove that the language L is not context-free:
L = {w ∈{a, b, c}∗: #aw = #bw = #cw > 0}
Solution. We apply the contrapositive of the pumping lemma to prove that L is not
context-free. Let N is a constant from the lemma and let z = aNbNcN ∈L. We consider
any split z = uvwxy satisfying premises of the lemma, that is, |vwx| ≤N and |vx| ≥1.
Thus, vwx cannot include three different symbols. As a result, vx is a string that in-
cludes at least one symbol of the alphabet and does not include at least one symbol,
that is, it is either a string of as or a string of bs or a string of cs, or it is a string of as
and bs or a string of bs and cs and it is never a string of as and bs and cs. In the word,
z0 = uv0wx0y number(s) of the symbol(s), which are present in vx, is decreased while
number(s) of the symbol(s), which are absent in vx is not changed comparing to the
word z. So then, z0 ∉L, and ends the proof.
Problem 3.5. Prove that the language L is context-free.
L = {wuwRv : u, v, w ∈{a, b}∗, |w| > 0}.
Problem 3.6. Prove that the language L = {ww : w ∈{a, b}∗} is not context-free.
Solution. Apply
contrapositive
of
the
pumping
lemma.
Consider
the
word
z = aNbNaNbN, where N is a natural number not less than the constant from the
lemma.
Problem 3.7. Prove that complement of the language considered in Problem 3.6, that
is, L = {a, b}∗−{ww : w ∈{a, b}∗}, is a context free one.
Solution. A context-free grammar generating the language L proves that this language
is context-free. Words of odd length are not of a form ww, so they belong to L. The
following productions generate such words:
P′
S →aR | bR
R →aaR | abR | baR | bbR | ε
Let us consider words of even length. Any word of L is of a form
w = a1 a2 . . . ai . . . an an+1 an+2 . . . an+i . . . an+n
where ai
̸= an+i for some 1 ≤i ≤n and no more restrictions are put on symbols of such
a word. This word can be rewritten as
ai−(i−1) ⋅⋅⋅ai−1ai ai+1+ ⋅⋅⋅ai+(i−1) a(n+i)−(n−i) ⋅⋅⋅a(n+i)−1an+i a(n+i)+1 ⋅⋅⋅a(n+i)+(n−i)
As shown, every indicated symbol has a left context and a right context, which
are any string of the same length for a given symbol.

80
|
3 Context-free grammars
Now, we can design a context-free grammar generating such words:
G = ({S, A, B, C}, {a, b}, P, S)
P′′:
S →AB | BA
A →a | CAC
B →b | CBC
C →a | b
Finally, joining both sets of productions P′ and P′′, we get productions of a
context-free grammar generating this language:
P:
S →AB | BA | aR | bR
A →a | CAC
B →b | CBC
C →a | b
R →aaR | abR | baR | bbR | ε
The detailed justification that the grammar G generates the language L is left to
the reader.
Also, the reader can prove that the language L is not regular.
Hint. Consider the Problem 2.5.
Problem 3.8. A language L is a context-free one. Is L′ context-free?
L′ = {a1 a2 a2 a3 a3 a3 a4 a4 a4 a4 . . . (an)n : a1 a2 a3 a4 . . . an ∈L}.
Solution. Let apply contrapositive of the pumping lemma. Let N is a natural num-
ber not less than the constant from the lemma such that there is a word z = a1 a2 . . .
aN ∈L. Take the word z′ = a1 a2 a2 a3 . . . (aN)N, which satisfies assumption of the
lemma. Notice that |z′| = N(N + 1)/2. Let z′ = uvwxy is a split holding the lemma as-
sumptions. A word z′
2 = uv2wx2y has length |z′
2| = N(N + 1)/2 + r such that 1 < r ≤N.
Therefore, N(N + 1)/2 < |z′| = N(N + 1)/2 + r ≤N(N + 1)/2 + N < (N + 1)(N + 2)/2 =
N(N + 1)/2 + N + 1. But words of such length do not belong to L′. This proves that L′ is
not context-free.

4 Context-sensitive grammars and unrestricted
grammars
The class of context-sensitive languages follows the class of context-free languages.
In the hierarchy of languages, a so-called Chomsky hierarchy, the next classes of lan-
guages, besides regular languages and context-free languages, are context-sensitive
and recursively enumerable languages. Context-sensitive languages are generated
by context-sensitive grammars, which happen to be a generalization of context-free
grammars. Recursively enumerable languages are generated by unrestricted gram-
mars, which are an extension of context-sensitive grammars. We will also distinguish
the class of recursive languages. However, a class of grammars generating recursive
languages is not known. The class of recursive languages is separated from the class
of recursively enumerable based on special class of automata. This topic will be pre-
sented in Chapter 5.
As mentioned above, context-sensitive and unrestricted grammars are much more
complex than context-free and regular grammars. Likewise, the structure of words
of these classes of languages is much more complex than the structure of words of
context-free languages. We neither know properties exhibiting restriction of words’
structure nor can we formulate any regularity rules of a language structure. Also, we
do know any property like, for instance, pumping lemmas, which would suggest a
character analogous to the finiteness of regular and context-free languages. Algebraic
characterization of these languages, as a substructure in the set of all words, is not
known. Therefore, we do not have effective tools for processing these languages, as it
is in cases of simpler classes of languages. Tools corresponding to pumping lemmas,
Myhill–Nerode theorem, CYK algorithm, etc., are not known for classes of context-
sensitive and recursively enumerable languages.
This chapter provides a short presentation of the basic properties of context-
sensitive and recursively enumerable languages.
4.1 Context-sensitive grammars
Productions of context-sensitive grammars satisfy monotonic condition (also called
noncontracting or nonerasing condition). For this reason, context-sensitive grammars
are also called monotonic, noncontracting or nonerasing grammars.
Definition 4.1. A grammar G = (V, T, P, S) is context-sensitive if and only if its produc-
tions are monotonic (or noncontracting), that is, they are of the form:
α →β,
where: α, β ∈(V ∪T)∗and 0 < |α| ≤|β|
where |w| denotes a length of the word w.
https://doi.org/10.1515/9783110752304-004

82
|
4 Context-sensitive grammars and unrestricted grammars
Definition 4.2. Context-sensitive languages are those generated by context-sensitive
grammars and only those.
The class of context-sensitive grammars and the class of context-sensitive lan-
guages are denoted by CSG and CSL, respectively.
The monotonic condition excludes the empty word ε from context-sensitive lan-
guages. In terms of the above definition, any language, which includes the empty
word, is not context-sensitive. This is the strict meaning of classes of CSL and CSG.
It would be unreasonable to exclude from the CSL class a context-sensitive lan-
guage with the empty word included. Thus, any language L, such that L −{ε} is a
context-sensitive language, will be included in the CSL class. Note that having a lan-
guage L generated by a monotonic grammar, we can add the empty word ε to L by
attaching the production S →ε to the grammar, where S is the initial symbol of
the grammar. This production breaks the monotonicity of the grammar, so it will be
considered to be the unique exception of context-sensitive grammar. This meaning of
context-sensitivity is called extensive context-sensitivity.
Summarizing the above notes: the classes of CSL and CSG will be considered in
the strict or extended sense depending on the context of the discussion. In the sequel,
we will not distinguish between strictness and extensiveness of context-sensitivity if
this does not lead to confusion.
Definition 4.3. A grammar G = (V, T, P, S) is in context-sensitive normal form if and
only if its productions have the following form:
γ A δ →γ α δ,
where A ∈V, α, γ, δ ∈(V ∪T)∗, α
̸= ε.
γ and δ are called left and right context, respectively, and A →α is called the core of
the production.
Note that a grammar in context-sensitive normal form is a context-sensitive gram-
mar since its productions are monotonic. Then the class of grammars in context-
sensitive normal form is included in the CSG class. The question is whether the CSG
class is equivalent to the class of grammars in context-sensitive normal form. This
question is equivalent to the question if, for any context-sensitive grammar, we can
find an equivalent grammar in context-sensitive normal form. The answer is affirma-
tive. Hence, grammars in context-sensitive normal form do not create a new class of
languages; they generate the CSL class.
Note that the core of a context-sensitive production in normal form is simply a
context-free production. Also observe, that only the core of such production can affect
derivation. However, the core of production can be used only if left and right contexts
are preserved. This is why grammars with monotonic productions are called context-
sensitive grammars.
Now, let us justify that every context-sensitive grammar can be transformed to
normal form.

4.1 Context-sensitive grammars
|
83
Lemma 4.1. Any context-sensitive grammar G = (V, T, P, S) can be transformed to an
equivalent grammar in normal form, that is, both grammars generate the same lan-
guage.
Proof. We build a context-sensitive grammar in the normal form, which is equivalent
to the grammar G. An idea of proof of equivalence of both grammars is illustrated in
Example 4.1. Below, an idea of such proof is outlined while details of proof is left to
the reader.
The grammar in a normal form equivalent to a given context-sensitive grammar is
designed as follows:
1.
for every terminal symbol a ∈T,
a.
create a new nonterminal symbol Aa and convert every production of the
grammar G replacing every occurrence of the symbol a with the new non-
terminal Aa;
b.
add the new production Aa →a.
We get a new grammar GT = (V ∪VT, T, P′ ∪PT, S) with an extended set V ∪VT of
nonterminal symbols and an extended set P′∪PT of productions, where VT = {Aa :
a ∈T}, P′ is the set of productions converted from P, PT = {Aa →a, a ∈T}. Pro-
ductions of P′ have now a form A1 A2 . . . Ak →B1 B2 . . . Bl, where k ≤l (since the
grammar G is a context-sensitive, i. e., it is a monotonic grammar) and all sym-
bols in the production are nonterminal symbols, that is, A1, . . . , Ak, B1, . . . , Bl ∈
V ∪VT;
2.
let us split the set P′ of productions to subsets P′
n (corresponding to the subset
Pn of P) of productions in normal form and P′
m (corresponding to the subset Pm
of P) of productions that are monotonic, but not in normal form, P = Pn ∪Pm and
P′ = P′
n ∪P′
m. Let us enumerate productions of the set P′
m, which are not in normal
form;
3.
for each production r : A1 A2 . . . Ak →B1 B2 . . . Bl in the set P′
m with assigned
number r do:
a.
create a new nonterminal symbols Ar
k;
b.
replace the production r : A1 A2 . . . Ak →B1 B2 . . . Bl with the following set Rr
of k + 1 productions in normal form:
–
A1 . . . Ak−1 Ak →A1 . . . Ak−1 Ar
k, where γ = A1 . . . Ak−1 is the left context, the
right context is empty and Ak →Ar
k is the core;
–
A1 A2 . . . Ak−1 Ar
k
→
B1 A2 . . . Ak−1 Ar
k, where the left context is empty,
δ = A2 A3 . . . Ak−1 Ar
k is the right context and A1 →B1 is the core;
–
B1 A2 A3 . . . Ak−1 Ar
k →B1 B2 A3 . . . Ak−1 Ar
k, where γ = A1 is the left context,
δ = A3 . . . Ak−1 Ar
k is the right context and A2 →B2 is the core;
–
. . .
–
B1 . . . Bk−2 Ak−1 Ar
k →B1 . . . Bk−2 Bk−1 Ar
k, where γ = B1 . . . Bk−2 is the left
context, δ = Ar
k is the right context and Ak−1 →Bk−1 is the core;

84
|
4 Context-sensitive grammars and unrestricted grammars
–
B1 . . . Bk−1 Ar
k →Ba . . . Bk−1 Bk . . . Bl, where γ = B1 . . . Bk−1 is the left con-
text, the right context is empty and Ar
k →Bk . . . Bl is the core;
4.
finally, we come up with the following context-sensitive grammar in normal form
GN = (VN, T, PN, S), where:
–
VN = V ∪VT ∪⋃r{Ar
kr} and r is the number of a production A1 A2 . . . Ak →
B1 B2 . . . Bl from P′
m;
–
PN = PT ∪Pn ∪⋃r Rr, where Rr is the set of productions in normal form
corresponding to the production r of Pm.
Example 4.1. Let us consider the context-sensitive grammar G = ({S}, {a, b, c}, P, S)
with the following productions:
P:
S →abcS | abc
(1) (2)
ab →ba
(3)
ac →ca
(4)
ba →ab
(5)
bc →cb
(6)
ca →ac
(7)
cb →bc
(8)
This is a context-sensitive grammar generating the set of words with the same
number of letters a, b and c, that is, the language L = {w : w ∈{a, b, c}∗and #aw =
#bw = #cw > 0}. The proof is straightforward: productions (1) and (2) insert any num-
ber of letters a, b and c (of course, the same number of letters a, b and c). Remaining
productions allow for migration of any letter to any place in the word, so any permu-
tation of letters generated by productions (1) and (2) is available.
The grammar G is not in the normal form. Let us transform it to the normal form
according to Lemma 4.1.
1.
replacement of terminal symbols by new nonterminals and extension of the set of
productions leads to the following grammar:
G′ = ({S, A, B, C}, {a, b, c}, P′ ∪PT, S)
P′:
S →ABCS | ABC
(1) (2)
AB →BA
(3)
AC →CA
(4)
BA →AB
(5)
BC →CB
(6)
CA →AC
(7)
CB →BC
(8)
PT:
A →a
B →b
C →c

4.1 Context-sensitive grammars
|
85
2.
for productions (3–8) of the grammar G′, which are not in normal form, create the
set P′ corresponding to the set P of original productions 3–8 of the grammar G (we
keep their numbers);
3.
the set P′ of productions is processed as follows:
a.
new nonterminals are created: B3, C4, A5, C6, A7, B8;
b.
every production of the set P′
m is replaced as follows:
–
the production 3: AB →BA is replaced by the following set of produc-
tions:
3N : AB →AB3, AB3 →BB3, BB3 →BA;
–
the production 4: AC →CA is replaced by the following set of produc-
tions:
4N : AC →AC4, AC4 →CC4, CC4 →CA;
–
etc.;
4.
finally, we build the following context-sensitive grammar in normal form GN =
(VN, T, PN, S), where:
–
VN = {S, A, B, C, B3, C4, A5, C6, A7, B8};
–
PN:
S →ABCS | ABC
(1) (2)
AB →AB3
(3)
AB3 →BB3
BB3 →BA
AC →AC4
(4)
AC4 →CC4
CC4 →CA
BA →BA5
(5)
BA5 →AA5
AA5 →AB
BC →BC6
(6)
BC6 →CC6
CC6 →CB
CA →CA7
(7)
CA7 →AA7
AA7 →AC
CB →CB8
(8)
CB8 →BB8
BB8 →BC
PT:
A →a
B →b
C →c
Both grammars G and GN are equivalent. To prove this equivalence, it is sufficient to
show that every word generated in one grammar is also generated in another one.

86
|
4 Context-sensitive grammars and unrestricted grammars
Having a derivation of a word w ∈{a, b, c}∗in the grammar G, we can turn it to a
derivation of the same word in the grammar GN:
–
every step of the derivation using the production number r, 1 ≤r ≤8, of the set
P is replaced by steps applying the corresponding set r of productions RN of the
grammar GN;
–
the new derivation generates the word W ∈{A, B, C}∗, which reflects the word w.
Now, applying productions of the set PT to nonterminal symbols A, B, C we get the
word w.
For example, the derivation in the grammar G:
S
1→abcS
3→bacS
4→bcaS
6→cbaS
2→cbaabc
could be transformed to the following derivation in the grammar GN:
S
1→ABCS
3N→AB3CS
3N→BB3CS
3N→BACS
4N
→BAC4S
4N
→BCC4S
4N
→BCAS
6N→BC6AS
6N→CC6AS
6N→CBAS
2→CBAABC →cBAABC →cbAABC
∗→cbaabc
In the above derivation, the number of production (or set of productions) is indi-
cated above the arrow. A part of the intermediate word used in a production is under-
lined.
On the other hand, a derivation in the grammar GN can be transformed to a deriva-
tion in the grammar G:
–
turn a derivation in the grammar GN into derivation in the grammar G′:
–
notice that if any production from groups 3–8 of PN is used, all three produc-
tions of this group must appear consecutively one after another. For instance,
any production of the 3rd group must appear in the following sequence:
⋅⋅⋅→. . . AB . . .
3N→. . . AB3 . . .
3N→. . . BB3 . . .
3N→. . . BA . . . →⋅⋅⋅
–
replace any triad as indicated above by the corresponding production from
the set P′,
–
turn the above derivation in the grammar GT into derivation in the grammar G,
–
replace any step of derivation applying a production Aa →a by the terminal
symbol a. For instance: replace ⋅⋅⋅→α A β →α aβ →⋅⋅⋅by ⋅⋅⋅→αaβ →⋅⋅⋅

4.2 Unrestricted grammars
|
87
Notice that application of any production from the set RN must start from the first
production of the given triad number r. It inserts the new nonterminal symbol Ar
k,
which is included in the left-hand side of the remaining productions of this set. Then
consecutive productions of the set RN must be applied; otherwise, the nonterminal
symbol Ar
k cannot be eliminated. Only such a sequence could be replaced by the pro-
duction number r of the grammar G′. If the sequence of productions of the set RN
is broken, then either a terminal word cannot be derived, or a break is made in this
sequence by a nested sequence of another set QN of productions. In the latter case,
if the result of productions is nested in a left-hand side of a production RN, then the
production number q can be omitted. Otherwise, it should be used in derivation prior
to the production number r.
4.2 Unrestricted grammars
The class of unrestrictedgrammars is the most general class of grammars. Unrestricted
grammars are similar to context-sensitive grammars except that productions are not
required to be monotonic (noncontracting). Unrestricted grammars generate the class
of recursively enumerable languages, which will be denoted as REL class of languages.
The formal definitions are as follows.
Definition 4.4. A grammar G = {V, T, P, S} is unrestricted if and only if its productions
are of the form:
α →β,
where: α, β ∈(V ∪T)∗and 0 < |α|
where: |w| denotes length of the word w.
Definition 4.5. Recursively enumerable languages are those generated by unre-
stricted grammars and only those.
Context-sensitive grammars are special cases of unrestricted grammars. Thus, the
class of context-sensitive languages is a subclass of recursively enumerable languages,
that is, CSL ⊂REL. But it is not obvious if this inclusion is proper, that is, if CSL
̸=
REL because nearly every language that we can imagine is context-sensitive. In the
consecutive chapters, we will design languages that are recursively enumerable but
not context-sensitive.
Example 4.2. The language L = {akbkckdk : k > 0} is not context-free (we can apply
contraposition of the pumping lemma for context-free languages to prove it). Design
a context-sensitive grammar generating this language.
First, we design an unrestricted grammar generating this language. Notice that
words of this language can be ordered according to their length:
L = {abcd, aabbccdd, aaabbbcccddd, . . .}.

88
|
4 Context-sensitive grammars and unrestricted grammars
The first word in this order is abcd. Having a word of this language, we can design the
next one (in this order) by supplementing: (i) with a the sequence of a’s, (ii) with b
the sequence of b’s, (iii) with c the sequence of c’s and (iv) with d the sequence of d’s.
Based on this observation, we will design a grammar in which consecutive words of
the language will be designed by enlarging sequences of letters a, b, c, d. Enlargement
will be done by symbols traveling along the word. The following grammar generates
the language: G = ({S, A, D}, {a, b, c, d}, P, S) with productions:
P:
S →abcDd
(1)
cD →Dc
(2)
bD →Db
(3)
aD →aaA
(4)
Ab →bA
(5)
bAc →bbcA
(6)
cAc →ccA
(7)
cAd →ccDdd
(8)
D →ε
(9)
A derivation of the word w = aaabbbcccddd is as follows:
S
1→abcDd
2→abDcd
3→aDbcd
4→aaAbcd
5→aabAcd
6→aabbcAd
8→aabbccDdd
2∗2
→aabbDccdd
2∗3
→aaDbbccdd
4→aaaAbbccdd
2∗5
→aaabAccdd
6→aaabbbcAcdd
7→aaabbbccAdd
8→aaabbbcccDddd
9→aaabbbcccddd
In the above derivation, the symbol
2∗3
→denotes an application of the production
number 3 two times.
The derivation is terminated if and only if production number 9 is applied. This
production may be used for any placement of the nonterminal symbol D in an inter-
mediate word of derivation. Note that production number 9 can be applied only to the
nonterminal symbol D. If this symbol appears in an intermediate word of derivation,
then this word includes the same number of letters a, b, c and d, so then removal of
this symbol produces a word of the language L.
The above grammar is not context-sensitive because production number 9 is nul-
lable (not monotonic). On the other hand, the empty word ε is not generated in this
grammar. Therefore, perhaps the grammar may be turned (though no evidence is
given for a general case) to a context-sensitive one by eliminating the nullable pro-
duction number 9. Elimination of the nullable production is done here by grouping
symbols cDd and substituting them by a new nonterminal symbol (denoted by [cDd]
to keep transformation clear). The following monotonic grammar generates the above

4.2 Unrestricted grammars
|
89
language:
G = ({S, A, D, [cDd]}, {a, b, c, d}, P, S)
P:
S →ab[cDd]
(1)
cD →Dc
(2)
bD →Db
(3)
aD →aaA
(4)
Ab →bA
(5)
bAc →bbcA
(6)
cAc →ccA
(7)
cAd →c[cDd]d
(8)
[cDd] →cDd
(9)
[cDd] →cd
(10)
A derivation of the word w = aaabbbcccddd looks now as follows:
S
1→ab[cDd]
9→abcDd
2→abDcd
3→aDbcd
4→aaAbcd
5→aabAcd
6→aabbcAd
8→aabbc[cDd]d
9→aabbccDdd
2∗2
→aabbDccdd
2∗3
→aaDbbccdd
4→aaaAbbccdd
2∗5
→aaabAccdd
6→aaabbbcAcdd
7→aaabbbccAdd
8→aaabbbcc[cDd]dd
10
→aaabbbcccddd


|
Part II: Automata and accepting languages


5 Turing machines
In Part I, the methods of generating languages were studied. Those methods are based
on different types of grammars and on regular expressions. Part II is devoted to a dis-
cussion on methods of languages acceptance. Acceptation of languages is based on
different types of automata: Turing machines, linear bounded automata, pushdown
automata and finite automata. Identification of grammars with the generation of lan-
guages and automata with the acceptation of languages is a subjective and intuitive
categorization done by the authors. However, it reflects the nature of tools for process-
ing languages.
Turing machines (and other types of automata) can be interpreted as models of
computation. Turing machines is a universal model of computation that is used for
such purposes as, for instance, acceptance of languages, computing functions, solv-
ing problems.
In this chapter, Turing machines, and automata in general, will be employed as
tools of acceptance of languages, that is, they will be queried whether a given the word
is in the language accepted by a given automaton or not.
Turing machines can also compute functions. Such machines compute functions
with natural numbers as domain and codomain. Another type of Turing machines
solves problems like, for instance, the sorting problem, the shortest paths problem,
etc. We only touch these aspects here.
5.1 Deterministic Turing machines
In this book, two categories of Turing machines (say automata, in general) will be stud-
ied: deterministic and nondeterministic. Roughly speaking, the computation of an au-
tomaton is a sequence of configurations organized according to some control informa-
tion. An automaton is a deterministic one if and only if there is at most one possibility
of doing a transition in any configuration. If, for a given automaton, there is a choice
of doing a transition in some configuration(s), then such an automaton is a nondeter-
ministic one.
In this section, different categories of deterministic Turing machines are studied:
basic model, model with the guard, multitrack model and multitape model. At the
end of this chapter, nondeterministic Turing machines are discussed. Equivalence of
these categories of Turing machines is drawn, that is, it is shown that for a Turing
machine in any model, we can find an equivalent machine in any other model. Equiv-
alence of Turing machines (equivalence of automata, in general) means that they ac-
cept the same language, compute the same function or solve the same problem. The
discussion leads to the main goal of this chapter, that is, that the class of determin-
istic Turing machines and the class of nondeterministic Turing machines are equiva-
lent.
https://doi.org/10.1515/9783110752304-005

94
|
5 Turing machines
5.1.1 Basic model of Turing machines
The definition of basic model of deterministic Turing machines is given below. Later in
this chapter, other deterministic models of Turing machines are discussed. They are
proved to be equivalent to basic model. As it was stated above, equivalence with regard
to accepted languages is considered. Besides, generalization of equivalence issue to
Turing machines computing functions or solving problems is straightforward.
Definition 5.1. A Turing machine in basic model is a system
M = (Q, Σ, Γ, δ, q0, B, F, C)
with components as follows:
Q
a finite set of states;
Γ
a finite set of tape symbols (tape alphabet);
B
the blank symbol (of tape alphabet), B ∈Γ;
Σ
an input alphabet, Σ ⊂(Γ −{B});
q0
the initial state, q0 ∈Q;
F
a set of accepting states, F ⊂Q;
C
a condition, its satisfaction is necessary and sufficient to stop computation;
δ
a transition function, which is a mapping: δ : Q × Γ →Q × Γ × {L, R}
where L, R denote left and right directions.
Notice that a transition function δ may not be a total function, that is, it may be
undefined for some of its arguments. Such a case is formally interpreted that the ma-
chine falls into an infinite computation. This comment will be explained in detail in
the further discussion of this chapter.
A Turing machine could be interpreted as a physical mechanism shown in Fig-
ure 5.1. This mechanism consists of:
–
a control unit, it is in a state of Q;
–
a one-way infinite tape split into cells; every cell contain a symbol (exactly one)
of the tape alphabet Γ;
–
the head, it is placed over a cell of a tape, it reads a symbol held in a cell, it stores
the desired symbol in the cell, it shifts left or right.
Figure 5.1: Basic model of Turing machine.

5.1 Deterministic Turing machines
|
95
Turing machines do the computation for a given input data. The computation of a
given Turing machine is done according to the following intuitive procedure:
1.
the initial configuration of a given Turing machine is described as follows:
a.
input data, a word w = a1a2 . . . an over input alphabet Σ, is stored in n begin-
ning cells of the tape; cf. Figure 5.1;
b.
all other cells of the tape, which is infinite to the right, are filled in with the
blank symbol B;
c.
the head of the Turing machine is placed over the first (leftmost) cell of the
tape;
d.
the control unit is in the beginning state q0;
2.
if the stop condition C is satisfied, then the computation is halted, the configura-
tion is called the final configuration, the machine responses whether its control
unit is in an accepting state or not;
3.
if the stop condition C is not satisfied, then – based on the state q of the control
unit and the symbol X read by the head – the Turing machine is doing the follow-
ing actions:
a.
the value (p, Y, D) of the transition function δ(q, X) is computed;
b.
the head stores the tape symbol Y in the cell under it;
c.
the control unit switches to the state p;
d.
the head shifts by one cell in the direction D;
4.
computation goes to the point 2.
The above intuitive procedure could be adapted to Turing machines, which compute
functions or solve problems. This adaptation needs a redefinition of input data. Input
data of a Turing machine computing function or solving problem is:
–
a sequence of arguments of the function computed by the machine. Arguments of
a function are encoded as numbers, for example, in the binary or decimal posi-
tional system or in the unary system. Of course, some sort of separators between
arguments must be used;
–
a data defining an instance of the problem solved by the machine. This data is
encoded in a way depending on the type of data.
Note that the stop condition C may never be satisfied and the machine will be doing
an infinite computation. When the infinite computation is done, the input data is not
accepted by the machine, that is, in the case of accepting a language, the input word
does not belong to the language accepted by the Turing machine. It is worth under-
lining that a Turing machine always stops its computation in an accepting state if and
only if its input data is a word of the language accepted by the machine. Turing ma-
chines raise a difficult problem: when one is performing long computation, there is no
indication if the machine has fallen into infinite computation or it will stop computa-
tion in the future.

96
|
5 Turing machines
Remark 5.1. For the sake of clarity, we assume that Turing machines will be designed
assuming that, if a machine terminates computation, its head is placed over the first
(leftmost) cell and:
–
when the machine accepts a language, then all cells of the taped are filled in with
the blank symbol B;
–
when the machine computes a function, then the value of the function is stored
in the beginning cells of the tape. All other cells are filled in with the blank sym-
bol B. The stored value is the correct result if and only if the machine stopped
computation in an accepting state;
–
when the machine solves a problem, then output data is stored in the beginning
cells of the tape. All other cells of the tape are filled in with the blank symbol B.
Output data is a correct solution of a computed instance of the problem if and only
if the machine stops computation in an accepting state.
Assumptions of the above remark are not included in Definition 5.1 and are not nec-
essary, but they are desired for clarity of computation. An epilogue of a computation
guarantying satisfaction of the above assumptions is called a cleaning procedure.
Definition 5.2. A step description (a configuration) of a Turing machine
M = (Q, Γ, Σ, δ, q0, B, F, C)
is a sequence of symbols:
α1 q α2
where:
–
q ∈Q is the current state of the control unit of a Turing machine;
–
α1 is a sequence of symbols stored in cells beginning from the leftmost one and
ending with the cell prior to the one under the head;
–
α2 is a sequence of symbols stored in cells beginning from the one under the head,
going to the right and ending with the rightmost one holding a nonblank symbol.
Note that both α1 and α2 sequences of symbols are words over the tape alphabet Γ
and that any of these sequences may be the empty word. However, none of these two
sequences can be infinite. This is due to the following reasons:
–
an input data is finite, so then only a finite number of cells are filled in with non-
blank symbols in the input configuration;
–
after any step of computation, only a finite number of cells could be visited by the
head. Therefore, only a finite number of cells may get a nonempty symbol.
For instance, the initial step description (configuration) is of the form q0 w, where q0
is the initial state, w is the input data. In this case, α1 is the empty word. On the other

5.1 Deterministic Turing machines
|
97
hand, a step description α1 q informs that all cells from the one under the head to
the right are filled in with the blank symbol B. Now, α2 is the empty word. Finally,
when a Turing machine accepting a language ends its computation in a state q, then
– according to Remark 5.1 – q will be the final step description.
Let us analyze transitions done by Turing machines. We assume that a step de-
scription is characterized by the following sequence of symbols:
X1 X2 . . . Xi−1 q Xi . . . Xn
Recall that:
–
q is the current state of the control unit;
–
X1 X2 . . . Xi−1 is the sequence of symbols of the tape alphabet Γ stored in cells pre-
ceding the cell under the head;
–
Xi Xi+1 . . . Xn is the sequence of symbols stored in cells beginning from the one
under the head, going to the right and ending with the rightmost one containing
a nonblank symbol;
–
the head is placed over the cell with the Xi symbol stored in. If the sequence
Xi Xi+1 . . . Xn is the empty word, then the head reads the blank symbol B.
The transition function determines the following step description:
–
if the value of the transition function is δ(q, Xi) = (p, Y, R), that is, the control unit
switches to the state p, the head stores Y and shifts right, then we get the following
configuration,
X1 X2 . . . Xi−1 Yp Xi+1 . . . Xn
–
if the value of the transition function is δ(q, Xi) = (p, Y, L), then we get the follow-
ing configuration:
X1 X2 . . . Xi−2 p Xi−1Y Xi+1 . . . Xn
We will use the symbol ≻to denote a transition of a Turing machine. The transition
symbol ≻may be supplement with a Turing machine name ≻M to emphasize that a
transition concerns a given Turing machine. It also can be supplemented with a su-
perscript ≻k to notify k transitions done.
The above two transitions done by a Turing machine will be denoted as follows:
X1 X2 . . . Xi−1 q Xi . . . Xn ≻X1 X2 . . . Xi−1 Y p Xi+1 . . . Xn
X1 X2 . . . Xi−1q Xi . . . Xn ≻X1 X2 . . . Xi−2 p Xi−1Y Xi+1 . . . Xn
Definition 5.3. Transitions of a Turing machine create a binary relation in the space of
all possible configurations of the machine, that is, any two configurations are related if
and only if the second one is derived from the first one utilizing the transition function.

98
|
5 Turing machines
This relation is called the transition relation of a given Turing machine and is marked
with the symbol ≻. We will also consider the transitive closure of the transition relation
and mark it with the symbol ≻∗.
Definition 5.4. A computation of a Turing machine M = (Q, Γ, Σ, δ, q0, B, F, C) is a se-
quence of configurations η1, η2, . . . , ηn such that η1 is the initial configuration, ηn is the
final configuration and a pair of any two successive configurations belongs to the tran-
sition relation. If the machine has fallen into an infinite computation, then its compu-
tation is an infinite sequence of configurations η1, η2, η3 . . . such that η1 is the initial
configuration and any pair of two successive configurations belongs to the transition
relation. A finite computations is denoted as η1 ≻η2 ≻⋅⋅⋅≻ηn and infinite computa-
tion is denoted as η1 ≻η2 ≻η3 ≻. . . .
Now we give a formal definition of acceptation of an input by a Turing machine.
Definition 5.5. A Turing machine accepts its input if and only if the computation ter-
minates in an accepting state. In other words, a Turing machine accepts its input if
and only if the pair of the initial configuration and the final configuration belongs to
the transitive closure of the transition relation, that is, η1 ≻∗ηT, where η1 is an initial
configuration and ηT is a final (i. e., accepting) configuration.
Based on the above discussion, we now give formal definitions of some concepts.
Definition 5.6. The language L(M) accepted by a Turing machine M is the set of words
w ∈Σ∗accepted by the Turing machine.
Definition 5.7. A function computed by a Turing machine M is a mapping from a space
of input data into a space of output data. If the machine accepts its input, then its out-
put is the correct value of the function. Otherwise, when the machine stops compu-
tation but not accepts the input, or if it is doing infinite computation, the function is
undefined for such input data.
Remark 5.2. We assume that the blank symbols B will neither separate nonblank sym-
bols on tape nor be placed in leftmost cells prior to a nonblank symbol. This assump-
tion is not required by definitions and concepts discussed so far. However, it simplifies
the designing of Turing machines for given tasks.
Example 5.1. Design a Turing machine computing the function:
f : N →N,
f(n) = ⌈n
3 ⌉,
where N = {0, 1, 2, . . .}
Solution. First, we briefly comment on an algorithm for a Turing machine computing
this function. The algorithm consists of a method of encoding input and output data
and of a description of computation:

5.1 Deterministic Turing machines
|
99
–
input and output data are stored in the unary system, that is, the number n is
stored as the sequence of n unary digits 0;
–
computation of the machine relies on repeated subtraction of the denominator
from the numerator. Subtraction is done by removing three digits 0 from data
stored on the tape:
–
marking the first digit 0, it increments the function value, that is, for every
subtraction, the value of the function is incremented by 1;
–
deleting the last two zeros;
–
in the last subtraction, there might be one or no digit 0 to delete;
–
the function value will be equal to the number of subtractions, that is, to the num-
ber of marked digits 0.
A Turing machine computing this function is as follows:
M = (Q, Σ, Γ, δ, q0, B, F, C)
where:
–
Q = {q0, q1, q2, q3, q4, q5, q6, qA};
–
Σ = {0};
–
Γ = {0, A, X, B};
–
F = {qA} and
–
the stop condition C is reached if control unit switches to the accepting state qA.
The transition function is given in Table 5.1.
Table 5.1: The transition function of the Turing machine of Example 5.1.
δ
0
A
X
B
q0
(q2, X, R)
(q6, B, R)
q1
(q2, A, R)
(q6, B, L)
q2
(q2, 0, R)
(q3, B, L)
q3
(q4, B, L)
(q6, 0, L)
(q6, 0, R)
q4
(q5, B, L)
(q6, 0, L)
(q6, 0, R)
q5
(q5, 0, L)
(q1, A, R)
(q1, X, R)
q6
(qA, 0, L)
(q6, 0, L)
(q6, 0, R)
(qA, B, L)
The transition function is a kind of computing program in a low-level programming
language. The detailed description of the transition function is given here in the form
of comments to states of the machine:
q0
terminates computation for the input data (argument of the function) equal to zero
or sets the output value to one for nonzero input data. The tape symbol X is stored
in the first (leftmost) cell. This symbol indicates the output value as well as marks
the leftmost cell of the tape;

100
|
5 Turing machines
q1
increments output data by one for every subtraction. Incrementing is done by stor-
ing the tape symbol A in the leftmost cell filled in with the digit 0;
q2
passes the head to the end of input data, that is, to the cell that directly follows
the cell with the rightmost digit 0 and then goes back to the cell with the rightmost
digit 0;
q3
deletes the rightmost digit 0 and moves the head one cell left. If there are no more
0’s, then a cleaning process begins with switching to the state q6;
q4
deletes the last but one 0, states q0, q3, q4 or q1, q3, q4 are in charge of removing
three digits 0 from input data, that is, they are in charge of subtraction 3 from the
input data. If there are no more 0’s, then a cleaning process begins with switching
to the state q6;
q5
passes the head to the beginning of input data, that is, to the cell with the leftmost
0. Then the control unit switches to the state q1 to repeat the process of subtraction
of 3 from input data;
q6
preparation to terminate computation passes the head to the beginning of input
data and turns symbols A and X to 0’s;
qA terminates computation.
Now we provide computation for given input data zero and five. Zero is represented as
the empty sequence of unary digits and five is represented as the sequence of 5 unary
digits 00000:
–
f (0) = ⌈0
3 ⌉= 0:
q0 ≻Bq6 ≻qA
–
f (0) = ⌈5
3⌉= 2:
q000000 ≻Xq2 000 ≻X0q2 000 ≻X00q2 00 ≻X000q2 0 ≻X0000q2 ≻
X000q3 0 ≻X00q4 0 ≻X0q5 0 ≻Xq5 00 ≻q5 X00 ≻Xq1 00 ≻XAq2 0 ≻XA0q2 ≻
XAq3 0 ≻Xq4 A ≻q6 X0 ≻0q6 0 ≻qA 00
5.1.2 Turing machine with the stop property
As noticed before, some Turing machines can fall into an infinite computation. Some
other will terminate their computation for any input data. This observation draws the
definition of the subclass of Turing machines, which always terminate their computa-
tion.
Definition 5.8. A Turing machine in basic model with the stop property is a system
introduced in Definition 5.1,
M = (Q, Σ, Γ, δ, q0, B, F, C),
and such that it terminates its computation for any input data.

5.1 Deterministic Turing machines
|
101
Table 5.2: An updated transition function of the Turing machine designed in Example 5.1.
δ
0
A
X
B
q0
(q2, X, R)
(qR, $, R)
(qR, $, R)
(q6, B, R)
q1
(q2, A, R)
(qR, $, R)
(qR, $, R)
(q6, B, L)
q2
(q2, 0, R)
(qR, $, R)
(qR, $, R)
(q3, B, L)
q3
(q4, B, L)
(q6, 0, L)
(q6, 0, R)
(qR, $, R)
q4
(q5, B, L)
(q6, 0, L)
(q6, 0, R)
(qR, $, R)
q5
(q5, 0, L)
(q1, A, R)
(q1, X, R)
(qR, $, R)
q6
(qA, 0, L)
(q6, 0, L)
(q6, 0, R)
(qR, $, R)
Example 5.2. Design a Turing machine with the stop property computing the function
exposed in Example 5.1.
Solution. The transition table of the Turing machine of Example 5.1, as shown in Ta-
ble 5.1, has empty entries for some arguments. The empty entries mean that the tran-
sition function is undefined for such arguments. Such configuration of a Turing ma-
chine, in which a value of its transition function is undefined, is understood as falling
into the infinite computation. In light of this interpretation, the Turing machine of
Example 5.1 does not have the stop property. On the other hand, a configuration in
which the transition function is undefined is never reached. Moreover, the machine
terminates its computation for any input data. However, we want to keep the assump-
tion that undefined transition function value starts infinite computation. So then, to
solve the inconsistency, we will turn the machine of Example 5.1 to have the (formal)
stop property, cf. Table 5.2.
A new state qR is added to states of the machine of Example 5.1. The control unit
switches to this state any time when the original transition function is undefined (a
special symbol is stored in the cell and the head goes right because this shift is always
possible). Computation is terminated if and only if the control unit switches to any
of two states qA or qR. Therefore, the response of the machine depends only on two
states: qA as accepting state and qR as rejecting (not accepting) one.
Note, due to the sake of simplicity and readability of the transition function,
the cleaning procedure of Remark 5.1 is not applied when computation reaches the
state qR.
It is worth underlining that a function or a language computed by a Turing ma-
chine with the stop property can also be computed by a Turing machine without stop
property. Moreover, such the Turing machine without stop property can perform infi-
nite computation for some input data.
Two Turing machines, one with the stop property and another one without the
stop property, if compute the same function or accept the same language, must termi-
nate computation in accepting states for the same input data. Both machines may also
terminate computation in rejecting states for the same data. They may yield different

102
|
5 Turing machines
outputs only for such input data, which is not accepted. In such a case, the machine
with the stop property terminates its computation in a rejecting state and the second
machine falls in infinite computation.
Definition 5.9. Turing machines are considered equivalent if and only if for the same
input data they either terminate computation in accepting state or none of them does
it.
5.1.3 Simplifying the stop condition
Now we will slightly change definitions of Turing machines in basic model to simplify
the definition of termination of its computation.
Definition 5.10. Turing machine with the halting accepting state is a system:
M = (Q, Σ, Γ, δ, q0, B, F)
where:
–
F = {qA} – there is only one accepting state qA;
–
the stop condition is satisfied if and only if the machine switches to the accepting
state qA;
–
other components of the system are as described in Definition 5.1.
Proposition 5.1. Turing machines with the halting accepting state are equivalent to Tur-
ing machines in basic model.
Proof. First of all, a Turing machine M = (Q, Σ, Γ, δ, q0, B, {qA}) with halting accepting
state formally matches Definition 5.1.
On the other hand, a Turing machine in basic model M = (Q, Σ, Γ, δ, q0, B, F, C) can
be updated to a machine with halting accepting state by:
–
adding new states q# and qA;
–
doing two transitions: (q#, X, R) (qA, Y, L), when the stop condition of the Turing
machine in basic model is satisfied and the machine is in accepting state, where
X and Y are symbols previously stored in cells under the head. These two tran-
sitions just switch the machine to the new state qA, keeping contents of the tape
unaffected and places the head in the same position as before these transitions;
–
changing status of former accepting states to not accepting and assuming qA as
the only accepting state;
–
redefining the stop condition: computation is halted if and only if the machine
switches to the state qA (now the only accepting state).
The machine with a halting accepting state may fall into infinite computation when
the machine in basic model stops its computation in a nonaccepting state. Anyway,

5.1 Deterministic Turing machines
|
103
the machine with the halting accepting state terminates its computation in accepting
state if and only if the machine in basic model does the same.
This proves the equivalence of both machines with regard to the accepted lan-
guage. Therefore, Turing machines in basic model are equivalent to Turing machines
with the halting accepting state.
Definition 5.11. Turing machine with halting states is a system
M = (Q, Σ, Γ, δ, q0, B, F, R)
such that it terminates its computation for any input data, where:
–
F = {qA} – includes only one accepting state qA;
–
R = {qR} – includes a special nonaccepting state qR;
–
computation for any input always reaches one of states qA or qR;
–
computation stops if and only if it reaches qA or qR;
–
other components of the system are as described in Definition 5.1.
Proposition 5.2. Turing machines with halting states are equivalent to Turing machines
in basic model with the stop property.
Proof. Modify the proof of Proposition 5.1 to justify this proposition.
5.1.4 Guarding the tape beginning
Basic model of Turing machines raises a practical problem of how to detect the tape
beginning. The machine of Example 5.1 stores the special symbol X in the first cell and
finally – when the head is passed to the beginning of the tape – this symbol is turned
to output 0. Of course, a general solution of this problem is similar: just to store special
symbols in the first cell of the tape, which replaces symbols defined by the transition
function. However, such a solution enlarges the set of states, the tape alphabet and the
transition function. The model of the Turing machine with guard avoids this problem.
Definition 5.12. The Turing machine in basic model with guard is a system
M = (Q, Σ, Γ, δ, q0, B, #, F, C)
where:
–
# – the guard symbol (of tape alphabet), # ∈Γ, # ∉Σ;
–
other components are as in Definition 5.1;
–
the input configuration is shown in Figure 5.2;
–
the head can visit the first cell (with guard) but cannot change its contents; that
is, must print the guard when is doing transition and shift of the head must be
done to the right;
–
the head cannot print the guard anywhere besides the first cell.

104
|
5 Turing machines
Figure 5.2: Turing machine with guard.
Proposition 5.3. Turing machines with guard are equivalent to Turing machines in ba-
sic model.
Proof. Given a Turing machine in basic model
M = (Q, Σ, Γ, δ, q0, B, F, C)
we get its formal equivalent Turing machine with guard by supplementing the system
with the guard symbol, shifting input data one cell right, storing the guard symbol in
the leftmost cell and placing the head over the second leftmost cell (over the leftmost
input symbol). Therefore, we get the following system:
MG = (Q, Σ, Γ ∪{#}, δ, q0, B, #, F, C)
Other components of the MG stay unchanged comparing to M. Any computation of MG
will be precisely the same as for M and the head will never visit the guard cell.
And oppositely, given a Turing machine with guard
MG = (Q, Σ, Γ, δ, q0, B, #, F, C)
the following machine is equivalent:
M′ = (Q′, Σ, Γ, δ′, q′
0, B, F′, C′)
The machine M will do the following computation:
–
shifts input data one cell right, stores the guard symbol # in the first cell and leaves
the head at the first input symbol (on the second leftmost cell);
–
simulates computation of MG;
–
shifts the output data one cell left (this deletes the guard symbol in the leftmost
cell) and leaves the head at the leftmost output symbol (at the leftmost cell).
Detailed description of M′ is given in Example 5.3.
Example 5.3. Design a Turing machine in basic model equivalent to a given Turing
machine with guard.

5.1 Deterministic Turing machines
|
105
Solution. Let us assume that a Turing machine with guard is given as
MG = (Q, Σ, Γ, δ, q0, B, #, F, C)
The following Turing machine in basic model is equivalent to MG:
M′ = (Q′, Σ, Γ, δ′, q′
0, B, F′, C′)
where:
–
Q′ = Q ∪{p0, pL} ∪{pa : a ∈Σ} ∪{r0, r′, rA, rR} ∪{rX : X ∈Σ −{#}};
–
q′
0 = p0;
–
F′ = {rA};
–
C′ – computation terminates if and only if any of state rA, rR is reached.
The part of the transition function, which shifts input data one cell right and inserts
the guard symbol, is given below:
–
δ′(p0, B) = (q0, #, R) – when input is empty, just store the guard symbol and shift
the head right;
–
δ′(p0, a) = (pa, #, R) for a ∈Σ – insert the guard symbol, remember an input sym-
bol in a respective state, shift the head right;
–
δ′(pa, b) = (pb, a, R) for a, b ∈Σ – remember an input symbol in a respective state
and simultaneously store in the cell the symbol remembered in the state of the
previous transition, shift the head right;
–
δ′(pa, B) = (pL, a, L) for a ∈Σ – store in the cell the last input symbol remembered
in the state of the previous transition, shift the head left;
–
δ′(pL, a) = (pL, a, L) for a ∈Σ – pass the head to the beginning of the tape;
–
δ′(pL, #) = (q0, #, R) – shift the head to the first input symbol.
The part of the transition function, which follows the computation of MG is
δ′(q, X) = δ(q, X) for q ∈Q −{qA}, X ∈Γ.
The part of the transition function, which shifts output data one cell left and re-
moves the guard symbol, is as follows:
–
δ′(qA, B) = (rB, B, L) – output data is empty;
–
δ′(qA, X) = (r0, X, R) for X ∈Γ −{B} and
δ′(r0, X) = (r0, X, R) for X ∈Γ −{B} – pass the head to the end of output data;
–
δ′(r0, B) = (rB, B, L) – remember the blank symbol (the first one right of output
data, the leftmost one at the tape) in states, begin passing the head left and shift-
ing output data left;
–
δ′(rX, Y) = (rY, X, L) for X, Y ∈Γ−{#} – remember an output symbol in a respective
state and simultaneously store in the cell the symbol remembered in a state of the
previous transition, shift the head left;

106
|
5 Turing machines
–
δ′(rX, #) = (r′, X, R) for X ∈Γ −{#} – remove the guard symbol, store the leftmost
output symbol instead, shift the head right (this is the leftmost cell, no shift left);
–
δ′(r′, X) = (rA, X, L) for X ∈Γ −{#} – return to the leftmost cell.
The transition function is undefined for any arguments not considered above. The ma-
chine is assumed to fall into infinite computation if not accepts an input. A discussion
on the adaptation of this solution to the transition function undefined for some argu-
ments is left to the reader.
Remark 5.3. The other models of Turing machines, for example, Turing machines
with the stop property or Turing machines with halting states, can be turned to mod-
els with guard. Formulation and proof of equivalence of models with guard and other
models of Turing machines are analogous to the proof of Proposition 5.3.
5.1.5 Turing machines with a multitrack tape
A Turing machine with multitrack tape has the tape split at its length to a given number
of tracks. Every track is split into cells and is one way infinite. The head reads symbols
of all cells of the same slice (column) and shifts simultaneously over all tracks. An
initial configuration of a Turing machine k-tracks tape is shown in Figure 5.3. Input
data is stored in the beginning cells of track number one while all other cells of track
number one and all cells of other tracks are filled in with the blank symbol B.
Figure 5.3: Turing machine in basic model with a multitrack tape.

5.1 Deterministic Turing machines
|
107
Definition 5.13. Turing machine in basic model with a multitrack (k-tracks) tape is a
system
M = (Q, Σ, Γ, δ, q0, B, F, C)
where:
–
δ : Q × Γk →Q × Γk × {L, R} is the transition function;
–
input data w = a1a2 . . . an is represented as sequence of k-tuples
((a1, B, . . . , B), (a2,
B, . . . , B), . . . , (an, B, . . . , B)), every k-tuple fills in one slice of the tape, cf. Figure 5.3;
–
output data is represented in a way similar to the representation of input data,
that is, it is stored in the beginning cells of the first track while all other cells are
filled in with the blank symbol;
–
other components are as in Definition 5.1.
Proposition 5.4. Turing machines with a multitrack tape are equivalent to Turing ma-
chines in basic model.
Proof. First of all, a Turing machine in basic model is a special case of a Turing ma-
chine with a multitrack tape having just one track.
Second, a Turing machine with k-tracks tape:
M′ = (Q, Σ′, Γ′, δ, q0, B′, F, C)
is equivalent to the following Turing machine in basic model:
M = (Q, Σ, Γ, δ, q0, B, F, C)
where:
–
Σ = Σ′ × {B} × {B} × ⋅⋅⋅× {B} – product of k sets;
–
Γ = Γ′ × Γ′ × ⋅⋅⋅× Γ′ – product of k sets;
–
B = (B′, B′, . . . , B′) – k-tuple;
–
other components are as in machine M′.
The above conclusion is not surprising. Turing machines with multitrack tape
work in a way quite similar to Turing machines in basic model, that is, they have one-
way infinite tape, their one head reads and writes data of the whole slice at a time,
etc. Therefore, when data of a slice is interpreted as one symbol of a tape alphabet,
a Turing machine with a multitrack tape is just a Turing machine in basic model. Tur-
ing machines with multitrack tape are essentially useful in proofs of equivalence of
other, more important, models of Turing machines. This is the main motivation for
discussing this model.

108
|
5 Turing machines
5.1.6 Turing machines with two-way infinite tape
Variations of Turing machines discussed so far are slightly modified Turing machines
in basic model. Two-way infinite tape is the first important modification of Turing ma-
chines in basic model; cf. Figure 5.4. When Remark 5.2 is employed, Turing machines
with two-way infinite tape permit avoiding problems with passing the head to the be-
ginning of the tape or to the beginning of data stored on the tape: the first cell with the
blank symbol prior to nonblank symbols indicates the beginning of data stored on the
tape (of course, assuming that blank symbol cannot appear between nonblank ones).
Figure 5.4: Turing machine with two-way infinite tape.
Definition of Turing machine with two-way infinite tape is identical with Definition 5.1.
Turing machine with two-way infinite tape does not need to identify the beginning
of the tape. However, the definition of configuration (step description) of this type of
machines is slightly different from that of machines in basic model. The difference is
in the description of the left sequence of symbols.
Definition 5.14. A step description (a configuration) of a Turing machine with two-
way infinite tape M = (Q, Γ, Σ, δ, q0, B, F, C) is the following sequence of symbols:
α1 q α2
where:
–
q ∈Q and α2 are the same as in Definition 5.2;
–
α1 is the sequence of symbols stored in cells left of the head, starting with a left-
most nonblank symbol and ending with the symbol in the cell before the one un-
der the head.
Proposition 5.5. Turing machines with two-way infinite tape are equivalent to Turing
machines in basic model.
Proof. We prove that Turing machines with two-way infinite tape and Turing machines
with multitrack tape are equivalent. Because Turing machines with a multi-track tape

5.1 Deterministic Turing machines
|
109
are equivalent to Turing machines in basic model (cf. Proposition 5.13), then we get
equivalence declared in this Proposition.
For a given Turing machine in basic model M1, an equivalent Turing machine with
two-way infinite tape M2 is equal to M1. Computation of M1 is being done on the right-
half of tape (the right part of the tape, which begins with the cell holding the first
symbol of input data).
Assuming that a machine with two-way infinite tape is given as follows:
M2 = (Q2, Σ, Γ2, δ2, q2
0, B2, F, C)
we will design a machine with a multitrack tape:
M1 = (Q1, Σ, Γ1, δ1, q1
0, B1, ̸c, F, C)
First of all, a method of representation of a two-way infinite tape must be found
out. A one-way infinite tape is a 2-tracks tape. The right-half of the two-way infinite
tape matches the upper track and the left-half rotated by 180∘matches the lower track;
cf. Figure 5.4 and Figure 5.5
Figure 5.5: Turing machine with two-track tape – simulation of two-way infinite tape.
Computation of M2 done on the right-half of tape is followed on the upper track of M1.
Computation of M2 done on the left-half of tape is followed on the lower track of M1
with the head shifting oppositely than the head of M2. Note that the first cell of the
lower track plays the role of the guard while the content of the left-half of tape is stored
beginning with the second cell; cf. Figure 5.6.
A formal and detailed description of M1 simulating M2 is as follows:
–
Q1 = Q2 × {U, B} ∪{q1
0}
–
Γ1 = Γ2 × Γ2 ∪Γ2 × { ̸c}
–
Σ1 = Σ2 × {B2}
–
F1 = {(q, U), (q, L) : q ∈F2}
–
B1 = (B2, B2)

110
|
5 Turing machines
Figure 5.6: Turing machine with two-track tape – simulation of computation of Turing machine with
two-way infinite tape.
Symbols U and L depict placement of the head of M2 on the right- and the left- half of
the tape.
The first transition of M1 stores the guard symbol in the first cell of the lower track
and simulates the first transition of M2 going to either the upper or the lower track:
δ1(q1
0, (a0, B)) = {
((p, U), (Y, ̸c), R)
if δ2(q2
0, a0) = (p, Y, R)
((p, L), (Y, ̸c), R)
if δ2(q2
0, a0) = (p, Y, L)
When the head of M1 is placed right of the first cell (computation of M2 cannot
change current half of the tape), then:
δ1 ((q, U), (X, Z)) = ((p, U), (Y, Z), D)
δ1 ((q, L), (Z, X)) = ((p, L), (Z, Y), D)
}
if δ2 (q, X) = (p, Y, D)
where: D is a direction of the head shift, D is opposite to D.
When the head of M1 is placed at the first cell (computation of M2 may change
current half of the tape), then:
δ1 ((q, U), (X, ̸c)) = ((p, U), (Y, ̸c), R)
δ1 ((q, L), (X, ̸c)) = ((p, U), (Y, ̸c), R)
}
if δ2 (q, X) = (p, Y, R)
δ1 ((q, U), (X, ̸c)) = ((p, L), (Y, ̸c), R)
δ1 ((q, L), (X, ̸c)) = ((p, L), (Y, ̸c), R)
}
if δ2 (q, X) = (p, Y, L)
where,

5.1 Deterministic Turing machines
|
111
Figure 5.7: A multitape Turing machine.
–
U and L, coming in states labels, stand for upper and lower track;
–
L and R, coming as the third element of the transition function (3-tuple) value,
stand for left and right direction of head shifts.
In light of Proposition 5.4 and Proposition 5.5, the following conclusion is fairly
obvious.
Proposition 5.6. Turing machines with amultitrack two-way infinite tape areequivalent
to Turing machines in basic model.
5.1.7 Multitape Turing machines
A multitape Turing machine (cf. Figure 5.7) satisfies the assumptions:
–
it has several tapes and one head for each tape;
–
tapes are two-way infinite;
–
the initial configuration assumes:
–
the control unit is in the initial state;
–
input data is stored on the first tape;
–
the head of the first tape is placed over the first (leftmost) symbol of input
data;
–
all other cells of the first tape and all cells of other tapes are filled in with the
blank symbol,
–
a transition of a multitape Turing machine is as follows:
–
the control unit switches to some state;
–
every head stores a tape symbols in its cell;

112
|
5 Turing machines
–
every head shifts left, right or stays in current position independently on other
heads.
Formal definition of a multitape Turing machine is as follows.
Definition 5.15. A multitape Turing machine (with k-tapes) is a system
M = (Q, Σ, Γ1 × Γ2 × ⋅⋅⋅× Γk, δ, q0, B, F, C)
where:
–
Γ1, Γ2, . . . , Γk are alphabets of tapes. It is assumed that all tapes have the same al-
phabet Γ, if not stated differently;
–
δ : Q × (Γ1 × Γ2 × ⋅⋅⋅× Γk) →Q × (Γ1 × Γ2 × ⋅⋅⋅× Γk) × {L, R, S}k is the transition
function with directions of head shift: left, right and stop (no shift of a head);
–
other components are as in Definition 5.1.
Proposition 5.7. Multitape Turing machines are equivalent to Turing machines in basic
model.
Proof. A Turing machine in basic model is equivalent to some Turing machine with
two-way infinite tape; cf. Proposition 5.5. Moreover, a Turing machine with two-way
infinite tape is a case of a multitape Turing machine; it is just a multitape Turing ma-
chine with one tape. Therefore, for any Turing machine in basic model, we can directly
find an equivalent multitape Turing machine.
Now, we give an idea of designing a Turing machine with a multitrack two-way
infinite tape M1 for a given k-tapes Turing machine Mm:
–
k-tapes are represented on a two-way infinite tape with 2-k-tracks; each tape cor-
responds to a pair of tracks; cf. Figure 5.8
–
content of every tape is stored on the bottom track of the corresponding pair
of tracks;
–
the special symbol H stored in a cell of upper track marks the head position
of the tape of the machine Mm;
–
all other cells of both tracks are filled in with the empty symbol B;
–
initial configuration of M1 is as follows:
–
the initial state of Mm corresponds to the relevant state of M1;
–
input data stored on the first tape of a multitape Turing machine is repre-
sented on the lower track of the first pair of tracks of M1;
–
the head markers of all tapes of Mm are placed in the tape slice holding the
first symbol of input data;
–
a transition of the machine Mm is simulated by several transitions of the ma-
chine M1:
–
the head of M1 passes from the tape slice holding the leftmost head marker H
to the tape slice holding the rightmost head marker. This is so-called collect-

5.1 Deterministic Turing machines
|
113
Figure 5.8: Turing machine with a multitrack two-way infinite tape simulating a multitape Turing
machine. The blank symbol is not printed for the sake of clarity; that is, all cells which are empty in
this figure hold the blank symbol.
data pass. Information about symbols under all tape markers (under heads
of Mm) is collected during this pass and remembered in a relevant state of M1;
–
the transition function of Mm is applied to collected data (state of Mm and
symbols under heads of Mm). The result of the transition function is remem-
bered in a state of M1. Recall that transition function of Mm returns: a state,
symbols to be printed by heads and directions of head shifts;
–
the head of M1 is passed to the time slice holding the leftmost tape marker H.
This is so-called update pass. Symbols under head markers and head markers’
positions are updated during this pass. Updates are done according to the
value of the transition function obtained in the previous point.
–
computation of M1 is terminated if and only if the stop condition of Mm is satis-
fied. Input data of M1 is accepted if and only if Mm terminates computation in an
accepting state.
Note that simulation of a multitape Turing machine by Turing machines with one
tape requires a huge set of states, which makes transition function highly enlarged. It
also increases the number of transitions for the same input data.
As it is stated in Proposition 5.7, data collected during the left to right pass of the
head on simulating machine are stored in the set of states. So, a normal set of states
Q1 of the simulating machine should be extended by the Cartesian product of:
–
a pass direction: Q1 × {C, U} to indicate if this is the collect-data pass or the update
pass;
–
the set of states of a multitape machine: Q1 × {C, U} × Qm to keep a state of Mm
during both passes;
–
alphabets of tapes: Q1 × {C, U} × Qm × Γ1 × Γ2 × ⋅⋅⋅× Γk for symbols under heads
during both passes;

114
|
5 Turing machines
–
markers of heads, which were visited in the collect-data pass, this might be done
by extending tapes’ alphabets with a special the marker-not-visited-yet symbol ℏ:
Q1 × {C, U} × Qm × Γ′
1 × Γ′
2 × ⋅⋅⋅× Γ′
k, where Γ′
i = Γi ∪{ℏ} for any tape i;
–
a value of the transition function δm is stored in already included components
Qm × Γ′
1 × Γ′
2 × ⋅⋅⋅× Γ′
k with the update pass indication;
–
heads’ shifts Q1 × {C, U} × Qm × Γ′
1 × Γ′
2 × ⋅⋅⋅× Γ′
k × {L, R, S}k
–
update heads’ positions counters would require four symbols to indicate status
(not updated, being shifted left, being shifted right, updated),
Q1 × {C, U} × Qm × Γ′
1 × ⋅⋅⋅× Γ′
k × {L, R, S}k × {Xi, Xii, Xiii, Xiv}k for each tape;
–
and more states is required to organize simulation details.
Thus, states could be labeled by elements of the above Cartesian product, that is, by
(3k + 3)-tuples. So then the number of states is not less than
r = 2 ∗3k ∗3k ∗|Q1| ∗|Qm| ∗|Γ′
1| ∗|Γ′
2| ∗⋅⋅⋅∗|Γ′
k|
On the other hand, symbols of the tape alphabet of a simulating machine could be
denoted by all tuples of the Cartesian product of tapes’ alphabets and symbols stored
in tracks with heads’ markers (the blank symbol B and the head’s marker symbol H)
{H, B}×Γ1×{H, B}×Γ2×⋅⋅⋅×{H, B}×Γk. Thus, the number of tape symbols of simulating
machine is equal to c = 2k ∗|Γ1| ∗|Γ2| ∗⋅⋅⋅∗|Γk|.
Finally, the size of the transition table of the simulating machine is not less than
r∗c comparing to the size of the transition table of a multitape Turing machine, which
is equal to |Qm| ∗(|Γ1| ∗|Γ2| ∗⋅⋅⋅∗|Γk|).
Let us analyze the possible increase in the number of transitions. The worse case
is when the head of one tape of a multitape Turing machine always shifts right and
the head of another tape always shifts left. Tape slices with these two heads’ markers
will be separated by:
–
one tape slices after simulation of the first transition;
–
three slices after simulation of two transitions;
–
2 ∗n −1 tape slices after simulation of n transitions.
Simulation of transitions of a multitape Turing machine, as described above, requires
the following numbers of transitions of simulating machine:
–
three transitions of M1 to simulate the first transition of Mm;
–
at least seven transitions of M1 to simulate the second transition of Mm;
–
. . .
–
at least 4∗n−1 transitions of M1 to simulate the n-th transition of Mm. This number
will be increased by transitions updating markers of those heads of a multitape
machine, which shift right. The increment will not exceed 2 ∗(k −2), where k is
the number of tapes.

5.1 Deterministic Turing machines
|
115
As a result, simulation of n transitions of a multitape Turing machine requires not less
than
f(n) = 3 + 7 + 11 + ⋅⋅⋅+ (4 ∗n −1) + n ∗(k −2)
= 4 + 8 + ⋅⋅⋅+ 4 ∗n + n ∗(k −3) = 2 ∗n2 + (k −1) ∗n
transitions of simulating machine. This number could be estimated: 2 ∗n2 ≤f(n) ≤
3 ∗n2 for n big enough. Therefore, we can say that simulation increases cost of com-
putation with square.
Configuration of a multitape Turing machine
Definition of a configuration (step description) of a multitape machine must consider
contents of several tapes and one state of control unit:
Definition 5.16. A configuration (step description) of a multitape Turing machine with
k-tapes is the following sequence of symbols:
(α1
1, α2
1, . . . , αk
1) q (α1
2, α2
2, . . . , αk
2)
where:
–
q is a state of control unit;
–
αi
1 is a sequence of symbols for ith tape in cells preceding the head (like in Turing
machines with two-way infinite tape);
–
αi
2 is a sequence of symbols for ith tape in cells from the head’s right (like in Turing
machines with two-way infinite tape).
In practice, step description will be shown in two ways presented below in exam-
ples:
–
state of the control unit plays a role of tape markers, contents of tapes is split for
left and right sequences of symbols and contents of tapes may be shifted indepen-
dently each from others:
X1
1X1
2X1
3
X2
1X2
2X2
3X2
4X2
5
q
X1
4X1
5X1
6X1
7
X2
6X2
7
BBX3
1 X3
2X3
3X3
4
–
state of the control unit is placed at the beginning, contents of tapes are fixed
together, heads’ positions are underscored:
q
X1
1X1
2X1
3X1
4X1
5X1
6X1
7
X2
1X2
2X2
3X2
4X2
5X2
6X2
7
BBX3
1 X3
2X3
3X3
4

116
|
5 Turing machines
Example 5.4. Design a Turing machine computing sum of two binary numbers.
Solution. Let us build a 2-tape Turing machine. According to the general assump-
tion, input data, two nonnegative binary numbers separated with the blank symbol,
is stored on the first tape (unlike it is assumed in Remark 5.2, for the sake of limiting
the size of the transition function, we use the blank symbol to separate input num-
bers instead of extra tape symbol). The head of the first tape is placed over the cell
holding the first (most significant) digit of the first number. The following algorithm
is applied:
–
the first number is moved to the second tape;
–
heads of both tapes are placed over the last (least significant) digits of both num-
bers;
–
addition is done for consecutive digits of both numbers from the least significant
to the most significant digits:
–
digits read by heads and carry are added, the result is stored by the head of
the first tape, a carry is remembered in states;
–
the result of addition is stored by the head of the first tape, the head of the
second tape stores the blank symbol; then both heads shift left;
–
the process of adding is terminated as soon as both heads read the blank
symbol and there is no carry.
A 2-tape Turing machine with halting accepting state implementing this algorithm is
as follows:
M = ({q0, q1, q0
2 , q1
2, qA}, {0, 1}, {0, 1, B}, δ, q0, B, {qA})
Both tapes have the same alphabet Γ = {0, 1, B}. The transition function is given in
Table 5.3.
A detailed description of the machine is given in the form of comments to the
purpose of states:
–
q0 – the first number is moved to the second tape;
–
q1 – the head of the first tape is passed to the end of data, heads of both tapes are
placed at the least significant digits of numbers stored on tapes;
–
q0
2 , q1
2 – superscripts are used to remember carry, adding both numbers, heads of
both tapes are simultaneously passed left, corresponding digits of numbers and
the carry are added step by step.
Note that the Turing machine designed above can be formally turned to a one with
the stop condition. It can be done by filling empty entries of Table 5.3 with a transition
to the halting rejecting state.

5.1 Deterministic Turing machines
|
117
Table 5.3: The transition function of the Turing machine considered in Example 5.4.
δ
0
1
B
B
B
B
B
0
1
B
q0
(q0, B
0, R
R)
(q0, B
1, R
R)
(q1, B
B, R
S)
q1
(q1, 0
B, R
S)
(q1, 1
B, R
S)
(q0
2, B
B, L
L)
q0
2
(q0
2, 0
B, L
L)
(q0
2, 1
B, L
L)
(q0
2, 0
B, L
L)
(q0
2, 1
B, L
L)
(qA, B
B, R
S)
q1
2
(q0
2, 1
B, L
S)
(q1
2, 0
B, L
S)
(q0
2, 1
B, L
L)
(q1
2, 0
B, L
L)
(q0
2, 1
B, L
S)
δ
0
0
1
1
0
1
0
1
q0
q1
q0
2
(q0
2, 0
B, L
L)
(q0
2, 1
B, L
L)
(q0
2, 1
B, L
L)
(q1
2, 0
B, L
L)
q1
2
(q0
2, 1
B, L
L)
(q1
2, 0
B, L
L)
(q1
2, 0
B, L
L)
(q1
2, 1
B, L
L)
Finally, let us simulate computation for given numbers 1101 and 101:
q0
1101B101 ≻1q0
101B101 ≻11q0
01B101 ≻110q0
1B101
≻1101q0
B101 ≻1101q1
101 ≻
1
1101q1
01 ≻10
1101q1
1 ≻101
1101q1
≻10
110q0
2
1
1 ≻1
11q1
2
00
0
≻1q0
2
110
1
≻q1
2
B010
1
≻q1
2
B0010
≻q0
2
B10010 ≻qA
10010
5.1.8 Programming with Turing machines
Implementation of algorithms in the form of computer programs employs such meth-
ods as modularity or top-down programming. Programming techniques use collabo-
rating units such as procedures or functions. As mentioned before, Turing machines
can serve as tools for solving problems. They can be seen as computational tools,
with transition function being a low-level programming language. Indeed, Turing ma-

118
|
5 Turing machines
chines can be organized as collaborating units in solving problems. Remark 5.1 allows
for easy data passing from one machine to another.
An example of linking Turing machines is given in Example 5.3. The solution is
an example of three collaborating Turing machines: a machine shifting input data
right and inserting the guard symbol, a given machine with guard, a machine shifting
output data right and removing the guard symbol.
Example 5.5. Design a Turing machine computing product of two natural numbers.
Solution. We assume that factors are positive numbers. This assumption permits
avoiding some details and not increase the number of states.
The product of two natural numbers will be computed as multiple additions of
one factor, with another factor being a counter of additions. The solution is based on
three Turing machines, which are finally joined into the final solution. The designed
machines are two-tape machines with the same input alphabet. The final Turing ma-
chine has two tapes and one transition table with three groups of states, one group
for each component machine. However, the below transition table of the joined Tur-
ing machine stays split into three separated parts.
The first Turing machine
MP = (QP = {p1, p2, pL, p2
L, r0}, {0, 1}, {0, 1, B}, δP, p1, B, {r0})
with transition table given in Table 5.4, prepares input data for further processing.
Input data consists of two positive binary numbers separated with the blank symbol
(what breaks assumptions of Remark 5.2 that blank symbol cannot be placed between
nonblank ones, but significantly reduces the size of transition tables of designed com-
ponent machines). This machine makes the copy of input data on the second tape,
erases the all but last symbols of the second input argument, that is, replaces these
symbols with the blank symbol. The last symbol of the second input argument (right-
most nonempty symbol) is with one. An input configuration of tapes is shown in Fig-
ure 5.9(a). An output configuration of this machine is shown in Figure 5.9(b).
A few comments on states’ purpose to clarify the design:
–
p1 – copy the first argument (the first factor of the computed product) to the second
tape, shifting both heads right, the original first argument (on the first tape) is an
initial value of the result;
–
p2 – move the second argument (the second factor of the product) to the second
tape, shifting both heads right;
–
pL – initialize the additions’ counter at the first tape with 1, that is, 1 is stored in
the cell of the rightmost not blank input symbol (it keeps compatibility with the
initial value of the result);
–
q2
L – shift both heads Left to place them between factors;
–
r0 – terminate computation and accept output data (when machines are joined,
this is initial state of the second Turing machine).

5.1 Deterministic Turing machines
|
119
Table 5.4: The transition function of the Turing machine MR preparing input data for further process-
ing.
δP
0
1
B
B
B
B
B
0
1
B
p1
(p1, 0
0, R
R)
(p1, 1
1, R
R)
(p2, B
B, R
R)
p2
(p2, B
0, R
R)
(p2, B
1, R
R)
(pL, B
B, L
L)
pL
(p2
L, 1
0, L
L)
(p2
L, 1
1, L
L)
p2
L
(p2
L, B
0, L
L)
(p2
L, B
1, L
L)
(r0, B
B, S
S)
δP
0
0
1
1
0
1
0
1
p1
p2
pL
p2
L
Figure 5.9: Tapes’ configurations of Turing machines built in Example 5.5.
The second Turing machine
MR = (QR = {r0, rP, r1
P, q1
L, q2
L, r1
D, r2
D, rA, q0}, {0, 1}, {0, 1, B}, δR, r0, B, {rA})
checks if the finalresultis computed,that is,if the additions’counteron thefirst tape is
equal to the second factor stored on the second tape. If not, then the additions’ counter
is incremented by one and computation is passed to the third machine in order to add
the first factor to the final result. The transition table of this machine is provided in
Table 5.5

120
|
5 Turing machines
Table 5.5: The transition function of the Turing machine MR checking whether the final result is com-
puted.
δ
0
1
B
B
B
B
B
0
1
B
r0
(rP, B
B, R
R)
rP
(r1
P, B
0, R
R)
(r1
P, B
1, R
R)
(r2
L , B
B, L
L)
r1
P
(r1
P, B
0, R
R)
(r1
P, B
1, R
R)
(r1
D, B
B, L
L)
r2
L
(r1
L , B
B, L
L)
r1
L
(r1
L , 0
B, L
L)
(r1
L , 1
B, L
L)
(rA, B
B, R
S)
r1
D
(r2
D, 1
0, L
L)
(r2
D, 1
1, L
L)
(q0, B
B, L
L)
r2
D
(r2
D, B
0, L
L)
(r2
D, B
1, L
L)
(q0, B
B, L
L)
δP
0
0
1
1
0
1
0
1
r0
rP
(rP, 0
0, R
R)
(r1
P, 0
1, R
R)
(r1
P, 1
0, R
R)
(rP, 1
1, R
R)
r1
P
(r1
P, 0
0, R
R)
(r1
P, 0
1, R
R)
(r1
P, 1
0, R
R)
(r1
P, 1
1, R
R)
r2
L
(r2
L , B
B, L
L)
(r2
L , B
B, L
L)
r1
L
d (r1
L , 0
B, L
L)
(r1
L , 0
B, L
L)
(r1
L , 1
B, L
L)
(r1
L , 1
B, L
L)
r1
D
(r2
D, 1
0, L
L)
(r2
D, 1
1, L
L)
(r1
D, 1
0, L
L)
(r1
D, 1
1, L
L)
r2
D
(r2
D, 0
0, L
L)
(r2
D, 0
1, L
L)
(r2
D, 1
0, L
L)
(r2
D, 1
1, L
L)
Input data is obtained from the first machine for the first time; then it is obtained from
the third machine in order to check the number of additions. Output data is passed to
the third machine to add the first factor again if it contributes to final results less than
the second factor. Otherwise, the product has been computed. An input configuration
of tapes of this machine is shown in Figure 5.9(b) and Figure 5.10(b). An output con-
figuration of this machine is shown in Figure 5.10(a). The final configuration is shown
in Figure 5.10(c).
A detailed description of the machine is given in the form of comments to the
purpose of states.

5.1 Deterministic Turing machines
|
121
Figure 5.10: Tapes’ configurations of Turing machines built in Example 5.5.
–
r0 – initiate computation, shift heads right, switch to the state rP in order to begin
comparing the current value of additions’ counter with the second factor;
–
rP – move heads right, comPare the current value of additions’ counter with the
second factor;
–
r2
L – comparing in the state rP shows that the current value of additions’ counter
and the second factor are equal. Therefore, move heads Left, clear the addition
counter and the 2nd factor;
–
r1
L – continue passing heads Left, clear the 1st factor on the second tape, place the
head of the first tape on the leftmost output symbol;
–
r1
P – comParing shows that the current value of additions’ counter is less than
the second factor, continue passing heads right, in order to increment the counter
by 1;
–
r1
D – aDding 1 to the additions’ counter, heads passes left;
–
r2
D – continue passing heads left;
–
q0 – pass control to the machine computing sum of two numbers.
The third Turing machine
MQ = (QQ{q0, q1, qR, r0}, {0, 1}, {0, 1, B}, δQ, q0, B, {r0})
with transition table given in Table 5.6 adds the first factor (kept on the second tape)
increasing the result on the first tape. Input data of this machine consists of the tem-
porary result and the first factor copied to the second tape by the first Turing machine.
An input configuration of tapes is shown in Figure 5.10(a). An output configuration of
this machine is shown in Figure 5.10(b).

122
|
5 Turing machines
Table 5.6: The transition function of the Turing machine adding the first factor to the current result.
δ
0
1
B
B
B
B
B
0
1
B
q0
(q0, 0
B, L
L)
(q0, 1
B, L
L)
(q0, 0
0, L
L)
(q0, 1
1, L
L)
(qR, B
B, R
R)
q1
(q0, 1
B, L
S)
(q1, 0
B, L
S)
(q0, 1
0, L
L)
(q1, 0
1, L
L)
(q0, 1
B, L
S)
qR
(qR, 0
B, R
R)
(qR, 1
B, R
R)
(r0, B
B, S
S)
δP
0
0
1
1
0
1
0
1
q0
(q0, 0
0, L
L)
(q0, 1
1, L
L)
(q0, 1
0, L
L)
(q1, 0
1, L
L)
q1
(q0, 1
0, L
L)
(q1, 0
1, L
L)
(q1, 0
0, L
L)
(q1, 1
1, L
L)
qR
(qR, 0
0, R
R)
(qR, 0
1, R
R)
(qR, 1
0, R
R)
(qR, 1
1, R
R)
Let us make a few comments on states’ purpose to clarify the design of the transition
table shown in Table 5.6:
–
q0, q1 – add numbers, heads simultaneously move left, corresponding digits of
added numbers and the carry are added step by step;
–
qR – move heads right and place right of added numbers;
–
r0 – terminate computation and accept output data (when machines are joined,
this state passes control to the second Turing machine).
The Turing machine linking the three ones discussed above is such as described be-
low:
M = (QP ∪QR ∪QQ, {0, 1}, {0, 1, B}, δ, p1, B, {rA})
where the transition function δ brings together transition functions δP, δR and δQ of
branched Turing machines.
5.2 Nondeterministic Turing machines
Nondeterministic model is a very important modification of Turing machines. The
equivalence of deterministic and nondeterministic Turing machines is the most im-
portant conclusion drawn from the discussion in this chapter.
As mentioned before, for any configuration of deterministic Turing machines, at
most, one transition could be done. Unlike nondeterministic Turing machines allow

5.2 Nondeterministic Turing machines
|
123
for a choice between several transitions for some (or all) configurations. This means
that the transition function may yield a set of possible transitions. In this section,
we provide a definition of nondeterministic Turing machine in basic model. Then we
prove the equivalence between nondeterministic Turing machines in basic model and
multitape deterministic Turing machines. A study on the equivalence of different mod-
els of nondeterministic Turing machines is similar to an analogous study on determin-
istic Turing machines. For that reason, we skip over such a discussion.
Definition 5.17. Nondeterministic Turing machine in basic model is a system
M = (Q, Σ, Γ, δ, q0, B, F, C)
with components as follows:
–
δ is the transition function: δ : Q × Γ →⋃∞
k=0 (Q × Γ × {L, R})k
where (Q × Γ × {L, R})0 stands for an undefined value of transition function and
(Q × Γ × {L, R})k denotes the set of values of transition function consisting of k
3-tuples being transition descriptions (p, X, D) ∈Q × Γ × {L, R}.
–
descriptions of other components are given in Definition 5.1.
Note that values of transition function are sets of several transitions (descriptions
of) rather than one transition (like it was in the case of deterministic Turing machines),
that is, we may have a set of k possible transitions as a value of the transition func-
tion:
δ(q, X) = {(p1, Y1, D1), (p2, Yq, D2), . . . , (pk, Yk, Dk)}
We will informally use a term that transition function yields k values.
Once the empty set or a one-element set {(p, Y, D)} is a value of the transition func-
tion, then the transition is performed like for a deterministic machine, that is:
1.
if the transition function yields the empty set, that is, it is undefined, then ma-
chine falls into infinite computation;
2.
for a value {(p, Y, D)} of the transition function, the control unit switches to state p,
the head stores the symbol Y and shifts in direction D.
However, when the transition function yields a set of k > 1 values, then a transition
could be explicated as follows:
3.
a choice is made between possible k values;
4.
for the chosen value, a transition is made like for deterministic Turing machine;
5.
it is assumed that a transition chosen in the first point leads to such a computation
(in the sense of deterministic Turing machines), which terminates in an accepting
state if such a computation exists.

124
|
5 Turing machines
A question can be raised, how to hold the assumption of point 5, when a choice is
made. Nondeterminism does not answer this question; it just assumes such choices.
Remark 5.4. The assumption made in point 5 of the above description is the funda-
mental assumption of nondeterminism. It creates an interpretation of nondetermin-
ism, assuming that computation is a sequence of configurations achieved with correct
choices between possible transitions.
Remark 5.5. Another interpretation of nondeterminism is reasonably practical. When
the transition function yields a set of k > 1 values, the machine creates k copies of
itself. Then each copy makes a transition matching to one value and continues its
own computation. The machine accepts if and only if such a copy is created during
computation, which terminates its own computation in an accepting state.
In the spirit of the last interpretation, a nondeterministic Turing machine would
be interpreted as a group of Turing machines. This group includes the original ma-
chine at the start of computation and might be enlarged during computation. Every
machine of this group creates copies of itself, as many copies as the number of values
yielded by the transition function. Then each copy makes a transition, as described
above. Note that each copy is doing computation like a deterministic Turing machine.
Let us notice that the configuration (step description) of a nondeterministic Turing
machine is exactly the same as for a deterministic one (of the same model). However,
notions of transition relation and computation must be redefined for nondeterministic
Turing machines. Below, we discuss a (practical) interpretation of computation in the
form of a tree representing all possible choices of values of a transition function.
Definition 5.18. A pair of configurations of a nondeterministic Turing machine is in
the transition relation if and only if the second configuration can be derived from the
first one by application of a transition yielded by the transition function; cf.
Defini-
tion 5.3.
Definition 5.19. A computation of a Turing machine M = (Q, Γ, Σ, δ, q0, B, F, C) is a tree
such that:
–
its nodes are labeled by configurations of the machine;
–
its root is labeled by the initial configuration;
–
for any node ηp, its each child ηc is related to it in the transition relation, that is,
ηp ≻ηc.
Note that a computation tree is a k-tree, where k is the maximal number of values
yielded by the transition function for given arguments. We say that degree of nonde-
terminism is k.
The computation of a nondeterministic Turing machine is a tree, which may have
finite paths from the root to leaves and infinite paths beginning in the root. Interpret-
ing a nondeterministic Turing machine as a group of its copies, we can associate finite

5.2 Nondeterministic Turing machines
|
125
paths with corresponding machine copies. Based on this interpretation, we can say
that a nondeterministic Turing machine accepts its input if and only if there is a copy
of the machine, which terminates its computation in an accepting state.
Definition 5.20. A nondeterministic Turing machine accepts its input if and only if
the computation tree has a path from the root to a leaf labeled by a configuration, for
which the control unit is in an accepting state.
Example 5.6. Design a Turing machine accepting the language L = {ww : w ∈{0, 1}∗}.
Solution. We design a nondeterministic Turing machine with halting accepting state
and with guard. It works according to an algorithm briefly described as follows:
1.
remember (store information in states) and mark the first symbol as belonging to
the first-half of the input word. This symbol begins the first-half of the input word;
2.
pass the head right, looking for the beginning of the second-half of the input word.
Since the second-half of the word must begin with the symbol matching the re-
membered one, only such symbols are considered during the pass. When such a
symbol is met, the machine makes a nondeterministic choice:
a.
either it is in the first-half of the input word, or
b.
this symbol begins the second-half of the input word;
3.
continue the pass for point 2a;
4.
mark the symbol as a symbol of the second-half of the input word for point 2b;
5.
shift left the head to the first unmarked symbol of the first-half of the input word;
6.
remember and mark the symbol as belonging to the first-half of the input word;
7.
pass the head right over the first-half and over symbols marked as belonging to
the second-half of the input word;
8.
mark the symbol as belonging to the second-half of the input word if it matches
the remembered one;
9.
perform points 5–8 as long as there are unmarked symbols in both halves,
10. accept if and only if both parts are emptied during the same pass left to right;
11. reject in any other case.
The following nondeterministic Turing machine implements the above algorithm:
M = (Q, Σ, Γ, δ, q0, B, #, {qA})
where:
–
Q = {q0, q0
1 , q1
1, q1, q0
2 , q1
2, q2, q0
3, q1
3, q3, q4, q5, qA, } – the set of states;
–
Γ = {0, 1, X, Y, #, B} – the tape alphabet;
–
Σ = {0, 1} – the input alphabet;
–
δ – the transition function given in Table 5.7.

126
|
5 Turing machines
Table 5.7: The transition function of the nondeterministic Turing machine designed in Example 5.6.
δ
0
1
X
Y
#
B
q0
(q0
0, X, R)
(q1
0, X, R)
(q5, B, L)
q0
0
(q0
0, 0, R)
(q1, Y, L)
(q0
0, 1, R)
q1
0
(q1
0, 0, R)
(q1
0, 1, R)
(q1, Y, L)
q1
(q1, 0, L)
(q1, 1, L)
(q2, X, R)
(q1, Y, L)
q2
(q0
2, X, R)
(q1
2, X, R)
(q4, Y, R)
q0
2
(q0
2, 0, R)
(q0
2, 1, R)
(q0
3, Y, R)
q1
2
(q1
2, 0, R)
(q1
2, 1, R)
(q1
3, Y, R)
q0
3
(q1, Y, L)
(q0
3, Y, R)
q1
3
(q1, Y, L)
(q1
3, Y, R)
q4
(q4, Y, R)
(q5, B, L)
q5
(q5, B, L)
(q5, B, L)
(qA, #, R)
Note that the transition function is nondeterministic because some entries of its table
hold more than one value. For this Turing machine, the maximal number of values
yielded by the transition function is 2, that is, the degree of nondeterminism is 2.
Let us comment on the construction of the machine describing the purpose of
states:
–
q0 – check if the input word is empty. If not, remember the input symbol in the
state q0
0 or q1
0 and mark with X the first symbol of the word as belonging to the
first-half of the word;
–
q0
0 and q1
0 – pass the head right over the first-half of the word, nondeterministically
find the first symbol of the second-half of the word matching the remembered one,
mark the found symbol with Y as belonging the second-half of the word, switch
to q1 to pass the head left;
–
q1 and q2 – pass the head to the first unmarked symbol of the first-half of the word;
–
q2 – the next symbol of the word, as belonging to the first-half of the word, is
remembered in the state q0
2 or q1
2 and marked with X;
–
q0
2 and q1
2 – the head is passed right over the first-half of the word when the first
symbol marking the second-half of the word is found, states q0
2 or q1
2 are changed
to q0
3 or q1
3 and pass of the head to the right is continued. Note that the middle of
the word has already been marked and there is no need for using nondetermin-
ism;
–
q0
3 and q1
3 – the head is passed over the marked part of the second-half of the
word if the first unmarked symbol of the second-half of the word matches the
remembered one, the matching symbol is marked, the control unit switches to q1
and the head shifts left;

5.2 Nondeterministic Turing machines
|
127
–
q4 – when there are no unmarked symbols in the first-half of the word, the head
is passed right to check if there is no unmarked symbol in the second-half of the
word;
–
q5 – when both halves match, the head is passed left cleaning the tape (cleaning
procedure), computation is terminated and input accepted;
–
infinite computation begins in all configurations not considered above.
An example of computation is shown in Figure 5.11. There are four paths in the compu-
tation tree. Three of them ended with the symbol of infinity which stands for infinite
computation. The second path (from left) stands for computation terminated in the
halting accepting state. Therefore, the input word is accepted by the machine.
Figure 5.11: Computation of the nondeterministic Turing machine of Example 5.6 for the input word
w = 0000.
For the sake of clarity, the transition table of the designed machine exposed in Exam-
ple 5.7 has empty entries. The machine can be turned into a Turing machine with the
stop property. It requires introducing the halting rejecting state and a few states for the
cleaning procedure. In the transition table, all empty entries should be filled in with
a switch to cleaning procedure (different than the one used for accepted word) and

128
|
5 Turing machines
then switching to the halting rejecting state. On the other hand, configurations of the
machine corresponding to empty entries of the transition table can never be reached.
Example 5.7. Rebuild the Turing machine designed in Example 5.6 to the one having
the stop property.
Solution. The following nondeterministic Turing machine with halting states and
with guard implements the above algorithm:
M = (Q, Σ, Γ, δ, q0, B, #, {qA}, {qR})
where:
–
Q = {q0, q0
1 , q1
1, q1, q0
2 , q1
2, q2, q0
3, q1
3, q3, q4, q5, q6, q7, qA, qR} – the set of states;
–
Γ = {0, 1, X, Y, #, B} – the tape alphabet;
–
Σ = {0, 1} – the input alphabet;
–
δ – the transition function is given in Table 5.8.
Table 5.8: The transition function of the nondeterministic Turing machine with halting states, cf.
Example 5.7. Empty entries of this table hold (q6, B, R), which is not displayed for the sake of clarity.
δ
0
1
X
Y
#
B
q0
(q0
0, X, R)
(q1
0, X, R)
(q5, B, L)
q0
0
(q0
0, 0, R)
(q1, Y, L)
(q0
0, 1, R)
q1
0
(q1
0, 0, R)
(q1
0, 1, R)
(q1, Y, L)
q1
(q1, 0, L)
(q1, 1, L)
(q2, X, R)
(q1, Y, L)
q2
(q0
2, X, R)
(q1
2, X, R)
(q4, Y, R)
q0
2
(q0
2, 0, R)
(q0
2, 1, R)
(q0
3, Y, R)
q1
2
(q1
2, 0, R)
(q1
2, 1, R)
(q1
3, Y, R)
q0
3
(q1, Y, L)
(q0
3, Y, R)
q1
3
(q1, Y, L)
(q1
3, Y, R)
q4
(q4, Y, R)
(q5, B, L)
q5
(q5, B, L)
(q5, B, L)
(qA, #, R)
q6
(q7, B, L)
q7
(q7, B, L)
(q7, B, L)
(q7, B, L)
(q7, B, L)
(qR, #, R)
(q7, B, L)
Note that the new state qR is the halting rejecting state and new states q6 and q7 are
used in the cleaning procedure for the rejected input word.
Proposition 5.8. The class of nondeterministic Turing machines is equivalent to the
class of deterministic Turing machines.

5.2 Nondeterministic Turing machines
|
129
Proof. We will show that for a given Turing machine of one type, an equivalent ma-
chine of another type can be designed.
Note that a deterministic Turing machine in basic model is also a nondeterministic
one (with the maximal number of values yielded by transition function not greater
than 1, that is, with the degree of nondeterminism equal to one). For that reason and
due to the equivalence of different types of Turing machines, any deterministic Turing
machine is equivalent to some nondeterministic one.
Now, for a given nondeterministic Turing machine in basic model, we can design
an equivalent deterministic multitype Turing machine.
Let us present an idea of designing a deterministic Turing machine equivalent to
a nondeterministic one. The idea is based on a deterministic simulation of a compu-
tation of a nondeterministic Turing machine. It can be briefly presented as follows:
–
a nondeterministic Turing machine accepts its input if and only if the computation
tree has a node, which terminates computation in an accepting state;
–
a computation tree is a k-tree, where k is the maximal number of values yielded
by the transition function, that is, degree of nondeterminism is k;
–
a breadth-first tree searching algorithm, which starts searching from the root and,
then visits nodes level by level, will eventually visit a node, which terminates com-
putation in an accepting state;
–
a simulation of a computation of a nondeterministic machine is rooted in the
breadth-first searching the computation tree. This simulation investigates nodes
visited by breadth-first search;
–
for each visited node, the computation from the root to this node is reproduced.
If this computation terminates in an accepting state, then the simulation is termi-
nated with acceptation;
–
otherwise, breadth-first searching is continued and a next node is investigated.
Note that the above method replicates transitions many times: closer a node to the
root, more replications of the path from the root to this node is done. The time com-
plexity of such a simulation is huge. Unfortunately, a more efficient deterministic
method to simulate nondeterminism is not known.
The above idea can be realized by a 3-tape deterministic Turing machine. Let us
assume the degree of nondeterminism is r. The simulation algorithm could be briefly
described as follows:
1.
an input of a nondeterministic machine is stored on the first tape;
2.
the simulating deterministic machine systematically generates all possible se-
quences of length being successive natural numbers 0, 1, 2, . . . . Elements of gener-
ated sequences are natural numbers from the interval [1, 2, . . . , r]. Each generated
sequence is stored on the second tape;
3.
for each generated sequence (i1, i2, . . . , in) the following computation is done:
–
the input is copied from the first tape to the third one;

130
|
5 Turing machines
–
a path from the computation tree of a nondeterministic Turing machine is
simulated. The path begins in the root of the computation tree and is defined
by the sequence (i1, i2, . . . , in):
–
length of the path is equal to the length of the sequence, that is, n transi-
tions are simulated;
–
the successive l-th transition is defined by the il-th value of the transition
function of the nondeterministic Turing machine. If the transition func-
tion yields less than k values for given arguments and the il-th value has
no corresponding transition, then the simulation is terminated and the al-
gorithm goes to generate and investigate the next sequence of numbers,
as described in point 2;
–
if, for a given generated sequence, a node terminating computation in accept-
ing state of the nondeterministic Turing machine is reached, then simulating
machine halts its computation and accepts the input. Otherwise, the algo-
rithm generates and investigates the next sequence of numbers, as described
in point 2.
The above simulation algorithm describes a deterministic Turing machine, which is
equivalent to the simulated nondeterministic Turing machine.
5.3 Linear bounded automata
We can observe that the Turing machine built, for instance, in Example 5.6 exploits
only this part of its tape during computation, which was used to store input data. We
can consider and explore a class of Turing machines with this restriction. Indeed, such
cases are already considered: Turing machines with stop property, which compute
only on the part of their tape storing input word, have been studied. Such subclass of
Turing machines is distinguished and it is called linear bounded automata. It occurs
that linear bounded automata are equivalent to the class of context-sensitive gram-
mars; that is, they accept the class of context-sensitive languages. The formal defini-
tion of linear bounded automata is as follows.
Definition 5.21. A linear bounded automaton in basic model is a Turing machine with
the stop property:
M = (Q, Σ, Γ, δ, q0, B, #, &, F, C)
where:
–
#, & are the left and the right guards, #, & ∈Γ, #, & ∉Σ;
–
# q0 a1 a2 . . . an & is the initial configuration, where a1 a2 . . . an is input data;
–
the transition function cannot yield a value that allows the head:

5.3 Linear bounded automata
|
131
–
to replace the left guard symbol with any other symbol or to make a shift left
when it reads the left guard symbol; that is, it may take only the following
values:
δ(q, #) = {(p1, #, R), (p2, #, R), . . . , (pk, #, R)} for any q, p1, p2, . . . , pk ∈Q;
–
to replace the right guard symbol with any other symbol or to make a shift
right when it reads the right guard symbol, that is, it may take only the fol-
lowing values:
δ(q, &) = {(p1, &, L), (p2, &, L), . . . , (pk, &, L)} for any q, p1, p2, . . . , pk ∈Q;
–
to store a guard symbol in any cell besides these holding them, that is,
if X ∈Γ −{#, &} and q ∈Q,
then δ(q, X) = {(p1, Y1, D1), (p2, Y2, D2), . . . , (pk, Yk, Dk)},
where Y1, Y2, . . . , Yk ∈Γ −{#, &}, p1, p2, . . . , pk ∈Q and
D1, D2, . . . , Dk ∈{L, R};
–
other components are as in Definition 5.1.
Notice that we can talk about two classes of linear bounded automata: determin-
istic and nondeterministic ones.
The discussion on varieties of Turing machines could hardly be adapted to the
analysis of linear bounded automata. First of all, the notion of guards is directly
adapted. We can talk about halting accepting and rejecting states and about a multi-
track tape. Furthermore, these notions are easily adaptable to linear bounded au-
tomata. However, the nature of most models of Turing machines does not match the
character of linear bounded automata. For instance, one-way and two-way infinite
tape do not have counterparts in the class of linear bounded automata. Also, we do
not discuss multitape linear bounded automata, though, formally, we can discuss a
multitape restricted Turing machine such that its all tapes have a length equal to the
length of input data limited by the left and the right guard.
There is a model, which raised the name of linear bounded automata. In this
model length of the tape is bounded by a linear function of the length of the input
data or, equivalently, is a multiply of the length of the input data. In these automata,
the beginning and the end of the tape are marked by guards, as in basic model.
Proposition 5.9. The following classes of deterministic linear bounded automata are
equivalent:
1.
in basic model;
2.
with halting states;
3.
with a multitrack tape;
4.
with the length of the tape bounded by a linear function.
The same classes of nondeterministic linear bounded automata are equivalent as well.
Proof. Proof of equivalence of these classes is quite similar to proofs of equivalence of
corresponding classes of Turing machines.

132
|
5 Turing machines
Hint. consider automaton with k-tracks tape of lengths equal to the length of an
input word, which is equivalent to an automaton with one track tape k times longer
than an input word.
Deterministic versus nondeterministic linear automata
It is trivial that for each deterministic linear bounded automaton, there exists an equiv-
alent nondeterministic one. It is sufficient to turn (single) values of deterministic tran-
sition function to sets being values of nondeterministic transition function includ-
ing every single value into the corresponding set. With regard to opposite equiva-
lence, we do not know if there exists a deterministic linear bounded automaton for
a given nondeterministic one. We know that it is possible to design a deterministic
square bounded automaton (using part of tape of length proportional to square of in-
put length), which is equivalent to a nondeterministic linear automaton.
Example 5.8. Design a linear bounded automaton accepting the language
L = {w ∈{a, b}∗: #a w = n, #b w = 2n and n ≥0}
Solution. Let us provide a linear bounded automaton with halting state:
M = (Q, Σ, Γ, δ, q0, Λ, #, &, {pA}, {pR})
where:
–
Q = {q0, qA, q1, q0
B, q1
B, q2, q3, pA, pR} – the set of states;
–
Γ = {a, b, A, B, #, &, Λ} – the tape alphabet;
–
Σ = {a, b} – the input alphabet
–
δ – the transition function given in Table 5.9;
–
Λ – the blank symbol;
–
pA – the accepting state.
Table 5.9: The transition function of the linear bounded automata designed in Example 5.8. Empty
entries of the transition table keep transitions to the halting rejecting state. These entries are left
empty due to the sake of clarity of the transition table.
δ
a
b
A
B
#
&
Λ
q0
(qA, A, R)
(q0, b, R)
(q0, A, R)
(q0, B, R)
(q2, &, L)
qA
(qA, a, R)
(qA, b, R)
(qA, A, R)
(qA, B, R)
(q1, &, L)
q1
(q1, a, L)
(q1
B, b, L)
(q1, A, L)
(q1, B, L)
q1
B
(q1
B, a, L)
(q0
B, B, L)
(q1
B, A, L)
(q1
B, B, L)
q0
B
(q0
B, a, L)
(q1
B, b, L)
(q0
B, A, L)
(q0
B, B, L)
(q0, #, R)
q2
(q3, Λ, L)
(q2, Λ, L)
(q2, Λ, L)
q3
(q3, Λ, L)
(q3, Λ, L)
(pA, #, R)

5.3 Linear bounded automata
|
133
An algorithm applied in the automaton is as follows:
–
the head is passed right and left along the tape,
–
during the walk right one letter a is marked;
–
during the walk left, half of the letters b are marked; that is, the number of
letters b is divided by 2,
–
passing the head right and left is continued until all letters a are marked, as a
result n letters a and 2n −1 letters b are marked;
–
the final pass right checks that there are no letters a, the final pass left checks that
there is only one letter b.
Let us comment on the construction of the automaton describing the purpose of states:
–
q0 – pass the head right from the beginning of the tape to its end, mark the first
letter a met and indicate it switching to qA; if no letter a is met, begin cleaning
process leading to the acceptation of input data;
–
qA – continue passing the head right, memorize that the letter a was marked;
–
q1 – pass the head left from the end of the tape to its beginning, look for the first
letter b, remember it switching to q1
B;
–
q1
B – continue passing the head left, look for the second letter b, mark it when met
and remember it switching to q0
B;
–
q0
B – continue passing the head left, look for the next letter b, remember it switch-
ing to q1
B;
–
q1, q1
B, q0
B – count pairs of letters b, mark every second letter b (divide number of
letters b by 2);
–
q2 and q3 – pass the head left from the end of the tape to its beginning, mark the
last letter b, terminate computation switching to the accepting state pA.
Computations for w1 = bb ∉L, w2 = b ∈L and w3 = bbaabb ∈L are shown below. Note
that for the word w1 the automaton rejects input data and that a cleaning procedure is
not implemented due to the sake of clarity:
–
# q0 b b & ≻# b q0 b & ≻# b b q0 & ≻# b q2 b & ≻# q3 b Λ & ≻qR # Λ Λ & ≻
# Λ qR Λ &
–
# q0 b & ≻# b q0 & ≻# q2 b & ≻q3 # Λ & ≻# pA Λ &
–
# q0 b b a a b b & ≻# b q0 b a a b b & ≻# b b q0 a a b b & ≻# b b A qA a b b & ≻
# b b A a qA b b & ≻# b b A a b qA b & ≻# b b A a b b qa & ≻# b b A a b q1 b & ≻
# b b A a q1
B b b & ≻# b b A q0
B a B b & ≻# b b q0
B A a B b & ≻# b q0
B b A a B b & ≻
# q1
B b b A a B b & ≻q0
B # B b A a B b & ≻# q0 B b A a B b & ≻# B q0 b A a B b & ≻
# B b q0 A a B b & ≻# B b A q0 a B b & ≻# B b A A qA B b & ≻2 # B b A A B b qA & ≻
# B b A A B q1 b & ≻# B b A A q1
B B b & ≻3 # B q1
B b A A B b & ≻# q0
B B B A A B b & ≻
q0
B # B B A A B b & ≻# q0 B B A A B b & ≻6 # B B A A B b q0 & ≻# B B A A B q2 b & ≻
# B B A A q3 B Λ & ≻5 q3 # Λ Λ Λ Λ Λ Λ & ≻# pA Λ Λ Λ Λ Λ Λ &

134
|
5 Turing machines
5.4 Problems
Problem 5.1. Design a Turing machine accepting the language:
L = {w ∈{a, b, c}∗: 0 ≤#a w ≤#b w ≤#c w}
Solution. Let us provide a linear bounded automaton with accepting states:
M = (Q, Σ, Γ, δ, q, B, #, &, {pA}, {pR})
where:
–
Q = {q, qa, qb, qc, qab, qac, qbc, qabc, pL, pT, pA, pR} – the set of states;
–
pA and pR – the accepting and rejecting states;
–
Γ = {a, b, c, #, &, B} – the tape alphabet;
–
Σ = {a, b, c} – the input alphabet
–
δ – the transition function given in Table 5.10;
–
B – the blank symbol.
Table 5.10: The transition function of the linear bounded automata designed in Problem 5.1. Empty
entries of the transition table keep transitions to the halting rejecting state: (qR, B, D), where D
defines shift to the left for the head not being over # or to the right for the head not being over &.
These entries are left empty due to the sake of clarity of the transition table.
δ
a
b
c
B
#
&
q
(qa, B, R)
(qb, B, R)
(qc, B, R)
(q, B, R)
(qT , &, L)
qa
(qa, a, R)
(qab, B, R)
(qac, B, R)
(qa, B, R)
qb
(qab, B, R)
(qb, b, R)
(qbc, B, R)
(qb, B, R)
qc
(qac, B, R)
(qbc, B, R)
(qc, c, R)
(qc, B, R)
(qL, &, L)
qab
(qab, a, R)
(qab, b, R)
(qabc, B, R)
(qabc, B, R)
qac
(qac, a, R)
(qabc, B, R)
(qac, c, R)
(qac, B, R)
qbc
(qabc, B, R)
(qbc, b, R)
(qbc, B, R)
(qbc, B, R)
(qL, &, L)
qabc
(qabc, a, R)
(qabc, b, R)
(qabc, c, R)
(qabc, B, R)
(qL, &, L)
qL
(qL, a, L)
(qL, b, L)
(qL, c, L)
(qL, B, L)
(q, #, R)
qT
(qT , B, L)
(pA, #, R)
We use a linear bounded automaton with halting accepting state (which is a Turing
machine) and apply the following algorithm to solve the problem:
–
the head is passed from the beginning to the end of the tape, one letter a, b and c
are marked, then the head is passed back to the beginning of the tape;
–
the process of the previous point is continued until there are unmarked letters a,
b and c or there are letters b and c or there is only letter(s) c;

5.4 Problems
|
135
–
when no unmarked letters are found during the pass from the beginning to the
end of the tape, then the head is passed back to the beginning of the tape and
input is accepted;
–
in all other cases the automaton rejects its input.
Let us comment the construction of the automaton describing purpose of states:
–
q, qa, qb, qc, qab, qac, qbc, qabc – pass the head right from the beginning of the tape
to its end, mark one letter a, b and c with the blank symbol and remember marked
letters in a respective state;
–
qL – pass the head back to the beginning of the tape and then start passing the
head right as in the previous point;
–
continue the process of passing the head right and marking letters a, b and c until
there are unmarked letters a, b and c or unmarked letters b and c or unmarked
letter(s) c;
–
qT – pass the head back to the beginning of the tape and accept the input.
Note that the transition table may be simplified. The state qabc and the row of the table
corresponding to this state may be dropped. Instead, the value (qabc, B, R) of transition
function should be replaced by (qL, B, L).
This optimization is not done in order to keep clearness of the process of passing
the head from the beginning to the end of the tape.
And, finally, let us construct computations for words w1 = cbaabb ∉L and w3 =
bbaccc ∈L, and are shown below. Note that for the word w1, the automaton rejects
the input data. In this case, a cleaning procedure is not implemented due to the sake
of simplification of the solution.
–
# q c b a a b &
≻
# B qc b a a b &
≻
# B B qbc a a b &
≻
# B B B qabc a b &
≻2≻
# B B B a b qabc & ≻# B B B a qL b & ≻5
qL # B B B a b & ≻≻# q B B B a b & ≻3
# B B B q a b & ≻5 # B B B B qa b & ≻# B B B B B qab & ≻# B B B B pR B &
–
# q b b a c c &
≻
# B qb b a c c &
≻
# B b qb a c c &
≻
# B b B qab c c &
≻
# B b B B qabc c & ≻# X b X X c qabc & ≻# B b B B qL c & ≻5 qL # B b B B c & ≻
# q B b B B c &
≻
# B q b B B c &
≻
# B B qb B B c &
≻2
# B B B B qb c &
≻
# B B B B B qbc &
≻
# B B B B qL B &
≻5
qL # B B B B B &
≻
# q B B B B B &
≻5
# B B B B B q & ≻# B B B B qT B & ≻5 qT # B B B B B & ≻# qA B B B B B &
Problem 5.2. Design a Turing machine computing the function f(x) = 2x for a natural
numbers x.
Problem 5.3. Design a Turing machine computing the function f(x) = log2⌈x⌉for
a positive natural numbers x.
Problem 5.4. Design a Turing machine converting numbers from the unary system to
the binary positional system.

136
|
5 Turing machines
Problem 5.5. Design a Turing machine converting numbers from the binary positional
system to the unary system.
Problem 5.6. Let L1 = L(M1) and L2 = L(M2) are languages accepted by deterministic
Turing machines with the stop property M1 and M2. Build a Turing machine, which
accepts:
–
union of L1 and L2, that is, the language L = L1 ∪L2;
–
intersection of L1 and L2, that is, the language L = L1 ∩L2.
Solution. The idea of designing a required Turing machine M is simple. M runs paral-
lel computation of both machines M1 and M2 for a given input word. M accepts union
L = L1 ∪L2 if and only if at least one machine of M1 and M2 accepts the input word.
Intersection L = L1 ∩L2 is accepted by M if and only if both machines M1 and M2
accept the input word. In case that one machine terminates and another one still con-
tinues computation, the former one may perform empty transitions, that is, its state,
head position and cell contents stay unchanged. For instance, when the union of lan-
guages is computed and one machine comes to the rejecting state, it is necessary to
wait for termination of computation of another machine. Details of a design are given
below.
Assume that M1 and M2 are given Turing machines with halting states and with
two-way infinite tape
M1 = (Q1, Σ, Γ1, δ1, q1
0, B, {q1
A}, {q1
R})
M2 = (Q2, Σ, Γ2, δ2, q2
0, B, {q2
A}, {q2
R})
where Q1 ∩Q2 = ⌀and Γ1 ∩Γ2 = {B}.
A two-tape Turing machine is designed as follows:
M = (Q1 × Q2, Σ, Γ1 ∪Γ2, δ, (q1
0, q2
0), B, (q1
A, q2
A), (q1
R, q2
R))
where an input word is copied to the second tape and heads of tapes are placed on
the leftmost input symbol every tape. The transition function is defined on the basis
of δ1 and δ2:
δ((q1, q2), (X1, X2)) = ((p1, p2), (Y1, Y2), (D1, D2)), where
δ1 (q1, X1) = (p1, Y1, D1), δ2(q2, X2) = (p2, Y2, D2) for
(q1, q2) ∈(Q1 −{q1
A, q1
R}) × (Q2 −{q2
A, q2
R}),
–
for union of : L1 and L2
δ((q1
R, q2), (X1, X2)) = ((q1
R, p2), (X1, Y2), (S, D2)) and
δ((q1, q2
R), (X1, X2)) = ((p1, q2
R), (Y1, X2), (D1, S))
where δ1(q1, X1) = (p1, Y1, D1), δ2(q2, X2) = (p2, Y2, D2),
that is, if one machine out of M1 and M2 rejects, then M waits for termination of
computation of another one,

5.4 Problems
|
137
δ((q1
A, q2), (X1, X2)) = ((q1
A, q2
A), (X1, X2), (S, S)) and
δ((q1, q1
A), (X1, X2)) = ((q1
A, q2
A), (X1, X2), (S, S))
where q1 ∈Γ1 and q2 ∈Γ2,
that is, if at least one machine out of M1 and M2 accepts then M accepts.
–
for intersection of L1 and L2
δ((q1
A, q2), (X1, X2)) = ((q1
A, p2), (X1, Y2), (S, D2)) and
δ((q1, q2
A), (X1, X2)) = ((p1, q2
A), (Y1, X2), (D1, S))
where δ1(q1, X1) = (p1, Y1, D1), δ2(q2, X2) = (p2, Y2, D2),
that is, if one machine out of M1 and M2 accepts, then M waits for termination of
computation of another one,
δ((q1
R, q2), (X1, X2), (D1, D2)) = ((q1
R, q2
R), (X1, X2), (S, S)) and
δ((q1, q1
R), (X1, X2), (D1, D2)) = ((q1
R, q2
R), (X1, X2), (S, S))
where q1 ∈Γ1 and q2 ∈Γ2,
that is, M rejects if at least one machine out of M1 and M2 rejects.
Problem 5.7. Let L = L(M) is a language accepted by a deterministic Turing machine
with the stop property M. Build a Turing machine, which accepts complement of L,
i. e., the language L = Σ∗−L, where Σ is an alphabet of L.
Solution. Assume that M is a given Turing machine with two-way infinite tape and
with halting states:
M = (Q, Σ, Γ, δ, q0, B, {qA}, {qR})
The machine M with the accepting and rejecting states exchanged accepts the com-
plement of L:
M = (Q, Σ, Γ, δ, q0, B, {qR}, {qA})
Problem 5.8. Let L1 = L(M1) and L2 = L(M2) are languages accepted by deterministic
Turing machines with the stop property M1 and M2. Build a Turing machine, which
accepts concatenation of L1 and L2, that is, the language L = L1 ∘L2.
Solution. A solution is quite similar to the solution of Problem 5.6 for intersection.
Instead of copying an input word to the second tape, its suffix is moved to the second
tape. The suffix is nondeterministically chosen.
Problem 5.9. Let L = L(M) is a language accepted by a Turing machine with the stop
property M. Build a Turing machine, which accepts Kleene closure of L, that is, the
language L′ = L∗.
Solution. A nondeterministic two-tape Turing machine M′ accepts the Kleene closure
of L(M). It realizes the following algorithm:
1.
store an input word on the first tape;
2.
if the first tape is empty, that is, it keeps the empty word ε, then M′ accepts;

138
|
5 Turing machines
3.
for a nonempty word at the input M′ chooses nondeterministically a prefix of the
current content of the first tape and moves the prefix to the second tape;
4.
then M′ runs the computation of M on the second tape;
5.
if M rejects the copied prefix, then M′ rejects its input;
6.
switch to the second point of this algorithm.
For the sake of clarity, a cleaning procedure is not employed in this algorithm. A de-
tailed description of M′ is left to the reader.
Problem 5.10. Let L1 = L(M1) and L2 = L(M2) are languages accepted by Turing ma-
chines M1 and M2. Build a Turing machine, which accepts.
–
union of L1 and L2, that is, the language L = L1 ∪L2;
–
concatenation of L1 and L2, that is, the language L = L1 ∘L2;
–
intersection of L1 and L2, that is, the language L = L1 ∩L2;
–
Kleene closure of L1, that is, the language L = (L1)∗.
Hint. Adapt solutions of Problem 5.6, Problem 5.7, Problem 5.8 and Problem 5.9 to
Turing machines without the stop property.
Problem 5.11. Adapt solutions of Problem 5.6, Problem 5.7, Problem 5.8 and Prob-
lem 5.9 for linear bounded automata instead of Turing machines with the stop prop-
erty.
Hint. Consider solutions of this problems with regard to space used.
Problem 5.12. Design a Turing machine accepting prime numbers. Discuss the possi-
bility of building a linear bounded automaton.
Problem 5.13. Design a Turing machine accepting a subset of natural numbers:
L = {k ∈N : (∃p ∈N, p – prime number) p + k is a prime number}
Does a machine with the stop property exist?
Problem 5.14. Design a Turing machine (a linear bounded automaton, if possible):
–
nondeterministic;
–
deterministic;
accepting the following language:
L = {www∗: w ∈{0, 1}∗}
Problem 5.15. Provide a reasonable definition of a two-dimensional Turing machine
(i. e., with a four-way infinite tape). Justify that two-dimensional Turing machines are
equivalent to one-dimensional Turing machines.

6 Pushdown automata
Pushdown automata vary from Turing machines in their definition and interpretation.
However, despite differences, in this chapter, we will prove that pushdown automata
are limited Turing machines. Pushdown automata are finite structures with a stack,
which is a potentially infinite element. A stack is a data structure, also called LIFO,
that is, “last in, first out,” which allows for storing abstract elements of data and re-
moving them. Usually two operations are used for operating a stack: push and pop.
The push operation adds an element to the stack, hiding elements pushed earlier or
initialized the stack if it is empty. The pop operation removes and returns the element
most recently added to the stack or returns the empty value if the stack is empty (this
is why stack is also called LIFO structure). Note that elements of the stack are not ac-
cessible except the one located on the top of the stack. To get access to a requested
element formerly pushed on the stack is necessary to pop all elements pushed later
than the requested one. In other words, elements are removed from the stack in the
reverse order to the order they came. Thus, stack data accessibility is significantly lim-
ited comparing to tapes of Turing machines. As a result, pushdown automata are less
powerful than Turing machines.
6.1 Nondeterministic pushdown automata
We discuss nondeterministic pushdown automata first. Deterministic ones should ful-
fill conditions, which are clearer on the basis of the general definition of nondetermin-
istic automata.
Definition 6.1. A pushdown automaton is a system
M = (Q, Σ, Γ, δ, q0, ▷, ◁, F, R)
with components as follows:
Q
a finite set of states;
Γ
a finite set of stack symbols (stack alphabet);
▷
an initial stack symbol, ▷∈Γ;
Σ
a finite input alphabet;
◁
a special end-of-input symbol;
q0
the initial state, q0 ∈Q;
F
a set of accepting states, F ⊂Q;
R
a set of rejecting states, R ⊂Q, F ⋂R = ⌀;
δ
a transition function, δ : Q × (Σ ∪{ε}) × Γ →⋃∞
k=0(Q × Γ∗)k
A pushdown automaton could be interpreted as a physical mechanism shown in
Figure 6.1. This mechanism consists of:
https://doi.org/10.1515/9783110752304-006

140
|
6 Pushdown automata
Figure 6.1: Pushdown automaton.
–
a control unit, it is in a state q ∈Q;
–
an input tape holding input data a1 a2 . . . an;
–
the special end-of-input symbol ◁is attached to input data, the symbol ◁neither
belongs to the input alphabet nor to the stack alphabet, it marks the end of input
easing practicing pushdown automata;
–
an input head, which reads an input symbol and shifts right or does not take any
action;
–
if an input head reads end-of-input symbol ◁, it does not shift;
–
a stack, which is a one-way infinite tape with stack structure, which gives access
only to its top cell, that is, its first cell.
A pushdown automaton is aimed at accepting a language, that is, answering if its input
word belongs to the language or not. Computation of a given pushdown automaton is
done according to the following intuitive procedure:
1.
the initial configuration of a given pushdown automaton is described as follows:
a.
an input data, a word w = a1a2 . . . an over input alphabet Σ, is stored on the
input tape; cf. Figure 6.1;
b.
the end-of-input symbol ◁is attached to the input data;
c.
the head of the input tape is placed over the first (leftmost) symbol of the input
word;
d.
the head of the stack is (always) placed over the top cell of the stack
e.
the control unit is in the initial state q0;
2.
if the input head reads the end-of-input symbol ◁and the control unit is in an
accepting state, then the computation is terminated and the automaton accepts
the input;
3.
if the control unit is in a rejecting state, then the computation is terminated and
the automaton rejects the input. Notice that in this case, it is not required that
the input head reads the end-of-input symbol ◁. Of course, it would be possible
to set such requirement, but better to be flexible for the sake of simplicity: the
automaton responds to its state, that is, if a state of its control unit is an accepting
one or a rejecting one;
4.
if conditions of point 2 are not satisfied, then based on

6.1 Nondeterministic pushdown automata
|
141
a.
a state q of the control unit;
b.
a symbol X read by the head of the stack;
c.
either an input symbol a or without it;
the automaton makes the following actions:
d.
values {(p1, α1), (p2, α2), . . . , (pk, αk)} of the transition function δ(q, a, X) or
δ(p, ε, X) are computed, where a ∈Σ is the input symbol, X ∈Γ is the top sym-
bol of the stack, q, p1, p2, . . . , pk ∈Q are states, α1, α2, . . . , αk ∈Γ∗are strings of
symbols of the stack alphabet;
e.
if values of the transition function are computed based on an input symbol
δ(q, a, X), then the input head is shifted right, otherwise the input head does
not change its position;
f.
a value (pi, αi) of the transition function is chosen nondeterministically;
g.
the top symbol X of the stack is removed and then symbols of the string αi are
pushed on the stack in reverse order, that is, the last symbol first, the first one
last. The first symbol of αi will be on the top of the stack after this operation;
h.
the control unit switches to the state pi,
5.
computation goes to the point 2.
We assume that pushdown automata always terminate computation. The value (Q ×
Γ∗)0 of a transition function, which is the empty word ε, is interpreted as termination
of computation in a rejecting state.
Note that pushdown automata cannot store output information, so then, unlike
Turing machines, they can only accept languages and cannot compute functions or
solve problems.
Now we define configuration (a step description), a transition relation and a com-
putation of pushdown automata.
Definition 6.2. A configuration (a step description) of a pushdown automaton M =
(Q, Σ, Γ, δ, q0, ▷, ◁, F, R) is the following sequence of symbols:
γ q w
where:
–
q ∈Q is the current state of the control unit of a pushdown automaton;
–
γ is the stack contents, the last symbol of γ is the top symbol of the stack;
–
ω is the current input, the first symbol of w is the symbol under the input head, ◁
is the last symbol of the input.
Note that both sequences γ and ω of symbols are words over the stack alphabet Γ
and the input alphabet Σ and that any of these sequences may include only the initial
stack symbol ▷and the end-of-input symbol ◁. However, none of these two sequences
can be infinite, like in a case of a step description of a Turing machine.

142
|
6 Pushdown automata
For instance, the initial configuration is of the form ⊳q0 w
⊲, where q0 is the
initial state, w is the input data. In this case, the stack is empty, so the initial stack
symbol is near the initial state symbol. On the other hand, a step description γ q ⊲
informs that input symbols have already been read.
Let us analyze transitions done by pushdown automata. We assume that the fol-
lowing sequence of symbols describes a step description:
⊳X1 X2 . . . Xm−1 Xm q ai ai+1 . . . an ⊲
Recall that:
–
q is the current state of the control unit;
–
▷X1 X2 . . . Xm−1 Xm is the sequence of symbols on a stack, Xm is the top symbol
of a stack;
–
ai ai+1 . . . an ◁is the sequence of input symbols, ai is the front input symbol
–
the head of a stack reads Xm;
–
the input head either reads ai, or does not read anything.
The transition function determines the next step description (configuration):
–
if the value of the transition function is δ(q, ai, Xm) = {(p1, ε), (p2, Y1
2 Y2
2 Y3
2 )}, for
instance, and the second value (p2, Y1
2 Y2
2 Y3
2 ) is chosen nondeterministically, then
the following step description is yielded:
▷X1 X2 . . . Xm−1 Y3
2 Y2
2 Y1
2 p2 ai+1 . . . an ◁
–
if the value of the transition function is δ(q, ε, Xm) = {(p1, ε), (p2, Y1
2 Y2
2 Y3
2 )}, for
instance, and the first value (p1, ε) is chosen nondeterministically, then the fol-
lowing step description is yielded:
▷X1 X2 . . . Xm−1 p1 ai ai+1 . . . an ◁
We will use the symbol ≻to denote a transition of a pushdown automaton. The transi-
tion symbol ≻may be supplement with a pushdown automaton name ≻M to emphasize
that a transition concerns a given pushdown automaton. It also can be supplemented
with a superscript ≻k to notify that k transitions are done.
The above two transitions done by a pushdown automaton will be denoted as
follows:
▷X1 . . . Xm−1 Xm q ai ai+1 . . . an ◁≻▷X1 . . . Xm−1 Y3
2 Y2
2 Y1
2 p2 ai+1 . . . an ◁
▷X1 . . . Xm−1 Xm q ai ai+1 . . . an ◁≻▷X1 . . . Xm−1 p1 ai ai+1 . . . an ◁
Definition 6.3. Transitions of a pushdown automaton create a binary relation in the
space of all possible configurations of the automaton, that is, any two configurations
are related if and only if the second is derived from the first one by application of a

6.1 Nondeterministic pushdown automata
|
143
transition function. This relation is called the transition relation of a given pushdown
automaton. The transitive closure of the transition relation is denoted by ≻∗.
Definition 6.4. Computation of a pushdown automaton is a tree such that:
–
its nodes are labeled by configurations of an automaton;
–
its root is labeled by the initial configuration;
–
for any node ηp, its each child ηc is related to it in the transition relation, that is,
ηp ≻ηc.
Note that a computation tree is a (k + l)-tree, where k and l are the maximal number of
values yielded by the transition function for given arguments. Note that a transition
for a given state and a given stack symbol can be chosen from k-transitions respective
to an input symbol and from l-transitions respective to so-called ε-transition, that is,
when an input symbol is neither checked nor read.
Now we give a formal definition of acceptation of an input by a pushdown au-
tomaton.
Definition 6.5. A pushdown automaton accepts its input if and only if the pair of the
initial configuration and a final configuration belongs to the transitive closure of a
transition relation, that i, η1 ≻∗ηT, where η1 is an initial configuration and ηT is a
final configuration.
Remark 6.1. A pushdown automaton accepts its input if and only if the computation
tree has a path from the root to a leaf labeled by an accepting configuration.
Based on the above discussion, we now give formal definitions of some concepts.
Definition 6.6. The language accepted by a pushdown automaton is the set of words
w ∈Σ∗accepted by that automaton.
Example 6.1. Design a pushdown automaton accepting the language of palindromes
over the alphabet Σ = {a, b}, that is,
L = {w ∈{a, b}∗: w = wR, where wR is reversed w},
Solution. First, we briefly comment on an algorithm for a pushdown automaton ac-
cepting this language:
–
symbols corresponding to letters of the first part of an input word will be pushed
on the stack;
–
the middle of an input word is found nondeterministically; if an input word is of
odd length, the middle letter is read without the stack change;
–
letters of the second part of an input word are compared to stack symbols; if they
match, the top symbol of the stack is popped out and the next pair of an input
letter and a stack symbol are compared;

144
|
6 Pushdown automata
–
an automaton accepts when both the input and the stack are empty;
–
an automaton rejects in all other cases.
A pushdown automaton accepting this language:
M = (Q, Σ, Γ, δ, p1, ▷, ◁, F, R)
where:
–
Q = {p1, p2, ACC, REJ} – a set of states;
–
Σ = {a, b} – an input alphabet;
–
Γ = {A, B, ▷} – a stack alphabet;
–
p1 – the initial state;
–
▷– the initial stack symbol;
–
◁– the end-of-input symbol;
–
F = {ACC} – a set of accepting states;
–
F = {REJ} – a set of rejecting states;
–
δ : Q×(Σ∪{ε})×Γ →(Q×Γ∗)0∪(Q×Γ∗)1∪(Q×Γ∗)2 – a transitions function given in
Table 6.1. Note that transition function depends on three arguments, so it cannot
be given in one two-dimensional array. Thus, there are tables for every state.
Table 6.1: The transition function of the pushdown automaton designed in Example 6.1. Note, in
order to make the table easy readable δ(pi, X, Y) = (REJ, Y) is denoted by REJ. Similarly, δ(pi, X, Y) =
(ACC, Y) is denoted by ACC.
a
b
ε
◁
δ(p1)
A
(p1, AA)
(p2, A)
(p1, BA)
(p2, A)
(p2, A)
REJ
B
(p1, AB)
(p2, B)
(p1, BB)
(p2, B)
(p2, B)
REJ
▷
(p1, A▷)
(p2, ▷)
(p1, B▷)
(p2, ▷)
(p2, ▷)
ACC
δ(p2)
A
(p2, ε)
REJ
REJ
REJ
B
REJ
(p2, ε)
REJ
REJ
▷
REJ
REJ
REJ
ACC
Let us comment on the design of the automaton describing the purpose of the states:
–
p1 – accept the empty word at the input. If an input word is nonempty:
–
push on the stack symbols A or B corresponding to input letters a and b;
–
switch to p2 when the middle of an input word is encountered, read the middle
input letter when an input word is of odd length, otherwise switch without
reading an input letter (make ε-transition);

6.2 Deterministic pushdown automata
|
145
–
p2 – read remaining input letters and pop symbols from the stack if they match,
otherwise reject an input word;
–
p2 – accept an input word when the input and the stack are empty.
Examples of computation of the above pushdown automaton are given in Figure 6.2
and Figure 6.3.
Figure 6.2: The computation of the pushdown automaton designed in Example 6.1 for the input word
w = aaa.
Figure 6.3: The computation of the pushdown automaton designed in Example 6.1 for the input word
w = aab.
In Figure 6.2, a computation for the input word w = aaa is given. The input word is
accepted because there is a path from the root to an accepting leaf.
A computation for the input word w = aab, given in Figure 6.3, has no accepting
leaf. As a result, the input word is rejected.
6.2 Deterministic pushdown automata
A pushdown automaton is deterministic if there is no more than one possible tran-
sition in any configuration. This condition needs that for any configuration of an au-

146
|
6 Pushdown automata
tomaton, a transition function yields at most one possible transition. However, for
pushdown automata, there is another nondeterministic factor: for a given state and a
given stack symbol, there might be a choice between transitions for given input sym-
bols and without involving an input symbol, that is, so called ε-transition. As a result,
the following definition formulates conditions for a pushdown automaton to be a de-
terministic one.
Definition 6.7. A pushdown automaton M = (Q, Σ, Γ, δ, q0, ▷, ◁, F, R) is a deterministic
one if and only if:
–
its transition function for any arguments, that is, for any triple (q, X, Y), q ∈Q, X ∈
(Σ ∪{ε, ◁}), Y ∈Γ, yields at most one transition;
–
for given q ∈Q and Y ∈Γ the transition function rejects for (q, ε, Y) or rejects for
all (q, a, Y), a ∈(Σ ∪{ε}).
Proposition 6.1. Deterministic pushdown automata are not equivalent to nondetermin-
istic ones.
Proof. It is intuitively clear that the language L = {w ∈{a, b}∗: w = wR} discussed
in Example 6.1 cannot be accepted by a deterministic pushdown automaton. On the
other hand, deterministic pushdown automata are special cases of nondeterministic
ones. From a perspective of accepted languages, the class of languages accepted by
deterministic automata is included but not equal to the class of languages accepted
by nondeterministic pushdown automata. A formal proof of strict inclusion can be
found in [1].
Example 6.2. Design a pushdown automaton accepting the following language over
the alphabet Σ = {a, b}:
L = {w ∈{a, b}∗:| #a w −#b w |≤2},
Solution. First, we briefly comment on an algorithm for a pushdown automaton ac-
cepting this language:
–
numbers of letters a and b are compared in an input word; superfluous letters are
pushed on the stack;
–
number of symbols on the stack is checked when the input is empty; an input
word is accepted if this number does not exceed 2.
A pushdown automaton accepting this language:
M = (Q, Σ, Γ, δ, p0, ▷, ◁, F, R)
where:
–
Q = {p0, p1, p2, ACC, REJ} – a set of states;
–
Σ = {a, b} – an input alphabet;

6.2 Deterministic pushdown automata
|
147
–
Γ = {A, B, ▷} – a stack alphabet;
–
δ : Q×Σ×Γ →Q×Γ∗– a transitions function given in Table 6.2. Note that transition
function neither yields multiply transitions, nor has ε-transitions. Therefore, the
automaton is a deterministic one.
Table 6.2: The transition function of the pushdown automaton designed in Example 6.2.
a
b
◁
δ(p0)
A
(p0, AA)
(p0, ε)
(p1, ε)
B
(p0, ε)
(p0, BB)
(p1, ε)
▷
(p0, A▷)
(p0, B▷)
ACC
δ(p1)
A
REJ
REJ
(p2, ε)
B
REJ
REJ
(p2, ε)
▷
REJ
REJ
ACC
δ(p2)
A
REJ
REJ
REJ
B
REJ
REJ
REJ
▷
REJ
REJ
ACC
–
other components of the automaton are like in Example 6.1.
The design of the automaton is commented in the form of the purpose of states de-
scriptions:
–
p0 – numbers of letters a and b are compared in an input word; superfluous letters
are pushed on the stack:
–
having a (or b) on the input, push the matching symbol A (or B) on the stack
if it has the same symbol A (or B) on the top or it is empty (has ▷on the top);
–
pop the top symbol from the stack if it does not match an input symbol;
–
p0 – if the input is empty then:
–
accept if the stack is empty;
–
pop the top symbol from the stack and switch to p1;
–
p1 – now the input is empty:
–
accept if the stack is empty;
–
pop the top symbol from the stack and switch to p2;
–
p2 – accept if the stack is empty:
–
reject in all other cases.
Note that states p0, p2 and p1 check if the number of remaining symbols on the stack
does not exceed 2.

148
|
6 Pushdown automata
Examples of computation of the automaton built in Example 6.2 for words aaaa ∉
L and aaba ∈L are given below:
▷p0 a a a a ◁≻▷A p0 a a a ◁≻▷A A p0 a a ◁≻▷A A A p0 a ◁
≻▷A A A A p0 ◁≻▷A A A p1 ◁≻▷A A p2 ◁≻REJ
▷p0 a a b a ◁≻▷Ap0 a b a ◁≻▷A A p0 b a ◁≻▷A p0 a ◁
≻▷A A p0 ◁≻▷A p1 ◁≻▷p2 ◁≻ACC
6.3 Accepting states versus empty stack
Examples presented in previous sections show that acceptance came with the initial
symbol of the stack ▷and the end-of-input symbol ◁. We may ask a question if this is
a coincidence or rather a rule, that is, if we can change the acceptation by accepting
state to the acceptation by the empty stack. Notice that the empty input is the default
because we already assumed that acceptance must be accompanied by empty input.
We can answer this question positively. We may move up a class of pushdown au-
tomata, which accept when the stack is empty, and prove that automata accepting by
empty stack are equivalent to automata accepting with a state. An idea of the proof is
quite clear.
On the one hand, having a pushdown automaton accepting with a state, we can
empty its stack when an accepting state is reached and then accept its input. On the
other hand, given a pushdown automaton accepting with an empty stack, it should
make an additional transition to an extra accepting state when its stack is empty. De-
tails are given below.
Definition 6.8. A pushdown automaton accepting by empty stack is a system
M = (Q, Σ, Γ, δ, q0, ▷, ◁, ⌀, R)
where the set of accepting states is empty and other components are as shown in Def-
inition 6.1.
Note that acceptance by empty stack does not depend on a state of a terminated
automaton description. Because of this, the set of accepting states of such an automa-
ton is empty.
Proposition 6.2. Acceptance by states is equivalent to acceptance by empty stack.
Proof. We prove that for any pushdown automaton accepting by accepting states,
there exists an equivalent automaton accepting by empty stack and oppositely.
Assume that there is a pushdown automaton accepting by accepting states
M = (Q, Σ, Γ, δ, q0, ▷, ◁, F, R)

6.3 Accepting states versus empty stack
|
149
The following pushdown automaton accepting with the empty stack is equivalent to
the above one:
M′ = (Q′, Σ, Γ′, δ′, q′
0, ▷′, ◁, ⌀, R),
where:
1.
Q′ = Q ∪{q′
0, q′} and Q ∩{q′
0, q′} = ⌀;
2.
Γ′ = Γ ∪{▷′} and Γ ∩{▷′} = ⌀;
3.
the transition function δ′ is designed as follows:
(a) δ′(q′
0, ε, ▷′) = {(q0, ▷▷′)};
(b) δ′(q, a, X) = δ(q, a, X) for q ∈Q, a ∈Σ, X ∈Γ;
(c) δ′(q, ε, X) = δ(q, ε, X) for q ∈(Q −F), X ∈Γ;
(d) δ′(q, ε, X) = {(q′, ε)} for q ∈F, X ∈Γ −{▷} start emptying the stack;
(e) δ′(q′, ε, X) = {(q′, ε)} for X ∈Γ −{▷} continue emptying the stack;
(f) δ′(q′, ◁, ▷) = {(q′, ε)}, removes the initial stack symbol and keeps the end-of-
input symbol, accept with empty stack and empty input;
(g) M′ rejects for all other configurations.
The automaton M′ moves to the initial configuration of M in its first transition, com-
pare 3a. Then it follows computation of M; see 3b and 3c. When M reaches an accept-
ing configuration (recall that it terminates computation for an accepting state), then
M′ pops a top symbol from its stack and switches to extra state q′ in order to begin
emptying the stack, compare 3d. Then M′ continues emptying the stack in 3d, and
finally it removes extra stack initial symbol ▷leaving empty its input and its stack;
compare 3f. An extra stack initial symbol was needed in order to prevent confusion
when the stack of the automaton M becomes empty during the computation of M.
It is worth noticing that the automaton M′ is a nondeterministic one. Also, let us
recall that accepting by switching to accepting state requires empty input, compare
point 3 of computation description in Section 6.1.
Now, we design a pushdown automaton accepting by states equivalent to a given
one accepting by the empty stack. Assume that there is a pushdown automaton ac-
cepting with the empty stack
M = (Q, Σ, Γ, δ, q0, ▷, ◁, ⌀, R)
The following pushdown automaton accepting with states is equivalent to the above
one:
M′ = (Q′, Σ, Γ′, δ′, q′
0, ▷′, ◁, F, R),
where:
–
Q′ = Q ∪{q′
0, qA} and Q ∩{q′
0, qA} = ⌀;
–
Γ′ = Γ ∪{▷′} and Γ ∩{▷′} = ⌀;

150
|
6 Pushdown automata
–
F = {qA};
–
the transition function δ′ is design as follows:
–
δ′(q′
0, ε, ▷′) = {(q0, ▷▷′)};
–
δ′(q, a, X) = δ(q, a, X) for q ∈Q, a ∈Σ, X ∈Γ;
–
δ′(q, ε, X) = δ(q, ε, X) for q ∈Q, X ∈(Γ −{▷});
–
δ′(q, ε, ▷) = {(qA, ε)} for q ∈F;
–
M′ rejects for all other configurations.
The automaton M′ goes to the initial configuration of M in its first transition; see the
first point. Then it follows a computation of M. When M empties its stack, then M′
pops a top symbol from its stack (it is the initial symbol of M) and switches (nonde-
terministically) to the accepting state qA and accepts if its input is empty. Otherwise,
when input is not empty, M′ continues (nondeterministically) simulation of M.
6.4 Pushdown automata as Turing machines
Pushdown automata are restricted Turing machines. Moreover, since pushdown au-
tomata always terminate their computation, they are restricted Turing machines with
the stop property.
Proposition 6.3. There is a Turing machine with the stop property equivalent to a given
pushdown automaton.
Proof. Let us design a Turing machine equivalent to a given pushdown automaton
accepting with states:
M = (Q, Σ, Γ, δ, q0, ▷, ◁, F, R)
We design a 2-tape Turing machine with terminating states, which is equivalent to the
automaton M:
MT = (QT, Σ, Γ1
T, Γ2
T, δT, B, {qA}, {qR}),
where:
1.
QT = (Q −(F ∪R)) ∪{qA, qR} ∪QS, Q ∩({qA, qR} ∪QS) = ⌀- the set of states of the
Turing machine MT includes:
a.
states of the automaton M except accepting and rejecting states (except states,
which terminate computation of the automaton);
b.
a new halting accepting state and a new halting rejecting state of the Turing
machine MT and
c.
additional states QS, which simulate transitions of the automaton M;
2.
Γ1
T = Σ ∪{◁, B} – an alphabet of the first tape;

6.4 Pushdown automata as Turing machines
|
151
3.
Γ2
T = Γ ∪{▷, B} – an alphabet of the second tape;
4.
B – the blank symbol of both tapes.
A configuration of a pushdown automaton is characterized by the following configu-
ration of the Turing machine; cf. Figure 6.4:
Figure 6.4: A pushdown automaton and its characterization by a Turing machine.
–
an input is stored on the first tape, the head of the first tape is placed over the first
input symbol;
–
a stack is stored on the second tape; the initial stack symbol is the leftmost non-
blank symbol of the tape, a top stack symbol is a rightmost nonblank tape symbol,
the head of the tape is placed on a rightmost nonblank symbol (a top symbol of
the stack);
–
the control unit of the Turing machine is in the same state as the control unit of
the pushdown automaton.
All values of the transition function δ of the automaton are uniquely enumerated.
Namely, if the transition function δ yields k transitions for a given triple of arguments:
δ(q, a, X) = {(p1, α1), (p2, α2), . . . , (pk, αk)} and the transition has index t, then each pair
(pi, αi) gets its own double index t and i.
For a given transition of the automaton (p, α) ∈δ(q, a, X) with a given number t
and p ∈Q −(F ∪R), a ∈Σ ∪{◁} and X ∈Γ, the Turing machine makes several transi-
tions in order to update content of the second tape. All such transitions are uniquely
identified by states corresponding to the given move of the pushdown automaton. The
simulation of one transition of the automaton is formally described as follows:
1.
if α is the empty sequence, then (p, B
B , R
L ) ∈δ′ (q, a
X );
2.
if α = X1X2 . . . Xr, then the set of states QT is expanded by adding states pt
1, pt
2, . . . , pt
r
and the following transitions are included:

152
|
6 Pushdown automata
a.
(pt
r, a
Xr , S
R ) ∈δ′ (q, a
X );
b.
δ′ (pt
r, a
B ) = {(pt
r−1,
a
Xr−1 , S
R )};
c.
δ′ (pt
r−1, a
B ) = {(pt
r−2,
a
Xr−2 , S
R )};
d.
. . .
e.
δ′ (pt
2, a
B ) = {(pt
1, a
X1 , S
R )};
f.
δ′ (pt
1, a
B ) = {(p, B
B , R
L )}
A simulation of a transition (p, α) ∈δ(q, ε, X) is similar to simulation of a transition
(p, α) ∈δ(q, a, X) with the two following changes:
–
point 1 changes to: if α is the empty sequence, then (p, a
B , S
L ) ∈δ′ (q, a
X );
–
point 2f changes to: if α = X1X2 . . . Xr, then δ′ (pt
1, a
B ) = {(p, a
B , S
L )}.
In transitions shown above, if p ∈F, then it should be replaced with qA and if p ∈R,
then it should be replaced with qR.
The above design of the Turing machine simulating a pushdown automaton
shows that the Turing machine has the stop property and it accepts input if and only
if the pushdown automaton accepts the same input.
6.5 Problems
Problem 6.1. Design a pushdown automaton accepting the language:
L = {w ∈{a, b}∗: #aw = 2 ∗#bw}.
Solution. A deterministic pushdown automaton solves the problem:
M = ({p1, p2, ACC, REJ}, {a, b}, {A, B, ▷}, δ, p1, ▷, ◁, {ACC}, {REJ})
where the transition function δ : Q × Σ × Γ →Q × Γ∗is given in Table 6.3.
Table 6.3: The transition function of the pushdown automaton designed in Problem 6.1.
a
b
◁
δ(p0)
A
(p1, A)
(p0, ε)
REJ
B
(p1, B)
(p0, BB)
REJ
▷
(p1, ▷)
(p0, B▷)
ACC
δ(p1)
A
(p0, AA)
(p1, ε)
REJ
B
(p0, ε)
(p1, BB)
REJ
▷
(p0, A▷)
(p1, B▷)
REJ

6.5 Problems
|
153
A brief description of an algorithm:
–
the number of pairs of letters a is compared to the number of letters b in an input
word:
–
the state p0 stands for an even number of as already read from the input. Next,
odd, a read from the input causes switch to the state p1 and does not change
the stack
–
the state p1 stands for an odd number of as already read from the input. Next,
even a read from the input switches the state to p0 and causes either deleting
B from the stack or pushing A on the stack. Deleting B from the stack corre-
sponds to matched pair of two as and one b while pushing A on the stack
means that two as read from the input does not meet the corresponding b;
–
due to assumptions of the above two points, in state p1, each pair of a and B
or pair of b and A stands for a matched pair of two as and one b; otherwise, A
or B corresponding to a or b is pushed on the stack;
–
alike, in state p0, the pair of b and A stands for a matched pair of two as and
one b;
–
stack symbols A and B on the stack correspond to superfluous pairs of letters
a or letters b;
–
an automaton accepts by the accepting state ACC, though the transition to the
accepting state is made only if the stack is empty.
Examples of computation of the above automaton for input words abaaaabab and
aaaaab:
▷p0 a b a a a a b a b ◁≻▷p1 b a a a a b a b ◁≻▷B p1 a a a a b a b ◁
≻▷p0 a a a b a b ◁≻▷p1 a a b a b ◁≻▷A p0 a b a b ◁
≻▷A p1 b a b ◁≻▷p1 a b ◁≻▷A p0 b ◁≻▷p0 ◁≻▷ACC ◁
▷p0 a a a a a b ◁≻▷p1 a a a a b ◁≻▷A p0 a a a b ◁≻▷A p1 a a b ◁
≻▷A A p0 a b ◁≻▷A A p1 b ◁≻▷A p1 ◁≻▷REJ ◁
Problem 6.2. Design a pushdown automaton accepting the language L = {wwR : w ∈
{a, b}∗}∗, that is, the language of sequences of even length palindromes over the al-
phabet {a, b}.
Solution. The following nondeterministic pushdown automaton solves the problem:
M = ({p1, p2, ACC, REJ}, {a, b}, {A, B, ▷}, δ, p1, ▷, ◁, {ACC}, {REJ})
where the transition function δ : Q × (Σ ∪{ε}) × Γ →Q × Γ∗is given in Table 6.4.
A brief description of the algorithm:
–
a part of an input word is checked if it is a palindrome of even length:
–
in the state, p1 stack symbols A and B corresponding to letters a and letters b
are pushed on the stack for the first half of the part of an input word;

154
|
6 Pushdown automata
Table 6.4: The transition function of the pushdown automaton designed in Problem 6.2.
a
b
ε
◁
δ(p1)
A
(p1, AA)
(p1, BA)
(p2, A)
REJ
B
(p1, AB)
(p1, BB)
(p2, B)
REJ
▷
(p1, A▷)
(p1, B▷)
REJ
ACC
δ(p2)
A
(p2, ε)
REJ
REJ
REJ
B
REJ
(p2, ε)
REJ
REJ
▷
REJ
REJ
(p1, ▷)
REJ
–
a nondeterministic transition is made to the state p2 to compare the contents
of the stack (the first half of the palindrome) to incoming input symbols;
–
if the input is nonempty, the (deterministic) transition is made to the state p1
in order to continue checking the next part of the input;
–
an automaton accepts by the accepting state ACC, though the transition to the
accepting state is made from the state p1 only if the stack is empty. Note that the
empty word is also accepted by the transition from the state p1.
A computation for the word w = aaaa is shown in Figure 6.5. Notice that the automaton
found two different splits of the input word to palindromes: the first one takes the
whole input as palindrome and the second split divides the input word into two halves
being palindromes as well.
Figure 6.5: The computation of pushdown automaton designed in Problem 6.2 for the input word
w = aaaa.

6.5 Problems
|
155
Problem 6.3. Design a pushdown automaton accepting the language:
L = {w = a1 a2 . . . an ∈{a, b}∗: #a w = #b w and a1 = an}
that is, language of words over the alphabet {a, b} with the same number of letters a
and b and with the first and the last letters equal.
Hint. Design a deterministic pushdown automaton. Store in states of an automa-
ton the first and the recently read input letters.
Problem 6.4. Design a pushdown automaton accepting the language:
L = {w = a1 a2 . . . an ∈{a, b}∗: #aw = #b w ∧(∀u, uv = w)#au ≥#bu}
that is, language of words over the alphabet {a, b} which have the same number of a’s
and b’s and have no more b’s than a’s in all its prefixes.
Hint. Design a deterministic pushdown automaton which:
–
pushes A on the stack for a on the input;
–
pops A from the stack for B on the input;
–
rejects the input only for empty stack and b on the input.
Problem 6.5. Design a pushdown automaton accepting the language:
L = {w =∈{a, b}∗: w = anbn, n > 0}
∗
that is, the language of words over the alphabet {a, b}, which is a concatenation of an
arbitrary number of equal length sequences of a’s and b’s.
Hint. Design a deterministic pushdown automaton.
Problem 6.6. Let L1 = L(M1) and L2 = L(M2) are languages accepted by pushdown
automata M1 and M2. Build a pushdown automaton M, which accepts:
(a) union of languages L1 and L2, i. e., the language L = L1 ∪L2;
(b) concatenation L = L1 ∘L2 of languages L1 and L2;
(c) Kleene closure of the language L1, that is, the language L = (L1)∗.
Hints. Design nondeterministic pushdown automata, which respectively:
(a) an automaton makes nondeterministic ε-move choosing either M1, or M2 and than
applies computation of the chosen automaton;
(b) an automaton nondeterministically splits the input word into two parts: it follows
computation of M1 for a part of the input word, which could be accepted by M1,
and then switches to the M2 and then follows its computation for remaining part
of the input word;
(c) an automaton nondeterministically splits the input word into several parts: it fol-
lows computation of M for a part of the input word, which M could accept. It then
begins new computation of M for the remaining part of the input word with the
possible beginning of new computation of M.

7 Finite automata
Finite automata are the simplest model of computation. They can be derived from
pushdown automata by removing the stack. Finite automata are finite structures with-
out any potentially infinite parts as, for instance, a stack in pushdown automata or a
tape in Turing machines. Despite these limitations, finite automata are important the-
oretical and practical tools. Three classes of finite automata are distinguished: deter-
ministic, nondeterministic and those with ε-transitions.
7.1 Deterministic finite automata
Deterministic model of finite automata is the simplest one among three types: deter-
ministic, nondeterministic and with ε-transitions. It is the simplest one in terms of
a description of automata as well as of computation realized for a given input word.
From now on, the term finite automata will denote deterministic finite automata. Any
reference to nondeterministic finite automata or finite automata with ε-transitions will
be explicitly acknowledged.
Definition 7.1. A deterministic finite automaton is a system
M = (Q, Σ, δ, q0, F)
with the following components:
Q
a finite set of states;
Σ
a finite input alphabet;
q0
the initial state, q0 ∈Q;
F
a set of accepting states, F ⊂Q;
δ
a transition function, δ : Q × Σ →Q.
A transition function of a finite automaton is a total function, that is, it is defined
for all its pairs of arguments (a transition function of Turing machines and pushdown
automata could be undefined for some arguments).
A finite automaton could be interpreted as a structure shown in Figure 7.1. It con-
sists of:
–
a control unit, it is in a state of q ∈Q;
–
an input tape holding input data a1a2 . . . an;
–
an input head, which reads an input symbol and shifts right.
Like in the case of pushdown automata, a finite automaton is, that is, at accepting
a language, that is, answering if an input word belongs to the language or does not.
Computation of a given finite automaton is done according to the following intuitive
procedure:
https://doi.org/10.1515/9783110752304-007

7.1 Deterministic finite automata
|
157
Figure 7.1: A deterministic finite automaton.
1.
the initial configuration of a given finite automaton is described as follows:
a.
an input data, a word w = a1a2 . . . an over input alphabet Σ, is stored on the
input tape; cf. Figure 7.1;
b.
the head of the input tape is placed over the first (leftmost) symbol of the input
word;
c.
the control unit is in the initial state q0;
2.
based on:
a.
a state q of the control unit;
b.
an input symbol a
the automaton realizes the following actions:
c.
a state p of the transition function δ(q, a) is computed, where a ∈Σ is the input
symbol and q, p ∈Q are states;
d.
the input head is shifted right;
e.
the control unit switches to the state p;
3.
if the input is not empty, then computation goes to point 2; otherwise, computa-
tion is terminated and an automaton responds to a state of the control unit.
Of course, the computation of a finite automaton always terminates.
Note that finite automata, like pushdown automata, cannot store output infor-
mation, so they can only accept languages and cannot compute functions or solve
problems.
Now we define a configuration, a transition relation and a computation of finite
automata.
Definition 7.2. A configuration of a finite automaton M = (Q, Σ, δ, q0, F) is the follow-
ing sequence of symbols:
q ω
where:
–
q ∈Q is the current state of the control unit of a finite automaton;
–
ω is the current input, the first symbol of ω is the symbol under the input head.
Note that a sequence of symbols ω is a word over the input alphabet Σ. It is empty
after the termination of a computation. It can never be infinite.

158
|
7 Finite automata
For instance, the initial configuration (configuration) is “q0 a1 a2 . . . an,” where q0
is the initial state, w = a1 a2 . . . an is the input data. On the other hand, a configuration
“q” informs that the control unit of a finite automaton is in a state q and an input
symbols have already been read.
As in the case of Turing machines and pushdown automata, we will use the sym-
bol ≻to denote a transition of a finite automaton. The transition symbol ≻may be
supplemented with a finite automaton name ≻M to emphasize that a transition con-
cerns a given finite automaton. It also can be supplemented with a superscript ≻k to
notify k transitions have been completed.
If q ai ai+1 . . . an is a configuration and δ(q, ai) = p is a transition function ap-
plicable for this configuration, then a next step description after a transition is made
is p ai+1 . . . an. These two step descriptions create a transition of a finite automaton,
which is denoted as
q ai ai+1 . . . an ≻p ai+1 . . . an
Definition 7.3. Transitions of a finite automaton create a binary relation in the space
of all possible configurations of the automaton, that is, any two configurations are re-
lated if and only if the latter is derived from the former one by application of a transi-
tion function. This relation is called the transition relation of a given finite automaton.
The transitive closure of the transition relation is denoted by ≻∗.
Definition 7.4. A computation of a finite automaton M = (Q, Σ, δ, q0, F) is a sequence
of configurations η1, η2, . . . , ηn such that η1 is the initial configuration, ηT is the final
configuration and a pair of any two successive configurations belongs to the transition
relation. A computation is denoted as η1 ≻η2 ≻⋅⋅⋅≻ηn.
Remark 7.1. A computation of a finite automaton is a finite sequence of configurations
q0 a1 a2 a3 . . . an ≻qi1 a2 a3 . . . an ≻qi2 a3 . . . an ≻qin−1 an ≻qin
Because any transition consists of reading an input symbol and switching to a state,
computation will be shown in a simpler form:
q0 a1 qi1 a2 qi2 a3 . . . an qin−1 an qin
where only input symbols under the head are displayed.
Now we give a formal definition of acceptance of an input by a finite automaton.
Definition 7.5. A finite automaton accepts its input if and only if the pair of the initial
configuration and a final configuration belongs to the transitive closure of a transi-
tive relation, that is, η1 ≻∗ηT, where η1 is an initial configuration and ηT is a final
configuration, that is, the state in ηT is an accepting one.
Based on the above discussion, we now give formal definitions of some concepts.

7.1 Deterministic finite automata
|
159
Definition 7.6. The language accepted by a finite automaton M = (Q, Σ, δ, q0, F) is the
set of words w ∈Σ∗accepted by a finite automaton.
Example 7.1. Design a finite automaton accepting binary numbers without non-
significant zeros. Note that binary numbers without nonsignificant zeros are zero
and numbers, which begin with 1:
L = {0, 1, 10, 11, 100, 101, 110, 111, 1000, 1001, 1010, . . .}
Solution. The following automaton accepts binary words without nonsignificant ze-
ros:
M = (Q = {q0, q1, q2, q3}, Σ = {0, 1}, δ, q0, F = {q1, q3})
where the transition function δ : Q×Σ →Q is given in Table 7.1. In this table, the initial
state is marked by an arrow left of the initial state and accepting states are marked with
arrows right of them.
Table 7.1: The transition function of the finite automaton designed in Example 7.1.
δ
0
1
→q0
q1
q3
q1 →
q2
q2
q2
q2
q2
q3 →
q3
q3
A finite automaton can also be shown in the form of a transition diagram. For instance,
a transition diagram of the above automaton is illustrated in Figure 7.2. The initial state
is indicated by the arrow going to it, accepting states are double circled and transitions
are shown as labeled arrows between states.
Figure 7.2: Transition diagram of the finite automaton designed in Example 7.1.
It is clear that this automaton accepts the language L:
–
computation for any binary word, which begins with 1 (it is a binary number with-
out nonsignificant zeros), goes to the state q3 and stays in this state, waiting for

160
|
7 Finite automata
any more input binary digits until reaching termination of the computation. Be-
cause q3 is an accepting state, then the word is accepted;
–
computation for 0 (0 is a binary number without nonsignificant zeros) goes and
terminates in q1, which is an accepting state. Thus, 0 is accepted;
–
any other binary word begins with 0 and has more than one binary digit. Such
words are binary numbers with nonsignificant zeros. For such a word, computa-
tion goes to q1 and then to q2 and then stays in q2 for any more input binary digits
until termination. Such a word will not be accepted because q2 is not an accepting
state.
Examples of computation:
–
q0 – the word ε is not accepted because computation for this word is terminated in
a state q0, which is not an accepting one. Note that the empty word ε is considered
not to be a number;
–
q0 1 q3 1 q3 0 q3 1 q3 0 q3 – the word 11010 is accepted because computation for
this word is terminated in the accepting state q3;
–
q0 0 q1 – the word 0 is accepted because computation for this word is terminated
in the accepting state q1;
–
q0 0 q1 1 q2 0 q2 1 q2 0 q2 – the word 01010 is not accepted because computation
for this word is terminated in the state q3, which is a not accepting one.
Example 7.2. Design a finite automaton, which accepts the language L of binary words
with sequences of the same letter not longer than 3, for instance: ε ∈L, 1000111 ∈L,
10000111 ∉L.
Solution. The following finite automaton accepts the language:
M = (Q, Σ, δ, q0, F)
where:
–
Q = {q0, q01, q02, q03, q11, q12, q13, qR} – the set of states;
–
Σ = {0, 1} – the input alphabet;
–
F = {q0, q01, q02, q03, q11, q12, q13} – the set of accepting states;
–
δ – the transition function given in Table 7.2. A transition diagram of this automa-
ton is shown in Figure 7.3.
The finite automaton counts successive 0s and successive 1s. Successive 0s are
counted by moving to states q01, q02, q03. Likewise, successive 1s are counted in
states q11, q12, q13. Words with more than three the same successive letters are not
included in the language. For that reason, when the fourth 0 or fourth 1s appears, that
is, the control unit is in the state q03 or q13, then the transition is made to the state qR,
which is not an accepting state and, in this way, it is a trap for words with successive
four or more 0s or 1s.

7.1 Deterministic finite automata
|
161
Table 7.2: The transition function of the finite automaton designed in Example 7.2.
δ
0
1
→q0 →
q01
q11
q01 →
q02
q11
q02 →
q03
q11
q03 →
qR
q11
q11 →
q01
q12
q12 →
q01
q13
q13
q01
qR
qR
qR
qR
Figure 7.3: Transition diagram of the finite automaton accepting the language given in Example 7.2.
Examples of computation:
–
q0 – the word ε is accepted because computation for this word is terminated in the
state q0, which is an accepting one;
–
q0 1 q11 1 q12 0 q01 0 q02 0 q03 1 q11 – the word 110001 is accepted because
computation for this word is terminated in the state q11, which is an accepting
one;
–
q0 1 q11 1 q12 0 q01 0 q02 0 q03 0 qR 1 qR – the word 1100001 is not accepted
because computation for this word is terminated in the state qR, which is not an
accepting one.
A transition relation identified in Definition 7.3 yields exactly one state for a given
configuration of a finite automaton. Namely, for a given state q and a given symbol of
an input alphabet a, there is exactly one state p related to the given state q. The state p
is yielded by a transition function: p = δ(q, a). This property comes from the definition
of a transition function, which is total and its value is a state: δ : Q × Σ →Q. In other
words, for a given state, exactly one state is related to it with regard to a given symbol of
an input alphabet. The transitive closure of a transition relation is a function as well,
that is, for a given state, exactly one state is related with regard to a given sequence of

162
|
7 Finite automata
symbols of an input alphabet. This property is exploited in a definition of a so-called
closure of transition function.
Definition 7.7. A closure of a transitions function δ of a given deterministic finite au-
tomaton M = (Q, Σ, δ, q0, F) is the function ̂δ satisfying the following conditions:
1.
̂δ : Q × Σ∗→Q;
2.
(∀q ∈Q) ̂δ(q, ε) = q;
3.
(∀q ∈Q)(∀a ∈Σ)(w ∈Σ∗) ̂δ(q, wa) = δ( ̂δ(q, w), a).
For deterministic finite automata, the closure of a transition function extends its do-
main from an input alphabet of an automaton to the set of all words over an input
alphabet. The closure of a transition function applied to an initial configuration of a
finite automaton immediately yields a result of computation ̂δ(q0, w) of this automaton
for the initial state q0 and an input word w = a1a2 . . . an.
Remark 7.2. The restriction of the closure ̂δ of a transition function δ to the domain
of δ is equal to δ:
̂δ|Q×Σ = δ
For that reason, both a transition function (of a deterministic finite automaton) and
its closure are denoted by the same symbol δ unless it might result in some misinter-
pretation.
7.2 Nondeterministic finite automata
In the previous section, we have studied deterministic finite automata. In this section,
we introduce and discuss nondeterministic finite automata. Nondeterministic finite
automata, compared to their deterministic counterparts, differ in the form of the tran-
sition function. A transition function of nondeterministic finite automata allows for
choosing a transition among several states. This new feature simplifies solutions of
problems, though it does not increase the computational abilities of finite automata.
Below, we will prove that both classes of automata, that is, deterministic finite au-
tomata and nondeterministic finite automata, are equivalent with regard to the na-
ture of the accepted languages. This means that each of these two classes of finite
automata accepts the same class of languages. In other words, for a given automa-
ton of one class, we can build an equivalent automaton of another class. The proof
of equivalence of both classes of automata is a constructive one, which means that it
formulates a method of designing a deterministic finite automaton, which is equiva-
lent to a given nondeterministic one. Of course, a trivial design of nondeterministic
automata equivalent to deterministic ones is also mentioned.

7.2 Nondeterministic finite automata
|
163
Definition 7.8. A nondeterministic finite automaton is a system
M = (Q, Σ, δ, q0, F),
where:
–
δ – a transition function, δ : Q × Σ →2Q;
–
Q, Σ, q0, F are the same as given in Definition 7.1.
A transition function of nondeterministic finite automata is a total function, the
same as for deterministic finite automata, that is, it is defined for all pairs of argu-
ments. The values of a transition function are subsets of a set of states, including the
empty set (which is a subset of the set of all states).
Interpretation of nondeterministic finite automata is similar to the interpretation
of deterministic ones, that is, as it is shown in Figure 7.1.
A definition of a configuration (step description) of nondeterministic finite au-
tomata is identical with the definition of a configuration of deterministic finite au-
tomata; cf. Definition 7.2.
Remark 7.3. Like in the case of deterministic finite automata, nondeterministic ones
are, that is, at accepting a language. The computation of a given nondeterministic fi-
nite automaton is done according to the following intuitive procedure:
1.
the initial configuration of a given finite automaton is described as follows:
a.
an input data, a word w = a1a2 . . . an over input alphabet Σ, is stored on the
input tape; cf. Figure 7.1;
b.
the head of the input tape is placed over the first (leftmost) symbol of the input
word;
c.
the control unit is in the initial state q0;
2.
based on:
a.
a state q of the control unit;
b.
an input symbol a;
the automaton realizes the following actions:
c.
a set of states {p1, p2, . . . , pk} being the value of the transition function δ(q, a) is
computed, where a ∈Σ is the input symbol and q, p1, p2, . . . , pk ∈Q are states;
d.
if the set yielded by a transition function is empty, then the computation is
terminated and input is rejected, otherwise
e.
a statepi is nondeterministically picked upfrom the set of states{p1, p2, . . . , pk};
f.
the input head is shifted right;
g.
the control unit switches to the state pi;
3.
if the input is not empty, then computation goes to point 2; otherwise, computa-
tion is terminated and an automaton responds with a state of the control unit.

164
|
7 Finite automata
Of course, the computation of a nondeterministic finite automaton always terminates,
as we have seen it for deterministic finite automata.
The symbols ≻, ≻M and ≻k are used in the usual way, refer to our discussion on
deterministic finite automata.
Definition 7.9. Transitions of a nondeterministic finite automaton from a binary re-
lation in the space of all possible configurations of the automaton, that is, any two
configurations are related if and only if the later one is derived from the former one
by a transition in terms of point (2) of Remark 7.3. This relation is called the transi-
tion relation of a given nondeterministic finite automaton. The transitive closure of
the transition relation is denoted by ≻∗.
Definition 7.10. A computation of a nondeterministic finite automaton
M = (Q, Σ, δ, q0, F)
is interpreted as a tree such that:
–
its nodes are labeled by configurations of the automaton;
–
its root is labeled by the initial configuration;
–
for any node ηp, its every child ηc is related to it in the transition relation, that is,
ηp ≻ηc.
Remark 7.4. Computation of a nondeterministic automaton is a sequence of configu-
rations, similar in the case of a deterministic one. Consecutive configurations are set
by nondeterministic choice of a state from the value of the transition function. How-
ever, since computation is usually interpreted as a tree, we will call the computation
tree a computation.
Remark 7.5. Note that a computation tree of a nondeterministic finite automaton is a
k-tree, where k is the maximal number of values yielded by the transition function for
given arguments, that is, k is the degree of nondeterminism. Moreover, the edges of
every level of such a tree are labeled with the same input symbol.
Remark 7.6. A nondeterministic finite automaton accepts its input if and only if the
computation tree has a path from the root to a leaf labeled by an accepting configura-
tion.
Remark 7.7. Formal definitions of acceptance of input and of a language accepted by
a nondeterministic finite automaton are identical with the respective definitions of
deterministic finite automata; cf. Definition 7.4 and Definition 7.6.
Example 7.3. Design a finite automaton accepting the language L of binary words with
three successive 0s or three successive 1s.

7.2 Nondeterministic finite automata
|
165
Solution. The following automaton accepts the language L:
M = (Q = {q0, q10, q20, q11, q21, qA}, Σ = {0, 1}, δ, q0, F = {qA})
where the transition function δ : Q × Σ →2Q is given in Table 7.3. A transition diagram
of this automaton is shown in Figure 7.4.
Table 7.3: The transition function of the finite automaton designed in Example 7.3.
δ
0
1
→q0
{q0, q10}
{q0, q11}
q10
{q20}
0
q20
{qA}
0
q11
0
{q21}
q21
0
{qA}
qA →
{qA}
{qA}
Figure 7.4: Transition diagram of the finite automaton designed in Example 7.3.
An example of computation of the above nondeterministic automaton is given in Fig-
ure 7.5. The input word w = 010001 is accepted because the computation tree has a
path from the root to an accepting leaf.
An algorithm realized by the automaton is shown below:
–
the automaton stays in the state q0 reading input symbols until a sequence of three
successive 0s or three successive 1s arrives;
–
when the beginning of a sequence of three successive 0s or 1s is nondeterministi-
cally encountered, a transition is made to the state q10 or q11, respectively;
–
next two consecutive 0s or 1s are counted by transitions to states q20 and qA or to
states q21 and qA;
–
when the state qA has been reached, computation stays in this state for next com-
ing input symbols;
–
input words, for which the accepting state qA is reached, are accepted and only
such words.

166
|
7 Finite automata
Figure 7.5: The computation of the automaton designed in Example 7.3 for the input word
w = 010001.
The transition relation of nondeterministic automata yields a subset of a set of states
for a given configuration (unlike the transition relation of deterministic automata,
which yields exactly a single state). For a given state q and for a given symbol a of
an input alphabet, states p ∈P ⊂Q, where P is the value of the transition function,
are related to the given state q. This property comes from the definition of a transition
function, which is total and its values are subsets of the set of states: δ : Q × Σ →2Q.
The transitive closure of a transition relation also relates subsets of the set Q for a given
state and a given symbol of an input alphabet. The following definition provides de-
tails of a so-called closure of transition function for nondeterministic automata.
Definition 7.11. A closure of a transitions function δ of a given nondeterministic finite
automaton M = (Q, Σ, δ, q0, F) is the function:
1.
̂δ : Q × Σ∗→2Q
2.
(∀q ∈Q) ̂δ(q, ε) = {q}
3.
(∀q ∈Q)(∀a ∈Σ)(∀w ∈Σ∗) ̂δ(q, wa) = δ( ̂δ(q, w), a)
where the following designation stands for δ( ̂δ(q, w), a): δ(P, a) = ⋃p∈P δ(p, a), for any
P ⊂Q.
As is in case of deterministic finite automata, the closure of a transition function
of a nondeterministic finite automaton extends a domain of a transition function from
an input alphabet of an automaton to the set of all words over an input alphabet. The
closure of a transition function applied to an initial configuration of a finite automaton
immediately yields the result of computation ̂δ(q0, w) of this automaton for the initial
state q0 and an input word w = a1 a2 . . . an.

7.2 Nondeterministic finite automata
|
167
Remark 7.8. For nondeterministic finite automata, the restriction of the closure ̂δ of a
transition δ to the domain Q × Σ is equal to the transition function δ:
̂δ|Q×Σ = δ
For that reason, both a transition function of a nondeterministic finite automaton
and its closure are denoted by the same symbol δ unless it may cause misinterpreta-
tion.
Remark 7.9. An input word w is accepted by the nondeterministic finite automaton
M = (Q, Σ, δ, q0, F) if and only if δ(q0, w) ∩F
̸= ⌀.
A question is raised whether the class of nondeterministic finite automata is equiv-
alent to the class of deterministic ones. The answer is positive.
On the one hand, a deterministic automaton is a nondeterministic one with the
degree of nondeterminism equal to one, that is, which does not exploit nondetermin-
ism. Formally, a deterministic automaton can be turned into a nondeterministic one
by replacing a value of its transition function, which is a state, by the set including
only this single state.
Conversely, we will prove that, for a given nondeterministic automaton, there ex-
ists an equivalent deterministic one, that is, accepting the same language. An idea of
designing such a deterministic automaton is based on observation of computations of
both classes of automata. The idea relies on turning a computation of a nondetermin-
istic automaton, which is a tree of configurations, into a sequence of configurations:
states of every level of a computation tree are collected into a set of them. In such a
way, a tree is twisted into a sequence of alternating sets of states and symbols of the
input alphabet. Such a sequence corresponds to the computation of a deterministic
finite automaton, supposing that sets of states represent potential states of a deter-
ministic finite automaton.
The details behind these intuitive observations are outlined as follows.
Proposition 7.1. The class of nondeterministic finite automata is equivalent to the class
of deterministic ones.
Proof. First, the following deterministic automaton:
M = (Q, Σ, δ, q0, F)
is equivalent to a nondeterministic one:
M = (Q, Σ, δ′, q0, F)
such that δ′ : Q × Σ →2Q, δ′(q, a) = {δ(q, a)} for q ∈Q and a ∈Σ.

168
|
7 Finite automata
Second, let us assume that now a nondeterministic finite automaton is
M = (Q, Σ, δ, q0, F)
An equivalent deterministic automaton is denoted as follows:
M′ = (Q′, Σ, δ′, q′
0, F′)
where:
–
Q′ ≅2Q – states of the deterministic automaton correspond to sets of the nonde-
terministic one. A state of Q′ corresponding to a set of states {qi1, qi2, . . . , qij} will
be denoted [qi1, qi2, . . . , qij] just to distinguish subsets of Q from states of Q′;
–
q′
0 = [q0] – the initial state of M′ is the set including the initial set of M;
–
F′ = {[qi1, qi2, . . . , qij] ∈Q′ : {qi1, qi2, . . . , qij} ∩F
̸= ⌀} – accepting states of M′ are
labeled by sets of states including an accepting state(s) of M;
–
δ′([qi1, qi2, . . . , qij], a) = [pi1, pi2, . . . pik] ⇔⋃j
l=1 δ′(qil, a) = {pi1, pi2, . . . , pik} – the tran-
sition function of M′ takes union of its values (which are sets of states of M), for
states included in its argument (which corresponds to a set of states of M).
A formal proof of equivalence of M and M′ is based on mathematical induction con-
cerning the length of input word of both automata.
Let us prove that the closure of transition functions of both automata hold the
equivalence for any w ∈Σ∗(note that the same symbol denotes both a transition func-
tion and its closure):
δ′([q0], w) = [qi1, qi2, . . . , qij] ⇔δ(q0, w) = {qi1, qi2, . . . , qij}
(∗)
1.
for the word of length 0, that is, for the empty word, the equivalence (∗):
δ′([q0], ε) = [q0] and δ(q0) = {q0} is derived directly from definitions of the closure
of transition functions;
2.
let us check if the equivalence (∗) holds for a word a1 a1 . . . an a = w a. In virtue
of the inductive assumption, we have that this equivalence holds for any word
w = a1 a1 . . . an ∈Σ∗. Let a ∈Σ. Then, based on inductive assumption, we get
δ′([q0], wa) = δ′(δ′([q0], w), a) = δ′([qi1, qi2, . . . , qij], a) and
δ(q0, wa) = δ(δ(q0, w), a) = δ({qi1, qi2, . . . , qij}, a)
for some set of states {qi1, qi2, . . . , qij} ⊂Q.
Now, let us compute δ′([qi1, qi2, . . . , qij], a) based on the above definition of δ′:
δ′([qi1, qi2, . . . , qij], a) = [pi1, qi2, . . . , pik], where
⋃j
l=1 δ′(qil, a) = {pi1, pi2, . . . , pik}
On the other hand,
δ({qi1, qi2, . . . , qij}, a) = ⋃j
l=1 δ′(qil, a) = {pi1, pi2, . . . , pik}

7.2 Nondeterministic finite automata
|
169
3.
utilizing mathematical induction based on (1) and (2) we conclude that the equiv-
alence (∗) holds.
Finally, let us notice that an input word is accepted by the nondeterministic finite
automaton M if and only if it is accepted by the deterministic finite automaton M′. This
property comes directly from the equivalence (∗), Remark 7.9 and definition of the set
of accepting states F′ of the automaton M′.
Example 7.4. Design a nondeterministic finite automaton accepting the language L of
binary words ending with at least three successive 0’s or at least three successive 1s.
Solution. The following automaton accepts the language L:
M = (Q = {q0, q10, q20, q30, q11, q21, q31}, Σ = {0, 1}, δ, q0, F = {q30, q31})
where the transition function δ : Q × Σ →2Q is given in Table 7.4 (also compare Prob-
lem 7.1). A transition diagram of this automaton is shown in Figure 7.6. Compare also
solution of Problem 7.1.
Table 7.4: Transition function of the finite automaton designed in Example 7.4.
δ
0
1
→q0
{q0, q10}
{q0, q11}
q10
{q20}
0
q20
{q30}
0
q30 →
0
0
q11
0
{q21}
q21
0
{q31}
q31 →
0
0
Figure 7.6: Transition diagram of the finite automaton designed in Example 7.4.
Let us outline the algorithm realized by the automaton:
–
the automaton stays in the state q0 reading input symbols until a sequence of suc-
cessive 0s or successive 1s arrives and this sequence ends the input word;

170
|
7 Finite automata
–
when the beginning of a sequence of successive 0s or 1s is nondeterministically
encountered, a transition is made to the state q10 or q11, respectively;
–
next consecutive 0s or next consecutive 1s are counted by transitions to states q20
and q30 or to states q21 and q31;
–
only such input words are accepted, for which the accepting state q30 or q31 is
reached.
A deterministic automaton M′ = (Q′, Σ, δ, q′
0, F′) is designed based on Proposition 7.1.
According to the designing method, in the automaton M′ the set of states Q′ has 27 =
128 states and the set of accepting states F′ has 27 −25 = 96 states. However, only a
few states are reachable from the initial state q′
0 = [q0]. All states, which cannot be
obtained in any computation starting from the initial state, are useless from the point
of view of acceptance of an input word. For that reason, only states reachable from the
initial state may be included in the deterministic finite automaton being designed. The
transition table is shown in Table 7.5. The table is filled in by starting with the initial
state and then including new states yielded by the transition function δ′. All states,
which are not obtained in this way, are useless.
Table 7.5: Transition function of the deterministic finite automaton equivalent to the one designed in
Example 7.4.
δ′
0
1
→[q0]
[q0, q10]
[q0, q11]
[q0, q10]
[q0, q10, q20]
[q0, q11]
[q0, q10, q20]
[q0, q10, q20, q30]
[q0, q11]
[q0, q10, q20, q30] →
[q0, q10, q20, q30]
[q0, q11]
[q0, q11]
[q0, q10]
[q0, q11, q21]
[q0, q11, q21]
[q0, q10]
[q0, q11, q21, q31]
[q0, q11, q21, q31] →
[q0, q10]
[q0, q11, q21, q31]
121 states are useless
their transitions
are not shown here
In the transition diagram of this automaton, which is given in Figure 7.7, the useless
states (accepting and not accepting) are indicated symbolically. Transitions may go
from the useless states to the useful ones, as revealed there in the form of symbolic
arrows. However, the transition cannot go from the useful states to the useless ones,
so there is no arrow going in the opposite direction.
As a result of the above notes, useless states will be removed from the determin-
istic finite automaton designed according to Proposition 7.1. The automaton designed

7.3 Finite automata with ε-transitions
|
171
Figure 7.7: Transition diagram of the deterministic finite automaton designed in Example 7.4. La-
bels of states include indexes only, for example, q0 represents the state [q0], q210 represents
[q0, q10, q20], q3211 represents [q0, q11, q21, q31].
here is
M′ = (Q′, Σ, δ′, q′
0, F′),
where:
–
Q = {[q0], [q0, q10], [q0, q10, q20], [q0, q10, q20, q30], [q0, q11], [q0, q11, q21],
[q0, q11, q21, q31]}
–
δ′ – the transition function is given in Table 7.5;
–
q0 = [q0];
–
F′ = {[q0, q10, q20, q30], [q0, q11, q21, q31]}.
7.3 Finite automata with ε-transitions
In previous sections of this chapter, we discussed two classes of finite automata: deter-
ministic finite automata and nondeterministic ones. We proved that both classes are
equivalent with regard to accepted languages; that is, for an automaton in one class,
an equivalent automaton of another class can be designed. The equivalence of both
classes effectively helps in solving problems because we can choose an automaton
from a class, which is more appropriate for a problem to be solved. Finite automata
with epsilon-transitions, ε-transitions for short, create a third class of finite automata.
ε-transitions is the next tool, which may significantly simplify solutions of problems.
However, ε-transitions do not increase the computational power of finite automata.
In further parts of this section, an equivalence of finite automata with ε-transitions
with nondeterministic automata is proved. The proof is a constructive one, that is, it
develops the method of designing a nondeterministic finite automaton that is equiva-
lent to a given finite automaton with ε-transitions. A design of a finite automaton with

172
|
7 Finite automata
ε-transitions that is equivalent to a given nondeterministic finite automaton is also
shown.
Definition 7.12. A finite automaton with ε-transitions is a system
M = (Q, Σ, δ, q0, F),
where:
–
δ – a transition function, δ : Q × (Σ ∪{ε}) →2Q;
–
Q, Σ, q0, F stay the same as described in Definition 7.1.
A transition function of finite automata with ε-transitions is a total function de-
fined for states and for symbols of the input alphabet with the empty word attached.
The type values of a transition function stay the same as for nondeterministic fi-
nite automata; that is, they are subsets of a set of states. Thus, finite automata with
ε-transitions are nondeterministic ones.
The difference between nondeterministic finite automata and finite automata with
ε-transitions lays in the ability of the last mentioned one to make a transition with-
out checking an input symbol. Let us recall that we already considered a property
ε-transitions of pushdown automata. When an ε-transition is made, an input symbol
is not checked. This means that ε-transitions are made only for a given state of a cur-
rent configuration. From another point of view, a configuration for ε-transition is only
a state and no input symbol.
A definition of a configuration (step description) of a finite automaton with
ε-transitions is identical with the definition of a configuration of deterministic finite
automata, cf. Definition 7.2.
An obvious property ε ∘ε = ε leads to an interesting idempotent operation on
states for a given finite automaton with ε-transitions
M = (Q, Σ, δ, q0, F)
The operation is called epsilon closure, εCl for short. It is defined as follows:
εCl(q) = {p ∈Q : p ∈δ∗(q, ε)}
for q ∈Q,
where
δ∗: Q →2Q, δ∗(q, ε) =
∞
⋃
k=0
δk(q, ε)
and
δ0(q, ε) = {q}
δk+1(q, ε) = δ(δk(q, ε))

7.3 Finite automata with ε-transitions
|
173
where
δ(P, X) = ⋃
p∈P
δ(p, X)
for P ⊂Q, X ∈(Σ ∪{ε})
The εCl is idempotent, that is,
εCl(q) = εCl(εCl(q)) =
⋃
p∈εCl(q)
εCl(p)
Note that δ∗(q, ε) = ⋃∞
k=0 δk(q, ε) = ⋃r
k=0 δk(q, ε), where r does not need to be
greater or equal than the number of states of an automaton. In fact, when a transition
diagram of an automaton is considered, then r is equal to the length of the longest
path starting at q and having edges labeled with ε.
Example 7.5. Let us consider a finite automaton with ε-transitions:
M = (Q, Σ = {a, b}, δ, q0, F = {qA}),
where:
–
Q = {q0, q0a, q1a, q2a, q0b, q1bq2b, qA}
–
δ : Q × (Σ ∪{ε}) →2Q is the transition function given in Table 7.6. The transition
diagram of this automaton is given in Figure 7.8. The automaton accepts words, in
which letters come in pairs.
Table 7.6: The transition function of the finite automaton designed in Example 7.5.
δ
a
b
ε
εCl
→q0
0
0
{q0a, q0b}
{q0, q0a, q0b, q2a, q2b, qA}
q0a
{q1a}
0
{q2a}
{q0, q0a, q0b, q2a, q2b, qA}
q1a
{q2a}
0
0
q1a
q2a
0
0
{qA}
{q0, q0a, q0b, q2a, q2b, qA}
q0b
0
q1b
{q2b}
{q0, q0a, q0b, q2a, q2b, qA}
q1b
0
{q2b}
0
q1b
q2b
0
0
{qA}
{q0, q0a, q0b, q2a, q2b, qA}
qA →
0
0
{qA}
{q0, q0a, q0b, q2a, q2b, qA}
The transition table shown in Table 7.6 has ε closure column. Such a column, which
contains ε closure of states, is not an integral part of a transition table. However, it is
recommended to attach it for the easiness of further discussion or problem-solving.
Remark 7.10. Like in the case of deterministic and nondeterministic finite automata,
finite automata with ε-transitions are, that is, at accepting languages. Computation of

174
|
7 Finite automata
Figure 7.8: The transition diagram of the deterministic finite automaton with ε-transitions given in
Example 7.5.
a given finite automaton with ε-transitions is done according to the following intuitive
procedure:
1.
the initial configuration of a given automaton is described as follows:
a.
an input data, a word w = a1 a2 . . . , an over input alphabet Σ, is stored on the
input tape;
b.
the head of the input tape is placed over the first (leftmost) symbol of the input
word;
c.
the control unit is in the initial state q0;
2.
based on a state q of the control unit and on an input symbol a ∈Σ:
a.
εCl(q) is computed;
b.
a state q′ ∈εCl(q) is nondeterministically picked up;
c.
a set of states {p1, p2, . . . , pk} = δ(q′, a) is computed;
d.
if the set yielded by a transition function is empty then computation is termi-
nated and an input is rejected, otherwise
e.
a state p′ ∈{p1, p2, . . . , pk} is nondeterministically picked up;
f.
εCl(p′) is computed;
g.
a state p ∈εCl(p′) is nondeterministically picked up;
h.
the input head is shifted right;
i.
the control unit switches to the state p;
3.
if the inputis nonempty, then computationproceedstothe point 2, otherwise com-
putation is terminated and an automaton responds a state of the control unit.
Like for deterministic and nondeterministic finite automata, computation of a finite
automaton with ε-transitions always terminates.
The symbol ≻denotes a transition (of a finite automaton with ε-transitions) in
terms of point (2) of Remark 7.10. The symbols ≻M and ≻k are used in a usual way; cf.
deterministic and nondeterministic finite automata.
Definition 7.13. A binary relation in the space of all possible configurations of the au-
tomaton with ε-transition is created by transitions of the automaton (both: transitions

7.3 Finite automata with ε-transitions
|
175
related to input symbol reading and ε-transitions). The symbol ≻∗denotes the transi-
tive closure of the transition relation.
Definition 7.14. A
computation
of
a
finite
automaton
with
ε-transitions
M = (Q, Σ, δ, q0, F) is a tree such that:
–
its nodes are labeled by configurations of an automaton;
–
its root is labeled by the initial configuration;
–
levels created by nodes are distinguished, the root creates the 0th level, children
of the root create the 1st level, etc.;
–
nodes of an odd level are yielded by ε closure applied to nodes of the previous
level;
–
nodes of an even level are yielded by transition function applied to nodes of the
previous level;
–
the bottom level has an odd number.
Remark 7.11. Note that, like for nondeterministic finite automata, a computation tree
of a finite automaton with ε-transitions is a k-tree, where k is the maximal number of
values yielded by the transition function for given arguments (input symbols or the
empty word). Moreover, edges of every level of such a tree are labeled with the same
input symbol or with the empty word.
Remark 7.12. Formal definitions of acceptation of input and of a language accepted
by a finite automaton with ε-transitions are identical with respective definitions for
deterministic and nondeterministic finite automata, that is, Definition 7.5 and Defini-
tion 7.6.
Example 7.6. Find a computation tree of a finite automaton with ε-transitions built in
Example 7.5 for sample words.
Solution. Computation trees for the empty word ε and for the word w = aa are dis-
played in Figure 7.9. Both words are accepted because in every tree, a path from the
root to an accepting state exists.
Note that transitions of a finite automaton with ε-transitions and a nondeter-
ministic finite automaton are essentially different; cf. point (2) of Remark 7.3 and
Remark 7.10. As a result, the closure of a transition function of finite automata with
ε-transitions differs basically from the closure of a transition function of deterministic
and nondeterministic finite automata. The following definition provides details of the
closure of transition function for finite automata with ε-transitions.
Definition 7.15. The closure of a transitions function δ of a given finite automaton with
ε-transitions M = (Q, Σ, δ, q0, F) is the function: ̂δ : Q × Σ∗→2Q
1.
(∀q ∈Q) ̂δ(q, ε) = εCl(q)
2.
(∀q ∈Q)(∀a ∈Σ)(∀w ∈Σ∗) ̂δ(q, wa) = εCl(δ( ̂δ(q, w), a))
where: δ(P, a) = ⋃p∈P δ(p, a) and εCl(P, a) = ⋃p∈P εCl(p, a) for P ⊂Q.

176
|
7 Finite automata
Figure 7.9: Computation trees of the automaton designed in Example 15 for the empty word ε and the
word w = aa.
As in the case of deterministic and nondeterministic automata, the closure of a transi-
tion function of a finite automaton with ε-transitions extends a domain of a transition
function from an input alphabet to the set of all words over the input alphabet. The
closure of a transition function applied to an initial configuration of a finite automa-
ton immediately yields a result of computation ̂δ(q0, w) of this automaton for the initial
state q0 and an input word w = a1 a2 . . . an. However, a restriction of the closure of a
transition function ̂δ to the domain Q × Σ is not equal to δ. In fact,
̂δ|Q×Σ = εCl(δ(εCl(q), a))
For that reason, a transition function of a finite automaton with ε-transitions and
its closure are denoted by different symbols, δ and ̂δ, respectively.
Remark 7.13. An input word w is accepted by a finite automaton with ε-transitions
M = (Q, Σ, δ, q0, F) if and only if ̂δ(q0, w) ∩F
̸= ⌀.
A question may be asked whether finite automata with ε-transitions are equiva-
lent to deterministic ones. The answer is positive. Below we present a proof that finite
automata with ε-transitions are equivalent to nondeterministic finite automata. Equiv-
alence with deterministic finite automata comes in the form of a direct conclusion of
Proposition 7.2.
It is clear that a nondeterministic finite automaton is such a finite automaton with
ε-transitions, which does not use ε-transitions. Formally, a nondeterministic finite au-
tomaton can be turned to a finite automaton with ε-transitions by extending a domain

7.3 Finite automata with ε-transitions
|
177
of its transition function to Q × (Σ ∪{ε}) and setting a set of the values of ε-transitions
to the empty set.
On the other hand, we will prove that, for a given finite automaton with ε-transi-
tions, there exists an equivalent nondeterministic one, that is, accepting the same lan-
guage. An idea of design of such a nondeterministic automaton is based on analysis
of a description of a transition given in point 2 of Remark 7.10. It could be concluded
that a transition, as described there, corresponds to a transition of a nondeterministic
finite automata. On the other hand, a transition described there is what closure of a
transition function for an input symbol yields. Details are given as follows.
Proposition 7.2. Finite automata with ε-transitions are equivalent to nondeterministic
ones.
Proof. First, the following nondeterministic finite automaton:
M = (Q, Σ, δ, q0, F)
is equivalent to an automaton with ε-transitions:
Mε = (Q, Σ, δ′, q0, F)
such that δ′ : Q × (Σ ∪{ε}) →2Q, δ′(q, a) = δ(q, a), δ′(q, ε) = ⌀for q ∈Q and a ∈Σ
Second, let us assume that now a finite automaton with ε-transitions:
Mε = (Q, Σ, δ, q0, F)
The equivalent nondeterministic automaton is denoted as follows:
M′ = (Q, Σ, δ′, q0, F′)
where:
–
δ′ ≡̂δ |Q×Σ – the transition function of M′ is equal to the closure of the transition
function of M restricted to Q × Σ;
–
F′ = { F ∪{q0}
if εCl(q0) ∩F
̸= ⌀
F
otherwise.
A formal proof of equivalence of Mε and M′ is based on mathematical induction with
regard to the length of input word of both automata.
Let us prove that the closure of transition function of both automata is equal for
any nonempty word w ∈Σ+
δ′(q0, w) = ̂δ(q0, w)
(∗)

178
|
7 Finite automata
Note that δ′ denotes the transition function of M′ and its closure. However, the tran-
sition function of Mε and its closure cannot be denoted by the same symbol.
1.
for words of length 1 equality (∗) is derived directly from definition of both func-
tions;
2.
let us check if equality (∗) holds for a word a1 a1 . . . an a = w a:
δ′(q0, wa)
=
⏟⏟⏟⏟⏟⏟⏟
from closure
of transition
function δ′
⋃
p∈δ′(q0,w)
δ′(p, a)
=
⏟⏟⏟⏟⏟⏟⏟
from
inductive
assumption
⋃
p∈̂δ(q0,w)
δ′(p, a)
=
⏟⏟⏟⏟⏟⏟⏟
from
definition
of δ′
⋃
p∈̂δ(q0,w)
̂δ(p, a)
=
⏟⏟⏟⏟⏟⏟⏟
from closure of
transition
function ̂δ
̂δ(q0, wa)
3.
utilizing mathematical induction to (1) and (2), we conclude that the equality (∗)
holds.
We have just confirmed that both automata Mε and M′ compute the same set of states
for given input word w ∈Σ+. This is only a step to finalize the proof, that is, to show
that a given input word w ∈Σ∗is either accepted by both automata Mε and M′, or is
rejected by them. Let us consider the following cases:
–
the empty word w = ε at input. Transition functions yield the following values in
this case: ̂δ(q0, ε) = εCl(q0) and δ′(q0, ε) = {q0}. Thus, ε is accepted by Mε if and
only if ̂δ(q0, ε) ∩F
̸= ⌀. On the other hand, q0 ∈F′ if and only if εCl(q0) ∩F
̸= ⌀.
Since ε is accepted by M′ if and only if q0 ∈F′, then the empty word ε is either
accepted by both automata, or is rejected by them;
–
a nonempty word w ∈Σ+ at input and εCl(q0) ∩F = ⌀or q0 ∈F; in such, the
case F′ = F. Joint acceptation is derived from the equality (∗) and definitions of
acceptation of an input word by Mε and M′;
–
a nonempty word w ∈Σ+ at input and εCl(q0) ∩F
̸= ⌀and q0 ∉F. Then for any
nonempty word w ∈Σ+ we get two subcases:
–
if q0 ∉δ′(q0, w), then either both δ′(q0, w) and ̂δ(q0, w) include an accepting
state, or none does. Again, a joint acceptation is derived from the equality (∗)
and the definitions of acceptance of an input word by Mε and M′;
–
if q0 ∈δ′(q0, w) than the word w is accepted by M′. Also δ′(q0, w) =
̂δ(q0, w)
due to (∗). We have εCl( ̂δ(q0, w))∩F
̸= ⌀, because q0 ∈̂δ(q0, w). Let us assume
that w = ua for u ∈Σ∗and a ∈Σ. Then, from Definition 7.15 and idempotency
of εCl we conclude that some accepting state belongs to ̂δ(q0, w) due to
εCl( ̂δ(q0, w)) = εCl(εCl(δ( ̂δ(q0, u), a))) = εCl(δ( ̂δ(q0, u), a)) = ̂δ(q0, w)
and finally, since ̂δ(q0, w) = δ′(q0, w) ∩F
̸= ⌀, we derive that the word w is
accepted by Mε.
–
no other case can be found.

7.3 Finite automata with ε-transitions
|
179
We have proved that a word w ∈Σ∗is either jointly accepted by both automata or is
jointly rejected by them. This completes the proof.
Example 7.7. Design a nondeterministic finite automaton equivalent to the automa-
ton shown in Example 7.5. Apply the design method used in the proof of Proposition 7.2.
Solution. The transition table of the nondeterministic finite automaton
M = (Q = {q0, q0a, q1a, q2a, q0b, q1b, q2b, qA}, Σ = {a, b}, δN, q0, F = {qA})
is shown in Table 7.7. The transition diagram of the automaton is shown in Figure 7.10.
There are numerous transitions in the nondeterministic automaton. At a glance, it be-
comes clear that many states and transitions are duplicates unnecessary for accep-
tance of the language.
Table 7.7: Transition table of the nondeterministic finite automaton equivalent to the automaton with
ε-transitions designed in Example 7.5.
δN
a
b
→q0 →
{q1a}
{q1b}
q0a
{q1a}
{q1b}
q1a
{q0, q0a, q0b, q2a, q2b, qA}
0
q2a
{q1a}
{q1b}
q0b
{q1a}
{q1b}
q1b
0
{q0, q0a, q0b, q2a, q2b, qA}
q2b
{q1a}
{q1b}
qA →
{q1a}
{q1b}
Figure 7.10: Transition diagram of a nondeterministic finite automaton equivalent to the automa-
ton with ε-transitions designed in Example 7.5. Solid edges identify transitions by letter a, dashed
edges identify transitions by letter b.
Transition table of the deterministic finite automaton.
M = (Q = {q0, q1a, q1b, qA, ⌀}, Σ = {a, b}, δD, q0, F = {qA})

180
|
7 Finite automata
is shown Table 7.8. The transition diagram of the automaton is shown in Figure 7.11.
The set of useful states is significantly reduced compared to the nondeterministic au-
tomaton. Based on the Myhill–Nerode theorem discussed in Chapter 8, we can prove
that this is a minimal deterministic automaton accepting this language, minimal in
terms of the number of states.
Table 7.8: Transition table of the nondeterministic finite automaton equivalent to the au-
tomaton with ε-transitions designed in Example 7.5. Notice that [q0,0a,0b,2a,2b,A] identifies
{q0, q0a, q0b, q2a, q2b, qA}.
δD
a
b
→[q0] →
[q1a]
[q1b]
[q1a]
[q0,0a,0b,2a,2b,A]
0
[q1b]
0
[q0,0a,0b,2a,2b,A]
[q0,0a,0b,2a,2b,A] →
[q1a]
[q1b]
0
0
0
Figure 7.11: Transition diagram of a deterministic finite automaton with transition table given in
Table 7.8. Square brackets are dropped in names of states. qA identifies {q0, q0a, q0b, q2a, q2b, qA}.
7.4 Finite automata as Turing machines
In this section, we justify that finite automata are restricted Turing machines. Further-
more, since finite automata always terminate their computation, they are restricted
Turing machines with the stop property. A discussion will be focused on deterministic
finite automata. However, since classes of nondeterministic automata and automata
with ε-transitions one are equivalent to the class of deterministic ones, the conclu-
sions of this discussion concern all three classes.
A deterministic finite automaton can be simulated by a Turing machine, which
shifts the head right and terminates computation as soon as an input word has been
read. The machine accepts its input if and only if the automaton accepts it. The details
of the design of a Turing machine equivalent to a deterministic finite automaton are
given below.

7.4 Finite automata as Turing machines
|
181
Proposition 7.3. There exists a Turing machine with the stop property equivalent to
a given deterministic finite automaton.
Proof. Let a deterministic finite automaton is given
M = (Q, Σ, δ, q0, F)
A Turing machine in basic model and with halting states equivalent to this determin-
istic finite automaton is
MT = (QT, Σ, ΓT, δT, q0, B, {qA}, {qR}),
where:
–
QT = Q ∪{qA, qR} – two halting states qA and qR are added to the set of states
Q, qA, qR ∉Q;
–
ΓT = Σ∪{B} – the blank symbol B and the input alphabet create the tape alphabet,
B ∉Σ;
–
the transition function δT is described with the conditions:
δT(q, a) = (δ(q, a), B, R)
for q ∈Q, a ∈Σ,
δT(q, B) = { (qA, B, R)
for q ∈F
(qB, B, R)
for q ∈Q −F
The input configuration of the Turing machine MT is:
–
an input word of the deterministic finite automaton M is stored on the tape of MT;
–
the head of the tape is placed over the first cell, that is, on the first input symbol;
–
the control unit of MT is in the initial state q0.
Of course, the Turing machine MT terminates computation as soon as it reaches
a halting state. It accepts its input if and only if the deterministic finite automaton M
accepts this input.
Finite automata are special cases of pushdown automata.
Proposition 7.4. There exists a pushdown automaton equivalent to a given determinis-
tic finite automaton.
Proof. Let a deterministic finite automaton is given as
M = (Q, Σ, δ, q0, F)
A deterministic pushdown automaton equivalent to this deterministic finite automa-
ton is
MS = (Q, Σ, Γ, δS, q0, F),

182
|
7 Finite automata
where:
–
Γ = {▷} – there is only one stack symbol, the initial stack symbol;
–
the transition function δS is described with the condition: δS(q, a, ▷)
=
{(δ(q, a), ▷)}.
7.5 Problems
Problem 7.1. Consider the automaton with the transition function given in the Ta-
ble 7.9 as a solution of Example 7.4.
Table 7.9: Another transition function of the finite automaton designed in Example 7.4.
δ
0
1
→q0
{q0, q10}
{q0, q11}
q10
{q20}
0
q20
{qA}
0
q11
0
{q21}
q21
0
{qA}
qA →
0
0
Problem 7.2. Build a deterministic finite automaton accepting binary numbers divis-
ible by 5.
Hint. Enumerate states of the automaton with remainders from division by 5. To
find transitions, consider remainder from division a binary number n by 5 and remain-
der from the division by 5 a number n with a binary digit attached.
Problem 7.3. Design a finite automaton accepting binary words having no more than
four 1s in every sequence of seven consecutive letters.
Problem 7.4. Let L1 = L(M1) and L2 = L(M2) are languages accepted by deterministic
finite automata M1 and M2. Build a finite automaton, which accepts:
–
union of L1 and L2, that is, the language L = L1 ∪L2;
–
concatenation of L1 and L2, that is, the language L = L1 ∘L2;
–
intersection of L1 and L2, that is, the language L = L1 ∩L2;
–
Kleene closure of L1, that is, the language L = (L1)∗;
–
complement of L1, that is, the language L = Σ∗−L1, where Σ is an alphabet of L1.
Solution. A design employs a parallel computations of both automata M1 and M2 for
union and intersection, a sequential computations of them for concatenation, a mul-
tisequential computations of one automaton for Kleene closure and a normal compu-
tation of it for complement.

7.5 Problems
|
183
Let assume that M1 = (Q1, Σ, δ1, q1
0, F1) and M2 = (Q2, Σ, δ2, q2
0, F2) are deterministic
finite automata with disjoint sets of states Q1 ∩Q2 = ⌀. The following automata M =
(Q, Σ, δ, q0, F) accept given languages, where:
–
for union and intersection:
–
Q = Q1 × Q2;
–
δ((q′, q′′), a) = (δ1(q′, a), δ2(q′′, a)) for a ∈Σ, q′ ∈Q1 and q′′ ∈Q2, this transi-
tion function simulates parallel computation of both automata;
–
q0 = (q1
0, q2
0);
–
for union F = F1 × Q2 ∪Q1 × F2, that is, M accepts if and only if at least one
automaton accepts;
–
for intersection F = F1 × F2, that is, M accept when both automata accept;
note that the automaton M is a deterministic one;
–
for concatenation:
–
Q = Q1 ∪Q2;
–
δ(q, a) = {δ1(q, a)}, for a ∈Σ, q ∈Q1, M simulates computation of M1 for a first
part of an input;
–
δ(q, a) = {δ2(q, a)}, for a ∈Σ, q ∈Q2, M simulates computation of M2 for a
second part of input;
–
δ(q, ε) = {q2
0}, for q ∈F1, this is a ε-transition, which transfers computation
from M1 to M2, assuming that M1 accepts a first part of an input;
–
q0 = q1
0, M starts computing simultaneously with M1;
–
F = F2, accept when the second part of an input is accepted (a first part has
already been accepted; cf. ε-transitions from F1);
–
M = (Q1, Σ, δ, q0, Q1 −F1) accept complement of L1 = L(M1);
–
M′ = (Q1, Σ, δ′, q0, F1) accept (L1)∗, where
–
δ′(q, a) = {δ(q, a)} for q ∈Q1 and a ∈Σ, realizes computation of M1 for parts
of an input word;
–
δ′(q0, ε) = F1 −{q0}, accepts the empty word;
–
δ′(q, ε) = {q0} for q ∈F1 −{q0}, when the previous part of an input has been
accepted, runs computation of M1 for next part of an input.
Problem 7.5. A language L is accepted by a deterministic finite automaton
M = (Q, Σ, δ, q0, F). Let
L′ = {w = a1 a2 a3 a4 . . . an ∈L : w includes all letters of the alphabet Σ}.
Hint. Design a deterministic finite automaton MΣ accepting the language LΣ of
words which include all letters of the alphabet Σ. Then build a finite automaton ac-
cepting intersection of both languages L and LΣ; cf. Problem 7.4.
Problem 7.6. A language L is accepted by a finite automaton. Prove that the language
LR = {w : wR ∈L} is accepted by a finite automaton.

184
|
7 Finite automata
Solution. Notice that if w = a1 a2 . . . ak, then wR = ak . . . a2 a1. An idea is to reverse the
computation q0 a1 qi1 a2 qi2 . . . qik−1 ak qi for the word w in a deterministic automaton
M = (Q, Σ, δ, q0, F). Thus, an automaton accepting LR has reversed transitions, the
set of accepting states includes only q0, an extra state plays role of the initial state
with ε-transitions from it to states from F. The formal description of this automaton is
MR = (Q ∪{q′}, Σ, δR, q′, {q0}) for q′ ∉Q and
–
δR(q, a) = {p ∈Q : δ(p, a) = q} for q ∈Q, a ∈Σ;
–
δR(q′, ε) = F;
–
δR(q′, a) = ⌀for a ∈Σ
–
δR(q, ε) = ⌀for q ∈Q.
The reader may formally prove equivalence of both automata.
Problem 7.7. A language L is accepted by a finite automaton. Prove that the language
L′ = {a1 a2 a3 a4 . . . a2k−1 a2k : a2 a1 a4 a3 . . . a2k a2k−1 ∈L} is accepted by a finite au-
tomaton.
Problem 7.8. A language L is accepted by a finite automaton. Prove that the language
L′ = {w ∈L : |w| is even} is accepted by a finite automaton.
Problem 7.9. A language L is accepted by a finite automaton. Prove that the language
L′ = {a1 a1 a2 a2 a3 a3 . . . ak−1 ak−1 ak ak : a1 a2 a a3 . . . ak−1 ak ∈L} is accepted by a
finite automaton.
Problem 7.10. A language L is accepted by a finite automaton. Prove that the language
of words of L with even letters removed,
L′ = {a1 a3 . . . a2k−1 : a1 a2 a3 a4 . . . a2k−1 a2k ∈L ∨a1 a2 a3 a4 . . . a2k−1 ∈L}
is accepted by a finite automaton.

|
Part III: Revisited: languages, grammars, automata


8 Grammars versus automata
In Chapter 2, we introduced regular expressions and regular grammars, which are
tools used to generate regular languages. Then, in Chapter 7 we defined and discussed
finite automata.
In this chapter, we integrate notions of grammars and automata. Specifically, we
show that regular expressions, regular grammars and finite automata are equivalent.
They generate (expressions and grammars) and accept (automata) the same family of
languages, that is, regular languages. Also, we prove the pumping lemma for regular
languages. Let us recall that this lemma was formulated in Chapter 2 and then used
as a tool to process languages. Then we formulate and prove the most profound issue
concerning regular languages, that is, the Myhill–Nerode theorem. As in the case of
the pumping lemma, we formulated a part of this theorem in Chapter 2, called the
Myhill–Nerode lemma, and used it as a tool for processing languages. Then we discuss
the equivalence of context-free grammars and pushdown automata. Without formal
proofs, we also briefly discuss the equivalence of unrestricted grammars and Turing
machines and context-sensitive grammars and linear bounded automata.
8.1 Regular expressions, regular grammars and finite automata
8.1.1 Regular expressions versus finite automata
Below we prove that regular expressions are equivalent to finite automata. First, we
design automata equivalent to regular expressions. The proof is based on the induc-
tive definition of regular expressions (cf. Definition 2.1) and applies mathematical in-
duction. A designed finite automaton equivalent to a given regular expression is an
automaton with ε-transitions. Then a regular expression equivalent to a deterministic
finite automaton is designed. Furthermore, the equivalence of finite automata and reg-
ular expression is drawn based on the above two constructs and on the equivalence
of all three classes of finite automata as shown in Figure 8.1.
Figure 8.1: Equivalence of finite automata and regular expressions, a dependency diagram.
https://doi.org/10.1515/9783110752304-008

188
|
8 Grammars versus automata
Proposition 8.1. Languages generated by regular expressions are accepted by finite au-
tomata.
Proof. We use mathematical induction to prove this proposition. A formal proof of
equivalence is done concerning the length of a regular expression, that is, the number
of symbols in it.
1.
There are 3 families of regular expressions of length 1:
a.
the empty regular expression Θ generates the empty language 0. An equiva-
lent finite automaton is shown in Figure 8.2(a).
b.
the empty regular expression ε generates the language with the empty word
{ε}. An equivalent finite automaton is shown in Figure 8.2(b).
c.
a family of regular expressions a, for each symbol of an input alphabet a ∈Σ,
generate languages with a one letter word {a}. An equivalent finite automaton
for a given regular expression a is shown in Figure 8.2(c).
Note that automata shown in Figure 8.2 are nondeterministic ones.
Figure 8.2: Finite automata equivalent to basic regular expressions.
2.
Assume that two regular expressions s and t are given and that these expressions
generate languages S and T, respectively. Assume that both expressions are given
in the same alphabet Σ. Otherwise, take the union of alphabets of both expressions
as a joint alphabet.
Based on inductive assumption, take finite automata MS and MT shown in Fig-
ure 8.3. Assume that these automata are equivalent to regular expressions s and t,
respectively. Now, we consider sum (s + t), concatenation (s ∘t) and Kleene clo-
sure (s∗) of regular expressions. Languages generated by these expressions are
S ∪T, S ∘T and S∗. Finite automata (with ε-transitions), which accept union and
concatenation of languages S and T, and a finite automaton accepting Klenee clo-
sure of the language S are designed as shown in Figures 8.4, 8.5 and 8.6, respec-
tively.
Figure 8.3: Finite automata MS and MT equivalent to given regular expressions s and t.

8.1 Regular expressions, regular grammars and finite automata
|
189
Figure 8.4: A finite automaton equivalent to sum of regular expressions s and t.
Figure 8.5: A finite automaton equivalent to concatenation of regular expressions s and t.
Figure 8.6: A finite automaton equivalent to Kleene closure of a regular expression s.
Notice, that those automata have ε-transitions. Namely, an arrow from the set of
states FS to the state qA in Figure 8.4 represents ε-transitions from each state of
the set FS to the state qA; alike, an arrow from the set of states FT and the state qA
or to the state qT
0 in Figures 8.4, 8.5 and 8.6.
A formal proof of equivalence for the union of languages is given in Remark 8.1.
Proofs for concatenation and Kleene closure are left to the reader.
3.
Employing mathematical induction to (1) and (2), we conclude that there exists a
finite automaton equivalent to any regular expression.
Remark 8.1. Let MS = (QS, Σ, δS, qS
0, FS) and MT = (QT, Σ, δT, qT
0, FT). The finite automa-
ton shown in Figure 8.4 is M = (Q, Σ, δ, q0, F), where:
–
Q = QS ∪QT ∪{q0, qA}, assuming that sets of states are pairwise disjoint, that is,
QS ∩QT = 0, QS ∩{q0, qA} = 0 and QT ∩{q0, qA} = 0;
–
F = {qA};

190
|
8 Grammars versus automata
–
the transitions function δ is based on transition functions of both automata QS
and QT:
–
δ(q, a) = δS(q, a) for q ∈QS and a ∈Σ;
–
δ(q, a) = δT(q, a) for q ∈QT and a ∈Σ;
–
δ(q, ε) = δS(q, ε) for q ∈QS \ FS;
–
δ(q, ε) = δT(q, ε) for q ∈QT \ FT;
–
δ(q, ε) = δS(q, ε) ∪{qA} for q ∈FS;
–
δ(q, ε) = δT(q, ε) ∪{qA} for q ∈FT;
–
δ(q0, ε) = {qS
0, qT
0};
–
δ(q, X) = 0 for such (q, X) ∈Q × (Σ ∪{ε}), which are not considered above.
Proposition 8.2. Languages accepted by finite automata are generated by regular ex-
pressions.
Proof. Let us assume that a deterministic finite automaton
M = ({q1, q2, . . . , qn}, Σ, δ, q1, F)
accepts the language L = L(M). A method of designing a regular expression equiva-
lent to the automaton M relies on the construction of families of languages, which are
regular and which are easy to get regular expressions generating them.
The families Rk
i,j of languages are designed, where Rk
i,j, for given natural numbers
i, j ≥1 and k ≥0, denotes a set of such words w ∈Σ∗, that a computation for a word
w, starting in the state qi, ends in the state qj = δ(qi, w) and does not visit states with
indexes greater than k. Note that indexes i and j may be greater than k. Let us recall
that a computation of a deterministic finite automaton is a sequence of alternating
states and input symbols. In our case, the computations for the word w begins with
the state qi and ends with the state qi.
Formal description of the family Rk
i,j of languages is as follows:
R0
i,j = { {a ∈Σ : δ(qi, a) = qj}
for i
̸= j
{a ∈Σ : δ(qi, a) = qj} ∪{ε}
for i = j
Rk
i,j = Rk−1
i,k ∘((Rk−1
k,k )
∗) ∘Rk−1
k,j ∪Rk−1
i,j
for k > 0
Languages of the family R0
i,j include one-letter words, for which there is a tran-
sition from the state qi to the state qj. The empty word is included in languages R0
i,i
according to the rule that the empty word always allows for a transition to the same
state. Note that any word longer than one cannot be included in a language of this
family because the computation for such the word includes a middle state with the
index greater than 0.
Languages of a family Rk
i,j, for a given i, j, k > 0, are assembled according to the
following rules:

8.1 Regular expressions, regular grammars and finite automata
|
191
–
a computation for a word w, which does not visit states with indexes greater
than k, but visits the state qk, may be decomposed to computations which do not
visit qk, such that:
–
a computation that begins in qi and ends in qk, this computation forms the
language (Rk−1
i,k );
–
multiple computations that begin and end in qk, they form the language
(Rk−1
k,k )∗;
–
a computation that begins in qk and ends in qj, it forms the language Rk−1
k,j .
Concatenation of languages Rk−1
i,k , (Rk−1
k,k )∗and Rk−1
k,j form the first part of the union
in the second formula;
–
a computation for a word w, which does not visit states with index greater than k
(except the beginning state qi and the ending state qj, which may be equal or even
greater than k), may not visit the state qk. In that case, w ∈Rk−1
i,j , what creates the
second part of the union in the second formula.
The language L(M) is a set of such words, for which computation begins in q1, ends in
q ∈F and may visit any state of M, that is,
L(M) =
⋃
{j:qj∈F}
Rn
1,j
Now, existence of regular expressions generating languages of families Rk
i,j can be
proved employing mathematical induction with regard to k:
–
languages of the family R0
i,j are generated by the following regular expressions and
this is a direct conclusion from the definition of regular expressions:
–
for i
̸= j,
*
if R0
i,j = {ai1, . . . , aip} for ai1, . . . , aip ∈Σ, then r0
i,j = ai1 + ⋅⋅⋅+ aip;
*
if R0
i,j = 0, then r0
i,j = Θ;
–
for i = j,
*
if R0
i,j = {ε, ai1, . . . , aip}, ai1, . . . , aip ∈Σ, then r0
i,j = ε + ai1 + ⋅⋅⋅+ aip;
*
if R0
i,j = {ε}, then r0
i,j = ε;
–
based on inductive hypothesis assume that languages of a family Rk−1
i,j
are gener-
ated by regular expressions rk−1
i,j . Notice that the formula for Rk−1
i,j
is an assembly of
languages of a family Rk−1
i,j using union, concatenation and Kleene closure. The as-
sembly operators correspond to operations on regular expressions: sum, concate-
nation and Kleene closure. These notes lead to the following regular expression
generating the Rk
i,j language for a given indexes i, j, k > 0:
rk
i,j = rk−1
i,k ∘(rk−1
k,k )
∗∘rk−1
k,j + rk−1
i,j

192
|
8 Grammars versus automata
In conclusion, the following regular expression is equivalent to the automaton M, as-
suming that F = {qi1, qi2, . . . , qip}:
rn
1,i1 + rn
1,i2 + ⋅⋅⋅+ rn
1,ip
Example 8.1. Design a deterministic finite automaton accepting the language of
nonempty binary words with numbers of 0s and 1s not equal modulo 3. The lan-
guage is described as L = {w ∈{0, 1}+ : #0w −#1w
̸= 0 mod 3}. Then find a regular
expression equivalent to the given automaton.
Solution. A transition diagram of a deterministic automaton accepting the language
L is given in Figure 8.7. Notice that a state qi informs that the part u of an input word
already read satisfy the condition #0u −#0u = i + 1.
Figure 8.7: A deterministic finite automaton accepting the language given in Example 8.1.
Helper regular expressions necessary for finding a final one equivalent to the automa-
ton are drawn in Table 8.1. Regular expressions with the upper parameter equal to
3 are outlined outside the table due to their size. Note: regular expressions given in
Table 8.1 and below are optimized in length with qualities discussed in Section 2.1.1.
r3
1,2 = r2
1,3(r2
3,3)
∗r2
3,2 + r2
1,2
= (0(10)∗(0 + 11) + 1)(01 + (00 + 1)(10) ∗(0 + 11))
∗((00 + 1)(10)∗) + 0(10)∗
r3
1,2 = r2
1,3(r2
3,3)
∗r2
3,2 + r2
1,2
= (0(10)∗(0 + 11) + 1)(01 + (00 + 1)(10) ∗(0 + 11))
∗((00 + 1)(10)∗) + 0(10)∗
The regular expression equivalent to the finite automaton M is rather complex:
rM = r3
1,2 + (r3
1,3)
= (0(10)∗(0 + 11) + 1)(01 + (00 + 1)(10) ∗(0 + 11))
∗((00 + 1)(10)∗(ε + 1) + 0)
+ 0(10)∗(ε + 0 + 11) + 1

8.1 Regular expressions, regular grammars and finite automata
|
193
Table 8.1: A helper table for the elaboration of a regular expression equivalent to a finite automaton
built in Example 8.1.
k = 0
k = 1
k = 2
k = 3
rk
1,1
ε
ε
ε + 0(10)∗1
rk
1,2
0
0
0(10)∗
r2
1,3(r2
3,3)∗r2
3,2 + r2
1,2
rk
1,3
1
1
0(10)∗(0 + 11) + 1
r2
1,3(r2
3,3)∗r2
3,3 + r2
1,3
rk
2,1
1
1
(10)∗1
rk
2,2
ε
ε + 10
(10)∗
rk
2,3
0
0 + 11
(10)∗(0 + 11)
rk
3,1
0
0
(00 + 1)(10)∗1 + 0
rk
3,2
1
00 + 1
(00 + 1)(10)∗
rk
3,3
ε
ε + 01
ε + 01 + (00 + 1)(10)∗(0 + 11)
8.1.2 Regular grammars versus finite automata
Now we prove that regular grammars are equivalent to finite automata. First, we de-
sign automata equivalent to right-linear grammars. Then, for a given deterministic
finite automaton, an equivalent right-linear grammar is designed. Afterward, it is
shown that right-linear grammars are equivalent to left-linear grammars. As a result,
we come to a graph of equivalence of regular expressions, regular grammars and finite
automata. The graph is displayed in Figure 8.8.
Figure 8.8: Equivalence of finite automata, regular expressions and regular grammars: a depen-
dency diagram.
Theorem 8.1. Languages accepted by finite automata are generated by right linear
grammars.
Proof. We will prove that for any deterministic automaton M, there is a right linear
grammar GM generating the language L(M), that is, the language accepted by the au-

194
|
8 Grammars versus automata
tomaton M. Construction of such a grammar relies on transforming computation of
the automaton M into a derivation in designed grammar BM.
Let us assume that we have the following deterministic finite automaton:
M = (Q, Σ, δ, q0, F)
A right-linear grammar GM, which generates the language L = L(M) accepted by the
automaton M, is as follows:
GM = (V, T, P, S),
where:
–
V = Q ∪{S}, Q ∩{S} = 0
–
T = Σ;
–
P – the set of productions includes the following productions and only such pro-
ductions:
–
S →q0;
–
S →ε if and only if q0 ∈F;
–
p →aq if and only if δ(p, a) = q, for all p, q ∈Q and a ∈Σ;
–
p →a if and only if δ(p, a) ∈F, for all p ∈Q and a ∈Σ.
Now, let us prove that a word w is accepted by the automaton M (w ∈L(M)) if and only
if it is generated in the grammar GM (w ∈L(GM)). To prove this, notice that the com-
putation of the automaton M for given the word is identical to this word’s derivation
in the grammar GM.
First, we confirm that if a word w ∈L(M), then w ∈L(GM). If ε ∈L(M), that is,
q0 ∈F, then its derivation is immediate: S →ε. Now, let us assume that W
̸= ε, that
is, w = a1a2 . . . an ∈L(M) ⊂Σ∗and n > 0. Then
q0 a1 qi1 a2 qi2 . . . qin−1 an qin
is the computation of M for the word w. Since w ∈L(M), then qin ∈F. The correspond-
ing derivation in GM is as follows:
S →q0 →a1 qi1 →a1 a2 qi2 →⋅⋅⋅→a1 a2 . . . an−1 qin−1 →a1 a2 . . . an−1 an
Notice that the production qin−1 →an is included in this grammar because δ(qin−1, an) ∈
F and this production is used as the last one in the derivation.
Now, we verify that if w ∈L(GM), then w ∈L(M). Let us assume that we have a
word w = a1a2 . . . an ∈L(GM), then for w
̸= ε there exists a derivation in GM and it gets
the form:
S →q0 →a1 qi1 →a1 a2 qi2 →⋅⋅⋅→a1 a2 . . . an−1 qin−1 →a1 a2 . . . an−1 an

8.1 Regular expressions, regular grammars and finite automata
|
195
The corresponding computation of M for any w
̸= ε is as follows:
q0 a1 qi1 a2 qi2 . . . qin−1 an qin
where the production qin−1 →anqin used in computation corresponds to the production
qin−1 →an employed in the derivation, for some qin ∈F.
Subsequently, for w = ε ∈L(M) there is the production, which constitutes the
derivation S →ε. The corresponding computation is q0.
Example 8.2. Design a right-linear grammar equivalent to the finite automaton shown
in Figure 8.9.
Figure 8.9: The finite automaton given in Example 8.2.
Solution. The transition graph of the automaton is given in Figure 8.9.
The right-linear grammar designed according to Theorem 8.1 is as follows:
G = ({S, q0, q1, q2, q3}, {0, 1}, P, S)
with productions:
P :
{
{
{
{
{
{
{
{
{
{
{
{
{
S →q0
q0 →0q1 | 1q3 | 0 | 1
q1 →0q2 | 1q2
q2 →0q2 | 1q2
q3 →0q3 | 1q3 | 0 | 1
This right-linear grammar is a context-free grammar. So, it may be simplified by re-
moving useless symbols. Finally, we get the grammar:
G = ({S, q3}, {0, 1}, P′, S)
P′ : { S →1q3 | 0 | 1
q3 →0q3 | 1q3 | 0 | 1
Theorem 8.2. Languages generated by right-linear grammars are accepted by finite au-
tomata.

196
|
8 Grammars versus automata
Proof. Let
G = (V, T, P, S)
is a right-linear grammar. A finite automaton with ε-transitions accepts the language
L = L(G) generated by the grammar G:
M = (Q, Σ, δ, q0, F),
where:
–
Q = {[α] ∈(V ∪T)∗: (∃β ∈(V ∪T)∗) A →βα ∈P}, that is, states are labeled with
all possible suffixes of right-hand sides of productions;
–
Σ = T;
–
q0 = [S];
–
δ – a transition function is assembled with the rules:
–
if A ∈V then δ([A], ε) = {[α] : A →α ∈P};
–
if a ∈T ∧α ∈(T∗∪T∗V) then δ([aα], a) = {[α]}
–
F = {[ε]}.
Justification of correctness of the automaton construction is based on observation of
a derivation in a right-linear grammar.
Any intermediate derivation word is of a form xA, where x ∈T∗, A ∈V. A right-
hand side of a production A →yB employed in a derivation replaces a nonterminal
symbol A and creates a next intermediate derivation word xyB. An automaton reads
terminal symbols inserted by a production and then switches to a state relevant to
an inserted nonterminal symbol. This action is repeated for all productions of this
form employed in a derivation. A derivation is terminated with a production A →z,
where z ∈T∗(without a nonterminal symbol on its right-hand side). In this case, an
automaton reads terminal symbols and then goes to an accepting state marked with
the empty word ε.
Computation of such an automaton, for a given the word, follows a derivation
of this word. States visited along a computation correspond to unread part of an in-
put word. This unread part is represented by beginning terminal symbols and by one
nonterminal symbol if the derivation is not terminated with a production to a string
of terminals employed. This nonterminal symbol produces a remaining part of an in-
put word. The last production of a derivation does not include a nonterminal, which
allows an automaton to read terminal symbols and to go to the accepting state.
The above notes can be turned to formal inductive proof with regard to the length
of derivation.
Example 8.3. Design a finite automaton equivalent to the right-linear grammar
G = ({S, A}, {0, 1}, {S →00A, A →11A | 11S | 1 | ε}, S)

8.1 Regular expressions, regular grammars and finite automata
|
197
Solution. The automaton is designed according to the method used in the proof of
Theorem 8.2. A transition diagram of the automaton is shown in Figure 8.10.
Figure 8.10: A finite automaton equivalent the right-linear grammar given in Example 8.3.
Theorem 8.3. Right-linear grammars are equivalent to left-linear grammars.
Proof. In Problem 7.6, it is proved that a language LR is regular if L is regular, where LR
is the language of reversed words of L. Observe that a right-linear grammar GR is turned
to a left-linear one GL if right-hand sides of GR productions are reversed. Moreover, a
language L(GL) generated by GL is an inverse of L(GR), that is, (L(GL))R = L(GR), assum-
ing that GL is created by reversing productions of GL. A formal proof of this equality
relies on a simple application of mathematical induction with regard to the length of
the derivation of words, that is, with regard to the number of productions employed
in a derivation.
8.1.3 The pumping lemma
The pumping lemma was formulated in Chapter 2, but not proved there. Here, it is
recalled and proved.
Theorem 8.4 (The pumping lemma for regular languages).
If
a language L is regular
then
there exists a constant nL such that for any word z ∈L the following condition
holds:
(|z| ≥nL) ⇒[( ⋁
u,v,w
z = uvw ∧|uv| ≤nL ∧|v| ≥1)
⋀
i=0,1,2,...
zi = uviw ∈L]
Proof. If a language L is a regular one, then there exists a deterministic finite automa-
ton M = (Q, Σ, δ, q0, F), which accepts L. Let denote the number of states of this au-
tomaton |Q| = n. If all words of the language L are shorter than n, then for the constant
nL = n, the implication holds because its antecedent |z| ≥nL is never true.

198
|
8 Grammars versus automata
Let z ∈L, z = a1a2 . . . am, where m ≥n. A computation of M for z is
q0a1qi1a2qi2 . . . am−1qmm−1amqim
and qim ∈F.
There are m+1 ≥n+1 states in this computation. This means that at least one state
is repeated because there are only n states in M. Let us take the leftmost pair of repeat-
ing states. They, of course, appear in a beginning part of the computation, including
no more than n letters and no more than n + 1 states. In the following computation,
leftmost repeating states are underscored
q0a1qi1a2 . . . ajqijaj+1qij+1 . . . aj+pqij+paj+p+1qij+p+1 . . . am−1qim−1amqim
where p ≥1.
For that reason, the part of computation between these underlined states includes
at least one letter. If the first underlined state together with the part between under-
lined (repeated) states, that is, qijaj+1qij+1 . . . aj+p, is removed. The remaining sequence
of states and letters is still a computation, which ends in an accepting state:
q0a1qi1a2 . . . ajqij+paj+p+1qij+p+1 . . . am−1qim−1amqim
On the other hand, the sequence qijaj+1qij+1 . . . aj+p may be inserted just before the
first repeated state and an obtained sequence is still a computation, which ends in an
accepting state:
q0a1qi1 . . . ajqijaj+1 . . . aj+pqijaj+1 . . . aj+pqij+paj+p+1 . . . am−1qim−1amqim
Insertion of this sequence may be repeated, developing a computation, which ends in
an accepting state.
As a result, the computations for following words are created:
–
z0 = a1a2 . . . ajaj+p+1 . . . am−1am;
–
z1 = a1a2 . . . ajaj+1 . . . aj+paj+p+1 . . . am−1am;
–
z2 = a1a2 . . . ajaj+1 . . . aj+paj+1 . . . aj+paj+p+1 . . . am−1am;
–
z3 = a1a2 . . . ajaj+1 . . . aj+paj+1 . . . aj+paj+1 . . . aj+paj+p+1 . . . am−1am;
–
etc.
and they end in an accepting state. Notice that a part of a word, which is repeated, has
at least one letter. Moreover, both the beginning part and a repeated part are not longer
than nL = n. Therefore, we have a sequence of words as required in the consequence
of the implication. This proves the lemma.
As a consequence of the pumping lemma, it may be concluded that computations
of a finite automaton are determined by a finite set of words shorter than a constant nL.

8.1 Regular expressions, regular grammars and finite automata
|
199
A computation for a word, which is not shorter than nL, can be shortened by remov-
ing its middle part(s), as in the pumping lemma. This implies that a set of accepting
states of a deterministic finite automaton can be effectively calculated by investigat-
ing a finite set of such words, which are not longer than the constant nL. Computations
for longer words cannot bring a new accepting state. This conclusion can be formally
expressed as follows.
Remark 8.2. For any word z ∈L, |z| ≥nL, there exists a word w ∈L, |w| < nL such that
the computation for the word z is αz = α1α2α3 and the computation for the word w is
αw = α1α3. Note that α2 may include many different repeating parts.
8.1.4 The Myhill–Nerode theorem
In this section, the Myhill–Nerode theorem is formulated and proved. The Myhill–
Nerode lemma, which was used in Chapter 2, is a direct consequence of the Myhill–
Nerode theorem.
Theorem 8.5 (The Myhill–Nerode theorem). The following conditions are equivalent:
1.
a language L is accepted by a deterministic finite automaton M = (Q, Σ, δ, q0, F);
2.
a language L is a union of some classes of a right invariant equivalence relation with
finite index;
3.
the relation RL induced by a language L has finite index.
Proof. The following implications between the above conditions will be shown: 1 ⇒
2 ⇒3 ⇒1.
1 ⇒2
Assume that a deterministic finite automaton M = (Q, Σ, δ, q0, F) is given. Let us
define a relation ρM ⊂Σ∗× Σ∗such that for any x, y ∈Σ∗, xρMy ⇔δ(q0, x) = δ(q0, y).
The relation is:
–
a right invariant relation because
(∀x, y, z ∈Σ∗)δ(q0, x) = δ(q0, y) ⇒δ(q0, xz) = δ(q0, yz);
–
an equivalence relation since the equality relation is an equivalence relation. It is
obvious that ρM is
–
reflexive, that is, (∀x ∈Σ∗) δ(q0, x) = δ(q0, x);
–
symmetric, that is, (∀x, y ∈Σ∗) δ(q0, x) = δ(q0, y) ⇒δ(q0, y) = δ(q0, x);
–
transitive, that is,
(∀x, y, z ∈Σ∗) δ(q0, x) = δ(q0, y) ∧δ(q0, y) = δ(q0, z) ⇒δ(q0, x) = δ(q0, z).
It is evident that all words for which computation ends in the same state create an
equivalence class. In conclusion, we wind up that the number of equivalence classes
is not greater than the number of states |Q| of the automaton M. It is also evident that

200
|
8 Grammars versus automata
the language L is a union of those equivalent classes, which correspond to accepting
states.
2 ⇒3
Let us assume that a relation ρ as described in condition 2 is given. For any x, y ∈
Σ∗, if xρy, then either x, y ∈L, or x, y ∉L (because L is a union of some equivalence
classes of ρ). Moreover, for any z ∈Σ∗, if xρy, then xzρyz, that is, either xz, yz ∈L, or
xz, yz ∉L (because ρ is a right invariant relation). For that reason, (∀x, y ∈Σ∗)xρy ⇒
xRLy. In conclusion, every equivalence class of the relation ρ is included in some equiv-
alence class of the relation RL induced by the language L. We get that RL has no more
equivalence classes than ρM has, that is, the number of equivalence classes of RL is
finite.
3 ⇒1
Assume that the relation RL induced by the language L has a finite number of
equivalence classes. The following deterministic finite automaton accepts the lan-
guage L:
M = (Q, Σ, δ, q0, F),
where:
–
Q = {q[w] : w ∈Σ∗} – states correspond to equivalence classes of RL;
–
Σ – an alphabet of the language L;
–
q0 = q[ε] – the state corresponding to the equivalence class [ε], which includes
the empty word ε, is the initial state;
–
F = {q[w] : w ∈L} – accepting states correspond to equivalence classes, which are
included in the language L;
–
δ – a transition function is defined by the formula δ(q[w], a) = q[wa], for any q[w] ∈
Q and any a ∈Σ, where [w] is an equivalence class of the relation RL represented
by a word w ∈Σ∗.
The automaton M designed above accepts the language L because:
–
for any w ∈L, δ(q0, w) = δ(q[ε], w) = q[w] (simple inductive proof justifies this
evidence), that is, δ(q0, w) ∈F;
–
likewise, for any w ∉L, δ(q0, w) ∉F.
The proof has been completed.
8.1.5 Minimization of deterministic finite automata
The Myhill–Nerode theorem helps to minimize deterministic finite automata. First of
all, note that the relation RL induced by a regular language L is the most general equiv-

8.2 More grammars and automata
|
201
alence relation defining a language L. Namely, an equivalence relation defines a lan-
guage if and only if a language is a union of some equivalence classes of this relation.
Recall that an equivalence relation E1 is more general than an equivalence relation E2
if and only if equivalence classes of E2 are included into equivalence classes of E1.
Second, a deterministic automaton designed in the proof of the third implication
is a minimal one concerning the number of states. If this were not true, then we would
be able to build a deterministic automaton M′, which has fewer states than the au-
tomaton M designed in the proof. However, the relation ρM considered in the proof
of the Myhill–Nerode theorem would have fewer equivalence classes than the rela-
tion RL. But this is not possible due to the second implication considered in the proof
of the Myhill–Nerode theorem.
Finally, an automaton designed in the proof of the Myhill–Nerode theorem is a
minimal one concerning the number of states.
8.2 More grammars and automata
8.2.1 Context-free grammars versus pushdown automata
In this section, relations between context-free grammars and pushdown automata
are discussed. It is shown that pushdown automata are equivalent to context-free
grammars. Thus, a class of languages accepted by pushdown automata is the class of
context-free languages.
Theorem 8.6. Languages generated by context-free grammars are accepted by push-
down automata.
Proof. Let a context-free grammar is given and the empty word is not generated in the
grammar. We design a pushdown automaton accepting the language generated by this
grammar. The automaton accepts with the empty stack.
We assume that a given context-free grammar
G = (V, T, P, S)
is in Greibach normal form. Let us recall that productions of a grammar in Greibach
normal form are A →aα, where A ∈V, a ∈T and α ∈V∗. For a given the word
w ∈L(G) a leftmost derivation in G is considered. A pushdown automaton, when com-
putes a given the word w, follows this leftmost derivation in G. A pushdown automaton
equivalent to the grammar G is
M = ({q0, q}, T, V ∪{▷}, δ, q0, ▷, ◁, 0)
where the transition function is designed as follows:

202
|
8 Grammars versus automata
–
begin with a given the word w ∈T∗on the input of M and with the initial symbol
S of the grammar G on the stack;
–
accept if the end-of-input symbol ◁is on the input and the initial stack symbol ▷
is on the stack;
–
if a ∈T is an input symbol, A ∈V is a top symbol of the stack and there is a
production A →aα in the grammar G, α ∈V∗, then read the input symbol and
replace the top symbol of the stack with α (remove A from the stack, push on the
stack symbols of α in reverse order);
–
reject in all other cases.
These rules could be rewritten as follows:
–
δ(q0, ε, ▷) = {(q, S ▷)};
–
δ(q, a, A) = {(q, α) : A →aα ∈P};
–
δ(q, ◁, ▷) = {(q, ◁)}.
Modification of the design in the case when ε is included in the language is fairly easy.
The automaton should be able to pop the top symbol of the stack up in its first transi-
tion, that is, the first rule of the presented above should be replaced with:
–
δ(q0, ε, ▷) = {(q, S ▷), (q, ▷)}.
A formal proof is based on mathematical induction concerning the length of deriva-
tion.
Notice that an automaton designed in Theorem 8.6 is, in general, a nondetermin-
istic one. Nondeterminism is raised by the ambiguity of grammar. If a grammar in
Greibach normal form is simple, that is, satisfies the Greibach uniqueness condition,
then an automaton is a deterministic one.
Example 8.4. Design a pushdown automaton equivalent to the grammar
G = ({S, A}, {a, b}, {S →aAA, A →a | aS | bS}, S)
Provide a computation of the designed automata for the word w = abaaaa.
Solution. The grammar G is in Greibach normal form. An equivalent pushdown au-
tomaton is
M = ({q0, q}, {a, b}, {S, A, ▷}, δ, q0, ▷, ◁, 0)
with the transition function given in Table 8.2.
The automaton accepts with the empty stack. Recalling: if a transition function is
undefined, which is denoted as the empty entry in the transition table, then a push-
down automaton rejects its input.

8.2 More grammars and automata
|
203
Table 8.2: Transition tables of the pushdown automaton equivalent to the grammar given in Exam-
ple 8.4.
a
b
ε
◁
δ(q0)
S
A
▷
(q, S ▷)
δ(q)
S
(q, AA)
A
(q, S)
(q, S)
(q, ε)
▷
ACC
A computation for the given word w is presented in Figure 8.11. Note that the word is
accepted because there is a path from the root to an accepting leaf (accept with empty
stack and empty input).
Figure 8.11: Transition tables of the pushdown automaton equivalent to the grammar given in Exam-
ple 8.4.
Example 8.5. Develop a method of designing pushdown automata equivalent to LL(1)
grammars. Build an automaton equivalent to the grammar given in Example 3.16 of
Chapter 3.

204
|
8 Grammars versus automata
Solution. Assume that a LL(1) grammar is given:
G = (V, T, P, S)
LL(1) grammars satisfy a uniqueness condition, which is similar to the Greibach
uniqueness condition. Therefore, it would be possible to design a deterministic au-
tomaton equivalent to a given LL(1) grammar. However, productions of a LL(1) gram-
mar may have a right-hand side not beginning with a terminal symbol or may be
nullable, which makes an automaton to be a nondeterministic one. In practice, if we
can check an input symbol without doing an immediate transition, an automaton
may be turned into a deterministic one. Below, a general, nondeterministic method
for designing an automaton is given. An automaton is defined as
M = ({q0, q}, T, V ∪T ∪{▷}, δ, q0, ▷, ◁, 0)
where the transition function is built according to the following rules (those are
slightly modified formulated in the proof of Theorem 8.6):
1.
begin with a given word w ∈T∗at the input of M and with the initial symbol S of
the grammar G on the stack;
2.
accept if the end-of-input symbol ◁is at the input and the initial stack symbol ▷
is on the stack;
3.
if the same symbol a ∈T of the input alphabet is at the input and at the stack,
read the input symbol and remove the top symbol of the stack;
4.
if A ∈V is a top symbol of the stack and there is a A-production in the grammar,
then choose nondeterministically an A-production A →α, α ∈(V ∪T)∗and re-
place the top symbol of the stack with α (remove A from the stack, push on the
stack symbols of α in reverse order);
5.
reject in all other cases.
These rules could be rewritten as follows:
–
δ(q0, ε, ▷) = {(q, S ▷)};
–
δ(q, a, a) = {(q, ε)} for a ∈T;
–
δ(q, ε, A) = {(q, aα) : A →aα ∈P} for A ∈V ∪{S};
–
δ(q, ◁, ▷) = {(q, ▷)}.
A formal proof is based on mathematical induction completed concerning the length
of derivation.
An automaton equivalent to the grammar given in Example 3.16 of Chapter 3 is as
follows:
M = ({q0, q}, Σ, Γ, δ, q0, ▷, ◁, 0)
where:

8.2 More grammars and automata
|
205
–
Σ = {id, +, −, ∗, /, ↑, (, )};
–
Γ = {E, El, T, Tl, P, Pl, Q, ▷, id, +, −, ∗, /, ↑, (, )};
–
δ – the transition function is given in Table 8.3. In this table, empty entries de-
note rejection of an input. For the sake of clarity and space saving, the values
+TEl, −TEl, ε should be read as {(q, +TEl), (q, −TEl), (q, ε)} (brackets and the state
q are dropped), that is, only symbols replacing a top symbol of the stack are
shown. The symbol τ stands for any terminal symbol ▷, id, +, −, ∗, /, ↑, (, ), that is,
δ(q, τ, τ) = (q, ε) for τ ∈{▷, id, +, −, ∗, /, ↑, (, )}.
Table 8.3: A pushdown automaton equivalent to the LL(1) grammar given in Example 3.16 of Chap-
ter 3.
δ
τ
ε
E
TEl
El
+TEl, −TEl, ε
T
PTl
Tl
∗PTl, /PTl, ε
P
QPl
Pl
↑QPl, ε
Q
−Q, (E), id
τ
ε
This automaton can be turned into a deterministic one, assuming that an input sym-
bol can be reused. In other words, if an input can be checked before a ε-transition
is (nondeterministically) done, then a suitable transition could be (deterministically)
chosen based on the uniqueness condition of LL(1) grammars and SELECT sets of rel-
evant productions.
Theorem 8.7. Languages accepted by pushdown automata are generated by context-
free grammars.
Due to its complexity, proof of this theorem does not fit this book. On the other
hand, this theorem has no impact on other parts of this book. Therefore, the formal
proof is skipped. The reader can consult the proof in, for instance, [1].
8.2.2 Unrestricted grammars versus Turing machines
Theorem 8.8. Languages generated by unrestricted grammars are accepted by Turing
machines.
Proof. We will describe a Turing machine, which accepts a language generated by a
given unrestricted grammar. A detailed description of such a machine does not fit this

206
|
8 Grammars versus automata
book. Moreover, such a huge and detailed description would be far away from an easy
understanding of how such a machine works.
Let us assume that a context-sensitive grammar G = (V, T, P, S) is given. We design
a Turing machine, which will nondeterministically follow a derivation of a given the
word. This machine has two tapes. An input word w = a1a2 . . . an is kept on the first
tape. An intermediate word of a derivation is stored on the second tape. A computation
is terminated when the contents of the second tape is equal to the input word w. Note
that we do not put any restriction on a derivation; it may neither be a leftmost nor a
rightmost one. The machine realizes the following algorithm:
1.
initialize the second tape with the initial symbol S of the grammar G, that is, put
this symbol in a cell of the tape;
2.
compare contents of both tapes. If they are equal, switch to an accepting state and
terminate computation;
3.
nondeterministically choose a symbol in a word on the second tape, e. g., such a
choice is done by the Turing machine designed in Example 5.6;
4.
nondeterministically choose a production α →β of the grammar G;
5.
compare the left-hand side α of this production with the part of the word on the
second tape, which begins with the symbol chosen in point 3. If they match, re-
place the part of the word with the right-hand side of the production (perhaps,
a remaining part of the word, which follows the part α to be replaced, needs a
shifting left or right, depending on whether |α| < |β| or |α| > |β|);
6.
go back to the step 2.
A Turing machine realizing the above algorithm accepts an input word if this word
is generated in the grammar G. This is guaranteed by the fundamental assumption
about nondeterminism. It is assumed that a nondeterministic choice always leads to
acceptance of input if such input can be accepted; cf. Section 5.2. If an input word is
not generated in the grammar G, then such a Turing machine will fall into an infinite
computation.
On the other hand, if there is a sequence of transitions leading to acceptance by
such a Turing machine, then this sequence of transitions defines a derivation in the
grammar G.
It can be proved by induction that the content of the second tape is an interme-
diate word of some derivation in the grammar G. Then, based on the fundamental as-
sumption of nondeterminism, it can be concluded that a derivation of an input word
is generated in the grammar. Thus, L(G) = L(M).
Theorem 8.9. Languages accepted by Turing machines are generated by unrestricted
grammars.
As in case of Theorem 8.7 the formal proof is skipped. The reader can consult the
proof in, for instance, [1].

8.2 More grammars and automata
|
207
8.2.3 Context-sensitive grammars versus linear bounded automata
Theorem 8.10. Languages generated by context-sensitive grammars are accepted by
linear bounded automata.
Proof. The proof is similar to the one in Theorem 8.8. Instead of using a two-tape Tur-
ing machine with tapes infinite two-way, we use a linear bounded automaton with
two tapes, both of length equal to the length of an input word. This restriction may
cause that it is impossible to perform an operation of the pint 5 of the proof in Theo-
rem 8.8. The reason for such a failure is that there may be no room for replacing a part
of the word with the right-hand side of a production. In such a case, an automaton
rejects its input. It is worth underlining that an automaton used here is nondetermin-
istic. According to the fundamental assumption of nondeterminism, we may interpret
a computation of an automaton as a sequence of configurations correctly chosen. To
summarize, a such designed linear bounded automaton accepts the language gener-
ated by a given unrestricted grammar.
Theorem 8.11. Languages accepted by linear bounded automata are generated by
context-sensitive grammars.
As in case of Theorem 8.7, the formal proof is skipped. The reader can consult the
proof in, for instance, [1].

9 Around the hierarchy of languages
In this section, we define more operations on languages: substitutions, homomor-
phism and quotients. Then we prove closure properties of classes of languages with
regard to operations on languages. Finally, we define the Chomsky hierarchy of fami-
lies of languages and the hierarchy, extension and prove its properties.
9.1 More operations on languages
9.1.1 Substitutions, homomorphisms
Definition 9.1. Let Σ and Δ are alphabets. A mapping f of letters of an alphabet Σ into
languages over an alphabet Δ:
f : Σ →2Δ∗
is called substitution. A substitution on an alphabet Σ can be extended:
–
to words over an alphabet Σ:
f : Σ∗→2Δ∗
f (ε) = ε
f (wa) = f(w) ∘f (a)
(∀w ∈Σ∗)(∀a ∈Σ)
–
and to languages over an alphabet Σ:
f : 2Σ∗
→2Δ∗
f (L) = ⋃
w∈L
f (w)
(∀L ⊂Σ∗)
This general definition of substitution may be restricted to a class of languages,
assuming that values of substitution are languages of this class as well as arguments
of a substitution are languages of this class as well. In this section, discussion on sub-
stitutions is restricted to regular languages. Explicitly, we assume that substitutions
are mappings: f : Σ →RgL(Δ), f : Σ∗→RgL(Δ) and f : RgL(Σ) →RgL(Δ), where
RgL(Σ) and RgL(Δ) are the classes of regular languages over alphabets Σ and Δ, re-
spectively.
The definition of substitution could be reformulated to regular expressions.
Definition 9.2. Let Σ and Δ are alphabets. A mapping F of letters of an alphabet Σ into
regular expressions REx(Δ) over an alphabet Δ:
f : {a : a ∈Σ} →REx(Δ)
https://doi.org/10.1515/9783110752304-009

9.1 More operations on languages
|
209
A substitution on an alphabet Σ can be extended to regular expressions REx(Σ) over
an alphabet Σ:
f : REx(Σ) →REx(Δ)
such that
f(0) = 0
f(ε) = ε
f (a) = the value defined by the mapping f (∀a ∈Σ)
and
f (s + t) = (f (s) + f (t))
f (s ∘t) = (f (s) ∘f (t))
f(s∗) = ((f(s))
∗)
where 0, ε and a for a ∈Σ are basic regular expressions, s and t are regular expressions
(used in inductive step of the definition).
Remark 9.1. In the class of regular languages, substitutions can be considered alter-
natively with regard to languages or concerning regular expressions generating these
languages.
Example 9.1. Let Σ = {a, b} and Δ = {0, 1} are alphabets and f is a substitution:
f(a) = {00, 0000, 000000, . . .}
f (b) = {1, 11, 111, 1111, 11111, . . .}
Redefine this substitution to regular expressions and compute its value for given reg-
ular expressions: bab and a∗(a + b)b∗.
Solution. The language {00, 0000, 000000, . . .} is generated by the regular expression
00(00)∗, the language {1, 11, 111, 1111, 11111, . . .} is generated by the regular expression
11∗. Hence, the substitution reformulated to regular expressions is as follows (notice
that, in light of Remark 2.2 of Chapter 2, unnecessary brackets are removed):
f(a) = 00(00)∗
f (b) = 11∗
Now, let us compute values of the substitution for given regular expressions:
–
f (bab) = f(b)f(a)f (b) = (11∗)(00(00)∗)(11∗) = 11∗00(00)∗11∗
–
f (a ∗(a + b)b∗)
=
f(a∗)f(a + b)f(b∗)
=
(f(a))∗(f(a) + f(b))(f(b))∗
=
(00(00)∗)∗(00(00)∗+11∗)(11∗)∗= (00)∗(00(00)∗+11∗)1∗= (00)∗001∗+(00)∗11∗=
(00)∗(00 + 1)1∗.

210
|
9 Around the hierarchy of languages
Definition 9.3. Let Σ and Δ are alphabets. A substitution h : Σ →2Δ∗, such that
|h(a)| = 1 for all a ∈Σ, is called a homomorphism. In other words, a homomorphism
is such a substitution, which yields one-word languages. An extension of a homomor-
phism to words and languages is a relevant extension of a substitution.
Remark 9.2. A homomorphism h is identified with a mapping h : Σ →Δ∗, which
yields a word over the alphabet Δ for a letter of the alphabet Σ rather than a language
including only this word.
Definition 9.4. Let h : Σ →2Δ∗is a homomorphism. An inverse homomorphic image
of a word w ∈Δ∗is a set of words (language):
h−1(w) = {x ∈Σ : h(x) = w}
An inverse homomorphic image of a language L ⊂Δ∗is a set of words:
h−1(L) = ⋃
w∈L
h−1(w) = {x ∈Σ∗: h(x) ∈L}
Example 9.2. Let Σ = {a, b} and Δ = {0, 1} are alphabets and h is a homomorphism:
h(a) = 010
h(b) = 101
Find an inverse homomorphic image of a language L generated by the regular expres-
sion (ε + 0)(10)∗(ε + 1).
Solution. Notice that the language L includes words with alternating 0s and 1s, that
is, none a word include two successive 0s or 1s:
L = {ε, 0, 1, 01, 10, 010, 101, 0101, 1010, 01010, 10101, . . .}
The language L includes the empty word ε of length 0 and two words of any length
greater than 0. Of course, only words of length divisible by 3 can be yielded by the ho-
momorphism h. This means that the inverse homomorphic image h−1 yields the empty
set for any word of length not divisible by 3. Having this in mind, we can show that
the inverse homomorphic image works for successive words of L of length divisible
by 3:
–
h−1(ε) = {ε};
–
h−1(010) = {a}, h−1(101) = {b};
–
h−1(010101) = {ab}, h−1(101010) = {ba};
–
h−1(010101010) = {aba}, h−1(101010101) = {bab};
–
h−1(010101010101) = {abab}, h−1(101010101010) = {baba};
–
. . . .

9.1 More operations on languages
|
211
As it is seen, the inverse homomorphic image h−1(L) includes words with alternat-
ing a’s and b’s, that is, the same language as L with 0s replaced by a’s and 1s re-
placed by b’s. The language h−1(L) is, of course, generated by the same regular expres-
sion, which generates L with the same replacements: (ε + a)(ba)∗(ε + b). Despite that
h−1(L) ≃L, it is not true that h(h−1(L)) ≃L. The homomorphic image of the language
generated by the regular expression (ε + a)(ba)∗(ε + b), that is, having all words with
alternating a’s and b’s, includes words with alternating 0s and 1s of length divisible
by 3.
9.1.2 Quotients
Quotients of languages are actually applied to words. Quotient of words is essentially
the opposite of concatenation. A quotient of words is a kind of reduction of one word,
a dividend, by another one, a divisor. Two types of quotients are defined: the right
quotient and the left quotient.
Definition 9.5. Let L1 and L2 are languages over an alphabet Σ.
The right quotient of a language L1 by a language L2 is the language L1/L2 consist-
ing of such words over the alphabet Σ, which concatenated with words of the divisor
give words of the dividend:
L1/L2 = {x ∈Σ∗: (∃y ∈L2)xy ∈L1}
The left quotient of a language L1 by a language L2 is the language L1\L2 consisting
of such words over the alphabet Σ words, which concatenated to words of the divisor
give words of the dividend:
L1\L2 = {y ∈Σ∗: (∃x ∈L2)xy ∈L1}
Example 9.3. Let L1 = {anbncn : n ≥0} and L2 = {akblcm : k, l, m ≥0}. Find quotients:
L1/L2, L1\L2, L2/L1 and L2\L1.
Solution. Let us consider every case:
1.
L1/L2 – analyzing definition of the right quotient we get
L1/L2 = {w ∈{a, b, c}∗: w ∘akblcm = anbncn, k, l, m, n ≥0}
The following cases are possible with regard to w:
–
w = anbncn and akblcm = ε, that is, k = l = m = 0 for n > 0 (notice that the
case n = 0 is considered in the forth item of this list) or
–
w = anbncp and akblcm = cn−p for n > p ≥0 or
–
w = anbp and akblcm = bn−pcn for n > p ≥0 or
–
w = ap and akblcm = an−pbncn for n > p ≥0.

212
|
9 Around the hierarchy of languages
Finally:
L1/L2 = L1 ∪{anbncp : n > p ≥0} ∪{anbp : n > p ≥0} ∪{ap : p ≥0};
2.
L1\L2 – solution is similar to the solution of the above case:
L1\L2 = {w ∈{a, b, c}∗: akblcm ∘w = anbncn, k, l, m, n ≥0},
L1\L2 = L1 ∪{apbncn : n > p ≥0} ∪{bpcn : n > p ≥0} ∪{cp : p ≥0};
3.
L2/L1 and L2\L1 give the same result. Let us consider L2/L1:
L2/L1 = {w ∈{a, b, c}∗: w ∘anbncn = akblcm, k, l, m, n ≥0}
Notice that an empty word ε is in the divisor. So, any word w = akblcm is in the
quotient. On the other hand, only a string of a’s concatenated with a nonempty
divisor’s word gives a word of the dividend. Analysis of L2\L1 is similar.
Finally, L2/L1 = L2\L1 = L1.
Remark 9.3. The definition of quotients of languages could be reformulated to regular
expressions instead of languages. Such a formulation corresponds, of course, to quo-
tients of languages generated by regular expressions, that is, to quotients of regular
languages.
Example 9.4. Languages R1, R2 and R3 are generated by regular expressions r1 =
0∗10∗, r2 = 10∗1 and r3 = 0∗1. Find the following quotients R1/R2, R1\R2, R1/R3, R1\R3,
R2/R3, R2\R3.
Solution. Results are given in a form of regular expressions, thus a symbol ≃is used
instead of equality – R1/R2 ≃0, R1\R2 ≃0, R1/R3 ≃0∗, R1\R3 ≃0∗, R2/R3 ≃10∗,
R2\R3 ≃0∗1.
9.1.3 Building automata with quotients
A language L = L(M) accepted by a deterministic finite automaton M = (Q, Σ, δ, q0, F)
is a set of words L = {w ∈Σ∗: δ(q0, w) ∈F}. Let consider languages:
–
La = {w ∈Σ∗: δ(qa, w) ∈F}, where δ(q0, a) = qa for a ∈Σ. These languages
are accepted by deterministic automata M = (Q, Σ, δ, qa, F). Note that La is derived
from L by removing the first letter from words of L, that is, La = {u ∈Σ∗: (∃w ∈
L) au = w}. The last formula defines the left quotient of the language L with the
divisor language {a} (the divisor language includes one word of unit length), that
is, La = L\{a};
–
Lab = {w ∈Σ∗: δ(qab, w) ∈F}, where qab = δ(q0, ab) for a, b ∈Σ. Lab is derived
from L by removing two leading first letters from words of L or – in other words –

9.1 More operations on languages
|
213
Lb is derived from La by removing the first letter from words of it: Lab = L\{ab} =
La\{b};
–
Labc = {w ∈Σ∗: δ(qabc, w) ∈F}, δ(q0, a) = qab for a, b, c ∈Σ∗;
–
. . . .
How many languages do we get in the above process of quotients? We get as many
words in the language L, at a glance. Nevertheless, since languages are tied to states
of a finite automaton, we get no more languages than the number of states. On the
other hand, languages are computed as quotients by words over the alphabet of this
language. They may be tied to any deterministic finite automaton, including an au-
tomaton with a minimal number of states. Hence, the number of languages does not
depend on a particular automaton. From the Myhill–Nerode theorem, we conclude
that these languages are tied with equivalence classes of the relation RL induced by
a regular language L rather than a particular deterministic finite automaton accept-
ing L. However, these languages are not equivalence classes of RL. Moreover, they are
not equivalence classes of any equivalence relation.
Now, consider which of the above languages correspond to equivalence classes
of RL included in the language L. Take a particular language Lu. Note that there are
many u ∈Σ∗defining the same language. In fact, a set of words defining a particular
language can be written as Ew = {u ∈Σ∗: Lu = Lw}. If an automaton M is a minimal
one, then Ew = {u ∈Σ∗: δ(q0, u) = δ(q0, w)} is an equivalence class of RL; cf. the
Myhill–Nerode theorem. If v is a shortest word in Ew, then the state δ(q0, v) = δ(q0, w)
is accepting one if and only if v ∈L, what is equivalent to ε ∈Lw.
As a conclusion of this discussion, we get the following proposition.
Proposition 9.1. A regular language L over an alphabet Σ∗is accepted by a determin-
istic finite automaton
M = (Q, Σ, δ, q0, F),
where:
–
Q = {qLw : Lw = L\{w}∧w ∈Σ∗}, that is, states are labeled by quotients of languages;
–
q0 = qL;
–
F = {qLw : ε ∈Lw};
–
δ(qLw, a) = qLwa for a ∈Σ, w ∈Σ∗and, in this convention, Lε = L.
Moreover, this automaton is a minimal one (with regard to a number of states) accept-
ing L assuming that equality of any two languages obtained from different quotients is
identified.
Example 9.5. Let a language L be generated by the regular expression r = (aa)∗(aa +
bb)(bb)∗. Employ Proposition 9.1 to design a deterministic finite automaton accept-
ing L.

214
|
9 Around the hierarchy of languages
Solution. Calculating quotients let us use regular expressions instead of languages,
one has:
–
r = (aa)∗(aa + bb)(bb)∗;
–
ra = r\a = a(aa)∗(aa + bb)(bb)∗+ a(bb)∗;
–
rb = r\b = b(bb)∗;
–
raa = ra\a = (aa)∗(aa + bb)(bb)∗+ (bb)∗;
–
rab = ra\b = 0;
–
rba = rb\a = 0;
–
rbb = rb\b = (bb)∗;
–
raaa = raa\a = r\a = ra;
–
raab = raa\b = b(bb)∗+ b(bb)∗= rb;
–
raba = rabb = rbaa = rbab = 0;
–
rbba = rbb\a = 0;
–
rbbb = rbb\b = b(bb)∗= rb.
No new language can be yielded when quoting is continued.
The automaton is M = ({r, ra, rb, raa, rbb, 0}, {a, b}, δ, r, {raa, rbb}), where the transi-
tion function is given as the transition diagram in Figure 9.1.
Figure 9.1: The transition diagram of the automaton designed in Example 9.5.
9.2 Closure
In this section, we investigate the closure of classes of languages with regard to opera-
tions on languages. Operations are defined in Subsection 1.5.2 of Section 1.1 and above
in this chapter. We consider classes of languages examined in the previous sections:
regular, context-free, context-sensitive, recursive, recursively enumerable classes of
languages, that is, RgL, CFL, CSL, RkL, REL classes, and the class of all languages, the
ALL class.

9.2 Closure
|
215
Let us examine this problem from two points of view. The first attempt employs
grammars, which generate languages of relevant classes. This attempt allows showing
that union is an internal operation in all classes of languages generated by grammars,
that is, in RgL, CFL, CSL, REL classes of languages. The second attempt is based on au-
tomata. It allows for proving union to be an internal operation in RgL, CFL, CSL, RkL
and REL classes. We give proofs for both attempts, despite that in this way proofs are
duplicated for some classes. We think that these proofs play utilitarian roles for dis-
cussion on closure as well as they provide useful techniques, which could be applied
in solving other problems.
Remark 9.4. Taking a subset of a given set, in this case a subset of a language, is the
very basic operation. However, it is not an internal operation in any class of languages,
except the ALL class. It means that a subset of a language of any class may not belong
to this class, besides the ALL class.
Remark 9.5. The ALL class is closed with regard to any operation. Therefore, the fur-
ther discussion does not take this class into account.
Proposition 9.2. Union is an internal operation in all classes of languages.
Proof. Let us assume that given two languages L1 and L2 belong to a given class, that
is, and they are either regular or context-free or context-sensitive or recursively enu-
merable. Hence, we can assume that there are grammars G1 = (V1, T1, P1, S1) and G2 =
(V2, T2, P2, S2) such that L1 = L(G1) and L2 = L(G2), that is, these grammars generate
given languages. Of course, both grammars are in a class to which these languages be-
long. We can assume that V1 ∩V2 = 0. Otherwise, the names of nonterminal symbols
of one of these grammars should be changed.
The following grammar generates union of both languages L = L1 ∪L2, that is,
L = L(G):
G = (V1 ∪V2 ∪{S}, T1 ∪T2, P1 ∪P2 ∪{S →S1 | S2}, S)
Note that a form of productions of a given grammar decides to which class this
grammar belongs. The grammar G belongs to the same class, to which grammars G1
and G2 do. This is because the new productions S →S1 | S2 hold assumptions on
productions of any grammar: regular, context-free, context-sensitive, or unrestricted
one. Because G is in the same class, in which G1 and G2 are, then L belongs to the same
class, to which L1 and L2 do.
Also note that L = L1∪L2 ⊂L(G), that is, any word w of L = L1∪L2 is generated in G,
which means that w ∈L(G). If w ∈L = L1 ∪L2, then w ∈L1 or w ∈L2. If w ∈L1, then
its derivation in G is of a form S →S1 →∗w, where S1 →∗w, where is a derivation
of w in G1. A similar derivation can be built for a word w ∈G2. On the other hand
L ⊂L = L1 ∪L2. If a derivation in G is given, then it either begins with the production

216
|
9 Around the hierarchy of languages
S →S1 →⋅⋅⋅or with the production S →S2 →⋅⋅⋅. A remaining part of a derivation is
either a derivation in G1 or in G2.
Hence, we have proved that the grammar G and the language L = L(G) belong
to the same class to which grammars G1 and G2 and languages L1 and L2 do. Then
we showed that the grammar G generates words of L = L1 ∪L2 and oppositely. This
completes the proof based on grammars that union is an internal operation in classes
given above.
Consequently, we have another prove that union is an internal operation in all
classes of languages. This proof is based on automata. Since we do not have a class
of grammars generating recursive languages, this is the only proof that union is an
internal operation in the RkL class.
For the context-sensitive, recursive or recursively enumerable languages L1 and
L2 we can assume that we have linear bounded automata, Turing machines with the
stop property or Turing machines M1 and M2, which accept these languages, that is,
L = L(M1) and L = L(M2), respectively. In Chapter 5, we have built a Turing ma-
chine, which accepts union of L1 and L2; namely, in Problem 5.6 we designed a Turing
machine accepting L(M1) ∪L(M2), for given Turing machines M1 and M2. Similarly,
Turing machines with the stop property and linear bounded automata are deemed
in Problem 5.10 and Problem 5.11. Likewise, a push-down automaton is designed in
Problem 6.6 in Chapter 6. A construction of finite automaton is given in Problem 7.4 in
Chapter 7.
Proposition 9.3. Concatenation is an internal operation in all classes of languages.
Proof. This proof is quite similar to the proof of Proposition 9.2. The difference is in a
construction of the set of productions of a grammar generating concatenation of lan-
guages. The productions S →S1 | S2 should be replaced with the production S →S1S2.
This task is also discussed in Problem 5.8, Problem 5.10 and Problem 5.11 in Chapter 5
and in Problem 7.4 in Chapter 7.
Proposition 9.4. Kleene closure is an internal operation in all classes of languages.
Proof. Again, this proof is quite similar to the proof of Proposition 9.2. Suppose
that a grammar G1
= (V1, T1, P1, S1) generates a language L1. Then the grammar
G = (V1, T1, P1 ∪{S1 →ε | S1S1}) generates L = (L1)∗. This task is also solved in Prob-
lem 5.9, Problem 5.10 and Problem 5.11 in Chapter 5 and in Problem 7.4 in Chapter 7.
Proposition 9.5. Substitution is an internal operation in classes of languages RgL, CFL,
CSL, REL and ALL.
Proof. We prove closure of the class of context-free languages with regard to substitu-
tion based on the class of context-free grammars. Notice that each class RgL, CFL, CSL
and REL of languages is generated by a corresponding class of grammars. Therefore,
the proof can be repeated for other classes of languages with the replacement of the
class of grammars.

9.2 Closure
|
217
Let us assume that Σ and Δ are alphabets and f : Σ →2Δ∗a substitution such that
f(a) is a context-free language La ⊂Δ∗for each a ∈Σ and that L ⊂Σ∗is a context-free
language. Let us assume that the context-free grammars G = (Va, Δ, Pa, Sa) generate
languages La for each a ∈Σ and that G = (V, Σ, P, S) is a context-free grammar gen-
erating the language L. Based on these assumptions, we will design a context-free
grammar generating the language Lf = f (L), which ends the proof that the class of
context-free languages is closed under substitution.
We design a context-free grammar Gf = (Vf , Δ, Pf , S) which generates the language
Lf . Let us assume that sets of nonterminal symbols Va, a ∈Σ are pairwise disjoint and
disjoint with the set V. Then:
–
Vf = V ∪⋃a∈Σ Va;
–
Pf = P′ ∪⋃a∈Σ Pa
where P′ are productions from P:
–
unchanged, if they do not contain any terminal;
–
each terminal symbol a ∈Σ is replaced with Sa, if they contain terminals.
Justification is straightforward: productions from P′ generate words of the language L,
but in these words, we have initial symbols Sa of grammars Ga instead of terminals
a ∈Σ. Then each such symbol Sa is used to generate a word of the language La. This is
consistent with Definition 9.1.
Proposition 9.6. Homomorphism is an internal operation in classes of languages RgL,
CFL, CSL, REL and ALL.
Proof. Homomorphism is a special case of substitution. Proof is a direct consequence
of proof for substitution.
Proposition 9.7. Intersection is an internal operation in RgL, CSL, RkL and REL classes
of languages.
Proof. Compare solutions of Problem 7.4 in Chapter 7 and Problem 5.6 and Problem 5.11
in Chapter 5.
Proposition 9.8. Intersection is not an internal operation in CFL class of languages.
Proof. Consider languages L1 = {w ∈{a, b, c}∗: w = akbkcl, k, l > 0} and L2 = {w ∈
{a, b, c}∗: w = akblcl, k, l > 0}. Both are context-free, but the intersection of them
L = {w ∈{a, b, c}∗: w = akbkck, k > 0} is not context free.
Proposition 9.9. Complement is an internal operation in RgL, CSL and RkL classes of
languages.
Proof. Compare solutions of Problem 7.4 in Chapter 7 and Problem 5.7 and Problem 5.11
in Chapter 5.
Proposition 9.10. Complement is not an internal operation in CFL class.

218
|
9 Around the hierarchy of languages
Proof. Compare solution of Problems 3.6 and 3.7 in Chapter 3.
Proposition 9.11. Complement is not an internal operation in REL class.
Proof. The complement of the diagonal language is recursively enumerable, but its
complement (i. e., the diagonal language) is not, compare Example 9.6 and Propo-
sitions 9.17 and 9.18. Also, the universal language is recursively enumerable, but its
complement is not.
Proposition 9.12. Quotients (left and right) are internal operations in the class of regu-
lar languages.
Proof. Let us assume that we have a regular language L1 ⊂Σ∗accepted by a finite
automaton A = (Q, Σ, δ, q0, F) and a language L2 ⊂Σ∗(not necessarily regular). We will
design automata accepting the right quotients L1/L2 and the left quotient L1\L2.
The right quotient
L1/L2 is accepted by the finite automaton
Ar = (Q, Σ, δ, q0, Fr)
where
Fr = {q ∈Q : (∃y ∈L2)δ(q, y) ∈F}
Indeed, for any x ∈Σ∗, δ(q0, x) ∈Fr if and only if there exists y ∈L2 such that δ(q0, xy) =
δ(δ(q0, x), y) ∈F.
The left quotient
L1\L2 is accepted by the finite automaton
Al = (Q ∪{q0
l }, Σ, δl, q0
l , F)
where
δl(q, a) = {δ(q, a)}
for all q ∈Q, a ∈Σ
δl(q0
l , ε) = {q ∈Q : (∃x ∈L2)δ(q0, x) ∈F} = ⋃
x∈L2
{δ(q0, x)}
δl(q, u) = 0
otherwise
Indeed, for any y ∈Σ∗, δl(q0
l , x) ⊂F if and only if there exists x ∈L2 such that
δ(q0, xy) = δ(δ(q0, x), y) ∈F.
The above construction is not efficient: it is necessary to check all words of a lan-
guage L2 in order to complete automata construction. If we assume that L2 is a regular

9.3 The hierarchy of languages
|
219
language, we can effectively find automata accepting quotients L1/L2 and L1\L2 check-
ing words of L2 shorter than the constant in pumping lemma. This is a direct conse-
quence of pumping lemma, which says that computation for any word not shorter than
the constant in this lemma can be shortened without changing its last state; cf. proof
of the pumping lemma.
The closure properties discussed above, concerning classes of languages and op-
erations on languages, are summarized in Table 9.1.
Table 9.1: Closure with regard to operations on languages.
Is internal in
RgL
CFL
CSL
RkL
REL
ALL
union
+
+
+
+
+
+
concatenation
+
+
+
+
+
+
Kleene closure
+
+
+
+
+
+
substitution
+
+
+
+
+
homomorphism
+
+
+
+
+
intersection
+
−
+
+
+
+
complement
+
−
+
+
−
+
quotients
+
+
9.3 The hierarchy of languages
Proposition 9.13. The class RgL of regular languages in included, but not equal to the
class CFL of context-free languages.
Proof. According to Theorem 8.1 and Theorem 8.3, regular languages are generated
by right-linear grammars. On the other hand, right-linear grammars are context-free
ones. Thus, we have inclusion. Moreover, the language L = {w ∈{a, b}∗: #aw =
#bw > 0} is not a regular one, but it is a context-free one; cf. Example 2.7 and Exam-
ple 2.7 in Chapter 2 and Problem 3.1 in Chapter 3.
Proposition 9.14. The class CFL of context-free languages in included, but not equal to
the class CSL of context-sensitive languages.
Proof. Again, context-free languages are generated by context-free grammars. On the
other hand, context-free grammars are also context-sensitive ones, which generate
context-sensitive languages. Moreover, the language L = {w ∈{a, b, c}∗: #aw = #bw =
#cw > 0} is not a context-free one, but it is a context-sensitive one; cf. Problem 3.3 in
Chapter 3 and Example 4.1 in Chapter 4. Thus, we have inclusion, but not equality.
Lemma 9.1. There is a Turing machine with the stop property (an algorithm) to check
if a given word z
= a1a2 . . . an is generated by a given context-sensitive grammar
G = (V, T, P, S).

220
|
9 Around the hierarchy of languages
Proof. The Turing machine realizes a shortest paths algorithm. Let us build a graph
with nodes labeled by all words w ∈(V∪T)∗, which are not longer than n. Note that the
initial symbol of the grammar S and the word z are among labels of nodes. There is a
directed edge between two nodes labeled with words u and v if and only if there is a di-
rect derivation of v from u in G. Checking if the word v is directly derived from u is easy
and could be performed by a Turing machine with stop property. In this way, w ∈L(G)
if and only if there is a path from the node labeled S to the node labeled w. A Turing ma-
chine with stop property could be built, which checks the existence of such a path and
find the path. Such a machine implements an algorithm for path searching in a graph.
Algorithms for path searching in graphs can be found in, for example, [3, 14].
Lemma 9.2. The set of context-sensitive grammars is countable, that is, context-sensi-
tive grammars can be enumerated with natural numbers.
Proof. In fact, we can encode context-sensitive grammars as natural numbers. As-
sume that G = (V, T, P, S) is a context sensitive grammar. Then the following is done:
–
terminal and nonterminal symbols are replaced with binary numbers represented
by a block of digits of fixed lengths. The number of binary digits necessary for
encoding terminal and nonterminal symbols and an additional symbol is equal
to p = ⌊log2(|V| + |T| + 1)⌋+ 1. Assume that the number 2p −1 enumerates a special
symbol, a separator. It is represented as the block 111 . . . 11 of p binary digits 1;
–
nonterminal symbols are enumerated by successive natural numbers starting
with 0 (represented as the block 000 . . . 00 of p binary digits 0). Assume that
the beginning symbol S of a grammar is enumerated with 0. Of course, every
number is represented by a string of p binary digits, some or all of them with
nonsignificant zeros;
–
enumeration of terminal symbols is continued with successive natural numbers
following enumeration of nonterminal symbols;
–
productions are represented as sequences of p-digits blocks enumerating sym-
bols. The special symbol (i. e., the block of p ones) separates both hand sides of
productions;
–
a grammar is encoded as the following sequence of blocks of p binary digits:
–
encoding begins with two separators (two blocks of 1s);
–
nonterminal symbols (|V| blocks of binary digits, the first one is the block of
0s);
–
the separator;
–
terminal symbols (|T| blocks of binary digits);
–
the separator and a production, these two elements are repeated for every
production;
–
encoding ends with two separators.
As a result, we get a binary number that encodes the given grammar G.

9.3 The hierarchy of languages
|
221
Note that at most one context-sensitive grammar can be encoded as a given nat-
ural number and not every number encodes a grammar, that is, such numbers, for
which binary representation is not a valid code of any grammar. However, we can as-
sume that numbers, which are not valid codes of context-sensitive grammars, encode
a grammar generating the empty language. Likewise, not every natural number rep-
resents a word over the set of terminal symbols T, for instance, binary words shorter
than p. Nevertheless, we can treat such natural numbers as not generated by the gram-
mar.
This encoding is ambiguous, that is, a given grammar may have many codes.
For instance, an order of symbols or productions affects the result of encoding. All
context-sensitive grammars are encoded as natural numbers and no number is a
code of two grammars. Therefore, grammars can be ordered according to the smallest
codes, which gives a method of enumeration of grammars and accessing the gram-
mar encoded as a given number. Simply take binary representation of successive
natural numbers and then check if it is correctly encoded grammar. If a grammar of a
given code is searched, continue this process until this code is found. Of course, any
grammar can be identified in this way.
Proposition 9.15. The class CSL of context-sensitive languages is included, but not
equal to the class RkL of recursive languages.
Proof. Context-sensitive languages are accepted by linear bounded automata. Linear
bounded automata are restricted Turing machines with the stop property. Recursive
languages are accepted by Turing machines with the stop property. Thus, we have the
inclusion of the class CSL in the class RkL.
Now we design a language that is in RkL class, but not in CSL class. Let us build
a relation r ⊂N × N. A pair (k, l) of natural numbers belongs to this relation if and
only if the context-sensitive grammar encoded as l generates the binary word at k-th
place in the canonical order, that is, rk,l = 1 if the lth grammar generates the kth word,
rk,l = 0 otherwise. Consider the language of words, which are not generated by the
corresponding grammar of the code equal to index of the word, that is, with 0 at the
main diagonal in Table 9.2. This is so-called diagonal language Ld = {w ∈{0, 1}∗:
w = wi ∧ri,i = 0} in the class CSL. We will come to contrary, if we assume that Ld is
context-sensitive. If it is context-sensitive, then – due to Lemma 9.2 – it is generated
by a context-sensitive grammar encoded as some natural number, say number k and
denote it Gk. Consider the word wk in canonical order. If it is in Ld, then rk,k = 0 by def-
inition of Ld. However, rk,k = 0 means that Gk does not generate wk, though it should.
On the other hand, if wk does not belong to Ld, then rk,k = 1 by definition of Ld. How-
ever, rk,k = 1 means that Gk generates wk, though it should not. Thus, the diagonal
language Ld in the class CSL cannot be a context-sensitive one.
The language Ld is accepted by a Turing machine with the stop property. Such a
machine realizes the following algorithm:
–
it finds the number k of a given binary word in canonical order, that is, w = wk;

222
|
9 Around the hierarchy of languages
Table 9.2: The membership table for context-sensitive grammars.
δ(q0, w)
0
1
2
3
. . .
k
. . .
w0 = ε
r0,0
r0,1
r0,2
r0,3
r0,k
w1 = 0
r1,0
r1,1
r1,2
r1,3
r1,k
w2 = 1
r1,0
r1,1
r1,2
r1,3
r1,k
w3 = 00
r1,0
r1,1
r1,2
r1,3
r1,k
. . .
wk
rk,0
rk,1
rk,2
rk,3
?
. . .
–
it finds the context-sensitive grammar Gk encoded as the number k;
–
it checks, if the grammar Gk generates the word w = wk or not. The method shown
in Lemma 9.1 can be employed for checking.
This method allows answering the question if any word is generated by a given gram-
mar or not. This shows that the diagonal language Ld in the class CSL belongs to the
RkL class. In this way, we have proved that the CSL class is included but not equal to
the RkL class.
Lemma 9.3. The set of Turing machines is countable, that is, Turing machines can be
enumerated with natural numbers.
Proof. The proof is similar to the proof of Lemma 9.2. We encode Turing machines as
natural numbers. Assume that M = (Q, Σ, Γ, δ, q0, B, {qA}) is a Turing machine with a
halting accepting state. Then we encode the machine M as a binary number in the
following way:
–
states, symbols of the tape alphabet Γ, symbols of the input alphabet Σ and two
symbols of directions of the head shift are replaced with binary numbers repre-
sented by blocks of digits of fixed length. An additional symbol, a separator, is
included in the set of codes. The number of binary digits necessary for such an
encoding is equal to p = ⌊log2(|Σ|+|Γ|+|Q|+3)⌋+1. Assume that the number 2p −1
enumerates a special symbol, a separator. It is represented as the string 111 . . . 11
of p binary digits 1;
–
states are enumerated by successive natural numbers starting with 0. Assume that
the initial state is enumerated with the number 0 and the accepting state – with
the number 1 (of course, every number is represented by a block of p binary digits,
some or all of them with nonsignificant zeros);
–
enumeration of symbols of Γ is continued with successive natural numbers fol-
lowing enumeration of states;
–
then enumeration of symbols of Σ is continued with successive natural numbers
following enumeration of symbols of Γ;

9.3 The hierarchy of languages
|
223
–
then enumeration of directions of the head shift is done with the next two succes-
sive natural numbers following enumeration of Σ;
–
every transition δ(q, X) = (p, Y, D) is represented as five p-digits blocks, namely
these blocks represent arguments of the transition function (a state q and a tape
symbol X) and result (a state p, a tape symbol Y and a direction D);
–
a Turing machine is encoded as the following sequence of blocks of p binary digits:
–
the encoding begins with two separators (two blocks of 1s);
–
states (|Q| blocks of binary digits, the first one is the block of 0s), recall that
the initial state is encoded with the number 0, the accepting state is encoded
– with the number 1;
–
the separator;
–
symbols of Γ, (|Γ| blocks of binary digits);
–
the separator;
–
symbols of Σ, (|Σ| blocks of binary digits);
–
the separator;
–
directions of the head moves (2 blocks of binary digits);
–
the separator;
–
the separator and a transition, these two elements are repeated for every tran-
sition (every entry of the transition table);
–
the encoding ends with two separators.
This encoding is ambiguous, as a similar encoding in Lemma 9.2. Natural numbers,
which are not valid codes of a Turing machine, are assumed to be codes of a machine
falling in infinite computation for every input. Binary words not representing any word
over Σ are assumed not to be accepted by a Turing machine. Finally, we can conclude
that all Turing machines can be enumerated with natural numbers.
Proposition 9.16. There are languages, which are not recursively enumerable.
Proof. The set of all words Σ∗over a given alphabet Σ in infinite and countable; cf.
Example 1.2 in Chapter 1.1. The class of languages ALL = {L : L ⊂Σ∗} is the power
set of Σ∗, so then it is uncountable. On the other hand, languages of the REL class are
accepted by Turing machines. The set of Turing machines is countable (cf. Lemma 9.3)
so the class REL of languages is countable. Since the class ALL is an uncountable set,
then it cannot be equal to its countable subset. This proves that there are languages,
which are not recursively enumerable.
Example 9.6. Let design the diagonal language Ld in the class of recursively enumer-
able languages. Let r ⊂N × N is a relation build in a similar way as in Proposition 9.15,
that is, a pair (k, l) of natural numbers belongs to this relation if and only if the Turing
machine encoded as l accepts the binary word at kth place in the canonical order. In
other words, rk,l = 1 if the l-th Turing machine accepts the kth word, rk,l = 0 otherwise.
Note, that rk,l = 0 means that the lth Turing machine either terminates computation
and rejects its input or it is doing infinite computation for the kth word. The diagonal

224
|
9 Around the hierarchy of languages
language is defined by the formula Ld = {w ∈{0, 1}∗: w = wi ∧ri,i = 0}, that is, it
includes words, for which there is zero at the main diagonal of the relation r.
Proposition 9.17. The diagonal language Ld is not recursively enumerable.
Proof. If we assume that Ld ∈REL, that is, it is recursively enumerable, then we will
come to contrary. If Ld is recursively enumerable, then there exists a Turing machine,
which accepts it, say the kth machine Mk. Consider rk,k. If rk,k = 1, then wk is accepted
by Mk, so wk ∈Ld, but it should not due to definition of Ld. If rk,k = 0, then wk is not
accepted by Mk, so wk ∉Kd, but it should due to definition of Ld. Thus, the diagonal
language Ld cannot be a recursively enumerable one.
Proposition 9.18. Complement od the diagonal language Ld = {w ∈{0, 1}∗: w = wi ∧
ri,i = 1} is recursively enumerable.
Proof. In order to prove this proposition, we need to design a Turing machine accept-
ing Ld, that is, such a Turing machine which for a given the word w ∈{0, 1}∗stops
computation in accepting state if and only if w ∈Ld. Let us recall that for a word w ∈Ld
it either stops computation in not accepting state or is doing infinite computation. For
a word w ∈{0, 1}∗, such a machine carries out the following computations:
–
it encounters index i of w in canonical order, that is, w = wi;
–
it finds the Turing machine Mi encoded as i;
–
it simulates computation of Mi for wi;
–
it accepts w = wi if and only if Mi accepts wi.
Notice that for any w ∈{0, 1}∗, for which we have one at the main diagonal of the
relation r defined in Example 9.6, this Turing machine will finish computation in an
accepting state and only for such words over the alphabet {0, 1}.
Example 9.7. The universal language Lu ⊂{0, 1}∗includes words ⟨M w⟩being con-
catenation of an encoded Turing machine ⟨M⟩and a binary word w such that M ac-
cepts w. Refer to Lemma 9.3 with regard to encoding Turing machines.
Lemma 9.4. The universal language Lu is a recursively enumerable one.
Proof. We design a Turing machine, which accepts the universal language Lu. This is
the so-called universal Turing machine Mu. The machine has three tapes. It realizes
the following algorithm:
1.
checks if an input word is of a form ⟨M w⟩, where ⟨M⟩is a valid code of a Turing
machine;
–
looks for beginning sequence of 1s, if it is not of even length 2r, rejects the
input;
–
stores r 0s on the third tape, the content of the third tape is used as a measure
of the length of the blocks of binary digits encoding the machine and as a
number of the current state;

9.3 The hierarchy of languages
|
225
–
looks for the next sequence of 2r digits 1;
–
moves the beginning part of the input word bounded by both blocks of 2r
digits 1 to the second tape, leaves w on the first tape, places the head of the
first tape on the leftmost symbol of w;
–
checks if the content of the second tape is a valid code ⟨M⟩of a Turing ma-
chine;
2.
repeats the following actions until the number stored on the third tape encodes
the accepting state:
–
for the state q stored on the third tape and for the symbol X stored as a block
of r binary digits with the head of the first tape placed on the leftmost digit
of this block retrieve a matching transition δ(q, X) = (p, Y, D). There may be
more than one matching transition, so this is nondeterministic choice;
–
replace the content of the third tape with p, replace X by Y and move the head
of the first tape to neighboring cell in the direction described by D.
Lemma 9.5. The universal language Lu is not a recursive one.
Proof. Let us assume that Lu is recursive, that is, that a Turing machine M′ with the
stop property accepts Lu. Based on this assumption, the machine Md accepting the
diagonal language in the class REL can be built, which contradicts Proposition 9.16.
Therefore, the universal language cannot be recursive.
A hypothetical Turing machine Md, assumed to accept the diagonal language in
the class REL, realizes the following algorithm:
–
for a given input word w, Md retrieves the index k of w in the canonical order, that
is, it finds such k, that w = wk;
–
Md retrieves binary representation ⟨Mk⟩of the kth Turing machine Mk;
–
Md concatenates binary representation ⟨Mk⟩of the kth Turing machine Mk with w;
–
Md simulates computation of the hypothetical machine M′ for the concatenation
of ⟨Mk⟩and w;
–
Md terminates computation if and only if M′ terminates its computation, then Md
reverses an output of M′, that is, Md accepts if and only if M′ rejects.
Note that Md accepts if and only if the Turing machine encoded as k rejects w = wk.
Moreover, Md has the stop property since the hypothetical Turing machine M′ is as-
sumed to have the stop property. Therefore, the assumption that M′ exists is false.
Remark 9.6. The following inclusions hold based on discussion in this section:
RgL ⊊CFL ⊊CSL ⊊RkL ⊊REL ⊊ALL
The following inclusions are called the Chomsky hierarchy:
RgL ⊊CFL ⊊CSL ⊊REL ⊊ALL
where ⊊denotes inclusion, but not equality.


Bibliography
References
[1]
J. E. Hopcroft, J. D. Ullman, Introduction to Automata Theory, Languages and Computation,
Addison-Wesley Publishing Company, Reading, Massachusetts, 1979, 2001.
[2]
J. E. Hopcroft, R. Motwani, J. D. Ullman, Introduction to automata theory, languages and
computation, Addison-Wesley, Boston, 2001.
[3]
T. Cormen, C. Leiserson, R. Rivest, C. Stein, Introduction to Algorithms, MIT Press, 2009.
Additional readings, not cited in the text
[4]
A. V. Aho, J. E. Hopcroft, J. D. Ullman, Data Structures and Algorithms, Addison-Wesley, Reading,
1983.
[5]
A. V. Aho, M. S. Lam, R. Sethi, J. D. Ullman, Compilers: Principles, Techniques, and Tools, 2nd
Edition, Pearson, 2007.
[6]
A. V. Aho, R. S. Sethi, J. D. Ullman, Compilers: principles, techniques and tools, Addison-Wesley,
Reading, 1986.
[7]
D. P. Bovet, P. Crescenzi, Introduction to the theory of Complexity, Prentice Hall, 2006.
[8]
N. Chomsky, Rules and representations, Columbia University Press, New York, 1980.
[9]
N. Chomsky, M. Halle, The sound patterns of English, Harper and Row, New York, 1968.
[10] N. Chomsky, Aspects of a theory of syntax, MIT Press, Cambridge, Massachusetts, 1965.
[11] N. Chomsky, Cartesian linguistics: a chapter in the history of rationalist thought, Harper and
Row, New York, 1965.
[12] M. B. Moret, The Theory of Computation, Addison-Wesley Publishing Company, 1998.
[13] C. H. Papadimitriou, Computational Complexity, Addison-Wesley Longman, 1995.
[14] R. Sedgewick, K. Wayne, Algorithms, Addison-Wesley Professional, 2011.
[15] J. R. Shoenfiled, The mathematical work of S. C. Kleene, Bull. Symb. Log., 1995.
[16] M. Sipser, Introduction to the Theory of Computation, Thomson, 2006.
[17] E. Kinber, C. Smith, Theory of Computing: A Gentle Introduction, Sacred Heart University,
University of Maryland, Pearson, 2001.
[18] R. J. Wilson, Introduction to Graph Theory, Addison Wesley, 1996; Pearson Higher Education,
2010.
[19] N. Wirth, Algorithms and Data Structures, Prentice Hall, 1985.
https://doi.org/10.1515/9783110752304-010


Index
ε-production 45
algorithm
– Cocke–Younger–Kasami 64
alphabet 13
automaton
– finite 156, 163, 172, 187, 188, 190, 193, 195
– closure of transition function 166, 175
– closure of transition relation 162
– computation 158, 164, 175
– configuration 157, 163, 172
– deterministic 156
– epsilon closure 172
– input accepted 158, 164, 175
– language accepted 159, 164, 175
– step description 157, 163, 172
– transition relation 158, 164, 174
– linear bounded 130, 207
– pushdown 139, 201, 205
– accepting with empty stack 148
– computation 143
– configuration 141
– deterministic 146
– input accepted 143
– language accepted 143
– step description 141
– transition relation 142
Chomsky normal form 50
cleaning procedure of
– Turing machine 95
closure
– Kleene 15
closure of relation 8
Cocke–Younger–Kasami algorithm 64
computation of
– finite automaton 158, 164, 175
– pushdown automaton 143
– Turing machine 98, 124
concatenation 15
configuration of
– finite automaton 157, 163, 172
– pushdown automaton 141
– Turing machine 96, 108, 115
context-free
– grammar 37, 201, 205
– language 37
context-sensitive grammar 207
derivation
– leftmost 38
– rightmost 38
– tree 39
derivation in grammar 18
diagonal language 221, 224
epsilon closure
– finite automaton 172
equivalence class 7
equivalence of
– Turing machines 102
equivalence relation 7
finite automaton 156, 172, 187, 188, 190, 193,
195
– closure of transition function 166, 175
– closure of transition relation 162
– computation 158, 164, 175
– configuration 157, 163, 172
– epsilon closure 172
– input accepted 158, 164, 175
– language accepted 159, 164, 175
– nondeterministic 163
– step description 157, 163, 172
– transition relation 158, 164, 174
function computed by
– Turing machine 98
grammar 17
– ambiguous 40
– Chomsky normal form 50
– context-free 37, 201, 205
– context-sensitive 207
– Greibach normal form 53
– Greibach uniqueness condition 57
– left-linear 32, 197
– LL(1) 71
– uniqueness condition 71
– production
– ε-production 45
– unit 48
– regular 32, 193, 195, 197
– right-linear 32, 193, 195, 197
https://doi.org/10.1515/9783110752304-011

230
|
Index
– symbol
– nullable 45
– useless 42
– translation 69
– unrestricted 205
Greibach normal form 53
Greibach uniqueness condition 57
halting accepting state of
– Turing machine 102
halting state of
– Turing machine 103
homomorphism 210, 217
– of languages 210
input accepted by
– finite automaton 158, 175
– pushdown automaton 143
– Turing machine 98, 125
Kleene closure 15, 216
language 14
– context-free 37
– diagonal 221, 224
– expression 27
– homomorphism 210
– inherently ambiguous 40
– quotient 211
– substitution 209
– universal 224
language accepted by
– finite automaton 159, 164, 175
– pushdown automaton 143
– Turing machine 98
lemma
– Myhill–Nerode 29
– Ogden 61
– pumping 31, 57, 197
linear bounded automaton 130, 207
LL(1) grammar 71
mathematical induction 9
Myhill–Nerode lemma 29
Myhill–Nerode theorem 199
nondeterminism 124
– basic assumption 124
– degree 124
– interpretation 124
nondeterministic
– finite automaton 163
– pushdown automaton 139
– Turing machine 123
nullable symbol 45
Ogden lemma 61
pumping lemma 31, 57, 197
pushdown automaton 139, 201, 205
– accepting with empty stack 148
– computation 143
– configuration 141
– deterministic 146
– input accepted 143
– language accepted 143
– step description 141
– transition relation 142
quotient 211
– of languages 211
regular
– expression 25, 187, 188, 190
– grammar 32
– language 27
relation
– closure 8
– equivalence 7
– induced by language 16
– right invariant 16
step description of
– finite automaton 157, 163, 172
– pushdown automaton 141
– Turing machine 96, 108, 115, 124
stop property of
– Turing machine 100
substitution 208, 216
– of languages 209
theorem
– Myhill–Nerode 199
transition relation of
– finite automaton 158, 164, 174
– pushdown automaton 142
– Turing machine 97, 124
translation grammar 69
tree 11
Turing machine 94, 205

Index
|
231
Turing machine
– basic model 94
– cleaning procedure 95
– computation 98, 124
– configuration 96, 108, 115
– degree of nondeterminism 124
– equivalence 102
– function computed 98
– halting accepting state 102
– halting state 103
– halting state with guard 106
– input accepted 98, 125
– language accepted 98
– multi-tape 112
– multi-track tape 106
– nondeterministic 123
– step description 96, 108, 115, 124
– stop property 100
– transition relation 97, 124
– two-way infinite tape 108
– with guard 103
uniqueness condition of
– LL(1) grammar 71
unit production 48
universal language 224
unrestricted grammar 205
useless symbol 42


