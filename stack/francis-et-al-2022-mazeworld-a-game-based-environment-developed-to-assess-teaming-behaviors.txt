 
MazeWorld: A Game-Based Environment developed to Assess 
Teaming Behaviors
 
Rick F. Francis, Jonathan Segal, Yvonne Farah, Joseph Rozell, Michael C. Dorneich, Stephen Gilbert 
Iowa State University 
 
A testbed has been developed to study multiple constructs of teamwork in a parameterized 
environment. MazeWorld is a testbed to learn and automate the collection of objective team metrics based 
on observable behaviors. Quantifying teamwork plays an important role in future agent cooperation. 
Challenges include differentiating individual skills from skills of the team to not only understand the team 
as an entity but each of the participating members. MazeWorld is a Unity-based cooperative game that 
involves three distinct roles working interdependently to achieve a goal. A series of trials were run in 
MazeWorld to test proposed observable measures of teaming constructs. Teams performed better as the 
trials progressed, and differences were found between teams. This effort serves as a proof of concept that 
MazeWorld can explore team behavior using automated team metrics to inform future work.  
 
INTRODUCTION 
This paper explores the team metrics used in MazeWorld, 
a team research testbed, and their effectiveness in facilitating 
comprehension of teaming dynamics. MazeWorld is a testbed 
explicitly designed to isolate and extract behavioral metrics to 
understand teaming (Cavanah et al., 2020). A team being two 
or more individuals who share common goals and interact to 
achieve the said goal (Kozlowski, 2007). At this stage of 
development, an agent has not been implemented. An agent is 
an autonomous, computer-based entity occupying a specific 
role on the team and hence can be recognized as a team 
member (O’Neill et al., 2020). However, team metrics are 
needed to assess agents properly. Research has shown that 
humans behave and interact in similar fashions with agents as 
they do with one another (Nass et al., 1996). Using teams of 
humans will assess teaming skill metrics. The goal is to 
uncover objective measures of team skills. 
How would they be assessed in scenarios where teams are 
very cooperative but not skilled in the task? The team may 
have a high or low level of team skills, but that might be hard 
to tell with just performance measures. Alternatively, there 
could be high-performing teams that do not work well 
together. Being able to differentiate between task skill 
proficiency and team skill proficiency would be insightful. 
The goal with the data collection is to have automated data 
collection. Observables are clearly defined and free 
researchers from the extensive time required to do it manually. 
Eighteen teams of three to four participants were run in 
the testbed over three gameplay trials. Team metrics were 
tested to understand each teams’ teaming skills. The results 
demonstrated that teams improved over trials. Furthermore, a 
differentiation between teams demonstrated that the metrics 
might be diagnostic enough to describe skill levels between 
teams. However, the results also demonstrated that more work 
is needed to modify the testbed to more directly isolate 
behavioral markers to result in more distinct metrics. 
 
RELATED WORK 
Several issues arise when assessing the behavior of a 
team. Behavioral markers of team constructs, when 
normalized and quantified in a reproducible manner, could 
establish reliable team metrics for the community. 
Standardizing measures or methods within a research 
community leads to the previous outcomes. Having 
reproducible and reliable measures across the community. 
(Münstermann et al. 2008).  
At the highest level of abstraction, constructs describe 
teamwork components (Ostrander et al., 2015). Constructs like 
coordination and cooperation can be found at this level. 
Constructs cannot easily be measured directly. However, a 
way to measure these constructs is to observe behavioral 
markers. Behavioral markers are objective, quantitative 
measurements that can reliably predict a construct (Sottilare et 
al., 2017). These markers can be complex because of the 
unpredictable relationships between individual team members’ 
task-related skills, teamwork skills, and the team’s experience 
working together (Salas et al., 2007). The challenge of 
understanding teaming becomes further complicated with 
agents. Humans with good teamwork skills can interact with 
others, while they may not have an appropriate understanding 
of how to interact with an agent. Additionally, those who 
struggle with teaming skills may find it difficult to work with 
simulated agents (Chen & Barnes, 2014). The eventual goal of 
this project is to develop a testbed that enables 
experimentation with different teaming structures, feedback 
types, agent capabilities, and Human autonomy teaming 
(HAT) characteristics. Several areas of related work are 
reviewed: measuring teams generally and platforms designed 
to facilitate HAT measurement.  
 
Measuring Teamwork 
Teamwork is considered a popular topic, especially as 
team-based structures become more complex in emergency 
rooms or combat (Salas et al., 2005). However, the question of 
what makes a successful team remains challenging, even 
though researchers have developed conceptualizations of team 
Copyright 2022 by Human Factors and Ergonomics Society. All rights reserved. 10.1177/1071181322661180
Proceedings of the 2022 HFES 66th International Annual Meeting
65

 
 
performance and models of team effectiveness. Most of them 
struggle to provide standardized tests for different factors of 
teamwork (Salas et al., 2005). Moreover, the patterns of these 
interactions between individuals depend on the team 
characteristics and the context where the team is performing 
(Kozlowski, 2007). Several factors targeted by researchers 
include team structure, communication, leadership, and mutual 
trust (Fiore, 2008). Measuring these constructs directly is 
challenging and thus generally accomplished via 
questionnaires and self-report surveys (Delice et al., 2019). 
While this has been successful in part, more objective, real-
time measurements of team performance are needed (Salas et 
al., 2008). One of these approaches relies on objectively 
definable behavioral markers in a team context (Sottilare et al., 
2017). This measurement technique can avoid the problems of 
judgment errors, biases, and different standards between 
participants that may arise in self-reported data (Sottilare et 
al., 2017). Eventually, using raw data to quantify behavioral 
markers would lead to team metrics, a quantitative 
representation of team constructs (Ostrander et al., 2015).  
 
 
Teaming Research Platforms  
Assessing team performance in a controlled environment 
is challenging due to the multiple variables to be measured 
during such an experiment (Delice et al., 2019). Comparing 
multiple teams requires a standard set of metrics derived from 
running teams through tasks that assess the teams against such 
metrics. Previous studies have analyzed teams using games 
such as BlocksWorld (Johnson et al., 2009) and Minecraft for 
urban search and rescue (Bartlett & Cooke, 2015). Custom-
built platforms include military surveillance (Ostrander et al., 
2015), the Team Multiple Errands Task (Walton et al., 2015), 
and NASA’s Moonbase Alpha (Wiltshire et al., 2017) and 
CERTT’s UAV-STE (Cooke & Shope, 2004). In general, 
simulated platforms have struggled to isolate teaming skills 
from task skills capable of understanding the teams separate 
from the context of the testbed (Cavanah et al., 2020). Several 
requirements for an ideal testbed inspired by some of these 
challenges informed the development of MazeWorld. 
 
APPROACH  
MazeWorld (Figure 1) is an application built with the 
Unity game engine and distributed online through a Qualtrics 
survey. The simulation consists of coins spread throughout a 
3D maze. Some doors block access throughout the maze until 
a specific player opens. These doors close after a set amount 
of time and must be reopened.  
Players were assigned one of three roles: Explorer, 
Collector, or Tactical. The Explorer’s role is to activate coins 
(i.e., make them visible to the Collector) by touching them and 
to open doors (only the Explorer could open doors). The 
Collector’s role is to collect as many activated coins as 
possible. The Tactical has an overhead view of the entire maze 
and sees both activated and non-activated coins. The 
Tactical’s role is to direct the Explorer and Collector to collect 
all the coins in the maze. A MazeWorld team consists of three 
members, one in each role. Each trial in MazeWorld is 240 
seconds, which through pilot testing, was found to be enough 
time for a skilled team to finish the maze and collect all the 
coins.  
 
 
Figure 1. A top view of a MazeWorld maze 
 
The Collector and the Explorer start in the same room and 
use a standard mouse and keyboard first-person game controls 
to navigate the environment. Explorers click on doors to open 
them. All players communicate through an integrated text 
chat. The Tactical can pan and zoom to overview the entire 
environment to see coins and where other players are.  
Roles are co-dependent, which forces measurable 
interactions. Interdependence allowed for the coupling of 
responsibilities. In addition, individual responsibilities allow 
for quantifying both individual and team performance. The 
team members need to communicate with each other to best 
utilize the strengths of the roles to enhance the team’s ability. 
The proposed metrics are meant to capture measurables 
for the constructs. Team task ability describes the collective 
task ability of the team. Task ability has been tied to mental 
model development, at least with an individual (Day et al. 
2001). Additional research makes it feasible to transition that 
reasoning to a team (Ree, Carretta, & Teachout, 1995; 
Schmidt & Hunter, 1992). The metrics consider the task skills 
of the individual and the team efficiency. Efficiency can 
differentiate teams that are similar otherwise. Tasks of the 
game require specific actions and interactions between 
specific players to complete tasks; these can form the basis of 
observable measures of workload within the team. 
 
METHODS 
The study had 18 teams and 56 participants. (24 female 
and 32 male) with an average age of 20 years old (SD = 2.0). 
All were placed in teams of three, except for two teams of 
four. The fourth member was added as an additional collector.  
Each participant received a description of the study, gave 
informed consent, and joined the MazeWorld Discord server. 
A researcher moderated the session and provided setup and 
assistance. The experiment was conducted remotely, where 
participants were not co-located. Participants were given a 
Qualtrics survey that started with a tutorial that explained the 
Copyright 2022 by Human Factors and Ergonomics Society. All rights reserved. 10.1177/1071181322661180
Proceedings of the 2022 HFES 66th International Annual Meeting
66

 
 
trial and the objective and tested the procedures. Next, 
demographics, level of previous experience with software 
agents, and video game experience were collected.  
The study involved three separate trials per team. The 
moderator assigned each participant a single role (Tactical, 
Collector, or Explorer) for all three trials. Participants were 
told to collect as many coins as quickly as possible. The maze 
elements remained the same for each trial.  
The analysis of the data from the trials utilized multiple 
tools. Through a replay feature, researchers visually analyzed 
trials post-hoc. For these participants, researchers also 
analyzed some of the data by hand to understand the output 
and map the trial events to the corresponding metrics. The 
individual trials were saved into a CSV file. A snapshot of the 
elements in the maze was taken ten times per second. The 
tracked elements were the doors, coins, and the participants’ 
coordinates. At any given second, one could see what doors 
were open or what coins had been collected. These files can 
also be loaded into MazeWorld to watch a full trial replay. 
 
Metrics 
Team Task Ability. Several MazeWorld design features 
could be used to measure a team’s ability. In MazeWorld, the 
teams were required to cooperate to achieve the shared goal of 
collecting coins. However, for the team to complete this 
common goal, individuals were expected to apply task-related 
abilities that reflect their competence in performing the task 
successfully (Breuer et al., 2019), such as the ability to 
activate and collect coins. Moreover, since Tactical, Explorer, 
and Collector were equipped with different yet complementary 
abilities (Morschheuser et al., 2017), their cooperative 
behaviors were essential for completing the task. Thus, for the 
researchers to measure the team's ability, performance 
measures that reflected individual and team competence were 
collected. Table 1 summarizes the metric collected, with short 
descriptions to follow. 
 
Table 1 Metrics collected. 
Construct 
Metric 
Units / Scale 
When Collected 
Team Task 
Ability (A) 
Game-activated 
coins collected 
0-3 
During trial 
 
Coins Activated 
0-10 
During trial 
 
Explorer monitoring 
Collector 
Seconds 
During trial 
Efficiency (E) Coins per minute 
 
During trial 
 
Amount of time 
players were in the 
same room 
Seconds 
Replay 
 
Amount of idle time 
Seconds 
Replay 
 
Unique Number of 
Doors Opened 
0-10 
During Trial 
Team 
Workload 
(W) 
Number of Doors 
Opened (Including 
repeats) 
 
During Trial 
 
 
Number of Explorer-
activated Coins 
Collected 
Depends on 
Coins 
Activated 
During Trial 
 
Percentage of Maze 
Zones Traversed (26) 
Percentage 
Replay 
 
Game Activated Coins Collected. In this version of 
MazeWorld, there were 13 available coins every trial. Three 
coins did not need the Explorer to activate them to be 
collected.  
Number of coins activated. There were ten available coins 
during the trials that the Explorer could activate. This metric 
showed how many of those were activated. 
Amount of time Explorer was monitoring Collector. This 
metric was a time measure to see how long the Explorer 
monitored the Collector. The time that Explorer would glance 
in the direction of Collector, it would be measured (excluding 
instances where the Explorer was changing direction). 
Efficiency. The efficiency with which a team completed 
the task was essential for quantifying the team’s proficiency. 
A team that completed collecting all the coins in less time was 
more effective. Data was collected on efficient and inefficient 
actions via multiple behavioral markers. For instance, 
participants who failed to collect coins in the adjacent or the 
same room were deemed inefficient. Additionally, for 
enhanced efficiency observation, the researchers tracked the 
strategies developed from one trial to another to assess what 
strategy would result in higher efficiency markers. 
Coins per minute This metric was a rate of how many 
coins were collected while the team was in the maze. 
Accounts for teams completing the task with spare time. 
Amount of Time Players spent in the same room was 
collected from replay observations. The maze consisted of 
rooms and hallways. The time the Explorer and Collector were 
in the same area was tracked. 
Amount of time the players were idle was tracked through 
replay observation. Once players were without movement, the 
time would be recorded. 
Unique Doors Opened (10) Each door was internally 
numbered in MazeWorld, and once it was opened, it would be 
recorded. 
Team Workload. Team workload was the overall effort 
involved with completing a task and the extent to which the 
task required cognitive effort. How many doors opened 
(including repeats) exhibited factors such as determining how 
many actions were taken to complete the task, where a higher 
action count indicated more work. The number of Explorer-
activated coins expressed teaming workload. It required coins 
to be activated and then collected, which required cooperative 
work between the Collector and Explorer roles. Additional 
workload may have included the communication provided by 
the Tactical role for the other two roles in the maze. The 
percentage of the maze explored additionally showed how 
much work was done. Going through more of the maze 
required more actions, and the Explorer opening more doors. 
Number of Doors Opened (Including repeats) This 
metric counted how many total doors were opened during the 
trial. 
Number of Explorer-Activated Coins Collected This 
metric counted the coins collected that had to be activated by 
the Explorer. 
Percentage of Maze Zones Traversed (26) This maze 
was split into 26 zones, and a team’s paths were recorded. 
Copyright 2022 by Human Factors and Ergonomics Society. All rights reserved. 10.1177/1071181322661180
Proceedings of the 2022 HFES 66th International Annual Meeting
67

 
 
This metric is a percentage of the total zones a team traversed 
during the trial. 
Data analysis 
One-way ANOVA tests were run to compare the 
averages across trials for each metric. Pairwise comparisons 
were analyzed using Tukey’s Honest Difference Test (HSD). 
The results are reported as significant for alpha <.05 and 
marginally significant for alpha <.10 (Gelman, 2013). Cohen’s 
d was calculated to check effect size (Cohen, 1988). The 
Cohen’s d results are reported as small effect for .20 < d <.50, 
medium effect for .50 < d <.80, and large effects for d >.80. 
 
RESULTS 
The scores of the 18 teams working through the 
MazeWorld challenge three times each are shown in Table 2.  
 
Table 2: Means (M) and standard deviations (SD) (n=19) 
Metrics 
Trial 1 
M (SD) 
Trial 2 
M (SD) 
Trial 3 
M (SD) 
A: Game-activated 
coins collected 
2.0 (0.8) 
2.4(0.8) 
2.8 (0.5) 
A: Coins activated 
6.1 (2.0) 
7.2 (2.0) 
8.0 (1.1) 
A: Explorer monitoring 
Collector 
15.3 (14.2) 
18.2 (16.0) 
12.6 (10.3) 
E: Coins per minute 
1.9 (0.7) 
2.4 (0.8) 
2.8 (0.6) 
E: Time players were in 
the same room 
176.4 (58.9) 
197.2 (49.6) 
220.0 (24.2) 
E: Amount of idle time 
7.8 (5.5) 
13.94 (11.4) 
15.7 (16.4) 
E: Unique Number of 
Doors Opened 
7.1 (2.2) 
6.7 (2.4) 
7.9 (1.5) 
W: Number of Doors 
Opened (incl. repeats) 
9.2 (4.8) 
9.2 (4.3) 
10.2 (2.9) 
W: Explorer-activated 
Coins Collected 
5.6 (2.3) 
6.9 (2.1) 
8.0 (1.1) 
W: Percentage of Maze 
Zones Traversed (26) 
65% (15%) 
72% (19%) 
78% (12%) 
 
There was a significant effect of trial for game activated 
coins collected, F(2,34) = 5.95, p = .007. Pairwise comparison 
showed that trail 1 (M = 2.0, SD = 0.8) was significantly lower 
than trial 3 (M = 2.8, SD = 0.5, p = .005, d = 1.20). There was 
a significant effect of trial for explorer-activated Coins 
collected, F(2,34) = 14.3, p < .001. Pairwise comparison 
showed that trail 1 (M = 5.6, SD = 2.3) was significantly lower 
than trial 2 (M = 6.9, SD = 2.1, p = .011, d = 0.59) and trial 3 
(M = 8.0, SD = 1.1, p < .001, d = 1.33). There was a 
significant effect of trial for coins per minute, F(2,34) = 15.7, 
p < .001. Pairwise comparison showed that trail 1 (M = 1.9, 
SD = 0.7) was significantly lower than trial 2 (M = 2.4, SD = 
0.8, p = .010, d = 0.66) and trial 3 (M = 2.8, SD = 0.6, p < 
.001, d = 1.38). Trail 2 (M = 2.4, SD = 0.2) was significantly 
lower than trial 3 (M = 2.8, SD = 0.2, p = .033, d = 0.89). 
There was a significant effect of trial for time players were in 
the same room, F(2,34) = 4.95, p = .013. Pairwise comparison 
showed that trail 1 (M = 176.4, SD = 58.9) was significantly 
lower than trial 3 (M = 220.0, SD = 24.2, p = .010, d = 0.97). 
There was a significant effect of trial for coins activated, 
F(2,34) = 9.06, p < .001. Pairwise comparisons showed that 
trail 1 (M = 6.1, SD = 2.0) was significantly lower than trial 3 
(M = 8.0, SD = 1.1, p < .001, d = 1.18). All other metrics did 
not show a significant difference for the main effect of the 
trial. 
 
DISCUSSION  
The objective of this paper was to outline how 
MazeWorld can be utilized to collect teaming metrics in a 
controlled environment. As MazeWorld is still under active 
development, this paper used data from an initial sample set of 
human-only teams to understand how to best continue the 
testbed development and derive better tasks to isolate teaming 
metrics. The aim is to develop observable metrics to quantify 
what was once measured subjectively through questionnaires. 
The metrics creation process shows that it is possible to 
move from abstract constructs to quantifiable metrics, which is 
considered the first step of the iterative process of developing 
metrics (Ostrander et al., 2015). Team task ability is a 
construct reflecting how team members’ abilities contribute to 
team-level outcomes (Barrick et al., 1998). As teams work 
together more, they are capable of mastering their task skills, 
enhancing their strategies, communicating, and accordingly 
start performing as a cooperative team (Delice et al., 2019). 
The data shows, on average, that the teams are performing 
better through the study, indicating a learning curve to the 
tasks required in the game. These differences could also be 
related to team developmental stages, as after a certain stage, 
the team starts performing with high cohesion, and there is 
little further room for improvement (Venezia, 1985).  
While ability (A) and workload (W) metrics reflected 
individual and task skills, efficiency markers (E) emphasized 
the team’s ability to work cooperatively and follow strategies. 
For instance, the mean percentage of time Explorer and 
Collector spent together increased for all teams throughout the 
three trials. This pattern reflects the efficient strategy teams 
developed to collect coins faster. Finally, the number of coins 
collected per minute also increased. This could be a result of 
as the teams became more efficient, they could collect coins at 
a higher speed. Moreover, the percentage of maze-explored 
increased, on average, throughout the trials for all teams, 
which indicates that while their skills increased, their ability to 
handle the game’s workload also increased.  
The proposed metrics show preliminary signs of success. 
Metrics like Explorer-activated coins collected take 
performance markers and show interactive team measures. 
Those coins need the Explorer to activate the coin and the 
Collector to obtain it. Researchers have some indication of 
how the team is working together because of interdependent 
tasks like that. However, a metric like the Number of Doors 
opened may not, at its current iteration, be a proper 
representation of workload. It describes how often a task is 
done but does not consider how difficult it is. Without 
knowing that context, the metric loses some context.  
Through an iterative process, the metrics will become 
more representative of what they are measuring. In future 
iterations, having subjective measures in tandem with 
proposed metrics will validate the metrics. 
For several reasons, the link between the collected 
markers and the teams’ constructs is not yet strongly 
Copyright 2022 by Human Factors and Ergonomics Society. All rights reserved. 10.1177/1071181322661180
Proceedings of the 2022 HFES 66th International Annual Meeting
68

 
 
established. Differentiating between ability and workload 
behavioral markers (and simple experience) is essential in 
future implementations and a better understanding of the 
correlation between ability and workload. Metric creation can 
be difficult due to situational differences between other 
testbeds. A metric designed for another testbed may not 
seamlessly transition to MazeWorld. Although some of the 
proposed metrics measured specific roles, more is needed. 
Developing metrics to evaluate individual players and the 
interactions between roles and the team is also needed. 
 
CONCLUSION 
The need for understanding human teaming will only 
increase as agents become more intelligent. It is important to 
measure what makes a good human-agent relationship. Our 
goal was to research the effectiveness of human teams using a 
flexible, scalable, and generalizable testbed to inform teaming 
research (MazeWorld). Ideally, metrics will provide objective 
measures to once subjective team processes. This study is the 
first step in the iterative process of refining those metrics in a 
parametrized testbed. 
 
ACKNOWLEDGMENTS 
This project was funded in part by the Game2Work 
research program, as part of the Iowa State University 
Presidential Interdisciplinary Research Initiative. 
 
REFERENCES 
Barrick, M. R., Stewart, G. L., Neubert, M. J., & Mount, M. K. (1998). 
Relating member ability and personality to work-team processes and 
team effectiveness. Journal of Applied Psychology, 83(3), 377–391. 
https://doi.org/10.1037/0021-9010.83.3.377 
Bartlett, C. E., & Cooke, N. J. (2015). Human-Robot Teaming in Urban 
Search and Rescue. Proceedings of the Human Factors and Ergonomics 
Society Annual Meeting, 59(1), 250–254. 
https://doi.org/10.1177/1541931215591051 
Breuer, C., Hüffmeier, J., Hibben, F., & Hertel, G. (2019). Trust in teams: A 
taxonomy of perceived trustworthiness factors and risk-taking behaviors 
in face-to-face and virtual teams. Human Relations, 73(1), 3–34. 
https://doi.org/10.1177/0018726718818721 Cannon-Bowers, J. A., 
Tannenbaum, S. I.,  
Cavanah, E., Ford, Z., Jasper, A., Stonewall, J., Gilbert, S. B., & Dorneich, M. 
(2020). Creating Metrics for Human-Agent Teams. Proceedings of the 
Human Factors and Ergonomics Society Annual Meeting, 64(1), 349–
353. https://doi.org/10.1177/1071181320641079 
Chen, J. Y. C., & Barnes, M. J. (2014). Human–Agent Teaming for 
Multirobot Control: A Review of Human Factors Issues. IEEE 
Transactions on Human-Machine Systems, 44(1), 13–29. 
https://doi.org/10.1109/thms.2013.2293535 
Cohen, J. (1988). Statistical power analysis for the behavioral sciences 
Lawrence Earlbaum Associates. Hillsdale, NJ, 20-26. 
Cooke, N. J., & Shope, S. M. (2004). Synthetic task environments for teams: 
CERTT’s UAV-STE. In Handbook of human factors and ergonomics 
methods (pp. 476-483). CRC Press. 
Day, E. A., Arthur, W., Jr., & Gettman, D. (2001). Knowledge structures and 
the acquisition of a complex skill.Journal of Applied 
Psychology,86,1022–1033 
Delice, F., Rousseau, M., & Feitosa, J. (2019). Advancing Teams Research: 
What, When, and How to Measure Team Dynamics Over Time. 
Frontiers in Psychology, 10. https://doi.org/10.3389/fpsyg.2019.01324 
Fiore, S. M. (2008). Interdisciplinarity as Teamwork. Small Group Research, 
39(3), 251–277. https://doi.org/10.1177/1046496408317797 
Gelman, A. (2013). Commentary: P values and statistical practice. 
Epidemiology, 24(1), 69-72. 
Johnson, M., Bradshaw, J. M., Duran, D., Vignati, M., Feltovich, P. J., Jonker, 
C., & Birna Van Riemsdijk, M. (2009). RT4T: A Reconfigurable 
Testbed for Joint Human-Agent-Robot Teamwork. 
Kozlowski, S. W. J. (2007). Team Learning, Development, and Adaptation 
[E-book]. In B. S. Bell (Ed.), Work Group Learning (1st ed., pp. 1–30). 
Taylor & Francis. 
Marks, M. A., Mathieu, J. E., & Zaccaro, S. J. (2001). A Temporally Based 
Framework and Taxonomy of Team Processes. Academy of 
Management Review, 26(3), 356–376. 
https://doi.org/10.5465/amr.2001.4845785 
Morschheuser, B., Riar, M., Hamari, J., & Maedche, A. (2017). How games 
induce cooperation? A study on the relationship between game features 
and we-intentions in an augmented reality game. Computers in Human 
Behavior, 77, 169–183. https://doi.org/10.1016/j.chb.2017.08.026 
Münstermann, Björn and Weitzel, Tim, "What Is Process Standardization?" 
(2008). CONF-IRM 2008 Proceedings. 64. 
http://aisel.aisnet.org/confirm2008/64 
Nass, C., Fogg, B., & Moon, Y. (1996). Can computers be teammates? 
International Journal of Human-Computer Studies, 45(6), 669–678. 
https://doi.org/10.1006/ijhc.1996.0073 
O’Neill, T., McNeese, N., Barron, A., & Schelble, B. (2020). Human–
Autonomy Teaming: A Review and Analysis of the Empirical 
Literature. Human Factors: The Journal of the Human Factors and 
Ergonomics Society, 001872082096086. 
https://doi.org/10.1177/0018720820960865 
Ostrander, A., Gilbert, S., & Dorneich, M. (2019). Team Data Analysis Using 
FATE: Framework for Automated Team Evaluation. In A. M. Sinatra & 
J. A. DeFalco (Eds.), Proceedings of the Approaches and Challenges in 
Team Tutoring Workshop (pp. 5-14). Springer. http://ceur-ws.org/Vol-
2501/ 
Ree, M. J., Carretta, T. R., & Teachout, M. S. (1995). Role of ability and prior 
job knowledge in complex training performance. Journal of Applied 
Psychology, 80,721–730. 
Salas, E., Cooke, N. J., & Rosen, M. A. (2008). On Teams, Teamwork, and 
Team Performance: Discoveries and Developments. Human Factors: 
The Journal of the Human Factors and Ergonomics Society, 50(3), 540–
547. https://doi.org/10.1518/001872008x288457 
Salas, E., Sims, D. E., & Burke, C. S. (2005). Is there a “Big Five” in 
Teamwork? Small Group Research, 36(5), 555–599. 
https://doi.org/10.1177/1046496405277134 
Salas, E., Stagl, K. C., Burke, C. S., & Goodwin, G. F. (2007). Fostering team 
effectiveness in organizations. Toward an integrative theoretical 
framework of team performance. Modeling Complex Systems. Nebraska 
Symposium on Motivation, 6, 185–243. https://doi.org/10.1016/s1475-
9144(07)06004-3 
Schmidt, F. L., & Hunter, J. E. (1992). Development of a causal model of 
processes determining job performance. Current Directions in 
Psychological Science, 1,89–92. 
Sottilare, R. A., Shawn Burke, C., Salas, E., Sinatra, A. M., Johnston, J. H., & 
Gilbert, S. B. (2017). Designing Adaptive Instruction for Teams: a 
Meta-Analysis. International Journal of Artificial Intelligence in 
Education, 28(2), 225–264. https://doi.org/10.1007/s40593-017-0146-z 
Venezia, I. (1985). On the statistical origins of the learning curve. European 
Journal of Operational Research, 19(2), 191–200. 
https://doi.org/10.1016/0377-2217(85)90172-9 
Wiltshire, T. J., Butner, J. E., & Fiore, S. M. (2017). Problem-Solving Phase 
Transitions During Team Collaboration. Cognitive Science, 42(1), 129–
167. https://doi.org/10.1111/cogs.12482 
Walton, J., Gilbert, S., Winer, E., Dorneich, M., & Bonner, D. (2015). 
Evaluating Distributed Teams with the Team Multiple Errands Test. In 
Interservice/Industry Training, Simulation, and Education Conference 
(I/ITSEC) (pp. 1-12, Paper 15264).  
 
 
Copyright 2022 by Human Factors and Ergonomics Society. All rights reserved. 10.1177/1071181322661180
Proceedings of the 2022 HFES 66th International Annual Meeting
69

