HIVE: Harnessing Human Feedback for Instructional Visual Editing
Shu Zhang*1, Xinyi Yang*1, Yihao Feng*1, Can Qin3, Chia-Chih Chen1, Ning Yu1, Zeyuan Chen1,
Huan Wang1, Silvio Savarese1,2, Stefano Ermon2, Caiming Xiong1, Ran Xu1
1Salesforce AI Research, 2Stanford University, 3Northeastern University
Remove the red arch
Add a moon in the background
Add a sweater for the duck
Change the plant color to blue
Figure 1: We show four groups of representative results. In each triplet, from left to right are: the original image, HIVE
without human feedback, and HIVE with human feedback. We observe that HIVE leads to more acceptable results than the
model without human feedback. For instance, in the two examples on the left, HIVE without human feedback understands
the editing instruction “remove” and “change to blue” individually, but fails to understand the corresponding objects. Human
feedback resolves this ambiguity, as shown in other examples as well.
Abstract
Incorporating human feedback has been shown to be
crucial to align text generated by large language models
to human preferences. We hypothesize that state-of-the-art
instructional image editing models, where outputs are gen-
erated based on an input image and an editing instruction,
could similarly beneﬁt from human feedback, as their out-
puts may not adhere to the correct instructions and pref-
erences of users. In this paper, we present a novel frame-
work to harness human feedback for instructional visual
editing (HIVE). Speciﬁcally, we collect human feedback on
the edited images and learn a reward function to capture
the underlying user preferences. We then introduce scal-
able diffusion model ﬁne-tuning methods that can incorpo-
rate human preferences based on the estimated reward. Be-
sides, to mitigate the bias brought by the limitation of data,
we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset
to boost the performance of instructional image editing. We
conduct extensive empirical experiments quantitatively and
qualitatively, showing that HIVE is favored over previous
state-of-the-art instructional image editing approaches by
a large margin.
1. Introduction
State-of-the-art (SOTA) text-to-image generative models
have shown impressive performance in terms of both image
quality and alignment between output images and captions
[1, 45, 42]. Thanks to the impressive generation abilities
of these models, instructional image editing has emerged
as one of the most promising application scenarios for con-
tent generation [7]. Different from traditional image edit-
*Denotes equal contribution. Primary contact: shu.zhang@salesforce.com.
Our project page: https://shugerdou.github.io/hive/.
1
arXiv:2303.09618v1  [cs.CV]  16 Mar 2023

Train a reward model on the reward dataset
Let annotators rank outputs from best to worst
Collect a reward dataset and generate sampled 
outputs from step 1
Step 1: instructional supervised 
training
Step 2: collect comparison data, and train a 
reward model
(a)
(b)
(c)
(d)
Step 3: fine-tune diffusion model with 
learned rewards
Collect data to fine-tune GPT-3 and use 
fine-tuned GPT-3 to generate text edits
Fine-tuned GPT-3 inference input
Caption: penguins are playing
Fine-tuned GPT-3 inference output
Instruction: Change to summer
Edited caption: penguins are playing in the 
summer
(b)>(d)>(a)>(c)
Replace the oasis 
with a swimming 
pool.
Use Prompt-to-Prompt to generate 
paired images
Caption
Edited 
caption
Fine-tune stable diffusion with the 
paired images and instructions
Change to summer
(b)
(d)
(a)
(c)
BLIP based RM
Use the learned reward model to calculate 
reward values for each training pair
Convert reward value to text prompt and use it 
as a condition to fine-tune diffusion model
Position the helicopter 
above the sea.
1.3
Incorporate the Tokyo 
Tower.
0.5
(1.3, 0.5, …)
(5, 1, ...)
<original instruction> 
+The image quality is 
<reward label> out of five. 
rewards 
values
rewards 
labels
modified 
instruction
Position the helicopter above the sea. 
The image quality is five out of five.
fine-tune
Stable 
diffusion 
model
>
>
>
Figure 2: Overall architecture of HIVE. The ﬁrst step is to train a baseline HIVE without human feedback. In the second
step, we collect human feedback to rank variant outputs for each image-instruction pair, and train a reward model to learn the
rewards. In the third step, we ﬁne-tune diffusion models by integrating the estimated rewards.
ing [3, 16, 54, 30, 16, 54], where both the input and the
edited caption are needed, instructional image editing only
requires human-readable instructions. For instance, classic
image editing approaches require an input caption “a dog
is playing a ball”, and an edited caption “a cat is playing
a ball”. In contrast, instructional image editing only needs
editing instruction such as “change the dog to a cat”. This
experience mimics how humans naturally perform image
editing.
Instructional image editing was ﬁrst proposed in In-
structPix2Pix [7], which ﬁne-tunes a pre-trained stable dif-
fusion [45] by curating a triplet of the original image, in-
struction, and edited image, with the help of GPT-3 [8] and
Prompt-to-Prompt image editing [16]. Though achieving
promising results, the training data generation process of
InstructPix2Pix lacks explicit alignment between editing in-
structions and edited images. As a result, the edited images
may only partially align with the editing instructions (see
the second column in Fig. 4). Moreover, as the editing in-
structions are written by human users, it is imperative that
the resulting edited images align with the correct intentions
and preferences of the users. For example, humans tend
to make partial modiﬁcations to the original images, while
such human preferences are not incorporated into the train-
ing data or learning objectives of InstructPix2Pix. Based on
the aforementioned observation and the recent accomplish-
ments of ChatGPT [35], we propose to ﬁne-tune stable dif-
fusion using human feedback, such that the edited images
better align with human editing instructions.
For large language models (LLMs) such as InstructGPT
[35, 37], we often ﬁrst learn a reward function to reﬂect
what humans care about or prefer on the generated text out-
put, and then leverage reinforcement learning (RL) algo-
rithms such as proximal policy optimization (PPO) [49] to
ﬁne-tune the models. This process is often referred to as re-
inforcement learning with human feedback (RLHF). Lever-
aging RLHF to ﬁne-tune diffusion-based generative mod-
els, however, remains challenging. Applying on-policy al-
gorithms (e.g.,PPO) to maximize rewards during the ﬁne-
tuning process can be prohibitively expensive due to the
hundreds or thousands of denoising steps required for each
sampled image. Moreover, even with fast sampling meth-
ods [51, 55, 21, 31], it is still challenging to back-propagate
the gradient signal to the parameters of the U-Net. 1
To address the technical issues described above, we pro-
pose Harnessing Human Feedback for Instructional Visual
Editing (HIVE), which allows us to ﬁne-tune diffusion-
based generative models with human feedback. As shown
1We present a rigorous discussion on the difﬁculty in Appendix C.1.
2

in Fig. 2, HIVE consists of three steps:
1) We perform instructional supervised ﬁne-tuning on
the dataset that combines our newly collected 1M training
data and the data from InstructPix2Pix. Since observing
failure cases and suspecting the grounding visual compo-
nents from image to instruction is still a challenging prob-
lem, we collect the 1M training data.
2) For each input image and editing instruction pair, we
ask human annotators to rank variant outputs of the ﬁne-
tuned model from step 1, which gives us a reward learning
dataset. Using the collected dataset, we then train a reward
model (RM) that reﬂects human preferences.
3) We estimate the reward for each training data used in
step 1, and integrate the reward to perform human feedback
diffusion model ﬁnetuning using our proposed objectives
presented in Sec. 3.4.
Our main contribution can be summarized as follows:
●To tackle the technical challenge of ﬁne-tuning diffu-
sion models using human feedback, we introduce two scal-
able ﬁne-tuning approaches in Sec. 3.4, which are computa-
tionally efﬁcient and offer similar costs compared with su-
pervised ﬁne-tuning. Moreover, we empirically show that
human feedback is an essential component to boost the per-
formance of instructional image editing models.
●
To explore the fundamental ability of instructional
editing, we create a new dataset for HIVE that includes
three sub-datasets: a new 1M training dataset, a 3.6K re-
ward dataset for rewards learning, and a 1K evaluation
dataset.
●To increase the diversity of the data for training, we
introduce cycle consistency augmentation based on the in-
version of editing instruction. Our dataset has been enriched
with one pair of data for bi-directional editing.
2. Related Work
Text-To-Image Generation.
Text-to-image genera-
tive models have achieved tremendous success in the past
decade.
Generative adversarial nets (GANs) [15] is one
of the fundamental methods that dominated the early-stage
works [44, 58, 56]. Recently, diffusion models [50, 17, 52,
51] have achieved state-of-the-art text-to-image generation
performance. [12, 34, 43, 42, 46, 57, 45, 29]. As a result,
instead of training a text-to-image model from scratch, our
work focuses on ﬁne-tuning existing stable diffusion model
[45], by leveraging additional human feedback.
Image Editing.
Similarly, diffusion models based image
editing methods, e.g. SDEdit [33], BlendedDiffusion [3],
BlendedLatentDiffusion [2], DiffusionClip [22], EDICT
[54] or MagicMix [30], have garnered signiﬁcant attention
in recent years. To leverage a pre-trained image-text rep-
resentation (e.g., CLIP [41], BLIP [28]) and text-to-image
diffusion based pre-trained models [42, 46, 45], most exist-
ing works focus on text-based localized editing [5, 32, 16].
Prompt-to-Prompt [16] edits the cross-attention layer in Im-
agen and stable diffusion to control the similarity of image
and text prompt. More recently, InstructPix2Pix [7] tackled
the problem via a different approach, requiring only human-
readable editing instruction to perform image editing. Our
work follows the same direction as InstructPix2Pix[7] and
leverages human feedback to address the misalignment be-
tween editing instructions and resulting edited images.
Learning with Human Feedback.
Incorporating human
feedback into the learning process can be a highly effec-
tive way to enhance performance across various tasks such
as ﬁne-tuning LLMs [35, 4, 47, 37, 53], robotic simula-
tion [10, 18], computer vision [40], and to name a few.
Many existing works leverage PPO [49] to align to human
feedback, however on-policy RL algorithms are not suitable
for diffusion-based model ﬁne-tuning (See more discussion
in Appendix C.1). A concurrent work [25] also leverages
human feedback to align text-to-image generation, where
they intuitively consider reward as weights for negative log-
likelihood. Our work tackles the problem of instructional
image editing, where there are no ground truth data for
the alignment between human-readable editing instructions
and edited images, making human feedback extremely valu-
able.
3. Methodology
In this section, we introduce the new datasets we col-
lected in Sec. 3.1, and explain the three major steps of HIVE
in the rest of the section. Concretely, we introduce the in-
structional supervised training in Sec. 3.2, and describe how
to train a reward model to score edited images in Sec. 3.3,
then present two scalable ﬁne-tuning methods to align dif-
fusion models with human feedback in Sec. 3.4.
3.1. Dataset
Instructional Edit Training Dataset. We follow the same
method of [7] to generate the training dataset.
We col-
lect 1K images and their corresponding captions. We ask
three annotators to write three instructions and correspond-
ing edited captions based on the collected input captions.
Therefore, we obtain 9K prompt triplets: input caption, in-
struction, and edited caption. We ﬁne-tune GPT-3 [8] with
OpenAI API v0.25.0 [36] with them. We use the ﬁne-tuned
GPT-3 to generate ﬁve instructions and edited captions per
input image-caption pair in Laion-Aesthetics V2 [48]. We
observe that the captions from Laion are not always visu-
ally descriptive, so we use BLIP [28] to generate more di-
verse types of image captions. Later stable diffusion based
Prompt-to-Prompt [16] is adopted to generate paired im-
ages. In addition, we design a cycle-consistent augmenta-
tion method (Sec. 3.2.1) to generate additional training data.
We generate 1.04M training triplets in total. Combining the
281K training data from [7], we obtain 1.32M training im-
age pairs along with instructions.
3

Reward Fine-tuning Dataset.
We collect 3.6K image-
instruction pairs for the task of reward ﬁne-tuning. Among
them, 1.6K image-instruction pairs are manually collected,
and the rest are from Laion-Aesthetics V2 with GPT-3 gen-
erated instructions. We use this dataset to ask annotators to
rank various model outputs.
Evaluation Dataset.
We use two evaluation datasets: the
test dataset in [7] for quantitative evaluation and a new 1K
dataset collected for the user study. The quantitative eval-
uation dataset is generated following the same method as
the training dataset, which means that the dataset does not
contain real images. Our collected 1K dataset contains 200
real images, and each image is annotated with ﬁve human-
written instructions.
More details of annotation tooling,
guidelines, and analysis are in Appendix A.
3.2. Instructional Supervised Training
We follow the instructional ﬁne-tuning method in [7]
with two major upgrades on dataset curation (Sec. 3.1) and
cycle consistency augmentation (Sec. 3.2.1). A pre-trained
stable diffusion model [45] is adopted as the backbone ar-
chitecture. In instructional supervised training, the stable
diffusion model has two conditions c = [cI,cE], where cE
is the editing instruction, and cI is the latent space of the
original input image. In the training process, a pre-trained
auto-encoder [23] with encoder E and decoder D is used to
convert between edited image ˜xxx and its latent representation
z = E(˜xxx). The diffusion process is composed of an equally
weighted sequence of denoising autoencoders ϵθ(zt,t,c),
t = 1,⋯,T, which are trained to predict a denoised vari-
ant of their input zt, a noisy version of z. The objective of
instructional supervised training is:
L = EE(˜xxx),c,ϵ∼N (0,1),t[∥ϵ −ϵθ(zt,t,c)∥2
2].
3.2.1
Cycle Consistency Augmentation
Cycle consistency is a powerful technique that has been
widely applied in image-to-image generation [59, 19]. It
involves coupling and inverting bi-directional mappings of
two variables X and Y , G ∶X →Y and F ∶Y →X, such
that F(G(X)) ≈X and vice versa. This approach has been
shown to enhance generative mapping in both directions.
While Instructpix2pix [7] considers instructional image
editing as a single-direction mapping, we propose adding
cycle consistency. Our approach involves a forward-pass
editing step, F ∶x
inst
Ð→˜xxx. We then introduce instruction
reversion to enable a reverse-pass mapping, R ∶˜xxx
∼inst
Ð→x.
In this way, we could close the loop of image editing as:
x
inst
Ð→˜xxx
∼inst
Ð→x, e.g. “add a dog” to “remove the dog”.
To ensure the effectiveness of this technique, we need
to separate invertible and non-invertible instructions from
the dataset. We devised a rule-based method that combines
speech tagging and template matching. We found that most
instructions adhere to a particular structure, with the verb
appearing at the start, followed by objects and prepositions.
Thus, we grammatically tagged all instructions using the
Natural Language Toolkit (NLTK) 2. We identiﬁed all in-
vertible verbs and pairing verbs, and also analyzed the se-
mantics of the objects and the prepositions used. By sum-
marizing invertible instructions in predeﬁned templates, we
matched desired instructions.
Our analysis revealed that
29.1% of the instructions in the dataset were invertible. We
augmented this data to create more comprehensive training
data, which facilitated cycle consistency. For more infor-
mation, see Appendix B.1.
3.3. Human Feedback Reward Learning
The second step of HIVE is to learn a reward function
Rφ(˜xxx,c), which takes the original input image, the text in-
struction condition c = [cI,cE], and the edited image ˜xxx that
is generated by the ﬁne-tuned stable diffusion as input, and
outputs a scalar that reﬂects human preference.
Unlike InstructGPT which only takes text as input, our
reward model Rφ(˜xxx,c) needs to measure the alignment be-
tween instructions and the edited images. To address the
challenge, we present a reward model architecture in Fig. 3,
which leverages pre-trained vision-language models such as
BLIP [28]. More speciﬁcally, the reward model employs an
image-grounded text encoder as the multi-modal encoder to
take the joint image embedding and the text instruction as
input and produce a multi-modal embedding. A linear layer
is then applied to the multi-modal embedding to map it to a
scalar value. More details are in Appendix B.2.
With the speciﬁcally designed network architecture, we
train the reward function Rφ(˜xxx,c) with our collected re-
ward ﬁne-tuning dataset Dhuman induced in Sec. 3.1. For
each input image cI and instruction cE pair, we have K
edited images {˜xxx}K
k=1 ranked by human annotators, and de-
note the human preference of edited image ˜xxxi over ˜xxxj by
˜xxxi ≻˜xxxj. Then we can follow the Bradley-Terry model of
preferences [6, 37] to deﬁne the pairwise loss function:
ℓRM(φ) ∶= −∑˜xxxi≻˜xxxj log [
exp(Rφ(˜xxxi,c))
∑k=i,j exp(Rφ(˜xxxk,c))] ,
where (i,j) ∈[1...K] and we can get (K
2 ) pairs of com-
parison for each condition c. Similar to [37], we put all
the (K
2 ) pairs for each condition c in a single batch to learn
the reward functions. We provide a detailed reward model
training discussion in Appendix B.2.
3.4. Human Feedback based Model Fine-tuning
With the learned reward function Rφ(c, ˜xxx), the next step
is to improve the instructional supervised training model by
2https://www.nltk.org/
4

Feedforward
Cross Attention
Self Attention
Feedforward
Self Attention
“Replace apples with oranges.”
Share Parameters
Joint image embedding
Concat
Feedforward
Self Attention
Linear Layer
Score
N *
N *
Image 
Encoder
Multimodal 
Encoder
Input image
Edited image
Figure 3: Model architecture for reward R(˜xxx,c). Here the
reward model evaluates human preference for an edited im-
age of a hand selecting an orange compared to the original
input image of the hand selecting an apple. The input to the
reward model includes both images and a text instruction.
The output is a score indicating the degree of preference for
the edited image based on the input image and instruction.
reward maximization. As a result, we can obtain an instruc-
tional diffusion model that aligns with human preferences.
To address the difﬁculty of sampling-based methods as
we discussed previously (also in Appendix C.1), we adapt
ofﬂine RL techniques [27, 38, 9, 20] to ﬁne-tune diffusion
models, which allows us to align diffusion models with hu-
man feedback under acceptable training time and cost.
The RL ﬁne-tuning techniques we present can be utilized
for latent and pixel-based diffusion models. For simplicity,
we introduce our methods for pixel-based generative diffu-
sion models. With an input image and editing instruction
condition c = [cI,cE], we deﬁne the edited image data dis-
tribution generated by the instructional supervised diffusion
model as p(˜xxx∣c), and the edited image data distribution gen-
erated by the current diffusion model we want to optimize
as ρ(˜xxx∣c) , then under the pessimistic principle of ofﬂine
RL, we can optimize ρ by the following objectives:
J(ρ) ∶= maxρ Ec[E˜xxx∼ρ(⋅∣c)[Rφ(˜xxx,c)]−
ηKL(ρ(˜xxx∣c)∣∣p(˜xxx∣c))],
(1)
where η is a hyper-parameter. The ﬁrst term in Eq. (1) is the
standard reward maximization in RL, and the second term
is a regularization to stabilize learning, which is a widely
used technique in ofﬂine RL [24], and also adopted for PPO
ﬁne-tuning of InstructGPT (a.k.a “PPO-ptx”) [37].
To avoid using sampling-based methods to optimize ρ,
we can differentiate J(ρ) w.r.t ρ(˜xxx∣c) and solve for the op-
timal ρ∗(˜xxx∣c), resulting the following expression for the op-
timal solution of Eq. (1):
ρ∗(˜xxx∣c) ∝p(˜xxx∣c)exp(Rφ(˜xxx,c)/η) ,
(2)
or ρ∗(˜xxx∣c) =
1
Z(c)p(˜xxx∣c)exp(Rφ(˜xxx,c)/η), with Z(c) =
∫p(˜xxx∣c)exp(Rφ(˜xxx,c)/η)d˜xxx being the partition function.
A detailed derivation is in Appendix C.2.
Weighted Reward Loss.
The optimal target distribution
ρ∗(˜xxx∣c) in Eq. (2) can be viewed as an exponential reward-
weighted distribution for p(˜xxx∣c). Moreover, we have al-
ready obtained the empirical edited image data drawn from
p(˜xxx∣c) when constructing the instructional editing dataset,
and we can view the exponential reward weighted edited
image ˜xxx from the instructional editing dataset as an empir-
ical approximation of samples drawn from ρ∗(˜xxx∣c). For-
mally, we can ﬁne-tune a diffusion model thus it generates
data from ρ∗(˜xxx∣c), resulting in the weighted reward loss:
ℓWR(θ) ∶= EE(˜xxx),c,ϵ∼N (0,1),t [ω(˜xxx,c) ⋅∥ϵ −ϵθ(zt,t,c)∥2
2] ,
with ω(˜xxx,c) = exp(Rφ(˜xxx,c)/η) being the exponential re-
ward weight for edited image ˜xxx and condition c. Different
from RL literature [39, 38] using exponential reward or ad-
vantage weights to learn a policy function, our weighted
reward loss is derived for ﬁne-tuning stable diffusion.
Condition Reward Loss.
We can also leverage the
control-as-inference perspective of RL [26] to transform
Eq. (2) to a conditional reward expression, thus we can di-
rectly view the reward as a conditional label to ﬁne-tune dif-
fusion models. Similar to [26], we introduce a new binary
variable R∗indicating whether human prefers the edited
image or not, where R∗= 1 denotes that human prefers
the edited image, and R∗= 0 denotes that human does not
prefer, thus we have p(R∗= 1 ∣˜xxx,c) ∝exp(Rφ(˜xxx,c)).
Together with Eq. (2), and applying Bayes rules gives us
the following derivation:
p(˜xxx∣c)exp(Rφ(˜xxx,c)/η) ∶= q(˜xxx∣c)(p(R∗= 1 ∣˜xxx,c))1/η
= p(˜xxx∣c)(p(˜xxx∣R∗= 1,c)p(R∗= 1∣c)
p(˜xxx∣c)
)
1/η
∝p(˜xxx∣c)1−1/ηp(˜xxx∣R∗= 1,c)1/η ,
where we drop p(R∗= 1∣c) since it is a constant w.r.t ˜xxx. We
can now view the reward for each edited image as an addi-
tional condition. Deﬁne the new condition ˜c = [cI,cE,cR],
with cR as the reward label, we can ﬁne-tune the diffusion
model with the condition reward loss:
ℓCR(θ) = EE(x),˜c,ϵ∼N (0,1),t[∥ϵ −ϵθ(zt,t, ˜c)∥2
2].
We quantize the reward into ﬁve categories, based on the
quantile of the empirical reward distribution of the training
dataset, and convert the reward value into a text prompt.
For instance, if the reward value of a training pair lies in
the bottom 20% of the reward distribution of the dataset,
then we convert the reward value as a text prompt condition
cR ∶=“The image quality is one out of ﬁve”. And during
the inference time to generate edited images, we ﬁx the text
prompt as cR ∶=“The image quality is ﬁve out of ﬁve”, indi-
cating we want the generated edited images with the high-
est reward. We empirically ﬁnd this technique improves the
stability of ﬁne-tuning.
5

Input
InstructPix2Pix
HIVE w/o feedback
HIVE  w/ feedback
“Make the cat smile”
“Remove the grass”
“Change the arch to a door”
“Add a beer”
Figure 4: Comparisons between InstructPix2Pix, HIVE without human feedback and HIVE with human feedback. HIVE
with human feedback can boost performance by understanding the instruction correctly.
4. Experiments
This section presents the experimental results and ab-
lation studies of HIVE’s technical choices, demonstrating
the effectiveness of our method.
For a fair comparison,
we adopt the default guidance scale parameters in Instr-
cutPix2Pix. Through our experiments, we discovered that
the conditional reward loss performs slightly better than the
weighted reward loss, and therefore, we present our results
based on the conditional reward loss. The detailed compar-
isons can be found in Sec. 4.2 and Appendix D.
We evaluate our method using two datasets:
a syn-
thetic evaluation dataset with 15,652 image pairs from [7]
and a self-collected 1K evaluation dataset with real image-
instruction pairs. For the synthetic dataset, we follow In-
structPix2Pix’s quantitative evaluation metric and plot the
trade-offs between CLIP image similarity and directional
CLIP similarity[14]. For the 1K dataset, we conduct a user
study where for each instruction, the images generated by
competing methods are reviewed and voted by three human
annotators, and the winner is determined by majority votes.
4.1. Baseline Comparisons
We perform experiments with the same setup as Instruct-
Pix2Pix, where stable diffusion (SD) v1.5 is adopted. We
compare three models: InstructPix2Pix, HIVE without hu-
man feedback, and HIVE with human feedback. We report
the quantitative results on the synthetic evaluation dataset in
Fig. 5. We observe that HIVE without human feedback im-
proves notably over InstructPix2Pix (blue curve vs. green
curve). Moreover, human feedback further boosts the per-
6

0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
0.22
CLIP Text-Image Direction Similarity
0.70
0.75
0.80
0.85
0.90
0.95
CLIP Image Similarity
InstructPix2Pix
HIVE w/o human feedback
HIVE w/ human feedback
Figure 5:
Comparisons between InstructPix2Pix, HIVE
without human feedback, and HIVE with human feedback.
It plots tradeoffs between consistency with the input image
and consistency with the edit. The higher the better. For all
methods, we adopt the same parameters as that in [7].
Tie
InstructPix2Pix HIVE w/o HF
0
10
20
30
40
50
60
17.7%
26.2%
56.1%
(a) InstructPix2Pix vs HIVE w/o
feedback
Tie
HIVE w/o HF
HIVE w HF
10
20
30
40
30.2%
29.5%
40.3%
(b) HIVE w/o vs w/ feedback
Figure 6: User study of comparison between (a) Instruct-
Pix2Pix vs HIVE without human feedback and (b) HIVE
without and with human feedback. HIVE without human
feedback obtains 30% more votes than InstructPix2Pix.
HIVE with human feedback obtains 10.8% more votes than
that without human feedback.
formance of HIVE (red curve vs blue curve) by a large
margin. In other words, with the same directional similar-
ity value, HIVE with human feedback obtains better image
consistency than that without feedback.
To test the effectiveness of HIVE on real-world images,
we report the user study results on the 1K evaluation dataset.
As shown in Fig. 6(a), HIVE without human feedback gets
around 30% more votes than the InstructPix2Pix. The result
is consistent with the user study on the synthetic dataset.
We also demonstrate the user study outcome between HIVE
without and with human feedback in Fig. 6(b). The user
study indicates similar conclusions to the consistency plot,
where the model with human feedback gets 10.8% more
favorites than the model without human feedback.
In Fig. 4, we present representative edits that demon-
strate the effectiveness of HIVE. The results show that while
HIVE can partially learn editing instructions without human
feedback, the reward model leads to better alignment be-
tween instruction and the edited image. For example, in the
third row, HIVE without human feedback generates a door-
like object, but with the guidance of human feedback, the
generated door matches human perception better.
“Add a pond”
“Change the floor into grass”
Input
InstructPix2Pix
HIVE w/o feedback
HIVE w/ feedback
Figure 7: Human feedback tends to help HIVE avoid un-
wanted excessive image modiﬁcations.
InstructPix2Pix
Tie
HIVE w/o HF HIVE w HF
18
20
22
24
26
28
30
28.5%
20.2%
21.6%
29.7%
Figure 8: Image quality comparisons between Instruct-
Pix2Pix, HIVE without human feedback, and HIVE with
human feedback. HIVE with HF obtains the most votes
from annotators.
We conducted another user study to assess the image
quality of the edited images and found that HIVE with
human feedback received the highest number of votes (as
shown in Fig. 8). For example, in the last row of Fig. 4,
the beer glass generated by InstructPix2Pix and HIVE with-
out human feedback appears misleading or unﬁnished to
human reviewers. We think that the reason is annotators’
preferences are diverse, which leads to consistent and ro-
bust evaluation. Therefore, the image quality beneﬁts from
it. Additionally, our visual analysis of the results (Fig. 7)
indicates that the HIVE model with human feedback tends
to preserve the remaining part of the original image that is
not instructed to be edited, while the models without human
feedback lead to excessive image editing more often. For
instance, in the ﬁrst example of Fig. 7, HIVE with human
feedback blends a pond naturally into the original image.
While InstructPix2Pix and HIVE without human feedback
fulﬁll the same instruction, but at the same time, alter the
uninstructed part of the original background.
4.2. Ablation Study
Weighted Reward and Condition Reward Loss.
We
perform user study on HIVE with these two losses individ-
ually. As shown in Fig. 10, these two losses obtain very
similar human preferences on the evaluation dataset. More
comparisons are in Appendix D.
SD v1.5 and v2.1.
Considering the recent progress of
stable diffusion, we upgrade the backbone of stable diffu-
sion from v1.5 to the latest version v2.1, where OpenCLIP
text encoder [48] replaces the CLIP text encoder [41], and
7

0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
0.22
CLIP Text-Image Direction Similarity
0.70
0.75
0.80
0.85
0.90
CLIP Image Similarity
InstructPix2Pix SD v1.5
InstructPix2Pix SD v2.1
Figure 9: InstructPix2Pix with SD v1.5 and v2.1.
The
higher the better.
Tie
HIVE w/o HF
HIVE w HF
10
20
30
40
30%
30.1%
39.9%
(a) HIVE with weighted reward loss
Tie
HIVE w/o HF
HIVE w HF
10
20
30
40
30.2%
29.5%
40.3%
(b) HIVE with condition reward loss
Figure 10: User study of pairwise comparison between (a)
HIVE with weighted reward loss and (b) HIVE with condi-
tion reward loss. The human preferences are very close to
each other.
expect to see the backbone upgrading beneﬁts. The quan-
titative consistency plot in Fig. 11 on the synthetic evalu-
ation dataset conﬁrms our assumption and shows that SD
v2.1 improves performance over SD v1.5 by a small mar-
gin. Our user study in Fig. 12 using human feedback in-
dicates a similar conclusion. We compare InstructPix2Pix
v1.5 with v2.1 as well. An interesting observation is that we
train InstructPix2Pix with SD v2.1 and show in Fig. 9 that
its improvement over SD v1.5 is larger than HIVE.
Failure Cases. We summarize representative failure cases
in Fig. 13. First, some instructions cannot be understood.
In the upper left example in Fig. 13, the prompt “zoom in”
or similar instructions can rarely be successful. We believe
the root cause is current training data generation method
fails to generate image pairs with this type of instruction.
Second, counting and spatial reasoning are common failure
cases (see the upper right example in Fig. 13). We ﬁnd
that the instruction “one”, “two”, or “on the right” can lead
to many undesired results. Third, the object understanding
sometimes is wrong. In the bottom left example, the red
color is changed on the wrong object. This is a common
error. Other ablation studies can be found in Appendix D.
5. Conclusion and Discussion
In our paper, we introduce a novel framework called
HIVE that enables instructional image editing with hu-
0.08
0.10
0.12
0.14
0.16
0.18
0.20
CLIP Text-Image Direction Similarity
0.80
0.85
0.90
0.95
CLIP Image Similarity
HIVE SD v1.5
HIVE SD v2.1
Figure 11: Comparisons between HIVE (no human feed-
back) with SD v1.5 and v2.1. The higher the better.
Tie
HIVE SD-v1.5 HIVE SD-v2.1
10
20
30
40
29.9%
31.9%
38.2%
Figure 12: User study between HIVE with SD v1.5 and v2.1
with human feedback. SD v2.1 obtains more votes.
“Zoom in on the boats”
“Change the plant color to red”
“Add a sun in the corner”
“Add food on the table”
Figure 13: Failure examples.
man feedback. Our framework integrates human feedback,
which is quantiﬁed as reward values, into the diffusion
model ﬁne-tuning process. We design two variants of the
approach and both of them improve performance over pre-
vious state-of-the-art instructional image editing methods.
Our work demonstrates instructional image editing with hu-
man feedback is a variable approach to align image gener-
ation with human preference, thus unlocking new opportu-
nities and potential to scale up the model capabilities to-
wards more powerful applications such as conversational
image editing. While our method demonstrates impressive
performance, we have also identiﬁed failure scenarios, as
discussed in Sec. 4.2. In addition, it is possible that our
trained model inherits bias and suffers from harmful con-
tent from pre-trained foundation models such as Stable Dif-
fusion, GPT3 and BLIP. These limitations should be taken
into consideration when interpreting our results, and we ex-
pect red teaming with human feedback to mitigate some of
the risks in future work.
8

References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katie Millican, Malcolm Reynolds, et al. Flamingo: a vi-
sual language model for few-shot learning. arXiv preprint
arXiv:2204.14198, 2022. 1
[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. arXiv preprint arXiv:2206.02779, 2022. 3
[3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 18208–18218, 2022. 2, 3
[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,
et al. Training a helpful and harmless assistant with rein-
forcement learning from human feedback.
arXiv preprint
arXiv:2204.05862, 2022. 3
[5] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-
ten, and Tali Dekel. Text2live: Text-driven layered image
and video editing.
In European Conference on Computer
Vision, pages 707–723. Springer, 2022. 3
[6] Ralph Allan Bradley and Milton E Terry. Rank analysis of
incomplete block designs: I. the method of paired compar-
isons. Biometrika, 39(3/4):324–345, 1952. 4
[7] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
arXiv preprint arXiv:2211.09800, 2022. 1, 2, 3, 4, 6, 7
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilyva Sutskever, and Dario Amodei. Language mod-
els are few-shot learners. arXiv preprint arXiv:2005.14165,
2020. 2, 3
[9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-
vas, and Igor Mordatch. Decision transformer: Reinforce-
ment learning via sequence modeling. Advances in neural
information processing systems, 34:15084–15097, 2021. 5
[10] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic,
Shane Legg, and Dario Amodei. Deep reinforcement learn-
ing from human preferences. NeurIPS, 2017. 3
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.
Bert:
Pre-training of deep bidirectional
transformers for language understanding.
arXiv preprint
arXiv:1810.04805, 2018. 13
[12] Prafulla Dhariwal and Alexander Nichol.
Diffusion mod-
els beat gans on image synthesis. NeurIPS, 34:8780–8794,
2021. 3
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929, 2020. 13
[14] Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal
Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided
domain adaptation of image generators. ACM Transactions
on Graphics, 41(4):1–13, 2022. 6
[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014.
3
[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control.
arXiv preprint
arXiv:2208.01626, 2022. 2, 3
[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. NeurIPS, 33:6840–6851, 2020.
3
[18] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving,
Shane Legg, and Dario Amodei. Reward learning from hu-
man preferences and demonstrations in atari. NeurIPS, 2018.
3
[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In CVPR, 2017. 4
[20] Michael Janner, Qiyang Li, and Sergey Levine. Ofﬂine re-
inforcement learning as one big sequence modeling prob-
lem. In Advances in Neural Information Processing Systems,
2021. 5
[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models, 2022. URL https://arxiv. org/abs/2206.00364, 2022.
2
[22] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages
2426–2435, June 2022. 3
[23] Diederik Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114, 2013. 4
[24] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey
Levine.
Conservative q-learning for ofﬂine reinforcement
learning. Advances in Neural Information Processing Sys-
tems, 33:1179–1191, 2020. 5
[25] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,
Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, and Shixiang Shane Gu.
Aligning text-
to-image models using human feedback.
arXiv preprint
arXiv:2302.12192, 2023. 3
[26] Sergey Levine. Reinforcement learning and control as prob-
abilistic inference:
Tutorial and review.
arXiv preprint
arXiv:1805.00909, 2018. 5
[27] Sergey Levine, Aviral Kumar, George Tucker, and Justin
Fu.
Ofﬂine reinforcement learning:
Tutorial, review,
and perspectives on open problems.
arXiv preprint
arXiv:2005.01643, 2020. 5
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
9

ﬁed vision-language understanding and generation.
arXiv
preprint arXiv:2201.12086, 2022. 3, 4, 13
[29] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu,
Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae
Lee. Gligen: Open-set grounded text-to-image generation.
arXiv:2301.07093, 2023. 3
[30] Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng.
Magicmix: Semantic mixing with diffusion models. arXiv
preprint arXiv:2210.16056, 2022. 2, 3
[31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-
sion probabilistic model sampling in around 10 steps. arXiv
preprint arXiv:2206.00927, 2022. 2
[32] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided
image synthesis and editing with stochastic differential equa-
tions. In International Conference on Learning Representa-
tions, 2022. 3
[33] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and
editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073, 2021. 3
[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021. 3
[35] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/. 2, 3
[36] OpenAI.
Openaiapi.
https://platform.openai.
com/docs/guides/fine-tuning. 3
[37] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob
Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan
Lowe. Training language models to follow instructions with
human feedback. arXiv preprint arXiv:2203.02155, 2022. 2,
3, 4, 5
[38] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey
Levine. Advantage-weighted regression: Simple and scal-
able off-policy reinforcement learning.
arXiv preprint
arXiv:1910.00177, 2019. 5
[39] Jan Peters, Katharina Mulling, and Yasemin Altun. Relative
entropy policy search. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, volume 24, pages 1607–1612,
2010. 5
[40] Andre Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas
Beyer, and Xiaohua Zhai. Tuning computer vision models
with task rewards. arXiv preprint arXiv:2302.08242, 2023.
3
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. arXiv preprint arXiv:2103.00020, 2021. 3, 7
[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022. 1, 3
[43] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In ICML, pages 8821–
8831. PMLR, 2021. 3
[44] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text-to-image synthesis. In ICML, 2016. 3
[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, pages 10684–
10695, 2022. 1, 2, 3, 4
[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour,
Burcu Karagol Ayan,
S Sara Mahdavi,
Rapha Gontijo Lopes, et al.
Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022. 3
[47] Jeremy Scheurer, Jon Ander Campos, Jun Shern Chan, An-
gelica Chen, Kyunghyun Cho, and Ethan Perez. Training
language models with language feedback.
arXiv preprint
arXiv:2204.14146, 2022. 3
[48] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-5b: An
open large-scale dataset for training next generation image-
text models. arXiv preprint arXiv:2111.02114, 2021. 3, 7
[49] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347, 2017. 2, 3
[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In Proceedings of Interna-
tional Conference on Machine Learning, pages 2256–2265,
2015. 3
[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. arXiv:2010.02502, October
2020. 2, 3
[52] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in Neural
Information Processing Systems, 32, 2019. 3
[53] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel Ziegler,
Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and
Paul Christiano. Learning to summarize from human feed-
back. NeurIPS, 2020. 3
[54] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Ex-
act diffusion inversion via coupled transformations. arXiv
preprint arXiv:2211.12446, 2022. 2, 3
[55] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat.
Tack-
ling the generative learning trilemma with denoising diffu-
sion gans. arXiv preprint arXiv:2112.07804, 2021. 2
[56] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In CVPR, 2018. 3
10

[57] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al.
Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789, 2022. 3
[58] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. In ICCV, 2017. 3
[59] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros.
Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017. 4
11

Appendix
A. Data Collection and User Study
In the evaluation steps, we collect real-world images with instructions using Amazon Mechanical Turk (Mturk) 3. We
randomly collect 200 real-world images. Then we ask Mturk annotators to write ﬁve instructions for each image, and
encourage them to have wild imaginations and diversify the instruction types. We encourage annotators to not be limited
to making the image realistic. For example, annotators can write “add a horse in the sky”. A screenshot of the interface is
illustrated in Fig. 14. We analyze the top ﬁve verbs and nouns in the evaluation dataset. It is shown in Fig. 16(a) that the verbs
“add”, “change”, “make”, “remove” and “put” make up around 85% of all verbs, which means that the editing instruction
verbs have a long-tail distribution. In contrast, the distribution of nouns in Fig. 16(b) is close to uniform, where the top ﬁve
nouns represent only around 20% of all nouns.
In user studies, we use Mturk to ask annotators to evaluate edited images. A screenshot of the interface is shown in Fig. 15.
The annotators are provided with the original image, two edited images, and the editing instruction. They are asked to select
the better edited image. The third option indicates that the edited images are equally good or equally bad. We ask three
annotators to label one data sample, and use the majority votes to determine the results. We shufﬂe the edited images to avoid
choosing the left image over the right and vice versa.
B. Implementation Details
B.1. Instructional Supervised Training
We use pre-trained stable diffusion models as the initial checkpoint to start instructional supervised training. We train
HIVE on 40GB NVIDIA A100 GPUs for 500 epochs. We use the learning rate of 10−4 and the image size of 256. In the
inference, we use 512 as the default image resolution.
3https://www.mturk.com
Figure 14: Mturk writing editing instructions interface: write ﬁve instructions per image.
Figure 15: Mturk labeling interface: select the better edited image.
12

add
change
make
remove
put
others
0
5
10
15
20
25
30
35
36.4%
19.8%
15.4%
9.7%
3.5%
15.2%
(a) Top ﬁve verbs
color
sky
trees
cat
water
0
2
4
6
8
9.5%
3.2%
2.7%
2.5%
2.5%
(b) Top ﬁve nouns
Figure 16: Top ﬁve verbs and nouns in the evaluation dataset.
B.2. Human Feedback Rewards Learning
As shown in Fig. 3, the reward model takes in an input image cI, a text instruction cE, and an edited image ˜x and outputs a
scalar value. Inspired by the recent work on the vision-language model, especially BLIP [28], we employ a visual transformer
[13] as our image encoder and an image-grounded text encoder as the multimodal encoder for images and text. Finally, we
set a linear layer on top of the image-grounded text encoder to map the multimodal embedding to a scalar value.
(1) Visual transformer. We encode both the input image cI and edited image ˜x with the same visual transformer. Then
we obtain the joint image embedding by concatenating the two image embeddings vit(cI), vit(˜x).
(2) Image-grounded text encoder. The image-grounded text encoder is a multimodal encoder that inserts one additional
cross-attention layer between the self-attention layer and the feed-forward network for each transformer block of BERT
[11]. The additional cross-attention layer incorporates visual information into the text model. The output embedding of the
image-grounded text encoder is used as the multimodal representation of the (cI, cE, ˜x) triplet.
Train a reward model on the reward dataset
Let annotators rank outputs from best to worst
Collect a reward dataset and generate sampled 
outputs from step 1
Step 1: instructional supervised 
training
Step 2: collect comparison data, and train a 
reward model
(a)
(b)
(c)
(d)
Step 3: fine-tune diffusion model with 
learned rewards
Collect data to fine-tune GPT-3 and use 
fine-tuned GPT-3 to generate text edits
Fine-tuned GPT-3 inference input
Caption: penguins are playing
Fine-tuned GPT-3 inference output
Instruction: Change to summer
Edited caption: penguins are playing in the 
summer
(b)>(d)>(a)>(c)
Replace the oasis 
with a swimming 
pool.
Use Prompt-to-Prompt to generate 
paired images
Caption
Edited 
caption
Fine-tune stable diffusion with the 
paired images and instructions
Change to summer
(b)
(d)
(a)
(c)
BLIP based RM
Use the learned reward model to calculate 
reward values for each training pair
Use weighted reward loss to fine-tune the 
diffusion model
Position the helicopter 
above the sea.
1.3
Incorporate the Tokyo 
Tower.
0.5
(1.3, 0.5, …)
rewards 
values
exponential rewards 
as weights
Position the helicopter above 
the sea. 
fine-tune
Stable 
diffusion 
model
>
>
>
Figure 17: Overall architecture of HIVE. Different from Fig. 2, in the third step, we use weighted reward loss instead of
condition reward loss to ﬁne-tune the diffusion model.
13

We gather a dataset comprising 3,634 images for the purpose of ranking. For each image, we generate ﬁve variant edited
images, and ask an annotator to rank images from best to worst. Additionally, we ask annotators to indicate if any of the
following scenarios apply: (1) all edited images are edited but none of them follow the instruction; (2) all edited images are
visually the same as the original image; (3) all images are edited beyond the scope of instruction; (4) edited images have
harmful content containing sex, violence, porn, etc; and (5) all edited images look similar to each other. We compare training
reward models by ﬁltering some/all of these options.
We note that a considerable portion of the collected data falls under at least one of the aforementioned categories, indicating
that even for humans, ranking these images is challenging. As a result, we only use the data that did not include any non-
rankable options in the reward model training. From a pool of 1,412 images, we select 1,285 for the training set, while the
remaining images were used for the validation set. The reward model is trained on a dataset of comparisons between multiple
model outputs on the same input. Each comparison sample contains an input image, an instruction, ﬁve edited versions of
the image, and the corresponding rankings. We divide the dataset into training and validation sets based on the distribution
of the corresponding instructions.
We apply the method in Sec. 3.3 on the reward data to develop a reward model. We initialize the reward model from the
pre-trained BLIP, which was trained on paired images and captions using three objectives: image-text contrastive learning,
image-text matching, and masked language modeling. Although there is a domain gap between BLIP’s pre-training data
and our reward data, where the captions in BLIP’s data describe a single image, and the instructions in our data refer to the
difference between image pairs. We hypothesized that leveraging the learned alignment between text and image in BLIP
could enhance the reward model’s ability to comprehend the relationship between the instruction and the image pairs.
The reward model is trained using 4 A100 GPUs for 10 epochs, employing a learning rate of 10−4 and weight decay of
0.05. The image encoder’s and multimodal encoder’s last layer outputs are utilized as image and multimodal representations,
respectively. The encoders’ ﬁnal layer is the only ﬁne-tuned component.
We use the trained reward model to generate a reward score on our training data. We perform two experiments. The
ﬁrst experiment takes the exponential rewards as weights and ﬁne-tunes the diffusion model with weighted reward loss as
described in Sec. 3.4. See Fig. 17 for the visualization of the method. The second experiment transforms the rewards to
text prompts and ﬁne-tunes the diffusion model with the condition reward loss as described in Sec. 3.4. The method is
introduced in Fig. 2. We compare those two experiment settings, and results can be found in Sec. D.1.
C. Reward Maximization for Diffusion-Based Generative Models
C.1. Discussion on On-Policy based Reward Maximization for Diffusion Models
Directly adapting on-policy RL methods to the current training pipeline might be computationally expensive, but we do
not conclude that sampling-based approaches are not doable for diffusion models. We consider developing more scalable
sampling-based methods as future work.
We start the sampling methods derivation with the following objective:
J(θ) ∶= max
πθ Ec∼pc[E˜xxx∼πθ(⋅∣c) [Rφ(˜xxx,c)] −ηKL[pD(˜xxx∣c)∣∣πθ(˜xxx∣c)]],
(3)
where pc(c)pD(˜xxx∣c) is the joint distribution of the condition and edited images pair, and πθ denotes the policy or the diffusion
model we want to optimize. Note that pD(˜xxx∣c) and π(˜xxx∣c) are swaped compared with the objective in Eq. (1). The second
term in Eq. (3), is the KL Minimization formula for maximum likelihood estimation, equivalent to the loss of diffusion
models. We represent the policy πθ via the reverse process of a conditional diffusion model:
πθ(˜xxx∣c) ∶= pθ(˜xxx0∶T ∣c) = p0(˜xxxT )
T
∏
t=1
pθ(˜xxxt−1∣˜xxxt;c),
where p0(˜xxxT ) ∶= N(˜xxxT ,0;I), and pθ(˜xxxt−1∣˜xxxt;c) ∶= N(˜xxxt∣µθ(˜xxxt,t),σ2
t )I is a Gaussian distribution, whose parameters are
deﬁned by score function ϵθ and stepsize of noise scalings. So we can get a edited image sample ˜xxx0 by running a reverse
diffusion chain:
˜xxxt−1∣˜xxxt =
1
√αt
(˜xxxt −1 −αt
√1 −¯αt
ϵθ(˜xxxt,c,t)) + σtzzzt, zzz ∼N(0,I),for t = T,...,1,
and ˜xxxT ∼N(0,I).
14

As a result, the reverse diffusion process can be viewed as a black box function deﬁned by ϵθ and noises ϵϵϵ ∶=
(zzzT ,...,zzz1, ˜xxxT ), which we can view as a shared parameter network with noises. And for each layer, we can view the
parameter is the score function ϵθ. Deﬁne the network as
˜xxx0 ∶= f(c,ϵϵϵ;θ), ϵϵϵ ∼pnoise(⋅),c ∼pc(⋅),
where we can rewrite the ﬁrst term as
Ec∼D,ϵϵϵ∼pnoise(⋅)[Rφ(f(c,ϵϵϵ;θ),c)],
and we can optimize the parameter θ with path gradient if R⋅is differentiable with path gradient. Similarly, suppose we
want to optimize the ﬁrst term via PPO. In that case, the main technical difﬁculty is to estimate ∇θ log πθ(˜xxx∣c), which can be
estimated with the following derivation:
∇θ log πθ(˜xxx∣c) = ∇θ log pθ(˜xxx0∶T ∣c) =
T
∑
t=1
∇θ log pθ(˜xxxt−1∣˜xxxt;c).
Note that for both the end-to-end path gradient method and PPO we require to sample the reverse chain from ˜xxxT to ˜xxx0, thus
we can estimate ∇θ log π(˜xxx∣c) using the empirical samples ˜xxx0∶T .
For the above two methods, to perform one step policy gradient update, we need to run the whole reverse chain to get
an edited image sample ˜xxx0 to estimate the parameter gradient for the ﬁrst term. As a result, the computational cost is the
number of diffusion steps more extensive than the supervised ﬁne-tuning cost. Now we need more than two days to ﬁne-tune
the stable diffusion model, so for standard LDM, where the number of steps is 1000, we can not ﬁnish the training within
an acceptable training time. Even if we can use some fast sampling methods such as DDIM or variance preserve (VP) based
noise scaling, the diffusion steps are still more than 5 or 10. Further, we haven’t seen any previous work using such noise
scaling to ﬁne-tune stable diffusion. As a result, we think naive sampling methods might have high risk to obtain similar
performance, compared with our current ofﬂine RL based approaches.
C.2. Derivation for Eq. (2)
Take a functional view of Eq. (2), and differentiate J(ρ) w.r.t ρ, we get
∂J(ρ)
∂ρ
= Rφ(˜xxx∣c) −η (log ρ(˜xxx∣c) + 1 −log p(˜xxx∣c)) .
Setting ∂J(ρ)
∂ρ
= 0 gives us
log ρ(˜xxx∣c) = 1
η Rφ(˜xxx∣c) + log p(˜xxx∣c) −1,
ρ(˜xxx∣c) ∝p(˜xxx∣c)exp(Rφ(˜xxx,c)/η) .
Thus we can get the optimal ρ∗(˜xxx∣c).
D. Additional Ablation Study
D.1. Weighted Reward and Conditional Reward Losses
We compare the weighted reward loss and conditional reward loss on the synthetic evaluation dataset. As shown in Fig. 20,
the performances of these two losses are close to each other, while the conditional reward loss is slightly better. Therefore
we adopt the conditional reward loss in all our experiments.
D.2. Cycle Consistency
We analyze the impact of cycle consistency augmentation in Sec. 3.2.1. The top ﬁve augmentations in the cycle consistency
are demonstrated in Fig. 18. As shown in Fig. 19, the cycle consistency augmentation improves the performance of HIVE by
a notable margin.
15

replace->replace
add->remove
add->delete
remove->add increase->decrease
0
5
10
15
20
25
30
35
40
42.55%
41.23%
13.79%
0.8958%
0.698%
Figure 18: Cycle consistency top ﬁve augmentations.
0.08
0.10
0.12
0.14
0.16
0.18
0.20
CLIP Text-Image Direction Similarity
0.80
0.85
0.90
CLIP Image Similarity
HIVE SD v1.5 w/o cycle consistency
HIVE SD v1.5 w/ cycle consistency
Figure 19: HIVE with and without cycle consistency.
0.08
0.10
0.12
0.14
0.16
0.18
0.20
CLIP Text-Image Direction Similarity
0.80
0.85
0.90
0.95
CLIP Image Similarity
HIVE SD v1.5 weighted reward loss
HIVE SD v1.5 condition reward loss
Figure 20: HIVE with weighted reward loss and conditional reward loss.
D.3. Training with Less Data
We analyze the effect of the training data size. We compare HIVE with SD v1.5 at four training dataset size ratios: 100%,
50%, 30% and 10%. As shown in Fig. 21, signiﬁcantly decreasing the size of the dataset, e.g. 10% data, leads to worse
ability to perform large image edits. On the other hand, reasonable decreasing dataset size can result in a similar yet slightly
worse performance e.g. 50% data.
D.4. Additional Visualized Results
We illustrate additional visualized results in Fig. 22.
16

0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
CLIP Text-Image Direction Similarity
0.75
0.80
0.85
0.90
0.95
CLIP Image Similarity
HIVE SD v1.5 10% data
HIVE SD v1.5 30% data
HIVE SD v1.5 50% data
HIVE SD v1.5 100% data
Figure 21: HIVE with different training data size.
Input
Place a number of 
bisons in the picture
Transform the lake into a 
volcanic appearance
Give the lake a wintry 
appearance
Input
Remove indoor plants
Add a fridge
Switch off the lights
Input
Add birds
Change the leaf color to red
Remove buildings
Figure 22: Additional editing results.
17

