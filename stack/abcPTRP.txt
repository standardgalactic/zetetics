A Brief Compilation
of Guides, Walkthroughs, and Problems
Probability Theory and
Random Processes
at the University of California, Berkeley
Alvin Wan

Contents
0.1
Purpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
0.1.1
Contributors
. . . . . . . . . . . . . . . . . . . . . . . . .
3
0.1.2
Structure
. . . . . . . . . . . . . . . . . . . . . . . . . . .
3
0.1.3
Breakdown
. . . . . . . . . . . . . . . . . . . . . . . . . .
3
0.1.4
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1
Random Variables
4
1.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.1.1
Random Variables . . . . . . . . . . . . . . . . . . . . . .
4
1.1.2
Law of Total Probability . . . . . . . . . . . . . . . . . . .
4
1.1.3
Conditional Probability . . . . . . . . . . . . . . . . . . .
4
1.1.4
Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.5
Independence . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.6
Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2
Expectation, Variance, Covariance
8
2.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.1.1
Expectation . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.1.2
Linearity of Expectation . . . . . . . . . . . . . . . . . . .
8
2.1.3
Linearity of Expectation . . . . . . . . . . . . . . . . . . .
9
2.1.4
Law of Total Expectation . . . . . . . . . . . . . . . . . .
9
2.1.5
Conditional Expectation . . . . . . . . . . . . . . . . . . .
9
2.1.6
Law of Iterated Expectation . . . . . . . . . . . . . . . . .
9
2.1.7
States . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.1.8
Covariance
. . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.1.9
Properties of Covariance . . . . . . . . . . . . . . . . . . .
10
2.1.10 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.1.11 Properties of Variance . . . . . . . . . . . . . . . . . . . .
10
2.1.12 Variance of Sum
. . . . . . . . . . . . . . . . . . . . . . .
11
2.1.13 Law of Total Variance . . . . . . . . . . . . . . . . . . . .
11
2.2
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2

Probability Theory and Random Processes
aaalv.in/abcPTRP
3
Bernoulli Processes
13
3.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.1.1
Bernoulli Distribution . . . . . . . . . . . . . . . . . . . .
13
3.1.2
Binomial Distribution . . . . . . . . . . . . . . . . . . . .
13
3.1.3
Geometric Distribution
. . . . . . . . . . . . . . . . . . .
14
3.1.4
Negative Binomial Distribution . . . . . . . . . . . . . . .
14
3.1.5
Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.1.6
Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.1.7
Combining Distributions . . . . . . . . . . . . . . . . . . .
14
3.2
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
4
Poisson Processes
17
4.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
4.1.1
Poisson Distribution . . . . . . . . . . . . . . . . . . . . .
17
4.1.2
Exponential Distribution
. . . . . . . . . . . . . . . . . .
17
4.1.3
Gamma Distribution . . . . . . . . . . . . . . . . . . . . .
18
4.1.4
Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
4.1.5
Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
4.1.6
Combining Distributions . . . . . . . . . . . . . . . . . . .
18
4.2
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
5
Conﬁdence Intervals
21
5.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
5.1.1
Markov’s Inequality
. . . . . . . . . . . . . . . . . . . . .
21
5.1.2
Chebyshev’s Inequality . . . . . . . . . . . . . . . . . . . .
21
5.1.3
Conﬁdence Interval . . . . . . . . . . . . . . . . . . . . . .
21
5.1.4
Weak Law of Large Numbers . . . . . . . . . . . . . . . .
22
5.2
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
6
Markov Chains
24
6.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
6.1.1
Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
6.1.2
Irreducibility . . . . . . . . . . . . . . . . . . . . . . . . .
24
6.1.3
Periodicity
. . . . . . . . . . . . . . . . . . . . . . . . . .
24
6.1.4
Invariant Distributions . . . . . . . . . . . . . . . . . . . .
25
6.1.5
Balance Equations . . . . . . . . . . . . . . . . . . . . . .
25
6.1.6
Hitting Time . . . . . . . . . . . . . . . . . . . . . . . . .
25
6.1.7
Probability of A before B . . . . . . . . . . . . . . . . . .
26
6.2
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
7
Transformations
28
7.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
7.1.1
Moment Generating Function . . . . . . . . . . . . . . . .
28
7.1.2
Expectation . . . . . . . . . . . . . . . . . . . . . . . . . .
28
7.1.3
Aﬃne Transformation of Random Variable
. . . . . . . .
28
7.1.4
Sum of Independent Random Variables
. . . . . . . . . .
28
Page 3

Probability Theory and Random Processes
aaalv.in/abcPTRP
7.1.5
Combining Distributions . . . . . . . . . . . . . . . . . . .
29
7.1.6
Inversion Property . . . . . . . . . . . . . . . . . . . . . .
29
7.1.7
Distributions . . . . . . . . . . . . . . . . . . . . . . . . .
29
7.2
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
8
Solutions
31
8.1
Probability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
8.2
Expectation, Variance, Covariance
. . . . . . . . . . . . . . . . .
39
8.3
Bernoulli Processes . . . . . . . . . . . . . . . . . . . . . . . . . .
43
8.4
Poisson Processes . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
8.5
Conﬁdence Intervals
. . . . . . . . . . . . . . . . . . . . . . . . .
51
8.6
Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
8.7
Transformations
. . . . . . . . . . . . . . . . . . . . . . . . . . .
59
Page 4

Probability Theory and Random Processes
aaalv.in/abcPTRP
0.1
Purpose
This compilation is (unoﬃcially) written for the Fall 2016 EE126: Probability
Theory and Random Processes at UC Berkeley.
Its primary purpose is to
oﬀer additional practice problems and walkthroughs to build intuition, as a
supplement to oﬃcial course notes and lecture slides. Including more diﬃcult
problems in walkthroughs, there are over 35 exam-level problems.
0.1.1
Contributors
A Special Thanks to Sinho Chewi for spending many hours suggesting
improvements, catching bugs, and discussing ideas and solutions for problems
with me.
0.1.2
Structure
Each chapter is structured so that this book can be read on its own.
A
minimal guide at the beginning of each section covers essential materials and
misconceptions but does not provide a comprehensive overview.
Each guide
is then followed by walkthroughs covering classes of diﬃcult problems and 3-5
exam-level (or harder) problems that I’ve written speciﬁcally for this book.
0.1.3
Breakdown
For the most part, guides are “cheat sheet”s for select chapters from oﬃcial
course notes, with additional comments to help build intuition.
For more diﬃcult parts of the course, guides may be accompanied by breakdowns
and analyses of problem types that might not have been explicitly introduced
in the course. These additional walkthroughs will attempt to provide a more
regimented approach to solving complex problems.
Problems are divvied up into two parts: (1) walkthroughs - a string of problems
that ”evolve” from the most basic to the most complex - and (2) exam-level
questions, erring on the side of diﬃculty where needed. The hope is that with
walkthroughs, students can reduce a relatively diﬃcult problem into smaller,
simpler subproblems.
0.1.4
Resources
Additional resources, including 20+ quizzes with
80 practice questions, and
other random worksheets and problems are posted online at alvinwan.com/cs70.
See also a related book, abcDMPT for Discrete Mathematics and Probability
Theory.
Page 5

Chapter 1
Random Variables
1.1
Guide
1.1.1
Random Variables
Let Ωbe the sample space.
A random variable is by deﬁnition a function
mapping events to real numbers. X : Ω→R, X(ω) ∈R. An indicator variable is
a random variable that only assumes values {0, 1} to denote success or failure for
a single trial. Note that for an indicator, expectation is equal to the probability
of success:
E[Xi] = 1 · P[Xi = 1] + 0 · P[Xi = 0] = P[Xi = 1]
1.1.2
Law of Total Probability
The law of total probability states that Pr[A] = Pr[A|B]Pr[B]+Pr[A| ¯B]Pr[ ¯B],
if the only values of B are B and ¯B. More generally speaking, for a set of Bi
that partition Ω,
Pr[A] =
X
i
Pr[A|Bi]Pr[Bi]
Do not forget this law. On the exam, students often forget to multiply by Pr[Bi]
when computing Pr[A].
1.1.3
Conditional Probability
Conditional probability gives us the probability of an event given priors. By
deﬁnition, the probability of A given B is deﬁned to be
Pr[A|B] = Pr[A ∩B]
Pr[B]
6

Probability Theory and Random Processes
aaalv.in/abcPTRP
1.1.4
Bayes’ Rule
Bayes’ expands on this idea, using the Law of Total Probability.
Pr[A|B] = Pr[B|A]Pr[A]
Pr[B]
1.1.5
Independence
Note that if the implication only goes in one direction, the converse is not
necessarily true. This is a favorite for exams, where the crux of a True-False
question may be rooted in a converse of one of the following implications.
X, Y independent ⇔(Pr[XY ] = Pr[X]Pr[Y ])
X, Y independent ⇒(E[XY ] = E[X]E[Y ])
X, Y independent ⇒(Var(X + Y ) = Var(X) + Var(Y ))
X, Y independent ⇒(Cov(X, Y ) = 0)
Using the above deﬁnition for independence with probabilities, we have the
following corollary.
X, Y independent ⇔(Pr[X|Y ] = Pr[X, Y ]
Pr[Y ]
= Pr[X]Pr[Y ]
Pr[Y ]
= Pr[X])
1.1.6
Symmetry
Given a set of trials, the principle of symmetry states that the probability of
each trial is independent of other trials, without additional information.
Page 7

Probability Theory and Random Processes
aaalv.in/abcPTRP
1.2
Problems
1. Consider a housing district, represented by a 8 × 8 grid. Tom is currently
at the bottom-left, (0, 0) and would like to get home to (7, 7).
If he
goes up with probability p and right with probability 1 −p, what is the
probability he makes it home without visiting the graveyard at (5, 6) or
school at (4, 7)? (Note that by design, it is impossible to reach both the
graveyard and school. This is to make the problem slightly simpler.)
2. Consider a game of cards with 3 other friends. You and your friends draw
cards from a standard 52-card deck.
Each player receives a random 4
cards randomly, and each player is assigned an ace. In total, each player
has 5 cards.
(a) One of your friends, Bob, claims he holds a full house.
What is
the probability he is bluﬃng?
Recall that a full house is a hand
containing two cards of one rank and three cards of another rank.
(b) Each player randomly draws an additional card at random from the
deck. Bob claims he now holds a full house. What is the probability
he is bluﬃng?
(c) The dealer veriﬁes Bob’s claim. Bob now claims that the sum of the
values for his cards is at least 13. What is the probability that Bob
is telling the truth?
3. Consider a loaded 6-sided die, where just one number is favored with
probability p. The remaining numbers have equal probability 1−p
5 . You
and ﬁve other friends must decide who will take out the trash today, for
your shared apartment. Develop a scheme that allows you to pick a victim,
uniformly at random.
4. Consider a standard 52-card deck. You are assigned a standard 5-card
hand, where each card is drawn randomly from the deck.
(a) What is the probability that we have exactly 1 ace?
(b) What is the probability that we have exactly 3 clubs? (Hint: Use
counting)
(c) Given we have three clubs, what is the probability of an ace of clubs?
(d) What is the probability that we have 3 clubs or 1 ace? (Not XOR,
Hint: Think about inclusion-exclusion.)
5. Siqi purchases s packets of strawberry Pocky (S), c packets of chocolate
Pocky (C), and m packets of mint Pocky (M), making n total packets of
Pocky, for her friend Tyler. He has since given in to temptation and has
elected to begin eating Pocky.
(a) Tyler randomly pulls a Pocky packet out, then places the packet back
inside his bag before drawing the next one. He repeats this m times.
What is the probability that he sees exactly k chocolate packets?
Page 8

Probability Theory and Random Processes
aaalv.in/abcPTRP
(b) Forest randomly steals k packets from Tyler’s bag of Pocky. What is
the probability that Forest has at least one chocolate packet?
(c) Given that Tyler has picked six packets of Pocky in sequential order,
what is the probability that Tyler picks S, C, S, C in that order?
(Note that interspersing other packets with this packet is also valid,
so S[M]CS[S]C would also satisfy this condition.)
6. You are rolling a 100-sided die. Across n trials, let X be the number of
rolls with an odd number of dots. Let Y be the number of rolls with an
even number of dots.
(a) Assume n is divisible by 4. What is Pr(Y −X ≥n
2 )?
(b) Assume n is odd. What is Pr(X > Y )?
Page 9

Chapter 2
Expectation, Variance,
Covariance
2.1
Guide
With expectation, we begin to see that some quantities no longer make sense.
Expressions that we compute the expectation for may in fact be far detached
from any intuitive meaning. We will speciﬁcally target how to deal with these,
in the below regurgitations of expectation laws and deﬁnitions.
2.1.1
Expectation
Expectation of a random variable X is intuitively, the mean
E[X] =
X
x Pr(X = x)
Don’t try to rationalize the following; taken as a whole, there is no intuitive
meaning for g(X).
E[g(X)] =
X
g(x) Pr(X = x)
The continuous analog is simply an integral over the values multiplied by the
probability density function (PDF), denoted fX(x) for the density of X
given the value x.
E[g(X)] =
Z ∞
−∞
g(x)fX(x)dx
2.1.2
Linearity of Expectation
The linearity of expectation always holds, regardless of the independence (or
lack thereof). For example,
10

Probability Theory and Random Processes
aaalv.in/abcPTRP
E[2X + 3Y ] = 2E[X] + 3E[Y ]
More generally, take random variables Xi and constants αi.
E[
X
i
αiXi] =
X
i
αiE[Xi]
2.1.3
Linearity of Expectation
Regardless of independence, the linearity of expectation always holds.
Said
succinctly, it is true that E[P
i aiXi] = P
i aiE[Xi]. Said once more in a less
dense format, using constants ai and random variables xi:
E[a1X1 + a2X2 + · · · + anXn] = a1E[X1] + a2E[X2] + · · · + anE[Xn]
Given a more complex combination of random variables, apply linearity of
expectation to solve.
2.1.4
Law of Total Expectation
Here, we expand our deﬁnition of expectation.
E[X] =
X
y
E[X|Y = y] Pr(Y = y)
2.1.5
Conditional Expectation
Note that the following is a function of X. In fact, E[Y |X] is a random variable,
unlike E[X].
E[Y |X] =
X
y
yPr[Y = y|X]
We know how to solve for Pr[Y = y|X], using deﬁnitions from the last chapter.
2.1.6
Law of Iterated Expectation
The Law of Total Expectation states simply that
E[E[Y |X]] = E[Y ]
Page 11

Probability Theory and Random Processes
aaalv.in/abcPTRP
2.1.7
States
We can thus model the evolution of a system over time using E[Xt|Xt−1]. In
other words, we can express the state at some time step t, Xt with a function g
in terms of the state at the previous time step, g(Xt−1).
Consider X(t + 1) = αX(t) for some constant α. In terms of X(0), we have
that X(t) is
X(t) = αtX(0)
Consider X(t + 1) = αX(t) + β for constants α, β. In terms of X(0), X(t) is
X(t) = αtX(0) + β(
t−1
X
i=0
αi)
(Note that the summation begins from a0 = 1)
2.1.8
Covariance
Take two random variables X, Y . They do not need to be independent. We
have the following expression. See 3.2 for a proof.
cov(X, Y ) = E[XY ] −E[X]E[Y ]
2.1.9
Properties of Covariance
Covariances sum, like vectors, and are symmetric.
• cov(X, Y ) = cov(Y, X)
• cov(A + B, Y ) = cov(A, Y ) + cov(B, Y )
2.1.10
Variance
Variance of a random variable X is intuitively, the squared distance from the
mean, or the spread.
var(X) = E[(X −E[X])2] = E[X2] −E[X]2
2.1.11
Properties of Variance
• Variance is unaﬀected by a constant shift, α. Remember variance is the
spread of our X.
var(X + α) = var(X)
Page 12

Probability Theory and Random Processes
aaalv.in/abcPTRP
• A scalar constant is squared when taken out of variance.
var(αX) = α2var(X)
2.1.12
Variance of Sum
Take two random variables X, Y . They do not need to be independent. We
have the following expression.
var(X + Y ) = var(X) + var(Y ) + 2cov(X, Y )
Here’s a nifty trick: Say you know variance and the expected value of a random
variable X. Then, we can compute E[X2] fairly eﬃciently! As a matter of fact,
it is E[X2] = var(X) + E[X]2
Note that if two random variables are independent, the linearity of variance
applies, as cov(X, Y ) = 0, we have that
var(X + Y ) = var(X) + var(Y )
2.1.13
Law of Total Variance
The following is most useful when you do not know the variance of random
variable directly, but given additional information, it is simpler to compute.
Take random variables, X, N.
var(X) = var(E[X|N]) + E[var(X|N)]
Page 13

Probability Theory and Random Processes
aaalv.in/abcPTRP
2.2
Problems
1. Consider a new, innovative textile, a 6-sided cube. (Think of it as a 6-sided
stamp) We place 373 textiles in an 37×37×37 cube, orienting these cubes
randomly.
Four textiles together, oriented in the correct direction will
print an insignia. The insignia is symmetric across both axes, all cubes
are identical, and each textile has 4 printable surfaces of 6 sides. e.g., If
either of the 2 remaining faces is facing up, that textile will not contribute
to a valid insignia. How many fully-formed insignias can we expect to see
across all 6 faces of this agglomerate n3 shape?
2. There are n total students getting a photo taken.
Let n be a perfect
square so that n = m2, for some integer m. The n students are randomly
organized into a grid of m × m. The cameraman stands at the head on a
slightly elevated pedestal. Consider student Xi; he can see the cameraman
if everyone before him is at most as tall as he is.
(a) There are only two heights a, b among all n students, where a < b.
Each person has height a with probability p. If everyone remains
standing, compute the number of people with an obstructed view of
the cameraman. Consider an inﬁnite number of students, so the grid
is m × ∞.
(b) Consider the scenario in the previous part. This time, return to the
original ﬁnite number of students, with an m × m grid.
(c) Now, consider three heights a, b, c among all students, where a <
b < c. Each person has height a with probability p, height b with
probability q, and height c with probability r, where p + q + r = 1.
Consider a single row of inﬁnitely many students. How many students
of height b do we expect to count, until we see a student of height c?
(d) Now again consider the original scenario. If there are three heights,
a, b, c among all n students, compute the number of people with an
obstructed view of the cameraman.
3. Sinho is eating from a bag of pistachios, and every time step, he ﬂips a coin.
If the coin lands heads, he randomly picks a pistachio. If the pistachio
has not been cracked, he will crack it and eat the nut. Regardless of the
pistachio’s state, he returns the shell to his bag. If the coin lands tails, he
digs around until he ﬁnds an uneaten pistachio and eats it, again returning
the shell to his bag. He begins with 500 nuts.
(a) In terms of the number of eaten pistachios at the current time step,
Xi, how many pistachios will Sinho have eaten at time i + 1?
(b) After n steps, how many pistachios has Sinho eaten? Assume n <
500, so he could not have eaten all the nuts.
Page 14

Chapter 3
Bernoulli Processes
3.1
Guide
Distributions help us model common patterns in real-life situations. In the end,
being able to recognize distributions quickly and eﬀectively is critical to solving
probability questions quickly.
3.1.1
Bernoulli Distribution
A single trial, whose probability of success is p.
• Parameter: p - the probability of success
• Probability: p
• Mean: p
• Variance: p(1 −p)
We can then consider multiple trials, which yields the binomial distribution
below.
3.1.2
Binomial Distribution
Number of successes in n trials, where each independent trial has probability p
of success. Alternatively, the probability of k successes.
• Parameters:
– n - number of trials
– p - probability of success for a given trial
• Probability (of k successes):
 n
k

pk(1 −p)n−k
• Mean: np
• Variance: np(1 −p)
15

Probability Theory and Random Processes
aaalv.in/abcPTRP
3.1.3
Geometric Distribution
Number of trials until the ﬁrst success, where each trial is independent and each
trial has probability p of success. Alternatively, the probability of k failures
before one success.
• Parameters:
– p - probability of success for a given, independent trial
• Probability (of k trials): (1 −p)k−1p
• Mean:
1
p
• Variance:
1−p
p2
We can consider a sum of geometric random variables (e.g., successive attempts
at rolling a 6)
3.1.4
Negative Binomial Distribution
(a.k.a., Pascal’s) Properties of a sum of geometrically-distributed random variables.
Number of times until ith success.
• Parameters:
– p - probability of success for a given, independent trial
– t - number of random variables to add
• Probability (of k successes in t time steps):
 t−1
k−1

pk(1 −p)t−k
• Mean:
t
p
• Variance:
t(1−p)
p2
For the following, consider Y = Y1 + Y2 . . . Yk and Z = Z1 + Z2 · · · + Zl, where
each Yi ∼Geom(p) and Zi ∼Geom(q).
3.1.5
Merging
Examine two Bernoulli processes. We count success if we see at least one arrival.
This means that X = Merge(Y, Z) and that Xi ∼Geom(p + q −pq).
3.1.6
Splitting
Now, examine a single Bernoulli process. For every arrival we ﬂip a coin with
bias q. Consider it a part of process A with heads and B otherwise. This means
that A = Split(Y ) and that Ai ∼Geom(pq) and B ∼Geom(p(1 −q)).
3.1.7
Combining Distributions
The minimum across k geometric distributions, each with the same parameter
p, is X ∼Geo((1 −p)k).
Page 16

Probability Theory and Random Processes
aaalv.in/abcPTRP
3.2
Problems
1. Every 10 minutes, Bob and Alice deliberates whether or not to do their
CS70 homework. With probability p, Bob is ready to work on homework.
With probability q, Alice is ready to work on homework. Otherwise, at
least one of them is on Facebook. If they are distracted for more than an
hour, all hope is lost, and they do not continue working on homework.
(a) If both of them are ready to do homework, they spend 10 minutes
ﬁnishing one problem. Consider N, the number of homework problems
that Bob and Alice complete. Find the PMF of N.
(b) Under this strategy, where both must agree to work on homework,
how long does it take for Bob and Alice to ﬁnish a homework with
10 problems?
(c) Now, if one between the two are ready to do homework, he/she will
convince the other to work on homework, and they spend 10 minutes
ﬁnishing one problem. Find the new PMF of N.
(d) Under this strategy, where at least one must be ready to work on
homework, how long does it take Bob and Alice to ﬁnish a homework
with 10 problems?
2. Alice has given up on Bob and is now working on the problem set alone.
Every 10 minutes, Alice deliberates whether or not to work on her CS70
homework. She chooses to work on her CS70 homework with probability
p and is otherwise distracted by Facebook.
(a) We pick a problem i from the problem set uniformly at random. How
long do we expect Alice to take ﬁnishing i and after ﬁnishing i −1?
Assume Alice takes the full 10 minutes to complete a problem.
(b) We pick a problem i from the problem set uniformly at random. How
long do we expect Alice to have spent on Facebook before starting
i and after ﬁnishing i −1? Again, assume Alice takes the full 10
minutes to complete a problem.
(c) Say we pick a random point in time. At this point in time, Alice
has ﬁnished i −1 problems. How long can we expect the length of
that interval, starting from the completion of problem i −1 to the
completion of problem i?
(d) Assume the time it takes for Alice to complete the ith problem is
Ti ∼U[0, 10]. Re-compute the expected amount of time Alice spends
on Facebook in between starting problem i and ﬁnishing problem
i −1.
3. Let Xi be the number of rolls you need to see the ith 6. Let Yi be the
number of rolls you need to see the ith 6 after rolling the i −1th 6, so
Yi = Xi −Xi−1. Compute the following quantities, keeping in mind that
each Yi ∼Geom( 1
6).
(a) Compute E[Y1 + Y2 + Y3|Y1 + Y2].
Page 17

Probability Theory and Random Processes
aaalv.in/abcPTRP
(b) Compute E[Y1 + Y2|Y1 + Y2 + Y3].
(c) Compute E[X1|X2].
(d) Let Z = min(X1, X2). Compute E[max(X1, X2)|Z]
(e) Compute E[min(Y1, Y2)|X2].
Page 18

Chapter 4
Poisson Processes
4.1
Guide
4.1.1
Poisson Distribution
Number of successes per unit time, space etc., if there are many trials and our
average of successes per unit time, space etc. is λ.
Continuous analog for Binomial Distribution Note that this is the binomial
distribution as n →∞; this approximation in general holds when n ≥100 and
p ≤0.01.
• Parameters:
– λ - average per unit time, space etc., the “rate”
• Probability (of k successes):
λk
k! eλ
• Mean: λ
• Variance: λ
4.1.2
Exponential Distribution
Number of unit time intervals until next success.
Continuous analog of Geometric Distribution
• Parameters:
– λ - average per unit time, space, the “rate”
• Probability (of k successes): λe−λx
• Mean:
1
λ
• Variance:
1
λ2
19

Probability Theory and Random Processes
aaalv.in/abcPTRP
4.1.3
Gamma Distribution
(a.k.a., Erlang) Properties of a sum of exponentially-distributed random variables.
Number of times until ith success.
Continuous analog of Negative Binomial
• Parameters:
– λ - average per unit time, space for each exponential
– k - number of exponential distributions
• Mean:
k
λ
• Variance:
k
λ2
For the following, consider Y = Y1 + Y2 . . . Yk and Z = Z1 + Z2 · · · + Zl, where
each Yi ∼Expo(λ1) and Zi ∼Expo(λ2).
4.1.4
Merging
Examine two Poisson processes. We count success if we see at least one arrival.
This means that X = Merge(Y, Z) and that Xi ∼Expo(λ1 + λ2).
4.1.5
Splitting
Examine a single Poisson process. For every arrival we ﬂip a coin with bias p
and . This means that X = Split(Y ) and that Xi ∼Expo(λ1p).
4.1.6
Combining Distributions
The minimum across k exponential distributions, each with the same parameter
λ, is X ∼Expo(kλ).
Page 20

Probability Theory and Random Processes
aaalv.in/abcPTRP
4.2
Problems
1. Every week Bob receives λ surveys. Approximate to 50 weeks in a year.
Each survey is emailed to at least 2,000 CS students, where each promises
gift cards for Ki lucky winners, where Ki ∼N(5,
√
5) The actual value of
Ki may be diﬀerent for each survey, but note they are i.i.d.
(a) Bob completes a survey with probability p. How many surveys do
we expect Bob to complete in a year?
(b) Assume that survey winners are picked uniformly at random. Give
an upper bound for the number of surveys we expect him to win, in
a year, given that he ﬁlls out surveys with probability p.
(c) Assume each survey is sent to exactly 2,000 CS students. Compute
the variance in the number of surveys that we expect Bob to win. Is
this an upper bound or a lower bound, given that surveys in reality
go to at least 2,000 students? Assume p = 1, λ = 0.01.
2. Consider CalCentric, a new piece of software that sees λ bug reports every
minute. With probability p1, production support marks a request as as
SEV-1, with probability p2 marks it as SEV-2 and otherwise as SEV-3.
Three teams, T1, T2, T3 have been created to handle each level of urgency.
Let Yi be the number of minutes until the next request. Let Zi be the
number of requests per hour. Let Xij represent the time until the jth
request for team i. Assume all teams have zero bug reports at midnight.
(a) Find the PMF of Zi, Yi, and Xij.
(b) We pick a bug uniformly at random. How long do we expect team i
to process bug j?
(c) We pick a time uniformly at random across all 24 hours; let us call
this time t, which is measured in minutes since midnight. How many
total bug reports do we expect to have on ﬁle? How many bug reports
for team i? For team i, how much time do we expect between the
last bug report and the next?
(d) For team i, what is the PDF of the time it takes between bugs at
time t? Let this time be T.
(e) Given the ﬁrst bug of the day for CalCentric as a whole comes at
time t1, at what time of day do we expect team 2 to receive its third
bug t3?
(f) Given that CalCentric has seen b bugs and that team 1 has b1 bug
reports to handle, how many bug reports to we expect team 2 to
have? Assuming b2 is more than the current number of bug reports
assigned to team 2, how much more time do we expect until team 2
has b2 bug reports?
3. With probability 1
3, Alice takes her motorcycle to school, which takes 5
minutes. With probability 1
2, Alice takes her bike to school. Otherwise,
Page 21

Probability Theory and Random Processes
aaalv.in/abcPTRP
she takes 20 minutes to walk to school. Each motorcycle tire has been
worn out and has an expected lifetime of 2 hours.
(a) How much time does Alice spend walking before the ﬁrst time she
rides her motorcycle?
(b) In expectation, after how many days will Alice pop a tire?
(c) Alice has lost her bike and walks with probability 2
3. If there are 180
days of school, how many days will Alice walk to school? Assume
that Alice does not have a tire replacement; if she pops a tire, she
cannot use her motorcycle for the remaining days.
Page 22

Chapter 5
Conﬁdence Intervals
5.1
Guide
5.1.1
Markov’s Inequality
Markov’s inequality states the following. Remember that α > 0.
Pr(X ≥α) ≤E[X]
α
This is a one-line derivation, using expectation.
E[X] =
X
a Pr(a) ≥
X
a>α
a Pr(a) ≥α
X
a>α
Pr(a) ≥α Pr(a ≥α)
We can then re-arrange to obtain our ﬁnal result Pr(a ≥α) ≤E[X]
α .
5.1.2
Chebyshev’s Inequality
Chebyshev’s inequality states the following. Intuitively, it is the probability that
we are more than some distance α from the mean. Remember that α > 0.
Pr(|X −µ| ≥α) ≤var(X)
α2
5.1.3
Conﬁdence Interval
Consider some random variable X, its mean µ, some positive α, and probability
p. A conﬁdence interval is an interval of α distance from µ that we know X
has probability p of falling in.
Remember the distinction between observable values and non-observable values.
We usually have some p that we’d like to estimate but cannot observe directly.
23

Probability Theory and Random Processes
aaalv.in/abcPTRP
As a result, we observe some q, express it in terms of p and then solve for p.
This is then our estimate for p.
5.1.4
Weak Law of Large Numbers
The Law of Large Numbers states that as n →∞, our estimate of the
mean approaches the true mean. More formally, Pr(|An −µ| ≥α) →0, where
An =
Pn
i=1 Xi
n
Page 24

Probability Theory and Random Processes
aaalv.in/abcPTRP
5.2
Problems
1. Bob is hanging a string of Christmas lights up. With probability p, a
lightbulb is dead. There are 500 total lightbulbs, and Bob will tolerate at
most 10 dead lightbulbs. If we wish to be 95% certain that we have 10 or
fewer dead lightbulbs, ﬁnd a lower bound on p.
2. Derek wishes to protect user privacy whilst performing large-scale analytics.
He has a sample size of 2,000 users, where each user clicks on an advertisement
with probability p. Assume all users act independently. He decides to
“jiggle” the data. Speciﬁcally, he replaces 2
5 of the data with randomly-generated
samples - “click” with probability 1
2 and “no click” otherwise.
(a) Let q be the probability of clicks that we observe. Find the estimate
for p, ˆp, in terms of q.
(b) Apply Chebyshev’s to ﬁnd a 95% conﬁdence interval for p.
3. Alice also wishes to protect user privacy. She creates n diﬀerent surveys
that are kept with probability p.
For each of the following, consider
whether or not the Law of Large Numbers still holds, when we consider Sn
to be the number of user responses considered, given n survey responses.
Prove your conjecture. Assume that k << n.
(a) Fixing a constant k and assigning k randomly-selected users to a
randomly-selected survey.
(b) Fixing a constant k and for each group of
n
k assign to a random
survey.
Page 25

Chapter 6
Markov Chains
6.1
Guide
6.1.1
Deﬁnition
A Markov chain is a set of states Xi, where each Xi+1 only depends on Xi. In
this class, we only consider Markov Chains with ﬁnitely many states.
• P is the transition matrix. P(i, j) gives us the probability of a transition
from Xi to Xj.
• The current state of a Markov Chain or distribution (i.e., values at all
the nodes) at time t is represented using πt. So, πt+1 = πtP.
• All values must sum to 1.
X
i
π(i) = 1
• All transition probabilities for a single destination must sum to 1.
X
j
Pr(i, j) = 1
6.1.2
Irreducibility
A Markov Chain is irreducible iﬀthere exists some path between every pair
of states. (i.e., for each state, all other states are ”reachable”). Note this means
the Markov Chain must be a single connected component.
6.1.3
Periodicity
A state Xi is aperiodic if the length of all paths starting at Xi and ending at
Xi has GCD 1. More formally,
26

Probability Theory and Random Processes
aaalv.in/abcPTRP
d(i) := G.C.D.{n > 0|P n(i, i) = Pr(Xn = i|X0 = i) > 0}, i ∈X
Xi is aperiodic if d(i) = 1.
A Markov Chain is aperiodic if all of its states are aperiodic.
6.1.4
Invariant Distributions
• An invariant distribution π with transition matrix P is a distribution
such that π = πP.
• An irreducible Markov Chain always has a unique invariant distribution.
To solve for the invariant distribution, do the following:
1. Check that the Markov Chain is irreducible. Only then can we guarantee
that there exists a unique invariant distribution.
Note that invariant
distributions may exist for other Markov Chains but are not guaranteed.
2. Write out all balance equations. As it turns out, this system of equations
is dependent.
3. So, replace one equation with the requirement that all π(i) sum to 1.
Explicitly, remove an arbitrary balance equation, and add the equation
π(1) + · · · + π(n) = Pn
i=1 π(i) = 1.
4. Write the balance equations as a matrix P. Solve the augmented matrix
[P|π].
6.1.5
Balance Equations
Balance equations specify transitions for a Markov Chain. Let π(j) denote
the value of state j. We express π(j) in terms of all possible paths from i to j.
So π(j) = P
i P(i, j)π(i).
6.1.6
Hitting Time
To solve a hitting time problem, write out your balance equations and solve
the linear system of equations. Remember that we consider the set of possible
destinations. In other words,
β(i) = pβ(i −1) + (1 −p)β(i −2) + 1
For the above equation, this means that from state i we have probability p of
reaching state i −1 next and probability 1 −p of reaching state i −2.
We
assume that this transition takes “time” 1. Note that this transition could take
a variable amount of time. In which case, add the amount of time the transition
takes, instead of 1.
Page 27

Probability Theory and Random Processes
aaalv.in/abcPTRP
6.1.7
Probability of A before B
To solve probability of A before B, consider the probability of reaching any
state in A from states neither in A nor in B. Then, consider the probability of
reaching B from states in A. We again consider the set of possible destinations.
Note there is no extra term.
α(i) = pα(i −1) + (1 −p)α(i −2)
The above means that from state i, we have probability p of reaching state i−1
and probability 1 −p of reaching i −2.
Page 28

Probability Theory and Random Processes
aaalv.in/abcPTRP
6.2
Problems
1. Bob is walking along a small bridge, 3 feet wide and 10 feet long. Every
time step, he walks 1 foot along the bridge. With probability p, he walks
straight forward, and otherwise, he takes one step 1 foot forward and 1
foot to the right with probability 1
2, forward 1 foot and left 1 foot with
probability 1
2. If he walks oﬀthe bridge, he falls into the water and foregoes
the bridge to get to the other side. How many time steps do we expect
Bob to walk before falling in the water, assuming he starts at the center
of the bridge?
2. Let us consider the following scenarios. Use Markov Chains only when
necessary, but consider your intuition ﬁrst. Justify all of your answers.
(a) Consider a coin with bias p. What is the probability that we see TH
before HH?
(b) Consider a fair dice. What is the probability that we see 5-3 before
seeing 3-5?
3. Consider a die.
(a) Let this die be fair. How much time do we expect until we roll 5
dots, followed by a roll of 3 dots?
(b) How many 5s do we expect to see before we roll 5 dots, then 3 dots?
(c) Let this die be loaded, favoring 5 with probability
6
11. All others are
have equal probability, or
1
11. How much time do we expect until we
roll 5 dots, followed by a roll of 3 dots?
(d) Again with the loaded die, how many 5s do we expect to see before
our ﬁrst sequence of 5-3?
(Note that we cannot simply take the
answer from the previous part and multiply by 1
6. This is because
the duration of survival is positively correlated with the number of
5s we expect to see.)
Page 29

Chapter 7
Transformations
7.1
Guide
7.1.1
Moment Generating Function
The MGF of a random variable X is
MX(t) = E[etX]
It is always true that MX(0) = 1.
7.1.2
Expectation
We can relate this ith derivative of the MGF to the ith moment of X.
∂kt
∂tk MX(0) = E[Xk]
7.1.3
Aﬃne Transformation of Random Variable
Consider random variables X, Y and constants a, b such that Y = aX + b.
MY (s) = E[es(aX+b)] = E[esaX+sb] = esbE[esaX] = esbMX(sa)
7.1.4
Sum of Independent Random Variables
Consider three random variables Z, X, Y such that Z = X + Y and X, Y are
independent.
MZ(s) = MX+Y (s) = E[es(X+Y )] = E[esXesY ] = E[esX]E[esY ] = MX(s)MY (s)
30

Probability Theory and Random Processes
aaalv.in/abcPTRP
7.1.5
Combining Distributions
For a random variable Y and various random variables X1, . . . Xn, then
fY (y) = p1fX1(y) + · · · + pnfXn(y)
gives us the following.
MY (s) = p1MX1(s) + · · · + pnMXn(s)
7.1.6
Inversion Property
“The transform MX(s) associated with a random variable X uniquely determines
the CDF of X, assuming that MX(s) is ﬁnite for all s in some interval [−a, a],
where a is a positive number.”
7.1.7
Distributions
• Bernoulli(p): MX(s) = 1 −p + pes
• Binomial(n, p): MX(s) = (1 −p + pes)n
• Geometric(p): MX(s) =
pes
1−(1−p)es
• Poisson(λ): MX(s) = eλ(es−1)
• Uniform(a, b) (discrete): MX(s) = esa(es(b−a+1)−1)
(b−a+1)(es−1)
• Uniform(a, b) (continuous): MX(s) = esb−esa
s(b−a)
• Exponential(λ): MX(s) =
λ
λ−s, where s < λ
• Normal(µ, σ2): MX(s) = eσ2s2/2)+µs
Page 31

Probability Theory and Random Processes
aaalv.in/abcPTRP
7.2
Problems
1. Find the PDF of Z given the following moment-generating function of z.
MZ(s) = 2
3
5
5 −s + 1
3
es2 −es
s
2. Let X be the time until lightbulb 1 dies, where its average lifetime is
λ1. Let Y be the time until lightbulb 2 dies, where its average lifetime
is λ2. Without applying linearity of expectation, ﬁnd the expectation of
Z = X + Y .
Page 32

Chapter 8
Solutions
This section contains completely-explained solutions for each of the problems
provided. Each one of these problems is designed to be at exam-level or harder,
err’ing on the side of diﬃculty. The goal is touch on all major topics presenting
in that chapter. In each of the following solutions, we identify ”Takeaways” for
every question at the bottom. You should understand just how the solution
appeals to those takeaways, and on the exam, be prepared to apply tips and
tricks presented here.
33

Probability Theory and Random Processes
aaalv.in/abcPTRP
8.1
Probability
1. Consider a housing district, represented by a 8 × 8 grid. Tom is currently
at the bottom-left, (0, 0) and would like to get home to (7, 7).
If he
goes up with probability p and right with probability 1 −p, what is the
probability he makes it home without visiting the graveyard at (5, 6) or
school at (4, 7)? (Note that by design, it is impossible to reach both the
graveyard and school. This is to make the problem slightly simpler.)
Solution:
Consider all possible ways to get to (7, 7). We can view this
as 16 actions, where we pick 8 to go up,
 16
8

. We note that all paths will
necessarily take 8 blocks up and 8 blocks to the right, so it is not necessary
to consider the probability of taking a particular path.
Now, consider all the ways to get to (5, 6), which by the same logic is
 11
5

.
From (5, 6), the number of ways to get to (7, 7) is
 3
2

. So, the number of
ways to walk through the graveyard is a =
 11
5
 3
2

. By the same logic, the
number of paths passing through (4, 7) is b =
 11
4
 3
3

.
In sum, we have the following
1 −a + b
 16
8
 = 1 −
 11
5
 3
2

+
 11
4

 16
8

2. Consider a game of cards with 3 other friends. You and your friends draw
cards from a standard 52-card deck.
Each player receives a random 4
cards randomly, and each player is assigned an ace. In total, each player
has 5 cards.
(a) One of your friends, Bob, claims he holds a full house.
What is
the probability he is bluﬃng?
Recall that a full house is a hand
containing two cards of one rank and three cards of another rank.
Solution: 0. To form a full house, either your friend has three aces
or two aces. Neither is possible, because the four aces are distributed
evenly, one to each friend.
(b) Each player randomly draws an additional card at random from the
deck. Bob claims he now holds a full house. What is the probability
he is bluﬃng?
Solution:
We consider the probability that your friend has a full
house and take the complement of that event.
1 −Pr(FH)
To compute the probability of a full house, we consider the possible
full houses. Recall that none of these full houses can include aces, so
we discount rank 1. We ﬁrst pick our ranks: from the 12 remaining
ranks, we pick 2.
 12
2

. Then, we pick our suits. For each card, there
Page 34

Probability Theory and Random Processes
aaalv.in/abcPTRP
are a total of 4 diﬀerent suits. 45. In total, we have
 48
4

diﬀerent
possibilities, as we are considering the cards after distributing aces.
1 −
 12
2

45
 48
4

(c) The dealer veriﬁes Bob’s claim. Bob now claims that the sum of the
values for his cards is at least 13. What is the probability that Bob
is telling the truth?
Solution:
We compute the probability that the sum is at most 12.
This is the complement of achieving a sum at least 13.
Pr(X ≤13) = 1 −Pr(X < 13)
Note there is only one possible way to achieve < 13. Since he has
an ace, the values for cards involved in his full house must sum to at
most 12. Considering that full house cannot involve his ace, he must
have 3 2s and 2 3s. Note he cannot achieve any lower value with a
full house and extra ace.
1 −45
 48
4

3. Consider a loaded 6-sided die, where just one number is favored with
probability p. The remaining numbers have equal probability 1−p
5 . You
and ﬁve other friends must decide who will take out the trash today, for
your shared apartment. Develop a scheme that allows you to pick a victim,
uniformly at random.
Solution:
Roll until you see six rolls in a row with unique values. If any
values repeat, restart the sequence. This ensures that we have outcomes
of equal probability.
Speciﬁcally, any sequence of 6 unique values has
probability p( 1−p
5 )5 of occurring.
The issue is that we now have 6! orderings of 6 distinct values. We need
to translate this into one of 6 unique outcomes. To do this, we arbitrarily
pick and ﬁx an index to consider. Say, we consider the ﬁrst number in
the sequence. The value at the index will determine the value of that
sequence. So, if we roll, 1 −2 −3 −4 −5 −6, then the value of that
permutation is 1.
Alternate Solution
We can consider only the following permutations of values. Again, we
discard any permutation that does not match one of the following. Make
the ﬁrst roll the value of our outcome:
• 1-2-3-4-5-6
Page 35

Probability Theory and Random Processes
aaalv.in/abcPTRP
• 2-3-4-5-6-1
• 3-4-5-6-1-2
...
•
We see that each case has the same probability of occurring, and since we
only consider these 6 outcomes, we have a 1
6 chance of achieving any value
in {1, 2, 3, 4, 5, 6}.
4. Consider a standard 52-card deck. You are assigned a standard 5-card
hand, where each card is drawn randomly from the deck.
(a) What is the probability that we have exactly 1 ace?
Solution: It is easier to reason about this using counting. We pick
an ace and then we pick from the non-Ace cards.
 4
1
 48
4

 52
5

Alternatively, we can consider the possibility of picking an Ace
4
52
and then picking four non-Ace cards,
48
51
47
50
46
49
45
48.
Multiply by the
number of ways to place the Ace among 5 cards, which is
 5
1

= 5.
5 ∗4 ∗48 ∗47 ∗46 ∗45
52 ∗51 ∗50 ∗49 ∗48
(b) What is the probability that we have exactly 3 clubs? (Hint: Use
counting)
Solution: It is easier to reason about this using counting. We pick
our three clubs. Since there are 13, we choose 3 from 13, and since
there are 52 - 13 = 39 non-club cards, we pick 2 from 39.
 13
3
 39
2

 52
5

(c) Given we have three clubs, what is the probability of an ace of clubs?
Solution: In the denominator, we count all ways to pick three clubs,
which is the numerator from part 2.
13
3
39
2

In the numerator, we count all ways to pick a 5-card hand with an
ace of clubs. Given that our ace is a club, we only have two more
clubs to choose and two non-clubs to choose.
12
2
39
2

Page 36

Probability Theory and Random Processes
aaalv.in/abcPTRP
Thus, our ﬁnal answer is the following.
 12
2
 39
2

 13
3
 39
2
 = 3
13
(d) What is the probability that we have 3 clubs or 1 ace? (Not XOR,
Hint: Think about inclusion-exclusion.)
Solution: We need to apply inclusion-exclusion. Recall that this
states for two events A and B,
Pr(A ∪B) = Pr(A) + Pr(B) −Pr(A ∩B)
We have that A is from part 1 and B is from part 2. All that remains
is to compute Pr(A∩B) which is the probability of 3 clubs and exactly
one ace. We have two cases; either the ace is a club or the ace is not
a club. Let C be the event that the ace is a club. By some variant
of the law of total probability:
Pr(A ∩B) = Pr(A ∩B ∩C) + Pr(A ∩B ∩¯C)
We now compute both probabilities. Pick the ace of clubs,
 1
1

. We
pick our two non-ace clubs from 12 non-ace clubs
 12
2

. Finally, we
pick our non-ace, non-club from 52 −4 −13 + 1 = 36.
Pr(A ∩B ∩C) =
 12
2
 36
2

 52
5

Given that our ace is not a club, we have all three non-Ace clubs to
choose (
 12
3

), not to mention a non-club, non-ace and a non-club ace
card. We know there are 52 −13 −3 = 36 non-club, non-ace cards
and there are 3 non-club, ace cards. Thus, we have the following.
Pr(A ∩B ∩¯C) =
 12
3
 36
1
 3
1

 52
5

Thus, we can combine these to get the following for Pr(A ∩B).
 12
2
 36
2

 52
5

+
 12
3
 36
1
 3
1

 52
5

We then have our ﬁnal expression for Pr(A ∪B).
 4
1
 48
4

 52
5

+
 13
3
 39
2

 52
5

−(
 12
2
 36
2

 52
5

+
 12
3
 36
1
 3
1

 52
5

)
Page 37

Probability Theory and Random Processes
aaalv.in/abcPTRP
5. Siqi purchases s packets of strawberry Pocky (S), c packets of chocolate
Pocky (C), and m packets of mint Pocky (M), making n total packets of
Pocky, for her friend Tyler. He has since given in to temptation and has
elected to begin eating Pocky.
(a) Tyler randomly pulls a Pocky packet out, then places the packet back
inside his bag before drawing the next one. He repeats this m times.
What is the probability that he sees exactly k chocolate packets?
Solution:
Let C be the number of chocolate packets that Tyler
sees. Note that C is binomially distributed with m trials and the
probability
c
n of successfully picking a chocolate packet on a given
trial, C ∼Bin(m, c
n). We can intuit about the probability as well.
First compute the probability.
We have k successes, or ( c
n)k and
m −k failures, so (1 −c
n)m−k. We then compute the number of ways
to distribute k successes among m trials, which is
 m
k

.
m
k

( c
n)k(1 −c
n)m−k
(b) Forest randomly steals k packets from Tyler’s bag of Pocky. What is
the probability that Forest has at least one chocolate packet?
Solution:
Instead, let us consider the complement of this event.
Let F be the number of chocolate Pocky packets that Forest has.
Speciﬁcally, we consider the case where Forest has zero chocolate
packets.
Pr(F ≥1) = 1 −Pr(F < 1) = 1 −Pr(F = 0)
Note that the F is not binomially distributed, because the trials are
not independent.
Instead, we have the following probability that
F = 0:
s + m
n
s + m −1
n −1
· · · s + m −(k −1)
n −(k −1)
= (s + m)!(n −k)!
(s + m −k)!n!
Thus, we have our ﬁnal answer:
1 −(s + m)!(n −k)!
(s + m −k)!n!
Takeaway: Consider the complement.
(c) Given that Tyler has picked six packets of Pocky in sequential order,
what is the probability that Tyler picks S, C, S, C in that order?
(Note that interspersing other packets with this packet is also valid,
so S[M]CS[S]C would also satisfy this condition.)
Solution:
Note that the two extra packets can be any ﬂavor. Thus,
we are implicitly multiplying by 12. It also does not matter where
Page 38

Probability Theory and Random Processes
aaalv.in/abcPTRP
our sequence begins or ends, by symmetry.
However, we need to
pick the slots that our pattern occupies.
 6
4

. We then consider the
probability that our speciﬁc permutation occurs.
Let Xa be the event that we ﬁnd a strawberry packet, Xb be the
event we ﬁnd a chocolate packet next, etc. We are thus interested in
computing the following.
Pr[Xa ∩Xb ∩Xc ∩Xd]
The probability of a strawberry packet at some position is s
n. There
are now n −1 packets remaining, of which s −1 are strawberry.
Pr[Xa] = s
n
The probability of a chocolate packet at some later position, regardless
of which position it is, is
c
n−1. There are now n −2 coins remaining,
of which c −1 are pennies.
Pr[Xb|Xa] =
c
n −1
The probability of another strawberry at some later position is s−1
n−2.
There are now n −3 coins remaining.
Pr[Xc|Xa, Xb] = s −1
n −2
The probability of another chocolate is c−1
n−3.
Pr[Xd|Xa, Xb, Xc] = c −1
n −3
Since all samples are made at random, we know the following holds.
Pr[Xa ∩Xb ∩Xc ∩Xd] = Pr[Xa]Pr[Xb|Xa]Pr[Xc|Xa, Xb]Pr[Xd|Xa, Xb, Xc]
=
6
4
 s
n
c
n −1
s −1
n −2
c −1
n −3
=
6
4
s(s −1)c(c −1)(n −4)!
n!
Takeaway:
Apply symmetry when we have no additional
information.
6. You are rolling a 100-sided die. Across n trials, let X be the number of
rolls with an odd number of dots. Let Y be the number of rolls with an
even number of dots.
Page 39

Probability Theory and Random Processes
aaalv.in/abcPTRP
(a) Assume n is divisible by 4. What is Pr(Y −X ≥n
2 )?
Solution:
Note that X + Y = n, so Y −X = n −2X.
Pr(Y −X ≥n
2 ) = Pr(n −2X ≥n
2 )
= Pr(−2X ≥−n
2 )
= Pr(X ≤n
4 )
Consider that X is binomially distributed, with n trials, where the
success of each trial is 1
2, X ∼Bin(n, 1
2).
Pr(X ≤n
4 ) =
n/4
X
i=1
Pr(X = i) =
n/4
X
i=1
n
i
 1
2n = 1
2n
n/4
X
i=1
n
i

(b) Assume n is odd. What is Pr(X > Y )?
Solution:
Intuitively, this is 1
2. Note that for every combination
where X > Y , we can swap all odd with even and even with odd, for
another combination where Y > X. Thus, the number of permutations
where X > Y must be the same as the number of permutations where
Y > X, so
Pr(X > Y ) = Pr(Y > X)
We additionally know that since n is odd, these are the only two
possibilities. Thus,
Pr(X > Y ) + Pr(Y > X) = 1
Finally, this means that
Pr(X > Y ) = Pr(Y > X) = 1
2
Takeaway: Consider intuitive approaches before diving into
the math.
Page 40

Probability Theory and Random Processes
aaalv.in/abcPTRP
8.2
Expectation, Variance, Covariance
1. Consider a new, innovative textile, a 6-sided cube. (Think of it as a 6-sided
stamp) We place 373 textiles in an 37×37×37 cube, orienting these cubes
randomly.
Four textiles together, oriented in the correct direction will
print an insignia. The insignia is symmetric across both axes, all cubes
are identical, and each textile has 4 printable surfaces of 6 sides. e.g., If
either of the 2 remaining faces is facing up, that textile will not contribute
to a valid insignia. How many fully-formed insignias can we expect to see
across all 6 faces of this agglomerate n3 shape?
Solution:
Let Xi be the number of insignias we observe on face i.
For each face, consider the interior vertices. For example, a 3 × 3 (think
tic-tac-toe grid) would have 4 interior vertices. More generally an n × n
grid has a (n −1)2 interior vertices. Let Xij be the jth interior vertex for
the ith face, which takes on value 1 of the jth interior vertex is the center
of a valid insignia. For any indicator random variable, we know that
E[Xij] = Pr(Xij = 1)
We consider the probability that a valid insignia is formed at vertex j.
All cubes must have a valid face showing, making ( 4
6)4 = ( 2
3)4. Each face
must also be rotated in the correct direction, which has probability ( 1
4)4
of occurring. Thus, we have that
Pr(Xij = 1) = 1
64
For each face, we have (n −1)2 = 362 = 64 interior vertices, and we have
a total of 6 faces. Thus, we have
E[X] = 6E[Xi] = 6(64)E[Xij] = 6(64) 1
64 = 6
2. There are n total students getting a photo taken.
Let n be a perfect
square so that n = m2, for some integer m. The n students are randomly
organized into a grid of m × m. The cameraman stands at the head on a
slightly elevated pedestal. Consider student Xi; he can see the cameraman
if everyone before him is at most as tall as he is.
(a) There are only two heights a, b among all n students, where a < b.
Each person has height a with probability p. If everyone remains
standing, compute the number of people with an obstructed view of
the cameraman. Consider an inﬁnite number of students, so the grid
is m × ∞.
Solution:
Let X be the total number of students that can see the
cameraman, where Xi is the number students in column i that can
Page 41

Probability Theory and Random Processes
aaalv.in/abcPTRP
see the cameraman. We see that Xi is geometrically distributed with
parameter 1 −p, with a maximum value of m, as Xi is the number
of students of height a that stand before a student with height b,
Xi ∼Geom(1 −p).
E[X] = mE[Xi] =
m
1 −p
(b) Consider the scenario in the previous part. This time, return to the
original ﬁnite number of students, with an m × m grid.
Solution:
Let X be the total number of students that can see the
cameraman, where Xi is the number students in column i that can
see the cameraman. We must split once more where Xij denotes if
the jth student in the ith row can see the cameraman. If the student
has height a, then the student can see if everyone in front has height
a. If the student has height b, the heights of students before do not
matter.
E[Xij] = Pr(Xij = 1) = (p)pj−1 + (1 −p)
Apply linearity of expectation twice:
E[X] = mE[Xi] = m
X
j
E[Xj] = m
X
j
(pj + (1 −p))
(c) Now, consider three heights a, b, c among all students, where a <
b < c. Each person has height a with probability p, height b with
probability q, and height c with probability r, where p + q + r = 1.
Consider a single row of inﬁnitely many students. How many students
of height b do we expect to count, until we see a student of height c?
Solution:
Let N be the number of students we see before the ﬁrst
student of height c, including the student of height c. Since height c
occurs with probability r, we have that N ∼Geom(r). Let B be the
number of students of height b that we see. Note B ∼Bin(N−1,
q
p+q).
Thus, we apply law of iterated expectations and then solve using
conditional expectation.
E[B] = E[E[B|N]] = E[(N −1)
q
p + q ] =
q
p + q (1
r −1) = q(1 −r)
p + q
(d) Now again consider the original scenario. If there are three heights,
a, b, c among all n students, compute the number of people with an
obstructed view of the cameraman.
Solution:
As in part a, let Xij denote whether or not the jth
person can see. Note that if the jth student has height a, then all
Page 42

Probability Theory and Random Processes
aaalv.in/abcPTRP
students before must have height a as well. If the jth student, all
students before must have height
Xij = pj + q(p + q)j−1 + r
Again, apply linearity of expectation twice.
E[X] = mE[Xi] = m
X
j
E[Xij] = m
X
j
(pj + q(p + q)j−1 + r)
3. Sinho is eating from a bag of pistachios, and every time step, he ﬂips a coin.
If the coin lands heads, he randomly picks a pistachio. If the pistachio
has not been cracked, he will crack it and eat the nut. Regardless of the
pistachio’s state, he returns the shell to his bag. If the coin lands tails, he
digs around until he ﬁnds an uneaten pistachio and eats it, again returning
the shell to his bag. He begins with 500 nuts.
(a) In terms of the number of eaten pistachios at the current time step,
Xi, how many pistachios will Sinho have eaten at time i + 1?
Solution:
With probability
1
2, Sinho pulls a random pistachios
from the bag. With probability
Xi
500, Sinho picks an already-eaten
pistachio.
Otherwise, he picks an uneaten pistachio, eats it, and
returns it to the bag, incrementing the number of eaten pistachios by
1.
With probability 1
2, Sinho digs around to ﬁnd an uneaten pistachio
and deﬁnitely increases the number of eaten pistachios.
E[Xi+1|Xi] = 1
2( Xi
500Xi + (1 −Xi
500)(Xi + 1)) + 1
2(Xi + 1)
= 1
2( Xi
500Xi + Xi + 1 −Xi
500Xi −Xi
500) + 1
2(Xi + 1)
= 1
2(499Xi
500
+ 1) + 1
2(Xi + 1)
= 999
1000Xi + 1
(b) After n steps, how many pistachios has Sinho eaten? Assume n <
500, so he could not have eaten all the nuts.
Solution:
By the law of iterated expectation, we have that
E[Xn] = E[E[Xn|Xn−1]] = E[ 999
1000Xn−1 + 1]
We would like to re-express this in terms of E[X0] = 500. We know
that a system of the form X(t + 1) = αX(t) + β has the following
solution:
Page 43

Probability Theory and Random Processes
aaalv.in/abcPTRP
X(t) = αtX(0) + β(1 −αt−1
1 −α
)
Thus, we can plug in
E[Xn] = ( 999
1000)nX0+1 −( 999
1000)t−1
1
1000
= ( 999
1000)n500+1000(1−( 999
1000)t−1)
Page 44

Probability Theory and Random Processes
aaalv.in/abcPTRP
8.3
Bernoulli Processes
1. Every 10 minutes, Bob and Alice deliberates whether or not to do their
CS70 homework. With probability p, Bob is ready to work on homework.
With probability q, Alice is ready to work on homework. Otherwise, at
least one of them is on Facebook. If they are distracted for more than an
hour, all hope is lost, and they do not continue working on homework.
(a) If both of them are ready to do homework, they spend 10 minutes
ﬁnishing one problem. Consider N, the number of homework problems
that Bob and Alice complete. Find the PMF of N.
Solution:
This is a merging of Bernoulli processes.
Consider the time it takes to complete a single homework problem.
Since both Bob and Alice must agree to work on the problem, we
note that Xi is geometrically distributed with probability pq, Xi ∼
Geom(pq). Additionally, an hour is comprised of 6 10-minute intervals,
thus we are interested in Pr(Xi ≤6) = 1 −Pr(Xi > 6) = 1 −
(1 −pq)6, which gives us the probability that Bob and Alice ﬁnish
another problem within an hour. Note that to terminate the process,
we cannot have Bob and Alice focused for any 10-minute interval,
(1 −pq)6.
Pr(N = k) = (1 −(1 −pq)6)k(1 −pq)6
(b) Under this strategy, where both must agree to work on homework,
how long does it take for Bob and Alice to ﬁnish a homework with
10 problems?
Solution:
It will take inﬁnite time, as we do not exclude the
possibility that Alice and Bob lose all hope.
(c) Now, if one between the two are ready to do homework, he/she will
convince the other to work on homework, and they spend 10 minutes
ﬁnishing one problem. Find the new PMF of N.
Solution:
This is another merging of of Bernoulli processes.
Consider the time it takes to ﬁnish a single homework problem. At
least one of Bob or Alice must agree to work, so we have 1 −(1 −
p)(1 −q) = p + q −pq, or Xi ∼Geom(1 −(1 −p)(1 −q)). We are
again interested in Pr(Xi ≤6) = 1 −Pr(Xi > 6) = 1 −((1 −p)(1 −
q))6. Again, to terminate the process, both Bob and Alice must be
distracted for the full hour, ((1 −p)(1 −q))6. We get an answer very
similar to one from part a.
Pr(N = k) = (1 −((1 −p)(1 −q))6)k((1 −p)(1 −q))6
Page 45

Probability Theory and Random Processes
aaalv.in/abcPTRP
(d) Under this strategy, where at least one must be ready to work on
homework, how long does it take Bob and Alice to ﬁnish a homework
with 10 problems?
Solution:
Again, it will take inﬁnite time, as we do not exclude the
possibility that Alice and Bob lose all hope.
2. Alice has given up on Bob and is now working on the problem set alone.
Every 10 minutes, Alice deliberates whether or not to work on her CS70
homework. She chooses to work on her CS70 homework with probability
p and is otherwise distracted by Facebook.
(a) We pick a problem i from the problem set uniformly at random. How
long do we expect Alice to take ﬁnishing i and after ﬁnishing i −1?
Assume Alice takes the full 10 minutes to complete a problem.
Solution:
First, consider the amount of time it takes Alice to ﬁnish
her ith problem after ﬁnishing the i −1th problem. We can appeal
to the memoryless property of the Geometric distribution to ignore
the time it took her to ﬁnish the i−1 problems beforehand. Instead,
we focus on the time from i −1 to i. This time, Ti, including the
10 minutes it takes to ﬁnish the ith problem, can be modeled as a
geometric random variable with parameter p.
Ti ∼Geom(p)
Thus, take the expectation to get the number of 10-minute intervals
Alice takes. Multiply by 10 to get the total amount of time.
E[Ti] = 10
p minutes
(b) We pick a problem i from the problem set uniformly at random. How
long do we expect Alice to have spent on Facebook before starting
i and after ﬁnishing i −1? Again, assume Alice takes the full 10
minutes to complete a problem.
Solution:
From the previous part, we have that Ti ∼Geom(p).
However, we need to subtract the time it takes to ﬁnish the last
problem, to get the time that Alice is distracted by Facebook, between
the i and i −1 problems.
Let Alice’s duration of distraction be modeled by a shifted geometric
random variable. T ∼Geom(p) −1, as we ignore the 10 minutes it
takes to ﬁnish the ith problem. After the i −1th problem, we expect
E[T] = E[Geom(p)] −1 = 1
p −1
This translates into
Page 46

Probability Theory and Random Processes
aaalv.in/abcPTRP
E[T] = 10(1
p −1) minutes
(c) Say we pick a random point in time. At this point in time, Alice
has ﬁnished i −1 problems. How long can we expect the length of
that interval, starting from the completion of problem i −1 to the
completion of problem i?
Solution: This is a tidbit of renewal theory. We invoke the Inspector’s
Paradox, as we note that if we pick points in time, we are more likely
to land in a longer interval than we are to land in a shorter interval.
Thus, this is not just the expected length of an interval. Instead, we
see that there is 1
p until the next completed problem, and 1
p −1 from
the last completed problem. In sum, the length of the interval is thus
2
p −1
(d) Assume the time it takes for Alice to complete the ith problem is
Ti ∼U[0, 10]. Re-compute the expected amount of time Alice spends
on Facebook in between starting problem i and ﬁnishing problem
i −1.
Solution:
We simply take the expectation from the previous part
and add the amount of time after Alice ﬁnishes a problem and the
start of the next 10-minute interval.
E[T] = 10(1
p −1) + E[Ti] = 10
p −5
3. Let Xi be the number of rolls you need to see the ith 6. Let Yi be the
number of rolls you need to see the ith 6 after rolling the i −1th 6, so
Yi = Xi −Xi−1. Compute the following quantities, keeping in mind that
each Yi ∼Geom( 1
6).
(a) Compute E[Y1 + Y2 + Y3|Y1 + Y2].
Solution:
Intuitively, we know that Y1 + Y2 is a known constant,
and we only need to compute E[Y3] = 6, since all Yi ∼Geom( 1
6).
E[Y1 + Y2 + Y3|Y1 + Y2] = Y1 + Y2 + 6
We can see this using linearity of expectation explicitly.
E[Y1 + Y2|Y1 + Y2] + E[Y3|Y1 + Y2] = Y1 + Y2 + E[Y3] = Y1 + Y2 + 6
Page 47

Probability Theory and Random Processes
aaalv.in/abcPTRP
(b) Compute E[Y1 + Y2|Y1 + Y2 + Y3].
Solution:
We know that Yi are all i.i.d., so we expect the following,
by linearity of expectation:
E[Y1 + Y2|Y1 + Y2 + Y3] = 2
3(Y1 + Y2 + Y3)
(c) Compute E[X1|X2].
Solution: We can view the sum total and consider the the geometric
in reverse. From the end of the second 6, we expect that the time of
the last arrival for a Geom(p) is 1
p −1. In our case, we have Geom( 1
6),
so the time since the last problem is 5. This makes
E[X1|X2] = X2 −5
(d) Let Z = min(X1, X2). Compute E[max(X1, X2)|Z]
Solution:
This is simply the exponential between the second and
ﬁrst geometric processes, regardless of the order.
E[max(X1, X2)|Z] = Z + E[Y2] = Z + 6
(e) Compute E[min(Y1, Y2)|X2].
Solution:
Consider the following.
Pr(min(Y1, Y2) ≥k) = Pr(min(Y1, X2 −Y1) ≥k)
= Pr(Y1 ≥k, X2 −Y1 ≥k)
Let us reason about this. We note that Y1 ≥k and that Y1 ≤X2 −k,
so we consider the probability of falling within X2 −k −k + 1 =
X2 −2k + 1. We then sum over all possible values of k.
X2−1
X
k=1
X2 −2k + 1
X2 −1
Page 48

Probability Theory and Random Processes
aaalv.in/abcPTRP
8.4
Poisson Processes
1. Every week Bob receives λ surveys. Approximate to 50 weeks in a year.
Each survey is emailed to at least 2,000 CS students, where each promises
gift cards for Ki lucky winners, where Ki ∼N(5,
√
5) The actual value of
Ki may be diﬀerent for each survey, but note they are i.i.d.
(a) Bob completes a survey with probability p. How many surveys do
we expect Bob to complete in a year?
Solution:
This is Poisson thinning.
Note that the number of
surveys that are sent, N, is Poisson distributed with parameter λ,
so N ∼Pois(λ).
Once receiving a survey, Bob ﬁlls it out with
probability p, so we see that this is a new Poisson process distributed
with parameter λp. We expect Bob to ﬁll out λp surveys per week.
Per year, he then ﬁlls about 50λp.
(b) Assume that survey winners are picked uniformly at random. Give
an upper bound for the number of surveys we expect him to win, in
a year, given that he ﬁlls out surveys with probability p.
Solution:
Intuitively, we see that the number of surveys is on
average λp50 and that the number of gift cards per survey is 5, so
we expect λp250 gift cards to be distributed among 2000 students.
We expect any student to the have λp
8 chance to receive a gift card.
Here is the formal computation:
First, we compute the number of surveys that Bob wins a single
survey, given K. Let W be the number of surveys that Bob wins,
and Wi be the indicator that Bob wins a single survey.
E[Wi|K] =
K
2000
Thus, we can apply the law of iterated expectation to get
E[E[Wi|K]] = E[ K
2000] =
1
400
Now, we can compute W = P
i Wi.
E[W|N] = E[NWi|N] = NE[Wi] = N
400
By the law of iterated expectation, we have
E[W] = E[E[W|N]] = E[ N
400] = 50λp
400 = λp
8
(c) Assume each survey is sent to exactly 2,000 CS students. Compute
the variance in the number of surveys that we expect Bob to win. Is
this an upper bound or a lower bound, given that surveys in reality
go to at least 2,000 students? Assume p = 1, λ = 0.01.
Page 49

Probability Theory and Random Processes
aaalv.in/abcPTRP
Solution:
Again compute variance for Wi and then apply linearity
of variance, by independence of each survey.
var(Wi|K) =
K
2000(1 −
K
2000)
By the law of total variance, we have (using E[Wi|K] from the previous
part)
var(Wi) = E[var(Wi|K)] + var(E[Wi|K])
=
1
400(1 −
1
400) +
5
20002
= 399
4002 +
1
4002(52)
=
499
200, 000
Apply linearity of variance, and we have
var(W|N) = Nvar(Wi) =
499
2 ∗105 N
Finally, we apply law of total variance once more, using E[W|N] from
the previous part.
var(W) = E[var(W|N)] + var(E[W|N])
=
499
2 ∗105 λ50p +
5
4002
=
499
400 ∗1002 +
5
4002
= 1023
40025
This is a lower bound for variance, since with more students, we have
higher uncertainty.
2. Consider CalCentric, a new piece of software that sees λ bug reports every
minute. With probability p1, production support marks a request as as
SEV-1, with probability p2 marks it as SEV-2 and otherwise as SEV-3.
Three teams, T1, T2, T3 have been created to handle each level of urgency.
Let Yi be the number of minutes until the next request. Let Zi be the
number of requests per hour. Let Xij represent the time until the jth
request for team i. Assume all teams have zero bug reports at midnight.
(a) Find the PMF of Zi, Yi, and Xij.
Solution:
Zi ∼Pois(60λpi), so we see that the PDF is Pr(Zi =
k) = (λpi)k
k!
e−λpi.
Page 50

Probability Theory and Random Processes
aaalv.in/abcPTRP
Yi ∼Expo(λpi), so the PDF is fYi(y) = λe−λy.
Xij is Erlang with intensity j, where each exponential is Yi. Thus,
fXij(x) = λjxj−1
(j−1)! e−λx.
(b) We pick a bug uniformly at random. How long do we expect team i
to process bug j?
Solution:
This is simply the expected value of an exponential for
team i, which is E[Ti] =
1
λpi .
(c) We pick a time uniformly at random across all 24 hours; let us call
this time t, which is measured in minutes since midnight. How many
total bug reports do we expect to have on ﬁle? How many bug reports
for team i? For team i, how much time do we expect between the
last bug report and the next?
Solution:
We expect there to be λt total bug reports. For team
i, there should be λpit bug reports.
We expect
2
λpi minutes, as
we expect
1
λpi until the next arrival and likewise,
1
λpi from the last
arrival.
(d) For team i, what is the PDF of the time it takes between bugs at
time t? Let this time be T.
Solution:
We take the PDF of Yi and scale by the length of
the segment, as there is a higher probability of landing in a longer
segment.
fT (t) = fYi(y)y
E[Yi]
= yλe−λy
1/λ
= yλ2e−λy
(e) Given the ﬁrst bug of the day for CalCentric as a whole comes at
time t1, at what time of day do we expect team 2 to receive its third
bug t3?
Solution:
The ﬁrst bug may or may not go to team 2.
With
probability p2, it is assigned to team 2 and we consider an Erlang of
mode 2. Otherwise, we consider an Erlang of mode 3.
E[t3|t1] = p2(t1 + E[X2,3 −X2,1]) + (1 −p2)E[X2,3]
= p2(t1 +
2
λp2
) + (1 −p2) 3
λp2
(f) Given that CalCentric has seen b bugs and that team 1 has b1 bug
reports to handle, how many bug reports to we expect team 2 to
have? Assuming b2 is more than the current number of bug reports
assigned to team 2, how much more time do we expect until team 2
has b2 bug reports?
Solution:
There are only b−b1 remaining bug reports and of those,
we expect p2 to be assigned to team 2. As a result, we expect team
2 to have
Page 51

Probability Theory and Random Processes
aaalv.in/abcPTRP
(b −b1)
p2
p2 + p3
bug reports. We now examine an Erlang with mode b2 −(b −b1)p2.
The mean of an Erlang distribution is
k
λp2 , where k is the mode.
Thus, we have
b2 −(b −b1)p2/(p2 + p3)
λp2
3. With probability 1
3, Alice takes her motorcycle to school, which takes 5
minutes. With probability 1
2, Alice takes her bike to school. Otherwise,
she takes 20 minutes to walk to school. Each motorcycle tire has been
worn out and has an expected lifetime of 2 hours.
(a) How much time does Alice spend walking before the ﬁrst time she
rides her motorcycle? Solution:
Consider N, the number of days
until Alice ﬁrst rides her motorcycle. Take X to be the number of
days Alice chooses to walk.
Note that N ∼Geom( 1
3) and that X ∼Bin(N −1, 1/6
2/3) = Bin(N −
1, 1
4).
20E[X] = 20E[E[X|N]] = 20E[(N −1)1
4] = 5E[N −1] = 10
(b) In expectation, after how many days will Alice pop a tire?
Solution:
In expectation, Alice uses her motorcycle every 3 days.
The tire in expectation can last up to 2 hours or 24 rides, meaning
Alice will last up to 3 ∗24 = 72 days.
(c) Alice has lost her bike and walks with probability 2
3. If there are 180
days of school, how many days will Alice walk to school? Assume
that Alice does not have a tire replacement; if she pops a tire, she
cannot use her motorcycle for the remaining days.
Solution:
The motorcycle, in expectation, can only be used 24
times. Thus, Alice is expected to walk 180 −24 = 156 days.
Page 52

Probability Theory and Random Processes
aaalv.in/abcPTRP
8.5
Conﬁdence Intervals
1. Bob is hanging a string of Christmas lights up. With probability p, a
lightbulb is dead. There are 500 total lightbulbs, and Bob will tolerate at
most 10 dead lightbulbs. If we wish to be 95% certain that we have 10 or
fewer dead lightbulbs, ﬁnd a lower bound on p.
Solution:
Let X be the number of dead light bulbs. The ﬁrst step is to
ﬁt the form Pr(|X −µ| ≥α).
Pr(X ≥10)
= Pr(X −500p ≥10 −500p)
≤Pr(∥X −500p∥≥10 −500p)
Second, we note that p is already included. We ﬁnd var(X), knowing
that X is binomial.
var(X) = 500p(1 −p)
Third, we plug into the bound and compute.
Pr(∥X −500∥≤10 −500p) = 1 −Pr(∥X −500p∥≥10 −500p)
≤1 −
var(X)
(10 −500p)2
= 1 −500p(1 −p)
102(1 −50p) = 0.95
Plugging in, we get p = 0.00700 or 0.7% chance of lightbulb death.
2. Derek wishes to protect user privacy whilst performing large-scale analytics.
He has a sample size of 2,000 users, where each user clicks on an advertisement
with probability p. Assume all users act independently. He decides to
“jiggle” the data. Speciﬁcally, he replaces 2
5 of the data with randomly-generated
samples - “click” with probability 1
2 and “no click” otherwise.
(a) Let q be the probability of clicks that we observe. Find the estimate
for p, ˆp, in terms of q.
Solution:
First, express the probability of clicks that we observe,
in terms of the true probability.
q = 2
5
1
2 + 3
5p
= 1
5 + 3
5p
Page 53

Probability Theory and Random Processes
aaalv.in/abcPTRP
Let us now express p in terms of q, to obtain our estimate ˆp.
ˆp = 5q −1
3
(b) Apply Chebyshev’s to ﬁnd a 95% conﬁdence interval for p.
Solution:
The ﬁrst step is to ﬁt the form Pr(|X −µ| ≥α).
Pr(|ˆp −p| ≥α) = Pr(|ˆq −q| ≥3
5α)
The second step is to deﬁne our random variable and incorporate n.
Deﬁne Y = 1
n
P
i Yi, where Yi is i is 1 if we observe user i clicks, after
“jiggling” the data. Note that by the Weak Law of Large Numbers
Y should converge to the true mean, or in other words, the true q.
Applying the fact that var(cX) = c2var(X) for some constant c and
then linearity of variance, we get the following:
var(Y ) = 1
n2 var(
X
i
Yi)
= 1
n2 (2n
5 p(1 −p) + 3n
5
1
4)
= 1
n(2
5p(1 −p) + 3
5
1
4)
Note that this quantity is maximized when p(1 −p) = 1
4, so
var(Y ) = 1
n(2
5
1
4 + 3
5
1
4) = 1
4n
Finally, we plug into the bound and compute.
Pr(|Y −q| ≥3
5α) ≤var(Y )
(3/5α)2
Note that we need an upper bound on falling within the interval.
Thus, we need the complement of the provided bound.
Also note that each Yi has probability q of success. Thus, the Bernoulli
distribution gives us var(Yi) = q(1−q). We wish to maximize this for
our upper bound. Note that for any probability q, this is maximized
when q = 1
2, making 1
4.
Page 54

Probability Theory and Random Processes
aaalv.in/abcPTRP
1 −var(Yn)
(3/5α)2 = 0.95
1
4n(3/5α)2 = 0.05
25
36nα2 =
5
100
1
2, 000α2 = 36
500
1
α2 = 36
4
α = 2
6 = 1
3
Thus, our bound is
[p −1
3, p + 1
3]
3. Alice also wishes to protect user privacy. She creates n diﬀerent surveys
that are kept with probability p.
For each of the following, consider
whether or not the Law of Large Numbers still holds, when we consider Sn
to be the number of user responses considered, given n survey responses.
Prove your conjecture. Assume that k << n.
(a) Fixing a constant k and assigning k randomly-selected users to a
randomly-selected survey.
Solution:
Yes. Let us deﬁne an “indicator” to denote the number
of users that the ith group of k users contributes. With probability
p, we ignore the survey that the user group is assigned to and thus
ignore the ith group.
Xi =
(
k
w.p. p
0
o.w.
Note that E[Xi] = pk and var(Xi) = k2p(1−p). Compute An = Sn
n .
E[An] = 1
nnpk = pk
var(An) = 1
n2 var(Sn) = k2
n p(1 −p)
Note that
Pr(∥An −E[An]∥> ϵ) = Pr(∥An −pk∥> ϵ) ≤k2p(1 −p)
nϵ2
which tends to 0 as n →∞.
This thus obeys the Law of Large
Numbers.
Page 55

Probability Theory and Random Processes
aaalv.in/abcPTRP
(b) Fixing a constant k and for each group of
n
k assign to a random
survey.
Solution:
No. Again, deﬁne Xi.
Xi =
(
n
k
w.p. p
0
o.w.
Note that E[Xi] = np
k and var(Xi) = n
k p(1−p). Compute An = Sn
n .
E[An] = p
k
var(An) = 1
n2 var(Sn) = 1
nvar(Xi) = p(1 −p)
k
Note that
Pr(∥An −E[An]∥> ϵ) = Pr(∥An −p
k ∥> ϵ) ≤p(1 −p)
kϵ2
which is constant, regardless of n. Thus, this does not obey the Law
of Large Numbers.
Page 56

Probability Theory and Random Processes
aaalv.in/abcPTRP
8.6
Markov Chains
1. Bob is walking along a small bridge, 3 feet wide and 10 feet long. Every
time step, he walks 1 foot along the bridge. With probability p, he walks
straight forward, and otherwise, he takes one step 1 foot forward and 1
foot to the right with probability 1
2, forward 1 foot and left 1 foot with
probability 1
2. If he walks oﬀthe bridge, he falls into the water and foregoes
the bridge to get to the other side. How many time steps do we expect
Bob to walk before falling in the water, assuming he starts at the center
of the bridge?
Solution: Consider a ﬁve-state Markov Chain. With X1, X2, X3, X4, X5,
where X1, X5 represent falling into water.
1
2
3
4
5
p
p
p
This is a hitting time problem, where our targets under consideration are
both X1, X5.
We will compute the time until falling into the water, for each state, β(i).
β(1) = 0
β(2) = 1 + 1 −p
2
β(1) + 1 −p
2
β(3) + pβ(2)
β(3) = 1 + 1 −p
2
β(2) + 1 −p
2
β(4) + pβ(3)
β(4) = 1 + 1 −p
2
β(3) + 1 −p
2
β(5) + pβ(4)
β(5) = 0
This is simply a linear system of equations, so we rearrange all constants
onto one side and plug into a matrix form to row reduce.
0 = β(1)
−1 = 1 −p
2
β(1) + (p −1)β(2) + 1 −p
2
β(3) + 0 + 0
−1 = 0 + 1 −p
2
β(2) + (1 + p)β(3) + 1 −p
2
β(4) + 0
−1 = 0 + 0 + 1 −p
2
β(3) + (p −1)β(4) + 1 −p
2
β(5)
0 = β(5)
This is equivalent to the following augmented matrix:
Page 57

Probability Theory and Random Processes
aaalv.in/abcPTRP


1
0
0
0
0
0
(1 −p)/2
p −1
(1 −p)/2
0
0
−1
0
(1 −p)/2
p −1
(1 −p)/2
0
−1
0
0
(1 −p)/2
p −1
(1 −p)/2
−1
0
0
0
0
1
0


Row reducing, we get the following matrix.


1
0
0
0
0
0
0
1
0
0
0
3/(1 −p)
0
0
1
0
0
4/(1 −p)
0
0
0
1
0
3/(1 −p)
0
0
0
0
1
0


Starting in the center of the bridge, we expect him to fall into the water
after
4
1−p time steps.
2. Let us consider the following scenarios. Use Markov Chains only when
necessary, but consider your intuition ﬁrst. Justify all of your answers.
(a) Consider a coin with bias p. What is the probability that we see TH
before HH?
Solution:
We can use intuition. If we roll T at any point, we must
see TH before HH; the probability would be 1. We can thus only
fail if we see HH in the ﬁrst two rolls. This is the complement of
rolling two heads in the ﬁrst two rolls.
1 −p2
(b) Consider a fair dice. What is the probability that we see 5-3 before
seeing 3-5?
Solution:
Consider all possible pairs of numbers, as our states.
Note that the probability of reaching 5-3 from 3-5 is the same as the
probability of reaching 3-5 from 5-3. By symmetry, we then expect
the probability of hitting 5-3 before 3-5 to be
1
2
3. Consider a die.
(a) Let this die be fair. How much time do we expect until we roll 5
dots, followed by a roll of 3 dots?
Solution:
We note that for fair die, we expect 6 rolls until we see
a 5. For each of those rolls, we expect 6 more rolls until we see a 3.
Thus, we expect 36 rolls until we see a sequence 5-3.
Page 58

Probability Theory and Random Processes
aaalv.in/abcPTRP
(b) How many 5s do we expect to see before we roll 5 dots, then 3 dots?
Solution:
We computed earlier that we expect the sequence of 5-3
to appear after 36 rolls. We know that the last two rolls are 5 and 3.
Thus, we only have randomness in the ﬁrst 34 rolls. We expect 1
6 of
these to be 5s. Thus, we expect 17
3 + 1 5s.
(c) Let this die be loaded, favoring 5 with probability
6
11. All others are
have equal probability, or
1
11. How much time do we expect until we
roll 5 dots, followed by a roll of 3 dots?
Solution:
First, let us consider the number of rolls it takes to see
5-3. Consider X1, X2, X3, where X1 is any set of two rolls that does
not start with a 5, X2 is any set of two rolls that starts with a 5,
and X3 is the state with 5-3. We consider time until X3, and list our
balance equations.
β(1) = 1 + 5
11β(1) + 6
11β(2)
β(2) = 1 + 4
11β(1) + 6
11β(2) + 1
11β(3)
β(3) = 0
We eﬀectively have two unknowns and two equations, we can solve
by hand. Rewrite both equations
x1 = 11
6 + x2
x2 = 11
5 + 4
5x1
Plug the second into the ﬁrst equation and solve for x1.
x1 = 11
6 + 11
5 + 4
5x1
1
5x1 = 121
30
x1 = 121
25 = (11
5 )2
Our answer is thus the following.
(11
5 )2
Page 59

Probability Theory and Random Processes
aaalv.in/abcPTRP
(d) Again with the loaded die, how many 5s do we expect to see before
our ﬁrst sequence of 5-3?
(Note that we cannot simply take the
answer from the previous part and multiply by 1
6. This is because
the duration of survival is positively correlated with the number of
5s we expect to see.)
Solution:
Using the states deﬁned in the previous part, we modify
our balance equations. Instead of counting time steps, we are now
counting the number of 5s that show up. Thus, we add 1 anytime
we exit node X2. Otherwise, the setup looks identical to that of our
equations in the previous part.
β(1) = 5
11β(1) + 6
11β(2)
β(2) = 1 + 4
11β(1) + 6
11β(2) + 1
11β(3)
β(3) = 0
Again, we can solve by hand.
x1 = x2
x2 = 11
5 + 4
5x1
Plug the second equation into the ﬁrst to ﬁnd x1, and we have
x1 = 11
5 + 4
5x1 = 11
Thus, the number of 5s we expect to see is 11.
Page 60

Probability Theory and Random Processes
aaalv.in/abcPTRP
8.7
Transformations
1. Find the PDF of Z given the following moment-generating function of z.
MZ(s) = 2
3
5
5 −s + 1
3
es2 −es
s
Solution: We have 2
3 probability of assuming an E ∼Expo(5). Otherwise,
we assume a shifted uniform U ∼[1, 2].
Our PDF is thus the following.
fZ(z) = 2
35e−5z + 1
3
1
3 = 3
55e−5z + 1
9
2. Let X be the time until lightbulb 1 dies, where its average lifetime is
λ1. Let Y be the time until lightbulb 2 dies, where its average lifetime
is λ2. Without applying linearity of expectation, ﬁnd the expectation of
Z = X + Y .
Solution:
X, Y are independent, so MZ(s) = MX(s)MY (s).
MZ(s) =
λ1
λ1 −s
λ2
λ2 −s
To compute expectation, we take the derivative once, and evaluate at
s = 0
λ1
λ1 −s
λ2
(λ2 −s)2 +
λ1
(λ1 −s)2
λ2
λ2 −s|s=0
= 1
λ2
+ 1
λ1
Page 61

