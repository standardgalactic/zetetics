BAYESIAN ADAPTIVE SAMPLING FOR DISCRETE
DESIGN ALTERNATIVES IN CONCEPTUAL DESIGN
A Thesis
Presented to
The Academic Faculty
by
Jos´e Eugenio Valenzuela del R´ıo
In Partial Fulﬁllment
of the Requirements for the Degree
Doctor of Philosophy in the
School of Aerospace Engineering
Georgia Institute of Technology
December 2013
Copyright c⃝2013 by Jos´e Eugenio Valenzuela del R´ıo

BAYESIAN ADAPTIVE SAMPLING FOR DISCRETE
DESIGN ALTERNATIVES IN CONCEPTUAL DESIGN
Approved by:
Professor Dimitri Mavris, Advisor
School of Aerospace Engineering
Georgia Institute of Technology
Professor Daniel Schrage
School of Aerospace Engineering
Georgia Institute of Technology
Professor Brian German
School of Aerospace Engineering
Georgia Institute of Technology
Professor Sankaran Mahadevan
School of Engineering
Vanderbilt University
Professor J. V. R. Prasad
School of Aerospace Engineering
Georgia Institute of Technology
Date Approved: 21 August 2013

A mis padres.
iii

ACKNOWLEDGEMENTS
I want ﬁrst to thank my advisor, Dr. Mavris, for his wise advice and support that
allow me to do research, write this thesis, and be more prepared for the professional
world. I also thank Dr. German, Dr. Prasad, Dr. Schrage, and Dr. Mahadevan for
their valuable feedback and recommendations provided as committee members.
I also express my gratitude to Dr. Chung Lee, Dr. Kyle Collins, Dr. Miguel
Walter, and Dr. Jonathan Murphy for their advice and assistance on my research.
Also to my colleagues Rajiv Shenoy, Sylvester Ashok, and Alek Gavrilovski.
My appreciation to “Caja Madrid” and “La Caixa” fellowships that have fully
funded me for the past four years.
I would like to thank all the people that have added a personal dimension to my
time in Atlanta.
Finally, a big thanks to my father, my mother, my sister, my brother, my niece,
and my nephew for their unconditional love and support that make life much easier.
¡Muchas gracias!
iv

TABLE OF CONTENTS
DEDICATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iii
ACKNOWLEDGEMENTS . . . . . . . . . . . . . . . . . . . . . . . . . .
iv
LIST OF TABLES
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
x
LIST OF FIGURES
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xi
GLOSSARY
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xvi
SUMMARY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiv
I
INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Meta-Modeling in Conceptual Design. State of the Art . . . . . . . .
6
1.3
Adaptive Sampling. State of the Art . . . . . . . . . . . . . . . . . .
12
1.4
Multi-Objective Optimization. State of the Art . . . . . . . . . . . .
16
1.5
Rotor-craft Design . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.6
Proposed Approach . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.6.1
Research Questions . . . . . . . . . . . . . . . . . . . . . . .
27
1.7
Practical Applications . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.8
Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
II
BACKGROUND AND THEORY REVIEW . . . . . . . . . . . . .
36
2.1
Discrete Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.1.1
Deﬁnition of Order and Distance . . . . . . . . . . . . . . . .
38
2.2
Nominal Distance Functions
. . . . . . . . . . . . . . . . . . . . . .
38
2.2.1
Classiﬁcation Techniques: Nominal Distances . . . . . . . . .
39
2.2.2
Correlation Functions: Categorical Variables
. . . . . . . . .
42
2.3
Types of Estimates. Reuse of Data in Engineering . . . . . . . . . .
44
2.4
Conceptual Design . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.5
Gaussian Meta-Models: Kriging
. . . . . . . . . . . . . . . . . . . .
48
v

2.5.1
Kriging Regression . . . . . . . . . . . . . . . . . . . . . . . .
54
2.5.2
Kriging Re-interpolation
. . . . . . . . . . . . . . . . . . . .
56
2.6
Multi-Fidelity Meta-Models: Gaussian Approach . . . . . . . . . . .
57
2.6.1
Multi-Fidelity Regression . . . . . . . . . . . . . . . . . . . .
62
2.6.2
Multi-Fidelity Re-interpolation . . . . . . . . . . . . . . . . .
64
2.7
Adaptive Sampling
. . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.7.1
Towards the Expected Improvement Inﬁll Criterion
. . . . .
67
2.8
Mixed-Integer Optimization . . . . . . . . . . . . . . . . . . . . . . .
70
2.8.1
Convex Programming . . . . . . . . . . . . . . . . . . . . . .
73
2.8.2
Non-Convex Programming
. . . . . . . . . . . . . . . . . . .
75
2.8.3
Stochastic or Random Search . . . . . . . . . . . . . . . . . .
76
2.9
Multi-Objective Expected Improvement . . . . . . . . . . . . . . . .
79
2.9.1
Hyper-Volume Deﬁnition and its Calculation . . . . . . . . .
83
2.9.2
Expected Improvement Based on Dominated Hyper-Volume .
84
2.10 Fenestron Tail Rotors . . . . . . . . . . . . . . . . . . . . . . . . . .
89
III RESEARCH METHODOLOGY
. . . . . . . . . . . . . . . . . . . .
91
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
3.2
Research Questions: Hypothesis and Predictions . . . . . . . . . . .
94
3.2.1
First Question . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.2.2
Second Question . . . . . . . . . . . . . . . . . . . . . . . . .
96
3.2.3
Third Question
. . . . . . . . . . . . . . . . . . . . . . . . .
98
3.3
Nominal Distance for Cross-Using Observations . . . . . . . . . . . .
99
3.3.1
Requirements for Nominal Distance . . . . . . . . . . . . . .
99
3.3.2
Integer Nominal Distance . . . . . . . . . . . . . . . . . . . .
100
3.3.3
Hamming Nominal Distance
. . . . . . . . . . . . . . . . . .
101
3.3.4
Intrinsic Nominal Distance . . . . . . . . . . . . . . . . . . .
102
3.4
MIC Surrogate: Leveraging Similar Trends across Categories
. . . .
106
3.4.1
Performance Indicators for MIC Surrogates . . . . . . . . . .
108
vi

3.5
ECMF Surrogate: Leveraging Similar Trends from Previous Concepts 111
3.5.1
Performance Indicators for ECMF Surrogates . . . . . . . . .
115
3.6
Adaptive Sampling of MIC and ECMF Surrogates . . . . . . . . . .
115
3.6.1
Performance Indicators for Adaptive Sampling . . . . . . . .
116
3.6.2
Mixed-Integer-Categorical Genetic Algorithm . . . . . . . . .
117
3.6.3
Mutation Study . . . . . . . . . . . . . . . . . . . . . . . . .
121
3.7
Fenestron Conﬁguration as the New Concept . . . . . . . . . . . . .
127
3.7.1
Fenestron Baseline Values . . . . . . . . . . . . . . . . . . . .
127
3.7.2
Weight Estimation . . . . . . . . . . . . . . . . . . . . . . . .
129
3.8
Research Methodology Diagrams . . . . . . . . . . . . . . . . . . . .
133
IV FLIGHTLAB UH60A MODEL . . . . . . . . . . . . . . . . . . . . . 137
4.1
UH60A Parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . .
138
4.2
Hover Model Validation . . . . . . . . . . . . . . . . . . . . . . . . .
138
4.2.1
Fenestron Modeling in FLIGHTLAB . . . . . . . . . . . . . .
141
4.3
Forward Model Validation
. . . . . . . . . . . . . . . . . . . . . . .
142
4.4
UH60A Power Consumption as a Noisy Function . . . . . . . . . . .
144
V
MIC EXPERIMENTS: TRAINING SIZE AND NOMINAL DIS-
TANCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.1
Disturbed Branin Function . . . . . . . . . . . . . . . . . . . . . . .
148
5.1.1
Inﬂuence of Training Size on Performance . . . . . . . . . . .
149
5.1.2
Inﬂuence of Nominal Distance on Performance . . . . . . . .
151
5.2
UH60A Hover Shaft Power. Screened Model
. . . . . . . . . . . . .
155
5.2.1
Screening of UH60A Hover Shaft Power . . . . . . . . . . . .
155
5.2.2
Landscapes of Screened Domain . . . . . . . . . . . . . . . .
157
5.2.3
Inﬂuence of Training Size on Performance . . . . . . . . . . .
161
5.2.4
Inﬂuence of Nominal Distance on Performance . . . . . . . .
164
VI MICGA AS THE EGO INFILL CRITERION SOLVER
. . . . . 167
6.1
Validation Mixed-Integer Genetic Algorithm
. . . . . . . . . . . . .
168
vii

6.2
Adaptive Sampling of the Disturbed Branin Function
. . . . . . . .
170
6.3
Adaptive Sampling of the UH60A Hover Shaft Power
. . . . . . . .
172
6.3.1
EGO Update Location
. . . . . . . . . . . . . . . . . . . . .
173
6.4
Study of Mutation Strategies . . . . . . . . . . . . . . . . . . . . . .
179
VII ECMF EXPERIMENTS: NEW CONCEPT AND OLD TRAIN-
ING SIZE
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
7.1
Michalewicz Canonical Function . . . . . . . . . . . . . . . . . . . .
185
7.1.1
Inﬂuence of New Concept Training Size on Performance . . .
185
7.1.2
Inﬂuence of Previous Concept Training Size on Performance .
187
7.2
UH60A with Fenestron Tail Hover Shaft Power. Screened Domain
.
190
7.2.1
Inﬂuence of New Concept Training Size on Performance . . .
193
7.2.2
Inﬂuence of Previous Concept Training Size on Performance .
196
VIIIDEMONSTRATING MIC ADAPTIVE SAMPLING ON ROTOR-
CRAFT PRACTICAL SCENARIO
. . . . . . . . . . . . . . . . . . 199
8.1
Multi-objective Adaptive Sampling of the UH60A Shaft Power. Screened
Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
8.2
Multi-objective Adaptive Sampling of the UH60A Shaft Power. Full
Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
IX DEMONSTRATING ECMF ADAPTIVE SAMPLING ON ROTOR-
CRAFT PRACTICAL SCENARIO
. . . . . . . . . . . . . . . . . . 210
9.1
Multi-objective Adaptive Sampling of the UH60A with Fenestron
Shaft Power. Screened Domain . . . . . . . . . . . . . . . . . . . . .
210
9.2
Multi-objective Adaptive Sampling of the UH60A with Fenestron
Shaft Power. Full Domain . . . . . . . . . . . . . . . . . . . . . . . .
218
X
SUMMARY, CONTRIBUTIONS, AND RECOMMENDATIONS 221
10.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
10.2 Contributions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
10.3 Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
10.3.1 MIC Surrogate . . . . . . . . . . . . . . . . . . . . . . . . . .
227
10.3.2 ECMF Surrogate
. . . . . . . . . . . . . . . . . . . . . . . .
228
viii

10.4 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
APPENDIX A
— DISTURBED BRANIN FUNCTION
. . . . . . 230
APPENDIX B
— CANONICAL CONCEPTS WITH SIMILAR TRENDS
233
APPENDIX C
— MIXED-INTEGER CANONICAL PROBLEMS 234
REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
ix

LIST OF TABLES
1
Fenestron Baseline Values
. . . . . . . . . . . . . . . . . . . . . . . .
130
2
Parameters of the UH60A Baseline Model [23] . . . . . . . . . . . . .
138
3
Parameters of the UH60A Baseline Rotors [23] . . . . . . . . . . . . .
139
4
Values of Constant α for the Fitting Curves of Rms of the “Standard-
ized Validation Error”.
Independent vs MIC Surrogate.
Disturbed
Branin Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
5
Values of Constant α for the Fitting Curves of Rms of the “Standard-
ized Validation Error”. All MIC Surrogates. Disturbed Branin Function155
6
Values of Constant α for the Fitting Curves of Rms of the “Standard-
ized Validation Error”. Independent vs MIC Surrogate. UH60A shphov
Screened Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
7
Values of Constant α for the Fitting Curves of Rms of the “Standard-
ized Validation Error”. All MIC Surrogates. UH60A shphov Screened
Domain
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
8
Solutions Obtained by Using Modiﬁed MATLAB R
⃝GA and Restricted-
Source MATLAB R
⃝Mixed-Integer GA . . . . . . . . . . . . . . . . . .
169
9
Solution EGO Runs on UH60A shphov. MIC Surrogate . . . . . . . .
173
10
Values of Constant α for the Fitting Curves of Rms of the “Standard-
ized Validation Error”. Mono-ﬁdelity vs ECMF Surrogate. Michalewicz
Function.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
11
Values of Constant α for the Fitting Curves of the Rms of the “Stan-
dardized Validation Error”. All ECMF Surrogates. Michalewicz Func-
tion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
12
Values of Constant α for the Fitting Curves of the Rms of the “Stan-
dardized Validation Error”. Mono vs ECMF Surrogate. UH60A with
Fenestron shpfen
hov Screened Domain
. . . . . . . . . . . . . . . . . . .
195
13
Values of Constant α for the Fitting Curves of the Rms of the “Stan-
dardized Validation Error”. All ECMF Surrogates. UH60A with Fen-
estron shpfen
hov Screened Domain
. . . . . . . . . . . . . . . . . . . . .
198
14
Categorical Minimum Values and their Location for the Disturbed
Branin Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
x

LIST OF FIGURES
1
Net Thrust vs Airspeed. Several Engines . . . . . . . . . . . . . . . .
10
2
Giromill CT vs Tip Speed Ratio for Several Airfoils [52] . . . . . . . .
22
3
Advantages and Disadvantages of Estimate Types . . . . . . . . . . .
46
4
Conceptual Design Wheel [151]
. . . . . . . . . . . . . . . . . . . . .
47
5
Bayesian Predictive Distribution . . . . . . . . . . . . . . . . . . . . .
49
6
Expected Improvement . . . . . . . . . . . . . . . . . . . . . . . . . .
69
7
Pareto Front Example
. . . . . . . . . . . . . . . . . . . . . . . . . .
82
8
Expected Improvement Integration Area for Two-Objective Case . . .
88
9
Process for ECMF Surrogate . . . . . . . . . . . . . . . . . . . . . . .
114
10
Qualities of the MIO Methods . . . . . . . . . . . . . . . . . . . . . .
118
11
Diagram of Adaptive Sampling on MIC Surrogates
. . . . . . . . . .
133
12
Diagram of Adaptive Sampling on ECMF Surrogates . . . . . . . . .
134
13
MIC Methodology Review . . . . . . . . . . . . . . . . . . . . . . . .
135
14
ECMF Methodology Review . . . . . . . . . . . . . . . . . . . . . . .
136
15
Total CP vs CW. Validation of UH60A Hover Shaft Power. FLIGHT-
LAB Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
16
Total CP vs Advance Ratio. Validation of UH60A Forward Flight Shaft
Power. FLIGHTLAB Model . . . . . . . . . . . . . . . . . . . . . . .
143
17
Noise on UH60A shphov. Lx1 = Lx2 = 10−2 . . . . . . . . . . . . . . .
145
18
Fast Fourier Transform UH60A shphov
. . . . . . . . . . . . . . . . .
146
19
Fast Fourier Transform UH60A shphov. Close-Up to Large Frequency
146
20
Contours of Disturbed Branin Function . . . . . . . . . . . . . . . . .
148
21
Surrogate “Standardized Validation Error”. MIC vs Independent Sur-
rogate. Disturbed Branin Function
. . . . . . . . . . . . . . . . . . .
150
22
Surrogate “Standardized Validation Residual”. MIC vs Independent
Surrogate. Disturbed Branin Function
. . . . . . . . . . . . . . . . .
152
23
Surrogate “Standardized Validation Error”. Comparison Several MIC
surrogates. Disturbed Branin Function . . . . . . . . . . . . . . . . .
154
xi

24
Limits of the Design Space to Screen
. . . . . . . . . . . . . . . . . .
156
25
Screening Results of the UH60A Hover Power Consumption
. . . . .
157
26
UH60A Total shphov versus θ1 and θ2. c = 0.750¯c
. . . . . . . . . . .
158
27
UH60A Total shphov versus θ1 and θ2. c = 0.917¯c
. . . . . . . . . . .
159
28
UH60A Total shphov versus θ1 and θ2. c = 1.083¯c
. . . . . . . . . . .
159
29
UH60A Total shphov versus θ1 and θ2. c = 1.250¯c
. . . . . . . . . . .
160
30
Surrogate “Standardized Validation Error”. MIC vs Independent Sur-
rogate. UH60A shphov Screened Domain
. . . . . . . . . . . . . . . .
161
31
Surrogate “Standardized Validation Residual”. MIC vs Independent
Surrogate. UH60A shphov Screened Domain
. . . . . . . . . . . . . .
163
32
Surrogate “Standardized Validation Error”. Comparison Several MIC
surrogates. UH60A shphov Screened Domain . . . . . . . . . . . . . .
166
33
Adaptive Sampling Disturbed Branin Function. Warm-up Size 54. Up-
dates 40. MIC Nominal Metric Hamming.
. . . . . . . . . . . . . . .
170
34
Disturbed Branin Function.
Warm-up Size 54.
Updates 40.
MIC
Nominal Metric Integer.
. . . . . . . . . . . . . . . . . . . . . . . . .
171
35
Adaptive Sampling of the UH60A shphov. Warm-up Size 32. Updates
30. c = 0.9¯c. MIC Surrogate . . . . . . . . . . . . . . . . . . . . . . .
174
36
Adaptive Sampling of the UH60A shphov. Warm-up Size 52. Updates
20. c = 0.9¯c. MIC Surrogate . . . . . . . . . . . . . . . . . . . . . . .
175
37
Adaptive Sampling of the UH60A shphov. Warm-up Size 52. Updates
30. c = 0.9¯c. MIC Surrogate . . . . . . . . . . . . . . . . . . . . . . .
175
38
Adaptive Sampling of the UH60A shphov. Warm-up Size 66. Updates
20. c = 0.9¯c. MIC Surrogate . . . . . . . . . . . . . . . . . . . . . . .
176
39
Adaptive Sampling of the UH60A shphov. Warm-up Size 84. Updates
10. c = 0.9¯c. MIC Surrogate . . . . . . . . . . . . . . . . . . . . . . .
177
40
Adaptive Sampling of the UH60A shphov. Warm-up Size 136. Updates
10. c = 0.9¯c. MIC Surrogate . . . . . . . . . . . . . . . . . . . . . . .
177
41
Statistics of Solution Error for Five Disturbed Branin Problems. MICGA
Mutation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
42
Statistics of Generation of the Last Improvement Larger than 1% for
Five Disturbed Branin Problems. MICGA Mutation Study . . . . . .
181
43
Statistics of Solution Error for ExI Optimization with Several Updates.
MICGA Mutation Study . . . . . . . . . . . . . . . . . . . . . . . . .
182
xii

44
Statistics of Generation of the Last Improvement Larger than 1% for
ExI Optimization with Several Updates. MICGA Mutation Study . .
183
45
Concept 2 Surrogate “Standardized Validation Error”. ECMF N1,tr.set =
7 vs Mono-Fidelity Surrogate. Michalewicz Function. . . . . . . . . .
186
46
Concept 2 Surrogate “Standardized Validation Residual”. ECMF N1,tr.set =
7 vs Mono-Fidelity Surrogate. Michalewicz Function. . . . . . . . . .
186
47
Concept 2 Surrogate “Standardized Validation Error”. ECMF Several
N1,tr.set. Michalewicz Function.
. . . . . . . . . . . . . . . . . . . . .
188
48
UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 0.750¯c . . .
191
49
UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 0.917¯c . . .
192
50
UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 1.083¯c . . .
192
51
UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 1.250¯c . . .
193
52
Concept 2 Surrogate “Standardized Validation Error”. ECMF vs Mono-
Fidelity Surrogate. UH60A with Fenestron shpfen
hov . . . . . . . . . . .
194
53
Concept 2 Surrogate “Standardized Validation Residual”. UH60A with
Fenestron shpfen
hov
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
54
Concept 2 Surrogate “Standardized Validation Error”. ECMF Several
N1,tr.set. UH60A with Fenestron shpfen
hov . . . . . . . . . . . . . . . . .
197
55
Non-Dominated Set Obtained from EGO Algorithm Applied on MIC
and Independent Surrogates. UH60A shp. Warm-Up Size 38 . . . . .
202
56
Evolution Non-Dominated Set Obtained from EGO Algorithm Applied
on MIC and Independent Surrogates. UH60A shp. Warm-Up Size 38
202
57
Average Distance to Pareto Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 36. Updates 40
203
58
Number Points Non-Dominated Set UH60A shp. EGO Algorithm Ap-
plied on MIC and Independent Surrogates. Warm-Up Size 36. Updates
40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
59
Average Distance to Pareto Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 48. Updates 40
204
60
Number Points Non-Dominated Set UH60A shp. EGO Algorithm Ap-
plied on MIC and Independent Surrogates. Warm-Up Size 48. Updates
40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
61
Average Distance to Pareto Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 66. Updates 40
205
xiii

62
Number Points Non-Dominated Set UH60A shp. EGO Algorithm Ap-
plied on MIC and Independent Surrogates. Warm-Up Size 66. Updates
40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
63
Pareto Fronts of the UH60A with Conventional Tail for the Large and
Screened Domain. EGO Algorithm on MIC Surrogates . . . . . . . .
208
64
Close-Up of the Pareto Fronts of the UH60A with Conventional Tail
for the Large and Screened Domain. EGO Algorithm on MIC Surrogates.209
65
“Real” Pareto Front Obtained from NSGA-II. UH60A with Fenestron
shp. Airfoil “SC 1095” . . . . . . . . . . . . . . . . . . . . . . . . . .
212
66
Non-Dominated Set Obtained from EGO Algorithm Applied on ECMF
and Mono-Fidelity Surrogates. UH60A with Fenestron shp. New Con-
cept Warm-Up Size 66. Reuse of 20 Old Concept Observations
. . .
213
67
Evolution Non-Dominated Set Obtained from EGO Algorithm Applied
on ECMF and Mono-Fidelity Surrogates. UH60A with Fenestron shp.
New Concept Warm-Up Size 66. Reuse of 20 Old Concept Observations 214
68
Average Distance to Pareto set UH60A with Fenestron shp.
EGO
Algorithm Applied on ECMF and Mono-Fidelity Surrogates.
New
Concept Warm-Up Size 66. Reuse of 20 Old Concept Observations.
Updates 40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
69
Number Points Non-Dominated Set UH60A with Fenestron shp. EGO
Algorithm Applied on ECMF and Mono-Fidelity Surrogates.
New
Concept Warm-Up Size 66. Reuse of 20 Old Concept Observations.
Updates 40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
70
Average Distance to Pareto set UH60A with Fenestron shp. EGO Al-
gorithm Applied on ECMF and Mono-Fidelity Surrogates. New Con-
cept Warm-Up Size 120. Reuse of 20 Old Concept Observations. Up-
dates 40 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
71
Number Points Non-Dominated Set UH60A with Fenestron shp. EGO
Algorithm Applied on ECMF and Mono-Fidelity Surrogates.
New
Concept Warm-Up Size 120. Reuse of 20 Old Concept Observations.
Updates 40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
72
Average Distance to Pareto set UH60A with Fenestron shp. EGO Al-
gorithm Applied on ECMF and Mono-Fidelity Surrogates. New Con-
cept Warm-Up Size 180. Reuse of 20 Old Concept Observations. Up-
dates 40 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
xiv

73
Number Points Non-Dominated Set UH60A with Fenestron shp. EGO
Algorithm Applied on ECMF and Mono-Fidelity Surrogates.
New
Concept Warm-Up Size 180. Reuse of 20 Old Concept Observations.
Updates 40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
74
Pareto Front of the UH60A with Fenestron for the Large and Screened
Domain. EGO Algorithm Applied on ECMF Surrogates
. . . . . . .
220
75
Example of Contours of Disturbed Branin Function. 6 Categories
. .
232
76
The Two Concepts of the Michalewicz Function . . . . . . . . . . . .
233
xv

GLOSSARY
AABO
Adaptive Approximation-Based Optimization, p. 13.
ANN
Artiﬁcial Neural Networks, p. 8.
BB
Branch and Bound, p. 70.
CFD
Computational Fluid Dynamic, p. 4.
CSD
Computational Structural Dynamics, p. 54.
DoE
Design of Experiments, p. 4.
ECMF
Evolutionary Concept Multi-Fidelity Surrogate, p. 33.
ECP
Extended Cutting Plane, p. 72.
EGO
Eﬃcient Global Optimization, p. 14.
EMOA
Evolutionary Multi-Objective Algorithm, p. 17.
ES
Evolution Strategies, p. 76.
ExI
Expected Improvement, p. 14.
FFT
Fast Fourier Transform, p. 144.
GA
Genetic Algorithm, p. 18.
GBD
Generalized Bender Decomposition, p. 72.
GRFM
Gaussian Random Field Metamodel, p. 8.
IP
Integer Programming, p. 72.
k-NN
k Nearest Neighbors, p. 41.
MIC
Mixed-Integer-Categorical Surrogate, p. 33.
MICGA
Mixed-Integer-Categorical Genetic Algorithm, p. 34.
MIGA
Mixed-Integer Genetic Algorithm, p. 97.
MILP
Mixed-Integer Linear Problem, p. 71.
MINLP
Mixed-Integer Non-Linear Problem, p. 71.
MIO
Mixed-Integer Optimization, p. 15.
MIP
Mixed-Integer Programming, p. 71.
xvi

MLE
Maximum Likelihood Estimation, p. 50.
M&S
Modeling and Simulation, p. 45.
NLP
Non-Convex Programming, p. 72.
OA
Outer Approximation, p. 72.
PoI
Probability of Improvement, p. 14.
rms
Root Mean Squared, p. 109.
SVM
Support Vector Machine, p. 8.
VDM
Value Diﬀerence Metric, p. 40.
xvii

Nomenclature
Greek
α
Slope of the logarithmic ﬁt of the rms of the surrogate error vs Ntr.set
∆
Increase of a variable
δ
Diﬀerence between ﬁdelities
ǫ
Error between a stochastic process and its mean µ
λ
Kriging regression constant
λi
Lagrange multiplier for the constraint i
µ
Mean of a stochastic process
µj
Mean of the mutation distribution pj for the parental category j
ρ
Ratio between ﬁdelities
σ
Standard deviation of the Kriging stochastic process error
θ
Kriging hyper-parameter
θ1
Linear twist of the inner part of the main rotor blade
θ2
Linear twist of the outer part of the main rotor blade
Ψ
Kriging correlation matrix
ψ
Kriging correlation vector between the training set points and design x
Ω
Rotor angular velocity
Roman
xviii

AR
Aspect ratio of an aerodynamic surface
b
Span of an aerodynamic surface
C
Multi-ﬁdelity approach correlation matrix
c
Multi-ﬁdelity approach correlation vector between the training set points and
design x
c
Rotor chord
Cd
Airfoil drag coeﬃcient curve
Cl
Airfoil lift curve
CP
Rotor-craft power coeﬃcient
CT
Rotor-craft thrust coeﬃcient
CW
Rotor-craft weight coeﬃcient
d
Diﬀerence between the expensive observations and the cheap surrogate predic-
tions
D
Rotor diameter
d
Distance function
D1
Design space of the previous concept
D2
Design space of the new concept
dj,k
Nominal distance between the categorical members j and k
E [I]
Expected improvement
error Surrogate standardized validation error
xix

exclD2 Exclusive design space of the new concept
F
Pareto Front
f
Objective function
FT
Fast Fourier transform of a function
fx1
Discrete fast Fourier transform frequency in the x1 direction
H
Hyper-volume
I
Identity matrix
I
Improvement with respect to the current best sample point
Ih
One dimensional Hamming distance function
J
Number of categorical properties to deﬁne the nominal intrinsic distance
k
Dimensionality of the design space
L
Likelihood of the observed data
m
Number of objective functions
n
Size of the training set
N1,tr.set Number of training points for the old concept surrogate
N2,tr.set Number of training points for the new concept surrogate
Ntr.set Number of training points of a surrogate
Nb
Number of rotor blades
nd
Number of members of a categorical variable
Neng
Number of engines
xx

Nsampl,x1 Number of samples in the x1 direction
p
Probability mutation distribution
P
Set of points in the objective space
PDF Probability distribution function
PF
Real Pareto front
P [I]
Probability of improvement
pj
k
Probability that the mutation child gets the k-th category given the parent
takes the j-th category
prop
Categorical property to deﬁne the nominal intrinsic distance
q
Number of members in the Pareto set
R
Real numbers
R
Kriging interpolation correlation matrix
R
Rotor radius
rcut-oﬀRadial position of the blade cut-oﬀ
Re
Reynolds Number
res
Surrogate standardized validation residual
rtw
Radial position of the twist change
ˆs2
Kriging mean squared error
S
Feasible objective space
shp
Shaft horsepower
xxi

V
Volume of a design or objective space
V el
Rotor-craft velocity
V ol
Volume function
w
Weighting vector
W
Weight of a system part
W0
Rotor-craft gross weight
X
Training set points
x
Design point
y
Objective values of the training set
Y
Noisy realization of the deterministic model y (x)
y
Objective function value
y⋆
ch
Characteristic change of the objective function in its domain
ymin
Current best sample point
yopt
Optimum of the objective function
ˆy
Kriging prediction
Z
Integer numbers
Z
Gaussian process
Subscript and others
ˆ
Estimation of optimal value
c
Cheap data of the multi-ﬁdelity approach
xxii

cont
Continuous variables
Conv Conventional anti-torque device
d
Additive factor of the multi-ﬁdelity approach
e
Expensive data of the multi-ﬁdelity approach
Fen
Fenestron anti-torque device
for
Forward ﬂight
hov
Hover
hs
Horizontal tail
int
Integer variables
nom
Nominal or categorical variables
opt
Optimal design conditions
quant Discrete-quantitative variables
r
Regression predictor
ri
Reinterpolation predictor
tr
Tail rotor
vt
Vertical tail
xxiii

SUMMARY
The number of technology alternatives has lately grown to satisfy the in-
creasingly demanding goals in modern engineering. These technology alternatives are
handled in the design process as either concepts or categorical design inputs. Ad-
ditionally, designers desire to bring into early design more and more accurate, but
also computationally burdensome, simulation tools to obtain better performing initial
designs that are more valuable in subsequent design stages. It constrains the com-
putational budget to optimize the design space. These two factors unveil the need
of a conceptual design methodology to use more eﬃciently sophisticated tools for
engineering problems with several concept solutions and categorical design choices.
Enhanced initial designs and discrete alternative selection are pursued.
Advances in computational speed and the development of Bayesian adaptive sam-
pling techniques have enabled the industry to move from the use of look-up tables and
simpliﬁed models to complex physics-based tools in conceptual design. These tech-
niques focus computational resources on promising design areas. Nevertheless, the
vast majority of the work has been done on problems with continuous spaces, whereas
concepts and categories are treated independently. However, observations show that
engineering objectives experience similar topographical trends across many engineer-
ing alternatives.
In order to address these challenges, two meta-models are developed. The ﬁrst one
borrows the Hamming distance and function space norms from machine learning and
functional analysis, respectively. These distances allow deﬁning categorical metrics
that are used to build an unique probabilistic surrogate whose domain includes, not
xxiv

only continuous and integer variables, but also categorical ones. The second meta-
model is based on a multi-ﬁdelity approach that enhances a concept prediction with
previous concept observations. These methodologies leverage similar trends seen from
observations and make a better use of sample points increasing the quality of the
output in the discrete alternative selection and initial designs for a given analysis
budget. An extension of stochastic mixed-integer optimization techniques to include
the categorical dimension is developed by adding appropriate generation, mutation,
and crossover operators. The resulted stochastic algorithm is employed to adaptively
sample mixed-integer-categorical design spaces.
The proposed surrogates are compared against traditional independent methods
for a set of canonical problems and a physics-based rotor-craft model on a screened
design space. Next, adaptive sampling algorithms on the developed surrogates are
applied to the same problems.
These tests provide evidence of the merit of the
proposed methodologies. Finally, a multi-objective rotor-craft design application is
performed in a large domain space.
This thesis provides several novel academic contributions. The ﬁrst contribution is
the development of new eﬃcient surrogates for systems with categorical design choices.
Secondly, an adaptive sampling algorithm is proposed for systems with mixed-integer-
categorical design spaces. Finally, previously sampled concepts can be brought to
construct eﬃcient surrogates of novel concepts. With engineering judgment, design
community could apply these contributions to discrete alternative selection and initial
design assessment when similar topographical trends are observed across diﬀerent
categories and/or concepts. Also, it could be crucial to overcome the current cost
of carrying a set of concepts and wider design spaces in the categorical dimension
forward into preliminary design.
xxv

CHAPTER I
INTRODUCTION
1.1
Motivation
In recent times, the aerospace industry has searched for eﬃcient designs and opti-
mization techniques that satisfy the more and more demanding system requirements.
This search emphasized on an increase in performance and reduction in cost. Much
of the work done focuses on continuous techniques. However, real aerospace design
problems normally engage not only continuous variables but also discrete ones, such
as materials, number of blades on a compressor disk, beam cross-sections, standard-
ized bolt diameters, commercially available plate thicknesses, and manufacturing pro-
cesses, among others. Also, diﬀerent design conﬁgurations are developed to meet the
increasing system requirements and trade-oﬀs in the form of several concepts. Typi-
cal concept examples are helicopter rotor conﬁgurations, airplane tail conﬁgurations,
gear-box conﬁgurations, and landing gear types, among others.
In the conceptual design phase, better discrete alternative selection and high per-
forming region bounds are pursued after the sizing of each alternative. There exists a
heavy computational load in current engineering problems because of the diﬃculty of
the phenomenon per se, iterations between diﬀerent disciplines or the designers’ desire
of multi-objective optimization. Traditionally, cheap empirical design tools have been
used in conceptual design; however, they may wrongly select the best concept and
its high-performing design space region. Additionally, these tools cannot accurately
deal with new concepts and extreme geometries in the early design phase which is
the precise time of the process to study new concepts and extreme geometries. Other
option are simple models; however, they do not model complex physics, therefore,
1

they provide poor concept choice and a poor identiﬁcation of the high-performing
regions. Thus, the design community tends to employ more and more faithful models
with the drawback of more time-consuming evaluations: designers have to trade oﬀ
between cost and quality of the model.
Discrete alternatives are handled as either diﬀerent concepts1 or categorical dis-
crete variables2. Discrete variable and concept selection are normally settled in early
design stages because their changes are large by nature so that the impact of these
choices in the design is important. Thus, they need to be correctly deﬁned in early
phases. These key decisions ﬁx a high percentage of the life cycle cost. Variable
changes in later stages of the design process could have a great enhancement in the
design performance but also in the whole design concept, making changes prohibitive
in many cases. Engineers prefer being aware of these desirable changes earlier in the
process where the impact of the change is lower; thus, more and more faithful codes
are used in conceptual design. As opposed to discrete variables, continuous changes
are sometimes small, allowing engineers to accept the change even in downstream
stages of the design process. Regarding discrete alternatives, there is little space for
major changes in these crucial decisions, such as concept and category selection, in
late stages of design. Therefore, this need of faithful codes in early design is even
more crucial when concepts have to be selected and discrete design parameters are
present.
Many engineering eﬀorts attempt to bring computationally intense codes into early
design. These eﬀorts focus on reducing two limitations stemming from the high com-
putational burden of the codes for concept evaluation and the multi-modality land-
scapes of typical objectives. The ﬁrst limitation is the time taken to evaluate design
1The author means by concepts the possible solutions for a given need. Concepts normally have
diﬀerent design space and sometimes have some design variables in common.
2A detailed classiﬁcation of discrete variables can be found in Section 2.1.
2

concepts, whereas the second one is the number of evaluations needed for the opti-
mization in conceptual design. These eﬀorts have been done only in the continuous
world; currently, each discrete alternative is modeled and optimized independently.
In order to reduce the concept evaluation time, engineers employ cheap surrogate
models for each concept when the tool to model the concept is computationally in-
tense. These surrogate models are based on training points. The number of training
points increases dramatically with the dimensionality of the concept design space for
a given accuracy; it is called the curse of dimensionality. Thus, the successful utiliza-
tion of surrogates is restricted to a certain dimensionality of the design space. Design
spaces with high dimensionality require a large number of function calls to build an
accurate surrogate, which can be impractical when using high-ﬁdelity tools. Thus,
more eﬃcient manners to treat surrogate models are needed in these high dimen-
sionality cases. Engineers have developed independent surrogates for each discrete
alternative, so the design space they deal with is continuous. Less often surrogates
for concepts with discrete-quantitative variables have been built.
The second limitation is set because of the multi-modal nature of typical engineer-
ing objective functions. Local optimizers are eﬃcient on smooth uni-modal objective
functions, but their results are not satisfactory when applying to functions with mul-
tiple local optima. Also, some of the engineering objective functions are non-smooth,
which represents another challenge for local solvers. Additionally, normally there is
no prior knowledge of the objective function landscape for each concept. Engineers
employ global optimization methods to ﬁnd optimal performance in the design space
of each concept. As is well known, the global optimization methods lack convergence
speed because they need many more function evaluations than traditional local op-
timizers. Again, these techniques are mainly applied to concepts with continuous
variables; thus, it is desirable their extension to concepts with continuous, discrete-
quantitative, and discrete-categorical variables. These design spaces are known as
3

mixed-integer-categorical ones.
In the case of modestly expensive functions, engineers opt for adaptive sampling
(also called on-line or inﬁll sampling) over traditional design of experiments (DoE).
Adaptive sampling features two possible focuses or a combination of both: the global
accuracy of the model to assure global search (exploration), and the accuracy of the
model in the region of the optimum (exploitation). It allows bounding a region of
high performance that reduces the design time in later stages of the process.
Regarding global model accuracy, sampling in low performance design regions
misuses the resources, especially if design evaluations are costly. Nevertheless, these
evaluations in low performance regions are necessary to brand the regions as low per-
forming. A combination of both focuses, exploration and exploitation, is appropriate
in early design where a compromise solution between global trends search, interaction
between design space variables, and search for an optimum is desired. As a drawback
of the adaptive sampling, there exists a loss of precision in the optimum due to the
global exploration; however, it does not represent a big issue in early design. The ex-
tension of these adaptive sampling techniques to mixed-integer-categorical problems
would allow more eﬃcient tools for scenarios with categorical variables.
Currently, high-ﬁdelity codes like Computational Fluid Dynamic (CFD) are mainly
used in later design stages and rarely employed in early stages. Neither the great in-
crease of processor speed that looks to approach the limits of silicon, nor the blossom
of parallel machine, allows designers to extensively employ high-ﬁdelity codes in early
design, even for single multi-objective design.
Also, some authors have pointed out that there are not only computational prob-
lems but also some profound problems in the use of high-ﬁdelity codes due to uncer-
tainties inherent to the early design phase. In this phase many variables are simpliﬁed
or just not considered in the ﬁrst analysis. These variables will be addressed in later
phases. Also, some variables come into the play in later stages once more details are
4

known about the design.
As a result, analysis from these high-ﬁdelity codes must be judged, specially in
these early design stages where uncertainties are higher. So, seeking for an accurate
numerical value of the optimum in the conceptual design stage is a waste of resources,
because there still exist some parameters left out from model reduction and uncer-
tainty due to unknowns in the design that are settled in the detailed design phase,
see Young et al. [208]. Therefore, using mid-ﬁdelity codes in early design makes more
sense than high-ﬁdelity ones.
Additional computational burden appears when dealing with not only high-ﬁdelity
codes but also multi-objective and multi-disciplinary problems. These engineering
scenarios prompt the need of building surrogates and inﬁll techniques as eﬃcient as
possible. Pareto sets are desired in multi-objective optimization. Since a set of design
solutions is sought, as opposed to a single solution, more optimization eﬀort is usually
required. Also, even coverage in the set of solutions is wished, which is neither simple
nor computationally cheap.
Sometimes engineers need to use multi-disciplinary tools to model complex engi-
neering problems where several disciplines are involved. Modeling these diﬃcult engi-
neering problems require iterative processes that normally result in computationally
intense codes, even when simple physics-based models are utilized for each discipline.
In this case, discrete changes in late design stages are even more prohibitive due to
the interaction between disciplines. Therefore, it is critical to use mid-ﬁdelity tools
to reliably choose a concept and successfully ﬁnd regions of high performance across
the mixed-integer-categorical design space of the concept.
An example of design that involves several disciplines is rotor-craft design, where
interactions between aerodynamic, structural dynamics, and control occur. Addition-
ally, the rotor is a complex-physics problem, therefore, models used should represent
5

the physics with at least some ﬁdelity. Helicopter design include not only continu-
ous variables but also discrete ones (number of blades, type of airfoils, articulated
or hingeless blades, ...). Also, a choice of concepts is involved (rotor conﬁguration,
anti-torque device, ...)
Nowadays, the techniques previously explained (surrogates and adaptive sampling
techniques) enable designers to use computationally intense models in the preliminary
and detailed design phases, where design spaces are small and continuous, and concept
and alternative selection is already made. Nevertheless, these techniques are not ready
for conceptual design stages where there are several alternatives to explore, increasing
the volume of the design space; in this case, the currently available surrogates and
adaptive sampling techniques provide poor accuracy for a given intense function call
budget or need too many function calls for the desired accuracy.
The main goal of the present work is to build surrogates and adaptive sampling
algorithms for a more eﬃcient use of computationally expensive tools in scenarios
with discrete design alternative choices in early design. The work focuses on the part
of conceptual design after each alternative is sized, where it is pursued the exploration
of the alternative design space, the selection of the best concept and category, and the
search for design regions with high performance. An accurate numeric value of the
global optimum is not a priority in conceptual design. Knowing the best concepts and
bounding their regions of high performance allow the designer to narrow the search
in later design phases; it reduces the likelihood of major changes in preliminary and
detailed design, which is especially undesirable for discrete variables and concepts.
1.2
Meta-Modeling in Conceptual Design. State of the Art
Conceptual design is an iterative process where design requirements, trade-oﬀs, and
analysis are used to guide and evaluate conﬁguration arrangements. Conceptual de-
sign outcomes are discrete alternative selection and initial designs. Typical conceptual
6

design spaces are large, so simple models have been used historically in this stage to
select concepts and initial designs. However, more and more attention is turning to-
wards physics-based tools to choose accurately the concept and initial designs. Two
kinds of discrete alternative selection techniques are utilized: qualitative and quanti-
tative.
Qualitative techniques typically rely on prior knowledge and designer experience.
They use simple mathematics. Examples of qualitative methods are group preference
and voting, such as feasibility screening [186], Pugh decision matrices [147], pairwise
comparison charts [48], and analytical hierarchy process [211]. These methods ignore
the potential for each concept to be a parameterization of its own design space, thus,
Pareto fronts are not captured. Also, they cannot properly manage revolutionary
concepts because there is no experience or historical trend about them as stated by
Choi et al. [28]. These setbacks motivate more rigorous methods of selection such as
quantitative ones.
Quantitative methods appear because of the desire of the designers for a more
eﬀective utilization of modeling and simulation in the conceptual design stage, see
[72, 194]. They have a more solid mathematical basis and are based on quantitative
analysis after the sizing of each alternative. Comparison between concepts is replaced
by scalar quantities in the form of objectives that are evaluated with computer mod-
els. They are especially beneﬁcial because more reliable tools could establish that a
concept must be ruled out over another one. Due to their computational expense,
trade-oﬀs between development time and accuracy of the model must be done as
Sobieszczanski-Sobieski and Haftka [177] point out. Among these quantitative meth-
ods one can ﬁnd: morphological matrix [216], sequential multi-objective optimization,
and simultaneous multi-objective optimization [7]. The design community’s focus is
turning to these quantitative methods [163]. Therefore, eﬃcient concept evaluation is
a key factor in early design when quantitative discrete alternative selection techniques
7

are utilized.
For many real world problems, a single simulation can take some time to com-
plete: from minutes to hours, or even days. Routine engineering tasks, such as con-
cept/category selection, design optimization, design space exploration, and sensitivity
analysis, could become too costly. Surrogate modeling appears to alleviate this com-
putational expense. They consist in constructing cheap approximation models to the
objective of a concept that intend to approximate the behavior of the real function.
The most common surrogate models are polynomial response surfaces, Gaussian ran-
dom ﬁeld meta-model (GRFM), support vector machines (SVM), and artiﬁcial neural
networks (ANN). They have been widely used, as it can be seen in the literature. For
more detail see [171, 94, 170, 104].
Surrogates models are widely used together with DoE techniques. These tech-
niques sample the design space in order to get observations that are necessary for the
surrogate construction. DoE is a sampling method that chooses the sampling plan a
priori3. Once the DoE provides the sampling plan, the expensive function is evaluated
at the sampling plan points and the surrogate model is ﬁtted. DoE puts the same
amount of eﬀort in all areas of the design space exhausting sampling capabilities in
low performance regions. Another limitation is that surrogate models based on DoE
sampling plans suﬀer curse of dimensionality: the required number of sample points
for a given accuracy of the meta-model increases exponentially with the number of
design variables.
Regarding discrete-quantitative variables, surrogates have been utilized in mixed-
integer problems, especially in composite structure design. Rikards et al. [153] employ
response surfaces in the latter area. Jansson et al. [92] utilize a GRFM , also known as
3DoE is also called an oﬄine sampling technique.
8

Gaussian processes or Kriging4, combined with DoE sampling in the same ﬁeld. How-
ever, as far as the author knows Kriging models of a concept integrating categorical
and discrete-quantitative design spaces have not been built yet.
Currently, objective functions dependent on continuous, integer and categorical
variables are treated independently for each value of the categorical parameter, i.e.,
a surrogate is ﬁtted for each categorical member where the surrogate domain is made
of the remaining design variables (continuous and discrete-quantitative). An example
of this approach in the aerospace industry is provided by Keane [104]. Boukouvala
et al. [22] show a similar example in the pharmaceutical industry using polynomial
response surfaces and Gaussian random ﬁeld meta-models as surrogates, where a
surrogate is ﬁtted for each category. A technique to include all the possible kinds of
variables, including categorical ones, in the same surrogate would enable wider and
more eﬃcient (more accuracy or fewer intense function evaluations) design searches
and concept/category selection.
Engineers ﬁrst optimize each non-numeric category one at a time. Then, once
all the optimization problems for all the non-numeric categories are solved, the best
optimum across all the categories is selected by comparison.
An instance of this
optimization approach is found by Keane [104]. This current method is clearly poor
for cases where design spaces along the categorical variables are too big to perform a
complete search (too many surrogates should be ﬁtted). In these situations, only some
subset of the categories that is believed to be the best by experience is examined. So,
the accuracy of the obtained optima is constrained by the accuracy of the engineers’
guess.
Likewise, diﬀerent concepts are treated independently from a surrogate point of
view. Therefore, after a concept evolves to a new one to try to meet some requirements
or trade-oﬀs, the previous concept observations are used no longer. In conceptual
4Currently, Kriging methods are wide. They don’t necessarily assume Gaussian ﬁelds.
9

design, concepts continuously evolve to new ones. Designers compare optimal values
and trends for the diﬀerent concepts.
Then, the concept with the most suitable
characteristics for the given requirements is chosen.
It is not rare that observed trends of objective functions for several categories
within the same concept are similar, so some resemblance between behaviors could
be taken advantage of. An example of this situation is the rotor ﬁgure of merit vs
the thrust coeﬃcient for diﬀerent blade airfoils.
Similarly, the trends of an objective function for diﬀerent concepts along common
continuous and discrete-quantitative design parameters are usually similar. In con-
ceptual design, concepts continuously evolve to new ones, thus, it is common that the
topographical behavior of immediate concepts are very much alike. An example of
this behavior is the net thrust of several engines versus airspeed, shown in Figure 1.
Figure 1: Net Thrust vs Airspeed. Several Engines
Hence, advantage of this resemblance could be taken by designers. When sizing
engineering systems and estimating cost and weight, this similarity between several
concepts have been exploited in analogy-based techniques (see [169, 131, 112]) and
regression-based techniques (see [146, 185, 150, 97, 102]). However, none advantage
10

of these similar trends has been taken of when predicting engineering objectives with
medium and high-ﬁdelity models.
Wilson and Martinez [201] handle nominal input5 attributes in instance-based
learning techniques by making use of distance functions. Wilson and Martinez review
some of the more important distance functions: Euclidean, Manhattan, Minkowsky,
Camberra, Chebychev, Quadratic, and Mahalanobis, among others. In order to in-
clude the nominal inputs, the Hamming distance ﬁrst introduced by Hamming [78], is
used. However, Wilson and Martinez’s work is in the ﬁeld of statistical classiﬁcation
techniques within machine learning far from engineering design. Li, Eggermont et al.
[126] make use of the Hamming distance to build their heterogeneous distance with
the purpose of developing evolutionary strategies for domain spaces with all kinds of
design variables.
Designers employ multi-ﬁdelity techniques to build surrogates when models of
several ﬁdelities are available for the concept to explore. A possible instance of this
situation is when a Reynolds Averaged Navier Stokes (RANS) model of a airfoil and
a Euler equations-based model of it are at the designers’ disposal. Alexander et al.
[5] use this example to test their ﬁrst-order multi-ﬁdelity algorithm. A large number
of observations of the lower ﬁdelity model and fewer observations of the higher one
can be combined to augment the accuracy of the surrogate. A similar application for
structure design is tested by Alexander, Lewis et al. [4]. In this case the low-ﬁdelity
model is a ﬁnite element model evaluated over a coarse mesh.
In some instances, the variable ﬁdelity models are deﬁned in diﬀerent design
spaces. Robinson, Eldred et al. [155] present space mapping and corrected space map-
ping methods for variable ﬁdelity models of the same concept with diﬀerent design
spaces. However, this method is developed only for trust-region model-management,
5Categorical variables are also called nominal, non-numeric and symbolic variables.
11

and it requires a large training set for the high-ﬁdelity model to get accurate map-
pings. Also, an overhead calculations in each iteration is needed.
Regarding Bayesian multi-ﬁdelity approaches, Journel and Huijbregts [101] present
an algorithm that combines two variable ﬁdelity Gaussian models. El-Beltagy [51]
applies one-ﬁdelity Gaussian approaches to the augmented training data-set that in-
cludes the low-ﬁdelity approximations to a satellite beam problem (two ﬁdelity models
are available). Hevesi et al. [86] apply it to predict the average annual precipitation
values using a sparse set of precipitation measurements from the site of interest; it
also uses information from additional elevations to improve the estimation accuracy.
Kennedy and O’Hagan [109] use an approach in which the variable ﬁdelity codes
are modeled as Gaussian processes. Forrester, Bressloﬀet al. [63] propose partially
converged high-ﬁdelity simulations as the low-ﬁdelity model in a multi-level scenario.
Later, Forrester, S´obester, and Keane [64] present the geo-statistical method of Krig-
ing to the multi-ﬁdelity world in detail. Han, Zimmermann et al. [79] propose a new
co-Kriging method to study aerodynamic coeﬃcients and drag polars. All the work
found in the literature about Bayesian multi-ﬁdelity approaches deal with the same
concept or engineering conﬁguration but with diﬀerent levels of accuracy which stem
from model reduction, physics simpliﬁcation, or variable discretization size.
1.3
Adaptive Sampling. State of the Art
The even distribution of DoE sampling plans wastes resources by placing samples
in poor performing regions of the design space. Also, large domains and the high
cost of function evaluations do not always make surrogates/DoE methods accurate
for optimization purposes. Adaptive sampling techniques represent an alternative to
the stiﬀnature of surrogate/DoE combination.
A very common adaptive technique builds surrogates on successive reductions of
the design space.
These surrogates are then optimized.
This technique is called
12

adaptive approximation-based optimization (AABO). The surrogate optimization of
a concept on a wide design area permits to reduce the optimization area for the next
step, where a new surrogate is ﬁtted on a smaller portion of design space. In this
technique the adaptive sampling is driven by performance and not by model uncer-
tainty. As a result, its performance is poor in problems where there are several local
optima with similar performance. It is especially targeted to reduce computational
expense in problems with large design domains. It would exploit a high performance
region, but other potential high performance regions would not be explored because
there exists little information about them. Recently Hao and Ying [80] implement
this adaptive approximation-based optimization method; however, categorical design
parameters are not included in their study. Also, Davis and Ierapetritou [37] propose
a hybrid methodology for mixed-integer nonlinear programs, where response surface
methods are locally applied at the most promising Kriging solutions.
More sophisticated adaptive procedures are called on-line, inﬁll, or adaptive sam-
pling.
The idea is to enhance the accuracy of the surrogate model using further
function calls, also known as inﬁll or update points. Two focuses are possible: the
global accuracy of the model to assure global search (exploration), and the accuracy of
the model in the region of the optimum (exploitation). In early design, it is intended
to combine both exploration and exploitation. The idea is to sample low performance
regions enough to brand them as such and to densely sample high performing regions
of the concept design space.
Traditionally, discrete alternatives are adaptively sampled in an independent and
sequential way. Lately, two competing concepts are adaptively sampled simultane-
ously by Rousis [158]; however, he still uses independent surrogates, where there is no
cross-use or reuse of computationally expensive observations. It results in the need
of heavy sampling for each concept to start with accurate surrogates.
In order to carry adaptive sampling out, predictive distributions of the response
13

are needed to guide the sampling. Therefore, Bayesian surrogate models like Kriging
must be used to approximate the objective function of a concept. Sampling points
are chosen according to an optimal probabilistic inﬁll criterion that combines both
exploration and exploitation with certain weights.
Many inﬁll criteria are found in the literature: Cox and John [33] use the statistical
lower bound, Kushner [120] ﬁrst proposes probability of improvement (PoI), Mockus
[136] ﬁrst introduces the expected improvement (ExI), Jones and Welch [100] work
with conditional lower bound, later Jones proposes the goal seeking inﬁll criterion [98],
S´obester et al. [176] present a parallel inﬁll criterion where several inﬁll points are
chosen, later S´obester et al. [175] work on weighted expected improvement criterion...
These techniques are normally employed to overcome the budget limitation when
simulations require long run times. The next sample point is obtained by taking into
account not only the value of the predicted function but also the uncertainty due to
the reduction of the function evaluation cost.
Among Bayesian adaptive sampling techniques, expected improvement is widely
used. Jones et al. [99] build Eﬃcient Global Optimization (EGO) algorithms that
combine Kriging with ExI for continuous variables. In his work, Jones points out some
monotonicity properties of the ExI landscapes. Up to date none adaptive sampling
algorithm is done on mixed-integer-categorical problems.
Bayesian adaptive sampling techniques require the optimization of the inﬁll crite-
rion over the design space of the concept for ﬁnding the next sampling point. As a
result, optimization algorithms for continuous and discrete variables are required to
adaptively sample mixed-integer-categorical design spaces. Among the optimization
methods developed for continuous and discrete variables, the ﬁrst and most straight
forward approach is to treat all discrete variables as continuous ones6; and then,
6If categorical discrete variable are present in the problem, a map is required to convert categorical
variables into discrete-quantitative ones
14

variables are truncated once a continuous optimization technique provides the design
point.
Li, Eggermont et al. [125] state the main drawback for this approach: the contin-
uous step size may reduce to a value that is too small to generate any improvement
in the discrete-quantitative variable treated as continuous. If the discrete design pa-
rameter is a ordinal or nominal value, the result is even worse, the implicit continuous
neighborhood and distance assumption could cause to converge to an artiﬁcial local
optimum.
A well-developed ﬁeld called Mixed-Integer Optimization (MIO) intends to solve
this need for optimization problems, where both continuous and discrete-quantitative
variables are present. MIO is capable to deal with non-numeric variables by mapping
them into integer ones. Other option is to use a set of switches to convert the cat-
egories into binary variables that represent the rejection or selection of a category.
However, this conversion of categorical variables into binary variables increases the
dimensionality of the design space.
Duran and Grossmann [46] build an eﬃcient outer approximation algorithm to
solve the problem of synthesis in chemical engineering, which involves binary and
continuous variables. Boukouvala et al. [21] also employ mixed-integer optimization
techniques for studying blending processes.
In the last case, binary variables are
associated with units in the superstructure. These variables imply the existence or
absence of the system unit and diﬀerentiate between the alternatives [21]. Berman
and Ashraﬁ[14] also use binary variables to select the modules in order to optimize
software system structures. In heat exchanger synthesis, pump network synthesis, and
trim loss minimization same procedure is taken by Floudas et al. [61], Westerlund
et al. [198], and Harjunkoski [81], respectively. Davis and Ierapetritou [37] propose
a methodology for mixed-integer nonlinear programs based on a branch-and-bound
method that uses Kriging and response surfaces of the objective at each node of the
15

binary tree.
Designers have worked on global optimization techniques for mixed-integer-categorical
problems.
He and Prempain [82] use particle swarm methods to optimize design
spaces with continuous, integer, and discrete-quantitative variables but not nominal
variables. Wang and Yin [195] develop a particle swarm optimizer based on ranking
selection, where the ranking selection includes categorical variables.
Also, random searches that require mutation operators have been utilized to op-
timize design spaces with categorical variables by some authors. B¨ack [8] proposes
to use a uniform distribution for the categorical variable. Other authors also opt for
the same mutation distribution for their random searches [57, 127]. Cao and Wu [26]
develop a mixed-variable evolutionary algorithm with diﬀerent mutation operators
for each type of variables. Li, Eggermont et al. [126] build an heterogeneous distance
to enable the mutation in evolutionary strategies for mixed-integer problems.
1.4
Multi-Objective Optimization. State of the Art
Typical engineering system requirements are made of several objectives. If objec-
tive preferences were speciﬁed a priori only a single design point would be selected.
However, designers poorly understand the inﬂuences of objective preferences in early
design, so they would like to know how they aﬀect the ﬁnal decision before choos-
ing a single design [174]; so, designers rely on Pareto fronts. Once the Pareto set is
obtained, weighting between desired goals is decided to get to a trade-oﬀsolution.
Constructing Pareto fronts is diﬃcult. Firstly, a set of solutions instead of a single
solution is wanted. It means that a large optimization eﬀort is required. Secondly,
a wide and continual set is pursued in the objective space. Interpolating between
design parameters and goals is not usually eﬃcient because of the highly nonlinear
relations among design parameters and goals. Therefore, obtaining an even coverage
of the Pareto front is diﬃcult, especially if relatively long time computer runs are
16

employed for each objective.
These two reasons shape the current algorithms to
construct Pareto sets.
There exist two common ways to obtain the Pareto front for a concept. In the ﬁrst
one, weighting functions to combine the objectives into a single objective problem are
chosen; in this case standard single-objective techniques can be brought. Then, the
weighting is varied to get diﬀerent single objective solutions that form the Pareto
set approximation. This approach is slow, one Pareto optimal point per weighting.
Another drawback is that it is not straight-forward to choose the weighting to get to
diﬀerent parts of the Pareto front.
The second methods are population-based schemes or evolutionary multi-objective
algorithms (EMOA). A set of designs is evolved according to dominance and spread
criterion towards the ﬁnal Pareto set. Since population based-schemes deal with a
group of candidate solutions, it seems natural to use them in multi-objective optimiza-
tion problems to ﬁnd the Pareto optimal solutions simultaneously. For more detail,
see [40, 31]. In this case no weighting function is needed. Instances of population-
based methods are NSGA-II developed by Deb [41], and SMS-EMOA developed by
Beume et al. [16], among others.
Adaptive sampling in multi-objective problems has been treated by several au-
thors. Mainly, the work done is on Bayesian adaptive sampling, concretely on the
statistical improvement criterion. Keane [106] extends the single-objective PoI and
ExI to the multi-objective world, the latter one based on the euclidean distance to
the nearest point of the Pareto front. Liu et al. [128] deﬁne a multi-objective ExI
with a weighted-sum over single-objective ExIs. However, Wagner et al. [192] deﬁne
necessary conditions that Keane’s work [106] and Liu’s work [128] do not satisfy.
The hyper-volume or Lebesgue measure7 has been an increasingly popular measure
to compare the quality of two Pareto set approximations. This Lebesgue measure
7Hyper-volume or Lebesgue measure is also known as S-metric
17

maps a set of points to a scalar, and it is ﬁrst introduced as a Pareto set quality
indicator by Zitzler [213]. It measures the size or volume of the objective space that
is dominated by a set of points. A nice feature of the Lebesgue measure is that it
captures the spread along the objective space and the accuracy of the set compared
to the optimal one. Zitzler, Thiele et al. [215] show that the hyper-volume has more
convenient properties than other possible metrics. Its main problem is that the direct
calculation of the Lebesgue measure is computationally expensive: its complexity
is exponential in the number of objectives. Other limitation is the requirement of
objective normalization to assure that the gains in all the objectives have the same
importance.
Several authors have developed algorithms to exactly calculate the Lebesgue mea-
sure [58, 200, 204, 199]. However, for large number of objectives, Monte Carlo inte-
gration is still preferred due to the exponential increase of the computational expense
of the direct algorithms. Additionally, a large number of points in the Pareto ap-
proximating set increases the computational cost of the direct calculation. The main
drawback of the Monte Carlo approximation is its lack of precision. Nevertheless,
While, Hingston et al. [200] carry out a survey in the literature that reveals that the
majority of authors who deal with multiple objectives study four or less objectives,
and none studied problems with more than 12 objectives. Thus, direct hyper-volume
calculation could be preferable for multi-objective problems with a few objectives.
Hyper-volume indicators are extensively employed together with EMOA, such as
genetic algorithms (GA) and evolutionary strategies, to identify Pareto sets for com-
putationally cheap objective functions or deterministic surrogates. Zitzler [213] ﬁrst
proposes the hyper-volume indicator to compare the approximate sets that are output
by diﬀerent EMOA; Knowles and Corne [115] compare the hyper-volume measure to
other metrics employed as outcomes of EMOA; Huband, Hingston et al. [89] suggest
a hyper-volume-based measure for resolving ties during the selection process of an
18

evolutionary strategy; Beume et al. [16] ﬁrst include the maximization of the dom-
inated hyper-volume as the primal selection operator of the genetic algorithm; Igel,
Hansen and Roth [90] employ the hyper-volume contribution as a sorting criterion for
their evolutionary strategy; Zitzler, Brockhoﬀet al. [214] employ the weighted ver-
sion of the hyper-volume to guide the search to desired points such as Pareto extreme
points; and Bader [9] trades oﬀthe accuracy of the hyper-volume indicator with the
computational resources in his evolutionary algorithm.
However, the interest herein is in Bayesian adaptive sampling of computation-
ally burdensome functions instead of the optimization of the cheap objective func-
tions or deterministic surrogates. Speciﬁcally, extensions of single-objective adaptive
sampling criterion to multidimensional spaces. Lately, the increment of dominated
hyper-volume of a set is a possible choice for the scalar to carry out this extension,
as suggested by Emmerich and Giannakoglou’s work [55]. Ponweiser [145] does it in
the form of Statistical Lower Bound. Emmerich and Giannakoglou [55] propose inde-
pendently the same multi-objective extension of the ExI criterion as Ponweiser. The
proposed ExI satisﬁes the necessary conditions set by Wagner [192]. Nevertheless,
the quadrature for the computation of the ExI measure is expensive: Monte Carlo
integration is employed by Emmerich and Giannakoglou [55] with the corresponding
low accuracy. Piecewise numerical integration over hyper-rectangles is a costly alter-
native. Lately, Emmerich [54] develops a direct computation procedure for the ExI
quadrature for two-objective problems that provides a more precise and quicker way
to calculate the multi-objective ExI than the numerical integration. Herein, the use
of the multi-objective ExI on problems with discrete design inputs is intended.
1.5
Rotor-craft Design
Conceptual design of rotor-craft includes the selection of concepts and categorical
variables, such as anti-torque device conﬁgurations, blade airfoil sections, materials,
19

engines, and cross-sections of structural elements, among others [123, 68, 151]. Al-
ternative selection in helicopter design is normally determined in early design stages
and are usually not changed in later stages. It is because their changes are large by
nature so that the impact of these choices in the design process is signiﬁcant, making
these changes prohibitive in latter stages. Once a blade airfoil section is selected,
it is aerodynamically optimized in later design stages; however, the possible airfoils
are limited to some similar in nature to the ﬁrst selected. Engine choice changes in
later stages of the rotor-craft design process could greatly enhance the design perfor-
mance, but it also aﬀects the entire design, resulting in excessive changing cost in the
majority of the cases.
Rotor-craft designers are aware of the importance of alternative choices. However,
the complexity and diﬃculty of the rotor physics, the multidisciplinary nature of
the rotor-craft, and the desire to include multi-objective optimization prevent design
team members from employing accurate tools for alternative selection. As in other
engineering ﬁelds, rotor-craft conceptual design teams are often left with dubiously
reliable traditional tools, such as empirical and/or simple models, that cannot model
complex physics and may dismiss choices that high-ﬁdelity tools can brand as optimal.
Rotor-craft engineers prefer being aware of major design changes in earlier stages
where the impact of the change is lower. Therefore, it is intended a more eﬃcient use
of computationally intense codes early in the design process to enhance the selection
of alternatives; it would make the rotor-craft design process better.
The two approaches to alternative selection, qualitative and quantitative ones,
have been used in rotor-craft design. In the qualitative approach, decisions are made
by design engineers. These choices are aﬀected by experience and personal preference,
they may ”feel right” but there is no quantitative supporting evidence. Walsh et al.
[193] choose the airfoils by avoiding exceeding the maximum section lift coeﬃcients
on the retreating side or exceeding the section drag divergence Mach number on the
20

advancing side. In the quantitative approach, the exploration of diﬀerent alternatives
is done one by one and with sizing tools.
Coupling numerical optimization tools
to sizing codes is diﬃcult [165] because of the low-ﬁdelity of sizing codes, and the
complexity of the simultaneous handling of discrete, integer and continuous variables,
typical in rotor-craft design scenarios.
Once each alternative has been explored,
the selection is made by comparison. The use of low-ﬁdelity codes makes results of
quantitative approaches unreliable.
Within the quantitative methods, Crossley et al. [35] attempt to better address
categories in helicopter early design. They realize that binary-coded genetic algo-
rithms can properly handle discrete variables; GA searches simultaneously all the
candidate airfoils as opposed to traditional independent searches in each category.
Crossley et. al [34] later extend the same approach to select not only the airfoil sec-
tion but also the rotor-craft engine in the frame of conceptual design; they minimize
the gross weight. Wells et al. [196] use a similar approach in a more advanced design
phase to search for acoustically eﬃcient rotor blades. Berry et al. [15] choose among
several candidate airfoils using a simple predictive tool coupled with a genetic algo-
rithm. However, these methods employ simple analysis tools that fail to capture the
complex physics by which categories could be accurately selected for initial designs.
Surrogates have been used in rotor-craft design to reduce the evaluation time. Sai-
jal et al. [162] use neural networks and polynomial meta-models to reduce helicopter
vibration. Sun et al. [180] ﬁt response surfaces to design airfoils aerodynamically.
Another popular family of meta-models is approximation-based stochastic pro-
cesses [160]. These surrogates have been successfully employed in several rotor-craft-
related ﬁelds. One application of these meta-models predicts unsteady aerodynamic
responses when ﬂow non-linearities are present [74, 75]. Many researchers apply this
stochastic techniques to rotor design [119, 19, 73, 188, 118, 91]. Vu et al. [191] apply
stochastic processes in rotor-craft design to minimize the rotor-craft power in hover.
21

It is easy to ﬁnd in the rotatory system literature examples of objective functions
with similar behavior for several blade airfoil sections, blade materials and conﬁgura-
tions [123, 151, 83, 29, 52]. The giromill performance experiences similar trends for
several airfoils, as shown in Figure 2 [52]. In rotor-craft design, again independent
meta-models are ﬁtted for each alternative, see [209, 180, 140]. Therefore, current
approaches in rotor-craft design do not take advantage of similar trends across alter-
natives.
 
 
NACA 0024
NACA 4420
NACA 4520
γ = 0.5
n = 4
c = 15 cm
r = 40 cm
Tip Speed Ratio
CT
0
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
2
0
0.05
0.1
0.15
0.2
0.25
Figure 2: Giromill CT vs Tip Speed Ratio for Several Airfoils [52]
Recently, optimization techniques have been extended to cope with surrogate mod-
eling in rotor-craft design. The most straight-forward approach is to directly optimize
the surrogate objective function without updating the surrogate during the process
[180, 141, 95, 162, 156]. These surrogate-based optimization techniques have been
applied to blade material and blade ply conﬁguration selection. Murugan and Gan-
guli [141] optimize each material on the composite blade via response surfaces, and
then, they compare the best design of each material to select the optimum one. Mu-
rugan et al. [140] optimize several composite ply conﬁgurations using surrogates and
choose the best performing one by comparison. Yuan and Friedmann [209] optimize
independently stiﬀ-in and soft-in blades and compare the results. Sun et al. [180]
22

proceed similarly to airfoil comparison when optimizing their aerodynamic curves.
The described investigations on categorical comparison with surrogates based op-
timization have been done in the context of aeroelastic stability enhancement, vi-
bration suppression, and airfoil aerodynamic optimization that fall typically within
preliminary design stages. These techniques that include high-ﬁdelity models via sur-
rogates can bring tremendous value if applied to early design stages where the choice
between categories is made. Nevertheless, the large conceptual design spaces due in
part to the presence of multiple alternatives hinders the application of this technique
to conceptual design.
In order to enhance the eﬃciency of optimization on meta-models, EGO algo-
rithms on stochastic process surrogates attempt to focus most of the expensive func-
tion observations in good performing areas. Rotor-craft engineers have applied EGO
algorithms to rotor vibration and noise, and performance optimization [119, 73, 118,
91]. EGO algorithms can explore large design spaces, typical in early design stages;
however, they have not been applied to alternative selection in rotor-craft design.
EGO algorithms are adequate candidates to implement conceptual design tools for
the more eﬃcient use of computationally intense models with the purpose of improv-
ing the selection of categorical variables and initial designs in rotor-craft design.
1.6
Proposed Approach
The goal of this research is to build conceptual design tools for the more eﬃcient use
of computationally intense codes in scenarios with discrete design alternative choices
in early design. It would lead to enhancements in the quality of the initial designs and
concept/category selection. The idea is to increase the tools (surrogate and adaptive
sampling) eﬃciency by getting more value of the learning on each discrete design
alternative. In order to achieve this goal, ﬁve paths are taken:
23

1. Meta-models that cross-use computationally expensive observations across cat-
egories.
2. Adaptive sampling on meta-models that cross-use observations across cate-
gories.
3. Several types of mutation operators for the mixed-integer-categorical genetic
algorithm are proposed.
4. Meta-models that re-use computationally expensive observations from previ-
ously explored concepts.
5. Adaptive sampling on meta-models that re-use observations from previously
explored concepts.
Meta-models that cross-use computationally expensive observations across
categories.
It has been previously mentioned that using intense computational
tools in early design stages constrains the function call budget. Thus, attention is
taken on designing accurate surrogates capable of leveraging the similarities across
categories, producing more accurate approximation for a given function call budget.
In current methods, the training set for each category does not interact with that of
other. Thus, these current independent surrogates for each category are not eﬀective
for cases where objective function trends resemble, which happens usually in engi-
neering scenarios. Including the non-numeric design variables in a unique surrogate
would allow cross-using information of dependence along design variables across cat-
egory members. It could result in surrogates that are more eﬃcient than traditional
independent ones for each category. “Eﬃciency” means fewer observations are needed
for a given accuracy or more accuracy for the same number of expensive observations.
The ﬁrst option to build the unique surrogate that includes not only continuous
24

and integer variables but also nominal ones is the Hamming distance. Sometimes, non-
numeric inputs could intrinsically have a distance provided by a measurable property
of the given category. This intrinsic nominal distance can be brought as the second
option to build the unique surrogate. When the nominal distance functions (Hamming
distance and the intrinsic nominal distance) are deﬁned, categorical variables can
be included in the surrogate domain. The eﬃciency of these unique surrogates is
measured against the current independent surrogates. Also, the inﬂuence of the size
of the training set is studied. Gaussian surrogates are chosen since adaptive sampling
is intended. If this unique surrogate is successfully built, the available resources will
be better employed when dealing with computationally burdensome tools.
Adaptive sampling on meta-models that cross-use observations across cat-
egories.
The ﬁrst step to build tools for the more eﬃcient use of computationally
intense codes is to develop meta-models that cross-use observations across categories.
The second step to achieve the research goal is to extend EGO algorithms to handle
the meta-models developed in the ﬁrst step. The adaptive sampling algorithm chosen
in this work applies the expected improvement inﬁll criterion on Kriging surrogates.
It allows designers to better explore the design space and exploit local optima in
potentially promising regions for restrictive function call budget.
However, the surrogate models that leverage similar trends across categories have
mixed-integer-categorical domains; thus, the inﬁll criterion must be optimized in these
domains. An optimization algorithm capable to optimize design spaces with not only
continuous and integer variables but also categorical ones is developed. This genetic
algorithm is constructed out of a mixed-integer GA by conveniently including new
generation, mutation, and crossover operators for the non-numeric input parameters.
This genetic algorithm, if successful, allows designers to develop more eﬃcient con-
ceptual design tools for intense multi-modal objective functions where there exists a
25

choice of discrete alternatives.
Several types of mutation operators for the mixed-integer-categorical ge-
netic algorithm are proposed.
The identiﬁcation of the possible speed-up switches
for the optimization algorithm is pursued. The speed of this algorithm may increase
with certain mutation operators.
Several categorical mutation distributions could
be obtained based on the two nominal distance functions: Hamming and intrinsic
distance.
Therefore, it makes sense to carry out an study to see which categori-
cal mutation operators are more eﬀective for some test problems. Also, as it was
previously mentioned, expected improvement landscapes have identical properties re-
gardless the problem.
Due to the similar ExI characteristics (independent of the
objective function), it is reasonable to extrapolate the results of the case study to
other problems diﬀerent from the tested ones.
Meta-models that re-use computationally expensive observations from pre-
viously explored concepts.
Attention is on designing accurate surrogates capable
of leveraging similarities from previously explored concepts. Currently, surrogates are
ﬁtted to each concept without reusing observations from previous concepts; however,
previous concepts usually experience similar trends to the new ones. The current ap-
proach is a strategy that provides poor surrogate accuracy if the function call budget
is limited (usual situation while using computationally expensive tools). A Gaussian
multi-ﬁdelity approach is developed to reuse previous concept observations when ﬁt-
ting a surrogate for a new concept. The eﬃciency of these multi-ﬁdelity surrogates
is measured against the current independent surrogates. Also, the inﬂuence of the
size of the new and previous concept training sets is studied. If the resulting multi-
ﬁdelity surrogate of the new concept is successful, the limited function call budget is
more conveniently exploited (more surrogate accuracy) when using computationally
expensive models.
26

Adaptive sampling on meta-models that re-use observations from previ-
ously explored concepts.
Once the meta-models that reuse computationally ex-
pensive observations from previously sampled concepts are implemented, the next step
is to apply EGO algorithms on these meta-models. Again, the expected improvement
is the inﬁll criterion chosen to adaptively sample the Gaussian multi-ﬁdelity surrogate.
The domain space of the surrogate that leverages similar trends from previously
sampled concepts could have continuous, integer and/or categorical design variables.
The most demanding case is when the surrogate domain is mixed-integer-categorical;
however, the adaptive sampling on this type of domain was already solved in the
second item of the above enumeration. Therefore, no technical challenges are ex-
pected when adaptively sampling meta-models that re-use observations if concepts
are explored one at a time.
1.6.1
Research Questions
Once the problem to tackle is deﬁned, research questions help to guide the investiga-
tions herein. It is important to remember the goal of this research: build conceptual
design tools for the more eﬃcient use of computationally intense codes with the pur-
pose of improving the quality of initial design and concept/category selection.
1.6.1.1
First Question
Is it possible to build eﬃcient surrogates for design scenarios where there exists a
design categorical choice with similar trends?
a) Is it possible to build surrogates that cross-use computationally expensive ob-
servations across categorical choices with similar trends?
b) Which nominal distances allow building these eﬃcient surrogates?
c) Does the MIC surrogate outperform independent surrogate modeling for each
category? How does the relative eﬃciency of MIC surrogates with respect to
27

the independent ones (state-of-the-art) depend on the training set size?
1.6.1.2
Second Question
How could adaptive sampling approaches be eﬃciently extended to a choice of cate-
gories that experiments similar trends?
a) Can the MICGA extended from MIGA drive the ExI search while doing adaptive
sampling on MIC surrogates?
b) Is the adaptive sampling on MIC surrogates more eﬃcient than simultaneous
adaptive sampling on independent surrogates (state-of-the-art) in some range
of training set sizes?
1.6.1.3
Third Question
Is it possible to build eﬃcient surrogates for design scenarios where there exist com-
putationally expensive observations from a previous concept with similar trends?
a) Is it possible to build surrogates that re-use computationally expensive obser-
vations from previous concepts with similar trends?
b) Do ECMF surrogates outperform mono-ﬁdelity surrogates? How does the rela-
tive eﬃciency of ECMF surrogates with respect to the mono-ﬁdelity ones (state-
of-the-art) depend on the new concept training set size?
c) What is the inﬂuence of the old concept training set size in the performance of
ECMF surrogates?
1.7
Practical Applications
First of all, it is worth mentioning that computationally intense codes are not required
to better interpret canonical test results or further support the research hypothesis or
predictions. Also, the use of high-ﬁdelity codes in conceptual design has philosophical
28

problems as mentioned in Section 1.1 due to model reduction and unknown unknowns.
However, tests on computationally burdensome models would support the practicality
of the engineering motivation.
In order to ﬁnd a proper practical application of this research, it is recommended
to keep in mind its goal: develop tools for the more eﬃcient use of computationally
intense tools early in the design process with the purpose of enhancing the initial
design and concept/category selection. Thus, the target is on engineering problems
where discrete variables and diﬀerent conﬁgurations are possible. Tools with long
evaluation time are the ideal aim of this work, whereas heuristic models in the form
of look-up tables or really simple models must be ruled out for application purposes.
Rotor-craft design fulﬁlls these two requirements: presence of design alternatives
and long concept evaluation times. Firstly, rotor and anti-torque conﬁgurations, num-
ber of blades, types of airfoil, types of articulation, gear box arrangement, and man-
ufacturing processes, among others, represent several concepts and discrete variables.
Also, some manufacturing constraints could convert some ﬁrstly thought continuous
variables into discrete ones, as it occurs with commercially available standardized
manufactured pieces.
Secondly, the multidisciplinary nature of the design, where
there is interaction between aerodynamic, structure mechanics, control, and aeroa-
coutics, among other ﬁelds, makes function evaluations intense, even when not too
complex physics are used.
Additionally, rotor-craft design is usually intended to be multi-objective. Typical
objectives are fuel consumption and noise emissions at several ﬂight conditions, and
system cost, among others. So, multi-objective optimization requires more evaluations
than single-objective scenarios; thus, even mid-ﬁdelity tools become less manageable
in rotor-craft multi-objective design.
Therefore, the use of high-ﬁdelity tools, such as CFDs, for the multi-objective
optimization of multi-disciplinary objectives would be an impractical task, so it is
29

not considered in this work. Another reason that hinders the use of CFD codes is
the limited accessibility to rotatory CFD methods. The only available CFD code for
rotatory wing is a full Navier-Stokes solvers, whose running time makes it impractical
in conceptual design.
The last but not least reason is the large amount of time
necessary to integrate CFD related set-ups into the tools to develop, especially when
the available time for the research is limited as in this case.
The ﬁnding of prohibitive high cost changes in late design stages (specially for dis-
crete variables) and the increase in computational eﬃciency have prompted helicopter
companies to be more and more interested in getting better sizing and predictions
tools in early design stages. This could shorten the design process and enhance the
product quality. These tools provide more accurate initial designs that, taken to later
design stages, reduce the likelihood of last-minute large design changes with their
corresponding high impact on the cost. Due to all these reasons, helicopter design is
a suitable application for the method developed in this thesis.
Regarding the choice of objective functions, interest is in power consumption and
noise reduction. The software PSU-WOPWOP done by Dr. Brentner is available to
the author. It implements the Ffowcs Williams Hawkings (FWH) analogy [25]. It is
well-known that in order to get reliable acoustics results, a high-ﬁdelity aeroelastic
model is needed to properly catch the blade pressures and displacements. However,
CFD analysis were ruled out in favor of mid-ﬁdelity models. Hence, the acoustic
results would not be accurate. Thus, the acoustic objective is not pursued in this
work. Only the optimization of the rotor-craft power consumption at several ﬂight
conditions is pursued.
The software used to model the rotor-craft is FLIGHTLAB[1]. It is a rotor-craft
simulation tool capable of multidisciplinary support with selective ﬁdelity modeling
options. FLIGHTLAB is not a conceptual design software per se, but other con-
ceptual design software, such as RCAS, is not available to the author. Conceptual
30

design software, such as NDARC[97] and CIRADS[38], are available. They are used
for sizing, and parametric selection of alternatives and initial designs. NDARC and
CIRADS include very simple models where there are only a few design variables.
For instance, NDARC or CIRADS do not permit detailed aerodynamic curves. More
design variables and complex physics models can be added through interfaces with
other software. However, these complex software packages have distribution restric-
tions. Thus, FLIGHTLAB is chosen as the modeling tool; it allows more complex
physics and more design variables in the parametric selection of alternatives and
initial designs.
The ﬁrst practical test carried out is the multi-objective optimization of the
UH60A power consumption at hover and cruise speed.
For the given weight the
six degree of freedom rotor-craft must be trimmed, i.e., iterations between disciplines
are necessary till agreement is reached. So, even the use of mid-ﬁdelity codes for
aerodynamics and structural dynamics result in computationally long function eval-
uations. The categorical variable in this work is the main rotor airfoil blade. The
UH60A power consumption is used to test the eﬃcient surrogates that leverage similar
trends across categories and do multi-objective adaptive sampling on them.
A new “evolutionary incremental concept” from the conventional UH60A is tested:
UH60A with a fenestron tail. In this second practical experiment, the optimization
of the power consumption of the UH60A with fenestron tail is carried out at hover
and cruise speed. The considered previous concept is UH60A with conventional tail.
The UH60A with a fenestron tail power consumption is used to test the eﬃcient
surrogates that leverage similar trends from previous concepts with similar trends
and do multi-objective adaptive sampling on them.
31

1.8
Thesis Organization
Following the present chapter, the remaining chapters go in more details through the
research. Chapter 2 provides a literature review with a summary of the necessary
background and current available theory to develop the desired techniques. It begins
with a classiﬁcation of types of discrete variables and the deﬁnition of order and dis-
tance. Then, distance functions are presented with focus on nominal distances used
in classiﬁcation techniques Later, the typical estimates utilized by engineers to size
systems and evaluate objectives are reviewed. Emphasis is on the capability of reusing
observations of concepts with similar trends. Then, the framework of conceptual de-
sign is presented. It is followed by an introduction of the GRFM used in this work,
Kriging, to get familiar with it and its nomenclature. Kriging regression aimed to
noisy functions is also explained. After it, an overview of typical multi-ﬁdelity tech-
niques, speciﬁcally Gaussian approaches, is presented. Again multi-ﬁdelity regression
is explained.
Later, a description of adaptive sampling techniques centered on the single-objective
expected improvement inﬁll criterion is provided.
Then, a summary of the most
common mixed-integer optimization techniques is presented. These techniques are
classiﬁed according to convex problems and non-convex problems that, in turn, are
divided by deterministic and stochastic searches. It is followed by the discussion of a
multi-objective extension of the expected improvement criterion based on the incre-
ment of dominated hyper-volume. The last part of the chapter is devoted to a review
of fenestron tails.
Following this literature review chapter, a detailed description of the development
of the new techniques is provided in Chapter 3. Firstly, the chapter start with an
introduction. Secondly, the research questions that guide the investigations, their
hypothesis, and predictions are discussed. Thirdly, several nominal distance func-
tions are proposed to enable the development of a common Gaussian surrogate that
32

leverages similar trends across categories.
Fourthly, the implementation of this common surrogate is explained together with
the deﬁnition of the success indicators; this meta-model is called mixed integer-
categorical surrogate (MIC). Then, it is presented the implementation of a new
concept surrogate that leverages observations from previously sampled concepts. A
multi-ﬁdelity approach is developed to accomplish the meta-model; it is called evo-
lutionary concept multi-ﬁdelity (ECMF) surrogate. Then, adaptive sampling on the
proposed surrogates is discussed with emphasis on the extension of mixed-integer ge-
netic algorithms to a mixed-integer-categorical ones. Later, a study is carried out
regarding the inﬂuence of the mutation operator on the performance of the mixed-
integer-categorical genetic algorithm. The principle of maximum entropy is employed
to come up with possible new mutation procedures. It is followed by the discussion
of the choice of the fenestron tail conﬁguration as the new “evolutionary incremental
concept”. Emphasis is on the assessment of the fenestron baseline values and the
weight estimation. Finally, the methodology diagrams are presented.
Chapter 4 presents the FLIGHTLAB rotor-craft computational model employed
in this research.
Firstly, the UH60A baseline characteristics and parameters are
reviewed. Then, the computational model is validated at hover and cruise speed.
Finally, the noisy nature of the computational model is shown.
The MIC surrogate experiments are presented in Chapter 5. MIC surrogate eﬃ-
ciency is compared with that of independent surrogates for each category in terms of
the success indicators. Experiments to understand the inﬂuence of the training set
size and the nominal distance in the MIC performance are carried out. Results are
discussed. The tests are done on a noise-free function (disturbed Branin function)
and on a noisy function (UH60A hover power consumption). A screening of the gen-
eral UH60A hover power consumption is performed with the purpose of testing MIC
surrogate on a handy function with a few design variables.
33

Chapter 6 presents the mixed-integer-categorical genetic algorithm (MICGA) as
the driver of EGO algorithms on MIC surrogates. Firstly, an intermediate mixed-
integer GA is validated for assuring appropriate further use. Then, EGO behavior
is shown when adaptively sampling the disturbed Branin function and the UH60A
hover power consumption.
The ECMF surrogate experiments are presented in Chapter 7. ECMF surrogate
eﬃciency is compared with that of surrogates where there is no reuse of observations
from previous concepts in terms of the success indicators. Experiments to understand
the inﬂuence of the new and old concept training set size are carried out. Results
are discussed. The tests are done on a noise-free canonical function (2-dimensional
Michalewicz function) and on a noisy function (UH60A with fenestron hover power
consumption).
Chapter 8 demonstrates MIC adaptive sampling on a rotor-craft practical scenario:
the multi-objective optimization of the UH60A power consumption at hover and cruise
speed. The objectives are optimized in two design spaces: a) the one with four design
variables resulted from the screening process presented in Chapter 5; and b) a design
space with seven design variables. The categorical variable is again the main rotor
airfoil blade. In the screened domain case, the Pareto front assessed with the adaptive
sampling on the MIC surrogate is compared with the one obtained from simultaneous
adaptive sampling on independent surrogates (current state-of-the-art).
The second practical conceptual design scenario is presented in Chapter 9: the
multi-objective optimization of the UH60A with fenestron power consumption at
hover and cruise speed. Again, two design spaces with diﬀerent dimensionality are
tested. Also, a screened domain is employed to compare the quality of the Pareto front
obtained from adaptive sampling on ECMF surrogate and on a surrogate that does
not include observations from previous concepts. The considered previous concept is
UH60A with conventional tail.
34

The ﬁnal part of the thesis, Chapter 10, summarizes the research and its contri-
butions. The recommendations on when to use the MIC and ECMF surrogates are
discussed. Finally, the future work is argued.
35

CHAPTER II
BACKGROUND AND THEORY REVIEW
This Chapter contains a summary of the necessary background and current available
theory to develop the desired techniques. It covers the following topics: classiﬁcation
of types of discrete variables; nominal distances and correlation functions in the con-
text of classiﬁcation techniques; summary of typical estimates utilized by engineers;
review of the conceptual design stage; brief introduction of the GRFM with focus on
Kriging; multi-ﬁdelity meta-models, speciﬁcally Gaussian approaches; description of
adaptive sampling techniques centered on the single-objective expected improvement
inﬁll criterion; summary of the most common mixed-integer optimization techniques;
review of a multi-objective extension of the expected improvement inﬁll criterion
based on the increment of dominated hyper-volume; and ﬁnally, a review of fenestron
tails is presented.
2.1
Discrete Variables
Discrete variables are those whose possible values are only distinct points on the scale,
as opposed to continuous ones that can take an uncountably inﬁnite number of values.
Discrete variables can assume ﬁnite or countably inﬁnite values. Any set which can
be put in a one-to-one correspondence with the natural numbers (or integers) so
that a prescription can be given for identifying its members one at a time is called
a countably inﬁnite (or denumerably inﬁnite) set. Discrete variables can be ordered
or unordered. The distance between set values can be equal, uneven or meaningless.
Herein, the distance between members of a discrete design variable is employed.
Discrete variables can be classiﬁed into several groups according to several char-
acteristics of the variable. In the literature, several classiﬁcations are found. The
36

ones proposed in this work are based on the distance and order characteristics of the
variable. The classiﬁcation is the following:
Categorical, nominal, or non-numeric variables Individual items can only be
measured in terms of whether they belong to a category or not. There is no
quantiﬁable distance or order for the categorical variable.
No arithmetic or
logical operation can be performed on this data. Only qualitative classiﬁcation is
possible and the category name is arbitrarily assigned. A categorical variable is
a generalization of the binary variable that has more than two states. Examples
of such variables in sociology are marital status (unmarried, married, divorce,
or widower) and gender (male or female). For aerospace design, possible non-
numeric variables can be the engine, material, and airfoil choice available for
the aircraft designer.
Ordinal variables The values of these variables are ordered in a meaningful way.
However, the intervals between members (the distance) are unquantiﬁable and
uneven. Consequently, arithmetic operations can not be realized; however, log-
ical operations are possible. A typical example of ordinal variable is the socio-
economic status of families (upper, upper-middle, middle, or low class). Middle
class income is higher than low class income, but one can not quantify what
the diﬀerence is between high and low class families. Engineering management
usually faces the decision of what qualiﬁcation is required for a given job: B.Sc,
M.Sc. or Ph.D.. The possible qualiﬁcations represent a discrete-ordinal vari-
able.
Quantitative variables These variables can be ordered and the interval between
scale points is meaningful. Therefore, both logical and arithmetic operations
are possible. Integers are a special subgroup of quantitative variables where the
distance between points is the same and equal to one; however, it does not apply
37

for all the discrete-quantitative variables. The number of children per family is
an example of discrete-quantitative variable in sociology. Also, the standardized
sizes of some engineering parts, the number of blades, the number of compressor
stages, and the screw diameters and pitches are discrete-quantitative variables.
The whole group of discrete-quantitative is sometimes referred as integer vari-
ables; however, in reality it is just a subgroup of it. It could create confusion.
For instance, mixed-integer programming and mixed-integer stochastic searches
can handle not only integer variables but also discrete-quantitative ones; how-
ever, from their name one can think that these techniques just handle integer
variables.
2.1.1
Deﬁnition of Order and Distance
Formal deﬁnitions of order and distance can be found in the topology literature [108].
Order is a relation < on a set X such that for each pair of points β1, and β2 of X,
one and only one of the following holds:
β1 < β2,
β1 = β2,
β2 < β1
A metric or distance function is a measure d that satisﬁes the following properties for
all points β1, β2, and β3 of X:
d(β1, β2) ≥0
(1)
d(β1, β2) =d(β2, β1)
(2)
if d(β1, β2) =0, then β1 = β2
(3)
d(β1, β2) + d(β2, β3) ≥d(β1, β3)
(4)
2.2
Nominal Distance Functions
Distance functions for continuous and discrete-quantitative variables are frequently
used in engineering design. For instance, surrogates and mutations of mixed-integer
38

evolutionary algorithms (ES and GA) are normally based on continuous and integer
distances. The euclidean distance is the most popular continuous distance between
points. It is deﬁned as
decl
 x(i), x(l)
=
v
u
u
t
k
X
j=1

x(i)
j −x(l)
j
2
(5)
where x(i) and x(l) are two points in the k-dimensional design space.
Regarding distance functions for discrete-quantitative variables, a common choice
is the Manhattan distance
dmanh
 x(i), x(l)
=
k
X
j=1
|x(i)
j −x(l)
j |
(6)
which is computationally cheaper than the Euclidean distance. None of the so far
presented distances, Equations 5 and 6, are appropriate for categorical attributes
because there is no possible order, so one can not place the categorical variable in a
coordinate.
2.2.1
Classiﬁcation Techniques: Nominal Distances
Classiﬁcation is a machine learning problem where a new observation is identiﬁed
with a class of a set of possible classes. It is done on the basis of a training set of
data containing observations whose category memberships are known. The observa-
tions are analyzed in terms of a set of quantiﬁable features. These features could be
categorical, ordinal, discrete-quantitative and/or real.
These classiﬁcation techniques require the deﬁnition of nominal distances for cat-
egorical attributes. Researchers in machine learning and pattern recognition have
investigated possible choices of nominal distances. The simplest nominal distance is
an arbitrary mapping of the categorical variable to a subset of integer values whose
size is the same as the number of categories.
39

A common nominal distance is the Hamming distance, also called overlap distance
in machine learning, where the distances between all the categorical attributes are
the same. For one categorical variable, it is deﬁned as follows
dham (a, b) =





0
if a = b
1
if a ̸= b
where a, and b are categories.
Also, it is common to employ the Value Diﬀerence Metric (VDM) for nominal
attributes, which was developed by Stanﬁll an Waltz[179]. It is deﬁned as follows:
dvdm (a, b) =
C
X
c=1
|Pa,c −Pb,c|2
(7)
where dvdm is the VDM distance, C is the number of possible classes, c is an index
that goes through the classes, Pa,c is the conditional probability that the output class
is c given that the attribute takes the categorical value a, and a and b are values of
the categorical attribute. The values of the conditional probabilities can be assessed
from the training set.
Additionally, more sophisticated nominal distance functions are found in ma-
chine learning literature: Minkowsky [13], Mahalanobis [142], Camberra, Chebychev,
Quadratic, Correlation, Chi-square [134], hyper-rectangle distance functions [164].
These distance functions for categorical variables have not been employed in engi-
neering design.
Typical classiﬁcation scenarios have not only nominal attributes but also discrete-
quantitative, and continuous. In this cases, Aha et al. [2] develop a heterogeneous
euclidean-overlap metric not only for continuous and nominal variables but also for
discrete-quantitative variables. Without loss of generality, one can order the design
space so that the ﬁrst nc dimensions host the continuous design variables, the following
nn dimensions contain the nominal ones, and the last nq the discrete-quantitative ones,
i.e.
x =

xcont,1, ..., xcont,nc, xnom,1, ..., xnom,nn, xquant,1, ..., xquant,nq

40

where xcont, xnom, and xquant are the continuous, nominal and discrete-quantitative
design variables, respectively. The heterogeneous euclidean-overlap metric is given by
expression 8
v
u
u
t
nc
X
j=1

dord

x(i)
cont,j, x(l)
cont,j
2
+
nn
X
j=1
dham

x(i)
nom,j, (x(j)
nom,j

+
nq
X
j=1

dord

x(i)
quant,j, x(l)
quant,j
2
(8)
where dord
 x(i), x(l)
= x(i) −x(l)
range (xj), and range (xj) is the range for the variable xj.
Also, this heterogeneous euclidean-overlap distance has been used for evolutionary
strategies applied on dynamic niching, see Li et al.’s work[126]. In each population,
the niche radius is obtained making use of the heterogeneous distance function.
Classiﬁcation techniques have normally two stages: inference in which the training
data is used to learn a probabilistic model, and decision in which the probability
distribution is used to make a class assignment. The techniques that solve the two
stages at the same time and simply provide a function that maps the new observation
directly to decision are called discriminant.
For more details about classiﬁcation
techniques, machine learning, and pattern recognition, see references [17, 135, 130, 18].
Common classiﬁcation techniques that use distance functions are the following:
k Nearest Neighbors (k-NN) The technique consists in ﬁnding the k instances in
the training dataset that are the closest to the new observation. After, these
k instances vote to determine the class of the new observation [32]. In order
to determine the k closest instances, distance functions are needed. Hamming
distance and its improved version via attribute weighting are usually used when
dealing with categorical attributes[124].
Kernel Methods The classiﬁcation outcome is based on kernel functions. These
kernel functions normally require a metric that measures the similarity of two
vectors in the design space. Several types of kernels are used in classiﬁcation
41

techniques. Among them, a popular one are the radial basis functions. Its main
limitation is that kernel functions have to be evaluated for all the members of
the training set, which can be computationally infeasible. For more details, see
reference[166].
Support Vector Machine Kernel-based algorithms that have sparse solutions, so
there is no need to evaluate kernel functions for all the training points. The
most common approach with nominal attributes is to assume equal distance
between nominal members, as explained in Tian and Deng’s work[183].
In other classiﬁcation instances, the boundaries for classifying a member are not
sharp, i.e., the observation can belong to or not belong to the class. Fussy sets are
developed for these cases. Membership functions are introduced to deal with this
uncertainty of the classiﬁcation. These functions are subjective as the boundaries
between members; also, these functions are generalizations of the indicator function
in classical sets [172]. Fuzzy sets and their corresponding membership functions are
employed in the context of control theory, and medical diagnosis. For more details,
see Zadeh’s work [210].
Herein, interest is on design, where no classiﬁcation is needed. However, the nom-
inal distances employed in these classiﬁcation techniques are useful for the purpose
of this research.
2.2.2
Correlation Functions: Categorical Variables
Correlation is the degree to which two or more variables are linearly associated. In
the case of a two-dimensional space, correlation coeﬃcient quantiﬁes the degree of
correlation between the values of the two variables. While calculating the correlation
coeﬃcient, the numerical values of the variables are needed. For more details, see
[178, 110].
42

Dependency and/or association is treated diﬀerently for categorical variables be-
cause there is no numerical value to input to the correlation coeﬃcient. Their associ-
ation is normally done by the chi-square test [137]. It tests the null hypothesis that
there is no relationship between two categorical variables.
In order to account for the epistemic uncertainty of a function, Gaussian process
meta-models treat deterministic responses as outcomes of a stochastic process. Then,
correlation functions between the stochastic processes at diﬀerent design locations are
used to quantify the inﬂuence of a sample point on its neighbor. The correlation func-
tions depend on the distance between sample points; then, the prediction is based on
observations from nearby locations[202]. Radial basis functions are a common choice
of correlation functions: their value is one for zero distance (very close correlation
between sample points), and their value tend to zero as the distance between sample
points becomes large (the sample points are not correlated)[65].
Categorical variables have not been modeled by surrogates in design. When it
comes to deal with categorical variables in classiﬁcation techniques, some authors
have used nominal distances, such as Hamming and VDM, for classiﬁcation using
k-NN[201] and SVM techniques[183], respectively.
These techniques use nominal
distances as inputs for their kernel functions. Also, nominal distances have been used
as inputs for correlation functions when doing ranking learning [30], also known as
ordinal regression.
Herein, nominal distances allow bringing correlation functions based on distance
to build Bayesian meta-models for engineering design when there is a categorical
choice. Engineering observations of similar trends across several categories encourage
the use of approximating methods that employ neighboring solutions, such as cor-
relation functions based on distance. These correlation functions allow cross-using
computationally expensive observations across categories.
43

2.3
Types of Estimates. Reuse of Data in Engineering
Sizing engineering systems and estimating/predicting engineering objectives are a
crucial part of the design process.
Engineers want estimation tools that are fast,
accurate, and reliable in order to properly design systems. As explained in Section
1.2, some engineering objectives experience similar trends across diﬀerent discrete
alternatives. Thus, the ability of reusing other alternative observations is included
and emphasized as a desired estimate quality in this research. The desired qualities
in this thesis for the estimating techniques are presented in the following description:
Reuse other alternative data The ability of reusing data from similar alterna-
tives makes the design process more eﬃcient, especially when computationally
expensive observations are brought.
High-ﬁdelity estimate The closer the estimation is to the actual value, the more
value the estimate holds.
Low computational eﬀort Engineers want fast estimation tools to design quickly
and be able to explore large design spaces.
No experience needed It is desirable that the estimating process does not rely on
experts’ opinions, which are usually subjective.
Deal with revolutionary designs The increasing requirements make design teams
to look for completely new concepts to meet the new high standards.
The
estimation tool should be able to accurately predict these revolutionary designs.
Include the eﬀect of many signiﬁcant variables The addition of many signiﬁ-
cant design factors assures a more detailed parameterization of the design space.
Thus, more optimal designs are possible.
44

Estimate typical engineering objectives Engineers want not only sizing tools
but also estimates of real and practical engineering objectives.
There exist several estimation methods in engineering.
The ﬁrst one is called
analogy-based prediction; it solves a new problem using and adapting solutions from
previously solved similar problems [131]. With the help of experience, similar previous
problems are chosen, relevant cost drivers are selected, and appropriate adjustments
to reﬂect technical diﬀerences are applied. It normally produces low-ﬁdelity estimates
at low computational cost. The quality of the estimate depends on the availability
of a similar previous problem. The number of signiﬁcant design variables included in
the estimate depends on the previous problem solution and the skills of the design
team. These techniques are normally used for cost and weight estimation in software
development [112] and engineering projects such as space missions [169].
The second estimating method is the regression-based prediction (also called
statistics-based predictions).
It consists in developing an estimation relationship
based on suﬃcient historical data of similar systems and projects [169]. They are
easy, fast, and simple to use at the expense of obtaining a low-ﬁdelity estimate.
However, some experience is needed to specify the ﬁtting model. If the historical
database is not big enough the estimate loses value. Also, the fact it is based on his-
torical databases hinders the validity of the prediction when assessing revolutionary
alternatives. Normally, a few signiﬁcant design factors are included, and it is mainly
used for sizing purposes. These regression-based methods have been widely used in
helicopter design [146, 185, 150, 97, 102].
The last prediction approach is modeling and simulation (M&S), where normally
high-ﬁdelity tools are employed in the estimation. M&S provides virtual duplica-
tion of products and processes, and represents those products or processes in readily
available and operationally valid environments [143]. It uses models, including emu-
lators, prototypes, and simulators, to develop data as a basis for making managerial
45

or technical decisions. The main drawback of these predicting techniques is the high
computational eﬀort when modeling systems with high-ﬁdelity. The labor eﬀort to
set the modeling environment is high, but there is no need of experts’ subjective
opinion. As many as desired design variables can be added in the simulation, and
revolutionary designs can be explored with M&S techniques. Also, typical and prac-
tical engineering objectives can be output from these techniques. Multiple examples
of M&S on engineering systems can be found in the literature [129, 85]. So far, it
does not reuse observations from previous similar alternatives. This work focuses on
bringing to M&S techniques this new capability of reusing previous observations of
alternatives with similar trends.
Figure 3 summarizes the advantages and disadvantages of the these estimation
techniques.
Figure 3: Advantages and Disadvantages of Estimate Types
46

2.4
Conceptual Design
The design process is broken into 3 main phases: conceptual design, preliminary
design, and detailed design. Conceptual design is an iterative process where design
requirements, trade-oﬀs, and analysis are used to guide and evaluate conﬁguration
arrangements. Basic questions of performance and conﬁgurations are answered in
the conceptual design stage. In this design stage, key decisions are made, ﬁxing up
to 80% of the life cycle cost [190]. Several concepts and categorical alternatives are
studied during the conceptual design phase. Figure 4 shows the typical conceptual
design wheel.
Figure 4: Conceptual Design Wheel [151]
Concepts evolve incrementally to meet requirements, achieve satisfactory trade-
oﬀs, and improve the performance of weak designs obtained from previous concepts.
The process is very ﬂuid and the conﬁguration incorporates changes that potentially
can make the design meet the requirements and produce satisfying trade-oﬀs. In this
thesis, the new concept obtained from incremental changes on the previous concept
is called “evolutionary incremental concept”. The new and old concept are expected
to experience similar trends if these incremental changes are not too large.
47

The conceptual design emphasis is normally on the crucial components interac-
tions instead of on deep details in each system conﬁguration and geometry. Finding
these interactions normally requires a lot of experience and sharpness. As the process
goes on, the analysis sophistication keeps increasing. In the aircraft industry a typ-
ical conceptual design period to select the best concept is around six months. Good
conceptual design reviews, speciﬁcally for aircraft design, are provided by Raymer
[151], and Roskam [157].
2.5
Gaussian Meta-Models: Kriging
In science and engineering, researchers use computer simulation codes instead of ex-
pensive physical experiments in order to improve the quality and performance of
engineering products. Scientists look for more and more ﬂexible and accurate models
to study a given phenomenon. Unfortunately, this tendency makes computer sim-
ulations to take a substantial computational time. One simulation may take many
minutes, hours, days or even weeks, rendering design studies uneconomical.
Normally, this issue is solved by the construction of simpler approximation models
that mimic and predict the expensive code output. The idea is to develop a cheaper
relationship between the system inputs and outputs. If the model is properly built,
these approximation models mimic the behavior of the intense simulation accurately,
and at the same time they can be evaluated faster. Several approximation methods
exist in the literature, each one with its pros and cons. Among the most popular
methods, one ﬁnds polynomial response surfaces [24], Gaussian random ﬁeld meta-
models [160], support vector machines [187], and artiﬁcial neural networks [17].
An important class of surrogates are the Bayesian ones. As opposed to other sur-
rogates, they quantify the uncertainty due to the lack of knowledge about the function
to approximate. Instead of estimating unique values of the meta-model parameters,
Bayesian surrogates provide, according to the data already observed, a probability
48

distribution that reﬂects the degree of certainty in the surrogate parameters, i.e., it
provides a measure of conﬁdence for its parameters. Prediction-wise, the Bayesian
surrogate output is also a probabilistic distribution instead of a deterministic one.
Poorly sampled areas of the design space have more uncertainty in the predictive
distribution. The prediction uncertainty is necessary for the use of adaptive sampling
techniques, where the conﬁdence in the prediction can drive the sampling plan, see
Section 2.7.
Figure 5 shows a predictive distribution for a given observed data.
The 95%
conﬁdence interval indicates the reliability of the estimate. Sparsely sampled regions
have a larger conﬁdence interval than highly sampled regions.
Figure 5: Bayesian Predictive Distribution
A very proliﬁc type of Bayesian surrogate is the GRFM. GRFM assumes that
the observed data is a noisy realization of a deterministic model. This realization is
modeled as a normally distributed random vector [104]. However, it must be clear
that GRFM builds surrogate models of deterministic computer functions, even though
the technique is based on stochastic processes. As a Bayesian surrogate, its prediction
49

is a probabilistic distribution.
GRFM, also known as Kriging1, has become popular because of its capability of
predicting the uncertainty of the prediction while being able to cope with non-linear
problems. Much of the eﬀorts in Bayesian modeling have been devoted to this type
of surrogates. Also, there exists a rich literature about it. These characteristics make
Kriging really appealing for engineering design problems. Krige ﬁrst developed it for
mining problems [117]. Sacks et al. were the ﬁrst ones, before many, to apply Kriging
to computer experiments[160].
The main Kriging drawback is the curse of dimensionality as shown by Koch et
al. [116], and Wang et al. [194]. As the dimension of the design space and number of
sample points increase, the Kriging tuning process becomes more and more resource-
demanding.
The tuning process is done with the maximum likelihood estimation
(MLE), that is later explained. It requires the optimization of the observed data
likelihood, which is normally performed with a stochastic search. Each likelihood
evaluation needs the factorization of a matrix whose size is the number of sample
points. For large number of sample points, the cost of the MLE is driven by these
time-consuming matrix factorizations. Also, a large dimensionality of the design space
provokes that, when optimizing for tuning the surrogate, the search is in a large design
volume which makes the process even more resource-demanding.
In spite of the curse of dimensionality, the widespread use of Kriging in the litera-
ture and its robustness are enough beneﬁts to choose it as the Bayesian surrogate. In
the following, a mathematical description of it is presented. There are many variations
of Kriging; the one presented herein is called ordinary Kriging.
Kriging bases its prediction on a training or observed set, X = {x(1), x(2), ...x(n)}.
Each input variable has k dimensions. The responses at the observed points are given
1Currently, Kriging methods are wide. They don’t necessarily assume Gaussian ﬁelds. However,
herein, the values of the exponents of the correlation or basis functions are 2, making the Kriging a
Gaussian process.
50

by y = {y(1), y(2), ...y(n)}. Let Y be a noisy realization of the deterministic model
y (x)
Y = µ1 + ǫ
where 1 is a k-dimensional vector of ones; and ǫ is the error between the stochastic
process, Y, and a constant vector µ1. ǫi is normally distributed with uniform variance
σ2, i.e., ǫi = N (0, σ2) for i = 1, .., n. The constant value of the mean µ is a speciﬁc
characteristic of the ordinary Kriging.
The components of this vector are correlated according to Kriging basis functions
[65, 184] given by
Ψ .= cor

Y
 x(i)
, Y
 x(l)
= exp
 
−
k
X
j=1
θj|x(i)
j −x(l)
j |pj
!
(9)
Note that θ and p are free parameters. For simplicity, this work assumes pj = 2
for all j, making the meta-model a Gaussian one. The parameter θj controls how
much inﬂuence the design variable xj has on the objective function. Large values of
θj implies low correlations for the dimension j. Thus, by comparing diﬀerent θj for a
ﬁtted model one can tell which design attributes are more inﬂuential in the objective.
For instance, θj > θk implies that the input xk has a lower eﬀect in the objective
function y (x) than xj.
Once Kriging basis functions have been introduced, one must estimate proper
values of hyperparameters2, θ, µ, and σ, that lead to an accurate surrogate model
given the training data, X and y. There are two main approaches to estimate the
unknown parameters: likelihood-based ones and fully Bayesian ones. The second ones
are ruled out in this investigation for their high computational expense and their need
of assigning a prior distribution.
The likelihood-based approaches are chosen to construct the surrogate model that
2Hyperparameters are the unknown parameters that are used to tune the model.
51

interpolates the data; thus, the likelihood of the observed data is needed. The training
dataset is assumed to be generated independently. The likelihood of the observed data
is given by the product of their probabilities according the Gaussian distributed noisy
realization,
L
 Y(1), ...Y(n)|µ, σ, θ

=
1
(2πσ2)n/2 exp
"
−
P  Y(i) −µ
2
2σ2
#
(10)
making use of the correlation matrix, Ψ, the likelihood takes the form of
L (y|µ, σ, θ) =
1
(2πσ2)n/2 |Ψ|
1
2 exp

−(y −1µ)′ Ψ−1 (y −1µ)
2σ2

(11)
The unknown parameters of the model are found by seeking for the values that
maximize the likelihood of the observed data, Equation 11. It is the MLE procedure.
MLE can be applied to not only Kriging hyperparameters tuning [65] but also any
general Bayesian inference parameter estimation as Tipping shows [184].
For simplicity’s sake, the natural logarithmic function, ln(x), is brought to the
analysis. It is a monotonic increasing function of x, so one can maximize the ln(L(.))
instead of the L(.). The natural logarithm of the training data likelihood is
ln L (y|µ, σ) = n
2 ln (2π) −n
2 ln
 σ2
−1
2 ln |Ψ| −(y −1µ)′ Ψ−1 (y −1µ)
2σ2
(12)
The hyperparemeters in the discussion are θ, µ, and σ. Derivatives respect to
µ and σ are relatively simple, so the maximization against these last two hyper-
paremeters can be analytically tackled. The optimization results in the optimal hy-
perparemeters given by Equations 13 and 14
ˆµ = 1′Ψ−1y
1′Ψ−11
(13)
ˆσ2 = (y −1ˆµ)′ Ψ−1 (y −1ˆµ)
n
(14)
52

Note that the hat, ˆ, implies estimation for optimal values.
Also, it is worth
realizing that the inverse of the correlation matrix shows up. It is the reason because
the previously mentioned matrix factorization is needed. Substituting optimal values
into the natural logarithm of the likelihood, and removing some constant terms that
are useless in the optimization process results in the following natural log of the
likelihood
ln (L) ≈−n
2 ln
 ˆσ2
−1
2 ln |Ψ|
(15)
The hyper-parameter θ is still unknown and can further maximize the likelihood.
Taking derivative of ln (L) respect to θ is analytically tedious, see Equation 15. As
alternative a numerical optimization technique like a genetic algorithm can be used
to maximize the likelihood against θ.
Forrester, S´obester, and Keane [65] point out the correlation matrix Ψ varies in
a logarithmic scale along θ. Hence, the optimization is done in terms of ln θk instead
of θk. Experience on typical engineering problems sets the search bound for optimal
values in the range of θ between 10−3 and 102. The optimal estimation obtained for
this last hyperparemeter is denoted as ˆθ.
Once the hyperparemeters of the Gaussian meta-model are tuned, one can predict
the expensive function over the design space. Let us say that the prediction, ˆy (x),
at the point x is wanted. The correlation vector between the point x and the sample
points X is
ψ = {cor

Y
 x(1)
, Y (x)

, ...cor

Y
 x(n)
, Y (x)

}
where the cor [, ] is given by Kriging basis functions, see Equation 9.
The idea is to maximize the likelihood of the augmented data (now the new point
x, where the prediction is pursued, is included).
The only free parameter is the
prediction value at the new point, ˆy (x). The natural log of the augmented data
53

likelihood is
ln (L) ≈−
 y−1ˆµ
ˆy−ˆµ
T  Ψ ψ
ψT 1
T y−1ˆµ
ˆy−ˆµ

2ˆσ2
(16)
where the terms non-dependent on the only free parameter, ˆy, are not written. Once
the maximization is done, the prediction ˆy (x) that maximizes the likelihood of the
augmented data is given by
ˆy (x) = ˆµ + ψTΨ−1 (y −ˆµ1)
(17)
The GRFM also estimates the error or uncertainty in the model, permitting to ﬁnd
new sample points not only in promising regions but also in sparsely sampled regions
where the uncertainty in the prediction is high. Hence, it enables the exploration
of the surrogate, while the prediction ˆy (x) enables the surrogate exploitation. This
error is called the mean squared error, ˆs2 (x). It is given by
ˆs2 (x) = ˆσ2 
1 −ψTΨ−1ψ

(18)
The mean squared error is inversely related to the curvature of the augmented
ln (L). A proof of Equation 18 is given by Jones [98]. Note that ˆs2 (x) is zero at
sample points because the Kriging proposed method is an interpolation through the
sample points.
2.5.1
Kriging Regression
The Gaussian interpolating surrogate fails when dealing with noisy data. It is well
known that computer simulations, such as computational structural dynamics (CSD)
and CFD codes, contain numerical noise. When observations are sparse, interpolating
techniques can deal with small amplitude noise; however, when the optimization
technique starts to converge to an optimum, the data becomes more dense at good
54

performing regions, and the interpolating model may fail to mimic the intense function
[66].
The regressive model is achieved by adding a regression constant, λ, to the diag-
onal of the Kriging correlation matrix
R + λI
(19)
λ is the regression constant that resolves the problem of approximating noisy func-
tions. This constant avoids that the prediction at an observed design point passes
through the observed objective value[66]. The Kriging correlation matrix, R, is again
chosen to be based on Gaussian radial basis functions
R = Ri,l

Y
 x(i)
, Y
 x(l)
= exp
 
−
k
X
j=1
θj|x(i)
j −x(l)
j |2
!
(20)
Note that the matrix R is the same as the one for the interpolating case, Ψ, deﬁned
in Equation 9. The likelihood of the observed data in the Kriging regression is given
by
ln L (y|µ, σ) = −n
2 ln
 σ2
−1
2 ln |R + λI| −(y −1µ)′ (R + λI)−1 (y −1µ)
2σ2
(21)
The unknown Kriging hyper-parameters are not only the same ones as for the in-
terpolating Kriging, (θ, µ, σ), but also λ. These hyper-parameters are again obtained
from the MLE of the observed data [66]. Regression adds one extra variable to the
process of searching for the MLE; it increases the tuning computational cost. The
optimal hyper-parameters ˆµ and ˆσ are obtained similarly as in Section 2.5. The as-
sessment of the optimal hyper-parameters ˆθ and ˆλ is done by a GA since the process
is analytically intractable.
Similarly as in the interpolating case, the maximization of the likelihood of the
augmented data is used to ﬁnd the regression predictor, ˆy. It is given by
55

ˆyr (x) = ˆµr + ψT 
R + ˆλI
−1
(y −ˆµr1)
(22)
where
ˆµr =
1′ 
R + ˆλI
−1
y
1′

R + ˆλI
−1
1
1 is a n-dimensional vector of ones. Notice that again the optimal hyper-parameter
values are denoted by a hat,ˆ.
ψ is the correlation vector between the point x where the prediction is wanted
and observed points x(i) for i = 1, 2, ..., n
ψ = {exp
 
−
k
X
j=1
θj|x(1)
j
−xj|2
!
, exp
 
−
k
X
j=1
θj|x(2)
j
−xj|2
!
, ... exp
 
−
k
X
j=1
θj|x(n)
j
−xj|2
!
}
Kriging models permit not only to predict the function value in new sample points,
but also to estimate the uncertainty in the prediction. The uncertainty, ˆs2
r (x) in the
regression case is given by Equation 23
ˆs2
r (x) = ˆσ2
r

1 + ˆλ −ψT 
R + ˆλI
−1
ψ

(23)
where
ˆσ2
r =
(y −1 ˆµr)′ 
R + ˆλI
−1
(y −1 ˆµr)
n
(24)
It is worth mentioning that the model error in the regressive case, ˆsr, is not zero at
the observed points because the value of the regression constant, ˆλ, diﬀers from zero;
it produces a non-zero ExI. It can lead to optimizations trapped at sample points as
is explained in the Subsection 2.5.2.
2.5.2
Kriging Re-interpolation
The Kriging regression mean squared error is not zero at the observed points because
the value of ˆλ is diﬀerent from zero, see Equation 23. Therefore, the ExI at these
points could diﬀer from zero. It may result in maximum expected improvement at
56

design points previously sampled. This is a plausible scenario for non-deterministic
experiments, where there is no repeatability. However, it stalls the EGO algorithm
for repeatable deterministic computer experiments (the ones in this work) because
the results of the observations at a given design point are always the same.
Forrester et al.
[66] redeﬁne the mean squared error when Kriging regression
is used on deterministic experiments. They build an interpolation of the regressive
Kriging surrogate through the values predicted by the regression meta-model at the
observed points. This surrogate is called re-interpolation. The re-interpolating pre-
dictor is the same as the regressive one, i.e. ˆyri = ˆyr∀x and ˆµri = ˆµr.
The re-interpolation mean squared error, ˆs2
ri, that Forrester et al. [66] derive is
given by
ˆs2
ri (x) = ˆσ2
ri

1 −ψTR−1ψ

(25)
where
ˆσ2
ri =
(y −1 ˆµr)′ 
R + ˆλI
−1
R

R + ˆλI
−1
(y −1 ˆµr)
n
(26)
Equation 25 guarantees that the re-interpolating mean squared error ˆsri is zero
at the sample locations, unlike what happens for the regression case, ˆsr. Thus, the
ExIs (see Equation 46) at the observed points are zero in the re-interpolating case,
and consequently, EGO process does not stall in this case.
2.6
Multi-Fidelity Meta-Models: Gaussian Approach
In many engineering situations more information than a mere vector of function
values is available.
It is not unusual to have access to several simulation models
with diﬀerent ﬁdelities.
Multi-ﬁdelity techniques merge all the objective function
information of varying ﬁdelity with the purpose of constructing a surrogate model.
These techniques help to soften the eﬀect of the curse of dimensionality while enabling
57

the user to include high-ﬁdelity tools. Techniques range from local approximations
to more general approaches that model objectives globally.
In the present discussion two ﬁdelities models are used to construct the approxima-
tion without loss of generality: high-ﬁdelity, ye, and low-ﬁdelity, yc. The number of
samples of the high and low-ﬁdelity model are ne and nc, respectively. The output of
the multi-ﬁdelity analysis is a surrogate model, again denoted by ˆy. This surrogate is
a corrected low-ﬁdelity approximation that takes into account high-ﬁdelity objective
information. The low-ﬁdelity correction is usually done by two methods: additive
scale factor ∆(x) (the diﬀerence between ﬁdelities is modeled) and multiplicative
scale factor ρ (x) (the ratio between ﬁdelities is modeled). A general framework for
the correction is given by
ye (x) = ρ (x) yc (x) + ∆(x)
Typical scenarios where designers ﬁnd varying ﬁdelity models are: a) governing
equations that capture diﬀerent amount of physical details; b) fully converged versus
partially converged runs for iterative solvers, such as CFD and CSD simulations; c)
heuristic approximation models versus physics-based ones; and d) converged solutions
for diﬀerent discretization sizes.
Within the local approximation techniques, Haftka [77] proposes zero order scaling
with both multiplicative and additive scaling. In the same work, Haftka also presents
a ﬁrst-order scaling approach when the ﬁrst-order derivatives of the varying ﬁdelity
models are available.
Eldred et al.
[53] formulate an extension of Haftka’s work
to the second-order scaling.
The applicability of the last extension is reduced to
models where the second-order derivative could be eﬃciently assessed. Same authors
[53] develop a hybrid approach that combines the multiplicative and the additive
correction. These methods improve the surrogate only in the neighborhood of single
points because of which these techniques are called local approximation techniques.
Global surrogate models improve the accuracy of the low-ﬁdelity model globally
58

by employing a training data-set. Global approaches allow improving the accuracy
of the models with respect to the ones that are built only out of high-ﬁdelity training
data-sets. The reason is that the scale factors may behave better than the objective
itself, as is shown by Keane [105] and Leary et al. [122]. One can ﬁnd response
surfaces, and Gaussian processes, among the typical multi-ﬁdelity approaches.
Manson et al. [132] build response surfaces with two variable-ﬁdelity aerodynamic
codes.
Knill et al.
[114] use Euler equations to construct response surfaces that
represent a correction to the linear theory, which reduces computational burden.
Vitali et al. [189] propose a correction response surface to relate the high-ﬁdelity
models with the low-ﬁdelity ones to study the crack propagation in the skin of the
stiﬀened panel. Keane and Pretruzzelli build a multi-level wing design environment
in the conceptual design of a commercial airliner wing [107]. However, the interest
herein is in the probabilistic multi-ﬁdelity methods.
In what follows a derivation of a multi-ﬁdelity Gaussian process approach is pre-
sented. The corrected Gaussian process could be written as
ye (x) = Zρ (x) yc (x) + Zd (x)
(27)
where Zρ and Zd are the Gaussian process multiplicative and additive scale factor,
respectively.
For simplicity, the multi-ﬁdelity algorithm developed herein assumes
only the additive factor, i.e., Zρ = 1 and only Zd is to be modeled. Eldred, Giunta
et al. [53] show that the additive scale factor is appropriate in a wider variety of
problems. Let Xe and Xc be the expensive and the cheap sample points, respectively.
The values of the low and high-ﬁdelity stochastic processes at the training set points
are Yc (Xc) and Ye (Xe), respectively. They are treated as noisy realizations of the
deterministic models yc and ye as in Section 2.5. The whole training data set could
be written as y (X) = {yc (Xc) , ye (Xe)}
In order to build this multi-ﬁdelity approach, cheap and expensive data must
59

be correlated.
Kennedy and O’Hagan [109] propose an auto-regressive model; it
assumes that nothing can be learned about the objective function from the low-
ﬁdelity model provided that the high-ﬁdelity model value ye at the point x(i) is known.
Mathematically, this condition takes the form of cov

Ye
 x(i)
, Yc (x) |Ye
 x(i)
= 0,
∀x ̸= x(i).
Using the mentioned auto-regressive model and Zρ = 1, the expensive Gaussian
process surrogate, Ze, takes the form
Ze (x) = Zc (x) + Zd (x)
(28)
where Zc is the low-ﬁdelity Gaussian process.
The covariance matrix of the whole data is given by
cov [Y (X) , Y (X)] =



σ2
cΨc (Xc, Xc)
σ2
cΨc (Xe, Xc)
σ2
cΨc (Xe, Xc)
σ2
cΨc (Xe, Xe) + σ2
dΨd (Xe, Xe)



(29)
More details about the derivation of Equation 29 are found in Forrester, S´obester,
and Keane’s work [64]. For compactness’ sake, the covariance matrix of the whole
data, cov [Y (X) , Y (X)], is denoted as C.
Now, there are two correlation matrices as opposed to Section 2.5 where there is
only one. The free parameters are µc, µd, σc, σd θc, and θd. The low-ﬁdelity data is
assumed to be independent of the high-ﬁdelity one. So, MLE for µc, and σc can be
assessed with Equations 13, and 14, respectively. Optimal values of θc, according to
the MLE, are obtained with the same numerical process explained in Section 2.5. In
such manner the low-ﬁdelity surrogate ˆyc can be built.
In order to estimate µd, σd, and θd, the variable d is deﬁned as the diﬀerence be-
tween the expensive observations and the cheap surrogate prediction in the expensive
sample points Xe, i.e., d = ye −ˆyc (Xe)
60

Ignoring terms that are already deﬁned, and therefore constant, such as µc, σc,
and θc, the likelihood of the expensive data can be written as
−ne
2 ln
 σ2
d

−1
2 ln |Ψd (Xe, Xe) | −(d −1µd)′ Ψd (Xe, Xe)−1 (d −1µd)
2σ2
d
(30)
The MLEs for µd, σd are obtained by taking the partial derivatives of Equation
30 respect to µd, and σd.
ˆµd = 1′Ψd (Xe, Xe)−1 d
1′Ψd (Xe, Xe)−1 1
(31)
ˆσ2
d = (d −1µd)′ Ψd (Xe, Xe)−1 (d −1µd)
ne
(32)
Taking the partial derivative of the likelihood respect to the hyper-parameter θd
is analytically tedious. As in Section 2.5, a GA is used to maximize the likelihood
against θd. The optimal estimation obtained for this last hyper-parameter is denoted
as ˆθd.
The multi-ﬁdelity surrogate prediction and mean squared error at a point x can be
assessed once the Gaussian process hyper-parameters are tuned. Forrester, S´obester,
and Keane [64] provide a detailed derivation via augmented training data-set.
ˆye (x) = ˆµ + cTC−1 (y −ˆµ1)
(33)
ˆse
2 (x) = ˆσ2
c + ˆσ2
d −cTC−1c
(34)
where ˆµ = 1′C−1y
1′C−11 and
c =



ˆσ2
cψc (Xc, x)
ˆσ2
cψc (Xe, x) + ˆσ2
dψd (Xe, x)



61

2.6.1
Multi-Fidelity Regression
As explained in Subsection 2.5.1, the multi-ﬁdelity interpolation method fails when
dealing with noisy data from computer simulations, such as CSD and CFD codes [64].
The regressive model is achieved by adding a regression constant for the expensive
model, λe, to the diagonal of the high-ﬁdelity block that contains the correlation
matrix Ψd (Xe, Xe). Forrester et al. [64] propose also to add a regression constant,
λc, to the diagonal of the low-ﬁdelity block that contains the correlation matrix
Ψc (Xc, Xc). However, in the present work sparse sampling is expected on the low-
ﬁdelity model, so there is no need of the regression constant λc. For this case the
covariance matrix takes the form
cov [Y (X) , Y (X)] =



σ2
cΨc (Xc, Xc)
σ2
cΨc (Xe, Xc)
σ2
cΨc (Xe, Xc)
σ2
cΨc (Xe, Xe) + σ2
d
 Ψd (Xe, Xe) + λeI(ne,ne)




(35)
The regression constant, λe, resolves the problem of approximating noisy func-
tions. Notice that the covariance matrix could be written as C + λ. Again, Gaussian
radial basis functions are employed.
The unknown hyper-parameters of the multi-ﬁdelity approach are not only the
same ones as for the interpolating case, (µc, σc, θc, µd, σd, θd), but also λe.
These
hyper-parameters are again obtained from the MLE of the observed data [66]. λe
adds one extra variable to the MLE process; it increases the tuning computational
cost.
MLE for µc, and σc can be assessed with Equations 13, and 14, respectively. MLE
for θc are obtained with the same numerical process as the one explained in Section
2.5. In such manner the low-ﬁdelity surrogate, ˆyc, can be built.
The estimations for µd and for σd are assessed similarly as in Section 2.6. The
assessment of the optimal hyper-parameters ˆθd and ˆλe is done by a GA since the
62

process is analytically intractable.
The resulting predictor, ˆye,r, is given by
ˆye,r (x) = ˆµr + cT 
C + ˆλ
−1
(y −ˆµ1)
(36)
where
ˆµr =
1′ 
C + ˆλ
−1
y
1′

C + ˆλ
−1
1
1 is a ne + nc-dimensional vector of ones.
Notice that again the optimal hyper-
parameter values are denoted by a hat,ˆ.
c is the correlation vector between the point x where the prediction is pursued
and the observed points Xc and Xe
c =



ˆσ2
cψc (Xc, x)
ˆσ2
cψc (Xe, x) + ˆσ2
dψd (Xe, x)



where the ψ [, ] is given by Gaussian radial basis functions, see Equation 9.
The multi-ﬁdelity approach permits not only to predict the function value in new
sample points, but also to estimate the uncertainty in the prediction ˆs2
e,r (x). In the
regressive case it is given by Equation 37
ˆs2
e,r (x) = ˆσ2
c,r + ˆσ2
d,r

1 + ˆλe

−cT 
C + ˆλ
−1
c
(37)
where
ˆσ2
c,r = (yc −1 ˆ
µc,r)′ Ψc (Xc, Xc)−1 (yc −1 ˆ
µc,r)
nc
ˆσ2
d,r =
(d −1 ˆ
µd,r)′ 
Ψd (Xe, Xe) + ˆλeI(ne,ne)
−1
(d −1 ˆ
µd,r)
ne
ˆµc,r = 1′Ψc (Xc, Xc)−1 yc
1′Ψc (Xc, Xc)−1 1
63

ˆµd,r =
1′ 
Ψd (Xe, Xe) + ˆλeI(ne,ne)
−1
d
1′

Ψd (Xe, Xe) + ˆλeI(ne,ne)
−1
1
It is worth mentioning that the model error for the regressive case ˆse,r is not zero
at the observed points because the value of the regression constant ˆλe diﬀers from
zero. It can produce a non-zero value of the ExI at the observed points, which can
lead to optimizations trapped at sample points as explained in the Subsection 2.5.2.
2.6.2
Multi-Fidelity Re-interpolation
The multi-ﬁdelity regression mean squared error, Equation 37, is not zero at the ob-
served points because the value of ˆλe is diﬀerent from zero. It may result in maximum
ExI at design points previously sampled, which would stall the EGO algorithm on
deterministic experiments.
Forrester et al. [64] redeﬁne the mean squared error when regressive multi-ﬁdelity
approaches are used on deterministic experiments. They build an interpolation of
the regressive multi-ﬁdelity surrogate through the values predicted by the regression
meta-model at the observed points. It is called re-interpolation. The re-interpolating
predictor is the same as the regressive one, i.e., ˆye,ri = ˆye,r∀x and ˆµri = ˆµr.
The re-interpolation mean squared error, ˆs2
e,ri, that Forrester et al. [64] derive is
given by
ˆs2
e,ri (x) = ˆσ2
c,ri + ˆσ2
d,ri −cTC−1c
(38)
where ˆσc,ri = ˆσc,r because λc = 0 and
ˆσd,ri = (dri −1ˆµd,r) Ψd (Xe, Xe)−1 (dri −1ˆµd,r) /ne
(39)
dri =1 ˆµr + {c
 x(1)
e

, ..., c
 x(ne)
e

}T 
C + ˆλ
−1
(y −1 ˆµr) −ˆyc (Xe)
(40)
64

It is important to realize that in Equation 38 c and C depend on ˆσc,ri and ˆσd,ri.
Equation 38 guarantees that the re-interpolating mean squared error ˆse,ri is zero
at the sample locations unlike what happens for the regression case ˆsr. Thus, the
ExIs (see Equation 46) at the observed points are zero in the re-interpolating case,
and then, the EGO process does not stall.
2.7
Adaptive Sampling
In Section 2.5 it is pointed out the engineers’ desire of using computationally burden-
some objective functions in design. A possible solution to these impractical simula-
tions is the construction of simpler and cheaper approximation models of the objective
function called surrogate models.
Suppose that due to long function evaluation times, there is a limit on the number
of function calls that can be done; where should one sample the intense function to
make the most out of the limited function call budget? A possible option is to choose
sample points iteratively, in places where the information gained in previous steps is
maximized according to a speciﬁc criterion. The reason to select this iterative process
is that the function is unknown.
No proactive strategy can be relied on when the intense function is unknown.
Therefore, traditional DoE space ﬁlling techniques do not provide eﬃcient outcomes
for limited function call budgets. When a surrogate is available, the only available
data are the current function values on the points already sampled and the surrogate
prediction built out of them. Thus, it is crucial to use a reactive strategy that is based
on previously observed data (as opposed to the blind strategies of conventional DoE)
and therefore cleverly samples the function in new designs. This reactive strategy is
carried out by an inﬁll criterion. This reactive procedure is usually called adaptive,
on-line, or inﬁll sampling.
The inﬁll criterion of an adaptive sampling technique is based on the following
65

two aims or a weighted combination of them: 1) accurate optimal value or 2) enhance
the global accuracy of the model. The ﬁrst aim is called exploitation, whereas the
second is called exploration. Exploitation focuses on the good performing areas using
the surrogate prediction, whereas exploration focuses on areas with high uncertainty
where there exists a lack of sampling. Therefore, Bayesian surrogate models, such as
GRFMs, are brought to provide a predictive distribution to the adaptive sampling
algorithm.
When a surrogate is available, new data points3 are evaluated with the purpose of
updating the surrogate and gaining information about the unknown objective. It is
done by the optimization of the inﬁll criterion according to the desired balance of the
two previous aims. An inﬁll criterion is a function that measures how interesting a
design point is. While oﬄine DoE sampling methods use the same sample density in
regions of both low and high performance, adaptive sampling distributes the samples
according to exploitation, exploration or a combination of both to make the most out
of the limited objective function call budget.
The simplest inﬁll criteria fully focus on either of the two possible goals: ex-
ploitation or exploration. Other more advanced criteria combine both. Inﬁll criterion
examples are statistical lower bound, probability of improvement, expected improve-
ment, goal seeking, and conditional lower bound. Each of these methods has a dif-
ferent balance between exploitation and exploration. Since the present research is
interested in early design phases, a good balance between the aims is pursued. The
expected improvement (ExI) is a well-known inﬁll criterion that can eﬀectively solve
this trade-oﬀ; it has been popularized by Jones [99].
ExI is chosen in this work to drive the adaptive sampling algorithm. This criterion
has been extensively employed in conceptual design. Also, there exists a wide litera-
ture about it that makes it a really attractive criterion to extend adaptive sampling
3also known as inﬁll or update points.
66

techniques to domains with categorical and discrete-quantitative design variables.
Subsection 2.7.1 explains ExI in detail.
The adaptive sampling process is iterative. First, a warm-up sampling plan is
needed to ﬁrst initialize the Bayesian surrogate. Normally space ﬁlling techniques
serve for this purpose. Then, the ﬁrst inﬁll point (or update) is obtained from the
inﬁll criterion optimization, and the surrogate model is updated with the ﬁrst inﬁll
point. Again, the inﬁll criterion searches the new surrogate for a new inﬁll point.
This iterative process is continued until a speciﬁed convergence criterion is satisﬁed.
2.7.1
Towards the Expected Improvement Inﬁll Criterion
The expected improvement has been previously branded as an adaptive sampling
criterion. It uses the predictive distribution of the probabilistic meta-model to ﬁnd a
new point to sample according to a balance between exploration and exploitation.
The idea of ExI was ﬁrst introduced by Mockus et al. [136]. ExI compares the
current best sample point, ymin, with the surrogate prediction over the design space.
Note that objective minimization is supposed in the next derivation without loss of
generality.
The Bayesian surrogate brands the function’s value, y (x), as a random variable,
Y (x). The probability distribution of Y (x) could be seen in Figure 6 (even though
the function y (x) is deterministic, the surrogate output is a probability distribution).
If a Gaussian surrogate is used, Y (x) is a normally distributed random variable
with a prediction ˆy (x) and a mean squared error ˆs (x) around the prediction. The
improvement with respect to the current best sample point of a design concept is a
random variable which is deﬁned as
I (x) = max (ymin −Y (x) , 0)
(41)
67

If Kriging is the chosen Bayesian surrogate, the probability of the random variable
Y (x) is given by a Gaussian distribution
1
ˆs (x)
√
2π
exp
 
−[Y (x) −ˆy (x)]2
2ˆs2 (x)
!
. The
probability of improvement, being the probability of I > 0, is given by
P [I (x)] =
1
ˆs (x)
√
2π
Z ymin
−∞
exp
 
−[Y (x) −ˆy (x)]2
2ˆs2 (x)
!
dY
(42)
Doing a change of variables from Y (x) to I (x), carrying out the integration, and
making use of the error function erf (.), the probability of improvement could be
written as
P [I (x)] = 1
2

1 + erf
ymin −ˆy (x)
ˆs (x)
√
2

(43)
The probability of improvement just quantiﬁes the likelihood of an improvement
with respect to the current best sample solution ymin. The amount of improvement
is not taken into account in the probability of improvement criterion. Nevertheless,
it is obvious that points in the bottom tail of the predictive distribution, far below
from ymin, represent a large amount of improvements, whereas the probability dis-
tribution close to ymin does not bring large improvements. However, the tail points
have low values of probability density. In order to consider these diﬀerent amounts
of improvement according to their probability, a weighted sum could be employed to
get the expected value of the improvement; it is called expected improvement. The
ﬁrst moment of area of the improvement is used for this purpose, which is the red
area of the probability distribution in Figure 6.
E [I (x)] = E [max (ymin −Y (x) , 0)]
which particularized for the Kriging predictor, N (ˆy (x) , ˆs (x)), takes the form of
E [I (x)] =
1
ˆs (x)
√
2π
Z ymin
−∞
(ymin −Y (x)) exp
 
−[ˆy (x) −Y (x)]2
2ˆs2 (x)
!
dY (x)
(44)
68

Figure 6: Expected Improvement
If one changes the variables according to I (x) = ymin −Y (x), Equation 44 trans-
forms to
E [I (x)] =
1
ˆs (x)
√
2π
Z ∞
0
I (x) exp
 
−[ˆy (x) + I (x) −ymin]2
2ˆs2 (x)
!
dI (x)
(45)
adding and subtracting ˆy (x) −ymin
ˆs (x)
√
2π
exp
 
−[ˆy (x) + I (x) −ymin]2
2ˆs2 (x)
!
leads to two sim-
pler immediate integrals that can be easily evaluated. So, E [I (x)] can be written
as
E [I] =





ˆsφ
ymin −ˆy
ˆs

+ (ymin −ˆy) Φ
ymin −ˆy
ˆs

if ˆs > 0
0
if ˆs = 0
(46)
where φ (.), and Φ (.) are the standard normal probability and cumulative distribution
function, respectively.
There exist two main explanations for a large value of E [I (x)] in regions of the
design space. Firstly, the points already sampled hint that the expected value at x
is likely to be smaller than the current best sample point. Secondly, the deviation of
the distribution is spread because not many samples have been taken in the design
region where x seats. The ﬁrst reason supports exploitation, whereas the second does
exploration. Hence, E [I (x)] represents a balance between the two phenomena, and
the maximum value of E [I (x)] is a solid choice to pick the next sample point. Usual
69

stopping criteria for the inﬁlling iterative process are: 1) reaching certain number
of updates given by the limited function budget or 2) when E [I (x)] is less than a
certain percentage of the current best sample point.
Landscapes of the expected improvement possess similar characteristics. Their
values are zero at the sample points and are positive between them. The ExI land-
scapes are multi-modal.
They usually feature several ﬂat regions where the ExI
values are close to zero because of the high concentration of sample points. These
characteristics make the optimization of the expected improvement rather diﬃcult.
Speciﬁcally, gradient techniques are not normally used due to this multi-modality
nature of the ExI function.
Jones [99] points out that E [I (x)] is decreasingly and increasingly monotonic in
ˆy, and ˆs, respectively. These properties allow setting an E [I (x)] upper bound on the
design space. This bound could be employed by a branch and bound (BB) method
to guarantee optimality. Herein, the focus is to explore and exploit the design space;
therefore, guarantee of optimality and high accuracy in the successive new sample
points are not strictly necessary. Hence, the E [I (x)] maximization is done by well-
known methods like GA where a trade between precision and speed can be easily set
by the user.
2.8
Mixed-Integer Optimization
Many engineering, scientiﬁc, and management applications involve both discrete and
continuous decisions. Also linear and/or nonlinear relationships deﬁne the feasibility
and optimality of the solutions. These kind of problems are solved with mixed-integer
optimization (MIO) techniques. Possible application ﬁelds are management science,
process industry, ﬁnance, optical ﬁlter design [167], portfolio selection, chemical en-
gineering [56], machine learning [125], optimal design of gas and water distribution
70

networks, automobile engineering, aircraft design, and integrated circuit manufactur-
ing, among others.
Examples of discrete-quantitative variables in mechanical engineering are number
of items of one type, size of standard manufactured items, ... In aerospace engineering
these integer variables can be the number of engines, helicopter blades, stages in
compressor and turbine, and number of composite layers and their orientation, among
others.
The general MIO problem could be laid down as follows
min
f (xcont, xint)
(47)
subjected to:
gj (xcont, xint) ≤0,
j = 1, ..ri
hj (xcont, xint) = 0
j = ri + 1, ..ri + re
xL
cont ≤xcont ≤xU
cont
xL
int ≤xint ≤xU
int
xint ∈Z
(48)
xcont = [xcont,1, xcont,2, ...xcont,n1]
xint = [xint,1, xint,2, ...xint,n2]
where xcont and xint represent the set of continuous and integer variables, respectively;
gj and hj the inequality and equality constraints; xL
cont and xU
cont the lower and upper
continuous bounds; and xL
int and xU
int the lower and upper integer bounds. The number
of continuous and integer decisions are n1 and n2, respectively, whereas the number
of inequalities and equalities are ri and re.
MIO algorithms can be classiﬁed into two main ones: mixed-integer programming
(MIP), and stochastic searches. A MIP can deal with linear problems (MILP) or
nonlinear ones (MINLP). MIP can also deal with constraints by which some or all
71

decision variables are restricted to certain values. A popular MIP technique, called
branch and bound (BB), were developed only for linear problems MILP. Later, new
techniques were implemented to handle nonlinear convex problems. These techniques
are outer approximation (OA), generalized bender decomposition (GBD), and ex-
tended cutting plane technique (ECP). Also, extensions of BB were developed to
tackle not only convex but also non-convex problems. The convex extensions guaran-
tee local optima convergence. However, for non-convex problems these programming
techniques may provide the wrong global optimum.
MINLP are hard to solve because they have all the diﬃculties of both the integer
programming (IP) and non-convex programming (NLP). IP problems are of combi-
natorial nature that provokes a great increase of the possible integer combinations
as the dimension and density of the integer space, y, grows. For example, in an IP
problem with k dimensions with nd points each dimension, the possible combinations
are nk
d. The idea is to arrange the problem such that one can get information that
can be used to exclude large numbers of solutions from further consideration. This
is the philosophy of the Balas method for IP problems [11], and branch and bound
methods for MIP problems [36].
For non-convex and highly nonlinear problems, stochastic searches have been pro-
posed in the literature with more successful results than the ones of programming
techniques. Among the stochastic searches, one can ﬁnd: particle swarm optimiza-
tion, line-up competition algorithm, simulated annealing techniques, and genetic al-
gorithms.
In this section, a brief review of the mixed-integer convex programming techniques
is presented (it includes the techniques for linear problems). It is followed by the
discussion of the extensions of the convex programming to non-convex scenarios.
Finally, an introduction to the MIO stochastic algorithms is presented.
72

2.8.1
Convex Programming
The following approaches are explained: branch and bound, outer approximation,
generalized benders decomposition, and extended cutting plane. The target problem
is the one described by Equations 47 and 48 when f (·) and g (·) are convex. The
approaches that are described in this Subsection only guarantee global optimality
under convexity conditions.
2.8.1.1
Branch and Bound Algorithms
It was introduced by Balas [11] for binary linear programming problems with k vari-
ables. It sets all the variable to zero for a maximization problem; then, successively
assigns the value 1 to certain variables, in such a manner that a smaller subset of
all possible combinations, 2k, is suﬃcient to obtain an optimal solution or conclude
that no feasible solution exists. In order to do it, branching and bounding techniques
are employed.
Branching refers to the fact that throughout the process a tree is
branched to assign possible values (0 and 1 for the binary case) to the variables. The
bounding technique groups a subset of the possible decision variable combinations,
assign a lower bound to this subset, and assess whether the set is impossible or not.
The lower bound might permit comparing subsets to rule out some of them, reducing
the number of design combination evaluations, therefore it speeds-up the process.
Also, impossible sets are ruled out because none of the containing designs satisﬁes
the restrictions, reducing again the number of evaluations.
Dakin [36] developed a branch and bound method for MILP. It ignores the integer
restrictions and solves the problem as if all the variables were continuous (using LP).
This relaxation provides a good lower bound for maximization problems. One of the
integer variables, xint,j, is chosen for branching the current node into two child nodes.
The parental node relaxation problem has directly been solved and the value of the
variable j for the optimal solution is between l and l + 1. These values are used to
73

impose bounds to each of the new child node problems:
Child Problem 1. Parental node constraints plus a new bound xint,j ≤l.
Child Problem 2. Parental node constraints plus a new bound xint,j ≥l + 1.
The same process is repeated until the linear relaxation solution has integer values
in all the integer variables and the solution is feasible, then non-improvement can be
achieved by the expansion of the node. This reminds of the bound obtained by Balas
for the node descendant subset. Again, comparison between feasible nodes leads to
the discard of nodes and their descendant subsets. It is noticeable that solving MILP
problems generally needs quite longer than its continuous counterpart, LP problems,
due to the mentioned combinatorial nature of the MILP problems.
Gupta and Ravidran [76] study the computational feasibility of branch and bound
methods in solving convex nonlinear programming problems. Quesada and Gross-
mann [148] apply the BB method to convex problems. The idea is to solve a NLP
subproblem by linearizations similar as in OA method, then a BB algorithm solves
the mentioned NLP subproblem at the nodes where integer feasible solutions are
found. Later, Borchers and Mitchell [20] describe a branch and bound method for
mixed-integer nonlinear programs with convex objective functions and constraints.
The BB method is usually attractive when NLP problems are inexpensive and only
few of them are to be solved. This conditions normally occur when a few discrete
variables are present in the problem.
2.8.1.2
Outer Approximation and Generalized Benders Decomposition
The MINLP problems are solved with successive solutions of related MIP problems.
The methods divide the MINLP problem into a subproblem that has ﬁxed discrete
variables, converting it into a NLP problem and a linear master problem MILP, as
Duran and Grossmann [47] demonstrate in their OA algorithm. Another OA solver
74

is presented by Fletcher and Leyﬀer [59]. Geoﬀrion [71] presents Bender’s approach
and generalizes it to a wider class of programming techniques in which subproblems
do not need to be linear.
The diﬀerence between OA and GBD resides in the deﬁnition of the MILP mas-
ter problem. The GBD master problem only considers active inequalities and the
continuous bounds are disregarded, whereas OA methods use tangential hyperplanes
(or linearizations) to set the feasible space in the master problems. These feasible
space linearizations provide a successively shrinking space for the master problem
that represents convergence to the solution.
2.8.1.3
Extended Cutting Plane
Unlike other methods previously explained in this Section (BB, OA, and GBD), ECP
does not make use of NLP subproblems. It relies on an iterative solution of the master
linear problem, which is a MILP problem, by successively adding linearizations; see
Westerlund and Pettersson [197]. The chosen linearization is the one of the constraint
that is the most disrespected at the previous solution point in the iteration process.
Since the functions f (x, y) and g (x, y) are convex, the nonlinear feasible region
deﬁned by g (x, y) is outer-approximated. So, the addition of hyperplanes throughout
the iterative process provides a non-decreasing bound for the minimization problems.
2.8.2
Non-Convex Programming
Some programming algorithms are implemented to solve non-convex and multi-modal
MINLP problems. These global deterministic optimization methods for mixed-integer
non-convex problems normally rely on a branch and bound procedure. The manner
to perform the branching on the variables is the main diﬀerence across these methods.
Convex relaxation is successfully employed to obtain solutions as demonstrated by
Tawarmalani and Sahidinis [182], and Floudas [60]. Smith and Pantelides [173] refor-
mulate the problem and apply convex relaxation. Then, they use an spatial branch
75

and bound search. Kesevan and Barton [111] propose their branch and cut method.
Other methods branch a tree on both discrete and continuous variables. Others
perform the BB on the continuous variables and solve each node problem, a MINLP,
by any of the methods explained previously. Other ones branch on the discrete vari-
ables and then use BB on nodes where feasibility on discrete variables is found. The
existing methods are not explained in details because they are not used in this work.
These deterministic non-convex programming methods are generally outperformed
by stochastic or random mixed-integer searches.
2.8.3
Stochastic or Random Search
In the last two decades many random search algorithms are developed for MINLP.
Among them, one can ﬁnd the following ones: particle swarm optimization by Yiqing
et al. [206] , line-up competition algorithm by Yan et al. [203], diﬀerential evolution
by Regulwar [152], simulated annealing techniques, evolution strategies (ES) by Li
et al.
[125], and GA by Yokota [207] and Deep et al.
[44].
Stochastic methods
are more powerful than the methods introduces in Subsection 2.8.2. As opposed to
programming methods, stochastic methods do not rely on gradient calculations after
continuous relaxation of the integer set. Gradients are not meaningful in the discrete
world. Also, the gradient information is not always useful towards global optimum
in multi-modal functions.
There is a enormous literature available for stochastic
processes, but only GAs are employed in this research.
Genetic algorithms are stochastic algorithms that copy the principles of genetics
and natural selection. Holland [88] introduced the concept of GA as a search method;
De-Jong [39] was the ﬁrst one in using it to solve an optimization problem. The set
of candidate solutions or individuals is called population. The ﬁrst population is ran-
domly chosen and it evolves toward better solutions. In each generation, individuals
are assigned a ﬁtness and rank according to their ﬁtness value. A new population is
76

formed from the current population based on the best ranked individuals and new
individuals obtained by crossover and mutation. The same process is repeated for
new generations until satisfactory ﬁtness levels are achieved or a maximum number
of generations is reached.
The random search philosophy of MINLP methods is similar to the one of their
continuous counterparts. The ﬁtness assignment and ranking techniques are usually
the same. However, the main diﬀerences for the GAs lie on the handling of integer
restriction, population mutation, and population crossover functions.
Generation techniques to initialize the ﬁrst population of a mixed-integer problem
are usually performed by using discrete techniques, such as full factorial techniques, or
just rounding-oﬀavailable continuous generation techniques, such as random Latin
hypercube and space-ﬁlling Latin hypercubes[113]. Forrester, S´obester and Keane
[65] explain in detail these techniques.
Regarding mutation and crossover techniques the simplest way to deal with integer
variables is to utilize continuous mutation and crossover functions pretending that the
integer variables are continuous. Then, simple truncation can convert the crossover
and mutation children back into the discrete world. Nevertheless, there exists a clear
drawback for this truncation: the step-size could be diminished to a so small value
that the necessary leap between two consecutive discrete points is not reached with
the consequent catastrophic results, as Li [125] and Deep [45] point out. It is because
in discrete spaces the smallest distance in l1-norm between two points is greater than
zero as opposed to what happens in continuous spaces. Hence, when the step size is
smaller than the smallest distance between discrete values, search stagnation occurs.
The situation is worse for categorical variables where a possible continuous relaxation
assumes neighboring values that may produce convergence to a local optima instead
of a global one.
Several mutation and crossover functions have been proposed in the literature for
77

the integer case. The principle of maximum entropy expresses maximum uncertainty
with respect to everything but the given information, as Jaynes [93] shows. Rudolph
[159] proposes a mutation algorithm for unbounded integer search spaces by using
this principle of maximum entropy: the resulting mutation distribution obtained
by Rudolph is a geometric distribution. Then, he applies the resulting distribution
to GA and ES. The latter distribution is extensively employed when dealing with
integer design variables. For the non-numeric variables, Li et al. [127] follow the
same maximum entropy principle to get to the conclusion that a uniform probability
mutation distribution is the most convenient.
They apply this uniform mutative
distribution in an ES.
Regarding crossover functions for integer and categorical variables, less instances
are found in the literature. Laplace crossover is proposed by Deep and Thakur [45],
although later truncation is needed to get integer values for the children.
Some
crossover techniques from continuous GA can be employed in the discrete case. The
main crossover techniques could be divided into two main groups:
1) Scattered Crossover. Genes are chosen with equal probability from the parents.
Thus, it combines the parental genes to form the children.
2) Intermediate or Blending Crossover.
An arithmetic or weighted sum of the
parental chromosomes is used to produce the children.
Note that crossover techniques can be based on not only two parents but also
more than two parents.
The scattered crossover can be easily applied to all kinds of discrete variables. The
intermediate technique can also be applied to discrete-quantitative variables, however
intermediate crossover is not useful for non-numeric variables due their lack of order
and intermediate values.
The main limitation of the GA for mixed-integer optimization is that none of
78

the freely available software provides open access to the generation, mutation, and
crossover functions.
For instance, a mixed-integer GA is available in MATLAB R
⃝
2011b, but the access to the MATLAB R
⃝ﬁles is restricted by encryption, therefore
variations of the code in the mutation, generation, and crossover functions are not
possible.
2.9
Multi-Objective Expected Improvement
Optimization techniques search for the best solution among all possible. However,
many engineering problems require the simultaneous optimization of two or more con-
ﬂicting objectives: these scenarios are called multi-objective optimization problems.
Satisfactory trade-oﬀs between these conﬂicting objectives have to be reached in the
ﬁnal design.
Typical ﬁelds where multi-objective optimization problems can be found are ﬁ-
nance, process design, aircraft and automobile design, robust design, or wherever
trade-oﬀs between two or more conﬂicting objectives exist.
Within the aerospace
industry, typical objectives are lightweight, high-performance, low-cost, robustness,...
Common instances of multi-objective optimization are the maximization of the air-
craft performance while minimizing engine fuel consumption; the minimization of the
weight while maximizing the strength of a particular aircraft component; and robust
design of a aircraft wing where the six-sigma objective function is included.
In mathematical terms, the general multi-objective optimization problem can be
written as follows:
minxcont,xquant,xnom [f1 (xcont, xquant, xnom) , ..., fm (xcont, xquant, xnom)]T
(49)
79

subjected to,
g (xcont, xquant, xnom)
≤
0
(50)
h (xcont, xquant, xnom)
=
0
(51)
xL
cont ≤
xcont
≤xU
cont
(52)
xL
quant ≤
xquant
≤xU
quant
(53)
xL
nom ≤
xnom
≤xU
nom
(54)
xquant
∈
Dquant
(55)
xnom
∈
Dnom
(56)
where xcont, xquant, and xnom represent the set of continuous, discrete-quantitative,
and nominal variables, respectively; [f1, ..fm] the m-component objective vector; g
and h the inequality and equality constraint vector; xL
cont and xU
cont the lower and
upper continuous bound vector; xL
quant and xU
quant the lower and upper discrete-
quantitative bound vector; and xL
nom and xU
nom the lower and upper nominal bound
vector. Finally, Dquant and Dnom represent the discrete subsets at which the discrete-
quantitative and nominal variables are restricted, respectively.
When dealing with more than one objective that are in conﬂict with each other, it
is usually not possible to ﬁnd a feasible solution which is optimal for all the conﬂicting
objectives. A subset of solutions must be considered optimal in terms of trade-oﬀs. It
means that it is not possible to improve one of the objectives without degrading one
or more of the others within this optimal family. No point of this optimal family of
solutions could be said to be better than other one, unless a given importance weight
of the objectives is known a priori. This optimal family is called Pareto optimal set
in honor to the economist Pareto [144] who introduced this concept within the ﬁeld
of welfare economics. Later, it has been extended to engineering and social sciences.
Before deﬁning formally a Pareto set, the concept of domination is presented. Let
us assume that two points y and z are in the solution space, i.e., y, z ∈Rm. The
80

point y strictly dominates z if and only if ∀i ∈1, .., m : yi ≤zi and y ̸= z. A usual
shorthand notation of strict domination is y ≺z. If a point y strictly dominates or
equals other point z, then the following notation is used y ⪯z. A ﬁnite set of points
P ⊂Rm dominates or equal a point z, in mathematical terms P ⪯z, if and only if
∃y ∈P : y ⪯z. A set of points P is a non-dominated set if and only if all the points
in the set P are mutually non-dominant.
A feasible point y is Pareto optimal or Pareto eﬃcient if and only if there is no
other feasible point that dominates it
y is a Pareto-optimal ∄z ∈S : z ⪯y
(57)
where S is the feasible objective space. In other words, for a Pareto optimal point
there exists no feasible solution point which would decrease some objectives without
provoking an increase in at least one of the other objectives. Generally, this concept
provides a set of points instead of a single solution. The Pareto front or Pareto set is
the set of points that are Pareto eﬃcient, see Equation 57; consequently, it is a non-
dominated front. Thus, a Pareto front includes designs which are so optimized that, in
order to improve one goal of any of its members, its performance in at least one of the
other goals diminishes. A Pareto front does not assume any speciﬁc relative weighting
between opposing goals. However, once the Pareto front is obtained engineers can
make a better compromise decision by weighting the conﬂicting goals.
Many multi-objective algorithms are developed to assess Pareto fronts, but in the
research herein, interest is in obtaining Pareto fronts when a precise evaluation of the
design is not available, but a prediction with a measure of the uncertainty is. The
prediction takes the form of a duple made of m-dimensional vectors: the mean µ and
the standard deviation σ of the prediction vector.
Typical quality indicators for ﬁnding Pareto fronts are the epsilon and hyper-
volume indicators. Deb, Mohan, and Mishra [42] select the next generation based on
81

f1
f2
Pareto Front
Objective 2 Minimized
Objective 1 Minimized
Figure 7: Pareto Front Example
the concept of epsilon-dominance. Friedrich, Horoba et al. [67] compare the eﬃciency
of the hyper-volume indicator with that of the epsilon based EMOA.
Emmerich [55] proposes the quality indicator hyper-volume, introduced by Zit-
zler [213] for comparing Pareto outcomes from diﬀerent EMOA, as an improvement
measure for extending the single-objective improvement to the multi-objective space.
The diﬃculty of the extension resides in the fact that the best found solution is not
clearly deﬁned in multi-objective problems. However, the increment of dominated
hyper-volume is a reduced scalar for the Pareto set with appropriate features as is
explained in Subsection 2.9.2.
Within this Bayesian scope, a survey in the literature shows that the two most
common criteria for multi-objective adaptive sampling are the statistical improvement
criterion proposed by Keane [106] and the hyper-volume based ExI by Emmerich [55].
The multi-objective adaptive sampling criterion used in this research is based on
the increment of dominated hyper-volume or Lebesgue measure, i.e., the one proposed
by Emmerich [55]. A study done by Wagner et al. [192] shows that this method based
on the Lebesgue measure satisﬁes the proposed necessary conditions, which justiﬁes
its use.
82

In Subsection 2.9.1 a formal deﬁnition of the hyper-volume of a set of points is
explained. Also, Fleisher’s approach, that exactly calculates the Lebesgue measure,
is presented. Later, in Subsection 2.9.2 the hyper-volume increment of a Pareto set
due to a new point is postulated as the improvement scalar needed to extend the ExI
criterion from uni-dimensional objective spaces to multi-dimensional ones.
2.9.1
Hyper-Volume Deﬁnition and its Calculation
Let V ol (U) be the m-dimensional volume of a space U ∈Rm. Then, the Lebesgue
measure or hyper-volume of a set of solutions P, H (P), quantiﬁes the size of the goal
space which is dominated by P, it could be written as
H (P) = V ol
 y ∈Rm : P ⪯y ⪯yref
(58)
where yref is a reference point to bound the inﬁnite domain dominated by the set of
points P.
If F is the set of non-dominated solutions of P, i.e., the Pareto front of P, then
H (P) = H (F)
A common and eﬃcient way to calculate the hyper-volume of a set of points P is
presented by Fleischer [58]. The algorithm he proposes trims oﬀhypercubes that are
dominated by P and adds their hyper-volume to the Lebesgue measure. The original
set of points is stored in a list denoted by List. In each step, a point of List is taken
and removed. Then, bound values, b, for this removed point are obtained based on
the remaining points in List. The contribution of the removed point is added to the
partial Lebesgue sum. Later, “’spawned points” are generated with the help of b. A
ﬁlter compares the remaining points in List with the “’spawned points”; then, the
non-dominated “’spawned points” are included in List. A more detailed discussion
is presented in Fleisher’s work [58].
83

2.9.2
Expected Improvement Based on Dominated Hyper-Volume
Similarly to the single-objective case, a statistical distribution [µ, σ] in the solution
space for each design space point, x, is given by the GRFM. The main challenge is
to generalize the single-objective best solution concept to the multi-objective Pareto
front idea. An improvement scalar similar to the one deﬁned in Equation 41 (see
Subsection 2.7.1) is searched for in order to generalize from one to multiple objectives.
The increment of the dominated hyper-volume of the Pareto front has been proved as
an appropriate scalar function for this purpose because of some of its characteristics.
Fleisher [58] proves that the dominated hyper-volume measure of a set is maximum
if the set is on the true Pareto front. Another attractive features is its behavior when a
new point, ynew, is added to the Pareto front F. The hyper-volume of the augmented
set H (F ∪ynew) increases if and only if no point in F dominates ynew. Hence, the
augmented set, F ∪ynew, could be branded as a better approximation to the Pareto
set than F.
As a drawback, normalization of the objective space is required to assure that
equal gains in objectives are equally reﬂected by the dominated hyper-volume mea-
sure. Finding crude bounds for the objective space is not normally hard, so a rude
normalization should not be really diﬃcult to perform. However, the bias of the im-
provement towards some objectives increases when only rough objective bounds are
available. Therefore, in these cases some care should be taken of when interpreting
the value of the improvement.
The improvement due to a new solution point y (x) is given by Equation 59
I (y (x)) =





H (F ∪y (x)) −H (F)
if no point of F dominates y (x)
0
otherwise
(59)
Once the improvement for multiple-objectives is deﬁned, the expectation of im-
provement has to be obtained in a similar way as is done in Subsection 2.7.1. The
84

main diﬀerence is that now the integration is on a multi-dimensional space Rm, where
m is the dimension of the objective space. The multi-objective E [I (x)] can be written
as
E [I (x)] =
Z
V
I (y) PDF (y) dy
(60)
where V represents the solution space. The integral is over the objective space, which
is assumed continuous in this research regardless the nature of the design space.
The integrand in Equation 60 is intended to be computed with a multi-objective
meta-model [µ, σ]. Monte Carlo integration of Equation 60 could be done, but it is
time consuming and not accurate. Emmerich [54] proposes a direct computation for
Equation 60 for the case of two objectives; it is explained in this Subsection with a
notation similar to the one utilized by Emmerich.
The idea to direct integrate Equation 60 is to divide the integration region into
boxes, where piecewise integration can be directly done, converting the integral into a
sum over the boxes. Let us assume that the current Pareto front consists in q points
y1, ..., yq.
A sorted list of all the i-components of the Pareto front points is b(1)
i , ..., b(q)
i . The
coordinates bi for i = 1, ..., m are called the grid coordinates. The coordinates bi are
augmented with b(0)
i
= −∞, b(q+1)
i
= yref
i
, and b(q+2)
i
= ∞for i = 1, ..., m.
The grid cells are named C (i1, i2, ..., im). Grid cells are deﬁned by their upper
and lower bound vectors: l (i1, i2, ..., im) and u (i1, i2, ..., im)
u (i1, i2, ..., im) =

b(i1+1)
1
, b(i2+1)
2
, ..., b(im+1)
m
T
l (i1, i2, ..., im) =

b(i1)
1
, b(i2)
2
, ..., b(im)
m
T
Another auxiliary variable required to directly integrate the expected improvement
in multi-objective problems is the vector v (i1, i2, ..., im) ∈Rm. The jth-component
of v (i1, i2, ..., im) is the jth-component of the intersection point between the Pareto
85

surface and a j coordinate line4 that passes through the lower bound of the given cell,
l (i1, i2, ..., im). v (i1, i2, ..., im) is called the cell reference point vector.
Cells are classiﬁed into active and inactive cells. The former ones are those where
the contribution to the integral is diﬀerent from zero, whereas the latter are those for
which the contribution is null. If a cell meets one of the following criteria, the cell is
branded as inactive
1) Cell lower bound is dominated or equal to points in the Pareto front F, i.e.
F ⪯l (i1, i2, ..., im).
2) Cell upper bound does not dominate the reference point, i.e., u (i1, i2, ..., im) 
yref.
Hence, active cells, denoted by C+, are those whose upper corner dominates the
reference point and whose lower bound dominates at least one point of the Pareto
front F.
The expected improvement could be written in terms of a sum throughout all the
active cells,
E [I (x)] =
X
C(i1,i2,...,im)∈C+
δ (i1, i2, ..., im)
(61)
where the active cell contribution δ (i1, i2, ..., im) is given by
δ (i1, i2, ..., im) =
Z
y∈C(i1,i2,...,im)
I (y) PDF (y) dy
(62)
Once the integration of Equation 60 has been divided, active cell contributions
are discussed.
For clarity and compactness’ sake, cell indexes are omitted.
The
improvement in a cell, I (y), can be divided into the contribution of the hyper-volume
of the non-dominated part in the hypercube [u, v], denoted by L+ (see Figure 8), and
4In this report, a j coordinate line refers to a line parallel to the jth coordinate axes
86

the “boundary contribution”, denoted by B+ (see Figure 8), given by [y, v]−[u, v] for
the two-objective case. For more than two objectives, the “boundary contribution”
has a more complex form. Then, in the two-objective case, the I (y) takes the form
I (y) = V ol ([y, v] −[u, v] + L+)
The reference point for the increment of the dominated hyper-volume of each cell
is given by the vector v. The improvement expression can be simpliﬁed by realizing
that S−= [u, v] −L+, where S−is the fraction of dominated hyper-volume inside
the hypercube [u, v], see Figure 8. Then, I (y) can be written as,
I (y) = V ol
 [y, v] −S−
(63)
Note that correction terms must be added for more than two dimensions.
Therefore, in order to calculate cell contribution to I (y), two volumes should be
obtained,
1) V ol ([y, v]) which is y dependent.
2) V ol (S−) which is independent for all y within the same cell.
In order to calculate the volume S−for the two-objective case, it is enough to
obtain the Pareto points that are dominated by the upper point of the cell, i.e.,
∀y ∈F : u ⪯y. For higher dimensions, the assessment of S−is more diﬃcult.
By plugging 63 into 62 one gets two terms for the cell contribution δ (i1, i2, ..., im) =
δ1 (i1, i2, ..., im) + δ2 (i1, i2, ..., im)
δ1 (i1, i2, ..., im) =
Z
y∈C(i1,i2,...,im)
m
Y
i=1
(vi −yi) · PDF (y) dy1dy2
(64)
δ2 (i1, i2, ..., im) = V ol
 S− Z
y∈C(i1,i2,...,im)
m
Y
i=1
PDF (y) dy1dy2
(65)
Equations 64 and 65 could be analytically integrated as follows
87

y1
y2
v1
v2
⃗v
⃗yref
⃗l
⃗y
⃗u
Figure 8: Expected Improvement Integration Area for Two-Objective Case
δ1 (i1, i2, ..., im) = V ol
 S− m
Y
i=1

Φ
ui −µi
σi

−Φ
li −µi
σi

(66)
δ2 (i1, i2, ..., im) =
m
Y
i=1
(Ψ (vi, ui, µi, σi) −Ψ (vi, li, µi, σi))
(67)
where Ψ (a, b, µ, σ) = σφ
 b−µ
σ

+(a −µ) Φ
  b−µ
σ

, and φ (.) and Φ (.) are the standard
normal probability and cumulative distribution function, respectively. It is necessary
to remember that in this Subsection just the two-objective extension of expected
improvement has been presented. Correction terms must be included for more than
two objectives.
88

2.10
Fenestron Tail Rotors
Fenestron tail rotors5 have been frequently used as anti-torque devices. They are
normally used in light and medium size helicopters. The main advantage of fenestron
tails is the lower power requirements than conventional open tail rotors to obtain a
given thrust. The ducting increases the anti-torque device eﬃciency by reducing blade
tip losses and adding some extra thrust. In order to obtain this eﬃciency enhance-
ment, ﬂow separation has to be avoided due to a proper inlet lip design. Additionally,
the ﬁn protects the anti-torque device from the main rotor wake, reducing the inter-
ference eﬀect of the conventional rotor-craft [138]. It also reduces the ﬁn interference
with the tail rotor [154].
Other signiﬁcant advantage is the reduction of noise, which makes military rotor-
crafts more diﬃcult to detect and civil rotor-crafts less disturbing for heliport com-
munities. Edwards et al. [50] obtain a dramatic improvement in the sound quality:
the total helicopter noise is reduced up to 6 dBA during hover and forward ﬂight with
reductions in tail rotor harmonics of 5 to 20 dBA. The helicopter noise spectrum is
extended over higher ranges of frequencies as the number of blades increases. Un-
fortunately, humans are more sensitive to these high frequencies. Nevertheless, this
high part of the spectrum is better absorbed by the atmosphere.
Other pros are the protection of the anti-torque device by the ﬁn, which reduces
the risk of any object impact and leads to safer ground approaching and landing [138].
Mouille [139] argues that more than 20% of helicopters crashes are due to a fail or
impact on tail rotors, so the increase in reliability and the improvement in security
provided by the fenestron rotors are important.
The main fenestron rotor drawback is the increase in anti-torque device weight.
The structural requirements of the hub are larger than for a conventional tail rotor
5Also called shrouded tail rotor or fan-in-ﬁn.
89

conﬁguration. Since centrifugal forces are inversely proportional to the rotor diameter
for the same rotor angular speed, fenestron centrifugal forces tend to be higher than
in the conventional tail case[154]; hence, more structural requirements are needed.
Normally fan-in-ﬁn rotors have more blades than conventional tail rotors. How-
ever, fenestron blade spans are shorter than in the conventional case. The rotational
speed of the ducted tail rotors is higher than that of conventional rotors with the
purpose of keeping a same order of magnitude tip speed. Also, typical blade root
cut-oﬀs are located in more outboard sections for fenestron conﬁgurations than for
conventional cases.
90

CHAPTER III
RESEARCH METHODOLOGY
3.1
Introduction
The aim of this research is on modifying available techniques and developing new
ones to build approaches for the more eﬃcient use of computationally intense tools
for initial design assessment and discrete alternative selection. Using computationally
high-cost tools in conceptual design represents still a challenge for not only high-
ﬁdelity codes, such as CFD and CSD, but also mid-ﬁdelity tools.
Long evaluation times could have diﬀerent origins: complete physics-based models
for single-objective problems, iterative tools in multidisciplinary problems, and the
increase of design evaluations featured in multi-objective problems.
Taking a glance at the current literature, surrogate and adaptive sampling tech-
niques handle ineﬃciently design alternatives. These design alternatives, generated
by the iterative process of conceptual design, normally experience similar trends; how-
ever, surrogate and adaptive sampling techniques do not leverage these similarities.
Similar trends can be identiﬁed in scenarios of “Evolutionary Incremental Concepts”,
where changes from one concept to another are small, and other scenarios where
designers are aware of similar behavior across alternatives by experience.
As mentioned in Chapter 1.1, an improvement in the computational eﬃciency of
concept surrogates where there is a choice of design alternatives could be crucial to
use more eﬃciently intense tools in conceptual design. Certain objective functions
show similar trends across all categories and concepts. Raymer [151] argues that in
the conceptual phase designers normally rely on existing airfoils. Then, airfoils could
be considered as a non-numeric variable in early design. For instance, while designing
91

a helicopter rotor, diﬀerent blade airfoils result in similar ﬁgure of merit versus thrust
coeﬃcient curves. Therefore, it is reasonable to think that surrogates that cross-use
computationally intense observations across lifting sections can provide more accurate
surrogates than ﬁtting a surrogate for each airfoil (current state-of-the-art) for a given
function call budget.
In order to build this surrogate that leverages similar trends across categories, a
distance function for categorical variables has to be deﬁned. The Hamming distance,
employed to handle nominal input attributes in instance-based learning techniques,
can be one of the candidates to accomplish this task.
Furthermore, some design variables that are usually treated as categorical can
have some natural distance due to underlying parameterizations that are not visible
in the conceptual design stage. This intrinsic distance can be used to build the target
surrogate. Measurable properties of the non-numeric input relevant for the pursued
objective are combined to construct an intrinsic distance function for the categorical
variables.
Therefore, diﬀerent objective functions hint diﬀerent intrinsic distance
functions for the same categorical set.
This intrinsic distance enables the handling of categorical design variable as a
discrete-quantitative ones, which eases the surrogate building. An intrinsic distance
function can be deﬁned on a choice of materials when studying their structural re-
sistance. A ductile material is expected to have similar behavior than other ductile
material. However, its behavior compared to a brittle one is expected to be diﬀerent.
Thus, the intrinsic distance between the two ductile materials should be smaller than
that between a ductile and a brittle material.
Distinct solutions for the same engineering problem, especially when immediate
in the iterative conceptual design phase, usually experience landscapes very much
alike. Multi-ﬁdelity techniques are brought to re-use intense observations of previously
sampled concepts while building a surrogate of a new concept whose objective function
92

is unknown. Current methods do not re-use observations from previous concepts while
ﬁtting new concept surrogates.
Accuracy of the optima in early stages is usually traded for interactions between
design parameters, and sensitivity studies, among others. The combination of explo-
ration and exploitation provided by Bayesian adaptive sampling methods is appro-
priate to study these interactions and sensitivities. Much research has been done in
continuous adaptive sampling; however, no focus on discrete alternatives is found in
the literature.
Among the Bayesian adaptive sampling techniques, the use of Kriging surrogates
together with the expected improvement criterion gives an appropriate trade-oﬀbe-
tween the exploration and exploitation. Also, Kriging is a well-documented technique
as a quick glance in the literature shows. The next sample in each step is determined
by the maximization of the expected improvement in the design space.
The extension of this Kriging/ExI technique to explore and exploit surrogates that
leverage similar trends across categories would use more eﬃciently computationally
intense tools in conceptual design stages. Also, optimization/sampling techniques
that can deal with not only continuous but also discrete variables would increase the
eﬃciency in the use of these intense tools. MIO provides algorithms to optimize prob-
lems that possess both continuous and integer variables. Mixed-integer optimization
techniques can be adapted to include more eﬃciently non-numeric variables. This
adaptation is achieved by developing generation, cross-over and mutation functions
suited to deal with categorical variables.
Later, the development of adaptive sampling techniques for surrogates that lever-
age similar trends from previously sampled concepts would help in making the most
out of the limited function call budget on multi-modal and intense functions.
It is worth noticing that ExI functions have some same characteristics regardless
the objective functions: the value is always greater or equal than zero in the whole
93

space, the value is zero in design points already sampled, and monotonicity properties
in the mean prediction value and the mean squared error. This similarity between
ExI landscapes allows extrapolating eﬃcient mutation functions from one problem
to another. It prompts to study the eﬃciency of several mutation operators while
optimizing the expected improvement in a mixed-integer-categorical domain.
Then, the UH60A with fenestron tail is presented as the new concept for the
ECMF surrogate practical test. It represents an incremental concept, which is built
out of the UH60A with conventional tail, so they are expected to experience similar
trends. The fenestron baseline values and its weight estimation are discussed.
Finally, the diagrams that review and summarize the research methodologies for
categories and concepts are presented.
3.2
Research Questions: Hypothesis and Predictions
Research questions guide the investigations that are carried out in this thesis. It is
important to remark that the proposed algorithms herein have a practical engineering
purpose: develop tools for the more eﬃcient use of computationally intense models
that enhance the initial design assessment and discrete alternative selection.
The scientiﬁc method is not ruled out when performing the investigations. Even
though no knowledge of physical phenomenon is pursued, eﬃcient methods for con-
ceptual design are pursued. Once the problem to tackle has been deﬁned, a set of
research questions are formulated. Then, their corresponding hypothesis to be further
tested (with computational experiments) are discussed. Answers for these questions
need a deep understanding of the current design tools and their limitations. Finally,
results of the computational experiments are predicted.
3.2.1
First Question
Is it possible to build eﬃcient surrogates for design scenarios where there exists a
design categorical choice with similar trends?
94

a) Is it possible to build surrogates that cross-use computationally expensive ob-
servations across categorical choices with similar trends?
b) Which nominal distances allow building these eﬃcient surrogates?
c) Does the MIC surrogate outperform independent surrogate modeling for each
category (state-of-the-art)? How does the relative eﬃciency of these surrogates
with respect to the independent ones depend on the training set size?
3.2.1.1
Hypothesis
Nominal distances permit building a Bayesian surrogate that includes not only con-
tinuous and integer design inputs but also categorical. This surrogate is called mixed-
integer-categorical (MIC) surrogate. MIC surrogates cross-use observations across all
categories.
Hamming distance is a possible nominal distance that allows building MIC surro-
gates. For certain engineering objectives, intrinsic distance measures can be obtained
from functional dependence of the objective. MIC surrogates can also be based on
this intrinsic distance.
The cross-use of observations in MIC surrogates is eﬃcient for small training
set sizes where only global trends are shared across categories.
This cross-use of
observations is not exploited by traditional independent surrogates ﬁtted for each
category.
The size of the training set aﬀects the surrogate accuracy. The beneﬁt obtained
from other category observations becomes a rigidity when the training set becomes
large. Above a large training set size, observations from other categories can capture
high frequency information, which normally goes beyond common trends, and there-
fore can mislead the MIC surrogate, aﬀecting its accuracy. So, for large training sets,
independent surrogates, which have available enough information, can outperform
MIC surrogates.
95

3.2.1.2
Prediction
A proof of concept can substantiate the hypothesis.
In terms of the performance metrics, MIC surrogates outperform the traditional in-
dependent surrogates (one surrogate for each category) for the small range of training
sets. However, as the training set becomes larger, traditional methods gain eﬃciency
up to a point where its performance metric surpasses that of MIC surrogates.
A working deﬁnition of performance metrics will be presented later. “Eﬃciency”
means fewer observations are needed for a given accuracy or more accuracy for the
same number of computationally expensive observations.
The resulting MIC surrogate is appropriate to be employed in conceptual design
when a design categorical choice has to be made and objective functions are compu-
tationally intense.
3.2.2
Second Question
How could adaptive sampling approaches be eﬃciently extended to a choice of cate-
gories that experiments similar trends?
a) Can the MICGA extended from MIGA drive the ExI search while doing adaptive
sampling on MIC surrogates?
b) Is the adaptive sampling on MIC surrogates more eﬃcient than simultaneous
adaptive sampling on independent surrogates (state-of-the-art) in some range
of training set sizes?
3.2.2.1
Hypothesis
A set of categories can be adaptively sampled by applying the ExI criterion on MIC
surrogates. The use of more accurate surrogates results in more eﬃcient adaptive
sampling processes. Therefore, the MIC surrogate, proposed in R.Q. 1, is chosen as
the surrogate for the eﬃcient extension of the adaptive sampling techniques.
96

Adaptive sampling approaches on surrogates like the ones proposed in R.Q. 1
needs of the extension of mixed-integer stochastic search techniques to spaces that
contain non-numeric variables. This extension is possible by modifying the generation,
crossover, and mutation functions of a mixed-integer genetic algorithm (MIGA). The
MIGA extension to mixed-integer-categorical design spaces is called mixed-integer-
categorical genetic algorithm (MICGA). MICGA can drive the adaptive sampling
algorithm on MIC surrogates via ExI inﬁll criterion.
While ﬁnding Pareto fronts with small warm-up training sets, the adaptive sam-
pling on MIC surrogates via MICGA is more eﬃcient than the state-of-the-art (si-
multaneous adaptive sampling on independent surrogates).
3.2.2.2
Prediction
A proof of concept can substantiate the hypothesis.
When a one-objective function is adaptively sampled using a MIC surrogate, the
regions of high sampling density are the globally best performing areas but not the in-
dividual best performing regions for each category. It is an evidence that the MICGA
can adequately drive the expected improvement search on MIC domains.
The quality of the Pareto frontier is the performance metric for multi-objective
optimization. The adaptive sampling on MIC surrogates via MICGA outperforms the
simultaneous adaptive sampling on traditional surrogates in terms of the quality of
the Pareto frontier. It happens for small warm-up training sets. A working deﬁnition
of the quality of the Pareto frontier will be presented later.
The resulting adaptive sampling approach is appropriate to be employed in con-
ceptual design when a design categorical choice has to be made and objective functions
are computationally intense and multi-modal.
97

3.2.3
Third Question
Is it possible to build eﬃcient surrogates for design scenarios where there exist com-
putationally expensive observations from a previous concept with similar trends?
a) Is it possible to build surrogates that re-use computationally expensive obser-
vations from previous concepts with similar trends?
b) Do ECMF surrogates outperform mono-ﬁdelity surrogates (state-of-the-art)?
How does the relative eﬃciency of ECMF surrogates with respect to the mono-
ﬁdelity ones depend on the new concept training set size?
c) What is the inﬂuence of the old concept training set size in the performance of
ECMF surrogates?
3.2.3.1
Hypothesis
A multi-ﬁdelity approach allows reusing observations from a previously sampled con-
cept while building a new concept surrogate, these surrogates are called Evolutionary
Concept Multi-Fidelity (ECMF) surrogates.
The re-use of observations in ECMF surrogates is eﬃcient for small new concept
training set sizes because supplementary information is useful in these cases. It is
assumed that the new and old concept experience similar trends.
This re-use of
observations is not exploited by traditional mono-ﬁdelity surrogates.
Training sets sizes for both concepts aﬀect the ECMF accuracy.
a) The size of the new concept training set size aﬀects the ECMF performance.
Above a large new concept training set size, observations available from the new
concept provide high frequency detail. In this case observations of the previ-
ously sampled concept are less needed and can only pollute this high frequency
information degrading the ECMF accuracy. It does not happen to mono-ﬁdelity
surrogates, which have no observations from the previous concept.
98

b) The similar trends beneﬁt obtained from previously sampled concept obser-
vations becomes a rigidity when the old concept training set becomes large.
Above a large old concept training set size, high frequency information, which
normally goes beyond common trends, is captured, so it can mislead the ECMF
surrogate and reduce its accuracy. Thus, for large old concept training set sizes,
the performance of the ECMF surrogate saturates or even degrades as the old
concept training set becomes bigger.
3.2.3.2
Prediction
A proof of concept can substantiate the hypothesis.
In terms of the performance metrics, the ECMF surrogates outperform the tradi-
tional mono-ﬁdelity surrogates for small ranges of new and old concept training set
sizes. However, as the new concept training set size increases, mono-ﬁdelity surro-
gates gain eﬃciency with respect to ECMF ones in terms of the performance metrics.
Also, for large old concept training sets, the ECMF surrogate error does not decrease
as the old concept training set becomes larger. A working deﬁnition of performance
metrics will be presented later.
The resulting ECMF surrogate is appropriate to be employed in conceptual design
when there exist evolutionary concepts with similar trends and objective functions
are computationally intense.
3.3
Nominal Distance for Cross-Using Observations
As discussed in Section 2.5, the distance between design points is the input to tune
Kriging surrogates.
In order to include categories in the same meta-model, it is
necessary to deﬁne a categorical distance, which is also called nominal distance.
3.3.1
Requirements for Nominal Distance
The desired requirements for the nominal distance are the following:
99

Lack of order As explained in Section 2.1, non-numeric variables are characterized
for the lack of order.
Deﬁnition to keep radial basis functions meaningful Since Kriging meta-models
are employed in this work, the nominal distance should result in positive deﬁnite
covariance matrices. If two categories are too close (distance-wise), these matri-
ces may be non-positive deﬁnite. It would bring bias in the Kriging parameters,
which has consequent numerical problems.
Versatility to diﬀerent objectives Designers would like to have a nominal dis-
tance that is useful for many objective functions.
Once the desired characteristics of the nominal distance are deﬁned, the search for
the nominal distance function can begin. Three possible nominal distance functions
are presented:
1) Integer Nominal Distance.
2) Hamming Nominal Distance.
3) Intrinsic Nominal Distance.
3.3.2
Integer Nominal Distance
The nominal variable is arbitrarily mapped to a set of integer values. Each category
of the non-numeric variable is mapped to an integer in the interval [1, |xnom|], where
|xnom| is the number of points in the categorical input xnom. Then, the distance that
is input to the Kriging surrogate is based on this new arrangement of xnom in the
integer line.
It is the simplest nominal distance function.
The ﬁrst disadvantage is that it
implicitly deﬁnes an order between categories. Another drawback is that the random
mapping to the integer set does not take into account the relation objective-category.
100

For a large number of categories, the distance between two integers could be too
close which may produce non-positive deﬁnite covariance matrices. However, it does
not happen for a small number of categories. It is versatile which is useless since
the integer distance implicitly deﬁnes an order and the covariance matrices can be
non-positive.
3.3.3
Hamming Nominal Distance
The most popular nominal distance is the Hamming distance proposed by Hamming
[78],
dham
 x(i), x(l)
=
k
X
j=1
Ih

x(i)
j , x(j)
j

(68)
where a, and b are categories and
Ih (a, b) =





0
if a = b
1
if a ̸= b
(69)
It is noticeable that Equation 68 assumes that the distance between points is the
same and no order is established. The Hamming distance permits building metric
spaces in information theory and general topology as is shown by Ash [6] and Kelley
[108], respectively. Also, Aha et al. [2] combine the Hamming and Euclidean distance
to include together categorical and continuous variables in instance-based learning
algorithms.
The Hamming distance respects the nature of non-numeric variables by not deﬁn-
ing an order between categories. It is versatile since the same distance can be used
for all the objectives. The main drawback is that the Hamming distance just com-
pares categories of design points without taking into account the objective function.
Finally, the Hamming distance keeps radial basis functions meaningful since the dis-
tance between points is always one, which avoids having two categories too close.
101

3.3.4
Intrinsic Nominal Distance
In some instances there is some intrinsic metric for a given objective; so, it may be
possible to deﬁne other eﬃcient nominal distances. This intrinsic distance can be
given by an underlying parameterization or properties of the categorical members.
Many categorical choices have an underlying parameterization that is not explored
in conceptual design. For instance, in preliminary or detailed design phases, airfoils
can be parametrized into continuous spaces; these parameterizations can only be
fully exploited by high-ﬁdelity tools such as CFD codes. However, it does not make
sense to use these parameterizations in conceptual design stages, where only the
most important system factors are studied and the tools do not include physics with
the required details to make the most out of the parameterization. Additionally, in
conceptual design, engineers cannot aﬀord these time-consuming parametric studies.
Therefore, design teams treat airfoils as categorical variables in conceptual design
ignoring their underlying parameterization.
In conceptual design, variables with an underlying continuous space could usu-
ally be simpliﬁed in terms of categorical properties instead of the hidden continuous
parameterization. For instance, in the case of airfoil choices the diﬀerence between
airfoils is given in terms of aerodynamic curves, which is considered a categorical
property. So, these properties can be used to build the intrinsic distance. Categorical
properties can be scalar properties or function properties.
A possible example of scalar properties is found when there is a set of materials
to study in homogeneous and isotropic elasticity. Each material’s Hooke’s law in 3D
is deﬁned by two scalar parameters. Two possible material deﬁning parameters are
the modulus of elasticity and Poisson’s ratio. It is reasonable to deﬁne a distance
between two materials using the value of these deﬁning parameters when the elastic
material behavior is pursued. However, for analysis diﬀerent from elasticity, other
parameters turn to be important: fatigue limit for fatigue analysis, ultimate strength
102

and ductility for plasticity analysis,...
An example of function properties is taken from helicopter design, which is an
application of this investigation. Let us assume that the designer has to study the
aerodynamic eﬃciency of a helicopter rotor in hover and is given a set of airfoils.
In this problem, the set of airfoils is seen as a non-numeric design input. A simple
rotor model using Blade Element Momentum (BEM) theory, details explained by
Leishman [123], hints that the inﬂuence of the airfoils on the rotor aerodynamic
eﬃciency is introduced by the airfoil aerodynamic characteristic curves: Cl(α), Cd(α),
or Cd(Cl), a.k.a. polar curve (the combination of Cl(α) and Cd(α) by removing the
angle of attack as parameter). Therefore, it makes sense to base the distance function
between two airfoils in terms of these characteristic curves if the rotor aerodynamic
eﬃciency is pursued. Note that the airfoil aerodynamic curves are the properties that
inﬂuence the objective. These properties are functions unlike in the ﬁrst example
about materials, where the properties were scalars.
In the what follows, a discussion for the assessment of the intrinsic distance func-
tion is presented. xnom is a categorical design input whose values have J properties
that could deﬁne the possible intrinsic distance between category members.
For
scalar properties, the intrinsic nominal distance between two categories, x(i)
nom and
x(l)
nom, could be given by
dintr
 x(i)
nom, x(l)
nom

=
J
X
j=1
wj|propj
 x(i)
nom

−propj
 x(l)
nom

|
(70)
where propj

x(l)
nom

is the j-th categorical property of the l-th categorical member,
and wj is the weight for the jth property of the categorical design input. wj weights
the relative importance of the jth property compared to other properties. If a sole
property deﬁnes the intrinsic distance, then, it can be written as
dintr
 x(i)
nom, x(l)
nom

= |prop
 x(i)
nom

−prop
 x(l)
nom

|
(71)
103

Clearly the above metric cannot be applied when the property is a function in-
stead of a scalar.
A simple alternative is to choose a representative point of the
function via averaging or experts’ guess. Speciﬁcally for the helicopter instance pre-
viously discussed, a desired operating point like αopt can be chosen, so aerodynamic
characteristic curve values at αopt are possible scalars to feed to Equation 70, and 71.
As is well-known, helicopters usually have diﬀerent angles of attack in diﬀerent blade
span locations because of not only blade twist, but also diﬀerent rotational velocity
and induced velocity in each span location. As a result, the scalar approach is poor,
unless one deals with the unlikely case of an optimal rotor in its optimal design point
(in this case all blade sections operate at the optimum angle of attack). For further
reference, see[123].
Then, the goal is to look for a distance measure where the properties are in
function spaces. Function spaces are the base of “Calculus of variations”, see Sagan’s
work [161] and Gelfand et al.’s work [70]). The space of continuous functions of one
variable between points a and b is denoted as S,
S ∈C[a, b]
A more detailed explanation about functional analysis and function spaces is pro-
vided by Balakrishnan [10] and Zeidler [212].
There exist several function space metrics based on diﬀerent norms. Sutherland
[181] presents them with examples.
Let f and g be two functions.
Among the
most important metrics, one ﬁnds inﬁnity metric based on inﬁnity norm d (f, g) =∥
f −g ∥∞, metrics over Lp integrable spaces d (f, g) =∥f −g ∥Lp, and metrics over H1
Sobolev spaces d (f, g) =∥f −g ∥H1. Equation 72 represents the L1 metric between
elements in S.
dintr
 x(i)
nom (z) , x(l)
nom (z)

=
Z b
a
|prop
 x(i)
nom (z)

−prop
 x(l)
nom (z)

|dz
(72)
104

where z is the independent variable of the categorical input property prop (.).
The deﬁnition of the distance for the space of continuous functions of one vari-
able, Equation 72, brings up the issue: the choice of the integration limits. Their
deﬁnition depends on the expected bounds of the categorical design input property.
If one hypothesizes that, for the helicopter problem, Cd(α) is the curve with the
major inﬂuence in the rotor aerodynamic eﬃciency, then prop

x(i)
nom (z)

turns into
Cd
 air(i) (α)

(or C(i)
d (α) for notation’s simplicity). Post-stall angles of attack are
not expected to happen because the rotor aerodynamic eﬃciency is pursued. So, an
upper limit for b can be αstall. For the lower limit, αinf, a negative angle of attack,
small in magnitude, seems to be a reasonable value. Particularizing Equation 72 to
the helicopter problem
dintr
 air(i) (α) , air(l) (α)

=
Z αstall
αinf
|C(i)
d (α) −C(l)
d (α) |dα
(73)
Similarly as in the scalar property case, weighting could generalize Equation 72
when more than one function property is involved in the intrinsic distance.
For
the sake of brevity, di,l denotes from now on the nominal intrinsic distance between
categorical members i and l unless otherwise said.
Regarding desired nominal distance requirements, the intrinsic distance respects
the nature of non-numeric variables by not deﬁning an order between members. By
deﬁnition, it is in agreement with the relation category-objective. However, it has to
be changed for a diﬀerent objective, so there is no versatility. Finally, the intrinsic
distance may result in non-positive covariance matrices in cases where categorical
properties of two members are too close.
Finally, a modiﬁed intrinsic distance is presented to avoid non-positive covariance
matrices. So, this modiﬁcation of the intrinsic nominal distance targets cases where
two categorical members, i and l, are too close, which produces non-positive covari-
ance matrices. In order to avoid this eﬀect, the minimum categorical distance of the
105

set of categories is increased in value. Let dmin denote the minimum nominal distance
of the set of categories Scat
dmin = min (di,l)
∀i, l ∈Scat
where di,l = dintr

x(i)
nom (z) , x(l)
nom (z)

is assessed by Equations 70 or 72 (the non-
modiﬁed intrinsic nominal distance).
The minimum distance can be adjusted to a desired value d⋆. It is done by trans-
lating and scaling di,l so that a new minimum distance and the maximum distance
are set to d⋆and 1, respectively.
d′
i,l = d⋆+ 1 −d⋆
1 −dmin
(di,l −dmin)
(74)
where d′
i,l is the new nominal distances for avoiding non-positive covariance matrices.
3.4
MIC Surrogate: Leveraging Similar Trends across Cat-
egories
As stated in Section 1.6, one of the main goals of this thesis is to build surrogates
that leverage similar trends across categories. The current state-of-the-art constructs
an independent surrogate for each category. If the proposed surrogate is successful,
a better use of the limited computational budget can be done resulting in more
optimal and reliable initial designs and categorical selection. As in any usual scientiﬁc
research, a set of computational experiments is used to test the eﬃciency of the
proposed surrogate.
Surrogates usually cope with continuous and/or integer (or discrete-quantitative)
variables. In the presence of categorical inputs, independent surrogates are ﬁtted
for each category. The proposed surrogate herein includes not only continuous and
discrete-quantitative variables but also non-numeric ones; it is called mixed-integer-
categorical (MIC) surrogate. The addition of the categorical variables is done with the
idea of cross-using useful observations across categories. It would lead to more eﬃcient
106

surrogates than the ones currently employed. The Hamming distance is a perfect
candidate by which to include the categorical design inputs in the surrogate domain,
in a similar manner that Wilson and Martinez [201] do for statistical classiﬁcation.
In order to build a MIC surrogate it is necessary to have a training set. They are
obtained with regular Latin hypercube sampling[96]. The categorical and discrete-
quantitative variables are treated as continuous when getting the training set. Specif-
ically, the categorical ones are ﬁrst mapped to integer variables and later handled as
continuous. Once the Latin hypercube sampling produces the set, the categorical and
discrete-quantitative variables are rounded-oﬀsuch that samples are evenly divided
among categories and discrete-quantitative values. It is important to emphasize the
selection of continuous Latin hypercube techniques that make the most out of the
training set; there are diﬀerent training points in each category, so the cross-use of
observations is more eﬃcient than in the case of same training points in each cate-
gory. In the case of the same training set in each category, observations from other
categories are not that much valuable for a given category (the objective values at
the sample points of the given category are already known).
The type of surrogate chosen to build the MIC surrogate is Kriging, discussed
in Section 2.5, because eventually adaptive sampling techniques are pursued, which
requires the surrogate prediction uncertainty. Kriging bases its estimates on its cor-
relation function, given in Equation 9, that needs the distance between the prediction
point x and the sample points. So, the prediction is made according to the proximity
to sample points: when the design point x, at which the objective value is pursued, is
close to a sample point, then the objective value at this sample point inﬂuences the
prediction at the design point x.
When dealing with continuous and discrete-quantitative variables, Kriging uses
the Manhattan distance between points to assess proximity. The reason for prefer-
ring the Manhattan over the Euclidean distance, even in cases with only continuous
107

design inputs, is to decouple design variables. It allows taking into account that some
variables aﬀect more the objective function than others. It is highly recommended to
normalize the bounds of the design space in a hypercube {0, 1}k, if possible, in order
to avoid a misleading dependence of the objective function on the design variables.1
This work claims that for categorical variables the Hamming distance, deﬁned
in Equation 68, is appropriate to be inputted to the Kriging correlation matrix, see
Equation 9. Other alternative, if possible, is to use the intrinsic distance for the given
objective function, deﬁned either by Equation 70 or 72 depending if the categorical
property is a scalar or a function. It is worth reminding that this intrinsic distance
is deﬁned with the use of relevant properties of the categorical variable to the given
pursued objective function.
3.4.1
Performance Indicators for MIC Surrogates
The MIC surrogate is considered successful if it predicts more accurately the objec-
tive function than the current multiple independent surrogates (one for category) in
a given range of training points.
Also, MIC surrogates based on several nominal
distances are to be compared. The comparisons are done in terms of some success
indicators2. A straight-forward measure of accuracy of a model is the “standardized
validation error” between the real and predicted value
error (x) = |y (x) −ˆy (x) |
y⋆
ch
(75)
where y (x) and ˆy (x) are the real model and its approximation, respectively, and y∗
ch
is the characteristic change of the function in the studied domain. This last value is
brought to normalize the error.
1It is a good practice to normalize the design space because, as it could be seen, if one of the
design inputs xi has a very large range, it could artiﬁcially overpower other design inputs.
2Also called performance indicators.
108

Probabilistic surrogates such as Gaussian processes provide the epistemic uncer-
tainty of the prediction. In Kriging, it is given by the mean squared error, ˆs (x);
Equation 18 shows it in the interpolating case. Other success indicator is the number
of mean squared errors that the predicted value is oﬀthe actual value. This indicator
called “standardized validation residual” could be written as follows
res (x) = |y (x) −ˆy (x) |
ˆs (x)
(76)
It is known that 99.7% of values drawn from a normal distribution, N (µ, σ2),
are within the interval [µ −3σ, µ + 3σ]. Thus, the “standardized validation residual”
represents the conﬁdence interval in the accuracy of the prediction.
In order to assess the accuracy of the meta-model in its entire domain, a possible
option is to calculate the root mean squared (rms) of the error (x) and res (x) over
the whole domain as follows
errorrms,global =
s
1
V
Z
V
|y (x) −ˆy (x) |2
y⋆2
ch
dV
(77)
resrms,global =
s
1
V
Z
V
|y (x) −ˆy (x) |2
ˆs2 (x)
dV
(78)
where V is the design space.
Expressions 77 and 78 are easily evaluated for cheap functions but not for com-
putationally intense objective functions due to the need of assessing the real function
value, y (x). The main point of a surrogate is to predict a computationally intense
objective function with few real function evaluations. So, an alternative method to
assess the success indicators is needed.
Other alternative is to select a few additional points as validation or test points
and compare the true and predicted values, but it is still a loss of resources because
109

it requires sampling beyond the points used to ﬁt the surrogate. A better option is
called “cross-validation” that permits to evaluate model accuracy without losing any
computational resources.
Jones [99] et al. propose this cross-validation technique. It consists in removing
the point i out of the training set; later, the surrogate is built based only on the
remaining N −1 points; and then, the value of the objective at the point that has
been left out is predicted, ˆy−i (xi) (the subscript −i highlights that the training point
xi has not been used as a training point in the prediction of ˆy−i (xi)). Instead of tuning
over and over again the model hyper-parameters for each reduced training set, the
values of the hyperparameters when tuning the whole training set (point xi included)
are used. It is because little changes are expected in those unless few observations
exist. In the Kriging surrogate, the i-th column and row of the correlation matrix Ψ
are removed, and so is the i-th component of the vectors ψ and y. Similarly, cross-
validated mean squared error, ˆs−i (xi), is assessed at the i-th point. Cross validation
has been employed as well by other authors, see Kleijnen et al.’s work [113].
New local success indicators are deﬁned: “standardized cross-validation error” by
Equation 79, and “standardized cross-validation residual” by Equation 80,
errorcvi,i = |y (xi) −ˆy−i (xi) |
y⋆
ch
(79)
rescvi,i = |y (xi) −ˆy−i (xi) |
ˆs−i (xi)
(80)
In order to get global success indicators, the rms of all the sample points are added
errorrms,cvi =
v
u
u
t 1
N
N
X
i=1
error2
cvi,i
(81)
resrms,cvi =
v
u
u
t 1
N
N
X
i=1
res2
cvi,i
(82)
110

Also, the inﬂuence of the size of the training set on the surrogate performance is
sought, see Section 3.2. Speciﬁcally, how quickly the rms of the surrogate “standard-
ized error” converges to zero as the training set becomes larger and larger. A power
relationship is ﬁtted to the predicted rms of the “standardized validation error” for
analyzing this convergence behavior.
errorrms = errorrms (Ntr.set = 0)
Nα
tr.set
(83)
where N is the number of training set samples, and α tells how quickly the rms of
the error converges to zero. The value of α enables to see the eﬀect of an increase in
the training set size on the rms of the error.
Note that the MIC surrogate and its performance indicators explained so far in
this section assume a noise-free function.
However, a similar procedure for noisy
functions is developed, where just the values of ˆy and ˆs are taken from Subsections
2.5.1 and 2.5.2, respectively.
3.5
ECMF Surrogate: Leveraging Similar Trends from Pre-
vious Concepts
Section 2.6 explains in details multi-ﬁdelity Gaussian random ﬁeld meta-models that
combine two diﬀerent ﬁdelities. The multi-ﬁdelity framework to be presented in this
Section re-uses observations of previously sampled concepts in new ones when there
exist similar trends. Nonetheless, current surrogates are ﬁtted independently for each
concept. Concepts with similar trends are common in conceptual design stages, where
the iterative process keeps incrementally evolving the conﬁguration of the solution
(“evolutionary incremental concept”). The multi-ﬁdelity surrogate presented herein
is targeted to these evolutionary concepts; thus, it is called evolutionary multi-ﬁdelity
(ECMF) surrogate.
The addition of previous concept observations via multi-ﬁdelity approach is done
111

with the idea that the reuse of the observations results in more eﬃcient surrogates
than the ones that just use observations of the concept itself. It allows designers to
make a better use of the limited computational budget. Again, computer experiments
are used to test the eﬃciency of reusing computationally intense observations from
previous concepts. The concept previously sampled is called concept 1, whereas the
new concept is called concept 2. Two main assumptions are made while building the
ECMF surrogate: 1) concept 1 design space is included in concept 2 design space;
and 2) there is a constant behavior in the concept 1 training set extension along
the concept 2 exclusive design variables. They are discussed in more detailed in this
Section.
Traditional multi-ﬁdelity methods deal with variable-ﬁdelity codes that have the
same design space. However, concepts have diﬀerent design spaces: dimensionalities
and design variables may diﬀer across concepts. Space mapping, which is developed
in the microwave circuit community [12] [155], is a method of linking variable-ﬁdelity
models.
This technique is capable of linking diﬀerent design spaces of the same
concept from variable ﬁdelities that may or may not share design variables.
For simplicity’s sake, it is assumed herein that the new concept has all the design
variables of the previous concept plus some new ones exclusive for the new concept.
It matches with the conceptual design iterative scenarios, where concepts evolve from
one to the other incrementally (which is previously called “evolutionary incremental
concepts”). The reason for the previous assumption is that the main purpose is to
study the eﬀectiveness of reusing computationally intense observations from previous
concepts; thus, the applicability of the ECMF surrogate to a wider range of design
spaces is left as a secondary factor to study in the future. Good candidates for this
extension to a wider range of design spaces are space mapping techniques [12]
Firstly, a Gaussian surrogate is ﬁtted to the concept 1, ˆy(D1)
c
, in its designed
space, denoted as D1. It means that the hyper-parameters µ(D1)
c
, σ(D1)
c
, and θ(D1)
c
are
112

found. Note that the super-index indicates the design space of the hyper-parameter
or training data-sets.
Then, the concept 1 training set is extended from D1 to the concept 2 design
space, denoted by D2. There are two possible ways for carrying this extension out.
The ﬁrst one is to assume a constant behavior of the training set along the concept
2 exclusive design variables. The second option is to use known data to assume a
given behavior along these concept 2 exclusive design variables. The present work
assumes that the only information available on the new concept is the following: a)
its observations (y(D2)
e
), and b) its similar trends to concept 1 (about which some
observations y(D1)
c
are known); thus, the constant behavior for the concept 1 training
set extension to D2 seems to be the best option. Also, it is important to realize
that the incremental changes from one concept to another are likely to produce small
changes in the response compared to those of the main design factors, which are
present in both concepts.
In order to carry out this concept 1 training data-set extension to D2, X(D1)
c
and y(D1)
c
are placed at the ends of the concept 2 exclusive design variables, i.e.,
X(D2)
c
= {
h
X(D1)
c
0(exclD2)i
,
h
X(D1)
c
1(exclD2)i
}, and y(D2)
c
= {y(D1)
c
, y(D1)
c
}.
Then, a new mono-ﬁdelity surrogate for concept 1, ˆy(D2)
c
, can be calculated in the
new concept design space D2. A surrogate computation from scratch is a waste of re-
sources because the surrogate is just a constant extension from the one in D1 to a new
one in D2. The hyper-parameters of the surrogate ˆy(D2)
c
can be obtained from the ones
for ˆy(D1)
c
, speciﬁcally µ(D2)
c
= µ(D1)
c
, θ(D2)
c
= {θ(D1)
c
, θ(exclD2)
c
}; once µ(D2)
c
and θ(D2)
c
are obtained, σ(D2)
c
is given by Equation 14 for the interpolating case. Since the new
surrogate is a constant extension along concept 2 exclusive design variables, denoted
by exclD2, θ(exclD2)
c
should be set to 0(exclD2). However, this option produces numeri-
cal problems such as singularities in the covariance matrices Ψc

X(exclD2)
c
, X(exclD2)
c

,
and Ψc

X(exclD2)
e
, X(exclD2)
e

, see Equation 29. In order to solve this problem, the
113

zero values are substituted for a fraction of the minimum component of the vector
θ(D1)
c
, i.e., θ(D2)
c
= {θ(D1)
c
, min θ(D1)
c
1(exclD2)
30
} where min θ(D1)
c
is the minimum value
of the vector θ(D1)
c
.
Once the hyper-parameters of the cheap model in D2 are tuned, Equations 31
and 32 provide the value of the surrogate hyper-parameters µ(D2)
d
and σ(D2)
d
for the
concept 2 surrogate, ˆy(D2)
e
. The hyper-parameter θ(D2)
d
is obtained by a GA search as
explained in Section 2.6. Figure 9 shows a diagram with the proposed multi-ﬁdelity
approach.
Figure 9: Process for ECMF Surrogate
Finally, the prediction and the mean squared error of the multi-ﬁdelity surrogate
for concept 2 could be assessed with Equations 33 and 34, respectively.
Note that the multi-ﬁdelity approach explained so far in this section assumes the
objective function is noise-free. However, a similar procedure for noisy functions is
114

developed with the addition of the hyper-parameter λe. Also, equations for ˆµ and ˆσ
are taken from Subsections 2.6.1 and 2.6.2.
3.5.1
Performance Indicators for ECMF Surrogates
The method is considered successful if the ECMF surrogate predicts more accurately
the objective function of the new concept than the current mono-ﬁdelity surrogate
for a given range of concept 1 and 2 training data-set sizes. The success indicators
employed to measure success are the same ones as in Subsection 3.4.1: rms of the
“standardized validation error”, and “standardized validation residual”.
If the objective function is available in the design space, the rms of the “standard-
ized validation error” and “standardized validation residual” are given by
errorrms,global =
s
1
V
Z
V
|ye (x) −ˆye (x) |2
y⋆2
e,ch
dV
(84)
resrms,global =
s
1
V
Z
V
|ye (x) −ˆye (x) |2
ˆse
2 (x)
dV
(85)
If the cross-validation technique is used (see Subsection 3.4.1), the rms of the
“standardized validation error” and “standardized validation residual” are given by
errorrms,cvi =
v
u
u
t
1
N2,tr.set
N2,tr.set
X
i=1
 
|ye (xi) −ˆye,−i (xi) |
y⋆
e,ch
!2
(86)
resrms,cvi =
v
u
u
t
1
N2,tr.set
N2,tr.set
X
i=1
|ye (xi) −ˆye,−i (xi) |
ˆse,−i (xi)
2
(87)
3.6
Adaptive Sampling of MIC and ECMF Surrogates
One of the goals of this work is to develop adaptive sampling techniques for surro-
gates that leverage similar trends. The ExI, explained in Subsection 2.7.1, is the
inﬁll criterion chosen herein to adaptively sample design spaces. Typical objective
115

functions in design are unknown and likely to be multi-modal and non-convex. ExI
permits exploring regions of the design domain with high uncertainty and exploiting
potentially good performing regions.
As previously said, the ExI is optimized throughout the whole domain. For a mod-
est size of training datasets, multiple valleys are expected in ExI functions; therefore,
gradient techniques are useless and are ruled out. Instead global search techniques
are needed to look for the maximum ExI. This investigation chooses a GA as a global
ExI optimizer due to its high popularity and easy availability. The adaptive sampling
algorithm is halted in this research when a given number of updates is reached.
A MICGA is developed to adaptively sample MIC surrogates whose domains have
continuous, discrete-quantitative, and non-numeric variables. The current state-of-
the-art EGO algorithm to compare with is the simultaneous adaptive sampling of
independent surrogates.
Also, adaptive sampling on ECMF surrogates is performed. However, it is not
a technical challenge because only one concept is handled at a time and its domain
is at most MIC. In the last case, a new concept with a MIC domain, the adaptive
sampling algorithm gets reduced to the same problem, adaptively sampling a MIC
surrogate.
Regarding the adaptive sampling of MIC surrogates, the ﬁrst step is to make sure
that the optimizer MICGA can drive the adaptive sampling in MIC domains. So,
computer experiments are done to see if there is EGO behavior in the sampling pro-
cess. EGO behavior means that there is a higher concentration of updates in globally
high-performing areas and not just in high performing regions of each category.
3.6.1
Performance Indicators for Adaptive Sampling
The second step is to deﬁne success indicators for multi-objective adaptive sampling
on MIC and ECMF surrogates. Optimal solutions for multi-objective scenarios are
116

Pareto fronts. So, the success indicators are given by the quality of Pareto fronts. The
adaptive sampling algorithms provide a non-dominated set that intends to approach
the ‘ “real” Pareto set. Two performance indicators are used for this purpose:
Number of points in the non-dominated set It measures the level of detail of
the non-dominated set.
The average distance between the non-dominated set and “real” Pareto front
It quantiﬁes the proximity between the approximating non-dominated set and
the real Pareto front. The average distance ¯d (F, PF) is given by
¯d (F, PF) =
qreal
X
i=1
min (yi, PF)
(88)
where yi is the i-th point of the approximating non-dominated set, PF the
“real” Pareto front which is assessed by the method NSGA-II developed by
Deb [41], and qreal is the number of Pareto members of PF.
Once the performance indicators of the adaptive sampling experiments are deﬁned,
the MICGA that would search mixed-integer-categorical design spaces is presented.
3.6.2
Mixed-Integer-Categorical Genetic Algorithm
The ExI inﬁll criterion drives the adaptive sampling of MIC surrogates. The next
sample point is obtained from the solution of an optimization problem in the MIC de-
sign space, which is composed with attributes of several nature: continuous, discrete-
quantitative, and categorical. Section 2.8 reviews the MIO techniques: mixed-integer
programming, and stochastic MIO searches. The desired qualities for the optimizer
of the ExI in the MIC domain are:
1) Global search. ExI function is multi-modal.
2) Non-linear functions. ExI is non-linear.
117

3) Category handling.
4) Availability and open source.
5) Low computational expense.
The ﬁrst three qualities are crucial due to the nature of the optimization problem
to solve (the ExI optimization in a MIC domain).
Figure 10 shows the qualities
satisﬁed by mixed-integer programming and stochastic MIO searches.
Stochastic
MIO searches satisfy two of the three crucial qualities; therefore, they are chosen to
optimize the ExI, speciﬁcally a mixed-integer genetic algorithm is selected. However,
the optimization tool needs to deal with categories, i.e., a mixed-integer-categorical
optimizer is needed. The question is: how can a MIGA be extended to search mixed-
integer-categorical design spaces?
Figure 10: Qualities of the MIO Methods
Note that in Figure 10, the symbol ≈means that the tools are available but they
are restricted-source.
First of all, it is explained the conversion from continuous GA to MIGA. The ﬁrst
possible option to obtain a MIGA is to treat all the variables as continuous; then,
118

once the optimal solution is obtained, it is rounded-oﬀ. Also Karoui et al. [103]
propose a progressive rounding-oﬀof discrete variables. Capitanescu and Wehenkel
[27] point out the problems for these two approaches: a) a likely deterioration of the
objective value, and b) to ensure feasibility a method to restore it is necessary, which
results in additional coding. These approaches are ruled out due to these downsides.
Better suited generation, crossover, and mutation functions are developed to properly
treat the integer variables. These functions have led to good performing MIGA [44].
Several modiﬁcation on MIGA will be proposed in this section to convert them into
mixed-integer-categorical GA.
Deep et al. [44] develop a GA to solve mixed-integer problems. However, its access
is strictly limited. MATLAB R
⃝2011b provides a GA with mixed-integer capabilities
based on Deep et al.’s work. Again, the functions to carry out the optimization process
are encrypted in private ﬁles, so no modiﬁcations can be done to build a MICGA out
of the MATLAB R
⃝MIGA. On the other hand, all the MATLAB R
⃝functions that are
called by the MATLAB R
⃝continuous GA solver are accessible. Therefore, the ﬁrst
step towards MICGA is to build a MIGA out of the MATLAB R
⃝continuous GA, and
later the MICGA will be implemented.
Modiﬁcations on the generation, mutations, and crossover functions are carried
out to convert the continuous GA into MIGA according to the literature available.
The initial population of continuous and integer variables are uniformly randomly
generated. Gaussian and geometric mutation distributions are used for continuous
and integer variables, respectively. Regarding crossovers, both continuous and integer
values are crossovered intermediately. If the resulting integer crossover value is not
integer, rounding is applied with equal probability for ceiling and ﬂooring. As a re-
minder the mutation and crossover functions are brieﬂy classiﬁed in Subsection 2.8.3.
The resulting solver is tested against typical mixed-integer problems and compared
with the source-restricted MATLAB R
⃝2011b MIGA.
119

MIGAs can be used to pursue the global optimum on a MIC domain. The MIGA
can optimize the ExI of one category at a time. Later, a comparison between cate-
gorical optimum selects the global optimum. However, a more eﬃcient handling of
the categorical variable is to modify the generation, cross-over, and mutation of the
categorical part of the population genome. In such a way, just one optimizer call is
needed, and the population is expected to focus on the best categories and, therefore,
save ExI calls in categories with low ExI.
Once the MIGA is available, modiﬁcations are performed to turn it into a MICGA.
Regarding the initial population generation, the categorical part of the genomes is
produced using full factorial techniques with as many levels as members in the cat-
egorical variable.
Then, the resulting sampling plan in the categorical dimension
is repeated to obtain the desired number of sample points for each combination of
categories. It assures that all categories are present in the initial population evenly,
i.e., similar number of initial population members for all points in the non-numeric
subset domain, denoted by D1 × D2 × ... × Dnn. Di represents the set of possible
members for the non-numeric variable i. The choice of full factorial techniques for
categorical inputs is made due to the lack of proper distance between members of
the same categorical variable. It is also made sure that the size of the population is
bigger than the number of members in the non-numeric subset |D1 × D2 × ... × Dnn|.
|A| represents the number of members in the set A.
Nominal values are scatteredly crossovered because intermediate points can not
be deﬁned when there is no order between points. Mutation-wise, uniform distribu-
tions are utilized, which agrees with the Hamming distance: the distance between all
categories is the same.
120

3.6.3
Mutation Study
Once the MICGA has been built, a possible spin-oﬀquestion is the following: is there
any categorical mutation operator that outperforms others for the optimization of the
ExI functions?
Mutation is the process that genetically alters chromosomes. ESs use mutations
as search operators to look for a better solution. An usual method to implement
mutation operators involves generating a random variable for each chromosome. This
random variable determines which value the chromosome will take after the mutation
process.
The ES user can modify the mutation operator by deﬁning a mutation
probability over the possible set of values.
Authors have implemented numerous mutation strategies for continuous and discrete-
quantitative design variables [44, 69]. Popular choices are the normal and geometrical
distributions for continuous and integer variables, respectively. These last mutation
distributions are obtained from “Principle of Maximum Entropy” [159, 127]. Uniform
distributions for the non-numeric variables are commonly employed.
Several mutation techniques are developed herein for categorical variables.
It
enables the study of the inﬂuence of several mutation operators in the performance of
the MICGA when optimizing the ExI over a MIC domain. The mutation distributions
to study are:
1) Uniform Distribution. When a non-numeric variable is to mutate, the likelihood
of the child value is uniformly distributed, i.e., all the other categories have equal
chance to appear in the genome of the child.3
2) Inverse Distance Distribution. The nominal part of the child genome follows a
distribution inversely proportional to the intrinsic distances between the father
3This technique could be considered a maximum entropy technique for categorical variables whose
metric is given by the Hamming distance.
121

categorical member and the remaining members.
3) Maximum Entropy. The nominal part of the child genome follows the maximum
entropy distribution given the intrinsic distances between the father categorical
member and the remaining members.
The concept of neighborhood between nominal points could be laid out thanks to
the intrinsic distance of some non-numeric variables, as explained in Subsection 3.3.4.
In this case, distances from one point to others in the categorical dimension is given
by a discrete set of real numbers. The second and third mutation distributions of the
above enumeration, “Inverse Distance Distribution” and “Maximum Entropy”, are
based on this discrete set of real numbers. In the following, a discussion about these
two mutation distributions is presented. Finally, a discussion is presented regarding
the performance indicators while testing the mutation distributions of all types of
variables on functions with MIC domains.
3.6.3.1
Inverse Distance Distribution
It represents a simple and straight-forward method to build a mutation operator
based on the intrinsic distance of a categorical set given a objective function.
Without loss of generality a problem with only one categorical variable is sup-
posed. Let pj be the probability mutation distribution for a parent whose nominal
chromosome takes the j-th value of the nd possible chromosome values, i.e., the non-
numeric variable can take on nd categories and the parent takes the j-th category.
Notice that the super-index j emphasizes the fact that each discrete point in the
categorical dimension has a diﬀerent mutation distribution. Also, let pj
k be the prob-
ability that the mutation child gets the k-th category given the parent takes the j-th
category.
For the parental category j, the inverse distance distribution assigns mutation
122

probabilities, pj
k, proportionally to the distance between the j-th and k-th categories
pj
k ∝
1
dj,k
where dj,k represents the intrinsic distance between the j-th and k-th categories in
the given categorical variable.
Normalization is required for probability distributions4. The normalization con-
dition is given by Equation 89
nd\j
X
k=1
pj
k = 1
(89)
Taking into account the normalization condition, the probability distribution for
a parent that belongs to the j-th category could be written as
pj
k =
1
Pnd\j
l=1
1
dj,l
1
dj,k
for k = 1...nd \ j
pj
j = 0
(90)
This method clearly makes more likely mutations to categories k that are close to
the parental category j. It represents a simple way to build the mutation distribution.
However, there exist more sophisticated mutation distributions.
3.6.3.2
Maximum Entropy Distribution
As mentioned in Subsection 2.8.3, Rudolph proposes that the mutation distribution
should maximize the entropy if no additional knowledge about the objective function
is known [159].
As is well-known from statistical thermodynamics and information theory, the
entropy deﬁnition for a discrete probabilistic distribution pj is
H
 pj
= −
nd\j
X
k=1
pj
k log
 pj
k

for j = 1...nd
(91)
4It is necessary for a probability distribution that the sum of the probabilities over all the possible
states is equal to one.
123

where the index k goes through all the categorical points diﬀerent from j.
As a probability distribution, normalization is again required. Also, the mean of
the pj distribution, µj, is to be speciﬁed to control the shape of the distribution.
These two conditions are written as follows
nd\j
X
k=1
pj
k
=
1
for j = 1...nd
(92)
nd\j
X
k=1
dj,kpj
k
=
µj
for j = 1...nd
The values dj,k can be better arranged by performing a translation, setting the
minimum distance to other categories to zero,
ˆdj,k = dj,k −min
k
(dj,k)
where mink is the minimum distance to the category j (in this minimization the value
of k = j is excluded). Notice that the entropy function, see Equation 91, is invariable
to translation in the sample space, which is the set of all possible outcomes for the
random process. It is because the entropy deﬁnition only involves the probability and
not the sample space as Equation 91 shows. Only the value of the mean gets aﬀected.
The maximization of the entropy leads to solve a nonlinear constrained optimiza-
tion problems, one per category j = 1, 2, ...nd. Constraints are treated with the help
of Lagrangian multipliers. For a given j, the optimal values can be obtained by partial
diﬀerentiation of the objective function. The entropy, augmented with the constraints
given by Equations 92, can be written as
L
 pj
k, λj
1, λj
2

= −
nd\j
X
k=1
pj
k log
 pj
k

+λj
1


nd\j
X
k=1
pj
k −1

+λj
2


nd\j
X
k=1
ˆdj,kpj
k −µj


for j = 1...nd
(93)
where λj
1 and λj
2 are the Lagrange multipliers for the normalization, and the mean µj
124

constraints, respectively. Notice that there is no coupling of the mutation probability
distribution between diﬀerent parental points j of the categorical design variable.
The main details of the derivation are omitted for compactness’ sake. Finally, a
non-analytically tractable equation is found.
µj =
Pnd\j
k=1 ˆdj,k exp

λj
2 ˆdj,k

Pnd\j
k=1 exp

λj
2 ˆdj,k

for j = 1...nd
(94)
If µj is chosen, Equation 94 is solved numerically for λj
2. Once λj
2 is obtained,
the other Lagrange multiplier λj
1 and the probability distribution pj for the parental
category j are
λj
1 = 1 −log


nd\j
X
k=1
exp

λj
2 ˆdj,k



for j = 1...nd
(95)
pj
k = exp(−1 + λj
1 + λj
2 ˆdj,k)
for k = 1...nd \ j
for j = 1...nd
(96)
It is worth realizing that, for each pj, the mean parameter µj is still free. Several
remarks should be stated regarding the choice of this last parameter.
1) In order to guarantee that the mutation probability distribution, pj
k, decays
with dj
k, the maximum value of the distribution mean, µj, has to be smaller
than the center of mass of the translated distance set to the j-th category, ˆdj,k,
µj
limit =
1
nd −1
nd\j
X
k=1
ˆdj,k
2) The parameter µj controls the shape and mean of the distribution, smaller
values provides a mutation distribution that highly beneﬁts categories close to
the j-th categorical member.
125

With the previous remarks being said, the mutation distribution is controlled by
a mutation parameter, νmut ∈[0, 1], that sets the value of the probability distribution
mean as follows,
µj = νmutµj
limit
(97)
Therefore, mutation distributions that are wished to highly beneﬁt categories close
to the parental one have values of νmut close to 0, whereas those intended not to take
into account the distance between categorical points have values of νmut close to 1.
The limit νmut −→1 represents an uniform mutation probability distribution; the
same one obtained with the Hamming distance.
It is convenient for non-numeric
variables where there is no a clear intrinsic distance between values.
3.6.3.3
Performance Indicators Mutation Study
Interest is in the combined mutation strategies for continuous, discrete-quantitative,
and categorical variables. Two performance indicators measure the success of the
method. The ﬁrst one is the error of the maximization process given by
error = | log10 (max (E [I (x)])est) −log10 (max (E [I (x)])real) |
(98)
where the sub-indexes {.}est and {.}real represent the estimated and real value of the
maximization process, respectively.
The error is given in terms of the negative log10 of the maximum E [I (x)]. The
second indicator is the generation at which the last successful improvement larger than
1% is produced in the optimization process. These parameters indicate the accuracy of
the solution and the eﬃciency of each mutation strategy in the optimization process,
respectively.
The mean and standard deviation of the two indicators are assessed because of
the statistical nature of the GA outputs.
126

3.7
Fenestron Conﬁguration as the New Concept
The choice of the fenestron tail as the new concept to test the ECMF surrogate is
made due to several constraints on the author. A possible new concept is a coax-
ial rotor. However, the author could not ﬁnd a coaxial control routine for rotors in
FLIGHTLAB, the commercial software employed to model the helicopter. The re-
maining options are to include a weapon as a external load in the basic UH60A model
or to substitute the conventional tail with a fenestron one.
Both possibilities have some drawbacks. The ﬁrst option could be seen more a
new ﬂight condition or mission for the helicopter than a new concept. The second
option is a new concept, but the UH60A is heavier than the largest helicopter that
has been equipped with the shrouded fan. On the other side, the fenestron tail option
allows the study of the trade-oﬀbetween the increase in weight due to the new tail
conﬁguration and the increase in anti-torque device eﬃciency because of the ducted
fan.
The fan-in-ﬁn tail is chosen as the new concept for two reasons.
First, it is
interesting and practical to study the previously mentioned trade-oﬀbetween the
weight increase and the enhancement of tail rotor eﬃciency[123]. The second reason
is the expected proximity between the old concept (UH60A with conventional tail)
and the new concept (UH60A with fenestron tail); it is reasonable to think that they
would experience similar trends.
The main challenge on building a UH60A with fenestron tail FLIGHTLAB model
is the estimation of the fenestron baseline parameters and its overall weight. These
estimations are assessed in the following Subsections.
3.7.1
Fenestron Baseline Values
Rand et al. [150] perform a statistical analysis on a helicopter database. The aim
of the study is to estimate geometry parameters, weight of components, preliminary
127

power, and ﬂight performance. Among the targeted geometry parameters, one ﬁnds
the rotor diameter, rotor angular speed, horizontal tail surface area, and vertical tail
average chord. The useful part of Rand et al.’s research to the present work is that
in the database some helicopters have a fenestron conﬁguration. Thus, the regression
ﬁts can be used to estimate the fenestron baseline parameters.
The desired fenestron parameters to assess are: Rfen
tr , bfen
vt , cfen
vt , Ωfen
tr , Nfen
b,tr , and
cfen
tr . Rand et al. [150] ﬁnd that the fenestron rotor diameter is correlated with the
gross weight W0 as follows
DF en
tr
= 0.3081W 0.154
0
(99)
The resulting fenestron diameter is 1.2852 m, where W0 = 10660 kg is assumed.
This value is too small compared with Super Puma SA330, one of the heaviest heli-
copter with fenestron. Super Puma diameter is 1.6 m with a gross weight of W0 = 7000
kg. Therefore, a more reliable estimation, according to the author, could be assessed
by a parallel ﬁt to the one in Equation 99 that passes through the Super Puma point
(DF en
tr
= 1.6 m and W0 = 7000 kg). The resulting UH60A diameter for the fenestron
tail conﬁguration is DF en
tr
= 1.7071 m.
The addition of the fenestron tail usually results in an increase in the vertical tail
span bfen
vt . No correlation has been found, but herein it is assumed that the vertical
tail span is increased one fenestron radius with respect to that of the UH60A with
conventional tail. So, for the baseline case one obtains bfen
vt
= bConv
vt
+ DF en
tr
2
= 3.14
m.
The correlation found by Rand et al. [150] for the fenestron vertical tail average
chord is
cfen
vt
= 0.909Dfen
tr
0.927
that results in cfen
vt
= 1.4923 m, which is 13.6% larger than that of the UH60A with
the conventional tail.
128

Regarding the fenestron angular speed, Rand et al.’s study does not provide a
statistical ﬁtting for the fenestron case. However, the two fenestron helicopters seem
to validate the regressive ﬁt calculated for the angular velocity of the conventional tail
rotor-crafts. Therefore, the conventional tail angular velocity correlation is applied
to estimate the fenestron angular velocity
Ωtr =
364
Dfen
tr
0.828
that leads to a rotor angular speed of Ωfen
tr
= 233.77 rad/s.
Neither is a sizing correlation for the number of fenestron blades found. Normally,
the number of blades go from 7 to 14. In this study it is assumed the lowest range,
i.e., from 7 to 9 fenestron blades, being Nfen
b,tr = 8 the baseline value for the fenestron
case.
The tail rotor blade chord correlation presented by Rand et al. [150] does not
include fenestron information either. However, it is used in the estimation of cfen
tr
because it is assessed for a situation that also applies to fenestron rotors: the ability
of the tail rotor to balance the torque of the main rotor in a full power vertical climb
at a speciﬁed altitude [150]. The correlation formula is
ctr = 0.0058W 0.506
0
N0.72
b,tr
using the previous values of Nfen
b,tr = 8 and W0 = 10660 kg, the resulting chord is
cfen
tr
= 0.1417 m.
Table 1 shows the baseline values for the UH60A with fenestron tail rotor.
3.7.2
Weight Estimation
There exist several models for rotor-craft weight estimation. Among them, one can
ﬁnd investigations on weight estimation for preliminary sizing of rotor-craft by Prouty
[146], Rand et al. [150], Tishchenko et al. [185], NASA design and analysis of rotor-
craft (NDARC) [97], and Kalra et al. [102]. These references provide formulas for the
129

Table 1: Fenestron Baseline Values
Fenestron Baseline Parameters
Values
Rfen
tr
1.7071 m
bfen
vt
3.140 m
cfen
vt
1.4923 m
Ωfen
tr
2333.77 rad/s
Nfen
b,tr
8
cfen
tr
0.1417 m
weight estimation of several parts of the rotor-craft in terms of some basic preliminary
information, such as empty weight, rotor diameter, number of blades, ...
However, not much sizing information is available for the fenestron weight esti-
mation; the main reference for this purpose is Rand et al.’s work [150]. Nevertheless,
the fenestron data is for rotor-craft gross weights up to 8000 kg. The UH60A conﬁg-
uration is heavier than the information in the database, but it is assumed that the
statistical correlations and weight estimation formulas still apply.
The change in rotor-craft weight due to the fenestron is broken down in 3 main
contributions:
1) Vertical ﬁn weight
2) Blades weight
3) Hub weight
In the above three contributions, the same procedure is used to calculate the
change in weight of the fenestron tail with respect to the conventional case. The item
contributions of the conventional tail can be known or estimated based on the basic
UH60A information. Also, the item contributions of the fenestron conﬁguration are
estimated with Rand et al.’s work[150].
130

3.7.2.1
Vertical Fin Weight Estimation
First, the weight of the conventional vertical tail, W Conv
vt
, is assessed with the formula
provided by NDARC [97]
W Conv
vt
= 1.05A0.94
v
AR0.53
v
N0.71
tr,gb
(100)
where Av is the vertical ﬁn area, ARv the vertical ﬁn aspect ratio, and Ntr,gb the
number of tail rotor gearboxes.
The resulted vertical tail weight for the conventional UH60A obtained is W Conv
vt
=
61.0287 lb. With the Prouty’s equivalent formula one gets a similar weight W Conv
vt
=
60.4474 lb.
The next step is to estimate the vertical ﬁn weight for the fenestron conﬁguration.
The fenestron diameter and vertical ﬁn chord, estimated in Subsection 3.7.1, are
brought to the weight estimation process: DF en
tr
= 1.7071 m and cF en
vt
= 1.4923 m.
It is convenient to remember that the vertical ﬁn chord is around 13.6% larger than
that of the conventional tail conﬁguration, i.e., ∆cvt = 13.6%.
Finally, the estimation of the fenestron vertical ﬁn is done assuming that the
relative thickness of the ﬁn is kept constant. Also, as previously explained, the length
of the fenestron conﬁguration ﬁn, bF en
vt , is one fan radius longer than the UH60A
conventional tail, bConv
vt
= 7.5 ft
W F en
vt
= W Conv
vt
∆c2
vt
bConv
vt
+ 0.5DF en
tr
bConv
vt
(101)
W F en
vt
, given by Equation 101, is in general a function of the value of DF en
tr
. For
the baseline case (DF en
tr
= 1.7071 m), its value is W F en
vt
= 49.047 kg.
3.7.2.2
Blades Weight Estimation
Kalra et al. [102] propose a detailed equation for the weight of main rotor blades
that is used in this work for the conventional and fenestron tail rotor blade weight
131

estimation, Wtr,bl.
Wtr,bl = ktr,bl
Nb
4
0.5348 σsolR2.7
 AR
18
0.7
(102)
where σsol is the rotor solidity, and ktr,bl is a constant of value 15.
Applying Equation 102 to the conventional tail conﬁguration results in a W Conv
tr,bl
=
22.45 kg (W Conv
tr,1bl = 5.6125 kg per blade). Regarding the fenestron case, the value of
W F en
tr,bl is determined by fenestron parameters, speciﬁcally DF en
tr
, cF en
tr , and NF en
b,tr . For
the baseline case (parameters are given by Table 1), the resulting weight is W Conv
tr,bl
=
21.529 kg.
3.7.2.3
Hub Weight Estimation
Tishchenko et al. [185] propose the following tail rotor hub weight estimation Wtr,hub
Wtr,hub = ktr,hubNb,trfz,tr,blN1.35
cf,tr,bl
(103)
where ktr,hub is a constant weight coeﬃcient for the tail rotor hub equals to 0.5, Nb,tr
the number of tail rotor blades, fz,tr,bl a factor to account the inﬂuence of the number
of blades in the hub weight, and Ncf,tr,bl the centrifugal force on the tail rotor blades.
The value of Ncf,tr,bl is assessed by the integration of the centrifugal force along
the blade
Ncf,tr,bl =
Z Rtr
rcut
ρtr,blΩ2rdr
where ρtr,bl = Wtr,1bl
Rtr
is the blade weight per unit of length, and rcut the root cut-oﬀ,
which is set to 1
3 for the fenestron case in this work.
The value of fz,tr,bl is to be determined.
The helicopter example provided in
Appendix A in Prouty’s work [146] is used to estimate its value.
The parameter
values of this helicopter example are NP rou
b,tr
= 3, cP rou
tr
= 1 ft, ΩP rou
tr
= 100 rad/sec,
and RP rou
tr
= 6.5 ft. In order to calculate the blade weight, Equation 102 is used,
which provides W P rou
tr,bl
= 24.4 kg.
132

It is known that the total weight of the whole tail rotor assembly in the Prouty’s
example is 82.1 kg. Thus, the diﬀerence between the rotor assembly and the blade
weight is the weight of the tail rotor hub, which is W P rou
tr,hub = 57.7 kg. Now all the
factors in Equation 103 are known except fz,tr,bl, so one can solve for it, obtaining a
value of fz,tr,bl = 9.15 · 10−6.
Once the value of fz,tr,bl is obtained, Equation 103 provides W Conv
tr,hub = 67.36 kg.
Also, Equation 103 is employed to obtain W F en
tr,hub; it is a function of DF en
tr
, cF en
tr , and
NF en
b,tr . For the baseline case the resulting weight is W Conv
tr,hub = 90.058 kg.
Finally, the total change in weight, ∆W, due to the fenestron tail is given by
Equation 104
∆W = W F en
vt
−W Conv
vt
+ W F en
tr,bl −W Conv
tr,bl
+ W F en
tr,hub −W Conv
tr,hub
(104)
which in general is a function of the fenestron design variables. When particularized
to the the baseline case, its value is ∆W = 43.143 kg.
3.8
Research Methodology Diagrams
Once the methods and the desired studies have been explained, it is helpful to plot
the adaptive sampling algorithms, that include the developed surrogates as well, for
categories and concepts. Adaptive sampling on MIC and ECMF surrogates are shown
in Figures 11, and 12, respectively. These Figures also contain the studies on the
inﬂuence of training set sizes, nominal distances, and mutation strategies.
Figure 11: Diagram of Adaptive Sampling on MIC Surrogates
133

Figure 12: Diagram of Adaptive Sampling on ECMF Surrogates
Figures 13 and 14 show the whole methodology diagrams for the categories and
concepts, respectively. They contain the motivation, technical challenges, research
questions, hypothesis, and experiments to achieve the purpose of this thesis.
134

Figure 13: MIC Methodology Review
135

Figure 14: ECMF Methodology Review
136

CHAPTER IV
FLIGHTLAB UH60A MODEL
A practical computational model is pursued to measure in the following Chapters the
eﬃciency of the proposed surrogates, MIC and ECMF. They are intended as tools
in conceptual design of engineering systems where there exists a choice of discrete
alternatives. The targeted application is helicopter design.
Also, the proposed methods are aimed to computationally expensive functions. A
possible example of these functions is the performance of a helicopter; the assessment
of the performance requires to combine several disciplines such as aerodynamics,
structural mechanics, and controls. Thus, even mid-ﬁdelity codes for each discipline
produce intense function calls due to the necessary iterations between disciplines. The
author has no access to the mid and high-ﬁdelity commercial software for helicopters
RCAS or OVERFLOW. However, the software FLIGHTLAB [1] is available.
Typical performance measures used in conceptual design of helicopters are the
engine shaft horsepower in hover and forward ﬂight. Herein, these two measures are
the objective functions to optimize the helicopter design.
The choice of the helicopter to model has to satisfy two conditions: availability
of a reliable FLIGHTLAB model and some experimental information to validate
the model. These restrictions lead the author to pick the UH60A as the baseline
helicopter.
In the remaining part of the Chapter, a review of the UH60A parameters is pre-
sented. Followed by the validation of the hover and forward ﬂight models. The noisy
nature of the FLIGHTLAB output is discussed. Finally, the FLIGHTLAB fenestron
modeling is presented.
137

Table 2: Parameters of the UH60A Baseline Model [23]
W = 17252.17 lb
V elhov = 0 knots
V elfor = 139 knots (Adv. Ratio = 0.3218)
UH60A Fuselage
Positionfus,c.g.1= [345.5, 234.0, 0] in
2 Horizontal Stabilizers
Surface Each Hor. Stab. = 22.5 ft2
AR = 4.6
Airfoils=NACA 0014
Positionattach,hs = [700.1, 244, 0] in
Surface Vert. Stab. = 32.3 ft2
AR = 1.92
Airfoils=NACA 0021
Positionattach,vs = [695, 273, 0] in
T700-GE-700 Engine
Neng = 2
Nominal Engine Torque = 355 lbf · ft
Main Rotor to Engine Gear Ratio = 0.012336
UH60A Landing Gear
1Position = {Fuselage, Waterline, Buttline}
4.1
UH60A Parameters
The FLIGHTLAB commercial package includes a example helicopter which is really
close to the UH60A helicopter.
Taking a look at the literature, one can ﬁnd ex-
perimental data on UH60A hover and forward ﬂight performance, see Bousman and
Kufeld [23], Yeo et al. [205] Lawrence et al. [121], and Shinoda et al. [168]. The
baseline helicopter and rotor conﬁgurations are given in Tables 2, and 3, respectively.
4.2
Hover Model Validation
The hover model consists of:
1) Main Rotor. Articulated rigid rotor. Flapping and lead-lag dynamics are in-
cluded. The aerodynamics is brought by a quasi-steady aerodynamic model with
look-up tables for the blade sections (“SC 1095” and “SC 1094R8”). Aerody-
namic tables obtained from JAVAFOIL[84], assuming the Re of the 75% of the
138

Table 3: Parameters of the UH60A Baseline Rotors [23]
R = 26.83 ft
Ω= 27 rad/sec
Nb = 4
¯c = 1.73 ft. Rectangular blade
Blade Twist of the UH60A Blade
Airfoils = {SC 1095, SC 1094R8, SC 1095}
Airfoil boundary = [0.0; 0.485; 0.835; 1.0]R
Hinge Oﬀset = 1.25 ft
Rotor Position = [342.215, 315, 0] in
Weight One Blade = 256.91 lb
Blade Moment Inertia About Hinge = 1512.6 slug · ft2
Longitudinal Shaft Tilt = −3 degrees
Articulated rotor: Flap and lead-lag hinge
Rtr = 5.5 ft
Ωtr = 124.62 rad/sec
Nb,tr = 4
¯ctr = 0.81 ft. Rectangular blade
θtr = −18 degrees
cl,α,tr = 5.73
cd0,tr = 0.0087
cd1,tr = −0.0216
cd2,tr = 0.4
Tail Rotor Position = [732, 324.7, −14] in
139

blade span at hover. A blade tip loss factor of 0.97 is assumed. The Peters-He
three states inﬂow computes the induced velocity. Interference between main
rotor and fuselage is allowed. Regarding discretization, there are a set of 20
aerodynamic segments and another of 6 structural segments, which are both
distributed by an equal annuli area.
2) Tail Rotor. It is a Bailey rotor with blade tip loss factor of 0.92. It is modeled
with a quadratic airfoil drag polar and a linear lift curve.
3) A rigid fuselage with empirical airloads.
4) Aerodynamic surfaces. Rigid surfaces with aerodynamic look-up tables for the
surface section.
5) Propulsion. Two ideal engines without power losses.
6) Control. Standard ﬂight control unit with a longitudinal, lateral and collective
stick, and pedals. Also, two attitudes as pseudo-controls.
This model is compared with the experimental data from “UH60A Airloads Pro-
gram”, speciﬁcally the “UH60A Airloads Catalog” by Bousman and Kufeld [23]. They
measured the UH60A hover performance in terms of the total helicopter CP for sev-
eral CW. Figure 15 shows the “UH60A Airloads Program” experimental results and
the corresponding FLIGHTLAB model simulations. The FLIGHTLAB model does
not take into account power losses like transmission, interference between rotors, elec-
trical or operation losses; thus, the FLIGHTLAB model has been corrected with a
helicopter eﬃciency, η, of 90%.
According to the results in Figure 15, the UH60A model represents decently the
hover performance of the UH60A.
140

 
 
Airloads Program
Flight Lab
CW
CP
×10−3
4
4.5
5
5.5
6
6.5
7
7.5
8
8.5
×10−4
3
4
5
6
7
8
9
10
Figure 15: Total CP vs CW. Validation of UH60A Hover Shaft Power. FLIGHTLAB
Model
4.2.1
Fenestron Modeling in FLIGHTLAB
FLIGHTLAB features a ducted fan airload model [1]. It is derived from momen-
tum theory combined with blade element analysis. Enhancement features are present
in the model, such as duct-fan interference, and fan wake contraction. These en-
hancements require the use of empirical corrections obtained either by experimental
measurements or advance CFD. However, the author does not have access to these
data; therefore, the ducted fan is kept in the most simple way: the fan wake contrac-
tion and the duct-fan interference are considered negligible. A proper duct lip design
for speciﬁc ﬂight conditions can diminish the fan wake contraction eﬀect.
141

4.3
Forward Model Validation
The forward model consists of:
1) Main Rotor. Articulated rigid rotor. Flapping and lead-lag dynamics are in-
cluded. The aerodynamics is brought by a quasi-steady aerodynamic model
with look-up tables for the blade section (“SC 1095” and “SC 1094R8”). Aero-
dynamic tables obtained from JAVAFOIL[84], assuming the Re of the 75% of
the blade span at hover. A blade tip loss factor of 0.97 is assumed. The inﬂow
model is a Glauert one with a nonuniform correction of 15%. No interference
between main rotor and fuselage is allowed. Regarding discretization there are
a set of 20 aerodynamic segments and another of 6 structural segments, which
are both distributed by an equal annuli area.
2) Tail Rotor. It is a Bailey rotor with blade tip loss factor of 0.92. It is modeled
with a quadratic airfoil drag polar and a linear lift curve.
3) A rigid fuselage with empirical airloads.
4) Aerodynamic surfaces. Rigid surfaces with aerodynamic look-up tables for the
surface section.
5) Propulsion. Two ideal engines without power losses.
6) Control. Standard ﬂight control unit with a longitudinal, lateral and collective
stick, and pedals. Also, two attitudes as pseudo-controls.
Once again, the model is compared with the experimental data from “UH60A Air-
loads Program”, speciﬁcally the “UH60A Airloads Catalog” by Bousman and Kufeld
[23]. They measured the UH60A forward ﬂight performance in terms of the total
helicopter CP vs advance ratio for several CW. In this work the weight coeﬃcient
chosen to validate the forward ﬂight model is CW = 0.0074. Figure 16 shows both
142

the “UH60A Airloads Program” experimental results and the FLIGHTLAB model
simulations. As in the hover case, see Section 4.2, the FLIGHTLAB model does not
take into account power losses. The losses are the same as in hover, but the rotors
interference is supposed to be larger for forward ﬂight.
The FLIGHTLAB model has been corrected with a helicopter eﬃciency, η, of 83%.
It could be thought as a slightly low helicopter eﬃciency; however, it is convenient
to mimic the experiment measures from the “UH60A Airloads Program”. Figure 16
shows some conﬁdence in the UH60A forward ﬂight FLIGHTLAB model.
 
 
Airloads Program
Flight Lab
µ
CP
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
×10−3
0
0.2
0.4
0.6
0.8
1
Figure 16: Total CP vs Advance Ratio. Validation of UH60A Forward Flight Shaft
Power. FLIGHTLAB Model
Notice that this forward ﬂight validation was carried out with a weight diﬀerent
from the baseline, see Table 2. It is done because no experimental data was found
for the exact baseline weight.
Also, the fenestron modeling in forward ﬂight is similar as the one in the hover
case, see Subsection 4.2.1.
143

4.4
UH60A Power Consumption as a Noisy Function
It is well known that numerical solvers require discretization and iterative methods.
The resulting solutions are noisy, which represents a problem for design teams when
exploring and exploiting objective functions [66]. The noisy nature of FLIGHTLAB
solutions are shown in this Section.
In order to demonstrate the existence of noise in the UH60A hover power con-
sumption, the landscape of the function is shown in some part of the domain space,
speciﬁcally along the main rotor linear twist design variables: θ1 and θ2. The function
is computationally expensive, so a detailed sweep in a large design space is not practi-
cal. Thus, a reduced design space is chosen to evaluate the hover power consumption.
The reduced domain where the function is evaluated is:
θ1 = −3.5 + 3L◦
x1/m
θ2 = −1.75 + 2.25L◦
x2/m
where Lx1 = Lx2 = 10−2.
Evaluations of the UH60A hover power consumption are performed on a grid of
this two-dimensional domain space. The samples on the grid are given by parameters
Nsampl,x1 = 101, and Nsampl,x2 = 101. Figure 17 exhibits the contours of the function.
It can be seen the noisy nature of the function.
In order to get a better insight of the noise of the FLIGHTLAB simulation, the
fast Fourier transform (FFT) of the function, FT (fx1, fx2), is carried out to see which
frequencies are present in the hover power consumption. The values of fx1 and fx2
are given by the spacing of the evaluation grid.
fx1 =
1
∆x1
{0, 1, 2, ..., floor
Nsampl,x1
2

−1}
fx2 =
1
∆x2
{0, 1, 2, ..., floor
Nsampl,x2
2

−1}
where ∆x1 and ∆x2 are the separation between the grid evaluation points in the
dimensions x1 and x2, respectively.
144

 
 
θ1
θ2
0
0.002
0.004
0.006
0.008
0.01
1660.4
1660.5
1660.6
1660.7
1660.8
1660.9
1661
1661.1
1661.2
1661.3
1661.4
0
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0.01
Figure 17: Noise on UH60A shphov. Lx1 = Lx2 = 10−2
The amplitude of the FFT, |FT (fx1, fx2) |, is plotted in Figure 18. It is seen that
the constant term is the dominating one. However, when zooming in high frequency
areas of the FT (fx1, fx2), non-zero values are seen, see Figure 19. Even though the
values of the amplitude are much smaller than that of the zero frequency, the ExI
criterion is expected to set points close to one other when a local optimum is found.
Interpolating surrogates cannot deal with noisy observations. Therefore, this high
frequency noise can result in ﬂawed surrogates when the adaptive sampling process
ﬁnds a local optimum.
145

fθ1
fθ2
|f (fθ1, fθ2) |
×105
0
0.5
1
1.5
2
2.5
×105
0
0.5
1
1.5
2
×106
0
2
4
6
8
10
12
14
16
18
Figure 18: Fast Fourier Transform UH60A shphov
fθ1
fθ2
|f (fθ1, fθ2) |
×105
0
0.5
1
1.5
2
2.5
×105
0
0.5
1
1.5
20
0.5
1
1.5
Figure 19: Fast Fourier Transform UH60A shphov. Close-Up to Large Frequency
146

CHAPTER V
MIC EXPERIMENTS: TRAINING SIZE AND NOMINAL
DISTANCE
As mentioned in previous sections, one of the goals of this research is to develop
surrogates that take advantage of similar trends across categorical alternatives. Two
Kriging surrogates are brought in this Chapter:
MIC Surrogates They are developed to leverage similar trends across categories.
They include categorical variables in its domain, so it hosts all kinds of design
attributes: continuous, discrete-quantitative and non-numeric. The deﬁnition
of a nominal distance is necessary to include the categorical variables in the
surrogate domain. A study is carried out to see the inﬂuence of several nominal
distances in the surrogate performance. However, the MIC surrogate that is
compared against the current state-of-the-art is based on the Hamming distance.
Independent Surrogates It is the current state-of-the-art method. A surrogate is
ﬁtted independently for each category.
Both surrogates, the independent surrogate and the MIC surrogate, are compared
to assess the MIC surrogate eﬃciency. The quality of these surrogates is expected to
increase as the number of the training set size does. Two functions are tested: the
noise-free and cheap disturbed Branin function; and the noisy and intense UH60A
hover shp. As was mentioned in Section 1.7, tests on computationally expensive mod-
els will not help to better interpret the canonical test results or support the research
hypothesis or predictions; however, it supports the practicality of the application.
147

The surrogate comparison is given in terms of the success indicators introduced
in Subsection 3.4.1: rms of the “standardized validation error”, and rms of the “stan-
dardized validation residual”, see Equations 77 and 78, respectively.
5.1
Disturbed Branin Function
The ﬁrst function tested is the disturbed Branin function. Some disturbances are
applied on the ﬁrst Fourier modes of the Branin function to obtain an objective
function that has similar categorical trends. Also, the second variable of the original
Branin function, x2, is transformed into a discrete-quantitative one by discretizing it
into 11 equally distanced discrete points. Appendix A provides a detailed explanation
on the generation of the disturbed Branin function. The landscape of the function is
plotted in Figure 20. For clarity’s sake, the variable x2 is shown as continuous, but it
is necessary to remind the reader that it is a discrete-quantitative one whose possible
values are

0, 1
10, 2
10, ...1

.
 
 
x1
Category 6
Category 5
Category 4
x2
Category 3
Category 2
Category 1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
50
100
150
200
250
300
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
Figure 20: Contours of Disturbed Branin Function
148

5.1.1
Inﬂuence of Training Size on Performance
The two meta-models are compared in a range of training set sizes from 20 to 400
points. When building the independent surrogates, the training set is equally divided
among the categories. Since both meta-models are probabilistic, two performance
indicators are used for the comparison: rms of the surrogate “standardized validation
error”, and rms of surrogate “standard validation residual”. A set of validation points
are evaluated to assess the performance indicators.
Curves are ﬁtted to try to understand the tendency of the results for each type of
surrogate. For the rms of the “standardized surrogate error” the ﬁtting curve chosen
is of the form
B
Nα
tr.set
(105)
it asymptotically tends to zero since the r.m.s. of the “standardized surrogate error”
is expected to do so. The curve ﬁt for the rms of the standardized surrogate residual
is
A +
B
Nα
tr.set
(106)
it asymptotically tends to A since the r.m.s. of the “standardized surrogate residual”
is expected to tend to a number, speciﬁcally to one. A, B, and α are constants.
Note that, as explained in Subsection 3.4.1, the value of the constant α determines
how quickly performance indicators converge to their ﬁnal value; thus, α enables the
study of the convergence as the training set size increases.
The success indicator rms of the “standardized validation error” is plotted in
Figure 21. It shows the comparison between the MIC Hamming surrogate versus the
current state-of-the-art (one independent surrogate for category).
Figure 21 shows that the MIC Hamming surrogate results in more accurate meta-
models than ﬁtting independent surrogates for each category for the small range of
sampling sizes. The reason is that, in the case of low number of points, only a few
149

 
 
MIC Hamming
Independent
Average Rms of Stand Error
Size of the Training Set
101
102
103
10−3
10−2
10−1
Figure 21: Surrogate “Standardized Validation Error”. MIC vs Independent Surro-
gate. Disturbed Branin Function
points are sampled for each category; thus, the rms of the “standardized validation
error” for the independent surrogate has little information, resulting in a poorer
performance than the MIC Hamming surrogate, where observations are cross-used
across categories.
Speciﬁcally, for 24 sample points and six categories, only four
points are used for building the two-dimensional surrogate of each category.
For high training set sizes, independent surrogates gain performance up to a point
where they produce a better surrogate than the MIC Hamming one. It implies that
when the training set is large enough, the cross-use of observations matters less. This
cross-use of observations at this high range of sample points are even disadvantageous,
producing surrogates less accurate than the ones that do not cross-use observations.
The reason could be that, in this large training set case, observations hold not only
similar trend information but also high frequency information for each category which
is diﬀerent across categories. Figure 21 shows that, for training sets larger than 280
samples, the current state-of-the-art outperforms the proposed MIC surrogate.
150

Table 4: Values of Constant α for the Fitting Curves of Rms of the “Standardized
Validation Error”. Independent vs MIC Surrogate. Disturbed Branin Function
Low Ntr.set
Large Ntr.set
Independent Surr.
0.7122
3.533
MIC Hamming Surr.
0.553
1.156
The gain in performance of the independent surrogate with respect to the MIC
Hamming surrogate is quantiﬁed by the values of the constant α in the ﬁtting curve of
the rms of the surrogate error (See Equation 105). Table 4 contains the values of αs.
It is seen that independent surrogates converge more quickly than the MIC Hamming
ones. Also, it is seen that if two ﬁtting curves are used (one for small and another
for large training sets), then the gain in performance of the independent surrogate in
large training sets is higher than in small ones.
Finally the “standard validation residual” is plotted in Figure 22 for the indepen-
dent surrogate and the MIC Hamming one. The independent surrogate “standardized
validation residual” is higher than one for small training sets, which implies that the
uncertainty estimated from the surrogate is lower than the real error, i.e., the surro-
gate uncertainty is underestimated. However, the “standardized validation residual”
for the MIC Hamming surrogate is around one for the whole studied range of training
set sizes.
Results are similar to the ones obtained for the “standardized validation error”:
the MIC Hamming surrogate outperforms the state-of-the-art for small training set
sizes while applied on the disturbed Branin function. For more details about the
tested function, see Appendix A.
5.1.2
Inﬂuence of Nominal Distance on Performance
MIC surrogates can be based on several nominal distances. The inﬂuence of these
distances in the performance of the MIC surrogate is studied in this Subsection. The
key to include the categorical input into the meta-model is the deﬁnition of a distance
151

 
 
MIC Hamming
Independent
Average Rms of Stand Res
Size of the Training Set
101
102
103
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
Figure 22:
Surrogate “Standardized Validation Residual”.
MIC vs Independent
Surrogate. Disturbed Branin Function
between categories that allows interpolating and regressing techniques, as is explained
in Section 3.4.
Several distance functions were presented in Section 3.3 to include the categorical
inputs into the Gaussian process meta-model.
They were the following: integer,
Hamming, and intrinsic.
The intrinsic-based nominal distances for the disturbed
Branin function are based on phases and amplitudes of the ﬁrst nine Fourier modes of
the disturbed Branin function. The possible nominal distances (four of them intrinsic-
distances) are the following:
Hamming distance The Hamming distance is employed for the categorical vari-
ables.
Integer distance The nominal variable is arbitrarily mapped to a set of integer
values. Each category of a non-numeric variable is mapped to an integer in the
interval [1, |xnom|], where |xnom| is the number of points in the input xnom. Then,
152

the distance brought to the Kriging surrogate is based on this new arrangement
of xnom in the integer line.
Phase-based distance 1 The distance between categorical members i and l is given
by the phase diﬀerence in the ﬁrst nine Fourier modes.
d
 x(i)
nom, x(l)
nom

=
9
X
j=1
|∠FTx(i)
nom (n1j, n2j) −∠FTx(l)
nom (n1j, n2j) |
(107)
Then, distances given by Equation 107 are normalized to set the largest distance
between categories to 1.
Phase-based distance 2 As “phase-based distance 1” but with an adjustment to
set the minimum distance between categories, d⋆, to 0.5 according to Equation
74.
Amplitude-based distance 1 The distance between categorical members i and l
is given by the amplitude diﬀerence in the ﬁrst nine Fourier modes.
d
 x(i)
nom, x(l)
nom

=
9
X
j=1
||FTx(i)
nom (n1j, n2j) | −|FTx(l)
nom (n1j, n2j) ||
(108)
Then, distances given by Equation 108 are normalized to set the largest distance
between categories to 1.
Amplitude-based distance 2 As “amplitude-based distance 1” but with an ad-
justment to set the minimum distance between categories, d⋆, to 0.5 according
to Equation 74.
In Equations 107 and 108 FTx(i)
nom is the Fourier transform of the i-th category of the
disturbed Branin function; n1j and n2j are the frequencies for the jth mode in the
ﬁrst and second coordinate, respectively.
The rms of the “standardized validation error” is plotted in Figure 23. It shows the
comparison between MIC surrogates based on the previously listed nominal distances.
153

 
 
MIC Integer
MIC Intr Phase 2
MIC Intr Phase 1
MIC Intr Ampl 2
MIC Intr Ampl 1
MIC Hamming
Average Rms of Stand Error
Size of the Training Set
101
102
103
10−2
10−1
Figure 23: Surrogate “Standardized Validation Error”. Comparison Several MIC
surrogates. Disturbed Branin Function
Once it has been shown that for small ranges of training set sizes the MIC surrogate
is convenient (see Subsection 5.1.1), interest is in the performance of MIC surrogates
based on several nominal distances, see Figure 23.
This Figure immediate shows
that the MIC integer performs badly in comparison with the other nominal distance-
based MIC surrogates. The performance of the Hamming and the intrinsic models
are similar according to “standardized validation error”. It is noticeable that models
that adjust the minimum distance (MIC intrinsic amplitude 2 and phase 2) behave
better than their non-adapted counterparts (MIC intrinsic amplitude 1 and phase 1).
Also, MIC surrogates based on the Fourier amplitude distance provide slightly poorer
rms of the “standardized validation error” than the ones based on Hamming and the
Fourier phase distances.
Table 5 contains the values of αs for all the MIC surrogates. It is seen that all
the MIC surrogates gain performance with the same speed, except the MIC integer
that is the slowest in both small and large training set sizes. It is important to realize
154

Table 5: Values of Constant α for the Fitting Curves of Rms of the “Standardized
Validation Error”. All MIC Surrogates. Disturbed Branin Function
Low Ntr.set
Large Ntr.set
MIC Hamming
0.553
1.156
MIC Ampl. 1
0.389
1.393
MIC Ampl. 2
0.433
1.326
MIC Phase 1
0.488
1.174
MIC Phase 2
0.481
1.318
MIC Integer
0.268
1.017
that the exact values of αs depend on where the splitting point between the large and
small training seats. Thus, a diﬀerent choice of the splitting point would change the
values of αs; however, these changes are expected to be small.
Regarding the “standardized validation residual”, its behavior for several nominal
distance-based MIC surrogates are similar to the one of MIC Hamming surrogate.
5.2
UH60A Hover Shaft Power. Screened Model
Once the ”MIC Surrogate” has been successfully tested on a noise-free canonical
function, the next step is to test it in a practical application. Rotor-craft design
is the selected application to apply the meta-modeling and adaptive sampling tech-
niques. The multidisciplinary nature of the rotor-craft, the complexity of the rotor-
craft physics, and the multi-objective desire of design teams make typical function
evaluations time-consuming. Physics based rotor-craft models are noisy due to itera-
tions, discretizations, ... Therefore, rotor-craft objective functions are a great scenario
to test MIC surrogates on noisy functions. In order to learn the MIC characteristics
when applied to noisy functions, the ﬁrst rotor-craft design tests are done in screened
domains that are more manageable for testing purposes.
5.2.1
Screening of UH60A Hover Shaft Power
In order to better understand the nature of the objective function, a screening of the
function is done to reduce the number of design variables. The objective function is
155

the power consumption of the whole UH60A helicopter, shphov. For further details of
the model, see Section 4.2. Typical design variables are inner θ1 and outer θ2 main
rotor twists, main rotor chord c, radial position of the twist change rtw, main rotor
airfoil, tail rotor radius Rtr, and tail rotor chord ctr. The design limits of the full
design space are shown in Figure 24.
Figure 24: Limits of the Design Space to Screen
In order to reduce the domain, a full factorial DoE of 2 levels is carried out in
JMP R
⃝. Note that the main rotor airfoil is not included in the screening because it is
the categorical variable chosen to later test MIC surrogates. The full factorial DoE
of 2 levels is done for the airfoil “SC 2110”. Figure 25 shows the screening results:
the main features of the functions are captured by keeping the inner θ1 and outer θ2
main rotor twists; the main rotor chord c; and the interactions between θ1, θ2, and c.
Fitting a regressive model in JMP R
⃝with θ1, θ2, and c as independent variables
results in a RSquare = 0.9852. The value of RSquare measures the proportion of
the function variation around the mean explained by the model rather than random
errors. The remaining variation is not explained by the model and is attributed to
random error. Therefore, the screened model that keeps θ1, θ2, and c catches much
of the function behavior (98.52% of the function variation).
156

Figure 25: Screening Results of the UH60A Hover Power Consumption
5.2.2
Landscapes of Screened Domain
The design variables are: the inner blade twist θ1 (continuous variable), the outer
blade twist θ2 (continuous variable), the chord length c (discrete-quantitative vari-
able), and the type of airfoil (categorical variable). Four possible airfoils are available:
“NACA 0012”, “SC 2110”, “NACA 23012”, and “SC 1095”. The objective function
is summarized as follows:
shphov (θ1, θ2, c, airfoil)
(109)
subjected to
θ1 ∈[−3.5◦/m, −0.5◦/m] ∈R
θ2 ∈[−1.75◦/m, 0.5◦/m] ∈R
(110)
157

c
¯c ∈0.75 +

0, 1
3, 2
3, 1

· 0.5
airfoil ∈[NACA 0012, ..., SC 1095]
(111)
where ¯c = 1.73ft is the baseline chord.
The remaining parameters are W = 16994lb (CT = 5.99 · 10−3), R = 26.83ft,
Ω= 27.0063 rad
sec, and rcut-oﬀ
R
= 0.047. As a numerical function this function is noisy. A
regression model is ﬁtted to visualize and understand its landscapes better. Figures
26, 27, 28, and 29 show the regression of the objective function for diﬀerent chord
values. The four airfoils clearly experience similar trends. It is seen that the regression
of shphov looks smoother than the Branin function studied in Section 5.1; however,
it is important to remember that shphov is a noisy function, and the Figures show a
regression ﬁt that absorbs the noise.
 
 
sc1095
naca23012
sc2110
naca0012
c = 0.750¯c
θ1
θ2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1700
1800
1900
2000
2100
2200
2300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 26: UH60A Total shphov versus θ1 and θ2. c = 0.750¯c
158

 
 
sc1095
naca23012
sc2110
naca0012
c = 0.917¯c
θ1
θ2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1750
1800
1850
1900
1950
2000
2050
2100
2150
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 27: UH60A Total shphov versus θ1 and θ2. c = 0.917¯c
 
 
sc1095
naca23012
sc2110
naca0012
c = 1.083¯c
θ1
θ2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1850
1900
1950
2000
2050
2100
2150
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 28: UH60A Total shphov versus θ1 and θ2. c = 1.083¯c
159

 
 
sc1095
naca23012
sc2110
naca0012
c = 1.250¯c
θ1
θ2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1950
2000
2050
2100
2150
2200
2250
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 29: UH60A Total shphov versus θ1 and θ2. c = 1.250¯c
160

5.2.3
Inﬂuence of Training Size on Performance
The goal of this section is to see the eﬃciency of MIC surrogates with respect to
independent surrogates (one for each categorical alternative). The objective to test
these surrogates is the UH60A hover power in the screened domain. The two meta-
models are compared in a range of training set sizes from 36 to 380 points. When
building the independent surrogates, the training set is equally divided among the
four airfoil surrogates. Since both meta-models are probabilistic, two performance
indicators are used for the comparison: rms of the surrogate “standardized validation
error”, and rms of surrogate “standard validation residual” given by Equations 77,
and 78, respectively. A set of 1024 validation points, 256 for each airfoil, are evaluated
to assess the mentioned performance indicators.
Again, curves are ﬁtted to try to understand the tendency of the results for each
type of surrogate. For the rms of the “standardized validation error”, the curve chosen
is Equation 105, and for the rms of the “standardized validation residual” Equation
106.
 
 
MIC Hamming
Independent
Average Rms of Stand Error
Size of the Training Set
101
102
103
10−2
10−1
Figure 30: Surrogate “Standardized Validation Error”. MIC vs Independent Surro-
gate. UH60A shphov Screened Domain
Figure 30 shows that the MIC Hamming surrogate produces a more accurate
161

surrogate than ﬁtting an independent surrogate for each airfoil in the small range of
sampling sizes, speciﬁcally from 36 to 125 points. The independent surrogate gains
performance, as training set size increases, up to a point where it produces a better
surrogate than the MIC one. It appears to reﬂect that, when the training set is large,
the cross-use of observations matters less, as experienced in Subsection 5.1.1.
The reason is that, for low number of points, only a few points are sampled for
each category; thus, the rms of the “standardized validation error” for the indepen-
dent surrogate has a poor performance. Speciﬁcally for 36 training points, only nine
points are used to build a surrogate in the remaining three design variables: θ1, θ2,
and c. Nine points are not enough to construct an accurate three dimensional sur-
rogate.
However, the MIC surrogate, although it also has only nine observations
for airfoil, successfully cross-uses observations from other airfoils, producing more
accurate surrogates.
For large training set sizes, the independent surrogate gains in performance until
it produces a better surrogate than the MIC Hamming one; this occurs with training
sets of approximately 125 points. The reason could be that, in this large training set
case, observations hold not only similar trend information but also high frequency
information for each airfoil which are diﬀerent across airfoils. This share of obser-
vations at this large range of points even produces surrogates less accurate than the
ones that do not share observations.
The gain in performance of independent surrogates with respect to the MIC Ham-
ming surrogates is again quantiﬁed by the values of the constant α in the ﬁtting curve
of the rms error, see Equation 105. Table 6 contains the values of αs. For small train-
ing sets (see “low Ntr.set” column in Table 6), the independent surrogate converges
more quickly than the MIC Hamming one as Ntr.set increases. For large training sets,
both surrogates converge at a similar speed (similar values of α).
Figure 31 exhibits the rms of the “standardized validation residual” of the UH60A
162

Table 6: Values of Constant α for the Fitting Curves of Rms of the “Standardized
Validation Error”. Independent vs MIC Surrogate. UH60A shphov Screened Domain
Low Ntr.set
Large Ntr.set
Independent Surrogate
1.876
2.993
MIC Hamming
1.11
2.78
 
 
MIC Hamming
Independent
Average Rms of Stand Res
Size of the Training Set
101
102
103
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
Figure 31:
Surrogate “Standardized Validation Residual”.
MIC vs Independent
Surrogate. UH60A shphov Screened Domain
shphov for the independent surrogate and the MIC Hamming one. The independent
surrogate “standardized validation residual” is higher than one for small training sets.
It implies that the uncertainty estimated with the independent surrogate is lower than
the real error, i.e., the surrogate uncertainty is underestimated.
When the training set becomes larger, the behavior of the independent surrogate
is the opposite: the rms of the “standardized validation residual” is lower than one,
implying that the uncertainty estimated from the surrogate is higher than the real
error. It is worth noticing that the curve ﬁt shown in Figure 31 is decreasing; however,
a closer look into large training set sizes shows that the local tendency of the inde-
pendent surrogate residual is to increase towards one. One can explain the erroneous
behavior of the ﬁtting curve at large training sets with the choice of the ﬁtting curve,
A + B
xα ,that only allows for monotonicity.
The rms of the “standardized validation residual” of the MIC Hamming surrogate
163

is also higher than one for the small range of Ntr.set. It also tends to one as the
training set increases.
Compared with the independent surrogate case, the MIC
Hamming residual is closer to one for the majority of the studied range of training
set sizes, so the MIC surrogate estimates the uncertainty of the UH60A shphov better
than the independent surrogate.
Both performance indicators demonstrate that the MIC Hamming surrogate out-
performs the state-of-the-art for small training set sizes when applied on the noisy
UH60A shphov.
5.2.4
Inﬂuence of Nominal Distance on Performance
MIC surrogates can be based on several nominal distances. The inﬂuence of these
nominal distances in the performance of the MIC surrogate is studied in this Sub-
section. A similar studied was done in Subsection 5.1.2 for the case of the noise-free
disturbed Branin function. For the noisy UH60A shphov, three nominal distances are
tested:
Hamming distance The Hamming distance is employed for the categorical vari-
ables.
Integer distance The nominal variable is arbitrarily mapped to a set of integer
values. Each category of a non-numeric variable is mapped to an integer in the
interval [1, |xnom|], where |xnom| is the number of points in the categorical input
xnom. Then, the distance brought to the Kriging surrogate is based on this new
arrangement of xnom in the integer line.
Cd-based distance The distance between airfoils i and l is based on the drag co-
eﬃcient. Since the aerodynamic drag curves are functions, the L1 distance is
brought, see Section 3.3. The distance takes the form of
d
 air(i) (α) , air(l) (α)

=
Z αstall
αinf
|C(i)
d (α) −C(l)
d (α) |dα
(112)
164

Then, distances given by Equation 112 are normalized to set the largest distance
between categories to one.
Results on the performance of MIC surrogates based on several nominal distances
are shown in Figure 32. This ﬁgure exposes similar results as in the free-noise dis-
turbed Branin function studied in Subsection 5.1.2. Again, the MIC integer performs
badly in comparison with the other nominal distance-based MIC surrogates. The
reason could be that the integer distance sets the distance randomly and an order
among categories, which is not in the nature of nominal variables.
The performance of the MIC Hamming and MIC intrinsic models are quite close
according to “standardized validation error”. The MIC intrinsic provides only slightly
better performance than the MIC Hamming surrogate. However, as mentioned in
Section 3.3, the intrinsic distance is not versatile, because it is based in the relation-
ship category-objective function. Also, the intrinsic nominal distance could produce
ill-deﬁned Kriging covariance matrix unlike the Hamming distance. Therefore, the
Hamming distance is the one used to build MIC surrogates for practical problems.
For blade airfoils there is an underlying space that is materialized in terms of
aerodynamic curves at the ﬁdelity level used in the FLIGHTLAB analysis. An intrin-
sic distance based on these curves provides a similar result to just modeling airfoils
as purely categorical variables (using the Hamming distance). The intrinsic distance
based on the underlying continuous parameterization for the airfoil choice may be
eﬀective while doing CFD analysis. However, it is not the purpose of the research
because the multi-disciplinary nature of rotor-crafts makes unaﬀordable the use of
high-ﬁdelity tools like CFD in conceptual design.
Table 7 contains the values of αs for all the MIC surrogates. It is seen that all
MIC surrogates gain performance at similar speed. However, the MIC integer has a
larger value of the rms of the surrogate “standardized validation error”, as shown in
Figure 32.
165

 
 
MIC Integer
MIC Intr Cd 1
MIC Hamming
Average Rms of Stand Error
Size of the Training Set
101
102
103
10−2
10−1
Figure 32: Surrogate “Standardized Validation Error”. Comparison Several MIC
surrogates. UH60A shphov Screened Domain
Table 7: Values of Constant α for the Fitting Curves of Rms of the “Standardized
Validation Error”. All MIC Surrogates. UH60A shphov Screened Domain
Low Ntr.set
Large Ntr.set
MIC Hamming
1.11
2.78
MIC Cd
1.051
2.498
MIC Integer
1.002
2.653
Regarding the “standardized validation residual”, the behaviors for diﬀerent nom-
inal distance-based MIC surrogates are similar to the one of the MIC Hamming sur-
rogate, as happened with noise-free functions (see Subsection 5.1.2).
166

CHAPTER VI
MICGA AS THE EGO INFILL CRITERION SOLVER
The goal is to check if the stochastic search algorithm MICGA, developed in Subsec-
tion 3.6.2, can drive the adaptive sampling process on a MIC surrogate, which has a
mixed-integer-categorical domain. A proper behavior of the “Expected Improvement”
criterion is characterized by adaptive sampling updates behaving as EGO optimizers:
explore and exploit the MIC surrogate. For more details, see Subsection 2.7.1.
First the validation of the modiﬁed MIGA is done to get conﬁdence on the solver
before building the MICGA. Several mixed-integer canonical functions from the lit-
erature are brought to test the modiﬁed MIGA.
Later, the MICGA is tested to see its capability to drive the adaptive sampling
process on MIC surrogates. Performance of MIC surrogates with respect to indepen-
dent ones was tested in Chapter 5. Results showed evidence that, for small training
set sizes, modeling a function dependent on non-numeric variables by MIC surrogate
is more eﬃcient than building an independent surrogate for each category. Also, ex-
periments on MIC surrogates showed that the Hamming distance is an appropriate
nominal distance to build MIC surrogates. Therefore, adaptive sampling algorithms
are run on MIC surrogates, which are based on the Hamming distance and whose
warm-up sampling plans are in the eﬃcient range of training set sizes pointed out in
Chapter 5.
Runs on MIC surrogates with a ﬁxed number of updates are performed to see
where the adaptive sampling algorithm places the updates. The goal is to see if the
layout of the updates corresponds to a typical EGO pattern: high density of updates
located in globally good performing regions. The layout of updates is compared for
167

MIC Hamming and MIC integer surrogates of the disturbed Branin function (see
Appendix A).
Later, the UH60A hover shp is adaptively sampled for several warm-up sampling
plans. EGO behavior is again searched by looking at the layout of updates.
6.1
Validation Mixed-Integer Genetic Algorithm
Subsection 3.6.2 proposes an algorithm to optimize the ExI criterion on a design space
composed of continuous, integer, and categorical design inputs. In order to get to this
MICGA, an intermediate step is needed: a MIGA. MIGA stems from the MATLAB R
⃝
continuous GA by properly adjusting generation, mutation, and crossover functions,
as explained in Subsection 3.6.2. Several canonical mixed-integer problems are found
in the literature; see Floudas, Pardalos et al. [62], and Himmelblau [87], among others.
The tested optimization problems are provided in detail in Appendix C. Results for
the proposed MIGA are compared with those given by the source-restricted MIGA
provided by MATLAB R
⃝2011b.
Each problem is optimized 100 times with the two algorithms in order to marginal-
ize the intrinsic randomness of GA. Three parameters measure the success of the run:
the percentage of success, the average number of function evaluations, and the aver-
age time (in seconds) on obtaining the optimal solution. The last two measures are
computed only for successful runs.
An optimization is considered successful if all these conditions are satisﬁed: a) the
optimal objective function is within 2% of the known optimal value; b) the average
cumulative change in value of the ﬁtness function over “StallGenLimit” generations
is less than “TolFun”; and c) the constraint violation is less than “TolCon”. In case
the optimal value of the objective is zero, an optimization whose optimum absolute
value is less than 0.02 is branded as a success.
The value of the GA parameters
for both MIGA are: TolFun = 10−6, TolCon = 10−6, StallGenLimit = 50, and
168

Table 8: Solutions Obtained by Using Modiﬁed MATLAB R
⃝GA and Restricted-
Source MATLAB R
⃝Mixed-Integer GA
MATLAB R
⃝Modiﬁed MIGA
MATLAB R
⃝Restricted-Source MIGA
Problem
% Success
ave Calls
time
% Success
ave Calls
time
1
0.94
5459.7
1.5977
0.56
3004.6
0.7664
2
1
4022.5
1.3054
0.93
1310.1
0.3773
3
1
1561.5
0.3074
1
1561
0.3855
4
1
4258.2
1.4002
0.91
1302.9
0.3951
PopulationSize = 25 and 30 for problems of 2 and 3 independent variables, respec-
tively. The optimization results for the canonical problems are given in Table 8.
Table 8 shows that the percentage of success is better for the modiﬁed MIGA,
whereas the time consumption is generally lower for the MATLAB R
⃝restricted-source
MIGA. However, for the test problem 3, the time consumption is lower for the mod-
iﬁed MIGA. MIGA parameters have not been tuned because of the corresponding
high cost. It is worth noting that the intention of this Section is just to show that
the modiﬁed MIGA can successfully be used to solve mixed-integer problems; it is
a sanity check before building the desired MICGA out of this modiﬁed MIGA. An
advantage of the modiﬁed MIGA is the accessibility to change the mutation, gen-
eration, and crossover functions unlike in the restricted-source MIGA provided by
MATLAB R
⃝where it is not possible due to privacy issues. This freedom is necessary
to convert the modiﬁed MIGA into a MICGA. This accessibility also allows a case
study about the inﬂuence of possible categorical mutations in the optimization of the
ExI functions on MIC domains.
169

6.2
Adaptive Sampling of the Disturbed Branin Function
The adaptive sampling algorithm is run for 40 updates starting with initial warm-up
sampling plans of 30, 54, and 78 points. The adaptive sampling technique is run for
the MIC Hamming, MIC integer and MIC phase-based 1. The problem chosen is the
disturbed Branin function explained in Appendix A. The update points are plotted
to see if the layout of the update points follows the typical pattern of EGO type
optimizer, see Subsection 2.7.1.
An example of update points for a MIC Hamming with 54 warm-up observations
is shown in Figure 33. The updates, green circumferences ﬁlled with black, are plotted
together with contours for each nominal variable; however, it is important to remark
that the variable x2 is discrete-quantitative even though the contour plots are ﬁlled.
It could be seen that all updates are on tenth values of the x2 design input.
 
 
x1
Category 6
Category 5
Category 4
x2
Category 3
Category 2
Category 1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
50
100
150
200
250
300
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
Figure 33: Adaptive Sampling Disturbed Branin Function. Warm-up Size 54. Up-
dates 40. MIC Nominal Metric Hamming.
Figure 33 shows EGO type optimizer’s behavior, i.e., samples are concentrated in
globally optimal areas of design (categories 2, 4, 5 and 6 as discussed in Appendix A).
170

So, one can conclude that the MICGA can successfully drive the adaptive sampling
process on MIC surrogates.
The 40 updates of the adaptive sampling algorithm on the MIC Integer surrogate
are plotted in Figure 34. Compared with the MIC Hamming one, Figure 33, the
adaptive sampling on the MIC integer surrogate places less updates than that on the
MIC Hamming one in interesting regions of the domain. The former surrogate invests
too many resources in category 1 that has a worse performance than other disturbed
Branin function categories such as two and four. Also, in the integer metric case, up-
dates are located in useless corners; it does not happen in the Hamming case. Figures
33 and 34 provide evidence that the Hamming metric is more appropriate than the
integer one for building a surrogate of the Branin disturbed problem; similar results
were found in Subsection 5.1.2 while comparing the eﬃciency of MIC surrogates based
on several nominal distances.
 
 
x1
Category 6
Category 5
Category 4
x2
Category 3
Category 2
Category 1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
50
100
150
200
250
300
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
Figure 34: Disturbed Branin Function. Warm-up Size 54. Updates 40. MIC Nominal
Metric Integer.
171

6.3
Adaptive Sampling of the UH60A Hover Shaft Power
In the present example the UH60A hover power consumption is adaptively sampled.
Also, the resulting best sample is compared with the baseline case (given by Tables
2, and 3). The optimal solution is searched by adaptive sampling the MIC surrogate.
The MICGA optimizes the ExI to search for the next update. The baseline value of
the objective function is
shphov = 1901.7hp
The optimization problem is deﬁned by
shphov (θ1, θ2, c, airfoil)
(113)
and subjected to
θ1 ∈[−3.4◦/m, −0.5◦/m]
θ1 ∈R
θ2 ∈[−1.5◦/m, 0.5◦/m]
θ2 ∈R
c
¯c ∈{0.9 +

0, 1
3, 2
3, 1

· 0.85}
airfoil ∈{NACA 0012, ..., SC 1095}
(114)
The adaptive sampling algorithm is run on MIC surrogates with the following
warm-up sampling sizes: 36, 48, 60, 72, 84 and 96.
Table 9 contains the results
of the adaptive sampling runs. The indicators of the results are: the percentage of
function samples in the best 3%, the optimal design (θ1,opt, θ2,opt, copt, Airfoilopt), and
the optimal objective (yopt).
Results in Table 9 demonstrate that the MICGA can drive the EGO algorithm
on a MIC meta-model of the UH60A hover shp. All the runs provide similar optimal
172

Table 9: Solution EGO Runs on UH60A shphov. MIC Surrogate
Warm-up Size
Updates
% in Best 3%
θ∗
1,opt
θ∗
2,opt
c∗
opt
Airfoil∗
opt
yopt
36
5
1
0.3352
0.08208
0
SC 1095
1709.0356
20
0.45
0.3352
0.08208
0
SC 1095
1709.0356
40
0.275
0.3352
0.08208
0
SC 1095
1709.0356
60
0.35
0.2743
0.3454
0
SC 1095
1708.1737
48
5
0.8
0.2106
0.0002473
0
SC 1095
1711.4107
20
0.7
0.229
0.4948
0
SC 1095
1708.1578
40
0.525
0.241
0.4743
0
SC 1095
1708.0969
60
0.3667
0.241
0.4743
0
SC 1095
1708.0969
60
5
1
0.295
0.2788
0
SC 1095
1708.3896
20
0.5
0.2837
0.2942
0
SC 1095
1708.3214
40
0.425
0.2837
0.2942
0
SC 1095
1708.3214
60
0.3833
0.2837
0.2942
0
SC 1095
1708.3214
72
5
1
0.301
0.1717
0
SC 1095
1708.7405
20
0.7
0.2847
0.2591
0
SC 1095
1708.4440
40
0.525
0.2782
0.3248
0
SC 1095
1708.2285
60
0.45
0.2782
0.3248
0
SC 1095
1708.2285
84
5
1
0.2
0.55
0
SC 1095
1708.7854
20
0.7
0.2283
0.4658
0
SC 1095
1708.1822
40
0.525
0.2437
0.4174
0
SC 1095
1708.0962
60
0.4
0.2437
0.4174
0
SC 1095
1708.0962
96
5
1
0.3302
0.097
0
SC 1095
1708.9790
20
0.7
0.3302
0.097
0
SC 1095
1708.9790
40
0.4
0.3302
0.097
0
SC 1095
1708.9790
60
0.35
0.2525
0.3803
0
SC 1095
1708.1316
results, which represent an improvement of around 10% with respect to the baseline
case. A large subset of the updates lands in globally high-performing areas as the
indicator “% in Best 3%” demonstrates. Also, once a good approximation to the
optimal value is obtained, the value of “% in Best 3%” begins to decrease indicating
that the adaptive sampling algorithm starts to explore the unknown parts of the
domain. This is again a typical EGO behavior.
6.3.1
EGO Update Location
Once it has been demonstrated that updates are located on globally good performing
areas instead of on good performing areas of each category, the adaptive sampling
technique is run for 30 updates on MIC Hamming surrogates with several warm-up
173

sampling sizes: 32, 52, 66, 84, and 136. The purpose is to actually plot the updates
and see their location instead of relying on the indicator “% in Best 3%”. Layouts of
the adaptive sampling updates are shown in Figures 35, 36, 37, 38, 39, and 40. It is
worth noticing that the globally best performing areas are designs in c = 0.9¯c, and
airfoils “SC 1095” and “NACA 0012”.
A 32 warm-up sampling plan is adaptively sampled 30 times. Figure 35 shows
the 26 updates at c = 0.9¯c. The remaining updates are two at c = 1.183¯c, and two
at c = 1.75¯c. It is seen that some of the updates explore bad performing categories,
whereas the majority of them are in “SC 1095”, the best performing category.
 
 
x1
sc1095
x2
naca23012
sc2110
naca0012
c = 0.900¯c
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
1700
1800
1900
2000
2100
2200
2300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 35: Adaptive Sampling of the UH60A shphov. Warm-up Size 32. Updates 30.
c = 0.9¯c. MIC Surrogate
For an initial sampling plan of 52 observations, Figures 36 and 37 show the ﬁrst
20 and 30 updates that seat on c = 0.9¯c, respectively. It is interesting to see that
all the 20 ﬁrst updates are at c = 0.9¯c, see Figure 36, i.e., the adaptive sampling
algorithm focuses in a globally good performing area, c = 0.9¯c.
As the number of updates increases, there is less uncertainty in the globally good
174

 
 
x1
sc1095
x2
naca23012
sc2110
naca0012
c = 0.900¯c
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
1700
1800
1900
2000
2100
2200
2300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 36: Adaptive Sampling of the UH60A shphov. Warm-up Size 52. Updates 20.
c = 0.9¯c. MIC Surrogate
 
 
x1
sc1095
x2
naca23012
sc2110
naca0012
c = 0.900¯c
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
1700
1800
1900
2000
2100
2200
2300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 37: Adaptive Sampling of the UH60A shphov. Warm-up Size 52. Updates 30.
c = 0.9¯c. MIC Surrogate
175

performing regions, such as c = 0.9¯c, because of the high density of updates. Conse-
quently, two updates out of the next 10 are in chord values diﬀerent from c = 0.9¯c
(one at c = 1.183¯c and another at c = 1.75¯c). Figure 37 exhibits the 28 updates out
of the 30 that lay on c = 0.9¯c.
19 out of the ﬁrst 20 updates for a MIC surrogate with 66 warm-up samples are
at c = 0.9¯c, as Figure 38 depicts. The remaining update is at c = 1.183¯c.
 
 
x1
sc1095
x2
naca23012
sc2110
naca0012
c = 0.900¯c
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
1700
1800
1900
2000
2100
2200
2300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 38: Adaptive Sampling of the UH60A shphov. Warm-up Size 66. Updates 20.
c = 0.9¯c. MIC Surrogate
Figure 39 exhibits the updates at c = 0.9¯c starting with 84 warm-up samples. It
is seen that, out of the ﬁrst 10 updates, nine are in the smallest chord. The remaining
one is at c = 1.183¯c.
The last adaptive sampling algorithm is run on a MIC Hamming surrogate starting
with 136 warm-up observations. Figure 40 shows that the 10 ﬁrst updates are all at
c = 0.9¯c. There exist enough warm-up points to disregard bad performing areas and
fully focus on exploitation.
176

 
 
x1
sc1095
x2
naca23012
sc2110
naca0012
c = 0.900¯c
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
1700
1800
1900
2000
2100
2200
2300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 39: Adaptive Sampling of the UH60A shphov. Warm-up Size 84. Updates 10.
c = 0.9¯c. MIC Surrogate
 
 
x1
sc1095
x2
naca23012
sc2110
naca0012
c = 0.900¯c
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
1700
1800
1900
2000
2100
2200
2300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 40: Adaptive Sampling of the UH60A shphov. Warm-up Size 136. Updates
10. c = 0.9¯c. MIC Surrogate
177

In all the shown cases, the EGO type behavior occurs: the samples are concen-
trated in the best performing areas across the whole design space, which are c = 0.9¯c,
and airfoils “SC 1095” and “NACA 0012”.
178

6.4
Study of Mutation Strategies
Once new nominal distance measures have been tested for building MIC surrogates,
see Chapter 5, new questions come up: could the distance metrics be eﬃciently used
in the mutation functions of MICGA when optimizing ExI on MIC domains? which
combination of mutation strategies for all types of variables provides best optimization
results?
Herein, MICGA, presented in Subsection 3.6.2, maximizes the ExI over the MIC
design space for obtaining the next update point. Normally, expected improvement
landscapes have a characteristic multi-modality with hills between points already sam-
pled as pointed out by Jones [99]. These characteristics make possible to extrapolate
the results of ExI optimization tests on canonical objectives to other objectives.
As explained in Subsection 3.6.3, several possibilities can be chosen for the cat-
egorical mutation function of the MICGA: uniform, inverse distance, and maximum
entropy. The following overall mutation strategies for each type of variable are tested:
1) Normal, Geometrical, and Uniform. Normal, geometrical, and uniform distri-
butions for continuous, integer, and categorical variables, respectively.
2) Normal, Geometrical, and Inverse Distance. Normal, geometrical, and inverse
distance distributions for continuous, integer, and categorical variables, respec-
tively. Subsection 3.6.3 explains the inverse distance distribution for categorical
variables.
3) Normal, Geometrical, and Maximum Entropy. Normal, geometrical and max-
imum entropy distributions for continuous, integer, and categorical variables,
respectively. Subsection 3.6.3 explains the maximum entropy distribution for
categorical variables.
4) Uniform, Uniform, and Uniform. Uniform distributions for all kinds of variables.
179

5) Uniform, Uniform, and Inverse Distance. Uniform distributions for continuous,
and integer variables; and inverse distance distribution for categorical ones.
6) Uniform, Uniform, and Maximum Entropy. Uniform distributions for contin-
uous, and integer variables; and maximum entropy distribution for categorical
ones.
The success of the method is given in terms of the error of the maximization
process, and the generation at which the last improvement larger than 1% is achieved
in the optimization process, deﬁned in Subsection 3.6.3.3. These parameters express
the precision of the solution, and the eﬃciency of each overall mutation strategy in
the optimization process.
The ultimate goal of this Subsection is to ﬁgure out which mutation strategy is
the most appropriate for the maximization of the ExI. However, before dealing with
it, an study of the performance of each mutation strategy in the maximization of
a canonical function is carried out. The canonical function chosen is the disturbed
Branin function, see Appendix A. Five distinct disturbed Branin functions are mini-
mized with the MICGA 150 times. This is done to marginalize the intrinsic random
nature of stochastic search. The success parameters are presented in Figures 41, and
42.
According to Figures 41 and 42 normal and geometrical distributions seem gener-
ally to provide better results than constant distributions for continuous and integer
variables, respectively, i.e., the mean and standard deviation of the success parame-
ters are generally smaller for normal and geometrical distributions than for uniform
distributions. Also, it is noticeable that the diﬀerence between the mutation strategies
with diﬀerent categorical mutations are not large.
The next step is to study the mutation strategies in the maximization of a ExI
on MIC domains. Several ExI landscapes have been generated at diﬀerent stages of
180

 
 
Unif, Unif, M Entr
Unif, Unif, Inv D
Unif, Unif, Unif
Norm, Geo, M Entr
Norm, Geo, Inv D
Norm, Geo, Unif
Std
Mean
P1
P2
P3
P4
P5
P1
P2
P3
P4
P5
0
2
4
6
8
10
0
2
4
6
8
Figure 41: Statistics of Solution Error for Five Disturbed Branin Problems. MICGA
Mutation Study
 
 
Unif, Unif, M Entr
Unif, Unif, Inv D
Unif, Unif, Unif
Norm, Geo, M Entr
Norm, Geo, Inv D
Norm, Geo, Unif
Std
Mean
P1
P2
P3
P4
P5
P1
P2
P3
P4
P5
0
5
10
15
20
25
0
5
10
15
20
Figure 42: Statistics of Generation of the Last Improvement Larger than 1% for Five
Disturbed Branin Problems. MICGA Mutation Study
181

the adaptive sampling process on the disturbed Branin function plotted in Figure
20. Again, the optimization is run 150 times to marginalize the randomness of the
MICGA.
Figures 43 and 44 exhibit the error of the maximization process and the genera-
tions at which the last improvement larger than 1% is achieved in the optimization
process, respectively.
 
 
Unif, Unif, M Entr
Unif, Unif, Inv D
Unif, Unif, Unif
Norm, Geo, M Entr
Norm, Geo, Inv D
Norm, Geo, Unif
Std
Mean
5Up
10Up
15Up
20Up
5Up
10Up
15Up
20Up
0
0.05
0.1
0.15
0.2
0
0.05
0.1
0.15
0.2
Figure 43: Statistics of Solution Error for ExI Optimization with Several Updates.
MICGA Mutation Study
Results in Figure 43 and 44 reveal again that normal and geometrical mutation
strategies for continuous and integer variables perform normally slightly better than
uniform mutation strategies in terms of both success parameters. Also, little perfor-
mance diﬀerences are found across the categorical mutation strategies, as experienced
in the direct optimization of the disturbed Branin function.
182

 
 
Unif, Unif, M Entr
Unif, Unif, Inv D
Unif, Unif, Unif
Norm, Geo, M Entr
Norm, Geo, Inv D
Norm, Geo, Unif
Std
Mean
5Up
10Up
15Up
20Up
5Up
10Up
15Up
20Up
0
5
10
15
20
0
5
10
15
20
Figure 44: Statistics of Generation of the Last Improvement Larger than 1% for ExI
Optimization with Several Updates. MICGA Mutation Study
183

CHAPTER VII
ECMF EXPERIMENTS: NEW CONCEPT AND OLD
TRAINING SIZE
One of the goals of this research is to leverage similar trends between concepts to en-
hance the current poor strategy of ﬁtting a surrogate independently for each concept.
Two Gaussian surrogates are brought in this Chapter:
ECMF Surrogates Surrogates that are developed to leverage similar trends on in-
crementally evolved concepts. A multi-ﬁdelity framework reuses observations of
previously sampled concepts when building a new concept surrogate, see Section
3.5.
Mono-ﬁdelity Surrogates It is the current state-of-the-art method. A surrogate is
ﬁtted for the new concept with no reuse of observations from previous concepts.
It is expected that the multi-ﬁdelity surrogate provides better results than the
independent surrogate for some range of the training set sizes of concept 1 and 2. A
case study is carried out to compare concept 2 ECMF surrogates with concept 2 mono-
ﬁdelity surrogates for several sizes of the training sets. In order to take advantage
of the similar trends, concept 1 has to be already sampled, so a concept 1 surrogate
with certain accuracy is available. Thus, the size of the concept 1 training data-set
must be chosen above a certain size to assure that a precise concept 1 surrogate is
available.
A canonical set of concepts is tested to see the eﬃciency of the proposed ECMF
method. The canonical set of functions is the Michalewicz one, which is a noise-free
set of functions. Appendix B explains in more detail the construction of this canonical
184

set of functions. Then, a test on a more practical scenario is done: the UH60A with
fenestron hover power when observations from the conventional tail UH60A hover
power are available. Both hover power simulations are noisy and computationally
intense functions.
As it was mentioned in Section 1.7, tests on computationally
expensive models do not help to better interpret the canonical test results or support
the research hypothesis or predictions; however, they support the practicality of the
application.
The comparison of the ECMF surrogate with the current state-of-the-art is made
in terms of the success indicators introduced in Subsection 3.5.1: rms of the “stan-
dardized validation error”, and rms of the “standardized validation residual”, see
Equations 84, and 85, respectively.
7.1
Michalewicz Canonical Function
7.1.1
Inﬂuence of New Concept Training Size on Performance
The concept 1 Michalewicz canonical function has one independent variable; its land-
scapes can be seen in Figure 76 in Appendix B. An old concept training set of seven
observations produces a decent old concept surrogate. The concept 2 training dataset
studied in this Subsection ranges in the interval N2,tr.set ∈[6, 30]. The success indica-
tors are: concept 2 rms of the “standardized validation error”, and concept 2 rms of
the “standardized validation residual” are plotted in Figures 45 and 46, respectively.
Both success indicators show that, in the small range of N2,tr.set, the proposed
concept 2 ECMF surrogate outperforms the concept 2 surrogate with no reuse of
information (mono-ﬁdelity surrogate). The range in which the ECMF surrogate pro-
vides better rms of the “standardized validation error” is smaller than that range
in the rms of the “standardized validation residual” case. It is worth realizing that
there is a value of N2,tr.set above which the mono-ﬁdelity surrogate outperforms the
ECMF one; thus, the observations from previous concepts must not be used beyond
185

 
 
ECMF C1 7
Mono-ﬁdelity
Average Rms of Stand Error
Size of the New Concept Training Set
101
102
10−2
10−1
Figure 45: Concept 2 Surrogate “Standardized Validation Error”. ECMF N1,tr.set = 7
vs Mono-Fidelity Surrogate. Michalewicz Function.
 
 
ECMF C1 7
Mono-ﬁdelity
Average Rms of Stand Res
Size of the New Concept Training Set
101
102
0
1
2
3
4
5
6
7
8
9
10
Figure 46:
Concept 2 Surrogate “Standardized Validation Residual”.
ECMF
N1,tr.set = 7 vs Mono-Fidelity Surrogate. Michalewicz Function.
186

Table 10: Values of Constant α for the Fitting Curves of Rms of the “Standardized
Validation Error”. Mono-ﬁdelity vs ECMF Surrogate. Michalewicz Function.
Low N2,tr.set
Large N2,tr.set
Mono-ﬁdelity Surr.
0.9231
2.0357
ECMF N1,tr.set = 7 Surr.
0.7218
1.1092
this value of N2,tr.set.
The gain in performance of the mono-ﬁdelity surrogate with respect to the ECMF
one is quantiﬁed by the values of the constant α in the ﬁtting curves of the rms of
the “standardized validation error”, similarly as in Subsection 5.1.1 (see Equation
105). Table 10 contains the values of α. It is seen that the mono-ﬁdelity surrogate
converges more quickly to zero than the ECMF with N1,tr.set = 7 as the size of the
new concept training set. Also, it is seen that if two ﬁtting curves are used (one for
small and another for large N2,tr.set), the gain in performance of the mono-ﬁdelity
surrogate in large training sets is higher than in the small ones (see Table 10).
7.1.2
Inﬂuence of Previous Concept Training Size on Performance
Also, it is interesting to see the inﬂuence of the N1,tr.set on the ECMF surrogate
performance. The studied range for N1,tr.set is [5, 11]. Figure 47 exhibits the rms
of the “standardized validation error” for ECMF surrogates with several values of
N1,tr.set.
For small values of N2,tr.set, the success indicator performance increases as N1,tr.set
does. It is noticeable that the rms of the “standardized validation error” for ECMF
surrogates with N1,tr.set = 9 and N1,tr.set = 11 are quite similar in this small range of
N2,tr.set.
On the high range of N2,tr.set, it seems to be a tendency that as N1,tr.set increases,
the surrogate rms of the “standardized validation error” becomes larger. Figure 47
demonstrates that around N2,tr.set ≈100, the ECMF surrogate with N1,tr.set = 9 out-
performs the ECMF surrogate with N1,tr.set = 11. Thus, the rms of the “standardized
187

validation error” dependence on N1,tr.set seems to reverse for large values of N2,tr.set.
It appears to reﬂect that, when the new concept training set is large, the re-use of
observations matters less. It is because the new concept sampling plan provides the
necessary features to ﬁt an accurate surrogate.
Also, as the number of previous concept observations, N1,tr.set, increases, the con-
cept 1 (low-ﬁdelity) surrogate captures features beyond common trends. This high
frequency features can mislead the new concept ECMF surrogate, specially for high
N2,tr.set where new concept high frequency information is already captured by the new
concept sampling plan.
 
 
ECMF C1 11
ECMF C1 9
ECMF C1 7
ECMF C1 5
Average Rms of Stand Error
Size of the New Concept Training Set
101
102
10−2
10−1
Figure 47: Concept 2 Surrogate “Standardized Validation Error”. ECMF Several
N1,tr.set. Michalewicz Function.
Table 11 contains the values of α for ECMF surrogates built with several values
of N1,tr.set. It is seen that all the ECMF surrogates gain performance with more or
less similar speed. Looking carefully, one can see the previously discussed rms of the
error dependence on N1,tr.set for the low and high ranges of N2,tr.set.
188

Table 11: Values of Constant α for the Fitting Curves of the Rms of the “Standard-
ized Validation Error”. All ECMF Surrogates. Michalewicz Function
Low N2,tr.set
Large N2,tr.set
ECMF N1,tr.set = 5
0.5290
1.4988
ECMF N1,tr.set = 7
0.7218
1.1092
ECMF N1,tr.set = 9
0.7517
1.5479
ECMF N1,tr.set = 11
0.9773
0.7891
189

7.2
UH60A with Fenestron Tail Hover Shaft Power. Screened
Domain
Before testing the ECMF surrogate on a UH60A model with a large design space,
some tests are done in a screened design space to better understand the ECMF sur-
rogate performance on noisy functions with manageable design spaces. The objective
function is the power consumption of the whole UH60A helicopter with a fenestron
tail, denoted by shpfen
hov. The previous or old concept is the UH60A with a conventional
tail.
The design variables are: the inner blade twist (continuous variable), the outer
blade twist (continuous variable), the number of fan blades (discrete-quantitative
variable), and the chord length (discrete-quantitative variable). One can see that the
new concept design variables are the ones obtained in the screening done in Subsection
5.2.1, plus the number of fan blades. This last variable is included to make the new
concept design space diﬀerent from that of the old concept.
The objective function is summarized as follows:
shpfen
hov (θ1, θ2, Nb,tr, c)
(115)
subjected to
θ1 ∈[−3.5◦/m, −0.5◦/m]
θ1 ∈R
θ2 ∈[−1.75◦/m, 0.5◦/m]
θ2 ∈R
Nb,tr ∈7, 8, 9
c
¯c ∈0.75 +

0, 1
3, 2
3, 1

· 0.5
(116)
where ¯c = 1.73ft is the baseline chord.
The remaining parameters are W = 16994lb (CT = 5.99 · 10−3), R = 26.83ft,
Ω= 27.0063 rad
sec, and rcut-oﬀ
R
= 0.047. The main rotor blade section is “NACA 0012”.
190

The numerical and iterative nature of the shpfen
hov make it a noisy function. Thus,
a regression model is ﬁtted to plot its landscapes. Figures 48, 49, 50, and 51 show
the regression of the objective function for the feasible chords. The regression of the
function looks less multi-modal than the Michalewicz function studied in Section 7.1.
However, it is worth realizing that the noise is not seen in Figures 48, 49, 50, and 51
because they are regressing meta-models, but the function is noisy.
 
 
θ2
θ1
9 Fan Blades
8 Fan Blades
7 Fan Blades
UH60A Fenestron
Naca 0012
c = 0.750¯c
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1700
1750
1800
1850
1900
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 48: UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 0.750¯c
The previous concept from which observations are reused is the UH60A with
conventional tail shpconv
hov . It is deﬁned in Equations 109, 110, and 111 with the only
change that just the airfoil “NACA 0012” is allowed.
Notice that each concept design space for the power consumption (shpfen
hov and
shpconv
hov ) is diﬀerent. The UH60A with the conventional tail has one less design vari-
able, Nb,tr, than the UH60A with fenestron. Therefore, when building the fenestron
concept ECMF surrogate, the previous concept behavior along Nb,tr is assumed to be
constant, as explained in Section 3.5.
191

 
 
θ2
θ1
9 Fan Blades
8 Fan Blades
7 Fan Blades
UH60A Fenestron
Naca 0012
c = 0.917¯c
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1750
1800
1850
1900
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 49: UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 0.917¯c
 
 
θ2
θ1
9 Fan Blades
8 Fan Blades
7 Fan Blades
UH60A Fenestron
Naca 0012
c = 1.083¯c
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1840
1860
1880
1900
1920
1940
1960
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 50: UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 1.083¯c
The landscape of shpconv
hov were shown in Figures 26, 27, 28, and 29. One can see
that the power consumptions of the two concepts experience similar trends along their
192

 
 
θ2
θ1
9 Fan Blades
8 Fan Blades
7 Fan Blades
UH60A Fenestron
Naca 0012
c = 1.250¯c
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1950
1960
1970
1980
1990
2000
2010
2020
2030
2040
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 51: UH60A with Fenestron Total shpfen
hov versus θ1 and θ2. c = 1.250¯c
common design variables.
7.2.1
Inﬂuence of New Concept Training Size on Performance
The goal of this Subsection is to see the eﬃciency of the ECMF surrogate compared
with that of the mono-ﬁdelity surrogate in a range of the new concept training set
size, speciﬁcally from 24 to 288 points. Since both meta-models are probabilistic,
two performance indicators are used for the comparison: rms of the concept 2 surro-
gate “standardized validation error”, and rms of concept 2 surrogate “standardized
validation residual” given by Equations 84, and 85, respectively. A set of 768 val-
idation points are evaluated to assess the performance indicators. The number of
observations from the previous concept is taken to be 20. Figures 52 and 53 show the
rms of the concept 2 surrogate “standardized validation error”, and rms of concept 2
surrogate “standardized validation residual”, respectively.
Again curves are ﬁtted to try to understand the tendency of the results for each
193

surrogate. For the rms of the concept 2 “standardized validation error” and “stan-
dardized validation residual”, the ﬁtting curves chosen are Equation 105 and 106,
respectively, see Section 5.1.
 
 
ECMF C1 20
Mono-ﬁdelity
Average Rms of Stand Error
Size of the New Concept Training Set
101
102
10−3
10−2
10−1
Figure 52: Concept 2 Surrogate “Standardized Validation Error”. ECMF vs Mono-
Fidelity Surrogate. UH60A with Fenestron shpfen
hov
Figure 52 shows that the fenestron concept ECMF surrogate (using 20 previous
concept observations) produces a more accurate surrogate than the mono-ﬁdelity
surrogate for the small range of new concept sampling sizes.
The reason is that, for low number of new concept points, observations from the
previous concept complement the incomplete information from the new concept sam-
pling plan. The ECMF surrogate successfully reuses observations from the previous
concept, UH60A with conventional tail, enabling more accurate surrogates.
However, as the new concept training set size becomes larger, the mono-ﬁdelity
surrogate gains in performance until it produces a similar accuracy surrogate as the
ECMF one; this occurs with new concept training sets of approximately 100 points.
It appears to reﬂect that, when the new concept training set is large, the re-use of
observations matters less. It is because the new concept sampling plan provides the
necessary features to ﬁt a good surrogate, i.e., previous concept observations could
194

Table 12: Values of Constant α for the Fitting Curves of the Rms of the “Standard-
ized Validation Error”. Mono vs ECMF Surrogate. UH60A with Fenestron shpfen
hov
Screened Domain
Low N2,tr.set
Large N2,tr.set
Mono-ﬁdelity
1.1256
0.3647
ECMF 20
0.7308
0.2927
be useless when the new concept training set is large enough to properly capture the
new concept landscape.
The gain in performance of the mono-ﬁdelity surrogate with respect to the ECMF
surrogate is again quantiﬁed by the values of the constant α in the ﬁtting curves of
the rms of the error , see Equation 105. Table 12 contains the values of αs. The
mono-ﬁdelity surrogate built with a few new concept observations converges more
quickly than the ECMF one. For large N2,tr.set, both surrogates converge at a similar
speed.
 
 
ECMF C1 20
Mono-ﬁdelity
Average Rms of Stand Res
Size of the New Concept Training Set
101
102
0
1
2
3
4
5
6
7
8
9
10
Figure 53: Concept 2 Surrogate “Standardized Validation Residual”. UH60A with
Fenestron shpfen
hov
Figure 53 exhibits the rms of the “standardized validation residual” of the UH60A
shpfen
hov for the mono-ﬁdelity and the ECMF surrogate. For both surrogates, the ﬁtted
curve is close to one. The residual of the ECMF surrogate built with 20 conventional
tail UH60A observations is slightly oﬀthe ideal value of one.
195

The “standardized validation error” indicator demonstrates that the ECMF sur-
rogate outperforms the state-of-the-art for small new concept training set sizes when
applied on the noisy shpfen
hov of the UH60A with fenestron tail.
7.2.2
Inﬂuence of Previous Concept Training Size on Performance
The inﬂuence of the number of previous concept observations, N1,tr.set, in the perfor-
mance of the ECMF surrogate is studied in this Subsection. A similar study was done
in Subsection 7.1.2 for the noise-free function “Michalewicz”. For the noisy shpfen
hov,
the UH60A with conventional tail training sizes tested, N1,tr.set, are 12 to 20, 28, and
36.
Figure 54 exhibits shpfen
hov rms of the “standardized validation error” for ECMF
surrogates built with several values of N1,tr.set. This ﬁgure exposes similar results as
the ones obtained in the free-noise Michalewicz function studied in Section 7.1.
For small values of N2,tr.set, the rms of the error decreases as N1,tr.set gets larger.
However, this tendency reverses for the high range of N2,tr.set, i.e., there exists a
tendency that, as N1,tr.set increases, the performance of the surrogate according to
the rms of the error is poorer. The ECMF surrogate with N1,tr.set = 36 has a poorer
performance than the one of the ECMF surrogate with N1,tr.set = 28. Same behavior
was seen in Subsection 7.1.2.
The reason could be that, as the number of previous concept observations, N1,tr.set,
increases, the low-ﬁdelity surrogate captures features beyond common trends. This
high frequency features can mislead the new concept ECMF surrogate, specially for
high values of N2,tr.set where new concept high frequency information is already cap-
tured. It results in a decrease in the ECMF performance as old concept observations
are added to the ECMF (it occurs for a large N1,tr.set). When there are many old
concept observations at the designer’s disposal, a possible remedy is to build the
ECMF surrogate only with a subset of the old concept observations that captures
196

only similar trends.
 
 
ECMF C1 36
ECMF C1 28
ECMF C1 20
ECMF C1 12
Average Rms of Stand Error
Size of the New Concept Training Set
101
102
10−2
Figure 54: Concept 2 Surrogate “Standardized Validation Error”. ECMF Several
N1,tr.set. UH60A with Fenestron shpfen
hov
Table 13 contains the values of αs for all the ECMF surrogates. All the ECMF
surrogates gain performance at the same speed for large N2,tr.set. It is worth men-
tioning that for ECMF surrogates with N1,tr.set = 36 (the largest previous concept
training set) the gain in performance is slower than for other ECMF surrogates with
lower N1,tr.set as shown in Table 13, and Figure 54. It reﬂects that at low N2,tr.set
ECMF surrogates with high N1,tr.set are better than their counterpart ECMF with
low N1,tr.set; however, this tendency reverses when N2,tr.set becomes larger.
197

Table 13: Values of Constant α for the Fitting Curves of the Rms of the “Stan-
dardized Validation Error”. All ECMF Surrogates. UH60A with Fenestron shpfen
hov
Screened Domain
Low N2,tr.set
Large N2,tr.set
ECMF N1,tr.set = 12
1.0718
0.2975
ECMF N1,tr.set = 20
0.7308
0.2927
ECMF N1,tr.set = 28
0.6957
0.3719
ECMF N1,tr.set = 36
0.5025
0.3813
198

CHAPTER VIII
DEMONSTRATING MIC ADAPTIVE SAMPLING ON
ROTOR-CRAFT PRACTICAL SCENARIO
In this Chapter, the multi-objective adaptive sampling of MIC surrogates is carried
out. The purpose is to understand the characteristics of adaptive sampling techniques
applied on surrogates that leverage similar trends across categories. The adaptive
sampling algorithm is applied to optimize the UH60A power consumption at two
ﬂight conditions. The optimization is performed in two design spaces with diﬀerent
dimensionalities.
8.1
Multi-objective Adaptive Sampling of the UH60A Shaft
Power. Screened Domain
Before testing the UH60A model on a large design space, some tests are done in a
minor design space to better understand the characteristics of the adaptive sampling
algorithm on MIC surrogates.
The example in these Sections aims to reduce the
UH60A power consumption at hover and forward ﬂight with respect to the baseline
(for further details of the model, see Section 4.2). The baseline case is presented in
Tables 2, and 3, its objective function values are
shphov = 1901.7hp
shpfwd = 1673.9hp
.
The design variables are the ones obtained in the screening process, see Subsection
5.2.1: the inner blade twist (continuous variable), the outer blade twist (continuous
variable), the chord length (discrete-quantitative variable), and the type of airfoil
(categorical variable). Four possible airfoils are available: “NACA 0012”, “SC 2110”,
199

“NACA 23012”, and “SC 1095”. The objective function is summarized as follows:
shphov (θ1, θ2, c, airfoil)
shpfwd (θ1, θ2, c, airfoil)
(117)
subjected to
θ1 ∈[−3.4◦/m, −0.5◦/m]
θ1 ∈R
θ2 ∈[−1.5◦/m, 0.5◦/m]
θ2 ∈R
c
¯c ∈{0.9 +

0, 1
3, 2
3, 1

· 0.85}
airfoil ∈{NACA 0012, ..., SC 1095}
(118)
The remaining parameters are W = 16994lb (CT = 5.99 · 10−3), R = 26.83ft,
Ω= 27.0063 rad
sec, and rcut-oﬀ
R
= 0.047. Figures 26, 27, 28, and 29 show a regressive
meta-model of the hover power consumption for the feasible chords. A meta-model
is shown instead of the real function because of the computational expense of the
model. The original function is noisy due to the numerical and iterative nature of the
software FLIGHTLAB.
In order to ﬁnd the optimal solutions for both ﬂight conditions, multi-objective
optimization is applied. Optimal solutions are given by Pareto optimal points, a set
of points that include designs which are so optimized that in order to improve one
goal of any of the Pareto members, its performance in at least one of the other goals
has to diminish.
First, a good approximation of the Pareto front is calculated by the well-known
evolutionary multi-objective algorithm NSGA-II developed by Deb [43]. This non-
dominated set is considered herein as the “real” Pareto front. In order to obtain it,
the expensive helicopter models have been evaluated 7280 times. The computational
intensity of the objective functions limits the number of evaluations and, consequently,
the quality of the assessment of the real Pareto front.
200

The Pareto front obtained with the NSGA-II algorithm is plotted in Figure 55,
and 56. The Pareto front is formed of two disconnected fronts as Figure 55 shows.
The larger front corresponds to the airfoil “SC 1095”, whereas the small one belongs
to the airfoil “NACA 0012”. Also, the obtained Pareto front from this optimization
study is better performing in the two objectives than the baseline, as shown in Figure
55.
The EGO algorithm is applied to two surrogates to determine the non-dominated
set that approaches the Pareto front. These surrogates are the following:
MIC Surrogates Surrogates that are developed to leverage similar trends across
categories, see Section 3.4.
Independent Surrogates It is the current state-of-the-art method. A surrogate is
ﬁtted for each category. The adaptive sampling algorithm is applied simultane-
ously to these independent surrogates.
The resulting Pareto fronts from both methods are compared with the “real”
Pareto front obtained from the evolutionary multi-objective genetic algorithm NSGA-
II.
The Pareto fronts after ten updates of the EGO algorithm on surrogates with
warm-up training sets of 38 expensive observations are plotted in Figure 55. It shows
that after ten updates, the EGO algorithm applied on the MIC meta-model estimates
a better approximation to the “real” UH60A Pareto front than the EGO algorithm
applied to independent surrogates simultaneously.
After 25 updates, see Figure 56, both EGO processes approach better the “real”
Pareto front, but there are twelve non-dominated points in the adaptive sampling
on the MIC surrogate over the six in the case of simultaneous adaptive sampling on
independent surrogates.
201

 
 
Baseline
Pareto
MIC 10 Updates
Indep 10 Updates
shpfwd (hp)
shphov (hp)
1750
1800
1850
1900
1500
1550
1600
1650
1700
Figure 55: Non-Dominated Set Obtained from EGO Algorithm Applied on MIC and
Independent Surrogates. UH60A shp. Warm-Up Size 38
 
 
Pareto
MIC 25 Updates
MIC 10 Updates
Indep 25 Updates
Indep 10 Updates
shpfwd (hp)
shphov (hp)
1710
1730
1750
1480
1500
1520
1540
1560
1580
Figure 56: Evolution Non-Dominated Set Obtained from EGO Algorithm Applied
on MIC and Independent Surrogates. UH60A shp. Warm-Up Size 38
In order to more formally quantify the quality of the non-dominated set for each
adaptive sampling method, two performance indicators are employed: the number of
points in the non-dominated set, and the average distance between the non-dominated
set and “real” Pareto front, see Equation 88. For more details, see Subsection 3.6.1.
These two performance indicators are assessed for warm-up datasets of 36, 48, and
66. Figures 57, 58, 59, 60, 61, and 62 show the results.
202

 
 
MIC
Indep
Average Distantce to Pareto Set
Updates
0
5
10
15
20
25
30
35
40
0
10
20
30
40
50
60
70
80
90
100
Figure 57: Average Distance to Pareto Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 36. Updates 40
 
 
MIC
Indep
Number Points Non-Dominated Set
Updates
0
5
10
15
20
25
30
35
40
2
4
6
8
10
12
14
16
18
20
Figure 58: Number Points Non-Dominated Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 36. Updates 40
The EGO runs on a starting surrogate with 36 warm-up observations, results are
in Figures 57 and 58, show evidence that the sampling on MIC surrogates provide a
much better Pareto approximation for both indicators: the average distance to the
“real” Pareto front for MIC surrogates is ten times smaller than in the independent
case at ﬁve updates; and the number of points in the non-dominated set is around
two times larger in the MIC case throughout the whole updating process.
203

 
 
MIC
Indep
Average Distantce to Pareto Set
Updates
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
35
40
45
50
Figure 59: Average Distance to Pareto Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 48. Updates 40
 
 
MIC
Indep
Number Points Non-Dominated Set
Updates
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
Figure 60: Number Points Non-Dominated Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 48. Updates 40
When the warm-up size is increased to 48, see Figures 59 and 60, results are
similar to the ones obtained in the smaller warm-up dataset case. Even though there
are more non-dominated points in the independent surrogate case after 23 updates,
the average distance to the “real” Pareto set for the independent surrogate case is
much larger than for the MIC case throughout the whole updating process (around
30 times larger above 23 updates).
204

 
 
MIC
Indep
Average Distantce to Pareto Set
Updates
0
5
10
15
20
25
30
35
40
0
1
2
3
4
5
6
7
8
9
10
Figure 61: Average Distance to Pareto Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 66. Updates 40
 
 
MIC
Indep
Number Points Non-Dominated Set
Updates
0
5
10
15
20
25
30
35
40
0
5
10
15
20
25
30
Figure 62: Number Points Non-Dominated Set UH60A shp. EGO Algorithm Applied
on MIC and Independent Surrogates. Warm-Up Size 66. Updates 40
The ﬁnal warm-up sets have 66 points.
Figure 61 illustrates that the average
distance is slightly smaller for the MIC case in the large part of the studied range.
However, when it comes to the number of non-dominated points (see Figure 62) the
simultaneous EGO on independent surrogates outperforms the EGO on the MIC
surrogate beyond 4 updates.
205

When calculating UH60A Pareto front for hover and forward ﬂight, results demon-
strate that EGO algorithm applied on MIC surrogates are more eﬃcient than on
independent surrogates for small warm-up training sets. As happened with the sur-
rogate eﬃciency, the performance of EGO algorithms on MIC surrogates degrades
with respect to that of the EGO applied simultaneously on independent surrogates
when the training set becomes large.
206

8.2
Multi-objective Adaptive Sampling of the UH60A Shaft
Power. Full Domain
The example in this Section aims to reduce the UH60A power consumption in hover
and forward ﬂight even further than in Section 8.1. In order to ﬁnd even more optimal
solutions for both ﬂight conditions, multi-objective adaptive sampling is applied in
a larger design space. The MIC surrogate is used. The new variables are: the main
rotor radial position where the twist changes rtw, the tail rotor radius Rtr, and the
tail rotor chord ctr.
The objective functions to optimize are
shphov (θ1, θ2, rtw, Rtr, ctr, c, airfoil)
shpfwd (θ1, θ2, rtw, Rtr, ctr, c, airfoil)
(119)
subjected to the full domain constraints given by
θ1 ∈[−3.4◦/m, −0.5◦/m]
θ1 ∈R
θ2 ∈[−1.5◦/m, 0◦/m]
θ2 ∈R
rtw
R ∈[0.72, 0.85]
rtw
R ∈R
Rtr
¯
Rtr
∈[0.98, 1.18]
Rtr ∈R
ctr
¯ctr
∈[0.93, 1.11]
ctr ∈R
c
¯c ∈{0.9 +

0, 1
3, 2
3, 1

· 0.85}
airfoil ∈{NACA 0012, ..., SC 1095}
(120)
where ¯
Rtr, ¯ctr, ¯c are the baseline values, see Table 3.
The starting MIC surrogate warm-up dataset has 1000 observations. Figure 63
contains the Pareto front of the full domain (seven variables) after 600 updates,
Pareto front of the reduced domain (four variables), and the baseline values. The
207

Pareto front of the full domain dominates the Pareto front of the reduced domain
as expected. The number of points in the non-dominated set is 200, one third of
the number of updates. A close-up of the Pareto front is shown in Figure 64. It is
noticeable that the Pareto front corresponding to the airfoil “NACA 0012” is much
larger in the full domain than in the domain with lower dimensionality.
 
 
Pareto 7 Var.
Baseline
Pareto 4 Var.
shpfwd
shphov
1700
1750
1800
1850
1900
1450
1500
1550
1600
1650
Figure 63: Pareto Fronts of the UH60A with Conventional Tail for the Large and
Screened Domain. EGO Algorithm on MIC Surrogates
208

 
 
Pareto 7 Var.
Pareto 4 Var.
shpfwd
shphov
1700
1710
1720
1730
1740
1750
1760
1460
1480
1500
1520
1540
1560
1580
Figure 64: Close-Up of the Pareto Fronts of the UH60A with Conventional Tail for
the Large and Screened Domain. EGO Algorithm on MIC Surrogates.
209

CHAPTER IX
DEMONSTRATING ECMF ADAPTIVE SAMPLING ON
ROTOR-CRAFT PRACTICAL SCENARIO
In this Chapter, the multi-objective adaptive sampling of ECMF surrogates is carried
out. The one-objective optimization of the ECMF surrogate is skipped because it is
not a technical challenge. This is because the ECMF is a stochastic surrogate whose
domain can be made of continuous, integer and/or categorical. The most demanding
scenario is the adaptive sampling of a meta-model with a mixed-integer-categorical
domain that was already carried out in Chapter 6. Thus, this work moves directly to
practical conceptual design applications without looking at canonical problems.
The research purpose of this Chapter is to understand the characteristics of the
adaptive sampling process on the ECMF surrogates; thus, for simplicity’s sake the
design domain is made of just continuous and discrete-quantitative variables but
not categorical ones. The idea is to avoid mixing the two diﬀerent methodologies
that are present in this work: the cross-use of intense observations across categorical
alternatives and the reuse of intense observations from a previous concept.
The adaptive sampling algorithm is applied to optimize the UH60A with fenestron
tail power consumption at two ﬂight conditions. The optimization is performed in
two design spaces with diﬀerent dimensionalities.
9.1
Multi-objective Adaptive Sampling of the UH60A with
Fenestron Shaft Power. Screened Domain
The example in these Sections aims to ﬁnd optimal UH60A power consumption in
hover and forward ﬂight for the fenestron conﬁguration case. Optimal designs of the
UH60A fenestron conﬁguration are compared with baseline values and the optimal
210

designs of the UH60A with regular tail conﬁguration (explored in Chapter 8). The
baseline case is presented in Tables 2, and 3. As proceeded in previous sections, the
optimal solution is given in terms of Pareto fronts. The baseline objective function
values are
shphov = 1901.7hp
shpfwd = 1673.9hp
The objective functions to optimize are given by Equations 121
shpfen
hov (θ1, θ2, Nb,tr, c)
shpfen
fwd (θ1, θ2, Nb,tr, c)
(121)
subjected to constraints given by Equations 122.
θ1 ∈[−3.4◦/m, −0.5◦/m] ∈R
θ2 ∈[−1.5◦/m, 0.5◦/m] ∈R
Nb,tr ∈7, 8, 9
c
¯c ∈0.9 +

0, 1
3, 2
3, 1

0.85
(122)
The remaining parameters are W = 16994lb (CT = 5.99 · 10−3), R = 26.83ft,
Ω= 27.0063 rad
sec, and rcut-oﬀ
R
= 0.047. The main rotor blade section is “SC 1095”.
The previous concept that will help to capture the functions shpfen
hov and shpfen
fwd is
the UH60A with conventional tail. Its corresponding objectives shpconv
hov and shpconv
fwd
are deﬁned in Equations 117; and subjected to Equations 118 with the only diﬀerence
that the airfoil “SC 1095” is the only one available for the designer.
Notice that the design space of the two concepts,

shpfen
hov, shpfen
fwd

and
 shpconv
hov , shpconv
fwd

,
are diﬀerent. The UH60A with the conventional tail has one less design variable than
the UH60A with fenestron tail. When building the ECMF surrogate, the previous
concept behavior along the variable Nb,tr is assumed to be constant, as explained in
Section 3.5.
211

As done for multi-objective adaptive sampling on MIC surrogates, see Section 8.1,
a good approximation of the Pareto front is calculated by the well-known evolutionary
multi-objective algorithm NSGA-II developed by Deb [43]. This approximation set is
considered herein as the “real” Pareto front. In order to obtain it, the computationally
expensive helicopter model has been evaluated 3380 times. The computational in-
tensity of the objective functions limits the number of evaluations and, consequently,
the quality of the assessment of the real Pareto front.
The Pareto front obtained with the NSGA-II algorithm is plotted in Figure 65. It is
seen that the obtained Pareto front from this optimization study is better performing
in the two objectives than the baseline. Also, the Pareto front for the new concept,
UH60A with fenestron tail, dominates the one obtained for the conventional tail
conﬁguration.
 
 
Pareto Conv
Pareto Fenes
Baseline
shpfwd
shphov
1700
1750
1800
1850
1900
1450
1500
1550
1600
1650
Figure 65: “Real” Pareto Front Obtained from NSGA-II. UH60A with Fenestron
shp. Airfoil “SC 1095”
The EGO algorithm is applied to two surrogates to determine the non-dominated
set that approaches the Pareto front. These two surrogates are the following:
ECMF Surrogates Surrogates that are developed to leverage similar trends of in-
crementally evolved concepts, see Section 3.5.
212

Mono-ﬁdelity Surrogates It is the current state-of-the-art method. A surrogate is
ﬁtted for the new concept with no reuse of observations from previous concepts.
The resulting Pareto fronts from both methods are compared with the “real”
Pareto front obtained from the evolutionary multi-objective genetic algorithm NSGA-
II.
The Pareto fronts after 15 updates of the EGO on warm-up training sets of 66
new concept observations and 20 old concept observations are plotted in Figure 66.
It shows that, after 15 updates, the adaptive sampling algorithm on the ECMF meta-
model estimates a better approximation to the “real” fenestron UH60A Pareto front
than the adaptive sampling algorithm applied on the mono-ﬁdelity surrogate.
 
 
Pareto
ECMF 15 Updates
Mono 15 Updates
shpfwd
shphov
1695
1700
1705
1710
1715
1720
1725
1730
1510
1520
1530
1540
1550
1560
1570
Figure 66: Non-Dominated Set Obtained from EGO Algorithm Applied on ECMF
and Mono-Fidelity Surrogates. UH60A with Fenestron shp. New Concept Warm-Up
Size 66. Reuse of 20 Old Concept Observations
After 30 updates, see Figure 67, both EGO processes approach better the “real”
Pareto front, but there are still more points and closer to the “real” Pareto front in
the ECMF case than in the state-of-the-art case.
Again, the number of points in the non-dominated set, and the average distance
between the non-dominated set and the “real” Pareto front, see Equation 88, are
213

 
 
Pareto
ECMF 30 Updates
ECMF 15 Updates
Mono 30 Updates
Mono 15 Updates
shpfwd
shphov
1695
1700
1705
1710
1715
1720
1725
1730
1510
1520
1530
1540
1550
1560
1570
Figure 67: Evolution Non-Dominated Set Obtained from EGO Algorithm Applied
on ECMF and Mono-Fidelity Surrogates. UH60A with Fenestron shp. New Concept
Warm-Up Size 66. Reuse of 20 Old Concept Observations
brought to quantify the quality of the non-dominated set for each method, as done in
Section 8.1. These two performance indicators are assessed for warm-up sets of 66,
120, and 180 observations; they are shown in Figures 68, 69, 70, 71, 72, and 73.
 
 
Mono
ECMF
Average Distantce to Pareto Set
Updates
0
5
10
15
20
25
30
35
40
45
50
0
1
2
3
4
5
6
7
Figure 68: Average Distance to Pareto set UH60A with Fenestron shp. EGO Al-
gorithm Applied on ECMF and Mono-Fidelity Surrogates. New Concept Warm-Up
Size 66. Reuse of 20 Old Concept Observations. Updates 40
The EGO runs on starting surrogates with 66 warm-up points, Figures 68 and 69,
214

 
 
Mono
ECMF
Number Points Non-Dominated Set
Updates
0
5
10
15
20
25
30
35
40
45
50
0
5
10
15
20
25
30
35
Figure 69: Number Points Non-Dominated Set UH60A with Fenestron shp. EGO
Algorithm Applied on ECMF and Mono-Fidelity Surrogates. New Concept Warm-Up
Size 66. Reuse of 20 Old Concept Observations. Updates 40
 
 
Mono
ECMF
Average Distantce to Pareto Set
Updates
0
5
10
15
20
25
30
35
40
45
50
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
Figure 70: Average Distance to Pareto set UH60A with Fenestron shp. EGO Al-
gorithm Applied on ECMF and Mono-Fidelity Surrogates. New Concept Warm-Up
Size 120. Reuse of 20 Old Concept Observations. Updates 40
show evidence that ECMF surrogates provide a much better Pareto front approxima-
tion for both indicators: the average distance is up to eight times smaller for ECMF
surrogates at 11 updates, and the number of points in the non-dominated is close to
twice as large above the 25th update.
When the warm-up size is increased to 120, see Figures 70 and 71, results are
215

 
 
Mono
ECMF
Number Points Non-Dominated Set
Updates
0
5
10
15
20
25
30
35
40
45
50
0
5
10
15
20
25
30
35
Figure 71: Number Points Non-Dominated Set UH60A with Fenestron shp. EGO
Algorithm Applied on ECMF and Mono-Fidelity Surrogates. New Concept Warm-Up
Size 120. Reuse of 20 Old Concept Observations. Updates 40
 
 
Mono
ECMF
Average Distantce to Pareto Set
Updates
0
5
10
15
20
25
30
35
40
45
50
0
0.5
1
1.5
2
2.5
3
3.5
4
Figure 72: Average Distance to Pareto set UH60A with Fenestron shp. EGO Al-
gorithm Applied on ECMF and Mono-Fidelity Surrogates. New Concept Warm-Up
Size 180. Reuse of 20 Old Concept Observations. Updates 40
similar to the ones obtained in the warm-up training sets with 66 observations. How-
ever, one can see that the quality of the Pareto front obtained from the mono-ﬁdelity
surrogate is competitive with the ECMF one during the ﬁrst updates.
Results for the largest warm-up training sets, 180 observations, are shown in
Figures 72 and 73. They demonstrate that the two Pareto fronts have similar quality.
216

 
 
Mono
ECMF
Number Points Non-Dominated Set
Updates
0
5
10
15
20
25
30
35
40
45
50
5
10
15
20
25
30
35
40
Figure 73: Number Points Non-Dominated Set UH60A with Fenestron shp. EGO
Algorithm Applied on ECMF and Mono-Fidelity Surrogates. New Concept Warm-Up
Size 180. Reuse of 20 Old Concept Observations. Updates 40
It shows evidence that the performance of EGO algorithms on ECMF surrogates
degrades with respect to that of EGO algorithm applied on mono-ﬁdelity surrogates
when the new concept training set becomes large, as happened with the surrogate
eﬃciency (see Chapter 7).
When calculating the UH60A Pareto front for hover and forward ﬂight, results
show evidence that EGO algorithms applied on ECMF surrogates are more eﬃcient
than on mono-ﬁdelity surrogates for the small range of new concept warm-up training
sets.
217

9.2
Multi-objective Adaptive Sampling of the UH60A with
Fenestron Shaft Power. Full Domain
The example in this Section aims to reduce the power consumption in hover and
forward ﬂight of the UH60A with fenestron tail even further than in Section 9.1. The
ECMF surrogate is used. A large design space is explored where the new variables
are: the radial position of the twist change, the fenestron radius, and the fenestron
chord.
The objective functions to optimize are
shpfen
hov (θ1, θ2, rtw, Rtr, ctr, Nb,tr, c)
shpfen
fwd (θ1, θ2, rtw, Rtr, ctr, Nb,tr, c)
(123)
subjected to the full domain constraints given by Equations 126.
θ1 ∈[−3.4◦/m, −0.5◦/m]
θ1 ∈R
θ2 ∈[−1.5◦/m, 0◦/m]
θ2 ∈R
rtw
R ∈[0.72, 0.85]
rtw
R ∈R
Rtr ∈[0.8m, 1m]
Rtr ∈R
ctr ∈[0.14m, 0.2m]
ctr ∈R
Nb,tr ∈7, 8, 9
c
¯c ∈{0.9 +

0, 1
3, 2
3, 1

· 0.85}
(124)
The remaining parameters are W = 16994lb (CT = 5.99 · 10−3), R = 26.83ft,
Ω= 27.0063rad
sec , and rcut-oﬀ
R
= 1
3. The main rotor blade section is again “SC 1095”.
The previous concept that helps to capture the functions shpfen
hov and shpfen
fwd is the
UH60A with conventional tail. Its corresponding objectives shpconv
hov and shpconv
fwd are
deﬁned as
218

shpconv
hov (θ1, θ2, rtw, c)
shpconv
fwd (θ1, θ2, rtw, c)
(125)
subjected to
θ1 ∈[−3.4◦/m, −0.5◦/m]
θ1 ∈R
θ2 ∈[−1.5◦/m, 0◦/m]
θ2 ∈R
rtw
R ∈[0.72, 0.85]
rtw
R ∈R
c
¯c ∈{0.9 +

0, 1
3, 2
3, 1

· 0.85}
(126)
The same blade section, “SC 1095”, is used for the previous concept. The remaining
previous concept parameters are the same as for the UH60A with fenestron tail:
W = 16994lb (CT = 5.99 · 10−3), R = 26.83ft, Ω= 27.0063rad
sec , and rcut-oﬀ
R
= 0.047.
Notice that the design space of the two concepts,

shpfen
hov, shpfen
fwd

and
 shpconv
hov , shpconv
fwd

,
are diﬀerent. The UH60A with fenestron tail has three more design variables than
the UH60A with conventional tail. These variables are Rtr, ctr, and Nb,tr. Therefore,
when building the ECMF surrogate, the previous concept behavior along these three
variables is assumed to be constant, as discussed in Section 3.5.
The starting ECMF surrogate warm-up size has 1000 observations and 40 obser-
vations of the UH60A with conventional tail are reused when building the ECMF
surrogate. Figure 74 contains the Pareto front of the full domain (seven variables)
after 400 updates, the Pareto front of the screened domain (four variables), and the
Pareto front of the UH60A with conventional tail (seven variables) calculated in Sec-
tion 8.2.
The seven variable domain Pareto front of the UH60A with fenestron tail dom-
inates the Pareto front of the reduced domain as expected. The number of points
in the non-dominated set is 60, around one seventh of the 400 updates. The seven
variable domain Pareto front of the UH60A with fenestron tail also dominates the
219

counterpart Pareto front of the UH60A with conventional tail. It means that the
fenestron tail provides a better design than that of the conventional tail according
to the power consumption at hover and cruise speed. In other words, the beneﬁts
obtained from the increase in eﬃciency of the fenestron tail are more important than
the weight penalty of the fenestron tail with respect to the conventional tail.
 
 
Pareto Fen 4 Var
Pareto Conv 7 Var
Pareto Fen 7 Var
shpfwd
shphov
1670
1690
1710
1730
1750
1480
1500
1520
1540
1560
1580
1600
Figure 74: Pareto Front of the UH60A with Fenestron for the Large and Screened
Domain. EGO Algorithm Applied on ECMF Surrogates
220

CHAPTER X
SUMMARY, CONTRIBUTIONS, AND
RECOMMENDATIONS
This ﬁnal Chapter ﬁrst summarizes the methodologies and experiments carried out to
test the proposed methods. Then, a review of the contributions is made, comparing
them with the current state-of-the art. It is followed by a list of recommendations to
use the surrogates MIC and ECMF, and the presented adaptive sampling algorithms
on them. Finally, the possible future work is proposed with the purpose of widening
the scenarios where the developed methodologies can be used and making them more
suitable for real engineering design situations.
10.1
Summary
Meta-models that identify and apply similar trends across categories and concepts
have been constructed. They are intended to accomplish the purpose of the present
research: developing new conceptual design tools for the more eﬃcient use of compu-
tationally expensive codes with the purpose of improving the quality of initial designs
and concept/alternative selection. Results on noise-free canonical problems and noisy
UH60A models indicate that, for surrogate modeling and surrogate based-sampling,
the surrogates MIC and ECMF are more eﬃcient than traditional independent sur-
rogates in the low range of training set sizes. The principal conclusions of this work
are:
1) New meta-models, called MIC, are proposed to approximate computationally
expensive models when there is a choice of categories.
MIC surrogates aim
to leverage similar trends across categories and apply them in the early design
221

phase. These meta-models contain not only continuous and integer variables but
also non-numeric ones. They use a nominal distance to include the categories
as a new variable in their domain. MIC surrogates can be constructed with two
nominal distances: Hamming distance and intrinsic distance when its deﬁnition
is possible. These meta-models are an alternative to the current state-of-the-art
that uses independent surrogates, one for each category.
2) When modeling the UH60A hover power consumption, MIC surrogates are
shown to be more “eﬃcient” for small training sets than independent modeling
of the rotor-craft for each airfoil. Same results are obtained for the canonical
disturbed Branin function. ”Eﬃciency” means fewer observations are needed for
a given accuracy or more accuracy for the same number of intense observations.
The second interpretation leads to the conclusion that MIC surrogates are a
major step in order to better use computationally intense codes in early design
of problems with a categorical choice. In other words, MIC surrogate is a con-
ceptual design tool that, for a given intense function call budget, provides better
accuracy than the current state-of-the-art surrogates. MIC meta-modeling is
shown to be a tool capable of identifying and leveraging the similarities across
categories, leading to better initial designs and categorical selection.
3) In the UH60A hover power consumption example, MIC eﬃciency degrades with
respect to that of the independent UH60A surrogate for each airfoil as the
training set becomes large. This behavior can be explained by the fact that
as the number of expensive observations increases, high frequency information
for each category is available. These high frequency features are diﬀerent for
each airfoil, so the cross-use of observations across airfoils, done by MIC, is no
longer eﬃcient. Similar results are found in the test on the canonical disturbed
Branin function. The small range of the training set where the MIC surrogate is
222

more eﬃcient than the independent surrogate depends on how similar the trends
are between categories, the dimension of the design space, and the number of
categories. For the speciﬁc tested problems, these ranges are plotted in Figures
21, and 30.
4) A MICGA is developed to search a domain with continuous, integer, and cate-
gorical variables. This stochastic algorithm is used to drive the adaptive sam-
pling process on the MIC surrogate.
5) An EGO algorithm is run on the MIC surrogate via MICGA to adaptively
sample the UH60A hover performance. Several EGO processes are performed
and it is observed that in all cases a large subset of the updates are in the globally
high-performing areas (best designs of best airfoils). It is a characteristic of
EGO optimizers. All the EGO runs show an increase in hover performance of
around 10% respect the baseline case.
6) New meta-models, called ECMF, are proposed to approximate computationally
expensive models when there are sequential concepts. ECMF surrogates aim
to leverage similar trends from previously sampled concepts and apply them in
the early design phase. “Evolutionary incremental concepts” are assumed; they
result from the small changes in concepts done by designers while trying to meet
requirements in conceptual design. Multi-ﬁdelity techniques are brought to feed
observations from one concept to another in an innovative way. These meta-
models are an alternative to the current state-of-the-art that uses independent
surrogates for each concept.
7) When modeling the hover power consumption of the UH60A with fenestron
tail, ECMF surrogates are shown to be more “eﬃcient” than the mono-ﬁdelity
surrogates for small new concept training sets, where no reuse of expensive ob-
servations is done. Same results are obtained for the canonical Michalewicz
223

function. ECMF surrogates are a major step in order to better use the compu-
tationally intense codes in early design of problems where there is a sequence
of concepts. ECMF meta-modeling is shown to be a tool capable of identifying
and leveraging the similarities from previous concepts, leading to better initial
designs and concept selection.
8) In the example of the hover power consumption of the UH60A with fenestron
tail, ECMF loses eﬃciency compared to the mono-ﬁdelity surrogate as the new
concept training set becomes large. The reason could be that previous concept
observations could be useless or misleading when the new concept training set
is large enough to properly capture the new concept landscape.
Also, ECMF eﬃciency saturates or even decreases as the old concept training
set becomes large. This behavior could be explained by the fact that, as the
number of old concept expensive observations increases, high frequency infor-
mation from the previous concept is available. These high frequency features
could be diﬀerent from one concept to another. So, the reuse of observations
across concepts, done by ECMF, is no longer eﬃcient. Similar results are found
in the test on the canonical Michalewicz function.
The small range of the new concept training set where the ECMF surrogate
is more eﬃcient than the mono-ﬁdelity surrogate depends on how similar the
trends between concepts are, the dimension of the design space, and the number
of old concept observations reused.
For the speciﬁc tested problems, these
ranges are plotted in Figures 52, and 45.
9) A multi-objective EGO algorithm is run on the MIC surrogate via MICGA
to optimize the UH60A hover and forward ﬂight power consumption. The re-
sulting Pareto front is compared with that obtained from the simultaneous
224

multi-objective EGO algorithm on independent surrogates, one for each air-
foil section. The indicators demonstrate again that EGO algorithms on MIC
meta-models are more “eﬃcient” to assess the Pareto front of the UH60A in the
low training set range. The updates in the MIC case are located in the high-
performing areas of the best airfoils earlier than in the current state-of-the-art
case. Multi-objective adaptive sampling on MIC surrogates allows a better use
of the computationally intense tools in rotor-craft conceptual design because
they provide more accurate Pareto fronts given a intense function call budget,
which again leads to better initial designs and airfoil selection.
10) A multi-objective EGO algorithm is run on the ECMF surrogate to optimize
the hover and forward ﬂight power consumption of the UH60A with a fenestron
tail. The resulting Pareto front is compared with that obtained from the multi-
objective EGO algorithm on mono-ﬁdelity surrogates. The indicators demon-
strate again that EGO algorithms on ECMF meta-models are more “eﬃcient”
to assess the Pareto front of the UH60A with fenestron tail in the low training
set range. The updates in the ECMF case are located in the high-performing de-
signs earlier than in the current state-of-the-art case. Multi-objective adaptive
sampling on ECMF surrogates allows a better use of computationally intense
tools in rotor-craft conceptual design because they provide more accurate Pareto
fronts given a intense function call budget, which again leads to better initial
designs and concept selection.
10.2
Contributions
The MIC and ECMF surrogates and their adaptive sampling algorithms provide new
scholarly contributions. Firstly, a new eﬃcient treatment is proposed for building
surrogates of concepts when observations show similar trends across several categories.
In the literature one can only ﬁnd surrogates for mixed-integer variables [153, 92].
225

The use of these proposed meta-models results in more accurate surrogates for a given
number of function calls.
It represents an alternative to the current independent
surrogates, one for each category. This alternative is more eﬃcient for small training
sets. It is a major step in order to better use computationally burdensome codes in
early design of problems with categorical choices.
The second novelty is the adaptive sampling of MIC surrogates, the ones of the ﬁrst
contribution. A mixed-variable genetic algorithm, MICGA, that searches a mixed-
integer-categorical design space at one algorithm call is developed to perform this
task. Adaptive Kriging on mixed-integer domains is done [113]; however, no adaptive
sampling on MIC domains is found in the literature. This contribution allows adap-
tive sampling on the whole design space of a surrogate that cross-uses observations
across categories, whereas currently, all the independent surrogates (with no cross-
use of observations) are adaptively sampled simultaneously that results in a waste
of resources when sampling in poor performing categories occurs. Also, the MICGA
could be considered as a minor contribution: it successfully optimizes the expected
improvement in a mixed-integer-categorical domain. However, one can ﬁnd in the
literature evolutionary strategies capable of searching MIC domains in the ﬁeld of
medical image analysis [127].
The third contribution is a methodology that leverages computationally expensive
observations from a previously sampled concept in the construction of a new concept
surrogate. In the literature one can ﬁnd Gaussian meta-models of concepts using
variable-ﬁdelities that lay on the same design space [79]. Also, trust-region model
management for variable parameterization design spaces has been implemented but
only for the same concept [155]. However, the proposed ECMF surrogate deals with
two diﬀerent concepts with diﬀerent design space. With this third contribution, ob-
servations from each concept are employed to build, not only their own concept sur-
rogate, but also other concept surrogates that experience similar trends. It opposes
226

to the current state-of-the-art that ﬁts independent surrogates for each concept with
no reuse of the previous concept intense observations. Again, ECMF meta-models for
a given number of function calls are more accurate than mono-ﬁdelity surrogates for
small training sets. It is due to the useful information brought from previously sam-
pled concepts. It is a major step in order to better use computationally burdensome
codes in early design of problems with sequential concepts.
10.3
Recommendations
In this Section recommendations of the proposed methodologies are discussed.
10.3.1
MIC Surrogate
MIC surrogates are a great tool to make a better use of computationally expensive
tools in conceptual design.
The recommendations for their use are based on the
assumptions and the results obtained in the computer experiments carried out in
Chapters 5, 6, and 8. The recommendations are presented in the following enumera-
tion:
1) Designers must choose between several categorical alternatives for the same
concept.
2) There are similar trends across the categories along the design variables.
3) Desire of a parametric study of each category design space. It is pursued the
exploration of the each category design space and comparison of optimal designs
across categories.
4) MIC surrogates must be used in early design when the designers’ focus is to
reveal important trends, interactions, and sensitivities.
5) MIC surrogates must be employed when the function budget is limited due
to: the use of computationally expensive tools, presence of many categorical
227

alternatives, or existence of large design spaces to explore.
10.3.2
ECMF Surrogate
ECMF surrogates are a great tool to better use of computationally expensive tools in
conceptual design. The recommendations for their use are based on the assumptions
and the results obtained in the computer experiments carried out in Chapters 7, and
9. The recommendations are presented in the following enumeration:
1) Sequential concepts where observations from previous concepts are available.
2) The new concept is an incremental change of the previous one. So similar trends
are expected for the previous and new concept.
3) Desire of a parametric study of each concept. It is pursued by the exploration
of the each concept design space and comparison with other concept optimal
designs.
4) ECMF surrogates must be used in early design when the designers’ focus is to
reveal important, trends, interactions and sensitivities.
5) ECMF surrogates must be employed when the function budget is limited due
to: the use of computationally expensive tools, presence of many concepts, or
existence of large design spaces to explore.
6) Do not use many previous concept observations because it can capture high
frequency information that usually diﬀers across concepts.
10.4
Future Work
The future work aims to extend the developed techniques to scenarios broader than
the research motivation. Also, the proposed methodologies can spin-oﬀnew scenarios
that are not directly related to the main research motivation, but where the method-
ologies can be applied. The next enumeration proposes possible future paths:
228

1) Take the idea of the cross-use and reuse of computationally expensive observa-
tions to deterministic surrogates. See how eﬀective the cross-use and reuse of
information are in the deterministic case.
2) Investigate on possible methods to select the number and location of the samples
to cross-use and reuse. It would help to increase the performance of the MIC
and ECMF surrogates.
3) Build surrogates that cross-use computationally expensive observations across
concepts (the proposed ECMF surrogate only reuses observations from previ-
ously sampled concepts).
4) Extend ECMF surrogates to less stiﬀdesign space assumptions. Space mapping
could be a perfect candidate to build ECMF surrogates where the concept 1
design space is not necessarily included in the concept 2 design space.
5) Extend ECMF surrogates to scenarios where observations from two or more
previous concepts are available.
6) Do a sensitivity study to understand the inﬂuence of the number of categories
on the MIC surrogate performance (scalability).
7) Combine ECMF and MIC surrogates and see how the cross-use of observations
across categories and the reuse of observations from previous concepts aﬀect
each other.
8) Formally develop the MICGA optimizer. Perform tests on canonical problems
of diﬀerent modality.
229

APPENDIX A
DISTURBED BRANIN FUNCTION
The Branin function is a two-variable function given by Equation 127
f (X1, X2) =

X2 −5.1
4π2X2 + 5
πX1 −6
2
+

1 −1
8π

cos (X1) + 1

+ 5X1 (127)
where the domain is X1 ∈[−5, 10] and X2 ∈[0, 15].
First of all, the original function is made dimensionless so that non-bias is pro-
duced when the Kriging surrogate is ﬁtted. The new independent variables are
x1 = X1 + 5
15
x2 = X2
15
(128)
The intention is to transform it to a test function on a mixed-integer-categorical
design space, called disturbed Branin function. The purpose of this function is to test
MIC surrogates. While including a new categorical variable to the Branin function
some requirements must be present. MIC surrogates are intended for functions with
similar trends across categories, so the modality of the all disturbed Branin function
categories must be similar to the original Branin function.
The method proposed to obtain disturbed Branin function consists in calculating
the Fourier series of the original Branin function and alter randomly the 9 ﬁrst Fourier
coeﬃcients. Dym and McKean [49], among many others, provide information about
the Fourier transform.
The amplitudes and phases of the 9 highest modes of the Fourier transform are
randomly modiﬁed by two normal distributions according to the following formula:
ˆf′ (n1, n2) = ˆf (n1, n2) exp{i · rnd1} (1 + 0.05 · rnd2)
(129)
230

where ˆf and ˆf′ are the Fourier coeﬃcient of the original Branin function and a
category of the disturbed Branin function, respectively; ni is the Fourier frequency in
the i coordinate dimension; and rndi is a standardized normally distributed random
number N (µ = 0, σ2 = 1). The second factor of the right hand side, exp{i · rnd1}, is
a change of phase of rnd1
2π
radians in the Fourier coeﬃcient, whereas the last factor
(1 + 0.05 · rnd2) represents a change of 0.05·rnd2 in the Fourier coeﬃcient amplitude.
Notice that to avoid large changes in the trends of the Branin function, the amplitude
of the Fourier coeﬃcient is limited to a typical change of 5%, and that of the phase
is 9.11 degrees.
Several disturbed Branin function categories could be built by generating sev-
eral sets of random variables (rnd1, rnd2). Each pair of (rnd1, rnd2) is considered a
category.
Five pairs of random numbers are produced and their corresponding disturbed
Branin function categories are compared with the original Branin function in Figure
75. Contours in Figure 75 show that there are similar trends in the disturbed Branin
function categories, but the depth and location of the valleys varies.
Once the six categories are created (the ﬁrst category is the original Branin func-
tion, i.e., rnd1 = 0, and rnd2 = 0), a further modiﬁcation is made to ﬁnally convert
the design space into a mixed-integer-categorical one. The x2 coordinate is divided
into 11 equally spaced points

0, 1
10, 2
10, ...1

. It makes the x2 a discrete-quantitative
design input. x1 is kept as a continuous variable.
Table 14 compares the value of the global minimum and its location for the dis-
turbed Branin function categories. Notice that the original Branin function is con-
sidered category 1 of the disturbed Branin function.
Location and value of categorical minimum changes across disturbed Branin func-
tion categories; however, they experience similar trends as depicted in Figure 75.
231

 
 
x1
Category 6
Category 5
Category 4
x2
Category 3
Category 2
Category 1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
50
100
150
200
250
300
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
Figure 75: Example of Contours of Disturbed Branin Function. 6 Categories
Table 14: Categorical Minimum Values and their Location for the Disturbed Branin
Function
Problem
x1
x2
Min Value
Dist Branin 1 (original)
0.1036
0.9
1.5892
Dist Branin 2
0.9921
0.2
-24.8723
Dist Branin 3
0.9893
0.2
4.0047
Dist Branin 4
0.9847
0.3
-21.0302
Dist Branin 5
0.5952
0.1
-13.4278
Dist Branin 6
0.1338
0.7
-15.1981
232

APPENDIX B
CANONICAL CONCEPTS WITH SIMILAR TRENDS
A canonical set of functions is needed to test ECMF surrogates. It is obtained out
of the two-dimensional function proposed by Michalewicz [133]. Concept 2 is the
original two-dimensional Michalewicz function whereas concept 1 is a modiﬁed spline
of a coordinate plane slice of the original two dimensional Michalewicz’s function. A
quadratic function has been added to the coordinate plane slice of the original two-
dimensional Michalewicz’s function to build the modiﬁed spline. Figure 76 shows the
two concepts of the Michalewicz canonical set.
Concept 1
Objective Function
x2
x1
Concept 2
Objective Function
x2
x1
−1
−0.5
0
0.5
1
0
0.5
1
0
0.5
1
0
0.2
0.4
0.6
0.8
1
−1.6
−1.4
−1.2
−1
−0.8
−0.6
−0.4
−0.2
−2
−1.5
−1
−0.5
0
Figure 76: The Two Concepts of the Michalewicz Function
233

APPENDIX C
MIXED-INTEGER CANONICAL PROBLEMS
1) Test Problem 1. Taken from Floudas, Pardalos et al. [62]. It is the example
12.2.2.
minx,y −0.7y + 5 (x1 −0.5)2 + 0.8
(130)
−exp (x1 −0.2) −x2
≤
0
(131)
x2 + 1.1y
≤
−1
(132)
x1 −1.2y
≤
0.2
(133)
0.2 ≤
x1
≤1
(134)
−2.22554 ≤
x2
≤−1
(135)
y
∈
{0, 1}
(136)
The known global optimum is (x1, x2, y, f) = (0.9419, −2.1, 1, 1.0765).
2) Test Problem 2. Taken from Floudas, Pardalos et al. [62]. It is the example
12.2.6.
minx,y3y −5x
(137)
234

2y2 −2y0.5 −2x0.5y2 + 11y + 8x
≤
39
(138)
−y + x
≤
3
(139)
2y + 3x
≤
24
(140)
1 ≤
x
≤10
(141)
y
∈
[1, 6] ∩N
(142)
The known global optimum is (x, y, f) = (4, 1, −17).
3) Test Problem 3. Taken from Appendix A of Ref. [87]. It is the example 21.
minx,y
9
X
i=1

exp

−(ui −y2)x
y1
−0.01i

(143)
where ui = 25 + (−50 log (0.01i))
2
3
0 ≤
x
≤5
(144)
y1
∈
[1, 100] ∩N
(145)
y2
∈
[0, 25] ∩Z
(146)
The known global optimum is (x, y1, y2, f) = (1.5, 50, 25, 0).
4) Test Problem 4. Constrained Branin function with the second variable discrete.
minx,y

y −5.1
4π2y + 5
πx −6
2
+ 10

1 −1
8π

cos x + 1

+ 5x
(147)
−xy
<
−0.2
(148)
−5 ≤
x
≤10
(149)
y
∈

0, 3
2, 3, 9
2, 6, ...15

(150)
The known global optimum is (x, y, f) =
 9.7315, 9
2, 8.833

.
235

REFERENCES
[1] “FLIGHTLAB model editor manual,” manual, Advanced Rotorcraft Technolo-
gies, May 2006.
[2] Aha, D. W., Kibler, D., and Albert, M. K., “Instance-Based learning
algorithms,” Machine Learning, vol. 6, pp. 37–66, Jan. 1991.
[3] Aha, D. W., Kibler, D., and Albert, M. K., “Instance-Based learning
algorithms,” Machine Learning, vol. 6, p. 3766, 1991.
[4] Alexandrov, N. M., Lewis, R. M., Gumbert, C. R., Green, L. L.,
and Newman, P. A., “Approximation and model management in aerody-
namic optimization with Variable-Fidelity models,” Journal of Aircraft, vol. 38,
pp. 1093–1101, Dec. 2001.
[5] Alexandrov, N. M., Nielsen, E. J., Lewis, R. M., and Anderson,
W. K., “First-Order model management with Variable-Fidelity physics applied
to Multi-Element airfoil optimization,” in 8th AIAA/USAF/NASA/ISSMO
Symposium on Multidisciplinary Analysis & Optimization, (Long Beach, CA),
Sept. 2000.
[6] Ash, R. B., Information Theory. New York, NY: Interscience Publishers, 1965.
[7] Avigad, G. and Moshaiov, A., “Simultaneous Concept-Based evolution-
ary Multi-Objective optimization,” Applied Soft Computing Journal, vol. 11,
pp. 193–207, Jan. 2011.
[8] B¨ack, T., Evolutionary Algorithms in Theory and Practice: Evolution Strate-
gies, Evolutionary Programming, Genetic Algorithms. New York, USA: Oxford
University Press, Jan. 1996.
[9] Bader, J. and Zitzler, E., “Hype: An algorithm for fast Hypervolume-
Based Many-Objective optimization,” Evolutionary Computation, vol. 19, no. 1,
pp. 45–76, 2011.
[10] Balakrishnan, A. V., Applied Functional Analysis. New York, NY: Springer-
Verlag, 2nd ed., 1981.
[11] Balas, E., Glover, F., and Zionts, S., “An additive algorithm for solv-
ing linear programs with Zero-One variables,” Operations Research, vol. 13,
pp. 517–549, July 1965.
236

[12] Bandler, J. W., Cheng, Q., Dakroury, S., Mohamed, A., Bakr, M.,
Madsen, K., and Sndergaard, J., “Space mapping: The state of the art,”
Electromagnetics-Based Optimization of Microwave Components and Circuits,
vol. 52, pp. 337–361, Jan. 2004.
[13] Batchelor, B. G., Pattern Recognition : Ideas in Practice. New York, NY:
Plenum Press, 1978.
[14] Berman, O. and Ashrafi, N., “Optimization models for reliability of mod-
ular software systems,” IEEE Transactions on Software Engineering, vol. 19,
pp. 1119–1123, Nov. 1993.
[15] Berry, B., Bowen-Davies, G., Gluesenkamp, K., Kaler, Z., Schmaus,
J., Staruk, W., Weiner, E., and Woods, B. K., “Design optimization of
gamera II: a human powered helicopter,” in Proceedings of the 68th Ameri-
can Helicopter Society International Annual Forum, vol. 1, (Fort Worth, TX),
pp. 646–664, May 2012.
[16] Beume, N., Naujoks, B., and Emmerich, M., “SMS-EMOA: multiobjective
selection based on dominated hypervolume,” European Journal of Operational
Research, vol. 181, pp. 1653–1669, Sept. 2007.
[17] Bishop, C. M., Neural Networks for Pattern Recognition. New York, NY:
Oxford University Press, 1st ed., 1996.
[18] Bishop, C. M., Pattern Recognition and Machine Learning. New York, NY:
Springer, 1st ed., 2007.
[19] Booker, A. J., Dennis, J. E., Frank, P. D., Serafini, D. B., and Tor-
czon, V., “Optimization using surrogate objectives on a helicopter test exam-
ple,” in Computational Methods in Optimal Design and Control (Borggaard,
J., Burns, J., Cliff, E., and Schreek, S., eds.), pp. 49–58, Boston, MA:
Birkhauser, 1998.
[20] Borchers, B. and Mitchell, J. E., “An improved branch and bound al-
gorithm for mixed integer nonlinear programs,” Computers & Operations Re-
search, vol. 21, pp. 359–367, Apr. 1994.
[21] Boukouvala, F., Dubey, A., Vanarase, A., Ramachandran, R.,
Muzzio, F. J., and Ierapetritou, M., “Computational approaches for
studying the granular dynamics of continuous blending processes, 2 - population
balance and Data-Based methods,” Macromolecular Materials and Engineering,
vol. 297, pp. 9–19, Jan. 2012.
[22] Boukouvala, F., Muzzio, F. J., and Ierapetritou, M. G., “Design space
of pharmaceutical processes using Data-Driven-Based methods,” Journal of
Pharmaceutical Innovation, vol. 5, pp. 119–137, Oct. 2010.
237

[23] Bousman, W. G. and Kufeld, R. M., “UH-60A airloads catalog,” Technical
Memorandum 212827, National Aeronautics and Space Administration, Ames
Research Center, Moﬀett Field, CA, 2005.
[24] Box, G. E. P., Empirical Model-Building and Response Surfaces. New York,
NY: John Wiley & Sons, 1987.
[25] Brentner, K. S., Bres, G. A., Perez, G., and Jones, H. E., “Maneu-
vering rotorcraft noise prediction,” Journal of Sound and Vibration, vol. 275,
pp. 719–738, Aug. 2004.
[26] Cao, Y. J. and Wu, Q. H., “A mixed variable evolutionary programming for
optimisation of mechanical design,” International Journal of Engineering In-
telligent Systems for Electrical Engineering and Communications, vol. 7, no. 2,
pp. 77–82, 1999.
[27] Capitanescu, F. and Wehenkel, L., “A new heuristic approach to deal
with discrete variables in optimal power ﬂow computations,” in 2009 IEEE
Bucharest PowerTech: Innovative Ideas Toward the Electrical Grid of the Fu-
ture, (Bucharest, Romania), June 2009.
[28] Choi, S., Alonso, J. J., Kroo, I. M., and Wintzer, M., “Multiﬁdelity
design optimization of Low-Boom supersonic jets,” Journal of Aircraft, vol. 45,
no. 1, pp. 106–118, 2008.
[29] Choi, S. C., Park, J. S., and Kim, J. H., “Vibration control of Pre-Twisted
rotating composite Thin-Walled beams with piezoelectric ﬁber composites,”
Journal of Sound and Vibration, vol. 300, pp. 176–196, Feb. 2007.
[30] Chu, W. and Ghahramani, Z., “Gaussian processes for ordinal regression,”
Journal of Machine Learning Research, vol. 6, pp. 1019–1041, July 2005.
[31] Coello, C. A., van Veldhuizen, D. A., and Lamont, G. B., Evolutionary
Algorithms for Solving Multi-Objective Problems.
New York, NY: Springer,
2002.
[32] Cover, T. and Hart, P., “Nearest neighbor pattern classiﬁcation,” IEEE
Transactions on Information Theory, vol. 13, pp. 21–27, Jan. 1967.
[33] Cox, D. D. and John, S., “A statistical method for global optimization,”
in IEEE International Conference on Systems, Man and Cybernetics, vol. 2,
(Chicago, IL), pp. 1241 –1246, Oct. 1992.
[34] Crossley, W. and Laananen, D. H., “The genetic algorithm as an auto-
mated methodology for helicopter conceptual design,” Journal of Engineering
Design, vol. 8, pp. 231–250, Sept. 1997.
238

[35] Crossley, W. A., Welss, V. L., and Laananen, D. H., “The potential of
genetic algorithms for conceptual design of rotor systems,” Engineering Opti-
mization, vol. 24, no. 3, pp. 221–238, 1995.
[36] Dakin, R. J., “A Tree-Search algorithm for mixed integer programming prob-
lems,” The Computer Journal, vol. 8, pp. 250 –255, Jan. 1965.
[37] Davis, E. and Ierapetritou, M., “A kriging based method for the solution
of mixed-integer nonlinear programs containing Black-Box functions,” Journal
of Global Optimization, vol. 43, pp. 191–205, Mar. 2009.
[38] Davis, J., Design Methodology For Developing Concept Independent Rotorcraft
Analysis And Design Software. PhD thesis, Georgia Institute of Technology,
Atlanta, GA, Dec. 2007.
[39] de Jong, K. A., Analysis of the Behavior of a Class of Genetic Adaptive
Systems. PhD thesis, University of Michigan, Ann Arbor, MI, 1975.
[40] Deb, K., Multi-Objective Optimization Using Evolutionary Algorithms. Chich-
ester, NY: John Wiley and Sons, 2001.
[41] Deb, K., Agrawal, S., Pratap, A., and Meyarivan, T., “A fast eli-
tist non-dominated sorting genetic algorithm for Multi-Objective optimization:
NSGA-II,” in Proceedings of 6th International Conference on Parallel Problem
Solving from Nature, (Berlin, Germany), pp. 849–58, Sept. 2000.
[42] Deb, K., Mohan, M., and Mishra, S., “Evaluating the ǫ-Domination based
Multi-Objective evolutionary algorithm for a quick computation of Pareto-
Optimal solutions,” Evolutionary Computation, vol. 13, no. 4, pp. 501–525,
2005.
[43] Deb, K., Pratap, A., Agarwal, S., and Meyarivan, T., “A fast and elitist
multiobjective genetic algorithm: NSGA-II,” IEEE Transactions on Evolution-
ary Computation, vol. 6, pp. 182–97, Apr. 2002.
[44] Deep, K., Singh, K. P., Kansal, M. L., and Mohan, C., “A real coded
genetic algorithm for solving integer and mixed integer optimization problems,”
Applied Mathematics and Computation, vol. 212, pp. 505–518, June 2009.
[45] Deep, K. and Thakur, M., “A new crossover operator for real coded genetic
algorithms,” Applied Mathematics and Computation, vol. 188, pp. 895–911, May
2007.
[46] Duran, M. A. and Grossmann, I. E., “A mixed integer nonlinear program-
ming algorithm for process systems synthesis,” AIChE Journal, vol. 32, no. 4,
pp. 592–606, 1986.
239

[47] Duran, M. A. and Grossmann, I. E., “An Outer-Approximation algorithm
for a class of Mixed-Integer nonlinear programs,” Mathematical Programming,
vol. 36, no. 3, pp. 307–339, 1986.
[48] Dym, C. L., Wood, W. H., and Scott, M. J., “Rank ordering engineering
designs: Pairwise comparison charts and borda counts,” Research in Engineer-
ing Design, vol. 13, pp. 236–242, Sept. 2002.
[49] Dym, H. and McKean, H. P., Fourier Series and Integrals. New York, NY:
Academic Press, 1972.
[50] Edwards, B., Andrews, J., and Rahnke, C., “Ducted tail rotor designs for
rotorcraft and their low noise features,” in Proceedings of the FVP Symposium
on ’Advances in Rotorcraft Technology’, (Ottawa, Canada), 1996.
[51] El-Beltagy, M. A., “An evolutionary risk adjusting model fusion framework
for optimizing models with variable ﬁdelity,” in Collection of Technical Papers
- 10th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference,
vol. 5, (Albany, NY), pp. 3042–3050, Aug. 2004.
[52] El-Samanoudy, M., Ghorab, A. A. E., and Youssef, S. Z., “Eﬀect of
some design parameters on the performance of a giromill vertical axis wind
turbine,” Ain Shams Engineering Journal, vol. 1, pp. 85–95, Sept. 2010.
[53] Eldred, M. S., Giunta, A. A., and Collis, S. S., “Second-Order correc-
tions for Surrogate-Based optimization with model hierarchies,” in Collection
of Technical Papers - 10th AIAA/ISSMO Multidisciplinary Analysis and Opti-
mization Conference, vol. 3, (Albany, NY), pp. 1754–1768, Aug. 2004.
[54] Emmerich, M., Deutz, A. H., and Klinkenberg, J. W., “Hypervolume-
Based expected improvement: Monotonicity properties and exact computa-
tion,” in Proceedings of the 2011 IEEE Congress of Evolutionary Computation,
(New Orleans, LA), pp. 2147–2154, June 2011.
[55] Emmerich,
M., Giannakoglou,
K. C., and Naujoks,
B., “Single-
and multiobjective evolutionary optimization assisted by gaussian random
ﬁeld metamodels,” IEEE Transactions on Evolutionary Computation, vol. 10,
pp. 421–439, Aug. 2006.
[56] Emmerich, M., Gr¨otzner, M., Groβ, B., and Sch¨utz, M., “Mixed-
Integer evolution strategy for chemical plant optimization with simulators,” in
Evolutionary Design and Manufacture: Selected papers ACDM 2000, London,
UK: Springer, 2000.
[57] Emmerich, M. T. M., Li, R., Zhang, A., Flesch, I., Lucas, P. J. F., and
Nijholt, A., “Mixed-integer bayesian optimization utilizing a-priori knowledge
on parameter dependences,” in The 20th Belgian-Netherlands Conference on
Artiﬁcial Intelligence, (Enschede, Netherlands), Oct. 2008.
240

[58] Fleischer, M. and Fleischer, M., “The measure of pareto optima. appli-
cations to multi-objective metaheuristics,” in Proceedings of the Evolutionary
Multi-Criterion Optimization: Second International Conference, (Faro, Portu-
gal), Apr. 2003.
[59] Fletcher, R. and Leyffer, S., “Solving mixed integer nonlinear pro-
grams by outer approximation,” Mathematical Programming, Series B, vol. 66,
pp. 327–349, Sept. 1994.
[60] Floudas, C. A., Deterministic Global Optimization: Theory, Methods, and
Applications. Dordrecht, The Netherlands: Kluwer Academic Publishers, 2000.
[61] Floudas, C. A., Ciric, A. R., and Grossmann, I. E., “Automatic synthesis
of optimum heat exchanger network conﬁgurations,” AIChE Journal, vol. 32,
pp. 276–290, Feb. 1986.
[62] Floudas, C. A., Pardalos, P. M., Adjiman, C., Esposito, W. R.,
G¨um¨us, Z. H., Harding, S. T., Klepeis, J. L., Meyer, C. A., and
Schweiger, C. A., Handbook of Test Problems in Local and Global Optimiza-
tion: Nonconvex Optimization and Its Applications. Dordrecht, The Nether-
lands: Kluwer Academic Publishers, 1st ed., June 1999.
[63] Forrester, A., Bressloff, N. W., and Keane, A., “Optimization us-
ing surrogate models and partially converged computational ﬂuid dynamics
simulations,” Proceedings of the Royal Society of London, Series A, vol. 462,
pp. 2177–2204, July 2006.
[64] Forrester, A., Sobester, A., and Keane, A., “Multi-ﬁdelity optimization
via surrogate modelling,” Proceedings of the Royal Society of London, Series A,
vol. 463, pp. 3251–69, Dec. 2007.
[65] Forrester, A., Sobester, A., and Keane, A., Engineering Design Via
Surrogate Modelling: A Practical Guide. Chichester, UK: John Wiley & Sons,
Sept. 2008.
[66] Forrester, A. I. J., Keane, A. J., and Bressloff, N. W., “Design and
analysis of ”Noisy” computer experiments,” AIAA Journal, vol. 44, pp. 2331–
2339, Oct. 2006.
[67] Friedrich, T., Horoba, C., and Neumann, F., “Multiplicative approxima-
tions and the hypervolume indicator,” in 11th Annual Genetic and Evolutionary
Computation Conference, (Montreal, Canada), pp. 571–578, July 2009.
[68] Ganguli, R., “Survey of recent developments in rotorcraft design optimiza-
tion,” Journal of Aircraft, vol. 41, pp. 493–510, June 2004.
[69] Gao, Y., Ren, Z., and Gao, Y., “Modiﬁed diﬀerential evolution algorithm
of constrained nonlinear mixed integer programming problems,” Information
Technology Journal, vol. 10, no. 11, pp. 2068–2075, 2011.
241

[70] Gelfand, I. M., Fomin, S. V., and Silverman, R. A., Calculus of Varia-
tions. Mineola, NY: Dover Publications, 2000.
[71] Geoffrion, A. M., “Generalized benders decomposition,” Journal of Opti-
mization Theory and Applications, vol. 10, no. 4, pp. 237–260, 1972.
[72] Giesing, J. and Barthelemy, J. F., “A summary of industry MDO appli-
cations and needs,” in AIAA/USAF/NASA/ISSMO Symposium of Multidisci-
plinary Analysis and Optimization, (St. Louis, MO), Sept. 1998.
[73] Glaz, B., Friedmann, P. P., and Liu, L., “Helicopter vibration reduc-
tion throughout the entire ﬂight envelope using surrogate-based optimization,”
Journal of the American Helicopter Society, vol. 54, pp. 1–15, Jan. 2009.
[74] Glaz, B., Liu, L., and Friedmann, P. P., “Reduced-Order nonlinear un-
steady aerodynamic modeling using a Surrogate-Based recurrence framework,”
AIAA Journal, vol. 48, pp. 2418–2429, Oct. 2010.
[75] Glaz, B., Liu, L., Friedmann, P. P., Bain, J., and Sankar, L. N., “A
Surrogate-Based approach to Reduced-Order dynamic stall modeling,” Journal
of the American Helicopter Society, vol. 57, Apr. 2012.
[76] Gupta, O. K. and Ravindran, A., “Branch and bound experiments in con-
vex nonlinear integer programming,” Management Science, vol. 31, pp. 1533–
1546, Dec. 1985.
[77] Haftka, R. T., “Combining global and local approximations,” AIAA Journal,
vol. 29, pp. 1523–1525, Sept. 1991.
[78] Hamming, R. W., “Error detecting and error correcting codes,” Bell System
Technical Journal, vol. 29, pp. 147–160, Apr. 1950.
[79] Han, Z. H., Zimmermann, R., and Gortz, S., “A new cokriging method
for Variable-Fidelity surrogate modeling of aerodynamic data,” in 48th AIAA
Aerospace Sciences Meeting Including the New Horizons Forum and Aerospace
Exposition, (Orlando, FL), Jan. 2010. Compendex.
[80] Hao, W., Ying, Y., Wei, Y., and Baohua, L., “Adaptive Approximation-
Based optimization of composite advanced grid-stiﬀened cylinder,” Chinese
Journal of Aeronautics, vol. 23, pp. 423–429, Aug. 2010.
[81] Harjunkoski, I., Application Of MINLP Methods On A Scheduling Problem
In The Paper Converting Industry. Ph.D. thesis, Ab˚o Akademi University, Ab˚o,
Finland, 1997.
[82] He, S., Prempain, E., and Wu, Q. H., “An improved particle swarm opti-
mizer for mechanical design optimization problems,” Engineering Optimization,
vol. 36, pp. 585–605, Oct. 2004.
242

[83] Hein, B. R. and Chopra, I., “Hover performance of a micro air vehicle:
Rotors at low reynolds number,” Journal of the American Helicopter Society,
vol. 52, pp. 254–262, July 2007.
[84] Hepperle, H., “JAVAFOIL user’s guide,” Tech. Rep. Manual, Dec. 2011.
[85] Hess, R. A., Zeyada, Y., and Heffley, R., “Modeling and simulation for
helicopter task analysis,” Journal of the American Helicopter Society, vol. 47,
pp. 243–252, Oct. 2002.
[86] Hevesi, J. A., Flint, A. L., and Istok, J. D., “Precipitation estimation in
mountainous terrain using multivariate geostatistics, part II: isohyetal maps,”
Journal of Applied Meteorology, vol. 31, pp. 677–688, July 1992.
[87] Himmelblau, D. M., Applied Nonlinear Programming.
New York, NY:
McGraw-Hill, 1972.
[88] Holland, J. H., Adaptation in Natural and Artiﬁcial Systems: An Introduc-
tory Analysis with Applications to Biology, Control, and Artiﬁcial Intelligence.
Ann Arbor, MI: University of Michigan Press, 1975.
[89] Huband, S., Hingston, P., While, L., and Barone, L., “An evolution
strategy with probabilistic mutation for multi-objective optimisation,” in 2003
Congress on Evolutionary Computation, vol. 4, (Piscataway, NJ), pp. 2284–91,
Dec. 2003.
[90] Igel, C., Hansen, N., and Roth, S., “Covariance matrix adaptation for
multi-objective optimization,” Evolutionary Computation, vol. 15, no. 1, pp. 1–
28, 2007.
[91] Imiela, M., “High-Fidelity optimization framework for helicopter rotors,”
Aerospace Science and Technology, vol. 23, pp. 2–16, Dec. 2012.
[92] Jansson, N., Wakeman, W. D., and M˚anson, J. A., “Optimization of
hybrid thermoplastic composite structures using surrogate models and genetic
algorithms,” Composite Structures, vol. 80, pp. 21–31, Sept. 2007.
[93] Jaynes, E. T., “Prior probabilities,” IEEE Transactions on Systems Science
and Cybernetics, vol. SSC-4, no. 3, pp. 227–241, 1968.
[94] Jin, R., Chen, W., and Simpson, T. W., “Comparative studies of metamod-
eling techniques under multiple modeling criteria,” Structural and Multidisci-
plinary Optimization, vol. 23, pp. 1–13, Dec. 2001.
[95] Johnson, C. and Barakos, G., “Development of a framework for optimising
aspects of rotor blades,” in Proceedings of the 66th American Helicopter Society
International Annual Forum, (Phoenix, AZ), pp. 2119–2134, May 2010.
243

[96] Johnson, M. E., Moore, L. M., and Ylvisaker, D., “Minimax and max-
imin distance designs,” Journal of Statistical Planning and Inference, vol. 26,
pp. 131–148, 1990.
[97] Johnson, W., “NDARC NASA design and analysis of rotorcraft,” Tech. Rep.
TP-2009-215402, NASA Ames Research Center, Moﬀett Field, CA, 2009.
[98] Jones, D. R., “A taxonomy of global optimization methods based on response
surfaces,” Journal of Global Optimization, vol. 21, pp. 345–383, Dec. 2001.
[99] Jones, D. R., Schonlau, M., and Welch, W. J., “Eﬃcient global opti-
mization of expensive Black-Box functions,” Journal of Global Optimization,
vol. 13, pp. 455–492, Dec. 1998.
[100] Jones, D. R. and Welch, W. J., “Global optimization using response sur-
faces,” in Fifth SIAM conference on optimization, (Victoria, Canada), May
1996.
[101] Journel, A. G. and Huijbregts, C. J., Mining Geostatistics. London, UK:
Academic Press, 1978.
[102] Kalra, T. S., Amiraux, M., Nagarai, V. T., Chopra, I., and Baeder,
J. D., “A comparative study of diﬀerent weight formulations aﬀecting prelim-
inary sizing of rotorcraft,” in 68th American Helicopter Society International
Annual Forum, vol. 1, (Fort Worth, TX), pp. 617–631, May 2012.
[103] Karoui, K., Platbrood, L., Crisciu, H., and Waltz, R. A., “New Large-
Scale security constrained optimal power ﬂow program using a new interior
point algorithm,” in 2008 5th International Conference on the European Elec-
tricity Market, (Lisboa, Portugal), May 2008.
[104] Keane, A. and Nair, P., Computational Approaches for Aerospace Design:
The Pursuit of Excellence. Chichester, UK: John Wiley & Sons, Aug. 2005.
[105] Keane, A. J., “Wing optimization using design of experiment, response sur-
face, and data fusion methods,” Journal of Aircraft, vol. 40, pp. 741–750, Aug.
2003.
[106] Keane, A. J., “Statistical improvement criteria for use in multiobjective design
optimization,” AIAA Journal, vol. 44, pp. 879–891, Apr. 2006.
[107] Keane, A. J. and Petruzzelli, N., “Aircraft wing design using Ga-Based
multi-level strategies,” in Eighth AIAA/USAF/NASA/ISSMO Symp. on Mul-
tidisciplinary Analysis and Optimization, (Long Beach, Ca), Sept. 2000.
[108] Kelley, J. L., General Topology. New York, NY: Springer-Verlag, 1975.
[109] Kennedy, M. C. and O’Hagan, A., “Predicting the output from a complex
computer code when fast approximations are available,” Biometrika, vol. 87,
no. 1, pp. 1–13, 2000.
244

[110] Kenney, J. F., Mathematics of Statistics. Princeton, NJ: Van Nostrand Com-
pany, 1962.
[111] Kesavan, P. and Barton, P. I., “Generalized Branch-and-Cut framework
for Mixed-Integer nonlinear optimization problems,” Computers & Chemical
Engineering, vol. 24, no. 2-7, pp. 1361–1366, 2000.
[112] Keung, J., “Software development cost estimation using analogy: A review,”
in Proceedings of the Australian Software Engineering Conference ASWEC,
(Gold Coast, Australia), pp. 327–336, Apr. 2009.
[113] Kleijnen, J. P., van Beers, W., and van Nieuwenhuyse, I., “Con-
strained optimization in expensive simulation:
Novel approach,” European
Journal of Operational Research, vol. 202, pp. 164–174, Apr. 2010.
[114] Knill, D. L., Giunta, A. A., Baker, C. A., Grossman, B., Mason,
W. H., Haftka, R. T., and Watson, L. T., “Response surface models com-
bining linear and euler aerodynamics for supersonic transport design,” Journal
of Aircraft, vol. 36, no. 1, pp. 75–86, 1999.
[115] Knowles, J. and Corne, D., “On metrics for comparing nondominated
sets,” in Proceedings of the 2002 Congress on Evolutionary Computation, vol. 1,
(Hawaii, HI), pp. 711–716, May 2002.
[116] Koch, P. N., Simpson, T. W., Allen, J. K., and Mistree, F., “Statistical
approximations for multidisciplinary design optimization: The problem of size,”
Journal of Aircraft, vol. 36, no. 1, pp. 275–286, 1999.
[117] Krige, D. G., “Statistical approach to some basic mine valuation problems on
witwatersrand,” Journal of Institute of Mine Surveyors of South Africa, vol. 7,
no. 4, pp. 145–154, 1953.
[118] Kumar, D., Cesnik, C. E., Rohl, P. J., and Sutton, M., “Optimization
framework for the dynamic analysis and design of active twist rotors,” in Pro-
ceedings of the 68th American Helicopter Society International Annual Forum,
(Fort Worth, TX), pp. 1280–1292, May 2012.
[119] Kumar, D., Glaz, B., Mok, J., Friedmann, P., and Cesnik, C., “De-
termination of optimum camber distribution in rotating wings with deformable
airfoils for vibration reduction and performance enhancement using surrogate
modeling,” in Proceedings of 10th European Rotorcraft Forum, (The Hague,
Holland), Aug. 1984.
[120] Kushner, H. J., “A new method of locating the maximum point of an arbi-
trary multipeak curve in the presence of noise,” Journal of Basic Engineering,
vol. 86, pp. 97–106, Mar. 1964.
245

[121] Lawrence, T. H., Corning, S., and Wharburton, F., “Helicopter ma-
neuverability and agility design sensitivity and air combat maneuver data corre-
lation study,” Technical Report 160550, US Army Aviation System Command,
1991.
[122] Leary, S. J., Bhaskar, A., and Keane, A. J., “A Knowledge-Based ap-
proach to response surface modelling in multiﬁdelity optimization,” Journal of
Global Optimization, vol. 26, pp. 297–319, July 2003.
[123] Leishman, J. G., Principles of Helicopter Aerodynamics.
New York, NY:
Cambridge University Press, 2006.
[124] Li, C. and Li, H., “A survey of distance metrics for nominal attributes,”
Journal of Software, vol. 5, pp. 1262–1269, Nov. 2010.
[125] Li, R., Eggermont, J., Emmerich, M., Bovenkamp, E., B¨ack, T., Di-
jkstra, J., and Reiber, J., “Mixed-Integer optimization of coronary vessel
image analysis using evolution strategies,” in 8th Annual Genetic and Evolu-
tionary Computation Conference, vol. 2, (Seattle, WA), July 2006.
[126] Li, R., Eggermont, J., Shir, O. M., Emmerich, M., B¨ack, T., Dijk-
stra, J., and Reiber, J., “Mixed-Integer evolution strategies with dynamic
niching,” in Parallel Problem Solving from Nature, vol. 5199, pp. 246–255,
Berlin, Germany: Springer Berlin Heidelberg, 2008.
[127] Li, R., Emmerich, M., Eggermont, J., Bovenkamp, E., B¨ack, T., Di-
jkstra, J., and Reiber, J., “Optimizing a medical image analysis system
using Mixed-Integer evolution strategies,” in Evolutionary Image Analysis and
Signal Processing, vol. 213, pp. 91–112, Berlin, Germany: Springer Berlin Hei-
delberg, 2009.
[128] Liu, W., Zhang, Q., Tsang, E., Liu, C., and Virginas, B., “On the per-
formance of metamodel assisted MOEA/D,” in 2nd International Symposium
on Intelligence Computation and Applications, (Wuhan, China), Sept. 2007.
[129] Lyle, K. H., Jackson, K. E., and Fasanella, E. L., “Development of
an ACAP helicopter ﬁnite element impact model,” Journal of the American
Helicopter Society, vol. 45, pp. 137–142, Apr. 2000.
[130] MacKay, D. J. C., Information Theory, Inference and Learning Algorithms.
Cambridge, UK: Cambridge University Press, 1st ed., 2003.
[131] Mair, C. and Shepperd, M., “The consistency of empirical comparisons of
regression and analogy-based software project cost prediction,” in International
Symposium on Empirical Software Engineering, pp. 509–518, Nov. 2005.
[132] Mason, W. H., Knill, D. L., Giunta, A. A., Grossman, B., Watson,
L. T., and Haftka, R. T., “Getting the full beneﬁts of CFD in conceptual
246

design,” in Sixteenth AIAA Applied Aerodynamics Conference, vol. 98, (Albur-
querque, NM), p. 2513, June 1998.
[133] Michalewicz, Z., Genetic Algorithms + Data Structures = Evolution Pro-
grams. Berlin, Germany: Springer-Verlag, 2nd ed., 1994.
[134] Michalski, R. S., Stepp, R. E., and Diday, E., “A recent advance in
data analysis: Clustering objects into classes characterized by conjunctive con-
cepts,” in Progress in Pattern Recognition, vol. 1, pp. 33–55, Amsterdam, The
Netherlands: North-Holland Publishing, 1981.
[135] Mitchell, T. M., Machine Learning.
New York, NY: McGraw-Hill Sci-
ence/Engineering/Math, 1 ed., 1997.
[136] Mockus, J., Tiesis, V., and Zilinskas, A., “The application of bayesian
methods for seeking the extremum,” in Towards global optimisation. II, pp. 117–
29, Amsterdam, Netherlands: North-Holland Publishing, 1978.
[137] Moore, D. S., The Basic Practice of Statistics. New York, NY: W.H. Freeman,
2nd ed. ed., 2000.
[138] Mouille, R., “The fenestron, shrouded tail rotor of the SA. 341 gazelle,”
Journal of the American Helicopter Society, vol. 15, pp. 31–37, Oct. 1970.
[139] Mouille, R. and d’Ambra, F., “The ’Fenestron’ a shrouded tail rotor con-
cept for helicopters,” in Proceedings of the 42nd American Helicopter Society
International Annual Forum, (Alexandria, VA), June 1986.
[140] Murugan, M., Ganguli, R., and Harursampath, D., “Surrogate based
design optimisation of composite aerofoil cross-section for helicopter vibration
reduction,” Aeronautical Journal, vol. 116, pp. 709–725, July 2012.
[141] Murugan, S. and Ganguli, R., “Aeroelastic stability enhancement and vi-
bration suppression in a composite helicopter rotor,” Journal of Aircraft, vol. 42,
pp. 1013–1024, July 2005.
[142] Nadler, M. and Smith, E. P., Pattern Recognition Engineering. New York,
NY: Wiley-Interscience, 1st ed., 1993.
[143] of Defense, D., Systems Engineering Fundamentals. Fort Belvoir, VA: De-
fense Acquisition University, 2001.
[144] Pareto, V., Cours d’`Economie Politique Profess´e `a l’Universit´e de Lausanne.
1896.
[145] Ponweiser, W., Wagner, T., Biermann, D., and Vincze, M., “Multi-
objective optimization on a limited budget of evaluations using Model-Assisted
S-Metric selection,” in 10th International Conference on Parallel Problem Solv-
ing from Nature, vol. 5199 LNCS, (Dortmund, Germany), pp. 784–794, Sept.
2008.
247

[146] Prouty, R. W., Helicopter Performance, Stability, and Control. Malabar,
FL: Krieger Publishing Company, 1995.
[147] Pugh, S., Total Design: Integrated Methods for Successful Product Engineer-
ing. Wokingham, England: Addison-Wesley Pub., 1991.
[148] Quesada, I. and Grossman, I. E., “An LP/NLP based branch and bound
algorithm for convex MINLP optimization problems,” Computers & Chemical
Engineering, vol. 16, pp. 937–47, Oct. 1992.
[149] Radcliffe, N. J., “Forma analysis and random respectful recombination,” in
Proceedings of the 4th International Conference on Genetic Algorithms, (San
Mateo, CA), pp. 222–229, Morgan-Kaufmann Publ, Inc., July 1991.
[150] Rand, O. and Khromov, V., “Helicopter sizing by statistics,” Journal of the
American Helicopter Society, vol. 49, pp. 300–317, July 2004.
[151] Raymer, D. P., Aircraft Design: A Conceptual Approach. Washington D.C.,
USA: American Institute of Aeronautics and Astronautics, 4th ed., 2006.
[152] Regulwar, D. G., “Diﬀerential evolution algorithm with application to opti-
mal operation of multipurpose reservoir,” Journal of Water Resource and Pro-
tection, vol. 2, pp. 560–568, June 2010.
[153] Rikards, R., Abramovich, H., Kalnins, K., and Auzins, J., “Surro-
gate modeling in design optimization of stiﬀened composite shells,” Composite
Structures, vol. 73, pp. 244–251, May 2006.
[154] Robinson, F., “Increasing tail rotor thrust and comments on other yaw control
devices,” Journal of the American Helicopter Society, vol. 15, pp. 46–52, Oct.
1970.
[155] Robinson, T. D., Eldred, M. S., Willcox, K. E., and Haimes, R.,
“Surrogate-Based optimization using multiﬁdelity models with variable param-
eterization and corrected space mapping,” AIAA Journal, vol. 46, pp. 2814–
2822, Nov. 2008.
[156] Rohl, P. J., Kumar, D., Dorman, P., Cesnik, C. E., and Sutton, M.,
“A composite rotor blade structural design environment for aeromechanical
assessments in conceptual and preliminary design,” in Proceedings of the 68th
American Helicopter Society International Annual Forum, (Fort Worth, TX),
pp. 632–645, May 2012.
[157] Roskam, J., Airplane Design Part I: Preliminary Sizing of Airplanes. Lawer-
ence, KS: DARcorporation, 3rd ed., 2003.
[158] Rousis, D., A Pareto Frontier Intersection-based Approach for Eﬃcient Multi-
objective Optimization of Competing Concept Alternatives. PhD thesis, Georgia
Institute of Technology, Atlanta, GA, Aug. 2011.
248

[159] Rudolph, G., “An evolutionary algorithm for integer programming,” in Par-
allel Problem Solving from Nature-PPSN III, vol. 866 of Lecture Notes in Com-
puter Science, pp. 139–148, Berlin, Germany: Springer-Verlag, 1994.
[160] Sacks, J., “Design and analysis of computer experiments,” Statistical Science,
vol. 4, pp. 409–423, Nov. 1989.
[161] Sagan, H., Introduction to the Calculus of Variations. Mineola, NY: Courier
Dover Publications, 1992.
[162] Saijal, K., Ganguli, R., and Viswamurthy, S., “Optimization of heli-
copter rotor using polynomial and neural network metamodels,” Journal of
Aircraft, vol. 48, pp. 553–66, Mar. 2011.
[163] Salonen, M. and Perttula, M., “Utilization of concept selection methods:
A survey of ﬁnnish industry,” in ASME International Design Engineering Tech-
nical Conferences and Computers and Information in Engineering Conference,
(Long Beach, CA), pp. 527–535, Sept. 2005.
[164] Salzberg, S., “A nearest hyperrectangle learning method,” Machine Learning,
vol. 6, pp. 251–276, May 1991.
[165] Schleicher, D. R., “Advanced civil tiltrotor design optimization and issues,”
in Proceedings of the 49th American Helicopter Society International Annual
Forum, (St. Louis, MO), pp. 885–901, May 1993.
[166] Sch¨olkopf, B. and Smola, A., Learning with Kernels Support Vector Ma-
chines, Regularization, Optimization, and Beyond. Cambridge, MA: MIT Press,
2002.
[167] Sch¨utz, M. and Sprave, J., “Application of parallel Mixed-Integer evolu-
tion strategies with mutation rate pooling,” in Proceedings of the Fifth Annual
Conference on Evolutionary Programming, pp. 345–354, Cambridge, MA: MIT
Press, 1996.
[168] Shinoda, P. M., Yeo, H., and Norman, T. R., “Rotor performance of a
UH-60 rotor system in the NASA ames 80- by 120-Foot wind tunnel,” Journal
of the American Helicopter Society, vol. 49, pp. 401–413, Oct. 2004.
[169] Shishko, R., “Developing analogy cost estimates for space missions,” in A
Collection of Technical Papers - AIAA Space 2004 Conference and Exposition,
vol. 2 of A Collection of Technical Papers - AIAA Space 2004 Conference and
Exposition, pp. 1530–1537, American Institute of Aeronautics and Astronautics
Inc., Sept. 2004.
[170] Simpson, T. W., Booker, A. J., Ghosh, D., Giunta, A. A., Koch,
P. N., and Yang, R. J., “Approximation methods in multidisciplinary anal-
ysis and optimization: A panel discussion,” Structural and Multidisciplinary
Optimization, vol. 27, pp. 302–313, June 2004.
249

[171] Simpson, T. W., Poplinski, J. D., Koch, P. N., and Allen, J. K., “Meta-
models for Computer-Based engineering design: Survey and recommendations,”
Engineering With Computers, vol. 17, pp. 129–150, July 2001.
[172] Singpurwalla, N. D. and Booker, J. M., “Membership functions and prob-
ability measures of fuzzy sets,” Journal of the American Statistical Association,
vol. 99, pp. 867–877, Sept. 2004.
[173] Smith, E. M. B. and Pantelides, C. C., “A symbolic Reformulation/Spatial
Branch-and-Bound algorithm for the global optimisation of nonconvex MIL-
NPs,” Computers & Chemical Engineering, vol. 23, pp. 457–478, May 1999.
[174] Sobek II, D. K., Ward, A. C., and Liker, J. K., “Toyota’s principles of
Set-Based concurrent engineering,” Sloan Management Review, vol. 40, pp. 67–
83, Jan. 1999.
[175] Sobester, A., Leary, S., and Keane, A., “On the design of optimization
strategies based on global response surface approximation models,” Journal of
Global Optimization, vol. 33, pp. 31–59, Sept. 2005.
[176] Sobester, A., Leary, S. J., and Keane, A., “A parallel updating scheme for
approximating and optimizing high ﬁdelity computer simulations,” Structural
and Multidisciplinary Optimization, vol. 27, pp. 371–383, July 2004.
[177] Sobieszczanski-Sobieski,
J. and Haftka,
R. T., “Multidisciplinary
aerospace design optimization: Survey of recent developments,” Structural and
Multidisciplinary Optimization, vol. 14, pp. 1–23, Aug. 1997.
[178] Spiegel, M. R., Theory And Problems Of Probability And Statistics (Schaum
S Outline Series). New York, NY: McGraw-Hill Education, 2003.
[179] Stanfill, C. and Waltz, D., “Towards Memory-Based reasoning,” Commu-
nications of the ACM, vol. 29, pp. 1213–1228, Dec. 1986.
[180] Sun, H., Kim, Y., Lee, S., and Lee, D., “Aerodynamic design of helicopter
rotor blade in forward ﬂight using response surface methodology,” Journal of
the American Helicopter Society, vol. 48, pp. 300–304, Oct. 2003.
[181] Sutherland, W. A., Introduction to Metric and Topological Spaces. Oxford,
NY: Oxford Science Publications, 2009.
[182] Tawarmalani, M. and Sahinidis, N. V., Convexiﬁcation and Global Op-
timization in Continuous and Mixed-Integer Nonlinear Programming: Theory,
Algorithms, Software, and Applications. Boston, MA: Kluwer Academic Pub-
lishers, Oct. 2002.
[183] Tian, D. and Deng, N., “Support vector classiﬁcation with nominal at-
tributes,” in Proceedings of the Computational Intelligence and Security - In-
ternational Conference, vol. 3801, (Xian, China), pp. 586–591, Dec. 2005.
250

[184] Tipping, M. E., “Bayesian inference: An introduction to principles and prac-
tice in machine learning,” in Advanced Lectures on Machine Learning, vol. 3176,
pp. 41–62, Berlin, Germany: Springer Berlin, 2004.
[185] Tishchenko, M. N., Nagaraj, V. T., and Chopra, I., “Preliminary design
of transport helicopters,” Journal of the American Helicopter Society, vol. 48,
pp. 71–79, Apr. 2003.
[186] Ulrich, K. T. and Eppinger, S. D., Product Design and Development. New
York, NY: McGraw-Hill, 2nd ed., 2000.
[187] Vapnik, V. N., Statistical Learning Theory. New York, NY: John Wiley &
Sons, 1998.
[188] Viana, F. A., Haftka, R. T., Hamman, R., and Venter, G., “Eﬃcient
global optimization with experimental data: Revisiting the paper helicopter
design,” in Proceedings of the 52nd AIAA/ASME/ASCE/AHS/ASC Structures,
Structural Dynamics and Materials Conference, (Denver, CO), Apr. 2011.
[189] Vitali, R., Haftka, R. T., and Sankar, B. V., “Multiﬁdelity design of
stiﬀened composite panel with a crack,” Structural and Multidisciplinary Opti-
mization, vol. 23, pp. 347–356, May 2002.
[190] Vos, J., Rizzi, A., Darracq, D., and Hirschel, E., “Navier-Stokes solvers
in european aircraft design,” Progress in Aerospace Sciences, vol. 38, Nov. 2002.
[191] Vu, N. A., Lee, J. W., and Shu, J. I., “Aerodynamic design optimization of
helicopter rotor blades including airfoil shape for hover performance,” Chinese
Journal of Aeronautics, vol. 26, pp. 1–8, Jan. 2013.
[192] Wagner, T., Emmerich, N., Deutz, A., and Ponweiser, W., “On
Expected-Improvement criteria for Model-Based Multi-Objective optimiza-
tion,” in 11th International Conference on Parallel Problem Solving from Nature
- PPSN XI, (Krakow, Poland), pp. 718–727, Sept. 2010.
[193] Walsh, J. L., Bingham, G. J., and Riley, M. F., “Optimization methods
applied to the aerodynamic design of helicopter rotor blades,” Journal of the
American Helicopter Society, vol. 32, pp. 39–44, Oct. 1987.
[194] Wang, G. G. and Shan, S., “Review of metamodeling techniques in support
of engineering design optimization,” Journal of Mechanical Design, vol. 129,
pp. 370–380, Apr. 2007.
[195] Wang, J. and Yin, Z., “A ranking Selection-Based particle swarm optimizer
for engineering design optimization problems,” Structural and Multidisciplinary
Optimization, vol. 37, pp. 131–147, Dec. 2008.
251

[196] Wells, V. L. and Han, A. Y., “Acoustic design of rotor blades using a
genetic algorithm,” in AGARD Symposium on Aerodynamics and Aeroacoustics
of Rotorcraft, (Berlin, Germany), pp. 1–10, Oct. 1994.
[197] Westerlund, T. and Pettersson, F., “An extended cutting plane method
for solving convex MINLP problems,” Computers & Chemical Engineering,
vol. 19, Supplement 1, pp. 131–136, 1995.
[198] Westerlund, T., Pettersson, F., and Grossmann, I., “Optimization of
pump conﬁgurations as a MINLP problem,” Computers and Chemical Engi-
neering, vol. 18, pp. 845–858, Sept. 1994.
[199] While, L., Bradstreet, L., and Barone, L., “A fast way of calculating ex-
act hypervolumes,” IEEE Transactions on Evolutionary Computation, vol. 16,
pp. 86–95, Feb. 2012.
[200] While, L., Hingston, P., Barone, L., and Huband, S., “A faster algo-
rithm for calculating hypervolume,” IEEE Transactions on Evolutionary Com-
putation, vol. 10, pp. 29–38, Feb. 2006.
[201] Wilson, D. R. and Martinez, T. R., “Improved heterogeneous distance
functions,” Journal of Artiﬁcial Intelligence Research, vol. 6, pp. 1–34, Jan.
1997.
[202] Xie, W., Nelson, B., and Staum, J., “The inﬂuence of correlation functions
on stochastic kriging metamodels,” in Proceedings of the 2010 Winter Simula-
tion Conference, (Baltimore, MD), pp. 1067–1078, Dec. 2010.
[203] Yan, L., Shen, K., and Hu, S., “Solving mixed integer nonlinear program-
ming problems with Line-Up competition algorithm,” Computers & Chemical
Engineering, vol. 28, pp. 2647–2657, Nov. 2004.
[204] Yang, Q. and Ding, S., “Novel algorithm to calculate hypervolume indicator
of pareto approximation set,” in Advanced Intelligent Computing Theories and
Applications: With Aspects of Contemporary Intelligent Computing Techniques,
vol. 2, pp. 235–244, Berlin, Germany: Springer, 2007.
[205] Yeo, H., Bousman, W. G., and Johnson, W., “Performance analysis of a
utility helicopter with standard and advanced rotors,” Journal of the American
Helicopter Society, vol. 49, pp. 250–270, July 2004.
[206] Yiqing, L., Xigang, Y., and Yongjian, L., “An improved PSO algorithm
for solving Non-Convex NLP/MINLP problems with equality constraints,”
Computers & Chemical Engineering, vol. 31, pp. 153–162, Jan. 2007.
[207] Yokota, T., Gen, M., and Li, Y. X., “Genetic algorithm for Non-Linear
mixed integer programming problems and its applications,” Computers and
Industrial Engineering, vol. 30, pp. 905–917, Sept. 1996.
252

[208] Young, P., Parkinson, S., and Lees, M., “Simplicity out of complexity in
environmental modelling: Occam’s razor revisited,” Journal of Applied Statis-
tics, vol. 23, no. 2-3, pp. 165–210, 1996.
[209] Yuan, K. A. and Friedmann, P. P., “Structural optimization for vibratory
loads reduction of composite helicopter rotor blades with advanced geometry
tips,” Journal of the American Helicopter Society, vol. 43, pp. 246–256, July
1998.
[210] Zadeh, L. A., “Fuzzy sets,” Information and Control, vol. 8, pp. 338–535,
1965.
[211] Zahedi, F., “The analytic hierarchy process - a survey of the method and its
applications,” Interfaces, vol. 16, pp. 96–108, July 1986.
[212] Zeidler, E., Nonlinear Functional Analysis and its Applications. New York,
NY: Springer-Verlag, 1985.
[213] Zitzler, E., Evolutionary Algorithms for Multiobjective Optimization: Meth-
ods and Applications. PhD thesis, Swiss Federal Institute of Technology Zurich,
Zurich, Switzerland, 1999.
[214] Zitzler, E., Brockhoff, D., and Thiele, L., “The hypervolume indicator
revisited: On the design of Pareto-Compliant indicators via weighted integra-
tion,” vol. 4403 LNCS of Lecture Notes in Computer Science, (Matsushima,
Japan), pp. 862–876, Mar. 2007.
[215] Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., and da Fon-
seca, V. G., “Performance assessment of multiobjective optimizers: An anal-
ysis and review,” IEEE Transactions on Evolutionary Computation, vol. 7,
pp. 117–32, Apr. 2003.
[216] Zwicky, F., Morphological Analysis and Construction. New York, NY: Wiley-
Interscience, 1948.
253

