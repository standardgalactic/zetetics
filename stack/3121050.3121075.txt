Text Retrieval based on Least Information Measurement
Weimao Ke
Drexel University
3141 Chestnut St
Philadelphia, Pennsylvania 19104
wk@drexel.edu
ABSTRACT
We developed a new information retrieval framework based on the
Least Information (LI) metric. We derived multiple term weighting
schemes and combined them with a vector space representation for
ad hoc retrieval. Given probability distributions in a collection as
prior knowledge, LI Binary (LIB) quantiﬁes least information due to
the binary occurrence of a term in a document whereas LI Frequency
(LIF) measures least information based on the probability of drawing
a term from a bag of words. Experiments on four benchmark TREC
collections for ad hoc retrieval showed that LIT-based methods
achieved superior performances compared to classic TF*IDF and
BM25, especially for verbose queries and hard search topics. Te
least information theory is a method for entropy-based information
measurement and oﬀers a novel approach for IR modeling.
CCS CONCEPTS
•Information systems →Probabilistic retrieval models; Sim-
ilarity measures; •Mathematics of computing →Information the-
ory;
KEYWORDS
information measure, metric, information retrieval, probability dis-
tribution, entropy, term weighting, relevance, ranking, eﬀectiveness
ACM Reference format:
Weimao Ke. 2017. Text Retrieval based on Least Information Measurement.
In Proceedings of ICTIR’17, October 1–4, 2017, Amsterdam, Netherlands., ,
8 pages.
DOI: htps://doi.org/10.1145/3121050.3121075
1
INTRODUCTION
Shannon’s mathematical theory of communication, commonly
known as the information theory, has been used in a wide spectrum
of areas including digital coding, communication, and information
technology applications [23, 24]. Modeling information as reduc-
tion of entropy (uncertainty) provides a valuable vehicle in the
design and engineering of information systems. In information
retrieval (IR), information and probability theories have provided
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permited. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ICTIR’17, October 1–4, 2017, Amsterdam, Netherlands.
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4490-6/17/10...$15.00
DOI: htps://doi.org/10.1145/3121050.3121075
important guidance to the development of classic techniques such
as TF*IDF, probabilistic retrieval, and language modeling [20].
Despite its broad use, there are assumptions that deﬁne the
boundary of the classic information theory, beyond which its appli-
cation requires careful examination of domain contexts [4, 21]. Te
original purpose of Shannon’s theory, as noted in his master piece,
was for engineering communication systems where the “meaning
of information was considered irrelevant” [23, p. 379]. Informa-
tion retrieval research is centered on the notion of relevance. To
quantify the “relevant amount” of information requires extension
of Shannon’s theory, beter clariﬁcation of the relationship between
information and entropy, and justiﬁcation of this relationship [24].
Although various measures such as mutual information and KL
information (relative entropy) have been adopted, we observe that
several important characteristics about an ideal information quan-
tity in the IR context are yet to be met [14, 31].
In our research, we proposed the Least Information Teory (LIT)
which quantiﬁes the amount of information in probability distribu-
tion changes. Te theory is built on the Shannon entropy but goes
beyond the entropy-reduction notion of information. We have con-
ducted experiments on related clustering and classiﬁcation tasks
and demonstrated strong performances of methods derived from
LIT. Similar to relative entropy, LIT is a non-linear function of
entropy. Te theory demonstrates several characteristic that are
desirable in applications such as information retrieval (IR). In this
study, we apply the new theory in modeling ad hoc retrieval and
show strong experimental results compared to classic TF*IDF and
Okapi BM25 on multiple benchmark IR collections.
2
LIT AND BACKGROUND
Information and probability theories have provided important guid-
ance on the development of classic techniques such as probabilistic
and language modeling [20]. Information-theoretic measures such
as mutual information and Kullback-Leibler (KL) divergence have
also been used for various processes including feature selection
and matching [2, 14, 31].
KL divergence measures information for discrimination between
two probability distributions by quantifying the entropy change
in an asymmetric manner [14]. KL divergence and its derived
methods such as Jensen-Shannon divergence are broadly adopted
in IR research [5, 16].
From an information-centric view, this research aims to develop
a new model for ad hoc information retrieval. By quantifying the
amount of information required to explain probability distribution
changes, the least information theory (LIT) establishes an impor-
tant information measurement and provides a new approach to
evaluate relevance given term distributions in a document vs. in
the collection. Here we present the Least Information Teory (LIT),
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
125

which quantiﬁes information via integration of Shannon entropy
[13].
Let X be prior (initially speciﬁed) probabilities for a set of exhaus-
tive and mutually exclusive inferences: X = [x1,x2,..,xn], where
xi is the prior probability of the ith inference on a given hypoth-
esis. Let Y denote posterior (changed) probabilities afer certain
information is known: Y = [y1,y2,..,yn], where yi is the informed
probability of the ith inference. Uncertainties/entropies of the two
distributions can be computed by Shannon entropy:
H (X )
=
−
n
X
i=1
xi lnxi
(1)
H (Y )
=
−
n
X
i=1
yi lnyi
(2)
Te amount of information obtained from X to Y, in Shannon’s
treatment, can be measured via the reduction of entropy:
∆H
=
H (X ) −H (Y )
(3)
We deﬁne dHi as the amount of entropy change due to a tiny
change dpi of probability pi:
dHi
=
−lnpi dpi
(4)
Every tiny change in the probabilities requires some explana-
tion (information). Aggregating (integrating) the small changes
of uncertainty leads to the amount of information required for a
macro-level change. Te least amount of information Ii required to
explain the probability change of the ith inference as the integra-
tion (aggregation) of all tiny absolute (positive) changes of entropy
dHi:
Ii
=

Z yi
xi
dHi

=
yi (1 −lnyi ) −xi (1 −lnxi )
(5)
where xi is the initial probability of the ith inference and yi the
posterior probability of the same inference. Te total Least Infor-
mation I is the sum of partial least information for every inference:
I
=
n
X
i=1
Ii
=
n
X
i=1
yi (1 −lnyi ) −xi (1 −lnxi )
(6)
where n is the number of inferences, xi is the initially speciﬁed
probability of the ith inference, and yi the revised probability of
the ith inference.
2.1
Important Model Characteristics
Based on Equation 6, several important characteristics of least in-
formation can be observed. We summarize some of these character-
istics below.
• Absolute information and symmetry: Te amount of least
information required for a probability change from X to
Y is the same as that from Y to X, though their semantic
meanings are diﬀerent.
• Addition of continuous change: Amounts of least infor-
mation for small, continuous probability changes in the
same semantic directions add linearly to the amount of
least information responsible for the overall change. In
short, I (X →Z) = I (X →Y ) + I (Y →Z), if and only if
X →Y and Y →Z are in the same semantic direction.
• Unit Information: In the special case when there are two
equally possible inferences, the amount of least information
needed to explain an outcome (certainty) is one: I (p1 =
p2 = 1
2 →p1 = 1) = 1, regardless of the log base in the
equation.
• In the special case of reducing uncertain inferences to cer-
tainty (with the ultimate case):
– With equally likely inferences, when there are more
choices, the least information needed to explain an
outcome is larger.
– Te less likely the outcome, the larger the amount of
least information needed to explain it.
• Zero least information: Te amount of least information
is zero if and only if there is no change in the probability
distribution (identical distributions).
2.2
Least Information Modeling for IR
In this work we apply the least information theory (LIT) to ad hoc
information retrieval (IR), particularly for term weighting. In the
bag-of-words approach to IR, a document can be viewed as a set of
terms with probabilities (estimated by frequencies) of occurrence.
While the entire collection represents the domain in which searches
are conducted, each document contains various pieces of informa-
tion which diﬀerentiate itself from other documents in the domain.
By analyzing a term’s probability (frequency) in a document vs.
that in the collection, we can compute information presented by
the document in the term to weight the term. In other words, tak-
ing domain distributions as prior knowledge, we can measure the
amount of least information conveyed by a speciﬁc document when
it is observed.
We conjecture that the larger amount least information is needed
to explain a term’s appearance in a document, the more heavily the
term should be weighted to represent the document. Hence, we
transform the question of document representation into weighting
terms according to their amounts of least information in documents.
In this study, we propose two speciﬁc weighting methods, one based
on a binary representation of term occurrence (0 vs. 1) and the
other based on term frequencies. Tese methods will be combined
for vector representation.
2.2.1
LI Binary (LIB) Model. In the binary model, a term either
occurs or does not occur in a document. If we randomly pick a
document from the collection, the chance that a term ti appears in
the document can be estimated by the ratio between the number of
documents containing the term ni (i.e., document frequency) and
the total number of documents N. Let p(ti |C) = ni/N denotes the
probability of term ti occurring in a randomly picked document
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
126

in collection C; p( ¯ti |C) is the probability that the term does not
appear:
p( ¯ti |C) = 1 −p(ti |C) = 1 −ni/N
When a speciﬁc document d is observed, it becomes certain
whether a term occurs in the document or not. Hence the term
probability given a speciﬁc document p(ti |d) is either 1 or 0. We
deﬁne дi as a function of probability pi:
дi
=
pi (1 −lnpi )
(7)
Te least amount of information from observing term ti in docu-
ment d can be computed by:
I (ti,d)
=
д(ti |d) −д(ti |C)
+д( ¯ti |d) + д( ¯ti |C)
(8)
Te above equation gives the amount of information a term
conveys in a document regardless of its semantic direction. When a
query term ti does not appear in document d, the least information
associated with the term should be treated as negative because it
makes the document less relevant to the term. Hence, the ranking
function should not only consider the amount of information but
also the sign (positive vs. negative) of the quantity. Hence, LI Binary
(LIB) is computed by:
LIB2(ti,d)
=
д(ti |d) −д(ti |C)
−д( ¯ti |d) −д( ¯ti |C)
(9)
For term weighting, we are more interested in the likelihood of
a term appearing in a document. Keeping only quantities related
to ti (and removing those associated with ¯ti), we simplify the LIB
equation to:
LIB(ti,d)
=
д(ti |d) −д(ti |C)
(10)
=
д(ti |d) −ni
N

1 −ln ni
N

(11)
Te total least information of all query terms in the document d
is computed by:
LIB(q,d)
=
X
ti ∈q
LIB(ti,d)
(12)
Te quantity LIB(ti,d) depends on the observation of term ti in
the document: д(ti |d) is 1 when ti appears in document d and 0 if
otherwise, according to Equation 7. Tat is:
LIB(ti,d) =

1 −ni
N

1 −ln ni
N

ti ∈d
−ni
N

1 −ln ni
N

ti < d
(13)
where ni is the document frequency of term ti and N is the total
number of documents. Te larger the LIB, the more information
the term contributes to the document and should be weighted more
heavily in the document representation. LIB is similar in spirit to
IDF and its value represents the discriminative power of the term
when it appears in a document.
2.2.2
LI Frequency (LIF) Model. In LI Frequency (LIF) model,
we use term frequencies to model least information. Treating a
document collection C as a meta-document, the probability of a
randomly picked term from the collection being a speciﬁc term ti
can be estimated by: p(ti |C) = Fi/L, where Fi is the total number
of occurrences of term ti in collection C and L the overall length of
C (i.e., the sum of all document lengths).
When a speciﬁc document d is observed, the probability of pick-
ing term ti from this document can be estimated by: p(ti |d) =
t fi,d/Ld, where t fi,d is the number of times term ti occurs in doc-
ument d and Ld is the length of the document. Again, for each
term ti, there are two exclusive inferences, namely the randomly
picked term being the speciﬁc term (ti) or not (¯ti). To quantify a
term’s LIF weight, we measure least information that explains the
change from the term’s probability distribution in the collection to
its distribution in the document in question:
LIF2(ti,d)
=
д(ti |d) −д(ti |C)
+д( ¯ti |C) −д( ¯ti |d)
(14)
We focus on the quantities д(ti |d) and д(ti |C) to estimate least
information of each term when a speciﬁc document is observed.
Without quantities д( ¯ti |C) and д( ¯ti |d), LIF is computed by:
LIF (ti,d)
=
д(ti |d) −д(ti |C)
(15)
=
t fi,d
Ld
(1 −ln t fi,d
Ld
)
−Fi
L (1 −ln Fi
L )
(16)
Hence, the LI Frequency (LIF) ranking score can be computed
by the sum of least information in all query terms:
LIF (q,d)
=
X
ti ∈q
д(ti |d) −д(ti |C)
(17)
=
X
ti ∈q
t fi,d
Ld
(1 −ln t fi,d
Ld
)
−
X
ti ∈q
Fi
L (1 −ln Fi
L )
(18)
where t fi,d is term frequency of term ti in document d and Ld is
the document length. Fi is collection frequency of term ti (sum of
term frequencies in all documents) whereas L is the overall length
of all documents. LIF can be compared to modeling term frequen-
cies with document length and collection frequency normalization.
For now, we use raw term frequencies with the Maximum Likeli-
hood Estimator (MLE) to estimate probabilities and do not use any
smoothing techniques to ﬁne tune the estimates.
2.2.3
Fusion of LIB & LIF. While LIB uses binary term occur-
rence to estimate least information a document carries in the query
terms, LIF measures the information based on term frequency. Te
two are related quantities with diﬀerent focuses. As discussed,
the LIB quantity is similar in spirit to IDF (inverse document fre-
quency) whereas LIF can be seen as a means to normalize TF (term
frequency). Given the classic fusion of TF and IDF, we reason that
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
127

combining the two will potentiate each quantity’s strength for
term weighting, ultimately leading to improved document rank-
ing. Hence we propose three fusion methods to combine the two
quantities by addition and multiplication:
(1) LIB+LIF: To weight a term, we simply add LIB and LIF
together by treating them as two separate pieces of infor-
mation. Te ranking score of a document is then the sum
of all LIB+LIF quantities in the query terms.
(2) LIB*LIF: In this fusion method, we follow the idea of TF*IDF
by multiplying LIB and LIF quantities for each term. Be-
cause individual least information values fall in the range
of [−1,1] and can be negative, we normalize LIB and LIF
values to [0,2] by adding 1 to each before multiplication.
Again, document ranking is then based on the linear sum
of LIB*LIF quantities in the query terms.
(3) LICos: Tis method combines LIB+LIF with cosine similar-
ity in the vector space model (VSM). We use LIB+LIF for
term weights to represent documents in VSM and rank doc-
uments based on their Cosine coeﬃcients with the binary
vector representation of a query.
Tese fusion methods allow us to examine potential strengths
and weaknesses of least information modeling for IR. We study
LIB and LIF individually as well as the above fusion methods in
experiments. And given the eﬀectiveness of TF*IDF and especially
its BM25 variation in traditional ad hoc retrieval experiments, we
use them as baselines in the experiments.
3
EXPERIMENTAL SETUP
3.1
Data Collections and Topics
We used the following data sets from the Linguistic Data Con-
sortium and NIST for retrieval experiments: the TIPSTER corpus
(Disks 2 and 3), TREC Disks 4 and 5, and the AQUAINT I corpus
(roughly a million news documents from New York Times, AP, and
Xinhua [29]). Tese data had been widely used in TREC for ad hoc
retrieval experiments. We relied on the following TREC topics and
relevance bases for IR evaluation:
• TREC 2 routing topics 51 - 100 with title, description, sum-
mary, narrative, and concepts (disk 3) [22];
• TREC 4 ad hoc topics 201 - 250 with natural language
descriptions only (disks 2 and 3) [7];
• TREC 7 ad hoc topics 351 - 400 with title, description, and
narrative (disks 4 and 5 minus the Congressional Record)
[30];
• TREC 2005 HARD/Robust 50 topics with title, description,
and narrative ranging from 303 - 689 (AQUAINT I data)
[29].
Tese collections represent a diversity of text data and query
tasks. In TREC 2, for example, the concepts ﬁeld in 51 - 100 topics
contains a verbose list of concepts to represent each search topic.
Text queries automatically generated from the concept lists are
likely to be more accurate than general descriptions in sentences.
On the other hand, TREC 4 topics 201 - 250 only have natural
language descriptions of queries. TREC 2005 HARD and Robust
topics were developed as a list of diﬃcult topics from previous
years’ ad hoc experiments. Using these diverse data and topics
enabled a relatively thorough examination of the proposed methods’
eﬀectiveness in various domain and task contexts.
3.2
Experimental System
We implemented the retrieval ranking methods using the Lucene
core search engine library in Java [8]. We reused the Okapi BM25 im-
plementation reported in [18] and validated by [20], which achieved
highly competitive results in recent years’ TREC competitions. We
set parameter values b = 0.75 and k1 = 1.5 for BM25, according
to existing research on related data. In addition, we developed the
following proposed methods for Lucene scoring (ranking): LIB, LIF,
LIB+LIF, LIB*LIF, and LICos. Two classic TF*IDF methods, one with
document length normalization (TFN *IDF) and the other without
(TF*IDF), were also implemented as baselines. We performed stan-
dard tokenization, casefolding, and stop-word removal for indexing.
For each data collection, one set of experiments were conducted
with stemming and the other without it.
3.3
Evaluation Metrics
We used human relevance judgment (QRELs) developed for TREC 2,
TREC 4, TREC 7, and TREC 2005 HARD (Robust) tracks as the gold
standard for each set of experiments. We compared the proposed
methods with classic TF*IDF and Okapi BM25 methods. Evalu-
ation metrics included mean average precision with arithmetic
averaging (MAP) and geometric (gMAP), best precision at rank 10,
normalized discounted cumulative gain at 10 (nDCG10), and recall
precision. While arithmetic average MAP provides a simple mean
score across multiple queries, the geometric average (gMAP) is
sensitive to poorly performed tasks and is a very useful metric de-
veloped for 2005 HARD track [29]. NDCG favors early retrieval of
highly relevant documents in a ranked list and has become widely
adopted for ranked retrieval evaluation [9].
4
EXPERIMENTAL RESULTS
Figure 1 provides an overview of major results. From the plots, the
proposed LICos method appeared to have achieved best results and
was beter than BM25 in most experiments. All LIB related methods
such as LIB, LIB+LIF, and LIB*LIF overwhelmingly outperformed
TF*IDF methods, especially in TREC2, TREC7, and TREC’05 HARD
(Robust) experiments. In many cases, the LIB-related methods were
more than 100% beter than TF*IDF baselines (i.e., relative scores
> 2). LICos consistently outperformed BM25 in terms of дMap in
all experiments, indicating that it did relatively well with poorly
performed topics.
In sections 4.1 - 4.4, we discuss detailed experimental results on
the four benchmark test collections. In each of Tables 1 - 6, one
set of experiments were conducted with stemming and the other
without. Best scores in each evaluation metric are highlighted in
bold fonts. Section 4.5 presents our observation about the impact
of query verbosity on proposed methods’ eﬀectiveness.
4.1
TREC 2 Topics on Disk 3
Table 1 shows results from experiments on disk 3. In TREC 2 topics,
each query was described using a verbose list of concepts (good
keywords). With these manually picked concept terms, which are
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
128

0.5
1.0
1.5
2.0
2.5
3.0
gMAP
MAP
P10
nDCG
Rpr
0
1
2
3
4
5
gMAP
MAP
P10
nDCG
Rpr
TREC 2 Experiments
TREC 4 Experiments
2
4
6
8
10
gMAP
MAP
P10
nDCG
Rpr
Methods:
BM25
TF*IDF
TFn*IDF
LIB
LIF
LIB+LIF
LIB*LIF
LICos
1.0
1.5
2.0
2.5
3.0
gMAP
MAP
P10
nDCG
Rpr
TREC 7 Experiments
TREC 2005 HARD/Robust
Figure 1: Overview of Experimental Results (with stem-
ming). X has evaluation metrics. Y is relative performance
score in each metric as a ratio to the TFN *IDF baseline.
TFN *IDF scores are always 1 as the baseline. A score at 2,
for example, indicates it is twice the baseline score.
Method
gMAP
MAP
P10
nDCG
RPR
Concept-only Search Without Stemming
BM25
0.288
0.407
0.597
0.504
0.451
TF*IDF
0.090
0.187
0.127
0.0911
0.182
TFN *IDF
0.125
0.300
0.328
0.241
0.304
LIB
0.223
0.348
0.516
0.417
0.389
LIF
0.125
0.294
0.309
0.251
0.294
LIB+LIF
0.236
0.357
0.545
0.434
0.399
LIB*LIF
0.240
0.361
0.562
0.446
0.402
LICos
0.301
0.413
0.635
0.523
0.464
Concept-only Search With Stemming
BM25
0.281
0.399
0.565
0.488
0.442
TF*IDF
0.0711
0.164
0.132
0.0822
0.160
TFN *IDF
0.110
0.282
0.310
0.238
0.286
LIB
0.173
0.313
0.467
0.374
0.352
LIF
0.064
0.278
0.313
0.244
0.280
LIB+LIF
0.162
0.331
0.494
0.406
0.370
LIB*LIF
0.185
0.341
0.508
0.416
0.372
LICos
0.309
0.423
0.659
0.554
0.477
Table 1: TREC 2 Concept-only Retrieval (Disk 3)
overall quite precise in deﬁning the topic, LICos (least informa-
tion with cosine similarity) outperformed all the other methods in
every evaluation metric we used. Stemming appeared to further
improve LICos’s eﬀectiveness. Overall, BM25 also performed very
well and was second only to LICos in most cases, followed closely
by LIB*LIF. Most proposed methods based on least information,
especially LIB*LIF and LIB+LIF, outperformed ordinary TF*IDF and
TFN *IDF (with length normalization of TF) by a good margin.
4.2
TREC 4 Topics on Disks 2&3
Method
gMAP
MAP
P10
nDCG
RPR
Desc-only Search Without Stemming
BM25
0.155
0.318
0.515
0.409
0.380
TF*IDF
0.0178
0.108
0.141
0.0674
0.117
TFN *IDF
0.0212
0.156
0.145
0.0914
0.163
LIB
0.0327
0.126
0.216
0.117
0.133
LIF
0.0052
0.135
0.139
0.086
0.141
LIB+LIF
0.0288
0.133
0.198
0.120
0.143
LIB*LIF
0.031
0.142
0.201
0.132
0.156
LICos
0.191
0.295
0.536
0.393
0.376
Desc-only Search With Stemming
BM25
0.190
0.316
0.501
0.394
0.370
TF*IDF
0.0122
0.122
0.176
0.0728
0.126
TFN *IDF
0.0584
0.155
0.129
0.0853
0.160
LIB
0.0282
0.0968
0.175
0.0905
0.105
LIF
0.00515
0.124
0.113
0.0768
0.125
LIB+LIF
0.0187
0.113
0.210
0.117
0.128
LIB*LIF
0.0235
0.123
0.202
0.125
0.135
LICos
0.217
0.304
0.559
0.403
0.391
Table 2: TREC 4 Desc-only Retrieval (Disks 2&3)
Table 2 shows results from TREC 4 experiments on disks 2 & 3.
Again, LICos continued to dominate best scores, especially when
stemming was used. TREC 4 topics only had descriptions writen
in natural language sentences. Stemming improved LICos eﬀective-
ness but slightly degraded BM25 performance.
While the two had very close scores in several metrics, LICos
was consistently beter than BM25 in terms of gMAP (geometric
averaging MAP) and P10. Te evaluation metric gMap is biased
toward poorly performed queries (hard tasks). LICos appeared to
perform beter on diﬃcult topics than BM25 did to achieve a higher
gMap. We shall see later most of the proposed methods performed
well on TREC 2005 HARD/Robust’s topics, which were considered
diﬃcult topics in TRECs.
4.3
TREC 7 Topics on Disks 4&5
In TREC 2 and TREC 4 experiments, we used two diﬀerent ﬁelds/sources,
namely concepts and description, to form long queries. In TREC 7 ex-
periments, we used the title ﬁeld to examine the eﬀectiveness of the
proposed methods with short queries. Table 3 shows results from
these experiments, in which BM25 achieved slightly beter scores in
P10 and nDCG10, which favor early retrieval of relevant documents.
However, with short queries based on title, LICos performed much
beter than BM25 did in terms of gMAP, which biased toward poorly
performed topics. Tis again indicates potential advantage of the
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
129

Method
gMAP
MAP
P10
nDCG
RPR
Title-only Search Without Stemming
BM25
0.0682
0.242
0.482
0.337
0.360
TF*IDF
0.0334
0.113
0.219
0.107
0.170
TFN *IDF
0.0129
0.087
0.188
0.0844
0.172
LIB
0.0653
0.236
0.349
0.250
0.282
LIF
0.012
0.0813
0.150
0.0803
0.133
LIB+LIF
0.0665
0.248
0.411
0.305
0.331
LIB*LIF
0.0662
0.247
0.429
0.317
0.334
LICos
0.173
0.251
0.466
0.316
0.346
Title-only Search With Stemming
BM25
0.0681
0.242
0.479
0.346
0.374
TF*IDF
0.0295
0.099
0.190
0.0963
0.151
TFN *IDF
0.0150
0.079
0.228
0.0931
0.161
LIB
0.0615
0.215
0.299
0.211
0.265
LIF
0.0110
0.0744
0.159
0.0816
0.132
LIB+LIF
0.066
0.226
0.415
0.287
0.325
LIB*LIF
0.0662
0.229
0.420
0.311
0.327
LICos
0.162
0.232
0.466
0.323
0.347
Table 3: TREC 7 Title-only Retrieval (Disks 4&5)
proposed methods in search tasks that may have been challenging
to traditional methods. Te other proposed methods such as LIB,
LIB+LIF and LIB*LIF came closely below BM25 but consistently
outperformed TF*IDF methods by a large margin in each evaluation
metric.
4.4
TREC 2005 HARD/Robust
Experiments on the earlier TREC collections above showed the
proposed methods, especially the LICos method, performed very
competitively and in many cases outperformed a well-tuned Okapi
BM25. Now we discuss experiments on the more recent TREC 2005
HARD/Robust collection, in which 50 topics are considered diﬃcult
retrieval tasks. We used title, description, and title+description as
queries in the experiments.
Table 4 shows retrieval performances using the topic title ﬁeld
for query representation. Te proposed methods, especially LIB
and LICos, achieved best results in terms of gMAP, MAP, and RPR.
BM25 and TF*IDF, without stemming, performed slightly beter in
P10 and nDCG10. Overall the proposed methods dominated best
results, especially when terms were stemmed.
When we used topic descriptions for query representation, as
shown in Table 5, the proposed methods outperformed BM25 and
TF*IDF methods across all metrics. In particular, LIB, LIB+LIF, and
LICos produced very competitive results.
When both title and description ﬁelds were used (combined)
for queries, the proposed methods demonstrated an even larger
advantage over BM25 and TF*IDF, as shown in Table 5. Whereas
LIB, LIB+LIF, and LIB*LIF all outperformed the classic methods,
LICos (with stemming) achieved a score roughly 20% higher than
that of BM25 in every metric.
TREC 2005 HARD/Robust topics represent diﬃcult information
needs, for which query speciﬁcation is challenging. Te proposed
Method
gMAP
MAP
P10
nDCG
RPR
Title-only Search Without Stemming
BM25
0.172
0.278
0.416
0.271
0.303
TF*IDF
0.174
0.282
0.410
0.301
0.301
TFN *IDF
0.0823
0.194
0.231
0.147
0.210
LIB
0.192
0.309
0.409
0.280
0.322
LIF
0.0933
0.226
0.228
0.154
0.227
LIB+LIF
0.195
0.301
0.402
0.273
0.326
LIB*LIF
0.194
0.300
0.384
0.269
0.330
LICos
0.225
0.301
0.361
0.282
0.340
Title-only Search With Stemming
BM25
0.166
0.263
0.381
0.273
0.296
TF*IDF
0.160
0.262
0.360
0.281
0.285
TFN *IDF
0.056
0.175
0.197
0.118
0.191
LIB
0.194
0.298
0.388
0.246
0.316
LIF
0.0727
0.186
0.216
0.124
0.195
LIB+LIF
0.186
0.284
0.410
0.278
0.313
LIB*LIF
0.186
0.283
0.406
0.274
0.315
LICos
0.214
0.283
0.401
0.295
0.321
Table 4: TREC’05 Title-only Retrieval (AQUAINT)
Method
gMAP
MAP
P10
nDCG
RPR
Desc-only Search Without Stemming
BM25
0.204
0.275
0.336
0.239
0.290
TF*IDF
0.203
0.262
0.386
0.273
0.286
TFN *IDF
0.0718
0.193
0.266
0.176
0.205
LIB
0.205
0.308
0.404
0.280
0.332
LIF
0.058
0.232
0.289
0.186
0.234
LIB+LIF
0.203
0.303
0.385
0.263
0.328
LIB*LIF
0.231
0.300
0.354
0.249
0.325
LICos
0.243
0.308
0.401
0.292
0.338
Desc-only Search With Stemming
BM25
0.209
0.293
0.409
0.316
0.315
TF*IDF
0.202
0.266
0.350
0.270
0.283
TFN *IDF
0.0663
0.197
0.243
0.159
0.209
LIB
0.232
0.353
0.460
0.318
0.377
LIF
0.0624
0.236
0.293
0.195
0.243
LIB+LIF
0.275
0.351
0.505
0.332
0.387
LIB*LIF
0.262
0.337
0.477
0.324
0.370
LICos
0.259
0.330
0.518
0.377
0.371
Table 5: TREC’05 Desc-only Retrieval (AQUAINT)
methods appeared to perform beter with these tougher tasks, as
was so suggested by the higher gMAP scores in earlier experiments.
Te methods also performed very competitively with long queries
(concepts and descriptions). Overall, stemming improved the pro-
posed methods’ eﬀectiveness.
Note that in all experiments, the proposed ranking methods
based on least information were used without any tuning. Neither
did we use additional data sources for query expansion. Although
our results remain very competitive compared to reported results in
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
130

Method
gMAP
MAP
P10
nDCG
RPR
Title+Desc Search Without Stemming
BM25
0.226
0.297
0.458
0.329
0.338
TF*IDF
0.210
0.274
0.386
0.289
0.293
TFN *IDF
0.0886
0.192
0.258
0.162
0.201
LIB
0.260
0.329
0.445
0.336
0.355
LIF
0.102
0.221
0.261
0.186
0.225
LIB+LIF
0.264
0.331
0.447
0.320
0.360
LIB*LIF
0.263
0.328
0.425
0.320
0.359
LICos
0.264
0.331
0.490
0.394
0.397
Title+Desc Search With Stemming
BM25
0.217
0.291
0.458
0.362
0.332
TF*IDF
0.202
0.267
0.349
0.271
0.286
TFN *IDF
0.0945
0.185
0.228
0.142
0.198
LIB
0.261
0.328
0.483
0.337
0.381
LIF
0.145
0.223
0.284
0.191
0.242
LIB+LIF
0.271
0.336
0.510
0.354
0.387
LIB*LIF
0.272
0.335
0.493
0.366
0.387
LICos
0.278
0.340
0.555
0.439
0.421
Table 6: TREC’05 Title+Desc Runs (AQUAINT)
TREC, this is not a fair comparison because participating systems
in TREC were ofen trained and tuned, sometimes with additional
data. In TREC 2005 Robust track, for example, additional resources
such as WordNet and Wikipedia were reportedly used to boost
results [17].
4.5
Impact of Qery Verbosity
We observed that query verbosity had an impact on the proposed
methods’ retrieval eﬀectiveness. With (longer) verbose queries,
methods such as LICos, LIB+LIF, and LIB*LIF appeared to outper-
form baseline methods by a greater margin. In TREC’05 experi-
ments, for example, LICos with queries based on the description
ﬁeld produced P10 and nDCG10 scores nearly 30% higher than those
based on title queries (see Figure 2). Te improvement was much
larger than that of BM25. With verbose queries, having good terms
(e.g., using the concepts ﬁeld and adding title to description) for query
representation also appeared to strengthen the proposed methods’
advantage over BM25 and TF*IDF.
5
REFLECTIONS ON RELATED WORK
Term probability distribution analysis has been an important part
of information retrieval modeling. Term frequency and document
frequency are basic examples of these frequency (probability) dis-
tributions. While term frequency (TF) may indicate the degree of a
document’s association with a term, inverse document frequency
(IDF) is a manifestation of a term’s speciﬁcity, key to determine the
term’s value toward weighting and relevance ranking [27]. Te two
quantities we developed from the least information theory, namely
LI Binary (LIB) and LI Frequency (LIB), can be related to IDF and
TF, though their formulations are very diﬀerent.
While a term’s IDF is equivalent to the mutual information be-
tween the term and the collection [25], the probabilistic retrieval
0.2
0.3
0.4
0.5
0.6
title
desc
title+desc
0.1
0.2
0.3
0.4
0.5
title
desc
title+desc
Methods:
BM25
TF*IDF
TFn*IDF
LIB
LIF
LIB+LIF
LIB*LIF
LICos
P10 (y-axis)
nDCG10 (y-axis)
Figure 2:
Retrieval eﬀectiveness vs.
query verbosity
(TREC’05). X denotes query verbosity, ranging from title-
only, desc-only, to title+desc query representations. Y is re-
trieval performance in terms of P10 and nDCG10.
framework provides an important theoretical ground to IDF weights
[19]. Mutual information can be interpreted as relative entropy that
quantiﬁes the diﬀerence between the joint probabilities and product
probabilities of two random variables [6]. Further development of
notions around information-theoretic entropy led to theories such
as maximum entropy and minimum (mutual) information principles,
providing important guidance to inferential statistics for retrieval
modeling [3, 10, 11, 26].
IDF can also be transformed into Kullback-Leibler (KL) informa-
tion between term probability distributions in a document and in
the collection [1], similar to the modeling of LIB in this work. KL di-
vergence (relative entropy) measures information for discrimination
between two probability distributions by quantifying the entropy
change in a non-symmetric manner [14]. Te non-symmetry of KL
divergence is due to the assumption that one of the two distribu-
tions is considered closer to the ultimate case and the information
quantity should be weighted by that distribution. Tis leads to the
consequence that the (absolute) amount of information is diﬀerent
if simply the direction of change is diﬀerent.
KL information has been used to support term-weighting models
by measuring the divergence of the actual term distribution from
that obtained under a random process [2]. Such models now serve as
good baseline alternatives to the standard TF*IDF model. Research
has also employed KL information in language modeling to measure
the diﬀerence between document and query models for ranking
and demonstrated strong empirical results [15, 28]. We believe
that the least information can be integrated as an alternative to KL
information with the aforementioned approaches.
Te least information theory (LIT) quantiﬁes information due to
probability changes as a symmetric function of two distributions.
Just as the probabilistic retrieval framework and KL information
oﬀer theoretical justiﬁcation for IDF, least information provides the
theory from which LIB is developed. While IDF can be obtained
from the binary independent (probabilistic) model, LIB is derived
from a binary model of least information. Tey both address a term’s
discriminative power or speciﬁcity. However, LIB falls in the range
of [0,1] without normalization – it is close to 1 for extremely rare
terms and 0 for stop-words.
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
131

6
CONCLUSION
Applying the least information theory (LIT) in information retrieval,
we developed multiple quantities for document representation
based on a term’s probability distributions in a document vs. in the
collection. Particularly, LI Binary (LIB) quantiﬁes least information
due to the binary occurrence of a term in a document, i.e., whether
the term appears in the document or not. LI Frequency (LIF), on
the other hand, measures the amount of least information based on
the likelihood of drawing a term from a bag of words. While LIB
and LIF are similar in spirit to classic IDF and TF respectively, the
formulation is very diﬀerent. Tree additional quantities, namely
LIB+LIF, LIB*LIF, and LICos, were developed for term weighting
and document ranking.
Ad hoc retrieval experiments on four benchmark TREC collec-
tions showed that the proposed methods performed very competi-
tively and in most cases outperformed classic TF*IDFs and a well-
tuned BM25. LIT-based methods such as LICos and LIB+LIF were
particularly eﬀective with good query terms (e.g., using concepts),
verbose queries (e.g., using description + title), and in diﬃcult tasks
(e.g., on TREC 2005 HARD/Robust collection). Note that none of
the proposed methods based on least information involved train-
ing or tuning. For Okapi BM25, on the other hand, we adopted
parameters that had demonstrated strong performances in existing
experiments. Least information oﬀers a means to quantify informa-
tion and presents a new way of thinking for modeling information
processes. We have used the theory in other important tasks such
as clustering and classiﬁcation, and obtained strong experimental
results [12, 13]. While other IR models can be derived from LIT,
the least information theory can also be used with existing frame-
works. For example, it can be integrated with document and query
language models, for which KL divergence has been used. With
demonstrated experimental results in this work, broader research
on least information modeling for IR is very promising.
ACKNOWLEDGMENTS
Tis work was supported in part by the National Science Foundation
under grant no. 1646955.
REFERENCES
[1] Akiko Aizawa. 2000. Te feature quantity: an information theoretic perspective
of TFIDF-like measures. In SIGIR’00. 104–111.
[2] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of
Information Retrieval Based on Measuring the Divergence from Randomness.
ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 357–389. htps://doi.org/10.1145/582415.
582416
[3] Javed A. Aslam, Emine Yilmaz, and Virgiliu Pavlu. 2005. Te maximum entropy
method for analyzing retrieval measures. In SIGIR’05. 27–34.
[4] Charles Cole. 1993. Shannon revisited: Information in terms of uncertainty.
Journal of the American Society for Information Science 44, 4 (1993), 204–211.
[5] Inderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar. 2003. A divisive
information theoretic feature clustering algorithm for text classiﬁcation. J. Mach.
Learn. Res. 3 (March 2003), 1265–1287.
[6] Robert M. Fano. 1961. Transmission of Information: A Statistical Teory of Com-
munication. MIT Press.
[7] D. Harman. 1995. Overview of the Fourth Text REtrieval Conference (TREC-4).
In Te Fourth Text REtrieval Conference. NIST, 1 – 24.
[8] Erik Hatcher, Otis Gospodneti´c, , and Michael McCandless. 2010. Lucene in
Action (second edition ed.). Manning Publications. 475 pages.
[9] Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumulated gain-based evaluation
of IR techniques. ACM Transactions on Information Systems 20, 4 (2002), 422–446.
[10] E. T. Jaynes. 1957. Information Teory and Statistical Mechanics. II. Phys. Rev.
108 (Oct 1957), 171–190. Issue 2.
[11] Paul B. Kantor and Jung Jin Lee. 1986. Te maximum entropy principle in
information retrieval. In SIGIR’86. 269–274.
[12] Weimao Ke. 2013. Information-theoretic Term Weighting Schemes for Document
Clustering. In Proceedings of the 13th ACM/IEEE-CS Joint Conference on Digital
Libraries (JCDL ’13). ACM, New York, NY, USA, 143–152. htps://doi.org/10.1145/
2467696.2467698
[13] Weimao Ke. 2015. Information-theoretic term weighting schemes for document
clustering and classiﬁcation. International Journal on Digital Libraries 16, 2 (2015),
145–159.
[14] S. Kullback and R. A. Leibler. 1951. On information and suﬃciency. Annals of
Mathematical Statistics 22 (1951), 79–86.
[15] John Laﬀerty and Chengxiang Zhai. 2001. Document language models, query
models, and risk minimization for information retrieval. In Proceedings of the
24th annual international ACM SIGIR conference on Research and development in
information retrieval (SIGIR ’01). 111–119.
[16] J. Lin. 2006. Divergence measures based on the Shannon entropy. IEEE Trans.
Inf. Teor. 37, 1 (Sept. 2006), 145–151.
[17] Shuang Liu and Clement Yu. 2005. UIC at TREC2005: Robust Track. In Text
REtrieval Conference (TREC).
[18] Joaqu´ın P´erez-Iglesias, Jos´e R. P´erez-Ag¨uera, V´ıctor Fresno, and Yuval Z. Fein-
stein. 2009. Integrating the Probabilistic Models BM25/BM25F into Lucene. CoRR
abs/0911.5046 (2009).
[19] S. Robertson. 2004. Understanding inverse document frequency: on theoretical
arguments for IDF. Journal of Documentation 60 (2004), 503–520.
[20] Stephen Robertson and Hugo Zaragoza. 2009. Te Probabilistic Relevance Frame-
work: BM25 and Beyond. Foundations and Trends® in Information Retrieva 3, 4
(2009), 333–389.
[21] J. S. Rowlinson. 1970. Probability, Information and Entropy. Nature 225, 5239 (28
03 1970), 1196–1198.
[22] S. Jones M. Hancock-Beaulieu M. Gatford S. Robertson, S. Walker. 1993. Okapi
at TREC-2. In Te Second Text REtrieval Conference. NIST, 21 – 34.
[23] Claude E. Shannon. 1948. A mathematical theory of communication. Bell System
Technical Journal 27 (July and October 1948), 379–423 and 623–656.
[24] Debora Shaw and Charles H. Davis. 1983. Entropy and information: A multidis-
ciplinary overview. Journal of the American Society for Information Science 34, 1
(1983), 67–74.
[25] M. Siegler and M. Witbrock. 1999. Improving the suitability of imperfect tran-
scriptions for information retrieval from spoken documents. In ICASSP’99. IEEE
Press, 505–508.
[26] Folke Snickars and Jrgen W. Weibull. 1977. A minimum information principle:
Teory and practice. Regional Science and Urban Economics 7, 1 (1977), 137–168.
[27] Karen Sp¨arck Jones. 2004. A statistical interpretation of term speciﬁcity and its
application in retrieval. Journal of Documentation 60 (2004), 493–502.
[28] Tao Tao and ChengXiang Zhai. 2007. An exploration of proximity measures
in information retrieval. In Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in information retrieval (SIGIR ’07).
295–302.
[29] E.M. Voorhees. 2005. Overview of TREC 2005. In Text Retrieval Conference
(TREC).
[30] Ellen M. Voorhees and Donna Harman. 1998. Overview of the Seventh Text
REtrieval Conference (TREC-7). In Te Seventh Text REtrieval Conference. NIST, 1
– 23.
[31] Yiming Yang and Jan O. Pedersen. 1997. A Comparative Study on Feature
Selection in Text Categorization. In ICML’97. 412–420.
Paper Session 4: Retrieval Models
ICTIR’17, October 1–4, 2017, Amsterdam, The Netherlands
132

