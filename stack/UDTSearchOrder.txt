UDT WITH KNOWN SEARCH ORDER
TSVI BENSON-TILSEN
Abstract. We consider logical agents in a predictable universe running a
variant of updateless decision theory.
We give an algorithm to predict the
behavior of such agents in the special case where the order in which they
search for proofs is simple, and where they know this order. As a corollary,
“playing chicken with the universe” by diagonalizing against potential spurious
proofs is the only way to guarantee optimal behavior for this class of simple
agents.
Contents
1.
Introduction
1
2.
The problem of spurious counterfactuals
2
3.
Setting up the universe and the agent
3
4.
Playing chicken is the only way to win
5
5.
Discussion
7
References
7
1. Introduction
Updateless decision theory (UDT) was developed to deal with Newcomb-like
scenarios—situations where the behavior of the environment may depend on the
logical fact of what decision an agent makes in certain situations, as opposed to
depending only on events that were physically caused by the agent’s actions. See
[Altair, 2013] or [Hintze, 2014] for an exposition of this ﬂavor of decision theory.
In Wei Dai’s original formulation of UDT, there is an agent A deciding between
possible observation-action policies π that it may follow [Dai, 2009, 2010]. The agent
picks the policy π for which the expected utility of the universe is highest, where the
expectation for π is computed after conditioning on the statement, “The algorithm
A always follows π.” In this paper we explore an issue with one formulation of UDT
in a logical setting, due to Vladimir Slepnev.
Section 2 explains that issue.
Section 3 gives the formal setting for proof-searching agents.
Section 4 proves the main results characterizing the behavior of proof-searching
UDT agents with too much self-knowledge.
Section 5 discusses other work on the problem of logical counterfactuals.
Date: August 22, 2014.
1

2
TSVI BENSON-TILSEN
2. The problem of spurious counterfactuals
Vladimir Slepnev provided a precise model for UDT in the setting of an agent
algorithm A that lives in a deterministic, known universe U [Slepnev, 2011]. For
simplicity, we will assume that A takes a single action, and then the universe
evaluates to a utility. The agent uses a formal system such as PA to search for
proofs of statements of the form A(U) = a →U(A) = u, for a utility u ∈R and an
action a. When the agent has proved such a statement for each available action, it
chooses the action with the highest resulting u.
We assume that the agent’s encoding of the environment is clear enough that
basic theorems are easily proven, like ¬(A(U) = a ∧A(U) = b) for a ̸= b. To avoid
non-halting proof searches, we assume that A has access to a halting oracle. That
is, A can make queries in the form of a statement in the language of PA, and receive
“yes” if that statement is provable in PA and “no” otherwise.
This decision algorithm is doing counterfactual reasoning, where A(U) = a →
U(A) = u is intended to represent the counterfactual, “If the algorithm A were to
output a in universe U, a utility of u would result.” However, logical implication
does not necessarily correspond to the intuitive notion of counterfactual, and indeed
we may have spurious counterfactuals. Indeed, if A knows enough about itself, so to
speak, we may have that PA ⊢A(U) ̸= a. Then A(U) = a →U(A) = u is vacuously
true for any u. But this “counterfactual” statement is seemingly an accident, or
at least irrelevant to decision making, because its truth was not a result of any
relationship between U and agents that take action a.
To be concrete, say the universe is very simple: it gives utility 5 if A does a5
and it gives utility 10 if A does a10. Those are the only actions available to A,
and A has access to the source code of U and A. We would like for A to prove
A(U) = a5 →U(A) = 5 and A(U) = a10 →U(A) = 10, in which case A will
actually take action a10. By the assumption of a transparent universe, there are
proofs of both of these statements; the worry is that A might fail to ﬁnd these
proofs, instead ﬁnding proofs of some other statements that lead it take action a5.
To see how this might happen, recall L¨ob’s theorem:
∀ϕ : PA ⊢□PA[ϕ] →ϕ =⇒PA ⊢ϕ
Here [ϕ] is the G¨odel number of ϕ, and □PA[ϕ] is the provability predicate for PA,
which states that there exists a proof in PA of ϕ. We usually denote □PA by just
□. We will also use □[ϕ] in the meta-language as a shorthand for PA ⊢ϕ.
Now, the agent A might reason as follows:
PA ⊢A(U) = a5 →U(A) = 5
PA ⊢□[A(U) = a5 →U(A) = 5]
(∗) PA ⊢A ﬁnds the proof of [A(U) = a5 →U(A) = 5]
PA ⊢□[A(U) = a5] →□[A(U) = a5]
PA ⊢□[A(U) = a5] →□[A(U) ̸= a10]
PA ⊢□[A(U) = a5] →□[A(U) = a10 →U(A) = 0]
(∗) PA ⊢□[A(U) = a5] →A ﬁnds the proof of [A(U) = a10 →U(A) = 0]
PA ⊢□[A(U) = a5] →A(U) = a5
PA ⊢A(U) = a5

UDT WITH KNOWN SEARCH ORDER
3
In words, this says that under the assumption that the agent provably does a5,
spurious counterfactuals about the outcome of a10 can lead A to conclude that it
in fact takes action a5. Then in the ﬁnal step of the derivation, we deduce that
by L¨ob’s theorem, the agent will actually conclude that it takes action a5. Then
by soundness, A does in fact do a5. This undesirable behavior is known as “the
5-and-10 problem” or “the problem of spurious counterfactuals.”
The steps marked by (∗) cannot always be carried out (otherwise the agent could
also prove A(U) = a10 in the same way, which would be an outright contradiction in
PA). For A to go from “there exists a proof” to “A will ﬁnd that proof” requires that
A have signiﬁcant information about the order in which it searches for proofs. In
particular, it needs to know that, for instance, if a proof of A(U) = a10 →U(A) = 0
exists, then it comes before any proof of A(U) = a10 →U(A) = u for u ̸= 0.
Thus it seems that this particular kind of self-knowledge is dangerous; if a proof-
searching agent knows in what order it would ﬁnd proofs if they existed, then that
agent may fail to reason correctly about certain actions. As reported in [Slepnev,
2011], Vladimir Nesov patched this problem by inserting a clause into the decision
procedure of a UDT agent. This clause says that before A starts looking for any
proofs of A(U) = a →U(A) = u, it ﬁrst checks if there are any proofs of the form
A(U) ̸= a. If it ﬁnds such a proof, it immediately takes action a.
The idea is that, assuming PA is sound, A will never actually prove A(U) ̸= a
and then do a. So A will always proceed to the usual proof-search step. It will
not be vulnerable to spurious counterfactuals because the implication A(U) = a →
U(A) = u is never proved by ﬁrst proving A(U) ̸= a.
This strategy has been termed “playing chicken against the universe”; it forces
itself and the universe to be opaque by diagonalizing against any transparency.
Playing chicken is not a wholly satisfactory solution to the problem of spurious
counterfactuals because it seems that the agent is deliberately sabotaging its own
self-knowledge. Also, it is not clear whether the diagonalization clause is reﬂectively
consistent—i.e.
whether or not a UDT agent would delete the clause from its
source code if given the opportunity. Furthermore, playing chicken doesn’t extend
so straightforwardly to probabilistic settings; we will discuss more in section 5.
With this in mind, it might be helpful to understand more closely how spurious
counterfactuals occur. In sections 3 and 4 we specify a certain model for a UDT
agent with ﬁnitely many possible actions, and investigate its behavior.
3. Setting up the universe and the agent
The agent, an algorithm A, is acting in an arbitrary ﬁxed universe, an algorithm
U. The agent picks a certain action ai ∈{a1, . . . , an}, after which the universe
returns some utility. Let ui denote the actual utility resulting from action ai. This
utility should depend only on A’s action; the universe is behavioral, i.e. for all
agents A and B, if A(U) = B(U) then U(A) = U(B). Assume that the universe is
transparent in the following sense:
(A(U) = ai =⇒U(A) = ui) =⇒□[A(U) = ai →U(A) = ui]
The agent will search for proofs of statements of the form A(U) = a →U(A) = u
using a halting oracle—it knows, for any given statement, whether or not that
statement is provable in PA. This is a formalization of the step in the spurious

4
TSVI BENSON-TILSEN
counterfactual described above where the agent goes from “there exists a proof” to
“A will ﬁnd a proof”; with a halting oracle, these statements are the same.
Remark 3.1. It may seem overcomplicated to search for proofs of A(U) = a →
U(A) = u. Indeed, under the assumption that the universe is behavioral, we could
just as well search for proofs of π(U) = a →U(π) = u, where π ranges over the
possible action policies. This would give correct evaluations of the consequences of
diﬀerent policies.
However, our motivation for investigating these counterfactuals is to understand
agents capable of, for instance, using logical deduction to recognize other copies of
themselves. Deducing and acting on the logical consequences of A(U) being one
thing or another is closer to capturing the reasoning of such an agent.
Deﬁne the universe U:
Data: the agent A
Result: a utility ui
for each action ai, i ≤n do
if A(U) = ai then
return ui
end
end
return 0
Algorithm 1: The universe U
It will not matter in what follows, but we allow U access to a double halting oracle
so that it can run A and return a utility even if A does not halt. Now we deﬁne
the agent A:
Data: the universe U, and for each ai an order {yi
1, yi
2, . . . } of the distinct
possible utilities
Result: an action ai
for each action ai, i ≤n do
for each outcome u ∈{yi
1, yi
2, . . . } do
if □PA[A(U) = ai →U(A) = u] then
store ←(ai, u);
break for
end
end
end
let k = max{i | (ai, u) ∈store and (∀(aj, u′) ∈store)(u′ ≤u)};
return ak
Algorithm 2: The agent A
This agent has access to a complete description of the universe, including itself,
and uses a halting oracle to ﬁnd proofs. Since the universe is transparent, A will
always ﬁnd at least one proof for every action, and will store exactly one (ai, u) for
each action. This also allows for inﬁnitely many possible utilities, since no search
will go on forever.

UDT WITH KNOWN SEARCH ORDER
5
The return statement gives the tiebreak rule “take the highest numbered action
that gives maximal payoﬀ”. These tiebreaks are necessary but inconvenient, so we
abstract away the outcomes as A sees them. Recall from the deﬁnition of A that
the yi
• are the possible outcomes for ai.
Deﬁnition 3.2 (Ordering on outcomes with tiebreak). Write yi
s ≻yj
t if i > j and
yi
s ≥yj
t , or if i < j and yi
s > yj
t .
Then A takes the ai with the ≺-greatest u with (ai, u) ∈store. We are abusing the
notation by writing yi
s ≻yj
t ; technically we should write (yi
s, i) ≻(yj
t , j), since the
yi
s are just numbers. (Note that since A never compares yi
s to yi
t, we won’t need to
either.)
4. Playing chicken is the only way to win
Let fi = yi
1 be the ﬁrst outcome searched for ai, and let ui = U(A) be the true
outcome given A(U) = ai.
Lemma 4.1. The action A(U), as a function of the search order, depends only on
the fi and the ui for i ≤n. That is, A(U) always stores either (ai, fi) or (ai, ui).
The latter statement is provable in PA for ﬁxed i.
Proof. Fix i ≤n and say ui = yi
s. By transparency of U we have □[A(U) = ai →
U(A) = yi
s]. So either A will store (ai, yi
s), or else for some t < s we have
□[A(U) = ai →U(A) = yi
t]
In the latter case, since □[¬(U(A) = yi
s∧U(A) = yi
t)], we have □[A(U) ̸= ai]. Then
by a vacuous counterfactual, □[A(U) = ai →U(A) = yi
1], so A will store (ai, fi).
By Σ1-completeness of the provability predicate □, this argument can be carried
out in PA.
Since A’s action is determined by which action/outcome pairs have been stored,
we are done.

Deﬁnition 4.2 (The aboveness and domination relations). Let i ̸= k. Say that
ak is above ai if uk ≻ui, and otherwise say that ak is below ai. The action ak
dominates ai if both uk ≻fi and fk ≻fi.
Deﬁnition 4.3 (Safety of actions). The action ak is safe from above if it domi-
nates ai for all ai above ak. The action ak is safe from below if it is not dominated
by ai for any ai below ak. The action ak is safe if it is both safe from above and
safe from below.
Lemma 4.4. If A(U) = ak, then ak is safe.
Proof. By soundness, ¬□[A(U) = ak →U(A) = yk
s ] for any yk
s ̸= uk, so A stores
(ak, uk).
Suppose ﬁrst that ak is not safe from above, so ak fails to dominate some action
ai above ak. It can’t be that A stored (ai, ui), since by aboveness we have ui ≻uk;
otherwise A(U) ̸= ak, contrary to the hypothesis. So in fact A stores (ai, fi), and
we have fi ≺uk for the same reason.
This happens only if □[A(U) = ai →U(A) = fi], so by Σ1-completeness,
□[□[A(U) = ai →U(A) = fi]]. Then □[A(U) stores (ai, fi)]. Since ak fails to

6
TSVI BENSON-TILSEN
dominate ai, but uk ≻fi, it must be that fk ≺fi. Then we have the L¨obian
argument:
PA ⊢□[A(U) ̸= ak] →A(U) stores (ai, fi) and (ak, fk)
PA ⊢□[A(U) ̸= ak] →A(U) ̸= ak
PA ⊢A(U) ̸= ak,
contradicting soundness. Therefore, ak is safe from above.
Suppose now that ak is not safe from below, so ak is dominated by some action
ai below ak. That is, ui ≻fk and fi ≻fk. But then, by Lemma 4.1 in PA for i,
PA ⊢□[A(U) ̸= ak] →A(U) stores (ak, fk) and either (ai, fi) or (ai, ui)
PA ⊢□[A(U) ̸= ak] →A(U) ̸= ak
PA ⊢A(U) ̸= ak,
again contradicting soundness. Therefore ak is safe from below, and so ak is safe.

Theorem 4.5. A(U) = ak for the unique safe ak.
Proof. The agent is well-deﬁned, so by Lemma 4.4, A(U) = ak for some safe ak.
Each ai above ak is dominated by ak, and so is not safe from below. Likewise, each
ai below ak does not dominate ak, and so is not safe from above. Done.
Let us see directly why there always exists a safe action ak. Say that ak defeats
ai if either ak dominates ai or ak is above ai and ai does not dominate ak; so ak is
safe if and only if ak defeats all ai, i ̸= k.
Defeat is transitive: suppose we have ai above aj above ak. If ak defeats aj
defeats ai, then by transitivity of ≺via fj we have that ak defeats ai. If ak defeats
ai defeats aj, then either fi ≻fj or fi ≻aj. If fi ≻aj, then ak ≻aj, contradicting
that aj is above ak. So in fact fi ≻fj, and by transitivity of ≺via fi we have that
ak defeats aj. The other cases are similar.
Therefore, the maximum action under the defeats ordering is safe.
We can
compute the unique safe action ak given the fi and ui as follows: do a single
pass through the actions, maintaining the largest action so far, under the defeats
ordering; the largest action overall is the action A(U).

Suppose we take all the fi to be ∞, i.e. a formal symbol representing something
that is ≺-greater than any other number. Then no action ak dominates any ai,
since uk ≺fi. Hence the defeats relation is just the aboveness relation, and so A
will take the action above all other actions, i.e. the action with the greatest actual
utility. (Note that the fi may have some order among themselves, but this will not
aﬀect the behavior of A. If there is a maximal possible outcome y, which happens
e.g. if there are only ﬁnitely many outcomes, then we can set fi = y.)
This strategy is analogous to Nesov’s strategy of playing chicken with the uni-
verse. Choosing fi = ∞means that if □[A(U) ̸= ai], then A stores (ai, ∞). If this
ever happens, A will take some action ak with (ak, ∞) stored, and this will contra-
dict soundness. Since this won’t happen, A will actually prove only the “correct”
counterfactuals A(U) = ai →U(A) = ui.
In fact, this is the only way to ensure victory:
Corollary 4.6 (Playing chicken with the universe is the only way to win). Given
just the set of actions and the set of possible outcomes y, the assignment ∀i(fi = ∞)

UDT WITH KNOWN SEARCH ORDER
7
or ∀i(fi = sup y) is the only search strategy which guarantees that U(A) is maximal
for available ak.
Proof. Take the ≺-least fi, and suppose that fi < y for some possible outcome y.
Take any other ak; then it may be that uk = y. By choice of fi we have fk ≻fi,
and by hypothesis we have uk ≻fi. Hence we have that ak dominates ai. But then
ui may be arbitrarily large, while A(U) will never be ai.
So we must have fi ≥y for all possible outcomes y, i.e. we have fi = ∞, or at
least fi = sup y if the supremum exists. Since fi is ≺-minimal, the same equations
hold for all the fj.

5. Discussion
For ease of analysis our model uses logical deduction as the only mode of rea-
soning, but the goal is to understand agents reasoning naturalistically under un-
certainty. Updateless decision theory can be cast in terms of probabilistic beliefs
and probabilistic reasoning [Fallenstein, 2014].
In that setting, the agent has a
probability distribution over possible complete ﬁrst order theories of the world. In-
stead of searching for proofs, the agent conditions its beliefs on the statement “My
algorithm always follows policy π,” and computes the expected value of its utility
function according to that conditional distribution. It follows the policy with the
highest expected utility.
The probabilistic setting has its own complications; for one thing, the agent is
now an algorithm that makes calls to an entire probability distribution over logical
theories.
This can be represented coherently using the framework developed in
[Christiano et al., 2013].
But if the agent has too much information about its
probabilistic beliefs, it may be able to deduce facts about the expected utilities of
some actions, and so deduce that it will not take certain actions.
The problem of spurious counterfactuals then becomes the problem of condition-
ing on statements of probability 0, as the agent will attempt to do when evaluating
its available actions.
The most straightforward analog of playing chicken with
the universe—immediately taking any action that has probability 0—may only, in
Christiano’s framework, force those actions to have inﬁnitesimal probability.
The present paper oﬀers some small evidence that the unappealing strategy of
crippling one’s own self-knowledge may be unavoidable if we allow agents to be
logically omniscient, and this inevitability is further supported by the appearance
of these issues even in the probabilistic setting. This gives one motivation for the
development of a theory of logical uncertainty—how to reason under uncertainty
without the assumption that deductive consequences of an agents beliefs are nec-
essarily available to that agent. See [Christiano, 2014] for current work on logical
uncertainty. This may oﬀer a way to coherently reason counterfactually about a
computation doing something other than what it actually ends up doing.
References
Alex Altair.
A comparison of decision algorithms on newcomblike problems.
http://intelligence.org/files/Comparison.pdf, 2013. Machine Intelligence
Research Institute.
Paul Christiano. Non-omniscience, probabilistic inference, and metamathematics.
http://intelligence.org/files/Non-Omniscience.pdf, 2014.
Machine In-
telligence Research Institute.

8
TSVI BENSON-TILSEN
Paul Christiano, Eliezer Yudkowsky, Marcello Herreshoﬀ, and Mihaly Barasz. De-
ﬁnability of truth in probabilistic logic (early draft). http://intelligence.
org/files/DefinabilityTruthDraft.pdf, 2013. Machine Intelligence Research
Institute.
Wei Dai.
Towards a new decision theory.
http://lesswrong.com/lw/15m/
towards_a_new_decision_theory/, 2009.
Wei Dai.
Explicit optimization of global strategy (ﬁxing a bug in UDT1).
http://lesswrong.com/lw/1s5/explicit_optimization_of_global_
strategy_fixing_a/, 2010.
Benja
Fallenstein.
SUDT:
A
toy
decision
theory
for
updateless
an-
thropics.
http://lesswrong.com/lw/jpr/sudt_a_toy_decision_theory_
for_updateless/, 2014.
Daniel
Hintze.
Problem
class
dominance
in
predictive
dilemmas.
https://www.dropbox.com/s/n262sxmsbzx305a/Hintze_Problem%20Class%
20Dominance_Approved.pdf, 2014.
Barrett, The Honors College at Arizona
State University.
Vladimir Slepnev. A model of UDT with a halting oracle. http://lesswrong.
com/lw/8wc/a_model_of_udt_with_a_halting_oracle/, 2011.

