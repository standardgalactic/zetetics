ARTICLE
Canonical neural networks perform active inference
Takuya Isomura
1✉, Hideaki Shimazaki
2 & Karl J. Friston
3
This work considers a class of canonical neural networks comprising rate coding models,
wherein neural activity and plasticity minimise a common cost function—and plasticity is
modulated with a certain delay. We show that such neural networks implicitly perform active
inference and learning to minimise the risk associated with future outcomes. Mathematical
analyses demonstrate that this biological optimisation can be cast as maximisation of model
evidence, or equivalently minimisation of variational free energy, under the well-known form
of a partially observed Markov decision process model. This equivalence indicates that the
delayed modulation of Hebbian plasticity—accompanied with adaptation of ﬁring thresholds
—is a sufﬁcient neuronal substrate to attain Bayes optimal inference and control. We cor-
roborated this proposition using numerical analyses of maze tasks. This theory offers a
universal characterisation of canonical neural networks in terms of Bayesian belief updating
and provides insight into the neuronal mechanisms underlying planning and adaptive beha-
vioural control.
https://doi.org/10.1038/s42003-021-02994-2
OPEN
1 Brain Intelligence Theory Unit, RIKEN Center for Brain Science, Wako, Saitama 351-0198, Japan. 2 Center for Human Nature, Artiﬁcial Intelligence, and
Neuroscience (CHAIN), Hokkaido University, Sapporo, Hokkaido 060-0812, Japan. 3 Wellcome Centre for Human Neuroimaging, Institute of Neurology,
University College London, 12 Queen Square, London, WC1N 3AR, UK. ✉email: takuya.isomura@riken.jp
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
1
1234567890():,;

T
he sentient behaviour of biological organisms is char-
acterised by optimisation. Biological organisms recognise
the state of their environment by optimising internal
representations of the external (i.e. environmental) dynamics
generating sensory inputs. In addition, they optimise their
behaviour for adaptation to the environment, thereby increasing
their probability of survival and reproduction. This biological
self-organisation is typically formulated as the minimisation of
cost functions1–3, wherein a gradient descent on a cost function
furnishes neural dynamics and synaptic plasticity. However, two
fundamental issues remain to be established—namely, the
characterisation of the dynamics of an arbitrary neural network
as a generic optimisation process—and the correspondence
between such neural dynamics and statistical inference4 found
in applied mathematics and machine learning. The present
work addresses these issues by demonstrating that a class of
canonical neural networks of rate coding models is functioning
as—and thus universally characterised in terms of—variational
Bayesian inference, under a particular but generic form of the
generative model.
Variational Bayesian inference offers a uniﬁed explanation for
inference, learning, prediction, decision making, and the evolu-
tion of biological form5,6. This kind of inference rests upon a
generative model that expresses a hypothesis about the generation
of sensory inputs. Perception and behaviour can then be read as
optimising the evidence for a ‘generative model’, inherent in
sensory exchanges with the environment. The ensuing evidence
lower bound (ELBO)7—or equivalently variational free energy,
which is the negative of the ELBO—then plays the role of a cost
function. Variational free energy is the standard cost function in
variational Bayes—and provides an upper bound on surprise (i.e.,
improbability) of sensory inputs. Minimisation of variational free
energy, with respect to internal representations, then yields
approximate posterior beliefs about external states. Similarly, the
minimisation of variational free energy with respect to action on
external states maximises the evidence or marginal likelihood
of resulting sensory samples. This framework integrates percep-
tual (unsupervised), reward-based (reinforcement), and motor
(supervised) learning in a uniﬁed formulation that shares many
commitments with neuronal implementations of approximate
Bayesian inference using spiking models8–12. In short, internal
states of an autonomous system under a (possibly none-
quilibrium) steady state can be viewed as parameterising posterior
beliefs of external states13–15. In particular, active inference aims
to optimise behaviours of a biological organism to minimise a
certain kind of risk in the future16–18, wherein risk is typically
expressed in a form of expected free energy (i.e., the variational
free energy expected under posterior predictive beliefs about the
outcomes of a given course of action).
Crucially, as a corollary of the complete class theorem19–21, any
neural network minimising a cost function can be viewed as
performing variational Bayesian inference, under some prior
beliefs. We have previously introduced a reverse-engineering
approach that identiﬁes a class of biologically plausible cost
functions for neural networks22. This foundational work identi-
ﬁed a class of cost functions for single-layer feedforward neural
networks of rate coding models with a sigmoid (or logistic)
activation function—based on the assumption that the dynamics
of neurons and synapses follow a gradient descent on a common
cost function. We subsequently demonstrated the mathematical
equivalence between the class of cost functions for such neural
networks and variational free energy under a particular form of
the generative model. This equivalence licences variational
Bayesian inference as a fundamental optimisation process that
underlies both the dynamics and function of such neural net-
works. Moreover, it enables one to characterise any variables and
constants in the network in terms of quantities (e.g. priors) that
underwrite variational Bayesian inference22. However, it remains
to be established whether the active inference is an apt explana-
tion for any given neural network that actively exchanges with its
environment. In this paper, we address this enactive or control
aspect to complete the formal equivalence of neural network
optimisation and the free-energy principle.
In most formulations, active inference goes further than simply
assuming action and perception minimise variational free energy—
it also considers the consequences of action as minimising expected
free energy, i.e. planning (and control) as inference, as a founda-
tional approach to sentient behaviour12,23–27. Thus, to evince active
inference in neural networks, it is necessary to demonstrate that
they can plan to minimise future risks.
To address this issue, this work identiﬁes a class of biologically
plausible cost functions for two-layer recurrent neural networks,
under an assumption that neural activity and plasticity minimise
a common cost function (referred to as assumption 1). Then, we
analytically and numerically demonstrate the implicit ability of
neural networks to plan and minimise future risk, when viewed
through the lens of active inference. Namely, we suppose a
network of rate coding neurons with a sigmoid activation
function, wherein the middle layer involves recurrent connec-
tions, and the output layer provides feedback responses to the
environment (assumption 2). In this work, we will call such
architectures canonical neural networks (Table 1). Then, we
demonstrate that the class of cost functions—describing their
dynamics—can be cast as variational free energy under an
implicit generative model, in the well-known form of a partially
observable Markov decision process (POMDP). The gradient
descent on the ensuing cost function naturally yields Hebbian
plasticity28–30 with an activity-dependent homoeostatic term.
In particular, we consider the case where an arbitrary
modulator31–33 regulates synaptic plasticity with a certain delay
(assumption 3) and demonstrate that such modulation is identical
to the update of a policy through a post hoc evaluation of past
decisions. The modulator renders the implicit cost function a risk
function, which in turn renders behavioural control Bayes opti-
mal—to minimise future risk. The proposed analysis afﬁrms that
active inference is an inherent property of canonical neural net-
works exhibiting delayed modulation of Hebbian plasticity. We
discuss possible neuronal substrates that realise this modulation.
Results
Overview of equivalence between neural networks and varia-
tional Bayes. First, we summarise the formal correspondence
between neural networks and variational Bayes. A biological agent is
formulated here as an autonomous system comprising a network of
rate coding neurons (Fig. 1a). We presume that neural activity,
action (decision), synaptic plasticity, and changes in any other free
parameters minimise a common cost function L :¼ Lðo1:t; φÞ (c.f.,
assumption 1 speciﬁed in Introduction). Here, o1:t :¼ fo1; ¼ ; otg
is a sequence of observations and φ :¼ fx1:t; y1:t; W; ϕg is a set of
internal states comprising the middle-layer (xτ) and output-layer
(yτ) neural activity, synaptic strengths (W), and other free para-
meters (ϕ) that characterise L (e.g. ﬁring threshold). Output-layer
activity yt determines the network’s actions or decisions δt. Based on
assumption 1 and the continuous updating nature of φ, the update
rule for the i-th component of φ is derived as the gradient descent
on the cost function, _φi / ∂L=∂φi. This determines the dynamics
of neural networks, including their activity and plasticity.
In contrast, variational Bayesian inference depicts a process of
updating the prior distribution of external states PðϑÞ to the
corresponding posterior distribution QðϑÞ based on a sequence of
observations. Here, QðϑÞ approximates (or possibly exactly equals
ARTICLE
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
2
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio

to) Pðϑjo1:tÞ. This process is formulated as a minimisation of the
surprise of past-to-present observations—or equivalently maximisa-
tion of the model evidence—which is attained by minimising
variational free energy as a tractable proxy. We suppose that
the generative model Pðo1:t; ϑÞ is characterised by a set of external
states, ϑ :¼ fs1:t; δ1:t; θ; λg, comprising hidden states (sτ), decision
(δτ), model parameters (θ) and hyper parameters (λ) (Fig. 1b).
Based on the given generative model, variational free energy
is deﬁned as a function of QðϑÞ as follows: Fðo1:t; QðϑÞÞ :¼
EQðϑÞ½ln Pðo1:t; ϑÞ þ ln QðϑÞ. Here, EQðϑÞ½ :¼
R
 QðϑÞdϑ denotes
the expectation over QðϑÞ. In particular, we assume that QðϑÞ is an
exponential family (as considered in previous works10) and the
posterior expectation of ϑ, ϑ :¼ EQðϑÞ½ϑ, or its counterpart, are
the sufﬁcient statistics that parameterise (i.e. uniquely determine)
QðϑÞ. Under this condition, F is reduced to a function of ϑ,
F ¼ Fðo1:t; ϑÞ. The variational update rule for the i-th component of
ϑ is given as the gradient descent on variational free energy,
_ϑi / ∂F=∂ϑi.
Crucially,
according
to
the
complete
class
theorem,
a
dynamical system that minimises its cost function can be viewed
as performing Bayesian inference under some generative model
and prior beliefs. The complete class theorem19–21 states that for
any pair of admissible decision rules and cost functions, there is
some generative model with prior beliefs that renders the
decisions Bayes optimal (refer to Supplementary Methods 1 for
technical details). Thus, this theorem ensures the presence of a
generative model that formally corresponds to the above-deﬁned
neural network characterised by L. Hence, this speaks to the
equivalence between the class of neural network cost functions
and variational free energy under such a generative model:
Lðo1:t; φÞ  Fðo1:t; ϑÞ
ð1Þ
wherein the internal states of a network φ encode or parameterise
the posterior expectation ϑ. This mathematical equivalence means
that an arbitrary neural network, in the class under consideration,
is implicitly performing active inference through variational free
Fig. 1 Schematic of an external milieu and neural network, and the corresponding Bayesian formation. a Interaction between the external milieu and
autonomous system comprising a two-layer neural network. On receiving sensory inputs or observations oðtÞ that are generated from hidden states sðtÞ, the
network activity xðtÞ generates outputs yðtÞ. The gradient descent on a neural network cost function L determines the dynamics of neural activity and
plasticity. Thus, L is sufﬁcient to characterise the neural network. The proposed theory afﬁrms that the ensuing neural dynamics are self-organised to
encode the posterior beliefs about hidden states and decisions. b Corresponding variational Bayesian formation. The interaction depicted in a is formulated
in terms of a POMDP model, which is parameterised by A; B; C 2 θ and D; E 2 λ. Variational free energy minimisation allows an agent to self-organise to
encode the hidden states of the external milieu—and to make decisions minimising future risk. Here, variational free energy F is sufﬁcient to characterise
the inferences and behaviours of the agent.
Table 1 Glossary of expressions.
Expression
Description
Canonical neural network
In this work, a canonical neural network is deﬁned by differential equations of neural activity derived as a reduction of
realistic neuron models through some approximations, which give a network of rate coding neurons with a sigmoid
activation function. In particular, we consider networks comprising a middle layer that involves recurrent connections and
the output layer that provides feedback responses to the environment.
ot; st; δt
Observations ot, hidden states st and decisions (actions) δt are random variables that follow categorical distributions. Each
element of them takes 0 or 1.
Γt
Risk function Γt parameterises a categorical distribution over γt: PðγtÞ ¼ CatððΓt; ΓtÞ
TÞ. This can be read as an arbitrary
neuromodulator that regulates synaptic plasticity through Eq. (9), which becomes a risk function under the variational Bayes
formulation.
A; B; C
Parameter matrices A, B, and C are random variables with Dirichlet distributions PðAÞ ¼ DirðaÞ, PðBÞ ¼ DirðbÞ and
PðCÞ ¼ DirðcÞ.
CatðAÞ
Categorical distribution. In this expression, the probability that oτ ¼ i occurs given sτ ¼ j is Aij; that is,
Pðoτ ¼ ijsτ ¼ j; AÞ ¼ Aij. For any pair of i and j, this is expressed as Pðoτjsτ; AÞ ¼ CatðAÞ, where matrix A is the likelihood
mapping that maps sτ to oτ. Although one may prefer to denote it as CatðAsτÞ to emphasise that it changes depending on sτ,
in this paper, we use CatðAÞ following the notation in previous work. Note that AðiÞ
k~l means that the probability that oðiÞ
τ ¼ k
occurs given sτ ¼ l! ¼ ðl1; ¼ ; lNÞT 2 f0; 1gNs.
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
ARTICLE
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
3

energy minimisation. Minimisation of F is achieved when and
only when the posterior beliefs best match the true conditional
probability of the external states. Thus, the dynamics that
minimise L must induce a recapitulation of the external states
in the internal states of the neural network. This is a fundamental
aspect of optimisation in neural networks. This notion is essential
to understand the functional meaning of the dynamics evinced by
an arbitrary neural network, which is otherwise unclear by simply
observing the network dynamics.
Note that being able to characterise the neural network in
terms of maximising model evidence lends it an ‘explainability’,
in the sense that the internal (neural network) states and
parameters encode Bayesian beliefs or expectations about the
causes of observations. In other words, the generative model
explains how outcomes were generated. However, the complete
class theorem does not specify the form of a generative model for
any given neural network. To address this issue, in the remainder
of the paper, we formulate active inference using a particular
form of POMDP models, whose states take binary values. This
facilitates the identiﬁcation of a class of generative models that
corresponds to a class of canonical neural networks—comprising
rate coding models with the sigmoid activation function.
Active inference formulated using a postdiction of past deci-
sions. In this section, we deﬁne a generative model and ensuing
variational free energy that corresponds to a class of canonical
neural networks that will be considered in the subsequent
section. The external milieu is expressed as a discrete state
space in the form of a POMDP (Fig. 2). The generation of
observations oτ :¼ ðoð1Þ
τ ; ¼ ; oðNoÞ
τ
Þ
T from external or hidden
states milieu sτ :¼ ðsð1Þ
τ ; ¼ ; sðNsÞ
τ
Þ
T is expressed in the form of a
categorical distribution, Pðoτjsτ; AÞ ¼ CatðAÞ, where matrix A is
also known as the likelihood mapping (Table 1). Here, Aij
means the probability that oτ ¼ i is realised given sτ ¼ j, and
1 ≤τ ≤t denotes an arbitrary time in the past or the present.
Our agent receives oτ, infers latent variables (hidden states) sτ,
and provides a feedback decision δτ :¼ ðδð1Þ
τ ; ¼ ; δðNδÞ
τ
Þ
T to the
external milieu. Thus, the state transition at time τ depends on
the previous decision δτ1, characterised by the state transition
matrix Bδ, Pðsτjsτ1; δτ1; BÞ ¼ CatðBδÞ. Moreover, decision at
time τ is conditioned on the previous state sτ1 and current risk
γt, characterised by the policy mapping C, Pðδτjsτ1; γt; CÞ,
where γt contextualises Pðδτjsτ1; γt; CÞ as described below.
Each element of sτ, oτ, and δτ adopts a binary value, which is
suitable for characterising generative models implicit in cano-
nical neural networks. Note that when dealing with external
states that factorise (e.g. what and where), block matrices A, B
and C are the outer products of submatrices (please refer
to Methods section ‘Generative model’ for further details; see
Fig. 2 Factor graph depicting a ﬁctive causality of factors that the generative model hypothesises. The POMDP model is expressed as a Forney factor
graph69,70 based upon the formulation in ref. 71. The arrows from the present risk γt—sampled from Γt—to past decisions δτ optimise the policy in a post
hoc manner, to minimise future risk. In reality, the current error γt is determined based on past decisions (top). In contrast, decision making to minimise the
future risk implies a ﬁctive causality from γt to δτ (bottom). Inference and learning correspond to the inversion of this generative model. Postdiction of past
decisions is formulated as the learning of the policy mapping, conditioned by γt. Here, A, B and C indicate matrices of the conditional probability, and bold
case variables are the corresponding posterior beliefs. Moreover, D and E indicate the true prior beliefs about hidden states and decisions, while D and
E indicate the priors that the network operates under. When and only when D ¼ D and E ¼ E, inferences and behaviours are optimal for a given task or
set of environmental contingencies, and are biased otherwise.
ARTICLE
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
4
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio

also ref. 22). Hence, we deﬁne the generative model as follows:
Pðo1:t; δ1:t; s1:t; γt; θÞ
¼ PðθÞPðγtÞ
Y
t
τ¼1
Pðoτjsτ; AÞPðsτjsτ1; δτ1; BÞPðδτjsτ1; γt; CÞ
ð2Þ
where θ :¼ fA; B; Cg constitute the set of parameters, and
Pðs1js0; δ0; BÞ ¼ Pðs1Þ and Pðδ1js0; γt; CÞ ¼ Pðδ1Þ denote prob-
abilities at τ ¼ 1. PðθÞ and PðγtÞ are the prior distributions of
the parameters and the risk, which implies that θ and γt are
treated as random variables in this work (Table 1). Initial states
and decisions are characterised by prior distributions Pðs1Þ ¼
CatðDÞ and Pðδ1Þ ¼ CatðEÞ, where D and E are block vectors.
The agent makes decisions to minimise a risk function Γt :¼
Γðo1:t; s1:t; δ1:t1; θÞ that it employs (where 0 ≤Γt ≤1; see Table 1).
Because the current risk Γt is a consequence of past decisions, the
agent needs to select decisions that minimise the future risk. In
this sense, Γt is associated with the expected free energy and
precision in the usual formulation of active inference17,18 (see
Methods section ‘Generative model’ and Supplementary Meth-
ods 2 for details).
To characterise optimal decisions as minimising expected risk,
in our POMDP model, we use a ﬁctive mapping from the current
risk Γt to past decisions δ1; ¼ ; δt1 (Fig. 2). Although this is not
the true causality in the real generative process that generates
sensory data, here we intend to model the manner that an agent
subjectively evaluates its previous decisions after experiencing
their consequences. This ﬁctive causality is expressed in the form
of a categorical distribution,
Pðδτjsτ1; γt; CÞ ¼ CatðCÞγtCatðC’  CÞγt
ð3Þ
wherein policy mapping C is switched by a binarized risk
γt 2 f0; 1g—sampled from PðγtÞ ¼ CatððΓt; ΓtÞ
TÞ—in a form of
mixture model. We select this form of generative model because it
speaks to the neuromodulation of synaptic plasticity, as shown in
the next section. Equation (3) says that the probability of selecting
a decision δτ after receiving sτ1 is determined by matrix C when
γt ¼ 0, whereas it is inversely proportional to C (in the element-
wise sense) when γt ¼ 1. We note that matrix C’ denotes a
normalisation factor that can be dropped from the following
formulations, and  indicates the element-wise division operator.
Throughout the manuscript, the overline variable indicates one
minus the variable; e.g. γt ¼ 1  γt.
Importantly, the agent needs to keep selecting ‘good’ decisions
while avoiding ‘bad’ decisions. To this end, Eq. (3) supposes that
the agent learns from the failure of decisions, by assuming that
the bad decisions were sampled from the opposite of the optimal
policy mapping. In other words, the agent is assumed to have the
prior belief such that the decision—sampled from CatðCÞ—
should result in γt ¼ 0, while sampling from CatðC’  CÞ should
yield γt ¼ 1. This construction enables the agent to conduct a
postdiction of its past decisions—and thereby to update the policy
mapping to minimise future risk—by associating the past decision
rule (policy) with the current risk. Further details are provided in
the Methods section ‘Generative model’. In the next section, we
will explain the biological plausibility of this form of adaptive
behavioural control, wherein the update of the policy mapping
turns out to be identical to a delayed modulation of Hebbian
plasticity.
In short, Eq. (3) presumes that a past decision δτ (1 ≤τ ≤t  1)
is determined based on a past state sτ1 and the current risk γt. In
contrast, the current decision δt is determined to minimise the
future risk, Pðδtjst1; CÞ ¼ CatðCÞ, because the agent has not yet
observed the consequences of the current decision. We note that
although by convention, active inference uses C to denote the
prior preference, this work uses C to denote a mapping to
determine a decision depending on the previous state. Herein, the
prior preference is implicit in the risk function Γt. Due to
construction, C’ does not explicitly appear in the inference; thus,
it is omitted in the following formulations.
Variational Bayesian inference refers to the process that
optimises the posterior belief QðϑÞ. Based on the mean-ﬁeld
approximation, QðϑÞ is expressed as
QðϑÞ ¼ QðAÞQðBÞQðCÞ
Y
t
τ¼1
QðsτÞQðδτÞ
ð4Þ
Here, the posterior beliefs about states and decisions are categorical
probability distributions, QðsτÞ ¼ CatðsτÞ and QðδτÞ ¼ CatðδτÞ,
whereas
those
about
parameters
are
Dirichlet
distributions,
QðAÞ ¼ DirðaÞ, QðBÞ ¼ DirðbÞ, and QðCÞ ¼ DirðcÞ. Throughout
the manuscript, bold case variables (e.g. sτ) denote the posterior
expectations of the corresponding italic case random variables (e.g.
sτ). Thus, sτ forms a block vector that represents the posterior
probabilities of elements of sτ taking 1 or 0. The agent samples a
decision δt at time t from the posterior distribution QðδtÞ. In this
paper, the posterior belief of transition mapping is averaged over all
possible decisions, B ¼ EQðδÞ½Bδ, to ensure the exact correspondence
to canonical neural networks. We use θ :¼ fa; b; cg to denote the
parameter posteriors. For simplicity, here we suppose that state and
decision priors (D, E) are ﬁxed.
Under the above-deﬁned generative model and posterior
beliefs,
the
ensuing
variational
free
energy
is
analytically
expressed as follows:
Fðo1:t; s1:t; δ1:t; θÞ ¼ ∑
t
τ¼1 sτ  ðln sτ  ln A  oτ  ln Bsτ1Þ
þ ∑
t
τ¼1 δτ  ðln δτ  ð1  2Γt;τÞln Csτ1Þ þ Oðln tÞ
ð5Þ
The derivation details are provided in the Methods section
‘Variational free energy’. Note that Γt;τ ¼ 0 for τ ¼ t; otherwise,
Γt;τ ¼ Γt. The order ln t term indicates the complexity of
parameters, which is negligible when the leading order term is
large. The gradient descent on variational free energy updates
the posterior beliefs about hidden states (st), decisions (δt) and
parameters (θ). The optimal posterior beliefs that minimise
variational free energy are obtained as the ﬁxed point of the
implicit gradient descent, which ensures that ∂F=∂st ¼ 0,
∂F=∂δt ¼ 0 and ∂F=∂θ ¼ O. The explicit forms of the posterior
beliefs are provided in the Methods section ‘Inference and
learning’.
To explicitly demonstrate the formal correspondence with the
cost functions for neural networks considered in the next section,
we further transform the variational free energy as follows:
based on Bayes theorem Pðsτjsτ1; BδÞ / Pðsτ1jsτ; BδÞPðsτÞ, the
inverse transition mapping is expressed as By ¼ BTdiag½D1
using the state prior PðsτÞ ¼ CatðDÞ (where Pðsτ1Þ is supposed
to be a ﬂat prior belief). Moreover, from Bayes theorem
Pðδτjsτ1; γt; CÞ / Pðsτ1jδτ; γt; CÞPðδτÞ, the inverse policy map-
ping is expressed as Cy ¼ CTdiag½E1 using the decision prior
PðδtÞ ¼ CatðEÞ. Using these relationships, Eq. (5) is transformed
into the form shown in Fig. 3 (top). Please see the Methods
section ‘Variational free energy’ for further details. This speciﬁc
form of variational free energy constitutes a class of cost functions
for canonical neural networks, as we will see below.
In summary, variational free energy minimisation underwrites
optimisation of posterior beliefs. In neurobiological formulations,
it is usually assumed that neurons encode st and δt, while
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
ARTICLE
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
5

synaptic strengths encode θ17,18. In what follows, we demonstrate
that the internal states of canonical neural networks encode
posterior beliefs.
Canonical neural networks perform active inference. In this
section, we identify the neuronal substrates that correspond to
components of the active inference scheme deﬁned above. We
consider a class of two-layer neural networks with recurrent
connections in the middle layer (Fig. 1a). The modelling of the
networks in this section (referred to as canonical neural net-
works) is based on the following three assumptions—that reﬂect
physiological knowledge: (1) gradient descent on a cost function L
determines
the
updates
of
neural
activity
and
synaptic
weights (see Methods section ‘Neural networks’ for details); (2)
neural activity is updated by the weighted sum of inputs, and its
ﬁxed point is expressed in a form of the sigmoid (or logistic)
function; and (3) a modulatory factor mediates synaptic plasticity
in a post hoc manner.
Based on assumption 2, we formulate neural activity in the
middle layer (x) and output layer (y) as follows:
_xðtÞ /  sig1ðxðtÞÞ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
leak current
þ ðW1  W0ÞoðtÞ þ ðK1  K0Þxðt  ΔtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
synaptic input
þ h1  h0
|ﬄﬄﬄ{zﬄﬄﬄ}
threshold
_yðtÞ /  sig1ðyðtÞÞ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
leak current
þ ðV1  V0Þxðt  ΔtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
synaptic input
þ m1  m0
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
threshold
8
>
>
>
<
>
>
>
:
ð6Þ
Here,
xðtÞ :¼ ðx1ðtÞ; ¼ ; xNxðtÞÞT
and
yðtÞ :¼ ðy1ðtÞ; ¼ ;
yNyðtÞÞT denote column vectors of ﬁring intensities; oðtÞ :¼
ðo1ðtÞ; ¼ ; oNoðtÞÞT is a column vector of binary sensory inputs;
W1; W0 2 RNx ´ No, K1; K0 2 RNx ´ Nx and V1; V0 2 RNy ´ Nx are
synaptic
strength
matrices;
and
h1 :¼ h1ðW1; K1Þ,
h0 :¼
h0ðW0; K0Þ, m1 :¼ m1ðV1Þ and m0 :¼ m0ðV0Þ are adaptive ﬁring
thresholds that depend on synaptic strengths. This model is
derived as a reduction of a realistic neuron model through some
approximations (see Supplementary Methods 3 for details).
One may think of W1, K1 and V1 as excitatory synapses,
whereas W0, K0 and V0 can be regarded as inhibitory synapses.
Here, ðW1  W0ÞoðtÞ represents the total synaptic input from the
sensory layer, and ðK1  K0Þxðt  ΔtÞ forms a recurrent circuit
with a time delay Δt > 0. Receiving inputs from the middle layer
xðtÞ, the output-layer neural activity yðtÞ determines the decision
δðtÞ :¼ ðδ1ðtÞ; ¼ ; δNδðtÞÞT, that is, Prob½δiðtÞ ¼ 1 ¼ yiðtÞ. We
select the inverse sigmoid (i.e. logit) leak current to ensure that
the ﬁxed point of Eq. (6) (i.e. x and y that ensure _x ¼ 0 and
_y ¼ 0) has the form of a sigmoid activation function (c.f.,
assumption 2). The sigmoid activation function is also known as
the neurometric function34.
Without loss of generality, Eq. (6) can be cast as the gradient
descent on cost function L. Such a cost function can be identiﬁed
by simply integrating the right-hand side of Eq. (6) with respect
to x and y, consistent with previous treatments22. Moreover, we
presume that output-layer synapses (V1, V0) are updated through
synaptic plasticity mediated by the modulator ΓðtÞ (c.f., assump-
tion 3; 0 ≤ΓðtÞ ≤1), as a model of plasticity modulations that are
empirically observed31–33. Because neural activity and synaptic
plasticity minimise the same cost function L, the derivatives of L
must generate the modulated synaptic plasticity. Under these
constraints reﬂecting assumptions 1–3, a class of cost functions is
identiﬁed as follows:
L ¼
Z t
0
xðτÞ
xðτÞ

T
ln
xðτÞ
xðτÞ



W1
W0


oðτÞ 
K1
K0


xðτ  ΔtÞ 
h1
h0




dτ
þ
Z t
0
yðτÞ
yðτÞ

T
ln
yðτÞ
yðτÞ


 ð1  2Γðt; τÞÞ
V1
V0


xðτ  ΔtÞ 
m1
m0




dτ
þ Oð1Þ
ð7Þ
where xðτÞ :¼ 1!  xðτÞ with a vector of ones 1! ¼ ð1; ¼ ; 1ÞT.
Here, Oð1Þ—that denotes a function of synaptic strengths—is of a
smaller order than the other terms that are of order t. Thus, Oð1Þ
is negligible when t is large. We suppose Γðt; τÞ ¼ 0 for t 
Δt < τ ≤t
and
Γðt; τÞ ¼ ΓðtÞ
for
0 ≤τ ≤t  Δt,
to
satisfy
assumptions 1–3. This means that the optimisation of L by
associative plasticity is mediated by ΓðtÞ. We note that a gradient
descent on L, i.e. _x / d=dt  ∂L=∂x and _y / d=dt  ∂L=∂y, has
the same functional form (and solution) as Eq. (6) (see Methods
section ‘Neural networks’ and Supplementary Methods 4 for
further details).
Synaptic
plasticity
rules
conjugate
to
the
above
rate
coding model can now be expressed as gradient descent on
the same cost function L, according to assumption 1. To
simplify notation, we deﬁne synaptic strength matrix as ωi 2
fW1; W0; K1; K0; V1; V0g,
presynaptic
activity
as
preiðtÞ 2
foðtÞ; oðtÞ; xðt  ΔtÞ; xðt  ΔtÞ; xðt  ΔtÞ; xðt  ΔtÞg, postsynaptic
activity as postiðtÞ 2 fxðtÞ; xðtÞ; xðtÞ; xðtÞ; yðtÞ;yðtÞg and ﬁring
thresholds
as
ni 2 fh1; h0; h1; h0; m1; m0g.
Note
that
some
variables
(e.g.
xðtÞ)
appear
several
times
because
some
synapses connect to the same pre- or postsynaptic neurons as
other synapses. Thus, synaptic plasticity in the middle layer
Fig. 3 Mathematical equivalence between variational free energy and neural network cost functions, depicted by one-to-one correspondence of their
components. Top: variational free energy transformed from Eq. (5) using the Bayes theorem. Here, By ¼ BTdiag½D1 and Cy ¼ CTdiag½E1 indicate the
inverse mappings, and D and E are the state and decision priors. Bottom: neural network cost function that is a counterpart to the aforementioned
variational free energy. In this equation, ^Wl :¼ sigðWlÞ, ^Kl :¼ sigðKlÞ, and ^Vl :¼ sigðVlÞ (for l ¼ 0; 1) indicate the sigmoid functions of synaptic strengths.
Moreover, ϕl and ψl are perturbation terms that characterise the bias in ﬁring thresholds. Here, ϕl :¼ ϕlðWl; KlÞ ¼ hl  ln ^Wl 1!  ln ^Kl 1! is a function of
Wl and Kl, while ψl :¼ ψlðVlÞ ¼ ml  ln ^Vl 1! is a function of Vl. When ^ωi :¼ sigðωiÞ is the sigmoid function of ωi, ωi  ln ^ωi  ln ^ωi holds for an arbitrary ωi.
Using this relationship, Eq. (7) is transformed into the form presented at the bottom of this ﬁgure. This form of cost functions formally corresponds to
variational free energy expressed on the top of this ﬁgure. Blue lines show one-to-one correspondence of their components.
ARTICLE
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
6
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio

(i ¼ 1; ¼ ; 4) is derived as follows:
_ωi /  1
t
∂L
∂ωi
¼ hpostiðtÞpreiðtÞTi
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Hebbian plasticity
þ hpostiðtÞ 1!T
i  ∂ni
∂ωi
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
homeostatic plasticity
ð8Þ
Moreover, synaptic plasticity in the output layer (i = 5, 6) is
derived as follows:
_ωi /  1
t
∂L
∂ωi
¼ ð1  2ΓðtÞÞhpostiðtÞpreiðtÞTi
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
modulated Hebbian plasticity
þ hpostiðtÞ 1!T
i  ∂ni
∂ωi
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
homeostatic plasticity
ð9Þ
Here, hpostiðtÞpreiðtÞTi :¼ 1
t
R t
0postiðτÞpreiðτÞTdτ indicates the
average over time,  indicates the element-wise product operator,
and t 	 Δt.
These synaptic update rules are biologically plausible as they
comprise Hebbian plasticity—determined by the outer product of
pre- and postsynaptic activity—accompanied by an activity-
dependent homoeostatic term. In Eq. (9), the neuromodulator
ΓðtÞ—that encodes an arbitrary risk—alters the form of Hebbian
plasticity in a post hoc manner. This can facilitate the association
between past decisions and the current risk, thus leading to the
optimisation of the decision rule to minimise future risk. In short,
ΓðtÞ < 0:5 yields Hebbian plasticity, whereas ΓðtÞ > 0:5 yields
anti-Hebbian plasticity. Empirical observations suggest that some
modulators31–33, such as dopamine neurons35–37, are a possible
neuronal substrate of ΓðtÞ; please see Discussion for further
details.
Based on the above considerations, we now establish the formal
correspondence between the neural network cost function and
variational free energy. Under the aforementioned three minimal
assumptions, we identify the neural network cost function as
Eq. (7). Equation (7) can be transformed into the form shown in
Fig. 3 (bottom) using sigmoid functions of synaptic strengths (e.g.
^Wl :¼ sigðWlÞ for l ¼ 0; 1). Here, the ﬁring thresholds (hl; ml)
are replaced with the perturbation terms in the thresholds,
ϕl :¼ hl  ln bWl 1!  ln bKl 1! and ψl :¼ ml  ln bVl 1!. Figure 3
depicts the formal equivalence between the neural network cost
function (Fig. 3, bottom) and variational free energy (Fig. 3, top),
visualised by one-by-one correspondence between their compo-
nents. The components of variational free energy—including the
log-likelihood function and complexities of states and decisions—
re-emerge in the neural network cost function.
This means that when ðxðτÞT; xðτÞTÞ
T ¼ sτ, ðyðτÞT;yðτÞTÞ
T ¼
δτ, ΓðtÞ ¼ Γt, ^Wl ¼ AT
1l, ^Kl ¼ ByT
1l , and ^Vl ¼ CyT
1l (for l ¼ 0; 1),
the neural network cost function is identical to variational free
energy, up to the negligible ln t residual. This further endorses the
asymptotic equivalence of Eqs. (5) and (7).
The neural network cost function is characterised by the
perturbation terms implicit in ﬁring thresholds ϕ :¼ ðϕT
1; ϕT
0Þ
T
and ψ :¼ ðψT
1; ψT
0ÞT. These terms correspond to the state and
decision priors, ln PðstÞ ¼ ln D ¼ ϕ and ln PðδtÞ ¼ ln E ¼ ψ,
respectively. Further, an arbitrary neuromodulator that regulates
synaptic plasticity as depicted in Eq. (9) plays the role of the risk
function in the POMDP model deﬁned in Eq. (2), where the
equivalence can be conﬁrmed by comparing Eqs. (9) and (18).
Hence, this class of cost functions for canonical neural networks
is formally homologous to variational free energy, under the
particular form of the POMDP generative model, deﬁned in the
previous section. In other words, Eqs. (2) and (5) express the class
of generative models—and ensuing variational free energy—that
ensure Eq. (1) is apt, for the class of canonical neural networks
considered. We obtained this result based on analytic derivations
—without reference to the complete class theorem—thereby
conﬁrming the proposition in Eq. (1). This in turn suggests that
any canonical neural network in this class is implicitly performing
active inference. Table 2 summarises the correspondence between
the quantities of the neural network and their homologues in
variational Bayes.
In summary, when a neural network minimises the cost
function with respect to its activity and plasticity, the network
self-organises to furnish responses that minimise a risk implicit in
Table 2 Correspondence of variables and functions.
Neural network formation
Variational Bayes formation
Sensory inputs
oðtÞ
()
ot
Observations
Middle-layer neural activity
xðtÞ
xðtÞ


()
st
State posterior
Output-layer neural activity
yðtÞ
yðtÞ


()
δt
Decision posterior
Feedback response
δðtÞ
()
δt
Decision
Neuromodulator
ΓðtÞ
()
Γt
Risk function
Synaptic strengths
Wl
()
sig1ðA1lÞ
Parameter posterior
Kl
()
sig1ðBy
1lÞ
Vl
()
sig1ðCy
1lÞ
Perturbation terms
ϕ :¼
ϕ1
ϕ0


()
ln D
State prior
ψ :¼
ψ1
ψ0


()
ln E
Decision prior
Firing thresholds
hl
()
ln A0l  1! þ ln By
0l  1! þ ln Dl
ml
()
ln Cy
0l  1! þ ln El
Initial synaptic strengths
λW
l
 ^W
init
l
()
a1l
Parameter prior
λK
l  ^K
init
l
()
b1l
λV
l  ^V
init
l
()
c1l
Bold case variables (e.g. sτ) denote the posterior expectations of the corresponding italic case random variables (e.g. sτ). Note that Winit
l
; Kinit
l
; Vinit
l
are initial values of Wl; Kl; Vl (for l ¼ 0; 1) and
λW
l ; λK
l ; λV
l are inverse learning rate factors that express the insensitivity of synaptic strengths to plasticity. Please refer to the previous paper22 for details.
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
ARTICLE
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
7

the cost function. This biological optimisation is identical to
variational free energy minimisation under a particular form of
the POMDP model. Hence, this equivalence indicates that
minimising the expected risk through variational free energy
minimisation is an inherent property of canonical neural
networks featuring a delayed modulation of Hebbian plasticity.
Numerical simulations. Here, we demonstrate the performance
of canonical neural networks using maze tasks—as an example of
a delayed reward task. The agent comprised the aforementioned
canonical neural networks (Fig. 4a). Thus, it implicitly performs
active inference by minimising variational free energy. The maze
affords a discrete state space (Fig. 4b). The agent received the
states of the neighbouring cells as sensory inputs, and its neural
activity represented the hidden states (Fig. 4a, panels on the right;
See Methods section ‘Simulations’ for further details). Although
we denoted s as hidden states, the likelihood mapping A was a
simple identity mapping in these simulations. When solving Eqs.
(6), (8), and (9), the agent’s neural network implicitly updates
posterior beliefs about its behaviour based on the policy mapping.
It then selects an appropriate action to move towards a neigh-
bouring cell according to the inferred policy. The action was
accepted if the selected movement was allowed.
Fig. 4 Simulations of neural networks solving maze tasks. a Neural network architecture. The agent receives the states (pathway or wall) of the
neighbouring 11 × 11 cells as sensory inputs. A decision here represents a four-step sequence of actions (selected from up, down, left or right), resulting
in 256 options in total. The panels on the right depict observations and posterior beliefs about hidden states and decisions. b General view of the
maze. The maze comprises a discrete state space, wherein white and black cells indicate pathways and walls, respectively. A thick blue cell indicates the
current position of the agent, while the thin blue line is its trajectory. Starting from the left, the agent needs to reach the right edge of the maze within
T ¼ 2 ´ 104 time steps. c Trajectories of the agent’s x-axis position in sessions before (black, session 1) and after (blue, session 100) training. d Duration to
reach the goal when the neural network operates under uniform decision priors Eright ¼ Eleft ¼ Eup ¼ Edown ¼ 1=256 
 0:0039 (where Eright indicates the
prior probability to select a decision involving the rightward motion in the next step). Blue and red circles indicate succeeded and failed sessions,
respectively. e Failure probability (left) and duration to reach the goal (right) when the neural network operates under three different prior conditions
Eright ¼ 0:0023; 0:0039; 0:0055 (black, blue and cyan, respectively), where Eleft ¼ 0:0078  Eright and Eup ¼ Edown ¼ 0:0039 hold. The line indicates the
average of ten successive sessions. Although the neural network with Eright ¼ 0:0055 exhibits better performance in the early stage, it turns out to
overestimate a preference of the rightward motion in later stages, even when it approaches the wall. e was obtained with 20 distinct, randomly generated
mazes. Shaded areas indicate the standard error. Refer to Methods section ‘Simulations’ for further details.
ARTICLE
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
8
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio

Before training, the agent moved to a random direction in each
step, resulting in a failure to reach the goal position (right end)
within the time limit. During training, the neural network
updated synaptic strengths depending on its neural activity and
ensuing outcomes (i.e. risk). The training comprised a cycle of
action and learning phases. In the action phase, the agent enacted
a sequence of decisions, until it reached the goal or T ¼ 2 ´ 104
time steps passed (Fig. 4c). In the learning phase, the agent
evaluated the risk associated with past decisions after a certain
period: the risk was minimum (i.e. ΓðtÞ ¼ 0) if the agent moved
rightwards with a certain distance during the period; otherwise
ΓðtÞ ¼ 0:45 if the agent moved rightwards during the period, or
ΓðtÞ ¼ 0:55 if it did not. The synaptic strengths V (i.e. the policy
mapping) were then potentiated if the risk was low, or suppressed
otherwise, based on Eq. (9). This mechanism made it possible to
optimise decision making. Other synapses (W, K) were also
updated based on Eq. (8), although we assumed a small learning
rate to focus on the implicit policy learning. Through training, the
neural network of the agent self-organised its behaviour to
efﬁciently secure its goal (Fig. 4d). We also observed that
modulations of Hebbian plasticity without delay did not lead to
optimal behaviour, resulting in a considerably higher probability
of failing to reach the goal. These results indicate that delayed
modulation is essential to enable canonical neural networks to
solve delayed reward tasks.
With this setup
in place, we
numerically validated the
dependency of performance on the threshold factors (ϕ; ψ).
Consistent with our theoretical prediction—that ϕ and ψ encode
prior beliefs about hidden states (D) and decisions (E)—alternations
of ψ ¼ ln E from the optimum to a suboptimal value changed the
landscape of the cost function (i.e. variational free energy), thereby
providing suboptimal inferences and decisions (in relation to the
environment). Subsequently, the suboptimal network ﬁring thresh-
olds led to a suboptimal behavioural strategy, taking a longer time
or failing to reach the goal (Fig. 4e). Thus, we could attribute the
agent’s impaired performance to its suboptimal priors. This
treatment renders neural activity and adaptive behaviours of the
agent highly explainable and manipulatable in terms of the
appropriate prior beliefs—implicit in ﬁring thresholds—for a given
task or environment. In other words, these results suggest that ﬁring
thresholds are the neuronal substrates that encode state and
decision priors, as predicted mathematically.
Furthermore, when the updating of ϕ and ψ is slow in relation
to experimental observations, ϕ and ψ can be estimated through
Bayesian inference based on empirically observed neuronal
responses (see Methods section ‘Data analysis’ for details). Using
this approach, we estimated implicit prior E—which is encoded
by ψ—from sequences of neural activity generated from the
synthetic neural networks used in the simulations reported in
Fig. 4. We conﬁrmed that the estimator was a good approxima-
tion to the true E (Fig. 5a). The estimation of ϕ and ψ based on
empirical observations offered the reconstruction of the cost
function (i.e. variational free energy) that an agent employs. The
resulting cost function could predict subsequent learning of
behaviours within previously unexperienced, randomly generated
mazes—without observing neural activity and behaviour (Fig. 5b).
This is because—given the canonical neural network at hand—the
learning self-organisation is based exclusively on state and
decision priors, implicit in ϕ and ψ. Therefore, the identiﬁcation
of these implicit priors is sufﬁcient to asymptotically determine
the ﬁxed point of synaptic strengths when t is large (see Methods
section ‘Neural networks’ for further details; see also ref. 22).
These results highlight the utility of the proposed equivalence to
understand neuronal mechanisms underlying adaptation of
neural activity and behaviour through accumulation of past
experiences and ensuing outcomes.
Discussion
Biological organisms formulate plans to minimise future risks.
In this work, we captured this characteristic in biologically
plausible terms under minimal assumptions. We derived simple
differential equations that can be plausibly interpreted in terms
of a neural network architecture that entails degrees of freedom
with respect to certain free parameters (e.g. ﬁring threshold).
These free parameters play the role of prior beliefs in variational
Bayesian formation. Thus, the accuracies of inferences and
decisions depend upon prior beliefs, implicit in neural net-
works. Consequently, synaptic plasticity with false prior beliefs
leads to suboptimal inferences and decisions for any task under
consideration.
Based on the view of the brain as an agent that performs
Bayesian inference, neuronal implementations of Bayesian belief
updating have been proposed, which enables neural networks to
store and recall spiking sequences8, learn temporal dynamics and
Fig. 5 Estimation of implicit priors enables the prediction of subsequent learning. a Estimation of implicit prior Eright—encoded by threshold factor ψ—
under three different prior conditions (black, blue and cyan; c.f., Fig. 4). Here, ψ was estimated through Bayesian inference based on sequences of neural
activity, obtained with ten distinct mazes. Then, Eright was computed by ln E1 ¼ ψ1 for each of 64 elements. The other 192 elements of E1 (i.e. Eleft; Eup; Edown)
were also estimated. The sum of all the elements of E1 was normalised to 1. b Prediction of the learning process within previously unexperienced, randomly
generated mazes. Using the estimated E, we reconstructed the computational architecture (i.e. neural network) of the agent. Then, we simulated the
adaptation process of the agent’s behaviour using the reconstructed neural network and computed the trajectory of the probability of failure to reach the
goal within T ¼ 2 ´ 104 time steps. The resulting learning trajectories (solid lines) predict the learning trajectories of the original agent (dashed lines) under
three different prior conditions, in the absence of observed neural responses and behaviours. Lines and shaded areas indicate the mean and standard error,
respectively. Inset panels depict comparisons between the failure probability of the original and reconstructed agent after learning (average over session
51–100), within ten previously unexperienced mazes. Refer to Methods section ‘Data analysis’ for further details.
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
ARTICLE
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
9

causal hierarchy9, extract hidden causes10, solve maze tasks11 and
make plans to control robots12. In these approaches, the update
rules are generally derived from Bayesian cost functions (e.g.
variational free energy). However, the precise relationship
between these update rules and the neural activity and plasticity
of canonical neural networks has yet to be fully established.
We identiﬁed a one-to-one correspondence between neural
network architecture and a speciﬁc POMDP implicit in that
network. Equation (2) speaks to a unique POMDP model con-
sistent with the neural network architecture deﬁned in Eq. (6),
where their correspondences are summarised in Table 2. This
means that our scheme can be used to identify the form of
POMDP, given an observable circuit structure. Moreover, the free
parameters—that parameterise Eq. (6)—can be estimated using
Eq. (24). This means that the generative model and ensuing
variational free energy can, in principle, be reconstructed from
empirical data. This offers a formal characterisation of implicit
Bayesian models entailed by neural circuits, thereby enabling a
prediction of subsequent learning. Our numerical simulations,
accompanied by previous work22, show that canonical neural
networks
behave
systematically—and
in
a
distinct
way—
depending on the implicit POMDP (Fig. 4). This kind of self-
organisation depends on implicit prior beliefs, which can there-
fore be characterised empirically (Fig. 5).
A simple Hebbian plasticity strengthens synaptic wiring when
pre- and post-synaptic neurons ﬁre together, which enhances the
association
between
(presynaptic)
causes
and
(postsynaptic)
consequences28.
Hebbian
plasticity
depends
on
the
activity
level29,30, spike timings38,39 or burst timings40 of pre- and post-
synaptic neurons. Furthermore, modulatory factors can regulate the
magnitude and parity of Hebbian plasticity, possibly with some
time delay, leading to the emergence of various associative
functions31–33. This means that neuromodulators can be read as
encoding precision which regulates inference and learning41. These
modulations have been observed empirically with various neuro-
modulators and neurotransmitters, such as dopamine35–37,42,43,
noradrenaline44,45, muscarine46 and GABA47,48, as well as glial
factors49.
In particular, a delayed modulation of synaptic plasticity is
well-known with dopamine neurons35–37. This speaks to a
learning scheme that is conceptually distinct from standard
reinforcement learning algorithms, such as the temporal differ-
ence learning with actor-critic models based on state-action value
functions3. Please see the previous work50 for a detailed com-
parison between active inference and reinforcement learning.
Delayed modulations are also observed with noradrenaline and
serotonin51. We mathematically demonstrated that such plasticity
enhances the association between the pre-post mapping and the
future value of the modulatory factor, where the latter is cast as a
risk function. This means that postsynaptic neurons self-organise
to react in a manner that minimises future risk. Crucially, this
computation corresponds formally to variational Bayesian infer-
ence under a particular form of POMDP generative models,
suggesting that the delayed modulation of Hebbian plasticity is a
realisation of active inference. Regionally speciﬁc projections of
neuromodulators may allow each brain region to optimise activity
to minimise risk and leverage a hierarchical generative model
implicit in cortical and subcortical hierarchies. This is reminiscent
of theories of neuromodulation and (meta-)learning developed
previously52. Our work may be potentially useful, when casting
these theories in terms of generative models and variational free
energy minimisation.
The complete class theorem19–21 ensures that any neural net-
work, whose activity and plasticity minimise the same cost
function, can be cast as performing Bayesian inference. However,
identifying the implicit generative model that underwrites any
canonical neural network is a more delicate problem because the
theorem does not specify a form of a generative model for a given
canonical neural network. The posterior beliefs are largely shaped
by prior beliefs, making it challenging to identify the generative
model by simply observing systemic dynamics. To this end, it is
necessary to commit to a particular form of the generative model
and elucidate how the posterior beliefs are encoded or para-
meterised by the neural network states. This work addresses these
issues by establishing a reverse-engineering approach to identify a
generative model implicit in a canonical neural network, thereby
establishing one-to-one correspondences between their compo-
nents. Remarkably, a network of rate coding models with a sig-
moid activation function formally corresponds to a class of
POMDP models, which provide an analytically tractable example
of the present equivalence (please refer to the previous paper22 for
further discussion).
It is remarkable that the proposed equivalence can be leveraged
to identify a generative model that an arbitrary neural network
implicitly employs. This contrasts with naive neural network
models that address only the dynamics of neural activity and
plasticity. If the generative model differs from the true generative
process—that generates the sensory input—inferences and deci-
sions are biased (i.e. suboptimal), relative to Bayes optimal
inferences and decisions based on the right sort of prior beliefs. In
general, the implicit priors may or may not be equal to the true
priors; thus, a generic neural network is typically suboptimal.
Nevertheless, these implicit priors can be optimised by updating
free parameters (e.g. threshold factors ϕ; ψ) based on the gradient
descent on cost function L. By updating the free parameters, the
network will eventually, in principle, becomes Bayes optimal for
any given task. In essence, when the cost function is minimised
with respect to neural activity, synaptic strengths and any other
constants that characterise the cost function, the cost function
becomes equivalent to variational free energy with the optimal
prior beliefs. Simultaneously, the expected risk is minimised
because variational free energy is minimised only when the pre-
cision of the risk (γt) is maximised (see Methods section ‘Gen-
erative model’ for further details).
When the rate coding activation function differs from the
sigmoid function, it can be assumed that neurons encode state
posteriors under a generative model that differs from a typical
POMDP model considered in the main text (see Supplementary
Methods 4 for details; see also ref. 22). Nevertheless, the complete
class theorem guarantees the existence of some pair of a gen-
erative model (i.e. priors) and cost function that corresponds to
an arbitrary activation function. The form or time window of
empirically observed plasticity rules can also be used to identify
the implicit cost and risk functions—and further to reverse
engineer the task or problem that the neural network is solving or
learning: c.f., inverse reinforcement learning53. In short, neural
activity and plasticity can be interpreted, universally, in terms of
Bayesian belief updating.
The class of neural networks we consider can be viewed as a
class of reservoir networks54,55. The proposed equivalence could
render such reservoir networks explainable—and may provide the
optimal plasticity rules for these networks to minimise future risk
—by using the formal analogy to variational free energy mini-
misation (under the particular form of POMDP models). A clear
interpretation of reservoir networks remains an important open
issue in computational neuroscience and machine learning.
Assumption 1 places constraints on the relationship between
neural activity and plasticity. The ensuing synaptic plasticity rules
(Eqs. (8) and (9)) represent a key prediction of the proposed
scheme. We have shown that these plasticity rules enable neural
circuits to assimilate sensory inputs—as Bayes optimal encoders
of external states—and generate responses, as Bayes optimal
ARTICLE
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
10
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio

controllers that minimise risk. Tracking changes in synaptic
strengths is empirically difﬁcult in large networks, and thus the
functional form of synaptic plasticity is more difﬁcult to establish,
compared to that of synaptic responses. The current scheme
enables one to predict the nature of plasticity, given observed
neural
(and
behavioural)
responses,
under
ideal
Bayesian
assumptions. This is sometimes referred to as meta-Bayesian
inference56. The validity of these predictions can, therefore, in
principle, be veriﬁed using empirical measures of neural activity
and behaviour. In short, the current scheme predicts speciﬁc
plasticity rules for canonical neural networks, when learning a
given task.
The equivalence between neural network dynamics and gra-
dient ﬂows on variational free energy is empirically testable using
electrophysiological recordings or functional imaging of brain
activity. For instance, neuronal populations in layers 2/3 and 5 of
the parietal lobe of rodents have been shown to encode posterior
expectations about hidden sound cues, which are used to reach a
goal in uncertain environments57. According to the current for-
mulation, synaptic afferents to these expectation-coding popula-
tions—from neurons encoding sensory information—should
obey the plasticity rule in Eq. (8) and self-organise to express the
likelihood mapping (A). Related predictions are summarised in
Table 2. We have previously shown that the self-organisation of
in vitro neural networks minimises empirically computed varia-
tional free energy in a manner consistent with variational free
energy minimisation under a POMDP generative model58,59. Our
analyses in the present work speak to the predictive validity of
the proposed formulation: when the threshold factors (ϕ; ψ) can
be treated as constants—during a short experiment—we obtain
the analytical form of ﬁxed points for synaptic update rules
(Methods section ‘Neural networks’). Furthermore, ϕ and ψ can
be estimated using empirical data (Methods section ‘Data ana-
lysis’). This approach enables the reconstruction of the cost
function and prediction of subsequent learning process, as
demonstrated in Fig. 5 using in silico data. Hence, it is possible to
examine the predictive validity of the proposed theory by com-
paring the predicted synaptic trajectory with the actual trajectory.
In future work, we hope to address these issues using in vitro and
in vivo data.
Crucially, the proposed equivalence guarantees that an arbitrary
neural network that minimises its cost function—possibly imple-
mented in biological organisms or neuromorphic hardware60,61—
can be cast as performing variational Bayesian inference. Thus, in
addition to contributions to neurobiology, this notion can dra-
matically reduce the complexity of designing self-learning neu-
romorphic hardware to perform various types of tasks; therefore,
it offers a simple architecture and low computational cost.
This leads to a uniﬁed design principle for biologically inspired
machines such as neuromorphic hardware to perform statistically
optimal inference, learning, prediction, planning, and decision
making.
In summary, a class of biologically plausible cost functions for
canonical neural networks can be cast as variational free energy.
Formal correspondences exist between priors, posteriors and cost
functions. This means that canonical neural networks that opti-
mise their cost functions implicitly perform active inference. This
approach enables identiﬁcation of the implicit generative model
and reconstruction of variational free energy that neural networks
employ. This means that, in principle, neural activity, behaviour
and learning through plasticity can be predicted under Bayes
optimality assumptions.
Methods
Generative model. The proposed POMDP model comprises Ns-dimensional
hidden states st 2 f0; 1gNs that depend on the previous states st1 through a
transition probability of Bδ, and a process of generating No-dimensional observa-
tions ot 2 f0; 1gNo from those states through a likelihood mapping A (Fig. 2). Here,
the transition probability Bδ is a function of Nδ-dimensional decisions of an agent
δt 2 f0; 1gNδ , indicating that the agent’s behaviour changes the subsequent states of
the external milieu. Each state, observation, and decision take the values 1 or 0. We
use o1:t ¼ fo1; ¼ ; otg to denote a sequence of observations. Hereafter, i indicates
the i-th observation, j indicates the j-th hidden state and k indicates the k-th
decision.
Due to the multidimensional (i.e. factorial) nature of the states, A, B and C are
usually the outer products of submatrices (i.e. tensors); see also ref. 22. The
probability of an observation is determined by the likelihood mapping, from st to
oðiÞ
t , in terms of a categorical distribution: PðoðiÞ
t jst; AðiÞÞ ¼ CatðAðiÞÞ, where the
elements of AðiÞ are given by AðiÞ
1~l ¼ PðoðiÞ
t ¼ 1jst ¼ l!; AðiÞÞ and AðiÞ
0~l ¼ 1!  AðiÞ
1~l
(see also Table 1). This encodes the probability that oðiÞ
t
takes 1 or 0 when
st ¼ l! ¼ ðl1; ¼ ; lNÞT 2 f0; 1gNs. The hidden states are determined by the
transition probability, from st1 to st, depending on a given decision, in terms of a
categorical distribution: PðsðjÞ
t jst1; BδðjÞÞ ¼ CatðBδðjÞÞ. As deﬁned in Eq. (3), a
generative model of decisions δτ is conditioned on the current risk γt 2 f0; 1g that
obeys PðγtÞ ¼ CatððΓt; ΓtÞ
TÞ, where Γt ¼ 1  Γt. This can be viewed as a
postdiction of past decisions. For 1 ≤τ ≤t  1, the conditional probability of δðkÞ
τ
has a form of a mixture model: PðδðkÞ
τ jsτ1; γt; CðkÞÞ ¼ CatðCðkÞÞ
γt CatðC0ðkÞ  CðkÞÞ
γt .
Conversely, for τ ¼ t, δðkÞ
t
is sampled from PðδðkÞ
t jst1; γt; CðkÞÞ ¼ CatðCðkÞÞ to
minimise the future risk because the agent does not yet observe the consequence of
the current decision.
Equation (3) represents a mechanism by which the agent learns to select the
preferred decisions. This is achieved by updating the policy mapping C depending
on the consequences of previous decisions. Following Eq. (3), when observing
γt ¼ 0, the agent regards that the past decisions were sampled from the preferable
policy mapping C that minimises Γt, and thereby updates the posterior belief of
C to facilitate the association between δτ and sτ1. In contrast, when observing
γt ¼ 1, the agent hypothesises that this is because the past decisions were sampled
from the unpreferable policy mapping, and thereby updates the posterior belief of
C to reduce (forget) the association. In short, this postdiction evaluates the past
decisions after observing their consequence. The posterior belief of C is then
updated by associating the past decision rule (policy) and current risk, leading to
the optimisation of decisions to minimise future risk. Interestingly, this behavioural
optimisation mechanism derived from the Bayesian inference turns out to be
identical to a post hoc modulation of Hebbian plasticity (see Eqs. (7) and (9), and
Methods section ‘Neural networks’).
An advantage of this generative model—based on counterfactual causality—is
that the agent does not need to explicitly compute the expected future risk based on
the current states, because it instead updates the policy mapping C, by associating
the current risk with past decisions. Note that this construction of risk corresponds
to a simpliﬁcation of expected free energy, that would normally include risk and
ambiguity, where risk corresponds to the Kullback–Leibler divergence between the
posterior predictive and prior distribution over outcomes17,18. However, by using a
precise likelihood mapping, ambiguity can be discounted and expected free energy
reduces to the sort of decision risk considered in this work. In other words, one can
consider that the expected free energy is implicit in the policy mapping C. From
this perspective, the risk Γt is associated with precision that regulates the
magnitude of expected free energy; refer to Supplementary Methods 2 for further
details.
We can now deﬁne the generative model as Eq. (2), where Pðs1js0; δ0; BÞ ¼
Pðs1Þ ¼ CatðDÞ and Pðδ1js0; γt; CÞ ¼ Pðδ1Þ ¼ CatðEÞ are assumed. We further
suppose that δτ, given sτ1, is conditionally independent of foτ; sτg, and that only
the generation of δτ depends on γt, as visualised in the factor graph (Fig. 2).
Equation (2) can be further expanded as follows:
Pðo1:t; s1:t; δ1:t; γt; θÞ ¼ PðγtÞ 
Y
No
i¼1
P AðiÞ



Y
Ns
j¼1
P BδðjÞ



Y
Nδ
k¼1
P CðkÞ



Y
t
τ¼1
 Y
No
i¼1
P oðiÞ
τ jsτ; AðiÞ



Y
Ns
j¼1
P sðjÞ
τ jsτ1; δτ1; BδðjÞ



Y
Nδ
k¼1
P δðkÞ
τ jsτ1; γt; CðkÞ


ð10Þ
The prior beliefs of AðiÞ
~l , BδðjÞ
~l , and CðkÞ
~l
are deﬁned by Dirichlet distributions
PðAðiÞ
~l Þ ¼ DirðaðiÞ
~l Þ, PðBδðjÞ
~l Þ ¼ DirðbδðjÞ
~l Þ and PðCðkÞ
~l Þ ¼ DirðcðkÞ
~l Þ with concentration
parameters aðiÞ
~l , bδðjÞ
~l
and cðkÞ
~l , respectively. As described in the Results section, this
form of the generative model is suitable to characterise a class of canonical neural
networks deﬁned by Eq. (6). This means that none of the aforementioned
assumptions—regarding the generative model—limit the scope of the proposed
equivalence between neural networks and variational Bayesian inference, as long as
neural networks satisfy assumptions 1–3.
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
ARTICLE
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
11

Variational free energy. The agent aims to minimise surprise, or equivalently
maximise the marginal likelihood of outcomes, by minimising variational free
energy as a tractable proxy. Thereby, they perform approximate or variational
Bayesian inference. From the above-deﬁned generative model, we motivate a mean-
ﬁeld approximation to the posterior distribution as follows:
Qðs1:t; δ1:t; θÞ ¼ QðθÞQðs1:tÞQðδ1:tÞ ¼ QðAÞQðBÞQðCÞ
Y
t
τ¼1
QðsτÞQðδτÞ
ð11Þ
Here, the posterior beliefs of sτ and δτ are categorical distributions, QðsτÞ ¼
CatðsτÞ and QðδτÞ ¼ CatðδτÞ, respectively. Whereas, the posterior beliefs of A, B
and C are Dirichlet distributions, QðAÞ ¼ DirðaÞ, QðBÞ ¼ DirðbÞ and
QðCÞ ¼ DirðcÞ, respectively. In this expression, sτ and δτ represent the expectations
between 0 and 1, and a, b and c express the (positive) concentration parameters.
In this paper, the posterior transition mapping is averaged over all possible
decisions, B ¼ EQðδÞ½Bδ, to ensure exact correspondence to canonical neural
networks. Moreover, we suppose that A comprises the outer product of
submatrices Aði;jÞ 2 R2 ´ 2 to simplify the calculation of the posterior beliefs, i.e.
AðiÞ
l ¼ Aði;1Þ
l
     Aði;NsÞ
l
for l ¼ 0; 1. We also suppose that B and C comprise the
outer products of submatrices Bðj;j’Þ 2 R2 ´ 2 and Cðk;jÞ 2 R2 ´ 2, respectively. The
expectation over the parameter posterior QðθÞ ¼ QNo
i¼1
QNs
j¼1 QðAði;jÞÞ 
QNs
j¼1
QNs
j’¼1 QðBðj;j’ÞÞ  QNδ
k¼1
QNs
j¼1 QðCðk;jÞÞ is denoted as EQðθÞ½ :¼ R  QðθÞdθ.
Using this, the posterior expectation of a parameter Θ 2 fAði;jÞ; Bðj;j’Þ; Cðk;jÞg is
expressed using the corresponding concentration parameter θ 2 faði;jÞ; bðj;j’Þ; cðk;jÞg
as follows:
Θl :¼ EQðΘlÞ½Θl ¼ θl  ðθ1l þ θ0lÞ
ln Θl :¼ EQðΘlÞ½ln Θl ¼ ψðθlÞ  ψðθ1l þ θ0lÞ ¼ lnðθl  ðθ1l þ θ0lÞÞ þ Oððθ1l þ θ0lÞ1Þ
(
ð12Þ
for l ¼ 0; 1, where ψðÞ is the digamma function.
In terms of decisions, because Pðδτjsτ1; C; γt ¼ 1Þ ¼ CatðC’  CÞ in this setup,
the complexity associated with past decision is given by DKL½QðδτÞjjPðδτjsτ1; γt; CÞ ¼
EQðδτÞQðsτ1ÞQðCÞPðγtÞ½ln QðδτÞ  ln Pðδτjsτ1; γt; CÞ ¼ δτ  fln δτ  ð1  ΓtÞln Csτ1þ
Γt ln Csτ1g for 1 ≤τ ≤t  1, up to the C’-dependent term which is negligible when
computing the posterior beliefs. Whereas, the current decision is made to minimise the
complexity DKL½QðδtÞjjPðδtjst1; CÞ ¼ δτ  ðln δτ  ln Csτ1Þ.
Variational free energy is deﬁned as a functional of the posterior beliefs, given
as:
Fðo1:t; Qðs1:t; δ1:t; θÞÞ : ¼ EQðs1:t;δ1:t;θÞPðγtÞ½ln Pðo1:t; δ1:t; s1:t; γt; θÞ þ ln Qðs1:t; δ1:t; θÞ
¼ ∑
t
τ¼1 EQðsτÞQðsτ1ÞQðAÞQðBÞ½ln QðsτÞ  ln Pðoτjsτ; AÞ  ln Pðsτjsτ1; δτ1; BδÞ
þ ∑
t
τ¼1 EQðδτÞQðsτ1ÞQðCÞPðγtÞ½ln QðδτÞ  ln Pðδτjsτ1; γt; CÞ
þ DKL½PðAÞjjQðAÞ þ DKL½PðBÞjjQðBÞ þ DKL½PðCÞjjQðCÞ þ H½γt
ð13Þ
This provides an upper bound of sensory surprise  ln Pðo1:tÞ. Here, DKL½jj is
the complexity of parameters scored by the Kullback–Leibler divergence.
Minimisation of variational free energy is attained when the entropy of the risk,
H½γt :¼ EPðγtÞ½ ln PðγtÞ ¼ Γt ln Γt  Γt ln Γt, is minimised. This is achieved
when Γt shifts toward 0, meaning that the risk minimisation is a corollary of
variational free energy minimisation (the case where Γt shifts toward 1 is
negligible). Under the POMDP scheme, F is expressed as a function of the posterior
expectations, F ¼ Fðo1:t; s1:t; δ1:t; θÞ. Thus, using the vector expression, variational
free energy under our POMDP model is provided as follows:
Fðo1:t; s1:t; δ1:t; θÞ ¼ ∑
t
τ¼1 sτ  ðln sτ  ln A  oτ  ln Bsτ1Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
accuracyþstate complexity
þ ∑
t
τ¼1 δτ  ðln δτ  ð1  2Γt;τÞln Csτ1Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
decision complexity
þ ða  aÞ  ln A  ln BðaÞ þ ðb  bÞ  ln B  ln BðbÞ þ ðc  cÞ  ln C  ln BðcÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
parameter complexity
ð14Þ
Here, BðalÞ  Γða1lÞΓða0lÞ=Γða1l þ a0lÞ is the beta function (where ΓðÞ is the
gamma function), and ln A  oτ indicates the inner product of ln A and one-hot
expressed oτ, which is a custom to express the sum of the product of ðlnAði;jÞÞ
T 2
R2 ´ 2 and ðoðiÞ
τ ; oðiÞ
τ Þ
T
2 R2 over all i.
The ﬁrst and second terms of Eq. (14)—comprising accuracy and the
complexity of state and decision—increases in proportion to time t. Conversely,
other terms—the complexity of parameters—increases in the order of ln t, which is
thus negligible when t is large. Hence, we will drop the parameter complexity by
assuming that the scheme has experienced a sufﬁcient number of observations.
Please see the previous work22 for further details. The entropy of the risk is omitted
as it does not explicitly appear in the inference.
Based on the Bayes theorem, Pðsτjsτ1; BδÞ / Pðsτ1jsτ; BδÞPðsτÞ and
Pðδτjsτ1; γt; CÞ / Pðsτ1jδτ; γt; θÞPðδτÞ hold, where Pðsτ1Þ is supposed to be a ﬂat
prior belief. Thus, the inverse transition and policy mappings are given as By ¼
BTdiag½D1 and Cy ¼ CTdiag½E1, respectively. Thus, sτ  ln Bsτ1 ¼ sτ  ðln By 
sτ1 þ ln DÞ and δτ  ð1  2Γt;τÞln Csτ1 ¼ δτ  ðð1  2Γt;τÞln Cy  sτ1 þ ln EÞ
hold. Accordingly, Eq. (14) becomes
Fðo1:t; s1:t; δ1:t; θÞ ¼ ∑
t
τ¼1 sτ  ðln sτ  ln A  oτ  ln By  sτ1  ln DÞ
þ ∑
t
τ¼1 δτ  ðln δτ  ð1  2Γt;τÞln Cy  sτ1  ln EÞ þ Oðln tÞ
ð15Þ
as expressed in Fig. 3 (top). Here, the prior beliefs about states and decisions,
PðsτÞ ¼ CatðDÞ and PðδτÞ ¼ CatðEÞ, alter the landscape of variational free energy.
We will see below that this speciﬁc form of variational free energy corresponds
formally to a class of cost functions for canonical neural networks.
Inference and learning. Inference optimises the posterior beliefs about the hidden
states and decisions by minimising variational free energy. The posterior beliefs are
updated by the gradient descent on F, _st / ∂F=∂st and _δt / ∂F=∂δt. The ﬁxed
point of these updates furnishes the posterior beliefs, which are analytically com-
puted by solving ∂F=∂st ¼ 0 and ∂F=∂δt ¼ 0. Thus, from Eq. (15), the posterior
belief about the hidden states is provided as follows:
st ¼ σðln A  ot þ ln By  st1 þ ln DÞ
ð16Þ
Moreover, the posterior belief about the decisions is provided as follows:
δt ¼ σðln Cy  st1 þ ln EÞ
ð17Þ
Here, σðÞ denotes the softmax function; and D and E denote the prior beliefs
about hidden states and decisions, respectively, which we assume are ﬁxed in this
paper. Note that Eqs. (16) and (17) are equivalent to st ¼ σðln A  ot þ ln Bst1Þ
and δt ¼ σðln Cst1Þ, respectively, as By ¼ BTdiag½D1 and Cy ¼ CTdiag½E1.
Notably, st ¼ ðsT
t1; sT
t0Þ
T ¼ ðsð1Þ
t1 ; ¼ ; sðNsÞ
t1 ; sð1Þ
t0 ; ¼ ; sðNsÞ
t0 Þ
T indicates a block column
vector of the state posterior under a mean-ﬁeld assumption, where sðiÞ
t1 is the
posterior belief that sðiÞ
t
takes a value of 1. Because sðiÞ
t
takes a binary value, sðiÞ
t1 has
the form of a sigmoid function. Here, we assume that only the state posterior st at
the latest time is updated at each time t; thus, no message pass exists from stþ1 to st.
The state posterior at time t–1 is retained in the previous value. This treatment
corresponds to the Bayesian ﬁlter, as opposed to the smoother usually considered
in active inference schemes.
Equations (16) and (17) are analogue to a two-layer neural network that entails
recurrent connections in the middle layer. In this analogy, st1 and δt1 are viewed as
the middle- and output-layer neural activity, respectively. Moreover, ln A  ot,
ln By  st1, and ln Cy  st1 correspond to synaptic inputs, and ln D and ln E relate
to ﬁring thresholds. These priors and posteriors turn out to be identical to the
components of canonical neural networks, as described in the Results and Methods
section ‘Neural networks’.
Furthermore, learning optimises the posterior beliefs about the parameters θ ¼
fa; b; cg by minimising variational free energy. The posterior beliefs are updated by
the gradient descent on F, _θ / ∂F=∂θ. By solving the ﬁxed point ∂F=∂θ ¼ O of
Eq. (14), the posterior beliefs about parameters are provided as follows:
a ¼ a þ ∑
t
τ¼1 oτ  sτ ¼ thot  sti þ Oð1Þ
b ¼ b þ ∑
t
τ¼1 sτ  sτ1 ¼ thst  st1i þ Oð1Þ
c ¼ c þ ∑
t1
τ¼1ð1  2ΓtÞδτ  sτ1 þ δt  st1 ¼ thð1  2ΓtÞhδt  st1ii þ Oð1Þ
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
ð18Þ
Note that  denotes the outer product operator, and hi indicates the average
over time, e.g. hot  sti :¼ 1
t ∑t
τ¼1oτ  sτ. These posterior beliefs are usually
obtained as an average over multiple sessions to ensure convergence. Here, a; b; c
are the prior beliefs, which are of order 1 and thus negligibly small relative to the
leading order term when t is large. Thus, the posterior expectations of any
parameters Θ (i.e. Θ and ln Θ) are obtained using Eqs. (12) and (18). These
parameter posteriors turn out to correspond formally to synaptic strengths
(W; K; V) owing to the equivalence of variational free energy and neural network
cost function.
Neural networks. In this section, we elaborate the one-to-one correspondences
between components of canonical neural networks and variational Bayes, via an
analytically tractable model. Neurons respond quickly to a continuous stimulus
stream with a timescale faster than typical changes in sensory input. For instance, a
peak of spiking responses in the visual cortex (V1) appears between 50 and 80 ms
after a visual stimulation62,63, which is substantially faster than the temporal
autocorrelation timescale of natural image sequences (~500 ms)64,65. Thus, we
ARTICLE
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
12
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio

consider the case where the neural activity converges to a ﬁxed point, given a
sensory stimulus. We note that the present equivalence is derived from the dif-
ferential equation (Eq. (6)), but not from its ﬁxed point; thus, the equivalence holds
true irrespective of the time constant of neurons. To rephrase, neural networks
with a large time constant formally correspond to Bayesian belief updating with a
large time constant, which implies an insufﬁcient convergence of the posterior
beliefs.
Updates of neural activity are deﬁned by Eq. (6). When the neural activity
asymptotically converges to a steady state, the ﬁxed point of Eq. (6)—i.e., x and y
that give _x ¼ 0 and _y ¼ 0—is provided as follows:
xðtÞ ¼ sigððW1  W0ÞoðtÞ þ ðK1  K0Þxðt  ΔtÞ þ h1  h0Þ
yðtÞ ¼ sigððV1  V0Þxðt  ΔtÞ þ m1  m0Þ

ð19Þ
The adaptive ﬁring thresholds are given as functions of synaptic strengths,
hl ¼ ln bWl 1! þ ln bKl 1! þ ϕl and ml ¼ ln bVl 1! þ ψl (for l ¼ 0; 1), where
expðϕ1Þ þ expðϕ0Þ ¼ 1! and expðψ1Þ þ expðψ0Þ ¼ 1! hold in the element-wise
sense. The form of Eq. (19) is identical to the state and decision posteriors in
Eqs. (16) and (17) of the variational Bayesian formation. Further details are
provided in Supplementary Methods 5.
Considering that neural activity corresponds to the posterior beliefs about states
and decisions, one might consider the relationship between synaptic strengths and
parameter posteriors. As we mathematically demonstrated in the Results section,
owing to the equivalence between variational free energy F and the neural network
cost function L, i.e. Eq. (5) versus Eq. (7), synaptic strengths correspond formally to
parameter posteriors. The ensuing synaptic update rules—derived as the gradient
descent on L—are expressed in Eqs. (8) and (9). They have a biologically plausible
form, comprising Hebbian plasticity accompanied by an activity-dependent
homoeostatic term. The product of ΓðtÞ and the associated term modulates
plasticity depending on the quality of past decisions, after observing their
consequences, leading to minimisation of the future risk. In other words, the
modulation by ΓðtÞ represents a postdiction that the agent implicitly conducts,
wherein the agent regards its past decisions as preferable when ΓðtÞ is low and
memorises the strategy. Conversely, it regards them as unpreferable when ΓðtÞ is
high and forgets the strategy.
In particular, when ϕ and ψ are constants, the ﬁxed point of synaptic strengths
that minimise L is expressed analytically as follows: for simpliﬁcation, we employ
the notation using ωi, prei, posti and ni, as described in the Results section. The
derivative of ﬁring threshold ni with respect to synaptic strength matrix ωi yields
the sigmoid function of ωi, i.e. ∂ni=∂ωi ¼ sigðωiÞ. The ﬁxed point of synaptic
strengths ensures ∂L=∂ωi ¼ O. Thus, from Eqs. (8) and (9), it is analytically
expressed as
ωi ¼ sig1
postiðtÞpreiðtÞT
	


postiðtÞ 1!T
D
E


ð20Þ
for i ¼ 1; ¼ ; 4, and
ωi ¼ sig1
1  2ΓðtÞ
ð
Þ postiðtÞpreiðtÞT
	

	


postiðtÞ 1!T
D
E


ð21Þ
for i ¼ 5; 6 using the element-wise division . They are obtained as an average
over multiple sessions. Equations (20) and (21) correspond to the posterior belief
about parameters A; B; C, which are shown in Eq. (18).
We note that when neural activity and plasticity follow differential equations,
without loss of generality, we can consider a common cost function for a
sufﬁciently long training time t. When neural activity and plasticity are expressed
as
_xðtÞ / f ’ðxðtÞ; oðtÞ; WÞ
_W / 1
t
R t
0g’ðxðτÞ; oðτÞ; WÞdτ
(
ð22Þ
a cost function of the following form is implied:
L ¼ 
Z t
tΔt
f ðxðτÞ; oðτÞ; WÞdτ 
Z tΔt
0
gðxðτÞ; oðτÞ; WÞdτ
ð23Þ
From this cost function, one can derive gradient descent rules for both neural
activity and plasticity in the limit of a large t. This owes to the different time scales
of updating xðtÞ and W, known as an adiabatic approximation. Therefore, for a
sufﬁciently large t, the existence of cost functions that satisfy assumption 1 is a
mathematical truism. It should be emphasised that unless f matches g, the neural
and synaptic dynamics result in biased inference and learning; therefore, the cost
function deﬁned in Eq. (7) is apt.
Simulations. In Fig. 4, the neural network of the agent was characterised by a set of
internal states φ ¼ fx; y; W; K; V; ϕ; ψg, where W means the set of W1 and W0.
Neural activity (x, y) was updated by following Eq. (6), while synaptic strengths (W,
K, V) were updated by following Eqs. (8) and (9). Here, we supposed that neural
activity converges quickly to the steady state relative to the change of observations.
This treatment allowed us to compute the network dynamics based on Eqs.
(19–21), which could reduce the computational cost for numerical simulations.
Synaptic strengths W were initialised as a matrix sufﬁciently close to the identity
matrix; whereas, synaptic strengths K and V were initialised as matrices with
uniform values. This treatment served to focus on the policy learning implicit in
the update of V. The threshold factors (ϕ; ψ), which encoded the prior beliefs about
hidden states (D) and decisions (E), were predeﬁned and ﬁxed over the sessions. In
Fig. 4e, we varied E to show how performance depends on these priors. Namely,
E1 ¼ ðEright; ¼ ; Eright; Eleft; ¼ ; Eleft; Eup; ¼ ; Eup; Edown; ¼ ; EdownÞT 2 ½0; 1256
was characterised by four values Eright; Eleft; Eup; Edown 2 ½0; 1, where Eright indi-
cates the prior probability to select a decision involving the rightward motion in the
next step.
Data analysis. When the belief updating of implicit priors (D, E) is slow in relation
to experimental observations, D and E (which are encoded by ϕ and ψ) can be
viewed as being ﬁxed over a short period of time, as an analogy to homoeostatic
plasticity over longer time scales66. Thus, ϕ and ψ can be statistically estimated by a
conventional Bayesian inference (or maximum likelihood estimation, given a ﬂat
prior) based on empirically observed neuronal responses. In this case, the esti-
mators of ϕ and ψ are obtained as follows:
ϕ ¼ ln
hxðtÞi
hxðtÞi


ψ ¼ ln
hyðtÞi
hyðtÞi


8
>
>
>
<
>
>
>
:
ð24Þ
Note that we suppose constraints expðϕ1Þ þ expðϕ0Þ ¼ 1! and
expðψ1Þ þ expðψ0Þ ¼ 1!. This assumption ﬁnesses the estimation of implicit priors
(Fig. 5a)—owing to the equivalence ϕ ¼ ln D and ψ ¼ ln E (see Fig. 3 and Table 2)
—and the ensuing reconstruction of variational free energy, which was conducted
by substituting Eq. (24) into Eq. (15). Equation (24) indicates that the average
activity encodes prior beliefs, which is consistent with the experimental
observations; in that the activity of sensory areas reﬂects prior beliefs67.
Furthermore, given a generative model, the posterior beliefs of external states
were updated based on a sequence of sensory inputs by minimising variational free
energy, without reference to neural activity or behaviour. Thus, the reconstructed
variational free energy enabled us to predict subsequent inference and learning of
the agent, without observing neural activity or behaviour (Fig. 5b). In Fig. 5, for
simplicity, the form of the risk function was supposed to be known when
reconstructing the cost function. Although we did not estimate ϕ in Fig. 5, the
previous work showed that our approach can estimate ϕ from simulated neural
activity data22.
Reporting summary. Further information on research design is available in the Nature
Research Reporting Summary linked to this article.
Data availability
All relevant data are within the paper. Figures 4 and 5 were generated using the authors’
scripts (see Code Availability).
Code availability
The MATLAB scripts are available at GitHub https://github.com/takuyaisomura/
reverse_engineering or Zenodo https://doi.org/10.5281/zenodo.574885068. The scripts
are covered under the GNU General Public License v3.0.
Received: 16 June 2021; Accepted: 21 December 2021;
References
1.
Linsker, R. Self-organization in a perceptual network. Computer 21, 105–117
(1988).
2.
Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine.
Neural Comput. 7, 889–904 (1995).
3.
Sutton, R. S. & Barto, A. G. Reinforcement Learning (MIT Press, 1998).
4.
Bishop, C. M. Pattern Recognition and Machine Learning (Springer, 2006).
5.
Friston, K. J., Kilner, J. & Harrison, L. A free energy principle for the brain. J.
Physiol. Paris 100, 70–87 (2006).
6.
Friston, K. J. The free-energy principle: a uniﬁed brain theory? Nat. Rev.
Neurosci. 11, 127–138 (2010).
7.
Blei, D. M., Kucukelbir, A. & McAuliffe, J. D. Variational inference: a review
for statisticians. J. Am. Stat. Assoc. 112, 859–877 (2017).
8.
Brea, J., Senn, W. & Pﬁster, J. P. Matching recall and storage in sequence
learning with spiking neural networks. J. Neurosci. 33, 9565–9575 (2013).
9.
Deneve, S. Bayesian spiking neurons II: learning. Neural Comput. 20, 118–145
(2008).
10. Kappel, D., Nessler, B. & Maass, W. STDP installs in winner-take-all circuits
an online approximation to hidden Markov model learning. PLoS Comput.
Biol. 10, e1003511 (2014).
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
ARTICLE
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
13

11. Jimenez Rezende, D. & Gerstner, W. Stochastic variational learning in
recurrent spiking networks. Front. Comput. Neurosci. 8, 38 (2014).
12. Rueckert, E., Kappel, D., Tanneberg, D., Pecevski, D. & Peters, J. Recurrent
spiking networks solve planning tasks. Sci. Rep. 6, 1–10 (2016).
13. Friston, K. J. Life as we know it. J. R. Soc. Interface 10, 20130475 (2013).
14. Friston, K. J. A free energy principle for a particular physics. Preprint at arXiv
1906.10184 (2019).
15. Parr, T., Da Costa, L. & Friston, K. J. Markov blankets, information geometry
and stochastic thermodynamics. Philos. Trans. R. Soc. A 378, 20190159
(2020).
16. Friston, K. J., Mattout, J. & Kilner, J. Action understanding and active
inference. Biol. Cybern. 104, 137–160 (2011).
17. Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. Active
inference and learning. Neurosci. Biobehav. Rev. 68, 862–879 (2016).
18. Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. Active
inference: a process theory. Neural Comput. 29, 1–49 (2017).
19. Wald, A. An essentially complete class of admissible decision functions. Ann.
Math. Stat. 18, 549–555 (1947).
20. Brown, L. D. A complete class theorem for statistical problems with ﬁnite-
sample spaces. Ann. Stat. 9, 1289–1300 (1981).
21. Berger, J. O. Statistical Decision Theory and Bayesian Analysis (Springer,
2013).
22. Isomura, T. & Friston, K. J. Reverse-engineering neural networks
to characterize their cost functions. Neural Comput. 32, 2085–2121
(2020).
23. Attias, H. Planning by Probabilistic Inference. In Proc. 9th International
Workshop on Artiﬁcial Intelligence and Statistics 6–16 (ML Research Press,
2003).
24. Botvinick, M. & Toussaint, M. Planning as inference. Trends Cogn. Sci. 16,
485–488 (2012).
25. Maisto, D., Donnarumma, F. & Pezzulo, G. Divide et impera: subgoaling
reduces the complexity of probabilistic inference and problem solving. J. R.
Soc. Interface 12, 20141335 (2015).
26. Kaplan, R. & Friston, K. J. Planning and navigation as active inference. Biol.
Cybern. 112, 323–343 (2018).
27. Millidge, B. Deep active inference as variational policy gradients. J. Math.
Psychol. 96, 102348 (2020).
28. Hebb, D. O. The Organization of Behavior: A Neuropsychological Theory
(Wiley, 1949).
29. Bliss, T. V. & Lømo, T. Long‐lasting potentiation of synaptic transmission in
the dentate area of the anaesthetized rabbit following stimulation of the
perforant path. J. Physiol. 232, 331–356 (1973).
30. Malenka, R. C. & Bear, M. F. LTP and LTD: an embarrassment of riches.
Neuron 44, 5–21 (2004).
31. Pawlak, V., Wickens, J. R., Kirkwood, A. & Kerr, J. N. Timing is not
everything: neuromodulation opens the STDP gate. Front. Syn. Neurosci. 2,
146 (2010).
32. Frémaux, N. & Gerstner, W. Neuromodulated spike-timing-dependent
plasticity, and theory of three-factor learning rules. Front. Neural Circuits 9, 85
(2016).
33. Kuśmierz, Ł., Isomura, T. & Toyoizumi, T. Learning with three factors:
modulating Hebbian plasticity with errors. Curr. Opin. Neurobiol. 46, 170–177
(2017).
34. Newsome, W. T., Britten, K. H. & Movshon, J. A. Neuronal correlates of a
perceptual decision. Nature 341, 52–54 (1989).
35. Yagishita, S. et al. A critical time window for dopamine actions on the
structural plasticity of dendritic spines. Science 345, 1616–1620 (2014).
36. Wieland, S. et al. Phasic dopamine modiﬁes sensory-driven output of striatal
neurons through synaptic plasticity. J. Neurosci. 35, 9946–9956 (2015).
37. Brzosko, Z., Zannone, S., Schultz, W., Clopath, C. & Paulsen, O. Sequential
neuromodulation of Hebbian plasticity offers mechanism for effective reward-
based navigation. eLife 6, e27756 (2017).
38. Markram, H., Lübke, J., Frotscher, M. & Sakmann, B. Regulation of synaptic
efﬁcacy by coincidence of postsynaptic APs and EPSPs. Science 275, 213–215
(1997).
39. Bi, G. Q. & Poo, M. M. Synaptic modiﬁcations in cultured hippocampal
neurons: dependence on spike timing, synaptic strength, and postsynaptic cell
type. J. Neurosci. 18, 10464–10472 (1998).
40. Butts, D. A., Kanold, P. O. & Shatz, C. J. A burst-based “Hebbian” learning
rule at retinogeniculate synapses links retinal waves to activity-dependent
reﬁnement. PLoS Biol. 5, e61 (2007).
41. Parr, T. & Friston, K. J. Uncertainty, epistemics and active inference. J. R. Soc.
Interface 14, 20170376 (2017).
42. Reynolds, J. N. J., Hyland, B. I. & Wickens, J. R. A cellular mechanism of
reward-related learning. Nature 413, 67–70 (2001).
43. Zhang, J. C., Lau, P. M. & Bi, G. Q. Gain in sensitivity and loss in temporal
contrast of STDP by dopaminergic modulation at hippocampal synapses.
Proc. Natl Acad. Sci. USA 106, 13028–13033 (2009).
44. Salgado, H., Köhr, G. & Treviño, M. Noradrenergic “tone” determines
dichotomous control of cortical spike-timing-dependent plasticity. Sci. Rep. 2,
417 (2012).
45. Johansen, J. P. et al. Hebbian and neuromodulatory mechanisms interact to
trigger associative memory formation. Proc. Natl Acad. Sci. USA 111,
E5584–E5592 (2014).
46. Seol, G. H. et al. Neuromodulators control the polarity of spike-timing-
dependent synaptic plasticity. Neuron 55, 919–929 (2007).
47. Paille, V. et al. GABAergic circuits control spike-timing-dependent plasticity.
J. Neurosci. 33, 9353–9363 (2013).
48. Hayama, T. et al. GABA promotes the competitive selection of dendritic
spines by controlling local Ca2+ signaling. Nat. Neurosci. 16, 1409–1416
(2013).
49. Ben Achour, S. & Pascual, O. Glia: the many ways to modulate synaptic
plasticity. Neurochem. Int. 57, 440–445 (2010).
50. Sajid, N., Ball, P. J., Parr, T. & Friston, K. J. Active inference: demystiﬁed and
compared. Neural Comput. 33, 674–712 (2021).
51. He, K. et al. Distinct eligibility traces for LTP and LTD in cortical synapses.
Neuron 88, 528–538 (2015).
52. Doya, K. Metalearning and neuromodulation. Neural Netw. 15, 495–506
(2002).
53. Ng, A. Y. & Russell, S. J. Algorithms for inverse reinforcement learning. In
Proc. Seventeenth International Conference on Machine Learning (ICML 2000)
(ed. Langley, P.) 2 (Morgan Kaufmann, 2000).
54. Sussillo, D. & Abbott, L. F. Generating coherent patterns of activity from
chaotic neural networks. Neuron 63, 544–557 (2009).
55. Laje, R. & Buonomano, D. V. Robust timing and motor patterns by taming
chaos in recurrent neural networks. Nat. Neurosci. 16, 925–933 (2013).
56. Daunizeau, J. et al. Observing the observer (I): meta-Bayesian models of
learning and decision-making. PLoS ONE 5, e15554 (2010).
57. Funamizu, A., Kuhn, B. & Doya, K. Neural substrate of dynamic Bayesian
inference in the cerebral cortex. Nat. Neurosci. 19, 1682–1689 (2016).
58. Isomura, T., Kotani, K. & Jimbo, Y. Cultured cortical neurons can perform
blind source separation according to the free-energy principle. PLoS Comput.
Biol. 11, e1004643 (2015).
59. Isomura, T. & Friston, K. J. In vitro neural networks minimise variational free
energy. Sci. Rep. 8, 16926 (2018).
60. Merolla, P. A. et al. A million spiking-neuron integrated circuit with a scalable
communication network and interface. Science 345, 668–673 (2014).
61. Roy, K., Jaiswal, A. & Panda, P. Towards spike-based machine intelligence
with neuromorphic computing. Nature 575, 607–617 (2019).
62. Lamme, V. A. & Roelfsema, P. R. The distinct modes of vision offered by
feedforward and recurrent processing. Trends Neurosci. 23, 571–579 (2000).
63. Benucci, A., Ringach, D. L. & Carandini, M. Coding of stimulus sequences by
population responses in visual cortex. Nat. Neurosci. 12, 1317–1324 (2009).
64. David, S. V., Vinje, W. E. & Gallant, J. L. Natural stimulus statistics alter the
receptive ﬁeld structure of v1 neurons. J. Neurosci. 24, 6991–7006 (2004).
65. Bull, D. Communicating pictures: A course in Image and Video Coding
(Academic Press, 2014).
66. Turrigiano, G. G. & Nelson, S. B. Homeostatic plasticity in the developing
nervous system. Nat. Rev. Neurosci. 5, 97–107 (2004).
67. Berkes, P., Orbán, G., Lengyel, M. & Fiser, J. Spontaneous cortical activity
reveals hallmarks of an optimal internal model of the environment. Science
331, 83–87 (2011).
68. Isomura, T. Canonical neural networks perform active inference [Code].
GitHub https://doi.org/10.5281/zenodo.5748850 (2021).
69. Forney, G. D. Codes on graphs: Normal realizations. IEEE Trans. Info Theory
47, 520–548 (2001).
70. Dauwels, J. On variational message passing on factor graphs. In 2007 IEEE
International Symposium on Information Theory (IEEE, 2007).
71. Friston, K. J., Parr, T. & de Vries, B. D. The graphical brain: belief propagation
and active inference. Netw. Neurosci. 1, 381–414 (2017).
Acknowledgements
We are grateful to Hitoshi Okamoto, Makio Torigoe and Lancelot Da Costa for dis-
cussions. This work was supported in part by the grant of Joint Research by the National
Institutes of Natural Sciences (NINS Programme No. 01112005, 01112102). T.I. is funded
by RIKEN Center for Brain Science. H.S. is funded by JSPS KAKENHI Grant Number JP
20K11709, 21H05246 and New Energy and Industrial Technology Development Orga-
nization (NEDO), Japan. K.J.F. is funded by a Wellcome Principal Research Fellowship
(Ref: 088130/Z/09/Z). The funders had no role in study design, data collection and
analysis, decision to publish, or preparation of the manuscript.
Author contributions
Conceptualisation, T.I., H.S., and K.J.F.; Formal analysis, Simulation and Writing—ori-
ginal draft T.I.; Writing—review & editing, T.I., H.S. and K.J.F. All authors made
ARTICLE
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
14
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio

substantial contributions to conception, design and writing of the article. All authors
have read and agreed to the published version of the manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version contains supplementary material
available at https://doi.org/10.1038/s42003-021-02994-2.
Correspondence and requests for materials should be addressed to Takuya Isomura.
Peer review information Communications Biology thanks the anonymous reviewers for
their contribution to the peer review of this work. Primary Handling Editors: Enzo
Tagliazucchi and Karli Montague-Cardoso. Peer reviewer reports are available.
Reprints and permission information is available at http://www.nature.com/reprints
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly from
the copyright holder. To view a copy of this license, visit http://creativecommons.org/
licenses/by/4.0/.
© The Author(s) 2022
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-021-02994-2
ARTICLE
COMMUNICATIONS BIOLOGY |   (2022) 5:55 | https://doi.org/10.1038/s42003-021-02994-2 | www.nature.com/commsbio
15

