The Visual Computer (2020) 36:1307–1324
https://doi.org/10.1007/s00371-019-01726-2
ORIGINAL ARTICLE
Automatic semantic style transfer using deep convolutional neural
networks and soft masks
Hui-Huang Zhao1
· Paul L. Rosin2 · Yu-Kun Lai2 · Yao-Nan Wang3
Published online: 31 July 2019
© Springer-Verlag GmbH Germany, part of Springer Nature 2019
Abstract
This paper presents an automatic image synthesis method to transfer the style of an example image to a content image. When
standard neural style transfer approaches are used, the textures and colours in different semantic regions of the style image
are often applied inappropriately to the content image, ignoring its semantic layout and ruining the transfer result. In order to
reduce or avoid such effects, we propose a novel method based on automatically segmenting the objects and extracting their
soft semantic masks from the style and content images, in order to preserve the structure of the content image while having
the style transferred. Each soft mask of the style image represents a speciﬁc part of the style image, corresponding to the soft
mask of the content image with the same semantics. Both the soft masks and source images are provided as multichannel
input to an augmented deep CNN framework for style transfer which incorporates a generative Markov random ﬁeld model.
The results on various images show that our method outperforms the most recent techniques.
Keywords Deep neural networks · Style transfer · Soft mask · Semantic segmentation
1 Introduction
Style transfer is a process of migrating a style from a “style
image” to a “content image”. The goal is to be able to
generate different renditions of the same scene according
to different style images. Image style transfer has become
a popular problem in computer vision and graphics and
can generate impressive results covering a wide variety of
styles for both images [16] and videos [35]. It has also been
widely employed to solve problems such as texture synthe-
B Hui-Huang Zhao
happyday.huihuang@gmail.com
Paul L. Rosin
RosinPL@cardiff.ac.uk
Yu-Kun Lai
LaiY4@cardiff.ac.uk
Yao-Nan Wang
yaonan@hnu.cn
1
College of Computer Science and Technology, Hengyang
Normal University, Hengyang, China
2
School of Computer Science and Informatics, Cardiff
University, Cardiff, UK
3
College of Electrical and Information Engineering, Hunan
University, Changsha, China
sis [9], inpainting [6], head portraits [12,36], super-resolution
[7,23],fontgeneration[1]andsmokesimulations[25].More-
over, a number of useful applications of image style transfer
have been shown, such as stylisation of 3D CAD mod-
els for more aesthetically pleasing presentation of design
solutions [33], stylisation of maps to provide better visu-
alisation [22], stylisation to provide seamless integration of
content in augmented reality [28] and stylisation of 3D mod-
els for technical illustration [20].
When existing neural style transfer methods are applied
to images with complex structures, visual elements from the
style image are often transferred to semantically irrelevant
areas of the content image. In order to achieve good results,
users must pay attention to the composition and/or the selec-
tion of the style image, because, for example, the background
colours or textures will often ruin the style transfer results,
especially for portraits where the artefacts can be particularly
off-putting. Addressing this problem, Champandard [4] (and
subsequently [17]) recently proposed a method which uses
a manually generated semantic map to help control the style
transfer and can achieve better results than some common
methods.
In this paper, we speciﬁcally consider the problem of
image style transfer guided by automatically extracted soft
semantic masks. To achieve this, we adapt various semantic
123

1308
H.-H. Zhao et al.
segmentation and labelling techniques to extract soft masks
associated with speciﬁc semantics. By deploying the seman-
tic masks to control the transfer, it is possible to avoid errors
such as those shown in Fig. 1c generated using the CNN-
MRF method [29] in which stylised foreground objects are
contaminated by the background texture, and vice verse.
The main contributions of the paper are as follows:
1. We adapt a state-of-the-art semantic segmentation
method [48] to generate semantic masks automatically.
Instead of using hard segmentation as [48], we propose to
use soft masks containing the probabilities of occurrence
of different objects in the image, since they preserve more
information and are more robust when image regions
have similar chances of belonging to multiple object cat-
egories. They are used to capture elements of the styles
for objects in the style image and to preserve the structure
of the content image. For the human face in particular,
we use a more detailed segmentation, in which different
facial parts such as the nose, eyes and mouth are also
automatically segmented, providing ﬁne-grained control
in perceptually crucial areas; these are also treated as
semantic masks.
2. We augment a trained deep convolutional neural network
by concatenating K soft mask channels and N channels
of regular ﬁlters. This is further combined with a gener-
ative Markov random ﬁeld (MRF) model [29] for image
style transfer. Both the style and content images and their
semantic maps are input into the augmented deep con-
volutional neural network. Extensive experiments show
that such higher-level semantic information improves the
quality of style transfer.
2 Related work
2.1 Style transfer using deep networks
The success of deep CNNs (DCNNs) in image process-
ing has also raised interest in image style transfer. Shih et
al. [38] proposed a new style transfer method for headshot
portraits. During their method, they presented a new multi-
scale technique based on deep networks to robustly transfer
the local statistics of an example portrait onto a new one.
Gatys et al. [15,16] showed remarkable results by using the
VGG 19-layer network for style transfer. Their approach
was employed in unguided settings and taken up by vari-
ous follow-up papers. Gatys et al. [17] in particular extended
the Gram matrix method beyond the paradigm of transferring
global style information between pairs of images, and they
introduced control over spatial location, colour information
and spatial scale. Zhang and Dana [46] built a Multi-style
Generative Network (MSG-Net), which achieved real-time
performance. Ulyanov et al. [41] presented an alternative
approach which trained compact feed-forward convolutional
networks. The resulting networks are extremely lightweight
and can generate images faster than [16]. By combining the
beneﬁts of training feed-forward convolutional neural net-
works and perceptual loss functions, Johnson et al. [23]
presented a novel approach for image style transfer. Li
and Wand [29] suggested an approach for preserving local
patterns of the style image. Instead of using a global rep-
resentation of the style computed as a Gram matrix, they
used patches of the neural activation from the style image.
Ruder et al. [35] presented an approach that transfers the style
from one image (e.g. a painting) to a whole video sequence.
In order to solve the problem of distortions which made the
result look like a painting in photographic style transfer, Luan
et al. [32] developed a photographic style transfer method
which constrains the transformation from the input to the
output by using a photorealism regularization term. Kang et
al. [24] proposed a new method named deep image analogy
which is based on ﬁnding semantically meaningful dense
correspondences between two input images for image visual
attribute transfer. Different from the image domain transfer
problem, Chang et al. [5] proposed a new approach which
involves two asymmetric functions, which are a forward
function that encodes example-based style and a backward
function that removes the style, for applying and removing
makeup.
Two main types of methods are used in deep learning-
based style transfer: global approaches based on the Gram
matrix or other global measures and local approaches based
on patch matching. Compared to the global methods, meth-
odsbasedonpatchmatchingaremoreﬂexibleandbettercope
with cases in which the visual styles or elements vary across
the image. However, they could also produce visible artefacts
when there are local matching errors. In order to control the
region of application of the style image, Gatys et al. [17]
used several manually speciﬁed spatial guidance channels,
containing values in [0,1], for both the content and style
images. Their experiments showed that the guidance chan-
nels can ensure that the style is transferred between regions
of similar scene content in the content and style images. It
is, however, time-consuming to produce masks. As a result,
for example, in their paper, they just used a mask to separate
two parts of the image (e.g. sky and non-sky) for simple spa-
tial control and did not distinguish more detailed content in
the images. For most methods not based on local matching
such as [30,42], they are prone to indiscriminately transfer
different styles, which belong to speciﬁc objects in the style
image, across the content image, thereby degrading the trans-
fer result. Also, some methods use domain-speciﬁc models
and therefore cannot be applied to general images [5].
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1309
2.2 MRF-based image synthesis
Markov random ﬁelds (MRFs) are a famous framework for
nonparametric image synthesis [10,13]. Kwatra et al. [26,27]
modelled the texture as an MRF and computed some approx-
imation to the optimal solution. Zhang et al. [47] formulated
the patch mapping problem as a labelling problem modelled
by a discrete MRF. Moreover, Frigo et al. [14] proposed a
novel unsupervised method for texture and colour transfer
based on MRFs. In their approach, an adaptive patch parti-
tion is used to capture the style of the example image and
preserve the structure of the source image. MRF models suf-
fer from a limitation that local image statistics are usually
not sufﬁcient for capturing complex image layouts at a global
scale. Wei and Levoy [44] and Kwatra et al. [26] proposed
a multi-resolution synthesis approach to improve this. We
adapt this in our method. Li and Wand [29] presented a com-
bination of generative Markov random ﬁeld (MRF) models
for image synthesis. Unlike other MRF-based texture syn-
thesis approaches, their combined system can both match
and adapt local features with considerable variability, and
therefore, our paper is based on this method.
2.3 Semantic segmentation
Recently, CNN architectures have been shown to be capa-
ble of providing semantic segmentation [19,40], generic
object detection [31] and image completion [18]. Girshick
et al. [19] proposed a method called R-CNN, which com-
bines region proposals with CNNs. In [18], a deep CNN
is used to disentangle patch structure and style, and visual
aesthetics (style) alongside structure and semantics are used
to enhance image completion. Noh et al. [34] applied a
trained network (VGG 16-layer net) to each proposal in an
input image and constructed the ﬁnal semantic segmentation
map by combining the results from all the proposals. Shel-
hamer et al. [37] proposed a fully convolutional network for
semantic segmentation. For producing accurate and detailed
segmentations, they deﬁned a skip architecture which com-
bines semantic information from a deep, coarse layer with
appearance information from a shallow, ﬁne layer. In order
to achieve better results, some existing face detection meth-
ods are also used in style transfer. By searching a database
using Face++ [11] to ﬁnd images with similar poses to a
given source portrait image, Yang et al. [45] presented a
novel colour transfer approach for portraits. Zheng et al. [48]
introduced a new form of convolutional neural network that
combines the strengths of convolutional neural networks and
conditionalrandomﬁelds-basedprobabilisticgraphicalmod-
elling. These models rely primarily on convolutional layers
to extract high-level patterns and then use deconvolution to
label the individual pixels. Currently, they have trained this
model to recognise 20 classes, and our paper uses this method
to obtain some semantic content from images.
2.4 Limitations of current methods
Although there has been considerable development on neu-
ral style transfer, the recent methods tend to have the same
types of problems as the earlier works. Most existing meth-
ods either use Gram matrices (or equivalent measures) which
treat images globally, or for methods based on local patch
matching, can often match regions of one object in the style
image to regions of a different object in the content image,
causing artefacts such as those shown in Fig. 1. This is
more critical for human faces as subtle mismatches can be
detrimental to the quality of synthesised images. To address
this, existing methods [4,17] use manual segmentation to
improve style transfer. However, manual segmentation is
time-consuming and laborious. In contrast, our method auto-
maticallyperformsapartialsoftsemanticsegmentationofthe
content and style images. We augment the CNNMRF model
usedin[29]tofurtherincorporatesoftsemanticmasks,which
can better capture features from the style image and preserve
the structure of the content image. Another issue is that it is
difﬁcult to disentangle content and style, and so the transfer
(a) style
(b) content
(c) [16]
(d) [23]
(e) [29]
(f) [42]
(g) [30]
(h) our method
Fig. 1 Automatic semantic style transfer using deep convolutional neural networks
123

1310
H.-H. Zhao et al.
from one image to another tends to include some (undesired)
elements of content in addition to style. Although we do not
address this problem explicitly, by limiting the transfer of
style to appropriate local regions the issue is mitigated.
We ﬁrst brieﬂy introduce our augmented DCNN archi-
tecture in Sect. 3, followed by details for the style transfer
algorithm in Sect. 4. We then provide details for automatic
semantic mask extraction in Sect. 5. The experimental results
and discussions are presented in Sect. 6, and ﬁnally conclu-
sions are drawn in Sect. 7.
3 Architecture
We now discuss our augmented DCNN architecture which
is based on VGG 19-layer network [39] for style transfer.
It takes as input a content image and a style image, both of
which are fed into the VGG 19-layer network. The DCNN
architecture combines pooling and convolution layers l with
3 × 3 ﬁlters (e.g. the ﬁrst layer after second pooling is called
Conv3_1). Like common DCNNs, the intermediate post-
activation results denoted as xl for the layer l consist of N
channels, which capture patterns from the source images for
each region of the image. Our augmented network is shown
in Fig. 2.
Our augmented network also takes K semantic soft masks
as input, which are down-sampled to produce semantic chan-
nels pl at layer l with the same resolution as xl.
We concatenate them to form the new output with N + K
channels, deﬁned as dl and labelled accordingly for each
layer (e.g. myConv4_1). Before concatenation, the seman-
tic channels are weighted by parameter β to balance their
importance:
dl = (xl, β pl).
(1)
We set β = 20 which we have found experimentally to
provide interesting results.
4 Semantic style transfer optimisation
function
Next, we introduce our style transfer model. We use an
augmented loss function which is based on a patch-based
approach [29] for style transfer, using optimisation to min-
imise content reconstruction error Ec and style remapping
error Es, which combines an MRF and a DCNN model,
given a style image xs
∈
R3×ws×hs, a content image
xc ∈R3×wc×hc and semantic maps mck ∈Rwc×hc and
msk ∈Rwc×hc associated with the content and style images,
respectively (k = 1, 2, . . . , K). For simplicity, the semantic
masks for the content and style images are also collectively
represented as mc ∈Rwc×hc×K and ms ∈Rws×hs×K . The
style transfer result image is denoted by x ∈R3×wc×hc.
Since the synthesised image x is expected to have the same
semantic layout as the content image, we treat mc also as
the semantic masks for the synthesised image. During our
method, we make the high-level neural encoding of x sim-
ilar to xc and use the local patches similar to patches in xs.
As a result, the style of xs is transferred onto the layout of
xc. Meanwhile, we penalise patch matches with inconsistent
semantic masks. We deﬁne an energy function as follows and
seek x that minimises it:
E(x) = α1Es(Φ(x), Φ(xs), Φ(mc), Φ(ms))
+ α2Ec(Φ(x), Φ(xc)).
(2)
Es and Ec are deﬁned as the style loss function and content
loss function, respectively, where Φ(x) is x’s feature map
(activation) that the network outputs in some layer, Φ(xs)
is the feature map (activation) of the style image xs in the
same layer and Φ(mc) and Φ(ms) are the semantic masks
of the content and style images down-sampled to the same
resolution as Φ(x) and Φ(xs). For our method, Es aims to
penaliseinconsistencies inneural activations and/or semantic
masks between x and xs. Ec computes the squared distance
between the feature map of the synthesised image and that of
the content source image xc. Since x is assumed to have the
same content layout as xc, Ec does not involve the semantic
masks.
4.1 Style loss function
We extract all the local patches from Φ(x), denoted as
Ψ (Φ(x)). For a given layer, assuming N is the number of
channels, each patch in Ψi(Φ(x)) has size t × t × N, where
t is the width and height of the patch. Similarly, ˜Ψ (Φ(mck))
and ˜Ψ (Φ(msk)) are the down-sampled semantic masks of
extracted patches, each of size t × t. We deﬁne the modiﬁed
energy function Es incorporating semantic masks as
Es(Φ(x), Φ(xs), Φ(mc), Φ(ms))
=
P

i=1
∥Ψ ∗
i (Φ(x)) −Ψ ∗
N N(i)(Φ(xs)) ∥2
+
P

i=1
K

k=1
∥˜Ψi(Φ(mck)) −˜ΨN N(i)(Φ(msk)) ∥2,
(3)
where P is the number of patches in the synthesised
image. For each patch Ψi(Φ(x)) with semantic masks
˜Ψi(Φ(mck)), we ﬁnd its best matching patch ΨN N(i)(Φ(ms))
orΨN N(i)(Φ(xs))usingnormalisedcross-correlationoverall
Ps example patches in Ψ ∗(Φ(xs)):
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1311
Fig. 2 Style transfer framework with deep neural networks and soft masks
N N(i) := arg max
j=1,...,Ps
Ψ ∗
i (Φ(x)) · Ψ ∗
j (Φ(xs))
| Ψ ∗
i (Φ(x)) | · | Ψ ∗
j (Φ(xs)) |,
(4)
where Ψ ∗
i (Φ(x)) =

Ψi(Φ(x)), β ˜Ψi(Φ(mc))

is the con-
catenation of neural activation and semantic masks for
the ith patch of the synthesised image and Ψ ∗
j (Φ(xs)) =

Ψ j(Φ(xs)), β ˜Ψ j(Φ(ms))

is the concatenation of neural
activation and semantic masks for the jth patch of the style
image. The nearest patch thus takes both style similarity and
semantic consistency into account.
4.2 Content loss function
In order to control the content of the synthesised image, we
deﬁne Ec as the squared Euclidean distance between Φ(x)
and Φ(xc):
Ec(Φ(x), Φ(xc)) = ∥(Φ(x) −Φ(xc)∥2.
(5)
Like method [29], we also minimise Eq. 2 using back-
propagation with L-BFGS. In Eq. 2, α1 and α2 are weights
for the style image and the content image constraints, respec-
tively. According to our experiments, we set α1 = 10−4 and
α2 = 20, and these values can be ﬁne-tuned to interpolate
between the content and the style preservation.
5 Automatic soft semantic mask extraction
Champandard [4] manually generated the semantic masks
that they used in their work to control the style transfer. Fišer
et al. [12] proposed a stylised facial animation method by
using facial segmentation. During [4], each image used one
mask containing semantic labels, where each component (not
necessarily connected) was indicated by a particular pixel
value in the image. Often these values were carefully cho-
sen so that components with similar appearance such as ear
and nose would be assigned similar mask values. Not only
is it tedious to manually segment the image, but for most
images some parts cannot be partitioned accurately. There-
fore, instead of using a single crisp mask to control an image
stylisation, we propose to use a set of soft masks. Such soft
masks provide more information than a single crisp mask
and do not require potentially unreliable boundaries to be
set in the semantic mask, which is especially beneﬁcial at
ill-deﬁned object boundaries.
In this paper, we aim to automatically generate soft
masks. Obviously, this would make mask-based style transfer
more convenient for the user. However, generating appropri-
ate masks is challenging. Ideally, the segmentation of the
style and content images should be consistent, e.g. using
co-segmentation [43]. However, such approaches have not
123

1312
H.-H. Zhao et al.
been developed for semantic segmentation. Moreover, the
different appearance of photographs compared to artwork
(typically used for style images) leads to the cross-depiction
problem [21], so that semantic segmentation techniques
trained on photographs will fail on paintings. In this paper,
we not only demonstrate our approach for the domain of por-
traits, which are a popular topic for style transfer [36], and
non-photorealistic rendering in general, but also show styli-
sation of scenes containing other objects, such as cars and
trains. Portrait style transfer allows us to leverage state-of-
the-art techniques for face detection, which are more robust
than general segmentation methods and are effective even for
many artworks. During our method, facial component masks
are automatically extracted using a combination of semantic
segmentation, facial landmark detection and skin detection.
5.1 Semantic image prediction
Zheng et al. [48] proposed a semantic segmentation method
named CRF-RNN. CRF-RNN achieves a good result on
the popular Pascal VOC segmentation benchmark. This
improvement can be attributed to the uniting of the strengths
of CNNs and CRFs in a single deep network. In our work,
we use CRF-RNN to produce semantic probability maps.
Instead of labelling each pixel with an object category, we
skip over the max pooling stage and extract the neural acti-
vations before that and rescale them to [0, 1]. These are
treated as probability maps predicting the chance of each
pixel belonging to each object category. An example is shown
in Fig. 3.
Using their pre-trained model with 20 categories pro-
vides 20 probability masks which represent different types
of objects. Since most images only contain a small number
of object types, rather than using all 20 semantic masks we
just use a subset of ﬁve so as to reduce memory requirements
and improve efﬁciency. For a given content and style image
pair, the ﬁve semantic masks are automatically selected as
the ﬁve masks maximising their average probability.
We have found that the CRF-RNN is mostly reliable for
photographs. For paintings, its performance degrades, espe-
cially as the style of the artwork becomes more extreme.
However, it is still capable of producing adequate extrac-
tions of people, cars, etc. for many paintings (used as style
images) that we have tested.
5.2 Skin detection
Skin detection is performed on the photographic images [3],
using a rule-based analysis of pixels in YCbCr colour space.
The skin mask is then intersected with the person mask pro-
vided by the CRF-RNN, so as to subdivide the person into
skin and non-skin (e.g. hair, clothing). An example is shown
in Fig. 4.
(a) content
(b) style
(c) probability
map for (a)
(d) probability
map for (b)
Fig. 3 Probabilistic semantic segmentations using CRF-RNN [48] for
person prediction
(a) skin
(b) nose
(c) eyes
(d) mouth
(e) masks
Fig. 4 Segmentation of facial components for the images in Fig. 3
Since skin detection is primarily colour based, it is not in
general effective on artwork due to the typical colour shifts,
as well as distortions caused by strong brush stroke textures.
Therefore, for paintings the facial region is detected using
the face detector, rather than using skin detection.
5.3 Face and facial part segmentation
Facial landmark detection aims to detect key points in human
faces, e.g. eyebrows, eyes, nose. There is an extensive liter-
ature on this topic. For example, Dong et al. [8] developed a
style-aggregated network (SAN) to deal with the large intrin-
sic variance of image styles for facial landmark detection.
One application of facial landmark detection is for face and
facial part segmentation, which is used in our method.
During our method, the facial landmark detection is per-
formed using OpenFace [2], which is based on conditional
local neural ﬁelds, a version of the well-known Constrained
Local Model approach. Alternative facial landmark detection
methods may be used instead. Sixty-eight facial landmarks
are located, from which the eye, nose, inner and outer mouth
regions are determined—see Fig. 4.
Since the facial landmarks only cover the lower half of
the face, the outline of the face is extended upwards and
intersected with the person mask provided by the semantic
segmentation to produce a good approximation to the head
region. This mask is used for artwork. For photographs, the
skin mask is used instead of the extended facial region as it
is more accurate (although prone to noise).
The above steps result in a set of masks that are blurred
to produce soft masks identifying the following objects:
face/skin, nose, eye, mouth, see Fig. 4 for an example. To pro-
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1313
(a) Content
(b) [16]
(c) [23]
(d) [17]
(e) [4]
(f) [29]
(g) [42]
(h) [30]
(i) Our
Fig. 5 Style transfer results from several methods using versions of a content image with varied backgrounds
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 6 Style (a, b, e, f) and content (c, d, g, h) images along with visualisations of their soft masks
vide a more compact visualisation, we also combine the set
of soft masks into a single-colour image, see Fig. 4. The soft
masks for body, background and face/skin are mapped to red,
green and blue, respectively, while the eyes, nose and mouth
values are mapped to cyan, yellow and magenta, respectively.
(Note that when performing style transfer, the multiple soft
image masks are used instead.)
5.4 Background masks
The individual foreground masks are combined by ﬁrst
applying a max operation to the set to produce a single
foreground mask. This is inverted (subtracted from one) to
generate a single background mask.
6 Results
We use the pre-trained VGG 19-layer network with the
augmented layers myConv3_1 and myConv4_1. For lay-
ers relu3_1, relu4_1, myConv3_1 and myConv4_1, we
use 3 × 3 patches, and we set the stride to one. Following
the patch-based approach of [29], we synthesise at multiple
increasing resolutions and randomly initialise the optimisa-
tion. On a GTX Titan with 12GB of GPU RAM, synthesis
takes from 5 to 30 minutes depending on the output quality
and resolution.
We will now compare the proposed method with several
popular methods: Gatys et al. [16], Li et al. [29,30], Ulyanov
et al. [42] which are representative global and local neural
style transfer methods, and [4,17] which use manual segmen-
tation to improve style transfer. Note that for our method,
multiple soft masks were used; the single-colour mask is just
shown for illustrative purposes. For [4], we set the content
weight to 10, style weight to 25 and semantic weight to 100,
and we use the masks from [4] when available and otherwise
manually draw them ourselves. For [17], we used two image
maps of values in the range [0,1] for content and style images
like Fig. 3c, d, similar to the examples used in their paper,
which are also used in our method. To partially overcome
orientation and scale differences between the style and the
content images, we also allow a range of rotations and scal-
123

1314
H.-H. Zhao et al.
Fig. 7 Human style transfer comparison
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1315
Fig. 8 Content (a, c) images along with visualisations of their soft
masks (b, d)
ings to be considered in the CNNMRF, following the settings
in [29].
We use Fig. 3a with several different backgrounds as
the content image and choose Fig. 3b as the style image.
The style transfer results obtained by different methods are
shown in Fig. 5. Considering the six existing methods and by
comparing the results in Fig. 5, it seems that [16,23,29] can-
not transfer the background texture well. Champandard [4]
achieves better background texture transfer, comparable to
our method, but some key facial parts (nose and mouth) are
lost. Gatys et al. [17] can control the spatial texture very well,
but the human style transfer is not so good. It also generates
errors in rows 1 and 2 of Fig. 5d. Because both our method
and [29] are based on the MRF regulariser, and [16,23,30,42]
have also demonstrated some good results, we mainly com-
pare our results with those methods.
Given content and style images in Figs. 6 and 7 show
style transfer applied separately to photographs of men and
women. We transfer the style of each style image to each
content image. We can see from Fig. 7 that our method can
achieve better results than the other methods and avoid errors
in applying style transfer to inappropriate parts. The style
images contain a range of simple and more complicated tex-
tures. In both cases, our method achieves effective results and
preserves the content of the images. Li and Wand [29] can
also achieve interesting results, but only for simple texture
images.
For style images that contain a mixture of textures—e.g.
rows 2 and 6 in Fig. 6—the results of [29] have many errors
in which styles are misapplied. Gatys et al. [16] cannot trans-
fer enough style to content image. Johnson et al. [23], Li et
al. [30] and Ulyanov et al. [42] also generate some imper-
fect results. Our method can achieve better results in speciﬁc
parts in mouth and eyes. For example, in rows 7 and 8 of
Fig. 6, our method can achieve better results in speciﬁc parts
in eyebrows, eyes and mouth area.
Detection of skin and facial parts is affected by differ-
ent skin colours and by signiﬁcant variations in illumination.
However, our pipeline is reasonably robust and is demon-
strated on the content images shown in Fig. 8 which provide
an example with harsh lighting and another with dark skin.
The visualisations of their soft masks in Fig. 8 reveal some
minor errors, but our method produces robust style transfer
results (Fig. 9). Incomparison, weseemanyexistingmethods
have difﬁculty stylising such images.
6.1 Style transfer of different object types
More examples of style transfer for objects like train, car,
bus and boat are shown in Figs. 10 and 11. In these exam-
ples, in the mask images the green part shows the background
probability mask, and the red part shows the object proba-
bility mask. Our method produces better results in all these
examples. The comparison results are shown in Fig. 11. Our
method achieves effective results and preserves the content
of the images. Li and Wand [29] can also achieve interest-
ing results, but the results of [29] still have many errors in
which styles are misapplied. Gatys et al. [16] cannot trans-
fer enough style to content images. Johnson et al. [23], Li
et al. [30] and Ulyanov et al. [42] generate some imperfect
results too. Our method can achieve better results in speciﬁc
parts in background area.
6.2 Comparison of soft masks and binary masks
Figure 12 shows probability masks of trains, and in the mask
imagesthegreenpartshowsthebackgroundprobabilitymask
and the red part shows the object probability mask. We com-
pare our method using soft masks (shown in the second row)
with alternative binary masks (shown in the third row). In
comparison, the results with Fig. 13b, d not only avoid choos-
ing thresholds but also are visually better than with hard
masks (a, c) since more information is preserved.
6.3 Automatic multi-probability map selection
Not only will probability maps provide a richer feature vec-
tor that will beneﬁt the style transfer, but avoiding the need
for thresholding or winner-take-all selection has the potential
to improve robustness. Figures 14 and 15 show an example
in which our automatic semantic mask selection effectively
chooses relevant object types (person and dog). It demon-
strates style transfer using our method when multiple object
categories are present. Note that even though the irrelevant
third–ﬁfth masks contain very little response, it is not a prob-
lem to include them.
Figure 16 shows a further example of style transfer using
multiple classes along with greater variation in pose and
image composition (Fig. 17).
One can see in Fig. 16 that the red colour in the style
image is propagated into the background in Fig. 16g when
the soft mask weight β = 20. When the soft mask weight
is set as β = 25 (i.e. with a higher semantic mask weight),
it effectively constrains style transfer, and no red colour is
propagated into the background, as shown in Fig. 16h.
123

1316
H.-H. Zhao et al.
(a) input
(b) [16]
(c) [23]
(d) [29]
(e) [42]
(f) [30]
(g) Our
Fig. 9 Portrait style transfer comparison for content images with harsh lighting and dark skin
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
Fig. 10 Content (a, b) and style (c, d) images and their soft masks
Our method also allows styles to be transferred from mul-
tiple style images to a single content image. In this case, the
semantic masks are essential to direct the method to choose
suitable patches. Some interesting style transfer results are
shown in Fig. 18 with input images and their masks presented
in Fig. 17.
6.4 Colour control
Our method better preserves objects and their styles, thanks
to soft semantic masks. However, similar to existing neural
style transfer methods, when the same object from the con-
tent and style images has substantially different colours, our
method tends to produce stylised images with colours mixed.
An example is shown in Fig. 19 where the red car in the con-
tent image and the blue car in the style image lead to the
purple car in the stylised result. Gatys et al. [17] proposed
two methods to control colour information in style transfer
which can better preserve colour in the content image during
stylisation, and we demonstrate applying their approach to
our method.
In the ﬁrst approach, we perform style transfer only in the
luminance channel, while keeping the chromiance channels
from the content image unchanged. To improve matching,
before style transfer, we also use intensity mapping for each
luminance pixel Ls in the style image to obtain Ls∗
Ls∗= Dc
Ds
(Ls −us) + uc,
(6)
where uc and us are the mean luminances of the content
and style images, respectively, and Ds and Dc are their stan-
dard deviations. For the second method, we apply colour
histogram matching before style transfer. Each RGB pixel
Ps in the style image is transformed as
P∗
s = APs + a,
(7)
where A is a 3×3 matrix and a is a 3-vector such that after the
transformation, the mean and covariance of the style image
match the content image. The colour control results based on
the two methods are shown in Fig. 19. The standard result
is shown in Fig. 19a. We can see that it can achieve a good
colour control result by using the luminance channel and
is shown as Fig. 19b. However, by using colour histogram
matching method, there is some wrong colour transferred in
background in Fig. 19c. This is because a global transforma-
tion is not sufﬁcient to capture colour differences between
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1317
(a) input
(b) [16]
(c) [23]
(d) [29]
(e) [42]
(f) [30]
(g) Our
Fig. 11 Objects style transfer comparison
(a) content and
content mask
(b) style image
ands tyle mask
(c) style image
and style mask
Fig. 12 Images and their masks (second row: soft masks, third row:
binary masks)
the content and style images. The method may work bet-
ter if the transformation is applied separately for individual
(a)
(b)
(c)
(d)
Fig. 13 Comparison of style transfer results obtained with binary (a,
c) and soft (b, d) masks
objects. However, since our method does not produce hard
segmentation, it is not obvious how this can be achieved.
6.5 Failure cases and limitations
Different categories of segmentation failures can occur and
affect the style transfer results in different ways. If an object
present in the content image does not exist in the style image,
then the transfer result for that object will be similar to the
results for the baseline CNNMRF method that we use [29].
If an object in the style image is mis-recognised as another
object, then there are two possibilities. If this object does not
123

1318
H.-H. Zhao et al.
(a) image
(b) 1st mask
(c) 2nd mask
(d) 3rd mask
(e) 4th mask
(f) 5th mask
(g) head masks
(h) background
Fig. 14 Object style transfer with automatic probability map selection. a Content and style images, b–f the automatically selected top ﬁve semantic
masks, g head masks, h background mask
(a) input
(b) [16]
(c) [23]
(d) [29]
(e) [42]
(f) [30]
(g) Our
Fig. 15 Multi-object style transfer
(a) input
(b) [16]
(c) [23]
(d) [29]
(e) [42]
(f) [30]
(g) Our β = 20 (h) Our β = 25
Fig. 16 Multi-object style transfer with different image compositions and poses
(a)
(b)
(c)
Fig. 17 Content (a) and style (b, c) images and their soft masks
exist in the content image, then again the result will be similar
to the baseline method. Alternatively, the faulty classiﬁcation
will result in faulty style transfer.
Anexampleoftheeffectofasegmentationfailureisshown
in Fig. 20. The content image is reasonably well segmented,
but the more challenging task of segmenting the artwork used
as the style image contains greater errors. While the skin
(generated using the face ﬁtted by OpenFace) is detected rea-
sonably well in the style image, the head (generated using the
CRF-RNN) is missing the left-hand part of the hair. This has
caused our method to match part of the content image’s back-
ground to hair and consequently stylise it as hairy. Despite
this ﬂaw, the result is still considerably better than the base-
line CNNMRF method which contains many instances of
inappropriate style transfer.
Our method relies on automatically calculated semantic
masks. When some objects, either in the content image or in
the style image, are not correctly detected, the method cannot
ﬁnd the correct matching for the missing objects. Figures 21
and 22 show some failure examples of this kind.
One can see from Fig. 21 that, because the dog in the
style image is not correctly detected in semantic segmen-
tation (Fig. 21f), the style of the dog in Fig. 21d is not
transferred to the dog in the output image. In the example
shown in Fig. 22, the dog in the content image in Fig. 22a
is not segmented correctly where part of it is considered to
belong to the man, and the style (red clothing) from the man
in the style image is erroneously transferred to the dog in the
output image.
6.6 Modifying the number of masks
The semantic segmentation signiﬁcantly affects the style
transfer results. For some style images, for example some
paintings of portraits, it is difﬁcult to automatically segment
the face, skin, month, eyes, etc. and to properly segment the
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1319
(a) input
(b) [16]
(c) [23]
(d) [29]
(e) input
(f) [42]
(g) [30]
(h) Our
Fig. 18 Multi-object style transfer
(a) Our result
(b) Luminance-only
style transfer
(c) Colour histogram
matching
Fig. 19 Colour control results
(a) style image (b) soft mask
(c) [29]
(d) Our
Fig.20 Result showing the effects on style transfer with faulty semantic
segmentation
background and foreground. If the accuracy and reliability
of the semantic segmentation can be improved, this will lead
to better style transfer results. Figure 23 shows an experi-
ment in which the number of labels in the semantic masks
is increased and demonstrates the importance of separately
labelling all the major components of the face.
6.7 Modifying the soft mask weight
There are three parameters in our style transfer model α1, α2
and β which are the weights for the style, content and seman-
ticmasklossterms.Sincetheeffectofα1 andα2 isconsidered
in [29], we focus on studying the effect of β. By default, we
set the soft mask weight β = 20. This value can be adjusted
to control the importance of semantic compliance. Figure 24
demonstrates the effect of modifying β using the content
image in Fig. 6c and style image in Fig. 3b, where α1 = 10−4,
α2 = 20. When β is too small, the result does not have sufﬁ-
cient semantic control and can produce semantically wrong
matches. On the other hand, setting β too large may result
in matched patches having poor content/style consistency.
According to our experiments, β ∈[15, 35] achieves best
results.
(a) content image
(b) woman
(c) dog
(d) style image
(e) woman
(f) dog
(g) Output
Fig. 21 Result showing the effects of stylisation with a missing style object in semantic segmentation
123

1320
H.-H. Zhao et al.
(a) content image
(b) man
(c) dog
(d) style image
(e) man
(f) dog
(g) Output
Fig. 22 Result showing the effects of style transfer with a missing content object in semantic segmentation
(a) no mask
(b) 2 masks
(c) 3 masks
(d) 6 masks
Fig. 23 Result showing the effects on style transfer with an increasing
number of labelled objects in the soft masks
(a) β = 0
(b) β = 10
(c) β = 20
(d) β = 30
(e) β = 40
(f) β = 80
Fig. 24 Result showing the effects of varying parameter β
6.8 Modifying the content and style weight
Further experiments are carried out, in which α1 and α2 are
modiﬁed while β = 20 is ﬁxed. Figure 25 demonstrates the
(a) α
α
α
α
α
α
1 = 10−1
(b)
1 = 10−2
(c)
1 = 10−3
(d)
1 = 10−4
(e)
1 = 10−5
(f)
1 = 10−6
Fig. 25 Result showing the effects of varying parameter α1
effect of modifying α1 using the content image in Fig. 6c
and style image in Fig. 3b, where α2 = 20. Using the same
images, Fig. 26 demonstrates the effect of modifying α2,
where α1 = 10−4.
When α1 or α2 is too small, the result does not have suf-
ﬁcient content/style information. On the other hand, setting
α1 or α2 too large may result in matched patches having
poor style/content consistency. According to our experi-
ments, α1 ∈[10−3, 10−4] and α2 ∈[20, 40] achieve best
results.
6.9 User evaluation
In addition to visual inspection, we also performed a quanti-
tative comparison with ﬁve existing methods. Since there is
no standard automatic style transfer measure or test, we per-
formed a user study in which the users were presented with a
style image, a content image and stylised output images from
the following six methods: I [16], II [23], III [29], IV, which
is our method, V [42] and VI [30].
The user study is designed using the 2AFC (Two-
alternative forced choice) paradigm, widely used in percep-
tual studies due to its simplicity and reliability. In each trial,
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1321
(a) α
α
α
α
α
α
2 = 10
(b)
2 = 20
(c)
2 = 40
(d)
2 = 60
(e)
2 = 80
(f)
2 = 100
Fig. 26 Result showing the effects of varying parameter α2
Fig. 27 Boxplots of user preferences for six different style transfer
methods in task 1, showing the mean (red lines), quartiles (blues lines)
and extremes (black lines) of the distributions
the user was asked to complete two tasks by answering the
following questions:
– Task 1 Given the two result images, which image better
matches the target style?
– Task 2 Given the two result images, which image do you
prefer?
For each question, the user can choose either of the two result
images.
To make the comparison more meaningful while limiting
the user effort to a reasonable level, we used the full set of
results that were contained in Figs. 3, 6 and 10, containing
12×6 = 72 test images (six sets for human faces and six sets
without human faces) and stylised results generated by the
six methods. For each task, 50 users participated in the user
study, with ages ranging from 17 to 54. To avoid bias, we
randomised the order of image pairs shown as well as their
left/right position. Altogether, the results of each method are
compared against 12×5 = 60 results of alternative methods.
We recorded the total number of user preferences (clicks) for
each method and treat these as random variables.
Fig. 28 Boxplots of user preferences for four different style trans-
fer methods in task 2, showing the mean (red lines), quartiles (blues
lines),and extremes (black lines) of the distributions
Table 1
p value of the ANOVA test of the proposed method against
the other methods for both tasks
Method
I [16]
II [23]
III [29]
V [42]
VI [30]
Task 1
8.0319e−05
0.0102
0.0172
0.0041
0.0129
Task 2
8.3080e−07
0.0163
0.0224
0.0304
0.0025
We performed the ANOVA test, and the results are shown
in Figs. 27 and 28. The p values comparing our method and
alternative methods are shown in Table 1. They show that the
method proposed in this paper has the highest mean score
and is preferred by the majority of the users. The difference
between our method and alternative methods is statistically
signiﬁcant (at the level of 0.05).
7 Conclusions
Our paper demonstrates the beneﬁts of automatic seman-
tic mask extraction by combining state-of-the-art methods
for both semantic segmentation and facial features. Using
soft masks helps mitigate this, but there is certainly scope
to improve semantic segmentation or to develop methods
dedicated to generating soft semantic masks. In most cases,
soft masks can achieve better results than binary masks,
especially in uncertain areas. The probability maps show
the likelihood of having speciﬁc objects in the image and
can help capture elements of the styles for objects in the
style image and preserve the structure of the content image.
The artwork can lead to problems with general segmenta-
tion methods which are mainly intended for photographs of
natural scenes. Therefore, we use a different approach to
extract facial skin for artworks, as compared to photographs,
of people. However, if the artwork is so highly abstracted
that automatic segmentation is impossible or unreliable, then
a semi-automatic approach to segmentation should be used.
There remain some areas with scope for improvement,
which suggests the following future work:
123

1322
H.-H. Zhao et al.
– Fine-tuning the weights of the semantic masks can be
used to achieve different sylisations. In the future, we will
carry out more extensive experiments to (1) determine
which semantic weights produce the best style trans-
fer results and (2) investigate the relationship between
semantic weights and the input images.
– The probability maps show the likelihood of having spe-
ciﬁc objects in the image and can help capture elements
of the styles for objects in the style image and pre-
serve the structure of the content image. Therefore, this
approach can be applied to improving special applica-
tions for which this is a requirement, such as makeup
transfer.
– Although our method is robust to minor segmentation
errors, better segmentation would obviously lead to bet-
ter stylisation results, as more appropriate patches will
be matched and chosen, leading to fewer faulty instances
of style transfer.
In the future, we will test the effectiveness of using the
most recent segmentation methods to obtain semantic
maps.
Our current method can only perform 2D image style
transfer, not geometric style transfer. It is an interesting future
direction which we will investigate as future work.
Acknowledgements This work was supported by National Natural
Science Foundation of China (61503128), Science and Technology
Plan Project of Hunan Province (2016TP1020), Scientiﬁc Research
Fund of Hunan Provincial Education Department (16C0226, 18A333),
Hengyang guided science and technology projects and Application-
oriented Special Disciplines (Hengkefa [2018]60-31), Double First-
Class University Project of Hunan Province (Xiangjiaotong [2018]469),
Hunan Province Special Funds of Central Government for Guid-
ing Local Science and Technology Development (2018CT5001) and
Subject Group Construction Project of Hengyang Normal University
(18XKQ02), Key Programme (61733004). We would like to thank
NVIDIA for the GPU donation.
References
1. Azadi, S., Fisher, M., Kim, V., Wang, Z., Shechtman, E., Darrell, T.:
Multi-content GAN for few-shot font style transfer. arXiv preprint
arXiv:1712.00516 (2017)
2. Baltrušaitis, T., Robinson, P., Morency, L.P.: OpenFace: an open
source facial behavior analysis toolkit. In: Winter Conference on
Applications of Computer Vision, pp. 1–10 (2016)
3. Brancati,N.,DePietro,G.,Frucci,M.,Gallo,L.:Humanskindetec-
tion through correlation rules between the YCb and YCr subspaces
based on dynamic color clustering. Comput. Vis. Image Underst.
155, 33–42 (2017)
4. Champandard, A.J.: Semantic style transfer and turning two-bit
doodlesintoﬁneartworks.arXivpreprintarXiv:1603.01768(2016)
5. Chang, H., Lu, J., Yu, F., Finkelstein, A.: PairedCycleGAN: asym-
metric style transfer for applying and removing makeup. In: The
IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2018)
6. Criminisi, A., Pérez, P., Toyama, K.: Region ﬁlling and object
removal by exemplar-based image inpainting. IEEE Trans. Image
Process. 13(9), 1200–1212 (2004)
7. Deng, X.: Enhancing image quality via style transfer for single
image super-resolution. IEEE Signal Process. Lett. 25, 571–575
(2018)
8. Dong, X., Yan, Y., Ouyang, W., Yang, Y.: Style aggregated net-
work for facial landmark detection. In: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2018)
9. Efros, A.A., Freeman, W.T.: Image quilting for texture synthe-
sis and transfer. In: Proceedings of the 28th Annual conference
on Computer Graphics and Interactive Techniques, pp. 341–346.
ACM (2001)
10. Efros, A.A., Leung, T.K.: Texture synthesis by non-parametric
sampling. In: Proceeding of the International Conference on Com-
puter Vision, vol. 2, pp. 1033–1038. IEEE (1999)
11. Face++: Face++. https://www.faceplusplus.com/face-detection/.
Accessed 4 Apr 2015
12. Fišer, J., Jamriška, O., Simons, D., Shechtman, E., Lu, J., Asente, P.,
Lukáˇc, M., Sýkora, D.: Example-based synthesis of stylized facial
animations. ACM Trans. Graph. 36(4), 155:1–155:11 (2017)
13. Freeman,W.T.,Pasztor,E.C.,Carmichael,O.T.:Learninglow-level
vision. Int. J. Comput. Vis. 40(1), 25–47 (2000)
14. Frigo, O., Sabater, N., Delon, J., Hellier, P.: Split and match:
example-based adaptive patch sampling for unsupervised style
transfer. In: Proceedings of the Conference on Computer Vision
and Pattern Recognition, pp. 553–561 (2016)
15. Gatys, L., Ecker, A.S., Bethge, M.: Texture synthesis using con-
volutional neural networks. In: Advances in Neural Information
Processing Systems, pp. 262–270 (2015)
16. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using
convolutionalneuralnetworks.In:ProceedingsoftheIEEEConfer-
ence on Computer Vision and Pattern Recognition, pp. 2414–2423
(2016)
17. Gatys, L.A., Ecker, A.S., Bethge, M., Hertzmann, A., Shechtman,
E.: Controlling perceptual factors in neural style transfer. In: IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)
(2017)
18. Gilbert, A., Collomosse, J., Jin, H., Price, B.: Disentangling struc-
ture and aesthetics for style-aware image completion. In: The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)
(2018)
19. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hier-
archies for accurate object detection and semantic segmentation.
In: Proceedings of the Conference on Computer Vision and Pattern
Recognition, pp. 580–587 (2014)
20. Gooch, A., Gooch, B., Shirley, P., Cohen, E.: A non-photorealistic
lighting model for automatic technical illustration. In: Conference
on Computer Graphics and Interactive Techniques (1998)
21. Hall, P., Cai, H., Wu, Q., Corradi, T.: Cross-depiction problem:
recognition and synthesis of photographs and artwork. Comput.
Vis. Media 1(2), 91–103 (2015)
22. Isenberg, T.: Visual abstraction and stylisation of maps. Cartogr. J.
50(1), 8–18 (2013)
23. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time
style transfer and super-resolution. In: European Conference on
Computer Vision, pp. 694–711. Springer (2016)
24. Kang, S.B., Kang, S.B., Kang, S.B., Kang, S.B., Kang, S.B.: Visual
attribute transfer through deep image analogy. ACM Trans. Graph.
36(4), 120 (2017)
25. Kim, B.C., Azevedo, V., Gross, M., Solenthaler, B.: Transport-
Based Neural Style Transfer for Smoke Simulations (2019).
arXiv:1905.07442
26. Kwatra, V., Essa, I., Bobick, A., Kwatra, N.: Texture optimization
for example-based synthesis. ACM Trans. Graph. ToG 24(3), 795–
802 (2005)
123

Automatic semantic style transfer using deep convolutional neural networks and soft masks
1323
27. Kwatra, V., Schödl, A., Essa, I., Turk, G., Bobick, A.: Graphcut
textures: image and video synthesis using graph cuts. In: ACM
TransactionsonGraphics(ToG),vol.22,pp.277–286.ACM(2003)
28. Lerotic, M., Chung, A.J., Mylonas, G., Yang, G.Z.: Pq-space Based
non-photorealistic rendering for augmented reality. In: Ayache
N., Ourselin S., Maeder A. (eds.) Medical Image Computing and
Computer-Assisted Intervention—MICCAI 2007. Lecture Notes
in Computer Science, vol 4792, pp. 102–109. Springer, Berlin,
Heidelberg (2007)
29. Li, C., Wand, M.: Combining Markov random ﬁelds and convo-
lutional neural networks for image synthesis. In: Proceedings of
the Conference on Computer Vision and Pattern Recognition, pp.
2479–2486 (2016)
30. Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H.: Univer-
sal style transfer via feature transforms. In: Advances in Neural
Information Processing Systems, pp. 386–396 (2017)
31. Liu, L., Ouyang, W., Wang, X., Fieguth, P.W., Chen, J., Liu, X.,
Pietikäinen, M.: Deep learning for generic object detection: a sur-
vey. CoRR arXiv:1809.02165 (2018)
32. Luan, F., Paris, S., Shechtman, E., Bala, K.: Deep photo style trans-
fer. In: The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2017)
33. Luft, T., Kobs, F., Zinser, W., Deussen, O.: Watercolor illustra-
tions of cad data. In: International Symposium on Computational
Aesthetics in Graphics Visualization and Imaging (2008)
34. Noh, H., Hong, S., Han, B.: Learning deconvolution network for
semantic segmentation. In: Proceedings of the International Con-
ference on Computer Vision, pp. 1520–1528 (2015)
35. Ruder, M., Dosovitskiy, A., Brox, T.: Artistic style transfer for
videos. In: German Conference on Pattern Recognition, pp. 26–
36. Springer (2016)
36. Selim, A., Elgharib, M., Doyle, L.: Painting style transfer for head
portraits using convolutional neural networks. ACM Trans. Graph.
TOG 35(4), 129 (2016)
37. Shelhamer, E., Long, J., Darrell, T.: Fully convolutional networks
for semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell.
39(4), 640–651 (2017)
38. Shih, Y., Paris, S., Barnes, C., Freeman, W.T., Durand, F.: Style
transfer for headshot portraits. ACM Trans. Graph. TOG 33(4),
148:1–148:14 (2014)
39. Simonyan, K., Zisserman, A.: Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556
(2014)
40. Thoma, M.: A survey of semantic segmentation. arXiv preprint
arXiv:1602.06541 (2016)
41. Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.: Texture net-
works: feed-forward synthesis of textures and stylized images. In:
International Conference on Machine Learning (ICML) (2016)
42. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture net-
works: maximizing quality and diversity in feed-forward styliza-
tion and texture synthesis. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, vol. 1, p. 6 (2017)
43. Vicente, S., Rother, C., Kolmogorov, V.: Object cosegmentation.
In: Conference on Computer Vision and Pattern Recognition, pp.
2217–2224. IEEE (2011)
44. Wei, L.Y., Levoy, M.: Fast texture synthesis using tree-structured
vector quantization. In: Proceedings of the 27th Annual Conference
on Computer Graphics and Interactive Techniques, pp. 479–488.
ACM Press/Addison-Wesley Publishing Co. (2000)
45. Yang, Y., Zhao, H., You, L., Tu, R., Wu, X., Jin, X.: Semantic
portrait color transfer with internet images. Multimed. Tools Appl.
76(1), 523–541 (2017)
46. Zhang, H., Dana, K.: Multi-style generative network for real-time
transfer. arXiv preprint arXiv:1703.06953 (2017)
47. Zhang, W., Cao, C., Chen, S., Liu, J., Tang, X.: Style transfer via
image component analysis. IEEE Trans. Multimed. 15(7), 1594–
1601 (2013)
48. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z.,
Du: Conditional random ﬁelds as recurrent neural networks. In:
Proceedings of the International Conference on Computer Vision,
pp. 1529–1537 (2015)
Publisher’s Note Springer Nature remains neutral with regard to juris-
dictional claims in published maps and institutional afﬁliations.
Hui-Huang
Zhao
received his
Ph.D. degree in 2010 from XiD-
ian University. He was a Spon-
sored Researcher in the School
of Computer Science and Infor-
matics, Cardiff University. Now
he is an Associate Professor in
the College of Computer Science
and Technology, Hengyang Nor-
mal University. His main research
interests
include
solder
joint
inspection, compressive sensing,
machine learning, and image pro-
cessing.
Paul L. Rosin received his B.Sc.
degree in Computer Science and
Microprocessor Systems in 1984
from Strathclyde University, Glas-
gow, and Ph.D. degree in Infor-
mation Engineering from City
University, London, in 1988. He
is a full professor in the School of
Computer Science and Informat-
ics, Cardiff University. His main
research interests include non-ph-
otorealistic rendering, mesh pro-
cessing, and computer vision.
123

1324
H.-H. Zhao et al.
Yu-Kun Lai received his bache-
lors and Ph.D. degrees in Com-
puter Science from Tsinghua Uni-
versity in 2003 and 2008, respec-
tively. He is currently a Reader of
Visual Computing in the School
of Computer Science and Infor-
matics, Cardiff University. His
research interests include comp-
uter graphics, geometry process-
ing, image processing, and com-
puter vision. He is on the editorial
board of The Visual Computer.
Yao-Nan Wang received his B.S.
degree in Computer Engineering
from the East China University of
Technology, Nanchang, China, in
1981, and M.S. and Ph.D. degrees
in Electrical Engineering from
Hunan
University,
Changsha,
China, in 1990 and 1994, respec-
tively. From 1998 to 2000, he was
a Senior Humboldt Fellow in Ger-
many, and from 2001 to 2004, he
was a Visiting Professor with the
University of Bremen, Bremen,
Germany. Since 1995, he has been
a Professor with Hunan Univer-
sity. His current research interests include intelligent control, image
processing, and computer vision systems for industrial applications.
123

