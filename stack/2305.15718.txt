Towards Higher Pareto Frontier in Multilingual Machine Translation
Yichong Huang:, Xiaocheng Feng:;, Xinwei Geng:, Baohang Li:, Bing Qin:;
:Harbin Institute of Technology
; Peng Cheng Laboratory
{ychuang,xcfeng,xwgeng,baohangli,qinb}@ir.hit.edu.cn
Abstract
Multilingual neural machine translation has
witnessed remarkable progress in recent years.
However, the long-tailed distribution of multi-
lingual corpora poses a challenge of Pareto op-
timization, i.e., optimizing for some languages
may come at the cost of degrading the perfor-
mance of others. Existing balancing training
strategies are equivalent to a series of Pareto
optimal solutions, which trade off on a Pareto
frontier1.
In this work, we propose a new
training framework, Pareto Mutual Distillation
(Pareto-MD), towards pushing the Pareto fron-
tier outwards rather than making trade-offs.
Specifically, Pareto-MD collaboratively trains
two Pareto optimal solutions that favor differ-
ent languages and allows them to learn from
the strengths of each other via knowledge distil-
lation. Furthermore, we introduce a novel strat-
egy to enable stronger communication between
Pareto optimal solutions and broaden the appli-
cability of our approach. Experimental results
on the widely-used WMT and TED datasets
show that our method significantly pushes the
Pareto frontier and outperforms baselines by up
to +2.46 BLEU2.
1
Introduction
Multilingual neural machine translation (MNMT)
is a popular paradigm that uses a unified model to
handle the entire translation process for multiple
language pairs (Ha et al., 2016; Firat et al., 2016;
Johnson et al., 2017). This paradigm is particu-
larly effective at improving the performance of low-
resource languages through transfer learning (Aha-
roni et al., 2019; Dabre et al., 2020; Siddhant et al.,
2022). Besides, MNMT is highly deployable since
only one model is required (Fan et al., 2021; Yang
et al., 2021; NLLB Team et al., 2022).
However, the severely imbalanced distribution
of multilingual training data puts the MNMT in
1In Pareto optimization, Pareto optimal solutions refer to
solutions in which none of the objectives can be improved
without sacrificing at least one of the other objectives. The set
HRLs
LRLs
A
A'
C
B
B'
5
7
9
13
10
12
16
18
14
11
Figure 1: Multilingual performance frontier shifts out-
wards. X-axis and Y-axis indicate the performance
of Low-Resource Languages and High-Resource Lan-
guages, respectively. Existing methods reflect a trade-
off on the Pareto frontier (i.e., the gray curve). Our work
aims to push the original Pareto frontier i.e., the blue
dotted curve. To this effect, we ameliorate each individ-
ual model‚Äôs shortcoming while retaining their strengths,
e.g., moving right the solution A to A1 and moving up
the solution B to B1, via our Pareto Mutual Distillation.
a situation of Pareto optimization (also known as
multi-objective optimization). That is, when some
languages are optimized, others degenerate. Ex-
isting methods can be considered a set of Pareto
optimal solutions that trade off on a Pareto fron-
tier, which focus on balancing the performance
across different languages by adjusting the sam-
pling distribution (Arivazhagan et al., 2019; Wang
et al., 2020; Wu et al., 2021). The widely-used
temperature-based sampling (Arivazhagan et al.,
2019) is typical evidence of the claim above, which
uses a hyper-parameter to smooth the training dis-
tribution over all language pairs to enhance the rep-
resentation of low-source Languages (LRLs) while
sacrificing the which of High-Resource Languages
(HRLs). Despite the emergence of several sophis-
of all Pareto optimal solutions forms a Pareto frontier.
2Our code is publicly available at https://github.com/
OrangeInSouth/Pareto-Mutual-Distillation
arXiv:2305.15718v1  [cs.CL]  25 May 2023

ticated dynamic sampling technologies designed
to overcome the inflexibility of temperature-based
sampling, their performance remains restricted to
this Pareto frontier (Wang et al., 2020; Zhou et al.,
2021; Zhang et al., 2021).
In this work, we propose a novel training frame-
work, named Pareto Mutual Distillation (Pareto-
MD), to push the Pareto frontier of multilingual
models. Specifically, Pareto-MD uses different
training distributions that favor dissimilar subsets
of languages to train two multilingual models si-
multaneously. These two models learn from each
other at each training step with knowledge distil-
lation. The underlying idea of Pareto-MD is to
address shortcomings of individual Pareto optimal
solutions via access to a better one in terms of that
shortcoming, thereby raising the Pareto frontier, as
Fig. 1 depicts. To fully exploit the potential of our
approach in multilingual settings, we further pro-
pose Automatic Pareto Mutual Distillation, which
dynamically determines the contribution of distilla-
tion learning loss on each objective. These contri-
butions, controlled by a set of distillation weights,
adapt automatically to the evolving models, elimi-
nating the need for manual hyper-parameter search.
While our method applies essentially to any
multi-objective optimization problem, we specif-
ically demonstrate its benefit on multilingual ma-
chine translation. The experimental results on two
widely-used datasets demonstrate the effectiveness
of our method, which improves up to +2.46 BLEU,
and the further analysis shows the Pareto frontier
is pushed outwards visibly.
2
Preliminaries
Neural machine translation (NMT) is a classic NLP
task that translates a sentence x in source language
into a sentence y in target language (Kalchbrenner
and Blunsom, 2013; Sutskever et al., 2014; Bah-
danau et al., 2015; Vaswani et al., 2017). Given a
parallel corpus D ‚Äú tpx, yq P X ÀÜ Yu, the NMT
model is commonly trained by minimizing the neg-
ative log-likelihood loss:
Lce ‚Äú
√ø
px,yq ‚ÄûD
√ø
iƒè|y|
¬¥ log ppyi|x, yƒÉi; Œ∏q,
(1)
where pp¬®|¬®; Œ∏q maps the source sentence and previ-
ous generated text to the next target token.
2.1
Multilingual Machine Translation
Given a set of language pairs L, the MNMT
model is trained on the combination of |L| parallel
datasets: tDtrain
‚Ñì
u|L|
‚Ñì‚Äú1, where Dtrain
‚Ñì
is the dataset
of language pair pS‚Ñì, T‚Ñìq. In order to encode and
decode the text in various languages into and from
a universal semantic space, a large multilingual
vocabulary V is constructed. The language tag is
appended to the beginning of source sentences as a
hint of the target language. The MNMT model is
also trained with the loss function as Eq.1 over the
multilingual datasets.
Temperature-based Sampling.
The multilin-
gual datasets form a distribution P, where Pp‚Ñìq ‚Äú
N‚Ñì
≈ô
j Nj is the sampling probability of language pair
‚Ñìand we denote the dataset size of Dtrain
‚Ñì
by N‚Ñì.
Since sampling probabilities of LRLs are substan-
tially lower than those of HRLs, the optimization
towards LRLs can be overwhelmed by those of
HRLs. To resolve this issue, Arivazhagan et al.
(2019) propose temperature-based sampling, intro-
ducing a hyper-parameter œÑ to re-scale the smooth-
ness of training distribution. Concretely, the sam-
pling probability of each language pair ‚Ñìis set to:
Pp‚Ñìq ‚Äú
N1{œÑ
‚Ñì
≈ô
j N1{œÑ
j
,
(2)
increasing the value of œÑ produces smoother train-
ing distributions and stronger preferences on LRLs.
2.2
Mutual Distillation
Knowledge Distillation (KD) is a popular tech-
nology for knowledge transfer, which originates
from compressing a static high-capacity model
(teacher model) into a small compact model (stu-
dent model) (Hinton et al., 2015). Mutual distilla-
tion is a variant of KD (Zhang et al., 2018; Guo
et al., 2020). Instead of using a pre-trained teacher
model, mutual distillation involves training more
than one model simultaneously, with each model
teaching the other throughout the training process.
Mutual distillation takes the same loss function as
vanilla knowledge distillation, that is:
Lkd ‚Äú
√ø
iƒè|y|
√ø
wPV
¬¥ ppw|x, yƒÉi; Œ∏T q
¬® log ppw|x, yƒÉi; Œ∏Sq,
(3)
where V is the target-side vocabulary, Œ∏S and Œ∏T
are the student model and teacher model. The ma-
jor difference of Pareto-MD from vanilla mutual
distillation is that we train two models with dif-
ferent sampling distributions to make them favor
different sets of objectives.

Model-1
Model-2
P2
Sampling
P1
Standard Training
Distillation Learning‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã
Sampling
ÀÜ‚Üµ1r`s
ÀÜ‚Üµ2r`s
px`, y`q
px`1, y`1q
Figure 2: Illustration of Pareto-MD, using different
sampling distributions to train two models. At each step,
both models additionally mimic the output of each other
via knowledge distillation. The distillation learning of
each model is weighted by language-specific distillation
weights Œ±ir‚Ñìs deduced with specific strategies.
3
Pareto Mutual Distillation
In this section, we first introduce our training frame-
work Pareto-MD (¬ß3.1). Next, two strategies that
determine the important distillation weights, UNI-
PMD and BI-PMD, are shown (¬ß3.2). To overcome
the flaws of these two strategies above, AUTO-
PMD is further proposed (¬ß3.3).
3.1
Framework
We illustrate our Pareto-MD in Fig. 2. Pareto-MD
simultaneously trains two models, denoted by Œ∏1
and Œ∏2, using different sampling distributions, P1
and P2, that make each model favor a different set
of language pairs. To obtain expected distributions,
we adopt temperature-based sampling, as shown
in Eq.2, and set œÑ ‚Äú 1 for P1, œÑ ƒÖ 1 (e.g., œÑ ‚Äú 5
commonly) for P2. In this way, Œ∏1 prefers HRLs,
and Œ∏2 prefers LRLs.
At each training step, for each model Œ∏i where
i P t1, 2u, Pareto-MD first draws a language pair
‚Ñìfrom training distribution Pi, then a mini-batch
of sentence pairs B‚Ñì‚Äú tx‚Ñì, y‚Ñìu are sampled from
Dtrain
‚Ñì
. Next, the model Œ∏i is trained to fit B‚Ñìand
match the output of the other model, i.e., Œ∏3¬¥i. The
overall loss function for model Œ∏i is defined as:
LPMD ‚Äú p1 ¬¥ Œ±ir‚Ñìsq ÀÜ LcepBl; Œ∏iq
` Œ±ir‚Ñìs ÀÜ LkdpB‚Ñì; Œ∏i, Œ∏3¬¥iq,
(4)
where Œ±i P R|L| is the multilingual distillation
weight vector of Œ∏i and Œ±ir‚Ñìs P r0, 1s is the distil-
lation weight for language pair ‚Ñì. Œ±ir‚Ñìs is crucial
as controlling the extent how much Œ∏i should learn
from Œ∏3¬¥i in direction ‚Ñì. When Œ±ir‚Ñìs ‚Äú 0, Œ∏i does
not acquire information from Œ∏3¬¥i in ‚Ñì. The values
Algorithm 1: Pareto-MD
Input
: Datasets tDtrain
‚Ñì
u|L|
‚Ñì‚Äú1, two training
distributions P1, P2, learning rate Œ∑,
distillation weights updating strategy S,
updating interval T
Initialize: Randomly initialize model Œ∏1 and Œ∏2, set
multilingual distillation weights
Œ±1, Œ±2 ‚Äú 0, training step t ‚Äú 0
1 while not converged do
2
t √ê t ` 1
3
for i √ê 1 to 2 do
4
Sample a language pair ‚Ñìfrom Pi
5
Draw a batch of samples B‚Ñìfrom Dtrain
‚Ñì
6
Œ∏i √ê Œ∏i ¬¥ Œ∑‚àáŒ∏iLP MDpB‚Ñì; Œ∏i, Œ∏3¬¥i, Œ±ir‚Ñìsq
7
end
8
if t % T ‚Äú 0 then
9
Update Œ±1, Œ±2 with the specific strategy S
10
end
11 end
of Œ±i are determined by the specific strategy. We
summarize the whole training framework in Alg.1.
3.2
UNI-PMD and BI-PMD
Multilingual distillation weights Œ±i play important
roles in Pareto-MD. We present two strategies, uni-
directional Pareto mutual distillation (UNI-PMD)
and bidirectional Pareto mutual distillation (BI-
PMD), for determining the values of Œ±i based on
different design philosophies.
UNI-PMD.
UNI-PMD is designed based on the
intuition that each model should only learn from the
strengths and avoid mimicking the shortcomings
of the other model. Therefore, in each translation
direction ‚Ñì, UNI-PMD lets the model that performs
less well, denoted by Œ∏worse
‚Ñì
, be distilled by the
model that performs better in this direction, de-
noted by Œ∏better
‚Ñì
, via setting a positive distillation
weight. Conversely, UNI-PMD zeros the weight to
forbid Œ∏better
‚Ñì
from being influenced by Œ∏worse
‚Ñì
.
Formally, given multilingual validation datasets
tDvalid
‚Ñì
u|L|
‚Ñì‚Äú1 and a pre-defined hyper-parameter
Œ± P r0, 1s, in each direction ‚ÑìP L, UNI-PMD
sets the distillation weight of Œ∏i as:
Œ±ir‚Ñìs ‚Äú Œ± ÀÜ 1ti ‚Äú arg max
jPt1,2u
LcepDvalid
l
; Œ∏jqu,
(5)
where the 1t¬®u is an indicator function, indicating
whether the model Œ∏i performs less well on the
translation of ‚Ñì. UNI-PMD updates the distillation
weights every T steps.
BI-PMD.
Besides, we design another strategy
BI-PMD based on the hypothesis that among the
two models that are trained with Pareto-MD, in

Model
Fr‚ÜíEn
De‚ÜíEn Ro‚ÜíEn
Tr‚ÜíEn
2.6
4.4
4.5
3.4
3.1
4.1
4.5
2.9
2.9
3.5
3.7
3.7
Fr‚ÜíEn
0.54
De‚ÜíEn
0.23
Ro‚ÜíEn
0.79
Tr‚ÜíEn
0.95
Make
Three
Trials
0.54
0.23
0.79
0.95
Unchanged
0.76
0.45
0.91
0.98
0.31
0.12
0.58
0.87
Increased
Decreased
Train
&
Evaluate
Fr‚ÜíEn
0.76
De‚ÜíEn
0.23
Ro‚ÜíEn
0.79
Tr‚ÜíEn
0.87
Select
Language
-Specific
Optimal 
Actions
Three Multilingual 
Distillation Weight Vectors 
for Trial Training
Trial Results 
(Language-Specific Dev Loss)
R
Input Multilingual
Distillation Weight
Vector Œ±k‚àí1
i
Updated Multilingual 
Distillation  Weight
Vector Œ±k
i
1
2
3
Figure 3: Process of AUTO-PMD updating the distillation weights. At the k-th update, AUTO-PMD makes three
trials that perform three actions to all language pairs‚Äô weights and then train the current model. Finally, the
language-specific optimal actions are selected to update the previous weights. Note that the value of each weight
will change by different magnitudes when increased or decreased due to the non-linear nature of sigmoid function.
each translation direction ‚Ñì, Œ∏worse
‚Ñì
is also possible
to improve Œ∏better
‚Ñì
via knowledge distillation. This
hypothesis is motivated by the recently proposed
theoretical framework of Multi-View Data (Allen-
Zhu and Li, 2020; He and Ozay, 2021), which
theoretically reveals that each well-trained network
only captures a different subset of relevant features,
limiting their generalization. The mechanism of
knowledge distillation is to help one model to learn
the relevant features of another model.
The discovery motivates us to suspect that Œ∏worse
‚Ñì
can also improve Œ∏better
‚Ñì
using distillation, as
Œ∏worse
‚Ñì
may possess relevant features that Œ∏better
‚Ñì
lacks. Therefore, BI-PMD allows Œ∏worse
‚Ñì
to affect
Œ∏better
‚Ñì
in direction ‚Ñì. Our implementation is simple:
BI-PMD sets all distillation weights to a positive
value. Formally, given a hyper-parameter Œ±, the
distillation weight of Œ∏i in direction ‚Ñìis:
Œ±ir‚Ñìs ‚Äú Œ±,
(6)
meaning that each model affects the other equally.
3.3
AUTO-PMD
Desiderata.
Both UNI-PMD and BI-PMD de-
termine the distillation weights of all translation
directions based on a pre-defined hyper-parameter
Œ±, which dissatisfies the following three expected
properties of distillation weights: 1) Language-
Adaptability: the optimal distillation weights for
different language pairs vary. However, the cur-
rent strategies set a uniform weight for all language
pairs, resulting in sub-optimal performance; 2) Dy-
namics: existing research on mutual distillation
uses a fixed distillation weight throughout the train-
ing process, which fails to adapt to the evolving
models; 3) Generality: it is empirically discov-
ered that the optimal value of distillation weight
varies across different datasets, incurring the ex-
tra cost of the manual hyper-parameter search. To
satisfy these three properties, we propose Auto-
matic Pareto Mutual Distillation (AUTO-PMD) to
automatically decide the value of each direction‚Äôs
distillation weight according to training dynamics.
Approach.
AUTO-PMD updates multilingual dis-
tillation weight vector Œ±i every T steps. We denote
the values of Œ±i after the k-th update by Œ±k. Note
that the subscript i of Œ±i is omitted for clarity. The
update process is modeled as Markov Chain (Nor-
ris and Norris, 1998). All distillation weights are
initialized at the beginning of training as a small
value, i.e., Œ±0r‚Ñìs ‚Äú 0.1. Three actions on distilla-
tion weight are defined:
F ‚Äú tf√íp¬®q, f√ìp¬®q, f‚Äúp¬®qu,
(7)
which aim to increase, decrease and keep the value
of distillation weight unchanged. At the k-th up-
date, AUTO-PMD decides the values of Œ±k accord-
ing to the previous state Œ±k¬¥1. We exemplify the
process of each update step in Fig. 3 and precisely
describe it in Alg. 2. As illustrated in Fig. 3, the
update process is divided into three steps.
In the first step, given the previous distillation
weights Œ±k¬¥1, AUTO-PMD makes three trials, gen-
erating three multilingual distillation weight vec-
tors for the trial training of the next step. Each
vector is obtained by performing an action (e.g.,
increasing) on all values of Œ±k¬¥1. These three
vectors, corresponding to three colorful vectors in
Fig. 3, form a set which is referred to as search
space rOk. In fact, the trial training of next step
should be conducted over the entire search space
Ok, which is the Cartesian product of possible sub-
sequent states of each language-specific distillation

Algorithm 2: AUTO-PMD
Input
: Multilingual trial datasets tDtrial
‚Ñì
u|L|
‚Ñì‚Äú1,
validation datasets tDvalid
‚Ñì
u|L|
‚Ñì‚Äú1, the
training model Œ∏1and Œ∏2, search space rOk
1,
rOk
2, distillation weights Œ±k¬¥1
1
, Œ±k¬¥1
2
Output : Œ±k
1, Œ±k
2
Initialize: Initialize trial results R P R|L|ÀÜ| r
Ok
i | to a
zero matrix
1 for i √ê 1 to 2 do
2
for j √ê 1 to | rOk
i | do
3
Œ±1
i √ê rOk
i rjs
4
Copy model Œ∏1
i √ê Œ∏i
5
Train Œ∏1
i on Dtrial for one epoch using
teacher model Œ∏3¬¥i and Œ±1
i with Eq.4
6
for ‚Ñì√ê 1 to |L| do
7
Rr‚Ñìsrjs √ê LcepDvalid
‚Ñì
; Œ∏1
iq
8
end
9
end
10
for ‚Ñì√ê 1 to |L| do
11
ÀÜj √ê arg min
j
Rr‚Ñìsrjs
12
Œ±k
i r‚Ñìs √ê rOk
i rÀÜjsr‚Ñìs
13
end
14 end
weight Œ±k¬¥1r‚Ñìs:
Ok ‚Äú
ƒÖ
‚ÑìPL
tfpŒ±k¬¥1r‚Ñìsq | f P Fu.
(8)
However, this search space grows exponentially as
the number of languages increases, that is, |Ok| ‚Äú
|F||L|. To overcome the non-trivial cost, the sub-
space rOk is adopted. Furthermore, we prove that
based on the Distillation Weights Independence as-
sumption, the optimal solution searched in rOk is
equivalent to that of Ok. The mathematical descrip-
tion of this assumption and the proof are demon-
strated in ¬ßA.
Next, AUTO-PMD uses each distillation weight
vector in rOk to train the current model on trial
set Dtrial, which is constructed by sampling œÅ of
Dtrain, for one epoch. The three trained models are
evaluated on the validation set, and the language-
specific dev losses of these models form a matrix,
which is represented by trial results R P R| rOk|ÀÜ|L|.
The model training of this step incurs overhead,
which is proportional to the value of œÅ ÀÜ | rOk|. In
this work, we set œÅ ‚Äú 0.1. Thereby, the extra
overhead is 30% of the actual model training.
Finally, the language-specific optimal actions are
selected according to the trial results and then per-
formed on Œ±k¬¥1r‚Ñìs, obtaining the results of Œ±kr‚Ñìs.
We exemplify this step with Fig. 3. The red model,
trained using the increased version of Œ±k¬¥1 (the
vector in red), achieves the best performance of
Fr√ëEn. Thus, the Œ±kr‚Ñìs of Fr√ëEn is obtained by
increasing the Œ±k¬¥1r‚Ñìs of Fr√ëEn.
Implementation of Actions.
As aforementioned,
three actions for updating distillation weights are
defined (in Eq.7). The f‚Äúp¬®q is simple:
f‚ÄúpŒ±r‚Ñìsq ‚Äú Œ±r‚Ñìs.
(9)
For f√íp¬®q and f√ìp¬®q, it must ensure that the out-
put is always between r0, 1s. Therefore, the input
is first mapped into p¬¥8, `8q using the inverse
of sigmoid function and then increased/decreased
the value by ¬µ, named step size. Finally, the in-
creased/decreased value is mapped back into r0, 1s
using sigmoid function. Formally:
f√ípŒ±r‚Ñìsq ‚Äú œÉpœÉ¬¥1pŒ±r‚Ñìsq ` ¬µq
(10)
f√ìpŒ±r‚Ñìsq ‚Äú œÉpœÉ¬¥1pŒ±r‚Ñìsq ¬¥ ¬µq
(11)
where œÉp¬®q is sigmoid function. The step size ¬µ
is crucial for weights search. A smaller step size
could improve the precision of searched weights
while may delay convergence to the optimal weight.
Therefore, we design a step size scheduler, setting
a large step size in the early training stage and then
deducing the step size:
¬µ ‚Äú
c
Tmax ¬¥ t
Tmax
(12)
where Tmax is the max training steps.
4
Experiments
4.1
Settings
Datasets.
We conduct experiments on two
datasets: the WMT-6 dataset provided by Huang
et al. (2022) and the widely-used TED-8-Diverse
dataset constructed by Wang et al. (2020). The
WMT-6 dataset involves the language pairs of 3
LRLs (et, ro, tr) and 3 HRLs (fr, de, zh) to English.
This dataset has around 5M training sentences from
parallel corpora that WMT provides over multiple
years, and the corresponding validation and test
sets are used. The data statistics are detailed in
Appendix B. The TED-8-Diverse contains the lan-
guage pairs of 4 LRLs (bos, mar, hin, mkd) and 4
HRLs (ell, bul, fra, kor) to English. This dataset
comprises around 570K sentence pairs. The data
statistics and the interpretation of language codes
are demonstrated in Appendix B. Compared to
TED-8-Diverse, the size of WMT-6 dataset is more
considerable and distributed more unevenly.

Method
Sampling
WMT-6
TED-8-DIVERSE
Many-to-One
One-to-Many
Many-to-One
One-to-Many
Existing Balancing Training Strategies
TEMPERATURE SAMPLING
œÑ ‚Äú 1
20.57
18.92
29.00
22.75
TEMPERATURE SAMPLING
œÑ ƒÖ 1
19.93
18.63
28.35
22.23
MULTIDDS-S (Wang et al., 2020)Àö
dyn.
‚Äì
‚Äì
27.00
18.24
MULTIUAT (Wu et al., 2021)Àö
dyn.
‚Äì
‚Äì
27.83
19.76
CCL-M (Zhang et al., 2021)Àö
dyn.
‚Äì
‚Äì
28.34
19.53
œá-IBR (Zhou et al., 2021)Àö
dyn.
‚Äì
‚Äì
29.74
23.44
Existing Knowledge Distillation-based Strategies
MULTI-DISTILL (Tan et al., 2019)
œÑ ‚Äú 1
20.18
18.57
29.52
22.31
LSSD (Huang et al., 2022)
œÑ ‚Äú 1
21.17
19.76
30.77
23.55
Our Pareto Mutual Distillation
UNI-PMD
œÑ ‚Äú 1
20.76:
18.96
29.76:
22.92
œÑ ƒÖ 1
21.74:
19.76:
29.97:
22.91
BI-PMD
œÑ ‚Äú 1
21.61:
19.53:
30.31:
23.00:
œÑ ƒÖ 1
21.92:
20.09:
30.42:
22.77
AUTO-PMD
œÑ ‚Äú 1
21.89:
20.16:
31.05:
23.31:
œÑ ƒÖ 1
22.39:
20.48:
30.71:
23.28:
Table 1: BLEU scores on the WMT-6 and TED-8-Diverse dataset. Bold indicates the highest BLEU score in each
setting. ‚Äò*‚Äô means results taken from the original paper. ‚Äò:‚Äô indicates significantly better than temperature-based
sampling with t-test p ƒÉ 0.001. The temperature-based sampling is tried with œÑ ‚Äú t1, 5u on WMT-6 and œÑ ‚Äú t1, 3u
on TED-8-Diverse. For each of our approaches, the first row is the result of model-1, and the second row is the
result of model-2. ‚Äòdyn.‚Äô is the abbreviation for ‚Äúdynamic sampling.‚Äù
For each dataset, our approach is evaluated in
two multilingual translation scenarios: 1) MANY-
TO-ONE (M2O): translating multiple languages to
English in this work; 2) ONE-TO-MANY (O2M):
translating English to other languages.
Hyper-parameters.
Even though our proposed
training framework can be applied to any model
architecture, we verify its effectiveness on the
popular Transformer (Vaswani et al., 2017) im-
plemented in fairseq (Ott et al., 2019) with the
base version. We use the same model configura-
tion, hyper-parameters, and preprocess procedure
as those of Huang et al. (2022) for all baselines and
our method. The only difference is that the dropout
rate is modified into 0.2 on WMT-6, to accelerate
the convergence without performance loss. The
complete set of hyper-parameters is demonstrated
in Appendix C. The performance is evaluated with
the BLEU score (Papineni et al., 2002) using the
SacreBLEU toolkit (Post, 2018).
As illustrated in ¬ß3.1, our Pareto-MD trains two
models using different sampling distributions, P1
and P2, and we adopt temperature-based sampling
with different values of œÑ to produce these two dis-
tributions. We set œÑ ‚Äú 1 for P1 and œÑ ‚Äú 5 for
P2 on WMT-6. On TED-8-Diverse, we set œÑ ‚Äú 1
for model-1 and œÑ ‚Äú 3 for model-2 since an overly
large value leads to poor performance. For the UNI-
PMD and BI-PMD, we manually search the opti-
mal Œ± (in Eq.5 and Eq.6) among t0.2, 0.4, 0.6, 0.8u.
The update interval of distillation weights T is set
to the step number of one epoch.
Baselines.
We primarily compare our Pareto-MD
with: (1) Temperature-based Sampling: the method
most related to our work; 2) œá-IBR (Zhou et al.,
2021), the state-of-the-art (SOTA) dynamic sam-
pling method, which enables the balancing train-
ing based on distributionally robust optimization;
3) LSSD (Huang et al., 2022), another distillation-
based training strategy which achieves SOTA per-
formance on TED-8-Diverse and WMT-6 dataset
via alleviating the convergence inconsistency prob-
lem of MNMT using self-distillation. More details
of baselines are demonstrated in Appendix D.
4.2
Main Results
We summarize the main results in Table 1. As
we observed, our methods significantly outper-
form the temperature-based sampling under M2O
and O2M settings on both datasets. The model-2
trained with AUTO-PMD has improved by up to
+2.46 BLEU under the M2O setting of WMT-6.

HRLs
24.00
24.88
25.75
26.63
27.50
LRLs
10.00
11.25
12.50
13.75
15.00
Baseline
Uni-MOMD
Bi-MOMD
Auto-MOMD
HRLs
23.00
23.75
24.50
25.25
26.00
LRLs
14.50
15.88
17.25
18.63
20.00
Baseline
Uni-MOMD
Bi-MOMD
Auto-MOMD
(a) Many-to-One
(b) One-to-Many
Model-1
(ùúè = 1)
Model-2
(ùúè = 5)
Model-1
(ùúè = 1)
Model-2
(ùúè = 5)
Figure 4: Multilingual performance Pareto frontier on
the WMT-6 dataset. Gray dotted curves indicate the
Pareto frontier of baselines and the colorful ones mark
the frontier made by AUTO-PMD. This figure shows
that the Pareto frontier is pushed outwards significantly.
Furthermore, Pareto-MD achieves higher BLEU
scores than previous methods in most settings. At
best, AUTO-PMD outperforms the previous SOTA
(LSSD) by +1.22 BLEU scores under the M2O set-
ting of WMT-6. When comparing UNI-PMD and
BI-PMD, it is obvious that BI-PMD consistently
exceeds UNI-PMD, verifying the motivation that
the worse model is also possible to improve the bet-
ter model via knowledge distillation. AUTO-PMD
further surpasses BI-PMD by +0.3‚Äû0.5 BLEU.
This improvement proves that our automatic search
of distillation weights is indeed reliable. Moreover,
AUTO-PMD is more general than UNI-PMD and
BI-PMD since it eliminates the need to search for
the hyper-parameter Œ± manually3.
5
Analysis
5.1
Visualization of Pareto Frontier
In order to clearly assess the impact of our meth-
ods on HRLs and LRLs, we visualize the Pareto
frontier in Fig. 4. Three important observations
3The effect of Œ± is shown in Appendix F.
Method
Sampling
BLEU
Vanilla MD
œÑ ‚Äú 1
20.93
œÑ ‚Äú 1
20.97
Vanilla MD
œÑ ‚Äú 5
21.13
œÑ ‚Äú 5
21.29
BI-PMD
œÑ ‚Äú 1
21.61
œÑ ‚Äú 5
21.92
AUTO-PMD
œÑ ‚Äú 1
21.89
œÑ ‚Äú 5
22.39
Table 2: Comparison between our method with vanilla
mutual distillation (Vanilla MD) under the Many-to-One
setting of the WMT-6 dataset.
can be drawn: 1) overall, the model-1 has been
significantly shifted right, and the model-2 has
been shifted upwards, proving that Pareto-MD ef-
fectively alleviates the shortcomings of each model
as we expected; 2) both of model-1 and model-2
are shifted right beyond the original model-2, indi-
cating that the performance of LRLs is improved
beyond the original performance bound. The rea-
son may be that the transfer learning from HRLs to
LRLs is more effective when the model achieves
high performance on both HRLs and LRLs; 3) the
model-1 degenerates on the translation of HRLs in
the O2M setting. One potential cause is the repre-
sentation space of HRLs undergoing more intense
squeezing in the O2M than in the M2O when the
model learns well on LRLs.
5.2
Effect of Diverse Sampling Strategies
In the Pareto-MD training framework, two mod-
els corresponding to different Pareto optimal so-
lutions are trained collaboratively using distinct
training distributions. One natural question that
arises is, how would the performance be affected
if we trained two models with the same training
distribution? This approach, in fact, degenerates
into the vanilla mutual distillation method. There-
fore, we conduct a comparison experiment on the
WMT-6 dataset (M2O setting) shown in Table 2.
The results indicate that vanilla mutual distillation
underperforms our BI-PMD by about 0.6 BLEU,
which supports the effectiveness of using different
sampling distributions for our Pareto-MD. More-
over, we propose AUTO-PMD to improve vanilla
mutual distillation by +1.1 BLEU totally.
5.3
Evolution of Distillation Weights
To better understand the process of AUTO-PMD,
we visualize the automatically searched distillation
weights in Fig. 5. As it depicts, the distillation

(a) Fr‚ÜíEn
(b) Tr‚ÜíEn
Figure 5: Visualization of automatically search distil-
lation weights in the many-to-one setting of WMT-6
dataset. Due to the space limitation, we only show the
weights of one HRL (Fr√ëEn) and one LRL (Tr√ëEn)
weights constantly vary to adapt the dynamic mod-
els with a decreasing variance made by the decay of
search step size (Eq.12). Besides, it is discovered
that the low-resource Tr√ëEn translation favors a
higher value of distillation weight than the high-
resource Fr√ëEn translation. This phenomenon
makes sense since LRLs suffer from more serious
over-fitting (Huang et al., 2022), requiring stronger
distillation learning.
5.4
Effect of Step Size Scheduler ¬µ
The performance of different step size schedulers
is listed in Table 3. The simple scheduler-1 fixes
the step size to 1.0, performing relatively poorly.
The scheduler-2 decreases the step size from 1.0 to
0.2. The scheduler-4 decreases the step size from
1.0 to 0.0, achieving the best performance. The
scheduler-3 also decrease the step size from 1.0 to
0.0, while not performing searching of distillation
weights at the end of training. We finally adopt the
scheduler-4 in our AUTO-PMD.
6
Related Work
For a long time, data imbalance has been a prob-
lem hindering multilingual models from perform-
ing evenly across different languages. Existing
methods pursue balanced performance via design-
ing heuristics (Arivazhagan et al., 2019) or auto-
matic sampling strategies (Arivazhagan et al., 2019;
Wang et al., 2020; Zhou et al., 2021; Wu et al.,
#
Scheduler
BLEU
(œÑ ‚Äú 1 / œÑ ‚Äú 5)
1
¬µ ‚Äú 1
20.71 / 21.80
2
¬µ ‚Äú
b
Tmax¬¥0.8ÀÜt
Tmax
21.90 / 22.21
3
¬µ ‚Äú maxp
b
Tmax¬¥1.2ÀÜt
Tmax
, 0q
21.74 / 22.31
4
¬µ ‚Äú
b
Tmax¬¥t
Tmax
21.89 / 22.39
Table 3: Effect of step size scheduler ¬µ in the many-to-
one translation of WMT-6 dataset. We have tried for
four implementations of the step size scheduler.
2021; Zhang et al., 2021). For example, Wang et al.
(2020) design a Reinforce Learning based method
to automatically adjust the sampling probability
of each language pair towards an overall optimal
solution. Zhou et al. (2021) vary the distribution
via distributional robust optimization. However,
their improvement is limited since increasing the
training weights of some languages leads to rela-
tive decreases in the weights of other languages,
resulting in a trade-off on the Pareto frontier. Dif-
ferent from their methods, we overcome this issue
by training two models collaboratively.
Before our work, there were two approaches also
based on knowledge distillation in MNMT. Tan
et al. (2019) use pre-defined bilingual models to
teach the multilingual model via knowledge distilla-
tion. Huang et al. (2022) propose language-specific
self-distillation to remedy the convergence incon-
sistency problem in MNMT using self-distillation.
Our Pareto-MD is an extension of mutual distilla-
tion on the Pareto optimization problems.
7
Conclusion
In this work, we propose a training framework
Pareto-MD to reach a higher Pareto frontier for
MNMT. The core of Pareto-MD is the synergy be-
tween diverse Pareto optimal solutions via mutual
distillation. Besides, we design a novel strategy for
deducing distillation weights automatically, achiev-
ing better performance and getting rid of hyper-
parameter searching. Experimental results on the
WMT and TED datasets show the effectiveness
of our method. Even though we experiment with
training two models in this work, our method can
naturally apply to train more models. In the fu-

ture, we are interested in exploring how to apply
our Pareto-MD to the training of large language
models (Zhao et al., 2023).
Limitations
Our Pareto-MD doubles computational cost due
to training two models simultaneously, which can
be a limitation of our approach. However, Pareto-
MD obtains significant improvement that is hard to
achieve for previous methods of training individual
models, thus worthy. Besides, our approach would
not necessarily result in double training time be-
cause these two models can be trained in parallel
as implemented by Guo et al. (2020). Moreover,
Pareto-MD does not affect inference efficiency.
Acknowledgements
Xiaocheng Feng is the corresponding author of this
work. We thank the anonymous reviewers for their
insightful comments. This work was supported
by the National Key R&D Program of China via
grant 2020AAA0106502, National Natural Science
Foundation of China (NSFC) via grant 62276078,
the Key R&D Program of Heilongjiang via grant
2022ZX01A32 and the International Cooperation
Project of PCL, PCL2022D01.
References
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.
Massively multilingual neural machine translation.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 3874‚Äì3884,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Zeyuan Allen-Zhu and Yuanzhi Li. 2020.
Towards
understanding ensemble, knowledge distillation and
self-distillation in deep learning.
Naveen Arivazhagan, Ankur Bapna, Orhan Firat,
Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,
Mia Xu Chen, Yuan Cao, George F. Foster, Colin
Cherry, Wolfgang Macherey, Zhifeng Chen, and
Yonghui Wu. 2019. Massively multilingual neural
machine translation in the wild: Findings and chal-
lenges. CoRR, abs/1907.05019.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015.
Neural machine translation by jointly
learning to align and translate. In ICLR.
Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.
2020. A survey of multilingual neural machine trans-
lation. ACM Computing Surveys (CSUR), 53(5):1‚Äì
38.
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi
Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep
Baines, Onur Celebi, Guillaume Wenzek, Vishrav
Chaudhary, et al. 2021. Beyond english-centric mul-
tilingual machine translation. J. Mach. Learn. Res.,
22(107):1‚Äì48.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016.
Multi-way, multilingual neural machine translation
with a shared attention mechanism. In Proceedings
of the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 866‚Äì875, San
Diego, California. Association for Computational
Linguistics.
Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng
Yu, Ding Liang, Xiaolin Hu, and Ping Luo. 2020.
Online knowledge distillation via collaborative learn-
ing. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages
11020‚Äì11029.
Thanh-Le Ha, Jan Niehues, and Alex Waibel. 2016.
Toward multilingual neural machine translation with
universal encoder and decoder. In Proceedings of the
13th International Conference on Spoken Language
Translation, Seattle, Washington D.C. International
Workshop on Spoken Language Translation.
Bobby He and Mete Ozay. 2021. Feature kernel dis-
tillation. In International Conference on Learning
Representations.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network.
Yichong Huang, Xiaocheng Feng, Xinwei Geng, and
Bing Qin. 2022. Unifying the convergences in multi-
lingual neural machine translation. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing. Association for Com-
putational Linguistics.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vi√©gas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google‚Äôs
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339‚Äì351.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700‚Äì1709, Seattle,
Washington, USA. Association for Computational
Linguistics.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical

Methods in Natural Language Processing: System
Demonstrations, pages 66‚Äì71, Brussels, Belgium.
Association for Computational Linguistics.
NLLB Team, Marta R. Costa-juss√†, James Cross, Onur
√áelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzm√°n, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022.
No language left behind: Scaling human-
centered machine translation.
James R Norris and James Robert Norris. 1998. Markov
chains. 2. Cambridge university press.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics (Demonstrations),
pages 48‚Äì53, Minneapolis, Minnesota. Association
for Computational Linguistics.
Myle Ott, Sergey Edunov, David Grangier, and Michael
Auli. 2018. Scaling neural machine translation. In
Proceedings of the Third Conference on Machine
Translation: Research Papers, pages 1‚Äì9, Brussels,
Belgium. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311‚Äì318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186‚Äì
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Aditya Siddhant, Ankur Bapna, Orhan Firat, Yuan Cao,
Mia Xu Chen, Isaac Caswell, and Xavier Garcia.
2022. Towards the next 1000 languages in multilin-
gual machine translation: Exploring the synergy be-
tween supervised and self-supervised learning. arXiv
preprint arXiv:2201.03110.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. JMLR.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In NeurIPS.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision. In
CVPR.
Xu Tan, Yi Ren, Di He, Tao Qin, and Tie-Yan Liu.
2019. Multilingual neural machine translation with
knowledge distillation. In International Conference
on Learning Representations.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS.
Xinyi Wang, Yulia Tsvetkov, and Graham Neubig. 2020.
Balancing training for multilingual neural machine
translation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 8526‚Äì8537, Online. Association for Computa-
tional Linguistics.
Minghao Wu, Yitong Li, Meng Zhang, Liangyou
Li,
Gholamreza Haffari,
and Qun Liu. 2021.
Uncertainty-aware balancing for multilingual and
multi-domain neural machine translation training.
In Proceedings of the 2021 Conference on Empir-
ical Methods in Natural Language Processing, pages
7291‚Äì7305, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Jian Yang, Shuming Ma, Haoyang Huang, Dongdong
Zhang, Li Dong, Shaohan Huang, Alexandre Muzio,
Saksham Singhal, Hany Hassan, Xia Song, and Furu
Wei. 2021. Multilingual machine translation systems
from Microsoft for WMT21 shared task. In Proceed-
ings of the Sixth Conference on Machine Translation,
pages 446‚Äì455, Online. Association for Computa-
tional Linguistics.
Mingliang Zhang, Fandong Meng, Yunhai Tong, and
Jie Zhou. 2021. Competence-based curriculum learn-
ing for multilingual machine translation. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2021, pages 2481‚Äì2493, Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Ying Zhang, Tao Xiang, Timothy M Hospedales, and
Huchuan Lu. 2018. Deep mutual learning. In Pro-
ceedings of the IEEE conference on computer vision
and pattern recognition, pages 4320‚Äì4328.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,
Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,
Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A
survey of large language models.

Chunting Zhou,
Daniel Levy,
Xian Li,
Marjan
Ghazvininejad, and Graham Neubig. 2021. Distri-
butionally robust multilingual machine translation.
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages
5664‚Äì5674, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.

A
Equivalence Between Searching in Ok and rOk
As illustrated in ¬ß3.3, our strategy AUTO-PMD first searches the language-specific optimal multilingual
distillation weight vector ÀÜŒ±‚Ñìfor each translation direction ‚Ñìfrom a search space and then take the ÀÜŒ±‚Ñìr‚Ñìs
as the searching result of Œ±kr‚Ñìs. To search the optimal solution, the search space should be the entire
space Ok, which is formalized as:
Ok ‚Äú
ƒÖ
‚ÑìPL
tfpŒ±k¬¥1r‚Ñìsq | f P Fu,
However, the size of Ok grows exponentially as the number of languages increases. Therefore, we instead
search in rOk, a subset of Ok, which is formalized as:
rOk ‚Äú t t f√ípŒ±k¬¥1r‚Ñìsq u‚ÑìPL,
t f√ìpŒ±k¬¥1r‚Ñìsq u‚ÑìPL,
t f‚ÄúpŒ±k¬¥1r‚Ñìsq u‚ÑìPL u.
In this section, we initially give a formal definition of the searching process. Subsequently, the Distillation
Weights Independence (DWI) assumption is introduced. Ultimately, we prove the equivalence between
searching in Ok and rOk based on the DWI assumption.
Definition A.1 (Searching Process). Given the multilingual trial set Dtrial ‚Äú tDtrial
‚Ñì
u|L|
‚Ñì‚Äú1, validation set
Dvalid ‚Äú tDvalid
‚Ñì
u|L|
‚Ñì‚Äú1, student mode Œ∏S, teacher model Œ∏T , and the search space O , for each translation
direction ‚Ñì, the searching process of Œ±kr‚Ñìs is:
Œ±kr‚Ñìs ‚Äú ÀÜŒ±‚Ñìr‚Ñìs
ÀÜŒ±‚Ñì
‚Äú arg min
Œ±PO
LcepDvalid
‚Ñì
; ÀÜŒ∏pŒ±qq
ÀÜŒ∏pŒ±q ‚Äú arg min
Œ∏
LPMDpDtrial; Œ∏S, Œ∏T , Œ±q.
Hypothesis A.1 (Distillation Weights Independence). Given two multilingual distillation weight vectors
Œ±1 and Œ±2:
D‚ÑìP L, Œ±1r‚Ñìs ‚Äú Œ±2r‚Ñìs
√± LcepDvalid
‚Ñì
; ÀÜŒ∏pŒ±1qq ‚Äú LcepDvalid
‚Ñì
; ÀÜŒ∏pŒ±2qq
Theorem A.1. Let ÀÜŒ±‚Ñìr‚Ñìs denote the searching result in the search space Ok for direction ‚Ñì, rŒ±‚Ñìr‚Ñìs denotes
the searching result in the search space rOk for direction ‚Ñì, based on the Distillation Weights Independence
assumption, it is satisfied that:
ÀÜŒ±‚Ñìr‚Ñìs ‚Äú rŒ±‚Ñìr‚Ñìs.
Proof. Let ÀÜŒ±‚Ñìr‚Ñìs ‚Äú ÀÜflpŒ±k¬¥1r‚Ñìsq, where ÀÜfl P F is the language-specific action, the following equation
holds:
LcepDvalid
‚Ñì
; Œ∏pÀÜŒ±‚Ñìqq ‚Äú LcepDvalid
‚Ñì
; Œ∏pt ÀÜflpŒ±k¬¥1r‚Ñì1squ‚Ñì1PLqq,
based on hypothesis A.1. Because t ÀÜflpŒ±k¬¥1r‚Ñì1squ‚Ñì1PL P rOk, and rOk ƒé Ok, then we can infer that:
√±
LcepDvalid
‚Ñì
; t ÀÜflpŒ±k¬¥1r‚Ñì1squ‚Ñì1PLq ‚Äú min
Œ±P rOk LcepDvalid
‚Ñì
; ÀÜŒ∏pŒ±qq
√±
t ÀÜflpŒ±k¬¥1r‚Ñì1squ‚Ñì1PL ‚Äú arg min
Œ±P rOk
LcepDvalid
‚Ñì
; ÀÜŒ∏pŒ±qq
√±
ÀÜflpŒ±k¬¥1r‚Ñìsq ‚Äú rŒ±‚Ñìr‚Ñìs
√±
ÀÜŒ±‚Ñìr‚Ñìs ‚Äú rŒ±‚Ñìr‚Ñìs

B
Data Statistics
We list data statistic of TED-8-Diverse dataset in
Table 4. Data statistics of WMT-6 dataset is listed
in Table 5.
Language
Num
bos (Bosnian)
5,664
mar (Marathi)
9,840
hin (Hindi)
18,798
mkd (Macedonian)
25,335
ell (Greek)
134,327
bul (Bulgarian)
174,444
fra (French)
192,304
kor (Korean)
205,640
Table 4: Data statistics for the TED-8-Diverse dataset.
‚Äònum‚Äô refers to the number of sentence pairs in the train-
ing set.
Language
Data Source
Num
tr (Turkish)
WMT17
5,000
ro (Romanian)
WMT16
10,000
et (Estonian)
WMT18
80,000
zh (Chinese)
WMT17
400,000
de (German)
WMT14
1,500,000
fr (French)
WMT14
3,000,000
Table 5: Data statistics for the WMT dataset. ‚Äònum‚Äô
refers to the number of sentence pairs in the training set.
C
Hyper-parameters
In this section, we report the hyper-parameters used
in our experiments.
‚Ä¢ We adopt the base-version of Transformer ar-
chitecture with 6 layers encoders/decoders
and 8 attention heads.
‚Ä¢ The embedding dimension is 512 and the
Feed-Forward Network has a dimension of
2048.
‚Ä¢ We train models with learning rate Œ∑
‚Äú
0.0015 and use Adam optimizer (Kingma and
Ba, 2015) with Œ≤1 ‚Äú 0.9, Œ≤2 ‚Äú 0.98, and the
same learning rate schedule as Vaswani et al.
(2017).
‚Ä¢ Batch size is set to 64K and half-precision
training is adopted (Ott et al., 2018).
19.5
20.3
21.0
21.8
22.5
0
0.2
0.4
0.6
0.8
Baseline
Uni-MOMD
Bi-MOMD
Auto-MOMD
18.5
19.0
19.6
20.1
20.6
(b) One-to-Many
0
0.2
0.4
0.6
0.8
Baseline
Uni-MOMD
Bi-MOMD
Auto-MOMD
(a) Many-to-One
Figure 6: Effect of different values of Œ± on WMT-6
dataset. For clarity, we only depict the results of model-
2 trained with œÑ ‚Äú 5.
‚Ä¢ For regularization, we use the label smooth-
ing as 0.1 (Szegedy et al., 2016). We set the
dropout as 0.3 (Srivastava et al., 2014) on
the TED-8-Diverse dataset and as 0.2 on the
WMT-6 dataset.
‚Ä¢ Models are trained for 70 epochs on WMT-6
and 300 epochs on TED-8-Diverse according
to the convergence.
‚Ä¢ For TED-8-Diverse, we preprocess sentececes
using sentencepiece (Kudo and Richardson,
2018) with a vocabulary size of 8K for each
language. For WMT-6, the vocabulary size is
64K for all languages.
‚Ä¢ For inference, we use beam search with beam
size 5.
All models are trained on Tesla V100 GPUs.
D
Details about Baselines
For temperature-based sampling (Arivazhagan
et al., 2019), we adopt the official implementa-
tion in fairseq. LSSD is re-implemented success-
fully with the code released by Huang et al. (2022).

Setting
Method
Sampling
fr
de
zh
et
ro
tr
Avg.
M2O
Temperature Sampling
œÑ ‚Äú 1
34.40
28.70
13.27
16.41
22.65
7.99
20.57
œÑ ƒÖ 1
31.59
26.61
12.56
16.48
23.06
9.29
19.93
AUTO-PMD
œÑ ‚Äú 1
34.96
28.79
13.81
17.9
25.22
10.65
21.89
œÑ ƒÖ 1
34.09
28.77
14.05
19.22
26.62
11.60
22.39
O2M
Temperature Sampling
œÑ ‚Äú 1
36.16
23.89
21.49
11.53
14.85
5.58
18.92
œÑ ƒÖ 1
31.21
20.76
20.76
13.28
17.54
8.20
18.63
AUTO-PMD
œÑ ‚Äú 1
35.38
23.12
20.84
13.2
18.79
9.65
20.16
œÑ ƒÖ 1
34.47
23.00
21.51
14.15
19.54
10.23
20.48
Table 6: BLEU score per language pair on the WMT-6 dataset. ‚ÄòAvg.‚Äô is the abbreviation of ‚Äúaverage values‚Äù. Bold
indicates the best performance of each language pair. Languages are sorted in decreasing order from left to right
according to data size.
Setting
Method
Sampling
kor
fra
bul
ell
mkd
hin
mar
bos
Avg.
M2O
Temperature Sampling
œÑ ‚Äú 1
19.73
40.73
39.74
38.71
34.34
23.38
11.13
24.88
29.08
œÑ ƒÖ 1
18.79
40.1
39.00
38.11
32.89
22.55
10.36
24.98
28.35
AUTO-PMD
œÑ ‚Äú 1
21.14
42.41
41.52
40.67
36.49
25.9
12.32
27.94
31.05
œÑ ƒÖ 1
20.51
42.03
40.93
40.00
36.04
25.71
12.44
28.02
30.71
O2M
Temperature Sampling
œÑ ‚Äú 1
9.06
40.26
36.10
33.63
25.67
15.56
4.90
16.82
22.75
œÑ ƒÖ 1
8.87
39.96
35.91
33.31
24.35
14.81
4.75
15.87
22.23
AUTO-PMD
œÑ ‚Äú 1
9.13
40.94
36.56
34.03
27.15
15.89
5.13
17.64
23.31
œÑ ƒÖ 1
8.90
40.65
36.55
33.64
27.44
16.29
4.90
17.89
23.28
Table 7: BLEU score per language pair on the TED-8-DIVERSE dataset. ‚ÄòAvg.‚Äô is the abbreviation of ‚Äúaverage
values‚Äù. Bold indicates the best performance of each language pair. Languages are sorted in decreasing order from
left to right according to data size.
We have tried to set Dropout rate to t0.2, 0.3u for
LSSD, and report the best results in terms of BLEU
for fair comparison. The code of œá-IBR (Zhou
et al., 2021) is also released. However, the result of
œá-IBR evaluated in our experiments is lower than
the original paper. Therefore, we report the results
in the original paper.
E
BLEU scores on Individual Languages
In this section, we report the BLEU scores of indi-
vidual language pairs. For clarity, we only show the
results of the temperature-based sampling and our
AUTO-PMD. As illustrated in Table. 6 and Table. 7,
our method achieves consistent improvements in 3
out of 4 settings.
In the one-to-many setting of WMT-6 dataset,
the performance of HRLs (i.e., fr and de) drops
about 0.7 BLEU. This may be due to the parameter
interference from the significantly improved LRLs.
F
Effect of Œ± for UNI-PMD and BI-PMD
In this section, we show the experimental results of
UNI-PMD and BI-PMD with different values of Œ±
in Fig. 6. As demonstrated, the value of Œ± is cru-
cial for the performance. The optimal value of Œ±
varies across different settings. This conclusion is
consistent with former work related to knowledge
distillation (Huang et al., 2022), which highlights
the importance of deducing distillation weights au-
tomatically.
G
Other Variants of Mutual Distillation
In this work, we design another two mutual
distillation-based strategies beyond AUTO-PMD:
Dynamic Mutual Distillation (DYNAMIC-MD) and
Language-Specific Mutual Distillation (LSMD).
DYNAMIC-MD adopts the same update process
of distillation weights as AUTO-PMD. That is, DY-
NAMIC-MD also makes three trials and uses the

Method
Sampling
BLEU
M2O
O2M
AUTO-PMD
œÑ ‚Äú 1
21.89
20.16
œÑ ‚Äú 5
22.39
20.48
DYNAMIC-MD
œÑ ‚Äú 1
22.06
20.33
œÑ ‚Äú 5
22.11
20.24
LSMD
œÑ ‚Äú 1
21.47
18.94
œÑ ‚Äú 5
21.03
19.46
Table 8: Other variants of mutual distillation designed
by us. DYNAMIC-MD is the abbreviation of Dynamic
Mutual Distillation.
LSMD is the abbreviation of
Language-Specific Mutual Distillation.
optimal action to uptate the distillation weight. Dif-
ferently, DYNAMIC-MD selects a uniform optimal
action instead of language-specific optimal actions.
LSMD sets fixed and language-specific distillation
weights for each language pair. To obtain suitable
language-specific distillation weights, we use the
distillation weights searched by AUTO-PMD at the
last update. The results of these two strategies
are listed in Table 8. As the results show, AUTO-
PMD achieves higher performance upper-bound
than these two strategies.

