Joseph Muscat
Functional 
Analysis
An Introduction to Metric Spaces, 
Hilbert Spaces, and Banach Algebras

Functional Analysis

Joseph Muscat
Functional Analysis
An Introduction to Metric Spaces,
Hilbert Spaces, and Banach Algebras
123

Joseph Muscat
Department of Mathematics
University of Malta
Msida
Malta
ISBN 978-3-319-06727-8
ISBN 978-3-319-06728-5
(eBook)
DOI 10.1007/978-3-319-06728-5
Springer Cham Heidelberg New York Dordrecht London
Library of Congress Control Number: 2014939386
Mathematics Subject Classiﬁcation: 46-01, 47-01
 Springer International Publishing Switzerland 2014
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or
information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed. Exempted from this legal reservation are brief
excerpts in connection with reviews or scholarly analysis or material supplied speciﬁcally for the
purpose of being entered and executed on a computer system, for exclusive use by the purchaser of the
work. Duplication of this publication or parts thereof is permitted only under the provisions of
the Copyright Law of the Publisher’s location, in its current version, and permission for use must
always be obtained from Springer. Permissions for use may be obtained through RightsLink at the
Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of
publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for
any errors or omissions that may be made. The publisher makes no warranty, express or implied, with
respect to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)
Whilst we have made considerable efforts to contact all holders of copyright material
contained in this book we have failed to locate some of them. Should holders wish to contact
the Publisher, we will make every effort to come to some arrangement with them.

To Jennifer-Claire and Bernadette

Preface
Originally, functional analysis was the study of functions. It is now considered to
be a unifying subject that generalizes much of linear algebra and real/complex
analysis, with emphasis on inﬁnite dimensional spaces. This book introduces this
vast topic from these elementary preliminaries and develops both the abstract
theory and its applications in three parts: (I) Metric Spaces, (II) Banach and Hilbert
Spaces, and (III) Banach Algebras.
Especially with the digital revolution at the turn of the millennium, Hilbert
spaces and least squares approximation have become necessary and fundamental
topics for a mathematical education, not only just for mathematicians, but also for
engineers, physicists, and statisticians interested in signal processing, data analy-
sis, regression, quantum mechanics, etc. Banach spaces, in particular L1 and L?
methods, have gained popularity in applications and are complementing or even
supplanting the classical least squares approach to many optimization problems.
Aim of this Book
The main aim of this book is to provide the reader with an introductory textbook
that starts from elementary linear algebra and real analysis and develops the theory
sufﬁciently to understand how various applications, including least squares
approximation, etc., are all part of a single framework. A textbook must try to
achieve a balance between rigor and understanding: not being too elementary by
omitting ‘hard’ proofs, but neither too advanced by using too strict a language for
the average reader and treating theorems as mere stepping stones to yet other
theorems. Despite the multitude of books in this area, there is still a perceived gap
in learning difﬁculty between undergraduate and graduate textbooks. This book
aims to be in the middle: it covers much material and has many exercises of
varying difﬁculty, yet the emphasis is for the student to remember the theory
clearly using intuitive language. For example, real analysis is redeveloped from
the broader picture of metric spaces (including a construction of the real number
space), rather than through the even more abstract topological spaces.
vii

Audience
This book is meant for the undergraduate who is interested in mathematical
analysis and its applications, or the research engineer/statistician who would like a
more rigorous approach to fundamental mathematical concepts and techniques. It
can also serve as a reference or for self-study of a subject that occupies a central
place in modern mathematics, opening up many avenues for further study.
The basic requirements are mainly the introductory topics of mathematics: Set
and Logic notation, Vector Spaces, and Real Analysis (calculus). Apart from these,
it would be helpful, but not necessary, to have taken elementary courses in Fourier
Series, Lebesgue Integration, and Complex Analysis. Reviews of Vector Spaces
and Measurable sets are included in this book, while the other two mentioned
subjects are developed only to the extent needed.
Examples are included from many areas of mathematics that touch upon
functional analysis. It would be helpful at the appropriate places, for the reader to
have encountered these other subjects, but this is not essential. The aim is to make
connections and describe them from the viewpoint of functional analysis. With the
modern facilities of searching over the Internet, anyone interested in following up
a speciﬁc topic can easily do so.
The sections follow each other in a linear fashion, with the three parts ﬁtting
into three one-semester courses, although Part II is twice as long as the others. The
following sections may be omitted without much effect on subsequent topics:
Section 6.4 C(X, Y)
Section 9.2 Function Spaces
Sections 11.5 Pointwise and Weak Convergence
Sections 12.1 and 12.2 Differentiation and Integration
Sections 14.4 and 14.5 Functional Calculus and the Gelfand Transform
Section 15.4 Representation Theorems.
Acknowledgments
This book grew out of lecture notes that were used in a course on functional
analysis for the last 20 years. I wish to thank my students over the years for their
feedback, but mostly my colleagues, Emanuel Chetcuti and Josef Lauri for their
encouragement and very helpful suggestions when reading the ﬁrst draft, as well as
the anonymous reviewers who, with their comments, helped shape the book. I wish
to thank also the Springer editors, Joerg Sixt and Catherine Waite, for their care in
preparing this manuscript.
Malta
Joseph Muscat
viii
Preface

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
Part I
Metric Spaces
2
Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.1
Balls and Open Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.2
Closed Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
3
Convergence and Continuity. . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.1
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.2
Continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4
Completeness and Separability . . . . . . . . . . . . . . . . . . . . . . . . . .
37
4.1
Completeness. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
4.2
Uniformly Continuous Maps . . . . . . . . . . . . . . . . . . . . . . . .
48
4.3
Separable Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
5
Connectedness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
5.1
Connected Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
5.2
Components. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
6
Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
6.1
Bounded Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
6.2
Totally Bounded Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
6.3
Compact Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
6.4
The Space C(X,Y). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
ix

Part II
Banach and Hilbert Spaces
7
Normed Spaces. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
7.1
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
7.2
Norms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
7.3
Metric and Vector Properties . . . . . . . . . . . . . . . . . . . . . . . .
101
7.4
Complete and Separable Normed Vector Spaces . . . . . . . . . .
105
7.5
Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
8
Continuous Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
8.1
Operators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
8.2
Quotient Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
8.3
RN and Totally Bounded Sets . . . . . . . . . . . . . . . . . . . . . . .
135
9
Main Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
9.1
Sequence Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
9.2
Function Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
10
Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
10.1
Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
10.2
Least Squares Approximation. . . . . . . . . . . . . . . . . . . . . . . .
179
10.3
Duality H  H . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
10.4
The Adjoint Map T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
10.5
Inverse Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
10.6
Orthonormal Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
11
Banach Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
11.1
The Open Mapping Theorem . . . . . . . . . . . . . . . . . . . . . . . .
221
11.2
Compact Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
11.3
The Dual Space X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
11.4
The Adjoint T > . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
11.5
Pointwise and Weak Convergence . . . . . . . . . . . . . . . . . . . .
246
12
Differentiation and Integration . . . . . . . . . . . . . . . . . . . . . . . . . .
257
12.1
Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
257
12.2
Integration for Vector-Valued Functions . . . . . . . . . . . . . . . .
261
12.3
Complex Differentiation and Integration . . . . . . . . . . . . . . . .
267
Part III
Banach Algebras
13
Banach Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
13.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
13.2
Power Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
288
x
Contents

13.3
The Group of Invertible Elements . . . . . . . . . . . . . . . . . . . .
295
13.4
Analytic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
299
14
Spectral Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
14.1
The Spectral Radius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
14.2
The Spectrum of an Operator. . . . . . . . . . . . . . . . . . . . . . . .
312
14.3
Spectra of Compact Operators . . . . . . . . . . . . . . . . . . . . . . .
318
14.4
The Functional Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
14.5
The Gelfand Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
15
C*-Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
15.1
Normal Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
347
15.2
Normal Operators in BðHÞ. . . . . . . . . . . . . . . . . . . . . . . . . .
353
15.3
The Spectral Theorem for Compact Normal Operators . . . . . .
360
15.4
Representation Theorems. . . . . . . . . . . . . . . . . . . . . . . . . . .
374
Hints to Selected Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393
Glossary of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
413
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
415
Contents
xi

Chapter 1
Introduction
Much of modern mathematics depends upon extending the ﬁnite to the inﬁnite.
In this regard, imagine extending the geometric vectors that we are familiar with to
an inﬁnite number of components. That is, consider
v = a1e1 + a2e2 + · · · = (a1, a2, a3, . . .)
where ei are unit independent vectors just like i, j and k in Cartesian geometry. It is
not at all clear that we can do so—for starters, what do those three dots “· · · ” on
the right-hand side mean? Surely they signify that as more terms are taken one gets
better approximations of v. This immediately suggests that not every such “inﬁnite”
vector is allowed; for example, it might be objected that the vector
v = e1 + e2 + e3 + · · ·
cannot be approximated by a ﬁnite number of these unit vectors, as the remainder
eN + · · · looks as large as v. Instead we might allow the inﬁnite vector
v = e1 + 1
2 e2 + 1
3e3 + · · ·
although even here, it is unclear whether this may also grow large, just as
1 + 1
2 + 1
3 + · · · = ∞.
To continue with our experiment, let us just say that the coefﬁcients become zero
rapidly enough.
There are all sorts of things we can attempt to do with these “inﬁnite” vectors, by
analogy with the usual vectors: addition of vectors and multiplication by a number
are easily accomplished,
(1, 1
2, 1
3, . . .) + (1, 1
2, 1
4, . . .) = (2, 1, 7
12, . . .),
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_1,
1
© Springer International Publishing Switzerland 2014

2
1
Introduction
2 × (0, 1, −1
2, . . .) = (0, 2, −1, . . .).
One can even generalize the “dot product”
(a1, a2, . . .) · (b1, b2, . . .) = a1b1 + a2b2 + · · · ,
assuming the series converges—and we have no guarantee that it always does.
For example, if x is equal to (1, 1/
√
2, 1/
√
3, . . .), then x · x = ∞
n=1 1/n is
inﬁnite. Again let us remedy this situation by insisting that vectors have coefﬁcients
that decrease to 0 fast enough.
Having done this, we may go on to see what inﬁnite matrices would look like.
They would take an inﬁnite vector and return another inﬁnite one, as follows,
⎛
⎜⎝
a11 a12 . . .
a21 a22
...
...
⎞
⎟⎠
⎛
⎜⎝
x1
x2
...
⎞
⎟⎠=
⎛
⎜⎝
y1
y2
...
⎞
⎟⎠,
where y1 = a11x1 + a12x2 + · · · = 
n a1nxn, etc. Perhaps we may need to have the
rows of the matrix vanish sufﬁciently rapidly as we go down and to the right of the
matrix.
Once again, many familiar ideas from ﬁnite matrices seem to generalize to this
inﬁnite setting. Not only is it possible to add and multiply these matrices with-
out any inherent difﬁculty, but methods such as Gaussian elimination can also be
applied in principle. There seems to be no intrinsic problem to working with inﬁnite-
dimensional linear algebra.
It may come as a slight surprise to the reader that in fact he/she has already met
these inﬁnite vectors before! When a function is expanded as a MacLaurin series
f (x) = f (0) + f ∗(0)x + 1
2 f ∗∗(0)x2 + · · · ,
it is in effect written as an inﬁnite sum of the basis vectors (or functions) 1, x, x2, . . .,
each with the numerical coefﬁcients
f (0), f ∗(0), 1
2 f ∗∗(0), . . ., respectively.
Adding two functions is the same as adding the two inﬁnite vectors (or series);
and multiplying by a number is equivalent to multiplying each coefﬁcient by the
same number. What about inﬁnite matrices? Take a look at the following form of
differentiation, here written in matrix form,
f ∗(x) =
⎛
⎜⎜⎜⎝
0 1 0
. . .
0 2 0
0 3
...
...
⎞
⎟⎟⎟⎠
⎛
⎜⎜⎜⎝
f (0)
f ∗(0)
1
2 f ∗∗(0)
...
⎞
⎟⎟⎟⎠.

1
Introduction
3
And just as there are various bases that can be used in geometry, so there are
different ways to expand functions, the most celebrated being the Fourier series
f (x) = a0 + a1 cos x + b1 sin x + a2 cos 2x + b2 sin 2x + · · · .
The basis vectors are now 1, cos x, sin x, cos 2x, etc. What matrix does differentia-
tion take with respect to this basis?
If we accept that all this is possible and makes sense, we are suddenly made
aware of a new uniﬁcation of mathematics: certain differential equations are matrix
equations, the Fourier and Laplace transforms can be thought of as generalized
“matrices” mapping a function (vector) to another function, etc. Solving a linear
differential equation, and ﬁnding the inverse Fourier transform, are equivalent to
ﬁnding the inverse of their “matrices”.
Do we gain anything by converting to a matrix picture? Apart from the practical
matter that there are many known algorithms that deal with matrices, a deeper reason
is that linear algebra and geometry give insights to the subject of functions that we
may not have had before. Euclid’s theorems may possibly still be valid for functions
if we think of them as ‘points’ in an inﬁnite-dimensional vector space. We wake up
to the possibility of a function being perpendicular to another, for example, and that
a function may have a closest function in a “plane” of functions.
Conversely, ideas from classical analysis may be transferred to linear algebra.
Since square matrices can be multiplied with themselves, can the geometric series
1 + A + A2 + · · · make sense for matrices? Perhaps one can take the exponential
of a matrix eA: = 1 + A + A2/2! + A3/3! + · · · . There’s no better way than to take
the plunge and try it out, say on the differentiation ‘matrix’ D,
eD f (x) = (1 + D + D2/2 + · · · ) f (x) = f (x) + f ∗(x) + f ∗∗(x)/2 + · · · = f (x + 1)
(by a Taylor expansion around x). The “matrix” eD certainly has meaning: it per-
forms an unexpected, if mundane, operation, it shifts the function f one step to the
left! Again, suppose we have the equation y∗−2y = ex; manipulating the deriv-
ative blindly as if it were a number gives a correct solution (but not the general
solution)
y = (D −2)−1ex = −1
2(1 + D/2 + D2/4 + · · · )ex = −ex.
Yet repeating for the equation y∗−2y = e2x fails to give a meaningful solu-
tion.
In fact, historically, the subject of functional analysis as we know it started in
the 19th century when mathematicians started to notice the connections between
differential equations and matrices. For example, the equation
y∗(x) = a(x)y(x) + g(x)

4
1
Introduction
can be written in equivalent form as
y(x) =
⎫x
x0
a(s)y(s) ds + ˜g(x).
(1.1)
The integral
⎪x
x0 a(s)y(s) ds is an inﬁnitesimal version of N
n=1 anyn and can be
thought of as a transformation of y(x). Equation (1.1) is akin to a matrix equation
y = A y + b, and we are tempted to try out the solution y = (1 −A)−1b =
(1 + A + A2 + · · · )b.
Nonetheless, technical problems in carrying out this generalization arise immedi-
ately: are the components of an inﬁnite vector unique? They would be if the vectors en
are in some sense ‘perpendicular’ to each other. But what is this supposed to mean,
say for the MacLaurin series? After all, there do exist non-zero functions whose
MacLaurin coefﬁcients are all zero. The question of whether the Fourier coefﬁcients
are unique took almost a century to answer! And extra care must be taken to handle
inﬁnite vectors. For example, let
v1 := (
1,
0,
0, 0, . . .)
v2 := ( −1,
1,
0, 0, . . .)
v3 := (
0, −1,
1, 0, . . .)
v4 := (
0,
0, −1, 1, . . .)
. . .
It seems clear that
v1 + v2 + v3 + · · · = 0,
yet the size of the sum of the ﬁrst n vectors never diminishes:
v := v1 + · · · + vn = (0, . . . , 0, 1, 0, . . .) ⇒v · v = 1.
Because of these trapfalls, we need to proceed with extra caution. It turns out that
many of the equations written above are capable of different interpretations and so
cannot be taken to be literally true.
These considerations force us to consider the meaning of convergence. The reader
may already be familiar with the real line R, in which one can speak about conver-
gence of sequences of numbers, and continuity of functions. Some of the main results
in real analysis are
(i) Cauchy sequences converge,
(ii) for continuous functions, if xn →x then f (xn) →f (x),
(iii) continuous real functions are bounded on intervals of type [a, b] and have the
intermediate value property.

1 Introduction
5
WeseekgeneralizationsofthesetoRN andpossiblytoinﬁnitedimensionalspaces.
We do seem to have an intuitive sense of what it means for vectors to converge
xn →x, but can it be made rigorous? Is it true that if xn →x and yn →y
then f (xn, yn) →f (x, y) when f is a continuous function? Are continuous real
functions bounded on “rectangles” [a, b]×[c, d], and is the latter the correct analogue
of an “interval”? Since vector functions are common in applications, it is important
to show how these theorems apply in a much more general setting than R, and this
can be achieved by stripping off any inessential structure, such as its order (⩽). As
we proceed to answer these questions, we will see that the real line is very special
indeed. Intervals play several roles in real analysis, roles that are distinguished apart
in RN, where we speak instead of connected sets, balls, etc.
The book is divided into three parts: the ﬁrst considers convergence, continuity,
and related concepts, the second part treats inﬁnite vectors and their matrices, and
the third part tackles inﬁnite series of matrices and more.
Functional analysis is a rich subject because it combines two large branches of
mathematics: the topological branch concerns itself with convergence, continuity,
connectivity, boundedness, etc.; the algebraic branch concerns itself with operations,
groups, rings, vectors, etc. Problems from such different ﬁelds as matrix algebras,
differential equations and approximation theory, can be uniﬁed in one framework. As
in most of mathematics, there are two streams of study: the abstract theory deduces
the general results, starting from axioms, while the concrete examples are shown
to be part of this theory. Inevitably, the former appears elegant and powerful, and
the latter full of detail and perhaps daunting. Nonetheless, both pedagogically and
historically, it is often by examples that one understands the abstract, and by the
theory that one makes headway with concrete problems.
Most sections contain a number of worked out examples, notes, and exercises: it
is suggested that a section is ﬁrst read in full, including its propositions and exercises.
These exercises are an essential part of the book; they should be worked out before
moving to the next section (some hints and answers are provided in the appendix,
and many worked solutions can be found in the book’s website) http://www.springer.
com/mathematics/analysis/book/978-3-319-06727-8. To prevent the exercises from
becoming a litany of “Show …” and “Prove …”, these terms have frequently been
omitted, partly to instil an attitude of critical reading. As a guide, the notes and
exercises have been marked as follows:
▶refers to important notes and results;
* more advanced or difﬁcult exercises that can be skipped on a ﬁrst reading;
♦side remarks that can be skipped without losing any essential ideas.

6
1
Introduction
1.1 Preliminaries
Familiarity with the following mathematical notions and notation is assumed:
Logic and Sets
The basic logical symbols are ⇒(implies), not, and, or, as well as the quantiﬁers
∃(there exists) and ∀(for all). The reader should be familiar with the basic proof
strategies, such as proving φ ⇒ψ by its contrapositive (not ψ) ⇒(not φ), and
proofs by contradiction. The negation of ∀x φx is ∃x (not φx); and not (∃x φx)
is the same as ∀x (not φx). The symbol := is used to deﬁne the left-hand symbol
as the right-hand expression, e.g. e := ∞
n=0
1
n!.
A set consists of elements, and x ∈A denotes that x is an element of the set A.
The empty set ∅contains no elements, so x ∈∅is a contradiction.
The following sets of numbers are the foundational cornerstones of mathematics:
the natural numbers N = { 0, 1, . . . }, the integers Z, the rational numbers Q, the real
numbers R, and the complex numbers C. The induction principle applies for N,
If A ⊆N and 0 ∈A and ∀n, (n ∈A ⇒n + 1 ∈A) then A = N.
Although variables should be quantiﬁed to make sense of statements, as in
∀a ∈Q, a2 ̸= 2, in practice one often takes shortcuts to avoid repeating the obvious.
This book uses the convention that if a statement mentions variables without accom-
panying quantiﬁers, say, ⇐x + y⇐⩽⇐x⇐+⇐y⇐, these are assumed to be ∀x, ∀y, etc.,
in the space under consideration. Natural numbers are usually, but not exclusively,
denoted by the variables m, n, N, . . ., real numbers by a, b, . . ., and complex num-
bers by z, w, . . .. An unspeciﬁed X (or Y) refers to a metric space, a normed space,
or a Banach algebra, depending on the chapter.
Sets are often deﬁned in terms of a property, A := { x ∈X : φx }, where X is
a given ‘universal set’ and φx a statement about x. For example, R+ := { x ∈R :
x ⩾0 }.
A ⊆B denotes that A is a subset of B, i.e., x ∈A ⇒x ∈B; A ⊂B means
A ⊆B but A ̸= B. A “non-trivial” or “proper” subset of X is one which is not
∅or X. “Nested sets” are contained in each other as in A1 ⊆A2 ⊆A3 ⊆. . . or
. . . ⊆A2 ⊆A1.
The complement of a set A is denoted by X∖A, or by Ac for short; Acc = A, and
A ⊆B ⇔Bc ⊆Ac. A ∩B and A ∪B are the intersection and union of two sets,
respectively. Two sets are “disjoint” when A ∩B = ∅. De Morgan’s laws state that
(A∪B)c = Ac ∩Bc and (A∩B)c = Ac ∪Bc. In general, the union and intersection
of a number of sets are denoted by ⎬
i Ai and ⎭
i Ai (where the range of the index
i is understood by the context). A “cover” of A is a collection of sets { Bi : i ∈I }
whose union includes A, i.e., A ⊆⎬
i Bi; a “partition” of X is a cover by disjoint
subsets of X.

1.1 Preliminaries
7
Pairs of elements are denoted by (x, y), or as
⎧x
y
⎨
, generalized to ﬁnite ordered
lists (x1, . . . , xN). The product of two sets is the set of pairs
X × Y := { (x, y) : x ∈X, y ∈Y }
in particular X2 := X × X = { (x, y) : x, y ∈X }, and by analogy
X N := { (x1, . . . , xN) : xi ∈X, i = 1, . . . , N }.
An important example is the plane R2, whose points are pairs of real numbers (called
“coordinates”). The unit disk is { (x, y) ∈R2 : x2 + y2 ⩽1 }; its perimeter is the
unit circle S1 := { (x, y) ∈R2 : x2 + y2 = 1 }.
Functions
A function f : X →Y, x →f (x), assigns, for every input x ∈X, a unique
output element f (x) ∈Y. (It need not be an explicit procedure.) X is called the
“domain” of f and Y its “codomain”. Functions are also referred to as “maps” or
“transformations”. To avoid being too pedantic, we sometimes say, for example, “the
function x →ex” without reference to the domain and codomain, when these are
obvious from the context. The “image” of a subset A ⊆X, and the “pre-image” of
a subset B ⊆Y are
f A := { f (a) ∈Y : a ∈A },
f −1B := { a ∈X : f (a) ∈B }.
The image of f is im f := f X. It is easy to show that for any number of sets Ai,
f
⎩
i
Ai =
⎩
i
f Ai,
f

i
Ai ⊆

i
f Ai,
f −1 ⎩
i
Ai =
⎩
i
f −1 Ai,
f −1 
i
Ai =

i
f −1 Ai.
The set of functions f : X →Y is denoted by Y X.
Some functions can be composed together f ◦g(x) := f (g(x)) whenever the
image of g lies in the domain of f . Composing with the trivial identity function
I : X →X, x →x (one for each set X), has no effect, f ◦I = f .
The restriction of a function f : X →Y to a subset M ⊆X is the function
f |M : M →Y which agrees with f on M, i.e., f |M(x) = f (x) whenever x ∈M.
Conversely, an extension of a function is another function ˜f : A →Y where X ⊆A,
such that ˜f (x) = f (x) whenever x ∈X.
The reader should be familiar with the functions x →−x, xn, |x|, for x ∈R
or C; (x, y) →x + y, xy, with domain R2 or C2; (x, y) →x/y for y ̸= 0; and
(x1, . . . , xN) →max(x1, . . . , xN) for real numbers xi. In particular, the absolute

8
1
Introduction
value function satisﬁes
|a + b| ⩽|a| + |b|, |a| ⩾0, |a| = 0 ⇔a = 0, |ab| = |a||b|.
Conjugation is the function¯: C →C, a + ib →a −ib; its properties are
z + w = ¯z + ¯w, zw = ¯z ¯w, ¯¯z = z, ¯zz = |z|2.
The Kronecker delta function is δi j :=
 1
i = j
0
i ̸= j . The exponential function
x →
ex, R →R, may be deﬁned by ex := ∞
n=0
xn
n! ; it satisﬁes e0 = 1 and
ex > 0.
Sequences are functions x : N →X, but x(n) is usually written as xn, and the
whole sequence x is referred to by (xn)n∈N or (x0, x1, . . .) or even just (xn); real or
complex-valued sequences are denoted by bold symbols, x. For example (1/2n) is the
sequence (1, 1/2, 1/4, . . .), which is shorthand for 0 →1, 1 →1/2, etc. It is impor-
tant to realize that (xn) is a function and not a set of values, e.g. (1, −1, 1, −1, . . .)
is quite different from (−1, 1, 1, 1, . . .) and (−1, 1, −1, 1, . . .), even if they have
the same set of values. The set of real-valued sequences is denoted by RN := { x :
N →R }, and of the complex-valued sequences by CN. Functions x : Z →X are
also sometimes called sequences and are denoted by (xn)n∈Z.
Polynomials (of one variable) are functions p : C →C that are a ﬁnite number of
compositions of additions and multiplications only; every polynomial can be written
in the standard form p(z) = anzn + · · · + a1z + a0 (ai ∈C, an ̸= 0 unless p = 0);
n is called the degree of p.
A function f : X →Y is 1-1 (“one-to-one”) or injective when f (x) = f (y) ⇒
x = y; it is onto or surjective when f X = Y. A bijection is a function which is both
1-1 and onto; every bijection has an inverse function f −1, whereby f −1 ◦f (x) = x,
f ◦f −1(y) = y.
Sets may be ﬁnite, countably inﬁnite, or uncountable, depending on whether there
exists a bijection from the set to, respectively, (i) a set { 1, . . . , n } for some natural
number n, or (ii) N, or (iii) otherwise. In simple terms, a set is countable when its
elements can be listed, and ﬁnite when the list terminates. If A, B are countable sets
then so is A× B; more generally, the union of the countable sets An, n = 0, 1, 2, . . .,
is again countable:
A0 = {
a00,
a01,
a02,
. . .
}
A1 = {
a10,
a11,
a12,
. . .
}
A2 = {
a20,
a21,
a22,
. . .
}
· · ·

1.1 Preliminaries
9
∞
⎩
n=0
An = { a00, a01, a10, a02, . . . }
A relation is a statement about pairs of elements taken from X ×Y, e.g. x = y2+1
for (x, y) ∈R2. An equivalence relation ≈on a set X is one which is
reﬂexive
x ≈x,
symmetric
x ≈y ⇔y ≈x,
transitive x ≈y ≈z ⇒x ≈z.
An equivalence relation induces a partition of the set X into equivalence classes
[a] := { x ∈X : x ≈a }.
An order ⩽is a relation which is reﬂexive, transitive and anti-symmetric x ⩽
y ⩽x ⇒x = y. One writes x < y when x ⩽y but x ̸= y. A linear order is
one which also satisﬁes x ⩽y or y ⩽x. A number x is “positive” when x ⩾0,
whereas “strictly positive” means x > 0. An “upper bound” of a set A is a number b
which is larger than any a ∈A. A “least upper bound”, denoted sup A, is the smallest
such upper bound (if it exists), i.e., every upper bound of A is greater than or equal to
sup A. There are analogous deﬁnitions of lower bounds and greatest lower bounds,
denoted inf A.
A group is a set G with an associative operation and an identity element 1, such
that each element x ∈G has an inverse element x−1,
x(yz) = (xy)z,
1x = x = x1,
xx−1 = 1 = x−1x.
A subgroup is a subset of G which is itself a group with the same operation and
identity. A normal subgroup is a subgroup H such that x−1Hx ⊆H for all x ∈G.
An example of a group is the set C∖{ 0 } with the operation of multiplication; the set
S := { eiθ : θ ∈R } is a subgroup since eiθeiφ = ei(θ+φ), 1 = ei0, (eiθ)−1 = e−iθ
are all in S.
A ﬁeld F is a set of numbers, such as Q, R, or C, whose elements can be added
and multiplied together associatively, commutatively, and distributively,
∀a, b, c ∈F,
(a + b) + c = a + (b + c),
(ab)c = a(bc),
a + b = b + a,
ab = ba,
(a + b)c = ac + bc,
there is a zero 0 and an identity 1, every element a has an additive inverse, or negative,
−a, and every a ̸= 0 has a multiplicative inverse, or reciprocal, 1/a.
0 + a = a,
1a = a
a + (−a) = 0, a 1
a = 1 (a ̸= 0).
The real number space R is that unique ﬁeld which has a linear order ⩽such that
(a) a ⩾b ⇒a + c ⩾b + c, and a, b ⩾0 ⇒ab ⩾0,

10
1
Introduction
(b) Every non-empty subset with an upper bound has a least upper bound.
The intervals are the subsets
[a, b] := { x ∈R : a ⩽x ⩽b }, ]a, b] := { x ∈R : a < x ⩽b },
[a, b[ := { x ∈R : a ⩽x < b }, ]a, b[ := { x ∈R : a < x < b },
[a, ∞[ := { x ∈R : a ⩽x },
]a, ∞[ := { x ∈R : a < x }
]−∞, a] := { x ∈R : x ⩽a },
]−∞, a[ := { x ∈R : x < a },
where a < b are ﬁxed real numbers. The real numbers satisfy the Archimedean
property
∀x > 0, ∃n ∈N,
n > x.
The proof is simple: If the set N had an upper bound in R then it would have a least
upper bound α; by deﬁnition, this implies that α −1 is not an upper bound, meaning
there is a number n ∈N such that n > α −1; yet n + 1 ⩽α. This contradiction
shows that no x ∈R is an upper bound of N: there is an n ∈N such that n > x.
There is an important set principle that is not usually covered in elementary
mathematics textbooks:
The Axiom of Choice: If A = { Aα : α ∈I } is a collection of non-empty subsets
of a set X (the index α ranges over some set I), then there is a function f : I →X
such that f (α) ∈Aα.
That is, this ‘choice’ function selects an element from each of the sets Aα. The
Axiom of Choice is often used to create a sequence (xn) from a given list of non-
empty sets An, with xn ∈An. It seems obvious that if a set is non-empty then an
element of it can be selected, but the existence of such a procedure cannot be proved
from the other standard set axioms.

Part I
Metric Spaces

Chapter 2
Distance
Metric spaces can be thought of as very basic spaces, with only a few axioms, where
the ideas of convergence and continuity exist. The fundamental ingredient that is
needed to make these concepts rigorous is that of a distance, also called a metric,
which is a measure of how close elements are to each other.
Deﬁnition 2.1
A distance (or metric) on a metric space X is a function
d :
X2 ∞R+
(x, y) √∞d(x, y)
such that the following properties (called axioms) hold for all x, y, z ∗X,
(i) d(x, y) ⩽d(x, z) + d(z, y) (Triangle Inequality),
(ii) d(y, x) = d(x, y), (Symmetry)
(iii) d(x, y) = 0 ⇒x = y.
x
z
y
A metric space is not just a set, in which the elements have no relation to each
other, but a set X equipped with a particular structure, its distance function d. One
can emphasize this by denoting the metric space by the pair (X, d), although it is
more convenient to denote different metric spaces by different symbols such as X, Y,
etc.
In what follows, X will denote an abstract set with a distance, not necessarily R
or RN, although these are of the most immediate interest. We still call its elements
“points”, whether they are in reality geometric points, sequences, or functions. What
matters, as far as metric spaces are concerned, is not the internal structure of its
points, but their outward relation to other points.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_2,
13
© Springer International Publishing Switzerland 2014

14
2
Distance
Maurice Fr´echet (1878–1973) studied under Hadamard (who
had proved the prime number theorem and had succeeded
Poincar´e) and Borel at the University of Paris (´Ecole Normale
Sup´erieure); his 1906 thesis developed “abstract analysis”, an
axiomatic approach to abstract functions that allows the Eu-
clidean concepts of convergence and distance, as well as the
usual algebraic operations, to be applied to functions. Many
terms, such as metric space, completeness, compactness etc.,
are due to him.
Fig. 2.1 Fréchet
Although most distance functions treated in this book are of the type d(x, y) =
|x −y|, as for R, the point of studying metric spaces in more generality is not only
that there are some exceptions that don’t ﬁt this type, but also to emphasize that
addition/subtraction is not essential, as well as to prepare the groundwork for even
more general spaces, called topological spaces, in which pure convergence is studied
without reference to distances (but which are not covered in this book).
There are two additional axioms satisﬁed by some metric spaces that merit par-
ticular attention: complete metrics, which guarantee that their Cauchy sequences
converge, and separable metric spaces whose elements can be handled by approx-
imations. Both properties are possessed by compact metric spaces, which is what
is often meant when the term “ﬁnite” is applied in a geometric sense. These are
considered in later sections.
Easy Consequences
1. d(x, z) ⩾|d(x, y) −d(z, y)|.
2. If x1, . . . , xn are points in X, then by induction on n,
d(x1, xn) ⩽d(x1, x2) + · · · + d(xn−1, xn).
Examples 2.2
1. ThespacesN,Z,Q,R,andChavethestandarddistanced(a, b) := |a −b|.Check
that the three axioms for a distance are satisﬁed, making use of the in/equalities
|s + t| ⩽|s| + |t|, |−s| = |s|, and |s| = 0 ⇒s = 0.
2. ▶The vector spaces RN and CN have the standard Euclidean distance deﬁned by
d(x, y) :=
N
i=1 |ai −bi|2 for x = (a1, . . . , aN), y = (b1, . . . , bN) (prove
this for N = 2).
3. One can deﬁne distances on other more general spaces, e.g. we will later show
that the space of real continuous functions f with domain [0, 1] has a distance
deﬁned by d( f, g) := maxx∗[0,1] | f (x) −g(x)|.
4. →The space of ‘shapes’ in R2 (roughly speaking, subsets that have an area) have
a metric d(A, B) deﬁned as the area of (A ♦B)∅(A ∃B).

2
Distance
15
5. ▶Any subset of a metric space is itself a metric space (with the ‘inherited’ or
‘induced’ distance). (The three axioms are such that they remain valid for points
in a subset of a metric space.)
6. ▶The product of two metric spaces, X × Y, can be given several distances, none
of which have a natural preference. Two of them are the following
D1
x1
y1

,
x2
y2

:= dX(x1, x2) + dY (y1, y2),
D∀
x1
y1

,
x2
y2

:= max(dX(x1, x2), dY (y1, y2)).
For convenience, we choose D1 as our standard metric for X × Y, except for RN
and CN, for which we take the Euclidean one.
Proof for D1: Positivity of D1 and axiom (ii) are obvious. To prove axiom (iii),
D1(x1, x2) = 0 implies dX(x1, x2) = 0 = dY (y1, y2), so x1 = x2, y1 = y2, and
x1 =
x1
y1

=
x2
y2

= x2. As for the triangle inequality,
D1(x1, x2) = dX(x1, x2) + dY (y1, y2)
⩽dX(x1, x3) + dX(x3, x2) + dY (y1, y3) + dY (y3, y2)
= D1(x1, x3) + D1(x3, x2).
Exercises 2.3
1. Show that if d(x, z) > d(z, y) then x ∈= y.
2. Write in mathematical language,
(a) The subsets A, B are close to within 2 distance units;
(b) A and B are arbitrarily close.
3. The set of bytes, i.e., sequences of 0s and 1s (bits) of length 8 (or any length),
has a “Hamming distance” deﬁned as the number of bits where two bytes differ;
e.g. the Hamming distance between 10010111 and 11001101 is 4.
4. Any non-empty set can be given a distance function. The simplest is the discrete
metric d(x, y) :=
1
x ∈= y
0
x = y . Indeed, there are inﬁnitely many other metrics
on the same set (except when there is only one point!); for example, if d is a
distance function then so are 2d and d/(1 + d).
(* Not every function of d will do though! The function d2 is not generally a
metric; what properties does f : im d ∞R+ need to have in order that f ⊆d
also be a metric?)
5. A set may have several distances deﬁned on it, but each has to be considered as
a different metric space. For example, the set of positive natural numbers has a
distance deﬁned by d(m, n) := |1/m−1/n| (prove!); the metric space associated
with it has very different properties from N with the standard Euclidean distance.

16
2
Distance
For example, in this space one can ﬁnd distinct natural numbers that are arbitrarily
close to each other.
6. Let n = ±2k3r · · · be the prime decomposition of any n ∗Z and deﬁne |n|2 :=
1/2k, |0|2 := 0. Show that | · |2 satisﬁes the same properties as the standard
absolute value and hence that d(m, n) := |m −n|2 is a distance on Z (called the
2-adic metric).
7. * Given the distances between n points in RN, can their positions be recovered?
Can their relative positions be recovered?
2.1 Balls and Open Sets
The distance function provides an idea of the “surroundings” of a point. Given a
point a and a number r > 0, we can distinguish between those points ‘near’ to it,
satisfying d(x, a) < r, and those that are not.
Deﬁnition 2.4
An (open) ball, with center a and radius r > 0, is the set
Br(a) := { x ∗X : d(x, a) < r }.
Despite the name, we should lay aside any preconception we may have of it being
“round” or symmetric. We are now ready for our ﬁrst, simple, proposition:
Proposition 2.5
Distinct points of a metric space can be separated by disjoint balls,
x ∈= y
⇒
⇐r > 0
Br(x) ∃Br(y) = ∅.
Proof If x ∈= y then d(x, y) > 0 by axiom (iii). Letting r := d(x, y)/2, then Br(x)
is disjoint from Br(y) else we get a contradiction,
z ∗Br(x) ∃Br(y) ⇒d(x, z) < r and d(y, z) < r
⇒d(x, y) ⩽d(x, z) + d(y, z)
< 2r = d(x, y).
⊂⇔

2.1 Balls and Open Sets
17
Examples 2.6
1. In R, every ball is an open interval
Br(a) = { x ∗R : |x −a| < r } = ]a −r, a + r[.
Conversely, any open interval of the type ]a, b[ is a ball in R, namely
B|b−a|/2( a+b
2 ).
2. In R2, the ball Br(a) is the disk with center a and radius r without the circular
perimeter.
3. In Z, B1/2(m) = { n ∗Z : |n−m| < 1
2 } = { m } and B2(m) = { m−1, m, m+1 }.
4. It is clear that balls differ depending on the context of the metric space; thus
B1/2(0) = ]−1
2, 1
2[ in R, but B1/2(0) = { 0 } in Z.
Open Sets
We can use balls to explore the relation between a point x and a given set A. As the
radius of the ball Br(x) is increased, one is certain to include some points which
are in A and some points which are not, unless A = X or A = ∅. So it is more
interesting to investigate what can happen when the radius is small. There are three
possibilities as r is decreased: either Br(x) contains (i) only points of A, or (ii) only
points in its complement Ac, or (iii) points of both A and Ac, no matter how small
we take r.
Deﬁnition 2.7
A point x of a set A is called an interior point of A when it can be “surrounded
completely” by points of A, i.e.,
⇐r > 0,
Br(x) ∩A.
In this case, A is also said to be a neighborhood of x.
A point x (not in A) is an exterior point of A when
⇐r > 0,
Br(x) ∩X∅A.
All other points are called boundary points of A.
Accordingly, the set X is partitioned into three parts: its interior A⊆, its
exterior ( ¯A)c, and its boundary ∂A. The set of interior and boundary points of
A is called the closure of A and denoted by ¯A := A⊆♦∂A.
A set A is open in X when all its points are interior points of it, i.e., A = A⊆
(Fig.2.2).

18
2
Distance
A small enough ball around an exterior point
Any ball around a boundary point
A small enough ball around an interior point
Fig. 2.2 The distinction between interior, boundary, and exterior points
Examples 2.8
1. In R, the intervals ]a, b[, [a, b[, ]a, b], and [a, b] have the same interior ]a, b[,
exterior, and boundary { a, b }; their closure is ]a, b[ = [a, b].
Proof For any a < x < b, let 0 < δ < min(x −a, b −x), then a < x −δ <
x + δ < b, that is Bδ(x) ∪]a, b[; this makes x an interior point of the interval.
For x < a, there is an δ < a −x such that x ∗Bδ(x) ∪]−∀, a[ ∪R∅[a, b].
Similarly, any x > b is an exterior point of the interval.
For x = a, any small interval Bδ(a) contains points such as a + δ/2, that are
inside Bδ(a), and points outside it, such as a −δ/2, making a (and similarly b)
a boundary point.
2. ▶The following sets are open in any metric space X:
(a) X∅{ x } for any point x. The reason is that any other point y ∈= x is separated
from x by disjoint balls (our ﬁrst proposition); this makes y an interior point
of X∅{ x }.
(b) The empty set is open by default, because it does not contain any point. The
whole space X is also open because Br(x) ∩X for any r > 0 and x ∗X.
a
x
y
B r (a)
B (x)
(c) Balls are open sets in any metric space.
Proof Let x ∗Br(a) be any point in the
given ball, meaning d(x, a) < r. Let δ :=
r−d(x, a) > 0; then Bδ(x) ∩Br(a) since
for any y ∗Bδ(x),
d(y, a) ⩽d(y, x) + d(x, a) < δ + d(x, a) = r.
3. ▶The least upper bound of a set A in R is a boundary point of it.
Proof Let α be the least upper bound of A. For any δ > 0, α + δ/2 is an upper
bound of A but does not belong to it (else α would not be an upper bound).

2.1 Balls and Open Sets
19
Even if α ∈∗A, then the interval ]α −δ/2, α[ cannot be devoid of elements of
A, otherwise α would not be the least upper bound. So the neighborhood Bδ(α)
contains elements of both A and Ac.
Proposition 2.9
The set of interior points A⊆is the largest open set inside A.
Proof If B ∩A then the interior points of B are
obviously interior points of A, so B⊆∩A⊆. In
particular every open subset of A lies inside A⊆
(because B = B⊆), and every (open) ball in A
lies in A⊆. This implies that if Br(x) ∩A then
Br(x) ∩A⊆, so that every interior point of A
is surrounded by other interior points, and A⊆is
open.
⊂⇔
A
B
Proposition 2.10
A set A is open ⇒A is the union of balls.
Proof Let A be an open set. Then every point of it is interior, and can be covered by
a ball Br(x)(x) ∩A. Taking the union of all the points of A gives
A =

x∗A
{ x } ∩

x∗A
Br(x)(x) ∩A,
forcing A = 
x∗A Br(x)(x), a union of balls.
Now let A := 
i Bri (ai) be a union of balls, and let x be any point in A.
Then x is in at least one of these balls, say, Br(a). But balls are open and hence
x ∗Bδ(x) ∩Br(a) ∩A. Therefore A consists of interior points and so is open. ⊂⇔
The early years of research in metric spaces have shown that most of the basic
theorems about metric spaces can be deduced from the following characteristic prop-
erties of open sets:
Theorem 2.11
Any union of open sets is open.
The ﬁnite intersection of open sets is open.

20
2
Distance
Proof (i) Consider the union of open sets, 
i Ai. Any x ∗
i Ai must lie in at least
one of the open sets, say A j. Therefore,
x ∗Br(x) ∩A j ∩

i
Ai
shows that it must be an interior point of the union.
A
B
(ii) It is enough, using induction (show!), to con-
sider the intersection of two open sets A ∃B. Let
x ∗A ∃B, meaning x ∗A and x ∗B, with
both sets being open. Therefore there are open balls
Br1(x) ∩A and Br2(x) ∩B. The smaller of these
two balls, with radius r := min(r1,r2), must lie in
A ∃B,
x ∗Br(x) = Br1(x) ∃Br2(x) ∩A ∃B.
⊂⇔
Examples 2.12
1. ▶The exterior ( ¯A)c = (Ac)⊆of a subset A is open in X.
2. A⊆= A∅∂A. So a set is open ⇒it does not contain any boundary points.
3. Let Y ∩X inherit X’s distance. Then A is open in Y if, and only if, A = U ∃Y
for some subset U open in X.
Proof Care must be taken to distinguish balls in Y from those in X: BY
r (x) =
B X
r (x) ∃Y. If A is open in Y, then by Proposition 2.10,
A =

a∗A
BY
r(a)(a) =

a∗A
B X
r(a)(a) ∃Y = U ∃Y.
For the converse, interior points of U ∩X which happen to be in Y are interior
points of A as a subset of Y,
y ∗B X
r (y) ∩U ⇒y ∗B X
r (y) ∃Y ∩U ∃Y = A.
Limit Points
It may happen that a point a of a set A is surrounded by points not in A, that is, there
is a ball Br(a) which contains no points of A other than a itself. We call such points
isolated points. The property that a point cannot be isolated from the rest of A is
captured by the following deﬁnition:

2.1 Balls and Open Sets
21
Deﬁnition 2.13
A point b (not necessarily in A) is a limit point of a set A when every ball
around it contains other points of A,
∀δ > 0, ⇐a ∈= b,
a ∗A ∃Bδ(b).
Thus every point of ¯A is either a limit point or an isolated point of A.
Exercises 2.14
1. In R, the set { a } has no interior points, a single boundary point a, and all other
points are exterior. It is not an open set in R. There are ever smaller open sets
that contain a, but there is no smallest one.
2. In R, { 1/n : n ∗N } = { 1/n : n ∗N } ♦{ 0 }.
3. The set Q, and also its complement, the set of irrational numbers Qc, do not
have interior (or exterior) points in R. Every real number is a boundary point of
Q.
Similarly every complex number is a boundary point of Q + iQ.
4. The set { m } in Z does not have any boundary points; it is an open set in Z
(B1/2(m) = { m }).
▶Notice that whether a point is in the interior (or boundary, or exterior) of a set
depends on the metric space under consideration. For example, { m } is open in N
but not open in R; the interval ]a, b[ is open in R, but not open when considered
as a subset of the x-axis in R2. We thus need to specify that a set A is open in X.
5. Describe the interior, boundary and exterior of the sets
{ (x, y) ∗R2 : |x| + |y| ⩽1 },
{ (x, y) ∗R2 : 1
2 < max(|x|, |y|) ⩽1 }.
6. Of the proper intervals in R, only ]a, b[, ]a, ∀[, and ]−∀, a[ are open.
7. In R2, the half-plane { (x, y) ∗R2 : y > 0 } and the rectangles ]a, b[×]c, d[ :=
{ (x, y) ∗R2 : a < x < b, c < y < d } are open sets.
8. ▶Ac has the same boundary as A; its interior is the exterior of A, that is,
( ¯A)c = (Ac)⊆(and ¯A = Ac⊆c); so ∂A = ¯A ∃Ac.
9. Find an open subset of R, apart from R itself, without an exterior.
So, the exterior of the exterior of A need not be the interior of A. Similarly, the
boundary of ¯A or A⊆need not equal the boundary of A.
10. ▶An inﬁnite intersection of open sets need not be open. For example, in R, the
open intervals ]−1/n, 1/n[ are nested one inside another. Their intersection is
the non-open set { 0 } (prove!). Find another example in R2.

22
2
Distance
11. Deduce from the theorem that if every { x } is open in X, then every subset of X
is open in X. This ‘extreme’ property is satisﬁed by N, and also by any discrete
metric space.
12. Any point x with d(x, a) > r is in the exterior of the open ball Br(a). But the
boundary of Br(a) need not be the set { x : d(x, a) = r }. Find a counterexample
in Z.
13. * Every open set in R is a countable disjoint union of open intervals. (Hint: An
open set in R is the disjoint union of open intervals; take a rational interior point
for each.)
In contrast to this simple case, the open sets in R2, say, can be much more
complicated—there is no simple characterization of them, apart from the deﬁn-
ition.
14. Can a set not have limit points? Can an inﬁnite set not have limit points?
15. In R, the set of integers Z has no limit points, but all real numbers are limit points
of Q.
16. (a) 1 is an interior isolated point of { 1, 2 } in Z;
(b) 1 is a boundary isolated point of { 1, 2 } in R;
(c) 1 is an interior limit point of [0, 2] in R;
(d) 1 is a boundary limit point of [0, 1] in R.
17. In R and Q, an isolated point of a subset must be a boundary point, or, equiva-
lently, an interior point is a limit point.
2.2 Closed Sets
Deﬁnition 2.15
A set F is closed in a space X when X∅F is open in X.
Proposition 2.16
A set F is closed ⇒F contains its boundary ⇒¯F = F.
Proof We have already seen that the boundary of a set F and of its complement Fc
are the same (because the interior of Fc is the exterior of F). So F is closed, and
Fc open, precisely when this common boundary does not belong to Fc, but belongs
instead to Fcc = F.
⊂⇔

2.2 Closed Sets
23
Examples 2.17
1. In R, the set [a, b] is closed, since R∅[a, b] = ]−∀, a[ ♦]b, ∀[ is the union of
two open sets, hence itself open. Similarly [a, ∀[ and ]−∀, a] are closed in R.
2. N and Z are closed in R, but Q is not.
3. ▶In any metric space X, the following sets are closed in X (by inspecting their
complements):
(a) the singleton sets { x },
(b) the ‘closed balls’ Br[a] := { x ∗X : d(x, a) ⩽r },
(c) X and ∅,
(d) the boundary of any set (the complement of ∂A is A⊆♦(Ac)⊆).
4. ▶The complement of an open set is closed. More generally, if U is an open set
and F a closed set in X, then U∅F is open and F∅U is closed. The reasons are
that U∅F = U ∃Fc and (F∅U)c = Fc ♦U.
Closed sets are complements of open ones, and their properties reﬂect this:
Proposition 2.18
The ﬁnite union of closed sets is closed.
Any intersection of closed sets is closed.
Proof These are the complementary results for open sets (Theorem 2.11). For F, G
closed sets in X, Fc, Gc are open, so the result follows from
(F ♦G)c = Fc ∃Gc,
	 
i
Fi
c
=

i
Fc
i ,
and the deﬁnition that the complement of a closed set is open.
⊂⇔
Theorem 2.19 Kuratowski’s closure ‘operator’
¯A is the smallest closed set containing A, called the closure of A.
A ∩B ⇒
¯A ∩¯B;
¯¯A = ¯A; A ♦B = ¯A ♦¯B.
Proof The complement of ¯A is the exterior of A, which is an open set, so ¯A is closed.
This implies ¯¯A = ¯A Proposition 2.16.

24
2
Distance
If A ∩B, then an exterior point of B is obviously an exterior point of A, that
is ( ¯B)c ∩( ¯A)c; so ¯A ∩¯B. It follows that if F is any closed set that contains A,
then ¯A ∩¯F = F, and this shows that ¯A is the smallest closed set containing A.
(Alternatively, Proposition2.9 can be used: how?)
Of course, ¯A ∩A ♦B follows from A ∩A ♦B; combined with ¯B ∩A ♦B, it
gives ¯A ♦¯B ∩A ♦B. Moreover, ¯A ♦¯B is a closed set which contains A ♦B, and
so must contain its closure A ♦B.
⊂⇔
Exercises 2.20
1. It is easy to ﬁnd sets in R which are neither open nor closed (so contain only
part of their boundary). Can you ﬁnd any that are both open and closed?
The terms “open” and “closed” are misnomers, but they have stuck in the liter-
ature, being derived from the earlier use of “open/closed intervals”.
2. The set { x ∗Q : x2 < 2 } is closed, and open, in Q.
3. In any metric space, a ﬁnite collection of points { a1, . . . , aN } is a closed set.
4. The following sets are closed in R: [0, 1] ♦{ 5 }, ∀
n=0[n, n + 1
2].
5. The inﬁnite union of closed sets may, but need not, be closed. For example, the
set ∀
n=1{ 1/n } is not closed in R; which boundary point is not contained in it?
6. Find two disjoint closed sets (in R2 or Q, say) that are arbitrarily close to each
other.
7. Start with the closed interval [0, 1]; remove the
open middle interval ] 1
3, 2
3[ to get two closed
intervals [0, 1
3] ♦[ 2
3, 1]. Remove the middle
interval of each of these intervals to obtain four
closed intervals [0, 1
9]♦[ 2
9, 1
3]♦[ 2
3, 7
9]♦[ 8
9, 1].
If we continue this process indeﬁnitely we end
up with the Cantor set. Show it is a closed set.
8. Denote the decimal expansion of any number in [0,1] by 0.n1n2n3 . . .. Show
that
{ x ∗[0, 1] : x = 0.n1n2n3 . . . ⇒
n1 + · · · + nk
k
⩽5 ∀k }
is closed in R.
9. ▶One can deﬁne the “distance” between a point and a subset of a metric space
by d(x, A) := infa∗A d(x, a). Then x ∗¯A exactly when d(x, A) = 0.
10. Let x be an exterior point of A, and let y ∗¯A have the least distance between x
and ¯A. Do you think that y is unique? or that it must be on the boundary of A?
Prove or disprove. For starters, take the metric space to be R2.

2.2 Closed Sets
25
11. Show equality need not hold in A ∃B ∩¯A ∃¯B. Indeed two disjoint sets may
‘touch’ at a common boundary point.
12. Show the complementary results of the theorem: A⊆∃B⊆= (A∃B)⊆, A⊆⊆= A⊆.
13. If A ∩¯B, does it follow that A⊆∩B?
Dense Subsets
We often need to approximate an element x ∗X to within some small distance δ
by an element from some special subset A ∩X. The elements of A may be simpler
to describe, or more practical to work with, or may have nicer theoretical qualities.
For example, computers cannot handle arbitrary real numbers and must approximate
them by rational ones; polynomials are easier to work with than general continuous
functions. The property that elements of a set A can be used to approximate elements
of X to within any δ, namely,
∀x ∗X, ∀δ > 0, ⇐a ∗A, d(x, a) < δ,
is equivalent to saying that any ball Bδ(x) contains elements of A, in other words A
has no exterior points.
Deﬁnition 2.21
A set A is dense in X when ¯A = X (so ¯A contains all balls).
A set A is nowhere dense in X when ¯A contains no balls.
Exercises 2.22
1. ▶Q is dense in R. (This is equivalent to the Archimedean property of R.) More
generally, a set A is dense in R when for any two distinct real numbers x < y,
there is an element a ∗A between them x < a < y.
2. The intersection of two open dense sets is again open and dense.
3. A ﬁnite union of straight lines in R2 is nowhere dense. Z and the Cantor set are
nowhere dense in R.
4. Nowhere dense sets have no interior points.
5. A is nowhere dense in X ⇒X∅¯A is dense in X ⇒
¯A is the boundary of an
open set.
6. * What are the nowhere dense sets in R? (Hint: Exercise 2.14(13))

26
2
Distance
Remarks 2.23
1. If d(x, y) = 0 does not guarantee x = y, but d satisﬁes the other two axioms,
then it is called a pseudo-distance. In this case, let us say that points x and y
are indistinguishable when d(x, y) = 0 ( ⇒∀z, d(x, z) = d(y, z)). This is
an equivalence relation, which induces a partition of the space into equivalence
classes [x]. The function D([x], [y]) := d(x, y) is then a legitimate well-deﬁned
metric.
In a similar vein, if d satisﬁes the triangle inequality, but is not symmetric, then
D(x, y) := d(x, y)+d(y, x) is symmetric and still satisﬁes the triangle inequal-
ity.
Positivityofd follows fromaxioms (i) and(ii), d(x, y) ⩾|d(x, z) −d(y, z)| ⩾0.
2. The axioms for a distance can be re-phrased as axioms for balls:
(a) B0(x) = ∅, 
r>0 Br(x) = { x }, 
r>0 Br(x) = X,
(b) { y : x ∗Br(y) } = Br(x),
(c) Bs ⊆Br(x) ∩Br+s(x), i.e., if y ∗Bs(z) where z ∗Br(x) then y ∗Br+s(x).
3. The concept of open sets is more basic than that of distance. One can give a set X
a collection of open sets satisfying the properties listed in Theorem 2.11 (taken
as axioms), and study them without any reference to distances. It is then called a
topological space; most theorems about metric spaces have generalizations that
hold for topological spaces. There are some important topological spaces that are
not metric spaces, e.g. the arbitrary product of metric spaces 
i Xi, and spaces
of functions XY := { f : Y ∞X }.

Chapter 3
Convergence and Continuity
3.1 Convergence
The previous chapter was primarily intended to expand our vocabulary of
mathematical terms in order to better describe and clarify the concepts that we will
need. Our ﬁrst task is to deﬁne convergence.
Deﬁnition 3.1
A sequence (xn) in a metric space X converges to a limit x, written
xn ∞x as n ∞√, when
∗∂> 0,
⇒N,
n ⩾N →xn ♦B∂(x).
x0
x1
x2
A sequence which does not converge is said to diverge.
One may express this as “any neighborhood of x contains all the sequence from
some point onwards,” or “eventually, the sequence points get arbitrarily close to the
limit”.
Proposition 3.2
In a metric space, a sequence (xn) can only converge to one limit, denoted
lim
n∞√xn.
Proof Suppose xn ∞x and xn ∞y as n ∞√, with x ∃= y. Then they can be
separated by two disjoint balls Br(x) and Br(y) (Proposition 2.5). But convergence
means
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_3,
27
© Springer International Publishing Switzerland 2014

28
3
Convergence and Continuity
Felix Hausdorﬀ(1868–1942) studied atmospheric refraction in
Bessel’s school at Leipzig in 1891. In 1914, at 46 years of age
in the University of Bonn, he published his major work on set
theory, with chapters on partially ordered sets, measure spaces,
topology and metric spaces, where he built upon Fr´echet’s ab-
stract spaces, using open sets and neighborhoods.
Later, in
1919, he introduced fractional dimensions. But in the late 1930s,
increasing Nazi persecution made life impossible for him.
Fig. 3.1 Hausdorff
⇒N1
n ⩾N1 →xn ♦Br(x),
⇒N2
n ⩾N2 →xn ♦Br(y).
For n ⩾max(N1, N2) this would result in xn
♦
Br(x) ∀Br(y) = ∅, a
contradiction.
∈⊆
Examples 3.3
1. In any metric space, xn ∞x
⇔
d(xn, x) ∞0 as n ∞√(because
xn ♦B∂(x) ⇔d(xn, x) < ∂). For example, xn ∞x when d(xn, x) ⩽1/n
holds.
2. In R, n/(n +1) ∞1 as n ∞√, since for any ∂, there is an N such that 1/N < ∂
(Archimedean property of R), so
n ⩾N →
1 −
n
n + 1
 =
1
n + 1 < 1
N < ∂.
3. Given two convergent real sequences an ∞a and bn ∞b, then an +bn ∞a+b.
Proof For any ∂> 0, there are N1, N2, such that
n ⩾N1 →|an −a| < ∂,
n ⩾N2 →|bn −b| < ∂.
Thus for n ⩾max(N1, N2),
|(an + bn) −(a + b)| ⩽|an −a| + |bn −b| < 2∂.
4. ▶A sequence
xn
yn

in X ×Y converges to
xy

if, and only if, xn ∞x and yn ∞y.
Proof Any distance in Example 2.2(6) can be used, but we will use the standard
metric here. The distance between
xn
yn

and
xy

is
δ := d
xn
yn

,
xy

= d(xn, x) + d(yn, y) ∞0, as n ∞√.
As both d(xn, x) and d(yn, y) are less than δ, the converse follows.

3.1 Convergence
29
5. Consider a composition of functions N ∞N ∞X where the ﬁrst function is 1–1,
and the second is a sequence. A subsequence is the case when the ﬁrst function is
strictly increasing, and a rearrangement is the case when it is 1–1 and onto. For
example, 1, 1/4, 1/9, . . . is a subsequence of (1/n), and 1/2, 1, 1/4, 1/3, . . . is a
rearrangement.Anysuch‘sub-selection’ofaconvergentsequencealsoconverges,
to the same limit.
Proof Suppose n ⩾N →d(xn, x) < ∂. Let (xni ) be a sub-selection of (xn).
As ni ⩽N can only be true of a ﬁnite number of indices i, with the largest, say,
M, it follows that
i > M →ni > N →d(xni , x) < ∂.
6. A sequence converges fast (or ‘linearly’) when d(xn, x) ⩽Acn for some real
constants A > 0, 0 < c < 1. Quadratic convergence, d(xn, x) ⩽Ac2n, is even
faster. Instead 1/n and
n⇐
2 converge slowly.
Limits and Closed Sets
There are many questions in analysis of the type: If xn has a property A, and xn ∞x,
does x still have this property? For example, if a convergent sequence of vectors in
the plane lies on a circle, will its limit also lie on the same circle? Or, can continuous
functions (or differentiable, or integrable, etc.) converge to a discontinuous function?
The following proposition answers this question in a general setting: the ‘property’
A needs to be closed in the metric space.
Proposition 3.4
If xn ♦A and xn ∞x, then x ♦¯A.
Conversely, in a metric space, for any x ♦¯A there is a sequence xn ♦A
which converges to x.
In particular, closed sets are “closed” under the process of taking the limit.
Proof Take any ball B∂(x) about x. If xn converges to x, then all the sequence points
will be in the ball for n large enough. Since xn ♦A, x cannot be an exterior point,
and so lim
n∞√xn = x ♦¯A. Of course, when A is closed, ¯A = A (Proposition 2.16).
For the converse, let B1/n(x) be a decreasing sequence of nested balls around
x ♦¯A; whether x is a boundary or interior point of A, B1/n(x) contains at least a
point an in A (which could be x itself). So d(an, x) < 1/n ∞0 as n ∞√, and
an ∞x.
∈⊆

30
3
Convergence and Continuity
Exercises 3.5
1. ▶In R,
(a) 1/n ∞0 (this is a rewording of the Archimedean property of the real num-
bers: for every x > 0, there is an n ♦N such that n > x).
(b) an ∞0 when 0 < a < 1, but diverges for a > 1. (Hint: When 1 < a = 1+δ,
then an = 1 + nδ + · · · > nδ; otherwise consider 1/a.)
(c) n/an ∞0 when a > 1, hence nk/an = (n/bn)k ∞0.
(d)
n⇐a ∞1 for any a > 0, and n1/n ∞1 (so (log n)/n ∞0). (Assuming
a > 1, expand a1/n =: 1 + an using the binomial theorem to show that
an < a/n ∞0; similarly show a2
n < 2/(n −1) for the second sequence.)
(e) * (1 + 1/n)n converges to a number denoted e. This is too hard to show for
the moment. Show at least that the sequence is increasing but bounded by 3,
using the binomial theorem. (This highlights the need of “convergence tests”:
how can one know that a sequence converges when the limit is unknown?)
(f)
n⇐
n! ∞√(what should this mean?)
2. What do the sequences 2+

2 + ⇐2 + · · · and 1+
1
1+
1
1+··· converge to, assuming
they do?
3. In R, if an ∞0 then an
n ∞0; ﬁnd examples where (i) an ∞0 but a1/n
n
∃∞0,
(ii) an ∞1 but an
n ∃∞1.
4. ▶If an ⩽bn for two convergent real sequences then lim
n∞√an ⩽lim
n∞√bn (Hint:
[0, √[ is closed). In particular, if an converges and an < a, then lim
n∞√an ⩽a.
5. Squeezing principle: In R, if an ⩽xn ⩽bn and lim
n∞√an = a = lim
n∞√bn, then xn
converges (to a).
6. It is possible for a divergent sequence to have a convergent subsequence. Find
one in the sequence (1, −1, 1, −1, . . .). But any rearrangement must diverge.
7. ▶We may occasionally encounter ‘sequences’ with two indices (amn) (they are
more properly called nets). The example n/(n + m) shows that in general
lim
m∞√lim
n∞√amn ∃= lim
n∞√lim
m∞√amn.
The same example shows that, in R, generally, supn infm anm ∃= infm supn anm.
But the following are true:
(a) supn supm anm = supm supn anm,
(b) supn(an + bn) ⩽supn an + supn bn.
8. If xn ∞x in a metric space X, and xn ∃= x for all n, then x is a limit point of the
set { x1, x2, x3, . . . }. But if xn is eventually constant (n ⩾N →xn = x), then
xn ∞x without x being a limit point of { xn }.

3.1 Convergence
31
Note that given a real sequence (xn), even one that does not converge, the largest
limit point of { xn } is denoted lim sup xn, and the smallest lim inf xn (if they exist).
3.2 Continuity
One is often not particularly interested in the actual values of the distances between
points: no new theorems will result by substituting metres with feet. What matters
more, in most cases, is convergence. Accordingly, functions that preserve conver-
gence (rather than distance) take on a central importance.
Deﬁnition 3.6
A function f : X ∞Y between metric spaces is continuous when it preserves
convergence,
xn ∞x in X →f (xn) ∞f (x) in Y.
In this case therefore, f (limn∞√xn) = limn∞√f (xn). Before we see any exam-
ples, let us prove that the following three statements are equivalent formulations of
continuity in metric spaces, so any of them can be taken as the deﬁnition of continuity.
Theorem 3.7
A function f : X ∞Y between metric spaces is (i) continuous
⇔(ii) ∗x ♦X, ∗∂> 0, ⇒δ > 0,
dX(x, x⊂) < δ →dY ( f (x), f (x⊂)) < ∂,
⇔(iii) For every open set V in Y, f −1V is open in X.
The second statement is often written as limx⊂∞x f (x⊂) = f (x) for all x.
Proof (i) →(ii) Suppose statement (ii) is false; then there is a point x ♦X and an
∂> 0 such that arbitrarily small changes to x can lead to sudden variations in f (x),
∗δ > 0,
⇒x⊂,
dX(x, x⊂) < δ and dY ( f (x), f (x⊂)) ⩾∂.
In particular, letting δ = 1/n, there is a sequence1 xn ♦X satisfying dX(x, xn) <
1/n but dY ( f (x), f (xn)) ⩾∂. This means that xn ∞x, but f (xn) ∃∞f (x),
contradicting statement (i).
1 This selection of points xn needs the Axiom of Choice for justiﬁcation.

32
3
Convergence and Continuity
(ii) →(iii) Note that (ii) can be rewritten as
∗x ♦X,
∗∂> 0,
⇒δ > 0,
x⊂♦Bδ(x) →f (x⊂) ♦B∂( f (x))
or even as
∗x ♦X,
∗∂> 0,
⇒δ > 0,
f Bδ(x) ⇔B∂( f (x)).
Let V be an open set in Y. To show that U := f −1V is open in X, let x be any point
of U; then f (x) ♦V , which is open. Hence
f (x) ♦B∂( f (x)) ⇔V,
and so
⇒δ > 0,
f Bδ(x) ⇔B∂( f (x)) ⇔V.
In other words, x is an interior point of U,
⇒δ > 0,
Bδ(x) ⇔f −1V = U.
X
Y
f
V
f −1V
x
f (x)
(iii) →(i) Let (xn) be a sequence converging to x. Consider any open neighborhood
B∂( f (x)) of f (x). Then f −1B∂( f (x)) contains x, and is an open set by (iii), so
⇒δ > 0,
x ♦Bδ(x) ⇔f −1B∂( f (x)),
→
⇒δ > 0,
f Bδ(x) ⇔B∂( f (x)).
But eventually all the points xn are inside Bδ(x),
⇒N > 0,
n > N →xn ♦Bδ(x)
→f (xn) ♦f Bδ(x) ⇔B∂( f (x))
→dY ( f (xn), f (x)) < ∂.
This shows that f (xn) ∞f (x) as n ∞√.
∈⊆

3.2 Continuity
33
Examples 3.8
1. The square root function on [0, √[ is continuous.
Proof Let x, ∂> 0, and δ := ∂⇐x (for x = 0, choose δ = ∂2), then
|x −y| < δ →|⇐x −⇐y| <
δ
⇐x + ⇐y ⩽
∂
1 + ⇐y/x < ∂.
2. Let X, Y, Z be metric spaces, then the function h : X ∞Y × Z deﬁned by
h(x) := ( f (x), g(x)) is continuous if, and only if, f , g are continuous. For
example, the circle path α ∩∞(cos α, sin α) is a continuous map R ∞R2.
Proof The statement follows directly from Example 3.3(4)
 f (xn)
g(xn)

∞
 f (x)
g(x)

⇔f (xn) ∞f (x) and g(xn) ∞g(x).
3. ▶If f : X ∞Y is continuous, then f ¯A ⇔f A. So if A is dense in X, then f A
is dense in f X.
Proof If x ♦¯A, then there is a sequence of elements of A that converge to x,
xn ∞x (Proposition 3.4). By continuity of f , f (xn) ∞f (x), so f (x) ♦f A.
It follows that if ¯A = X then f X ⇔f A ∀f X.
The following three propositions afﬁrm that continuity is well-behaved with
respect to composition and products, and that the distance function is continuous.
They allow us to build up continuous functions from simpler ones.
Proposition 3.9
If f : X ∞Y and g: Y ∞Z are continuous, so is g ∪f : X ∞Z.
Proof Let xn ∞x in X. Then by continuity of f, f (xn) ∞f (x) in Y, and by
continuity of g,
g ∪f (xn) = g( f (xn)) ∞g( f (x)) = g ∪f (x) in Z.
Alternatively, let W be any open set in Z. Then g−1W is an open set in Y, and so
f −1g−1W is an open set in X. But this set is precisely (g ∪f )−1W.
∈⊆
Proposition 3.10
The distance function d : X2 ∞R is continuous.

34
3
Convergence and Continuity
Proof Let xn ∞x and yn ∞y in X. Then, by the triangle inequality,
|d(xn, yn) −d(x, y)| ⩽|d(xn, yn) −d(x, yn)| + |d(x, yn) −d(x, y)|
⩽d(xn, x) + d(yn, y) ∞0,
which gives d(xn, yn) ∞d(x, y) as n ∞√.
Homeomorphisms
Continuous functions preserve convergence, a central concept in metric spaces; in
this sense, they correspond to homomorphisms of groups and rings, which preserve
the group and ring operations. The analogue of an isomorphism is called a homeo-
morphism:
Deﬁnition 3.11
A homeomorphism between metric spaces X and Y is a mapping J : X ∞Y
such that
J is bijective (1–1 and onto),
J is continuous,
J −1 is continuous.
A metric space X is said to be embedded in another space Y, when there is a
subset Z ⇔Y such that X is homeomorphic to Z.
Like all other isomorphisms, “X is homeomorphic
to Y” is an equivalence relation on metric spaces.
When X and Y are homeomorphic, they are not only
the same as sets (the bijection part) but also with
respect to convergence:
xn ∞x ⇔J(xn) ∞J(x),
and
A is open in X ⇔J A is open in Y.
The elements of Y are those of X in different clothing, as far as convergence is
concerned. The most vivid picture is that of “deforming” one space continuously
and reversibly from the other. The by-now classic example is that a ‘teacup’ is
homeomorphic to a ‘doughnut’.

3.2 Continuity
35
Exercises 3.12
1. Any constant function f : x ∩∞y0 ♦Y is continuous. The identity function
I : X ∞X, x ∩∞x, is always continuous.
2. The functions that map the real number x to x + 1, 2x, xn(n ♦N), ax(a > 0),
and |x| are all continuous.
3. In R, addition and multiplication are continuous, i.e., if xn ∞x and yn ∞y then
xn + yn ∞x + y and xnyn ∞xy. Deduce that if f, g: X ∞R are continuous
functions, then so are f + g and f g. For example, the polynomials on R are
continuous. The function max: R2 ∞R is also continuous, i.e., max(xn, yn) ∞
max(x, y).
4. The function f : ]0, √[ ∞]0, √[, deﬁned by f (x) := 1/x is continuous.
5. Conjugation in C, z ∩∞¯z, is continuous.
6. In R, the characteristic function 1A(x) =
1 x ♦A
0 x ∃♦A is always discontinuous
except when A = ∅or A = R. Is this true for all metric spaces?
7. When f : X ∞R is a continuous function, the set { x ♦X : f (x) > 0 } is open
in X.
8. Any function f : N ∞N is continuous.
9. The graph of a continuous function f : X ∞Y, namely { (x, f (x)) : x ♦X },
is closed in X × Y (with the D1 metric).
10. Find examples of continuous functions f (in X = Y = R) such that
(a)
f is invertible but f −1 is not continuous.
(b)
f (xn) ∞f (x) in Y but (xn) does not converge at all.
(c) U is open in X but f U is not open in Y. However functions which map
open sets to open sets do exist (ﬁnd one) and are called open mappings.
11. If F is a closed set in Y and f : X ∞Y is a continuous function, then f −1F
is closed in X. But f may map a closed set to a non-closed set (even if f is an
open mapping!).
12. It is not enough that f (x, y) is continuous in x and y separately in order that f
be continuous. For example, show that the function
f (x, y) :=
xy
x2 + y2 ,
f (0, 0) := 0,
is discontinuous at (0, 0) even though f (xn, 0) ∞0, f (0, yn) ∞0, when xn ∞
0, yn ∞0. It needs to be “jointly continuous” in the sense that f (xn, yn) ∞
f (x, y) for any (xn, yn) ∞(x, y).

36
3
Convergence and Continuity
13. The function f (x) := p(⇐x) on the domain R+, where p is a polynomial, is
continuous.
14. The roots of a quadratic equation ax2 + bx + c = 0 vary continuously as the
coefﬁcients change (but maintaining b2 ⩾4ac), except at a = 0.
15. ▶Usethecontinuityofd toﬁndashortproofthatthesphere Sr :={y :d(x, y)=r}
is closed.
16. Given a set A ⇔X, the map x ∩∞d(x, A) is continuous. (Hint: d(y, A) ⩽
d(y, x) + d(x, A).)
17. Given disjoint non-empty closed subsets A, B ⇔X, ﬁnd a continuous function
f : X ∞[0, 1] such that f A = 0, f B = 1 (Hint: use d(x, A) and d(x, B)).
18. Every interval in R is homeomorphic to [0, 1], [0, √[, or R.
19. N is homeomorphic to the discrete metric space on a countable set, but Q is
not. (Hint: The convergent sequence 1/n ∞0 must correspond to a divergent
sequence in N.)
20. ♦A bent line in the plane, consisting of two straight line segments meeting at
their ends, is homeomorphic to the unbent line. Thus angles are meaningless
as far as homeomorphisms are concerned; triangles, squares and circles are
homeomorphic.

Chapter 4
Completeness and Separability
4.1 Completeness
Our task of rigorously deﬁning convergence in a general space has been achieved,
but there seems to be something circular about it, because convergence is deﬁned
in terms of a limit. For example, take a convergent sequence xn ∞x in a metric
space X, and “artiﬁcially” remove the point x to form X∅x (assume √n, xn ∗= x).
The other points xn still form a sequence in this subspace, but it no longer converges
(otherwise it would have converged to two points in X)—its limit is “missing”. The
sequence (xn) is convergent in X but divergent in X∅x. How are we to know whether
a metric space has “missing” points? And if it has, is it possible to create them when
the bigger space X is unknown?
To be more concrete, let us take a look at the rational numbers: consider the
sequences (1, 2, 3, . . .), (1, −1, 1, −1, . . .), and (1, 1.5, 1.417, 1.414, 1.414, . . .),
the last one deﬁned iteratively by a0 := 1, an+1 := an
2 + 1
an . It is easy to show
that the ﬁrst two do not converge, but, contrary to appearances, neither does the
third, the reason being that were it to converge to a ⇒Q, then a = a/2 + 1/a,
implying a2 = 2, which we know cannot be satisﬁed by any rational number. This
sequence seems a good candidate of one which converges to a “missing” number not
found in Q. Having found one missing point, there are an inﬁnite number of them:
(2, 2.5, 2.417, 2.414, . . .) and (2, 3, 2.834, 2.828, . . .) cannot converge in Q.
But could it be that the ﬁrst two sequences also converge to “missing” numbers?
How are we to distinguish between sequences that “truly” diverge from those that
converge to “missing” points? There is a property that characterizes intrinsic conver-
gence: suppose that (xn) is divergent in the metric space Y, but converges xn ∞a
in a bigger space X. Then the points get close to each other (in Y),
dY (xn, xm) = dX(xn, xm) ⩽dX(xn, a) + dX(a, xm) ∞0, as n, m ∞→.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_4,
37
© Springer International Publishing Switzerland 2014

38
4
Completeness and Separability
Deﬁnition 4.1
A Cauchy sequence is one such that d(xn, xm) ∞0 as n, m ∞→, that is,
√∂> 0,
♦N,
n, m ⩾N ∃d(xn, xm) < ∂.
To clarify this idea further, we prove:
Proposition 4.2
Two sequences (xn), (yn) are deﬁned to be asymptotic when d(xn, yn) ∞0
as n ∞→.
(i) Being asymptotic is an equivalence relation.
(ii) For (xn) asymptotic to (yn),
(a) if (xn) is Cauchy then so is (yn),
(b) if (xn) converges to x then so does (yn).
(iii) A sequence (xn) is Cauchy if, and only if, every subsequence of (xn)
is asymptotic to (xn).
Proof (i) Let (xn) ∀(yn) signify d(xn, yn) ∞0 as n ∞→. Reﬂexivity and
symmetry of ∀are obvious. If (xn) ∀(yn) ∀(zn) then transitivity holds:
d(xn, zn) ⩽d(xn, yn) + d(yn, zn) ∞0 as n ∞→.
(ii) If d(xn, yn) ∞0 and d(xn, xm) ∞0 as n, m ∞→, then
d(yn, ym) ⩽d(yn, xn) + d(xn, xm) + d(xm, ym) ∞0.
Similarly, if d(xn, x) ∞0 then d(yn, x) ⩽d(yn, xn) + d(xn, x) ∞0.
(iii) A Cauchy sequence satisﬁes
√∂> 0, ♦N, n, m ⩾N ∃d(xn, xm) < ∂.
Given a subsequence (xni ), its indices satisfy ni ⩾i (by induction on i: n1 ⩾1,
n2 > n1 ⩾1 so n2 ⩾2, etc.). Thus
i ⩾N ∃ni, i ⩾N ∃d(xni , xi) < ∂
and d(xni , xi) ∞0 as i ∞→.

4.1 Completeness
39
Conversely, suppose (xn) is not Cauchy. Then
♦∂> 0, √i, ♦ni, mi ⩾i, d(xni , xmi ) ⩾∂,
from which we can create the subsequences (xn1, xn2, . . .) and (xm1, xm2, . . .). If
both these subsequences were asymptotic to (xn) then there would exist an N such
that i > N implies d(xi, xni ) < ∂/2 as well as d(xi, xmi ) < ∂/2. Combining these
two then gives a contradiction
d(xni , xmi ) ⩽d(xi, xni ) + d(xi, xmi ) < ∂,
so one of the two subsequences is not asymptotic to (xn).
∈⊆
Examples 4.3
1. Convergent sequences are always Cauchy, since if xn ∞x then d(xn, xm) ∞
d(x, x) = 0 by continuity of the distance function. But the discussion above
gives examples of Cauchy sequences which do not converge.
2. In R or Q, any increasing sequence that is bounded above, an ⩽b, is Cauchy.
Proof Split the interval [a0, b] into subintervals of length ∂. Let I be the last
subinterval which contains a point, say aN. As the sequence is increasing, I con-
tains all of the sequence from N onward, proving the statement.
a0
b0
a1
b1
a2
b2
a3
b3
3. RandQhavethebisectionproperty:Let[a0, b0]
beanintervalinRorQ,anddivideitintohalves,
[a0, c] and [c, b0], where c := (a0 + b0)/2
is the midpoint. Choose [a1, b1] to be either
[a0, c]or[c, b0],randomlyoraccordingtosome
criterion; continue taking midpoints to get a
nested sequence of intervals [an, bn], whose
lengths are
bn −an = (b0 −a0)/2n ∞0.
So, for any ∂> 0, there is an N > 0 such that bN −aN < ∂, and for any n ⩾N,
an, bn ⇒[aN, bN]. Hence (an) and (bn) are asymptotic Cauchy sequences.
4. Let Brn be a nested sequence of balls (Brn+1 ⊆Brn), with rn ∞0. Then choosing
any points xn ⇒Brn gives a Cauchy sequence.
Proof For any m ⩾n,
xm ⇒Brm ⊆Brm−1 ⊆· · · ⊆Brn
so that d(xm, xn) < 2rn ∞0 as n, m ∞→.

40
4
Completeness and Separability
5. A Cauchy sequence cannot stray too far in the sense that d(x0, xn) ⩽R for all
n, for some R ⩾0. Hence Cauchy sequences are “bounded”.
Proof By the deﬁnition of a Cauchy sequence for ∂:= 1 say, there is an N such
that n, m ⩾N ∃d(xn, xm) < ∂. Therefore
d(x0, xn) ⩽d(x0, xN) + d(xN, xn) < d(x0, xN) + ∂.
6. A Cauchy sequence in Q either converges to 0, or is eventually greater than some
∂> 0 or less than some −∂< 0. In each case, an asymptotic sequence behaves
in the same manner.
Proof If an ∗∞0 yet is Cauchy, then
♦∂> 0, √M, ♦m ⩾M,
|am| ⩾∂,
♦N,
m, n ⩾N ∃|an −am| < ∂/2.
Assuming, for example, am ⩾∂for some m ⩾N,
n ⩾N ∃an ⩾am −|an −am| > ∂/2.
If (bn) is an asymptotic sequence, there is an M such that |an −bn| < ∂/2
whenever n ⩾M, and so
n ⩾max(N, M) ∃bn ⩾an −|an −bn| ⩾∂/2.
Complete Metric Spaces
Deﬁnition 4.4
A metric space is complete when every Cauchy sequence in it converges.
In a complete metric space, there are no “missing” points and any divergent
sequence is “truly” divergent—there is no bigger metric space which makes it con-
vergent.
It follows that the space of rational numbers Q (with the standard metric) is not
complete, a fact that allegedly deeply troubled Pythagoras and his followers. They
shouldn’t have worried because there is a way of creating the missing numbers (but
skip the proof if it worries you on a ﬁrst reading!):
Theorem 4.5
The real number space R is complete.

4.1 Completeness
41
Proof (i) For this to be a theorem, we need to be clear about what constitutes R. The
usual deﬁnition is that it is a set with an addition + and multiplication · which satisfy
the axioms of a ﬁeld (see p. 9), and with a linear order relation ⩽that is compatible
with these operations:
x ⩾y ∃x + z ⩾y + z,
x, y ⩾0 ∃xy ⩾0,
and in addition satisﬁes the completeness axiom:
Every non-empty subset A of R with an upper bound has a least upper bound.
Assuming all these axioms, let (an) be a Cauchy sequence in R, that is, for any ∂> 0,
there is an N beyond which |an −am| < ∂. Let
B := {x ⇒R : ♦M, n ⩾M ∃x ⩽an}.
Its elements might be called eventual lower bounds of {an : n ⇒N}. The fact that
Cauchy sequences are bounded implies that {an : n ⇒N} has a lower bound and so
B ∗= ∅, while any upper bound of {an : n ⇒N} is also one of B. Hence, by the
completeness axiom, B has a least upper bound δ. Two facts follow,
(a) δ + ∂is not an element of B, so there must be an inﬁnite number of terms
ani < δ + ∂;
(b) δ −∂is not an upper bound of B, so there must exist an x ⇒B and an M such
that n ⩾M ∃δ −∂< x ⩽an.
These facts together imply that for ni ⩾M we have δ −∂⩽ani ⩽δ + ∂. Then
n ⩾max(M, N) ∃|an −δ| ⩽|an −ani | + |ani −δ| < 2∂
as required to show an ∞δ.
This proof is open to the criticism that we have not proved whether, in fact, there
exists such a set with all these properties. We need to ﬁll this logical gap by giving
a construction of R that satisﬁes these axioms.
(ii) The whole idea is to treat the Cauchy sequences of rational numbers themselves
as the missing numbers! How can a sequence be a number? Actually, this is not
really that novel—the familiar decimal representation of a real number is a particular
Cauchysequence:e := 2.71828 . . . is just short for (2, 2.7, 2.71, 2.718, . . .). Thereis
of coursenothingspecial about thedecimal system—thebinaryexpansion (2, 2 1
2, 2+
1
2+ 1
8, . . .),alongwithseveralotherCauchysequences,alsoconvergestoe.Weshould
be grouping these asymptotic Cauchy sequences together, and treat each class as one
real number. For example, the asymptotic sequences 0.32999 . . . and 0.33000 . . .
represent the same real number.
Accordingly, R is deﬁned as the set of equivalence classes of asymptotic Cauchy
sequences of rational numbers; each real number is here written as x = [an] (instead
of the cumbersome [(an)]). We now develop the structure of R: addition and multi-
plication, its order and distance function. Deﬁne

42
4
Completeness and Separability
x + y = [an] + [bn] := [an + bn],
xy = [an][bn] := [anbn].
That addition is well-deﬁned follows from an application of the triangle inequality in
Q; that it has the associative and commutative properties follows from the analogous
properties for addition of rational numbers. The new real zero is [0, 0, . . .], and the
negatives are −x = −[an] = [−an]. Similarly, multiplication is well deﬁned and
has all the properties that make R a ﬁeld.
It is less straightforward to deﬁne an inequality relation on R. Let (an) > 0 mean
that the Cauchy sequence (an) is eventually strictly positive Example 4.3(6),
♦∂⇒Q+, ♦N,
n ⩾N ∃an ⩾∂> 0.
Any other asymptotic Cauchy sequence must also eventually be strictly positive.
Correspondingly, let x < y mean that y −x > 0, or equivalently,
[an] < [bn] ⇐♦∂⇒Q+, ♦N, √n ⩾N, an + ∂⩽bn.
This immediately shows that x < y ⇐x + z < y + z. We make a few more
observations about this relation:
1. if an ⩾0 for all n, then [an] ⩾0,
2. if 0 < x and 0 < y then 0 < xy and 0 < x + y (gives transitivity of ⩽),
3. x > 0 or x = 0 or x < 0 (Example 4.3(6)).
4. if x < 0 then −x > 0.
Anti-symmetry of ⩽follows from the fact that (bn −an) cannot eventually be both
strictly positive and strictly negative. This makes R a linearly ordered ﬁeld.
Given a real number x = [an] = [bn], let |x| := [|an|], which makes sense since
|an| −|am|
 ⩽|an −am| ∞0 as n, m ∞→,
|an| −|bn|
 ⩽|an −bn| ∞0 as n ∞→.
In fact |x| = x when x > 0 and |x| = −x when x < 0, so it satisﬁes the properties
|x| ⩾0, |x| = 0 ⇐x = 0, |−x| = |x|, and |x + y| ⩽|x| + |y|. Thus d(x, y) :=
|x −y| is a distance, as in Example 2.2(1).
Q is dense in R: Note that a rational number a can be represented in R by the con-
stant sequence [a, a, . . .]. The Archimedean property holds since [an] > 0 implies
that eventually an ⩾p > 0, ♦p ⇒Q, so [an] ⩾[p/2] > 0. Also, if x = [an] then
an ∞x in R, since for any ∂> 0, let p ⇒Q, 0 < p < ∂, so
♦N,
n, m ⩾N ∃|an −am| < p
∃
d(an, x) = d([an, an, . . .], [a1, a2, . . .])
= [|an −a1|, |an −a2|, . . . ]
< ∂.

4.1 Completeness
43
The completeness axiom is satisﬁed: Let A be any non-empty subset of R that is
bounded above. Split R into the set B of upper bounds of A, and its complement Bc,
both of which are non-empty, say a0 ⇒Bc, b0 ⇒B; these can even be taken to be
rational, by the Archimedean property.
B
B
c
A
α
Divide [a0, b0] in two using the midpoint c := (a0 + b0)/2; if c ⇒B then select
[a1, b1] = [a0, c], otherwise take [a1, b1] = [c, b0]. Continue dividing and selecting
sub-intervals like this, to get two asymptotic Cauchy sequences (an), (bn), with
bn ⇒B, an ⇒Bc. Let δ := [an], so an ∞δ, bn ∞δ, and (Exercise 3.5(4))
√a ⇒A, a ⩽bn ∃√a ⇒A, a ⩽δ,
δ is an upperbound of A,
√b ⇒B, an ⩽b ∃√b ⇒B, δ ⩽b,
δ is the least upperbound.
A dual argument shows that every non-empty set with a lower bound has a greatest
lower bound, denoted inf A.
R is complete: This now follows from part (i), but we can see this directly in this
context. Start with any Cauchy sequence of real numbers (in decimal form, say) and
replace each number by a rational number to an increasing number of signiﬁcant
places, for example:
xn ⇒R
⊂∞an ⇒Q
2.6280 . . .
2
2.7087 . . .
2.7
2.7173 . . .
2.71
2.7181 . . .
2.718
. . .
The crucial point is that the two sequences are asymptotic by construction. Since the
ﬁrst one is Cauchy, so must be the second one. But a Cauchy sequence of rational
numbers is, by deﬁnition, a real number x. Moreover, an ∞x implies xn ∞x.
∈⊆
This “completion” process generalizes readily to any metric space.
Theorem 4.6
Every metric space X can be completed, that is, there is a complete met-
ric space X, containing (a dense copy of) X and extending its distance
function.
Any such complete metric space X is called the completion of X.

44
4
Completeness and Separability
Proof Construction of X: Let C be the set of Cauchy sequences of X. For any
two Cauchy sequences a = (xn), b = (yn), the real sequence d(xn, yn) is also
Cauchy (Exercise 4.10(6)), and since R is complete, it converges to a real number
D(a, b) := limn∞→d(xn, yn). Symmetry and the triangle inequality of D follow
from that of d, by taking the limit n ∞→in the following:
d(yn, xn) = d(xn, yn)
d(xn, yn) ⩽d(xn, zn) + d(zn, yn)

∃
 D(b, a) = D(a, b)
D(a, b) ⩽D(a, c) + D(c, b).
The only problem is that D(a, b) = 0, meaning d(xn, yn) ∞0, is perfectly possi-
ble without a = b. It happens when the Cauchy sequences (xn), (yn) are asymptotic.
We have already seen that this is an equivalence relation, so C partitions into equiv-
alence classes. Write ˜d([a], [b]) := D(a, b); it is well-deﬁned since for any other
representative sequences a⇔⇒[a] and b⇔⇒[b], we have
D(a⇔, b⇔) ⩽D(a⇔, a) + D(a, b) + D(b, b⇔) = D(a, b);
similarly D(a, b) ⩽D(a⇔, b⇔); so D(a, b) = D(a⇔, b⇔). Let X be this space of
equivalence classes of Cauchy sequences, with the metric ˜d.
There is a dense copy of X in X: For any x ⇒X, there corresponds the constant
sequence x := (x, x, . . .) in C. Since
˜d([x], [y]) = D((x), (y)) = lim
n∞→d(x, y) = d(x, y),
this set of constant sequences is a true copy of X, preserving distances between points.
To show that this copy is dense in X, we need to show that any representative Cauchy
sequence a = (xn)inC hasconstantsequencesarbitrarilyclosetoit.Bythedeﬁnition
ofCauchysequences,forany∂> 0,thereisan N ⇒Nwithd(xn, xN) < ∂forn ⩾N.
Let x be the constant sequence (xN). Then D(a, x) = limn∞→d(xn, xN) ⩽∂< 2∂
proves that [x] is within 2∂of [a].
X is complete: Let ([an]) be a Cauchy sequence in X; this means ˜d([an], [am]) =
D(an, am) ∞0, as n, m ∞→. For each n, we can ﬁnd a constant sequence xn
which is as close to an as needed, i.e., D(xn, an) < ∂n; by choosing ∂n ∞0, we
can select (xn) to be asymptotic to (an). As (an) is Cauchy, so is (xn). In fact,
xn ∞x := (xn) since
lim
n∞→D(xn, x) =
lim
m,n∞→d(xn, xm) = 0,
so that the asymptotic sequence an also converges to x, and [an] to [x].
∈⊆
Proving that a given metric space is complete is normally quite hard: Even showing
that a particular Cauchy sequence converges may not be an easy matter because one
has to identify which point it converges to, let alone doing this for arbitrary Cauchy

4.1 Completeness
45
sequences. But once a space is shown to be complete, one need not go through the
same proof process to show that a subspace or a product is complete:
Proposition 4.7
Let X, Y be complete metric spaces. Then,
(i) A subset F ⊆X is complete ⇐F is closed in X,
(ii) X × Y is complete.
Proof (i) Let F ⊆X be complete, i.e., any Cauchy sequence in F converges to a
limit in F. Let x ⇒¯F, with a sequence xn ∞x, xn ⇒F (Proposition 3.4). Since
convergent sequences are Cauchy and F is complete, x must be in F. Thus F = ¯F
is closed. The completeness of X has not been used, so in fact a complete subspace
of any metric space is closed.
Conversely, let F be a closed set in X and let (xn) be a Cauchy sequence in F.
Then (xn) is a Cauchy sequence in X, which is complete. Therefore xn ∞x for
some x ⇒X. In fact, x ⇒¯F = F. Thus any Cauchy sequence of F converges in F.
(ii) Let
 xn
yn

be a Cauchy sequence in X × Y. Recall that
d
 xn
yn

,
 xm
ym

:= dX(xn, xm) + dY (yn, ym) ⩾dX(xn, xm).
Since the left-hand sequence converges to 0 as n, m ∞→, we get dX(xn, xm) ∞0,
so that the sequence (xn) is Cauchy in the complete space X. It therefore con-
verges xn ∞x ⇒X. By similar reasoning, yn ∞y ⇒Y. Consequently,
d
 xn
yn

,
 x
y

= dX(xn, x) + dY (yn, y) ∞0 as n ∞→, which is equiva-
lent to
 xn
yn

∞
 x
y

in X × Y.
∈⊆
Examples 4.8
1. The completion of a subset A in a complete metric space X is ¯A.
Proof The completion Y of A must satisfy two criteria: Y must be complete, and
A must be dense in Y. Now, ¯A is closed in X, so is complete, and A is dense in
¯A (by deﬁnition).
2. Two metric spaces may be homeomorphic yet one space be complete and the
other not. For example, R is homeomorphic to ]0, 1[ (Exercise 3.12(18)), but the
latter is not closed in R.

46
4
Completeness and Separability
3. Let f : X ∞Y be a continuous function. If it can be extended to the completions
as a continuous function ˜f : ˜X ∞˜Y, then this extension is unique.
Proof Any x ⇒˜X has a sequence (an) in X converging to it (Proposition 3.4).
As ˜f is continuous, we ﬁnd that ˜f (x) is uniquely determined by
˜f (x) = lim
n∞→
˜f (an) = lim
n∞→f (an).
4. But not every continuous function f : X ∞Y can be extended continuously to
the completions ˜f : X ∞Y. For example, the continuous function f (x) := 1/x
on ]0, →[ cannot be extended continuously to [0, →[.
5. (Cantor) The completion of Q to R has come at a price: R is not countable. Prove
this by taking the binary expansion of a list of real numbers in [0, 1], arranged in
an inﬁnite array, and creating a new number from the diagonal that is different
from all of them. The next theorem is a strong generalization of this statement.
Theorem 4.9 Baire’s category theorem
A complete metric space cannot be covered by a countable number of
nowhere-dense sets.
Proof Suppose that the metric space X = →
n=1 An, where An are nowhere dense.
We are going to create a nested sequence of balls whose centers form a non-
convergent Cauchy sequence, as follows: To start with, ¯A1 ∗= X so its exterior
contains a ball Br1(x1) ⊆( ¯A1)c. Now ¯A2 contains no balls, so the open set
( ¯A2)c ∩Br1(x1) is non-empty and there is a ball Br2(x2) ⊆( ¯A2)c ∩Br1(x1).
¯A n +1
¯A n
x n
x n +1
Continuing like this, we can ﬁnd a sequence of
points (using the Axiom of Choice)
xn+1 ⇒Brn+1(xn+1) ⊆( ¯An+1)c ∩Brn(xn).
Moreover at each stage, rn can be chosen small
enough that
rn ∞0 (e.g.rn ⩽1/n),
Brn+1[xn+1] ⊆Brn(xn) (e.g.rn+1 < rn −d(xn, xn+1)).
Thus (xn) is a Cauchy sequence (Example 4.3(4)).
Now suppose that xn ∞x. For all m > n we have xm ⇒Brn+1(xn+1) and taking
the limit xm ∞x we ﬁnd x ⇒Brn+1[xn+1] ⊆Brn(xn). Since this holds for any n we
obtain

4.1 Completeness
47
Ren´e-Louis Baire (1874–1932), after graduating from Paris
around 1894, tackled the problem of convergence and limits of
functions, namely that no space of functions then known was
“closed” under pointwise convergence. Progress on this issue
was made by his colleague Borel in the direction of measurable
sets.
Fig. 4.1 Baire
x ⇒
	
n
Brn(xn) ⊆
	
n
( ¯An)c =

 
n
¯An
c
⊆

 
n
An
c
= Xc = ∅
a contradiction. Having constructed a non-convergent Cauchy sequence, X must be
incomplete.
∈⊆
Exercises 4.10
1. Any sequence in Q of the type (3.1, 3.14, 3.141, 3.1415, . . .) is Cauchy.
2. The sequences (1, 2, 3, . . .) and (1, −1, 1, −1, . . .) are not Cauchy.
3. * Try to prove that the sequence deﬁned by a0 := 1, an+1 := an
2 + 1
an is Cauchy.
4. If a sequence (xn), chosen from a ﬁnite set of points, e.g. (x, y, x, x, y, . . .), is
Cauchy then it must eventually repeat (x0, . . . , xN, xN, . . .). (Hint: Generalize
Exercise 2.)
5. ▶If d(xn+1, xn) < acn with c < 1 then xn is Cauchy. But a sequence which
decreases at the rate d(xn+1, xn) ⩽1/n need not be Cauchy. For example,
use the principle of induction to show that the example in Exercise 3 satisﬁes
|an+1 −an| ⩽( 1
2)n+1.
The following give sufﬁcient conditions for Cauchy sequences:
(a) If d(xn+1, xn) ⩽c d(xn, xn−1) with c < 1, then d(xn, xm) ⩽acn,
(b) If d(xn+1, xn) ⩽c d(xn, xn−1)2 with c d(x1, x0) < 1 then d(xn, xm) ⩽
c−1b2n,
for n ⩽m and appropriate constants a, b.
6. If (xn), (yn) are Cauchy sequences in X, then so is dn := d(xn, yn) in R.
7. ▶A continuous function need not map Cauchy sequences to Cauchy sequences.
8. If xn ∞x and yn ∞x, then (xn), (yn) are asymptotic.
9. ∪n and ∪n + 1 are asymptotic divergent sequences in R.
10. ▶A subsequence of a Cauchy sequence is itself Cauchy, and if it converges so
does its parent sequence.

48
4
Completeness and Separability
11. If (xn) is a Cauchy sequence, and the set of values {xn} has a limit point x, then
xn ∞x.
12. The completion of ]0, 1[ and of [0, 1[ is [0, 1]. Any Cauchy sequence in the Can-
tor set C must converge in C. However a Cauchy sequence of rational numbers
need not converge to a rational number because Q is not closed in R.
13. ▶RN := R × · · · × R and C are complete.
14. Is N complete? Any discrete metric space is complete.
15. (Cantor) We have already seen that the centers of a nested sequence of balls with
rn ∞0 form a Cauchy sequence (Example 4.3(4)). Show, furthermore, that in
a complete metric space, 
n Brn[xn] = {limn∞→xn}.
16. The only functions f : Q ∞Q satisfying f (x + y) = f (x)+ f (y) are f : x ⊂∞
αx. Deduce that the only continuous functions ˜f : R ∞R with this property
are of the same type.
17. * The completion of X is essentially unique, in the sense that any two such
completions (such as the one deﬁned in the theorem) are homeomorphic to each
other.
18. The Cantor set is complete and nowhere dense in R; why doesn’t this contradict
Baire’s theorem?
4.2 Uniformly Continuous Maps
We have seen that a continuous function need not preserve completeness, or even
Cauchysequences.Ifoneanalyzestherootoftheproblem,oneﬁndsthatitsresolution
lies in the following strengthening of continuity:
Deﬁnition 4.11
A function f : X ∞Y is said to be uniformly continuous when
√∂> 0, ♦γ > 0, √x ⇒X,
f Bγ(x) ⊆B∂( f (x)).
The difference from continuity is that, here, γ is independent of x.
Easy Consequences
1. Uniformly continuous functions are continuous.
2. But not every continuous map is uniformly so; an example is f (x) := 1/x on
]0, →[.

4.2 Uniformly Continuous Maps
49
3. ▶The composition of uniformly continuous maps is again uniformly continuous.
Proof √∂> 0, ♦γ, γ⇔> 0, √x, g( f (Bγ(x))) ⊆g(Bγ⇔( f (x))) ⊆B∂(g( f (x))).
The key properties of uniformly continuous maps are the following two proposi-
tions:
Proposition 4.12
A uniformly continuous function maps any Cauchy sequence to a Cauchy
sequence.
Proof By deﬁnition f : X ∞Y is uniformly continuous when
√∂> 0, ♦γ > 0, √x, x⇔,
dX(x, x⇔) < γ ∃dY ( f (x), f (x⇔)) < ∂.
In particular, for a Cauchy sequence (xn) in X, with this γ,
♦N,
n, m > N ∃dX(xn, xm) < γ
∃dY ( f (xn), f (xm)) < ∂,
proving that ( f (xn)) is a Cauchy sequence in Y.
∈⊆
More generally, the same proof shows that a function f : X ∞Y is uniformly
continuous if, and only if, it maps any asymptotic sequences (an), (bn) in X to
asymptotic sequences ( f (an)), ( f (bn)) in Y.
Theorem 4.13
Every uniformly continuous function f : X ∞Y has a unique uniformly
continuous extension to the completions ˜f : X ∞Y.
Proof In order not to complicate matters unnecessarily, let us suppose that X and
Y are dense subsets of X and Y respectively, instead of being embedded in them.
Nothing is lost this way, except quite a few extra symbols!
Let xn ∞x ⇒X, with xn ⇒X. The sequence f (xn) is Cauchy in Y by the
previous proposition, so must converge to some element y ⇒Y. Furthermore, if
an ∞x as well (an ⇒X), then (xn) and (an) are asymptotic (Example 4.10(8))
forcing f (xn) and f (an) to be asymptotic in Y, hence f (an) ∞y. This allows us
to deﬁne ˜f (x) := y without ambiguity. Moreover, this choice is imperative and ˜f is
unique, if it is to be continuous.

50
4
Completeness and Separability
The uniform continuity of ˜f follows from that of f . For any ∂> 0, there is a
γ > 0 for which
√a, b ⇒X,
d(a, b) < γ ∃d( f (a), f (b)) < ∂.
Let x, x⇔⇒X with d(x, x⇔) < γ, let an ∞x, bn ∞x⇔with an, bn ⇒X and, by the
above, f (an) ∞˜f (x), f (bn) ∞˜f (x⇔). Among these terms, we can ﬁnd a close to
x and b close to y to within r := (γ −d(x, x⇔))/2 < γ, while also f (a) is close to
˜f (x) and f (b) is close to ˜f (x⇔) to within ∂. Then
d(a, b) ⩽d(a, x) + d(x, x⇔) + d(x⇔, b) < 2r + d(x, x⇔) = γ
∃d( ˜f (x), ˜f (x⇔)) ⩽d( ˜f (x), f (a)) + d( f (a), f (b)) + d( f (b), ˜f (x⇔)) < 3∂.
∈⊆
The following are easily shown to be uniformly continuous functions:
Deﬁnition 4.14
A function f : X ∞Y is called a Lipschitz map when
♦c > 0, √x, x⇔⇒X,
dY ( f (x), f (x⇔)) ⩽c dX(x, x⇔).
Furthermore, it is called
an equivalence (or bi-Lipschitz) when f is bijective and both f and f −1 are
Lipschitz,
a contraction when it is Lipschitz with constant c < 1,
an isometry, and X, Y are said to be isometric, when f preserves distances,
i.e.,
√x, x⇔⇒X,
dY ( f (x), f (x⇔)) = dX(x, x⇔).
Examples 4.15
1. Any f : [a, b] ∞R with continuous derivative is Lipschitz.
Proof As f ⇔is continuous, it is bounded on [a, b], say | f ⇔(x)| ⩽c. The result
then follows from the mean value theorem,
f (x) −f (x⇔) = f ⇔(ξ)(x −x⇔),
♦ξ ⇒]0, 1[.
2. To show f : R2 ∞R2 is Lipschitz, where f = ( f1, f2), it is enough to show
that

4.2 Uniformly Continuous Maps
51
| fi(x1, y1) −fi(x2, y2)| ⩽c(|x1 −x2| + |y1 −y2|),
i = 1, 2
for then (using (a + b)2 ⩽2(a2 + b2) for a, b ⇒R)

 f1(x1, y1) −f1(x2, y2)
f2(x1, y1) −f2(x2, y2)
  ⩽| f1(x1, y1) −f1(x2, y2)| + | f2(x1, y1) −f2(x2, y2)|
⩽2c(|x1 −x2| + |y1 −y2|)
⩽2c
∪
2
x1−x2
y1−y2
.
3. ▶Lipschitz maps are uniformly continuous, since for any ∂> 0, we can let
γ := ∂/2cindependentof x toobtaind(x, x⇔) < γ ∃d( f (x), f (x⇔)) ⩽cγ < ∂.
4. But not every uniformly continuous function is Lipschitz. For example, ∪x on
[0, 1] is uniformly continuous (show!); were it also Lipschitz, it would satisfy
|∪x −
∪
0| ⩽c|x −0| which leads to ∪x ⩾1/c.
The next theorem is one of the important unifying principles of mathematics. It
has applications in such disparate ﬁelds as differential equations, numerical analysis,
and fractals.
Theorem 4.16 The Banach ﬁxed point theorem
Let X ∗= ∅be a complete metric space. Then every contraction map
f : X ∞X has a unique ﬁxed point x = f (x), and the iteration xn+1 :=
f (xn) converges to it for any x0.
Proof Consider the iteration xn+1 := f (xn) starting with any x0 in X. Note that
d(xn+1, xn) = d( f (xn), f (xn−1)) ⩽c d(xn, xn−1).
Hence, by induction on n,
d(xn+1, xn) ⩽cnd(x1, x0),
so (xn) is Cauchy since c < 1 (Exercise 4.10(5)). As X is complete, xn converges
to, say, x, and by continuity of f ,
f (x) = f ( lim
n∞→xn) = lim
n∞→f (xn) = lim
n∞→xn+1 = x.
Moreover, the rate of convergence is given at least by d(x, xn) ⩽
cn
1−cd(x1, x0).

52
4
Completeness and Separability
Suppose there are two ﬁxed points x = f (x)
and y = f (y); then
d(x, y) = d( f (x), f (y)) ⩽c d(x, y)
implying d(x, y) = 0 since c < 1.
∈⊆
Exercises 4.17
1. Show that
(a)
f : [a, b] ∞R, f (x) := x + 1/x, is a contraction when a > 2−1
2 ;
(b)
f : [0, 1]2 ∞R2, f (x, y) :=
2 −xy
x2 −y

is Lipschitz.
2. The composition of two Lipschitz maps is Lipschitz.
3. ▶A Lipschitz map (with constant c) sends the ball Br(a) into the ball Bcr( f (a)).
4. Isometries are necessarily 1–1. Onto isometric maps are equivalences, and the
latter are homeomorphisms.
5. ▶Two metric spaces are said to be equivalent when there is an equivalence
map between them. Equivalent metric spaces must be both complete or both
incomplete.
6. ▶If a space has two distances, the inequality d1(x, y) ⩽c d2(x, y), where
c > 0, states that the identity map is Lipschitz. In the same vein, two distances
are equivalent when there are c, c⇔> 0 such that c⇔d2(x, y) ⩽d1(x, y) ⩽
c d2(x, y). Show that two equivalent distances have exactly the same Cauchy
sequences.
7. The unit circle has two natural distance functions, (i) the arclength θ and (ii) the
Euclidean distance 2 sin(θ/2), where θ is the angle between two points (⩽π).
Prove that the two are equivalent by ﬁrst showing
2θ/π ⩽sin θ ⩽θ,
0 ⩽θ ⩽π/2.
8. The distances D1 and D→for X × Y (Example 2.2(6)) are equivalent.
9. The ﬁxed point theorem can be generalized to the case when f : Br[x0] ∞X
is a contraction map, as long as the starting point satisﬁes d(x0, x1) < (1 −c)r.
Use the triangle inequality to show that xn remain in Br[x0].
10. The classic example of an iteration converging to a ﬁxed point is that provided
by a continuously differentiable function f : R ∞R with | f ⇔(x)| < 1; it is a
contraction map in a neighborhood of x.

4.2 Uniformly Continuous Maps
53
11. If f : R ∞R is a contraction with Lipschitz constant c < 1, then f (x) = x can
also be solved by iterating xn+1 := F(xn) where F(x) := x −δ(x −f (x)),
0 < δ < 2/(c + 1). Hence ﬁnd an approximate solution of x = sin x + 1 near
to x = π; experiment by choosing different values of δ and compare with the
iteration xn+1 := f (xn).
4.3 Separable Spaces
Completeness is a “nice” property that a metric can have. A different type of property
of a metric space is whether it is, in a sense, “computable” or “constructive”. Starting
from the simplest, and speaking non-technically, we ﬁnd:
Finite metric spaces
There are a ﬁnite number of possible distances to
compute.
Countable metric spaces
With an inﬁnite number of points, an algorithm may
still calculate distances precisely, but it may take
longer and longer to do so.
Separable metric spaces
Points can be approximated by one of a countable
number of points; any distance can be evaluated, not
precisely, but to any accuracy.
Non-separable metric spaces
There may be no algorithm that ﬁnds the distance
between two generic points, even approximately.
Non-separable metric spaces are, in a sense, too large, while countable metric
spaces leave out most spaces of interest.
Deﬁnition 4.18
A metric space is separable when it contains a countable dense subset,
♦A ⊆X, Acountable and ¯A = X.
Examples 4.19
1. Countable metric spaces, such as N, Z, Q, are obviously separable.
2. ▶R is separable because the countable subset Q is dense in it. By the next
proposition, C and RN are also separable.1
1 There is a catch here: The metric used in the proposition is not the Euclidean one. But
the inequalities used there remain valid for the Euclidean metric,

dX(an, x)2 + dY (bn, y)2 <

∂2/4 + ∂2/4 < ∂.

54
4
Completeness and Separability
Proposition 4.20
Any subset of a separable metric space is separable.
The product of two separable spaces is separable.
The image of a separable space under a continuous map is separable.
Proof (i) Let Y ⊆X and ¯A = X, with A = {an : n ⇒N} countable. For each
an, let Yn,m := {y ⇒Y : d(an, y) < 1/m}, and pick a representative point from
each, yn,m ⇒Yn,m, whenever the set is non-empty. This array of points is certainly
countable, and we now show that it is dense in Y.
Fix 0 < ∂<
1
2; any y ⇒Y can be approximated by some an ⇒A with
d(an, y) < ∂. Pick the smallest integer m such that m > 1/2∂; then m −1 ⩽1/2∂,
so m ⩽1/∂; therefore ∂⩽1/m < 2∂. Then y ⇒Yn,m ∗= ∅, so that there must be a
representative yn,m with d(an, yn,m) < 1/m < 2∂. Combining the two inequalities,
we get
d(yn,m, y) ⩽d(yn,m, an) + d(an, y) < 3∂.
(ii) Let {a1, a2, . . .} be dense in X, and {b1, b2, . . .} dense in Y. Then for any ∂> 0
and any pair
 x
y

⇒X×Y, x can be approximated by some an such that dX(an, x) <
∂/2, and y by some bm with dY (bm, y) < ∂/2; then
d
an
bm

,
 x
y

= dX(an, x) + dY (bm, y) < ∂
shows that the countable set of points
an
bm

(n, m ⇒N) is dense in X × Y.
(iii) Let f : X ∞Y be continuous and let A be countable and dense in X. Then fA
is countable because the number of elements of a set cannot increase by a mapping.
Moreover, as f is continuous, fA is dense in fX (Example 3.8(3)), and fX is separable.
∈⊆
Exercises 4.21
1. A metric space X is separable when there is a countable number of points an such
that the set of balls B∂(an) covers X for any ∂.
2. * In a separable space, we can do with a countable number of balls (with say
rational radii), in the sense that every open set is a countable union of some of
these. It then follows that every cover of the space using open sets has a countable
subcover.
3. The union of a (countable) list of separable subsets is separable.

4.3 Separable Spaces
55
4. ▶If there are an uncountable number of disjoint balls, then the space is non-
separable, e.g. an uncountable set with the discrete metric is non-separable. We
shall meet some non-trivial examples of non-separable metric spaces later on
(Theorem 9.1).
Remarks 4.22
1. * The proof of Baire’s theorem can be modiﬁed to show that the countable union
of closed nowhere dense sets in a complete metric space X is nowhere dense in
X.
2. Note that d( f (x), f (y)) < d(x, y) does not necessarily give a contraction map.
For example, f (x) := 2/(
∪
x2 + 4−x). In this case, the iteration xn+1 := f (xn)
may satisfy d(xn+1, xn) ∞0 but need not be a Cauchy sequence.
3. The reader has most probably seen images of
fractals; many of these are the ﬁxed ‘point’,
or attractor, of a contraction on the space of
shapes (Example 2.2(4)) (see [19]).
4. The Banach ﬁxed point theorem is also valid when f N := f ◦· · · ◦f , rather
than f , is a contraction map; in this case the convergence is “cyclic”.

Chapter 5
Connectedness
5.1 Connected Sets
We have an intuitive notion of what it means for a shape to be in one piece. The
following deﬁnition makes this idea precise:
Deﬁnition 5.1
A subset C of a metric space is disconnected when it can be divided into (at
least) two disjoint non-empty subsets C = A ∞B such that each subset is
covered exclusively by an open set, i.e.,
A √U,
B ∗U = ∅,
U open,
B √V,
A ∗V = ∅,
V open.
U
V
A
B
Otherwise a set is called connected.
Examples 5.2
1. Single points are always connected because they cannot be split into two non-
empty sets. Similarly the empty set is connected.
2. ▶Any subset of Z (or any discrete metric space) is disconnected except the
single points and the empty set. Metric spaces with this property are called totally
disconnected.
Proof Let C contain more than one point, say a and b. Take A = U := { a }
and B = V := C∖{ a } ⇒= ∅. Then U and V are open (any subset is open) and
respectively contain A and B exclusively.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_5,
57
© Springer International Publishing Switzerland 2014

58
5
Connectedness
Kazimierz Kuratowski (1896–1980 Poland) rewrote much of
Hausdorﬀ’s theory in 1921, introducing his closure axioms and
connectedness. Similarly Aleksandrov and Urysohn, and later
Tykhonov, in Moscow, built upon Hausdorﬀ’s work with com-
pactness.
Fig. 5.1 Kuratowski
3. ▶A set A is connected when every continuous function f : A →{ 0, 1 } ♦Z is
constant. Otherwise the open sets f −1{ 0 } and f −1{ 1 } cover and disconnect A.
Proposition 5.3
A set C is connected ∃every non-trivial subset of C has a non-empty
boundary in C that is,
∅⇒= A ♦C ∀∂C A ⇒= ∅.
Here, ∂C A = { x ∈C : ⊆δ > 0, ∃a ∈A ∗C, ∃b ∈C∖A, d(a, x) < δ,
d(b, x) < δ }.
Proof Let ∅⇒= A ♦C be without a boundary in C. Then all the points of C are
either interior points or exterior points of A; thus A and B := C∖A are open in
C. But then there are open sets U, V in X, with A = U ∗C and B = V ∗C
(Example 2.12(3)), and
U ∗B = U ∗(C∖A) = U ∗C ∗Ac = ∅
(similarly V ∗A = ∅), so C = A ∞(C∖A) = A ∞B is disconnected.
Conversely, if C is disconnected, then C = A ∞B, with A √U, B √V , both
non-empty, and U, V open sets in X with A∗V = ∅= B ∗U. For any point a ∈A,
a ∈Br(a) √U; hence
a ∈{ x ∈C : d(x, a) < r } = Br(a) ∗C √U ∗C = A
shows that A is open in C. Similarly B = C∖A is open in C, hence A is closed in
C. This leaves A without a boundary in C.
⇐⊂

5.1 Connected Sets
59
Theorem 5.4
The connected subsets of R are precisely the intervals.
Proof Every non-trivial subset of an interval I √R has a boundary point: Let A
be a non-trivial subset of I; that A is non-trivial means that there exist a0 ∈A and
b0 ∈I∖A. We can assume a0 < b0, otherwise switch the roles of A and I∖A in
what follows.
Divide the interval [a0, b0] into halves, [a0, c]
and [c, b0], where c := (a0+b0)/2 is the midpoint.
If c ∈A let [a1, b1] := [c, b0], otherwise if c ∈Ac
let [a1, b1] := [a0, c]. Continue taking midpoints
to get a nested sequence of intervals [an, bn] in I,
with an ∈A, bn ∈I∖A.
A
a0
b0
a1
b1
a2
b2
a3
b3
By the bisection property (Example 4.3(3)), the sequences (an) and (bn) are
Cauchy and asymptotic, and since R is complete, they converge an →a and bn →a.
Theconsequenceisthat,insideanyopenneighborhood Bδ(a),therearepointsan ∈A
and bn ∈I∖A, making a a boundary point of A. From the preceding proposition,
this translates as “every interval is connected”.
Every connected subset C
of
R
has the interval property a, b ∈C
∀
[a, b] √C: Let C be a connected set, and let a, b ∈C (say, a < b). Any x ∈[a, b]
which is not in C would disconnect C using the disjoint open sets ]−⇔, x[ and
]x, ⇔[.
Every subset of
R with the interval property is an interval: Let A have the
interval property. If A ⇒= ∅, say x ∈A, and has an upper bound, then it has a least
upper bound b. The interval [x, b[ is a subset of A because there are points of A
arbitrarily close to b. Similarly if a is the greatest lower bound then ]a, x] √A.
Going through all the possibilities of whether A has upper bounds or lower bounds
or none, and whether these belong to A or not, results in all the possible cases of
intervals. For example, if it contains its least upper bound b but has no lower bound,
then [x, b] √A for any x < b, so that A = ]−⇔, b].
⇐⊂
By contrast, the connected sets in other metric spaces may be very difﬁcult to
describe and imagine. Even in R2, there are inﬁnite connected sets such that when
a single point is removed, the remaining set is totally disconnected! (For further
information search for “Cantor’s teepee”.) Connectedness is an important intrinsic
property that a set may have: it is preserved by any continuous function. Even though
the codomain space may be very different from the domain, a connected set remains
in ‘one piece’.

60
5
Connectedness
Proposition 5.5
Continuous functions map connected sets to connected sets,
f : X →Y continuous and C √X is connected ∀f C is connected.
Proof Let C be a subset of X, and suppose f C is disconnected into the non-empty
disjoint sets A and B covered exclusively by the open sets U and V , that is,
f C = A ∞B √U ∞V,
U ∗B = ∅= V ∗A.
Then,
C = f −1A ∞f −1B √f −1U ∞f −1V,
f −1U ∗f −1B = ∅= f −1V ∗f −1A.
f
f −1 A
f −1 B
A
B
f −1 U
f −1 V
U
V
X
Y
Moreover f −1A and f −1B are non-empty and disjoint, and f −1U and f −1V are
open sets (Theorem 3.7). Hence f C disconnected implies C is disconnected.
⇐⊂
Almost surprisingly, this simple proposition is the generalization of the classical
“Intermediate Value Theorem” of Bolzano and Weierstraß. In effect, IVT has been
dissected into this abstract, but transparent, statement and the previous one that
intervals are connected.
Proposition 5.6 Intermediate Value Theorem
Let C be a connected space, and f : C →R a continuous function. For
any c with f (a) < c < f (b) there exists an x ∈C such that f (x) = c.
Proof f C is connected in R and so must be an interval. By the interval property,
f (a), f (b) ∈f C ∀c ∈f C, so c = f (x) for some x ∈C.
⇐⊂

5.1 Connected Sets
61
Exercises 5.7
1. Any two distinct points of a metric space are disconnected. More generally,
(a) any set of N points (N ⩾2), (b) the union of two disjoint closed sets, are
disconnected.
2. The space of rational numbers Q is disconnected, e.g. using the open sets
]−⇔, ∩2[ ∗Q and ]∩2, ⇔[ ∗Q. In fact Q is totally disconnected.
3. Suppose that there is an x ∈X and an r > 0 such that d(x, y) ⇒= r for all y ∈X,
but there are points y with d(x, y) > r. Show that X is disconnected.
4. ▶An open set (such as the whole metric space) is disconnected precisely when
it consists of (at least) two disjoint open subsets. Find a connected set whose
interior is disconnected.
5. *Any two disjoint non-empty closed sets A and B are completely separated in
the sense that there are disjoint open sets A √U, B √V , U ∗V = ∅. (Hint:
use Exercise 3.12(17).)
6. ▶A path is a continuous function I →X where I is an interval in R. Its
image is connected. Hence show that the parametric curves of geometry, such as
straight line segments, circles, ellipses, parabolas, and branches of hyperbolas
in R2, are connected.
7. (a) The function f (x) := xn is continuous on R, for n = 0, 1, . . .. Show that,
for any ﬁxed n ⩾1, xn can be made arbitrarily large. Let y be a positive real
number; use the intermediate value theorem to show that
n∩y exists. More
generally every real monic polynomial xn + · · · + a1x + a0 (n ⩾1), where
a0 is negative or when n is odd, has a root.
(b) Every continuous function f : [0, 1] →[0, 1] has a ﬁxed point. (Hint:
consider f (x) −x.)
8. If f : [0, 1]2 →R is continuous and f (a) < c < f (b) then there is an x ∈
[0, 1]2 such that c = f (x) (assuming [0, 1]2 is connected).
9. Suppose X is connected and f : X →R is continuous and locally constant, that
is, every x ∈X has a neighborhood taking the value f (x). Then f is constant
on X. (Hint: Show f −1 f (a) is closed and open in X.)
10. Q has non-interval subsets with the interval property (e.g. [0,
∩
2[ ∗Q).
11. Use the intermediate value theorem to show that a 1–1 continuous function on
[a, b] must be increasing or decreasing
x ⩽y ∀f (x) ⩽f (y)
or
x ⩽y ∀f (x) ⩾f (y).

62
5
Connectedness
5.2 Components
It seems intuitively clear that every space is the disjoint union of connected subsets.
To make this rigorous, let us present some more propositions that go some way in
helping us show whether a set is connected, especially the principle that whenever
connected sets intersect, their union is connected; this allows us to build connected
sets from smaller ones.
Proposition 5.8
If C is connected then so is C with some boundary points (in particular ¯C).
Proof Let D be C with the addition of some boundary points. Suppose it separates
as D = A ∞B each covered exclusively by open sets U and V . Then C would also
split up in the same way, unless C √U say. This cannot be the case, for let x ∈B
be a boundary point covered by V . Then there is a ball Br(x) √V containing points
of C, a contradiction. Thus D disconnected implies C is disconnected.
⇐⊂
Theorem 5.9
If Ai, B are connected sets and ⊆i Ai ∗B ⇒= ∅then B ∞
i Ai is connected.
If An are connected for n = 1, 2, . . ., and An ∗An+1 ⇒= ∅then 
n An is
connected.
B
Ai
An
Proof (i) Suppose the union B ∞
i Ai is disconnected and splits up into two parts
covered exclusively by open sets U and V . Then B would split up into the two parts
B ∗U and B ∗V were these to contain elements. But as B is known to be connected,
one of these must be empty, say B ∗U = ∅. For any other A := Ai that is partly
covered by U (and there must be at least one) we get A ∗V = ∅and A √U, for
the same reason. But then A ∗B √U ∗B = ∅, contradicting the assumptions.
In particular, note that if A, B are connected and A ∗B ⇒= ∅, then A ∞B is also
connected. But the statement is true even for an uncountable number of Ai.

5.2 Components
63
(ii) If CN := N
n=1 An is connected, then CN+1 = CN ∞AN+1 is also connected by
the ﬁrst part of the theorem, since CN ∗AN+1 ⇒= ∅. By induction CN is connected
for all N. As A1 √CN for all N, it follows that ⇔
N=1 CN = ⇔
n=1 An is also
connected.
⇐⊂
The converses of both these statements are false, but the following holds:
Proposition 5.10
Given non-empty connected sets A, B,
A ∞B is connected ∃∃x ∈A ∞B, { x } ∞A and{ x } ∞B are connected.
Proof Suppose no point x ∈A makes { x } ∞B connected. That is, for each x ∈A
there are two open sets which separate { x }∞B. Call the set which contains x, Ux, and
the other one Vx. They would also separate B unless B √Vx, and Ux ∗B = ∅. So

x Ux is an open set containing A but disjoint from B. If the same were to hold for
points in B, then there would be an open set containing B but disjoint from A, making
A ∞B disconnected. The converse is a special case of the previous proposition.
⇐⊂
Theorem 5.11
A metric space partitions into disjoint closed maximal connected subsets,
called components. Any connected set is contained in a component.
By a maximal connected set is meant a connected set C such that any A ∪C
(A ⇒= C) is disconnected.
Proof The relation x ∼y, deﬁned by { x, y } √C for some connected set C, is
trivially symmetric; it is reﬂexive since { x, x } = { x } is connected, and it is transitive
because if x, y ∈C1 and y, z ∈C2 then x, z ∈C1 ∞C2, which is connected by
Theorem 5.9 as y ∈C1 ∗C2. Moreover, another way of writing the relation x ∼y
is as
y ∈

{ C connected : x ∈C },
so that the equivalence class [x] (called the component) of x is the union of all the
connected sets containing x. What this implies is that any connected set C that con-
tains x must be part of the component of x. In addition, the component is connected
by Theorem 5.9 and it is maximally so, as no strictly larger connected set containing
x can exist. In particular since [x] is connected it must be the case that [x] = [x] and
[x] is closed (Proposition 2.16).
⇐⊂

64
5
Connectedness
Exercises 5.12
1. Show that R2 is connected by considering the radial lines all intersecting the
origin.
2. ▶More generally, if there exists a path between any two points, then the metric
space is connected. (It is enough to ﬁnd a path between any point and a single
ﬁxed point; why?) Such a space is said to be path-connected.
3. The square [0, 1]2 and the half-plane ]a, ⇔[ × R are connected.
4. Any disk in R2 is path-connected. Do balls in a general metric space have to be
connected? Consider the space X := ]−⇔, −1[ ∞]1, ⇔[ and ﬁnd a ball in this
space which is not connected.
5. ▶If X, Y are connected spaces then so is X × Y.
6. The set R2∖{ x } is connected. But R∖{ x } is disconnected. Deduce that R and
R2 are not homeomorphic.
Using the same idea, show that [a, b], [a, b[ and ]a, b[ are not homeomorphic
to each other, and neither is a circle to a parabola.
7. A connected metric space, such as R, has one component, itself. At the other
extreme, in totally disconnected spaces, the components are the single points
{ a }, e.g. Q and Z.
8. If a subset of X has no boundary (so is closed and open) then it is the union of
components of X.
9. Components need not be open sets (e.g. in Q).
10. A metric space in which Br(x) is connected for any x and any r sufﬁciently
small is said to be locally connected. Show that for a locally connected space X,
(a) the components are open in X,
(b) any convergent sequence converges inside some component,
(c) if X is also separable, then the components are countable in number.

Chapter 6
Compactness
6.1 Bounded Sets
Deﬁnition 6.1
A set B is bounded when the distance between any two points in the set has
an upper bound,
diam
B
∞r > 0,
√x, y ∗B,
d(x, y) ⩽r.
The least such upper bound is called the diameter of the set:
diamB := sup
x,y∗B
d(x, y).
In everyday, but not very helpful, terms one can say that a bounded set does not
“reach to inﬁnity”, or even that it is “ﬁnite” in a geometric sense (the unit circle has
an inﬁnite number of points but is bounded in R2). The characteristic properties of
bounded sets are
Proposition 6.2
Any subset of a bounded set is bounded.
The union of a ﬁnite number of bounded sets is bounded.
Proof (i) Let B be a bounded set with d(x, y) ⩽r for any x, y ∗B. In particular
this holds for x, y in any subset A ⇒B, so A is bounded.
(ii) Given a ﬁnite number of bounded sets B1,…,BN, with diametersr1,…,rN, respec-
tively, let r := maxn rn. Pick a representative point from each set, an ∗Bn, and take
the maximum distance between any two,r := maxm,n d(am, an); it certainly exists
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_6,
65
© Springer International Publishing Switzerland 2014

66
6
Compactness
as there are only a ﬁnite number of such pairs. Now, for any two points x, y ∗
n Bn,
that is, x ∗Bi, y ∗B j, and using the triangle inequality twice,
am
an
rm
rn
d(x, y) ⩽d(x, ai) + d(ai, a j) + d(a j, y)
⩽ri +r + r j
⩽2r +r,
an upper bound for the distances between points in
N
n=1 Bn is found.
→♦
Examples 6.3
1. In any metric space, ﬁnite subsets are bounded. In N, only the ﬁnite subsets are
bounded (since d(a0, an) ⩽N for all n implies n ⩽N). Consequently, N, Q, R,
and C are all unbounded.
2. In a discrete metric space, every subset is bounded. A metric space may be “large”
(non-separable) yet be bounded.
3. ▶A set B is bounded ∃it is a subset of a ball,
∞r > 0, ∞a ∗X,
B ⇒Br(a).
Proof Balls (and their subsets) are obviously bounded,
√x, y ∗Br(a),
d(x, y) ⩽d(x, a) + d(y, a) < 2r.
Conversely, if a non-empty set is bounded by R > 0, pick any points a ∗X and
b ∗B to conclude x ∗Br(a):
√x ∗B,
d(x, a) ⩽d(x, b) + d(b, a) < R + 1 + d(b, a) =: r.
4. The set [0, 1[∀]2, 3[∈R is bounded because it can be covered by the ball B3(0),
or because it is the union of two bounded sets.
5. ▶Boundedness is not necessarily preserved by continuous functions: If B is
bounded and f is a continuous function, then fB need not be bounded. Worse, a
set may be bounded in one metric space X, but unbounded in a homeomorphic
copy Y.
For example, N with the standard metric is unbounded, but its homeomorphic
copy, N with the discrete metric, is bounded.

6.1 Bounded Sets
67
Exercises 6.4
1. The set [−1, 1[ is bounded in R with diameter 2; in fact diam[a, b[= b −a.
2. Show that if diam(A) ⩽r, diam(B) ⩽s, and assuming A ⊆B ̸= ∅then
diam(A ∀B) ⩽r + s.
3. Any closed ball Br[a] := {x : d(x, a) ⩽r} is bounded; hence the closure of a
bounded set is bounded.
4. ▶Cauchy sequences are bounded (Example 4.3(5)). So unbounded sequences
cannot possibly converge.
5. ▶Prove that Lipschitz functions map bounded sets to bounded sets (Exercise
4.17(3)). So equivalent metric spaces have corresponding bounded subsets.
6.2 Totally Bounded Sets
We have seen that boundedness is not an intrinsic property of a set, as it is not
necessarily preserved by continuous functions. Let us try to capture the “ﬁniteness”
of a set with another deﬁnition:
Deﬁnition 6.5
A subset B ⇒X is totally bounded when it can be covered by a ﬁnite number
of ∂-balls, however small their radii ∂,
√∂> 0, ∞N ∗N, ∞a1, . . . , aN ∗X,
B ⇒
N

n=1
B∂(an).
Easy Consequences
1. Any subset of a totally bounded set is totally bounded (the same ∂-cover of the
parent covers the subset).
2. A ﬁnite union of totally bounded sets is totally bounded (the ﬁnite collection of
∂-covers remains ﬁnite).
3. A totally bounded set is bounded (it is a subset of a ﬁnite number of bounded
balls).

68
6
Compactness
Examples 6.6
1. The interval [0, 1] is totally bounded in R because it can be covered by the balls
B∂(n∂) for n = 0, . . . , N, where 1/∂−1 < N ⩽1/∂.
2. Not all bounded sets are totally bounded. For example, in a discrete metric space,
any subset is bounded but only ﬁnite subsets are totally bounded (take ∂< 1).
3. ▶A totally bounded space X is separable.
Proof For each n = 1, 2, . . ., consider ﬁnite covers of X by balls B1/n(ai,n) and
let An := {ai,n} be the ﬁnite set of the centers, so A := ⇐
n=1 An is countable.
For any ∂> 0 and any point x ∗X, let n ⩾1/∂, then x is covered by some ball
B1/n(ai,n), i.e., d(x, ai,n) < ∂, thus ¯A = X.
4. The center points an of the deﬁnition may, without loss of generality, be
assumed to lie in B. Otherwise cover B with balls B∂/2(xn), and take repre-
sentative points an ∗B ⊆B∂/2(xn) whenever non-empty; then 
n B∂(an) ⊂
B ⊆
n B∂/2(xn) ⊂B.
Proposition 6.7
A uniformly continuous function maps totally bounded sets to totally
bounded sets.
Proof Let f : X ⇔Y be a uniformly continuous function,
√∂> 0, ∞δ > 0, √x ∗X,
f Bδ(x) ⇒B∂( f (x)).
Let A be a totally bounded subset of X, covered by a ﬁnite number of balls A ⇒
N
n=1 Bδ(xn). Then
f A ⇒
N

n=1
f Bδ(xn) ⇒
N

n=1
B∂( f (xn)).
→♦
A totally bounded set is geometrically ‘ﬁnite’, so an inﬁnite sequence of points
in a totally bounded set is caged in, so to speak, with nowhere to escape to:
Theorem 6.8
A set B is totally bounded ∃
Every sequence in B has a Cauchy subsequence.

6.2 Totally Bounded Sets
69
Proof Let the totally bounded set K be covered by a ﬁnite number of balls of radius
1, and let {x1, x2, . . .} be an inﬁnite subset of K. (If K is ﬁnite, a selected sequence
must take some value xi inﬁnitely often and so has a constant subsequence.) A ﬁnite
number of balls cannot cover an inﬁnite set of points, unless at least one of the balls,
B1(a1), has an inﬁnite number of these points, say {x1,1, x2,1, . . .}.
Now cover K with a ﬁnite number of 1
2-balls. For the same reason as above, at
least one of these balls, B1/2(a2) covers an inﬁnite number of points of {xn,1}, say
the new subset {x1,2, x2,2, . . .}. Continue this process forming covers of 1
m -balls and
inﬁnite subsets {xn,m} of B1/m(am). The sequence (xn,n) is Cauchy, since for m ⩽n,
both xm,m and xn,n are elements of the set {x1,m, x2,m, . . .}, and so d(xn,n, xm,m) <
2
m ⇔0 as n, m ⇔⇐.
For the converse, start with any a1 ∗A. If B∂(a1) covers A then there is a single-
element ∂-ball cover. If not, pick a2 in A but not in B∂(a1). Continue like this to
get a sequence of distinct points an ∗A with an ̸∗n−1
i=1 B∂(ai), all of which are
at least ∂distant from each other. This process cannot continue indeﬁnitely else we
get a sequence (an) whose points are not close to each other, and so has no Cauchy
subsequence. So after some N steps we must have A ⇒N
i=1 B∂(ai).
→♦
Exercises 6.9
1. ▶If X and Y are totally bounded metric spaces, then so is X × Y.
(Hint: If B∂(xn) (n = 1, . . . , N) cover X and B∂(ym) (m = 1, . . . , M) cover Y,
show that every point (x, y) ∗X ×Y lies in B2∂(xi, y j) for some i ⩽N, j ⩽M.)
2. ▶In RN (and CN), a set is bounded ∃it is totally bounded.
(Hint: Show that if B is a bounded set in RN, with a bound R > 0, then B is a
subset [−R, R]N, which is totally bounded by the previous exercise.)
3. The set of values of a Cauchy sequence is totally bounded.
4. The closure of a totally bounded set is totally bounded.
5. Let B ⇒Y ⇒X, then B is totally bounded in Y ∃it is totally bounded in X
(Example 2.12(3)).
6. Any bounded sequence in RN (or CN) contains a convergent subsequence. (Hint:
xn ∗[−R, R]N is totally bounded.)
7. A continuous function f : X ⇔Y, with X, Y complete metric spaces, maps
totally bounded subsets of X to totally bounded subsets of Y. (Hint: consider a
sequence in f B for a totally bounded set B ⇒X.)
6.3 Compact Sets
In the presence of completeness, continuous functions preserve totally bounded sets.
Alternatively, we can strengthen the deﬁnition of boundedness even further to a
property that is preserved by continuous functions; such a property is compactness,

70
6
Compactness
but it will emerge that compact sets are precisely the complete and totally bounded
subsets.
Deﬁnition 6.10
A set K is said to be compact when given any cover of balls (of possibly
unequal radii), there is a ﬁnite sub-collection of them that still cover the set (a
subcover),
K ⇒

i
B∂i (ai)
∩
∞i1, . . . , iN,
K ⇒
N

n=1
B∂in (ain).
Examples 6.11
1. Any ﬁnite set, including ∅, is compact.
2. The subset [0, 1[∈R is totally bounded but not compact. For example, the cover
using balls B1−1/n(0) for n = 2, . . . has no ﬁnite subcover. On the other hand,
we will soon see that the closed intervals [a, b] are compact.
3. ▶Compact metric spaces are totally bounded, and so bounded and separable
(consider the cover by all ∂-balls). Thus, R and N are not compact.
An equivalent formulation of compactness is the following. By an open cover is
meant a cover consisting of open sets, K ⇒
i Ai (Ai open subsets of X).
Proposition 6.12
A set is compact ∃any open cover of it has a ﬁnite subcover.
Proof Let open sets Ai cover a compact set, K ⇒
j A j. Each open set A j consists
of a union of balls. It follows that K is included in a union of balls. By the deﬁnition
of compactness, there is a ﬁnite number of these balls B∂1(a1), . . . , B∂N (aN) that still
cover the set K. Each of these balls is inside one of the open sets, say B∂i (ai) ⇒A ji ,
and
K ⇒
N

i=1
B∂i (ai) ⇒
N

i=1
A ji
as claimed.
Conversely, suppose K is such that any open cover of it has a ﬁnite subcover. This
holds in particular for a cover of (open) balls, so K is compact.
→♦

6.3 Compact Sets
71
We will soon strengthen the following proposition to show that compact sets are
complete, but the following proof is instructive, and remains valid in more general
topological spaces:
Proposition 6.13
Compact sets are closed.
Proof Let K be compact and x ∗X∖K. To show x is exterior to K, we need to
surround it by a ball outside K. We know that x can be separated from any y ∗K
by disjoint open balls Bry(x) and Bry(y) (Proposition 2.5). Since y ∗Bry(y), these
latter balls cover K. But K is compact, so there is a ﬁnite sub-collection of these
balls that still cover K,
K ⇒Br1(y1) ∀· · · ∀BrN (yN).
Now let r := min{r1, . . . ,rN}; then Br(x) ⊆K = ∅since
z ∗Br(x) ∩z ∗Bri (x) for i = 1, . . . , N
∩z ̸∗Br1(y1) ∀· · · ∀BrN (yN) ⊂K.
Therefore, x ∗Br(x) ⇒X∖K.
→♦
Proposition 6.14
A closed subset of a compact set is compact.
A ﬁnite union of compact sets is compact.
Proof (i) Let F be a closed subset of a compact set K, and let the open sets Ai cover
F; then
K ⇒F ∀(X∖F) ⇒

i
Ai ∀(X∖F).
The right-hand side is the union of open sets since X∖F is open when F is closed.
But K is compact and therefore a ﬁnite number of these open sets are enough to
cover it,
K ⇒
N

i=1
Ai ∀(X∖F),
so
F ⇒
N

i=1
Ai.
(ii) Let the open sets Ai cover the ﬁnite union of compact sets K1 ∀· · · ∀KN. Then
they cover each individual Kn, and a ﬁnite number will then sufﬁce in each case,

72
6
Compactness
Kn ⇒Rn
k=1 Aik. For n = 1, . . . , N, the collection of chosen Aik remains ﬁnite, and
together cover all the Kn.
→♦
Compactness is strong enough that it is preserved by continuous functions; it is
thus a truly intrinsic property of a set, as any homeomorphic copy of a compact set
must also be compact.
Proposition 6.15
Continuous functions map compact sets to compact sets,
f : K ⇒X ⇔Y continuous and K compact ∩f K compact.
Proof Let the sets Ai be an open cover for f K,
f K ⇒

i
Ai
From this can be deduced
K ⇒f −1 
i
Ai =

i
f −1 Ai.
But f −1 Ai are open sets since f is continuous (Theorem 3.7). Therefore the right-
hand side is an open cover of K. As K is compact, a ﬁnite number of these open sets
will do to cover it,
K ⇒
N

k=1
f −1Aik.
It follows that there is a ﬁnite subcover, f K ⇒N
k=1 Aik, as required to show f K
compact.
→♦
To summarize,
Continuous functions preserve compactness,
Uniformly continuous functions preserve total boundedness,
Lipschitz continuous functions preserve boundedness.
An immediate corollary is this statement from classical real analysis:

6.3 Compact Sets
73
Corollary 6.16
Let f : K ⇔R be a continuous function on a compact space K. Then its
image f K is bounded, and the function attains its bounds,
∞x0, x1 ∗K, √x ∗K,
f (x0) ⩽f (x) ⩽f (x1).
Proof The image f K is compact, and so bounded, f K ⇒BR(0), i.e., | f (x)| ⩽R for
all x ∗K. Moreover compact sets are closed and so contain their boundary points. In
particular f K contains inf f K and sup f K (Example 2.8(3)), i.e., inf f K = f (x0),
sup f K = f (x1) for some x0, x1 ∗K.
→♦
A property that holds locally, i.e., in a ball around any point, will often also hold in
a compact set by using a ﬁnite number of these balls. As an example of this, consider
a continuous function with compact domain. By the deﬁnition of continuity, any x
in the domain is surrounded by a small ball Bδx (x) on which the function varies by
at most a small ﬁxed amount ∂; on a compact domain, a ﬁnite number of these balls
and radii sufﬁce to cover the set, so a single δ can be chosen irrespective of x. More
formally,
Proposition 6.17
Any continuous function from a compact space to a metric space,
f : K ⇔Y, is uniformly continuous.
If, moreover, f is bijective, then f is a homeomorphism.
Proof (i) By continuity of f , every x ∗K has a δx for which f Bδx (x) ⇒B∂( f (x))
(Theorem 3.7). Since the balls Bδx/2(x) cover K, there is a ﬁnite subcover, from
which can be chosen the smallest value of δ. Let a, b ∗K be any points with
d(a, b) < δ/2. The point a is covered by a ball Bδx/2(x) from the ﬁnite list. Indeed,
Bδx (x) covers b too since
d(x, b) ⩽d(x, a) + d(a, b) < δx/2 + δ/2 ⩽δx.
As both a and b belong to Bδx (x), their images under f satisfy f (a), f (b) ∗
B∂( f (x)), so that
d( f (a), f (b)) ⩽d( f (a), f (x)) + d( f (x), f (b)) < 2∂.

74
6
Compactness
This inequality was achieved with one δ independently of a and b, so f is uniformly
continuous.
(ii) If f is continuous and onto, Y = f K is compact. But when in addition it is also
1–1, it preserves open sets: if A is open in K, then K∖A is closed, hence compact,
in K; this is mapped 1–1 to the closed compact set f (K∖A) = Y∖f A, implying
that f A is open in Y. This is precisely what is needed for f −1 to be continuous, and
thus for f to be a homeomorphism.
→♦
We are now ready for some concrete examples, starting with that of R, the simplest
non-trivial complete space.
Proposition 6.18 Heine-Borel’s theorem
The closed interval [a, b] is compact in R.
Proof Let 
i Ai ⊂[a, b] be an open cover of the closed interval. We seek to obtain
a contradiction by supposing there is no ﬁnite subcover. One of the two subintervals
[a, (a+b)/2] and [(a+b)/2, b] (and possibly both) does not admit a ﬁnite subcover:
call it [a1, b1]. Repeat this process of dividing, each time choosing a nested interval
[an, bn] of length (b −a)/2n which does not admit a ﬁnite subcover.
Now (an) and (bn) are asymptotic Cauchy sequences, which must therefore con-
verge to the same limit, say, an ⇔x and bn ⇔x (Proposition 4.2 and Theorem 4.5).
This limit x is in the set [a, b] (Proposition 3.4) and is therefore covered by some
open set Ai0. As an interior point of it, x can be surrounded by an ∂-ball (in this case,
an interval)
x ∗B∂(x) ⇒Ai0.
But an ⇔x and bn ⇔x imply that there is an N such that aN, bN ∗B∂(x), and
so [aN, bN] ⇒B∂(x) ⇒Ai0. This contradicts how [aN, bN] was chosen not to be
covered by a ﬁnite number of Ai’s, so there must have been a ﬁnite subcover to start
with.
→♦
The Heine-Borel theorem generalizes readily to arbitrary metric spaces.
Theorem 6.19
A set K is compact ∃K is complete and totally bounded.
Proof Compact sets are totally bounded: Let K be a compact set. For any ∂> 0,
cover K with the balls B∂(x) for all x ∗K. This open cover has a ﬁnite sub-cover.

6.3 Compact Sets
75
Compact sets are complete: Let (xn) be a Cauchy sequence which has no limit in
K, so that for each x ∗K,
∞∂> 0, √N, ∞n ⩾N,
d(xn, x) ⩾∂.
For this ∂(which may depend on x),
∞M,
n, m ⩾M ∩d(xn, xm) < ∂/2,
∴
∂⩽d(xn, x) ⩽d(xn, xm) + d(xm, x) < ∂/2 + d(xm, x),
∴
m ⩾M ∩d(xm, x) ⩾∂/2.
For m < M, the distances d(xm, x) take only a ﬁnite number of values. Hence, for
each x ∗K, there is a small enough ball Br(x)(x) which contains no points xn unless
xn = x. This gives an open cover of K, which must have a ﬁnite sub-cover. But this
implies that the sequence takes a ﬁnite set of values and so must eventually repeat
and converge (Exercise 4.10(4)). In any case, there must be a limit in K.
Complete and totally bounded sets are compact: Let K be a complete and totally
bounded set. Suppose it to be covered by open sets Vi, but that no ﬁnite number of
these open sets is enough to cover K. Since K is totally bounded,
K ⇒
N

i=1
B1(yi)
for some yi ∗K (Example 6.6(4)). If each of these balls were covered by a ﬁnite
number of the open sets Vi, then so would K. So at least one of these balls needs an
inﬁnite number of Vi’s to cover it; let us call this ball B1(x1).
Now consider B1(x1)⊆K, also totally bounded. Once again, it can be covered by
a ﬁnite number of balls of radius 1/2, one of which does not have a ﬁnite subcover,
say B1/2(x2). Repeat this process to get a nested sequence of balls B1/2n(xn), with
xn ∗K, none of which has a ﬁnite subcover. The sequence (xn) is Cauchy since
d(xn, xm) < 1/2n (for m > n), and K is complete, hence xn ⇔x in K.
But x is covered by some open set Vi0. Therefore there is an ∂> 0 such that
x ∗B∂(x) ⇒Vi0.
Moreover since 1/2n ⇔0 and xn ⇔x, an N can be found such that 1/2N < ∂/2
and d(xN, x) < ∂/2, so that for d(y, xN) < 1/2N,
d(y, x) ⩽d(y, xN) + d(xN, x) < ∂
i.e.,
B1/2N (xN) ⇒B∂(x) ⇒Vi0,
which contradicts the way that the balls B1/2n(xn) were chosen.
→♦

76
6
Compactness
Corollary 6.20
In a complete metric space, a subset K is compact
∃K is closed and totally bounded.
In RN, K is compact ∃K is closed and bounded.
Proof In a complete metric space, a subset is complete if, and only if, it is closed
(Proposition 4.7).
In the complete space RN, a set is totally bounded if, and only if, it is bounded
(Exercise 6.9(2)). Note carefully that this remains true whether the distance is Euclid-
ean, D1, or D⇐(Example 2.2(6)).
→♦
Theorem 6.21 Bolzano-Weierstraß property
In a metric space, a subset K is compact
∃every sequence in K has a subsequence that converges in K
∃every inﬁnite subset of K has a limit point in K.
Proof (i) A compact set is totally bounded, and so every sequence in it has a Cauchy
subsequence (Theorem 6.8). But compact metric spaces are also complete, implying
convergence of this subsequence in K.
(ii) Let A be an inﬁnite subset of K, and select a sequence of distinct terms a1, a2, . . .
in A. Assuming that every sequence in K has a convergent subsequence, then ani ⇔
a ∗K, as i ⇔⇐. For any ball B∂(a), there are an inﬁnite number of points
ani ∗B∂(a), making a a limit point of A (a can be equal to at most one of these
distinct points). Thus K satisﬁes the Bolzano-Weierstraß property that every inﬁnite
subset has a limit point in K.
(iii) Let K have the Bolzano-Weierstraß property, let (xn) be any sequence in K
and let A be the set of its values {x0, x1, x2, . . .}. If A is inﬁnite, then it has a limit
point x ∗K and so there is a convergent subsequence xn ⇔x with xn ∗A
(Proposition 3.4). Otherwise, if A is ﬁnite, one can pick a constant subsequence. In
either case there is a (Cauchy) convergent subsequence in K.
This shows, ﬁrstly, that K is totally bounded, and secondly, that every Cauchy
sequence in K converges in K (Exercise 4.10(10)), that is, K is complete.
→♦
Exercises 6.22
1. A compact set that consists of isolated points is ﬁnite.
2. In Z, and any discrete metric space, the compact subsets are ﬁnite.
3. Show that [0, 1] ⊆Q is closed and totally bounded in Q but not compact. (Hint:
ﬁrst show that [0,r[⊆Q is not compact when r is irrational.)

6.3 Compact Sets
77
4. (Cantor) Let Kn be a decreasing nested sequence of non-empty compact sets.
If 
n Kn = ∅then X∖Kn (n = 2, 3, . . .) form an open cover of K1. Deduce
that 
n Kn is compact and non-empty. Moreover, if diamKn ⇔0 then 
n Kn
consists of a single point.
5. The Cantor set is compact, totally disconnected, and has no isolated points
(Exercise 2.20(7)). (In fact, it is the only non-empty space with these proper-
ties, up to homeomorphism.)
6. The least distance between a compact set and a disjoint closed subset of a metric
space is strictly positive.
7. Suppose K isacompactsubsetofR2 whichliesinthehalf-plane{(x, y) : x > 0}.
Show that the open disks with centers (x + x−1, 0) and radii x > 1 cover the
half-plane, and deduce that K is enclosed by a circle that does not meet the
y-axis.
8. The circle S1 is compact; more generally, any continuous path [0, 1] ⇔X has
a compact image.
9. Show that there can be no continuous map (i) S1 ⇔[0, 2α[ which is onto, or
(ii) S1 ⇔R which is 1–1.
10. A continuous function f : R2 ⇔R takes a maximum, and a minimum, value
on a continuous path γ : [0, 1] ⇔R2. For example, there is a maximum and a
minimum distance between points on the path and the origin. Give an example
to show that this is false if [0, 1] is replaced by ]0, 1].
11. If f : X ⇔K is bijective and continuous, and K is compact, it does not follow
that X iscompact.Showthatthemapping f (ξ) := (cos ξ, sin ξ)for0 ⩽ξ < 2α,
is a counter-example.
12. Generalize the Heine-Borel theorem to closed rectangles [a, b]×[c, d] in R2, by
repeatedly dividing it into four sub-rectangles and adapting the same argument
of the proof. Can you extend this further to RN?
13. ▶The spheres and the closed balls in RN are compact.
14. Verify that [a, b]⊆Q is not compact by ﬁnding an inﬁnite set of rational numbers
in [a, b] that does not have a rational limit point.
15. Let f : RN ⇔RN be a continuous function; consider the following iteration
xn+1 := f (xn)/| f (xn)| of mapping by f and normalizing. Show that there is a
convergent subsequence (one for each limit point), assuming f (xn) ̸= 0.
16. ▶If X, Y are compact metric spaces then so is X × Y.
17. It is instructive to ﬁnd an alternative proof that a continuous function maps a
compact set to a compact set, using the BW property.

78
6
Compactness
Karl Weierstraß (1815–1897) After belatedly becoming a sec-
ondary school mathematics teacher at 26 years, he privately
studied Abel’s subject of integrals and elliptic functions, until
in 1854 he wrote a paper on his work and was given an hon-
e
m
a
c
e
b
n
e
h
t
e
H
.g
r
e
b
s
gi
n
o¨
K
f
o
y
tisr
e
v
i
n
U
e
h
t
y
b
e
e
r
g
e
d
y
r
a
r
o
famous with his programme of “arithmetization” based upon
the construction of the real numbers, and of a function that is
continuous but nowhere diﬀerentiable.
Fig. 6.1 Weierstraß
6.4 The Space C(X,Y)
We are now ready to turn the set of continuous functions f : [0, 1] ⇔C into a
complete metric space C[0, 1], thereby giving one precise meaning to fn ⇔f . To
appreciate the difﬁculty involved, note that if we were to deﬁne fn ⇔f to mean
pointwise convergence, that is, fn(x) ⇔f (x) for all x ∗[0, 1], then we would
get an incomplete space: The polynomials xn converge pointwise to a discontinuous
function. In fact we will consider the more general case of bounded functions from
any set to a metric space. A bounded function is one such that im f is bounded in the
codomain Y, that is,
∞r > 0, √a, b ∗X,
dY ( f (a), f (b)) ⩽r.
Theorem 6.23
The space of bounded functions from a set X to a metric space Y is itself
a metric space, with distance deﬁned by
d( f, g) := sup
x∗X
dY ( f (x), g(x)),
which is complete when Y is.
It contains the closed subspace Cb(X, Y) of bounded continuous functions,
when X is a metric space.
Proof Distance: The distance is well-deﬁned because if im f and img are bounded,
then so is their union, and dY ( f (x), g(x)) ⩽diam(im f ∀im g) for all x ∗X.
That d satisﬁes the distance axioms follows from the same properties for dY ;
d( f, g) = 0 ∃√x ∗X,
dY ( f (x), g(x)) = 0
∃√x ∗X,
f (x) = g(x)
∃f = g,

6.4 The Space C(X,Y)
79
d( f, g) = sup
x∗X
dY ( f (x), g(x))
⩽sup
x∗X

dY ( f (x), h(x)) + dY (h(x), g(x))

⩽sup
x∗X
dY ( f (x), h(x)) + sup
x∗X
dY (h(x), g(x)) (Exercise 3.5(7b))
= d( f, h) + d(h, g).
The axiom of symmetry d(g, f ) = d( f, g) is easily veriﬁed.
Completeness: Let fn : X ⇔Y be a Cauchy sequence of bounded functions,
then for every x ∗X,
dY ( fn(x), fm(x)) ⩽d( fn, fm) ⇔0, as n, m ⇔⇐
so ( fn(x)) is a Cauchy sequence in Y. When Y is complete, fn(x) converges to,
say, f (x).
Normally, this convergence would be expected to depend on x, being slower for
some points than others. In this case however, the convergence is uniform, as it is
d( fn, fm) := supx dY ( fn(x), fm(x)) which converges to 0. So given any ∂> 0
there is an N, such that dY ( fn(x), fm(x)) < ∂/2 for any n, m ⩾N and any
x ∗X. For each x, we can choose m ⩾N, dependent on x and large enough
so that dY ( fm(x), f (x)) < ∂/2, and this implies
√x ∗X,
dY ( fn(x), f (x)) ⩽dY ( fn(x), fm(x)) + dY ( fm(x), f (x)) < ∂
(6.1)
for any n ⩾N. Since this N is independent of x, it follows that d( fn, f ) ⇔0.
The function f is bounded because for any x, y ∗X, using (6.1),
dY ( f (x), f (y)) ⩽dY ( f (x), fN(x)) + dY ( fN(x), fN(y)) + dY ( fN(y), f (y))
< ∂+ R fN + ∂
(6.2)
with N independent of x and y, where R fN is the diameter of im fN.
Cb(X, Y) is closed: If X is a metric space and fn are continuous, then this same
inequality (6.2) shows that f is also continuous: if δN is small enough, then
dX(x, y) < δN ∩dY ( fN(x), fN(y)) < ∂
∩dY ( f (x), f (y)) < 3∂,
so that fn ⇔f ∗Cb(X, Y).
→♦
Often we write C(X) for the complete metric space Cb(X, C).

80
6
Compactness
The convergence fn ⇔f in Cb(X, Y) is called uniform convergence. It is much
stronger than pointwise convergence fn(x) ⇔f (x), √x ∗X; since || fn −f || =
supx | fn(x) −f (x)| is decreasing to 0, fn approximates f for large n at all values
of x uniformly.
Recall that continuous functions on a compact domain are uniformly continuous
(Proposition 6.17). Thus any ball of a ﬁxed radius δ is mapped by a real-valued
continuous function f into a ball of radius ∂. So, if [a, b] ∈R is partitioned into
intervals [xi, xi + δ[, then f maps each into an interval of length at most 2∂. Letting
f take a constant value f (xi) on each interval gives a uniform approximation by a
“step” function. Of course, step functions are usually discontinuous. We can improve
the approximation by constructing a function consisting of straight-line segments
from one end-point (xi, f (xi)) to the next (xi + δ, f (xi + δ). In fact, extending this
idea further, one can ﬁnd quadratic or cubic polynomial ﬁts, called “splines” that are
widely used to approximate real continuous functions, but these spline polynomials
do not normally join up together as a single polynomial on [a, b]. Such a line of
argument does give a valid proof that C[a, b] is separable; in fact one can even
generalize it to show that C(K) is separable whenever K is a compact metric space.
Stone’s theorem goes further and shows that the complex-valued functions on any
compact subset K of C can be approximated by polynomials on K.
Theorem 6.24 Stone–Weierstraß
The polynomials (in z and ¯z) are dense in C(K), when K ∈C is compact.
Proof The proof is in ﬁve steps. The ﬁrst two steps show that if a real-valued function
f ∗C(K) can be approximated by a polynomial p, then another polynomial can be
found that approximates | f |. Since the maximum of two functions max( f, g) can be
written in terms of | f −g|, it can also be approximated by polynomials if f and g can.
The fourth step, which is the main one, shows how a piecewise-linear approximation
of f ∗C(K) can be written in terms of max and min. Together these steps prove
that the polynomials R[x, y] are dense in the space of real continuous functions on
K. The ﬁnal step extends this to complex-valued continuous functions.
(i) There are real polynomials that approximate |x| on −1 ⩽x ⩽1: For example,
let q1(x) := x2, q2(x) := 2x2 −x4, …, deﬁned iteratively by
qn+1(x) := qn(x) + (x2 −qn(x)2), starting from q0(x) := 0.
Let yn := qn(x) for brevity, where 0 < x < 1. Notice that

6.4 The Space C(X,Y)
81
−1
1
yn+1 −x = yn −x −(yn −x)(yn + x)
= (yn −x)(1 −x −yn).
When |yn −x| ⩽|y1 −x| = x −x2, we get
0 < x2 ⩽yn ⩽2x −x2 < 1
∩−x < 1 −x −yn < 1 −x
∩
|yn+1 −x| ⩽c|yn −x|
where c := max(x, 1 −x) < 1. By induction, it follows that as n ⇔⇐,
|yn+1 −x| ⩽cn|y1 −x| ⇔0.
The special cases x = 0 and x = 1 converge immediately to 0 and 1 respectively,
while qn(x) ⇔|x| when x ∗[−1, 0[ by the symmetry of the expression in the
deﬁnition of qn.
Moreover, the convergence is uniform in x (certainly for 0 ⩽x < ∂and 1 −∂<
x ⩽1, but for the other positive values of x it takes at most −2 log ∂/∂iterates for
|yn −x| ⩽cnx < ∂).
(ii) Let f ∗C(K, R) ( f ̸= 0) with c := maxx∗K | f (x)| > 0 (Corollary 6.16). Then
the scaled function F := f/c takes values in [−1, 1] so |F| can be approximated
by q ∪F, where the polynomial q approximates the function x ⇔|x| on [−1, 1]. If
the polynomial p approximates f , it can be expected that the polynomial cq ∪(p/c)
ought to approximate | f | on C(K). This indeed holds since q is uniformly continuous
on [−1, 1],
√∂> 0, ∞δ > 0, √x ∗K,
|F(x) −α| < δ ∩|q ∪F(x) −q(α)| < ∂
so writing P := p/c,
d( f, p) < cδ ∩d(F, P) < δ
∩d(q ∪F, q ∪P) < ∂
∩d(|F|, q ∪P) ⩽d(|F|, q ∪F) + d(q ∪F, q ∪P)
⩽d(|x|, q)C[0,1] + d(q ∪F, q ∪P) < 2∂
∩d(| f |, cq ∪P) ⩽2c∂.
(iii) For real functions, deﬁne max( f, g)(x) := max( f (x), g(x)) as well as
min( f, g)(x) := min( f (x), g(x)); a short exercise shows that
max( f, g) = ( f + g + | f −g|)/2,
min( f, g) = ( f + g −| f −g|)/2.

82
6
Compactness
But if f and g can be approximated by polynomials, then so can their sum and
difference, and by (ii), also | f −g|, and hence max( f, g) and min( f, g).
(iv) The real polynomials are dense among the real continuous functions C(K, R):
Let f ∗C(K, R); for any z ̸= w in K, there is a linear function (a polynomial) pz,w
which agrees with f at the points z, w, i.e., pz,w(z) = f (z), pz,w(w) = f (w).
For a ﬁxed z, let
Uz,w := {a ∗K : pz,w(a) < f (a) + ∂} = ( f −pz,w)−1]−∂, ⇐[
a non-empty open set (since f −pz,w is continuous
and Uz,w contains z). As w ∗Uz,w, we have K ⇒

z̸=w Uz,w; but K is compact so it can be covered
by a ﬁnite number of subsets of this open cover, K =
Uz,w1 ∀· · ·∀Uz,wM . Let gz := min(pz,w1, . . . , pz,wM )
< f + ∂; it is continuous and can be approximated by
polynomials, from (iii).
x
y
Ux,y
f
px,y
Now let
Vz := {a ∗K : gz(a) > f (a) −∂} = ( f −gz)−1]−⇐, ∂[
a non-empty open set ( f −gz is continuous, and z ∗Vz). Once again, K ⇒
z Vz, and
so K = Vz1 ∀· · · ∀VzN . Let h := max(gz1, . . . , gzN ), a continuous function which
can be approximated by polynomials, since gzi can. Furthermore f −∂< h < f +∂;
and as this holds uniformly in z, we have d( f, h) < ∂.
(v) The set of polynomials in z and ¯z is dense in C(K): If f ∗C(K) is complex-
valued, then it can be written as f = u + iv with u, v real-valued and continuous,
that can be approximated by real polynomials p, q, say. Then,
√z ∗K, |(p(z) + iq(z)) −(u(z) + iv(z))| ⩽|p(z) −u(z)| + |q(z) −v(z)|
∩d(p + iq, u + iv) ⩽d(p, u) + d(q, v)
shows that p + iq approximates f . But is, say, x2y + i(x3 −xy2) a polynomial in
z? Not necessarily: for example, take the polynomial x itself and suppose Re(z) =
x = amzm + · · · + anzn with am ̸= 0 being the ﬁrst non-zero coefﬁcient; then
am = limz⇔0 x
zm , but Re(z)/zm can be made real or imaginary, so am = 0, a
contradiction. Nevertheless, writing x = (z + ¯z)/2 and y = (z −¯z)/2i shows that
every polynomial p(x, y) + iq(x, y) is a polynomial in z and ¯z.
→♦
The last theorem in this chapter characterizes the totally bounded sets of the space
C(K, Y) of continuous functions on a compact space K (in this case, C(K, Y) =
Cb(K, Y)), in terms of an explicit property of families of functions:

6.4 The Space C(X,Y)
83
Marshall Stone (1903–1989) studied at Harvard under Birkhoﬀ
(1926), with a thesis on ordinary diﬀerential equations and or-
thogonal expansions (Hermite, etc.). He then worked on spec-
tral theory in Hilbert spaces, obtaining his big breakthrough in
1937 when he generalized the Weierstrass approximation theo-
rem, which led him to the Stone-ˇCech compactiﬁcation theory.
Fig. 6.2 Stone
Deﬁnition 6.25
A set F ⇒C(X, Y) of continuous functions on metric spaces is said to be
equicontinuous when
√∂> 0, ∞δ > 0, √f ∗F, √x, x◦∗X, d(x, x◦) < δ ∩d( f (x), f (x◦)) < ∂.
The equi in equicontinuous refers to the fact that δ is independent of f ∗F.
Theorem 6.26 Arzelà-Ascoli
Let K and Y be metric spaces, with K compact. Then
F ⇒C(K, Y) is totally bounded ∃F K is totally bounded in Y and F is
equicontinuous.
Here F K denotes the set { f (x) : f ∗F, x ∗K}.
Proof (i) Let F be a totally bounded subset of C(K, Y). This means that for any
∂> 0, there are a ﬁnite number of continuous functions f1, . . . , fn ∗F that are
close to within ∂of every other function in F.
FK is totally bounded: Let ∂> 0 be arbitrary. Each fi K is compact (Proposi-
tion 6.15), so n
i=1 fi K is totally bounded (Proposition 6.14 and Theorem 6.19),
and covered by a ﬁnite number of balls B∂(y j), j = 1, . . . , m. This means that for
every x ∗K and i = 1, . . . , n, fi(x) is close to some y j ∗Y. Combining this with
the fact that any function f ∗F is close to some fi, gives
d( f (x), y j) ⩽d( f (x), fi(x)) + d( fi(x), y j) < 2∂.
Thus each f (x), where f ∗F and x ∗K, is close to some y j ( j depends on x and
f ), in other words the ﬁnite number of balls B2∂(y j) cover F K.

84
6
Compactness
F is equicontinuous: We have seen previously that functions f ∗C(K), in par-
ticular fi, are uniformly continuous (Proposition 6.17): each ∂> 0 gives parameters
δi. But we can say more. Since there are only a ﬁnite number of the functions fi, the
minimum δ := mini δi can be chosen such that
√∂> 0, ∞δ > 0, √i, √x, x◦∗K,
d(x, x◦) < δ ∩d( fi(x), fi(x◦)) < ∂.
But indeed this works for any f ∗F:
√∂> 0, ∞δ > 0, √f ∗F, √x, x◦∗K,
d(x, x◦) < δ ∩
d( f (x), f (x◦)) ⩽d( f (x), fi(x)) + d( fi(x), fi(x◦)) + d( fi(x◦), f (x◦)) < 3∂.
(ii) Let F K be totally bounded and F be equicontinuous. Then F K can be covered
by a ﬁnite number of balls B∂(y j), j = 1, . . . , m, i.e., any value f (x) for f ∗F
and x ∗K is close to some y j to within ∂. ‘F is equicontinuous’ means that for
any ∂> 0, the distance d( f (x), f (x◦)) < ∂for any f ∗F, whenever x and x◦are
sufﬁciently close together to within some δ > 0 that does not depend on x, x◦, or f .
We also require that K is totally bounded, so that it can be covered by a ﬁnite number
of balls of diameter δ. By removing any overlaps between the balls, we can replace
them by a ﬁnite partition of subsets Bi, i = 1, . . . , n, each of diameter at most δ.
For any f ∗F and x ∗Bi, f (x) is close to some y j, d( f (x), y j) < ∂. Indeed,
for any other x◦∗Bi, we have
d( f (x◦), y j) ⩽d( f (x◦), f (x)) + d( f (x), y j) < 2∂,
because d(x, x◦) ⩽δ and F is equicontinuous. In other words, the function f maps
each Bi into a ball B2∂(y j) ( j depending on i), and the whole partitioned space K
into some of these balls. That is, we know f to within the approximation 2∂, if we
know precisely how it maps each Bi to which ball B2∂(y j); this is equivalent to
an “encoding” i ⇔j from i = 1, . . . , n to j = 1, . . . , m. There are at most mn
such maps, although not all need be represented by the functions in F. For those
combinations that are in fact represented by functions in F, select one from each and
denote it by gk, k = 1, . . . , N.
Going back to f ∗F, with an encoding i ⇔j, pick gk with the same encoding.
Then for any x ∗K, pick y j close to f (x) (and gk(x)),
d( f (x), gk(x)) ⩽d( f (x), y j) + d(y j, gk(x)) < 4∂
and taking the supremum over x, we have d( f, gk) ⩽4∂. To summarize, the ﬁnite
number of functions gk are close to within 4∂to any function f ∗F, so that F is
totally bounded.
→♦

6.4 The Space C(X,Y)
85
Exercises 6.27
1. Show that C(X) contains the closed subset Cb(X, R).
2. ▶Uniform convergence, fn ⇔f in C(X), implies pointwise convergence.
Plot the functions (i) f (nx) on [0, 1], where f (x) := max(0, x(1 −x), and (ii)
x ⇔1/(1 + nx) on ]0, ⇐[; then show they converge pointwise to 0 as n ⇔⇐,
but not uniformly.
3.

fn ⇔

f and f ◦
n(x) ⇔f ◦(x) need not hold if fn converges to f pointwise.
Show that x ⇔nxn and sin nx
n
are counterexamples in C(0, 1).
4. * (Dini) If K is compact and fn ∗C(K) is an increasing sequence of real-valued
functions, converging pointwise to f ∗C(K), then fn ⇔f in C(K). (Hint:
Cover K by balls Bδ(x) inside which f −∂< fn < f for n > Nx.)
5. * The space C[a, b] is separable (using piecewise linear functions with kinks at
rational numbers), but C(R+) is not.
6. The subspace of polynomials in C[a, b] is not closed (and so is incomplete): con-
struct a sequence of polynomials that converges to a non-polynomial continuous
function in C[0, 1].
7. If J : X ⇔˜X and L : Y ⇔˜Y are homeomorphisms then f ⇔L ∪f ∪J −1 is
a homeomorphism between C(X, Y) and C( ˜X, ˜Y).
8. Follow the proof of the Stone-Weierstrass theorem to ﬁnd a quadratic approxi-
mation to the function f (x) :=
	 x
0 ⩽x ⩽1
0
−1 ⩽x < 0 .
9. Suppose fn : K ⇔C are continuous functions on a compact set K, converging
pointwise to f . By the Arzelà-Ascoli theorem, if fn are equicontinuous and
uniformly bounded (| fn(x)| ⩽c for all x ∗K, n ∗N), then f is also continuous.
10. A set of Lipschitz functions f : [a, b] ⇔R (Deﬁnition 4.14) with the same
Lipschitz constant c, | f (x) −f (y)| ⩽c|x −y|, form a totally bounded set in
C[a, b]. The fact that one c works for all, implies that they are equicontinuous;
and their collective image in R is bounded (|x −y| ⩽|b −a|), hence totally
bounded.
11. Show that the set of functions {sin x, sin 2x, . . .} and {x, x2, x3, . . .} on [0, 1]
are not equicontinuous.

Part II
Banach and Hilbert Spaces

Chapter 7
Normed Spaces
7.1 Vector Spaces
It is assumed that the reader has already encountered vectors and matrices before but
a brief summary of their theory is provided here for reference purposes.
Deﬁnition 7.1
A vector space V over a ﬁeld F is a set on which are deﬁned an operation of
vector addition + : V 2 ∞V satisfying associativity, commutativity, zero and
inverse axioms, and an operation of scalar multiplication F × V ∞V that
satisﬁes the respective distributive laws: for every x, y, z √V and φ, μ √F,
x + (y + z) = (x + y) + z,
x + y = y + x,
0 + x = x,
x + (−x) = 0,
φ(x + y) = φx + φy,
(φ + μ)x = φx + μx,
(φμ)x = φ(μx),
1x = x.
0
x
y
x+y
λx
λy
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_7,
89
© Springer International Publishing Switzerland 2014

90
7
Normed Spaces
Review
1. (−1)x = −x, −(−x) = x, 0x = 0, φ0 = 0. There is little danger that the
zero scalar is confused with the zero vector, so no attempt is made to distinguish
them.
2. F is itself a vector space with scalar multiplication being plain multiplication.
The smallest vector space is { 0 } (often written as 0).
3. The product of vector spaces (over the same ﬁeld), V × W, is a vector space
with addition and scalar multiplication deﬁned by
 x1
y1
⎛
+
 x2
y2
⎛
:=
 x1 + x2
y1 + y2
⎛
,
φ
 x
y
⎛
:=
φx
φy
⎛
.
The zero in this case is
⎜0
0
⎝
and the negatives are −
⎜x
y
⎝
=
⎜−x
−y
⎝
. By extension,
FN := F × · · · × F is a vector space.
4. If V is a vector space, then so is the set of functions V A := { f : A ∞V } (for
any set A) with
( f + g)(x) := f (x) + g(x),
(φ f )(x) := φ f (x).
The zero of V A is 0(x) := 0, and the negatives are (−f )(x) := −f (x).
5. A subset of a vector space V which is itself a vector space with respect to the
inherited vector addition and scalar multiplication is called a linear subspace.
Since associativity and commutativity are obviously inherited properties, one
need only check that the non-empty subset is “closed” under vector addition
and scalar multiplication (then the zero 0 = 0x and inverses −x = (−1)x are
automatically in the set). There are always the trivial linear subspaces { 0 } and V .
6. The intersection of linear subspaces is itself a linear subspace.
7. An important example of a linear subspace is that generated by a set of vectors
[[A]] := { φ1a1 + · · · + φnan : ai √A, φi √F, n √N },
with the convention that [[∅]] := { 0 }. It is the smallest linear subspace that
includes A, and we say that A spans, or generates, [[A]]. Each element of [[A]]
is said to be a linear combination of the vectors in A.
8. The set A is linearly independent when any vector a √A is not generated by
the rest, a ∗√[[A∖{ a }]]. (In particular A does not contain 0.) This is equivalent
to saying that φa √[[A∖{ a }]] ⇒φ = 0, or that for distinct ai √A,
n
⎞
i=1
φiai = 0 ⇒φi = 0, i = 1, . . . , n.

7.1 Vector Spaces
91
A vector generated by a linearly independent set A has unique coefﬁcients φi,
x =
n
⎞
i=1
φiai =
n
⎞
i=1
μiai ⇒φi = μi, i = 1, . . . , n.
9. A basis is a minimal set of generating vectors; it must be linearly independent.
Conversely, every generating set of linearly independent vectors is a basis.
10. A vector space is said to be ﬁnite-dimensional when it is generated by a ﬁnite
number of vectors, V = [[a1, . . . , aN]] (:= [[{ a1, . . . , aN }]]). The smallest such
number of generating vectors is called the dimension of the vector space, denoted
dim V , and is equal to the number of vectors in a basis.
11. For example, F has dimension 1, because it is generated by any non-zero element,
while dim{ 0 } = 0. The linear subspace generated by two linearly independent
vectors [[x, y]] is 2-dimensional and is called a plane (passing through the origin).
12. The space of m × n matrices is a ﬁnite-dimensional vector space, generated by
the mn matrices Ei j consisting of 0s everywhere with the exception of a 1 at
row i and column j.
13. If V is ﬁnite-dimensional, then so is any linear subspace W, and dim W ⩽dim V
(strictly less if it is a proper subspace).
14. We write A + B := { a + b : a √A and b √B } and φA := { φa : a √A } for
any subsets A, B →V , e.g.Q + Q = Q, C = R + iR. Thus φ(A ♦B) = φA ♦
φB, and φ(A ∃B) = φA ∃φB(φ ∗= 0); a non-empty set A is a linear subspace
when φA + μA →A for all φ, μ √F. For brevity, x + A is written instead of
{ x } + A; it is a translation of the set A by the vector x. Care must be taken in
interpreting these symbols: A −A = { a −b : a, b √A } is not usually { 0 }.
15. For non-empty subsets of R, sup(A + B) ⩽sup A + sup B, sup(φA) = φ sup A
(φ ⩾0).
Proof Let a + b √A + B, then a ⩽sup A and b ⩽sup B, so sup A + sup B is
an upper bound of A + B, and hence greater than its least upper bound.
Similarly, for all a √A, a ⩽sup A ∀φa ⩽φ sup A, so sup(φA) ⩽φ sup A.
Hence, sup A = sup( 1
φφA) ⩽1
φ sup(φA) and equality holds.
16. The space V is said to decompose as a direct sum of its subspaces M and N,
written V = M ∈N, when V = M + N and M ∃N = 0. For example,
R2 = [[
⎜1
1
⎝
]] ∈[[
⎜2
1
⎝
]].
17. ▶For vector spaces over R or C, a subset C is said to be convex when it contains
the line segment between any two of its points,
⊆x, y √C, 0 ⩽t ⩽1 ∀tx + (1 −t)y √C,

92
7
Normed Spaces
equivalent to sC + tC = (s + t)C for s, t ⩾0. This generalizes easily to
t1x1 +· · ·+tnxn √C when t1 +· · ·+tn = 1, ti ⩾0, and xi √C. Clearly, linear
subspaces are convex.
18. The intersection of convex sets is convex. There is a smallest convex set con-
taining a set A, called its convex hull, deﬁned as the intersection of all convex
sets containing A, which equals
{ t1x1 + · · · + tnxn : xi √A, ti ⩾0, t1 + · · · + tn = 1, n √N }.
Hausdorff’s Maximality Principle
The Hausdorff Maximality Principle is a statement that can be used to possibly extend
arguments that work in the ﬁnite or countable case to sets of arbitrary size. There are
a few proofs in this book that make use of this principle; it is only needed to extend
results to “uncountably inﬁnite” dimensions. As such, it is mainly of theoretical
value, and this section can be skipped if the main interest is in applications.
Consider a collection M of subsets M →X that satisfy a certain property P. A
chain C = { Mψ } of such sets is a nested sub-collection, meaning that for any two
sets Mψ, Mδ √C, either Mψ →Mδ or Mδ →Mψ. A chain can contain any number of
nested subsets, even uncountable. A chain is called maximal when it cannot be added
to by the insertion of any subset in M. Hausdorff’s maximality principle states that
Every chain in M is contained in some maximal chain in M.
Hausdorff’s Maximality principle is often used to show there is a maximal set
E that satisﬁes some property P as follows: the empty chain can be extended to a
maximal chain of sets Mψ; if it can be shown that the union of this chain E := ⎟
ψ Mψ
also satisﬁes P, then there are no sets properly containing E which satisfy P, by the
maximality of { Mψ }, i.e., E is a maximal set in M.
At the end of this chapter, it is proved that Hausdorff’s maximality principle
implies the Axiom of Choice. Conversely, the Hausdorff Maximality principle can
be proved from the Axiom of Choice (using the other standard set axioms), i.e., they
are logically equivalent to each other, as well as to a number of other formulations
such as Zorn’s lemma and the Well-Ordering principle. These statements are not
constructive in the sense that they give no explicit way of ﬁnding the choice function
or the maximal chain, but simply assert their existence.
The purpose in introducing Hausdorff’s Maximality Principle here is to prove:
Every vector space has a basis.

7.1 Vector Spaces
93
Proof Consider the collection of linearly independent sets of vectors in V . By
Hausdorff’s maximality principle, there is a maximal chain M of nested linearly
independent sets Aψ. We show that E := ⎟
ψ Aψ is linearly independent and spans V ,
hence a basis. If ⎠n
i=1 φiai = 0 for ai √E, then each of the vectors ai (i = 1, . . . , n)
belongs to some Aψi , and hence they all belong to some single Aψ because these sets
are nested in each other; but as Aψ is linearly independent, φi = 0 for i = 1, . . . , n.
Thus E is linearly independent. Suppose E does not span V , meaning there is a
vector v ∗√[[E]], so that E ♦{ v } is linearly independent. As it properly contains E
and every Aψ, it contradicts the maximality of the chain M.
⊓⇐
7.2 Norms
We would like to consider vector spaces having a metric space structure. Any set can
be given a metric, so this is quite possible, but it is more interesting to have a metric
that is related to vector addition and scalar multiplication in a natural way. Taking
cue from Euclid’s ideas of congruence, the properties that we have in mind are
(a) translation-invariance, distances between vectors should remain the same when
they are translated by the same amount,
d(x + a, y + a) = d(x, y),
a
x
y
(b) scaling-homogeneity, distances should scale in proportion when vectors are
scaled,
d(φx, φy) = |φ|d(x, y).
x
y
λx
λy
These properties are valid only for special types of metric. When d is translation
invariant, then d(x, y) = d(x −y, y −y) = d(x −y, 0) and d becomes essentially
a function of one variable, namely the norm function ⊂x⊂:= d(x, 0) with d(x, y) =
⊂x −y⊂. Conversely, any such d deﬁned this way is translation invariant because
d(x + a, y + a) = ⊂x + a −y −a⊂= d(x, y). This function is then scaling-
homogeneous precisely when

94
7
Normed Spaces
⊂φx⊂= d(φx, 0) = |φ|d(x, 0) = |φ|⊂x⊂.
What properties does a norm need to have, for d to be a distance? It is easy to see
that
d(x, z) ⩽d(x, y) + d(y, z)
d(y, x) = d(x, y)
d(x, y) ⩾0
d(x, y) = 0 ⇒x = y
⎫
⎪⎪⎬
⎪⎪⎭
⇒
⎧
⎪⎪⎨
⎪⎪⎩
⊂a + b⊂⩽⊂a⊂+ ⊂b⊂
⊂−a⊂= ⊂a⊂
⊂a⊂⩾0
⊂a⊂= 0 ⇒a = 0
where a = x −y, b = y −z. Of these, the symmetry property follows from scaling-
homogeneity, while positivity follows from 0 = ⊂x −x⊂⩽⊂x⊂+ ⊂−x⊂= 2⊂x⊂.
Deﬁnition 7.2
A normed space X is a vector space over F = R or C with a function called
the norm ⊂· ⊂: X ∞R such that for any x, y √X, φ √F,
⊂x + y⊂⩽⊂x⊂+ ⊂y⊂,
⊂φx⊂= |φ|⊂x⊂,
⊂x⊂= 0 ⇒x = 0.
If necessary, norms on different spaces are distinguished by a subscript such as
⊂· ⊂X. A positive function that satisﬁes the ﬁrst two axioms is termed a semi-norm.
Easy Consequences
1. ⊂x −y⊂⩾⊂x⊂−⊂y⊂.
2. ⊂x1 + · · · + xn⊂⩽⊂x1⊂+ · · · + ⊂xn⊂(by induction).
Examples 7.3
1. The absolute value functions, |·|, for R and C are themselves norms, making these
the simplest normed spaces.
2. ▶The spaces RN and CN of geometric vectors have a Euclidean norm deﬁned by
⊂a⊂2 :=
 N
⎞
i=1
|ai|2
1/2
.
There are other possibilities, e.g.⊂a⊂1 := ⎠N
i=1 |ai|, or ⊂a⊂⇔:= maxi |ai|. Thus
⊂
⎜3
−4
⎝
⊂1 = 3+4 = 7,
⊂
⎜3
−4
⎝
⊂2 =
∩
9 + 16 = 5,
⊂
⎜3
−4
⎝
⊂⇔= max(3, 4) = 4.
ThedifferentnormsgivethedifferentdistancesalreadydeﬁnedinExample2.2(6).

7.2 Norms
95
3. ▶A sequence of vectors xn = (a1n, . . . , aNn) in FN converges, xn ∞a (in any
of these norms), precisely when each coefﬁcient converges in F, ain ∞ai for
i = 1, . . . , N.
Proof Using the 2-norm, for any ﬁxed i,
|ain −ai|2 ⩽|a1n −a1|2 + · · · + |aNn −aN|2 = ⊂xn −a⊂2
2
so when the latter diminishes to 0, so does the left-hand side.
Conversely, if ain ∞ai for i = 1, . . . , N, then
⊂xn −a⊂2 =

|a1n −a1|2 + · · · + |aNn −aN|2 ∞0,
by continuity of the various constituent functions.
With minor changes, the same proof works for the other norms as well.
4. More generally, we can deﬁne the norm ⊂a⊂p :=
p⎠N
i=1 |ai|p for p ⩾1.
Shortly, we will see that all these norms are equivalent in ﬁnite dimensions, so
we usually take the most convenient ones, such as p = 1, 2, ⇔.
5. ▶Sequences: sequences can be added and multiplied by scalars, and form a
vector space.
(a0, a1, . . .) + (b0, b1, . . .) := a0 + b0, a1 + b1, . . .),
φ(a0, a1, . . .) := (φa0, φa1, . . .).
The zero sequence is (0, 0, . . .) and −(a0, a1, . . .) = (−a0, −a1, . . .).
The different norms introduced above generalize to sequences; the three most
important normed sequence spaces are:
(a) ∂1 := { (an) : ⎠⇔
n=0 |an| < ⇔} with norm deﬁned by
⊂(an)⊂∂1 :=
⇔
⎞
n=0
|an|.
(b) ∂2 := { (an) : ⎠⇔
n=0 |an|2 < ⇔} with norm deﬁned by
⊂(an)⊂∂2 :=




⇔
⎞
n=0
|an|2.
(c) ∂⇔:= { (an) : ∪c, |an| ⩽c } with norm deﬁned by ⊂(an)⊂∂⇔:= sup
n
|an|.
For example, for the sequence (1/n) =
⎜
1, 1
2, 1
3, . . .
⎝
,
⊂(1/n)⊂∂1 = ⇔,
⊂(1/n)⊂∂2 = θ/
∩
6,
⊂(1/n)⊂∂⇔= 1.

96
7
Normed Spaces
In each case there are two versions of the spaces, depending on whether an √R
or C; the scalar ﬁeld is then, correspondingly, real or complex. By default, we
take the complex spaces as standard, unless speciﬁed otherwise.
Note carefully that an implicit assumption is being made here that adding two
sequences in a space gives another sequence in the same space. This follows from
the triangle inequality for the respective norm; it is left as an exercise for ∂1 and
∂⇔, but is proved for ∂2 in the next proposition. See Proposition 9.12 for ∂p.
6. These spaces are different from each other. Not only do they contain different
sequences, but convergence is different in each. For example, the sequences
x1 := (1, 0, 0, . . .)
x2 := (1/2, 1/2, 0, . . .)
x3 := (1/3, 1/3, 1/3, 0, . . .)
. . .
are all in ∂1, ∂2, and ∂⇔. They converge xn ∞0 in ∂⇔as n ∞⇔,
⊂xn⊂∂⇔= sup{ 1
n , 0 } = 1/n ∞0.
(Show that they also converge to 0 in ∂2.) But they do not converge in ∂1,
⊂xn⊂∂1 = 1
n + · · · + 1
n = 1 ∗∞0.
Thus, convergence of each coefﬁcient is necessary, but not sufﬁcient, for the
convergence of xn.
7. ▶Functions A ∞F, where A is an interval in R, say, also form a vector space,
with
( f + g)(x) := f (x) + g(x),
(φ f )(x) := φ f (x),
and different norms can be deﬁned for them as well (once again, there are two
versions of each space, depending on whether the functions are real- or complex-
valued):
(a) The space L1(A) := { f : A ∞C,

A | f (x)| dx < ⇔} with norm
deﬁned by
⊂f ⊂L1 :=

A
| f (x)| dx.
Or rather, this would be a norm, except that ⊂f ⊂L1 =

A | f (x)| dx = 0 not
when f = 0 but when f = 0 a.e. (Section9.2). The failure of this axiom
is not drastic, and those functions that are equal almost everywhere can be
identiﬁed into equivalence classes to create a proper normed space, called

7.2 Norms
97
Lebesgue space (Remark 2.23(1)). But to adopt a special notation for them,
such as [ f ], would be too pedantic to be useful; the symbol f , when used in
the context of Lebesgue spaces, represents any function in its equivalence
class. (The same comment holds for the next two spaces.)
(b) The space L2(A) := { f : A ∞C :

A | f (x)|2 dx < ⇔}, with norm
deﬁned by ⊂f ⊂L2 :=

A
| f (x)|2 dx
⎛1
2
. More generally there are the
L p(A) spaces for p ⩾1.
(c) The space
L⇔(A) := { f : A ∞C: f is measurable and ∪c | f (x)| ⩽c a.e.x },
with norm deﬁned by ⊂f ⊂L⇔:= supx a.e. | f (x)| (i.e., the smallest c such
that | f (x)| ⩽c a.e.x).
(d) The space Cb(X, Y) of bounded continuous functions, deﬁned previously
(Theorem 6.23), is a normed space when Y is, with
⊂f ⊂C := sup
x√X
⊂f (x)⊂Y .
(Check that d as deﬁned on Cb(X, Y) is translation-invariant and scaling-
homogeneous.) Cb(X) is a linear subspace of L⇔(X), with the same norm.
Note that Cb(N) = ∂⇔.
For example, on A := [0, 2θ], ⊂sin ⊂L1 = 4, ⊂sin ⊂L2 = ∩θ, and ⊂sin ⊂L⇔= 1.
More details and proofs for the ﬁrst three spaces can be found in Section8.2.
8. ▶When X, Y are normed spaces over the same ﬁeld, X × Y is also a normed
space, with
 x1
y1
⎛
+
 x2
y2
⎛
:=
 x1 + x2
y1 + y2
⎛
,
φ
 x
y
⎛
:=
 φx
φy
⎛
,

 x
y
⎛ := ⊂x⊂X + ⊂y⊂Y .
The induced metric is D1, deﬁned previously for X × Y as metric spaces
(Example 2.2(6)).
9. ▶Suppose a vector space has two norms ⊂· ⊂and |||·|||. Convergence with respect
to one norm is the same as convergence with respect to the other norm when they
are equivalent in the sense of metrics (Exercise 4.17(6)), i.e., there are positive
constants c, d > 0,
c⊂x⊂⩽|||x||| ⩽d⊂x⊂.

98
7
Normed Spaces
Proof Suppose the inequalities hold and ⊂xn −x⊂∞0, then
|||xn −x||| ⩽d⊂xn −x⊂∞0
as well; similarly if |||xn −x||| ∞0 then ⊂xn −x⊂⩽c−1|||xn −x||| ∞0.
Conversely, suppose the ratios |||x|||/⊂x⊂approach 0 as x varies in X. Then a
sequence of vectors xn can be found such that ⊂xn⊂= 1 but |||xn||| ⩽1/n, i.e.,
xn ∞0 with respect to ||| · ||| but not with respect to ⊂· ⊂. For this not to happen,
|||x|||/⊂x⊂⩾c > 0, and similarly, ⊂x⊂/|||x||| ⩾1/d > 0.
A good strategy to adopt when tackling a question about normed spaces, is to
try to answer it ﬁrst for R or C, then CN, then for a sequence space such as ∂⇔
or ∂1, and ﬁnally for a function space C[0, 1], L⇔[0, 1], or L1(R). Theoretically,
sequence spaces are useful as model spaces that are rich enough to exhibit most
generic properties of normed spaces. But they are also indispensable in practice: a
real-life function f (t) is discretized, or digitized, into a sequence of numbers fi,
before it can be manipulated by algorithms.
Letusjustifytheclaimthat∂2 isanormedspace,byshowingthatthestandardnorm
⊂(an)⊂∂2 :=
⎠
n |an|2 satisﬁes the triangle inequality, even in inﬁnite dimensions.
Proposition 7.4 Cauchy’s inequality
For an, bn √C,

⇔
⎞
n=0
anbn
 ⩽




⇔
⎞
n=0
|an|2




⇔
⎞
n=0
|bn|2




⇔
⎞
n=0
|an + bn|2 ⩽




⇔
⎞
n=0
|an|2 +




⇔
⎞
n=0
|bn|2
Proof (i) It is easy to show from (a −b)2 ⩾0 that ab ⩽(a2 + b2)/2 for any real
numbers a, b. Hence,
 ⎞
n
anbn
2
=
⎞
i, j
aibia jb j ⩽
⎞
i, j
(a2
i b2
j + a2
jb2
i )/2 =
⎞
i
a2
i
⎞
j
b2
j.
It follows that, for complex numbers an, bn,

⎞
n
anbn
 ⩽
⎞
n
|an||bn| ⩽
⎞
n
|an|2
⎞
n
|bn|2

7.2 Norms
99
(ii)
⎞
n
|an + bn|2 ⩽
⎞
n
|an|2 + |bn|2 + 2|anbn|
⩽
⎞
n
|an|2 +
⎞
n
|bn|2 + 2
⎞
n
|an|2 ⎞
n
|bn|2
=


⎞
n
|an|2 +
⎞
n
|bn|2


2
.
⊓⇐
Thus for any two real sequences x = (an), y = (bn) in ∂2, one can deﬁne their
‘dot product’
x · y :=
⇔
⎞
n=0
anbn
whose convergence is assured by Cauchy’s inequality. The identity ⊂x⊂2 = x · x,
familiar for Euclidean spaces, remains valid for ∂2. Note that the two inequalities
above can be written as |x · y| ⩽⊂x⊂⊂y⊂and ⊂x + y⊂⩽⊂x⊂+ ⊂y⊂, and that x · y
need not be ﬁnite unless x and y are in ∂2.
Since the metric of a normed space is translation invariant, it is not surprising that
balls do not change their shape when translated.
Proposition 7.5
All balls in a normed space have the same convex shape:
Br(x) = x + r B1(0),
Br(x) + Bs(y) = Br+s(x + y),
φBr(x) = B|φ|r(φx).
Proof The norm axioms can be recast as axioms for the shape of balls. The
translation-invariance and scaling-homogeneity of the distance are equivalent to
Br(x + a) = { y : d(y, x + a) < r } = { y : d(y −a, x) < r }
= { a + z : d(z, x) < r } = Br(x) + a,
φB1(0) = { φy : ⊂y⊂< 1 } = { z : ⊂z⊂< |φ| } = B|φ|(0), (φ ∗= 0).
Combining the two gives Br(a) = a + r B1(0), showing that all balls have the same
shape as the ball of radius 1 centered at the origin.
The third norm axiom is equivalent to "
r>0 Br(0) = { 0 }, while the triangle
inequality becomes Br(0) + Bs(0) = Br+s(0) since

100
7
Normed Spaces
⊂x⊂< r and ⊂y⊂< s ∀⊂x + y⊂< r + s,
⊂x⊂< r + s ∀x =
r
r + s x +
s
r + s x √Br(0) + Bs(0).
Recasting this equation as (r + s)B1(0) = r B1(0) + sB1(0) for r, s ⩾0 shows that
B1(0), and hence all other balls, are convex: for x, y √B1(0) and 0 ⩽t ⩽1,
(1 −t)x + ty √(1 −t)B1(0) + t B1(0) = B1(0).
In particular,
Br(x) + Bs(y) = x + r B1(0) + y + sB1(0) = Br+s(x + y),
φBr(x) = φx + φr B1(0) = B|φ|r(φx).
⊓⇐
The unit ball is often denoted by BX := B1(0) and takes a central role as repre-
sentative of all other balls; it contains all the information about the norm of X.
Examples 7.6
1. The boundary of a ball Br(x) is the sphere Sr(x) := { y √X : d(x, y) = r }.
Any point on the sphere has nearby points inside and outside the ball ((1 −α)y
and (1 + α)y). Thus Br(x) = Br[x] := { y √X : d(x, y) ⩽r }.
2. * Balls can have quite counter-intuitive properties. For example, consider the
path of functions ft(x) := 2|x −t| −1 in C[0, 1], starting from the function
f0(x) = 2x −1 and ending at the function f1 = −f0. It lies on the unit sphere
of C[0, 1], but has a total length equal to the distance between f0 and f1,
0
1
f 0
f 1
length =
 1
0
⊂d
dt ft⊂dt =
 1
0
2 dt = 2,
distance = ⊂f1 −f0⊂C[0,1] = 2⊂f0⊂C[0,1] = 2.
Exercises 7.7
1. Prove that ⊂· ⊂1 and ⊂· ⊂⇔are norms. Which axiom does ⊂· ⊂p fail when p < 1?
2. What do the unit balls of R2 in each norm of Example 7.3(2) look like?
3. The sequence (1, 1, . . . , 1, 0, 0, . . .) is not a good approximation to the constant
sequence (1, 1, . . .) in ∂⇔; but (1 −α, 1 −α, . . .) is.
4. The norm axioms for ∂1 and ∂⇔are, when interpreted correctly,
⎠
n |an + bn| ⩽⎠
n |an| + ⎠
n |bn|,
supn |an + bn| ⩽supn |an| + supn |bn|,
⎠
n |φan| = |φ| ⎠
n |an|,
supn |φan| = |φ| supn |an|,
⎠
n |an| = 0 ⇒⊆n, an = 0
supn |an| = 0 ⇒⊆n, an = 0.
Prove these, assuming any results about series (Section 7.5). What are they for ∂2?

7.2 Norms
101
5. A set A is bounded when there is a c > 0 such that ⊆x √A, ⊂x⊂⩽c (Section6.1).
A non-trivial normed space is not bounded.
6. For any subset A, and α > 0, A + Bα(0) is an open set containing A.
7. ▶The norms ⊂· ⊂1, ⊂· ⊂2 and ⊂· ⊂⇔are all equivalent on RN since (prove!)
⊂x⊂⇔⩽⊂x⊂2 ⩽⊂x⊂1 ⩽N⊂x⊂⇔.
But they are not equivalent for sequences or functions! Find sequences of func-
tions that converge in L1[0, 1] but not in L⇔[0, 1], or vice-versa. Can sequences
converge in ∂1 but not in ∂⇔?
8. * Minkowski semi-norm: Let C be a convex set which is balanced, eiρC = C
(⊆ρ √R), and such that ⎟
r>0 rC = X. Then
|||x||| := inf{r > 0 : x √rC }
is a semi-norm on X.
7.3 Metric and Vector Properties
By construction, normed spaces are metric spaces, as well as vector spaces. We
can apply ideas related to both, in particular open/closed sets, convergence, com-
pleteness, continuity, connectedness, and compactness, as well as linear subspaces,
linear independence and spanning sets, convexity, linear transformations, etc. Many
of these notions have better characterizations in normed spaces, as the following
propositions attest.
Proposition 7.8
Vector addition,
(x, y) ∞x + y,
X2 ∞X,
scalar multiplication,
(φ, x) ∞φx,
F × X ∞X,
and the norm
x ∞⊂x⊂,
X ∞R,
are continuous.
Proof Vector addition and the norm are in fact Lipschitz maps,
⊂(x1 + y1) −(x2 + y2)⊂⩽⊂x1 −x2⊂+ ⊂y1 −y2⊂= ⊂(x1, y1) −(x2, y2)⊂X2,
⊂x⊂−⊂y⊂
 ⩽⊂x −y⊂.
Scalar multiplication is continuous: for any α > 0, take |φ −μ| to be smaller than
min(α/3(1 + ⊂x⊂), 1) and ⊂x −y⊂< min(α/3(1 + |φ|), 1), to get

102
7
Normed Spaces
⊂φx −μy⊂⩽⊂φx −μx⊂+ ⊂μx −μy⊂
= |φ −μ|⊂x⊂+ |μ|⊂x −y⊂
⩽|φ −μ|⊂x⊂+ |φ|⊂x −y⊂+ |φ −μ|⊂x −y⊂
< α.
⊓⇐
Corollary 7.9
When (xn) and (yn) converge,
lim
n∞⇔(xn + yn) = lim
n∞⇔xn + lim
n∞⇔yn,
lim
n∞⇔φxn = φ lim
n∞⇔xn.
lim
n∞⇔⊂xn⊂= ⊂lim
n∞⇔xn⊂.
Ofparticularimportanceareclosedlinearsubspaces,becausetheyare“closed”not
only with respect to the algebraic operations of addition + and scalar multiplication
φ · , but also with respect to convergence ∞.
Proposition 7.10
If M is a linear subspace of X, then so is M.
[[A]] is the smallest closed linear space containing A.
Proof (i) Let x, y √M, with sequences xn √M, yn √M converging to them,
xn ∞x and yn ∞y (Proposition 3.4). But xn + yn and φxn both belong to M, so
x + y = lim
n∞⇔xn + lim
n∞⇔yn = lim
n∞⇔(xn + yn) √M,
φx = φ lim
n∞⇔xn = lim
n∞⇔(φxn) √M.
Thus M is closed under vector addition and scalar multiplication. In particular this
holds when M is generated by A.
(ii) [[A]] is the smallest linear subspace containing A, and [[A]] is the smallest closed
set containing [[A]]. So any closed linear subspace containing A must also contain
[[A]], and its closure [[A]].
⊓⇐

7.3 Metric and Vector Properties
103
Examples 7.11
1. The following sets are closed linear subspaces of their respective spaces:
(a) A := { (ai) √∂1 : ⎠⇔
i=0 ai = 0 },
(b) B := { f √C[a, b] : f (a) = f (b) },
The proofs for closure (linearity is left as an exercise) depend on the following
inequalities that hold when an ∞a in ∂1, an √A, and fn ∞f in C[0, 1],
fn √B,

⇔
⎞
i=0
ai
 =

⇔
⎞
i=0
ain +
⇔
⎞
i=0
(ai −ain)
 ⩽
⇔
⎞
i=0
|ai −ain| = ⊂a −an⊂∂1 ∞0
f (a) = lim
n∞⇔fn(a) = lim
n∞⇔fn(b) = f (b)
2. * If M and N are closed subsets of a normed space, M + N need not be closed
(see also Exercise 7.14(5)).
(i) Let f : X ∞Y be a continuous function between normed spaces; let
M := { (x, f (x)) : x √X }, N := { (x, 0) : x √X }; they are closed sub-
sets of X × Y (prove!). But M + N = { (˜x, f (x)) : x, ˜x √X } is closed if, and
only if, im f is closed, which need not be the case. To take a speciﬁc example,
{ (x, 0) : x √R } + { (x, ex) : x √R } = R × ]0, ⇔[.
(ii) This is true even if M, N are linear subspaces. Let M be the set of bounded
sequences (a1, 0, a2, 0, . . .) whose even terms vanish, and let N consist of
boundedsequencesofthetype(a1, a1/1, a2, a2/22, a3, a3/32, . . .).Theyareboth
closed subspaces of ∂⇔(check!). Now consider
xn : = (1, 1, 2, 1
2, 3, 1
3, 4, 1
4, . . . , n, 1
n , 0, 0, . . .) √N
yn : = (1, 0, 2, 0, 3, 0, 4, 0, . . . , n, 0, 0, 0, . . .) √M
xn −yn = (0, 1, 0, 1
2, 0, 1
3, 0, 1
4, . . . , 0, 1
n , 0, 0, . . .) √M + N
xn −yn converges to the bounded sequence (0, 1, 0, 1
2, . . .) which cannot be
expressed as a vector in M + N.
Connected and Compact Subsets
Recall that connected sets may be complicated objects in general metric spaces. This
is still true in normed spaces, but at least for open subsets, connectedness reduces to
path-connectedness, which is more intuitive and usually easier to prove.

104
7
Normed Spaces
Proposition 7.12
An open connected set in a normed space is path-connected.
Proof Let C be a non-empty open connected set in X. Recall that “path-connected”
means that any two points in C can be joined by a continuous path r : [0, 1] ∞C
starting at one point and ending at the other. Fix any x √C, and let P be the subset
of C consisting of those points that are path-connected to x. We wish to show that
P = C.
P has no boundary in C: Given any boundary point
z of P, there is a ball Bα(z) →C since C is open, and
thus a point y √P in the ball. This means that there is
a path r from x to y. In normed spaces, it is obvious
that balls, like all convex sets, are path-connected (by
straight paths). So we can extend the path r to one
that starts from x and ends at any other w √Bα(z),
simply by adjoining the straight line at the end. More
rigorously, the function ˜r : [0, 1] ∞C deﬁned by
x
y
z
w
P
˜r(t) :=
#
r(2t)
t √[0, 1
2]
y + (2t −1)(w −y) t √] 1
2, 1]
is continuous. So z is surrounded by points of P, a contradiction.
But a connected set such as C, cannot contain a subset, such as P, without
a boundary (Proposition 5.3), unless P = ∅(which is not the case here) or
P = C.
⊓⇐
There is quite a bit to say about bounded and totally bounded sets. As we will
see later on, they are the same in ﬁnite dimensional normed spaces, but in inﬁnite
dimensionalones,noopensetcanbetotallybounded,althoughballsareboundedsets.
For now, let us show that translations and scalings of bounded and totally bounded
sets remain so.
Proposition 7.13
If A, B are both bounded, totally bounded, or compact sets, then so are,
respectively, φA and A + B.
Proof Proposition 7.5 is used throughout the following.
Boundedness: If A →Br(x) and B →Bs(y), then
φA →φBr(x) = B|φ|r(φx),
A + B →Br(x) + Bs(y) = Br+s(x + y).

7.3 Metric and Vector Properties
105
Total boundedness:
φA →φ
N
$
i=1
Bα/|φ|(xi) =
N
$
i=1
Bα(φxi),
A + B →
N
$
i=1
Bα/2(xi) +
M
$
j=1
Bα/2(y j) =
$
i, j
Bα(xi + y j).
Compactness: If A is compact, then scalar multiplication, being continuous, sends
it to the compact set φA (Proposition 6.15). If B is also compact, then A × B is
compact (Exercise 6.22(16)), and vector addition, being continuous, maps it to the
compact set A + B.
⊓⇐
Exercises 7.14
1. Show that the following sets are closed subspaces of their respective spaces:
(a) { (ai) √∂⇔: a0 = 0 },
(b) { (ai) √∂2 : a1 = a3 and a0 = ⎠⇔
i=1 ai/i },
(c) { f √C[0, 1] :
 1
0 f = 0 }.
2. The set of polynomials in x forms a linear subspace of C[0, 1]. Its dimension is
inﬁnite because the elements 1, x, x2, . . . are linearly independent. Is it closed,
or if not, what could be the closure of the polynomials in this space?
3. The convex hull of a closed set need not be closed; a counterexample is given by,
(R × { 0 }) ♦{ (0, 1) }. But the closure of a convex set C is convex.
4. Line segments are path-connected; so linear subspaces and convex subsets (such
as balls) are connected.
5. The continuity of + and φ · imply that φA = φ ¯A and ¯A + ¯B →A + B. Find an
example to show that equality need not necessarily hold.
7.4 Complete and Separable Normed Vector Spaces
Deﬁnition 7.15
When the induced metric d(x, y) := ⊂x −y⊂is complete, the normed space
is called a Banach space.

106
7
Normed Spaces
Stefan Banach (1892–1945) After WW1, at 24 years, a chance
event led him to meet Steinhaus, who had studied under Hilbert
in 1911, and was then at Krakow university.
His 1920 the-
sis on abstract normed real vector spaces earned him a post
at the University of Lwow; working mostly in the “Scottish
caf´e”, he continued research on “linear operations”, where he
introduced weak convergence and proved various theorems such
as the Hahn-Banach, Banach-Steinhaus, Banach-Alaoglu, his
ﬁxed-point theorem, and the Banach-Tarski paradox.
Fig. 7.1 Banach
Examples 7.16
1. ▶RN and CN are separable Banach spaces. It is later shown (Section 9.1.4) that
the sequence spaces ∂p and the Lebesgue function spaces L p[0, 1] (1 ⩽p <
⇔) are also separable Banach spaces, but ∂⇔is a non-separable Banach space
(Theorem 9.1).
2. A closed linear subspace of a Banach space is itself a Banach space
(Proposition 4.7).
3. ▶When X, Y are Banach spaces over the same ﬁeld, so is X ×Y (Proposition 4.7).
4. Cb(X, Y) is a Banach space whenever Y is (Theorem 6.23).
5. Not every normed space is complete (when inﬁnite dimensional).
(i) The set c00 of ﬁnite sequences (a0, . . . , an, 0, 0, . . .), n
√
N, is an
incomplete linear subspace of ∂⇔. For example, the vectors (1, 0, 0, . . .),
(1, 1
2, 0, 0, . . .), …, (1, 1
2, . . . , 1
n , 0, 0, . . .), …, form a Cauchy sequence which
does not converge in c00.
(ii) Take the vector space of continuous functions C[−1, 1] with the 1-norm
⊂f ⊂:=
 1
−1 | f (x)| dx. This is indeed a norm but it is not complete on that space.
For consider the sequence of continuous functions deﬁned by
−1
1
f 1
f 2
fn(x) :=
⎧
⎪⎨
⎪⎩
0
−1 ⩽x < 0
nx
0 ⩽x ⩽1/n
1
1/n < x ⩽1
.
It is Cauchy:
⊂fn −fm⊂=
 1
−1
| fn −fm| = 1
2
1
n −1
m
 ∞0, as n, m ∞⇔
but were it to converge to some f √C[−1, 1], i.e.,
 1
−1 | fn(x) −f (x)| dx ∞0,
then

7.4 Complete and Separable Normed Vector Spaces
107
 0
−1
| f (x)| dx = 0 =
 1
1/n
|1 −f (x)| dx,
so that f (x) = 0 on [−1, 0[ and f (x) = 1 on ]0, 1], implying it is discontinuous.
Similarly the set C[a, b] is not closed as a linear subspace of L2[a, b].
Proposition 7.17
Every normed space can be completed to a Banach space.
Proof Let %X be the completion of the normed space X (Theorem 4.6). We need to
prove that vector addition, scalar multiplication and the norm on X can be extended
to %X. Using the notation of Theorem 4.6, let x = [xn], y = [yn] be elements of %X,
with (xn), (yn) Cauchy sequences in X. Since
⊂xn + yn −xm −ym⊂⩽⊂xn −xm⊂+ ⊂yn −ym⊂∞0
⊂φxn −φxm⊂= |φ|⊂xn −xm⊂∞0
⊂xn⊂−⊂xm⊂
 ⩽⊂xn −xm⊂∞0,
as n, m ∞⇔, we ﬁnd that (xn + yn), (φxn) and (⊂xn⊂) are all Cauchy sequences.
For the same reasons, if (x◦
n) is asymptotic to (xn), and (y◦
n) to (yn), then (x◦
n + y◦
n)
and (xn + yn), (φx◦
n) and (φxn), and ⊂x◦
n⊂and ⊂xn⊂, are asymptotic to each other,
respectively. So we can deﬁne
x + y := [xn + yn],
φx := [φxn],
⊂x⊂:= lim
n∞⇔⊂xn⊂.
Note that ˜d(x, y) = ⊂x −y⊂. It is easy to check that they give a legitimate vector
addition, scalar multiplication and a norm; the required axioms follow from the same
properties in X and the continuity of these operations, e.g.
⊂x + y⊂= lim
n∞⇔⊂xn + yn⊂⩽lim
n∞⇔(⊂xn⊂+ ⊂yn⊂) = ⊂x⊂+ ⊂y⊂,
⊂x⊂= 0 ∀⊂xn⊂∞0 ∀x = [xn] = [0] = 0.
Note that the zero can be represented by the Cauchy sequence (0), and −x by
(−xn). Furthermore, recall that there is a copy of X in %X (as constant sequences);
the operations just deﬁned on %X reduce to the given operations on X, when restricted
to it.
⊓⇐
Proposition 7.18
A normed space X is separable if, and only if, there is a countable subset
A such that X = [[A]].

108
7
Normed Spaces
Proof If X = ¯A, such as when X is separable, then X = ¯A →[[A]] →X.
Conversely, suppose X = [[A]] with A countable; this means that for any vector
x, there is a linear combination of ai √A (ai ∗= 0), such that
⊂φ1a1 + · · · + φnan −x⊂< α
φi √R or C.
(7.1)
[[A]] is not countable (unless A →{ 0 }), but the set of (ﬁnite) linear combinations of
vectors in A using coefﬁcients from Q + iQ is countable (why? hint: ⎟
n(Q2)n is
countable). Choosing ri = pi + iqi √Q + iQ, such that |ri −φi| < α/n⊂ai⊂, and
combining with (7.1), we get
⊂r1a1 + · · · + rnan −x⊂⩽|r1 −φ1|⊂a1⊂+ · · · + |rn −φn|⊂an⊂+ α < 2α.
This shows that X is separable.
⊓⇐
7.5 Series
Sequences and convergence play a big role in metric spaces. Normed spaces allow
sequences to be combined with summation, thereby obtaining series x1 + · · · + xn.
Deﬁnition 7.19
A series ⎠
n xn is a sequence of vectors in a normed space obtained by addition,
(x1, x1 + x2, x1 + x2 + x3, . . .); the Nth term of the sequence is denoted by
N
⎞
n=1
xn. Therefore, a series converges when
x −
N
⎞
n=1
xn
 ∞0 for some x √X
as N ∞⇔; in this case the limit x is called its sum
x1 + x2 + · · · =
⇔
⎞
n=1
xn := lim
N∞⇔
N
⎞
n=1
xn = x.
A series is said to converge absolutely when ⎠
n ⊂xn⊂converges in R.
Examples 7.20
1. We can convert some results about convergence of sequences to series:
(a) ⎠
n(xn + yn) = ⎠
n xn + ⎠
n yn when the latter converge; similarly,
⎠
n φxn = φ ⎠
n xn.
(b) A series is Cauchy when xn + · · · + xm ∞0 as n, m ∞⇔.

7.5 Series
109
2. If a series converges both normally and absolutely, then

⇔
⎞
n=1
xn
 ⩽
⇔
⎞
n=1
⊂xn⊂.
Proof Take the limit of ⊂x1 + · · · + xn⊂⩽⊂x1⊂+ · · · + ⊂xn⊂as n ∞⇔.
3. There are series that converge but not absolutely. As an example, take any decreas-
ing sequence of positive real numbers an ∞0, then ⎠
n(−1)nan converges in R
(Leibniz); yet ⎠
n an may diverge.
Indeed, when ⎠
n an = ⇔and 0 ⩽an ∞0, the series ⎠
n ±an can converge to
any a √R by a judicious choice of signs. Take enough terms an to just exceed a,
then reverse sign to lower the sum to just less than a, then reverse sign again and
continue.
4. A rearrangement of a series need not converge; even if it does, it need not have
the same sum. For example,
1 −1
2 + 1
3 −1
4 + 1
5 −1
6 + 1
7 −1
8 + · · · ∞log 2,
1 −1
2 + 1
3 + 1
5 + 1
7 + 1
9 −1
4 + 1
11 + · · · ∞⇔,
1 −1
2 −1
4 + 1
3 −1
6 −1
8 + 1
5 −1
10 + · · · ∞1
2 log 2,
1 −1
2 + 1
3 + 1
5 −1
4 + 1
7 + 1
9 −1
6 + · · · ∞1.
5. The sum of a ‘sequence’ (xn)n√Z can also be given a meaning:
⎞
n√Z
xn =
⇔
⎞
n=−⇔
xn :=
⇔
⎞
n=1
x−n +
⇔
⎞
n=0
xn,
when the latter two series converge.
In general, absolute convergence does not imply, nor is it implied by, convergence
of ⎠
n xn. But for Banach spaces, one implication holds:
Proposition 7.21
A normed space X is complete if, and only if, any absolutely convergent
series in X converges.
Proof Let X be a Banach space, and suppose that ⎠
n ⊂xn⊂converges. Let yN :=
⎠N
n=0 xn, so that for M > N
⊂yM −yN⊂=

M
⎞
n=N+1
xn
 ⩽
M
⎞
n=N+1
⊂xn⊂∞0
as N, M ∞⇔.
Hence (yN) is a Cauchy sequence in the complete space X, and so converges.

110
7
Normed Spaces
Conversely, let X be a normed space for which every absolutely convergent series
converges. Let (xn) be a Cauchy sequence in X, so that for n, m ⩾Nα large enough,
⊂xn −xm⊂< α. Letting α := 1/2r, r = 1, 2, . . ., we can ﬁnd ever larger numbers nr
such that ⊂xnr+1 −xnr ⊂< 1/2r. Thus,
⇔
⎞
r=1
⊂xnr+1 −xnr ⊂⩽
⇔
⎞
r=1
1
2r = 1.
By assumption, since its absolute series converges, so does ⎠
r(xnr+1 −xnr ), i.e.,
xn1 −xnr = (xn1 −xn2) + (xn2 −xn3) + · · · + (xnr+1 −xnr )
converges as r ∞⇔. This forces the subsequence xnr to converge, and so must the
parent Cauchy sequence (xn) (Proposition 4.2).
⊓⇐
Series can be used to extend the idea of a basis as follows: a ﬁxed list of unit vectors
en is called a Schauder basis when for any x √X there are unique coefﬁcients ψn
such that
x =
⇔
⎞
n=1
ψnen.
This implies that X = [[e1, e2, . . . ]], and by necessity X must be separable (though
not every separable space has a Schauder basis [41]). Since a vector x = ⎠
n ψnen
is identiﬁed by its sequence of coefﬁcients (ψn) with respect to a Schauder basis, the
space X is essentially a sequence space (with norm ⊂(ψn)⊂:= ⊂⎠
n ψnen⊂X). There
are cases where a permutation of a Schauder basis does not remain a basis; if it does,
the basis is termed unconditional; again, not every space has an unconditional basis
(e.g. L1(R) and C[0, 1]).
Convergence Tests
Real series are easier to handle than series of vectors, and a number of tests for
absolute convergence have been devised:
Comparison Test. If ⊂xn⊂⩽⊂yn⊂then ⎠N
n=0 ⊂xn⊂⩽⎠N
n=0 ⊂yn⊂. If the lat-
ter converges to ⎠⇔
n=0 ⊂yn⊂, then ⎠
n ⊂xn⊂is increasing and bounded above, so
converges.
An important special case is comparison with the geometric series, ⊂xn⊂⩽rn
with r < 1, because 1 + r + r2 + · · · = 1/(1 −r). This leads to:
Root Test. Let r := lim supn ⊂xn⊂1/n;

7.5 Series
111
Juliusz Schauder (1899–1943), after ﬁghting in WWI, gradu-
ated at 24 years from the University of Lwow under Steinhaus
with a dissertation on statistics. He continued researching in
the Banach/Steinhaus school, giving the theory of compact op-
erators its modern shape; he proved that the adjoint of a com-
pact operator is compact, the Schauder ﬁxed point theorem,
and generalized aspects of orthonormal bases to Banach spaces;
later he specialized to partial diﬀerential equations. Along with
many other Polish academics, he was killed by the Nazis during
WWII.
Fig. 7.2 Schauder
(a) if r < 1 then the series ⎠
n xn is absolutely convergent,
(b) if r = 1 then the series may or may not converge,
(c) if r > 1 then the series diverges.
Proof (a) ⊂xn⊂⩽(r +α)n except for ﬁnitely many terms. Since the right-hand side is
a convergent geometric series when r < 1 and α is taken small enough, the left-hand
side series also converges by comparison.
(b) The series ⎠
n
1
n = ⇔and ⎠
n
1
n2 < 2 both have r = 1.
(c) When r > 1, ⊂xn⊂⩾(1 + α)n > 1 for inﬁnitely many terms, so the series
⎠
n ⊂xn⊂cannot possibly converge.
Ratio Test. (D’Alembert’s) If the ratios ⊂xn+1⊂/⊂xn⊂∞r then ⊂xn⊂1/n ∞r; it
is often easier to ﬁnd the ﬁrst limit, if it exists, than the second.
Proof The idea is that for large n, ⊂xn⊂≈r⊂xn−1⊂≈rn⊂x0⊂, so ⊂xn⊂1/n ≈r.
More precisely, for n ⩾N large enough,
r −α < ⊂xn⊂/⊂xn−1⊂< r + α,
∴
(r −α)n−N⊂xN⊂< ⊂xn⊂< (r + α)n−N⊂xN⊂,
∴
r −2α < ⊂xn⊂1/n < r + 2α,
since (r ± α)−N/n⊂xN⊂1/n ∞1.
Cauchy’s Test. If ⊂xn⊂is decreasing, then ⎠
n ⊂xn⊂converges ⇒⎠
n 2n⊂x2n⊂
converges.
Proof Let rn := ⊂xn⊂; the test follows from two comparisons,
r1 + r2 + · · · + r2n+1−1 = r1 + (r2 + r3) + · · · + (r2n + · · · + r2n+1−1)
⩽r1 + 2r2 + · · · + 2nr2n.
r1 + 2r2 + 4r4 + · · · + 2nr2n ⩽r1 + 2r2 + 2(r3 + r4) + · · · + 2(r2n−1+1 + · · · + r2n)
⩽2(r1 + r2 + · · · + r2n).

112
7
Normed Spaces
Kummer’s Test. Let ⎠
n
1
rn be a divergent series of positive terms and
rn+1
rn
⊂xn+1⊂
⊂xn⊂
= 1 −ψ
rn
+ o(1/rn).
If ψ > 0, then the series ⎠
n xn converges absolutely, otherwise when ψ < 0 the
series diverges. For example, rn := 1 gives the ratio test, rn := n is Gauss’s or
Raabe’s test, and rn := n log n is Bertrand’s test.
Proof When ψ > 0, we are given that c⊂xn⊂⩽rn⊂xn⊂−rn+1⊂xn+1⊂for n ⩾N
large enough, and some 0 < c < ψ. Summing up these inequalities results in
c(⊂xN⊂+ · · · + ⊂xm⊂) ⩽rN⊂xN⊂−rm+1⊂xm+1⊂⩽rN⊂xN⊂
so the series converges as it is increasing but bounded above.
When ψ < 0, we have rn⊂xn⊂< rn+1⊂xn+1⊂for n ⩾N large enough. Hence
⊂xn⊂> rN⊂xN⊂
rn
and the series diverges by comparison with the series ⎠
n
1
rn .
There are other tests, for example, Cauchy’s inequality shows that ⎠
n anbn con-
verges when ⎠
n a2
n and ⎠
n b2
n do.
Exercises 7.22
1. If a series ⎠
n xn converges, then xn ∞0. The converse is false:
1 + 1
2 + 1
3 + 1
4 + · · · + 1
n ∞⇔.
Moregenerally,foranyﬁxedk, xN +xN+1+· · ·+xN+k ∞0 and⎠⇔
n=N xn ∞0,
as N ∞⇔.
2. If ⎠
n ⊂xnm −xn⊂∞0 as m ∞⇔, then lim
m∞⇔
⎞
n
xnm =
⎞
n
lim
m∞⇔xnm =
⎞
n
xn, if the latter converges.
3. From the geometric series, it follows that 1 −a + a2 −a3 + · · · and ⎠
n arn
(rn ⩾n) converge for |a| < 1 in R.
4. The series 1 + 1
2! + 1
3! + · · · , ⎠
n
n
2n , and ⎠
n
2n
n! converge by comparison with
a geometric series (or using the ratio test).
5. 1 + 1
22 + 1
32 + · · · = θ2
6 . This series was too hard to sum before Euler; show
at least that it converges, using the comparison 1
n2 ⩽
1
n(n−1) =
1
n−1 −1
n .

7.5 Series
113
Generalize this to the case
1
(n−1)p−1 −
1
n p−1 ⩾p−1
n p to show that ⎠
n
1
n p converges
for p > 1. Deduce that ⎠
n
∩n+1
n2−1 converges, by comparison.
6. These last series are examples that converge slower than the geometric series; in
fact they are not decided by the root and ratio tests. Are there series that converge
even slower?
7. The Cauchy or Raabe tests can also be used to show that 1+ 1
2p + 1
3p +· · · con-
verges only when p > 1. Show further that ⎠
n
1
n , ⎠
n
1
n log n , ⎠
n
1
n log n log log n ,
…, diverge.
8. TheWeierstraßM-test (comparisontestfor L⇔):if⊂fn⊂L⇔⩽Mn where⎠
n Mn
converges, then ⎠
n fn converges in L⇔(A) (i.e., uniformly). Use it to show that
the function f (x) := ⎠⇔
n=1
xn
n2 converges uniformly on [−1, 1].
9. Let fn(x) := e−nx/n, then ⊂fn⊂L1[0,1] ⩽1/n2, and so ⎠
n fn converges in
L1[0, 1].
10. What is wrong with this argument: When ⊂xn⊂1/n ∞1, then ⊂xn⊂> (1 −α)n
for inﬁnitely many terms; the right-hand side sums to 1/α, which is arbitrarily
large; hence the series cannot converge absolutely.
11. A rearrangement of an absolutely convergent series also converges, to the same
sum. (Hint: Eventually, the rearranged series will contain the ﬁrst N terms.)
12. Suppose a series x1 + x2 + · · · is split up into two subseries, say x1 + x4 + · · ·
and x2 + x3 + · · · , denoted by ⎠
i xni and ⎠
j xn◦
j . If they both converge, to
x and y respectively, then the original series ⎠
n xn also converges, to x + y.
If one converges, and the other diverges, then the series ⎠
n xn diverges. But it
is possible for two subseries to diverge, yet the original series to converge; for
example, 1 −1
2 + 1
3 −1
4 + · · · ∞log 2.
13. Cesáro limit: A sequence (xn) is said to converge in the sense of Cesáro when
x1+···+xn
n
converges. Show that if a = limn∞⇔xn exists then the Cesáro limit is
also a. Show that the divergent sequence (−1)n is Cesáro convergent to 0.
Remarks 7.23
1. Weighted spaces are deﬁned similarly to ∂p and L p but with a different mea-
sure or weight. For example, an ∂1
w space with weights wn > 0 consists of
sequences with bounded norms ⊂x⊂∂1w := ⎠
n |xn|wn. Similarly, L2
w(A) has norm
(

| f (x)|2w(x) dx)
1
2 . In fact, weighted spaces are isomorphic to the unweighted
spaces; for example ∂1
w ≡= ∂1 via the map (xn) ∞(wnxn).
2. The second norm axiom requires that the ﬁeld be normed. A famous theorem by
Frobenius states that the only normed ﬁelds over the reals are R and C.

114
7
Normed Spaces
3. Cauchy’s inequality was known to Lagrange in the form
N
⎞
n=1
a2
n
N
⎞
m=1
b2
m −
 N
⎞
n=1
anbn
2
=
N
⎞
n=1
⎞
m>n
(ambn −anbm)2.
4. Hausdorff’s Maximality Principle ∀Axiom of Choice.
Proof Let A = { Aψ →X : ψ √I } be a collection of non-empty subsets of a set X.
Consider pairs (J, g) where J is a subset of I and g is an associated choice function
g: J ∞X, i.e., g(ψ) √Aψ for all ψ √J. To prove the axiom of choice we need to
show that there is a choice function f with domain I.
Let (J, g) →(%J, ˜g) mean that J →%J and ˜g extends g, i.e., ˜g(ψ) = g(ψ) whenever
ψ √J. By Hausdorff’s maximality principle, there is a maximal chain of nested sets
and their choice functions (Ji, gi). The union J := ⎟
i Ji also has a choice function,
namely f (ψ) := gi(ψ) whenever ψ √Ji, and it is the one sought for:
f is well-deﬁned: If ψ belongs to more than one index set, say Ji and Jj, then
without loss of generality, Jj →Ji and gi extends g j, say, so gi(ψ) = g j(ψ).
f isachoicefunctionon J:Ifψ √J thenψ √Ji forsomei,so f (ψ) = gi(ψ)√Aψ.
J = I: Otherwise there is some index δ √I∖J, and an element xδ √Aδ; f can
be extended further to ˜f deﬁned by ˜f (ψ) :=
#
f (ψ) ψ √J
xδ
ψ = δ . Then ˜f is a choice
function on its domain J ♦{δ}, and extends every choice function gi : Ji ∞X in
the maximal chain, a contradiction.
⊓⇐

Chapter 8
Continuous Linear Maps
8.1 Operators
In every branch of mathematics which concerns itself with sets having some partic-
ular structure, the functions which preserve that structure, called morphisms, feature
prominently. Such maps allow us to transfer equations from one space to another, to
compare them with each other and state when two spaces are essentially the same,
or if not, whether one can be embedded in the other, etc. Even in applications, it is
often the case that certain aspects of a process are conserved. For example, a rotation
of geometric space yields essentially the same space. The morphisms on normed
spaces are formalized by the following deﬁnition.
Deﬁnition 8.1
An operator1 is a continuous linear transformation T : X ∞Y between
normed spaces (over the same ﬁeld), that is, it preserves vector addition, scalar
multiplication, and convergence,
T (x + y) = T x + T y,
T (∂x) = ∂T x,
T ( lim
n∞√xn) = lim
n∞√T xn.
A functional is a continuous linear map δ : X ∞F from a normed space to
its ﬁeld. The set of operators from X to Y is denoted by B(X, Y), and the set
of functionals, denoted by X∗, is called the dual space of X.
1 The use of the term operator is not standardized: it may simply mean a linear transformation,
or even just a function, especially outside Functional Analysis. But it is standard to write T x
instead of T(x).
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_8,
115
© Springer International Publishing Switzerland 2014

116
8
Continuous Linear Maps
Easy Consequences
1. T 0 = 0.
2. T (√
n=0 ∂nxn) = √
n=0 ∂nT xn.
3. A linear map is determined by the values it takes on the unit sphere.
A simple test for continuity of a linear transformation is the following Lipschitz
or “bounded” property,
Proposition 8.2
A linear transformation T : X ∞Y is continuous if, and only if, T is a
Lipschitz map
⇒c > 0, →x ♦X,
∃T x∃Y ⩽c∃x∃X.
Proof The deﬁnition of a Lipschitz map reads, when applied for normed spaces,
∃f (x) −f (y)∃⩽c∃x −y∃for some c > 0. When f is in fact a linear map T , it
becomes ∃T (x −y)∃⩽c∃x −y∃, or equivalently, ∃T a∃⩽c∃a∃for all a ♦X. That
Lipschitz maps are (uniformly) continuous is true in every metric space (Examples
4.15(3)), but can easily be seen in this context. If xn ∞x, then T xn ∞T x, since
∃T xn −T x∃= ∃T (xn −x)∃⩽c∃xn −x∃∞0.
Conversely, suppose the ratios ∃T x∃/∃x∃are unbounded. Since scaling x does not
affect this ratio (because T is linear), there must be vectors xn such that ∃T xn∃= 1
but ∃xn∃⩽1/n. So xn ∞0 yet T xn ∀∞0, and T is not continuous.
∈⊆
Proposition 8.3
If T : X ∞Y is an operator,
(i) the image of a linear subspace A of X is a linear subspace T A :=
{ T x : x ♦A } of Y,
(ii) the pre-image of a closed linear subspace B of Y is a closed linear
subspace T −1B := { x ♦X : T x ♦B } of X.
The image and pre-image of convex subsets are convex.
In particular, its image im T
:= T X is a linear subspace; and its kernel
ker T := T −10 is a closed linear subspace.

8.1 Operators
117
Proof (i) Let T x, T y
♦
T A, then T x + T y
=
T (x + y)
♦
T A, and
∂T x = T (∂x) ♦T A.
(ii) Let x, y, xn ♦T −1B, that is, T x, T y, T xn ♦B, and let ∂♦F. Then
T (x + y) = T x + T y ♦B,
T (∂x) = ∂T x ♦B,
xn ∞a ⇒Ta = T ( lim
n∞√xn) = lim
n∞√T xn ♦B,
show that T −1B is a closed linear subspace.
(iii) Let T x, T y ♦T A, where A ⇐X is a convex subset. Then for any 0 ⩽t ⩽1,
z := tx + (1 −t)y is in A, so
t T x + (1 −t)T y = T (tx + (1 −t)y) = T z ♦T A
shows T A is also convex. Now let B ⇐Y be convex, and let x, y ♦T −1B, i.e.,
a := T x, b := T y are both in B. Then, by convexity of B,
T (tx + (1 −t)y) = ta + (1 −t)b ♦B
and tx + (1 −t)y ♦T −1B as required.
∈⊆
Examples 8.4
1. An operator T maps the linear subspace [[A]] to [[T A]] because
x =
n
⎛
i=1
αiai ⇒T x =
n
⎛
i=1
αiTai.
In particular it maps a straight line to another straight line (or to the origin),
hence the name “linear” applied to operators.
2. ▶A linear transformation from CN to CM takes the form of a matrix. Letting
CN = [[e1, . . . , eN]], CM = [[e⊂
1, . . . , e⊂
M]], x = N
i=1 αi ei =
⎜
⎝⎞
α1
...
αN
⎟
⎠⎫, and
T ei = 
j Tji e⊂
j, then
T x =
N
⎛
i=1
αiT ei =
N
⎛
i=1
αi
M
⎛
j=1
Tji e⊂
j =
⎜
⎝⎞
T11 . . . T1N
...
...
...
TM1 . . . TM N
⎟
⎠⎫
⎜
⎝⎞
α1
...
αN
⎟
⎠⎫.
Every matrix is continuous,
∃T x∃2 ⩽
M
⎛
j=1
|Tji|
N
⎛
i=1
|αi| ⩽
⎜
⎞
M
⎛
j=1
|Tji|
⎟
⎫N∃x∃2
(Exercise 7.7(7)).

118
8
Continuous Linear Maps
3. A functional from CN to F is then a 1 × N matrix, otherwise known as a row
vector,
δ
⎜
⎝⎞
a1
...
aN
⎟
⎠⎫= δ
⎜
⎞
N
⎛
n=1
anen
⎟
⎫=
N
⎛
n=1
δ(en)an =
N
⎛
n=1
bnan =
⎪b1 . . . bN
⎬
⎜
⎝⎞
a1
...
aN
⎟
⎠⎫.
4. Generalizing this to functionals on complex sequences, let y⇔(x) := y · x :=

n bnan, where x = (an) and y = (bn). Then y⇔is linear,
y · (x + x⊂) =
⎛
n
bn(an + a⊂
n) =
⎛
n
bnan +
⎛
n
bna⊂
n = y · x + y · x⊂,
y · (∂x) =
⎛
n
bn∂an = ∂
⎛
n
bnan = ∂y · x,
but may or may not be continuous, depending on y and the normed spaces
involved. For example, to show that δ(an) := √
n=1
(−1)nan
n2
deﬁned on γ√is
continuous, note
|δx| =
⎭⎭⎭
⎛
n
(−1)n
n2
an
⎭⎭⎭⩽
⎛
n
1
n2 sup
n
|an| ⩽2∃x∃γ√.
5. When X has a Schauder basis (en), a functional must be in the form of a series:
δx = δ
⎪⎛
n
anen
⎬
=
⎛
n
anδen =
⎛
n
bnan,
(bn := δen, an ♦F).
6. The identity operator I : X ∞X, x ∩∞x, is trivially linear and continuous.
Similarly for scalar multiplication, ∂: x ∩∞∂x.
7. ▶The left-shift operator L : γ1 ∞γ1 deﬁned by (an) ∩∞(an+1), i.e.,
L(a0, a1, a2, . . .) := (a1, a2, a3, . . .),
isonto,linear,continuous,andsatisﬁes∃Lx∃⩽∃x∃;itskernelisspannedby e0.
Proof That L is onto is obvious; linearity and continuity follow from
L(an + bn) = (a1 + b1, a2 + b2, . . .) = (a1, a2, . . .) + (b1, b2, . . .)
= L(an) + L(bn),
L(∂an) = (∂a1, ∂a2, . . .) = ∂(a1, a2, . . .) = ∂L(an),
∃Lx∃γ1 =
√
⎛
n=1
|an| ⩽
√
⎛
n=0
|an| = ∃x∃γ1

8.1 Operators
119
For x ♦ker L, (a1, a2, . . .) = Lx = 0, so x = (a0, 0, 0, . . .) = a0e0; in fact
Le0 = 0.
8. ▶In general, the multiplication of sequences x ∩∞yx, deﬁned by (bn)(an) :=
(bnan),islinearonthevectorspaceofsequences.When|bn| ⩽c,itiscontinuous
as a map γp ∞γp (p ⩾1); e.g. for p = 1,
∃yx∃γ1 =
√
⎛
n=0
|bnan| ⩽c
√
⎛
n=0
|an| = c∃x∃γ1.
In ﬁnite dimensions, this is equivalent to multiplying x by a diagonal matrix.
9. Solving linear equations T x = b, where T and b are given, is probably the single
mostusefulapplicationinthewholeofmathematics.Thecompletesetofsolutions
isx0 + ker T ,wherex0isanyindividualorparticularsolutionT x0 = b,andker T
isthesetofsolutionsofthehomogeneousequationT x = 0(sinceT (x −x0) = 0).
10. The kernel subspace of a functional ker δ is called a hyperplane.
Integral Operators
We now consider a broad class of operators that act on spaces of functions. An
integral operator (or transform) is a mapping on functions
T f (y) :=
⎧
A
k(x, y) f (x) dx,
where k is called the kernel of T (not to be confused
with ker T ). To motivate this deﬁnition, suppose T
is a linear operator that inputs a function f : A ⇐
R ∞C and outputs a function g: B ⇐R ∞C.
If A and B are partitioned into small subintervals,
the functions f and g are discretized into vectors
( f j) and (gi), and the linear operator T becomes
approximately some matrix [Ti j]. As the partitions
are reﬁned, one might hope that Tji would converge
to some function k(x, y) on A × B, and the ﬁnite
sums involved in the matrix multiplication 
j Ti j f j
become integrals
⎨
A k(x, y) f (x) dx. (This is not
necessarily the case, as the identity map attests.)
T f = g
=

120
8
Continuous Linear Maps
Proposition 8.5
An integral operator T f (y) :=
⎨
A k(x, y) f (x) dx is linear, and is continu-
ous on the following spaces:
sup
x♦A,y♦B
|k(x, y)| < √⇒T : L1(A) ∞L√(B),
⎧
B
sup
x♦A
|k(x, y)| dy < √⇒T : L1(A) ∞L1(B),
sup
y♦B
⎧
A
|k(x, y)| dx < √⇒T : L√(A) ∞L√(B),
⎧
B
⎧
A
|k(x, y)| dx dy < √⇒T : L√(A) ∞L1(B).
Proof Linearity follows easily from
⎧
A
k(x, y)(∂f (x) + g(x)) dx = ∂
⎧
A
k(x, y) f (x) dx +
⎧
A
k(x, y)g(x) dx.
(i) Continuity on L1(A) ∞L√(B):
∃T f ∃L√(B) ⩽sup
y♦B
⎧
A
|k(x, y) f (x)| dx ⩽sup
x,y
|k(x, y)|
⎧
A
| f (x)| dx.
(ii) Continuity on L1(A) ∞L1(B):
∃T f ∃L1(B) =
⎧
B
⎭⎭⎭
⎧
A
k(x, y) f (x) dx
⎭⎭⎭dy ⩽
⎧
B
sup
x♦A
|k(x, y)| dy
⎧
A
| f (x)| dx.
(iii) Continuity on L√(A) ∞L√(B):
∃f ∃L√(B) ⩽sup
y♦B
⎧
A
|k(x, y) f (x)| dx ⩽sup
y♦B
⎧
A
|k(x, y)| dx ∃f ∃L√(A)
(iv) Continuity on L√(A) ∞L1(B):
∃T f ∃L1(B) ⩽
⎧
B
⎧
A
|k(x, y)|| f (x)| dx ⩽
⎧
A×B
|k(x, y)| dx dy ∃f ∃L√(A)∈⊆

8.1 Operators
121
Examples 8.6
1. Integration, f ∩∞
⎨
A f , is a functional on L1(A).
2. The Volterra operator on L1[0, 1] is V f (y) :=
⎨y
0 f . It is an integral operator
with k(x, y) :=
⎩
1
x ⩽y
0
y < x .
3. ▶The Fourier transform of a function f ♦L1(R) is deﬁned to be the function
F f (ξ) = f (ξ) :=
⎧√
−√
e−2πixξ f (x) dx.
It is an operator F : L1(R) ∞L√(R).
4. For integral operators S, T , with kernels kS, kT respectively,
(a) S = T only when kS = kT a.e., (since for all f, (S −T ) f =
⎨
(kS(x, y) −
kT (x, y)) f (y) dy = 0);
(b) S + T has kernel kS + kT , and ∂T has kernel ∂kT ,
(c) ST has kernel kST (x, z) :=
⎨
kS(y, z)kT (x, y) dy.
The kernel acts like a “matrix” with real-valued indices, kx,y in place of Ai, j. The
properties listed here are analogous to those of the addition and multiplication of
matrices.
5. Which integral operators on L1(R) are translation-invariant, meaning T Ta f =
TaT f , where Ta f (x) = f (x −a)? The requirement is, for all f ♦L1(R),
⎧
k(x, y) f (x −a) dx =
⎧
k(x, y −a) f (x) dx.
By changing the x-variable in the left-hand integral to ˜x = x −a, we obtain
k(x + a, y) = k(x, y −a) a.e., as f is arbitrary. Equivalently, k(x, y) =
k(x −y, 0) =: k(x −y) a.e.(x, y) for some function k ♦L1(R). That is,
T f = k ∗f :=
⎧
k(x −y) f (y) dy
called the convolution of k with f .
6. An example of a functional that is not integral is given by πx0( f ) := f (x0), acting
on C(X), where x0 ♦X.
Proof Linearity is immediate, e.g. πx0( f +g) = ( f +g)(x0) = f (x0)+g(x0) =
πx0( f ) + πx0(g). For continuity,
|πx0 f | = | f (x0)| ⩽sup
x♦X
| f (x)| = ∃f ∃C(X).

122
8
Continuous Linear Maps
Vito Volterra (1860–1940) studied hydrodynamics at Pisa under
Betti (1883); this led him over the next ten years to consider
integral equations of the type f(x) −
x
a k(x, y)f(y) dy = g(x),
which he showed can be solved by iteration. He applied such
“functionals” to the theory of optics and distortions, Hamilton-
Jacobi dynamics, elasticity and electro-magnetism. He moved
from one professorship in Turin to another in Rome, becoming
a senator in 1905, and ﬁnding the time to write his Volterra
equations about the numbers of predators and prey in mathe-
matical biology, until in 1931 he preferred exile to the reign of
Mussolini.
Fig. 8.1 Volterra
7. Differentiation of functions is linear (say on the vector space of differentiable
functions) but it is not continuous in the √-norm, e.g.
∃D cos(nx)∃C(R) = ∃−n sin(nx)∃C(R) = n
whereas ∃cos(nx)∃C(R) = 1. Similarly, ∃Dxn∃C[0,1]/∃xn∃C[0,1] ∞√as
n ∞√. (Note: here, xn and cos(nx) denote functions.)
Theorem 8.7
B(X, Y) is a vector space with a norm deﬁned by
∃T ∃:= sup
x∀=0
∃T x∃Y
∃x∃X
= sup
∃x∃=1
∃T x∃Y .
B(X, Y) is complete when Y is complete. In particular, X∗is a Banach
space, with norm
∃δ∃= sup
x∀=0
|δx|
∃x∃.
Proof Thenormiswell-deﬁnedinthesensethatif T isanoperator,then∃T x∃/∃x∃⩽
c for all non-zero x ♦X, and the supremum ∃T ∃of such upperbounds c exists. In
fact, a linear map belongs to B(X, Y) if, and only if, ∃T ∃< √, in which case
∃T x∃⩽∃T ∃∃x∃.
This inequality is used extensively in the rest of the text.
Addition and scalar multiplication of operators is deﬁned by
(S + T )x := Sx + T x,
(∂T )x := ∂T x.

8.1 Operators
123
That B(X, Y) with these operations is a vector space is a straightforward calculation,
using the linearity and continuity of these operations in X and Y (Proposition 7.8).
For example,
(∂T )(x + y) = ∂T (x + y) = ∂T x + ∂T y = (∂T )x + (∂T )y.
More crucially,
∃S + T ∃= sup
∃x∃=1
∃Sx + T x∃⩽sup
∃x∃=1
(∃Sx∃+ ∃T x∃)
⩽sup
∃x∃=1
∃Sx∃+ sup
∃x∃=1
∃T x∃
= ∃S∃+ ∃T ∃
∃∂T ∃= sup
∃x∃=1
∃∂T x∃= sup
∃x∃=1
|∂|∃T x∃= |∂|∃T ∃
∃T ∃= 0 ∪→x ∃T x∃= 0 ∪T = 0.
B(X, Y) is complete if Y is : Let Tn be a Cauchy sequence of operators in B(X, Y),
that is, ∃Tn −Tm∃∞0 as n, m ∞√. Then, for each x ♦X,
∃Tnx −Tmx∃⩽∃Tn −Tm∃∃x∃∞0
implies that (Tnx) is a Cauchy sequence in Y, so that Tnx converges to some vector
which can be denoted by T (x), if Y is complete. We now show that T is linear:
Tn(x + y) = Tnx + Tny,
Tn(∂x) = ∂Tnx,
T (x + y)
T x + T y,
T (∂x)
∂T x,
as n ∞√
by continuity of addition and scalar multiplication.
Finally, for any β > 0 and any x ♦X,
∃(Tn −T )x∃⩽∃Tn −Tm∃∃x∃+ ∃Tmx −T x∃< β∃x∃+ β∃x∃,
where m is chosen large enough, depending on x, to make ∃Tmx −T x∃< β∃x∃,
and n, m ⩾N large enough to make ∃Tn −Tm∃< β. Hence ∃Tn −T ∃< 2β for
n ⩾N. This shows that Tn −T , and so T , are continuous, and furthermore that
Tn ∞T .
∈⊆
Proposition 8.8
If T : X ∞Y and S : Y ∞Z are operators, then so is their composition
ST , with ∃ST ∃⩽∃S∃∃T ∃.
B(X) := B(X, X) is closed under multiplication.

124
8
Continuous Linear Maps
Proof That ST is linear is obvious: ST (x + y) = S(T x + T y) = ST x + ST y and
ST (∂x) = S(∂T x) = ∂ST x. Also,
∃ST x∃= ∃S(T x)∃⩽∃S∃∃T x∃⩽∃S∃∃T ∃∃x∃,
and the result follows by taking the supremum for unit vectors x.
∈⊆
Examples 8.9
1. ∃0∃= 0, ∃I∃= 1; more generally, ∃∂I∃= |∂|.
2. ▶Every matrix T : RN ∞RM is continuous. Let a matrix T have coefﬁcients
Ti j, then similar reasoning as in Proposition 8.5 shows T is continuous with
(a) the 2-norms: ∃T ∃⩽

i j |Ti j|2, and
∃T ∃⩽

max
j (
⎛
i
|Ti j|) max
i (
⎛
j
|Ti j|),
(b) the √-norms: ∃T ∃= maxi

j |Ti j|,
(c) the 1-norms: ∃T ∃= max j

i |Ti j|.
Note that, just like vectors in RN, there are various norms applicable to matrices,
but that in any of them ∃T ∃depends continuously on its coefﬁcients: changing
them slightly by at most β does not change T drastically, e.g.
∃S −T ∃⩽N max
i j
|Si j −Ti j| ⩽Nβ.
Proof of (a). Let x = (ai). By Cauchy’s inequality,
⎭⎭
j Ti ja j
⎭⎭2 ⩽
j |Ti j|2

j |a j|2 for each i, so
∃T x∃2 =
⎛
i
⎭⎭⎭
⎛
j
Ti ja j
⎭⎭⎭
2
⩽
⎛
i j
|Ti j|2∃x∃2.
The second inequality, known as Schur’s test and sometimes an improvement
on the ﬁrst inequality, states that ∃T ∃is at most the geometric mean √cr of its
“largest” column and row. Again by Cauchy’s inequality,
→i,
⎭⎭⎭
⎛
j
Ti ja j
⎭⎭⎭⩽
⎛
j

|Ti j|

|Ti j||a j| ⩽
⎛
j
|Ti j|
⎛
j
|Ti j||a j|2,
∴
∃T x∃2 =
⎛
i
⎭⎭⎭
⎛
j
Ti ja j
⎭⎭⎭
2
⩽r
⎛
i j
|Ti j||a j|2 ⩽rc∃x∃2.
3. The norm of the operator y⇔is ∃y∃γ√when considered as a map γ1 ∞F.

8.1 Operators
125
Proof Taking x = (an), y = (bn),
|y · x| ⩽
⎛
n
|bn||an| ⩽(sup
n
|bn|)
⎛
n
|an| = ∃y∃γ√∃x∃γ1,
gives ∃y⇔∃⩽∃y∃γ√. Since the supremum ∃y∃γ√is a boundary point of the set
{ |bn| : n = 0, 1, . . . }, there is a sequence |bni | ∞∃y∃γ√, so that ∃y⇔∃⩾∃y∃γ√,
∃y⇔∃⩾|y · eni | = |bni | ∞∃y∃γ√
(∃eni ∃= 1).
4. ▶Any linear continuous operator on normed spaces, T : X ∞Y, is Lipschitz,
hence uniformly continuous. For example, it maps the ball Br(x) into the ball
B∃T ∃r(T x) (Exercise 4.17(3)). By Theorem 4.13, it can be extended uniquely to
an operator on their (Banach) completion spaces, T : X ∞Y. This extension
remains linear and continuous, and retains the same norm, ∃T ∃= ∃T ∃.
Proof For any vector x ♦X, there exist vectors xn ♦X such that xn ∞x; let
T x := limn∞√T xn. Then, for any other vector y ♦Y, with yn ∞y, yn ♦Y,
T (∂x + y) = lim
n∞√T (∂xn + yn) = lim
n∞√∂T xn + T yn = ∂T (x) + T (y)
∃T x∃= lim
n∞√∃T xn∃⩽∃T ∃lim
n∞√∃xn∃= ∃T ∃∃x∃.
So ∃T ∃⩽∃T ∃, but, as the domain of T includes that of T , equality holds.
5. ∃T ∃⩽∃S∃∀⇒∃T x∃⩽∃Sx∃, for example, T = I, S =
2 0
0 0

, x =
0
1

.
6. Let δ ♦X∗and y ♦Y; then the map yδ : x ∩∞(δx)y is continuous and linear,
with ∃yδ∃= ∃y∃∃δ∃.
Proof
∃yδ∃= sup
∃x∃=1
∃yδx∃= ( sup
∃x∃=1
|δx|)∃y∃= ∃δ∃∃y∃.
7. Suppose we wish to ﬁnd the solution of T x = y (T ♦B(X, Y)), but it is time-
consuming or impossible to calculate T −1. If S ♦B(X, Y) is easily inverted and
close to T , i.e., T = S + R and ∃R∃< ∃S−1∃−1, then ∃S−1R∃< 1, and the
iteration
xn+1 := xn + S−1(y −T xn) = S−1(y −Rxn)
converges to the solution of the equation by the Banach ﬁxed point theorem.
Exercises 8.10
1. Show that the following are continuous functionals,
(a) δx := √
n=1
1
n an on γ2;

126
8
Continuous Linear Maps
(b) δx := √
n=0 einωan, and δx := √
n=0 sin(nω)an on γ1 (ω is a ﬁxed real
number);
(c) π1x := a1 on γ1, γ2, γ√.
2. If (en) is a Schauder basis, with x = 
n anen for each x, show that the map
x ∩∞aN is linear. (That it is also continuous is true in a Banach space, but not
obviously.)
3. ▶The right-shift operator is deﬁned by R(an) := (0, a0, a1, . . .). It is an oper-
ator that satisﬁes ∃Rx∃= ∃x∃both as γ√∞γ√and γ1 ∞γ1; it is 1-1 and
its image is closed. Note that L R = I ∀= RL. Show that it is also continuous as
R : γ1 ∞γ√.
4. The mapping T : γ1 ∞γ1, deﬁned by T (an) := (a0, a1/2, a2/3, . . .), is linear
and continuous. It is 1-1, and its image, denoted γ1
1 := im T ◦γ1, is not closed
in γ1. (Hint: consider (1, 1/2, . . . , 1/n, 0, 0, 0, . . .).)
5. The mapping D : γ1
1 ∞γ1, deﬁned by D(an) := (nan), is linear and invertible,
but not continuous. (Hint: D(en/n) = en.)
6. Other examples of operators (on γ1 or γ√) are
S(an) := (a1, a0, a3, a2, . . .),
T (an) := (an+4 −an).
7. Conjugation in C, z ∩∞¯z, is continuous but not linear. It is conjugate-linear,
because ∂z = ¯∂¯z ∀= ∂¯z in general.
8. T [[A]] ⇐[[T A]] for a continuous linear operator T .
9. If a linear map is continuous at one point, say 0, then it is continuous everywhere.
10. When T : X ∞Y is 1–1 and linear, then the map x ∩∞∃T x∃is a norm on X.
11. When im T and/or ker T are ﬁnite-dimensional, their dimensions are called the
rank and nullity of T : X ∞Y. For matrices,
(a) rank(ST ) ⩽min(rank(S), rank(T )), rank(S + T ) ⩽rank(S) + rank(T ),
(b) rank(T )+ nullity(T ) = dim X,
(c) Sylvester’s inequality: nullity(ST ) ⩽nullity(S) + nullity(T ).
12. Typical examples of functionals acting on functions are of the form
f ∩∞
⎨
k(x) f (x) dx, where k has to satisfy certain properties for the func-
tional to be continuous. For example, δf :=
⎨√
0
e−x f (x) dx is a functional on
L√[0, √[.
13. The integral operator T f (y)
:=
⎨√
1
x−(y+1) f (x) dx is continuous as
L√[1, √[ ∞L√[1, √[, satisfying ∃T f ∃L√⩽∃f ∃L√.
14. Some examples of continuous linear maps on C(R) are:
(a) T f (x) := ( f (x) + f (−x))/2,

8.1 Operators
127
(b) Translations Ta f (x) := f (x −a); they are isometries and form a group
with TaTb = Ta+b, I = T0, T −1
a
= T−a,
(c) Multipliers Mg f (x) := g(x) f (x), where g ♦C(R).
What are their kernels and image subspaces?
15. Find, where possible, the norms of the above mentioned operators. For example,
∃πx0∃= 1 on C(X), and the Volterra operator on L√[0, 1] has norm 1.
16. It is not so easy to calculate ∃T ∃in general, even when T is a matrix. Show that,
with the Euclidean norms,

∂0
0 μ
 = max(|∂|, |μ|) and

0 1
0 0
 = 1 =

0
1
−1 0
. If you feel up to it, show that for real 2 × 2 matrices,

 a b
c d
  =

a2 + b2 + c2 + d2 +

((a −d)2 + (b + c)2)((a + d)2 + (b −c)2)
2
(Hint: Use Lagrange multipliers to ﬁnd the maximum of (ax +by)2+(cx +dy)2
subject to x2 + y2 = 1. See also Exercise 15.20(7).)
17. An integral operator T : L1[0, 1] ∞L√[0, 1], with kernal k ♦L√[0, 1]2,
has ∃T ∃⩽∃k∃L√. So if Tn have kernels kn with kn ∞k in L√[0, 1]2, then
Tn ∞T .
18. If Tnxn ∞0 for any choice of unit vectors xn, then Tn ∞0.
19. If S, T ♦B(X) commute, ST = T S, then S preserves ker T and im T .
20. An ‘afﬁne’ map f (x) := a +T x with T ♦B(X) is a contraction mapping when
∃T ∃< 1. The iteration xn+1 := a + T xn, starting from any x0, converges to its
ﬁxed point y = a + T y (Theorem 4.16).
21. Let Ax = b be a matrix equation, where A is a square matrix. Use Example 8.9(7)
above to describe iterative algorithms for ﬁnding the solution of the equation in
the following cases:
(a) (Jacobi) A is almost diagonal in the sense that A = D + R, with D being
the diagonal of A, and ∃R∃< ∃D−1∃−1.
(b) (Gauss-Seidel) A is almost a lower triangular matrix, in the sense that A =
L + U where L is lower triangular and ∃U∃< ∃L−1∃−1. The inverse of a
triangular matrix is fairly easy to compute.
22. Perturbation Theory. When the solution of an invertible linear equation Sx0 = y
is known, one can also ﬁnd the solutions of ‘nearby’ equations (S + βE)x = y,
where βE is a ‘perturbation’. Writing E = −ST , the new solution satisﬁes
(I −βT )x = x0. We might try an expansion of the type x = x0+βx1+β2x2+· · · ;
show that xn+1 = T xn, and the series converges if ∃E∃< ∃S−1∃−1 and β < 1.

128
8
Continuous Linear Maps
Isomorphisms
Wesometimesneedtoshowthattwonormedspacesareessentiallythesame,meaning
that any process involving addition, scalar multiplication, or convergence, in one
space is mirrored in precise fashion in the other space, and vice-versa. This is the
idea of an isomorphism.
Deﬁnition 8.11
An isomorphism between normed vector spaces is a bijective map T : X ∞Y
such that both T and T −1 are linear and continuous. The spaces are then said
to be isomorphic to each other, X ≈= Y.
An isometric isomorphism is an isomorphism that preserves distance,
∃T x∃Y = ∃x∃X for all x ♦X, and isometrically isomorphic spaces are denoted
by X ≡Y.
We say that X is embedded in Y, denoted X ◦
≈Y when X ≈= Z ⇐Y for
some subspace Z, and the isomorphism X ∞Z is called an embedding.
Thus, isomorphic normed spaces are isomorphic as vector spaces and homeomor-
phic (in fact equivalent) as metric spaces. Intuitively speaking, if X is embedded in
Y, one can treat it as if it were a subspace of Y even if its elements are not in Y.
Isomorphisms are also important in practical applications of functional analysis,
where linear equations of the type T x = y, with y given, are very common. Three
requirements are prescribed for such an equation to be well-posed: (i) a solution
exists, (ii) the solution is unique, and (iii) the solution is stable, i.e., small variations
in y do not lead to sudden large changes in x, in other words, x depends continuously
on y. In operator terminology, this means that T is (i) onto, (ii) 1-1, and (iii) T −1 is
continuous.
Proposition 8.12
If T : X ∞Y is a bijective linear map, then T −1 is linear, and is continuous
when c∃x∃X ⩽∃T x∃Y for some c > 0.
When T is an isomorphism, ∃T −1∃⩾∃T ∃−1.
Proof Let T be a bijective linear map, let x, y ♦X, and let u := T −1x, v := T −1y;
then T (u + v) = T u + T v = x + y, so that u + v = T −1(x + y). Similarly
T (∂u) = ∂T u = ∂x gives T −1(∂x) = ∂u = ∂T −1x. This shows T −1 is linear.
The inverse is continuous when ∃T −1y∃⩽c∃y∃for all y ♦Y, in particular for
y = T x: ∃x∃⩽c∃T x∃for all x ♦X. Since T is onto, the two inequalities are
logically equivalent.
By the previous proposition, 1 = ∃I∃= ∃T T −1∃⩽∃T ∃∃T −1∃.
∈⊆

8.1 Operators
129
Examples 8.13
1. ▶Suppose a vector space X is normed in two ways, giving two normed spaces
X∃·∃and X|||·|||. The two norms are equivalent if, and only if, the identity map
I : X∃·∃∞X|||·||| is an isomorphism (Example 7.3(9)); equivalently, there are
constants c, d > 0,
→x,
c∃x∃⩽|||x||| ⩽d∃x∃.
For example, RN with the 1-norm is equivalent to RN with the √-norm.
2. γ1 is not isomorphic to γ√. It is not enough to exhibit a sequence, such as
(1, 1, . . .), which belongs to γ√but not to γ1, because such a sequence may,
in principle, correspond to some other sequence in γ1. One must demonstrate a
property that γ1 satisﬁes but γ√doesn’t; e.g. we will show later on that the former,
but not the latter, is separable.
3. ▶The inequality c∃x∃⩽∃T x∃(c > 0), valid for all x in a Banach space X,
implies that im T is closed and T is 1 −1.
Proof If T x = T y, then c∃x −y∃⩽∃T x −T y∃= 0 and x = y. Suppose
T xn ∞y in Y; then c∃xn −xm∃⩽∃T xn −T xm∃∞0 as n, m ∞√, so (xn)
is Cauchy and converges to, say, x ♦X. By continuity of T , T xn ∞T x = y,
hence y ♦im T and im T is closed.
Exercises 8.14
1. (a) The map
⎪a1
a2
⎬
∩∞(0, a1, a2, 0, 0, . . .) embeds R2 in the real space γ1.
(b) The map J : (an) ∩∞(an/2n), γ√∞γ1, is 1 −1, linear, and continuous,
but is not an embedding (∃x∃γ√∀⩽c∃J x∃γ1).
2. An inﬁnite-dimensional space may be properly embedded in itself: for example,
the right-shift operator R : γ√∞im R ◦γ√is an embedding. This cannot
happen in ﬁnite dimensions.
3. Separate each sequence x = (an) into two parts xe := (a0, a2, . . .) and xo :=
(a1, a3, . . .). Then the map x ∩∞(xe, xo) is an isometric isomorphism γ1 ≡
γ1 × γ1.
4. The space γ1(Z) consists of ‘sequences’ . . . , a−2, a−1, a0, a1, a2, . . . such that
√
n=−√|an| < √. It contains γ1 as a proper subspace, even if γ1 ≡γ1(Z).
5. Consider a well-posed linear equation T x = y. An error πy in y gives a corre-
sponding ﬂuctuation πx in the solution x, T (x + πx) = y + πy. Show that
∃πx∃
∃x∃⩽∃T −1∃∃T ∃∃πy∃
∃y∃.

130
8
Continuous Linear Maps
The number ∃T −1∃∃T ∃is called the condition number of T . If it is relatively
large, then the equation is said to be ill-conditioned because the relative error of
the solution could be larger than that of the data.
6. * Let T : γ√∞γ√be an operator with matrix coefﬁcients Ti j, i.e., it maps a
sequence (ai)i♦N ♦γ√to (√
j=0 Ti ja j)i♦N ♦γ√. Suppose also that the matrix
is dominated by its diagonal, meaning that for some c > 0,
|Tii| −
⎛
j∀=i
|Ti j| ⩾c.
Then ∃T x∃⩾c∃x∃. (Hint: use |a + b| ⩾|a| −|b|.)
7. * If X1 and X2 are isomorphic then so are their completions X1 ≈= X2.
8. * If X1 ≈= X2 and Y1 ≈= Y2 then B(X1, Y1) ≈= B(X2, Y2).
Projections
Our next aim is to show ﬁrstly that all N-dimensional spaces are isomorphic to each
other (for each N), and secondly to seek an analogue of the ﬁrst isomorphism theorem
of vector spaces, namely V/ ker T ≈= im T . Accordingly we need to introduce an
important type of operator called a projection, and then construct quotient spaces.
Deﬁnition 8.15
A projection is a continuous linear map P : X ∞X such that P2 = P.
imP
ker P
x
P x
(I−P )x
For example, shadows are the projection of objects in R3
to shapes in a two-dimensional plane; a ﬂat object on the
ground is its own shadow.
Playing around with the deﬁnition gives a number of conse-
quences:
Examples 8.16
1. (I −P)2 = I −2P + P2 = I −P is also a projection.
2. (I −P)P = 0, so x ♦im P ∪x −Px = 0, and im P = ker(I −P) is a closed
subspace. Similarly im(I −P) = ker(I −I + P) = ker P.
3. Any x ♦X can be written as x = Px + (I −P)x ♦im P + ker P. If x ♦
im P ∩ker P = ker(I −P) ∩ker P, then x = Px + (I −P)x = 0, so that
X = im P ∇ker P.

8.1 Operators
131
4. Any linear map on a Banach space, which satisﬁes P2 = P, is automatically
continuous when im P and ker P are closed subspaces, but more powerful results
are needed to show this (Proposition 11.5).
Exercises 8.17
1. Show that the following are projections:
(a)
1 0
1 0

and 1
2
1 1
1 1

; they have the same image, but different kernels, and
their norms are √2 and 1 respectively.
(b) P :=
 0 0
0 1

and Q :=
 1 1
0 0

; ker P = im Q, so P Q = 0 is a projection
but QP is not.
(c) RL, where R and L are the shift-operators.
(d) xδ ♦B(X), where δ ♦X∗and x ♦X such that δx = 1; in this case,
X = [[x]] ∇ker δ.
2. If P and Q are commutative projections, then P Q projects onto im P ∩im Q,
and P + Q −P Q projects onto im P + im Q.
3. By induction, if I = P1 + · · · + Pn, with the projections Pi satisfying Pi Pj = 0
for i ∀= j, then X = im P1 ∇· · · ∇im Pn.
4. ** Given a closed linear subspace, is there always a projection that maps onto it?
8.2 Quotient Spaces
A linear subspace M of a vector space can be translated to form cosets x + M. For
example, a straight line L ◦R2 passing through the origin, gives the parallel copies
x + L. Except that with some translations, the resulting line is indistinguishable from
L; it is easy to see that x + L = L ∪x ♦L. More generally, x + L = y + L ∪
x −y ♦L. This latter is an equivalence relation (check!), so the space R2 ‘foliates’
into a stack of parallel lines, each a coset x + L. It is obvious that when a line L
is translated by x, and then by y, the result is the line (x + y) + L; in fact, since
translation in the direction of a ♦L is irrelevant to the coset, one can even talk about
the addition of lines, (x + L) + (y + L) as meaning x + (y + L). Similarly lines can
be stretched, ∂(x + L) = ∂x + L (unless ∂= 0), and the distance between lines is
deﬁned in elementary geometry as the minimum distance between them. This space
of parallel lines is a good candidate for a normed space.
Turning to the general case, a vector space partitions into the cosets of M to form
a vector space X/M, which is normed when M is closed, and complete when X is
complete:

132
8
Continuous Linear Maps
Proposition 8.18
If X is a normed space and M is a closed linear subspace, then the space
of cosets
X/M: = { x + M : x ♦X }
is a normed space with addition, scalar multiplication, and norm deﬁned
by
(x + M) + (y + M) := (x + y) + M,
∂(x + M) := ∂x + M,
∃x + M∃= d(x, M) := inf
v♦M ∃x −v∃.
If M is complete, then X/M is complete ∪X is complete.
Proof That the relation x−y ♦M is an equivalence relation with equivalence classes
x + M, and that the deﬁned addition and scalar multiplication of these classes satisfy
the axioms of a vector space should be clear; the zero coset is M and the negative of
x + M is −x + M. Let us show that we do indeed get a norm:
∃(x + M) + (y + M)∃= ∃x + y + M∃= inf
w♦M ∃x + y −w∃
=
inf
u,v♦M ∃x + y −u −v∃
⩽
inf
u,v♦M(∃x −u∃+ ∃y −v∃)
= inf
u♦M ∃x −u∃+ inf
v♦M ∃y −v∃
= ∃x + M∃+ ∃y + M∃
∃∂(x + M)∃= ∃∂x + M∃= inf
v♦M ∃∂x −v∃
= inf
u♦M ∃∂x −∂u∃
(for ∂∀= 0)
= inf
u♦M |∂|∃x −u∃
= |∂|∃x + M∃
∃x + M∃= inf
v♦M ∃x −v∃⩾0.
∃x + M∃= 0 ∪d(x, M) = 0 ∪x ♦M = M ∪x + M = 0
+ M (Exercise 2.20(9)).

8.2 Quotient Spaces
133
Completeness. Let xn + M be an absolutely convergent series in X/M, i.e.,

n ∃xn + M∃converges. Now, for each n, there is a vn ♦M such that
∃xn −vn∃⩽∃xn + M∃+ 1/2n.
The left-hand side can be summed by comparison with the right, so 
n(xn −vn)
converges to some x, since X is complete (Proposition 7.21). Thus

N
⎛
n=1
(xn + M) −(x + M)
 =

N
⎛
n=1
xn −x + M
 ⩽

N
⎛
n=1
(xn −vn) −x
 ∞0
since in general ∃a + M∃⩽∃a + v∃for any v ♦M. Hence 
n(xn + M) converges,
along with every other absolutely summable series, and X/M is complete.
Conversely, let (xn) be a Cauchy sequence in X; then
∃(xn + M) −(xm + M)∃= ∃xn −xm + M∃⩽∃xn −xm∃
implies that (xn + M) is Cauchy in X/M, so converges to, say, x + M. This means
there are vn ♦M such that xn −(x + vn) ∞0; but then,
∃vn −vm∃⩽∃xn −xm −vn + vm∃+ ∃xn −xm∃∞0
shows (vn) is Cauchy in M and converges to, say, v ♦M. Thus xn ∞x + v.
∈⊆
If M is a linear subspace of X such that X/M is ﬁnite dimensional, then its
codimension is deﬁned by codim M: = dim(X/M).
Examples 8.19
1. The cosets of the closed subspace M := [[
⎪1
1
⎬
]] ◦R2 are the lines parallel to M,
and R2/M ≈= R. xs
Proof A vector x belongs to x0 + M when x =
⎪a0
b0
⎬
+t
⎪1
1
⎬
for some t ♦R, which
is the equation of a line parallel to
⎪1
1
⎬
. The map a ∩∞
⎪a
0
⎬
+ M, R ∞R2/M is
linear and continuous. It is bijective since
⎪a
b
⎬
+ M =
⎪a−b
0
⎬
+ M and
a1
0

−
a2
0

♦M ∪⇒∂,
a1 −a2
0

= ∂
 1
1

∪a1 = a2.
The inverse map is continuous as the distance ∃
⎪a
0
⎬
+ M∃equals |a|/√2.
2. If X is ﬁnite-dimensional, then so is X/M, with
codim M = dim X/M = dim X −dim M.

134
8
Continuous Linear Maps
Proof Let e1, . . . , em be a basis for M, extended by em+1, . . . , en to a basis for
X. Then, for any vector x = n
i=1 ∂iei, its coset,
x + M =
n
⎛
i=1
∂iei + M =
n
⎛
i=m+1
∂i(ei + M),
isgeneratedbyem+1 + M, . . . , en + M.Moreover,thesearelinearlyindependent,
since
n
⎛
i=m+1
∂iei + M = 0 + M ∪
n
⎛
i=m+1
∂iei =
m
⎛
i=1
αiei ♦M
∪∂i = 0, i = m + 1, . . . , n.
Hence dim X/M = n −m.
3. If δ ♦X∗then ∃x + ker δ∃= |δx|
∃δ∃.
Proof For δx ∀= 0, then X = [[x]] ∇ker δ, and
∃δ∃= sup
y∀=0
|δy|
∃y∃=
sup
a♦ker δ
|∂||δx|
∃∂x + a∃=
|δx|
inf
a♦ker δ ∃x + a∃
The following proposition states, in effect, that when one translates a closed linear
subspace to any distance c < 1 from the origin, the resulting coset intersects the unit
sphere:
Proposition 8.20 Riesz’s lemma
For any non-trivial closed linear subspace M, and 0 ⩽c < 1, there is a
unit vector x such that ∃x + M∃= c.
Proof Let y ∀♦M so that ∃y + M∃> 0; by re-scaling y if necessary, one can assume
∃y + M∃= c. The map f : M ∞R, deﬁned by f (a) := ∃y + a∃, takes values close
to c, as well as arbitrarily large values (∃y + ∂a∃⩾|∂|∃a∃−∃y∃∞√as ∂∞√,
for M ∀= 0). Since M is connected, and f is continuous, its image must include ]c, √[
by the intermediate value theorem (Proposition 5.6). In particular there is an a ♦M
such that ∃y + a∃= 1, so letting x := y +a gives ∃x + M∃= ∃y + M∃= c.
∈⊆
Exercises 8.21
1. ▶The mapping x ∩∞x + M, X ∞X/M, is linear and continuous.
2. Let M := { f ♦C[0, 1] : f (0) = 0 }, then 2 + M = { f ♦C[0, 1] : f (0) = 2 },
and C[0, 1]/M ≈= C.

8.2 Quotient Spaces
135
3. (a) X/X ≡0, X/0 ≡X.
(b) If X, Y are normed spaces, then X × Y
X × 0 ≡Y.
4. Let X be a ﬁnite-dimensional space generated by a set of unit vectors E :=
{ ei : i = 1, . . . , n }, and let Mi := [[E \ { ei }]]. Then the coefﬁcient |αi| in
x = n
i=1 αiei is at most ∃x∃/∃ei + Mi∃. Thus, in ﬁnding a basis for X, it is
best to select unit vectors that are as ‘far’ from each other as possible.
5. Let M be a closed subspace of X. If both M and X/M are separable, then so is X.
8.3 RN and Totally Bounded Sets
That ﬁnite-dimensional normed spaces ought to be better behaved than inﬁnite-
dimensional ones is to be expected. What is slightly surprising is the following result
that they allow only a unique way of deﬁning convergence: Any norm on CN is
equivalent to the complete Euclidean norm. This is an example of a mathematical
“small is beautiful” principle, in the same league of results as “ﬁnite integral domains
are ﬁelds”.
Theorem 8.22
Every N-dimensional normed space over C is isomorphic to CN, and so is
complete.
The theorem is also true for real ﬁnite-dimensional normed spaces: they are iso-
morphic to RN.
Proof Let X be an N-dimensional normed space, with a basis of unit vectors
e1, . . . , eN, and let CN be given the complete 1-norm (Example 7.16(3)). There
is a map between them, J : CN ∞X, deﬁned by
x =
⎜
⎝⎞
α1
...
αN
⎟
⎠⎫∩∞α1e1 + · · · + αN eN.
Linearity of J follows from the distributive laws of vectors; that it is 1-1 and onto
follow from the linear independence and spanning of { en } respectively.
J is continuous since
∃J x∃X = ∃α1e1 + · · · + αN eN∃X
⩽|α1| + · · · + |αN|
= ∃x∃1

136
8
Continuous Linear Maps
To show J −1 is continuous, let f (x) := ∃J x∃X, which is a composition of two
continuous functions: the norm and J. The unit sphere S := { u ♦CN : ∃u∃1 = 1 }
is a compact set (since it is closed and bounded in CN = R2N (Corollary 6.20)), so
f S is also compact (thus closed in R). One point that is outside f S is 0,
f (x) = 0 ∪∃J x∃= 0 ∪J x = 0 ∪x = 0.
Zero is therefore an exterior point contained in an open interval ]−c, c[ outside f S.
This means that c ⩽∃Ju∃for any unit vector u. Applying this to u = x/∃x∃1 for any
(non-zero) vector x ♦CN, we ﬁnd c∃x∃1 ⩽∃J x∃as required (Proposition 8.12).
Clearly, the proof does not depend critically on the use of complex rather than
real scalars.
∈⊆
Proposition 8.23 Riesz’s theorem
A subset K of a normed space X is totally bounded ∪K is bounded and
lies arbitrarily close to ﬁnite-dimensional subspaces, meaning
→β > 0, ⇒Y N-dim subspace of X, →x ♦K,
∃x + Y∃< β.
Balls are totally bounded only in ﬁnite-dimensional normed spaces.
Proof (i) Let K ⇐N
i=1 Bβ(xi) be a totally bounded set in the normed space X,
and let Y := [[x1, . . . , xN]]. Any point x ♦K is covered by some ball Bβ(xi), i.e.,
∃x −xi∃< β, so that ∃x + Y∃= inf y♦Y ∃x −y∃< β. Since β can be chosen
arbitrarily small, this proves one implication in the ﬁrst statement.
In a ﬁnite-dimensional normed space, bounded sets are totally bounded: This is
true for CN because balls (and their subsets) are totally bounded (Exercise 6.9(2)).
Any ﬁnite-dimensional space Y has an isomorphism J : CN ∞Y by the previous
theorem. If A is a bounded subset of Y, J −1 A is a bounded set in CN (Exercise
4.17(3)), hence totally bounded; mapping back to Y, A = J J −1 A is totally bounded
(Proposition 6.7).
For the converse of the proposition, suppose K is bounded by r, and lies within β
of an N-dimensional subspace Y. This means that if x ♦K then ∃x∃⩽r, and there
is a y ♦Y such that ∃x −y∃< β, so
∃y∃⩽∃x∃+ ∃y −x∃< r + β.
But we have just seen that the ball Br+β(0) ∩Y is totally bounded in Y, and can be
covered by a ﬁnite number of β-balls, Bβ(yi), i = 1, . . . , n. In particular, there is
some yi for which ∃y −yi∃< β, and so

8.3
RN and Totally Bounded Sets
137
∃x −yi∃⩽∃x −y∃+ ∃y −yi∃< 2β,
⇒
K ⇐n
i=1 B2β(yi).
K
Y
yi
y
x
(ii) Suppose X has a totally bounded ball, which by re-scaling and translation can
be taken to be the unit ball BX (Proposition 7.5). It must be within β < 1
2 of a
ﬁnite-dimensional closed subspace Y. In fact X = Y, otherwise we can use Riesz’s
lemma to ﬁnd a vector y ♦BX with d(y, Y) = ∃y + Y∃⩾1
2 > β.
∈⊆
Examples 8.24
1. All norms on CN are equivalent.
2. Given a point x ♦X and a ﬁnite-dimensional subspace M, there is always
a best approximation a ♦M to x. We need only look in the compact ball
B := B∃x∃[0] ∩M, and since the function a ∩∞∃a −x∃on it is continuous,
it achieves the minimum (Corollary 6.16).
For example, there is always a polynomial of degree at most n that best approxi-
mates a function with respect to any given norm.
3. If M is a complete subspace of a normed space, and N a ﬁnite-dimensional
subspace, then M + N is complete (see Example 7.11(2)).
Proof It is enough to show that M +[[e]] is complete when e ∀♦M; the result then
follows by induction. For any x ♦M, α ♦C,
|α|∃e + M∃= ∃αe + M∃⩽∃αe + x∃,
∃x∃⩽∃x + αe∃+ |α|∃e∃⩽c∃αe + x∃.
So if (xn + αne) is a Cauchy sequence in M + [[e]], then so are (αn) and (xn), in
C and M respectively. Hence, xn + αne ∞x + αe ♦M + [[e]].
Exercises 8.25
1. Totally bounded sets cannot be open (or have a proper interior) in an inﬁnite
dimensional normed space.
2. The set of polynomials of degree at most n forms a closed linear subspace of
L1[a, b] with dimension n + 1; a basis for this space is 1, x, . . . , xn.
3. As anillustrationof Riesz’s theorem, theunit ball intheinﬁnite-dimensional space
γ√(or γ1) is not totally bounded. (Hint: Show (en) has no Cauchy subsequence.)
4. In ﬁnite-dimensional normed spaces only, the compact sets are the closed and
bounded ones.
5. Totally bounded sets need not lie in a ﬁnite-dimensional subspace, just arbitrarily
close to them. Can you think of an inﬁnite-dimensional totally bounded set?

138
8
Continuous Linear Maps
6. * A space generated by an inﬁnite but countable number of linearly independent
vectors, [[e1, e2, . . . ]], cannot be complete: the linear subspaces [[e1]], [[e1, e2]], . . .
are closed in it, and do not have an interior (why?), so by Baire’s theorem their
union cannot be a complete metric space.
Remarks 8.26
1. Continuous operators are widely referred to as being “bounded”; except for the
zero operator, their image is certainly not bounded! The reason they are called so
is that, being Lipschitz maps, they send bounded sets to bounded sets. The usage
of “bounded” is avoided in this text, in favor of the equivalent term “continuous”.
2. By analogy with matrices, it is customary to write T x instead of T (x). This is a
slight abuse of notation; a linear map on the vector space of matrices need not act
on the left, e.g. A ∩∞AB, A ∩∞AB + B A, A ∩∞A⇔, and A ∩∞B−1 AB are all
linear.
3. For the initiated, the idea of continuous linear maps can be extended to continuous
multi-linear maps (tensors); they also form a Banach space with norm
∃T ∃:= sup |T (x1, . . . , δ1, . . .)|/∃x1∃. . . ∃δ1∃. . . .
4. B(X, Y) forms part of the larger space of Lipschitz functions X ∞Y. For such
functions, ∃f ∃:= supx1∀=x2♦X ∃f (x1) −f (x2)∃/∃x1 −x2∃satisﬁes the norm
axioms, except that ∃f ∃= 0 ∪f is constant.

Chapter 9
Main Examples
Having ﬂeshed out a substantial amount of abstract theory, we turn to the concrete
examples of normed spaces and identify which are complete and separable. Unavoid-
ably, the proofs become more technical once we leave the familiarity of ﬁnite dimen-
sions and enter the realm of inﬁnite-dimensional spaces, having to deal as it were
with sequences of sequences or functions and different types of norms. However, a
careful study of this section will be rewarded by having an armory of spaces, so to
speak, ready to serve as examples to conﬁrm or refute conjectured statements.
9.1 Sequence Spaces
The Space ℓ∞
A sequence in ∂∞is a sequence of sequences, xn = (ani). Convergence in ∂∞means
uniform convergence of the components, that is,
xn √0 ∗supi |ani| √0 as n √∞
∗|ani| √0 as n √∞, uniformly for all components i,
∗⇒δ > 0, →N, ⇒n ♦N, ⇒i,
|ani| < δ.
For example, of the following three sequences of sequences, only the ﬁrst converges
to 0, even though each component converges to 0.
(1, 1, 1, 1, . . .)
(1, 0, 0, 0, . . .)
(1, 1, 1, 1, 1, . . .)
( 1
2, 1
2, 1
2, 1
2, . . .)
(0, 1, 0, 0, . . .)
(0, 0, 1, 1, 1, . . .)
( 1
3, 1
3, 1
3, 1
3, . . .)
(0, 0, 1, 0, . . .)
(0, 0, 0, 0, 1, . . .)
...
...
...
∃
∀∃
∀∃
(0, 0, 0, 0, . . .)
(0, 0, 0, 0, . . .)
(0, 0, 0, 0, . . .)
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_9,
139
© Springer International Publishing Switzerland 2014

140
9
Main Examples
Theorem 9.1
∂∞is complete but not separable.
Proof (i) Let (xn) be a Cauchy sequence in ∂∞, i.e., ∈xn −xm∈∂∞√0 as
n, m √∞. Note that ∈xn∈∂∞⩽c since Cauchy sequences are bounded (Exam-
ple4.3(5)).
x1 a11 a12 a13 . . . ⩽∈x1∈∂∞
x2 a21 a22 a23 . . . ⩽∈x2∈∂∞
...
...
...
...
...
∃
∃
∃
∃
x
a1 a2 a3 . . .
⩽c
(The absolute signs of ani are omitted in the horizontal rows.)
For each column i, |ani −ami| ⩽∈xn −xm∈∂∞√0, so (ani) is a Cauchy
sequence in C, which converges to, say, ai := limn√∞ani.
That x := (ai) is in ∂∞follows from taking the limit n √∞of
|ani| ⩽∈xn∈∂∞⩽c.
More crucially, xn √x in ∂∞since, for each column i and any n ⊆N, one can
choose an m ⩾n large enough that |ami −ai| < 1/n, so that
|ai −ani| ⩽|ai −ami| + |ami −ani| ⩽1
n + ∈xm −xn∈∂∞√0,
as n √∞, independently of i.
(ii) To show ∂∞is not separable we display an uncountable number of disjoint balls
(Exercise4.21(4)). Consider the sequences that consist of 1s and 0s. The distance
between any two of them is exactly 1, so that the balls centered on them with radius
1/2 are disjoint. Moreover, these sequences are uncountable for the same reason that
the real numbers are uncountable: If one were able to list them as
x1 = (a11, a12, a13, . . .)
x2 = (a21, a22, a23, . . .)
x3 = (a31, a32, a33, . . .)
...
one could take the diagonal sequence (a11, a22, . . .), and swap its 1s and 0s, giving
a sequence (1 −ann) that cannot be in the list, for 1 −ann ∀= ann.
⊓⇐

9.1 Sequence Spaces
141
Proposition 9.2
The space of convergent complex sequences, and of those sequences that
converge to 0,
c := { (an) : →a ⊆C, lim
n√∞an = a },
c0 := { (an) : lim
n√∞an = 0 },
are complete separable subspaces of ∂∞.
Proof The spaces are nested in each other as c0 ⊂c ⊂∂∞since convergent
sequences are bounded. They are easily shown to be linear subspaces: an + bn √
a + b, αan √αa when an √a and bn √b as n √∞.
c0 is closed in ∂∞: Let xn √x in ∂∞, with xn ⊆c0; their components converge
uniformly ani √ai as n √∞.
x1 a11 a12 a13 . . . √0
x2 a21 a22 a23 . . . √0
...
...
...
∃
∃
∃
∃
∃
x
a1 a2 a3 . . .
?√0
Now, for any δ > 0, there is an xn in c0 such that ∈xn −x∈∂∞< δ, and for this
sequence, there is an integer N, such that
i ⩾N ⇔|ani| < δ.
It follows that for i ⩾N,
|ai| ⩽|ani| + |ai −ani| ⩽|ani| + ∈x −xn∈∂∞< 2δ
so limi√∞ai = 0 and x ⊆c0.
c0 is separable: The vectors en := (γni) = (0, . . . , 0, 1, 0, . . .), with the 1 occur-
ring at the nth position, form a Schauder basis for c0: for any x = (an) ⊆c0,
x −
N

n=0
anen

∂∞= sup
n>N
|an| √0, as N √∞.
If 
n anen = 
n bnen, then (a0 −b0, a1 −b1, . . .) = 0 hence an = bn and the
coefﬁcients are unique.

142
9
Main Examples
The spaces c and c0 are isomorphic: Let J : c √c0 ⊂∂∞be deﬁned by
J(a0, a1, a2, . . .) := (−a, a0 −a, a1 −a, . . .),
where a := lim
n√∞an.
J is 1–1 since
J(an) = J(bn) ⇔a = b and ⇒n, an −a = bn −b ⇔(an) = (bn).
J is onto c0 for, given any y = (bn) ⊆c0, it is clear that x := (b1 −b0, b2 −b0, . . .)
maps to it. In fact, writing 1 := (1, 1, . . .),
J x = Rx −a1,
J −1 y = L y −b01,
where R and L are the shift operators. This observation shows that both J and J −1
are continuous and linear since (an) ∩√a, as well as (bn) ∩√b0, are functionals
|a| = | lim
n√∞an| = lim
n√∞|an| ⩽sup
n
|an| = ∈(an)∈∂∞
|b0| ⩽sup
n
|bn| = ∈(bn)∈∂∞.
It follows that c has the same properties of completeness and separability that c0
enjoys.
⊓⇐
Theorem 9.3
Every functional on c0 is of the type (an) ∩√
n bnan where (bn) ⊆∂1, and
c∪
0 ≡∂1.
Proof Given y = (bn) ⊆∂1 and x = (an) ⊆c0, the inequality
|y · x| =

∞

n=0
bnan
 ⩽
∞

n=0
|bn||an| ⩽sup
n
|an|
∞

n=0
|bn| = ∈x∈∂∞∈y∈∂1
shows that the linear map y◦: x ∩√y · x := ∞
n=0 bnan (Example8.4(4)) is
well-deﬁned and continuous on ∂∞(including c0), with ∈y◦∈⩽∈y∈∂1.
Every functional on c0 is of this type: By the linearity and continuity of any
ξ ⊆c0∪,
ξx = ξ
 ∞

n=0
anen

=
∞

n=0
anbn = y · x,
where bn := ξen, y := (bn).

9.1 Sequence Spaces
143
Also, writing bn = |bn|eiθn in polar form,
∞

n=0
|bn| =
∞

n=0
e−iθnξen = ξ
 ∞

n=0
e−iθn en

⩽∈ξ∈∈(e−iθn)∈∂∞= ∈ξ∈,
hence y ⊆∂1, with ∈y∈∂1 ⩽∈ξ∈= ∈y◦∈. Combined with the above, we get
∈y∈∂1 = ∈y◦∈.
Isometric isomorphism: Let J : ∂1 √c∪
0 be the map y ∩√y◦. The above
conclusions can be summarized as stating that J is an onto isometry. That J is linear
is easily seen from the following statement that holds for every x ⊆c0, u, v, y ⊆∂1,
(u + v) · x = ∞
n=0(un + vn)an = ∞
n=0 unan + ∞
n=0 vnan = u · x + v · x,
(αy) · x = ∞
n=0(αbn)an = α ∞
n=0 bnan = α(y · x),
so (u + v)◦= u◦+ v◦and (αy)◦= αy◦.
⊓⇐
Exercises 9.4
1. The kernel of the functional Lim : (an) ∩√lim
n√∞an on c, is c0.
2. Any convergent complex sequence an √a can be written as
(an) =

n
(an −a)en + a1,
where 1 := (1, 1, . . .). Deduce that the vectors en together with 1 form a
Schauder basis for c; what is its dual space c∪?
3. ▶One can multiply bounded sequences together as (an)(bn) := (anbn), to
get another bounded sequence, ∈x y∈∂∞⩽∈x∈∂∞∈y∈∂∞. This multiplication is
commutative and associative, and has unity 1. Only those sequences which are
bounded away from 0 (i.e., |an| ⩾c > 0) have an inverse, namely (an)−1 =
(a−1
n ).
4. * The inequality ∈x y∈∂1 ⩽∈x∈∂∞∈y∈∂1 is also true, so the map x ∩√Mx, where
Mx y := x y, embeds ∂∞in B(∂1).
5. The vector space [[e0, e1, . . . ]] is often denoted by c00; it is the space of sequences
with a ﬁnite number of non-zero terms. Its closure in the ∂∞-norm is c00 = c0.
6. c0 contains the space of sequences ∂∞
s
:= { (an) : →c, ⇒n ⩾1, |an| ⩽c/ns }
(s > 0). What is its closure? Can you think of a sequence which is in c0 but not
in any ∂∞
s ?
7. The distance between a sequence (an) ⊆∂∞and c0 is lim supn |an|.
8. * C[0, 1] can be embedded in ∂∞, since f ⊆C[0, 1] is determined by its values
on the dense subset Q≈[0, 1] which can be listed as a sequence (qn). Check that

144
9
Main Examples
the mapping f ∩√( f (qn)) is linear and isometric. The Banach-Mazur theorem
states that every separable Banach space is embedded in C[0, 1].
The Space ℓ1
Convergence in ∂1 is more stringent than in ∂∞. This can be seen by the inequality
⇒x = (ai) ⊆∂1,
∈x∈∂∞= sup
i
|ai| = max
i
|ai| ⩽
∞

i=0
|ai| = ∈x∈∂1
so xn √0 in ∂∞does not guarantee xn √0 in ∂1. For the latter to occur, not
only must the components approximate 0 together, but their sum must also diminish.
Fewer sequences manage to do this, and this is reﬂected in the fact that ∂1 is separable.
Theorem 9.5
∂1 is complete and separable.
Proof (i) Since ∂1 ≡= c∪
0, one can argue that ∂1 is complete, as are all dual spaces
(Theorem 8.7).
Alternatively, the following direct proof shows that every absolutely summable
series in ∂1 converges (Proposition 7.21) (Note: as ∂1 is deﬁned in terms of sums,
it is more straight-forward to use series instead of Cauchy sequences). Suppose
x1 + x2 + · · · is a series such that 
n ∈xn∈∂1 = s. In the following diagram, we
will show convergence of the various vertical sums.
x1
+
x2
+
...
∃
x

a10 + a11 + a12 + · · ·
+
+
+
a20 + a21 + a22 + · · ·
+
+
+
...
∃
∃
∃
a0
a1
a2
· · ·

∈x1∈∂1
+
∈x2∈∂1
+
...
∃
s
(Note that the absolute signs of ani are omitted in the horizontal sums.)
The main point of the proof is that any rectangular sum of terms in this array is
less than the corresponding sum on the right-hand column:

J

i=I
N

n=M
ani
 ⩽
J

i=I
N

n=M
|ani| ⩽
N

n=M
∈xn∈∂1.

9.1 Sequence Spaces
145
In particular, taking theith column, | 
n ani| ⩽
n |ani| ⩽s shows that it converges
in C to, say, ai := ∞
n=0 ani. In fact, the whole array sum is bounded, 
i |ai| =

i
∞
n=0 ani
 ⩽s, so that x := (ai) belongs to ∂1.
Finally, note that any rectangular sum goes to 0 as it moves downward, because
∞
n=N ∈xn∈∂1 √0 as N √∞. Hence
x −
N

n=1
xn

∂1 =
∞

i=0
ai −
N

n=1
ani
 =
∞

i=0

∞

n=N+1
ani
 √0
giving x = ∞
n=0 xn.
(ii) The sequences en := (0, . . . , 0, 1, 0, . . .), with the 1 occurring at the nth position,
is a Schauder basis because for any vector x = (an) ⊆∂1,
∈x −
N

n=0
anen∈
∂1
= ∈(a0, a1, . . .) −(a0, . . . , aN, 0, 0, . . .)∈∂1
= ∈(0, . . . , 0, aN+1, . . .)∈∂1
=
∞

n=N+1
|an| √0 as N √∞
since 
n |an| converges. If x = ∞
n=0 bnen as well, then bm = em · x = am for
each m ⊆N, so en form a Schauder basis.
⊓⇐
Proposition 9.6
Every functional on ∂1 is of the type (an) ∩√
n bnan where (bn) ⊆∂∞,
and
∂1∪≡∂∞.
Proof The proof is practically identical to the one for c∪
0 ≡∂1, except that now
y = (bn) ⊆∂∞and x = (an) ⊆∂1. The inequality
|y · x| ⩽

n
|bn||an| ⩽sup
n
|bn|

n
|an| = ∈y∈∂∞∈x∈∂1
shows that the linear mapping y◦: ∂1 √C is well-deﬁned and continuous with
∈y◦∈⩽∈y∈∂∞.
Every functional on ∂1 is of this type: Let ξ ⊆∂1∪, then by linearity and continuity
of ξ,

146
9
Main Examples
ξx = ξ
 ∞

n=0
anen

=
∞

n=0
anbn = y · x,
where bn := ξen, y := (bn).
Moreover |bn| = |ξen| ⩽∈ξ∈∈en∈∂1 = ∈ξ∈so that y ⊆∂∞, with ∈y∈∂∞⩽∈ξ∈.
As ξ = y◦, ∈y∈∂∞= ∈y◦∈.
Isomorphism: The mapping J : ∂∞√∂1∪, y ∩√y◦, is linear and the above
assertions state that J is an onto isometry.
⊓⇐
Exercises 9.7
1. Suppose each coefﬁcient of xn = (ani) ⊆∂1 converges, ani √ai as n √∞,
and let x := (ai) ⊆∂1; then it does not follow that xn √x in ∂1, e.g. en ∀√0.
But if |ani −ai| is decreasing with n (for each i), then xn √x in ∂1.
2. ▶∂1 has a natural product, called convolution:
(an) ∪(bn) := (a0b0, a1b0 + a0b1, a2b0 + a1b1 + a0b2, . . . ,
n

i=0
an−ibi, . . .).
This is indeed in ∂1 because the sum to n terms (a triangle of terms aib j) is less
than (|a0| + · · · + |an|)(|b0| + · · · + |bn|) (a square of terms), so that
∈x ∪y∈∂1 ⩽∈x∈∂1∈y∈∂1.
Convolution is commutative and associative, and e0 acts as the identity element
e0 ∪x = x. The inverse of (1, a, 0, . . .) is (1, −a, a2, −a3, . . .), which is in ∂1
only when |a| < 1.
3. If x ⊆∂1, but y ⊆∂∞, then x ∪y is a bounded sequence
∈x ∪y∈∂∞⩽∈x∈∂1∈y∈∂∞.
4. The right-shift operator can be written as a convolution Rx = e1 ∪x. In general,
Rnx = en ∪x, since en ∪em = en+m. The “running average” of a “time-series”
x is 1
N (1, . . . , 1
 	
 
N
, 0, . . .) ∪x.
5. * A subset K of ∂1 is totally bounded ∗it is bounded and
⇒δ > 0, →N ⊆N, →n1, . . . , nN, ⇒x ⊆K,
∈x|N\{ n1,...,nN }∈∂1 < δ.
(Recall that K lies arbitrarily close to ﬁnite-dimensional subspaces.)
6. ∂1 has the functional Sum(bn) := ∞
n=0 bn. It corresponds to the bounded
sequence 1 = (1, 1, . . .), i.e., Sum x = 1 · x. Hence if 
n,i |ani| < ∞then

9.1 Sequence Spaces
147

i

n
ani =

n

i
ani.
7. The functionals γN(an) := aN correspond to eN ⊆∂∞, i.e., γN x = eN · x.
Similarly, the sum SumN(an) := N
n=0 an corresponds to e0 + · · · + eN =
(1, . . . , 1, 0, . . .). Since (1, . . . , 1, 0, . . .) ∀√1 in ∂∞, we also have SumN ∀√
Sum in ∂1∪, yet SumN(x) √Sum(x) for any sequence x ⊆∂1. We’ll discuss
this apparent paradox in a later section (Section 11.5).
The Space ℓ2
This normed space has properties that are, in many respects, midway between ∂1 and
∂∞. Yet it stands out, as it has a dot product x · y deﬁned for any two of its sequences,
and ¯x · x = ∈x∈2; we will have much more to say about normed spaces with such
dot products in the next chapter.
Theorem 9.8
∂2 is complete and separable.
Proof (i) Let xn = (ani) be a Cauchy sequence in ∂2; the terms are uniformly
bounded ∈xn∈⩽c. For each i,
|ani −ami|2 ⩽

i
|ani −ami|2 = ∈xn −xm∈2 √0 as n, m √∞,
so (ani) is a complex Cauchy sequence which converges to, say, ai := limn√∞ani.
The sequence x := (ai) belongs to ∂2 by taking the limit N √∞of
N

i=0
|ai|2 = lim
n√∞
N

i=0
|ani|2 ⩽lim
n√∞∈xn∈2 ⩽c2.
As xn is Cauchy, for each δ > 0 there is a positive integer M such that
n, m ⩾M ⇔∈xn −xm∈< δ.
Moreover, for each i ⊆N, there exists an integer Mi such that
m ⩾Mi ⇔|ami −ai| < δ
2i .

148
9
Main Examples
Therefore, for any N ⊆N, picking m larger than M, M0, M1,…, MN, gives




N

i=0
|ani −ai|2 ⩽




N

i=0
|ani −ami|2 +




N

i=0
|ami −ai|2
< ∈xn −xm∈+




N

i=0
δ2
4i < 3δ,
which implies ∈xn −x∈< 3δ for n ⩾M.
(ii) For separability, ∂2 has the Schauder basis en, since for any x = (an) ⊆∂2,
∈x −
N

n=0
anen∈
∂2
= ∈(0, . . . , 0, aN+1, . . .)∈∂2 =




∞

n=N+1
|an|2 √0.
Uniqueness of the coefﬁcients follows as in the proof of Theorem 9.5.
⊓⇐
Proposition 9.9
Every functional on ∂2 is of the type (an) ∩√
n bnan where (bn) ⊆∂2,
and
∂2∪≡∂2
‘Proof ’. The argument is so similar to the previous ones about c∪
0 and ∂1∪that it is
left as an exercise (use Cauchy’s inequality at one point).
Exercises 9.10
1. Show that |x · y| = ∈x∈∈y∈if, and only if, y is a multiple of x (or x = 0).
2. The map (a1, . . . , aN) ∩√(a1, . . . , aN, 0, 0, . . .) embeds CN in ∂2.
3. ∂2 contains the interesting compact convex set { (an) : |an| ⩽1/n }, called the
Hilbert cube. It is totally bounded in ∂2, as it is close within any δ to a ﬁnite-
dimensional space { (an) : ⇒n > Nδ, an = 0 }, yet it is inﬁnite-dimensional; it
cannot enclose any ball (else the ball would be totally bounded).
4. ▶The various sequence spaces are subsets of each other as follows:
c00 ⊂∂1 ⊂∂2 ⊂c0 ⊂c ⊂∂∞,
because ∈x∈∂∞⩽∈x∈∂2 ⩽∈x∈∂1,
but ∂1 ⊂∂2 ⊂c0 are not Banach space embeddings! Show further that c00 with
the respective norms is dense in ∂1, ∂2, and c0 (c00 cannot be complete in any
norm, Exercise8.25(6)).

9.1 Sequence Spaces
149
The Space ℓp
The space ∂p := { (an) : an ⊆C, 
n |an|p < ∞}, p ⩾1, is endowed with addition
and scalar multiplication like the other sequence spaces, and the norm
∈x∈∂p :=
 ∞

n=0
|an|p1/p
.
Our aim in this section is to prove the triangle inequality for this norm, otherwise
known as Minkowski’s inequality, and show ∂p is complete and separable.
As the reader is probably becoming aware, it is inequalities that are at the heart
of most proofs about continuity, including isomorphisms. They can be thought of as
a ‘process’ transforming numbers from one form to another, perhaps more useful,
form, but losing some information on the way. Much like tools to be chosen with
care, some are “sharper” than others. (See [8] for much more.) The following three
inequalities are continually used in analysis. The ﬁrst is a gem, simple yet rich:
aπbβ ⩽πa + βb,
for π, β, a, b ⩾0, π + β = 1.
(9.1)
This inequality states that any weighted geometric mean is less than or equal to
the same-weighted arithmetic mean. The special case
√
ab ⩽(a + b)/2 has already
been encountered previously. Writing a = ex, b = ey gives
eπx+βy ⩽πex + βey.
This is equivalent to the convexity of the
exponential function, and can be taken as its
proof (any real function with a positive second
derivative is convex).
x
y
αx+βy
a
b
The same idea applied to the convexity of x p, p ⩾1, gives
(πa + βb)p ⩽πa p + βbp,
for π, β, a, b ⩾0, π + β = 1.
(9.2)
A third inequality of importance is
a p + bp ⩽(a + b)p,
for p ⩾1, a, b ♦0.
(9.3)
Its normalized form 1 + t p ⩽(1 + t)p, for t = b/a ⩾0, can be obtained by
comparing their derivatives p t p−1 ⩽p(1+t)p−1, as they start from the same value
at t = 0.

150
9
Main Examples
Proposition 9.11
For a, b, π, β ⩾0, π + β = 1, p ⩾1, q > 0,
min(a, b) ⩽

πa−q + βb−q−1/q
harmonic mean
⩽aπbβ
geometric mean
⩽

πa1/p + βb1/pp
⩽πa + βb
arithmetic mean
⩽
p
πa p + βbp,
root-mean-“square”
⩽max(a, b).
Proof (i) If a ⩽b (without loss of generality), then aq ⩽bq, so
π
aq + β
bq ⩽π + β
aq
= 1
aq
which is equivalent to the ﬁrst inequality of the proposition.
(ii) The second inequality is equivalent to a−πqb−βq ⩽πa−q +βb−q, which is (9.1)
with a, b replaced by a−q, b−q respectively.
(iii) Similarly, the third inequality is essentially aπ/pbβ/p ⩽πa1/p + βb1/p, which
is (9.1) with a, b replaced by a1/p, b1/p respectively.
(iv) If a, b in (9.2) are substituted by a1/p and b1/p one obtains (πa1/p +βb1/p)p ⩽
πa + βb.
(v) The ﬁfth inequality is precisely (9.2), while the sixth one follows easily if we
assume, say, a ⩽b; for then, a p ⩽bp, so πa p + βbp ⩽(π + β)bp = bp.
Substituting q/p for p in (9.3), when p ⩽q, and a p for a, bp for b, yields
(aq + bq)1/q ⩽(a p + bp)1/p
for 0 < p ⩽q,
and furthermore, substituting π1/qa for a and β1/qb for b in this inequality, gives
(πaq + βbq)1/q ⩽(πa p + βbp)1/p
which is implicitly implied in the scheme of inequalities above.
⊓⇐
An induction proof generalizes all these inequalities to arbitrary sums or products,
aπ1
1 · · · aπn
n
⩽π1a1 + · · · + πnan ⩽
p
π1a p
1 + · · · + πna p
n ,
(9.4)
when ai, πi ⩾0, π1 + · · · + πn = 1, p ⩾1, as well as
q
aq
1 + · · · + aq
n ⩽
p
a p
1 + · · · + a p
n ,
forp ⩽q.

9.1 Sequence Spaces
151
Hermann Minkowski (1864–1907) studied under Lindemann (of
π
-
o
t
,g
r
e
b
s
gi
n
o¨
K
f
o
y
tisr
e
v
i
n
U
e
h
t
t
a
)
e
m
a
f
y
tila
t
n
e
d
n
e
c
s
n
a
rt-
gether with Hilbert. At 19 years of age, two years before he
graduated with a thesis on quadratic forms, he had already
won the prestigious French Academy’s Grand Prix.
Starting
1889, he developed his “geometry of numbers” ideas on lattices,
including his inequality. After teaching in Zurich (where Ein-
d
e
ts
e
r
e
t
n
i
e
m
a
c
e
b
,
n
e
g
n
itt
o¨
G
o
t
d
e
v
o
m
e
h
,)t
n
e
d
u
ts
a
s
a
w
n
ie
ts
in physics and presented his version of special relativity as a
uniﬁed space-time.
Fig. 9.1 Minkowski
This last inequality remains valid for inﬁnite sums, ∈x∈∂q ⩽∈x∈∂p when p ⩽q,
implying ∂p ∇∂q. It shows that a bounded sequence lies in a whole range of ∂p
spaces, down to some inﬁmum p.
Proposition 9.12 Minkowski’s inequality
∈x + y∈∂p ⩽∈x∈∂p + ∈y∈∂p, where 1 ⩽p ⩽∞.
Proof All norms in this proof are taken to be the ∂p-norm. Let u = (an) and v = (bn)
be two sequences in ∂p. Summing the inequality (π|a| + β|b|)p ⩽π|a|p + β|b|p
(π + β = 1, π, β ⩾0) for a sequence of terms gives

n
|πan + βbn|p ⩽

n
(π|an| + β|bn|)p ⩽π

n
|an|p + β

n
|bn|p.
or
∈πu + βv∈p ⩽π∈u∈p + β∈v∈p.
Substituting u = x/∈x∈, v = y/∈y∈, π = ∈x∈/(∈x∈+∈y∈), β = ∈y∈/(∈x∈+∈y∈),
gives
∈x + y∈
∈x∈+ ∈y∈= ∈πu + βv∈≤(π + β)1/p = 1.
⊓⇐
Proposition 9.13 Hölder’s inequality
|x · y| ⩽∈x∈∂p∈y∈∂p′ , where 1
p + 1
p′ = 1, p ⩾1.
Proof Substitute a1/π and b1/β instead of a and b in aπbβ ⩽πa+βb, with π = 1/p,
β = 1/p′, to get

152
9
Main Examples
ab ⩽a p
p + bp′
p′ .
(9.5)
Summing this for a sequence of terms in C leads to
|u · v| ⩽

n
|anbn| ⩽

n

|an|p
p
+ |bn|p′
p′

= 1
p ∈u∈p
∂p + 1
p′ ∈v∈p′
∂p′ .
In particular, for unit vectors u = x/∈x∈∂p, v = y/∈y∈∂p′ , we get Hölder’s inequal-
ity,
|x · y|
∈x∈∂p∈y∈∂p′ ⩽1
p + 1
p′ = 1.
Proposition 9.14
For p ⩾1, ∂p is a separable Banach space, with ∂p∪≡∂p′, where 1
p + 1
p′ =1.
Proof Minkowski’s inequality is the non-trivial part in showing that ∂p is indeed a
normed space. It is separable with the Schauder basis en, since for any x = (an) ⊆∂p,
the series 
n |an|p converges to ∈x∈p
∂p, so
∈x −
N

n=0
anen∈
p
∂p
= ∈(0, . . . , 0, aN+1, . . .)∈p
∂p =
∞

n=N+1
|an|p √0,
so x = 
n anen. The coefﬁcients are unique since if x = 
n bnen = (b0, b1, . . .),
then bn = an.
Dual of ∂p: Any vector y ⊆∂p′ acts on ∂p via x ∩√y · x, with the latter being
ﬁnite by Hölder’s inequality |y · x| ⩽∈y∈∂p′ ∈x∈∂p. By Exercise 2 below, there is an
x ⊆∂p which makes this an equality. Thus ∈y◦∈= ∈y∈∂p′ .
Conversely, let ξ be a functional on ∂p; then ξx = ∞
n=0 anbn = y · x, where
bn := ξen, y := (bn). Writing bn = |bn|eiθn and noting p(p′ −1) = p′,
N

n=0
|bn|p′ =
N

n=0
bne−iθn|bn|p′−1 = |ξ(e−iθn|bn|p′−1)| ⩽∈ξ∈
 N

n=0
|bn|p′
1/p
.
Dividing the right-hand series gives
N
n=0 |bn|p′1/p′
⩽∈ξ∈; as N is arbitrary,
y ⊆∂p′.

9.1 Sequence Spaces
153
Completeness: In common with all dual spaces, ∂p ≡∂p′∪is complete (or from
an argument similar to the one for ∂2).
⊓⇐
Exercises 9.15
1. Given 1 ⩽q < p, ﬁnd an example of a sequence which is in ∂p but not in ∂q.
2. For each y ⊆∂p′, ﬁnd a sequence x ⊆∂p which makes Hölder’s inequality an
equality.
3. Generalized Hölder’s inequalities
∈x y∈∂r ⩽∈x∈∂p∈y∈∂q, where 1
p + 1
q = 1
r ,
 
n
anbncn
 ⩽∈(an)∈∂p∈(bn)∈∂q∈(cn)∈∂r , where 1
p + 1
q + 1
r = 1.
(Hint: Apply Hölder’s inequality to the product |an|r|bn|r.)
4. Littlewood’s inequality: ∈x∈∂r ⩽∈x∈π
∂p∈x∈1−π
∂q
, where 1
r = π
p + 1−π
q .
(Hint: Apply the generalized Hölder’s inequality above to |an|π|an|1−π, using
p/π and q/(1 −π) instead of p and q.)
5. * Young’s inequality:
∈x ∪y∈∂r ⩽∈x∈∂p∈y∈∂q,
where 1
p + 1
q = 1 + 1
r , p, q ⩾1 (Exercises 9.7(2,3)).
Justify the steps of the following proof. First note that 1
p′ + 1
q′ + 1
r = 1 (where
1
p′ = 1 −1
p, etc.); then using the second generalized Hölder’s inequality above
on the positive numbers an, bn, cn, and an exquisite juggling of indices, (where
k := n −m)
N

n=0
n

m=0
an−mbmcn =
N

n=0
n

m=0
(ak)p/r(bm)q/r(ak)p/q′(cn)r′/q′(bm)q/p′(cn)r′/p′
⩽
 
n,k
a p
k bq
m
1/r 
n,k
a p
k cr′
n
1/q′ 
n,m
bq
mcr′
n
1/p′
=
 N

n=0
a p
n
1/p N

n=0
bq
n
1/q N

n=0
cr′
n
1/r′
.
Hence if (cn) ⊆∂r′, (an) ⊆∂p, and (bn) ⊆∂q, then (an) ∪(bn) ⊆(∂r′)∪≡∂r.
6. * Prove the reverse Minkowski inequality for 0 < p ⩽1, and positive real
sequences x = (an), y = (bn), an, bn ⩾0,

154
9
Main Examples
∈x∈p + ∈y∈p ⩽∈x + y∈p.
(Hint: the reverse inequality has its roots in x p being concave.)
9.2 Function Spaces
Much of the above can be generalized from sequences to functions, where summation

n an becomes integration

f (t) dt. For example, the proof that ∂∞is complete
generalizes to the space L∞(R), practically untouched. Even though it is function
spaces that are at the heart of “functional analysis”, we do not prove all these gen-
eralizations here, as laying the groundwork for integration and measures would take
us too far aﬁeld. Instead a review is provided, referring the reader to [6] for more
details. However, we allow for vector-valued functions, because it does not incur any
extra difﬁculty. To avoid confusion with the scalar ∈f ∈, we write | f | for the function
x ∩√∈f (x)∈.
Lebesgue Measure on RN
1. A measure μ on RN is an assignment of positive numbers or ∞to certain
subsets E ∇RN with the properties that it be
(i) additive,
μ(E ˆ F) = μ(E) + μ(F) for E, F, disjoint,
(ii) continuous,
En √E ⇔μ(En) √μ(E).
We haven’t deﬁned a distance function on sets, but it is enough for now to
take En √E to mean that En is a decreasing sequence of sets of ﬁnite
measure, with 
n En = E.
One ﬁnal property that we expect μ to satisfy, at least in RN, is
(iii) a translated copy of a set has the same measure, μ(E + x) = μ(E).
Examples of measures are the standard length, area, and volume of Euclidean
geometry.
2. Taking R as our main example, and deﬁning μ[0, 1[ := 1, these properties
completely determine the length of any interval, namely μ[a, b] = b −a =
μ[a, b[. (Hint: divide [0, 1[ into equal intervals to show μ[0, m/n] = m/n.)
3. As a ﬁrst step in constructing μ on R, therefore, the length of any interval is
deﬁned to be the difference of its endpoints, e.g. m[a, b] := b−a. This function
can be extended in two ways to
(a) the length of any countable union of disjoint intervals
m(

n
In) :=

n
m(In),
(b) the length of the set obtained by removing a countable union of disjoint
subintervals from a bounded interval

9.2 Function Spaces
155
m(I \

n
In) := m(I) −

n
m(In).
4. For general sets, deﬁne
m∪(A) := inf{ m(U) : A ∇U =

n
In }
m∪(A) := sup{ m(K) : A ˙ K = I \

n
In }
(Note that since we are taking the inﬁmum and supremum, respectively, we might
as well take I to be a closed and bounded interval and In to be open intervals,
in which case U is an open set, and K a compact set.)
It is a fact that there exist sets for which these two values do not agree (see [6]).
A “well-behaved” set, called measurable, satisﬁes m∪(E) = m∪(E), which is
then called its Lebesgue measure μ(E).
5. m∪(
n An) ⩽
n m∪(An) and A ∇B ⇔m∪(A) ⩽m∪(B) (since open covers
for each An provide an open cover for their union). Of course, these statements
continue to hold for μ applied to measurable sets.
6. A useful equivalent criterion of measurability of E is:
For any set A, m∪(E ≈A) + m∪(Ec ≈A) = m∪(A).
7. Using this criterion, it follows that, for E, F, and En measurable sets,
(a) Ec, E ˆ F, E ≈F, E \ F, and E△F are measurable; when they are disjoint,
μ(E ˆ F) = μ(E) + μ(F).
(b) ∞
n=1 En and ∞
n=1 En are measurable, and when En are disjoint,
μ(
∞

n=1
En) =
∞

n=1
μ(En).
The sets that can be obtained by starting with the intervals and applying these
constructions are called Borel sets; they include the open and closed sets.
8. Sets with (m∪-)measure 0 are obviously measurable and are called null sets. For
example, any countable set is null; but most null sets are uncountable, e.g. the
Cantor set. The countable union of null sets is null.
Adding (or removing) a null set N from a measurable set E does not affect its
measure,
μ(E ˆ N) = μ(E) + μ(N) = μ(E).
Because measures don’t distinguish sets up to a null set, we say that two sets
are equal almost everywhere, E = Fa.e., when they differ by a null set. More

156
9
Main Examples
generally, we qualify a statement “Pa.e.” when P(t) is true for all t except on
a null set; for example, we say f = g a.e. when f (t) = g(t) for all t in their
domain apart from a null set.
9. The distance between measurable sets is deﬁned as d(E, F) := μ(E△F). It is
a metric, with the proviso that d(E, F) = 0 ∗E = F a.e. The measure μ is
continuous with respect to it, En √E ⇔μ(En) √μ(E).
10. A similar procedure gives the Lebesgue measure on RN, with the modiﬁcation
that cuboids are used instead of intervals to generate the measurable sets. Most
subsets of RN that the reader is likely to have encountered are measurable,
including the unit sphere and ball in R3.
Measurable Functions
1. The characteristic function of a set is deﬁned by 1E(t) :=
 1 t ⊆E
0 t ∀⊆E . Linear
combinations of characteristic functions k
n=1 1En xn, where En are bounded
measurable subsets of R and xn ⊆C, are called simple functions (or step func-
tions). More generally, R can be replaced by a ﬁxed measurable set A, and xn
can belong to a Banach space X. The simple functions form a vector space S.
2. A function f : A √X is said to be measurable when it is the pointwise limit
of simple functions, sn √f a.e. For real-valued functions, this is equivalent to
f −1[a, ∞[ being measurable for all a ⊆R.
Note that simple functions supported in E (i.e., are zero outside E) can converge
only to measurable functions supported in E (since sn1E √f 1E a.e.).
3. Measurable functions form a vector space: αf and f + g are measurable when
f , g are. It follows from
|sn|−| f |
 ⩽|sn −f | that | f | : A √R is measurable.
For real-valued measurable functions, f g, max( f, g), and supn( fn), are also
measurable. Real-valued continuous functions are measurable.
4. ▶In fact the space of measurable functions is in a sense complete: if fn are
measurable and fn √f a.e., then f is measurable.
5. L∞(A) is deﬁned as the space of (equivalence classes of) bounded measur-
able functions f : A √C, over a measurable set A, with the supremum
norm ∈f ∈L∞:= supta.e. | f (t)|, i.e., the smallest real number c such that
| f (t)| ⩽c a.e.t
6. L∞(R) contains the closed subspace of bounded continuous functions Cb(R),
which in turn contains C0(R) := { f ⊆C(R) :
lim
t√±∞f (t) = 0 }. The space
C[a, b] is embedded in C0(R).
7. L∞[a, b] is not separable: the uncountable number of characteristic functions
1[x,y], a < x < y < b, are at unit distance from each other.

9.2 Function Spaces
157
Proposition 9.16
L∞(A) is a Banach space.
Proof If | f (t)| ⩽∈f ∈L∞except on the null set E1, and |g(t)| ⩽∈g∈L∞except on
the null set E2, then for all t ⊆A \ (E1 ˆ E2),
| f (t) + g(t)| ⩽| f (t)| + |g(t)|,
|αf (t)| = |α|| f (t)|,
so ∈f + g∈L∞⩽∈f ∈L∞+ ∈g∈L∞,
∈αf ∈L∞= |α|∈f ∈L∞.
Clearly ∈f ∈L∞= 0 only when | f (t)| = 0 a.e. It follows that L∞(A) is a normed
space, as long as we identify ae-equal functions into equivalence classes.
Completeness: Let fn ⊆L∞(A) be a Cauchy sequence, where | fn(t)| ⩽∈fn∈L∞
for all t ⊆A except in some null set En. Copying the proof of the completeness of
∂∞(Theorem 9.1),
| fn(t) −fm(t)| ⩽∈fn −fm∈L∞√0
for each t ⊆A, except possibly on the null set 
n En, so fn(t) is Cauchy and
converges fn(t) √
f (t) a.e.(t). The function f is evidently measurable, and
fn √f uniformly away from this null set, since for any δ > 0 and n large enough
(but independent of t),
| fn(t) −f (t)| ⩽| fn(t) −fm(t)| + | fm(t) −f (t)|
⩽∈fn −fm∈L∞+ | fm(t) −f (t)|
a.e.(t)
< 2δ
where m ⩾n is chosen, depending on t, to make | fm(t) −f (t)| < δ. This means
that fn √f in L∞, and implies ∈f ∈L∞⩽∈f −fn∈L∞+ ∈fn∈L∞< ∞, so
f ⊆L∞(A).
⊓⇐
Integrable Functions
1. Given a set E of ﬁnite measure and its characteristic function, let

1E := μ(E).
For a simple function, deﬁne its integral

N

n=1
1En xn :=
N

n=1
μ(En)xn.

158
9
Main Examples
It is well-deﬁned, since a simple function has a unique representation in terms
of disjoint En. It is straightforward to verify that

(s + r) =

s +

r and

αs = α

s for s,r ⊆S.
2. The function ∈s∈:=

|s| = 
n μ(En)∈xn∈is a norm on S. Here, |s| is the real-
valued simple function |s| := 
n 1En∈xn∈⩾0. In particular, for real-valued
simple functions, r ⩽s ⇔

r ⩽

s.
Proof (i) ∈s + r∈= 
n μ(En)∈xn + yn∈⩽
n μ(En)(∈xn∈+∈yn∈) = ∈s∈+
∈r∈,
(ii) ∈αs∈= 
n μ(En)∈αxn∈= |α|∈s∈,
(iii)

|s| = 0 when 
n μ(En)∈xn∈= 0. This implies μ(En)∈xn∈= 0 for all
n, i.e., xn = 0 or μ(En) = 0, so s = 0 a.e..
3. The integral is a continuous functional on S, ∈

s∈⩽

|s|, since,


s
 =
 
n
μ(En)xn
 ⩽

n
μ(En)∈xn∈=

|s|.
4. The space of real (or complex) simple functions with this norm is separable (the
simple functions with xn ⊆Q and En equal to intervals with rational endpoints,
are countable and dense), but not complete.
5. A Cauchy sequence of simple functions converges a.e. to a measurable function.
Proof Let sn be a Cauchy sequence in S. Given any δ > 0, let
EN := { t ⊆R : →n, m ⩾N, ∈sn(t) −sm(t)∈⩾δ },
∴δ μ(EN) =

δ1EN ⩽

|sn −sm| = ∈sn −sm∈√0 as N √∞.
This shows that for t not in the null set Fδ := 
N EN,
→N, ⇒n, m ⩾N, ∈sn(t) −sm(t)∈< δ.
In particular, for t not in the null set 
k⊆N F1/k, ∈sn(t) −sm(t)∈√0 as n, m √
∞. Thus, except for a null set, (sn(t)) is a Cauchy sequence in X and hence
converges.
6. A function f : R √X is said to be integrable when it is the ae-limit of a Cauchy
sequence of simple functions sn √f a.e. Its integral is given by the extension
of the integral on S,

f := lim
n√∞

sn.

9.2 Function Spaces
159
Note that

sn is a Cauchy sequence in X (∈

sn −

sm∈X ⩽

|sn −sm| √0).
The space of (equivalence classes of) integrable functions R √X is denoted
by L1(R, X); it is the completion of S (Theorem 4.6). By Proposition 7.17, the
space L1(R, X) is a normed vector space with
∈f ∈L1 := lim
n√∞∈sn∈= lim
n√∞

|sn| =

| f |,
so f ⊆L1(R, X) ∗| f | ⊆L1(R). It also follows that for real-valued integrable
functions f ⩽g ⇔

f ⩽

g.
7. The integral is a continuous functional on L1(R, X) (Example8.9(4)),

f + g =

f +

g,

αf = α

f,


f
 ⩽

| f |.
If fn √f in L1(R, X) then

fn √

f in X.
8. (a) f ⊆L1(R) ⇔

f (t)x dt = (

f )x,
(b) T ⊆B(X, Y) ⇔

T f = T

f .
Proof (a) is a special case of (b) with T : F √X, T (α) := αx.
As an operator T : X √Y acts linearly on a simple function s =
n 1En xn ⊆S,
T s =
N

n=1
1EnT xn ⇔

T s =
N

n=1
μ(En)T xn = T

s.
If sn √f in L1(R, X) then T sn √T f in L1(R, Y), so

T f = T

f .
9. For a measurable set A ∇R, deﬁne L1(A) := { f 1A : f ⊆L1(R) }, and let

A f :=

f 1A.
Note that

A f = 0 for any null set A. Hence if f = g a.e., with g ⊆L1(R), and
E = F a.e., then f ⊆L1(R) as well and

E f =

F g.
10. For E, F disjoint measurable sets,

EˆF
f =

E
f +

F
f
It follows that E ∇F ⇔

E | f | ⩽

F | f |.
Theorem 9.17
L1(A) is a separable Banach space.

160
9
Main Examples
Henri Lebesgue (1875–1941) graduated at the ´Ecole Normale
Sup´erieure of Paris at 27 years.
His thesis built upon work
of Baire, Borel and Jordan, to generalize lengths and areas,
and so an integration powerful enough to tackle functions too
discontinuous for Riemann’s integration — the ﬁrst complete
space of integrable functions. After a century of attempts by
other mathematicians, he ﬁnally proved that uniformly bounded
series of integrable functions, such as the Fourier series, could be
integrated term by term. Although his achievement was widely
seen as abstract, in his words, “Reduced to general theories,
mathematics would be a beautiful form without content. It would
quickly die.”
Fig. 9.2 Lebesgue
Proof Completeness: Let fn be a Cauchy sequence in L1(A), i.e., ∈fn −fm∈√0.
Choose sn ⊆S close to fn, say ∈sn −fn∈< 1/n. Then (sn) is a Cauchy sequence
of simple functions, asymptotic to fn. By Notes 5 and 7 above, sn converges to an
integrable function f in L1(A). Hence, so does the asymptotic sequence fn.
Separability: By construction, the separable set S of simple functions is dense
in L1(A): Any f ⊆L1(A) has a sequence of simple functions converging to it
(sn) √f a.e., so ∈f −sn∈L1 √0 as n √∞.
⊓⇐
Much the same analysis can be made starting with the norm ∈s∈p :=

|s|p1/p,
1 ⩽p < ∞, on S. The completion of S in this norm is denoted by L p(A), which is
thus complete and separable (S dense in it).
Proposition 9.18
If fn √f in L∞(R), that is, uniformly, and
(i)
fn are continuous, then f is continuous,
(ii)
fn are integrable, then f is integrable on [a, b], and
 b
a
fn √
 b
a
f,
(iii)
f ′
n are continuous and converge uniformly, then f ′
n √f ′.
Proof (i) The ﬁrst assertion is a restatement of the fact that C(R) is closed in L∞(R)
(Theorem 6.23).
(ii) The second follows from the completeness of L1[a, b] and the continuity of the
integral

9.2 Function Spaces
161
 b
a
| fn −f | ⩽(b −a)∈fn −f ∈L∞[a,b] √0.
(iii) If f ′
n √g uniformly, then f ′
n √g in L1[a, t] by (ii), and
 t
a f ′
n √
 t
a g. But,
assuming the fundamental theorem of calculus (Theorem 12.8),
 t
a f ′
n = fn(t) −
fn(a), which converge to f (t) −f (a) uniformly and in L1[a, t]. So
 t
a g = f (t) −
f (a), showing f is differentiable, with f ′ = g.
⊓⇐
Examples 9.19
1. Convergence in L1(R) is quite different from uniform convergence. For example,
the sequence of functions 1
n 1[0,n] converge uniformly to 0, but not in L1(R),
whereas the sequence n1[0,1/n2] converges to 0 in L1(R) but not uniformly.
2. The product x · y of sequences becomes f · g :=

f g for functions. Hölder’s
inequalities are valid:
∈f g∈L1 ⩽∈f ∈L p∈g∈L p′ , 1 = 1
p + 1
p′ ,
∈f g∈Lr ⩽∈f ∈L p∈g∈Lq,
1
r = 1
p + 1
q ,
∈f ∈Lr ⩽∈f ∈π
L p∈f ∈1−π
Lq , 1
r = π
p + 1−π
q ,
thus f lies in L p(A) for p in an interval of values.
Proof Integrating |a(t)b(t)| ⩽|a(t)|p
p
+ |b(t)|p′
p′
and putting a = f/∈f ∈L p, b =
g/∈g∈L p′ , gives the ﬁrst inequality. Substituting | f |r for f , |g|r for g, p/r for
p, and q/r for p′ gives the second inequality. Finally, the substitutions of | f |π
for f , | f |1−π for g, and p/π for p, gives the third.
3. ▶When the domain of the functions is compact, the spaces are included in each
other as sets, in the reverse order of the sequence spaces,
C[a, b] ∇L∞[a, b] ∇L2[a, b] ∇L1[a, b],
because, by Hölder’s inequality, I : L∞[a, b] √L2[a, b] √L1[a, b] is con-
tinuous,
∈f ∈L1[a,b] ⩽(b −a)
1
2 ∈f ∈L2[a,b] ⩽(b −a)∈f ∈L∞[a,b].
4. The notation
 ∞
−∞f is capable of at least three interpretations, as (i)

R f when
f ⊆L1(R), (ii) limR,S√∞
 R
−S f , (iii) limR√∞
 R
−R f . It should be clear that
the ﬁniteness of these integrals follow (i) ⇔(ii) ⇔(iii), but the examples
 R
−R x dx = 0 and
 R
0
sin x
x
dx √ω/2 as R √∞show that the converses are
false.

162
9
Main Examples
Approximation of Functions
Proposition 9.20
The polynomials are dense in L1[a, b], L2[a, b] and C[a, b].
Proof By construction, the step functions are dense in L1(R). Now, intuitively speak-
ing, any real-valued step function s can be “nudged” into a continuous function g
by replacing its discontinuities with steep slopes, and the distance ∈s −g∈L1 can be
made as small as needed by making the slopes steeper. More precisely and more
generally, any bounded measurable set E in R lies between a compact set K and an
open set U, such that μ(U \ K) < δ; also, there is a continuous function gE taking
values in [0, 1] such that gE K = 1, gEU c = 0 (Exercise 3.12(17)). So
⇒δ > 0, →gE ⊆C(R),
∈gE −1E∈L1 =

U\K
|gE −1E| ⩽μ(U \ K) < δ.
Consequently, taking any non-zero simple function s = N
n=1 1En xn and replacing
each 1En with continuous functions gn, where ∈gn −1En∈L1 < δ/ N
n=1 ∈xn∈, gives
a continuous function g := N
n=1 gnxn, which approximates s in L1,
∈s −g∈L1 ⩽
N

n=1
∈1En −gn∈L1∈xn∈< δ.
But any function f ⊆L1(R) has a simple function approximation s, which in turn
can be approximated by a continuous function g. Combining these two facts gives
∈f −g∈L1 ⩽∈f −s∈L1 + ∈s −g∈L1 < 2δ
showing that the set of (integrable) continuous functions is dense in L1(R). Note
further that precisely the same arguments work for L2(R).
We have already seen, in the Stone-Weierstraß theorem (Theorem 6.24), that the
set of polynomials p(z, ¯z) is dense in C[a, b]. But, in this case, z = ¯z = x ⊆[a, b],
so such polynomials are of the usual form p ⊆C[x]. Combining this with the above
result shows that C[x] is also dense in L1[a, b] and L2[a, b]: for any δ > 0, there is
a polynomial p ⊆C[x] such that
∈f −p∈L1[a,b] ⩽∈f −g∈L1[a,b] + ∈g −p∈L1[a,b] < 3δ
since ∈g −p∈L1[a,b] ⩽(b −a)∈g −p∈C[a,b] < δ.
⊓⇐
More generally, the polynomial splines are dense in the real version of these
spaces. A spline of degree N is a function 
n 1En pn, where En are disjoint intervals

9.2 Function Spaces
163
and pn are polynomials of degree at most N such that the ﬁrst N −1 derivatives
match at the endpoints of En. They are often used in numerical techniques and
graphics computing. Another useful way of approximating integrable functions uses
the convolution:
Proposition 9.21 Approximations of the Identity
If hn ⊆C(R) are such that hn ⩾0,

hn = 1 and hn √0 uniformly on
R \ [−γ, γ] for any γ > 0, then hn ∪f √f in C(R) and L1[a, b].
Proof Let g be a continuous function, and let x ⊆R; on the one hand,
hn
⇒δ > 0, →γ > 0, |y| < γ ⇔|g(x + y) −g(x)| < δ,
(9.6)
and on the other hand, for this γ,
→N, n ⩾NAnd |y| ⩾γ ⇔0 ⩽hn(y) < δ.
(9.7)
Therefore, for all x and n ⩾N,
|hn ∪g(x) −g(x)| =


hn(y)

g(x −y) −g(x)

dy

⩽

hn(y)
g(x −y) −g(x)
 dy
⩽
 γ
−γ
hn(y) δ dy +

R\[−γ,γ]
2δ ∈g∈C dy
by (9.6) and (9.7)
⩽δ(1 + 2∈g∈C)
and ∈hn ∪g −g∈C √0 as required.
Infacthn∪f approximates f ⊆L1[a, b]inthe L1-norm,for,choosingg ⊆C[a, b]
close to f, ∈f −g∈L1[a,b] < δ, and n large enough that ∈hn ∪g −g∈C < δ
holds, then
∈hn ∪f −f ∈L1[a,b] ⩽∈hn ∪g −g∈L1[a,b] + ∈hn ∪( f −g)∈L1[a,b]
+ ∈f −g∈L1[a,b] < 3δ,
since ∈hn ∪( f −g)∈L1 ⩽∈hn∈L1∈f −g∈L1.
⊓⇐

164
9
Main Examples
Corollary 9.22
∈f (x + y) −f (x)∈L1(R) √0 as y √0
Proof The step functions hn := n1[−1/2n,1/2n] clearly form an approximation of the
identity, and so hn ∪f √f in L1. But their translations by Ty f (x) := f (x −y),
namely h+
n := T1/2nhn = n1[0,1/n] and h−
n := T−1/2nhn = n1[−1/n,0], form other
approximations of the identity. Since (Tyh) ∪f = h ∪(Ty f ),
  f (x ± 1
2n ) −f (x)
 dx = ∈T±1/2n f −f ∈L1
⩽∈T±1/n f −(T±1/2nhn) ∪f ∈L1 + ∈(T±1/2nhn) ∪f −f ∈L1
= ∈T±1/2n f −hn ∪(T±1/2n f )∈L1 + ∈h±
n ∪f −f ∈L1
√0 as n √∞.
⊓⇐
Exercises 9.23
1. The map (an) ∩√
n an1[n,n+1[ embeds ∂1 into L1(R).
2. If 
n ∈fn∈L1 exists, then ∞
n=1

fn =
 ∞
n=1 fn.
3. The map L1(A) √C, f ∩√

g f is linear, and continuous when g ⊆L∞(A).
Assuming surjectivity, show L1(K)∪≡L∞(K) for K ∇R compact, and simi-
larly L p(K)∪≡L p′(K)(p > 1).
4. Show that the functional γa( f ) := f (a) on C[a, b] does not correspond to any
L1-function γ in the sense of γa( f ) =

γf . Hence the dual space of C[a, b] is
not L1[a, b]; it consists of functionals called measures of bounded variation.
5. Minkowski’s inequality: Emulate the proof of Proposition 9.12 to show
∈f + g∈L p ⩽∈f ∈L p + ∈g∈L p
(p ⩾1).
6. * L1(R) has a convolution deﬁned on it,
f ∪g(t) :=

f (t −s)g(s) ds.
Just like the same-named operation in ∂1, it is associative and commutative; but
it has no identity, although Gibbs and Dirac audaciously added one and called it
γ. Young’s inequality is satisﬁed,
∈f ∪g∈Lr ⩽∈f ∈L p∈g∈Lq,
1
p′ + 1
q′ = 1
r′ .

9.2 Function Spaces
165
7. Matched Filter: An electronic ﬁlter is a circuit acting on a signal f ⊆L2(R)
and outputting the convolution g ∪f (g ⊆L1(R)). Signals often have white
noise η(t), where ∈g ∪η∈L2 = δ∈g∈L2. The signal-to-noise ratio is S/N :=
∈g ∪f ∈2
L2/∈g ∪η∈2
L2; show that S/N ⩽∈f ∈2
L2/δ2, with equality holding when
g(x −t) = αf (t), α ⊆R.
The Fourier Series
We end this chapter with a look at one of the most important operators on L1[0, 1].
Back to the days of Fourier, there arose the question of whether every periodic
function f can be built up as a Fourier series 
n an cos nx + bn sin nx. This claim
of Fourier was disputed by Lagrange and others; Dirichlet obtained a partial result
for the case f ⊆C2, and Riemann later vastly extended this result. Despite these
protests, the use of Fourier series grew, mainly because they actually worked in many
examples.
Deﬁnition 9.24
The Fourier coefﬁcients of an integrable function f
⊆L1[0, 1] are the
sequence of numbers deﬁned by
F f (n) = f (n) :=
 1
0
e−2ωinx f (x) dx,
n ⊆Z.
This section cannot do justice to the immense number of results and applications
of Fourier series. It must sufﬁce here to present a couple of main results, with the
aim of generalizing them later on. Refer to [7] for more details.
Theorem 9.25
F : L1[0, 1] √c0(Z) is a 1–1 continuous operator with
∈f ∈∂∞⩽∈f ∈L1[0,1]
Here, c0(Z) is deﬁnedas consistingof those‘sequences’ (an)n⊆Z suchthatan √0
as n √±∞.
Proof That F is linear is easy to show. It is continuous because
∈f ∈∂∞= sup
n⊆Z

 1
0
e−2ωinx f (x) dx
 ⩽
 1
0
| f (x)| dx = ∈f ∈L1[0,1].

166
9
Main Examples
The characteristic function 1[a,b], for [a, b] ∇[0, 1], has Fourier coefﬁcients
1[a,b](n) =
 b
a
e−2ωinx dx = e−2ωina −e−2ωinb
2ωin
√0 as n √±∞.
Hence the vector space of simple functions, as well as its closure L1[0, 1], are mapped
into the complete space c0 (Exercises 8.10(8)).
F is 1–1: If f (n) = 0 for every n, then
 1
0
e−2ωiny f (y) dy = 0,
⇒n ⊆Z.
The aim is to show that f = 0 a.e. Firstly,
 1
0
e−2ωiny f (x −y) dy =
 1
0
e−2ωin(x−y) f (y) dy = 0.
Secondly, since (cos ωy)2n = (e2ωiy + e−2ωiy + 2)n/22n is a linear combination
of exponentials of various frequencies that are all multiples of 2ωy, we have, for
hn(y) := (cos ωy)2n/cn,
hn ∪f (x) = 1
cn
 1
0
(cos ωy)2n f (x −y) dy = 0,
where cn :=
 1
0 (cos ωy)2n dy = (2n−1)(2n−3)···
(2n)(2n−2)···
⩾1
2n .
The functions hn satisfy the criteria of Proposition 9.21, as they are positive and
fall rapidly to 0 for |y| ⩾γ, as n √∞. Thus ∈f ∈L1 = ∈hn ∪f −f ∈L1 √0, and
f = 0 a.e.
⊓⇐
The Fourier coefﬁcients have properties that appear remarkable: when f is trans-
lated the coefﬁcients rotate in C, with each f (n) performing n turns as f is translated
one whole period; differentiation of f scales the coefﬁcients by a multiple of n; and
convolutions are transformed to multiplications.
Proposition 9.26
For periodic functions, with period 1,
∅
Ta f (n) = e−2ωian f (n),
f ′(n) = 2ωin f (n),
∅
f ∪g = f g.

9.2 Function Spaces
167
Proof A translation Ta f (x) := f (x −a) has the effect
∅
Ta f (n) =
 1
0
e−2ωinx f (x −a) dx
=
 1
0
e−2ωin(x+a) f (x) dx = e−2ωina f (n).
Differentiation, f ′(x) = limh√0
f (x+h)−f (x)
h
, gives
f ′(n) = lim
h√0 F
T−h f −f
h

(n) = lim
h√0
e2ωinh −1
h
f (n) = 2ωin f (n),
and the convolution of f and g becomes
∅
f ∪g(n) =
 1
0
e−2ωinx
 1
0
f (x −y)g(y) dy dx
=
 1
0
 1
0
e−2ωin(x+y) f (x) dx g(y) dy
=
 1
0
e−2ωinx f (x) dx
 1
0
e−2ωinyg(y) dy = f (n)g(n).
⊓⇐
Exercises 9.27
1. Show
(a) F : 1 ∩√e0 = (. . . , 0, 0, 1, 0, 0, . . .),
(b) F : x ∩√
i
2ω (. . . , −1
2, −1, ω
i , 1, 1
2, . . . , 1
n , . . .),
(c) F : |x −1
2| ∩√
1
ω2 (. . . , 0, 1, 1
4, 1, 0, 1
9, 0, 1
25, . . .),
(d) F : x(x −1
2)(x −1) ∩√−3i
4ω3 (. . . , −1
8, −1, 0, 1, 1
8, . . . , 1
n3 , . . .).
2. Using the open mapping theorem, show that F is not onto c0.
3. The power spectrum of a function is a plot of | f (n)|2 (often with n varying
continuously in R). It displays the dominant frequencies of f . A better plot is
the Nyquist diagram, where f (n) is graphed in three dimensions, with one axis
representing n, and the other two representing f = | f |eiξ.
Prove that F : Ck[0, 1] √ck(Z), where Ck[0, 1] is the space of k-times contin-
uously differentiable periodic functions, and ck(Z) := { (an)c⊆Z : nkan √0 }.
Therefore, how fast the power spectrum decays as n √∞measures how smooth
the function is.

168
9
Main Examples
4. The operator Sa : f (x) ∩√a1/2 f (ax) (a > 0) stretches or compresses f , while
preserving its L2-norm; prove ∅
Sa f (n) = S1/a f (n). This should be familiar:
playing a sound clip in half its normal time doubles the frequencies.
5. Recall the analogous Fourier transform on L1(R), f (ξ) :=

e−2ωiξt f (t) dt.
Similarly to the Fourier series,
(a) it is a continuous linear operator F : L1(R) √C0(R),
(b) 
1[−a,a](ξ) = a sin(ωaξ)/(ωaξ) =: a sinc(ωaξ) √0 as ξ √±∞,
(c) ∅
Ta f (ξ) = e−2ωiaξ f (ξ),
(d) f ′(ξ) = 2ωiξ f (ξ),
(e) ∅
f ∪g = f g.
6. F 1
√σ e−ωx2/σ 2 = √σ e−ωσ 2ξ2. Deduce that the convolution of two Gaussian
functions is another Gaussian function,
e−x2/2σ 2 ∪e−x2/2τ 2 =
√
2ω
στ
√
σ 2 + τ 2 e−x2/2(σ 2+τ 2).
Notice how there is a trade-off between the ‘width’ σ of the original Gaussian
and that of its Fourier transform, namely 1/σ.
7. Wiener-Khinchin theorem: For f ⊆L1(R), deﬁne f ∪(x) := f (−x). Show

f ∪= f , and the auto-correlation function f ∪∪f (x) =

f (t) f (t + x) dt
is transformed to the power spectrum | f (ξ)|2. More generally, f ∪∪g is called
the cross-correlation function of f and g.
Remarks 9.28
1. The functionals on ∂∞are more difﬁcult to describe. Every sequence y ⊆∂1 still
acts as a functional on ∂∞via x ∩√y · x, but ∂∞∪is a complicated non-separable
space that includes much more than just ∂1 (look up “ﬁnitely additive measures”
for more).
2. We often make remarks like “the dual space of c0 is ∂1”—this is not literally true
because a functional on c0 is not a sequence, but the application of one, i.e., it is
y◦not y. But the two are mathematically the same object in different clothing,
and functionals on c0 do behave like the sequences in ∂1.
3. ∂∞= Cb(N), so the completeness part of Theorem 9.1 is included in Theo-
rem 6.23.
4. The Fibonacci iteration an := an−1 + an−2, starting from a0 = 1 = a1, is an
equation on sequences. It can be written as

9.2 Function Spaces
169
x = Rx + R2x + e1 + e0
or as
(e0 −e1 −e2) ∪x = e0 + e1
(1, −1, −1, 0, . . .) ∪x = (1, 1, 0, . . .)
when expressed in terms of convolutions. Convoluting with the inverse of
(1, −1, −1, 0, . . .) gives the terms of the Fibonacci sequence (but note that the
inverse is not in ∂1). Traditionally, “generating functions” are used to get the same
results, the connection being elucidated in Chap.14.
5. ∂1 contains the space of sequences ∂1
s := { (an) : (nsan) ⊆∂1 }, (s ⩾0), which
in turn contains ∂∞
s+1+δ.
6. One can show that as p √∞, ∈x∈∂p √∈x∈∂∞, if x belongs to some ∂q.
7. The following are some classical criteria for determining that a sequence of mea-
surable functions fn that converges pointwise a.e. is Cauchy in L1(A),
(a) | fn| are increasing but

| fn| are bounded (Monotone Convergence Theo-
rem),
(b) | fn| ≤g ⊆L1(A) (Dominated Convergence Theorem),
(c)

E fn converges for all measurable sets E (Vitali’s theorem).
8. A function has both local and global integrability properties: locally about x ⊆R,
it may belong to some L p[x −γ, x + γ] space, while globally, the sequence of
numbers an := ∈f ∈L p[n,n+1] may belong to ∂q. For example, f is in L1(R) when
it is locally in L1 and globally in ∂1. L p
loc are spaces of functions that are only
locally in L p.
9. The Fourier series maps F : L p[0, 1] √∂p′ for 1 ≤p ≤2 (see Exer-
cise 10.35(14) for p = 2).

Chapter 10
Hilbert Spaces
10.1 Inner Products
There are spaces, such as ∂2, whose norms have special properties because they are
induced from what are termed inner products. Not only do such spaces have a concept
of length but also of orthogonality between vectors.
Deﬁnition 10.1
An inner product on a vector space X is a positive-deﬁnite sesquilinear form,1
namely a map
∞, √: X × X ∗F
such that for all x, y, z ⇒X, φ ⇒F,
∞x, y + z√= ∞x, y√+ ∞x, z√,
∞x, φy√= φ∞x, y√,
∞y, x√= ∞x, y√,
∞x, x√⩾0;
∞x, x√= 0 →x = 0.
Easy Consequences
1. If for all x ⇒X, ∞x, y√= 0, then y = 0.
2. ∞x + y, z√= ∞x, z√+ ∞y, z√, but ∞φx, y√= ¯φ∞x, y√(conjugate-linear).
3. ∞x, x√is real (and positive); its square-root is denoted by ♦x♦:= ∃∞x, x√.
4. ♦φx♦= |φ|♦x♦, and ♦x♦= 0 →x = 0.
1 In the mathematical literature, the inner product is often taken to be linear in the ﬁrst variable;
this is a matter of convention. The choice adopted here is that of the “physics” community; it
makes many formulas, such as the deﬁnition x∀(y) := ∞x, y√, more natural and conforming
with function notation.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_10,
171
© Springer International Publishing Switzerland 2014

172
10
Hilbert Spaces
5. ♦x + y♦2 = ♦x♦2 + 2 Re ∞x, y√+ ♦y♦2.
6. (Pythagoras) If ∞x, y√= 0 then ♦x + y♦2 = ♦x♦2 + ♦y♦2.
More generally, if ∞xi, x j√= 0 for i ∈= j then (by induction)
♦x1 + · · · + xN♦2 = ♦x1♦2 + · · · + ♦xN♦2.
We will see next that the triangle inequality is also true, making ♦· ♦a norm, thus
innerproductspacesarenormedspaces.Twovectorsareorthogonal orperpendicular
when ∞x, y√= 0, also written as x ⊆y. More generally, two subsets are said to be
orthogonal, A ⊆B, when any two vectors, a ⇒A, b ⇒B, are orthogonal, ∞a, b√= 0.
Examples 10.2
1. The simplest examples are the Euclidean spaces RN and CN with
∞

⎛⎜
a1
...
aN
⎝
⎞⎟,

⎛⎜
b1
...
bN
⎝
⎞⎟√:=
⎠¯a1 · · · ¯aN
⎫

⎛⎜
b1
...
bN
⎝
⎞⎟=
N
⎪
n=1
¯anbn.
More generally, take any basis v1, . . . , vN of FN, expand any two vectors x and
y as x = ⎬N
n=1 anvn, y = ⎬N
n=1 bnvn, and deﬁne ∞x, y√:= ⎬N
n=1 ¯anbn. (The
inner product differs depending on the choice of the basis.)
2. The matrices of size M × N have an inner product given by
∞A, B√:=
M
⎪
i=1
N
⎪
j=1
¯Ai j Bi j.
3. ▶∂2 has the inner product ∞(an), (bn)√:= ⎬∞
n=0 ¯anbn. The fact that this series
converges follows from Cauchy’s inequality | ⎬
n ¯anbn| ⩽♦(an)♦♦(bn)♦.
4. ▶L2(A) has the inner product ∞f, g√:=
⎭
A ¯f g. That this integral has a ﬁnite
value follows from Hölder’s inequality |
⎭
A ¯f g| ⩽♦¯f g♦L1 ⩽♦f ♦L2♦g♦L2.
5. The weighted ∂2 and L2 spaces generalize these formulae to
∞(an), (bn)√:=
⎪
n
¯anbnwn,
∞f, g√:=
⎧
f (x)g(x)w(x) dx
respectively, where wn and w(x) are called weights; what properties do they
need to have for the inner product axioms to hold?
Our ﬁrst proposition generalizes Cauchy’s inequality (Proposition 7.4) from ∂2 to
a general inner product space. It is probably the most used inequality in analysis.

10.1 Inner Products
173
Proposition 10.3 Cauchy-Schwarz inequality
|∞x, y√| ⩽♦x♦♦y♦
Proof The inequality need only be shown for y non-zero. Any other vector x can
be decomposed uniquely into two parts, one in the direction of y, and the other
perpendicular to it:
λy
x −λy
y
x
0
x = φy + (x −φy),
with ∞x −φy, y√= 0.
This yields φ = ∞y, x√/∞y, y√. Applying Pythago-
ras’ theorem, we deduce that
♦x♦2 = ♦φy♦2 + ♦x −φy♦2,
hence ♦φy♦⩽♦x♦, or |φ| ⩽♦x♦/♦y♦, from which
follows the assertion.
⇐⊂
Corollary 10.4
♦x + y♦⩽♦x♦+ ♦y♦
Proof Using the Cauchy-Schwarz inequality, Re ∞x, y√⩽|∞x, y√| ⩽♦x♦♦y♦, so
♦x + y♦2 = ♦x♦2 +2 Re ∞x, y√+♦y♦2 ⩽♦x♦2 +2♦x♦♦y♦+♦y♦2 = (♦x♦+♦y♦)2.
⇐⊂
Hence ♦· ♦is a norm, and all the facts about normed spaces apply to inner product
spaces. For example, the norm is continuous.
Proposition 10.5
The inner product is continuous.
Proof Let xn ∗x and yn ∗y, then since yn are bounded (Example 4.3(5)),
|∞xn, yn√−∞x, y√| ⩽|∞xn −x, yn√| + |∞x, yn −y√|
⩽♦xn −x♦♦yn♦+ ♦x♦♦yn −y♦
∗0.
⇐⊂
It follows that taking limits commutes with the inner product:
lim
n∗∞∞xn, yn√= ∞lim
n∗∞xn, lim
n∗∞yn√.

174
10
Hilbert Spaces
David Hilbert (1862–1943) studied invariant theory under Lin-
sr
e
w
o
p
ci
d
e
p
olc
y
c
n
e
si
H
.5
8
8
1
lit
n
u
g
r
e
b
s
gi
n
o¨
K
t
a
n
n
a
m
e
d
motivated him to explore much of mathematics; in 1899, in
;
y
rt
e
m
o
e
g
n
a
e
d
ilc
u
E
r
o
f
s
m
oi
x
a
s
u
o
r
o
gir
e
v
a
g
e
h
,
n
e
g
n
itt
o¨
G
1904-9, he studied Fredholm’s integral equations, with his stu-
dent Schmidt; he deﬁned compact operators, proving they are
limits of matrices, with their spectrum of eigenvalues; (Schmidt)
deﬁned
2 with its inner product. On to mathematical physics,
quite possibly he inspired Einstein’s general relativity. His 1918
‘formalist’ research programme set out to prove that set axioms
are consistent, “one can solve any problem by pure thought”.
Fig. 10.1 Hilbert
Deﬁnition 10.6
A Hilbert space is an inner product space which is complete as a metric space.
In the rest of the text, the letter H denotes a Hilbert space.
Examples 10.7
1. RN, CN, ∂2 and L2(R) are all Hilbert spaces (Theorem 8.22, 9.8).
2. Every inner product space can be completed to a Hilbert space. In the completion
as a normed space (Proposition 7.17), take ∞x, y√:= lim
n∗∞∞xn, yn√, for repre-
sentative Cauchy sequences x = [xn], y = [yn]. Note that ∞xn, yn√is a Cauchy
sequence in C since
|∞xn, yn√−∞xm, ym√| ⩽|∞xn, yn√−∞xm, yn√| + |∞xm, yn√−∞xm, ym√|
⩽♦xn −xm♦♦yn♦+ ♦xm♦♦yn −ym♦∗0
as n, m ∗∞, with ♦xm♦, ♦yn♦bounded.
3. ▶For an inner product space over C, if ∞x, T x√= 0 for all x ⇒X, then T = 0.
Proof The identities
0 = ∞x + y, T (x + y)√= ∞x, T y√+ ∞y, T x√,
0 = ∞x + iy, T (x + iy)√= i∞x, T y√−i∞y, T x√,
together imply ∞x, T y√= 0, for any x, y ⇒X, in particular ♦T y♦2 = 0.
4. An alternative proof of the Cauchy-Schwarz inequality is
0 ⩽♦u −φv♦2 = 1 −2 Re φ∞u, v√+ |φ|2

10.1 Inner Products
175
for u := x/♦x♦, v := y/♦y♦unit vectors and all φ ⇒F, in particular for
φ = |∞u, v√|/∞u, v√.
5. ♦x♦= sup
♦y♦=1
|∞x, y√|, with the maximum achieved when y = x/♦x♦.
Doallnormsonvectorspacescomefrominnerproducts,andifnot,whichproperty
characterizes inner product spaces? The answer is given by
Proposition 10.8 Parallelogram law
A norm is induced from an inner product if, and only if, it satisﬁes, for all
vectors x, y,
♦x + y♦2 + ♦x −y♦2 = 2(♦x♦2 + ♦y♦2).
x
y
x −y
x + y
The statement asserts that the sum of the lengths
squared of the diagonals of a parallelogram equals
that of the sides.
Proof The parallelogram law follows from adding the identities,
♦x + y♦2 = ♦x♦2 + 2 Re ∞x, y√+ ♦y♦2,
♦x −y♦2 = ♦x♦2 −2 Re ∞x, y√+ ♦y♦2.
Subtracting the two gives 4 Re ∞x, y√. This is already sufﬁcient to identify the inner
product when the scalar ﬁeld is R. Over C, notice that Im∞x, y√= −Re i∞x, y√=
Re ∞ix, y√, so
∞x, y√= 1
4
⎨
♦y + x♦2 −♦y −x♦2 + i♦y + ix♦2 −i♦y −ix♦2⎩
.
(10.1)
This remarkable polarization identity expresses the inner product purely in terms of
norms. Accordingly, for the converse of the proposition, deﬁne
for any normed space,
∞∞x, y√√:= 1
4(♦y + x♦2 −♦y −x♦2),
for a complex space,
∞x, y√:= ∞∞x, y√√+ i∞∞ix, y√√.
Two of the inner product axioms follow from ∞∞y, x√√= ∞∞x, y√√and ∞x, x√=
∞∞x, x√√= ♦x♦2, as well as ∞x, 0√= ∞∞x, 0√√= 0; ∞y, x√= ∞x, y√is readily veri-
ﬁed using
4∞∞iy, x√√= ♦x + iy♦2 −♦x −iy♦2 = ♦y −ix♦2 −♦y + ix♦2 = −4∞∞ix, y√√.

176
10
Hilbert Spaces
Showing that linearity holds when the parallelogram law is satisﬁed is the hardest
part of the proof. Writing
2y ± x = (y + z ± x) + (y −z),
2z ± x = (y + z ± x) −(y −z),
and using the parallelogram law,
4∞∞x, 2y√√+ 4∞∞x, 2z√√= ♦2y + x♦2 −♦2y −x♦2 + ♦2z + x♦2 −♦2z −x♦2
= ♦2y + x♦2 + ♦2z + x♦2 −♦2y −x♦2 −♦2z −x♦2
= 2♦y + z + x♦2 + 2♦y −z♦2 −2♦y + z −x♦2 −2♦y −z♦2
= 8∞∞x, y + z√√.
In particular, putting z = 0 gives ∞∞x, 2y√√= 2∞∞x, y√√, reducing the above identity to
∞∞x, y + z√√= ∞∞x, y√√+ ∞∞x, z√√.
(10.2)
By induction, it follows that ∞∞x, ny√√= n∞∞x, y√√for n ⇒N. For the negative integers,
∞∞x, −y√√= ♦−y + x♦2 −♦−y −x♦2 = −∞∞x, y√√
while for rational numbers p = m/n, m, n ⇒Z, n ∈= 0,
n∞∞x, m
n y√√= ∞∞x, my√√= m∞∞x, y√√
so ∞∞x, py√√= p∞∞x, y√√. Note that ∞∞x, y√√is continuous in x and y since the norm is
continuous, so if the rational numbers pn ∗ψ ⇒R, then
∞∞x, ψ y√√= lim
n∗∞∞∞x, pn y√√= lim
n∗∞pn∞∞x, y√√= ψ∞∞x, y√√.
This completes the proof when the scalar ﬁeld is R. Over the complex numbers,
∞x, φy√= φ∞x, y√for φ ⇒C is evident from (10.1), (10.2), and
∞x, iy√= −∞∞ix, y√√+ i∞∞x, y√√= i∞x, y√.
⇐⊂
In a sense, it is the presence of orthogonality that distinguishes inner product
spaces from normed ones. By the polarization identity, two vectors are perpendicular
when ♦x + y♦= ♦x −y♦and ♦x + iy♦= ♦x −iy♦. Each vector, and more gener-
ally each subspace, is complemented by a subspace of those vectors that are perpen-
dicular to it.

10.1 Inner Products
177
Proposition 10.9 Properties of Orthogonal Spaces
The orthogonal spaces of subsets A ⇔X,
A⊆:= { x ⇒X : ∞x, a√= 0, ∩a ⇒A },
satisfy
(i) A ∪A⊆⇔0,
(ii) A ⇔B →B⊆⇔A⊆, and A ⇔A⊆⊆
(iii) A⊆is a closed subspace of X,
(iv) A⊆= [[A]]⊆.
Proof (i) If a vector a ⇒A is also in A⊆, then it is orthogonal to all vectors in A,
including itself, ∞a, a√= 0, so a = 0.
(ii) If a ⇒A ⇔B and x ⇒B⊆, then ∞x, a√= 0, so x ⇒A⊆. For any a ⇒A and
x ⇒A⊆, ∞a, x√= ∞x, a√= 0, so a ⇒A⊆⊆.
(iii) If x and y are in A⊆and a ⇒A, then
∞φx, a√= ¯φ∞x, a√= 0,
∞x + y, a√= ∞x, a√+ ∞y, a√= 0,
so φx, x + y ⇒A⊆. If xn ⇒A⊆and xn ∗x, then 0 = ∞xn, a√∗∞x, a√, and
x ⇒A⊆.
(iv) That [[A]]⊆⇔A⊆follows from A ⇔[[A]]. Conversely, let x ⇒A⊆; for any
a, b ⇒A,
∞x, a + b√= ∞x, a√+ ∞x, b√= 0,
∞x, φa√= φ∞x, a√= 0,
so x is orthogonal to the space generated by A, x ⇒[[A]]⊆. Let an ∗y with
an ⇒[[A]], then 0 = ∞x, an√∗∞x, y√and x ⇒[[A]]⊆.
⇐⊂
Exercises 10.10
1. If T, S : X ∗Y are linear maps on inner product spaces such that ∞y, T x√=
∞y, Sx√for all x ⇒X, y ⇒Y, then T = S. Example 10.7(3) is false for real
spaces: Find a non-zero 2 × 2 real matrix T such that ∞x, T x√= 0 for all
x ⇒R2.
2. The Cauchy-Schwarz inequality becomes an equality if, and only if, x = φy
for some scalar φ (or y = 0). Similarly, ♦x + y♦= ♦x♦+ ♦y♦precisely when
x = φy, φ ⩾0. More generally, ♦⎬
n xn♦= ⎬
n ♦xn♦if, and only if, xn = φnx
for some φn ⩾0.

178
10
Hilbert Spaces
3. For any x, y ⇒H, ﬁnd scalars |φ| = 1 = |μ| such that ♦x♦2 + ♦y♦2 ⩽
♦φx + μy♦2.
4. A vector space may have various inner products. When T : X ∗X is 1-1
and linear, ∞∞x, y√√:= ∞T x, T y√is another legitimate inner product on X. What
properties does S need to have to ensure that ∞∞x, y√√:= ∞x, Sy√is also an inner
product?
5. * Every inner product on RN is of the type ∞x, A y√= ⎬
i j Ai jaib j where A is
a positive symmetric matrix. Deduce that balls have the shape of an ellipse in
R2, and of an ellipsoid in R3.
6. ▶The product of two inner product spaces, X ×Y, has an inner product deﬁned
by
∞
 x1
y1

,
 x2
y2

√:= ∞x1, x2√X + ∞y1, y2√Y .
Then the maps x ∗
⎠x
0
⎫
and y ∗
⎠0y
⎫
embed X and Y as orthogonal subspaces
of X × Y. Although the induced norm is not the same one we deﬁned for X × Y
as normed spaces (Example 7.3(8)), the two norms are equivalent.
When X, Y are complete, so is X × Y with the induced norm (note that
♦x♦⩽
⎠xy
⎫).
7. In any inner product space,
(a) ♦x −y♦2 + ♦x + y −2z♦2 = 2♦x −z♦2 + 2♦y −z♦2.
(b) ♦x + y + z♦2 + ♦x + y −z♦2 + ♦x −y + z♦2 + ♦x −y −z♦2
= 4(♦x♦2 + ♦y♦2 + ♦z♦2).
8. Verify that the norms for ∂2 and L2(R) satisfy the parallelogram law, and show
that the inner product obtained from the polarization identity is the same one
deﬁned previously (Examples 10.2(3, 4)).
9. The 1-norm and ∞-norm deﬁned on R2 do not come from inner products. Find
two vectors that do not satisfy the parallelogram law.
10. ▶Similarly, ∂1, ∂∞, L1(R) and L∞(R) are not inner product spaces. Neither is
B(X, Y) in general.
11. A norm ♦·♦that satisﬁes the parallelogram law gives rise to its associated inner
product, by the polarization identity. In turn, this inner product induces the norm
|||x||| := ∃∞x, x√. Show that the two norms are identical.
12. The polynomials x and 2x2−1 are orthogonal in L2[0, 1]. So are sine and cosine
in the space L2[−δ, δ]; can you ﬁnd a function orthogonal to both?
13. 0⊆= X, X⊆= 0. In fact, A⊆= X →A ⇔{ 0 }. Do you think it is true that
A⊆= 0 →A = X? What if A is a closed linear subspace of X?

10.1 Inner Products
179
14. Show that (a) (A + B)⊆= A⊆∪B⊆, (b) A⊆⊆⊆= A⊆. (Hint: use property (ii)
of the proposition.)
15. Let d := d(x, [[y]]) = infφ ♦x + φy♦, where y is a unit vector; show that (a)
d = ♦x + φ0y♦for some φ0, (b) |∞x, y√|2 = ♦x♦2 −d2, and (c) y ⊆(x −φ0y).
16. To illustrate the strength of orthogonality, prove that if M ⊆N are orthogonal
complete subspaces of X, then M + N is also complete (Example 7.11(2)).
17. Suppose a vector space X satisﬁes all the axioms for an inner product space
except that it contains non-zero vectors with ∞x, x√= 0. Show that if ∞x, x√= 0,
then ∩y, ∞x, y√= 0 (Hint: expand ♦y −φx♦2).
Deduce that Pythagoras’ theorem and Cauchy-Schwarz’s inequality remain
valid. Show that Z := { x : ∞x, x√= 0 } is a closed linear subspace, and that
there is a well-deﬁned inner product on X/Z, ∞x + Z, y + Z√:= ∞x, y√.
18. A light ‘ray’ has a frequency proﬁle f (θ). Oversimplifying slightly, our eyes
convert it to a color vector (∞r, f √, ∞g, f √, ∞b, f √) where r(θ), g(θ), b(θ) are
the absorption proﬁles of the retinal cones. So any two points (rays) in the coset
f + [[r, g, b]]⊆have the same color.
10.2 Least Squares Approximation
By Exercise 10.10(15) above, the distance between a point and a line can be min-
imized by a unique point on the line. This has a generalization with far-reaching
consequences:
Theorem 10.11
If M is a closed convex subset of a Hilbert space H, then any point in H
has a unique point in M which is closest to it,
∩x ⇒H, ◦!y∀⇒M, ∩y ⇒M,
♦x −y∀♦⩽♦x −y♦.
Proof Let d := d(x, M) = inf y⇒M ♦x −y♦be the smallest distance from M to x.
Then there is a sequence of vectors yn ⇒M such that ♦x −yn♦∗d. Now, using
the parallelogram law and the convexity of M, (yn) is a Cauchy sequence,
♦yn −ym♦2 = 2♦yn −x♦2 + 2♦ym −x♦2 −♦(yn + ym) −2x♦2
= 2♦yn −x♦2 + 2♦ym −x♦2 −4
 yn + ym
2
−x
2
⩽2♦yn −x♦2 + 2♦ym −x♦2 −4d2
∗0,
as n, m ∗∞.

180
10
Hilbert Spaces
But M is closed (hence complete) and so yn ∗y∀⇒M. It follows, by continuity of
the norm, that ♦x −y∀♦= lim
n∗∞♦x −yn♦= d.
Suppose a ⇒M is another closest point to x, i.e., ♦x −a♦= d. Then y∀= a
since
♦y∀−a♦2 = 2♦y∀−x♦2 + 2♦a −x♦2 −♦(y∀+ a) −2x♦2
⩽2♦y∀−x♦2 + 2♦a −x♦2 −4d2
= 0.
⇐⊂
Let us concentrate on the special case when M is a closed subspace of H.
Theorem 10.12
When M is a closed linear subspace of a Hilbert space H, then y ⇒M is
the closest point y∀to x ⇒H if, and only if,
x −y ⇒M⊆.
The map P : x ∗y∀is a continuous ‘orthogonal’ projection with im P =
M orthogonal to kerP = M⊆, so
H = M ≈M⊆.
x 0
x
λa
b
M
a
Proof (i) Let a be any non-zero point of M and let
b := x−(y∀+φa)whereφischosensothata ⊆b,that
is, φ := ∞a, x −y∀√/♦a♦2. By Pythagoras’ theorem,
we get
♦x −y∀♦2 = ♦b + φa♦2 = ♦b♦2 + ♦φa♦2 ⩾♦b♦2
making y∀+φa even closer to x than the closest point
y∀, unless φ = 0, i.e., ∞a, x −y∀√= 0. Since a is
arbitrary, this gives x −y∀⊆M.
Conversely, if (x −y) ⊆a≡for any a≡⇒M, then (x −y) ⊆(a≡−y) and
Pythagoras’ theorem implies
♦x −a≡♦2 = ♦x −y♦2 + ♦y −a≡♦2,
so that ♦x −y♦⩽♦x −a≡♦, making y the closest point in M to x.
(ii) By the above, for any x ⇒H, P(x) is that unique vector in M such that x−P(x) ⇒
M⊆. This characteristic property has the following consequences:

10.2 Least Squares Approximation
181
• P is linear since
(x + y) −(Px + Py) = (x −Px) + (y −Py) ⇒M⊆,
Px + Py ⇒M,
hence P(x + y) = Px + Py. Similarly, P(φx) = φPx.
• The closest point in M to a ⇒M is a itself, i.e., Pa = a, so im P = M. Since
Px ⇒M, it also follows that P2x = Px, and P2 = P.
• When x ⇒M⊆, then x −0 ⇒M⊆and 0 ⇒M so Px = 0. As Px = 0 implies
x = x −Px ⇒M⊆, this justiﬁes kerP = M⊆.
• P is continuous since ♦x♦2 = ♦x −Px♦2 + ♦Px♦2 by Pythagoras’ theorem so
that ♦Px♦⩽♦x♦.
Finally H = im P ≈kerP = M ≈M⊆, since any vector can be decomposed as
x = Px + (x −Px), and M ∪M⊆= 0.
⇐⊂
Corollary 10.13
For any subset A ⇔H,
A⊆⊆= [[A]].
Proof Let M be a closed linear subspace of a Hilbert space H. By Proposition 10.9,
M ⇔M⊆⊆, so we require the opposite inclusion. Let x ⇒M⊆⊆, then x = a + b
where a ⇒M and b ⇒M⊆, and
0 = ∞b, x√= ∞b, a√+ ∞b, b√= ♦b♦2,
forcing b = 0 and x ⇒M; thus M⊆⊆⇔M. In particular, A⊆⊆= [[A]]⊆⊆= [[A]]. ⇐⊂
Note that M⊆= 0 →M = M⊆⊆= 0⊆= H, answering Exercise 10.10(13) in
the case of a closed linear subspace of a Hilbert space.
Examples 10.14
1. Let M := { f ⇒L2[0, 1] :
⎭1
0 f = 0 }. To ﬁnd that function f0 in M which most
closely approximates a given function g, we ﬁrst note
M = { f ⇒L2[0, 1] : ∞1, f √= 0 } = { 1 }⊆,
so M⊆= [[1]].
Then f0 must satisfy f0 ⇒M and g −f0 ⇒M⊆, i.e., f0 = g + φ and
0 =
⎭1
0 f0 =
⎭1
0 g + φ, hence f0 = g −
⎭1
0 g.
2. The “afﬁne” projection onto a plane with equation x · n = d (n a unit vector) is
given by P(x) := x + (d −x · n)n.

182
10
Hilbert Spaces
Proof Translate all points x ∗y := x −dn, so that the plane becomes the
subspace M with equation y · n = 0, i.e., M = { n }⊆. The required point
satisﬁes (y −y0)· ˜y = 0 for all ˜y ⇒M, so y0 = y +ψn. Dotting with n implies
ψ = −y · n = d −x · n, which can be substituted into x0 = x + ψn.
3. A projection is orthogonal if, and only if, ♦P♦= 1 (unless P = 0).
Proof Using ∞x −Px, Px√= 0 and the Cauchy-Schwarz inequality,
♦Px♦2 = ∞x, Px√⩽♦x♦♦Px♦,
so ♦Px♦⩽♦x♦; but Px = x for x ⇒im P, so ♦P♦= 1. Conversely, let
a ⇒kerP, b ⇒im P; then for any φ,
♦b♦2 = ♦P(φa + b)♦2 ⩽♦φa + b♦2 = |φ|2♦a♦2 + 2 Re φ∞b, a√+ ♦b♦2
and after letting φ = |φ|eiα with |φ| ∗0, we ﬁnd Re eiα∞b, a√⩾0 for any α,
hence ∞b, a√= 0.
4. ▶[[A]] is dense in H if, and only if, A⊆= 0.
Proof If A⊆= 0, then [[A]] = A⊆⊆= 0⊆= H. Conversely, if A is dense in H,
then A⊆= [[A]]⊆= H⊆= 0.
Application: Least Squares Approximation
A common problem in mathematical applications is to approximate a generic vector
x by one which is more easily handled, such as a linear combination of simpler
vectors y1, . . . , yN. For Hilbert spaces, there is a guarantee that a unique closest
approximation exists, and this lies at the heart of the method of least squares.
Let M := [[y1, . . . , yN]], a closed linear subspace of H; then the closest point
in M to x is y∀= ⎬N
j=1 ψ j y j such that x −y∀⊆M. Since M is generated by
y1, . . . , yN, this is equivalent to
∞yi, x −y∀√= 0,
i = 1, . . . , N,
∴
∞yi, x√= ∞yi, y∀√=
N
⎪
j=1
∞yi, y j√ψ j.
These N linearequationsinthe N unknownsψ1, . . . , ψN,canberecastinmatrixform,

⎛⎜
∞y1, y1√. . . ∞y1, yN√
...
...
∞yN, y1√. . . ∞yN, yN√
⎝
⎞⎟

⎛⎜
ψ1
...
ψN
⎝
⎞⎟=

⎛⎜
∞y1, x√
...
∞yN,x√
⎝
⎞⎟.
Given x, the coefﬁcients ψi can be found by solving these equations. The Gram
matrix [∞yi, y j√], and possibly its inverse, need only be calculated once, and used to
approximate other points.

10.2 Least Squares Approximation
183
Example The space of cubic polynomials, a+bx +cx2+dx3, is a four-dimensional
closed linear subspace of the Hilbert space L2[0, 1], with basis 1, x, x2, x3. Their
Gram matrix and inverse are given by

⎛⎛⎜
1 1/2 1/3 1/4
1/2 1/3 1/4 1/5
1/3 1/4 1/5 1/6
1/4 1/5 1/6 1/7
⎝
⎞⎞⎟
−1
=

⎛⎛⎜
16
−120
240
−140
−120 1200 −2700 1680
240 −2700 6480 −4200
−140 1680 −4200 2800
⎝
⎞⎞⎟.
So, to approximate the sine function by a cubic polynomial over the region [0, 1], we
ﬁrst calculate ∞xi, sin √L2[0,1], which work out to (0.460, 0.301, 0.223, 0.177), and
then apply the inverse of the Gram matrix to it, giving
p(x) ≈−0.000253 + 1.005x −0.0191x2 −0.144x3.
Notice that the coefﬁcients are close to, but not the same, as the ﬁrst terms of
the MacLaurin expansion of sine. The difference is that, whereas the MacLau-
rin expansion is accurate at 0 and becomes progressively worse away from it,
the L2-approximation balances out the ‘root-mean-square error’ throughout the
region [0, 1].
Exercises 10.15
1. Find the closest point in the plane 2x + y −3z = 0 to a point x ⇒R3. (Hint:
Find M⊆.)
2. Let (a) M := [[y]], or (b) M := { y }⊆, where y is a unit vector. The orthogonal
projection P which maps any point x to its closest point in M is (a) Px = ∞y, x√y,
(b) Px = x −∞y, x√y.
3. ▶In the decomposition x = a +b with a ⇒M and b ⇒M⊆, a and b are unique.
Deduce that if H = M ≈N, where M is a closed linear subspace and M ⊆N,
then N = M⊆.
4. Let a + M be a coset of a closed linear subspace M. Show that there is a
unique vector x ⇒a + M with smallest norm. (Hint: this is equivalent to ﬁnding
the closest vector in M to −a.) Deduce that Riesz’s lemma (Proposition8.20)
continues to hold in a Hilbert space even when c = 1.
5. If M ⇔N are both closed linear subspaces, then M ≈(M⊆∪N) = N.
6. Let T be a square matrix, and suppose both subspaces M and M⊆are T -invariant,
so that T takes the schematic form
 A 0
0 B

. Show that ♦T ♦= max(♦A♦, ♦B♦).
(Hint: take x = a + b, then ♦T x♦2 = ♦Ta♦2 + ♦T b♦2.)

184
10
Hilbert Spaces
7. ▶There is a 1–1 correspondence between closed linear subspaces of a Hilbert
space and orthogonal projections (onto them). Properties about subspaces are
reﬂected as properties of the projections, e.g. if the orthogonal projections PM
and PN project onto M and N respectively, then
(a) M ⇔N →PM PN = PM = PN PM,
(b) M ⊆N →PM PN = 0 = PN PM,
(c) N = M⊆→I = PM + PN,
(d) M is T -invariant →T PM = PMT PM,
(e) M and M⊆are both T -invariant →T PM = PMT ,
8. (a) Since ∞x, a√= ∞Px, a√for any point a in a closed linear subspace M, it fol-
lows that |∞x, a√| ⩽♦Px♦♦a♦with equality when a ⇒[[Px]]. Deduce that in
a real Hilbert space, the angle between x and a is at least cos−1(♦Px♦/♦x♦).
(b) Let H = M ≈N with M, N non-zero closed subspaces. Show that there is
a minimum distance d > 0 between the disjoint closed sets BH ∪M and
BH ∪N; thus for any unit vectors x ⇒M, y ⇒N, ♦x −y♦⩾d > 0.
Deduce that Re ∞x, y√⩽ψ := 1 −d2/2, and hence that
∩x ⇒M, ∩y ⇒N,
|∞x, y√| ⩽ψ♦x♦♦y♦.
9. The main theorem, which does not refer to inner products, is not true in Banach
spaces in general.
(a) In R2 with the 1-norm, the vector
⎠−1
1
⎫
has many closest vectors in the closed
ball B1[0].
(b) In ∂∞, there are many sequences in c0 that have the minimum distance to
(1, 1, . . .).
(c) Show that, in a normed space, the set of best approximations in a convex set
M to a point x is convex.
(d) * On the other hand, in ∂∞, the sequence 0 has no closest sequence in the
closed convex set M := { (an) ⇒c0 : ⎬
n an/2n = 1 }.
10. * Consider two orthogonal projections P and Q in RN. Show that the iteration
yn+1 := QP yn starting from y0 = x converges to a point x∀⇒im P ∪im Q.
11. Find
(a) the best-ﬁtting quadratic and cubic polynomials to the sine function in
[0, 2δ],
(b) the linear combination of sin and cos which is closest to 1 −x3 in L2[0, 1].

10.2 Least Squares Approximation
185
12. (a) The Gram matrix of vectors y1, . . . , yN is G := A∀A where the columns
of A are yn, and the rows of A∀are y∇
n. It is invertible when yn are linearly
independent.
(b) Show that in order to write a vector x as a linear combination of basis vectors
x = ⎬N
n=1 ψn yn, given the numbers bn := ∞yn, x√, then one needs to solve
the matrix equation Gα = b.
(c) Given the total mass and moment of inertia of a radially symmetric planar
object,
M = 2δ
⎧R
0
ρ(r)r dr = 2δ∞r, ρ(r)√L2[0,R],
I = 2δ
⎧R
0
ρ(r)r3 dr = 2δ∞r3, ρ(r)√L2[0,R],
ﬁnd an estimate of ρ(r) as some function ψ + βr.
13. The symmetric Gram matrix of a set of vectors xn ⇒RN is useful in other
contexts as well. Show how to recover
(a) the vectors xn from their Gram matrix, up to an isomorphism (use diago-
nalization to ﬁnd A such that A2 = G),
(b) the Gram matrix of the vectors from the mutual distances between vectors
di j, and their norms ri,
(c) the Gram matrix from di j only, assuming ⎬
n xn = 0.
This is essentially what is done in the Global Positioning System, when 3 to 4
distances obtained by time-lags from satellites are converted to a position.
10.3 Duality H∗≈H
An inner product is a function acting on two variables. But if one input vector is
ﬁxed, it becomes a scalar-valued function on vectors, indeed a continuous functional
x∀: X ∗F
y ∗∞x, y√.
This is linear by the inner product axioms, while continuity follows from the Cauchy-
Schwarz inequality |x∀y| = |∞x, y√| ⩽♦x♦♦y♦.
Are there any other functionals besides these? Not when the space is complete:

186
10
Hilbert Spaces
Theorem 10.16 Riesz representation theorem
Every continuous functional of a Hilbert space H is of the form x∀:= ∞x, √,
∩φ ⇒H∀, ◦!x ⇒H,
φ = ∞x, √.
The Riesz map
J : H ∗H∀
x ∗x∀
is a bijective conjugate-linear isometry.
Proof (i) Given φ ⇒H∀, ﬁrst notice that for any z and y in H,
(φy)z −(φz)y ⇒ker φ.
Assuming φ ∈= 0, pick a unit vector z ⊆ker φ; this is possible since ker φ ∈= H, so
(ker φ)⊆∈= 0. Then
0 = ∞z, (φy)z −(φz)y√= (φy) −(φz)∞z, y√,
φy = (φz)∞z, y√= ∞x, y√,
where x = (φz)z. To show that it is unique, suppose ˜x is another such x, then
∩y ⇒H,
∞x −˜x, y√= ∞x, y√−∞˜x, y√= φy −φy = 0 →x = ˜x.
(ii) Part (i) proves that J is onto and 1–1. Let x and y be two vectors in H. Then for
any z ⇒H,
(x + y)∀(z) = ∞x + y, z√= ∞x, z√+ ∞y, z√= x∀z + y∀z,
(φx)∀(z) = ∞φx, z√= ¯φ∞x, z√= ¯φ x∀z,
showing that (x + y)∀= x∀+ y∀and (φx)∀= ¯φx∀(conjugate-linear).
To see that J is isometric, note that
♦x∀♦H∀= sup
y∈=0
|x∀y|
♦y♦= sup
y∈=0
|∞x, y√|
♦y♦
= ♦x♦,
using the Cauchy-Schwarz inequality, in particular with y = x.
⇐⊂

10.3 Duality H∀≈H
187
Frigyes Riesz (1880-1956) was a Hungarian mathematics profes-
sor who proved that L2(R) is complete; in 1907, with E.S. Fis-
cher, he proved that Hilbert’s
2 space is equivalent to L2(R); he
deﬁned compact operators abstractly for more general spaces,
including C[a, b] (1918); he introduced the resolvent projection
to part of the spectrum and thus f(T ) for compact operators.
Fig. 10.2 Riesz
Examples 10.17
1. The dual space of R is (isomorphic to) R itself. Any φ : R ∗R that is linear
must be of type φ(t) = φt where φ ⇒R.
2. Functionals are simply row vectors when H = CN; thus H∀is isometric to CN
and is generated by the dual basis e∇
1, . . . , e∇
N.
Proof Let e1, …, eN, be the standard basis for CN. Then every functional φ in
(CN)∀is of the type φ = (bn)∇, where bn := φen, Example 8.4(3). Thus the
map CN ∗(CN)∀, y ∗y∇, where y∇x := y · x, is onto; it is easily seen to
be linear, and continuous from Cauchy’s inequality |y · x| ⩽♦y♦♦x♦. In fact
♦y∇♦= ♦y♦(using x = (¯bn)). Note that y∇= ⎬N
n=1 bne∇
n, and e∇
n em = δnm.
3. It was noted previously that ∂2∀≤∂2 and L2(R)∀≤L2(R) (Exercise9.23(3)).
These are special cases of the Riesz correspondence.
Exercises 10.18
1. ▶For T ⇒B(X, Y) (X, Y Hilbert spaces),
♦x♦= sup
♦y♦=1
|∞y, x√|,
♦T ♦=
sup
♦x♦=1=♦y♦
|∞y, T x√|.
2. Show that the norm of H∀comes from the inner product ∞x∀, y∀√H∀:= ∞y, x√H.
3. A functional φ ⇒H∀corresponds to some vector x ⇒H; if M is a closed linear
subspace of H, φ can be restricted to act on it, ˜φ ⇒M∀. As M is a Hilbert space
in its own right, what vector a ⇒M corresponds to ˜φ?
4. A second inner product on H which satisﬁes |∞∞x, y√√| ⩽c♦x♦♦y♦must be of
the type ∞∞x, y√√= ∞T x, y√= ∞x, T y√, where T ⇒B(H), ♦T ♦⩽c.
5. Riesz’s theorem holds only for complete inner product spaces (it is false for, say,
c00 ⊂∂2). Where is completeness used in the proof of the theorem?

188
10
Hilbert Spaces
10.4 The Adjoint Map T∗
We now seek to ﬁnd a generalization of the transpose operation on matrices. In
ﬁnite dimensions, we have (A∀v)∀= v∀A; in terms of inner products, this becomes
∞A∀v, x√= ∞v, Ax√. In this form, it can be generalized to any Hilbert space:
Deﬁnition 10.19
The (Hilbert) adjoint of an operator T : X ∗Y between Hilbert spaces, is
the operator T ∀: Y ∗X uniquely deﬁned by the relation
∞T ∀y, x√X = ∞y, T x√Y
∩x ⇒X, y ⇒Y.
That T ∀y is uniquely deﬁned follows from the Riesz correspondence applied to
the functional x ∗∞y, T x√. Linearity and continuity of T ∀follow from
∞T ∀(y1 + y2), x√= ∞y1 + y2, T x√= ∞y1, T x√+ ∞y2, T x√= ∞T ∀y1 + T ∀y2, x√
∞T ∀(φy), x√= ∞φy, T x√= ¯φ∞y, T x√= ∞φT ∀y, x√
♦T ∀♦=
sup
♦y♦=1=♦x♦
|∞T ∀y, x√| =
sup
♦y♦=1=♦x♦
|∞y, T x√| = ♦T ♦
The properties of the adjoint map are:
Proposition 10.20
(S + T )∀= S∀+ T ∀,
(φT )∀= ¯φT ∀,
(ST )∀= T ∀S∀,
I ∀= I,
T ∀∀= T,
♦T ∀T ♦= ♦T ♦2
Proof These assertions follow from the following identities, valid for all x ⇒X,
y ⇒Y:
∞(S + T )∀y, x√= ∞y, (S + T )x√= ∞y, Sx√+ ∞y, T x√= ∞(S∀+ T ∀)y, x√
∞(φT )∀y, x√= ∞y, φT x√= φ∞T ∀y, x√= ∞¯φT ∀y, x√
∞(ST )∀y, x√= ∞y, ST x√= ∞S∀y, T x√= ∞T ∀S∀y, x√
∞I ∀y, x√= ∞y, I x√= ∞y, x√
∞y, T ∀∀x√= ∞T ∀∀x, y√= ∞x, T ∀y√= ∞T ∀y, x√= ∞y, T x√,
♦T ∀T ♦= sup
x,y⇒S
|∞y, T ∀T x√| = sup
x,y⇒S
|∞T y, T x√|
= sup
x,y⇒S
♦T y♦♦T x♦= ♦T ♦2,

10.4 The Adjoint Map T ∀
189
where S := { x : ♦x♦= 1 }, and the equation before the last is valid by the Cauchy-
Schwarz inequality, in particular choosing y = x.
⇐⊂
The following proposition reveals an orthogonality between subspaces of adjoint
operators. In particular, both M and M⊆are T -invariant if, and only if, M is T - and
T ∀-invariant.
Proposition 10.21
For an operator T on Hilbert spaces,
kerT ∀= (im T )⊆,
im T ∀= (kerT )⊆.
If T ⇒B(H) and M is a closed linear subspace of H,
M is T -invariant →M⊆is T ∀-invariant.
Proof The deﬁnition ∞T ∀x, y√= ∞x, T y√implies that
x ⊆T y
→
T ∀x ⊆y,
in particular x ⊆im T →T ∀x ⊆Y →x ⇒kerT ∀. Consequently, kerT ∀=
(im T )⊆and thus kerT = kerT ∀∀= (im T ∀)⊆; furthermore,
(kerT )⊆= (im T ∀)⊆⊆= im T ∀.
Suppose M is T -invariant, and let x ⇒M⊆, y ⇒M, then ∞T ∀x, y√= ∞x, T y√= 0,
and T ∀x ⇒M⊆. Conversely, if M⊆is T ∀-invariant then M⊆⊆is T ∀∀-invariant; but
T ∀∀= T and M⊆⊆= M for a closed subspace M.
⇐⊂
Unitary Operators
Deﬁnition 10.22
A unitaryisomorphism J : X ∗Y of inner product spaces is deﬁned as a
map which preserves all the structure of an inner product space, namely
J is bijective
(preservation of the elements),
J is linear
(preservation of vector addition and scalar multiplication), and
∞Jx, Jy√Y = ∞x, y√X (preservation of the inner product).

190
10
Hilbert Spaces
It is obvious that a unitary isomorphism preserves the induced norm (an isometry);
the converse is also partly true in Hilbert spaces, because, by the polarization identity,
the inner product can be written in terms of norms:
Proposition 10.23
An operator U ⇒B(X, Y) on Hilbert spaces preserves the inner product
when U preserves the norm,
∩x, ˜x ⇒X, ∞Ux,U ˜x√= ∞x, ˜x√→U ∀U = I
→♦Ux♦= ♦x♦
∩x ⇒X.
U is unitary when it is also onto.
This statement is basically saying that preserving the inner product (lengths and
‘angles’) is equivalent to preserving lengths.
Proof The ﬁrst equivalence is trivial
∩x, ˜x, ∞x, ˜x√= ∞Ux,U ˜x√= ∞x,U ∀U ˜x√→U ∀U = I.
In particular (taking ˜x = x), U is isometric. The converse implication from the third
statement to the ﬁrst follows from the polarization identity (10.1),
∞Ux,Uy√= 1
4(♦Ux + Uy♦+ · · · ) = 1
4(♦x + y♦+ · · · ) = ∞x, y√.
A superﬁcially different proof of this last fact can be given for complex Hilbert
spaces (Example 10.7(3)),
∩x, ∞x, x√= ∞Ux,Ux√= ∞x,U ∀Ux√→U ∀U = I.
Since isometries are 1–1, we need only require in addition that it is onto for U to be
invertible, in which case U −1 = U ∀.
⇐⊂
Examples 10.24
1. The adjoint of a matrix A = [Ai j] is the conjugate of its transpose, ¯A∇, since
∞x, A y√=
⎪
i j
¯xi Ai j y j =
⎪
i j
( ¯Ai j xi)y j = ∞¯A∇x, y√.
2. ▶The adjoint of the left-shift operator (on ∂2) is the right-shift, L∀= R, since

10.4 The Adjoint Map T ∀
191
∞L∀y, x√= ∞y, Lx√=
∞
⎪
n=0
bnan+1 =
∞
⎪
n=1
bn−1an = ∞R y, x√
and R∀= L∀∀= L.
3. The adjoint of an integral operator on L2(R),
T f (y) :=
⎧
k(x, y) f (x) dx
is
T ∀g(x) =
⎧
k(x, y)g(y) dy.
Proof
∞g, T f √=
⎧
g(y)
⎧
k(x, y) f (x) dx dy
=
⎧⎧
k(x, y)g(y) f (x) dy dx
=
⎧⎧
k(x, y)g(y) dy f (x) dx = ∞T ∀g, f √.
4. The unitary2 isomorphisms of R2 are the rotations and reﬂections. More gen-
erally, those of Rn are the matrices whose columns are orthonormal (mutually
orthogonal and of unit norm).
Proof The column vectors ui of a unitary matrix U satisfy ui = Uei, where ei
are the standard basis for Rn. Then, ∞ui, u j√= ∞Uei,Ue j√= ∞ei, e j√= δi j.
5. ▶By itself, U ∀U = I ensures that a linear operator U : X ∗Y is isometric
(and 1–1), but not that it is onto, that is, it is an isometric embedding of X into
Y. For example, the matrix

⎜
0 1
1 0
0 0
⎝
⎟embeds R2 into R3. In general, UU ∀is not
equal to I but is a projection of Y onto im U ⇔Y.
Proof Clearly, UU ∀UU ∀= UU ∀is a projection from Y to im U. It is onto
since UU ∀(Ux) = Ux.
10.5 Inverse Problems
When an operator T : X ∗Y is not onto, the equation T x = y need not have a
solution. The next best thing to ask for is a vector x which minimizes ♦T x −y♦.
2 More properly called orthogonal isomorphisms when the space is real.

192
10
Hilbert Spaces
Proposition 10.25
For an operator T : H1 ∗H2 between Hilbert spaces and a vector
y ⇒H2, a vector x ⇒H1 minimizes ♦T x −y♦if, and only if
T ∀T x = T ∀y.
Proof Suppose T ⇒B(X, Y), and consider the closed linear subspace M := im T ⇔
Y. For each y ⇒Y, there is a unique vector y∀⇒M which is closest to it. As proved in
Theorem 10.12, a necessary and sufﬁcient condition for y∀is y −y∀⇒M⊆= kerT ∀
(Proposition10.21), that is, T ∀y∀= T ∀y.
If y∀happens to be in im T , i.e., y∀= T x, then the equation becomes T ∀T x =
T ∀y; this can only occur when y ⇒im T ≈(im T )⊆, a dense subspace of Y. When
im T is closed, e.g. in ﬁnite dimensions, this is the case for all y ⇒Y.
If y∀∈⇒im T then we can only conclude that there is some sequence of vectors
xn ⇒X such that T xn ∗y∀, and so T ∀T xn ∗T ∀y. Thus ♦T xn −y♦converges to
♦y∀−y♦, but is never equal to it (by uniqueness of y∀).
⇐⊂
To continue this discussion, the above situation in the case of ﬁnite dimensions
is typical of an overdetermined system of equations, that is, a system T x = b that
represents more equations than there are unknowns. The least squares solution is
then found to be
x = (T ∀T )−1T ∀b
at least in the generic case when T is 1–1. Then T ∀T is also 1–1 since T ∀T x = 0 →
♦T x♦2 = ∞x, T ∀T x√= 0 →x = 0, so it is invertible at least on im T ∀.
The dual problem is that of an underdetermined system of equations, T x = b,
where there are less equations than unknowns. There is an oversupply of solutions,
namely any vector in x0 +kerT , where x0 is any single solution of the equation, and
ker(T ∀T ) = kerT ∈= 0.Inthiscase,aunique x thatisclosestto0 canbeselectedfrom
all these solutions, i.e., has the least norm. That is, we seek x ⇒(kerT )⊆= im T ∀(in
ﬁnite dimensions, every subspace is closed). Thus x = T ∀y and b = T x = T T ∀y,
so the required least norm vector is
x = T ∀(T T ∀)−1b.
In the general case, an operator need be neither 1-1 nor onto, so the set of vectors
which minimize ♦T x −y♦is a coset, x +kerT . But since kerT is a closed subspace,
it has a unique vector with smallest norm. The mapping from y to this x ⇒kerT ⊆
is then well-deﬁned for y ⇒im T + im T ⊆and is denoted by T †, called the Moore-
Penrose pseudo-inverse. To recap,

10.5 Inverse Problems
193
T † : im T + im T ⊆⇔Y ∗X,
y ∗x where T ∀T x = T ∀y, x ⇒kerT ⊆.
In the simple case when T is invertible, so im T = Y, it reduces to the usual inverse
T † = (T ∀T )−1T ∀= T −1. For example, every m × n matrix and vector has a
pseudo-inverse, e.g. x† = x∀/♦x♦2, so that x†x = 1 (except that 0† = 0).
T
T †b
x
b
y = T x
x + ker T
im T
0
X
Y
The equations introduced above have found an extremely fertile scope for appli-
cations. In many scientiﬁc or engineering contexts, an abundant number of measure-
ments of a few variables in general gives an overdetermined system of equations.
This also occurs when there is loss of information during measurement, so that the
‘space of measurements’ (im T ) is a proper subspace of the space of variables (H).
A small sample of applications is given below:
Regression
To ﬁnd the best-ﬁtting (least-squares) line y = mx + c to N given points
⎠xn
yn
⎫
⇒R2,
minimizing the errors in yn, we require that mxn + c be collectively as close to yn
as possible. In matrix form, we require

⎛⎜
x1 1
...
...
xN 1
⎝
⎞⎟
m
c

=

⎛⎜
y1
...
yN
⎝
⎞⎟,
written as Am = b. As this usually has no exact solution, the best alternative is
A∀Am = A∀b,
 x1 · · · xN
1 · · · 1


⎛⎜
x1 1
...
...
xN 1
⎝
⎞⎟
m
c

=
 x1 · · · xN
1 · · · 1


⎛⎜
y1
...
yN
⎝
⎞⎟,
⎬
n x2
n
⎬
n xn
⎬
n xn
⎬
n 1
 m
c

=
⎬
n xnyn
⎬
n yn.

Solving for m =
⎠mc
⎫
gives the usual regression line as used in statistics.
This technique is not at all restricted to ﬁtting straight lines. Suppose it is required
to approximate data points
⎠xn
yn
⎫
by a quadratic polynomial a + bx + cx2. This is the

194
10
Hilbert Spaces
same as trying to solve the matrix equation

⎛⎛⎛⎜
1 x1 x2
1
1 x2 x2
2
...
1 xN x2
N
⎝
⎞⎞⎞⎟

⎜
a
b
c
⎝
⎟=

⎛⎛⎛⎜
y1
y2
...
yN
⎝
⎞⎞⎞⎟.
Repeating the above procedure gives the solution

⎜
a
b
c
⎝
⎟= 1
δ

⎜
(S2
2−S1S3) ⎬
n x2
n yn+(S1S4−S2S3) ⎬
n xn yn+(S2
3−S2S4) ⎬
n yn
(S0S3−S1S2) ⎬
n x2n yn+(S2
2−S0S4) ⎬
n xn yn+(S1S4−S2S3) ⎬
n yn
(S2
1−S0S2) ⎬
n x2
n yn+(S0S3−S1S2) ⎬
n xn yn+(S2
2−S1S3) ⎬
n yn
⎝
⎟
where Sk = ⎬
n xk
n, and δ = S3
2 −2S1S2S3 + S2
1 S4 −S0S2S4 + S0S2
3. (Note: In
practice, one does not need to program these formulae; multiplying out T ∀T as a
numerical matrix and solving T ∀T x = T ∀b directly is usually a better option.)
Tikhonov Regularization
The Moore-Penrose pseudo-inverse is usually either not a continuous operator or
has a large condition number; its solutions tend to ﬂuctuate with slight changes in
the data (e.g. errors). To address this deﬁciency, a number of different regularization
techniques are employed whose aim is to improve the ill-conditioning. One of the
more popular techniques is attributed to Tikhonov; it balances out ﬁnding the best
approximate solution of T x = y with x having a small norm by seeking the minimum
of ♦T x −y♦2 + ψ♦x♦2, where ψ > 0 is some pre-determined parameter.
To solve this minimization problem, consider the following more general formu-
lation: Let H be a real Hilbert space and suppose A ⇒B(H), b ⇒H, and c ⇒R; to
ﬁnd the minimum of the quadratic function q : H ∗R,
q(x) := ∞x, Ax√+ ∞b, x√+ c.
Taking small variations of the minimum point x, namely x + tv, we deduce
∩t ⇒R, ∩v ⇒H,
q(x) ⩽q(x + tv) = ∞x + tv, Ax + t Av√+ ∞b, x + tv√+ c
∴0 ⩽t∞v, Ax + A∀x + b√+ t2∞v, Av√,
∴∩t > 0,
−t∞v, Av√⩽∞v, Ax + A∀x + b√⩽t∞v, Av√.
As t and v are arbitrary, it must be the case that x satisﬁes
(A + A∀)x + b = 0.

10.5 Inverse Problems
195
In particular, minimizing ♦T x −y♦2 = ∞x, T ∀T x√−2∞T ∀y, x√+♦y♦2 gives the
equation inferred previously, T ∀T x = T ∀y. Similarly, that x which minimizes
♦T x −y♦2 + ψ♦x♦2 = ∞x, (T ∀T + ψI)x√−2∞T ∀y, x√+ ♦y♦2
solves the equation
(T ∀T + ψI)x = T ∀y.
This is the regularized version of the last proposition. It will be proved later that
T ∀T + ψI is always invertible (regular) for ψ > 0 (Proposition15.42). This gives
an excellent alternative to the Moore-Penrose solution when y ∈⇒im T + (im T )⊆,
although choosing the parameter ψ may not be straightforward.
Algebraic Reconstruction Technique
ART isaniterativealgorithmthatgeneratesasolution x ofthe(real)equation Ax = b.
The matrix equation can be rewritten as ∞an, x√= bn, n = 1, . . . , N, where an are the
rows of A. The iteration is deﬁned in terms of afﬁne projections (Example 10.14(2))
xn = xn−1 + bn −∞an, xn−1√
♦an♦2
an,
x0 ⇒H.
The indices of an and bn are to be understood as modulo N (aN+1 = a1, etc). We
show below that starting from any x0 ⇒H, the iteration converges to the closest
point x∀to x0 that is a solution of Ax∀= b. Note that starting from x0 = 0 results
in the Moore-Penrose inverse.
To see why this works, let Mn := a⊆
n (cycling through n = 1, . . . , N), then
M := 
n Mn contains all the solutions of Av = 0; let also vn := xn −x∀. The
iteration becomes
vn = vn−1 −∞ˆan, vn−1√ˆan = Pnvn−1 ⇒Mn,
where ˆan = an/♦an♦, and Pn is the projection onto the hyperplane Mn. Notice that
v0 = x0 −x∀⇒M⊆, as well as vn −vn−1 ⇒M⊆, so the entire sequence vn lies in
M⊆.
Consider the operator Q := PN · · · P1 acting on M⊆; its norm is bounded by 1
because ♦Pi♦⩽1 for each i. If 1 = ♦Q♦= sup♦w♦=1 ♦Qw♦, then the supremum
is achieved by some unit vector w ⇒M⊆since the unit ball is compact in ﬁnite
dimensions and w ∗♦Qw♦is a continuous function. Denote wi := Piwi−1 =
wi−1 −∞ˆai, wi−1√ˆai, with w0 := w; then
1 = ♦Qw♦= ♦PNwN−1♦⩽♦wN−1♦⩽♦wN−2♦⩽· · · ⩽♦w1♦⩽♦w♦= 1

196
10
Hilbert Spaces
forces all wi to have norm 1. But, since ♦wi−1♦2 = ♦wi♦2 +|∞ˆai, wi−1√|2, it follows
that ∞ai, wi−1√= 0 and wi = wi−1 for i = 1, . . . , N. Hence w ⇒M1 ∪· · ·∪MN =
M, yet w ⇒M⊆is a unit vector.
This contradiction implies ♦Qv♦⩽c♦v♦, c < 1, for any v ⇒M⊆. Hence
♦vn+N♦= ♦Qvn♦⩽c♦vn♦; combined with ♦vn+1♦⩽♦vn♦, we get vn ∗0.
Equivalently, xn converges to x∀.
The advantages of ART are that it uses less computer memory and is ﬂexible in
that it can be used even if there is missing data or newly available data (missing or
new rows of A); but, being an iterative procedure, it is generally slower to converge.
Wiener Deconvolution
When a signal f ⇒L2(R) passes through a ‘circuit’ (which could be the atmosphere,
say, or a measuring apparatus), it is modiﬁed in two ways: (i) the signal is distorted
slightly to K f := k ∀f , where k ⇒L1(R) is characteristic of the circuit (recall
convolution Example 8.6(5))), (ii) random noise in the process adds a little error
ϵ ⇒L2(R) to the signal. The net effect is a distorted output signal y = k ∀f + ϵ. Is
it possible to extract the original signal f back again from y? A full reconstruction
by solving K f = y is impossible as lost information cannot be regained; the im K
subspace is not the full space L2(R), and the error displaces the signal off this
subspace. But one can use Tikhonov regularization and solve (K ∀K + ψ) f = K ∀y.
The simplest way to do this is to use the properties of the Fourier transform, which
converts convolution to multiplication. As in Example 10.24(3), the adjoint of K is
given by K ∀g = k−∀g where k−(t) := k(−t), since
∞K ∀g, f √= ∞g, K f √=
⎧⎧
g(s)k(s−t) f (t) dt ds =
⎧⎧
k(s −t)g(s) ds f (t) dt.
The Fourier transform of k−is

k−(ξ) =
⎧
e−2δiξtk(−t) dt =
⎧
e2δiξtk(t) dt = k(ξ),
so that (K ∀K + ψ) f = K ∀y transforms to
f =
k y
|k|2 + ψ.
This is a recipe for ﬁnding f from y, called deconvolution, that is commonly imple-
mented as a computer program using the Fast Fourier Transform, or directly as an
electrical ﬁlter circuit.

10.5 Inverse Problems
197
Fig. 10.3 Image reconstruction. (i) The original image, (ii) after it passes an imaging device (exag-
gerated), (iii) the best-ﬁt image
Image Reconstruction
An image can also be considered as a ‘signal’, this time in L2(R2), or, when dis-
cretized, as a vector of numbers in the form of an array of pixels. Each number
represents the brightness of a pixel (neglecting the color content for simplicity). An
imaging apparatus transforms the original image x to y = Ax + e, where A is
assumed to be a linear operator, as above; examples include a slight spherical aber-
ration or blurring in general. Since such modiﬁcation incurs a loss of information,
the distortion matrix A is not invertible, but the best-ﬁt “regularized” solution of
x = (A∀A + ψI)−1A∀y restores the image somewhat, as seen in Fig. 10.3.
In practice, implementing the reconstruction encounters difﬁculties that are spe-
ciﬁc to images. Images are typically in the order of about a million pixels in size;
the matrix A would therefore consist of about a trillion coefﬁcients (most of which
are zero), and ﬁnding the inverse of A∀A + ψI is prohibitively time-consuming.
Fortunately, blurring is to a good approximation usually independent of the pixel
positions; for example, a linear motion blur produces the same streaks everywhere
across the picture (but note that this is not true for a rotation blur). In mathemati-
cal terms, the transformation A can be taken to be translation invariant, so that it is
equivalent to the convolution by some vector k ⇒H. With this simpliﬁcation, image
reconstruction becomes a 2-dimensional version of Wiener deconvolution; the same
technique using the Fourier transform can be applied,
x = F−1
ky
|k|2 + ψ
.
Here, y represents the discrete version of the Fourier transform, namely ym =
⎬
n e−2δimnyn. The resulting x may have negative coefﬁcients; these are mean-
ingless and usually replaced by 0.
Tomography
Suppose that instead of a vector x, one is given ‘views’ of it, yn := ∞an, x√, where
an is a list of known vectors: Is it possible to reconstruct x from these views? If

198
10
Hilbert Spaces
Fig. 10.4 Computed tomography. (i) The original image (360×360 pixels), (ii) 80 parallel ‘views’
of the object, (iii) the best-ﬁt reconstruction from 6,400 views (80 directions)
an are assembled as rows of a matrix A, one obtains a matrix equation Ax = y.
In such problems, it may be the case that the number of views is less than the
dimension of the vector space, so that the system is under-determined, or that there
are a large number of views, making the equation over-determined. In either case, a
least-squares solution can be found as above, using the techniques of inverse problem
solving (Fig.10.4).
CT scans: An x-ray passing through a 3-D object of density f diminishes in
intensity by an amount e
⎭
f (a+bt) dt where a + bt is the straight line followed by the
ray. The emitted and received intensity can be measured and, after taking logs, one
obtains a ‘view’ of the object
y =
⎧
f (a + bt) dt = ∞La,b, f √,
where La,b is the characteristic function of the ray, i.e., a function that is 1 along the
ray and 0 outside it (in practice, the ray has a ﬁnite width). It should be possible to
reconstruct f from a large number of these views. A CT-scan does precisely this: an
x-ray source coupled with a detector rotate around the object to produce these views.
In one simple conﬁguration, b =
cos α
sin α

and a = s
−sin α
cos α

; the collection
of these views, as a function of α and s, is called the Radon transform R of f . The
best-ﬁt f that reproduces the data is computed by solving (R∀R+ψ) f = R∀y, either
directly in the form of the optimized Filtered Back Projection (FBP) algorithm or
by iterative algorithms such as some variants of ART. Other conﬁgurations include
a ﬁxed source and a rotating detector, producing a fan-shaped collection of rays.
In yet other applications, the ‘rays’ move along curved lines; more generally, the
output may depend non-linearly on f and the source (see [21] for an overview of
tomography and inverse scattering theory).
The idea obviously has lots of potential: x-ray tomography has revolutionized
medical diagnosis, archaeology, and fossil analysis; crystal x-ray diffraction
tomography recreates the atomic conﬁguration of molecules in a lattice; impedance
tomography takes output currents from input voltages to reconstruct the interior

10.5 Inverse Problems
199
resistance density of an object; seismographs measure the output vibrations after the
occurrence of earthquakes to reconstruct the interior density of the Earth; gravity,
magnetic, or sound measurements at the Earth’s surface can determine rock densi-
ties underneath, aiding in the exploration for oil or minerals; ultrasound echoes or
scattered light can be used to reconstruct 3-D images of internal organs (or of moths
and ﬁsh/squid by bats and dolphins). The list is long and increasing!
Exercises 10.26
1. If T is invertible then (T −1)∀= (T ∀)−1.
2. Use ♦T ∀T ♦= ♦T ♦2 to show ♦T ∀♦= ♦T ♦.
3. ▶The adjoint of the multiplier operator in ∂2, x ∗ax, is y ∗¯a y.
4. Let a ⇒∂1(Z), then Young’s inequality (Exercise 9.15(5)) shows that the linear
map x ∗a ∀x is continuous on ∂2(Z). Its adjoint is y ∗a† ∀y where
(an)† := (¯a−n).
5. The Volterra operator on L2[0, 1], V f (x) :=
⎭x
0 f , has adjoint V ∀f (x) =
⎭1
x f .
6. Let ∞∞x, y√√:= ∞x, Ay√be a new inner product, then the adjoint of T with respect
to it is T α := (AT A−1)∀.
7. If R ⇒B(X, Y) then T ∗RT R∀is an operator B(X) ∗B(Y).
8. For any T ⇒B(H1, H2), ker(T ∀T ) = kerT and im T ∀T = im T ∀.
9. A linear map T : X ∗Y is said to be conformal when it preserves orthogonality,
∩x, ˜x ⇒X,
∞x, ˜x√= 0 →∞T x, T ˜x√= 0.
Show that this is the case if, and only if, T ∀T = φI for some φ ⩾0. Moreover,
angles between vectors are preserved (for φ > 0).
In particular, two inner products on the same vector space are conformal when
∞∞x, y√√= φ∞x, y√for some φ > 0.
10. * Show that a map between Hilbert spaces which preserves the inner product
must be linear. Deduce that isometries on a real Hilbert space must be of the
type f (x) = Ux + a where U ∀U = I and a ⇒H.
(Hint: Let g(x) :=
f (x) −f (0), an isometry; show ∞g(x + y), g(z)√=
∞g(x) + g(y), g(z)√, so g(x + y) −g(x) −g(y) ⇒[[ im g]] ∪(im g)⊆.)
11. Find best approximate solutions for
(i)

⎜
1 4 7
2 5 8
3 6 9
⎝
⎟x =

⎜
4
−1
0
⎝
⎟,
(ii)
 1 4 7
2 5 8

x =
 4
−1

.
12. To ﬁnd the best-ﬁtting plane z = ax +by +c to a number of points (xn, yn, zn),
where zn is the dependent variable, least squares approximation gives

200
10
Hilbert Spaces

⎜
⎬
n 1
⎬
n xn
⎬
n yn
⎬
n xn
⎬
n x2
n
⎬
n xnyn
⎬
n yn
⎬
n xnyn
⎬
n y2
n
⎝
⎟

⎜
c
a
b
⎝
⎟=

⎜
⎬
n zn
⎬
n xnzn
⎬
n ynzn
⎝
⎟.
13. * The method is not at all restricted to linear geometric objects. Find the best-
ﬁtting circle x2 + y2 + ax + by = c to a number of points (xn, yn).
14. The pseudo-inverse of the left-shift operator on ∂2 is the right-shift operator, and
vice versa.
15. For any T ⇒B(X, Y), T T †T = T , because both x and T †T x belong to x+kerT .
So T †T and T T † are projections; which precisely?
16. The transformation T † : im T ≈im T ⊆∗kerT ⊆is linear but continuous only
when im T is closed (Hint: if T xn ∗y then T xn = T T †T xn ∗T T †y).
17. Recall the Volterra 1–1 operator V f (x) :=
⎭x
0 f on L2[0, 1]. If g is differ-
entiable, then V †g = g≡, and the Tikhonov regularization solves the equation
f −ψ f ≡≡= g≡.
18. An oscillating pendulum is captured on video at 25 frames/s. The angle α (in
rad) that the pendulum makes with the vertical, for 1 s worth of frames (1–26),
is given in the table below. Theoretically, α satisﬁes
¨α + κr
m
˙α2 + g
r sin α = 0,
where g = 9.81ms−2 and κ/m, and r are unknown numbers. From the data,
estimate ˙αn by (αn+1 −αn−1)/2δt, and ¨αn by (αn+1 −2αn + αn−1)/δt, thereby
getting equations of the type axn + byn = zn, where xn = ˙α2
n, yn = sin αn,
zn = −¨αn, and a, b are unknown constants. Use regression to ﬁnd a, b (hence r
and κ/m) that best ﬁt these data.
1
2
3
4
5
6
7
8
9
0.372
0.210
0.043
-0.126
-0.291
-0.447
-0.589
-0.714
-0.816
10
11
12
13
14
15
16
17
18
-0.900
-0.957
-0.988
-0.993
-0.972
-0.923
-0.854
-0.756
-0.640
19
20
21
22
23
24
25
26
-0.505
-0.353
-0.192
-0.025
0.144
0.308
0.462
0.600
19. Phylogeny: Bioinformaticians can create a score of how far apart two species
are genetically. An example is given in the adjoining table, together with the
suspected evolutionary tree. Assign constants to each edge in the tree which best
match the given scores, i.e., the sum of the edge constants along the path from,
say, A to D should be as close to 6.16 as possible.

10.5 Inverse Problems
201
B
2.22
-
C
6.12
5.60
-
D
6.16
5.70
1.70
-
E
5.79
5.06
3.12
3.72
A
B
C
D
A
B
C
D
E
10.6 Orthonormal Bases
Deﬁnition 10.27
An orthonormal basis of a Hilbert space H is a set of orthonormal vectors E
whose span is dense,
∩ei, e j ⇒E, ∞ei, e j√= δi j,
[[E]] = H.
The second condition is equivalent to E⊆= 0 (Example 10.14(4)), i.e.,
∩e ⇒E, ∞e, x√= 0 →x = 0.
Examples 10.28
1. The sequences en := (δni) = (0, . . . , 0, 1, 0, . . .) form an orthonormal basis for
∂2.
Proof Orthonormality is obvious,
∞en, em√∂2 = ∞(0, . . . , 0
  n
, 1, 0, . . .), (0, . . . , 0
  m
, 1, 0, . . .)√= δnm.
If the sequence x = (a0, a1, . . .) is in [[e0, e1, . . . ]]⊆, then an = ∞en, x√∂2 = 0
for any n; hence x = 0.
2. Gram-Schmidt orthogonalization: Any countable number of vectors { vn } can
be replaced by a set of orthonormal vectors having the same span, using the
Gram-Schmidt algorithm:
u0 := v0,
e0 := u0/♦u0♦
un := vn −⎬n−1
i=0 ∞ei, vn√ei, en := un/♦un♦.
It may very well happen that un = 0, in which case it and vn are discarded
and vn+1 relabeled as vn. Clearly, [[e1, . . . , en]] = [[v1, . . . , vn]], not taking the
discarded vn into account. Hence [[e0, e1, . . . ]] = [[v0, v1, . . . ]].

202
10
Hilbert Spaces
3. Suppose x = ⎬
m ψmem for an orthonormal basis { e0, e1, e2, . . . }; then taking
the inner product with en gives the simple formula ψn = ∞en, x√. The next section
discusses whether every x can be so written.
4. The set of basis vectors need not be countable; when uncountable, the Hilbert
space is not separable, because the vectors en are equally distant from each
other ♦en −em♦=
∃
2, so that the balls Bϵ(en) are disjoint for ϵ <
∃
2/2
(Exercise 4.21(4)). Conversely, if E := { en } is a countable orthonormal basis,
then [[E]], and H = [[E]], are separable.
5. * Every Hilbert space has an orthonormal basis.
Proof Consider the collection of all orthonormal sets of vectors. It is nonempty,
so Hausdorff’s maximality principle implies that there is a maximal chain of
orthonormal sets Eψ. But E := 
ψ Eψ is also an orthonormal set, for pick any
two distinct vectors eψ ⇒Eψ and eβ ⇒Eβ ⇔Eψ, say, then eψ ⊆eβ. So E is
a maximal set of orthonormal vectors. E⊆= 0 otherwise E can be extended
further, so [[E]] = H.
Fourier Expansion
The utility of orthonormal bases lies in the ease of calculation of the inner product:
Proposition 10.29
Parseval’s identity
If x = ⎬
n ψnen and y = ⎬
n βnen, where { en } are orthonormal, then
∞x, y√=
⎪
n
ψnβn =
⎪
n
∞x, en√∞en, y√.
In particular, ♦x♦=
⎠⎪
n
|ψn|2⎫1/2.
Proof A simple expansion of the two series in the inner product, making essential
use of the linearity and continuity of ∞, √as well as orthonormality, gives the result:
∞x, y√=
⎪
n
⎪
m
ψnβm∞en, em√=
⎪
n
ψnβn.
⇐⊂
Parseval’s identity is the generalization of Pythagoras’ theorem to inﬁnite dimen-
sions. The question remains: when can a vector be written as a series of orthonormal
vectors? The next proposition and theorem give an answer.

10.6 Orthonormal Bases
203
Proposition 10.30
Let { e1, e2, . . . } be a countable orthonormal set of vectors in a Hilbert
space H, then
∞
⎪
n=1
ψnen converges in H →(ψn) ⇒∂2.
Proof By Pythagoras’ theorem we have
♦ψnen + · · · + ψmem♦2 = |ψn|2 + · · · + |ψm|2.
This shows that ⎬N
n=1 ψnen is a Cauchy sequence in H if and only if ⎬N
n=1 |ψn|2 is
Cauchy in C (Example 7.20(1)). Since H and ∂2 are complete, ⎬
n ψnen converges
if, and only if, (ψn) is in ∂2.
⇐⊂
The convergence of ⎬
n ψnen need not be absolute in inﬁnite dimensions; for
the latter to be true requires that ⎬
n ♦ψnen♦= ⎬
n |ψn| converges, that is, (ψn) ⇒
∂1 ⊂∂2. Nevertheless, a rearrangement σ of an orthonormal basis does not affect
the expansion, ⎬
n ψnen = ⎬
n ψσ(n)eσ(n), because eσ(n) remain orthonormal and
(ψσ(n)) ⇒∂2.
Theorem 10.31 Bessel’s inequality
If { e1, e2, . . . } are orthonormal in an inner product space, then
⎪
n
|∞en, x√|2 ⩽♦x♦2.
When { en } is an orthonormal basis of a Hilbert space,
x =
⎪
n
∞en, x√en.
Proof (i) Fix x and let xN := ⎬N
n=1 ∞en, x√en. Writing ψn := ∞en, x√, we have
0 ⩽♦x −xN♦2 = ♦x♦2 −∞xN, x√−∞x, xN√+ ∞xN, xN√
= ♦x♦2 −2
N
⎪
n=1
ψnψn +
N
⎪
n,m=1
ψnψm∞en, em√

204
10
Hilbert Spaces
= ♦x♦2 −
N
⎪
n=1
|ψn|2,
hence
N
⎪
n=1
|∞en, x√|2 ⩽♦x♦2.
(10.3)
As a bounded increasing series, the left-hand side must converge as N ∗∞, and
Bessel’s inequality holds.
As a matter of fact, even if { ei } is an uncountable orthonormal set of vectors,
the same analysis can be made for any ﬁnite subset of them. Inequality (10.3) then
shows that there can be at most N −1 vectors ei with |∞ei, x√|2 > ♦x♦2/N, for
any positive integer N, and so only a countable number of terms with ∞ei, x√∈= 0.
Therefore ⎬
i |∞ei, x√|2 is in fact a countable sum, bounded above by ♦x♦2.
(ii) By the previous proposition, the series ⎬
n ∞en, x√en converges in a Hilbert space,
say to y ⇒H. But x −y ⇒{ e1, e2, . . . }⊆= 0, since for all N ⇒N,
∞eN, x −y√= ∞eN, x√−
∞
⎪
n=1
∞en, x√∞eN, en√= 0.
An orthonormal basis is thus a Schauder basis.
⇐⊂
Proposition 10.32
Every N-dimensional Hilbert space is unitarily isomorphic to RN or CN.
Every separable inﬁnite-dimensional Hilbert space is unitarily isomorphic
to ∂2 (real or complex).
Proof Suppose H is a separable Hilbert space, with some dense countable subset
A = { a1, a2, . . . }. The Gram-Schmidt process converts this to a list of orthonormal
vectors E = { e1, e2, . . . }, which is then a countable orthonormal basis of H since
[[E]] = [[A]] ⊇¯A = H.
Consider the map
J : H ∗∂2
x ∗(ψn),
ψn := ∞en, x√
Bessel’s inequality shows that (ψn) is indeed in ∂2 (if H is a real Hilbert space, ψn
are also real). Linearity of J follows from that of the inner product. Preservation
of the inner products and norms, ∞x, y√H = ∞Jx, Jy√∂2, is precisely the content of
Parseval’s identity.
J is onto: for any (ψn) ⇒∂2, the series ⎬
n ψnen converges to some vector x by
Proposition 10.30, and this is mapped by J to

10.6 Orthonormal Bases
205
Joseph Fourier (1768–1830) A Napoleonic supporter, almost
guillotined in the aftermath of the French revolution, he suc-
ceeded his teacher Lagrange in 1797.
Besides being a gov-
ernment oﬃcial and an accomplished Egyptologist, his math-
ematical work culminated in his 1822 book on Fourier series:
“sines and cosines as the atoms of all functions”; it revolution-
ized how diﬀerential equations were solved. But Lagrange had
pointed out that the expansion might not be unique, or even
exist. Which functions have a Fourier series? This question led
to reﬁned treatments of integration such as Riemann’s, and to
Cantor’s set theory; but also to studies into what convergence
of functions is all about, when it is not pointwise.
Fig. 10.5 Fourier
Jx = (∞en, ⎬
m ψmem√) = (ψn).
The Hilbert space is N-dimensional precisely when E has N vectors; in this case
it is a classical basis of H. J remains a surjective isometry, with RN or CN replacing
∂2.
⇐⊂
Examples of Orthonormal Bases
Orthonormal bases are widely used to approximate functions, and are indispensable
for actual calculations. There are various orthonormal bases commonly used for the
space of L2 functions on different domains. Each basis has particular properties that
are useful in speciﬁc contexts. One should treat these in the same way that one treats
bases in ﬁnite-dimensional vector spaces — a suitable choice of basis may make a
problem amenable. For example, for a problem that has spherical symmetry, it would
probably make sense to use an orthonormal basis adapted to spherical symmetry.
Consider the simplest domain, the real line. There are three different classes of
non-empty closed intervals (up to a homeomorphism): [a, b], [a, ∞[, and R. Various
orthonormal bases have been devised for each, with the most popular being listed
here.
L2[a, b]—Fourier series
Proposition 10.33
The functions e2δinx, n ⇒Z, form an orthonormal basis for L2[0, 1].
Proof Orthonormality of the functions is trivial to establish,
∞e2δinx, e2δimx√=
⎧1
0
e2δix(m−n) dx = δnm.

206
10
Hilbert Spaces
Suppose f ⇒{ e2δinx }⊆i.e.,
⎭1
0 e−2δinx f (x) dx = 0 for all n ⇒Z. Recall that the
Fourier coefﬁcients give a 1-1 operator F : L1[0, 1] ∗c0(Z) (Theorem9.25) (note:
L2[0, 1] ⊂L1[0, 1]), so F f = 0 implies f = 0 and hence { e2δinx : n ⇒Z }⊆= 0.
⇐⊂
Of course, there is nothing special about the interval [0, 1]. Any other interval
[a, b] has a modiﬁed Fourier basis. For example, {
1
∃
2δeinx : n ⇒Z } is an orthonor-
mal basis for L2[−δ, δ].
Examples 10.34
1. ▶The Fourier expansion becomes, for f ⇒L2[0, 1],
f (x) =
∞
⎪
n=−∞
ψne2δinx
where ψn = ∞e2δinx, f √=
⎭1
0 e−2δinx f (x) dx are the Fourier coefﬁcients of f ,
and the convergence is in L2[0, 1] not necessarily pointwise. (However, a difﬁ-
cult proof [39] shows that there is pointwise convergence a.e.; see also Exam-
ple 11.29(5))
2. The classical Parseval identity is
⎧δ
−δ
| f (x)|2 dx =
∞
⎪
n=−∞
|an|2 + |bn|2,
wherean−ibn =
1
∃
2δ
⎭δ
−δ e−inx f (x) dx are the L2[−δ, δ]-Fourier coefﬁcients.
3. Fourier series have a wide range of applications, especially in signal processing.
For example, the operator F∀1[−N,N]F is called a low(frequency)-pass ﬁlter:
Given a signal f , 1[−N,N] discards the higher-frequency terms from the Fourier
coefﬁcients F f ; F∀then builds a function from the remaining coefﬁcients,
resulting in a smoothed out low frequency band signal (for example, without a
high frequency hiss).
L2[−1, 1]—Legendre polynomials
We’ve seen that the set of polynomials is dense in the space L2[a, b] (Proposi-
tion 9.20) but the simplest basis, namely 1, x, x2, . . ., is not orthogonal, as can be
easily veriﬁed by calculating, say, ∞1, x2√= (b3 −a3)/3. This can be rectiﬁed by
applying the Gram-Schmidt algorithm. On the interval [−1, 1], the resulting poly-
nomials are called the (normalized) Legendre polynomials (Fig.10.6). The ﬁrst few
are
1
∃
2
,

3
2 x,
3
2

5
2

x2 −1
3

, . . .

10.6 Orthonormal Bases
207
Legendre polynomials
Laguerre functions
Hermite functions
Fig. 10.6 Orthonormal bases (The ﬁrst ten functions of each basis are plotted as rows in each
image; brightness is proportional to the value of the function, mid-grey being 0)
with the general formula being
pn(x) =

n + 1
2
2nn!
 d
dx
n
(x2 −1)n.
These polynomials satisfy the differential equation
Lpn = −n(n + 1)pn,
where L = D(1 −x2)D = (1 −x2)D2 −2x D.
L2[0, ∞[—Laguerre functions
This Hilbert space does not contain any polynomials xn, but their modiﬁed versions
xne−x/2 do belong. A Gram-Schmidt orthonormalization of them gives the Laguerre
functions, the ﬁrst few terms of which are
e−x/2,
(1 −x)e−x/2,
(1 −2x + 1
2 x2)e−x/2, . . .
and the general formula is
ln(x) = 1
n!ex/2Dn(xne−x).
The Laguerre functions satisfy (prove!)
Sln = −(n + 1
2)ln,
where S := Dx D −x/4.
The Laguerre polynomials (the polynomial part of ln) can also be thought of as an
orthonormal basis for L2
w(R+) with the weight e−x.
L2(R)—Hermite functions

208
10
Hilbert Spaces
Here, orthonormalization is performed on the functions xne−x2/2 (equivalently, take
xn in L2
w(R) with the weight e−x2) to get the Hermite functions,
1
δ1/4 e−x2/2,
2
∃
2δ1/4 xe−x2/2,
1
∃
2δ1/4 (2x2 −1)e−x2/2, . . . .
hn(x) =
(−1)n
∃
2nn!δ1/4 ex2/2Dne−x2.
To prove orthogonality, ﬁrst show that D(ex2 Dne−x2) = −2nex2 Dn−1e−x2, and
deduce that ∞hn, hm√= 2n∞hn−1, hm−1√. The Hermite functions satisfy
Rhn = −(2n + 1)hn,
where R := D2 −x2.
Other Domains
Some other useful orthogonal bases on L2(A) spaces are, in brief:
Circle L2(S1): Since the circle S1 is essentially the interval [0, 2δ] as far as
L2-functions are concerned, the periodic Fourier functions einα form an orthogonal
basis for it.
The Chebyshev polynomials, Tn(cos α) := cos nα, are the projection of the cos nα
part of this Fourier basis, from the unit semi-circle to the x-axis [−1, 1]. They are thus
orthogonal on L2
w[−1, 1] with the weight 1/
∃
1 −x2 (since dα = −dx/
∃
1 −x2).
There are many other orthonormal bases adapted to L2
w[a, b]. Rodrigues’ formula
describes orthogonal functions on L2
w[a, b],
fn(x) := w(x)−1Dn(w(x)p(x)n)
for a quadratic polynomial p with roots at the endpoints a, b, and weight function
w: the Legendre, Laguerre, Hermite, and Chebyshev functions are all of this type.
PlaneL2(R2): An orthonormal basis for the plane can be obtained by multiplying
Hermite functions hn(x)hm(y). In general, if en(x) and ˜en(y) are orthonormal bases
for L2(A) and L2( ˜A), then en(x)˜em(y) form an orthonormal basis of L2(A × ˜A).
Disk L2(B1(0)) Bessel functions: The functions on the unit disk taking the value
zero at the boundary have an orthogonal basis Jn(φm,nr)einα, where φm,n are the
zeros of the Bessel function Jn(x) := ⎬∞
m=0
(−1)m
m!(n+m)!(x/2)2m+n (Fig.10.7).
Sphere L2(S2) Spherical Harmonics:
Y l
m(α, φ) :=

(2l + 1)(l −m)!
4δ(l + m)!
Pl
m(cos α)eimφ,
where Pl
m(x) = (−1)m(1−x2)m/2Dm Pl(x) are the “associated Legendre functions”.
They depend on two indices, l ⇒N and m = −l, . . . , +l.

10.6 Orthonormal Bases
209
Fig. 10.7 Bessel’s functions,
Jn(φm,nr) cos(nα), n, m =
0, 1, 2
Exercises 10.35
1. Orthonormal vectors must be linearly independent.
2. In ﬁnite dimensions, orthonormal bases span the vector space, [[e1, . . . , eN]] =
H (Theorem8.22).
In inﬁnite dimensions, an orthonormal basis is not a basis in the linear algebra
sense (Hamel basis), which requires the stronger spanning condition [[E]] = H.
3. Comparing coefﬁcients: if ⎬
n ψnen = ⎬
n βnen, then ψn = βn.
4. If { en } and { ˜em } are orthonormal bases for Hilbert spaces X and Y respec-
tively, then { (en, 0) } ∪{ (0, ˜em) } form an orthonormal basis for X × Y (Exer-
cise 10.10(6)).
5. Let E := { e1, e2, . . . } be a set of orthonormal vectors, with [[E]] = M ⊂H.
For any x ⇒H, the sum ⎬
n ∞en, x√en gives the closest point x∀in M to x.
6. ▶An operator U ⇒B(H1, H2) is a unitary isomorphism if, and only if, it maps
orthonormal bases to orthonormal bases.
7. * It is quite possible for x = ⎬
n ∞en, x√en to hold true for all x in a Hilbert
space, without en being orthonormal. Find three such vectors e1, e2, e3, in R2.
But if Parseval’s identity ♦x♦2 = ⎬
n |∞en, x√|2 holds for all x ⇒H, and ♦en♦=
1 for all n, then the vectors en form an orthonormal basis.
8. Expand the function x on [0, 1] as a Fourier series.
(a) Assuming pointwise convergence, deduce Gregory’s formula

210
10
Hilbert Spaces
1 −1
3 + 1
5 −1
7 + · · · = δ
4 .
(b) Use Parseval’s identity to deduce Euler’s formula
1 + 1
22 + 1
32 + · · · = δ2
6 .
9. When f ⇒L2[0, 1] is an even function about 1
2, meaning f ( 1
2 +x) = f ( 1
2 −x),
then ψ−n = ψn and
∞
⎪
n=−∞
ψne2δinx = ψ0 +
∞
⎪
n=1
2ψn cos(2δnx).
What if f is odd, or neither odd nor even?
10. Show that cos nδx, n = 0, 1, . . ., is an orthogonal basis for the real space
L2[0, 1].
11. Show that U f (x) :=
1
∃b−a f ( x−a
b−a ) is a unitary operator L2[0, 1] ∗L2[a, b].
Hence ﬁnd an orthonormal basis for L2[a, b].
12. ▶The Fourier operator F : L2[0, 1] ∗∂2 is a unitary isomorphism between
Hilbert spaces. Its adjoint is F∀(an) =
∞
⎪
n=−∞
ane2δinx.
13. Prove that the Legendre polynomials are orthonormal in L2[−1, 1], as follows:
Deﬁne un(x) := (x2 −1)n, and qn := Dnun; show by induction that
(a) Dkun(±1) = 0, for k < n,
(b) ∞Dnun, Dmum√= −∞Dn−1un, Dm+1um√,
(c) ∞qn, qm√= 0 unless n = m.
14. * The Legendre polynomials Pn := pn/

n + 1
2 have the property,
1/♦u −y♦=
∞
⎪
n=0
rn Pn(cos α)
where u is a unit vector, r := ♦y♦< 1, and α is the angle between u and y.
(Hint: Show fr(x) := 1/
∃
1 + r2 −2rx satisﬁes T fr = r ∂2
∂r2 (r fr), then write
fr(x) = ⎬
n ψn(r)pn(x).)
15. ▶A frame is a sequence of vectors en ⇒H (not necessarily linearly independent)
for which the mapping J : x ∗(∞en, x√)n⇒N is an embedding H ∗M ⇔∂2.
By Proposition 7.12, this is equivalent to there being positive constants a, b > 0,
a♦x♦H ⩽♦Jx♦∂2 ⩽b♦x♦H, i.e.,

10.6 Orthonormal Bases
211
◦c > 0,
1
c ♦x♦2 ⩽
⎪
n
|∞en, x√|2 ⩽c♦x♦2.
Let δk(an) := ak and L := (J ∀)−1; then x ∗δk Lx is a continuous functional,
hence there is a unique vector ˜ek such that δk Lx = ∞˜ek, x√.
(a) The two sets of vectors en and ˜en are bi-orthogonal, that is, ∞˜em, en√= δmn.
(b) J ∀L = I = L∀J, so
x =
⎪
n
∞˜en, x√en =
⎪
n
∞en, x√˜en.
Applications
Frequency-Time Orthonormal Bases
An improvement on the classical orthonormal bases for functions t ∗f (t) in
L2(R) are bases that give information in both ‘frequency’ and ‘time’. In contrast, the
Fourier coefﬁcients, for example, only give information about the frequency content
of the function. A large nth Fourier coefﬁcient means that there is a substantial
amount of the term e2δint, somewhere in the function f (t) without indicating at all
where. The aim of frequency-time bases is to have coefﬁcients am,n that depend on
two parameters n and m, one of which is a frequency index, the other a “time” index.
The am,n coefﬁcients, much like musical notes placed on a score, indicate how much
of the frequency corresponding to n, is “played” at the time corresponding to m;
they are able to track the change of frequency content of f with time. Of course, the
reference to t as time is not of relevance here; t can represent any other varying real
quantity.
Windowed Fourier Bases (Short Time Fourier Transform): A basic way to achieve
this is to deﬁne the basis functions by
hm,n(t) := e2δinth(t −m),
where h is a carefully chosen (real) window function, with ♦h♦L2 = 1, such that
hm,n are orthonormal. The simplest choice of window function is h = 1[−1
2 , 1
2 ]; other
popular possibilities, such as the Hann window cos2(δt) (−1
2 ⩽t ⩽1
2) and the
Gaussian cσe−t2/2σ2 do not give orthonormal bases but are useful nonetheless.
One can then obtain a picture of f spread out in time and frequency, called a
spectrogram (Fig.10.8), by plotting the coefﬁcients |∞hm,n, f √|2 (often letting m and
n vary continuously in R and R+ respectively to get a smooth picture).
Note that the coefﬁcients ∞hm,n, f √are really just (am,n) = F(h(t −m) f (t)).
So summing the coefﬁcients in n, keeping the position m ﬁxed, gives the windowed
function:

212
10
Hilbert Spaces
frequency f
time t
Fig. 10.8 Spectrogram of a piano piece, showing clearly the duration, frequency, and harmonics
of each note
⎪
n
am,ne2δint = h(t −m) f (t)
and similarly, when ⎬
m h(t −m) = 1,
f (n) =
⎧
e−2δint ⎪
m
h(t −m) f (t) dt =
⎪
m
am,n
The greatest disadvantage of these bases is that the window ‘width’ is predetermined;
it ought to be large enough to contain the low frequency oscillations, but then the
time localization of the high frequencies is lost. The aim of the windowed Fourier
basis is only achieved over a limited range of frequencies. To circumvent this, one
can make the window width decrease with the frequency parameter n — this is the
idea of wavelets.
WaveletBases:Thebasisinthiscaseconsistsofthefollowingfunctionsin L2[0, 1]
ψm,n(t) := TmS2nψ(t) = 2n/2ψ(2nt −m), (m, n ⇒Z)
where ψ and φ are carefully chosen ‘mother’ and ‘father’ functions in L2(R). The
function ψ serves both as a window (ideally with compact support) and an oscil-
lation. The basis functions ψmn are then scaled and translated versions of ψ. They
have the advantage that the resolution in ‘time’ is better for higher frequencies than
the windowed Fourier bases, and so require less coefﬁcients to represent a func-
tion to the same level of detail. One example is the classical Haar basis, generated
by ψ(t) := 1[0,1] −1[1,2] (prove orthogonality of ψm,n). Other wavelets, gener-
ated by continuous functions, are more popular, e.g. Mexican-hat ((1 −t2)e−t2/2),
Gabor/Morlet (e2δi f te−t2/2, usually f = 1; Fig.10.9). The analogue of the spec-
trogram is the scalogram, which is a plot of the coefﬁcients W f (a, b) := ∞ψa,b, f √
where ψa,b(t) =
1
∃a ψ( t−b
a ).
In a multi-resolution wavelet scheme, a subspace Vk of the Hilbert space L2(R)
is split recursively into low and high resolution parts as Vn+1 = Vn ≈Wn, where
Wn = V ⊆
n ∪Vn+1

10.6 Orthonormal Bases
213
Fig. 10.9 Three wavelets: Haar, Mexican hat (with a translated and scaled version), and Morlet
(real and imaginary parts)
Vk = Vk−1 ≈Wk−1 = · · · = V0 ≈W0 ≈W1 ≈· · · ≈Wk−1
If we suppose Vn and Wn to be spanned by orthonormal bases { φm,n : m =
0, . . . , N −1 } and { ψm,n : m = 1, . . . , N −1 }, that are generated by scaling
and translation from a “father” and “mother” wavelets φ and ψ respectively, then,
by recursion, one need only ensure V1 = V0 ≈W0 = [[φ]] ≈[[ψ]] for this scheme to
work. Therefore the requirements are that φ, ψ ⇒V1 be orthonormal. For N even,
the following “reﬁnement equations” are sufﬁcient,
φ(x) = a0φ(2x) + a1φ(2x −1) + · · · + aN−1φ(2x −N + 1),
ψ(x) = aN−1φ(2x) −aN−2φ(2x −1) + · · · −a0φ(2x −N + 1)
a2
0 + · · · + a2
N−1 = 2.
Recall here that φ(2x −m) = 2−1/2φm,1(x) has norm 1/
∃
2, so ♦φ♦2 = ⎬
m a2
m/2.
For example, the Haar basis satisﬁes φ(t) = φ(2t) + φ(2t −1), ψ(t) = φ(2t) −
φ(2t −1). The Daubechies wavelet basis of order N is a multi-resolution scheme
with an optimal choice of coefﬁcients ai, in which the wavelet ψ is taken to be of
compact support and ‘smooth’ (more precisely, with N zero moments; see [27]).
Solving Linear Equations
Orthonormal expansions can be used to solve linear equations T x = y, where x and
y are elements of some (separable) Hilbert space, and T an operator on it. Given
an orthonormal basis { en }, the vectors x and y can be written in terms of it as
x = ⎬
n anen and b = ⎬
n bnen. Of these, the scalar coefﬁcients an := ∞en, x√
are unknown and to be determined, but bn := ∞en, y√can be calculated explicitly.
Substituting into T x = y we get
⎪
n
anT en = T
⎪
n
anen

=
⎪
n
bnen.
Moreover the vectors T en can also be expanded as T en = ⎬
m Tm,nem for some num-
bers Tm,n = ∞em, T en√.So,comparingcoefﬁcientsintheequation⎬
n,m anTm,nem =
⎬
m bmem, we ﬁnd

214
10
Hilbert Spaces
⎪
n
Tm,nan = bm.
This can be thought of as a matrix equation in ∂2 with the matrix [Tm,n] having a
countable number of rows and columns:

⎛⎜
T11 T12 . . .
T21 T22 . . .
...
...
...
⎝
⎞⎟

⎛⎜
a1
a2
...
⎝
⎞⎟=

⎛⎜
b1
b2
...
⎝
⎞⎟.
It is precisely the equation T x = y written in terms of the coefﬁcients of T , x and y
in the orthonormal basis en. Effectively, the problem has been transferred from one
in H to one in ∂2, via the isomorphism J : H ∗∂2.
For practical purposes, one can truncate the matrix and vectors to yield a ﬁnite
N × N matrix equation that can then be solved. This can be justiﬁed because the
remainder terms of y and x, namely ⎬∞
n=N+1 bnen, etc., converge to 0 as N ∗∞.
For theoretical purposes, the method is useful if the orthonormal basis elements
en are eigenvectors of T , that is, T en = φnen. This makes the matrix of T diagonal.

⎛⎜
φ1 0 . . .
0 φ2
...
...
⎝
⎞⎟

⎛⎜
a1
a2
...
⎝
⎞⎟=

⎛⎜
b1
b2
...
⎝
⎞⎟.
The equation is easily solved, an = bn/φn, unless φn = 0. If φn = 0 (i.e., T x = 0
has non-trivial solutions) there are no solutions of 0an = bn unless bn = 0, in which
case the an are arbitrary. Thus there will be a solution x if, and only if, bn vanishes
whenever φn does, or equivalently, y ⊆kerT . Separating the vectors em that satisfy
T em = 0 from the rest, the complete solution is
x =
⎪
m:φm=0
ψmem +
⎪
n:φn∈=0
bn
φn
en,
where ψm are arbitrary constants. The ﬁrst series is a solution of the “homogeneous
equation” T x = 0, while the second series is a “particular solution” of T x = y.
For the case of the Hilbert space L2(A), with en and b = f all functions, the
particular solution can be rewritten as
⎪
n
bn
φn
en =
⎪
n
∞en, f √
φn
en =
⎧
A
⎪
n
en(s)en(x)
φn

f (s) ds.
The kernel G(x, s) := ⎬
n en(s)en(x)/φn is called the Green’s function of the oper-
ator T .

10.6 Orthonormal Bases
215
Gaussian quadrature
A central problem in numerical analysis is to ﬁnd an approximation for the integral
of a real function, in the form
⎧b
a
f ≈a1 f (x1) + · · · + aN f (xN) =: φ( f ),
where ai, xi are ﬁxed numbers; note that φ is a functional acting on f . The familiar
trapezoid rule and Simpson’s rule are of this type, where the xn are equally spaced
along [a, b]. The question arises as to whether we can do better by choosing xn in
some other optimal way.
Let en(x) be real orthonormal polynomials of degree n in the space L2[a, b],
obtained from 1, x, x2, …, by the Gram-Schmidt process. By orthogonality, their
integrals vanish since
⎭b
a en = ∞1, en√= 0, except for
⎭b
a e0 = ♦1♦L2[a,b]. Certainly,
for φ(en) to agree with the integral
⎭b
a en for n = 1, . . . , N −1, we must require

⎛⎛⎛⎜
e0(x1)
. . .
e0(xN)
e1(x1)
. . .
e1(xN)
...
...
eN−1(x1) . . . eN−1(xN)
⎝
⎞⎞⎞⎟

⎛⎜
a1
...
aN
⎝
⎞⎟=

⎛⎛⎛⎜
♦1♦
0
...
0
⎝
⎞⎞⎞⎟,
whichcanbesolvedforan when xn areknown.ThemainpointofGaussianquadrature
is that if xn are chosen to be the N roots of the polynomial eN(x) (assuming they lie
in [a, b]), we also get
⎭b
a en = 0 = φ(en) for n ⩽2N −1.
For consider the division of any e := em (1 ⩽m ⩽2N −1) by eN, e = qeN + r
whereq andr are real polynomials of degree at most N−1. Then, as e0 is proportional
to 1, and q ⇒[[1, x, . . . , x N−1]] = [[e0, . . . , eN−1]],
0 = ∞1, e√=
⎧b
a
qeN + r = ∞q, eN√+ ∞1,r√= ∞1,r√.
Hence r = ⎬N−1
k=1 bkek for some scalars bk, and by the choice of the coefﬁcients an,
and eN(xn) = 0,
e(xn) = q(xn)eN(xn) + r(xn) = r(xn),
so φ(e) =
N
⎪
n=1
ane(xn) =
N
⎪
n=1
anr(xn) =
N−1
⎪
k=1
bk
N
⎪
n=1
anek(xn) = 0 =
⎧b
a
e.
Thus the integral of any f = ⎬
n ψnen ⇒L2[a, b] agrees with φ( f ) up to order
n = 2N −1,

216
10
Hilbert Spaces
⎧b
a
f =
⎪
n
ψn
⎧b
a
en ≈
2N−1
⎪
n=1
ψnφ(en) ≈φ( f ).
The residual error can be made as small as needed by taking a larger N.
For example, using the Legendre polynomials, (prove!)
⎧1
−1
f (x) dx ≈0.35 f (−0.86) + 0.65 f (−0.34) + 0.65 f (0.34) + 0.35 f (0.86).
All this applies equally well for weighted L2
w(A) spaces; for example, using Laguerre
polynomials,
⎧∞
0
f (x)e−x dx ≈0.60 f (0.32) + 0.36 f (1.75) + 0.039 f (4.5) + 0.00054 f (9.4).
In practice, the algorithm of choice of most mathematics software is currently the
Gauss-Kronrod algorithm, which performs Gaussian quadrature but reﬁnes it adap-
tively by taking more evaluation points if necessary.
Signal Processing
Sounds, images, and signals in general can be thought of as vectors in L2(R), L2(R2),
and L2(A) respectively. They can thus be decomposed into orthonormal sums with
all the advantages that that entails. Three applications are:
(a) Storingonlythe“largest”coefﬁcients ψn := ∞en, x√of anorthonormal expansion
leads to a useful compressed form of the vector x. Compression ratios of about
100 are quite typical. A close copy of x can easily be regenerated from these
coefﬁcients using x = ⎬
n ψnen. Although not identical to the original (because
the small terms were omitted), it may be good enough for the purpose, especially
since the smallest coefﬁcients are usually unappreciated ﬁne detail or noise.
(b) A vector can be altered intentionally by manipulating its coefﬁcients. For exam-
ple, it can be improved by ﬁltering out noise coefﬁcients, or particular features
in a function may be picked out, e.g. image contrast may be enhanced if certain
coefﬁcients are weighted more than others.
(c) A vector may be matched with a database of other vectors, by taking the inner
product with each of them, using Parseval’s identity ∞x, y√= ⎬
n ψnβn. That
vector with the largest correlation ∞x, y√gives the best match and can be selected
for further investigation.
Consequently, the storage, transmission, rapid retrieval, and comparison of images
and sounds have seen a tremendous change in the past two decades, in part feeding
the growth not only of the internet and mobile phones, but also of new scientiﬁc tools.
For example, speech-, handwriting-, and face-recognition software ﬁnd phonemes,
characters, and faces that best match the given input; an E.C.G./E.K.G. or E.E.G.
signal may be compared to a database for the early detection of cardiac arrest or

10.6 Orthonormal Bases
217
epileptic ﬁts; the U.S. F.B.I. performs more than 50,000 ﬁngerprint matches daily,
etc.
To see one application in some detail, let us look at one popular image format—
JPEG (1992 standard). Color images consist of an array of pixels, each digitized into
three numbers (R, G, B) ⇒[0, 1]3 representing the red, green, and blue content.
In the JPEG algorithm, the three RGB color bytes for each pixel are usually ﬁrst
converted to brightness, excess red, and excess blue,
Y := r R + gG + bB,
Cr := 1
2 +
1
2(g + b)(R −Y),
Cb := 1
2 +
1
2(r + g)(B −Y),
wherer ≈0.25, g ≈0.65, b ≈0.1areagreed-uponconstantssuchthatr+g+b = 1.
This is done to avoid effects due to color-shifts and because the brightness picture
carries most of the visible information; in fact the excess red/blue pixels are reduced
in number by a factor of 4 because the eye is not sensitive to ﬁne detail in pure color.
The image is then split into 8×8 blocks, and each block is expanded with respect
to the cosine basis cos(δn(x + 1
2)/8) cos(δm(y + 1
2)/8) (the cosine transform is
preferred for positive functions in general because the ﬁrst few coefﬁcients are larger;
however it is not so good for sharp lines). The resulting 64 coefﬁcients for each block
are discretized (by multiplying by a user-deﬁned weight, and taking the integer part).
Most are now zero, and the rest are squeezed further using the standard Huffman
compression algorithm. This way, a 4Mpixel image, that normally requires 12 million
bytes in raw formats, can easily be reduced a hundredfold in ﬁle-size without any
visible loss of quality. JPEG 2000 uses wavelets instead but works in essentially the
same way; MPEG is JPEG 1992 adapted to video.
Similarly a 5 min CD-quality stereo sound clip, sampled at 44,000 times 16 bits a
second, would normally need at least 52 Mbytes. It can be compressed to about 10%
of that by MP3, an algorithm that works in an analogous way as JPEG, but adapted
to sound signals.
Remarks 10.36
1. The norm on matrices in B(CN, CM) that comes from the inner product deﬁned
in Example 10.2(2) is not the same as that deﬁned in Theorem 8.7 (but recall
that all norms on ﬁnite-dimensional Banach spaces are equivalent).
2. Re ∞x, y√is a real-valued inner product (over the reals), but Im∞x, y√fails the
last two axioms.
3. A real inner product on the real vector space X can be uniquely extended to its
complexiﬁcation X + i X, by
∞x1 + ix2, y1 + iy2√:= (∞x1, y1√+ ∞x2, y2√) + i(∞x1, y2√−∞x2, y1√).

218
10
Hilbert Spaces
Thus an inner product on RN can extend in several ways to R2N, but in only one
way to CN.
4. There is an interesting analogy between linear subspaces and logic: Think of sub-
spaces as “statements”, with A ⇒B meaning A ⇔B, and False, True, A and
B, A or B, not A, corresponding to 0, X, A ∪B, A + B, and A⊆, respectively.
What are the logical rules that correspond to Proposition 10.9? Are all classical
logic rules true in this sense?
5. The polarization identity states that a complex inner product ∞x, y√is a weighted
average of lengths on a circle of radius ♦x♦, centered at y. It can be generalized
further: if θN = 1 (N > 2), then
∞x, y√= 1
N
N
⎪
n=1
θn♦y + θnx♦2.
Even more generally, ∞x, y√=
1
2δi
⎭
S1 ♦y + zx♦2 dz.
6. A normed space with a conjugate-linear “isomorphism” J : X ∗X∀, has a
sesquilinear product ∞x, y√:= (x∀y + y∀x)/2 (where x∀:= Jx). The additional
property x∀x = ♦x♦2 turns it into an inner product space, compatible with the
norm of X.
7. The conjugate gradient method is an iteration to solve T ∀T x = y, used espe-
cially when T is a very large matrix. Note that ∞∞x, y√√:= ∞x, T ∀T y√is an inner
product when T is 1–1. If ei were an orthonormal basis with respect to this inner
product, and x = ⎬
j ψ je j, then
ψ j = ∞∞e j, x√√= ∞e j, T ∀T x√= ∞e j, y√,
and x canbefound.TheiterationisessentiallytheGram-Schmidtprocessapplied
to the residual vectors rn = y −T ∀T xn, while calculating the approximate
solutions xn on the go, (|||x|||2 := ∞∞x, x√√)
e0 := y/|||y|||,
e≡
n+1 := rn −∞∞en,rn√√en,
en+1 := e≡
n+1/|||e≡
n+1|||,
x0 = ∞e0, y√e0,
xn+1 := xn + ∞en+1, y√en+1,
r0 := y −T ∀T x0,
rn+1 := y −T ∀T xn+1.
8. QR decomposition: any operator T : X ∗Y between Hilbert spaces maps
an orthonormal basis ei ⇒X to a sequence of vectors T ei ⇒Y. If these are
orthonormalizedtoe≡
i usingtheGram-Schmidtprocess,then T ei = ⎬i
j=1 ψi je≡
j.
This means that, with respect to the bases ei and e≡
j, T has the upper-triangular
matrix R. If Q represents the change of bases in Y from e≡
j to the original one,
then the matrix of T is QR.

10.6 Orthonormal Bases
219
9. A continuous function f : [0, 2δ] ∗C, f (0) = f (2δ), traces out a looped path
or ‘orbit’ in the complex plane. If the Fourier coefﬁcients are written in polar
form, it is clear that each term ψneinα = rnei(nα+φn) describes a circle; and the
sum of two terms describes the motion along a circle whose center also moves
in a circle. The whole Fourier sum then represents a motion along regressively
smaller circles. Ptolemy and other Greek astronomers were the ﬁrst to describe
a periodic motion in terms of these cycles within cycles.
10. A non-separable Hilbert space is still isomorphic to an ∂2(A) space, one with
an uncountable number of orthonormal basis vectors. For example every Hilbert
space with an orthonormal basis { et } where t ⇒[0, 1] is isomorphic to the space
∂2[0, 1] consisting of functions ψt for which ♦ψ♦2 := ⎬
t |ψt|2 < ∞(Note: ψ
can take only a countable number of non-zero values.)
11. The ﬁrst important application of the least-squares method was by Gauss. In
1801, G. Piazzi found the long-sought ‘missing’ planet between the orbits of
Jupiter and Mars, but could not observe it again after it went behind the Sun.
Gauss managed to recover its orbital parameters from Piazzi’s observations,
and Ceres was relocated almost a year after its discovery. Essentially the same
techniques were used in 1846 to predict the location of a new planet, Neptune,
from the irregularities in the observed positions of Uranus.
12. There is a discrete version of the Fourier basis, on L2[0, 1], called the Walsh
basis, which consists of step functions. For each N = 1, 2, . . ., there are 2N
Walsh basis functions, each with a step-width of 1/2N and the list of heights are
the normalized column vectors of the Hadamard matrices.

Chapter 11
Banach Spaces
In this chapter, we explore deeper into the properties of operators and functionals
on general Banach spaces. At the same time, we generalize several deﬁnitions and
propositions that hold for Hilbert spaces. As these spaces are, in many ways, very
special and non-typical examples of Banach spaces, we need to modify these results
in several technical ways: There are no orthonormal bases, or Riesz correspondence,
or orthogonal projections available in Banach spaces.
11.1 The Open Mapping Theorem
The following theorem holds the key to several unanswered questions that were
raised earlier.
Theorem 11.1
The Open Mapping Theorem
Every onto continuous linear map between Banach spaces maps open sets
to open sets.
Proof Let T : X ∞Y be an onto operator between the Banach spaces X and Y. Let
U be an open subset of X, and let x √U, so that x √B∂(x) ∗U. If it can be shown
that T BX contains a ball Bδ(0), then
T x √Bδ∂(T x) = T x + ∂Bδ(0) ∗T x + ∂T BX = T B∂(x) ∗TU
implies that TU is an open set in Y, proving the theorem.
Now X = ⇒
n=1 Bn(0), so T X = ⇒
n=1 T Bn(0). But T X = Y is complete, so
by Baire’s category theorem, not all the sets T Bn(0) are nowhere dense: there must
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_11,
221
© Springer International Publishing Switzerland 2014

222
11
Banach Spaces
be an N such that T BN(0) contains a ball. By re-scaling we ﬁnd that T BX contains
a ball Br(a). It follows that for every y √Br(0) we have
a + y = lim
n∞⇒T xn,
for some xn √BX,
a −y = lim
n∞⇒T x→
n,
for some x→
n √BX,
∴
y = lim
n∞⇒T
⎛xn −x→
n
2
⎜
√T BX
since ♦xn −x→
n♦< 2. Consequently we have that Br(0) ∗T BX.
Claim: T BX ∗T B3(0). Let y √T BX, so that there must be an x0 √BX such that
♦y −T x0♦< r/2; that is, ♦x0♦< 1 and y −T x0 √Br/2(0) ∗T B1/2(0). But this
implies that there is an x1 √B1/2(0) such that ♦y −T x0 −T x1♦< r/4. Continuing
in this fashion, we get a sequence xn such that
♦xn♦< 1
2n ,
♦y −T (x1 + · · · + xn)♦< r
2n .
We can conclude that x := ⎝
n xn converges absolutely, with ♦x♦⩽⎝⇒
n=0
1
2n = 2,
and that y = T x √T B2[0] ∃T B2+∂(0).
Re-scaling the vectors in Br(0) ∗T B3(0) gives Br/3(0) ∗T BX and closes the
argument.
∀∈
Corollary 11.2
Every bijective operator between Banach spaces is an isomorphism.
With this fact, we are ready for the analogue of the ﬁrst isomorphism theorem of
vector spaces, which is a generalization of the corollary.
Proposition 11.3
For any operator T : X ∞Y between Banach spaces,
X/ ker T ⊆= im T
⇔
im T is closed in Y
⇔⇐c > 0, ⊂x √X, ♦x + ker T ♦⩽c♦T x♦.
Proof The mapping J : x + ker T ⇔∞T x is well-deﬁned because T (x + a) = T x
for any a √ker T . It is obviously onto im T , is 1–1 because
T x = 0 ⇔x √ker T ⇔x + ker T = ker T,

11.1 The Open Mapping Theorem
223
is trivially linear, and continuous since, for an √ker T chosen to satisfy ♦x + an♦∞
♦x + ker T ♦,
♦T x♦= ♦T (x + an)♦⩽♦T ♦♦x + an♦∞♦T ♦♦x + ker T ♦.
So J is an isomorphism precisely when J −1 is continuous, i.e., when the stated the
inequality holds (Proposition 8.12).
By the corollary to the open mapping theorem, this is the case if the range of
J, namely im T , is complete (closed in Y by Proposition 4.7). For the converse,
X/ ker T is complete (Proposition 8.18), as must be any isomorphic copy such as
im T .
∀∈
Examples 11.4
1. If T √B(X, Y) is an operator on Banach spaces, and Y = im T ∩M for some
closed linear subspace M of Y, then im T is closed in Y.
Proof The mapping X/ ker T ∞im T deﬁned in the proof above can be extended
to (X/ ker T ) × M ∞Y by (x + ker T, a) ⇔∞T x + a; it is continuous and
bijective, hence an isomorphism. The conclusion follows since it sends the closed
set (X/ ker T ) × { 0 } to im T .
2. ▶Let T : X ∞Y be a linear map between Banach spaces; its graph M :=
{ (x, T x) : x √X } is a linear subspace of X × Y, and the map J : M ∞X,
deﬁned by J(x, T x) := x is 1–1, onto, linear, and continuous.
Closed Graph Theorem: If M is also closed in X ×Y, then it is a Banach subspace,
and the open mapping theorem implies that J is an isomorphism, so that
♦T x♦Y ⩽♦(x, T x)♦M ⩽c♦x♦X
and T must be continuous.
3. ▶It is important that Y be complete for the open mapping theorem to be valid.
The identity map α1 ∞α⇒is continuous and 1–1, but α1 is not isomorphic
to its image, because the latter is not complete (in the ⇒-norm). For example,
xN := (1, 1
2, . . . , 1
N , 0, . . .) converge in the ⇒-norm, but not to an α1-sequence.
4. If X has two complete norms, and ♦x♦⩽c|||x||| for some ﬁxed c > 0, then
the two norms are equivalent: the identity map X|||||| ∞X♦♦is continuous by
hypothesis, and obviously linear and bijective; so its inverse is also continuous.
Put differently, if two complete norms on X are inequivalent, then one can ﬁnd
vectors xn which are unit with respect to one norm, but growing indeﬁnitely with
respect to the other. Clearly, this can only happen in inﬁnite dimensions.

224
11
Banach Spaces
Complementarity
We are now in a position to answer an earlier question about projections: It is not
always possible to project continuously to a closed subspace. The following propo-
sition determines exactly when such a projection exists:
Proposition 11.5
There is a projection onto a closed linear subspace M of a Banach space
X if, and only if,
X = M ∩N
for some closed linear subspace N. In this case M = im P, N = ker P, and
M ∩N ⊆= M × N.
We say that M, N are complementary closed subspaces.
Proof The forward implication has already been proved (Example 8.16(3)).
Conversely, suppose X = M ∩N, so that any x = a +b for some a √M, b √N.
Uniqueness of a, b follows from
a1 + b1 = x = a2 + b2 ∪a1 −a2 = b2 −b1 √M ∩N = 0,
∪a1 = a2 and b1 = b2.
This allows us to deﬁne the function P : X ∞X by P(x) := a. It is linear since
P(γx1 + x2) = P(γa1 + γb1 + a2 + b2) = γa1 + a2 = γPx1 + Px2,
When x belongs to M or N, we get the special cases
⊂a √M, Pa = P(a + 0) = a;
⊂b √N, Pb = P(0 + b) = 0,
so im P = M and ker P = N, since any x √ker P satisﬁes 0 = Px = a implying
x = b √N.
P is a continuous projection: P2 = P since, for any x = a + b √M ∩N,
P2x = Pa = a = Px. Finally, the map J : M × N ∞X, J(a, b) := a + b,
between Banach spaces, is 1–1, onto and continuous
♦a + b♦X ⩽♦a♦X + ♦b♦X = ♦(a, b)♦M×N
and so is an isomorphism by the open mapping theorem. Therefore
♦Px♦= ♦a♦⩽♦a♦+ ♦b♦= ♦(a, b)♦M×N ⩽c♦a + b♦X = c♦x♦
∀∈

11.1 The Open Mapping Theorem
225
Every subspace M can be extended by another subspace N such that X = M ∩N
(by extending a basis for M to span X) but complementarity requires M, N to be
closed.
Examples 11.6
1. Finite-dimensional subspaces are always complemented.
Proof The projection to M = [[e1, . . . , eN]] is simply x ⇔∞δ1(x)e1 + · · · +
δN(x)eN, where δm are the dual basis for M◦(δm(en) := δnm). Although δm are
deﬁned on M, they can be extended to X◦as seen later (Theorem 11.17).
2. Finite-codimensional closed subspaces are complemented.
Proof Let e1 + M, . . . , en + M be a basis for X/M, and let N := [[e1, . . . , en]]
(complete). Then, for any x,
x + M =
n
⎞
i=1
ξi(ei + M) =
n
⎞
i=1
ξiei + M = a + M
which shows x −a √M, a √N, so x √M + N. If x √M ∩N, then the above
identity gives M = ⎝
i ξi(ei + M), so ξi = 0 (linear independence of ei + M)
and x = 0.
3. For Banach spaces, if T : X ∞Y is onto, and X = ker T ∩M then it follows
that Y ⊆= X/ ker T ⊆= M is embedded in X.
4. * A Banach theorem: If X is a separable Banach space, then there is an onto
operator T : α1 ∞X.
(‘Proof’ Let xn be dense in BX, and let T : α1 ∞X be deﬁned by T (en) := xn,
extended linearly. Then it follows easily that ♦T ♦= 1, so T Bα1 = BX, and by a
similar argument of the proof of the open mapping theorem, T Bα1 = BX.)
Hence, if X is not embedded in α1, then ker T is not complemented (by the
previous example).
Exercises 11.7
1. For a projection P : X ∞X, X/ im P ⊆= ker P, while ♦x + ker P♦⩽♦Px♦.
2. Second isomorphism theorem: If M, N, and M + N are closed subspaces of a
Banach space, then (M + N)/N ⊆= M/M ∩N, using the map M ∞(M + N)/N,
x ⇔∞x + N.
3. Third isomorphism theorem: Let M ∗N be closed subspaces of X, then X/M
N/M ⊆=
X
N using the map X/M ∞X/N, x + M ⇔∞x + N. If M is ﬁnite-codimensional
then codim N ⩽codim M.
4. Let T : X ∞Y and S : X ∞Z be operators on Banach spaces.
(a) If M is a closed linear subspace of ker T , then x + M ⇔∞T x is well deﬁned,
linear, and continuous.

226
11
Banach Spaces
(b) If S is onto and Sx = 0 ∪T x = 0, then Sx ⇔∞T x is a well-deﬁned
operator in B(Z, Y).
5. * Suppose the Banach space X has a Schauder basis en (of unit norm). For
x = ⎝
n ξnen, it can be shown that |||x||| := supn ♦⎝n
i=1 ξiei♦exists and is a
complete norm. Show ♦x♦⩽|||x||| and deduce that the map φn : x ⇔∞ξn is in
X◦. These functionals form a Schauder basis for X◦, called the bi-orthogonal or
dual basis, and satisfy φn(em) = δnm.
6. Let M, N be closed subspaces of a Banach space, with M ∩N = { 0 }. Then
M + N is closed ⇔P : M + N ∞M, x + y ⇔∞x, is continuous.
7. If φ : X ∞F is linear with ker φ closed, then φ is continuous.
8. If M is a complemented closed subspace of X, then X ⊆= X
M × M.
9. If X = M∩N with M, N closed, then there is a minimum separation ♦u −v♦∴c
between any unit vectors u √M, v √N.
11.2 Compact Operators
A linear map is continuous when it maps bounded sets to bounded sets. There is a
special subclass of linear maps that go further:
Deﬁnition 11.8
A linear mapping between Banach spaces is called compact when it maps
bounded sets to totally bounded sets.
Easy Consequences
1. Compact linear maps are continuous (originally called completely continuous).
2. If T , S are compact operators, then so are T +S and γT (since B bounded implies
γT B and subsets of T B + SB are totally bounded (Proposition 7.13)).
3. The identity map I : X ∞X is not compact when the Banach space is inﬁnite
dimensional (it cannot convert the unit ball to a totally bounded set (Proposition
8.23)).
4. It is enough to show that T maps the unit ball to a totally bounded set for T to be
compact (since B ∗Br(0) ∪T B ∗rT BX).

11.2 Compact Operators
227
Proposition 11.9
If T is compact and S continuous linear, then ST and T S are compact
(when deﬁned).
If Tn are compact and Tn ∞T then T is compact.
For a compact operator T , im T is separable, and is closed only when
ﬁnite-dimensional.
Proof (i) Starting from a bounded set, T maps it to a totally bounded set and S, being
Lipschitz, maps this to another totally bounded set (Proposition 6.7); or starting with
a bounded set, S maps it to another bounded set (Exercise 4.17(3)), which is then
mapped by T to a totally bounded set.
(ii) Let B be a bounded set, with its vectors having norm at most c. Then for any
x √B, T x = Tnx + (T −Tn)x, and
♦(T −Tn)x♦⩽♦T −Tn♦♦x♦⩽c♦T −Tn♦∞0.
Hence for n large enough, independent of x √B, ♦(T −Tn)x♦< ∂/2; in other
words (T −Tn)B ∗B∂/2(0). Moreover Tn B is totally bounded and so,
T B ∗Tn B + (T −Tn)B ∗
N
⎟
i=1
B∂/2(xi) + B∂/2(0) =
N
⎟
i=1
B∂(xi).
Thus T B is totally bounded and T is compact.
(iii) Totally bounded sets are separable (Example 6.6(3)), so the image of T ,
im T = T X = T
⇒
⎟
n=1
Bn(0) =
⇒
⎟
n=1
T Bn(0),
being the countable union of separable sets, is separable (Exercise 4.21(3)).
If im T is complete, then it is a Banach space in its own right. The open mapping
theorem can be used to conclude that the unit (open) ball BX is mapped to an open
and totally bounded set T BX ∗im T . As 0 is an interior point of it, there is a
totally bounded ball Br(0) ∩im T ∗T BX. This can only happen if im T is ﬁnite
dimensional.
∀∈
Examples 11.10
1. An operator whose image has ﬁnite dimension (ﬁnite rank) is compact. The reason
is that, in a ﬁnite-dimensional space, bounded sets are necessarily totally bounded
(Proposition 8.23, Exercise 6.9(5)). For example, matrices and functionals are
compact operators of ﬁnite rank.

228
11
Banach Spaces
2. ▶A common way of showing that an operator is compact is to show that it is the
limit of operators of ﬁnite rank.
For example let T : α2 ∞α2 be deﬁned by T (an) := (an/n). First cleave the
operator to TN deﬁned by TN(an) := (a1/1, a2/2, . . . , aN/N, 0, 0, . . .). This
operator maps α2 to an N-dimensional space. Showing it is continuous would
imply it is compact of ﬁnite rank:
♦TN(an)♦2
α2 =
N
⎞
n=1
|an/n|2 ⩽
N
⎞
n=1
|an|2 ⩽♦(an)♦2
α2.
Furthermore, TN ∞T :
♦(T −TN)(an)♦2
α2 =
⇒
⎞
n=N+1
|an/n|2 ⩽1
N 2
⇒
⎞
n=N+1
|an|2 ⩽♦(an)♦2
α2/N 2.
Hence ♦T −TN♦⩽1/N ∞0 as N ∞⇒as required.
3. TN f (x) :=
N
⎞
n=−N
⎠f (n)e2πinx is an example of an operator of ﬁnite rank on
L1[0, 1].
4. ▶If T is a compact operator on Banach spaces and (xn) is bounded, then (T xn)
has a convergent subsequence.
Proof The sequence (T xn) is totally bounded, hence has a Cauchy subsequence,
which converges by virtue of the completeness of the codomain.
An important source of examples of compact operators is the following:
Proposition 11.11
If the kernel k is a continuous function [a, b]×[c, d] ∞C, then the integral
operator T : C[a, b] ∞C[c, d],
T f (y) :=
⎫b
a
k(x, y) f (x) dx
is compact.
Proof Let F be the unit ball of functions in C[a, b]. For any y √[c, d], and f √F,
|T f (y)| ⩽(b −a)♦k♦L⇒♦f ♦L⇒⩽(b −a)♦k♦L⇒,
so (T F)[c, d] is bounded in C, hence totally bounded.

11.2 Compact Operators
229
As k is continuous on the compact set [a, b] × [c, d], it is uniformly continuous
(Proposition 6.17). So for any ∂> 0 there is a δ > 0 such that for |y1 −y2| < δ,
|T f (y1) −T f (y2)| ⩽
⎫b
a
|k(x, y1) −k(x, y2)|| f (x)| dx ⩽∂(b −a).
This implies that T f is continuous and, as δ is independent of f , T F is equicon-
tinuous. By the Arzelà-Ascoli theorem (Theorem 6.26), T F is totally bounded in
C[c, d], and the integral operator T is compact.
∀∈
Fredholm Operators
Deﬁnition 11.12
A Fredholm operator is one whose kernel is ﬁnite-dimensional and whose
image has ﬁnite codimension. The index of a Fredholm operator is the differ-
ence
index(T ) := dim ker T −codim im T.
A Fredholm operator T : X ∞Y gives rise to decompositions
X = ker T ∩M,
Y = im T ∩N,
for some closed linear subspaces M, N by Examples 11.6(1, 2) and 11.4(1). The
restricted operator R : M ∞im T , x ⇔∞T x is then bijective and continuous, and
thus an isomorphism by the open mapping theorem.
Proposition 11.13
Index Theorem
The composition of Fredholm operators is again Fredholm, and
index(ST ) = index(S) + index(T ).
Proof Let T √B(X, Y), S √B(Y, Z), both Fredholm, with n := dim(ker T ),
m := codim(im S). Y decomposes as
Y = N ∩im T = ker S ∩M = A ∩B ∩C ∩D
where
A := ker S ∩N of dimension a,

230
11
Banach Spaces
B := im T ∩ker S of dimension b,
C := M ∩N of dimension c,
D := M ∩im T .
Then dim ker ST = n + b, codim im ST = c + m, both ﬁnite, and the index of ST
is n + b −c −m = (a + b −m) + (n −a −c) = index(S) + index(T ).
∀∈
What is the connection with compact operators, one might ask?
Proposition 11.14
An operator T : X ∞Y on Banach spaces is Fredholm, if and only if T
is invertible “up to compact operators”, that is, there exist K1 √B(X),
K2 √B(Y) compact and S √B(Y, X), such that
ST = I + K1,
T S = I + K2.
In fact, K1, K2 can be taken to be of ﬁnite rank.
Proof Suppose T is Fredholm, so X = ker T ∩M, Y = im T ∩N, and the map
R : M ∞im T is an isomorphism. Let P be the ﬁnite-rank projection onto ker T
with kernel M, and Q that projection onto N along im T (∴QT = 0 = T P), and
let S := R−1(I −Q). Then T R−1 = I and R−1T = I −P, so for any x √X,
y √Y,
ST x = R−1(I −Q)T x = R−1T x = (I −P)x,
T Sy = T R−1(I −Q)y = (I −Q)y,
so ST = I −P, T S = I −Q.
Conversely,
ker T ∗ker ST = ker(I + K1) =: M.
For x √M, K1x = −x, i.e., I = −K1|M compact, hence M, and thus ker T , are
ﬁnite-dimensional. Similarly,
im T ≈im T S = im(I + K2).
Now, in general, the operator R := I + K has a closed image, for any compact
operator K. For suppose there are vectors yn such that
♦yn + ker R♦= 1 and Ryn ∞0.

11.2 Compact Operators
231
The ﬁrst condition implies that there are vectors vn √ker R such that 1 ⩽♦un♦⩽2,
where un := yn −vn. As K is compact and un bounded, there is a subsequence um
such that Kum ∞y (Example 11.10(4)). Subtracting this from Rum = Rym ∞0
gives um ∞y. Consequently, Rum converges to both Ry and to 0, that is, y √ker R.
However, ym −vm ∞y then contradicts the given condition that ym are at unit
distance from ker R. It must be the case that there is a constant c > 0 such that
♦y + ker R♦⩽c♦Ry♦,
and im R is closed (Proposition 11.3).
It should be clear that the map Y ∞Y/ im R deﬁned by
y ⇔∞K y + im R = (y + K y) −y + im R = −y + im R
is both compact and onto, hence it is of ﬁnite rank. This means that Y/ im R, and by
implication Y/ im T , are ﬁnite-dimensional.
∀∈
Exercises 11.15
1. The multiplication operator (an) ⇔∞(bnan) (on α1, α2, or α⇒) is compact ⇔
bn ∞0.
2. The operator V (an) := (0, a0, a1/2, a2/3, . . .) (on α1, say) is compact. But the
shift operators are not.
3. The operator T x := ⎝N
n=1(φnx)yn, for any φn √X◦, yn √Y, is of ﬁnite rank.
In the limit N ∞⇒it gives a compact operator if ⎝⇒
n=1 ♦φn♦♦yn♦< ⇒.
In fact, any operator of ﬁnite rank must be of this type T x = ⎝N
n=1(φnx)en with
φn √X◦and en a basis for im T .
4. If S, T are linear of ﬁnite rank, then so are γT and S + T ; if S is any linear map,
then ST and T S are of ﬁnite rank, when deﬁned.
5. No isomorphism between inﬁnite-dimensional Banach spaces X, Y, can be com-
pact. If T : X ∞Y is compact and invertible, then T −1 cannot be continuous.
6. If T : X ∞Y is compact, then so is its restriction to a closed subset M ∗X,
T |M : M ∞Y.
7. The index of an m × n matrix is n −m.
8. The right-shift operator R (on α⇒say) is Fredholm with index −1; that of the
left-shift operator is +1. The index of a projection is 0 when deﬁned.
11.3 The Dual Space X∞
Functionals provide very useful tools in converting vectors to numbers, and vector
sequences to more amenable numerical sequences. Thus if we are uncertain whether
xn ∞x then we might try to see if φxn ∞φx for some continuous functional—if it

232
11
Banach Spaces
does not converge, neither does xn. Moreover, X◦is a sort of mirror-image, or dual,
of X: every vector in X can be thought of as a linear operator x : F ∞X, γ ⇔∞γx,
while functionals are linear operators φ : X ∞F, x ⇔∞φx. It turns out that the
space X◦is at least as “rich” as the normed space X, in the sense that X can be
recovered from X◦as a subspace of X◦◦.
Examples 11.16
1. The functionals of a Hilbert space are in 1–1 correspondence with the vectors by
the Riesz representation theorem.
2. Recall that α1◦≡α⇒, α2◦≡α2, and c◦
0 ≡α1 (Propositions 9.6, 9.9, and Theo-
rem 9.3).
3. We will see later that every functional on B(CN) is of the type φT = tr(ST )
where tr S is the trace of the matrix S (Theorems 15.31 and 10.16).
4. (X × Y)◦⊆= X◦× Y ◦, via the isomorphism (φ, β) ⇔∞ω where ω(x, y) :=
φx + βy.
5. For φi, β √X◦, β ⎪n
i=1 ker φi = 0 ⇔β √[[φ1, . . . , φn]].
Proof If β ker φ = 0, (φ ̸= 0), then the map C ∞C, φ(x) ⇔∞β(x) is well-
deﬁned and linear, hence must be multiplication by some scalar γ, i.e., β = γφ.
Suppose β ⎪n+1
i=1 ker φi = 0. On the space ker φn+1, if φi x = 0, i = 1, . . . , n,
then βx = 0, so by induction, β = ⎝n
i=1 ξiφi. Let η := β −⎝n
i=1 ξiφi; then
φn+1x = 0 ∪ηx = 0, so η = ξn+1φn+1 as required. The converse is easy.
Our ﬁrst result concerning functionals is a powerful theorem which asserts the
existence of a functional on the whole space X, starting from a “fragment” of it on
a, perhaps much smaller, subspace Y. Like many existence-type theorems, the path
to construct such an extension is not straightforward.
Theorem 11.17
The Hahn-Banach Theorem
Let Y be a subspace of a normed space X. Then every functional φ √Y ◦
can be extended to some ˜φ √X◦, with ♦˜φ♦X◦= ♦φ♦Y ◦.
Proof Let us try to extend φ from a functional on Y to a functional ˜φ on Y + [[v]],
for a vector v ̸√Y, by selecting a number ˜φv := c. Once c is chosen, we are forced
to set ˜φ(y + γv) := φy + γc, for any γv √[[v]], to make ˜φ a linear extension of φ;
and to retain continuity with ♦˜φ♦= ♦φ♦, we need, for any y √Y and γ √F (γ ̸= 0),
|φy + γc| = | ˜φ(y + γv)| ⩽♦φ♦♦y + γv♦
⇔
|φ(y/γ) + c| ⩽♦φ♦♦y/γ + v♦
⇔
|φy + c| ⩽♦φ♦♦y + v♦,
(11.1)

11.3 The Dual Space X◦
233
(since the vectors y/γ account for all of Y). To proceed, we consider ﬁrst the case
of real scalars and then generalize to the complex ﬁeld.
Real Normed Space: Let us suppose that φ is real-valued. Thus we are required
to ﬁnd a c √R that satisﬁes inequality (11.1)
−φy −♦φ♦♦y + v♦⩽c ⩽−φy + ♦φ♦♦y + v♦,
⊂y √Y.
Is this possible? Yes, because for any y1, y2 √Y,
φy1 −φy2 ⩽|φ(y1 −y2)| ⩽♦φ♦♦y1 −y2♦
⩽♦φ♦(♦y1 + v♦+ ♦y2 + v♦)
= ♦φ♦♦y1 + v♦+ ♦φ♦♦y2 + v♦
∪
−φy2 −♦φ♦♦y2 + v♦⩽−φy1 + ♦φ♦♦y1 + v♦.
Since y1, y2 are arbitrary vectors in Y, there must be a constant c separating the two
sides of the inequality, as sought. Choosing any such c gives an extended functional
with ♦˜φ♦⩽♦φ♦(inequality (11.1)); but ˜φ extends φ, so ♦˜φ♦= ♦φ♦.
Complex Normed Space: Now consider the case when the the functional is
complex-valued. It decomposes into its real and imaginary parts φ = φ1 + iφ2,
but the two are not independent of each other because
φ1(iy) + iφ2(iy) = φ(iy) = iφy = iφ1(y) −φ2(y)
so that φ2(y) = −φ1(iy). Being real-valued, they cannot possibly belong to Y ◦, but
they do qualify as functionals on Y when restricted to the real scalars,
φ1(y1 + y2) = Re(φ(y1) + φ(y2)) = φ1(y1) + φ1(y2),
φ1(γy) = Re φ(γy) = γφ1(y),
⊂γ √R,
|φ1(y)| = | Re φy| ⩽|φy| ⩽♦φ♦♦y♦
(for φ2, substitute Re with Im ). So they have real-valued extensions ˜φi to Y + [[v]]
that are linear over the real scalars; actually, extending φ1 to ˜φ1 automatically gives
the extension for φ2. That is, deﬁne ˜φ(x) := ˜φ1(x)−i ˜φ1(ix). This is obviously linear
over the real scalars since ˜φ1 is. It is also linear over the complex scalars because
˜φ(ix) = ˜φ1(ix) −i ˜φ1(−x) = i(−i ˜φ1(ix) + ˜φ1(x)) = i ˜φ(x).
Moreover it is continuous since, using the polar form ˜φx = | ˜φx|eiθx ,
| ˜φx| = e−iθx ˜φx = ˜φ(e−iθx x) = ˜φ1(e−iθx x) ⩽♦˜φ1♦♦x♦= ♦φ1♦♦x♦⩽♦φ♦♦x♦,
so that ♦˜φ♦⩽♦φ♦; in fact, equality holds because the domain of ˜φ includes that
of φ.

234
11
Banach Spaces
Extending to X: If X can be generated from Y and a countable number of vectors
vn, then φ can be extended in steps, ﬁrst to some φ1 acting on Y + [[v1]], then to φ2
acting on Y + [[v1]] + [[v2]], etc. The ﬁnal extension is then ˜φx := φnx for x √X,
whenever x √Y +[[v1, . . . , vn]]. If these vectors are only dense in X (e.g. when X is
separable), ˜φ can be extended further with the same norm via ˜φ(x) := limn∞⇒˜φ(xn)
when xn ∞x, as a special case of extending a linear continuous function to the
completion spaces (Example 8.9(4)).
But even if X needs an uncountable number of generating vectors, then “Haus-
dorff’s maximality principle” can be applied to conclude that the extension goes
through to X. Let M be the collection of functionals φM acting on linear subspaces
M containing Y and extending φ with the same norm
M := { φM √M◦: ⊂y √Y, φM y = φy, and ♦φM♦= ♦φ♦}.
By Hausdorff’s maximality principle, M contains a maximal chain of subspaces
{ Mξ }, where φξ extends φσ whenever Mσ ∗Mξ. But E := 
ξ Mξ also allows
an extension of φ, namely β(x) := φξx for x √Mξ. It is well-deﬁned because
x √Mξ ∩Mσ implies Mξ ∗Mσ say, so φξx = φσx. It is linear and continuous
with the same norm as φ,
|βx| = |φξx| ⩽♦φξ♦♦x♦= ♦φ♦♦x♦.
Hence β is a maximal extension in M; in fact, E = X, for were it to exclude
any vector v, the ﬁrst part of the proof assures us of an extension that includes v,
contradicting the maximality of β.
∀∈
Proposition 11.18
For any x ̸= 0, there is a unit φ √X◦with φx = ♦x♦.
More generally, if M is a closed linear subspace and x ̸√M, then there is
a functional φ √X◦with ♦φ♦= 1, such that
φM = 0,
φx ̸= 0.
Proof If x ̸= 0, there are non-zero functionals on [[x]], such as β(γx) := γc (c ̸= 0);
in particular, to satisfy the requirement ♦φ♦= 1, choose φ(γx) := γ♦x♦. By the
Hahn-Banach theorem, it has an extension to all of X, with the same norm.
More generally, given x ̸√M, form the linear subspace
Y := [[x]] + M = { γx + a : γ √C, a √M }.
Y ◦contains the functional deﬁned by β(γx + a) := γ ♦x + M♦. It is clearly zero
when γ = 0 and is linear and continuous since

11.3 The Dual Space X◦
235
β(γ1x + a1 + μγ2x + a2) = (γ1 + μγ2)♦x + M♦= β(γ1x + a1) + μβ(γ2x + a2),
|β(γx + a)| = |γ|♦x + M♦= ♦γx + M♦⩽♦γx + a♦
and in fact ♦β♦= 1,
|β(x + an)|/♦x + an♦= ♦x + M♦/♦x + an♦∞1
for an
√
M chosen so that convergence of ♦x + an♦∞♦x + M♦occurs
(Proposition 8.18). So β can be extended to a functional φ on all of X with the
same norm.
∀∈
The Hahn-Banach theorem and its corollaries show that there is a ready supply of
functionals on normed spaces; admittedly, this does not sound exciting, but consider
that there are vector spaces (not normed), such as L p(R) with p < 1, that have
only trivial continuous functionals. For our purposes, its greater importance lies in
its ability to show a certain duality between X and its space of functionals X◦. For
example, the dual of the statement ♦φ♦= sup
♦x♦=1
|φx| is
Proposition 11.19
♦x♦= sup
♦φ♦=1
|φx|,
♦T ♦=
sup
♦φ♦=1=♦x♦
|φT x|
Proof |φx| ⩽♦x♦for all unit φ √X◦. But the functional just constructed satisﬁes
φx = ♦x♦and ♦φ♦= 1, so sup♦φ♦=1 |φx| = ♦x♦.
This in turn allows us to deduce
♦T ♦= sup
♦x♦=1
♦T x♦= sup
♦x♦=1
sup
♦φ♦=1
|φT x|.
∀∈
These identities generalize those for Hilbert spaces (Exercise 10.18(1)).
Proposition 11.20
Separating Hyperplane Theorem
If x √X does not lie in the closed ball Br[0], then there is a ‘hyperplane’
φ−1ξ which separates the two, that is,
⇐φ √X◦, ⇐ξ √R,
⊂y √Br[0],
|φy| < ξ < |φx|.
Proof Let φ : [[x]] ∞F, φ(γx) := γ♦x♦; its norm is 1 and φx = ♦x♦> r. It can
be extended to a functional on X with the same norm. Hence for any y in the closed
ball, |φy| ⩽♦φ♦♦y♦⩽r. The hyperplane is then γx + ker φ where r/♦x♦< γ < 1.
∀∈

236
11
Banach Spaces
Note that the proof remains valid when Br[0] is replaced by a closed balanced
convex set C since C + B∂(0) determines a semi-norm in which it is the open unit
ball (Exercise 7.7(8)).
Examples 11.21
1. The Hahn-Banach theorem and its corollaries are evident for Hilbert spaces:
(a) Any functional φ on a closed subspace M corresponds to a vector x √M,
and hence has the obvious extension ˜φ := ∇x, ≤on H.
(b) x = 0 ⇔⊂y √H, ∇y, x≤= 0.
(c) ♦x♦= supy̸=0
|∇x,y≤|
♦y♦
= supy◦̸=0
|y◦x|
♦y◦♦.
(d) One hyperplane separating x from Br[0] is x⊥+ ξx, r < ξ < ♦x♦.
2. Operators do not extend automatically as functionals do:
(a) If M is a complemented closed subspace of X, then every operator
T : M ∞Y can be extended continuously to X ∞Y.
(b) If the identity map I on the closed subspace M can be extended to X ∞M,
then M is complemented in X.
Proof Let X = M ∩N with M, N closed subspaces, and deﬁne ˜T (a+b) := Ta
for a √M, b √N. Then ♦˜T (a + b)♦= ♦Ta♦⩽c♦T ♦♦a + b♦(Proposition
11.5).
If ˜I : X ∞M is an extension of I : M ∞M, then ˜I 2x = I ˜I x = ˜I x, so it is a
projection in B(X). X then splits up as ker ˜I ∩im ˜I, where im ˜I = M.
3. If X is not separable then neither is X◦.
Proof Assume X◦separable, with φ1, φ2, . . . dense in it. By deﬁnition of their
norm, there must be (unit) vectors xn such that for a ﬁxed ∂> 0,
|φnxn| > (♦φn♦−∂)♦xn♦.
The claim is that M := [[xn]] is equal to X, making X separable. For if not, then
there is a unit functional β √X◦such that β M = 0; and there is a φn close to it,
♦β −φn♦< ∂, so
|φnxn| = |(β −φn)xn| ⩽♦β −φn♦♦xn♦< ∂♦xn♦.
Combining the two inequalities yields ♦φn♦< 2∂, and this contradicts that φn is
within ∂of the unit functional β.
4. Banach Limits. The functional Lim on c (Exercise 9.4(1)) can be extended (non-
uniquely) to a functional on α⇒.

11.3 The Dual Space X◦
237
Annihilators
Let us explore the duality between X and X◦more closely. The connection between
the two is the following construction, which allows us to shuttle between subspaces of
X and those of X◦. It is the generalization of the orthogonal spaces in Hilbert spaces
which, under the Riesz correspondence J, can be rewritten in terms of functionals,
A⊥= { x √H : ∇x, a≤= 0, ⊂a √A }
J
−−∞{ φ √H◦: φa = 0, ⊂a √A }.
Deﬁnition 11.22
The annihilator of a set of vectors A ∗X is the set of functionals
A⊥:= { φ √X◦: φx = 0,
⊂x √A }.
Similarly, given a set of functionals τ ∗X◦then the pre-annihilator is
⊥τ := { x √X : φx = 0,
⊂φ √τ }.
Easy Consequences
1. 0⊥= X◦, X⊥= 0.
2. A ∗B ∪B⊥∗A⊥.
3. A ∗⊥τ ⇔τA = 0 ⇔τ ∗A⊥.
The properties of A⊥generalize those for Hilbert spaces, such as Proposition 10.9
and Exercise 10.14(4).
Proposition 11.23
A⊥is a closed linear subspace of X◦with the following properties:
(i) (A ˆ B)⊥= A⊥∩B⊥and A⊥+ B⊥∗(A ∩B)⊥,
(ii) ⊥(A⊥) = [[A]],
(iii) [[A]] is dense in X ⇔A⊥= 0.
Proof That A⊥is a linear subspace is evident from
⊂φ, β √A⊥, a √A, γ √F
(φ + β)a = φa + βa = 0,
(γφ)a = γφa = 0.
Let φn ∞φ with φn √A⊥; for any a √A, 0 = φna ∞φa, so φ √A⊥and A⊥is
closed.

238
11
Banach Spaces
(i) Clearly, (A ˆ B)⊥is a subset of A⊥and B⊥, while φA = 0 = φB imply
φ(Aˆ B) = 0. If φ √A⊥, β √B⊥, and x √A∩B, then (φ +β)x = φx +βx = 0.
(ii) ⊥(A⊥) is a closed linear subspace of X (Ex. 2 below), and it contains A, since for
a √A and any φ √A⊥, φa = 0, so a √⊥(A⊥). Thus [[A]] ∗⊥(A⊥) (Proposition
7.10).
Conversely, let x ̸√[[A]]. Then by Proposition 11.18, there is a functional φ
satisfying both φ[[A]] = 0, hence φ √A⊥, and φx ̸= 0, hence x ̸√⊥(A⊥).
(iii) Consequently, [[A]] is dense precisely when ⊥(A⊥) = [[A]] = X, and this is
equivalent to A⊥= 0 (⊂x √X, φx = 0 ⇔φ = 0).
∀∈
The Double Dual X∞∞
A functional φ is an assignment of numbers φx as the vectors x vary in X. Suppose
we ﬁx x and vary φ instead, φ ⇔∞φx, what kind of object do we get? It is a mapping
from X◦to F, which is a possible candidate for a “double” functional in X◦◦.
Proposition 11.24
For any x √X, the map x◦◦φ := φx is a functional on X◦, and x ⇔∞x◦◦is
a linear isometry, embedding X in X◦◦.
Proof The mapping x◦◦: X◦∞F, φ ⇔∞φx, is clearly linear in φ, and continuous
with |x◦◦φ| = |φx| ⩽♦x♦♦φ♦, i.e., x◦◦√X◦◦.
Hence we can form the map J : X ∞X◦◦, deﬁned by J(x) := x◦◦. It is linear,
since for any φ √X◦, x, y √X, γ √F,
(x + y)◦◦(φ) = φ(x + y) = φx + φy = x◦◦(φ) + y◦◦(φ),
(γx)◦◦(φ) = φ(γx) = γ φx = γ x◦◦(φ).
J is isometric by Proposition 11.19, ♦x◦◦♦= sup
♦φ♦=1
|x◦◦φ| = sup
♦φ♦=1
|φx| = ♦x♦.
∀∈
Examples 11.25
1. Given any normed space X, the double dual X◦◦is a Banach space. Hence the
closure J X, being a closed linear subspace of X◦◦, is itself a Banach space. It is
isomorphic to the completion of X, denoted by ⎬X.
2. Several Banach spaces, called reﬂexive spaces, have the property that the mapping
x ⇔∞x◦◦is an isomorphism. Examples include αp (p > 1) and all Hilbert spaces
(Proposition 10.16).

11.3 The Dual Space X◦
239
3. But in general, X need not be isomorphic to X◦◦, even if X is complete. For
example, some elements of (α1)◦◦are not of the type x◦◦for any x √α1.
4. In this embedding, A ∃
⊆A⊥⊥(since for x √A and φ √A⊥, x◦◦φ = φx = 0, so
x◦◦√A⊥⊥). Note that A⊥⊥is always a closed linear subspace even if A isn’t.
Question: if M is a closed linear subspace is it necessarily true that M ⊆= M⊥⊥?
5. Since a functional is determined by its values on the unit sphere, we can think of
the double-functional x◦◦as a continuous function on the unit sphere in X◦; its
norm is none other than its maximum value there, ♦x◦◦♦= sup♦φ♦=1 |φx|. Hence
the vectors of any normed space can be thought of as continuous functions on a
(possibly inﬁnite-dimensional) sphere!
Exercises 11.26
1. X◦distinguishes points: If x ̸= y then there is a φ √X◦such that φx ̸= φy.
2. If x ̸√[[y]], ﬁnd a functional on X with φx = 1 and φy = 0.
3. For normed spaces, X◦= 0 ⇔X = 0.
4. Show that the functional φx := x, x √R, has many equal-norm extensions to R2
with the 1-norm.
5. The set { φ √X◦: φx = ♦x♦} (for a given x) is a non-empty convex subset of
X◦(called the set of “tangent functionals” at x)
6. Show that if { x }⊥= X◦then x = 0, and if { x }⊥= 0 then X ⊆= F or X = 0.
7. Show ⊥τ is a closed linear subspace of X.
8. (⊥τ)⊥need not equal [[τ]]. For example, take τ := { δn : n √N } in α1◦.
9. Let M be a closed subspace of a normed space X. The following maps are iso-
morphisms
M⊥∞(X/M)◦
X◦/M⊥∞M◦
φ ⇔∞β
φ + M⊥⇔∞φ|M.
β(x + M) := φx,
Hence, dim M⊥= codim M and codim M⊥= dim M, when ﬁnite.
11.4 The Adjoint T ≈
Recall the adjoint of an operator on Hilbert spaces T ◦: Y ∞X deﬁned by the
identity ∇T ◦y, x≤= ∇y, T x≤. Is there an analogous deﬁnition that can be applied
to Banach spaces? First, one needs to recast the deﬁning relation, replacing inner
products by functionals, (T ◦y)◦x = y◦T x. Although not exactly the same thing,
the deﬁnition (T ˙φ)x := φT x captures the essentials of this identity in terms of

240
11
Banach Spaces
functionals. The relation between them is T ◦: y ⇔∞y◦⇔∞T ˙y◦= a◦⇔∞a.
More formally, using the Riesz correspondences JY : Y ∞Y ◦and JX : X ∞X◦,
T ◦:= J −1
X T ˙JY .
X
Y
X
Y
T
T
J X
J Y
T
T ◦is sometimes called the Hilbert adjoint to distinguish it from the adjoint T ˙.
Deﬁnition 11.27
The adjoint1 of an operator T : X ∞Y is T ˙ : Y ◦∞X◦deﬁned by
(T ˙φ)x := φ(T x) for any φ √Y ◦and x √X.
That T ˙φ : X ∞F is linear and continuous can be seen from
T ˙φ(x + y) = φT (x + y) = φT x + φT y = T ˙φ(x) + T ˙φ(y)
T ˙φ(γx) = φT (γx) = γφT x = γT ˙φ(x)
|(T ˙φ)x| = |φ(T x)| ⩽♦φ♦♦T x♦⩽♦φ♦♦T ♦♦x♦.
(11.2)
Proposition 11.28
T ˙ is linear and continuous when T is, and the map T ⇔∞T ˙ is a linear
isometry from B(X, Y) into B(Y ◦, X◦),
(S + T )˙ = S˙ + T ˙,
(γT )˙ = γT ˙,
♦T ˙♦= ♦T ♦.
When deﬁned, (ST )˙ = T ˙S˙.
1 There is no standard name or notation for the adjoint operator. It has also been called the dual or
transpose and denoted by various symbols such as T →, T ◦, T ×, T ♯etc.

11.4 The Adjoint T ˙
241
Proof Linearity of T ˙: For all x √X, φ, β √Y ◦, γ √F,
T ˙(φ + β)(x) = (φ + β)(T x) = φT x + βT x = (T ˙φ)x + (T ˙β)x,
T ˙(γφ)x = γφ T x = (γT ˙φ)x.
That T ˙ is continuous follows from ♦T ˙φ♦⩽♦T ♦♦φ♦by (11.2).
The other assertions are implied by the following statements, true for all x √X
and all φ √Y ◦:
(S + T )˙φ x = φ(Sx + T x) = φSx + φT x = (S˙φ + T ˙φ)x,
(γT )˙φ x = φ(γT x) = γφT x = (γT ˙φ)x.
Using Proposition 11.19,
♦T ♦= sup
♦x♦=1
sup
♦φ♦=1
|φT x| = sup
♦φ♦=1
sup
♦x♦=1
|(T ˙φ)x| = sup
♦φ♦=1
♦T ˙φ♦= ♦T ˙♦.
Finally, when T √B(X, Y), S √B(Y, Z), and any β √Z◦,
(ST )˙β = βST = (S˙β)T = T ˙S˙β.
∀∈
Examples 11.29
1. 0˙ = 0, I ˙ = I.
2. The adjoint of a (complex) matrix is its transpose, with the columns becoming
the rows, φT x = y · T x = (T ˙ y) · x, e.g.
⎛y1
y2
⎜
·
⎛a b c
d e f
⎜⎭
⎧
x1
x2
x3
⎨
⎩=
⎭
⎧
a d
b e
c f
⎨
⎩
⎛y1
y2
⎜
·
⎭
⎧
x1
x2
x3
⎨
⎩,
∴
⎛a b c
d e f
⎜˙
=
⎭
⎧
a d
b e
c f
⎨
⎩,
and generally, ⎝
i yi
⎝
j Ti j x j = ⎝
j
 ⎝
i Ti j yi

x j, so T ˙
ji = Ti j.
3. ▶To ﬁnd the adjoint of an operator T on the sequence spaces α1, α2, or c0, the
effect of T on a vector x needs to reevaluated as an effect on a functional φ, which
recall is associated to a vector y in the dual spaces α⇒, α2, or α1, respectively,
φT x = y · T x = (T ˙ y) · x.
For example, to show that the adjoint of the operator T (an) := (a1, 0, 0, . . .) in
B(α1) is T ˙(bn) = (0, b0, 0, . . .) in B(α⇒), consider
y · T x = (b0, b1, b2, . . .) · (a1, 0, 0, . . .)
= b0a1 = (0, b0, 0, . . .) · (a0, a1, a2, . . .).

242
11
Banach Spaces
4. ▶The adjoint of the left-shift operator is the right-shift operator, on α1 or c0:
φLx = y · Lx =
⇒
⎞
n=0
bnan+1 = (0, b0, b1, . . .) · (a0, a1, a2, . . .) = (R y) · x.
5. The adjoint of the Fourier transform F : L1[0, 1] ∞c0 is F ˙ : α1 ∞L⇒[0, 1]
deﬁned by F ˙(an) = ⎝
n ane−2πinx. (Compare with F◦Exercise 10.35(14))
Proof For y = (an) √α1,
y · F f =
⎞
n
an
⎫
e−2πinx f (x) dx
=
⎫ ⎞
n
ane−2πinx
f (x) dx = (F ˙ y) · f
with the placement of the sum in the integral justiﬁed by ⎝
n ane−2πinx √
L⇒[0, 1].
* Note that α1 ∃c0, so the composition F ˙F is not deﬁned on all of L1[0, 1],
i.e., rebuilding an L1-function from its Fourier coefﬁcients is not guaranteed to
converge uniformly back to the function. However, with this machinery in place,
it is now easy to prove part of Dirichlet’s assertion for periodic functions:
F ˙F : C2[0, 1] ∞c2(Z) ∃α1 ∞L⇒[0, 1] (Exercise 9.27(3)).
6. * Even if the codomain of T : X ∞Y is reduced to a linear subspace M such
that im T ∗M ∗Y, the image of T ˙ remains the same.
Proof Let ⎬T : X ∞M, ⎬T x := T x, be the new operator; then ⎬T ˙ : M◦∞X◦.
Any functional φ √M◦can be extended to ˜φ √Y ◦, and for all x √X
(T ˙ ˜φ)x = ˜φT x = φ⎬T x = (⎬T ˙φ)x.
Hence im ⎬T ˙ ∗im T ˙. Conversely, any φ √Y ◦can be restricted to M, and the
same reasoning shows the opposite inclusion.
7. For a Hilbert space H, every operator T √B(H) is paired up with its adjoint
T ◦√B(H). This fact makes B(H) much more special than spaces of operators
on Banach spaces, as we shall see later in Chapter16 on C◦-algebras.
The Hilbert space fact ker T ◦= (im T )⊥generalizes to Banach spaces, but the
closure of im T ˙ is not always (ker T )⊥.

11.4 The Adjoint T ˙
243
Proposition 11.30
(Closed Range Theorem)
If X, Y are Banach spaces and T √B(X, Y), then
ker T ˙ = (im T )⊥,
ker T = ⊥im T ˙,
im T = ⊥ker T ˙,
im T ˙ ∗(ker T )⊥.
Moreover, im T ˙ = (ker T )⊥⇔im T is closed ⇔im T ˙ is closed.
Proof The central statement is, for T √B(X, Y),
φT x = (T ˙φ)x.
If these quantities vanish for all x √X, then the two sides of the equation state
φ √(im T )⊥and φ √ker T ˙, which must therefore be logically equivalent. If they
vanish for all φ √Y ◦, then they state x √ker T and x √⊥im T ˙ respectively.
We have already seen that τ ∗A⊥⇔A ∗⊥τ; so the statements in the second
line of the proposition follow from the identities in the top line, using ﬁrst τ =
ker T ˙, A = im T , and secondly A = ker T , τ = im T ˙. Moreover (Proposition
11.23),
im T = ⊥(im T ⊥) = ⊥(ker T ˙).
X
im T
T
φ
˜φ
im T
closed ⇔
im T ˙ closed: Suppose im T is
closed.Weshowthatequalityholdsinim T ˙ ∗(ker T )⊥.
Let φ √(ker T )⊥, i.e., T x = 0 ∪φx = 0. T can be
considered as an onto operator T : X ∞im T , so the
mapping ˜φ : T x ⇔∞φx is a well-deﬁned functional on
im T (Exercise 11.7(4)). It can be extended to a functional
β √Y ◦by the Hahn-Banach theorem.
Then, for all x √X,
φx = ˜φT x = βT x = (T ˙β)x,
so φ = T ˙β and im T ˙ is equal to the closed subspace (ker T )⊥.
Conversely, let im T ˙ be closed, and deﬁne ⎬T : X ∞im T =: M, ⎬T x := T x;
by Example 11.29(6) above and the fact that the annihilator of im ⎬T in M is 0, it
follows that ⎬T ˙ is 1–1 and has the closed image im T ˙. Hence, for all φ √M◦,
♦⎬T ˙φ♦∴c♦φ♦(Proposition 11.3). Now C := ⎬T BX is a closed balanced convex
subset of Y, so by the separating hyperplane theorem, any y ̸√C can be separated
from it by means of a functional β √Y ◦,

244
11
Banach Spaces
⊂x √B1[0],
|β⎬T x| ⩽r < |βy|.
Note that ♦⎬T ˙β♦⩽r. Then
r < |βy| ⩽♦β♦♦y♦⩽1
c ♦⎬T ˙β♦♦y♦⩽r
c♦y♦
and ♦y♦> c. This implies that ⎬T BX contains the ball Bc(0). But we have already
seen in the proof of the open mapping theorem that when this is the case, then ⎬T BX
contains some open ball B∂(0) of M. This can only be true if ⎬T is onto, that is,
im ⎬T = im T is equal to the closed space M.
∀∈
Proposition 11.31 Schauder’s Theorem
If T is compact then so is its adjoint T ˙.
Proof Let T : X ∞Y be a compact operator, so T BX is totally bounded in Y, that
is, for arbitrarily small ∂> 0, it can be covered by a ﬁnite number of balls B∂(T xi)
where x1, . . . , xN √BX. We want to show that T ˙ maps the unit ball of functionals
BY ◦∃Y ◦to a totally bounded set of functionals in X◦.
The linear map S : Y ◦∞FN deﬁned by Sβ := (βT x1, . . . , βT xN) is continu-
ous (because T is, and N is ﬁnite), so compact of ﬁnite-rank. Hence SBY ◦is totally
bounded in FN and can be covered by balls B∂(Sβ j) for a ﬁnite number of β j √BY ◦.
X
Y
X
Y
T
T
B X
x i
T x i
T B X
T B Y
j
B Y

11.4 The Adjoint T ˙
245
We now show that balls of radius 4∂centered at T ˙β j cover T ˙BY ◦. For any
β √BY ◦and any x √BX, there are T xi and Sβ j close to T x and Sβ respectively,
resulting in
|βT x −β jT x| ⩽|βT x −βT xi| + |βT xi −β jT xi| + |β jT xi −β jT x|
⩽♦β♦♦T x −T xi♦+ ♦Sβ −Sβ j♦Fn + ♦β j♦♦T xi −T x♦
< ♦β♦∂+ ∂+ ♦β j♦∂
< 3∂
So ♦T ˙β −T ˙β j♦⩽3∂, and T ˙BY ◦∗
j B4∂(T ˙β j).
∀∈
Exercises 11.32
1. The adjoint of a multiplier operator My(x) := yx, where My √B(α1), is
My √B(α⇒).
2. The adjoint of a ﬁnite-rank operator T x := ⎝N
n=1(φx)en is another ﬁnite-rank
operator T ˙β = ⎝N
n=1 (βen)φn.
3. The adjoint is continuous: If Tn ∞T then T ˙
n ∞T ˙.
4. T maps a linear subspace M onto T M; show T ˙ maps (T M)⊥into M⊥. So, if
M is T -invariant, i.e., T M ∗M, then M⊥is T ˙-invariant.
5. * In the embedding of X in X◦◦, show that T ˙˙ : X◦◦∞Y ◦◦is an extension
of T : X ∞Y in the sense that T ˙˙x◦◦= (T x)◦◦.
6. T ˙ is 1–1 ⇔im T is dense in Y; and im T ˙ is dense in X◦∪T is 1–1.
7. Let T √B(X, Y),
T is an isomorphism ⇔T ˙ is an isomorphism, with (T ˙)−1 = (T −1)˙,
⇔T and T ˙are onto.
8. A necessary condition for the equation T x = y to have a solution in x is that y
have the property T ˙φ = 0 ∪φy = 0. When is it also sufﬁcient?
9. If P is a projection, then so is P˙, with kernel (im P)⊥and image (ker P)⊥.
Deduce
X = M ∩N ∪X◦= N ⊥∩M⊥.
10. If T is a Fredholm operator (Deﬁnition 11.12), then so is T ˙ and its index
is index(T ˙) = −index(T ). Moreover, index(T ) = dim ker T −dim ker T ˙.
(Hint: Exercise 11.26(9).)

246
11
Banach Spaces
11.5 Pointwise and Weak Convergence
We have already encountered two types of convergence for operators Tn √B(X, Y),
to which can be added yet another, weaker, type:
(i) Convergence in norm
Tn ∞T
⇔
♦Tn −T ♦∞0,
(ii) Strong, or pointwise, convergence
Tnx ∞T x
⊂x √X
⇔
♦Tnx −T x♦Y ∞0 ⊂x √X.
(iii) Weak convergence
Tn ⇀T
⇔
φTnx ∞φT x
⊂x √X,
⊂φ √Y ◦.
Examples 11.33
1. ▶Convergence in norm is “stronger” than pointwise convergence, since for each
x √X,
♦Tnx −T x♦= ♦(Tn −T )x♦⩽♦Tn −T ♦♦x♦∞0.
But the converse is false: it is possible to have pointwise convergence without
convergence in norm. For example, let δN : α1 ∞C be deﬁned by δN(an) := aN;
then δN x ∞0 as N ∞⇒for each x √α1, but ♦δN♦= 1.
Similarly, when deﬁned on c, δN converge pointwise to Lim, since δN(an) =
aN ∞limn∞⇒an yet δN ̸∞Lim (because δN = e˙
N can converge only if eN
converge in α1).
2. Another example is the projection operator deﬁned as n left shifts followed by
n right shifts, Tn := RnLn : α1 ∞α1. It converges pointwise to the 0 operator,
since for each x = (ai) √α1, ♦RnLnx♦= ⎝⇒
i=n |ai| ∞0. However there are
sequences, such as x := en, for which Tnx = x, so that ♦Tn♦= 1 ̸∞0.
3. If Tn converge pointwise, Tnx ∞T x, ⊂x, it does not follow that T ˙
n converge
pointwise, T ˙
n φ ∞T ˙φ, ⊂φ. For example, in α1, Lnx ∞0 for the left-shift
operator L; but Rnx ̸∞0 in α⇒. Another example is Tn(ai) := (an, 0, 0, . . .).
It often happens that a map is deﬁned as the pointwise limit of a sequence of
operators, T (x) := limn∞⇒Tnx, assuming this is deﬁned for all x √X. It is then
natural to ask what properties does T enjoy: That it is linear is easy to prove, but
is it also necessarily continuous? The answer is yes when X is a Banach space, as
follows from the following stronger assertion:

11.5 Pointwise and Weak Convergence
247
Theorem 11.34
Banach-Steinhaus’s Uniform Bounded theorem
For a Banach space X and Ti √B(X, Y),
(⊂x √X, ⇐Cx > 0, ⊂i, ♦Ti x♦⩽Cx) ∪⊂i, ♦Ti♦⩽C.
(The index set of i need not be countable.)
Proof The sets Ac := { x √X : ⊂i, ♦Ti x♦⩽c } are closed, since if xm √Ac
and xm ∞x, then taking the limit m ∞⇒in the inequality ♦Ti xm♦⩽c, we ﬁnd
♦Ti x♦⩽c, by continuity of Ti and the norm, showing x √Ac.
The given hypothesis is that X = ⇒
k=1 Ak. By Baire’s category theorem, not all
these sets can be nowhere dense. That is, there must be at least one N for which AN
contains a ball Br(a), in fact Br[a] since AN is closed,
⊂y √X,
♦y −a♦⩽r ∪⊂i, ♦Ti y♦⩽N.
Thus
♦y −a♦⩽r ∪♦Ti(y −a)♦⩽♦Ti y♦+ ♦Tia♦⩽2N,
which can be rewritten as ♦y−a
r ♦⩽1 ∪♦Ti( y−a
r )♦⩽2N/r. But every vector x
can be written in this form
x
♦x♦= y−a
r
for a suitable y, so for all i,
♦Ti x♦⩽2N
r ♦x♦.
∀∈
Corollary 11.35
If Tn √B(X, Y) with X a Banach space, and Tnx ∞T (x) for all x, then T
is linear and continuous,
♦T ♦⩽lim inf
n
♦Tn♦.
Proof T is necessarily linear, by continuity of addition and scalar multiplication
(see the proof of Theorem 8.7). Any convergent sequence is bounded, so ♦Tnx♦is
bounded for each x, from which follows that ⊂n, ♦Tn♦⩽C, by the uniform bounded
theorem.
If we now choose a subsequence of Tn, for which ♦Tn♦∞ξ := lim infn ♦Tn♦,
and take the limit n ∞⇒of ♦Tnx♦⩽♦Tn♦♦x♦, we get ♦T x♦⩽ξ♦x♦and ♦T ♦⩽ξ.
∀∈

248
11
Banach Spaces
Examples 11.36
1. The uniform bounded theorem can be restated as: If ♦Tn♦∞⇒then there must
be a vector x such that ♦Tnx♦∞⇒.
* In fact the set of such x is dense in X: If any Ak of the proof contains a ball, the
conclusion of the theorem would hold; so with all Ak nowhere dense in X, the
complement of 
k Ak is dense (Remark 4.22(1)).
2. A common error is to deﬁne or prove T x = ⎝
n Tnx for all x and then deduce
T = ⎝
n Tn. It is true that two functions are the same, f = g, when f (x) = g(x)
for all x √X, but the point is that the meaning of the limit in the sum ⎝
n differs
in the two expressions, the ﬁrst occurring in Y and the second in B(X, Y).
3. * Let SN f := ⎝N
n=−N ⎠f (n)e2πinx, where x √[0, 1] is ﬁxed and f √C[0, 1].
Show (a) SN is a functional on C[0, 1], (b) SN f = DN ◦f , where
DN(x) :=
N
⎞
n=−N
e2πinx = sin(2N + 1)πx
sin πx
is called the Dirichlet kernel, and (c) ♦SN♦= ♦DN♦L1. Assuming one can show
that
 1
0 |Dn(x)| dx ∞⇒, use the uniform boundedness theorem to deduce that
there is a dense set of continuous functions f for which the Fourier series does
not converge at x, SN f ∞⇒in C[0, 1].
Weak Convergence
Let us now consider weak convergence of operators
Tn ⇀T
⇔
φTnx ∞φT x
⊂x √X, ⊂φ √Y ◦.
For vectors (considered as operators F ∞X, γ ⇔∞γx), weak convergence takes the
form
xn ⇀x
⇔
φxn ∞φx,
⊂φ √X◦.
For functionals (X ∞F), this convergence is called weak* convergence and coin-
cides with their pointwise convergence,
φn ⇀φ
⇔
φnx ∞φx,
⊂x √X.
One must guard against a possible source of confusion: the weak convergence of
functionals, when thought of as vectors in X◦, is different
φn ⇁φ
⇔
φn ∞φ,
⊂ √X◦◦,

11.5 Pointwise and Weak Convergence
249
hence the need for a new name.
Examples 11.37
1. Strong convergence implies weak convergence because, by continuity of φ,
Tnx ∞T x ∪φTnx ∞φT x.
2. ▶But the converse is false in general: For example, in c0, Rn ⇀0, since for any
x = (ai) √c0, andy = (bi) √α1 ≡c◦
0,
|y · Rnx| =

⇒
⎞
i=0
bi+nai
 ⩽
⇒
⎞
i=n
|bi|♦x♦∞0 as n ∞⇒,
yet Rnx ̸∞0,
♦Rnx♦= ♦(0, . . . , 0, a0, a1, . . .)♦c0 = ♦x♦̸∞0.
3. To prove weak convergence, xn ⇀x, given that (xn) is bounded in X, it is enough
to check βxn ∞βx for β in a dense subset of X◦.
Proof Any φ √X◦can be approximated by functionals βn ∞φ, by their density
in X◦. For yn := xn −x (bounded), it is not hard to show that βnyn ∞0, so
φyn = βnyn + (φ −βn)yn ∞0 as n ∞⇒.
4. Weak convergence of vectors and operators in an inner product space become
xn ⇀x ⇔∇y, xn≤∞∇y, x≤as n ∞⇒,
⊂y √X,
Tn ⇀T ⇔∇y, Tnx≤∞∇y, T x≤as n ∞⇒,
⊂x, y √X.
5. In an inner product space,
xn ⇀x and ♦xn♦∞♦x♦⇔xn ∞x.
Proof When xn ⇀x, we get ∇x, xn≤∞∇x, x≤since x◦is a functional, so
♦x −xn♦2 = ♦x♦2 −2 Re ∇x, xn≤+ ♦xn♦2 ∞0.
Proposition 11.38
In ﬁnite dimensions, all three convergence types are equivalent.

250
11
Banach Spaces
Proof Let An ⇀A where An, A are M × N matrices. This means that for any
φ √(FM)◦and x √FN, φ(An −A)x ∞0 as n ∞⇒. In particular if we let
φ = ˜e˙
i , x = e j be basis vectors for FM◦and FN respectively, then each component
of An converges to the corresponding component in A:
An,i j = ˜e˙
i Ane j ∞˜e˙
i Ae j = Ai j, as n ∞⇒.
This then implies that ♦An −A♦⩽
⎝N
j=1
⎝M
i=1 |An,i j −Ai j|2 ∞0 (Example
8.9(2)).
∀∈
The analogous result of the Banach-Steinhaus theorem for weak convergence is
also true, but more care is needed: Although every convergent sequence is bounded
(Example4.3(5)), that fact was proved using a metric, whereas weak convergence
Tn ⇀T is not equivalent, in general, to such a strong type of convergence as
d(Tn, T ) ∞0 for any distance function.
Proposition 11.39
If Tn ⇀T where Tn √B(X, Y), X a Banach space, then
(i) { Tn : n √N } is bounded, and
(ii) T √B(X, Y) with ♦T ♦⩽lim inf ♦Tn♦.
Proof (i) Let Tn ⇀T ; the set { T1x, T2x, . . . } is weakly bounded in the sense that
for all n √N, φ √X◦, |φTnx| ⩽Cφ,x, since (φTnx) is a convergent sequence
in C. But an application of the uniform bounded theorem twice shows ﬁrst that
♦Tnx♦⩽Cx, and then that Tn is bounded. Of course, a simpliﬁed version of this
argument applies equally well to weakly convergent sequences of vectors xn ⇀x
and to weak*-convergent sequences of functionals φn ⇀φ.
(ii) Take the limit of φTn(x + y) = φTnx + φTny and φTn(γx) = γφTnx to show
linearity of T . Similarly, the set { ♦Tn♦: n √N } is bounded in R and possesses a
smallest limit point ξ, so taking a subsequence of ♦Tn♦which converges to it, we
obtain
|φTnx| ⩽♦φ♦♦Tn♦♦x♦
⊂x √X, φ √Y ◦
↓
↓
|φT x|
ξ♦φ♦♦x♦
and ♦T ♦⩽ξ follows. Thus B(X, Y) is closed under weak convergence.
∀∈
As a partial converse there is

11.5 Pointwise and Weak Convergence
251
Theorem 11.40
When X is a separable Banach space, every bounded sequence in X◦has
a weak*-convergent subsequence.
If x1, x2, . . . √X are dense in the unit ball, then X◦has a norm
♦φ♦w :=
⇒
⎞
n=1
1
2n |φxn| ⩽♦φ♦
such that for φn bounded,
φn ⇀φ ⇔♦φn −φ♦w ∞0.
Thus the unit closed ball of X◦is a compact metric space with this norm.
This theorem can be generalized to non-separable spaces (see [10]), when it is
known as the Banach-Alaoglu theorem: The unit closed ball of X◦is a compact
topological space.
Proof (i) Let { xm } be a countable dense subset of X, and suppose ♦φn♦⩽c.
Then the sequence of complex numbers φnx1 is bounded, |φnx1| ⩽c♦x1♦, and so
must have a convergent subsequence (Exercise 6.9(6)), which we shall denote by
φ1,nx1 ∞β(x1). This subsequence is also bounded on x2, |φ1,nx2| ⩽c♦x2♦, and so
we can extract, by the same means, a convergent sub-subsequence, φ2,nx2. Notice
that, not only does φ2,nx2 ∞β(x2) but also φ2,nx1 ∞β(x1). Continuing this
way, we get subsequences φm,n and numbers β(xm) such that φm,nxi ∞β(xi), for
i ⩽m, and |β(xm)| ⩽c♦xm♦.
φn φ1 φ2 φ3 φ4 φ5 . . .
φ1,n φ1
φ3 φ4 φ5 . . . φ1,nx1 ∞β(x1)
φ2,n φ1
φ3
φ5 . . . φ2,nx2 ∞β(x2)
φ3,n
φ5 . . . φ3,nx3 ∞β(x3)
. . .
φk,k φ1
φ3
. . . φk,kxm ∞β(xm)
Let βk := φk,k, a subsequence of the original sequence φn. In fact, βk is a
subsequence of every φm,n from some point onward (k ∴m), so βkxm ∞β(xm),
as k ∞⇒. This implies that the function β is Lipschitz on the dense set { xm },
|β(xi) −β(x j)| = lim
k∞⇒|βkxi −βkx j)| = lim
k∞⇒|φk,k(xi −x j)| ⩽c♦xi −x j♦

252
11
Banach Spaces
and so can be extended uniformly to a continuous function on X (Theorem 4.13),
and still satisfying |β(x)| ⩽c♦x♦. It is linear, as seen by taking the limit k ∞⇒of
βk(x + y) = βkx + βk y and βk(γx) = γβkx.
Now, for any ∂> 0, there is an xm close to x, ♦xm −x♦< ∂, so that
⇐K √N, k ∴K ∪|βkxm −βxm| < ∂
∪
|βkx −βx| ⩽|βkx −βkxm| + |βkxm −βxm| + |βxm −βx|
⩽(2c + 1)∂,
in other words βkx ∞βx for all x, or βk ⇀β, as k ∞⇒.
(ii) That ♦φ♦w is well-deﬁned and bounded by ♦φ♦follows from |φxn| ⩽♦φ♦♦xn♦⩽
♦φ♦; that it is a norm follows from |φxn + βxn| ⩽|φxn| + |βxn| and |γφxn| =
|γ||φxn|, as well as
0 = ♦φ♦w =
⇒
⎞
n=1
1
2n |φxn| ⇔⊂n, |φxn| = 0 ⇔φ = 0
since { xn } is dense in BX.
(iii) When ♦φn♦⩽c, φn ⇀φ ⇔♦φn −φ♦w ∞0: It is enough to consider
functionals φn such that φn ⇀0. Let ∂> 0 and M large enough that 1/2M < ∂. For
all m, φnxm ∞0 as n ∞⇒; this convergence may not be uniform in m, but it will
be for the ﬁrst M points x1, . . . , xM, i.e.,
⇐N, n ∴N ∪|φnxm| < ∂, ⊂m = 1, . . . , M.
So ♦φn♦w ∞0, because for n ∴N,
♦φn♦w =
⇒
⎞
m=1
1
2m |φnxm| <
M
⎞
m=1
1
2m ∂+
⇒
⎞
m=M+1
1
2m ♦φn♦♦xm♦< (1 + c)∂.
Conversely, let φn be bounded functionals such that ♦φn♦w ∞0. This implies
that for any ﬁxed m,
1
2m |φnxm| ⩽
⇒
⎞
m=1
1
2m |φnxm| ∞0, as n ∞⇒,
(11.3)
so φnxm ∞0. For any x √X, choose xm close to within ∂of y := x/♦x♦. This is
possible because { xm } are dense in the unit ball. Then, for n large enough,
|φny| ⩽|φnxm| + |φn(xm −y)| ⩽∂+ c∂
∪
|φnx| ⩽(1 + c)♦x♦∂

11.5 Pointwise and Weak Convergence
253
Hence φnx ∞0 for any x and so φn ⇀0.
(iv) B X◦is compact with respect to ♦·♦w: Every sequence φn in B1[0] has a weak*-
convergent subsequence by (i), i.e., ♦φn −φ♦w ∞0. For any x √X,
|φx| = lim
n∞⇒|φnx| ⩽♦φn♦♦x♦⩽♦x♦,
so ♦φ♦⩽1, and B1[0] has the Bolzano-Weierstraß property of compactness (Theo-
rem 6.21). Note carefully, however, that it is not necessarily compact in the standard
norm of X◦; only in ﬁnite-dimensions are balls totally bounded (Proposition 8.23).
∀∈
Examples 11.41
1. In a Banach space, if xn ⇀x and { xn : n √N } is totally bounded, then xn ∞x.
Proof Every totally bounded set has a Cauchy subsequence, so xni ∞y √X.
By continuity, φxni ∞φy = φx for any functional φ, hence x = y. If xn ̸∞x,
then one could ﬁnd another Cauchy subsequence converging to y ̸= x, which is
impossible.
2. A subset A ∗X is said to be weakly bounded when ⊂φ √X◦, φA is bounded. It
turns out that A is weakly bounded ⇔A is bounded.
Proof Given that |φa| ⩽Rφ for all a √A and φ √X◦, recall that φa = a◦◦φ,
where a◦◦: X◦∞F, so the uniform bounded theorem can be used to yield
♦a♦= ♦a◦◦♦⩽C.
The idea of using functionals to transfer sets in X to sets in F is so convenient and
useful that it is applied, not just to convergence, but to various other properties.
In a general sense, we say that a set A ∗X is weakly P when for all φ √X◦, φA
has the property P.
3. A vector x is a weak limit point of a subset A when for any φ √X◦, every open
ball in F which contains φx also contains another point φa for a √A, a ̸= x. A is
said to be weakly closed when it contains all its weak limit points. Every weakly
closed set is closed, since xn ∞x ∪xn ⇀x.
4. If T islinearandφT iscontinuousforeachφ √Y ◦(i.e., xn ∞x ∪T xn ⇀T x),
then in fact T is continuous.
Proof For every bounded set B, φT B is bounded by continuity. So T B is weakly
bounded, which is the same as bounded.
5. A Hilbert space is weakly complete, i.e., if (φxn) is Cauchy in F for each φ √H◦,
then xn ⇀x for some x.
Proof Let φ(y) := limn∞⇒∇xn, y≤; φ is linear and continuous by the uniform
bounded theorem, so must be of the form φ = ∇x, ·≤and ∇xn, y≤∞∇x, y≤for
each y.

254
11
Banach Spaces
6. * Closed and bounded sets of a Hilbert space are weakly sequentially compact,
meaning any bounded sequence has a weakly convergent subsequence.
Proof Let M := [[x1, x2, . . . ]]. Then M◦≈M, so Theorem 11.40 can be used
to conclude that there is a subsequence xni that converges weakly in M, i.e.,
∇a, xni ≤∞∇a, x≤for all a √M. But in fact, for any vector y √H and its
orthogonal projection Py √M, ∇y, xn≤= ∇Py, xn≤∞∇Py, x≤= ∇y, x≤.
7. If xn ⇀x ∪T xn ∞T x, then T is compact.
Proof If B is a bounded set and T xn any sequence in T B, then xn √B has
a weakly convergent subsequence by the note above; by hypothesis its image
converges, T xni ∞T x, and is thus a Cauchy sequence in T B. T B is therefore
totally bounded.
8. * The “Least Distance Theorem” 10.11 can be generalized to when M is weakly
closed. (Note that closed convex subsets are weakly closed.)
Proof The sequence (yn) of the theorem is bounded, hence has a weakly conver-
gent subsequence yni ⇀y◦√M. Moreover ♦yn −x♦∞d. Taking the limit of
|∇yni −x, y◦−x≤| ⩽♦yni −x♦♦y◦−x♦gives ♦y◦−x♦⩽d.
Exercises 11.42
1. Show en ⇀0 in c0 or α2, yet en ̸∞0.
2. (a) For α1, α2, and α⇒, if xn = (ani) ⇀x = (ai) then each component
converges ani ∞ai as n ∞⇒. But the converse is false; e.g. en ̸⇀0 in
α1.
(b) For α2, xn ⇀x if, and only if, xn are bounded and each component con-
verges, ani ∞ai. (Hint: approximate any φ by ⎝N
i=1 bi e˙
i .) Can you gen-
eralize this to αp (1 < p < ⇒)?
3. In L1[0, 1], the functions fn(x) := e2πinx converge weakly fn ⇀0, but not
pointwise fn(x) ̸∞0 at any x (see Theorem 9.25).
4. ▶The weak limit of Tn, if it exists, is unique. A subsequence of Tn also converges
to the same weak limit.
5. ▶If xn ⇀x then T xn ⇀T x, for T √B(X, Y).
6. Show that the norm is not continuous with respect to weak convergence, by
ﬁnding a sequence in c0 such that xn ⇀x yet ♦xn♦̸∞♦x♦. Similarly, the inner
product of a Hilbert space is not weakly continuous: xn ⇀x, yn ⇀y do not
imply ∇xn, yn≤∞∇x, y≤.
7. In a Hilbert space with an orthonormal basis en,
(a) en ⇀0,
(b) ⎝
n ξnen ⇀x ⇔⎝
n ξnen ∞x.
(Hint:Theseriesisbounded,byProposition11.39,i.e.,♦ξ1e1 + · · · + ξnen♦2
⩽c and so (ξn) √α2; or use Example 11.37(5).)

11.5 Pointwise and Weak Convergence
255
8. φn ⇁φ ∪φn ⇀φ.
9. * Schur’s theorem: In α1, weak convergence of xn is the same as convergence in
norm. Prove this as follows:
(a) If the statement were false there would be unit xn = (ani) √α1 such that
xn ⇀0.
(b) For each n there is an Nn such that ⎝Nn
i=1 |ani| > 4
5.
(c) Each coefﬁcient converges to 0 as n ∞⇒, so
⊂k, ⇐M, n ∴M ∪
⎞
i<k
|ani| < 1
5.
(d) A subsequence of (xn) exists with
⎞
i<Nn−1
|ani| < 1
5,
Nn
⎞
i=Nn−1
|ani| > 3
5,
⎞
i>Nn
|ani| < 1
5.
(e) Let y := (|ani|/ani) √α⇒where for each i, n is such that Nn−1 ⩽i < Nn.
Show |y · xn| ∴1
5 to obtain a contradiction.
10. Addition and scalar multiplication are continuous with respect to weak
convergence, that is, if Tn ⇀T and Sn ⇀S then Tn + Sn ⇀T + S, and
γTn ⇀γT . Of course, they are also continuous with respect to norm-wise and
strong convergence.
11. Multiplication of operators is not continuous with respect to weak convergence.
The most that can be said is
(a) if Tn ⇀T then TnS ⇀T S and STn ⇀ST ,
(b) if Tn ⇀T and Snx ∞Sx for all x, then TnSn ⇀T S,
(c) if ⊂φ √X◦, φSn ∞φS and Tn ⇀T then SnTn ⇀ST .
12. (a) For Banach spaces, if T ˙
n ⇀T ˙ then Tn ⇀T (but not conversely).
(b) For Hilbert spaces, if Tn ⇀T then T ◦
n ⇀T ◦(weakly continuous).
13. If T is compact then xn ⇀x ∪T xn ∞T x (Hint: { xn } must be bounded.)
14. If xn ⇀x in X, then φ ⇔∞(φxn) maps X◦into c. For example, when X is α1,
this map converts bounded sequences to convergent ones.
15. Every closed linear subspace is weakly closed (by Proposition 11.18). Thus, if
xn ⇀x, then there is a sequence yn √[[x1, x2, . . . ]] which converges in norm,
yn ∞x.
16. A set in X◦is weak*-closed when it contains all weak*-limit points; for example,
A⊥.

256
11
Banach Spaces
17. The strong limit of unitary isomorphisms Un between two Hilbert spaces is an
isometry U. But U need not be unitary; e.g. let Un be deﬁned on α2 by
Un(a1, a2, . . .) := (an+1, a1, a2, . . . , an, an+2, an+3, . . .).
Then Un converges strongly to the right-shift operator R.
18. The Hadamard matrices are deﬁned recursively by T1 :=
⎛1 1
1 −1
⎜
, Tn+1 :=
⎛Tn Tn
Tn −Tn
⎜
. Sn := Tn/2n/2 are 2n × 2n unitary matrices; they can be extended
to unitary operators on α2 by Unx := Snx when x √Mn := [[e0, . . . , e2n−1]],
and Unx := x when x √M⊥
n , and then Un ⇀0.
19. If a sequence of unitary isomorphisms Un converges weakly to U, then ♦U♦⩽1.
If U is known to be unitary, then the convergence is pointwise. (Hint: expand
♦Unx −Ux♦2.)
Remarks 11.43
1. Not every closed subspace of a Banach space need be “complemented”, e.g. the
space α⇏= c0 ∩M for any closed linear subspace M (see Proposition 9.2 for the
deﬁnition of c0) (see [38]). Indeed there exist inﬁnite-dimensional Banach spaces
whose only complemented subspaces are the ﬁnite-dimensional or codimensional
closed ones [42].
2. It is a theorem that Hilbert spaces are the only Banach spaces in which every
closed subspace is complemented [40].
3. Weak convergence does not obey all the convergence properties of metric spaces.
For example, not every weak limit point of a set M need have a sequence in M
that converges weakly to it.
4. There are yet other types of convergence. For example, B(X, Y) is itself a Ba-
nach space, and so there is weak convergence with respect to B(X, Y)◦, meaning
τTn ∞τT for all τ √B(X, Y)◦.

Chapter 12
Differentiation and Integration
12.1 Differentiation
Although continuous linear transformations are stressed throughout the book—with
good reason, for they are the morphisms of normed spaces—they represent, of course,
a very special part of all the functions from one normed space to another. To put things
in perspective, recall that the linear maps on R are x ∞√∂x, a very restricted set
of functions in comparison with the non-linear real continuous functions. However,
the linear maps are still relevant for one class of continuous functions: maps that
are ‘locally linear’, meaning that they can be approximated by linear operators up to
second-order errors:
Deﬁnition 12.1
A function f : X √Y between normed spaces (over the same ﬁeld) is said
to be (Fréchet) differentiable at x when there is a continuous linear map
f ∗(x) ⇒B(X, Y) such that for h in a neighborhood of 0,
f (x + h) = f (x) + f ∗(x)h + o(h)
where →o(h)→/→h→√0 as h √0.
Note that f need not be deﬁned on all of X but only on a neighborhood of x. The
set of functions f : U ♦X √Y, where U is an open subset of a normed space X
and f is differentiable at all points x ⇒U, is here denoted D(U, Y).
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_12,
257
© Springer International Publishing Switzerland 2014

258
12
Differentiation and Integration
Proposition 12.2
The set of differentiable functions D(U, Y) forms a vector space.
Differentiation D : f ∞√f ∗is a linear map, which takes composition of
functions to operator products,
( f + g)∗= f ∗+ g∗,
(∂f )∗= ∂f ∗,
( f ∃g)∗(x) = f ∗(g(x))g∗(x).
Note that the domain of D is D(U, Y) and its codomain is the vector space
of functions { g : U √B(X, Y) }. The last identity is called the chain rule of
differentiation.
Proof The statements follow from the following identities and inequalities:
( f + g)(x + h) = f (x + h) + g(x + h)
= f (x) + f ∗(x)h + o f (h) + g(x) + g∗(x)h + og(h)
= f (x) + g(x) + ( f ∗+ g∗)(x)h + (o f (h) + og(h))
∂f (x + h) = ∂f (x) + ∂f ∗(x)h + ∂o(h)
f ∃g(x + h) = f

g(x + h)

= f

g(x) + g∗(x)h + og(h)

= f

g(x)

+ f ∗
g(x)

g∗(x)h + og(h)

+ o f (h)
= f

g(x)

+ f ∗
g(x)

g∗(x)h + ( f ∗(g(x))og(h) + o f (h))
→o f (h) + og(h)→⩽→o f (h)→+ →og(h)→,
→∂o(h)→= |∂|→o(h)→,
→T og(h) + o f (h)→⩽→T →→og(h)→+ →o f (h)→,
for any T ⇒B(X, Y).
∀∈
Examples 12.3
1. The constant functions f (x) := y0 are differentiable with f ∗= 0.
2. In R or C, the functions f (x) := xn are differentiable with
f (x + h) = (x + h)n = xn + nxn−1h + o(h),
so f ∗(x) = nxn−1. Polynomials are thus differentiable.
3. Continuous linear maps are differentiable, T (x +h) = T x + T h, so T ∗(x) = T .
A special case of the composition law is (T ∃f )∗= T ∃f ∗when T is a ﬁxed
operator.

12.1 Differentiation
259
4. The derivative of F : R √R2, F(t) :=
 f (t)
g(t)

= f (t)
1
0

+g(t)
0
1

is F∗(t) =
 f ∗(t)
g∗(t)

. A differentiable path r:R √X is called a curve. The direction of its
derivativer∗is called its tangent. The arclength of a curve is

r ds :=

|r∗(t)| dt.
5. Deﬁne f : R2 √R by f (x, y): = x2 −y. Then f ∗(x, y) : R2 √R is its
gradient f ∗(x, y) = (2x, −1) since
f (x + h, y + k) = (x + h)2 −(y + k) = (x2 −y) +
2x −1  h
k

+ h2.
The map (h, k) ∞√(x2
0 −y0) + 2x0h −k gives the tangent plane to the surface
z = f (x, y) at the point (x0, y0, z0).
6. A real inner product ⊆·, ·⟩: X2 √R is differentiable,
⊆x + h, y + k⟩= ⊆x, y⟩+ (⊆x, k⟩+ ⊆h, y⟩) + ⊆h, k⟩.
The middle term is linear in (h, k), and the last term is o(h, k) by the Cauchy-
Schwarz inequality,
|⊆h, k⟩|
→h→+ →k→⩽
→h→→k→
→h→+ →k→⩽→h→√0 as (h, k) √(0, 0).
7. We often write Dv f (x) := f ∗(x)v. Note that
Dv+w f = Dv f + Dw f,
D∂v f = ∂Dv f.
Because of this last property, v is usually taken to be a unit vector.
When X = R there are only two unit vectors, v = ±1, and the notation used is
d
dx := D1 for the derivative in the positive direction. Similarly, for C, d
dz := D1.
In RN, the standard basis consists of N unit vectors en, and we deﬁne δn := Den.
8. For X = R, the derivative can be taken to be a function f ∗: R √Y, since
B(R, Y) ⇐Y.
9. ▶Differentiable functions are continuous in x, in fact are Lipschitz in a neigh-
borhood of any point
→f (y) −f (x)→= →f ∗(x)(y −x) + o(y −x)→⩽c→y −x→.
In particular, f (y) √f (x) as y √x. But there are Lipschitz functions, such
as x ∞√|x| on R, that are not differentiable.
10. * The set of functions f ⇒C(R) with bounded continuous derivatives, f ∗⇒
C(R), is denoted by C1(R). It is a non-closed linear subspace of C(R). However,
it can be given a complete norm
→f →C1 := →f →C + →f ∗→C.

260
12
Differentiation and Integration
Differentiation is then an operator C1(R) √C(R).
Proof The functions sin nx have unit norms in C(R), but their derivatives n cos nx
have arbitrarily large ⊂-norm. This allows us to deﬁne
f (x) :=
⊂

n=1
1
2n sin 4nx
with the partial sums fN converging in C(R) (the series is absolutely convergent). But
this is an example of a nowhere-differentiable function (check it is not differentiable
at 0 at least), so although fN ⇒C1(R) and fN √f uniformly, f ⇔⇒C1(R).
C1(R) is complete: if ( fn) is a Cauchy sequence in C1(R), then ( fn) and ( f ∗
n) are
Cauchy sequences in the complete space C(R), so they converge to f, g ⇒C(R). By
Proposition 9.18, f ∗= g and fn √f in C1(R).
That differentiation is continuous is trivial for this space:
→Df →C = →f ∗→C ⩽→f →C + →f ∗→C = →f →C1.
It is not continuous when C1(R) is considered as a subspace of C(R).
Proposition 12.4
The kernel of D on D(X, Y) consists of the constant functions,
Df = 0 =∩f is constant.
Proof Weﬁrstidentifythekernelwhenthedifferentiablefunctionsarereal,g: R√R.
Suppose g∗(t) = 0 for all t ⇒[a, b], and let
G(t) := g(t) −(t −a)g(b) + (b −t)g(a)
b −a
also differentiable, with G(a) = 0 = G(b), and
G(t +h)−G(t) = G∗(t)h +o(h) = −g(b) −g(a)
b −a
h +o(h),
t ⇒]a, b[. (12.1)
G is continuous on the compact set [a, b], so it must have maximum and minimum
points. We can assume one of them to be inside ]a, b[, for if they are at a and b, then
trivially G is 0 throughout [a, b].
Now, on any minimum of G within ]a, b[, as h changes sign from negative to
positive, G(t0 + h) −G(t0) remains positive; on a maximum it remains negative.
From (12.1), this can only hold if g(a) = g(b). As a and b are arbitrary, this shows
that g is constant.

12.1 Differentiation
261
For f ∗= 0 on X, we can use functionals to reduce it to a real-valued function:
let g(t) := α ∃f (tx) for any non-zero x ⇒X and α ⇒Y ∪. It is differentiable,
g(t + h) = α ∃f (tx + hx) = α

f (tx)

+ α

f ∗(tx)hx

+ o(hx) = g(t) + o(hx),
with derivative g∗(t) = 0. By the ﬁrst part, g(t) = g(0) = α ∃f (0) constant. But
with α and x arbitrary, this shows that f = f (0), a constant function.
∀∈
Exercises 12.5
1. Show that for differentiable functions ∂: R √F, f, g: R √X, T : R √
B(X, Y),
(a)
d
dt (∂(t) f (t)) = ∂∗(t) f (t) + ∂f ∗(t),
(b) ⊆f, g⟩∗= ⊆f ∗, g⟩+ ⊆f, g∗⟩,
(c)
d
dt F( f (t), g(t)) = δ1F( f (t), g(t)) f ∗(t) + δ2F( f (t), g(t))g∗(t),
(d)
d
dt T (t) f (t) = T ∗(t) f (t) + T (t) f ∗(t).
2. For a curve on the sphere r : [0, 1] √S2, the tangent t at any point satisﬁes
t · r = 0.
3. ▶For a differentiable function y : RN √RM, y∗is the Jacobi matrix [δi y j].
4. The derivative itself, f ∗(x), need not be continuous in x. For example, show
that f (x) := x2 sin(1/x) (and f (0) := 0) is differentiable at all points, yet its
derivative is not continuous at 0.
5. If f : X √R is differentiable and has a maximum/minimum at x in some open
set U ♦X, then f ∗(x) = 0.
6. L’Hôpital’s rule: If f : R √X, g: R √F are differentiable functions satisfying
f (a) = 0, g(a) = 0, but g∗(a) ⇔= 0, then
lim
x√a
f (x)
g(x) = f ∗(a)
g∗(a) .
12.2 Integration for Vector-Valued Functions
The construction of L1(R) can be extended to include functions f : R √X, where
X is a Banach space, as done in Section9.2. Brieﬂy,
• a vector-valued characteristic function x1E maps t to x ⇒X when t ⇒E ♦R
and to 0 otherwise;
• a simple function is a linear combination of vector characteristic functions on sets
of ﬁnite measure, in which case,

262
12
Differentiation and Integration

N

n=1
1En xn :=
N

n=1
μ(En)xn.
The set of simple functions is a normed space with →s→:=

→s(t)→X dt.
• a function f : R √X is integrable when it is the ae-limit of a Cauchy sequence
of simple functions sn √f a.e., →sn −sm→√0 as n, m √⊂; its integral is

f := lim
n√⊂

sn.
• on a measurable set A ♦R,

A f :=

f 1A, e.g.
 b
a f =

[a,b] f for a ⩽b.
Quoting the results of Section9.2,
Proposition 12.6
For f, g: R √X integrable,
(i)

f + g =

f +

g,

∂f = ∂

f
(∂⇒F),
(ii) →

f →⩽

→f (t)→dt,
(iii)

∂(t)x dt = (

∂)x for ∂⇒L1(R), x ⇒X,
(iv)

T f = T

f for T ⇒B(X, Y).
Examples 12.7
1.
  f (t)
g(t)

dt =

f (t)
1
0

+ g(t)
0
1

dt =

f

g

, when f, g: R √R are
integrable. Similarly,
 1
0
1 t
t2 t3

dt =
1
1/2
1/3 1/4

.
2. Any continuous function f : [a, b] √X is integrable, since
 b
a →f (t)→dt ⩽
(b −a)→f →C.
3. If fn(t) √f (t) in X, uniformly in t ⇒[a, b], then
 b
a fn √
 b
a f in X, since
		
 b
a
( fn −f )
		 ⩽
 b
a
→fn(t) −f (t)→dt ⩽→fn −f →L⊂[a,b](b −a).
The connection between differentiation and integration is one of the cornerstones
of classical mathematics. It remains valid for vector-valued functions:

12.2 Integration for Vector-Valued Functions
263
Theorem 12.8 Fundamental Theorem of Calculus
If f : [a, b] √X isintegrable,andcontinuousatt ⇒]a, b[,thenitsintegral
is differentiable at t, and
d
dt
 t
a
f = f (t).
If f ∗: [a, b] √X is continuous, then
 b
a
f ∗= f (b) −f (a).
Proof (i) The ﬁrst part is a consequence of
 t+h
a
f =
 t
a
f + f (t)h +
 t+h
t
f −f (t)h

and
		1
h
 t+h
t
f (γ) dγ −f (t)
		 =
		
 t+h
t
f (γ) −f (t)
h
dγ
		
⩽


 t+h
t
→f (γ) −f (t)→
|h|
dγ


<
ξ
|h|


  t+h
t
dγ


 = ξ
for arbitrary ξ > 0 and |h| sufﬁciently small, since f is continuous at t.
(ii) For the second part, let F(t) :=
 t
a f ∗. By (i) we obtain F∗= f ∗on ]a, b[, so
their difference F(t) −f (t) must be a constant c. As F(a) = 0, c = −f (a).
∀∈
Proposition 12.9
Mean value theorem
For a continuous function f : [a, b] √X,
1
b −a
 b
a
f (t) dt
belongs to the closed convex hull of f [a, b].
Proof The function is uniformly continuous (Proposition 6.17), so splitting [a, b]
into small enough intervals [tn, tn+1] of size h = (b −a)/N each (tn := a + nh),
ensures that →f (t) −f (t∗)→< ξ whenever t, t∗are in the same sub-interval. This

264
12
Differentiation and Integration
means that f can be approximated uniformly by a simple function which takes the
value f (tn) on the interval [tn, tn+1[, and its integral
 b
a f can be approximated to
within ξ(b −a) by the sum

f (a) + f (t1) + · · · + f (tN−1)

h.
Thus
1
b−a
 b
a f is within ξ of ( f (a)+ f (a + h)+· · · + f (b −h))/N which belongs
to the convex hull of f [a, b]. Since ξ is arbitrarily small, the result follows.
∀∈
Corollary 12.10
For a continuously differentiable function f : [a, b] √X,
f (b) −f (a)
b −a
belongs to the closed convex hull of f ∗[a, b].
Proof
f (b) −f (a)
b −a
=
1
b −a
 b
a
f ∗.
∀∈
Recall that f ∗is a function U √B(X, Y); it may itself be differentiable,
with derivative denoted by f ∗∗(x) ⇒B(X, B(X, Y)). This Banach space is actu-
ally isomorphic to the space of bilinear maps B(X2, Y) via the identiﬁcation
Tx1x2 = T (x1, x2). Because of this, f ∗∗(x) is akin to an operator that converts a
pair of vectors of X into a vector in Y; in particular, f ∗∗(x)(h, h) makes sense, and
is often shortened into the form f ∗∗(x)h2.
More generally, f (n) is the nth derivative of f : it takes n vectors in X and outputs
a vector in Y. The set of n-times differentiable functions f : R √X, with f (n)
continuous, is denoted by Cn(R, X).
Theorem 12.11 Taylor’s theorem
For f ⇒Cn(R, X) (n = 1, 2, . . .),
f (t + h) = f (t) + f ∗(t)h + · · · + f (n)(t)hn
n!
+ o(hn).

12.2 Integration for Vector-Valued Functions
265
Proof As expected the proof proceeds by induction on n. To illustrate the idea behind
the inductive step, we only consider how the statement for n = 2 follows from that
for n = 1. Let f ⇒C2(R, X), and let
F(s) := f (t + s) −f (t) −f ∗(t)s −f ∗∗(t)s2/2!
We wish to show F(h) = o(h2). F is continuously differentiable in s because it
consists of sums and products of continuously differentiable functions, in fact
F∗(s) = f ∗(t + s) −f ∗(t) −f ∗∗(t)s = o(s),
since f ∗is differentiable. Using the above corollary, it follows that F(h)−F(0)
h
belongs
to the closed convex hull of F∗[0, h], whose values are at most of order o(h). Since
F(0) = 0, we have F(h) = o(h2) as required.
The reader is invited to adapt this proof to show that if the statement is correct for
n then it is also true for n + 1. The case n = 1 is, of course, part of the deﬁnition of
the derivative.
∀∈
Exercises 12.12
1. Integrationbyparts:
 b
a f (t)F∗(t) dt = [ f F]b
a−
 b
a f ∗(t)F(t) dt,where f : R √
F and F : R √X have continuous derivatives.
2. Change of variables:
 x
a f (x) dx =
 y(x)
y(a) F(y) dx
dy dy, where y : R √R has an
invertible continuous derivative, and F(y(x)) = f (x).
3. If f : [a, b] √M is continuous, where M is a closed linear subspace of X, then
 b
a f ⇒M.
4. The symbol o(h) satisﬁes →o(h)→⩽c→h→for h small enough, but not necessarily
→o(h)→⩽c→h→2. However show that the latter inequality is true if f ∗(y) is
Lipschitz in y in some ball about x, by evaluating
		
 1
0
d
dt f (x + th) −f ∗(x)h dt
		.
Application: The Newton-Raphson Algorithm
If it is required to ﬁnd a vector x which solves f (x) = 0, where f is differentiable,
we might start with a ﬁrst estimate x and ﬁnd a better approximation from
0 = f (x + h) ≈f (x) + f ∗(x)h,
namely h = −f ∗(x)−1 f (x). This suggests the following iteration:

266
12
Differentiation and Integration
Proposition 12.13
The Newton-Raphson Method
Let ˜x be a zero of f and suppose that in a neighborhood of ˜x, f is dif-
ferentiable with f ∗(x) Lipschitz in x and →f ∗(x)−1→⩽c. Then if x0 is
sufﬁciently close to ˜x, the iteration
xn+1 := xn −f ∗(xn)−1 f (xn)
converges to ˜x.
Proof The differentiability of f at ˜x states that for h = xn −˜x, |h| < ξ
f (˜x + h) = f (˜x) + f ∗(˜x)h + o(h),
∴
f (xn) = f ∗(xn)h + ( f ∗(˜x) −f ∗(xn))h + o(h),
∴f ∗(xn)−1 f (xn) = xn −˜x + f ∗(xn)−1 
( f ∗(˜x) −f ∗(xn))h + o(h)

∴
→xn+1 −˜x→⩽3ck
2 →h→2 = ˜c→xn −˜x→2,
where k is the Lipschitz constant of f ∗and →o(h)→⩽1
2k→h→2 (Exercise 12.12(4)
above). If ξ < 1/˜c then it implies ﬁrstly that if xn belongs to Bξ(˜x), then so does
xn+1, and secondly by induction it follows that →xn −˜x→⩽(˜c→x0 −˜x→)2n/˜c √0
as n √⊂.
∀∈
This method is very effective since it converges quadratically, as long as x0 and
˜x are already close enough. In practice, other algorithms are utilized to perform a
broad search for a zero, and Newton’s method is then used to rapidly home in on it.
Another caveat is that it may be computationally expensive: one has to calculate not
only the derivative f ∗(x) but effectively also its inverse. The methods that are most
often used employ modiﬁed iterations like xn+1 := xn −Hn f (xn), where Hn are
operators that approximate f ∗(xn)−1 but are easier to calculate.
Examples 12.14
1. To solve for eiz = 1 close to z = 6, Newton’s iteration can be applied to f (z) :=
eiz −1,
xn+1 := xn + i(1 −e−ixn)
x0
6
x1
6.27942 + 0.03983i
x2
6.28334 −0.00080i
x3
6.28319 −0.0i
Examples of other equations whose solutions are routinely found using this
method are (a) roots of polynomials e.g. x3 = 2, (b) transcendental equations

12.2 Integration for Vector-Valued Functions
267
such as x −sin x = 1 or x tan x = 1, (c) simultaneous non-linear equations,
e.g. x2 + y = 1, x + y2 = 2.
2. The method can be used to ﬁnd the minimum of a scalar differentiable function,
say on R2, which is equivalent to ﬁnding zeros of its derivative. For example, if
the function were exactly quadratic
f (x) = a + b · x + 1
2 x◦Ax
then the minimum occurs when Ax + b = 0, and Newton’s method ﬁnds the
minimum point in one step: x1 = −A−1b. The more undulating a function is, the
more demanding it becomes to ﬁnd the true minimum. Two challenging functions
that have served as benchmarks are the following
(a) (1 −x)2 + 100(y −x2)2 (Rosenbrock’s valley),
(b) (x2 + y −11)2 + (x + y2 −7)2 (Himmelblau’s function).
3. * To align two real-valued functions f and g as best as possible, one may ﬁnd a
that minimizes

( f (x + a) −g(x))2 dx. Expanding this out, then differentiating
in a, gives

( f (x) −g(x))2 + 2( f (x) −g(x)) f ∗(x)a + ( f ∗(x)a)2
+ ( f (x) −g(x))a◦f ∗∗(x)a dx + o(a2)
∴

( f (x) −g(x)) f ∗(x) + ( f ∗(x)a) f ∗(x)
+ ( f (x) −g(x)) f ∗∗(x)a dx + o(a) = 0
The Newton-Raphson estimate of a is
a = −(⊆f ∗, f ∗◦⟩+ ⊆f −g, f ∗∗⟩)−1⊆f −g, f ∗⟩.
Letting fn+1(x) := fn(x + a), f0(x) := f (x), and iterating aligns the two
functions. (You can try this out with f (x) = cos x and g(x) = cos(x + 1) over
the interval [0, 2π].) This method has been implemented to align images (when
x, a ⇒R2), for example to compensate for video camera jitter from one frame to
the next.
12.3 Complex Differentiation and Integration
Let X be a complex Banach space, then a differentiable function f : C √X is also
called analytic, i.e., for all z, h,
f (z + h) = f (z) + f ∗(z)h + o(h).

268
12
Differentiation and Integration
The set of functions f : C √C which are analytic at all points z in an open set
U ≈A, is denoted by Cπ(A).
A function f : C √X is integrable along a differentiable path w : [t0, t1] √C,
when the composition f ∃w : [t0, t1] √C √X is integrable. Its integral is then

w
f (z) dz :=
 t1
t0
f (w(t))w∗(t) dt.
Notice that dz/i is along the normal to a path. Proposition 12.6 remains true, for
example property 2 becomes
		

w
f (z) dz
		 ⩽

→f (w(t))→ds, where ds := |w∗(t)| dt.
Examples 12.15
1. Along any curve w which starts at w(0) = a + bi and ends at w(1) = c + di,
 c+di
a+bi
1 dz =
 1
0
w∗(t) dt = [w(t)]1
0 = [z]c+di
a+bi.
More generally,
 c+di
a+bi f ∗(z) dz =
 1
0 f ∗(w(t))w∗(t) dt = [ f (z)]c+di
a+bi for f ana-
lytic (with f ∗continuous). Thus one can integrate analytic functions in the same
manner as real-valued functions.
2. The map z ∞√1
z is analytic except at z = 0. On a circular path w(t) := reit,
0 ⩽t ⩽2π,

∃
1
z dz =
 2π
0
1
r e−itireit dt = 2πi
(independent of the radius). Thus the integral
 1
−1
1
z dz does not have a unique
answer, but depends on whether one traverses a path that passes above or below
the origin, and how often it loops around it. But otherwise

∃
1
zn dz = 0.
3. Cauchy-Riemann equations: An analytic function f : C √C, x + iy ∞√
u(x, y) + iv(x, y) satisﬁes the equations
δu
δx = δv
δy ,
δu
δy = −δv
δx ,
since f ∗(z) = δu
δx + i δv
δx = δv
δy −i δu
δy , which can be obtained by comparing
f (z + h) = u(x, y) + δu
δx h + iv(x, y) + i δv
δx h + o(h) = f (z) + f ∗(z)h + o(h),
f (z + ih) = u(x, y) + δu
δy h + iv(x, y) + i δv
δy h + o(h) = f (z) + f ∗(z)ih + o(h),

12.3 Complex Differentiation and Integration
269
4. The conjugate map z ∞√¯z is not analytic, z + h = ¯z + ¯h. Therefore, Re(z) =
(z + ¯z)/2, Im(z) = (z −¯z)/2i, and |z| = z¯z, are not analytic. Indeed the Cauchy-
Riemann equations can be written symbolically as δ f
δ ¯z = 0, and interpreted as f
being independent of ¯z.
Cauchy’s Theorems
Analytic functions f : C √X are profoundly different from the similar-looking
functions f : R2 √X that are simply differentiable over the reals. This is borne out
by a string of results discovered by Augustin Cauchy in the 19th century. We will
only present here the essential theorems (See [20] for a more thorough presentation).
Theorem 12.16 Cauchy’s Theorem
Let β ≡C be a bounded open set having a ﬁnite number of differentiable
curves as boundary. Let f be a function from C into a Banach space, which
is analytic on and in β, then along these boundary curves,

f (z) dz = 0.
Warning: the curves must be traversed in a consistent manner, say with the region
β to the left of each curve. A fully rigorous proof requires results that are too technical
to be presented in a simpliﬁed form (see [10]). These details will be disregarded in
favor of a more intuitive approach, both for this theorem and its corollaries.
Proof At any analytic point, f (z + h) = f (z) + f ∗(z)h + o(h), where o(h)/h √0
as h √0. So for any ξ > 0 and |h| < ω small enough, we have →o(h)→< ξω. For
any closed curve ∴inside a disk Bω(z0) ♦β we get, using Example 1 above,

□
f (w) dw =

□
f (z0 + z) dz
=

□
f (z0) + f ∗(z0)z + o(z) dz
= f (z0)

□
1 dz + f ∗(z0)

□
z dz +

□
o(z) dz
=

□
o(z) dz
∴
		

□
f (w) dw
		 ⩽

□
→o(h)→ds ⩽ξω × Perimeter(∴)
(12.2)

270
12
Differentiation and Integration
Each point z0 ⇒β might need a different ω, but since β is compact, there is a
minimum ω that works at all points (as in Proposition 6.17).
The region β can be covered by an array of squares of side ω, as shown in the
diagram. The integral on the boundary δβ can be split up into a sum of integrals
along the squares that are within β, except that when a square intersects the boundary
δβ, the integral is partly along the square and partly along the boundary. Each tiny
loop has perimeter at most 4ω + l, where l is the length of that part of the boundary
curve which lies inside the square.
If β is enclosed in a square of side L, there are at most (L/ω)2 squares in all, so
the sum of the integrals is at most
		

δβ
f (w) dw
		 ⩽

i
		

□i
f (w) dw
		
⩽

i
ξω(4ω + li) by (12.2)
⩽

4L2 + Perimeter(β)ω

ξ
With ξ arbitrarily small, the integral must vanish.
∀∈
Corollary 12.17
If f is analytic in the interior β of a simple closed curve w, then the integral
 b
a f (z) dz is well-deﬁned when a, b ⇒β, independent of the path taken
(within β).
Proof Any two paths inside β, from a to b, together form one or more simple closed
paths, inside which f is analytic. Hence the integral of f on this closed loop is 0. ∀∈
One of the surprising results of Cauchy’s theorem is that the value of the integral

f (z) dz is independent of the bounding curve itself, but only on interior “distant”
regions!

12.3 Complex Differentiation and Integration
271
Augustin Louis Cauchy (1789–1857) studied under Lagrange
and Laplace as a military engineer, but decided to continue
with mathematics. A staunch royalist, he replaced Monge at
the Acad´emie des Sciences after the fall of Napoleon. Although
he published important papers in the ﬁelds of elasticity and
waves, he became famous for his taught courses on analysis and
calculus in the 1820s, in which he proved the diagonalization of
real quadratic forms and pushed forward the new standards of
rigor, e.g. limits, continuity, convergence.
Fig. 12.1 Cauchy
Corollary 12.18
Cauchy’s Residue Theorem
The integral over a closed simple curve depends only on those regions
inside where f is not analytic,
1
2πi

f (z) dz =

i
Residuei( f )
Proof Enclose the non-analytic parts by a ﬁnite number of curves wi—the outer
wi
γ
boundary curve η already does this, but it may be
possible to further isolate the non-analytic parts—
to form one analytic region, around which the inte-
gral is zero,
1
2πi

η
f +
1
2πi

i

wi
f = 0
traversing each curve wi in a clockwise direction.
The value of the integral around each non-analytic
region in a counter-clockwise direction may be
called a ‘residue’ of f .
∀∈
Because of this, the integral around a closed simple curve is often denoted by

f (z) dz, without reference to the (counter-clockwise) path taken, as long as it is
clear from the context which non-analytic regions are included.
The simplest cases in which a function fails to be analytic are of isolated points,
called isolated singularities. An example of an isolated singularity a is a pole of order
n when the function is of the type f (z)/(z −a)n with f analytic in a neighborhood
of a and f (a) ⇔= 0. A simple pole is a pole of order 1. All other isolated singularities
are called essential singularities. We shall see later that the residue of a function at

272
12
Differentiation and Integration
a pole of order n + 1 is f (n)(a)/n!, but what can be proved here is the case for a
simple pole:
Proposition 12.19
Cauchy’s Integral Formula
If f : C √X is analytic inside a simple closed path that contains a, then
f (a) =
1
2πi

f (z)
z −a dz.
Proof The integrand f (z)/(z −a) is analytic except at z = a, so by Cauchy’s
theorem the path of integration can be taken to be a small circle of radius r about a.
As f is analytic at a, we know f (a + w) = f (a) + f ∗(a)w + o(w), so
f (z)
z −a = f (a + w)
w
= f (a)
w
+ f ∗(a) + o(w)
w .
Integrating around a closed simple path eliminates the constant function f ∗(a), and




1
2πi
 o(w)
w
dz




 ⩽1
2π
 2π
0
|o(w)|
|w| r dt < rξ
if r is small enough that |o(w)|/|w| < ξ. Thus in the limit as we take smaller circles,
only the term
1
2πi

f (a)
w dw = f (a) is left.
∀∈
Examples 12.20
r
R
1. Interpreting the residue theorem in actual examples
often yields results that would be harder to obtain
otherwise. For example, the function eiz/z has a
simple pole at 0 with residue 1. So using a contour
as shown in the diagram, we obtain
2πi =
 R
r
eix
x dx +
 −r
−R
eix
x dx +
 π
0
e−R(sin θ−i cos θ)i dθ +
 2π
π
eireiθ i dθ
As R √⊂and r √0, the imaginary part 2
 ⊂
r
sin x
x
dx + π converges to 2π,
to give lim
r√0
R√⊂
 R
r
sin x
x
dx = π/2.
2. Maximum modulus principle: If f : C √C is analytic and has a local maximum
(or minimum) at a, then f is constant in a neighborhood of a. It follows that on a

12.3 Complex Differentiation and Integration
273
compact subset K, | f | attains its maximum and minimum at the boundary of K.
Proof Using a circular path of any radius r,
| f (a)| =




1
2πi

f (z)
z −a dz




 ⩽1
2π
 2π
0
| f (a + reiθ)| dθ ⩽| f (a)|
∴
 2π
0
| f (a)| −| f (z)| dθ = 0
so | f (z)| = | f (a)| within the disk, which in turn implies f (z) is constant
(Exercise 12.21(4)). Let f −1M be the subset of the interior of K where | f |
attains the maximum M := maxz⇒K ∃| f (z)|. It is open by the above, and closed
in K ∃((Exercises 3.12(11))), hence must contain whole components of K ∃, unless
empty. By continuity, f takes the same value M on the boundary.
3. We say that a function f has a zero of order n at a when f (z) = (z −a)ng(z),
with g(a) ⇔= 0, g analytic in a neighborhood of a.
If f : C √C has a zero (or pole) of order n at a, then f ∗/f has a simple pole at
a with residue n (resp. −n)
f ∗(z)
f (z) = n(z −a)n−1g(z) + (z −a)ng∗(z)
(z −a)ng(z)
=
n
z −a + g∗(z)
g(z) ,
(g∗/g is analytic at a). Thus
1
2πi

f ∗
f = n; more generally it equals the difference
between the number of zeros and poles (counted with their order) inside the curve
of integration.
4. Rouché’s theorem: If pn √f inside a closed simple curve η , with f non-zero on
η , then f and pn have the same number of zeros inside η , from some n onwards.
Proof As | f | has a non-zero minimum on η , there is an n such that | pn
f −1| < 1
on η . Let F := pn/f then

η
F∗
F =

F∃η
1
z dz = 0, since F ∃η is a closed curve
that excludes 0. By the previous example, this implies that F has the same number
of zeros as poles, that is, the zeros of pn and of f are the same in number.
Exercises 12.21
1. Show that, along any closed curve ∴in C,

□1 dz = 0 and

□z dz = 0, but on
a unit circle centered at the origin,

∃Re(z) dz = πi.
2. If fn(z) √f (z) in X for all z on a simple closed curve w, on which fn and f
are continuous, then

w fn(z) dz √

w f (z) dz.
3. Assuming u and v are sufﬁciently differentiable, deduce from the Cauchy-
Riemann equations that the real and imaginary parts of an analytic function
f = u + iv are harmonic,
δ2u
δx2 + δ2u
δy2 = 0,
δ2v
δx2 + δ2v
δy2 = 0.

274
12
Differentiation and Integration
4. Let f : C √C be analytic. Suppose | f | is constant in some open set, then f is
constant. (Hint: Differentiate | f |2 = u2 + v2.)
5. Find the poles and residues of (a) eiz/(z2 + 1), (b)
1
z3−1
 ez
e−z

, (c) (sin z)/z2 (First
show (sin z)/z is analytic at 0).
6.
1
2πi

z2 + 2
z(z2 −1) dz = −1
2 along a simple closed counter-clockwise path that
includes 0, 1, but note −1.
7. Show
(a)
 2π
0
1
2+cos θ dθ = 2π/
√
3 using f (z) := 1/(z2 + 4z + 1), z(θ) = eiθ
(b)
 ⊂
−⊂
cos x
x2+1 dx = π
e using f (z) :=
eiz
z2+1.
(c)
 ⊂
0
1−cos x
x2
dx = π
2 using f (z) := (1 −eiz)/z2.
8. By applying Example 3 to f = eg, prove that the order of any of its poles must
be zero. As this is impossible, the isolated singularities of f must be essential
singularities.
9. Use Rouché’s theorem to show that cosh z −2 cos z has 2 zeros in the unit disk,
assuming it equals its MacLaurin series.
Remarks 12.22
1. The ﬁrst use of the Newton-Raphson method was by the “Babylonians” who used
it to ﬁnd square roots, x2 = n. Newton’s method was initially restricted to ﬁnding
roots of polynomials, and it was Simpson (1740) who described the iteration we
use today.
2. Cauchy’s theorem for analytic functions is a special case of Green’s or Stoke’s
theorem

F · dr =

∇× F · dA. In this case, using the Cauchy-Riemann
equations,

f (z) dz =

(u + iv)(dx + idy) =

u dx −v dy + i

v dx + u dy
= −
  δv
δx + δu
δy

dA + i
 δu
δx −δv
δy

dA
= 0.

Part III
Banach Algebras

Chapter 13
Banach Algebras
13.1 Introduction
We now turn our attention to the space of operators B(X). We have seen that it is a
Banach space when X is one (Theorem 8.7), but additionally, one can compose, or
multiply, operators in B(X). This extra structure turns the vector space B(X) into
what is called an algebra. We shall mostly study these spaces as abstract algebras
X without speciﬁc reference to them being spaces of operators, in order to include
other examples of algebras and to make some of the proofs clearer. Nonetheless,
B(X) remains our primary interest, and accordingly, the elements of an algebra will
be denoted in general by upper-case letters T, S, . . . to remind us of operators and
to distinguish them from mere vectors x.
Deﬁnition 13.1
A unital Banach algebra X is a Banach space over C that has an associative
multiplication of vectors with unity 1, such that for all R, S, T ∞X, ∂∞C,
(R + S)T = RT + ST,
R(S + T ) = RS + RT,
(∂S)T = ∂(ST ) = S(∂T ),
√ST √⩽√S√√T √,
√1√= 1.
Throughout this book, a Banach algebra will mean a unital Banach algebra. Of
course, Banach algebras over R are also of interest, and all the results in this chapter
apply to them in modiﬁed form; but complex scalars are necessary for an adequate
spectral theory of X.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_13,
277
© Springer International Publishing Switzerland 2014

278
13
Banach Algebras
Easy Consequences
1. 1 is unique, because 1∗= 1∗1 = 1 for any other unity 1∗.
2. T is said to be invertible (or regular) when there is an element S, called its inverse,
such that ST = 1 = T S. The inverse of T is unique when it exists, and is denoted
T −1. If AT = 1 = T B then A = A(T B) = (AT )B = B so A = T −1.
3. (S + T )2 = S2 + ST + T S + T 2, and more generally,
(S + T )n = Sn + (ST n−1 + T ST n−2 + · · · + T n−1S) + · · · + T n.
4. √T n√⩽√T √n.
Proposition 13.2
Multiplication, (T, S) ⇒→T S, is a differentiable map.
Proof In the identity
(T + H)(S + K) = T S + (T K + H S) + H K,
(13.1)
the map (H, K) ⇒→T K + H S, X 2 →X, is linear and continuous, and H K is of
lower order, since
√T K + H S√⩽√T √√K√+ √S√√H√⩽max(√T √, √S√)(√H√+ √K√)
√H K√⩽√H√√K√⩽(√H√+ √K√)2 = √(H, K)√2.
Of course, every differentiable map is continuous.
♦∃
Examples 13.3
1. CN with the the ∀-norm and the following pointwise multiplication and unity:

⎛⎜
a1
...
aN
⎝
⎞⎟

⎛⎜
b1
...
bN
⎝
⎞⎟:=

⎛⎜
a1b1
...
aNbN
⎝
⎞⎟,
1 =

⎛⎜
1
...
1
⎝
⎞⎟.
2. ▶δ∀with pointwise multiplication x y, and unity 1 = (1, 1, . . .) (Exercise
9.4(3)).
3. C(K), the space of continuous functions on a compact set K, with pointwise
multiplication f g(x) := f (x)g(x), and unity the constant function 1. For exam-
ple, C[0, 1] is a space of paths in the complex plane.
4. ▶δ1 with the convolution product; unity is e0 = (1, 0, . . .) (Exercise 9.7(2)).

13.1 Introduction
279
5. The space L1(R) with convolution as a product; although it does not have a unity,
wecanartiﬁciallyaddaα,calledDirac’s“function”,suchthatα∈f := f =: f ∈α.
(To make this rigorous, one needs to consider L1(R) × C with elements ( f, a)
representing f + aα.)
The above examples happen to be commutative, i.e., ST = T S holds. But this
is not assumed in general. For example, T 2 −S2 ⊆= (T −S)(T + S) in general.
6. ▶B(X) for any Banach space X; the product is operator composition (Proposi-
tion 8.8).
7. ▶If X and Y are Banach algebras, then so is X × Y with
⎠S1
T1
⎫⎠S2
T2
⎫
:=
⎠S1S2
T1T2
⎫
,
1 =
⎠1X
1Y
⎫
,
⎪⎪
⎠S
T
⎫⎪⎪:= max(√S√X , √T √Y).
8. Every normed algebra can be completed to a Banach algebra.
Proof Using the notation of Proposition 7.17, if T = [Tn] and S = [Sn], let
ST := [SnTn] and 1 := [1]. Note that SnTn is a Cauchy sequence by
√SnTn −SmTm√⩽√SnTn −SnTm√+ √SnTm −SmTm√
⩽√Sn√√Tn −Tm√+ √Sn −Sm√√Tm√
⩽c(√Sn −Sm√+ √Tn −Tm√).
Hence
R(ST ) = [Rn(SnTn)] = [(RnSn)Tn] = (RS)T,
∂(ST ) = [∂(SnTn)] = (∂S)T = S(∂T ),
√ST √= lim
n→∀√SnTn√⩽lim
n→∀√Sn√√Tn√= √S√√T √.
9. The polynomials C[z] on ¯BC with the ∀-norm form an incomplete algebra. As
we shall see shortly, its completion is the space of analytic functions Cγ( ¯BC).
More general is the tensor algebra, consisting of polynomials and series in N
non-commuting variables.
10. ▶If ST = 0 and S is invertible, then T = 0. But there may exist non-zero
non-invertible elements S, T , called divisors of zero, for which ST = 0. Note
that T S need not also be 0, so S and T are more precisely called left and right
divisors of zero, respectively.
11. ▶The product of invertible elements is invertible, with (ST )−1 = T −1S−1.
Also, (T −1)−1 = T . If T n is invertible, for some n ⩾1, then so is T .
But it is possible for two non-invertible elements to have an invertible product,
i.e., ST invertible ⊆⇒T invertible (unless T R is also invertible for some R). In

280
13
Banach Algebras
particular, ST = 1 by itself is not enough to ensure T and S are invertible. For
example, in B(δ1), the product of the (non-invertible) shift-operators is L R = I.
12. Suppose an element satisﬁes some non-zero polynomial, p(T ) = 0. The unique
such polynomial of minimum degree and leading coefﬁcient 1 is called its min-
imal polynomial pm. It divides all other polynomials p such that p(T ) = 0.
Proof There cannot be two minimal polynomials, pm and p, otherwise pm −p
has a lesser degree than both and pm(T ) −p(T ) = 0. If p(T ) = 0, then
p = qpm + r by the division algorithm of polynomials. As r has a strictly
smaller degree than pm, yet r(T ) = p(T ) −q(T )pm(T ) = 0, it must be the
zero polynomial.
13. The derivative of the map T ⇒→ST is S. Similarly the derivative of T ⇒→T n is
H ⇒→HT n−1 + T HT n−2 + · · · + T n−1H.
Because of commutativity, this simpliﬁes to (zn)∗= nzn−1 in C. Thus, any
polynomial in T is differentiable in T .
Subalgebras and Ideals
Deﬁnition 13.4
A subalgebra of an algebra X is a subset which is itself an algebra with
the same (induced) addition, scalar multiplication, product, and unity. It is a
Banach subalgebra (with the induced norm) when it is also complete.
An ideal is a linear subspace I such that ST, T S ∞I for any T ∞X,
S ∞I.
To show that a non-empty subset A is a subalgebra of X, one need only show
closure of the various operations, i.e., for any S, T ∞A, S + T ∞A, ∂T ∞A,
ST ∞A, 1 ∞A. The required properties of the induced operations are obviously
inherited from those of X.
Examples 13.5
1. C is embedded in every (complex) Banach algebra as C1 = { z1 : z ∞C }. In
fact, it is customary to write z when we mean z1.
2. An element T generates the subalgebra of polynomials
C[T ] := { a0 + a1T + · · · + anT n : a1, . . . , an ∞C, n ∞N }.

13.1 Introduction
281
More generally, a ﬁnite number of commuting elements T1, . . . , Tn gener-
ate the commutative algebra C[T1, . . . , Tn], which may contain, for example,
1 −2T2 + T 2
1 T2.
3. The algebra δ∀contains the closed ideal c0.
Proof That c0 is a closed linear subspace of δ∀is proved in Proposition 9.2. Let
(an) ∞δ∀, (bn) ∞c0, then (an)(bn) ∞c0 since
| lim
n→∀anbn| ⩽sup
n
|an| lim
n→∀|bn| = 0.
We will see later that every commutative Banach algebra, except C, has non-trivial
ideals (Example 14.5(4)).
4. The center X ∗:= { T : ST = T S, ⇐S ∞X } is a commutative closed subalgebra
of X.
Proof If T1, T2 ∞X ∗, then
S(T1 + ∂T2) = ST1 + ∂ST2 = T1S + ∂T2S = (T1 + ∂T2)S,
S(T1T2) = T1ST2 = (T1T2)S,
SI = S = I S.
The algebra is commutative by deﬁnition of X ∗.
5. ▶Proper ideals do not contain 1, or any other invertible element T , otherwise
it would have to contain every element S = ST −1T . (However, as remarked in
Example 13.3(11), the set of non-invertible elements need not be an ideal, or even
a subspace.)
6. A closed ideal gives rise to a quotient algebra X/I with multiplication and unity
deﬁned by
(S + I)(T + I) := ST + I,
1 + I.
7. A maximal ideal is a proper ideal I for which the only other ideal containing it
is X itself,
I ⊂J ⊂X ⇒J = I or J = X.
Maximal ideals are necessarily closed, assuming that the closure of a proper ideal
is also a proper ideal (Example 13.22(3)).
8. * Every proper ideal is contained in a maximal ideal.
Proof Let C be the collection of all proper ideals that contain the proper ideal
I. By Hausdorff’s maximality principle, C contains a maximal chain of nested
ideals Iξ. Then M := ⎬
ξ Iξ is an ideal, since if T ∞Iξ and S ∞Iβ ⊂Iξ, say,
then S + T ∞Iξ ⊂M, and for any S ∞X, both ST and T S are in Iξ ⊂M.

282
13
Banach Algebras
It is obvious that M is proper and contains I since 1 ⊆∞Iξ ⇔I for every ξ, and
that M is maximal since the chain Iξ is maximal.
Morphisms
Deﬁnition 13.6
A morphism π : X →Y of Banach algebras is a continuous linear map
(preserving limits, addition, and scaling) which preserves multiplication and
the unity,
π(ST ) = π(S)π(T ),
π(1X ) = 1Y.
A character is a Banach algebra morphism β : X →C. The set of char-
acters, denoted by ω, is called the character space, or spectrum, of X.
Examples 13.7
1. InvertibleelementsofX aremappedbyalgebramorphismstoinvertibleelements
of Y,
π(T )−1 = π(T −1),
sinceπ(T )π(T −1) = π(T T −1) = π(1) = 1andsimilarly,π(T −1)π(T ) = 1.
2. ▶The kernel of a Banach algebra morphism, ker π := { T : π(T ) = 0 }, is a
closed ideal. It is maximal when π ∞ω.
Proof If π(T ) = 0, then π(ST ) = π(S)π(T ) = 0; similarly, π(T S) = 0.
Maximality: Let π : X →C be a morphism, and let the ideal I contain ker π
as well as some T ⊆∞ker π. Then π(T ) = ∂⊆= 0, and π(∂−T ) = 0; so
∂= (∂−T ) + T ∞I, and I must equal X (Example 13.5(5) above).
(Every maximal ideal of a commutative Banach algebra is of the type ker β with
β ∞ω, but the proof requires Exercise 13.10(19) and Example 14.5(4); see the
proof of Theorem 14.38.)
3. An isomorphism of Banach algebras is deﬁned to be an invertible morphism
π : X →Y such that π−1 is also a morphism. In fact, an invertible morphism is
automatically an isomorphism.
4. An automorphism of a Banach algebra X is an isomorphism from X to itself. For
example, the inner automorphisms T ⇒→S−1T S, for any ﬁxed invertible S.
5. Since C is commutative, commutators [S, T ] := ST −T S are mapped to 0 by
characters (if they exist).

13.1 Introduction
283
Representation in B(X)
Some mathematical theories contain a set of theorems stating that any abstract model
of the theory can be represented concretely. For example, every group can be repre-
sented by a permutation group, and every smooth manifold is embedded as a smooth
“surface” of a Euclidean space. In this regard, every ﬁnite-dimensional Banach alge-
bra can be embedded, or “faithfully represented”, as a matrix algebra, and more
generally, we have the following representation theorem:
Theorem 13.8
Every Banach algebra can be embedded as a closed subalgebra of B(X),
for some Banach space X.
Proof The Banach space X is the Banach algebra X itself without the product
(although there may well be ‘smaller’ Banach spaces that ﬁt the job). That is, the
theorem claims that X is embedded in B(X). To avoid confusion, we temporarily
denote elements of X by lower-case letters, and the operators on them by upper-case
letters.
Let La(x) := ax beleft-multiplicationbya.Then La ∞B(X)sincemultiplication
is distributive and continuous:
La(x + y) = a(x + y) = ax + ay = La(x) + La(y),
La(∂x) = a(∂x) = ∂(ax) = ∂La(x),
√La(x)√= √ax√⩽√a√√x√,
so that √La√⩽√a√. Furthermore,
La+b(x) = (a + b)x = ax + bx = La(x) + Lb(x),
L1(x) = 1x = x = I (x),
L∂a(x) = (∂a)x = ∂La(x),
La(1) = a1 = a,
Lab(x) = (ab)x = a(bx) = LaLb(x),
so √a√= √La1√⩽√La√√1√= √La√and √La√= √a√. These show that the
mapping L : X →B(X) deﬁned by L : a ⇒→La is an isometric morphism of
Banach algebras. In fact, the space of such operators, im L, is a closed subalgebra of
B(X) since isometries preserve completeness (Exercise 4.17(5)). Note that all the
Banach algebra axioms have been used.
♦∃
As one may anticipate, B(X) and B(Y) are not isomorphic as Banach algebras,
when X and Y are not isomorphic as Banach spaces. The proof, however, is not as
obvious as one might expect.

284
13
Banach Algebras
Theorem 13.9
Let X and Y be Banach spaces. A Banach algebra isomorphism J :
B(X) →B(Y) induces a Banach space isomorphism L : X →Y, such
that
J(T ) = LT L−1.
Thus, every automorphism of B(X) is inner.
Proof The idea is to establish a 1–1 correspondence between vectors x ∞X and
certain projection-like operators Px ∞B(X), and similarly y ∩Ry for Y; using the
given mapping J : T ⇒→⎭T , the sought isomorphism would then be
L : x ⇒→Px
J⇒→Ry ⇒→y.
The correspondence x ∩Px: For the remainder of the proof, ﬁx a vector a ∞X,
a ⊆= 0, and a functional β ∞X∈such that βa = 1. Multiplying x by β gives an
operator Px := xβ, that is, Pxu := (βu)x; conversely, multiplying Px with a gives
back the vector Pxa = xβa = x. The crucial characteristic of these operators is, for
any T ∞B(X),
T Px = T xβ = (T x)β = PT x,
Px1+x2 = (x1 + x2)β = Px1 + Px2.
In particular Px Pa = xβaβ = Px. Note that √Px√= √xβ√⩽√x√√β√and
√x√= √Pxa√⩽√Px√√a√. Thus, P : X →B(X), x ⇒→Px is an embedding.
The isomorphism J maps Px ∞B(X) to a similar operator Ry ∞B(Y): The
relation P2
a = Pa is preserved by J, so ⎭Pa := J(Pa) is a non-zero projection in
B(Y). Pick b ∞im ⎭Pa and η ∞Y ∈such that ηb = 1 and η ker ⎭Pa = 0 (Proposition
11.18), and deﬁne Ry := yη. Ry satisﬁes analogous properties as Px, such as
Ryb = y and ⎭T Ry = R⎭T y. Now suppose c ∞im ⎭Pa, and let T ∞B(X) correspond
to Rc ∞B(Y) under J; then J transforms the identity
PaT Pa = a(βTa)β = ∂Pa,
where
∂= βTa,
to ⎭Pa Rc ⎭Pa = ∂⎭Pa, so im ⎭Pa = [[b]] since
c = ⎭Pac = ⎭Pa Rcb = ⎭Pa Rc ⎭Pab = ∂⎭Pab = ∂b.
Thus the projections ⎭Pa and Rb have the same image and the same kernel, and we
can conclude that they are equal to each other.
Hence, the identity Px = Px Pa becomes, in B(Y),
⎭Px = ⎭Px Rb = R⎭Pxb = Ry,
where y = ⎭Pxb.

13.1 Introduction
285
The map L : x ⇒→y = J(Px)b is an isomorphism: That L is linear, continuous,
and 1–1 follow from:
L(x1 + x2) = J(Px1+x2)b = J(Px1 + Px2)b = L(x1) + L(x2),
L(∂x) = J(P∂x)b = J(∂Px)b = ∂L(x),
√Lx√= √J(Px)b√⩽√J√√β√√x√√b√,
Lx = 0 ∪J(Px)b = 0 ∪J(Px) = 0 ∪Px = 0 ∪x = 0.
Given any y ∞Y, J −1 maps the identity Ry = Ry Rb to S = SPa = PSa. So for
x := Sa,
Lx = J(PSa)b = Ryb = y,
and L is onto. By the open mapping theorem (Theorem 11.1), L is an isomorphism.
⎭T = LT L−1: J maps the identity T Px = PT x to ⎭T RL(x) = RL(T x). Multiplying
by b to get the vector form, this reads ⎭T Lx = LT x for all x ∞X.
When X = Y, then L ∞B(X), and J is an inner automorphism.
♦∃
Exercises 13.10
1. Banach algebras of square matrices abound: the sets of matrices of type
⎠a 0
b c
⎫
,
⎠a b
0 a
⎫
,
⎠a b
b a
⎫
,or
⎠a
b
−b
a
⎫
areeachclosedunderadditionandmultiplication,
and are Banach subalgebras of B(C2).
2. C := C2 with
⎠a
b
⎫⎠c
d
⎫
:=
⎠
ac
ad + bc + bd
⎫
is a Banach algebra, with unity
⎠1
0
⎫
. (Hint: it is a matrix algebra in disguise.)
3. Find examples of 2 × 2 matrix divisors of zero, ST = 0 ⊆= T S.
4. Show that in an N-dimensional algebra, every element has a minimal polynomial
of degree at most N; e.g. every square matrix A has a minimal polynomial.
Show also how the Gram-Schmidt process (with respect to the inner product of
Example 10.2(2)) can be applied to the sequence I, A, A2, . . . to construct this
minimal polynomial.
5. An idempotent satisﬁes P2 = P. They are the projections in B(X); what are
they in CN and δ∀? The idempotents of C[0, 1] are trivial. Show further that
PX P is an algebra with unity P, called a “reduced algebra”.
6. A nilpotent satisﬁes Qn = 0 for some n, e.g.
⎠0 1
0 0
⎫
and
⎠1 i
i −1
⎫
. In CN,
δ∀, and C[0, 1], there are no nilpotents except zero. Find all the 2 × 2 matrix
nilpotents of order 2, i.e., Q2 = 0.

286
13
Banach Algebras
7. An element is cyclic when T n = 1 for some n, e.g.
⎠1 0
0 i
⎫
. In CN and δ∀they
are sequences whose terms are of the type e2πim/n for a ﬁxed n.
8. The product of differentiable functions is again differentiable, with
( f g)∗(T )H = [ f ∗(T )H]g(T ) + f (T )[g∗(T )H].
This can be written in short as the familiar product rule ( f g)∗= f ∗g + f g∗,
provided it is remembered that the vector H is acted upon by each derivative.
9. If F : R →X is integrable and T ∞X, then
⎧
F(t)T dt = (
⎧
F)T (First show
it true for simple functions).
10. * Group Algebra: Let G be a ﬁnite group of order N, and { eg : g ∞G } be
an orthonormal basis for CN; deﬁne eg ∈eh := egh, and extend the product to
all other vectors by distributivity. The result is a Banach algebra CG (or δ1(G))
with unity e1 and the 1-norm. Every basis element is cyclic.
For example, the cyclic group { 1, g : g2 = 1 }, gives rise to an algebra generated
by e1 :=
⎠1
0
⎫
and eg :=
⎠0
1
⎫
, and the product
⎠a
b
⎫
∈
⎠c
d
⎫
:= (ae1 + beg) ∈(ce1 + deg) =
⎠ac + bd
bc + ad
⎫
.
11. The closure of a subalgebra is an algebra (use continuity of the product).
12. If I and J are ideals, then so are I + J and I.
13. The center of B(X) is C. (Hint: Consider projections xβ, for any x ∞X, β ∞X∈.)
14. ▶The centralizer or commutant of a subset A ⊂X,
A∗:= { T : AT = T A, ⇐A ∞A }
is a closed subalgebra of X. (In fact, when X = B(H), A∗is weakly closed by
Exercise 11.42(11a).)
Prove:
(a) A ⊂B ⇒B∗⊂A∗,
(b) A ⊂A∗∗and A∗∗∗= A∗,
(c) If T ∞A∗is invertible in X then T −1 ∞A∗,
(d) If elements of A commute, then A ⊂A∗and A∗∗is a commutative Banach
algebra.
15. A left-ideal is a linear subspace of X such that T I ⊂I for any T ∞X.
Similarly, for a right-ideal, IT ⊂I. For example, X S is a left-ideal, and SX is

13.1 Introduction
287
a right-ideal, but X SX need not be an ideal. Instead, the ideal generated by S
is [[X SX]].
16. Let A be a closed subset of [0, 1], then
IA := { f ∞C[0, 1] : ⇐x ∞A, f (x) = 0 }
is a closed ideal of C[0, 1]. Conversely, given a closed ideal I of C[0, 1], let
A := { x ∞[0, 1] : ⇐f ∞I, f (x) = 0 },
then I = IA. What are the maximal ideals?
17. Let IA be a closed ideal of C[0, 1], where A is a closed subset. Then the mapping
f + IA ⇒→f |A is an isomorphism C[0, 1]/IA ≡C(A).
18. An algebra morphism π : X →Y ‘pulls’ ideals I in Y to ideals π−1I in X.
19. If I is a closed ideal, then π(T ) := T + I gives a Banach algebra morphism
π : X →X/I with kernel ker π = I.
20. The mapping ⎨
n anzn ⇒→(an) from the set of power series converging
absolutely on the closed unit disk D of C, considered as a subspace of C(D), to
δ1 is a 1–1 Banach algebra morphism.
21. Letσ beapermutationof1, . . . , N;thenthemappingdeﬁnedby(z1, . . . , zN) ⇒→
(zσ(1), . . . , zσ(N)) is an automorphism of CN.
22. For the group algebra CG, let σ be an automorphism of the group G; then
eg ⇒→eσ(g) induces an automorphism on CG.
23. The algebra CN is embedded in B(CN) as diagonal matrices. C is represented
by the matrices
⎠a
0
b a + b
⎫
. The group algebra CG is generated by the Cayley
matrices of G.
24. Show that every Banach algebra of dimension 2 (over C) can be represented by
the matrices generated from I and
⎠0 ξ
1 β
⎫
, where ξ is a ﬁxed number and β is
0 or 1. What are ξ and β for the group algebra generated by { 1, g : g2 = 1 }?
25. Let X be a Banach algebra contained in B(X). Its unity P = P2 is a projection,
so X = M ◦N where M = im P. For every T ∞X, PT = T = T P implies
M is T -invariant and T N = 0, hence X acts on M.

288
13
Banach Algebras
13.2 Power Series
Deﬁnition 13.11
A power series is a series ⎨
n anT n where an ∞C and T ∞X.
Recall that the root test can help determine whether such a series converges or
not: if √anT n√1/n = |an|1/n√T n√1/n converges to a number less than 1, then the
power series converges. It is important to know that √T n√1/n converges:
Proposition 13.12
For any T in a Banach algebra, the sequence √T n√1/n converges to a num-
ber denoted by τ(T ), where
⇐n ∞N,
τ(T ) ⩽√T n√1/n ⩽√T √.
Proof It is clear that 0 ⩽√T n√1/n ⩽√T √. Let τ(T ) be the inﬁmum value of
√T n√1/n, meaning that √T n√1/n is bounded below by τ(T ) and
⇐ϵ > 0, ≈N,
τ(T ) ⩽√T N√
1/N < τ(T ) + ϵ.
Although the sequence √T n√1/n is not necessarily decreasing towards τ(T ), notice
that √T qm√1/qm ⩽√T m√1/m. For any n, let n = qn N + rn with 0 ⩽rn < N (by the
remainder theorem), then 0 ⩽rn/n < N/n →0 and qn/n = 1
N (1 −rn
n ) →1
N as
n →∀, so that
τ(T) ⩽√T n√1/n = √T qn N T rn√
1/n ⩽√T N√
qn/n√T √rn/n →√T N√
1
N < τ(T) + ϵ.
Since ϵ is arbitrarily small, this shows that √T n√1/n →τ(T ) from above.
♦∃
Examples 13.13
1. ▶(a) τ(1) = 1, (b) τ(∂T ) = |∂|τ(T ), (c) τ(ST ) = τ(T S), (d) τ(T n) = τ(T )n,
since
√1√1/n = 1,
√∂nT n√1/n = |∂|√T n√1/n,
τ(ST ) ⩽√(ST )n√
1
n ⩽√S√
1
n √(T S)n−1√
1
n √T √
1
n →τ(T S),
√(T n)m√1/m = √T nm√
n
nm →τ(T )n as m →∀.
But τ(T ) may be 0 without T = 0; and τ(S + T ) ⊆⩽τ(S) + τ(T ) in general
⎠
e.g.
⎠0 1
0 0
⎫
,
⎠0 0
1 0
⎫⎫
. So τ is not usually a norm on X.

13.2 Power Series
289
2. ▶τ(T ) = √T √∪√T n√= √T √n, ⇐n ∞N, since √T √= τ(T ) ⩽√T n√1/n ⩽
√T √.
3. ▶If τ(T ) < 1, then T n →0 (even though √T √may be bigger than 1). If
τ(T ) > 1, then T n →∀.
Proof For ϵ small enough and n large enough,
√T n√1/n ⩽τ(T ) + ϵ < 1
⇒
√T n√⩽(τ(T ) + ϵ)n →0, as n →∀,
√T n√1/n ⩾τ(T ) > 1 + ϵ
⇒
√T n√⩾(1 + ϵ)n →∀.
Theorem 13.14
Cauchy-Hadamard
The power series ⎨∀
n=0 anT n, where an ∞C, T ∞X,
• converges absolutely when τ(T ) < R, and
• diverges when τ(T ) > R,
where R := 1/ lim sup |an|1/n is called the radius of convergence of the
series.
Proof This is a simple application of the root test. The nth root of the general term
satisﬁes
lim sup
n
√anT n√1/n = lim sup
n
|an|1/nτ(T ) = τ(T )/R.
Thus, if τ(T ) < R, then the series converges absolutely, while if τ(T ) > R,
then it diverges. Assuming X is complete, the power series converges or diverges
accordingly.
♦∃
Examples 13.15
1. Ratio test: If |an|/|an+1| →R then so does |an|−1/n (Section 7.5), hence R
would be the radius of convergence of ⎨
n anT n.
2. Some aspects of power series may seem mysterious from the point of view of
real numbers: The series 1 −x2 + x4 −x6 + · · · has a radius of convergence
of 1 and converges to
1
1+x2 which takes a ﬁnite value at all x ∞R (but not at
x = i). Moreover the same series can also be written as (5 −(4 −x2))−1 =
1
5
⎨∀
n=0
⎩4−x2
5
n, yet in this form it converges in the larger range −3 < x < 3.
3. The theorem also applies to power series ⎨
n Anzn, where An is a sequence of
elements in X. The radius of convergence is then 1/ lim supn √An√1/n.

290
13
Banach Algebras
4. When |an| ⩽c for all n, then a2T 2 + a3T 3 + · · · = o(T ) for small T , since it is
bounded above by c√T √2/(1 −√T √).
When can a function be written as a power series? We wish to establish that being
analytic in a neighborhood of 0 is a necessary and sufﬁcient condition. The necessary
part is the content of the following proposition, but sufﬁciency will be shown later
(Theorem 13.26).
Proposition 13.16
A power series f (z) := ⎨∀
n=0 anzn is analytic strictly within its radius of
convergence R, and
f ∗(z) =
∀

n=1
nanzn−1.
Proof First of all, the power series ⎨
n annzn−1 converges, with the same radius of
convergence R as ⎨
n anzn,
lim sup
n
|nan|1/n = lim
n→∀n1/n lim sup
n
|an|1/n = 1/R
(Exercise 3.5(1d)).
For each individual term of the given power series,
(z + h)n = zn + nzn−1h + on(h).
It needs to be shown that | ⎨
n anon(h)|/|h| →0 as h →0. One trick is to ﬁnd an
alternative way of expanding (z + h)n as follows:
(z + h)n = (z + h)n−1h + (z + h)n−1z
= (z + h)n−1h + (z + h)n−2zh + (z + h)n−2z2
= (z + h)n−1h + · · · + (z + h)n−kzk−1h + · · · + zn−1h + zn
⇒
|(z + h)n −zn| ⩽(|z + h|n−1 + · · · + |z|n−1)|h|
⩽nrn−1|h|,
(13.2)
where r is larger than |z| + |h| but smaller than R. Now,
on(h) = (z + h)n −zn −nzn−1h
= (z + h)n−1h + · · · + (z + h)n−kzk−1h + · · · + zn−1h
−zn−1h −· · · −zn−kzk−1h −· · · −zn−1h
=
n

k=1
⎩
(z + h)n−k −zn−k
zk−1h
so |on(h)| ⩽(n −1)rn−2|h|2 + · · · + rn−2|h|2
by (13.2)

13.2 Power Series
291
= n(n −1)
2
rn−2|h|2
But the series c :=
∀

n=2
|an|n(n −1)
2
rn−2 converges for r < R, so

∀

n=0
anon(h)
 ⩽
∀

n=2
|an||on(h)| ⩽c|h|2
which proves that the remainder term ⎨
n anon(h) is o(h).
♦∃
There are two important consequences: Since differentiating a power series gives
another power series with the same radius of convergence, then we can differentiate
repeatedly. Secondly, we know that polynomials are distinct as functions on C when
they have different coefﬁcients; this property remains valid for power series: If a
function can be written as a power series, then its coefﬁcients are unique to it.
Proposition 13.17
Assuming a strictly positive radius of convergence,
(i) a power series f (z) := ⎨∀
n=0 anzn is inﬁnitely many times differen-
tiable, and
an = f (n)(0)
n!
(ii) distinct power series do not have identical coefﬁcients.
By distinct power series is meant ⎨
n bnT n ⊆= ⎨
n cnT n for at least one T .
Proof (i) By induction on n, f (n) has the power series
f (n)(z) = n!an + (n + 1)!an+1z + (n + 2)!
2
an+2z2 + · · ·
Substituting z = 0 gives the stated formula.
(ii) Suppose ⎨
n bnT n = ⎨
n cnT n for all T such that τ(T ) < R, the smaller of
their radii of convergence. By taking the difference of the two series, it is enough to
show that if f (z) := ⎨
n anzn = 0 for all z ∞BR(0), then an = 0 for all n. But this
is immediate from (i) since f (n)(0) = 0 in this case.
♦∃

292
13
Banach Algebras
The Exponential and Logarithm Maps
There are a couple of power series of supreme importance. As motivation, consider
the possibility of converting addition in a Banach algebra to multiplication,
f (x + y) = f (x) f (y),
f (0) = 1.
Apart from the constant function f = 1, are there any others? If f exists, it would
have to satisfy a number of properties:
(a) f (nx) = f (x)n, f (−x) = f (x)−1,
(b) When the algebra is R, f (m/n) = am/n where a := f (1) > 0 (Hint: f (n/n) =
f (1/n)n),
(c) f is uniformly continuous on Q ≡[0, 1], so it can be extended to a continuous
function on R, usually denoted by f (x) = ax,
(d) f ∗(x) = f ∗(0) f (x) if f is differentiable at 0, since f (h) = 1 + f ∗(0)h + o(h)
so
f (x + h) = f (x) f (h) = f (x) + f (x) f ∗(0)h + o(h);
consequently f isinﬁnitelymanytimesdifferentiablewith f (n)(x) = f ∗(0)nf (x).
Taking the simplest case f ∗(0) = 1 (so f (n)(0) = 1) leads to the following def-
inition:
The exponential function is deﬁned by
eT := 1 + T + T 2
2! + T 3
3! + · · · =
∀

n=0
1
n!T n.
Its radius of convergence is lim infn |an|−1/n = limn→∀
1/n!
1/(n+1)! = ∀by the
ratio test, so eT exists for any T and satisﬁes √eT √⩽e√T √.
Similarly, starting with f (xy) = f (x) + f (y), we are led to the logarithm
function, deﬁned by
log(1 + T ) := T −T 2
2 + T 3
3 + · · · =
∀

n=1
(−1)n+1
n
T n,
with radius of convergence lim infn |an|−1/n = limn→∀
1/n
1/(n+1) = 1.
Proposition 13.18
When S, T commute,
eS+T = eSeT . For τ(T ) < 1,
elog(1+T ) = 1 + T .

13.2 Power Series
293
Proof (i) The product eSeT can be obtained in table form as,
eS =
1
+
S
+
1
2!S2 + · · ·
eT
=
1
1
S
1
2S2
+
T
T
ST
1
2S2T
+
1
2!T 2
1
2T 2
1
2ST 2
1
4S2T 2
+
...
The general term in this array is 1
n!
1
m! SnT m =
1
N!
⎩N
n

SnT N−n where N := n + m
is the Nth diagonal from the top left corner. This is precisely the nth term of the
expansion of
1
N!(S + T )N when S and T commute, so the array sum is eS+T .
(ii) The second part can be (tediously) proved by making a power series expansion
as above (Exercise 13.19(8)). We defer the proof until we have better tools available
(Example 13.30(3)).
♦∃
Exercises 13.19
1. Calculate τ(T ) for the following matrices
(a)
⎠0 1
0 0
⎫
, (b)
⎠1 a
0 0
⎫
, (c)
⎠a 0
0 b
⎫
, (d)
⎠a 1
0 a
⎫
.
Only one of these examples satisﬁes τ(T ) = √T √.
2. Every idempotent P, except 0, satisﬁes τ(P) = 1; every nilpotent Q has
τ(Q) = 0, and every cyclic element T has τ(T ) = 1.
3. For any invertible S, τ(S−1T S) = τ(T ), yet √S−1T S√may be much larger than
√T √. For example, let P :=
⎠1 c
0 0
⎫
and S :=
⎠1 0
0 a
⎫
, then S−1PS =
⎠1 ac
0 0
⎫
has norm

1 + |ac|2.
4. If ST = T S, then τ(ST ) ⩽τ(S)τ(T ). Deduce τ(T −1)−1 ⩽τ(T ), and ﬁnd
examples of non-commuting matrices where τ(ST ) > τ(S)τ(T ).
5. Theequation T −AT B = C hasasolution T = ⎨∀
n=0 AnC Bn ifτ(A)τ(B) < 1.
6. The radii of convergence of
∀

n=0
nnT n,
∀

n=0
nT n,
∀

n=1
T n/n,
∀

n=0
T n/n!

294
13
Banach Algebras
are 0, 1, 1, ∀, respectively. A quick way of estimating the radius of convergence R
is to judge how fast the coefﬁcients grow: if c0rn
0 ⩽|an| ⩽c1rn
1 then 1
r1 ⩽R ⩽1
r0 .
7. How are the radii of convergence of ⎨
n(an + bn)T n and ⎨
n anbnT n related to
those of ⎨
n anT n and ⎨
n bnT n?
8. Let f (T ) := ⎨∀
n=0 anT n and g(T) := ⎨∀
n=0 bnT n. Find the power series
expansions of f + g, f g and f ◦g. In particular, ﬁnd the ﬁrst few terms of
−f (T ), f (T )−1, f −1(T).
9. Let f (T) := ⎨
n anT n be a power series, and F(T) := ⎨
n |an|T n; they have the
same radius of convergence R. If √T√< R, then √f (T)√⩽F(√T√).
10. The convergence of a power series is uniform in T , for √T √⩽r < R.
11. When T satisﬁes a polynomial p(T ) = 0, then every (convergent) power series on
T reduces to a polynomial in T .
12. (a) e0 = 1, (b) the inverse of eT is e−T , (c) enT = (eT )n.
13. By analogy with the complex case, deﬁne the hyperbolic and trigonometric
functions of T as power series, and show (a) e
⎩0 −1
1 0

x =
⎠
cos x −sin x
sin x
cos x
⎫
, (b)
cos
⎠1 1
0 1
⎫
x
=
⎠cos x −x sin x
0
cos x
⎫
,
(c)
eT
=
cosh T
+ sinh T ,
(d) eiT = cos T + i sin T .
14. Prove that there is a non-zero complex number ξ such that eξ = 1. Thus the
exponential function has a period, eT+nξ = eT . The smallest such number is
6.283 . . . i =: 2πi.
15. * (1 + T/n)n →eT as n →∀.
(Hint: each component in the series is 1
nk
⎩n
k

T k →1
k!T k, then use Exercise 9.7(1).)
16. * The product of n terms, (1 + S/n)(1 + T/n)(1 + S/n) · · · (1 + T/n) →eS+T
as n →∀. (At least show convergence for each power term.)
17. * Trotter formula: eS/neT/neS/n · · · eT/n →eS+T . For example,
eS+T ∇eS/2eT/2eS/2eT/2.
Find the exact coefﬁcients used in the Trotter-Suzuki approximation
e0.293Se0.707T e0.707Se0.293T ,
that make it the best possible to second order. These formulas are very useful to
approximate eS+T whenever S and T do not commute.

13.3 The Group of Invertible Elements
295
13.3 The Group of Invertible Elements
Among the invertible elements of a Banach algebra, one ﬁnds all the exponentials
eT (including all non-zero complex numbers) and all their products, as well as the
unit ball around 1, as the next key theorem proves:
Theorem 13.20
If τ(T ) < 1 then 1 −T is invertible, (1 −T )−1 = 1 + T + T 2 + · · ·
Proof The radius of convergence of the series ⎨
n T n is 1, by Hadamard’s formula.
For τ(T ) < 1, let SN := 1 + T + · · · + T N →⎨∀
n=0 T n. Then, remembering that
τ(T ) < 1 ⇒T N →0 as N →∀(Example 13.13(3)),
SN = 1 + T + · · · + T N
T SN =
T + · · · + T N + T N+1
⇒(1 −T )SN = 1
−T N+1 →1.
Similarly, SN(1 −T ) →1 as N →∀. This shows that ⎨∀
n=0 T n is the inverse of
1 −T .
♦∃
Theorem 13.21
The invertible elements of a Banach algebra X form a group G(X) with
the operation of multiplication. G(X) is an open set in X, and the map
T ⇒→T −1 is differentiable on it.
Proof Multiplication in a Banach algebra is associative and has a unity 1 ∞G(X).
To prove G(X) is a group, it needs to be shown that if S, T ∞G(X), then ST and
T −1 are invertible, a fact that is evident from
(ST )−1 = T −1S−1,
(T −1)−1 = T.
Let T be any invertible element of X, and consider any neighboring element
T + H = T (1 + T −1H)
with √H√< √T −1√−1. Then τ(T −1H) ⩽√T −1√√H√< 1, so that 1 + T −1H, and
by implication T + H, are invertible. As the neighboring points of T are invertible,
T is an interior point of G(X) and the group is open in X.
In fact, writing T + H = T (I + T −1H),
(T + H)−1 = (1 + T −1H)−1T −1 = T −1 −T −1HT −1 + T −1HT −1HT −1 + · · ·

296
13
Banach Algebras
This shows that T ⇒→T −1 is differentiable with derivative H ⇒→−T −1HT −1,
by verifying
√T −1HT −1√⩽√T −1√
2√H√
√T −1HT −1HT −1 + · · · √⩽
∀

n=0
√H√n+2√T −1√
n+3 =
√H√2√T −1√3
1 −√T −1√√H√= o(H).
♦∃
A group, for which the acts of multiplication and taking the inverse are differen-
tiable, is called a ‘Lie group’, a topic that has a vast literature devoted to it.
In particular note that for H = z1,
(T + z)−1 = T −1 −zT −2 + z2T −3 + · · · ,
(13.3)
and that the map z ⇒→T −z ⇒→(T −z)−1 is analytic wherever the inverse exists;
its derivative is (T −z)−2.
Examples 13.22
1. The group of N × N invertible complex matrices is often denoted GL(N, C).
It has a group-morphism, the determinant det : GL(N, C) →C× = G(C),
det AB = det A det B
whose kernel is the normal subgroup SL(N, C) of ‘special matrices’ with
determinant 1.
2. In C, when z is large, z−1 is small. But for general Banach algebras there is
no such relation between √T −1√and √T √, e.g.the inverse of (10, 0.01) ∞C2 is
(0.1, 100).
3. The set of non-invertible elements is closed in X. So the closure of a proper ideal
is a proper ideal.
Proof By Example 13.5(5), I ≤G(X)c, so I ⊂G(X)c and I does not contain 1.
4. If T is invertible, then Bϵ(T S) ⊂T Bϵ√T −1√(S). Consequently, multiplication by
T is an open mapping.
Proof Let √A −T S√< ϵ; then √T −1A −S√⩽√T −1√√A −T S√< √T −1√ϵ,
as required. If U is an open set in X and S ∞U, then S ∞Bϵ(S) ⊂U, so
T S ∞Bϵ/√T −1√(T S) ⊂T Bϵ(S) ⊂TU
and TU is open in X.
5. The set of non-invertible elements is path-connected (to the origin, say), and may
disconnect the group of invertible elements, e.g. GL(2, R) disconnects into the

13.3 The Group of Invertible Elements
297
two open sets of matrices whose determinants are strictly positive and strictly
negative, respectively.
The following proposition conﬁrms that as an invertible operator R approaches
the boundary of G(X), √R−1√grows to inﬁnity, as expected.
Proposition 13.23
Let T be on the boundary of the group of invertible elements.
(i) For any invertible element R, √R−1√⩾1/√R −T √,
(ii) T is a topological divisor of zero, meaning there are unit elements Sn
such that
T Sn →0 AND SnT →0, as n →∀.
Proof (i) Since T is at the boundary of the open set of invertible elements, it cannot
be invertible, whereas R and all elements in its surrounding ball of radius √R−1√−1
are invertible, by the proof of the previous theorem. Thus √R −T √⩾√R−1√−1 as
claimed.
(ii) Let invertible elements Rn converge to a boundary element T , and let Sn :=
R−1
n /√R−1
n √; then
√T Sn√= √T R−1
n √/√R−1
n √
= √(T −Rn)R−1
n
+ 1√/√R−1
n √
⩽√T −Rn√+ 1/√R−1
n √
⩽2√T −Rn√→0 as n →∀,
and similarly SnT →0 as well.
♦∃
As remarked earlier, the group G(X) need not be a connected set, but splits into
connected components, with, say, G1 being the component containing 1. Recall that
a component is maximal connected, so if G1 contains part of a connected subset of
G(X), it must contain all of it (Theorem 5.11).
Proposition 13.24
The component of invertible elements containing 1, is that open normal
subgroup generated by eT for all T .

298
13
Banach Algebras
Proof G1 is open in G(X): Any T ∞G1 is an interior point of G(X), so T ∞Bϵ(T ) ⊂
G(X). But the ball Bϵ(T ) is (path-)connected and intersects G1, so Bϵ(T ) ⊂G1.
G1 is a subgroup of G(X): Multiplication by T is a continuous operation, so T G1
is connected (Proposition 5.5). When T ∞G1, then T = T 1 ∞T G1 ⊂G(X), so G1
contains part, and therefore all, of T G1. Hence T, S ∞G1 ⇒T S ∞T G1 ⊂G1.
Similarly, inversion is a continuous mapping, so G−1
1
is connected; it contains 1, so
must be a subset of G1, i.e., T ∞G1 ⇒T −1 ∞G1.
G1 is a normal subgroup: By the same reasoning, for any invertible T , T −1G1T
is a connected subset of G(X) and contains 1, so it is a subset of G1 (in fact it must
equal it).
G1 is generated by the exponentials: Let E be the group generated by the exponen-
tials eT for all T ∞X; its elements are ﬁnite products eT · · · eS, and their inverses
are of the same type (eT · · · eS)−1 = e−S · · · e−T . It contains 1 = e0, and is con-
nected since there is a continuous path from 1 to every element eT · · · eS, namely
t ⇒→etT · · · etS for t ∞[0, 1]. We can conclude that E lies inside G1.
The elements near to 1 are all exponentials,1 1 + H = elog(1+H), and so a small
enough neighborhood around E := eT · · · eS ∞E consists of elements
E + H = E(1 + E−1H) = eT · · · eSelog(1+E−1H) ∞E
for √H√< e−√S√· · · e−√T √. This means that eT · · · eS is an interior point of E, which
is thus open. Its complement in G1 is also open, since G1\E = ⎬
T ∞G1∅E T E (prove!)
and each T E is open (Example 13.22(4)). E, being open and closed in G1, must equal
G1 (Proposition 5.3).
♦∃
Exercises 13.25
1. The invertible elements of CN are (z1, . . . , zN) such that none of the components
are zero.
2. In δ∀, a sequence (an) is invertible if, and only if, it is bounded away from 0,
i.e., 0 < c ⩽|an|. Paths t ⇒→w(t) in C[0, 1] are invertible when they do not
pass through 0.
3. In B(X), the invertible elements are the automorphisms of X.
4. In B(X), √T −1√= 1/ inf√x√=1 √T x√.
5. In X × Y, (S, T ) is invertible if, and only if, both S and T are invertible.
6. The integral operator on C[a, b], T f (y) :=
⎧b
a k(x, y) f (x) dx has norm satis-
fying √T √⩽√k√L∀|b−a|. Deduce that when √k√L∀< 1/|b−a|, the equation
T f + g = f has the unique solution f = ⎨∀
n=0 T ng.
7. If T is invertible and T x = y, (T + H)(x + xϵ) = y, then √xϵ√
√x√⩽√T −1√√H√
1−√T √√H√.
1 This was stated, not proved, in Proposition 13.18 but the argument is not circular.

13.4 Analytic Functions
299
8. The map t ⇒→etT is a differentiable group-morphism R →G(X); its derivative
at t is T etT .
9. * Conversely, every differentiable group-morphism A : R →G(X), meaning
At+s = At As, is of this type:
(a) ≈h > 0,
⎧h
0 At dt is invertible, by the mean value theorem (Proposition
12.9), and
⎧t+h
t
A = (
⎧h
0 A)At;
(b) Let T := (Ah −1)(
⎧h
0 A)−1, so that At+h = At + hT At + o(h);
(c)
d
dt (Ate−tT ) = ( d
dt At)e−tT −AtT e−tT = 0, so At = A0etT = etT .
10. Verify Proposition 13.23 for
⎠1 1
0 1
n
⎫
→
⎠1 1
0 0
⎫
.
11. A topological divisor of zero, also called a generalized divisor of zero, does not
have right or left inverses.
12. The right-shift operator R on δ∀is a right divisor of zero but not a topological
divisor of zero.
13. In ﬁnite dimensions, there is no distinction between divisors of zero and topo-
logical ones. (Hint: Sn ∞BX , which is compact.)
14. AnisomorphismbetweenBanachalgebraspreservestopologicaldivisorsofzero.
15. If R is invertible, then √R−1√⩾1/d(R, ∂G(X)). (Hint: By the deﬁnition of
d(μ, ∂G(X)) (Example 2.20(9)), there is a sequence Tn ∞∂G(X) such that
√Tn −R√→d(R, G(X)).)
13.4 Analytic Functions
There are two ways of connecting the coefﬁcients of a power series to its function
f (z) = a0 + a1z + a2z2 + · · · ,
(i) by differentiation
f (n)(z) = n!an + (n + 1)!an+1z + · · ·
⇒
f (n)(0) = n! an.
(ii) by integration
f (z)
zn
= a0
zn + · · · + an−1
z
+ an + · · ·
⇒

f (z)
zn
dz = 2πi an−1.
These formulas raise the possibility of creating a power series from a given function,
by deﬁning the coefﬁcients in these ways. The latter one is more useful because it
does not assume f to be differentiable inﬁnitely often.

300
13
Banach Algebras
Theorem 13.26
Taylor series
If f : C →C is analytic in a disk BR(0), then it is a power series inside
the disk. For τ(T ) < R,
f (T ) :=
1
2πi

f (z)(z −T )−1 dz =
∀

n=0
anT n,
where
an =
1
2πi

f (z)z−1−n dz = f (n)(0)
n!
,
⇐n ∞N
and
⇐r < R, ≈cr, ⇐n ∞N,
|an| ⩽cr
rn .
Proof The path of integration is along a circle with center 0 and radius r just less
than R (but larger than τ(T )). For z on this circle, τ(T/z) = τ(T )/r < 1, so
(z −T )−1 = z−1(1 −T/z)−1 =
∀

n=0
z−1−nT n, and
1
2πi

f (z)(z −T )−1 dz =
∀

n=0
1
2πi

f (z)z−1−n dz T n =
∀

n=0
anT n.
However we need to justify the swap of the summation with the integral. Recall that
z ⇒→(z −T ) ⇒→(z −T )−1 is continuous in z by (13.3), and the circle is a compact
set, so √f (z)(z −T )−1√⩽C for z on the circle (Corollary 6.16). It follows that
⎪⎪
∀

n=N
f (z)T n/zn+1⎪⎪= √T N f (z)(z −T )−1/zN+1√⩽C√T N√/r N+1 →0
uniformly in z. So ⎨N
n=0

f (z)T n/zn+1 dz →
 ⎨
n f (z)T n/zn+1 dz.
Note that
|an| ⩽1
2π

c/rn+1dt = c/rn,
where c is the maximum value of f on the compact disk Br[0] ≤C. The radius of
convergence of this power series is at least R since
lim inf
n
|an|−1/n ⩾lim
n→∀
r
c1/n = r,
⇐r < R.

13.4 Analytic Functions
301
To justify the use of the notation f (T ), we need to show that when T is a complex
number a1, the two uses of the symbol f agree, i.e., f (a1) = f (a)1; but this is just
Cauchy’s integral formula, f (a) =
1
2πi

f (z)/(z −a) dz. Consequently an analytic
function is indeed a power series.
♦∃
Proposition 13.27
Liouville’s theorem
If an analytic function on C grows polynomially | f (z)| ⩽c|z|n, then f is
a polynomial of degree at most n. In particular, if f is bounded then it is
constant.
Proof If f : C →C were analytic on C, and grows polynomially, then its maximum
value on a disk of radius r is cr ⩽crn. So the mth Taylor coefﬁcient vanishes for
m > n,
|am| ⩽cr/rm ⩽crn−m →0
as r →∀.
This also applies to vector-valued analytic functions F : C →X. For any func-
tional β ∞X∈, β ◦F : C →C is also analytic. If F grows polynomially, then so
does β ◦F
|β ◦F(z)| ⩽√β√√F(z)√⩽√β√c|z|n,
which implies that β◦F(z) is a polynomial a0+a1z+· · ·+anzn. In fact, by Example
12.3(3), an = β ◦F(n)(0)/n!, so that
β ◦F(z) = β ◦(F(0) + F∗(0)z + · · · + F(n)(0)zn/n!).
As β is arbitrary, we deduce that F(z) is a polynomial in z.
♦∃
Theorem 13.28
Laurent series
If f : C →C is analytic in a ring BR(0) \ Br[0], and r < τ(T −1)−1 ⩽
τ(T ) < R, then
f (T ) :=
1
2πi

f (z)(z −T )−1 dz =
∀

n=−∀
anT n,
where an =
1
2πi

f (z)z−1−n dz, ⇐n ∞Z. The residue of f in Br[0] is a−1.
The path of integration is here understood to be just within the boundary of the
ring, going counter-clockwise around a circle of radius just smaller than R, and

302
13
Banach Algebras
clockwise around a circle just larger than r. Note that R is allowed to be inﬁnite, in
which case substitute R with any value larger than τ(T ).
Proof A Laurent series can be thought of as the sum of two separate power series,
⎨∀
n=0 anT n + ⎨∀
n=1 a−nT −n, one in T and the other in T −1. If R and R∗are
the respective radii of convergence, then absolute convergence occurs only when
τ(T ) < R and τ(T −1) < R∗.
For z on the bigger circle, τ(T/z) = τ(T )/|z| < 1 if the radius is close enough
to R, so just like the proof of the Taylor series,
1
2πi

1
f (z)(z −T )−1 dz =
∀

n=0
anT n.
For z on the smaller circle, τ(zT −1) = |z|τ(T −1) < 1 when its radius is close
enough to r, so
(z −T )−1 = −(1 −zT −1)−1T −1 = −
∀

n=0
znT −n−1,
and (along an counter-clockwise path)
1
2πi

2
f (z)(z −T )−1 dz = −
∀

n=1
1
2πi

f (z)zn−1 dz T −n = −
∀

n=1
a−nT −n.
Combining the two integrals and series gives Laurent’s expansion. Note that the
second series vanishes when f is analytic within Br(0), by Cauchy’s theorem, so it
is consistent with Taylor’s theorem.
Since the Laurent series converges uniformly strictly within the annulus, we obtain
1
2πi

f (z) dz =
1
2πi
∀

n=−∀

anzn dz = a−1.
♦∃
These two theorems of course also apply, by translating, to disks and rings with
center z0; the resulting series will then be ⎨
n an(T −z0)n.
Proposition 13.29
The zeros of a non-zero analytic function, deﬁned on an open connected
subset of C, are isolated.

13.4 Analytic Functions
303
Proof Suppose an interior zero w of f :  →C is a limit point of other zeros,
zn →w (zn ⊆= w). Then f can be written as a power series f (z) = ⎨
k ak(z −w)k
in some neighborhood of w. If aK is the ﬁrst non-zero coefﬁcient, then
0 = f (zn) = (zn −w)K (aK + aK+1(zn −w) + · · · ),
∴0 = aK + aK+1(zn −w) + · · · →aK as zn →w.
This contradiction determines that f is locally zero in . Hence it is zero in 
(Exercise 5.7(9)).
♦∃
Examples 13.30
1. The Fourier series ⎨∀
n=−∀aneinθ is a Laurent series with T = eiθ.
2. ▶For polynomials (and circular paths as in the theorems),
p(T ) =
1
2πi

p(z)(z −T )−1 dz.
For example,
1 =
1
2πi

(z −T )−1 dz,
T =
1
2πi

z(z −T )−1 dz,
T −1 =
1
2πi
 1
z (z −T )−1 dz.
Proof for T −1: We can use Laurent’s expansion on a path z(θ) = reiθ, since 1/z
is analytic everywhere except at 0,
an =
1
2πi

1
zn+2 dz = 1
2π
 2π
0
1
rn+1 e−i(n+1)θ dθ = 0
unless n = −1, in which case a−1 = 1. So ⎨
n anT n = T −1.
3. ▶We can ﬁnally show elog(1+T ) = 1 + T for τ(T ) < 1.
Proof Let f (z) := elog(1+z) for |z| < 1; then f ∗(z) = elog(1+z)/(1 + z) and
f ∗∗(z) = 0 (check!). So the non-zero coefﬁcients of its Taylor series are a0 =
f (0) = e0 = 1 and a1 = f ∗(0) = 1. Hence f (T ) = 1 + T .
4. Binomial theorem: (1 + T )p := ep log(1+T ) = 1 + pT +
⎩p
2

T 2 + · · · provided
τ(T ) < 1, p ∞C, and
⎩p
n

:= p(p−1)···(p−n+1)
n!
.

304
13
Banach Algebras
Proof Deﬁne the analytic function f (z) := (1 + z)p = ep log(1+z) inside the unit
disk BC. Its derivatives are, by induction,
f (n)(z) = p(p −1) · · · (p −n + 1)e(p−n+1) log(1+z)(1 + z)−1
= p(p −1) · · · (p −n + 1)(1 + z)p−n,
so its power series coefﬁcients are an = f (n)(0)/n! =
⎩p
n

.
5. ▶There are versions of these series expansions valid for a vector-valued func-
tion F : C →X, where X is a Banach space and F is analytic inside a ring,
r < |z| < R,
F(z) =
1
2πi

F(w)(w −z)−1 dw =

n
Anzn,
where An :=
1
2πi

F(w)w−1−n dw ∞X.
Proof For any β ∞X∈, the map β ◦F : C →C, being the composition of dif-
ferentiable functions, is analytic on the ring BR(0) \ Br[0], so it has a Laurent
expansion β ◦F(z) =
1
2πi

β ◦F(w)(w −z)−1 dw = ⎨
n bnzn for r < |z| < R
and bn = βAn. But β is linear and continuous, so it can be extracted out of the
integrals and series,
β ◦F(z) = β
⎠1
2πi

F(w)(w −z)−1 dw
⎫
= β

n
Anzn,
and as β is arbitrary, the result follows.
Exercises 13.31
1. Let T :=
⎠0 1
0 0
⎫
; verify directly that T =
1
2πi

z(z −T )−1 dz by calculating
the integral in a circular path around the origin.
2. Show that there are no analytic functions in C which grow at a fractional power
rate |z|m/n (m/n ⊆∞N).
3. Show that the Laurent series for cot T , valid for τ(T ) < π, τ(T −1) > 0, is
cot T = T −1 −1
3T −1
45T 3 −
2
945T 5 −· · · ,
and ﬁnd its residue at 0. (Hint: cot z = (1−z2/2+z4/24+· · · )/z(1−z2/6+· · · ).)
4. If an identity between analytic functions, f (z) = g(z), holds in a complex disk
BR(0), then it holds for any T with τ(T ) < R.

13.4 Analytic Functions
305
5. Justify the identity n log(1 + T ) = log(1 + T )n, hence deduce the assertion
lim
n→∀(1 + T/n)n = eT .
6. A function on C has a pole a of order N if, and only if, it has a Laurent series
expansion ⎨∀
n=−N an(z −a)n about a; its residue is a−1.
7. * Two analytic functions on an open connected subset of C must be identically
equal if they are equal on an interior disk. (Consider the interior of the set for
which f = g.)
8. Suppose f is analytic on the extended complex plane, except for isolated points,
i.e., f (1/z) is also analytic at 0.
(a) Show that f has a ﬁnite number of zeros and poles (except for f = 0),
(b) Using polynomials p, q whose roots are these zeros and poles, respectively,
deduce that f is a rational function p/q.
Remarks 13.32
1. A subalgebra must have the same unity as the algebra—it is not enough that it
has a unity. For example, C (Exercise 13.10(2)) contains the set { (0, a) : a ∞C }
which is closed under addition and multiplication and has its own unity (0, 1),
different from C’s unity (1, 0); it is an algebra, but not a subalgebra of C. Instead,
the set { (a, 0) : a ∞C } is a subalgebra of C.
2. The axiom π1 = 1 of an algebra morphism does not follow from the other
properties of π. For example, the map π : C →C deﬁned by π(z) :=
(0, z) satisﬁes all the properties of a Banach algebra morphism, except that
π(1) = (0, 1) ⊆= (1, 0). But continuity of characters follows from their other
properties (Proposition 14.34).
3. * The proof of the embedding of X into B(X) does not make essential use of the
axiom √1√= 1, or of √ax√⩽√a√√x√. If instead, √1√= c and √ax√⩽c∗√a√√x√,
one gets
√a√= √La1√⩽c√La√,
√La√⩽c∗√a√.
Thus X has an equivalent norm deﬁned by |||a||| := √La√, with |||1||| = √I√= 1
and
|||xy||| = √Lxy√= √Lx L y√⩽√Lx√√L y√= |||x||| |||y|||.
4. In the Banach algebra B(X), one can deﬁne τx(T ) := lim supn √T nx√
1
n ; so
0 ⩽τx(T ) ⩽τ(T ). The series ⎨
n anT nx converges absolutely when τx(T ) is
less than the radius of convergence.

Chapter 14
Spectral Theory
A moment’s reﬂection shows that, by Cauchy’s residue theorem, the path of
integration in f (T ) =
1
2∂i

f (z)(z −T )−1 dz can be modiﬁed, as long as f and
(z −T )−1 remain analytic over the swept region. We are thus led to study the region
where z −T is not invertible, called the spectrum of T .
Deﬁnition 14.1
The spectrumof an element T in a Banach algebra is deﬁned as the set
δ(T ) := {α ∞C : T −α is not invertible}.
Its complement C \ δ(T ) is called the resolventof T .
Examples 14.2
1. δ(z) = {z} (since z −α is not invertible when α = z).
2. ▶Recall that a square matrix A is non-invertible √A is not 1–1 √det A = 0.
The spectrum of an n × n matrix consists of its eigenvalues, i.e., the roots of the
characteristicpolynomial equation det(T −α) = 0 of degree n.
For example, the spectra of the 2 × 2 matrices
⎛0 1
0 0
⎜
,
⎛0 0
1 0
⎜
,
⎛0 1
1 0
⎜
, and
⎛a 0
0 b
⎜
, are {0}, {0}, {−1, 1}, and {a, b} respectively.
Note that it is possible to have different elements with the same spectrum. The
spectrum is a sort of ‘shadow’ of T — it yields important information about T ,
but need not identify it.
3. ▶The spectrum of a sequence x = (an) ∞γ∗is δ(x) = im x = {an : n ∞N}.
Proof The inverse of the sequence x−α = (an−α) is bounded iff |an−α| ⩾c > 0
for all n, hence α ⇒∞δ(x) √α is an exterior point of {an}.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_14,
307
© Springer International Publishing Switzerland 2014

308
14
Spectral Theory
4. A spectral value of an operator T ∞B(X) is a complex number α for which the
equation (T −α)x = y is not well-posed; one sometimes sees in practice that as
one varies a parameter α of a model, some speciﬁc values have unstable solutions
that ‘resonate’.
5. (a) ▶Translations, ‘rotations’ (in the sense of multiplication by eiξ) and scaling
of T have corresponding actions on its spectrum:
δ(T + z) = δ(T ) + z,
δ(zT ) = zδ(T ),
since (T + z) −α = T −(α −z), so α ∞δ(T + z) √α −z ∞δ(T ); for
z ⇒= 0, (zT ) −α = z(T −α/z), so α ∞δ(zT ) √α/z ∞δ(T ).
(b) If T is invertible, then δ(T −1) = δ(T )−1 := {α−1 : α ∞δ(T )}, since
T −1 −α = −α T −1(T −α−1), so α ∞δ(T −1) √α−1 ∞δ(T ) (note that
α ⇒= 0).
(c) The matrices S :=
⎛0 1
0 0
⎜
and T :=
⎛0 0
1 0
⎜
show that there is no simple
relation between δ(S + T ) or δ(ST ) and δ(S) and δ(T ) in general.
(d) δ(ST ) = δ(T S) →{0} or δ(ST ) = δ(T S) \ {0}.
Proof For α ⇒= 0 and ST −α invertible, (T S−α)−1 = 1
α(T (ST −α)−1S−1),
since
(T S −α)(T (ST −α)−1S −1) = T (ST −α)(ST −α)−1S −(T S −α) = α,
(T (ST −α)−1S −1)(T S −α) = T (ST −α)−1(ST −α)S −(T S −α) = α.
Thus, δ(T S) ♦δ(ST ) →{0}; indeed, reversing the roles of S and T shows
δ(T S) →{0} = δ(ST ) →{0}.
(e) In particular, δ(S−1T S) = δ(T ).
Example: Quadratic Forms
Extracting the spectrum of matrices is one of the most useful application of
mathematics. Quadratic forms are expressions of degree 2 in a number of variables,
such as
q(x, y, z) = ax2+by2+cz2+dxy+eyz+ f zx = (x y z)
⎝
⎞
a
d/2 f/2
d/2
b
e/2
f/2 e/2
c
⎟
⎠
⎝
⎞
x
y
z
⎟
⎠.
They are found in the equations of conics and quadrics, the fundamental forms
of surface geometry, the inertia tensor and stress tensor of mechanics, the integral
forms of number theory, the covariances of statistics, etc. They can always be written
as q(x) = x∃Ax, with A a symmetric matrix. We will see later that when the
coefﬁcients are real, such matrices have real eigenvalues, α1, . . . , αN, and there
exists an orthogonal matrix P such that P−1 AP = D, where D consists solely of the

14 Spectral Theory
309
eigenvaluesonthemaindiagonal.Sotheorthogonaltransformation x ∀∈˜x := P−1x
gives a new simpliﬁed quadratic form
q(x) = x∃Ax = ˜x∃P∃AP ˜x = ˜x∃D ˜x = α1 ˜x2
1 + · · · + αN ˜x2
N =: ˜q(˜x).
These eigenvalues are intrinsic to the quadratic form, in the sense that any rotation
of the variables gives a quadratic form with the same spectrum, and so represent real
information about it rather than about the choice of variables. Not surprisingly these
values were discovered before the connection with linear algebra became clear, and
called by a variety of names such as “principal curvatures”, “principal moments”,
“principal component variances”, etc., in the different contexts. For example, a conic
that satisﬁes the equation ax2 + bxy + cy2 = 1 can also be represented by the
equation α˜x2 + μ ˜y2 = 1, where (˜x, ˜y) are obtained by a rotation/reﬂection of (x, y).
Hence there are four types, depending on the signs of α, μ: ellipses, hyperbolas,
parallel lines, or the empty set.
14.1 The Spectral Radius
Determining the exact spectral values of an element is usually a non-trivial problem.
The fundamental theorem for the general case is:
Theorem 14.3
The spectrum of T is a non-empty compact subset of C. The largest extent
of δ(T ), called the spectral radiusof T , is
max{|α| : α ∞δ(T )} = ρ(T ) = lim
n∈∗⊆T n⊆
1
n .
Proof δ(T ) is compact: If |α| > ρ(T ), then ρ(T/α) = ρ(T )/|α| < 1, so T −α =
−α(1 −T/α) is invertible (Theorem 13.20). Spectral values are therefore bounded
by ρ(T ).
The resolvent set is none other than f −1G(X) where f (z) := T −z, and G(X)
is the set of invertible elements of X. Since G(X) is open in X and f is con-
tinuous, it follows that the resolvent is open (Theorem 3.7), and the spectrum is
closed in C. More concretely, if T −α is invertible, and z is close enough to α, then
|z−α| = ⊆(T −z) −(T −α)⊆implies that T −z is also invertible (Theorem13.21).
The spectrum δ(T ), being a closed and bounded set in C, is compact (Corollary
6.20).
δ(T ) is non-empty: Applying Theorem 13.26, with f (z) := 1, and a circular path
centered at the origin with radius larger than ρ(T ), gives
1 =
1
2∂i
⎫
(z −T )−1 dz.

310
14
Spectral Theory
But the map z ∀∈(z −T )−1 is analytic on C\δ(T ) by (13.3). This would contradict
Cauchy’s theorem (Theorem 12.16) were the spectrum empty.
The spectral radius is ρ(T ): Let rδ be the largest extent of δ(T ), and consider
the function f : z ∀∈(z −T )−1; it is analytic on C\δ(T ), in particular on C\Brδ [0].
So it has a Laurent series ⎪
n Anzn, valid for all |z| > rδ (Example 13.30(5)). On
the other hand, we know that
(z −T )−1 = 1
z (1 −T/z)−1 =
∗
⎬
n=0
T n
z1+n
for|z| > ρ(T ).
The two series must be identical, ⎪∗
n=−∗Anzn = ⎪∗
n=0 T n/zn+1, and remain valid
forall|z| > rδ.Butthesecondseriesdivergeswhenρ(T ) > lim infn |z−n|−1/n = |z|
by the Cauchy-Hadamard theorem, so there can be no z ∞C such that rδ < |z| <
ρ(T ), in other words, rδ = ρ(T ).
⊓⇐
This is a surprising result: one might expect ρ(T ) to depend on the speciﬁc norm
used for a square matrix T , but the spectrum of T consists of its eigenvalues, which
are determined by an algebraic equation.
Corollary 14.4 Fundamental Theorem of Algebra
Every non-constant polynomial in C has a root.
Proof The roots of the polynomial equation zn + an−1zn−1 + · · · + a0 = 0 are
precisely the spectral values of the matrix
⎝
⎭⎭⎭⎭⎞
0
0
−a0
1 0
0
−a1
0
0
0
0 1 −an−1
⎟
⎧⎧⎧⎧⎠
.
⊓⇐
Examples 14.5
1. The smallest extent of δ(T ) is ρ(T −1)−1 when T is invertible (otherwise it is 0).
Thus the condition r < ρ(T −1)−1 ⩽ρ(T ) < R for a Laurent series expansion
to exist (Theorem 13.28) can be restated as “the spectrum of T lies inside the ring
with radii r and R”.
2. ▶Every Banach division algebrais isomorphic to C (Gelfand-Mazur theorem).
Proof Adivisionalgebraisdeﬁnedasoneinwhichtheonlynon-invertibleelement
is 0. Hence T −α is not invertible precisely when T = α ∞C1. But δ(T ) is
non-empty, so this must be the case for some α.

14.1 The Spectral Radius
311
3. ▶Every Banach algebra, except C, has non-zero topological divisors of zero.
Proof Suppose that the only topological divisor of zero is 0. Since the spectrum
δ(T ) of every T has a non-empty boundary (Proposition 5.3), there is a T −α
which is a topological divisor of zero, so T = α ∞C1.
4. ▶Every commutative Banach algebra, except C, has non-trivial ideals.
Proof Suppose the only ideals are {0} and X. Then the ideal generated by T ⇒= 0,
namely X T (in a commutative algebra), must equal X. It follows that ST = 1
for some S ∞X, and T is invertible. But the only Banach division algebra is C.
5. A morphism J : X ∈Y may only decrease the spectrum of an element, since
a non-invertible element in X may become invertible in Y, but an invertible in
X cannot become non-invertible in Y. If J is an embedding, the boundary of
the spectrum in X, consisting of topological divisors of zero, is preserved in
Y (Exercise 13.25(14)). The spectrum may decrease but its boundary (and the
spectral radius) does not.
6. Recall the commutant algebra Y := A⊂⊂♦X when the elements of A commute
(Exercise 13.10(14)). By part (c) of that exercise, for any T ∞Y, if T −α is
invertible in X then its inverse is in Y, so δY(T ) = δ(T ).
Little else can be said about spectra of general elements of an algebra. The fol-
lowing proposition shows that the spectrum δ(T ) depends somewhat ‘continuously’
on T :
Proposition 14.6
If Tn ∈T , then
⇔π > 0, ∩N,
n ⩾N ∪δ(Tn) ♦δ(T ) + Bπ(0).
Br(0)
U
σ(T )
Proof Let U be any open subset of C containing δ(T ),
for example δ(T ) + Bπ(0). It is claimed that for all
z /∞U, ⊆(T −z)−1⊆⩽c When |z| ⩾r > ⊆T ⊆,
⊆(T −z)−1⊆=
⎨⎨
∗
⎬
n=0
T n
zn+1
⎨⎨⩽
∗
⎬
n=0
⊆T ⊆n
rn+1 =
1
r −⊆T ⊆,
while on the remaining closed and bounded set Br[0] \ U, the continuous function
z ∀∈⊆(T −z)−1⊆is bounded (Corollary 6.16). If ⊆T −S⊆< 1
c, then when z /∞U,
⊆(T −z)−1(T −S)⊆< 1. This implies that
S −z = (T −z) −(T −S) = (T −z)(1 −(T −z)−1(T −S))

312
14
Spectral Theory
is invertible (Theorem 13.20). Thus δ(S) ♦U, and we have shown that any open
set that contains δ(T ) also contains δ(S) for S close enough to T .
For example, if U
:= δ(T ) + Bπ(0) and Tn is close enough to T , then
δ(Tn) ♦U.
⊓⇐
Exercises 14.7
1. The spectrum of (z1, . . . , zN) ∞CN is {z1, . . . , zN}.
2. The spectrum of f ∞C[0, 1] is δ( f ) = im( f ).
3. Verify directly that for a matrix A with eigenvalue α, A −α is a divisor of zero.
4. * Prove that δ(T 2) = δ(T )2 = {α2 : α ∞δ(T )}. (We will see later a broad
generalization of this (Theorem 14.25)).
5. Show that δ(L R) = {1}, but δ(RL) = {0, 1}, where L and R are the shift
operators.
6. Show that ST −T S = z ⇒= 0 for S, T ∞X implies δ(ST ) is unbounded, which
is impossible (Hint: α ∞δ(ST ) ∪α + z ∞δ(ST )).
7. The spectrum of (S, T ) ∞X × Y is δ(S) →δ(T ).
8. If T ∞B(X) and S ∞B(Y), let T ⊙S : X × Y ∈X × Y be deﬁned by
T ⊙S(x, y) := (T x, Sy). Then δ(T ⊙S) = δ(T ) →δ(S).
9. If α is a boundary point of the spectrum, then T −α is at the boundary of G(X),
and so is a topological divisor of zero (Proposition 13.23). Moreover, if T −μ is
invertible, then
⊆(T −μ)−1⊆⩾1/d(μ, δ(T )).
14.2 The Spectrum of an Operator
An operator T on a Banach space X is invertible in B(X) when T has a continuous
and linear inverse T −1 ∞B(X). By the open mapping theorem, this is automatically
true once T is bijective. So an operator T ∞B(X) is not invertible when one of the
following cases holds:
T not invertible in B(X)
T not 1-1
T is 1-1 but not onto
im T = X
im T = X

14.2 The Spectrum of an Operator
313
• T is not 1–1 (i.e., ker T ⇒= 0). In this case, T is a left divisor of zero as T S = 0
for any non-zero S ∞B(X) with im S ♦ker T .
• T is 1–1, but not onto, yet it is “almost” onto, in the sense that its image is dense,
im T = X. Here, it cannot be the case that ⊆T x⊆⩾c⊆x⊆for all x and some c > 0,
otherwise im T would be closed (Example 8.13(3)) and T onto. This means that
one can decrease ⊆T x⊆but keep ⊆x⊆ﬁxed, i.e., there are unit vectors xn such that
T xn ∈0. By taking any unit operators with im Sn = [[xn]], we get T Sn ∈0, so
T is a topological left divisor of zero.
• T is 1–1, and its image is not even dense in X. In this case, by Proposition 11.18,
there is a non-zero S ∞B(X) with kernel containing im T , so ST = 0, and T is
a right divisor of zero.
The spectrum of an operator T ∞B(X) thus consists of α in:
• the point spectrum δp(T ), when T −α is not 1–1, i.e., T x = αx for some x ⇒= 0;
we say that α is an eigenvalue and x an eigenvector of α (note that a non-zero
multiple of an eigenvector is another eigenvector, so they are often taken to be of
unit length); the subspace ker(T −α) of eigenvectors of α (together with the zero
vector) is called its eigenspace.
• the continuous spectrum δc(T ), when T −α is 1–1, not onto, but im(T −α) = X.
• the residual spectrum δr(T ), when T −α is 1–1, and im(T −α) ⇒= X.
Proposition 14.8
Eigenvectors of distinct eigenvalues are linearly independent.
Proof Let vi ⇒= 0 be eigenvectors associated with the distinct eigenvalues αi,
i = 1, 2, . . ., so that (T −α)vi = (αi −α)vi. The sum ⎪N
i=1 βivi = 0 implies
0 = (T −α2) · · · (T −αN)
⎬N
i=1 βivi
= (T −α2) · · · (T −αN−1)
⎬N
i=1 βi(αi −αN)vi
= (T −α2) · · · (T −αN−1)
⎬N−1
i=1 βi(αi −αN)vi
= · · · = β1(α1 −α2) · · · (α1 −αN)v1
forcing β1 = 0. Since the argument can be repeated for any other index i, we have
βi = 0.
⊓⇐

314
14
Spectral Theory
Proposition 14.9
If α is a limit of eigenvalues, or is in δc(T ), or is a boundary point of δ(T )
, then α is an approximate eigenvalue, meaning there are unit vectors xn ,
such that
(T −α)xn ∈0 as n ∈∗.
Proof If αn ∈α and T xn = αnxn with ⊆xn⊆= 1, then
(T −α)xn = (αn −α)xn ∈0.
α is an approximate eigenvalue exactly when T −α is a topological left divisor
of zero, because suppose there are unit operators Sn with (T −α)Sn ∈0. Let xn
be vectors such that ⊆Snxn⊆= 1 and ⊆xn⊆⩽2 (possible since ⊆Sn⊆= 1); then
(T −α)Snxn ∈0, and α is an approximate eigenvalue.
Conversely, given (T −α)xn ∈0 with xn unit vectors, let Sn := xnω for any
ω ∞X◦with unit norm. Then ⊆Sn⊆= 1 and (T −α)Sn = (T −α)xnω ∈0 as
n ∈∗.
This includes the case when α is at the boundary of δ(T ) (Proposition 13.23),
and when α ∞δc(T ) as we have just seen at the beginning of this section.
⊓⇐
Examples 14.10
1. ▶The spectrum of the left-shift operator L(an) := (an+1), on γ∗is the unit
closed ball.
Proof The norm of L is 1, so δ(L) ♦B1[0]. To ﬁnd its eigenvalues, we need to
solve Lx = αx for some non-zero x = (an) ∞γ∗, i.e.,
⇔n, an+1 = αan,
|an| ⩽c.
This recurrence relation gives an = αna0, satisfying |a0||α|n = |an| ⩽c. Thus
the only possible candidates for eigenvalues are |α| ⩽1. In fact, for any such α,
the sequence (1, α, α2, . . .) is an eigenvector in γ∗. Hence δ(L) = B1[0], and
all spectral points are eigenvalues.
2. ▶The spectrum of the left-shift operator on γ1 is the unit closed ball.
Proof The same analysis as in Example 1 applies: ρ(L) ⩽⊆L⊆= 1, and an =
αna0. This time, the condition x ∞γ1 is ⎪
n |an| = |a0| ⎪
n |α|n < ∗. This
is only possible when |α| < 1. Once again, but only for |α| < 1, the sequence
(1, α, α2, . . .) is an eigenvector in γ1. Still, since it is closed, bounded by 1, and
contains B1(0), the spectrum must be the closed disk. The spectral values in
the interior are eigenvalues, and those on the circular perimeter are approximate
eigenvalues.

14.2 The Spectrum of an Operator
315
3. Let T : γ2 ∈γ2 be the multiplier operator T (an) := (bnan) where bn are
bounded. Its eigenvalues are bn, and its spectrum is K := {b1, b2, . . .}.
Proof For eigenvalues, T (an) = (bnan) = α(an), so (bn −α)an = 0 for all n.
This implies α = bn for some n, otherwise (an) = 0. In fact, T en = bnen, so bn
is indeed an eigenvalue. Now, suppose α is not a limit point of {b1, b2, . . .}; there
is then a minimum positive distance between α and K, i.e., |α −bn| ⩾d > 0.
So the equation (T −α)(an) = (cn) can be inverted, an = cn/(bn −α), with
|an| ⩽|cn|/d; ⊆(T −α)−1⊆⩽1/d. The spectrum therefore must include the
eigenvalues and their limit points, but nothing else.
4. Let T : L∗[0, 1] ∈L∗[0, 1] be deﬁned by T f (x) :=
⎩1
1−x f (s) ds. Then T is
linear, and continuous with ⊆T ⊆⩽1 since
⊆T f ⊆L∗= sup
x∞[0,1]

 1
1−x
f (s) ds
 ⩽⊆f ⊆L∗sup
x∞[0,1]
 1
1−x
ds = ⊆f ⊆L∗.
For eigenvalues, we need to solve
⎩1
1−x f (t) dt = αf (t). Differentiating twice
gives f ⊂⊂(x) + 1
α2 f (x) = 0 with boundary conditions f (0) = 0 = f ⊂(1). Thus
the eigenvectors (or “eigenfunctions”) are f (x) = sin(x/α) with eigenvalues
α = 2/k∂, k odd. The spectrum must also include 0, because it is their limit
point, but at this stage we cannot conclude anything further about the spectrum.
5. If S : X ∈Y, T : Y ∈X are operators, then ST and T S share the same
non-zero eigenvalues.
Proof If ST x = αx (x ⇒= 0), then T S(T x) = T (ST )x = α(T x), so either
T x = 0, in which case α = 0, or T x is an eigenvector of T S with the same
eigenvalue α; similarly, every non-zero eigenvalue of T S is also an eigenvalue of
ST . (Compare with Example 14.2(5d).)
6. Gershgorin’s theorem:If T = [Ti j] is an operator on c0, then each eigenvalue
belongs to a disk Br[Tj j] for some j, where r := ⎪
i⇒= j |Tji|.
Proof Let x = (ai) be an eigenvector of T and let |a j| be its largest coefﬁcient.
Then rearranging T x = αx we get
αa j =
⎬
i
Tjiai = Tj ja j +
⎬
i⇒= j
Tjiai,
∴|α −Tj j||a j| ⩽
⎬
i⇒= j
|Tji||ai| ⩽r|a j|.
7. Real eigenvalues of real operators have real eigenvectors, i.e., if X is a real Banach
space, then T ∞B(X) is not guaranteed to have a spectral element, but it will have
when considered as an operator on the complex space X + i X. Nevertheless if

316
14
Spectral Theory
the eigenvalue is real, with eigenvector u +iv, then u and v are also eigenvectors
(unless 0),
T (u + iv) = α(u + iv) ∪T u = αu, T v = αv.
The Spectrum of the Adjoint
There is a relation between the eigenvalues of T ∃and the residual spectrum of T :
Proposition 14.11
δ(T ∃) = δ(T )
δr(T ) ♦δp(T ∃) ♦δp(T ) →δr(T ).
δc(T ∃) ♦δc(T )
Proof (i) T −α is invertible in B(X), if and only if, its adjoint is invertible (Exercise
11.32(7)),
(T ∃−α)−1 = (T −α)−1∃.
So α ⇒∞δ(T ) √α ⇒∞δ(T ∃).
(ii) By deﬁnition, α ∞δp(T ∃) when there is a ω ⇒= 0 in X◦such that
ω ≈(T −α) = (T ∃−α)ω = 0.
This implies there is an x ∞X, ωx ⇒= 0, so that x ⇒∞im(T −α). In turn, if x ∞
X \ im(T −α) exists, then there is a ω ⇒= 0 such that ω(T −α) = 0 (Proposition
11.18), and we have proved
α ∞δp(T ∃) √im(T −α) ⇒= X.
This condition is certainly satisﬁed when α is a residual spectral value of δ(T ), but
not when it is in the continuous spectrum of T , so
α ∞δr(T ) ∪α ∞δp(T ∃) ∪α ⇒∞δc(T ).
(iii) When A∃is 1–1 but im A∃= X◦, then we can infer, by Proposition 11.30,
that (a) (ker A)≡⊇im A∃= X◦, so A is 1–1; and (b) (im A)≡= ker A∃= 0, so
im A = X. Applying this to A := T −α when α ∞δc(T ∃), we ﬁnd that T −α is
1–1 and has a dense image, that is, α ∞δc(T ).
⊓⇐
Examples 14.12
1. When T ∃∃= T (e.g. on a Hilbert space) then δr(T ∃) ♦δp(T ) as well as
δc(T ∃) = δc(T ).

14.2 The Spectrum of an Operator
317
2. In c0 (or γ2), the left-shift and right-shift operators have
δp(L) = B1(0), δr(L) = ∅,
δc(L) = S1,
δp(R) = ∅,
δr(R) = B1(0), δc(R) = S1.
Proof That δp(L∃) = ∅has already been shown since L∃is the right shift on
γ1; in the same way can be proved δp(L) = B1(0). Applying this proposition,
we ﬁnd that δr(L) ♦δp(L∃) = ∅, leaving δc(L) = S1.
Similarly for R, δr(R) ♦δp(R∃) ♦δr(R) since δp(R) = ∅(prove!), hence
δr(R) = δp(R∃) = B1(0) and δc(R) = S1.
Exercises 14.13
1. Show that the right-shift operator R (on γ∗or γ1) has no eigenvalues.
2. The right-shift operator R ∞B(γ1) and its adjoint L ∞B(γ∗) have spectra
δ(L) = δp(L) = B1[0] = δr(R) = δ(R).
3. The spectrum of L on γ1(Z) is the circle S1. This is an example of the hollowing
out of a spectrum when the algebra increases, in this case when γ1 is embedded
in γ1(Z).
4. The operator T (a0, a1, . . .) := (a0, 0, a1, a2, . . .), on c0, has a single eigenvalue
1, but its adjoint has δp(T ∃) = B1(0)→{1}. Deduce that δp(T ) = {1}, δr(T ) =
B1(0), and δc(T ) = S1 \ {1}.
But the same operator restricted to γ1 has a single eigenvalue 1 and no continuous
spectrum.
5. The operator T (a0, a1, . . .) := (a0, 0, a1, a2/2, a3/3, . . .), on c0, has a single
eigenvalue 1, and its adjoint has two eigenvalues, 1 and 0.
6. The spectrum of the multiplier operator T x := ax, on γ2, has no residual spec-
trum.
7. The spectrum of xω ∞B(X), where x ∞X and ω ∞X◦, consists of the
eigenvalues ωx and 0 (unless X is 1-dimensional).
8. Let T : X ∈Y, S : Y ∈X be operators and consider R ∞B(X ×Y) deﬁned by
R(x, y) := (Sy, T x); the ‘matrix’ form of R looks like
⎛0 S
T 0
⎜
. Then non-zero
eigenvalues of R come in pairs ±α. (Hint: consider (x, −y).)
9. Let T : C[0, 1] ∈C[0, 1] be deﬁned by T f (x) := x f (x). Show that T is linear
and continuous, ﬁnd its norm and show that its spectrum is the line [0, 1] in C,
consisting of only the residual part.
More generally the spectrum of T f := g f in C[0, 1], where g ∞C[0, 1], is
im g.

318
14
Spectral Theory
The reader is encouraged to explore the spectrum of this operator in other spaces,
such as L1[0, 1] or L2[0, 1].
10. * Let V : C[0, 1] ∈C[0, 1] be the Volterra operator V f (x) :=
⎩x
0 f . Show
that
V n+1 f (x) = 1
n!
 x
0
(x −y)n f (y) dy,
and that ⊆V n⊆⩽1/n!. Deduce, using the spectral radius formula, that its spec-
trum is just {0}. Show that 0 is not an eigenvalue (hint: differentiate!) but a
residual boundary spectral value.
11. Find the eigenvalues of T f (x) :=
⎩1
0 x2y2 f (y) dy on C[0, 1].
12. The spectrum of an isometry T lies in B1[0]. Any eigenvalues or approximate
eigenvalues lie in eiR. If T is an invertible isometry, then δ(T ) ♦eiR, otherwise
the spectrum must be the whole closed unit disk (e.g. the right-shift operator).
(Hint: T −α = T (1 −αS).)
13. Show that the set {T ∞B(X) : T is1 −1 and has a closed image} is open in
B(X). (Hint: Proposition 11.3.)
14.3 Spectra of Compact Operators
Ascents and Descents
For any operator, the eigenspace associated with an eigenvalue α is ker(T −α).
But this is not the whole story: for example, T :=
⎛
0 1
0 0
⎜
has just one eigenvalue,
and a one-dimensional eigenspace generated by
1
0

; the vector v :=
0
1

is mapped
by T to
1
0

, and only a second application of T kills it off. We can think of it as
a “generalized” eigenvector, with (T −α)2v = 0. In general, one can consider the
spaces of vectors that vanish when (T −α)n is applied to them. Two nested sequences
of spaces can be formed (here shown for α = 0),
• an ascending sequence
0 ♦ker T ♦ker T 2 ♦· · · ♦ker T n ♦· · · ♦

n
ker T n,
• a descending sequence
X ⊇im T ⊇im T 2 ⊇· · · ⊇im T n ⊇· · · ⊇

n
im T n.

14.3 Spectra of Compact Operators
319
Finite Ascents and Descents
Suppose there is an n such that ker T n = ker T n+1, i.e., for all x,
T nx = 0 √T n+1x = 0.
Substituting T x instead of x gives
T n+1x = 0 √T n+2x = 0
and ker T n+2 = ker T n+1 = ker T n. By induction, all the subsequent spaces in the
ascending sequence are identical, ker T n+k = ker T n. Operators with this property
are said to have a ﬁnite ascent up to n, 0 ∇ker T ∇· · · ∇ker T n.
Similarly, if im T m = im T m+1 then for any x ∞im T m+1,
x = T m+1y = T (T m y) = T (T m+1z) = T m+2z ∞im T m+2.
By induction, im T m+k = im T m. Operators with this property are said to have a
ﬁnite descent down to m.
Proposition 14.14
An operator T has
(i) ﬁnite ascent up to at most n √im T n ≤ker T k = 0, ⇔k,
(ii) ﬁnite descent down to at most m √X = ker T m + im T k, ⇔k,
(iii) ﬁnite ascent up to n and descent down to m ∪m = n and
X = ker T n ⊕im T n.
Proof (i) If im T n ≤ker T = 0, then T n+1x = 0 ∪T nx ∞im T n ≤ker T = 0,
and T has ﬁnite ascent up to at most n.
For the converse, let x ∞im T n ≤ker T k, that is, x = T ny and T kx = 0. Then
T n+k y = 0 and y ∞ker T n+k = ker T n; so x = T ny = 0.
(ii) Let x ∞X, then T mx = T m+1y = · · · = T m+kz, assuming ﬁnite descent to m.
So T m(x −T kz) = 0 and x = T kz + (x −T kz) ∞im T k + ker T m.
Conversely, if X = im T +ker T m, then for any x = T y+z, we have T mx = T m+1y
and im T m = im T m+1.
(iii) Suppose im T n = im T n+1, but ker T n ∇ker T n+1. Then there is an x1 such
that T n+1x1 = 0 but
0 ⇒= T nx1 = T n+1x2 = T n+2x3 = · · ·

320
14
Spectral Theory
so xk ∞ker T n+k \ ker T n+k−1, and T has an inﬁnite ascent. This shows that a ﬁnite
ascent cannot be longer than the descent.
Next suppose the ascent goes up to ker T n = ker T n+1 but the descent goes down to
im T m = im T m+1 with m ⩾n. Then for any x ∞X, there is a y such that
T mx = T m+1y ∪T m(x −T y) = 0
∪x −T y ∞ker T m = ker T n
∪T nx = T n+1y
so a ﬁnite descent cannot be longer than the ascent.
Combining the results of (i) and (ii) gives X = ker T n ⊕im T n.
⊓⇐
Proposition 14.15 (Fredholm Alternative)
A Fredholm operator T with
(i) ﬁnite ascent, satisﬁes index(T ) ⩽0,
(ii) escent, satisﬁesindex(T ) ⩾0,
(iii) scent and descent, satisﬁesindex(T ) = 0 and
T is 1-1 √T is onto.
Proof Recall that the codimension of a closed subspace Y ♦X is deﬁned as
dim(X/Y), that Fredholm operators have ﬁnite-dimensional kernels and ﬁnite codi-
mensional images, and index(T ) = dim ker T −codim im T (Deﬁnition 11.12). For
T with ﬁnite ascent to n, by the index theorem,
0 ⩽codim im T k = dim ker T k −index(T k)
= dim ker T n −k index(T ), for k ⩾n,
Since k can be arbitrarily large, it must be the case that index(T ) ⩽0.
For Fredholm operators with ﬁnite descent to m,
0 ⩽dim ker T k = codim im T k + index(T k)
= codim im T m + k index(T) for k ⩾m.
This time, we must have index(T ) ⩾0.
A special case is when m = n = 0, known as the Fredholm alternative: ker T = 0
if, and only if, im T = X, i.e., T is 1–1 √T is onto; in other words, T is either
invertible or it is neither 1–1 nor onto.
⊓⇐

14.3 Spectra of Compact Operators
321
Ivar Fredholm (1866–1927) studied p.d.e.s under Mittag-Leﬄer
in 1893 at the new University of Stockholm; he saw the con-
nection between Volterra’s equation and potential theory, espe-
cially in 1899 while working on Dirichlet’s problem; in 1903
he analyzed the theory of general integral equations f(x) −
λ
b
a k(x, y)f(y) dy = g(x) covering much that was then known
about boundary value problems (mostly self-adjoint), proved
the Fredholm alternative and deﬁned the Fredholm determi-
nant det(1 −K) = e−
n
1
n tr Kn. He was then ‘distracted’ by
actuarial science and government.
Fig. 14.1 Fredholm
Examples 14.16
1. The spaces M := im T m and N := ker T n are both T -invariant and such that
T |M is an isomorphism while T |N is nilpotent.
2. For matrices, the Fredholm alternative boils down to the statement that either
Ax = b has a unique solution or Ax = 0 has non-trivial solutions.
3. The Fredholm alternative only applies to (Fredholm) operators with ﬁnite ascent
and descent; e.g. the right-shift operator is 1–1 but not onto.
4. If T is Fredholm with ﬁnite ascent and descent, then dim ker T = dim ker T ∃
(Exercise 11.32(10)).
The Spectrum of a Compact Operator
The following two results are peaks in the landscape of Operator Theory.
Proposition 14.17
Let T : X ∈X be compact on a Banach space X , then I −T is a Fredholm
operator with ﬁnite ascent and descent.
Proof I −T is Fredholm since it is invertible up to the compact operator T
(Proposition 11.14).
Suppose S := I −T has inﬁnite ascent, so ker Sn−1 ∇ker Sn. By Riesz’s lemma
(Proposition 8.20), choose unit vectors xn ∞ker Sn with ⊆xn + ker Sn−1⊆⩾1
2. Then
for m < n,
⊆T xn −T xm⊆= ⊆(xn −xm) −S(xn −xm)⊆⩾1
2
since Sn−1(xm + S(xn −xm)) = 0. So (T xn) has no Cauchy subsequence, contra-
dicting the compactness of T .

322
14
Spectral Theory
Suppose S has inﬁnite descent, with im Sn−1 ˆ im Sn. One can choose unit
vectors xn ∞im Sn with ⊆xn + im Sn+1⊆⩾1
2. Then for m > n,
⊆T xn −T xm⊆= ⊆(xn −xm) −S(xn −xm)⊆⩾1
2
since xm + S(xn −xm) ∞im Sn+1. Again this would contradict the hypothesis.
It follows from the propositions above, that the index of T vanishes and
dim ker(S∃) = dim ker S.
⊓⇐
Theorem 14.18 Riesz-Schauder
If T ∞B(X) is compact, then
(i) its spectrum δ(T ) is a countable set, whose only possible limit point
may be 0,
(ii) each non-zero α ∞δ(T ) is an eigenvalue with a ﬁnite-dimensional
eigenspace ker(T −α),
(iii) T ∃has the same non-zero eigenvalues and eigenspace dimensions as
T .
Proof For α ⇒= 0, T −α = α(I −T/α) is a Fredholm operator with ﬁnite ascent and
descent, so its kernel is ﬁnite dimensional and it satisﬁes the Fredholm alternative,
namely it is either invertible (α ⇒∞δ(T )) or not 1–1 (α is an eigenvalue). T −α has
index 0, so T ∃has the same number of eigenvectors of α as T ,
dim ker(T ∃−α) = dim im(T −α)≡= codim im(T −α) = dim ker(T −α).
Consider those eigenvalues α for which |α| ⩾π > 0. Taking any list of them, αn
(distinct), choose a unit eigenvector en for each, such that ⊆en + [[e1, . . . , en−1]]⊆⩾
1
2 (Propositions 8.20 and 14.8). Hence, taking n > m, say,
⊆T en −T em⊆= ⊆αnen −αmem⊆= |αn|⊆en −αm
αn
em⊆⩾1
2|αn| ⩾π
2.
Now the bounded set {e1, e2, . . .} is mapped to {T e1, T e2, . . .}. If the ﬁrst set is
inﬁnite, the latter set would have no Cauchy subsequence, contradicting the com-
pactness of T . So the number of such eigenvectors, and corresponding eigenvalues,
is ﬁnite. The rest of the eigenvalues must be within π of 0. By taking π = 1/n ∈0,
it follows that the number of non-zero eigenvalues is countable.
⊓⇐
To clarify, in ﬁnite dimensions, the set of eigenvalues is ﬁnite and need not include
0, but in inﬁnite dimensions, 0 must be part of the spectrum (else I = T −1T is
compact). If there is a inﬁnite sequence of non-zero eigenvalues, then αn ∈0, and

14.3 Spectra of Compact Operators
323
0 is an approximate eigenvalue. What remains to complete the theory is to ﬁnd the
form of T on each generalized eigenspace.
Proposition 14.19 Jordan Canonical Form
Oneachﬁnite-dimensionalspace ker(T −α)n (α ⇒= 0)ofacompactoperator
T on a Banach space X , there is a matrix of T consisting of blocks on the
main diagonal, each of the type
⎝
⎭⎭⎭⎭⎞
α 1 0
0
0 α
0
1
0
0 α
⎟
⎧⎧⎧⎧⎠
Proof The operator T can be split as α + (T −α). The latter is nilpotent on the
subspace ker(T −α)n (ﬁnite dimensional since (T −α)n is Fredholm), while αI is
diagonal. This is the claimed Jordan form, once it is shown that a nilpotent operator
has the following form.
A nilpotent operator on a ﬁnite-dimensional space can be represented by a matrix
of 0s except for 1s and 0s in the super-diagonal: Suppose A is a nilpotent operator
of order N, AN = 0; it has a descending sequence down to N, and an ascending
sequence up to N, 0 ∇ker A ∇· · · ∇ker AN. For each non-zero vector AN−1u ∞
im AN−1 there is a sequence of vectors e1 := AN−1u, e2 := AN−2u, …, eN := u.
They are linearly independent because ei ∞ker Ai \ ker Ai−1, so to have em ∞
[[e1, . . . , em−1]] ♦ker Am−1 is impossible. Since Aei = ei−1 and Ae1 = 0, the
matrix of A restricted to the space generated by these vectors is
⎝
⎭⎭⎭⎭⎞
0
1
0
0
0
1
0
0
⎟
⎧⎧⎧⎧⎠
.
A remains nilpotent on the rest of the space ker AN/[[e1, . . . , eN]], with perhaps
a lower order. The same argument can be repeated to yield other sets of independent
vectors. As X = ker AN is ﬁnite-dimensional, this process ends with a ﬁnite basis
for X and the matrix of A with respect to it consists of such blocks placed on the
diagonal.
⊓⇐
Examples 14.20
1. The total number of αs in a Jordan matrix, called its algebraic multiplicity, is the
dimension of ker(T −α)N, the largest generalized eigenspace. The number of

324
14
Spectral Theory
Jordan blocks associated with α is dim ker(T −α), called the geometric multi-
plicity of α. The size of the largest Jordan block is sometimes called its (Jordan)
index. For example, the matrix to the right has an eigenvalue 2 with algebraic
multiplicity 4, geometric multiplicity 2, and index 3; the other eigenvalue 3 has
algebraic multiplicity 2, geometric multiplicity 1, and index 2.
⎝
⎭⎭⎭⎭⎭⎭⎞
2
2 1
2 1
2
3 1
3
⎟
⎧⎧⎧⎧⎧⎧⎠
2. The set of N × N matrices with distinct eigenvalues is dense and open in B(CN).
Proof Suppose a matrix A has the Jordan-form matrix A = D + C where D is
diagonal with the eigenvalues α1, . . . , αr and C is nilpotent. Alter each eigenvalue
slightly so α⊂
i are all distinct and let A⊂:= D⊂+C; then ⊆A⊂−A⊆= ⊆D⊂−D⊆=
maxi |α⊂
i −αi| < π.
Because of this, the Jordan canonical form of a numerical matrix is impossible to
calculate, due to the limited accuracy of the matrix coefﬁcients; small changes in
the coefﬁcients result in a diagonal Jordan matrix with distinct eigenvalues.
Exercises 14.21
In these exercises, let K be a compact operator on a Banach
space X.
1. When T is 1–1, the ascending sequence of spaces are all 0.
When T is onto, the descending sequence of spaces are all X.
2. For the matrix
⎛0 1
0 0
⎜
, the ascending and descending sequences are the same.
3. The left-shift operator L is onto and has an inﬁnite ascending sequence; R is
1–1 and has an inﬁnite descending sequence.
The operator f (x) ∀∈x f (x) acting on C[0, 1], is 1–1, and also has an inﬁnite
descending sequence, e.g. each of the functions 1, x, x2, . . . belongs to a different
image space.
4. If T has a ﬁnite descent then T ∃has a ﬁnite ascent.
5. (a) Suppose that ker T n ♦im T for some n. Show that T x = 0 ∪x = T 2z
so
ker T n−1 ♦im T 2, . . . , ker T ♦im T n.
(b) Suppose ker T ♦im T n for some n, then x ∞ker T 2 ∪x −T y ∞ker T
for some y, so
ker T 2 ♦im T n−1, . . . , ker T n ♦im T.

14.3 Spectra of Compact Operators
325
6. There is an eigenvalue at the spectral radius of K, except possibly when this
is 0.
7. In γ1, the multiplier map M(an) := (cnan) is compact when cn ∈0; its eigen-
values are cn. 0 is part of the continuous spectrum, unless it is an eigenvalue.
For example, take cn := 1/n (and c0 := 1), and the shift operators L and R;
then ML is also compact but has no eigenvalues except 0; RM is compact with
no eigenvalues at all but 0 is part of the residual spectrum.
8. (The original Fredholm alternative) For α ⇒= 0, either (K −α)x = y has a
unique solution for each y or K ∃y = αy has a non-trivial solution.
9. The minimal polynomial of each Jordan block is (z −α)n.
10. Cayley-Hamilton theorem: If p is the characteristic polynomial of a matrix T ,
then p(T ) = 0. (Hint: consider the characteristic polynomial of each Jordan
block.)
14.4 The Functional Calculus
The previous deﬁnition of f (T ) in Taylor’s theorem can be extended to functions that
are analytic on the spectrum of T , since, by Cauchy’s theorem, the path of integration
can be swept over analytic regions of f and (z −T )−1.
Deﬁnition 14.22
For any function f : C ∈C which is analytic in a neighborhood of δ(T ), let
f (T ) :=
1
2∂i
⎫
f (z)(z −T )−1 dz,
where the path of integration is taken along simple closed curves enclosing
δ(T ) in a direction which keeps δ(T ) to its left.
Note that the integral is deﬁned since f (z) and ⊆(z −T )−1⊆are continuous in z
on the selected compact path; hence
⊆f (T )⊆⩽1
2∂

| f (z)|⊆(z −T )−1⊆ds < ∗.

326
14
Spectral Theory
Examples 14.23
1. ▶If T S = SR then f (T )S = Sf (R) when f is analytic on a neighborhood of
δ(T ) →δ(R), since
S(z −R) = (z −T )S
∴(z −T )−1S = S(z −R)−1
∴f (T )S =
⎫
f (z)(z −T )−1S dz =
⎫
f (z)S(z −R)−1 dz = Sf (R).
In particular
(a)
f (S−1T S) = S−1 f (T )S; for example, eS−1T S = S−1eT S.
(b) ST = T S implies f (T )S = Sf (T ) and f (T )g(S) = g(S) f (T ).
2. If f ∞Cη(δ(T)) is zero on δ(T ), it does not follow that f (T ) = 0, because f (T )
is deﬁned in terms of a path-integral just outside δ(T ). For example, T :=
⎛0 1
0 0
⎜
has δ(T ) = {0}, and f (z) := z vanishes there, yet f (T ) = T ⇒= 0.
3. * f is differentiable (and continuous) at T : for H sufﬁciently small, f (T + H)
is deﬁned since δ(T + H) ♦δ(T ) + Bπ(0) (Proposition 14.6), and
f (T + H) = f (T ) +
1
2∂i
⎫
f (w)(w −T )−1H(w −T )−1 dw + o(H).
The next theorem proves that all algebraic properties of a complex function are
mirrored by properties of f (T ).
Theorem 14.24 The Functional Calculus
Given T ∞X , the map f ∀∈f (T ), Cη(δ(T )) ∈X , satisﬁes
( f + g)(T ) = f (T ) + g(T ),
(αf )(T ) = αf (T ),
( f g)(T ) = f (T )g(T ),
1(T ) = 1,
f ≈g(T ) = f (g(T )),
fn ∈f in C(δ(T ) + Bπ(0)) (∩π > 0) ∪fn(T ) ∈f (T ) in X.
Proof We have already seen part of this theorem in action when analyzing power
series. In particular, the cases 1 =
1
2∂i

(z −T )−1 dz and T −1 =
1
2∂i

z−1(z −
T )−1 dz were covered (Example 13.30(2)).
(i) ( f + g)(T ) =
f (T ) + g(T ) and (αf )(T ) = αf (T ) express the linearity
property of the integral.

14.4 The Functional Calculus
327
(ii) ( f g)(T) = f (T )g(T ): We require the identity
(z −w)(z −T )−1(w −T )−1 = (w −T )−1 −(z −T )−1,
which follows easily from z −w = (z −T ) −(w −T ). In the following analysis,
consider two paths around δ(T ), one (with variable z) nested inside another (with
variable w).
f (T )g(T) =
1
(2∂i)2
⎫⎫
f (z)g(w)(z −T )−1(w −T )−1 dz dw
=
1
(2∂i)2
⎫⎫
f (z)g(w)
⎛(w −T )−1
z −w
+ (z −T )−1
w −z
⎜
dz dw
=
1
2∂i
⎫
g(w)(w −T )−1 1
2∂i
⎫
f (z)(z −w)−1 dz dw
+
1
2∂i
⎫
f (z)(z −T )−1 1
2∂i
⎫
g(w)(w −z)−1 dw dz
=
1
2∂i
⎫
f (z)(z −T )−1g(z) dz,
= ( f g)(T )
where we have changed the order of integration in the third line, and used the fact
that (w −z)−1 leaves a residue when integrated on the outer path, but not when
integrated on the inner path (because the singularity at w would then be outside the
path of integration).
In particular, note that if f is invertible on a neighborhood of δ(T ),
f (T )−1 =
1
2∂i
⎫
f (z)−1(z −T )−1 dz.
(14.1)
(iii) f (g(T )) :=
1
2∂i
⎫
f (z)(z −g(T ))−1 dz, where the right part of the integrand
is (z −g(T ))−1 =
1
2∂i
⎫
(z −g(w))−1(w −T )−1 dw by (14.1). Combining the
two and using Cauchy’s integral formula (Proposition 12.19), we get
f (g(T )) =
1
(2∂i)2
⎫⎫
f (z)(z −g(w))−1(w −T )−1 dw dz,
=
1
(2∂i)2
⎫⎫
f (z)(z −g(w))−1 dz (w −T )−1 dw,
=
1
2∂i
⎫
f ≈g(w)(w −T )−1 dw,
= f ≈g(T ).

328
14
Spectral Theory
Note that f has to be analytic on δ(g(T )) and gδ(T ) for f (g(T )) and f ≈g(T)
to be deﬁned, but the two sets are equal by the next theorem (which only uses part
(ii) of this theorem).
(iv) The mapping is continuous, since ⊆(z −T )−1⊆is bounded by some constant c
on the compact path enclosing the open set U := δ(T ) + Bπ(0):
⊆f (T ) −g(T )⊆⩽1
2∂
⎫
| f (z) −g(z)|⊆(z −T )−1⊆ds
⩽c⊆f −g⊆C(U).
⊓⇐
Theorem 14.25 Spectral Mapping Theorem
The spectrum of f (T ) is equal to the set { f (α) : α ∞δ(T )}, that is,
δ( f (T )) = f (δ(T ))
Proof For any f analytic in a neighborhood of δ(T ):
(i) α ⇒∞f δ(T ) ∪α ⇒∞δ( f (T )): Let α ⇒= f (z) for all z ∞δ(T ); since f δ(T ) is
a closed set, there is a minimum distance between α and f δ(T ). So ( f (z) −α)−1
is analytic on δ(T ) + Bπ(0) if π is small enough, and by the functional calculus
( f (T ) −α)−1 exists. Thus f (T ) −α is invertible.
(ii) f (T ) −f (α) invertible ∪T −α invertible: if f (T ) −f (α) has an inverse S,
we see from rewriting f (z) −f (α) = (z −α)F(z), and the functional calculus, that
(T −α)F(T )S = 1 = SF(T )(T −α)
which implies that the factor T −α itself is invertible. This is justiﬁed once it is
shown that F(z) is analytic about δ(T ); this is apparent when z ⇒= α, but even so,
f (z) = f (α) + f ⊂(α)(z −α) + 1
2 f ⊂⊂(α)(z −α)2 + o(z −α)2,
∪F(z) = f (z) −f (α)
z −α
= f ⊂(α) + 1
2 f ⊂⊂(α)(z −α) + o(z −α),
meaning F is analytic at α.
⊓⇐
Examples 14.26
1. log T can be deﬁned whenever there is a path, or “branch”, connecting 0 to ∗
without meeting δ(T ), because in this case, log z can be deﬁned and is analytic on
δ(T ). But note that log z, and consequently log T , depends on the actual branch
used.
When deﬁned, elog T = T . Such elements must be in G1 (Proposition 13.24).

14.4 The Functional Calculus
329
2. Similarly one can deﬁne T a := ea log T (again not uniquely); then (T 1/n)n = T
(n = 1, 2, . . .), and T a+b = T aT b (at least for a, b real). By the spectral mapping
theorem, ρ(T a) = ρ(T )a for a ⩾0.
3. If T satisﬁes a polynomial p(T ) = 0, then δ(T ) consists of the roots of the
minimal polynomial of T (Example 13.3(12)).
Proof The spectral theorem shows that p(δ(T )) = 0, i.e., that the spectrum
consists of roots of p. Conversely, if α is a root of the minimal polynomial,
p(α) = 0, then p(z) = (z −α)nq(z), so 0 = p(T ) = (T −α)nq(T ), where
q(T ) ⇒= 0 and thus T −α is not invertible.
4. ▶If α is an eigenvalue of T ∞B(X) then f (α) is an eigenvalue of f (T ), with
the same eigenvector.
Proof When T x = αx, then (z −T )x = (z −α)x and (z −T )−1x = (z −α)−1x
(z ⇒∞δ(T )), so
f (T )x =
1
2∂i
⎫
f (z)(z −T )−1x dz =
1
2∂i
⎫
f (z)(z −α)−1x dz = f (α)x.
Conversely suppose f (T ) −f (α) is not 1–1. Take an open neighborhood U ˆ
δ(T ) in which f is analytic. Then, either f is constant on U, or else there are
only a ﬁnite number of αi ∞δ(T ) satisfying f (αi) = f (α). So, for z ∞U,
f (z)−f (α) = (z −α1) · · · (z −αk)g(z) (where multiple roots are repeated) with
g analytic and non-zero on U, and consequently
f (T ) −f (α) = (T −α1) · · · (T −αk)g(T ).
But f (T ) −f (α) is not 1–1, so there must be a αi such that T −αi is not 1–1
(g(T) is invertible), and f (αi) = f (α).
Proposition 14.27
If δ(T ) disconnects into two closed sets δ1→δ2, each surrounded by simple
closed paths in open neighborhoods of them, then
(i) T = T P1 + T P2textbf, with P1, P2 (called spectral idempotents) such
that 1 = P1 + P2, Pi Pj = δi j ,
(ii) In the reduced algebras P1X P1, P2X P2 respectively,
δ(T P1) = δ1,
δ(T P2) = δ2.
Proof The disjoint closed sets δ1 and δ2 can be separated by disjoint open sets U1,
U2 (Exercise 5.7(5)). Consider the functions σi (i = 1, 2) which take the constant

330
14
Spectral Theory
value 1 on one open set Ui ˆ δi, and 0 on the other. They are analytic on U1 →U2,
so we can deﬁne
Pi := σi(T ) =
1
2∂i
⎫
δi
(z −T )−1 dz.
The path of integration is the union of the two paths surrounding δ1 and δ2, but one
of the two integrals vanishes.
Pi are idempotents, P1P2 = 0, and P1 + P2 = 1, because σ2
i = σi, σ1σ2 = 0
and σ1 + σ2 = 1 on U1 →U2 ˆ δ(T ).
Let fi(z) := zσi(z); then fi(T ) = T Pi and δ( fi(T )) = fi(δ(T )) = δi →{ 0 }.
However, if we restrict to the reduced algebra PiX Pi, with unity Pi, this changes
slightly. Since z −α is invertible in Cη(δi) if, and only if, α ⇒∞δi, it follows that
there exists an S such that S(T −α)Pi = Pi = (T −α)SPi whenever α ⇒∞δi; this
means that (T −α)Pi is invertible in PiX Pi. Thus, δ(T Pi) = δi in this algebra.
⊓⇐
Examples 14.28
1. ▶When the algebra is B(X), Pi are projections, and the spectral decomposition
of an operator T into T P1 and T P2 also gives a decomposition of X = X1 ⊕X2
where Xi = im Pi are T -invariant, and δ(T |Xi ) = δi. (Theorem13.8(11))
2. If 0 is an isolated point of δ(T ), with spectral idempotent P, then there is a
Laurent expansion
(z −T )−1P = Pz−1 + T Pz−2 + T 2Pz−3 + · · · .
3. If 0 ⇒∞δ1, then P1 = T

1
2∂i

{ α }
(z−T )−1
z
dz

. For example, when T is a compact
operator and α ⇒= 0 is an isolated point of δ(T ), then the projection Pα is also
compact, conﬁrming that the eigenspace of α is ﬁnite-dimensional.
Exercises 14.29
1. The non-trivial idempotents have spectrum { 0, 1 }, and the nilpotents have spec-
trum { 0 }. What can the spectrum of a cyclic element be?
2. If f takes the value 0 inside δ(T ) then f (T ) is not invertible.
3. Use the spectral mapping theorem to show that if eT = 1 then δ(T ) ∇2∂iZ. If
P is an idempotent, then e2∂i P = 1.
4. If J is a Banach algebra morphism, then f (J(T )) = J( f (T )) (recall δ(J(T )) ♦
δ(T )).
5. Show directly that the matrix
⎛0 1
0 0
⎜
has no square root at all.
The shift operators on γ2, say, cannot have a square root because their spectrum
encloses 0 (even on γ1(Z) when L and R are invertible). Prove this directly by
showing the contradictions

14.4 The Functional Calculus
331
(a) if T 2 = L, then T must be onto and ker T = ker L = [[e0]], so e0 = βT e0 =
0;
(b) if T 2 = R, then T is 1–1, and im T = im R, so T Rx = RT x =
(0, 0, y0, . . .).
6. A simple linear electronic circuit with feedback can be modeled as an operator,
transforming an input signal x = (xn) to an output signal y = (yn) such that
yn = bxn −a1yn−1 −· · · −ar yn−r,
where b, ai are parameters determined by the circuit. Equivalently,
(1 + a1R + · · · + ar Rr)y = bx,
where R is the right-shift operator. To avoid the once-familiar feedback loop
instability, it is desired that the values yn do not grow of their own accord, meaning
that 1 + a1R + · · · + ar Rr has a continuous inverse. This is the case when the
roots of the polynomial 1 + a1z + · · · + arzr all have magnitude greater than 1.
14.5 The Gelfand Transform
Quasinilpotents and the Radical
Deﬁnition 14.30
The quasinilpotentsare those elements Q with ρ(Q) = 0. The (Jacobson)
radical J of X is
J := { Q ∞X : ⇔T ∞X, ρ(T Q) = 0 }.
A Banach algebra with a trivial radical is called semi-primitive or semi-simple.
The next proposition shows that the radical is a closed ideal, which can be factored
out to leave a semi-simple Banach algebra.
Examples 14.31
1. The prime examples of quasinilpotents are the nilpotents, deﬁned as those ele-
ments which satisfy Qn = 0 for some n, so ρ(Q) ⩽⊆Qn⊆1/n = 0; e.g.
⎛0 1
0 0
⎜
.
2. Every operator T f (x) :=
⎩x
0 k(x, y) f (y) dy on C[0, 1], where k ∞L∗[0, 1]2,
is a quasinilpotent.

332
14
Spectral Theory
Proof |T f (x)| ⩽
⎩x
0 |k(x, y)|| f (y)| dy ⩽⊆k⊆⊆f ⊆x. By induction one can con-
clude |T n f (x)| ⩽⊆k⊆n ⊆f ⊆xn/n!,
|T n+1 f (x)| = |
 x
0
k(x, y)T n f (y) dy|
⩽
 x
0
⊆k⊆n+1 ⊆f ⊆yn/n! dy
⩽⊆k⊆n+1 ⊆f ⊆xn+1/(n + 1)!
so ⊆T n⊆⩽⊆k⊆n /n! and ρ(T ) ⩽⊆T n⊆1/n ⩽⊆k⊆/
n˙
n! ∈0.
3. The sum and product of quasinilpotents need not be quasinilpotents, e.g.
⎛0 1
0 0
⎜
and
⎛0 0
1 0
⎜
.
4. The quasinilpotents are topological divisors of zero since their spectrum is a
boundary point. Idempotents (except 0 and 1) are divisors of zero but not quasi-
nilpotents.
topological divisors of zero
quasi-
nilpotents
divisors
of zero
nilpotents
5. Radical elements are obviously quasinilpotents, ρ(Q) = ρ(1Q) = 0.
6. It is enough to show that 1 ⇒∞δ(T Q) for all T , in order that Q ∞J .
Proof For any α ⇒= 0, 1 ⇒∞δ(T Q/α) = δ(T Q)/α ∪α ⇒∞δ(T Q).
7. ▶For any T ∞X, Q ∞J , δ(T + Q) = δ(T ).
Proof For any invertible S, the sum S + Q = S(1 + S−1Q) is also invertible,
since ρ(S−1Q) = 0 (Theorem 13.20). Thus
α ⇒∞δ(T + Q) √T + Q −α is invertible √T −α is invertible √α ⇒∞δ(T ).
8. B(X) has nilpotents (except for X = C) but only a trivial radical.

14.5 The Gelfand Transform
333
Proof For any Q ⇒= 0, an operator T can be found such that 1 −T Q is non-
invertible, so 1 ∞δ(T Q). One such operator is T := xω, where Qx ⇒= 0, ω ∞X◦,
ωQx = 1; then (1 −T Q)x = x −xωQx = 0 but x ⇒= 0.
Proposition 14.32
The radical is a closed ideal.
Proof J is contained in every maximal left-ideal: Recall that a maximal left-ideal
is closed and that every proper left-ideal can be enlarged to a maximal left-ideal
(Examples 13.5(7,8)). Let Q ∞J , and let M be a maximal left-ideal. Then M+X Q
is a left-ideal which contains M. Either
(a) M + X Q = X, in which case 1 = R + T Q for some R ∞M, T ∞X, so that
R = 1 −T Q is invertible, contradicting R ∞M (Example 13.5(5)); or else,
(b) M + X Q = M, in which case Q = 0 + 1Q ∞M.
Thus J ♦M as required; an analogous argument shows that J is contained in
every maximal right-ideal.
J is the intersection of the maximal left-ideals: Let P be an element that is
contained in every maximal left-ideal. For any T ∞X, the left-ideal X(1 −T P)
cannot be proper, otherwise it would lie inside some maximal left-ideal M, forcing
P ∞M, and T P ∞M, and so 1 = T P + (1 −T P) ∞M, a contradiction. Hence
X(1 −T P) = X, and there is an S such that S(1 −T P) = 1.
To show 1 −T P is invertible we need to prove (1 −T P)S = 1 as well. To this
end one can substitute −ST for T in the above argument, to conclude that there is
an R ∞X such that
1 = R(1 + ST P) = R(S + 1 −S(1 −T P)) = RS.
But RS = 1 = S(1 −T P) implies 1 −T P = S−1 is invertible. With 1 ⇒∞δ(T P)
for any T , P must be in the radical.
J is a closed ideal: Being the intersection of closed sets, J is also closed (Propo-
sition 2.18). For any S, T ∞X and Q, Q⊂∞J ,
(a) ρ(ST Q) = 0 = ρ(SQT ), so T Q, QT ∞J ,
(b) δ(T (Q + Q⊂)) = δ(T Q) = { 0 } from Example 14.31(7) above (T Q⊂∞J ), so
Q + Q⊂∞J ,
(c) ρ(T (αQ)) = |α|ρ(T Q) = 0, so αQ ∞J ,
and J is an ideal.
⊓⇐

334
14
Spectral Theory
The State Space
Deﬁnition 14.33
The state space of a Banach algebra X is the set of functionals
S(X) := { ω ∞X ◦: ω1 = 1 = ⊆ω⊆}.
We often write S for S(X) and S(T ) := { ωT ∞C : ω ∞S(X) }, for example,
S(1) = { 1 }.
Proposition 14.34
The state space S(X) is a convex set containing the character space τ(X).
For any T ∞X, S(T ) is a compact convex subset of C, and
τ(T ) ♦δ(T ) ♦S(T ).
Proof (i) S(X) and S(T ) are convex: For ω, ψ ∞S and 0 ⩽t ⩽1,
(tω + (1 −t)ψ)1 = t + 1 −t = 1, and
⊆tω + (1 −t)ψ⊆⩽t⊆ω⊆+ (1 −t)⊆ψ⊆= 1.
It follows from tωT + (1 −t)ψT = (tω + (1 −t)ψ)T ∞S(T ) that S(T ) is convex.
S(T ) is compact: S(T ) is bounded since |ωT | ⩽⊆T ⊆for any ω ∞S. Now recall
that every bounded sequence in X ◦has a weak*-convergent subsequence (Theorem
11.40 for X separable. So whenever ωnT ∞S(T ) converges to a limit point z, there
is a subsequence of ωn that converges in the weak* sense, ωni ⇀ω ∞X ◦, implying
(a) ωni T ∈ωT = z and 1 = ωni 1 ∈ω1,
(b) ⊆ω⊆⩽lim infi ⊆ωni ⊆= 1 (Corollary 11.35).
Hence ω ∞S and z ∞S(T ), that is, S(T ) is closed and bounded.
(ii) δ(T ) ♦S(T ): If S ∞X is not invertible, then 1 ⇒∞[[S]]; indeed d(1, [[S]]) = 1
as [[S]] contains no invertible elements (Theorem 13.20). So by the Hahn-Banach
theorem, there is a ω ∞X ◦satisfying ω1 = 1 = ⊆ω⊆and ωS = 0 (Proposition
11.18). In particular, for S = T −α, where α ∞δ(T ), there is a ω ∞S such that
0 = ω(T −α) = ωT −α,
so α = ωT ∞S(T ).

14.5 The Gelfand Transform
335
(iii) τ(T ) ♦δ(T ): Recall that any character ψ ∞τ maps invertible elements
to invertible complex numbers (Example 13.7(1)), including ψ1 = 1. So for any
α ⇒∞δ(T ), ψT −α = ψ(T −α) ⇒= 0, and α ⇒∞τ(T ). Equivalently, τ(T ) ♦δ(T )
and |ψT | ⩽ρ(T ) ⩽⊆T ⊆. This means that ψ is automatically continuous with
⊆ψ⊆= 1, and so τ ♦S.
⊓⇐
Examples 14.35
1. ▶The characters of γ1 are of the type ψ(an) = ⎪∗
n=0 anzn, where |z| ⩽1
depends on ψ.
Proof Let ψ ∞τ ♦γ1◦≡γ∗(Proposition 9.6); then every sequence in γ1 can
be written as
x = (a0, a1, . . .) =
∗
⎬
n=0
anen =
∗
⎬
n=0
an(e1 ◦· · · ◦e1


n
),
∴ψx =
∗
⎬
n=0
anψ(e1 ◦· · · ◦e1) =
∗
⎬
n=0
anzn,
(z := ψe1),
where the multiplicative property ψ(e1 ◦· · · ◦e1) = (ψe1)n was used. The
requirement 1 = ⊆ψ⊆= ⊆(zn)⊆γ∗implies |z| ⩽1, else |z|n would grow beyond
1 as n ∈∗..
2. The characters of γ1(Z) are ψξ(an) = ⎪
n∞Z aneinξ.
The proof is the same as above except |z| = 1, that is, z = eiξ ∞S1 for some
0 ⩽ξ < 2∂.
3. For L1(S1), the characters are ψn( f ) =
⎩2∂
0
einξ f (ξ) dξ, (n ∞Z).
Proof Let ψ ∞τ ♦L1(S1)◦≡L∗(S1), so ψ( f ) =
⎩2∂
0
h(ξ) f (ξ) dξ for
some h ∞L∗(S1). Recall that L1(A) does not contain a unity for convolution
(Example 13.3(5)); nevertheless, one can be added artiﬁcially, so τ exists and its
characters act on L1(A). Again we require
(a) 1 = ⊆ψ⊆= ⊆h⊆L∗, so |h(ξ)| ⩽1 for almost all ξ;
(b) ψ( f ◦g) = ψ( f )ψ(g), or equivalently,
 2∂
0
h(ξ)
 2∂
0
f (ξ −η)g(η) dη dξ =
 2∂
0
h(ξ) f (ξ) dξ
 2∂
0
h(η)g(η) dη.
This implies that h(ξ + η) = h(ξ)h(η) a.e.; we’ve met this identity before in
our preliminary discussion on the exponential function in Section13.2, where we
concluded that h(ξ) = h(1)ξ = ezξ, assuming h is continuous. That this can be
taken to be the case follows from Corollary 9.22,

336
14
Spectral Theory

 
h(y + π) −h(y)

f (y) dy
 =


h(y)

f (y −π) −f (y)

dy
⩽

| f (y −π) −f (y)| dy ∈0.
Moreover, h(2∂) = h(0) = 1 implies that h(1) = ein for some n ∞Z.
4. For L1(R), the characters are ψξ( f ) =
⎩
R eixξ f (x) dx, (ξ ∞R).
Proof Let ψ ∞τ ♦L1(R)◦≡L∗(R); so ψ( f ) =
⎩
h(x) f (x) dx. As before,
|h(x)| ⩽1 for all x, while the condition ψ( f ◦g) = ψ( f )ψ(g) is equivalent to
h(x + y) = h(x)h(y) a.e., so h(x) = h(1)x. To avoid h(x) growing arbitrarily
large as x ∈±∗, |h(1)| must be 1, and h(x) = eixξ.
5. Repeating for L1(R+), τ ∼= { e−zx : Re z ⩾0 }.
6. * For C[0, 1], τ = { δx ∞C[0, 1]◦: δx( f ) = f (x), x ∞[0, 1] } ∼= [0, 1].
Proof That δx are functionals (with unit norm) is Example 8.6(6). In addition,
δx( f g) = ( f g)(x) = f (x)g(x) = δx( f )δx(g), and δx(1) = 1.
Note that for x ⇒= y, δx( f ) ⇒= δy( f ) for some f ∞C[0, 1].
0
1
0
i−1
2n , i
2n ,i+1
2n
1
For the converse, let ψ be a character of
C[0, 1]. Deﬁne ‘triangle’ functions, τn,i(x),
as in the accompanying plot; note that these
functions overlap and sum to 1 everywhere,
⎪2n
i=0 τn,i = 1.
Then 1 = ψ1 = ⎪
i ψ(τn,i) and at least one triangle function must give
ψ(τn,in) ⇒= 0. In fact, ψ(τn,i) = 0 for i ⇒= in −1, in, in +1, since τn,iτn,in = 0. By
taking larger values of n, and selected values ofin, thenestedintervals [ in−1
2n , in+1
2n ]
shrink to some point x. For any function f ∞C[0, 1],
ψ f =
⎬
i
ψ(τn,i)ψ( f ) = ψ
 in+1
⎬
i=in−1
τn,i f

∈f (x),
as n ∈∗.
The map x ∀∈δx is thus 1–1 and onto τ. Furthermore xn ∈x √δxn ⇀δx,
since the latter means f (xn) ∈f (x) for all f ∞C[0, 1], in particular for the
identity function f (x) := x.
7. * The character space of the Banach algebra C[T1, . . . , Tn] generated by com-
muting elements, is isomorphic to a compact subset of Cn (use the map ψ ∀∈
(ψT1, . . . , ψTn)).
8. * The character space is weakly closed, i.e., ψn ∞τ and ψn ⇀ψ ∪ψ ∞τ.
Consequently, for a separable Banach algebra, τ is a compact metric space.

14.5 The Gelfand Transform
337
Israel Gelfand (1913–2009) studied functional analysis at the
University of Moscow under Kolmogorov in 1935, specializing
in commutative normed rings. During 1939-41 he studied Ba-
nach algebras, introducing his transform and proving the spec-
tral radius formula, which gave much impetus to the subject; in
1943, with Naimark, he proved the embedding of special com-
mutative ∗-algebras into B(H); and then in 1948 he simpliﬁed
the subject-matter with the introduction of the C∗-condition
x∗x = x 2.
Fig. 14.2 Gelfand
Proof Taking the limits of ψn(S + T ) = ψnS + ψnT , ψn(αT) = αψnT ,
ψn(ST ) = (ψnS)(ψnT ), and ψn1 = 1, shows that ψ is an algebraic morphism.
Also |ψnT | ⩽⊆T ⊆becomes |ψT | ⩽⊆T ⊆in the limit n ∈∗, and ψ is contin-
uous. For a separable Banach algebra, the unit ball in X ◦is compact with respect
to the weak*-metric (Theorem 11.40), and so is its weakly closed subset τ.
The Gelfand Transform
To see why characters may be useful, consider the algebra γ1 and its characters pz.
A sequence such as x = (1/2, 1/4, 1/8, . . .) can be encoded as a complex power
series in terms of its characters, pz(x) = ⎪∗
n=0 zn/2n+1 = (2 −z)−1. Then the
convolution product x ◦· · · ◦x can be evaluated using characters instead of working
it out directly,
pz(x ◦· · · ◦x) = pz(x)N =
1
(2 −z)N =
∗
⎬
n=0
N(N + 1) · · · (N + n −1)
n!2N+n
zn.
For an example from probability theory, consider a random variable that outputs a
natural number n = 0, 1, 2, . . . , with probability 1/2n+1. The probability distribu-
tion of the sum of N such random outputs is x ◦· · · ◦x, which can be read off from
the coefﬁcients of pz(x)N; e.g. the probability of getting a total of, say 2, after N
trials is N(N + 1)/2N+3. Further, the mean of such a sum of random variables is
given by differentiating (2 −z)−N at z = 1, that is N. The key step is to consider
pz(x) as a function of z. Its generalization leads to:
Deﬁnition 14.36
The Gelfand transform of T is the map T : τ(X) ∈δ(T ) deﬁned by
T (ψ) := ψT .
The element T is transformed into a function on the compact space τ. The alge-
braic structure is preserved, but the transform is generally neither 1–1 nor onto.

338
14
Spectral Theory
Proposition 14.37
The Gelfand transform G : T
∀∈T is a Banach algebramorphism
X ∈C(τ),

S + T = S + T ,

αT = αT ,
1 = 1,

ST = S T ,
⊆T ⊆⩽⊆T ⊆.
Its kernel ker G contains the quasinilpotents and the commutators.
For any analytic function on the spectrum of T , f ∞Cη(δ(T )),

f (T ) = f ≈T .
Proof It is clear from
|T (ψ) −T (ω)| = |ψT −ωT | ⩽⊆ψ −ω⊆⊆T ⊆,
and |T (ψ)| = |ψT | ⩽⊆T ⊆, forall ψ, ω ∞τ,
that T is a (continuous) Lipschitz and bounded function on τ, with ⊆T ⊆C ⩽⊆T ⊆.
For any ψ ∞τ, we have:
1(ψ) = ψ1 = 1,

αT (ψ) = ψ(αT ) = αψT = αT (ψ),

S + T (ψ) = ψ(S + T ) = ψS + ψT = (S + T )(ψ),

ST (ψ) = ψ(ST ) = ψS ψT = S(ψ) T (ψ) = (ST )(ψ).
Clearly, from T (ψ) = ψT , T = 0 √τ(T ) = 0. If Q is a quasinilpotent then
τ(Q) ♦δ(Q) = { 0 }. Also, 
[S, T ] = ST −TS = 0 since C(τ) is commutative.
Lastly, as ψ(S−1) = (ψS)−1, for any ψ ∞τ, S ∞X,

f (T )(ψ) = ψ f (T ) = ψ
⎛1
2∂i
⎫
f (z)(z −T )−1 dz
⎜
=
1
2∂i
⎫
f (z)(z −ψT )−1 dz
(ψT ∞δ(T ))
= f (ψT ) = f ≈T (ψ).
⊓⇐
We cannot expect the Gelfand transform to be very useful for general algebras
as it loses information by representing X as a subspace of the special commutative

14.5 The Gelfand Transform
339
algebra C(τ); for example, 
S−1T S = S−1TS = T . But for commutative Banach
algebras the situation is much improved:
Theorem 14.38
For a commutative Banach algebra,
im T = τ(T ) = δ(T ),
⊆T ⊆C(τ) = ρ(T ),
ker G = J .
Proof Any maximal ideal of a commutative Banach algebra is the kernel of some
character: Given a closed ideal M, the mapping (T ) := T + M is a Banach
algebra morphism X ∈X/M with M = ker  (Exercise 13.10(19)). By Exercise
13.10(18), when M is also maximal in X, then X/M has no non-trivial ideals, and
so is isomorphic to C (Example 14.5(4)). Hence  : X ∈X/M ∼= C is a character.
But any non-invertible T belongs to some maximal ideal M (Example 13.5(8));
so there must be some ψ ∞τ such that M = ker ψ, implying ψT = 0. Thus T −α
is not invertible if, and only if, there is a ψ ∞τ, with ψT −α = ψ(T −α) = 0,
i.e., α ∞τ(T ), and therefore τ(T ) = δ(T ). (Note that this shows the existence of
characters in a commutative Banach algebra.) Since the two sets are the same, they
have the same greatest extent,
⊆T ⊆C = max
ψ∞τ |ψT | = ρ(T ).
The quasinilpotents are in the radical: If Q is a quasinilpotent, and T ∞X, then
ρ(T Q) = lim
n∈∗
⎨⎨(T Q)n⎨⎨1/n = lim
n∈∗
⎨⎨T n Qn⎨⎨1/n ⩽ρ(T )ρ(Q) = 0,
so Q is in the radical. Moreover, ker G = J since
T = 0 √τ(T ) = { 0 } √δ(T ) = { 0 }.
⊓⇐
Proposition 14.39
A Banach algebra which satisﬁes, for some c > 0 and all T ,
⊆T ⊆2 ⩽c⊆T 2⊆,
can be embedded in the commutative semi-simple Banach algebra C(τ),
via the Gelfand map.

340
14
Spectral Theory
Proof By induction on n,
⊆T ⊆2n ⩽(c⊆T 2⊆)2n−1 ⩽· · · ⩽c2n−1⊆T 2n⊆
from which can be concluded
⊆T ⊆⩽lim
n∈∗c1−2−n⊆T 2n⊆
2−n
= c ρ(T ).
This inequality has various strong implications:
X is semi-simple: 0 is clearly the only quasinilpotent.
X is commutative: For any S, T ∞X,
⊆ST ⊆⩽c ρ(ST ) = c ρ(T S) ⩽c⊆T S⊆.
Hence, the analytic function F(z) := e−zT SezT is bounded,
⇔z ∞C,
⊆F(z)⊆⩽c⊆SezT e−zT ⊆= c⊆S⊆.
By Liouville’s theorem, F must be constant, e−zT SezT = S, that is, ezT S = SezT .
Comparing the second terms of their power series expansions,
(1 + zT + o(z))S = S(1 + zT + o(z)),
gives T S = ST .
The Gelfand map is an embedding: G has the trivial kernel J , and is thus an
algebra isomorphism onto 
X ♦C(τ). Moreover, ⊆T ⊆⩽c ρ(T ) = c⊆T ⊆C, so G−1
is continuous.
⊓⇐
Exercises 14.40
1. In C, as well as CN, γ∗and C[0, 1], the only quasinilpotent is 0.
2. Quasinilpotents are preserved by Banach algebra morphisms.
3. A quasinilpotent upper triangular matrix must have 0s on the main diagonal, so
is nilpotent. Deduce, using the Jordan canonical form and Theorem 13.8, that
every quasinilpotent of a ﬁnite-dimensional Banach algebra is nilpotent.
4. (Q, R) ∞X × Y is quasinilpotent (or radical) when both Q and R are.
5. The operator V : γ∗∈γ∗deﬁned by V (an) := (0, a0, a1/2, a2/3, . . .) is
quasinilpotent.
6. Prove directly that the Volterra operator f ∀∈
⎩x
0 f , on C[0, 1], is a quasinilpo-
tent.

14.5 The Gelfand Transform
341
7. A quasinilpotent for which ⊆(z −T )−1⊆⩽
c
|z|n for all z in a neighborhood of 0,
must in fact be a nilpotent. (Hint: use ⊆T n⊆⩽
1
2∂
⎩
|z|n⊆(z −T )−1⊆dz ⩽πc.)
8. ρ(T QS) = 0 for any S, T ∞X, Q ∞J . (Hint: Example 14.2(5).)
9. If ψ ∞τ and f ∞Cη(δ(T )), then ψ f (T ) = f (ψT ).
10. S(T ) and τ(T ) have better properties than δ(T ), and may yield useful infor-
mation about it:
(a) S(S + T ) ♦S(S) + S(T ), S(1) = { 1 }, S(αT ) = α S(T ),
(b) τ(S + T ) ♦τ(S) + τ(T ), τ(ST ) ♦τ(S)τ(T ).
11. For CN, τ = { δ1, . . . , δN } where δi(z1, . . . , zN) := zi are the dual basis. The
same is true for the space c0, τ = { δi ∞c◦
0 : δi(a0, a1, . . .) = ai }.
12. For B(C2) (and B(CN)), τ = ∅. (Hint: Consider products of
⎛1 0
0 0
⎜
,
⎛0 1
0 0
⎜
,
etc.)
13. For characters of the group algebra CG, ψ(eh−1gh) = ψ(eg) and |ψ(eg)| = 1.
14. ▶The invertible elements of a commutative X correspond to the invertible
elements of 
X.
15. The Gelfand transform on CN, mapping CN ∈C(τ) ∼= CN, is the identity
map. The same is true for C[0, 1], so δ( f ) = im f for f ∞C[0, 1].
16. ▶The Gelfand transform gathers together various classical transforms under
one theoretical umbrella:
(a) Generating functions: G : γ1 ∈C(D), maps a sequence x = (an) to a
power series on D, the unit closed disk in C,
(an) ∀∈
∗
⎬
n=0
anzn.
(b) G : γ1(Z) ∈C(S1) is similar,x(ξ) :=
∗
⎬
n=−∗
aneinξ. It follows that δ(x) =
{x(ξ) : 0 ⩽ξ < 2∂}, and the sequence x is invertible in γ1(Z) (in the
convolution sense) exactly when ⎪
n aneinξ ⇒= 0 for all ξ. This is essentially
Wiener’s theorem: If f ∞C(S1) is nowhere 0 and f ∞γ1(Z) then the Fourier
coefﬁcients of 1/f are also in γ1(Z).
(c) Fourier coefﬁcients: L1(S1) ∈C(Z) ≡γ∗(Z),
f (n) :=
 2∂
0
e−inξ f (ξ) dξ.

342
14
Spectral Theory
(d) Fourier transform: L1(R) ∈C(R),
f (ξ) :=

e−ixξ f (x) dx.
(e) Laplace transform: L1(R+) ∈C(C+),
L f (s) :=
 ∗
0
e−sx f (x) dx,
Re s ⩾0.
In all these cases, 
f ◦g = f g.
17. * In any Banach algebra, if ST = T S then δ(S + T ) ♦δ(S) + δ(T ) and
δ(ST ) ♦δ(S)δ(T ). (Hint: Consider the commutant algebra { S, T }⊂⊂(Exercise
13.10(14) and Example 13.5(6)).)
18. In a commutative Banach algebra, eS+T = eSeT , and DeT = eT .
The set of exponentials eX is a connected group, so eX = E = G1 (Proposition
13.24).
19. A Banach algebra which satisﬁes ⊆T 2⊆= ⊆T ⊆2 is isometrically isomorphic to
a subalgebra of C(τ): the condition is equivalent to ⊆T ⊆= ρ(T) = ⊆T ⊆.
20. Conversely to the proposition, a Banach algebra that can be embedded in some
C(K) (K compact) satisﬁes ⊆T ⊆2 ⩽c⊆T 2⊆.
Remarks 14.41
1. Given a compact set K ∇C, is there an element T with spectrum δ(T ) = K? Of
course, this is false in the Banach algebra C, where all spectra consist of single
points, and in B(CN), where the spectra are ﬁnite sets of points. But in γ∗there
are elements with any given compact set K for spectrum (Example 14.2(3)).
2. The distinction between δp, δc and δr is not purely of mathematical interest.
In quantum mechanics, a solution of Schrödinger’s time-independent equation
Hψ = Eψ gives energy-eigenvalues with eigenfunctions that are “localized”
(since ψ ∞L2(R3)), whereas the continuous spectrum corresponds to “free”
states.
3. Among the operators in Section14.2, one can ﬁnd examples without point, con-
tinuous or residual spectra (and any combination thereof, except all empty). Note
also that the spectra of these examples are misleadingly not hard to compute in
contrast to generic operators.
4. There are various deﬁnitions of spectra of T that are subsets of δ(T ). The singular
spectrum isthesetofαsuchthat T −αisatopologicaldivisorofzero.Theessential
spectrum consists of α such that T −α is not Fredholm.

14.5 The Gelfand Transform
343
5. Recalling ρx(T ) := lim supn ⊆T nx⊆
1
n , deﬁned for T ∞B(X) and x ∞X (Remark
13.32(4)), suppose a closed subset of the spectrum of T is isolated from the rest
of the spectrum by a disk, δ1 ∇Br(a). If ρx(T −a) < r then x ∞X1 since
P1x =
1
2∂i
⎫
δ1
(z −T )−1x dz =
⎬
n
an(T −a)nx = x.
6. The Gelfand transform can be extended to S(X) ∈S(T ) retaining the same
(non-multiplicative) properties.

Chapter 15
C∗-Algebras
B(H) is a special Banach algebra when H is a Hilbert space because there is an
adjoint operation that pairs up operators together. Its properties can be generalized
to Banach algebras as follows.
Deﬁnition 15.1
A (unital) C∞-algebra is a unital Banach algebra with an involutionmap ∞:
X √X having the properties:
T ∞∞= T,
(T + S)∞= T ∞+ S∞,
(φT )∞= ¯φ T ∞,
(ST )∞= T ∞S∞,
∗T ∞T ∗= ∗T ∗2.
A ∞-morphismis deﬁned as a Banach algebra morphism ∂which also preserves
the involution ∂(T ∞) = (∂T )∞.
Easy Consequences
1. 0∞= 0, 1∞= 1, z∞= ¯z (by expanding (0 + 1)∞, (1∞1)∞, and (z1)∞).
2. ∗T ∗= ∗T ∞∗(since ∗T ∗2 = ∗T ∞T ∗⩽∗T ∞∗∗T ∗, and so ∗T ∗⩽∗T ∞∗⩽
∗T ∞∞∗); the involution map is thus continuous and bijective. But it is neither
linear ((iT )∞= −iT ∞), nor differentiable (since (T + H)∞= T ∞+ H∞).
3. ∗T T ∞∗= ∗T ∗2.
4. (T ∞)−1 = (T −1)∞when T is invertible.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5_15,
345
© Springer International Publishing Switzerland 2014

346
15
C∞-Algebras
5. ψ(T ∞) = ψ(T ), δ(T ∞) = δ(T )∞(since (T ∞−¯φ)−1 = (T −φ)−1∞).1
One might expect that ∗T ∞∗= ∗T ∗be taken as an axiom, and indeed Banach
algebras with involutions satisfying this weaker axiom are studied and called Banach
∞-algebras. C∞-algebras resemble C more closely, except for commutativity: the
chosen axiom, which is the analogue of the familiar one ¯zz = |z|2, is much stronger
and can only be satisﬁed by a unique norm, if at all (Example 15.10(6)).
Examples 15.2
1. The simplest example is Cwith conjugacy. CN has an involution
(z1, . . . , zN)∞: = (¯z1, . . . , ¯zN).
This example extends to δ⇒.
2. C[0, 1]with conjugacy, ¯f (z) := f (z).
3. B(H) with the adjoint operator, where H is a Hilbert space (Proposition 10.20).
We will see later (Gelfand-Naimark’s Theorem 15.48) that every C∞-algebra can
be embedded into B(H) for some Hilbert space H.
4. B(H) contains the closed ∞-subalgebra
C →K := { a + T : a ♦C, T ♦B(H) compact }
5. If X and Y are C∞-algebras then so is X ×Y with (S, T )∞:= (S∞, T ∞) (Examples
13.3(7)).
6. ∃δ1(Z) has an involution (an)∞:= (¯a−n), that satisﬁes ∗x∞∗= ∗x∗but not
∗x∞∞x∗= ∗x∗2. However, it can be given a new norm, |||x||| := ∗Lx∗where
Lx y := x ∞y for y ♦δ2, and L : x ∀√Lx embeds δ1(Z) as a commutative
C∞-subalgebra of B(δ2). Similarly for L1(R).
7. ∃The group algebraCG has an involution making it a ∞-algebra, but not a C∞-
algebra,
x∞=
 ⎛
g♦G
ageg
⎜∞
:=
⎛
g♦G
¯ageg−1.
(However, it is a C∞-algebra when represented by matrices and their norms.)
Exercises 15.3
1. Polarization identity: If θ is a primitive root of unity, θn = 1, then
1 To avoid ambiguity with the closure ¯A of a set A ∈C, A∞will denote the set of conjugate numbers
{ ¯z : z ♦A }.

15 C∞-Algebras
347
T ∞S = 1
n
n
⎛
i=1
θi(S + θiT )∞(S + θiT ),
S∞S + T ∞T = 1
n
n
⎛
i=1
(S + θiT )∞(S + θiT ).
2. For any real polynomial (or power series) in T , p(T )∞= p(T ∞).
3. If T is a nilpotent, a quasinilpotent, a divisor of zero, or a topological divisor of
zero, then so is T ∞, respectively. If T ∞T is a nilpotent, then so is T T ∞; but ﬁnd
an example in B(δ2) where T ∞T is invertible yet T T ∞isn’t.
4. If T ∞T and T T ∞are both invertible then so is T ,
T −1 = (T ∞T )−1T ∞= T ∞(T T ∞)−1.
5. If the condition number of T is c, that of T ∞T is c2 (Exercise 8.14(5)).
6. The inner-automorphism T ∀√S−1T S is a ∞-automorphism exactly when SS∞
belongs to the center X ⊆(in which case S∞S = SS∞).
7. * A ∞-isomorphism B(H1) √B(H2) is of the type T ∀√LT L−1 where
L = φU, φ ̸= 0 real, and U : H1 √H2 is a Hilbert-space isomorphism.
8. A ∞-ideal is an ideal that is closed under involution. Examples include the kernel
of any ∞-morphism and the Jacobson radical.
9. If A ∈X is closed under adjoints (A∞= A), then so is its commutant A⊆(which
is thus a C∞-subalgebra) (Exercise 13.10(14)).
10. * Suppose X has no unity but otherwise satisﬁes all the axioms of a C∞-algebra.
Show that the embedding L : X √B(X) (Theorem 13.8) is still isometric,
and that LX →[[I]] with the adjoint operation (La + φ)∞:= La∞+ ¯φ is a unital
C∞-algebra.
15.1 Normal Elements
It is a well-known fact in Linear Algebra that real symmetric matrices are
diagonalizable with real eigenvalues and orthogonal eigenvectors. This makes
them particularly useful and simple to work with, e.g. if T = P DP−1 then
f (T ) = P f (D)P−1 can easily be calculated when D is diagonal. However, these
matrices do not exhaust the set of diagonalizable matrices via orthogonal eigenvec-
tors: for example, diagonalizable matrices, such as
⎝1 −1
1 1
⎞
, may have complex
eigenvalues. As we shall see later, diagonalization is closely related to the commu-
tativity of T with T ∞.

348
15
C∞-Algebras
Deﬁnition 15.4
An element T is called normal when T ∞T = T T ∞, unitary when T ∞= T −1,
and self-adjoint when T ∞= T .
Examples 15.5
1. It is clear that self-adjoint and unitary elements are normal.
2. Any z ♦C is normal; it is self-adjoint only when z ♦R; it is unitary only when
|z| = 1.
3. A diagonal matrix is normal; it is self-adjoint when it is real, and unitary when
each diagonal element is of unit length |aii| = 1.
More generally, diagonalizable matrices, of the type T = U DU∞where U is
unitary and D is diagonal, are normal: T ∞T = U D∞U ∞U DU ∞= U D∞DU ∞=
U DD∞U ∞= T T ∞.
4. The operator T f (x) :=
⎟1
0 k(x, y) f (y) dy on L2[0, 1] is normal when (Example
8.6(4c))
1
⎠
0
k(s, x)k(s, y) dy =
1
⎠
0
k(x, s)k(y, s) ds
a.e.(x, y)
5. When T is normal, a polynomial in T and T ∞looks like
p(T, T ∞) =
⎛N
n=1
⎛M
m=1 an,mT nT ∞m.
The set of such polynomials C[T, T ∞] is a commutative ∞-subalgebra. The char-
acter space of its closure C[T, T ∞] is denoted by αT .
6. A unitary matrix is a square matrix whose column vectors are orthonormal. A
self-adjoint matrix is a square matrix [ai j] such that a ji = ai j, e.g.
⎝1 i
−i 0
⎞
.
Proof If ui denotes the ith column of U, then U ∞U = I implies
⇐ui, u j⊂= u∞
i u j = αi j.
7. The unitary operators of B(H) are the Hilbert-space automorphisms of H
(Proposition 10.23).
8. ▶If T is normal, then so are T ∞, T + z, zT , T n, and T −1 when it exists. But the
addition and product of normal elements need not be normal, e.g.
⎝1 0
0 2
⎞
and
⎝i i
i i
⎞
.

15.1 Normal Elements
349
Proof for T −1. Taking the inverse of T T ∞= T ∞T together with (T −1)∞=
(T ∞)−1 gives the normalcy of T −1.
9. ▶If Tn are normal and Tn √T , then T is also normal, i.e., the set of normal
elements is closed (as are the sets of self-adjoint and unitary elements).
Proof The limit as n √⇒of T ∞
n Tn = TnT ∞
n is T ∞T = T T ∞since the adjoint is
continuous. Similarly take the limit of T ∞
n = Tn or T ∞
n = T −1
n
to prove the other
statements.
10. ▶If S, T are self-adjoint, then so are S + T , φT (φ ♦R), p(T ) for any real
polynomial p, and T −1 if it exists. But ST is self-adjoint iff ST = T S.
11. ▶If T is self-adjoint, then eiT is unitary; in fact, letting U t := eitT , t ♦R, gives
a one-parameter group of unitary elements (Exercises 13.25(9) for deﬁnition).
The analogy of self-adjoint elements with real numbers and unitary elements with
unit complex numbers raises the issue of which propositions about complex numbers
generalize to C∞-algebras.
Proposition 15.6
Every element T can be written uniquely as A + i B with A and B self-
adjoint, called the real and imaginary parts of T , respectively.
The real and imaginary parts of T are denoted Re T and Im T .
Proof Simply check that A := (T + T ∞)/2 and B := (T −T ∞)/2i are self-adjoint.
The sum A+i B is obviously T . Uniqueness follows from the fact that if A+i B = 0
for A, B self-adjoint then A = 0 = B since
A = A∞= (−i B)∞= i B = −A.
⇔∩
Proposition 15.7
The set of unitary elements U(X) is a closed subgroup of G(X),
U, V unitary ∪UV, U −1 unitary.
Unitary elements have unit norm, ∗U∗= 1.
Proof IfUn are unitary andUn √T , then by continuity of the involution,U ∞
n √T ∞.
Also, the equations U ∞
n Un = 1 = UnU ∞
n become T ∞T = 1 = T T ∞in the limit, that
is, T −1 = T ∞.

350
15
C∞-Algebras
For any U, V ♦U(X), UV and U ∞(= U −1) are also unitary,
(UV )∞= V ∞U ∞= V −1U −1 = (UV )−1
U ∞∞= U = (U−1)−1 = (U ∞)−1.
Finally, ∗U∗2 = ∗U ∞U∗= ∗1∗= 1.
⇔∩
The next theorem starts to unravel the close connection between normal elements
and their spectra.
Proposition 15.8
For T normal, ψ(T ) = ∗T ∗, and S(T ) is the closed convex hull of δ(T).
Proof (i) For any normal element T , ∗T 2∗= ∗T ∗2 since
∗T ∗4 = ∗T ∞T ∗2 = ∗(T ∞T )∞(T ∞T )∗= ∗(T 2)∞T 2∗= ∗T 2∗
2.
But T 2 itself is normal, so the doubling game can be repeated to get, by induction,
∗T 2k∗= ∗T ∗2k and
ψ(T ) = lim
n√⇒∗T n∗1/n = lim
k√⇒∗T 2k∗2−k = ∗T ∗.
(ii) As S(T ) is a closed convex set that contains δ(T ) (Proposition 14.34), it must
also contain the convex hull of the latter. Notice that, by (i), δ(T) reaches to the
boundary of S(T ).
λ
co(σ(T ))
Br[z]
Conversely, suppose φ is not in the closed con-
vex hull of δ(T ). There must be a straight line
through φ not intersecting δ(T ) (why? Hint: con-
sider rays emanating from φ; they intersect the
closed convex hull over an interval of angles). So
the spectrum can be enclosed by a ball Br[z] that
does not meet the line (Exercise 6.22(7)).
For any ρ ♦S,
|ρT −z| = |ρ(T −z)| ⩽∗T −z∗= ψ(T −z) ⩽r < |φ −z|
so φ ̸= ρT . It follows that S(T ) has the same points as the closed convex hull of
δ(T).
⇔∩

15.1 Normal Elements
351
Proposition 15.9
Fuglede’s theorem
If T is normal and ST = T S then ST ∞= T ∞S.
Proof From f (T )S = Sf (T ) (Example 14.23(1b)), we have e−¯zT Se¯zT = S. Writ-
ing ¯zT = A + i B and noting that ¯zT is normal, so AB = B A (Example 15.10(1a)),
we ﬁnd
F(z) := e−zT ∞SezT ∞= e−A+i B SeA−i B
= e2i Be−¯zT Se¯zT e−2i B
= e2i BSe−2i B
∴∗F(z)∗⩽∗S∗
by Example 15.5(11).
As F is a bounded analytic function of z, by Liouville’s theorem it is constant,
F(z) = F(0) = S, i.e., ezT ∞S = SezT ∞. Comparing the second term of their power
series gives T ∞S = ST ∞.
⇔∩
Examples 15.10
1. If T = A + i B, where A, B are self-adjoint, then T ∞= A −i B and
T ∞T = (A2 + B2) + i[A, B],
T T ∞= (A2 + B2) −i[A, B],
(note that i[A, B] = 1
2[T ∞, T ] is self-adjoint). So,
(a) T is normal if, and only if, AB = B A;
(b) T is unitary if, and only if, AB = B A and A2 + B2 = 1;
(c) T is self-adjoint if, and only if, B = 0.
2. X is commutative if, and only if, every element is normal.
Proof If every element is normal, then for any T = A + i B, AB = B A, i.e., any
two self-adjoint elements commute. But then T S = (A + i B)(C + i D) = ST .
The converse is obvious.
3. (a) For T normal, ∗T n∗= ∗T ∗n, since ∗T ∗= ψ(T ) ⩽∗T n∗1/n ⩽∗T ∗.
(b) For any T , ∗T ∗2n = ∗(T ∞T )n∗and ∗T ∗= √ψ(T ∞T ).
4. ▶0 is the only normal quasinilpotent and the only radical element, that is, every
C∞-algebra is semi-simple. More generally, if T is normal with δ(T ) = { z }, then
T = z.
Proof If Q is a normal quasinilpotent, then ∗Q∗= ψ(Q) = 0, so Q = 0. If P is
a radical element, then ∗P∗2 = ∗P∞P∗= ψ(P∞P) = 0.

352
15
C∞-Algebras
5. Every C∞-algebra has a unique norm satisfying ∗T ∞T ∗= ∗T ∗2.
Proof Suppose there is a second C∞-norm. Then the norms must agree on normal
elements, ∗T ∗= ψ(T ) = |||T|||, and so must agree on all elements
∗T ∗= ∗T ∞T ∗
1
2 = |||T ∞T |||
1
2 = |||T|||.
Exercises 15.11
1. What are the normal, self-adjoint and unitary elements of δ⇒and C[0, 1]?
2. Generalizing from diagonal matrices, any multiplier operator on δ2, (an) ∀√
(bnan) is normal. It is self-adjoint when bn ♦R, and unitary when |bn| = 1,
for all n.
Find similar conditions for a multiplier operator on L2(R), T f
:= g f ,
(g ♦C(R)).
3. Triangular matrices, such as
⎝1 1
0 2
⎞
, are not normal (unless diagonal). A real
diagonalizable matrix, such as
⎝0 1
−1 0
⎞
, need not be self-adjoint.
4. For any T , αT + βT ∞is normal when |α| = |β|.
5. A ∞-morphism preserves normal, self-adjoint, and unitary elements.
6. If Pi arenormalidempotentswith Pi Pj = αi j Pi aswellas P1+· · ·+Pn = 1,then
z1P1 + · · · + zn Pn is normal (unitary when |zi| = 1) and for any polynomial p,
p(z1P1 + · · · + zn Pn) = p(z1)P1 + · · · + p(zn)Pn.
Unitary elements
7. The shift-operators on δ2(Z) are unitary, with δ(R) = δ(L) = S1 (but on δ2,
they are not even normal).
8. Translations Ta f (x) := f (x −a) and stretches Sa f (x) := a
1
2 f (ax) (a > 0),
acting on L2(R), are unitary.
9. If U is unitary then for any T , ∗UT ∗= ∗T ∗= ∗TU∗.
10. If U ♦X is unitary and V := φU (φ ̸= 0), then T ∀√V −1T V is an inner
∞-automorphism of X.
11. If T is an invertible normal element, then T ∞T −1 is unitary.
For example, the Cayley transformation U := (i −T ∞)(i + T )−1 maps T to a
unitary element if i + T is invertible. Compare with the Möbius transformation
z ∀√(i −¯z)/(i +z), which takes R to the unit circle (0 ∀√1, 1 ∀√i, ⇒∀√−1).
12. U(X) need not be a normal subgroup of G(X); when does T −1UT ∈U hold?

15.1 Normal Elements
353
Self-Adjoint elements
13. The operator T f (x) :=
⎟
k(x, y) f (y) dy on L2(R) (k ♦L2(R2)) is self-adjoint
when k(y, x) = k(x, y) a.e. (Hint: Examples 10.24(3), 8.6(4)).
14. For any T ♦X, the elements T + T ∞, T ∞T and T T ∞are self-adjoint.
15. The real and imaginary parts of T satisfy ∗Re T ∗⩽∗T ∗, ∗Im T ∗⩽∗T ∗.
16. Find the real and imaginary parts of ST when S and T are self-adjoint.
Spectra of Normal Elements
17. For S, T normal, ψ(S + T ) ⩽ψ(S) + ψ(T ), and ψ(ST ) ⩽ψ(S)ψ(T ).
18. When T is normal, then ∗T ∗eiθ is a spectral value for some θ.
19. Let Q ̸= 0 be a quasinilpotent, then 1 + Q is not normal. More generally, if T
is normal and T Q = QT , then T + Q is not normal.
20. If A∞B
= 0 =
AB∞, then ∗A + B∗= max(∗A∗, ∗B∗). (Hint: Show
∗A + B∗2n = ∗(A∞A)n + (B∞B)n∗.)
21. If S and T are commuting normal elements, then ST is also normal.
22. If T ∞T is an idempotent then so is T T ∞.
23. A commutative C∞-algebra is isometrically embedded in some C(K) (Exercise
14.40(19)).
24. Let ∂: X √Y be a ∞-morphism between C∞-algebras with X commutative.
Then ∂(T ) is normal in Y for any T ♦X, and ∂is continuous with ∗∂∗⩽1
(Hint: δ(∂(T)) ∈δ(T )).
15.2 Normal Operators in B(H)
Let us see what properties normal elements have for the most important C∞-algebra,
B(H) when H is a Hilbert space.
Proposition 15.12
For a normal operator T ♦B(H),
(i) ∗T ∞x∗= ∗T x∗,
(ii) ker T 2 = ker T = ker T ∞= (im T )◦,
(iii) im T is dense in H ≈T is 1–1,
(iv) T is invertible in B(H) ≈
≡c > 0, ∀x ♦H,
c∗x∗⩽∗T x∗.

354
15
C∞-Algebras
Proof (i) follows from
∗T ∞x∗2 = ⇐T ∞x, T ∞x⊂= ⇐x, T T ∞x⊂= ⇐x, T ∞T x⊂= ⇐T x, T x⊂= ∗T x∗2.
(ii) ker T = ker T ∞is due to T ∞x = 0 ≈∗T ∞x∗= ∗T x∗= 0 ≈T x = 0, using
(i). ker T 2 = ker T , i.e., T 2x = 0 ≈T x = 0 follows from
∗T x∗2 = ⇐x, T ∞T x⊂⩽∗x∗∗T ∞T x∗= ∗x∗∗T 2x∗.
From Proposition 10.21, (im T )◦= ker T ∞= ker T .
(iii) By (ii), T is 1–1 if, and only if im T = (ker T )◦= 0◦= H.
(iv)If T hasacontinuousinverse,then∗x∗= ∗T −1T x∗⩽∗T −1∗∗T x∗.Conversely,
if the given inequality is true for all x ♦H, then T is 1–1 and the image of T is closed
(Examples 8.13(3)). By (iii), im T = H and T is bijective. Its inverse is continuous:
c∗T −1x∗⩽∗T T −1x∗= ∗x∗,
∀x ♦H.
⇔∩
Proposition 15.13
For a normal operator T ♦B(H),
(i) T v = φv ≈T ∞v = ¯φv, and eigenvectors of distinct eigenvalues of
T are orthogonal,
(ii) δ(T) contains no residual spectrum,δr(T ) = ∅,
(iii) isolated points of δ(T ) are eigenvalues.
Proof (i) is a direct application of ker(T −φ) = ker(T ∞−¯φ), as T −φ is normal.
Note that the eigenvectors of T and T ∞are identical. For eigenvalues φ and μ with
corresponding eigenvectors x and y, we have
φ⇐y, x⊂= ⇐y, T x⊂= ⇐T ∞y, x⊂= ⇐¯μy, x⊂= μ⇐y, x⊂,
implying either φ = μ or ⇐y, x⊂= 0.
(ii) Let φ ♦δ(T); either T −φ is not 1–1, in which case φ is an eigenvalue (point spec-
trum); or it is 1–1, in which case its image is dense in H by the previous proposition,
and φ forms part of the continuous spectrum.
(iii) If { φ } is an isolated point of δ(T ), form the projection
P :=
1
2πi
⎫
{φ}
(z −T )−1 dz

15.2 Normal Operators in B(H)
355
onto a space Xφ ̸= 0 (Example 14.28(1)). Then δ(T|Xφ) = { φ }, and since T |Xφ is
normal as well, ∗T |Xφ −φ∗= ψ(T |Xφ −φ) = 0, i.e., T x = φx for any x ♦Xφ. ⇔∩
Examples 15.14
1. ▶A projection P ♦B(H) is normal ≈self-adjoint ≈orthogonal ≈∗P∗= 0
or 1.
Proof If P is orthogonal (Theorem 10.12), then (x −Px) ◦Px, so
⇐x, Px⊂= ⇐(I −P)x + Px, Px⊂= ∗Px∗2 ♦R
hence ⇐x, Px⊂= ⇐Px, x⊂= ⇐x, P∞x⊂for all x ♦H, and P = P∞(Example
10.7(3)).
If ∗P∗= 1, let x ♦(ker P)◦, so that x ◦x −Px. Then ∗Px∗2 = ∗x∗2 +
∗Px −x∗2, yet ∗Px∗⩽∗x∗, so x = Px ♦im P and ker P ◦im P. The other
implications should be obvious.
2. Allspectralvaluesofanormaloperatorareapproximateeigenvalues(eithereigen-
values or part of the continuous spectrum) and there are no proper generalized
eigenvectors (Section 14.3). Note that a normal operator need not have any eigen-
values, e.g. T f (x) := x f (x) on L2[0, 1].
Exercises 15.15
1. ▶Conversely to the proposition, an operator which satisﬁes ∗T ∞x∗= ∗T x∗for
all x is normal.
2. When T is a normal operator, ker T and im T are both T - and T ∞-invariant.
3. Suppose Tnx √T x for all x ♦H where Tn are normal operators in B(H).
(T is an operator by Corollary 11.35.) Then T is normal if, and only if, ∀x,
T ∞
n x √T ∞x.
4. The eigenvalues of self-adjoint operators are real, and those of unitary operators
satisfy |φ| = 1.
5. A normal operator on a separable Hilbert space can have at most a countable
number of distinct eigenvalues.
6. Suppose H has an orthonormal basis of eigenvectors of an operator T ♦B(H).
Show that T is normal. (Hint: show ∗T ∞x∗= ∗T x∗.)
7. If T x = φx, T ∞y = μy, and μ ̸= ¯φ then ⇐y, x⊂= 0 (T not necessarily normal).
8. An Ergodic Theorem: Consider the Cesáro sum
Tn := (I + T + · · · + T n−1)/n.
If ψ(T ) < 1 then Tn ∇(I −T )−1/n √0 as n √⇒. Now let T be a normal
operator with ψ(T ) = 1.

356
15
C∞-Algebras
(a) For T x = x (i.e., x ♦ker(T −I)), we get Tnx = x;
(b) For x = y −T y ♦im(T −I) we get Tnx = (y −T ny)/n √0;
(c) For any x ♦H, Tnx √x0 ♦ker(T −I), the closest ﬁxed point of T .
If T is not normal then Tn may diverge, e.g. T
=
⎝1 a
0 1
⎞
gives Tn =
⎝1 (n −1)a/2
0
1
⎞
.
The Numerical Range
To help us further with analyzing the spectra of normal operators, we require an
additional tool. A given vector x need not, of course, be an eigenvector of an operator
T , but we can ask for that value of φ which minimizes ∗T x −φx∗. According to
Theorem 10.12 there is indeed a unique vector φx ♦[[x]] which is closest to T x,
and it satisﬁes (T x −φx) ◦x, or equivalently, φ = ⇐x, T x⊂/∗x∗2. This number is
sometimes called the mean value of T at x, or the Rayleigh coefﬁcient, and denoted
by ⇐T ⊂x. We are thus led to the following deﬁnition:
Deﬁnition 15.16
The numerical rangeof an operator T ♦B(H) Is the set
W(T ) := { ⇐x, T x⊂: ∗x∗= 1 }.
Examples 15.17
1. ⇐I⊂x = 1, ⇐T + S⊂x = ⇐T ⊂x + ⇐S⊂x, ⇐φT ⊂x = φ⇐T ⊂x, ⇐T ∞⊂x = ⇐T ⊂x.
These are easily veriﬁed, e.g.
⇐x, T ∞x⊂= ⇐T x, x⊂= ⇐x, T x⊂
2. ▶For operators on a complex Hilbert space,
(a) W(I) = { 1 }, and W(z) = { z } (z ♦C),
(b) W(T + z) = W(T ) + z (translations), and W(φT ) = φW(T ),
(c) W(S + T ) ∈W(S) + W(T ),
(d) W(T ∞) = W(T )∞.
3. W(T ) includes the eigenvalues of T and is bounded by ∗T ∗.
Proof If T x = φx for x a unit vector, then ⇐T ⊂x = ⇐x, T x⊂= φ. Also, for unit
x, |⇐x, T x⊂| ⩽∗T x∗⩽∗T ∗by the Cauchy-Schwarz inequality.

15.2 Normal Operators in B(H)
357
4. Although the quadratic form x ∀√⇐x, T x⊂is unique to T , i.e., ⇐x, T x⊂= ⇐x, Sx⊂
for all x if, and only if, T = S (Example 10.7(3)), the numerical range W(T )
does not identify T in general, e.g. W(U −1TU) = W(T ) when U is unitary.
5. For a ﬁxed unit x ♦H, one can deﬁne two semi-inner-products on B(H),
(a) ⇐S, T ⊂:= ⇐Sx, T x⊂= ⇐S∞T ⊂x (with associated semi-norm |||T|||x :=
∗T x∗), and
(b) the covariance semi-inner-product
Cov(S, T ) := ⇐S −⇐S⊂x,
T −⇐T ⊂x⊂= ⇐S∞T ⊂x −⇐S⊂x⇐T ⊂x,
with the associated semi-norm called the standard deviation
δ2
T := Cov(T, T ) = ∗T x∗2 −
⎪⎪⇐T ⊂x
⎪⎪2 .
(c) The uncertainty principlestates that δSδT ∴|Cov(S, T )| (essentially the
Cauchy-Schwarz inequality (Exercise 10.10(17))). The normalized inner
product Cov(S, T )/δSδT is called the correlation; T and S are called
independent when they are orthogonal, Cov(S, T ) = 0, so that ⇐S, T ⊂=
⇐S⊂x⇐T ⊂x.
These deﬁnitions are usually applied to L2(A), where x corresponds to a function
p ♦L2(A),with|p(s)|2 interpretedasaprobabilitydistribution,andtheoperators
are multiplications by functions T p := f p, that is,
the mean ⇐f ⊂p =
⎟
A f (s)|p(s)|2 ds,
the rms ||| f |||p =
⎬⎟
A | f |2|p|2
Cov( f, g) =
⎟
A( f −⇐f ⊂)(g −⇐g⊂)|p|2.
We can now elucidate the connection between the numerical range and the spec-
trum of an operator, hinted at in the examples above.
Proposition 15.18
(Hausdorff-Toeplitz)
W(T ) is a convex compact subset of C, such that
δ(T ) ∈W(T ) ∈S(T ).
Proof Recall the state space S(X) from Deﬁnition 14.33, where we now take the
case X = B(H). The inclusion W(T ) ∈S(T ) is obvious: for any unit vector x, the
functional ρ(T ) := ⇐x, T x⊂is linear in T , maps I to 1, and |ρ(T)| = |⇐x, T x⊂| ⩽
∗T ∗, so ∗ρ∗= 1 and ρ ♦S. As S(T ) is compact (Proposition 14.34), so must be
its closed subset W(T ).

358
15
C∞-Algebras
The main part of the proof is to show the other inclusion δ(T) ∈W(T ): for
∗x∗= 1, φ ♦C,
α := d(φ, W(T )) ⩽|⇐x, T x⊂−φ| = |⇐x, (T −φ)x⊂| ⩽∗(T −φ)x∗,
so for any x ♦H,
α∗x∗⩽∗(T −φ)x∗.
When φ ̸♦W(T ), α is strictly positive, and the inequality shows that T −φ is 1–
1 with a closed image (Example 8.13(3)). Moreover, since W(T ∞) = W(T )∞and
d(¯φ, W(T )∞) = d(φ, W(T )),
α∗x∗⩽∗(T ∞−¯φ)x∗.
This implies that (T −φ)∞is 1–1, hence T −φ is onto (Proposition 10.21). Thus
T −φ has an inverse, which is continuous (Proposition 8.12),
α∗(T −φ)−1x∗⩽∗(T −φ)(T −φ)−1x∗= ∗x∗,
and φ ̸♦δ(T).
W(T ) is convex: Given φ, μ in W(T ) (φ ̸= μ), let x, y be unit vectors such that
⇐x, T x⊂= φ, ⇐y, T y⊂= μ. Any vector v := αeiρ1x + βeiρ2 y (α, β, ρ1, ρ2 ♦R)
has norm
∗v∗2 = α2 + 2αβ Re ei(ρ2−ρ1)⇐x, y⊂+ β2 = 1 + sin 2θ Re(eiρ⇐x, y⊂),
for α = cos θ, β = sin θ, ρ := ρ2 −ρ1. Then ⇐v, T v⊂works out to
⇐αeiρ1x + βeiρ2 y, αeiρ1T x + βeiρ2T y⊂= α2φ + αβ(eiρ⇐x, T y⊂+ e−iρ⇐y, T x⊂) + β2μ
= φ cos2 θ + sin 2θ(w cos ρ + z sin ρ) + μ sin2 θ
= φ + μ
2
+ φ −μ
2
cos 2θ + (w cos ρ + z sin ρ) sin 2θ
where w := 1
2(⇐x, T y⊂+⇐y, T x⊂), z := i
2(⇐x, T y⊂−⇐y, T x⊂). But wρ := w cos ρ +
z sin ρ traces out an ellipse as ρ varies. By choosing the correct value of ρ, wρ can
be made to point in any direction in the complex plane, including that of φ−μ. With
this choice, ⇐v, T v⊂/∗v∗2 gives a line segment as θ varies, a line that contains φ and
μ (at θ = 0, π/2). Thus W(T ), and its closure W(T ), are convex sets.
⇔∩
As an immediate corollary, this proposition allows us to identify the self-adjoint
operators among the normal ones from their spectrum:

15.2 Normal Operators in B(H)
359
Proposition 15.19
For a normal operator T ,
(i) W(T ) = S(T ),
(ii) T is self-adjoint ≈W(T ) is real.
Proof (i) By the previous theorem, W(T ) ∈S(T ), so the reverse inclusion remains
to be shown. The closure of the numerical range W(T ) is a convex set contain-
ing δ(T), so it contains its closed convex hull, which is S(T ) when T is normal
(Proposition 15.8).
(ii) When T ♦B(H) is self-adjoint, ⇐x, T x⊂= ⇐T x, x⊂= ⇐x, T x⊂for all x ♦H,
which implies W(T ) ∈R. Conversely, if ⇐x, T x⊂♦R for all vectors x, then
⇐T x, x⊂= ⇐x, T x⊂= ⇐T ∞x, x⊂
which can only hold when T ∞= T (Example 10.7(3)). Note that this implies that T
is self-adjoint ≈δ(T ) ≤R, since W(T ) would be a line interval.
⇔∩
Exercises 15.20
1. W(T ) = { z } ≈T = z.
2. Show that, for the shift operators on δ2, W(L) = B1[0] = W(R).
3. Let T be a square matrix
⎝A B
C D
⎞
with respect to an orthonormal basis, where
A, D are square sub-matrices.
(a) W(A) ∪W(D) ∈W(T ).
(b) If B = C = 0, then W(T ) is the closed convex hull of W(A) ∪W(D).
4. Write a program that plots W(T ) for 2 × 2 matrices, and test it on random
matrices. Verify, and then prove, that W(T ) for
(a) T :=
⎝a 0
0 b
⎞
is the line joining a to b;
(b) T :=
⎝a 1
0 a
⎞
is the closed disk B 1
2 [a] (although its spectrum is { a });
(c) * T :=
⎝a b
c d
⎞
is generically an ellipse with its interior.

360
15
C∞-Algebras
5. Let T be a square matrix with positive coefﬁcients. If x = (a1, . . . , aN) ♦CN
and x+ := (|a1|, . . . , |aN|), then
|⇐x, T x⊂| ⩽⇐x+, T x+⊂
so that the largest extent of W(T ) (and W(T ∞)) is a positive real number.
6. The classical proofs of some of the statements above do not use the convexity
properties of the numerical range. For a self-adjoint operator T ,
(a) δ(T ) is real. Prove this by letting φ := α + iβ with β ̸= 0, and showing
∗(T −φ)x∗2 = ∗(T −α)x∗2 + β2∗x∗2 ∴|β|2∗x∗2.
(b) W(T ) is the smallest interval containing δ(T). Show this by taking δ(T) ∈
[a, b], letting c := (a + b)/2, and proving that for any unit vector x,
|⇐x, T x⊂−c| = |⇐x, (T −c)x⊂| ⩽b −c = c −a.
7. For any T ♦B(H1, H2), W(T ∞T ) = [a, b], where a ∴0 and b = ∗T ∗2.
8. If φ ̸♦W(T ), then ∗(φ −T )−1∗⩽1/d(φ, W(T )).
9. A coercive operator T ♦B(H) satisﬁes |⇐x, T x⊂| ∴c > 0 for all unit x ♦H.
Show that it has a continuous inverse. An elliptic operator is one which satisﬁes
⇐x, T x⊂∴c > 0, a special case of a coercive self-adjoint operator.
10. Let ρ : B(H) √C be deﬁned by T ∀√⇐x, T y⊂for some ﬁxed unit x, y ♦H;
show that ρ ♦S ≈x = y.
11. (a) Cov(I, T ) = 0, Cov(S, T + φ) = Cov(S, T ), δT +φ = δT ;
(b) For every φ, δT ⩽∗(T −φ)x∗, so δT ⩽1
2 diam δ(T) for T normal;
(c) δT = 0 ≈x is an eigenvector of T , with eigenvalue ⇐T ⊂x.
(d) If S, T are self-adjoint operators, let A := 1
i [S, T ] and h := ⇐A⊂x/2 =
Cov(S, T ), then
δSδT ∴h.
15.3 The Spectral Theorem for Compact Normal Operators
As seen before, multiplier operators such as diagonal matrices are normal. In fact,
all normal operators are of this type; we show this ﬁrst in the simple case of compact
normal operators.

15.3 The Spectral Theorem for Compact Normal Operators
361
Theorem 15.21 Spectral Theorem for Compact Normal Operators
If T is a compact normal operator on a Hilbert space, then
T x =
⇒
⎛
n=0
φn⇐en, x⊂en,
where en are the eigenvectors of T with corresponding non-zero eigenval-
ues φn .
The statement is written supposing an inﬁnite number of eigenvectors; otherwise
the sum is ﬁnite.
Proof Let T be a compact normal operator. We show that H has an orthonormal
basis of eigenvectors.
(a) The fact that T is compact implies that the non-zero part of its spectrum
consists of a countable set of eigenvalues, and each generalized eigenspace
Xφ := ker(T −φ)kφ is ﬁnite-dimensional (Theorem 14.18).
(b) The fact that T −φ is normal implies, ﬁrst, that Xφ = ker(T −φ) consists
of eigenvectors, and second, that Xφ are orthogonal to each other (Proposition
15.12,13).
Note that the eigenvalues decrease to 0 (unless there are a ﬁnite number of them).
This is part of Theorem 14.18, but its proof in the present context is much simpler:
As T is compact, for any inﬁnite set of orthonormal eigenvectors en, T en (= φnen)
has a Cauchy subsequence, so
|φn|2 + |φm|2 = ∗φnen −φmem∗2 = ∗T en −T em∗2 √0, as n, m √⇒
implying both φn √0 and that each eigenspace ker(T −φ) is ﬁnite-dimensional.
Thus a countable number of orthonormal eigenvectors en (a ﬁnite number from
each Xφ) account for all the non-zero eigenvalues, and form an orthonormal basis
for the closed space M := [[e1, e2, . . . ]] generated by them. M◦is T -invariant since
x ♦M◦implies that for all n, ⇐en, x⊂= 0, and as T ∞en = ¯φnen,
⇐en, T x⊂= ⇐T ∞en, x⊂= φn⇐en, x⊂= 0.
Thus T can be restricted to M◦, when it remains compact (Exercise 6.9(5)) and
normal, yet without non-zero eigenvalues, because those are all accounted for by
the eigenvectors in M. Its spectrum must therefore be 0, implying T |M◦= 0, i.e.,
M◦= ker T . Unless M◦= 0, there is an orthonormal basis of eigenvectors eα for
it, and collectively with en, form a basis for H = M →M◦,

362
15
C∞-Algebras
x =
⎛
n
⇐en, x⊂en +
⎛
α
⇐eα, x⊂eα.
Finally, since T is linear and continuous, and T eα = 0, we ﬁnd that
T x = T
 ⎛
n
⇐en, x⊂en
⎜
=
⎛
n
⇐en, x⊂T en =
⎛
n
⇐en, x⊂φnen.
⇔∩
Corollary 15.22
Spectral Theorem in Finite Dimensions
A normal complex matrix is diagonalizable.
There is a remarkable generalization of this diagonalization to any compact oper-
ator between Hilbert spaces, including rectangular matrices:
Theorem 15.23 Singular Value Decomposition (SVD)
If T : X √Y is a compact operator between Hilbert spaces, then there
are isometry operators U : Y √Y and V : X √X such that T = U DV ∞
with D diagonal.
Proof T ∞T and T T ∞are compact self-adjoint operators, on X and Y respectively.
They share the same non-zero eigenvalues (Examples 14.10(5)), which are strictly
positive, since if T ∞T v = φv, ∗v∗= 1, then
φ = ⇐v, T ∞T v⊂= ∗T v∗2 > 0.
By the spectral theorem there is an orthonormal set of eigenvectors vn ♦X of T ∞T
with eigenvalues φn = δ2
n > 0. It turns out that the vectors T vn ♦Y are also
orthogonal,
⇐T vm, T vn⊂= ⇐vm, T ∞T vn⊂= δ2
nαnm,
so un := T vn/δn form an orthonormal set in Y. Note that, by the above,
T vn = δnun,
T ∞un = δnvn.
The positive numbers δn are called the singular values of T and vn, un are called its
singular vectors (un are also called the principal components of T ). In fact, vn form
an orthonormal basis for (ker T ∞T )◦= (ker T )◦= im T ∞, and similarly un is an
orthonormal basis for im T (Exercise 10.26(8) and Proposition 10.21).
It follows that for any x ♦X and y ♦Y,
x = Px +
⎛
n
⇐vn, x⊂vn,
T x =
⎛
n
δn⇐vn, x⊂un,
T ∞y =
⎛
n
δn⇐un, y⊂vn

15.3 The Spectral Theorem for Compact Normal Operators
363
where P ♦B(X) is the orthogonal projection onto ker T . Indeed a stronger statement
is true:
T =
⎛
n
δnunv∞
n
That is, the convergence is in norm, not just pointwise, the reason being
⎭⎭(T −
N
⎛
n=1
δnunv∞
n)x
⎭⎭2 =
⎭⎭
⇒
⎛
n=N+1
δn⇐vn, x⊂un
⎭⎭2
=
⇒
⎛
n=N+1
δ2
n|⇐vn, x⊂|2
⩽(max
n>N δ2
n)∗x∗2
and maxn>N δn √0 as N √⇒since δn √0 as n √⇒.
Let U be that operator representing a change of basis in im T from un to some
arbitrary basis (leaving the perpendicular space ker T ∞invariant), V a similar change
of basis in im T ∞from vn. Then the ‘matrix’ of T with respect to vn and un is
D := U ∞T V ; as T vn := δnun and T x := 0 for x ♦ker T , D is diagonal.
⇔∩
Examples 15.24
1. The spectral theorem is often stated as: If a compact normal operator has “matrix”
T with respect to a given orthonormal basis ˜en, then T = U DU −1, where D is
diagonal and U is the unitary change-of-basis operator that maps (˜en) to (en), the
orthonormal basis of eigenvectors of T .
2. The converse of the spectral theorem is true, i.e., deﬁning the operator
T x :=
⇒
⎛
n=0
φn⇐en, x⊂en
in terms of an orthonormal basis, with φn √0, gives a compact normal oper-
ator — compact because it is the limit of ﬁnite-rank operators, normal because
∗T x∗2 = ⎧
n |φn|2|⇐en, x⊂|2 = ∗T ∞x∗2.
3. Given a compact normal operator in B(H), and any function f ♦C(δ(T )), with
f (0) = 0, one can deﬁne the compact operator f (T ) by the formula
f (T )x :=
⇒
⎛
n=0
f (φn)⇐en, x⊂en.
For example,

364
15
C∞-Algebras
(a)
√
T is compact when T is a self-adjoint compact operator with positive
eigenvalues,
(b) for any φ ̸= 0, there is a projection Pφ := fφ(T ), where fφ is a continuous
function which takes the value 1 around φ and 0 around all other eigenvalues.
4. The projections Pn to the eigenspaces Xφn of T commute and are orthogonal, so
En := P1 +· · ·+ Pn is a projection onto Xφ1 +· · ·+ Xφn (Exercise 8.17(2)). The
spectral decomposition can be rewritten as T x = ⎧
n φnαEnx, where αEn :=
En −En−1 = Pn. This can be seen as a breakup of T =
1
2πi
⎟
δ(T ) z(z −T )−1 dz
into integrals on the disconnected components of the spectrum.
5. According to SVD, any matrix T can be approximated by ⎧
n δn An where
An = unv∞
n and the sum is taken over the largest singular values. Typically,
data from variables x1, . . . , xn is organized in the form of a matrix T with the
rows representing the different variables and the columns the normalized mea-
sured instances; the resulting un associated with the largest singular values are
linear combinations of the variables xn that account for the most variability in the
data.
6. If T ♦B(X) is compact normal, then the singular values of T are the absolute
values of its non-zero eigenvalues.
Proof Clearly, if T x = φx then T ∞T x = φ2x. Conversely, if T ∞T x = μx
(μ ̸= 0) then
0 = (T ∞T −μ)x =
⎛
n
(|φn|2 −μ)⇐en, x⊂en
so μ = |φn|2 for some n.
Exercises 15.25
1. Find the singular values and vectors of
⎝2 3
0 2
⎞
and
⎝1 1 0
0 1 1
⎞
.
2. If S and T are commuting self-adjoint compact operators, then they are simulta-
neously diagonalizable (Hint: consider S + iT ).
3. (a) Let T be an n × n self-adjoint matrix, with eigenvalues φ1 ⩽· · · ⩽φn
(including repeated eigenvalues), and corresponding orthonormal eigenvec-
tors v1, . . . , vn. If M is a closed linear subspace, with orthogonal projection
P,thentherestrictionof PT P to M isalsoself-adjointwitheigenvalues,say,
μ1 ⩽· · · ⩽μm, and corresponding orthonormal eigenvectors u1, . . . , um.
Taking a unit vector x ♦[[u1, . . . , ui]] ˆ [[vi, . . . , vn]] ̸= 0, we get
μ1 ⩽⇐x, T x⊂⩽μi
and
φi ⩽⇐x, T x⊂⩽φn.
It follows that φi⩽μi. Similarly, take x ♦[[ui, . . . , um]]ˆ[[v1, . . . , vi+n−m]]
̸= 0 to deduce μi ⩽φi+n−m. Combining the results we get

15.3 The Spectral Theorem for Compact Normal Operators
365
φi ⩽μi ⩽φn−m+i.
(b) Interlacing theorem: If the kth row and column of a self-adjoint matrix are
removed, the new eigenvalues μi are interlaced with the old ones φi:
φ1 ⩽μ1 ⩽φ2 ⩽· · · ⩽φn−1 ⩽μn−1 ⩽φn
4. Picard’s criterion: Suppose T ♦B(X, Y) is a compact operator on Hilbert spaces
X, Y, having singular values δn and singular vectors vn, un. In solving T x = y,
we ﬁnd that ⇐un, y⊂= δn⇐vn, x⊂for all n. A necessary condition is ⇐un, y⊂/δn ♦
δ2 as well as y ♦(ker T ∞)◦. Thus the coefﬁcients of y must ‘diminish faster’
than δn.
5. Truncated Singular Value Decomposition (TSVD) The series solution
x =
⎛
n
⇐un, y⊂
δn
vn
of T ∞T x = T ∞y need not converge in general. Even if it does, any small errors in
⇐un, y⊂are magniﬁed as δn √0. In practice, the series is truncated at some stage
toavoidthis.Thecutoffpointisbesttakenwhentheerrorin y becomesappreciable
compared to δn. Use the Tikhonov regularization method (Section 10.5) to derive
another way of doing this (for the right choice of α),
x =
⎛
n
|δn|2
|δn|2 + α
⇐un, y⊂
δn
vn.
But any other weighting ⎧
n wn
⇐un,y⊂
δn
vn where wn vanishes sufﬁciently rapidly
as δn √0, is just as valid.
6. It is instructive to compare with the case of solving the equation (T −φ)x = y
where T is compact in B(H) and 0 ̸= φ ♦δ(T ) (the case φ ̸♦δ(T ) is trivial). It
has a solution ≈y ♦ker(T −φ)◦. That solution of minimum norm is then
x =
⎛
n
⇐en, y⊂
φn −φen −y0/φ,
where the sum is taken over φn ̸= φ, 0, and y0 is the projection of y to ker T .
There is no issue of convergence of the series as |φn −φ| ∴c > 0.
7. * If T is a compact normal operator, then the iteration vn+1 := T vn/∗T vn∗
(starting from a generic vector v0) converges to an eigenvector of the largest
eigenvalue, if this is unique and strictly positive. What happens otherwise?

366
15
C∞-Algebras
Ideals of Compact Operators
Another way of looking at the spectral theorem (or even the singular value decom-
position), is the following:
Proposition 15.26
Any compact operator on a separable Hilbert space can be approximated
by a square matrix.
A compact normal operator on a separable complex Hilbert space can be
approximated by a diagonalizable matrix.
Proof An operator T ♦B(H) takes the matrix form, in terms of a countable ortho-
normal basis ei of H,
⎨
⎩⎩
PnT Pn
PnT (I −Pn)
(I −Pn)T Pn
(I −Pn)T (I −Pn)


where Pn is the self-adjoint/orthogonal projection onto [[e1, . . . , en]] (Example
15.14(1)). Note that for any vector x ♦H, Pnx √x as n √⇒(Theorem 10.31).
The claim is that when T is compact, the ﬁnite square matrices PnT Pn converge to
T . This is the same as claiming that the other three sub-matrices vanish as n √⇒.
(I −Pn)T √0: Suppose, for contradiction, that there are unit vectors xn such
that ∗(I −Pn)T xn∗∴c > 0. Since T is compact, there is a convergent subsequence
T xn √x, hence
(I −Pn)T xn = (I −Pn)x + (I −Pn)(T xn −x) √0
leads to an impossibility.
(I −Pn)T Pn √0 and (I −Pn)T (I −Pn) √0 now follow from ∗Pn∗= 1 =
∗I −Pn∗. Finally, T (I −Pn) √0 is also true and follows from (I −Pn)T ∞√0,
since T ∞is also a compact operator (Proposition 11.31).
For a compact normal operator, the orthonormal basis ei can be chosen to consist
of the eigenvectors of T by the Spectral Theorem, in which case PnT Pn is a diagonal
matrix
PnT Pn =
n
⎛
i=1
φieie∞
i .

15.3 The Spectral Theorem for Compact Normal Operators
367
⇔∩
Proposition 15.27
The compact operators of ﬁnite rank acting on a Hilbert space H form
asimple ∞-idealKF(H),whichiscontainedineverynon-zeroidealof B(H).
The closure of KF(H) in B(H) is the ∞-ideal of compact operators K(H).
Proof The facts that the sum of compact operators, the product of a compact operator
with any other operator, and the adjoint of a compact operator, are compact have
already been proved earlier (Propositions 11.9 and 11.31), so K(H) is a ∞-ideal
in B(H).
Similarly, it is not difﬁcult to show that the sum of two ﬁnite-rank operators, and
the product (left or right) of a ﬁnite-rank operator with any other operator, are again
ﬁnite-rank. The details are left to the reader.
Let I be an ideal in B(H) which contains a non-zero operator S. There exist non-
zero vectors a, b such that Sa = b. For any vectors x, y ̸= 0, deﬁne the operator
Exy := xy∞/∗y∗2, so that Exy y = x, but Exyu = 0 whenever u ◦y. The operator
ExbSEay has precisely the same effect
ExbSEay y = ExbSa = Exbb = x,
ExbSEayu = 0 (u ◦y),
so Exy = ExbSEay ♦I. Now let T be any operator on H. If e1, . . . , en are linearly
independent in (ker T )◦, then T e1, . . . , T en remain linearly independent in im T ,
for
T (⎧
i αiei) = ⎧
i αiT ei = 0 ∪
⎛
i
αiei ♦ker T ˆ (ker T )◦= 0
∪αi = 0,
i = 1, . . . , N.
Thus, if T is of ﬁnite-rank then (ker T )◦is ﬁnite-dimensional and has a ﬁnite ortho-
normal basis e1, . . . , eN, say, extended to an orthonormal basis for all of H. Inciden-
tally, this shows that T ∞is also of ﬁnite rank, since im T ∞= (ker T )◦. Given any
vector x = ⎧
n αnen ♦H,
T x = T (
⎛
n
αnen) =
N
⎛
n=1
αnT en =
N
⎛
n=1
αn ET en,enen =
N
⎛
n=1
ET en,en x
so T is a linear combination of operators ET en,en and belongs to I. We have shown
that KF(H) ∈I and KF(H) is closed under adjoints.
In particular KF(H) contains no non-zero ideals; we say it is simple. That the
closureofKF(H)isK(H)isessentiallythecontentofthepreviousproposition:More
precisely, recall that the image of a compact operator is separable, so M := im T

368
15
C∞-Algebras
has a countable basis (ei). Let Pn be the orthogonal projection onto [[e1, . . . , en]].
Then, as in the proof of the previous proposition, the ﬁnite-rank operators PnT
converge to T .
⇔∩
Examples 15.28
1. The ideal of compact operators, being the closure K(H) = KF(H), is contained
in every closed ideal of B(H).
2. The algebra of matrices B(CN) = KF(CN) = K(CN) is simple.
3. ∃The above argument can be extended to show, more generally, that compact
operators on a Banach space with a Schauder basis can be approximated by ﬁnite-
rank operators. Spaces for which this is true are said to have the “approximation
property”; even separable spaces may fail to have this property [41].
Hilbert-Schmidt Operators
Deﬁnition 15.29
The traceof an operator T on a Hilbert space with an orthonormal basis en,
is, when ﬁnite,
tr(T ) :=
⎛
n
⇐en, T en⊂.
A Hilbert-Schmidt operator is one such that tr(T ∞T ) = ⎧
n ∗T en∗2 is ﬁnite.
As deﬁned, the trace of an operator can depend on the choice of orthonormal
basis. But for a Hilbert-Schmidt operator, tr(T ∞T ) is well-deﬁned as the proof of the
next proposition shows:
Proposition 15.30
If the right-hand traces exist,
tr(S + T ) = tr(S) + tr(T ),
tr(φT ) = φ tr(T ),
tr(T ∞) = tr(T ).
If S, T are Hilbert-Schmidt, then tr(ST ) = tr(T S).
Proof The identities tr(S + T ) = tr(S) + tr(T ) and tr(φT ) = φ tr(T ) follow easily
from the linearity of the inner product and summation, while
tr(T ∞) =
⎛
n
⇐en, T ∞en⊂=
⎛
n
⇐T ∞en, en⊂=
⎛
n
⇐en, T en⊂= tr(T ).

15.3 The Spectral Theorem for Compact Normal Operators
369
Let en and ˜em be orthonormal bases for the Hilbert space H; then T en =
⎧
m ⇐˜em, T en⊂˜em and ST en = ⎧
m ⇐˜em, T en⊂S ˜em, so
tr(ST ) =
⎛
n
⇐en, ST en⊂=
⎛
n,m
⇐˜em, T en⊂⇐en, S˜em⊂=
⎛
m
⇐˜em, T S ˜em⊂,
(15.1)
exchanging the order of summation. This would be justiﬁed if the convergence is
absolute, which is the case when S∞and T are Hilbert-Schmidt,
⎛
n,m
|⇐˜em, T en⊂⇐en, S ˜em⊂| ⩽
⎛
n,m
|⇐˜em, T en⊂|2
⎛
n,m
|⇐en, S˜em⊂|2
=
⎛
n
∗T en∗2 ⎛
n
∗S∞en∗2,
(15.2)
applying the Cauchy-Schwarz inequality and Parseval’s identity. So, putting S = T ∞
and ˜en = en in (15.1) shows that tr(T ∞T ) = tr(T T ∞), when T is Hilbert-Schmidt,
i.e., T ∞is also Hilbert-Schmidt. This, in turn, implies that when S and T are Hilbert-
Schmidt, (15.2) and (15.1) are satisﬁed, so tr(T S) = tr(ST ) (in particular tr(T ∞T ))
is independent of the orthonormal basis.
⇔∩
Theorem 15.31
The Hilbert-Schmidt operators of B(H) form a Hilbert space HS , with
inner product
⇐S, T ⊂HS := tr(S∞T ) =
⎛
n
⇐Sen, T en⊂,
which is a ∞-idealof compact operators, and
∗T ∗⩽∗T ∗HS,
∗ST ∗HS ⩽∗S∗∗T ∗HS.
Proof Leten beanorthonormalbasisfor H.Firstnotethat∗T ∗HS := √⇐T, T ⊂HS =
√tr(T ∞T ) is ﬁnite for Hilbert-Schmidt operators.
(i) We have remarked in the preceding proposition that if T ♦HS then T ∞♦HS,
and
∗T ∞∗HS =

tr(T T ∞) =

tr(T ∞T ) = ∗T ∗HS.
The product ⇐S, T ⊂:= tr(S∞T ) is ﬁnite and independent of the choice of ortho-
normal basis when S, T ♦HS, by (15.1) and (15.2). Moreover, both of the following
traces are ﬁnite,

370
15
C∞-Algebras
tr(S + T )∞(S + T ) = tr(S∞S) + tr(S∞T ) + tr(T ∞S) + tr(T ∞T )
tr(φT )∞(φT ) = |φ|2 tr(T ∞T ),
so that HS is a vector space.
Linearity and ‘symmetry’ of the product follow from
⇐S, T1 + T2⊂= tr(S∞T1 + S∞T2) = tr(S∞T1) + tr(S∞T2) = ⇐S, T1⊂+ ⇐S, T2⊂,
⇐S, φT ⊂= tr(S∞φT ) = φ tr(S∞T ) = φ⇐S, T ⊂,
⇐T, S⊂= tr(T ∞S) = tr(S∞T )∞= tr(S∞T ) = ⇐S, T ⊂.
That ∗T ∗⩽∗T ∗HS (and hence ∗T ∗HS = 0 ∪T = 0) follows from
∗T x∗= ∗
⎛
n
⇐en, x⊂T en∗⩽
⎛
n
|⇐en, x⊂|∗T en∗
⩽
⎛
n
|⇐en, x⊂|2
⎛
n
∗T en∗2 = ∗x∗∗T ∗HS.
⇐·, ·⊂is therefore a legitimate inner product on HS.
Finally, HS is an ideal of B(H), since for any S ♦B(H) and T ♦HS,
∗ST ∗2
HS =
⎛
n
∗ST en∗2 ⩽
⎛
n
∗S∗2∗T en∗2 = ∗S∗2∗T ∗2
HS,
and
∗T S∗HS = ∗(T S)∞∗HS ⩽∗S∞∗∗T ∞∗HS = ∗S∗∗T ∗HS.
(ii) Hilbert-Schmidt operators are compact: Given T ♦HS, deﬁne the ﬁnite-rank
operator TN by TNen :=
 T en
if n ⩽N
0
if n > N .
∗T −TN ∗2 ⩽∗T −TN ∗2
HS =
⇒
⎛
n=1
∗(T −TN )en∗2 =
⇒
⎛
n=N+1
∗T en∗2 √0 as N √⇒.
T is thus the limit of ﬁnite-rank operators, making it compact (Proposition 11.9).
(iii) The space HS is complete in the HS-norm (but not necessarily in the operator
norm): let (Tn) be an HS-Cauchy sequence
∗Tn −Tm∗2
HS =
⎛
i
∗(Tn −Tm)ei∗2 √0
as n, m √⇒,
then it is a Cauchy sequence in the operator norm, and thus Tn √T in B(H).
But writing the Cauchy condition in a slightly different way, the sequences xn :=
(∗(Tn −T )ei∗) form a Cauchy sequence in δ2,

15.3 The Spectral Theorem for Compact Normal Operators
371
∗xn −xm∗2
δ2 =
⎛
i
⎪⎪∗(Tn −T )ei∗−∗(Tm −T )ei∗
⎪⎪2 ⩽
⎛
i
∗(Tn −Tm)ei∗2 √0,
as n, m √⇒; so xn converges to some sequence (ai) ♦δ2. Combining Tnei √T ei
with ∗Tnei −T ei∗√ai for all i, each ai must be 0, and
∗Tn −T ∗2
HS =
⎛
i
∗(Tn −T )ei∗2 = ∗xn∗2
δ2 √0
as n √⇒,
so Tn √T in HS, and T ♦HS since ∗T ∗HS ⩽∗T −Tn∗HS + ∗Tn∗HS < ⇒. ⇔∩
Having established a theory of Hilbert-Schmidt operators, we now exhibit an
important speciﬁc example:
Theorem 15.32
If k ♦L2(R2), then the operator on L2(R)
T f (x) :=
⎠
k(x, y) f (y) dy,
is Hilbert-Schmidt with ∗T ∗HS = ∗k∗L2 .
Proof Let en(x) be any orthonormal basis for L2(R). Then any function of x in
L2(R) can be written as a sum of these basis functions. Analogously any function
of two variables x, y in L2(R2) can be written as a sum (convergent in L2(R2))
k(x, y) =
⎛
m,n
αn,men(x)em(y),
by ﬁrst ﬁxing y and expanding in terms of en(x) and then treating the result as a
function of y. Write en ˙ em for the basis functions (x, y) ∀√en(x)em(y). They are
orthonormal, since
⇐en ˙ em, en⊆˙ em⊆⊂=
⎠⎠
en(x)em(y)en⊆(x)em⊆(y) dx dy
= ⇐en⊆, en⊂⇐em⊆, em⊂= αn⊆nαm⊆m.
By Parseval’s identity ∗k∗2
L2 =
⎟⎟
|k(x, y)|2 dx dy = ⎧
m,n |αn,m|2. Clearly,
⇐en, T em⊂=
⎠⎠
en(x)k(x, y)em(y) dx dy = ⇐em ˙ en, k⊂L2(R2) = αm,n,
so

372
15
C∞-Algebras
∗T ∗2
HS =
⎛
n
∗T en∗2 =
⎛
n,m
|⇐en, T em⊂|2 =
⎛
n,m
|αm,n|2 = ∗k∗2
L2.
⇔∩
Examples 15.33
1. ▶For square matrices T = [Ti j], S = [Si j],
tr T = ⎧
i Tii,
∗T ∗HS =
⎬⎧
i, j |Ti j|2,
⇐S, T ⊂HS = ⎧
i j ¯Si jTi j.
2. More generally, for any Hilbert space, and using Parseval’s identity,
∗T ∗2
HS =
⎛
i
∗T ei∗2 =
⎛
i, j
|⇐e j, T ei⊂|2.
3. ▶For a Hilbert-Schmidt normal operator, ∗T ∗HS =
⎬⎧
n |φn|2, where φn are
the eigenvalues of T . In this case it is evident that ∗T ∗HS ∴maxn |φn| = ∗T ∗.
4. Find the eigenvalues and eigenfunctions of the integral operator on L2[0, 1] with
kernel k(x, y) :=
 y(1 −x) 0 ⩽y ⩽x ⩽1
x(1 −y) 0 ⩽x ⩽y ⩽1 .
Solution. The operator is Hilbert-Schmidt since |k(x, y)| ⩽1. The eigenvalue
equation is
x
⎠
0
y(1 −x) f (y) dy +
1
⎠
x
x(1 −y) f (y) dy = φ f (x).
The eigenfunctions can be assumed to be differentiable, essentially because they
are integrals. Differentiating gives
x(1 −x) f (x) −
x⎟
0
y f (y) dy −x(1 −x) f (x) +
1⎟
x
(1 −y) f (y) dy = φ f ⊆(x),
and again,
−x f (x) −(1 −x) f (x) = φ f ⊆⊆(x),
f ⊆⊆(x) + 1
φ f (x) = 0,
f (0) = 0 = f (1).
The solutions of this differential equation are the eigenfunctions fn(x) =
sin(nπx) with eigenvalues φn = 1/(n2π2).
5. A traceless operator in B(CN) has a matrix with a zero diagonal, with respect to
some orthonormal basis.
Proof Let A be an N × N matrix with tr A = 0. The proof is by induction on N.
Since the numerical range of A is convex,
0 = 1
N tr A = 1
N
N
⎛
n=1
φn ♦W(A)

15.3 The Spectral Theorem for Compact Normal Operators
373
whereφn aretheeigenvaluesof A.Sothereisaunitvector u suchthat⇐u, Au⊂= 0.
The matrix restricted to u◦, ˜A := A|u◦, is still traceless
0 = tr A = tr ˜A + ⇐u, Au⊂= tr ˜A.
Therefore, by induction, there is an orthonormal basis e1, . . . , eN−1 of u◦in
which ˜A has zero diagonal, i.e., ⇐ei, Aei⊂= 0. This basis, together with u is the
required basis for the whole N-dimensional space.
6. ∃There is a correspondence between various ideals of compact operators and
the sequence spaces of their singular values (φn):
Finite-rank operators
KF(H)
(φn) ♦c00
Trace-class operators
Tr(H)
(φn) ♦δ1
Hilbert-Schmidt operators
HS(H)
(φn) ♦δ2
Compact operators
K(H)
(φn) ♦c0
Bounded operators
B(H)
(φn) ♦δ⇒
where the set of trace-classoperators has been added to complete the pic-
ture (Exercise 15.49(11)). More generally, the Schatten-von Neumann class of
operators Cp corresponds to (φn) ♦δp. The analogy goes deeper than this:
K(H)∞∼= Tr(H) and Tr(H)∞∼= B(H) (via the functionals T ∀√tr(ST )).
Exercises 15.34
1. (a) ⇐S∞, T ∞⊂HS = ⇐T, S⊂HS,
(b) ⇐RT ∞, S⊂HS = ⇐R, ST ⊂HS = ⇐S∞R, T ⊂HS.
2. The closest number to an n × n matrix T (in the HS-norm) is tr(T )/n. (Hint:
φ −T ◦I.)
3. The map x ∀√Mx, where Mx y := x y, embeds δ2 into HS(δ2) (isometrically).
More generally, if xn ♦H satisfy ⎧
n ∗xn∗2 < ⇒, then T := ⎧
n xne∞
n is
Hilbert-Schmidt with ∗T ∗2
HS = ⎧
n ∗xn∗2.
4. The Volterra operator on L2[0, 1], V f (x) :=
⎟x
0 f is Hilbert-Schmidt (without
any eigenvalues).
5. If k(x, y) = k(x −y) for a real function k(x) ♦L2[0, 1] (Example 8.6(5)), then
T f := k ∞f is Hilbert-Schmidt, with eigenvalues k(n).
6. Find the eigenfunctions and eigenvalues of the HS-compact self-adjoint operators
T f :=
⎟1
0 k(x, y) f (y) dy (on L2[0, 1]), where
(a) k(x, y) := x + y,
(b) k(x, y) :=
1
1 −x ⩽y ⩽1
0
0 ⩽y ⩽1 −x ,
(c) k(x, y) := min(x, y); deduce that ⎧
n
1
(2n+1)4 = π4
96 and ⎧
n
1
n4 = π4
90.

374
15
C∞-Algebras
7. In the original Fredholm theory, it was proved under certain hypotheses that the
equation
f (x) +
b
⎠
a
k(x, y) f (y) dy = g(x)
either has a unique solution, or else the same equation with g = 0 admits a
ﬁnite number of linearly independent solutions. Show this for f, g ♦L2(R),
k ♦L2(R2), using Proposition 14.17.
15.4 Representation Theorems
We return to a general unital C∞-algebra X and recover some of the previous propo-
sitions in this setting. The aim is to widen the functional calculus for normal elements
and to prove that X is embedded in B(H) for some Hilbert space H.
Proposition 15.35
For any ρ ♦S(X), T ♦X ,
ρT ∞= ρT ,

T ∞= T ∞.
Proof If A is self-adjoint and t ♦R, then
ρ(A)
σ(A + it)
it
0
∗A + it∗2 = ∗(A + it)∞(A + it)∗
= ∗A2 + t2∗⩽∗A∗2 + t2
(As a matter of fact, equality holds as the accom-
panying diagram shows.)
Writing ρA =: a + ib, we ﬁnd
|b + t| ⩽|a + ib + it| = |ρ(A + it)| ⩽∗A + it∗⩽

∗A∗2 + t2
∴
(2t + b)b ⩽∗A∗2
for t ♦R
so b = 0 and ρA ♦R. Note that α(A) ∈δ(A) ∈S(A) ≤R. More generally, for
any T = A + i B ♦X, with A, B self-adjoint,
ρT ∞= ρ(A −i B) = ρA −iρB = ρA + iρB = ρT.
In particular, every ξ ♦α is automatically a ∞-morphism, and

T ∞(ξ) = ξT ∞= ξT = T (ξ)∞.
⇔∩

15.4 Representation Theorems
375
Theorem 15.36
The Functional Calculus for Normal Elements
When T is normal, T := C[T, T ∞] is a commutative closed ∞-subalgebra
of X, isometrically ∞-isomorphic to C(δ(T )).
The identity 
f (T ) = f ◦T deﬁnes a normal element f (T ) whenever
f ♦C(δ(T)); then δ( f (T )) = f (δ(T )).
Proof T isacommutativeclosed ∞-subalgebraof X:Since T isnormal, T n(T ∞)m =
(T ∞)mT n (by induction), so it should be obvious that (i) any polynomial in T and
T ∞can be written uniquely in the form ⎧
n,m an,mT n(T ∞)m, (ii) the product (and
addition) of two polynomials in T and T ∞is another polynomial, (iii) this product
commutes, and (iv) the involute of a polynomial p(T, T ∞) remains in T ,
p(T, T ∞)∞=
 ⎛
n,m
an,mT n(T ∞)m⎜∞
=
⎛
n,m
an,m T m(T ∞)n ♦C[T, T ∞].
C[T, T ∞] is thus a commutative ∞-subalgebra. The closure of such a subalgebra in X
remains a commutative ∞-subalgebra (Prove!). Note that T is obviously separable.
The spectrum of
S ♦Y, with respect to a closed ∞-subalgebra Y ∈X, is
δ(S): Clearly, if S (or S −φ) is invertible in Y, it remains so in X. Conversely,
if S is invertible in X, then so are S∞, S∞S and SS∞. But S∞S is self-adjoint, with
a real spectrum (in Y and X), hence S∞S + i/n is invertible in Y. As Y is closed
and (S∞S + i/n)−1 √(S∞S)−1 in X, as n √⇒, we can deduce (S∞S)−1 ♦Y.
Similarly (SS∞)−1 ♦Y, implying S is invertible in Y (Exercise 15.3(4)).
T : αT √δ(T ) is a homeomorphism: (αT is the character space of T .) T is
1–1 since suppose T (ξ1) = T (ξ2) for some ξ1, ξ2 ♦αT , i.e., ξ1T = ξ2T . Then
ξ1T ∞= ξ1T = ξ2T = ξ2T ∞
∴ξ1 p(T, T ∞) = ξ1
 ⎛
n,m
an,mT n(T m)∞⎜
=
⎛
n,m
an,m(ξ2T )n(ξ2T )m = ξ2 p(T, T ∞)
for any polynomial p; ﬁnally, by continuity of ξ1 and ξ2, ξ1S = ξ2S for all S ♦T ,
proving ξ1 = ξ2. That T is onto was proved in Theorem 14.38. It is continuous
because
ξn γ ξ ∪T (ξn) = ξnT √ξT = T (ξ).
So T is a homeomorphism since αT is a compact metric space (Proposition 6.17
and Example 14.35(8)). Hence any z ♦δ(T ) corresponds uniquely to some ξ ♦αT
via z = T (ξ) = ξT .

376
15
C∞-Algebras
The Gelfand transformG : T
√C(αT ) ∼= C(δ(T ))
is an isometric ∞-
isomorphism: Recall that G is a Banach algebra morphism (Theorem 14.37). In
a commutative C∞-algebra such as T , every element S ♦T is normal, so ∗S∗C =
ψ(S) = ∗S∗(Theorem 14.38); furthermore 
S∞= S∞, and the Gelfand transform is
an isometric ∞-embedding.
In fact it is onto: for any polynomial p, p(T, T ∞) is mapped by it to p(z, ¯z) when
regarded as a function on δ(T ). By the Stone-Weierstraß theorem, these polynomials
are dense in C(δ(T )). Hence, since G is isometric, it extends to T √C(αT ).
The continuous function calculus: The correspondence between elements in T and
functions in C(αT ) allows us to extend the analytic function calculus established
earlier. For any continuous function f ♦C(δ(T )), the composition f ◦T : αT √C
corresponds to some (normal) element in T which is denoted by f (T ). By this
deﬁnition, 
f (T ) = f ◦T . The following identities are true because they mirror the
same properties in C(αT ),
( f + g)(T) = f (T ) + g(T ), (φ f )(T ) = φ f (T ), ( f g)(T ) = f (T )g(T ), ¯f (T ) = f (T )∞.
Finally ∗f (T )∗= ∗f ∗C is due to G being an isometry and g ◦f (T ) = g( f (T ))
follows after
δ( f (T )) = im 
f (T ) = im f ◦T = f im T = f (δ(T)).
⇔∩
Examples 15.37
1. To take a simple example, consider a 2 × 2 diagonalizable matrix T with distinct
eigenvalues φi and corresponding orthonormal eigenvectors vi, i = 1, 2. Its char-
acter space αT consists of the two morphisms ξi S := ⇐vi, Svi⊂for S ♦T . The
Gelfand transform takes T to (φ1, φ2); any other matrix f (T ) is simultaneously
‘diagonalized’ to ( f (φ1), f (φ2)).
2. ▶For any elements S1, S2 ♦T ,
δ(S1 + S2) ∈δ(S1) + δ(S2),
δ(S1S2) ∈δ(S1)δ(S2).
Proof As T is commutative, Theorem 14.38 shows that δ(S) = αT S for any
S ♦T . Hence the statements follow from Exercise 14.40(10b)).
3. If S, T are commuting normal elements, and f ♦C(δ(S)), g ♦C(δ(T)), then
f (S)g(T ) = g(T ) f (S).
Proof Take polynomials p and q, in z and z∞, then p(S, S∞)q(T, T ∞) = q(T, T ∞)
p(S, S∞) since they are sums of terms of the form
aSnS∞mT iT ∞j = aT iT j∞SnS∞m
by an application of Fuglede’s theorem. Taking the limit of polynomials converg-
ing to f , g (by the Stone-Weierstrass theorem) gives the required result.

15.4 Representation Theorems
377
4. The self-adjoint elements of T correspond to the real-valued functions f ♦
C(αT ) and form a real Banach algebra, while the unitary elements correspond
to functions with unit absolute value, | f | = 1.
5. Every commutative unital C∞-algebra, is isometrically ∞-isomorphic to C(α),
via the Gelfand map. The algebras C(K), with K a compact metric space, are
therefore typical separable commutative C∞-algebras.
Proposition 15.38
For T normal,
T is unitary ≈δ(T ) ∈eiR,
T is self-adjoint ≈δ(T) ∈R.
Proof (i) The spectrum of a unitary element U must lie in the unit closed ball since
∗U∗= 1. Now, U −φ = U(1 −φU ∞) and ∗φU ∞∗= |φ|∗U ∞∗= |φ|; so |φ| < 1
implies 1 −φU ∞, and thus U −φ, are invertible (Theorem 13.20).
(Equivalently, if φ ♦δ(U) then φ−1 ♦δ(U −1) = δ(U ∞) = δ(U)∞and so both
|φ| and 1/|φ| are less than 1.)
(ii) We have already seen that S(T ) ≤R when T is self-adjoint, and S(T ) includes
δ(T). (Alternatively, eiT is unitary (Example 15.5(11)) and the spectral mapping
theorem gives eiδ(T ) = δ(eiT ) ∈eiR. But |ei(a+ib)| = e−b is 1 only when b = 0,
from which follows that δ(T ) ∈R.)
(iii) For the converses, let T be normal with δ(T ) ≤R. Writing it as A + i B with
A, B commuting self-adjoint, we see that i B = T −A, so
δ(i B) ∈δ(T ) + δ(−A) ≤R,
Example 2 above
yet δ(i B) = iδ(B) ≤iR. Thus δ(B) = { 0 }, B = 0, and T = A is self-adjoint.
(Alternatively, we can work with S: if T is normal and δ(T) is real, then S(T ) ≤
R; for any ρ ♦S, ρ(T −T ∞) = ρT −ρT = 0, hence T −T ∞= 0.)
(iv) If T is normal with δ(T ) ∈eiR, then
δ(T ∞T ) ∈δ(T ∞)δ(T ) = δ(T )∞δ(T) ∈eiR.
As T ∞T is self-adjoint and has a real spectrum, that leaves only ±1 as possible
spectral values. But 1 + T ∞T is invertible, otherwise there is a ξ ♦αT such that
−1 = ξ(T ∞T ) = ξT ∞ξT = |ξT|2,
a contradiction. So δ(T ∞T ) = { 1 }, 1 = T ∞T = T T ∞and T is unitary.
⇔∩

378
15
C∞-Algebras
Exercises 15.39
1. Find an example of an operator T having a real spectrum, without T being
self-adjoint.
2. If J is a ∞-morphism and T is normal, then J( f (T )) = f (J(T )) (ﬁrst prove,
for any polynomial p, J(p(T, T ∞)) = p(J(T ), J(T )∞)).
3. ▶In a C∞-algebra, S(T ) = 0 ∪T = 0 (write T = A+i B). We say that S(X)
separates points of X: if T ̸= S, then there is a ρ ♦S such that ρT ̸= ρS.
4. Suppose a C∞-algebra has two involutions, ∞and ξ (with the same norm).
Show that T ∞= T ξ for all T — the involution is unique. (Hint: ρ(T ∞) = ρT
= ρ(T ξ).)
5. Every normal cyclic element is unitary. In particular, the normal elements of a
ﬁnite subgroup of G(X) are unitary.
6. The Fourier transformF : L2(R) √L2(R) is unitary; in fact it is cyclic
F4 = 1, so that it has four eigenvalues ±1, ±i. Verify that the following are
eigenfunctions: e−πx2, xe−πx2, (4πx2 −1)e−πx2, (4πx3 −3x)e−πx2.
7. A normal T such that ∗T ∗= 1 = ∗T −1∗is unitary.
8. Normal idempotents are self-adjoint. A normal element T with δ(T) ∈{ 0, 1 }
is an idempotent, e.g. when T is normal and T n+1 = T n for some integer n.
9. Suppose M is a closed subspace of a Hilbert space which is invariant under a
group of unitary operators. Show that M◦is also invariant.
10. If Tn are self-adjoint operators and Tn γ T then T is self-adjoint.
Positive Self-Adjoint Elements
For T , S self-adjoint, let T ⩽S be deﬁned to mean δ(S−T) ∈[0, ⇒]. Equivalently,
since S(X)(S −T ) is the closed convex hull of δ(S −T ) (Proposition 15.8),
T ⩽S ≈∀ρ ♦S(X), ρT ⩽ρS.
Proposition 15.40
The self-adjoint elements form an ordered real Banach space, such that
T ⩽S AND R ⩽Q ∪T + R ⩽S + Q,
T ⩽S ∪R∞T R ⩽R∞SR
∀R ♦X.

15.4 Representation Theorems
379
Proof First note that, by the deﬁnition, T ⩽S ≈0 ⩽S −T ≈T −S ⩽0
(≈−S ⩽−T ), so we might as well consider A := S−T ∴0 and B := Q −R ∴0
in proving some of the assertions.
(i) It is trivially true that self-adjoint elements form a real vector subspace
(S + T )∞= S∞+ T ∞= S + T,
(φT )∞= ¯φT ∞= φT,
∀φ ♦R.
If Tn √T with T ∞
n = Tn, then in the limit, T ∞= T , so the subspace is closed.
(ii) That T ⩽T is immediate from δ(0) = { 0 }. For anti-symmetry, note that
0 ⩽A ⩽0 ∪δ(A) = { 0 } ∪∗A∗= ψ(A) = 0 ∪A = 0,
so
S ⩽T ⩽S ∪T = S.
(iii) To facilitate the rest of the proof, we demonstrate
a ⩽T ⩽b ≈δ(T ) ∈[a, b]
(15.3)
in two parts,
a ⩽T ≈δ(T ) −a = δ(T −a) ∈[0, ⇒] ≈δ(T) ∈[a, ⇒]
T ⩽b ≈δ(T ) −b = δ(T −b) ∈]−⇒, 0] ≈δ(T) ∈]−⇒, b].
In particular, note that T ⩽ψ(T ) = ∗T ∗and that if 0 ⩽T ⩽b then ψ(T ) ⩽b.
(iv) A, B ∴0 ∪A + B ∴0: In general,
C + D ⩽∗C + D∗⩽∗C∗+ ∗D∗= ψ(C) + ψ(D).
Let a := ψ(A), then 0 ⩽A ⩽a can be rewritten as 0 ⩽a −A ⩽a and hence
ψ(a −A) ⩽a. Similarly ψ(b −B) ⩽b := ψ(B), so (a −A) + (b −B) ⩽a + b, or
equivalently, A + B ∴0.
(v) A special case of this shows transitivity of the order relation,
T ⩽S ⩽R ∪0 ⩽(R −S) + (S −T ) = R −T ∪T ⩽R
(vi) We are not at this stage able to prove the full product-inequality rule as claimed
in the proposition. The proof is deferred to the next proposition. Here we show
only the simple case when R is scalar, i.e., if φ ∴0 and A = S −T ∴0, then
δ(φA) = φδ(A) ∈R+.
⇔∩
The continuous functional calculus allows us to extend the domain of all contin-
uous real functions f : R √R to the set of self-adjoint elements. Two functions in
particular stand out:

380
15
C∞-Algebras
(i) the positive square root
√
A when A ∴0, satisfying (
√
A)2 = A =
√
A2, and
(ii) A+ for all A self-adjoint, from the function x+ :=
 x when x ∴0
0 when x < 0 ; similarly
A−from x−:= (−x)+. Their sum then gives |A|, which corresponds to the
function x ∀√|x|.
Examples 15.41
1. (a) If −T ⩽S ⩽T then ∗S∗⩽∗T ∗.
(b) If 0 < a ⩽T ⩽b then T is invertible and b−1 ⩽T −1 ⩽a−1.
(c) If ST ∴0 then T S ∴0.
(d) If S, T ∴0 and ST is self-adjoint, then ST ∴0. In particular, T ∴0 ∪
T n ∴0.
(e) If Sn ⩽Tn and Sn √S, Tn √T , then S ⩽T .
Proof (a) −∗T ∗⩽S ⩽∗T ∗, so δ(S) ∈[−∗T ∗, ∗T ∗] and ∗S∗= ψ(S) ⩽∗T ∗.
(b) δ(T) ∈[a, b] does not include 0; δ(T −1) = δ(T)−1 ∈[b−1, a−1].
(c) δ(T S) is the same as δ(ST ) except possibly for the inclusion or exclusion
of 0. In any case δ(ST ) ∈R+ ≈δ(T S) ∈R+.
(d) Recall that ST is self-adjoint exactly when ST
= T S. So, by Exer-
cise 14.40(17)), δ(ST ) ∈δ(S)δ(T ) ∈R+.
(e) Let An := Tn −Sn ∴0 and An √A := T −S. Then 0 ⩽ρAn √ρA for
any ρ ♦S, so S(A) ∈[0, ⇒].
2. The set of positive elements is a closed convex ‘cone’ (meaning T ∴0 and
φ ∴0
∪
φT ∴0), with non-empty interior in the real Banach space of
self-adjoints.
Proof The only non-trivial statement is that the cone contains an open set of
self-adjoints, namely the unit ball around 1: If A is self-adjoint and ∗A∗< 1
then −1 ⩽A ⩽1, so 1 + A ∴0.
3. Positive continuous functions f : R √R+ give positive elements f (A) ∴0
for A self-adjoint. For example, A+, A−, |A|, and A2 are all positive. More
generally, for any normal operator T and f ♦C(C, R+), f (T ) ∴0.
Proof By the functional calculus, δ( f (T )) = f δ(T) ∈[0, ⇒].
4. Every self-adjoint element decomposes into two positive elements
(a) A = A+ −A−, |A| = A+ + A−,
(b) A+A−= 0, A±|A| = A2
±, A±A = ±A2
±, and A+, A−, A and |A| all
commute with each other,
(c) −A−⩽A ⩽A+ ⩽|A| ⩽∗A∗.

15.4 Representation Theorems
381
Proof The identities x = x+ −x−, |x| = x+ + x−, x+x−= 0, x±|x| = x2
±,
x±x = ±x2
± imply (a) and (b). Moreover, A + A−= A+ ∴0, |A| −A+ =
A+ −A = A−∴0. Finally, δ(|A|) = { |φ| : φ ♦δ(A) } is bounded above by
ψ(A) = ∗A∗.
5. By the spectral mapping theorem, the spectral values of
√
A are the positive
square roots of those of A ∴0. Overall there may be an inﬁnite number of
square roots of A, e.g. for any z ♦C,
⎝
z
1 + z
1 −z
−z
⎞2
=
⎝1 0
0 1
⎞
.
6. 0 ⩽S ⩽T ∪
√
S ⩽
√
T .
Proof If T is invertible, then T −1
2 ST −1
2 ⩽1 (Proposition 15.40), so ∗S
1
2 T −1
2 ∗
2
= ∗T −1
2 ST −1
2 ∗⩽1, from which follows T −1
4 S
1
2 T −1
4 ⩽1 and S
1
2 ⩽T
1
2 .
Proposition 15.42
For any T ♦X and ρ ♦S(X),
(i) T ∞T ∴0,
(ii) T ∴0 ≈T = R∞R, for some R ♦X ,
(iii) ⇐S, T ⊂:= ρ(S∞T ) gives a semi-inner product,
(iv) |ρ(S∞T )|2 ⩽ρ(S∞S)ρ(T ∞T ), |ρT |2 ⩽ρ(T ∞T ),
(v) |ρ(S∞T S)| ⩽ρ(S∞S)∗T ∗.
Proof (i) T ∞T is certainly self-adjoint, and can be decomposed as T ∞T = A −B
where A, B ∴0, AB = B A = 0 (Example 4b above). Now
(T B)∞(T B) = BT ∞T B = B(A −B)B = −B3 ⩽0
and hence (T B)(T B)∞⩽0 (Examples 15.41(1c)). Writing T B = C + i D, with
C, D self-adjoint, we ﬁnd
0 ⩽2(C2 + D2) = (T B)∞(T B) + (T B)(T B)∞⩽0
∴
0 ⩽C2 = −D2 ⩽0
∴
C = 0 = D
so T B = 0. But then, 0 = (T B)∞(T B) = −B3 forces B = 0 and T ∞T =
A ∴0.
This allows us to conclude the proof of Proposition 15.40(vi). If T ⩽S let
A := S −T ∴0, so for any R ♦X, R∞AR = (
√
AR)∞(
√
AR) ∴0, i.e.,
R∞T R ⩽R∞SR.
(ii) Conversely, if T is positive, let R :=
√
T ∴0, so R∞R = R2 = T .

382
15
C∞-Algebras
(iii) The product satisﬁes the following inner-product axioms,
⇐S, φT1 + μT2⊂= ρ(φST1 + μST2) = φ⇐S, T1⊂+ μ⇐S, T2⊂,
⇐T, S⊂= ρ(T ∞S) = ρ(S∞T )∞= ⇐S, T ⊂,
⇐T, T ⊂= ρ(T ∞T ) ∴0
since T ∞T ∴0.
However, it need not be deﬁnite, i.e., ρ(T ∞T ) = 0 may be possible without T = 0.
(iv) This is the Cauchy-Schwarz inequality, which is valid even for semi-deﬁnite
inner products (Example 10.10(17)). In particular, taking S = 1 gives the second
inequality.
(v) As ρ preserves inequalities,
T ∞T ⩽∗T ∞T ∗= ∗T ∗2 ∪S∞T ∞T S ⩽∗T ∗2S∞S
∪ρ(S∞T ∞T S) ⩽ρ(S∞S)∗T ∗2.
∴
|ρ(S∞(T S))|2 ⩽ρ(S∞S)ρ(S∞T ∞T S) by (iv),
⩽ρ(S∞S)2∗T ∗2
⇔∩
Proposition 15.43
If J : X √Y is an algebraic ∞-morphism between C∞-algebras, then it is
continuous with ∗J∗= 1, and preserves ⩽.
If J is also 1–1, then it is isometric.
By an algebraic ∞-morphism is meant a map which preserves +, ·, 1, and ∞.
Proof If A ∴0, then A = R∞R and J(A) = J(R)∞J(R) ∴0. Thus J preserves the
order of self-adjoint elements,
S ⩽T ∪J(T −S) ∴0 ∪J(S) ⩽J(T ).
Now for any T (noting that J(1) = 1),
0 ⩽T ∞T ⩽∗T ∗2,
∴
0 ⩽J(T ∞T ) ⩽∗T ∗2,
∴∗J(T )∗= ∗J(T )∞J(T )∗
1
2 = ∗J(T ∞T )∗
1
2 ⩽∗T ∗.
If J is 1–1, then one can form the ‘inverse’ J −1 : im J √X. It is automatically
an algebraic ∞-morphism (check!), for example, for any S ♦im J,
J −1(S∞) = J −1(JT )∞= J −1J(T ∞) = T ∞= (J −1(JT ))∞= (J −1S)∞,
and so ∗J −1(S)∗⩽∗S∗. Thus ∗T ∗⩽∗J(T )∗⩽∗T ∗as required.

15.4 Representation Theorems
383
(Alternatively, deﬁning |||T||| := ∗J(T )∗Y gives a C∞-norm on X. But there
can only be one C∞-norm (Exercise 15.10(6)), so J is an isometry and im J
is closed.)
⇔∩
Exercises 15.44
1. 0 ⩽1 (as self-adjoint elements), and the order relation of R is subsumed in that
of the self-adjoint elements. Similarly, in C[0, 1], f ⩽g ≈∀x, f (x) ⩽g(x).
2.
⎝
0
1 −i
1 + i
−1
⎞
⩽
⎝1 1
1 0
⎞
in B(C2). Note that T ⩽S does not mean “δ(T ) ⩽
δ(S)” in general.
3. (a) A diagonal matrix is positive when all its diagonal coefﬁcients are real and
positive.
(b) If the coefﬁcients of a real symmetric matrix are positive, it does not follow
that it is positive: ∀i, j, Ai j ∴0 ̸∪A ∴0.
(c) But if a real symmetric matrix is dominated by its positive diagonal, meaning
Aii ∴⎧
j̸=i |Ai j|, then A ∴0 (Gershgorin’s theorem (Examples 14.10(6)).
4. Show Re(T ) ∴0 ≈Re S(T ) ∴0.
5. The similarity between self-adjoints and real numbers is striking. But not every
property about inequalities of real numbers carries through to self-adjoints:
(a) Not every two self-adjoints S and T are comparable, e.g. T :=
⎝1 0
0 −1
⎞
satisﬁes neither T ⩽0 nor T ∴0;
(b) 0 ⩽S ⩽T does not imply S2 ⩽T 2 (unless S, T commute), e.g. S :=
⎝2 1
1 1
⎞
, T :=
⎝3 1
1 1
⎞
.
6. In B(H), S ⩽T ≈⇐x, Sx⊂⩽⇐x, T x⊂for all x ♦H. In particular, S∞S ⩽
T ∞T ≈∗Sx∗⩽∗T x∗for all x ♦H (e.g. T ∞T ∴0); deduce
(a) If T is compact then so is S,
(b) If T is Hilbert-Schmidt, then so is S,
(c) For self-adjoint projections in B(H), P ⩽Q when im P ∈im Q.
7. Prove S ⩽T ∪R∞SR ⩽R∞T R for all R, in B(H).
8. In B(H), if T ∴0 then ⇐⇐x, y⊂⊂:= ⇐x, T y⊂is “almost” an inner product on H,
except that it need not be deﬁnite; it still satisﬁes the Cauchy-Schwarz inequality
though,
|⇐x, T y⊂|2 ⩽⇐x, T x⊂⇐y, T y⊂.
Conversely, every bounded inner product ⇐⇐, ⊂⊂on H, in the sense that |⇐⇐x, y⊂⊂| ⩽
c∗x∗∗y∗, is of this type. Use Example 11.21(1c) to deduce that, for all x ♦H,

384
15
C∞-Algebras
∗T x∗⩽

∗T ∗

⇐x, T x⊂.
In particular, ⇐x, T x⊂= 0 ≈T x = 0.
9. If f : R √R is increasing and a ⩽T ⩽b then f (a) ⩽f (T ) ⩽f (b).
10. To calculate f (A) for a positive self-adjoint matrix A, ﬁrst diagonalize it
A = P DP−1, then work out f (A) = P f (D)P−1. For example,
⎝0 1
1 0
⎞
±
= 1
2
⎝1 ±1
±1 1
⎞
,
⎝5 4
4 5
⎞
=
⎝2 1
1 2
⎞
.
11. There exists Aα ∴0 for α > 0 when A ∴0, for which (Aα)1/α = A.
12. If −1 ⩽A ⩽1 then A + i
√
1 −A2 is unitary. Hence any T ♦X is the linear
combination of at most four unitary elements. (Hint: A = (U + U∞)/2.)
13. Solvetheequation T AT = B fortheunknown T ∴0,given A, B ∴0 invertible
(Hint: A
1
2 T AT A
1
2 = (A
1
2 T A
1
2 )2).
14. Consider ρ ♦X ∞which preserves inequalities, 0 ⩽A ∪0 ⩽ρA; it satisﬁes
Proposition 15.42 except that |ρT |2 ⩽ρ1ρ(T ∞T ) ⩽(ρ1)2∗T ∗2. Such positive
functionals, as they are called, are positive multiples of states.
15. If J : X √Y is an algebraic ∞-morphism, then
X/ ker J ∼= im J ≈im J is closed.
Polar Decomposition An important application of the use of square roots of positive
self-adjoint elements is the following generalization of the polar decomposition of
complex numbers to B(H):
Proposition 15.45
Polar Decomposition
Every operator T ♦B(H) has a decomposition T = U|T | , in which
|T | :=
√
T ∞T ∴0 and U : im |T | √im T is an isometry.
Proof T ∞T is positive, so its square root R :=
√
T ∞T ∴0 can be deﬁned. R reduces
to the previous deﬁnition of |T | when T is normal, so it is common to write |T | for
R. Then ∗|T |x∗= ∗T x∗for all x ♦H, as
⇐|T |x, |T |y⊂= ⇐x, |T |2y⊂= ⇐x, T ∞T y⊂= ⇐T x, T y⊂.
(15.4)
Let U: im |T | √im T be deﬁned by U(|T |x) := T x; it is well-deﬁned by (15.4),

15.4 Representation Theorems
385
|T |(x −y) = 0 ≈T (x −y) = 0,
and isometric, so can be extended isometrically to im |T | √im T (Examples 8.9(4)).
It can be extended further to the whole of the Hilbert space H by letting Ux = 0
whenever x belongs to the orthogonal space ker |T |, in which case it is called a
partial isometry.
⇔∩
Furthermore, when T is normal and U is extended to a partial isometry, T = |T |U
is also true: ker |T| = ker T by (15.4) and since ker T ∞= ker T (Proposition 15.12),
im |T | = (ker |T |)◦= (ker T )◦= (ker T ∞)◦= im T .
In fact,
for x ♦ker |T |,
|T |Ux = 0 = T x,
for x = |T |y ♦im |T |, |T |Ux = |T |U|T |y = |T |T y = T |T|y = T x,
and by extension |T|Ux = T x for x ♦im |T | as well.
On the other hand, if T is invertible, then it implies, in succession, that T ∞, T ∞T ,
and |T | are invertible; thus U is an onto isometry on H, hence unitary.
Proposition 15.46
Every unitary operator in B(H) is of the type eiT with T ♦B(H) self-
adjoint.
The group of invertible operators G(H) ∈B(H) is connected and gener-
ated by the exponentials.
Proof (i) The polar decomposition of any self-adjoint operator B ♦B(H) is B =
V |B| where
V x: =
 x
x ♦ker B−
−x x ♦(ker B−)◦= im B−
since B+x ♦ker B−(B−B+ = 0). Note that V 2 = I. Hence
V |B|x = V B+x + V B−x = B+x −B−x = Bx.
Let U be any unitary operator on H. It equals U = A + i B where A, B are
commuting self-adjoint operators such that A2 + B2 = I. It follows that A com-
mutes with B−(Example 15.37(3)) and thus preserves ker B−and im B−(Exercise
8.10(19)). Accordingly, if B = V |B| is the polar decomposition of B, as above, then
V commutes with A: for all x = a + b ♦ker B−→im B−,

386
15
C∞-Algebras
V Ax = V A(a + b) = Aa −Ab = A(a −b) = AV x.
The function arccos : [−1, 1] √[0, π] is a continuous function, and −1 ⩽
A ⩽1, so we can deﬁne C := arccos A ♦B(H), and this commutes with V . Let
T := VC, so that T 2 = V 2C2 = C2. Hence,
eiT = (I −T 2
2! + · · · ) + iT (I −T 2
3! + · · · )
= (I −C2
2! + · · · ) + iV (C −C3
3! + · · · )
= cos C + iV sin C
= A + iV |B|
(sin ◦arccos(A) =

1 −A2 = |B|)
= U.
(ii) Consider the polar decomposition of an invertible operator T = U|T |, where U
is unitary and |T| is invertible. By the above, U = ei A, while |T | has a logarithm,
|T| = ei B (Exercise 14.26(1)). Hence T = ei AeB lies in the connected component
of I (Proposition 13.24), which must therefore equal G(H).
⇔∩
Spectral Theorem for Normal Operators
There is one further extension of the functional calculus of the C∞-algebra B(H):
when T is a normal operator, f (T ) may be deﬁned even for bounded measurable
functions.
Let 1 be the characteristic function deﬁned on a bounded open subset  ∈C.
To ﬁnd an operator that corresponds to 1, we will be needing the following lemma:
Monotone Convergence Theorem for Self-Adjoint Operators: If
An ∴0 is a
decreasingsequenceofcommutingself-adjointoperatorsin B(H)then An converges
strongly to some operator A ∴0.
Proof It is easy to show that when 0 ⩽S ⩽T commute,
S2 ⩽S2 + (T −S)2 = T 2 −2S(T −S) ⩽T 2.
From this it follows that A2
n is also a decreasing sequence, as is ∗Anx∗by Example 6
above. Also ∗Anx −Amx∗2 ⩽
⎪⎪∗Amx∗2 −∗Anx∗2⎪⎪√0 as n, m √⇒, since
An Am ∴A2
n for n ∴m, so (Anx) is a Cauchy sequence in H. Now apply the
corollary of the uniform bounded theorem (Corollary 11.35).
⇔∩
It follows easily from this that an increasing sequence of bounded self-adjoint
operators An ⩽c converges strongly to some operator A ⩽c.
There exist increasing sequences of positive continuous functions fn : C √R+
which converge pointwise to 1; for example, take fn(z) := min(1, n d(z, c)).

15.4 Representation Theorems
387
Using the continuous functional calculus deﬁned in Theorem 15.36, fn(T) exist as
positive self-adjoint operators on H with norm equal to ∗fn(T )∗= ∗fn∗C = 1.
Wecanthereforedeﬁne1(T )x := limn√⇒fn(T )x forall x ♦H.Foranyclosed
subset F of C, there are nested open sets Un such that F = 
n Un. So 1F(T) can
be deﬁned by 1F(T )x := limn√⇒1Un(T )x by the monotone convergence theorem
above. Some properties of 1(T ) are:
1. 1(T ) is an orthogonal projection; so 1(T ) ∴0.
Proof Write An := fn(T ) and A := 1(T ). Then
⇐Ay, x⊂= lim
n√⇒⇐Any, x⊂= lim
n√⇒⇐y, Anx⊂= ⇐y, Ax⊂,
∗(A2
n −A2)x∗= ∗(An + A)(An −A)x∗⩽(1 + ∗A∗)∗(An −A)x∗√0.
Thus 1(T )2 = 1(T ) is self-adjoint, and hence othogonal (Example 15.14(1)).
2. (a) If U,V are disjoint open sets, then 1U(T ) + 1V (T ) = 1U∪V (T ),
(b) 1UˆV (T ) = 1U(T )1V (T ).
Proof If fn(z) √1U(z) and gn(z) √1V (z) for z ♦C then fn(z)+gn(z) √
1U(z) + 1V b(z) = 1U∪V (z). So by the continuous functional calculus and
the strong convergence of fn and gn, it follows that fn(T )x + gn(T )x √
1U∪V (T )x for any x ♦H.
Similarly, the second statement results from fn(z)gn(z) √1U(z)1V (z) =
1UˆV (z).
3. 1∅(T ) = 0, 1δ(T )(T ) = I (since if δ(T ) ∈U and fn √1U, then fn|δ(T ) = 1
for n large enough).
The projections 1E(T ) for Borel sets E are deﬁned by the same procedure and
are said to be the spectral measure associated with T . We gloss over the details of
the exact deﬁnition (see [10]).
One can now follow the same steps of creating the space of step functions through
to L1(C), but starting from the projections 1E(T ) as ‘step functions’. The end
result is a functional calculus in which f (T ) is deﬁned for any complex-valued
f ♦L⇒(δ(T)): if f is approximated by ⎧
i ai1Ui , then f (T ) is approximately
⎧
i ai1Ui (T ). Indeed, f (T ) is still meaningful even if f ♦L1(δ(T)) but need not
be a “bounded” (i.e., continuous) operator.
Proposition 15.47
von Neumann’s Spectral Theorem
For any normal operator T and f ♦L⇒(δ(T )), there is a spectral measure
Eφ such that
f (T ) =
⎠
δ(T )
f (φ) dEφ

388
15
C∞-Algebras
Proof For any x, y ♦H, deﬁne μx,y(U) := ⇐x, 1U(T )y⊂for any open bounded
subset U ∈C. By the properties proved above, μx,y can be extended to a measure
with support equal to δ(T ). (It is not a Lebesgue measure on C as it is not translation
invariant, but Borel sets are μx,y-measurable.) It has the additional properties:
μx,y1+y2 = μx,y1 + μx,y2,
μx,φy = φμx,y,
μy,x = μx,y,
0 ⩽μx,x ⩽∗x∗2.
It follows that for any f
♦L⇒(δ(T )), ⇐⇐x, y⊂⊂:=
⎟
δ(T ) f dμx,y is a semi-
inner-product which is bounded in the sense |⇐⇐x, y⊂⊂| ⩽∗f ∗L⇒∗x∗∗y∗. Thus, by
Exercise 15.44(8), ⇐⇐x, y⊂⊂= ⇐x, Sy⊂for some continuous operator S which we
henceforth call f (T ),
⇐x, f (T )y⊂=
⎠
δ(T )
f dμx,y.
f (T ) agrees withtheearlier deﬁnitionfor f ♦C(δ(T )): Any such f is uniformly
continuous, so for α small enough f Bα(z) ∈Bκ( f (z)), independently of z ♦δ(T ).
Let Bi be squares, with centers φi and diameter less than α, which partition δ(T); one
can ﬁnd slightly smaller closed squares Ai ≤Bi and slightly larger open squares
Ci ⊃Bi, such that ⎧
i μx,y(Ci \ Ai) < κ. Moreover, one can ﬁnd continuous
functions hi such that 1Ai ⩽hi ⩽1Ci and ⎧
i hi = 1; for example, let hi(s, t) :=
h(s)h(t) where h(t) = min(1,r d(t, I c)) is a continuous real function with support
equal to I and taking the value 1 just inside it. Then (writing μ = μx,y)
⇐x, f (T )y⊂=
⎛
i
⇐x, f hi(T )y⊂∇f (φi)⇐x, hi(T )y⊂∇
⎛
i
f (φi)μ(Bi).
More rigorously, (it is enough to consider real-valued functions)
⇐x, f hi(T )y⊂⩽( f (φi) + κ)μ(Ci)
= ( f (φi) + κ)μ(Bi) + ( f (φi) + κ)(μ(Ci) −μ(Bi))
−⇐x, f hi(T )y⊂⩽−f (φi)μ(Bi) + κμ(Bi) + ( f (φi) −κ)(μ(Bi) −μ(Ai))
∴|⇐x, f hi(T )y⊂−f (φi)μ(Bi)| ⩽κμ(Bi) + | f (φi) + κ|(μ(Ci) −μ(Ai))
∴|⇐x, f (T )y⊂−
⎛
i
f (φi)μ(Bi)| =
⎪⎪⎛
i
⇐x, f hi(T )y⊂−
⎛
i
f (φi)μ(Bi)
⎪⎪
⩽
⎛
i
|⇐x, f hi(T )y⊂−f (φi)μ(Bi)|
⩽
⎛
i
(| f (φi)| + κ)(μ(Ci) −μ(Ai)) + κμ(Bi)
⩽(∗f ∗C + κ)κ + κ
Hence, in the limit κ √0, ⇐x, f (T )y⊂=
⎟
δ(T ) f dμx,y.

15.4 Representation Theorems
389
The map
f ∀√f (T ) is a ∞-morphism from L⇒(δ(T)) to B(H): Linearity is
immediate, ( f + φg)(T ) =
⎟
δ(T )( f + φg) dμx,y = f (T) + φg(T).
¯f (T ) = f (T )∞since
⇐x, ¯f (T )y⊂=
⎠
¯f dμx,y =
⎠
f dμx,y = ⇐y, f (T)x⊂= ⇐f (T )x, y⊂= ⇐x, f (T )∞y⊂.
f g(T ) = f (T )g(T ) follows from
⎠
δ(T )
dμx, f (T )y = ⇐x, f (T )y⊂=
⎠
δ(T )
f dμx,y
∪
⇐x, f g(T)y⊂=
⎠
f g dμx,y =
⎠
f dμx,g(T )y = ⇐x, f (T )g(T)y⊂.
⇔∩
In particular, T =
⎟
δ(T ) φ dEφ. This result, and the next one, are often claimed to
be the pinnacle of the subject of functional analysis.
Embedding in B(H)
Theorem 15.48
Gelfand-Naimark
Every C∞-algebra is embedded in B(H), for some Hilbert space H .
Proof We have already seen that every Banach algebra X is embedded in B(X)
(Theorem 13.8); as in the proof of that theorem, we will again denote elements of
X by lower-case letters. The main difﬁculty is that there is no natural inner product
deﬁned on X or B(X). Rather there are many semi-inner-products, one for each
ρ ♦S, ⇐x, y⊂ρ := ρ(x∞y).
Let Mρ := { x : ρ(x∞x) = 0 }; it is a closed left-ideal, since for any a ♦X and
x ♦M, then ax ♦Mρ
0 ⩽ρ(x∞a∞ax) ⩽ρ(x∞x)∗a∗2 = 0.
This allows us to turn X/Mρ into an inner product space, which can be completed
to a Hilbert space Hρ (Examples 10.7(2)) and 13.5(6). The inner product on X/Mρ
is given by
⇐x + Mρ, y + Mρ⊂:= ρ(x∞y).
The ∞-morphism L : X √B(Hρ): For any a ♦X, consider the linear map
deﬁned by La(x +Mρ) := ax +Mρ on X/Mρ; this is well-deﬁned since aMρ ∈
Mρ. It is continuous with ∗La∗⩽∗a∗since,

390
15
C∞-Algebras
John von Neumann (1903–1957) Originally from Budapest, he
studied in Berlin, under Weyl and Polya, but graduated at 23
years under Fej´er in Budapest with a thesis on ordinal num-
bers. A young party-going genius, in 1926-30 he deﬁned Hilbert
spaces axiomatically as foundation for the brand new quantum
mechanics and generalized the spectral theorem to unbounded
self-adjoint operators. In the 1930s he went to the Princeton
Institute, proved the ergodic theorem, and studied rings of op-
erators and group representations; only turbulent ﬂuid dynam-
ics proved too hard (it remains unsolved today); in 1944 he
started game theory, proving the mini-max theorem, then on to
computers and automata theory.
Fig. 15.1 von Neumann
∗La(x + Mρ)∗= ∗ax + Mρ∗=

ρ(x∞a∞ax) ⩽

ρ(x∞x)∗a∗= ∗a∗∗x + Mρ∗.
This map extends uniquely to one in B(Hρ) (Example 8.9(4)).
Clearly La is linear in a, Lab = LaLb, and L1 = I, but it also preserves the
involution La∞= L∞
a,
⇐x + Mρ, La(y + Mρ)⊂= ρ(x∞ay) = ρ((a∞x)∞y) = ⇐La∞x + Mρ, y + Mρ⊂.
It remains a ∞-morphism when extended to B(Hρ), by continuity of the adjoint.
The ﬁnal Hilbert space: However L need not be 1–1. To remedy this deﬁciency,
let H := 
ρ♦S Hρ be the Hilbert space of “sequences” x := (xρ)ρ♦S such that
xρ ♦Hρ and ⎧
ρ♦S ⇐xρ, xρ⊂Hρ < ⇒; it has the inner product
⇐x, y⊂:=
⎛
ρ♦S
⇐xρ, yρ⊂Hρ.
It is straightforward to show that H is indeed a Hilbert space, by analogy with δ2.
Let Jax := (Laxρ)ρ♦S, so that Ja : H √H is obviously linear, and also
continuous since
∗Jax∗2 =
⎛
ρ
∗Laxρ∗2 ⩽∗a∗2 ⎛
ρ
∗xρ∗2 = ∗a∗2∗x∗2.
The mapping a ∀√Ja, X ∀√B(H) is an algebraic ∞-morphism,
⇐y, Jax⊂=
⎛
ρ
⇐yρ, Laxρ⊂=
⎛
ρ
⇐L∞
a yρ, xρ⊂= ⇐Ja∞y, x⊂.
Moreover it is 1–1, for if Ja = 0 then Laxρ = 0 for any xρ and ρ ♦S, in particular
a + Mρ = La1 = 0. But this means that for all ρ ♦S, a ♦Mρ, i.e., ρ(a∞a) = 0,

15.4 Representation Theorems
391
and this can only hold when δ(a∞a) ∈S(a∞a) = 0, so ∗a∗2 = ∗a∞a∗= 0 and
a = 0.
Since every such ∞-morphism between C∞-algebras is isometric, the theorem is
proved.
⇔∩
Exercises 15.49
1. Examples of polar decompositions are
⎝1 0
0 −2
⎞
=
⎝1 0
0 −1
⎞⎝1 0
0 2
⎞
, and
⎝1 1
0 1
⎞
=
⎝0.89 0.45
−0.45 0.89
⎞⎝0.89 0.45
0.45 1.34
⎞
.
2. If T is a compact operator in B(H) with singular values φn and singular vectors
en, e⊆
n, then |T|en = φnen and U : en ∀√e⊆
n.
3. The polar decomposition of the right-shift operator in δ2 is trivial: |R| = I.
What is it for the left-shift operator?
4. T ∞= |T|U ∞, |T | = U ∞T = T ∞U, and |T ∞| = UT ∞= TU ∞, since U ∞U is a
projection onto im |T | and UU∞is a projection onto im T . ∗|T |∗= ∗T ∗.
5. (a) T is normal ≈|T ∞| = |T |,
(b) T is positive self-adjoint ≈T = |T |,
(c) T is unitary ≈|T | = I and T is invertible.
6. If |S| = |T | and T is invertible then ST −1 is unitary.
7. When T is compact normal, with polar decomposition T = |T|U = U|T|, then
U and |T | are simultaneously diagonalizable, U = P−1eiπP, |T | = P−1DP,
so that T = P−1DeiπP.
8. Adapt the proof of the Polar Decomposition theorem to show that if T ∞T ⩽S∞S
then the map U : im S √im T , Sx ∀√T x, is a well-deﬁned operator with
∗U∗⩽1 and T = U S.
9. Every ideal in B(H) is a ∞-ideal since
T ♦I ∪|T | = U ∞T ♦I ∪T ∞= |T |U ∞♦I.
10. Every invertible element T of a C∞-algebra can be written uniquely as T = U|T |
where U is unitary.
11. Trace-class Operators: Let Tr := { T ♦B(H) : tr |T | < ⇒} with norm
∗T ∗Tr := tr |T | (Proposition 15.30 and Examples 15.33(6)).
(a) ∗T ∗Tr = ∗|T |
1
2 ∗
2
HS, and T ♦Tr ≈|T |
1
2 ♦HS,
(b) tr(T ) is independent of the orthonormal basis,
(c) | tr(ST )| ⩽∗S∗∗T ∗Tr; in particular ∗T ∗HS ⩽∗T ∗Tr,

392
15
C∞-Algebras
(d) Tr is a closed ∞-ideal in B(H),
(e) T ♦Tr ≈T = AB where A, B ♦HS,
(f) tr |T| = ⎧
n |φn|, where (φn) are the singular values of T (repeated accord-
ing to their multiplicities). tr T = ⎧
n φn holds when T is normal and φn
are its eigenvalues.
12. GNS construction: When X is represented in B(H), every state ρ ♦S(X) is
associated with a unit vector x ♦H, such that ρy = ⇐x, Jyx⊂.
Proof The vector in question is x := (xξ)ξ♦S where xρ = 1 + Mρ and xξ = 0
otherwise. For every y ♦X,
ρ(y) = ⇐1 + Mρ, y + Mρ⊂Hρ = ⇐xρ, L yxρ⊂Hρ =
⎛
ξ
⇐xξ, L yxξ⊂Hξ = ⇐x, Jyx⊂H.
Remarks 15.50
1. The Banach algebra axiom ∗1∗= 1 is redundant for C∞-algebras as it follows
from ∗T ∞T ∗= ∗T ∗2 (assuming X ̸= 0).
2. The use of A < B is best avoided: it may either mean A ⩽B but A ̸= B or that
δ(B −A) ≤]0, ⇒[.

Hints to Selected Problems
2.2 (1) Writing a := x −z, b := z −y, and substituting into |a + b| ⩽|a| + |b|
gives the triangle inequality.
2.3 (2) (a) ∞a √A, ∞b √B, d(a, b) ⩽2, (b) ∗∂> 0, ∞a √A, ∞b √B, d(a, b)
⩽∂.
2.14 (5) The two sets have, respectively, the shapes of a diamond, and a square with
a smaller concentric square removed.
(9) For example, R \ a.
2.20 (2) The complement of the set is { x √Q : x2 > 2 } since
⇒
2 is irrational. To
prove the set is open, one needs to ﬁnd a small enough ∂such that
2 < (x −∂)2 = x2 −2∂x + ∂2.
(6) Try the graph of the exponential function and the x-axis in R2.
(7) The Cantor set is the intersection of all of these closed intervals.
(8) First show the set { x √[0, 1] : n1+···+nk
k
⩽5 } for ﬁxed k is closed.
(10) The answer to the ﬁrst question is of course no: all points on a circle are equally
close to the center; the second is also false e.g. in Z; it is true however in R2 because
the line joining an interior point to x contains closer points. What properties does
the metric space need to have for this statement to be true?
(13) No. Take the subsets A := [−1, 1] and B := R \ { 0 } in R.
2.22 (2) Any ball Br(x) will contain a point a of the dense open set A. There will
therefore be a small ball B∂(a) →A ♦Br(x) which contains a point b √B.
(3) The complement of the Cantor set is open and dense.
(5) δU = ¯U \ U contains no balls.
3.5 (1c) n/an = n/(1 + α)n ⩽
n
n(n−1)α2/2 ⩽4
α2
1
n ∃0.
(1e) an := (1 + 1
n )n = 2 + 1
2(1 −1
n ) + 1
3!(1 −1
n )(1 −2
n ) + · · · + 1
nn . So an+1 > an,
yet an < 2 + 1
2 + 1
3! + · · · < 2 + 1
2 + 1
4 + · · · = 3.
(1f) an ∃∀means ∗∂> 0, ∞N, n ⩾N =∈an > ∂.
(2) The limits must satisfy x = 2 + ⇒x and x = 1 + 1/x respectively.
(3) Eventually, |an| < a < 1, so |an|n < an.
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5,
393
© Springer International Publishing Switzerland 2014

394
Hints to Selected Problems
3.12 (3) See Proposition 7.8
(4) If xn ∃x then x ⊆= 0 and |xn| ⩾c > 0, so that |1/xn −1/x| = |x −xn|/|xn|
|x| ∃0.
(10a) The map f (x) = (cos x, sin x) is a continuous bijective map from [0, 2γ[ to
the circle. The inverse map is discontinuous at (−1, 0).
(10b) Take f to be a constant function, and xn = n.
(10c) Take f (x) := x2 and U := ]−1, 1[. Examples of open mappings on R are
polynomials which have no local maxima/minima.
(11) ( f −1F)c = f −1Fc is open. The identity map [0, 1[ ∃[0, 2] is a continuous
open mapping whose image is not closed.
(17) d(x, A)/(d(x, A) + d(x, B)).
(18) All non-empty open intervals of the type ]a, b[ are homeomorphic to, say, ]0, 1[
by stretching and translating. ]0, 1[ is homeomorphic to ]0, ∀[ via x ∃1/x −1, and
this in turn, is homeomorphic to R via x ∃x +
⇒
x2 + 1 (for example). Similarly,
]a, ∀[ and ]−∀, b[ are homeomorphic to them as well.
(19) Points { x } are open in N but not in Q.
4.10 (1) The difference between the nth and mth terms of decimal approximations
is at most 10−min(m,n).
(4) The ﬁnite number of values have a minimum distance ∂between them.
(5) Taking m ⩽n,
d(xn, xm) ⩽d(xn, xn−1) + · · · + d(xm+1, xm)
⩽a(cn−1 + · · · + cm)
⩽acm
1 −c ∃0
as m, n ∃∀.
Note that 
n 1/n ∃∀.
(6) |d(xn, yn) −d(xm, ym)| ⩽|d(xn, yn) −d(yn, xm)| + |d(xm, yn) + d(xm, ym)|
⩽d(xn, xm) + d(yn, ym)
(7) For example, the continuous function f (x) := 1/x, deﬁned on ]0, 1] ∃[1, ∀[,
maps the Cauchy sequence (1/n) to the unbounded sequence (n).
(9) ⇒n + 1 −⇒n = ⇒n((1 + 1/n)1/2 −1) =
1
2⇒n + · · ·
(11) If { xn } are the values of a Cauchy sequence, and x is a boundary point, then
there is a subsequence xm ∃x (by Proposition 3.4).
(14) Any Cauchy sequence in a discrete metric space must eventually be constant.
(15) The intersection of the balls can contain at most one point, since rn ∃0. In
fact, if xn ∃x, then x √Brn[xn] for all n, since the balls are nested.
(16) First show that f (n) = f (1 + · · · + 1) = nf (1), then f (m/n) = m
n f (1).
4.17(1b) |(x2 −x1)y2 + x1(y2 −y1)| ⩽(|y2| + |x1 + x2|)|x1 −x2| ⩽3|x1 −x2|,
|(x1 + x2)(x1 −x2) + (y2 −y1)| ⩽2|x1 −x2| + |y1 −y2|.

Hints to Selected Problems
395
(5) Let f : X ∃Y be an equivalence; then every Cauchy sequence (xn) in X
corresponds to a Cauchy sequence in Y, by uniform continuity and Proposition 4.12.
Since equivalences are homeomorphisms, (xn) converges precisely when ( f (xn))
does. So X is complete ⇐∈Y is complete.
4.21 (2) Repeat the proof of Proposition 2.10, using Brn(an) instead of Br(x)(x),
where an is an approximation of x.
(4) Let X be an uncountable set with the discrete metric. Then B1/2(x), for each
x √X, form an uncountable collection of disjoint sets.
5.7 (1) Take X \ { x1 } and X \ { x2 } as the open sets; alternatively take small enough
balls. For (b), take X \ F1 and X \ F2.
(2) To show that every subset of Q is disconnected, use the same idea with some
other irrational.
(5) Consider the open sets f −1{ 0 } and f −1{ 1 }.
(11) Suppose f (a) < f (y); f (x) > f (y) is impossible else there is some z √[a, x]
such that f (z) = f (y).
5.12 (2) The metric space is the union of the path images, whose intersection contains
the ﬁxed point.
(5) Use Theorem 5.9 with Ay := X × { y } and B := { x0 } × Y.
(6) Without loss of generality, take x = 0; then R2 \ { x } is connected using the unit
circle and radial lines te for t > −1 and unit vectors e.
(8) Otherwise, the interior and exterior of the set would disconnect a component.
(10a) If a component C has a boundary point a /√C, then C ⊂B∂(a) would be a
strictly larger connected set.
6.4 (3) If B is bounded, so B →Br(x), then B →Br[x].
6.9 (3) From some N onwards, xn √B∂(xN); cover the rest of the values xm with
B∂(xm).
(4) Let B →N
i=1 B∂/2(xi), then B →N
i=1 B∂/2[xi] →N
i=1 B∂(xi) (Theo-
rem2.19).
6.22 (6) Suppose d(K, F) = 0, then there are asymptotic sequences an √K, bn √F;
(an) has a convergent subsequence, and therefore (bn) converges to the same limit.
But then K ♦F ⊆= ⇔.
(7) After showing K ∩Br(r, 0), use the fact that there is a point a √K which has
maximum distance from (r, 0) less than r.
(13) The unit sphere is a closed subset of the cube [−1, 1]N.
(16) X × Y is complete and totally bounded by Proposition 4.7 and Exercise 6.9(1).
6.27 (1) If fn ∃f with fn √C(X, R), then fn(x) ∃f (x) in C, and taking the
imaginary parts shows that f (x) √R.
(4) f (y) −fn(y) ⩽f (y) −fN(y) ⩽| f (y) −f (x)| + | f (x) −fN(x)| + | fN(x) −
fN(y)| < ∂where N depends on x, and |x −y| < α, small enough but independent
of x (Proposition6.17). So f −∂⩽fn ⩽f on Bα(x) for n ⩾N. By compactness,
one N will sufﬁce.
(5) Convert any binary sequence (of 0s and 1s) into a “tent” function in C(R+); there
are uncountably many such functions and their distance from each other is at least 1.
(8) (x + |x|)/2 ∪x(x + 1)/2.

396
Hints to Selected Problems
7.7 (2) Balls look like circles, squares and diamonds in the 2-norm, ∀-norm, and
1-norm respectively.
(4) Let A := { |an| }, B := { |bn| }. Then from Note 15 of Sect. 7.1, sup |ξan| =
sup |ξ|A = |ξ| sup A, sup |an+bn| ⩽sup(A+B) ⩽sup A+sup B, and if sup A = 0,
then 0 ⩽|an| ⩽0 implying an = 0 for all n.
(7) The functions fn := 1[0,1/n] converge to 0 in L1[0, 1] but not in L∀[0, 1]. The
inequality ∥x∥ℓ∀⩽∥x∥ℓ1 remains true for sequences, so convergence in ℓ1 implies
that in ℓ∀.
(8) For r > |||x|||, x √rC, so ξx √ξrC = |ξ|rC, i.e., |||ξx||| ⩽|ξ||||x|||; but
then |||x||| ⩽
1
ξ|||ξx|||. If s > |||y|||, then x + y √rC + sC = (r + s)C, hence
|||x + y||| ⩽|||x||| + |||y|||.
7.14 (3) Let x, y √¯C; then there are points a, b √C within ∂of x and y. So any
point on the line tx + (1 −t)y is also close to a point on the line ta + (1 −t)b which
lies in C because
∥tx + (1 −t)y −ta −(1 −t)b∥⩽t∥x −a∥+ (1 −t)∥y −b∥< ∂.
(4) A convex set C is the union of line segments that start from a ﬁxed point x0 √C,
then use Theorem 5.9.
(5) If ξan ∃x, an √A, then an ∃x/ξ (for ξ ⊆= 0) and x/ξ √¯A. Conversely, if
x √ξ ¯A, i.e., x = ξa with an ∃a, then ξan ∃ξa = x and x √ξA.
Similarly, when an ∃a, an √A, and bn ∃b, bn √B, then an + bn ∃a + b,
so a + b √A + B. An example in R is A := { n + 1/n : n = 2, 3, . . . } and
B := { −n : n = 1, 2, . . . }.
7.20 (1b) N+k
i=N xi = N+k
i=0 xi −N−1
i=0 xi ∃0 as N ∃∀, since convergent
sequences are Cauchy.
(3) The odd sub-sums a1 −(a2 −a3) −(a4 −a5) + · · · are decreasing, and bounded
below by the increasing even sub-sums (a1 −a2) + (a3 −a4) + · · · .
7.22 (5) Applying the Cauchy test to 
n
1
n p : the series 
n 2n/(2np) converges only
when p −1 < 0; for p = 1, 
n
1
n diverges; 
n
1
n log2 n becomes 
n
2n
2nn which
diverges; etc.
(11) For N large enough ∥x1 + · · · + xN −x∥< ∂as well as ∀
n=N+1 ∥xn∥< ∂.
So for k large enough that n1,…,nk include 1, . . . , N,
∥xn1 + · · · + xnk −x∥⩽∥x1 + · · · + xN −x∥+

∥xextra∥
8.10 (3) im R is closed since for Rxn ∃y, the ﬁrst components give 0 ∃y0, so
y = (0, y1, . . .) = R(y1, . . .).
(4) Proof that im T is not closed: Let vn := (1, 1/2, . . . , 1/n, 0, 0, 0, . . .), then
T vn = (1, 1/4, . . . , 1/n2, 0, . . .) converges to (1, 1/4, . . .) √ℓ1 as n ∃∀since
∥(0, . . . , 0, 1/(n + 1)2, . . .)∥ℓ1 =
∀

n=N+1
1
n2 ∃0

Hints to Selected Problems
397
Yet, there is no sequence in ℓ1 which maps to this sequence since (1, 1/2, 1/3, . . .)
⊆√ℓ1.
(5) en/n ∃0 in ℓ1 because ∥(0, . . . , 0, 1/n, 0, . . .)∥ℓ1 = 1/n ∃0, but en ⊆∃0
since ∥(0, . . . , 0, 1, 0, . . .)∥ℓ1 = 1.
(6) T = L4 −I.
(9) If xn ∃x then xn −x ∃0 and T xn −T x = T (xn −x) ∃T 0 = 0.
(11) (a) If T ei are linearly independent, then ei are linearly independent. So if ST ei
are linearly independent, so are T ei and dim(im(ST )) ⩽dim(im(T )); the other
statements follow from im(ST ) →im(S) and im(S + T ) →im(S) + im(T ).
(b) If e1, . . . , ek form a basis for ker T , extended by ek+1, . . . , en to a basis for X,
then T ei, i = k + 1, . . . , n, form a basis for im T .
(c) Let T ei, i = 1, . . . , k, be a basis for ker S ♦im T . Extend ei with a basis e◦
j for
ker T . Then ST x = 0 implies T x = k
i=1 πiT ei, hence x = k
i=1 πiei +
j β je◦
j.
(15) (3) ∥L∥= 1 = ∥R∥; (6), using ℓ∀, ∥S∥= 1, ∥T ∥= 2; (8.4(8)) when T x = ax
on ℓ1, ∥T ∥= ∥a∥ℓ∀; (8.6(1)) ∥

A ∥= 1; (8.6(4)) ∥F∥= 1; (8.6(6)) use ‘spike’
functions that are zero except near to 0; (12) ∥ω∥= 1; (14) ∥T ∥= 1, ∥Ta∥= 1,
∥Mg∥= ∥g∥C;.
(16) Proof for ﬁrst matrix. Assuming, without loss of generality, that |μ| ⩽|ξ|,

ξ 0
0 μ
 x
y
 2 =

ξx
μy
 2 = |ξ|2|x|2 + |μ|2|y|2 ⩽|ξ|2(|x|2 + |y|2)
so ∥T x∥⩽|ξ|∥x∥. However for x =
1
0

, T x = ξx, so |ξ| ⩽∥T ∥⩽|ξ|.
(18) Choose unit xn such that ∥Tnxn∥⩾∥Tn∥−1/2n.
8.14 (6) For x = (ai), take the supremum over i of
|Tiiai +

j⊆=i
Ti ja j| ⩾(|Tii||ai| −

j⊆=i
|Ti j|∥x∥)
⩾c∥x∥−(sup
i
|Tii|)(∥x∥−|ai|) ∪c∥x∥.
(8) If JX : X1 ∃X2 and JY : Y1 ∃Y2 are the isomorphisms, then J(T ) :=
JY T J −1
X
gives the required isomorphism; note that J −1(S) = J −1
Y SJX.
8.21 (3b) Show y ∃(0, y) + X × 0 is an isometry.
(5) Let { an } be dense in M and { bn + M } dense in X/M. Then { an, bm } is dense
in X.
8.25 (5) See the Hilbert cube Exercise9.10(3).
(6) Every point x √[[e1, . . . , ek]] is a boundary point (consider x + ∂ek+1).
9.4 (2) The functionals on c are y≈(y √ℓ1) and Lim.
(6) c00 ∩ℓ∀
s , so ℓ∀
s
= c0; 1/ log n does not belong to any ℓ∀
s .
9.7 (1) Let yn := xn −x √ℓ1; then ∀
i=N+1 |yni| ⩽∀
i=N+1 |y1i| < ∂for some
N and all n. But |yn1| + · · · + |ynN| ∃0 as n ∃∀, so 
i |yni| < 2∂.

398
Hints to Selected Problems
9.10 (4) It is enough to show ∥x −a∥ℓ1 < ∂for a = (a0, . . . , aN, 0, . . .) √c00, N
large enough.
9.15 (2) Try |an|p/p◦e−iηn.
9.27 (2) Look at the dual spaces of L1[0, 1] and c0 to see why they are not isomorphic.
(6) Write γx2
σ 2 + 2γixσ = γ
σ 2 (x + iσ 2σ)2 + γσ 2σ2 to simplify the integral.
10.10 (2) In Pythagoras’ theorem, ∥y + z∥2 = ∥y∥2 exactly when z = 0.
Consider
∥

n
xn∥2 = |

n,m
≡xn, xm⟩| ⩽

n,m
|≡xn, xm⟩| ⩽

n,m
∥xn∥∥xm∥=
	 
n
∥xn∥

2
.
This can only be an equality when |≡xn, xm⟩| = ∥xn∥∥xm∥for each n, m.
(5) Writing x = 
n anvn and y = 
m bmvm for a basis v1,…,vN, we ﬁnd
≡x, y⟩=

nm
anbm≡vn, vm⟩
(10) (1, 1, 0, . . .) and (1, −1, 0, . . .) do not satisfy the parallelogram law; write these
as step functions for L1 and L∀.
(12)
 γ
−γ sin(x) cos(x) dx = 1
2
 γ
−γ sin(2x) dx = [−cos 2x]γ
−γ = 0, and
 1
0 2x3 −
x dx = 1
2[x4 −x2]1
0 = 0.
(15) Substitute ξ = π + iβ, then ﬁnd the minimum by differentiating in π, β to get
ξ = −≡x, y⟩.
(16) ∥xn −xm∥⩽∥xn + yn −xm −ym∥∃0 since ≡xn −xm, yn −ym⟩= 0.
(17) The ‘inner product’ remains continuous, so Z is closed.
10.15 (1) Answer 1
14(22x0 + 2y0 −6z0, x0 + 19y0 −3z0, −6x0 −3y0 + 27z0).
(2a) Px √M so Px = ξy, and x −Px √M∇, so ≡y, x −ξy⟩= 0. Expanding gives
ξ = ≡y, x⟩.
(3) Consider x √M∇, and x = a + b where a √M, b √N; since N →M∇it
follows that a = 0.
(5) Any vector x √N can be written x = a + b where a √M, b √M∇. Since
M →N, then b = x −a √N as well.
(6) Let x = a + b, a √M, b √M∇; then T x = Ta + T b, Ta = Aa √M,
T b = Bb √M∇.
∥T ∥2 = sup ∥Ta∥2 + ∥T b∥2
∥a∥2 + ∥b∥2
by Pythagoras’ theorem. But ∥Ta∥⩽∥A∥∥a∥and ∥T b∥⩽∥B∥∥b∥, so ∥T ∥2 ⩽
t∥A∥2 + (1 −t)∥B∥2, where t = ∥a∥2/(∥a∥2 + ∥b∥2). Now take t = 0 or t = 1
depending on which is the maximum of the two.
(8b) Expand d2 ⩽∥x −y∥2 = 2 −2 Re ≡x, y⟩with y = eiηv.
(9c) If ∥x −a∥= d = ∥x −b∥is the shortest distance from x to M, then
∥tx + (1 −t)x −ta −(1 −t)b∥= d.
(9d) The closest sequence would be 1 /√c0.

Hints to Selected Problems
399
(10) ∥P1yn+1∥⩽∥yn+1∥⩽∥yn∥, so ∥yn∥converges. But in general, as Py ∇
(y −Py), ∥y∥2 = ∥Py∥2 + ∥y −Py∥2, so ∥yn −P1yn∥∃0, and similarly
P1yn −P2P1yn ∃0. In ﬁnite dimensions, the bounded sequence yn has a convergent
subsequence, yni ∃y, so y = P1y = P2P1y, and y is in im P1 ♦im P2.
(11) sin x ∪0.955 −0.304x ∪−0.20 + 1.91x −0.88x2 + 0.093x3; 1 −x3 ∪
1.13 cos x −0.43 sin x.
(12c) Answer: π = 2
γ
3M R2−5I
R4
, β = 15
2γ
2I−M R2
R5
.
10.18 (2) Check that ∥x≤∥H≤satisﬁes the parallelogram law, then use the polarization
identity, noting that (ix)≤= −ix≤.
(3) ˜ω corresponds to Px.
(4) The map x ∃≡≡x, ⟩⟩is a functional so corresponds to some vector T x.
10.26 (2) ∥T ∥2 = ∥T ≤T ∥⩽∥T ≤∥∥T ∥, so ∥T ∥⩽∥T ≤∥⩽∥T ≤≤∥= ∥T ∥.
(3) For x = (an), y = (bn), z = (cn),
≡z, yx⟩=

n
cnbnan =

n
¯bncnan = ≡¯yz, x⟩
(5)
 1
0 g(x)V f (x) dx =
 1
0
 x
0 g(x) f (t) dt dy =
 1
0
 1
x g(x) f (t) dy dt.
(8) T ≤T x = 0 =∈0 = ≡x, T ≤T x⟩= ≡T x, T x⟩.
(9) Fix a unit vector u √X, ξ := ≡T u, T u⟩> 0, and let v be any orthogo-
nal unit vector; then ≡T u, T v⟩= ≡u, v⟩= 0; similarly, ≡T (u + v), T (u −v)⟩=
≡u + v, u −v⟩= 0, so ≡T v, T v⟩= ξ > 0 constant. For vectors x = πu,
y = β1u + β2v, ≡T x, T y⟩= ¯πβ1ξ = ξ≡x, y⟩.
(11) Answers: (−5/2, −2/3, 7/6), (−17, −5, 7)/3.
(15) T †T is the projection onto ker T ∇; T T † is the projection onto im T .
(17) V ≤V f = V ≤g is
 1
y
 x
0 f (t) dt dx =
 1
y g(x) dx.
(18) Answer:r = 0.497m and τ/m = 0.0062m−1 (the actual values used to generate
the data were r = 0.5m and τ/m = 0.003m−1).
10.35 (1) Take the inner product of 
n πnen = 0 with em.
(4) ≡(en, 0), (0, ˜em)⟩= ≡en, 0⟩+ ≡0, ˜em⟩= 0; if x and y can be approximated by
xN := N
n=1 πnen and yM := M
m=1 βm ˜em respectively, then
∥(x, y) −(xN, yM)∥= ∥(x −xN, y −yM)∥=

∥x −xN∥2 + ∥y −yM∥2
can be made small; note that (xN, yM) = (xN, 0) + (0, yM) = N
n=1 πn(en, 0) +
M
m=1(0, ˜em).
(5) ≡x −x≤, en⟩= 0
(6) Suppose en and Uen are both orthonormal bases. Then, by Parseval’s identity,
≡Ux,Uy⟩=

n,m
¯πnβm≡Uen,Uem⟩= ≡x, y⟩.
U is onto because y = 
n πnUen = U(
n πnen).

400
Hints to Selected Problems
Conversely, if { en } is an orthonormal basis for H1, and y √{ Uen }∇, then 0 =
≡y, Uen⟩= ≡U ≤y, en⟩for all n, so U ≤y √{ en }∇= 0 and ∥y∥= ∥U ≤y∥= 0.
The column vectors of the matrix of U are Uen, so ≡Uen, Uem⟩= ≡en, em⟩= αnm.
(8) For example, take ξ
1
0

, ξ
2
 −1
±⇒3

. For the second part, substitute em instead of
x, and deduce orthogonality; if x √{ en }∇, then ∥x∥= 0.
(9) Show x = 1
2 + 1
γ

n⊆=0
1
2γn e2γinx, then take x = 1/4. It is interesting to generate
other series using other points and functions (e.g. |x|, x/|x|, | sin x|).
(10) For f odd about 1/2, π−n = −πn. In general, every f is the sum of an even
and an odd function.
11.7 (4b) Continuity of T (Sx) := T x: For any v √ker S and y √Y, ∥T y∥=
∥T x∥= ∥T (x + v)∥⩽c∥T ∥∥x + v∥, then use ∥x + ker S∥⩽c∥Sx∥.
(5) |πn| = ∥πnen∥⩽∥n
i=1 πiei −n−1
i=1 πiei∥⩽2c|||x|||.
11.15 (6) If xn √Br(0) then T xn √T Br(0), so has a Cauchy subsequence, which
converges.
11.26 (4) The requirement is ω(x, y) = x + ξy, |x + ξy| ⩽|x| + |y|, so |ξ| ⩽1.
(8) ∇ = 0, so (∇)∇= ℓ1≤. Now in the correspondence of ℓ1≤with ℓ∀, we get
[[]] ≡c00 and so [[]] ≡c0.
(9) |ωx| = |ω(x +a)| ⩽∥ω∥∥x + a∥for any a √M; in fact this approaches equality
for certain a √M, so ∥ψ∥= ∥ω∥. Onto: for any ψ √(X/M)≤, let ωx := ψ(x + M).
Hint for the second part: the norm of ∥ω + M∇∥= infψ√M∇∥ω + ψ∥is the same
as ∥ω|M∥.
11.32 (5) (T ≈≈x≤≤)ω = x≤≤(T ≈ω) = (T ≈ω)x = ωT x.
(7) If T ≈is onto, then T is 1-1 by (1) and has a closed image; if T ≈is also 1-1, then
im T is dense, hence T is onto. If T is onto, use the open mapping theorem.
11.42 (1) For c0, a functional is of the type y≈where y = (bn) √ℓ1. Now y · en =

i biαni = bn ∃0 as n ∃∀since ℓ1 ∩c0.
(2) Use the functional ei · xn = ani. The converse is true for ℓp, 1 < p < ∀,
whose dual space has the Schauder basis e≈
n; any ω √ℓp≤can be approximated by
N
i=0 bi e≈
i . So
ωxn ∪
N

i=0
bi ei · xn =
N

i=0
biani ∃
N

i=1
biai ∪ωx
For ℓ1, a functional is of the same type but y √ℓ∀. This time y · en = bn need not
converge to 0, e.g. y := 1.
(4) If Tn ⇀T and Tn ⇀S, then ωT x = ωSx for all ω and x, so T = S.
(11b) |ω(TnSn −T S)x| ⩽∥ω∥∥Tn∥∥(Sn −S)x∥+ |ω(TnS −T S)x| ∃0.
(15) If x ⊆√M, there is a ω √X≤such that ωx = 1, ωM = 0, so x is not a weak
limit point of M. More generally, every closed convex set is weakly closed, because
a hyperplane (so a functional) separates it from any point not in it.

Hints to Selected Problems
401
12.12(4)∥o(h)∥= ∥f (x + h) −f (x) −f ◦(x)h∥=

 1
0
dt f (x + th) −f ◦(x)h dt

⩽
 1
0
∥f ◦(x + th) −f ◦(x)∥∥h∥dt
⩽1
2k∥h∥2.
12.21 (5) Poles and residues are (a) i: 1/2ie, and −i : ie/2; (b) 1 : (e, e−1)/3, and
ω : (eω, e−ω)/3ω2, and ω2 : (eω2, e−ω2)/3ω; (c) 0 : 1.
13.3 (11) If T R is invertible, then P(ST ) = 1 = (T R)Q, so T is invertible.
13.10 (2) Each vector (a, b) corresponds to the matrix
a
0
b a + b

.
(4) 1, A, . . . , AN cannot be linearly independent, so Am = p(A) must be true for
some polynomial p.
(10) This is a generalization of the convolution operation on ℓ1. The proofs are very
similar to that case Exercise9.7(2).
(13) For any ω, T xωx = xωT x, i.e., T x = ξxx. So if x, y are linearly dependent
then T y = ξyy, implying T x = ξyx and ξy = ξx; if not, then ξy = ξx+y = ξx.
(14d) If S, T √A◦◦, then T R = RT for any R √A◦ˆ A◦◦, including R = S.
(16) To show IA →I, let f √IA and let K be a closed subset of [0, 1] \ A;
then for any x √K, one can ﬁnd a function gx √I such that gx(x) > 1 in a
neighborhood of x. By compactness of K, a ﬁnite number of such functions “cover”
K, so g := gx1 +· · ·+gxn √I is greater than 1 on K. Let h(x) :=

1
g(x) > 1
g(x) g(x) ⩽1,
a continuous function with h|K = 1 and belonging to I (h = gk). By making K
larger, one can ﬁnd a sequence of functions such that hng ∃g, so g √I.
(17) To show ∥f + IA∥= ∥f |A∥, it is required to ﬁnd functions gn √IA such
that ∥f −gn∥∃∥f |A∥. This can be done as follows: take B := [0, 1] \ U, where
U = A + B∂(0), and let h be a function such that h|A = 0, h|B = 1; so f h √IA
yet f −f h = 0 on B, and ∥f −f h∥∃∥f |A∥as ∂∃0.
(19) Multiplication is well-deﬁned, for if S −S √I, T −T √I, then ST −ST =
(S −S)T + S(T −T ) √I. Associativity and distributivity follow from those of
X. Suppose ∥S + An∥∃∥S + I∥, ∥T + Bn∥∃∥T + I∥, for some An, Bn √I,
then
∥ST + I∥⩽∥(S + An)(T + Bn)∥⩽∥S + An∥∥T + Bn∥∃∥S + I∥∥T + I∥
Finally, ∥1 + I∥⩽∥1 + 0∥= 1 yet ∥1 + I∥⊆= 0; but also in any normed algebra
in which ∥ST ∥⩽∥S∥∥T ∥holds, 1 ⩽∥1∥, since ∥1∥= ∥12∥⩽∥1∥2.

402
Hints to Selected Problems
(24) X has a basis of two vectors, which can be taken to be 1 =
1
0

and
0
1

.
Multiplication by 1 acts of course as the identity matrix; if
0
1
 0
1

=
π
β

, then
0
1
 x
y

=
 πy
x + βy

=

0 π
1 β
 
x
y

.
13.19 (1) Answers (a) 0, (b) 1, (c) max(|a|, |b|), (d) (a ⊆= 0)
a 1
0 a
n
=

an nan−1
0
an

= an
1 n/a
0
1

.
Now

1 n/a
0
1
 
1
0

=

1
0

, so 1 ⩽


1 n/a
0
1
  ⩽

2 + n2/a2 (Example7.9(2)).
Taking the nth root gives (2 + n2/a2)1/2n ∃1, so ρ(T ) = |a|. Note how, in this
case, ∥T n∥ﬁrst increases then decreases to 0. Only (c) has ρ(T ) = ∥T ∥.
(3) Use the Cauchy inequality for |x + ay| ⩽

1 + |a|2
|x|2 + |y|2.
(7) Let R and S be the radii of convergence of 
n anzn and 
n bnzn. Then

n(an +bn)zn = 
n anzn +
n bnzn has radius of convergence at least min(R, S).

n anbnzn hasradiusofconvergence RS sincelim inf |anbn|−1/n = lim inf |an|−1/n
|bn|−1/n.
(8) f + g and f g have coefﬁcients an + bn, a0bn + a1bn−1 + · · · + anb0.
f ˙ g(T) = a0 + a1g(T ) + a2g(T )2 + · · ·
= (a0 + a1b0 + a2b2
0 + · · · ) + (a1b1 + 2a2b1 + · · · )T
+ (a1b2 + a2b2
1 + · · · )T 2
(9) ∥f (T ) −N
n=0 anT n∥= ∥∀
n=N+1 anT n∥⩽∀
n=N+1 |an|∥T ∥n ∃0 when
∥T ∥< R.
(14) cos 0 = e0 = 1, but cos 2 = (cos 1 −sin 1)(cos 1 + sin 1) < 0, so there is a
number 0 < β < 2, cos β = 0. Since the conjugate of eiη is e−iη, it follows that
|eiη| = 1, so sin β = 1; hence eiβ = i and e4βi = 1.
(17) Expand eπ1Seπ2T eπ3T eπ4T to second order, and equate with eS+T ∪1 + (S +
T ) + (S + T )2/2, to get π2π3 = 1/2; the two values can be chosen to be equal.
13.25 (2) f (t)g(t) = 1 ⇐∈
f (t) = 1/g(t) ⊆= 0, ∗t √[0, 1]. g has a minimum
distance to the origin Exercise6.22(10), so f = 1/g is also bounded.
(4) ∥T −1∥= supx ∥T −1x∥/∥x∥= supy ∥y∥/∥T y∥.
(8)
e(t+s)T = etT +sT = etT esT since (tT )(sT ) = (sT )(tT ).
e(t+h)T = etT ehT = etT (1 + hT + o(h))
so the derivative at t is etT T .
(12) SR = 0 for S(a0, a1, . . .) := (a0, 0, . . .). But ∥RT ∥= ∥T ∥for all T , so
RTn ⊆∃0 when Tn are unit elements.

Hints to Selected Problems
403
13.31 (2) If | f (z)| ⩽c|z|m/n ⩽c|z|k, then f is still a polynomial.
(8) If a is a zero or pole of order ±N, then (z −a)±N f (z) is analytic and non-zero
at a. Thus q f/p is bounded analytic on C, so must be constant.
14.7 (2) f (t)−ξ is not invertible precisely when f (t0)−ξ = 0 for some t0 √[0, 1].
(4) T 2 −z2 = (T −z)(T + z), so z2 := ξ √σ(T 2) =∈ξ = ±z √σ(T ) (one
of them). Conversely, if T 2 −z2 has an inverse S, then S(T + z)(T −z) = 1 =
(T −z)(T + z)S, so T −z is invertible.
(7) (S, T ) −ξ(1, 1) = (S −ξ, T −ξ) is not invertible iff S −ξ or T −ξ is not
invertible.
(8) The map T ⊙S −ξ : (x, y) ∃(T x −ξx, Sy −ξy) is invertible exactly when
T −ξ and S −ξ are invertible.
14.13 (1) Rx = ξx means an = ξan+1, so an = a0/ξn; but also 0 = ξa0. There are
no solutions to these algebraic equations.
(3) ℓ1 is embedded in ℓ1(Z), so σ(L) decreases from the ﬁrst case to the second.
In fact, in ℓ1(Z), there are no eigenvalues, because ∀
n=−∀|ξ|n cannot converge
for any ξ. Yet the boundary of σ(T ) in ℓ1, consisting of generalized eigenvalues, is
preserved in ℓ1(Z).
(4) T ≈x = (a0, a2, a3, . . .) on ℓ1.
(5) T ≈x = (a0, a2, a3/2, . . .) on ℓ1.
(9) The operator (T −ξ) f (x) = (x −ξ) f (x) is invertible only when ξ ⊆√[0, 1].
There are no eigenvalues because x f (x) = ξf (x) for all x implies f = 0. The image
of T −ξ is a subset of { g √C[0, 1] : g(ξ) = 0 }; as this set is closed and not C[0, 1],
all ξ √[0, 1] are residual spectral values.
(10) Induction on n: Expand V V n f as a double integral and change the order of
integration.
(12)1−|ξ| ⩽∥T xn −ξxn∥∃0; T −ξ = T (1−ξT −1),so|ξ| < 1 =∈ξ ⊆√σ(T ).
The boundary of σ(T ) must be part of the circle.
(13) T is 1-1 with a closed image ⇐∈∥T x∥⩾c∥x∥, so
∥(T + H)x∥⩾∥T x∥−∥Hx∥⩾(c −∥H∥)∥x∥
shows T is an interior point of the set.
14.21 (7) The eigenvalue equation for ML is an+1 = nξan, so an = n!ξnx0 ∃∀.
For RM, { 0 } = σp((RM)≈) →σr(RM).
14.29 (3) eσ(T ) = σ(eT ) = σ(1) = { 1 }, so σ(T ) →2γiZ. For an idempotent P,
e2γ P = 1 + P(2γi + (2γi)2
2!
+ · · · ) = 1 + P(e2γi −1) = 1.
14.40 (11) CN is generated by ei, where ei e j = 0 when i ⊆= j, and ei ei = 1. So
a character satisﬁes αeiαe j = 0 and αei = ±1. If αe1 = ±1, say, then αei = 0 for
i ⊆= 1. In fact 1 = α(1) = 
i α(ei) = α(e1).
(12) B(C2) is generated by

1 0
0 0

,

0 1
0 0

,

0 0
1 0

, and

0 0
0 1

. A character α maps
them to w1, …, w4, which must satisfy w2
2 = 0, w2
3 = 0, w2w3 = w1, w3w2 = w4,
for which there are no non-zero solutions.
(15) x acts on the N points in  as (αi x) = (xi) = x.

404
Hints to Selected Problems
(17) In the commutative Banach algebra Y := { S, T }◦◦, the spectra remain the same,
σY(A) = σ(A), so (A) = σ(A) and the inclusions follow from (S + T ) →
(S) + (T ) and (ST ) →(S)(T ).
15.3 (4) T is left- and right-invertible: T T ≤R = 1 = R◦T ≤T .
(7) Use Theorem 13.9; note that L≤L = π √R, so π = ξ2.
15.11 (5) What is meant is that if T √X is normal, and J is a ≤-morphism, then
J(T ) √Y is also normal, etc.
(8) The inverse of Ta is T−a, which is the adjoint:

g(x)Ta f (x) dx =

g(x) f (x −a) dx =

g(t + a) f (t) dt =

T−ag(t) f (t) dt.
(20) ∥(T ≤T )n∥1/2n = ∥(A≤A)n + (B≤B)n∥1/2n ⩽(∥A∥2n + ∥B∥2n)1/2n ∃
max(∥A∥, ∥B∥)
(22) If T ≤T is idempotent, then σ(T T ≤) →σ(T ≤T ) ⊂{ 0 } →{ 0, 1 }. Hence
σ(T T ≤T T ≤−T T ≤) = { 0 }.
15.15 (1) ≡x, T T ≤x⟩= ∥T ≤x∥2 = ∥T x∥2 = ≡x, T ≤T x⟩and use Example 10.7(3).
(3) ∥T ≤
n x −T ≤x∥= ∥(Tn −T )≤x∥= ∥Tnx −T x∥∃0. Conversely, take the limit
of ∥T ≤
n x∥= ∥Tnx∥and use Exercise 1.
(4) |ξ|2∥x∥2 = ∥ξx∥2 = ∥Ux∥2 = ∥x∥2.
(5) Each distinct eigenvalue comes with an orthogonal eigenvector. In a separable
space, there can only be a countable number of these.
(6) ≡em, T ≤en⟩= ≡T em, en⟩= ¯ξnαnm, so T ≤en = 
m ≡em, T ≤en⟩em = ¯ξnen. Then
show ∥T ≤x∥= ∥T x∥.
(8) For (b), note that σ(I −T n) = 1−σ(T )n →B1[1], so ∥I −T n∥= ρ(I −T n) ⩽
2. For (c) use H = ker(T ≤−I) ⊕ker(T ≤−I)∇= ker(T −I) ⊕im(T −I).
15.20 (1) Use Example 10.7(3).
(3b) Let M, M∇be the domains of A and D. For any x = a + b √M ⊕M∇,
≡x, T x⟩= ≡a + b, T a + T b⟩= ≡a, Ta⟩+ ≡b, T b⟩,
≡x, x⟩= ≡a + b, a + b⟩= ∥a∥2 + ∥b∥2.
As ≡a, Ta⟩= ∥a∥2ξ with ξ √W(A), and similarly ≡b, T b⟩= ∥b∥2μ, μ √W(D),
the values of ≡x, T x⟩/∥x∥2 includes the line between ξ and μ. The collection of
these lines is the convex hull of W(A) ⊂W(D).
(4b) For T :=
a 1
0 a

, let x =
π
β

, then ≡x, T x⟩= |π|2a + ¯πβ + |β|2a = a + ¯πβ,
because of the condition 1 = ∥x∥2 = |π|2 + |β|2. But ¯πβ = cos t sin t eiη takes the
value of any complex number in the closed ball B1/2[0].
(11c) Let ξ := ≡T ⟩x, so 0 = σ 2
T = σ 2
T −ξ = ∥T −ξ∥2.

Hints to Selected Problems
405
15.25 (1) The singular values are (i) 4 with singular vectors proportional to
1
2

,
2
1

,
and 1 with
 2
−1

,
 1
−2

; (ii)
⇒
3 with
1
2
1

,
1
1

, and 1 with
 1
0
−1

,
 1
−1

.
(7) Let S := T/ξ where ξ is the largest eigenvalue (in the sense of magnitude); it
has the same eigenvectors en as T except with eigenvalues μn := ξn/ξ. If v0 =

n anen + y, where y √ker(T −ξ) then Skv0 = 
n μk
nanen + y. So
∥Skv0 −y∥2 =

|μn|2k|an|2 ⩽c2k∥v0∥2,
(0 ⩽c < 1)
and Skv0 ∃y as k ∃∀. Hence |ξ|k
ξk
T kv0
∥T kv0∥=
Skv0
∥Skv0∥∃y/∥y∥, and vk+1 ∪
ξky
∥ξky∥;
the sequence does not converge unless ξ = |ξ| but behaves like eikηy/∥y∥.
15.34 (6) Answers: (b) eigenvalues 1/(n + 1
2)γ, eigenvectors sin(n + 1
2)γx; (c)
1/(n + 1
2)2γ2, sin(n + 1
2)γx; so

n
1
(n + 1
2)4γ4 =
1

0
1

0
min(x, y)2 dy dx = 1/6.
15.39 (3) If ωA = 0 for all ω √S and A is self-adjoint, then σ(A) →S(A) = { 0 }
and A = 0.
(7) σ(T ) →B1[0], and σ(T )−1 = σ(T −1) →B1[0].
(8) By the spectral mapping theorem { 0 } = σ(P2 −P) = { ξ2 −ξ : ξ √σ(P) },
so ξ = 0, 1.
15.44 (4) Let T = A + i B with A, B self-adjoint. Then A ⩾0 implies S(T ) →
S(A)+iS(B) →R+ +iR. Conversely, A = (T +T ≤)/2, so ωA = (ωT +ωT )/2 =
Re ωT ⩾0 for ω √S.
15.49 (4) |T ≤|2 = T |T |U ≤= TU ≤U|T |U ≤= (TU ≤)2.
(10) |T | is invertible, so let U := T |T |−1; it is unitary, e.g. UU ≤= T |T|−2T ≤
T T −1 = 1.
(11) (b) T = U|T| = S|T |
1
2 , where := U|T |
1
2 √HS, so tr(T ) = tr(S|T |
1
2 ) is
independent of the basis.
(c) | tr(ST )| = | tr(SU|T |)| = |≡U|T |
1
2 , S≤|T |
1
2 ⟩HS|
⩽∥U|T |
1
2 ∥HS∥S≤|T |
1
2 ∥HS ⩽∥S≤∥∥|T |
1
2 ∥
2
HS = ∥S∥∥T ∥Tr
(d) The norm axioms are satisﬁed because ∥T ∥Tr = ∥|T|
1
2 ∥
2
H S and
tr |S + T | = tr U ≤(S + T ) = ≡U, S⟩HS + ≡U, T ⟩HS ⩽∥S∥HS + ∥T ∥HS.
Also,

406
Hints to Selected Problems
∥T ≤∥Tr = tr UT ≤= tr T ≤U = tr |T |.
(e) |T | = U ≤T = C B where C := U ≤A √HS. So tr |T | = ≡C≤, B⟩⩽
∥A∥HS∥B∥HS.
(f) If en, e◦
n are the singular vectors of T , then |T |2en = |ξn|2en. Take the polar
decomposition of ≡e◦
n, T en⟩= eiηn|≡e◦
n, T en⟩|, and let Uen := eiηne◦
n. Then

n
|≡e◦
n, T en⟩| =

n
≡en,U ≤T en⟩= tr(U ≤T ) ⩽∥T ∥Tr
If T en = ξne◦
n, then ∥T ∥Tr = 
n ≡e◦
n, T en⟩= 
n ξn.

Glossary of Symbols
∃Converges to
⇀Weak convergence
∥· ∥X Norm of space X
≡·, ·⟩X Inner product of space X
1E Characteristic function on E

n A series of terms
[an] Equivalence class of sequence (an)
T ≤Hilbert adjoint of an operator T , or the involute of an algebra element
T ≈Adjoint of an operator T
x≈Dual of a sequence x
T Gelfand/Fourier transform of T
[[A]] Span of vectors in A
Ac Complement of set A
A◦Commutant algebra of A
A˙ Interior of set A
A∇Annihilator or orthogonal complement of A
∇A Pre-annihilator of A
X≤Dual space of X, or set of conjugates of X
δ A Boundary of set A
¯A Closure of set A
xy Multiplication of sequences
x · y Dot product of sequences
x ≤y Convolution of sequences or functions
A + B Addition of sets
A ⊕B Direct sum of subspaces
X ∼= Y Isomorphic spaces
X ≡Y Isometric spaces
X ∩
∼Y X is embedded in Y
X/M Quotient space of X by M
B(X) Space B(X, X)
B(X, Y) Space of continuous linear operators X ∃Y
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5,
407
© Springer International Publishing Switzerland 2014

408
Glossary of Symbols
Br(a) Ball of radius r, center a
Br[a] Closed ball
BX Unit open ball of X
c Space of convergent sequences
c0 Space of sequences that converge to zero
C(X) Space Cb(X, C)
Cb(X, Y) Space of bounded continuous functions f : X ∃Y
Cn(R, X) Space of n-times continuously differentiable functions
Cω(A) Space of analytic functions on A
C[x, y] Space of polynomials in x, y
codim A Codimension of subspace A
d Distance function
D Differentiation operator
D(U, Y) Set of differentiable functions
D1 “Taxicab” distance on X × Y
D∀Max distance on X × Y
 Character space of X
αx Dirac functional
dim X Dimension of space X
F A ﬁeld, usually R or C
G(X) Group of invertibles of X
I Identity operator
im T Image of a linear map T
index(T ) Index of a Fredholm operator T
J Radical of an algebra
ker T Kernel or null space of a linear map T
L Left-shift operator
ℓp Space of sequences with the p-norm
L p(A) Space of functions on A with the p-norm
limn∃∀Limit as n ∃∀
μ Lebesgue measure on RN
Ma Multiplication operator by a
S(X) State space of an algebra
R Right-shift operator
ρ(T) Spectral radius of T
σ(T ) Spectrum of T
Ta Translation by a
tr(T ) Trace of T
W(T ) Numerical range of T

Further Reading
Functional analysis impinges upon a wide range of mathematical branches, from
linear algebra to differential equations, probability, number theory, and optimization,
to name just a few, as well as such varied applications as ﬁnancial investment/risk
theory, bioinformatics, control engineering, quantum physics, etc.
As an example of how functional analysis techniques can be used to simplify
classical theorems consider Picard’s theorem for ordinary differential equations. The
differential equation y◦= F(x, y), y(a) = ya, is equivalent to the integral equation
y(x) = T (y) := ya +
 x
a F(s, y(s)) ds. It is not hard to show that if F is Lipschitz
in y and continuous in x, then T is a contraction map on C[a −h, a + h] for some
h > 0, and the Banach ﬁxed point theorem then implies that the equation has a
unique solution locally.
However, the classical derivative operator is in many ways inadequate: its domain
is not complete and it is unbounded on several norms of interest. But there is a way
to extend differentiation to much larger spaces, namely Sobolev spaces and Dis-
tributions. The former are Banach spaces L p
s of functions that have certain grades
of integrability (p) and differentiability (s), while the latter are spaces of function-
als that act on them with weak*-convergence. Distributions include all the familiar
functions in L1
loc, but also other ‘singular’ ones, such as Dirac’s delta ‘function’ α
and 1/xn. Differentiation can be extended as a continuous operator on these spaces,
e.g. L p
s ∃L p
s−1. Moreover, distributions can be differentiated inﬁnitely many times;
for example, the derivative of the discontinuous Heaviside function 1R+ is α. But,
in general, ‘singular’ distributions cannot be multiplied together. A central result
is the Sobolev inequality, ∥u∥Lq(Rn) ⩽cn,p∥Du∥L p(Rn), for n ⩾2, 1
q = 1
p −1
n ,
which implies that the identity map L p
s (Rn) ∃Lq
t (Rn), along the arrows in Fig.1,
is continuous. The study of operators on such generalized spaces is of fundamen-
tal importance: from extensions of the convolution and the Fourier transform, to
pseudo-differential operators of the type f (x, D), singular integrals, and various
other transforms (see [12, 26, 28]).
Although unbounded, classical differential operators are normal ‘closed opera-
tors’: these have a graph { (x, T x) : x √X } which is closed in X × X. Quite
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5,
409
© Springer International Publishing Switzerland 2014

410
Further Reading
1 −1
p
s
diﬀerentiability
Lp
s
L1
s
Lp
integrability
L2
L1
C
distributions
Cs
slope of line
equals n
Fig. 1 Sobolev Spaces
a lot of the spectral theory extends in modiﬁed form to them. For example their
spectrum remains closed but not necessarily bounded. So, if one inverts in a point
ξ ⊆√σ(T ) then (T −ξ)−1 becomes a regular continuous operator, which can often be
expressed as an integral operator, whose kernel is called its Green’s function. Indeed,
it turns out that ‘elliptic’ differential operators become Fredholm self-adjoint opera-
tors under this inversion. This immediately gives certain results, usually falling under
the heading of Sturm-Liouville theory, such as that the spectrum of the Laplace oper-
ator −△on a compact shape in RN is an unbounded sequence of isolated positive
eigenvalues, called the “resonant frequencies” or “harmonics” of the shape. Deeper
results include the Atiyah-Singer index theorem: the Fredholm index of an elliptic
differential operator is equal to a certain topological invariant of the domain.
The concept of a Banach space can be generalized to a topological vector space,
namely a vector space with a topology that makes its operations continuous. Many
theorems continue to hold at least for “locally convex topological vector spaces”,
including the Hahn-Banach theorem, the open mapping theorem, and the uniform
boundednesstheorem.OtherimportantresultsareSchauder’sﬁxedpointtheorem,the
Krein-Milman theorem, the analytic Fredholm index theorem, and the Hille-Yosida
theorem.
Harmonic analysis is the study of general (but usually locally compact) group
algebras, especially the Fourier transform. The central results are the Pontryagin
duality theorem, which asserts that the character space of L1(G) is itself a group that
is ‘dual’ to G, and the Peter-Weyl theorem. von Neumann algebras are *-algebras that
arise as double commutators of C≤-algebras. Equivalently, they are the weakly closed
subspaces of B(H). The spectral theorem holds for them. There is a lot of theory
devoted to their structure, and a complete classiﬁcation is still an open problem.
One must also include some outstanding conjectures: whether every operator on
a separable Hilbert space has a non-trivial closed invariant subspace; whether every
inﬁnite-dimensional Banach space admits a quotient which is inﬁnite-dimensional

Further Reading
411
and separable; Selberg’s conjecture about the ﬁrst eigenvalue of a speciﬁc Laplace-
Beltrami operator on Maass waveforms; the Hilbert-Pólya conjecture that the non-
trivial zeros of the Riemann zeta function are the eigenvalues of some unbounded
operator 1
2 + i A with A self-adjoint; etc.

References
1. Bollobás B (1999) Linear analysis: an introductory course. Cambridge University Press,
Cambridge
2. Kreyszig E (1978) Introductory functional analysis with applications. Wiley, New York
3. Riesz F, S-Nagy B (1990) Functional analysis. Dover Publications, New York
4. Schechter M (2002) Principles of functional analysis. AMS Bookstore, Boston
5. Shilov G (1996) Elementary functional analysis. Dover Publications, New York
6. Bear H (2002) A primer of Lebesgue integration. Academic Press, California
7. Körner T (1989) Fourier analysis. Cambridge University Press, Cambridge
8. Steele J (2004) The Cauchy-Schwarz master class. Cambridge University Press, Cambridge
More Advanced Books
9. Bachman G, Narici L (2000) Functional analysis. Dover Publications, New York
10. Conway J (1990) A course in functional analysis. Springer, New York
11. Lax P (2002) Functional analysis. Wiley, New York
12. Lieb EH, Loss M (2001) Analysis. AMS Bookstore, California
13. Megginson R (1998) An introduction to Banach space theory. Birkhäuser, Berlin
14. Murphy GJ (1990) C≤-algebras and operator theory. Academic Press, New York
15. Palmer TW (2001) Banach algebras and the general theory of ≤-algebras (vol I and II).
Cambridge University Press, Cambridge
16. Rudin W (1991) Functional analysis. McGraw-Hill, New York
17. Yoshida K (1995) Functional analysis. Springer, New York
Other References
18. Aster RC, Borchers B, Thurber CH (2012) Parameter estimation and inverse problems, 2nd
edn. Academic Press, New York
19. Barnsley M (2000) Fractals everywhere. Morgan Kaufmann, California
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5,
413
© Springer International Publishing Switzerland 2014

414
References
20. Brown JW, Churchill RV (2008) Complex variables and applications, 8th edn. McGraw-Hill,
New York
21. Devaney A (2012) Mathematical foundations of imaging, tomography and waveﬁeld inversion.
Cambridge University Press, Cambridge
22. Diestel J, Uhl J (1977) Vector measures, vol 15. Mathematical Surveys, AMS, Boston
23. Herman GT (2010) Fundamentals of computerized tomography: image reconstruction from
projections, 2nd edn. Springer, New York
24. Jahnke HN (ed) (2003) A history of analysis, history of mathematics, vol 24. AMS, Boston
25. Peitgen HO, Jürgens H, Saupe D (2004) Chaos and Fractals: new frontiers of science. Springer,
New York
26. Stein EM (1993) Harmonic analysis. Princeton University Press, Princeton
27. Walker JS (2008) A primer on wavelets and their scientiﬁc applications, 2nd edn. Chapman
and Hall/CRC, Florida
28. Ziemer WP (1989) Weakly differentiable functions. Springer, New York
Selected Historical Articles
29. Volterra V (1887) Sopra le funzioni che dipendono da altre funzioni; Sopra le funzioni dipen-
denti da linee. In: Rendiconti della Reale Academia dei Lincei, vol 3.
30. Lebesgue H (1902) Intégrale, longueure, aire, Thesis.
31. Fredholm EI (1903) Sur une classe d’equations fonctionnelles. Acta Mathematica 27:365–390
32. Fréchet M (1906) Surquelques points du calcul fonctionnel. Thesis.
33. Hilbert D (1904–1910) Grundzüge einer allgemeinen Theorie der linearen Integralgleichungen,
I-VI. Nachr Akad Wiss Göttingen Math -Phys Kl.
34. Hausdorff F (1914) Grundzüge der Mengenlehre. von Veit, Leipzig
35. Riesz F (1918) Sur une espèce de géométrie analytiques des systèmes de fonctions sommables.
C R Acad Sci Paris 144:1409–1411
36. von Neumann J (1929) Zur Algebra der Funktionaloperationen und Theorie der normalen
Operatoren. Mathematische Annalen 102:370–427
37. Banach S (1932) Théorie des Opérations Linéaires. Subwencji Funduszu Kultury Narodowej,
Warsaw
38. Phillips RS (1940) On linear transformations. Trans Amer Math Soc 48:516–541
39. Carleson L (1966) On convergence and growth of partial sums of Fourier series. Acta Math
116(1–2):135–137
40. Lindenstrauss J, Tzafriri L (1971) On the complemented subspaces problem. Israel J Math
9:263–269
41. Enﬂo P (1973) A counterexample to the approximation problem in Banach spaces. Acta Math
130:1
42. Gowers WT, Maurey B (1993) The unconditional basic sequence problem. J Am Math Soc
6:851–874 Selected Historical Articles.

Index
A
Absolute convergence, 108, 109, 203, 289
Adjoint
operator, 240, 244, 346
space, see dual space
Adjoint operator, 188
Algebra
division, 310
simple, 367
Analytic function, 267, 300, 325
Annihilator, 237
Approximate eigenvalue, 314, 355
Approximation of identity, 163
Archimedean property, 10, 30
Arzela-Ascoli theorem, 83
Ascending sequence of eigenspaces, 318
Auto-correlation, 168
Automorphism, 282, 284, 298, 347
Axiom of choice, 10, 31
B
Baire, 47
category theorem, 46, 221, 247
Ball, 10, 16, 66, 99, 136, 178
closed, 23, 251
Banach, 106
algebra, 277
commutative, 281, 311, 339, 348
morphism, 282
semi-simple, 331
ﬁxed point theorem, 51
space, 105, 122, 221
Banach-Alaoglu theorem, 251
Banach-Mazur theorem, 225
Banach-Steinhaus’s theorem, 246
Basis, 91
dual, 187, 225, 226, 341
Hamel, 209
orthonormal, 201, 355, 361
Schauder, 110, 118
Bertrand’s convergence test, 112
Bessel
functions, 208
inequality, 203
Binomial theorem, 303
Bolzano-Weierstrass property, 76
Boundary, 17, 58, 100, 297
Bounded map, see operator, continuous
Bounded set, 65, 101, 104
totally, 67, 104, 136
Bounded variation, 164
C
C≤-algebra, 345
commutative, 375
Cantor
nested set theorem, 77
set, 24, 25, 77
Cauchy, 271
convergence test, 111
inequality, 98, 173
integral formula, 272
residue theorem, 271
sequence, 38, 49, 67, 68
theorem, 269
Cauchy-Hadamard’s theorem, 289
Cauchy-Riemann equations, 268
Cauchy-Schwarz inequality, 173, 357
Cayley transformation, 352
Cayley-Hamilton theorem, 325
Center of an algebra, 281, 347
Centralizer, 286
J. Muscat, Functional Analysis, DOI: 10.1007/978-3-319-06728-5,
415
© Springer International Publishing Switzerland 2014

416
Index
Cesaro sum, 113
Chain, 92
Character, 282, 334
Characteristic
function, 35
polynomial, 307
Chebyshev polynomials, 208
Closed graph theorem, 223
Closed range theorem, 242
Closed set, 22, 29, 45, 67, 71
Closure, 23, 69, 102, 286
Codimension, 133
Coercive operator, 360
Commutant, 286
Commutative
Banach algebra, 281, 311, 339, 348
C≤-algebra, 377
Commutator, 282, 338
Compact
operator, 226, 321, 346, 369, 383
set, 70, 74, 104, 137, 309
Complementary subspace, 180, 224
Complete metric space, 37, 40
Completeness of
c, 141
CN, 135
C(X, Y), 78
L1, 159
ℓ1, 144
ℓ2, 147
L∀, 157
ℓ∀, 140
ℓp, 153
Completion, 43, 49, 107, 174, 238
Complex numbers, 14, 48, 346
Complexiﬁcation, 217
Component, 63, 297
Condition number, 130
Conformal, 199
Conjugate
gradient algorithm, 218
space, see dual space
Connected set, 57, 105
component, 63
path-connected, 64, 104, 296
Continuity, 31
of inner product, 173
of norm, 101
Continuous
functions, 78, 106, 278, 346
spectrum, 313
Contraction map, 50, 51
Convergence, 27, 31
absolute, 108, 109, 203, 289
in norm, 246
linear, 29
pointwise, 78, 246
quadratic, 29
strong, 246
uniform, 80
weak, 246, 248
weak*, 248
Convex set, 91, 99, 179, 357
Convolution, 121, 146, 164, 278, 341
Coset, 131, 183
Cross-correlation, 168
Curve, 259
D
Deconvolution, 196
Delta function, 121
Dense set, 25, 33, 53, 162, 182
Descending sequence of spaces, 318
Determinant, 296
Diameter of a set, 65
Differentiation, 122, 257, 278, 291
product rule, 286
Dimension, 91
Direct sum, 91
Dirichlet kernel, 248
Disconnected set, 57
totally, 57
Discrete metric space, 15, 22, 68, 76
Distance, 13, 33
between sets, 24
inherited, 15
Division algebra, 310
Divisor of zero, 279, 313, 347
topological, 297, 313, 347
Dot product, see inner product
Dual
basis, 187, 225, 226, 341
operator, see adjoint operator
space, 115, 122, 185, 231
double dual, 238, 245
E
Eigenspace, 313
Eigenvalue, 313, 322, 329, 354
approximate, 314, 355
Eigenvector, 313, 318, 354
Elliptic operator, 360
Embedding, 128, 238, 280, 283, 389
Equicontinuous function, 83

Index
417
Ergodic Theorem, 355
Euclidean distance, 14
Euclidean space, 14, 48, 69, 94, 135, 172,
174, 249, 278, 287, 341, 346
Exponential function, 342
Extension of a
functional, see Hahn-Banach theorem
uniformly continuous function, 49
Exterior, 17
point, 17
F
Factor space, see quotient space
Fibonacci sequence, 168
Filter, 165
Finite-dimensional vector space, 91, 136
Fixed point, 51
Fourier, 206
series, 165, 205
transform, 121, 168, 242, 342, 378
Fréchet, 14
Fredholm, 320
alternative, 320
operator, 229
Frequency-time orthonormal bases, 211
Function
composition of, 33, 49
continuous, 60, 66, 72
contraction, 50, 51
equicontinuous, 83
equivalence or bi-Lipschitz, 50
inverse of, 35
Lipschitz, 50, 67, 85, 116
open, 35
spaces, 96, 154, 172, 205, 335
Functional, 115
Functional calculus, 325, 375
Fundamental sequence, see Cauchy seq.
Fundamental theorem of
algebra, 310
calculus, 262
G
Gauss’s convergence test, 112
Gauss-Seidel algorithm, 127
Gaussian quadrature, 215
Gelfand, 337
transform, 337, 376
Gelfand-Mazur theorem, 310
Gelfand-Naimark theorem, 389
Generalized nilpotent, see quasinilpotent
Generating functions, 341
Gershgorin’s theorem, 315
Gram matrix, 182
Gram-Schmidt orthogonalization, 201
Green’s function, 214
Group algebra, 286, 287, 346
H
Haar basis, 212
Hadamard matrices, 219, 256
Hahn-Banach theorem, 232, 236
Hamel basis, 209
Hamming distance, 15
Hausdorff, 28
maximality principle, 92, 202, 234, 281
Hausdorff-Toeplitz theorem, 357
Heine-Borel theorem, 74
Hermite functions, 207
Hermitian operator, see self-adjoint operator
Hilbert, 174
cube, 148
space, 174, 369, 389
Hilbert-Schmidt operator, 368
Hölder’s inequality, 151, 161
Holomorphic function, see analytic function
Homeomorphism, 34, 64
Homomorphism, see morphism
I
Ideal, 280, 333, 366, 369, 391
Idempotent, 285, 330, 353
Ill-conditioned equation, 130
Image
of an operator, 116
reconstruction, 197
Imaginary part, 349
Index
of a Fredholm operator, 229
theorem, 229
Inner product, 171, 369
Integers, 14, 22, 57
Integration, 121, 268
by parts, 265
change of variable, 265
Interior, 17
Intermediate value theorem, 60
Interval, 59
Inverse problem, 191
Invertible elements, group of, 295
Involution, 345
Isolated point, 20
Isometry, 50, 186, 210, 318, 376, 382, 384
partial, 385

418
Index
Isomorphism, 34, 128, 189, 222, 282, 340,
377
J
Jacobi algorithm, 127
Jacobson radical, 331, 339
Jordan canonical form, 323
JPEG, 217
K
Kernel, 282, 338
Dirichlet, 248
of an operator, 116
Kummer’s convergence test, 112
Kuratowski, 58
L
L’Hopital’s rule, 261
Laguerre functions, 207
Laplace transform, 342
Laurent series, 301
Least squares approximation, 182
Lebesgue, 160
Legendre polynomials, 206
Leibniz test, 109
Liminf, 31
Limit, 27
Limit point, 21
Limsup, 31
Linear transformation, 115, 258
Linearly independent vectors, 90, 313
Liouville’s theorem, 301
Lipschitz function, 50, 67, 85, 116
Locally connected, 64
Logarithm function, 328
M
Map, see function
Matrix, 117, 124, 172, 190, 283, 285, 307,
341, 366, 372
Maximum modulus principle, 272
Mean value theorem, 263
Metric, see distance
Metric space, 13
completion of, 43, 49
discrete, 68, 76
equivalent, 50, 67
isometric, 50
separable, 53, 68, 85
Minimal polynomial, 325, 329
Minkowski, 151
inequality, 151, 164
semi-norm, 101
Möbius transformation, 352
Morphism, 282, 338, 345, 382
Multiplication operator, 143, 199, 245, 315,
325, 352, 355
N
Natural numbers, 14
Neighborhood, 17
Newton-Raphson algorithm, 265
Nilpotent, 285, 330, 331, 347
generalized, see quasinilpotent
Norm, 94
completion, 107, 174, 238
equivalent, 97
Normal element, 348, 353
Nowhere dense set, 25, 46
Nullity, 126
Numerical range, 356
O
Open
ball, see ball
cover, 70
mapping theorem, 221
set, 16
Operator
compact, 226, 321, 346, 369, 383
continuous, 115, 258
integral, 119, 127, 228
multiplication, 119, 143, 199, 245, 315,
325
multiplier, 352, 355
shift, 118, 129, 146, 190, 231, 242, 246,
299, 314, 317, 324, 330, 352, 359
trace-class, 373, 391
Orthogonal, 172
Orthonormal basis, 201, 209, 355, 361
P
Parallelogram law, 175
Parseval’s identity, 202
Partial isometry, 385
Path, 61
Path-connected set, 64, 104, 296
Perpendicular, see orthogonal
Point spectrum, 313
Pointwise convergence, 78, 246
Polar decomposition, 384

Index
419
Polarization identity, 175, 346
Polynomial, 35, 80, 105, 137, 183, 310
Positive functional, 384
Power series, 288, 347
Power spectrum, 167, 168
Pre-annihilator, 237
Product space
Banach algebras, 279
Banach spaces, 106
C≤-algebras, 346
metric spaces, 15
normed spaces, 97
vector spaces, 90
Projection, 130, 224, 231, 330, 355, 363, 383
orthogonal, 180
Pseudo-distance, 26
Pythagoras’ theorem, 172, 202
Q
QR decomposition, 218
Quadratic form, 357
Quasinilpotent, 331, 339, 347, 351
Quotient space, 131
R
Raabe’s convergence test, 112
Radical, 331, 339
Radius of convergence, 289
Rank, 126, 227, 366
Ratio convergence test, 111
Rational numbers, 14, 40, 46, 61
Rayleigh coefﬁcient, 356
Real numbers, 14, 22, 59
construction of, 40
Real part, 349
Reﬂexive space, 238
Regression, 193
Residual spectrum, 313, 354
Resolvent set, 307
Riesz, 187
lemma, 321
map, 186
theorem, 322
Rodrigues’ formula, 208
Root convergence test, 110
Rouché’s theorem, 273
S
Scalar multiplication, 89, 101
Schauder, 110
basis, 110, 118
Schrödinger equation, 342
Schur’s test, 124
Schur’s theorem, 255
Schwarz inequality, 173
Self-adjoint element, 348
positive, 378
Semi-metric, see pseudo-distance
Semi-simple Banach algebra, 331
Separable, 53, 68, 85, 202
Separating hyperplane theorem, 235
Sequence
asymptotic, 38
Cauchy, 38, 49, 67, 68
convergent, 27
divergent, 27
increasing, 39
rearrangement, 29
Sequence spaces, 95, 139, 172, 178, 204,
278, 335, 341, 346
Series, 108
convergence tests, 110
power, 288
rearrangement, 109
Shift operator, 129, 146, 190, 231, 242, 246,
299, 314, 317, 324, 330, 352, 359
Simple algebra, 367
Singular value decomposition, 362
Spanning vectors, 90
Spectral
mapping theorem, 328
radius, 288, 309
theorem for compact normal operators,
361
Spectrum, 282, 307, 322
continuous, 313
normal element, 377
of an algebra, 334
of an operator, 312
point, 313
residual, 313, 354
Sphere, 36, 100
Spherical harmonics, 208
Spline, 80, 162
Square root operator, 380
Standard deviation, 357
State space, 334, 378
Stone, 83
Stone-Weierstrass theorem, 80, 83
Strong convergence, 246
Subalgebra, 280
Subsequence, 29, 38
Sylvester’s inequality, 126

420
Index
T
Tangent, 259
Taylor
series, 300
theorem, 264
Tikhonov regularization, 194, 365
Tomography, 197
Topological
divisor of zero, see divisor of zero, topo-
logical
nilpotent, see quasinilpotent
space, 26
Total set, see orthonormal basis
Totally bounded set, 67, 104, 136, 226
Totally disconnected, 64
Trace, 368
Trace-class operator, 373, 391
Translation, 352
Transpose, see adjoint operator
Triangle inequality, 13
Trotter formula, 294
U
Uncertainty principle, 357
Uniform
bounded theorem, 246
convergence, 80
Uniformly continuous function, 48, 68, 73
Unitary
element, 348, 349
operator, 189
V
Vector addition, 101
Vector space, 89
addition, 89
linear subspace, 90
Volterra, 122
operator, 121, 199, 318, 340, 373
Von Neumann, 390
W
Walsh basis, 219
Wavelet bases, 212
Weak convergence, 246, 248
Weakly
bounded set, 250
closed set, 253
Weierstrass, 83
M-test, 113
Weight, 113, 172
Well-ordering principle, 92
Wiener deconvolution, 196
Wiener’s theorem, 341
Wiener-Khinchin theorem, 168
Windowed Fourier bases, 211
Z
Zorn’s lemma, 92

