MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks
Animesh Karnewar
TomTom
animesh.karnewar@tomtom.com
Oliver Wang
Adobe Research
owang@adobe.com
CelebA-HQ
FFHQ
Figure 1: Results of our proposed MSG-GAN technique where the generator synthesizes images at all resolutions simulta-
neously and gradients ﬂow directly to all levels from a single discriminator. The ﬁrst column has a resolution of 4x4 which
increases towards the right reaching the ﬁnal output resolution of 1024x1024. Best viewed zoomed in on screen.
Abstract
While Generative Adversarial Networks (GANs) have
seen huge successes in image synthesis tasks, they are no-
toriously difﬁcult to adapt to different datasets, in part due
to instability during training and sensitivity to hyperparam-
eters. One commonly accepted reason for this instability is
that gradients passing from the discriminator to the gener-
ator become uninformative when there isn’t enough over-
lap in the supports of the real and fake distributions. In
this work, we propose the Multi-Scale Gradient Genera-
tive Adversarial Network (MSG-GAN), a simple but effec-
tive technique for addressing this by allowing the ﬂow of
gradients from the discriminator to the generator at multi-
ple scales. This technique provides a stable approach for
high resolution image synthesis, and serves as an alterna-
tive to the commonly used progressive growing technique.
We show that MSG-GAN converges stably on a variety of
image datasets of different sizes, resolutions and domains,
as well as different types of loss functions and architectures,
all with the same set of ﬁxed hyperparameters. When com-
pared to state-of-the-art GANs, our approach matches or
exceeds the performance in most of the cases we tried.
1. Introduction
Since their introduction by Goodfellow et al. [10], Gen-
erative Adversarial Networks (GANs) have become the de
facto standard for high quality image synthesis. The suc-
cess of GANs comes from the fact that they do not require
manually designed loss functions for optimization, and can
therefore learn to generate complex data distributions with-
out the need to be able to explicitly deﬁne them. While
ﬂow-based models such as [6, 7, 27, 18] and autoregressive
models such as [32, 31, 29] allow training generative mod-
els directly using Maximum Likelihood Estimation (explic-
itly and implicitly respectively), the ﬁdelity of the generated
images has not yet been able to match that of the state-of-
the-art GAN models [15, 16, 17, 3]. However, GAN train-
ing suffers from two prominent problems: (1) mode col-
lapse and (2) training instability.
The problem of mode collapse occurs when the genera-
tor network is only able to capture a subset of the variance
present in the data distribution. Although numerous works
[28, 41, 15, 21] have been proposed to address this problem,
it remains an open area of study. In this work, however, we
address the problem of training instability. This is a funda-
mental issue with GANs, and has been widely reported by
arXiv:1903.06048v4  [cs.CV]  12 Jun 2020

Figure 2: Architecture of MSG-GAN, shown here on the base model proposed in ProGANs [15]. Our architecture includes
connections from the intermediate layers of the generator to the intermediate layers of the discriminator. Multi-scale im-
ages sent to the discriminator are concatenated with the corresponding activation volumes obtained from the main path of
convolutional layers followed by a combine function (shown in yellow).
previous works [28, 22, 2, 11, 19, 33, 14, 15, 37, 25]. We
propose a method to address training instability for the task
of image generation by investigating how gradients at mul-
tiple scales can be used to generate high resolution images
(typically more challenging due to the data dimensionality)
without relying on previous greedy approaches, such as the
progressive growing technique [15, 16]. MSG-GAN allows
the discriminator to look at not only the ﬁnal output (high-
est resolution) of the generator, but also at the outputs of
the intermediate layers (Fig.
2). As a result, the discrim-
inator becomes a function of multiple scale outputs of the
generator and importantly, passes gradients to all the scales
simultaneously (more details in section 1.1 and section 2).
Furthermore, our method is robust to different loss func-
tions (we show results on WGAN-GP and Non-saturating
GAN loss with 1-sided gradient penalty), datasets (we
demonstrate results on a wide range of commonly used
datasets and a newly created Indian Celebs dataset), and ar-
chitectures (we integrate the MSG approach with both Pro-
GANs and StyleGAN base architectures). Much like pro-
gressive growing [15], we note that multi-scale gradients
account for a considerable improvement in FID score over
the vanilla DCGAN architecture.
However, our method
achieves better performance with comparable training time
to state-of-the-art methods on most existing datasets with-
out requiring the extra hyperparameters that progressive
growing introduces, such as training schedules and learning
rates for different generation stages (resolutions). This ro-
bustness allows the MSG-GAN approach to be easily used
“out-of-the-box” on new datasets. We also show the impor-
tance of the multi-scale connections on multiple generation
stages (coarse, medium, and ﬁne), through ablation experi-
ments on the high resolution FFHQ dataset.
In summary, we present the following contributions.
First, we introduce a multiscale gradient technique for im-
age synthesis that improves the stability of training as de-
ﬁned in prior work.
Second, we show that we can ro-
bustly generate high quality samples on a number of com-
monly used datasets, including CIFAR10, Oxford102 ﬂow-
ers, CelebA-HQ, LSUN Churches, Flickr Faces HQ and our
new Indian Celebs all with the same ﬁxed hyperparameters.
This makes our method easy to use in practice.
1.1. Motivation
Arjovsky and Bottou [1] pointed out that one of the
reasons for the training instability of GANs is due to the
passage of random (uninformative) gradients from the dis-
criminator to the generator when there is insubstantial over-
lap between the supports of the real and fake distributions.
Since the inception of GANs, numerous solutions have been
proposed to this problem.
One early example proposes
adding instance noise to the real and the fake images so
that the supports minimally overlap [1, 30]. More recently,
Peng et al. [25] proposed a mutual information bottleneck

between input images and the discriminator’s deepest rep-
resentation of those input images called the variational dis-
criminator bottleneck (VDB) [25], and Karras et al. [15]
proposed a progressive growing technique to add continu-
ally increasing resolution layers. The VDB solution forces
the discriminator to focus only on the most discerning fea-
tures of the images for classiﬁcation, which can be viewed
as an adaptive variant of instance noise. Our work is orthog-
onal to the VDB technique, and we leave an investigation
into a combination of MSG-GAN and VDB to future work.
The progressive growing technique tackles the instability
problem by training the GAN layer-by-layer by gradually
doubling the operating resolution of the generated images.
Whenever a new layer is added to the training it is slowly
faded in such that the learning of the previous layers are
retained. Intuitively, this technique helps with the support
overlap problem because it ﬁrst achieves a good distribution
match on lower resolutions, where the data dimensionality
is lower, and then partially-initializes (with substantial sup-
port overlap between real and fake distributions) higher res-
olution training with these previously trained weights, fo-
cusing on learning ﬁner details.
While this approach is able to generate state-of-the-art
results, it can be hard to train, due to the addition of hyper-
parameters to be tuned per resolution, including different
iteration counts, learning rates (which can be different for
the Generator and Discriminator [12]) and the fade-in itera-
tions. In addition, a concurrent submission [17] discovered
that it leads to phase artifacts where certain generated fea-
tures are attached to speciﬁc spatial locations. Hence our
main motivation lies in addressing these problems by pro-
viding a simpler alternative that leads to high quality results
and stable training.
Although the current state-of-the-art in class conditional
image generation on the Imagenet dataset, i.e. BigGAN
[4], doesn’t employ multi-scale image generation, note that
the highest resolution they operate on is 512x512. All
high resolution state-of-the-art methods [15, 16, 17, 34, 40]
use some or the other form of multi-scale image synthe-
sis.
Multi-scale image generation is a well established
technique, with methods existing well before deep net-
works became popular for this task [20, 35].
More re-
cently, a number of GAN-based methods break the pro-
cess of high resolution image synthesis into smaller sub-
tasks [36, 39, 38, 8, 9, 15, 34, 40].
For example, LR-
GAN [36] uses separate generators for synthesizing the
background, foreground and compositor masks for the ﬁ-
nal image. Works such as GMAN and StackGAN employ a
single generator and multiple discriminators for variation in
teaching and multi-scale generation respectively [8, 39, 38].
MAD-GAN [9], instead uses multiple generators to address
mode-collapse by training a multi-agent setup in such a way
that different generators capture different modalities in the
training dataset. LapGAN [5] models the difference be-
tween the generated multi-scale components of a Laplacian
pyramid of the images using a single generator and mul-
tiple discriminators for different scales. Pix2PixHD [34]
uses three architecturally similar discriminators acting upon
three different resolutions of the images obtained by down-
sampling the real and the generated images.
Our proposed method draws architectural inspiration
from all these works and builds upon their teachings and
ideologies, but has some key differences. In MSG-GAN,
we use a single discriminator and a single generator with
multi-scale connections, which allows for the gradients to
ﬂow at multiple resolutions simultaneously. There are sev-
eral advantages (driven largely by the simplicity) of the pro-
posed approach. If multiple discriminators are used at each
resolution [39, 38, 5, 40, 34], the total parameters grow ex-
ponentially across scales, as repeated downsampling lay-
ers are needed, whereas in MSG-GAN the relationship is
linear. In addition, multiple discriminators with different
effective ﬁelds [34, 40] are not able to share information
across scales, which could make the task easier. Besides
having fewer parameters and design choices required, our
approach also avoids the need for an explicit color consis-
tency regularization term across images generated at multi-
ple scales, which was necessary, e.g. in StackGAN [38].
2. Multi-Scale Gradient GAN
We conduct experiments with the MSG-GAN framework
applied to two base architectures, ProGANs [15] and Style-
GAN [16]. We call these two methods MSG-ProGAN and
MSG-StyleGAN respectively. Despite the name, there is
no progressive growing used in any of the MSG variants,
and we note that ProGANs without progressive growing is
essentially the DCGAN [26] architecture. Figure 2 shows
an overview of our MSG-ProGAN architecture, which we
deﬁne in more detail in this section, and include the MSG-
StyleGAN model details in the supplemental material.
Let the initial block of the generator function ggen be de-
ﬁned as ggen : Z 7→Abegin, such that sets Z and Abegin are
respectively deﬁned as Z = R512, where z ∼N(0, I) such
that z ∈Z and Abegin = R4×4×512 contains [4x4x512]
dimensional activations. Let gi be a generic function which
acts as the basic generator block, which in our implemen-
tation consists of an upsampling operation followed by two
conv layers.
gi : Ai−1 7→Ai
(1)
where, Ai = R2i+2×2i+2×ci
(2)
and, i ∈N; A0 = Abegin
(3)
where ci is the number of channels in the ith intermediate
activations of the generator. We provide the sizes of ci in
all layers in the supplementary material. The full generator

GEN (z) then follows the standard format, and can be de-
ﬁned as a sequence of compositions of k such g functions
followed by a ﬁnal composition with ggen:
y′ = GEN (z) = gk ◦gk−1 ◦...gi ◦...g1 ◦ggen(z).
(4)
We now deﬁne the function r which generates the output
at different stages of the generator (red blocks in Fig. 2),
where the output corresponds to different downsampled ver-
sions of the ﬁnal output image. We model r simply as a
(1x1) convolution which converts the intermediate convo-
lutional activation volume into images.
ri : Ai 7→Oi
(5)
where, Oi = R2i+2×2i+2×3
[0−1]
(6)
hence, ri(gi(z)) = ri(ai) = oi
(7)
where, ai ∈Ai and oi ∈Oi
(8)
In other words, oi is an image synthesized from the output
of the ith intermediate layer of the generator ai. Similar to
the idea behind progressive growing [15], r can be viewed
as a regularizer, requiring that the learned feature maps are
able to be projected directly into RGB space.
Now we move on to deﬁning the discriminator. Because
the discriminator’s ﬁnal critic loss is a function of not only
the ﬁnal output of the generator y′, but also the intermediate
outputs oi, gradients can ﬂow from the intermediate layers
of the discriminator to the intermediate layers of the gen-
erator. We denote all the components of the discriminator
function with the letter d. We name the ﬁnal layer of the dis-
criminator (which provides the critic score) dcritic(z′), and
the function which deﬁnes the ﬁrst layer of the discrimina-
tor d0(y) or d0(y′), taking the real image y (true sample) or
the highest resolution synthesized image y′ (fake sample) as
the input. Similarly, let dj represent the intermediate layer
function of the discriminator. Note that i and j are always
related to each other as j = k−i. Thus, the output activation
volume a′
j of any jth intermediate layer of the discriminator
is deﬁned as:
a′
j = dj(φ(ok−j, a′
j−1))
(9)
= dj(φ(oi, a′
j−1)),
(10)
where φ is a function used to combine the output oi of the
(i)th intermediate layer of the generator (or correspond-
ingly downsampled version of the highest resolution real
image y) with the corresponding output of the (j −1)th in-
termediate layer in the discriminator. In our experiments,
we experimented with three different variants of this com-
bine function:
φsimple(x1, x2) = [x1; x2]
(11)
φlin cat(x1, x2) = [r′(x1); x2]
(12)
φcat lin(x1, x2) = r′([x1; x2])
(13)
where, r′ is yet another (1x1) convolution operation similar
to r and [; ] is a simple channelwise concatenation operation.
We compare these different combine functions in Sec 4.
The ﬁnal discriminator function is then deﬁned as:
DIS(y′, o0, o1, ...oi, ...ok−1) =
(14)
dcritic ◦dk(., o0) ◦dk−1(., o1) ◦...dj(., oi) ◦...d0(y′)
(15)
We experimented with two different loss functions for
the dcritic function namely, WGAN-GP [11] which was
used by ProGAN [15] and Non-saturating GAN loss with
1-sided GP [10, 23] which was used by StyleGAN [16].
Please note that since the discriminator is now a function of
multiple input images generated by the generator, we mod-
iﬁed the gradient penalty to be the average of the penalties
over each input.
3. Experiments
While evaluating the quality of GAN generated images is
not a trivial task, the most commonly used metrics today are
the Inception Score (IS, higher is better) [28] and Fr´echet
Inception Distance (FID, lower is better) [12]. In order to
compare our results with the previous works, we use the IS
for the CIFAR10 experiments and the FID for the rest of the
experiments, and report the “number of real images shown”
as done in prior work [15, 16].
New Indian Celebs Dataset
In addition to existing
datasets, we also collect a new dataset consisting of Indian
celebrities. To this end, we collected the images using a
process similar to CelebA-HQ. First, we downloaded im-
ages for Indian celebrities by scraping the web for related
search queries. Then, we detected faces using an off the
shelf face-detector and cropped and resized all the images
to 256x256. Finally, we manually cleaned the images by
ﬁltering out low-quality, erroneous, and low-light images.
In the end, the dataset contained only 3K samples, an order
of magnitude less than CelebA-HQ.
3.1. Implementation Details
We evaluate our method on a variety of datasets of dif-
ferent resolutions and sizes (number of images); CIFAR10
(60K images at 32x32 resolution); Oxford ﬂowers (8K
images at 256x256), LSUN churches (126K images at
256x256), Indian Celebs (3K images at 256x256 res-
olution), CelebA-HQ (30K images at 1024x1024) and
FFHQ (70K images at 1024x1024 resolution).
For each dataset, we use the same initial latent dimen-
sionality of 512, drawn from a standard normal distribution
N(0, I) followed by hypersphere normalization [15]. For
all experiments, we use the same hyperparameter settings
for MSG-ProGAN and MSG-StyleGAN (lr=0.003), with

(a) LSUN churches
(b) Indian Celebs
(c) Oxford Flowers
Figure 3: Random, uncurated samples generated by MSG-StyleGAN on different mid-level resolution (256x256) datasets.
Our approach generates high quality results across all datasets with the same hyperparameters. Best viewed zoomed in on
screen.
Dataset
Size
Method
# Real Images
GPUs used
Training Time
FID (↓)
Oxford Flowers (256x256)
8K
ProGANs∗
10M
1 V100-32GB
104 hrs
60.40
MSG-ProGAN
1.7M
1 V100-32GB
44 hrs
28.27
StyleGAN∗
7.2M
2 V100-32GB
33 hrs
64.70
MSG-StyleGAN
1.6M
2 V100-32GB
16 hrs
19.60
Indian Celebs (256x256)
3K
ProGANs∗
9M
2 V100-32GB
37 hrs
67.49
MSG-ProGAN
2M
2 V100-32GB
34 hrs
36.72
StyleGAN∗
6M
4 V100-32GB
18 hrs
61.22
MSG-StyleGAN
1M
4 V100-32GB
7 hrs
28.44
LSUN Churches (256x256)
126K
StyleGAN∗
25M
8 V100-16GB
47 hrs
6.58
MSG-StyleGAN
24M
8 V100-16GB
50 hrs
5.2
Table 1: Experiments on mid-level resolution (i.e. 256x256) datasets. We use author provided scores where possible, and
otherwise train models with the ofﬁcial code and recommended hyperparameters (denoted “∗”)
the only differences being the number of upsampling lay-
ers (fewer for lower resolution datasets).
All models were trained with RMSprop (lr=0.003) for
generator and discriminator. We initialize parameters ac-
cording to the standard normal N(0, I) distribution.
To
match previously published work, StyleGAN and MSG-
StyleGAN models were trained with Non-saturating GAN
loss with 1-sided GP while ProGANs and MSG-ProGAN
models were trained with the WGAN-GP loss function.
We also extend the MinBatchStdDev technique [15, 16],
where the average standard deviation of a batch of activa-
tions is fed to the discriminator to improve sample diversity,
to our multiscale setup. To do this, we add a separate Min-
BatchStdDev layer at the beginning of each block in the
discriminator. This way, the discriminator obtains batch-
statistics of the generated samples along with the straight-
path activations at each scale, and can detect some degree
of mode collapse by the generator.
When we trained the models ourselves, we report train-
ing time and GPUs used. We use the same machines for
corresponding set of experiments so that direct training time
comparisons can be made. Please note that the variation
in numbers of real images shown and training time is be-
cause, as is common practice, we report the best FID score
obtained in a ﬁxed number of iterations, and the time that
it took achieve that score.
All the code and the trained
models required for reproducing our work are made avail-
able for research purposes at https://github.com/
akanimax/msg-stylegan-tf.
3.2. Results
Quality
Table 1 shows quantitative results of our method
on various mid-level resolutions datasets. Both our MSG-
ProGAN and MSG-StyleGAN models achieve better FID

(a) CelebA-HQ
(b) FFHQ
Figure 4: Random, uncurated samples generated by MSG-StyleGAN on high resolution (1024x1024) datasets. Best viewed
zoomed in on screen.
Dataset
Size
Method
# Real Images
GPU Used
Training Time
FID (↓)
CelebA-HQ (1024x1024)
30K
ProGANs [16]
12M
-
-
7.79
MSG-ProGAN
3.2M
8 V100-16GB
1.5 days
8.02
StyleGAN [16]
25M
-
-
5.17
MSG-StyleGAN
11M
8 V100-16GB
4 days
6.37
FFHQ (1024x1024)
70K
ProGANs∗
12M
4 V100-32GB
5.5 days
9.49
ProGANs [15]
12M
-
-
8.04
MSG-ProGAN
6M
4 V100-32GB
6 days
8.36
StyleGAN∗
25M
4 V100-32GB
6 days
4.47
StyleGAN [16]
25M
-
-
4.40
MSG-StyleGAN
9.6M
4 V100-32GB
6 days
5.8
Table 2: Experiments on high resolution (1024x1024) datasets. We use author provided scores where possible, and other-
wise train models with the ofﬁcial code and recommended hyperparameters (denoted “∗”).
scores than the respective baselines of ProGANs and Style-
GAN on the (256x256) resolution datasets of Oxford
Flowers, LSUN Churches and Indian Celebs. While each
iteration of MSG-GAN is slower than the initial lower reso-
lution iterations of progressive growing, due to all layers be-
ing trained together, MSG-GAN tends to converge in fewer
iterations, requiring fewer total hours of GPU training time
to achieve these scores. Figure 3 shows random samples
generated on these datasets for qualitative evaluation.
For high-resolution experiments (Table 2), the MSG-
ProGAN model trains in comparable amount of time and
gets similar scores on the CelebA-HQ and the FFHQ
datasets (8.02 vs 7.79) and (8.36 vs 8.04) respectively. We
note a small difference in the author reported scores and
what we were able to achieve with the author provided code.
This could be due to subtle hardware differences or variance
between runs. Our MSG-StyleGAN model was unable to
beat the FID score of StyleGAN on the CelebA-HQ dataset
(6.37 vs 5.17) and the FFHQ dataset (5.8 vs 4.40). We dis-
cuss some hypotheses for why this might be in Sec 4, but
note that our method does have other advantages, namely
that it seems to be easier to generalize to different datasets
as shown in our other experiments. Also, our generated im-
ages do not show any traces of the phase artifacts [17] which
are prominently visible in progressively grown GANs.
Stability during training
To compare the stability of
MSG-ProGAN with ProGANs during training, we measure
the changes in the generated samples for the same ﬁxed la-
tent points as iterations progress (on CelebA-HQ dataset).
This method was introduced by [37] as a way to measure
stability during training, which we quantify by calculat-

Figure 5: During training, all the layers in the MSG-GAN synchronize across the generated resolutions fairly early in the
training and subsequently improve the quality of the generated images at all scales simultaneously. Throughout the training
the generator makes only minimal incremental improvements to the images generated from ﬁxed latent points.
Figure 6: Image stability during training. These plots show the MSE between images generated from the same latent code at
the beginning of sequential epochs (averaged over 36 latent samples) on the CelebA-HQ dataset. MSG-ProGAN converges
stably over time while ProGANs [15] continues to vary signiﬁcantly across epochs.
ing the mean squared error between two consecutive sam-
ples. Figure 6 shows that while ProGANs tends towards
convergence (making less changes) for lower resolutions
only, MSG-ProGAN shows the same convergence trait for
all the resolutions. The training epochs for the ProGANs
take place in sequence over each resolution, whereas for the
MSG-ProGAN they are simultaneous (Fig. 5). While not
necessary for generating good results, methods with high
stability can be advantageous in that it is easier to get a rea-
sonable estimate for how the ﬁnal result will look by vi-
sualizing snapshots during training, which can help when
training jobs take on the order of days to weeks.
Robustness to learning rate
It has been observed by
prior work [28, 14, 24, 23] and also our experience, that
convergence of GANs during training is very heavily de-
pendant on the choice of hyperparameters, in particular,
learning rate. To validate the robustness of MSG-ProGAN,

Method
# Real Images
Learning rate
IS (↑)
Real Images
-
-
11.34
MSG-ProGAN
12M
0.003
8.63
MSG-ProGAN
12M
0.001
8.24
MSG-ProGAN
12M
0.005
8.33
MSG-ProGAN
12M
0.01
7.92
Table 3: Robustness to learning rate on CIFAR-10. We
see that our approach converges to similar IS scores over
a range of learning rates.
Level of Multi-scale connections
FID (↓)
No connections (DC-GAN)
14.20
Coarse Only
10.84
Middle Only
9.17
Fine Only
9.74
All (MSG-ProGAN)
8.36
ProGAN∗
9.49
Table 4:
Ablation experiments for varying degrees of
multiscale gradient connections on the high resolution
(1024x1024) FFHQ dataset. Coarse contains connections
at (4x4) and (8x8), middle at (16x16) and (32x32); and
ﬁne at (64x64) till (1024x1024).
Method
Combine function
FID (↓)
MSG-ProGAN
φlin cat
11.88
φcat lin
9.63
φsimple
8.36
MSG-StyleGAN
φsimple
6.46
φlin cat
6.12
φcat lin
5.80
Table 5: Experiments with different combine functions on
the high resolution (1024x1024) FFHQ dataset.
we trained our network with four different learning rates
(0.001, 0.003, 0.005 and 0.01) for the CIFAR-10 dataset
(Table. 3). We can see that all of our four models converge,
producing sensible images and similar inception scores,
even with large changes in learning rate. Robust training
schemes are signiﬁcant as they indicate how easily a method
can be generalized to unseen datasets.
4. Discussion
Ablation Studies
We performed two types of ablations
on the MSG-ProGAN architecture.
Table 4 summarizes
our experiments on applying ablated versions of the Multi-
Scale Gradients, where we only add subsets of the con-
nections from the generator to the discriminator at differ-
ent scales. We can see that adding multi-scale gradients at
any level to the ProGANs/DCGAN architecture improves
the FID score. Interestingly, adding only mid-level con-
nections performs slightly better than adding only coarse
or ﬁne-level connections, however the overall best perfor-
mance is achieved with the connections at all levels.
Table 5 presents our experiments with the different vari-
ants of the combine function φ on the MSG-ProGAN and
the MSG-StyleGAN architectures.
φsimple (Eq 11) per-
formed best on the MSG-ProGAN architecture while the
φcat lin (Eq 13) has the best FID score on the MSG-
StyleGAN architecture.
All results shown in this work
employ these respective combine functions. We can see
through these experiments that the combine function also
plays an important role in the generative performance of
the model, and it is possible that a more advanced combine
function such as multi-layer densenet or AdaIN [13] could
improve the results even further.
Limitations and Future Work
Our method is not with-
out limitations. We note that using progressive training, the
ﬁrst set of iterations at lower resolutions take place much
faster, whereas each iteration of MSG-GAN takes the same
amount of time. However, we observe that MSG-GAN re-
quires fewer total iterations to reach the same FID, and often
does so after a similar length of total training time.
In addition, because of our multi-scale modiﬁcation in
MSG-StyleGAN, our approach cannot take advantage of
the mixing regularization trick [16], where multiple latent
vectors are mixed and the resulting image is forced to be
realistic by the discriminator. This is done to allow the mix-
ing of different styles at different levels at test time, but also
improves overall quality. Interestingly, even though we do
not explicitly enforce mixing regularization, our method is
still able to generate plausible mixing results (see supple-
mentary material).
Conclusion
Although huge strides have been made to-
wards photo-realistic high resolution image synthesis [3, 16,
17], true photo-realism has yet to be achieved, especially
with regards to domains with substantial variance in appear-
ance. In this work, we presented the MSG-GAN technique
which contributes to these efforts with a simple approach
to enable high resolution multi-scale image generation with
GANs.
5. Acknowledgements
We would like to thank Alexia Jolicoeur-Martineau
(Ph.D. student at MILA) for her guidance over Relativism
in GANs and for proofreading the paper. Finally we ex-
tend special thanks to Michael Hoffman (Sr. Mgr. Software
Engineering, TomTom) for his support and motivation.

References
[1] Mart´ın Arjovsky and L´eon Bottou. Towards principled meth-
ods for training generative adversarial networks.
CoRR,
2017.
[2] Mart´ın Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein generative adversarial networks. In ICML, 2017.
[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthe-
sis. In International Conference on Learning Representa-
tions, 2019.
[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthe-
sis. In International Conference on Learning Representa-
tions, 2019.
[5] Emily L. Denton, Soumith Chintala, Arthur Szlam, and Rob
Fergus.
Deep generative image models using a laplacian
pyramid of adversarial networks. In NIPS, 2015.
[6] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE:
non-linear independent components estimation. CoRR, 2014.
[7] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
Density estimation using real NVP. CoRR, 2016.
[8] Ishan Durugkar,
Ian Gemp,
and Sridhar Mahadevan.
Generative multi-adversarial networks.
arXiv preprint
arXiv:1611.01673, 2016.
[9] Arnab Ghosh, Viveka Kulharia, Vinay P. Namboodiri,
Philip H.S. Torr, and Puneet K. Dokania. Multi-agent di-
verse generative adversarial networks. In CVPR, June 2018.
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems, 2014.
[11] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville.
Improved training of
wasserstein GANs. In Advances in Neural Information Pro-
cessing Systems, 2017.
[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. GANs trained by
a two time-scale update rule converge to a local nash equi-
librium. In Advances in Neural Information Processing Sys-
tems, 2017.
[13] Xun Huang and Serge J. Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. ICCV, 2017.
[14] Alexia Jolicoeur-Martineau. The relativistic discriminator: a
key element missing from standard GAN. In International
Conference on Learning Representations, 2019.
[15] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of GANs for improved quality, stability,
and variation. In ICLR, 2018.
[16] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR, pages 4401–4410, 2019.
[17] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan, 2019.
[18] Durk P Kingma and Prafulla Dhariwal. Glow: Generative
ﬂow with invertible 1x1 convolutions. In Advances in Neural
Information Processing Systems, 2018.
[19] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt
Kira. On convergence and stability of GANs. arXiv preprint
arXiv:1705.07215, 2017.
[20] Sylvain Lefebvre and Hugues Hoppe. Parallel controllable
texture synthesis. In ACM Transactions on Graphics (ToG),
volume 24. ACM, 2005.
[21] Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh.
PacGAN: The power of two samples in generative adversar-
ial networks. In Advances in Neural Information Processing
Systems, 2018.
[22] Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau,
and Zhen Wang. Multi-class generative adversarial networks
with the l2 loss function. ArXiv, abs/1611.04076, 2016.
[23] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
Which training methods for GANs do actually converge? In
ICML, 2018.
[24] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-
Dickstein. Unrolled generative adversarial networks, 2016.
[25] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel,
and Sergey Levine. Variational discriminator bottleneck: Im-
proving imitation learning, inverse RL, and GANs by con-
straining information ﬂow. In ICLR, 2019.
[26] Alec Radford, Luke Metz, and Soumith Chintala.
Un-
supervised representation learning with deep convolu-
tional generative adversarial networks.
arXiv preprint
arXiv:1511.06434, 2015.
[27] Danilo Jimenez Rezende and Shakir Mohamed. Variational
inference with normalizing ﬂows. In ICML, 2015.
[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training GANs. In Advances in neural information pro-
cessing systems, 2016.
[29] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P.
Kingma. PixelCNN++: A PixelCNN implementation with
discretized logistic mixture likelihood and other modiﬁca-
tions. In ICLR, 2017.
[30] Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wen-
zhe Shi, and Ferenc Husz´ar. Amortised MAP inference for
image super-resolution. ArXiv, abs/1610.04490, 2016.
[31] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,
Oriol Vinyals, Alex Graves, et al. Conditional image gen-
eration with pixelcnn decoders. In Advances in Neural In-
formation Processing Systems, 2016.
[32] A¨aron van den Oord,
Nal Kalchbrenner,
and Koray
Kavukcuoglu.
Pixel recurrent neural networks.
CoRR,
abs/1601.06759, 2016.
[33] Ruohan Wang, Antoine Cully, Hyung Jin Chang, and Yiannis
Demiris. Magan: Margin adaptation for generative adversar-
ial networks. ArXiv, abs/1704.03817, 2017.
[34] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2018.
[35] Yonatan Wexler, Eli Shechtman, and Michal Irani. Space-
time completion of video.
IEEE Transactions on Pattern
Analysis & Machine Intelligence, (3), 2007.

[36] Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi
Parikh. Lr-gan: Layered recursive generative adversarial net-
works for image generation. ICLR, 2017.
[37] Yasin Yazıcı, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui
Yap, Georgios Piliouras, and Vijay Chandrasekhar. The un-
usual effectiveness of averaging in GAN training. In Inter-
national Conference on Learning Representations, 2019.
[38] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xi-
aogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stack-
gan++: Realistic image synthesis with stacked generative ad-
versarial networks. arXiv: 1710.10916, 2017.
[39] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N. Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In ICCV, Oct 2017.
[40] Zizhao Zhang, Yuanpu Xie, and Lin Yang.
Photographic
text-to-image synthesis with a hierarchically-nested adver-
sarial network. In CVPR, 2018.
[41] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation. In Advances
in Neural Information Processing Systems, 2017.
6. Appendix
6.1. Architecture Details
MSG-ProGAN
Tables 6 and 7 provide the detailed con-
ﬁgurations of the generator and the discriminator of MSG-
ProGAN respectively. After every block in the generator,
a 1 x 1 conv layer is used to convert the output activa-
tion volume into an RGB image which is passed onto the
discriminator. On the discriminator’s side, these RGB im-
ages are combined with straight path activation volumes us-
ing the combine function φ. In case of φsimple, a simple
channelwise concatenation operation is used (see Table 7).
For the φlin cat variant of the combine function, a 1 x 1
conv layer is used to project the RGB images into activation
space which is then followed by channelwise concatenation
operation. The number of channels output by the 1 x 1
conv layer is equal to half of the output channels in that
block of the discriminator, e.g. for block 3 (see Table 7),
the output of the 1 x 1 conv layer is 32 x 256 x 256
and the output of φlin cat operation is 96 x 256 x 256
(32 + 64). Finally, for the φcat lin, the RGB images are
ﬁrst concatenated with the straight path activation volumes
followed by a 1 x 1 conv layer. The number of channels
output by this 1 x 1 conv layer is again equal to the preva-
lent number of channels in that block (e. g. 64 for block 3).
Model 1, Model 2 and Model 3 blocks of the generator
(Tab 6) are used to synthesize 32 x 32, 128 x 128 and
256 x 256 sized images respectively. And, after every
3 x 3 conv operation the feature vectors are normalized
according to the PixNorm [15] scheme (only for the gener-
ator).
Block
Operation
Act.
Output Shape
1.
Latent Vector
Norm
512 x 1 x 1
Conv 4 x 4
LReLU
512 x 4 x 4
Conv 3 x 3
LReLU
512 x 4 x 4
2.
Upsample
-
512 x 8 x 8
Conv 3 x 3
LReLU
512 x 8 x 8
Conv 3 x 3
LReLU
512 x 8 x 8
3.
Upsample
-
512 x 16 x 16
Conv 3 x 3
LReLU
512 x 16 x 16
Conv 3 x 3
LReLU
512 x 16 x 16
4.
Upsample
-
512 x 32 x 32
Conv 3 x 3
LReLU
512 x 32 x 32
Conv 3 x 3
LReLU
512 x 32 x 32
Model 1 ↑
5.
Upsample
-
512 x 64 x 64
Conv 3 x 3
LReLU
256 x 64 x 64
Conv 3 x 3
LReLU
256 x 64 x 64
6.
Upsample
-
256 x 128 x 128
Conv 3 x 3
LReLU
128 x 128 x 128
Conv 3 x 3
LReLU
128 x 128 x 128
Model 2 ↑
7.
Upsample
-
128 x 256 x 256
Conv 3 x 3
LReLU
64 x 256 x 256
Conv 3 x 3
LReLU
64 x 256 x 256
Model 3 ↑
8.
Upsample
-
64 x 512 x 512
Conv 3 x 3
LReLU
32 x 512 x 512
Conv 3 x 3
LReLU
32 x 512 x 512
9.
Upsample
-
32 x 1024 x 1024
Conv 3 x 3
LReLU
16 x 1024 x 1024
Conv 3 x 3
LReLU
16 x 1024 x 1024
Model full ↑
Table 6: Generator architecture for the MSG-ProGAN mod-
els used in training.
MSG-StyleGAN
The MSG-StyleGAN model uses all the
modiﬁcations proposed by StyleGAN [16] to the ProGANs
[15] architecture except the mixing regularization. Simi-
lar to MSG-ProGAN, we use a 1 x 1 conv layer to ob-
tain the RGB images output from every block of the Style-
GAN generator leaving everything else (mapping network,
non-traditional input and style adaIN) untouched. The dis-
criminator architecture is same as the ProGANs (and con-
sequently MSG-ProGAN, Tab. 7) discriminator.
6.2. Additional Qualitative Results
Here we include additional results for further empiri-
cal validation. We show full resolution results from MSG-
StyleGAN for the 256 x 256 Oxford102 ﬂower dataset,
and the MSG-ProGAN architecture for the 128 x 128
CelebA and LSUN bedroom datasets. The CelebA model
was trained for 28M real images and obtained an FID of
8.86.
Because of the huge size of the LSUN bedrooms
dataset (30M), we trained it for 150M real images (roughly
5 epochs) which resulted in an FID of 18.32. Figures 12
and 13 show the 128 x 128 (highest resolution) samples
generated for the CelebA and LSUN bedrooms datasets re-

Block
Operation
Act.
Output Shape
Model full ↓
Raw RGB images 0
-
3 x 1024 x 1024
FromRGB 0
-
16 x 1024 x 1024
1.
MinBatchStd
-
17 x 1024 x 1024
Conv 3 x 3
LReLU
16 x 1024 x 1024
Conv 3 x 3
LReLU
32 x 1024 x 1024
AvgPool
-
32 x 512 x 512
Raw RGB images 1
-
3 x 512 x 512
Concat/φsimple
-
35 x 512 x 512
2.
MinBatchStd
-
36 x 512 x 512
Conv 3 x 3
LReLU
32 x 512 x 512
Conv 3 x 3
LReLU
64 x 512 x 512
AvgPool
-
64 x 256 x 256
Model 3 ↓
Raw RGB images 2
-
3 x 256 x 256
Concat/φsimple
-
67 x 256 x 256
3.
MinBatchStd
-
68 x 256 x 256
Conv 3 x 3
LReLU
64 x 256 x 256
Conv 3 x 3
LReLU
128 x 256 x 256
AvgPool
-
128 x 128 x 128
Model 2 ↓
Raw RGB images 3
-
3 x 128 x 128
Concat/φsimple
-
131 x 128 x 128
4.
MinBatchStd
-
132 x 128 x 128
Conv 3 x 3
LReLU
128 x 128 x 128
Conv 3 x 3
LReLU
256 x 128 x 128
AvgPool
-
256 x 64 x 64
Raw RGB images 4
-
3 x 64 x 64
Concat/φsimple
-
259 x 64 x 64
5.
MinBatchStd
-
260 x 64 x 64
Conv 3 x 3
LReLU
256 x 64 x 64
Conv 3 x 3
LReLU
512 x 64 x 64
AvgPool
-
512 x 32 x 32
Model 1 ↓
Raw RGB images 5
-
3 x 32 x 32
Concat/φsimple
-
515 x 32 x 32
6.
MinBatchStd
-
516 x 32 x 32
Conv 3 x 3
LReLU
512 x 32 x 32
Conv 3 x 3
LReLU
512 x 32 x 32
AvgPool
-
512 x 16 x 16
Raw RGB images 6
-
3 x 16 x 16
Concat/φsimple
-
515 x 16 x 16
7.
MinBatchStd
-
516 x 16 x 16
Conv 3 x 3
LReLU
512 x 16 x 16
Conv 3 x 3
LReLU
512 x 16 x 16
AvgPool
-
512 x 8 x 8
Raw RGB images 7
-
3 x 8 x 8
Concat/φsimple
-
515 x 8 x 8
8.
MinBatchStd
-
516 x 8 x 8
Conv 3 x 3
LReLU
512 x 8 x 8
Conv 3 x 3
LReLU
512 x 8 x 8
AvgPool
-
512 x 4 x 4
Raw RGB images 7
-
3 x 4 x 4
Concat/φsimple
-
515 x 4 x 4
9.
MinBatchStd
-
516 x 4 x 4
Conv 3 x 3
LReLU
512 x 4 x 4
Conv 4 x 4
LReLU
512 x 1 x 1
Fully Connected
Linear
1 x 1 x 1
Table 7: Discriminator Architecture for the MSG-ProGAN
and MSG-StyleGAN Models used in training.
spectively. Figure 10 and Fig 11 shows samples generated
by the MSG-StyleGAN model at all resolutions on the Ox-
ford Flowers and Cifar-10 datasets respectively. Figure 14
shows additional qualitative results (random samples) from
the CelebA-HQ dataset, trained using our Model full archi-
tecture at 1024 x 1024 resolution.
6.3. Observations
In this section, we present some of our observations and
hypotheses about the differences in results generated by
our method and StyleGAN. We show an overview of ran-
domly selected samples from both models in Fig 7. In our
analysis of the results, we ﬁnd that while the actual result-
ing image quality is very close, StyleGAN samples exhibit
slightly higher variation in terms of pose. In contrast, MSG-
StyleGAN results are slightly more globally consistent and
more realistic. This trade-off between diversity and result
quality is widely reported [41], and may explain some of the
difference in FID score. Further investigation into methods
to control either axis (realism vs diversity), and the impact
this has on the FID score, would be an interesting avenue
for future work.
We also conducted experiments investigating the role
that the pixelwise noise added to each block of the Style-
GAN generator plays in image generation. We found that
on non-face datasets, these noise layers model semantic as-
pects of the images and not just stochastic variations, as
was their initial intent [16] (see Fig 8). We observed that
MSG-StyleGAN also shows this type of effect, although
to a slightly less degree.
We conjecture that this disen-
tanglement between the stochastic and semantic features is
more straightforward for the face modelling task (e.g., on
CelebA-HQ and FFHQ datasets), and the different models
sensitivity to this noise could contribute to some of the the
performance differences we observe as well, on face vs non-
face datasets.
As mentioned in the discussion section of the main pa-
per, we do not use the mixing regularization technique de-
scribed in the StyleGAN [16] work (the question of how to
integrate such a regularization is an interesting direction for
future work). However, we note that in spite of not using
it, the model still learns to disentangle high level seman-
tic features of the images due to the scale based constraint
(see Fig. 9). As apparent from the ﬁgure, the high level
mixing is much more coherent and generates more visually
realistic results; while lower level mixing often generates
incorrect visual cues, such as improper lighting and unbal-
anced hair. This shows that performance gains might be
possible by ensuring proper style-based mixing at the low
(coarse-grained) level of generation.

(a) StyleGAN generated images
(b) MSG-StyleGAN generated images
Figure 7: Random generated samples for qualitative comparison between StyleGAN [16] and MSG-StyleGAN. All the
samples were generated without truncating the input latent space for both because the FID calculation is done on non-
truncated latent spaces. Best viewed zoomed in.
Figure 8: LSUN Church images generated by StyleGAN (top) and MSG-StyleGAN (bottom) using different realizations of
the per-pixel noise while keeping the input latent vectors constant.

Figure 9: Images generated by mixing the styles coming from two different latent vectors at different levels (granularity) of
generation. As in StyleGAN [16], the ﬁrst column images are source 1 and ﬁrst row are source 2. Rows numbered 2, 3, and
4 have the mixing at resolutions (4 x 4 and 8 x 8), while rows 5 and 6 at (16 x 16 and 32 x 32), and the row 6 images
are generated by swapping the source 2 latents at resolutions (64 x 64 till 1024 x 1024).

Figure 10: Random samples generated at all 7 resolutions for the Oxford102 ﬂowers dataset.

Figure 11: Random samples generated at all 4 resolutions for the CIFAR-10 dataset.

Figure 12: Random generated CelebA Faces at resolution 128 x 128.

Figure 13: Random generated LSUN bedrooms at resolution 128 x 128.

Figure 14: Random generated CelebA-HQ Faces at resolution 1024 x 1024.

