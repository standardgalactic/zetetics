
2

A PERIGEE BOOK
Published by the Penguin Group
Penguin Group (USA) LLC
375 Hudson Street, New York, New York 10014
USA • Canada • UK • Ireland • Australia • New Zealand • India • South Africa • China
penguin.com
A Penguin Random House Company
THE FORMULA
Copyright © 2014 by Luke Dormehl
Penguin supports copyright. Copyright fuels creativity, encourages diverse voices, promotes free
speech, and creates a vibrant culture. Thank you for buying an authorized edition of this book and
for complying with copyright laws by not reproducing, scanning, or distributing any part of it in any
form without permission. You are supporting writers and allowing Penguin to continue to publish
books for every reader.
PERIGEE is a registered trademark of Penguin Group (USA) LLC.
The “P” design is a trademark belonging to Penguin Group (USA) LLC.
ISBN: 978-0-698-15884-9
First American edition: November 2014
Previously published in the UK in 2014 by Virgin Books, an imprint of Ebury Publishing.
While the author has made every effort to provide accurate telephone numbers, Internet addresses,
and other contact information at the time of publication, neither the publisher nor the author
assumes any responsibility for errors, or for changes that occur after publication. Further, the
publisher does not have any control over and does not assume any responsibility for author or
third-party websites or their content.
Most Perigee books are available at special quantity discounts for bulk purchases for sales
promotions, premiums, fund-raising, or educational use. Special books, or book excerpts, can also
be created to fit specific needs. For details, write: Special.Markets@us.penguingroup.com.
3

To my friend, Tim Plester
4

Contents
Title Page
Copyright
Dedication
Acknowledgments
An Explanation of the Title, and Other Cyberbole
1 The Quantified Selves
2 The Match & the Spark
3 Do Algorithms Dream of Electric Laws?
4 The Machine That Made Art
 
Conclusion: Predicting the Future
A Note on Author Interviews
Notes
Index
5

Acknowledgments
WriƟng a book is almost always a solitary acƟvity, but I was fortunate enough
to be surrounded by a group of people whose love and/or support made The
Formula a pleasure to work on. Thanks ﬁrst of all to Clara, Tim and Celia Lunt,
as well as members of my family. I could not have completed this project
without the invaluable aid of Ed Faulkner, while it would never have got
through the door in the ﬁrst place were it not for my agent Maggie Hanbury,
Henry de Rougemont, Simon Garﬁeld and Jake Lingwood. Many thanks to
Marian Lizzi, my U.S. editor. AppreciaƟve nods also go in the direcƟon of all
those who spent countless hours speaking with me as part of my research (a
full list of their names is printed on page 243), in addiƟon to my FastCo.Labs
editor Chris Dannen, Cult of Mac’s Leander Kahney, the excellent Nicole
MarƟnelli, Karl French, tech comms guru Alice Bonasio-Atkinson, Tim MaƩs,
Alex Millington, Michael Grothaus, Tom Atkinson, Simon Callow, and my
brothers-from-other-mothers, Andre and Nathan Trantraal. All helped this
book along in one way or another. All I can take full credit for are the (hopefully
few) mistakes.
6

An Explanation of the Title, and Other
Cyberbole
At their root, algorithms are liƩle more than a series of step-by-
step instrucƟons, usually carried out by a computer. However, if
their descripƟon is straighƞorward, their inner workings and
impact on our lives are anything but.
Algorithms sort, ﬁlter and select the informaƟon that is
presented to us on a daily basis. They are responsible for the
search results shown to us by Google, the informaƟon about our
friends that is highlighted by Facebook, and the type of products
Amazon predicts we will be most likely to buy. Increasingly, they
will also be responsible for what movies, music and other
entertainment look like, which people we are partnered with in
predicƟve relaƟonships, and even the ways in which laws are
enforced and police operate. An algorithm can scan through your
metadata and recommend that you will likely make a
hardworking employee, just as one could accuse you of a crime, or
determine that you are unﬁt to drive a car. In the process,
algorithms are profoundly changing the way that we view (to
quote Douglas Adams) life, the universe and everything.
One of my favorite observaƟons about technology is the one
oŌen aƩributed to the cultural theorist Paul Virilio: “The
invenƟon of the ship was also the invenƟon of the shipwreck.”
One could, of course, turn this around and say that the inventor of
the shipwreck was also the person that invented the ship.
Algorithms have had their fair share of shipwrecks (which I will
discuss during the course of this book), but they also perform
incredibly useful funcƟons: allowing us to navigate through the
2.5 quintillion bytes of data that are generated each day (a million
Ɵmes more informaƟon than the human brain is capable of
holding) and draw actionable conclusions from it.
7

As with the old adage about how to carve an elephant statue
(you chip away everything that isn’t an elephant), I will start out
by explaining what this book is not. It is not, for one thing, a
computer science textbook about algorithms. There are far
beƩer books (and, indeed, far more qualiﬁed writers) to achieve
this task.
Neither is it a history of the algorithm as a concept. While I
considered aƩempƟng such a thing, I was put oﬀ by both the
sheer scale of the project and the fact that its end result—while
no doubt fascinaƟng under the stewardship of the right author—
would be not enƟrely dissimilar to the textbook I also shied away
from. By this I do not mean that a history book and a textbook are
necessarily the same thing, but rather that a history of a once-
niche mathemaƟcal concept would likely appeal only to those
mathematicians or computer scientists already familiar with it.
Instead, I want to tell the story of the myriad ways (some
subtle, others less so) that algorithms aﬀect all of our lives: from
the entertainment we enjoy to the way we think about human
relaƟonships. What do scoring hot dates, shooƟng Hollywood
turkeys, bagging your own poo, and cuƫng opportuniƟes for
lawyers’ fees have in common? This is a book about the
algorithmization of life as we know it.
In my day job, wriƟng about a ﬁeld known as the “digital
humaniƟes” for Fast Company, I’m constantly considering the
implicaƟons of “algorithmic” culture and the idea (not always a
misguided one) that no maƩer what the problem, it can be solved
with the right algorithm.
A typical illustraƟon of what I mean can be seen in Bill Tancer’s
2009 book Click: What We Do Online and Why It MaƩers. Tancer
—described in at least one place as “the world’s preeminent
expert on online [behavior]”—begins his book by describing a
radio interview he listened to in the car one day. Being
interviewed 
was 
a 
BriƟsh 
psychologist 
referring 
to 
a
mathemaƟcal formula he had developed to determine the most
8

objecƟvely depressing week of the year. AŌer much work, he had
discovered that this was the third week of January, a feat brought
about by the convergence of failed New Year’s resoluƟons, credit-
card debt accumulated over the holiday season, and the usual
dismal weather paƩerns. Tancer notes that he remained
unconvinced: a perspecƟve that was later backed up when the
formula was severely criƟcized for its lack of scienƟﬁc rigor.
However, his lack of convicƟon has nothing to do with the
suggesƟon that a reducƟve formula could possibly provide
answers on a topic as complex and mulƟfaceted as depression,
but rather because he believes that he had come up with a beƩer
formula.1
In other words, his problem wasn’t with the existence of the
sum, but rather with its working.
This book was spurred by years of hearing similar observaƟons,
all claiming that there is no problem technology cannot reduce to
its most formulaic level and thereby determine objecƟve answers
in response to. This thinking is the reason “The Formula” is in
upper case rather than simply exisƟng as a catchall for the various
technical processes I describe. It implies an ideological element,
and that ideology is evidenced by the more expansive view I take
of algorithms and their associated technological apparatus:
conceiving of them as the embodiment of a parƟcular form of
techno-raƟonality, symptomaƟc of a type of social ordering built
around the promise of objecƟvity. In this way I use The Formula
much as the late American poliƟcal scienƟst and communicaƟons
theorist Harold Lasswell used the word “technique”: referring, in
Lasswell’s words, to “the ensemble of pracƟces by which one uses
available resources to achieve values.” It is about both the
applicaƟon and the scope of applicaƟon, as well as the existence
of objecƟve truths lurking just beneath the surface—to be teased
out with the right data-mining tools.
Writers on technology tend, with a few notable excepƟons, to
be overwhelmingly utopian in their outlook. To them, all progress
9

is posiƟve. As a result, there is a tendency among technology
writers to christen each new invenƟon as the totemic ﬁgurehead
of its own “era”—something that has led to the disdainful term
“cyberbole.” While this book could very well join the number of
volumes about algorithms and big data already lining the shelves,
what I am interested in goes back much further than simply the
birth of the Internet or the age of the personal computer.
WriƟng during the ﬁrst half of the 1960s, the French sociologist
(and ChrisƟan anarchist!) Jacques Ellul described a creature
known as the Technical Man, an individual “fascinated by results,
by the immediate consequences of seƫng standardized devices
into moƟon . . . commiƩed to the never-ending search for ‘the
one best way’ to achieve any designated objective.” This objective
could occasionally be clouded (or else speeded up) by a naive
enthusiasm for the means of geƫng there: not by anything so
unquanƟﬁable as ethical concerns, but rather by an enthusiasm
for the ingenuity, elegance and “spectacular eﬀecƟveness” of
man’s ability to dream up solutions.
As Ellul’s observaƟon proves, this approach is not therefore a
new one, and the founders of Google and the heads of the various
high-tech companies I discuss are not the ﬁrst people to display
what the late American sociologist Lewis Mumford called the
“will-to-order”—meaning the desire to make formulaic sense of
the world. WriƟng in the 1930s, long before the birth of the
modern computer, Mumford noted that automaƟon was
simultaneously for “enlarging the mechanical or sensory
capaciƟes of the human body” and “for reducing to a measurable
order and regularity the processes of life.” To make sense of a big
picture, we reduce it, he suggested. To take an abstract concept
such as human intelligence and turn it into something
quanƟﬁable, we abstract it further, stripping away complexity
and assigning it a seemingly arbitrary number, which becomes a
person’s IQ.
What is new is the scale that this idea is now being enacted
10

upon, to the point that it is diﬃcult to think of a ﬁeld of work or
leisure that is not subject to algorithmizaƟon and The Formula.
This book is about how we reached this point, and how the age of
the algorithm impacts and shapes subjects as varied as human
creaƟvity, human relaƟonships (and, more speciﬁcally, romanƟc
relationships), notions of identity and matters of law.
Algorithms are very good at providing us with answers in all of
these cases.
The real question is whether they give the answers we want.
11

CHAPTER 1
The Quantified Selves
Larry Smarr weighed close to 200 pounds when he arrived in La
Jolla, California, in 2000. The photograph from his driver’s license
at the Ɵme depicts an overweight 51-year-old with a soŌ, round
face and the ﬂeshy ripple of a double chin. Although he regularly
tested his mind as a leading physicist and expert in
supercompuƟng, Smarr had not exercised his body properly in
years. He regularly drank Coke and enjoyed chowing down on
deep-fried, sugar-coated pastries.1 Moving to the West Coast to
run a new insƟtute at the University of California called the
California InsƟtute for TelecommunicaƟons and InformaƟon
Technology, he was suddenly confronted with a feeling he hadn’t
experienced in years: a sense of deep inadequacy. “I looked
around and saw all these thin, ﬁt people,” he remembers. “They
were running, bicycling, looking beauƟful. I realized how diﬀerent
I was.”
Smarr’s next quesƟon was one shared by scienƟsts and
philosophers alike: Why? He visited his local bookshop and bought
“about a zillion” diet books. None of them was deemed
saƟsfactory to a man whose brain could handle the labyrinthine
details of supernovas and complex star formaƟons, but for whom
healthy eaƟng and regular exercise seemed, ironically enough,
like complex astrophysics. “They all seemed so arbitrary,” he says.
It wasn’t unƟl he discovered a book called The Zone, wriƩen by
biochemist Barry Sears, that Smarr found what it was that he was
looking for. Sears treated the body as a coupled nonlinear system
in which feedback mechanisms like the glucose-insulin and
immune systems interact with one another. That was an
12

approach that Smarr could relate to. Inspired, he started
measuring his weight, climbing naked onto a pair of scales each
day and wriƟng down the number that appeared in front of him.
Next, he hired a personal trainer and began keeping track of the
amount of exercise he parƟcipated in on a daily basis. AŌer that it
was on to diet—breaking food down into its “elemental units” of
protein, fat, carbohydrates, ﬁber, sugar and salt, and modifying
his diet to remove those inputs that proved detrimental to well-
being. “Think of it like being an engineer, reverse-engineering the
subsystems of a car,” Smarr says. “From that you can derive that
you need a certain level of petrol in order to run, and that if you
put water in your gas tank you will tear apart the car. We don’t
think that way about our bodies, but that is the way we ought to
think.”
It didn’t take long unƟl Smarr turned to more complex forms of
technology to help him lose weight. He purchased and began
wearing Polar WearLink heart-rate straps, FitBits, BodyMedia,
and other pieces of wearable tech that use algorithms to convert
body metrics into data. WanƟng to check his progress, Smarr
started paying for blood tests at a private laboratory and then—in
a quest for yet more numbers to pore over—began FedEx-ing oﬀ
his stool for regular analysis. “I didn’t have a biology or medical
background, so I had to teach myself,” Smarr says of his process.
One of the numbers he wound up ﬁxaƟng on related to
complex reacƟve proteins, which act as a direct measure of
inﬂammaƟon in the body. In a normal human body this number
should be less than one. In Smarr’s case it was ﬁve. Over Ɵme it
rose to 10, then 15. As a scienƟst, he had discovered a paradox.
“How was it that I had reduced all of the things that normally
drive inﬂammaƟon in terms of my food supply, but the numbers
were growing and growing with this chronic inﬂammaƟon?” he
muses. “It didn’t make any sense.”
At that point, Smarr decided to visit his doctor to present the
findings. The appointment didn’t go as planned.
13

“Do you have any symptoms?” the doctor asked.
“No,” Smarr answered. “I feel fine.”
“Well, why are you bothering me, then?”
“Well, I’ve got these great graphs of my personal data.”
“Why on earth are you doing that?” came the response.
The doctor told Smarr that his data was too “academic” and
had no use for clinical pracƟce. “Come back when there’s
something actually wrong with you, rather than just anomalies in
your charts,” the doctor said.
Several weeks later, Smarr felt a severe pain in the leŌ side of
his abdomen. He went back to the doctor’s and was diagnosed
with diverƟculiƟs, a disease caused by acute inﬂammaƟon. It was
the perfect illustraƟon of Smarr’s problem: doctors would deal
only in clinical symptoms, unwilling to delve into the data that
might actually prove preventaƟve. Having learned an important
lesson, Smarr decided to take over his own health tracking.
“People have been brainwashed into thinking that they have no
responsibility for the state of their bodies,” he says. “I did the
calculaƟon of the raƟo of two 20-minute doctor visits per year,
compared to the total number of minutes in the year, and it turns
out to be one in 10,000. If you think that someone is going to be
able to tell you what’s wrong with you and ﬁx the problem in one
10,000th of the Ɵme that you have available to do the same, I’d
say that’s the deﬁniƟon of insanity. It just doesn’t make any
sense.”
In the aŌermath of the doctor’s visit, Smarr began obsessively
tracking any and all symptoms he noƟced and linking each of
these to ﬂuctuaƟons in his body data. He also upped the amount
of informaƟon he was looking at, and started using complex data-
mining algorithms to siŌ through it looking for irregulariƟes.
Another high number he zeroed in on referred to lactoferrin, an
anƟbacterial agent shed by white blood cells when they are in
aƩack mode, a bit like a canister of tear gas being dispersed into
a crowd of people. This number was meant to be less than seven.
14

It was 200 when Smarr ﬁrst checked it, and by May 2011 had
risen to 900. Searching through scienƟﬁc literature, Smarr
diagnosed himself as having a chronic autoimmune disorder,
which he later narrowed down to something called Crohn’s
disease. “I was entirely led there by the biomarkers,” he notes.
In this sense, Smarr is the epitome of algorithmic living. He
tracks 150 variables on a constant basis, walks 7,000 steps each
day, and has access to millions of separate data points about
himself. As such, both his body and his daily life are divided,
mathemaƟzed and codiﬁed in a way that means that he can
follow what is happening inside him in terms of pure numbers. “As
our technological ability to ‘read out’ the state of our body’s main
subsystems improves, keeping track of changes in our key
biochemical markers over Ɵme will become rouƟne, and
deviaƟons from the norm will more easily reveal early signals of
disease development,” Smarr argues in an essay enƟtled
“Towards Digitally Enabled Genomic Medicine: A 10-Year
Detective Story of Quantifying My Body.”2
Using 
his 
access 
to 
the 
University 
of 
California’s
supercomputers, Smarr is currently working on creaƟng a
distributed planetary computer composed of a billion processors
that—he claims within ten years—will allow scienƟsts to create
working algorithmic models of the human body. What, aŌer all, is
the body if not a complex system capable of tweaks and
modifications?
As journalist Mark Bowden observed in a 2012 arƟcle for the
Atlantic, “If past thinkers leaned heavily on the steam engine as
an all-purpose analogy—e.g., contents under pressure will
explode (think Marx’s ideas on revoluƟon or Freud’s about
repressed desire)—today we prefer our metaphors to be
electronic.”
And when it comes to symbolic representaƟons, many people
(Smarr included) prefer formulas to metaphors.3
15

Self-Knowledge Through Numbers
Fiƫng an “n = 1” study in which just one person is the subject,
Larry Smarr’s story is excepƟonal. Not everyone is an expert in
supercompuƟng and not everyone has the ability, nor the
resources (his regimen costs between $5,000 and $10,000 each
year) to capture huge amounts of personal data, or to make sense
of it in the event that they do.
But Smarr is not alone. As a data junkie, he is a valued member
of the so-called QuanƟﬁed Self movement: an ever-expanding
group of similar individuals who enthusiasƟcally take part in a
form of self-tracking, somaƟc surveillance. Founded by Wired
magazine editors Gary Wolf and Kevin Kelly in the mid-2000s, the
QuanƟﬁed Self movement casts its aspiraƟons in bold
philosophical terms, promising devotees “self-knowledge through
numbers.”4 Taking the PosiƟvist view of veriﬁcaƟon and
empiricism, and combining this with a liberal dose of technological
determinism, the QuanƟﬁed Self movement begs the existenƟal
quesƟon of what kind of self can possibly exist that is unable to be
number-crunched using the right algorithms?
If Socrates concluded that the unexamined life was not worth
living, then a 21st-century update might suggest the same of the
unquanƟﬁed life. As with René Descartes’ famous statement,
cogito ergo sum (“I think, therefore I am”)—I measure, therefore
I exist.
In a sense, “Selfers” take Descartes’ ideas regarding the self to
an even more granular level. Descartes imagined that
consciousness could not be divided into pieces in the way that the
body can, since it was not corporeal in form. Selfers believe that a
person can be summarized eﬀecƟvely so long as the correct
technology is used and the right data gathered. Inputs might be
food consumed or the quality of surrounding air, while states can
be measured through mood, arousal and blood oxygen levels, and
performance in terms of mental and physical well-being.
16

“I like the idea that someone, somewhere is collecƟng all of
this data,” says Kevin Conboy, the creator of a quanƟﬁed sex app,
Bedpost, which I will return to later on in this book. “I have this
sort of philosophical hope that these numbers exist somewhere,
and that maybe when I die I’ll get to see them. The idea that
computer code can give you an insight into your real life is a very
powerful one.”
A typical QuanƟﬁed Self devotee (if there is such a thing) is
“Michael.” Every night, Michael goes to bed wearing a headband
sensor. He does this early, because this is when the sensor informs
him that his sleep cycles are likely to be at their deepest and most
restoraƟve. When Michael wakes up he looks at the data for
evidence of how well he slept. Then he gets up, does some push-
ups and meditates for a while, before turning on his computer and
loading a wriƟng exercise called “750 Words” that asks him to
write down the ﬁrst 750 words that come to mind.5 When he has
done this, text-analysis algorithms scour through the entry and
pull up revealing stats about Michael’s mood, mind-set and
current preoccupaƟons—some of which he may not even be
consciously aware he is worrying about. AŌer this, he is ﬁnally
ready to get moving (using a FitBit to monitor his steps, of
course). If he doesn’t carry out these steps, he says, “I’m oﬀ for
the rest of the day.”6
17

Robo-cize the World
While QS’s reliance on cuƫng-edge technology, social networking
and freedom-through-surveillance might seem quintessenƟally
modern—very much a creaƟon of post-9/11 America—the roots
of what can be described as “body-hacking” go back a number of
years. The 1980s brought about the rise of the “robo-cized”
athletes who used NauƟlus, Stairmaster and other pieces of high-
tech gym equipment to sculpt and hone their bodies to physical
perfecƟon. That same decade saw the advent of the portable
technology known as the Sony Walkman (a nascent vision of
Google Glass to come), which transformed public spaces into a
controllable private experience.7 Building on this paradigm, the
1990s was home to MIT’s Wearable CompuƟng Group, who took
issue with what they considered to be the premature usage of the
term “personal computer” and insisted that:
A person’s computer should be worn, much as
eyeglasses or clothing are worn, and interact with
the user based on the context of the situaƟon. With
heads-up 
displays, 
unobtrusive 
input 
devices,
personal wireless local area networks, and a host of
other context sensing and communicaƟon tools, the
wearable computer can act as an intelligent
assistant, whether it be through a Remembrance
Agent, 
augmented 
reality, 
or 
intellectual
collectives.8
There appear to be few limits to what today’s QuanƟﬁed
Selfers can measure. The beauty of the movement (if one can
refer to it in such aestheƟc terms) is the mass customizaƟon that
it makes possible. By quanƟfying the self, a person can ﬁnd
apparently rigorous answers to quesƟons as broad or speciﬁc as
how many minutes of sleep are lost each night per unit of alcohol
18

consumed, how consistent their golf swing is, or whether or not
they should stay in their current job. Consider, for example, the
story of a young female member of the QuanƟﬁed Self
movement, referred to only as “Angela.”
Angela was working in what she considered to be her dream
job, when she downloaded an app that “pinged” her mulƟple
Ɵmes each day, asking her to rate her mood each Ɵme. As
paƩerns started to emerge in the data, Angela realized that her
“mood score” showed that she wasn’t very happy at work, aŌer
all. When she discovered this, she handed in her notice and quit.
“The one commonality that I see among people in the
QuanƟﬁed Self movement is that they have quesƟons only the
data can answer,” says 43-year-old Selfer Vincent Dean Boyce.
“These quesƟons may be very simplisƟc at ﬁrst, but they very
quickly become more complex. A person might be interested in
knowing how many miles they’ve run. Technology makes that
very easy to do. A more advanced quesƟon, however, would be
not only how many miles a person has run, but how many other
people have run the same amount? That’s where the data and
algorithms come in. It’s about a quest for knowledge, a quest for
a deeper understanding not only of ourselves, but also of the
world we live in.”
Boyce has always been interested in quanƟﬁcaƟon. As a New
York 
University 
student 
enrolled 
in 
the 
InteracƟve
TelecommunicaƟons Program, he once aƩached some sensors,
micro-controllers and an accelerometer to a model rocket and
launched it oﬀ the ground. “What was interesƟng,” he says, “is
that I was able to make a self-contained component that could be
sent somewhere, that could gather informaƟon, and that I could
then retrieve and learn something about.” AŌer analyzing the
rocket’s data, Boyce had his “Eureka!” moment. A lifelong
skateboarder and surfer, he decided to aƩach similar sensors to
his trusty skateboard and surĩoard to measure the mechanical
movements made by each. He also broke the surrounding
19

environment down into hundreds of quanƟﬁable variables,
ranging from weather and Ɵme of day to (in the case of surﬁng)
Ɵdal changes and wave height. “From a QuanƟﬁed Self
standpoint,” Boyce notes, “I can . . . think about where it was
that I surfed from a geospaƟal type of framework, or what
equipment I was using, what the condiƟons were like . . . [It]
represents me doing something in space and time.”
In this way, Selfers return to the romanƟc image of the rugged
individualists of the American fronƟer: an image regularly drawn
upon by Silicon Valley industrialists and their followers. The man
who tracks his data is no different from the one who carves out his
own area of land to live on, who draws his own water, generates
his own power, and grows his own food. In a world in which user
data and personal informaƟon is gathered and shared in
unprecedented quanƟƟes, self-tracking represents an aƩempt to
take back some measure of control. Like Google Maps, it puts the
individual back at the center of his or her universe. “My belief is
that this will one day become the norm,” Boyce says of the
QuanƟﬁed Self. “It will become a commodity, with its own sense
of social currency.”
20

Shopping Is Creating
One of the chapters in Douglas Coupland’s debut novel
GeneraƟon X, wriƩen at the birth of the networked computer
age, is Ɵtled “Shopping Is Not CreaƟng.”9 It is a wonderfully pithy
observaƟon 
about 
1960s 
acƟvism 
sold 
oﬀ 
as 
1990s
commercialism, from an author whose ﬁcƟon books Microserfs
and JPod perfectly lampoon techno-opƟmism at the turn of the
millennium. It is also no longer true. Every Ɵme a person shops
online (or in a supermarket using a loyalty card) their idenƟty is
slightly altered, being created and curated in such a way that is
almost imperceptible.
This isn’t just limited to shopping, of course. The same thing
happens whenever you open a new web-browsing window and
surf the Internet. Somewhere on a database far away, your
movements have been idenƟﬁed and logged. Your IP address is
recorded and “cookies” are installed on your machine, enabling
you to be targeted more eﬀecƟvely with personalized
adverƟsements and oﬀers. Search regularly for news on a
parƟcular sport and you will begin to spot adverts related to it
wherever you look—like the murderer played by Robert Walker in
Hitchcock’s Strangers on a Train, who sees constant reminders of
the woman he killed. MenƟon the words “Cape Town” in an e-
mail, for instance, and watch the ﬂood of “Cheap ﬂights to South
Africa” messages flood in.
It was the American philosopher and psychologist William
James who observed, in volume one of his 1890 text The Principles
of Psychology, that “a man’s self is the sum total of all that he
[can] call his, not only his body and his psychic powers, but his
clothes and his house, his wife and children, his ancestors and
friends, his reputaƟon and works, his lands, and yacht and bank
account.”10 This counts for double in the age of algorithms and
The Formula. Based on a person’s location, the sites that they visit
and spend Ɵme on, and the keywords that they use to search,
21

staƟsƟcal inferences are made about gender, race, social class,
interests and disposable income on a constant basis. Visiting Perez
Hilton suggests a very diﬀerent thing from Gizmodo, while buying
airline Ɵckets says something diﬀerent from buying video games.
To all intents and purposes, when combined, these become the
algorithmic self: idenƟty and idenƟﬁcaƟon shiŌed to an enƟrely
digital (and therefore measurable) plane.
22

Your Pleasure Is Our Business
IdenƟty is big business in the age of The Formula. The ability to
track user movements across diﬀerent websites and servers has
led to the rise of a massive industry of web analyƟcs ﬁrms. These
companies make it their mission not only to amass large amounts
of informaƟon about individuals, but also to use proprietary
algorithms to make sense of that data.
One of the largest companies working in this area is called
Quantcast. Headquartered in downtown San Francisco—but with
addiƟonal oﬃces in New York, Dublin, London, Detroit, Atlanta,
Chicago and Los Angeles—Quantcast ranks among the top ﬁve
companies in the world in terms of measuring audiences, having
raised in excess of $53.2 million in venture capital funding since it
was founded in 2006. Its business revolves around ﬁnding a
formula that best describes speciﬁc users and then advising
companies on how to best capitalize on this. “You move away
from the human hypothesis of adverƟsing,” explains cofounder
Konrad Feldman, “where someone theorizes what the ideal
audience for a product would be and where you might be able to
ﬁnd these people—to actually measuring an adverƟser’s
campaign, looking at what’s actually working, and then reverse-
engineering the characterisƟcs of an audience by analyzing
massive quantities of data.”
Before starƟng Quantcast, English-born University College
London graduate Feldman founded another business in which he
used algorithms to detect money laundering for some of the
world’s leading banks. “We looked through the billions of
transacƟons these banks deal with every month to ﬁnd suspicious
acƟvity,” he says. It was looking at fraud that made Feldman
aware of the power of algorithms’ ability to sort through masses
of data for paƩerns that could be acted upon. “It could represent
anything that people were interested in,” he says excitedly.
“Finances were interesƟng data, but it only related to what
23

people spend money on. The Internet, on the other hand, has
informaƟon about interests and changes in trends on the macro
and micro level, all in a single data format.” He was hooked.
“Historically, measurement was done in retrospect, at the
aggregate level,” Feldman says of the adverƟsing industry.
“That’s what people understood: the aggregate characterisƟcs of
an audience.” When Feldman ﬁrst moved to the United States, he
was baﬄed by the amount of adverƟsing on television, which
oŌen represented 20 minutes out of every hour. It was a
scaƩergun approach, rather like spraying machine-gun bullets
into a river and hoping to hit individual ﬁsh. Whatever was caught
was more or less done so by luck. Of course, a television channel
can’t change it for every viewer, Feldman explains. The Internet,
however, was diﬀerent. Much like the customized user
recommendaƟons on Amazon, Quantcast’s algorithmically
generated insights meant that online shopkeepers could
redecorate the shop front for each new customer. In this way,
audiences are able to be divided into demographics,
psychographics, 
interests, 
lifestyles 
and 
other 
granular
categories. “Yep, we’re almost psychic when it comes to reading
behavior paƩerns and interpreƟng data,” brag Quantcast’s
promoƟonal materials. “We know before they do. We know
before you do. We can tell you not only where your customers are
going, but how they’re going to get there, so we can actually
influence their paths.”
Quantcast’s way of thinking is rapidly becoming the norm, both
online and oﬀ. A Nashville-based start-up called Facedeals
promises shops the opportunity to equip themselves with facial
recogniƟon-enabled cameras. Once installed, these cameras
allow retailers to scan customers and link them to their Facebook
proﬁles, then target them with personalized oﬀers and services
based upon the “likes” they have expressed online. In late 2013,
UK supermarket giant Tesco announced similar plans to install
video screens at its checkouts around the country, using inbuilt
24

cameras equipped with custom algorithms to work out the age
and gender of individual shoppers. Like loyalty cards on steroids,
these would then allow customers to be shown tailored
adverƟsements, which can be altered over Ɵme, depending on
both the date and Ɵme of day, along with any extra insights
gained from monitoring purchases. “It is Ɵme for a step-change in
adverƟsing,” said Simon Sugar, chief execuƟve of Amscreen, who
developed the OpƟmEyes technology behind the screens. “Brands
deserve to know not just an esƟmaƟon of how many eyeballs are
viewing their adverts, but who they are, too.”11
25

The Wave Theory
This noƟon of appealing to users based on their individual surﬁng
habits taps—ironically enough—into the so-called wave theory of
futurist Alvin Toﬄer. 12 In his 1980 book The Third Wave, Toﬄer
described the way in which technology develops in waves, with
each successive wave sweeping aside older socieƟes and
cultures.13 There have been three such waves to date, Toﬄer
claimed. The first was agricultural in nature, replacing the hunter-
gatherer cultures and centering on human labor. The second
arrived with the Industrial RevoluƟon, was built around large-
scale machinery, and brought with it the various “masses” that
proliferated in the years since: mass producƟon, mass
distribuƟon, mass consumpƟon, mass educaƟon, mass media,
mass recreaƟon, mass entertainment and weapons of mass
destrucƟon. The Third Wave, then, was the InformaƟon Age,
ushering in a glorious era of “demassiﬁcaƟon” under which
individual freedoms could ﬁnally be exercised outside the heaving
constraints of mass society. DemassiﬁcaƟon would, Toﬄer
argued, be “the deepest social upheaval and creaƟve
restructuring of all Ɵme,” responsible for the “building [of] a
remarkable new civilizaƟon from the ground up.” And it was all
built on personalization.
26

Please Hold to Be Connected to Our Algorithm
It is well known that not every call-center agent is equipped to
handle every type of call that comes in. The larger the company,
the less likely it is that any one person will be able to deal with
every single inquiry, which is the reason customers are typically
routed to diﬀerent departments in which agents are trained to
have diﬀerent skills and knowledge bases. A straighƞorward
example might be the global company whose call centers
regularly receive calls in several diﬀerent languages. Both callers
and agents may speak one or more of several possible languages,
but not necessarily all of them. When the French-speaking
customer phones up, they may be advised to press “1” on their
keypad, while the English-speaking customer might be instructed
to press “2.” They are then routed through to the person best
suited to deal with their call.
But what if—instead of simply redirecƟng customers to
diﬀerent call-center agents based upon language or specialist
knowledge—an algorithm could be used to determine parƟcular
qualiƟes of the person calling in: based upon speech paƩerns, the
parƟcular words they used, and even details as seemingly trivial
as whether they said “um” or “err”—and then uƟlize these
insights to put them through to the agent best suited for dealing
with their emotional needs?
Chicago’s MaƩersight CorporaƟon does exactly that. Based on
custom algorithms, MaƩersight calls its business “predicƟve
behavioral rouƟng.” By dividing both callers and agents into
diﬀerent personality types, it can make business both faster and
more saƟsfactory to all involved. “Each individual customer has
diﬀerent expectaƟons and behaviors,” MaƩersight notes in
promoƟonal materials. “Similarly, each individual employee has
diﬀerent strengths and weaknesses handling diﬀerent types of
calls. As a result, the success of a given customer interacƟon is
oŌen determined by which employee handles that interacƟon
27

and how well their competencies and behavioral characterisƟcs
align with each specific customer’s needs.”
The man behind MaƩersight’s behavioral models is a clinical
psychologist named Dr. Taibi Kahler. Kahler is the creator of a
type of psychological behavioral proﬁling called Process
CommunicaƟon. Back in the early 1970s, Kahler interned at a
private psychiatric hospital. While he was there, he created
something called a “Miniscript” based on his observaƟons about
paƟents in distress. The work wound up winning him the 1977 Eric
Berne Memorial ScienƟﬁc Award. What Kahler noƟced was that
certain predictable signs precede parƟcular incidents of distress,
and that these distress signs are linked to speciﬁc speech
paƩerns. These, in turn, led to him developing proﬁles on the six
different personality types he saw recurring. The personality types
are as follows:
Personality type
Personality traits
How
common?
“Thinkers”
Thinkers view the world through data. Their
primary way of dealing with situations is based
upon logical analysis of a situation. They have
the potential to become humorless and
controlling.
1 in 4
people
“Rebels”
Rebels interact with the world based on
reactions. They either love things or hate them.
Many innovators come from this group. Under
pressure they can be negative and blameful.
1 in 5
people
“Persisters”
Persisters filter everything through their
opinions. Everything is measured up against
their worldview. This describes the majority of
politicians.
1 in 10
people
“Harmonizers”
Harmonizers deal with everything in terms of
emotions and relationships. Tight situations
make this group overreactive.
3 in 10
people
“Promoters”
Promoters view everything through action.
These are the salesmen of the world, always
looking to close a deal. They can be irrational
1 in 20
people
28

and impulsive.
“Imaginers”
Imaginers deal in unfocused thought and
reflection. These people operate in vivid
internal worlds and are likely to spot patterns
where others cannot.
1 in 10
people
Although everyone has all six personality types to a greater or
lesser degree, people will respond best to individuals who reﬂect
their own primary personality type. If people’s communicaƟon
needs are not met by being given the kind of posiƟve “feedback”
they require (a feelings-oriented person being asked cold hard
facts, for example) they go into distress, which can be diﬀused
only if the person on the other end of the conversaƟon is able to
adequately pick up on the warning signals and respond
appropriately.
In a call-center environment this knowledge results in an
extraordinary qualitaƟve change, according to MaƩersight. A
person patched through to an individual with a similar personality
type to their own will have an average conversaƟon length of ﬁve
minutes, with a 92 percent problem-resoluƟon rate. A caller
paired up to a conﬂicƟng personality type, on the other hand, will
see their call length double to ten minutes—while the problem-
resolution rate tumbles to 47 percent.
Process CommunicaƟon isn’t only being used by MaƩersight,
however. In the past, Kahler has helped NASA develop algorithms
to aid with the selecƟon of its astronauts, since his model can
accurately predict the personality types that won’t crack under
the high-pressure atmosphere of space travel. (“Persisters”—who
strive for perfecƟon and encourage others to reach their peak
performance—prove to be the best personality ﬁt.) Kahler’s
company, Kahler CommunicaƟons, also has a number of ongoing
projects designed to help organizaƟons come up with data-driven
and algorithmic solutions to questions related to personality.
“From our perspecƟve this is the key to diversity,” says Robert
Wert, a former aƩorney who was employed as the COO of Kahler
29

CommunicaƟons when I had the opportunity to speak with him.
“If all cultures are made up of the same building blocks, all of
whom have the same type of interacƟons both posiƟve and
negaƟve, then the real diversity is in personality type. It’s not in
ethnicity, it’s not in gender, it’s not in anything else. I see this as
the great equalizer. If you can walk into a room and immediately
start speaking to someone who’s of a diﬀerent background to
you, and you can idenƟfy the same traits in them that you’ve
dealt with for the rest of your life, that person is no longer the
Other.”
30

The Lake Wobegon Strategy
Founded in 2011, Gild is a recruitment company that serves some
of the tech industry’s biggest and best-known players. Currently
focused on automaƟng the discovery of talented programmers,
Gild’s mission statement is to apply The Formula to the
notoriously unreliable hiring process. To do this, the company uses
algorithms to analyze individuals on tens of thousands (soon
hundreds of thousands) of diﬀerent metrics and data points—
mining them for insights in what Gild refers to as “broad
predictive modeling.”
The success stories the company trots out are impressive. A
typical one tells of 26-year-old college dropout Jade Dominguez,
who lived oﬀ an increasing line of credit-card debt in South
Pasadena, 
California, 
while 
teaching 
himself 
computer
programming.14 AŌer being “discovered” by Gild’s algorithm, he
now works as a programmer at the company that found him. His
story is hardly unique, either. “These are people whose CVs you
wouldn’t look twice at, but who our algorithm predicts would be
perfect for the job,” says Vivienne Ming, Gild’s chief scienƟst. “For
some of our customers, that is exactly what they’re looking for.
These are companies that are ﬂooded with résumés. They don’t
need us to find people; they need us to find different people.”
The ﬁrst Ɵme I spoke with Ming, it was May 2013, and she was
siƫng in the back of a taxicab on her way to San Francisco
InternaƟonal Airport. A tall, striking woman with silver-blue eyes
and strawberry-blond hair, Ming is a theoreƟcal neuroscienƟst
with a Carnegie Mellon University pedigree. Eﬀortlessly assured,
her geeky engineering side is evidenced by the fact that she wears
a prerelease Google Glass headset. In addiƟon to her
neuroscience background, Ming’s TwiƩer proﬁle describes her as
an “intrepid entrepreneur, undesirable superhero [and] very
sleepy mother.”
Ming is deeply invested in Gild’s utopian vision of turning the
31

workplace into the kind of meritocracy she believes it should be.
“This is the way things ought to work, right?” she says,
rhetorically. “The person making the hiring decisions really should
have an accurate picture of who I am—not just a snap judgment
made because I look a certain way. But believe me, the way that
people look is a huge influence on hiring.”
If there is a reason why the idea of people being misjudged on
ﬁrst appraisal hits home parƟcularly hard with Ming it may have
something to do with her background. Born Evan Campbell Smith,
Ming underwent gender reassignment surgery in 2008, having
“ghosted [her] way through life” up unƟl that point. “I was not a
classically good student,” she explains. “I frequently failed my
classes, I was constantly in trouble at school. I was not engaged,
but I deeply cared about the learning experience. The most
trouble I ever got in was lying to stay in an honors chemistry
course. I loved being there. I loved learning.” AŌer Ming
underwent the operaƟon to become a woman, she noƟced that
she was treated diﬀerently: being asked fewer quesƟons about
math than she had as a man, and not being invited to so many
social events by male colleagues and business connections.
To Ming, there exist two main problems with classic hiring
strategies. The ﬁrst is that they are inherently biased. While the
majority of people appreciate the value of recruiƟng people with
a diﬀerent background from themselves, they are oŌen not
exposed to these individuals in social seƫngs. Why, she asks, is a
typical start-up composed of similar-looking individuals of
approximately the same age, with the same scruﬀy engineering
look? Because they hired people they knew. The person who is
good friends with a white, male, upper-middle-class, hardworking
engineer is staƟsƟcally more likely to be a hardworking engineer
themselves. They are also likely to be white, male and upper
middle class. As data-driven algorithmic culture has taken over,
these casual assumpƟons have in many cases become codiﬁed. To
get a job at Facebook, one of the iniƟal tests used in the weeding-
32

out process is to ﬁnd a person already working for Facebook who
knows you. This is the same idea as LinkedIn, whose algorithms
search for connecƟons between an individual and the person they
are trying to meet. Although the idea is certainly neat on one
level, it can also have the unfortunate eﬀect of excluding a
signiﬁcant number of people from diverse regional, social and
cultural backgrounds.
The other problem that Ming explains (and to a scienƟst this is
almost certainly worse) is that previous hiring strategies have
proven inaccurate when it comes to forecasƟng who will succeed
in a workplace role. In a place like Silicon Valley, where the
supposed objecƟvity of data-driven hiring is prized above all else,
this is parƟcularly unforgivable. Google, for example, employs
what it calls the Lake Wobegon Strategy for hiring—named aŌer
American humorist Garrison Keillor’s claim that he grew up on the
ﬁcƟƟous Lake Wobegon, where “all the women are strong, all the
men are good-looking, and all the children are above average.”
According to Google’s Lake Wobegon Strategy, to maintain a high
level of skill in an organizaƟon that is doubling in size each year,
new employees should be above the mean skill level of current
Googlers in order to be hired. To measure something as
unquanƟﬁable as “skill,” Google tradiƟonally placed a strong
emphasis on academic results. A person’s GPA and university
were considered a strong predictor of workplace success, since
they showed past evidence of rigor, sƟckability and the ability to
meet deadlines. An individual who studied computer science at
MIT might not be the best computer scienƟst in the world, but it
is surely safe to assume that they are at least “good enough” to
have goƩen into the course in the ﬁrst place. Pick a random
person who didn’t go to MIT, on the other hand, and while there
is sƟll the chance that they will be brilliant, the likelihood that
they will be terrible is far higher. In a risk-averse industry where
people are rarely given a bonus for beƫng on the long shot that
pays oﬀ—but could very easily lose their job for hiring a person
33

deemed unsuitable for a parƟcular role—it is no wonder that
many high-tech companies would choose to play it safe.
As datasets piled up, however, and algorithms began scouring
that informaƟon for paƩerns, Google realized that the metrics
they were using to predict job performance (including school
grades, exam results, previous job experience and even face-to-
face interviews) oﬀered very liƩle in the way of accuracy when
forecasƟng who it was that was likely to excel in a parƟcular
posiƟon. This in itself was not an enƟrely new revelaƟon. In the
1960s, telecommunicaƟons giant AT&T conducted IQ tests on
low-level managers and then followed them for the next 20 years
of their career to see how each employee progressed within the
organizaƟon. In line with popular wisdom, AT&T’s assumpƟon
was that those individuals with a higher IQ would rise to the
highest level, while those with lower IQs would seƩle lower down
in the company, like water that ﬁnds its own level. Instead, what
was discovered was that IQ scores explained less than 15 percent
of the variance between managers in terms of career
achievement. The rest was an unmeasurable combinaƟon of
personality traits, emoƟonal aƩributes, sociability and a number
of other characteristics that can determine success.
Once Google realized the need to open up the parameters of
what it looked for in a new employee, the cultural space was
cleared for Gild. Instead of hiring from a populaƟon of tens of
thousands each year, high-tech companies now have a populaƟon
consisƟng of almost everybody to choose from. AŌer all, it is
never knowable where the diamond in the rough might pop up.
To hedge bets, a prospecƟve employee being looked over by Gild
is measured on pracƟcally every scrap of informaƟon in existence
about them. Vivienne Ming likens the diﬀerence between this
approach and that of a human recruiter to a human chess player
compeƟng against Deep Blue—the supercomputer that famously
defeated grand master Garry Kasparov in what Newsweek
described as “The Brain’s Last Stand.” “The computer is ploƫng
34

every possible move and choosing the opƟmal move,” she says.
“Chess experts are not. They have implicitly discarded the vast
majority of possible moves and are only considering two, three,
four possibiliƟes. They just happen to be great ones.” That’s also
true of people making hiring decisions, Ming suggests—only that
the metrics considered in this case don’t happen to be so great.
By looking at as many data points as possible about a person,
anomalous factors like whether a person being interviewed was
having an oﬀ day are bypassed. Gild addiƟonally looks at where
individuals spend Ɵme online, since this has been shown to be a
strong predictor of workplace skills. “If you spend a lot of Ɵme
blogging it suggests that you’re not quite as good a programmer
as someone who spends their Ɵme on Quora,” Ming says,
referring to the quesƟon-and-answer website founded by two
former Facebook employees. Even TwiƩer feeds are mined for
their insights, using semanƟc and senƟment analysis. At the end,
factors are combined to give prospecƟve employees a “Gild
Score” out of 100.
“It’s very cool if you’re geeky about algorithms, but the really
important take-away is that what we end up with is truly
independent dimensions for describing people out in the world,”
she says. “We’re talking about algorithms whose entire intent and
purpose is to aggregate across your enƟre life to build up a very
accurate representation of who you are.”
35

Quantifying Human Potential
Gild is not the only interested party looking to open up the
number of metrics individuals are judged on in the workplace. In
2012, three universiƟes carried out a study as to whether or not
Facebook proﬁles can be used to predict how successful a person
is likely to be at their job. By analyzing photos, wall posts,
comments and proﬁles, researchers argued that quesƟons such as
“Is this person dependable?” and “How emoƟonally stable is this
person?” can be answered with a high level of certainty.
Favorable evaluaƟons were given to those students who had
traveled, had more friends and demonstrated a wide range of
hobbies and interests. Partying photos didn’t necessarily count as
negaƟve either, since people depicted as parƟers were
characterized as extroverted and friendly, both viewed as ideal
qualiƟes for workplace success. Six months aŌer making their
iniƟal predicƟons, the study’s authors followed up with the
employers of their 56 test subjects and found a strong correlaƟon
between job performance and the Facebook scores that had been
awarded for traits such as conscienƟousness, agreeability and
intellectual curiosity. Their conclusion was that Facebook proﬁles
are strong predictors since candidates will have a harder Ɵme
“faking . . . their personaliƟes” on a social network than they
would in a conventional job interview.15
This aƫtude toward quanƟﬁcaƟon and staƟsƟcal analysis is
one that is heard regularly from exponents of The Formula. It
owes its biggest debt to the Belfast-born mathemaƟcal physicist
and engineer Lord Kelvin, who suggested that the thing that
cannot be measured cannot possibly be improved. In the second
half of the 19th century, a cousin of Charles Darwin named Francis
Galton seized upon Kelvin’s suggesƟon as the basis for a number
of unusual studies designed to measure the unmeasurable. For
example, infuriated by the vagueness associated with a term like
“beauty,” Galton set out to create a “beauty map” of the BriƟsh
36

Isles, whereby each woman he came into contact with was
classiﬁed as either “aƩracƟve, indiﬀerent, or repellent.” London,
he claimed, had the highest concentraƟon of beauƟful women,
while Aberdeen was mathemaƟcally proven to be home to the
ugliest.16 Another study saw him measure listlessness by
construcƟng a uniﬁed “Measure of Fidget,” as Galton felt the
“muƟny of constraint” epitomized in a ﬁdget lent “numerical
expression to the amount of boredom expressed by [an]
audience.” The more ﬁdgeƟng, the higher the levels of
boredom.17 Even God wasn’t safe from quanƟﬁcaƟon, since
Galton saw no reason that the “eﬃcacy of prayer” (the rate at
which prayers were answered versus ignored) should not be a
“perfectly appropriate and legiƟmate subject of scienƟﬁc
inquiry.”18
It is into this quanƟﬁed space that Silicon Valley start-up Knack
enters the picture. Founded by Israeli entrepreneur Guy HalŌeck,
Knack has a decepƟvely simple aim: to use a combinaƟon of
gaming technology, machine-learning algorithms and the latest
ﬁndings from behavioral science to come up with universal
measures for terms like “quick-thinking,” “percepƟveness,”
“empathy,” “insighƞulness,” “spontaneity” and “creaƟvity.” By
doing this, HalŌeck says that he hopes to trigger a “fundamental
change in the human capital space” that will seek to unlock an
individual’s previously untapped potential.
The basis for Knack’s work is an insight that has been explored
by psychologists for the last half century: that the way we play
games can be used to predict how we behave in the real world.
“Even though to your eye, your behavior in a game does not
necessarily characterize your real-world behavior, it is highly likely
that the way you play a game and another person plays that same
game would reveal diﬀerences about personality and the way
that your brain works,” HalŌeck says. “Your working memory,
your strategic thinking, your risk-taking—these are all things
which are manifested in how we game.”
37

Knack’s games currently include Wasabi Waiter and Balloon
Brigade. Both are straighƞorward, pick-and-play aﬀairs that
nonetheless oﬀer the player a number of diﬀerent ways to
compete. In Wasabi Waiter, for example, players take on the role
of a waiter and chef as they take customers’ orders and then
prepare the dish that matches his or her facial expression. This
expression might be “happy,” “sad,” “angry” or “any mood” in the
event that the player is unsure. When a customer ﬁnishes eaƟng,
the player brings their plate back to the sink and starts the
process again with someone new.
This may appear simple, but beneath the surface this is
anything but. In a game, literally everything is measurable: each
acƟon, message, item and rule is composed of raw data. For
every millisecond of play in Wasabi Waiter or Balloon Brigade,
hundreds of data variables are gathered, processed and analyzed
based on the decisions players make, the speed at which they do
things, and the degree to which their game playing changes over
time. What Halfteck perceives to be the accompanying behavioral
qualiƟes are then teased out using machine-learning tools and
data-mining algorithms. “This is a very rich data stream we’re
collecƟng,” HalŌeck says. “It really allows us to get closer to the
unique behavioral genome of a person.”
There is, however, an innate danger in aƩempƟng to quanƟfy
the unquanƟﬁable. When it comes to taking complex ideas and
reducing these to measurable elements, the most famous criƟque
came from evoluƟonary biologist Stephen Jay Gould. In his 1981
book, The Mismeasure of Man, Gould warned about the dangers
of converƟng concepts such as “intelligence” into simpliﬁed
measures such as IQ. Gould’s concerns weren’t just about the
potenƟal dangers of abstracƟon, but about the ways in which
apparently objecƟve truths can be used to back up human biases,
rather than to expose genuine insights. In his own words, The
Mismeasure of Man is an aƩack on “the abstracƟon of
intelligence as a single enƟty, its locaƟon within the brain, its
38

quanƟﬁcaƟon as one number for each individual, and the use of
these numbers to rank people in a single series of worthiness,
invariably to ﬁnd that oppressed and disadvantaged groups—
races, classes, or sexes—are innately inferior and deserve their
status.”19 Measurement and reducƟonism, of course, go hand in
hand. Each Ɵme we begin to measure something, we lose
whatever it is that the measurement tool is not designed to
capture, or that the person measuring is not aware of the need to
measure. As I will discuss in the coming chapters, technological
aƩempts to create simple quanƟﬁable measures for ideas like
“creaƟvity” and “love” meet with ﬁerce opposiƟon—largely
because the concepts are far from simple.
But HalŌeck disagrees that the beauty of terms like “empathy”
and “insighƞulness” is in their abstract amorphousness. “We are
talking about an ever-expanding universe of things that are being
measured,” he says. “In the case of Knack, we’re moving in a
posiƟve direcƟon from a paradigm where people are being
measured on a single dimension, to one in which people are
measured in a polydimensional way on exponenƟally more
aspects of their personality. It’s not just about ‘intelligence,’ but
rather the sum total of the human condiƟon. It’s far more
nuanced than anything we’ve seen before.”
39

Your Life According to Twitter
In November 2013, I wrote an article for Fast Company about two
computer science researchers who had created an algorithm that
used the informaƟon posted in your TwiƩer feed to generate
customized user biographies.20 It was a fascinaƟng experiment in
the subject of “topic extracƟon” and—based on the feedback and
the number of hits the arƟcle received—I was not alone in feeling
that way. TwiƩer, explained one of the study’s authors, Jiwei Li,
was the perfect tool for researchers. First, it encouraged its users
to keep diaries in a publicly accessible medium. Second, the micro-
blogging site’s imposed 140-character limit for messages forced
users to be concise: compressing complex life events into a few
short sentences.
Since reading through a person’s TwiƩer feed going back a
number of years could prove prohibiƟvely Ɵme consuming
(parƟcularly if you were doing this for a large number of people,
as you might if you were the boss wanƟng to know more about
the people who worked for you), the algorithm’s job was to scan
through this informaƟon and output it in a more accessible and
easily readable format; pulling out only the relevant Ɵdbits that
might inform you about the major events in a person’s life. The
algorithm worked by analyzing the contents of each tweet and
dividing these into “public” and “private” categories (say, your
opinion on a major sporƟng event versus your own birthday) and
then subdividing each of these categories again into “general”
and “speciﬁc” headings. “General” events would typically be
things like complaints about the commute to work or comments
on your weekly yoga class, and would show their predictability by
virtue of recurring over a long period of Ɵme. “Speciﬁc” events,
on the other hand, would be life-changing events like a person’s
wedding, or the oﬀer of a new job, and would frequently be the
subject of a large amount of acƟvity taking place over a short
time span.
40

Unsurprisingly, it was this last “private-speciﬁc” event that was
of most interest when it came to generaƟng a mini-biography. As
anyone who has ever read a popular biography will know, more
space is typically given to the unique events in a person’s life that
are speciﬁc to them rather than a broader contextual look at
where they ﬁt within the wider society. Walter Isaacson’s Steve
Jobs, for example, focuses far more on Jobs’s work creaƟng the
iPhone than it does on what he saw on his drive to and from his
Palo Alto home each day. 21 But assuming that “public” or
“general” observaƟons are simply noise to be ﬁltered out risks
grossly simplifying a person’s life by viewing them as exisƟng in a
context-free void. Wherever you are on the poliƟcal spectrum, it
is impossible to deny that public events like general elecƟons or
overseas conﬂicts have a bearing on the lives of the typical
individual. The same is true of events that recur regularly, but are
nonspeciﬁc in nature. The worker’s commute to the oﬃce, or the
inner-city family that has its power turned oﬀ or lives in a crime-
ridden neighborhood, may not be “speciﬁc” but does as much, if
not more, to explain their circumstances than which geo-tagged
location they were in when they proposed to their partner.
I am not enƟrely blaming the two researchers for this. Their
concepƟon of the self is a neat one, which ﬁts neatly within the
Western value system that presents the individual as both a
unique and fundamentally autonomous being. This idea forms not
just the basis of the social sciences, but also poliƟcs, economics
and the legal system. In poliƟcs, the self is the ciƟzen who
parƟcipates in democracy through voƟng and other poliƟcal
acƟviƟes. In a market economy, the self is the opƟmizer of costs
and beneﬁts in such a way that they correspond with a person’s
best interests. In the legal system (which I explore in more detail
in Chapter 3), the self is usually imagined as an agent who is
responsible for his or her own behavior within society. What
underlines all of these interpretaƟons is the noƟon that at its root
the self is a profoundly rational entity.
41

In today’s digital world the concepƟon of the self relies largely
on the inferences of algorithms—comparing individual qualiƟes
against large data sets of “knowable” qualiƟes to ﬁnd
correlaƟons. These are, in a very real sense, formulas of the self.
Some are extremely complex and depend on gathering as many
data points as possible about parƟcular people before coming up
with conclusions. Others are about simplicity: as with abstract art,
taking the broadest possible “shapes” that deﬁne a human being,
reducƟo ad absurdum. One start-up named Hunch claims that
with just ﬁve data points (in other words, a user answering ﬁve
quesƟons) it can answer pracƟcally any consumer preference
quesƟon with 80 to 85 percent accuracy. YouAreWhatYouLike,
meanwhile, oﬀers to create detailed proﬁles of parƟcular users by
analyzing the common associaƟons of their Facebook “likes” with
a data set of “social dimensions.” We are told, for instance, that
users who “like” online art community deviantART.com are liberal
in their poliƟcal leanings, while those who “like” NASCAR tend
toward the conservaƟve. Other “likes” prove similarly predicƟve
of personality types who might be “distrusƞul” or “reserved.” A
similar study carried out by University of Cambridge researchers in
2013 suggested that algorithms provided with a dataset of 58,000
American Facebook users were able to accurately predict
qualiƟes and traits, including race, age, IQ, sexual preference,
personality, substance use and poliƟcal views—all based upon
“likes.”22 Another service—TweetPsych—claims to use algorithms
to score a person’s emoƟonal and intellectual quoƟents based
upon the topics they choose to tweet about, including learning,
money, emoƟons and anxiety. Yet more studies have been shown
to be able to deduce gender, sexual orientaƟon, poliƟcal
preference, religion and race with a greater than 75 percent level
of accuracy. A 2010 invesƟgaƟon by psychologist Tal Yarkoni of
the University of Colorado at Boulder analyzed the words in 695
blogs and compared these to the personaliƟes of their owners as
revealed through personality tests. Yarkoni suggested that
42

neuroƟc bloggers are more likely to use words like “awful” and
“lazy,” conscienƟous ones overuse “completed,” and generally
agreeable ones fall back on describing things as “wonderful.”23
It is the gulf between the idea of the autonomous individual
and the algorithmic tendency to view the individual as one
categorizable node in an aggregate mass that can result in The
Formula’s equivalent of a crisis of self. Writing in 2012, a Facebook
user commented on the new Timeline feature being rolled out in
the social network’s user interface at the Ɵme. Unlike the
previous Facebook user interface, the Timeline had the
narraƟvizing eﬀect of proceduring history into a series of events
(jobs, relaƟonships) with unknown categories marked by blank
spaces so that the implicaƟon was that the user should conƟnue
adding materials to their Timeline—retroacƟvely tracing their
own personal narrative back until they reach the category “born.”
“I feel betrayed by . . . an interface that appears to give so many
choices on the surface, while limiƟng almost every bit of our
creaƟve endeavor to the predeﬁned and prepackaged boxes and
categories within which we’re supposed to ﬁnd a place,” the user
noted.
It hurts us all, in diﬀerent, small ways. Sure, I feel
ﬁne clicking the “female” category, but I know at
least two dozen friends who wouldn’t be able to
choose a box. I’m supposed to declare a
“hometown” but I’ve not had one for more than
twenty years, so that’s not useful at all. I now have
to mark places on a map, or accept the default map
that appeared on my proﬁle just this aŌernoon . . .
What if I don’t want to be deﬁned by Ɵme or any
other moment that Facebook has determined is
“relevant” in my life?24
43

Big Brother, Sort Of
A number of cultural criƟcs have commented upon the large
number of ways in which bureaucraƟc measures have intensiﬁed
under neoliberalism—despite its presentaƟon as being profoundly
and fundamentally antibureaucratic by nature.
Such criƟques can certainly be applied to those high-tech
companies in thrall to The Formula. One example is the high-tech
start-up CourseSmart, which allows teachers to surveil their
students even when they are away from the classroom. Much like
e-book analyƟcs that can be fed back to publishers (something
that I will describe later on in this book), CourseSmart uses
algorithms to track whether students are skipping pages in their
textbooks, not highlighƟng signiﬁcant passages, hardly bothering
to take notes, or even failing to study at all. In April 2013,
CourseSmart was the subject of an arƟcle in the New York Times,
under the headline “Teacher Knows If You’ve Done the E-
Reading.” The story related the plight of a teacher who tracked
70 students in three of his classes. Despite one student regularly
scoring high marks in mini-tests, CourseSmart alerted the teacher
that his student was doing all of their studying the night before
tests, as opposed to taking a long-haul approach to learning. The
arƟcle quoted the university’s school of business dean as
describing the service as “Big Brother, sort of, but with a good
intent.”25 According to the story:
Students do not see their engagement indexes
(CourseSmart’s proprietary analyƟcs tool) unless a
professor shows them, but they know the books are
watching them. For a few, merely hearing the
number is a shock. Charles Tejeda got a C on the last
quiz, but the real revelaƟon that he is struggling was
a low CourseSmart index.
“They caught me,” said Mr. Tejeda, 43. He has
44

two jobs and three children, and can study only late
at night. “Maybe I need to focus more,” he said.
On the surface, CourseSmart oﬀers considerably more freedom
than the kind of factory model “industrial schooling” that rose to
prominence with the Industrial RevoluƟon, piƫng warden-
teachers against prisoner-students. Students are less classroom-
bound and are aﬀorded opportuniƟes to study on their own. (Or,
at least, ostensibly on their own.) However, such tools actually
represent a more conƟnuous form of control system based on
increasingly abstract enƟƟes like “engagement.” AŌer all, as the
New York Times story demonstrates, a person could receive a
“satisfactory” C grade, only to fail a class on engagement.
A parallel to CourseSmart is the kind of deep data analyƟcs
Google uses to track its own workforce. Like many high-tech
businesses, Google models itself as a libertarian utopia: the type
of company where employees used to be allowed one extra day
per week to pursue their own lines of inquiry, and are as likely to
spend their Ɵme ascending Google’s indoor rock-climbing wall or
having free food served up to them by a former Grateful Dead
chef as they are to be coding. However, as Steven Levy points out
in In the Plex, his 2011 study of Google, the search leviathan’s
apparent loopiness is “the crazy-like-a-fox variety and not the kind
calling for straightjackets.”26 Despite Google’s widely publicized
quirks, its irreverent touches are data-driven to a fault. “At Ɵmes
Google’s largesse can sound excessive,” notes an arƟcle in Slate.
“Yet it would be a mistake to conclude that Google doles out such
perks just to be nice. [The company] rigorously monitors a slew of
data about how employees respond to beneﬁts, and . . . rarely
throws money away.”27
For instance, there is a dedicated team within Google called
the People AnalyƟcs group, whose job is to quanƟfy the
“happiness” of employees working for the company. This is done
using “Googlegeist,” a scienƟﬁcally constructed employee survey,
45

which is then mined for insights using state-of-the-art proprietary
algorithms. An example of what the People AnalyƟcs team does
occurred several years ago, when Google noƟced that a larger-
than-normal number of female employees were leaving the
company. Drilling down with data-mining tools, the People
AnalyƟcs group discovered that this wasn’t so much a “woman”
problem as it was a “mother” problem: women who had recently
given birth were twice as likely to leave Google as its average
departure rate. The most cost-effective answer, it was concluded,
was to increase maternity leave from the standard 12 weeks of
paid absence to a full ﬁve months. Once the problem had been
idenƟﬁed and acted upon, the company’s aƩriƟon rate for new
mothers dropped by 50 percent.
Similar data-driven insights are used to answer a plethora of
other quesƟons. Just how oŌen, for example, should employees
be reminded to contribute to pension plans, and what tone should
best be used when addressing them? Do successful middle
managers possess certain skills in common, and could these be
taught to less successful managers? And what is the best way to
maximize happiness and, thus, eﬃciency in staﬀ? A salary
increase? Cash bonus? Stock options? More time off?
For all its hiding behind the image of soŌ “servant leadership”
the real thing an enƟty such as Google’s People AnalyƟcs group
returns to prominence is the concept of “Taylorism.” Created in
the early 20th century by engineer Frederick Taylor, the ideas
behind Taylorism were outlined in a 1911 book called The
Principles of ScienƟﬁc Management.28 At the center of Taylor’s
beliefs was the idea that the goal of human labor and thought
should be increased eﬃciency; that technical calculaƟon is always
superior to human judgment; that subjecƟvity represents a
dumbing-down of clear-thinking objecƟvity; and that whatever is
unable to be quanƟﬁed either does not exist or has no value. “It
is,” he argued, “only through enforced standardizaƟon of
methods, enforced adopƟon of the best implements and working
46

condiƟons, and enforced cooperaƟon that . . . faster work can be
assured.”
47

Work Faster and Happier
Of course, it’s not just about faster work. As the quanƟﬁcaƟon of
“happiness” and “engagement” demonstrate, it is no longer
enough to simply be an eﬀecƟve laborer. A person must also be
an affective laborer, oﬀering “service with a smile.” In this way it
is necessary to ask the degree to which The Formula is genuinely
improving working condiƟons, or whether it is simply (to quote
cultural criƟc Paul Virilio) transforming workers into unwiƫng
parƟcipants in a Marxist state pageant, “miming the joys [of]
being liberated”—with full knowledge that anything other than
full enthusiasm will be noted down against their CourseSmart-
style engagement index and unearthed by an algorithm in Ɵme
for a future job interview.
Here it is worth turning once more to the work of Alvin Toﬄer,
whose concept of “demassiﬁcaƟon” laid out many of the
principles described in this chapter. In The Third Wave, Toﬄer
quesƟons why it is that everyone should be asked to start work at
9 A.M. and ﬁnish at 5 P.M. each day. By changing this massiﬁed
approach to a more individual one, centered around the self,
Toﬄer argues that both employers and employees would
experience beneﬁts. The former group could use insights about
the Ɵmes their paid employees are at their most producƟve to
maximize eﬃciency. Employees, meanwhile, could arrange their
working hours around their other nonwork duƟes, or simply their
natural biological rhythms (which we now know can be
ascertained through wearable sensors). Of course, what sounded
a utopian formula to Toﬄer now exists as the commonplace
“ﬂexiƟme” approach to employment, in which many companies
have laid oﬀ their permanent workforce in favor of a free-ﬂoaƟng
pool of part-Ɵme, ﬁxed-term and freelance workers lacking in
beneﬁts and job security. In Becky Hogge’s Barefoot into
Cyberspace, the author relates this directly to the dream of the
techno-soluƟonists, noƟng how “the eighƟes personal computer
48

gurus are . . . the same folk who went around major corporaƟons
advising them on ways to decouple their fortunes from those of
their employees, ushering in the era of precarious employment
that is my generaƟon’s norm.” 29 In the gamiﬁed collapse of work
into play and play into work, concepts like performance-based pay
(presented as another level of personalizaƟon) mean that even
those jobs that do not immediately lend themselves to increased
speed and efficiency can be subjected to The Formula.
This neo-Taylorist dynamic becomes more apparent the further
you look down the high-tech food chain. In Amazon’s warehouses,
for example, product pickers (known as “fulﬁllment associates”)
are issued handheld computers that transmit instrucƟons to
reveal where individual products can be picked up or dropped oﬀ.
Because of the size of Amazon’s warehouses, a rouƟng algorithm
is used to work out the shortest possible journey from point to
point. That is not all the handheld computers do, however. They
also collect a constant, real-Ɵme stream of data that monitors
how fast employees walk and complete individual orders, thus
quanƟfying their producƟvity. 30 Like the bomb-loaded bus in the
movie Speed, workers must maintain a certain minimum speed,
or else see their jobs go up in smoke. As with Henry Ford’s
assembly lines, so too here does machinery determine the pace of
work. A warehouse manager at Amazon has been quoted as
describing workers as “sort of like a robot, but in human form.”31
An arƟcle for Fast Company paints a depressingly Orwellian
picture:
An Amazon fulﬁllment associate might have to walk
as far as 15 miles in a single shiŌ, endlessly looping
back and forth between shelves in a warehouse the
size of nine soccer ﬁelds. They do this in complete
silence, except for the sound of their feet. The
atmosphere is so quiet that workers can be ﬁred for
even talking to one another. And all the while,
49

cardboard cutouts of happy Amazon workers look
on, cartoon speech bubbles frozen above their
heads: “This is the best job I ever had!”32
Similar reports can be seen elsewhere. In Tesco warehouses in
the UK, workers are made to wear arm-mounted electronic
terminals so that managers can grade them on how hard they are
working. Employees are allocated a certain amount of Ɵme to
collect an order from a warehouse and then complete it. If they
meet the target, they are awarded a 100 percent score, rising to
200 percent if the task is completed in double Ɵme. Conversely,
scores fall dramaƟcally in situaƟons where the task takes longer
than expected.33
50

Decimated-Reality Aggregators
Speaking in October 1944, during the rebuilding of the House of
Commons, which had sustained heavy bombing damage during
the BaƩle of Britain, former BriƟsh prime minister Winston
Churchill observed, “We shape our buildings; thereaŌer they
shape us.”34 A similar senƟment might be said in the age of The
Formula, in which users shape their online proﬁles, and from that
point forward their online proﬁles begin to shape them—both in
terms of what we see and, perhaps more crucially, what we
don’t.
WriƟng about a start-up called Nara, in the middle of 2013, I
coined the phrase “decimated reality aggregators” to describe
what the company was trying to do.35 Starting out as a restaurant
recommender system by connecƟng together thousands of
restaurants around the world, Nara’s ulƟmate goal was to
become the recommender system for your life: drawing on what
it knew about you from the restaurants you ate in, to suggest
everything from hotels to clothes. Nara even incorporated the
idea of upward mobility into its algorithm. Say, for example, you
wanted to be a wine connoisseur two years down the line, but
currently had no idea how to tell your Chardonnay from your
ChianƟ. Ploƫng a path through a mass of aggregated user data,
Nara could subtly poke and prod you to make sure that you ended
up at a parƟcular end point aŌer a certain amount of Ɵme. If you
trained Nara’s algorithms to recognize what you wanted, Nara’s
algorithms could then train you to ﬁt a certain desired mold. In
this way, “decimated reality” was a way of geƫng away from the
informaƟonal overload of the Internet. Users wouldn’t see more
opƟons than they could handle—they would see only what was
deemed relevant.
“The Internet has evolved into a transacƟonal machine where
we give our eyeballs and clicks, and the machine gives us back
adverƟsing and cluƩer,” says Nathan Wilson, chief technology
51

oﬃcer at Nara Logics Inc. “I’m interested in trying to subvert all
of that; removing the cluƩer and noise to create a more eﬃcient
way to help users gain access to things.” The problem, of course,
is that in order to save you Ɵme by removing the “cluƩer” of the
online world, Nara’s algorithms must make constant decisions on
behalf of the user about what it is that they should and should not
see.
This eﬀect is oŌen called the “ﬁlter bubble.” In his book of the
same Ɵtle, Eli Pariser notes how two diﬀerent users searching for
the same thing using Google will receive very diﬀerent sets of
results.36 A liberal who types “BP” into his browser might get
informaƟon about the April 2010 oil spill in the Gulf of Mexico,
while a conservaƟve typing the same two leƩers is more likely to
receive investment informaƟon about the oil company. Similarly,
algorithms are more likely to respond to a female search for
“wagner” by direcƟng the user toward sites about the composer
“Richard Wagner,” while a male is taken as meaning “Wagner
USA” paint supplies. As such, what is presented by search
algorithms is not a formula designed to give ideologically
untampered answers but precisely the opposite: search results
that ﬂaƩer our personal mythologies by reinforcing what we
already “know” about parƟcular issues, while also downgrading in
importance the concerns that do not marry up to our exisƟng
worldview.
Despite the apparent freedom of such an innovaƟon (surely,
personalizaƟon equals good?), it is not diﬃcult to see the all-too
apparent downside. Unlike the libertarian technologist’s pipe
dream of a world that is free, ﬂat and open to all voices, a key
component of code and algorithmic culture is soŌware’s task of
sorƟng, classifying and creaƟng hierarchies. Since so much of the
revenue of companies like Google depends on the cogniƟve
capital generated by users, this “soŌware sorƟng” immediately
does away with the idea that there is no such thing as a digital
caste system. As with the “ﬁlter bubble,” it can be diﬃcult to tell
52

whether 
the 
endless 
disƟncƟons 
made 
regarding 
geo-
demographic proﬁles are helpful examples of mass customizaƟon
or exclusionary examples of coded discriminaƟon. Philosopher
Félix GuaƩari imagined the city in which a person was free to
leave their apartment, their street or their neighborhood thanks
to an electronic security card that raised barriers at each
intersecƟon. However, while this card represents freedom, it also
represents repression, since the technology that opens doors for
us might just as easily keep them shut. Much the same might be
said of the algorithm, which is directly and automaƟcally
responsible for providing social and geographical access to a
number of goods, services and opportunities for individuals.
The idea that a certain class of user can be willfully
inconvenienced in favor of another more desirable class is a key
part of market segmentaƟon. At various Ɵmes, UK supermarkets
have invesƟgated the possibility of charging a premium for food
shopping at peak shopping hours, in an eﬀort to deter the “cash-
poor but Ɵme-rich” customers from negaƟvely impacƟng upon
the shopping experience of the more desirable “cash-rich but
Ɵme-poor” professionals.37 Many airlines oﬀer premium schemes
that allow valuable business travelers to pay an extra surcharge
to bypass certain border controls. SorƟng processes divide
passengers into groups of those enrolled on the scheme and those
that are not. In the case of the former premium group,
passengers are oﬀered parking spaces close to the airport
terminal and then allowed to pass through to dedicated
members’ lounges with speed. In the case of the latter “cash-poor
but Ɵme-rich” group, assigned parking spaces are typically a long
distance from the airport terminal, and passengers are excluded
from VIP lounges and forced to endure lengthy “check-in” Ɵmes
and security queues. A similar brand of thinking is behind schemes
set up in ciƟes such as Los Angeles, San Diego, Toronto,
Melbourne and Tokyo, in which privately funded highways are
built for use by aﬄuent motorists, who access them by paying a
53

premium fee. To keep these highways exclusive, algorithms are
used to esƟmate the exact level of price per journey that is likely
to deter enough drivers so as to guarantee free-ﬂowing traﬃc,
regardless of how bad congesƟon might be on the surrounding
public highway system.38
54

Squelching the Scavengers
The digital world is not immune to these pracƟces. In the past,
Internet networking provider Cisco has referred to its less-than-
premium consumers as a “scavenger class” for whom the
company provides “less-than-best-eﬀort services” to certain
applicaƟons. At Ɵmes of Internet congesƟon and slow download
Ɵmes, traﬃc can be (in Cisco’s words) “squelched to virtually
nothing” for scavenger-class users, while more valued business
users are provided with access to Cisco’s answer to free-moving,
privately funded highways. Similar disƟncƟons might be made
elsewhere. A markeƟng brochure from communicaƟons company
the Avaya CorporaƟon similarly promises that its bespoke systems
allow for algorithms to compare the numbers of individual call-
center callers to a database, and then route calls through to
agents as high-priority if the caller is in the top 5 percent of
customers. In this scenario, when the agent picks up the call, they
hear a whispered announcement that the caller they are speaking
with is “Top 5.” Much the same technology could be put into
pracƟce in situaƟons where algorithms are used to ﬁlter the
personaliƟes of individual callers. Callers might receive service
according to personality types, so that more lucraƟve customers
with an increased likelihood of losing paƟence with a service
quicker could be ranked above personality types likely to
procrasƟnate and ask lots of quesƟons, while being unlikely to
spend large amounts of money with a company in the future.
Perhaps the most granular example of this “human soŌware
sorƟng” can be seen with the algorithm-driven idea of
“diﬀerenƟal pricing.” This is something a number of online stores,
including Amazon, have already experimented with. Without the
added gloss of markeƟng speak, diﬀerenƟal pricing means that
certain users can be charged more than others for the same
product. In the abstract, the algorithms used for this are no
diﬀerent from those that predict that, since a person bought the
55

Harry PoƩer and Twilight books, they might also enjoy the
Hunger Games trilogy. It is presented as another level of
personalizaƟon, in line with the website that remembers your
name (and, more importantly, your credit-card details). In
September 2012, Google was granted a patent for “Dynamic
Pricing on Electronic Content,” which allows it to change the
listed price of online materials, such as video and audio
recordings, e-books and computer games, based upon whether its
algorithms determine a user is more or less likely to purchase a
parƟcular item. The ﬁled patent was accompanied by illustraƟons
gleefully suggesƟng that certain users could be convinced to pay
up to four Ɵmes what others are charged for the exact same
digital ﬁle.39 In other words, if Google’s algorithms “know” that
you are suscepƟble to tween-fodder like Harry PoƩer and
Twilight, based on what you have searched for in the past, it can
ensure that you pay through the nose for The Hunger Games
—while also enƟcing the person who has only ever demonstrated
the slightest of interests in teenage wizards and sparkly vampires
to buy the product by lowering the price to tempt them.
Recent years have also seen a rise in so-called emoƟon sniﬃng
algorithms, designed to predict a user’s emoƟonal state based on
their tone of voice, facial expression—or even browsing history. A
study carried out by MicrosoŌ Research analyzed people’s phone
records, app usage and current locaƟon, and then used these
metrics to predict their mood. According to MicrosoŌ, the
algorithm’s daily mood esƟmates start at 66 percent accuracy
and gradually increase to 93 percent aŌer a two-month training
period.40 
Since 
mood 
signiﬁcantly 
inﬂuences 
consumer
preferences, informaƟon like this could prove invaluable to
marketers.41 Let’s say, for example, that your computer or
smartphone determines that you’re likely to be feeling
parƟcularly vulnerable at any given moment. In such a scenario it
might be possible to surrepƟƟously raise the price of products you
are likely to be interested in since an algorithm designed to “sniﬀ”
56

your mood has determined that you’re staƟsƟcally more likely to
be susceptible to a purchase in this state.
Free-market idealists might argue in favor of such an approach.
In the same way that an upmarket restaurant charges more for
the exact same bottle of beer that a person could buy for half that
price somewhere else, so concepts like diﬀerenƟal pricing could
be used to ensure that a person pays the exact price he or she is
willing to spend on a product—albeit with more scienƟﬁc
precision. However, while this is undoubtedly true, the more
accurate analogy may be the restaurant whose waiters riﬂe
through your belongings for previous receipts before deciding
what they think you ought to be charged for your food and drink.
DiﬀerenƟal pricing could be used to even the playing ﬁeld
(everyone pays 1/250 of their weekly salary for a beer, for
example). More likely, however, it could be used to do just the
opposite: to raise the price of parƟcular goods with the stated
aim of marginalizing or driving away those less lucraƟve—and
therefore less desirable—users. A high-fashion brand might, for
instance, want to avoid becoming associated with a supposed
“undesirable” consumer base, as happened to Burberry in the
early 2000s. Since it cannot outright bar parƟcular groups from
purchasing its products, the company could pay to have its
products disappear from the algorithmic recommendaƟons given
to those who earned under $50,000 per year or—in the event
that its products were speciﬁcally searched for—priced out of
their range.
The real problem with this is the invisibility of the process. In
the same way that all we see are the end results when algorithms
select the personalized banners that appear on websites we
browse, or determine the suggested ﬁlms recommended to us on
Neƞlix, so with diﬀerenƟal pricing are customers not informed
that they are being asked to pay more money than their next-
door neighbor. AŌer all, who would conƟnue shopping if this were
the case? As Joseph Turow, a professor at the University of
57

Pennsylvania’s Annenberg School for CommunicaƟon and
frequent writer about all things markeƟng, has pointed out in an
arƟcle that appeared in the New York Times: “The ﬂow of data
about us is so surrepƟƟous and so complex that we won’t even
know when price discriminaƟon starts. We’ll just get diﬀerent
prices, different news, different entertainment.”42
58

The Discrimination Formula?
A number of Internet theorists have argued that in the digital
world, previous classiﬁcaƟons used for discriminaƟon (including
race, gender or sexuality) will fall away—if they haven’t already.
Alvin Toﬄer’s Third Wave idenƟﬁes a number of individuals and
groups subtly or openly discriminated against during the last
centuries and argues that this marginalizaƟon is the product of
Second Wave socieƟes. According to Toﬄer, in the unbundled,
personality-driven Third Wave society such discriminatory
pracƟces will slink oﬀ into the digital ether. This is a popular
utopian view. In Mark Hansen’s essay “DigiƟzing the Racialized
Body, or The PoliƟcs of Common Impropriety,” the author builds
on this point by suggesƟng that the web makes possible an
unprecedented number of opportuniƟes for ethical encounters
between people of different races, since race as a visual signifier is
rendered invisible.43 The mistake made by both Toﬄer and
Hansen is assuming that discriminaƟon is always collecƟvist in
nature. The Formula suggests that this is far from the case. In an
age in which power no longer has to be embodied within a set
structure and can be both codiﬁed and free-ﬂoaƟng,
discriminaƟon is able to become even more granular, with
categories such as race and gender made increasingly nonstaƟc
and ﬂuid. As can be seen with Internet-daƟng proﬁles, which I
describe in more detail in Chapter 2, in order for algorithmic
sorƟng to take place, individuals must ﬁrst be subjected to the
process of segmentaƟon, where they are divided up into their
individual components for reasons of analyƟcs. Instead of being
“individuals,” they are turned into “dividuals.”
The concept of “dividuals” is not mine. The French philosopher
Gilles Deleuze coined this phrase to describe physically embodied
human beings who are nonetheless endlessly divided and reduced
to data representaƟons using tools such as algorithms. The
Formula, Deleuze and his coauthor Félix GuaƩari argue in A
59

Thousand Plateaus, has turned man into a “segmentary animal.”
We are segmented in a binary fashion, following the
great major dualist opposiƟons: social classes, but
also men-women, adults-children, and so on. We are
segmented in a circular fashion, in ever larger
circles, ever wider disks or coronas, like Joyce’s
“leƩer”: my aﬀairs, my neighborhood’s aﬀairs, my
city’s, my country’s, the world’s . . . We are
segmented in a linear fashion, along a straight line
or a number of straight lines, of which each segment
represents an episode or “proceeding”: as soon as
we ﬁnish one proceeding we begin another, forever
proceduring or procedured, in the family, in the
school, in the army, on the job.44
Deleuze expanded on this idea later in his life by discussing
what he called the “society of control.”45 Echoing Toﬄer’s Third
Wave thesis that we have progressed from a disciplinary society
based on the producƟon of physical goods, to an economy
founded on informaƟon and ﬁnancializaƟon, Deleuze examined
how the structures of power and control have changed. In
previous socieƟes, he argues that control occurred within speciﬁc
linear sites, such as the school, the workplace or the family home.
Each one of these came with its own unique set of rules, which
applied only to that site. Between spaces, people were relaƟvely
unmonitored and sƟll had space for uncontrolled life to occur. This
changes in a control society, in which there is conƟnuous
regulaƟon, although this is less obvious than in a disciplinary
society. Since power is everywhere it also appears to be nowhere.
Rather than being forced to ﬁt into preexisƟng molds, Deleuze
argues that we are encased in an enclosure that transforms from
moment to moment—like a giant sieve whose mesh strains and
bulges from point to point.
60

This seems to do a good job of summing up the new algorithmic
idenƟty. In the digital age, everyone can have a formula of his or
her own. Companies like Quantcast and Google get no beneﬁt at
all from everyone acƟng in the same way, since this allows for no
market segmentaƟon to occur. It is for this reason that arƟcles
like Steven Poole’s May 2013 cover story for the New Statesman,
“The Digital PanopƟcon,” invoke the wrong metaphor when it
comes to big data and algorithmic sorting.46
The panopƟcon, for those unfamiliar with it, was a prison
designed by English philosopher and social theorist Jeremy
Bentham in the late 18th century. Circular in design and with a
large watchtower in the center, the theory behind the
panopƟcon was that prisoners would behave as if they were being
watched at all Ɵmes—with the mere presence of the watchtower
being as eﬀecƟve as iron bars in regulaƟng behavior and ensuring
that everyone acted the same way. (A similar idea is behind
today’s open-plan offices.)
As former Wired editor Chris Anderson argues in The Long Tail,
modern commerce depends upon market segmentaƟon.47 Unless
you’re selling soap, aiming a product at a homogeneous mass
audience is a waste of Ɵme. Instead, vendors and marketers
increasingly focus on niche audiences. For niches to work, it is
important to companies that they know our eccentriciƟes, so that
they can ﬁgure out which Ɵny interest group we belong to. In this
sense, an authoritarian device like the panopƟcon—which makes
everyone act the same way—is counterproducƟve. In algorithmic
sorƟng, audiences know they are being surveilled; they just don’t
care. The apparatus of capture (how companies recognize us) and
the apparatus of freedom (buy the products that best sum you up
as an individual) are entwined so totally as to be almost
inseparable. As French philosopher Jacques Ellul argued in The
Technological Society, the ciƟzens of the future (he was wriƟng in
the early 1960s) would have everything their heart desired,
except for their freedom.48 This chilling concept is one that was
61

more recently expanded upon by media historian Fred Turner:
If the workers of the industrial factory found
themselves laboring in an iron cage, the workers of
many of today’s post-industrial informaƟon ﬁrms
oŌen ﬁnd themselves inhabiƟng a velvet gold
mine . . . a workplace in which the pursuit of self-
fulﬁllment, reputaƟon, and community idenƟty, of
interpersonal relaƟonships and intellectual pleasure,
help to drive the production of new media goods.49
In her work on the digital idenƟty, MIT psychoanalyst Sherry
Turkle talks about our diﬀerent “selves” as windows. “Windows
have become a powerful metaphor for thinking about the self as a
mulƟple, distributed system,” she wrote in her 1995 book Life on
Screen, a magnum opus that landed her on the cover of Wired
magazine. “The self is no longer simply playing diﬀerent roles in
diﬀerent seƫngs at diﬀerent Ɵmes. The life pracƟce of windows
is that of a [de-centered] self that exists in many worlds, that
plays many roles at the same time.”50
This view of the self as a “mulƟple, distributed system” (or else
a slightly clunky MicrosoŌ operaƟng system) was meant as
empowering. The de-centered, windowed self means that the
woman who wakes up in bed next to her husband can walk
downstairs, close the “wife” window and open the “mother” one
in order to make breakfast for her daughter, before hopping in
the car and driving to work, where she will once again clear her
exisƟng windows and open a new one Ɵtled “lawyer” or “doctor.”
This is, of course, the thing about windows: they can be opened
and closed at will.
What Fred Turner describes, on the other hand, is a world in
which mulƟple subjecƟviƟes exist, but these subjecƟviƟes
constantly crash into one another. Unlike the “windowed self” or
the “segmentary animal” who ﬁlls diﬀerent roles at school, in the
62

workplace and at the home, where The Formula is involved these
rules are not isolated to one locaƟon but aﬀect one another in
intricate, granular and often invisible ways.
63

Keeping Up Appearances
It might not even maƩer whether speciﬁc pieces of data shaping
our idenƟty are “true” or not. Whether we are physically male or
female, or else consider ourselves to be male or female,
ulƟmately what will determine how we are treated online is the
conclusions reached by algorithms. A New York Times arƟcle from
April 2013 related the story of an unnamed friend of the writer’s
who received, by mail, a ﬂyer adverƟsing a seminar for paƟents
suﬀering from mulƟple sclerosis, hosted by two drug companies,
Pﬁzer and EMD Serono. Spam e-mails and their physical
counterparts are, of course, nothing new. What was alarming
about this situaƟon, however, was not the existence of the spam
message, but the targeƟng of it. The recipient was not an MS
suﬀerer, although she had spent some Ɵme the previous year
looking up the disease on a number of consumer health sites. As
the arrival of the ﬂyer proved, somewhere her name and contact
details had been added to a database of MS suﬀerers in
Manhattan. The ramifications of this were potentially vast. “Could
[my friend], for instance, someday be denied life insurance on the
basis of that proﬁle?” the author asks. “She wanted to track down
the source of the data, correct her proﬁle and, if possible, prevent
further disseminaƟon of the informaƟon. But she didn’t know
which company had collected and shared the data in the ﬁrst
place, so she didn’t know how to have her entry removed from
the original marketing list.”51
John Cheney-Lippold, the scholar who coined the term “new
algorithmic idenƟty,” says that the top-performing male students
in his classroom are regularly characterized as female online.
“Who is to say that they’re not female in that case?” he asks
rhetorically. In a world in which terms like “male” and “female”
are simply placeholders for speciﬁc types of behavior, tradiƟonal
disƟncƟons of discriminaƟon break down. With this in mind, who
is to say what “gender” or “racial” discriminaƟon will look like by,
64

for example, the year 2040? If gender discriminaƟon is not based
on anything physical but rather on the inferences of algorithms,
can a man be discriminated against in terms of the barring of
certain services because he skews staƟsƟcally female in his clicks?
What does it mean for the future of racial poliƟcs if a young white
male growing up in a working-class environment in inner-city
Detroit is classiﬁed “blacker” than an older, educated African-
American female living in Cambridge, MassachuseƩs? Could a
person be turned down for a job on the basis that it is a role black
males staƟsƟcally do worse in (whatever that might be), even if
the individual in question is physically a white female?
These types of discriminatory behavior could prove challenging
to break, parƟcularly if they are largely invisible and in most cases
users will never know how they have been categorized. Unlike the
shared history that was drawn on to help bring about the civil
rights or women’s lib movements, algorithmically generated
consumer categories have no cultural background to draw upon.
What would have happened in the case of Rosa Parks’s December
1955 protest—which garnered the support of the African-
American community at large—had she not been discriminated
against purely on the basis of her skin color, but on several
thousand uniquely weighted variables based upon age, locaƟon,
race and search term history? There is racism, ageism and sexism,
but is there an “ism” for every possible means of demographic and
psychographic 
exclusion? 
Unlike 
previous 
discriminatory
apparatuses, today’s categories of diﬀerenƟaƟon may be mulƟply
cross-referenced to the point where it even becomes diﬃcult to
single out the single overriding factor that leads to a person being
denied credit, or enables them to proactively alter their perceived
desirability.
It is also worth noƟng that gender and race do not exist as
stable concepts but rather in a state of constant ﬂux. Like the
concept of a “character” that a person builds but never ﬁnishes,
so too are speciﬁc categories like maleness not simply inferred by
65

algorithms and then established from that point on, but instead
have to be reinforced on a constant basis.52 A user may be
considered male, but in the event that they then begin searching
for more “female” subjects, they will be reclassified. The man who
regularly buys airline Ɵckets might be recognized as increasingly
female, while a female with a keen interest in sports or world
news becomes statistically male.
Categorizing addiƟonally has the ability to move beyond what
we might tradiƟonally think of as categories. If concepts like
“creaƟvity” and “percepƟveness” are successfully quanƟﬁed and
linked to consistent types of behavior, these might take on as
much importance as gender or race. It was this world that
Deleuze was addressing when he predicted that increasingly our
credit cards and social security numbers will become more
signiﬁcant idenƟty markers than the color of our skin or the place
we went to school. “A man is no longer a man conﬁned,” he
wrote, “but a man in debt.”
66

CHAPTER 2
The Match & the Spark
Each summer, thousands of freshly qualiﬁed doctors graduate
from medical school in the United States. The next step of their
training involves being paired up with a teaching hospital where
they can conƟnue to learn their craŌ; a process that is referred to
as residency.
Deciding which doctors go to which hospitals involves a two-
way matching process in which both doctors and hospitals have
their list of preferences, and the task is to match both parƟes up
in such a way that everyone is pleased with the decision.
Of course, this is easier said than done. Among both people and
hospitals, some are more popular and in higher demand than
others. A hospital might be geographically preferable, for
example: perhaps situated in a big city, or in an especially scenic
locaƟon. It might be preferable based on its overall reputaƟon, or
because of a parƟcular member of staﬀ who is held in particularly
high regard within the medical teaching community.
Think about how diﬃcult it could be to decide, based on
consensus, where to go on a family holiday. Now imagine that
each locaƟon around the world can only be visited by one family
at a Ɵme, but that all families have to sƟll go on holiday during
the same week. Now imagine that those holiday desƟnaƟons also
have their own list about which family they want to welcome.
In 1962, two American economists named David Gale and
Lloyd Shapley set out to devise an algorithmic soluƟon to just this
conundrum. What they came up with was called The Stable
Marriage Problem—also known as “The Match.”1
To explain The Match, picture a remote island, cut oﬀ from the
67

rest of civilizaƟon. On this island, there live an even number of
men and women of marriageable age. The problem asks that all
of these people are paired up in what we might consider a stable
relaƟonship. To explain what is meant by “stable” ﬁrst allow me
to explain what counts as an “unstable” marriage. Suppose that
two of the marriageable men on this island are named James and
Rob, while two of the marriageable women are named Ruth and
Alice. James is married to Ruth, although he secretly prefers Alice.
Alice is married to Rob, but she secretly prefers James. Both, in
other words, are already married to other people, but would be
beƩer oﬀ if they were matched together. It is not beyond the
limits of our imaginaƟon to suppose that one day James and Alice
will run oﬀ together—hence the lack of stability. The goal of the
algorithm is to come up with a soluƟon in which everyone is
matched up in such a way that no couples exist that would rather
be paired with a person other than with their respective partners.
There are a few rules to consider before any actual matching
takes place. At the start of the problem, every marriageable man
and woman on the island is asked to compile a list of preferences,
in which they rank every member of the opposite sex in the order
in which they most appeal. James might list Alice ﬁrst, then Ruth,
then Ruth’s friend CharloƩe, and so on. Only men may propose to
women, although women have the right to not only refuse to
marry a parƟcular man if they deem him a bad match, but also to
accept proposals on a tentaƟve basis, keeping their opƟons open
in case someone beƩer comes along. The marriage proposal
process works in rounds, which I will describe as “days” in order
for things to be kept simple.
On the morning of the ﬁrst day, every man proposes to his ﬁrst
choice of wife. Certain women will be in the fortunate posiƟon of
having received mulƟple proposals, while others will have
received none. On the aŌernoon of the ﬁrst day, each woman
rejects all suitors except for her current best available opƟon,
who she tentaƟvely agrees to marry, knowing that she can ditch
68

him later on. (This is referred to as a “deferred acceptance.”)
Come dawn of the second day and those men who remain single
propose to their next best choice. That aŌernoon, the women
who accepted marriage on day one have the chance to trade up,
if the man who proposed to them on day two is, in their view,
preferable to the person they are engaged to. This process
conƟnues for days three, four, ﬁve, et cetera, unƟl all couples are
matched in stable relaƟonships. At this point the algorithm
terminates.
Here’s an example:
Name
First Choice
Second Choice
Third Choice
Fourth Choice
Alice
Tim
James
Rob
Rajiv
Ruth
James
Rajiv
Tim
Rob
Charlotte
Rob
Rajiv
Tim
James
Bridgette
Rajiv
Rob
James
Tim
James
Alice
Bridgette
Charlotte
Ruth
Rob
Alice
Ruth
Charlotte
Bridgette
Tim
Ruth
Bridgette
Charlotte
Alice
Rajiv
Charlotte
Alice
Ruth
Bridgette
On day one, each man proposes to his top choice. James and
Rob propose to Alice, Tim proposes to Ruth, and Rajiv proposes to
CharloƩe. Of Alice’s two proposals she prefers James to Rob and
so accepts his oﬀer; knowing that she might well do beƩer later
on. Ruth, meanwhile, accepts Tim’s proposal, while CharloƩe
accepts Rajiv’s. On day two, Rob—rejected by Alice on the ﬁrst
day—proposes to Ruth. Rob, however, is Ruth’s fourth choice and
she remains engaged to Tim. On day three, Rob tries again and
asks CharloƩe to marry him. Rob is CharloƩe’s ﬁrst choice and so
she ditches Rajiv and becomes engaged to Rob. On day four, the
newly single Rajiv asks Alice to marry him, although she elects to
stay with James. On day ﬁve, Rajiv then asks Ruth to marry him,
69

and Ruth breaks it oﬀ with Tim and becomes engaged to Rajiv. On
day six, Tim proposes to BridgeƩe who, while he remains her
fourth and last choice of match, has no other proposals and so
agrees to marry him. The final couples are therefore as follows:
James and Alice
Rob and Charlotte
Rajiv and Ruth
Tim and Bridgette
There are several neat aƩributes to this parƟcular algorithm.
Among its most impressive qualiƟes is the fact that not only does
it manage to ﬁnd a partner for everyone, but it does this with
maximum eﬃciency. It is, of course, impossible in all but the most
unlikely of cases that everyone will receive their ﬁrst choice of
partner, and this eﬀect is only ampliﬁed as the numbers increase.
Four boys and four girls may well receive their ﬁrst choice, but
would 40 boys and 40 girls? Or 400?
This algorithm, it should be noted, favors whoever it is that is
doing the proposing (in this case the men). Were it to work the
other way around from this male-opƟmal scenario (with the
women asking the men to marry them, and the men doing the
rejecting or the accepting) the algorithm for solving this particular
problem would have terminated aŌer one day, with the couples
arranged as follows:
Alice and Tim
Ruth and James
Charlotte and Rob
Bridgette and Rajiv
For all its good qualiƟes, however, there are a few conceptual
problems with this problem, which follow as such. For one thing,
The Match imagines that all of the men and women on the island
are heterosexual, and therefore wish to be paired with members
of the opposite sex. This staƟsƟcally unlikely event is done only for
the sake of mathemaƟcal simplicity. A scenario where men want
to marry men, women want to marry women, or everyone wants
to marry everyone, does not necessarily yield the same stable
outcome as the relatively straightforward matching I described.
70

Another issue is that the Stable Marriage Problem presumes
that all people of marriageable age do, in fact, wish to get
married, and that the locaƟon in which they live is so remote that
there is no chance whatsoever that any residents could marry
someone from outside its boundaries. Yet another problem is that
the algorithm assumes marriages only fail in the event of a
disrupƟve third party, who is a beƩer match for one part of a
parƟcular couple. While it is undeniably true that a certain
percentage of marriages do end for this exact reason, it is by no
means a universal, staƟsƟcally robust rule. Some couples simply
ﬁnd themselves incompaƟble and conclude that they would be
happier living on their own than they would with one another. To
return to our original example, perhaps Alice falls head over heels
in love with James only to realize aŌer living with him for several
months that her idea of a perfect evening is to go out dancing,
while James would rather stay in and play computer games. Or
maybe James chews with his mouth open, and bad manners are a
deal-breaker for Alice.
The part of The Match that causes me the most problems,
however, is the piece of informaƟon we are asked to take as writ
before the algorithm even gets going: namely the idea that each
of the men and women addressed by the puzzle is able to compile
a list in which they rank, without error, everyone of the opposite
gender in the order that they would most happily be married to
them. Of all the assumpƟons made by David Gale and Lloyd
Shapley, this seems the most grievous.
I do, of course, write these words with tongue ﬁrmly planted in
cheek. As noted, the Stable Marriage Problem invokes romanƟc
marriage as metaphor only, being designed for the purpose of
matching medical students with hospitals. For this task it is largely
suitable—since issues of one party chewing with their mouth
open, or ogling a third party, are unlikely to result in separaƟon.
But by poinƟng out these conceptual diﬃculƟes, I do make a
serious point: that outside of mathemaƟcal puzzles human beings
71

have a nasty habit of behaving unpredictably.
And particularly when love is involved.
72

Madness in Love, Reason in Madness
In 2006, the pop staƟsƟcian Garth Sundem was asked by the New
York Times to create a formula to predict the breakup rate of
celebrity marriages.2 Sundem was well known for what he calls his
“math in everyday life” equaƟons. He’d appeared previously on
the BBC and Good Morning America, and his formulas had proven
so popular that he had published a book of them, Geek Logik,
which contained mathemaƟcal answers to determine everything
from whether a person should get a taƩoo to how many beers
they might want to take on their next work picnic. When the New
York Times contacted him, Sundem says that he did what he
always does when starƟng work on a new equaƟon: he sat down
at his desk and began thinking of criteria to test against the data
on celebrity divorce rate. Did, for instance, the hair color of the
couple in quesƟon make them more or less likely to divorce? How
about the proximity of their home in relaƟon to the Hollywood
sign? In most cases, the answer was a resounding no. But data-
mining did eventually produce results—as it always does. For
example, Sundem discovered that the number of Google hits for a
starlet that showed her in skimpy clothing posiƟvely correlates
with the duraƟon of her marriage. So too did the combined
numbers of the couple’s previous marriages.
Here is the formula he eventually came up with:
P = The couple’s combined number of previous marriages
Ab = His age in years
Ag = Her age in years (biological, not cosmetic)
Gb = In millions, the number of hits when Googling his name
Gg = In millions, the numbers of hits when Googling her name
S = Of her ﬁrst ﬁve Google hits, the number showing her in clothing (or lack
thereof) designed to elicit libidinous thoughts
73

D = Number of months they knew each other before geƫng married (enter a
fraction if necessary)
T = Years of marriage. To ﬁnd the likelihood of their marriage surviving 1 year,
enter 1; for the likelihood of it lasting 5 years, enter 5, etc.
Blis is the percentage chance that this couple’s marriage will last for the
number of years you chose.
While Sundem’s celebrity marriage formula was less than
serious, it nonetheless proved immensely popular, as well as
remarkably prescient. Using it to chart a string of recent nupƟals
involving the rich and famous, some conclusions were to be
expected. For example, Prince William and Kate, the Duke and
Duchess of Cambridge, turned out to have a far beƩer chance of
being a long-term item than, say, Khloe Kardashian and Lamar
Odom—the laƩer of whom announced their divorce in December
2013. But the formula also correctly surmised that, while KaƟe
Holmes and Tom Cruise’s marriage would last ﬁve years, there
was liƩle to no possibility it would reach ﬁŌeen. (Holmes and
Cruise announced their divorce in 2012, following ﬁve and a half
years as husband and wife.) The same fate was predicted for Will
Smith and Jada PinkeƩ, who have passed their ﬁve-year
anniversary, but who are regular tabloid fodder predicƟng
impending doom. Meanwhile, the formula suggested that if
“Brangelina”—Angelina Jolie and Brad PiƩ—had married
immediately aŌer PiƩ divorced previous wife Jennifer Aniston,
there would have been a 1 percent chance of their marriage
lasƟng 15 years. Today, their chances of doing so stand at more
than 50 percent.
By Sundem’s own admission, he is not a serious mathematician.
The success of his pop formulas speaks less to the rigor of his
empirical approach than to the general public’s overwhelming
eagerness to ﬁnd answers to replace those that in a less
technological society might have been chalked up to fate. If there
is one absolute truth that comes out of his celebrity marriage
equaƟon, it is not that millions of Google hits doom a relaƟonship
74

from the very start, but that as people we are spectacularly bad
at predicƟng who they are going to be well matched with when it
comes to marriage.
AŌer all, if the wealthiest, most successful and best-looking 1
percent of the populaƟon (i.e., celebriƟes) can’t guarantee that
they will be happy with their chosen marriage partners, what
hope do the remaining 99 percent of us have? About half of ﬁrst
marriages fail in the United States, as do two-thirds of second
marriages and three-quarters of third marriages. That we are
extraordinarily bad at choosing our romanƟc partners in marriage
shouldn’t come as a great surprise. If there is one thing that we
are told over and over again in movies, songs and novels, it is that
love is fundamentally unpredictable by its very nature. Take
Wagner’s 1870 opera Die Walküre, for instance, in which the
lovers turn out to be long-lost brother and sister, although this
fact alone is not enough to stop them falling in love. Or consider
the immortal words of pop singer Joe Jackson: “Is She Really Going
Out with Him?”
In many cases, the unpredictability of love has rendered even
the most scienƟﬁc of minds incapable of explanaƟon. “The heart
has its reasons, which reason knows nothing of,” Blaise Pascal, the
French mathemaƟcian and inventor of the mechanical calculator,
famously proclaimed. At the start of his 1822 book On Love, the
French writer Stendhal lays out his desire to describe “simply,
raƟonally, mathemaƟcally . . . the diﬀerent emoƟons which . . .
taken together are called the passion of Love.”3 The results, at
least according to fellow wordsmith Henry James, were
“unreadable.” The problem, at least for those in the natural
sciences, may be that there is not necessarily a natural law as
relates to love. It might be eternal, but is it also external? Maybe
not. “Love is intrinsically ametric,” observed the BriƟsh
psychotherapist David Brazier, speaking of love’s refusal to
succumb to quantification.4 Compounding the problem yet further
is the suggesƟon that the merest aƩempt to analyze pleasure or
75

beauty all but destroys it—not dissimilar to trying to get a closer
look at an exoƟc bird, only to have it ﬂy away when it senses our
designs.5
This may be a defeaƟst approach, however, which is why two
centuries aŌer Stendhal’s failed aƩempt to describe love,
technologists conƟnue to try to discover love’s code—perhaps
taking at face value Friedrich Nietzsche’s asserƟon that “there is
always some madness in love, but there is always some reason in
madness.”6 And when it comes to this nobly scienƟﬁc pursuit
(worth upward of $4 billion globally in online daƟng fees alone7)
there are few beƩer people to examine than Neil Clark Warren
and his eHarmony empire.
76

Seeking Harmony
Judged on ﬁrst impressions, Neil Clark Warren is one of the more
unlikely entrepreneurs of the Internet age. Born September 18,
1934, Warren was 65 years old when he launched his Internet
daƟng company, eHarmony, in 2000. In the world of high tech,
where acne is good, wrinkles are bad, and programmers throw in
the towel at 30 because “coding is a young man’s game,” 65
might as well have been 165. Warren wasn’t—and isn’t—a
computer whiz. According to him, he didn’t even know how to
send and receive e-mail unƟl his company was up and running.
“I’m confessing up front that I don’t know much of anything about
algorithms,” he says soon aŌer he and I begin our conversaƟon. “I
think you’re going to be terribly unimpressed.”
But if Warren is too old to have grown up as part of the
computer revoluƟon, he was just the right age to have lived
through another major change in American life. On October 28,
1935, when Warren was barely one year old, the New York Times
ran a story in which it reported that the country’s divorce rate
had increased by more than 2,000 percent. Worse, the staƟsƟcs
didn’t tell the whole story: according to the arƟcle, half of all
American couples were supposedly unhappy; living, as one expert
phrased it, “not really married but simply undivorced . . . in a sort
of purgatory.”8
As is oŌen the case with moments of profound social change,
the Ɵpping point that had led to this crisis of marriage wasn’t
limited to one factor but was rather the result of a number of
diﬀerent factors converging simultaneously. Love was being
increasingly emphasized over duty in marriage—with the
implicaƟon that matrimony should be the fount of all human
happiness. For the ﬁrst Ɵme, divorce was also a real opƟon to
those outside of elite society. America’s ciƟes were growing
bigger by the day, as people moved away from their small, Ɵght-
knit communiƟes and experienced a smorgasbord of new,
77

previously unimaginable possibiliƟes. The country was similarly
more secular, as religion’s inﬂuence receded and The Formula’s
advanced.
Right on cue came the new ﬁeld of sociologists: invoking the
metaphor of marriage as a machine, to be kept well oiled and in
good working order at all Ɵmes. As Paul Popenoe, one of the
founding pracƟƟoners of American marriage counselling,
observed:
There 
is 
nothing 
mysterious 
about 
[marital
relaƟons], any more than there is about the
overhauling of an automobile that is not working
properly. The mechanic invesƟgates one possibility
at [a] Ɵme: he checks the igniƟon, the carbureƩor,
the transmission, the valves, and so on; ﬁnds where
the trouble lies; and removes the cause if possible.
We do the same with a marriage.9
Warren’s own parents fell into the category of troubled
marriages. Although they stayed married for 70 years, Warren
never felt that they were well matched with one another. “My
dad was just so sƟnking bright, and my mom was so sweet, but
she was two standard deviaƟons below him in intelligence,” he
would recall. Warren’s father—a business-minded man who
owned a Chevrolet dealership, a John Deere ouƞit and a food
shop—was interested in world poliƟcs, constantly quesƟoning
issues like the ongoing conﬂict between the Jews and Arabs. His
mother, Warren has said, “didn’t know there was a Middle East.”
When his father ran for oﬃce in Polk County, his mother voted for
the other candidate.10
“I originally planned to go into the ministry, but discovered that
I’m more of an entrepreneur than I am a minister,” Warren says.
AŌer earning a PhD in clinical psychology from the University of
Chicago in 1967, he started out working as a marriage counselor.
78

In 1992, he found fame by publishing a book called Find the Love
of Your Life. It sold around a million copies, but Warren found
himself feeling despondent. Even premarital counseling, he had
concluded, was leaving things too late for many couples. “People
have a tendency to form bonds very early in their relaƟonship,
and no maƩer what they ﬁnd out at that point about whether
they should get married or not, they will very seldom break up,”
he says. “I came to realize that the only way to match people up
with one another is to do it before they have met and got
involved.”
Unlike a lot of psychologists, Warren discovered early in his
career that he was interested in the quanƟtaƟve parts of the ﬁeld
that scared oﬀ most people. He decided to put this to work by
invesƟgaƟng the quality of marital relaƟons. AŌer Find the Love
of Your Life, Warren started work on a large-scale research
project, in which he conducted in-depth interviews with 800
couples. When this was done, he compared the results of the 200
most saƟsﬁed and least saƟsﬁed couples and used this to derive a
robust set of psychometric factors he found remarkably predicƟve
of compaƟbility in marriage. “The results overlapped so much
with what I had found from 40 years of therapy that I became
more and more conﬁdent that I knew what I was talking about,”
he says. To help make sense of the data, he called in the services
of a young staƟsƟcian named Steve Carter—who is today
eHarmony’s Vice President of Matching. On one occasion early in
the two men’s working relaƟonship, Warren threw out a
curveball.
“How about building a website?” he said.
“To test for marital quality, you mean?” Carter answered.
“Not exactly,” Warren countered. “I was thinking more along
the lines of a matchmaking site. We could use our compaƟbility
models as a start and go from there.”
Despite some iniƟal misgivings, Carter began working on the
site: diligently reinterpreƟng the data Warren had gathered as a
79

436-quesƟon personality proﬁling test. Matching was done
according to a checklist called the “29 Dimensions of
CompaƟbility.” These dimensions were composed of two main
variables described as Core Traits (“deﬁning aspects of who you
are that remain largely unchanged”) and Vital AƩributes (“based
on learning and experience and are more likely to change based
on life events and decisions you make as an adult”). Although the
algorithms involved remain largely black-boxed to avoid scruƟny,
a number of details have come to light. Women, for instance, are
never matched with men shorter than themselves, which means
that men are similarly never paired with taller women. There are
age parameters, too, which vary with gender, so that a 30-year-
old woman will be matched with men between the ages of 27 and
40, while a 30-year-old man will be matched with women
between 23 and 33.
In all, eHarmony’s arrival represented more than just another
addition to an already crowded field of Internet dating websites—
but a qualitaƟve change in the way that Internet daƟng was
carried out. “Neil was adamant that this should be based on
science,” Carter says. Before eHarmony, the majority of daƟng
websites took the form of searchable personal ads, of the kind
that have been appearing in print since the 17th century. 11 AŌer
eHarmony, the search engine model was replaced with a
recommender system praised in press materials for its “scienƟﬁc
precision.” Instead of allowing users to scan through page aŌer
page of proﬁles, eHarmony simply required them to answer a
series of quesƟons—and then picked out the right opƟon on their
behalf.
The website opened its virtual doors for the ﬁrst Ɵme on
August 22, 2000. There were a few iniƟal teething problems.
“Some people were criƟcal of the matches they were geƫng,”
Warren admits. “One woman who worked for a public relaƟons
ﬁrm was annoyed because we had matched her with two truck
drivers, one aŌer the other. My point was, ‘You know, a truck
80

driver can be very smart,’ but in her mind there was such a status
diﬀerence that she just thought it was absurd. She felt we
weren’t doing our job at all.”
The point where, in Steve Carter’s words, “shit took oﬀ” was
when Jay Leno put on a wig and started spooﬁng Neil Clark
Warren on his television talk show. “He was fascinated that this
old guy, with gray hair, was talking about matching up people for
marriage,” Warren says. Although he was embarrassed, this was
the moment he knew the website had struck a chord. By the end
of 2001, eHarmony had more than 100,000 members registered.
By August the following year, it had 415,000 members. By
November 2003, the number had jumped to 2.25 million; followed
by almost 6 million by December 2004; 8 million by October 2006;
and 14 million by March 2007. Today, eHarmony credits its
algorithms with generaƟng 600,000 marriages in the United
States—and a growing number overseas.12 As it turned out, there
was a demand for scientific approaches to matchmaking, after all.
“From my point of view, I can’t imagine anything I could be doing
using staƟsƟcs that would be more impacƞul on society than
what I’m doing,” Carter says.
81

Categorize Your Desire
eHarmony doesn’t present itself simply as another coﬀee shop,
nightclub, workplace or anywhere else that we might meet
potenƟal spouses. In its own words, it is designed to “deliver more
than just dates” by promising connecƟons to “singles who have
been prescreened on . . . scienƟﬁc predictors of relaƟonship
success.” To put it another way, the person who uses eHarmony’s
algorithm to ﬁnd dates isn’t just being oﬀered more dates, but
better ones.
Certainly, Internet daƟng oﬀers quanƟtaƟvely beƩer odds of
ﬁnding a partner. Upward of 30 percent of the 7 billion people
currently alive on Earth have access to the Internet, with this
ﬁgure rising dramaƟcally to around 80 percent in the United
Kingdom and North America, where usage is among the highest.
Where previously the romanƟc “ﬁeld of eligibles” available to us
(as sociologist Alan Kerckhoff once phrased it) was limited to those
people we were likely to come into contact with as part of our
everyday social network, the Internet now provides access to an
unprecedented quanƟty of people, numbering around 2 billion in
total.13 Of these, a growing percentage have shown interest in
using the Internet to ﬁnd love. In a typical month, almost 25
million unique users from around the world access some form of
online dating site.14
Does it oﬀer a qualitaƟvely beƩer experience, however? This
is, aŌer all, the mission statement of websites like eHarmony—
which jealously guard not only their user base of customers, but
more importantly the proprietary algorithms used to match
them. Generally speaking, sites that oﬀer matching services tend
to rely on similarity algorithms (with a certain amount of
complementarity matching added in). Partners are matched on
numerous personality traits and values, reﬂecƟng not only what
the creators of a parƟcular algorithm deem important, but also
what is prized by its customers. This, in turn, has allowed a broad
82

range of matching sites to pop up around eHarmony, each one
catering to a diﬀerent clientele who stress the importance of
different aspects of the romantic experience.
For those interested in playing the numbers game, there is
OkCupid, whose tagline borrows from eHarmony’s in-your-face
scientism by promising that “we use math to get you dates.”15 For
the ultra-scienƟﬁc (or perhaps the latent Eugenist) there is
GenePartner.com, a website that claims to have “developed a
formula to match men and women for a romanƟc relaƟonship
based on their genes.”16 For the low, low price of just $249,
would-be daters can order a kit with which to swab the inside of
their mouths. This saliva sample is then mailed back to the lab at
GenePartner, where it is analyzed, and the results fed back to the
user. For an addiƟonal fee, GenePartner can even match
geneƟcally similar users with their supposed soul mates. “With
geneƟcally highly compaƟble people we feel that rare sensaƟon
of perfect chemistry,” the company’s press materials state. “This
is the body’s recepƟve and welcoming response when immune
systems harmonize and ﬁt well together. GeneƟc compaƟbility
results in an increased likelihood of forming an enduring and
successful relaƟonship, a more saƟsfying sex life, [and] higher
fertility rates.”
For the narcissisƟc, meanwhile, there is FindYourFaceMate
.com, which hails itself as a “revoluƟonary new online daƟng site
that employs sophisƟcated facial recogniƟon soŌware and a
proprietary algorithm to idenƟfy partners more likely to ignite
real passion and compaƟbility.” Based on the theory that we are
innately drawn to those people with features that resemble our
own, subscribers to FindYourFaceMate upload proﬁle pictures,
which are then analyzed on nine diﬀerent facial parameters
(eyes, ears, nose, chin and various parts of the mouth) in order to
find suitable “face mates.”17
Then there is BeauƟfulPeople.com—described by its founder as
a “gated community for the aestheƟcally blessed”—which
83

marries skin-deep shallowness with a striking free-market
ideology, summed up by its promise that “beauty lies in the eyes
of the voter.” 18 For the plus-sized dater, there is LargeAndLovely
.com (“Where Singles of Size & Their Admirers Meet”) and, on the
other end of the spectrum, FitnessSingles.com (“Where
RelaƟonships Workout”). There are sites like SeaCaptainDate
.com (“Find Your First Mate”) and TrekPassions.com (“Love Long
& Prosper”); those that match based upon your parƟcular strain
of vegetarianism (VeggieDate.org); and those that pair people on
a shared taste in books (ALikeWise.com) or a fondness for those
who work in uniform (UniformDating.com).
The idea, essenƟally, is to drill down unƟl we discover the
parƟcular weighted node that best captures our fancy. For some,
their vegetarianism might be a deﬁning characterisƟc and,
therefore, a “must have” demand. For others it is nonessenƟal, or
even incidental. Online, not only is everyone a formula, as argued
in the previous chapter, there is also a formula for everyone.
84

Love in the Time of Algorithms
There is a scene in the 2009 comedy ﬁlm Up in the Air in which
Natalie, an upwardly mobile businesswoman just out of college,
describes the qualiƟes she is looking for in a partner. She calls this
her “type.” “You know, white collar,” she says, lisƟng her ideal
mate’s aƩribuƟons. “College grad. Loves dogs. Likes funny
movies. Six foot one. Brown hair. Kind eyes. Works in ﬁnance but
is outdoorsy, you know, on the weekends. I always imagined he’d
have a single-syllable name like MaƩ or John or Dave. In a perfect
world, he drives a 4Runner and the only thing he loves more than
me is his golden Lab. Oh . . . and a nice smile.”
We laugh at the scene because of the speed and exactness
with which Natalie is able to recite all of these details. But the
joke also rings true because all of us have met someone like
Natalie. We may even be someone like her.
In one sense, the idea that selecƟng the perfect lover is the
result of ﬁnding the individual whose list of aƩributes best
measures up against our own wish list seems like the most natural
thing in the world. Whether we are looking for a spouse, a holiday
or a new laptop, all of us make mental lists of the qualiƟes we are
searching for and then match up whichever potenƟal oﬀerings we
come across with our checklist of minimum demands. If a
potenƟal relaƟonship is deemed not aƩracƟve enough for us on
some level, a holiday is too expensive, or a laptop won’t carry out
the tasks we are buying it for, we dismiss it and move on to the
next opƟon. However, is this really the right way to think about
love? In his book How the Mind Works, the experimental
psychologist and author Steven Pinker poses a quesƟon very
similar to the one asked in the Stable Marriage algorithm
described at the start of this chapter: namely, how can a person
be sure in a relaƟonship that their partner will not leave them the
moment that it is raƟonal to do so? Pinker gives the potenƟally
problemaƟc example of a more physically aƩracƟve “10-out-of-
85

10” neighbor moving in next door to us. The answer economists
David Gale and Lloyd Shapley would give us is that we are in the
clear just so long as this neighbor is already paired with someone
more preferable to themselves than our spouse. The answer
Pinker instead presents us with is the more humanisƟc suggesƟon
that we ought not to accept a partner who wants us for any
raƟonal reason to begin with—but rather who is commiƩed to
staying because of who we are.
Murmuring that your lover’s looks, earning power,
and IQ meet your minimal standards would probably
kill the romanƟc mood, even though the statement
is staƟsƟcally true. The way to a person’s heart is to
declare the opposite—that you’re in love because
you can’t help it.19
This anƟraƟonal view of love is one that may be breaking down
in the age of The Formula. In Love in the Time of Algorithms,
author Dan Slater relates the story of “Jacob,” a thirtysomething
online dater who meets a 22-year-old secretary named Rachel on
the Internet. AŌer daƟng for a few months and moving in
together, the couple split up aŌer coming to the conclusion that
they want diﬀerent things from life. So far, so normal. What is
diﬀerent, however, are the comments made by Jacob in the wake
of the relaƟonship’s failure. “I’m about 95 percent certain that if
I’d met Rachel oﬄine, and if I’d never done online daƟng, I
would’ve married her,” he tells Slater. “At that point in my life I
would’ve overlooked everything else and done whatever it took to
make things work. Did online daƟng change my percepƟon of
permanence? No doubt. When I sensed the breakup coming, I
was okay with it. It didn’t seem like there was going to be much of
a mourning period, where you stare at your wall thinking you’re
desƟned to be alone and all that. I was eager to see what else
was out there.” In other words, there are plenty more ﬁsh in the
86

sea—or, in the words of daƟng website PlentyofFish, at least “145
million [unique] monthly visitors.”
87

Are You Sure You Want to Delete This
Relationship?
Jacob’s words oﬀer a perfect summaƟon of what the Polish
sociologist Zygmunt Bauman refers to as “virtual relaƟonships” in
his book Liquid Love: On the Frailty of Human Bonds. Categorizing
a more scienƟﬁc approach to love alongside quick ﬁxes, foolproof
recipes, all-risk insurance and money-back guarantees, Bauman
describes how the ulƟmate promise of virtual relaƟonships is to
“take the waiƟng out of wanƟng, [the] sweat out of eﬀort and
[the] effort out of results.”
Unlike “real relaƟonships,” “virtual relaƟonships”
are easy to enter and to exit. They look smart and
clean, feel easy to use and user-friendly, when
compared with the heavy, slow-moving, inert messy
“real stuﬀ.” A twenty-eight-year-old man from Bath,
interviewed in connecƟon with the rapidly growing
popularity of computer daƟng at the expense of
singles bars and lonely heart columns, pointed to one
decisive advantage of electronic relaƟon: “You can
always press ‘delete.’”20
It is explanaƟons such as this suggesƟon of “liquid love” that
might mean—in the words of FreeDaƟng website founder Dan
Winchester—that the future will be made up of “beƩer
relaƟonships but more divorce.” Although this seems a
paradoxical statement, it is something that could be the end
result of beƩer and beƩer algorithms, Winchester suggests. “I
oŌen wonder,” he says, “whether matching you up with great
people is geƫng so eﬃcient, and the process so enjoyable, that
marriage will [eventually] become obsolete.”
What Winchester is expressing is not unique, although it might
88

well be a new phenomenon. In his 2004 book, The Paradox of
Choice: Why More Is Less, the American psychologist Barry
Schwartz argues that the overwhelming amount of available
choice in everything from shopping to, yes, daƟng has become for
many people a source of anxiety in itself. 21 In terms of
relaƟonships, this “paradox of choice” is dealt with by subjecƟng
individual lovers to segmentaƟon: an industrial term that denotes
how eﬃciency can be gained by dividing up and isolaƟng the
means of producƟon. In an age of mass customizaƟon,
relaƟonships become just any commodity to be shaped according
to fads, changing desires and flux-like whims.
Such a posƟndustrial approach to daƟng runs counter to what
we have been culturally condiƟoned to believe. The Lover is
supposed to be unique; not just a combinaƟon of answers to set
quesƟons, each one to be answered correctly or incorrectly.
Anyone who has ever scanned through page aŌer page of
Internet daƟng “matches,” however, will soon ﬁnd themselves, to
quote the French novelist Marcel Proust, “unable to isolate and
idenƟfy . . . the successive phrases, no sooner disƟnguished than
forgotten.”22 
Proﬁles 
become 
a 
seemingly 
never-ending
repeƟƟon of terms like “cute,” “fun-loving,” “outgoing,”
“romanƟc” and “adventurous”: each striving to break the
formulaic mold of uniformity, but all ending up drawing from the
same cultural scripts of desirable characterisƟcs nonetheless. A
similarly auƟsƟc approach to human relaƟons can come across in
the proﬁle-building quesƟons asked by daƟng websites. “Does
money turn you on less or more than thunderstorms?” “What
about body piercings versus eroƟca?” “Would you rather your
potenƟal spouse has power or long hair?” Since everything is
given equal weighƟng on an interface level, and so many of the
algorithm’s inner workings are obscured, there is no way of
knowing just what affects our final score.
This issue is one that was touched upon in a 2013 arƟcle for the
Guardian’s “Datablog” (whose tagline reminds us that “Facts are
89

sacred”). 
RecounƟng 
her 
experiences 
of 
algorithmic
matchmaking, journalist Amy Webb writes, “When I ﬁrst started
online daƟng, I was faced with an endless stream of quesƟons. In
response, I was blunt, honest, and direct. Then my paƟence
started to wear thin, so I clicked on what I thought sounded
good.” Before long, Webb began to second-guess the answers
that she was being asked to enter. Certainly, she liked strong men
who work with their hands, but was this a veiled aƩempt to ask
whether she would date a lumberjack? AŌer all, “they’re strong
and work with their hands,” she says. “But I don’t want to marry
a lumberjack. I don’t even like trees that much.”23
Perhaps even worse is the existenƟal crisis that results from a
seemingly objecƟve algorithm determining that it has scoured the
Internet and that there is no one out there for you. “About 16
percent of the people that apply to us for membership we don’t
allow to parƟcipate on our site,” Neil Clark Warren says. “We
have seven diﬀerent reasons for excluding people. If they are
depressed—because depression is highly correlated with other
pathologies—we don’t let people parƟcipate. If they’ve been
married more than three Ɵmes we don’t let them parƟcipate,
which eliminates 15.5 percent of the marriages in America, which
involve at least one person that’s been married three Ɵmes
previously. 
In 
addiƟon, 
we 
have 
something 
called
‘Obstreperousness,’ who are people that you just can’t saƟsfy.
You give them one person and they’ll criƟcize them for being too
asserƟve. You give them another and they’ll claim they’re too
shy. We have ways of measuring that, and we ask that those
people don’t continue on the site.”
90

Taking the Chance Out of Chance
Gary Shteyngart’s 2010 novel Super Sad True Love Story is a
coming-of-age fable set in a dystopian New York City of the near
future. Because of the world’s total informaƟonal transparency,
no scrap of personal informaƟon is kept secret. All a character has
to do—as occurs during one scene in which the novel’s bumbling
protagonist, Lenny Abramov, visits a Staten Island nightclub with
his friends—is to set the “community parameters” of their iPhone-
like device to a parƟcular physical space and hit a buƩon. At this
point, every aspect of a person’s profile is revealed, including their
“fuckability” and “personality” scores (both ranked on a scale of
800), along with their ranked “anal/oral/vaginal” preferences.
There is even a recommender system incorporated, so that a
user’s history of romanƟc relaƟonships can be scruƟnized for
insights in much the same way that a person’s previous orders on
Amazon might dictate what they will be interested in next. As one
of Abramov’s friends notes, “This girl [has] a long mulƟmedia
thing on how her father abused her . . . Like, you’ve dated a lot of
abused girls, so it knows you’re into that shit.”24
The world presented by Super Sad True Love Story is, in many
ways, closer than you might think. Several years ago, the Human
Dynamics group at MIT created a mobile system designed to
sound an alarm if it was within ten yards of an aƩracƟve potenƟal
date. According to two of the researchers, Nathan Eagle and Alex
Pentland, the system was “developed to enable serendipitous
interacƟons between people” and was thus given the name
Serendipity. As per the promoƟonal literature supplied by the
team:
In a crowded room you don’t even have to bother
working out who takes your fancy. The phone does
all that. If it spots another phone with a good match
—male or female—the two handsets beep and
91

exchange 
informaƟon 
using 
Bluetooth 
radio
technology. The rest is up to you.25
Apps like Serendipity are part of a new trend in technology
called social discovery, which has grown out of social networking.
Where social networking is about connecƟng with people already
on your social graph, social discovery is all about meeƟng new
people. There are few beƩer examples in this book of The
Formula in acƟon than MIT’s Serendipity system. Here is a
problem (“chance”) and a task (“making it more eﬃcient”).
Executed correctly, the computer might provide an answer to the
quesƟon asked by Humphrey Bogart’s character in Casablanca.
“Of all the gin joints in all the towns in all the world, why did such-
and-such a person walk in to yours?” Because the device in their
pocket beeped.
Naturally, this raises as many quesƟons as it answers.
Serendipity is the occurrence of events by chance in such a way
that winds up being beneﬁcial to those involved. Equal weighƟng
is placed on both the “random” and “beneﬁcial” nodes, the
absence of either one meaning that no maƩer what occurs, we
may say with some certainty that it is no longer serendipitous.
The distinction becomes more pointed when love is involved. After
all, for many people the essence of love (if it can be described as
such) is the forging of a certain universal value out of pure
randomness: the idea that an apparently meaningless, chance
encounter brings with it the ultimate meaning.
One of the best literary analyses of this phenomenon comes in
Alain de BoƩon’s debut novel, On Love, in which the narrator
becomes smiƩen by a woman he meets on a Paris-to-London
ﬂight. In an eﬀort to inject a degree of raƟonality into the
irraƟonal, de BoƩon’s protagonist tries to calculate the odds of
such a meeƟng taking place, eventually coming up with an
answer of 1 in 5840.82. “And yet,” he writes, “it happened.”
92

The calculaƟon, far from convincing us of the
raƟonal arguments, only backed up the mythical
interpretaƟon of our fall into love. If the chances
behind an event are enormously remote, yet the
event occurs nevertheless, may one not be forgiven
for invoking a fatalisƟc explanaƟon? . . . [A]
probability of one in 5840.82 [makes it seem]
impossible, from within love at least, that [our
meeting] could have been anything but fate.26
Of course, as the late Steve Jobs might have said about fate:
“There’s an app for that.” Serendipity’s creators proudly state,
“Technology is changing the way we date. For the shy and single,
it has been the biggest aid to romance since the creaƟon of the
red rose.”
93

Wear Your Heart on Your Sleeve
MIT’s Serendipity project is not the first investigation of its kind. In
the late 1990s, a proximity matchmaking device called Lovegety
brieﬂy became all the rage in Japan, selling more than 1.3 million
units at an approximate price of $21. The aim of Lovegety was to
allow users to ﬁnd potenƟal dates in their vicinity. Users were
asked to input responses to several quesƟons, which in turn
became their personal list of preferences. When a mutual match
was discovered within range, the device alerted both parƟes to
one another’s presence. Compared to eHarmony’s barrage of
quesƟons and 29-point compaƟbility scale, Lovegety’s preference
opƟons were admiƩedly sparse. Even with this being the case,
however (and despite the preferences on oﬀer being extremely
superﬁcial, such as a shared desire to partake in karaoke), a
surprising number of users sƟll described the experience as
qualitaƟvely diﬀerent from that of being approached by a
stranger. “When you’re picked up out of the blue, there is always
an element of suspicion,” said one female customer. “But when
you’re brought together through the Lovegety, you’re more at
ease because you already have something in common. You
already have something to talk about.”27 In other words, the
technology was more than just an invisible mediator between two
parƟes. Like the shared ownership of a Porsche, being part of the
Lovegety club meant there was an automaƟc commonality
between both parƟes. To paraphrase Marshall McLuhan, the
medium really was the message.
Such devices don’t have to remain the sole province of bored
teenagers in Tokyo’s Harajuku district, of course. In the Republic
of Iceland, the Islendiga-App (“App of Icelanders”) applies similar
technology to the problem of solving the issue of accidental incest
in a country of 320,000, where almost everyone is distantly
related to one another. By accessing an online database of
residents and their family trees stretching back 1,200 years, and
94

then using an algorithm to determine the shortest path between
two points, the app is able to inform users how closely related
they are to the person they might be considering sleeping with.
The app is acƟvated by “bumping” phones, which in turn triggers
an “incest prevenƟon alarm” in situaƟons where the number of
genealogical steps is suﬃciently small as to cause potenƟal
distress at future family gatherings. In the words of the app’s
admiƩedly catchy slogan: “Bump the app before you bump in
bed.”28
Today, a large number of proximity-based social-discovery apps
—from Grindr to Crowded Room—vie for prominence in the
marketplace. Among the more interesƟng of those I came across
during my research was Anomo, an app that lays out its ambiƟons
to “democraƟze the way we socialize” in a way that combines
both the actual and virtual worlds. Cofounded by a former runner-
up on the U.S. version of The Apprentice, Anomo asks its users to
create video-game-style avatars for themselves, which then
become their virtual proxies as they negoƟate their way through
the actual world. “A lot of exisƟng social discovery apps are about
real proﬁles,” says cofounder Benjamin Liu. “You put your real
picture and name on there. The problem with that is that
people’s ﬁrst reacƟon to it is oŌen, ‘Wow, that’s really creepy.’”
Anomo users do enter their real informaƟon to the app, but this is
hidden unƟl users choose to reveal it in a series of quid pro quo,
“you show me yours, I’ll show you mine” exchanges. Prior to that,
users have the option of either chatting to one another using their
avatars (which consist of a short descripƟon, and one of around
100 diﬀerent cartoon representaƟons), or else playing a series of
“icebreaker” games to determine how much they have in
common with a person without having to endure a potenƟally
awkward chat ﬁrst. By comparing responses to quesƟons such as
“Do you prefer beer or wine?” or “Is a bicycle built for two cheesy,
cool, or dangerous?,” Anomo’s app provides a “compaƟbility
raƟng” ranking your answers next to those given by the other
95

person. “InteracƟng anonymously changes the playing ﬁeld,”
states 
Anomo’s 
promoƟonal 
literature. 
“Suddenly 
ﬁrst
impressions are not based on photos, but via a genuine
connection.”
96

Tapping the Scene
For the most part, social discovery works along the same lines as
Instant Messenger. Rather than focusing on ﬁnding “the one”
users are instead dealing with a pool of available parƟcipants that
exists in a constant state of ﬂux. It doesn’t maƩer if a parƟcular
message is not returned, or the person we originally wished to
“chat” with is no longer present. A person simply moves to the
next user in line and begins the process over again, since there
are always enough people online (at least in the case of those
apps that ﬁnd success) to counteract the loneliness of any given
moment.
While most of these apps require that people consciously
engage with them, this rule is by no means an absolute. In his
most recent book, Who Owns the Future?, computer scienƟst and
virtual reality pioneer Jaron Lanier recalls a panel he served on at
UC Berkeley, judging start-up proposals submiƩed by graduate
engineering students enrolled in an entrepreneurial program. A
group of three students presented a concept for quanƟfying
nights out so as to ensure maximum romanƟc success for those
involved. “Suppose you’re darƟng around San Francisco bars and
hot spots on a Saturday night,” Lanier remembers the group
pitching. “You land in a bar and there are a bounteous number of
seemingly accessible, lovely, and unaƩached young women
hanging out looking for aƩenƟon . . . Well, you whip your mobile
out and alert the network.”29 Such an idea would, of course,
never work, Lanier observes, since the data would invariably be
inaccurate and the scheme would wind up running on hope.
His words of warning have not been enough to derail the
technology’s core concept, however. SceneTap—previously
known as BarTabbers—started life in Chicago, although it has
since expanded to cover San Francisco, AusƟn, Columbus, New
York, Boston and Miami. With cameras installed in more than 400
drinking 
establishments, 
SceneTap 
uses 
facial-recogniƟon
97

technology and people-counƟng algorithms to help bar-hoppers
decide which locaƟons to hit up on a parƟcular night out.
Currently, the tool can provide real-Ɵme informaƟon on crowd
sizes, gender raƟos and the average age of patrons in any given
locaƟon—although that is not everything that’s planned. In 2012,
the start-up ﬁled a patent that, in the words of Forbes.com,
“[crosses] the creepy line . . . and then keeps running and spikes
the creepy football in the creepy end zone.”30 In a nutshell, the
patent is designed to allow SceneTap to collect much more
detailed data, including bar-goers’ race, height, weight,
aƩracƟveness, hair color, clothing type and the presence of
idenƟfying features like facial hair or glasses. In other words, one
might access the app and be presented with informaƟon along
the lines of: “The Raven is 73% full. Its crowd is made up of 33%
natural blondes, 57% bruneƩes, 3% redheads, 5% boƩle blondes,
and 2% other. The men on average are 5.8 feet tall, and 70% are
dressed in business casual. Women on average weigh 154.7
pounds, and 24% wear short skirts. 73% of patrons are white, 21%
Asian and 6% black. AƩracƟveness average for the locaƟon is 7
out of 10.”
The patent also allows for microphones to be placed in cameras
in order to pick up on what customers are saying, as well as for
the facial recogniƟon technology to idenƟfy people and link them
with their social networking proﬁles to determine “relaƟonship
status, intelligence, education and income.”
No doubt STD tracking could be added at a later date.
98

Your Sex Life with Models
“I’ve been in a relaƟonship with just one woman my enƟre adult
life, ever since I was in high school,” says Kevin Conboy, a soŌ-
spoken computer programmer with a mop of brown, curly hair
and a Brian Blessed beard. About ten years ago, when Conboy
was in his mid-twenƟes, working as a user-interface engineer, he
tried to work out how many Ɵmes he and his wife had had sex in
their Ɵme together. The idea grew in scale and, before long,
Conboy was spending his evenings working on an applicaƟon to
keep tabs on his sex life: methodically cataloguing and modeling
everything from the duraƟon and frequency of sex to its quality
and average time frame.
“I strive for honesty in my life as much as possible, but I thought
that this was one thing that was beƩer to build in secret and then
to show my wife once it was completed to ask for permission,” he
says. “At ﬁrst, she thought it was kind of weird, but she also
understood that to me code is a form of self-expression. She asked
me for the link to view it, and she would check up every now and
then to see how our sex life was doing. It got to the point where
she would tell her friends about what I had created, and it sort of
became this talking point.”
It was a friend of Conboy’s wife who eventually convinced him
to open up the app for public use. Giving it the name Bedpost, and
the tagline “Ever wonder how oŌen you get busy?,” the app
advises users to “simply log in aŌer every Ɵme [they] have sex and
ﬁll out a few simple ﬁelds. PreƩy soon, you’ll have a rolling history
of your sex life on which to reflect.”31
This mass of data can be visualized in a variety of forms—
including pie charts and scaƩer plots—with heat maps showing
diﬀerent intensiƟes of color based on the quanƟty and quality of
sex a parƟcular user is having. “The amount of data you can
aƩach to sexual acƟvity is uncapped,” Conboy says. “For instance,
if you’re on your phone, there is no reason you can’t record the
99

GPS data. You might be having sex all over the world and it could
be fun to look back at all that information.”
The goal of his work, Conboy says, “is to get you to think about
your sex life in a way that you hadn’t previously.” Its success hints
at something interesƟng: that the act of measuring and
quantifying sex in the form of zeroes and ones can in itself become
an eroƟc act. AŌer all, if text-based cybersex based enƟrely
around semioƟc interacƟon can be arousing then why can’t the
code that underpins it?
When I spoke with Conboy for the ﬁrst Ɵme, he was busy
rethinking the Bedpost user interface, which had remained
largely unchanged for several years. A number of the site’s best
suggesƟons, he said, had come from the app’s users themselves.
The popular feature of menstruaƟon tracking, for example, was
something he says he never would have come up with by himself.
In parƟcular, he was freƫng over whether or not to add an
orgasm counter to the list of available features. There was also
the quesƟon of adding greater social-networking capabiliƟes
(“This might freak some people out, but I like the idea of being
able to pull people in via your Facebook and TwiƩer accounts”),
along with the ability for users’ partners to log in and add their
own “tags”—thereby creaƟng a type of wiki criƟcism of “sex-as-
performance,” which could take place long aŌer the act had
taken place.
The ability to share the experience of sex is one that is
parƟcularly fascinaƟng. What if we were able to get closer to the
other’s sexual experience not simply by asking, “How was it for
you?” but by actually delving into the data for evidence of the
other person’s saƟsfacƟon? This wouldn’t necessarily have to rely
on anything as subjecƟve as user tags. A number of pieces of
wearable tech have already shown themselves to be uniquely
valuable when it comes to quanƟfying the sex experience. By
recording heart rate, perspiraƟon output and moƟon paƩerns,
the BodyMedia FIT armband can, for instance, recognize whether
100

wearers are having sex at any parƟcular Ɵme. More intrusively, it
is even able to gauge whether wearers are having good sex or
not, since analyzing spikes in the various metrics can reveal if a
partner has faked an orgasm.32
Awkward conversaƟons aside, this available data has the
potenƟal to raise a number of issues. For example, the spouse
who suspects his or her signiﬁcant other of cheaƟng could access
their data and quesƟon why 100 calories had been burned oﬀ
between 0:13 and 2:00 A.M. on a parƟcular evening, without their
taking a single step, and with them falling asleep immediately
aŌerward. Removing said tech (or else “forgeƫng” to wear it)
might be grounds for suspicion in itself. Even arranging for illicit
encounters to take place during daylight hours, when data
anomalies would theoreƟcally be more easily explained away,
would be of liƩle use since sexual acƟviƟes look diﬀerent in terms
of ﬁtness data than other energeƟc acƟviƟes like weight-liŌing,
jogging, yoga, martial arts and cycling.
For Conboy, Bedpost helps to normalize him within the sexual
spectrum. “The aggregate data gives me a sexual conﬁdence,” he
says. “It’s nice to know that you have had sex a certain number of
Ɵmes this month, or this year. It’s a reminder that my sex life is
healthy. The numbers help me to relax about whatever I’m
obsessing over at the Ɵme. I don’t think I could have an eroƟc sex
life without the confidence the data gives me.”
Of course, this raises yet more quesƟons—not least how much
data is enough?
No maƩer how scienƟﬁc the intenƟon, the moment we start
measuring we also begin limiƟng, not just based on what the
measurement tool is designed to capture, but on those metrics
we consider worthy of measurement at the Ɵme. In the case of
Bedpost, Conboy might add an orgasm counter, but how about
respiratory rate? Or if he adds respiratory measurements, what
about perspiraƟon levels and heart rate? And so on it goes—the
ideal and meaning of what we are measuring receding further
101

and further from view, like advancing on an Impressionist painƟng
and seeing what from a distance had been a convincing re-
creation of a landscape dissipate into a sea of painted dots.
102

Night of the Loving Dead
If we accept the (highly quesƟonable) view that true love, like
opportunity, knocks only once, then how about the idea that love
must automaƟcally be separated by its deathly binary opposite? It
has long been a technological pipe dream that man should be able
to transport his “spirit” beyond the corporeal body and into the
metallic laƫces of a computer. In The Enchanted Loom: Mind in
the Universe, NASA futurist Robert Jastrow waxes lyrical about his
hopes for a future in which the human brain could be “ensconced
in a computer . . . liberated from the weakness of the mortal
ﬂesh . . . in control of its own desƟny.” 33 As with natural birth,
such a form of reproducƟon would ensure our immortality
through the conƟnuaƟon of our perceived wisdom, good and
happiness. To frame this as a quesƟon about love, what if
apparent death didn’t therefore part us from our loved ones but
rather, as with a buƩerﬂy emerging from a chrysalis, simply
meant a change in form factor?
It is into this space that applicaƟons such as IfIDie, DeadSocial
and LivesOn enter the frame. All three apps exist in various
magnitudes of complexity. DeadSocial and IfIDie funcƟon as what
can essenƟally be described as netherworldly “out of oﬃce”
services: allowing nothing more complicated than the wriƟng and
recording of Ɵmed Facebook messages, acƟvated upon a
parƟcular user’s death, and which can then be sent out to lovers,
friends and family for years to come. In a video designed to
promote the former service, we are told how the widow of a
deceased DeadSocial user can even be “the recipient of an
inappropriate joke from Ɵme to Ɵme.”34 IfIDie, meanwhile,
recommends the recording of a webcam video in the event that
one wishes to bid a fond farewell or (accompanied in literature by
an image of a heavenly cloud in the shape of a raised middle
finger) “settle . . . an old score” with a member of the living.35
More complex, both ethically and technologically, is LivesOn—
103

which carries the disƟnctly Twilight Zone tagline, “When your
heart stops beaƟng, you’ll keep tweeƟng.”36 LivesOn uses
machine-learning processes to siŌ through your past social-
network feeds, looking for the subjects, likes and news arƟcles
that interested you during life, so that similar subjects can be
tweeted about on your behalf aŌer death. Users are encouraged
to help their LivesOn “train and grow” by following a person’s
exisƟng TwiƩer feed and analyzing it, learning tastes, likes,
dislikes and eventually even syntax. “The goal is to get it to almost
become like a twin,” says creator Dave Bedwood.
That isn’t all. Several years ago, news broke that the U.S.
Department of Defense was developing a project designed to
create “a highly interacƟve PC or web-based applicaƟon to allow
family members to verbally interact with virtual rendiƟons of
deployed Service Members.” Using high-resoluƟon 3-D rendering,
the high-level concept was to allow a child or spouse to engage in
a simulated conversaƟon with an absent parent or partner, and
receive responses to stock phrases including “I love you” and “I
miss you.” “I’m not saying this kind of ghost is for everyone,”
commented one writer for Slate, with what is perhaps something
of an understatement,
but I dare you to tell a child who has lost her father
in Iraq or Afghanistan that she can’t keep a virtual
rendiƟon of him to help her go to sleep. And I dare
you to stop the millions of others who will want
ghosts of their own when today’s military project
becomes, once again, tomorrow’s mass market.37
While it would be the harsh writer who would willingly inﬂict
pain on a person who has lost their loved one (although it seems
odd to blame said writer for the absence of the parent/spouse to
begin with), one cannot help but think that Slate has not properly
thought through its posiƟon here. A computer program that is
104

able to convincingly replicate the appearance, voice and sentence
structure of an absent or deceased relaƟve is neither
quanƟtaƟvely or qualitaƟvely the same thing as having said
person there—much as a dead body is not the same as a live one,
even if it might look no diﬀerent in a sƟll photograph. To presume
that the person we fall in love with, or even just the objects that
hold personal value for us, are simply a composite of their
individual properƟes is incorrect. Studies have shown that taking
away the security blanket or teddy bear of a young child and
subsƟtuƟng it with a duplicate will not see the replacement
accepted in place of the original. We may well react in the same
way if our Rolex watch was replaced with an unoﬃcial replica, no
maƩer how convincing a knockoﬀ this might prove to be. Taken
to extremes, it would be the remarkably unfeeling doctor who
would reassure the parent of a dead child, or the widowed spouse
of a dead husband or wife, by telling them that their loss needn’t
maƩer since we can clone the person in quesƟon—or beƩer yet,
that they have a twin who is sƟll alive. In his wriƟng, the French
phenomenological philosopher Maurice Merleau-Ponty explored
this idea, by disƟnguishing between what he views as the living
and dead properƟes of an object. It is in The Formula’s empirical
desire to reduce percepƟon and feeling to that which is
observable, he argues, that all elements of “mystery” are lost.38
105

Desiring Machines
There is liƩle dispuƟng that Kari is an aƩracƟve girl. She has
bright blue eyes, made all the more aƩenƟon-grabbing by the
smoky eye makeup she wears. Her hair is cut to shoulder length
and is boƩle blond in color, with just the slightest trace of what
one presumes to be her natural brown creeping in at the roots.
Her face suggests that she is in her early twenƟes, and as she
looks toward the camera, her full lips part slightly as a coqueƫsh
smile plays over her features. It’s only then that you noƟce for the
first time that she is wearing cotton candy– colored lip gloss.
If Kari sounds a liƩle, well, fake, that is enƟrely
understandable. She is not real, at least not in the sense that you
or I are—exisƟng purely in the form of computer code. Her name
is an acronym, standing for Knowledge Acquiring and Response
Intelligence, and she is what is known in the trade as a
“chaƩerbot,” an algorithm designed to simulate an intelligent
conversaƟon with an actual person. Kari (or, perhaps, K.A.R.I.) is
the creaƟon of Sergio Parada, a thirtysomething programmer
living in West Palm Beach, Florida.
Born into a military family in San Salvador, El Salvador, Parada
moved to the United States with his parents when he was ten in
order to escape the Salvadoran civil war. AŌer studying video
games and programming at Chicago’s Columbia College, Parada
picked up a job working on the Leisure Suit Larry series of adult
video games for the now-defunct developer Sierra Entertainment.
It was while he was employed on the game tentaƟvely Ɵtled Lust
in Space (aka Larry Explores Uranus) that Parada came up with
the idea of creaƟng a piece of soŌware centered enƟrely around
one’s interacƟons with a solo female character. “It was
something brand new,” he recalls, describing his aim as being
“not just a girl simulaƟon, but a relaƟonship simulaƟon.” Unlike
Leisure Suit Larry, which followed an aﬀable loser as he aƩempts
to bed a procession of ludicrously aƩracƟve women, in Kari
106

Virtual Girlfriend there would be no set narraƟve. This necessarily
meant that in the tradiƟonal game-playing sense there was also
no way to win. Like a real relaƟonship, the game conƟnues
indeﬁnitely, with the reward being a relaƟonship that grows and
deepens.
In the same way that people anthropomorphize pets,
aƩribuƟng them with human characterisƟcs to explain behavior,
so too does Kari build on the willingness to protect, feed, shelter
and guard machines that is second nature to a generaƟon who
grew up with Tamagotchis: the handheld digital “pets” that
enjoyed a burst of popularity in the late 1990s. A number of
psychology researchers (with two of the most prominent being
Mihaly Csikszentmihalyi and Eugene Rochberg-Halton) have
invesƟgated the creaƟon-of-meaning process that is called
“psychic energy.” The idea is that as a person invests more psychic
energy in an object, they aƩach more meaning to it, thus making
the object more important to them, which makes their
aƩachment grow all the stronger. This eﬀect is only strengthened
by the fact that in Kari’s case, she encourages the user to talk and
proves an adept listener. This might suggest an addiƟonal reason
for the magnetic pull that Kari wields over some users.
In a 1991 study by Arthur Aron, a psychology professor at Stony
Brook University in New York, pairs of students who had never
previously met were placed in a room together for 90 minutes,
during which Ɵme they exchanged inƟmate pieces of personal
informaƟon, such as their likely reacƟon to the loss of a parent or
their most embarrassing personal moments. At the end of the
Ɵme period, the two were asked to stare unspeaking into one
another’s eyes for a further two minutes. The students were
asked to rate the closeness they felt at various intervals and, aŌer
just 45 minutes, these raƟngs outscored the closeness felt by 30
percent of other students, ranking the closest relaƟonships they
had experienced in their lives. Aron’s conclusion was that
disclosure is therefore both fast-acƟng and powerful as a device
107

to increase personal aƩracƟon. (As if to bear this theory out,
despite the students exiƟng the room from diﬀerent doors aŌer
the experiment was over, so as to remove any sense of obligaƟon
to stay in touch, the ﬁrst pair to take part were married six
months later.)
One major diﬀerence in the relaƟonship Kari has with her
users, when compared to the vast majority of romanƟc
relaƟonships (aside from the obvious one), is that in order to avail
oneself of her services, a person must ﬁrst purchase her. The
standard version of the program costs $24, while the “Kari
Ultimate Package”—which comes with an Avatar Studio to “make
your own girls”—will set users back $90.39
For those who do buy her, the relaƟonship conforms to what
French philosopher Michel Foucault refers to as “spirals of power
and pleasure.” The user’s pleasure is linked to the power they
hold over their virtual girlfriend. It is a technological reimagining
of the Pygmalion fantasy, in which Kari takes on the role of Eliza
DooliƩle: the naive, young, working-class girl iniƟated into the
sexual world by the older, wealthier, more experienced Professor
Henry Higgins.40 Of course, where Henry Higgins handcraŌed his
idealized companion by teaching her genteel manners and
elocuƟon, with Kari the user has the ability to ﬁne-tune speciﬁc
areas of her personality, such as her “love,” “ego” and “libido”
raƟngs, which are measured on an adjustable scale from 0 to 10.
Find that Kari skips from topic to topic when she talks? Just
increase her “stay on topic”-ness, or else change the number of
seconds between unprovoked comments. Worry that she is
starƟng to act too aloof? Lower her “independence” and, if that
doesn’t work, her “memory scope.” This can have an
unfortunately detrimental eﬀect on Kari’s AI, leading to one
prickly reviewer complaining that “one minute she’s spouƟng
some dime store philosophy, the next she’s asking to be ‘f***ed
so hard,’” and describing the result as “the video game equivalent
of daƟng a drunken ﬁrst-year philosophy student with really low
108

self-esteem.”41
UlƟmately, it is Kari’s malleability that forms the crux of her
appeal. Unlike the other “ﬂickering signiﬁers” we might fall in love
with—be they ﬁlm actor, pop star, or doe-eyed supermodel
plastered across a billboard—Kari can adjust to appeal to each
person individually; quite literally living up to the prosƟtute’s
classic sales pitch, “I can be whoever you want me to be.”
“Kari ﬁlls that hole we all have inside ourselves for a connecƟon
with someone,” Parada notes. “Some users write to me with
word of the interesƟng projects they are conducƟng. One wanted
to get her pregnant and see the nine-month cycle of having a
baby with his Kari. Another wanted to give her menstrual cycles.
Another fed his enƟre journals to it and said it was beƩer than
therapy. Whatever you’re up to, Kari is a big canvas to work with.
It’s a canvas of awareness and thought; an extension of the self.
Whatever we choose to expose our Karis to is what they will
become. It’s like creaƟng a being from the things we choose to
teach it. The best part is that this is all integrated into an avatar
that can talk back to you, be your best friend—and even love
you.”42
109

Love and Sex with Algorithms
The Kari-esque dream of an algorithmic lover has existed at least
as long as there have been personal computers, from 1984’s
Electric Dreams (in which a San Francisco architect’s newly
installed computer system grows jealous of its owner’s
burgeoning relaƟonship with an aƩracƟve neighbor) to Spike
Jonze’s 2013 film, Her.
In Japan, there exists a popular genre of video game known as
the “daƟng sim.” The typical daƟng sim (short for “simulaƟon”)
allows the player to take control of an avatar, who is then
confronted with a number of potenƟal love interests whom they
must choose between. As with role-playing games, daƟng sims
rely heavily on staƟsƟcs. A character’s conversaƟon with a would-
be mate is measured according to their choice of appropriate
lines of dialogue, with overall scores improving or worsening
depending upon which line is chosen at a parƟcular Ɵme. In one
such scene, a female character is depicted eaƟng an ice cream, a
daub of it having smeared her cheek. Players are given the opƟon
of “Wipe it oﬀ for her” or “Pretend I didn’t see it,” while an
addiƟonal piece of informaƟon warns, “Her aﬀecƟon level will
change according to your choices.”43
Perhaps the most notable exponent of this game-space view of
relaƟonships is arƟﬁcial intelligence scholar and chess master
David Levy. In Levy’s most recent book, Love and Sex with Robots,
he oﬀers the predicƟon that not only will the concepts suggested
in his Ɵtle be possible in the future but that, by 2050, they will be
routine. As Levy argues:
One can reasonably expect that a robot will be
beƩer equipped than a human partner to saƟsfy the
needs of its human, simply because a robot will be
beƩer 
at 
recognizing 
those 
needs, 
more
knowledgeable about how to deal with them, and
110

lacking any selﬁshness or inhibiƟons that might, in
another human being, miƟgate against a caring,
loving approach to whatever gives rise to those
needs.44
In 
a 
moment 
of 
parƟcularly 
quesƟonable 
human
understanding, Levy theorizes that an algorithm could analyze
the brain’s “love measure” by way of an fMRI scanner and use this
informaƟon to hone speciﬁc strategies to make a person fall in
love. Like a game of chess, the path to true love would thus
consist of a series of steps, each move either lowering or raising
the love measure exhibited in the brain. In the manner of a smart
central-heaƟng thermostat, the aim would be to automaƟcally
adjust warmth or coolness so as to keep condiƟons at an opƟmal
level. SuggesƟon that you watch a Tchaikovsky ballet together
met with a frosty response? Try complimenƟng the other person’s
new haircut instead. An algorithm could even take note of the
low-level features we might be struck by but unable to verbalize
in partners, such as the way the lover ﬂicks their hair or lights a
cigarette, and incorporate this into its seductive repertoire.
Like Kari, Levy’s proposed automated lover could even answer
the old philosopher’s quesƟon of how love demands that inﬁnite
desire be directed toward a ﬁnite object. Fancy a gawky, reƟring
lover one night and a “woman looking like Marilyn Monroe . . .
with the brainpower and knowledge of a university professor and
the conversaƟonal style of a party-loving teenager” the next? No
problem. Rather than having to willfully idealize the human lover
to make them unique, the ideal partner could be created, then
constantly modiﬁed in the manner of a Facebook proﬁle. Even the
suggesƟon that we might be drawn to individuals with ﬂaws
doesn’t appear to faze Levy. On the contrary, if a “perfect”
relaƟonship “requires some imperfecƟons of each partner to
create occasional surprises . . . it might . . . prove necessary to
program robots with a varying level of imperfecƟon in order to
111

maximize their owner’s relationship satisfaction,” he writes. Later
on, he gives an example of how this might pracƟcally funcƟon, as
he imagines a lover’s Ɵﬀ in which the human party in the couple
ﬁnally loses paƟence with their algorithm’s operaƟng eﬃciency
and yells, “I wish you weren’t always so goddamn calm.” To
resolve this issue and restore stability to its opƟmal level, Levy
suggests that here it might be necessary for the algorithmic
partner to simply “reprogram itself to be slightly less emoƟonally
stable.”
It’s easy when you know how. Although one can’t help but
think you risk losing a certain spark in the process.
112

CHAPTER 3
Do Algorithms Dream of Electric Laws?
Adecade ago, Walmart stumbled upon an oddball piece of
informaƟon while using its data-mining algorithms to comb
through the mountains of information generated by its 245 million
weekly customers. What it discovered was that, alongside the
expected emergency supplies of duct tape, beer and boƩled
water, no product saw more of an increase in demand during
severe weather warnings than strawberry Pop-Tarts. To test this
insight, when news broke about the impending Hurricane Frances
in 2004, Walmart bosses ordered trucks stocked with the
Kellogg’s snack to be delivered to all its stores in the hurricane’s
path. When these sold out just as quickly, Walmart bosses knew
that they had gained a valuable glimpse into both consumer
habits and the power of The Formula.1
Walmart execuƟves weren’t alone in seeing the value of this
discovery. At the Ɵme, psychologist Colleen McCue and Los
Angeles police chief Charlie Beck were collaboraƟng on a paper
for the law-enforcement magazine The Police Chief. They too
seized upon Walmart’s revelaƟon as a way of reimagining police
work in a form that would be more predicƟve and less reacƟve.
EnƟtled “PredicƟve Policing: What Can We Learn from Walmart
and Amazon about FighƟng Crime in a Recession?,” their 2009
paper immediately captured the imaginaƟon of law-enforcement
professionals around the country when it was published.2 What
McCue and Beck meant by “predicƟve policing” was that, thanks
to advances in compuƟng, crime data could now be gathered and
analyzed in near-real Ɵme—and subsequently used to anƟcipate,
prevent and respond more eﬀecƟvely to those crimes that would
113

take place in the future. As per the slogan of Quantcast—the
web-analyƟcs company I described back in Chapter 1—it means
that police could “Know Ahead. Act Before.”™
Today, there is no man more associated with the ﬁeld of
predicƟve policing than Los Angeles Police Department’s Captain
Sean Malinowski. Despite his reputaƟon within the force as a
“computer guy,” Malinowski’s background is actually in
markeƟng. Prior to his joining the LAPD in 1994, he worked as a
markeƟng account execuƟve, helping chewing gum and
margarine companies roll out new products by ﬁguring out how
best to hit customers with targeted coupons and special oﬀers.
Through a stroke of good fortune Malinowski ended up working
on a drunk-driving campaign with several oﬃcers from the City of
Detroit Police Department. He found working with police to be a
revelation. “Up until that point I was all gung-ho on the marketing
thing,” he says. “I hadn’t thought about the police force before
that. Part of it was that I had reached a Ɵme in my markeƟng
career where I was thinking, ‘Christ, is this it: my whole life’s
mission is going to be to sell edible fats and oils?’ The cops I was
working with had a real mission; they were trying to do something
bigger.”
Through some mutual friends, Malinowski was introduced to a
former New York cop, who had recently moved to Chicago to
work as an academic. AŌer speaking with him, Malinowski quit
the markeƟng business and went back to school. Several years
later he graduated from the University of Illinois with a master of
arts in criminal jusƟce. Then it was on to the LAPD Training
Division, where Malinowski wound up being elected class
president. His ﬁrst job with the police department proper was on
Paciﬁc Beach Detail, cycling endlessly up and down the Venice
beachfront to interview street vendors and ensure public safety.
Malinowski’s big break came when he was assigned to work for
Chief William BraƩon: ﬁrst as his aide and later as his execuƟve
oﬃcer. BraƩon had just moved to Los Angeles from New York
114

City, where he’d established a formidable reputaƟon as the cop
who had halved the city’s murder rate in the span of a few years.
BraƩon’s methods, though eﬀecƟve, were undeniably oĪeat.
Prior to the New York Police Department, he had headed up the
New York transit police, where he had transformed the New York
subway system from a veritable hellhole into a clean, orderly
microcosm of a law-abiding society by . . . cracking down on fare
dodging. In other words, at a Ɵme when serious crimes on the
subway were at an all-Ɵme high, BraƩon focused his aƩenƟon on
making sure that people paid for their Ɵckets. His reasons, as
recalled in his 2009 memoir, were simple: fare evasion was a
gateway drug to more serious crime. “LegiƟmate riders felt that
they were entering a place of lawlessness and disorder,” he
noted. “They saw people going in for free and began to quesƟon
the wisdom of abiding by the law . . . The system was veering
toward anarchy.” 3 By stopping and searching law violators for
even the most minor of infracƟons, troublemakers decided that it
was easier to pay their fares and leave their weapons (which were
oŌen uncovered during searches) at home. Crime fell
exponentially.
Relocated to Los Angeles, Chief BraƩon wanted to use some of
that preempƟve mojo on a grander scale. Working under him for
ﬁve years, Malinowski witnessed ﬁrsthand how BraƩon was able
to take a department stymied by inerƟa and push through
changes by sheer force of will. “When you’re in a bureaucraƟc
organization, you get so used to the barriers coming up that it can
limit people’s creativity,” Malinowski says. “What Bratton instilled
in me was not to be affected by all that. He taught me to think big
and make things happen.”
More than anything, BraƩon was always on the lookout for the
next “big idea” to revoluƟonize his work. In predicƟve analyƟcs,
he felt that he had found it. What BraƩon had noƟced was a
correlaƟon between crime rate and the speed at which that data
could be analyzed. In 1990, crime data was collected and
115

reviewed only on an annual basis. At the Ɵme, crime was on a
steep rise in the majority of American ciƟes. When 1995 rolled
around, crime data could be looked at on a month-by-month
basis. Crime rates during that same period slowed. With crime
rates now viewable on a moment-to-moment basis, BraƩon
posited that this could lead to an actual drop in crime rates by
predicƟng where crimes would next take place. In the same way
that companies like Quantcast and Google are able to mine user
data for insights, so too was the idea of predicƟve policing, that
rather than simply idenƟfying past crime paƩerns, analysts could
focus on ﬁnding the next crime locaƟon within an exisƟng
paƩern. To put it in Amazon terms: You stole a handbag; how
about robbing a liquor store?
116

Why Is Crime Like an Earthquake?
It is widely accepted that crimes don’t occur randomly dispersed
across a parƟcular area, but rather that they exist in small
geographical clusters, known as “hotspots.” In SeaƩle, for
example, crime data gathered over a 14-year period shows that
half of all crime can be isolated to just 4.5 percent of the city’s
streets. A similar observaƟon holds true for Minneapolis, where
3.3 percent of all street addresses generate more than 50
percent of calls to police. Over a 28-year stretch, a marginal 8
percent of streets in Boston accounted for a whopping 66 percent
of street robberies. Knowing about these hotspots, and the type
of crime that is likely to take place at them, can be vital in helping
direct police to specific parts of a city.4
Imagine, for example, that there are arrests for assault every
Saturday night outside your local pub, the White Hart. If that
proves to be the case, it wouldn’t be too diﬃcult to predict that
future Saturday nights will see similar behavior at that locaƟon,
and that staƟoning a police oﬃcer on the door at closing Ɵme
could be enough to prevent future fights from breaking out.
It was this insight that prompted Chief BraƩon to ask Sean
Malinowski to help him.
On BraƩon’s advice, Malinowski started driving over to the
University of California, Los Angeles every Friday aŌernoon to
meet with members of the math and computer science
departments. The Los Angeles Police Department had agreed to
hand over its impressive data set of crime staƟsƟcs—which
amounted to approximately 13 million crime incidents recorded
over an 80-year period—for the largest study of its kind.
Malinowski relished the experience of working with the UCLA
researchers. As had happened a decade earlier when he ﬁrst
started working with police on his drunk-driving campaign, he
found himself being drawn into the work being done by the
computer scienƟsts as they combed through the data looking for
117

patterns and, hopefully, formulas.
“I loved those days,” Malinowski recalls. Of parƟcular interest
to him was the work of George Mohler, a young mathemaƟcian
and computer scienƟst in his mid-twenƟes, who was busy working
on an algorithm designed to predict the aŌereﬀects of
earthquakes. Mohler’s work was more relevant than it might
iniƟally sound. In the same way that earthquakes produce
aŌershocks, so too does crime. In the immediate aŌermath of a
house burglary or car theŌ, that parƟcular locaƟon becomes
between 4 and 12 Ɵmes more likely to be the scene of a similar
crime event. This is a type of contagious behavior that is known as
the “near repeat” eﬀect. “OŌen a burglar will return to the same
house or a neighboring house a week later and commit another
burglary,” Mohler explains. Taking some of Mohler’s conclusions
about earthquakes—and with help from an anthropologist named
Jeﬀ BranƟngham and a criminologist named George Tita—the
team of UCLA researchers were able to create a crime predicƟon
algorithm that divided the city into diﬀerent “boxes” of around
0.15 kilometers squared, and then ranked each of these boxes in
order of the likelihood of a crime taking place.
A three-month randomized study using the algorithm began in
November 2011. “Today . . . is historic,” began Malinowski’s
address in that day’s Patrol Alert. His division, known as Foothill,
covered seven main beats: La Tuna Canyon, Lake View Terrace,
Pacoima, Shadow Hills, Sun Valley, Sunland and Tujunga. When
divided up, these amounted to 5,200 boxes in total. At the start of
that day’s roll call, Foothill patrol oﬃcers were handed individual
mission maps, each with one or more boxes highlighted. These
were the locaƟons deemed as “high probability” and were
accompanied by staƟsƟcal predicƟons about the type of crime
that was likely to occur there. “What we’re asking you to do is to
use your available Ɵme to get into those boxes and look around
for people or condiƟons that indicate that a crime is likely to
occur,” Malinowski said, addressing his team. “Then take
118

enforcement or preventative action to stop it.”
The experiment ran unƟl February the following year. In
March, the results were evaluated, and a decision was made
about whether or not to roll out the technology. Findings were
impressive. During the trial, Foothill had seen a 36 percent drop in
its crime rate. On days where the algorithm was dictaƟng patrols,
predicƟons about which crimes were likely to take place were
twice as accurate as those made by a human analyst. “Part of the
reason is that human brains are not suited to rank 20 hotspots
across a city,” George Mohler says. “Maybe they can give you the
top one or two, but at seven or eight they’re just giving you
random guesses.”
If there was a teething problem to all of this, it oŌen came
from Malinowski’s own men. “You do run into people who say
they don’t need a computer to tell them where the crime is,” he
admits. “A lot of guys try to resist it. You show them the forecasts
and they say, ‘I could have told you that. I could have told you
that the corner of Van Nuys and Glenoaks has always been a
problem.’ I say, ‘That’s always been our problem, huh? How long
have you been working here?’ They go, ‘Ten years I’ve been
working this spot.’ And I say, ‘Then why the hell is it still a problem
if you’ve known about it for ten years? Get out there and fix it.’”
Following the Foothill study, algorithmic policing was made
available to all Los Angeles precincts. Similar algorithms have now
been adopted by other police departments around the United
States. Malinowski says that he sƟll feels responsible for his
oﬃcers but is geƫng used to his less hands-on role in their
deployment. “You have to give up a bit of control to let the
algorithm do its work,” he notes. Chief BraƩon, meanwhile,
reƟred from the Los Angeles Police Department. Following the
2011 England riots, he was approached by David Cameron about
coming to the UK as Commissioner of London’s Metropolitan
Police Service. The oﬀer was ulƟmately vetoed on the basis that
BraƩon is not a BriƟsh ciƟzen. Instead, he was oﬀered an
119

advisory role on controlling violence, which he gladly accepted.5
The UCLA team has since raised several million dollars in
venture funding and spun their algorithm out as a private
company, which they named PredPol. 6 In December 2012,
PredPol made it to England, with a four-month, $200,000 trial
taking place in Medway, Kent. In that case, the algorithm was
credited with a 6 percent fall in street violence. Similar schemes
have now taken place in Greater Manchester, West Yorkshire and
the Midlands with similarly promising results.7 Although some
local councillors were worried, believing that predicƟve analytics
would leave rural areas without police cover, or else lead to job
cuts, others felt the soŌware was innovaƟve and could bring
about a more effective use of resources.8
As Malinowski says, predicƟve policing isn’t simply a maƩer of
catching criminals. “What we’re trying to do is to be in the right
place at the right Ɵme, so that when the bad guy shows up he
sees the police and is deterred from commiƫng a crime.” In the
end it all comes back to supermarkets. “We’re like a greeter in
Walmart,” Malinowski says. “It just puts people on noƟce that
you’re looking at them.”
120

The Moral Statisticians
The idea of integraƟng staƟsƟcs into the world of criminology
might seem new. In fact, its roots go back to 19th-century France
and to two men named André-Michel Guerry and Adolphe
Quetelet. Both Guerry and Quetelet were talented staƟsƟcians,
who came to the ﬁeld aŌer ﬁrst pursuing other careers. In
Guerry’s case this had been law. For Quetelet, astronomy. Each
was profoundly inﬂuenced by the work of a man named Isidore
Marie Auguste François Xavier Comte—beƩer known as Auguste
Comte. Between 1817 and 1823, Comte had worked on a
manuscript entitled Plan of the Scientific Operations Necessary for
the ReorganizaƟon of Society. In it he argued that the ideal
method for determining how best to run a society would come
from studying it in the manner of the natural sciences. In the
same way that Isaac Newton could formulate how physical forces
might impact upon an object, so did Comte posit that social
scienƟsts should be able to discover the universal laws of “social
physics” that would predict human behavior. 9
This idea appealed immensely to Guerry and Quetelet, who
had a shared interest in subjects like criminology. At the age of
just 26, Guerry had been hired by the French Ministry of JusƟce to
work in a new ﬁeld called “moral staƟsƟcs.” Quetelet,
meanwhile, was enthusiasƟc about the opportunity to take the
mathemaƟcal tools of astronomy and apply them to social data.
To him:
The possibility of establishing moral staƟsƟcs, and
deducing instrucƟve and useful consequences
therefrom, depends enƟrely on this fundamental
fact, that man’s free choice disappears, and remains
without sensible eﬀect, when the observaƟons are
extended over a great number of individuals.10
121

Of beneﬁt to Guerry and Quetelet was the fact that each was
living through what can be described as the ﬁrst “golden age” of
Big Data. From 1825, the Ministry of JusƟce had ordered the
creaƟon of the ﬁrst centralized, naƟonal system of crime
reporƟng—to be collected every three months from each region
of the country, and which recorded all criminal charges brought
before French courts. These broke crimes down into the category
of charge, the sex and occupaƟon of the accused, and the
eventual outcome in court. Other naƟonally held data sets
included staƟsƟcs concerning individual wealth (indicated through
taxaƟon), levels of entrepreneurship (measured through number
of patents filed), the percentage of military troops who could both
read and write, immigraƟon and age distribuƟon around the
country, and even detailed lists of Parisian prosƟtutes—ordered
by year and place of birth.11
During the late 1820s and early 1830s, Guerry and Quetelet
worked independently to analyze the available data. One of the
ﬁrst things each remarked upon was the lack of variance that
existed in crime from year to year. This had parƟcular relevance
in the ﬁeld of social reform—since reformers had previously
focused on redeeming the individual criminal, rather than viewing
them as symptoms of a larger problem.12 Quetelet referred to
“the terrifying exacƟtude with which crimes reproduce
themselves” and observed that this consistency carried over even
to a granular scale—meaning that the proporƟon of murders
commiƩed by gun, sword, knife, cane, stones, ﬁre, strangulaƟon,
drowning, kicks, punches and miscellaneous instruments used for
cuƫng and stabbing remained almost enƟrely stable on an
annual basis. “We know in advance,” he proclaimed, “how many
individuals will dirty their hands with the blood of others; how
many will be forgers; how many poisoners—nearly as well as one
can enumerate in advance the births and deaths that must take
place.’’ Guerry, too, was struck by “this ﬁxity, this constancy in
the reproducƟon of facts,” in which he saw ample evidence that
122

Comte’s theories of social physics were correct; that amid the
noise of unfiltered data there emitted the dim glow of a signal.
A number of fascinaƟng Ɵdbits emerged from the study of the
two scholars. For instance, Quetelet noƟced a higher than usual
correlaƟon when examining the relaƟonship between suicide by
hanging and marriages involving a woman in her twenƟes and a
man in his sixƟes. Not to be outdone, Guerry also turned his
aƩenƟon to suicide (subdivided by moƟve and method for ending
one’s life) and concluded that younger men favored death by
pistol, while older males tended toward hanging.
Other relaƟonships proved more complex. Previously, it had
been widely thought that poverty was the biggest cause of crime,
which meant that wealthier regions of the country would surely
have a lower crime rate than poorer ones. In fact, Guerry and
Quetelet demonstrated that this was not necessarily the case.
While the wealthiest regions of France certainly had lower rates
of violent crime than did poorer regions, they also experienced far
higher rates of property crime. From this, Guerry was able to
suggest that poverty itself was not the cause of property crime.
Instead, he pointed to opportunity as the culprit, and argued that
in wealthier areas there is more available to steal. Quetelet built
on this noƟon by suggesƟng the idea of “relaƟve poverty”—
meaning that great inequality between poverty and wealth in the
same area played a key role in both property and violent crimes.
To Quetelet, relaƟve poverty incited people to commit crimes
through envy. This was especially true in places where changing
economic condiƟons meant the impoverishment of some, while at
the same Ɵme allowing others to retain (or even expand) their
wealth. Quetelet found less crime in poor areas than in wealthier
areas, so long as the people in the poor areas were able to saƟsfy
their basic needs.
Guerry published his ﬁndings in a slim 1832 volume called Essai
sur la staƟsƟque morale de la France (Essay on the Moral
StaƟsƟcs of France). Quetelet followed with his own Sur l’homme
123

et le développement de ses facultés (On Man and the
Development of His FaculƟes) three years later. Both works
proved immediately sensaƟonal: rare instances in which the
conclusions of a previously obscure branch of academia truly
captures the popular imaginaƟon. Guerry and Quetelet were
translated into a number of diﬀerent languages and widely
reviewed. 
The Westminster Review—an English magazine
founded by UƟlitarians John Stuart Mill and Jeremy Bentham—
devoted a parƟcularly large amount of space to Guerry’s book,
which it praised for being of “substanƟal interest and
importance.” Charles Darwin read Quetelet’s work, as did Fyodor
Dostoyevsky (twice), while no less a social reformer than Florence
NighƟngale based her staƟsƟcal methods upon his own.13
NighƟngale later gushingly credited Quetelet’s ﬁndings with
“teaching us . . . the laws by which our Moral Progress is to be
attained.”14
In all, Guerry and Quetelet’s work showed that human beings
were beginning to be understood—not as free-willed, self-
determining creatures able to do anything that they wanted, but
as beings whose acƟons were determined by biological and
cultural factors.
In other words, they were beings that could be predicted.
124

The Real Minority Report
In 2002, the Steven Spielberg movie Minority 
Report was
released. Starring Tom Cruise and based on a short story by
science-ﬁcƟon author Philip K. Dick, the ﬁlm tells the story of a
futurisƟc world in which crime has been all but wiped out. This is
the result of a specialized “PreCrime” police department, which
uses predicƟons made by three psychics (“precogs”) to apprehend
potential criminals based on foreknowledge of the crimes they are
about to commit.
The advantage of such a PreCrime unit is clear: in the world
depicted by Minority Report, perpetrators can be arrested and
charged as if they had commiƩed a crime, even without the
crime in quesƟon having to have actually taken place. These
forecasts prove so uncannily accurate that at the start of the
movie the audience is informed that Washington, D.C.—where
the story is set—has remained murder-free for the past six years.
While Minority Report is clearly science ﬁcƟon, like a lot of
good sci-ﬁ the world it takes place in is not a million miles away
from our own. Even before Sean Malinowski’s predicƟve policing
came along, law enforcement oﬃcials forecasted on a daily basis
by deciding between what are considered “good risks” and “bad
risks.” Every Ɵme a judge sets bail, for instance, he is determining
the future likelihood that an individual will return for trial at a
certain date. Search warrants are similar predicƟons that
contraband will be found in a parƟcular locaƟon. Whenever police
oﬃcers intervene in domesƟc violence incidents, their job is to
make forecasts about the likely course of that household over
Ɵme—making an arrest if they feel the future risks are high
enough to warrant their doing so. With each of these cases the
quesƟon of accuracy comes down to both the quality of data
being analyzed and the choice of metrics that the decision-making
process is based on. With human fallibility being what is, however,
this is easier said than done.
125

Parole hearings represent another example of forecasƟng.
Using informaƟon about how a prisoner has behaved while
incarcerated, their own plans for their future if released, and
usually a psychiatrist’s predicƟons about whether or not they are
likely to serve as a danger to the public, parole boards have the
opƟon of freeing prisoners prior to the compleƟon of their
maximum sentence. According to a 2010 study, though, the single
most important factor in determining whether a prisoner is
paroled or not may be nothing more scienƟﬁc than the Ɵme of
day that their hearing happens to take place. The unwiƫng
parƟcipants in this parƟcular study were eight parole judges in
Israel. In a situaƟon where enƟre days are spent reviewing parole
applicaƟons (each of which lasts for an average of six minutes),
the study’s author ploƩed the number of parole requests
approved throughout the day. They discovered that parole
approval rates peaked at 65 percent aŌer each of the judge’s
three meal breaks, and steadily declined in the Ɵme aŌerward—
eventually hiƫng zero immediately prior to the next meal. 15 The
study suggests that when faƟgue and hunger reach a certain
point, judges are likely to revert to their default posiƟon of
denying parole requests. Even though each of the judges would
likely place the importance of facts, reason and objecƟvity over
the rumbling of their stomachs, this illustrates the type of
problem that rears its head when decision-makers happen to be
human.
Richard Berk relies on no such gut insƟnct. Professor of
criminology and staƟsƟcs at the Wharton School of the University
of Pennsylvania, Berk is an expert in the ﬁeld of computaƟonal
criminology, a hybrid of criminology, computer science and
applied mathemaƟcs. For the past decade, he has been working
on an algorithm designed to forecast the likelihood of individuals
commiƫng violent crimes. In a sly nod to the world of “precogs”
envisioned by Philip K. Dick, Berk calls his system “RealCog.”
“We’re not in the world of Minority Report yet,” he says, “but
126

there’s no quesƟon that we’re heading there.” With Berk’s
RealCog algorithm, he can aid law enforcement oﬃcials in making
a number of important decisions. “I can help parole boards decide
who to release,” he says, raƩling oﬀ the areas he can (and does)
regularly advise on. “I can help probaƟon and parole departments
decide how best to supervise individuals; I can help judges
determine the sentences that are appropriate; I can help
departments of social services predict which of the families that
are in their area will have children at a high risk of child abuse.”
In a previous life, Berk was a sociologist by trade. AŌer
receiving his bachelor’s in psychology from Yale, followed by a
PhD from the Johns Hopkins University, he took a job working as
an assistant professor of sociology at Northwestern. He regularly
published arƟcles on subjects like the best way to establish a
rapport with deviant individuals, and how to bridge the gap
between public insƟtuƟons and people living in poor urban areas.
Then he changed tack. “I got interested in staƟsƟcs as a discipline
and preƩy much abandoned sociology,” he says. “I have not been
in a sociology department for decades.” A self-described
pragmaƟst, Berk saw the academic work around him producing
great insights but doing very liƩle to change the way that things
actually worked. When he discovered the ﬁeld of machine
learning, it was a godsend. For the ﬁrst Ɵme he was able to use
large datasets, detailing more than 60,000 crimes, along with
complex algorithms so that staƟsƟcal tools could all but replace
clinical judgment.
127

Are You a Darth Vader or a Luke Skywalker?
Berk’s algorithm is a black box, meaning that its insides are
complex, mysterious and unknowable. What goes in is a dataset
of x’s, containing informaƟon on an individual’s background, as
well as demographic variables like their age and gender. What
comes out is a set of y’s, represenƟng the risk they are calculated
as posing. The challenge, Berk says, is to ﬁnd the best formula to
allow x to predict y. “When a new person comes through the
door, we want to ﬁnd out whether they are high risk or low risk,”
he explains. “You enter that person’s ID, which is sent out to the
various databases. The informaƟon in those databases is brought
back to the local computer, which ﬁgures out what the risk is.
That information is then passed along to the decision-maker.”
Berk makes no apology for the opacity of his system. “It frees
me up,” he explains. “I get to try diﬀerent black boxes and pick
the one that forecasts best.” What he doesn’t care about is causal
models. “I make no claims whatsoever that what I’m doing
explains why it is that individuals fail,” he says. “I’m not trying to
develop a cause-and-eﬀect rendering of whatever it is that’s
going on. I just want to forecast accurately.”
For this reason, Berk will place more emphasis on what he
personally considers to be a strong predictor—such as a prisoner
missing work assignments—than he will on full psychological
evaluaƟons. If it turns out that liver spots can tell him something
about a person’s risk of commiƫng future crime, they become a
part of his model, with no quesƟons asked. “I don’t have to
understand why [liver] spots work,” Berk says. “If the computer
ﬁnds things I’m unaware of, I don’t care what they are, just so
long as they forecast. I’m not trying to explain.”
The majority of Berk’s metrics for predicƟng future
dangerousness are somewhat more obvious than liver spots. Men
are more likely to commit violent crimes than women, while
younger men are more likely to behave violently than older men.
128

A man’s odds of commiƫng a violent crime are at their highest
when he is in his mid-twenƟes. From there, the chance steadily
decreases until the age of 40, after which they plummet to almost
zero. It is for this reason that violent crimes commiƩed early in
life are more predicƟve of future crime than those commiƩed
later on.
“People assume that if someone murdered, then they will
murder in the future,” Berk has noted. “But what really matters is
what that person did as a young individual. If they commiƩed
armed robbery at age 14 that’s a good predictor. If they
commiƩed the same crime at age 30, that doesn’t predict very
much.”
There is, of course, the question of potential errors. Berk claims
that his forecasƟng system can predict with 75 percent accuracy
whether a person released on parole will be involved in a
homicide at some point in the future. That is certainly an
impressive number, but one that sƟll means he will be wrong as
oŌen as one in every four Ɵmes. “No maƩer how good this
forecasƟng is going to be we are always going to make mistakes,”
Berk acknowledges. “Everyone appreciates this, although that
doesn’t make it any less painful.” The margin for error in crime
predicƟon is something of a theme in the ﬁlm Minority Report,
where the “minority report” alluded to in the Ɵtle refers to the
suppressed informaƟon that the three precogs used to predict
crimes occasionally disagree on predicƟons. “Are you saying I’ve
[arrested] innocent people?” asks Tom Cruise’s police chief when
he discovers that this vital piece of informaƟon has been kept
from him. “I’m saying that every so oŌen, those accused of a
PreCrime might, just might, have an alternate future,” comes the
answer. It is easy to see why news of such minority reports might
have been kept from the public. In order for PreCrime to funcƟon,
there can be no suggesƟon of fallibility. Who wants a jusƟce
system that instills doubt, no matter how effective it might be?
To Berk, mistakes come in one of two diﬀerent forms: false
129

posiƟves and false negaƟves. A false posiƟve (which he refers to
as a “Luke Skywalker”) is a person incorrectly idenƟﬁed as a high-
risk individual. A false negaƟve (a “Darth Vader”) is a high-risk
individual who is not recognized as such. This, Berk says, is a
poliƟcal quesƟon rather than a staƟsƟcal one. Is it worse to
falsely accuse a Luke Skywalker, or fail to ﬁnd a Darth Vader? “In
theory, false posiƟves and false negaƟves are treated as equally
serious errors,” he says. “But in pracƟce, this turns out not to be
true. If you work with criminal jusƟce oﬃcials, or talk to
stakeholders or ciƟzens, some mistakes are worse than others. In
general it is worse to fail to idenƟfy a high-risk individual, than it is
to falsely represent someone as if they were a high-risk
individual.” That is therefore the way that Berk’s algorithm is
weighted. In many criminal jusƟce seƫngs—parƟcularly when it
comes to violent crime—decision-makers are likely to accept
weaker evidence if this means avoiding leƫng a potenƟal Darth
Vader slip through the cracks. The price that you pay is that more
Luke Skywalkers will be falsely accused.
Soon, Berk says, the availability of data sources will expand
even further. Rather than just relying on the oﬃcial records
rouƟnely available in criminal-jusƟce ﬁles, Berk and his colleagues
will be able to use custom data sources to predict future
criminality. GPS ankle bracelets, for instance, can examine how
people spend their free Ɵme—and algorithms can then compare
this to a database of criminals to see whether a person happens
to share similar pasƟmes with an Al Capone or a Ted Bundy. “Are
they at home watching TV, or are they spending [their free Ɵme]
on a parƟcular street corner, which historically has been used for
drug transacƟons?” Berk asked in a 2012 talk for Chicago Ideas
Week. “Knowing whether someone is out at 2 A.M. on a Saturday
morning versus 10 A.M. will, we think, help us forecast better.”
Given Ɵme it might even prove possible to begin building up
proﬁles of people before they commit a crime in the ﬁrst place:
one step closer to the authoritarian world of PreCrime imagined
130

by Philip K. Dick. As fraught with legal and ethical dilemmas as this
is, a person who grows up in a high-crime area, has a family
history of drug abuse, or perhaps a sibling or parent already in
prison, could be idenƟﬁed by Berk’s algorithm—despite not yet
having an arrest history.
Another area in which such technology will likely be used is as
part of the school system. “Schools want to know whether the
students they have are going to create problems,” Berk says.
“There’s some iniƟal work being done using school data for school
kids who have no criminal record, to determine which are high
risk for dropping out of school, for being truant, for geƫng in
ﬁghts, for vandalism, and so on.” In 2013, he was working with
children, aged between 8 and 10, to use predicƟve modeling to
establish how likely it is that they may commit a felony later in
life. For now, however, realizing this technology remains the stuﬀ
of the future. As Berk himself acknowledges, “That’s just a gleam
in our eye. We’re just starting to do that.”
131

Delete All the Lawyers
There is a line in Shakespeare’s Henry VI that is likely a favorite of
anyone who has ever had adverse dealings with those in the legal
profession. Adding his two cents to a plan to stage a social revolt
in England, the character of Dick the Butcher pipes up with a
suggesƟon he sees as all but guaranteeing utopia. “The ﬁrst thing
we do,” he says, “let’s kill all the lawyers.”
Approaching 500 years aŌer Shakespeare’s play was ﬁrst
performed, lawyers might not yet be dead, but The Formula may
be rendering an increasing number of them irrelevant. Consider
the area of legal discovery, for instance. Legal discovery refers to
the pretrial phase of a lawsuit, in which each party obtains and
sorts through the material it requires that may lead to admissible
evidence in court. In previous years, discovery was mainly carried
out by junior lawyers, dispatched by law ﬁrms to comb through
large quanƟƟes of possible evidence by hand. This task was both
good for law ﬁrms and bad for clients, who inevitably found
themselves on the receiving end of costly fees. In 1978, ﬁve
television studios became entangled in an anƟtrust lawsuit ﬁled
against broadcasƟng giant CBS. To examine the 6 million
documents deemed to be relevant to the case, the ﬁve television
studios hired a team of lawyers who worked for a period of
several months. When their bill eventually came in, it was for an
amount in excess of $2.2 million—close to $8 million in today’s
money.16
Thanks to advances in arƟﬁcial intelligence, today discovery
can be carried out using data-mining “e-discovery” tools, in
addiƟon to machine-learning processes such as predicƟve coding.
PredicƟve coding allows for human lawyers to manually review a
small percentage of the available documents, and in doing so to
“teach” the computer to disƟnguish between relevant and
irrelevant informaƟon. Algorithms can then process the bulk of
the informaƟon in less than a third of the Ɵme it would take for
132

even the most competent of human teams to carry out the same
task. Such systems have shown a repeated ability to outperform
both junior lawyers and paralegals in terms of precision. AŌer all,
computers don’t get headaches.
OŌen e-discovery requires nothing more complex than basic
indexing or the imposing of simple legal classiﬁcaƟon. It can be
used to go further, though. Algorithms can extract relevant
concepts (such as pulling out all documents pertaining to social
protest in the Middle East) and are becoming increasingly adept
at searching for speciﬁc ideas, regardless of the wording used.
Algorithms can even search for the absence of parƟcular terms,
or look for the kind of underlying paƩerns that would likely have
eluded the aƩenƟon of human lawyers. BeƩer yet, not only can
this work be carried out faster than it would take human lawyers
—but for a fracƟon of the cost as well. One of the world’s leading
e-discovery ﬁrms, Palo Alto’s Blackstone Electronic Discovery, is
regularly able to analyze 1.5 million documents for a cost of less
than $100,000.
Located in the heart of Silicon Valley, Blackstone boasts a client
list that includes Neƞlix, Adobe and the United States
Department of JusƟce. In 2012, the company worked on the
Apple v. Samsung patent case, described by Fortune magazine as
the “(Patent) trial of the century.” 17 Blackstone’s founder is an
MBA named John Kelly, who started the company in 2003 in
response to what he saw as the obvious direcƟon the legal
profession was headed in. “The amount of data we have to deal
with today is absolutely exploding,” Kelly says. “Twenty years ago
a typical case might have involved ten boxes of hard copy. Today
it’s easy to pull 100GB of soŌ copy just from local servers,
desktops and smartphones. That’s the equivalent of between 1
and 2 million pages right there.”
Part of the explanaƟon for the exponenƟal increase in data
comes down to the ease with which informaƟon can now be
stored. Rather than giant, space-consuming ﬁling cabinets of
133

physical documents, modern companies increasingly store their
data in the form of digital ﬁles. In a world of cloud-based storage
soluƟons there is literally no excuse for throwing anything away.
In the Apple v. Samsung case, Samsung found itself verbally
chasƟsed by the presiding judge aŌer admiƫng that all corporate
e-mails carried an expiry date, which caused them to be
automaƟcally deleted every two weeks. As Fast Company pointed
out, “Samsung lost [the case] anyway, but this [infracƟon] . . .
might have sealed the outcome.”18
Where previously the ﬁeld of discovery was about ﬁnding
enough data to build a case, now the e-discovery process focuses
instead on how much informaƟon can be safely ignored. As Kelly
points out, “In the digital world, it’s more about ﬁguring out how
to limit the scope of our invesƟgaƟons.” This is another
applicaƟon in which the algorithms come into their own. Kelly
acknowledges that the eﬃciency with which this limiƟng process
can be carried out doesn’t always endear his company to those
working in more tradiƟonal law ﬁrms. “Some ﬁrms might see a
case and think of it as a $5 million opportunity,” he says. “A
company like Blackstone then comes in and for $100,000 can take
the number of relevant documents down to just 500 e-mails in the
blink of an eye.”
If there’s one area Kelly admits to worrying about it is for the
new generaƟon of junior lawyers, whose livelihood is being
threatened by automaƟon. “One of the quesƟons our work
provokes is what happens to that cadre of folks just out of law
school, who don’t have clients yet, who aren’t rainmakers—what
are they going to do?” Kelly says, with a twinge of genuine pain in
his voice. “In the old days there was tons of stuﬀ around for them.
It might not always have been exciƟng work, but at least it was
available. Now guys like us can do a lot of that work just by using
the right algorithm.”
134

Divorce by Algorithm
Business-management guru Clayton Christensen idenƟﬁes two
types of new technology: “sustaining” and “disrupƟve”
innovations.19 A sustaining technology is something that supports
or enhances the way a business or market already operates. A
disrupƟve technology, on the other hand, fundamentally alters
the way in which a parƟcular sector funcƟons. An example of the
former might be something like the advent of computerized
accounƟng systems, while the arrival of digital cameras (which
famously led to the downfall of Kodak) represents the laƩer. Tools
like e-discovery algorithms are disruptors. But they’re also far
from the excepƟon to the rule when it comes to the many ways in
which the legal profession is being irreversibly altered by the
arrival of The Formula.
In his classic book, The Selﬁsh Gene, evoluƟonary biologist
Richard Dawkins describes the way in which legal cases have
“evolved” to become as ineﬃcient as possible—thereby enabling
lawyers, working with one another in “elaborately coded
cooperaƟon,” to jointly milk their clients’ bank accounts for the
longest amount of Ɵme possible. In its own way this is an
algorithm in itself, albeit one that is diametrically opposed to a
computer-based algorithm designed to produce eﬃcient results in
as few steps as possible.20
With the unbalanced weighƟng of the legal system in favor of
those pracƟcing it,21 it is no surprise that many lawyers criƟcize
the use of disrupƟve technologies in law, worried about the
detrimental eﬀects that it is likely to have on their earning power.
A large number of these aƩack what is viewed as the
“commodiƟzaƟon” of law: unfavorably comparing “rouƟnized”
legal work to the kind of “bespoke” work you would receive from
a human lawyer. (Think about the diﬀerence between oﬀ-the-
rack and hand-tailored clothing in both quality and price.)
But while this criƟcism makes sense if you are a lawyer carrying
135

out what you feel to be bespoke work, it also heavily downplays
the number of legal tasks that a bot can perform as well as, if not
beƩer than, a person. One such area that The Formula is
revoluƟonizing is the process of contract draŌing, thanks to the
rise of automated document assembly systems like the snappily
named LegalZoom. Founded by two corporate-law refugees in
2001, LegalZoom has since served more than 2 million customers
and in the process become a better-known brand name within the
United States than any other law firm. Charging as little as $69 for
wills and $99 for arƟcles of incorporaƟon, LegalZoom uses
algorithms for its high-volume, low-cost business of providing basic
consumer and business documents: doing for the legal profession
what Craigslist did for the newspaper industry’s proﬁtable
classiﬁed ad business.22 Another area is trademark analysis, with
the Finnish start-up OnomaƟcs having created an algorithm
capable of generaƟng instant reports showing how far apart two
different trademarks might be: an area notorious for its high level
of subjective stickiness.
A similar technology is Wevorce, a Mountain View, California,
start-up, located several miles from Google’s corporate
headquarters. If Internet daƟng’s fricƟonless approach to
coupling—discussed last chapter—promises to take the pain out
of love, then Wevorce makes the same promise about divorce:
oﬀering a divorce service mediated by algorithm. Not only does
Wevorce provide a standardized service, based on a system that
idenƟﬁes 18 diﬀerent archetypal paƩerns in divorcing couples—
but it can even advise on what stage in the grieving process a
user’s former partner is likely to be experiencing at any given
moment. 
By 
asking 
couples 
to 
think 
raƟonally 
(even
computaƟonally) about separaƟon, Wevorce claims that it can
“[change] divorce for the beƩer.” 23 “Because the soŌware keeps
the process structured, it’s less scary for divorcing couples and
more eﬃcient for aƩorneys, which leads to overall lower costs,”
says CEO Michelle Crosby.24
136

The Invisible Law Enforcer
Many technology commentators have remarked on the major
shiŌ that has accompanied our changing understanding of the
term “transparency” over the past 40 years. In the early days of
personal compuƟng, a computer that was transparent meant a
computer on which one could “liŌ the lid” and Ɵnker with its inner
workings. Jump forward to the present day and transparency
instead denotes something entirely more opaque: namely that we
can make something work without understanding how it works.
To quote MIT psychoanalyst Sherry Turkle, as we have moved
from a modernist culture of calculaƟon to a postmodern one of
simulaƟon, computers increasingly ask to be taken at “interface
value.”25 This idea of interface value was perfectly encapsulated
during a 2011 interview with Alex Kipman, one of MicrosoŌ’s
engineers on the Kinect moƟon-sensing device. Speaking to a
reporter from the New York Times, Kipman proudly explained
that increasingly we are headed toward “a world where
technology more fundamentally understands you, so you don’t
have to understand it.”26
It is into this frame that “Ambient Law” enters. 27 Ambient Law
refers to the idea that instead of requiring lawyers to call
aƩenƟon to items of legal signiﬁcance around us, laws can be
both embedded within and enforced by our devices and
environment. Ambient Law is a vision of the future in which
autonomic smart environments take an unprecedented number
of decisions for and about us on a constant, real-Ɵme basis.
Autonomic compuƟng’s central metaphor is that of the human
body’s central nervous system. In the same way that the body
regulates temperature, breathing and heart rate, without us
having to be consciously aware of what is happening, so too is the
dream of autonomic compuƟng for algorithms to self-manage,
self-conﬁgure and self-opƟmize—without the need for physical or
mental input on the part of users.
137

One example of Ambient Law might be the “smart oﬃce,”
which conƟnuously monitors its own internal temperature and
compares these levels to those sƟpulated by health and safety
regulaƟons. In the event that a speciﬁed legal limit is exceeded,
an alarm could be programmed to sound. Another usage of
Ambient Law is the car that refuses to be driven by individuals
with an excessive level of alcohol in their bloodstream. A number
of diﬀerent car manufacturers have developed similar technology
in recent years. In a system developed by Japanese carmaker
Nissan, would-be motorists are monitored from the moment they
get behind the wheel. An alcohol odor sensor is used to check
their breath, another sensor tests for alcohol in the sweat of their
palm as they touch the gear sƟck, and a miniature dashboard
camera monitors their face and eye movements—looking for
increased blinking to indicate drowsiness, or a drooping mouth to
suggest yawning. Based on an averaging of all of these
biometrics, the car’s in-built algorithms then decide whether or
not an individual is safe to drive. If the answer is negaƟve, the
car’s transmission locks, a “drunk-driving” voice alert sounds over
the car’s satellite navigaƟon system, and the driver’s seat belt
Ɵghtens around them to provide (in the words of a Nissan
spokesperson) a “mild jolt” designed to snap them out of their
stupor.28
138

The Politics of Public Space
These technologies unnerve some people because of what they
suggest about algorithms’ new role as moral decision-makers.
One only has to look at the hostile reaction afforded Apple when it
started censoring “objecƟonable” content in its App Store, to see
that many computer users view morality and technology as two
unrelated subjects. This is an understandable reacƟon, but one
that also shows a lack of awareness about the historical role of
“technology.” If science aims for a beƩer understanding of the
world, then technology (and, more accurately, technologists) has
always sought to change it. The result is a discipline that is
inextricably Ɵed in with a sense of morality, regardless of how
much certain individuals might try to deny it. In this way,
technology is a lot like law, with both designed as man-made
forces for regulating human behavior.
In a 1980 essay enƟtled “Do ArƟfacts Have PoliƟcs?,” the
sociologist Langdon Winner singled out several of the bridges over
the parkways on Long Island, New York. 29 Many of these bridges,
Winner observed, were extraordinarily low, with as liƩle as nine
feet of clearance at the curb. Although the majority of people
seeing them would be unlikely to aƩach any special meaning to
their design, they were actually an embodiment of the social and
racial prejudice of designer Robert Moses, who was responsible
for building many of the roads, parks, bridges and other public
works in New York between the 1920s and 1970s. With the low
bridges, Moses’s intenƟon was to allow only whites of “upper”
and “comfortable middle” classes access to the public park, since
these were the only demographics able to aﬀord cars. Because
poorer individuals, which included many blacks, relied on taller
public buses they were denied access to the park, since the buses
were unable to handle the low overpasses and were forced to ﬁnd
alternaƟve routes. In other words, Moses built bias (and a skewed
sense of morality) into his designs. As New York town planner Lee
139

Koppleman later recalled, “The old son of a gun . . . made sure
that buses would never be able to use his goddamned
parkways.”30
While the neo-libertarian Google might be a million miles from
Moses’s aƫtudinal bias, it is diﬃcult not to look at the company’s
plans to use data-mining algorithms to personalize maps and see
(perhaps unintenƟonal) strains of the same stuﬀy conservaƟsm.
Over the past decade, Google Maps has become a ubiquitous part
of many people’s lives, vital to how we move from one place to
another on a daily basis. As journalist Tom Chivers wrote in the
Daily Telegraph, “Of all of the search giant’s many tentacles
reaching octopus-like into every area of our existence, Maps,
together with its partner Google Earth and their various offspring,
can probably claim to be the one that has changed our day-to-day
life the most.”31 In 2011, while speaking to the website
TechCrunch, Daniel Graf, the director of Google Maps for mobile,
asked rhetorically, “If you look at a map and if I look at a map [my
emphasis], should it always be the same for you and me? I’m not
sure about that, because I go to diﬀerent places than you do.”32
The result of this insight was that from 2013 onward, Google
Maps began incorporaƟng user informaƟon to direct users
toward those places most likely to be home to like-minded
individuals, or subjects that they have previously expressed an
interest in. “In the past, such a noƟon would have been
unbelievable,” Google crowed in promoƟonal literature. “[A] map
was just a map, and you got the same one for New York City,
whether you were searching for the Empire State Building or the
coﬀee shop down the street. What if, instead, you had a map
that’s unique to you, always adapƟng to the task you want to
perform right this minute?”
But while this might be helpful in some senses, its intrinsic
“ﬁlter bubble” eﬀect may also result in users experiencing less of
the serendipitous discovery than they would by using a tradiƟonal
map. Like the algorithmic matching of a daƟng site, only those
140

people and places determined on your behalf as suitable or
desirable will show up.33 As such, while applying The Formula to
the ﬁeld of cartography might be a logical step for Google, it is
potenƟally troubling. Stopping people of a lower economic status
from seeing the houses and shops catering to those of higher
economic means—or those of one religion from seeing on their
maps the places of worship belonging to those of another—might
iniƟally seem a viable means of reducing conﬂict, but it would do
nothing in the long term to promote tolerance, understanding or
an evening of the playing field.
This situaƟon might only be further exacerbated were certain
algorithms, like the Nara recommender system I described in
Chapter 1, to be implemented incorrectly. By looking not at
where an individual currently is, but where adverƟsers would
eventually like them to be, people reliant on algorithms for
direcƟon could be channeled down certain routes—like actors
playing along to a script.
141

Your Line, My Line
It was the French philosopher and anthropologist Bruno Latour—
picking up from where sociologist Langdon Winner leŌ oﬀ—who
ﬁrst put forward the noƟon of technological “scripts.”34 In the
same way that a ﬁlm script or stage play prescribes the acƟons of
its performers, so too did Latour argue that technology can serve
to modify the behavior of its users by demanding to be dealt with
in a certain way. 35 For instance, the disposability of a plasƟc
coﬀee cup, which begins to disintegrate aŌer only several uses,
will encourage people to throw it away. A set of heavy weights
aƩached to hotel keys similarly makes it more likely that they will
be returned to the recepƟon desk, since the weight will make the
keys cumbersome to carry around.
Less subtle might be the springs attached to a door that dictate
the speed at which people should enter a building—or the
concrete speed bumps that prompt drivers to drive slowly, or else
risk damaging their shock absorbers. Less subtle sƟll would be the
type of aforemenƟoned Ambient Law that ensures that a vehicle
will not start because its driver is inebriated, or the oﬃce building
that not only sounds an alarm but also turns oﬀ workers’
computer screens because a certain heat threshold has been
reached, and they should exit for their own safety.
As with Moses’s low-hanging bridges, such scripts can be
purposely inscribed by designers. By doing this, designers delegate
speciﬁc responsibiliƟes to the objects they create, and these can
be used to inﬂuence user behavior—whether that be encouraging
them to conform to parƟcular social norms or forcing them into
obeying certain laws.36 Because they serve as an “added extra”
on top of the basic funcƟonality of an object or device, scripts
pose a number of ethical quesƟons. What, for example, is the
speciﬁc responsibility of the technology designer who serves as
the inscriber of scripts? If laws or rules are an eﬀort to moralize
other people, does this diﬀer from aƩempts to moralize
142

technology? Can we quanƟfy in any real sense the diﬀerence
between a rule that asks that we not waste water in the shower
and the use of a water-saving showerhead technology that
ensures that we do not?
In their book Nudge: Improving Decisions about Health,
Wealth, and Happiness, authors Richard Thaler and Cass Sunstein
recount the story of a fake houseﬂy placed in each of the urinals
at Schiphol Airport in Amsterdam. By giving urinaƟng men
something to aim at, spillage was reduced by a whole 80
percent.37 While few would likely decry the kind of soŌ
paternalism designed to keep public toilets clean, what about the
harder paternalism of a car that forcibly brakes to stop a person
breaking the speed limit? To what degree can acƟons be
considered moral or law-abiding if the person carrying them out
has no choice but to do so? And when parƟcular acƟons are
inscribed by designers (or, in the case of The Formula, computer
scientists), who has the right to implement and enforce them?
While it would be a brave (and likely misguided) person who
would step up and defend a drunk driver’s right to drive purely on
democraƟc grounds, the quesƟon of the degree to which
behavior should be righƞully limited or regulated is one central to
moral philosophy. In some situaƟons it may appear to be morally
jusƟﬁed. In others, it could just as easily raise associaƟons with
the kind of totalitarian technocracy predicted in George Orwell’s
Nineteen Eighty-Four.
Unsurprisingly, there is a high level of disagreement about
where the line in the sand should be drawn. Roger Brownsword, a
legal scholar who has wriƩen extensively on the topic of
technological regulaƟon and the law, argues that the autonomy
that underpins human rights means that a person should have the
opƟon of either obeying or disobeying a parƟcular rule.38 At the
other end of the spectrum is Professor Sarah Conly, whose boldly
Ɵtled book, Against Autonomy, advocates “[saving] people from
themselves” by banning anything that might prove physically or
143

psychologically detrimental to their well-being. These include (but
are by no means limited to) cigareƩes, trans fats, excessively
sized meals, the ability to rack up large amounts of debt, and the
spending of too much of one’s paycheck without ﬁrst making the
proper saving provisions. “SomeƟmes no amount of public
educaƟon can get someone to realize, in a suﬃciently vivid sense,
the potenƟal dangers of his course of behavior,” Conly writes. “If
public educaƟon were eﬀecƟve, we would have no new smokers,
but we do.” Needless to say, in Conly’s world of hard paternalism
there are more speed bumps than there are plastic coffee cups.39
144

The Prius and the Learning Tree
On the surface, the idea that we should be able to enforce laws by
algorithm makes a lot of sense. Since legal reasoning is logical by
nature, and logical operaƟons can be automated by a computer,
couldn’t codifying the legal process help make it more eﬃcient
than it already is? In this scenario, deciding legal cases would
simply be a maƩer of entering the facts of a parƟcular case,
applying the rules to the facts, and ulƟmately determining the
“correct” answer.
In his work, American scholar Lawrence Lessig idenƟﬁes law
and computer code as two sides of the same coin. Lessig refers to
the laws created by Congress in Washington, D.C., as “East Coast
code” and the laws that govern computer programs as “West
Coast code,” in reference to the locaƟon of Silicon Valley. 40 Once
a law of either type is created, Lessig argues, it becomes virtual in
the sense that from this point forward it has an existence
independent of its original creator. Lessig was hardly the ﬁrst
person to explore this similarity. Three hundred years before
Lessig’s birth, the great mathemaƟcian and coinventor of
calculus, Goƪried Leibniz, speculated that legal liability could be
determined using calculaƟon. Toward the end of the 19th century
another larger group of legal scholars formed the so-called
jurimetrics movement, which argued that the “ideal system of
law should draw its postulates and its legislaƟve jusƟﬁcaƟon from
science.”41
While both Leibniz and the jurimetrics were misguided in their
imagining of the legal system as a series of staƟc natural laws,
their dream was—at its root—an honest one: based on the idea
that science could be used to make the law more objecƟve.
ObjecƟvity in a legal seƫng means fairness and imparƟality. The
person who fails to act objecƟvely has allowed self-interest or
prejudice to cloud their judgment. By aƩempƟng to turn legal
reasoning into a system that would interpret rules the same way
145

every Ɵme, the belief was that a consistency could be found to
rival that which is seen in the hard sciences.
The problem with the jurimetrics’ approach to law was
challenged most eﬀecƟvely by an experiment carried out in 2013
—designed to examine the challenges of turning even the most
straighƞorward of laws into an algorithm. For the study, 52
computer programmers were assembled and split into two
groups. Each group was tasked with creaƟng an algorithm that
would issue speeding Ɵckets to the driver of a car whenever it
broke the speed limit. Both groups were provided with two
datasets: the legal speed limit along a parƟcular route, and the
informaƟon about the speed of a parƟcular vehicle (a Toyota
Prius) traveling that route on a previous occasion, collected by
using an on-board computer. The data showed that the Prius
rarely exceeded the speed limit, and on those occasions that it
did, did so only brieﬂy and by a moderate degree. To make things
more morally ambiguous, these violaƟons occurred at Ɵmes
during the journey when the Prius was set to cruise control. The
journey was ultimately completed safely and without incident.
The ﬁrst group of computer programmers was asked to write
their algorithm so that it conformed to “the leƩer of the law.”
The second was asked to meet “the intent of the law.”
Unsurprisingly, both came to very diﬀerent conclusions. The
“intent of the law” group issued between zero and 1.5 Ɵckets to
the driver for the journey. The “leƩer of the law” group, on the
other hand, issued a far more draconian 498.3 Ɵckets. The
astonishing disparity between the two groups came down to two
principal factors. Where the “intent of the law” group allowed a
small amount of leeway when crossing the speed limit, the “leƩer
of the law” group did not. The “leƩer of the law” group also
treated each separate sample above the speed limit as a new
oﬀense, thereby allowing a conƟnuous stream of Ɵckets to be
issued in a manner not possible using single speed cameras.42
146

Rules and Standards
This raises the quesƟon of “rules” versus “standards.” Broadly
speaking, individual laws can be divided up into these two disƟnct
camps, each of which exists on opposite ends of the legal
spectrum. To illustrate the diﬀerence between a rule and a
standard, consider two potenƟal laws both designed to crack
down on unsafe driving. A rule might state, “No vehicle shall drive
faster than 65 miles per hour.” A standard, on the other hand,
may be arƟculated as “No one shall drive at unsafe speeds.” The
subjecƟvity of standards means that they require human
discreƟon to implement, while rules exist as hard-line binary
decisions, with very little in the way of flexibility.
Computers are constantly geƫng beƩer at dealing with the
kind of contextual problems required to operate in the real world.
For example, an algorithm could be taught to abandon enforced
minimum speed limits on major roads in the event that traﬃc or
weather condiƟons make adhering to them impossible.
Algorithms used for processing speed camera informaƟon have
already shown themselves capable of picking out people who are
new to a certain area. Drivers who are unfamiliar with a place
might ﬁnd themselves let oﬀ if they are marginally in excess of the
speed limit, while locals spoƩed regularly along a parƟcular route
may ﬁnd themselves subject to harsher treatment. However,
both of these excepƟons take the form of preprogrammed rules,
as opposed to evidence that computers are good at dealing with
matters of ambiguity.
This technological drive toward more objecƟve “rules” is one
that has played out over the past several centuries. For instance,
in his book Medicine and the Reign of Technology, Stanley Joel
Reiser observes how the invention of the stethoscope
helped to create the objecƟve physician, who could
move away from involvement with the paƟent’s
147

experiences and sensaƟons, to a more detached
relaƟon, less with the paƟent but more with the
sounds from within the body.43
Similar senƟments are echoed by sociologist Joseph Gusﬁeld in
The Culture of Public Problems: Drinking-Driving and the Symboli
Order, in which he argues that the rise of law enforcement
technologies stems from a desire for objecƟvity, centered around
that which is quanƟﬁable. To illustrate his point, Gusﬁeld looks at
the eﬀect that the introducƟon of the Breathalyzer in the 1950s
had on the previously subjecƟve observaƟons law enforcement
oﬃcials had relied on to determine whether or not a person was
“under the influence.” As Gusfield writes:
[Prior to such tests, there was] a morass of
uncorroborated reports, individual judgments, and
criteria diﬃcult to apply to each case in the same
manner. Both at law and in the research
“laboratory,” the technology of the blood level
sample and the Breathalyzer meant a deﬁniƟve and
easily validated measure of the amount of alcohol in
the blood and, consequently, an accentuated law
enforcement 
and 
a 
higher 
expectancy 
of
convictions.44
In other words, the arrival of the Breathalyzer turned a
person’s ability to drive aŌer several drinks from abstract
“standard” into concrete “rule” in the eyes of the law. This issue
will become even more pressing as the rise of Ambient Law
conƟnues—with technologies not only having the power to
regulate behavior but to dictate it as well, someƟmes by barring
particular courses of action from being taken.
Several years ago, Google announced that it was working on a
148

ﬂeet of self-driving cars, in which algorithms would be used for
everything from planning the most eﬃcient journey routes, to
changing lanes on the motorway by determining the smoothest
path combining trajectory, speed and safe distance from nearby
obstacles. At the Ɵme of wriƟng, these cars have completed
upward of 300,000 miles of test drives in a wide range of
condiƟons, without any reported accidents—leading to the
suggesƟon that a person is safer in a car driven by an algorithm
than they are in one driven by a human.45 Since cars driven by an
algorithm already conform to a series of preprogrammed rules, it
is understandable why speciﬁc laws would become just more to
add to the collecƟon. These could lead to a number of ethical
challenges, however. As a straighƞorward example, what would
happen if the passenger in the car needed to reach a hospital as a
maƩer of urgency—and that this meant breaking the speed limit
on a largely empty stretch of road? It is one thing if the
driver/passenger was Ɵcketed at a later date thanks to the car’s
built-in speed tracker. But what if the self-driving car, bound by
ﬁxed Ambient Laws, refused to break the regulated speed limit
under any conditions?
You might not even have to wait for the arrival of self-driving
cars for such a scenario to become reality. In 2013, BriƟsh
newspapers reported on road-safety measures being drawn up by
EC oﬃcials in Brussels that would see all new cars ﬁƩed with
“Intelligent Speed AdaptaƟon” measures similar to those already
installed in many heavy-goods vehicles and buses. Using satellite
feeds, or cameras designed to automatically detect and read road
signs, vehicles could be forced to conform to speed limits.
AƩempts to exceed them would result in the deployment of the
car’s brakes.46
149

One Man’s Infrastructure Is Another Man’s
Difficulty
The move toward more rule-based approaches to law will only
conƟnue to deepen as more and more laws are wriƩen with
algorithms and soŌware in mind. The fundamental problem,
however, is that while rules and standards do exist as opposite
abstract poles in terms of legal reasoning, they are exactly that:
abstract concepts. The majority of rules carry a degree of
“standard-ness,” while many standards will be “rule-ish” in one
way or another. Driving at a maximum of 60 miles per hour along
a parƟcular stretch of road might be a rule (as opposed to the
standard “don’t drive at unsafe speeds”), but the fact that we
might not receive a speeding Ɵcket for driving at, say, 62 miles
per hour suggests that these are not hard-and-fast in their rule-
ishness. While standards might be open to too much subjecƟvity,
rules may also not be desirable on account of their lack of
flexibility in achieving broader social goals.
As an illustration, consider the idea of a hypothetical sign at the
entrance of a public park that states, “No vehicles are allowed in
this park.” Such a law could be enforced through the use of CCTV
cameras, equipped with algorithms designed to recognize moving
objects that do not conform to the shape of a human. In the hard
paternalisƟc “script” version, cameras might be posiƟoned at
park entrances and linked to gates that are automaƟcally opened
only when the algorithm is saƟsﬁed that all members of a party
are on foot. In the “soŌer” paternalisƟc version, cameras may be
posiƟoned all over the park and could idenƟfy the driver of any
vehicles seen inside park boundaries by matching their face to a
database of ID images and then issuing an automated ﬁne, which
is sent directly to the offender’s home address.
This might sound fair—parƟcularly if the park is one that has
previously experienced problems with people driving their cars
through it. But would such a rule also apply to a bicycle?
150

DeducƟve reasoning may well state that since a bicycle is a
vehicle also, any law that states that no vehicles should be
allowed in a park must also apply to a bicycle. However, is this the
intenƟon of the law, or is the law in this case designed to stop
motor vehicles entering a park since they will create noise and
polluƟon? Does such a rule also mean barring the entrance of an
ambulance that needs to enter the park to save a person’s life?
And if it does not, does this mean that algorithmic laws would
have to be wriƩen in such a way that they would not apply to
certain classes of citizen?
One more obvious challenge in a world that promises
“technology [that] more fundamentally understands you, so you
don’t have to understand it” is that people might break laws they
are not even aware of. In many parts of the world, it is against
the law to be drunk in public. With CCTV cameras increasingly
equipped with the kind of facial and even gait recogniƟon
technology (i.e., analyzing the way you walk) that might allow
algorithms to predict whether or not a person is drunk, could an
individual be Ɵcketed for staggering home on foot aŌer several
drinks at the pub? Much the same is true of cycling at night
without the presence of amber reﬂectors on bike pedals, which is
illegal in the UK—or jaywalking, which is legal in the UK but illegal
in the United States, Europe and Australia.47 In both of these
cases, facial-recogniƟon technology could be used to idenƟfy
individuals and charge them. As a leading CCTV industry
representaƟve previously explained in an arƟcle for trade
magazine CCTV Today, “Recognizing aberrant behavior is for a
scienƟst a maƩer of grouping expected behavior and wriƟng an
algorithm that recognizes any deviation from the ‘normal.’”48
I should hardly have to point out what is wrong with this
statement. “Normal” behavior is not an objecƟve measure, but
rather a social construct—with all of the human bias that
suggests.
Not all uses of algorithmic surveillance are immediately
151

prejudiced, of course. For example, researchers have developed
an algorithm for spoƫng potenƟal suicide jumpers on the London
Underground, by watching for individuals who wait on the
plaƞorm for at least ten minutes and miss several available trains
during that Ɵme.49 If this proves to be the case, the algorithm
triggers an alarm. Another potenƟal applicaƟon helps spot ﬁghts
breaking out on the street by idenƟfying individuals whose legs
and arms are moving back and forth in rapid moƟon, suggesƟng
punches and kicks being thrown. Things become more
quesƟonable, however, when an algorithm might be used to alert
authoriƟes of a gathering crowd in a locaƟon where none is
expected. Similarly, in countries with overtly discriminatory laws,
algorithms could become a means by which to inƟmidate and
marginalize members of the public. In Russia, where the gay
community has been targeted by retrograde anƟgay laws,
algorithmic surveillance may be a means of idenƟfying same-sex
couples exhibiting affectionate behavior.
In such a scenario, algorithms would funcƟon in the opposite
way to that which I described in Chapter 1. Where companies like
Quantcast and Amazon prize “aberrant” behavior on the basis
that it gives them insights into the unique behavior of individual
users, algorithms could instead become a way of ensuring that
people conform to similar behavior—or else. As the American
sociologist Susan Star once phrased it, one person’s infrastructure
is another’s difficulty.50
All of these are (somewhat alarmingly) examples of what would
happen if law enforcement algorithms got it right: upholding the
rules with the kind of steely determinaƟon that would put even
ﬁcƟƟous hard-nosed lawman Judge Dredd to shame. What would
happen if they got things wrong, on the other hand, is potenƟally
even more frightening. . . .
152

The Deadbeat Dad Algorithm
On April 5, 2011, 41-year-old John Gass received a leƩer from the
MassachuseƩs Registry of Motor Vehicles. The leƩer informed
Gass that his driver’s license had been revoked and that he should
stop driving, eﬀecƟve immediately. The only problem was that, as
a conscienƟous driver who had not received so much as a traﬃc
violaƟon in years, Gass had no idea why it had been sent. AŌer
several franƟc phone calls, followed up by a hearing with Registry
oﬃcials, he learned the reason: his image had been automaƟcally
ﬂagged by a facial-recogniƟon algorithm designed to scan
through a database of millions of state driver’s licenses looking for
potenƟal criminal false idenƟƟes. The algorithm had determined
that Gass looked suﬃciently like another MassachuseƩs driver
that foul play was likely involved—and the automated leƩer from
the Registry of Motor Vehicles was the end result. The RMV itself
was unsympatheƟc, claiming that it was the accused individual’s
“burden” to clear his or her name in the event of any mistakes,
and arguing that the pros of protecƟng the public far outweighed
the inconvenience to the wrongly targeted few.51
John Gass is hardly alone in being a vicƟm of algorithms gone
awry. In 2007, a glitch in the California Department of Health
Services’ new automated computer system terminated the
beneﬁts of thousands of low-income seniors and people with
disabiliƟes. Without their premiums paid, Medicare canceled
those citizens’ health care coverage.52 Where the previous system
had noƟﬁed people considered no longer eligible for beneﬁts by
sending them a leƩer through the mail, the replacement CalWIN
soŌware was designed to cut them oﬀ without noƟce, unless they
manually logged in and prevented this from happening. As a
result, a large number of those whose premiums were
disconƟnued did not realize what had happened unƟl they started
receiving expensive medical bills through the mail. Even then,
many lacked the necessary English skills to be able to navigate the
153

online health care system to find out what had gone wrong.53
Similar faults have seen voters expunged from electoral rolls
without noƟce, small businesses labeled as ineligible for
government contracts, and individuals mistakenly idenƟﬁed as
“deadbeat” parents. In a notable example of the laƩer, 56-year-
old mechanic Walter Vollmer was incorrectly targeted by the
Federal Parent Locator Service and issued a child-support bill for
the sum of $206,000. Vollmer’s wife of 32 years became suicidal
in the aŌermath, believing that her husband had been leading a
secret life for much of their marriage.54
Equally alarming is the possibility that an algorithm may falsely
profile an individual as a terrorist: a fate that befalls roughly 1,500
unlucky airline travelers each week.55 Those ﬁngered in the past
as the result of data-matching errors include former Army majors,
a four-year-old boy, and an American Airlines pilot—who was
detained 80 times over the course of a single year.
Many of these problems are the result of the new roles
algorithms play in law enforcement. As slashed budgets lead to
increased staﬀ cuts, automated systems have moved from simple
administraƟve tools to become primary decision-makers. In a
number of cases, the problem is about more than simply ﬁnding
the right algorithm for the job, but about the problemaƟc nature
of believing that any and all tasks can be automated to begin
with. Take the subject of using data-mining to uncover terrorist
plots, for instance. With such aƩacks staƟsƟcally rare and not
conforming to well-deﬁned proﬁles in the way that, for example,
Amazon purchases do, individual travelers end up surrendering
large amounts of personal privacy to data-mining algorithms, with
liƩle but false alarms to show for it. As renowned computer
security expert Bruce Schneier has noted:
Finding terrorism plots . . . is a needle-in-a-haystack
problem, and throwing more hay on the pile doesn’t
make that problem any easier. We’d be far beƩer
154

oﬀ puƫng people in charge of invesƟgaƟng
potenƟal plots and leƫng them direct the
computers, instead of puƫng the computers in
charge and leƫng them decide who should be
investigated.56
While it is clear why such emoƟve subjects would be considered
ripe for The Formula, the central problem once again comes down
to the spectral promise of algorithmic objecƟvity. “We are all so
scared of human bias and inconsistency,” says Danielle Citron,
professor of law at the University of Maryland. “At the same time,
we are overconﬁdent about what it is that computers can do.”
The mistake, Citron suggests, is that we “trust algorithms,
because we think of them as objecƟve, whereas the reality is that
humans craŌ those algorithms and can embed in them all sorts of
biases and perspecƟves.” To put it another way, a computer
algorithm might be unbiased in its execuƟon, but, as noted, this
does not mean that there is not bias encoded within it. What the
speed limit algorithm experiment menƟoned earlier in this
chapter shows more than anything is the degree to which
assumpƟons are built into the code that computer programmers
write, even when those problems being solved might be relaƟvely
mechanical in nature. As technology historian Melvin Kranzberg’s
first law of technology states: “Technology is neither good nor bad
—nor is it neutral.”
Implicit or explicit biases might be the work of one or two
human programmers, or else come down to technological
diﬃculƟes. For example, algorithms used in facial recogniƟon
technology have in the past shown higher idenƟﬁcaƟon rates for
men than for women, and for individuals of non-white origin than
for whites. An algorithm might not target an African-American
male for reasons of overt prejudice, but the fact that it is more
likely to do this than it is to target a white female means that the
end result is no diﬀerent.57 Biases can also come in the abstract
155

patterns hidden within a dataset’s chaos of correlations.
Consider the story of African-American Harvard University PhD
Latanya Sweeney, for instance. Searching on Google one day,
Sweeney was shocked to noƟce that her search results were
accompanied by adverts asking, “Have you ever been arrested?”
These ads did not appear for her white colleagues. Sweeney
began a study that ulƟmately demonstrated that the machine-
learning tools behind Google’s search were being inadvertently
racist, by linking names more commonly given to black people to
ads relaƟng to arrest records.58 A similar revelaƟon is the fact
that Google Play’s recommender system suggests users who
download Grindr, a locaƟon-based social-networking tool for gay
men, also download a sex-oﬀender locaƟon-tracking app. In both
of these cases, are we to assume that the algorithm has made an
error, or that they are revealing inbuilt prejudice on the part of
their makers? Or, as is more likely, are they revealing distasteful
large-scale cultural associaƟons between—in the former case—
black people and criminal behavior and—in the laƩer—
homosexuality and predatory behavior?59 Regardless of the
reason, no maƩer how reprehensible these codiﬁed links might
be, they demonstrate another part of algorithmic culture. A
single human showing explicit bias can only ever aﬀect a ﬁnite
number of people. An algorithm, on the other hand, has the
potential to impact the lives of exponentially more.
156

Transparency Issues
Compounding the problem is the issue of transparency, or lack
thereof. Much like Ambient Law, many of these algorithmic
soluƟons are black-boxed—meaning that people reliant on their
decisions have no way of knowing whether conclusions that have
been reached are correct or the result of distorted or biased
policy, or even of erroneous facts. Because of the step-by-step
process at the heart of algorithms, codifying laws should make it
more straighƞorward to examine audit trails about parƟcular
decisions, certainly when compared to dealing with a human. In
theory, an algorithm can detail the speciﬁc rules that have been
applied in each mini-decision, leading up to the ﬁnal major one. In
fact, the opacity of many automated systems means that they are
shielded from scruƟny. For a variety of reasons, source code is not
always released to the public. As a result, ciƟzens are unable to
see or debate new rules that are made; experiencing only the end
results of decisions, as opposed to having access to the decision-
making process itself. Individuals idenƟﬁed as potenƟal terror
suspects may receive quesƟoning lasƟng many hours, or even be
forced to miss ﬂights, without ever ﬁnding out exactly why the
automated system targeted them. This, in turn, means that there
is a chance that they will be detained each Ɵme they aƩempt to
board an airplane. In a KaŅa-like situaƟon, it is diﬃcult to argue a
certain conclusion if you do not know how it has been reached.
It’s one thing to have a formula theoreƟcally capable of deciding
parƟcular laws, another enƟrely to have its inner workings
transparent in a way that the general populace has access to it.
While these problems could, as noted, be the result of explicit
bias, more oŌen than not they are likely to be created
accidentally by programmers with liƩle in the way of legal
training. As a result, there is a strong possibility that
programmers might change the substance of parƟcular laws
when translaƟng them into machine code. This was evidenced in
157

a situaƟon that occurred between September 2004 and April
2007, when programmers brought in from private companies
embedded more than 900 incorrect rules within Colorado’s public
beneﬁts system. “They got it so wrong that there were hundreds
of thousands of incorrect assessments, because the policy
embedded in the code was wrong,” says University of Maryland
law professor Danielle Citron. “It was all because they interpreted
policy without a policy background.”
The errors made by the coders included denying medical
treatment to paƟents with breast and cervical cancer based upon
income, as well as refusing aid to pregnant women. One 60-year-
old, who had lost her apartment and was now living on the
streets, found herself turned down for extra food stamps because
she was not a “beggar,” which is how coders had chosen to term
“homelessness.” In the end, eligibility workers for the Colorado
Beneﬁts Management System (CBMS) were forced to use
fictitious data to get around the system’s numerous errors.60
The Colorado situaƟon went beyond erroneous computer code.
The laws the programmers had entered were distorted to such a
degree that they had eﬀecƟvely been changed. As Citron points
out, were such amendments to have taken place within the
framework of the legal system, they would have taken many
months to push through the system. “If administraƟve law is
going to pass a new policy regulaƟon they may be required to put
the policy up for comment, to have a noƟce period when they
hear comments from interested advocates and policy-makers,”
she says. “They then incorporate those comments, provide an
explicit explanaƟon for their new policy, and respond to any
comments made. Only then—aŌer this extended noƟce and
comment period, which is highly public in nature—can a new rule
be passed. What happened here was a bypassing of democraƟc
process designed to both allow parƟcipaƟon and garner experƟse
from others.” As it was, programmers were given vast and
unreviewable policy-making power, defying any kind of judicial
158

review.
While not all errors are as egregious as those denying
treatment to cancer paƟents on the basis of income, a number of
coded laws can lack the subtlety of their wriƩen counterparts:
the result of either laziness or ignorance on the part of
programmers. For instance, the United States’ Food Stamp Act
limits unemployed childless adults to three months of food
stamps. However, it also provides six excepƟons to this rule,
which cross-reference other excepƟons, which then refer to yet
more excepƟons. Since these excepƟons are staƟsƟcally rare,
programmers may be tempted to write code that employs just
the simpliﬁed three-month version of the rule, leaving out the
complicated and potentially confusing exceptions.
While at present this is a problem that relates to the
retroﬁƫng of exisƟng laws into code, longer term it presents
other possibiliƟes. It might be, for instance, that agencies will be
increasingly inclined to adopt policies that favor simple quesƟons
and answers over more nuanced approaches—even when this
would be the preferable opƟon, since the former is easier to
translate into algorithm than the laƩer. Going forward, this could
result in complex laws being unnecessarily simpliﬁed so as to allow
them to be beƩer automated. Evidence of this is already being
seen. In MassachuseƩs, IT specialists persuaded agency decision-
makers to avoid adopƟng public beneﬁts policies that would prove
both challenging and expensive to automate. “SomeƟmes it feels
like the IT department is running the policy,” said one person
close to the situation.61
159

Judge, Jury and Executable Code
Judge Richard Posner is the most cited legal scholar of the 20th
century. Years ago, when Posner was a younger, less experienced
judge, he presided over a patent case involving an early
applicaƟon of the kind of targeted recommendaƟon technology
Amazon would later carry out using algorithms. In the nascent
days of satellite television, American television viewers
experienced a seismic leap from having access to around 5 or 6
channels to up to 500. For many people, this was the birth of the
so-called paradox of choice. With so many opƟons available, how
could they possibly be expected to pick the channel they most
wanted to watch? One company came up with an answer. Asking
for a single channel in each home, they promised to send
quesƟonnaires to everyone who received the channel, asking
them to list the type of programs they watched most regularly.
On the basis of this quesƟonnaire, the company then claimed it
would pick the programming they predicted individual customers
would most enjoy, thereby giving them their own personalized
television channels. “In the morning they could work out that you
might like to watch news,” Posner recalls. “In the aŌernoon they
might see that you like to watch soap operas, and in the evening
that you enjoy watching horror movies.” Why worry about the
other 499 channels when there was one that would have
everything you wanted on it?
Posner thought the concept was “ingenious.” The idea stuck
with him, and over the years he considered how it could aﬀect his
own profession. “You could imagine a similar thing being done
with judges,” he reﬂects. “You take their opinions, their public
statements, and whatever other informaƟon you have about
them and use it to create a proﬁle that could funcƟon as a
predicƟve device.” Posner elaborated on the noƟon in a recent
paper enƟtled “The Role of the Judge in the Twenty-First
Century,” in which he postulates on the ways in which judicial
160

pracƟces will likely be altered by the arrival of The Formula. “We
are all familiar,” he writes, “with how Amazon.com creates and
modiﬁes reader proﬁles, and some of us are familiar with data-
mining, which is the same procedure writ large—the computer
identifies patterns and updates them as new data are received.”
I look forward to a Ɵme when computers will create
proﬁles of judges’ philosophies from their opinions
and their public statements, and will update these
proﬁles conƟnuously as the judges issue addiƟonal
opinions. [These] proﬁles will enable lawyers and
judges to predict judicial behavior more accurately,
and will assist judges in maintaining consistency with
their previous decisions—when they want to.62
Judges can be prickly when it comes to the complexity of their
decision-making process. “Our business is prophecy, and if
prophecy were certain, there would not be much credit in
prophesying,” wrote Judge Max Radin in a 1925 essay enƟtled
“The Theory of Judicial Decision: Or How Judges Think.”63 Since he
was wriƟng close to a century ago, Radin cannot enƟrely be
blamed for underesƟmaƟng the power of computaƟon—and yet
it is possible to take him to task for his judicial arrogance. After all,
if a judge is objecƟve in his thought processes and therefore not
subject to hunches, biases or any other “processes . . . alien to
good judges”64 (as federal judge Joseph C. Hutcheson Jr. would
note four years aŌer Radin’s essay was published), it stands to
reason that their decision-making process should be apparent to
anyone with the proper legal grounding.
As it turns out, even suﬃcient legal training may not be
necessary, as was suggested in a 2004 study in which an algorithm
competed with a team of legal experts to see which could predict
the greater number of Supreme Court verdicts. The algorithm
surprised the study’s authors by correctly predicƟng 75 percent of
161

the verdicts (based on only a handful of diﬀerent metrics), as
compared to the team of legal scholars, who guessed just 59
percent, despite having access to far more specialized
information.65 In its own way, the “Supreme Court ForecasƟng
Project” was the legal profession’s equivalent of IBM’s Watson
supercomputer winning $1 million on Jeopardy! in 2011—
marking, as it did, the culminaƟon of a long-held techno dream
ﬁrst proposed by the jurimetrics movement. In 1897, Oliver
Wendell Holmes Jr. wrote enthusiasƟcally of his belief that the
legal system, as with science’s natural laws, should be quantifiably
predictable. “The object of our study . . . is predicƟon,” he
observed, “the predicƟon of the incidence of the public force
through the instrumentality of the courts.”66
But if anything, the Supreme Court ForecasƟng Project was a
bastardizaƟon of the jurimetrics’ utopian vision. Rather than
demonstraƟng that the legal system’s innate objecƟvity made it
predictable, the 2004 study showed that the ability to correctly
predict judges’ verdicts resided in the fact that they were not
objecƟve, but rather ﬁltered in each case through a combinaƟon
of ideological preferences. For instance, one of the metrics the
algorithm used to predict verdicts came down to whether judges
voted Democrat or Republican, a factor that, objecƟvely
speaking, should no more inﬂuence whether a judge ﬁnds a
defendant guilty or innocent than the length of Ɵme since they
last ate.
It is these “aƫtudinal” biases that data-mining algorithms
could be used to discover. “This could be a useful tool for judges,
to help them be more self-aware when it comes to bias,” Judge
Posner says. A judge might, for instance, be soŌ on criminals, but
tough on business fraud. “When they receive their proﬁle they
may become aware that they have certain unconscious biases
that push them in certain directions,” Posner continues.
Harry Surden, associate professor at the University of Colorado
Law School and formerly a soŌware engineer for Cisco Systems,
162

agrees with Posner’s proposal, but says that the revelaƟons about
the biases that aﬀect judicial decision-making could shock the
general public. “As a democraƟc society, we might not like what
we see,” he says.
One area where such algorithms would likely be embraced
would be in the tool kit of lawyers, who could use them to
accurately model a judge’s behavior to shape witnesses and
arguments for maximum impact in a courtroom. “This is probably
going on today in some of the more sophisƟcated legal enƟƟes,”
Surden speculates. He points to the likes of hedge funds that
make large bets whose outcomes are predicated on legal results.
Using sophisƟcated legal modeling algorithms to predict verdicts
in such a scenario could directly result in vast quanƟƟes of money
being made.
Other legal scholars have also suggested that data mining
could be used to reveal a granularity of wrongfulness in criminal
trials, using machine-readable criteria to calculate the exact
extent of a person’s guilt. One imagines that it will not be too long
before neuroscience (currently in the throes of an algorithmic
turn) seeks to establish the exact degree to which a person is free
to comply with the law, arguing over quesƟons like determinism
versus voluntarism. Data-mining tools may addiƟonally have an
applicaƟon when it comes to meƟng out punishments: perhaps
using algorithms to compare datasets of verdicts with datasets
showing the postconvicƟon behaviors of oﬀenders. Could it be, for
example, that there are precise lengths of ideal sentence to avoid
wrongdoers relapsing into crime?
Pushing the idea yet further, what about the concept of
automated judges? If we could construct a recommender system
able to predict with 99 percent accuracy how a certain judge
might rule on a parƟcular case (perhaps even with greater levels
of consistency than the human judge it is based on), would this
help to make trials fairer? “In principle, it would be possible,
although it’s still a way away,” Judge Posner says. “The main thing
163

that would be leŌ out would be how the judge’s views changed
with new information. Any change that may affect the way judges
think would somehow have to be entered into the computer
program, and weighed in order to decide how he would decide a
case.”
At least at present it takes the creaƟvity of a (human) judge to
resolve mulƟple parƟes’ grievances, while also reconciling
diﬀering interpretaƟons of the law. In this way, the judicial
process is less about a mechanical objecƟvity than it is about a
high level of intersubjecƟve agreement: a seemingly minor, but
crucial diﬀerence. Algorithms may have plenty of applicaƟons in
the courtroom—and could even be used eﬀecƟvely to make the
exisƟng system fairer—but they’re unlikely to start handing down
sentences.
For now, at least.
164

CHAPTER 4
The Machine That Made Art
Quite possibly the most famous statement ever made about
Hollywood is the one that screenwriter William Goldman laid out
in his 1983 memoir, Adventures in the Screen Trade. Combing
through his decades in the ﬁlm industry for something
approaching a profundity, all Goldman was able to muster was
the idea that, when it comes to the tricky business of
moviemaking, “Nobody knows anything. Did you know,” he asks,
that Raiders of the Lost Ark was oﬀered to every
single studio in town and they all turned it down? All
except Paramount. Why did Paramount say yes?
Because nobody knows anything. And why did all the
other studios say no? Because nobody knows
anything. And why did Universal, the mighƟest
studio of them all, pass on Star Wars . . . ? Because
nobody . . . knows the least goddam [sic] thing about
what is or isn’t going to work at the box office.1
Goldman is hardly alone in his protestaƟons. In the
autobiography of studio execuƟve Mike Medavoy, the legendary
ﬁlm execuƟve who had a hand in Apocalypse Now, One Flew Over
the Cuckoo’s Nest and The Silence of the Lambs, opines, “The
movie business is probably the most irraƟonal business in the
world . . . [It] is governed by a set of rules that are absolutely
irrational.”2 Hollywood lore is liƩered with stories of sureﬁre
winners that become ﬂops, and sureﬁre ﬂops that become
winners. There are niche ﬁlms that appeal to everyone, and
165

mainstream ﬁlms that appeal to no one. PracƟcally no Hollywood
decision-maker has an unblemished record and, when one
considers the facts, it’s difficult to entirely blame them.
•   •   •
Take the ballad of the two A-list directors, for example. In the
mid-2000s, James Cameron announced that he was busy working
on a mysterious new script, called “Project 880.” Cameron had
previously directed a string of hit movies, including The
Terminator, Terminator 2, Aliens, The Abyss, True Lies and
Titanic, the laƩer of which won 11 Academy Awards and became
the ﬁrst ﬁlm in history to earn more than a billion dollars. For the
past ten years, however, he had been sidelined making
documentaries. Moreover, his proposed new ﬁlm didn’t have any
major stars, he was planning to shoot it in the experimental 3-D
format, and he was asking for $237 million to do so. Perhaps
against more raƟonal judgment, the ﬁlm was nonetheless made,
and when it arrived in cinemas it was instantly labeled in the
words of one reviewer, “The Most Expensive American Film
Ever . . . And Possibly the Most Anti-American One Too.”3
Would you have given Cameron the money to make it? The
correct answer, as many cinema-goers will be aware, is yes.
ReƟtled Avatar, Project 880 proceeded to smash the record set
by Titanic, becoming the ﬁrst ﬁlm in history to earn more than $2
billion at the box office.
At around the same Ɵme that Avatar was gaining momentum,
a second project was doing the rounds in Hollywood. This was
another science-ﬁcƟon ﬁlm, also in 3-D, based upon a classic
children’s story, with a script cowriƩen by a Pulitzer Prize–
winning author, and was to be made by Andrew Stanton, a
director with an unimpeachable record who had previously helped
create the highly successful Pixar ﬁlms WALL-E and Finding Nemo,
along with every entry in the acclaimed Toy Story series.
Stanton’s ﬁlm (let’s call it “Project X”) came with a proposed $250
166

million asking price to bring to the screen—a shade more than
Project 880. Project X received the go-ahead too, only this didn’t
turn out to be the next Avatar, but rather the ﬁrst John Carter, a
ﬁlm that lost almost $200 million for its studio and resulted in the
resignaƟon of the head of Walt Disney Studios (the company who
bankrolled it), despite the fact that he had only taken the job
aŌer the project was already in development. As per Goldman’s
Law, nobody knows anything.
167

Patterns Everywhere
This is, of course, exactly the type of conclusion that challenge-
seeking technologists love to hear about. The idea that there
should be something (the entertainment industry at that) that is
enƟrely unpredictable is catnip to the formulaic mind. As it
happens, blockbuster movies and high-tech start-ups do have a
fair amount in common. Aside from the fact that most are ﬂops,
and investors are therefore reliant on the winners being
suﬃciently big that they more than oﬀset the losers, there are
few other industries where the power of the “elevator pitch”
holds more sway. The elevator pitch is the idea that any popular
concept should be suﬃciently simple that it can be explained over
the course of a single elevator ride. Perhaps not coincidentally,
this is roughly the Ɵme frame of a 30-second commercial: the tool
most commonly used for selling movies to a wide audience.
Like almost every popular idea, the elevator pitch (also known
in Hollywood as the “high concept” phenomenon) has been
aƩributed to a number of industry players, although the closest
thing to an agreed-upon dicƟonary deﬁniƟon sƟll belongs to
Steven Spielberg. “If a person can tell me the idea in twenty-ﬁve
words or less, it’s going to make a preƩy good movie,” the
director of Jurassic Park and E.T. the Extra-Terrestrial has said. “I
like ideas . . . that you can hold in your hand.”4 Would you be in
the least surprised to hear that Spielberg’s father, Arnold
Spielberg, was a pioneering computer scienƟst who designed and
patented the ﬁrst electronic library system that allowed the
searching of what was then considered to be vast amounts of
data? Steven Spielberg might have been a ﬂop as a science
student, but The Formula was in his blood.5
The Formula also runs through the veins of another Hollywood
ﬁgure, whom Time magazine once praised for his “machine-like
drive,” and who originally planned to study engineering at MIT,
before instead taking the acƟng route and going on to star in an
168

almost unblemished run of box oﬃce smashes. Early in Will
Smith’s career, when he was liƩle more than a fad pop star
appearing in his ﬁrst television show, The Fresh Prince of Bel-Air,
the aspiring thespian sat down with his manager and aƩempted
to work out a formula that would transform him from a nobody
into “the biggest movie star in the world.” Smith describes himself
as a “student of the paƩerns of the universe.” At a Ɵme when he
was struggling to be seen by a single casƟng director, Smith spent
his days scruƟnizing industry trade papers for trends that
appeared in what global audiences wanted to see. “We looked at
them and said, ‘Okay, what are the paƩerns?’” he later recalled
in an interview with Time magazine.6 “We realized that ten out of
ten had special eﬀects. Nine out of ten had special eﬀects with
creatures. Eight out of ten had special eﬀects with creatures and
a love story . . .” Two decades later, and with his ﬁlms having
grossed in excess of $6.36 billion worldwide, Smith’s methodology
hasn’t changed a great deal. “Every Monday morning, we sit
down [and say], ‘Okay, what happened this weekend, and what
are the things that resemble things that have happened the last
ten, twenty, thirty weekends?’” he noted.
Of course, as big a box-oﬃce aƩracƟon as Smith undoubtedly
is, when it comes to universal paƩern spoƫng, he is sƟll strictly
small-fry.
169

The Future of Movies
In the United Kingdom, there is a company called Epagogix that
takes the Will Smith approach to movie predicƟon, only several
orders of magnitude greater and without the movie star good
looks. OperaƟng out of a cramped oﬃce in Kennington, where a
handful of data analysts sit hunched over their computers and the
walls are covered with old ﬁlm posters, Epagogix is the unlikely
secret weapon employed by some of the biggest studios in
Hollywood. Named aŌer the Greek word for the path that leads
from experience to knowledge, Epagogix carries a bold claim for
itself: it can—so CEO and cofounder Nick Meaney claims—
accurately forecast how much money a parƟcular ﬁlm is going to
earn at the box office, before the film in question is even made.
Epagogix’s route to Hollywood was something of an unusual
one. Meaney, a fortysomething Brit with a mop of thick black hair
and a face faintly reminiscent of a midcareer Orson Welles, had a
background in risk management. During his career Meaney was
introduced by several mathemaƟcian friends to what are called
neural networks: vast, arƟﬁcial brains used for analyzing the link
between cause and eﬀect in situaƟons where this relaƟonship is
complex, unclear or both. A neural network could be used, for
example, to read and analyze the sound recordings taken from
the wheels of a train as it moves along railway tracks. Given the
right informaƟon it could then predict when a parƟcular stretch
of track is in need of engineering, rather than waiƟng for a major
crash to occur. Meaney saw that neural networks might be a
valuable insurance tool, but the idea was quickly shot down by his
bosses. The problem, he realized, is that insurance doesn’t work
like this: premiums reﬂect the actuarial likelihood that a
parƟcular event is going to occur. The beƩer you become at
stopping a parƟcular problem from happening, the lower the
premiums that can be charged to insure against it.
Movies, however, could beneﬁt from that kind of out-of-the-
170

box thinking. With the average producƟon costs for a big budget
movie running into the tens or even hundreds of millions of
dollars, a suitably large ﬂop can all but wipe out a movie studio.
This is exactly what happened in 1980, when the infamous turkey
Heaven’s Gate (singled out by Guardian criƟc Joe Queenan as the
worst ﬁlm ever made) came close to destroying United ArƟsts.
One studio boss told Meaney that if someone was able to come up
with an algorithm capable of stopping a single money-losing ﬁlm
from being made each year, the overall eﬀect on that studio’s
boƩom line would be “immeasurable.” Intrigued, Meaney set
about bringing such a turkey-shooƟng formula to life. He teamed
up with some data-analyst friends who had been working on an
algorithm designed to predict the television raƟngs of hit shows.
Between them they developed a system that could analyze a
movie script on 30,073,680 unique scoring combinations—ranging
from whether there are clearly deﬁned villains, to the presence or
lack of a sidekick character—before cranking out a projected box
office figure.
To test the system, a major Hollywood studio sent Epagogix the
scripts for nine completed ﬁlms ready for release and asked them
to use the neural network to generate forecasts for how much
each would make. To complicate maƩers, Meaney and his
colleagues weren’t given any informaƟon about which stars the
ﬁlms would feature, who they were directed by, or even what the
markeƟng budget was. In three of the nine cases, the algorithm
missed by a considerable margin, but in the six others the
predicƟons were eerily accurate. Perhaps the most impressive
predicƟon of all concerned a $50 million ﬁlm called Lucky You.
Lucky You starred the famous actress Drew Barrymore, was
directed by CurƟs Hanson, the man who had made the hit movies
8 Mile and L.A. ConﬁdenƟal, and was wriƩen by Eric Roth, who
had previously penned the screenplay for Forrest Gump. It also
concerned a popular subject: the world of high-stakes professional
poker. The studio was projecƟng big numbers for Lucky You.
171

Epagogix’s neural network, on the other hand, predicted a paltry
$7 million. The ﬁlm ended up earning just $6 million. From this
point on, Epagogix began to get regular work.
IllustraƟon of a simpliﬁed neural network. The cat’s cradle of
connecƟons in the middle, labeled the “intermediate
neurons,” is the proprietary “secret sauce” that makes
Epagogix’s system tick.
A fair quesƟon, of course, is why it takes a computer to do this.
As noted, Will Smith does something not enƟrely dissimilar from
his kitchen table at the start of every week. Could a person not go
through the 30,073,680 unique scoring combinaƟons and check
oﬀ how many of the ingredients a parƟcular script adhered to?
The simple answer to this is no. While it would certainly be
possible (albeit Ɵme-consuming) to plot each factor separately,
this would say nothing about how the individual causal variables
interact with one another to aﬀect box oﬃce takings. To
paraphrase a line from George Orwell’s Animal Farm, all numbers
might be equal, but some numbers are more equal than others.
172

Think about it in terms of a successful movie. In March 2012,
The Hunger Games was released at cinemas and rapidly became
a huge hit. But did The Hunger Games become a hit because it
was based on a series of books, which had also been hits, and
therefore had a built-in audience? Did it become a hit because it
starred the actress Jennifer Lawrence, who Rolling Stone
magazine once referred to as “the coolest chick in Hollywood”?7
Or did it become a hit because it was released at a Ɵme of year
when a lot of young people were out of school or college and
were therefore free to go to the cinema? The best that anyone
can say about any one of these quesƟons is: maybe. The Hunger
Games was based on a successful series of books, it did star a hot
young actress popular with the ﬁlm’s key demographic, and it was
released during the spring break holiday in the United States
when large numbers of young people were on holiday. But the
same could be said for plenty of other ﬁlms that don’t go on to
become massive hits. And although The Hunger Games ultimately
took more than $686 million in cinemas, how does anyone know
whether all of the factors menƟoned resulted in posiƟve gains?
Could it be that there was a potenƟal audience out there who
stayed home because they had heard that the film was based on a
book, or because it starred Jennifer Lawrence, or because they
knew that the cinema would be full of rowdy youths fresh out of
school? Might the ﬁlm have earned a further $200 million if only
its distributors had known to hold out unƟl later in the year to
release it?
These are the types of quesƟons Epagogix seeks to quanƟfy. A
studio that employs Meaney and his colleagues will send Epagogix
a shooƟng script, a proposed cast list, and a note about the
speciﬁc Ɵme of year they plan to release their ﬁlm. In return, they
receive a sealed brown envelope containing the neural network’s
report. “We used to send reports that were this thick,” Meaney
says, creaƟng a gap between his thumb and his foreﬁnger to
indicate a dossier the thickness of an average issue of Vogue
173

magazine. Unconvinced that they were being read all the way
through, the company now sends just two or three pages, bound
by a single staple. “You might think that studios would want more
than that, but in fact we spent a lot of Ɵme trimming these
down,” he continues.
The last page of the report is the most important one: the
place where the projected box-oﬃce forecast for the ﬁlm is listed.
There is also a second, mysterious number: usually around 10
percent higher than the ﬁrst ﬁgure, but someƟmes up to twice its
value. This ﬁgure is the predicted gross for the ﬁlm on the
condiƟon that certain recommended tweaks are made to the
script. Since regression tesƟng is used to analyze each script in
forensic detail, Meaney explains that the neural network can be
used to single out individual elements where the potenƟal yield is
not where it should be—or where one part of the ﬁlm is dragging
down others. Depending upon your disposiƟon and trust in
technology, this is the point at which Epagogix takes a turn for
either the miraculous or the unnerving. It is not diﬃcult to
imagine certain screenwriters would welcome the types of notes
that might allow them to create a record-breaking movie, while
others will detest the idea that an algorithm is telling them what
to do.
It’s not just scriptwriters who have the potential to be confused
either. “One of the studio heads that we deal with on a regular
basis is a very smart guy,” Meaney says. “Early on in our dialogue
with him he used to ask quesƟons like, ‘What would happen if the
main character wears a red shirt? What diﬀerence does that
make in your system?’ He wasn’t trying to catch us out; he was
trying to grasp what we do. I was never able to explain to his
saƟsfacƟon that it all depends. Is this a change that can be made
without altering any of the other variables that the neural
network ranks upon? Very seldom is there a movie where any
signiﬁcant alteraƟon doesn’t mean changes elsewhere.” To
modify a phrase coined by chaos theory pioneer Edward Lorenz, a
174

buƩerﬂy that ﬂaps its wings in the ﬁrst minute of a movie may
well cause a hurricane in the middle of the third act.
The studio boss to whom Meaney refers was likely picking a
purposely arbitrary detail by menƟoning the color of a character’s
shirt. AŌer all, who ever formed their opinion about which movie
to go and see on a Saturday night, or which ﬁlm to recommend to
friends, on the basis of whether the protagonist wears a blue shirt
or a red shirt? But Meaney was nonetheless bothered by the
comment: not because the studio boss was wrong, but because
he wanted to reassure himself that saying “it depends” wasn’t a
cop-out. That evening he phoned one of his Epagogix colleagues
back in the UK. “Quick as a ﬂash, they said to me, ‘Of course it
depends.’ Think about Schindler’s List,” Meaney recalls. “At the
end of the ﬁlm you get a glimpse of color and it’s an absolutely
pivotal moment in the narraƟve. In terms of our system it
suddenly put the quesƟon into an historical context. In that
parƟcular ﬁlm it makes the world of diﬀerence, while in another it
might make no difference at all. Everything’s relative.”8
175

Parallel Universes
One way to examine whether there really are universal rules to
be found in art would be to rewind Ɵme and see whether the
same things became popular a second Ɵme around. If the
determinisƟc formula that underpins companies like Epagogix is
correct, then a blockbuster ﬁlm or a best-selling novel would be
successful no maƩer how many Ɵmes you repeated the
experiment over again. Avatar would always have grossed in
excess of $2 billion, while John 
Carter would always have
performed the exact same belly ﬂop. Wolfgang Amadeus Mozart
was always desƟned for greatness, while Antonio Salieri was
always doomed to be an also-ran. This would similarly mean that
there is no such thing as an “unlikely hit,” since universal truths
would state that there are rules that deﬁne something as a
success or a failure. Adhere to them and you have a hit. Fail to do
so and you have a flop.
A few years ago, a group of researchers from Princeton
University came up with an ingenious way of tesƟng this theory
using computer simulaƟons. What sparked MaƩhew Salganik,
Peter Dodds and Duncan WaƩs’s imaginaƟon was the way in
which they saw success manifest itself in the entertainment
industry. Much as some companies start out in a crowded ﬁeld
and go on to monopolize it, so too did they noƟce that particular
books or ﬁlms become disproporƟonate winners. These are what
are known as “superstar” markets. In 1997, for instance, Titanic
earned nearly 50 Ɵmes the average U.S. box-oﬃce take for a ﬁlm
released that year. Was Titanic really 50 Ɵmes beƩer than any
other ﬁlm released that year, the trio wondered, or does success
depend on more than just the intrinsic qualiƟes of a parƟcular
piece of content? “There is tremendous unpredictability of
success,” Salganik says. “You would think that superstar hits that
go on to become so successful would be somehow diﬀerent from
all of the other things they’re compeƟng against. But yet the
176

people whose job it is to ﬁnd them are unable to do so on a
regular basis.”
To put it another way, if Goldman’s Law that nobody knows
anything is right, is this because experts are too stupid to realize a
hit when they have one on their hands, or does nobody know
anything because nothing is for certain?
In order to test their hypothesis, Salganik, Dodds and WaƩs
created what they referred to as an “Experimental Study of
Inequality and Unpredictability in an ArƟﬁcial Cultural Market.”9
This consisted of an online music market, a bit like iTunes, but
featuring unknown songs from unknown bands. The 14,341
parƟcipants recruited online were given the chance to listen to
the songs and rate them between 1 (“I hate it”) and 5 (“I love it”).
They could then download those songs that they liked the most.
The most downloaded tracks were listed in a Top 40–style “leader
board” that was displayed in a prominent position on the website.
What made this website diﬀerent from iTunes was that
Salganik, Dodds and WaƩs had not created one online music
market, but many. When users logged on to the site, they were
randomly redirected to one of nine “parallel universes” that were
idenƟcal in every way with the excepƟon of the leader board.
According to the researchers’ logic, if superstar hits really were
orders of magnitude beƩer than average, one would expect the
same songs to appear in the same spot in each universe.
What Salganik, Dodds and WaƩs discovered instead was
exactly what they had suspected: that there is an accidental
quality to success, in which high-ranking songs take an early lead
for reasons that seem inconsequenƟal, based upon those taste-
makers who sample it ﬁrst. Once this lead is established, it is
exacerbated through social feedback. A bookshop, for instance,
might noƟce that one parƟcular book is proving more popular
than others, and therefore decide to order more copies. At this
stage, the book may be selling 11 copies for every 10 sold by its
next most popular rival—a marginal improvement. But when the
177

new copies arrive they are displayed in favorable places around
the shop (on a table next to the front door, for example) and soon
the book is selling twice as many copies as its closest rival. To sell
even more, the bookshop then decides to try to aƩract new
customers by lowering its own proﬁt margins and selling the book
at a reduced price. At this point the book is selling four Ɵmes as
many copies as its closest rival. Because customers have the
impression that the book is popular (and therefore must be good)
they are more likely to buy it, thereby driving sales up even more.
This is what psychologists call the “mere-exposure” eﬀect. At a
certain juncture a Ɵpping point is reached, where people will buy
copies of the book so as not to be leŌ out of what they see as a
growing phenomenon, in much the same way that we might tune
in to an episode of a television show that has gained a lot of buzz,
just to see what all the fuss is about.
In Salganik, Dodds and Watts’s experiment the songs that were
ranked as the least popular in one universe would never prove the
most popular in another, while the most popular songs in one
universe would never prove the least popular somewhere else.
Beyond this, however, any other result was possible.
178

The Role of Appeal
As you may have detected, there was a problem posed by the
formulaƟon of Salganik, Dodds and WaƩs, one that they
acknowledged when it came Ɵme to write up their ﬁndings. In an
experiment designed to determine the relaƟonship between
popularity and quality, how could any meaningful conclusions be
drawn without ﬁrst deciding upon a quanƟﬁable deﬁniƟon for
quality? “Unfortunately,” as the three researchers gravely noted
in their paper, “no generally agreed upon measure of quality
exists, in large part because quality is largely, if not completely, a
social construcƟon.” To get around this issue (which they referred
to as “these conceptual diﬃculƟes”) Salganik, Dodds and WaƩs
chose to eschew debate about the arƟsƟc “quality” of individual
songs altogether, and instead to focus on the more easily
measurable characteristic of “appeal.”
A song’s appeal was established through the creaƟon of one
more music market, this Ɵme with no scoreboard visible. Lacking
the presence of any obvious social feedback mechanisms,
Salganik, Dodds and WaƩs theorized that whichever song turned
out to be the most popular in this scenario would do so based
wholly on the merits of its objecƟve appeal. What those were
didn’t matter. All that mattered was that they existed.
This market-driven reading of “appeal” over “quality” is (no pun
intended) a popular one. “There are plenty of ﬁlms that, to me,
might be beƩer than Titanic, but in the marketplace it’s Titanic
that earns the most,” says Epagogix’s Nick Meaney. If what
emanates from his company’s neural network happens to coincide
with what Meaney considers a great work of art, that is
wonderful. If it doesn’t, it’s beƩer business sense to recommend
studios fund a ﬁlm that a lot of people will pay money to see and
not feel cheated by, rather than one that a few criƟcs might rave
about but nobody else will watch. Neƞlix followed a similar logic
to Meaney when in 2006 it implemented its (now abandoned)
179

$1,000,000 open compeƟƟon to ask users to create a ﬁltering
algorithm 
that 
markedly 
improved 
upon 
Neƞlix’s 
own
recommender system. Instead of an “improvement” being an
algorithm that directed users toward relaƟvely obscure criƟcal
favorites like Yasujirô Ozu’s 1953 masterpiece Tokyo Story or Jean
Renoir’s 
1953 La Règle du jeu, Neƞlix judged “beƩer
recommendaƟons” as recommendaƟons that most accurately
predicted the score users might give to a ﬁlm or TV show. To put
it another way, this is “best” in the manner of the old adage
staƟng that the best roadside restaurants are those with the
most cars parked outside them.
The advent of mass appeal is a fairly modern concept,
belonging to the rise of factory producƟon lines in the late 19th
and early 20th century. For the ﬁrst Ɵme in history, a true mass
market emerged as widespread literacy coincided with the large-
scale move of individuals to ciƟes. This was the birth of the
packaged formula, requiring the creaƟon of products designed to
sell to the largest number of people possible. Mass producƟon
also meant standardizaƟon, since the producƟon and distribuƟon
process demanded that everything be reduced to its simplest
possible components. This mantra didn’t just apply to consumer
goods, but also to things that didn’t inherently require
simpliﬁcaƟon as part of their producƟon process. As the authors
of newspaper history The Popular Press, 1833–1865 observe, for
example, the early newspaper tycoons “packaged news as a
product [my emphasis] to appeal to a mass audience.”10
In this way, art, literature and entertainment were no
diﬀerent from any other product. In the dream of the utopianists
of the age, mass appeal meant that the world would move away
from the elitist concept of “art” toward its formalized big brother,
“engineering.” Cars, airplanes and even enƟre houses would roll
oﬀ the factory conveyor belt en masse, signaling an end to an
existence in which inequality was commonplace. How could it,
when everyone drove the same Model T Ford and lived in the
180

same homes? Art was eliƟst, irraƟonal and superﬁcial;
engineering was collecƟvist, funcƟonal and hyperraƟonal. BeƩer
to serve the democraƟzed objecƟvity of the masses than the
snobbish subjectivity of the few.11
It was cinema that was seized upon as the ideal medium for
conveying popular, formulaic storytelling, represenƟng the ﬁrst
example of what computaƟonal scholar Lev Manovich refers to as
New Media. “Irregularity, nonuniformity, the accident and other
traces of the human body, which previously, inevitably
accompanied moving image exhibiƟons, were replaced by the
uniformity of machine vision,” Manovich writes. 12 In cinema, its
pioneers imagined a medium that could apply the engineering
formula to the ﬁeld of entertainment. WriƟng excitedly about the
bold new form, Soviet ﬁlmmaker and propagandist Sergei
Eisenstein opined, “What we need is science, not art. The word
creation is useless. It should be replaced by labor. One does not
create a work, one constructs it with ﬁnished parts, like a
machine.”
Eisenstein was far from alone in expressing the idea that art
could be made more scienƟﬁc. A great many arƟsts of the Ɵme
were similarly inspired by the noƟon that stripping art down to its
granular components could provide their work with a social
funcƟon on a previously unimaginable scale, thereby achieving
the task of making art “useful.” A large number turned to
mechanical forms of creaƟvity such as texƟle, industrial and
graphic design, along with typography, photography and
photomontage. In a state of euphoria, the Soviet arƟsts of the
InsƟtute for ArƟsƟc Culture declared that “the last picture has
been painted” and “the ‘sancƟty’ of a work of art as a single
enƟty . . . destroyed.” Art scholar Nikolai Punin went one step
further sƟll, both calling for and helpfully creaƟng a mathemaƟcal
formula he claimed to be capable of explaining the creaƟve
process in full.13
Unsurprisingly, this mode of techno-mania did not go
181

unchallenged. ReacƟng to the disrupƟve arrival of the new
technologies, several tradiƟonally minded scholars turned their
aƩenƟons to criƟquing what they saw as a seismic shiŌ in the
world of culture. For instance, in his essay “The Work of Art in the
Age of Mechanical ReproducƟon,” German philosopher and
literary critic Walter Benjamin observed:
With the advent of the ﬁrst truly revoluƟonary
means of reproducƟon, photography, . . . art sensed
the approaching crisis . . . Art reacted with the
doctrine of l’art pour l’art, that is, with a theology of
art. This gave rise to . . . “pure” art, which not only
denied any social funcƟon of art but also any
categorizing by subject matter.14
Less than a decade later in 1944, two German theorists named
Theodor Adorno and Max Horkheimer elaborated on Benjamin’s
argument in their DialecƟc of Enlightenment, in which they
aƩacked what they biƟngly termed the newly created “culture
industry.” Adorno and Horkheimer’s accusaƟon was simple: that
like every other aspect of life, creaƟvity had been taken over by
industrialists obsessed with measurement and quanƟﬁcaƟon. In
order to work, arƟsts had to conform, kowtowing to a system
that “crushes insubordinaƟon and makes them subserve the
formula.”
Had they been alive today, Adorno and Horkheimer wouldn’t
for a moment have doubted that a company like Epagogix could
successfully predict the box oﬃce of Hollywood movies ahead of
producƟon. Forget about specialist reviewers; ﬁlms planned from
a staƟsƟcal perspecƟve call for nothing more or less than
statistical analysis.
182

Universal Media Machines
In 2012, another major shiŌ was taking place in the culture
industry. Although it was barely remarked upon at the Ɵme, this
was the ﬁrst year in which U.S. viewers watched more ﬁlms
legally delivered via the Internet than they did using physical
formats such as Blu-ray discs and DVDs. Amazon, meanwhile,
announced that, less than two years aŌer it ﬁrst introduced the
Kindle, customers were now buying more e-books than they were
hardcovers and paperbacks combined.
At ﬁrst glance, this doesn’t sound like such a drasƟc alteraƟon.
After all, it’s not as if customers stopped watching films or reading
books altogether, just that they changed something about the
way that they purchased and consumed them. An analog might
be people conƟnuing to shop at Gap, but switching from buying
“boot fit” to “skinny” jeans.
However, while this analogy works on the surface, it fails to
appreciate the extent of the transiƟon that had taken place. It is
not enough to simply say that a Kindle represents a book read on
screen as opposed to on paper. Each is its own enƟty, with its own
techniques and materials. In order to appear on our computer
screens, tablets and smartphones, ﬁlms, music, books and
painƟngs must ﬁrst be rendered in the form of digital code. This
can be carried out regardless of whether a parƟcular work was
originally created using a computer or not. For the ﬁrst Ɵme in
history, any artwork can be described in mathemaƟcal terms
(literally a formula), thereby making it programmable and subject
to manipulation by algorithms.
In the same way that energy can be transferred from
movement into heat, so too can informaƟon now shiŌ easily
between mediums. For instance, an algorithm could be used to
idenƟfy the presence of shadows in a two-dimensional
photograph and then translate these shadows to pixel depth by
measuring their posiƟon on the grayscale—ulƟmately outpuƫng
183

a three-dimensional object using a 3-D printer. 15 Even more
impressively, in recent years Disney’s R&D division has been hard
at work on a research project designed to simulate the feeling of
touching a real, tacƟle object when, in reality, users are only
touching a ﬂat touch screen. This eﬀect is achieved using a hapƟc
feedback algorithm that “tricks” the brain into thinking it is
feeling ridges, bumps or potenƟally even textures, by re-creaƟng
the sensaƟon of fricƟon between a surface and a ﬁngerƟp.16 “If
we can arƟﬁcially stretch skin on a ﬁnger as it slides on the touch
screen, the brain will be fooled into thinking an actual physical
bump is on a touch screen even though the touch surface is
completely smooth,” says Ivan Poupyrev, the director of Disney
Research, who describes the technology as a means by which
interactions with virtual objects can be made more realistic.17
There is also the possibility of combining diﬀerent mediums in
enƟrely new ways, something increasingly common in a world
used to web pages, PowerPoint presentaƟons, and mobile
mulƟmedia messages. It is no coincidence that the advent of the
programmable computer in the 20th century saw the art world
take its ﬁrst tentaƟve steps away from the concept of media
speciﬁcity. As the computer became a mulƟpurpose canvas for
everything from illustraƟon to composiƟon, so too did modern
artists over the past 50 years seek to establish formulas capable of
bringing together previously separate enƟƟes, such as musical
and visual composition.
ScienƟsts and arƟsts alike have long been fascinated by the
neurological condiƟon of synesthesia (Greek for “joined
percepƟon”), in which aﬀected individuals see words as colors,
hear sounds as textures, or register smells as shapes. A similar
response is now reproducible on computer, and this can be seen
through the increasing popularity of “info-aestheƟcs”18 that has
mirrored the rise of data analyƟcs. More than just a method of
compuƟng, info-aestheƟcs takes numbers, text, networks, sounds
and video as its source materials and re-creates them as images
184

to reveal hidden patterns and relationships in the data.
Past data visualizaƟons by arƟsts include the musical
composiƟons of Bach presented as wave formaƟons, the thought
processes of a computer as it plays a game of chess, and the
ﬂuctuaƟons of the stock market. In 2013, Bill Gates and MicrosoŌ
chief technology oﬃcer Nathan Myhrvold ﬁled a patent for a
system capable of taking selected blocks of text and using this
informaƟon to generate sƟll images and even full-moƟon video.
As they point out, such technology could be of use in a classroom
seƫng—especially for students suﬀering from dyslexia, aƩenƟon
deﬁcit disorder, or any one of a number of other condiƟons that
might make it difficult to read long passages of text.19
185

To Thine Own Self Be True/False
Several years ago, as an English graduate student, Stephen
Ramsay became interested in what is known as graph theory.
Graph theory uses the mathemaƟcal relaƟonship between
objects to model their connecƟons—with individual objects
represented by “nodes” and the lines that connect them referred
to as “edges.” Looking around for something in literature that
was mathemaƟcal in structure, Ramsay seƩled upon the plays of
William Shakespeare. “A Shakespearean play will start in one
place, then move to a second place, then go back to the ﬁrst
place, then on to the third and fourth place, then back to the
second, and so on,” he says. Intrigued, Ramsay set about wriƟng
a computer program capable of transforming any Shakespearean
play into a graph. He then used data-mining algorithms to analyze
the graphs to see whether he could predict (based wholly on their
mathemaƟcal structure) what he was looking at was a comedy,
tragedy, history or romance. “And here’s the thing,” he says. “I
could. The computer knew that The Winter’s Tale was a romance,
it knew that Hamlet was a tragedy, it knew that A Midsummer
Night’s Dream was a comedy.” There were just two cases in
which the algorithm, in Ramsay’s words, “screwed up.” Both
Othello and Romeo and Juliet came back classiﬁed as comedies.
“But this was the part that was ﬂat-out amazing,” he says. “For a
number of years now, literary criƟcs have been starƟng to noƟce
that both plays have the structure of comedies. When I saw the
conclusion the computer had reached, I almost fell oﬀ my chair in
amazement.”
The idea that we might pracƟcally use algorithms to ﬁnd the
“truths” obscured within parƟcular arƟsƟc works is not a new
one. In the late 1940s, an Italian Jesuit priest named Roberto
Busa used a computer to “codify” the works of inﬂuenƟal
theologian Thomas Aquinas. “The reader should not simply aƩach
to the words he reads the signiﬁcance they have in his mind,”
186

Busa explained, “but should try to ﬁnd out what signiﬁcance they
had in the author’s mind.”20
Despite this early isolated example, however, the scienƟﬁc
community of the ﬁrst half of the 20th century for the most part
doubted that computers had anything useful to say about
something as unquanƟﬁable as art. An algorithm could never, for
example, determine authorship in the case of two painters with
similar styles—parƟcularly not in situaƟons in which genuine
experts had experienced diﬃculty doing so. In his classic book
Faster Than Thought: A Symposium on Digital CompuƟng
Machines, the late English scienƟst B. V. Bowden oﬀers the view
that:
It seems most improbable that a machine will ever
be able to give an answer to a general quesƟon of
the type: “Is this picture likely to have been painted
by Vermeer, or could van Meegeren have done it?”
It will be recalled that this quesƟon was answered
conﬁdently (though incorrectly) by the art criƟcs
over a period of several years.21
To Bowden, the evidence is clear, straighƞorward and
damning. If Alan Turing suggested that the benchmark of an
intelligent computer would be one capable of replicaƟng the
intelligent acƟons of a man, what hope would a machine have of
resolving a problem that even man was unable to make an
intelligent judgment on? A cooling fan’s chance in hell, surely.
In recent years, however, this view has been challenged. Lior
Shamir is a computer scienƟst who started his career working for
the NaƟonal InsƟtutes of Health, where he used roboƟc
microscopes to analyze the structure of hundreds of thousands of
cells at a Ɵme. AŌer that he moved on to astronomy, where he
created algorithms designed for scouring images of billions of
galaxies. Next he began working on his biggest challenge to date:
187

creaƟng the world’s ﬁrst fully automated, algorithmic art criƟc,
with a rapidly expanding knowledge base and a range of
extremely well-researched opinions about what does and does
not consƟtute art. Analyzing each painƟng it is shown based on
4,024 diﬀerent numerical image content descriptors, Shamir’s
algorithm studies everything that a human art criƟc would
examine (an arƟst’s use of color, or their distribuƟon of geometric
shapes), as well as everything that they probably wouldn’t (such
as a painƟng’s descripƟon in terms of its Zernike polynomials,
Haralick textures and Chebyshev staƟsƟcs). “The algorithm ﬁnds
paƩerns in the numbers that are typical to a certain arƟst,”
Shamir explains.22 Already it has proven adept at spoƫng
forgeries, able to disƟnguish between genuine and fake Jackson
Pollock drip paintings with an astonishing 93 percent accuracy.
Much like Stephen Ramsay’s Shakespearean data-mining
algorithm, Shamir’s automated art criƟc has also made some
fresh insights into the connecƟons that exist between the work of
certain arƟsts. “Once you can represent an arƟst’s work in terms
of numbers, you can also visualize the distance between their
work and that of other arƟsts,” he says. When analyzing the work
of Pollock and Vincent Van Gogh—two arƟsts who worked within
completely diﬀerent art movements—Shamir discovered that 19
of the algorithm’s 20 most informaƟve descriptors showed
signiﬁcant similariƟes, including a shared preference for low-level
textures and shapes, along with a similar deployment of lines and
edges.23 Again, this might appear to be a meaningless insight
were it not for the fact that several inﬂuenƟal art criƟcs have
recently begun to theorize similar ideas.24
188

Bring on the Reading Machines
This newfound ability to subject media to algorithmic
manipulaƟon has led a number of scholars to call for a so-called
algorithmic criƟcism. It is no secret that the ﬁeld of literary
studies is in trouble. AŌer decades of downward trends in terms
of enrollments, the subject has become a less and less signiﬁcant
part of higher educaƟon. So how could this trend be reversed?
According to some, the answer is a straighƞorward one: by
turning it into the “digital humaniƟes,” of course. In a 2008
editorial for the Boston Globe enƟtled “Measure for Measure,”
literary criƟc Jonathan GoƩschall dismissed the current state of
his ﬁeld as “moribund, aimless, and increasingly irrelevant to the
concerns . . . of the ‘outside world.’” Forget about vague terms
like the “beauty myth” or Roland Barthes’s concept of the death
of the author, GoƩschall says. What is needed instead is a
productivist approach to media built around correlations, pattern-
seeking and objectivity.
As such, GoƩschall lays out his Roberto Busa–like beliefs that
genuine, veriﬁable truths both exist in literature and are
desirable. In keeping with the discoverable laws of the natural
sciences, in GoƩschall’s mind there are clear right and wrong
answers to a quesƟon such as, “Can I interpret [this painƟng/this
book/this film] in such-and-such a way?”
While these comments are likely to shock many of those
working within the humaniƟes, GoƩschall is not altogether wrong
in suggesƟng that there are elements of computer science that
can be usefully integrated into arts criƟcism. In the world of The
Formula, what it is that is possible to know changes dramaƟcally.
For example, algorithms can be used to determine “vocabulary
richness” in literature by measuring the number of diﬀerent
words that appear in a 50,000-word block of text. This can bring
about a number of surprises. Few criƟcs would ever have
suspected that a “popular” author like Sinclair Lewis—someƟmes
189

derided for his supposed lack of style—regularly demonstrates
twice the vocabulary of Nobel laureate William Faulkner, whose
work is considered notoriously difficult.
One of the highest-proﬁle uses of algorithms to analyze text
took place in 2013 when a new crime ﬁcƟon novel, The Cuckoo’s
Calling, appeared on bookshelves around the world, wriƩen by a
ﬁrst-Ɵme author called Robert Galbraith. While the book
aƩracted liƩle aƩenƟon early on, selling just 1,500 printed
copies, it became the center of controversy aŌer a BriƟsh
newspaper broke the story that the author may be none other
t ha n Harry PoƩer author J. K. Rowling, wriƟng under a
pseudonym. To prove this one way or the other, computer
scienƟsts were brought in to verify authorship. By using data-
mining techniques to analyze the text on four diﬀerent variables
(average word length, usage of common words, recurrent word
pairings, and distribuƟon of “character 4-grams”), algorithms
concluded that Rowling was most likely the author of the novel,
something she later admitted to.25
As Stephen Ramsay observes, “The rigid calculus of
computaƟon, which knows nothing about the nature of what it’s
examining, can shock us out of our preconceived noƟons on a
parƟcular subject. When we read, we do so with all kinds of
biases. Algorithms have none of those. Because of that they can
take us oﬀ our rails and make us say, ‘Aha! I’d never noƟced that
before.’”
190

Data-tainment
A quick scan of the best-seller list will be enough to convince us
that, for beƩer or worse, book publishers are not the same as
literary professors. This doesn’t mean that they are exempt from
the allure of using algorithms for analysis, however. Publishers, of
course, are less interested in understanding a parƟcular text than
they are in understanding their customers. In previous years, the
moment that a customer leŌ a bookshop and took a book home
with them, there was no quanƟﬁable way a publisher would know
whether they read it straight through or put it on a reading pile
and promptly forgot about it. Much the same was true of VHS
tapes and DVDs. It didn’t maƩer how many Ɵmes an owner of
Star Wars rewound their copy of the tape to watch a
Stormtrooper bump his head, or paused Basic InsƟnct during the
infamous leg-crossing scene: no studio execuƟve was ever going
to know about it. All of that is now changing, however, due to the
amount of data that is able to be gathered and fed back to
content publishers. For example, Amazon is able to tell how
quickly its customers read e-books, whether they scruƟnize every
word of an introducƟon or skip over it altogether, and even which
secƟons they choose to highlight. They know that science ﬁcƟon,
romance and crime novels tend to be read faster than literary
ﬁcƟon, while nonﬁcƟon books are less likely to be ﬁnished than
fiction ones.
These insights can then be used to make creaƟve decisions. In
February 2013, Neƞlix premiered House of Cards, its poliƟcal
drama series starring Kevin Spacey. On the surface, the most
notable aspect of House of Cards appeared to be that Neƞlix—an
on-demand 
streaming-media 
company—was 
changing 
its
business model from distribuƟon to producƟon, in an eﬀort to
compete with premium television brands like ShowƟme and HBO.
GeneraƟng original video content for Internet users is sƟll
something of a novel concept, parƟcularly when it is done on a
191

high budget and, at $100 million, House of Cards was absolutely
that. What surprised many people, however, was how bold Netflix
was in its decisions. ExecuƟves at the Los Gatos–based company
commissioned a full two seasons, comprising 26 episodes in total,
without ever viewing a single scene. Why? The reason was that
Neƞlix had used its algorithms to comb through the data
gathered from its 25 million users to discover the trends and
correlaƟons in what people watched. What it discovered was that
a large number of subscribers enjoyed the BBC’s House of Cards
series, evidenced by the fact that they watched episodes mulƟple
Ɵmes and in rapid succession. Those same users tended to also
like ﬁlms that starred Kevin Spacey, as well as those that were
directed by The Social Network’s David Fincher. Neƞlix rightly
ﬁgured that a series with all three would therefore have a high
probability of succeeding.26
The gamble appeared to pay oﬀ. Under a review Ɵtled “House
of Cards Is All Aces,” USA Today praised the show as “money well-
spent” and among the “most gorgeous [pieces] of television”
people were likely to see all year. 27 President Obama admiƩed to
being a fan. Neƞlix followed up its House of Cards success with
three more well-received series: Hemlock Grove, Arrested
Development and Orange Is the New Black. At the 2013 Emmy
Awards, the company notched up a total of 14 nominaƟons for its
efforts.28 “It took HBO 25 years to get its ﬁrst Emmy nominaƟon,”
noted American TV criƟc and columnist David Bianculli in an
article for the New York Times. “It took Netflix six months.”29
Neƞlix’s success has seen it followed by online retailer Amazon,
which also has access to a vast bank of customer informaƟon,
revealing the kind of detailed “likes” and “dislikes” data that
tradiƟonal studio bosses could only dream of. “It’s a completely
new way of making movies,” Amazon founder Jeﬀ Bezos told
Wired magazine. “Some would say our approach is unworkable—
we disagree.”30
192

In Soviet Russia, Films Watch You
In a previous life, Alexis Kirke worked as a quanƟtaƟve analyst on
Wall Street, one of the so-called rocket scienƟsts whose job
concerns a heady blend of mathemaƟcs, high ﬁnance and
computer skills. Having completed a PhD in computer science,
Kirke should have been on top of the world. “Quants” are highly in
demand and can earn upward of $250,000 per year, but Kirke
nonetheless found himself feeling surprisingly disenfranchised.
“AŌer about a year, I decided that this wasn’t what I wanted to
do,” he says. What he wanted instead was to pursue an arƟsƟc
career. Kirke leŌ the United States, moved back home to
Plymouth, England, and enrolled in a music degree course. Today,
he is a research fellow at Plymouth University’s Interdisciplinary
Center for Computer Music Research.
In 2013, Kirke achieved his greatest success to date when he
created Many Worlds, a ﬁlm that changed the direcƟon of its
narraƟve based upon the response of audience members. Many
Worlds premiered at the Peninsula Arts Contemporary Music
FesƟval in 2013, and its interacƟvity marked a major break from
tradiƟonal cinema by transforming audiences from passive
consumers into acƟve parƟcipants. At screenings, audience
members were ﬁƩed with special sensors capable of monitoring
their brain waves, heart rate, perspiraƟon levels and muscle
tension. These indicators of physical arousal were then fed into a
computer, where they were averaged and analyzed in real Ɵme,
with the reacƟons used to trigger diﬀerent scenes. A calm
audience could conceivably be jolted to aƩenƟon with a more
dramaƟc sequence, while an already tense or nervous audience
could be shown a calmer one. This branching narraƟve ulƟmately
culminated in one of four different endings.31
In a sense, companies like Epagogix, which I menƟoned at the
start of the chapter, oﬀer a new twist on an old idea: that there is
such a thing as a work of art that will appeal to everyone. Anyone
193

who has ever read two opposite reviews of the same ﬁlm—one
raving about it and the other panning it—will realize that this is
not necessarily true. Our own preferences are based on syntheƟc
concepts based around inherited ideas, as well as our own
previous experiences. My idea of how Macbeth should be
performed on the stage is based on those performances I have
aƩended in the past, or what I have read about the play. The
same is true of the ﬁlms I like, the music I enjoy, and the books I
read.
“A fixed film appeals to the lowest common denominator,” says
Kirke. “What it does is to plot an average path through the
audience’s emoƟonal experience, and this has to translate across
all audiences in all countries. Too oŌen this can end in
compromise.” Kirke isn’t wrong. For every Iron Man—a
Hollywood blockbuster that appeals to vast numbers without
sacriﬁcing quality—there are dozens of other ﬁlms from which
every ounce of originality has been airbrushed in an eﬀort to
appease the widest possible audience. Many Worlds suggests an
alternaƟve: that in the digital age, raƟonalizaƟon no longer has
to be the same as standardizaƟon. Formulas can exist, but these
don’t have to ensure that everything looks the same.32
A valid quesƟon, of course, concerns the cost of implemenƟng
this on a wider level. Alexis Kirke created Many Worlds on what
he describes as a “nano-budget” of less than $4,000—along with
some lights, tripods and an HD camera borrowed from Plymouth
University’s media department for a few days. How would this
work when scaled up to Hollywood levels? AŌer all, at a Ɵme
when blockbuster movies can cost upward of $200 million, can
studios really aﬀord the extra expenditure of shooƟng four
diﬀerent endings in the way that Kirke did? He certainly believes
they can. As I described earlier in this chapter, the entertainment
industry currently operates on a highly ineﬃcient (some would say
unscienƟﬁc) business model reliant on staƟsƟcally rare
“superstar” hits to oﬀset the cost of more likely losses. The movie
194

studio that makes ten ﬁlms and has two of these become hits will
be reasonably content. But what if that same studio ramped up
its spending by shooƟng alternate scenes and commissioning
several possible sound tracks at an addiƟonal cost of 50 percent
per ﬁlm, although this in turn meant that the ﬁlm was more likely
to become a hit? If branching ﬁlms could be all things to all
people, studios might only have to make ﬁve ﬁlms to create two
sizeable hits.
Following the debut of Many Worlds, Kirke was approached by
several major media companies interested in bringing him on
board to work as a consultant. The BBC twice invited him to its
headquarters in Manchester to screen the ﬁlm and discuss his
thoughts on the future of interacƟve media. Manufacturers were
parƟcularly interested in how this technology could usefully be
integrated into the next generaƟon of television sets. “This is
something that’s already starƟng to happen,” Kirke says. In 2013,
MicrosoŌ was awarded a patent for a camera capable of
monitoring the behavior of viewers, including movement, eye
tracking and heart rate. This behavior can then be compiled into
user-speciﬁc reports and sent, via the cloud, to a remote device
able to determine whether certain goals have been met.33
AdverƟsers, for instance, will have the opƟon of rewarding
viewers who sit through commercial breaks with digital credits
(iTunes vouchers, perhaps) or physical prizes. Because MicrosoŌ’s
camera sensor has the ability to recognize gestures, adverƟsers
could create dances or acƟons for viewers to reproduce at home.
The more enthusiasƟc the reproducƟon, the more iTunes
vouchers the viewer could win.
Another company, named AﬀecƟva, is beginning to market
facial expression analysis soŌware to the consumer product
manufacturers, retailers, marketers and movie studios. Its mission
is to mine the emoƟonal response of consumers to help improve
the designs and markeƟng campaigns of products.34 Film and
television audiences will similarly increasingly be watched by
195

nonspeech microphones and eye line sensors, along with social
network scanners built into mobile devices, which adjust
whatever they are watching according to reacƟons. If it is
determined that a person’s eyes are straying from the screen too
oŌen, or that they are showing more interest in Facebook than
the entertainment placed in front of them, ﬁlms will have the
opƟon of adjusƟng ediƟng, sound track or even narraƟve to
ensure that maximum engagement level is maintained at all
times.
196

A Moving Target
TradiƟonally, the moment that a painƟng was ﬁnished, a
photograph was printed or a book was published it was ﬁxed in
place. We might even argue that such a quality forms part of our
appreciaƟon. With its ﬁxed number of pages bound by a single
spine, the physical organizaƟon of a book invites the reader to
progress through it in a linear, predetermined manner—moving
from leŌ to right across the page, then from page to page, and
ulƟmately from chapter to chapter, and cover to cover. 35 As a
result, a book appeals to our desire for completion, wholeness and
closure.
No such permanence or ﬁxedness exists in the world of The
Formula, in which electronic books, ﬁlms and music albums can be
skipped through at will.36 This, in turn, represents a ﬂaƩening of
narraƟve, or a division of it into its most granular elements. As
computer scienƟst Steven DeRose argues in a 1995 paper enƟtled
“Structured InformaƟon: NavigaƟon, Access and Control,” this
analysis of structured informaƟon does not get us close to certain
universal truths, “in the sense that a Sherlock Holmes should peer
at it and discern hidden truth . . . but rather in the sense that the
informaƟon is divided into component parts, which in turn have
components, and so on.”37
This narraƟve unwinding was demonstrated to great eﬀect
several years ago when the American arƟst Jason Salavon
digiƟzed the hit movie Titanic and broke it up into its separate
frames. Each of these frames was then algorithmically averaged
to a single color using a computer, before the frames were
recollected as a uniﬁed image, mirroring the narraƟve sequence
of the ﬁlm. Reading the artwork from leŌ to right and top to
bottom, the movie’s rhythm was laid out in pure color.38
Both Alexis Kirke’s Many Worlds and Salavon’s reimagining of
Titanic represent two sides of the same coin. In a post-9/11 age in
which our own sense of impermanence is heightened, past and
197

present are ﬂaƩened in the manner of a Facebook Ɵmeline, and
the future is an uncertain prospect, what relevance do tradiƟonal
beginnings, middles and ends have? This is further seen by the
number of artworks that, imbued with the power of code and
real-Ɵme data streams, exist in a state of constant ﬂux. In the
same way that the Internet will never be completed—any more
than technology itself can be completed—these algorithmic
artworks are able to adapt and mutate as new data inputs are
absorbed into the whole.
An example of this was created in Cambridge, MassachuseƩs,
where two members of Google’s Big Picture data visualizaƟon
group, Fernanda Viégas and MarƟn WaƩenberg, coded an online
wind map of the United States, which presents data from the
NaƟonal Digital Forecast Database in the hypnoƟc form of a
swirling, constantly changing animation.39 “On calm days it can be
a soothing meditaƟon on the environment,” WaƩenberg says.
“During hurricanes it can become ominous and frightening.”40 In
a previous age of ﬁxedness, a work of art became Ɵmeless by
containing themes universal enough to span generaƟons. Today
“Ɵmeless” means changing for each successive audience: a
realization of the artist’s dilemma that work is never finished, only
abandoned.
In his latest book, Present Shock, cyberpunk media theorist
Douglas Rushkoﬀ seizes upon a similar idea to discuss the ways in
which today’s popular culture reﬂects The Formula. Much as the
arƟsts of the early 20th century adopted the techniques and
aestheƟcs of heavy-duty industrial machinery as their model of
choice for the direcƟon in which to take their art, so too does
today’s entertainment industry reﬂect the ﬂux-like possibiliƟes of
code. Unlike the predictable narraƟve character arcs of classic
ﬁlms like The Godfather, today’s most lauded creaƟons are
ongoing 
shows 
such 
as Game 
of 
Thrones that avoid
straighƞorward, three-act structures and simply conƟnue
indefinitely.
198

Looking at shows like the NBC series Community and Seth
MacFarlane’s Family Guy, Rushkoﬀ further demonstrates the
technology-induced collapse of narraƟve structure at work.
Community features a group of misﬁts at Greendale Community
College, who constantly refer to the fact that they are characters
within a sitcom. What story arcs do exist in the show are executed
with the full knowledge that the viewing audience is well versed in
the clichés that make up most tradiƟonal sitcoms. Family Guy
similarly breaks away from tradiƟonal narraƟve storytelling in
favor of self-contained “cutaway” gags, which prove equally
amusing regardless of the order in which they are played, making
it the perfect comedy for the iPod Shuﬄe generaƟon. Like its
obvious forerunner, The Simpsons, rarely does a plot point in
Family Guy have any lasƟng impact—thereby allowing all manner
of nonsensical occurrences to take place before the “reset”
button is hit at the end of each episode.
A more poignant illustraƟon of this conceit can be found in the
more serious drama series on television. Shows like The Wire,
Mad Men, The Sopranos and Dexter all follow ostensibly diﬀerent
central characters (ranging from BalƟmore police and Madison
Avenue admen to New Jersey mobsters and Miami serial killers)
whose chief similarity is their inability to change their nature, or
the world they inhabit. As Rushkoff writes, these series
don’t work their magic through a linear plot, but
instead create contrasts through associaƟon, by
nesƟng screens within screens, and by giving viewers
the tools to make connecƟons between various
forms of media . . . The beginning, the middle, and
the end have almost no meaning. The gist is
experienced in each moment as new connecƟons
are made and false stories are exposed or reframed.
In short, these sorts of shows teach paƩern
recognition, and they do it in real time.41
199

Even today’s most popular ﬁlms no longer exist as unitary
enƟƟes, but as nodes in larger franchises—with sequels regularly
announced even before the ﬁrst ﬁlm is shown. It’s no accident
that in this seƫng many of the most popular blockbusters are
based on comic-book properƟes: a medium in which, unlike a
novel, plot points are ongoing with liƩle expectaƟon of an
ultimate resolution.
In this vein, Alexis Kirke’s Many Worlds does not exist as an
experimental outlier, but as another step in the unwinding of
tradiƟonal narraƟve and a sign of things to come. While stories
aren’t going anywhere, Kirke says, in the future audiences are
likely to be less concerned with narraƟve arcs than they will with
emotional ones.
200

Digital Gatekeepers
A lack of ﬁxedness in art and the humaniƟes can have other,
potenƟally sinister, implicaƟons. Because the “master” copy of a
particular book that we are reading—whether this be on Kindle or
Google Books—is stored online and accessed via “the cloud,”
publishers and authors now possess the ability to make changes to
works even aŌer they have been purchased and taken home. A
poignant illustraƟon of this fact occurred in 2009 when Amazon
realized that copies of George Orwell’s classic novel Nineteen
Eighty-Four being sold through its Kindle plaƞorm were under
copyright, rather than exisƟng in the public domain as had been
assumed. In a panic, Amazon made the decision to delete the
book altogether, resulƟng in it vanishing from the libraries of all
those who had purchased it. The irony, of course, is that Nineteen
Eighty-Four concerns a dystopian future in which the ruling
superpower manipulates its populace by rewriƟng the history
books on a daily basis. More than 60 years aŌer the novel was
ﬁrst published, such amendments to the grand narraƟve are now
technically possible.
WriƟng in Wired magazine in July 2013, Harvard computer-
science professor Jonathan ZiƩrain described this as “a worrisome
trend” and called for digital books and other texts to be placed
under the control of readers and libraries—presumed to have a
vested interest in the sancƟty of text—rather than with
distributors and digital gatekeepers. Most insidious of all, ZiƩrain
noted, was the fact that changes can be made with no evidence
that things were ever any other way. “If we’re going to alter or
destroy the past,” he wrote, “we should [at least] have to see,
hear and smell the paper burning.”42
201

A Standardized Taste
In the early 1980s, a computer science and electronic engineering
graduate from UC Berkeley set out to create a musical
synthesizer. What Dave Smith wanted was to establish a
standardized protocol for communicaƟon between the diﬀerent
electronic musical instruments made by diﬀerent manufacturers
around the world. What he came up with was christened the
“Musical Instrument Digital Interface” and—beƩer known by the
name MIDI—became the entrenched unitary measurement for
music. As a musical medium, MIDI is far from perfect. Although it
can be used to mimic a wide paleƩe of sounds using a single
keyboard, it retains the keyboard’s staccato, mosaic qualiƟes,
which means that it cannot emulate the type of curvaceous
sounds produceable by, say, a talented singer or saxophonist. As
virtual-reality innovator (and talented musician) Jaron Lanier
observes:
Before MIDI, a musical note was a boƩomless idea
that transcended absolute deﬁniƟon . . . AŌer MIDI,
a musical note [is] no longer just an idea, but a rigid,
mandatory structure you couldn’t avoid in the
aspects of life that had gone digital.43
This sort of technological “lock-in” is an unavoidable part of
measurement. The moment we create a unitary standard, we
also create limitaƟons. More than two centuries before Dave
Smith created MIDI, an 18th-century Scoƫsh philosopher named
David Hume wrote an essay enƟtled “(Of the) Standard of Taste.”
In it, Hume argued that the key component to art (the thing that
would come aŌer the equals sign were it formulated as an
equaƟon) was the presence of what he termed “agreeableness.”
Hume observed, “it is natural for us to seek a Standard of Taste; a
rule, by which the various senƟments of men may be
202

reconciled.”44
Unlike many of the ﬁgures discussed at the start of this
chapter, Hume believed that there were not objecƟve measures
of aestheƟc value, but that these were rather subjecƟve
judgments. As he phrased it, “to seek the real beauty, or the real
deformity, is as fruitless an enquiry, as to seek the real sweet or
real biƩer.” At the same Ɵme, Hume acknowledged that, within
subjecƟvity, aspects do indeed exist that are either “calculated to
please” or “displease”—thus bringing about his “standard of
taste.”
Hume was ahead of his Ɵme in various ways. In recent years, a
number of organizaƟons around the world have been
invesƟgaƟng what is referred to as “EmoƟonal OpƟmizaƟon.”
EmoƟonal OpƟmizaƟon relates to the discovery that certain parts
of the brain correspond to diﬀerent emoƟons. By asking test
subjects to wear electroencephalography (EEG) brain caps,
neuroscienƟsts can measure the electrical acƟvity that results
from ionic current ﬂows within the neurons of the brain. These
readings can then be used to uncover the posiƟve and negaƟve
reacƟons experienced by a person as they listen to a piece of
music or watch a scene from a ﬁlm. Through the addiƟon of
machine-learning tools, the possibility of discovering which low-
level features in art prompt parƟcular emoƟonal responses
becomes a reality.
Looking to the future, the potenƟal of such work is clear. The
addiƟon of a feedback loop, for instance, would allow users not
simply to have their EEG response to parƟcular works read, but
also to dictate the mood they wanted to achieve. Instead of
having playlists to match our mood, a person would have the
opƟon of entering their desired emoƟon into a computer, with a
customized playlist then generated to provoke that speciﬁc
response. This may have parƟcular applicaƟon in the therapeuƟc
world to help treat those suﬀering from stress or forms of
depression. Runners, meanwhile, could have their pulse rates
203

measured by the headphones they’re wearing, with music
selected according to whether heart rate rises or falls. Translated
to literature, electronic novels could monitor the electrical
acƟvity of neurons in the brain while they are being read, leading
to algorithms rewriƟng secƟons to match the reacƟons elicited.
In the same way that a stand-up comic or live musician subtly
alters their performance to ﬁt a parƟcular audience, so too will
media increasingly resemble its consumer. The medium might
stay the same, but the message will change depending on who is
listening.
In an arƟcle published in the New Statesman, journalist
Alexandra Coughlan refers to this idea as “aural pill-popping,” in
which EmoƟonal OpƟmizaƟon will mean that there will be “one
[music] track to bring us up [and] another to bring us down.”45
This comment demonstrates a belief in funcƟonal form—the idea
that, as I described earlier in this chapter, it is desirable that art
be “made useful” in some way. Coughlan’s suggesƟon of “aural
pill-popping” raises a number of quesƟons—not least whether the
value of art is simply as a creaƟve subsƟtute for mind-altering
drugs.
We might feel calm looking at Mark Rothko’s UnƟtled (Green
on Blue) painƟng, for example, but does this relegate it to the
arƟsƟc equivalent of Valium? In his book To Save Everything, Click
Here, Belarusian technology scholar Evgeny Morozov takes this
uƟlitarian idea to task. Suppose, Morozov says, that Google
(selecƟng one company that has made clear its ambiƟons to
quanƟfy everything) knows that we are not at our happiest aŌer
receiving a sad phone call from an ex-girlfriend. If art equals
pleasure—and the quickest way to achieve pleasure is to look at a
great painƟng—then Google knows that what we need more
than anything for a quick pick-me-up is to see a painƟng by
Impressionist painter Renoir:
Well, Google doesn’t exactly “know” it; it knows only
204

that you are missing 124 units of “art” and that,
according to Google’s own measurement system,
Renoir’s painƟngs happen to average in the 120s.
You see the picture and—boom!—your mood stays
intact.46
Morozov conƟnues his line of inquiry by asking the perƟnent
quesƟons that arise with such a proposiƟon. Would keeping our
mood levels stabilized by looking at the painƟngs of Renoir turn us
into a world of art lovers? Would it expand our horizons? Or would
such aƩempts to consume art in the manner of self-help
literature only serve to demean arƟsƟc endeavors? SƟll more
problems not touched on by Morozov surface with eﬀorts to
quanƟfy art as unitary measures of pleasure, in the manner of
Sergei Eisenstein’s “aƩracƟons.” If we accept that Renoir’s work
gives us a happiness boost of, say, 122, while Pablo Picasso’s score
languishes at a mere 98, why bother with Picasso’s work at all?
Similarly, let’s imagine for a moment that the complexity of
Beethoven’s 7th Symphony turns out to produce measurably
greater neurological highs than JusƟn Bieber’s hit song “Baby,”
thereby giving us the ability to draw a mathemaƟcal disƟncƟon
between the fields of “high” and “low” art. Should this prove to be
the case, could we receive the same dosage of arƟsƟc
nourishment—albeit in a less eﬃcient Ɵme frame—by watching
mulƟple episodes of Friends (assuming the sitcom is classiﬁed as
“low” art) as we could from reading Leo Tolstoy’s War and Peace
(supposing that it is classiﬁed as “high” art)? UlƟmately,
presuming that War and Peace is superior to Friends, or that
Beethoven is superior to JusƟn Bieber, simply because they top up
our arƟsƟc needs at a greater rate of knots, is essenƟally the
same argument as suggesƟng that James PaƩerson is a greater
novelist than J. M. Coetzee on the basis that data gathered by
Kindle shows that Patterson’s Kill Alex Cross can be read in a single
aŌernoon, while Coetzee’s Life & Times of Michael K takes
205

several days, or even weeks. It may look mathemaƟcally rigorous,
but something doesn’t quite add up.
206

The Dehumanization of Art
All of this brings us ever closer to the inevitable quesƟon of
whether algorithms will ever be able to generate their own art.
Perhaps unsurprisingly, this is a line of inquiry that provokes
heated comments on both sides. “It’s only a maƩer of when it
happens—not if,” says Lior Shamir, who built the automated art
criƟc I described earlier. Much as Epagogix’s movie predicƟon
system spots places in a script where a potential yield is not where
it should be and then makes recommendaƟons accordingly, so
Shamir is convinced that in the long term his creaƟon will be able
to spot the features great works of art have in common and
generate entire new works accordingly.
While this might seem a new concept, it is not. In 1787, Mozart
anonymously published what is referred to in German as
Musikalisches Würfelspiel (“musical dice game”). His idea was
simple: to enable readers to compose German waltzes, “without
the least knowledge of music . . . by throwing a certain number
with two dice.” Mozart provided 176 bars of music, arranged in
16 columns, with 11 bars to each column. To select the ﬁrst
musical bar, readers would throw two dice and then choose the
corresponding bar from the available opƟons. The technique was
repeated for the second column, then the third, and so on. The
total number of possible composiƟons was an astonishing 46 ×
1,000,000,000,000,000, with each generated work sounding
Mozartian in style.47
A similar concept—albeit in a diﬀerent medium—is the current
work of CelesƟno Soddu, a contemporary Italian architect and
designer who uses what are referred to as “geneƟc algorithms”
to generate endless variaƟons on individual themes. A geneƟc
algorithm replicates evoluƟon inside a computer, adopƟng the
idea that living organisms are the consummate problem solvers
and using this to opƟmize speciﬁc soluƟons. By inpuƫng what he
considers to be the “rules” that deﬁne, say, a chair or a Baroque
207

cathedral, Soddu is able to use his algorithm to conceptualize
what a parƟcular object might look like were it a living enƟty
undergoing thousands of years of natural selection. Because there
is (realisƟcally speaking) no limit to the amount of results the
geneƟc algorithm can generate, Soddu’s “idea-products” mean
that a trendy adverƟsing agency could conceivably ﬁll its oﬃces
with hundreds of chairs, each one subtly diﬀerent, while a
company engaged in building its new corporate headquarters
might generate thousands of separate designs before deciding
upon one to go ahead with.
There are, however, sƟll problems with the concept of creaƟng
art by algorithm. Theodor Adorno and Max Horkheimer noted in
the 1940s how formulaic art does not oﬀer new experiences, but
rather remixed versions of what came before. Instead of the joy
of being exposed to something new, Adorno saw mass culture’s
reward coming in the form of the smart audience member who
“can guess what is coming and feel ﬂaƩered when it does
come.”48 This prescient comment is backed up by algorithms that
predict the future by establishing what has worked in the past. An
artwork in this sense might achieve a quanƟﬁable perfecƟon, but
it will only ever be perfecƟon measured against what has already
occurred.
For instance, Nick Meaney acknowledges that Epagogix would
have been unable to predict the huge success of a ﬁlm like Avatar.
The reason: there had been no $2 billion ﬁlms before to measure
it against. This doesn’t mean that Epagogix wouldn’t have
realized it had a hit on its hands, of course. “Would we have said
that it would earn what it did in the United States? Probably not,”
Meaney says. “It would have been ﬂagged up as being oﬀ the
scale, but because it was oﬀ the scale there was nothing to
measure it against. The next Avatar, on the other hand? Now
there’s something to measure it against.”
The issue becomes more pressing when it relates to the
generaƟng of new art, rather than the measurement of exisƟng
208

works. Because Lior Shamir’s automated art criƟc algorithm
measures works based on 4,024 diﬀerent numerical descriptors,
there is a chance that it might be able to quanƟfy what would
comprise the best illustraƟon of, say, pop art and generate an
artwork that conforms to all of these criteria. But these criteria
are themselves based upon human creaƟvity. Would it be possible
for algorithms themselves to move art forward in a meaningful
way, rather than simply aping the style of previous works? “At
ﬁrst, no,” Shamir says. “UlƟmately, I would be very careful in
saying there are things that machines can not do.”
A beƩer quesƟon might be whether we would accept such
works if they did—knowing that a machine rather than a human
arƟst had created them? For those that see creaƟvity as a
profoundly human acƟvity (a relaƟvely new idea, as it happens),
the quesƟon is one that goes beyond technical ability and touches
on somewhat close to the essence of humanity.
In 2012, the London Symphony Orchestra took to the stage to
perform composiƟons wriƩen enƟrely by a music-generaƟng
algorithm called Iamus.49 Iamus was the project of professor and
entrepreneur Francisco Vico, under whose coding it has composed
more than one billion songs across a wide range of genres. In the
aŌermath of Iamus’s concert, a staﬀ writer for the Columbia
Spectator named David Ecker put pen to paper (or rather ﬁnger
to keyboard) to write a polemic taking aim at the new
technology. “I use computers for damn near everything, [but]
there’s something about this computer that I ﬁnd deeply
troubling,” Ecker wrote.
I’m not a purist by any stretch. I hate overt music
categorizaƟon, and I hate most debates about
“real” versus “fake” art, but that’s not what this is
about. This is about the very essence of humanity.
Computers can compete and win at Jeopardy!, beat
chess masters, and connect us with people on the
209

other side of the world. When it comes to emoƟon,
however, they lack much of the necessary
equipment. We live every day under the pretense
that what we do carries a certain weight, partly due
to the knowledge of our own mortality, and this
always comes through in truly great music. Iamus
has neither mortality nor the urgency that comes
with it. It can create sounds—some of which may be
pleasing—but it can never achieve the emoƟonal
complexity and creaƟve innovaƟon of a musician or
a composer. One could say that Iamus could be an
ideal tool for creating meaningless top-40 tracks, but
for me, this too would be troubling. Even the most
transient and superﬁcial of pop tracks take root in
the human experience, and I believe that even those
are worth protecting from Iamus.50
Perhaps there is sƟll hope for those who dream of an algorithm
creaƟng art. However, as Iamus’s Francisco Vico points out: “I
received one comment from a woman who admiƩed that Iamus
was a milestone in technology. But she also said that she had to
stop listening to it, because it was making her feel things. In some
senses we see this as creepy, and I can fully understand that. We
are not ready for it. Part of us sƟll thinks that computers are
Terminators that want to kill us, or else simple tools that are
supposed to help us with processing informaƟon. The idea that
they can be arƟsts, too, is something unexpected. It’s something
new.”
210

CONCLUSION
Predicting the Future
In 1954, a 34-year-old American psychology professor named Paul
E. Meehl published a groundbreaking book with a somewhat
unwieldy Ɵtle. Clinical vs. StaƟsƟcal PredicƟon: A TheoreƟca
Analysis and a Review of the Evidence presented 20 case studies
in which predicƟons made by staƟsƟcal algorithms were
compared with clinical predictions made by trained experts.1
A sample study asked trained counselors to predict the end-of-
year grades of ﬁrst-year students. The counselors were allowed
three-quarters of an hour to interview each student, along with
access to their previous grades, mulƟple apƟtude tests, and a
personal statement that ran four pages in length. The algorithm,
meanwhile, required only high school grades and a single apƟtude
test. In 11 out of 14 cases, the algorithm proved more accurate at
predicting students’ finishing grades than did the counselors.
The same proved true of the textbook’s other studies, which
analyzed topics as diverse as parole violation rates (as per Chapter
3’s Richard Berk) to would-be pilots’ success during training. In 19
out of 20 cases—which Meehl later argued should be modiﬁed to
a clean sweep—the staƟsƟcal algorithms were demonstrably
more accurate than those made by the experts, and almost
always required less data in order to get there. “It is clear,”
Meehl concluded, “that the dogmaƟc, complacent asserƟon
someƟmes heard from clinicians that ‘naturally’ clinical
predicƟons, being based on ‘real understanding’ is superior, is
simply not justified by the facts to date.”
Meehl was, perhaps understandably, something of an outsider
in academic circles from this point on. His anƟ-expert stance
211

amounted to suggesƟng—in the words of a colleague quoted in
Meehl’s 2003 New York Times obituary—that “clinicians could be
replaced by a clerk with a hand-cranked Monroe calculator.” 2
(Meehl’s status as prototypical Internet troll was only further
added to by the publishing of a later paper in his career, enƟtled
“Why I Do Not AƩend Case Conferences,” in which he dismissed
academic conferences on the basis that they were boring to the
point of offensiveness.)
Regardless of his divisive status at the Ɵme, Meehl’s views of
the predicƟve power of algorithms have been borne out in the
years since. In the roughly 200 similar studies that have been
carried out in the half century since, algorithms have triumphed
over human intuiƟon with a success rate of around 60 percent. In
the remaining 40 percent, the diﬀerence between staƟsƟcal and
clinical 
predicƟons 
proved 
staƟsƟcally 
insigniﬁcant, 
sƟll
represenƟng a Ɵck in the “win” column for the algorithmic
approach, since this is almost always cheaper than hiring an
expert.
212

The Power of Thinking Without Thinking
Why do algorithms interest us? The ﬁrst point to make is that it is
quite likely that many of the computer scienƟsts reading this book
will be the same people who would have picked up a similar book
in 1984, or 1964. But not all of us (including this writer) are
computer scienƟsts by trade, and the quesƟon of how and why a
once obscure mathemaƟcal concept came to occupy the front
page of major newspapers and other publicaƟons was one that
often occurred to me when I was carrying out my research.
In this ﬁnal chapter, I would like to share some of my thoughts
on that quesƟon. It seems obvious to point out that the reason for
this comes down to the growing role that algorithms have to play
in all of our lives on a daily basis. Search engines like Google help
us to navigate massive databases of informaƟon. Recommender
systems like those employed by Amazon meanwhile map our
preferences against those of other people and suggest new bits of
culture for us to experience. On social networking sites,
algorithms highlight news that is “relevant” to us, and on daƟng
sites like eHarmony they match us up with potenƟal life partners.
It is not “cyberbole,” then, to suggest that algorithms represent a
crucial force in our participation in public life.
They go further than the four main areas I have chosen to look
at in this book, too. For instance, algorithmic trading now
represents a whopping 70 percent of the U.S. equity market,
running on supercomputers that are able to buy and sell millions
of shares at pracƟcally the speed of light. Algorithmic trading has
become a race measured in milliseconds, with billions of dollars
dependent on the laying of new ﬁber-opƟc cables that will shave
just ﬁve milliseconds oﬀ the communicaƟon Ɵme between
ﬁnancial markets in London and New York. (To put this in
perspective, it takes a human 300 milliseconds to blink.)3
Medicine, too, has taken an algorithmic turn, as doctors
working in hospitals are oŌen asked to rely on algorithms rather
213

than their own clinical judgment. In his book Blink: The Power of
Thinking Without Thinking, Malcolm Gladwell recounts the story
of one hospital that adopted an algorithm for diagnosing chest
pain. “They instructed their doctors to gather less informaƟon on
their paƟents,” Gladwell writes, explaining how doctors were told
to instead zero in “on just a few criƟcal pieces of informaƟon
about paƟents . . . like blood pressure and the ECG—while
ignoring everything else, like the paƟent’s age and weight and
medical history. And what happened? Cook County is now one of
the best places in the United States at diagnosing chest pain.”4
Recent medical algorithms have been shown to yield equally
impressive results in other areas, such as an algorithm able to
diagnose for Parkinson’s disease by listening to a person’s voice
over the telephone, and another paƩern-recogniƟon algorithm
able to, quite literally, “sniff” for diseases like cancer.
214

Algorithmizing the World
Can everything be subject to algorithmizaƟon? There are two
ways to answer this quesƟon. The ﬁrst is to approach it purely on
a technical level. At present, no, everything cannot be “solved” by
an algorithm. At Ɵme of wriƟng, for instance, recognizing objects
with anything close to the ability of a human is sƟll a massive
challenge. A young child only has to be shown a handful of
“training examples” in order to idenƟfy a parƟcular object—even
if they have never seen that object before. An algorithm designed
for a similar task, however, will frequently require long pracƟce
sessions in which the computer is shown thousands of diﬀerent
versions of the same thing and corrected by a human when it is
wrong. Even then, an algorithm may struggle with the task when
carrying it out in a real-world environment, in which it is
necessary to disambiguate contours belonging to diﬀerent
overlapping objects.
Computer scienƟst and teacher John MacCormick similarly
gives the example of an algorithm’s unsuitability for being used as
a teaching aid for grading students’ work, since this is a task that
is too complex (and, depending on the subject, too subjecƟve) for
a bot to carry out.5 Could both of these tasks be performed
algorithmically in the future as computers conƟnue to get more
powerful? Absolutely. It is for this reason that it is dangerous to
bet against a bot. A decade ago, respected MIT and Harvard
economists Frank Levy and Richard Murnane published a well-
researched book entitled The New Division of Labor, in which they
compared the respecƟve capabiliƟes of human workers and
computers. In an opƟmisƟc second chapter called “Why People
SƟll MaƩer,” the authors described a spectrum of informaƟon-
processing tasks ranging from those that could be handled by a
computer (e.g., arithmeƟc), to those that only a human could do.
One illustration they gave was that of a long-distance truck driver:
215

The . . . truck driver is processing a constant stream
of [visual, aural and tacƟle] informaƟon from his
environment. . . . [T]o program this behavior we
could begin with a video camera and other sensors
to capture the sensory input. But execuƟng a leŌ
turn against oncoming traﬃc involves so many
factors that it is hard to imagine discovering the set
of rules that can replicate a driver’s behavior. . . .
ArƟculaƟng [human] knowledge and embedding it in
soŌware for all but highly structured situaƟons are
at present enormously diﬃcult tasks. . . . Computers
cannot easily subsƟtute for humans in [jobs like truck
driving].6
At least one part of this asserƟon is correct: computers cannot
easily subsƟtute for humans when it comes to driving. Certainly
Levy and Murnane were not mistaken at the Ɵme that they were
wriƟng. The year their book was released, DARPA announced its
Grand Challenge, in which entrants from the country’s top AI
laboratories competed for a $1 million prize by construcƟng
driverless vehicles capable of navigaƟng a 142-mile route through
the Mojave Desert. The “winning” team made it less than eight
miles (in several hours) before it caught ﬁre and shuddered to a
halt.
A lot can change in a decade, however, as you will know from
reading Chapter 3, in which I discuss the success of Google’s self-
driving cars. Should such technologies prove suitably eﬃcient,
there is every possibility that they will take over the jobs currently
occupied by taxi drivers and long-distance drivers.
Similar paradigm shiŌs are now taking place across a wide
range of ﬁelds and industries. Consider Amazon, for instance. In
Amazon’s early days (when it was just an online book retailer,
rather than the leviathanic “everything store” it was referred to
as in a recent biography) it featured two rival departments,
216

whose interdepartmental squabbling serves as a microcosm of
sorts for the type of ﬁght regularly seen in the age of The
Formula. One department was made up of the editorial staﬀ,
whose job it was to review books, write copy for the website’s
home page and provide a reassuringly human voice to customers
sƟll wary about handing over their credit card details to a faceless
machine. The other group was referred to as the personalizaƟon
team and was tasked with the creaƟon of algorithms that would
recommend products to individual users.
Of the two departments, it was this laƩer division that won
both the short-term baƩle and the long-term war. Their winning
weapon was called Amabot and replaced what had previously
been person-to-person, handcrafted sections of Amazon’s website
with automaƟcally generated suggesƟons that conformed to a
standardized layout. “The system handily won a series of tests and
demonstrated it could sell as many products as the human
editors,” wrote Brad Stone in his well-researched 2013 history of
Amazon.
AŌer the editorial staﬀ had been rounded up and either laid oﬀ
or else assigned to other parts of the company, an employee
summed up the prevailing mood by placing a “lonely hearts”
adverƟsement in the pages of a local SeaƩle newspaper on
ValenƟne’s Day in 2002, addressing the algorithm that had
rendered them obsolete:
DEAREST AMABOT: If you only had a heart to absorb
our hatred . . . Thanks for nothing, you jury-rigged
rust bucket. The gorgeous messiness of ﬂesh and
blood will prevail!7
This is a senƟment that is sƟll widely argued—parƟcularly
when algorithms take on the kind of humaniƟes-oriented ﬁelds I
have approached in this book. However, it is also necessary to
note that drawing a deﬁnite line only capable of being crossed by
217

the “gorgeous messiness of [the] ﬂesh” is a liƩle like Levy and
Murnane’s statements about which jobs are safe from
automation.
Certainly, there are plenty of jobs and other areas of life now
carried out by algorithm, which were previously thought to have
been the sole domain of humans. Facial recogniƟon, for instance,
was once considered to be a trait performable only by a select
few higher-performing animals—humans among them. Today
algorithms employed by Facebook and Google regularly recognize
individual faces out of the billions of personal images uploaded by
users.
Much the same is true of language and automated translaƟon.
“There is no immediate or predictable prospect of useful machine
translaƟon,” concluded a U.S. NaƟonal Academy of Sciences
commiƩee in 1965. Leap forward half a century and Google
Translate is used on a daily basis, oﬀering two-way translaƟon
between 58 diﬀerent languages: 3,306 separate translaƟon
services in all. “The service that Google provides appears to
ﬂaƩen and diversify inter-language relaƟons beyond the wildest
dreams of even the E.U.’s most enthusiasƟc language parity
proponents,” writes David Bellos, author of Is That a Fish in Your
Ear?: TranslaƟon and the Meaning of Everything.8 Even if Google
Translate’s results aren’t always perfect, they are oŌen “good
enough” to be useful—and are getting better all the time.
218

The Great Restructuring
What is notable about The Formula is how, in many cases, an
algorithm can replace large numbers of human workers. Jaron
Lanier makes this point in his most recent book, Who Owns The
Future?, by comparing the photography company Kodak with the
online video-sharing social network Instagram. “At the height of
its power . . . Kodak employed more than 140,000 people and was
worth $28 billion,” Lanier observes. “They even invented the ﬁrst
digital camera. But today Kodak is bankrupt, and the new face of
digital photography has become Instagram. When Instagram was
sold to Facebook for $1 billion in 2012, it employed only 13 people.
Where did all those jobs disappear to? And what happened to the
wealth that all those middle-class jobs created?”9
What causes shock for many people commenƟng on this
subject is how indiscriminate the automaƟon is. Nothing, it
seems, is safe from a few well-designed algorithms offering speed,
eﬃciency and value for money. Increasing numbers of books carry
doom scenarios related to industries struggling in the wake of The
Formula. In Failing Law Schools, law professor Brian Tamanha
points to U.S. government staƟsƟcs suggesƟng that unƟl 2018
there will only be 25,000 new openings available for young
lawyers—despite the fact that law schools will produce around
45,000 graduates during that same Ɵme frame. It is quite
possible, Tamanha writes, that this raƟo will one day be
remembered as the “good old days.” Indeed, it is quite
conceivable to imagine a future in which law ﬁrms stop hiring
junior and trainee lawyers altogether, and pass much of this work
over to arƟﬁcial intelligence systems instead. In keeping with this,
a number of experts predict that there will be between 10 and 40
percent fewer lawyers a decade from now as there are today.10
As Erik Brynjolfsson and Andrew McAfee suggest in their
pamphlet “Race Against the Machine,” this is not so much the
result of a Great Recession or a Great StagnaƟon, so much as it is
219

a Great Restructuring.11 The new barometer for which jobs are
safe from The Formula has less to do with the social class of those
tradiƟonally holding them than it does to do with a trade-oﬀ
between cost and eﬃciency. Professions and ﬁelds that have
evolved to operate as ineﬃciently as possible (lawyers,
accountants, barristers and legislators, for example) while also
charging the most money will be parƟcularly vulnerable when it
comes to automaƟon. To survive—as economist Theodore LeviƩ
famously phrased it in his 1960 arƟcle “MarkeƟng Myopia”—
every industry must “plot the obsolescence of what now produces
their livelihood.”12
In the new algorithmic world, it is the computer scienƟsts and
mathemaƟcians who will be increasingly responsible for making
cultural determinaƟons and will ulƟmately thrive in the Great
Restructuring. Others will suﬀer the “end of work” described by
social theorist Jeremy Rifkin in his book of the same name. This is a
workplace in which “fewer and fewer workers will be needed to
produce the goods and services for the global populaƟon.” As
costs of everything from legal bills to entertainment come down,
so too will the availability of many types of work decrease. As
André Gorz writes in Farewell to the Working Class, “The majority
of the populaƟon [will end up belonging to] the post-industrial
neo-proletariat which, with no job security or deﬁnite class
idenƟty, ﬁlls the area of probaƟonary, contracted, casual,
temporary and part-Ɵme employment.”13 As job security and
class idenƟty are replaced by automaƟon and algorithmic user
proﬁles, the world may ﬁnally get the “twenty-hour working week
and reƟrement at ﬁŌy” that previous generaƟons of techo-
utopianists dreamed about. It just won’t necessarily be
voluntary.14
220

Spoons Instead of Shovels
There is a famous anecdote about the American economist and
staƟsƟcian Milton Friedman visiƟng a country in Asia during the
1960s. Taken to a worksite where a new canal was being
excavated, Friedman was shocked to see that the workers were
using shovels instead of modern tractors and earthmovers. “Why
are there so few machines?” he asked the government
bureaucrat traveling with him. “You don’t understand,” came the
answer. “This is a jobs program.” Friedman considered for a
second, then replied, “Oh, I thought you were trying to build a
canal. If it’s jobs you want, then you should give these workers
spoons, not shovels.”
You could, of course, extend this to any number of
technologies. Tractors are more eﬃcient earthmovers than
shovels, as shovels are more eﬃcient than spoons, and spoons are
more eﬃcient than hands. The quesƟon is, where do we stop the
process? The cultural theorist Paul Virilio once pointed out how
the invenƟon of the ship was also the invenƟon of the shipwreck.
If this is the case, then how many shipwrecks do we need before
we stop building ships? Those looking for stories of algorithms run
amok can certainly ﬁnd them with relaƟve ease. On May 6, 2010,
the Dow Jones Industrial Average plunged 1,000 points in just 300
seconds—eﬀecƟvely wiping out close to $1 trillion of wealth in a
stock market debacle that became known as the Flash Crash.
Unexplained to this day, the Flash Crash has been pinned on
everything from the impact of high-speed trading to a technical
glitch.15
Yet few people would seriously put forward the view that
algorithms are, in themselves, bad. Indeed, it’s not simply a
maƩer of algorithms doing the jobs that were once carried out
manually; in many cases algorithms perform tasks that would be
impossible for a human to perform. ParƟcularly, algorithms like
those uƟlized by Google that rely on unimaginably large datasets
221

could never be reenacted by hand. Consider also the algorithm
developed by mathemaƟcian Max LiƩle that is able to diagnose
Parkinson’s disease down the phone line, by “listening” to callers’
speech paƩerns for vocal tremors that are inaudible to the
human ear.16
The French economist Jean FourasƟé humorously asked
whether prehistoric man felt the same trepidaƟon at the
invenƟon of the bronze sword that 20th-century man did at the
birth of the atomic bomb. As technologies are invented and prove
not to be the end of humanity, they recede into background
noise, where they become fodder for further generaƟons of
disrupƟve technology, just as the strongest lions eventually
weaken and are overtaken by younger, ﬁƩer ones. Confusing the
maƩer further is the complex relaƟonship we enjoy with
technology on a daily basis. Like David Ecker, the Columbia
Spectator journalist I quoted in the last chapter, most of us hold
concerns over “bad” uses of technology, while enjoying everything
good technology makes possible. To put it another way, how did I
ﬁnd out the exact details of the Flash Crash I menƟoned above? I
Googled it.
222

Objectivity in the Post-mechanical Age
One topic I conƟnued to buƩ up against during the wriƟng of this
book (and in my other tech wriƟng for publicaƟons like Fast
Company and Wired) is the subject of objecƟvity. In each chapter
of this book, the subject of objecƟvity never strayed far from
either my own mind or the various conversaƟons I enjoyed with
the technologists I had the opportunity to interview. In Chapter 1,
the quesƟon of objecƟvity arose with the idea that there are
certain objecƟve truths algorithms can ascertain about an
individual, be those the clinician-free means by which Larry Smarr
diagnosed himself with Crohn’s disease or the “dividual” proﬁles
constructed by companies like Quantcast. In Chapter 2, the
dream of objecƟvity revolved around the ultraraƟonal matching
process at the heart of algorithmic daƟng, supposed to provide us
with a “beƩer” way of matching us with romanƟc partners. In
Chapter 3, objecƟvity was a way of making the law fairer, by
creaƟng legal algorithms that would ensure that the law was
enforced the same way every Ɵme. Finally, in Chapter 4,
objecƟvity was about the universal rules relaƟng to what deﬁnes
something as a work of art.
ObjecƟvity is a term that is oŌen associated with algorithms,
and companies in thrall of them. For example, Google notes in its
“Ten Things We Know to Be True” manifesto that “our users trust
our objecƟvity.” Were we to think for too long about the fact that
we expect a publicly traded company to be enƟrely objecƟve (or
even that such a thing is possible when it comes to ﬁltering and
ranking informaƟon), we might see the fundamental ﬂaw in this
conundrum—but then again, when Google is providing search
results in 0.21 seconds, there isn’t a great deal of Ɵme to think.
“This is a very unique way of thinking about the world,” says
scholar Ted Striphas, author of The Late Age of Print, who has
spent the past several years invesƟgaƟng what he calls
algorithmic culture. “It’s about degrees of pure objecƟvity, where
223

you are never in the realm of subjecƟvity; you’re only in the realm
of geƫng closer and closer to some inexorable truth . . . You are
never wrong, you’re only ever more right.”
As it happens, Google may actually be telling the truth here:
their users really do seem to trust in their objectivity. According to
a survey of web users carried out in 2005, only 19 percent of
individuals expressed a lack of trust in their search engines, while
more than 68 percent considered the search engines they used
regularly to be fair and unbiased.17
Science-ﬁcƟon author Arthur C. Clarke famously wrote, “any
suﬃciently advanced technology is indisƟnguishable from magic.”
Just like photography appeared to people over 100 years ago, so
too does the speed with which algorithms work make us view
them as authoritaƟve and unaﬀected by human fallibility. Paste a
block of text into Google’s translaƟon services and in less than a
second its algorithms can transform the words into any one of 58
diﬀerent languages. The same is true of Google’s search
algorithms, which, as a result of its “knowledge” about individual
users, allow our speciﬁc desires and requirements to be predicted
with an almost preternatural ability. “Google works for us
because it seems to read our minds,” says media scholar Siva
Vaidhyanathan, “and, in a way, it does.”18
As with magic, our reverence for Google’s work comes partly
because we see only the end result and none of the working. Not
only are these black-boxed and obscured, they are pracƟcally
instantaneous. This eﬀect doesn’t only serve to fool the
technologically uniformed. At the start of his book Nine
Algorithms 
That 
Changed 
the 
Future, 
accomplished
mathemaƟcian and computer scienƟst John MacCormick writes,
“at the heart of every algorithm . . . is an ingenious trick that
makes the whole thing work.” He goes on to expand upon the
statement, suggesting that:
Since I’ll be using the word “trick” a great deal, I
224

should point out that I’m not talking about the kind
of tricks that are mean and deceiƞul—the kind of
trick a child might play on a younger brother or
sister. Instead, the tricks . . . resemble tricks of the
trade or even magic tricks: clever techniques for
accomplishing goals that would otherwise be difficult
or impossible.19
While perhaps a well-intenƟoned disƟncƟon, MacCormick’s
error is his casual assumpƟon about a sense of algorithmic
morality. StaƟng that algorithms and the goals they aim to
accomplish are neither good nor bad (although, to return to
Melvin Kranzberg’s ﬁrst law of technology, nor are they neutral)
seems an extraordinarily sweeping and unqualiﬁed statement.
Nonetheless, it is one that has been made by a number of
renowned technology writers. In a controversial 2008 article
published 
in W i r e d magazine, journalist Chris Anderson
announced that the age of big datasets and algorithms equaled
what he grandly referred to as “The End of Theory.” No more
would we have to worry about the eliƟst theories of so-called
experts, Anderson said.
There is now a beƩer way. Petabytes allow us to
say: “CorrelaƟon is enough.” We can stop looking for
models. We can analyze the data without
hypotheses about what it might show. We can throw
the numbers into the biggest compuƟng clusters the
world has ever seen and let staƟsƟcal algorithms
find patterns where science cannot.20
The problem with Anderson’s enthusiasƟc embrace of The
Formula, of course, is his failure to realize that data mining, even
on large datasets, is itself founded on a theory. As I have shown
225

throughout this book, algorithms can oŌen reﬂect the biases of
their creators—based upon what it is that they deem to be
important when answering a parƟcular quesƟon. When an
algorithm is created to determine what informaƟon is relevant,
or the best way to solve a problem, this represents a hypothesis in
itself. Even data is not free of human bias, from what data is
collected to the manner in which it is cleaned up and made
algorithm-ready. A computaƟonal process that seeks to sort,
classify and create hierarchies in and around people, places,
objects and ideas carries considerable poliƟcal connotaƟons. So
too does a subject like the kind of user categorizaƟon I discussed
in Chapter 1. What are the categories? Who belongs to each
category? And how do we know that categories are there to help
—rather than hinder—us?
226

Can an Algorithm Defame?
In March 2013, a T-shirt manufacturer called Solid Gold Bomb
found itself in a heated row with Amazon over a slogan generated
by an algorithm. Seizing upon the trend for “Keep Calm and . . .”
paraphernalia sweeping the UK at the Ɵme, Solid Gold Bomb
created a simple bot to generate similar designs by running
through an English dicƟonary and randomly matching verbs with
adjecƟves. In all, 529,493 similarly themed clothing items
appeared on Amazon’s site—with the T-shirts only printed once a
customer had actually bought one. As website BoingBoing wrote
of the business plan: “It costs [Solid Gold Bomb] nothing to create
the design, nothing to submit it to Amazon and nothing for
Amazon to host the product. If no one buys it then the total cost
of the experiment is eﬀecƟvely zero. But if the algorithm
stumbles upon something special, something that is both unique
and funny and actually sells, then everyone makes money.”21
Unfortunately, no one at Solid Gold Bomb had apparently
considered the possibility that the algorithm might generate
oﬀensive slogans while running down its available list of words.
The variaƟon that set Amazon’s blood boiling was the oﬀensive
“Keep Calm and Rape a Lot”—although had this not done the job,
it would likely have been equally appalled by the misogynisƟc
“Keep Calm and Hit Her,” or the ever-unpopular “Keep Calm and
Grope On.” When it found out what was going on, Amazon
responded by immediately pulling all of Solid Gold Bomb’s online
inventory. The T-shirt manufacturer (which wound up going out of
business several months later) was upset. Why, its owners
wondered, should they be punished when the fault was not with
any human agency—but with an algorithm, for whom the words
in question meant nothing?
A similar variaƟon on this problem took place the previous
year, when Google’s “auto-complete” algorithm came under ﬁre
for alleged defamaƟon from Beƫna Wulﬀ, wife of the former
227

German president ChrisƟan Wulﬀ. 22 Originally an algorithm
designed to help people with physical disabiliƟes increase their
typing speed, auto-complete was added to Google’s funcƟonality
as a way of saving users Ɵme by predicƟng their search terms
before they had ﬁnished typing them. “Using billions of searches,
Google has prototyped an anonymous proﬁle of its users,” says
creator Marius B. Well. “This reﬂects the fears, inquiries,
preoccupaƟons, obsessions and ﬁxaƟons of the human being at a
certain age and our evoluƟon through life.”23 Type Barack
Obama’s name into the Google search box, for example, and you
would be presented with potenƟally useful suggesƟons including
“Barack Obama,” “Barack Obama TwiƩer,” “Barack Obama
quotes” and “Barack Obama facts.” Type in the name of United
Kingdom deputy prime minister Nick Clegg, on the other hand,
and you are liable to ﬁnd “Nick Clegg is a prick,” “Nick Clegg is a
liar,” “Nick Clegg is sad” and “Nick Clegg is ﬁnished.” Of these two
camps, Beƫna Wulﬀ’s suggested searches fell more in line with
Nick Clegg’s than Barack Obama’s. A person searching for Wulﬀ’s
name was likely to ﬁnd search terms linking her to prosƟtuƟon
and escort businesses.24
Realizing the eﬀect that this was likely to have on someone
searching for her name online, Wulﬀ took Google to court and
won. A German court decided that Google would have to ensure
that the terms algorithmically generated by auto-complete were
not oﬀensive or defamatory in any way. Google was upset,
claiming to be extremely “disappointed” by the ruling, since this
impugned the supposed objecƟve imparƟality of its algorithms.
“We believe that Google should not be held liable for terms that
appear in auto-complete as these are predicted by computer
algorithms based on searches from previous users, not by Google
itself,” said a spokesperson for the company. “We are currently
reviewing our opƟons.” The problem is the amount, both
ﬁguraƟvely and literally, that Google has feƟshisƟcally invested in
its algorithmic vision. Like the concept of sci-ﬁ author Philip K.
228

Dick’s “minority reports” referenced in Chapter 3, if the algorithm
proves fallible then tugging on this thread could have catastrophic
results. “Google’s spiritual deferral to ‘algorithmic neutrality’
betrays the company’s growing unease with being the world’s
most important informaƟon gatekeeper,” writes Evgeny Morozov
in his book The Net Delusion. “Its founders prefer to treat
technology as an autonomous and fully objecƟve force rather
than spending sleepless nights worrying about inherent biases in
how their systems—systems that have grown so complex that no
Google engineer fully understands them—operate.”25
A narraƟve thread oŌen explored in books and arƟcles about
Google is the degree to which Google’s rise has helped speed up
the decline of tradiƟonal news outlets, like newspapers. In this
sense, Google has displaced tradiƟonal media, even though it
does not generate news stories itself. If Google’s algorithms
ought to be subject to the same standards as newspapers,
though, this poses some problems. In a classic study of newsroom
objecƟvity, sociologist Gaye Tuchman observed that it was a fear
of defamaƟon that kept journalism objecƟve. By reporƟng the
views of others rather than relying on their own opinion,
journalists were protected against allegaƟons that they were
biased in their reporƟng. In terms of Google’s auto-complete
algorithm, it had also relied on quoƟng others rather than
expressing opinions, since its suggested terms were based on the
previous searches of thousands, or even millions, of users. By not
censoring these searches, however, and keeping its algorithms
apparently objective, Google had been accused of defamation.26
229

Why Is TwiƩer Like a Newspaper? (And Why Isn’
Google?)
The Beƫna Wulﬀ case marked several interesƟng developments.
For one thing, it represented one of the ﬁrst Ɵmes that the
poliƟcs of algorithms became the subject of a legal trial.
Algorithms can be diﬃcult to criƟcize. In contrast to the likes of a
service such as Google Street View—whose arrival sparked street
protests in some countries due to its perceived violaƟon of privacy
—the invisibility of an algorithm can make it tough to spot its
eﬀects. It is one thing to be able to log on to a server and see a
detailed image of your house as taken from the end of your
driveway. It is another to criƟque the inner workings of the
algorithms that underpin companies such as Google, Facebook
and Amazon. In the majority of cases, these algorithms are black-
boxed in such a way that users have no idea how they work. Like
the changing concept of transparency in the digital world
(something I discussed in Chapter 3), oŌen the idea that complex
technology can work under a purposely simplisƟc interface is
viewed by Silicon Valley decision-makers as a key selling point.
Speaking about Google’s algorithms in 2008, Marissa Mayer—
today the president and CEO of Yahoo!—had the following to say:
We think that that’s the best way to do things. Our
users don’t need to understand how complicated the
technology and the development work that happens
behind this is. What they do need to understand is
that they can just go to a box, type what they want,
and get answers.27
Wulﬀ’s concern about the poliƟcs of auto-correct was also
proof posiƟve of the power algorithms carry in today’s world in
terms of changing the course of public opinion. By suggesƟng
230

parƟcular unﬂaƩering searches, a user with no preconceived
views about Beƫna Wulﬀ could be channeled down a parƟcular
route. In this way, algorithms aren’t just predicƟng user behavior
—they are helping dictate it. Consider, for example, Neƞlix’s
recommendaƟon algorithms, which I discussed in Chapter 4.
Neƞlix claims that 60 percent of its rentals are done according to
its algorithm’s suggesƟons, rather than users speciﬁcally
searching for a Ɵtle. If this is the case, are we to assume that the
algorithm simply guessed what users would next want to search
for, or that the users in fact made a certain selecƟon because an
algorithm had placed particular options in front of them?
Here the quesƟon becomes almost irrelevant. As the
sociologists William and Dorothy Thomas famously noted, “If men
deﬁne situaƟons as real, they are real in their consequences.” Or
to put it in the words Kevin Slavin memorably used during his TED
Talk, “How Algorithms Shape Our World,” the math involved in
such computer processes has transiƟoned from “something that
we extract and derive from the world, to something that actually
starts to shape it.”28
This can quite literally be the case. On September 6, 2008, an
algorithm came dangerously close to driving United Airlines’
parent company UAL out of business. The problem started when a
reporter for a news company called Income SecuriƟes Advisors
entered the words “bankruptcy 2008” in Google’s search bar and
hit “enter.” Google News immediately pointed the reporter to an
arƟcle from the South Florida Sun-SenƟnel, revealing that UAL
had ﬁled for bankruptcy protecƟon. The reporter—who worked
for a company responsible for feeding stories to the powerful
Bloomberg news service—immediately posted an alert to the
news network, lacking any further contextual informaƟon,
enƟtled “United Airlines ﬁles for Ch. 11 to cut costs.” The news
that the airline was seeking legal protecƟon for its debtors was
quickly read by thousands of inﬂuenƟal readers of Bloomberg’s
ﬁnancial news updates. The problem, as later came to light, was
231

that the news was not actually new, but referred to a bankruptcy
ﬁling from 2002, which the company had later successfully
emerged from, thanks to a 2006 reorganizaƟon. Because the
South Florida Sun-SenƟnel failed to list a date with its original
news bulletin, Google’s algorithms had assigned it one based upon
the September 2008 date that its web-crawling soŌware found
and indexed the arƟcle. As a result of the misinformaƟon, UAL
stock trading on the NASDAQ plummeted from $12.17 per share
to just $3.00, as panicked sellers unloaded 15 million shares within
a couple of hours.29
With algorithms carrying this kind of power, is it any real
wonder we have started to rely on them to tell us what is
important and what is not? In early 2009, a small town in France
called Eu decided to change its name to one made up of a longer
string of text—“Eu-en-Normandie” or “la Ville d’Eu”—because
Google searches for “Eu” generated too many results for the
European Union, colloquially known as the E.U. Consider also the
TwiƩer-originated 
concept 
of 
incorporaƟng 
hashtags
(#theformula) into messages. A September 2013 comedy sketch
on Late Night with Jimmy Fallon demonstrated how ill-suited the
whole hashtag phenomenon is for meaningful communicaƟon in
the real world. (“Check it out,” says one character, “I brought you
some 
cookies. 
#Homemade, 
#OatmealRaisin,
#ShowMeTheCookie.”) But of course the idea of hashtags isn’t to
beƩer explain ourselves to other people, but rather to allow us to
modify our speech in a way that makes it more easily recognized
and distributed by Twitter’s search algorithms.
When the content of our speech is not considered relevant, it
can prompt the same kind of reacƟon as a daƟng website’s
algorithms determining there is no one well matched with you.
During the Occupy Wall Street protests, many parƟcipants and
supporters used TwiƩer to coordinate, debate and publicize their
eﬀorts. Even though the protests gained considerable media
coverage, however, the term failed to “trend” according to
232

TwiƩer’s algorithms—referring to the terms TwiƩer shows on its
home page to indicate the most discussed topics, as indexed from
the 
250 
million 
tweets 
sent 
every 
day. 
Although
#OccupyWallStreet failed to trend, less pressing comedic memes
like #WhatYouFindInLadiesHandbags and #ThingsThirstyPeopleDo
seemingly had no diﬃculty showing up during that same Ɵme
span.
Although TwiƩer denied censorship, what is interesƟng about
the user outcry is what it says about the cultural role we imbue
algorithms with. “It’s a signal moment where the trending topics
on TwiƩer are read as being an indicaƟon of the importance of
diﬀerent sorts of social acƟons,” says Paul Dourish, professor of
informaƟcs at the Donald Bren School of InformaƟon and
Computer Sciences at the University of California, Irvine. Dourish
likens trending to what appearing on the front page of the New
York Times or the Guardian meant at a Ɵme when print
newspapers were at the height of their power. To put it another
way, TwiƩer’s trending algorithms are a technological
reimagining of the old adage about trees falling in the woods with
no one around to hear them. If a social movement like Occupy
Wall Street doesn’t register on so-called social media, has it really
taken place at all?
This is another task now aƩributed to algorithms. In the same
way that debates in the 20th century revolved around the idea of
journalisƟc objecƟvity at the heart of media freedom, so too in
the 21st century will algorithms become an increasingly
important part of the objecƟvity conversaƟon. ReappropriaƟng
free-speech 
advocate 
Alexander 
Meiklejohn’s 
famous
observaƟon, “What is essenƟal is not that everyone shall speak,
but that everything worth saying shall be said,” those in charge of
the most relied-upon algorithms are given the jobs of cultural
gatekeepers, tasked with deciding what is worth hearing and how
to deal with that material which is not. A decision like
algorithmically blocking those comments on a news site that have
233

received a high raƟo of negaƟve comments versus posiƟve ones
might sound like a neat way of countering spam messages, but it
also poses profound queries relating to freedom of speech.30
What can be troubling about these processes is that bias can be
easily hidden. In many cases, algorithms are tweaked on an
ongoing basis, while the interface of a parƟcular service might
remain the same. “OŌen it’s the illusion of neutrality, rather than
the reality of it,” says Harry Surden, associate professor at the
University of Colorado Law School, whom I introduced in Chapter
3. To put it another way, many of us assume that algorithms are
objective until they aren’t.
“It’s very diﬃcult to have an open and frank conversaƟon
about culture and what is valued versus what falls oﬀ the radar
screen when most people don’t have a clear sense of how
decisions are being made,” says scholar Ted Striphas. “That’s not
to say that algorithms are undemocraƟc, but it does raise
quesƟons when it comes to the relaƟonship between democracy
and culture.” Examples of this cultural unknowability can be seen
everywhere. In April 2009, more than 57,000 gay-friendly books
disappeared from Amazon’s sales ranking lists, based on their
erroneous categorizaƟon as “adult” Ɵtles. While that was a
temporary glitch, the so-called #amazonfail incident revealed
something that users had previously not been aware of: that the
algorithm used to establish the company’s sales ranks—previously
believed to be an objecƟve measurement tool—was purposely
designed to ignore books designated as adult Ɵtles. Similar types
of algorithmic demoƟon can be seen all over the place. YouTube
demotes sexually suggesƟve videos so that they do not appear on
“Most Viewed” pages or the “Top Favorite” home page
generated for new users. Each Ɵme Facebook changes its
EdgeRank algorithm—designed to ensure that the “most
interesƟng” content makes it to users’ News Feeds—there is an
existenƟal crisis whereby some content becomes more visible,
while others are rendered invisible.31
234

As was the case with Google’s “auto-correct” situaƟon,
someƟmes these decisions are not leŌ up to companies
themselves, but to governments. It is the same kind of algorithmic
tweaking that means that we will not see child pornography
appearing in the results of search engines, that dissident poliƟcal
speech doesn’t appear in China, and that websites are made
invisible in France if they promote Nazism. We might argue about
the various merits or demerits of parƟcular decisions, but simply
looking at the results we are presented with—without
quesƟoning the algorithms that have brought them to our
aƩenƟon—is, as scholar Tarleton Gillespie has noted, a liƩle like
taking in all the viewpoints at a public protest, while failing to
noƟce the number of speakers who have been stopped at the
front gate.32
235

Organize the World
Near the start of this conclusion, I asked whether everything
could be subject to algorithmizaƟon. The natural corollary to this
query is, of course, the quesƟon of whether everything should be
subject to algorithmizaƟon. To use this book’s Ɵtle, is there
anything that should not be made to subserve The Formula? That
is the real billion-dollar quesƟon—and it is one that is asked too
little.
In this book’s introducƟon, I quoted Bill Tancer, wriƟng in his
2009 book Click: What We Do Online and Why It MaƩers, about a
formula designed to mathemaƟcally pinpoint the most depressing
week of the year. As I noted, Tancer’s concern had nothing to do
with the fact that such a formula could possibly exist (that there
was such a thing as the quanƟﬁably most depressing week of the
year) and everything to do with the fact that he felt the wrong
formula had been chosen.33
The idea that there are easy answers to quesƟons like “Who
am I?,” or “Who should I fall in love with?,” or “What is art?” is an
appealing one in many ways. In Chapter 2, I introduced you to
Garth Sundem, the staƟsƟcian who created a remarkably
prescient formula designed to predict the breakup rate of
celebrity marriages. When I put the quesƟon to him of why such
formulas engage the general populace as they do, he gave an
answer that smacks more of religiosity than it does of Logical
PosiƟvism. “I think people like the idea that there are answers,”
he says. “I do silly equaƟons, based on quesƟons like whether you
should go talk to a girl in a bar. But the thought that there may
actually be an answer to things that don’t seem answerable is
extremely aƩracƟve.” Does he ﬁnd it frightening that we might
seek to quanƟfy everything, reducing it down to its most granular
levels—like the company Hunch I discussed in Chapter 1, which
claims to be able to answer any consumer-preference quesƟon
with 80 to 85 percent accuracy, based only on ﬁve data points?
236

“Personally, I think the ﬂip side is scarier,” Sundem counters. “I
think uncertainty is a lot more terrifying than the potenƟal for
mathematical certainty. While I was first coming up with formulas
at college, trying to mathemaƟcally determine whether we
should go to the library to get some work done, deep down in the
recesses of our dorky ids I think that what we were saying is that
life is uncertain and we were trying to make it more certain. I’m
not as disturbed by numbers providing answers as I am by the
potential that there might not be answers.”
What is it about the modern world that makes us demand easy
answers? Is it that we are naturally paƩern-seeking creatures, as
the staƟsƟcian Nate Silver argues in The Signal and the Noise? Or
is there something about the eﬀects of the march of technology
that demands the kind of answers only an algorithm can provide?
“[The algorithm does] seem to be a key metaphor for what
maƩers now in terms of organizing the world,” acknowledges
McKenzie Wark, a media theorist who has wriƩen about digital
technologies for the last 20 years. “If one thinks of algorithms as
processes which terminate and generate a result, there’s a
moment when the process ceases and you have your answer. If
something won’t terminate then it probably means that your
computer has gone wrong. There’s a sense that we [increasingly]
structure reality around processes that will yield results—that
we’ve embedded machine logic in the world.”
The idea of the black box is one that comes up a lot when
discussing algorithms, and it is one that Bruno Latour seizes upon
as a powerful metaphor in his work. The black box is, he notes, a
term used by cyberneƟcians whenever a piece of machinery or
else a set of commands is too complex. In its place, the black box
stands in as an opaque subsƟtute for a technology in which
nothing needs to be known other than inputs and outputs. Once
opened, it makes both the creators and the users confront the
subjecƟve biases and processes that have resulted in a certain
answer. Closed, it becomes the embodiment of objecƟvity: a
237

machine capable of spiƫng out binary “yes” or “no” answers
without further need of qualiﬁcaƟon. “Insofar as they consider all
the black boxes well sealed, people do not, any more than
scienƟsts, live in a world of ﬁcƟon, representaƟon, symbol,
approximaƟon, convenƟon,” Latour observes. “They are simply
right.” In this vein, we might also turn to Slavoj Žižek’s concepƟon
of the toilet bowl: a seemingly straighƞorward technological
mechanism through which excrement disappears from our reality
and enters another space we phenomenologically perceive to be
a messier, more chaotically primordial reality.34
It is possible to see some of this thinking in the work of Richard
Berk I proﬁled in Chapter 3. “It frees me up,” Berk said of his
future crime predicƟon algorithm. “I don’t care whether it makes
sense in any kind of causal way.” While Berk’s comments are
designed to get acƟonable informaƟon to predict future
criminality, one could argue that by black-boxing the inner
workings of the technology, something similar has taken place
with the underlying social dynamics. In other areas—parƟcularly
as relate to law—a reliance on algorithms might simply jusƟfy
exisƟng bias and lack of understanding, in the same way that the
“ﬁlter bubble” eﬀect described in Chapter 1 can result in some
people not being presented with certain pieces of informaƟon,
which may take the form of opportunities.
“It’s not just you and I who don’t understand how these
algorithms work—the engineers themselves don’t understand
them enƟrely,” says scholar Ted Striphas. “If you look at the
Neƞlix Prize, one of the things the people responsible for the
winning entries said over and over again was that their algorithms
worked, even though they couldn’t tell you why they worked.
They might understand how they work from the point of view of
mathemaƟcal principles, but that math is so complex that it is
impossible for a human being to truly follow. That troubles me to
some extent. The idea that we don’t know the world that we’re
creaƟng makes it very diﬃcult for us to operate ethically and
238

mindfully within it.”
239

How to Stay Human in the World of The Formula
One of the most disconcerƟng algorithms I came across during my
research was the so-called TruthTeller algorithm developed by
the Washington Post newspaper in 2012, to coincide with that
summer’s presidenƟal elecƟon season. Capable of scanning
through poliƟcal speeches in real Ɵme and informing us of when
we are being lied to, the TruthTeller is an uncomfortable
reminder of both our belief in algorithmic objecƟvity and our
desire for easy answers. In a breathless arƟcle, Geek.com
described it as “the most robust, automated way to tell whether
a poliƟcian is lying or not, even more [accurate] than a polygraph
test . . . because poliƟcians are so delusional they end up
genuinely believing their lies.” The algorithm works by using
speech recogniƟon technology developed by MicrosoŌ, which
converts audio signals into words, before handing the completed
transcript over to a matching algorithm to comb through and
compare alleged “facts” to a database of previously recorded,
proven facts.35
Imagine the potenƟal for manipulaƟon should such a
technology ever ascend beyond simple gimmickry to enjoy the
ubiquity of, for instance, automated spell-checking algorithms. If
such a tool was to be implemented within a future ediƟon of MS
Word or Google Docs, it is not inconceivable that users may one
day ﬁnish typing a document and hit a single buƩon—at which
point it is auto-checked for spelling, punctuaƟon, formaƫng and
truthfulness. Already there is widespread use of algorithms in
academia for siŌing through submiƩed work and pulling up
passages that may or may not be plagiarized. These will only
become more widespread as natural language processing
becomes more intuiƟve and able to move beyond simple passage
comparison to detailed content and idea analysis.
There is no one-size-ﬁts-all answer to how best to deal with
algorithms. In some cases, increased transparency would appear
240

to be the answer. Where algorithms are used to enforce laws, for
instance, releasing the source code to the general public would
both protect against the dangers of unchecked government
policy-making and make it possible to determine how speciﬁc
decisions have been reached. But this approach also wouldn’t
work in maƩers of naƟonal security, where revealing the inner
workings of speciﬁc black boxes would enable certain individuals
to “game” the system in such a way as to render the algorithm
useless.36
A not dissimilar paradox can be seen in what happened to
Google’s Flu Trends algorithm in 2013. Heralded as a
breakthrough development thanks to its ability to track the
spread of ﬂu through semanƟc analysis of user searches, Flu
Trends ran into an unexpected problem when it received so much
media aƩenƟon that its algorithm began to malfuncƟon.
Previously designed to look for searches like “medicine for cough
and fever” and assume that these users were sick, what Google
found instead was that people were typing in ﬂu-related searches
to look for informaƟon about Google’s own algorithm.
Experiencing spikes in its data, Google predicted near-epidemic
levels of flu, which then failed to materialize. The company wound
up admiƫng that while its algorithms might be on top of the ﬂu
problem, they were also “suscepƟble to heightened media
coverage.” “The lesson here is rich with irony,” wrote
InformaƟonWeek journalist Thomas Claburn when he reported
the story. “To eﬀecƟvely assess data from a public source, the
algorithm must remain private, or [else] someone will aƩempt to
introduce bias.”37
Beyond this there is the quesƟon of how we stay human in the
world of The Formula. AŌer all, if the original dream of hard AI
was to create a computer that could believably behave like a
human, then the uƟlitarian opposite of this is to somehow reduce
human acƟvity to the point where it is made as predictable as a
computer.
241

We can ﬁnd many cases where the algorithmizaƟon of these
profoundly human acƟviƟes risks losing what makes them special
in the ﬁrst place. As Jaron Lanier argues in his techno-skepƟcal
book, You Are Not a Gadget, “Being a person is not a pat formula,
but a quest, a mystery, a leap of faith.”38
But this is also easier said than done in a world increasingly
subject to algorithmic proceduring. So how to survive then?
Certainly, there is a minority currently engaged in what we might
term “algorithmical data jamming,” trying to develop tacƟcs to
obscure or evade algorithms’ aƩempts to know and categorize
them. But to do this means losing out on some of the most
valuable tools of the modern age. Since algorithms increasingly
deﬁne what is relevant, it also means stepping away from many
matters of public discourse.
Instead, I propose that users learn more about the world of
The Formula, since knowledge of these algorithmic processes is
going to be key to many of the important debates going forward
—in everything from human relaƟonships to law. For instance,
police must establish reasonable suspicion for a “stop and search”
to take place, by poinƟng to “speciﬁc and arƟculable facts which,
taken together with raƟonal inferences from those facts,
reasonably warrant that intrusion.” In this case, does “the
algorithm said so” (provided the algorithm is shown to work
eﬀecƟvely) provide enough probable cause to carry out such a
stoppage?39
Similarly, if human relaƟonships with algorithms are not “real,”
are they at least “real enough”? In 2013, a group of researchers
in Spain successfully coded a piece of soŌware designed to mimic
the language and aƫtude of a 14-year-old girl. Negobot—also
referred to as the “Virtual Lolita”—is designed to trap potenƟal
pedophiles in online chat rooms. StarƟng in a neutral mode that
talks with strangers about ordinary subjects, Negobot goes into
“game mode” when a stranger starts communicaƟng in innuendo
or with overt sexual overtones, trying to provoke the person on
242

the other end of the conversaƟon to agree to a meet-up. Should
such a technology be adopted by law enforcers, it will suggest
that feelings toward an algorithm are at least “real enough” to
warrant prosecuƟon. As Al Gore has said, “The ability to code and
understand the power of compuƟng is crucial to success in today’s
hyper-connected world.”40
Most important of all is asking quesƟons—and not expecƟng
simple answers. One of these quesƟons involves not just what
algorithms are doing to us—but what they are designed to do in
the ﬁrst place. This is a pressing quesƟon, and one that needs to
be asked parƟcularly in cases where a service is ostensibly free to
users. Words like “relevant” and “newsworthy” are loaded terms
that encourage (but oŌen fail to answer) the seemingly obvious
follow-up quesƟon: “relevant” and “newsworthy” to whom? In
the case of a company like Google, the answer is simple: to the
company’s shareholders, of course. Facebook’s algorithms can
similarly be viewed as a formula for maintaining and building your
friendship circle—but of course the reality is that Facebook’s
purpose isn’t to make you friends, but rather to moneƟze your
social graph through advertising.41
Hopefully, this quesƟoning process is starƟng to happen. A
number of researchers working with recommender systems have
told me how user expectaƟons have changed in recent years.
Where ﬁve or ten years ago, people would be happy with any
recommendaƟons, today an increasing number want to know
why these recommendaƟons have been made for them. With
asking why we are expected to take things at “interface value”
will come the ability to criƟque the conƟnued algorithmizaƟon of
everything. UlƟmately there are no catchall answers. Our lives
would look a lot diﬀerent—and, most likely, be far worse—
without algorithms. But that doesn’t mean we stop asking the
important questions.
And particularly when easy answers are seemingly involved.
243

A Note on Author Interviews
(Conducted 2012–2013)
NB: I was in the privileged posiƟon of speaking to a large number of individuals
while researching The Formula. Not everyone is menƟoned by name in the text,
but below is a list reﬂecƟng the individuals whose contribuƟons are worthy of
noting:
Vincent Dean Boyce, Steve Carter, KrisƟn Celello, John Cheney-Lippold, Noam
Chomsky, Danielle Citron, Kevin Conboy, Tom Copeman, Paul Dourish, Robert
Epstein, Konrad Feldman, Lee Goldman, Graeme Gordon, Jonathan GoƩschall,
Guy HalŌeck, David Hursh, John Kelly, Alexis Kirke, Alec Levenson, Jiwei Li,
Benjamin Liu, Sean Malinowski, Lev Manovich, Nick Meaney, Vivienne Ming,
George Mohler, John Parr (UK representaƟve for PCM), Giles Pavey, Richard
Posner, Ivan Poupyrev, Stephen Ramsay, Emily Riehl, Anna Ronkainen, Adam
Sadilek, MaƩhew Salganik, Lior Shamir, Larry Smarr, Mark Smolinski, CelesƟno
Soddu, Ted Striphas, Garth Sundem, Harry Surden, Francisco Vico, McKenzie
Wark, Neil Clark Warren, Robert Wert.
244

Notes
An Explanation of the Title, and Other Cyberbole
1 Tancer, Bill. Click: What We Do Online and Why It MaƩers (London:
HarperCollins, 2009).
Chapter 1: The Quantified Selves
1 technologyreview.com/featuredstory/426968/the-patient-of-the-future/.
2 Smarr, Larry. “Towards Digitally Enabled Genomic Medicine: A 10-Year
DetecƟve 
Story 
of 
QuanƟfying 
My 
Body.” 
September 
2011.
lsmarr.calit2.net/repository/092811_Special_Letter,_Smarr.final.pdf.
Gorbis, 
Marina. The Nature of the Future: Dispatches from the
Socialstructed World (New York: Free Press, 2013).
3 Bowden, Mark. “The Measured Man.” The AtlanƟc, July 13, 2012.
theatlantic.com/magazine/archive/2012/07/the-measured-
man/309018/.
4 quantifiedself.com.
5 750words.com.
6 Nafus, Dawn, and Jamie Sherman. “This One Does Not Go Up to Eleven: The
QuanƟﬁed Self Movement as an Alternate Big Data PracƟce” (Review
draŌ). 
April 
2013. 
quanƟﬁedself.com/wp-
content/uploads/2013/04/NafusShermanQSDraft.pdf.
7 Friedman, Ted. Electric Dreams: Computers in American Culture (New York:
New York University Press, 2005).
8 media.mit.edu/wearables/.
9 Copeland, Douglas. GeneraƟon X: Tales for an Accelerated Culture (New York:
St. Martin’s Press, 1991).
10 James, William. The Principles of Psychology, Vol. 1 (New York: Henry Holt,
1890).
11 Cockerton, Paul. “Tesco Using Minority Report–Style Face Tracking
Technology So Ads on Screens Can Be Tailored.” Irish Mirror, November 4,
2013. 
irishmirror.ie/news/tesco-using-minority-report-style-face-
2674367.
12 Toffler, Alvin. The Third Wave (New York: Morrow, 1980).
Elias, Norbert. The Civilizing Process (New York: Urizen Books, 1978).
245

13 This wave metaphor was not, in itself, new: the German sociologist Norbert
Elias had referred to “a wave of advancing integraƟon over several
centuries” in his book The Civilizing Process, as had other writers over the
previous century.
14 Richtel, MaƩ. “How Big Data Is Playing Recruiter for Specialized Workers.”
New 
York 
Times, 
April 
27, 
2013.
nytimes.com/2013/04/28/technology/how-big-data-is-playing-recruiter-
for-specialized-workers.html?_r=0.
15 Kwoh, Leslie. “Facebook Proﬁles Found to Predict Job Performance.” Wall
Street 
Journal, 
February 
21, 
2012.
online.wsj.com/news/articles/SB10001424052970204909104577235474
16 Bulmer, Michael. Francis Galton: Pioneer of Heredity and Biometry
(Baltimore: Johns Hopkins University Press, 2003).
17 Pearson, Karl. The Life, LeƩers and Labours of Francis Galton (Cambridge, UK:
Cambridge University Press, between 1914 and 1930).
18 Galton, Francis. Statistical Inquiries into the Efficacy of Prayer (Melbourne: H.
Thomas, Printer, between 1872 and 1880).
19 Gould, Stephen Jay. The Mismeasure of Man (New York: Norton, 1981).
20 Dormehl, Luke. “This Algorithm Can Tell Your Life Story Through TwiƩer.”
Fast Company, November 4, 2013. fastcolabs.com/3021091/this-
algorithm-can-tell-your-life-story-through-twitter.
21 Isaacson, Walter. Steve Jobs (New York: Simon & Schuster, 2011).
22 Kosinskia, Michal, David SƟllwella, and Thore Graepelb. “Private Traits and
AƩributes Are Predictable from Digital Records of Human Behavior.”
PNAS, vol. 110, no. 15, April 9, 2013. pnas.org/content/110/15/5802.full.
23 Larsen, Noah. “Bloggers Reveal PersonaliƟes with Word, Phrase Choice.”
Colorado 
Arts 
& 
Sciences 
Magazine, 
January 
2011.
artsandsciences.colorado.edu/magazine/2011/01/bloggers-word-choice-
bares-their-personalities/.
24 Markham, AnneƩe. “The Algorithmic Self: Layered Accounts of Life and
IdenƟty in the 21st Century.” Selected Papers of Internet Research, 14.0,
2013. spir.aoir.org/index.php/spir/article/download/891/466.
25 Streiƞeld, David. “Teacher Knows If You’ve Done the E-Reading.” New York
Times, April 8, 2013. nyƟmes.com/2013/04/09/technology/coursesmart-
e-textbooks-track-students-progress-for-teachers.html?hp&_r=0.
26 Levy, Steven. In the Plex: How Google Thinks, Works, and Shapes Our Lives
(New York: Simon & Schuster, 2011).
27 Manjoo, Farhad. “The Happiness Machine.” Slate, January 21, 2013.
slate.com/articles/technology/technology/2013/01/google_people_opera
246

28 Taylor, Frederick. The Principles of ScienƟﬁc Management (New York: Norton,
1967, 1947).
29 Hogge, Becky. Barefoot into Cyberspace: Adventures in Search of Techno-
Utopia (Saffron Walden, UK: Barefoot, 2011).
30 O’Connor, Sarah. “Amazon’s Human Robots: They Trek 15 Miles a Day
Around a Warehouse, Their Every Move Dictated by Computers Checking
Their Work. Is This the Future of the BriƟsh Workplace?” Daily Mail,
February 28, 2013. dailymail.co.uk/news/arƟcle-2286227/Amazons-
human-robots-Is-future-British-workplace.html.
31 O’Connor, Sarah. “Amazon Unpacked.” FT Magazine, February 8, 2013.
ft.com/cms/s/2/ed6a985c-70bd-11e2-85d0-00144feab49a.html#slide0.
32 Brownlee, John. “Think Your Oﬃce Is Soulless? Check Out This Amazon
Fulﬁllment 
Center.” Fast 
Company, 
July 
1, 
2013.
fastcodesign.com/1672939/think-your-office-is-soulless-check-out-this-
amazon-fulfillment-center#disqus_thread.
33 Rawlinson, Kevin. “Tesco Accused of Using Electronic Armbands to Monitor
Its 
Staﬀ.” Independent, 
February 
13, 
2013.
independent.co.uk/news/business/news/tesco-accused-of-using-
electronic-armbands-to-monitor-its-staff-8493952.html.
34 This version of the quote was reprinted in a 1960 Time magazine arƟcle.
Other retellings have Churchill say, “We shape our dwellings, and
afterwards our dwellings shape us.”
35 Dormehl, Luke. “Why We Need ‘Decimated Reality’ Aggregators.” Fast
Company, June 21, 2013. Available at: fastcolabs.com/3013382/why-we-
need-decimated-reality-aggregators.
36 Pariser, Eli. The Filter Bubble: What the Internet Is Hiding from You (New York:
Penguin Press, 2011).
37 Graham, Stephen, and Simon Marvin. Splintering Urbanism: Networked
Infrastructures, Technological Mobilities and the Urban Condition (London;
New York: Routledge, 2001).
38 Graham, Stephen. “SoŌware-Sorted Geographies.” Progress in Human
Geography, vol. 29, no. 5, October 2005. dro.dur.ac.uk/194/1/194.pdf.
39 Conway, David. “Dynamic Pricing of Electronic Content” patent. ApplicaƟon
no. 12973121. Filed December 20, 2010. paƞt.uspto.gov/netacgi/ph-
Parser?
Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrch
40 Wuyts, Ann. “Inferring Mood from Your Smartphone RouƟne.” Blog post, July
31, 
2013. 
jini.co/blog/2013/07/infering-mood-from-your-smartphone-
routine/.
41 Machleit, Karen, and Susan Powell Mantel. “EmoƟonal Response and
247

Shopping SaƟsfacƟon: ModeraƟng Eﬀects of Shopper AƩribuƟons.”
Journal of Business Research, vol. 54, no. 2, November 2001.
42 Heﬀernan, Virginia. “Amazon’s Prime Suspect.” New York Times, August 6,
2010. nytimes.com/2010/08/08/magazine/08FOB-medium-t.html?_r=0.
43 Hansen, Mark. “DigiƟzing the Racialized Body, or The PoliƟcs of Universal
Address.” SubStance, vol. 33, no. 2, 2004.
44 Deleuze, Gilles, and Félix GuaƩari. A Thousand Plateaus: Capitalism and
Schizophrenia (Minneapolis: University of Minnesota Press, 1987).
45 Deleuze, Gilles. “Postscript on the SocieƟes of Control.” October, vol. 59,
Winter 1992.
46 Poole, Steven. “The Digital PanopƟcon.” New Statesman, May 29, 2013.
newstatesman.com/sci-tech/sci-tech/2013/05/are-you-ready-era-big-
data.
47 Anderson, Chris. The Long Tail (London: Random House Business, 2006).
48 Ellul, Jacques. The Technological Society (New York: Knopf, 1964).
49 Turner, Fred. From Counterculture to Cyberculture: Stewart Brand, the Whole
Earth Network, and the Rise of Digital Utopianism (Chicago: University of
Chicago Press, 2006).
50 Turkle, Sherry. Life on Screen: IdenƟty in the Age of the Internet (New York:
Simon & Schuster, 1995).
51 Singer, Natasha. “When Your Data Wanders to Places You’ve Never Been.”
New 
York 
Times, 
April 
27, 
2013.
nytimes.com/2013/04/28/technology/personal-data-takes-a-winding-
path-into-marketers-hands.html.
52 Sussman, Warren. Culture as History: The Transformation of American Society
in the Twentieth Century (New York: Pantheon Books, 1984).
Chapter 2: The Match & the Spark
1 Gale, David, and Lloyd Shapley. “College Admissions and the Stability of
Marriage.” American Mathematical Monthly, vol. 69, 1962.
2 Sundem, Garth, and John Tierney. “From Tinseltown to Splitsville: Just Do the
Math.” New York Times, September 19, 2006.
3 Stendhal. On Love (London: Penguin, 1975).
4 Brazier, David. Love and Its Disappointment: The Meaning of Life, Therapy and
Art (Winchester, UK: O Books, 2009).
5 Mulvey, Laura. “Visual Pleasure and NarraƟve Cinema,” in Visual and Other
Pleasures (London: Macmillan, 1989).
6 Nietzsche, Friedrich. Thus Spoke Zarathustra (New York: Barnes & Noble
248

Classics, 2007).
7 Woods, Judith. “Internet DaƟng: We Just Clicked.” Telegraph, March 23,
2012. 
telegraph.co.uk/women/sex/online-daƟng/9160622/Internet-
dating-We-just-clicked.html.
8 Moskowitz, Eva. In Therapy We Trust: America’s Obsession with Self-Fulﬁllment
(Baltimore: Johns Hopkins University Press, 2001).
9 Ibid.
10 Slater, Dan. Love in the Time of Algorithms: What Technology Does to Meeting
and Mating (New York: Current, 2013).
11 The ﬁrst personal ad on record was published in the Athenian Mercury in
1692, around 50 years aŌer the start of the modern newspaper. A sample
adverƟsement—published in 1777—was wriƩen by a young lady seeking
“A man of fashion, honour, and senƟment, blended with good nature, and
noble spirit, such as one would chuse [sic] for her guardian and
protector.”
12 E-mail exchange with Jaime Rupert, eHarmony’s director of corporate
communications. December 10, 2013.
13 Kerckhoﬀ, Alan. “PaƩerns of Homogamy and the Field of Eligibles.”  Social
Forces, vol. 42, no. 3, 1964.
14 Finkel, Eli, Paul Eastwick, Benjamin Karney, Harry Reis and Susan Sprecher.
“Online DaƟng: A CriƟcal Analysis from the PerspecƟve of Psychological
Science.” Psychological Science in the Public Interest, vol. 13, no. 3, 2012.
faculty.wcas.northwestern.edu/eli-
finkel/documents/2012_FinkelEastwickKarneyReisSprecher_PSPI.pdf.
15 okcupid.com/about.
16 genepartner.com/index.php/aboutgenepartner.
17 itunes.apple.com/us/app/find-your-facemate/id703849612?mt=8.
18 Rogers, Abby. “Models, Cheaters and Geeks: How 15 Niche DaƟng Websites
Are Helping All Sorts of People Find Love.” Business Insider, March 15,
2012. 
businessinsider.com/15-niche-daƟng-websites-2012-3?
op=1#ixzz2nBFQ44r3.
19 Pinker, Steven. How the Mind Works (New York: Norton, 1997).
20 Bauman, Zygmunt. Liquid Love: On the Frailty of Human Bonds (Cambridge,
UK: Polity Press).
21 Schwartz, Barry. The Paradox of Choice: Why More Is Less (New York: Ecco,
2004).
22 McDonald, ChrisƟe. The ProusƟan Fabric: AssociaƟons of Memory  (Lincoln:
University of Nebraska Press, 1991).
23 Webb, Amy. “Why Data Is the Secret to Successful DaƟng.” Guardian,
249

January 28, 2013. guardian.co.uk/news/datablog/2013/jan/28/why-data-
secret-successul-dating.
24 Shteyngart, Gary. Super Sad True Love Story (New York: Random House,
2010).
25 Pettman, Dominic. Look at the Bunny: Totem, Taboo, Technology (Ropley, UK:
Zero, 2013).
26 De Botton, Alain. On Love (New York: Atlantic Monthly Press, 1993).
27 Iwatani, Yukari. “Love: Japanese Style.” Wired, June 11, 1998.
wired.com/culture/lifestyle/news/1998/06/12899.
28 GoƩlieb, Jenna, and Jill Lawless. “New App Helps Icelanders Avoid
Accidental 
Incest.” 
Associated 
Press, 
April 
18, 
2013.
salon.com/2013/04/18/new_app_helps_icelanders_avoid_accidental_inc
29 Lanier, Jaron. Who Owns the Future? (New York: Simon & Schuster, 2013).
30 Hill, Kashmir. “SceneTap Wants to One Day Tell You the Weights, Heights,
Races and Income Levels of the Crowd at Every Bar.” Forbes, September
25, 2012. forbes.com/sites/kashmirhill/2012/09/25/scenetap-wants-to-
one-day-use-weight-height-race-and-income-to-help-you-decide-which-
bar-to-go-to/.
31 bedposted.com.
32 Grothaus, Michael. “About 9% of You Would Have Sex with a Robot.” Fast
Company, August 7, 2013. fastcolabs.com/3013217/the-forerunners-of-
future-sexbots-now.
33 Jastrow, Robert. The Enchanted Loom: Mind in the Universe (New York: Simon
and Schuster, 1981).
34 deadsoci.al/index.php.
35 ifidie.net/.
36 Coldwell, Will. “Why Death Is Not the End of Your Social Media Life.”
Guardian, 
February 
18, 
2013.
theguardian.com/media/shortcuts/2013/feb/18/death-social-media-
liveson-deadsocial.
37 Saletan, William. “Night of the Living Dad.” Slate, January 12, 2009.
slate.com/articles/health_and_science/human_nature/2009/01/night_of_
38 Merleau-Ponty, Maurice. Phenomenology of PercepƟon: An IntroducƟon
(London: Routledge, 2002).
39 The ﬁrst conversaƟon I had with my freshly downloaded “demo” Kari ended
on a disƟnctly gold-digger note when my would-be virtual girlfriend
observed that our trial Ɵme together had expired and opined, “I am so
sad. Register me, so that we can keep talking. I love you.”
40 Not coincidentally, the world’s first chatterbot—conceived by MIT computer
250

scientist Joseph Weizenbaum—was named ELIZA.
41 Shakespeare, Geoﬀ. “The Six Best Girlfriend SubsƟtutes Technology Has to
Oﬀer.” Spike, August 13, 2010. spike.com/arƟcles/myqbz4/the-six-best-
girlfriend-substitutes-technology-has-to-offer.
42 When I asked Sergio Parada about the impediment of being unable to
consummate a relaƟonship with Kari—thereby making her a less than
ideal lover in some ways—he told me about one user who had bought a
sex doll that they had then implanted with a microcomputer, running both
Kari and voice recogniƟon soŌware. “He had [the doll] siƫng on the
couch while he was able to talk to her like a real person,” Parada says.
Recently Parada was approached by a technology company interested in
developing “some kind of gadget that works on sucƟon. It plugs into the
USB port and comes with a soŌware development kit.” The idea, he says,
“was to have Kari interact with this gadget that you slip over your
member.”
43 Richie, Donald, and Roy Garner. The Image Factory: Fads and Fashions in
Japan (London: Reaktion, 2003).
44 Levy, David. Love and Sex with Robots (New York: HarperPerennial, 2008).
Chapter 3: Do Algorithms Dream of Electric Laws?
1 Hays, Constance. “What Wal-Mart Knows About Customers’ Habits.” New
York 
Times, 
November 
14, 
2004.
nytimes.com/2004/11/14/business/yourmoney/14wal.html.
2 Beck, Charlie, and Colleen McCue. “PredicƟve Policing: What Can We Learn
from Walmart and Amazon about FighƟng Crime in a Recession?” Police
Chief, November 2009. policechiefmagazine.org/magazine/index.cfm?
fuseaction=display_arch&article_id=1942&issue_id=112009.
3 BraƩon, William, and Peter Knobler. Turnaround: How America’s Top Cop
Reversed the Crime Epidemic (New York: Random House, 1998).
4 Ferguson, Andrew. “PredicƟve Policing and Reasonable Suspicion.” May 2,
2012. law.emory.edu/fileadmin/journals/elj/62/62.2/Ferguson.pdf.
5 
thenextweb.com/uk/2011/10/14/us-super-cop-and-new-uk-adviser-talks-
data-driven-policing-and-putting-cops-on-the-dots/.
6 
pandodaily.com/2012/07/09/predpol-brings-big-data-to-law-enforcement-
with-1-3-million-round/.
7 Infante, Francesca. “Met’s Minority Report: They Use Computer Algorithms to
Predict Where Crime Will Happen.” Daily Mail, September 29, 2013.
dailymail.co.uk/news/article-2437206/Police-tackle-burglars-muggers-
using-Minority-Report-style-technology-tackle-future-crime.html.
251

8 “PredicƟve Policing Comes to the UK.” Statewatch, March 5, 2013.
statewatch.org/news/2013/mar/01uk-predictive-policing.htm.
9 Comte, Auguste, and Ronald Fletcher. The Crisis of Industrial CivilizaƟon: The
Early Essays of Auguste Comte (London: Heinemann Educational, 1974).
10 Bohm, Robert. A Primer on Crime and Delinquency Theory (Belmont, Calif.:
Wadsworth, 2001).
11 Hacking, Ian. The Taming of Chance (Cambridge, UK; New York: Cambridge
University Press, 1990).
12 RaŌer, Nicole. The Origins of Criminology: A Reader (New York: Routledge,
2009).
13 Mlodinow, Leonard. The Drunkard’s Walk: How Randomness Rules Our Lives
(New York: Pantheon Books, 2008).
14 Belt, Elmer, and Louise Darling. Elmer Belt Florence NighƟngale CollecƟon
(San Francisco: Internet Archive, 2009).
15 Danzigera, Shai, Jonathan Levav and Liora Avnaim-Pesso. “Extraneous
Factors in Judicial Decisions.” PNAS, vol. 108, no. 17, April 26, 2011.
pnas.org/content/108/17/6889.full.
16 Markoﬀ, John. “Armies of Expensive Lawyers, Replaced by Cheaper
S o Ō w a r e . ” New 
York 
Times, 
March 
4, 
2011.
nytimes.com/2011/03/05/science/05legal.html.
17 Lev-Ram, Michal. “Apple v. Samsung: (Patent) Trial of the Century.” Fortune,
January 22, 2013. tech.fortune.cnn.com/2013/01/22/apple-samsung-
patent-lawsuit/.
18 Dannen, Chris. “The Amazing Forensic Tech Behind the Next Apple, Samsung
Legal Dust-Up (and How to Hack It).” Fast Company, December 6, 2012.
fastcolabs.com/3003732/amazing-forensic-tech-behind-next-apple-
samsung-legal-dust-and-how-hack-it.
19 Christensen, Clayton. Innovator’s Dilemma: When New Technologies Cause
Great Firms to Fail (Boston: Harvard Business School Press, 1997).
20 Dawkins, Richard. The Selﬁsh Gene (Oxford, UK; New York: Oxford University
Press, 1989).
21 Consider, for instance, the way in which divorce cases are presumed—in
their very “Smith v. Smith” language—to be adversarial, even when this
might not be the case at all.
22 Fisher, Daniel. “Silicon Valley Sees Gold in Internet Legal Services.” Forbes,
October 5, 2011. forbes.com/sites/danielﬁsher/2011/10/05/silicon-
valley-sees-gold-in-internet-legal-services/.
23 Casserly, Meghan. “Can This Y-Combinator Startup’s Technology Keep
Couples 
Out 
of 
Divorce 
Court?” Forbes, 
April 
10, 
2013.
252

forbes.com/sites/meghancasserly/2013/04/10/wevorce-y-combinator-
technology-divorce-court/.
24 “AŌer Beta Period, Wevorce SoŌware for Making Every Divorce Amicable Is
Now 
Generally 
Available 
NaƟonwide.” 
May 
22, 
2013.
marketwired.com/press-release/after-beta-period-wevorce-software-
making-every-divorce-amicable-is-now-generally-available-1793675.htm.
25 Turkle, Sherry. Life on Screen: IdenƟty in the Age of the Internet (New York:
Simon & Schuster, 1995).
26 Lohr, Steve. “Computers That See You and Keep Watch Over You.” New York
Times, January 1, 2011. nyƟmes.com/2011/01/02/science/02see.html?
pagewanted=all.
27 Hildebrandt, Mireille. “A Vision of Ambient Law,” in RegulaƟng Technologies
(Oxford, UK: Hart, 2008).
28 Massey, Ray. “The Car That Stops You Drink-Driving.” Daily Mail, August 4,
2007. 
dailymail.co.uk/news/arƟcle-473040/The-car-stops-drink-
driving.html.
29 Winner, Langdon. “Do ArƟfacts Have PoliƟcs?” Daedalus, vol. 109, no. 1,
Winter 1980.
30 Caro, Robert. The Power Broker: Robert Moses and the Fall of New York (New
York: Knopf, 1974).
31 Chivers, Tom. “The Story of Google Maps.” Telegraph, June 4, 2013.
telegraph.co.uk/technology/google/10090014/The-story-of-Google-
Maps.html.
32 Lardinois, Frederic. “The Next FronƟer for Google Maps Is PersonalizaƟon.”
TechCrunch, February 1, 2013. techcrunch.com/2013/02/01/the-next-
frontier-for-google-maps-is-personalization/.
33 Seefeld, Bernhard. “Meet the New Google Maps: A Map for Every Person and
Place.” 
Google 
Maps, 
May 
15, 
2013. 
google-
latlong.blogspot.co.uk/2013/05/meet-new-google-maps-map-for-
every.html.
34 A similar idea was proposed at around the same Ɵme by Madeline Akrich in
her essay “The De-ScripƟon of Technical Objects,” which appears in the
b o o k Shaping Technology/Building Society: Studies in Sociotechnical
Change by Wiebe Bijker and John Law.
35 Latour, Bruno. “On Technical MediaƟon.” Common Knowledge, vol. 3, no. 2,
1994.
36 To extend this argument to its determinisƟc extreme, we might turn to Karl
Marx and his asserƟon in The Poverty of Philosophy: “The hand-mill gives
you society with the feudal lord; the steam-mill society with the industrial
capitalist.” I take more of a social construcƟonist perspecƟve, seeing
253

technological development as the interplay of inventors, entrepreneurs,
customers and social circumstance.
37 Thaler, Richard, and Cass Sunstein. Nudge: Improving Decisions about
Health, Wealth, and Happiness (New Haven, Conn.: Yale University Press,
2008).
38 Brownsword, Roger. “What the World Needs Now: Techno-RegulaƟon,
Human Rights and Human Dignity,” in Global Governance and the Quest for
Justice. Vol. 4: Human Rights. (Oxford, UK: Hart, 2004).
39 Conly, Sarah. Against Autonomy (Cambridge, UK: Cambridge University
Press, 2013).
40 Lessig, Lawrence. Code: Version 2.0 (New York: Basic Books, 2006).
41 Holmes, Oliver Wendell. The Mind and Faith of JusƟce Holmes: His Speeches,
Essays, Letters, and Judicial Opinions (New York: Modern Library, 1943).
42 Shay, Lisa, Woodrow Hartzog, John Nelson and Gregory ConƟ. “Do Robots
Dream of Electric Laws: An Experiment in the Law as Algorithm,” March 29,
2013. rumint.org/gregconti/publications/201303_AlgoLaw.pdf.
43 Reiser, Stanley. Medicine and the Reign of Technology (Cambridge, UK; New
York: Cambridge University Press, 1978).
44 Gusﬁeld, Joseph. The Culture of Public Problems: Drinking-Driving and the
Symbolic Order (Chicago: University of Chicago Press, 1981).
45 “Google’s Self-Driving Cars Are Safer Than Human Drivers.” Macworld,
August 8, 2012. macworld.com.au/news/googles-self-driving-cars-are-
safer-than-human-drivers-67261/#.Uh2-DLyE5eo.
46 Owen, Glen. “Britain Fights EU’s ‘Big Brother’ Bid to Fit Every Car with Speed
Limiter.” Daily Mail, August 31, 2013. dailymail.co.uk/news/arƟcle-
2408012/Britain-fights-EUs-Big-Brother-bid-fit-car-speed-limiter.html.
47 Moskvitch, KaƟa, and Richard Fisher. “Penal Code.” New ScienƟst,
September 7, 2013.
48 Hook, P. “Police Systems to AutomaƟcally Detect Crime.” CCTV Today,
March 19–20, 2001.
49 Graham-Rowe, Duncan. “Warning! Strange Behaviour.” New ScienƟst,
December 
11, 
1999. 
newscienƟst.com/arƟcle/mg16422164.700-
warning-strange-behaviour.html.
50 Star, Susan. “The Ethnography of Infrastructure.” American Behavioral
Scientist, vol. 43, no. 3, November 1999. bscw.wineme.ĩ5.uni-
siegen.de/pub/nj_bscw.cgi/d759204/11_2_Star_EthnographyOfInfrastruct
51 Irons, Meghan. “Caught in a Dragnet.” Boston Globe, July 17, 2011.
boston.com/yourtown/natick/articles/2011/07/17/man_sues_registry_aft
page=1.
254

52 
calegaladvocates.org/news/arƟcle.132896-
COMPUTER_GLITCH_LEAVES_CALIFORNIAS_NEEDIEST_MEDICARE_RECIPIENTS_
53 sfexaminer.com/sanfrancisco/medicare-clients-sue-state-over-computer-
flub/Content?oid=2155092.
54 Garvey, Meghan. “Net to Snag Deadbeats Also Snares Innocent.” Los Angeles
Times, April 12, 1998. articles.latimes.com/1998/apr/12/local/me-38538.
55 United States Government Accountability Oﬃce. “Terrorist Watch List
Screening: Eﬀorts to Help Reduce Adverse Eﬀects on the Public—Report to
Congressional Requesters.” 2006.
56 Schneier, Bruce. “Why Data Mining Won’t Stop Terror.” Wired, March 9,
2006.
wired.com/politics/security/commentary/securitymatters/2006/03/7035
57 Graham, S. (2005) “SoŌware-Sorted Geographies,” Progress in Human
Geography.
58 Sweeney, Latanya. “Google Ads, Black Names and White Names, Racial
DiscriminaƟon, and Click AdverƟsing.” ACM Queue, vol. 11, no. 3, March
2013. queue.acm.org/detail.cfm?id=2460278.
59 Ananny, Mike. “The Curious ConnecƟon Between Apps for Gay Men and Sex
O ﬀe n d e r s . ” The 
AtlanƟc, 
April 
14, 
2011.
theatlantic.com/technology/archive/2011/04/the-curious-connection-
between-apps-for-gay-men-and-sex-offenders/237340/.
60 Citron, Danielle. “Technological Due Process.” Washington University Law
Review, vol. 85, 2007.
61 Citron, Danielle. “Technological Due Process” (Luncheon Video/Audio).
January 
15, 
2008.
cyber.law.harvard.edu/interactive/events/luncheon/2008/01/citron.
62 Posner, Richard. “The Role of the Judge in the Twenty-First Century.” Boston
University Law Review, vol. 86, 2006.
63 Radin, Max. “The Theory of Judicial Decision: Or How Judges Think.” 1925.
64 Hutcheson Jr., Joseph C. “The Judgment IntuiƟve: The FuncƟon of the
‘Hunch’ in Judicial Decision.” 1929.
65 Ruger, Theodore, Pauline Kim, Andrew MarƟn and Kevin Quinn. “The
Supreme Court ForecasƟng Project: Legal and PoliƟcal Science
Approaches to PredicƟng Supreme Court Decisionmaking.” Columbia Law
Review, vol. 104, no. 4, May 2004.
66 Holmes Jr., Oliver Wendell. “The Path of the Law.” 1897.
Chapter 4: The Machine That Made Art
255

1 Goldman, William. Adventures in the Screen Trade: A Personal View of
Hollywood and Screenwriting (New York: Warner Books, 1983).
2 Medavoy, Mike, and Josh Young. You’re Only as Good as Your Next One: 100
Great Films, 100 Good Films, and 100 for Which I Should Be Shot (New York:
Pocket Books, 2002).
3 Johnston, Rich. “Review: Avatar—The Most Expensive American Film Ever . . .
and Possibly the Most AnƟ-American One Too.” Bleeding Cool, December
11, 
2009. 
bleedingcool.com/2009/12/11/review-avatar-the-most-
expensive-american-film-ever-and-the-most-anti-american-one-too/.
4 McBride, Joseph. Steven Spielberg: A Biography (New York: Simon & Schuster,
1997).
5 An exchange between Spielberg and his dad, remembered by Arnold
Spielberg and recorded in Joseph McBride’s acclaimed biography of the
director, went like this: “I said, ‘Steve, you’ve goƩa study math.’ He said,
‘I don’t like it.’ He’d ask me to do his chemistry for him. And he would
never even do the damn chemistry lab, he would just come home and say,
‘Dad, I’ve goƩa prepare this experiment.’ I’d say, ‘You don’t have any data
there. How am I supposed to tell you what you’ve done?’ So I’d try to
reconstruct the experiment for him, I’d come down with some answers.
He’d come back [from school] and say, ‘Jesus, Dad, you flunked!’”
6 Keegan, Rebecca. “The Legend of Will Smith.” Time, November 29, 2007.
content.time.com/time/magazine/article/0,9171,1689234,00.html.
7 Eells, Josh. “Jennifer Lawrence: America’s Kick-Ass Sweetheart.” Rolling
Stone, April 12, 2012. rollingstone.com/movies/news/cover-story-excerpt-
jennifer-lawrence-20120328.
8 Whatever it proves, this moment has proven a divisive one among criƟcs.
While some love it, others view it as the weakest moment of the ﬁlm. In
an arƟcle for the New Republic, criƟc David Thompson referred to it as
“appalling.” Thompson, David. “Schindler’s Girl in the Red Coat Speaks
O u
t
. ” New 
Republic, 
March 
7, 
2013.
newrepublic.com/article/112598/schindlers-girl-red-coat-speaks-out.
9 Salganik, MaƩhew, Peter Dodds and Duncan WaƩs. “Experimental Study of
Inequality and Unpredictability in an ArƟﬁcial Cultural Market.” Social
Psychology 
Quarterly, 
vol. 
71, 
no. 
4, 
December 
2008.
princeton.edu/~mjs3/salganik_dodds_watts06_full.pdf.
10 Huntzicker, William. The Popular Press, 1833–1865 (Westport, Conn.;
London: Greenwood Press, 1999).
11 Rutsky, R. L. High Techne¯: Art and Technology from the Machine AestheƟc to
the Post-human (Minneapolis: University of Minnesota Press, 1999).
12 Manovich, Lev. The Language of New Media (Cambridge, Mass.: MIT Press,
256

2002).
13 For anyone interested, the formula he came up with was S(pi + Pii + Piii . . . P)
Y = T, where S equals the sum of the principles (P), Y equals intuiƟon, and T
equals artistic creation.
14 Benjamin, Walter. The Work of Art in the Age of Mechanical ReproducƟon
(London: Penguin, 2008).
15 Clark, Liat. “2D Photos Translated into 3D-Printed Translucent Artworks.”
Wired, May 23, 2013. wired.co.uk/news/archive/2013-05/23/3d-printed-
touch-photos.
16 Kim, Seung-Chan, Ali Israr, and Ivan Poupyrev. “TacƟle Rendering of 3D
Features on Touch Surfaces.” Proceedings of the 26th Annual ACM
Symposium on User Interface SoŌware and Technology—UIST 2013 (2013):
531–38. disneyresearch.com/wp-content/uploads/uist-2013-final.pdf.
17 Poupyrev, Ivan. “Researchers Develop Algorithm for Rendering 3-D TacƟle
Features on Touch Surfaces.” October 7, 2013. phys.org/news/2013-10-
algorithm-d-tactile-features-surfaces.html.
18 The term “info-aestheƟcs” was ﬁrst coined in the 1950s by Max Bense, a
German philosopher with a parƟcular interest in art, technology and
science.
19 Bishop, Todd. “Bill Gates, Nathan Myhrvold Have Another Wild Idea:
AutomaƟcally GeneraƟng Video from Text.” Geekwire, August 13, 2013.
geekwire.com/2013/gates-myhrvold-crazy-idea-autogenerating-video-
text/.
20 Ramsay, Stephen. Reading Machines: Toward an Algorithmic CriƟcism
(Urbana: University of Illinois Press, 2011).
21 Bowden, B. V. Faster Than Thought: A Symposium on Digital CompuƟng
Machines (New York; London: Pitman, 1953).
22 Shamir, Lior, and Jane Tarakhovsky. “Computer Analysis of Art.” Journal on
Computing and Cultural Heritage, vol. 5, no. 2, July 2012.
23 Shamir, Lior. “Computer Analysis Reveals SimilariƟes between the ArƟsƟc
Styles of Van Gogh and Pollock.” Leonardo, vol. 45, no. 2, April 2012.
24 “Vincent Van Gogh and Jackson Pollock: Changing What Art Is.”
www3.dmagazine.com/events/details/Vincent-Van-Gogh-and-Jackson-
Pollock-Changing-What-Art-Is.
25 Dormehl, Luke. “Should We Teach Literature Students How to Analyze Texts
Al gori thmi ca l l y?” Fast 
Company, 
September 
3, 
2013.
fastcolabs.com/3016699/should-we-teach-literature-students-how-to-
analyze-texts-algorithmically.
26 Leonard, Andrew. “How Neƞlix Is Turning Viewers into Puppets.” Salon,
257

February 
1, 
2013.
salon.com/2013/02/01/how_netflix_is_turning_viewers_into_puppets/.
27 Bianco, Robert. “House of Cards Is All Aces.” USA Today, February 1, 2013.
usatoday.com/story/life/tv/2013/01/31/bianco-review-house-of-
cards/1880835/.
28 Blakely, Rhys. “Emmy Awards Brings the Computer Algorithm to Hollywood.”
Times, 
September 
20, 
2013. 
theƟmes.co.uk/Ʃo/arts/tv-
radio/article3874137.ece.
29 nyƟmes.com/2013/07/22/business/media/tv-foresees-its-future-neƞlix-is-
there.html?pagewanted=all&_r=0.
30 Levy, Steven. “In ConversaƟon with Jeﬀ Bezos: CEO of the Internet.” Wired,
December 
12, 
2011.
wired.co.uk/magazine/archive/2012/01/features/ceo-of-the-
internet/page/2.
31 Dormehl, Luke. “Can Alternate Endings Save the Hollywood Blockbuster?”
Fast 
Company, 
July 
30, 
2013. 
fastcolabs.com/3015037/open-
company/can-alternate-endings-save-the-hollywood-blockbuster.
32 This idea is backed up by Wired editor Chris Anderson’s concept of the “98
Percent Rule,” described in his 2006 book The Long Tail. In his discussion
of how technology is turning mass markets into millions of niches,
Anderson argues that niche products are now within reach economically
thanks to digital distribuƟon, and when aggregated can sƟll make up a
significant market—as Amazon’s business model has shown.
33 Warman, MaƩ. “Xbox One Will Track Viewers’ TV Habits and Reward Them
for 
Watching 
Ads.” Telegraph, 
May 
29, 
2013.
telegraph.co.uk/technology/video-games/Xbox/10087148/Xbox-One-will-
track-viewers-TV-habits-and-reward-them-for-watching-ads.html.
34 Lohr, Steve. “Computers That See You and Keep Watch Over You.” New York
Times, January 1, 2011. nyƟmes.com/2011/01/02/science/02see.html?
pagewanted=all.
35 Small, David. “Rethinking the Book,” in Graphic Design & Reading (New York:
Allworth Press, 2000).
36 There have been several creaƟve aƩempts to retain the linearity of
electronic books using a variety of innovaƟve encrypƟon algorithms. In
1992, cyberpunk author William Gibson created an “electronic novel”
called Agrippa (A Book of the Dead), which was mailed to readers on a
three-and-a-half-inch ﬂoppy disk. Once opened, the book’s text would be
displayed for a single Ɵme—gradually disappearing as the user scrolled
down the computer screen.
37 DeRose, Steven. “Structured InformaƟon: NavigaƟon, Access and Control.”
258

April 1995. sunsite.berkeley.edu/FindingAids/EAD/derose.html.
38 
The 
Top 
Grossing 
Film 
of 
All 
Time, 
1 
× 
1, 
2000.
http://salavon.com/work/TopGrossingFilmAllTime/.
39 Visit hint.fmw/wind/ to see the wind map in action.
40 bewitched.com/windmap.html.
41 Rushkoﬀ, Douglas. Present Shock: When Everything Happens Now (New York:
Current, 2013).
42 ZiƩrain, Jonathan. “How Amazon Kindled the Bookburners’ Flames.” Wired,
July 
2013. 
wired.co.uk/magazine/archive/2013/07/ideas-bank/how-
amazon-kindled-the-bookburners-flames.
43 Lanier, Jaron. You Are Not a Gadget: A Manifesto (New York: Alfred A. Knopf,
2010).
44 Hume, David, and John Lenz. Of the Standard of Taste, and Other Essays.
(Indianapolis: Bobbs-Merrill, 1965).
45 Coughlan, Alexandra. “Reviewed: Sensing Memory FesƟval at the University
of 
Plymouth.” New 
Statesman, 
February 
21, 
2013.
newstatesman.com/culture/music-and-performance/2013/02/aural-pill-
popping.
46 Morozov, Evgeny. To Save Everything, Click Here: Technology, SoluƟonism,
and the Urge to Fix Problems That Don’t Exist (London: Allen Lane, 2013).
47 Levy, David. Robots Unlimited: Life in a Virtual Age (Wellesley, UK: A. K.
Peters, 2006).
48 Adorno, Theodor, and Max Horkheimer. DialecƟc of Enlightenment (New
York: Herder and Herder, 1972).
49 Bell, Philip. “Iamus, Classical Music’s Computer Composer, Live from
M a l a g a . ” Guardian, 
July 
1, 
2012.
theguardian.com/music/2012/jul/01/iamus-computer-composes-
classical-music.
50 Ecker, David. “Of Music and Men.” Columbia Spectator, January 25, 2013.
columbiaspectator.com/2013/01/25/music-and-men.
Conclusion: Predicting the Future
1 Meehl, Paul. Clinical vs. StaƟsƟcal PredicƟon: A TheoreƟcal Analysis and a
Review of the Evidence (Minneapolis: University of Minnesota Press, 1954).
2 Goode, Erica. “Paul Meehl, 83, An Example for Leaders of Psychotherapy,
D i e s . ” New 
York 
Times, 
February 
19, 
2003.
nytimes.com/2003/02/19/obituaries/19MEEH.html.
3 “Automated Computer Algorithms Now Carry Out 70% of Trades on U.S. Stock
259

M a r k e t . ” Colors, 
no. 
85, 
December 
3, 
2012.
colorsmagazine.com/stories/magazine/85/story/algorithms.
4 Gladwell, Malcolm. Blink: The Power of Thinking Without Thinking (New York:
Little, Brown, 2005).
5 MacCormick, John. Nine Algorithms That Changed the Future: The Ingenious
Ideas That Drive Today’s Computers (Princeton, N.J.: Princeton University
Press, 2012).
6 Levy, Frank, and Richard Murnane. The New Division of Labor: How Computers
Are CreaƟng the Next Job Market (New York: Russell Sage FoundaƟon;
Princeton, N.J.: Princeton University Press, 2004).
7 Stone, Brad. The Everything Store: Jeﬀ Bezos and the Age of Amazon (New York:
Little, Brown, 2013).
8 Bellos, David. Is That a Fish in Your Ear? TranslaƟon and the Meaning of
Everything (New York: Faber and Faber, 2011).
9 Lanier, Jaron. Who Owns the Future? (New York: Simon & Schuster, 2013).
10
jay.law.ou.edu/faculty/Jmaute/Lawyering_21st_Century/Spring%202012%
11 Brynjolfsson, Erik, and Andrew McAfee. Race Against the Machine: How the
Digital RevoluƟon Is AcceleraƟng InnovaƟon, Driving ProducƟvity, and
Irreversibly Transforming Employment and the Economy (Lexington, Mass.:
Digital Frontier Press, 2012).
12 Levitt, Theodore. Marketing Myopia (Boston: Harvard Business Press, 2008).
13 Gorz, André. Farewell to the Working Class: An Essay on Post-Industrial
Socialism (London: Pluto Press, 1982).
RiŅin, Jeremy. The End of Work: The Decline of the Global Labor Force and
the Dawn of the Post-Market Era (New York: G. P. Putnam’s Sons, 1995).
14 Evans, Christopher. The Mighty Micro (Sevenoaks, UK: Coronet, 1980).
15 Keim, Brandon. “Nanosecond Trading Could Make Markets Go Haywire.”
Wired, February 16, 2012. wired.com/wiredscience/2012/02/high-speed-
trading/.
16 bbc.co.uk/news/technology-18427851.
17 Fallows, Deborah. Search Engine Users, January 23, 2005. Pew Research
Center and American Life Project, pewinternet.org.
18 Vaidhyanathan, Siva. The GooglizaƟon of Everything (and Why We Should
Worry) (Berkeley: University of California Press, 2011).
19 MacCormick, John. Nine Algorithms That Changed the Future: The Ingenious
Ideas That Drive Today’s Computers (Princeton, N.J.: Princeton University
Press, 2012).
260

20 Anderson, Chris. “The End of Theory: The Data Deluge Makes the ScienƟﬁc
Method 
Obsolete.” Wired, 
June 
23, 
2008.
wired.com/science/discoveries/magazine/16-07/pb_theory.
21 Doctorow, Cary. “How an Algorithm Came Up with Amazon’s ‘Keep Calm and
Rape 
a 
Lot’ 
T-Shirt.” BoingBoing, 
March 
2, 
2013.
boingboing.net/2013/03/02/how-an-algorithm-came-up-with.html.
22 “Google Sued over Beƫna Wulﬀ Search Results.” BBC, September 10, 2012.
bbc.co.uk/news/technology-19542938.
23 “Life Through Google’s Eyes: Do You Fit Your Search Engine Age Proﬁle?”
Huﬃngton 
Post 
UK, 
January 
5, 
2013.
huffingtonpost.co.uk/2013/05/01/life-through-googles-eyes-video-
avatar_n_3190281.html.
24 “The Google Autocomplete Guide to Politicians.” Huffington Post UK, April 3,
2013. 
huﬃngtonpost.co.uk/2013/04/03/google-autocomplete-
politicians_n_3007248.html.
25 Morozov, Evgeny. The Net Delusion: The Dark Side of Internet Freedom (New
York: PublicAffairs, 2011).
Tung, Liam. “Google Ordered to Muzzle Defamatory Autocompletes by
German Court.” ZDNet, May 15, 2013. zdnet.com/google-ordered-to-
muzzle-defamatory-autocompletes-by-german-court-7000015406/.
26 Tuchman, Gaye. “ObjecƟvity as Strategic Ritual: An ExaminaƟon of
Newsmen’s Notions of Objectivity.” American Journal of Sociology, vol. 77,
no. 
4, 
January 
1972.
https://umdrive.memphis.edu/cbrown14/public/Mass%20Comm%20Theo
27 Mayer, Marissa. “Google I/O ’08 Keynote Address.” June 5, 2008.
28 Slavin, Kevin. “How Algorithms Shape Our World.” TED Talk, 2011.
youtube.com/watch?v=ENWVRcMGDoU.
Thomas, W. I., and D. S. Thomas. The Child in America: Behavior Problems
and Programs (New York: Knopf, 1928).
29 “United Airlines Stock Decline & the Power of Google.” OneUpWeb.
oneupweb.com/blog/united_airlines/.
30 Meiklejohn, Alexander. PoliƟcal Freedom: The ConsƟtuƟonal Powers of the
People (New York: Harper, 1960).
31 Resende, Patricia. “YouTube Clamps Down on Sexual Content.” NewsFactor,
December 
3, 
2008. 
newsfactor.com/news/YouTube-Gets-Tough-on-
Sleaze/story.xhtml?story_id=0030009XC5M6.
32 Gillespie, Tarleton. “The Relevance of Algorithms,” in Media Technologies:
Paths Forward in Social Research (Cambridge, Mass.: MIT Press, 2013).
33 Tancer, Bill. Click: What We Do Online and Why It MaƩers (London:
261

HarperCollins, 2009).
34 Latour, Bruno. Science in AcƟon: How to Follow ScienƟsts and Engineers
Through Society (Cambridge, Mass.: Harvard University Press, 1987).
35 truthteller.washingtonpost.com/about/.
Raby, Mark. “Truth Teller Algorithm Can Fact-Check PoliƟcians in Real
Time.” Geek, January 30, 2013. geek.com/arƟcles/news/truth-teller-can-
fact-check-politicians-in-real-time-20130130/.
36 Citron, Danielle. “Technological Due Process.” Washington University Law
Review, vol. 85, 2007.
37 Claburn, Thomas. “How Google Flu Trends Blew It.” InformationWeek,
October 25, 2013. informaƟonweek.com/applicaƟons/how-google-ﬂu-
trends-blew-it/d/d-id/1112081?
38 Lanier, Jaron. You Are Not a Gadget: A Manifesto (New York: Alfred A. Knopf,
2010).
Badiou, Alain. In Praise of Love (London: Serpent’s Tail, 2012).
39 Ferguson, Andrew. “PredicƟve Policing and Reasonable Suspicion,” May 2,
2012. law.emory.edu/fileadmin/journals/elj/62/62.2/Ferguson.pdf.
40 Lovejoy, Ben. “Apple Oﬀers Free 1-Hour Computer Science Workshops for
Kids 
& 
Teens.” 9to5Mac, 
December 
9, 
2013.
9to5mac.com/2013/12/09/apple-oofers-free-1-hour-computer-science-
workshops-for-kids-teens/.
Grothaus, Michael. “Does the ‘Lolita Bot’ Help Catch Online Predators or
Create 
More 
of 
Them?” Fast 
Company, 
July 
16, 
2013.
fastcolabs.com/3013217/the-forerunners-of-future-sexbots-now.
41 Žižek, Slavoj. Living in the End Times (London; New York: Verso, 2010).
262

Index
The page numbers in this index refer to the printed version of this book. The link
provided will take you to the beginning of that print page. You may need to
scroll forward from that locaƟon to ﬁnd the corresponding reference on your e-
reader.
Adams, Douglas 2
Adorno, Theodor 179, 205
Adventures in the Screen Trade (Goldman) 161
Affectiva 193
Against Autonomy (Conly) 138–39
Agrippa (Gibson) 238n
Akrich, Madeline 136n
algorithms:
and Amazon’s “fulfilment associates” 44
and astronaut selection 24
and black-boxing 151–52, 227–28, 235–36
and call centers 21–22, 24–25, 49–50
cars driven by 141–42
in CCTV cameras 145–46
and death 96–98
and differential pricing 50–53
and differentiated search results 47–48
and discriminatory practices 53–54
for divorce 130–31
for drunk-driving detection 131–33
and early computer dating 82
and emotion sniffing 51–52
and equity market 210–11
and exclusive highways 48–49
explained 1–6
for ﬁnding love and sex 61– 9 5 passim, 98–1 0 5 ; see also ALikeWise;
BeauƟfulPeople; Bedpost; eHarmony; FindYourFaceMate; FitnessSingles;
Internet: daƟng; Kari; LargeAndLovely; love and sex; “Match”; Match .com;
OKCupid; PlentyOfFish; SeaCaptainDate; Serendipity; TrekPassions;
UniformDating; VeggieDate
Flu Trends 238–39
to forecast crime 119–24; see also Berk, Richard; law and law enforcement
and gaming technology 32–34
263

“genetic” 203–4
growing role of 210
Iamus 206–7
inferences of, and conceptions of self 36–38
and Kari 98–103; see also love and sex
and legal documents 129–30
and London Symphony Orchestra 206
and “The Match,” see “Match”
measuring students’ progress with 39–41
and medic 211
and money laundering 18–19
and offence 224–27
and police work, public policy and judicial system, see law and law
enforcement
and recruitment 25–31
and same-sex couples 147
and student grades 208, 212
and terrorism 149–50, 153
as “tricks” 221–23
and “truths” in art 182–83; see also art and entertainment
TruthTeller 237
and Twitter 35–36, 38
upward mobility programmed into 46–47
why we trust 149–51
ALikeWise 79
Amabot 214–15
Amazon 85, 188, 198
and disappearing gay-friendly books 232
e-books sold by 179–80
“fulfilment associates” at 44–45
how algorithms work with 1–2
and Solid Gold Bomb 224–25
two departments of 213–15
work practices at 44–46
Ambient Law 131–33, 137, 143–44
Amscreen 20
Anderson, Chris 56, 191n, 223
Angela (Quantified Self devotee) 14
see also Quantified Self movement
Aniston, Jennifer 69
Anomo 89–90
Apple 133
Apple v. Samsung 127–28
264

Apprentice, The (US) 89
Aquinas, Thomas 183
Aron, Arthur 101
art and entertainment 161–207
and dehumanisation 203–4
and films via Internet 179–80
and mass market 175–77
and Salganik–Dodds–Watts study 172–73
and “superstar” markets 173 see also Epagogix; individual parƟcipants;
individual titles
artificial intelligence 126, 217
AT&T 29
Atlantic 11
Avatar 163, 172, 205
Avaya 49
Balloon Brigade 33
Barefoot Into Cyberspace (Hogge) 44
Barrymore, Drew 167
Barthes, Roland 186
Bauman, Zygmunt 81–82
BeautifulPeople 78
beauty map 31–32
Beck, Charlie 106–7
Bedpost 13, 93–95
Bedwood, David 97
Beethoven, Ludwig 202
Bellos, David 215
Benjamin, Walter 178–79
Bense, Max 182n
Bentham, Jeremy 55, 118
Berk, Richard 120–24, 236
Bezos, Jeff 190
Bianculli, David 189
Bieber, Justin 202–3
Blackstone Electronic Discovery 127–28
Blink (Gladwell) 211
blogging 30
and owners’ personalities 38–39
and word choice 38–39
body-hacking 13–14
BodyMedia 94
books, see art and entertainment
265

Bowden, B. V. 184
Bowden, Mark 11
Boyce, Vincent Dean 15–16
Bratton, William 108–10, 113
Brazier, David 70
Breathalyzer 143–44
Brownsword, Roger 138
Brynjolfsson, Erik 217
Burberry 52
Busa, Roberto 183
California Institute for Telecommunications and Information Technology 7
call centers 21–22, 24, 49
CalWIN 159
Cameron, David 113
Cameron, James 162–63
Carter, Steve 74–76
Casablanca 86
Catherine, Duchess of Cambridge 68
CBS 126
CCTV 145–46
CCTV Today 146
celebrity marriages, predicting breakup of 67–69
chatterbot 99
Cheney-Lippold, John 58–59
Chivers, Tom 135
Christensen, Clayton 129
Churchill, Winston 45
Cisco 49
Citron, Danielle 150, 153–54
Claburn, Thomas 239
Clarke, Arthur C. 221
Clegg, Nick 225
Click: What We Do Online and Why It Matters (Tancer) 3, 233–34
Clinical vs. Statistical Prediction (Meehl) 208
Coetzee, J. M. 203
cogito ergo sum (I think, therefore I am) 12
Colorado public-benefits system, errors in 153–54
Community 196
Comte, Auguste 114–16
Conboy, Kevin 13, 92–95
Conly, Sarah 138–39
cookies 17
266

Coughlan, Alexandra 201
Coupland, Douglas 16
CourseSmart 39–41
crime, see law and law enforcement
Crohn’s disease 10
Crosby, Michelle 131
Crowded Room 89
Cruise, Tom 68–69, 118
Csikszentmihalyi, Mihaly 100
Cuckoo’s Calling, The (Galbraith) 187
Culture of Public Problems, The (Gusfield) 142–43
“cyberbole” 210
origin of 4
DARPA 213
Darwin, Charles 31, 118
data-mining 10, 41, 67–68, 106, 126, 134–35, 150, 156, 158–60, 183, 187, 223
dating sim 103–4
Dawkins, Richard 129
de Botton, Alain 87
“De-Scription of Technical Objects, The” (Akrich) 136n
DeadSocial 96
death, apps concerning 96–98
decimated-reality aggregators 45–47
Deep Blue 29
deferred acceptance 63
see also “Match”
Deleuze, Gilles 54–55, 60
demassification 21
DeRose, Steven 194
Descartes, René 12
deviantART 37
Dialectic of Enlightenment (Adorno, Horkheimer) 179
Dick, Philip K. 118, 226
Die Walküre 70
differential pricing 50–52
digital humanities 3
“Digital Panopticon, The” (Poole) 55–56
“Digitizing the Racialized Body” (Hansen) 53
Disney 181
“dividuals” 54
divorce:
algorithm for 129
267

rate of 67, 71–72
“Do Artifacts Have Politics?” (Winner) 134
Dodds, Peter 172–76
Dominguez, Jade 25
Dostoyevsky, Fyodor 118
Dourish, Paul 231
Dow Jones 219
drunk driving 142–44
Eagle, Nathan 85
Ecker, David 206–7, 219
eHarmony 71, 74–77, 88
see also Internet; love and sex; Warren, Neil Clark
Eisenstein, Sergei 178
Electric Dreams 103
Ellul, Jacques 5, 56
EMD Serono 58
emotion sniffing 51–52
Emotional Optimisation 200–201
Enchanted Loom, The (Jastrow) 96
entertainment, see art and entertainment
Epagogix 165–68, 170–72, 176, 179, 191, 203, 205
Eric Berne Memorial Scientific Award 23
Essay on the Moral Statistics of France (Guerry) 117
“Experimental Study of Inequality and Unpredictability in an ArƟﬁcial Cultural
Market” 173
Facebook 232, 241
and Facedeals 20
and facial recognition 215
how algorithms work with 2
jobs at 27
profiles, and people’s success 30–31
profiles, traits inferred from 37–38
Timeline feature on 38–39
and YouAreWhatYouLike 37
Facedeals 20
facial recognition and analysis 20, 33, 91, 146, 151, 193, 215
and Internet dating 78
Failing Law Schools (Tamanha) 216
Family Guy 196
Farewell to the Working Class (Gorz) 217–18
Fast Company 3, 35, 128, 220
268

on Amazon 44–5
Faster Than Thought (Bowden) 184
Faulkner, William 187
Feldman, Konrad 18–19
films, see art and entertainment
Filter Bubble, The (Pariser) 47
Fincher, David 189
Find the Love of Your Life (Warren) 73
FindYourFaceMate 78
Fitbit 13
FitnessSingles 78
Flash Crash 219
flexitime 43
Food Stamp Act (US) 154–55
Ford, Henry 44
Foucault, Michel 101
Fourastie, Jean 219
Freud, Sigmund 11
Friedman, Milton 218
Galbraith, Robert 187
Gale, David 62–63, 66
Galton, Francis 31–32
gaming technology 32–33
Gass, John 148
Gates, Bill 182
Geek Logik (Sundem) 67–68
gender reassignment 26
GenePartner 77–78
Generation X (Coupland) 16
Gibson, William 194n
Gild 25–26, 29–30
Gillespie, Tarleton 233
Gladwell, Malcolm 211
Goldman, William 161, 173
Good Morning America 67
Google 201–2
and auto-complete 225–27
claimed objectivity of 220–21
differentiated results from 46–48
dynamic-pricing patent granted to 50; see also differential pricing
employment practices of 41–42
and facial recognition 215
269

Flu Trends algorithm of 238–39
how algorithms work with 2
and inadvertent racism 151
and Lake Wobegone Strategy 27–29
Levy’s study of 41
and news-outlet decline 225–27
People Analytics Group within 41–42; see also web analytics
and self-driving cars 143, 213
Slate article on 41
and UAL 229
Google Earth 135
Google Glass 14, 26
Google Maps 16, 134–35
Google Street View 227
Google Translate 215, 221
Gorz, André 217
Gottschall, Jonathan 186
Gould, Stephen Jay 33–34
Graf, Daniel 135
graph theory 182
Grindr 89, 152
Guardian 84
Guattari, Félix 48, 54
Guerry, André-Michel 114–18
Gusfield, Joseph 142–43
Halfteck, Guy 32–34
Hansen, Mark 53
Hanson, Curtis 167
Heaven’s Gate 167
Henry VI (Shakespeare) 125–26
Her 103
Hitchcock, Alfred 17
Hogge, Becky 44
Holmes, Katie 68–69
Holmes, Oliver Wendell Jr. 158
Horkheimer, Max 179, 205
House of Cards 188–89
House of Commons, rebuilding of 45
How the Mind Works (Pinker) 80
Human Dynamics (at MIT) 85
Hume, David 199–200
Hunch 37, 234
270

Hunger Games, The 169
Hutcheson, Joseph C. Jr. 157
Iamus 206–7
IfIDie 96
In The Plex (Levy) 41
Industrial Revolution 21, 40
info-aesthetics 182
Information Age, coming of 21
Instagram 216
Interactive Telecommunications Program 15
Internet:
and cookies 17
daƟng 71, 75–79, 81; see also eHarmony; FindYourFaceMate; GenePartner;
love and sex
growing access to 76–77
and scavenger-class customers 49–50
shopping via 16–17, 20; see also Amazon
tracking of user movements on 18
as transactional machine 46
and web analytics 18–19, 41; see also Google
iPhone 36
Is That a Fish in Your Ear? (Bellos) 215
Isaacson, Walter 36
Islendiga-App 88–89
Jackson, Joe 70
James, Henry 70
James, William 17
Jastrow, Robert 96
Jeopardy! 158
Jobs, Steve 36, 87
John Carter 163, 172
Jolie, Angelina 69
Jonze, Spike 103
JPod (Coupland) 16
judges 156–58
judicial behavior, predicting 156–59
and automated judges 160
jurimetrics 139–40, 158
Kahler Communications 24–25
Kahler, Dr. Taibi 22–24
271

Kardashian, Khloe 68
Kari 99–103, 105
Kasparov, Garry 29
Keillor, Garrison 28
Kelly, John 127–29
Kelly, Kevin 12
Kelvin, Lord 31
Kerckhoff, Alan 77
Kindle 180, 197–98, 203
Kinect 132
Kipman, Alex 132
Kirke, Alexis 190–92, 194, 197
Knack 32–34
Knowledge Acquiring and Response Intelligence 99
Kodak 129, 216
Koppleman, Lee 134
Kranzberg, Melvin 151, 222
lactoferrin 10
Lake Wobegone Strategy 28–29
Lanier, Jaron 90–91, 199, 216, 239
LargeAndLovely 78
Lasswell, Harold 5
Late Age of Print, The (Stiphas) 221
Latour, Bruno 136, 235–36
law and law enforcement 106–33, 137–60
and age and gender 121–23
and Ambient Law 132, 137, 143–44
and automated judges 160
and bail and parole 119–21
and crime hotspots 110–112
and drunk-driving detection 131–33
and legal discovery 125–28
and “PreCrime” 118–19, 123–25
and predicting judicial behavior 155–60
and predictive policing 107–9, 119
and PredPol 113
and “RealCog” 120–21
and relative poverty 117
and rules vs. standards 141–43
and school students 125
Lawrence, Jennifer 169
LegalZoom 130
272

Leibniz, Gottfried 139–40
Lessig, Lawrence 139
Levitt, Theodore 217
Levy, David 104–5
Levy, Frank 212–13
Levy, Steven 41
Lewis, Sinclair 186–87
Li, Jiwei 35
Life & Times of Michael K (Coetzee) 203
Life on Screen (Turkle) 57
LinkedIn 27
Liquid Love (Bauman) 82
Liu, Benjamin 89
LivesOn 96–97
London Symphony Orchestra 206
Long Tail, The (Anderson) 56, 191n
Lorenz, Edward 171
Love in the Time of Algorithms (Slater) 81
love and sex:
algorithms and technology for 61– 9 5 passim, 98–1 0 5 , 239; see also
ALikeWise; BeauƟfulPeople; Bedpost; eHarmony; FindYourFaceMate;
FitnessSingles; Kari; LargeAndLovely; love and sex; “Match”; Match.com;
OKCupid; SeaCaptainDate; Serendipity; UniformDating; VeggieDate
and celebrity marriages, see celebrity marriages, predicting breakup of
genetic matching for 77–78
Warren’s researches into 72–74
and wearable tech 94–5
see also divorce; “Match”; PlentyOfFish
Love and Sex with Robots (Levy) 104–5
Lovegety 87–88
Lucky You 167–68
Lust in Space 100
McAfee, Andrew 217
Macbeth (Shakespeare) 191
McBride, Joseph 164n
MacCormick, John 212, 222
McCue, Colleen 106–7
McLuhan, Marshall 88
Malinowski, Sean 107–14
Manovich, Lev 177–78
Many Worlds 190–92, 194, 197
maps 134–36
273

Marx, Karl 11, 137n
Massachusetts Institute of Technology (MIT) 28
Human Dynamics group in 85
Serendipity project of 85–87
“Match” 62–66
tabulated example of 64
see also love and sex
Mattersight Corporation 22–24
Mayer, Marissa 228
Meaney, Nick 166–67, 170–72, 176, 205
Measure of Fidget 32
Medavoy, Mike 162
Medicine and the Reign of Technology (Reiser) 142
Meehl, Paul E. 208–9
Meiklejohn, Alexander 231
Merleau-Ponty, Maurice 98–99
Michael (Quantified Self devotee) 13
see also Quantified Self movement
Microserfs (Coupland) 16
Microsoft 51–52, 132, 192–93, 237
MIDI 199
Mill, John Stuart 118
Ming, Vivienne 25–27, 29–30
Miniscript 22–23
Minority Report 118–20, 123
Mismeasure of Man, The (Gould) 33–34
Mohler, George 111–12
money laundering 19
Morozov, Evgeny 201–2, 226, 243
Moses, Robert 134
movies, see art and entertainment
Mozart, Wolfgang 172, 203–4
Mumford, Lewis 5
Murnane, Richard 212–13
musical dice game 204
Myhrvold, Nathan 182
Nara 46–47, 136
NASA 24
NASCAR 37
Nautilus 14
Negobot 240
Net Delusion, The (Morozov) 226
274

Netflix 52, 127, 176, 188–89, 228, 236
neural networks 166
illustration of 168
neuroscience 159
new algorithmic identity 55, 58
New Division of Labor, The (Levy, Murnane) 212–13
New Statesman 55
New York Times 40–41, 52, 58, 67, 71
Newton, Isaac 114
Nietzsche, Friedrich 70
Nightingale, Florence 118
Nine Algorithms That Changed the Future (MacCormick) 222
Nineteen Eighty-Four (Orwell) 138, 198
Nudge (Thaler, Sunstein) 137–38
Obama, Barack 189, 225
Odom, Lamar 68
“(Of the) Standard of Taste” (Hume) 199–200
OKCupid 77
On Love (de Botton) 87
On Love (Stendhal) 70
On Man and the Development of his Faculties (Quetelet) 117
online dating, see Internet: dating
online shopping, see Internet: shopping via
Onomatics 130–31
OptimEyes 20
Orwell 138
panopticon 55
Parada, Sergio 99–100, 102–3
paradox of choice 82–83, 156
Paradox of Choice, The (Winchester) 82–83
Pariser, Eli 47
Parks, Rosa 59
Pascal, Blaise 70
Patterson, James 203
Pentland, Alex 85
personality types 23–25
tabulated 23
Pfizer 58
Pinker, Steven 80–81
Pinkett, Jada 69
Pitt, Brad 69
275

Plan of Scientific Operations . . . (Comte) 114
PlentyOfFish 81
police, see law and law enforcement
Pollock, Jackson 185
Poole, Steven 55
Popenoe, Paul 72
Popular Press, 1833–1865, The 177
Posner, Richard 155–56, 158–60, 168, 169
Poupyrev, Ivan 181
Poverty of Philosophy, The (Marx) 137n
“precogs” 118
see also law and law enforcement
predictive behavioral routing 22–23
“Predictive Policing” (McCue, Beck) 107
see also law and law enforcement
PredPol 113
Present Shock (Rushkoff) 195
Principles of Psychology, The (James) 17
Principles of Scientific Management, The (Taylor) 42
Process Communication 22–24
Proust, Marcel 83
psychic energy 100–101
Punin, Nikolai 178
Pygmalion (Shaw) 102
Quantcast 18–20, 55, 107
Quantified Self movement 12–16
Queenan, Joe 167
Quetelet, Adolphe 114–18
Quora 30
“Race Against the Machine” (McAfee, Brynjolfsson) 217
Radin, Max 157
Raiders of the Lost Ark 161
Ramsay, Stephen 182–83, 187
RealCog 120
see also law and law enforcement
reductio ad absurdum 37
Reiser, Stanley 142
Renoir, Pierre-Auguste 202
Rifkin, Jeremy 217
riots, UK (2011) 113
robo-cizing the world 13–15
276

Rochberg-Halton, Eugene 100
“Role of the Judge in the Twenty-First Century, The” (Posner) 156–57
Roth, Eric 167
Rothko, Mark 201
Rowling, J. K. 187
Rushkoff, Douglas 195–97
Salavon, Jason 194
Salganik, Matthew 172–76
Salieri, Antonio 172
Samsung, see Apple v. Samsung
scavenger-class customers 49–50
SceneTap 91
Schindler’s List 171
Schwartz, Barry 82
“scripts”, technological 136–37
SeaCaptainDate 78–79
Sears, Barry 8
self-driving cars 143–44, 213
Selfish Gene, The (Dawkins) 129
Serendipity 85–87
“750 words” 13
sex, see love and sex
Shakespeare, William 125–26, 182–83
Shamir, Lior 184–85, 203, 205–6
Shapley, Lloyd 62, 80
Shteyngart, Gary 85
Signal and the Noise, The (Silver) 235
Silver, Nate 235
Slate 41, 97
Slater, Dan 81
Slavin, Kevin 228
Smarr, Larry 7–12, 220
Smith, Dave 208–9
Smith, Will 69, 164–65
social discovery 86–90
“society of control” 54–55
Socrates 12
Soddu, Celestino 204
Solid Gold Bomb 224
Sony Walkman 14
Spacey, Kevin 188–89
spam 58
277

Speed 44
Spielberg, Arnold 164
Spielberg, Steven 118, 164
Stable Marriage Problem, see “Match”
Stairmaster 14
Stanton, Andrew 163
Star Wars 161
Stendhal 70
Steve Jobs (Isaacson) 36
Stone, Brad 214
Strangers on a Train 17
Striphas, Ted 232, 236
“Structured Information” (DeRose) 194
Sugar, Simon 20
Sundem, Garth 67–69, 234
Sunstein, Cass 137
Super Sad True Love Story (Shteyngart) 85–86
Supreme Court Forecasting Project 158
Surden, Harry 159, 232
Sweeney, Latanya 151
synaesthesia 181
tamagotchi 100
Tamanha, Brian 216
Tancer, Bill 3, 233
Taylor, Frederick 42
Taylorism 42
Technical Man 5
technological “scripts” 136
Technological Society, The (Ellul) 56
Tejeda, Charles 40
television scheduling 155–56
Tesco 20
work practices at 45
Thaler, Richard 137
“Theory of Judicial Decision, The” (Radin) 157
Third Wave, The (Toffler) 20–21, 43, 53–54
Thomas, Dorothy 228
Thomas, William 228
Thompson, David 172n
Thousand Plateaus, A (Deleuze, Guattari) 54
Titanic 162–63, 194
To Save Everything, Click Here (Morozov) 201
278

Toffler, Alvin 20–21, 43, 53–54
Tolstoy, Leo 203
“Towards Digitally Enabled Genomic Medicine” (Smarr) 11
TrekPassions 79
TruthTeller 237
Tuchman, Gaye 226–27
Turing, Alan 184
Turkle, Sherry 56, 131
Turner, Fred 56–57
Turow, Joseph 52
TweetPsych 38
Twitter 26, 30, 227, 230–31
and algorithms 35–36
and hashtags 230
UAL 229
UniformDating 79
Untitled (Green on Blue) 201
Up in the Air 79
Vaidhyanathan, Siva 221–22
Van Gogh, Vincent 185
VeggieDate 79
Vico, Francisco 206–7
Viegas, Fernanda 195
Virilio, Paul 2, 43, 218
Vollmer, Walter 149
Wagner, Richard 70
Walker, Robert 17
Walmart 106–7
War and Peace (Tolstoy) 203
Wark, McKenzie 235
Warren, Neil Clark 71–75, 84
Wasabi Waiter 33
Wattenberg, Martin 195
Watts, Duncan 172–75
wave theory 20–21
Wearable Computing Group 14
web analytics 18–20
at Google 41
see also Google
Webb, Amy 84
279

Well, Marius B. 225
Wert, Robert 24–25
Westminster Review 118
Wevorce 131
Who Owns the Future? (Lanier) 90–91, 216
“will-to-order” 5
William, Prince, Duke of Cambridge 68
Wilson, Nathan 46
Winchester, Dan 82
Winner, Langdon 134, 136
Wired 12, 56–57
Wolf, Gary 12
“Work of Art in the Age of Mechanical Reproduction, The” (Benjamin) 178–79
Wulff, Bettina 225, 227–28
Yarkoni, Tal 38
You Are Not a Gadget (Lanier) 239
YouAreWhatYouLike 37
YouTube 232
Zittrain, Jonathan 198
Žižek, Slavoj 236
Zone, The (Sears) 8
280

281

