•
•
•
•
Research Article 
E-Learning Simulations for Teaching Clinical 
Decision-Making in an Audiology Training Program 
Dani Tomlin,a 
Patrick Bowers,a 
and Kelley Graydona 
a Department of Audiology and Speech Pathology, The University of Melbourne, Australia 
A R  T  I  C L E  
I  N  F  O  
Article History: 
Received November 3, 2022 
Revision received March 16, 2023 
Accepted July 11, 2023 
Editor-in-Chief: Ryan W. McCreery 
Editor: Nicholas S. Reed 
https://doi.org/10.1044/2023_AJA-22-00210 
Correspondence to Kelley Graydon: kgraydon@unimelb.edu.au. Dis-
closure: The authors have declared that no competing financial or non-
financial interests existed at the time of publication. 
A B  S T  R  A  C  T  
Purpose: Clinical decision-making is an essential component of most clinical 
processes across the health sector and an ongoing challenge for clinical educa-
tion programs to teach to students. The traditional methods of teaching these 
skills outside of lectures are mainly through clinical placements and problem-
based learning (PBL) sessions, but availability and consistency can be variable 
and resource heavy. To address these challenges, an e-simulation module spe-
cific to pediatric infant diagnostic testing has been developed and implemented 
into the teaching program. We aimed to establish whether e-simulation resulted 
in student skill acquisition. 
Method: Academic performance measures for 67 audiology students who used 
a traditional PBL and 108 students who used the e-simulation module were col-
lected and analyzed. Student survey results generated both quantitative and 
qualitative data, which were analyzed using a thematic analysis with an induc-
tive approach. 
Results: Findings indicated that the e-simulation platform introduced in this 
study yielded learning outcomes similar to the traditional PBL format previously 
used. Participants experienced a significant in situ increase in understanding of 
infant diagnostic testing concepts following the use of the e-simulation and 
evaluated the e-simulation platform positively. 
Conclusion: The data support that an e-simulation–based approach in clinical 
education presents pedagogical benefits and can provide a meaningful sustain-
able inclusion in today’s clinical teaching programs. 
The pedagogy of today’s clinical teaching programs 
must include real-world experience for skill development. 
Clinical decision-making and higher order thinking are 
poorly taught when only a traditional didactic approach is 
employed (Prince & Felder, 2006). Effective development 
of these skills relies on problem- and case-based learning in 
classes, as well as adequate exposure to clinical placements 
under the supervision of an experienced clinician to ensure 
competency to practice (Amey et al., 2017). Students in 
clinical placements consolidate knowledge and have the 
opportunity for hands-on experience to strengthen skills 
and confidence in a safe learning environment. Unfortu-
nately, there is a global shortage of these rich learning envi-
ronments across allied health clinical programs including 
audiology, speech pathology, physiotherapy, and nursing 
(Nisbet et al., 2021). The high demand and decreasing sup-
ply is largely a result of external factors placed on place-
ment providers including reduced funding and staffing, 
changes in health care models, and the competing demand 
between patient turnover and time required to effectively 
support a student on a clinical rotation (Spence et al., 
2019). Increases in student cohorts and new programs have 
also led to placement shortages. The mismatch between stu-
dent numbers and clinical placements is only expected to 
worsen, with an increased need for health care professionals 
to offset an aging population and general population 
increase. Audiology courses in the United States anticipate 
that the academic programs will need to increase graduate 
numbers by 50% and maintain these higher enrollments for 
the next 30 years to match the need (Windmill & Freeman, 
2013). Similar demands are being placed on Australian uni-
versity programs, with increases in audiology graduates 
necessary to service rural and remote areas (Wilson et al.,
American Journal of Audiology
Vol. 32
878–888
December 2023
Copyright © 2023 American Speech-Language-Hearing Association
878
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

2011), as well as counteract population increases (Dawkins 
et al., 2019). Audiology courses have a need to develop 
alternative methods for teaching clinical decision-making, 
given the competing demands placed on the already limited 
placement opportunities. Audiologists may see specialized 
caseloads that are not accessible or equivalent for most of 
the student cohort. Some subfields of audiology simply do 
not have enough clinical placements on offer to all students 
enrolled in a given clinical teaching program due to their 
smaller patient cohort compared to other specialties. Sub-
fields working with more sensitive or complex patient 
cohorts may also offer a limited number of clinical place-
ments (or none at all). Infant diagnostic hearing assess-
ments are an example of a highly specialized clinical field 
in which students require competency; however, there are 
limited clinical encounters where students can acquire the 
necessary skills to meet competency markers. If students do 
not have opportunities to demonstrate competency in cer-
tain skills, they risk being unable to graduate from their 
clinical program. Finding solutions that do not rely on clin-
ical placements (yet provide audiology students with alter-
native experiential learning opportunities to assist them 
reaching competency) are therefore vital. 
Problem-Based Learning for Clinical 
Skill Development 
Problem-based learning (PBL) is an effective method 
for teaching medical/health care students the critical think-
ing required for real-world clinical situations in a class-
room environment (Barrett & Moore, 2010; Trullàs et al., 
2022). PBL typically occurs in small groups, with students 
working through a clinical scenario to lead to a diagnosis 
and management plan for the patient. The role of the 
facilitator in PBL is to guide rather than directly teach, 
with students required to take responsibility for their own 
learning. PBL sessions can improve students’ teamwork 
skills, further their understanding of concepts and effec-
tively teach clinical reasoning skills (Prince & Felder, 
2006). The benefit that students gain from PBL is, how-
ever, highly dependent on their baseline knowledge levels; 
students possessing a lack of requisite knowledge to guide 
themselves through more complex cases are at risk of fall-
ing behind and receiving less benefit than their peers. Tra-
ditional PBLs are also resource intensive, as significant 
time is taken for the teacher to prepare materials and run 
the session, while varying levels of learner understanding 
can lead to inconsistent session length (Jin & Bridges, 
2014; Trullàs et al., 2022). PBLs can be less lifelike than 
what the student would be typically exposed to in a clini-
cal setting and may not necessarily expose students to the 
clinical skills and decision-making necessary in difficult 
and complex clinical settings (Prince & Felder, 2006). 
Designing alternative learning activities that provide the 
benefit of traditional PBL, while ameliorating the evident 
limitations, is therefore desirable. 
Clinical E-Simulation Approach to PBL 
Simulation has been widely employed in health care 
teaching programs as a tool to enhance student clinical 
skill development (Watson et al., 2012), as well as to help 
deal with increasing student numbers and reduced clinical 
placement opportunities (Larue et al., 2015). The advan-
tages to simulation include a safe learning environment 
for students to experiment and to fail, the ability for 
teachers to ensure consistency in clinical skills, and an 
opportunity to expose students to more complex and sensi-
tive clinical scenarios (Duff et al., 2016). Importantly, simu-
lation can help guide students through key clinical decision-
making processes. Well-designed simulations are able to 
place focus on information processing, critical thinking, 
and problem-solving skills—all necessary for optimization 
of patient health and minimization of patient harm (Woda 
et al., 2017). Despite these benefits, in-person-only simula-
tions can be limited by being time consuming and resource 
heavy, with multiple demonstrators and actors required for 
small group teaching and limited opportunities to provide 
real-time feedback (Stow et al., 2017). 
An 
e-simulation 
approach 
provides 
an 
effective 
alternative to traditional PBL or in-person simulation 
approaches (Cook et al., 2010; Coyne et al., 2021; Duff 
et al., 2016). E-learning simulations can provide a deeper 
understanding of how a variety of cases operate in clinical 
contexts and the nuances of the role of the clinician in 
patient diagnostics and treatment (Cook et al., 2010). 
Additionally, a range of critical learning skills can be 
simultaneously developed while engaging in this kind of 
learning (Duff et al., 2016). Innovations such as this also 
have the capacity to close the learning loop by offering 
students an opportunity to evaluate the work that they 
have completed, by offering both formative and summa-
tive feedback as they progress. Simulating the clinical pro-
cesses that occur across a clinical encounter may improve 
a student’s learning experience and preparedness for clini-
cal encounters, and the type of feedback students receive 
allows them to reflect on their current level of understand-
ing (Coyne et al., 2021). E-simulations appear to present 
an effective response to the reduction in access to clinical 
placements. The reduction in instructor/student interac-
tions also allows for a more time-efficient mode of train-
ing and instruction (Mistry et al., 2019). By incorporating 
digital pedagogies as a teaching and learning approach, 
low-cost case development can be achieved, which draws 
on a variety of sources of information, including medical 
results, multimedia, and interactivity through gamification 
and 
real-time 
feedback. 
Game-like 
e-simulations 
for
Tomlin et al.: Simulations for Teaching Clinical Decision-Making
879
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

•
•
•
clinical skill development are a relatively new (although 
rapidly expanding) pedagogical approach within medical 
and allied health education (Gorbanev et al., 2018). A 
gamified approach incorporates interactive game design 
elements to improve academic outcomes; early research 
has shown some positive impact on academic performance 
from 
using 
serious 
games 
in 
comparison 
to 
control 
groups; however, further research is required in this field 
to expand understanding in this area (Gorbanev et al., 
2018; van Gaalen et al., 2021). 
Purpose 
Although enrollments in audiology programs con-
tinue to grow, there is increasing pressure from limited clin-
ical placement opportunities. Given this shortfall, programs 
must explore other mechanisms for teaching clinical skills 
and clinical decision-making. Research is limited to alterna-
tive clinical education approaches for audiology when com-
pared to other clinical professions (Morgan et al., 2022). 
PBL sessions have been introduced to provide clinical rea-
soning learning, but these have been found to be resource 
heavy. 
To 
date, 
there 
is 
a 
paucity 
of 
research 
on 
approaches to clinical education of infant hearing testing, 
and none is exploring an e-simulation approach. Previous 
studies have focused on teaching this clinical skill with the 
involvement of “in-person” standardized parents and/or 
lifelike baby mannequins such as the Intelligent Hearing 
Systems (2022) manufactured Baby Isao simulator (Alanazi 
et al., 2016; Reed et al., 2021; Wilson et al., 2020). There is 
a 
need 
to 
explore 
the 
feasibility 
of 
an 
e-simulation 
approach for clinical education in this space. 
In this study, we describe an e-simulation that has 
been developed with the purpose of immersing students in 
experiential clinical reasoning for the measurement of 
infant hearing through a series of cases of increasing com-
plexity. Formative and summative feedback is provided 
throughout the cases and guides the student through 
decision-making processes based on information obtained. 
Although there is evidence that simulation is effective in 
teaching clinical skills, the primary goal of this study was 
to investigate the feasibility of replacing PBL with an 
e-simulation to teach infant diagnostic clinical decision-
making. The second goal was to evaluate the students’ 
engagement with the e-simulation platform. 
Method 
Overview 
This project has ethics approval under The Univer-
sity 
of 
Melbourne 
Human 
Research 
Ethics 
project 
1852876.1. The Master of Clinical Audiology is a 2-year 
entry to practice program, incorporating classroom didac-
tic teaching and experiential learning through practical 
sessions and clinical placements. Infant diagnostics is 
taught in the second year of the program, with a series of 
theory-based didactic lectures, clinical placements, and 
structured clinical reasoning cases. As presented below, 
these structured clinical reasoning cases have been deliv-
ered through either traditional in-person PBL or e-simula-
tion. The content covered by both the traditional PBL 
and the e-simulation module sits within a large year-long 
clinical subject with a written exam as the final assessment 
piece. This subject is pediatric focused, with intended 
learning outcomes of being able to conduct accurate 
audiometric assessment with children of all ages and dem-
onstrate ability to analyze, interpret, and integrate test 
results, leading to formulation of appropriate management 
and intervention strategies. Learning objectives for both 
the traditional PBL and e-learning simulations were to 
identify when each test is relevant for a particular infant, 
to demonstrate how each test may be performed, to be 
able to correctly interpret infant diagnostic hearing results, 
and to form appropriate management plans for infants 
with a range of hearing levels. 
Participants 
Four cohorts of final year Master of Clinical Audi-
ology students across four academic years contributed to 
the overall data set. A total of 108 final year students 
(across two cohorts) were provided with the e-simulation 
to teach the infant diagnostic content, and in the preced-
ing 2 years, a total of 67 students (across two cohorts) 
received the traditional PBL to deliver the same content. 
Participation was voluntary and did not involve any mon-
etary or other incentive. 
Traditional PBL 
The traditional 4-hr PBL session, which 67 of the 
students participated in, required four staff members to 
run. The focus of the session was on testing an infant’s 
hearing from start to finish. Students received a short 
briefing at the start of the session in the form of a didactic 
lecture, which gave case details such as the infant’s name, 
age, and their objective of diagnosing the infant’s hearing 
status. Students were encouraged to work through the case 
as they would during an infant diagnostic appointment, 
making sound clinical decisions as they progressed. They 
were instructed to make a series of choices about what 
objective test(s) they wished to perform on the imaginary 
infant and when, including specific detail about test setup 
(presentation levels, ear). No actual or simulated testing 
occurred during the traditional PBL; however, students
880
American Journal of Audiology
Vol. 32
878–888
December 2023
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

were expected to describe how the tests would occur in 
real life. The progress of the students throughout the tra-
ditional PBL was monitored by tutors who asked ques-
tions of the students such as, “What test would you per-
form next?” and “What does this result show?” Once stu-
dents provided an answer, they would then be given the 
next relevant piece of case study information from the 
tutors. Information given included verbal descriptions of 
test results (e.g., “The distortion product otoacoustic emis-
sions were present at the requested frequencies in the left 
ear”) or printed images of results (e.g., auditory brainstem 
response waveforms for the requested ear, images of tym-
panograms). As the students could request information at 
any point and tutors were facilitating their progress with 
discussion, there was a need for multiple tutors to con-
stantly move between groups to prevent long wait times 
for students wanting the next piece of information. The 
traditional PBL would end with a group discussion of 
what each of the case study results showed, and the cor-
rect diagnosis and management of the infant was dis-
cussed with the whole student group. 
E-Simulation Module 
The e-simulation was designed around a “virtual” 
infant named Malcolm, created through images, text, and 
animation that students could access from both desktop 
and 
mobile 
devices. 
Using 
a 
commercial 
e-learning 
authoring package (Articulate, 2019), and a suite of design 
packages (Adobe Photoshop, Adobe Illustrator), an end-
to-end training program was created which allowed stu-
dents to engage in infant diagnostic hearing testing with a 
simulated baby. The infant’s hearing levels, ideal testing 
process, and suggested clinical management were all iden-
tical to that of the traditional PBL. The pedagogical 
design of the module attempted to theorize a progressive 
approach to clinical education in a blended and fully 
online context. The “story” of the module was written in 
the second person, present tense—“You are testing the 
baby...” “Decide what you will do. . .” and so forth—to 
place the student in the position of a practicing audiolo-
gist in a clinical context (see Figure 1). 
Figure 1. Screenshot of instructions that students would receive while using the e-simulation. In this example, once the student selects 
“Next,” they are presented with the impedance values needed to make their next clinical decision. 
Students were presented with a selection of objective 
test(s) that they could perform on the imaginary infant 
and had to decide whether and when they needed to per-
form each one. The e-simulation was designed to be 
completely interactive, incorporating a variety of different 
buttons, dials, and icons that students could select to indi-
cate their decision. Students were instructed to choose spe-
cific details about the test setup (electrode placement, pre-
sentation levels, ear) and decide what to do when given 
information about aspects of their setup (such as skin-
electrode impedance values; see Figure 2). 
Each action, whether correct or incorrect, reveals 
another piece of the “story,” as the student works through 
the process of diagnosing an infant’s hearing. As they 
received more and more information, they were able to 
record this in a virtual notebook, accessed by clicking on 
a notebook icon. This notebook could be referred to any 
time, allowing users to review completed tests and their 
results. Throughout the e-simulation module, there is both 
formative and summative feedback. There was a level of 
performance assessment included, which was simulated 
true to the clinical experience, where students can make 
three mistakes before the baby “wakes up,” the test needs 
to be restarted, and an unsuccessful test is logged. Their 
objective is to minimize their mistakes, make good deci-
sions, and successfully diagnose the infant’s hearing loss, 
at which point, the summative assessment is drawn from 
the number of times that they woke the baby. This num-
ber translates into a ranking, the ranking is explained (see
Tomlin et al.: Simulations for Teaching Clinical Decision-Making
881
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

•
•
•
Figure 3), and they are encouraged to attempt the test 
again to “beat their score,” by diagnosing the baby’s hear-
ing loss with a reduced number of errors. 
Figure 2. Screenshot of a clinical decision required by the student during the e-simulation. Students can choose to click “Rescrub” or “Pro-
ceed with the test,” depending on what they think about the information given. 
In real  time, students also received feedback on the  mis-
takes made as they progress through the case. If students 
made an incorrect choice, a pop-up message would appear 
informing the students of this, along with an explanation 
about why their choice was considered incorrect. The incorrect 
pop-up message would also encourage the students to proceed 
by trying an alternative option (see Figure 4). Conversely, 
when students made a correct decision, they would receive 
a message confirming why this choice was appropriate. 
Although the e-simulation module was designed to 
be completed outside of the classroom, a 1-hr session was 
set aside for the 108 students to complete at their own 
pace on their own device. One staff member facilitated 
this session by ensuring that students were accessing the e-
simulation online and progressing through the required 
tasks (as well as answering any student questions that 
arose). The staff member also ran a prebrief and debrief at 
the start and at the end of the hour. In the prebrief, they 
covered the expected learning outcomes of the e-simulation 
and provided detail about how the e-simulation could be 
used. The case details such as the infant’s name and age 
were presented at the beginning of the e-simulation itself. 
In the debrief, the staff member facilitated group discussion 
and analysis of students’ decision-making throughout the 
e-simulation. There was also opportunity for student reflec-
tion of their experience virtually testing an infant’s hearing  
and to plan what they might do differently in the future. 
Figure 3. Screenshot of a possible final score and rank achievable by students upon e-simulation completion. 
Evaluation of E-Simulation Through Student 
Assessment 
The two cohorts of students were compared by score 
on the same question that targeted clinical reasoning 
within infant diagnostics contained within the end-of-year
882
American Journal of Audiology
Vol. 32
878–888
December 2023
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

exam, 4 weeks after PBL/module completion. To compare 
learning 
of 
students 
who 
completed 
the 
e-simulation 
module with the traditional PBL, a 10-mark short answer 
question was repeated across cohorts to enable direct com-
parison between the traditional PBL group and the e-
simulation module group. The examiner grading the ques-
tion was unaware of this study occurring and used an 
identical rubric to score students across the different 
cohorts. 
Figure 4. Screenshot of the e-simulation displaying an incorrect message shown to the student upon them selecting to “Proceed with the 
test” instead of choosing to “Rescrub.” 
The traditional PBL cohort results were collected 
retrospectively, so no further evaluation of their learning 
experience was analyzed. However, given the novel nature 
of e-simulation in audiology education, further evaluation 
of the e-simulation cohort was warranted. E-simulation 
students were invited to participate in the study prior to 
the scheduled teaching session via e-mail to the cohort. 
Students implied consent by completing an anonymous 
quiz prior to the teaching session, hosted outside of the 
learning management system. The quiz included a set of 
randomized multiple-choice questions, taken from a larger 
bank of questions, all addressing the concepts taught in 
the module. Students were then invited to retake the quiz 
at the completion of the module. This quiz did not con-
tribute to the students’ final course grade. 
Qualitative and Quantitative Evaluation of 
E-Simulation by Students 
E-simulation students were also asked to respond to 
an evaluation survey at the completion of the module. A 
Likert scale from 1 to 5, with 1 indicating strongly dis-
agree and 5 indicating strongly agree, was used to obtain 
responses to six questions, which can be found along with 
the mean ratings to each question in Figure 5. 
Figure 5. Student evaluation of e-module. 
The link for this was provided at the end of the 
module, was again anonymous, and was collated outside 
of the learning management system. Students were also 
requested to provide free text feedback and comments.
Tomlin et al.: Simulations for Teaching Clinical Decision-Making
883
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

•
•
•
The themes of these evaluation pieces are presented in the 
results section. 
Data Analysis 
This 
study 
used 
a 
mixed-methods 
approach 
to 
explore group outcomes, using both quantitative and qual-
itative methods. Data were collected between March 2018 
and November 2021 and appropriately anonymized prior 
to analysis. In order to determine whether subgroups 
could be combined (cohorts who completed the same 
teaching content in different years), a t test was applied. If 
a nonsignificant result was obtained, the groups were then 
combined to increase the power of the sample. A t test 
was also applied to test for between-group differences in 
final exam scores and between pre- and postquiz scores. 
Only group effects were able to be explored, as all data 
were de-identified. 
Survey data were collected, which provided both 
qualitative (Likert) and quantitative (open-ended survey 
questions on completion) data. Likert data were provided 
as a mean score for the cohort for each question. An induc-
tive 
approach 
thematic 
analysis 
was 
performed 
on 
responses to open-ended feedback questions about the stu-
dent’s perceptions on the simulations. Elements of the sim-
ulations were mapped to a framework where responses 
were coded and three themes were formed: components 
that the students liked and wanted to continue (positive), 
those they would like pending modification (suggestive), or 
components that the students wanted to discontinue (nega-
tive). The second author then cross-checked this analysis. 
Data extracts corresponding to each theme are quoted in 
the results section. All data were de-identified before analy-
ses. A p value of .05 was considered significant. 
Results 
Evaluation of E-Simulations Through 
Student Assessment 
Each 
group 
(e-simulation 
and 
traditional 
PBL) 
within each cohort was compared using the t test to deter-
mine whether any significant differences existed between 
the two cohorts. Two groups of 29 and 38 students 
formed the traditional learning group, with mean percent-
age final exam scores of 83.3% (±10.6 SD) and 78.4% 
(±11.3 SD), respectively. A t test was not significant, 
t(65) = 1.82, p = .07, indicating similar scores for each 
group. These two groups were then combined for further 
analyses. Two groups of 51 and 57 students formed the e-
learning group, with mean final exam percentage scores of 
77.5% (±18.5 SD) and 79.5% (±19.7 SD), respectively. A 
t test was not significant, t(106) = −0.55, p = .58, indicat-
ing similar final exam scores for each group. These two 
groups were then combined for further analyses. The final 
exam score of the two teaching methods was then com-
pared to determine whether either approach was preferen-
tial for learning. The mean percentage correct scores of 
the 
combined 
traditional 
learning 
(67 
students) 
and 
e-learning (108 students) groups were 78.5% (±19.1 SD) 
and 80.5% (±11.2 SD), respectively. A t test was not sig-
nificant, t(173) = 0.88, p = .38, indicating no significant 
difference in exam performance for each learning method. 
Eighty-three of the two cohorts totaling 108 students 
that completed the e-simulation instead of traditional PBL 
participated 
in 
the 
study, 
by 
completing 
the 
first 
presimulation quiz. Seventy-four students completed the 
second, postmodule quiz. Before combining the results of 
the two cohorts, within groups differences were again 
explored for the two separate cohorts. Mean premodule 
quiz percentage correct score for first e-simulation cohort 
(32 students) was 46.3% (±16.8 SD) and 52.0% (±12.2 
SD) for the second e-simulation cohort (51 students). A 
t test was not significant, t(81) = −1.67, p = .10, indicating 
no large difference in quiz scores for both cohorts. These 
two groups were then combined for further analyses. 
Mean postmodule quiz percentage correct score for the 
first cohort (28 students) was 70.4% (±12.0 SD) and 
70.8% (±17.2 SD) for the second cohort (46 students). A 
t test was not significant, t(72) = −0.13, p = .89, indicating 
no large difference in quiz scores across groups. These two 
groups were then combined for further analyses. Mean 
quiz scores were 49.8% (±14.3 SD) premodule completion 
(83 students) and 70.8% (±15.2 SD) postmodule comple-
tion (74 students). A t test showed a significant result, 
t(155) = −8.83, p = .00, indicating greater quiz perfor-
mance after students had completed the e-simulation. 
Qualitative and Quantitative Evaluation of 
E-Simulation by Students 
Sixty-four students provided quantitative feedback 
with 34 of these 64 students also providing written quali-
tative feedback. All questions scored above 4.3 on the 1–5 
Likert scale (see Figure 1). This indicated a positive evalu-
ation by the students and showed that the use of the e-
simulation as a learning activity was well received. 
A total of 34 free text comments were submitted by 
students, with 25 of them categorized into the theme of 
“positive,” six “suggestive,” and three “negative.” Com-
ments in the positive theme were those that did not offer 
feedback other than indicating benefit from the e-simula-
tion. These comments indicate that, for some students, the 
use of the simulation was viewed as a useful learning 
experience. The second theme of suggestive comments was
884
American Journal of Audiology
Vol. 32
878–888
December 2023
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

those that gave specific feedback on how to improve the 
e-simulation learning experience. Here, students felt that 
they could offer insight as to ways of improving the learn-
ing activity or sometimes reflected on aspects of the e-
simulation that could be altered for better usability. Third, 
negative-themed comments offered criticism without any 
suggestion of improvement. These comments tended to 
provide a sentiment of not accepting the e-simulation as a 
useful learning experience. Often these sentiments were 
quite limited in nature and did not offer insight on ways 
to improve the learning activity. Examples of comments 
from each theme can be found in Table 1. 
Discussion 
Given the increasing demand on the already limited 
placements in audiology, and the complex testing sur-
rounding infant diagnostic hearing assessment, there is a 
need for new models of teaching students the clinical 
decision-making skills necessary to accurately diagnose a 
hearing loss in an infant. The e-simulation that has been 
developed appears to be an effective platform for teaching 
audiology students the necessary infant diagnostic skills, 
with high levels of engagement, favorable feedback, and 
comparable student scores to the cohorts completing tradi-
tional PBL. 
Student Performance 
Clinical skill development requires complex decision-
making to be taught in a controlled environment (Prince & 
Felder, 2006). These skills would traditionally be developed 
through clinical placement experience; however, place-
ments cannot be assured to be evenly distributed among 
students, given varying complexity of cases and limits on 
availability. E-simulation has been introduced as a supple-
mentary tool to help counteract the reduction in hands-on, 
real-world experience in infant diagnostic assessment. With 
its inclusion, it was essential to determine that the e-
simulation did not compromise student understanding. 
Exam 
results 
demonstrated 
overall 
knowledge, 
and clinical skill reasoning was equal in both teaching 
approaches. Despite the growing student cohort across 
4 years (29 vs. 57 = 100% increase), we have evidence of 
a teaching style that is effective in leading to similar 
assessment outcomes and provides students with a com-
parable learning experience. 
Table 1. Examples of student comments in each of the three identified themes from the evaluation survey. 
Positive
Suggestive
Negative 
“Was fun and informative”
“Sound effects could be included for more emphasis”
 “Stop online modules” 
“Online modules and practice tests are good for us to 
practice test order. More of them would be helpful” 
“It would be useful to have an option for what you did 
emailed so that you could revise and check that 
you took the most efficient pathways” 
“Definitely keep up with the e-modules, more hands-on 
things like these are helpful with applying knowledge” 
“With the image of the baby where we put on the 
stickers, make spots a bit more obvious. . .  had a 
hard time figuring out the true position of a 
mastoid!” 
“I really enjoyed these PBLs. I found them very 
engaging, even fun!” 
“These would be very useful to have throughout the 
course, a very handy learning tool” 
The benefit of the individualized learning experience 
was also apparent, with students performing significantly 
better on the postmodule quiz. The active formative feed-
back provided at the time of the clinical reasoning likely 
helped students to immediately identify their gaps in 
understanding, providing a more effective teaching experi-
ence (van Gaalen et al., 2021). Consolidation of clinical 
application skills was driven by the summative assessment 
and feedback provided at e-simulation completion. Inde-
pendent learning was also promoted by the e-simulation; 
once the students finished, they were encouraged to make 
repeat attempts to improve their final score. 
Benefits of E-Simulation Compared to 
Traditional PBL Approach 
The inbuilt feedback in the e-simulation module 
appeared to facilitate engagement with the task and 
reduced the need for in-person instruction. The self-
directedness of the e-simulation allows for an individual-
ized learning experience that provides summative feedback 
without the need for multiple instructors or tutors, thus 
decreasing the strain on teaching resources. This is in con-
trast with the traditional PBL that is not only resource 
heavy (16 teaching hours compared to 1) but also lacks 
provision of timely and individualized feedback (Trullàs 
et al., 2022). Other advantages of the e-simulation over 
the traditional PBL include the ability to make scenarios 
more lifelike and realistic of the clinical scenarios students 
would be exposed to in a clinical encounter. The module
Tomlin et al.: Simulations for Teaching Clinical Decision-Making
885
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

•
•
•
can also be easily adapted to different cases with varying 
complexities at a low cost, allowing for scalability. Due to 
the modular nature of the design, new simulations could be 
added to the sequence, allowing for the cross-pollination of 
different design projects and for the development of a 
“library” of clinical simulations to be accessible. 
The usefulness of the e-simulation within the course 
has been highlighted during the recent pandemic, allowing 
students to complete the module at their own pace, in 
their own time and at a location that suits. The flexibility 
that the module provides in dissolving boundaries of time 
and space is appealing to current students, particularly 
with the increased demands placed with work life pres-
sures. Providing students with a way of learning clinical 
reasoning skills outside of dedicated class time or clinical 
placements (while being exposed to a consistent teaching 
approach) enables them to identify and fill in their own 
gaps in knowledge, rather than being left behind (which 
can be the case in the traditional PBL format). Ensuring 
that students have a safe place and flexibility to develop 
these complex clinical skills necessary for infant assess-
ment without the potential for patient harm is a signifi-
cant advantage of the e-simulation. 
Student Experience 
This study further supports the use of well-designed 
clinical e-simulations as not only providing equivalent skill 
development but also as being viewed positively by stu-
dents. There are clear benefits apparent in the approach 
that has been described, in terms of both maintaining stu-
dent outcomes and consolidating student engagement with 
learning material. Measured in this study are the com-
ments that show students clearly value this type of learn-
ing platform, with the highest score on the survey being 
the engagement question. The importance of student 
engagement is clear, with a strong link known to exist 
between engagement with subject matter and ability to 
retain information ultimately leading to graduate satisfac-
tion and course completion (Groccia, 2018). In addition 
to the high mean rating on the engagement-related ques-
tion, the free-text comments revealed more about the stu-
dents’ views of the e-simulation. Most comments in this 
study were positive and indicated a preference for more 
similar e-simulations following module completion, with 
some describing the learning activity as “fun.” A propor-
tion of students indicated high satisfaction and enjoyment 
from using the e-simulation, perhaps in part due to it 
being an active learning tool incorporating game design 
elements. This is in line with literature suggesting gener-
ally high-satisfaction rates with similar educational inter-
ventions (van Gaalen et al., 2021). A smaller number of 
comments 
were 
suggestive 
and 
indicated 
that 
some 
students identified areas for improvement with the e-
simulation module learning experience. The student learn-
ing experience could be improved in future e-simulation 
iterations by considering these user comments. It is of note 
that only three comments were found to be negative or 
indicative of rejection of the e-simulation module. As neg-
ative comments did not offer much detail about what was 
disliked or what aspect of the learning experience could be 
improved, it was difficult to draw any conclusions about 
why the students who made these comments held a nega-
tive viewpoint. Still, it is important to know that a pro-
portion of students seem to prefer other learning activities 
over the e-simulation approach used in this study. 
Although there are clear benefits to self-directed e-
learning, evidence does suggest that students prefer blended 
learning (Jin & Bridges, 2014; Petrarca et al., 2018). It is 
also possible that e-learning suits some students’ learning 
preferences over others. However, it is also likely that, for 
some students, introduction of technology places a higher cog-
nitive burden with assumed technical skills (Jin & Bridges, 
2014). To address this, a combination of e-learning and 
traditional/didactic educational experiences may be the 
ideal way to incorporate such innovations into a curricu-
lum. In doing so, e-learning is positioned as a complement 
to (rather than replacement of) traditional methods (Wu 
et al., 2022). Technology is best introduced into a teaching 
pedagogy not to replicate existing teaching practices but to 
transform the learning experience. The introduction of the 
e-module aims to achieve this, offering an alternative learn-
ing 
experience 
for 
complex 
decision-making 
processes 
required in infant hearing assessment. 
Limitations 
A limitation of this study is the sample included—we 
were only able to assess the different teaching methods and 
evaluate the e-simulation with students from the one clini-
cal audiology program at one university. It could be quite 
informative for future studies to involve students from dif-
ferent university programs, to further investigate the appli-
cation of e-simulation across various cohorts. Furthermore, 
as the traditional PBL cohort results were collected retro-
spectively, we were unable to evaluate the students’ percep-
tions of this learning activity. Although the e-simulation 
was well received by the students, we did not establish 
whether the traditional PBL was similarly received or if 
one was viewed superiorly to the other by the students. 
Conclusions 
The introduction of an infant hearing testing e-
simulation into a clinical audiology teaching program
886
American Journal of Audiology
Vol. 32
878–888
December 2023
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

offers a novel and engaging way of learning complex clini-
cal decision-making, 
without 
compromising pedagogy. 
The e-simulation module described allows students to 
explore, analyze, and make decisions about test order and 
assessments at their own pace, and safely without risking 
patient care. This type of module offers significant benefits 
to teaching and learning teams in terms of departmental 
assets and iterative resources. Modules can be easily 
updated, refreshed, and adjusted on an ongoing basis, 
ensuring that the content provides students with currency 
in terms of best practice and emerging research. Future 
studies should aim to measure students’ confidence in their 
clinical skills following the use of similar e-simulations, to 
further explore the impact of such innovations on their 
practice. Although this study was specific to the field of 
audiology, similar modules are likely to have equal value 
in all health science disciplines. 
Data Availability Statement 
The datasets generated during and/or analyzed dur-
ing this study are not publicly available due to ethical 
restrictions but are available from the corresponding 
author on reasonable request. 
Acknowledgments 
The authors thank the cohorts of audiology students 
who participated in this study and provided contributions 
to this research. The authors declare no specific funding 
source. 
References 
Alanazi, A. A., Nicholson, N., Atcherson, S. R., Franklin, C., Anders, 
M., Nagaraj, N., & Highley, P. (2016). Use of Baby Isao simu-
lator and standardized parents in hearing screening and par-
ent counseling education. American Journal of Audiology, 
25(3), 211–223. https://doi.org/10.1044/2016_aja-16-0029 
Amey, L., Donald, K. J., & Teodorczuk, A. (2017). Teaching clin-
ical reasoning to medical students. British Journal of Hospital 
Medicine, 78(7), 399–401. https://doi.org/10.12968/hmed.2017. 
78.7.399 
Articulate. (2019). Storyline 360 [Computer software]. https:// 
articulate.com/360/storyline 
Barrett, T., & Moore, S. (2010). New approaches to problem-
based learning: Revitalising your practice in higher education. 
Routledge. https://doi.org/10.4324/9780203846926 
Cook, D. A., Erwin, P. J., & Triola, M. M. (2010). Computerized 
virtual patients in health professions education: A systematic 
review and meta-analysis. Academic Medicine, 85(10), 1589– 
1602. https://doi.org/10.1097/acm.0b013e3181edfe13 
Coyne, E., Calleja, P., Forster, E., & Lin, F. (2021). A review of 
virtual-simulation for assessing healthcare students’ clinical 
competency. Nurse Education Today, 96, Article 104623. https:// 
doi.org/10.1016/j.nedt.2020.104623 
Dawkins, P., Hurley, P., & Noonan, P. (2019). Rethinking and 
revitalising tertiary education in Australia. Mitchell Institute. 
Duff, E., Miller, L., & Bruce, J. (2016). Online virtual simulation 
and diagnostic reasoning: A scoping review. Clinical Simulation in 
Nursing, 12(9), 377–384. https://doi.org/10.1016/j.ecns.2016.04.001 
Gorbanev, I., Agudelo-Londoño, S., González, R. A., Cortes, A., 
Pomares, A., Delgadillo, V., Yepes, F. J., & Muñoz, Ó. (2018). 
A systematic review of serious games in medical education: Quality 
of evidence and pedagogical strategy. Medical Education Online, 
23(1), Article 1438718. https://doi.org/10.1080/10872981.2018. 
1438718 
Groccia, J. E. (2018). What is student engagement? New Direc-
tions for Teaching and Learning, 2018(154), 11–20. https://doi. 
org/10.1002/tl.20287 
Intelligent Hearing Systems. (2022). Baby ISAO [Apparatus and 
software]. https://ihsys.info/site/en/training/baby-isao/ 
Jin, J., & Bridges, S. M. (2014). Educational technologies in 
problem-based learning in health sciences education: A sys-
tematic review. Journal of Medical Internet Research, 16(12), 
Article e251. https://doi.org/10.2196/jmir.3240 
Larue, C., Pepin, J., & Allard, É. (2015). Simulation in prepara-
tion or substitution for clinical placement: A systematic review 
of the literature. Journal of Nursing Education and Practice, 5(9), 
132–140. https://doi.org/10.5430/jnep.v5n9p132 
Mistry, K., Chetty, N. C., Gurung, P., & Levell, N. J. (2019). 
Digital problem-based learning: An innovative and efficient 
method of teaching medicine. Journal of Medical Education 
and 
Curricular 
Development, 
6. 
https://doi.org/10.1177/ 
2382120518825254 
Morgan, S. D., Zeng, F.-G., & Clark, J. (2022). Adopting change 
and incorporating technological advancements in audiology educa-
tion, research, and clinical practice. American Journal of Audiology, 
31(3S), 1052–1058. https://doi.org/10.1044/2022_AJA-21-00215 
Nisbet, G., McAllister, S., Morris, C., & Jennings, M. (2021). 
Moving beyond solutionism: Re-imagining placements through 
an activity systems lens. Medical Education, 55(1), 45–54. https:// 
doi.org/10.1111/medu.14345 
Petrarca, C. A., Warner, J., Simpson, A., Petrarca, R., Douiri, A., 
Byrne, D., & Jackson, T. L. (2018). Evaluation of elearning for 
the teaching of undergraduate ophthalmology at medical school: 
A randomised controlled crossover study. Eye, 32(9), 1498–1503. 
https://doi.org/10.1038/s41433-018-0096-1 
Prince, M. J., & Felder, R. M. (2006). Inductive teaching and 
learning methods: Definitions, comparisons, and research 
bases. Journal of Engineering Education, 95(2), 123–138. https:// 
doi.org/10.1002/j.2168-9830.2006.tb00884.x 
Reed, A., Andre, A., Ananthakrishnan, S., & Korczak, P. (2021). 
Effectiveness of simulation training on graduate audiology 
students’ auditory brainstem response testing skills. American 
Journal of Audiology, 30(2), 394–403. https://doi.org/10.1044/ 
2021_aja-20-00191 
Spence, D., Zambas, S., Mannix, J., Jackson, D., & Neville, S. 
(2019). Challenges to the provision of clinical education in 
nursing. Contemporary Nurse, 55(4–5), 458–467. https://doi. 
org/10.1080/10376178.2019.1606722 
Stow, J., Morphet, J., Griffiths, D., Huggins, C., & Morgan, P. 
(2017). Lessons learned developing and piloting interprofes-
sional handover simulations for paramedic, nursing, and phys-
iotherapy students. Journal of Interprofessional Care, 31(1), 
132–135. https://doi.org/10.1080/13561820.2016.1251404 
Trullàs, J. C., Blay, C., Sarri, E., & Pujol, R. (2022). Effectiveness 
of problem-based learning methodology in undergraduate
Tomlin et al.: Simulations for Teaching Clinical Decision-Making
887
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

•
•
•
medical education: A scoping review. BMC Medical 
Education, 22(1), 104. https://doi.org/10.1186/s12909-022-
03154-8 
van Gaalen, A. E. J., Brouwer, J., Schönrock-Adema, J., Bouwkamp-
Timmer, T., Jaarsma, A. D. C., & Georgiadis, J. R.  (2021). 
Gamification of health professions education: A systematic 
review. Advances in Health Sciences Education: Theory and 
Practice, 26(2), 683–711. https://doi.org/10.1007/s10459-020-
10000-3 
Watson, K., Wright, A., Morris, N., McMeeken, J., Rivett, D., 
Blackstock, F., & Watson, G. (2012). Can simulation replace 
part of clinical time? Two parallel randomised controlled tri-
als. Medical Education, 46(7), 657–667. https://doi.org/10. 
1111/j.1365-2923.2012.04295.x 
Wilson, W., Goulios, H., Kapadia, S., Patuzzi, R., Kei, J., 
Vitkovic, J., & Marshall, A. (2011). A national approach for 
the integration of simulated learning environments into audiol-
ogy education. Health Workforce Australia. 
Wilson, W. J., Schmulian, D., Sher, A., Morris, S., & Hill, A. E. 
(2020). Student perceptions of two simulated learning envi-
ronments in paediatric audiology. International Journal of 
Audiology, 59(1), 16–23. https://doi.org/10.1080/14992027.2019. 
1660004 
Windmill, I. M., & Freeman, B. A. (2013). Demand for audiology 
services: 30-yr projections and impact on academic programs. 
Journal of the American Academy of Audiology, 24(5), 407–416. 
https://doi.org/10.3766/jaaa.24.5.7 
Woda, A., Hansen, J., Paquette, M., & Topp, R. (2017). The 
impact of simulation sequencing on perceived clinical decision 
making. Nurse Education in Practice, 26, 33–38. https://doi. 
org/10.1016/j.nepr.2017.06.008 
Wu, Q., Wang, Y., Lu, L., Chen, Y., Long, H., & Wang, J. 
(2022). Virtual simulation in undergraduate medical educa-
tion: A scoping review of recent practice. Frontiers in Medi-
cine, 
9, 
Article 
855403. 
https://doi.org/10.3389/fmed.2022. 
855403
888
American Journal of Audiology
Vol. 32
878–888
December 2023
Downloaded from: https://pubs.asha.org Wikimedia Foundation, Inc. on 12/21/2023, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions 

