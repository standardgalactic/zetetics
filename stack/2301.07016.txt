Consciousness is learning: predictive 
processing systems that learn by binding 
may perceive themselves as conscious. 
 
V.A. Aksyuk,  
NATIONAL INSTITUTE OF STANDARDS AND TECHNOLOGY,  
100 BUREAU DR., GAITHERSBURG MD 20899 USA 
VLADIMIR.AKSYUK@NIST.GOV 
 
Abstract 
Machine learning algorithms have achieved superhuman performance in specific complex domains. Yet 
learning online from few examples and efficiently generalizing across domains remains elusive. In humans 
such learning proceeds via declarative memory formation and is closely associated with consciousness. 
Predictive processing has been advanced as a principled Bayesian inference framework for understanding 
the cortex as implementing deep generative perceptual models for both sensory data and action control. 
However, predictive processing offers little direct insight into fast compositional learning or the mystery 
of consciousness. Here we propose that through implementing online learning by hierarchical binding of 
unpredicted inferences, a predictive processing system may flexibly generalize in novel situations by 
forming working memories for perceptions and actions from single examples, which can become short- 
and long-term declarative memories retrievable by associative recall. We argue that the contents of such 
working memories are unified yet differentiated, can be maintained by selective attention and are 
consistent with observations of masking, postdictive perceptual integration, and other paradigm cases of 
consciousness research. We describe how the brain could have evolved to use perceptual value prediction 
for reinforcement learning of complex action policies simultaneously implementing multiple survival and 
reproduction strategies. ‘Conscious experience’ is how such a learning system perceptually represents its 
own functioning, suggesting an answer to the meta problem of consciousness. Our proposal naturally 
unifies feature binding, recurrent processing, and predictive processing with global workspace, and, to a 
lesser extent, the higher order theories of consciousness. We provide a qualitative but specific functional 
description of the proposed information processing architecture to facilitate experimental testing, 
refinement or falsification. While such a system is in principle straightforward to implement numerically, 
ethical implications of such numerical experiments ought to be considered carefully. 
 
 
 

Contents 
Abstract ......................................................................................................................................................... 1 
INTRODUCTION ............................................................................................................................................. 3 
PERCEPTION .................................................................................................................................................. 5 
Background – predictive processing framework. ..................................................................................... 5 
Hierarchical learning by binding – rationale. ............................................................................................ 7 
Unified episodic and semantic memory record and associative recall. ................................................. 10 
Perceptual unity. ..................................................................................................................................... 13 
Quickly forming causal connections entails a global workspace. ........................................................... 14 
Working memory enabled by selective attention. ................................................................................. 15 
Defining ‘consciousness’ and the ‘conscious contents’. ......................................................................... 17 
Transparency of perceptual models, higher order theories, ‘experience’ and ‘self’. ............................. 19 
The meta problem of consciousness. ..................................................................................................... 21 
ACTION ........................................................................................................................................................ 23 
Reinforcement learning of complex gene-proliferating actions. ............................................................ 23 
Value learning. ........................................................................................................................................ 25 
Action encoding in predictive processing. .............................................................................................. 26 
Action learning and exploration-exploitation tradeoff. .......................................................................... 27 
Imagination and thinking. ....................................................................................................................... 28 
Procedural knowledge and perceptual knowledge. ............................................................................... 28 
DISCUSSION ................................................................................................................................................. 30 
States of consciousness. ......................................................................................................................... 30 
Measurement of consciousness. ............................................................................................................ 31 
Outlook and open questions. .................................................................................................................. 32 
Summary. ................................................................................................................................................ 33 
BOX 1. Meeting the Hard Criteria for a theory of consciousness ............................................................... 33 
Empirical phenomena of consciousness being addressed by the proposal: .......................................... 33 
Meeting Hard Criteria: ............................................................................................................................ 34 
ACKNOWLEDGEMENTS ............................................................................................................................... 34 
REFERENCES ................................................................................................................................................ 34 
 

INTRODUCTION 
The field of machine learning advanced explosively in the last 3 decades and enabled numerous practical 
applications in the areas of image and language processing and autonomous control. Deep learning and 
reinforcement learning (Sutton and Barto 1998) principles enable machines that learned chess, go and 
computer games from scratch (Silver et al. 2018; Mnih et al. 2015), and left humans well behind in 
performance. However, while deep learning, artificial neural networks and reinforcement learning were 
largely inspired by the biological systems, the overarching information processing principles in the 
biological brains remain poorly understood. In biological systems in-situ learning and behavior adaptation 
have been honed by evolution to confer survival and reproduction advantages despite their significant 
energetic and other costs. Meanwhile, learning machines are still unable to efficiently learn online, in the 
real world, from few examples and to incrementally combine and generalize useful behavior patterns 
across domains (Kaelbling 2020) – the tasks mammals excel at. In humans this type of learning can be 
linked to forming declarative memories, which is an indicator of conscious information processing. Thus, 
understanding consciousness is not only one of the preeminent intellectual challenges of our time, but, 
viewed from the fundamental statistical and machine learning perspective, may lead to radical practical 
advances enabling the next generation of artificial learning agents. 
Since the turn of the century the problem of human consciousness has been at the focus of intense and 
growing experimental, theoretical and philosophical attention. While initial empirical investigations were 
organized around searching for the neural correlates of consciousness (Crick and Koch 1990), more 
recently a variety of conceptual frameworks and theories of consciousness (ToCs) (Seth and Bayne 2022) 
have been advanced to provide the intellectual scaffold for orienting the empirical studies. However, 
despite the proliferation of ToCs, they remain largely incongruent, with each one seemingly focusing on 
specific aspects of consciousness and the corresponding neurobiological and empirical data (Doerig, 
Schurger, and Herzog 2021). There is currently no common theoretical paradigm connecting across the 
field and no clear agreement as to the exact meaning of the term ‘consciousness’. All leading ToCs contain 
important empirically supported insights, but there is currently no conceptual approach that can unify 
these descriptions as aspects of a common model. Ideally, consciousness would be understood as an 
inherent aspect of a broader unified model of cognition seamlessly including attention and affect together 
with learning, perception and action, describing how the perceptions and reports of the ‘subjective 
experience’ may arise, and how the system’s overall function confers an evolutionary advantage. 
Maintaining organism homeostasis, while crucially important, is only one of many gene proliferating 
strategies that are evidently being implemented through the general and flexible adaptation and control 
enabled by the brain. If efficient and general learning is a core function of this system, and declarative 
learning is a key part of it, so is consciousness.  
Declarative learning, proceeding through working memory (Kandel, Schwartz, and Jessell 2000), is directly 
related to consciousness – we form declarative memories of things we are ‘conscious of’. While 
declarative learning can dramatically fail in fully conscious humans with certain types of amnesia, such 
failure may be understood as downstream of consciousness. The ability to quickly learn by dynamically 
forming novel associations of multiple perceptions and actions in ‘working memory’ remains intact, 
enabling flexible and adaptive behavioral response to novel stimuli, despite the failure to retain and 
consolidate (optimize) this new knowledge for future use via the short- and long-term memory. It is 
therefore compelling to consider consciousness as a manifestation of a learning process. The relationship 
between consciousness and learning figures prominently in several ToCs (Lamme 2006; Cleeremans et al. 

2020; Cleeremans 2011; Birch, Ginsburg, and Jablonka 2020; Singer 2001). Notably, consciousness has 
been connected to dynamic binding of known perceptual features for representing novel compound 
objects (Singer 2001; Crick and Koch 2003; Treisman 2003), and both learning and forming dynamic 
connections between perceptual features are key in the recurrent processing theory (Lamme 2006).  
Here we develop a conceptual proposal for the functional organization of a biological or an artificial 
conscious agent as a learning system that combines learning by binding with predictive processing (PP) 
(Hohwy and Seth 2020; Hohwy 2020; Clark 2013) for perception, and active inference (K. Friston et al. 
2017; K. J. Friston et al. 2010; Parr and Friston 2019) and reinforcement learning (RL) (Montague, Dayan, 
and Sejnowski 1996; Schultz, Dayan, and Montague 1997; Cohen et al. 2012) for action. Functionally, the 
proposed learning architecture constantly posits and tests new perceptual hypotheses by introducing new 
causes that minimize the largest and most time-correlated prediction errors, attempting to find common 
hidden causes that bind these otherwise-unpredicted perceptual inferences. The specific perceptual 
features that are being bound are what the system perceives as its conscious contents, and the process 
of hypothesis generation is perceived as ‘being conscious’. We describe how the typical reports referring 
to ‘conscious experience’ originate from perceptual representations the system learns to infer about itself 
and how the proposed functional organization entails these representations, including unified yet 
differentiated perception, short and long-term memory formation and associative recall, attention and 
working memory, affect, multimodal thinking, action and different types of learning. 
Neural implementations of our proposal’s three constitutive information processing functions – PP, 
feature binding, and RL – have been broadly discussed in the literature, with neural circuits proposed for 
PP and identified for RL, and feature binding variously related to recurrent processing (Lamme 2006), 
neural coalitions (Crick and Koch 2003) and synchronization in the cortico-thalamic system (Singer 2001). 
Furthermore, we will argue that our proposal entails as its consequences the key insights underlying global 
workspace (Baars 1995; 2005; Dehaene and Changeux 2011; Mashour et al. 2020) and high order theories 
(Cleeremans et al. 2020; Cleeremans 2011; Brown, Lau, and LeDoux 2019). Lastly, our answer to the meta 
version (Chalmers 2018) of the hard problem (Chalmers 1995) proceeds largely along the illusionist 
approach, similar, for example, to the one described recently in the context of the attention schema ToC 
(Graziano et al. 2020).  
In the following manuscript, the first major section introduces the learning functional architecture and 
discusses its consequences with the focus on perception. The second major section expands the 
architecture to include action learning and optimization by future value estimation and RL. The first 
section introduces learning-by-binding as a natural complement to the conventional PP learning, enabling 
Bayesian learning of new categorical causes for sensory data. We then explore the implications of this 
architecture, arguing that it directly entails formation of a unified perceptual representation and a 
corresponding memory record accessible by associative recall, as well as the appearance of a global 
workspace, whereby binding immediately connects perceived features to a broad range of possible 
actions, including maintaining them in the workspace by selective attention. We discuss how conscious 
contents are defined by this functional account of the proposed learning architecture. Finally, we discuss 
the transparent and hierarchical nature of the learned perceptual models, including the higher order 
perceptions of ‘experience arising’ and ‘self’, and argue how transparency gives raise to the hard problem 
illusion.  

In the second section we consider an information processing system evolved for learning, via temporal 
difference RL (TDRL) (Sutton and Barto 1998), action policies that implement multiple survival and 
reproduction strategies. We discuss how perception may be optimized for guiding action and estimating 
the future value of the current state needed for TDRL. We adapt the active inference approach of 
representing both actions and perceptions within the same categorical PP model, whereby actions may 
manifest as selective attention and imagination as well as motor execution. We argue that the learning-
by-binding mechanism, applied to actions, rapidly and directly modifies the ongoing action policy by 
making new cross-predictive links between perceptions and actions – forming declarative action 
memories, subject to RL rules. The TDRL framework with explicit future value estimation allows action 
learning toward distinct survival and reproduction goals implicitly encoded via separate categories of 
neuro-biologically generated internal reward signals. We believe that learning by binding applied to deep 
generative models for perception and action results in the sample-efficient, generalizable, compositional 
and incremental online learning, which is the hallmark of conscious information processing in humans and 
which is currently lacking in artificial agents (Kaelbling 2020).  
 
PERCEPTION 
Background – predictive processing framework. 
Within the predictive processing framework, we first consider a simplified perception-only system that 
learns, over long timescales, a generative model of incoming sensory data and perceives by dynamically 
inferring the learned causes at each moment to minimize sensory data prediction errors. A perfect 
prediction would be achieved if using the inferred causes and the perceptual data at a given moment in 
time, the system updates the causes at the subsequent times such that the model exactly predicts the 
subsequent sensory data. However, no prediction is perfect, and the inferences of causes are dynamically 
updated based on minimizing the local prediction errors within the model, implementing approximate 
Bayesian inference. Moreover, the same perception errors are used to improve the model by learning 
over the long term, modifying its causal structure, such as by quantitatively changing how individual 
causes are inferred from and, in turn, predict their ‘feature’ causes, as well as by learning new causes with 
predictive power over sensory data.  
In a typical PP model (Figure  1a), causes are interconnected in a deep hierarchical architecture with a bi-
directional, generative dynamic relationship. Within such a generative model, the causes predict the 
likelihoods of their features down to the sensory data, as well as their own and other cause likelihoods 
forward in time. While higher-level causes influence their lower-level features via prediction, the 
prediction errors from features in turn dynamically change the inferred likelihoods of the higher-level 
causes. Learning conventionally proceeds by finding mutual activation weights, that is, inference and 
prediction strengths minimizing the local prediction errors over the long term. These locally-hierarchical 
relationships express the key assumption (induction bias defining the hypothesis space) that the sensory 
data is described by causal relationships between wholes and their features. Additional regularizing 
assumptions may be used, such as, for example, particular global hierarchical topology, sparsity within 
subsets of causes (e.g., mutual exclusion), or particular time dependence such as finite persistence. 
Both continuous-valued and discrete (categorical) causes can be combined within PP models (K. J. Friston, 
Parr, and de Vries 2017). The simpler categorical causes are described by single scalar values denoting the 

likelihood of a cause being present, i.e., a single continuous-valued precision is the only number associated 
with each discrete cause. For example, how likely is the ‘cat’ explanation for the current sensory data, on 
a scale from 0 to 1? Continuous-valued scalar and vector quantities and more general continuous-valued 
objects can also be represented by categorical causes as likelihood distributions on discretized maps that 
can be 1-dimensional, 2-dimensional or have more complex topologies, and can have global or local 
likelihood normalization, smoothness and other regularization rules imposed among the map elements. 
This is consistent with many map-like neural column areas across the neocortex. 
 
Figure 1. Predictive processing and learning new causes by feature binding. A generative predictive 
processing model (a) encodes the world and action state within a hierarchy of discrete connected 
elements (middle). Each element (right) encodes a scalar state of a single cause (e.g., a categorical cause 
likelihood) and the corresponding scalar prediction error. Multiple elements may interact to represent 
probability distribution maps (top right). The causes higher in the hierarchy predict the lower ones (blue 
solid arrows) and receive their prediction errors (red dashed arrows) for inference. The lowest levels 
represent sensory inputs with externally-given states and actuation outputs with the prediction-dictated 
states. To maintain the predictive power in the presence of processing latencies, the cause dynamics at 
the higher levels is slower than at the lower levels, such that higher levels encode more persistent and 
more abstract causes. (b) illustrates the theorized learning mechanism for new categorical causes. 
Whenever several feature causes, such as shapes, colors, locations, etc., are inferred coincidentally and 
exceed a specific (prediction error) x (duration) threshold, such coincident unexplained causes are 
attributed to a new common cause via learning by binding. The common cause is provisionally added to 
the model and immediately thereafter predicts the features that are thus bound. As the time passes, if 
the new cause is inferred repeatedly, its predictions are refined based on experience and consolidated. 
On the contrary, without repeat inference the prediction strengths are gradually attenuated to zero, and 
the new cause is thereby discarded. 
 

The overall PP model is encoded by the network of all learned causes and their prediction-strength links 
to each other. The model is dynamic, with some links embedding time delays to account for sensory 
latency, feature asynchrony and prediction in time. We will refer to this conceptual PP modeling 
description in the following discussion and will further expand it to include learning future-value and 
behavior policy via RL and to discuss active inference. However, the specific details of the model are not 
critical for our central arguments. The key model features are the encoding of distinct inferred causes, 
prediction errors for each cause and the bi-directional, generative inference-prediction relationships. A 
cause is inferred from, and predicts, its feature causes such that each cause’s precision (inferred 
likelihood) adjusts dynamically, over short timescales, to locally minimize prediction errors.  
The local PP learning rules for long-term modification of prediction strengths to minimize the prediction 
error over multiple epochs remain active across the hierarchy. In particular, at the lower levels within each 
perceptual domain, statistical regularities in sensory data are a function of the physics of the world and 
the sensors. Therefore, low-level sensory cortexes have evolved specific arrangements of cortical maps 
and initial connectivity topologies to facilitate rapid and massively parallel statistical learning via these 
local prediction update rules. 
Hierarchical learning by binding – rationale. 
Our key proposal is a new type of induction bias which can be added to PP systems for efficient online 
learning (Figure 1b). Typically, the conventional PP learning rules attribute the prediction errors to 
imperfectly learned prediction strengths between the already-existing causes and learning proceeds by 
adjusting these strengths to minimize errors. We propose an additional, separate type of learning that 
attributes errors to the existence of yet-unknown causes, and attempts to learn such causes, first 
provisionally, and then permanently. Consider a case when several significant prediction errors occur 
together, closely correlated in time, and there is no known common cause that can already be inferred to 
simultaneously predict all of them. Furthermore, suppose each individual prediction error is from a cause 
that has been used many times previously and learned extensively, that is, this cause’s relationship to the 
known causes, including its own features, has already been well learned over many examples. It is an 
intuitive assumption to tentatively posit a new cause accounting for all the coincident prediction errors – 
to assume that the causes with significant concurrent errors are due to (are features of) a common 
previously-unknown cause.  
Different sensory modalities have different latencies; therefore, PP models have to predict causally-
related features that are asynchronous in time, fusing them across modalities. They also must learn to 
predict the future state from the current one. Thus, individual causes must be able to predict features 
spread in time, including prediction of delayed features at their proper times. Common causes must be 
found for not only concurrent events, but time-correlated events, including temporal sequences spread 
over hundreds of milliseconds, consistent with postdictive effects, such as color-phi and meta-contrast 
masking (Herzog, Drissi-Daoudi, and Doerig 2020), as well as large latency differences between sensory 
domains. The proposed learning process forms such causes by binding features with time-correlated but 
asynchronous prediction errors, across up to few hundred millisecond long delays, depending on the 
perceptual modality. The resulting new-bound causes are functionally the same as other causes within 
the PP model and can be inferred from, and predict, their feature’s relative timing (see further discussion 
of timing effects below and in Figure 3). 

When the unpredicted features are first bound into a new cause, such cause is a temporary hypothesis, 
based on one observation. Once it is no longer actively perceptually inferred, it can only exist as a new 
cause in the PP hierarchy over a short initial time period, a short-term memory timescale of seconds to 
tens of seconds. If the cause is repeatedly or continuously inferred within that time, thereby providing a 
consistent reduction of prediction errors, it is retained within the PP model, while otherwise it is discarded 
(forgotten). Compared to well-learned causes, the initial feature prediction strengths of a new cause are 
more plastic, subject to large learning updates according to the usual PP learning rules each time the cause 
is inferred. If it is not repeatedly inferred, its prediction strengths gradually attenuate with time. With 
increasing cumulative activation of the new cause both the active update size and the passive attenuation 
rate decrease, and ultimately the small residual plasticity and zero attenuation are reached as the cause 
becomes a permanent part of the PP model.  
This can be viewed as Bayesian updates of the beliefs about the causal structure of the world, distinct 
from the Bayesian inference of the current state of the world. A new belief, represented by the new added 
cause, has a low initial precision and undergoes large updates with each new observation. The precision 
gradually increases, and the updates decrease with the number of observations and their duration. The 
new causes lacking sufficient cumulative activation are completely discarded – their prediction strengths 
attenuated to zero – to decrease the model complexity and increase the predictive power. After many 
repeat activations, plasticity of the well-learned causes reaches a low but finite limit, to account for the 
possibility of the world changing on long time scales. After this transition from short-term to long-term 
memory, the well-learned causes are no longer deliberately forgotten over time: the lack of use does not 
constitute evidence of uselessness.  
We suggested that declarative memory formation is entailed by the learning-by-binding mechanism 
added within the PP framework, thus providing the rationale for the declarative memory function from 
the perspective of perceptual Bayesian learning. However, the apparent semantic complexity and 
structure of individual declarative memories, associatively recallable as a single unit, suggests that this 
binding process is both parallel and hierarchical. Multiple groups of prediction errors, time-correlated 
within each group, are simultaneously detected within and across sensory domains. The corresponding 
groups of unpredicted inferred causes are separately bound into multiple new causes based on 
correlations within but not between each of them. The total number of concurrent separately-bound 
groups may be limited – keeping the number of hypothesized new causes small would serve a complexity-
reducing regularization function. Furthermore, a cause recently formed by binding, being new, cannot 
itself be a feature predicted by any known causes. Therefore, each time such new-bound cause is inferred 
for a sufficient duration from its features, it necessarily has a large prediction error and is subject to further 
binding with other near-concurrent or time-correlated unpredicted causes. Thus, the binding process 
itself is hierarchical and builds shallow trees binding multiple new, as well as previously-known, causes 
together, and culminating in a single root cause, which unites a large fraction of the unpredicted features 
of the present perceptual moment.  
In one example (Figure 2a), a ‘red’ ‘loud’ ‘car’ located ‘on the left’ is perceived as a new compound cause 
for the unpredicted concurrent perceptions of ‘redness’, ‘loudness’, ‘car’ and activation of a particular 
‘leftward’ location on a spatial map. Perhaps head orientation and repeated visual saccades to this 
location, triggered by the auditory system’s sound localization, create high temporal correlation of 
activation between a particular place ‘on the left’ in the body-centric-frame spatial map, the 
corresponding visual field map location, and the specific visual inferences of ‘red’ and ‘car’. The concurrent 

auditory inferences describe the specific sound – ‘loud’, etc. – with the auditory localization activating the 
same body-centric location. All these multimodal perceptions have high prediction errors – they are not 
expected – and are highly temporally correlated with each other and with (directing attention to) a 
particular spatial location. Thus, the system binds a new cause ‘The Car’ which predicts a ‘red’ ‘loud’ ‘car’ 
‘on the left’, now represented as a unified compound object. Similarly, a ‘green’ ‘tree’, of a particular ‘tall’ 
height, may be simultaneously perceived at a specific location ‘on the right’ as a new unified object ‘The 
Tree’. 
In addition to the direct sensory perceptions, the hierarchical PP model may also, simultaneously, 
represent higher-order causes that describe the perception process itself. A cause may be inferred 
representing the ongoing ‘experience’ of some objects, distinct from the mere presence of these objects 
– if one looks away, the objects may still be inferred as present but no longer as being ‘experienced’. 
Moreover, a cause of ‘self’ may also be inferred, to which this ‘experience’ is attributed to belong to. In 
this schematic example, the new-bound causes for ‘The Car’ and ‘The Tree’ are bound with the higher-
order causes for ‘I have’ ‘ongoing experience’ to form a top-level episodic cause that includes ‘I experience 
The Car (a red loud car on the left) and The Tree (a tall green tree on the right).’ These become 
associatively recallable as a unit and may be also unified with other concurrent unpredicted perceptions. 
The hyperparameters of the learning by binding – the minimum prediction error size, activation duration 
and maximum asynchrony between features that can still lead to binding – vary across sensory domains 
and PP hierarchy levels, such as between single-domain, multimodal and abstract (amodal) mental 
objects. In biological brains they are optimized by evolution to efficiently infer typical structures in sensory 
data within each domain, and in particular those structures that are most useful for guiding evolutionary-
advantageous actions. These binding parameters, as well as plasticity of prediction strengths vs. 
cumulative activation and consolidation/forgetting timescales can be empirically elucidated, such as by 
analyzing existing cognitive studies data. As we will specifically discuss in the next section, these 
parameters directly affect both perception and learning of events in time. The hyperparameters are 
modulated on the short and long time scales by global state variables, such as the affective states, and 
exploration-exploitation balance discussed in the following sections, whereby the slow global variations 
define the global states of consciousness (Bayne, Hohwy, and Owen 2016). As we elaborate below, the 
major functional role of perception is to estimate future value for reinforcement-learning an optimized 
action policy, encoded within the same PP generative model. Therefore, on short time scales high positive 
or negative valance strongly and dynamically modulates the learning-by-binding parameters to provide 
both value-relevant perceptual learning and value-increasing action policy learning. 
To recap, temporary new causes are hierarchically learned from single- or few-examples by binding of 
perceptions with time-correlated prediction errors, according to an induction bias that large, correlated 
prediction errors must be due to common hidden causes. This learning-by-binding process operates 
continuously on top of the PP framework. The causes are subsequently retained and consolidated or 
discarded depending on whether they are repeatedly inferred and thus consistently reduce the prediction 
errors. This learning architecture is a conjecture that remains to be confirmed by statistical learning theory 
and numerical machine-learning experiments to demonstrate that rapid learning by binding within the PP 
framework is indeed possible and stable. However, analyzing this proposal and how it may relate with 
established data and theory may advance understanding of human perception and learning, including the 
elusive concept of consciousness. Therefore, we will now discuss some properties entailed by it, and how 
these properties correspond to functional processes commonly associated with consciousness. 

 
Figure 2. Predictive processing combined with learning by binding entail commonly recognized mental 
phenomena. (a) Learning by binding functions hierarchically, further binding multiple known and new 
object causes with large prediction errors into a unified perceptual representation for the present 
moment – a shallow tree structure encoding the unified yet differentiated content describing an episode. 
(b) When some of the same features, such as the ‘motor sound’ localized ‘on the left’ are inferred again, 
within a similar context, this may lead to the inference of the previous episode and ‘The Car’ object, and 
prediction of the hidden features, such as the shape and color of the object, in the process of the 
associative recall. (c) An action of attention to a feature, such as a specific spatial location, increases the 
likelihood of that feature. When one or more features of an object are thus predicted, the object may be 
maintained in perception even in the complete absence of any feature’s direct inference from sensory 
data. This describes maintaining the object in working memory by selective attention. (d) When an object 
is formed by binding, its features predict each other. This causal connection between features makes the 
object globally available. In response to a prompt “What color is the car”, the feature prompt “car” 
increases the likelihood of the car shape, which predicts the color red via “The Car”. The red color 
combined with the color query prompt triggers the action of the red color report. 
Unified episodic and semantic memory record and associative recall. 
As described above, at any point in time the newly-bound cause which is top-most in the PP hierarchy 
provides the root-cause hypothesis for a large fraction of the persistent prediction errors, giving a single 
unified episodic description of a large fraction of perceptions about the present moment that are unique, 
i.e., not predicted by other ongoing perceptions (Figure 2a). By virtue of being bound into a unified whole, 
the various perceptions can be associatively recalled together. This unity is a result of the induction bias 
inherent in this learning system, which assumes that concurrent unpredicted events have a common 
cause (see discussion in the Perceptual Unity section, below). The top-most, episodic, cause and all the 
newly-bound causes that are its features, together, give the best concise generative description of the 
sensory data leveraging the known PP model. I.e., the few-layer deep tree of the newly-bound causes can 

be thought of as a compressed representation of the present moment using the already-learned PP causal 
structure as the perceptual code vocabulary, which is, however, generative and dynamic.  
To be bound into a new cause, the features must have prediction errors – inferred likelihoods above 
predictions from all other causes – and the errors must exceed certain thresholds in size and duration or 
their product (Figure 3a,b). The hierarchical binding of new causes further, e.g., into an episode, requires 
additional persistence time. Therefore, anything that decreases the size and time-persistence of 
prediction errors interferes with binding, such as when unpredicted object stimuli are presented for a 
short duration and followed by masking to control the duration of their inference by the PP model (Figure 
3a). This relates the new cause binding with the experimental control variables of duration and 
presentation contrast (Figure 3b). In another paradigm case, a set of features is presented simultaneously 
or in succession, and then a posteriori predicted or, more accurately, postdicted by inferring a single 
known common cause within the conventional PP (Figure 3c,d). Such features do not have sufficient 
duration of large prediction errors to be individually bound into a new cause before the known cause is 
inferred, and only the common cause, if it remains unpredicted for long enough, becomes bound (Figure 
3d, left). Therefore, even if the features are strongly inferred by the PP for an extended period, they are 
not associatively recallable separately from their cause – this cause may be bound to become reportable 
and actionable, but the individual features are not.  Specifically, it is impossible to recall which particular 
feature combination has led to the recallable instance of the cause’s inference. This accounts for the 
postdiction effects in experiments showing mandatory integration of time-sequenced features (Herzog, 
Drissi-Daoudi, and Doerig 2020) as well as unconscious integration of features into wholes, generally.  
Learning by binding not only entails the mandatory unconscious feature integration (Figure 3c,d), but also 
allows reinterpretation of the apparent discreteness of time in postdiction (Herzog, Drissi-Daoudi, and 
Doerig 2020; Drissi-Daoudi, Doerig, and Herzog 2019). Whenever a feature remains unpredicted for a 
sufficiently long period, it is bound and recallable. Whenever two features are postdicted by a common 
cause before the first feature can be thus bound, neither feature is bound and only their common cause 
may be bound and recallable (Figure 3d, left). Whenever a third feature is inferred after the common 
cause for the first two features is already inferred, the prediction errors of the first two features are 
already low, and only their common cause and the third feature can be bound into a recallable episode 
(Figure 3d, right). Inferring a known cause with a long postdiction interval is only possible if feature binding 
into a new cause does not occur faster than that interval. Therefore, the binding timescales directly dictate 
what time sequences can be directly perceptually represented by single causes. This leads to the causes 
higher up in the PP hierarchy representing generally longer time intervals and correspondingly having 
longer binding times, perhaps up to ≈1 second. The PP modeling is thus regularized by enforcing more 
sparce time-persistent representations at higher hierarchy levels, to both optimize predictive power and 
maintain stability in the presence of physical latency of deep inference.  

 
Figure 3. Learning by binding explains time domain observations. (a) For high visibility stimuli S binding 
(dashed box) occurs after a large prediction error ε persists for a minimum time τ and masking introduced 
at ∆t > τ does not interfere with binding (left). Masking at time ∆t < τ prevents binding by decreasing the 
inferred likelihood of S (right) prior to binding. (b) For stimulus with larger prediction error binding occurs 
faster (left) while when contrast is decreased the binding is delayed and may not occur within the finite 
stimulus duration (right). (c) When unpredicted stimuli S1 and S2 are separated by ∆t > τ, they are bound 
separately (upper). When the separation ∆t is short compared to τ, the near-coincident stimuli are bound 
together into a cause representing a sequence (S1 ∆t S2) with a specific separation ∆t (lower). (d) Left: if a 
sequence cause has already been learned, it is inferred and postdicts the S1 and S2, reducing their 
prediction errors with a short latency time delay tlatency << τ. The inference is mandatory. The sequence 
cause (S1 ∆t S2) is subject to further binding, while the individual stimuli S1 and S2 are not. Right: If a third 
stimulus S3 occurs a short time δt after the S1 and S2 such that tlatency < δt < ∆t, the (S1 ∆t S2) will be inferred 
before S3, and after time t they may be bound together into a new sequence ((S1 ∆t S2) δt S3). The 
mandatory nature of the (S1 ∆t S2) integration and the condition ∆t < τ explain the experimental 
observation of the apparent discreteness of conscious perception in time. 
Depending on the level in the PP hierarchy, the binding timescale may be between ≈200 ms (e.g., from 
low-level visual modality) all the way to, perhaps, ≈1 s for multimodal and amodal perceptions. However, 
the persistence timescale of the continuous or repeated inference of the newly-bound causes can be 
much longer, governed by the continuous activation by the action of attention and the competition with 
perceptual distractors. This attention-mediated perceptual dynamics spans at least ≈3 s and likely can be 
much longer with attention training (Srinivasan, Tripathi, and Singhal 2020). We thus provide  mechanistic 
functional account for the three time ranges corresponding to cinematic, extensional and retentional 
levels in (Singhal and Srinivasan 2021), which are defined by the two separate domain-dependent binding 

and attentional dynamics timescales. Note that attentional feedback operates on both fast, pre-binding 
(therefore, unconscious) as well as the slow timescales, as further discussed in the Action section. 
The process of associative recall (Figure 2b) is entailed by our functional architecture, without assuming 
any additional mechanisms. Each newly-bound cause, up to and including the episode, is added to the PP 
model, at least temporarily, which is what allows those features that are bound together to be recalled, 
i.e., re-inferred, associatively. Associative recall occurs whenever enough of these bound features are 
present with sufficiently high precision for the newly-bound cause to be inferred, to reduce the prediction 
errors. The inferred cause, in turn, predicts the remaining bound features, which acquire non-zero 
likelihood and are thereby associatively recalled.  Upon recall these features affect inferences of other 
causes and predict their own features according to PP, thereby contributing to perception and manifesting 
as priming and automatic action triggering. Whenever the new-bound cause encodes a time-sequence, 
e.g., in auditory modality, time sequences of bound features are replayed when associatively recalled, as 
the cause predicts the feature’s timing. The recall may occur unconsciously (i.e., is not itself recallable) if 
no further binding occurs, however the highest-level recalled causes may have prediction errors of 
sufficient size and duration to be re-bound into the new episode cause, which in turn becomes available 
for later recall. With repeated recall, the prediction strengths made by the new-bound causes are both 
updated and further consolidated, until only the residual plasticity of the recall reconsolidation remains.  
Consider the above example where a new compound cause ‘The Car’ has been formed by binding. If a 
verbal prompt for ‘red’ increases the precision of ‘red’ via auditory perception, the system might infer the 
‘The Car,’ which decreases prediction error for the unexpected high-likelihood ‘red’. This inference will be 
more likely within the common perceptual context, i.e., if many of the other features of the recent 
episode-cause containing ‘The Car’ are also present, such that the whole episode might be partially 
recalled as well, predicting ‘The Car’. Recalling ‘The Car’ activates its features, including the ‘on left’ 
location in the spatial map.   
Furthermore, the binding and record formation process is strongly modulated by selective attention. As 
discussed below in the context of perception and elaborated in the Action section, functionally the 
selective attention to specific locations, attributes or objects are learned actions, parts of the ongoing 
action policy encoded within the generative PP model. Such actions strongly modulate, but are 
functionally separable from, the core processes of perceptual inference, binding, and the resulting 
declarative memory record formation. New causes can be formed for unattended objects and even 
further bound into the recallable episodic memory record.  
The described ability to learn and recall specific new objects and events entails creation of a perceptual 
record of declarative memory that can be accessed later by the system via associative recall. If a new 
cause bound from a single or few examples is repeatedly re-activated by similar examples, it is generalized 
to a broader example class by the conventional PP learning rules. This process converts the initially 
episodic knowledge into semantic perceptual knowledge.  
Perceptual unity. 
The induction bias underlying the learning-by-binding system presumes everything unpredicted is 
stemming from common hidden causes, even when events are unrelated and coincident by chance. If the 
assumption is that any two or more unpredicted causes must have a common hidden cause, a singular 
episodic cause is generated necessarily (Figure 2a). This is apparent in the associative and episodic recall, 

where a variety of causally independent contents are bound and recalled together by virtue of simply 
having occurred at the same time. The “what it’s like” of perceiving a specific instance of a cat, as we may 
learn from recalling it, not only includes the visual features that give rise to the inference of the cat, but 
also includes inferences about the attention state, expectations and other internal and external conditions 
that modulate the perceptual processing. Moreover, ‘the cat’ is bound together with other concurrent 
related and unrelated inferences, including (the action of attending to) its specific spatial location, 
distinctive attributes of ‘the cat’, nearby objects, as well as other concurrent unpredicted perceptions in 
multiple modalities. A large fraction of these contents is associatively recalled when the cat is recalled. 
This gives the process of perception a unified nature quite distinct from simply representing a set of 
separable contents. This unified yet differentiated nature, a hallmark of the content of consciousness 
(Bayne 2012), is entailed by the learning architecture proposed here.  
On the other hand, the full unity of binding is not required. Multiple new causes are continuously formed 
by binding of various concurrent unpredicted features at the low sensory levels. Whenever one or more 
new causes are formed, but do not persist long enough to become further bound with other concurrent 
causes into an episode, they are not recallable as part of the episode and are not unified with it. Yet the 
cause binding the new combination of features is formed and can later contribute to perception and 
behavior. This binding has been empirically demonstrated for unattended visual stimuli (Meuwese et al. 
2013), whereas masking the stimuli prevented them from binding. We further argue that the new bound 
cause formed without attention in this work could not include as one of its features the action of attending 
to its spatial location, color or another one of its own attributes. This may explain the extra learning via 
feedback required in test trials in (Meuwese et al. 2013) to connect the new cause to the appropriate 
response action. 
A separate way in which the proposed learning system is unified is that it is designed to learn and 
implement an actions policy that maximizes survival and reproduction benefits and for this purpose it 
generates a single, unified perceptual estimate of the future value – whether the current state is “good” 
or “bad” overall. While this future value may be generated by combining multiple distinct specific future 
values for each of the perceived causes, and there may be more than one kind of reward, a single scalar 
total value is calculated for action learning by the system as a whole – e.g., whether the actions that 
immediately led to this state should be retained for the future or discarded. The details of this process are 
discussed in the Action section below. 
Quickly forming causal connections entails a global workspace. 
Beyond example-efficient learning using the full power of previous perceptual knowledge, and formation 
of recallable current-state descriptions, the learning by binding also entails rapidly forming generative 
causal connections between previously-unrelated causes (Figure 2d). Whenever a new-bound cause is 
inferred by the PP model from a subset of its features, the remaining, hidden features are then 
generatively predicted. For example, once a temporary cause ‘The Car’ binding otherwise-unrelated 
features ‘car’-object, ‘red’ color, ‘left’ location and ‘motor’ sound is formed, the cause ‘The Car’ may be 
inferred from the presence of only ‘car’-object, inferred from a prompt for ‘car,’ and predict specific color, 
location and sound. Thus, within a few hundred milliseconds needed to bind ‘The Car’, the ‘car’-object 
feature starts to predict the ‘red’ color. The predicted features in turn contribute to inferring dependent 
higher-level causes, as well as predicting lower-level features. Within active inference this includes 
triggering or modulating actions connected to, for example, ‘red’ within the PP hierarchy, such as selecting 

a specific color report in response to a prompt. Therefore, the temporary binding immediately and 
strongly modifies the overall PP generative model dynamics – perception and action inferences and 
predictions made immediately thereafter – by allowing a set of previously-unrelated causes , such as ‘car’-
object and ’red’ color, to effectively cross-predict each other once they are bound together. 
This functional property, entailed by the addition of feature binding to PP, directly connects our theory to 
the global workspace theories of consciousness: the newly-bound features constitute the contents of a 
global workspace by virtue of being able to cross-predict each other and thereby being able to influence 
inference and prediction of a large number of other perceptions and actions. Dynamically binding a set of 
select causes into a new shallow-tree generative model structure allows the bound perceptions to 
immediately modulate many actions they previously had no functional connections to, and thereby to 
exert what appears to be a “global” causal influence. With inferred A, when perceptual inference or 
selective attention activates B, this now activates C and thereby modulates any model dynamics 
dependent on C. Functionally, this may also be viewed as passing of objects by reference into procedures 
that can now act on them, whereby different objects C, D or E, including previously unknown, newly-
bound ones, when temporarily bond to A and B may be  referred to (activated by) whenever some 
procedure activates A and B.  
Using ‘ The Car’ and ‘The Tree’ as an example, and considering ‘point to a location’ as an action previously 
learned for all available locations, the prompt of: “Point to the car (tree)”, can be responded to by first 
the ‘car’ (‘tree’) of the prompt inferring ‘The Car’ (‘The Tree’), which then predicts the bound location ‘on 
left’ (‘on right’), while the prompt also infers the ‘point to a location’ action. The action executes by 
pointing to the correct active location. Another prompt might be: “What color is the car (tree)?”, whereby 
the generic object name will again activate a specific bound object and thereby its color attribute that can 
then be reported. We see that the two compound objects, once bound, became available to a range of 
previously learned actions. Most importantly, these objects may comprise never previously seen 
combinations of generic attributes, yet they can immediately be acted upon in a large variety of very 
complex ways that have been previously learned. This type of flexible generalization is perhaps the most 
important feature of the proposed learning architecture. 
Notably, this type of global availability requires neither information broadcast separate from the binding, 
nor existence of neural codes other than the PP model, since only previously-learned actions triggered by, 
and acting upon, specific previously-learned objects (causes) are being proposed, all within PP with active 
inference-like description for actions as the PP predictions. To further explore the implications of the 
global availability enabled by binding, in the following example we will illustrate how a novel compound 
object becomes available to be maintained in working memory via a previously-learned action of selective 
attention to a spatial location or another generic attribute. 
Working memory enabled by selective attention. 
So far, we have been describing a learning system for perception, i.e., for predicting sensory data, while 
only mentioning that the PP system can also implement actions, as have been proposed by active 
inference theories. In the Action section we describe actions as causes making predictions, increasing the 
inferred likelihoods of their target causes within the generative PP model. Such actions may manifest (and 
be perceived and recognized) as instances of selective attention.  Within the PP framework, selective 
attention describes a selective increase of precision, which for a discrete, categorical cause is an increase 

of its likelihood, or what we have been also calling activation level. In the following we explain how 
working memory may arise by the interaction of binding, PP and selective attention (Figure 2c). 
A generic action of selectively attending to a particular spatial location, or to the most-active spatial 
location – i.e., an action of continuously maintaining or intermittently reactivating that spatial map 
location – may be learned to be triggered by certain PP model causes. For example, overt attention to a 
location of a perceptually present object modulates the physical sensory apparatus, increasing the 
precision (the likelihood or the ‘strength’ of inference in the categorical PP model) and duration of the 
sensory features comprising the object. The object is inferred with high likelihood, and the object’s large 
prediction errors facilitate its binding into the episode. Similarly, covert attention increases compound 
object’s precision and facilitates binding by selectively increasing and sustaining high precision of one or 
more of its features. For ‘The Car’, attention to a specific location ‘on left’ may result in its sustained 
activation and becoming part of the present episode. If the car is present and attention is overt, perceptual 
inference from sensory data will maintain ‘The Car’ at high likelihood. However, even if the car is absent 
and attention to the spatial location is covert, the particular ‘on left’ is bound to the car, therefore its 
activation, within the appropriate perceptual context, can help maintain ‘The Car’ at a nonzero likelihood, 
even in the absence of the direct sensory perception. A novel compound object can thus be continuously 
perceived in the absence of direct sensory stimuli. Since a novel object cannot be predicted by any other 
cause, it will be bound with other concurrent unpredicted causes, therefore participating in the global 
workspace and in a series of consecutive recallable episodes. We argue that this is precisely what it means 
for the object to be maintained in the working memory.  
Notably, the action of imagining a never previously seen compound object, i.e., attending directly to it, 
rather than to its location or other generic features, could not have been learned within the PP model. In 
contrast, attending to a generic feature of the object, such as color or location, could have been learned, 
thereby allowing maintaining any compound attended object with that feature in working memory. To 
accomplish this, a system may have a well-learned action of selective attention to the most active (highest 
likelihood) spatial location or to one of a set of predetermined spatial locations. Such action will activate 
the objects and features bound together with those specific locations (and the present perceptual 
context), even if these objects are otherwise not fully inferred from sensory data. Thereby, binding 
enables maintenance of arbitrary, never previously perceived compound objects in working memory 
without the need to rapidly learn to attend to or to imagine each and every one of the object’s features. 
This explanation of working memory relies on binding and reciprocal activation between spatial neural 
maps, generally in the front of the cortex, and objects and features represented in the back cortex, 
consistent with neuroimaging observations in working memory tasks. However, there are multiple 
modalities of working memory and not all of them work through attention to a spatial location – the 
auditory buffering being one example.  
As described, the binding process dynamically connecting separate causes together and allowing global 
workspace effects and maintenance in working memory is functionally separate from the short-term 
storage and long-term consolidation of declarative memory – the ability to retain the bound causes within 
the PP model over extended periods without inferring them. The latter functions are known to 
physiologically depend on hippocampus and nearby structures. Consistent with empirical data, within our 
theory the memory storage deficits do not grossly impair the working memory and formation of the global 
workspace via cross-prediction. Our view adds key functional descriptions elaborating the previously 

made connections between feature integration and neural coalitions, formation of a global workspace 
and the neural modifications during recurrent processing.  
We have used the term ‘attention’ here only in one specific sense to conceptually illustrate how 
maintenance of objects in working memory may arise in the proposed information processing system 
architecture. We have previously stated that maintaining an object in working memory by selective 
attention is not a necessary condition for it to undergo binding, and objects unattended in this narrow 
sense can be bound and can enter the associatively recallable memory record. However, ‘attention’ covers 
a wide variety of phenomena in different contexts and, considering the term more broadly, we see that 
within the proposed architecture the increase of precision in the absence of a top-down prediction is 
precisely what results in binding, formation of a record and global availability. Hence everything in the 
record may be seen as having ‘received attention’ in a broader sense of there having been an increase in 
estimated likelihood. This includes either bottom-up attention via the normal perceptual PP inference or 
top-down attention via an action, or both. Within this broader sense of ‘attention’, the only possible cases 
of separation between the perceptual record contents and the ‘attended’ contents are the cases where 
the action of top-down attention is known to have been applied to certain objects, but the objects were 
not bound or were bound but are not recallable, e.g., the new cause was quickly forgotten. This tight link 
between attention and binding in out account finds some parallels with the observations underlying the 
attention schema theory of consciousness (Graziano et al. 2020). However, when we consider the 
meaning of the term consciousness, we do not equate it with either attention or the schema for it. 
Defining ‘consciousness’ and the ‘conscious contents’. 
So far, we have been describing clearly defined functions of perceptual inference, binding, recall, etc., 
rather than referring to consciousness, conscious contents (CC) or conscious experience. This is both 
because there are multiple ways in which these terms are currently operationalized in different contexts, 
as well as to ensure avoiding Cartesian materialism (Dennett 1991), such as saying that something is being 
‘experienced’ or ‘consciously experienced’ without first defining what that means, mechanistically. On the 
contrary, the described formation of a recallable unified and differentiated perceptual record that binds 
causes which are uniquely descriptive of the present moment is testable against experimental data and 
introspective intuitions. Our account entails that this record is both imperfect and transitory. 
Furthermore, once temporary binding occurs, it enables immediate generative cross-prediction between 
the bound attributes, and thereby allows the new compound causes to be maintained in working memory 
by attention and to modulate a broad range of actions via a global workspace-like functionality. These are 
concrete functional, mechanistic relationships that learning-by-binding imposes on the inferences made 
by the system. When describing cross-prediction appearing as a global workspace we are not positing a 
mental or neural “theater”. Rather than ‘broadcasting’ binding temporarily connects existing causes in 
novel ways resulting in novel inference and prediction relationships between them. The effect is the 
strictly-local codependent cross-activation between the bound percepts. The “actors” in such a local 
group are the only audience. Furthermore, there is no spotlight – attention is not understood that way. 
The bound contents, i.e., the causes inferred by PP with sufficiently large and persistent prediction errors 
and bound into a shallow tree structure of new and known causes forming a perceptual episode, satisfy a 
common criterion for contents to be access-conscious: being available for thought and rational action. 
The cross-prediction within the bound contents allows each element to modulate all actions that can be 
triggered (predicted within active PP) by each of the other bound elements.  

Given the proposed functional combination of PP and binding, and the resulting conceptual explanation 
for the access consciousness, we further ask whether there is a broader ‘phenomenal consciousness’ to 
be defined or described, either as a real phenomenon or as an illusion. This includes two types of related 
questions which we will address in this section, in reference to our functional architecture: 
1. Is there a consistent definition of ‘phenomenality’ that reasonably corresponds to the various 
relevant intuitions and is operationally useful for describing empirical data? 
2. Is there any conceptual flexibility in defining which perceptual contents should be called CC? If 
so, what is the most operationally convenient and useful definition? 
a. What types of contents are or are not conscious? 
b. Are CCs rich or relatively sparse? How can this be empirically determined? 
To the first question, in the subsequent section the intuitive ‘phenomenality’ is attributed to the 
perceptual inferences of ‘experience arising’ or ‘I am having an experience’, which are inferred by the 
system to account for its own perceptual contingencies, in addition to, and distinct from, the invariant 
outside world objects. Specifically, in the same way as the cause ‘red’ can be bound with the cause ‘car’ 
forming ‘the red car’, the cause ‘experience of’ can be bound to ‘the red car’ to form ‘the experience of 
the red car’, a compound cause that includes phenomenality as one of its attributes.  This type of 
phenomenality is a perception, an inference the system makes, taking its own modeling process into 
account to generate better predictions. As noted below, unlike higher order theories, we do not link the 
inference of such higher order perceptions to granting ‘conscious’ status to any lower order perceptions.  
To the second question, in our view, the CC are composed from various learned causes, including 
temporary bound ones, and their status as within or outside the CC is fully determined by whether they 
are being (a) inferred with a nonzero likelihood, and (b) bound. Causes that are not inferred at a given 
moment, having near zero likelihood, are not part of the CC. Similarly, known causes that are fully 
predicted within the current state of the PP model, having near zero prediction errors, only contribute to 
the perception and action but do not participate in learning by binding or in the global workspace resulting 
from binding. While contributing to priming, their effects may be reasonably defined as unconscious. This 
unconscious PP perception and action can be highly complex, while proceeding outside the learning by 
binding and not entering the perceptual record. The non-declarative learning of perception and action 
also proceeds unconsciously, via the prediction strength updates withing the existing PP cause network, 
according to our view. Such non-declarative learning is gradual, in contrast to the learning by binding, 
which quickly and strongly connects unrelated causes. 
This argument limits the CC to those causes that participate in binding. As discussed in the perceptual 
unity section above, a large fraction of such causes is hierarchically bound into a shallow tree structure 
representing an episode and forming a global workspace. The bound episode structure typically includes 
actions of attending to some of the contents within the episode. Simultaneously, causes not participating 
in the current episode are being continuously formed by binding. Once formed, they may or may not be 
further bound into the new episode, depending on whether they themselves exceed the required 
persistence time and prediction error threshold. The possibility of forming new bound causes that are not 
bound into the episode and do not participate in the global workspace reconciles the divergent views on 
the CC within the recurrent processing and global workspace ToCs, explaining both views and making the 
distinction a matter of the CC definition. From the perspective of declarative learning as the defining 
function of consciousness, the broader CC definition that includes the locally-bound causes outside the 

episode and global workspace seems operationally useful. However, the narrower CC definition excluding 
such causes is also coherent and fits better the reportable, easily recallable and perceptually unified 
contents.  
Given either definition, perceptual richness becomes an empirical question of what perceptual content is 
being bound or being bound together. In a visual domain example, a broad range of data is available to 
the senses. A narrower range is being sensed depending on the direction of gaze. That sense data narrows 
the broad range of all visual inferences available within the existing PP model to those that best match 
the data. Out of all the inferences made that are not fully predicted by other inferences, a few subsets are 
persistent and correlated enough to be bound into new causes. Of all those a narrower subset forms a 
unified tree of the global workspace and episode. Of those, even less are retained over time and become 
available for a delayed recall. Notably, at each stage our intuition may overestimate the richness by 
confusing access with representation, and the richness question must be answered by carefully controlled 
empirical studies.  
For example, in the Sperling’s experiment and its variations (Sperling 1960; Sligte, Scholte, and Lamme 
2009) multiple unpredicted visual symbols are all perceived, but a specific subset ends up being bound to 
and actively maintained in working memory by the selective attention to the experimentally prompted 
spatial location. While not all symbols are recallable, it is an empirically unresolved question whether all 
symbols have been bound or only the prompted subset – this question may be addressed by an 
experiment similar to the (Meuwese et al. 2013) study of learning without attention, applied to the 
unprompted symbols. While the narrower definition of the CC as being bound together definitively 
excludes the larger symbol set, by the broader CC definition the symbols would be conscious if they can 
be empirically shown to be specifically bound to each other or other unpredicted concurrent stimuli. 
Lacking such evidence, the more likely hypothesis is that all symbols are perceived but only the prompted 
symbols are maintained, by attention to the prompt’s spatial location, long enough to be bound and 
become CC.  
This generally illustrates how out of a wide set of inferred unpredicted causes only a fraction enters the 
record or is acted upon, and out of the same set, different fractions may end up bound together depending 
on perception and attention contingencies, such as different internal states or additional concurrent and 
subsequent stimuli. All inferences that have been bound or only subsets maintainable by attention, 
available to trigger action or recallable from memory record can be validly included in different possible 
consistent definitions of the CC. On the other hand, a person may intuitively reason and describe a much 
wider superset of causes that must have been perceived and could have been bound, attended to, 
triggered action and recalled, under some loosely-defined set of possible contingencies, as having been 
in a richer ‘total phenomenal consciousness field’.  
Transparency of perceptual models, higher order theories, ‘experience’ and ‘self’. 
The proposed functional architecture learns ever more sophisticated Bayesian generative models of the 
time-dependent sensory data stream. This model construction process is hierarchical, adding new causes 
modeling regularities in the lower-level causes inferred by the existing models: the results of existing 
models serve as input data for further modeling in a learning process that is, in principle, limitless. In 
specific organisms the model depth and structure are limited by their brains’ particular neural 
architectures. 

The learning by binding creates an associatively recallable record consisting of the variously bound causes 
that were not fully predicted at specific times. As a matter of this perceptual record, these bound causes 
are the only things that comprise it and can be recalled by the system. They may be represented as ‘having 
occurred’ if an appropriate model for representing past events has been learned. The majority of these 
perceived and recallable causes are not additionally recognized and represented as being outputs of its 
perceptual models, but appear to the system as direct perceptions, the elementary units constituting the 
perception record. In other words, the models giving raise to these inferences are both transparent (not 
directly perceivable) (Metzinger 2003) and cognitively impenetrable from the system’s perspective when 
the process of perceptual inference is not itself represented and inferred by a suitable learned perceptual 
model within a system.  
However, the hierarchical-learning system learns causal structures for predicting the contingencies of its 
own perceptual and modeling apparatus and the object-and-event inferences made by it. I.e., the system 
learns to predict how its perception of objects and events is modulated by the presence of other objects 
and events, including the states of the physical sensory apparatus, of the perceptual model and of the 
system more broadly. To do that the system learns to infer causes describing such states of the perceptual 
model and the physical body and learns models that represent objects and events as not only ‘occurring 
out there’, but also being ‘perceived’, or ‘not perceived’ within the system in a way that make the event 
recallable and available for a broad range of actions. In other words, the system learns to infer that an 
‘experience’ of an event has occurred within the system. Moreover, in its continual learning to more 
accurately predict both what it perceives and how it perceives it, the system might learn to infer various 
other causes predicting its own perceptual modeling states, such as whether something was perceived 
clearly or not and how attention was directed or distracted. 
As with all causes, the new inference of an ‘experience occurring,’ together with the inferences about the 
characteristics of perception at each moment, are subject to being bound into the perceptual record by 
the learning process. Thus, the perceptual record includes these directly-perceived higher-level 
representations of the perceptual process. Generally similar types of high order representations are 
discussed within the higher order theories of consciousness (Cleeremans et al. 2020; Cleeremans 2011; 
Brown, Lau, and LeDoux 2019). However, rather than postulating them, here we argue that the higher 
order perceptions are entailed by our learning-by-binding functional architecture, as inferences within the 
learned hierarchical model. In contrast to higher order theories, in our account the higher order 
perceptions are not necessarily about the specific low order perceptions and inferring them does not 
determine whether a given low order perception enters or does not enter the declarative memory record 
or can modulate a large range of actions via the global-workspace-like effect of binding. In our view, 
systems with simple models having limited or no higher order representations can have the binding and 
the resulting global workspace and associative recall functions we associated with consciousness. 
However, the high order perception of ‘experience occurring,’ inferred by a transparent model for this 
high order perception, amounts to the system representing its low order perceptions as having a 
‘phenomenal’ character, i.e., as being ‘experienced’. Notably, the higher order model only detects a 
characteristic of the lower order model output, and does not represent that the low order inferences are 
made by a model, allowing the lower order model to remain transparent.  
Transparent perceptual models span a range from basic, such as the illumination-invariant color 
recognition to highly complex, such as the model for ‘self’ and the “phenomenal models of the 
intentionality relation” based on it (Metzinger 2003; 2005; 2020b). A broad range of self-models, including 

procedural and declarative in addition to the perceptual ones, have been discussed as the implicit beliefs 
comprising m-consciousness by (Graziano et al. 2020). Following this line of reasoning, a sufficiently 
sophisticated system may learn to infer animistic ‘agent’ causes for describing people and animals and, 
furthermore, learn to perceive itself is one such agent. Thereby it learns to infer the cause of ‘self’ as an 
‘owner of experience’ (“I have an experience”), and the ‘source’ of action, and infer ‘beliefs’, ‘motivations’ 
and other properties ascribed to such agents and to ‘self’. Based on these perceptions the system may 
also learn the actions of reporting and discursive reasoning with the belief of having/being a ‘self’. This 
view of the self has parallels with the ones described by Thomas Metzinger (Metzinger 2003), Tim Bayne 
(Bayne 2012) chapter 12 and others (Graziano et al. 2020). As a cause inferred within a transparent model, 
the ‘self’ is clearly perceived as truly existing, as a matter of systems’ perceptual record. However, this 
perceived ‘self’, like many naïve perceptions, is an illusion in the sense that the reality is different from its 
perception. Prior to an accurate scientific theory of the information processing in the brain, which is the 
ultimate goal motivating this work, ‘experience occurring’ and ‘self’ are naïve and transparent perceptual 
models learned by the system as part of modeling to predict its perceptions of its own functioning.  
The meta problem of consciousness. 
The meta problem of consciousness (Chalmers 2018) is the problem of explaining why we may think there 
is a hard problem (Chalmers 1995) of consciousness. To solve the meta problem is to explain why within 
a particular learning system the perception of ‘experience’ necessarily arises, and why this perception 
appears to resist reductive explanation. Within our hypothesis the model making the higher order 
inference ‘an experience is occurring’ is a transparent model of the same kind as sensory perception 
models inferring ‘trees’ and ‘cats’ (Figure 2, Figure 4).  The system simply represents it as presently 
occurring with high likelihood, rather than recognizing it as a result of any modeling or reasoning. The 
recognition of ‘an experience occurring’ is of the same kind as the recognition of a ‘tree’ or a ‘cat’, as a 
matter of its consequences within the system’s functional structure such as the global availability or the 
perceptual memory record. Whatever the system learns to recognize, whether about the outside world 
or about itself, appears to it as experientially-given truths, at least until it can learn to perceive and 
represent otherwise. As the system makes inferences about its own processes of perception and learning, 
‘experiencing’ appears to it to objectively exist, in exactly the way ‘cats’ and ‘trees’ appear to exist (Figure 
4). 
We suggest that the ‘explanatory gap’ difficulty lies in the previous theories’ inability to envision a 
mechanistic functional description entailing occurrence of the direct perceptions of the ‘experience 
arising’ and ‘I am having an experience’ of the same kind as perceptions of objects and events attributed 
by the system to the outside world, such as ‘cats’ and ‘trees.’ Meanwhile, from our system’s perspective 
both the higher and the lower order perceptions are equally apparent. The higher order perceptions 
appear to it as directly perceived, the same way physical objects are perceived, therefore higher order 
perceptions demand the same kind of ‘objective’ explanation used for physical objects. People do not 
routinely question the ‘objective’ existence of cats and trees and typically do not perceptually recognize 
them as mere constructs – learned model inferences made by our minds to predict raw sensory data. 
Similarly, people feel ‘experience arising’ as no less than an ‘objective’ fact. Physical theories describe the 
external objects as being ‘out there,’ independent of the process of perception. Thus, we expect and 
demand the same kind of objective explanation for the perception of ‘experience arising,’ yet without 
referring to the process of perception this is logically impossible.  

Here we provide the same kind of description for the object perceptions and the higher order perceptions, 
both Bayesian inferences made by the system to predict the sensory data (and distinct from the physical-
science objects as predictive descriptors within formal models for the various aspects of the world). One 
fallacy leading to the appearance of the illusory hard problem is the treatment of the perceptions of 
physical objects and events as direct, veridical representations of physical reality, rather than as constructs 
we learn to infer. These constructs are based on the outside world only insofar as describing predictable 
regularities in the sensorium-dependent sensory data using a particular hierarchical causal structure – the 
hypothesis space of the learning system implemented by the brain. The perception of ‘experience’ is 
simply another learned perceptual construct, inferred to describe predictable contingencies in the 
perceptual modeling inferences at a lower level. The functional, mechanistic account of the process of 
perception and learning proposed here explains how the directly-felt ‘I am experiencing’ becomes part of 
the system’s perceptual record and reportable working memory. Inferences made by transparent 
perceptual models appear to the system as objectively existing things and events including the ‘experience 
arising,’ the ‘self’ and the ‘consciousness’ in the ‘I am experiencing’ and ‘I am conscious’.  
 
Figure 4. Meta hard problem of consciousness. The system makes perceptual inferences about external 
world and about its own internal states, such as to represent whether an object is sensorily experienced 
or imagined. The internal state cause of “having a sensory experience” has the same properties as the 

physical object cause of “The Tree”: both are clearly apparent and cannot be unperceived. Without 
understanding the nature of perceptions as learned and inferred causes describing internal and external 
states, the internal and external perceptions appear qualitatively different. For example, physical objects 
are understood by naïve observers as properties of the outside world independent of the system 
perceiving it. Importantly, they system with this functional organization must learn to perceive internal 
states and thereby is able to perceive, remember and report having experiences. 
ACTION 
Reinforcement learning of complex gene-proliferating actions. 
Evolution selects for maximal gene proliferation. Genes are proliferated in complex organisms via a 
combination of multiple interacting mechanisms, including maintaining approximate homeostasis for the 
organism’s lifetime while maximizing reproduction, offspring survival, certain types of group cooperation 
and using other gene proliferating strategies. Systems maintaining homeostasis have attracted 
considerable attention because according to the free energy principle (K. Friston 2019) all such systems, 
including inanimate ones, can be viewed as self-evidencing in information-theoretic sense (Hohwy 2016). 
However, for biological organisms this is a partial view – the flexible and adaptive information processing 
functionality of the evolved brain is clearly broader and supports implementation of multiple 
evolutionary-advantageous strategies in addition to maintaining homeostasis. 
How could a perception- and action-learning system be organized to serve these evolutionary purposes? 
One well-known and general machine learning approach is reinforcement learning (Sutton and Barto 
1998), in which a system learns to perceive and act to maximize the discounted future cumulative reward 
– the future value, which the system learns to predict from the reward signals generated by (neural) 
mechanisms outside the learning system proper. RL is, arguably, the most successful currently known 
machine learning approach for complex action optimization, used in self-driving cars (Thrun et al. 2006; 
Kiran et al. 2022) and the best self-learning game-playing algorithms (Silver et al. 2018).  
The remainder of this section elaborates on the second key point in this work, the view that brains 
implement temporal difference reinforcement learning (TDRL) (Montague, Dayan, and Sejnowski 1996; 
Schultz, Dayan, and Montague 1997; Redish 2004; Cohen et al. 2012) of action policies beneficial for 
survival and reproduction (Figure 5). Here we propose that in biological organisms multiple specific, 
relatively simple and shortsighted, evolutionary-old neural reward mechanisms have evolved to detect 
situations with immediate positive or negative consequences. These detectors address body homeostasis, 
external danger, reproduction, as well as more complex species-specific variables, such as social status. In 
mammalian brains amygdala and the value system (nucleus accumbens and ventral tegmental area (VTA)) 
are neural structures known to process such rewards. By receiving input from, and projecting to, the 
sensory, associative, and prefrontal cortexes and the corresponding thalamic regions, they form a 
combined system for not only learning to represent complex regularities in sensory data, but for using 
them to estimate the future value of the present state based on the history of positive or negative 
rewards, and to predict such rewards. These future value estimates are then used to learn complex 
hierarchical action policies maximizing them, i.e., maximizing the best-estimate cumulative benefit for 
gene proliferation.  
In view of recent work by (Jeong et al. 2022), we emphasize that we do not propose the narrow TDRL for 
learning perceptions and cue-reward associations, which is soundly rejected by this work, but rather 

describe a RL mechanism for learning evolutionary-value-maximizing action policies. In the following 
section we start by describing the future value learning via attributing value to remembered past cues 
based the presently-detected reward prediction errors. Our value learning is precisely the learning of 
contingencies between past perceptions and “meaningful” events, that is, occurrences of significant and 
unpredicted positive or negative reward. Therefore, our proposal is largely in agreement with both the 
theory and the empirical observations in (Jeong et al. 2022). 
 
Figure 5. Predictive processing with active inference learns to maximize cumulative future value via 
reinforcement learning. The predictive processing model combines perception and action, which have 
distinct learning goals: perceptual causes predict future value accurately, while actions maximize it. 
Immediate rewards of several types are calculated directly from external and body sensory data. Each 
perceptual cause can contribute a positive or negative value of each type. Predictive value estimation via 
perception is learned by attributing reward prediction errors backward in time to recently-active 
perceptual causes. Action policy is improved by increasing (decreasing) the future likelihood for 
advantageous (detrimental) actions: overall positive (negative) reward prediction error is used to sensitize 
(inhibit) the connections that triggered the recently-active actions.  
 

Value learning. 
Future value can be perceptually estimated by learning specific future values for each of the learned 
causes and by summing these values, weighted by each cause’s presently inferred likelihood. These cause-
specific future values can be learned locally by a conventional online TDRL algorithm: the future-value 
surprise (reward prediction error) is propagated back by modifying the specific values of the recently-
active causes. For learning the specific values, TD update size may follow the above-described Bayesian 
logic distinguishing the recently-formed from the well-learned causes: larger value updates are applied to 
the less well-known values of the recently added causes, while the update size (plasticity) is decreased 
with the number of times a cause is inferred, approaching a fixed low limit. The TD value learning is local, 
requiring only that the specific value strengths for recently-inferred causes remain plastic over a short 
reconsolidation time, perhaps seconds to tens of seconds, similar to the conditioning timescales, and that 
the single global value error (value surprise) be widely distributed back to each of these causes 
represented in the cortex. In other words, the future value is a special type of cause, which is computed 
forward from a large number of perceptual PP causes, with weights learned from the reward prediction 
errors via the TDRL rule. The widely distributed reciprocal connections between the amygdala and the 
value system on the one hand and the thalamocortical system on the other are generally consistent with 
this view. These value-estimating and value-learning connections should not be confused with the 
dopaminergic signaling from the VTA largely to the front of the cortex. Within the RL framework, the value 
system uses the dopaminergic pathway to signal the value surprise, i.e., the reward prediction error, 
forward to areas encoding action, such that the recent actions which may have caused an unpredicted 
value estimate increase or decrease (surprise) are reinforced or suppressed, respectively (Cohen et al. 
2012).  
Since we have suggested multiple reward signals provided by distinct neural detectors of specific gene-
proliferation-relevant states, several different types of future values may be simultaneously estimated, 
separately predicting the future cumulative sum for each reward type or for their multiple specific 
combinations. These values place the current state within a multidimensional affective state space 
corresponding to the multiple emotions with positive or negative valence that can be present 
simultaneously. These distinct future value estimates are ultimately combined to generate the single 
dopaminergic phasic reward prediction error output from VTA to the frontal thalamo-cortical system for 
action learning. 
Within conventional PP the functions of perception is learning, inferring and representing predictive 
regularities in sensory data. However, from the evolutionary perspective perception would be selected 
only for facilitating learning and execution of beneficial action. Thus, the perception learning goal is not 
to predict all sensory data with uniform accuracy, but to learn regularities in sensory data which are 
predictive of the future value or can facilitate learning and guiding action. While the value system learns 
positive and negative affective values specific to individual causes, perceptual cause learning itself, both 
via binding of new causes and via adjustment of prediction strengths of existing causes, must be 
specifically optimized for value learning. Minimizing the future-value prediction errors requires learning 
specifically the sensory data regularities predictive of current and future reward. For example, a large 
positive or negative value surprise concurrent with a newly-bound cause not only might be assigned as 
the specific value attributed to this cause, but also such high-valence cause must be forgotten more slowly 
and, with repeat inference, more quickly become a permanent long-term memory.  

Additionally, perception learning must be similarly biased toward learning causes useful for triggering and 
guiding action. Similar to reward circuits, heuristic neural ‘saliency’ and ‘novelty’ detectors have evolved 
to specifically detect the presence of sensory signals that are useful to learn, even though they do not 
immediately correlate with either positive or negative gene-proliferation outcomes and therefore carry 
no explicit affective value. Signals from these detectors may modulate the perceptual learning 
hyperparameters similarly to the affective surprise signals.  
Action encoding in predictive processing. 
Complex action policies need to be encoded by the brain to be executed. While actions may in principle 
be encoded largely outside the perceptual hierarchy of PP, the work on active inference (Parr and Friston 
2019; K. J. Friston et al. 2010; K. Friston et al. 2017) considers the possibility of encoding actions as 
predictions within the PP hierarchy. In addition to receiving sensory data, the lowest level of the hierarchy 
is also connected to predict, and thereby command, the actuator outputs from the motor cortex. Moving 
up the hierarchy implements more and more abstract actions and perceptions, interconnecting and 
influencing each other at each level. Thus, the hierarchical PP structure, so far discussed here mainly in 
the context of perception, can also encode highly complex and hierarchical perception-guided action 
policies. In such encoding, unitary actions at all abstraction levels can be understood as increases of the 
likelihoods of target causes in response to high likelihoods of trigger causes. For example, this can be 
implemented by inference of an action cause from the trigger causes, and prediction of the target causes 
by the action cause. Since we suggested that individual PP causes can represent short timed sequences of 
features, an elementary action cause may simply represent a timed sequence, with learned intervals, 
where trigger features precede the target features, thereby triggers predict targets, but not vice versa. 
The relationship of PP and active inference to RL is an open area of research (K. J. Friston, Daunizeau, and 
Kiebel 2009; Tschantz et al. 2020; Millidge et al. 2020) with much yet to be understood. Within this largely 
uncharted territory, we will attempt to qualitatively delineate, as a conjecture, some logical principles for 
how a system combining PP and TDRL might be organized. In doing this we note and explicitly set to one 
side the active inference result that biasing inference can generate a bias confirming action by a free-
energy minimizing system through the outside world (Parr and Friston 2019; K. J. Friston et al. 2010), 
thereby making the perceptual ‘prophesy’ self-fulfilling. Reformulating RL as active inference, while 
intriguing, applies to systems described as maintaining some generalized homeostasis, while the rewards 
of RL may be able to define a broader range of action policy learning goals more explicitly, and point to 
how such policies may be learned in the absence of good world models. Within explicit RL, accurate and 
unbiased estimation of future value is key for learning actions which maximize the reward. Thus, even if 
action and perception are encoded within a common hierarchy, their functional roles are separate. 
Perception is learned to accurately estimate the future value and represent the regularities of the world 
helpful for controlling action, while action is learned to maximize the future value, without undue 
decrease of the perception accuracy.  
One way of accomplishing this is by explicitly separating PP causes into action-like and perception-like by 
providing different learning rules and biases for each, encoded via their hyperparameters. Unlike 
perceptual cause learning via prediction error minimization, existing action causes’ prediction strengths 
may be learned based on the dopaminergic reinforcement signaling value prediction error: the prediction 
strengths for the action’s trigger and target features is increased (decreased) based on the positive 
(negative) future-value surprise integrated over a seconds-to-minutes long period of plasticity following 

the inference of the action. The action is strengthened if it has led to a positive value surprise and vice 
versa. This local rule is largely consistent with the known dopaminergic signals from the VTA distributed 
throughout the frontal cortex. This organization is also consistent with the view that actions are 
represented mostly in the front of the cortex. 
Action learning and exploration-exploitation tradeoff. 
Other than periods of sleep (where off-line learning processes are known to occur, see also the States of 
consciousness section below), an organism learns online, while it is perceiving and acting. Given the 
enormity of the action space, computing to select the best action for each state via Q-learning (Sutton 
and Barto 1998) appears intractable. Therefore, perception can only estimate the future value of the state 
itself, on-policy, i.e., for the combination of the world and the present ongoing action policy. Even when 
the on-policy value is perfectly learned, it is not clear how any meaningful gradients of the value in the 
policy space can be computed. Lacking value gradients, policy improvements would have to be learned by 
experimenting: making a modification and retaining or discarding the modification based on the 
subsequent value surprise. 
Considering that some causes are actions, binding action- and object-causes as features into a new cause 
adds new coupling between these objects and actions. These actions can now be triggered by the objects. 
Thus, learning by binding can be understood not only as perceptual learning but also as policy 
modification. Following the RL logic, a policy modification should be retained and consolidated or 
forgotten based not only on its perceptual usefulness (both perceptual prediction error reduction and 
improved value prediction), but also based on the policy modification’s value benefit, estimated as the 
positive or negative value surprise over a period following the modification. Both perceptual usefulness 
and action usefulness must be weighted by a stable and efficient learning process. Importantly, they 
appear to be distinguishable in time – while the perceptual usefulness is given immediately by how much 
the prediction errors and the future value surprise are reduced by the new cause’s inference, the value 
benefit is given not by the immediate but by the subsequent value surprise, attributable to the 
consequences of the action. Therefore, retention and consolidation of a perceptual cause might be 
increased when there is an immediate value surprise, either positive or negative, while action causes 
would be retained or discarded based on the time-integrated positivity or negativity, respectively, of the 
subsequent, delayed value surprise.  
While the ongoing binding process is one type of policy modification, exploring may be broadened to 
include generation of off-policy actions. There is a vast variety of explorable actions, most being irrelevant 
to the circumstances. One possibility is to incrementally perturb the policy via random test actions 
generated by lowering the action triggering (inference) threshold. This way only context-relevant actions 
slightly outside the current policy would be inferred and explored, i.e., those actions that would be 
triggered by similar but not identical perceptions. If a positive value surprise follows, the policy 
modification should be retained, so that the next time it will also be triggered in similar circumstances. 
The present moment’s unique features are described by the newly-bound causes of the present moment, 
and triggering in similar circumstances can be accomplished by binding the new action as part of the 
present episode binding these causes, and retaining and consolidating the episode, such that the new 
action can be later triggered by associative recall.  
Other ways of learning to improve the policy, both principled and heuristic, remain to be studied. A well-
known general exploration vs. exploitation tradeoff in RL is whether to try policy modifications in hopes 

of learning a better policy or to stick with the existing policy and exploit the benefits already learned. 
Biologically evolved approaches for making this tradeoff dynamically may consider various state variables, 
such as whether the future value is presently high or low. The described lowering of the action inference 
threshold is one way to increase exploration by trying new actions. A speculative but intriguing possibility 
is that exploration is modulated by tonic, rather than phasic, aminergic signaling, such as tonic dopamine 
levels from VTA controlling action thresholds and tonic serotonin levels modulating inference, prediction 
and/or binding thresholds in the perceptual PP hierarchy. This conjecture may help understand both the 
hallucinatory effects related to tonic prefrontal and limbic dopamine imbalance, as well as shedding light 
on the action of psychedelic compounds via serotonin receptors, as manifestations of increased 
exploration within the PP action and perception realms, respectively. 
Imagination and thinking. 
High-level actions directly predict (trigger) specific other actions and high-level perceptions. Whenever 
the prediction target is a perceptual cause, a perceived likelihood of the cause increases. We have 
previously discussed how the action of selective attention to the spatial location of a newly-bound object 
can maintain the object in the global workspace and working memory. When this happens in the absence 
of sensory data directly supporting the object’s inference, this object is imagined. Inferences of causes 
directly predicting sequences of features result in replays of such visual, auditory or multimodal 
sequences. Like actions triggered by perceptions inferred from sensory data, complex and highly specific 
actions can be triggered by imagined perceptual inferences, making long imagined action-perception 
chains possible. They may include sensory images, amodal/abstract imagination, kinesthetic action 
imagination, as well as imagined generation and perception of speech sequences in humans. Some of 
these inferences will remain unpredicted long enough to be bound into new causes and unified episodes, 
thus making them into the conscious contents. Excluding some states of focused attention, flow and 
meditation, a large fraction of the human conscious contents appears to be generated by such multimodal 
imagination action-perception sequences, in addition to the more direct perceptual inferences from the 
sensory data. 
Imagining reuses the predictive machinery of perception – imagining objects or actions predicts object 
features as well as the perceptual consequences of actions. This agrees with everyday experiences such 
as mental visual object rotation or internal dialog generation. Furthermore, imagined objects and actions 
are subject to all the learning mechanisms in the system, consistent with learning through mental 
rehearsing. 
In principle, generating sequences of imagined percepts does not seem to require language. Language 
recognition can be understood as a special set of actions for imagining the semantic content in response 
to linguistic code perceptual triggers. Correspondingly, imagined verbal conversation sequences 
accompanied by the multimodal perceptions of the semantic content constitute the discursive thought. 
Importantly, inability of the PP model to fully predict the results of these complex actions, i.e., to predict 
the imagined utterances and the associated perceptual content, leads to them being bound and becoming 
conscious.  
Procedural knowledge and perceptual knowledge. 
We can thus distinguish two types of knowledge in our learning system: perceptual knowledge and 
procedural knowledge. The first is the ability to perceptually recognize objects and events. The second is 
the ability to generate sequences of mental and physical actions, which consist of combinations of 

perceived objects and events triggering perceptions of other objects and events.  For illustration, the 
ability to think and say that Earth rotates relative to the Sun differs from unambiguously perceiving oneself 
standing on a rotating ball of Earth whenever watching a sunrise. Missing this distinction between the 
procedural knowledge and the learned perception has led to considerable philosophical confusion, such 
as in the Mary-the-color-scientist thought experiment (Jackson 1982; Blackmore and Troscianko 2018). 
Normal color perception relies on a perceptual model making complex inferences from visual sensory data 
and is only functional when it provides input to further perceptual inferences related to colored objects. 
It is entirely distinct from the discursive scientific knowledge about color perception – a set of mostly 
procedural skills, such as predicting consequences based on conditions, combined with high-level 
perceptual skills of recognizing abstract concepts of scientific models in the real-world experimental 
reality.  
The perceptual and procedural (such as language-mediated discursive) skills are often tightly coupled. 
Procedural skills are built using perceptual skills, i.e., perception is needed to guide action. On the other 
hand, learned actions resulting in unpredicted activations of a percept B after a percept A makes them 
subject to learning by binding, thus facilitating perceptual learning of a predictive relationship between 
them. We can see how in the absence of accurate discursive knowledge, or through cultural exposure to 
inaccurate knowledge, illusory perceptions may be more easily learned. Meanwhile, accurate perceptual 
models can be learned, and perceptual illusions can be corrected with the help of accurate discursive 
knowledge of underlying phenomena, when such procedural models generate accurate perceptual 
examples and outcomes. For example, should one choose to, one can not only discursively know that 
Earth is rotating, but also train to perceive being on a rotating Earth when experiencing a sunrise.  
Unlike this particular example, learning accurate perceptions can be highly consequential, since changing 
the perceptual model entails automatic changes to when and how various dependent perceptions and 
actions occur. Once learned, the perceptual inference is automatic, while ‘unperceiving’ or changing the 
perceptual model requires additional training.  
Using scientific theories combines procedural and perceptual skills. Being able to solve an equation or 
manipulate a formal model must be accompanied by perceptually recognizing which scientific model 
concepts and equations can be used to describe the relevant aspects of a concrete natural or experimental 
situation. Application of discursive and procedural scientific knowledge requires the perceptual skill of 
mapping the perception of the outside world to the abstract notions procedurally manipulated and 
discursively described by a theory. Scientific breakthroughs are often intuitive insights generated by direct 
perceptions of the causal relationships of the abstract scientific phenomena, which are only subsequently 
proven by formal manipulations. The formal manipulations are of course key for both training the 
perceptual model needed to produce the insight, as well as for communicating it to be learned by others.  
As any scientific theory, the functional account presented in this work is a procedural, discursive model. 
If confirmed, it may provide the procedural framework for correcting illusory perceptions that may exist 
about its subject. As the procedural knowledge within a conscious system, it relates the physical objects 
and events and the system’s representations arising from the perception- and action-learning 
mechanisms and includes the systems’ own perceptions of objects and actions, emotion and affect, the 
‘experience’, ‘consciousness’ and ‘self’. Therefore, it is hoped that this work may not only contribute to 
the general scientific knowledge, but also aid those who are inclined to learn to perceive themselves 
without illusion. 

 
DISCUSSION 
States of consciousness. 
In our theory the hyperparameters controlling the prediction and inference, the PP learning and the 
learning by binding, modulated by the value system, define the state of consciousness (Bayne, Hohwy, 
and Owen 2016).  While specifying the exact dependencies is well beyond the scope of the present work, 
one important concept is the exploration-exploitation tradeoff. As we have discussed, lowering inference 
threshold for action may lead to off-policy action generation for exploration. Perceptual inference might 
be similarly modulated. Additionally, the duration, level of prediction error and its correlation leading to 
binding may also be modulated. As mentioned above, some of these variables may be signaled by the 
tonic activity of the brain’s aminergic systems, in particular VTA dopaminergic signaling for actions and 
serotonin signaling for perception or binding, potentially shedding light on the hallucinogenic effects 
resulting from their pathological or pharmacologically-induced imbalances.  
Considering perceptual value estimation, it is important to recall that much of the perceptual inference is 
unconscious. Some fraction of this unconscious perceptual content is stable in time, continuously or 
repeatedly inferred over long periods. When these unconscious percepts have large specific positive or 
negative values along one or more emotional space dimensions, they provide a continuous bias input to 
the value system. Excessive chronic positive and, particularly, negative bias may affect the system’s 
hyperparameters, including offsetting the exploration-exploitation balance. This is one way our model 
connects to mood and its disorders, such as depression. Notably, if such unconscious inference becomes 
conscious in a type of perceptual shift that allows us to continue to perceive a cause while no longer fully 
predicting it, the inference participates in learning by binding and both its perceptual inference, and its 
specific future value may be rapidly changed by this single-example learning mechanism. 
In the focused attention and mindfulness meditation training, the assignment of a high specific value to a 
goal of continuously inferring the meditation object with high level of likelihood gradually modifies the 
policy and the perceptual model to reduce the inference of distractors. Over time this makes possible a 
large reduction or even elimination of most perceptual inferences arising either from sensory data or from 
actions of imagination, except for those representing the task-relevant state of the system and the goal 
(Laukkonen and Slagter 2021). In a generative PP model this also means reduction in sensory and motor 
predictions. Sensory maps may have regularizing normalization mechanisms, which may react to such 
tonic reductions of predictions and inferences by decreasing inference thresholds and increasing 
background likelihoods, which may explain accounts of vivid internally generated perceptions at certain 
stages of meditation practice, and the uniform visual illumination reported by experienced meditators. 
Furthermore, lack of perception equals lack of input for future value estimation. When the goal of 
effortless focused object perception without distractors is initially achieved, the positive evaluation of the 
task performance is the dominant positively-valenced perception, providing a singular positive input to 
the value system. In the absence of any other concurrent inputs, this is consistent with the meditative 
rapture typically described upon reaching this stage of practice. With further practice the estimated future 
value of being in this state reduces, eventually giving rise to the affectively-neutral equanimity. 
Hypothetically, the reduction in the unpredicted perceptual content in deep meditation would mean 
progressively simpler episode content, consistent with reports of “pure awareness” and related 
discussions of “minimal phenomenal experience” (Metzinger 2020a) and perhaps even the failure of 

associative recall for a fully predicted state, consistent with the cessation events reported in some 
contemplative traditions. 
Flow states (Csikszentmihalyi 1975) are the states where only comparably narrow perception and action 
subspaces are being occupied, while much of the self-referential default mode perception and action 
content is temporarily not being inferred, so that task interference is avoided. The future value in a flow 
state is estimated only from this narrow task-relevant perceptual content, which is neutral or positive 
when the task is being successfully executed. Notably, this positive affective state may differ strongly from 
the default mode affective state.  
Sleep is known to be important for off-line learning. Consistent with our hypothesis, there may be offline 
regimes that activate parts of the PP model and/or change the prediction weights in the absence of 
binding and therefore fully unconsciously. Action and object encoding parts of the PP hierarchy may be 
activated differently or not at all. In contrast, dreaming appears to be the result of action exploration in 
response to tonic dopamine signaling to the front of the brain, whereby in the absence of sensory input 
the explored actions of imagination result in hallucinatory perceptions.  An intriguing possibility is that 
dreaming might act as off-line RL value-learning iterations. In TDRL, multiple iterations are necessary to 
propagate the value backward over large time delays and assign it specifically to one or more of the 
predictive causes. By repeatedly replaying perceptual models forward in time, such specific values of 
perceptual causes can be learned. Binding during dreaming is likely necessary for the correct dynamic 
replay, resulting in conscious perception of dreams, while distorted declarative learning form dreams is 
inhibited by modifying the hyperparameters to disable memory retention and consolidation, inducing 
sleep amnesia. 
Measurement of consciousness. 
Generally, we propose that the defining consequence of consciousness is the efficient declarative learning 
not accessible to unconscious processes or systems. Therefore, the empirically testable presence of these 
types of learning modalities in biological organisms can serve as a measure of consciousness, separate 
from and broader than the introspective reports. Such measures taken together with other cognitive and 
neurophysiological data and numerical models provide a path for developing a validated theoretical 
framework for consciousness. 
According to our theory, the recallable conscious content and conscious action arise through the specific 
interactions of PP, binding, future value estimation and RL, therefore experiments might attempt to 
isolate and target these specific processes in the brain for both measurement and controlled 
manipulation. PP without binding is unconscious. At the lowest level, a conscious perception is a binding 
event that creates temporary cross-prediction between previously unrelated causes. A mere correlation 
is not sufficient to confirm the new causal connection, but rather one of the bound causes has to be 
manipulated and effect on the other measured, such as in an associative recall. Importantly, the binding 
should be studied between causes that were previously unrelated.  
This binding may be studied both at the low level of the PP, the presumed result of recurrent processing 
in low sensory layers, and at the high level of the PP hierarchy, where the global workspace is formed. 
Experiments may manipulate the inference of the causes that are being bound, such as specific perceptual 
features at the sensory PP level or specific actions of selective attention to attributes at the higher-level. 
The manipulations can target the strength and duration of both perceptual and attention action 

inferences – stimulus contrast and masking for perception and distractors to trigger interfering attention 
actions for attention. Naturally, these are already common experimental paradigms, but our view 
highlights the need to ensure novelty of the presented combination of stimuli, i.e., controlling for pre-
existing PP causes capable of predicting the combination. We advocate measurements of binding of 
specific, controlled features to each other rather than on the less controllable binding of a feature to the 
whole experimental context, which is often the case in the present paradigms. 
When considering measures of the conscious state, such as the perturbation complexity index (Casali et 
al. 2013), such measures might be aimed to distinguish the PP without binding, the low-level perceptual 
binding, the high-level binding involving actions of attention and imagination, and the full procedural 
processing – generation of a train of conscious thought modulated by an initial high-level binding. As we 
have discussed, a broader definition of consciousness content includes the nonunified low-level binding 
occurring within each sensory modality (aligned with the recurrent processing ToC), while a less inclusive 
definition only includes the causes bound into the unified episode structure (aligned with the global 
workspace ToC). The action triggering and thought generation is an additional process, relying on the 
binding, but possibly reduced or absent in some conscious states such as meditation. 
Outlook and open questions. 
Our hypothesis is at present qualitative and makes conjectures about the properties entailed by the 
hypothesized learning system’s functional organization and how they explain relevant empirical 
observations. The first direction for further research is the search for empirical data that can corroborate 
or falsify this hypothesis, such as by being incompatible with how the proposed system must function. 
The second direction is theoretical analysis and numerical modeling of the instantiations of the proposed 
system to study its feasibility and functional properties. The aim here is to ascertain whether such a system 
can indeed stably and efficiently learn in an incremental and compositional manner with high sample 
efficiency and generalizability (Kaelbling 2020), surpassing existing learning architectures. Understanding 
this architecture from the fundamental statistical learning theory / machine learning perspective and 
developing its numerical implementations will constrain and firm up predictions for such systems, to be 
compared with experimental data from biological systems, including humans. The third direction is to 
understand with high specificity how neural mechanisms and structures in the brain might map onto the 
functional units proposed here and see if our proposal might provide a useful framework for better 
understanding of the brain.  
The proposed architecture should be further investigated to put it on a solid theoretical foundation 
supported by numerical models in several areas. The first area is the combination of learning by binding 
with generative perceptual modeling such as PP, where PP may be a mixture of categorical and 
continuous, and may include causes directly modeling time sequences of features, i.e., generatively 
replaying features in time. Modeling considerations may include priors, network topology constraints and 
regularization mechanisms, particularly within the lower and intermediate levels of the PP hierarchy, and 
how binding may enable learning within such architectures. Potential mechanisms for learning and 
implementing perception and action inhibition, either direct or through built-in normalization constraints, 
should be further elucidated. Another area is using generative modeling to represent both perception and 
action within RL, where learning accurate predictive perception needs to be balanced with learning useful 
action, and exploration-exploitation tradeoff needs to be implemented.  

While at this time the overall correctness of this hypothesis is by no means assured and many aspects 
remain to be understood, the numerical testing of such functional architectures is straightforward in 
principle. However, it is critically important that proper considerations are given to the ethical aspects of 
such research. The ethical goal of gaining knowledge and reducing epistemic indeterminacy should be 
weighted carefully against increasing the risk of creating unnecessary artificial suffering (Metzinger 2021). 
Summary. 
A broad range of observations commonly related to the term ‘consciousness’ can be functionally 
explained by considering a system combining generative perceptual modeling, such as predictive 
processing, with learning by binding and reinforcement learning of complex actions. Biological conscious 
systems have evolved because they enhance gene proliferation by efficiently and adaptively learning to 
implement complex action policies. Our theoretical proposal is comprehensive, tying together the leading 
insights into consciousness and action learning. It is subject to a rigorous mathematical description, 
numerical modeling, and formal theoretical analysis to be developed within the frameworks of statistical 
learning theory, machine learning and predictive processing. The properties entailed by the hypothesized 
functional architecture were illustrated by examples, wherein we preferred to be specific and possibly 
wrong rather than vague and unfalsifiable, aiming to facilitate empirical testing. No doubt many of these 
examples will have to be refined and corrected, while it is our hope that the main ideas will withstand 
empirical tests. At a bare minimum, this work provides insight into how a comprehensive scientific theory 
of consciousness may be conceived.  
 
 
BOX 1. Meeting the Hard Criteria for a theory of consciousness  
Here we specifically describe this proposal following the hard criteria for a theory of consciousness 
proposed in (Doerig, Schurger, and Herzog 2021): 
Empirical phenomena of consciousness being addressed by the proposal: 
1. Our theory addresses both the content and the state of consciousness by describing (1) the 
functional mechanisms necessary and sufficient for consciousness (state) and (2) the formation 
of perceptual content, and which perceptual content is unified, enter the memory record and is 
available for action, and which is not (content). 
2. The conscious state is governed by hyperparameters of the various learning functions (the 
prediction error size, time persistence and correlation necessary for binding; parameters 
governing the ongoing inference of PP; the bound cause forgetting and consolidation rates; the 
value learning and action learning timescales and rates and the exploration vs. exploitation 
tradeoff). All these global parameters modulate the state of consciousness continuously, and their 
different values distinguish conscious states. 
3. Consciousness is unifying in a sense of (temporarily) attributing various contents to common 
causes and thereby (temporarily) constraining these groups of perceptions to covary. This 
unification is hierarchical and at the highest level contents are unified into an episode. Given the 
proposed binding rules, binding between low-level perceptual features may occur without further 

binding with higher level features and the episode, accounting for the experimental observations 
in (Meuwese et al. 2013). 
4. The theory is temporally continuous, but posits the existence of a time threshold for binding. 
Discontinuously varying outcomes are predicted dependent on whether time periods between 
perceptual inferences are smaller or larger than the binding time threshold, accounting for 
experimental observations in (Herzog, Drissi-Daoudi, and Doerig 2020). 
5. Unconscious contents are not bound or unified, therefore they can have causal influence only on 
other contents with which they have a prior learned association, e.g., priming and triggering of 
previously associated actions and perceptions.   
Meeting Hard Criteria: 
1. Paradigm cases: The theory addresses several experimental paradigm cases as described. It is 
comprehensive and experimentally falsifiable. 
2. Unfolding and structure vs. function: Theory is functional and does not limit how the functions 
are implemented. However, consciousness is a property than cannot be meaningfully ascribed to 
any separate part of the full system’s functional organization, which must include future value 
estimation, and action learning as well as the PP and binding. To function, such system must be 
connected with a world via sensors and actuators and provided the rewards. 
3. Network size: The theory describes a functional organization that implements consciousness, 
irrespective of size. Accordingly, small networks implementing this functional organization are 
deemed conscious, even if this consciousness is simple and limited in what it can represent, learn 
or enact. Each instance of a complete implementation of this functional organization within a 
large network is deemed separately conscious. Other than split brain patients, there is no clear 
evidence of multiple instances of this organization within a single brain. It is likely that two largely 
separate instances of this functional organization exist in split brain patients, thus containing two 
consciousnesses.  However, when the split brains are interconnected, they can no longer be 
considered separate and independent implementations of the functional organization. A normally 
connected brain implements a single instance of the functional organization. 
4. Other systems: Multiple implementations are possible, including non-biological. An appropriately 
functionally organized numerical model is conscious.  
 
ACKNOWLEDGEMENTS 
I am grateful to Prof. Paula Droege for many insightful comments and suggestions on the earlier version 
of the manuscript. I would like to thank Dr. Matthew Daniels and Ishan Singhal for their comments and 
suggestions as well. 
 
REFERENCES 
Baars, Bernard J. 1995. A Cognitive Theory of Consciousness. Reprinted. Cambridge: Cambridge 
University Press. 
———. 2005. “Global Workspace Theory of Consciousness: Toward a Cognitive Neuroscience of Human 
Experience.” In Progress in Brain Research, edited by Steven Laureys, 150:45–53. The 

Boundaries of Consciousness: Neurobiology and Neuropathology. Elsevier. 
https://doi.org/10.1016/S0079-6123(05)50004-9. 
Bayne, Tim. 2012. The Unity of Consciousness. 1. publ. in paperback. Oxford: Oxford University Press. 
Bayne, Tim, Jakob Hohwy, and Adrian M. Owen. 2016. “Are There Levels of Consciousness?” Trends in 
Cognitive Sciences 20 (6): 405–13. https://doi.org/10.1016/j.tics.2016.03.009. 
Birch, Jonathan, Simona Ginsburg, and Eva Jablonka. 2020. “Unlimited Associative Learning and the 
Origins of Consciousness: A Primer and Some Predictions.” Biology & Philosophy 35 (6): 56. 
https://doi.org/10.1007/s10539-020-09772-0. 
Blackmore, Susan J., and Emily Troscianko. 2018. Consciousness: An Introduction. 3rd edition. London ; 
New York: Routledge, Taylor & Francis Group. 
Brown, Richard, Hakwan Lau, and Joseph E. LeDoux. 2019. “Understanding the Higher-Order Approach 
to Consciousness.” Trends in Cognitive Sciences 23 (9): 754–68. 
https://doi.org/10.1016/j.tics.2019.06.009. 
Casali, Adenauer G., Olivia Gosseries, Mario Rosanova, Mélanie Boly, Simone Sarasso, Karina R. Casali, 
Silvia Casarotto, et al. 2013. “A Theoretically Based Index of Consciousness Independent of 
Sensory Processing and Behavior.” Science Translational Medicine 5 (198): 198ra105-198ra105. 
https://doi.org/10.1126/scitranslmed.3006294. 
Chalmers, David. 1995. “Facing Up to the Problem of Consciousness.” Journal of Consciousness Studies 2 
(3): 200–219. 
———. 2018. “The Meta-Problem of Consciousness.” Journal of Consciousness Studies 25 (9–10): 6–61. 
Clark, Andy. 2013. “Whatever next? Predictive Brains, Situated Agents, and the Future of Cognitive 
Science.” Behavioral and Brain Sciences 36 (3): 181–204. 
https://doi.org/10.1017/S0140525X12000477. 
Cleeremans, Axel. 2011. “The Radical Plasticity Thesis: How the Brain Learns to Be Conscious.” Frontiers 
in Psychology 2. https://www.frontiersin.org/articles/10.3389/fpsyg.2011.00086. 
Cleeremans, Axel, Dalila Achoui, Arnaud Beauny, Lars Keuninckx, Jean-Remy Martin, Santiago Muñoz-
Moldes, Laurène Vuillaume, and Adélaïde de Heering. 2020. “Learning to Be Conscious.” Trends 
in Cognitive Sciences 24 (2): 112–23. https://doi.org/10.1016/j.tics.2019.11.011. 
Cohen, Jeremiah Y., Sebastian Haesler, Linh Vong, Bradford B. Lowell, and Naoshige Uchida. 2012. 
“Neuron-Type-Specific Signals for Reward and Punishment in the Ventral Tegmental Area.” 
Nature 482 (7383): 85–88. https://doi.org/10.1038/nature10754. 
Crick, Francis, and Christof Koch. 1990. “Towards a Neurobiological Theory of Consciousness.” Seminars 
in the Neurosciences 2: 263–75. 
———. 2003. “A Framework for Consciousness.” Nature Neuroscience 6 (2): 119–26. 
https://doi.org/10.1038/nn0203-119. 
Csikszentmihalyi, Mihaly. 1975. Beyond Boredom and Anxiety. San Francisco: Jossey-Bass Publishers. 
Dehaene, Stanislas, and Jean-Pierre Changeux. 2011. “Experimental and Theoretical Approaches to 
Conscious Processing.” Neuron 70 (2): 200–227. https://doi.org/10.1016/j.neuron.2011.03.018. 
Dennett, D. C. 1991. Consciousness Explained. 1st ed. Boston: Little, Brown and Co. 
Doerig, Adrien, Aaron Schurger, and Michael H. Herzog. 2021. “Hard Criteria for Empirical Theories of 
Consciousness.” Cognitive Neuroscience 12 (2): 41–62. 
https://doi.org/10.1080/17588928.2020.1772214. 
Drissi-Daoudi, Leila, Adrien Doerig, and Michael H. Herzog. 2019. “Feature Integration within Discrete 
Time Windows.” Nature Communications 10 (1): 4901. https://doi.org/10.1038/s41467-019-
12919-7. 
Friston, Karl. 2019. “A Free Energy Principle for a Particular Physics.” 
https://doi.org/10.48550/ARXIV.1906.10184. 

Friston, Karl, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. 2017. 
“Active Inference: A Process Theory.” Neural Computation 29 (1): 1–49. 
https://doi.org/10.1162/NECO_a_00912. 
Friston, Karl J., Jean Daunizeau, and Stefan J. Kiebel. 2009. “Reinforcement Learning or Active 
Inference?” PLOS ONE 4 (7): e6421. https://doi.org/10.1371/journal.pone.0006421. 
Friston, Karl J., Jean Daunizeau, James Kilner, and Stefan J. Kiebel. 2010. “Action and Behavior: A Free-
Energy Formulation.” Biological Cybernetics 102 (3): 227–60. https://doi.org/10.1007/s00422-
010-0364-z. 
Friston, Karl J., Thomas Parr, and Bert de Vries. 2017. “The Graphical Brain: Belief Propagation and 
Active Inference.” Network Neuroscience 1 (4): 381–414. 
https://doi.org/10.1162/NETN_a_00018. 
Graziano, Michael S. A., Arvid Guterstam, Branden J. Bio, and Andrew I. Wilterson. 2020. “Toward a 
Standard Model of Consciousness: Reconciling the Attention Schema, Global Workspace, Higher-
Order Thought, and Illusionist Theories.” Cognitive Neuropsychology 37 (3–4): 155–72. 
https://doi.org/10.1080/02643294.2019.1670630. 
Herzog, Michael H., Leila Drissi-Daoudi, and Adrien Doerig. 2020. “All in Good Time: Long-Lasting 
Postdictive Effects Reveal Discrete Perception.” Trends in Cognitive Sciences 24 (10): 826–37. 
https://doi.org/10.1016/j.tics.2020.07.001. 
Hohwy, Jakob. 2016. “The Self-Evidencing Brain.” Noûs 50 (2): 259–85. 
https://doi.org/10.1111/nous.12062. 
———. 2020. “New Directions in Predictive Processing.” Mind & Language 35 (2): 209–23. 
https://doi.org/10.1111/mila.12281. 
Hohwy, Jakob, and Anil Seth. 2020. “Predictive Processing as a Systematic Basis for Identifying the 
Neural Correlates of Consciousness.” Philosophy and the Mind Sciences 1 (II). 
https://doi.org/10.33735/phimisci.2020.II.64. 
Jackson, Frank. 1982. “Epiphenomenal Qualia.” The Philosophical Quarterly (1950-) 32 (127): 127–36. 
https://doi.org/10.2307/2960077. 
Jeong, Huijeong, Annie Taylor, Joseph R Floeder, Martin Lohmann, Stefan Mihalas, Brenda Wu, 
Mingkang Zhou, Dennis A Burke, and Vijay Mohan K Namboodiri. 2022. “Mesolimbic Dopamine 
Release Conveys Causal Associations.” Science 378 (6626): eabq6740. 
https://doi.org/10.1126/science.abq6740. 
Kaelbling, Leslie Pack. 2020. “The Foundation of Efficient Robot Learning.” Science 369 (6506): 915–16. 
https://doi.org/10.1126/science.aaz7597. 
Kandel, Eric R., James H. Schwartz, and Thomas M. Jessell, eds. 2000. Principles of Neural Science. 4th 
ed. New York: McGraw-Hill, Health Professions Division. 
Kiran, B Ravi, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and 
Patrick Pérez. 2022. “Deep Reinforcement Learning for Autonomous Driving: A Survey.” IEEE 
Transactions on Intelligent Transportation Systems 23 (6): 4909–26. 
https://doi.org/10.1109/TITS.2021.3054625. 
Lamme, Victor A. F. 2006. “Towards a True Neural Stance on Consciousness.” Trends in Cognitive 
Sciences 10 (11): 494–501. https://doi.org/10.1016/j.tics.2006.09.001. 
Laukkonen, Ruben E., and Heleen A. Slagter. 2021. “From Many to (n)One: Meditation and the Plasticity 
of the Predictive Mind.” Neuroscience & Biobehavioral Reviews 128 (September): 199–217. 
https://doi.org/10.1016/j.neubiorev.2021.06.021. 
Mashour, George A., Pieter Roelfsema, Jean-Pierre Changeux, and Stanislas Dehaene. 2020. “Conscious 
Processing and the Global Neuronal Workspace Hypothesis.” Neuron 105 (5): 776–98. 
https://doi.org/10.1016/j.neuron.2020.01.026. 

Metzinger, Thomas. 2003. Being No One: The Self-Model Theory of Subjectivity. Cambridge, Mass.: MIT 
Press. 
———. 2005. “Précis: Being No One.” PSYCHE: An Interdisciplinary Journal of Research On Consciousness 
11 (5). 
———. 2020a. “Minimal Phenomenal Experience: Meditation, Tonic Alertness, and the Phenomenology 
of ‘Pure’ Consciousness.” Philosophy and the Mind Sciences 1 (I): 1–44. 
https://doi.org/10.33735/phimisci.2020.I.46. 
———. 2020b. “Self-Modeling Epistemic Spaces and the Contraction Principle.” Cognitive 
Neuropsychology 37 (3–4): 197–201. https://doi.org/10.1080/02643294.2020.1729110. 
———. 2021. “Artificial Suffering: An Argument for a Global Moratorium on Synthetic Phenomenology.” 
Journal of Artificial Intelligence and Consciousness 08 (01): 43–66. 
https://doi.org/10.1142/S270507852150003X. 
Meuwese, Julia D. I., Ruben A. G. Post, H. Steven Scholte, and Victor A. F. Lamme. 2013. “Does 
Perceptual Learning Require Consciousness or Attention?” Journal of Cognitive Neuroscience 25 
(10): 1579–96. https://doi.org/10.1162/jocn_a_00424. 
Millidge, Beren, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. 2020. “Reinforcement 
Learning as Iterative and Amortised Inference.” ArXiv:2006.10524 [Cs, Stat], July. 
http://arxiv.org/abs/2006.10524. 
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex 
Graves, et al. 2015. “Human-Level Control through Deep Reinforcement Learning.” Nature 518 
(7540): 529–33. https://doi.org/10.1038/nature14236. 
Montague, P. R., P. Dayan, and T. J. Sejnowski. 1996. “A Framework for Mesencephalic Dopamine 
Systems Based on Predictive Hebbian Learning.” Journal of Neuroscience 16 (5): 1936–47. 
https://doi.org/10.1523/JNEUROSCI.16-05-01936.1996. 
Parr, Thomas, and Karl J. Friston. 2019. “Generalised Free Energy and Active Inference.” Biological 
Cybernetics 113 (5): 495–513. https://doi.org/10.1007/s00422-019-00805-w. 
Redish, A. David. 2004. “Addiction as a Computational Process Gone Awry.” Science 306 (5703): 1944–
47. https://doi.org/10.1126/science.1102384. 
Schultz, Wolfram, Peter Dayan, and P. Read Montague. 1997. “A Neural Substrate of Prediction and 
Reward.” Science 275 (5306): 1593–99. https://doi.org/10.1126/science.275.5306.1593. 
Seth, Anil K., and Tim Bayne. 2022. “Theories of Consciousness.” Nature Reviews Neuroscience, May. 
https://doi.org/10.1038/s41583-022-00587-4. 
Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc 
Lanctot, et al. 2018. “A General Reinforcement Learning Algorithm That Masters Chess, Shogi, 
and Go through Self-Play.” Science 362 (6419): 1140–44. 
https://doi.org/10.1126/science.aar6404. 
Singer, Wolf. 2001. “Consciousness and the Binding Problem.” Annals of the New York Academy of 
Sciences 929 (1): 123–46. https://doi.org/10.1111/j.1749-6632.2001.tb05712.x. 
Singhal, Ishan, and Narayanan Srinivasan. 2021. “Time and Time Again: A Multi-Scale Hierarchical 
Framework for Time-Consciousness and Timing of Cognition.” Neuroscience of Consciousness 
2021 (2): niab020. https://doi.org/10.1093/nc/niab020. 
Sligte, Ilja G., H. Steven Scholte, and Victor A. F. Lamme. 2009. “V4 Activity Predicts the Strength of 
Visual Short-Term Memory Representations.” The Journal of Neuroscience: The Official Journal 
of the Society for Neuroscience 29 (23): 7432–38. https://doi.org/10.1523/JNEUROSCI.0784-
09.2009. 
Sperling, George. 1960. “The Information Available in Brief Visual Presentations.” Psychological 
Monographs: General and Applied 74: 1–29. https://doi.org/10.1037/h0093759. 

Srinivasan, Narayanan, Shradhanjali Tripathi, and Ishan Singhal. 2020. “Meditators Exercise Better 
Endogenous and Exogenous Control of Visual Awareness.” Mindfulness 11 (12): 2705–14. 
https://doi.org/10.1007/s12671-020-01496-2. 
Sutton, Richard S., and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. Adaptive 
Computation and Machine Learning. Cambridge, Mass: MIT Press. 
Thrun, Sebastian, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, 
Philip Fong, et al. 2006. “Stanley: The Robot That Won the DARPA Grand Challenge.” Journal of 
Field Robotics 23 (9): 661–92. https://doi.org/10.1002/rob.20147. 
Treisman, Anne. 2003. “Consciousness and Perceptual Binding.” In The Unity of Consciousness: Binding, 
Integration, and Dissociation, edited by Chris Frith and Axel Cleeremans, 0. Oxford University 
Press. https://doi.org/10.1093/acprof:oso/9780198508571.003.0005. 
Tschantz, Alexander, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. 2020. “Reinforcement 
Learning through Active Inference.” ArXiv:2002.12636 [Cs, Eess, Math, Stat], February. 
http://arxiv.org/abs/2002.12636. 
 

