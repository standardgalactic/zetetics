Auton Agent Multi-Agent Syst
DOI 10.1007/s10458-014-9253-5
A study of computational and human strategies
in revelation games
Noam Peled Â· Yaâ€™akov (Kobi) Gal Â· Sarit Kraus
Â© The Author(s) 2014
Abstract Many negotiations in the real world are characterized by incomplete information,
and participantsâ€™ success depends on their ability to reveal information in a way that facilitates
agreements without compromising their individual gain. This paper presents an agent-design
that is able to negotiate proï¬ciently with people in settings in which agents can choose to
truthfully reveal their private information before engaging in multiple rounds of negotiation.
Such settings are analogous to real-world situations in which people need to decide whether
to disclose information such as when negotiating over health plans and business transac-
tions. The agent combined a decision-theoretic approach with traditional machine-learning
techniques to reason about the social factors that affect the playersâ€™ revelation decisions on
peopleâ€™s negotiation behavior. It was shown to outperform people as well as agents playing
the equilibrium strategy of the game in empirical studies spanning hundreds of subjects. It
was also more likely to reach agreement than people or agents playing equilibrium strategies.
In addition, it had a positive effect on peopleâ€™s play, allowing them to reach signiï¬cantly bet-
ter performance when compared to peopleâ€™s play with other people. These results are shown
to generalize for two different settings that varied how players depend on each other in the
negotiation.
Keywords
Humanâ€“computer negotiation Â· Opponent modeling Â· Empirical studies
N. Peled
Gonda Brain Research Center, Bar-Ilan University, Ramat Gan, Israel
Y. Gal (B)
Department of Information Systems Engineering, Ben-Gurion University, Beer Sheva, Israel
e-mail: gal@eecs.harvard.edu
S. Kraus
Department of Computer Science, Bar-Ilan University, Ramat Gan, Israel
S. Kraus
Institute of Advanced Computer Studies, University of Maryland, College Park, MD, USA
123

Auton Agent Multi-Agent Syst
1 Introduction
In negotiations under incomplete information, people commonly need to make strategic
decisions about whether and how to reveal information to others. For example, consider a
scenario in which a bank is offering to purchase a struggling company in return for potential
job cuts. The unions may not allow the company to accept the offer because they refuse to
agree to layoffs. However, if the bank discloses that it is committed to keeping the company
aï¬‚oat, the unions may agree to the buy-out despite the layoffs. On the other hand, revealing
goals is often associated with a cost or can be exploited by the other party. In our example,
if the bank declares it is committed not to liquidate the company, the unions may demand no
layoffs.
The focus of this paper is on negotiation settings in which participants lack information
about each otherâ€™s preferences, often hindering their ability to reach beneï¬cial agreements.
Speciï¬cally, we study a particular class of such settings we call â€œrevelation gamesâ€, in which
players are given the choice to truthfully reveal private information before commencing in
a ï¬nite sequence of alternating negotiation rounds. Revealing this information narrows the
search space of possible agreements and may lead to agreement more quickly, but may also
lead players to be exploited by others.
Revelation games combine two types of interactions that have been studied in the past in
the economics literature: Signalling games [25], in which players choose whether to convey
private information to each other, and bargaining [19], in which players engage in multiple
negotiation rounds. They are analogous to real-world scenarios in which parties may choose
to truthfully reveal information before negotiation ensues, such as the example presented
above.
Constructing effective agent strategies for such settings is challenging. On the one hand,
behavioral economics work has shown that people often follow equilibrium strategies [4]
when deciding whether to reveal private information to others. On the other hand, peopleâ€™s
bargaining behavior does not adhere to equilibrium [8,18], and computers cannot use such
strategies to negotiate well with people [17].
The main contribution of the paper is an agent-design that incorporates information rev-
elation decisions into its negotiation strategy using decision theory and machine learning.
The agent tries to determine which proposals people are likely to accept based on past data,
or whether they are more or less likely to accept an offer when information is revealed.
It combines a prediction model of peopleâ€™s behavior in the game with a decision-theoretic
approach to make optimal decisions. The model includes the social factors that affect peopleâ€™s
decisions whether to reveal private information, (e.g., the generosity, competitiveness and
the selï¬shness of the offers they make, as measured in the scoring function of the game). In
addition, the model included the effects of peopleâ€™s revelation decisions on their negotiation
behavior. It evaluated the agent-design with agents playing equilibrium strategies, as well as
other people, in two types of revelation games that varied how players depend on each other
in the game.
The results showed that the agent was able to outperform human players playing other
people, as well as the equilibrium agent. It learned to make offers that were signiï¬cantly
more beneï¬cial to people than the offers made by other people, while not compromising its
own beneï¬t, and was able to reach agreement signiï¬cantly more often than did people as
well as the equilibrium agent. In particular, it was able to exploit peopleâ€™s tendency to agree
to offers that are beneï¬cial to the agent if people revealed information at the onset of the
negotiation. It also positively affected peopleâ€™s play, in that their overall performance was
signiï¬cantly better when paying the agent than when playing other people. The paper thus
123

Auton Agent Multi-Agent Syst
has signiï¬cance for agent designers, showing that (1) people do not adhere to equilibrium
strategies when revealing information in negotiation; (2) reasoning about the social factors
that affect peopleâ€™s decisions can signiï¬cantly improve agentsâ€™ performance as compared to
using equilibrium strategies.
This paper extends an initial study reporting on revelation games [22] in several ways.
First, it provides a formal equilibrium analysis of revelation games. Second, it analyzes
peopleâ€™s behavior in these games, including the extent to which they respond to proposals
made by the agent, and how their performance is affected by playing with the agent. Lastly,
the empirical analysis includes additional games to the ones originally reported using subjects
from different countries. This provides further empirical support to the generalizability of
the agent to different demographic groups.
2 Related work
Our work is related to a growing line of work in multi-agent systems that use opponent
modeling to build agents for repeated negotiation in heterogeneous human-computer set-
tings. These include the KBAgent that made offers with multiple attributes in settings which
supported opting out options and partial agreements [20]. This agent used a social utility
function to consider the trade-offs between its own beneï¬t from an offer and the probability
that it is accepted by people. It used density estimation to model peopleâ€™s behavior following
a method suggested by Coehoorn and Jennings for modeling computational agents [6] and
approximated peopleâ€™s reasoning by assuming that people would accept offers from com-
puters that are similar to offers they make to each other. Other works employed Bayesian
techniques [7,15] or approximation heuristics [16] to estimate peopleâ€™s preferences in nego-
tiation and integrated this model with a pre-deï¬ned negotiation or concession strategy to
make offers. Bench-Capon [5] provide an argumentation based mechanism for explaining
human behavior in the ultimatum game. Work by Rosenfeld and Kraus [23] used Aspira-
tion Adaptation Theory was more useful that other bounded and strictly rational models in
quantifying peoplesâ€™ negotiation preferences. There is also prior work dealing with selective
information disclosure in human-agent settings [2,14,24].
None of these works allowed for agents to reveal private information during negotiation.
In addition, we are the ï¬rst to develop a strategic model of peopleâ€™s negotiation behavior that
reasons about information revelation, to formalize an optimal decision-making paradigm for
agents using this model.
Gal and Pfeffer [11] proposed a model of human reciprocity in a setting consisting of
multiple one-shot take-it-or-leave-it games, but did not evaluate a computer agent or show
how the model can be used to make decisions in the game. Other works [9,10,13] combined
a decision-theoretic approach with a set of rules to adapt to peopleâ€™s negotiation in settings
with non-binding agreements. Our work augments these studies in allowing players to reveal
private information and in explicitly modeling the effect of revelation on peopleâ€™s negotiation
behavior.
Our work is also related to computational models of argumentation, in that peopleâ€™s
revelation decisions provide an explanation of the type of offers they make during negotiation.
Work in interest-based negotiation has studied different protocols that allows players to reveal
their goals in negotiation in a controlled fashion [21,26]. These works assume that agents
follow pre-deï¬ned strategies for revealing information and do not consider or model human
participants.
123

Auton Agent Multi-Agent Syst
Lastly, revelation games, which incorporate both signaling and bargaining, were inspired
by canonical studies showing that people learn to play equilibrium strategies when they
need to signal their private information to others [4]. On the other hand, peopleâ€™s bargaining
behavior does not adhere to equilibrium [8,18], and computers cannot use such strategies
to negotiate well with people [17]. Our work shows that integrating opponent modeling and
density estimation techniques is an effective approach for creating agents that can outperform
people as well equilibrium strategies in revelation games.
3 Implementation: colored trails
We based our empirical work on a test-bed called Colored Trails [12], which we adapted to
model revelation games with 2 rounds, the minimal number that allows an offer to be made
by both players. Our revelation game is played on a board of colored squares. Each player
has a square on the board that is designated as its goal. The object of the game is to reach the
goal square. To move to an adjacent square required surrendering a chip in the color of that
square. Players had full view of the board and each othersâ€™ chips. Both players were shown
two possible locations for their goals with associated belief probabilities, but each player
could only see its own goal.
We deï¬ne the no-negotiation alternative score for a player as the score obtained when
agreement is not reached. The study included two CT board games. The ï¬rst board game,
called symmetric, is shown in Fig. 1a. The initial chip allocation for each player is speciï¬ed in
the Player Chip Display panel. Here, the â€œmeâ€ and â€œOâ€ icons represent two players, Bob and
Alice, respectively. Each player has two possible goals. For example, Bob has two possible
goals. Bobâ€™s true goal is located four steps to the left of the â€œmeâ€ icon (appearing as a white
G square), while Bobâ€™s other goal is located four steps below the â€œmeâ€ icon (appearing as
(a) ymmetric Board Game
(b) Asymmetric Board
Game
(c) A possible proposal
Fig. 1 Two CT revelation games (shown from Bobâ€™s point of view). a Symmetric Board Game, b asymmetric
Board Game, c a possible proposal
123

Auton Agent Multi-Agent Syst
a grey square outlined with a â€œ?â€ symbol). In turn, Aliceâ€™s possible goals are presented as
two grey circles outlined with â€œ?â€ symbols. The board is presented from Bobâ€™s point of
view. Bob can see its true goal location but Alice does not observe it. Unless Bob chooses
to reveal its goal, Alice does not know whether Bob needs a purple or light-green chip to
reach the goal. Similarly, Bob cannot observe Aliceâ€™s true goal location. The number â€œ50â€
on each goal square represent a 50% probability that the true goal lies in that square. In
the symmetric board, the length of the path between the Aliceâ€™s location and each of her
possible goal squares was equal. Consequently, for the symmetric board, the no-negotiation
alternative score of each player does not depend on its true goal location. The analysis from
Bobâ€™s point of view is similar. In contrast, in the second board, called asymmetric, shown in
Fig. 1b, one of the playersâ€™ goal locations is closer to its starting position than the other.
Our CT game progresses in three phases with associated time limits. In the revelation
phase (round 0), both players can choose to truthfully reveal their goal to the other player.1 In
the proposal phase (round 1), one of the players is randomly assigned the role of proposer and
can offer to exchange a (possibly empty) subset of its chips with a (possibly empty) subset
of the chips of the other player. If the responder accepts the offer, the chips are transferred
automatically according to the agreement, both participants will automatically be moved as
close as possible to the goal square given their chips and the game will end. If the responder
rejects (or no offer was received), it will be able to make a counter-proposal (round 2). If the
proposal is accepted, the game will end with the agreement result as above. Otherwise, the
game will end with no agreement.
At the end of the game, the score for each player is computed as follows: 100 points
bonus for reaching the goal; 5 points for each chip left in a playerâ€™s possession, and 10 points
deducted for any square in the path between the playersâ€™ ï¬nal position and the goal-square.2
The CT game conï¬guration described above is analogous to task settings involving infor-
mation revelation and incomplete information. We illustrate this analogy using the union
agreement scenario presented in the Sect. 1 Goals on the board represent private information
that is available to the negotiation parties, such as whether the bank is intending to liquidate
the company. Different squares on the board represent different types of sub-tasks, such as
agreeing on the number of layoffs, hiring new personnel, and deciding on compensation.
Chips correspond to agent capabilities and skills required to fulï¬ll sub-tasks. Traversing a
path through the board to the goal square represents reaching agreement that is composed
of the various sub-tasks on the paths. The game is interesting because players need to rea-
son about the tradeoff between revealing their goals and providing information to the other
player, or not to reveal their goals to possibly receive or ask for more chips than they need.
In addition, there is an advantage to the proposer player in the second round, in that it makes
the ï¬nal offer in the game. But the identity of the second proposer is not known at the time
that players decide whether to reveal their goals.
4 Equilibrium analysis
In this section we formalize the colored trails game described earlier as a repeated game of
imperfect information. Each player has a type ti that represents the true position of its goal
on the board.3 Let Ï‰n represent an offer made by a proposer player at round n âˆˆ{1, 2} in
1 This decision is performed simultaneously by all players, and goals are only revealed at the end of the phase.
2 This path is computed by the Manhattan distance.
3 Revealing goals in the game thus corresponds to making playersâ€™ types common knowledge.
123

Auton Agent Multi-Agent Syst
a game. Let rn âˆˆ{accept, reject} represent the response to Ï‰n by a responder player. Let
si represent the score in the game as described in the previous section. The no-negotiation
alternative (NNA) score to player i of type ti is the score for i in the game given that no
agreement was reached. We denote the score for this event as si(âˆ…| ti). If no agreement was
reached in round 1 of the game, playersâ€™ ï¬nal score depends on whether the counter-proposal
in round 2 is accepted. If no agreement was reached in round 2 (the last round) of the game,
playersâ€™ NNA score is also their ï¬nal score in the game. We denote the beneï¬t to player i
from Ï‰n given that rn = accept as Ï€i(Ï‰n | ti). This is deï¬ned as the difference in score to i
between an offer Ï‰n and the NNA score:
Ï€i(Ï‰n | ti) = si(Ï‰n | ti) âˆ’si(âˆ…| ti)
(1)
Let Ti denote a set of types for a general player i. Let revi denote the event in which i
reveals its type ti âˆˆTi, and let noRevi denote the event in which i does not reveal its type.
Let hn denote a history of moves, including for both players i and j their revelation decision
at the onset of the game, and the proposals and responses for rounds 1 through n. For the
remainder of this section we will assume (without loss of generality) that player j is the
proposer in round 1, and player i as the proposer in round 2. We denote by p(t j | hn) the
probabilistic belief of playerâ€™s i on player j being of type t j (and similarily for player j).
Let a2
j(Ï‰, t j) deï¬ne a strategy for responder j at round 2 of type t j that accepts any
proposal Ï‰ with positive beneï¬t.
a2
j(Ï‰, t j) =

accept
if Ï€ j

Ï‰ | t j

â‰¥0
reject
otherwise
(2)
We will abuse notation and write a2
j when the proposal Ï‰ is clear from context. We extend
Eq. 1 to deï¬ne the beneï¬t to proposer i given the response strategy a2
j(Ï‰, t j).
Ï€i(Ï‰, a2
j, t j | ti) =

Ï€i(Ï‰ | ti)
if a2
j(Ï‰, t j) = accept
0
otherwise
(3)
We denote a proposal from j to i in round 1 as Ï‰1
j,i and a proposal from i to j in round
2 as Ï‰2
i, j (also called iâ€™s counter-proposal). We deï¬ne several possible strategies for making
revelation decisions and proposals in the game, and proceed to show that these strategies are
in equilibrium under certain sufï¬cient conditions.
4.1 Revelation by both players
We formalize a revelation equilibrium using backward induction and then provide the condi-
tions under which the equilibrium holds. We assume two possible types for player j, denoted
t j1 and t j2, and two possible types for player i, denoted ti1 and ti2. We begin with the following
deï¬nitions. Let Ï‰2âˆ—
i, j(t j) denote a proposal in round 2 that maximizes the beneï¬t of player i
given that player j is of type t j, the history h1 = {revi, revj, Ï‰1
j,i, reject}, and that player j
uses strategy a2
j of Eq. 2 to accept proposals.
Ï‰2âˆ—
i, j(t j) âˆˆarg max
Ï‰2
i, jâˆˆÎ©i, j
Ï€i

Ï‰2
i, j, a2
j, t j | ti

(4)
where (without loss of generality) Î©i, j is all the set of all the proposals player i can propose
to player j.
123

Auton Agent Multi-Agent Syst
Let Ï‰1âˆ—
j,i(ti) be the proposal that will provide player j with the same beneï¬t as the proposal
made by player i in round 2.
Ï‰1âˆ—
j,i(ti) âˆˆarg max
Ï‰ j,iâˆˆÎ© j,i

Ï€ j(Ï‰ j,i | t j) s.t. Ï€i(Ï‰ j,i | ti) = Ï€i(Ï‰2âˆ—
i, j | ti)

(5)
where Ï‰2âˆ—
i, j satisï¬es Eq. 4.
We say that t j1 is preferable for j if its beneï¬t from the proposal that j receives in round
2 is greater when it is computed for type t j1 than type t j2, given that j is of type t j1.
Ï€ j(Ï‰2âˆ—
i, j(t j1) | t j1) â‰¥Ï€ j(Ï‰2âˆ—
i, j(t j2) | t j1)
(6)
Similarly, we can deï¬ne when t j2 is preferable for j. For example, consider the asymmetric
game of Fig. 1b. For each player, there are two types in this game, a â€œweakâ€ type, that is
missing two chips to get to the goal, and a â€œstrongâ€ type, that is missing one chip to get to
the goal. The weak type will prefer the proposal that assumes it is the weak type (which will
provide it with two chips) than the proposal that assumes it is the strong type (which will
provide it with one chip). Thus, for each player, the weak type is preferable to the strong
type.
We propose the following perfect Bayesian equilibrium.
Round 0: Both players reveal their types for each of their types.
Round 1: We distinguish between two cases. In each case we specify how i updates its beli
In this case, player i updates its beliefs to assign probability 1 to jâ€™s type.
In the second case, player j deviates from equilibrium and does not reveal its
type. In this case, player i will update its beliefs over j as follows. If type t j1 is
preferable to j, then player i will update its beliefs that player j is of type t j2.
P(t j1 | h1) = 0
(7)
(And conversely for the case in which type t j2 is preferable to player j). Ifi revealed
its type in round 0 then player j assign probability 1 to iâ€™s type. Otherwise, it will
arbitrarily assign probability 1 to ti1.
We now describe the actions for players i and j in round 1. Player j makes
a proposal Ï‰1âˆ—
j,i(ti) that satisï¬es Eq. 5. In turn, player i will use the following
acceptance strategy a1
i (Ï‰, ti, t j) such that
a1
i (Ï‰, ti, t j) =

accept
if Ï€i(Ï‰ | ti) â‰¥Ï€i(Ï‰2âˆ—
i, j(t j) | ti)
reject
otherwise
(8)
Round 2: Both players do not update their beliefs over each otherâ€™s actions in round 1.
Player i makes a proposal Ï‰2âˆ—
i, j that satisï¬es Eq. 4. In turn, player j accepts any
proposal according to the strategy a2
j(Ï‰, t j) speciï¬ed in Eq. 2.
4.2 Revelation by neither player
In this case neither of the players disclose their true types at the onset of the game. We make
the following deï¬nitions. Let Eh1

Ï€i

Ï‰2
i, j | ti

denote the expected beneï¬t of i for proposal
Ï‰2
i, j given its updated beliefs over jâ€™s types and that j accepts beneï¬cial proposals.
Eh1
	
Ï€i

Ï‰2
i, j | ti

=

t jâˆˆTj
p

t j | h1
Ï€i

Ï‰2
i, j, a2
j, t j | ti

(9)
123

Auton Agent Multi-Agent Syst
Let Ï‰2âˆ—
i, j(h1, ti) be the proposal in round 2 of player i of type ti that maximizes Eq. 9 given
the history h1:
Ï‰2âˆ—
i, j(h1, ti) âˆˆarg max
Ï‰2
i, jâˆˆÎ©i, j
Eh1
	
Ï€i

Ï‰2
i, j | ti

(10)
Let Ï‰1âˆ—
j,i be the proposal of player j in round 1 that provides it with the same beneï¬t from
the counter-proposal made by one of the types of player i in round 2. This proposal does not
depend on the type of j.
Ï‰1âˆ—
j,i âˆˆ{Ï‰2âˆ—
i, j(h1, ti1), Ï‰2âˆ—
i, j(h1, ti2)}
(11)
where Ï‰2âˆ—
i, j(h1, ti) satisï¬es Eq. 10.
We deï¬ne a perfect Bayesian equilibrium as follows:
Round 0: Neither player reveals its type for each of its type.
Round 1: We distinguish between two cases. In each case we specify how i updates its
beliefs about jâ€™s type and which proposal j makes in round 1. In the ï¬rst case,
player j did not reveal its type (i.e., followed the equilibrium strategy). In this
case, player i does not update its beliefs over jâ€™s types. In the second case, player
j revealed its type. In this case, player i updates its beliefs to assign probability 1
to jâ€™s type. (And similarly for for player i). We now describe playersâ€™ actions in
round 1. Player j will make a proposal Ï‰1âˆ—
j,i that satisï¬es Eq. 11. In turn, player i
will use strategy a1
i (Ï‰, ti) such that
a1
i (Ï‰, ti) =

accept
if Ï€i(Ï‰ | ti) â‰¥Ï€i(Ï‰2âˆ—
i, j(h1, ti) | ti)
reject
otherwise
(12)
where proposal Ï‰2âˆ—
i, j(h1, ti) satisï¬es Eq. 10.
Round 2: Player i does not update its beliefs over jâ€™s types (and similarly for player j).
Player i will make a proposal Ï‰2âˆ—
i, j(h1, ti) that satisï¬es Eq. 10. player j accepts
any beneï¬cial proposal following Eq. 2.
4.3 Sufï¬cient condition for equilibria
We now describe sufï¬cient conditions for which the revelation and non-revelation equilibria
hold. For simplicity, we do make the assumption that player j has two types t j1 and t j2.
Suppose without loss of generality that j is of type t j1. Let Ï‰2âˆ—
i, j1 be the proposal made by
player i to player j of type t j1 that satisï¬es Eq. 4 (when player j reveals its type) and let
Ï‰2âˆ—
i, j(h1) be the proposal that satisï¬es Eq. 10 (when player j does not reveal its type) and that
h1 is the history up until round 2. We will postulate two propositions:
Proposition 1 If one of jâ€™s type is preferable (Eq. 6) for j, then the strategies speciï¬ed in
Sect. 4.1 constitute a perfect Bayesian equilibrium in which both players i and j reveal their
types for each type in round 0.
Proposition 2 If the below conditions hold, then the strategies speciï¬ed in Sect. 4.2 constitute
a perfect Bayesian equilibrium in which both players do not reveal their types for each of
their types in round 0.
123

Auton Agent Multi-Agent Syst
The ï¬rst condition says j prefers not to disclose its type. Formally, for every type t j and
ti, we say that
Ï€ j(Ï‰2âˆ—
i, j(t j) | t j) â‰¤Ï€ j

Ï‰2âˆ—
i, j(h1, ti) | t j

(13)
The second condition says that any proposal that is greater or equal to i in round 1 is (at
least) worse off for j for each of its types. Formally, we say that for every type t j and ti, the
following holds.
âˆ€Ï‰1
j,i if Ï€i(Ï‰1
j,i | ti) â‰¥Ï€i

Ï‰2âˆ—
i, j(h1, ti) | t j

then Ï€ j(Ï‰1
j,i | t j) â‰¤Ï€ j

Ï‰2âˆ—
i, j(h1, ti) | t j

(14)
In the Appendix, we provide a proof to the Proposition above, and also show that the
sufï¬cient conditions hold in the board games in our study.
4.4 Adaptation of equilibria to CT game
We will exemplify the equilibrium strategies in the revelation games for the two boards in
Fig. 1a, b and demonstrate that they are in equilibrium. In this analysis, each of the two types
of each player is assigned a 50% prior probability. For expository convenience we assume
that the â€œmeâ€ player is the ï¬rst proposer in the game, as shown in the ï¬gure.4
We begin with the symmetric game in Fig. 1a. Here, both players are located at equal
distance of each of their goals. The no-negotiation alternative for both players is 80 points.
The â€œmeâ€ player has 24 chips at the onset of the game, and is missing one purple chip to
reach the goal square from its initial location. In this game we describe an equilibrium in
which neither player reveals its goal.
Round 0: Neither of the players reveals its goal.
Round 1: The â€œmeâ€ player will propose to give 21 chips (7 olive-green, 7 orange chips and 7
grey chips) in return for two chips (one purple and one green chip). This proposal
will yield a score of 105 for the â€œmeâ€ player and a score of 295 for the â€œOâ€ player.
The â€œOâ€ player accepts any proposal that provides it with higher beneï¬t than the
proposal it will make in round 2.
Round 2: The â€œOâ€ player makes the same offer as in round 1. The â€œmeâ€ player accepts any
proposal that provides it with positive beneï¬t.
In contrast to the symmetric game, in the asymmetric game the distance of each player
from the goal depends on its true goal location, thus the no-negotiation alternative score
depends on the type of the player. We therefore distinguish between two possible types
of players, weak and strong. The weak player is missing two chips to get to the goal,
and its no-negotiation alternative score is 70 points. The strong player is missing a single
chip to get to the goal, and its no-negotiation alternative score is 90 points. In Fig. 1b,
which shows playersâ€™ starting positions on the game board, the â€œmeâ€ playerâ€™s type is
strong, because its starting position is located closer to its true goal location (the high-
lighted square marked â€œGâ€) than to its other possible goal location. For the asymmetric
game of Fig. 1b we present an equilibrium strategy that depend on the type of the â€œmeâ€
player.
4 In practice, the identity of the ï¬rst proposer was determined stochastically according to a known probability
distribution. In practice this did not affect the equilibrium strategies that are described in this section.
123

Auton Agent Multi-Agent Syst
Round 0: Both of the player disclose their goals.
Round 1: The proposal made by the â€œmeâ€ player depends on its known type. A weak player
will propose 10 green chips and 11 gray chips to player â€œOâ€ in return for 2 purple
chips. A strong player will propose 10 green chips and 12 gray chips to player â€œOâ€
in return for 1 purple chip. The â€œOâ€ player accepts any proposal that provides it
with higher beneï¬t than the proposal it will make in round 2.
Round 2: The â€œOâ€ player proposes the offer in round 1 for the the â€œmeâ€ player, which
depends on the â€œmeâ€ playerâ€™s type. The â€œmeâ€ player will accept any proposal with
positive beneï¬t.
5 The SIGAL agent
In this section we describe the Sigmoid Acceptance Learning Agent (SIGAL). SIGAL uses
a decision-theoretic approach to negotiate in revelation games, that is based on a model of
how humans make decisions in the game. Before describing the strategy used by SIGAL we
make the following deï¬nitions.
For the remainder of this section, we assume that the SIGAL agent (denoted a) is playing
a person (denoted p). Let Ï‰n
a,p represent an offer made by the agent to the person in round
n and let rn
p represent the response of the person to Ï‰n
a,p. The expected beneï¬t to SIGAL
from Ï‰n
a,p given history hnâˆ’1 and SIGALâ€™s type tp is denoted Ea

Ï‰n
a,p | hnâˆ’1, ta

. Let
p(rn
p = accept | Ï‰n
a,p, hnâˆ’1) denote the probability that Ï‰n
a,p is accepted by the person
given history hnâˆ’1.
We now specify the strategy of SIGAL for the revelation game deï¬ned in Sect. 3. The
strategy assumes there exists a model of how humans make and accept offers in both rounds.
We describe how to estimate the parameters of this model in Sect. 5. We begin by describing
the negotiation strategies of SIGAL for rounds 2 and 1.
Round 2: If SIGAL is the second proposer, its expected beneï¬t from an offer (Ï‰2
a,p) depends
on its model of how people accept offers in round 2, encapsulated in the probability p(r2
p =
accept | Ï‰2
a,p, h1). The beneï¬t to SIGAL is
Ea

Ï‰2
a,p | h1, ta

= Ï€a(Ï‰2
a,p | ta) Â· p(r2
p = accept | Ï‰2
a,p, h1)
+ Ï€a(âˆ…| ta) Â· p(r2
p = reject | Ï‰2
a,p, h1)
Here, the term Ï€a(âˆ…| ta) represents the beneï¬t to SIGAL from the NNA score, which is
zero. SIGAL will propose an offer that maximizes its expected beneï¬t in round 2 out of all
possible proposals for this round.
Ï‰2âˆ—
a,p = arg max
Ï‰2a,p
Ea

Ï‰2
a,p | h1, ta

(15)
If SIGAL is the second responder, its optimal action is to accept any proposal from the
person that gives it positive beneï¬t as described in the equilibrium strategy (Eq. 2). Let
r2âˆ—
a (Ï‰2
p,a | h1) denote the response of SIGAL to offer Ï‰2
p,a, deï¬ned as
r2âˆ—
a (Ï‰2
p,a | h1) =

accept
Ï€a(Ï‰2
p,a | ta) > 0
reject
otherwise
(16)
123

Auton Agent Multi-Agent Syst
where Ï€a(Ï‰2
p,a | ta) is deï¬ned in Eq. 1. The beneï¬t to SIGAL from this response is deï¬ned as
Ï€a

r2âˆ—
a
| Ï‰2
p,a, h1, ta

=
â§
â¨
â©
Ï€a(Ï‰2
p,a | ta)
r2âˆ—
a (Ï‰2
p,a | h1) = accept
Ï€a(âˆ…| ta)
otherwise
(17)
Round 1: If SIGAL is the ï¬rst proposer, its expected beneï¬t from making a proposal Ï‰1
a,p
depends on its model of the person: If the person accepts Ï‰1
a,p, then the beneï¬t to SIGAL is
just Ï€a(Ï‰1
a,p | ta). If (Ï‰1
a,p) is rejected by the person, then the beneï¬t to SIGAL depends on the
counter-proposal Ï‰2
p,a made by the person in round 2, which itself depends on SIGALâ€™s model
p(Ï‰2
p,a | h1) of how people make counter-proposals. The expected beneï¬t to SIGAL from
behaving optimally as a second responder for a given offer Ï‰2
p,a is denoted Ea(resp2 | h1, ta),
and deï¬ned as
Ea(resp2 | h1, ta)
=

Ï‰2p,a
p(Ï‰2
p,a | h1) Â· Ï€a(r2âˆ—
a
| Ï‰2
p,a, h1, ta)
(18)
where Ï€a(r2âˆ—
a
| Ï‰2
p,a, h1, ta) is deï¬ned in Eq. 17.
Its expected beneï¬t from Ï‰1
a,p is:
Ea

Ï‰1
a,p | h0, ta

= Ï€a(Ï‰1
a,p | ta) Â· p(r1
p = accept | Ï‰1
a,p, h0)
+ Ea(resp2 | h1, ta) Â· p(r1
p = reject | Ï‰1
a,p, h0)
(19)
where h1 =

h0, Ï‰1
a,p,r1
p = reject

. SIGAL will propose an offer in round 1 that
maximizes its expected beneï¬t in this round:
Ï‰1âˆ—
a,p = arg max
Ï‰1a,p
Ea

Ï‰1
a,p | h0, ta

(20)
If SIGAL is the ï¬rst responder, it accepts any offer that provides it with a larger beneï¬t
than it would get from making the counter-proposal Ï‰2âˆ—
a,p in round 2, given its model of how
people respond to offers in round 2:
r1âˆ—
a (Ï‰1
p,a | h0) =
â§
âªâªâ¨
âªâªâ©
accept
Ï€a(Ï‰1
p,a | ta) >
Ea

Ï‰2âˆ—
a,p | h1, ta

reject
otherwise
(21)
Here, h1 =

h0, Ï‰1
p,a,r1
a = reject

, Ï€a(Ï‰1
p,a | ta) is deï¬ned in Eq. 1 and Ea

Ï‰2âˆ—
a,p | h1, ta

is the beneï¬t to SIGAL from making an optimal proposal Ï‰2âˆ—
a,p at round 2, as deï¬ned in
Eq. 15.
Let Ï€a

r1âˆ—
a
| Ï‰1
p,a, h0, ta

denote the beneï¬t to SIGAL from its response to offer Ï‰1
p,a in
round 1. If SIGAL accepts this offer, it receives the beneï¬t associated with Ï‰1
p,a. If it rejects
123

Auton Agent Multi-Agent Syst
this offer, it will receive the expected beneï¬t Ea

Ï‰2âˆ—
a,p | h1, ta

from making an optimal
counter-proposal at round 2:
Ï€a

r1âˆ—
a
| Ï‰1
p,a, h0, ta

=
â§
â¨
â©
Ï€a(Ï‰1
p,a | ta)
r1âˆ—
a (Ï‰1
p,a | h0) = accept
Ea

Ï‰2âˆ—
a,p | h1, ta

otherwise
(22)
The expected beneï¬t to SIGAL as a responder in round 1 is denoted as Ea

resp1 | h0, ta

.
This beneï¬t depends on its model of all possible offers made by people for each type, given
that SIGAL responds optimally to the offer.
Ea

resp1 | h0, ta

=

tpâˆˆTp
p(tp | h0) Â·
â›
âœâ

Ï‰1p,a
p(Ï‰1
p,a | tp, h0) Â· Ï€a(r1âˆ—
a
| Ï‰1
p,a, h0, ta)
â
âŸâ 
(23)
Note that when the person reveals his/her type at round 0, this is encapsulated in the history
h0, and p(tp | h0) equals 1 for the personâ€™s true type. Otherwise p(tp | h0) equals the
probability p(tp).
Round 0: In the revelation round SIGAL needs to decide whether to reveal its type. Let
Ea(h0, ta) denote the expected beneï¬t to SIGAL given that h0 includes a revelation decision
for both players and that ta is the type of agent. This beneï¬t depends on the probability that
SIGAL is chosen to be a proposer (p(prop)) or responder (p(resp)) in round 1:
Ea(h0, ta) = p(resp) Â· Ea

resp1 | h0, ta

+p(prop) Â· Ea(Ï‰1âˆ—
a,p | h0, ta)
(24)
Here, Ï‰1âˆ—
a,p is the optimal proposal for SIGAL in round 1, and Ea

Ï‰1âˆ—
a,p | h0, ta

is the
expected beneï¬t associated with this proposal, deï¬ned in Eq. 19.
Because players do not observe each otherâ€™s revelation decisions, the expected beneï¬t for
a revelation decision Ï†a of the SIGAL agent sums over the case where people revealed their
type (i.e., Ï†a = tp) or did not reveal their type (i.e., Ï†a = null). We denote p(Ï†p = tp) as
the probability that the person revealed its type tp, and p(Ï†p = null) as the probability that
the person did not reveal its type tp.
Ea (Ï†a) =

tpâˆˆTp

p(Ï†p = tp)Â· Ea

h0 =

Ï†a, Ï†p = tp

, ta

+p(Ï†p = null) Â· Ea

h0 =

Ï†a, Ï†p = null

, ta

]
(25)
Given that SIGAL is of type ta âˆˆTa, it reveals its type only if its expected beneï¬t from
revelation is greater or equal to not revealing its type:
123

Auton Agent Multi-Agent Syst
Ï†âˆ—
a =
â§
âªâ¨
âªâ©
ta
Ea (Ï†a = ta) â‰¥
Ea (Ï†a = null)
null
otherwise
(26)
The value of the game for SIGAL for making the optimal decision whether to reveal its type
is deï¬ned as Ea

Ï†âˆ—
a

.
Lastly, we wished SIGAL to take a risk averse approach to making decisions in the game.
Therefore SIGAL used a convex function to represent its utility in the game from an offer
Ï‰n, which modiï¬ed Eq. 1.
Ï€â€²
a(Ï‰n | ta) = Ï€a(Ï‰n | ta)(1âˆ’Ï)
1 âˆ’Ï
(27)
The strategy used by SIGAL is obtained by â€œplugging inâ€ the risk averse utility Ï€â€²
a(Ï‰n | ta)
instead of Ï€i(Ï‰n | ti).
6 Modeling human players
In this section we describe a model of peopleâ€™s behavior used by SIGAL to make optimal
decisions in the game. We assume that there is a training set of games played by people, as
we show in the next section.
6.1 Accepting proposals
We modeled peopleâ€™s acceptance of proposals in revelation games using a stochastic model
that depended on a set of features. These comprised past actions in the game (e.g., a responder
may be more likely to accept a given offer if it revealed its type as compared to the case in
which it did not reveal its type) as well as social factors (e.g., a responder player may be less
likely to accept a proposal that offers more beneï¬t to the proposer than to itself).5
Let Ï‰n
i, j represent a proposal from a player i to a player j at a round n. We describe the
following features that affect the extent to which player j will accept proposal Ï‰n
i, j. These
features are presented from the point of view of proposer i, therefore we assume that the type
of the proposer ti is known, while the type of the responder t j is known only if j revealed
its type. We ï¬rst detail the features that relate to playersâ€™ decisions whether to reveal their
types.
â€“ REV 0
j . Revelation by j. This feature equals 1 if the responder j has revealed its type and
0 otherwise. The superscript 0 indicates this feature is relevant to the revelation phase,
which is round 0.
â€“ REV 0
i . Revelation by i. This feature equals 1 if the proposer has revealed its type ti.
We now describe the set of features relating to social factors of the responder player j.
â€“ BE N n
j . Beneï¬t to j. The beneï¬t to j from proposal Ï‰n
i, j in round n. This measures the
extent to which the proposal Ï‰n
i, j is generous to the responder. In the case where j revealed
its type, this feature equals Ï€ j(Ï‰n
i, j | t j) and computed directly from Eq. 1. Otherwise,
the value of this feature is the expected beneï¬t to the responder from Ï‰n
i, j for all possible
responder types Tj:
5 Both of these patterns were conï¬rmed empirically, as shown in the Sect. 8
123

Auton Agent Multi-Agent Syst

t jâˆˆTj
p(t j | hnâˆ’1) Â· Ï€ j(Ï‰n
i, j | t j)
â€“ AI n
i . Advantageous inequality of i. The difference between the beneï¬t to proposer i
and responder j that is associated with proposal Ï‰n
i, j. This measures the extent to which
proposer i is competitive, in that Ï‰n
i, j offers more for i than for j. This feature equals the
difference between Ï€i(Ï‰n
i, j, accept | ti) and BE N n
j .
To capture the way the behavior in round n = 1 affects the decisions made by participants
in round n = 2, we added the following features that refer to past offers.
â€“ P.BE N n
j . Beneï¬t to j in the previous round. This feature equals BE N 1
j if n = 2, and 0
otherwise.
â€“ P.BE N n
i .Beneï¬ttoproposeri inthepreviousround.ThisfeatureequalsÏ€i(Ï‰1
i, j, accept |
ti) if n = 2 and 0 otherwise.
To illustrate, consider the asymmetric CT board game shown in Fig. 1b. Alice is missing
two green chips to get to the goal and Bob is missing 1 purple chip to get to the goal. Suppose
Bob is the ï¬rst proposer (player i) and that Alice is the ï¬rst responder (player j), and that
Bob revealed its goal to Alice, so its type is common knowledge, while Alice did not reveal
her goal. We thus have that REV 0
j = 0 and REV 0
i = 1. Aliceâ€™s no-negotiation alternative
(NNA) score, s j(âˆ…), is 70 points and Bobâ€™s NNA score is 90 points.
According to the offer shown in the Figure, Bob offered two green chips to Alice in return
for two purple chips. If accepted, this offer would allow Alice to get to the goal in 5 steps, so
she will have 19 chips left at the end of the game, worth 19 Â· 5 = 95 points. Similarly, Bob
will have 21 chips left at the end of the game, worth 105 points. Both will also earn a bonus
of 100 points for getting to the goal. Therefore we have that BE N 1
j = 95+100âˆ’70 = 125.
Similarly, Bobâ€™s beneï¬t from this proposal is 105 + 100 âˆ’90 = 115 points. The difference
between the beneï¬t to Bob and to Alice is âˆ’10, so we have that AI 1
i = âˆ’10. Lastly, because
the offer is made in round 1, we have that P.BE N 1
j = P.BE N 1
i = 0. This offer is more
generous to Alice than it is to Bob.
Suppose now that Alice rejects this offer and makes a counter proposal in round 2, that
proposes one purple chip to Bob in return for four greens. In this example, Alice is using her
knowledge of Bobâ€™s type to make the minimal offer that would allow Bob to reach the goal
while providing additional beneï¬t to Alice. Alice is the proposer (player i) and Bob is the
responder (player j). Recall that Bob has revealed its goal while Alice did not, so we have
REV 0
j = 1 and REV 0
i = 0. Using a similar computation from before, we get that Bobâ€™s
score from the counter proposal is 190 points. Therefor we have that BE N 2
j = 190 âˆ’90 =
100. Aliceâ€™s beneï¬t from the counter-proposal is 210 âˆ’70 = 140, therefore we have that
AI 2
i = 140 âˆ’100 = 40. The last features in the example capture the beneï¬t to both players
from the proposal made in the ï¬rst round to Alice and Bob, so we have P.BE N 2
j = 125,
and P.BE N 2
i = 115.
6.1.1 Social utility function
We model the person as using a social utility function to decide whether to accept proposals
in the game. This social utility depends on a weighted average of the features deï¬ned above.
We deï¬ne a transition function, T n, that maps an offer Ï‰n and history hnâˆ’1 to an (ordered)
set of feature values xn as follows.6
6 These weights are estimated from data using statistical techniques as described in the following section.
123

Auton Agent Multi-Agent Syst
xn =

REV 0
j , REV 0
i , BE N n
j , AI n
i , P.BE N n
j , P.BE N n
i

To illustrate, in the example above, we have that x1 = (0, 1, 125, âˆ’10, 0, 0) and x2 =
(1, 0, 100, 40, 125, 115).
Let u(xn) denote the social utility function which is deï¬ned as the weighted sum of these
features. To capture the fact that a decision might be implemented noisily, we use a sigmoid
function to describe the probability that people accept offers, in a similar way to past studies
for modeling human behavior [11]. We deï¬ne the probability of acceptance for a particular
features values xn by a responder to be
p(rn
i = accept | Ï‰n, hnâˆ’1) =
1
1 + eâˆ’u(xn)
(28)
where xn = T n(Ï‰n, hnâˆ’1). In particular, the probability of acceptance converges to 1 as
u(xn) becomes large and positive, and to 0 as the utility becomes large and negative. We
interpret the utility to be the degree to which one decision is preferred. Thus, the probability
of accepting a proposal is higher when the utility is larger.
6.1.2 Estimating weights
To predict how people respond to offers in the game, it is needed to estimate the weights
in their social utility function in a way that best explains the observed data. In general, we
need to model the probability that an offer is accepted for any possible instantiation of the
history. The number of possible proposals in round 1 is exponential in the combined chip set
of players.7 It is not possible to use standard density estimation techniques because many
such offers were not seen in the training set or were very rare. Therefore, we employed a
supervised learning approach that assumed people used a noisy utility function to accept
offers that depended on the features deï¬ned above. Let Î©i,p denote a data set of offers
proposed by some participant i to a person p.8 For each offer Ï‰n
i,p âˆˆÎ©i,p let y(rn
p | Ï‰n
i,p)
denote an indicator function that equals 1 if the person accepted proposal Ï‰n
i,p, and zero
otherwise. The error of the predictor depends on the difference between y(rn
p | Ï‰n
i,p) and the
predicted response p(rn
p = accept | Ï‰n
a,p, hnâˆ’1), as follows:

Ï‰n
i,pâˆˆÎ©i,p

p(rn
j = accept | Ï‰n
i,p, hnâˆ’1) âˆ’y(rn
j | Ï‰n
i,p)
2
(29)
where p(rn
j = accept | Ï‰n
i, j, hnâˆ’1) is deï¬ned in Eq. 28.
We used a standard Genetic algorithm to estimate weight values for the features of peopleâ€™s
social utility that minimize the aggregate error in the training set. To avoid over-ï¬tting the
training set, we used a held-out cross-validation set consisting of 30% of the data. We chose
the instance with minimal error (on the training set) in the generation that corresponded to
the smallest error on the cross-validation set. We used ten-fold cross-validation, repeating
this process ten times, each time choosing different training and testing sets, producing ten
candidate instances. To pick the best instance, we computed the value of the game Ea

Ï†âˆ—
a

for
SIGAL for each of the learned models, where Ï†âˆ—
a is deï¬ned in Eq. 26. This is the expected ben-
eï¬t for SIGAL given that it chooses optimal actions using a model of people that corresponds
to the feature values in each instance.
7 In one of the boards we studied the number of possible offers that provided the same beneï¬t to both players
was about 27,000, out of a total of 224 possible offers.
8 We explain how we collected this data set in the Sect. 7
123

Auton Agent Multi-Agent Syst
6.2 Proposing and revealing
This section describes our model of how people make proposals in revelation games and
reason about whether to reveal information.
6.2.1 First proposal model
We used standard density estimation techniques (histograms) to predict peopleâ€™s offers for
different types. Based on the assumption that proposals for the ï¬rst round depend on the
proposerâ€™s type and its decision whether to reveal, we divided the possible proposals to
equivalence classes according to the potential beneï¬t for the proposer player, and counted
how many times each class appears in the set. Let p(Ï‰1
p, j | tp, Ï†i) denote the probability that
a human proposer of type tp offers Ï‰1
p, j in round 1. Let Ntp,Ï†p

Ï€p(Ï‰1
p, j | tp)

denote the
number of proposals in round 1 which gives the human a beneï¬t of Ï€p(Ï‰1
p, j | tp), given the
human is of type tp and its revelation decision was Ï†p. Let Ntp,Ï†p(Î©1
p, j) denote the number
of proposals in round 1 in this subset. p(Ï‰1
p, j | tp, Ï†p) is deï¬ned as:
p(Ï‰1
p, j | tp, Ï†p) =
Ntp,Ï†p

Ï€p(Ï‰1
p, j | tp)

Ntp,Ï†p(Î©1
p, j)
(30)
6.2.2 Counter-proposal model
According to our model, a playerâ€™s proposal in the second round also depends on the history,
this two dimensional probability density function tends to be too sparse to calculate it directly
as described in Sect. 6.2.1. Inspired by studies showing that people engage in tit-for-tat
reasoning [27] we used this principal to model the counter-proposals made by people. We
assumed that a responder player i will be proposed offer Ï‰2
p,i by a human player in the
second round with beneï¬t Ï€i(Ï‰2
p,i | ti) that is equal to the beneï¬t Ï€p(Ï‰1
i,p | tp) from offer
Ï‰1
i,p made to the person in the ï¬rst round, when the human was a responder. For example,
suppose that Bob is the proposer in round 1 and propose to Alice a beneï¬t of 125. According
to the model, if Alice rejects the offer she will propose Bob a counter-proposal that provides
Bob with the same beneï¬t, 125. Note that this does not assume that the proposal will provide
Alice with the same beneï¬t she got from Bob in the proposal from round 1. Formally,
let NÎ©2
p,i (Ï€p(Ï‰1
i,p | tp)) denote the number of counter-proposals Ï‰2
p,i which give beneï¬t
Ï€p(Ï‰1
i,p | tp). We assume that there always exists at least one proposal that meets this
criterion, i.e., NÎ©2
p,i (Ï€p(Ï‰1
i,p | tp)) Ì¸= 0. The â€œtit for tatâ€ heuristic is as follows:
p(Ï‰2
p,i | h1) =
â§
â¨
â©
0
Ï€i(Ï‰2
p,i) Ì¸= Ï€p(Ï‰1
i,p)
1/NÎ©2
p,i (Ï€p(Ï‰1
i,p | tP))
otherwise
(31)
This heuristic is used in Eq. 18 to facilitate the computation of the expected beneï¬t from
SIGAL as a responder in round 1.
Lastly, we detail the model used by SIGAL to predict whether the person reveals its goal.
Let Ntp denote the number of instances in which people were of type tp, and let Ntp(Ï†p)
denote the number of times that people of type tp chose to reveal their type. The probability
that a human player p revealed its type tp is deï¬ned as:
123

Auton Agent Multi-Agent Syst
p(Ï†p | tp) = Ntp(Ï†p)
Ntp
(32)
7 Empirical methodology
In this section we describe the methodology we used in order to learn the parameters of
the model of how people play revelation games, and to evaluate it. For these purposes we
recruited 260 students enrolled in a computer science or software engineering program at
several universities and colleges. An additional 143 people were recruited using the Ama-
zon Turk framework. All subjects received an identical tutorial on revelation games that was
exempliï¬ed on a board (not the boards used in the study). Actual participation was contingent
on successfully answering a set of basic comprehension questions about the game. Partic-
ipants were seated in front of a terminal for the duration of the study, and could not speak
to any of the other participants. Each participant played two revelation games on different
boards.
The boards in the study fulï¬lled the following conditions at the onset of the game: (1)
There were two goals for each player; (2) Every player lacked one or two chips to reach each
of its possible goals; (3) Every player possessed the chips that the other needed to get to each
of its possible goals; (4) There existed at least one exchange of chips which allowed both
players to reach each of their possible goals; (5) the goals were distributed with a probability
of 50% for both players.
We used the asymmetric and symmetric boards shown in Fig. 1a, b. Participants played
both symmetric and asymmetric boards in random order. They engaged in a neutral activity
(answering demographic questions) between games to minimize the effects of their behavior
in the ï¬rst game on their behavior in the second game. The participant chosen to be the
proposer in the ï¬rst game was randomly determined, and participants switched roles in the
second game, such that the proposer in the ï¬rst game was designated as the responder in
the second game. A central server (randomly) matched each participant with a human or
an agent counterpart for each game. The identity of each participant was not disclosed. We
collected playersâ€™ proposals, responses and revelation decisions for all of the games played.
To avoid deception all participants were told they would be interacting with a computer or
a person. Participants received ï¬xed compensation (course credit) for participating in the
experiment.9
We divided subjects into four disjoint pools. The ï¬rst pool consisted of people playing
other people (66 games). The second pool consisted of people playing a computer agent
that used a randomized strategy to make offers and responses (170 games). The purpose for
this pool was to collect peopleâ€™s actions for diverse situations, for example, their response
to offers that were never made by other people. Two thirds (44 games) of the data from
the ï¬rst pool and the data from the second pool were used for training a model of peopleâ€™s
behavior. The third pool consisted of people playing the SIGAL agent (238 games). The
fourth pool (118 games) consisted of people playing an agent using several of the equilibrium
strategies deï¬ned in Sect. 4 to play revelation games. The equilibrium agent adapted a non-
revelation and revelation strategy equilibrium as specifed in Sects. 4.1 and 4.2 for each
game.
9 Our goal was to build an agent that negotiates well with people, not to explain peopleâ€™s incentives, therefore
ï¬xed compensation was sufï¬cient.
123

Auton Agent Multi-Agent Syst
8 Results and discussion
The performance of SIGAL was measured by comparing its performance against people (the
third pool) with peopleâ€™s play against other people (the ï¬rst pool). We list the number of
observations and means for each result. All results reported in this section are statistically
signiï¬cant in the p < 0.05 range using t tests for normally distributed data and two-sample
Mannâ€“Whitney rank tests for non-normally distributed data.
8.1 Analysis: general performance
We ï¬rst present a comparison of the performance of SIGAL and people. Figure 2 shows the
average beneï¬t (the difference in score between agreement and the no-negotiation alternative
score) for different roles (proposer and responder). As shown by the Figure, the SIGAL agent
outperformed people in all roles. In addition, SIGAL was also more successful at reaching
agreements than were people. Figure 3 shows the percentage of offers accepted by people
for the different roles. As shown by the Figure, proposals made by SIGAL in round 1 were
accepted 62% of the time, while proposals made by people in round 1 were accepted only
49% of the time. This difference is more pronounced in round 2, in which proposals made
by SIGAL were accepted 83% of the time, while offers made by people in round 2 were only
accepted 63% of the time. If an offer is rejected at this last round, the game ends without
agreement. This striking difference shows that SIGAL learned to make good offers at critical
points in the game.
As shown in Figure 2 SIGAL also outperformed the equilibrium agent in both rounds.
The equilibrium agent was fully strategic and assumed the other player was unboundedly
rational. Although not shown in the Figure, it made very selï¬sh offers in the last round,
offering only 25 average beneï¬t points to people and 215 beneï¬t points to itself. Most of
these offers (54%) were rejected. In the ï¬rst round, it made offers that were highly beneï¬cial
to people, offering 219 average points to people and 20 to itself. Most of these offers (82%)
were accepted, but the small beneï¬t it incurred in these proposals not aid its performance.
Fig. 2 Performance comparison
123

Auton Agent Multi-Agent Syst
Fig. 3 Agreement comparison
Fig. 4 Average proposed beneï¬t in ï¬rst and second rounds
To explain the success behind SIGALâ€™s strategy, we present a comparison of the beneï¬t
from proposals made by the SIGAL agent and people in both game rounds in Fig. 4. As shown
by the Figure, both people and SIGAL made offers that were beneï¬cial to both players in
rounds 1 and 2. However, SIGAL made offers that were signiï¬cantly more generous to human
responders than did human proposers (118 beneï¬t points provided by SIGAL as proposer
in round 1 versus 96 points provided by human proposers; 110 beneï¬t points provided by
SIGAL as proposer in round 2 versus 81 beneï¬t points provided by human proposers). In
fact, the proposals made by SIGAL pareto dominated the proposals made by people in both
123

Auton Agent Multi-Agent Syst
Table 1 Features coefï¬cients
weights
Feature
Value
REV 0
j
0.258
REV 0
i
0.035
BE Nn
j
0.956
AI n
i
âˆ’0.792
P.BE Nn
j
0.496
P.BE Nn
i
0.334
Free parameter
0.608
rounds 1 and 2. Thus, SIGAL learned to make offers that were better for human responders
without compromising its own utility
SIGALâ€™s strategy is highlighted by examining the weights learned for the different features
of how people accept offers. As shown in Table 1, the largest weight was assigned to BE N n
j ,
the beneï¬t to the responder from an offer. In addition, the weight for AI n
i measuring the
difference between the beneï¬t for the proposer and responder was large and negative. This
means that responders prefer proposals that provide them with large beneï¬ts, and are also
competitive, in that they dislike offers that provide more to proposers than to responders.
The offers made by SIGAL reï¬‚ect these criteria. In particular, proposers asked more for
themselves than for responders in both rounds. In contrast, SIGAL equalized the difference
in beneï¬t between proposers and responders in round 1, and decreased the difference between
its own beneï¬t and responderâ€™s beneï¬t in round 2 as compared to human proposer.
8.2 Analysis: revelation of goals
We now turn to analyzing the affect of goal revelation on the behavior of SIGAL. Recall
that Ea

Ï†âˆ—
a = ta

denotes the value of the game for SIGAL when deciding to reveal its
goal in round 0, and behaving optimally according to its model of how people make offers.
Similarly, Ea

Ï†âˆ—
a = null

denotes the value of the game for SIGAL when deciding not to
reveal its goal in round 0. Our model predicted no signiï¬cant difference in value to SIGAL
between revealing and not revealing its goal, i.e. Ea

Ï†âˆ—
a = null

â‰ˆEa

Ï†âˆ—
a = ta

for each
type ta âˆˆTa. Therefore we used two types of SIGAL agents, one that consistently revealed
its goal at the onset of the game and one that did not reveal. In all other respects these
agents followed the model described in Sect. 5. The empirical results conï¬rmed the modelâ€™s
prediction, in that there was no signiï¬cant difference in the performance of the two SIGAL
agents for all boards and types used in the empirical study.
However, the decision of the person to reveal or not reveal its goal had a signiï¬cant
affect on the negotiation strategy of SIGAL. When people revealed their goals, SIGAL was
signiï¬cantly more competitive as compared to the case in which people did not reveal their
goals. Speciï¬cally, the competitive weight AI n
i from proposals made by SIGAL in round 1
was 14 points when people revealed their goal, and âˆ’3 points when people did not reveal their
goal. This means that SIGAL learned to make offers that were signiï¬cantly more competitive
when people revealed their goals.
8.3 Analysis: peopleâ€™s strategies
Wenowturntodescribeseveralfeaturesinpeopleâ€™sbehaviordemonstratingstrategicbehavior
in the game that resembled the way SIGAL played. In the asymmetric game, people in weak-
123

Auton Agent Multi-Agent Syst
type roles were more likely to reveal their goals than people in strong-type roles. In particular,
in 65% of games played in the asymmetric game, strong-type players engaged in â€œblufï¬ngâ€,
that is, not disclosing their true goal and asking for more chips than they actually need to get to
the goal in round 1. Similarly, in 56% of games in the asymmetric condition, weak-type play-
ers engaged in blufï¬ng. Interestingly, this trend was also apparent for the symmetric game, in
which blufï¬ng occurred in 63% of the games played. Second, peopleâ€™s proposals were signif-
icantly more selï¬sh and less generous in round 2 than in round 1. This is apparent from Fig. 2.
Interestingly, peopleâ€™s performance was signiï¬cantly higher when playing with SIGAL than
when playing with other people. Speciï¬cally, people playing with SIGAL achieved an aver-
age performance of 112 points, while people playing with other people achieved an average
performance of 91 points. This shows that SIGAL had a positive affect on peopleâ€™s play.
The last part of the analysis compares performance of subjects enlisted using Amazon
Turk (MTurk) with students. There are several works dupicating lab studies using MTurk
showing similar patterns of behavior [1]. In our work, we found that in general, students
were better performers than MTurk workers (average score was 123 points for students
and 109 points for MTurk workers). This can be explained by the fact that students were
signiï¬cantly more selï¬sh as second proposers (average 45 points for students and 20 points
for MTurk workers). Also, MTurk workers accepted signiï¬cantly more proposals than did
students in the ï¬rst round (75% average acceptance rate for MTurk compared to 59% average
acceptance rate for students). These results suggest that integrating information revelation
with negotiation required more cognitive effort from people than canonical decision-making
tasks.
8.4 Limitations
We conclude this section with describing several limitations of our approach. First, our def-
inition of revelation games, which the SIGAL decision-making model is tailored to support,
deï¬nes a single revelation phase followed by two rounds of take-it-or-leave-it offers. This
model fails to describe more involved negotiations which may include additional rounds or
multiple instances of revelation. Indeed, some agents may wish to reveal their goal only after
not succeeding to reach agreement in several rounds. Allowing for more involved revelation
protocols raises signiï¬cant computational challenges which remain outside the scope of this
paper. Second, the decision-making model used by SIGAL in this paper assumes that all
people use tit-for-tat reasoning when predicting their performance, which may not hold in
practice. Indeed, or results showed considerable variance in peopleâ€™s behavior for different
countries. We hypothesize that learning separate decision-making models for each conuntry
may additionally improve the prediction power of SIGAL and consequently, its performance.
9 Conclusion and future work
This paper presented an agent-design for interacting with people in â€œrevelation gamesâ€, in
which participants are given the choice to truthfully reveal private information prior to nego-
tiation. The decision-making model used by the agent reasoned about the social factors that
affect peopleâ€™s decisions whether to reveal their goals, as well as the effects of peopleâ€™s rev-
elation decisions on their negotiation behavior. The parameters of the model were estimated
from data consisting of peopleâ€™s interaction with other people. In empirical investigations,
the agent was able to outperform people playing other people as well as agents playing equi-
librium strategies and was able to reach agreement signiï¬cantly more often than did people.
123

Auton Agent Multi-Agent Syst
We are currently extending this work in two directions. First, we are considering more elab-
orate settings in which players are able to control the extent to which they reveal their goals.
Second, we are using this work as the basis for a more broad argumentation in which agents
integrate explanations and justiï¬cations within their negotiation process. Lastly, information
revelation strategies can also be adopted by the negotiation community at large, for example
in the International Automated Negotiation Agent Competition (ANAC) [3].
Acknowledgments
This research is supported in part by the U.S. Army Research Laboratory and the U.S.
Army Research Ofce under Grant number W911NF-08-1-0144, by ERC Grant #267523, EU Grant FP7-ICT-
2011-9 #600854 and Marie Curie Grant #268362.
10 Appendix
Before proving Proposition 1, we will postulate the following lemma:
Lemma 1 Suppose that player j uses response strategy a2
j(Ï‰, t j) (Eq. 2) to accept proposals
Ï‰ at round 2. Then player i is always indifferent between revealing and not revealing its type,
regardless of the revelation decision of player j.
Proof We will show that the the revelation decisions of player i cannot affect its outcome
in the game. The strategy a2
j(Ï‰, t j) of player j to accept the proposal by i at round 2 does
not depend on iâ€™s type. Hence the revelation decision of i in round 0 does not affect its
beneï¬t from proposals in round 2. Suppose that player i revealed its type. If it can get a
more beneï¬cial proposal from player j by not revealing its type, then it will reject jâ€™s offer
and make the more beneï¬cial proposal in round 2. A similar argument can be used to show
that player i can get its more preferred outcome when it does not reveal its type. Hence the
revelation decision of i does not affect its outcome.
We provide a proof to Proposition 1 by showing that the strategies speciï¬ed in Sect. 4.1
correspond to a perfect Bayesian equilibrium given the sufï¬cient conditions.
Proof We denote the possible types for j as t j1 and t j2 and the possible types for i as ti1 and
ti2. We prove that when one type of player j is preferable to the other type (Eq. 6), there
always exists a revelation equilibrium in which players reveal their goals for both types.10 In
round 2, player i makes the proposal Ï‰2âˆ—
i, j(t j) using Eq. 4. This proposal gives the maximum
beneï¬t for player i among the proposals that are accepted by player j. Any proposal with
higher beneï¬t for i will be rejected by player j.11 Player j also cannot beneï¬t from deviation
as rejecting the proposal incurs zero beneï¬t to both players.
We proceed to round 1. We ï¬rst specify why the belief update rules are part of a perfect
Bayesian equilibrium. Suppose j is of type t j2 and that this type is preferable for j (without
loss of generality). If j revealed its type, then i uses Bayes rule to assign probability 1 to type
t j2, as speciï¬ed in equilibrium. If j does not reveal its type, then the resulting information set
is off the equilibrium path. In this case i makes an arbitrary belief update to assign probability
0 to type t j2. Using a similar argument, we can show that the belief update rules are part of
a perfect Bayesian equilibrium when type t j1 is preferable to j. Suppose that player i is of
10 The existence of a preferable type for a player is a reasonable assumption in negotiation. Only if it is the
case that both players prefer the proposal that is associated with the type of the other, there is no preferable
type.
11 In the case where there are no proposals that are accepted by player j, the beneï¬t for player i is zero.
123

Auton Agent Multi-Agent Syst
type ti1. If i reveals its type, then j uses Bayes rule to assign probability 1 to ti2, as speciï¬ed
in equilibrium. If i does not reveal its type, then the resulting information set is off the
equilibrium path. In this case j makes an arbitrary belief update to assign probability 1 to
ti1. Using a similar argument, we can show that the belief update rules are part of a perfect
Bayesian equilibrium when i is of type ti2.
Next, we specify why playersâ€™ actions are in equilibrium in round 1. Player j makes the
proposal w1âˆ—
j,i(ti) using Eq. 5. Any proposal with a higher beneï¬t to j will necessarily be
rejected by player i. As a result, player jâ€™s proposal maximizes its beneï¬t given iâ€™s response
strategy. Player i also cannot beneï¬t from deviation because player jâ€™s proposal gives it the
same beneï¬t as its counter-proposal Ï‰2âˆ—
i, j(t j).
Finally, we consider round 0, in which both players i and j reveal their types for both of
their types. A deviation for player j of type t j2 in round 0 means that j chooses not to reveal its
type. According to the belief-update rules, player i will update its beliefs to assign probability
1 to type t j1. As a result, it will make a proposal in round 2 to j that is less preferable to the
one it would receive if it revealed its type. Thus, deviation from its revelation decision is not
strictly beneï¬cial for j when it is of type t j2. In a similar fashion, we can show that deviation
from a revelation decision is not beneï¬cial for j when it is of type t j1. Thus the revelation
strategies of round 0 are in Equilibrium for j.
Due to Lemma 1, a decision by player i to reveal its type in round 0 for each of its types
is in equilibrium. Thus there always exists a revelation equilibrium in which players reveal
their goals for both types.
We provide a proof to Proposition 1 by showing that the strategies speciï¬ed in Sect. 4.2
specify a perfect Bayesian Equilibrium.
Proof We use backward induction. In Round 2 there are two cases. On the equilibrium path.
In this case the players did not update their beliefs according to the equilibrium speciï¬cation.
Player i makes the proposal Ï‰2âˆ—
i, j(h1, ti) using Eq. 10. This proposal gives the maximum
expected beneï¬t for player i, so it cannot on expectation beneï¬t from deviation. Player j also
cannot beneï¬t from deviation as rejecting the proposal incurs zero beneï¬t to both players.
Lets examine the cases where the information set is off the equilibrium path. The ï¬rst
case is related to round 0: if i revealed its type, this will not affect the proposal at round
2 (Proposition 2). If j revealed its type, then i knows its true type, and the strategies are
in equilibrium as in the revelation equilibrium. The other case is related to round 1: In this
case, as deï¬ned in the Equilibrium, the players do not change their beliefs, and the strategies
in round 2 are as in the equilibrium. We now proceed to Round 1. we show neither player
has incentive to deviate from its strategy. Suppose player j of type t j wishes to deviate and
propose a different proposal. If the proposal is worse off for player i, given its type ti, then
player i will reject the offer and player j will not beneï¬t. If player i accepts the proposal
(according to Eq. 12), this means that it is greater or equal in beneï¬t to i than the equilibrium
proposal in round 2. By the second condition (Eq. 14), this means that the proposal will be at
most equal to the beneï¬t that player j will receive from the equilibrium proposal. So player
j will not beneï¬t from making this proposal. For player i, it cannot beneï¬t from rejecting
jâ€™s proposal. According to Eq. 12, player i accepts player jâ€™s proposal only if it cannot get
a higher beneï¬t from its own counter-proposal.
Finally, Lets examine if the player can beneï¬t from deviation in Round 0. Suppose that
player j of type t j1 deviates and reveals its type. By deï¬nition, then player i will update
its beliefs to its true type. In this case, player i will make the proposal Ï‰2âˆ—
i, j(t j1). However,
by the ï¬rst condition (Eq. 13), this proposal is at most equal to the proposal that player j
123

Auton Agent Multi-Agent Syst
will get from not revealing its type. Therefore it is not beneï¬cial for player j to deviate. By
Proposition 1, player i is indifferent between its two types. Thus the strategy for both players
satisï¬es a perfect Bayesian equilibrium.
We conclude by showing that the sufï¬cient conditions of proposal 2 hold in the symmetric
(Fig. 1a) and the asymmetric (Fig. 1b) boards described in the paper:
Symmetric board: For the ï¬rst condition (13), Ï€ j(Ï‰2âˆ—
i, jl(t j) | tl) = 20 for l âˆˆ1, 2, is
smaller than Ï€ j(Ï‰2âˆ—
i, j(h1, ti) = 25, thus the condition holds. For the
second condition (14), Ï‰2âˆ—
i, j(h1, ti) let player i reach its goal, and so for
both types of player j. In such case, the only way for a player to get
more points is by asking for more chips, which means that the game
turns to be a zero-sum game. In such case, there is no proposal that can
be more beneï¬cial for both players, thus the condition holds.
Asymmetric board: For the ï¬rst condition (13), Ï€ j(Ï‰2âˆ—
i, jl(t j) | t1) = 30 for t1 as the
weak type, and Ï€ j(Ï‰2âˆ—
i, jl(t j) | t1) = 10 for t2 as the strong type.
Ï€ j(Ï‰2âˆ—
i, j(h1, ti) = 35 for t1 and 15 for t2, thus the condition holds.
For the second condition (14), like in the symmetric board Ï‰2âˆ—
i, j(h1, ti)
let both types of player j and player i to reach their goal, so the game
turns to be a zero-sum game, and the condition holds.
References
1. Amir, O., Rand, D. G., et al. (2012). Economic games on the internet: The effect of $1 stakes. PloS ONE,
7(2), e31461.
2. Azaria, A., Rabinovich, Z., Kraus, S., & Goldman, C. V. (2011). Strategic information disclosure to people
with multiple alternatives. In Proceedings of AAAI.
3. Baarslag, T., Fujita, K., Gerding, E. H., Hindriks, K., Ito, T., Jennings, N. R., et al. (2013). Evaluat-
ing practical negotiating agents: Results and analysis of the 2011 international competition. Artiï¬cial
Intelligence, 198, 73â€“103.
4. Banks, J., Camerer, C. F., & Porter, D. (1994). Experimental tests of nash reï¬nements in signaling games.
Games and Economic Behavior, 6, 1â€“31.
5. Bench-Capon, T., Atkinson, K., & McBurney, P. (2009). Altruism and agents: an argumentation based
approach to designing agent decision mechanisms. In Proceedings of AAMAS.
6. Coehoorn, R. M., & Jennings, N. R. (2004). Learning on opponentâ€™s preferences to make effective multi-
issue negotiation trade-offs. In Proceedings of EC.
7. de Melo, C., Carnevale, P., Read, S., Antos, D., & Gratch, J. (2012). Bayesian model of the social effects
of emotion in decision-making in multiagent systems. In Proceedings of AAMAS.
8. Erev, I., & Roth, A. E. (1998). Predicting how people play games: Reinforcement learning in experimental
games with unique, mixed strategy equilibria. American Economic Review, 88(4), 848â€“881.
9. Gal, Y., Grosz, B., Kraus, S., Pfeffer, A., & Shieber, S. (2010). Agent decision-making in open-mixed
networks. Artiï¬cial Intelligence, 174(18), 1460â€“1480.
10. Gal, Y., Kraus, S., Gelfand, M., Khashan, H., & Salmon, E. (2012). Negotiating with people across cultures
using an adaptive agent. ACM Transactions on Intelligent Systems and Technology, 3(1).
11. Gal, Y., & Pfeffer, A. (2007). Modeling reciprocity in human bilateral negotiation. In Proceedings of
AAAI.
12. Grosz, B., Kraus, S., Talman, S., & Stossel, B. (2004). The inï¬‚uence of social dependencies on Decision-
Making. In Proceedings of AAMAS.
13. Haim, G., Gal, Y., Kraus, S., & Gelfand, M. J. (2012). A cultural sensitive agent for Humanâ€“Computer
negotiation. In Proceedings of AAMAS.
14. Hajaj, C., Hazon, N., Sarne, D., & Elmalech, A. (2013). Search more, disclose less. In Proceedings of
AAAI.
15. Hindriks, K., & Tykhonov, D. (2008). Opponent modelling in automated multi-issue negotiation using
bayesian learning. In Proceedings of AAMAS, (pp. 331â€“338).
123

Auton Agent Multi-Agent Syst
16. Jonker, C. M., Robu, V., & Treur, J. (2007). An agent architecture for multi-attribute negotiation using
incomplete preference information. Autonomous Agents and Multi-Agent Systems, 15(2), 221â€“252.
17. Lin, R., & Kraus, S. (2010). Can automated agents proï¬ciently negotiate with humans? Communications
of the ACM, 53(1), 78â€“88.
18. McKelvey, R. D., & Palfrey, T. R. (1992). An experimental study of the centipede game. Econometrica:
Journal of the Econometric Society, 60(4), 803â€“836.
19. Osborne, M. J., & Rubinstein, A. (1999). A course in game theory. Cambridge, MA: MIT Press.
20. Oshrat, Y., Lin, R., & Kraus, S. (2009). Facing the challenge of human-agent negotiations via effective
general opponent modeling. In Proceedings of AAMAS.
21. Pasquier, P., Hollands, R., Dignum, F., Rahwan, I., & Sonenberg, L. (2007). An empirical study of
interest-based negotiation. In Proceedings of ICEC.
22. Peled, N., Gal, Y., & Kraus, S. (2011). A study of computational and human strategies in revelation games.
In Proceedings of AAMAS.
23. Rosenfeld, A., & Kraus, S. (2012). Modeling agents based on aspiration adaptation theory. Autonomous
Agents and Multi-Agent Systems, 24(2), 221â€“254.
24. Sarne, D., Elmalech, A., Grosz, B. J., & Geva, M. (2011). Less is more: Restructuring decisions to improve
agent search. In Proceedings of AAMAS.
25. Spence, A. M. (1974). Market signaling. Cambridge, MA: Harvard University Press.
26. Sycara, K. P. (1990). Persuasive argumentation in negotiation. Theory and decision, 28(3), 203â€“242.
27. Wedekind, C., & Milinski, M. (1996). Human cooperation in the simultaneous and the alternating pris-
onerâ€™s dilemma. PNAS, 93(7), 2686.
123

