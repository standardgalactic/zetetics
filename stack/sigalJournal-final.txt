Auton Agent Multi-Agent Syst
DOI 10.1007/s10458-014-9253-5
A study of computational and human strategies
in revelation games
Noam Peled · Ya’akov (Kobi) Gal · Sarit Kraus
© The Author(s) 2014
Abstract Many negotiations in the real world are characterized by incomplete information,
and participants’ success depends on their ability to reveal information in a way that facilitates
agreements without compromising their individual gain. This paper presents an agent-design
that is able to negotiate proﬁciently with people in settings in which agents can choose to
truthfully reveal their private information before engaging in multiple rounds of negotiation.
Such settings are analogous to real-world situations in which people need to decide whether
to disclose information such as when negotiating over health plans and business transac-
tions. The agent combined a decision-theoretic approach with traditional machine-learning
techniques to reason about the social factors that affect the players’ revelation decisions on
people’s negotiation behavior. It was shown to outperform people as well as agents playing
the equilibrium strategy of the game in empirical studies spanning hundreds of subjects. It
was also more likely to reach agreement than people or agents playing equilibrium strategies.
In addition, it had a positive effect on people’s play, allowing them to reach signiﬁcantly bet-
ter performance when compared to people’s play with other people. These results are shown
to generalize for two different settings that varied how players depend on each other in the
negotiation.
Keywords
Human–computer negotiation · Opponent modeling · Empirical studies
N. Peled
Gonda Brain Research Center, Bar-Ilan University, Ramat Gan, Israel
Y. Gal (B)
Department of Information Systems Engineering, Ben-Gurion University, Beer Sheva, Israel
e-mail: gal@eecs.harvard.edu
S. Kraus
Department of Computer Science, Bar-Ilan University, Ramat Gan, Israel
S. Kraus
Institute of Advanced Computer Studies, University of Maryland, College Park, MD, USA
123

Auton Agent Multi-Agent Syst
1 Introduction
In negotiations under incomplete information, people commonly need to make strategic
decisions about whether and how to reveal information to others. For example, consider a
scenario in which a bank is offering to purchase a struggling company in return for potential
job cuts. The unions may not allow the company to accept the offer because they refuse to
agree to layoffs. However, if the bank discloses that it is committed to keeping the company
aﬂoat, the unions may agree to the buy-out despite the layoffs. On the other hand, revealing
goals is often associated with a cost or can be exploited by the other party. In our example,
if the bank declares it is committed not to liquidate the company, the unions may demand no
layoffs.
The focus of this paper is on negotiation settings in which participants lack information
about each other’s preferences, often hindering their ability to reach beneﬁcial agreements.
Speciﬁcally, we study a particular class of such settings we call “revelation games”, in which
players are given the choice to truthfully reveal private information before commencing in
a ﬁnite sequence of alternating negotiation rounds. Revealing this information narrows the
search space of possible agreements and may lead to agreement more quickly, but may also
lead players to be exploited by others.
Revelation games combine two types of interactions that have been studied in the past in
the economics literature: Signalling games [25], in which players choose whether to convey
private information to each other, and bargaining [19], in which players engage in multiple
negotiation rounds. They are analogous to real-world scenarios in which parties may choose
to truthfully reveal information before negotiation ensues, such as the example presented
above.
Constructing effective agent strategies for such settings is challenging. On the one hand,
behavioral economics work has shown that people often follow equilibrium strategies [4]
when deciding whether to reveal private information to others. On the other hand, people’s
bargaining behavior does not adhere to equilibrium [8,18], and computers cannot use such
strategies to negotiate well with people [17].
The main contribution of the paper is an agent-design that incorporates information rev-
elation decisions into its negotiation strategy using decision theory and machine learning.
The agent tries to determine which proposals people are likely to accept based on past data,
or whether they are more or less likely to accept an offer when information is revealed.
It combines a prediction model of people’s behavior in the game with a decision-theoretic
approach to make optimal decisions. The model includes the social factors that affect people’s
decisions whether to reveal private information, (e.g., the generosity, competitiveness and
the selﬁshness of the offers they make, as measured in the scoring function of the game). In
addition, the model included the effects of people’s revelation decisions on their negotiation
behavior. It evaluated the agent-design with agents playing equilibrium strategies, as well as
other people, in two types of revelation games that varied how players depend on each other
in the game.
The results showed that the agent was able to outperform human players playing other
people, as well as the equilibrium agent. It learned to make offers that were signiﬁcantly
more beneﬁcial to people than the offers made by other people, while not compromising its
own beneﬁt, and was able to reach agreement signiﬁcantly more often than did people as
well as the equilibrium agent. In particular, it was able to exploit people’s tendency to agree
to offers that are beneﬁcial to the agent if people revealed information at the onset of the
negotiation. It also positively affected people’s play, in that their overall performance was
signiﬁcantly better when paying the agent than when playing other people. The paper thus
123

Auton Agent Multi-Agent Syst
has signiﬁcance for agent designers, showing that (1) people do not adhere to equilibrium
strategies when revealing information in negotiation; (2) reasoning about the social factors
that affect people’s decisions can signiﬁcantly improve agents’ performance as compared to
using equilibrium strategies.
This paper extends an initial study reporting on revelation games [22] in several ways.
First, it provides a formal equilibrium analysis of revelation games. Second, it analyzes
people’s behavior in these games, including the extent to which they respond to proposals
made by the agent, and how their performance is affected by playing with the agent. Lastly,
the empirical analysis includes additional games to the ones originally reported using subjects
from different countries. This provides further empirical support to the generalizability of
the agent to different demographic groups.
2 Related work
Our work is related to a growing line of work in multi-agent systems that use opponent
modeling to build agents for repeated negotiation in heterogeneous human-computer set-
tings. These include the KBAgent that made offers with multiple attributes in settings which
supported opting out options and partial agreements [20]. This agent used a social utility
function to consider the trade-offs between its own beneﬁt from an offer and the probability
that it is accepted by people. It used density estimation to model people’s behavior following
a method suggested by Coehoorn and Jennings for modeling computational agents [6] and
approximated people’s reasoning by assuming that people would accept offers from com-
puters that are similar to offers they make to each other. Other works employed Bayesian
techniques [7,15] or approximation heuristics [16] to estimate people’s preferences in nego-
tiation and integrated this model with a pre-deﬁned negotiation or concession strategy to
make offers. Bench-Capon [5] provide an argumentation based mechanism for explaining
human behavior in the ultimatum game. Work by Rosenfeld and Kraus [23] used Aspira-
tion Adaptation Theory was more useful that other bounded and strictly rational models in
quantifying peoples’ negotiation preferences. There is also prior work dealing with selective
information disclosure in human-agent settings [2,14,24].
None of these works allowed for agents to reveal private information during negotiation.
In addition, we are the ﬁrst to develop a strategic model of people’s negotiation behavior that
reasons about information revelation, to formalize an optimal decision-making paradigm for
agents using this model.
Gal and Pfeffer [11] proposed a model of human reciprocity in a setting consisting of
multiple one-shot take-it-or-leave-it games, but did not evaluate a computer agent or show
how the model can be used to make decisions in the game. Other works [9,10,13] combined
a decision-theoretic approach with a set of rules to adapt to people’s negotiation in settings
with non-binding agreements. Our work augments these studies in allowing players to reveal
private information and in explicitly modeling the effect of revelation on people’s negotiation
behavior.
Our work is also related to computational models of argumentation, in that people’s
revelation decisions provide an explanation of the type of offers they make during negotiation.
Work in interest-based negotiation has studied different protocols that allows players to reveal
their goals in negotiation in a controlled fashion [21,26]. These works assume that agents
follow pre-deﬁned strategies for revealing information and do not consider or model human
participants.
123

Auton Agent Multi-Agent Syst
Lastly, revelation games, which incorporate both signaling and bargaining, were inspired
by canonical studies showing that people learn to play equilibrium strategies when they
need to signal their private information to others [4]. On the other hand, people’s bargaining
behavior does not adhere to equilibrium [8,18], and computers cannot use such strategies
to negotiate well with people [17]. Our work shows that integrating opponent modeling and
density estimation techniques is an effective approach for creating agents that can outperform
people as well equilibrium strategies in revelation games.
3 Implementation: colored trails
We based our empirical work on a test-bed called Colored Trails [12], which we adapted to
model revelation games with 2 rounds, the minimal number that allows an offer to be made
by both players. Our revelation game is played on a board of colored squares. Each player
has a square on the board that is designated as its goal. The object of the game is to reach the
goal square. To move to an adjacent square required surrendering a chip in the color of that
square. Players had full view of the board and each others’ chips. Both players were shown
two possible locations for their goals with associated belief probabilities, but each player
could only see its own goal.
We deﬁne the no-negotiation alternative score for a player as the score obtained when
agreement is not reached. The study included two CT board games. The ﬁrst board game,
called symmetric, is shown in Fig. 1a. The initial chip allocation for each player is speciﬁed in
the Player Chip Display panel. Here, the “me” and “O” icons represent two players, Bob and
Alice, respectively. Each player has two possible goals. For example, Bob has two possible
goals. Bob’s true goal is located four steps to the left of the “me” icon (appearing as a white
G square), while Bob’s other goal is located four steps below the “me” icon (appearing as
(a) ymmetric Board Game
(b) Asymmetric Board
Game
(c) A possible proposal
Fig. 1 Two CT revelation games (shown from Bob’s point of view). a Symmetric Board Game, b asymmetric
Board Game, c a possible proposal
123

Auton Agent Multi-Agent Syst
a grey square outlined with a “?” symbol). In turn, Alice’s possible goals are presented as
two grey circles outlined with “?” symbols. The board is presented from Bob’s point of
view. Bob can see its true goal location but Alice does not observe it. Unless Bob chooses
to reveal its goal, Alice does not know whether Bob needs a purple or light-green chip to
reach the goal. Similarly, Bob cannot observe Alice’s true goal location. The number “50”
on each goal square represent a 50% probability that the true goal lies in that square. In
the symmetric board, the length of the path between the Alice’s location and each of her
possible goal squares was equal. Consequently, for the symmetric board, the no-negotiation
alternative score of each player does not depend on its true goal location. The analysis from
Bob’s point of view is similar. In contrast, in the second board, called asymmetric, shown in
Fig. 1b, one of the players’ goal locations is closer to its starting position than the other.
Our CT game progresses in three phases with associated time limits. In the revelation
phase (round 0), both players can choose to truthfully reveal their goal to the other player.1 In
the proposal phase (round 1), one of the players is randomly assigned the role of proposer and
can offer to exchange a (possibly empty) subset of its chips with a (possibly empty) subset
of the chips of the other player. If the responder accepts the offer, the chips are transferred
automatically according to the agreement, both participants will automatically be moved as
close as possible to the goal square given their chips and the game will end. If the responder
rejects (or no offer was received), it will be able to make a counter-proposal (round 2). If the
proposal is accepted, the game will end with the agreement result as above. Otherwise, the
game will end with no agreement.
At the end of the game, the score for each player is computed as follows: 100 points
bonus for reaching the goal; 5 points for each chip left in a player’s possession, and 10 points
deducted for any square in the path between the players’ ﬁnal position and the goal-square.2
The CT game conﬁguration described above is analogous to task settings involving infor-
mation revelation and incomplete information. We illustrate this analogy using the union
agreement scenario presented in the Sect. 1 Goals on the board represent private information
that is available to the negotiation parties, such as whether the bank is intending to liquidate
the company. Different squares on the board represent different types of sub-tasks, such as
agreeing on the number of layoffs, hiring new personnel, and deciding on compensation.
Chips correspond to agent capabilities and skills required to fulﬁll sub-tasks. Traversing a
path through the board to the goal square represents reaching agreement that is composed
of the various sub-tasks on the paths. The game is interesting because players need to rea-
son about the tradeoff between revealing their goals and providing information to the other
player, or not to reveal their goals to possibly receive or ask for more chips than they need.
In addition, there is an advantage to the proposer player in the second round, in that it makes
the ﬁnal offer in the game. But the identity of the second proposer is not known at the time
that players decide whether to reveal their goals.
4 Equilibrium analysis
In this section we formalize the colored trails game described earlier as a repeated game of
imperfect information. Each player has a type ti that represents the true position of its goal
on the board.3 Let ωn represent an offer made by a proposer player at round n ∈{1, 2} in
1 This decision is performed simultaneously by all players, and goals are only revealed at the end of the phase.
2 This path is computed by the Manhattan distance.
3 Revealing goals in the game thus corresponds to making players’ types common knowledge.
123

Auton Agent Multi-Agent Syst
a game. Let rn ∈{accept, reject} represent the response to ωn by a responder player. Let
si represent the score in the game as described in the previous section. The no-negotiation
alternative (NNA) score to player i of type ti is the score for i in the game given that no
agreement was reached. We denote the score for this event as si(∅| ti). If no agreement was
reached in round 1 of the game, players’ ﬁnal score depends on whether the counter-proposal
in round 2 is accepted. If no agreement was reached in round 2 (the last round) of the game,
players’ NNA score is also their ﬁnal score in the game. We denote the beneﬁt to player i
from ωn given that rn = accept as πi(ωn | ti). This is deﬁned as the difference in score to i
between an offer ωn and the NNA score:
πi(ωn | ti) = si(ωn | ti) −si(∅| ti)
(1)
Let Ti denote a set of types for a general player i. Let revi denote the event in which i
reveals its type ti ∈Ti, and let noRevi denote the event in which i does not reveal its type.
Let hn denote a history of moves, including for both players i and j their revelation decision
at the onset of the game, and the proposals and responses for rounds 1 through n. For the
remainder of this section we will assume (without loss of generality) that player j is the
proposer in round 1, and player i as the proposer in round 2. We denote by p(t j | hn) the
probabilistic belief of player’s i on player j being of type t j (and similarily for player j).
Let a2
j(ω, t j) deﬁne a strategy for responder j at round 2 of type t j that accepts any
proposal ω with positive beneﬁt.
a2
j(ω, t j) =

accept
if π j

ω | t j

≥0
reject
otherwise
(2)
We will abuse notation and write a2
j when the proposal ω is clear from context. We extend
Eq. 1 to deﬁne the beneﬁt to proposer i given the response strategy a2
j(ω, t j).
πi(ω, a2
j, t j | ti) =

πi(ω | ti)
if a2
j(ω, t j) = accept
0
otherwise
(3)
We denote a proposal from j to i in round 1 as ω1
j,i and a proposal from i to j in round
2 as ω2
i, j (also called i’s counter-proposal). We deﬁne several possible strategies for making
revelation decisions and proposals in the game, and proceed to show that these strategies are
in equilibrium under certain sufﬁcient conditions.
4.1 Revelation by both players
We formalize a revelation equilibrium using backward induction and then provide the condi-
tions under which the equilibrium holds. We assume two possible types for player j, denoted
t j1 and t j2, and two possible types for player i, denoted ti1 and ti2. We begin with the following
deﬁnitions. Let ω2∗
i, j(t j) denote a proposal in round 2 that maximizes the beneﬁt of player i
given that player j is of type t j, the history h1 = {revi, revj, ω1
j,i, reject}, and that player j
uses strategy a2
j of Eq. 2 to accept proposals.
ω2∗
i, j(t j) ∈arg max
ω2
i, j∈Ωi, j
πi

ω2
i, j, a2
j, t j | ti

(4)
where (without loss of generality) Ωi, j is all the set of all the proposals player i can propose
to player j.
123

Auton Agent Multi-Agent Syst
Let ω1∗
j,i(ti) be the proposal that will provide player j with the same beneﬁt as the proposal
made by player i in round 2.
ω1∗
j,i(ti) ∈arg max
ω j,i∈Ω j,i

π j(ω j,i | t j) s.t. πi(ω j,i | ti) = πi(ω2∗
i, j | ti)

(5)
where ω2∗
i, j satisﬁes Eq. 4.
We say that t j1 is preferable for j if its beneﬁt from the proposal that j receives in round
2 is greater when it is computed for type t j1 than type t j2, given that j is of type t j1.
π j(ω2∗
i, j(t j1) | t j1) ≥π j(ω2∗
i, j(t j2) | t j1)
(6)
Similarly, we can deﬁne when t j2 is preferable for j. For example, consider the asymmetric
game of Fig. 1b. For each player, there are two types in this game, a “weak” type, that is
missing two chips to get to the goal, and a “strong” type, that is missing one chip to get to
the goal. The weak type will prefer the proposal that assumes it is the weak type (which will
provide it with two chips) than the proposal that assumes it is the strong type (which will
provide it with one chip). Thus, for each player, the weak type is preferable to the strong
type.
We propose the following perfect Bayesian equilibrium.
Round 0: Both players reveal their types for each of their types.
Round 1: We distinguish between two cases. In each case we specify how i updates its beli
In this case, player i updates its beliefs to assign probability 1 to j’s type.
In the second case, player j deviates from equilibrium and does not reveal its
type. In this case, player i will update its beliefs over j as follows. If type t j1 is
preferable to j, then player i will update its beliefs that player j is of type t j2.
P(t j1 | h1) = 0
(7)
(And conversely for the case in which type t j2 is preferable to player j). Ifi revealed
its type in round 0 then player j assign probability 1 to i’s type. Otherwise, it will
arbitrarily assign probability 1 to ti1.
We now describe the actions for players i and j in round 1. Player j makes
a proposal ω1∗
j,i(ti) that satisﬁes Eq. 5. In turn, player i will use the following
acceptance strategy a1
i (ω, ti, t j) such that
a1
i (ω, ti, t j) =

accept
if πi(ω | ti) ≥πi(ω2∗
i, j(t j) | ti)
reject
otherwise
(8)
Round 2: Both players do not update their beliefs over each other’s actions in round 1.
Player i makes a proposal ω2∗
i, j that satisﬁes Eq. 4. In turn, player j accepts any
proposal according to the strategy a2
j(ω, t j) speciﬁed in Eq. 2.
4.2 Revelation by neither player
In this case neither of the players disclose their true types at the onset of the game. We make
the following deﬁnitions. Let Eh1

πi

ω2
i, j | ti

denote the expected beneﬁt of i for proposal
ω2
i, j given its updated beliefs over j’s types and that j accepts beneﬁcial proposals.
Eh1
	
πi

ω2
i, j | ti

=

t j∈Tj
p

t j | h1
πi

ω2
i, j, a2
j, t j | ti

(9)
123

Auton Agent Multi-Agent Syst
Let ω2∗
i, j(h1, ti) be the proposal in round 2 of player i of type ti that maximizes Eq. 9 given
the history h1:
ω2∗
i, j(h1, ti) ∈arg max
ω2
i, j∈Ωi, j
Eh1
	
πi

ω2
i, j | ti

(10)
Let ω1∗
j,i be the proposal of player j in round 1 that provides it with the same beneﬁt from
the counter-proposal made by one of the types of player i in round 2. This proposal does not
depend on the type of j.
ω1∗
j,i ∈{ω2∗
i, j(h1, ti1), ω2∗
i, j(h1, ti2)}
(11)
where ω2∗
i, j(h1, ti) satisﬁes Eq. 10.
We deﬁne a perfect Bayesian equilibrium as follows:
Round 0: Neither player reveals its type for each of its type.
Round 1: We distinguish between two cases. In each case we specify how i updates its
beliefs about j’s type and which proposal j makes in round 1. In the ﬁrst case,
player j did not reveal its type (i.e., followed the equilibrium strategy). In this
case, player i does not update its beliefs over j’s types. In the second case, player
j revealed its type. In this case, player i updates its beliefs to assign probability 1
to j’s type. (And similarly for for player i). We now describe players’ actions in
round 1. Player j will make a proposal ω1∗
j,i that satisﬁes Eq. 11. In turn, player i
will use strategy a1
i (ω, ti) such that
a1
i (ω, ti) =

accept
if πi(ω | ti) ≥πi(ω2∗
i, j(h1, ti) | ti)
reject
otherwise
(12)
where proposal ω2∗
i, j(h1, ti) satisﬁes Eq. 10.
Round 2: Player i does not update its beliefs over j’s types (and similarly for player j).
Player i will make a proposal ω2∗
i, j(h1, ti) that satisﬁes Eq. 10. player j accepts
any beneﬁcial proposal following Eq. 2.
4.3 Sufﬁcient condition for equilibria
We now describe sufﬁcient conditions for which the revelation and non-revelation equilibria
hold. For simplicity, we do make the assumption that player j has two types t j1 and t j2.
Suppose without loss of generality that j is of type t j1. Let ω2∗
i, j1 be the proposal made by
player i to player j of type t j1 that satisﬁes Eq. 4 (when player j reveals its type) and let
ω2∗
i, j(h1) be the proposal that satisﬁes Eq. 10 (when player j does not reveal its type) and that
h1 is the history up until round 2. We will postulate two propositions:
Proposition 1 If one of j’s type is preferable (Eq. 6) for j, then the strategies speciﬁed in
Sect. 4.1 constitute a perfect Bayesian equilibrium in which both players i and j reveal their
types for each type in round 0.
Proposition 2 If the below conditions hold, then the strategies speciﬁed in Sect. 4.2 constitute
a perfect Bayesian equilibrium in which both players do not reveal their types for each of
their types in round 0.
123

Auton Agent Multi-Agent Syst
The ﬁrst condition says j prefers not to disclose its type. Formally, for every type t j and
ti, we say that
π j(ω2∗
i, j(t j) | t j) ≤π j

ω2∗
i, j(h1, ti) | t j

(13)
The second condition says that any proposal that is greater or equal to i in round 1 is (at
least) worse off for j for each of its types. Formally, we say that for every type t j and ti, the
following holds.
∀ω1
j,i if πi(ω1
j,i | ti) ≥πi

ω2∗
i, j(h1, ti) | t j

then π j(ω1
j,i | t j) ≤π j

ω2∗
i, j(h1, ti) | t j

(14)
In the Appendix, we provide a proof to the Proposition above, and also show that the
sufﬁcient conditions hold in the board games in our study.
4.4 Adaptation of equilibria to CT game
We will exemplify the equilibrium strategies in the revelation games for the two boards in
Fig. 1a, b and demonstrate that they are in equilibrium. In this analysis, each of the two types
of each player is assigned a 50% prior probability. For expository convenience we assume
that the “me” player is the ﬁrst proposer in the game, as shown in the ﬁgure.4
We begin with the symmetric game in Fig. 1a. Here, both players are located at equal
distance of each of their goals. The no-negotiation alternative for both players is 80 points.
The “me” player has 24 chips at the onset of the game, and is missing one purple chip to
reach the goal square from its initial location. In this game we describe an equilibrium in
which neither player reveals its goal.
Round 0: Neither of the players reveals its goal.
Round 1: The “me” player will propose to give 21 chips (7 olive-green, 7 orange chips and 7
grey chips) in return for two chips (one purple and one green chip). This proposal
will yield a score of 105 for the “me” player and a score of 295 for the “O” player.
The “O” player accepts any proposal that provides it with higher beneﬁt than the
proposal it will make in round 2.
Round 2: The “O” player makes the same offer as in round 1. The “me” player accepts any
proposal that provides it with positive beneﬁt.
In contrast to the symmetric game, in the asymmetric game the distance of each player
from the goal depends on its true goal location, thus the no-negotiation alternative score
depends on the type of the player. We therefore distinguish between two possible types
of players, weak and strong. The weak player is missing two chips to get to the goal,
and its no-negotiation alternative score is 70 points. The strong player is missing a single
chip to get to the goal, and its no-negotiation alternative score is 90 points. In Fig. 1b,
which shows players’ starting positions on the game board, the “me” player’s type is
strong, because its starting position is located closer to its true goal location (the high-
lighted square marked “G”) than to its other possible goal location. For the asymmetric
game of Fig. 1b we present an equilibrium strategy that depend on the type of the “me”
player.
4 In practice, the identity of the ﬁrst proposer was determined stochastically according to a known probability
distribution. In practice this did not affect the equilibrium strategies that are described in this section.
123

Auton Agent Multi-Agent Syst
Round 0: Both of the player disclose their goals.
Round 1: The proposal made by the “me” player depends on its known type. A weak player
will propose 10 green chips and 11 gray chips to player “O” in return for 2 purple
chips. A strong player will propose 10 green chips and 12 gray chips to player “O”
in return for 1 purple chip. The “O” player accepts any proposal that provides it
with higher beneﬁt than the proposal it will make in round 2.
Round 2: The “O” player proposes the offer in round 1 for the the “me” player, which
depends on the “me” player’s type. The “me” player will accept any proposal with
positive beneﬁt.
5 The SIGAL agent
In this section we describe the Sigmoid Acceptance Learning Agent (SIGAL). SIGAL uses
a decision-theoretic approach to negotiate in revelation games, that is based on a model of
how humans make decisions in the game. Before describing the strategy used by SIGAL we
make the following deﬁnitions.
For the remainder of this section, we assume that the SIGAL agent (denoted a) is playing
a person (denoted p). Let ωn
a,p represent an offer made by the agent to the person in round
n and let rn
p represent the response of the person to ωn
a,p. The expected beneﬁt to SIGAL
from ωn
a,p given history hn−1 and SIGAL’s type tp is denoted Ea

ωn
a,p | hn−1, ta

. Let
p(rn
p = accept | ωn
a,p, hn−1) denote the probability that ωn
a,p is accepted by the person
given history hn−1.
We now specify the strategy of SIGAL for the revelation game deﬁned in Sect. 3. The
strategy assumes there exists a model of how humans make and accept offers in both rounds.
We describe how to estimate the parameters of this model in Sect. 5. We begin by describing
the negotiation strategies of SIGAL for rounds 2 and 1.
Round 2: If SIGAL is the second proposer, its expected beneﬁt from an offer (ω2
a,p) depends
on its model of how people accept offers in round 2, encapsulated in the probability p(r2
p =
accept | ω2
a,p, h1). The beneﬁt to SIGAL is
Ea

ω2
a,p | h1, ta

= πa(ω2
a,p | ta) · p(r2
p = accept | ω2
a,p, h1)
+ πa(∅| ta) · p(r2
p = reject | ω2
a,p, h1)
Here, the term πa(∅| ta) represents the beneﬁt to SIGAL from the NNA score, which is
zero. SIGAL will propose an offer that maximizes its expected beneﬁt in round 2 out of all
possible proposals for this round.
ω2∗
a,p = arg max
ω2a,p
Ea

ω2
a,p | h1, ta

(15)
If SIGAL is the second responder, its optimal action is to accept any proposal from the
person that gives it positive beneﬁt as described in the equilibrium strategy (Eq. 2). Let
r2∗
a (ω2
p,a | h1) denote the response of SIGAL to offer ω2
p,a, deﬁned as
r2∗
a (ω2
p,a | h1) =

accept
πa(ω2
p,a | ta) > 0
reject
otherwise
(16)
123

Auton Agent Multi-Agent Syst
where πa(ω2
p,a | ta) is deﬁned in Eq. 1. The beneﬁt to SIGAL from this response is deﬁned as
πa

r2∗
a
| ω2
p,a, h1, ta

=
⎧
⎨
⎩
πa(ω2
p,a | ta)
r2∗
a (ω2
p,a | h1) = accept
πa(∅| ta)
otherwise
(17)
Round 1: If SIGAL is the ﬁrst proposer, its expected beneﬁt from making a proposal ω1
a,p
depends on its model of the person: If the person accepts ω1
a,p, then the beneﬁt to SIGAL is
just πa(ω1
a,p | ta). If (ω1
a,p) is rejected by the person, then the beneﬁt to SIGAL depends on the
counter-proposal ω2
p,a made by the person in round 2, which itself depends on SIGAL’s model
p(ω2
p,a | h1) of how people make counter-proposals. The expected beneﬁt to SIGAL from
behaving optimally as a second responder for a given offer ω2
p,a is denoted Ea(resp2 | h1, ta),
and deﬁned as
Ea(resp2 | h1, ta)
=

ω2p,a
p(ω2
p,a | h1) · πa(r2∗
a
| ω2
p,a, h1, ta)
(18)
where πa(r2∗
a
| ω2
p,a, h1, ta) is deﬁned in Eq. 17.
Its expected beneﬁt from ω1
a,p is:
Ea

ω1
a,p | h0, ta

= πa(ω1
a,p | ta) · p(r1
p = accept | ω1
a,p, h0)
+ Ea(resp2 | h1, ta) · p(r1
p = reject | ω1
a,p, h0)
(19)
where h1 =

h0, ω1
a,p,r1
p = reject

. SIGAL will propose an offer in round 1 that
maximizes its expected beneﬁt in this round:
ω1∗
a,p = arg max
ω1a,p
Ea

ω1
a,p | h0, ta

(20)
If SIGAL is the ﬁrst responder, it accepts any offer that provides it with a larger beneﬁt
than it would get from making the counter-proposal ω2∗
a,p in round 2, given its model of how
people respond to offers in round 2:
r1∗
a (ω1
p,a | h0) =
⎧
⎪⎪⎨
⎪⎪⎩
accept
πa(ω1
p,a | ta) >
Ea

ω2∗
a,p | h1, ta

reject
otherwise
(21)
Here, h1 =

h0, ω1
p,a,r1
a = reject

, πa(ω1
p,a | ta) is deﬁned in Eq. 1 and Ea

ω2∗
a,p | h1, ta

is the beneﬁt to SIGAL from making an optimal proposal ω2∗
a,p at round 2, as deﬁned in
Eq. 15.
Let πa

r1∗
a
| ω1
p,a, h0, ta

denote the beneﬁt to SIGAL from its response to offer ω1
p,a in
round 1. If SIGAL accepts this offer, it receives the beneﬁt associated with ω1
p,a. If it rejects
123

Auton Agent Multi-Agent Syst
this offer, it will receive the expected beneﬁt Ea

ω2∗
a,p | h1, ta

from making an optimal
counter-proposal at round 2:
πa

r1∗
a
| ω1
p,a, h0, ta

=
⎧
⎨
⎩
πa(ω1
p,a | ta)
r1∗
a (ω1
p,a | h0) = accept
Ea

ω2∗
a,p | h1, ta

otherwise
(22)
The expected beneﬁt to SIGAL as a responder in round 1 is denoted as Ea

resp1 | h0, ta

.
This beneﬁt depends on its model of all possible offers made by people for each type, given
that SIGAL responds optimally to the offer.
Ea

resp1 | h0, ta

=

tp∈Tp
p(tp | h0) ·
⎛
⎜⎝

ω1p,a
p(ω1
p,a | tp, h0) · πa(r1∗
a
| ω1
p,a, h0, ta)
⎞
⎟⎠
(23)
Note that when the person reveals his/her type at round 0, this is encapsulated in the history
h0, and p(tp | h0) equals 1 for the person’s true type. Otherwise p(tp | h0) equals the
probability p(tp).
Round 0: In the revelation round SIGAL needs to decide whether to reveal its type. Let
Ea(h0, ta) denote the expected beneﬁt to SIGAL given that h0 includes a revelation decision
for both players and that ta is the type of agent. This beneﬁt depends on the probability that
SIGAL is chosen to be a proposer (p(prop)) or responder (p(resp)) in round 1:
Ea(h0, ta) = p(resp) · Ea

resp1 | h0, ta

+p(prop) · Ea(ω1∗
a,p | h0, ta)
(24)
Here, ω1∗
a,p is the optimal proposal for SIGAL in round 1, and Ea

ω1∗
a,p | h0, ta

is the
expected beneﬁt associated with this proposal, deﬁned in Eq. 19.
Because players do not observe each other’s revelation decisions, the expected beneﬁt for
a revelation decision φa of the SIGAL agent sums over the case where people revealed their
type (i.e., φa = tp) or did not reveal their type (i.e., φa = null). We denote p(φp = tp) as
the probability that the person revealed its type tp, and p(φp = null) as the probability that
the person did not reveal its type tp.
Ea (φa) =

tp∈Tp

p(φp = tp)· Ea

h0 =

φa, φp = tp

, ta

+p(φp = null) · Ea

h0 =

φa, φp = null

, ta

]
(25)
Given that SIGAL is of type ta ∈Ta, it reveals its type only if its expected beneﬁt from
revelation is greater or equal to not revealing its type:
123

Auton Agent Multi-Agent Syst
φ∗
a =
⎧
⎪⎨
⎪⎩
ta
Ea (φa = ta) ≥
Ea (φa = null)
null
otherwise
(26)
The value of the game for SIGAL for making the optimal decision whether to reveal its type
is deﬁned as Ea

φ∗
a

.
Lastly, we wished SIGAL to take a risk averse approach to making decisions in the game.
Therefore SIGAL used a convex function to represent its utility in the game from an offer
ωn, which modiﬁed Eq. 1.
π′
a(ωn | ta) = πa(ωn | ta)(1−ρ)
1 −ρ
(27)
The strategy used by SIGAL is obtained by “plugging in” the risk averse utility π′
a(ωn | ta)
instead of πi(ωn | ti).
6 Modeling human players
In this section we describe a model of people’s behavior used by SIGAL to make optimal
decisions in the game. We assume that there is a training set of games played by people, as
we show in the next section.
6.1 Accepting proposals
We modeled people’s acceptance of proposals in revelation games using a stochastic model
that depended on a set of features. These comprised past actions in the game (e.g., a responder
may be more likely to accept a given offer if it revealed its type as compared to the case in
which it did not reveal its type) as well as social factors (e.g., a responder player may be less
likely to accept a proposal that offers more beneﬁt to the proposer than to itself).5
Let ωn
i, j represent a proposal from a player i to a player j at a round n. We describe the
following features that affect the extent to which player j will accept proposal ωn
i, j. These
features are presented from the point of view of proposer i, therefore we assume that the type
of the proposer ti is known, while the type of the responder t j is known only if j revealed
its type. We ﬁrst detail the features that relate to players’ decisions whether to reveal their
types.
– REV 0
j . Revelation by j. This feature equals 1 if the responder j has revealed its type and
0 otherwise. The superscript 0 indicates this feature is relevant to the revelation phase,
which is round 0.
– REV 0
i . Revelation by i. This feature equals 1 if the proposer has revealed its type ti.
We now describe the set of features relating to social factors of the responder player j.
– BE N n
j . Beneﬁt to j. The beneﬁt to j from proposal ωn
i, j in round n. This measures the
extent to which the proposal ωn
i, j is generous to the responder. In the case where j revealed
its type, this feature equals π j(ωn
i, j | t j) and computed directly from Eq. 1. Otherwise,
the value of this feature is the expected beneﬁt to the responder from ωn
i, j for all possible
responder types Tj:
5 Both of these patterns were conﬁrmed empirically, as shown in the Sect. 8
123

Auton Agent Multi-Agent Syst

t j∈Tj
p(t j | hn−1) · π j(ωn
i, j | t j)
– AI n
i . Advantageous inequality of i. The difference between the beneﬁt to proposer i
and responder j that is associated with proposal ωn
i, j. This measures the extent to which
proposer i is competitive, in that ωn
i, j offers more for i than for j. This feature equals the
difference between πi(ωn
i, j, accept | ti) and BE N n
j .
To capture the way the behavior in round n = 1 affects the decisions made by participants
in round n = 2, we added the following features that refer to past offers.
– P.BE N n
j . Beneﬁt to j in the previous round. This feature equals BE N 1
j if n = 2, and 0
otherwise.
– P.BE N n
i .Beneﬁttoproposeri inthepreviousround.Thisfeatureequalsπi(ω1
i, j, accept |
ti) if n = 2 and 0 otherwise.
To illustrate, consider the asymmetric CT board game shown in Fig. 1b. Alice is missing
two green chips to get to the goal and Bob is missing 1 purple chip to get to the goal. Suppose
Bob is the ﬁrst proposer (player i) and that Alice is the ﬁrst responder (player j), and that
Bob revealed its goal to Alice, so its type is common knowledge, while Alice did not reveal
her goal. We thus have that REV 0
j = 0 and REV 0
i = 1. Alice’s no-negotiation alternative
(NNA) score, s j(∅), is 70 points and Bob’s NNA score is 90 points.
According to the offer shown in the Figure, Bob offered two green chips to Alice in return
for two purple chips. If accepted, this offer would allow Alice to get to the goal in 5 steps, so
she will have 19 chips left at the end of the game, worth 19 · 5 = 95 points. Similarly, Bob
will have 21 chips left at the end of the game, worth 105 points. Both will also earn a bonus
of 100 points for getting to the goal. Therefore we have that BE N 1
j = 95+100−70 = 125.
Similarly, Bob’s beneﬁt from this proposal is 105 + 100 −90 = 115 points. The difference
between the beneﬁt to Bob and to Alice is −10, so we have that AI 1
i = −10. Lastly, because
the offer is made in round 1, we have that P.BE N 1
j = P.BE N 1
i = 0. This offer is more
generous to Alice than it is to Bob.
Suppose now that Alice rejects this offer and makes a counter proposal in round 2, that
proposes one purple chip to Bob in return for four greens. In this example, Alice is using her
knowledge of Bob’s type to make the minimal offer that would allow Bob to reach the goal
while providing additional beneﬁt to Alice. Alice is the proposer (player i) and Bob is the
responder (player j). Recall that Bob has revealed its goal while Alice did not, so we have
REV 0
j = 1 and REV 0
i = 0. Using a similar computation from before, we get that Bob’s
score from the counter proposal is 190 points. Therefor we have that BE N 2
j = 190 −90 =
100. Alice’s beneﬁt from the counter-proposal is 210 −70 = 140, therefore we have that
AI 2
i = 140 −100 = 40. The last features in the example capture the beneﬁt to both players
from the proposal made in the ﬁrst round to Alice and Bob, so we have P.BE N 2
j = 125,
and P.BE N 2
i = 115.
6.1.1 Social utility function
We model the person as using a social utility function to decide whether to accept proposals
in the game. This social utility depends on a weighted average of the features deﬁned above.
We deﬁne a transition function, T n, that maps an offer ωn and history hn−1 to an (ordered)
set of feature values xn as follows.6
6 These weights are estimated from data using statistical techniques as described in the following section.
123

Auton Agent Multi-Agent Syst
xn =

REV 0
j , REV 0
i , BE N n
j , AI n
i , P.BE N n
j , P.BE N n
i

To illustrate, in the example above, we have that x1 = (0, 1, 125, −10, 0, 0) and x2 =
(1, 0, 100, 40, 125, 115).
Let u(xn) denote the social utility function which is deﬁned as the weighted sum of these
features. To capture the fact that a decision might be implemented noisily, we use a sigmoid
function to describe the probability that people accept offers, in a similar way to past studies
for modeling human behavior [11]. We deﬁne the probability of acceptance for a particular
features values xn by a responder to be
p(rn
i = accept | ωn, hn−1) =
1
1 + e−u(xn)
(28)
where xn = T n(ωn, hn−1). In particular, the probability of acceptance converges to 1 as
u(xn) becomes large and positive, and to 0 as the utility becomes large and negative. We
interpret the utility to be the degree to which one decision is preferred. Thus, the probability
of accepting a proposal is higher when the utility is larger.
6.1.2 Estimating weights
To predict how people respond to offers in the game, it is needed to estimate the weights
in their social utility function in a way that best explains the observed data. In general, we
need to model the probability that an offer is accepted for any possible instantiation of the
history. The number of possible proposals in round 1 is exponential in the combined chip set
of players.7 It is not possible to use standard density estimation techniques because many
such offers were not seen in the training set or were very rare. Therefore, we employed a
supervised learning approach that assumed people used a noisy utility function to accept
offers that depended on the features deﬁned above. Let Ωi,p denote a data set of offers
proposed by some participant i to a person p.8 For each offer ωn
i,p ∈Ωi,p let y(rn
p | ωn
i,p)
denote an indicator function that equals 1 if the person accepted proposal ωn
i,p, and zero
otherwise. The error of the predictor depends on the difference between y(rn
p | ωn
i,p) and the
predicted response p(rn
p = accept | ωn
a,p, hn−1), as follows:

ωn
i,p∈Ωi,p

p(rn
j = accept | ωn
i,p, hn−1) −y(rn
j | ωn
i,p)
2
(29)
where p(rn
j = accept | ωn
i, j, hn−1) is deﬁned in Eq. 28.
We used a standard Genetic algorithm to estimate weight values for the features of people’s
social utility that minimize the aggregate error in the training set. To avoid over-ﬁtting the
training set, we used a held-out cross-validation set consisting of 30% of the data. We chose
the instance with minimal error (on the training set) in the generation that corresponded to
the smallest error on the cross-validation set. We used ten-fold cross-validation, repeating
this process ten times, each time choosing different training and testing sets, producing ten
candidate instances. To pick the best instance, we computed the value of the game Ea

φ∗
a

for
SIGAL for each of the learned models, where φ∗
a is deﬁned in Eq. 26. This is the expected ben-
eﬁt for SIGAL given that it chooses optimal actions using a model of people that corresponds
to the feature values in each instance.
7 In one of the boards we studied the number of possible offers that provided the same beneﬁt to both players
was about 27,000, out of a total of 224 possible offers.
8 We explain how we collected this data set in the Sect. 7
123

Auton Agent Multi-Agent Syst
6.2 Proposing and revealing
This section describes our model of how people make proposals in revelation games and
reason about whether to reveal information.
6.2.1 First proposal model
We used standard density estimation techniques (histograms) to predict people’s offers for
different types. Based on the assumption that proposals for the ﬁrst round depend on the
proposer’s type and its decision whether to reveal, we divided the possible proposals to
equivalence classes according to the potential beneﬁt for the proposer player, and counted
how many times each class appears in the set. Let p(ω1
p, j | tp, φi) denote the probability that
a human proposer of type tp offers ω1
p, j in round 1. Let Ntp,φp

πp(ω1
p, j | tp)

denote the
number of proposals in round 1 which gives the human a beneﬁt of πp(ω1
p, j | tp), given the
human is of type tp and its revelation decision was φp. Let Ntp,φp(Ω1
p, j) denote the number
of proposals in round 1 in this subset. p(ω1
p, j | tp, φp) is deﬁned as:
p(ω1
p, j | tp, φp) =
Ntp,φp

πp(ω1
p, j | tp)

Ntp,φp(Ω1
p, j)
(30)
6.2.2 Counter-proposal model
According to our model, a player’s proposal in the second round also depends on the history,
this two dimensional probability density function tends to be too sparse to calculate it directly
as described in Sect. 6.2.1. Inspired by studies showing that people engage in tit-for-tat
reasoning [27] we used this principal to model the counter-proposals made by people. We
assumed that a responder player i will be proposed offer ω2
p,i by a human player in the
second round with beneﬁt πi(ω2
p,i | ti) that is equal to the beneﬁt πp(ω1
i,p | tp) from offer
ω1
i,p made to the person in the ﬁrst round, when the human was a responder. For example,
suppose that Bob is the proposer in round 1 and propose to Alice a beneﬁt of 125. According
to the model, if Alice rejects the offer she will propose Bob a counter-proposal that provides
Bob with the same beneﬁt, 125. Note that this does not assume that the proposal will provide
Alice with the same beneﬁt she got from Bob in the proposal from round 1. Formally,
let NΩ2
p,i (πp(ω1
i,p | tp)) denote the number of counter-proposals ω2
p,i which give beneﬁt
πp(ω1
i,p | tp). We assume that there always exists at least one proposal that meets this
criterion, i.e., NΩ2
p,i (πp(ω1
i,p | tp)) ̸= 0. The “tit for tat” heuristic is as follows:
p(ω2
p,i | h1) =
⎧
⎨
⎩
0
πi(ω2
p,i) ̸= πp(ω1
i,p)
1/NΩ2
p,i (πp(ω1
i,p | tP))
otherwise
(31)
This heuristic is used in Eq. 18 to facilitate the computation of the expected beneﬁt from
SIGAL as a responder in round 1.
Lastly, we detail the model used by SIGAL to predict whether the person reveals its goal.
Let Ntp denote the number of instances in which people were of type tp, and let Ntp(φp)
denote the number of times that people of type tp chose to reveal their type. The probability
that a human player p revealed its type tp is deﬁned as:
123

Auton Agent Multi-Agent Syst
p(φp | tp) = Ntp(φp)
Ntp
(32)
7 Empirical methodology
In this section we describe the methodology we used in order to learn the parameters of
the model of how people play revelation games, and to evaluate it. For these purposes we
recruited 260 students enrolled in a computer science or software engineering program at
several universities and colleges. An additional 143 people were recruited using the Ama-
zon Turk framework. All subjects received an identical tutorial on revelation games that was
exempliﬁed on a board (not the boards used in the study). Actual participation was contingent
on successfully answering a set of basic comprehension questions about the game. Partic-
ipants were seated in front of a terminal for the duration of the study, and could not speak
to any of the other participants. Each participant played two revelation games on different
boards.
The boards in the study fulﬁlled the following conditions at the onset of the game: (1)
There were two goals for each player; (2) Every player lacked one or two chips to reach each
of its possible goals; (3) Every player possessed the chips that the other needed to get to each
of its possible goals; (4) There existed at least one exchange of chips which allowed both
players to reach each of their possible goals; (5) the goals were distributed with a probability
of 50% for both players.
We used the asymmetric and symmetric boards shown in Fig. 1a, b. Participants played
both symmetric and asymmetric boards in random order. They engaged in a neutral activity
(answering demographic questions) between games to minimize the effects of their behavior
in the ﬁrst game on their behavior in the second game. The participant chosen to be the
proposer in the ﬁrst game was randomly determined, and participants switched roles in the
second game, such that the proposer in the ﬁrst game was designated as the responder in
the second game. A central server (randomly) matched each participant with a human or
an agent counterpart for each game. The identity of each participant was not disclosed. We
collected players’ proposals, responses and revelation decisions for all of the games played.
To avoid deception all participants were told they would be interacting with a computer or
a person. Participants received ﬁxed compensation (course credit) for participating in the
experiment.9
We divided subjects into four disjoint pools. The ﬁrst pool consisted of people playing
other people (66 games). The second pool consisted of people playing a computer agent
that used a randomized strategy to make offers and responses (170 games). The purpose for
this pool was to collect people’s actions for diverse situations, for example, their response
to offers that were never made by other people. Two thirds (44 games) of the data from
the ﬁrst pool and the data from the second pool were used for training a model of people’s
behavior. The third pool consisted of people playing the SIGAL agent (238 games). The
fourth pool (118 games) consisted of people playing an agent using several of the equilibrium
strategies deﬁned in Sect. 4 to play revelation games. The equilibrium agent adapted a non-
revelation and revelation strategy equilibrium as specifed in Sects. 4.1 and 4.2 for each
game.
9 Our goal was to build an agent that negotiates well with people, not to explain people’s incentives, therefore
ﬁxed compensation was sufﬁcient.
123

Auton Agent Multi-Agent Syst
8 Results and discussion
The performance of SIGAL was measured by comparing its performance against people (the
third pool) with people’s play against other people (the ﬁrst pool). We list the number of
observations and means for each result. All results reported in this section are statistically
signiﬁcant in the p < 0.05 range using t tests for normally distributed data and two-sample
Mann–Whitney rank tests for non-normally distributed data.
8.1 Analysis: general performance
We ﬁrst present a comparison of the performance of SIGAL and people. Figure 2 shows the
average beneﬁt (the difference in score between agreement and the no-negotiation alternative
score) for different roles (proposer and responder). As shown by the Figure, the SIGAL agent
outperformed people in all roles. In addition, SIGAL was also more successful at reaching
agreements than were people. Figure 3 shows the percentage of offers accepted by people
for the different roles. As shown by the Figure, proposals made by SIGAL in round 1 were
accepted 62% of the time, while proposals made by people in round 1 were accepted only
49% of the time. This difference is more pronounced in round 2, in which proposals made
by SIGAL were accepted 83% of the time, while offers made by people in round 2 were only
accepted 63% of the time. If an offer is rejected at this last round, the game ends without
agreement. This striking difference shows that SIGAL learned to make good offers at critical
points in the game.
As shown in Figure 2 SIGAL also outperformed the equilibrium agent in both rounds.
The equilibrium agent was fully strategic and assumed the other player was unboundedly
rational. Although not shown in the Figure, it made very selﬁsh offers in the last round,
offering only 25 average beneﬁt points to people and 215 beneﬁt points to itself. Most of
these offers (54%) were rejected. In the ﬁrst round, it made offers that were highly beneﬁcial
to people, offering 219 average points to people and 20 to itself. Most of these offers (82%)
were accepted, but the small beneﬁt it incurred in these proposals not aid its performance.
Fig. 2 Performance comparison
123

Auton Agent Multi-Agent Syst
Fig. 3 Agreement comparison
Fig. 4 Average proposed beneﬁt in ﬁrst and second rounds
To explain the success behind SIGAL’s strategy, we present a comparison of the beneﬁt
from proposals made by the SIGAL agent and people in both game rounds in Fig. 4. As shown
by the Figure, both people and SIGAL made offers that were beneﬁcial to both players in
rounds 1 and 2. However, SIGAL made offers that were signiﬁcantly more generous to human
responders than did human proposers (118 beneﬁt points provided by SIGAL as proposer
in round 1 versus 96 points provided by human proposers; 110 beneﬁt points provided by
SIGAL as proposer in round 2 versus 81 beneﬁt points provided by human proposers). In
fact, the proposals made by SIGAL pareto dominated the proposals made by people in both
123

Auton Agent Multi-Agent Syst
Table 1 Features coefﬁcients
weights
Feature
Value
REV 0
j
0.258
REV 0
i
0.035
BE Nn
j
0.956
AI n
i
−0.792
P.BE Nn
j
0.496
P.BE Nn
i
0.334
Free parameter
0.608
rounds 1 and 2. Thus, SIGAL learned to make offers that were better for human responders
without compromising its own utility
SIGAL’s strategy is highlighted by examining the weights learned for the different features
of how people accept offers. As shown in Table 1, the largest weight was assigned to BE N n
j ,
the beneﬁt to the responder from an offer. In addition, the weight for AI n
i measuring the
difference between the beneﬁt for the proposer and responder was large and negative. This
means that responders prefer proposals that provide them with large beneﬁts, and are also
competitive, in that they dislike offers that provide more to proposers than to responders.
The offers made by SIGAL reﬂect these criteria. In particular, proposers asked more for
themselves than for responders in both rounds. In contrast, SIGAL equalized the difference
in beneﬁt between proposers and responders in round 1, and decreased the difference between
its own beneﬁt and responder’s beneﬁt in round 2 as compared to human proposer.
8.2 Analysis: revelation of goals
We now turn to analyzing the affect of goal revelation on the behavior of SIGAL. Recall
that Ea

φ∗
a = ta

denotes the value of the game for SIGAL when deciding to reveal its
goal in round 0, and behaving optimally according to its model of how people make offers.
Similarly, Ea

φ∗
a = null

denotes the value of the game for SIGAL when deciding not to
reveal its goal in round 0. Our model predicted no signiﬁcant difference in value to SIGAL
between revealing and not revealing its goal, i.e. Ea

φ∗
a = null

≈Ea

φ∗
a = ta

for each
type ta ∈Ta. Therefore we used two types of SIGAL agents, one that consistently revealed
its goal at the onset of the game and one that did not reveal. In all other respects these
agents followed the model described in Sect. 5. The empirical results conﬁrmed the model’s
prediction, in that there was no signiﬁcant difference in the performance of the two SIGAL
agents for all boards and types used in the empirical study.
However, the decision of the person to reveal or not reveal its goal had a signiﬁcant
affect on the negotiation strategy of SIGAL. When people revealed their goals, SIGAL was
signiﬁcantly more competitive as compared to the case in which people did not reveal their
goals. Speciﬁcally, the competitive weight AI n
i from proposals made by SIGAL in round 1
was 14 points when people revealed their goal, and −3 points when people did not reveal their
goal. This means that SIGAL learned to make offers that were signiﬁcantly more competitive
when people revealed their goals.
8.3 Analysis: people’s strategies
Wenowturntodescribeseveralfeaturesinpeople’sbehaviordemonstratingstrategicbehavior
in the game that resembled the way SIGAL played. In the asymmetric game, people in weak-
123

Auton Agent Multi-Agent Syst
type roles were more likely to reveal their goals than people in strong-type roles. In particular,
in 65% of games played in the asymmetric game, strong-type players engaged in “blufﬁng”,
that is, not disclosing their true goal and asking for more chips than they actually need to get to
the goal in round 1. Similarly, in 56% of games in the asymmetric condition, weak-type play-
ers engaged in blufﬁng. Interestingly, this trend was also apparent for the symmetric game, in
which blufﬁng occurred in 63% of the games played. Second, people’s proposals were signif-
icantly more selﬁsh and less generous in round 2 than in round 1. This is apparent from Fig. 2.
Interestingly, people’s performance was signiﬁcantly higher when playing with SIGAL than
when playing with other people. Speciﬁcally, people playing with SIGAL achieved an aver-
age performance of 112 points, while people playing with other people achieved an average
performance of 91 points. This shows that SIGAL had a positive affect on people’s play.
The last part of the analysis compares performance of subjects enlisted using Amazon
Turk (MTurk) with students. There are several works dupicating lab studies using MTurk
showing similar patterns of behavior [1]. In our work, we found that in general, students
were better performers than MTurk workers (average score was 123 points for students
and 109 points for MTurk workers). This can be explained by the fact that students were
signiﬁcantly more selﬁsh as second proposers (average 45 points for students and 20 points
for MTurk workers). Also, MTurk workers accepted signiﬁcantly more proposals than did
students in the ﬁrst round (75% average acceptance rate for MTurk compared to 59% average
acceptance rate for students). These results suggest that integrating information revelation
with negotiation required more cognitive effort from people than canonical decision-making
tasks.
8.4 Limitations
We conclude this section with describing several limitations of our approach. First, our def-
inition of revelation games, which the SIGAL decision-making model is tailored to support,
deﬁnes a single revelation phase followed by two rounds of take-it-or-leave-it offers. This
model fails to describe more involved negotiations which may include additional rounds or
multiple instances of revelation. Indeed, some agents may wish to reveal their goal only after
not succeeding to reach agreement in several rounds. Allowing for more involved revelation
protocols raises signiﬁcant computational challenges which remain outside the scope of this
paper. Second, the decision-making model used by SIGAL in this paper assumes that all
people use tit-for-tat reasoning when predicting their performance, which may not hold in
practice. Indeed, or results showed considerable variance in people’s behavior for different
countries. We hypothesize that learning separate decision-making models for each conuntry
may additionally improve the prediction power of SIGAL and consequently, its performance.
9 Conclusion and future work
This paper presented an agent-design for interacting with people in “revelation games”, in
which participants are given the choice to truthfully reveal private information prior to nego-
tiation. The decision-making model used by the agent reasoned about the social factors that
affect people’s decisions whether to reveal their goals, as well as the effects of people’s rev-
elation decisions on their negotiation behavior. The parameters of the model were estimated
from data consisting of people’s interaction with other people. In empirical investigations,
the agent was able to outperform people playing other people as well as agents playing equi-
librium strategies and was able to reach agreement signiﬁcantly more often than did people.
123

Auton Agent Multi-Agent Syst
We are currently extending this work in two directions. First, we are considering more elab-
orate settings in which players are able to control the extent to which they reveal their goals.
Second, we are using this work as the basis for a more broad argumentation in which agents
integrate explanations and justiﬁcations within their negotiation process. Lastly, information
revelation strategies can also be adopted by the negotiation community at large, for example
in the International Automated Negotiation Agent Competition (ANAC) [3].
Acknowledgments
This research is supported in part by the U.S. Army Research Laboratory and the U.S.
Army Research Ofce under Grant number W911NF-08-1-0144, by ERC Grant #267523, EU Grant FP7-ICT-
2011-9 #600854 and Marie Curie Grant #268362.
10 Appendix
Before proving Proposition 1, we will postulate the following lemma:
Lemma 1 Suppose that player j uses response strategy a2
j(ω, t j) (Eq. 2) to accept proposals
ω at round 2. Then player i is always indifferent between revealing and not revealing its type,
regardless of the revelation decision of player j.
Proof We will show that the the revelation decisions of player i cannot affect its outcome
in the game. The strategy a2
j(ω, t j) of player j to accept the proposal by i at round 2 does
not depend on i’s type. Hence the revelation decision of i in round 0 does not affect its
beneﬁt from proposals in round 2. Suppose that player i revealed its type. If it can get a
more beneﬁcial proposal from player j by not revealing its type, then it will reject j’s offer
and make the more beneﬁcial proposal in round 2. A similar argument can be used to show
that player i can get its more preferred outcome when it does not reveal its type. Hence the
revelation decision of i does not affect its outcome.
We provide a proof to Proposition 1 by showing that the strategies speciﬁed in Sect. 4.1
correspond to a perfect Bayesian equilibrium given the sufﬁcient conditions.
Proof We denote the possible types for j as t j1 and t j2 and the possible types for i as ti1 and
ti2. We prove that when one type of player j is preferable to the other type (Eq. 6), there
always exists a revelation equilibrium in which players reveal their goals for both types.10 In
round 2, player i makes the proposal ω2∗
i, j(t j) using Eq. 4. This proposal gives the maximum
beneﬁt for player i among the proposals that are accepted by player j. Any proposal with
higher beneﬁt for i will be rejected by player j.11 Player j also cannot beneﬁt from deviation
as rejecting the proposal incurs zero beneﬁt to both players.
We proceed to round 1. We ﬁrst specify why the belief update rules are part of a perfect
Bayesian equilibrium. Suppose j is of type t j2 and that this type is preferable for j (without
loss of generality). If j revealed its type, then i uses Bayes rule to assign probability 1 to type
t j2, as speciﬁed in equilibrium. If j does not reveal its type, then the resulting information set
is off the equilibrium path. In this case i makes an arbitrary belief update to assign probability
0 to type t j2. Using a similar argument, we can show that the belief update rules are part of
a perfect Bayesian equilibrium when type t j1 is preferable to j. Suppose that player i is of
10 The existence of a preferable type for a player is a reasonable assumption in negotiation. Only if it is the
case that both players prefer the proposal that is associated with the type of the other, there is no preferable
type.
11 In the case where there are no proposals that are accepted by player j, the beneﬁt for player i is zero.
123

Auton Agent Multi-Agent Syst
type ti1. If i reveals its type, then j uses Bayes rule to assign probability 1 to ti2, as speciﬁed
in equilibrium. If i does not reveal its type, then the resulting information set is off the
equilibrium path. In this case j makes an arbitrary belief update to assign probability 1 to
ti1. Using a similar argument, we can show that the belief update rules are part of a perfect
Bayesian equilibrium when i is of type ti2.
Next, we specify why players’ actions are in equilibrium in round 1. Player j makes the
proposal w1∗
j,i(ti) using Eq. 5. Any proposal with a higher beneﬁt to j will necessarily be
rejected by player i. As a result, player j’s proposal maximizes its beneﬁt given i’s response
strategy. Player i also cannot beneﬁt from deviation because player j’s proposal gives it the
same beneﬁt as its counter-proposal ω2∗
i, j(t j).
Finally, we consider round 0, in which both players i and j reveal their types for both of
their types. A deviation for player j of type t j2 in round 0 means that j chooses not to reveal its
type. According to the belief-update rules, player i will update its beliefs to assign probability
1 to type t j1. As a result, it will make a proposal in round 2 to j that is less preferable to the
one it would receive if it revealed its type. Thus, deviation from its revelation decision is not
strictly beneﬁcial for j when it is of type t j2. In a similar fashion, we can show that deviation
from a revelation decision is not beneﬁcial for j when it is of type t j1. Thus the revelation
strategies of round 0 are in Equilibrium for j.
Due to Lemma 1, a decision by player i to reveal its type in round 0 for each of its types
is in equilibrium. Thus there always exists a revelation equilibrium in which players reveal
their goals for both types.
We provide a proof to Proposition 1 by showing that the strategies speciﬁed in Sect. 4.2
specify a perfect Bayesian Equilibrium.
Proof We use backward induction. In Round 2 there are two cases. On the equilibrium path.
In this case the players did not update their beliefs according to the equilibrium speciﬁcation.
Player i makes the proposal ω2∗
i, j(h1, ti) using Eq. 10. This proposal gives the maximum
expected beneﬁt for player i, so it cannot on expectation beneﬁt from deviation. Player j also
cannot beneﬁt from deviation as rejecting the proposal incurs zero beneﬁt to both players.
Lets examine the cases where the information set is off the equilibrium path. The ﬁrst
case is related to round 0: if i revealed its type, this will not affect the proposal at round
2 (Proposition 2). If j revealed its type, then i knows its true type, and the strategies are
in equilibrium as in the revelation equilibrium. The other case is related to round 1: In this
case, as deﬁned in the Equilibrium, the players do not change their beliefs, and the strategies
in round 2 are as in the equilibrium. We now proceed to Round 1. we show neither player
has incentive to deviate from its strategy. Suppose player j of type t j wishes to deviate and
propose a different proposal. If the proposal is worse off for player i, given its type ti, then
player i will reject the offer and player j will not beneﬁt. If player i accepts the proposal
(according to Eq. 12), this means that it is greater or equal in beneﬁt to i than the equilibrium
proposal in round 2. By the second condition (Eq. 14), this means that the proposal will be at
most equal to the beneﬁt that player j will receive from the equilibrium proposal. So player
j will not beneﬁt from making this proposal. For player i, it cannot beneﬁt from rejecting
j’s proposal. According to Eq. 12, player i accepts player j’s proposal only if it cannot get
a higher beneﬁt from its own counter-proposal.
Finally, Lets examine if the player can beneﬁt from deviation in Round 0. Suppose that
player j of type t j1 deviates and reveals its type. By deﬁnition, then player i will update
its beliefs to its true type. In this case, player i will make the proposal ω2∗
i, j(t j1). However,
by the ﬁrst condition (Eq. 13), this proposal is at most equal to the proposal that player j
123

Auton Agent Multi-Agent Syst
will get from not revealing its type. Therefore it is not beneﬁcial for player j to deviate. By
Proposition 1, player i is indifferent between its two types. Thus the strategy for both players
satisﬁes a perfect Bayesian equilibrium.
We conclude by showing that the sufﬁcient conditions of proposal 2 hold in the symmetric
(Fig. 1a) and the asymmetric (Fig. 1b) boards described in the paper:
Symmetric board: For the ﬁrst condition (13), π j(ω2∗
i, jl(t j) | tl) = 20 for l ∈1, 2, is
smaller than π j(ω2∗
i, j(h1, ti) = 25, thus the condition holds. For the
second condition (14), ω2∗
i, j(h1, ti) let player i reach its goal, and so for
both types of player j. In such case, the only way for a player to get
more points is by asking for more chips, which means that the game
turns to be a zero-sum game. In such case, there is no proposal that can
be more beneﬁcial for both players, thus the condition holds.
Asymmetric board: For the ﬁrst condition (13), π j(ω2∗
i, jl(t j) | t1) = 30 for t1 as the
weak type, and π j(ω2∗
i, jl(t j) | t1) = 10 for t2 as the strong type.
π j(ω2∗
i, j(h1, ti) = 35 for t1 and 15 for t2, thus the condition holds.
For the second condition (14), like in the symmetric board ω2∗
i, j(h1, ti)
let both types of player j and player i to reach their goal, so the game
turns to be a zero-sum game, and the condition holds.
References
1. Amir, O., Rand, D. G., et al. (2012). Economic games on the internet: The effect of $1 stakes. PloS ONE,
7(2), e31461.
2. Azaria, A., Rabinovich, Z., Kraus, S., & Goldman, C. V. (2011). Strategic information disclosure to people
with multiple alternatives. In Proceedings of AAAI.
3. Baarslag, T., Fujita, K., Gerding, E. H., Hindriks, K., Ito, T., Jennings, N. R., et al. (2013). Evaluat-
ing practical negotiating agents: Results and analysis of the 2011 international competition. Artiﬁcial
Intelligence, 198, 73–103.
4. Banks, J., Camerer, C. F., & Porter, D. (1994). Experimental tests of nash reﬁnements in signaling games.
Games and Economic Behavior, 6, 1–31.
5. Bench-Capon, T., Atkinson, K., & McBurney, P. (2009). Altruism and agents: an argumentation based
approach to designing agent decision mechanisms. In Proceedings of AAMAS.
6. Coehoorn, R. M., & Jennings, N. R. (2004). Learning on opponent’s preferences to make effective multi-
issue negotiation trade-offs. In Proceedings of EC.
7. de Melo, C., Carnevale, P., Read, S., Antos, D., & Gratch, J. (2012). Bayesian model of the social effects
of emotion in decision-making in multiagent systems. In Proceedings of AAMAS.
8. Erev, I., & Roth, A. E. (1998). Predicting how people play games: Reinforcement learning in experimental
games with unique, mixed strategy equilibria. American Economic Review, 88(4), 848–881.
9. Gal, Y., Grosz, B., Kraus, S., Pfeffer, A., & Shieber, S. (2010). Agent decision-making in open-mixed
networks. Artiﬁcial Intelligence, 174(18), 1460–1480.
10. Gal, Y., Kraus, S., Gelfand, M., Khashan, H., & Salmon, E. (2012). Negotiating with people across cultures
using an adaptive agent. ACM Transactions on Intelligent Systems and Technology, 3(1).
11. Gal, Y., & Pfeffer, A. (2007). Modeling reciprocity in human bilateral negotiation. In Proceedings of
AAAI.
12. Grosz, B., Kraus, S., Talman, S., & Stossel, B. (2004). The inﬂuence of social dependencies on Decision-
Making. In Proceedings of AAMAS.
13. Haim, G., Gal, Y., Kraus, S., & Gelfand, M. J. (2012). A cultural sensitive agent for Human–Computer
negotiation. In Proceedings of AAMAS.
14. Hajaj, C., Hazon, N., Sarne, D., & Elmalech, A. (2013). Search more, disclose less. In Proceedings of
AAAI.
15. Hindriks, K., & Tykhonov, D. (2008). Opponent modelling in automated multi-issue negotiation using
bayesian learning. In Proceedings of AAMAS, (pp. 331–338).
123

Auton Agent Multi-Agent Syst
16. Jonker, C. M., Robu, V., & Treur, J. (2007). An agent architecture for multi-attribute negotiation using
incomplete preference information. Autonomous Agents and Multi-Agent Systems, 15(2), 221–252.
17. Lin, R., & Kraus, S. (2010). Can automated agents proﬁciently negotiate with humans? Communications
of the ACM, 53(1), 78–88.
18. McKelvey, R. D., & Palfrey, T. R. (1992). An experimental study of the centipede game. Econometrica:
Journal of the Econometric Society, 60(4), 803–836.
19. Osborne, M. J., & Rubinstein, A. (1999). A course in game theory. Cambridge, MA: MIT Press.
20. Oshrat, Y., Lin, R., & Kraus, S. (2009). Facing the challenge of human-agent negotiations via effective
general opponent modeling. In Proceedings of AAMAS.
21. Pasquier, P., Hollands, R., Dignum, F., Rahwan, I., & Sonenberg, L. (2007). An empirical study of
interest-based negotiation. In Proceedings of ICEC.
22. Peled, N., Gal, Y., & Kraus, S. (2011). A study of computational and human strategies in revelation games.
In Proceedings of AAMAS.
23. Rosenfeld, A., & Kraus, S. (2012). Modeling agents based on aspiration adaptation theory. Autonomous
Agents and Multi-Agent Systems, 24(2), 221–254.
24. Sarne, D., Elmalech, A., Grosz, B. J., & Geva, M. (2011). Less is more: Restructuring decisions to improve
agent search. In Proceedings of AAMAS.
25. Spence, A. M. (1974). Market signaling. Cambridge, MA: Harvard University Press.
26. Sycara, K. P. (1990). Persuasive argumentation in negotiation. Theory and decision, 28(3), 203–242.
27. Wedekind, C., & Milinski, M. (1996). Human cooperation in the simultaneous and the alternating pris-
oner’s dilemma. PNAS, 93(7), 2686.
123

