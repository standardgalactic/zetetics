Cost-Eﬃcient Online Hyperparameter Optimization
Jingkang Wang∗1,2
wangjk@cs.toronto.edu
Mengye Ren∗1,2
mren@cs.toronto.edu
Ilija Bogunovic3
ilijab@ethz.ch
Yuwen Xiong1,2
yuwen@cs.toronto.edu
Raquel Urtasun1,2
urtasun@cs.toronto.edu
University of Toronto1, Uber ATG2, ETH Z¨urich3
Abstract
Recent work on hyperparameters optimization (HPO) has shown the possibility of training
certain hyperparameters together with regular parameters. However, these online HPO
algorithms still require running evaluation on a set of validation examples at each training
step, steeply increasing the training cost. To decide when to query the validation loss, we
model online HPO as a time-varying Bayesian optimization problem, on top of which we
propose a novel costly feedback setting to capture the concept of the query cost. Under
this setting, standard algorithms are cost-ineﬃcient as they evaluate on the validation
set at every round.
In contrast, the cost-eﬃcient GP-UCB algorithm proposed in this
paper queries the unknown function only when the model is less conﬁdent about current
decisions. We evaluate our proposed algorithm by tuning hyperparameters online for VGG
and ResNet on CIFAR-10 and ImageNet100. Our proposed online HPO algorithm reaches
human expert-level performance within a single run of the experiment, while incurring only
modest computational overhead compared to regular training.
Keywords:
Bayesian Optimization, Hyperparameter Tuning, Gaussian Process
1. Introduction
Training deep neural networks involves a large number of hyperparameters, many of them
found through repetitive trial-and-error. Often the results are very sensitive to the selec-
tion of hyperparameters and researchers typically follow classic “cookbooks” with many of
their hyperparameters copied from the predecessor models. Hyperparameter optimization
(HPO) (Hutter et al., 2011; Snoek et al., 2015; Jamieson and Talwalkar, 2016) could po-
tentially save much time from grid search procedures performed in nested loops; however,
repetitive experiments on the order of hundreds are still required, which makes applying
HPO prohibitively expensive in practice.
Recent advances in meta-optimization (Lorraine and Duvenaud, 2018; MacKay et al.,
2019) have shown that one can actually tune certain hyperparameters, e.g., data augmenta-
tion, dropout, example weighting, entirely online throughout the training of the main model
parameters, by constantly inspecting at the validation loss. While this means that the train-
ing job only needs to be launched once, the cost of each training step rises sharply: to learn
the hyperparameters, one has to take a gradient step from the reward signals obtained by
1
arXiv:2101.06590v1  [cs.LG]  17 Jan 2021

evaluating on a separate set of validation examples to the hyperparameters. Depending on
the size of the validation set, this can make the training time several times longer.
Ultimately, the goal is to design an online HPO algorithm that is eﬃcient in terms
of computation cost that arises from validation loss evaluations, i.e., whenever the algo-
rithm queries for “ground-truth” of validation loss. Towards building such an algorithm,
we model the environment as a time-varying Bayesian Optimization (BO) problem with un-
known time-varying reward/objective. In contrast with the standard BO setting, we require
the agent to pay a certain cost to observe the reward every time it decides to query the un-
known function. In this paper, we propose a time-varying cost-eﬃcient GP-UCB (Srinivas
et al., 2010; Bogunovic et al., 2016) algorithm, and provide kernel-based regret guarantees.
Our algorithm makes use of a Gaussian process (GP) model to quantify uncertainty in
the estimates and cost-eﬃcient query rule. Based on this rule, the algorithm queries the
unknown objective only in rounds when it is ”less” conﬁdent in its decision.
We verify the eﬀectiveness of our proposed algorithm empirically by tuning hyperparam-
eters of large scale deep networks. First, we automatically adjust the tuning schedules of
self-tuning networks (MacKay et al., 2019) on CIFAR-10 (Krizhevsky et al., 2009). Second,
we optimize data augmentation parameters of a state-of-the-art unsupervised contrastive
representation learning algorithm (Chen et al., 2020) on ImageNet100 (Deng et al., 2009).
We show that with modest computation overhead compared to regular training, one can
achieve the same or better performance compared to baselines that have been extensively
tuned by human experts.
To summarize, the contributions in this paper are as follows: (1) We introduce a novel
costly feedback setting for BO to model the evaluation cost for online HPO; (2) We propose a
cost-eﬃcient GP-UCB algorithm with kernel-based theoretical guarantees for time-varying
BO with costly feedback; (3) We demonstrate the eﬀectiveness of our method in tuning
hyperparameters for modern deep networks in two challenging tasks.
2. Related Work
Bayesian optimization (BO):
BO is a popular framework for optimizing an unknown
objective function from point queries that are assumed to be costly (Shahriari et al., 2015).
Besides the standard problem formulation (Srinivas et al., 2010), diﬀerent works have con-
sidered various settings including contextual setting (Krause and Ong, 2011), batch and
parallel setting (Desautels et al., 2014), optimization under budget constraints (Lam et al.,
2016) and robust optimization (Bogunovic et al., 2018). Of particular interest to our work
are time-varying BO methods (Bogunovic et al., 2016; Imamura et al., 2020) that consider
objectives that vary with time. In Bogunovic et al. (2016), the authors consider time vari-
ations that follow a simple Markov model. Their proposed algorithm TV-GP-UCB comes
with the model-based forgetting mechanism and attains regret bounds that jointly depend
on the time horizon and rate of function variation. In this work, we consider the same
time-varying model but in the setting of sparse observations. That is, while previous works
have focused on the setting where observations are received at every time step, our goal is
to optimize an unknown time-varying objective subject to a speciﬁed budget constraint.
Hyperparameter Optimization (HPO):
There are three mainstream frameworks in
HPO: model-free, model-based and gradient-based approaches. Speciﬁcally, grid search,
2

random search (Bergstra and Bengio, 2012) and its extensions (e.g., successive halving (Jamieson
and Talwalkar, 2016) and Hyperband (Li et al., 2017)) are standard model-free HPO meth-
ods that ignore the structure of the problem and model. In contrast, BO (Hutter et al.,
2011; Bergstra et al., 2011; Snoek et al., 2012, 2015) is a common model-based approach that
aims to model the conditional probability of performance given the hyperparameters and
a dataset. Other assumptions such as learning curve behavior (Swersky et al., 2014; Klein
et al., 2017b; Nguyen et al., 2019) and computational cost (Klein et al., 2017a) are taken
into account in BO to avoid learning from scratch every time. However, both model-free and
model-based approaches involve repetitive trial-and-error processes that are very expensive
in practice. An alternative gradient-based solution is to cast HPO as a bilevel optimization
problem and take gradients with respect to the hyperparameters. Since unrolling the whole
learning trajectories is prohibitively expensive, researchers usually consider a biased several-
step look-ahead approximation (Domke, 2012; Luketina et al., 2016; Franceschi et al., 2018)
or the implicit function theorem (Larsen et al., 1996; Pedregosa, 2016; Lorraine et al., 2019;
Bertrand et al., 2020) to obtain the gradients. To further improve the eﬃciency of HPO, re-
cent works (Lorraine and Duvenaud, 2018; MacKay et al., 2019) utilize hypernetworks (Ha
et al., 2017) to approximate the inner optimization loop and a held-out validation set to
collect reward signals. Instead of constantly evaluating the validation loss, which brings
remarkable computation overhead, this work propose a cost-eﬃcient evaluation rule, and
emprirically demonstrates it eﬀectiveness in the real HPO tasks.
3. Bayesian Optimization with Costly Feedback
Recent online HPO algorithms require to obtain reward signals constantly by evaluating the
metrics such as performance gain on the validation set, drastically increasing the training
cost. Motivated by this observation, we model HPO as a time-varying Bayesian optimization
(BO) problem where the unknown function is treated as the reward signals obtained through
subsequent evaluations. To capture the evaluation cost induced by observing the rewards,
we introduce a novel costly feedback setting that allows the agent to decide, at every round,
whether to receive the observation. In turn, the agent is required to pay a certain cost
whenever it receives feedback.
3.1 Problem Setup
We aim to sequentially optimize an unknown objective function ft(x) : D × T →R deﬁned
on composite space X = D × T , where D is the ﬁnite input domainand T = {1, 2, . . . , T}
represents the time domain. At each round 1 ≤t ≤T, t ∈N, the agent decides upon a
data point xt. After selecting the point xt, it has the option to decide whether to receive
the feedback by interacting with ft. If the agent chooses to observe the feedback, then
it receives a noisy observation yt = ft(x) + zt, where zt is assumed to be independently
sampled from a Gaussian distribution N(0, σ2). Let Sn
t = {(xh(i), yh(i), i)}n
i=1 denote the
data obtained through n observations till round t. Here h(i) denotes the mapping function
that records the round at which the agent observes the i-th data point. The agent then
chooses its next point xt+1 based on the previously collected data Sn
t . If the agent decides
to query the unknown function at round t, then it needs to pay a cost ct. We let ct = 1 if
3

the feedback yt is received at round t, and otherwise ct = 0. We deﬁne the cumulative cost
as CT = PT
t=1 ct that records the total number of queries within T rounds.
Connection to online HPO:
In this paper, online HPO is modeled as the time-varying
BO with costly feedback – hyperparameter conﬁgurations xt are selected sequentially to
maximize the reward signals ft(x). Since evaluation on the validation set is expensive, the
agent is allowed to skip the evaluation or pay a certain cost ct to see the reward.
To measure the performance of algorithms, the regret for t-th round is deﬁned as rt =
maxx∈D ft(x) −ft(xt). The cumulative regret RT is the sum of instantaneous regrets RT =
PT
t=1 rt. Furthermore, we deﬁne the cumulative loss as LT = RT + CT to balance between
regret and cost. We note that if an agent queries the unknown objective function at each
time step, we recover the standard time-varying BO setting Bogunovic et al. (2016).
4. Cost-Eﬃcient GP-UCB Algorithm
Since the reward signals for online HPO vary as the learning proceeds, and similar hy-
perparameter conﬁgurations lead to similar performance, we consider using a time-varying
Gaussian process (GP) to model the objective function ft.
Consequently, we start this
section by studying the behavior of time-varying GP models with sparse observations. We
then propose a novel cost-eﬃcient algorithm and study its theoretical performance. We
defer all the analysis to the supplementary material.
4.1 Time-varying Gaussian Process Model
We use a spatio-temporal GP to model the underlying function ft. The smoothness prop-
erties of ft are depicted by a composite kernel function (Bogunovic et al., 2016; Imamura
et al., 2020): k = kspace ⊗ktime, where kspace : D × D →R+ and ktime : T × T →R+
are spatial and temporal kernels; (kspace ⊗ktime)((x, t), (x′, t′)) := kspace(x, x′) · ktime(t, t′).
Following Bogunovic et al. (2016), we consider the following transition relation of the un-
derlying function: ft+1(x) = √1 −ϵft(x) + √ϵgt+1(x), where gt is sampled from a GP
with a mean function µ and a kernel function k, i.e., gt ∼GP(µ, k)1; ϵ ∈[0, 1] is the for-
getting rate that constrains the variation of the function. This particular Markov model
maps to the following exponential temporal kernel as shown in Bogunovic et al. (2016):
ktime (τ, τ ′) = (1 −ϵ)
|τ−τ′|
2
.
Given observed data points xn
t = {xh(1), . . . , xh(n)} and yn
t = {yh(1), . . . , yh(n)}, the
posterior over f is a GP with mean ˜µt(x), covariance kt(x, x′) and variance ˜σ2
t (x) = kt(x, x):
˜µt(x) = ekn
t (x)T 
eKn
t + σ2In
t
−1
yn
t ,
(1)
kt(x, x′) = k(x, x′) −ekn
t (x)T 
eKn
t + σ2In
t
−1 ekn
t (x′),
(2)
where eKn
t = Kn
t ◦Dn
t , with Dn
t =

(1 −ϵ)|h(i)−h(j)|/2n
i,j=1, and ˜kn
t (x) = kn
t (x) ◦dn
t with
dn
t =

(1 −ϵ)(t+1−h(i))/2t
i=1. Here Kn
t = [k (xi, xj)]n
i,j=1, kn
t (x) = [k (xi, x)]n
i=1, ◦is the
Hadamard product, and In
t is the n × n identity matrix.
1. Without loss of generality, as in Srinivas et al. (2010), we assume µ = 0 for GPs not conditioned on data.
4

One key challenge in BO is to balance the exploration-exploitation tradeoﬀs rigorously.
Various works (e.g., Srinivas et al. (2010); Bogunovic et al. (2016)) have focused on the
upper-conﬁdence bound (UCB) rule that selects points by maximizing a linear combination
of posterior mean and variance.
In this paper, our focus is also on the UCB rule due
to its strong theoretical guarantees and empirical performance.
In what follows, we let
ucbt(x) := ˜µt(x) + β1/2
t+1˜σt(x), where {βt}T
t=1, each βt ∈R+, is a non-decreasing sequence of
exploration parameters, selected as Bogunovic et al. (2016), such that (w.h.p.) ucbt(x) is a
valid upper conﬁdence bound on ft(x) for every x and t.
4.2 Strategies for Querying Feedback
In BO with costly feedback, there are two decisions for the agent to make at every round:
1) where to evaluate the unknown objective; 2) whether to receive the feedback (and con-
sequently incur cost). In this section, we focus on the latter problem. To minimize regret
while reducing the query cost, we propose two strategies for the query allocation.
Querying with Bernoulli Sampling Schedule:
A simple and model-agnostic method
for our problem is to query the objective function according to a ﬁxed probability. Speciﬁ-
cally, we query the feedback based on a Bernoulli random variable Ber(B/T), which guar-
antees that the algorithm receives in expectation B observations of yt.
Leveraging Uncertainty Information from GP:
Although the Bernoulli strategy de-
ﬁned above can reduce the query cost, it does not leverage any knowledge from the GP
model. As a consequence, in practice, there is often a signiﬁcant performance loss com-
pared to standard algorithms with full observations. To overcome this diﬃculty, we propose
a cost-eﬃcient query rule that automatically assesses the uncertainty of the current decision
for time-varying GP-UCB (Bogunovic et al., 2016) and its variants.
On a high level, the agent aims to maintain informative queries but skip uninformative
ones to save the query cost. Hence, we consider the following cost-eﬃcient query rule: the
agent only queries the feedback when the following condition satisﬁes:
P (ˆyt(xt) > ˆyt(x)) < κ, ∃x ∈D \ {xt},
where ˆyt(x) is the posterior predictive estimation of the unknown objective at point x,
which is a random variable distributed according to N
 ˜µt(x), ˜σ2
t (x)

. The analytic solution
to P (ˆyt(xt) > ˆyt(x)) < κ, as well as the analysis of the choices of κ are provided in Ap-
pendix A. This rule explicitly encourages receiving feedback when the agent is less conﬁdent
in distinguishing the best candidate. The intuition behind this rule is to skip querying ft
when the selected xt leads to “small” regret.
4.3 Cost-Eﬃcient GP-UCB Algorithm
We integrate the two strategies for querying feedback in time-varying GP-UCB model and
obtain two algorithms for our costly feedback setting. Speciﬁcally, we give the cost-eﬃcient
algorithm (CE-GP-UCB) that leverages the uncertainty information from GP in Algo-
rithm 1. Our algorithm selects the points with largest UCB at every round (Line 3) but
only observes the feedback yt when the when the model is most uncertain (Line 4). Note
5

Algorithm 1 CE-GP-UCB
Require: Input Domain D, total rounds T, GP prior (˜µ0, ˜σ0, k), forgetting rate ϵ, user-speciﬁc
conﬁdence threshold κ
1: Initialize observation set S0
0 = ∅, number of interactions n = 0.
2: for t = 1, 2, · · · , T do
3:
Choose xt = arg maxx∈D ˜µt−1(x) + √βt˜σt−1(x)
▷Select points according to UCB rule
4:
if P (ˆyt(xt) > ˆyt(x)) < κ, ∃x ∈D \ {xt} then
▷Cost-eﬃcient query strategy
5:
Receive feedback yt = ft (xt) + zt, n ←n + 1
▷Interact with ft and receive feedback
6:
Add (xt, yt, t) to observation set: Sn+1
t
= Sn
t−1 ∪(xt, yt, t)
7:
Perform Bayesian update to obtain ˜µt and ˜σt
8:
else ˜µt = ˜µt−1, ˜σt = ˜σt−1, Sn
t = Sn
t−1
▷Do not observe feedback
9:
end if
10: end for
that the query strategy can be replaced by Bernoulli sampling strategy to consume the
budget B as introduced in Sec 4.2.
When no feedback is received (Line 8), there is no update of the GP model; however,
due to the time-varying nature, the model will be less conﬁdent about the estimation for
the candidates thus the posterior variance will increase.
Remark:
Note that if the candidates are close to each other (e.g., quantized candidates
for continuous variables), the model will never be conﬁdent in its current decision since
the UCB of best and second-best candidates are very close.
In other words, the cost-
eﬃcient query rule (Line 4 in Algorithm 1) will continuously be activated. However it is
less informative to query the feedback for two close candidates within one mode. Therefore,
in practice, we ﬁrst ﬁnd the local optima of the UCB function and their corresponding data
points, then deploy the cost-eﬃcient query rule for these local optima.
5. Experiments
We thoroughly evaluate the proposed algorithm on both synthetic data and practical on-
line hyperparameter optimization problems: (1) tuning schedules for self-tuning networks
(STN MacKay et al. (2019)); (2) data augmentations for unsupervised representation learn-
ing (Chen et al., 2020). Extensive experiments show that cost-eﬃcient query rule leads to
substantial improvements over simple Bernoulli strategy. We leave extra experimental de-
tails in Appendix B.
5.1 Evaluation on Synthetic Data
We follow the same synthetic setting as previous study Bogunovic et al. (2016). Speciﬁcally,
we used a one-dimensional input domain D = [0, 1] and quantized it into 1,000 uniformly
divided points. We generated the time-varying objective functions according to the following
Markov model ft+1(x) = √1 −ϵft(x) + √ϵgt+1(x) under diﬀerent forgetting rate ϵ, where
gt(x) ∼GP(0, k). We use Mat´ern3/2 kernel for GP models and set sampling noise variance
σ2 = 0.01. The time horizon T is set as 500.
Figure 1 shows the trade-oﬀcurves between average regret RT /T and query cost CT
for diﬀerent algorithms, where the performance is averaged over 50 independent trials.
6

0
200
400
Cost CT
0.2
0.4
0.6
0.8
1.0
1.2
Average Regret RT/T
= 0.003
R-GP-PI
R-GP-EI
R-GP-UCB
TV-GP-UCB
CE-GP-UCB
0
200
400
Cost CT
0.2
0.4
0.6
0.8
1.0
1.2
= 0.005
0
200
400
Cost CT
0.2
0.4
0.6
0.8
1.0
1.2
= 0.01
100
200
300
400
500
Cost CT
0.2
0.4
0.6
0.8
1.0
1.2
= 0.03
100
200
300
400
500
Cost CT
0.2
0.4
0.6
0.8
1.0
1.2
= 0.05
Figure 1: Comparison of trade-oﬀbetween average regret RT /T and query cost CT .
(a) TV-GP-UCB
(b) TV-GP-UCB Ber(0.8)
(c) ours (κ = 0.90)
(d) ours (κ = 0.95)
Figure 2: Visualizations of GP models. The purple points and green points denote the
agents’ choices to receive the feedback or not. Data points with darker color mean that
they are visited more recently. The blue line (—) and dashed red line (---) indicate
the ground truth of unknown objective function and the posterior mean. The gray area
denotes the conﬁdence area.
Speciﬁcally, we consider the TV-GP-UCB (Bogunovic et al., 2016), R-GP-UCB (Bogunovic
et al., 2016), and its variants with other popular acquisition functions (PI: probability of
improvement; EI: expected improvement) with Bernoulli query policy. We observe that our
method suﬀers from a minor regret loss when using 50% query cost (CT = 250), whereas
other baselines with Bernoulli strategy lead to a larger performance loss. We then visualize
the ﬁnal GP models in Figure 2 (ϵ = 0.03). In particular, CE-GP-UCB maintains a similar
posterior mean and variance for the unknown function compared with TV-GP-UCB with
full observations and skips the queries when chosen candidates are close to the optimal ones.
By contrast, TV-GP-UCB with Bernoulli strategy skips the queries even when the selected
points are far from the optimal ones, thus leading a larger regret.
5.2 Self-Tuning Networks
To further study the eﬀectiveness of our algorithm in real applications, we ﬁrst evaluate the
proposed method in adjusting the tuning schedule for self-tuning networks (Lorraine and
Duvenaud, 2018) (STN). STN is an online HPO algorithm that uses a hypernetwork (Ha
et al., 2017) to approximate the inner loop of bilevel optimization. Standard STN tunes
hyperparameters (e.g., dropout, weight-decay and data augmentation) by alternating the
following two steps (i.e., train/val=1:1): (1) (training phase) train one mini-batch data on
training set; (2) (tuning phase) evaluate one mini-batch data on held-out validation set then
backpropagate gradients through the hypernetwork to update hyperparameters. However,
7

20
40
60
80
100
120
140
Epoch
0.78
0.80
0.82
0.84
0.86
0.88
Validation Accuracy
20
40
60
80
100
120
140
Epoch
0.4
0.5
0.6
0.7
0.8
Validation Loss
CE-GP-UCB ( = 0.8)
train:val=1:1
train:val=1:1->2:1
train:val=1:1->5:1
train:val=1:1->10:1
train:val=2:1
train:val=2:1->1:1
train:val=2:1->5:1
train:val=2:1->10:1
train:val=5:1
train:val=5:1->1:1
train:val=5:1->2:1
train:val=5:1->10:1
train:val=10:1
train:val=10:1->1:1
train:val=10:1->2:1
train:val=10:1->5:1
1000
1200
1400
1600
1800
Training time (s)
0.42
0.44
0.46
0.48
0.50
0.52
0.54
0.56
Validation Loss
CE-GP-UCB ( = 0.8)
1:1
10:1
10:1
1:1
2:1
10:1
10:1
2:1
1:1
5:1
5:1
1:1
2:1
5:1
5:1
2:1
CE-GP-UCB ( = 0.7)
10:1
5:1
2:1
1:1
1:1
2:1
2:1
1:1
5:1
10:1
10:1
5:1
Figure 3: (a) Validation accuracy and & (b) validation loss for STN with diﬀerent tuning
schedules. (c) Trade-oﬀs between validation loss and corresponding training time (excluding
querying cost).
we found that STN is sensitive to diﬀerent tuning schedules and standard “dense tuning”
is expensive and sub-optimal.
Table 1: Comparison with conventional MAB al-
gorithms for STN (VGG16 on CIFAR-10).
ℓval
Accval
ℓtest
Acctest
Ttotal
Grid Search
0.421
0.887
0.444
0.879
11.69 ×
Ber(0.1)
EXP3.R
0.466
0.841
0.479
0.835
0.93 ×
ϵ-greedy
0.419
0.864
0.438
0.859
0.92 ×
Softmax
0.433
0.854
0.459
0.846
0.93 ×
UCB
0.447
0.869
0.451
0.867
0.94 ×
GP-UCB
0.422
0.877
0.449
0.868
0.96 ×
Ber(0.2)
EXP3.R
0.446
0.853
0.455
0.847
1.13 ×
ϵ-greedy
0.401
0.881
0.443
0.873
1.13 ×
Softmax
0.449
0.862
0.480
0.852
1.13 ×
UCB
0.398
0.878
0.421
0.871
1.13 ×
GP-UCB
0.413
0.880
0.439
0.870
1.17 ×
CE-GP-UCB (κ = 0.7)
0.390
0.888
0.428
0.881
1.10 ×
CE-GP-UCB (κ = 0.8)
0.373
0.892
0.404
0.881
1.21 ×
As a consequence, we model STN as
a two-armed bandits:
“training only”
and “tuning + training”.
Since tun-
ing one mini-batch data is biased, we
deﬁne the unknown objective function
as the accuracy gain on the whole val-
idation dataset, which is expensive to
obtain thus leading a large query cost.
We evaluate the proposed approach and
other TV bandit algorithms for VGG16
on CIFAR-10. Speciﬁcally, we randomly
choose 10,000 training images as the val-
idation set, and tune layerwise dropout
and data augmentation parameters, fol-
lowing Lorraine and Duvenaud (2018).
The network is trained 150 epochs with an initial learning rate 0.1.
The learning rate
is decayed at 60, 100 and 120 epochs with a decay rate of 0.1.
We ﬁrst consider two baselines including static schedules (MacKay et al., 2019) and a
simple switching heuristics (e.g., train:val=1:1→2:1) that occurs at 100 epochs in which
the learning rate is decayed.
Since smaller learning rate is easier to cause overﬁtting,
we expect to ﬁnd a better schedule by also decaying the tuning frequency at the late
training stage. The dashed lines in Figure 3 (a) & (b) show that the performance of STN
is sensitive to varied tuning schedules.
As shown in Figure 3 (a) & (b), our approach
leads to the best performance in terms of validation accuracy and loss with only a single
run of the experiment. In Figure 3 (c), CE-GP-UCB successfully ﬁnds an eﬃcient tuning
schedule online that saves around 20% training cost meanwhile achieving lower validation
loss. Table 1 gives a quantitative comparison between our algorithm and other baselines
including grid search and TV bandits algorithms with Bernoulli strategy.
After taking
account of the cost for evaluation on the validation set, we ﬁnd that the GP-UCB algorithm
with cost-eﬃcient query rule outperforms other baselines on both validation and testing set
with modest computation overhead.
8

5.3 Unsupervised Representation Learning
We evaluate our approach on tuning data augmentations in unsupervised contrastive learn-
ing. The goal of unsupervised learning is to learn meaningful representations directly from
unlabeled data since acquiring annotations is expensive. SimCLR (Chen et al., 2020) is
the state-of-the-art approach that learns representations by maximizing agreement between
diﬀerently augmented views of the same data example. It is shown in Chen et al. (2020)
that the types of data augmentations are critical in learning meaningful representations so
researchers conducted extensive ablation study in data augmentations. By contrast, we aim
to apply CE-GP-UCB in tuning the probability of randomly applying eight common data
augmentations including cropping, color distortion, cutout, ﬂipping horizontally and verti-
cally, rotation, Gaussian blur and gray scale with one single run. We deﬁne the accuracy
gain (%) on validation set as the costly feedback. The initial probability for applying each
data augmentation is set 0.5 and the tuning range is [0, 1.0]. The forgetting rate ϵ and βt
are set as 0.01 and 1.0, respectively. We leave the implementation details of SimCLR in
Appendix B.
Table
2:
Linear
readout
performance
(Ima-
geNet100 top-1 accuracy) of ResNet50 with dif-
ferent data augmentations.
Top1 (R10)
Top1 (R100)
Time
Baseline
70.91
73.20
1.00 ×
TV-GP-UCB (full)
75.14
77.95
1.97 ×
TV-GP-UCB Ber(0.6)
72.15
75.31
1.64 ×
CE-GP-UCB (κ = 0.9)
74.80
77.56
1.88 ×
CE-GP-UCB (κ = 0.8)
74.77
77.62
1.71 ×
CE-GP-UCB (κ = 0.7)
74.58
77.27
1.61 ×
Human expert (Chen et al., 2020)
75.00
77.99
-
Table 2 shows the linear read-
out performance for diﬀerent models,
where the baseline is SimCLR with
ﬁxed initial probability (0.5) for eight
data augmentations.
As shown in
Table 2, our method reaches human
expert-level performance with one sin-
gle run of experiment and surpass base-
line (no HPO) by a large margin. More
importantly, proposed CE-GP-UCB is
able to reduce 40% query cost for vali-
dation with minor performance loss compared to TV-GP-UCB with full observations; how-
ever, TV-GP-UCB with Bernoulli strategy results in a large performance loss. It again
veriﬁes proposed cost-eﬃcient query rule is able to successfully maintain the most informa-
tive queries and provide an alternative BO solution when resources are limited.
6. Conclusion
Online HPO typically requires constantly evaluating on the validation set and taking gra-
dient steps w.r.t. hyperparameters, resulting in drastically higher training cost. In this
paper, we propose a novel costly feedback Bayesian optimization (BO) setting to model the
computation cost for querying the reward signals from the validation set. To keep most
informative queries and skip less informative ones, we introduce a cost-eﬃcient GP-UCB
algorithm that automatically assesses the uncertainty of current GP model. We further ver-
ify the eﬀectiveness of our proposed approach with extensive experiments on both synthetic
data and large-scale real world online HPO for deep neural networks.
9

Acknowledgement
We thank Renjie Liao, Sivabalan Manivasagam and James Tu for their thoughtful discus-
sions and useful feedback.
References
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine learning, 47(2-3):235–256, 2002a.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic
multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002b.
Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham,
Andrew Gordon Wilson, and Eytan Bakshy. Botorch: Programmable bayesian optimiza-
tion in pytorch. CoRR, abs/1910.06403, 2019.
James Bergstra and Yoshua Bengio.
Random search for hyper-parameter optimization.
Journal of machine learning research, 13(Feb):281–305, 2012.
James Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-
parameter optimization. In NIPS, pages 2546–2554, 2011.
Quentin Bertrand, Quentin Klopfenstein, Mathieu Blondel, Samuel Vaiter, Alexandre
Gramfort, and Joseph Salmon. Implicit diﬀerentiation of lasso-type models for hyperpa-
rameter optimization. CoRR, abs/2002.08943, 2020.
Ilija Bogunovic, Jonathan Scarlett, and Volkan Cevher.
Time-varying gaussian process
bandit optimization. In AISTATS, volume 51 of JMLR Workshop and Conference Pro-
ceedings, pages 314–323. JMLR.org, 2016.
Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, and Volkan Cevher.
Adversarially
robust optimization with gaussian processes. In Conference on Neural Information Pro-
cessing Systems (NeurIPS), 2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey E. Hinton.
A simple
framework for contrastive learning of visual representations.
CoRR, abs/2002.05709,
2020.
Yizong Cheng. Mean shift, mode seeking, and clustering. IEEE Trans. Pattern Anal. Mach.
Intell., 17(8):790–799, 1995.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-
scale hierarchical image database. In CVPR, pages 248–255. IEEE Computer Society,
2009.
Thomas Desautels, Andreas Krause, and Joel W. Burdick.
Parallelizing exploration-
exploitation tradeoﬀs in gaussian process bandit optimization. Journal of machine learn-
ing research, 15(1):3873–3923, 2014.
10

Justin Domke. Generic methods for optimization-based modeling. In AISTATS, volume 22
of JMLR Proceedings, pages 318–326. JMLR.org, 2012.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pon-
til. Bilevel programming for hyperparameter optimization and meta-learning. In ICML,
volume 80 of Proceedings of Machine Learning Research, pages 1563–1572. PMLR, 2018.
David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In ICLR (Poster). OpenRe-
view.net, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, pages 770–778. IEEE Computer Society, 2016.
Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based opti-
mization for general algorithm conﬁguration. In LION, volume 6683 of Lecture Notes in
Computer Science, pages 507–523. Springer, 2011.
Hideaki Imamura, Nontawat Charoenphakdee, Futoshi Futami, Issei Sato, Junya Honda,
and Masashi Sugiyama. Time-varying gaussian process bandit optimization with non-
constant evaluation time. CoRR, abs/2003.04691, 2020.
Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identiﬁcation and hy-
perparameter optimization. In AISTATS, volume 51 of JMLR Workshop and Conference
Proceedings, pages 240–248. JMLR.org, 2016.
Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter.
Fast
bayesian optimization of machine learning hyperparameters on large datasets. In AIS-
TATS, volume 54 of Proceedings of Machine Learning Research, pages 528–536. PMLR,
2017a.
Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve
prediction with bayesian neural networks. In ICLR (Poster). OpenReview.net, 2017b.
Andreas Krause and Cheng Soon Ong. Contextual gaussian process bandit optimization.
In NIPS, pages 2447–2455, 2011.
Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny
images. 2009.
Volodymyr Kuleshov and Doina Precup. Algorithms for the multi-armed bandit problem.
In Journal of machine learning research. Citeseer, 2000.
R´emi Lam, Karen Willcox, and David H. Wolpert. Bayesian optimization with a ﬁnite
budget: An approximate dynamic programming approach.
In NIPS, pages 883–891,
2016.
Jan Larsen, Lars Kai Hansen, Claus Svarer, and M Ohlsson. Design and regularization
of neural networks: the optimal use of a validation set. In Neural Networks for Signal
Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, pages
62–71. IEEE, 1996.
11

Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar.
Hyperband: Bandit-based conﬁguration evaluation for hyperparameter optimization. In
ICLR (Poster). OpenReview.net, 2017.
Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale
optimization. Math. Program., 45(1-3):503–528, 1989.
Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through
hypernetworks. CoRR, abs/1802.09419, 2018.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparam-
eters by implicit diﬀerentiation. CoRR, abs/1911.02590, 2019.
Jelena Luketina, Tapani Raiko, Mathias Berglund, and Klaus Greﬀ. Scalable gradient-based
tuning of continuous regularization hyperparameters.
In ICML, volume 48 of JMLR
Workshop and Conference Proceedings, pages 2952–2960. JMLR.org, 2016.
Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger B. Grosse.
Self-tuning networks: Bilevel optimization of hyperparameters using structured best-
response functions. In ICLR (Poster). OpenReview.net, 2019.
Vu Nguyen, Sebastian Schulze, and Michael A. Osborne. Bayesian optimization for iterative
learning. CoRR, abs/1909.09593, 2019.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In ICML,
volume 48 of JMLR Workshop and Conference Proceedings, pages 737–746. JMLR.org,
2016.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking
the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE,
104(1):148–175, 2015.
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of
machine learning algorithms. In NIPS, pages 2960–2968, 2012.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sun-
daram, Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams.
Scalable bayesian
optimization using deep neural networks. In ICML, volume 37 of JMLR Workshop and
Conference Proceedings, pages 2171–2180. JMLR.org, 2015.
Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian
process optimization in the bandit setting: No regret and experimental design. In ICML,
pages 1015–1022. Omnipress, 2010.
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimiza-
tion. CoRR, abs/1406.3896, 2014.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. CoRR,
abs/1906.05849, 2019.
12

Appendix A. Choices of Conﬁdence threshold κ in CE-GP-UCB
In what follows, we ﬁrst give an important property induced by no overlapping between the
conﬁdence bounds of best candidate xt and the other points x ∈D \ {xt}.
Lemma 1 Let δ ∈(0, 1) and xt = arg maxx∈D ˜µt−1(x)+√βt˜σt−1(x), if ucb(x) ≤lcb(xt), ∀x ∈
D \ {xt}, then picking xt at round t produces no regret with at least probability 1 −δ.
Proof [Proof of Lemma 1] From previous literature, we know that, with high probability
|ft(x) −˜µt−1(x)| ≤β
1
2
t ˜σt−1(x).
∀x ∈D, ∀t ≥1
(3)
As a result, we have ˜µt−1(x) −β
1
2
t ˜σt−1(x) ≤ft(x) ≤˜µt−1(x) + β
1
2
t ˜σt−1(x) ∀x ∈D ∀t ≥1.
Let x∗
t = maxx∈D ft(x). If x∗
t = xt, then the instantaneous regret rt = 0. Otherwise,
ft(x∗) −ft(xt) ≤˜µt−1(x∗
t ) + β
1
2
t ˜σt−1(x∗
t ) −

˜µt−1(xt) −β
1
2
t ˜σt−1(xt)

≤0
(4)
From the deﬁnition of x∗
t , we known ft(x∗) −ft(xt) ≥0. So we have picking xt at round t
produces no regret with high probability.
Lemma 1 indicates that if there is no overlapping between the conﬁdence bounds of
best candidate xt and the others, the agent can assure that xt is the optimal choice with
high probability. As a consequence, if the above condition satisﬁes, the agent is able to
receive no feedback meanwhile resulting in no performance loss at this time. This inspires
us to leverage the information that exists in relative magnitudes of posterior mean and
variance for the given candidates and obtain the cost-eﬃcient query strategy, which is a
loosed condition with a conﬁdence threshold κ ∈(0, 1).
We know ˆyt(x) follows the Gaussian distribution N(˜µt−1(x), ˜σt−1(x)). Then we know
ˆyt(xt) −ˆyt(x∗
t ) follows the distribution
N

˜µt−1(xt) −˜µt−1(x∗
t ),
q
˜σ2
t−1(xt) + ˜σ2
t−1(x∗
t )

.
As a consequence,
1 −Φ

−˜µt−1(xt) −˜µt−1(x∗
t )
q
˜σ2
t−1(xt) + ˜σ2
t−1(x∗
t )

≥γt,
(5)
where Φ(·) is the CDF of standard Gaussian distribution. To simplify further, we have
˜µt−1(xt) −˜µt−1(x∗
t ) ≥Φ−1(γt)
q
˜σ2
t−1(xt) + ˜σ2
t−1(x∗
t )
(6)
≥Φ−1(γt)
√
2
(˜σt−1(xt) + ˜σt−1(x∗
t )) .
(7)
13

Table A1: Comparison of CE-GP-UCB (κ = 95%) with standard TV bandits algorithms
with Bernoulli query strategy on three kinds of synthetic data.
Sine
Gaussian
Piecewise
RT /T
CT
RT /T
CT
RT /T
CT
EXP3.S (Auer et al., 2002b)
0.384 ± 0.036
201 ± 15
0.257 ± 0.017
200 ± 11
0.437 ± 0.046
201 ± 14
ϵ-greedy (Kuleshov and Precup, 2000)
0.167 ± 0.041
198 ± 14
0.145 ± 0.026
200 ± 12
0.196 ± 0.026
199 ± 14
Softmax (Kuleshov and Precup, 2000)
0.132 ± 0.025
199 ± 13
0.159 ± 0.032
201 ± 14
0.256 ± 0.037
198 ± 14
UCB (Auer et al., 2002a)
0.084 ± 0.010
200 ± 14
0.112 ± 0.026
201 ± 15
0.172 ± 0.042
202 ± 14
GP-UCB (Srinivas et al., 2010)
0.104 ± 0.012
200 ± 14
0.139 ± 0.025
200 ± 14
0.122 ± 0.009
201 ± 12
CE-GP-UCB
0.017 ± 0.009
52 ± 7
0.038 ± 0.017
53 ± 20
0.028 ± 0.002
52 ± 5
If x∗
t ̸= xt, we have
ft(x∗) −ft(xt) ≤˜µt−1(x∗
t ) + β
1
2
t ˜σt−1(x∗
t ) −

˜µt−1(xt) −β
1
2
t ˜σt−1(xt)

(8)
≤˜µt−1(x∗
t ) −˜σt−1(x∗
t ) + β
1
2
t (˜σt−1(x∗
t ) + ˜σt−1(xt))
(9)
≤

β
1
2
t −Φ−1(γt)
√
2

(˜σt−1(x∗
t ) + ˜σt−1(xt)) .
(10)
Let γt = Φ(
√
2βt), we recover the LCB-UCB rule (free no regret). If γt < Φ(
√
2βt), then
the agents suﬀer from a “small regret”

β
1
2
t −Φ−1(γt)
√
2

(˜σt−1(x∗
t ) + ˜σt−1(xt)).
In practice, we found LCB-UCB rule (Lemma 1) is a very strict rule that might has
limited eﬀect in reducing the query cost. For instance, as shown in Table A2, we found
LCB-UCB rule is more strict than κ = 0.99.
Appendix B. Supplementary Experiments
In this section, we provide the details of experimental set-up and supplementary results.
B.1 Evaluation on Synthetic Data
Apart from BO setting where the candidates are correlated (e.g., kspace is SE or Mat´ern
kernels), we consider the ﬁnite-arm bandits setting where each arm is assumed independent
(i.e., kspace = I). Speciﬁcally, we consider a three-armed bandit problem where the reward
for each arm is sampled from its underlying time varying functions (σ2 = 0.01). Speciﬁcally,
we design three kinds of functions: (1) a sine curve (2) a Gaussian curve, and (3) a piecewise
step function, where the time horizon T = 2000. Figure A1 shows an example of the time-
varying functions. We compare with other TV bandits algorithms with Bernoulli sampling
policy (Sec 4.2), i.e., m ∼Ber(0.1) incorporated, including EXP3.S (Auer et al., 2002b),
ϵ-greedy (Kuleshov and Precup, 2000), Softmax (Boltzmann Exploration) (Kuleshov and
Precup, 2000), UCB (Auer et al., 2002a) and GP-UCB (Srinivas et al., 2010). We used
the RBF kernel in GP models and the parameters (e.g., length scale) for GP are optimized
using maximum likelihood. Each experiment is repeated 100 times with diﬀerent random
seeds.
14

0
250
500
750
1000
1250
1500
1750
2000
Timestep
0.0
0.2
0.4
0.6
0.8
1.0
Reward
(a) Sine
0
250
500
750
1000
1250
1500
1750
2000
Timestep
0.0
0.2
0.4
0.6
0.8
1.0
Reward
(b) Gaussian
0
250
500
750
1000
1250
1500
1750
2000
Timestep
0.0
0.2
0.4
0.6
0.8
1.0
Reward
Arm 1
Arm 2
Arm 3
(c) Piecewise
Figure A1: Time-varying functions considered for the synthetic bandit experiments.
0
250
500
750
1000
1250
1500
1750
2000
Timestep
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Regret Rt/t
(a) Sine
0
250
500
750
1000
1250
1500
1750
2000
Timestep
0.0
0.1
0.2
0.3
0.4
Regret Rt/t
(b) Gaussian
0
250
500
750
1000
1250
1500
1750
2000
Timestep
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Regret Rt/t
EXP3.R
UCB
eps-greedy
Softmax
GP-UCB
CE-GP-UCB
(c) Piecewise
Figure A2: Average regret RT /T for time-varying bandits algorithms on three diﬀerent
synthetic data.
As shown in Table A1, we found proposed CE-GP-UCB consistently achieves lower
regrets while interacting less with the unknown functions. This is because CE-GP-UCB
automatically assesses the conﬁdence of current policy based on current GP models and
given problems, and skips the queries if it is pretty conﬁdent the decision is correct. More-
over, compared to GP-UCB with full observations, our method saves around 97.5% queries,
which results in 40× computation cost saving in evaluating the unknown objective. Fig-
ure A2 shows how Rt/t evolves as training proceeds for diﬀerent algorithms. It again veriﬁes
cost-eﬃcient query rule can adapt to diﬀerent problems quickly and achieve lower regrets.
TV Bayesian optimization
Table A2 presents numerical performance of CE-GP-UCB
on synthetic data with diﬀerent conﬁdence threshold κ. The performance is averaged over
50 independent trials. We found that CE-GP-UCB can almost recover the performance
of original TV-GP-UCB meanwhile signiﬁcantly cutting oﬀthe cost for interacting with
unknown functions which is usually very expensive.
Taking ϵ = 0.05 for instance, CE-
GP-UCB (κ = 0.9) reduces up to 40% queries meanwhile only suﬀers from 2.6% regret
loss.
B.2 Self-Tuning Networks
Figure A3 shows the training curves (loss and accuracy) under diﬀerent tuning schedules.
We observed that the training curves of STN diﬀer from standard training curves since
large amounts of data augmentations are added at the late stage of training which results
in a huge performance loss on training data. However, even though the performance is
15

Table A2: Numerical performance on synthetic data. RT /T and CT denote average regret
and total cost for observing rewards. @κ denotes the conﬁdence threshold for CE-GP-UCB
is κ. @* denotes the LCB-UCB rule (Lemma 1). We highlight the setting within 10% regret
loss compared to TV-GP-UCB with full observations. The performance is averaged over 50
independent trials.
ϵ = 0.003
ϵ = 0.005
ϵ = 0.01
ϵ = 0.03
ϵ = 0.05
RT /T
CT
RT /T
CT
RT /T
CT
RT /T
CT
RT /T
CT
R-GP-UCB
0.352 ± 0.187
499 ± 0
0.371 ± 0.166
499 ± 0
0.425 ± 0.118
499 ± 0
0.516 ± 0.093
499 ± 0
0.581 ± 0.093
499 ± 0
TV-GP-UCB
0.094 ± 0.035
499 ± 0
0.126 ± 0.044
499 ± 0
0.184 ± 0.056
499 ± 0
0.305 ± 0.034
499 ± 0
0.392 ± 0.034
499 ± 0
TV-GP-UCB Ber(0.2)
0.231 ± 0.089
99 ± 8
0.275 ± 0.084
99 ± 9
0.350 ± 0.094
97 ± 9
0.561 ± 0.122
100 ± 9
0.694 ± 0.090
100 ± 8
TV-GP-UCB Ber(0.3)
0.176 ± 0.093
147 ± 9
0.218 ± 0.104
150 ± 12
0.312 ± 0.089
144 ± 10
0.485 ± 0.089
150 ± 10
0.592 ± 0.096
153 ± 10
TV-GP-UCB Ber(0.4)
0.147 ± 0.072
200 ± 10
0.186 ± 0.073
200 ± 10
0.275 ± 0.090
201 ± 11
0.428 ± 0.073
203 ± 11
0.522 ± 0.073
203 ± 9
TV-GP-UCB Ber(0.5)
0.121 ± 0.060
247 ± 10
0.160 ± 0.061
249 ± 12
0.241 ± 0.073
252 ± 12
0.382 ± 0.064
249 ± 12
0.480 ± 0.058
250 ± 12
TV-GP-UCB Ber(0.6)
0.119 ± 0.055
296 ± 9
0.144 ± 0.041
300 ± 11
0.230 ± 0.078
298 ± 11
0.363 ± 0.045
298 ± 11
0.452 ± 0.045
298 ± 12
TV-GP-UCB Ber(0.7)
0.112 ± 0.049
348 ± 10
0.151 ± 0.060
348 ± 11
0.209 ± 0.068
348 ± 10
0.335 ± 0.038
350 ± 11
0.429 ± 0.040
350 ± 9
TV-GP-UCB Ber(0.8)
0.102 ± 0.046
401 ± 9
0.140 ± 0.058
399 ± 11
0.198 ± 0.067
399 ± 9
0.328 ± 0.035
400 ± 8
0.415 ± 0.037
400 ± 7
TV-GP-UCB Ber(0.9)
0.098 ± 0.048
450 ± 7
0.132 ± 0.051
449 ± 7
0.196 ± 0.066
448 ± 7
0.311 ± 0.036
448 ± 6
0.404 ± 0.035
449 ± 7
CE-GP-UCB @0.60
0.535 ± 0.416
8 ± 5
0.558 ± 0.319
12 ± 7
0.600 ± 0.289
20 ± 11
0.642 ± 0.141
53 ± 11
0.680 ± 0.119
85 ± 17
CE-GP-UCB @0.70
0.410 ± 0.329
16 ± 11
0.435 ± 0.253
21 ± 13
0.442 ± 0.186
41 ± 25
0.504 ± 0.096
91 ± 21
0.533 ± 0.071
134 ± 21
CE-GP-UCB @0.75
0.341 ± 0.249
22 ± 17
0.367 ± 0.199
34 ± 20
0.368 ± 0.176
62 ± 27
0.441 ± 0.077
117 ± 25
0.484 ± 0.074
167 ± 30
CE-GP-UCB @0.80
0.286 ± 0.169
31 ± 27
0.297 ± 0.157
51 ± 36
0.304 ± 0.117
76 ± 27
0.387 ± 0.065
146 ± 31
0.451 ± 0.053
200 ± 30
CE-GP-UCB @0.85
0.226 ± 0.131
51 ± 41
0.248 ± 0.095
78 ± 42
0.266 ± 0.069
101 ± 46
0.356 ± 0.054
180 ± 34
0.416 ± 0.041
235 ± 35
CE-GP-UCB @0.90
0.188 ± 0.089
82 ± 73
0.202 ± 0.080
101 ± 63
0.238 ± 0.070
140 ± 48
0.322 ± 0.045
233 ± 37
0.400 ± 0.036
291 ± 30
CE-GP-UCB @0.95
0.157 ± 0.074
125 ± 77
0.161 ± 0.051
163 ± 73
0.210 ± 0.052
207 ± 60
0.307 ± 0.037
310 ± 42
0.397 ± 0.034
371 ± 29
CE-GP-UCB @0.99
0.112 ± 0.037
292 ± 104
0.128 ± 0.030
309 ± 83
0.176 ± 0.031
363 ± 60
0.295 ± 0.031
435 ± 26
0.386 ± 0.033
468 ± 15
CE-GP-UCB @*
0.103 ± 0.034
386 ± 79
0.126 ± 0.031
414 ± 50
0.168 ± 0.033
442 ± 41
0.292 ± 0.031
482 ± 11
0.384 ± 0.031
492 ± 7
0
20
40
60
80
100
120
140
Epoch
0.4
0.5
0.6
0.7
0.8
0.9
Training Accuracy
(a) Training Accuracy
0
20
40
60
80
100
120
140
Epoch
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Training Loss
CE-GP-UCB ( = 0.8)
train:val=1:1
train:val=1:1->2:1
train:val=1:1->5:1
train:val=1:1->10:1
train:val=2:1
train:val=2:1->1:1
train:val=2:1->5:1
train:val=2:1->10:1
train:val=5:1
train:val=5:1->1:1
train:val=5:1->2:1
train:val=5:1->10:1
train:val=10:1
train:val=10:1->1:1
train:val=10:1->2:1
train:val=10:1->5:1
(b) Training Loss
Figure A3: Learning curves on (a) training accuracy & (b) training loss under diﬀerent
tuning schedules. Note that the training curves for STN is diﬀerent form standard training
since at the late stage of training, large amounts of data augmentations are added to avoid
overﬁtting.
Our proposed CE-GP-UCB ﬁnds an unique pattern that diﬀers from static
tuning schedules or switching schedules automatically.
signiﬁcantly worse on the training set (e.g., 60% to 80% training accuracy), the model still
performs well on the clean images without augmentations on the validation set and test set
(see Figure 3).
Figure A4 presents the hyperparameter schedule prescribed by the STN under diﬀerent
tuning schedules.
In general, the amount of noise added to the image increases as the
training proceeds to alleviate the eﬀects of overﬁtting. However, we found that CE-GP-
UCB results in a slightly diﬀerent pattern. Speciﬁcally, STN decreases the brightness at
16

0
25
50
75
100
125
150
Epoch
0
2
4
6
8
10
Amount of Noise
Hue
Contrast
Saturation
Brightness
Cutout length
Cutout holes
(a) train/val=1:1
0
25
50
75
100
125
150
Epoch
0
2
4
6
8
10
Amount of Noise
Hue
Contrast
Saturation
Brightness
Cutout length
Cutout holes
(b) train/val=10:1-¿1:1
0
25
50
75
100
125
150
Epoch
0
2
4
6
8
10
Amount of Noise
Hue
Contrast
Saturation
Brightness
Cutout length
Cutout holes
(c) CE-GP-UCB (ours)
Figure A4: The hyperparameter schedule prescribed by the STN (VGG16 on CIFAR-10).
the late training stage with CE-GP-UCB, however, it increases the brightness as other data
augmentations under static or switching tuning schedules.
B.3 Unsupervised Contrastive Representation Learning
Implementation Details for SimCLR
Following Chen et al. (2020), we take ResNet-
50 (He et al., 2016) as the encoder network, and a 2-layer MLP projection head to project the
representation. We trained SimCLR with 64 GPUs on ImageNet100 (Tian et al., 2019) (100
classes of ImageNet) for 200 epochs and set the batch-size as 64 × 56 = 3584. To stabilize
the training with large batch size, the LARS optimizer is adopted with the learning rate
4.8. For linear readout evaluation, a linear layer is trained from scratch for 10 epochs. We
adopt SGD optimizer with a learning rate of 10 and momentum 0.9 following Tian et al.
(2019).
Implementation Details for CE-GP-UCB
We integrate CE-GP-UCB algorithm into
the popular botorch library (Balandat et al., 2019) for higher eﬃciency and stable per-
formance. Speciﬁcally, we use Matern5/2 and set the forgetting rate as ϵ = 0.01. To ﬁnd
all the local optima, we randomly initialize 50 locations and use L-BFGS algorithm (Liu
and Nocedal, 1989) to ﬁnd the local optima. Furthermore, we use meanshift (Cheng, 1995)
algorithm to suppress close candidates with a bandwidth of 0.2. To avoid catastrophic per-
formance loss and misleading signals by some extremely biased decision, we clip the reward
signals between [−2, 2] and set the minimal probability to apply the cropping as 0.5.
17

