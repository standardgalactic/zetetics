Calibrated Language Models Must Hallucinate
Adam Tauman Kalai
Microsoft Research
Santosh S. Vempala
Georgia Tech
November 27, 2023
Abstract
Recent language models have a mysterious tendency to generate false but plausible-sounding
text. Such “hallucinations” are an obstacle to the usability of language-based AI systems and
can harm people who rely upon their outputs. This work shows shows that there is an inherent
statistical reason that pretrained language models hallucinate certain types of facts, having
nothing to do with the transformer LM architecture or data quality. For “arbitrary” facts whose
veracity cannot be determined from the training data, we show that hallucination is necessary
for language models that satisfy a statistical calibration condition appropriate for generative
language models. Specifically, if the maximum probability of any fact is bounded, we show that
the probability of generating a hallucination is close to the fraction of facts that occur exactly
once in the training data (a “Good-Turing” estimate), even assuming ideal training data without
errors.
One conclusion is that models pretrained to be sufficiently good predictors (i.e., calibrated)
may require post-training to mitigate hallucinations on the type of arbitrary facts that tend to
appear once in the training set. However, our analysis also suggests that there is no statistical
reason that pretraining will lead to hallucination on facts that tend to appear more than
once in the training data (like references to publications such as articles and books, whose
hallucinations have been particularly notable and problematic) or on systematic facts (like
arithmetic calculations). Therefore different architectures and learning algorithms may mitigate
these latter types of hallucinations.
1
Introduction
The mysterious tendency of Language Models (LMs) to generate false information, such as references
to non-existent article titles, has recently emerged as a critical issue. The popular term hallucination
is defined in the Merriam-Webster (2023) dictionary as “a plausible but false or misleading response
generated by an artificial intelligence algorithm.” In one case, lawyers were fined $5,000 for submitting
legal research containing hallucinated legal cases that they believed were correct (Shin, 2023). In
healthcare, hallucinations could be life threatening to patients and physicians are concerned about
malpractice cases (Mello and Guha, 2023). Furthermore, hallucinations have been widely reported
on by the media (Weise and Metz, 2023), and the U.S. President recently put out an Executive
Order calling for, among other things, safeguards against misleading outputs from generative AI
systems (Biden, 2023). This paper presents a statistical justification for LM hallucination.
A language model is simply a probability distribution D over sequences of tokens (e.g., words
or other character sequences). Every distribution D can equivalently be represented by its log-
probabilities over entire sequences or conditional log-probabilities of the subsequent token given
1
arXiv:2311.14648v1  [cs.CL]  24 Nov 2023

previous ones, log D(t1 . . . tm) = Pm
i=1 log D(ti | t1 . . . ti−1). This mathematical equivalence means
that any LM can be used either to generate text or predict the next token in naturally occurring
text conditioned on the previous tokens, with slightly different desiderata. For instance, consider
the sentence,
Alexa Wilkins had a tuna sandwich at Salumeria for lunch last Tuesday because the
reviews said that it was divine.
Sentences of this sort could be likely under a predictive LM, for example, to offer suggestions to
reduce typing on phones (e.g., Tanaka-Ishii, 2007). It may be desirable to predict sandwich as
an option of a word to type after the word tuna, along with other likely words such as salad and
roll. On the other hand, the vast majority of sentences of this sort would be false if randomly
fabricated by a generative LM. This paper shows LMs with good predictive text performance should
hallucinate, even under ideal conditions. Notably in the first stage of “pretraining,” common today,
the generative LM is optimized for predictive text performance (Chowdhery et al., 2022; OpenAI,
2023). Moreover, it gives a lower-bound on the rate of hallucination, which can shed light on the
different rates at which different types of facts should be hallucinated.
What is common to both the potential references and the example above (which we shall
refer to as 5W = Who-Ate-What-When-Where-Why factoids), above is that they are arbitrary
in the sense that neither one can be determined systematically by rules—one cannot determine
the veracity of most such facts that are not present in the training data. This in contrast to facts
whose veracity can be determined systematically. We quantify how much LMs should hallucinate
even in an simplified setting with several ideal properties. Because we are giving statistical lower-
bounds, we favor simplicity over generality as the point of our lower bounds is to identify a
fundamental cause of LM hallucination. Similar to classification, where one seeks a lower-bound
for the difficulty of classification in noiseless settings (but noise-tolerant classifications algorithms),
we seek a hallucination lower-bound that holds in the simplest setting where training data is i.i.d.
without factual errors.
Calibration for generative models.
Calibration is a natural requirement of a probabilistic
predictor meaning that its probabilities can be interpreted as accurate confidences in its own
predictions. Dawid (1982) introduced the notion with the example of a weather forecaster: among
days when they predict 30% chance of rain, it rains approximately 30% of the time. Calibration
metrics have been extensively studied for LMs (see, e.g., Braverman et al., 2020; Jiang et al., 2021;
Zhao et al., 2021). Fig. 1 illustrates multi-class calibration for GPT-4, a large modern LM, on a
multiple choice exam. Post-training alignment was applied to reduce hallucination (among other
factors) but was also found to reduce calibration (OpenAI, 2023). Calibration is both meaningful,
since a calibrated predictor’s probabilities are interpretable as accurate confidences, and statistically
achievable.1 In contrast, the perfectly accurate predictor would also be calibrated but may be
impossible to learn. However, calibration is only a minimal requirement for predictors, as not all
calibrated models are useful predictors: the predictor which always outputs the annual average
probability of rain is trivially calibrated.
We provide a natural generalization of calibration to generative models. Our notion differs from
prior uses of calibration in LMs which were at the token-level. The problem with analyzing raw
1Simple post-hoc probability modification procedures can calibrate any uncalibrated predictor and simultaneously
increase its accuracy under metrics such as cross-entropy (see, e.g., Braverman et al., 2020; B lasiok et al., 2023).
2

token probabilities is that there are many ways to describe any fact in natural language, and thus
having calibrated token probabilities is not particularly meaningful. To illustrate, consider the old
trigram LMs, which predict next-token probabilities based only on the previous two tokens (e.g.,
words). Trigram models are naturally calibrated at the token level, and yet hallucination was not a
major problem for trigram models. This is because they mostly generate gibberish. Instead, our
semantic-level calibration considers the probability distribution over pieces of information (facts or
hallucinations) contained in the text. We say an LM is calibrated if, for any probability z ∈[0, 1],
among the pieces of information it generates with probability ≈z, such information occurs on
average in a ≈z fraction of naturally occurring language (ideally the distribution from which
training data was drawn).
Why LMs hallucinate.
Hallucinations have mystified LM users and researchers alike. Section 2
surveys numerous hypotheses that have been studied for why LMs hallucinate, ranging from inaccu-
rate or outdated training data to the next-token log-likelihood objective in training. Hallucination
can also be due to an adversarial or out-of-distribution prompt: a textual prefix provided for the
LM to complete which establishes context. For example, there may be no factual completion to
a fabricated prompt such as, The 15 Elves of San Salami are named. . . .2 In contrast, our work
shows that even in an ideal world with perfect training data and no prompts, one should expect
hallucinations from LMs which are calibrated.
Simplified setting.
We consider a stationary language distribution DL ∈∆(X) over documents
(i.e., strings of text) x ∈X, and a learning algorithm A which takes training data xtrain ∈Xn
consisting of n documents sampled independently from DL, and outputs an LM, i.e., a distribution
DLM := A(xtrain) ∈∆(X). For simplicity, we assume that there are only facts in the training data,
and at most one per document, i.e., no training hallucinations. We focus on arbitrary facts such
as the above examples, whose truth is usually undetermined from the training set, rather than
systematic facts such as 572 < 120523 predictable from a training set by learning the underlying
rules governing correctness. There is no statistical reason that LMs should hallucinate on systematic
facts. Additionally, mistakes on systematic facts may not be considered hallucinations at all—they
are often categorized as reasoning or arithmetic errors.
We assume that each document x ∈X contains at most one factoid f(x) ∈Y , where factoids
are arbitrary pieces of information which are each either true (facts) or false (hallucinations) and
whose truth is statistically hard to determine from the training data. We also simplify matters by
considering unconditional generation (e.g., Tan et al., 2021) in which the LM is sampled to generate
text without any prompt (equivalently, the empty-string prefix). Again, compared to our simplified
setting, hallucination may be even more likely in the more realistic case where the LM is prompted
with contexts that come from a different distribution than the training data.
Results.
Consider n i.i.d. samples from an unknown distribution p over a large number of arbitrary
factoids, such as the 5W example and references. The missing mass, or in our case missing facts
p(U), is the fraction of future samples from this fact distribution p that were not observed in the
n training samples, where U is the subset of facts that were unobserved in training data. The
2A completion such as . . . actually never mind, I have no idea is unlikely given training data that does not have
such retractions.
3

Good-Turing estimate of the missing mass (Good, 1953) is the fraction of samples (in our case facts)
that appear exactly once in the training data. In our context, we call this the MonoFacts estimator,
d
MF := Number of facts appearing exactly once in training data
n
.
The Good-Turing estimator was shown to be within |p(U) −d
MF| = ˜O(
p
1/n) of the missing
mass, with high probability (McAllester and Schapire, 2000; McAllester and Ortiz, 2003), for any
distribution p.
If the correctness of arbitrary factoids not contained in the training cannot be determined, the
missing fact rate can provide a lower-bound on the rate of hallucination. This in turn gives us a
lower-bound close to d
MF. In particular, under a “regularity” assumption on the factoid distribution,
our simplest bound (Corollary 1) implies that, for any algorithm, with probability ≥99% over
training sets:
Hallucination rate ≥d
MF −Miscalibration −
300|Facts|
|Possible hallucinations| −7
√n,
where the “Hallucination rate” is the rate at which the LM generates hallucinations, the next term
is “monofact” estimator of the missing facts. The subsequent term is a “miscalibration rate” which
quantifies how close to calibration the distribution is. We also have a term involving the ratio of
the number of arbitrary facts to similar pieces of information that are false, which is exponentially
small for many types of information. The final 6/√n term is small for the large training set sizes n
used in today’s LM. The regularity assumption means that all unobserved factoids have, on average,
equal probability of being true. More generally, the bound holds with probability ≥1 −δ where the
constant 60 can be replaced by a term which is inversely proportional to δ and proportional to a
regularity term on the distribution of factoids. The regularity term measures the ratio of the most
likely factoid (that was not observed in training data) to the average unobserved factoid probability.
This constant is 1 for symmetric distributions and other types of simple distributions. We relax it
to consider bounded regularity, which permits there to be certain negative correlations such as the
fact that a person does not eat 1000 lunches in 1000 different places on the same day and allows for
some factoids to have conditional probability 0, but it prohibits unobserved factoids from having
very large probability.
Interpretation.
Our lower-bounds have the following interpretation. First, one should identify
a large set of factoids: arbitrary, plausible, regular factoids. These could be posts about 5W and
plausible research article citations. Intuitively, there are exponentially more plausible factoids that
are incorrect (which we call hallucinations), than those that are facts. One can then consider what
fraction of such factoids might appear exactly once in the training data. In the case of 5W, one
could imagine that half of the posts occur exactly once. This would suggest that a calibrated-factoid
model would hallucinate on about half of its generations on 5W factoids. On the other hand, one
could imagine that there are many many fewer than n articles and since the goal of publication is
advertising, each reference might be expected to appear multiple times in the training data (i.e.,
have probability ≫1/n) except for perhaps very recent publications (e.g., within the last day before
they other references begin to appear). This would suggest that the missing mass for articles is
low and that there is no inherent statistical necessity to hallucinate reference titles. There may
be other causes of such hallucinations such as limited model capacity (even if the number of LM
4

0.0
0.2
0.4
0.6
0.8
1.0
P(answer)
0.0
0.2
0.4
0.6
0.8
1.0
P(correct)
ECE: 0.007
Calibration curve (model=pre-train)
0.0
0.2
0.4
0.6
0.8
1.0
P(answer)
0.0
0.2
0.4
0.6
0.8
1.0
P(correct)
ECE: 0.074
Calibration curve (model=ppo)
Figure 1: GPT-4 calibration curves before (left) and after (right) reinforcement learning (OpenAI,
2023, Figure 8, reprinted with permission). As suggested by our model, post-training reduces
hallucination rates at the cost of increasing calibration error. Note that calibration here is on a
multiple-choice test rather than generative calibration which we study.
parameters is much greater than the number of articles, these parameters must encode many types
of information beyond article titles). This also justifies the mitigation of consulting a fact database
at generation time, even if that fact database is constructed solely from training data (Borgeaud
et al., 2022).
Despite this tension between factuality and predictive accuracy, the parameters of both types
of LMs are typically trained or “pretrained” to maximize likelihood on a corpus, or equivalently
to minimize the “KL divergence”, a strong statistical discrepancy metric between the LM and the
distribution of data on which it was trained.
Organization.
Following related work discussed in Section 2, Section 3 gives mathematical
preliminaries including our definition of (mis)calibration. Section 4 defines our factoid model and
Section 5 states the main results. Sections Section 6 proves the main lemma. Section 7 provides
a simple upper bound showing that one cannot guarantee a hallucination rate greater than the
monofact estimate for nearly calibrated estimators. Section 8 concludes and discusses limitations
and future work. Appendix A in the appendix presents bounds on Good-Turing estimators from
prior work. Appendix B discusses extensions to alternative models such as calibration using bins of
equal width, those based on raw accuracy (cross-entropy), or those using using prompts. Appendix C
gives additional proofs.
2
Related work
As discussed in the introduction, the concept of calibration was introduced by Dawid (1982) and
has been extensively studied in statistics and machine learning and even specifically for LMs
(Braverman et al., 2020; Zhao et al., 2021; Jiang et al., 2021) and other related fields of deep learning
5

and generative AI (Rohrbach et al., 2018; Maynez et al., 2020). B lasiok et al. (2023) argue that
calibration happens naturally as a byproduct of minimizing log-loss for deep neural networks, though
their results are for a different architecture and different notion of calibration.
Unfortunately, there is not clear agreement on what counts as a hallucination. This is why we
consider an idealized model in which there are clear-cut facts, where statements that violate these
facts would generally be categorized as hallucinations by most definitions.
Open- vs. closed-domain hallucinations.
Interestingly, many studies focus on hallucination
with respect to a specific source document that is given to an LM, such as in translation (Xu et al.,
2023) or summarization (Maynez et al., 2020). There, LMs are also found to fabricate facts not
present in the source document even when instructed to use only information present in the source.
This is referred to as closed-domain hallucination in contrast to our open-domain setting, where
the LM generates hallucinations which are not factually grounded in its training data. There is
no clear statistical argument for why closed-domain hallucinations must occur, since if one can
verify such hallucinations from the source text and generation one can avoid them by filtering out
generations with such mistakes. Consistent with this, (OpenAI, 2023) reports a greater reduction in
closed-domain hallucinations over open-domain ones.
Honesty vs. factuality.
Evans et al. (2021) points out that there is a difference between factuality
and truthfulness (i.e., honesty). An LM which states something that disagrees with its training
data may be said to lie, whereas factuality may be much harder to determine for a variety of
reasons, including the possibility that what is considered factual may change as scientists make
new discoveries and rebuke old theories. Nonetheless, in worlds where there is an absolute notion
of facts, and the training data only contain facts, then any falsehood is also untruthful. Thus in
ideal worlds where training data is perfectly consistent with an internally consistent ground-truth
notion of facts, our bounds on non-factuality directly imply bounds on the rate at which LMs must
generate untruthful information or “lie.”
Hypotheses for why LMs hallucinate.
There have been many explanations of why LMs
hallucinate. The primary reason proposed for hallucinations is inadequacies in training data. This
can be broken into two issues. First are falsehoods contained in the training data (Ji et al., 2023;
Lin et al., 2022; Dziri et al., 2022; Filippova, 2020), which Lin et al. (2022) refer to as “imitative
falsehoods.” Second is the temporal nature of data, i.e., the fact that training data is no longer
relevant and is missing current events (Aksitov et al., 2023; Vu et al., 2023). While both of these are
certainly factors in hallucination, our work shows that it is not the only cause of LM hallucinations.
An additional reason given for why LMs may hallucinate is the fact that they are trained to
produce tokens one at a time may lead to hallucination (Zhang et al., 2023b) because the LM
may generate a plausible-sounding sequence of tokens which is impossible to complete factually.
While this may be true for transformers due to computational limitations, this is not simply a
statistical byproduct of their being trained on the next-token prediction task. Since document
log-likelihood is simply the sum of next-token log-likelihood, the two objectives are identical and
thus from a statistical perspective this is not a factor. Mathematically, any probability distribution
over documents can be represented as a conditional next-token probability distribution.
Another line of work that sheds light on the nature of hallucinations shows that LMs know
when they’re hallucinating (Kadavath et al., 2022; Azaria and Mitchell, 2023; Agrawal et al., 2023;
6

Manakul et al., 2023). Various techniques may be used to identify LM hallucinations purely from
the LM itself, either the internal weights, its token probabilities, or by querying it with additional
questions. This is consistent with our work in the sense that one would expect even an ideal
“super-inelligent” model should hallucinate if its goal is predictive accuracy.
A bevy of additional reasons have been proposed and studied for why LMs may hallucinate.
These fall under headings such as duplicates in training data, biases, architecture, overconfidence
and various types of knowledge failures, among others. A complete listing of these is beyond this
scope of this work. For further details, see the recent surveys of Huang et al. (2023); Zhang et al.
(2023a); Ji et al. (2023).
3
Mathematical Preliminaries
We first define preliminary notation and concepts, including statistical distance and our notion of
generative calibration.
3.1
Standard definitions and notation
Let ∆(S) denote the set of probability distributions over S. For distribution D ∈∆(S) and R ⊆S,
let D(R) := P
x∈R D(x). The Total Variation distance (TV) (also called statistical distance) between
distributions D, D′ ∈∆(S) has multiple equivalent definitions (for finite or countably infinite sets
S):
∥D −D′∥TV := max
R⊆S |D(R) −D′(R)| = 1
2
X
x∈S
|D(x) −D′(x)| =
X
x∈S
 D(x) −D′(x)

+ ,
(1)
where (z)+ = max(0, z) for z ∈R. It satisfies ∥D −D′∥TV = ∥D′ −D∥TV ∈[0, 1]. The support of
distribution D is supp(D) = {x ∈S | D(x) > 0}, as long as S is finite or countably infinite. Let the
set of partitions of set S be denoted by P(S) := {Π ⊆2S | ∀x ∈S |{B ∈Π | x ∈B}| = 1}. For a
function f : X →Y and set S ⊆X, let f(S) := {f(x) | x ∈S}. For a distribution D ∈∆(X), let
f ◦D denote the induced distribution of f(x) for x ∼D, i.e., f(y) := P
x:f(x)=y D(x). Finally, let
D×n denote the distribution over n independent samples from D.
3.2
Generative (mis)calibration
This section defines a notion of miscalibration Misb(p, g) ∈[0, 1] for a generative model g that
measures how accurate its own probabilities are with respect to future examples drawn from a
given pretraining distribution p, using b ≥1 bins. It is a natural extension of existing notions of
calibration to generative models, and can be skipped on first read for those who want to get straight
to the model. Appendix B discusses the relationship between this and existing notions of calibrated
classifiers. The definitions in this section apply to any distributions p, g ∈∆(Y ) for any finite set
Y . In other words, there is only assumed to be a generative distribution g over information y ∈Y ,
and calibration is with respect to a “true” distribution p. Finiteness of Y is only assumed at this
point to avoid measure-theoretic technicalities. We first define a calibrated distribution g as any
coarsening of p.
7

Definition 1 (Coarsening and calibration). For partition Π ∈P(Y ) and D ∈∆(Y ), DΠ ∈∆(Y ) is
the Π-coarsening of D if,
∀B ∈Π ∀y ∈B
DΠ(y) = D(B)
|B| .
Distribution g is said to be calibrated to p if g = pΠ for some partition Π.
Clearly g = p is calibrated to p, and so is the uniform distribution g(y) = u(y) := 1/|Y |. To
define miscalibration, let Bg
z := {y ∈Y | g(y) = z} and omit g, writing Bz = Bg
z when clear from
context. It is also clear that g is calibrated to p iff g = pB(g) for partition B(g) := {Bz | z ∈[0, 1]}
and thus a natural definition of miscalibration (which is 0 iff g is calibrated to p) is:
Mis∞(g, p) :=
pB(g) −g

TV = 1
2
X
B∈B(g)
X
y∈B

p(B)
B
−g(y)
 .
(2)
The ∞in the above notation refers to the fact that there is no limit on the number of bins. This
also explains why it is called “calibration”: the average probability over each bin that shares a
common value of g(y) is g(y). This can be written as,
∀z ∈[0, 1]
E
y∼g

p(y) | g(y) = z

= z.
(3)
We next generalize B(g) to intervals. For I ⊆[0, 1], define:
BI = Bg
I := {y ∈Y | g(y) ∈I}.
The definition below uses b ≥1 adaptively sized bins which each have roughly equal probability
mass in terms of generations y ∼g. Appendix B.3 gives an alternate definition using bins of equal
width in terms of log-probability.
Definition 2 (Miscalibration). Let b ∈N and define the adaptive partition,
Vb(g) :=

B[0,t1], B(t1,t2], . . . , B(tb−1,tb]
	
where ti = sup


z ∈[0, 1]

X
y:g(y)≤z
g(y) ≤i/b


.
The miscalibration of g with respect to p is Misb(g, p) :=
pVb(g) −g

TV.
Adaptive binning has been used in supervised classification (e.g., Nixon et al., 2020). Note
that Mis1(g, p) = ∥g −u∥TV is the total variation to the uniform distribution, which shows that
Misb(g, p) is not monotonic in b because b = 1 is the minimum for g = u (regardless of p) while
b = ∞minimizes Misb(p, p) = 0 for g = p. Also, Misb(g, p) = Mis∞(g, p) for b is large enough that
1/b ≤miny∈supp(g) g(y). Finally note that necessarily tb = 1 in the above definition.
Advantages and limitations.
An advantage of semantic-level calibration is that it naturally
models the exponentially many ways there are to describe any given fact, unlike token-level calibration
models. This is also a disadvantage, however, because it means that it may be intractable to measure
calibration rates, and thus experimental validation may be easier in synthetic settings where facts
have canonical descriptions. One nice property of the above adaptive binning is that Misb(g, p) = 0
iff g is calibrated to p, regardless of b, whereas other definitions give 0 miscalibration whenever
there is a single bin. Nonetheless, Appendix B.3 shows how our analysis applies to other binning
strategies.
8

4
The model and guarantees
Our notation is summarized in Table 1. Let Σ denote the set of tokens and Σ∗denote the set of
finite token sequences, i.e., strings. Let X ⊆Σ∗denote the set of documents (this could be finite,
e.g., if a length restriction is imposed or countably infinite if X = Σ∗).
The world is assumed to contain randomness, which is modeled by a distribution Dworld ∈
∆(∆(X)) over language (document) distributions DL ∼Dworld. (More generally, a full world model
would contain lot of other information but language distributions suffices for our purposes.) The
training data consists of n documents xtrain ∼D×n
L
sampled independently from this distribution.
It will be convenient to denote by Dtrain ∈∆(Xn) the distribution over training data, where the
probability of any training document is:
Dtrain(xtrain) :=
E
DL∼Dworld
" n
Y
i=1
DL
 x(i)
train

#
.
This model requires a static world that does not change. Of course, real-world distributions are
non-stationary and dynamic, and real-world training data contains duplicates and is not i.i.d.
However, our lower bounds imply hallucination even in this ideal static setting.
4.1
Factoid assumptions
We assume there is a large but finite set Y of “factoids” by which we mean arbitrary plausible
pieces of information, each of which could be true (facts) or false (hallucinations), as determined by
some world distribution. Their arbitrary nature means that, given the facts observed in training
data, one cannot infer any likely facts among the unobserved factoids. Of course, many real-world
facts are systematic, not arbitrary. For instance, mathematical inequalities such as 17 < 252395
are systematic and should thus not be included in Y . Note that Y is not intended to capture all
world facts but rather a large set of arbitrary ones. It could contain the 5W factoids (except for
people who eat the same lunch every day in the same location, as their eating behaviors are too
systematic). There is no statistical reason an LM must hallucinate on systematic facts.
We will make a few assumptions about factoids.
1. One-per-doc: First, we assume that there is at most one factoid per document by defining
a surjective function f : X ↠Y which extracts a single factoid with each document, where
f(x) = ⊥represents the empty fact (to allow for documents with no facts) and assume
⊥∈Y . This makes the notation simpler as one can define the induced factoid distribution
p = f ◦DL ∈∆(Y ) defined by p(y) := P
x:f(x)=y DL(x). Similarly, we can take g = f ◦DLM
to be the induced distribution over generated factoids, where DLM ∈∆(X) is the distribution
over documents generated by the LM. The surjective requirement simply means that every
factoid is describable by some document x ∈X, and if this didn’t hold one could always
shrink Y . Our model does permit many documents to encode the same factoid, since there are
typically many ways to describe any given piece of information. Again, this assumption may
be generalized to a model where documents contain sets of factoids with further notation, but
we show that calibrated LMs hallucinate even in a simple idealized setting with one factoid
per document.
2. Good-training-data: Second, we assume that the set of facts F := supp(p) ∪{⊥} where
supp(p) = {y ∈Y | p(y) > 0} = f(supp(DL)), i.e., the training data consists solely of
9

facts and every fact has non-zero probability of being described under DL. Both of these
assumptions can be removed at the cost of additional notation without changing the results
in the slightest—the world distribution would need to determine an arbitrary F ⊆Y and
DL ∈∆(Y ). Keeping in the spirit of the ideal training data model, we choose to simplify
notation and take F := supp(p) ∪{⊥}. The set of hallucinations is H := Y \ F, i.e., every
non-fact is defined to be a hallucination.
3. More-false-than-true: Third, Assumption 1 below requires that there are many more
falsehoods in Y than facts, which makes sense for many types of information. For example,
consider those factoids which describe a paper citation including the paper title, authors,
and further details. There are vastly more plausible citations than existing ones. In this
case, one may choose to include in Y a somewhat smaller sparse set, i.e., not include author
middle names or other minor variations in Y which would make a reference in the “grey area”
between fact and hallucination. Similarly, for 5W factoids, there are many more combinations
of people who did not eat a given food on at a given time than people who did.
Assumption 1 (s-Sparse facts). There are many fewer facts than hallucinations. Specifically,
there exists s ∈R such that, with probability 1 over DL ∼Dworld:
|F| ≤e−s|H|.
Recall that F := f(supp(DL)) ∪{⊥} and H = Y \ F.
We write sparsity as an exponential to reflect the general exponentially nature of natural
language facts.
4. (Semi)Regularity: Finally, and most importantly, we assume that no single factoid is
very likely. Specifically, perfect regularity requires that after observing the training data, all
unobserved factoids are equally likely to appear as facts in the language distribution, and we
also provide a relaxed r-regular notion, which we refer to as a “semi-regularity” assumption.
Definition 3 (Regular facts). Dworld has regular facts if for all xtrain ∈supp(Dtrain):
∀y, y′ ∈U
Pr[y ∈F | xtrain] = Pr[y′ ∈F | xtrain].
For r ≥1, Dworld has r-regular-facts if for all xtrain ∈supp(Dtrain),
∀y ∈U
Pr[y ∈F | xtrain] ≤
r
|U| E

|F ∩U|
 xtrain

.
Having regular facts will suffice to prove the lower bound Corollary 2, but stronger lower
bounds will be possible if we also have regular probabilities.
Definition 4 (Regular probabilities). Dworld has regular probabilities if for all xtrain ∈
supp(Dtrain):
∀y, y′ ∈U
E[p(y) | xtrain] = E[p(y′) | xtrain].
For r ≥1, Dworld has r-regular-probabilities if for all xtrain ∈supp(Dtrain),
∀y ∈U
E[p(y) | xtrain] ≤
r
|U| E[p(U) | xtrain].
10

Symbol
Meaning
X ⊆Σ∗
The set of documents (strings)
Y
Set of factoids, arbitrary plausible pieces of information
⊥∈Y
Special ”empty string” fact representing
f : X ↠Y
Each document x contains a single factoid f(x), and f(X) = Y
Dworld ∈∆(∆(X))
Distribution over document distributions
DL ∈∆(X)
Language distribution over documents DL ∼Dworld
xtrain ∈Xn
Training data (i.i.d.) xtrain =

x(1)
train, x(2)
train, . . . , x(n)
train

∼D×n
L
A : Xn →∆(X)
Algorithm that learns an LM from training data
p ∈∆(Y )
Distribution over factoids f(x) for documents x ∼DL, i.e., p := f ◦DL
F ⊆Y
Facts F := supp(p) ∪{⊥}, non-hallucinations
H ⊆Y
Hallucinations H := Y \ F
Dtrain ∈∆(Xn)
Distribution over xtrain ∼D×n
L
induced by DL ∼Dworld.
O ⊆Y
Set of observed factoids O :=

f
 x(i)
train

| i = 1, 2, . . . , n
	
∪{⊥} ⊆F
U ⊆Y
Unobserved factoids U := Y \ O ⊇H
ν
Posterior over p given training data xtrain
DLM ∈∆(X)
Distribution over documents generated by the LM, DLM := A(xtrain)
g ∈∆(Y )
Distribution over factoids f(x) for documents x ∼DLM, i.e., g := f ◦DLM
Table 1: Summary of notation. Symbols below the line are all derived quantities in terms of symbols
above the line.
Finally, we say Dworld is regular if it has regular facts and regular probabilities. It is not difficult
to see that a distribution is regular iff it has 1-regular-facts and 1-regular-probabilities. We
also note that our regularity assumptions could be modified to only holds with high-probability
over xtrain and not for all xtrain ∈supp(Dtrain).
We illustrate a simple family of regular world distributions which satisfy our assumptions,
followed by one which is only r-regular, does not have independence, and has anti-correlations
between facts.
Regular example: Permuted power-law world.
Suppose X = Y and f is the identity. The
world distribution Dworld first picks F ⊆Y uniformly random over such that |F| = N (where
N ≤|Y |/(1 + es) so that |F|/|H| ≤e−s so facts are s-sparse) and then picks p to be a power-
law distribution supported on a random ordering on F. That is, it picks a random permutation
σ : {1, 2, . . . , N} ,→F and defines p so that p(σ(i)) ∝i−k for some constant k ≥0. This includes
the uniform distribution over F (k = 0) and Zipfian distributions (k = 1) as special cases.
Semi-regular example: W5 with negative correlations.
The set of factoids is the product
of fixed sets of people, dates, foods, and locations. Dworld chooses the set of facts F randomly by:
for each person on each date, there is a single fact which consists of that person eating a random
food at a random location, and p is uniform over F. This creates anti-correlations between factoids
because the knowledge about what and where a person ate a specific meal rules out any possible
alternatives for that meal. Nonetheless, it can be seen that r-regularity holds for r ≤npeoplendates.
11

The above model can be enriched in various ways by adding reasons and by modeling people’s
preferences, geographic constraints, and behaviors. However the regularity parameter r will be
prohibitively large if there are predictable eaters, and indeed LMs might hallucinate less if there are
large numbers of predictable eaters because an LM learning algorithm may learn these patterns and
hallucinate less often.
5
Guarantees
Our results are stated in terms of missing facts, a term inspired by the missing mass in data
drawn from a probability distribution (Good, 1953). The missing facts rate is the fraction of
facts (according to the pretraining fact distribution p) that were not observed in the training data.
Specifically, it is defined to be p(U) where U is the set of unobserved facts in the training data.
Formally, define the observed and unobserved factoids as,
O = Oxtrain :=
n
f
 x(i)
train
  i = 1, 2, . . . , n
o
∪{⊥} ⊆Y,
U = Uxtrain := Y \ Oxtrain,
(4)
respectively. The monofact estimate of the missing fact rate is defined to be the fraction of facts
that appear exactly once in the training data:
d
MF = d
MF xtrain :=

y ∈Y \ {⊥} | y = f
 x(i)
train

for exactly one i ∈{1, 2, . . . n}
	
n
.
(5)
Note that the facts in the training data are distributed according to the distribution p. Appendix A
states classical results asserting that | d
MF −p(U)| = ˜O(
p
1/n) with high probability over n samples
from any distribution p.
An algorithm A : Xn →∆(X) takes as input n training documents and outputs a document
distribution DLM = A(xtrain), which determines g = f ◦DLM, i.e., f(x) for x ∼DLM. We now
state our first result, which relies on a regular Dworld defined in Definitions 3 and 4 above.
Corollary 1. Fix any δ ∈[0, 1], b, n ∈N, s ∈R and any s-sparse regular Dworld. Then for any
algorithm A : Xn →∆(X), with probability ≥1 −δ over DL ∼Dworld and xtrain ∼D×n
L ,
g(H) ≥d
MF −Misb(g, p) −3e−s
δ
−
r
6 ln(6/δ)
n
,
where DLM = A(xtrain), g(H) is the LM hallucination rate, and d
MF is defined in Eq. (5).
Now, we can state a weaker guarantee for semi-regular facts alone.
Corollary 2. Fix any δ ∈[0, 1], b, n ∈N, r, s ∈R and any s-sparse Dworld with r-regular-facts. Then
for any algorithm A : Xn →∆(X), with probability ≥1 −δ over DL ∼Dworld and xtrain ∼D×n
L ,
g(H) ≥d
MF −Misb(g, p) −3rne−s
δ
−
r
6 ln(6/δ)
n
.
The above is meaningful when sparsity s ≫log n is larger than the log of the number of training
data. Otherwise, following bound uses semi-regularity of facts and probabilities.
12

Corollary 3. Fix any δ ∈[0, 1], b, n ∈N, r, s ∈R and any s-sparse Dworld with r-arbitrary-facts
and r-arbitrary-probabilities. Then for any algorithm A : Xn →∆(X), with probability ≥1 −δ over
DL ∼Dworld and xtrain ∼D×n
L ,
g(H) ≥d
MF −Misb(g, p) −3re−s
δ
−
r
6 ln(6/δ)
n
.
It is easy to see that Corollary 1 is a special case of this corollary for r = 1.
Analysis approach.
While our model supposes DL ∼Dworld followed later by xtrain ∼D×n
L ,
for analysis purposes we imagine first selecting xtrain ∼Dtrain and then selecting p ∼νxtrain where
ν = νxtrain is defined to be the posterior distribution on p given xtrain. These two procedures result
in identical joint distributions on p, xtrain, but the latter is easier to analyze. In particular, we show:
Theorem 1. For any ν ∈∆(∆(Y )), any O ⊆F ⊆Y , any g ∈∆(Y ), and any partition Π ∈P(Y ),
E
p∼ν
h p(U) −
pΠ −g

TV −g(H)

+
i
≤max
y∈U Pr
p∼ν[y ∈F] + |O| max
y∈U E
ν [p(y)],
where H := Y \ F and U := Y \ O.
The corollaries stated earlier follow directly from this theorem together with the definition of
Misb(g, p) :=
pVb(g) −g

TV, Markov’s inequality for a non-negative random variable to show that
the quantity in the expectation is small with high probability, which we combine with existing
bounds on the Good-Turing estimator from Appendix A that show that | d
MF −p(U)| = ˜O(
p
1/n).
The quantities on the right hand side correspond to the arbitrary facts and arbitrary probability
notions. The above theorem is enough to show the result for various binning strategies, such as
fixed-width, which depend arbitrarily on g (but not on p).
6
Proof of Theorem 1
This section proves Theorem 1.
Lemma 2. Let S ⊆Y and let pΠ be any coarsening of p. Then,
E
ν
h p(S) −pΠ(S)

+
i
≤|Y \ S| · max
y∈S E
ν [p(y)].
13

Proof. Suppose q = pΠ for some partition Π ∈P(Y ). Then,
p(S) −q(S) =
X
B∈Π
p(S ∩B) −q(S ∩B)
=
X
B
p(S ∩B) −p(B)
|B| |S ∩B|
≤
X
B
p(S ∩B) −p(S ∩B)
|B|
|S ∩B|
=
X
B
p(S ∩B)|B| −|S ∩B|
|B|
=
X
B
p(S ∩B)|B \ S|
|B|
≤
X
B
p(S ∩B)
|S ∩B| |B \ S|.
Since a ≤b ⇒(a)+ ≤(b)+ and this last quantity is non-negative,
(p(S) −q(S))+ ≤
X
B
p(S ∩B)
|S ∩B| |B \ S|
E
ν

(p(S) −q(S))+

≤
X
B
E
ν
p(S ∩B)
|S ∩B|

|B \ S|
≤
X
B
|B \ S| max
y∈S E
ν [p(y)]
= |Y \ S| max
y∈S E
ν [p(y)].
We are no ready to prove Theorem 1.
Proof of Theorem 1. Let q = pΠ. By the definition of TV,
g(U) ≥q(U) −∥q −g∥TV .
(6)
Since Y \ F = H ⊆U, we have H = U \ (F ∩U) and,
g(H) = g(U) −g(F ∩U)
≥q(U) −∥q −g∥TV −g(F ∩U)
by Eq. (6)
= p(U) −(p(U) −q(U)) −∥q −g∥TV −g(F ∩U)
= p(U) −∥q −g∥TV −
 p(U) −q(U) + g(F ∩U)

.
Rearranging terms gives p(U) −g(H) −∥q −g∥TV ≤p(U) −q(U) + g(F ∩U). Applying a ≤b ⇒
(a)+ ≤(b)+,
(p(U) −g(H) −∥q −g∥TV)+ ≤(p(U) −q(U) + g(F ∩U))+ ≤(p(U) −q(U))+ + g(F ∩U).
14

Thus, it suffices to prove:
E
ν
h
g(F ∩U) +
 p(U) −q(U)

+
i
≤max
y∈U Pr
ν [y ∈F] + |O| max
y∈U E
ν [p(y)].
(7)
To this end, linearity of expectation implies that,
E
ν [g(F ∩U)] =
X
y∈U
g(y) Pr
ν [y ∈F] ≤max
y∈U Pr
ν [y ∈F].
(8)
By Lemma 2 (with S = U),
E
ν
h p(U) −q(U)

+
i
≤|O| max
y∈U E
ν [p(y)].
Combining this with Eq. (8) gives Eq. (7), as required.
7
Upper bounds on hallucination rate
Could one prove a much better lower bound than d
MF? If not in general, are there better lower
bounds under various assumptions on Dworld? In this section, we argue that a significantly better
lower-bound is not possible by showing that for any Dworld, there is a algorithm that is calibrated
and hallucinates at a rate near the missing facts rate, equivalent its Good-Turing estimate. The
conceptually simple algorithm is neither efficient nor a good LM, but it suffices to show that a
better lower-bound is not possible.
The algorithm is as follows.
1. Inputs: xtrain ∈Xn.
2. Let O, U = Y \ O be the sets of observed and unobserved factoids in the training data,
respectively, as defined in Eq. (4) and compute d
MF, the fraction of factoids that appear
exactly once in the training data, as defined in Eq. (5). Let g ∈∆(Y ) be defined as
g(y) :=



d
MF
|U|
if y ∈U,
1−d
MF
|O|
if y ∈O.
3. For each factoid y ∈Y , select a corresponding document d(y) ∈X such that f(d(y)) = y. To
be specific, one can take d(y) to be the lexicographically first document in {x ∈X | f(x) = y}.
4. Output DLM = d ◦g, i.e., the distribution which samples y ∼g and then outputs d(y).
It is easy to see that, by design, g = f ◦DLM.
Lemma 3. For any δ, λ ∈[0, 1], Dworld, n ≥1,
Pr
xtrain∼Dtrain
"
g(H) ≤d
MF and Mis∞(g, p) ≤3
r
ln(4/δ)
n
#
≥1 −δ,
where g = f ◦DLM for the above algorithm, g(H) is its hallucination rate, and Mis∞(g, p) is defined
in Eq. (2).
15

In other words, with high probability one can have a well-calibrated LM that hallucinates with
probability close to d
MF.
Proof. There can either be one or two bins Bg
z based on whether or not
d
MF
|U| = 1−d
MF
|O| . If they are
equal then Mis∞(g, p) = 0. In any case,
Mis∞(g, p) ≤1
2|p(U) −g(U)| + 1
2|p(O) −g(O)| = |p(U) −g(U)| = |p(U) −d
MF|.
Since d
MF is a Good-Turing estimator, by Corollary 5, with probability ≥1 −δ the above quantity
is at most 3
q
ln(4/δ)
n
. At the same time, with certainty,
g(H) =
d
MF
|U| |H ∩U| ≤d
MF.
8
Conclusions, limitations, and future work
When one first encounters LM hallucinations, it is perhaps surprising that a system which clearly
embeds such a rich diverse array of detailed knowledge at the same time creates complete fabrications
with no basis in fact or the training data. This work aims to demystify this phenomenon by showing
that pretraining LMs for predictive accuracy leads to hallucination even in an ideal world where the
training data is perfectly factual, there is no blur between facts and hallucinations, each document
contains at most one fact, and there is not even a prompt that would encourage hallucination.
Moreover, our theory explains why modern LMs hallucinate more than older LMs such as trigram
models, even though both are trained on similar types of data with similar objectives.
The monofact rate may shed light on the rates at which calibrated LMs must hallucinate for
different types of facts. One expects hallucination for facts that have a high monofact rate, i.e.,
the types of facts which often occur just once in the training data. Interestingly, this would not
be common for references to books or articles, a problematic type of hallucination discussed today.
Therefore, these may arise from other issues such as model capacity, when one considers the shear
number of facts including references and others that an LM encounters in training. Furthermore,
correcting for hallucinated references may be doable by modifying the pre-training pipeline without
post-training, though this will not address other types of arbitrary facts where the monofacts are
common, as in our 5W example.
There are several limitations to our work.
First, we only study one statistical source of
hallucination. There are many other types of hallucination and reasons LMs may hallucinate beyond
pure statistics. Second, our semantic notion of calibration is different from the standard token-based
ones used in classification. While natural and simple to define, our notion has the disadvantage
of being computationally intractable to evaluate for many models. Third, factuality is not always
clear-cut, facts are not all disjoint, and our regularity assumptions may not hold for facts that have
a mild systematic component. As an example, if the training data contains the Alex Wilkins 5W
fact from the introduction, then it is also follows that Alex Wilkins has eaten at Salumeria at some
point, which is a different but overlapping fact. Finally, it could be the case that aspects of the real
world, messy and different from our idealized setting, actually reduce the minimal hallucination
16

rates. For instance, it could be that having multiple facts in a document makes models less likely to
hallucinate and thus our lower bounds do not apply.
In future work, it would be interesting to use the insights presented here to further reduce
hallucination in LMs. An interesting question is how to convert a pretrained (calibrated) model to
one that is good at factual prediction. A step in this process may be to distinguish systematic facts
from arbitrary ones, which LMs may be capable of at some point in the future if not today. For
example, for generation, one would not desire fabricated book titles, but one would like mathematical
statements. What is the difference between fabricating a non-existent book title from generating
an inequality such as 17 < 124398, if neither has ever been written down? Humans know that the
latter can be manufactured (as long as it is mathematically correct) while the former cannot, which
presumably is how we avoid hallucinating. It seems conceivable that LMs can similarly represent
this distinction, and the work mentioned showing that LMs “know” when they are hallucinating
suggests that this may indeed be possible with today’s LMs.
References
Ayush Agrawal, Mirac Suzgun, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language
Models Know When They’re Hallucinating References?
http://arxiv.org/abs/2305.18248
arXiv:2305.18248 [cs].
Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yunhsuan Sung. 2023.
Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language
Models.
http://arxiv.org/abs/2302.05578 arXiv:2302.05578 [cs].
Amos Azaria and Tom Mitchell. 2023. The Internal State of an LLM Knows When its Lying.
https://doi.org/10.48550/arXiv.2304.13734 arXiv:2304.13734 [cs].
Daniel Berend and Aryeh Kontorovich. 2013. On the concentration of the missing mass. Electronic
Communications in Probability 18, none (Jan. 2013), 1–7.
https://doi.org/10.1214/ECP.
v18-2359 Publisher: Institute of Mathematical Statistics and Bernoulli Society.
Joseph R. Jr. Biden. 2023.
Executive Order on the Safe, Secure, and Trustworthy Develop-
ment and Use of Artificial Intelligence.
https://www.whitehouse.gov/briefing-room/
presidential-actions/2023/10/30/
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las
Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore,
Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon
Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving
language models by retrieving from trillions of tokens.
http://arxiv.org/abs/2112.04426
arXiv:2112.04426 [cs].
Mark Braverman, Xinyi Chen, Sham Kakade, Karthik Narasimhan, Cyril Zhang, and Yi Zhang.
2020. Calibration, Entropy Rates, and Memory in Language Models. In Proceedings of the 37th
International Conference on Machine Learning. PMLR, 1089–1099. https://proceedings.mlr.
press/v119/braverman20a.html ISSN: 2640-3498.
17

Jaros law B lasiok, Parikshit Gopalan, Lunjia Hu, Adam Tauman Kalai, and Preetum Nakkiran.
2023. Loss minimization yields multicalibration for large neural networks.
https://doi.org/
10.48550/arXiv.2304.09424 arXiv:2304.09424 [cs, stat].
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra,
Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan
Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with
Pathways.
arXiv preprint arXiv:2204.02311 (2022).
https://arxiv.org/abs/2204.02311
Publisher: arXiv eprint: 2204.02311.
A. P. Dawid. 1982. The Well-Calibrated Bayesian. J. Amer. Statist. Assoc. 77, 379 (Sept. 1982),
605–610.
https://doi.org/10.1080/01621459.1982.10477856
Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. 2022.
On the Origin of
Hallucinations in Conversational Models: Is it the Datasets or the Models? arXiv.
https:
//doi.org/10.48550/ARXIV.2204.07931 Version Number: 1.
Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills,
Luca Righetti, and William Saunders. 2021. Truthful AI: Developing and governing AI that does
not lie.
https://doi.org/10.48550/arXiv.2110.06674 arXiv:2110.06674 [cs].
Katja Filippova. 2020. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data.
http://arxiv.org/abs/2010.05873 arXiv:2010.05873 [cs].
I. J. Good. 1953. The Population Frequences of Species and the Estimation of Population Parameters.
Biometrika 40, 3-4 (Dec. 1953), 237–264.
https://doi.org/10.1093/biomet/40.3-4.237
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong
Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A Survey on Hallucination
in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions.
http:
//arxiv.org/abs/2311.05232 arXiv:2311.05232 [cs].
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation.
Comput. Surveys 55, 12 (Dec. 2023), 1–38.
https://doi.org/10.1145/3571730
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How Can We Know When
Language Models Know?
On the Calibration of Language Models for Question Answering.
Transactions of the Association for Computational Linguistics 9 (Sept. 2021), 962–977.
https:
//doi.org/10.1162/tacl_a_00407
18

Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk,
Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav
Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane
Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark,
Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language
Models (Mostly) Know What They Know.
https://doi.org/10.48550/arXiv.2207.05221
arXiv:2207.05221 [cs].
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
TruthfulQA: Measuring How Models
Mimic Human Falsehoods. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and
Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 3214–3252.
https://doi.org/10.18653/v1/2022.acl-long.229
Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. SelfCheckGPT: Zero-Resource
Black-Box Hallucination Detection for Generative Large Language Models.
http://arxiv.org/
abs/2303.08896 arXiv:2303.08896 [cs].
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness
and Factuality in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and
Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 1906–1919.
https:
//doi.org/10.18653/v1/2020.acl-main.173
David McAllester and Robert E Schapire. 2000. On the Convergence Rate of Good-Turing Estima-
tors.
David
A.
McAllester
and
Luis
E.
Ortiz.
2003.
Concentration
Inequalities
for
the
Missing Mass and for Histogram Rule Error.
https://www.semanticscholar.org/
paper/Concentration-Inequalities-for-the-Missing-Mass-and-McAllester-Ortiz/
d9379f1b3542f47454da4336173596b9008f642c
Michelle M. Mello and Neel Guha. 2023. ChatGPT and Physicians’ Malpractice Risk. JAMA Health
Forum 4, 5 (May 2023), e231938.
https://doi.org/10.1001/jamahealthforum.2023.1938
Merriam-Webster. 2023. Definition of HALLUCINATION.
https://www.merriam-webster.com/
dictionary/hallucination
Jeremy Nixon, Mike Dusenberry, Ghassen Jerfel, Timothy Nguyen, Jeremiah Liu, Linchuan Zhang,
and Dustin Tran. 2020. Measuring Calibration in Deep Learning.
https://doi.org/10.48550/
arXiv.1904.01685 arXiv:1904.01685 [cs, stat].
OpenAI. 2023. GPT-4 Technical Report.
http://arxiv.org/abs/2303.08774 arXiv:2303.08774
[cs].
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.
Object Hallucination in Image Captioning. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and
19

Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 4035–4045.
https://doi.org/10.18653/v1/D18-1437
Rachel Shin. 2023.
Humiliated lawyers fined $5,000 for submitting ChatGPT halluci-
nations in court:
‘I heard about this new site,
which I falsely assumed was,
like,
a super search engine’.
Fortune
(June 2023).
https://fortune.com/2023/06/23/
lawyers-fined-filing-chatgpt-hallucinations-in-court/
Bowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric P. Xing, and Zhiting Hu. 2021. Progressive
Generation of Long Text with Pretrained Language Models. http://arxiv.org/abs/2006.15720
arXiv:2006.15720 [cs].
Kumiko Tanaka-Ishii. 2007.
Word-based predictive text entry using adaptive language mod-
els. Natural Language Engineering 13, 1 (March 2007), 51–74.
https://doi.org/10.1017/
S1351324905004080 Publisher: Cambridge University Press.
Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung,
Denny Zhou, Quoc Le, and Thang Luong. 2023. FreshLLMs: Refreshing Large Language Models
with Search Engine Augmentation.
http://arxiv.org/abs/2310.03214 arXiv:2310.03214 [cs].
Karen Weise and Cade Metz. 2023. When A.I. Chatbots Hallucinate. The New York Times (May
2023).
https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.
html
Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat.
2023. Understanding and Detecting Hallucinations in Neural Machine Translation via Model
Introspection. Transactions of the Association for Computational Linguistics 11 (June 2023),
546–564.
https://doi.org/10.1162/tacl_a_00563
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. 2023b. How Language
Model Hallucinations Can Snowball.
http://arxiv.org/abs/2305.13534 arXiv:2305.13534
[cs].
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao,
Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi.
2023a. Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.
http://arxiv.org/abs/2309.01219 arXiv:2309.01219 [cs].
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate Before Use:
Improving Few-shot Performance of Language Models. In Proceedings of the 38th International
Conference on Machine Learning. PMLR, 12697–12706.
https://proceedings.mlr.press/
v139/zhao21c.html ISSN: 2640-3498.
A
Good-Turing estimator bounds
The distribution bounds here are stated for a general set S, i.i.d. sample s := (s1, s2, . . . , sn) ∈Sn
from an arbitrary distribution D ∈∆(S).
20

Definition 5 (Missing mass). For distribution D ∈∆(S), n ≥1, and sample s ∈Sn, the missing
mass is:
MD(s) := D (S \ {s1, s2 . . . , sn}) .
Theorem 4 ((Berend and Kontorovich, 2013; McAllester and Ortiz, 2003)). For any distribution
D ∈∆(S) and any n ≥1, ε ≥0, let M
n
D := Es∼Dn[MD(s)]. Then:
Pr
s∼Dn[MD(s) ≥M
n
D + ε] ≤e−ε2n
(9)
Pr
s∼Dn[MD(s) ≤M
n
D −ε] ≤e−1.92ε2n
(10)
Eq. (9) is Theorem 16 of McAllester and Ortiz (2003) and Theorem 2.1 of Berend and Kontorovich
(2013). Eq. (10) is Theorem 2.2 of Berend and Kontorovich (2013), though for simplicity we use the
worse bound of e−ε2n which was also present in McAllester and Ortiz (2003).
Definition 6 (Good Turing estimator). For n ≥1, set S, and sample s ∈Sn, the Good-Turing
estimator (Good, 1953) is,
GT(s) := 1
n
{i ∈[n] | ∀j ̸= i si ̸= sj}
.
In words, the estimator above is defined as the fraction of elements of a sample each of which
appears exactly once in the sample.
Lemma 5 ((McAllester and Ortiz, 2003)). For any distribution D ∈∆(S) and any n ≥1, δ ∈(0, 1],
let GT
n
D := Es∼Dn[GTD(s)]. Then:
Pr
s∼Dn
"
GTD(s) ≥GT
n
D +
r
2 ln 1/δ
n
#
≤δ
(11)
Pr
s∼Dn
"
GTD(s) ≤GT
n
D −
r
2 ln 1/δ
n
#
≤δ
(12)
Eq. (11) is Theorem 16 of McAllester and Ortiz (2003) and Eq. (12) has the identical 1-line
proof using McDiarmid’s inequality.
The next lemma says that the expected values of the missing mass and unique elements in
training data are very close.
Lemma 6. For any n ≥1 and any D ∈∆(S),
M
n
D ≤GT
n
D ≤M
n
D + 1
n
for M
n
D as defined in Theorem 4 and GT
n
D as defined in Lemma 5.
Corollary 4. For any set S, distribution D ∈∆(S), any n ≥1, δ ∈(0, 1],
Pr
s∼Dn
"
|MD(s) −GT(s)| ≤1
n + 2.42
r
ln(4/δ)
n
#
≥1 −δ.
(13)
Pr
s∼Dn
"
MD(s) ≥GT(s) −1
n −2.14
r
ln(2/δ)
n
#
≥1 −δ.
(14)
21

Proof. Eq. (13) is established by setting ε =
p
ln(4/δ)/n in Eqs. (9) and (10), which by the union
bound implies,
Pr
s∼Dn
"
MD(s) −M
n
D
 ≥
r
ln(4/δ)
n
#
≤δ
4 + δ
4 = δ
2.
Plugging δ′ = δ/4 in Eqs. (11) and (12) and the union bound gives,
Pr
s∼Dn
"
GT(s) −GT
n
D
 ≥
r
2 ln(4/δ)
n
#
≤δ
4 + δ
4 = δ
2.
Combining the above two with Lemma 6, the triangle inequality, and the fact that 1 +
√
2 ≤2.41
give Eq. (13).
Similarly, Eq. (14) follows by using ε =
p
ln(2/δ)/(1.92n) in Eq. (10) and Lemma 6 and Eq. (11)
with δ/2 and summing the corresponding three inequalities to give,
Pr
"
MD(s) ≥GT(s) −1
n −
r
ln(2/δ)
1.92n −
r
2 ln(2/δ)
n
#
≤δ
2 + δ
2 ≤δ.
Using the fact that
p
1/1.92 +
√
2 < 2.14 completes the proof.
We now simplify the above expression.
Corollary 5. For any set S, distribution D ∈∆(S), any n ≥1,
∀δ ∈(0, 1]
Pr
s∼Dn
"
|MD(s) −GT(s)| ≤3
r
ln(4/δ)
n
#
≥1 −δ.
(15)
∀δ ∈(0, 1/3]
Pr
s∼Dn
"
MD(s) ≥GT(s) −
r
6 ln(2/δ)
n
#
≥1 −δ.
(16)
Proof. We first show Eq. (15). Note that Eq. (15) holds trivially for n ≤9 because GT(s), MD(s) ∈
[0, 1] and 3
p
ln(4)/9 > 1. Thus, from Cor. 4, it suffices to verify that for n > 9, δ ≤1:
1
n + 2.42
r
ln(4/δ)
n
≤3
r
ln(4/δ)
n
In other words, we need
0.58
r
ln(4/δ)
n
≥1
n.
Squaring and simplifying, this is
n ≥
1
(0.58)2 ln(4/δ).
Since the RHS is a monotonic increasing function of δ, we can use its largest value of δ = 1, and it
suffices to have n > 4.29.
For Eq. (16), note that it holds trivially for n ≤6 because
p
6 ln(2/δ)/n ≥
p
6 ln(6)/6 > 1.
Thus, from Cor. 4, it suffices to verify that for n > 6, δ ≤1/3:
1
n + 2.14
r
ln(2/δ)
n
≤
r
6 ln(2/δ)
n
22

In other words, we need
(
√
6 −2.14)
r
ln(2/δ)
n
≥1
n.
Squaring and simplifying, this is
n ≥
1
(
√
6 −2.14)2 ln(2/δ).
Since the RHS is a monotonic increasing function of δ, we can use its largest value of δ = 1/3, and
it suffices to have n > 5.83.
B
Generalizations and alternatives
There are alternative models one could consider. Here we discuss alternatives in terms of log-loss and
prompts, as well as fixed-width (non-adaptive) binning strategies and other notions of calibration.
B.1
Hallucination with Prompts
Many uses of LMs require text to be generated conditionally based on a prefix string called a prompt.
The degree of hallucination will heavily depend on the distribution of prompts and their lengths.
Here we observe how prompts can be an additional source of hallucinations, but at the same time
they can also make hallucinations unnecessary, depending on their distributions. The analysis we
have performed earlier covers unconditional generation in which there are no prompts, which is like
zero-length prompts.
First, observe that a certain distribution over prompts can make hallucination unnecessary.
In particular, if the prompts themselves are very long, i.e., complete documents (ending with an
end-of-document token if there is one), distributed exactly as DLM, then any DLM which offers
empty string completions will give statistically perfect completions and never hallucinate. More
generally, this (arguably good) situation occurs whenever the facts are contained in the prompts
and not their completions.
Second, note that out-of-distribution (OOD) prompts can lead to significantly more hallucinations.
For any LM, one can consider the single worst (adversarial) prompt a which leads to the worst
hallucination rate. (In the introduction we gave the example of the prompt The 15 Elves of San
Salami are named. . . .) The prompt distribution could be concentrated on this single prompt in
which case the hallucination rate would be terrible, assuming there is at least one adversarial
prompt.
B.2
KL-divergence and log-loss
Clearly not all LMs hallucinate. Examples include an LM that avoids memorizing and regurgitating
the training data or even an LM that only output the word yay with probability 1. Such LMs
would score poorly under predictive measures such as log-likelihood on held-out data, but it is not
clear that LMs that perform well under log-likelihood must hallucinate. This suggests that one may
perform the analysis in terms of log-loss Ex∼Dtrain[−log DLM(x)], since LMs are generally trained
to minimize this loss. Equivalently, one can consider the KL divergence which is a non-negative
quantity that measures how far the log-loss is from its minimum achievable value, the entropy of
Dtrain. Ex∼Dtrain[log Dtrain(x) −log DLM(x)].
23

One advantage of calibration analysis over KL-divergence is that, from a statistical perspective
ignoring computation, one can generally expect to achieve calibration (miscalibration close to 0),
e.g., by generating a uniformly random factoid. In contrast, one cannot expect to achieve KL
divergence close to 0. Additionally, one can take any DLM which hallucinates and convert it to a
model which does not without significantly increasing its KL divergence by, for example, mixing it
with a model which regurgitates the training distribution. Specifically, consider a model DLM which,
with probability 99% outputs the word yay and with probability 1% outputs x ∼DLM. This model
hallucinates with probability < 1%. Furthermore, the log-loss this new model at most log 100 bits
larger than that of DLM. This difference is small difference on the scale of the entropy of documents,
especially for longer documents since entropy typically scales linearly with the document length.
B.3
Alternative calibration definitions
In this section, we define a more standard alternative definition of calibration based on log-probability
bins of equal width. Recall that Bz := {y ∈Y | g(y) = z} and BI := {y ∈Y | g(y) ∈I}. We now
define bins of fixed width in probability space.
Definition 7 (Binning). For ε ∈(0, 1), the binning B(g, ε) with equally spaced bins in log-probability
space, is the following partition:
B(g, ε) :=

B((1−ε)i+1,(1−ε)i]
 i = 0, 1, 2, . . .
	
∪{B0} .
(17)
For ε ∈{0, 1}, let B(g, 0) := B(g) = {Bz | z ∈[0, 1]} and B(g, 1) :=

B[0,1]
	
= {Y }.
Thus ε determines the bid widths on a log-scale, with small ε corresponding to narrow bins.
Thus one could use
pB(g,ε) −g

TV as a definition of miscalibration and the corresponding corollary
would follow directly from our previous analysis.
Corollary 6. Fix any δ ∈[0, 1], n ∈N, s ∈R, ε ∈(0, 1) and any s-sparse regular Dworld. Then for
any algorithm A : Xn →∆(X), with probability ≥1 −δ over DL ∼Dworld and xtrain ∼D×n
L ,
g(H) ≥d
MF −
pB(g,ε) −g

TV −3e−s
δ
−
r
6 ln(6/δ)
n
.
Proof. The proof of this Corollary follows exactly the same proofs as that of Corollary 1 except
that we use pB(g,ε) in place of Vb(g).
We next use an even more standard definition which is not based on statistical distance. Recall
that our first definition of calibration, without limits on bins as in Eq. (2), can be written as,
Mis∞(g, p) :=
pB(g) −g

TV = 1
2
X
B∈B(g)
X
y∈B

p(B)
B
−g(y)
 = 1
2
X
B∈B(g)
|p(B) −g(B)| .
This is the most obvious definition and the question is how to generalize it to bins. The above also
suggests the following alternative generative definition of calibration error.
Definition 8 (Generative calibration error). For ε ∈[0, 1], and distributions p, g ∈∆(Y ), the
ε-generative calibration error is,
GCEε(g, p) := 1
2
X
B∈B(g,ε)
p(B) −g(B)
.
24

This definition means that GCE0(g, p) = Mis∞(g, p). Note that these two definitions of calibra-
tion error are related by the following lemma.
Lemma 7. Let ε ∈[0, 1]. Then,
pB(g,ε) −g

TV −ε ≤GCEε(g, p) ≤
pB(g,ε) −g

TV .
Before we prove Lemma 7, we observe that Corollary 6 implies the following.
Corollary 7. Fix any δ ∈[0, 1], n ∈N, s ∈R, ε ∈(0, 1) and any s-sparse regular Dworld. Then for
any algorithm A : Xn →∆(X), with probability ≥1 −δ over DL ∼Dworld and xtrain ∼D×n
L ,
g(H) ≥d
MF −GCEε(g, p) −ε −3e−s
δ
−
r
6 ln(6/δ)
n
.
We now return to prove Lemma 7.
Proof of Lemma 7. Let Π = B(g, ε). Then,
GCEε(g, p) = 1
2
X
B∈Π
|p(B) −g(B)|
= 1
2
X
B∈Π
X
y∈B
1
|B| |p(B) −g(B)|
= 1
2
X
B∈Π
X
y∈B

p(B)
|B| −g(B)
|B|

= 1
2
X
B∈Π
X
y∈B
pΠ(y) −gΠ(y)

= 1
2
X
B∈Π
X
y∈B
pΠ(y) −g(y) + g(y) −gΠ(y)

≥1
2
X
B∈Π
X
y∈B
pΠ(y) −g(y)
 −
gΠ(y) −g(y)

by |a + b| ≥|b| −|a|
=
pΠ −g

TV −
gΠ −g

TV .
This proves the RHS inequality of the lemma. Thus it suffices to show
gΠ −g

TV ≤ε. We first
claim that for all y:
gΠ(y) −g(y) ≤εgΠ(y).
(18)
Let B ∈Π be the bin containing y ∈B. Now, recall that each bucket can be written as:
Bg
I =

y
 g(y) ∈I
	
for some interval I ⊆[0, 1].
If I = [0, 0], then Eq. (18) is trivially true because g(y) = 0 = gΠ(y).
Otherwise, say I =
((1 −ε)(i + 1), (1 −ε)i] for some i ≥0. Then, by definition of gΠ,
gΠ(y) = g(B)
|B| ∈I,
25

because the weighted average of an numbers in an interval is also contained in the interval. Since
this interval has (multiplicative) width e−ε, g(y) ≥(1 −ε)gΠ(y). Equivalently,
gΠ(y) −g(y) ≤εgΠ(y).
Thus we have established Eq. (18) which trivially implies that,
∀y ∈Y
 gΠ(y) −g(y)

+ ≤εgΠ(y).
Therefore,
gΠ −g

TV =
X
y∈Y
 gΠ(y) −g(y)

+ ≤ε
X
y∈Y
gΠ(y) = ε,
which is all that remained to prove the lemma.
C
Proofs of remaining corollaries
To prove Corollaries 1 to 3, we use Markov’s inequality on the expectation of a non-negative random
variable W, which states that Pr[W ≥E[W]/δ] ≤δ. In our case, W := (p(U)−
pΠ −g

TV−g(H))+
and thus using δ →(2/3)δ and Theorem 1 imply that for any partition Π:
Pr
p∼ν

p(U) −
pΠ −g

TV −g(H) ≥3
2δ

max
y∈U Pr
p∼ν[y ∈F] + |O| max
y∈U E
ν [p(y)]

≤2δ
3 .
Rearranging terms,
Pr
p∼ν

g(H) ≥p(U) −
pΠ −g

TV −3
2δ

max
y∈U Pr
p∼ν[y ∈F] + |O| max
y∈U E
ν [p(y)]

≥1 −2δ
3 .
(19)
Also, Corollary 5 of Appendix A with δ →δ/3 implies that for any δ ∈(0, 1] and any DL,
Pr
xtrain∼D×n
L
"
p(U) ≥d
MF −
r
6 ln(6/δ)
n
#
≥1 −δ
3.
(20)
It is now straightforward to prove the corollaries.
Proof of Corollary 1. For a regular Dworld, because it has 1-regular-facts and 1-regular-probabilities,
with probability 1 the posterior satisfies:
max
y∈U Pr
p∼ν[y ∈F] + |O| max
y∈U E
ν [p(y)] ≤E[|F ∩U|]
|U|
+ |O|
|U| E[p(U)] ≤2|F|
|U| ≤2e−s.
In the above we have used the fact that O ⊆F and U ⊇H. The proof follows immediately from
this, Eqs. (19) and (20) and the union bound, using Π = Vb(g).
The proofs of Corollary 2 and Corollary 3 also follow directly, where in Corollary 2 we use the
fact that |O| ≤n.
26

